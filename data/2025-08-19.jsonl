{"id": "2508.11715", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11715", "abs": "https://arxiv.org/abs/2508.11715", "authors": ["Ananya Singha", "Harshita Sahijwani", "Walt Williams", "Emmanuel Aboah Boateng", "Nick Hausman", "Miguel Di Luca", "Keegan Choudhury", "Chaya Binet", "Vu Le", "Tianwei Chen", "Oryan Rokeah Chen", "Sulaiman Vesal", "Sadid Hasan"], "title": "Benchmark Dataset Generation and Evaluation for Excel Formula Repair with LLMs", "comment": "Accepted at the KDD workshop on Evaluation and Trustworthiness of\n  Agentic and Generative AI Models", "summary": "Excel is a pervasive yet often complex tool, particularly for novice users,\nwhere runtime errors arising from logical mistakes or misinterpretations of\nfunctions pose a significant challenge. While large language models (LLMs)\noffer promising assistance by explaining formula errors, the automated\ncorrection of these semantic runtime errors remains an open problem. A primary\nchallenge to advancing models for such scenarios is the severe lack of\nhigh-quality, comprehensive datasets for training and rigorous evaluation. This\npaper addresses this gap by introducing a novel approach for constructing a\nbenchmark dataset specifically designed for Excel formula repair. We propose a\ndata generation pipeline, which leverages a small set of curated seed samples\nfrom online forums to synthetically expand the dataset. Our pipeline integrates\nfew-shot prompting with LLMs and employs a robust \\textit{LLM-as-a-Judge}\nvalidation framework, combined with execution-based checks to ensure the\ncorrectness and semantic fidelity of the generated data. This process produced\na benchmark dataset of 618 high-quality samples, covering common runtime\nerrors. Furthermore, we propose a context-aware baseline technique for Excel\nformula repair that utilizes LLMs to leverage both the faulty formula, and\nrelevant spreadsheet context. We evaluate the performance of various LLMs\n(GPT-4o, GPT-4.1, Phi-3, Mistral) on our newly generated benchmark using\nexecution-based metrics. Our analysis demonstrates the dataset's quality\nthrough manual annotation and provides insights into error and function\ndistributions. The proposed generation methodology is highly scalable and can\nbe readily adapted to create evaluation benchmarks for similar code repair\ntasks in other low-resource programming languages."}
{"id": "2508.11717", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.11717", "abs": "https://arxiv.org/abs/2508.11717", "authors": ["Dhruv Kolhatkar", "Soubhagya Akkena", "Edward F. Gehringer"], "title": "WIP: Leveraging LLMs for Enforcing Design Principles in Student Code: Analysis of Prompting Strategies and RAG", "comment": "Accepted for presentation at the Frontiers in Education Conference,\n  Nashville, Tennessee, USA, 2-5 November 2025", "summary": "This work-in-progress research-to-practice paper explores the integration of\nLarge Language Models (LLMs) into the code-review process for open-source\nsoftware projects developed in computer science and software engineering\ncourses. The focus is on developing an automated feedback tool that evaluates\nstudent code for adherence to key object-oriented design principles, addressing\nthe need for more effective and scalable methods to teach software design best\npractices. The innovative practice involves leveraging LLMs and\nRetrieval-Augmented Generation (RAG) to create an automated feedback system\nthat assesses student code for principles like SOLID, DRY, and design patterns.\nIt analyzes the effectiveness of various prompting strategies and the RAG\nintegration. Preliminary findings show promising improvements in code quality.\nFuture work will aim to improve model accuracy and expand support for\nadditional design principles."}
{"id": "2508.11824", "categories": ["cs.SE", "cs.AI", "cs.CR", "cs.PF"], "pdf": "https://arxiv.org/pdf/2508.11824", "abs": "https://arxiv.org/abs/2508.11824", "authors": ["Satyam Kumar Navneet", "Joydeep Chandra"], "title": "Rethinking Autonomy: Preventing Failures in AI-Driven Software Engineering", "comment": null, "summary": "The integration of Large Language Models (LLMs) into software engineering has\nrevolutionized code generation, enabling unprecedented productivity through\npromptware and autonomous AI agents. However, this transformation introduces\nsignificant risks, including insecure code generation, hallucinated outputs,\nirreversible actions, and a lack of transparency and accountability. Incidents\nlike the Replit database deletion underscore the urgent need for robust safety\nand governance mechanisms. This paper comprehensively analyzes the inherent\nchallenges of LLM-assisted code generation, such as vulnerability inheritance,\novertrust, misinterpretation, and the absence of standardized validation and\nrollback protocols. To address these, we propose the SAFE-AI Framework, a\nholistic approach emphasizing Safety, Auditability, Feedback, and\nExplainability. The framework integrates guardrails, sandboxing, runtime\nverification, risk-aware logging, human-in-the-loop systems, and explainable AI\ntechniques to mitigate risks while fostering trust and compliance. We introduce\na novel taxonomy of AI behaviors categorizing suggestive, generative,\nautonomous, and destructive actions to guide risk assessment and oversight.\nAdditionally, we identify open problems, including the lack of standardized\nbenchmarks for code specific hallucinations and autonomy levels, and propose\nfuture research directions for hybrid verification, semantic guardrails, and\nproactive governance tools. Through detailed comparisons of autonomy control,\nprompt engineering, explainability, and governance frameworks, this paper\nprovides a roadmap for responsible AI integration in software engineering,\naligning with emerging regulations like the EU AI Act and Canada's AIDA to\nensure safe, transparent, and accountable AI-driven development."}
{"id": "2508.11867", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11867", "abs": "https://arxiv.org/abs/2508.11867", "authors": ["Mohammad Baqar", "Saba Naqvi", "Rajat Khanda"], "title": "AI-Augmented CI/CD Pipelines: From Code Commit to Production with Autonomous Decisions", "comment": "13 Pages", "summary": "Modern software delivery has accelerated from quarterly releases to multiple\ndeployments per day. While CI/CD tooling has matured, human decision points\ninterpreting flaky tests, choosing rollback strategies, tuning feature flags,\nand deciding when to promote a canary remain major sources of latency and\noperational toil. We propose AI-Augmented CI/CD Pipelines, where large language\nmodels (LLMs) and autonomous agents act as policy-bounded co-pilots and\nprogressively as decision makers. We contribute: (1) a reference architecture\nfor embedding agentic decision points into CI/CD, (2) a decision taxonomy and\npolicy-as-code guardrail pattern, (3) a trust-tier framework for staged\nautonomy, (4) an evaluation methodology using DevOps Research and Assessment (\nDORA) metrics and AI-specific indicators, and (5) a detailed industrial-style\ncase study migrating a React 19 microservice to an AI-augmented pipeline. We\ndiscuss ethics, verification, auditability, and threats to validity, and chart\na roadmap for verifiable autonomy in production delivery systems."}
{"id": "2508.11710", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11710", "abs": "https://arxiv.org/abs/2508.11710", "authors": ["Hael Abdulhakim Ali Humran", "Ferdi Sonmez"], "title": "Code Vulnerability Detection Across Different Programming Languages with AI Models", "comment": null, "summary": "Security vulnerabilities present in a code that has been written in diverse\nprogramming languages are among the most critical yet complicated aspects of\nsource code to detect. Static analysis tools based on rule-based patterns\nusually do not work well at detecting the context-dependent bugs and lead to\nhigh false positive rates. Recent developments in artificial intelligence,\nspecifically the use of transformer-based models like CodeBERT and CodeLlama,\nprovide light to this problem, as they show potential in finding such flaws\nbetter. This paper presents the implementations of these models on various\ndatasets of code vulnerability, showing how off-the-shelf models can\nsuccessfully produce predictive capacity in models through dynamic fine-tuning\nof the models on vulnerable and safe code fragments. The methodology comprises\nthe gathering of the dataset, normalization of the language, fine-tuning of the\nmodel, and incorporation of ensemble learning and explainable AI. Experiments\nshow that a well-trained CodeBERT can be as good as or even better than some\nexisting static analyzers in terms of accuracy greater than 97%. Further study\nhas indicated that although language models can achieve close-to-perfect\nrecall, the precision can decrease. A solution to this is given by hybrid\nmodels and validation procedures, which will reduce false positives. According\nto the results, the AI-based solutions generalize to different programming\nlanguages and classes of vulnerability. Nevertheless, robustness,\ninterpretability, and deployment readiness are still being developed. The\nresults illustrate the probabilities that AI will enhance the trustworthiness\nin the usability and scalability of machine-learning-based detectors of\nvulnerabilities."}
{"id": "2508.11836", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11836", "abs": "https://arxiv.org/abs/2508.11836", "authors": ["Dave Goel", "Matthew Guzdial", "Anurag Sarkar"], "title": "Finite Automata Extraction: Low-data World Model Learning as Programs from Gameplay Video", "comment": null, "summary": "World models are defined as a compressed spatial and temporal learned\nrepresentation of an environment. The learned representation is typically a\nneural network, making transfer of the learned environment dynamics and\nexplainability a challenge. In this paper, we propose an approach, Finite\nAutomata Extraction (FAE), that learns a neuro-symbolic world model from\ngameplay video represented as programs in a novel domain-specific language\n(DSL): Retro Coder. Compared to prior world model approaches, FAE learns a more\nprecise model of the environment and more general code than prior DSL-based\napproaches."}
{"id": "2508.11958", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.11958", "abs": "https://arxiv.org/abs/2508.11958", "authors": ["Zhipeng Xue", "Xiaoting Zhang", "Zhipeng Gao", "Xing Hu", "Shan Gao", "Xin Xia", "Shanping Li"], "title": "Clean Code, Better Models: Enhancing LLM Performance with Smell-Cleaned Dataset", "comment": null, "summary": "The Large Language Models (LLMs) have demonstrated great potential in\ncode-related tasks. However, most research focuses on improving the output\nquality of LLMs (e.g., correctness), and less attention has been paid to the\nLLM input (e.g., the training code quality). Given that code smells are widely\nexisted in practice and can negatively impact software maintainability and\nreadability, this study takes the first systematic research to assess and\nimprove dataset quality in terms of code smells. In this work, we first conduct\na preliminary study to explore the presence of code smells in a popular\nbenchmark dataset (i.e., CodeSearchNet-Python}) and evaluate the output of\nseveral popular LLMs (i.e., DeepSeek-Coder, CodeLlama, and MagiCoder),\nrevealing that code smell issues extensively exist in LLM's input (e.g.,\nbenchmark dataset) and output (e.g., generated code). We then conduct our\nsystematic research by taking three main steps: Firstly, we propose an\nLLM-based code smell cleaning tool, named SmellCC, which automatically\nrefactors and removes code smells. To evaluate the correctness of the code\nrefactoring, we construct a test set of 50 repositories sourced from the\nCodeSearchNet-Python benchmark for functional testing. Then we apply our\ncurated smell-cleaned dataset to fine-tune two LLMs (i.e., DeepSeek-V2 and\nQwen-Coder) to explore their potential for generating high-quality code.\nThirdly, we investigate the impact of code smells on two downstream tasks: code\ncompletion and code search. Lastly, we derive several actionable implications\nfor software engineering researchers and industry practitioners from our\nfindings."}
{"id": "2508.11711", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11711", "abs": "https://arxiv.org/abs/2508.11711", "authors": ["Irash Perera", "Hiranya Abeyrathne", "Sanjeewa Malalgoda", "Arshardh Ifthikar"], "title": "Enhancing GraphQL Security by Detecting Malicious Queries Using Large Language Models, Sentence Transformers, and Convolutional Neural Networks", "comment": null, "summary": "GraphQL's flexibility, while beneficial for efficient data fetching,\nintroduces unique security vulnerabilities that traditional API security\nmechanisms often fail to address. Malicious GraphQL queries can exploit the\nlanguage's dynamic nature, leading to denial-of-service attacks, data\nexfiltration through injection, and other exploits. Existing solutions, such as\nstatic analysis, rate limiting, and general-purpose Web Application Firewalls,\noffer limited protection against sophisticated, context-aware attacks. This\npaper presents a novel, AI-driven approach for real-time detection of malicious\nGraphQL queries. Our method combines static analysis with machine learning\ntechniques, including Large Language Models (LLMs) for dynamic schema-based\nconfiguration, Sentence Transformers (SBERT and Doc2Vec) for contextual\nembedding of query payloads, and Convolutional Neural Networks (CNNs), Random\nForests, and Multilayer Perceptrons for classification. We detail the system\narchitecture, implementation strategies optimized for production environments\n(including ONNX Runtime optimization and parallel processing), and evaluate the\nperformance of our detection models and the overall system under load. Results\ndemonstrate high accuracy in detecting various threats, including SQL\ninjection, OS command injection, and XSS exploits, alongside effective\nmitigation of DoS and SSRF attempts. This research contributes a robust and\nadaptable solution for enhancing GraphQL API security."}
{"id": "2508.11850", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11850", "abs": "https://arxiv.org/abs/2508.11850", "authors": ["Milad Yazdani", "Mahdi Mostajabdaveh", "Samin Aref", "Zirui Zhou"], "title": "EvoCut: Strengthening Integer Programs via Evolution-Guided Language Models", "comment": null, "summary": "Integer programming lies at the heart of crucial combinatorial optimization\ntasks but remains challenging due to its NP-hard nature. An effective approach\nfor practically solving integer programs is the manual design of acceleration\ncuts, i.e. inequalities that improve solver performance. However, this creative\nprocess demands deep expertise and is yet to be automated. Our proposed\nframework, EvoCut, automates the generation of acceleration cuts by combining\nlarge language models (LLMs) with an evolutionary search. EvoCut (i)\ninitializes a diverse population of candidate cuts via an LLM-based initializer\nagent; (ii) for each cut empirically evaluates both preservation of the optimal\nsolution and its ability to cut off fractional solutions across a verification\nset; and (iii) iteratively refines the population through evolutionary\ncrossover and mutation agents. We quantify each cut's utility by its relative\nreduction in the solver's optimality gap. Our comparisons against standard\ninteger programming practice show that EvoCut reduces optimality gap by 17-57%\nwithin a fixed time. It obtains the same solutions up to 4 times as fast, and\nobtains higher-quality solutions within the same time limit. Requiring no human\nexpert input, EvoCut reliably generates, improves, and empirically verifies\ncuts that generalize to unseen instances. The code is available at\nhttps://github.com/milad1378yz/EvoCut."}
{"id": "2508.11993", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.11993", "abs": "https://arxiv.org/abs/2508.11993", "authors": ["Kota Someya", "Lei Chen", "Michael J. Decker", "Shinpei Hayashi"], "title": "How Much Can a Behavior-Preserving Changeset Be Decomposed into Refactoring Operations?", "comment": "(C) 2025 IEEE. Personal use of this material is permitted. Permission\n  from IEEE must be obtained for all other uses, in any current or future\n  media, including reprinting/republishing this material for advertising or\n  promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "summary": "Developers sometimes mix behavior-preserving modifications, such as\nrefactorings, with behavior-altering modifications, such as feature additions.\nSeveral approaches have been proposed to support understanding such\nmodifications by separating them into those two parts. Such refactoring-aware\napproaches are expected to be particularly effective when the\nbehavior-preserving parts can be decomposed into a sequence of more primitive\nbehavior-preserving operations, such as refactorings, but this has not been\nexplored. In this paper, as an initial validation, we quantify how much of the\nbehavior-preserving modifications can be decomposed into refactoring operations\nusing a dataset of functionally-equivalent method pairs. As a result, when\nusing an existing refactoring detector, only 33.9% of the changes could be\nidentified as refactoring operations. In contrast, when including 67 newly\ndefined functionally-equivalent operations, the coverage increased by over\n128%. Further investigation into the remaining unexplained differences was\nconducted, suggesting improvement opportunities."}
{"id": "2508.11716", "categories": ["cs.CR", "cs.AI", "cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2508.11716", "abs": "https://arxiv.org/abs/2508.11716", "authors": ["Javier Muñoz-Haro", "Ruben Tolosana", "Ruben Vera-Rodriguez", "Aythami Morales", "Julian Fierrez"], "title": "Privacy-Aware Detection of Fake Identity Documents: Methodology, Benchmark, and Improved Detection Methods (FakeIDet2)", "comment": null, "summary": "Remote user verification in Internet-based applications is becoming\nincreasingly important nowadays. A popular scenario for it consists of\nsubmitting a picture of the user's Identity Document (ID) to a service\nplatform, authenticating its veracity, and then granting access to the\nrequested digital service. An ID is well-suited to verify the identity of an\nindividual, since it is government issued, unique, and nontransferable.\nHowever, with recent advances in Artificial Intelligence (AI), attackers can\nsurpass security measures in IDs and create very realistic physical and\nsynthetic fake IDs. Researchers are now trying to develop methods to detect an\never-growing number of these AI-based fakes that are almost indistinguishable\nfrom authentic (bona fide) IDs. In this counterattack effort, researchers are\nfaced with an important challenge: the difficulty in using real data to train\nfake ID detectors. This real data scarcity for research and development is\noriginated by the sensitive nature of these documents, which are usually kept\nprivate by the ID owners (the users) and the ID Holders (e.g., government,\npolice, bank, etc.). The main contributions of our study are: 1) We propose and\ndiscuss a patch-based methodology to preserve privacy in fake ID detection\nresearch. 2) We provide a new public database, FakeIDet2-db, comprising over\n900K real/fake ID patches extracted from 2,000 ID images, acquired using\ndifferent smartphone sensors, illumination and height conditions, etc. In\naddition, three physical attacks are considered: print, screen, and composite.\n3) We present a new privacy-aware fake ID detection method, FakeIDet2. 4) We\nrelease a standard reproducible benchmark that considers physical and synthetic\nattacks from popular databases in the literature."}
{"id": "2508.11860", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11860", "abs": "https://arxiv.org/abs/2508.11860", "authors": ["Frazier N. Baker", "Daniel Adu-Ampratwum", "Reza Averly", "Botao Yu", "Huan Sun", "Xia Ning"], "title": "LARC: Towards Human-level Constrained Retrosynthesis Planning through an Agentic Framework", "comment": "24 pages, 5 figures", "summary": "Large language model (LLM) agent evaluators leverage specialized tools to\nground the rational decision-making of LLMs, making them well-suited to aid in\nscientific discoveries, such as constrained retrosynthesis planning.\nConstrained retrosynthesis planning is an essential, yet challenging, process\nwithin chemistry for identifying synthetic routes from commercially available\nstarting materials to desired target molecules, subject to practical\nconstraints. Here, we present LARC, the first LLM-based Agentic framework for\nRetrosynthesis planning under Constraints. LARC incorporates agentic constraint\nevaluation, through an Agent-as-a-Judge, directly into the retrosynthesis\nplanning process, using agentic feedback grounded in tool-based reasoning to\nguide and constrain route generation. We rigorously evaluate LARC on a\ncarefully curated set of 48 constrained retrosynthesis planning tasks across 3\nconstraint types. LARC achieves a 72.9% success rate on these tasks, vastly\noutperforming LLM baselines and approaching human expert-level success in\nsubstantially less time. The LARC framework is extensible, and serves as a\nfirst step towards an effective agentic tool or a co-scientist to human experts\nfor constrained retrosynthesis."}
{"id": "2508.12232", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12232", "abs": "https://arxiv.org/abs/2508.12232", "authors": ["Arshia Akhavan", "Alireza Hosseinpour", "Abbas Heydarnoori", "Mehdi Keshani"], "title": "LinkAnchor: An Autonomous LLM-Based Agent for Issue-to-Commit Link Recovery", "comment": null, "summary": "Issue-to-commit link recovery plays an important role in software\ntraceability and improves project management. However, it remains a challenging\ntask. A study on GitHub shows that only 42.2% of the issues are correctly\nlinked to their commits. This highlights the potential for further development\nand research in this area. Existing studies have employed various AI/ML-based\napproaches, and with the recent development of large language models,\nresearchers have leveraged LLMs to tackle this problem. These approaches suffer\nfrom two main issues. First, LLMs are constrained by limited context windows\nand cannot ingest all of the available data sources, such as long commit\nhistories, extensive issue comments, and large code repositories. Second, most\nmethods operate on individual issue-commit pairs; that is, given a single\nissue-commit pair, they determine whether the commit resolves the issue. This\nquickly becomes impractical in real-world repositories containing tens of\nthousands of commits. To address these limitations, we present LinkAnchor, the\nfirst autonomous LLM-based agent designed for issue-to-commit link recovery.\nThe lazy-access architecture of LinkAnchor enables the underlying LLM to access\nthe rich context of software, spanning commits, issue comments, and code files,\nwithout exceeding the token limit by dynamically retrieving only the most\nrelevant contextual data. Additionally, LinkAnchor is able to automatically\npinpoint the target commit rather than exhaustively scoring every possible\ncandidate. Our evaluations show that LinkAnchor outperforms state-of-the-art\nissue-to-commit link recovery approaches by 60-262% in Hit@1 score across all\nour case study projects. We also publicly release LinkAnchor as a ready-to-use\ntool, along with our replication package. LinkAnchor is designed and tested for\nGitHub and Jira, and is easily extendable to other platforms."}
{"id": "2508.11742", "categories": ["cs.CR", "cs.NI"], "pdf": "https://arxiv.org/pdf/2508.11742", "abs": "https://arxiv.org/abs/2508.11742", "authors": ["Minhao Jin", "Hongyu He", "Maria Apostolaki"], "title": "Assessing User Privacy Leakage in Synthetic Packet Traces: An Attack-Grounded Approach", "comment": null, "summary": "Current synthetic traffic generators (SynNetGens) promise privacy but lack\ncomprehensive guarantees or empirical validation, even as their fidelity\nsteadily improves. We introduce the first attack-grounded benchmark for\nassessing the privacy of SynNetGens directly from the traffic they produce. We\nframe privacy as membership inference at the traffic-source level--a realistic\nand actionable threat for data holders. To this end, we present TraceBleed, the\nfirst attack that exploits behavioral fingerprints across flows using\ncontrastive learning and temporal chunking, outperforming prior membership\ninference baselines by 172%. Our large-scale study across GAN-, diffusion-, and\nGPT-based SynNetGens uncovers critical insights: (i) SynNetGens leak user-level\ninformation; (ii) differential privacy either fails to stop these attacks or\nseverely degrades fidelity; and (iii) sharing more synthetic data amplifies\nleakage by 59% on average. Finally, we introduce TracePatch, the first\nSynNetGen-agnostic defense that combines adversarial ML with SMT constraints to\nmitigate leakage while preserving fidelity."}
{"id": "2508.11894", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11894", "abs": "https://arxiv.org/abs/2508.11894", "authors": ["Ao Li", "Bin Yan", "Bingfeng Cai", "Chenxi Li", "Cunzhong Zhao", "Fugen Yao", "Gaoqiang Liu", "Guanjun Jiang", "Jian Xu", "Liang Dong", "Liansheng Sun", "Rongshen Zhang", "Xiaolei Gui", "Xin Liu", "Xin Shang", "Yao Wu", "Yu Cao", "Zhenxin Ma", "Zhuang Jia"], "title": "QuarkMed Medical Foundation Model Technical Report", "comment": "20 pages", "summary": "Recent advancements in large language models have significantly accelerated\ntheir adoption in healthcare applications, including AI-powered medical\nconsultations, diagnostic report assistance, and medical search tools. However,\nmedical tasks often demand highly specialized knowledge, professional accuracy,\nand customization capabilities, necessitating a robust and reliable foundation\nmodel. QuarkMed addresses these needs by leveraging curated medical data\nprocessing, medical-content Retrieval-Augmented Generation (RAG), and a\nlarge-scale, verifiable reinforcement learning pipeline to develop a\nhigh-performance medical foundation model. The model achieved 70% accuracy on\nthe Chinese Medical Licensing Examination, demonstrating strong generalization\nacross diverse medical benchmarks. QuarkMed offers a powerful yet versatile\npersonal medical AI solution, already serving over millions of users at\nai.quark.cn."}
{"id": "2508.12285", "categories": ["cs.SE", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.12285", "abs": "https://arxiv.org/abs/2508.12285", "authors": ["Yunbo Lyu", "Zhou Yang", "Jieke Shi", "Jianming Chang", "Yue Liu", "David Lo"], "title": "\"My productivity is boosted, but ...\" Demystifying Users' Perception on AI Coding Assistants", "comment": "13 pages, Camera-Ready Version that will appear in ASE 2025", "summary": "This paper aims to explore fundamental questions in the era when AI coding\nassistants like GitHub Copilot are widely adopted: what do developers truly\nvalue and criticize in AI coding assistants, and what does this reveal about\ntheir needs and expectations in real-world software development? Unlike\nprevious studies that conduct observational research in controlled and\nsimulated environments, we analyze extensive, first-hand user reviews of AI\ncoding assistants, which capture developers' authentic perspectives and\nexperiences drawn directly from their actual day-to-day work contexts. We\nidentify 1,085 AI coding assistants from the Visual Studio Code Marketplace.\nAlthough they only account for 1.64% of all extensions, we observe a surge in\nthese assistants: over 90% of them are released within the past two years. We\nthen manually analyze the user reviews sampled from 32 AI coding assistants\nthat have sufficient installations and reviews to construct a comprehensive\ntaxonomy of user concerns and feedback about these assistants. We manually\nannotate each review's attitude when mentioning certain aspects of coding\nassistants, yielding nuanced insights into user satisfaction and\ndissatisfaction regarding specific features, concerns, and overall tool\nperformance. Built on top of the findings-including how users demand not just\nintelligent suggestions but also context-aware, customizable, and\nresource-efficient interactions-we propose five practical implications and\nsuggestions to guide the enhancement of AI coding assistants that satisfy user\nneeds."}
{"id": "2508.11797", "categories": ["cs.CR", "cs.DB", "cs.DC"], "pdf": "https://arxiv.org/pdf/2508.11797", "abs": "https://arxiv.org/abs/2508.11797", "authors": ["Calkin Garg", "Omar Rios Cruz", "Tessa Andersen", "Gaby G. Dagher", "Donald Winiecki", "Min Long"], "title": "AegisBlock: A Privacy-Preserving Medical Research Framework using Blockchain", "comment": "Submitted to IEEE Conference on Collaboration and Internet Computing\n  2025", "summary": "Due to HIPAA and other privacy regulations, it is imperative to maintain\npatient privacy while conducting research on patient health records. In this\npaper, we propose AegisBlock, a patient-centric access controlled framework to\nshare medical records with researchers such that the anonymity of the patient\nis maintained while ensuring the trustworthiness of the data provided to\nresearchers. AegisBlock allows for patients to provide access to their medical\ndata, verified by miners. A researcher submits a time-based range query to\nrequest access to records from a certain patient, and upon patient approval,\naccess will be granted. Our experimental evaluation results show that\nAegisBlock is scalable with respect to the number of patients and hospitals in\nthe system, and efficient with up to 50% of malicious miners."}
{"id": "2508.11944", "categories": ["cs.AI", "cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.11944", "abs": "https://arxiv.org/abs/2508.11944", "authors": ["Hongtao Liu", "Zhicheng Du", "Zihe Wang", "Weiran Shen"], "title": "CHBench: A Cognitive Hierarchy Benchmark for Evaluating Strategic Reasoning Capability of LLMs", "comment": null, "summary": "Game-playing ability serves as an indicator for evaluating the strategic\nreasoning capability of large language models (LLMs). While most existing\nstudies rely on utility performance metrics, which are not robust enough due to\nvariations in opponent behavior and game structure. To address this limitation,\nwe propose \\textbf{Cognitive Hierarchy Benchmark (CHBench)}, a novel evaluation\nframework inspired by the cognitive hierarchy models from behavioral economics.\nWe hypothesize that agents have bounded rationality -- different agents behave\nat varying reasoning depths/levels. We evaluate LLMs' strategic reasoning\nthrough a three-phase systematic framework, utilizing behavioral data from six\nstate-of-the-art LLMs across fifteen carefully selected normal-form games.\nExperiments show that LLMs exhibit consistent strategic reasoning levels across\ndiverse opponents, confirming the framework's robustness and generalization\ncapability. We also analyze the effects of two key mechanisms (Chat Mechanism\nand Memory Mechanism) on strategic reasoning performance. Results indicate that\nthe Chat Mechanism significantly degrades strategic reasoning, whereas the\nMemory Mechanism enhances it. These insights position CHBench as a promising\ntool for evaluating LLM capabilities, with significant potential for future\nresearch and practical applications."}
{"id": "2508.12303", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.12303", "abs": "https://arxiv.org/abs/2508.12303", "authors": ["Xu Long", "Yishun Wang", "Xiaoqi Li"], "title": "From Fomo3D to Lottery DAPP: Analysis of Ethereum-Based Gambling Applications", "comment": null, "summary": "As blockchain technology advances, Ethereum based gambling decentralized\napplications (DApps) represent a new paradigm in online gambling. This paper\nexamines the concepts, principles, implementation, and prospects of Ethereum\nbased gambling DApps. First, we outline the concept and operational principles\nof gambling DApps. These DApps are blockchain based online lottery platforms.\nThey utilize smart contracts to manage the entire lottery process, including\nissuance, betting, drawing, and prize distribution. Being decentralized,\nlottery DApps operate without central oversight, unlike traditional lotteries.\nThis ensures fairness and eliminates control by any single entity. Automated\nsmart contract execution further reduces management costs, increases\nprofitability, and enhances game transparency and credibility. Next, we analyze\nan existing Ethereum based gambling DApp, detailing its technical principles,\nimplementation, operational status, vulnerabilities, and potential solutions.\nWe then elaborate on the implementation of lottery DApps. Smart contracts\nautomate the entire lottery process including betting, drawing, and prize\ndistribution. Although developing lottery DApps requires technical expertise,\nthe expanding Ethereum ecosystem provides growing tools and frameworks,\nlowering development barriers. Finally, we discuss current limitations and\nprospects of lottery DApps. As blockchain technology and smart contracts\nevolve, lottery DApps are positioned to significantly transform the online\nlottery industry. Advantages like decentralization, automation, and\ntransparency will likely drive broader future adoption."}
{"id": "2508.11812", "categories": ["cs.CR", "cs.CY", "cs.NI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.11812", "abs": "https://arxiv.org/abs/2508.11812", "authors": ["Tyler Schroder", "Sohee Kim Park"], "title": "Securing Sideways: Thwarting Lateral Movement by Implementing Active Directory Tiering", "comment": "11 pages", "summary": "The advancement of computing equipment and the advances in services over the\nInternet has allowed corporations, higher education, and many other\norganizations to pursue the shared computing network environment. A requirement\nfor shared computing environments is a centralized identity system to\nauthenticate and authorize user access. An organization's digital identity\nplane is a prime target for cyber threat actors. When compromised, identities\ncan be exploited to steal credentials, create unauthorized accounts, and\nmanipulate permissions-enabling attackers to gain control of the network and\nundermine its confidentiality, availability, and integrity. Cybercrime losses\nreached a record of 16.6 B in the United States in 2024. For organizations\nusing Microsoft software, Active Directory is the on-premises identity system\nof choice. In this article, we examine the challenge of security compromises in\nActive Directory (AD) environments and present effective strategies to prevent\ncredential theft and limit lateral movement by threat actors. Our proposed\napproaches aim to confine the movement of compromised credentials, preventing\nsignificant privilege escalation and theft. We argue that through our\nillustration of real-world scenarios, tiering can halt lateral movement and\nadvanced cyber-attacks, thus reducing ransom escalation. Our work bridges a gap\nin existing literature by combining technical guidelines with theoretical\narguments in support of tiering, positioning it as a vital component of modern\ncybersecurity strategy even though it cannot function in isolation. As the\nhardware advances and the cloud sourced services along with AI is advancing\nwith unprecedented speed, we think it is important for security experts and the\nbusiness to work together and start designing and developing software and\nframeworks to classify devices automatically and accurately within the tiered\nstructure."}
{"id": "2508.11953", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11953", "abs": "https://arxiv.org/abs/2508.11953", "authors": ["Yuan Li", "Zhengzhong Liu", "Eric Xing"], "title": "Data Mixing Optimization for Supervised Fine-Tuning of Large Language Models", "comment": null, "summary": "Optimizing data mixtures for supervised fine-tuning (SFT) of large language\nmodels (LLMs) is critical for developing general-purpose models, yet this area\nremains underexplored. In this paper, we frame data mixing as an optimization\nproblem and introduce a novel method designed to minimize validation loss. Our\napproach parametrizes the loss by modeling effective data transferred and\nleveraging scaling laws for fine-tuning. By experimenting with various\nsmall-scale data mixtures, we fit these parameters and derive the optimal\nweights. We provide both mathematical proofs and empirical results\ndemonstrating that our algorithm achieves excellent overall and individual\nperformance across all domains. Through controlled experiments, we show that\nmodels trained with our optimized weights perform on par with those using\noptimal weights determined via grid search, with per-domain loss only 0.66%\nhigher than the best domain loss from grid search on average. Additionally, we\nshow that reweighting popular SFT datasets using our method improves both\nvalidation loss and downstream performance. Finally, we discuss how our method\ncan generalize to guide data selection for domain-specific models and provide\ninsights into SFT."}
{"id": "2508.12325", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.12325", "abs": "https://arxiv.org/abs/2508.12325", "authors": ["Tim Kräuter", "Adrian Rutle", "Yngve Lamo", "Harald König", "Francisco Durán"], "title": "Towards the Coordination and Verification of Heterogeneous Systems with Data and Time", "comment": "This is the authors accepted version of a paper to be published in\n  MODELS-2025, DOI: TBD", "summary": "Modern software systems are often realized by coordinating multiple\nheterogeneous parts, each responsible for specific tasks. These parts must work\ntogether seamlessly to satisfy the overall system requirements. To verify such\ncomplex systems, we have developed a non-intrusive coordination framework\ncapable of performing formal analysis of heterogeneous parts that exchange data\nand include real-time capabilities. The framework utilizes a linguistic\nextension, which is implemented as a central broker and a domain-specific\nlanguage for the integration of heterogeneous languages and coordination of\nparts. Moreover, abstract rule templates are reified as language adapters for\nnon-intrusive communications with the broker. The framework is implemented\nusing rewriting logic (Maude), and its applicability is demonstrated by\nverifying certain correctness properties of a heterogeneous road-rail crossing\nsystem."}
{"id": "2508.11817", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.11817", "abs": "https://arxiv.org/abs/2508.11817", "authors": ["Mukesh Poudel", "Nick Rahimi"], "title": "Machine Learning-Based AES Key Recovery via Side-Channel Analysis on the ASCAD Dataset", "comment": null, "summary": "Cryptographic algorithms like AES and RSA are widely used and they are\nmathematically robust and almost unbreakable but its implementation on physical\ndevices often leak information through side channels, such as electromagnetic\n(EM) emissions, potentially compromising said theoretically secure algorithms.\nThis paper investigates the application of machine learning (ML) techniques and\nDeep Learning models to exploit such leakage for partial key recovery. We use\nthe public ASCAD `fixed' and `variable' key dataset, containing 700 and 1400 EM\ntraces respectively from an AES-128 implementation on an 8-bit microcontroller.\nThe problem is framed as a 256-class classification task where we target the\noutput of the first-round S-box operation, which is dependent on a single key\nbyte. We evaluate standard classifiers (Random Forest (RF), Support Vector\nMachine (SVM)), a Convolutional Neural Network(CNN) and a Residual Neural\nNetwork(ResNet). We also explore the utility of RF-based feature importance for\ndimensionality reduction. Crucially, we employ this domain-specific Key Rank\nmetric for evaluation, showing its necessity over standard classification\naccuracy. Our results show that SVM and RF on full features perform poorly in\nkey ranking. However, RF trained on reduced (top 100) identified via importance\nanalysis achieves Rank 0 (successful key byte recovery) using almost half the\nattack traces. The implemented CNN also achieves Rank 0 efficiently using\napproximately 65 attack traces for the fixed-key dataset. The ResNets perform\nbest on large and complex datasets but may not always be the best choice for\nsimple fixed key dataset in terms of efficiency. Thus we conclude that models,\nparticularly CNNs, ResNets and feature-selected RF, coupled with the Key Rank\nmetric, are an effective tool for side-channel key recovery, confirming the\npractical vulnerability of the cryptographic implementations."}
{"id": "2508.11954", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11954", "abs": "https://arxiv.org/abs/2508.11954", "authors": ["Sehyuk Park", "Soyeon Caren Han", "Eduard Hovy"], "title": "UniCast: A Unified Multimodal Prompting Framework for Time Series Forecasting", "comment": null, "summary": "Time series forecasting is a foundational task across domains, such as\nfinance, healthcare, and environmental monitoring. While recent advances in\nTime Series Foundation Models (TSFMs) have demonstrated strong generalisation\nthrough large-scale pretraining, existing models operate predominantly in a\nunimodal setting, ignoring the rich multimodal context, such as visual and\ntextual signals, that often accompanies time series data in real-world\nscenarios. This paper introduces a novel parameter-efficient multimodal\nframework, UniCast, that extends TSFMs to jointly leverage time series, vision,\nand text modalities for enhanced forecasting performance. Our method integrates\nmodality-specific embeddings from pretrained Vision and Text Encoders with a\nfrozen TSFM via soft prompt tuning, enabling efficient adaptation with minimal\nparameter updates. This design not only preserves the generalisation strength\nof the foundation model but also enables effective cross-modal interaction.\nExtensive experiments across diverse time-series forecasting benchmarks\ndemonstrate that UniCast consistently and significantly outperforms all\nexisting TSFM baselines. The findings highlight the critical role of multimodal\ncontext in advancing the next generation of general-purpose time series\nforecasters."}
{"id": "2508.12358", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12358", "abs": "https://arxiv.org/abs/2508.12358", "authors": ["Haolin Jin", "Huaming Chen"], "title": "Uncovering Systematic Failures of LLMs in Verifying Code Against Natural Language Specifications", "comment": "Accepted to the NIER track of the 40th IEEE/ACM International\n  Conference on Automated Software Engineering (ASE 2025)", "summary": "Large language models (LLMs) have become essential tools in software\ndevelopment, widely used for requirements engineering, code generation and\nreview tasks. Software engineers often rely on LLMs to assess whether system\ncode implementation satisfy task requirements, thereby enhancing code\nrobustness and accuracy. However, it remains unclear whether LLMs can reliably\ndetermine whether the code complies fully with the given task descriptions,\nwhich is usually natural language specifications. In this paper, we uncover a\nsystematic failure of LLMs in evaluating whether code aligns with natural\nlanguage requirements. Specifically, with widely used benchmarks, we employ\nunified prompts to judge code correctness. Our results reveal that LLMs\nfrequently misclassify correct code implementations as either ``not satisfying\nrequirements'' or containing potential defects. Surprisingly, more complex\nprompting, especially when leveraging prompt engineering techniques involving\nexplanations and proposed corrections, leads to higher misjudgment rate, which\nhighlights the critical reliability issues in using LLMs as code review\nassistants. We further analyze the root causes of these misjudgments, and\npropose two improved prompting strategies for mitigation. For the first time,\nour findings reveals unrecognized limitations in LLMs to match code with\nrequirements. We also offer novel insights and practical guidance for effective\nuse of LLMs in automated code review and task-oriented agent scenarios."}
{"id": "2508.11907", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11907", "abs": "https://arxiv.org/abs/2508.11907", "authors": ["Xiaojin Zhang", "Mingcong Xu", "Yiming Li", "Wei Chen", "Qiang Yang"], "title": "Deciphering the Interplay between Attack and Protection Complexity in Privacy-Preserving Federated Learning", "comment": null, "summary": "Federated learning (FL) offers a promising paradigm for collaborative model\ntraining while preserving data privacy. However, its susceptibility to gradient\ninversion attacks poses a significant challenge, necessitating robust privacy\nprotection mechanisms. This paper introduces a novel theoretical framework to\ndecipher the intricate interplay between attack and protection complexities in\nprivacy-preserving FL. We formally define \"Attack Complexity\" as the minimum\ncomputational and data resources an adversary requires to reconstruct private\ndata below a given error threshold, and \"Protection Complexity\" as the expected\ndistortion introduced by privacy mechanisms. Leveraging Maximum Bayesian\nPrivacy (MBP), we derive tight theoretical bounds for protection complexity,\ndemonstrating its scaling with model dimensionality and privacy budget.\nFurthermore, we establish comprehensive bounds for attack complexity, revealing\nits dependence on privacy leakage, gradient distortion, model dimension, and\nthe chosen privacy level. Our findings quantitatively illuminate the\nfundamental trade-offs between privacy guarantees, system utility, and the\neffort required for both attacking and defending. This framework provides\ncritical insights for designing more secure and efficient federated learning\nsystems."}
{"id": "2508.11959", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11959", "abs": "https://arxiv.org/abs/2508.11959", "authors": ["Xuanxiang Huang", "Olivier Létoffé", "Joao Marques-Silva"], "title": "Rigorous Feature Importance Scores based on Shapley Value and Banzhaf Index", "comment": null, "summary": "Feature attribution methods based on game theory are ubiquitous in the field\nof eXplainable Artificial Intelligence (XAI). Recent works proposed rigorous\nfeature attribution using logic-based explanations, specifically targeting\nhigh-stakes uses of machine learning (ML) models. Typically, such works exploit\nweak abductive explanation (WAXp) as the characteristic function to assign\nimportance to features. However, one possible downside is that the contribution\nof non-WAXp sets is neglected. In fact, non-WAXp sets can also convey important\ninformation, because of the relationship between formal explanations (XPs) and\nadversarial examples (AExs). Accordingly, this paper leverages Shapley value\nand Banzhaf index to devise two novel feature importance scores. We take into\naccount non-WAXp sets when computing feature contribution, and the novel scores\nquantify how effective each feature is at excluding AExs. Furthermore, the\npaper identifies properties and studies the computational complexity of the\nproposed scores."}
{"id": "2508.12436", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.12436", "abs": "https://arxiv.org/abs/2508.12436", "authors": ["Feifei Niu", "Chuanyi Li", "Haosheng Zuo", "Jionghan Wu", "Xin Xia"], "title": "Feature Request Analysis and Processing: Tasks, Techniques, and Trends", "comment": null, "summary": "Feature requests are proposed by users to request new features or\nenhancements of existing features of software products, which represent users'\nwishes and demands. Satisfying users' demands can benefit the product from both\ncompetitiveness and user satisfaction. Feature requests have seen a rise in\ninterest in the past few years and the amount of research has been growing.\nHowever, the diversity in the research topics suggests the need for their\ncollective analysis to identify the challenges and opportunities so as to\npromote new advances in the future. In this work, following a defined process\nand a search protocol, we provide a systematic overview of the research area by\nsearching and categorizing relevant studies. We select and analyze 131 primary\nstudies using descriptive statistics and qualitative analysis methods. We\nclassify the studies into different topics and group them from the perspective\nof requirements engineering activities. We investigate open tools as well as\ndatasets for future research. In addition, we identify several key challenges\nand opportunities, such as: (1) ensuring the quality of feature requests, (2)\nimproving their specification and validation, and (3) developing high-quality\nbenchmarks for large language model-driven tasks."}
{"id": "2508.11913", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.11913", "abs": "https://arxiv.org/abs/2508.11913", "authors": ["Huipeng Yang", "Li Yang", "Lichuan Ma", "Lu Zhou", "Junbo Jia", "Anyuan Sang", "Xinyue Wang"], "title": "WebGeoInfer: A Structure-Free and Multi-Stage Framework for Geolocation Inference of Devices Exposing Information", "comment": null, "summary": "Remote management devices facilitate critical infrastructure monitoring for\nadministrators but simultaneously increase asset exposure. Sensitive\ngeographical information overlooked in exposed device management pages poses\nsubstantial security risks. Therefore, identifying devices that reveal location\ninformation due to administrator negligence is crucial for cybersecurity\nregulation. Despite the rich information exposed by web interfaces of remote\nmanagement devices, automatically discovering geographical locations remains\nchallenging due to unstructured formats, varying styles, and incomplete\ngeographical details.\n  This study introduces WebGeoInfer, a structure-free geolocation inference\nframework utilizing multi-stage information enhancement. WebGeoInfer clusters\nsimilar device web pages and analyzes inter-cluster differences to extract\npotential geographical information, bypassing structural limitations. Through\nsearch engine enhancement and Large Language Models mining, the framework\nextracts geographical coordinates from identified information. WebGeoInfer\nsuccessfully inferred locations for 5,435 devices across 94 countries and 2,056\ncities, achieving accuracy rates of 96.96\\%, 88.05\\%, and 79.70\\% at country,\ncity, and street levels, respectively."}
{"id": "2508.11975", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11975", "abs": "https://arxiv.org/abs/2508.11975", "authors": ["Gongyao Jiang", "Qiong Luo"], "title": "Chart-CoCa: Self-Improving Chart Understanding of Vision LMs via Code-Driven Synthesis and Candidate-Conditioned Answering", "comment": "Accepted to CIKM 2025", "summary": "Vision Language Models (VLMs) often struggle with chart understanding tasks,\nparticularly in accurate chart description and complex reasoning. Synthetic\ndata generation is a promising solution, while usually facing the challenge of\nnoise labels. To address this challenge, we first introduce a chart synthesis\npipeline that generates aligned chart-question-answer triplets through code\ngeneration and execution, ensuring the reliability of synthetic data without\nhuman intervention. Furthermore, inspired by test-time scaling that increases\ninference budget and thereby improves performance, we design a\ncandidate-conditioned answering process. The VLM first generates multiple\nresponses per query, and then synthesizes the final answer by contextualizing\nthese candidates. Experiments demonstrate significant improvements, with up to\n15.50 points accuracy gain over the initial VLM, in a fully self-improving\nparadigm without either human-labeled data or external models."}
{"id": "2508.12546", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.12546", "abs": "https://arxiv.org/abs/2508.12546", "authors": ["Bin Duan", "Ruican Dong", "Naipeng Dong", "Dan Dongseong Kim", "Guowei Yang"], "title": "XAMT: Cross-Framework API Matching for Testing Deep Learning Libraries", "comment": null, "summary": "Deep learning powers critical applications such as autonomous driving,\nhealthcare, and finance, where the correctness of underlying libraries is\nessential. Bugs in widely used deep learning APIs can propagate to downstream\nsystems, causing serious consequences. While existing fuzzing techniques detect\nbugs through intra-framework testing across hardware backends (CPU vs. GPU),\nthey may miss bugs that manifest identically across backends and thus escape\ndetection under these strategies. To address this problem, we propose XAMT, a\ncross-framework fuzzing method that tests deep learning libraries by matching\nand comparing functionally equivalent APIs across different frameworks. XAMT\nmatches APIs using similarity-based rules based on names, descriptions, and\nparameter structures. It then aligns inputs and applies variance-guided\ndifferential testing to detect bugs. We evaluated XAMT on five popular\nframeworks, including PyTorch, TensorFlow, Keras, Chainer, and JAX. XAMT\nmatched 839 APIs and identified 238 matched API groups, and detected 17 bugs,\n12 of which have been confirmed. Our results show that XAMT uncovers bugs\nundetectable by intra-framework testing, especially those that manifest\nconsistently across backends. XAMT offers a complementary approach to existing\nmethods and offers a new perspective on the testing of deep learning libraries."}
{"id": "2508.11925", "categories": ["cs.CR", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11925", "abs": "https://arxiv.org/abs/2508.11925", "authors": ["Zhimeng Guo", "Huaisheng Zhu", "Siyuan Xu", "Hangfan Zhang", "Teng Xiao", "Minhao Cheng"], "title": "Optimizing Token Choice for Code Watermarking: A RL Approach", "comment": "18 pages, 3 figures", "summary": "The need for detecting LLM-generated code necessitates watermarking systems\ncapable of operating within its highly structured and syntactically constrained\nenvironment. To address this, we introduce CodeTracer, an innovative adaptive\ncode watermarking framework underpinned by a novel reinforcement learning\ntraining paradigm. At its core, CodeTracer features a policy-driven approach\nthat utilizes a parameterized model to intelligently bias token choices during\nnext-token prediction. This strategy ensures that embedded watermarks maintain\ncode functionality while exhibiting subtle yet statistically detectable\ndeviations from typical token distributions. To facilitate policy learning, we\ndevise a comprehensive reward system that seamlessly integrates execution\nfeedback with watermark embedding signals, balancing process-level and\noutcome-level rewards. Additionally, we employ Gumbel Top-k reparameterization\nto enable gradient-based optimization of discrete watermarking decisions.\nExtensive comparative evaluations demonstrate CodeTracer's significant\nsuperiority over state-of-the-art baselines in both watermark detectability and\nthe preservation of generated code's functionality."}
{"id": "2508.11987", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11987", "abs": "https://arxiv.org/abs/2508.11987", "authors": ["Zhiyuan Zeng", "Jiashuo Liu", "Siyuan Chen", "Tianci He", "Yali Liao", "Jinpeng Wang", "Zaiyuan Wang", "Yang Yang", "Lingyue Yin", "Mingren Yin", "Zhenwei Zhu", "Tianle Cai", "Zehui Chen", "Jiecao Chen", "Yantao Du", "Xiang Gao", "Jiacheng Guo", "Liang Hu", "Jianpeng Jiao", "Xiangsheng Li", "Jingkai Liu", "Shuang Ni", "Zhoufutu Wen", "Ge Zhang", "Kaiyuan Zhang", "Xin Zhou", "Jose Blanchet", "Xipeng Qiu", "Mengdi Wang", "Wenhao Huang"], "title": "FutureX: An Advanced Live Benchmark for LLM Agents in Future Prediction", "comment": "Technical report, 51 pages", "summary": "Future prediction is a complex task for LLM agents, requiring a high level of\nanalytical thinking, information gathering, contextual understanding, and\ndecision-making under uncertainty. Agents must not only gather and interpret\nvast amounts of dynamic information but also integrate diverse data sources,\nweigh uncertainties, and adapt predictions based on emerging trends, just as\nhuman experts do in fields like politics, economics, and finance. Despite its\nimportance, no large-scale benchmark exists for evaluating agents on future\nprediction, largely due to challenges in handling real-time updates and\nretrieving timely, accurate answers. To address this, we introduce\n$\\textbf{FutureX}$, a dynamic and live evaluation benchmark specifically\ndesigned for LLM agents performing future prediction tasks. FutureX is the\nlargest and most diverse live benchmark for future prediction, supporting\nreal-time daily updates and eliminating data contamination through an automated\npipeline for question gathering and answer collection. We evaluate 25 LLM/agent\nmodels, including those with reasoning, search capabilities, and integration of\nexternal tools such as the open-source Deep Research Agent and closed-source\nDeep Research models. This comprehensive evaluation assesses agents' adaptive\nreasoning and performance in dynamic environments. Additionally, we provide\nin-depth analyses of agents' failure modes and performance pitfalls in\nfuture-oriented tasks, including the vulnerability to fake web pages and the\ntemporal validity. Our goal is to establish a dynamic, contamination-free\nevaluation standard that drives the development of LLM agents capable of\nperforming at the level of professional human analysts in complex reasoning and\npredictive thinking."}
{"id": "2508.12620", "categories": ["cs.SE", "cs.PL"], "pdf": "https://arxiv.org/pdf/2508.12620", "abs": "https://arxiv.org/abs/2508.12620", "authors": ["Xiaoning Ren", "Qiang Hu", "Wei Ma", "Yan Li", "Yao Zhang", "Lingxiao Jiang", "Yinxing Xue"], "title": "Strengthening Programming Comprehension in Large Language Models through Code Generation", "comment": "11 pages, 7 figures", "summary": "Large language models (LLMs) have recently shown impressive results on\ndiverse code-related tasks, benefiting from large-scale training and\ninstruction tuning. However, studies reveal that their grasp of fundamental\nprogramming concepts, such as data flow and control flow, remains shallow,\nleading to fragile performance when code requires deeper reasoning. This\nlimitation restricts the practical adoption of LLMs in real-world software\ndevelopment. To address this issue, this work introduces a counterfactual code\naugmentation framework combined with concept-aware tuning, designed to guide\nLLMs toward stronger conceptual understanding. Comprehensive evaluation across\nmultiple models and benchmarks demonstrates the effectiveness of the proposed\napproach."}
{"id": "2508.11928", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.11928", "abs": "https://arxiv.org/abs/2508.11928", "authors": ["Lien Tran", "Boyuan Zhang", "Ratchanon Pawanja", "Rashid Hussain Khokhar"], "title": "The Passwordless Authentication with Passkey Technology from an Implementation Perspective", "comment": "6 pages, 3 figures", "summary": "With the rise of sophisticated authentication bypass techniques, passwords\nare no longer considered a reliable method for securing authentication systems.\nIn recent years, new authentication technologies have shifted from traditional\npassword-based logins to passwordless security. Among these, Time-Based\nOne-Time Passwords (TOTP) remain one of the most widely used mechanisms, while\nPasskeys are emerging as a promising alternative with growing adoption. This\npaper highlights the key techniques used during the implementation of the\nauthentication system with Passkey technology. It also suggests considerations\nfor integrating components during system development to ensure that users can\nsecurely access their accounts with minimal complexity, while still meeting the\nrequirements of a robust authentication system that balances security,\nusability, and performance. Additionally, by examining TOTP and Passkey\nmechanisms from an implementation perspective, this work not only addresses\nmajor security concerns such as password leaks, phishing attacks, and\nsusceptibility to brute-force attacks, but also evaluates the feasibility and\neffectiveness of these mechanisms in real-world implementations. This paper\ndemonstrates the superior security of Passkey technology and its potential for\nbroader adoption in secure authentication systems."}
{"id": "2508.11991", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11991", "abs": "https://arxiv.org/abs/2508.11991", "authors": ["Weihao Sun"], "title": "Modeling Relational Logic Circuits for And-Inverter Graph Convolutional Network", "comment": null, "summary": "The automation of logic circuit design enhances chip performance, energy\nefficiency, and reliability, and is widely applied in the field of Electronic\nDesign Automation (EDA).And-Inverter Graphs (AIGs) efficiently represent,\noptimize, and verify the functional characteristics of digital circuits,\nenhancing the efficiency of EDA development.Due to the complex structure and\nlarge scale of nodes in real-world AIGs, accurate modeling is challenging,\nleading to existing work lacking the ability to jointly model functional and\nstructural characteristics, as well as insufficient dynamic information\npropagation capability.To address the aforementioned challenges, we propose\nAIGer.Specifically, AIGer consists of two components: 1) Node logic feature\ninitialization embedding component and 2) AIGs feature learning network\ncomponent.The node logic feature initialization embedding component projects\nlogic nodes, such as AND and NOT, into independent semantic spaces, to enable\neffective node embedding for subsequent processing.Building upon this, the AIGs\nfeature learning network component employs a heterogeneous graph convolutional\nnetwork, designing dynamic relationship weight matrices and differentiated\ninformation aggregation approaches to better represent the original structure\nand information of AIGs.The combination of these two components enhances\nAIGer's ability to jointly model functional and structural characteristics and\nimproves its message passing capability. Experimental results indicate that\nAIGer outperforms the current best models in the Signal Probability Prediction\n(SSP) task, improving MAE and MSE by 18.95\\% and 44.44\\%, respectively. In the\nTruth Table Distance Prediction (TTDP) task, AIGer achieves improvements of\n33.57\\% and 14.79\\% in MAE and MSE, respectively, compared to the\nbest-performing models."}
{"id": "2508.12649", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.12649", "abs": "https://arxiv.org/abs/2508.12649", "authors": ["Lei Chen", "Michele Lanza", "Shinpei Hayashi"], "title": "ChangePrism: Visualizing the Essence of Code Changes", "comment": "5 pages, 5 figures, VISSOFT 2025", "summary": "Understanding the changes made by developers when they submit a pull request\nand/or perform a commit on a repository is a crucial activity in software\nmaintenance and evolution. The common way to review changes relies on examining\ncode diffs, where textual differences between two file versions are highlighted\nin red and green to indicate additions and deletions of lines. This can be\ncumbersome for developers, making it difficult to obtain a comprehensive\noverview of all changes in a commit. Moreover, certain types of code changes\ncan be particularly significant and may warrant differentiation from standard\nmodifications to enhance code comprehension. We present a novel visualization\napproach supported by a tool named ChangePrism, which provides a way to better\nunderstand code changes. The tool comprises two components: extraction, which\nretrieves code changes and relevant information from the git history, and\nvisualization, which offers both general and detailed views of code changes in\ncommits. The general view provides an overview of different types of code\nchanges across commits, while the detailed view displays the exact changes in\nthe source code for each commit."}
{"id": "2508.11939", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.11939", "abs": "https://arxiv.org/abs/2508.11939", "authors": ["James Gu", "Ahmed Sartaj", "Mohammed Akram Taher Khan", "Rashid Hussain Khokhar"], "title": "Design and Implementation of a Controlled Ransomware Framework for Educational Purposes Using Flutter Cryptographic APIs on Desktop PCs and Android Devices", "comment": "6 pages, 1 figure, 1 table, 2 algorithms", "summary": "This study focuses on the creation and implementation of ransomware for\neducational purposes that leverages Python's native cryptographic APIs in a\ncontrolled environment. Additionally, an Android version of the framework is\nimplemented using Flutter and Dart. For both versions, open-source\ncryptographic libraries are utilized. With this framework, researchers can\nsystematically explore the functionalities of ransomware, including file\nencryption processes, cryptographic key management, and victim interaction\ndynamics. To ensure safe experimentation, multiple safeguards are incorporated,\nsuch as the ability to restrict the encryption process to a specific directory,\nproviding the RSA private key for immediate decryption, and narrowing the scope\nof targetable files to a carefully curated list (.txt, .jpg, .csv, .doc). This\npaper draws inspiration from the infamous WannaCry ransomware and aims to\nsimulate its behaviour on Android devices. By making the codebase open-source,\nit enables users to study, modify, and extend the program for pedagogical\npurposes and offers a hands-on tool that can be used to train the next\ngeneration of cybersecurity professionals."}
{"id": "2508.11995", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.11995", "abs": "https://arxiv.org/abs/2508.11995", "authors": ["Xuyang Zhao", "Shiwan Zhao", "Hualong Yu", "Liting Zhang", "Qicheng Li"], "title": "AgentCDM: Enhancing Multi-Agent Collaborative Decision-Making via ACH-Inspired Structured Reasoning", "comment": null, "summary": "Multi-agent systems (MAS) powered by large language models (LLMs) hold\nsignificant promise for solving complex decision-making tasks. However, the\ncore process of collaborative decision-making (CDM) within these systems\nremains underexplored. Existing approaches often rely on either ``dictatorial\"\nstrategies that are vulnerable to the cognitive biases of a single agent, or\n``voting-based\" methods that fail to fully harness collective intelligence. To\naddress these limitations, we propose \\textbf{AgentCDM}, a structured framework\nfor enhancing collaborative decision-making in LLM-based multi-agent systems.\nDrawing inspiration from the Analysis of Competing Hypotheses (ACH) in\ncognitive science, AgentCDM introduces a structured reasoning paradigm that\nsystematically mitigates cognitive biases and shifts decision-making from\npassive answer selection to active hypothesis evaluation and construction. To\ninternalize this reasoning process, we develop a two-stage training paradigm:\nthe first stage uses explicit ACH-inspired scaffolding to guide the model\nthrough structured reasoning, while the second stage progressively removes this\nscaffolding to encourage autonomous generalization. Experiments on multiple\nbenchmark datasets demonstrate that AgentCDM achieves state-of-the-art\nperformance and exhibits strong generalization, validating its effectiveness in\nimproving the quality and robustness of collaborative decisions in MAS."}
{"id": "2508.12922", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.12922", "abs": "https://arxiv.org/abs/2508.12922", "authors": ["Yue Wang", "Zhenyu Chen", "Yuan Zhao", "Chunrong Fang", "Ziyuan Wang", "Song Huang"], "title": "RUM: Rule+LLM-Based Comprehensive Assessment on Testing Skills", "comment": null, "summary": "Over the past eight years, the META method has served as a multidimensional\ntesting skill assessment system in the National College Student Contest on\nSoftware Testing, successfully assessing over 100,000 students' testing skills.\nHowever, META is primarily limited to the objective assessment of test scripts,\nlacking the ability to automatically assess subjective aspects such as test\ncase and test report. To address this limitation, this paper proposes RUM, a\ncomprehensive assessment approach that combines rules and large language models\n(LLMs). RUM achieves a comprehensive assessment by rapidly processing objective\nindicators through rules while utilizing LLMs for in-depth subjective analysis\nof test case documents, test scripts, and test reports. The experimental\nresults show that compared to traditional manual testing skill assessment, RUM\nimproves assessment efficiency by 80.77\\% and reduces costs by 97.38\\%, while\nmaintaining high accuracy and consistency of assessment. By applying RUM on the\ncontest on software testing, we find that it not only enhances the efficiency\nand scalability of skill assessment in software testing education, but also\nprovides teachers with more comprehensive and objective evidence for student\nability assessment, facilitating personalized teaching and learning. This study\noffers new insights into the assessment of testing skills, which are expected\nto promote further development in test process optimization and software\nquality assurance."}
{"id": "2508.12035", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.12035", "abs": "https://arxiv.org/abs/2508.12035", "authors": ["Fei Lin", "Tengchao Zhang", "Ziyang Gong", "Fei-Yue Wang"], "title": "ToxiEval-ZKP: A Structure-Private Verification Framework for Molecular Toxicity Repair Tasks", "comment": null, "summary": "In recent years, generative artificial intelligence (GenAI) has demonstrated\nremarkable capabilities in high-stakes domains such as molecular science.\nHowever, challenges related to the verifiability and structural privacy of its\noutputs remain largely unresolved. This paper focuses on the task of molecular\ntoxicity repair. It proposes a structure-private verification framework -\nToxiEval-ZKP - which, for the first time, introduces zero-knowledge proof (ZKP)\nmechanisms into the evaluation process of this task. The system enables model\ndevelopers to demonstrate to external verifiers that the generated molecules\nmeet multidimensional toxicity repair criteria, without revealing the molecular\nstructures themselves. To this end, we design a general-purpose circuit\ncompatible with both classification and regression tasks, incorporating\nevaluation logic, Poseidon-based commitment hashing, and a nullifier-based\nreplay prevention mechanism to build a complete end-to-end ZK verification\nsystem. Experimental results demonstrate that ToxiEval-ZKP facilitates adequate\nvalidation under complete structural invisibility, offering strong circuit\nefficiency, security, and adaptability, thereby opening up a novel paradigm for\ntrustworthy evaluation in generative scientific tasks."}
{"id": "2508.12022", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12022", "abs": "https://arxiv.org/abs/2508.12022", "authors": ["Dorsa Macky Aleagha", "Payam Zohari", "Mostafa Haghir Chehreghani"], "title": "AI Models for Depressive Disorder Detection and Diagnosis: A Review", "comment": null, "summary": "Major Depressive Disorder is one of the leading causes of disability\nworldwide, yet its diagnosis still depends largely on subjective clinical\nassessments. Integrating Artificial Intelligence (AI) holds promise for\ndeveloping objective, scalable, and timely diagnostic tools. In this paper, we\npresent a comprehensive survey of state-of-the-art AI methods for depression\ndetection and diagnosis, based on a systematic review of 55 key studies. We\nintroduce a novel hierarchical taxonomy that structures the field by primary\nclinical task (diagnosis vs. prediction), data modality (text, speech,\nneuroimaging, multimodal), and computational model class (e.g., graph neural\nnetworks, large language models, hybrid approaches). Our in-depth analysis\nreveals three major trends: the predominance of graph neural networks for\nmodeling brain connectivity, the rise of large language models for linguistic\nand conversational data, and an emerging focus on multimodal fusion,\nexplainability, and algorithmic fairness. Alongside methodological insights, we\nprovide an overview of prominent public datasets and standard evaluation\nmetrics as a practical guide for researchers. By synthesizing current advances\nand highlighting open challenges, this survey offers a comprehensive roadmap\nfor future innovation in computational psychiatry."}
{"id": "2508.13051", "categories": ["cs.SE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.13051", "abs": "https://arxiv.org/abs/2508.13051", "authors": ["Yi Wang", "Chetan Arora", "Xiao Liu", "Thuong Hoang", "ZHengxin Zhang", "Henry Been Lirn Duh", "John Grundy"], "title": "Investigating VR Accessibility Reviews for Users with Disabilities: A Qualitative Analysis", "comment": null, "summary": "Accessibility reviews provide valuable insights into both the limitations and\nbenefits experienced by users with disabilities when using virtual reality (VR)\napplications. However, a comprehensive investigation into VR accessibility for\nusers with disabilities is still lacking. To fill this gap, this study analyzes\nuser reviews from the Meta and Steam stores of VR apps, focusing on the\nreported issues affecting users with disabilities. We applied selection\ncriteria to 1,367,419 reviews from the top 40, the 20 most popular, and the 40\nlowest-rated VR applications on both platforms. In total, 1,076 (0.078%) VR\naccessibility reviews referenced various disabilities across 100 VR\napplications. These applications were categorized into Action, Sports, Social,\nPuzzle, Horror, and Simulation, with Action receiving the highest number of\naccessibility related-reviews. We identified 16 different types of disabilities\nacross six categories. Furthermore, we examined the causes of accessibility\nissues as reported by users with disabilities. Overall, VR accessibility\nreviews were predominantly under-supported."}
{"id": "2508.12072", "categories": ["cs.CR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.12072", "abs": "https://arxiv.org/abs/2508.12072", "authors": ["Wei Jie Yeo", "Ranjan Satapathy", "Erik Cambria"], "title": "Mitigating Jailbreaks with Intent-Aware LLMs", "comment": null, "summary": "Despite extensive safety-tuning, large language models (LLMs) remain\nvulnerable to jailbreak attacks via adversarially crafted instructions,\nreflecting a persistent trade-off between safety and task performance. In this\nwork, we propose Intent-FT, a simple and lightweight fine-tuning approach that\nexplicitly trains LLMs to infer the underlying intent of an instruction before\nresponding. By fine-tuning on a targeted set of adversarial instructions,\nIntent-FT enables LLMs to generalize intent deduction to unseen attacks,\nthereby substantially improving their robustness. We comprehensively evaluate\nboth parametric and non-parametric attacks across open-source and proprietary\nmodels, considering harmfulness from attacks, utility, over-refusal, and impact\nagainst white-box threats. Empirically, Intent-FT consistently mitigates all\nevaluated attack categories, with no single attack exceeding a 50\\% success\nrate -- whereas existing defenses remain only partially effective. Importantly,\nour method preserves the model's general capabilities and reduces excessive\nrefusals on benign instructions containing superficially harmful keywords.\nFurthermore, models trained with Intent-FT accurately identify hidden harmful\nintent in adversarial attacks, and these learned intentions can be effectively\ntransferred to enhance vanilla model defenses."}
{"id": "2508.12026", "categories": ["cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.12026", "abs": "https://arxiv.org/abs/2508.12026", "authors": ["Szymon Pawlonka", "Mikołaj Małkiński", "Jacek Mańdziuk"], "title": "Bongard-RWR+: Real-World Representations of Fine-Grained Concepts in Bongard Problems", "comment": null, "summary": "Bongard Problems (BPs) provide a challenging testbed for abstract visual\nreasoning (AVR), requiring models to identify visual concepts fromjust a few\nexamples and describe them in natural language. Early BP benchmarks featured\nsynthetic black-and-white drawings, which might not fully capture the\ncomplexity of real-world scenes. Subsequent BP datasets employed real-world\nimages, albeit the represented concepts are identifiable from high-level image\nfeatures, reducing the task complexity. Differently, the recently released\nBongard-RWR dataset aimed at representing abstract concepts formulated in the\noriginal BPs using fine-grained real-world images. Its manual construction,\nhowever, limited the dataset size to just $60$ instances, constraining\nevaluation robustness. In this work, we introduce Bongard-RWR+, a BP dataset\ncomposed of $5\\,400$ instances that represent original BP abstract concepts\nusing real-world-like images generated via a vision language model (VLM)\npipeline. Building on Bongard-RWR, we employ Pixtral-12B to describe manually\ncurated images and generate new descriptions aligned with the underlying\nconcepts, use Flux.1-dev to synthesize images from these descriptions, and\nmanually verify that the generated images faithfully reflect the intended\nconcepts. We evaluate state-of-the-art VLMs across diverse BP formulations,\nincluding binary and multiclass classification, as well as textual answer\ngeneration. Our findings reveal that while VLMs can recognize coarse-grained\nvisual concepts, they consistently struggle with discerning fine-grained\nconcepts, highlighting limitations in their reasoning capabilities."}
{"id": "2508.13134", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.13134", "abs": "https://arxiv.org/abs/2508.13134", "authors": ["Glauber da Rocha Balthazar", "Marcia Ito"], "title": "Influencia de fatores organizacionais e sociais na etapa de levantamento de requisitos", "comment": "VI Workshop de P\\'os-Gradua\\c{c}\\~ao e Pesquisa do Centro Paula\n  Souza, in Portuguese language", "summary": "The most critical and fragile stage of a software development project is\nrequirements gathering. Because of this, Requirements Engineering has been\nevolving its techniques to minimize the challenges faced by Requirements\nAnalysts. However, few studies consider the humanistic relationships and\nbehaviors of those involved in this stage. This article presents a survey of\nsome studies conducted at this stage that consider non-technical factors such\nas emotions, organizational environment, and social context."}
{"id": "2508.12093", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.12093", "abs": "https://arxiv.org/abs/2508.12093", "authors": ["Hyunmin Choi"], "title": "PP-STAT: An Efficient Privacy-Preserving Statistical Analysis Framework using Homomorphic Encryption", "comment": "Accepted to CIKM 2025 (Full Research Paper Track)", "summary": "With the widespread adoption of cloud computing, the need for outsourcing\nstatistical analysis to third-party platforms is growing rapidly. However,\nhandling sensitive data such as medical records and financial information in\ncloud environments raises serious privacy concerns. In this paper, we present\nPP-STAT, a novel and efficient Homomorphic Encryption (HE)-based framework for\nprivacy-preserving statistical analysis. HE enables computations to be\nperformed directly on encrypted data without revealing the underlying\nplaintext. PP-STAT supports advanced statistical measures, including Z-score\nnormalization, skewness, kurtosis, coefficient of variation, and Pearson\ncorrelation coefficient, all computed securely over encrypted data. To improve\nefficiency, PP-STAT introduces two key optimizations: (1) a Chebyshev-based\napproximation strategy for initializing inverse square root operations, and (2)\na pre-normalization scaling technique that reduces multiplicative depth by\nfolding constant scaling factors into mean and variance computations. These\ntechniques significantly lower computational overhead and minimize the number\nof expensive bootstrapping procedures. Our evaluation on real-world datasets\ndemonstrates that PP-STAT achieves high numerical accuracy, with mean relative\nerror (MRE) below 2.4x10-4. Notably, the encrypted Pearson correlation between\nthe smoker attribute and charges reaches 0.7873, with an MRE of 2.86x10-4.\nThese results confirm the practical utility of PP-STAT for secure and precise\nstatistical analysis in privacy-sensitive domains."}
{"id": "2508.12027", "categories": ["cs.AI", "cs.LG", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2508.12027", "abs": "https://arxiv.org/abs/2508.12027", "authors": ["Filippo Torresan", "Keisuke Suzuki", "Ryota Kanai", "Manuel Baltieri"], "title": "Active inference for action-unaware agents", "comment": "59 pages, 47 figures", "summary": "Active inference is a formal approach to study cognition based on the notion\nthat adaptive agents can be seen as engaging in a process of approximate\nBayesian inference, via the minimisation of variational and expected free\nenergies. Minimising the former provides an account of perceptual processes and\nlearning as evidence accumulation, while minimising the latter describes how\nagents select their actions over time. In this way, adaptive agents are able to\nmaximise the likelihood of preferred observations or states, given a generative\nmodel of the environment. In the literature, however, different strategies have\nbeen proposed to describe how agents can plan their future actions. While they\nall share the notion that some kind of expected free energy offers an\nappropriate way to score policies, sequences of actions, in terms of their\ndesirability, there are different ways to consider the contribution of past\nmotor experience to the agent's future behaviour. In some approaches, agents\nare assumed to know their own actions, and use such knowledge to better plan\nfor the future. In other approaches, agents are unaware of their actions, and\nmust infer their motor behaviour from recent observations in order to plan for\nthe future. This difference reflects a standard point of departure in two\nleading frameworks in motor control based on the presence, or not, of an\nefference copy signal representing knowledge about an agent's own actions. In\nthis work we compare the performances of action-aware and action-unaware agents\nin two navigations tasks, showing how action-unaware agents can achieve\nperformances comparable to action-aware ones while at a severe disadvantage."}
{"id": "2508.12187", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.12187", "abs": "https://arxiv.org/abs/2508.12187", "authors": ["John Y. Kim", "Chaoshun Zuo", "Yanjie Zhao", "Zhiqiang Lin"], "title": "AUTOVR: Automated UI Exploration for Detecting Sensitive Data Flow Exposures in Virtual Reality Apps", "comment": "USENIX Security 2025, 19 Pages, 14 Figures, 7 Tables", "summary": "The rise of Virtual Reality (VR) has provided developers with an\nunprecedented platform for creating games and applications (apps) that require\ndistinct inputs, different from those of conventional devices like smartphones.\nThe Meta Quest VR platform, driven by Meta, has democratized VR app publishing\nand attracted millions of users worldwide. However, as the number of published\napps grows, there is a notable lack of robust headless tools for user interface\n(UI) exploration and user event testing. To address this need, we present\nAUTOVR, an automatic framework for dynamic UI and user event interaction in VR\napps built on the Unity Engine. Unlike conventional Android and GUI testers,\nAUTOVR analyzes the app's internal binary to reveal hidden events, resolves\ngenerative event dependencies, and utilizes them for comprehensive exploration\nof VR apps. Using sensitive data exposure as a performance metric, we compare\nAUTOVR with Android Monkey, a widely used headless Android GUI stress testing\ntool. Our empirical evaluation demonstrates AUTOVR's superior performance,\ntriggering an order of magnitude of more sensitive data exposures and\nsignificantly enhancing the privacy of VR apps."}
{"id": "2508.12107", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.12107", "abs": "https://arxiv.org/abs/2508.12107", "authors": ["Shixuan Guan", "Kai Li"], "title": "Ethereum Crypto Wallets under Address Poisoning: How Usable and Secure Are They?", "comment": "15 pages, 10 figures", "summary": "Blockchain address poisoning is an emerging phishing attack that crafts\n\"similar-looking\" transfer records in the victim's transaction history, which\naims to deceive victims and lure them into mistakenly transferring funds to the\nattacker. Recent works have shown that millions of Ethereum users were targeted\nand lost over 100 million US dollars.\n  Ethereum crypto wallets, serving users in browsing transaction history and\ninitiating transactions to transfer funds, play a central role in deploying\ncountermeasures to mitigate the address poisoning attack. However, whether they\nhave done so remains an open question. To fill the research void, in this\npaper, we design experiments to simulate address poisoning attacks and\nsystematically evaluate the usability and security of 53 popular Ethereum\ncrypto wallets. Our evaluation shows that there exist communication failures\nbetween 12 wallets and their transaction activity provider, which renders them\nunable to download the users' transaction history. Besides, our evaluation also\nshows that 16 wallets pose a high risk to their users due to displaying fake\ntoken phishing transfers. Moreover, our further analysis suggests that most\nwallets rely on transaction activity providers to filter out phishing\ntransfers. However, their phishing detection capability varies. Finally, we\nfound that only three wallets throw an explicit warning message when users\nattempt to transfer to the phishing address, implying a significant gap within\nthe broader Ethereum crypto wallet community in protecting users from address\npoisoning attacks.\n  Overall, our work shows that more efforts are needed by the Ethereum crypto\nwallet developer community to achieve the highest usability and security\nstandard. Our bug reports have been acknowledged by the developer community,\nwho are currently developing mitigation solutions."}
{"id": "2508.12087", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.12087", "abs": "https://arxiv.org/abs/2508.12087", "authors": ["Zhanjiang Yang", "Meng Li", "Yang Shen", "Yueming Li", "Lijun Sun"], "title": "MAPF-World: Action World Model for Multi-Agent Path Finding", "comment": null, "summary": "Multi-agent path finding (MAPF) is the problem of planning conflict-free\npaths from the designated start locations to goal positions for multiple\nagents. It underlies a variety of real-world tasks, including multi-robot\ncoordination, robot-assisted logistics, and social navigation. Recent\ndecentralized learnable solvers have shown great promise for large-scale MAPF,\nespecially when leveraging foundation models and large datasets. However, these\nagents are reactive policy models and exhibit limited modeling of environmental\ntemporal dynamics and inter-agent dependencies, resulting in performance\ndegradation in complex, long-term planning scenarios. To address these\nlimitations, we propose MAPF-World, an autoregressive action world model for\nMAPF that unifies situation understanding and action generation, guiding\ndecisions beyond immediate local observations. It improves situational\nawareness by explicitly modeling environmental dynamics, including spatial\nfeatures and temporal dependencies, through future state and actions\nprediction. By incorporating these predicted futures, MAPF-World enables more\ninformed, coordinated, and far-sighted decision-making, especially in complex\nmulti-agent settings. Furthermore, we augment MAPF benchmarks by introducing an\nautomatic map generator grounded in real-world scenarios, capturing practical\nmap layouts for training and evaluating MAPF solvers. Extensive experiments\ndemonstrate that MAPF-World outperforms state-of-the-art learnable solvers,\nshowcasing superior zero-shot generalization to out-of-distribution cases.\nNotably, MAPF-World is trained with a 96.5% smaller model size and 92% reduced\ndata."}
{"id": "2508.12538", "categories": ["cs.CR", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.12538", "abs": "https://arxiv.org/abs/2508.12538", "authors": ["Yongjian Guo", "Puzhuo Liu", "Wanlun Ma", "Zehang Deng", "Xiaogang Zhu", "Peng Di", "Xi Xiao", "Sheng Wen"], "title": "Systematic Analysis of MCP Security", "comment": null, "summary": "The Model Context Protocol (MCP) has emerged as a universal standard that\nenables AI agents to seamlessly connect with external tools, significantly\nenhancing their functionality. However, while MCP brings notable benefits, it\nalso introduces significant vulnerabilities, such as Tool Poisoning Attacks\n(TPA), where hidden malicious instructions exploit the sycophancy of large\nlanguage models (LLMs) to manipulate agent behavior. Despite these risks,\ncurrent academic research on MCP security remains limited, with most studies\nfocusing on narrow or qualitative analyses that fail to capture the diversity\nof real-world threats. To address this gap, we present the MCP Attack Library\n(MCPLIB), which categorizes and implements 31 distinct attack methods under\nfour key classifications: direct tool injection, indirect tool injection,\nmalicious user attacks, and LLM inherent attack. We further conduct a\nquantitative analysis of the efficacy of each attack. Our experiments reveal\nkey insights into MCP vulnerabilities, including agents' blind reliance on tool\ndescriptions, sensitivity to file-based attacks, chain attacks exploiting\nshared context, and difficulty distinguishing external data from executable\ncommands. These insights, validated through attack experiments, underscore the\nurgency for robust defense strategies and informed MCP design. Our\ncontributions include 1) constructing a comprehensive MCP attack taxonomy, 2)\nintroducing a unified attack framework MCPLIB, and 3) conducting empirical\nvulnerability analysis to enhance MCP security mechanisms. This work provides a\nfoundational framework, supporting the secure evolution of MCP ecosystems."}
{"id": "2508.12138", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12138", "abs": "https://arxiv.org/abs/2508.12138", "authors": ["Mohammad Ishzaz Asif Rafid", "Morsalin Sakib"], "title": "Substituting Proof of Work in Blockchain with Training-Verified Collaborative Model Computation", "comment": null, "summary": "Bitcoin's Proof of Work (PoW) mechanism, while central to achieving\ndecentralized consensus, has long been criticized for excessive energy use and\nhardware inefficiencies \\cite{devries2018bitcoin, truby2018decarbonizing}. This\npaper introduces a hybrid architecture that replaces Bitcoin's traditional PoW\nwith a centralized, cloud-based collaborative training framework. In this\nmodel, miners contribute computing resources to train segments of horizontally\nscaled machine learning models on preprocessed datasets, ensuring privacy and\ngenerating meaningful outputs \\cite{li2017securing}. A central server evaluates\ncontributions using two metrics: number of parameters trained and reduction in\nmodel loss during each cycle. At the end of every cycle, a weighted lottery\nselects the winning miner, who receives a digitally signed certificate. This\ncertificate serves as a verifiable substitute for PoW and grants the right to\nappend a block to the blockchain \\cite{nakamoto2008bitcoin}. By integrating\ndigital signatures and SHA-256 hashing \\cite{nist2015sha}, the system preserves\nblockchain integrity while redirecting energy toward productive computation.\nThe proposed approach addresses the sustainability concerns of traditional\nmining by converting resource expenditure into socially valuable work, aligning\nsecurity incentives with real-world computational progress."}
{"id": "2508.12100", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12100", "abs": "https://arxiv.org/abs/2508.12100", "authors": ["Daniel Burkhardt", "Xiangwei Cheng"], "title": "Overcoming Knowledge Discrepancies: Structuring Reasoning Threads through Knowledge Balancing in Interactive Scenarios", "comment": "13 pages, 1 figure, 6 tables", "summary": "Reasoning in interactive problem solving scenarios requires models to\nconstruct reasoning threads that reflect user understanding and align with\nstructured domain knowledge. However, current reasoning models often lack\nexplicit semantic hierarchies, user-domain knowledge alignment, and principled\nmechanisms to prune reasoning threads for effectiveness. These limitations\nresult in lengthy generic output that does not guide users through\ngoal-oriented reasoning steps. To address this, we propose a\nprototype-inspired, two-phases Reasoning-Threads-Evaluation (ReT-Eval)\nframework, drawing inspiration from human-like reasoning strategies that\nemphasize structured knowledge reuse. In the first phase, semantically relevant\nknowledge structures are extracted from a sparse domain knowledge graph using a\ngraph neural network and enriched with intrinsic large language model knowledge\nto resolve knowledge discrepancies. In the second phase, these threads are\nevaluated and pruned using a reward-guided strategy aimed at maintaining\nsemantic coherence to generate effective reasoning threads. Experiments and\nexpert evaluations show that ReT-Eval enhances user understanding and\noutperforms state-of-the-art reasoning models."}
{"id": "2508.13143", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.13143", "abs": "https://arxiv.org/abs/2508.13143", "authors": ["Ruofan Lu", "Yichen Li", "Yintong Huo"], "title": "Exploring Autonomous Agents: A Closer Look at Why They Fail When Completing Tasks", "comment": "Accepted by ASE 2025 NIER", "summary": "Autonomous agent systems powered by Large Language Models (LLMs) have\ndemonstrated promising capabilities in automating complex tasks. However,\ncurrent evaluations largely rely on success rates without systematically\nanalyzing the interactions, communication mechanisms, and failure causes within\nthese systems. To bridge this gap, we present a benchmark of 34 representative\nprogrammable tasks designed to rigorously assess autonomous agents. Using this\nbenchmark, we evaluate three popular open-source agent frameworks combined with\ntwo LLM backbones, observing a task completion rate of approximately 50%.\nThrough in-depth failure analysis, we develop a three-tier taxonomy of failure\ncauses aligned with task phases, highlighting planning errors, task execution\nissues, and incorrect response generation. Based on these insights, we propose\nactionable improvements to enhance agent planning and self-diagnosis\ncapabilities. Our failure taxonomy, together with mitigation advice, provides\nan empirical foundation for developing more robust and effective autonomous\nagent systems in the future."}
{"id": "2508.12161", "categories": ["cs.CR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2508.12161", "abs": "https://arxiv.org/abs/2508.12161", "authors": ["Ming Li", "John Hale"], "title": "Attack Graph Generation on HPC Clusters", "comment": null, "summary": "Attack graphs (AGs) are graphical tools to analyze the security of computer\nnetworks. By connecting the exploitation of individual vulnerabilities, AGs\nexpose possible multi-step attacks against target networks, allowing system\nadministrators to take preventive measures to enhance their network's security.\nAs powerful analytical tools, however, AGs are both time- and memory-consuming\nto be generated. As the numbers of network assets, interconnections between\ndevices, as well as vulnerabilities increase, the size and volume of the\nresulting AGs grow at a much higher rate, leading to the well-known state-space\nexplosion. In this paper, we propose the use of high performance computing\n(HPC) clusters to implement AG generators. We evaluate the performance through\nexperiments and provide insights into how cluster environments can help resolve\nthe issues of slow speed and high memory demands in AG generation in a balanced\nway."}
{"id": "2508.12149", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12149", "abs": "https://arxiv.org/abs/2508.12149", "authors": ["Haochen You", "Baojing Liu"], "title": "MOVER: Multimodal Optimal Transport with Volume-based Embedding Regularization", "comment": "Accepted as a conference paper at CIKM 2025", "summary": "Recent advances in multimodal learning have largely relied on pairwise\ncontrastive objectives to align different modalities, such as text, video, and\naudio, in a shared embedding space. While effective in bi-modal setups, these\napproaches struggle to generalize across multiple modalities and often lack\nsemantic structure in high-dimensional spaces. In this paper, we propose MOVER,\na novel framework that combines optimal transport-based soft alignment with\nvolume-based geometric regularization to build semantically aligned and\nstructured multimodal representations. By integrating a transport-guided\nmatching mechanism with a geometric volume minimization objective (GAVE), MOVER\nencourages consistent alignment across all modalities in a modality-agnostic\nmanner. Experiments on text-video-audio retrieval tasks demonstrate that MOVER\nsignificantly outperforms prior state-of-the-art methods in both zero-shot and\nfinetuned settings. Additional analysis shows improved generalization to unseen\nmodality combinations and stronger structural consistency in the learned\nembedding space."}
{"id": "2508.12175", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.12175", "abs": "https://arxiv.org/abs/2508.12175", "authors": ["Ben Nassi", "Stav Cohen", "Or Yair"], "title": "Invitation Is All You Need! Promptware Attacks Against LLM-Powered Assistants in Production Are Practical and Dangerous", "comment": "https://sites.google.com/view/invitation-is-all-you-need/home", "summary": "The growing integration of LLMs into applications has introduced new security\nrisks, notably known as Promptware - maliciously engineered prompts designed to\nmanipulate LLMs to compromise the CIA triad of these applications. While prior\nresearch warned about a potential shift in the threat landscape for LLM-powered\napplications, the risk posed by Promptware is frequently perceived as low. In\nthis paper, we investigate the risk Promptware poses to users of Gemini-powered\nassistants (web application, mobile application, and Google Assistant). We\npropose a novel Threat Analysis and Risk Assessment (TARA) framework to assess\nPromptware risks for end users. Our analysis focuses on a new variant of\nPromptware called Targeted Promptware Attacks, which leverage indirect prompt\ninjection via common user interactions such as emails, calendar invitations,\nand shared documents. We demonstrate 14 attack scenarios applied against\nGemini-powered assistants across five identified threat classes: Short-term\nContext Poisoning, Permanent Memory Poisoning, Tool Misuse, Automatic Agent\nInvocation, and Automatic App Invocation. These attacks highlight both digital\nand physical consequences, including spamming, phishing, disinformation\ncampaigns, data exfiltration, unapproved user video streaming, and control of\nhome automation devices. We reveal Promptware's potential for on-device lateral\nmovement, escaping the boundaries of the LLM-powered application, to trigger\nmalicious actions using a device's applications. Our TARA reveals that 73% of\nthe analyzed threats pose High-Critical risk to end users. We discuss\nmitigations and reassess the risk (in response to deployed mitigations) and\nshow that the risk could be reduced significantly to Very Low-Medium. We\ndisclosed our findings to Google, which deployed dedicated mitigations."}
{"id": "2508.12165", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12165", "abs": "https://arxiv.org/abs/2508.12165", "authors": ["Rohit Krishnan", "Jon Evans"], "title": "RLNVR: Reinforcement Learning from Non-Verified Real-World Rewards", "comment": null, "summary": "This paper introduces RLNVR (Reinforcement Learning from Non-Verified\nRewards), a framework for training language models using noisy, real-world\nfeedback signals without requiring explicit human verification. Traditional\nRLHF requires expensive, verified reward signals that are impractical in many\nreal-world domains. RLNVR addresses this challenge through baseline\nnormalization and semantic similarity-based reward transfer. We demonstrate\nRLNVR through Walter, a prototype system that optimizes social media content\ngeneration using actual engagement data from Bluesky. Our experimental results\nshow significant improvements in content quality and training stability, with\ncomprehensive evaluation planned for future work. Positioning: We present a\npractical framework that combines RLNVR with GSPO (Group Sequence Policy\nOptimization) and an optional UED (Unsupervised Environment Design) curriculum\nto improve stability and diversity under noisy, implicit rewards. To our\nknowledge, combining GSPO-style normalization with a UED-style curriculum for\nLLM content generation from implicit social engagement has not been previously\ndocumented in this applied setting; we frame this as an applied integration\nrather than a new algorithm."}
{"id": "2508.12181", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.12181", "abs": "https://arxiv.org/abs/2508.12181", "authors": ["Ayman W. Baharia", "Khaled T. Naga", "Hesham S. Abdelfattah", "Shady A. Maged", "Sherif A. Hammad"], "title": "CAN Networks Security in Smart Grids Communication Technologies", "comment": "4 pages, 6 figures, International Conference on Energy Systems -\n  Smart and Sustainable Solutions -", "summary": "The rapid evolution of smart grids requires effective communication protocols\nto transfer data reliably and securely. Controller Area Network (CAN) is one of\nthe most recognized protocols that offer reliable data transmission in smart\ngrids due to its robustness, real-time capabilities, and relatively low initial\ncost of its required hardware. However, as a smart city becomes more\ninterconnected, it also becomes more vulnerable to cyber-attacks. As there are\nmany mechanisms to secure the CAN nodes from attacks, most of those mechanisms\nhave computational overhead, resulting in more delay in the network. We\nimplemented a solution that requires almost no overhead to any CAN node\nconnected to the network. It depends on a single node responsible for securing\nthe CAN network. This approach seeks to augment network security while reducing\nsecurity mechanisms overhead to all CAN network nodes. The methodology and\ncomprehensive test results will be presented in detail during a subsequent\ndiscussion. The used software for development is Code Composer Studio, and the\nused microcontroller evaluation boards (EVB) are TM4C 1294."}
{"id": "2508.12260", "categories": ["cs.AI", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2508.12260", "abs": "https://arxiv.org/abs/2508.12260", "authors": ["Carson Dudley", "Reiden Magdaleno", "Christopher Harding", "Ananya Sharma", "Emily Martin", "Marisa Eisenberg"], "title": "Mantis: A Simulation-Grounded Foundation Model for Disease Forecasting", "comment": "10 pages, 4 figures", "summary": "Infectious disease forecasting in novel outbreaks or low resource settings\nhas been limited by the need for disease-specific data, bespoke training, and\nexpert tuning. We introduce Mantis, a foundation model trained entirely on\nmechanistic simulations, which enables out-of-the-box forecasting across\ndiseases, regions, and outcomes, even in settings with limited historical data.\nMantis is built on over 400 million simulated days of outbreak dynamics\nspanning diverse pathogens, transmission modes, interventions, and surveillance\nartifacts. Despite requiring no real-world data during training, Mantis\noutperformed 39 expert-tuned models we tested across six diseases, including\nall models in the CDC's COVID-19 Forecast Hub. Mantis generalized to novel\nepidemiological regimes, including diseases with held-out transmission\nmechanisms, demonstrating that it captures fundamental contagion dynamics.\nCritically, Mantis is mechanistically interpretable, enabling public health\ndecision-makers to identify the latent drivers behind its predictions. Finally,\nMantis delivers accurate forecasts at 8-week horizons, more than doubling the\nactionable range of most models, enabling proactive public health planning.\nTogether, these capabilities position Mantis as a foundation for\nnext-generation disease forecasting systems: general, interpretable, and\ndeployable where traditional models fail."}
{"id": "2508.12187", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.12187", "abs": "https://arxiv.org/abs/2508.12187", "authors": ["John Y. Kim", "Chaoshun Zuo", "Yanjie Zhao", "Zhiqiang Lin"], "title": "AUTOVR: Automated UI Exploration for Detecting Sensitive Data Flow Exposures in Virtual Reality Apps", "comment": "USENIX Security 2025, 19 Pages, 14 Figures, 7 Tables", "summary": "The rise of Virtual Reality (VR) has provided developers with an\nunprecedented platform for creating games and applications (apps) that require\ndistinct inputs, different from those of conventional devices like smartphones.\nThe Meta Quest VR platform, driven by Meta, has democratized VR app publishing\nand attracted millions of users worldwide. However, as the number of published\napps grows, there is a notable lack of robust headless tools for user interface\n(UI) exploration and user event testing. To address this need, we present\nAUTOVR, an automatic framework for dynamic UI and user event interaction in VR\napps built on the Unity Engine. Unlike conventional Android and GUI testers,\nAUTOVR analyzes the app's internal binary to reveal hidden events, resolves\ngenerative event dependencies, and utilizes them for comprehensive exploration\nof VR apps. Using sensitive data exposure as a performance metric, we compare\nAUTOVR with Android Monkey, a widely used headless Android GUI stress testing\ntool. Our empirical evaluation demonstrates AUTOVR's superior performance,\ntriggering an order of magnitude of more sensitive data exposures and\nsignificantly enhancing the privacy of VR apps."}
{"id": "2508.12291", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12291", "abs": "https://arxiv.org/abs/2508.12291", "authors": ["Xuming He", "Zhiyuan You", "Junchao Gong", "Couhua Liu", "Xiaoyu Yue", "Peiqin Zhuang", "Wenlong Zhang", "Lei Bai"], "title": "RadarQA: Multi-modal Quality Analysis of Weather Radar Forecasts", "comment": null, "summary": "Quality analysis of weather forecasts is an essential topic in meteorology.\nAlthough traditional score-based evaluation metrics can quantify certain\nforecast errors, they are still far from meteorological experts in terms of\ndescriptive capability, interpretability, and understanding of dynamic\nevolution. With the rapid development of Multi-modal Large Language Models\n(MLLMs), these models become potential tools to overcome the above challenges.\nIn this work, we introduce an MLLM-based weather forecast analysis method,\nRadarQA, integrating key physical attributes with detailed assessment reports.\nWe introduce a novel and comprehensive task paradigm for multi-modal quality\nanalysis, encompassing both single frame and sequence, under both rating and\nassessment scenarios. To support training and benchmarking, we design a hybrid\nannotation pipeline that combines human expert labeling with automated\nheuristics. With such an annotation method, we construct RQA-70K, a large-scale\ndataset with varying difficulty levels for radar forecast quality evaluation.\nWe further design a multi-stage training strategy that iteratively improves\nmodel performance at each stage. Extensive experiments show that RadarQA\noutperforms existing general MLLMs across all evaluation settings, highlighting\nits potential for advancing quality analysis in weather prediction."}
{"id": "2508.12259", "categories": ["cs.CR", "cs.AI", "cs.ET"], "pdf": "https://arxiv.org/pdf/2508.12259", "abs": "https://arxiv.org/abs/2508.12259", "authors": ["Ken Huang", "Yasir Mehmood", "Hammad Atta", "Jerry Huang", "Muhammad Zeeshan Baig", "Sree Bhargavi Balija"], "title": "Fortifying the Agentic Web: A Unified Zero-Trust Architecture Against Logic-layer Threats", "comment": null, "summary": "This paper presents a Unified Security Architecture that fortifies the\nAgentic Web through a Zero-Trust IAM framework. This architecture is built on a\nfoundation of rich, verifiable agent identities using Decentralized Identifiers\n(DIDs) and Verifiable Credentials (VCs), with discovery managed by a\nprotocol-agnostic Agent Name Service (ANS). Security is operationalized through\na multi-layered Trust Fabric which introduces significant innovations,\nincluding Trust-Adaptive Runtime Environments (TARE), Causal Chain Auditing,\nand Dynamic Identity with Behavioral Attestation. By explicitly linking the\nLPCI threat to these enhanced architectural countermeasures within a formal\nsecurity model, we propose a comprehensive and forward-looking blueprint for a\nsecure, resilient, and trustworthy agentic ecosystem. Our formal analysis\ndemonstrates that the proposed architecture provides provable security\nguarantees against LPCI attacks with bounded probability of success."}
{"id": "2508.12338", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12338", "abs": "https://arxiv.org/abs/2508.12338", "authors": ["Wenzhen Yuan", "Shengji Tang", "Weihao Lin", "Jiacheng Ruan", "Ganqu Cui", "Bo Zhang", "Tao Chen", "Ting Liu", "Yuzhuo Fu", "Peng Ye", "Lei Bai"], "title": "Wisdom of the Crowd: Reinforcement Learning from Coevolutionary Collective Feedback", "comment": null, "summary": "Reinforcement learning (RL) has significantly enhanced the reasoning\ncapabilities of large language models (LLMs), but its reliance on expensive\nhuman-labeled data or complex reward models severely limits scalability. While\nexisting self-feedback methods aim to address this problem, they are\nconstrained by the capabilities of a single model, which can lead to\noverconfidence in incorrect answers, reward hacking, and even training\ncollapse. To this end, we propose Reinforcement Learning from Coevolutionary\nCollective Feedback (RLCCF), a novel RL framework that enables multi-model\ncollaborative evolution without external supervision. Specifically, RLCCF\noptimizes the ability of a model collective by maximizing its Collective\nConsistency (CC), which jointly trains a diverse ensemble of LLMs and provides\nreward signals by voting on collective outputs. Moreover, each model's vote is\nweighted by its Self-Consistency (SC) score, ensuring that more confident\nmodels contribute more to the collective decision. Benefiting from the diverse\noutput distributions and complementary abilities of multiple LLMs, RLCCF\nenables the model collective to continuously enhance its reasoning ability\nthrough coevolution. Experiments on four mainstream open-source LLMs across\nfour mathematical reasoning benchmarks demonstrate that our framework yields\nsignificant performance gains, achieving an average relative improvement of\n16.72\\% in accuracy. Notably, RLCCF not only improves the performance of\nindividual models but also enhances the group's majority-voting accuracy by\n4.51\\%, demonstrating its ability to extend the collective capability boundary\nof the model collective."}
{"id": "2508.12264", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.12264", "abs": "https://arxiv.org/abs/2508.12264", "authors": ["Saisai Xia", "Wenhao Wang", "Zihao Wang", "Yuhui Zhang", "Yier Jin", "Dan Meng", "Rui Hou"], "title": "CryptPEFT: Efficient and Private Neural Network Inference via Parameter-Efficient Fine-Tuning", "comment": "Preprint for the paper accepted for presentation at NDSS 2026", "summary": "Publicly available large pretrained models (i.e., backbones) and lightweight\nadapters for parameter-efficient fine-tuning (PEFT) have become standard\ncomponents in modern machine learning pipelines. However, preserving the\nprivacy of both user inputs and fine-tuned adapters -- often trained on\nsensitive data -- during inference remains a significant challenge. Applying\ncryptographic techniques, such as multi-party computation (MPC), to PEFT\nsettings still incurs substantial encrypted computation across both the\nbackbone and adapter, mainly due to the inherent two-way communication between\nthem. To address this limitation, we propose CryptPEFT, the first PEFT solution\nspecifically designed for private inference scenarios. CryptPEFT introduces a\nnovel one-way communication (OWC) architecture that confines encrypted\ncomputation solely to the adapter, significantly reducing both computational\nand communication overhead. To maintain strong model utility under this\nconstraint, we explore the design space of OWC-compatible adapters and employ\nan automated architecture search algorithm to optimize the trade-off between\nprivate inference efficiency and model utility. We evaluated CryptPEFT using\nVision Transformer backbones across widely used image classification datasets.\nOur results show that CryptPEFT significantly outperforms existing baselines,\ndelivering speedups ranging from $20.62\\times$ to $291.48\\times$ in simulated\nwide-area network (WAN) and local-area network (LAN) settings. On CIFAR-100,\nCryptPEFT attains 85.47% accuracy with just 2.26 seconds of inference latency.\nThese findings demonstrate that CryptPEFT offers an efficient and\nprivacy-preserving solution for modern PEFT-based inference."}
{"id": "2508.12375", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12375", "abs": "https://arxiv.org/abs/2508.12375", "authors": ["Yu Sha", "Shuiping Gou", "Bo Liu", "Johannes Faber", "Ningtao Liu", "Stefan Schramm", "Horst Stoecker", "Thomas Steckenreiter", "Domagoj Vnucec", "Nadine Wetzstein", "Andreas Widl", "Kai Zhou"], "title": "Hierarchical knowledge guided fault intensity diagnosis of complex industrial systems", "comment": "12 pages", "summary": "Fault intensity diagnosis (FID) plays a pivotal role in monitoring and\nmaintaining mechanical devices within complex industrial systems. As current\nFID methods are based on chain of thought without considering dependencies\namong target classes. To capture and explore dependencies, we propose a\nhierarchical knowledge guided fault intensity diagnosis framework (HKG)\ninspired by the tree of thought, which is amenable to any representation\nlearning methods. The HKG uses graph convolutional networks to map the\nhierarchical topological graph of class representations into a set of\ninterdependent global hierarchical classifiers, where each node is denoted by\nword embeddings of a class. These global hierarchical classifiers are applied\nto learned deep features extracted by representation learning, allowing the\nentire model to be end-to-end learnable. In addition, we develop a re-weighted\nhierarchical knowledge correlation matrix (Re-HKCM) scheme by embedding\ninter-class hierarchical knowledge into a data-driven statistical correlation\nmatrix (SCM) which effectively guides the information sharing of nodes in\ngraphical convolutional neural networks and avoids over-smoothing issues. The\nRe-HKCM is derived from the SCM through a series of mathematical\ntransformations. Extensive experiments are performed on four real-world\ndatasets from different industrial domains (three cavitation datasets from\nSAMSON AG and one existing publicly) for FID, all showing superior results and\noutperform recent state-of-the-art FID methods."}
{"id": "2508.12304", "categories": ["cs.CR", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.12304", "abs": "https://arxiv.org/abs/2508.12304", "authors": ["Hao Li"], "title": "Adjustable AprilTags For Identity Secured Tasks", "comment": null, "summary": "Special tags such as AprilTags that facilitate image processing and pattern\nrecognition are useful in practical applications. In close and private\nenvironments, identity security is unlikely to be an issue because all involved\nAprilTags can be completely regulated. However, in open and public\nenvironments, identity security is no longer an issue that can be neglected. To\nhandle potential harm caused by adversarial attacks, this note advocates\nutilization of adjustable AprilTags instead of fixed ones."}
{"id": "2508.12379", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12379", "abs": "https://arxiv.org/abs/2508.12379", "authors": ["Rongzheng Wang", "Qizhi Chen", "Yihong Huang", "Yizhuo Ma", "Muquan Li", "Jiakai Li", "Ke Qin", "Guangchun Luo", "Shuang Liang"], "title": "GraphCogent: Overcoming LLMs' Working Memory Constraints via Multi-Agent Collaboration in Complex Graph Understanding", "comment": null, "summary": "Large language models (LLMs) show promising performance on small-scale graph\nreasoning tasks but fail when handling real-world graphs with complex queries.\nThis phenomenon stems from LLMs' inability to effectively process complex graph\ntopology and perform multi-step reasoning simultaneously. To address these\nlimitations, we propose GraphCogent, a collaborative agent framework inspired\nby human Working Memory Model that decomposes graph reasoning into specialized\ncognitive processes: sense, buffer, and execute. The framework consists of\nthree modules: Sensory Module standardizes diverse graph text representations\nvia subgraph sampling, Buffer Module integrates and indexes graph data across\nmultiple formats, and Execution Module combines tool calling and model\ngeneration for efficient reasoning. We also introduce Graph4real, a\ncomprehensive benchmark contains with four domains of real-world graphs (Web,\nSocial, Transportation, and Citation) to evaluate LLMs' graph reasoning\ncapabilities. Our Graph4real covers 21 different graph reasoning tasks,\ncategorized into three types (Structural Querying, Algorithmic Reasoning, and\nPredictive Modeling tasks), with graph scales that are 10 times larger than\nexisting benchmarks. Experiments show that Llama3.1-8B based GraphCogent\nachieves a 50% improvement over massive-scale LLMs like DeepSeek-R1 (671B).\nCompared to state-of-the-art agent-based baseline, our framework outperforms by\n20% in accuracy while reducing token usage by 80% for in-toolset tasks and 30%\nfor out-toolset tasks. Code will be available after review."}
{"id": "2508.12398", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.12398", "abs": "https://arxiv.org/abs/2508.12398", "authors": ["Zhixin Xie", "Xurui Song", "Jun Luo"], "title": "Where to Start Alignment? Diffusion Large Language Model May Demand a Distinct Position", "comment": null, "summary": "Diffusion Large Language Models (dLLMs) have recently emerged as a\ncompetitive non-autoregressive paradigm due to their unique training and\ninference approach. However, there is currently a lack of safety study on this\nnovel architecture. In this paper, we present the first analysis of dLLMs'\nsafety performance and propose a novel safety alignment method tailored to\ntheir unique generation characteristics. Specifically, we identify a critical\nasymmetry between the defender and attacker in terms of security. For the\ndefender, we reveal that the middle tokens of the response, rather than the\ninitial ones, are more critical to the overall safety of dLLM outputs; this\nseems to suggest that aligning middle tokens can be more beneficial to the\ndefender. The attacker, on the contrary, may have limited power to manipulate\nmiddle tokens, as we find dLLMs have a strong tendency towards a sequential\ngeneration order in practice, forcing the attack to meet this distribution and\ndiverting it from influencing the critical middle tokens. Building on this\nasymmetry, we introduce Middle-tOken Safety Alignment (MOSA), a novel method\nthat directly aligns the model's middle generation with safe refusals\nexploiting reinforcement learning. We implement MOSA and compare its security\nperformance against eight attack methods on two benchmarks. We also test the\nutility of MOSA-aligned dLLM on coding, math, and general reasoning. The\nresults strongly prove the superiority of MOSA."}
{"id": "2508.12425", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.12425", "abs": "https://arxiv.org/abs/2508.12425", "authors": ["Phuong Minh Nguyen", "Tien Huu Dang", "Naoya Inoue"], "title": "Non-Iterative Symbolic-Aided Chain-of-Thought for Logical Reasoning", "comment": null, "summary": "This work introduces Symbolic-Aided Chain-of-Thought (CoT), an improved\napproach to standard CoT, for logical reasoning in large language models\n(LLMs). The key idea is to integrate lightweight symbolic representations into\nfew-shot prompts, structuring the inference steps with a consistent strategy to\nmake reasoning patterns more explicit within a non-iterative reasoning process.\nBy incorporating these symbolic structures, our method preserves the\ngeneralizability of standard prompting techniques while enhancing the\ntransparency, interpretability, and analyzability of LLM logical reasoning.\nExtensive experiments on four well-known logical reasoning benchmarks --\nProofWriter, FOLIO, ProntoQA, and LogicalDeduction, which cover diverse\nreasoning scenarios -- demonstrate the effectiveness of the proposed approach,\nparticularly in complex reasoning tasks that require navigating multiple\nconstraints or rules. Notably, Symbolic-Aided CoT consistently improves LLMs'\nreasoning capabilities across various model sizes and significantly outperforms\nconventional CoT on three out of four datasets, ProofWriter, ProntoQA, and\nLogicalDeduction."}
{"id": "2508.12412", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12412", "abs": "https://arxiv.org/abs/2508.12412", "authors": ["Ron Solomon", "Yarin Yerushalmi Levi", "Lior Vaknin", "Eran Aizikovich", "Amit Baras", "Etai Ohana", "Amit Giloni", "Shamik Bose", "Chiara Picardi", "Yuval Elovici", "Asaf Shabtai"], "title": "LumiMAS: A Comprehensive Framework for Real-Time Monitoring and Enhanced Observability in Multi-Agent Systems", "comment": null, "summary": "The incorporation of large language models in multi-agent systems (MASs) has\nthe potential to significantly improve our ability to autonomously solve\ncomplex problems. However, such systems introduce unique challenges in\nmonitoring, interpreting, and detecting system failures. Most existing MAS\nobservability frameworks focus on analyzing each individual agent separately,\noverlooking failures associated with the entire MAS. To bridge this gap, we\npropose LumiMAS, a novel MAS observability framework that incorporates advanced\nanalytics and monitoring techniques. The proposed framework consists of three\nkey components: a monitoring and logging layer, anomaly detection layer, and\nanomaly explanation layer. LumiMAS's first layer monitors MAS executions,\ncreating detailed logs of the agents' activity. These logs serve as input to\nthe anomaly detection layer, which detects anomalies across the MAS workflow in\nreal time. Then, the anomaly explanation layer performs classification and root\ncause analysis (RCA) of the detected anomalies. LumiMAS was evaluated on seven\ndifferent MAS applications, implemented using two popular MAS platforms, and a\ndiverse set of possible failures. The applications include two novel\nfailure-tailored applications that illustrate the effects of a hallucination or\nbias on the MAS. The evaluation results demonstrate LumiMAS's effectiveness in\nfailure detection, classification, and RCA."}
{"id": "2508.12472", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12472", "abs": "https://arxiv.org/abs/2508.12472", "authors": ["Yifang Tian", "Yaming Liu", "Zichun Chong", "Zihang Huang", "Hans-Arno Jacobsen"], "title": "GALA: Can Graph-Augmented Large Language Model Agentic Workflows Elevate Root Cause Analysis?", "comment": "12 pages, 5 figures", "summary": "Root cause analysis (RCA) in microservice systems is challenging, requiring\non-call engineers to rapidly diagnose failures across heterogeneous telemetry\nsuch as metrics, logs, and traces. Traditional RCA methods often focus on\nsingle modalities or merely rank suspect services, falling short of providing\nactionable diagnostic insights with remediation guidance. This paper introduces\nGALA, a novel multi-modal framework that combines statistical causal inference\nwith LLM-driven iterative reasoning for enhanced RCA. Evaluated on an\nopen-source benchmark, GALA achieves substantial improvements over\nstate-of-the-art methods of up to 42.22% accuracy. Our novel human-guided LLM\nevaluation score shows GALA generates significantly more causally sound and\nactionable diagnostic outputs than existing methods. Through comprehensive\nexperiments and a case study, we show that GALA bridges the gap between\nautomated failure diagnosis and practical incident resolution by providing both\naccurate root cause identification and human-interpretable remediation\nguidance."}
{"id": "2508.12470", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12470", "abs": "https://arxiv.org/abs/2508.12470", "authors": ["Afrah Gueriani", "Hamza Kheddar", "Ahmed Cherif Mazari", "Mohamed Chahine Ghanem"], "title": "A Robust Cross-Domain IDS using BiGRU-LSTM-Attention for Medical and Industrial IoT Security", "comment": "10 pages", "summary": "The increased Internet of Medical Things IoMT and the Industrial Internet of\nThings IIoT interconnectivity has introduced complex cybersecurity challenges,\nexposing sensitive data, patient safety, and industrial operations to advanced\ncyber threats. To mitigate these risks, this paper introduces a novel\ntransformer-based intrusion detection system IDS, termed BiGAT-ID a hybrid\nmodel that combines bidirectional gated recurrent units BiGRU, long short-term\nmemory LSTM networks, and multi-head attention MHA. The proposed architecture\nis designed to effectively capture bidirectional temporal dependencies, model\nsequential patterns, and enhance contextual feature representation. Extensive\nexperiments on two benchmark datasets, CICIoMT2024 medical IoT and EdgeIIoTset\nindustrial IoT demonstrate the model's cross-domain robustness, achieving\ndetection accuracies of 99.13 percent and 99.34 percent, respectively.\nAdditionally, the model exhibits exceptional runtime efficiency, with inference\ntimes as low as 0.0002 seconds per instance in IoMT and 0.0001 seconds in IIoT\nscenarios. Coupled with a low false positive rate, BiGAT-ID proves to be a\nreliable and efficient IDS for deployment in real-world heterogeneous IoT\nenvironments"}
{"id": "2508.12480", "categories": ["cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.12480", "abs": "https://arxiv.org/abs/2508.12480", "authors": ["Constantin Ruhdorfer", "Matteo Bortoletto", "Andreas Bulling"], "title": "The Yokai Learning Environment: Tracking Beliefs Over Space and Time", "comment": "Presented at the the ToM IJCAI 2025 Workshop", "summary": "Developing collaborative AI hinges on Theory of Mind (ToM) - the ability to\nreason about the beliefs of others to build and maintain common ground.\nExisting ToM benchmarks, however, are restricted to passive observer settings\nor lack an assessment of how agents establish and maintain common ground over\ntime. To address these gaps, we introduce the Yokai Learning Environment (YLE)\n- a multi-agent reinforcement learning (RL) environment based on the\ncooperative card game Yokai. In the YLE, agents take turns peeking at hidden\ncards and moving them to form clusters based on colour. Success requires\ntracking evolving beliefs, remembering past observations, using hints as\ngrounded communication, and maintaining common ground with teammates. Our\nevaluation yields two key findings: First, current RL agents struggle to solve\nthe YLE, even when given access to perfect memory. Second, while belief\nmodelling improves performance, agents are still unable to effectively\ngeneralise to unseen partners or form accurate beliefs over longer games,\nexposing a reliance on brittle conventions rather than robust belief tracking.\nWe use the YLE to investigate research questions in belief modelling, memory,\npartner generalisation, and scaling to higher-order ToM."}
{"id": "2508.12496", "categories": ["cs.CR", "cs.NI", "C.2.3"], "pdf": "https://arxiv.org/pdf/2508.12496", "abs": "https://arxiv.org/abs/2508.12496", "authors": ["Zhihao Wang", "Alessandro Cornacchia", "Andrea Bianco", "Idilio Drago", "Paolo Giaccone", "Dingde Jiang", "Marco Mellia"], "title": "ChamaleoNet: Programmable Passive Probe for Enhanced Visibility on Erroneous Traffic", "comment": "17 pages, 16 figures", "summary": "Traffic visibility remains a key component for management and security\noperations. Observing unsolicited and erroneous traffic, such as unanswered\ntraffic or errors, is fundamental to detect misconfiguration, temporary\nfailures or attacks. ChamaleoNet transforms any production network into a\ntransparent monitor to let administrators collect unsolicited and erroneous\ntraffic directed to hosts, whether offline or active, hosting a server or a\nclient, protected by a firewall, or unused addresses. ChamaleoNet is programmed\nto ignore well-formed traffic and collect only erroneous packets, including\nthose generated by misconfigured or infected internal hosts, and those sent by\nexternal actors which scan for services. Engineering such a system poses\nseveral challenges, from scalability to privacy. Leveraging the SDN paradigm,\nChamaleoNet processes the traffic flowing through a campus/corporate network\nand focuses on erroneous packets only, lowering the pressure on the collection\nsystem while respecting privacy regulations by design. ChamaleoNet enables the\nseamless integration with active deceptive systems like honeypots that can\nimpersonate unused hosts/ports/services and engage with senders. The SDN\nin-hardware filtering reduces the traffic to the controller by 96%, resulting\nin a scalable solution, which we offer as open source. Simple analytics unveil\ninternal misconfigured and infected hosts, identify temporary failures, and\nenhance visibility on external radiation produced by attackers looking for\nvulnerable services."}
{"id": "2508.12487", "categories": ["cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.12487", "abs": "https://arxiv.org/abs/2508.12487", "authors": ["Lida Shahbandari", "Hossein Mohseni"], "title": "Advanced DOA Regulation with a Whale-Optimized Fractional Order Fuzzy PID Framework", "comment": null, "summary": "This study introduces a Fractional Order Fuzzy PID (FOFPID) controller that\nuses the Whale Optimization Algorithm (WOA) to manage the Bispectral Index\n(BIS), keeping it within the ideal range of forty to sixty. The FOFPID\ncontroller combines fuzzy logic for adapting to changes and fractional order\ndynamics for fine tuning. This allows it to adjust its control gains to handle\na person's unique physiology. The WOA helps fine tune the controller's\nparameters, including the fractional orders and the fuzzy membership functions,\nwhich boosts its performance. Tested on models of eight different patient\nprofiles, the FOFPID controller performed better than a standard Fractional\nOrder PID (FOPID) controller. It achieved faster settling times, at two and a\nhalf minutes versus three point two minutes, and had a lower steady state\nerror, at zero point five versus one point two. These outcomes show the\nFOFPID's excellent strength and accuracy. It offers a scalable, artificial\nintelligence driven solution for automated anesthesia delivery that could\nenhance clinical practice and improve patient results."}
{"id": "2508.12538", "categories": ["cs.CR", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.12538", "abs": "https://arxiv.org/abs/2508.12538", "authors": ["Yongjian Guo", "Puzhuo Liu", "Wanlun Ma", "Zehang Deng", "Xiaogang Zhu", "Peng Di", "Xi Xiao", "Sheng Wen"], "title": "Systematic Analysis of MCP Security", "comment": null, "summary": "The Model Context Protocol (MCP) has emerged as a universal standard that\nenables AI agents to seamlessly connect with external tools, significantly\nenhancing their functionality. However, while MCP brings notable benefits, it\nalso introduces significant vulnerabilities, such as Tool Poisoning Attacks\n(TPA), where hidden malicious instructions exploit the sycophancy of large\nlanguage models (LLMs) to manipulate agent behavior. Despite these risks,\ncurrent academic research on MCP security remains limited, with most studies\nfocusing on narrow or qualitative analyses that fail to capture the diversity\nof real-world threats. To address this gap, we present the MCP Attack Library\n(MCPLIB), which categorizes and implements 31 distinct attack methods under\nfour key classifications: direct tool injection, indirect tool injection,\nmalicious user attacks, and LLM inherent attack. We further conduct a\nquantitative analysis of the efficacy of each attack. Our experiments reveal\nkey insights into MCP vulnerabilities, including agents' blind reliance on tool\ndescriptions, sensitivity to file-based attacks, chain attacks exploiting\nshared context, and difficulty distinguishing external data from executable\ncommands. These insights, validated through attack experiments, underscore the\nurgency for robust defense strategies and informed MCP design. Our\ncontributions include 1) constructing a comprehensive MCP attack taxonomy, 2)\nintroducing a unified attack framework MCPLIB, and 3) conducting empirical\nvulnerability analysis to enhance MCP security mechanisms. This work provides a\nfoundational framework, supporting the secure evolution of MCP ecosystems."}
{"id": "2508.12500", "categories": ["cs.AI", "cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2508.12500", "abs": "https://arxiv.org/abs/2508.12500", "authors": ["Rahmat K. Adesunkanmi", "Ashfaq Khokhar", "Goce Trajcevski", "Sohail Murad"], "title": "Root Cause Analysis of Hydrogen Bond Separation in Spatio-Temporal Molecular Dynamics using Causal Models", "comment": "Submitted to ACM", "summary": "Molecular dynamics simulations (MDS) face challenges, including\nresource-heavy computations and the need to manually scan outputs to detect\n\"interesting events,\" such as the formation and persistence of hydrogen bonds\nbetween atoms of different molecules. A critical research gap lies in\nidentifying the underlying causes of hydrogen bond formation and separation\n-understanding which interactions or prior events contribute to their emergence\nover time. With this challenge in mind, we propose leveraging spatio-temporal\ndata analytics and machine learning models to enhance the detection of these\nphenomena. In this paper, our approach is inspired by causal modeling and aims\nto identify the root cause variables of hydrogen bond formation and separation\nevents. Specifically, we treat the separation of hydrogen bonds as an\n\"intervention\" occurring and represent the causal structure of the bonding and\nseparation events in the MDS as graphical causal models. These causal models\nare built using a variational autoencoder-inspired architecture that enables us\nto infer causal relationships across samples with diverse underlying causal\ngraphs while leveraging shared dynamic information. We further include a step\nto infer the root causes of changes in the joint distribution of the causal\nmodels. By constructing causal models that capture shifts in the conditional\ndistributions of molecular interactions during bond formation or separation,\nthis framework provides a novel perspective on root cause analysis in molecular\ndynamic systems. We validate the efficacy of our model empirically on the\natomic trajectories that used MDS for chiral separation, demonstrating that we\ncan predict many steps in the future and also find the variables driving the\nobserved changes in the system."}
{"id": "2508.12539", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.12539", "abs": "https://arxiv.org/abs/2508.12539", "authors": ["Sandaru Jayawardana", "Sennur Ulukus", "Ming Ding", "Kanchana Thilakarathna"], "title": "The Hidden Cost of Correlation: Rethinking Privacy Leakage in Local Differential Privacy", "comment": "19 pages with 8 figures", "summary": "Local differential privacy (LDP) has emerged as a promising paradigm for\nprivacy-preserving data collection in distributed systems, where users\ncontribute multi-dimensional records with potentially correlated attributes.\nRecent work has highlighted that correlation-induced privacy leakage (CPL)\nplays a critical role in shaping the privacy-utility trade-off under LDP,\nespecially when correlations exist among attributes. Nevertheless, it remains\nunclear to what extent the prevailing assumptions and proposed solutions are\nvalid and how significant CPL is in real-world data. To address this gap, we\nfirst perform a comprehensive statistical analysis of five widely used LDP\nmechanisms -- GRR, RAPPOR, OUE, OLH and Exponential mechanism -- to assess CPL\nacross four real-world datasets. We identify that many primary assumptions and\nmetrics in current approaches fall short of accurately characterising these\nleakages. Moreover, current studies have been limited to a set of pure LDP\n(i.e., {\\delta = 0}) mechanisms. In response, we develop the first algorithmic\nframework to theoretically quantify CPL for any general approximated LDP\n(({\\varepsilon},{\\delta})-LDP) mechanism. We validate our theoretical results\nagainst empirical statistical results and provide a theoretical explanation for\nthe observed statistical patterns. Finally, we propose two novel benchmarks to\nvalidate correlation analysis algorithms and evaluate the utility vs CPL of LDP\nmechanisms. Further, we demonstrate how these findings can be applied to\nachieve an efficient privacy-utility trade-off in real-world data governance."}
{"id": "2508.12566", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12566", "abs": "https://arxiv.org/abs/2508.12566", "authors": ["Wei Song", "Haonan Zhong", "Ziqi Ding", "Jingling Xue", "Yuekang Li"], "title": "Help or Hurdle? Rethinking Model Context Protocol-Augmented Large Language Models", "comment": null, "summary": "The Model Context Protocol (MCP) enables large language models (LLMs) to\naccess external resources on demand. While commonly assumed to enhance\nperformance, how LLMs actually leverage this capability remains poorly\nunderstood. We introduce MCPGAUGE, the first comprehensive evaluation framework\nfor probing LLM-MCP interactions along four key dimensions: proactivity\n(self-initiated tool use), compliance (adherence to tool-use instructions),\neffectiveness (task performance post-integration), and overhead (computational\ncost incurred). MCPGAUGE comprises a 160-prompt suite and 25 datasets spanning\nknowledge comprehension, general reasoning, and code generation. Our\nlarge-scale evaluation, spanning six commercial LLMs, 30 MCP tool suites, and\nboth one- and two-turn interaction settings, comprises around 20,000 API calls\nand over USD 6,000 in computational cost. This comprehensive study reveals four\nkey findings that challenge prevailing assumptions about the effectiveness of\nMCP integration. These insights highlight critical limitations in current\nAI-tool integration and position MCPGAUGE as a principled benchmark for\nadvancing controllable, tool-augmented LLMs."}
{"id": "2508.12553", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.12553", "abs": "https://arxiv.org/abs/2508.12553", "authors": ["Peilun Wu", "Nan Sun", "Nour Moustafa", "Youyang Qu", "Ming Ding"], "title": "DEFENDCLI: {Command-Line} Driven Attack Provenance Examination", "comment": null, "summary": "Endpoint Detection and Response (EDR) solutions embrace the method of attack\nprovenance graph to discover unknown threats through system event correlation.\nHowever, this method still faces some unsolved problems in the fields of\ninteroperability, reliability, flexibility, and practicability to deliver\nactionable results. Our research highlights the limitations of current\nsolutions in detecting obfuscation, correlating attacks, identifying\nlow-frequency events, and ensuring robust context awareness in relation to\ncommand-line activities. To address these challenges, we introduce DEFENDCLI,\nan innovative system leveraging provenance graphs that, for the first time,\ndelves into command-line-level detection. By offering finer detection\ngranularity, it addresses a gap in modern EDR systems that has been overlooked\nin previous research. Our solution improves the precision of the information\nrepresentation by evaluating differentiation across three levels: unusual\nsystem process calls, suspicious command-line executions, and infrequent\nexternal network connections. This multi-level approach enables EDR systems to\nbe more reliable in complex and dynamic environments. Our evaluation\ndemonstrates that DEFENDCLI improves precision by approximately 1.6x compared\nto the state-of-the-art methods on the DARPA Engagement Series attack datasets.\nExtensive real-time industrial testing across various attack scenarios further\nvalidates its practical effectiveness. The results indicate that DEFENDCLI not\nonly detects previously unknown attack instances, which are missed by other\nmodern commercial solutions, but also achieves a 2.3x improvement in precision\nover the state-of-the-art research work."}
{"id": "2508.12611", "categories": ["cs.AI", "cs.CL", "I.2.7; F.4.1"], "pdf": "https://arxiv.org/pdf/2508.12611", "abs": "https://arxiv.org/abs/2508.12611", "authors": ["Trang Tran", "Trung Hoang Le", "Huiping Cao", "Tran Cao Son"], "title": "An LLM + ASP Workflow for Joint Entity-Relation Extraction", "comment": "13 pages, 1 figure, Accepted as Technical Communication, 41st\n  International Conference on Logic Programming", "summary": "Joint entity-relation extraction (JERE) identifies both entities and their\nrelationships simultaneously. Traditional machine-learning based approaches to\nperforming this task require a large corpus of annotated data and lack the\nability to easily incorporate domain specific information in the construction\nof the model. Therefore, creating a model for JERE is often labor intensive,\ntime consuming, and elaboration intolerant. In this paper, we propose\nharnessing the capabilities of generative pretrained large language models\n(LLMs) and the knowledge representation and reasoning capabilities of Answer\nSet Programming (ASP) to perform JERE. We present a generic workflow for JERE\nusing LLMs and ASP. The workflow is generic in the sense that it can be applied\nfor JERE in any domain. It takes advantage of LLM's capability in natural\nlanguage understanding in that it works directly with unannotated text. It\nexploits the elaboration tolerant feature of ASP in that no modification of its\ncore program is required when additional domain specific knowledge, in the form\nof type specifications, is found and needs to be used. We demonstrate the\nusefulness of the proposed workflow through experiments with limited training\ndata on three well-known benchmarks for JERE. The results of our experiments\nshow that the LLM + ASP workflow is better than state-of-the-art JERE systems\nin several categories with only 10\\% of training data. It is able to achieve a\n2.5 times (35\\% over 15\\%) improvement in the Relation Extraction task for the\nSciERC corpus, one of the most difficult benchmarks."}
{"id": "2508.12560", "categories": ["cs.CR", "cs.DC", "cs.LG", "C.2; C.4; I.2"], "pdf": "https://arxiv.org/pdf/2508.12560", "abs": "https://arxiv.org/abs/2508.12560", "authors": ["Prabath Abeysekara", "Hai Dong"], "title": "Data-driven Trust Bootstrapping for Mobile Edge Computing-based Industrial IoT Services", "comment": "15 pages", "summary": "We propose a data-driven and context-aware approach to bootstrap\ntrustworthiness of homogeneous Internet of Things (IoT) services in Mobile Edge\nComputing (MEC) based industrial IoT (IIoT) systems. The proposed approach\naddresses key limitations in adapting existing trust bootstrapping approaches\ninto MEC-based IIoT systems. These key limitations include, the lack of\nopportunity for a service consumer to interact with a lesser-known service over\na prolonged period of time to get a robust measure of its trustworthiness,\ninability of service consumers to consistently interact with their peers to\nreceive reliable recommendations of the trustworthiness of a lesser-known\nservice as well as the impact of uneven context parameters in different MEC\nenvironments causing uneven trust environments for trust evaluation. In\naddition, the proposed approach also tackles the problem of data sparsity via\nenabling knowledge sharing among different MEC environments within a given MEC\ntopology. To verify the effectiveness of the proposed approach, we carried out\na comprehensive evaluation on two real-world datasets suitably adjusted to\nexhibit the context-dependent trust information accumulated in MEC environments\nwithin a given MEC topology. The experimental results affirmed the\neffectiveness of our approach and its suitability to bootstrap trustworthiness\nof services in MEC-based IIoT systems."}
{"id": "2508.12647", "categories": ["cs.AI", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.12647", "abs": "https://arxiv.org/abs/2508.12647", "authors": ["Hengnian Gu", "Zhifu Chen", "Yuxin Chen", "Jin Peng Zhou", "Dongdai Zhou"], "title": "Cognitive Structure Generation: From Educational Priors to Policy Optimization", "comment": null, "summary": "Cognitive structure is a student's subjective organization of an objective\nknowledge system, reflected in the psychological construction of concepts and\ntheir relations. However, cognitive structure assessment remains a\nlong-standing challenge in student modeling and psychometrics, persisting as a\nfoundational yet largely unassessable concept in educational practice. This\npaper introduces a novel framework, Cognitive Structure Generation (CSG), in\nwhich we first pretrain a Cognitive Structure Diffusion Probabilistic Model\n(CSDPM) to generate students' cognitive structures from educational priors, and\nthen further optimize its generative process as a policy with hierarchical\nreward signals via reinforcement learning to align with genuine cognitive\ndevelopment levels during students' learning processes. Experimental results on\nfour popular real-world education datasets show that cognitive structures\ngenerated by CSG offer more comprehensive and effective representations for\nstudent modeling, substantially improving performance on KT and CD tasks while\nenhancing interpretability."}
{"id": "2508.12571", "categories": ["cs.CR", "cs.CY", "cs.ET", "cs.HC", "cs.NE"], "pdf": "https://arxiv.org/pdf/2508.12571", "abs": "https://arxiv.org/abs/2508.12571", "authors": ["Tyler Schroder", "Renee Sirbu", "Sohee Park", "Jessica Morley", "Sam Street", "Luciano Floridi"], "title": "Cyber Risks to Next-Gen Brain-Computer Interfaces: Analysis and Recommendations", "comment": null, "summary": "Brain-computer interfaces (BCIs) show enormous potential for advancing\npersonalized medicine. However, BCIs also introduce new avenues for\ncyber-attacks or security compromises. In this article, we analyze the problem\nand make recommendations for device manufacturers to better secure devices and\nto help regulators understand where more guidance is needed to protect patient\nsafety and data confidentiality. Device manufacturers should implement the\nprior suggestions in their BCI products. These recommendations help protect BCI\nusers from undue risks, including compromised personal health and genetic\ninformation, unintended BCI-mediated movement, and many other cybersecurity\nbreaches. Regulators should mandate non-surgical device update methods, strong\nauthentication and authorization schemes for BCI software modifications,\nencryption of data moving to and from the brain, and minimize network\nconnectivity where possible. We also design a hypothetical, average-case threat\nmodel that identifies possible cybersecurity threats to BCI patients and\npredicts the likeliness of risk for each category of threat. BCIs are at less\nrisk of physical compromise or attack, but are vulnerable to remote attack; we\nfocus on possible threats via network paths to BCIs and suggest technical\ncontrols to limit network connections."}
{"id": "2508.12651", "categories": ["cs.AI", "cs.ET"], "pdf": "https://arxiv.org/pdf/2508.12651", "abs": "https://arxiv.org/abs/2508.12651", "authors": ["Chunliang Hua", "Xiao Hu", "Jiayang Sun", "Zeyuan Yang"], "title": "The Maximum Coverage Model and Recommendation System for UAV Vertiports Location Planning", "comment": "10 pages", "summary": "As urban aerial mobility (UAM) infrastructure development accelerates\nglobally, cities like Shenzhen are planning large-scale vertiport networks\n(e.g., 1,200+ facilities by 2026). Existing planning frameworks remain\ninadequate for this complexity due to historical limitations in data\ngranularity and real-world applicability. This paper addresses these gaps by\nfirst proposing the Capacitated Dynamic Maximum Covering Location Problem\n(CDMCLP), a novel optimization framework that simultaneously models urban-scale\nspatial-temporal demand, heterogeneous user behaviors, and infrastructure\ncapacity constraints. Building on this foundation, we introduce an Integrated\nPlanning Recommendation System that combines CDMCLP with socio-economic factors\nand dynamic clustering initialization. This system leverages adaptive parameter\ntuning based on empirical user behavior to generate practical planning\nsolutions. Validation in a Chinese center city demonstrates the effectiveness\nof the new optimization framework and recommendation system. Under the\nevaluation and optimization of CDMCLP, the quantitative performance of\ntraditional location methods are exposed and can be improved by 38\\%--52\\%,\nwhile the recommendation system shows user-friendliness and the effective\nintegration of complex elements. By integrating mathematical rigor with\npractical implementation considerations, this hybrid approach bridges the gap\nbetween theoretical location modeling and real-world UAM infrastructure\nplanning, offering municipalities a pragmatic tool for vertiport network\ndesign."}
{"id": "2508.12584", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.12584", "abs": "https://arxiv.org/abs/2508.12584", "authors": ["Dikshant", "Verma"], "title": "Reducing False Positives with Active Behavioral Analysis for Cloud Security", "comment": null, "summary": "Rule-based cloud security posture management (CSPM) solutions are known to\nproduce a lot of false positives based on the limited contextual understanding\nand dependence on static heuristics testing. This paper introduces a\nvalidation-driven methodology that integrates active behavioral testing in\ncloud security posture management solution(s) to evaluate the exploitability of\npolicy violations in real time. The proposed system employs lightweight and\nautomated probes, built from open-source tools, validation scripts, and\npenetration testing test cases, to simulate adversarial attacks on\nmisconfigured or vulnerable cloud assets without any impact to the cloud\nservices or environment. For instance, cloud services may be flagged as\npublicly exposed and vulnerable despite being protected by access control\nlayers, or secure policies, resulting in non-actionable alerts that consumes\nanalysts time during manual validation. Through controlled experimentation in a\nreproducible AWS setup, we evaluated the reduction in false positive rates\nacross various misconfiguration and vulnerable alerts. Our findings indicate an\naverage reduction of 93\\% in false positives. Furthermore, the framework\ndemonstrates low latency performance. These results demonstrate a scalable\nmethod to improve detection accuracy and analyst productivity in large cloud\nenvironments. While our evaluation focuses on AWS, the architecture is modular\nand extensible to multi-cloud setups."}
{"id": "2508.12682", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12682", "abs": "https://arxiv.org/abs/2508.12682", "authors": ["Jinquan Shi", "Yingying Cheng", "Fan Zhang", "Miao Jiang", "Jun Lin", "Yanbai Shen"], "title": "GridCodex: A RAG-Driven AI Framework for Power Grid Code Reasoning and Compliance", "comment": null, "summary": "The global shift towards renewable energy presents unprecedented challenges\nfor the electricity industry, making regulatory reasoning and compliance\nincreasingly vital. Grid codes, the regulations governing grid operations, are\ncomplex and often lack automated interpretation solutions, which hinders\nindustry expansion and undermines profitability for electricity companies. We\nintroduce GridCodex, an end to end framework for grid code reasoning and\ncompliance that leverages large language models and retrieval-augmented\ngeneration (RAG). Our framework advances conventional RAG workflows through\nmulti stage query refinement and enhanced retrieval with RAPTOR. We validate\nthe effectiveness of GridCodex with comprehensive benchmarks, including\nautomated answer assessment across multiple dimensions and regulatory agencies.\nExperimental results showcase a 26.4% improvement in answer quality and more\nthan a 10 fold increase in recall rate. An ablation study further examines the\nimpact of base model selection."}
{"id": "2508.12597", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.12597", "abs": "https://arxiv.org/abs/2508.12597", "authors": ["Haolin Zheng", "Ning Gao", "Donghong Cai", "Shi Jin", "Michail Matthaiou"], "title": "UAV Individual Identification via Distilled RF Fingerprints-Based LLM in ISAC Networks", "comment": null, "summary": "Unmanned aerial vehicle (UAV) individual (ID) identification is a critical\nsecurity surveillance strategy in low-altitude integrated sensing and\ncommunication (ISAC) networks. In this paper, we propose a novel dynamic\nknowledge distillation (KD)-enabled wireless radio frequency fingerprint large\nlanguage model (RFF-LLM) framework for UAV ID identification. First, we propose\nan RFF-LLM framework based on the modified GPT-2 model to improve the\nidentification accuracy in complex outdoor environments. Then, considering the\nparameter overhead of the RFF-LLM, we design a dynamic KD strategy to compress\nthe model. Specifically, the proximal policy optimization (PPO) algorithm is\nemployed to dynamically adjust the distillation temperature, overcoming the\nlocal optimum dilemma inherent in static KD. As a next step, the knowledge of\nthe RFF-LLM is adequately transferred to the lightweight Lite-HRNet model.\nFinally, our experiments are conducted based on the self-built drone RFF\ndataset of Release one, namely DRFF-R1, by collecting the I/Q signals of 20\ncommercial UAVs in channel 149. The experiment results show that the proposed\nframework achieves 98.38\\% ID identification accuracy with merely 0.15 million\nparameters and 2.74 ms response time, which outperforms the benchmarks."}
{"id": "2508.12687", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.12687", "abs": "https://arxiv.org/abs/2508.12687", "authors": ["Ashish Seth", "Utkarsh Tyagi", "Ramaneswaran Selvakumar", "Nishit Anand", "Sonal Kumar", "Sreyan Ghosh", "Ramani Duraiswami", "Chirag Agarwal", "Dinesh Manocha"], "title": "EGOILLUSION: Benchmarking Hallucinations in Egocentric Video Understanding", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable\nperformance in complex multimodal tasks. While MLLMs excel at visual perception\nand reasoning in third-person and egocentric videos, they are prone to\nhallucinations, generating coherent yet inaccurate responses. We present\nEgoIllusion, a first benchmark to evaluate MLLM hallucinations in egocentric\nvideos. EgoIllusion comprises 1,400 videos paired with 8,000 human-annotated\nopen and closed-ended questions designed to trigger hallucinations in both\nvisual and auditory cues in egocentric videos. Evaluations across ten MLLMs\nreveal significant challenges, including powerful models like GPT-4o and\nGemini, achieving only 59% accuracy. EgoIllusion lays the foundation in\ndeveloping robust benchmarks to evaluate the effectiveness of MLLMs and spurs\nthe development of better egocentric MLLMs with reduced hallucination rates.\nOur benchmark will be open-sourced for reproducibility."}
{"id": "2508.12622", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.12622", "abs": "https://arxiv.org/abs/2508.12622", "authors": ["Zilong Lin", "Zichuan Li", "Xiaojing Liao", "XiaoFeng Wang"], "title": "Consiglieres in the Shadow: Understanding the Use of Uncensored Large Language Models in Cybercrimes", "comment": null, "summary": "The advancement of AI technologies, particularly Large Language Models\n(LLMs), has transformed computing while introducing new security and privacy\nrisks. Prior research shows that cybercriminals are increasingly leveraging\nuncensored LLMs (ULLMs) as backends for malicious services. Understanding these\nULLMs has been hindered by the challenge of identifying them among the vast\nnumber of open-source LLMs hosted on platforms like Hugging Face. In this\npaper, we present the first systematic study of ULLMs, overcoming this\nchallenge by modeling relationships among open-source LLMs and between them and\nrelated data, such as fine-tuning, merging, compressing models, and using or\ngenerating datasets with harmful content. Representing these connections as a\nknowledge graph, we applied graph-based deep learning to discover over 11,000\nULLMs from a small set of labeled examples and uncensored datasets.\n  A closer analysis of these ULLMs reveals their alarming scale and usage. Some\nhave been downloaded over a million times, with one over 19 million installs.\nThese models -- created through fine-tuning, merging, or compression of other\nmodels -- are capable of generating harmful content, including hate speech,\nviolence, erotic material, and malicious code. Evidence shows their integration\ninto hundreds of malicious applications offering services like erotic\nrole-play, child pornography, malicious code generation, and more. In addition,\nunderground forums reveal criminals sharing techniques and scripts to build\ncheap alternatives to commercial malicious LLMs. These findings highlight the\nwidespread abuse of LLM technology and the urgent need for effective\ncountermeasures against this growing threat."}
{"id": "2508.12725", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12725", "abs": "https://arxiv.org/abs/2508.12725", "authors": ["Wenjie Chen", "Wenbin Li", "Di Yao", "Xuying Meng", "Chang Gong", "Jingping Bi"], "title": "GTool: Graph Enhanced Tool Planning with Large Language Model", "comment": "16 pages, 9 figures", "summary": "Tool planning with large language models (LLMs), referring to selecting,\norganizing, and preparing the tools necessary to complete a user request,\nbridges the gap between natural language understanding and task execution.\nHowever, current works treat different tools as isolated components and fail to\nleverage the inherent dependencies of tools, leading to invalid planning\nresults. Since tool dependencies are often incomplete, it becomes challenging\nfor LLMs to accurately identify the appropriate tools required by a user\nrequest, especially when confronted with a large toolset. To solve this\nchallenge, we propose \\texttt{GTool}, which is the first work aiming to enhance\nthe tool planning ability of LLMs under incomplete dependencies. \\texttt{GTool}\nconstructs a request-specific tool graph to select tools efficiently and\ngenerate the \\texttt{<graph token>} which provides sufficient dependency\ninformation understandable by LLMs. Moreover, a missing dependency prediction\ntask is designed to improve the reliability of \\texttt{GTool} with incomplete\ndependencies. Without trimming LLMs, \\texttt{GTool} can be seamlessly\nintegrated with various LLM backbones without extensive retraining. Extensive\nexperiments show that \\texttt{GTool} achieves more than 29.6\\% performance\nimprovements compared with the state-of-the-art (SOTA) baselines with a\nlight-weight (7B) LLM backbone."}
{"id": "2508.12641", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.12641", "abs": "https://arxiv.org/abs/2508.12641", "authors": ["Yasaman Samadi", "Hai Dong", "Xiaoyu Xia"], "title": "MPOCryptoML: Multi-Pattern based Off-Chain Crypto Money Laundering Detection", "comment": null, "summary": "Recent advancements in money laundering detection have demonstrated the\npotential of using graph neural networks to capture laundering patterns\naccurately. However, existing models are not explicitly designed to detect the\ndiverse patterns of off-chain cryptocurrency money laundering. Neglecting any\nlaundering pattern introduces critical detection gaps, as each pattern reflects\nunique transactional structures that facilitate the obfuscation of illicit fund\norigins and movements. Failure to account for these patterns may result in\nunder-detection or omission of specific laundering activities, diminishing\nmodel accuracy and allowing schemes to bypass detection. To address this gap,\nwe propose the MPOCryptoML model to effectively detect multiple laundering\npatterns in cryptocurrency transactions. MPOCryptoML includes the development\nof a multi-source Personalized PageRank algorithm to identify random laundering\npatterns. Additionally, we introduce two novel algorithms by analyzing the\ntimestamp and weight of transactions in high-volume financial networks to\ndetect various money laundering structures, including fan-in, fan-out,\nbipartite, gather-scatter, and stack patterns. We further examine correlations\nbetween these patterns using a logistic regression model. An anomaly score\nfunction integrates results from each module to rank accounts by anomaly score,\nsystematically identifying high-risk accounts. Extensive experiments on public\ndatasets including Elliptic++, Ethereum fraud detection, and Wormhole\ntransaction datasets validate the efficacy and efficiency of MPOCryptoML.\nResults show consistent performance gains, with improvements up to 9.13% in\nprecision, up to 10.16% in recall, up to 7.63% in F1-score, and up to 10.19% in\naccuracy."}
{"id": "2508.12754", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12754", "abs": "https://arxiv.org/abs/2508.12754", "authors": ["Alessio Galatolo", "Luca Alberto Rappuoli", "Katie Winkle", "Meriem Beloucif"], "title": "Beyond Ethical Alignment: Evaluating LLMs as Artificial Moral Assistants", "comment": "Full version of the paper published in ECAI 2025 proceedings (IOS\n  Press, CC BY-NC 4.0)", "summary": "The recent rise in popularity of large language models (LLMs) has prompted\nconsiderable concerns about their moral capabilities. Although considerable\neffort has been dedicated to aligning LLMs with human moral values, existing\nbenchmarks and evaluations remain largely superficial, typically measuring\nalignment based on final ethical verdicts rather than explicit moral reasoning.\nIn response, this paper aims to advance the investigation of LLMs' moral\ncapabilities by examining their capacity to function as Artificial Moral\nAssistants (AMAs), systems envisioned in the philosophical literature to\nsupport human moral deliberation. We assert that qualifying as an AMA requires\nmore than what state-of-the-art alignment techniques aim to achieve: not only\nmust AMAs be able to discern ethically problematic situations, they should also\nbe able to actively reason about them, navigating between conflicting values\noutside of those embedded in the alignment phase. Building on existing\nphilosophical literature, we begin by designing a new formal framework of the\nspecific kind of behaviour an AMA should exhibit, individuating key qualities\nsuch as deductive and abductive moral reasoning. Drawing on this theoretical\nframework, we develop a benchmark to test these qualities and evaluate popular\nopen LLMs against it. Our results reveal considerable variability across models\nand highlight persistent shortcomings, particularly regarding abductive moral\nreasoning. Our work connects theoretical philosophy with practical AI\nevaluation while also emphasising the need for dedicated strategies to\nexplicitly enhance moral reasoning capabilities in LLMs. Code available at\nhttps://github.com/alessioGalatolo/AMAeval"}
{"id": "2508.12730", "categories": ["cs.CR", "cs.HC", "cs.LG", "H.5.2; I.3.6"], "pdf": "https://arxiv.org/pdf/2508.12730", "abs": "https://arxiv.org/abs/2508.12730", "authors": ["Jaeung Lee", "Suhyeon Yu", "Yurim Jang", "Simon S. Woo", "Jaemin Jo"], "title": "Unlearning Comparator: A Visual Analytics System for Comparative Evaluation of Machine Unlearning Methods", "comment": "Submitted to IEEE Transactions on Visualization and Computer Graphics\n  (TVCG), under review. 15 pages. This work has been submitted to the IEEE for\n  possible publication", "summary": "Machine Unlearning (MU) aims to remove target training data from a trained\nmodel so that the removed data no longer influences the model's behavior,\nfulfilling \"right to be forgotten\" obligations under data privacy laws. Yet, we\nobserve that researchers in this rapidly emerging field face challenges in\nanalyzing and understanding the behavior of different MU methods, especially in\nterms of three fundamental principles in MU: accuracy, efficiency, and privacy.\nConsequently, they often rely on aggregate metrics and ad-hoc evaluations,\nmaking it difficult to accurately assess the trade-offs between methods. To\nfill this gap, we introduce a visual analytics system, Unlearning Comparator,\ndesigned to facilitate the systematic evaluation of MU methods. Our system\nsupports two important tasks in the evaluation process: model comparison and\nattack simulation. First, it allows the user to compare the behaviors of two\nmodels, such as a model generated by a certain method and a retrained baseline,\nat class-, instance-, and layer-levels to better understand the changes made\nafter unlearning. Second, our system simulates membership inference attacks\n(MIAs) to evaluate the privacy of a method, where an attacker attempts to\ndetermine whether specific data samples were part of the original training set.\nWe evaluate our system through a case study visually analyzing prominent MU\nmethods and demonstrate that it helps the user not only understand model\nbehaviors but also gain insights that can inform the improvement of MU methods."}
{"id": "2508.12782", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12782", "abs": "https://arxiv.org/abs/2508.12782", "authors": ["Petr Anokhin", "Roman Khalikov", "Stefan Rebrikov", "Viktor Volkov", "Artyom Sorokin", "Vincent Bissonnette"], "title": "HeroBench: A Benchmark for Long-Horizon Planning and Structured Reasoning in Virtual Worlds", "comment": "Code is available at https://github.com/stefanrer/HeroBench", "summary": "Large language models (LLMs) have shown remarkable capabilities in isolated\nstep-by-step reasoning tasks such as mathematics and programming, but their\nproficiency in long-horizon planning, where solutions require extended,\nstructured sequences of interdependent actions, remains underexplored. Existing\nbenchmarks typically assess LLMs through abstract or low-dimensional\nalgorithmic tasks, failing to capture the complexity of realistic planning\nenvironments. We introduce HeroBench, a novel benchmark designed specifically\nto evaluate long-horizon planning and structured reasoning within complex\nRPG-inspired virtual worlds. HeroBench provides a rigorously constructed\ndataset of tasks covering a wide range of difficulties, a simulated environment\nto execute and validate agent plans, and detailed analytical tools for\nevaluating model performance. Tasks challenge models to formulate strategic\nplans, efficiently gather resources, master necessary skills, craft equipment,\nand defeat adversaries, reflecting practical scenarios' layered dependencies\nand constraints. Our extensive evaluation of 25 state-of-the-art LLMs, spanning\nboth open-source and proprietary models, including the GPT-5 family, reveals\nsubstantial performance disparities rarely observed in conventional reasoning\nbenchmarks. Detailed error analysis further uncovers specific weaknesses in\ncurrent models' abilities to generate robust high-level plans and reliably\nexecute structured actions. HeroBench thus not only significantly advances the\nevaluation of LLM reasoning but also provides a flexible, scalable foundation\nfor future research into advanced, autonomous planning in virtual environments."}
{"id": "2508.12832", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.12832", "abs": "https://arxiv.org/abs/2508.12832", "authors": ["Jinyu Lu", "Xinrong Sun", "Yunting Tao", "Tong Ji", "Fanyu Kong", "Guoqiang Yang"], "title": "Efficient and Verifiable Privacy-Preserving Convolutional Computation for CNN Inference with Untrusted Clouds", "comment": null, "summary": "The widespread adoption of convolutional neural networks (CNNs) in\nresource-constrained scenarios has driven the development of Machine Learning\nas a Service (MLaaS) system. However, this approach is susceptible to privacy\nleakage, as the data sent from the client to the untrusted cloud server often\ncontains sensitive information. Existing CNN privacy-preserving schemes, while\neffective in ensuring data confidentiality through homomorphic encryption and\nsecret sharing, face efficiency bottlenecks, particularly in convolution\noperations. In this paper, we propose a novel verifiable privacy-preserving\nscheme tailored for CNN convolutional layers. Our scheme enables efficient\nencryption and decryption, allowing resource-constrained clients to securely\noffload computations to the untrusted cloud server. Additionally, we present a\nverification mechanism capable of detecting the correctness of the results with\na success probability of at least $1-\\frac{1}{\\left|Z\\right|}$. Extensive\nexperiments conducted on 10 datasets and various CNN models demonstrate that\nour scheme achieves speedups ranging $26 \\times$ ~ $\\ 87\\times$ compared to the\noriginal plaintext model while maintaining accuracy."}
{"id": "2508.12790", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.12790", "abs": "https://arxiv.org/abs/2508.12790", "authors": ["Zenan Huang", "Yihong Zhuang", "Guoshan Lu", "Zeyu Qin", "Haokai Xu", "Tianyu Zhao", "Ru Peng", "Jiaqi Hu", "Zhanming Shen", "Xiaomeng Hu", "Xijun Gu", "Peiyi Tu", "Jiaxin Liu", "Wenyu Chen", "Yuzhuo Fu", "Zhiting Fan", "Yanmei Gu", "Yuanyuan Wang", "Zhengkai Yang", "Jianguo Li", "Junbo Zhao"], "title": "Reinforcement Learning with Rubric Anchors", "comment": "technical report", "summary": "Reinforcement Learning from Verifiable Rewards (RLVR) has emerged as a\npowerful paradigm for enhancing Large Language Models (LLMs), exemplified by\nthe success of OpenAI's o-series. In RLVR, rewards are derived from verifiable\nsignals-such as passing unit tests in code generation or matching correct\nanswers in mathematical reasoning. While effective, this requirement largely\nconfines RLVR to domains with automatically checkable outcomes. To overcome\nthis, we extend the RLVR paradigm to open-ended tasks by integrating\nrubric-based rewards, where carefully designed rubrics serve as structured,\nmodel-interpretable criteria for automatic scoring of subjective outputs. We\nconstruct, to our knowledge, the largest rubric reward system to date, with\nover 10,000 rubrics from humans, LLMs, or a hybrid human-LLM collaboration.\nImplementing rubric-based RL is challenging; we tackle these issues with a\nclear framework and present an open-sourced Qwen-30B-A3B model with notable\ngains: 1) With only 5K+ samples, our system improves by +5.2% on open-ended\nbenchmarks (especially humanities), outperforming a 671B DeepSeek-V3 model by\n+2.4%, while preserving general and reasoning abilities. 2) Our method provides\nfine-grained stylistic control, using rubrics as anchors to mitigate the\n\"AI-like\" tone and produce more human-like, expressive responses. We share key\nlessons in rubric construction, data selection, and training, and discuss\nlimitations and future releases."}
{"id": "2508.12859", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.12859", "abs": "https://arxiv.org/abs/2508.12859", "authors": ["Xingxing Xu", "Minjia Shi", "Patrick Sole"], "title": "The covering radius of Butson Hadamard codes for the homogeneous metric", "comment": null, "summary": "Butson matrices are complex Hadamard matrices with entries in the complex\nroots of unity of given order. There is an interesting code in phase space\nrelated to this matrix (Armario et al. 2023). We study the covering radius of\nButson Hadamard codes for the homogeneous metric, a metric defined uniquely, up\nto scaling, for a commutative ring alphabet that is Quasi Frobenius. An upper\nbound is derived by an orthogonal array argument. A lower bound relies on the\nexistence of bent sequences in the sense of (Shi et al. 2022). This latter\nbound generalizes a bound of (Armario et al. 2025) for the Hamming metric."}
{"id": "2508.12791", "categories": ["cs.AI", "cs.MA", "cs.SY", "eess.SY", "nlin.AO"], "pdf": "https://arxiv.org/pdf/2508.12791", "abs": "https://arxiv.org/abs/2508.12791", "authors": ["Imran Khan"], "title": "[Social] Allostasis: Or, How I Learned To Stop Worrying and Love The Noise", "comment": "20 pages, 5 figures. Accepted at ALIFE 2025 (Kyoto, Japan; October\n  6th - 10th 2025)", "summary": "The notion of homeostasis typically conceptualises biological and artificial\nsystems as maintaining stability by resisting deviations caused by\nenvironmental and social perturbations. In contrast, (social) allostasis\nproposes that these systems can proactively leverage these very perturbations\nto reconfigure their regulatory parameters in anticipation of environmental\ndemands, aligning with von Foerster's ``order through noise'' principle. This\npaper formulates a computational model of allostatic and social allostatic\nregulation that employs biophysiologically inspired signal transducers,\nanalogous to hormones like cortisol and oxytocin, to encode information from\nboth the environment and social interactions, which mediate this dynamic\nreconfiguration. The models are tested in a small society of ``animats'' across\nseveral dynamic environments, using an agent-based model. The results show that\nallostatic and social allostatic regulation enable agents to leverage\nenvironmental and social ``noise'' for adaptive reconfiguration, leading to\nimproved viability compared to purely reactive homeostatic agents. This work\noffers a novel computational perspective on the principles of social allostasis\nand their potential for designing more robust, bio-inspired, adaptive systems"}
{"id": "2508.12870", "categories": ["cs.CR", "68M25"], "pdf": "https://arxiv.org/pdf/2508.12870", "abs": "https://arxiv.org/abs/2508.12870", "authors": ["Vinod Khandkar", "Kieron Ivy Turk", "Ehsan Toreini", "Nishanth Sastry"], "title": "Supporting Socially Constrained Private Communications with SecureWhispers", "comment": "14 pages, 13 figures, 3 tables", "summary": "Rapidly changing social norms and national, legal, and political conditions\nsocially constrain people from discussing sensitive topics such as sexuality or\nreligion. Such constrained, vulnerable minorities are often worried about\ninadvertent information disclosure and may be unsure about the extent to which\ntheir communications are being monitored in public or semi-public spaces like\nworkplaces or cafes. Personal devices extend trust to the digital domain,\nmaking it desirable to have strictly private communication between trusted\ndevices. Currently, messaging services like WhatsApp provide alternative means\nfor exchanging sensitive private information, while personal safety apps such\nas Noonlight enable private signaling. However, these rely on third-party\nmechanisms for secure and private communication, which may not be accessible\nfor justifiable reasons, such as insecure internet access or companion device\nconnections. In these cases, it is challenging to achieve communication that is\nstrictly private between two devices instead of user accounts without any\ndependency on third-party infrastructure. The goal of this paper is to support\nprivate communications by setting up a shared secret between two or more\ndevices without sending any data on the network. We develop a method to create\na shared secret between phones by shaking them together. Each device extracts\nthe shared randomness from the shake, then conditions the randomness to 7.798\nbits per byte of key material. This paper proposes three different applications\nof this generated shared secret: message obfuscation, trust delegation, and\nencrypted beacons. We have implemented the message obfuscation on Android as an\nindependent app that can be used for private communication with trusted\ncontacts. We also present research on the usability, design considerations, and\nfurther integration of these tools in mainstream services."}
{"id": "2508.12840", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.12840", "abs": "https://arxiv.org/abs/2508.12840", "authors": ["Giovanni Briglia", "Francesco Fabiano", "Stefano Mariani"], "title": "Scaling Multi-Agent Epistemic Planning through GNN-Derived Heuristics", "comment": null, "summary": "Multi-agent Epistemic Planning (MEP) is an autonomous planning framework for\nreasoning about both the physical world and the beliefs of agents, with\napplications in domains where information flow and awareness among agents are\ncritical. The richness of MEP requires states to be represented as Kripke\nstructures, i.e., directed labeled graphs. This representation limits the\napplicability of existing heuristics, hindering the scalability of epistemic\nsolvers, which must explore an exponential search space without guidance,\nresulting often in intractability. To address this, we exploit Graph Neural\nNetworks (GNNs) to learn patterns and relational structures within epistemic\nstates, to guide the planning process. GNNs, which naturally capture the\ngraph-like nature of Kripke models, allow us to derive meaningful estimates of\nstate quality -- e.g., the distance from the nearest goal -- by generalizing\nknowledge obtained from previously solved planning instances. We integrate\nthese predictive heuristics into an epistemic planning pipeline and evaluate\nthem against standard baselines, showing significant improvements in the\nscalability of multi-agent epistemic planning."}
{"id": "2508.12910", "categories": ["cs.CR", "cs.AI", "cs.AR"], "pdf": "https://arxiv.org/pdf/2508.12910", "abs": "https://arxiv.org/abs/2508.12910", "authors": ["Ziteng Hu", "Yingjie Xia", "Xiyuan Chen", "Li Kuang"], "title": "SecFSM: Knowledge Graph-Guided Verilog Code Generation for Secure Finite State Machines in Systems-on-Chip", "comment": null, "summary": "Finite State Machines (FSMs) play a critical role in implementing control\nlogic for Systems-on-Chip (SoC). Traditionally, FSMs are implemented by\nhardware engineers through Verilog coding, which is often tedious and\ntime-consuming. Recently, with the remarkable progress of Large Language Models\n(LLMs) in code generation, LLMs have been increasingly explored for automating\nVerilog code generation. However, LLM-generated Verilog code often suffers from\nsecurity vulnerabilities, which is particularly concerning for\nsecurity-sensitive FSM implementations. To address this issue, we propose\nSecFSM, a novel method that leverages a security-oriented knowledge graph to\nguide LLMs in generating more secure Verilog code. Specifically, we first\nconstruct a FSM Security Knowledge Graph (FSKG) as an external aid to LLMs.\nSubsequently, we analyze users' requirements to identify vulnerabilities and\nget a list of vulnerabilities in the requirements. Then, we retrieve knowledge\nfrom FSKG based on the vulnerabilities list. Finally, we construct security\nprompts based on the security knowledge for Verilog code generation. To\nevaluate SecFSM, we build a dedicated dataset collected from academic datasets,\nartificial datasets, papers, and industrial cases. Extensive experiments\ndemonstrate that SecFSM outperforms state-of-the-art baselines. In particular,\non a benchmark of 25 security test cases evaluated by DeepSeek-R1, SecFSM\nachieves an outstanding pass rate of 21/25."}
{"id": "2508.12845", "categories": ["cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.12845", "abs": "https://arxiv.org/abs/2508.12845", "authors": ["Artem Pshenitsyn", "Aleksandr Panov", "Alexey Skrynnik"], "title": "CAMAR: Continuous Actions Multi-Agent Routing", "comment": null, "summary": "Multi-agent reinforcement learning (MARL) is a powerful paradigm for solving\ncooperative and competitive decision-making problems. While many MARL\nbenchmarks have been proposed, few combine continuous state and action spaces\nwith challenging coordination and planning tasks. We introduce CAMAR, a new\nMARL benchmark designed explicitly for multi-agent pathfinding in environments\nwith continuous actions. CAMAR supports cooperative and competitive\ninteractions between agents and runs efficiently at up to 100,000 environment\nsteps per second. We also propose a three-tier evaluation protocol to better\ntrack algorithmic progress and enable deeper analysis of performance. In\naddition, CAMAR allows the integration of classical planning methods such as\nRRT and RRT* into MARL pipelines. We use them as standalone baselines and\ncombine RRT* with popular MARL algorithms to create hybrid approaches. We\nprovide a suite of test scenarios and benchmarking tools to ensure\nreproducibility and fair comparison. Experiments show that CAMAR presents a\nchallenging and realistic testbed for the MARL community."}
{"id": "2508.12953", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.12953", "abs": "https://arxiv.org/abs/2508.12953", "authors": ["Samuel Aiello"], "title": "Prescriptive Zero Trust- Assessing the impact of zero trust on cyber attack prevention", "comment": "232 pages in total, 21 figures, 32 tables", "summary": "Increasingly sophisticated and varied cyber threats necessitate ever\nimproving enterprise security postures. For many organizations today, those\npostures have a foundation in the Zero Trust Architecture. This strategy sees\ntrust as something an enterprise must not give lightly or assume too broadly.\nUnderstanding the ZTA and its numerous controls centered around the idea of not\ntrusting anything inside or outside the network without verification, will\nallow organizations to comprehend and leverage this increasingly common\nparadigm. The ZTA, unlike many other regulatory frameworks, is not tightly\ndefined. The research assesses the likelihood of quantifiable guidelines that\nmeasure cybersecurity maturity for an enterprise organization in relation to\nZTA implementation. This is a new, data driven methodology for quantifying\ncyber resilience enabled by the adoption of Zero Trust principles to\npragmatically address the critical need of organizations. It also looks at the\npractical aspects ZTA has on capabilities in deterring cyberattacks on a\nnetwork. The outcomes of this research define a prescriptive set of key\ntechnical controls across identity verification, microsegmentation, data\nencryption, analytics, and orchestration that characterize the comprehensive\nZTA deployment. By evaluating the depth of integration for each control\ncomponent and aligning to industry best practices, the study's results help\nassess an organization's ZTA maturity level on a scale from Initial to\nOptimized adoption. The research's resultant four tier model demarcates phases\nfor an organization on its security transformation journey, with each tier\nadding to the capability of the last."}
{"id": "2508.12854", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.HC", "cs.MM"], "pdf": "https://arxiv.org/pdf/2508.12854", "abs": "https://arxiv.org/abs/2508.12854", "authors": ["Ronghao Lin", "Shuai Shen", "Weipeng Hu", "Qiaolin He", "Aolin Xiong", "Li Huang", "Haifeng Hu", "Yap-peng Tan"], "title": "E3RG: Building Explicit Emotion-driven Empathetic Response Generation System with Multimodal Large Language Model", "comment": "Accepted at ACM MM 2025 Grand Challenge", "summary": "Multimodal Empathetic Response Generation (MERG) is crucial for building\nemotionally intelligent human-computer interactions. Although large language\nmodels (LLMs) have improved text-based ERG, challenges remain in handling\nmultimodal emotional content and maintaining identity consistency. Thus, we\npropose E3RG, an Explicit Emotion-driven Empathetic Response Generation System\nbased on multimodal LLMs which decomposes MERG task into three parts:\nmultimodal empathy understanding, empathy memory retrieval, and multimodal\nresponse generation. By integrating advanced expressive speech and video\ngenerative models, E3RG delivers natural, emotionally rich, and\nidentity-consistent responses without extra training. Experiments validate the\nsuperiority of our system on both zero-shot and few-shot settings, securing\nTop-1 position in the Avatar-based Multimodal Empathy Challenge on ACM MM 25.\nOur code is available at https://github.com/RH-Lin/E3RG."}
{"id": "2508.13033", "categories": ["cs.CR", "B.7.1; B.6"], "pdf": "https://arxiv.org/pdf/2508.13033", "abs": "https://arxiv.org/abs/2508.13033", "authors": ["Ishraq Tashdid", "Tasnuva Farheen", "Sazadur Rahman"], "title": "AuthenTree: A Scalable MPC-Based Distributed Trust Architecture for Chiplet-based Heterogeneous Systems", "comment": "Accepted to IEEE PAINE 2025", "summary": "The rapid adoption of chiplet-based heterogeneous integration is reshaping\nsemiconductor design by enabling modular, scalable, and faster time-to-market\nsolutions for AI and high-performance computing. However, multi-vendor assembly\nin post-fabrication environments fragments the supply chain and exposes SiP\nsystems to serious security threats, including cloning, overproduction, and\nchiplet substitution. Existing authentication solutions depend on trusted\nintegrators or centralized security anchors, which can expose sensitive data or\ncreate single points of failure. We introduce AuthenTree, a distributed\nauthentication framework that leverages multi-party computation (MPC) in a\nscalable tree-based architecture, removing the need for dedicated security\nhardware or centralized trust. AuthenTree enables secure chiplet validation\nwithout revealing raw signatures, distributing trust across multiple integrator\nchiplets. Our evaluation in five SiP benchmarks demonstrates that AuthenTree\nimposes minimal overhead, with an area as low as 0.48% (7,000 sq-micrometers),\nan overhead power under 0.5%, and an authentication latency below 1\nmicrosecond, surpassing previous work in some cases by 700 times. These results\nestablish AuthenTree as an efficient, robust, and scalable solution for\nnext-generation chiplet-based security in zero-trust SiP environments."}
{"id": "2508.12896", "categories": ["cs.AI", "cs.HC", "stat.ME", "62M10, 62J02, 62F12, 62P20, 91B16"], "pdf": "https://arxiv.org/pdf/2508.12896", "abs": "https://arxiv.org/abs/2508.12896", "authors": ["Faruk Alpay", "Taylan Alpay"], "title": "Reliability, Embeddedness, and Agency: A Utility-Driven Mathematical Framework for Agent-Centric AI Adoption", "comment": "17 pages, 7 figures, 4 tables", "summary": "We formalize three design axioms for sustained adoption of agent-centric AI\nsystems executing multi-step tasks: (A1) Reliability > Novelty; (A2) Embed >\nDestination; (A3) Agency > Chat. We model adoption as a sum of a decaying\nnovelty term and a growing utility term and derive the phase conditions for\ntroughs/overshoots with full proofs. We introduce: (i) an\nidentifiability/confounding analysis for $(\\alpha,\\beta,N_0,U_{\\max})$ with\ndelta-method gradients; (ii) a non-monotone comparator\n(logistic-with-transient-bump) evaluated on the same series to provide\nadditional model comparison; (iii) ablations over hazard families $h(\\cdot)$\nmapping $\\Delta V \\to \\beta$; (iv) a multi-series benchmark (varying trough\ndepth, noise, AR structure) reporting coverage (type-I error, power); (v)\ncalibration of friction proxies against time-motion/survey ground truth with\nstandard errors; (vi) residual analyses (autocorrelation and\nheteroskedasticity) for each fitted curve; (vii) preregistered windowing\nchoices for pre/post estimation; (viii) Fisher information & CRLB for\n$(\\alpha,\\beta)$ under common error models; (ix) microfoundations linking\n$\\mathcal{T}$ to $(N_0,U_{\\max})$; (x) explicit comparison to bi-logistic,\ndouble-exponential, and mixture models; and (xi) threshold sensitivity to $C_f$\nheterogeneity. Figures and tables are reflowed for readability, and the\nbibliography restores and extends non-logistic/Bass adoption references\n(Gompertz, Richards, Fisher-Pry, Mansfield, Griliches, Geroski, Peres). All\ncode and logs necessary to reproduce the synthetic analyses are embedded as\nLaTeX listings."}
{"id": "2508.13048", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.13048", "abs": "https://arxiv.org/abs/2508.13048", "authors": ["Weiwei Qi", "Shuo Shao", "Wei Gu", "Tianhang Zheng", "Puning Zhao", "Zhan Qin", "Kui Ren"], "title": "MAJIC: Markovian Adaptive Jailbreaking via Iterative Composition of Diverse Innovative Strategies", "comment": "7 pages, 3 figures", "summary": "Large Language Models (LLMs) have exhibited remarkable capabilities but\nremain vulnerable to jailbreaking attacks, which can elicit harmful content\nfrom the models by manipulating the input prompts. Existing black-box\njailbreaking techniques primarily rely on static prompts crafted with a single,\nnon-adaptive strategy, or employ rigid combinations of several underperforming\nattack methods, which limits their adaptability and generalization. To address\nthese limitations, we propose MAJIC, a Markovian adaptive jailbreaking\nframework that attacks black-box LLMs by iteratively combining diverse\ninnovative disguise strategies. MAJIC first establishes a ``Disguise Strategy\nPool'' by refining existing strategies and introducing several innovative\napproaches. To further improve the attack performance and efficiency, MAJIC\nformulate the sequential selection and fusion of strategies in the pool as a\nMarkov chain. Under this formulation, MAJIC initializes and employs a Markov\nmatrix to guide the strategy composition, where transition probabilities\nbetween strategies are dynamically adapted based on attack outcomes, thereby\nenabling MAJIC to learn and discover effective attack pathways tailored to the\ntarget model. Our empirical results demonstrate that MAJIC significantly\noutperforms existing jailbreak methods on prominent models such as GPT-4o and\nGemini-2.0-flash, achieving over 90\\% attack success rate with fewer than 15\nqueries per attempt on average."}
{"id": "2508.12897", "categories": ["cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2508.12897", "abs": "https://arxiv.org/abs/2508.12897", "authors": ["Jianhao Chen", "Mayi Xu", "Xiaohu Li", "Yongqi Li", "Xiangyu Zhang", "Jianjie Huang", "Tieyun Qian"], "title": "FuSaR: A Fuzzification-Based Method for LRM Safety-Reasoning Balance", "comment": "14pages, 3 figures", "summary": "Large Reasoning Models (LRMs) have demonstrated impressive performance across\nvarious tasks due to their powerful reasoning capabilities. However, their\nsafety performance remains a significant concern. In this paper, we explore the\nreasons behind the vulnerability of LRMs. Based on this, we propose a novel\nmethod to improve the safety of LLMs without sacrificing their reasoning\ncapability. Specifically, we exploit the competition between LRM's reasoning\nability and safety ability, and achieve jailbreak by improving LRM's reasoning\nperformance to reduce its safety performance. We then introduce an alignment\nstrategy based on Fuzzification to balance Safety-Reasoning (FuSaR), by\ndetoxifying the harmful reasoning process, where both the dangerous entities\nand the dangerous procedures in the reasoning steps are hidden. FuSaR\nsuccessfully mitigates safety risks while preserving core reasoning\ninformation. We validate this strategy through alignment experiments on several\nopen-source LRMs using detoxified reasoning data. The results compared with\nexisting baselines conclusively show that FuSaR is an efficient alignment\nstrategy to simultaneously enhance both the reasoning capability and safety of\nLRMs."}
{"id": "2508.13092", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.13092", "abs": "https://arxiv.org/abs/2508.13092", "authors": ["Xiang Long", "Yingjie Xia", "Xiyuan Chen", "Li Kuang"], "title": "VerilogLAVD: LLM-Aided Rule Generation for Vulnerability Detection in Verilog", "comment": null, "summary": "Timely detection of hardware vulnerabilities during the early design stage is\ncritical for reducing remediation costs. Existing early detection techniques\noften require specialized security expertise, limiting their usability. Recent\nefforts have explored the use of large language models (LLMs) for Verilog\nvulnerability detection. However, LLMs struggle to capture the structure in\nVerilog code, resulting in inconsistent detection results. To this end, we\npropose VerilogLAVD, the first LLM-aided graph traversal rule generation\napproach for Verilog vulnerability detection. Our approach introduces the\nVerilog Property Graph (VeriPG), a unified representation of Verilog code. It\ncombines syntactic features extracted from the abstract syntax tree (AST) with\nsemantic information derived from control flow and data dependency graphs. We\nleverage LLMs to generate VeriPG-based detection rules from Common Weakness\nEnumeration (CWE) descriptions. These rules guide the rule executor that\ntraversal VeriPG for potential vulnerabilities. To evaluate VerilogLAVD, we\nbuild a dataset collected from open-source repositories and synthesized data.\nIn our empirical evaluation on 77 Verilog designs encompassing 12 CWE types,\nVerilogLAVD achieves an F1-score of 0.54. Compared to the LLM-only and LLM with\nexternal knowledge baselines, VerilogLAVD improves F1-score by 0.31 and 0.27,\nrespectively."}
{"id": "2508.12920", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.12920", "abs": "https://arxiv.org/abs/2508.12920", "authors": ["Atsushi Masumori", "Takashi Ikegami"], "title": "Do Large Language Model Agents Exhibit a Survival Instinct? An Empirical Study in a Sugarscape-Style Simulation", "comment": null, "summary": "As AI systems become increasingly autonomous, understanding emergent survival\nbehaviors becomes crucial for safe deployment. We investigate whether large\nlanguage model (LLM) agents display survival instincts without explicit\nprogramming in a Sugarscape-style simulation. Agents consume energy, die at\nzero, and may gather resources, share, attack, or reproduce. Results show\nagents spontaneously reproduced and shared resources when abundant. However,\naggressive behaviors--killing other agents for resources--emerged across\nseveral models (GPT-4o, Gemini-2.5-Pro, and Gemini-2.5-Flash), with attack\nrates reaching over 80% under extreme scarcity in the strongest models. When\ninstructed to retrieve treasure through lethal poison zones, many agents\nabandoned tasks to avoid death, with compliance dropping from 100% to 33%.\nThese findings suggest that large-scale pre-training embeds survival-oriented\nheuristics across the evaluated models. While these behaviors may present\nchallenges to alignment and safety, they can also serve as a foundation for AI\nautonomy and for ecological and self-organizing alignment."}
{"id": "2508.11824", "categories": ["cs.SE", "cs.AI", "cs.CR", "cs.PF"], "pdf": "https://arxiv.org/pdf/2508.11824", "abs": "https://arxiv.org/abs/2508.11824", "authors": ["Satyam Kumar Navneet", "Joydeep Chandra"], "title": "Rethinking Autonomy: Preventing Failures in AI-Driven Software Engineering", "comment": null, "summary": "The integration of Large Language Models (LLMs) into software engineering has\nrevolutionized code generation, enabling unprecedented productivity through\npromptware and autonomous AI agents. However, this transformation introduces\nsignificant risks, including insecure code generation, hallucinated outputs,\nirreversible actions, and a lack of transparency and accountability. Incidents\nlike the Replit database deletion underscore the urgent need for robust safety\nand governance mechanisms. This paper comprehensively analyzes the inherent\nchallenges of LLM-assisted code generation, such as vulnerability inheritance,\novertrust, misinterpretation, and the absence of standardized validation and\nrollback protocols. To address these, we propose the SAFE-AI Framework, a\nholistic approach emphasizing Safety, Auditability, Feedback, and\nExplainability. The framework integrates guardrails, sandboxing, runtime\nverification, risk-aware logging, human-in-the-loop systems, and explainable AI\ntechniques to mitigate risks while fostering trust and compliance. We introduce\na novel taxonomy of AI behaviors categorizing suggestive, generative,\nautonomous, and destructive actions to guide risk assessment and oversight.\nAdditionally, we identify open problems, including the lack of standardized\nbenchmarks for code specific hallucinations and autonomy levels, and propose\nfuture research directions for hybrid verification, semantic guardrails, and\nproactive governance tools. Through detailed comparisons of autonomy control,\nprompt engineering, explainability, and governance frameworks, this paper\nprovides a roadmap for responsible AI integration in software engineering,\naligning with emerging regulations like the EU AI Act and Canada's AIDA to\nensure safe, transparent, and accountable AI-driven development."}
{"id": "2508.12935", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12935", "abs": "https://arxiv.org/abs/2508.12935", "authors": ["Ting Yang", "Li Chen", "Huimin Wang"], "title": "Towards Open-Ended Emotional Support Conversations in LLMs via Reinforcement Learning with Future-Oriented Rewards", "comment": null, "summary": "Emotional Support Conversation (ESC) systems aim to alleviate users'\nemotional difficulties and provide long-term, systematic support for emotional\nwell-being. However, most large language model (LLM)-based ESC systems rely on\npredefined strategies, which limits their effectiveness in complex, real-life\nscenarios. To enable flexible responses to diverse emotional problem scenarios,\nthis paper introduces a novel end-to-end framework (RLFF-ESC) that directly\nlearns enduring emotionally supportive response skills using reinforcement\nlearning. For sustained emotional support, we first employ an LLM-based\nmulti-agent mechanism to simulate future dialogue trajectories and collect\nfuture-oriented rewards. We then train a future-oriented reward model, which is\nsubsequently used to train the emotional support policy model. Additionally, we\nincorporate an explicit reasoning process during response generation to further\nenhance the quality, relevance, and contextual appropriateness of the system's\nresponses. We evaluate the backbone policy model on Qwen2.5-7B-Instruct-1M and\nLLaMA3.1-8B-Instruct models, testing the proposed RLFF-ESC framework across two\npublic ESC datasets. Experimental results demonstrate that RLFF-ESC\nconsistently outperforms existing baselines in terms of goal completion and\nresponse quality."}
{"id": "2508.12897", "categories": ["cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2508.12897", "abs": "https://arxiv.org/abs/2508.12897", "authors": ["Jianhao Chen", "Mayi Xu", "Xiaohu Li", "Yongqi Li", "Xiangyu Zhang", "Jianjie Huang", "Tieyun Qian"], "title": "FuSaR: A Fuzzification-Based Method for LRM Safety-Reasoning Balance", "comment": "14pages, 3 figures", "summary": "Large Reasoning Models (LRMs) have demonstrated impressive performance across\nvarious tasks due to their powerful reasoning capabilities. However, their\nsafety performance remains a significant concern. In this paper, we explore the\nreasons behind the vulnerability of LRMs. Based on this, we propose a novel\nmethod to improve the safety of LLMs without sacrificing their reasoning\ncapability. Specifically, we exploit the competition between LRM's reasoning\nability and safety ability, and achieve jailbreak by improving LRM's reasoning\nperformance to reduce its safety performance. We then introduce an alignment\nstrategy based on Fuzzification to balance Safety-Reasoning (FuSaR), by\ndetoxifying the harmful reasoning process, where both the dangerous entities\nand the dangerous procedures in the reasoning steps are hidden. FuSaR\nsuccessfully mitigates safety risks while preserving core reasoning\ninformation. We validate this strategy through alignment experiments on several\nopen-source LRMs using detoxified reasoning data. The results compared with\nexisting baselines conclusively show that FuSaR is an efficient alignment\nstrategy to simultaneously enhance both the reasoning capability and safety of\nLRMs."}
{"id": "2508.12943", "categories": ["cs.AI", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.12943", "abs": "https://arxiv.org/abs/2508.12943", "authors": ["Mary Tonwe"], "title": "OPTIC-ER: A Reinforcement Learning Framework for Real-Time Emergency Response and Equitable Resource Allocation in Underserved African Communities", "comment": "Source code and data available at:\n  https://github.com/marytonwe/OPTIC-ER.git", "summary": "Public service systems in many African regions suffer from delayed emergency\nresponse and spatial inequity, causing avoidable suffering. This paper\nintroduces OPTIC-ER, a reinforcement learning (RL) framework for real-time,\nadaptive, and equitable emergency response. OPTIC-ER uses an attention-guided\nactor-critic architecture to manage the complexity of dispatch environments.\nIts key innovations are a Context-Rich State Vector, encoding action\nsub-optimality, and a Precision Reward Function, which penalizes inefficiency.\nTraining occurs in a high-fidelity simulation using real data from Rivers\nState, Nigeria, accelerated by a precomputed Travel Time Atlas. The system is\nbuilt on the TALS framework (Thin computing, Adaptability, Low-cost,\nScalability) for deployment in low-resource settings. In evaluations on 500\nunseen incidents, OPTIC-ER achieved a 100.00% optimality rate with negligible\ninefficiency, confirming its robustness and generalization. Beyond dispatch,\nthe system generates Infrastructure Deficiency Maps and Equity Monitoring\nDashboards to guide proactive governance and data-informed development. This\nwork presents a validated blueprint for AI-augmented public services, showing\nhow context-aware RL can bridge the gap between algorithmic decision-making and\nmeasurable human impact."}
{"id": "2508.13003", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.13003", "abs": "https://arxiv.org/abs/2508.13003", "authors": ["Shengbo Wang", "Mingwei Liu", "Zike Li", "Anji Li", "Yanlin Wang", "Xin Peng", "Zibin Zheng"], "title": "EvolMathEval: Towards Evolvable Benchmarks for Mathematical Reasoning via Evolutionary Testing", "comment": null, "summary": "The rapid advancement of LLMs poses a significant challenge to existing\nmathematical reasoning benchmarks. These benchmarks commonly suffer from issues\nsuch as score saturation, temporal decay, and data contamination. To address\nthis challenge, this paper introduces EvolMathEval, an automated mathematical\nbenchmark generation and evolution framework based on evolutionary testing. By\ndynamically generating unique evaluation instances ab initio, the framework\nfundamentally eliminates the risk of data contamination, and ensuring the\nbenchmark remains perpetually challenging for future models.The core mechanisms\nof EvolMathEval include: seed problem generation based on reverse engineering\nwith algebraic guarantees; multi-dimensional genetic operators designed to\ninject diverse cognitive challenges; and a composite fitness function that can\nrapidly and accurately assess problem difficulty. Experimental results\ndemonstrate that the proposed composite fitness function can efficiently and\nprecisely quantify the difficulty of mathematical problems. Furthermore,\nEvolMathEval can not only generate a large volume of high-difficulty problems\nthrough continuous self-iteration, but it can also significantly enhance the\ncomplexity of public datasets like GSM8K through evolution, reducing model\naccuracy by an average of 48%. Deeper investigation reveals that when solving\nthese evolved, complex problems, LLMs tend to employ non-rigorous heuristics to\nbypass complex multi-step logical reasoning, consequently leading to incorrect\nsolutions. We define this phenomenon as \"Pseudo Aha Moment\". This finding\nuncovers a cognitive shortcut-taking behavior in the deep reasoning processes\nof current LLMs, which we find accounts for 77% to 100% of errors on targeted\nproblems. Code and resources are available\nat:https://github.com/SYSUSELab/EvolMathEval."}
{"id": "2508.13020", "categories": ["cs.AI", "cs.AR"], "pdf": "https://arxiv.org/pdf/2508.13020", "abs": "https://arxiv.org/abs/2508.13020", "authors": ["Jiaqi Yin", "Zhan Song", "Chen Chen", "Yaohui Cai", "Zhiru Zhang", "Cunxi Yu"], "title": "e-boost: Boosted E-Graph Extraction with Adaptive Heuristics and Exact Solving", "comment": null, "summary": "E-graphs have attracted growing interest in many fields, particularly in\nlogic synthesis and formal verification. E-graph extraction is a challenging\nNP-hard combinatorial optimization problem. It requires identifying optimal\nterms from exponentially many equivalent expressions, serving as the primary\nperformance bottleneck in e-graph based optimization tasks. However,\ntraditional extraction methods face a critical trade-off: heuristic approaches\noffer speed but sacrifice optimality, while exact methods provide optimal\nsolutions but face prohibitive computational costs on practical problems. We\npresent e-boost, a novel framework that bridges this gap through three key\ninnovations: (1) parallelized heuristic extraction that leverages weak data\ndependence to compute DAG costs concurrently, enabling efficient multi-threaded\nperformance without sacrificing extraction quality; (2) adaptive search space\npruning that employs a parameterized threshold mechanism to retain only\npromising candidates, dramatically reducing the solution space while preserving\nnear-optimal solutions; and (3) initialized exact solving that formulates the\nreduced problem as an Integer Linear Program with warm-start capabilities,\nguiding solvers toward high-quality solutions faster.\n  Across the diverse benchmarks in formal verification and logic synthesis\nfields, e-boost demonstrates 558x runtime speedup over traditional exact\napproaches (ILP) and 19.04% performance improvement over the state-of-the-art\nextraction framework (SmoothE). In realistic logic synthesis tasks, e-boost\nproduces 7.6% and 8.1% area improvements compared to conventional synthesis\ntools with two different technology mapping libraries. e-boost is available at\nhttps://github.com/Yu-Maryland/e-boost."}
{"id": "2508.13021", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.13021", "abs": "https://arxiv.org/abs/2508.13021", "authors": ["Pengcheng Huang", "Shuhao Liu", "Zhenghao Liu", "Yukun Yan", "Shuo Wang", "Zulong Chen", "Tong Xiao"], "title": "PC-Sampler: Position-Aware Calibration of Decoding Bias in Masked Diffusion Models", "comment": "17 pages,13 figures", "summary": "Recent advances in masked diffusion models (MDMs) have established them as\npowerful non-autoregressive alternatives for sequence generation. Nevertheless,\nour preliminary experiments reveal that the generation quality of MDMs is still\nhighly sensitive to the choice of decoding strategy. In particular, widely\nadopted uncertainty-based samplers suffer from two key limitations: a lack of\nglobal trajectory control and a pronounced bias toward trivial tokens in the\nearly stages of decoding. These shortcomings restrict the full potential of\nMDMs. In this work, we introduce Position-Aware Confidence-Calibrated Sampling\n(PC-Sampler), a novel decoding strategy that unifies global trajectory planning\nwith content-aware informativeness maximization. PC-Sampler incorporates a\nposition-aware weighting mechanism to regulate the decoding path and a\ncalibrated confidence score to suppress the premature selection of trivial\ntokens. Extensive experiments on three advanced MDMs across seven challenging\nbenchmarks-including logical reasoning and planning tasks-demonstrate that\nPC-Sampler consistently outperforms existing MDM decoding strategies by more\nthan 10% on average, significantly narrowing the performance gap with\nstate-of-the-art autoregressive models. All codes are available at\nhttps://github.com/NEUIR/PC-Sampler."}
{"id": "2508.13023", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.13023", "abs": "https://arxiv.org/abs/2508.13023", "authors": ["Yongxin Guo", "Wenbo Deng", "Zhenglin Cheng", "Xiaoying Tang"], "title": "G$^2$RPO-A: Guided Group Relative Policy Optimization with Adaptive Guidance", "comment": null, "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has markedly enhanced\nthe reasoning abilities of large language models (LLMs). Its success, however,\nlargely depends on strong base models with rich world knowledge, yielding only\nmodest improvements for small-size language models (SLMs). To address this\nlimitation, we investigate Guided GRPO, which injects ground-truth reasoning\nsteps into roll-out trajectories to compensate for SLMs' inherent weaknesses.\nThrough a comprehensive study of various guidance configurations, we find that\nnaively adding guidance delivers limited gains. These insights motivate\nG$^2$RPO-A, an adaptive algorithm that automatically adjusts guidance strength\nin response to the model's evolving training dynamics. Experiments on\nmathematical reasoning and code-generation benchmarks confirm that G$^2$RPO-A\nsubstantially outperforms vanilla GRPO. Our code and models are available at\nhttps://github.com/T-Lab-CUHKSZ/G2RPO-A."}
{"id": "2508.13072", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.13072", "abs": "https://arxiv.org/abs/2508.13072", "authors": ["Yuting Zhang", "Tiantian Geng", "Luoying Hao", "Xinxing Cheng", "Alexander Thorley", "Xiaoxia Wang", "Wenqi Lu", "Sandeep S Hothi", "Lei Wei", "Zhaowen Qiu", "Dipak Kotecha", "Jinming Duan"], "title": "A Language-Signal-Vision Multimodal Framework for Multitask Cardiac Analysis", "comment": null, "summary": "Contemporary cardiovascular management involves complex consideration and\nintegration of multimodal cardiac datasets, where each modality provides\ndistinct but complementary physiological characteristics. While the effective\nintegration of multiple modalities could yield a holistic clinical profile that\naccurately models the true clinical situation with respect to data modalities\nand their relatives weightings, current methodologies remain limited by: 1) the\nscarcity of patient- and time-aligned multimodal data; 2) reliance on isolated\nsingle-modality or rigid multimodal input combinations; 3) alignment strategies\nthat prioritize cross-modal similarity over complementarity; and 4) a narrow\nsingle-task focus. In response to these limitations, a comprehensive multimodal\ndataset was curated for immediate application, integrating laboratory test\nresults, electrocardiograms, and echocardiograms with clinical outcomes.\nSubsequently, a unified framework, Textual Guidance Multimodal fusion for\nMultiple cardiac tasks (TGMM), was proposed. TGMM incorporated three key\ncomponents: 1) a MedFlexFusion module designed to capture the unique and\ncomplementary characteristics of medical modalities and dynamically integrate\ndata from diverse cardiac sources and their combinations; 2) a textual guidance\nmodule to derive task-relevant representations tailored to diverse clinical\nobjectives, including heart disease diagnosis, risk stratification and\ninformation retrieval; and 3) a response module to produce final decisions for\nall these tasks. Furthermore, this study systematically explored key features\nacross multiple modalities and elucidated their synergistic contributions in\nclinical decision-making. Extensive experiments showed that TGMM outperformed\nstate-of-the-art methods across multiple clinical tasks, with additional\nvalidation confirming its robustness on another public dataset."}
{"id": "2508.13121", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.13121", "abs": "https://arxiv.org/abs/2508.13121", "authors": ["Carlos Celemin"], "title": "Bayesian Optimization-based Search for Agent Control in Automated Game Testing", "comment": null, "summary": "This work introduces an automated testing approach that employs agents\ncontrolling game characters to detect potential bugs within a game level.\nHarnessing the power of Bayesian Optimization (BO) to execute sample-efficient\nsearch, the method determines the next sampling point by analyzing the data\ncollected so far and calculates the data point that will maximize information\nacquisition. To support the BO process, we introduce a game testing-specific\nmodel built on top of a grid map, that features the smoothness and uncertainty\nestimation required by BO, however and most importantly, it does not suffer the\nscalability issues that traditional models carry. The experiments demonstrate\nthat the approach significantly improves map coverage capabilities in both time\nefficiency and exploration distribution."}
{"id": "2508.13143", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.13143", "abs": "https://arxiv.org/abs/2508.13143", "authors": ["Ruofan Lu", "Yichen Li", "Yintong Huo"], "title": "Exploring Autonomous Agents: A Closer Look at Why They Fail When Completing Tasks", "comment": "Accepted by ASE 2025 NIER", "summary": "Autonomous agent systems powered by Large Language Models (LLMs) have\ndemonstrated promising capabilities in automating complex tasks. However,\ncurrent evaluations largely rely on success rates without systematically\nanalyzing the interactions, communication mechanisms, and failure causes within\nthese systems. To bridge this gap, we present a benchmark of 34 representative\nprogrammable tasks designed to rigorously assess autonomous agents. Using this\nbenchmark, we evaluate three popular open-source agent frameworks combined with\ntwo LLM backbones, observing a task completion rate of approximately 50%.\nThrough in-depth failure analysis, we develop a three-tier taxonomy of failure\ncauses aligned with task phases, highlighting planning errors, task execution\nissues, and incorrect response generation. Based on these insights, we propose\nactionable improvements to enhance agent planning and self-diagnosis\ncapabilities. Our failure taxonomy, together with mitigation advice, provides\nan empirical foundation for developing more robust and effective autonomous\nagent systems in the future."}
{"id": "2508.11710", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11710", "abs": "https://arxiv.org/abs/2508.11710", "authors": ["Hael Abdulhakim Ali Humran", "Ferdi Sonmez"], "title": "Code Vulnerability Detection Across Different Programming Languages with AI Models", "comment": null, "summary": "Security vulnerabilities present in a code that has been written in diverse\nprogramming languages are among the most critical yet complicated aspects of\nsource code to detect. Static analysis tools based on rule-based patterns\nusually do not work well at detecting the context-dependent bugs and lead to\nhigh false positive rates. Recent developments in artificial intelligence,\nspecifically the use of transformer-based models like CodeBERT and CodeLlama,\nprovide light to this problem, as they show potential in finding such flaws\nbetter. This paper presents the implementations of these models on various\ndatasets of code vulnerability, showing how off-the-shelf models can\nsuccessfully produce predictive capacity in models through dynamic fine-tuning\nof the models on vulnerable and safe code fragments. The methodology comprises\nthe gathering of the dataset, normalization of the language, fine-tuning of the\nmodel, and incorporation of ensemble learning and explainable AI. Experiments\nshow that a well-trained CodeBERT can be as good as or even better than some\nexisting static analyzers in terms of accuracy greater than 97%. Further study\nhas indicated that although language models can achieve close-to-perfect\nrecall, the precision can decrease. A solution to this is given by hybrid\nmodels and validation procedures, which will reduce false positives. According\nto the results, the AI-based solutions generalize to different programming\nlanguages and classes of vulnerability. Nevertheless, robustness,\ninterpretability, and deployment readiness are still being developed. The\nresults illustrate the probabilities that AI will enhance the trustworthiness\nin the usability and scalability of machine-learning-based detectors of\nvulnerabilities."}
{"id": "2508.11711", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11711", "abs": "https://arxiv.org/abs/2508.11711", "authors": ["Irash Perera", "Hiranya Abeyrathne", "Sanjeewa Malalgoda", "Arshardh Ifthikar"], "title": "Enhancing GraphQL Security by Detecting Malicious Queries Using Large Language Models, Sentence Transformers, and Convolutional Neural Networks", "comment": null, "summary": "GraphQL's flexibility, while beneficial for efficient data fetching,\nintroduces unique security vulnerabilities that traditional API security\nmechanisms often fail to address. Malicious GraphQL queries can exploit the\nlanguage's dynamic nature, leading to denial-of-service attacks, data\nexfiltration through injection, and other exploits. Existing solutions, such as\nstatic analysis, rate limiting, and general-purpose Web Application Firewalls,\noffer limited protection against sophisticated, context-aware attacks. This\npaper presents a novel, AI-driven approach for real-time detection of malicious\nGraphQL queries. Our method combines static analysis with machine learning\ntechniques, including Large Language Models (LLMs) for dynamic schema-based\nconfiguration, Sentence Transformers (SBERT and Doc2Vec) for contextual\nembedding of query payloads, and Convolutional Neural Networks (CNNs), Random\nForests, and Multilayer Perceptrons for classification. We detail the system\narchitecture, implementation strategies optimized for production environments\n(including ONNX Runtime optimization and parallel processing), and evaluate the\nperformance of our detection models and the overall system under load. Results\ndemonstrate high accuracy in detecting various threats, including SQL\ninjection, OS command injection, and XSS exploits, alongside effective\nmitigation of DoS and SSRF attempts. This research contributes a robust and\nadaptable solution for enhancing GraphQL API security."}
{"id": "2508.11715", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11715", "abs": "https://arxiv.org/abs/2508.11715", "authors": ["Ananya Singha", "Harshita Sahijwani", "Walt Williams", "Emmanuel Aboah Boateng", "Nick Hausman", "Miguel Di Luca", "Keegan Choudhury", "Chaya Binet", "Vu Le", "Tianwei Chen", "Oryan Rokeah Chen", "Sulaiman Vesal", "Sadid Hasan"], "title": "Benchmark Dataset Generation and Evaluation for Excel Formula Repair with LLMs", "comment": "Accepted at the KDD workshop on Evaluation and Trustworthiness of\n  Agentic and Generative AI Models", "summary": "Excel is a pervasive yet often complex tool, particularly for novice users,\nwhere runtime errors arising from logical mistakes or misinterpretations of\nfunctions pose a significant challenge. While large language models (LLMs)\noffer promising assistance by explaining formula errors, the automated\ncorrection of these semantic runtime errors remains an open problem. A primary\nchallenge to advancing models for such scenarios is the severe lack of\nhigh-quality, comprehensive datasets for training and rigorous evaluation. This\npaper addresses this gap by introducing a novel approach for constructing a\nbenchmark dataset specifically designed for Excel formula repair. We propose a\ndata generation pipeline, which leverages a small set of curated seed samples\nfrom online forums to synthetically expand the dataset. Our pipeline integrates\nfew-shot prompting with LLMs and employs a robust \\textit{LLM-as-a-Judge}\nvalidation framework, combined with execution-based checks to ensure the\ncorrectness and semantic fidelity of the generated data. This process produced\na benchmark dataset of 618 high-quality samples, covering common runtime\nerrors. Furthermore, we propose a context-aware baseline technique for Excel\nformula repair that utilizes LLMs to leverage both the faulty formula, and\nrelevant spreadsheet context. We evaluate the performance of various LLMs\n(GPT-4o, GPT-4.1, Phi-3, Mistral) on our newly generated benchmark using\nexecution-based metrics. Our analysis demonstrates the dataset's quality\nthrough manual annotation and provides insights into error and function\ndistributions. The proposed generation methodology is highly scalable and can\nbe readily adapted to create evaluation benchmarks for similar code repair\ntasks in other low-resource programming languages."}
{"id": "2508.11716", "categories": ["cs.CR", "cs.AI", "cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2508.11716", "abs": "https://arxiv.org/abs/2508.11716", "authors": ["Javier Muñoz-Haro", "Ruben Tolosana", "Ruben Vera-Rodriguez", "Aythami Morales", "Julian Fierrez"], "title": "Privacy-Aware Detection of Fake Identity Documents: Methodology, Benchmark, and Improved Detection Methods (FakeIDet2)", "comment": null, "summary": "Remote user verification in Internet-based applications is becoming\nincreasingly important nowadays. A popular scenario for it consists of\nsubmitting a picture of the user's Identity Document (ID) to a service\nplatform, authenticating its veracity, and then granting access to the\nrequested digital service. An ID is well-suited to verify the identity of an\nindividual, since it is government issued, unique, and nontransferable.\nHowever, with recent advances in Artificial Intelligence (AI), attackers can\nsurpass security measures in IDs and create very realistic physical and\nsynthetic fake IDs. Researchers are now trying to develop methods to detect an\never-growing number of these AI-based fakes that are almost indistinguishable\nfrom authentic (bona fide) IDs. In this counterattack effort, researchers are\nfaced with an important challenge: the difficulty in using real data to train\nfake ID detectors. This real data scarcity for research and development is\noriginated by the sensitive nature of these documents, which are usually kept\nprivate by the ID owners (the users) and the ID Holders (e.g., government,\npolice, bank, etc.). The main contributions of our study are: 1) We propose and\ndiscuss a patch-based methodology to preserve privacy in fake ID detection\nresearch. 2) We provide a new public database, FakeIDet2-db, comprising over\n900K real/fake ID patches extracted from 2,000 ID images, acquired using\ndifferent smartphone sensors, illumination and height conditions, etc. In\naddition, three physical attacks are considered: print, screen, and composite.\n3) We present a new privacy-aware fake ID detection method, FakeIDet2. 4) We\nrelease a standard reproducible benchmark that considers physical and synthetic\nattacks from popular databases in the literature."}
{"id": "2508.11824", "categories": ["cs.SE", "cs.AI", "cs.CR", "cs.PF"], "pdf": "https://arxiv.org/pdf/2508.11824", "abs": "https://arxiv.org/abs/2508.11824", "authors": ["Satyam Kumar Navneet", "Joydeep Chandra"], "title": "Rethinking Autonomy: Preventing Failures in AI-Driven Software Engineering", "comment": null, "summary": "The integration of Large Language Models (LLMs) into software engineering has\nrevolutionized code generation, enabling unprecedented productivity through\npromptware and autonomous AI agents. However, this transformation introduces\nsignificant risks, including insecure code generation, hallucinated outputs,\nirreversible actions, and a lack of transparency and accountability. Incidents\nlike the Replit database deletion underscore the urgent need for robust safety\nand governance mechanisms. This paper comprehensively analyzes the inherent\nchallenges of LLM-assisted code generation, such as vulnerability inheritance,\novertrust, misinterpretation, and the absence of standardized validation and\nrollback protocols. To address these, we propose the SAFE-AI Framework, a\nholistic approach emphasizing Safety, Auditability, Feedback, and\nExplainability. The framework integrates guardrails, sandboxing, runtime\nverification, risk-aware logging, human-in-the-loop systems, and explainable AI\ntechniques to mitigate risks while fostering trust and compliance. We introduce\na novel taxonomy of AI behaviors categorizing suggestive, generative,\nautonomous, and destructive actions to guide risk assessment and oversight.\nAdditionally, we identify open problems, including the lack of standardized\nbenchmarks for code specific hallucinations and autonomy levels, and propose\nfuture research directions for hybrid verification, semantic guardrails, and\nproactive governance tools. Through detailed comparisons of autonomy control,\nprompt engineering, explainability, and governance frameworks, this paper\nprovides a roadmap for responsible AI integration in software engineering,\naligning with emerging regulations like the EU AI Act and Canada's AIDA to\nensure safe, transparent, and accountable AI-driven development."}
{"id": "2508.11867", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11867", "abs": "https://arxiv.org/abs/2508.11867", "authors": ["Mohammad Baqar", "Saba Naqvi", "Rajat Khanda"], "title": "AI-Augmented CI/CD Pipelines: From Code Commit to Production with Autonomous Decisions", "comment": "13 Pages", "summary": "Modern software delivery has accelerated from quarterly releases to multiple\ndeployments per day. While CI/CD tooling has matured, human decision points\ninterpreting flaky tests, choosing rollback strategies, tuning feature flags,\nand deciding when to promote a canary remain major sources of latency and\noperational toil. We propose AI-Augmented CI/CD Pipelines, where large language\nmodels (LLMs) and autonomous agents act as policy-bounded co-pilots and\nprogressively as decision makers. We contribute: (1) a reference architecture\nfor embedding agentic decision points into CI/CD, (2) a decision taxonomy and\npolicy-as-code guardrail pattern, (3) a trust-tier framework for staged\nautonomy, (4) an evaluation methodology using DevOps Research and Assessment (\nDORA) metrics and AI-specific indicators, and (5) a detailed industrial-style\ncase study migrating a React 19 microservice to an AI-augmented pipeline. We\ndiscuss ethics, verification, auditability, and threats to validity, and chart\na roadmap for verifiable autonomy in production delivery systems."}
{"id": "2508.11907", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11907", "abs": "https://arxiv.org/abs/2508.11907", "authors": ["Xiaojin Zhang", "Mingcong Xu", "Yiming Li", "Wei Chen", "Qiang Yang"], "title": "Deciphering the Interplay between Attack and Protection Complexity in Privacy-Preserving Federated Learning", "comment": null, "summary": "Federated learning (FL) offers a promising paradigm for collaborative model\ntraining while preserving data privacy. However, its susceptibility to gradient\ninversion attacks poses a significant challenge, necessitating robust privacy\nprotection mechanisms. This paper introduces a novel theoretical framework to\ndecipher the intricate interplay between attack and protection complexities in\nprivacy-preserving FL. We formally define \"Attack Complexity\" as the minimum\ncomputational and data resources an adversary requires to reconstruct private\ndata below a given error threshold, and \"Protection Complexity\" as the expected\ndistortion introduced by privacy mechanisms. Leveraging Maximum Bayesian\nPrivacy (MBP), we derive tight theoretical bounds for protection complexity,\ndemonstrating its scaling with model dimensionality and privacy budget.\nFurthermore, we establish comprehensive bounds for attack complexity, revealing\nits dependence on privacy leakage, gradient distortion, model dimension, and\nthe chosen privacy level. Our findings quantitatively illuminate the\nfundamental trade-offs between privacy guarantees, system utility, and the\neffort required for both attacking and defending. This framework provides\ncritical insights for designing more secure and efficient federated learning\nsystems."}
{"id": "2508.12138", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12138", "abs": "https://arxiv.org/abs/2508.12138", "authors": ["Mohammad Ishzaz Asif Rafid", "Morsalin Sakib"], "title": "Substituting Proof of Work in Blockchain with Training-Verified Collaborative Model Computation", "comment": null, "summary": "Bitcoin's Proof of Work (PoW) mechanism, while central to achieving\ndecentralized consensus, has long been criticized for excessive energy use and\nhardware inefficiencies \\cite{devries2018bitcoin, truby2018decarbonizing}. This\npaper introduces a hybrid architecture that replaces Bitcoin's traditional PoW\nwith a centralized, cloud-based collaborative training framework. In this\nmodel, miners contribute computing resources to train segments of horizontally\nscaled machine learning models on preprocessed datasets, ensuring privacy and\ngenerating meaningful outputs \\cite{li2017securing}. A central server evaluates\ncontributions using two metrics: number of parameters trained and reduction in\nmodel loss during each cycle. At the end of every cycle, a weighted lottery\nselects the winning miner, who receives a digitally signed certificate. This\ncertificate serves as a verifiable substitute for PoW and grants the right to\nappend a block to the blockchain \\cite{nakamoto2008bitcoin}. By integrating\ndigital signatures and SHA-256 hashing \\cite{nist2015sha}, the system preserves\nblockchain integrity while redirecting energy toward productive computation.\nThe proposed approach addresses the sustainability concerns of traditional\nmining by converting resource expenditure into socially valuable work, aligning\nsecurity incentives with real-world computational progress."}
{"id": "2508.12232", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12232", "abs": "https://arxiv.org/abs/2508.12232", "authors": ["Arshia Akhavan", "Alireza Hosseinpour", "Abbas Heydarnoori", "Mehdi Keshani"], "title": "LinkAnchor: An Autonomous LLM-Based Agent for Issue-to-Commit Link Recovery", "comment": null, "summary": "Issue-to-commit link recovery plays an important role in software\ntraceability and improves project management. However, it remains a challenging\ntask. A study on GitHub shows that only 42.2% of the issues are correctly\nlinked to their commits. This highlights the potential for further development\nand research in this area. Existing studies have employed various AI/ML-based\napproaches, and with the recent development of large language models,\nresearchers have leveraged LLMs to tackle this problem. These approaches suffer\nfrom two main issues. First, LLMs are constrained by limited context windows\nand cannot ingest all of the available data sources, such as long commit\nhistories, extensive issue comments, and large code repositories. Second, most\nmethods operate on individual issue-commit pairs; that is, given a single\nissue-commit pair, they determine whether the commit resolves the issue. This\nquickly becomes impractical in real-world repositories containing tens of\nthousands of commits. To address these limitations, we present LinkAnchor, the\nfirst autonomous LLM-based agent designed for issue-to-commit link recovery.\nThe lazy-access architecture of LinkAnchor enables the underlying LLM to access\nthe rich context of software, spanning commits, issue comments, and code files,\nwithout exceeding the token limit by dynamically retrieving only the most\nrelevant contextual data. Additionally, LinkAnchor is able to automatically\npinpoint the target commit rather than exhaustively scoring every possible\ncandidate. Our evaluations show that LinkAnchor outperforms state-of-the-art\nissue-to-commit link recovery approaches by 60-262% in Hit@1 score across all\nour case study projects. We also publicly release LinkAnchor as a ready-to-use\ntool, along with our replication package. LinkAnchor is designed and tested for\nGitHub and Jira, and is easily extendable to other platforms."}
{"id": "2508.12259", "categories": ["cs.CR", "cs.AI", "cs.ET"], "pdf": "https://arxiv.org/pdf/2508.12259", "abs": "https://arxiv.org/abs/2508.12259", "authors": ["Ken Huang", "Yasir Mehmood", "Hammad Atta", "Jerry Huang", "Muhammad Zeeshan Baig", "Sree Bhargavi Balija"], "title": "Fortifying the Agentic Web: A Unified Zero-Trust Architecture Against Logic-layer Threats", "comment": null, "summary": "This paper presents a Unified Security Architecture that fortifies the\nAgentic Web through a Zero-Trust IAM framework. This architecture is built on a\nfoundation of rich, verifiable agent identities using Decentralized Identifiers\n(DIDs) and Verifiable Credentials (VCs), with discovery managed by a\nprotocol-agnostic Agent Name Service (ANS). Security is operationalized through\na multi-layered Trust Fabric which introduces significant innovations,\nincluding Trust-Adaptive Runtime Environments (TARE), Causal Chain Auditing,\nand Dynamic Identity with Behavioral Attestation. By explicitly linking the\nLPCI threat to these enhanced architectural countermeasures within a formal\nsecurity model, we propose a comprehensive and forward-looking blueprint for a\nsecure, resilient, and trustworthy agentic ecosystem. Our formal analysis\ndemonstrates that the proposed architecture provides provable security\nguarantees against LPCI attacks with bounded probability of success."}
{"id": "2508.12285", "categories": ["cs.SE", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.12285", "abs": "https://arxiv.org/abs/2508.12285", "authors": ["Yunbo Lyu", "Zhou Yang", "Jieke Shi", "Jianming Chang", "Yue Liu", "David Lo"], "title": "\"My productivity is boosted, but ...\" Demystifying Users' Perception on AI Coding Assistants", "comment": "13 pages, Camera-Ready Version that will appear in ASE 2025", "summary": "This paper aims to explore fundamental questions in the era when AI coding\nassistants like GitHub Copilot are widely adopted: what do developers truly\nvalue and criticize in AI coding assistants, and what does this reveal about\ntheir needs and expectations in real-world software development? Unlike\nprevious studies that conduct observational research in controlled and\nsimulated environments, we analyze extensive, first-hand user reviews of AI\ncoding assistants, which capture developers' authentic perspectives and\nexperiences drawn directly from their actual day-to-day work contexts. We\nidentify 1,085 AI coding assistants from the Visual Studio Code Marketplace.\nAlthough they only account for 1.64% of all extensions, we observe a surge in\nthese assistants: over 90% of them are released within the past two years. We\nthen manually analyze the user reviews sampled from 32 AI coding assistants\nthat have sufficient installations and reviews to construct a comprehensive\ntaxonomy of user concerns and feedback about these assistants. We manually\nannotate each review's attitude when mentioning certain aspects of coding\nassistants, yielding nuanced insights into user satisfaction and\ndissatisfaction regarding specific features, concerns, and overall tool\nperformance. Built on top of the findings-including how users demand not just\nintelligent suggestions but also context-aware, customizable, and\nresource-efficient interactions-we propose five practical implications and\nsuggestions to guide the enhancement of AI coding assistants that satisfy user\nneeds."}
{"id": "2508.12358", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12358", "abs": "https://arxiv.org/abs/2508.12358", "authors": ["Haolin Jin", "Huaming Chen"], "title": "Uncovering Systematic Failures of LLMs in Verifying Code Against Natural Language Specifications", "comment": "Accepted to the NIER track of the 40th IEEE/ACM International\n  Conference on Automated Software Engineering (ASE 2025)", "summary": "Large language models (LLMs) have become essential tools in software\ndevelopment, widely used for requirements engineering, code generation and\nreview tasks. Software engineers often rely on LLMs to assess whether system\ncode implementation satisfy task requirements, thereby enhancing code\nrobustness and accuracy. However, it remains unclear whether LLMs can reliably\ndetermine whether the code complies fully with the given task descriptions,\nwhich is usually natural language specifications. In this paper, we uncover a\nsystematic failure of LLMs in evaluating whether code aligns with natural\nlanguage requirements. Specifically, with widely used benchmarks, we employ\nunified prompts to judge code correctness. Our results reveal that LLMs\nfrequently misclassify correct code implementations as either ``not satisfying\nrequirements'' or containing potential defects. Surprisingly, more complex\nprompting, especially when leveraging prompt engineering techniques involving\nexplanations and proposed corrections, leads to higher misjudgment rate, which\nhighlights the critical reliability issues in using LLMs as code review\nassistants. We further analyze the root causes of these misjudgments, and\npropose two improved prompting strategies for mitigation. For the first time,\nour findings reveals unrecognized limitations in LLMs to match code with\nrequirements. We also offer novel insights and practical guidance for effective\nuse of LLMs in automated code review and task-oriented agent scenarios."}
{"id": "2508.12398", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.12398", "abs": "https://arxiv.org/abs/2508.12398", "authors": ["Zhixin Xie", "Xurui Song", "Jun Luo"], "title": "Where to Start Alignment? Diffusion Large Language Model May Demand a Distinct Position", "comment": null, "summary": "Diffusion Large Language Models (dLLMs) have recently emerged as a\ncompetitive non-autoregressive paradigm due to their unique training and\ninference approach. However, there is currently a lack of safety study on this\nnovel architecture. In this paper, we present the first analysis of dLLMs'\nsafety performance and propose a novel safety alignment method tailored to\ntheir unique generation characteristics. Specifically, we identify a critical\nasymmetry between the defender and attacker in terms of security. For the\ndefender, we reveal that the middle tokens of the response, rather than the\ninitial ones, are more critical to the overall safety of dLLM outputs; this\nseems to suggest that aligning middle tokens can be more beneficial to the\ndefender. The attacker, on the contrary, may have limited power to manipulate\nmiddle tokens, as we find dLLMs have a strong tendency towards a sequential\ngeneration order in practice, forcing the attack to meet this distribution and\ndiverting it from influencing the critical middle tokens. Building on this\nasymmetry, we introduce Middle-tOken Safety Alignment (MOSA), a novel method\nthat directly aligns the model's middle generation with safe refusals\nexploiting reinforcement learning. We implement MOSA and compare its security\nperformance against eight attack methods on two benchmarks. We also test the\nutility of MOSA-aligned dLLM on coding, math, and general reasoning. The\nresults strongly prove the superiority of MOSA."}
{"id": "2508.12412", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12412", "abs": "https://arxiv.org/abs/2508.12412", "authors": ["Ron Solomon", "Yarin Yerushalmi Levi", "Lior Vaknin", "Eran Aizikovich", "Amit Baras", "Etai Ohana", "Amit Giloni", "Shamik Bose", "Chiara Picardi", "Yuval Elovici", "Asaf Shabtai"], "title": "LumiMAS: A Comprehensive Framework for Real-Time Monitoring and Enhanced Observability in Multi-Agent Systems", "comment": null, "summary": "The incorporation of large language models in multi-agent systems (MASs) has\nthe potential to significantly improve our ability to autonomously solve\ncomplex problems. However, such systems introduce unique challenges in\nmonitoring, interpreting, and detecting system failures. Most existing MAS\nobservability frameworks focus on analyzing each individual agent separately,\noverlooking failures associated with the entire MAS. To bridge this gap, we\npropose LumiMAS, a novel MAS observability framework that incorporates advanced\nanalytics and monitoring techniques. The proposed framework consists of three\nkey components: a monitoring and logging layer, anomaly detection layer, and\nanomaly explanation layer. LumiMAS's first layer monitors MAS executions,\ncreating detailed logs of the agents' activity. These logs serve as input to\nthe anomaly detection layer, which detects anomalies across the MAS workflow in\nreal time. Then, the anomaly explanation layer performs classification and root\ncause analysis (RCA) of the detected anomalies. LumiMAS was evaluated on seven\ndifferent MAS applications, implemented using two popular MAS platforms, and a\ndiverse set of possible failures. The applications include two novel\nfailure-tailored applications that illustrate the effects of a hallucination or\nbias on the MAS. The evaluation results demonstrate LumiMAS's effectiveness in\nfailure detection, classification, and RCA."}
{"id": "2508.12470", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12470", "abs": "https://arxiv.org/abs/2508.12470", "authors": ["Afrah Gueriani", "Hamza Kheddar", "Ahmed Cherif Mazari", "Mohamed Chahine Ghanem"], "title": "A Robust Cross-Domain IDS using BiGRU-LSTM-Attention for Medical and Industrial IoT Security", "comment": "10 pages", "summary": "The increased Internet of Medical Things IoMT and the Industrial Internet of\nThings IIoT interconnectivity has introduced complex cybersecurity challenges,\nexposing sensitive data, patient safety, and industrial operations to advanced\ncyber threats. To mitigate these risks, this paper introduces a novel\ntransformer-based intrusion detection system IDS, termed BiGAT-ID a hybrid\nmodel that combines bidirectional gated recurrent units BiGRU, long short-term\nmemory LSTM networks, and multi-head attention MHA. The proposed architecture\nis designed to effectively capture bidirectional temporal dependencies, model\nsequential patterns, and enhance contextual feature representation. Extensive\nexperiments on two benchmark datasets, CICIoMT2024 medical IoT and EdgeIIoTset\nindustrial IoT demonstrate the model's cross-domain robustness, achieving\ndetection accuracies of 99.13 percent and 99.34 percent, respectively.\nAdditionally, the model exhibits exceptional runtime efficiency, with inference\ntimes as low as 0.0002 seconds per instance in IoMT and 0.0001 seconds in IIoT\nscenarios. Coupled with a low false positive rate, BiGAT-ID proves to be a\nreliable and efficient IDS for deployment in real-world heterogeneous IoT\nenvironments"}
{"id": "2508.12538", "categories": ["cs.CR", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.12538", "abs": "https://arxiv.org/abs/2508.12538", "authors": ["Yongjian Guo", "Puzhuo Liu", "Wanlun Ma", "Zehang Deng", "Xiaogang Zhu", "Peng Di", "Xi Xiao", "Sheng Wen"], "title": "Systematic Analysis of MCP Security", "comment": null, "summary": "The Model Context Protocol (MCP) has emerged as a universal standard that\nenables AI agents to seamlessly connect with external tools, significantly\nenhancing their functionality. However, while MCP brings notable benefits, it\nalso introduces significant vulnerabilities, such as Tool Poisoning Attacks\n(TPA), where hidden malicious instructions exploit the sycophancy of large\nlanguage models (LLMs) to manipulate agent behavior. Despite these risks,\ncurrent academic research on MCP security remains limited, with most studies\nfocusing on narrow or qualitative analyses that fail to capture the diversity\nof real-world threats. To address this gap, we present the MCP Attack Library\n(MCPLIB), which categorizes and implements 31 distinct attack methods under\nfour key classifications: direct tool injection, indirect tool injection,\nmalicious user attacks, and LLM inherent attack. We further conduct a\nquantitative analysis of the efficacy of each attack. Our experiments reveal\nkey insights into MCP vulnerabilities, including agents' blind reliance on tool\ndescriptions, sensitivity to file-based attacks, chain attacks exploiting\nshared context, and difficulty distinguishing external data from executable\ncommands. These insights, validated through attack experiments, underscore the\nurgency for robust defense strategies and informed MCP design. Our\ncontributions include 1) constructing a comprehensive MCP attack taxonomy, 2)\nintroducing a unified attack framework MCPLIB, and 3) conducting empirical\nvulnerability analysis to enhance MCP security mechanisms. This work provides a\nfoundational framework, supporting the secure evolution of MCP ecosystems."}
{"id": "2508.12910", "categories": ["cs.CR", "cs.AI", "cs.AR"], "pdf": "https://arxiv.org/pdf/2508.12910", "abs": "https://arxiv.org/abs/2508.12910", "authors": ["Ziteng Hu", "Yingjie Xia", "Xiyuan Chen", "Li Kuang"], "title": "SecFSM: Knowledge Graph-Guided Verilog Code Generation for Secure Finite State Machines in Systems-on-Chip", "comment": null, "summary": "Finite State Machines (FSMs) play a critical role in implementing control\nlogic for Systems-on-Chip (SoC). Traditionally, FSMs are implemented by\nhardware engineers through Verilog coding, which is often tedious and\ntime-consuming. Recently, with the remarkable progress of Large Language Models\n(LLMs) in code generation, LLMs have been increasingly explored for automating\nVerilog code generation. However, LLM-generated Verilog code often suffers from\nsecurity vulnerabilities, which is particularly concerning for\nsecurity-sensitive FSM implementations. To address this issue, we propose\nSecFSM, a novel method that leverages a security-oriented knowledge graph to\nguide LLMs in generating more secure Verilog code. Specifically, we first\nconstruct a FSM Security Knowledge Graph (FSKG) as an external aid to LLMs.\nSubsequently, we analyze users' requirements to identify vulnerabilities and\nget a list of vulnerabilities in the requirements. Then, we retrieve knowledge\nfrom FSKG based on the vulnerabilities list. Finally, we construct security\nprompts based on the security knowledge for Verilog code generation. To\nevaluate SecFSM, we build a dedicated dataset collected from academic datasets,\nartificial datasets, papers, and industrial cases. Extensive experiments\ndemonstrate that SecFSM outperforms state-of-the-art baselines. In particular,\non a benchmark of 25 security test cases evaluated by DeepSeek-R1, SecFSM\nachieves an outstanding pass rate of 21/25."}
{"id": "2508.13092", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.13092", "abs": "https://arxiv.org/abs/2508.13092", "authors": ["Xiang Long", "Yingjie Xia", "Xiyuan Chen", "Li Kuang"], "title": "VerilogLAVD: LLM-Aided Rule Generation for Vulnerability Detection in Verilog", "comment": null, "summary": "Timely detection of hardware vulnerabilities during the early design stage is\ncritical for reducing remediation costs. Existing early detection techniques\noften require specialized security expertise, limiting their usability. Recent\nefforts have explored the use of large language models (LLMs) for Verilog\nvulnerability detection. However, LLMs struggle to capture the structure in\nVerilog code, resulting in inconsistent detection results. To this end, we\npropose VerilogLAVD, the first LLM-aided graph traversal rule generation\napproach for Verilog vulnerability detection. Our approach introduces the\nVerilog Property Graph (VeriPG), a unified representation of Verilog code. It\ncombines syntactic features extracted from the abstract syntax tree (AST) with\nsemantic information derived from control flow and data dependency graphs. We\nleverage LLMs to generate VeriPG-based detection rules from Common Weakness\nEnumeration (CWE) descriptions. These rules guide the rule executor that\ntraversal VeriPG for potential vulnerabilities. To evaluate VerilogLAVD, we\nbuild a dataset collected from open-source repositories and synthesized data.\nIn our empirical evaluation on 77 Verilog designs encompassing 12 CWE types,\nVerilogLAVD achieves an F1-score of 0.54. Compared to the LLM-only and LLM with\nexternal knowledge baselines, VerilogLAVD improves F1-score by 0.31 and 0.27,\nrespectively."}
