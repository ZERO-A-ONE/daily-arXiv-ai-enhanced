<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 19]
- [cs.MA](#cs.MA) [Total: 3]
- [cs.SE](#cs.SE) [Total: 15]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Adversarial Text Generation with Dynamic Contextual Perturbation](https://arxiv.org/abs/2506.09148)
*Hetvi Waghela,Jaydip Sen,Sneha Rakshit,Subhasis Dasgupta*

Main category: cs.CR

TL;DR: 提出了一种名为动态上下文扰动（DCP）的新型对抗文本攻击方法，通过动态生成上下文感知的扰动，确保语义保真和流畅性，显著提升了对抗攻击的效果。


<details>
  <summary>Details</summary>
Motivation: 现有对抗攻击方法通常局限于单词或局部文本段落的修改，忽略了更广泛的上下文，导致扰动容易被检测或语义不一致。

Method: 利用预训练语言模型，通过对抗目标函数迭代优化扰动，平衡模型误导和文本自然性。

Result: 实验结果表明，DCP能有效挑战最先进NLP系统的鲁棒性，生成更自然的对抗样本。

Conclusion: 研究强调了上下文在对抗攻击中的关键作用，为构建更鲁棒的NLP系统奠定了基础。

Abstract: Adversarial attacks on Natural Language Processing (NLP) models expose
vulnerabilities by introducing subtle perturbations to input text, often
leading to misclassification while maintaining human readability. Existing
methods typically focus on word-level or local text segment alterations,
overlooking the broader context, which results in detectable or semantically
inconsistent perturbations. We propose a novel adversarial text attack scheme
named Dynamic Contextual Perturbation (DCP). DCP dynamically generates
context-aware perturbations across sentences, paragraphs, and documents,
ensuring semantic fidelity and fluency. Leveraging the capabilities of
pre-trained language models, DCP iteratively refines perturbations through an
adversarial objective function that balances the dual objectives of inducing
model misclassification and preserving the naturalness of the text. This
comprehensive approach allows DCP to produce more sophisticated and effective
adversarial examples that better mimic natural language patterns. Our
experimental results, conducted on various NLP models and datasets, demonstrate
the efficacy of DCP in challenging the robustness of state-of-the-art NLP
systems. By integrating dynamic contextual analysis, DCP significantly enhances
the subtlety and impact of adversarial attacks. This study highlights the
critical role of context in adversarial attacks and lays the groundwork for
creating more robust NLP systems capable of withstanding sophisticated
adversarial strategies.

</details>


### [2] [What is the Cost of Differential Privacy for Deep Learning-Based Trajectory Generation?](https://arxiv.org/abs/2506.09312)
*Erik Buchholz,Natasha Fernandes,David D. Nguyen,Alsharif Abuadbba,Surya Nepal,Salil S. Kanhere*

Main category: cs.CR

TL;DR: 论文研究了在生成合成轨迹时如何平衡差分隐私（DP）与数据效用，探讨了DP-SGD对生成模型的影响，并提出了一种新的条件生成DP机制。


<details>
  <summary>Details</summary>
Motivation: 轨迹数据包含敏感信息，现有生成模型缺乏正式隐私保护，研究旨在评估DP对模型效用的影响并提出改进方法。

Method: 使用DP-SGD评估生成模型（Diffusion、VAE、GAN）的效用，提出新的条件生成DP机制，并在两个数据集和11个效用指标上验证。

Result: DP-SGD显著影响性能，但大数据集下仍保留部分效用；新机制提升训练稳定性，GAN在DP-SGD下表现最佳。

Conclusion: DP轨迹生成仍具挑战性，正式隐私保护仅在大数据集和受限场景下可行。

Abstract: While location trajectories offer valuable insights, they also reveal
sensitive personal information. Differential Privacy (DP) offers formal
protection, but achieving a favourable utility-privacy trade-off remains
challenging. Recent works explore deep learning-based generative models to
produce synthetic trajectories. However, current models lack formal privacy
guarantees and rely on conditional information derived from real data during
generation. This work investigates the utility cost of enforcing DP in such
models, addressing three research questions across two datasets and eleven
utility metrics. (1) We evaluate how DP-SGD, the standard DP training method
for deep learning, affects the utility of state-of-the-art generative models.
(2) Since DP-SGD is limited to unconditional models, we propose a novel DP
mechanism for conditional generation that provides formal guarantees and assess
its impact on utility. (3) We analyse how model types - Diffusion, VAE, and GAN
- affect the utility-privacy trade-off. Our results show that DP-SGD
significantly impacts performance, although some utility remains if the
datasets is sufficiently large. The proposed DP mechanism improves training
stability, particularly when combined with DP-SGD, for unstable models such as
GANs and on smaller datasets. Diffusion models yield the best utility without
guarantees, but with DP-SGD, GANs perform best, indicating that the best
non-private model is not necessarily optimal when targeting formal guarantees.
In conclusion, DP trajectory generation remains a challenging task, and formal
guarantees are currently only feasible with large datasets and in constrained
use cases.

</details>


### [3] [DAVSP: Safety Alignment for Large Vision-Language Models via Deep Aligned Visual Safety Prompt](https://arxiv.org/abs/2506.09353)
*Yitong Zhang,Jia Li,Liyi Cai,Ge Li*

Main category: cs.CR

TL;DR: DAVSP是一种针对大型视觉语言模型（LVLMs）的安全对齐方法，通过视觉安全提示和深度对齐技术，有效抵御恶意查询并保持良性输入的实用性。


<details>
  <summary>Details</summary>
Motivation: 现有对齐方法难以同时抵御恶意查询并保持良性输入的实用性，DAVSP旨在解决这一问题。

Method: DAVSP通过视觉安全提示（在输入图像周围添加可训练填充区域）和深度对齐（在模型激活空间中进行监督训练）实现目标。

Result: 在五个基准测试中，DAVSP成功抵御恶意查询并保持良性输入实用性，且具有跨模型泛化能力。

Conclusion: DAVSP的视觉安全提示和深度对齐是其有效性的关键，代码已开源。

Abstract: Large Vision-Language Models (LVLMs) have achieved impressive progress across
various applications but remain vulnerable to malicious queries that exploit
the visual modality. Existing alignment approaches typically fail to resist
malicious queries while preserving utility on benign ones effectively. To
address these challenges, we propose Deep Aligned Visual Safety Prompt (DAVSP),
which is built upon two key innovations. First, we introduce the Visual Safety
Prompt, which appends a trainable padding region around the input image. It
preserves visual features and expands the optimization space. Second, we
propose Deep Alignment, a novel approach to train the visual safety prompt
through supervision in the model's activation space. It enhances the inherent
ability of LVLMs to perceive malicious queries, achieving deeper alignment than
prior works. Extensive experiments across five benchmarks on two representative
LVLMs demonstrate that DAVSP effectively resists malicious queries while
preserving benign input utility. Furthermore, DAVSP exhibits great cross-model
generation ability. Ablation studies further reveal that both the Visual Safety
Prompt and Deep Alignment are essential components, jointly contributing to its
overall effectiveness. The code is publicly available at
https://github.com/zhangyitonggg/DAVSP.

</details>


### [4] [ContextBuddy: AI-Enhanced Contextual Insights for Security Alert Investigation (Applied to Intrusion Detection)](https://arxiv.org/abs/2506.09365)
*Ronal Singh,Mohan Baruwal Chhetri,Surya Nepal,Cecile Paris*

Main category: cs.CR

TL;DR: ContextBuddy是一个AI助手，通过学习分析师的历史调查数据，帮助识别新警报的最相关上下文，显著提高了警报验证的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现代SOC整合了多种工具，但分析师仍需手动确定上下文的关联性，ContextBuddy旨在通过学习分析师的行为模式，自动化这一过程。

Method: 将上下文选择建模为顺序决策问题，应用模仿学习（IL）捕捉分析师策略，并在两个入侵检测数据集（HIKARI-2021、UNSW-NB15）上进行评估。

Result: 实验显示，ContextBuddy显著提高了分类准确性（F1提升2.5%-9%），减少了假阴性，假阳性保持在1%以下。用户研究中，非专家的分类准确性提高了21.1%，验证时间减少了24%。

Conclusion: ContextBuddy通过学习分析师的上下文选择模式，显著提升了调查的效率和效果。

Abstract: Modern Security Operations Centres (SOCs) integrate diverse tools, such as
SIEM, IDS, and XDR systems, offering rich contextual data, including alert
enrichments, flow features, and similar case histories. Yet, analysts must
still manually determine which of these contextual cues are most relevant when
validating specific alerts. We introduce ContextBuddy, an AI assistant that
learns from analysts' prior investigations to help them identify the most
relevant context for new alerts. Rather than providing enrichments,
ContextBuddy models how analysts have previously selected context and suggests
tailored cues based on the characteristics of each alert. We formulate context
selection as a sequential decision-making problem and apply imitation learning
(IL) to capture analysts' strategies, evaluating multiple IL approaches.
Through staged evaluation, we validate ContextBuddy using two intrusion
detection datasets (HIKARI-2021, UNSW-NB15). In simulation-based experiments,
ContextBuddy helped simulated reinforcement learning analysts improve
classification accuracy (p < 0.001) (increasing F1 by 2.5% for HIKARI and 9%
for UNSW), reducing false negatives (1.5% for HIKARI and 10% for UNSW), and
keeping false positives below 1%. Decision confidence among agents also
improved by 2-3% (p < 0.001). In a within-subject user study (N=13; power =
0.8), non-experts using ContextBuddy improved classification accuracy by 21.1%
(p = 0.008) and reduced alert validation time by 24% (p = 0.01). These results
demonstrate that by learning context-selection patterns from analysts,
ContextBuddy can yield notable improvements in investigation effectiveness and
efficiency.

</details>


### [5] [Epass: Efficient and Privacy-Preserving Asynchronous Payment on Blockchain](https://arxiv.org/abs/2506.09387)
*Weijie Wang,Jinwen Liang,Chuan Zhang,Ximeng Liu,Liehuang Zhu,Song Guo*

Main category: cs.CR

TL;DR: 论文提出了一种高效且保护隐私的区块链异步支付方案（Epass），解决了BNPL平台中交易透明性导致的隐私问题和时间开销问题。


<details>
  <summary>Details</summary>
Motivation: BNPL平台结合区块链技术时，交易透明性可能泄露用户财务隐私，且异步支付增加了时间开销，影响了服务的可扩展性。

Method: Epass利用本地可验证签名保护交易隐私，并通过时间释放加密技术控制可编辑区块链的陷阱门，减少时间开销。

Result: Epass实现了KB级通信成本，时间开销比现有方案减少四倍以上。

Conclusion: Epass在保护隐私的同时显著提升了BNPL服务的可扩展性。

Abstract: Buy Now Pay Later (BNPL) is a rapidly proliferating e-commerce model,
offering consumers to get the product immediately and defer payments.
Meanwhile, emerging blockchain technologies endow BNPL platforms with digital
currency transactions, allowing BNPL platforms to integrate with digital
wallets. However, the transparency of transactions causes critical privacy
concerns because malicious participants may derive consumers' financial
statuses from on-chain asynchronous payments. Furthermore, the newly created
transactions for deferred payments introduce additional time overheads, which
weaken the scalability of BNPL services. To address these issues, we propose an
efficient and privacy-preserving blockchain-based asynchronous payment scheme
(Epass), which has promising scalability while protecting the privacy of
on-chain consumer transactions. Specifically, Epass leverages locally
verifiable signatures to guarantee the privacy of consumer transactions against
malicious acts. Then, a privacy-preserving asynchronous payment scheme can be
further constructed by leveraging time-release encryption to control trapdoors
of redactable blockchain, reducing time overheads by modifying transactions for
deferred payment. We give formal definitions and security models, generic
structures, and formal proofs for Epass. Extensive comparisons and experimental
analysis show that \textsf{Epass} achieves KB-level communication costs, and
reduces time overhead by more than four times in comparisons with locally
verifiable signatures and Go-Ethereum private test networks.

</details>


### [6] [Securing Open RAN: A Survey of Cryptographic Challenges and Emerging Solutions for 5G](https://arxiv.org/abs/2506.09418)
*Ryan Barker,Fatemeh Afghah*

Main category: cs.CR

TL;DR: 本文综述了开放无线接入网（O-RAN）在5G部署中的安全挑战，分析了密码学工具的性能，并探讨了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: O-RAN的模块化和灵活性为5G部署带来新机遇，但也引入了跨解耦接口的安全问题。

Method: 通过综合13篇学术和行业文献，分析了O-RAN接口的漏洞（如密码降级攻击、部分加密暴露）和密码学工具（SNOW-V、AES-256、ZUC-256）的性能。

Result: 研究发现，AI驱动的控制器和新兴测试平台有助于动态编排和异常检测，但需进一步研究硬件卸载和跨层密码适应。

Conclusion: 未来研究应聚焦于集成零信任架构，以应对6G的安全需求。

Abstract: The advent of Open Radio Access Networks (O-RAN) introduces modularity and
flexibility into 5G deployments but also surfaces novel security challenges
across disaggregated interfaces. This literature review synthesizes recent
research across thirteen academic and industry sources, examining
vulnerabilities such as cipher bidding-down attacks, partial encryption
exposure on control/user planes, and performance trade-offs in securing O-RAN
interfaces like E2 and O1. The paper surveys key cryptographic tools -- SNOW-V,
AES-256, and ZUC-256 -- evaluating their throughput, side-channel resilience,
and adaptability to heterogeneous slices (eMBB, URLLC, mMTC). Emphasis is
placed on emerging testbeds and AI-driven controllers that facilitate dynamic
orchestration, anomaly detection, and secure configuration. We conclude by
outlining future research directions, including hardware offloading,
cross-layer cipher adaptation, and alignment with 3GPP TS 33.501 and O-RAN
Alliance security mandates, all of which point toward the need for integrated,
zero-trust architectures in 6G.

</details>


### [7] [LLMs Cannot Reliably Judge (Yet?): A Comprehensive Assessment on the Robustness of LLM-as-a-Judge](https://arxiv.org/abs/2506.09443)
*Songze Li,Chuokun Xu,Jiaying Wang,Xueluan Gong,Chen Chen,Jirui Zhang,Jun Wang,Kwok-Yan Lam,Shouling Ji*

Main category: cs.CR

TL;DR: RobustJudge是一个自动化框架，用于评估LLM-as-a-Judge系统的鲁棒性，发现其易受攻击，但通过优化提示模板和模型选择可提升防御能力。


<details>
  <summary>Details</summary>
Motivation: LLM-as-a-Judge系统在自动测试中广泛应用，但其鲁棒性和信任度因对抗攻击而受质疑，缺乏统一评估框架。

Method: 提出RobustJudge框架，系统研究攻击方法、防御策略（RQ1）、提示模板和模型选择的影响（RQ2），以及实际应用中的鲁棒性（RQ3）。

Result: 发现LLM-as-a-Judge系统易受攻击，但防御机制有效；提示模板和模型选择对鲁棒性影响显著；在PAI平台中发现新漏洞。

Conclusion: RobustJudge为LLM-as-a-Judge系统提供了全面的鲁棒性评估方法，揭示了改进方向。

Abstract: Large Language Models (LLMs) have demonstrated remarkable intelligence across
various tasks, which has inspired the development and widespread adoption of
LLM-as-a-Judge systems for automated model testing, such as red teaming and
benchmarking. However, these systems are susceptible to adversarial attacks
that can manipulate evaluation outcomes, raising concerns about their
robustness and, consequently, their trustworthiness. Existing evaluation
methods adopted by LLM-based judges are often piecemeal and lack a unified
framework for comprehensive assessment. Furthermore, prompt template and model
selections for improving judge robustness have been rarely explored, and their
performance in real-world settings remains largely unverified. To address these
gaps, we introduce RobustJudge, a fully automated and scalable framework
designed to systematically evaluate the robustness of LLM-as-a-Judge systems.
RobustJudge investigates the impact of attack methods and defense strategies
(RQ1), explores the influence of prompt template and model selection (RQ2), and
assesses the robustness of real-world LLM-as-a-Judge applications (RQ3).Our
main findings are: (1) LLM-as-a-Judge systems are still vulnerable to a range
of adversarial attacks, including Combined Attack and PAIR, while defense
mechanisms such as Re-tokenization and LLM-based Detectors offer improved
protection; (2) Robustness is highly sensitive to the choice of prompt template
and judge models. Our proposed prompt template optimization method can improve
robustness, and JudgeLM-13B demonstrates strong performance as a robust
open-source judge; (3) Applying RobustJudge to Alibaba's PAI platform reveals
previously unreported vulnerabilities. The source code of RobustJudge is
provided at https://github.com/S3IC-Lab/RobustJudge.

</details>


### [8] [Efficient Modular Multiplier over GF (2^m) for ECPM](https://arxiv.org/abs/2506.09464)
*Ruby Kumari,Gaurav Purohit,Abhijit Karmakar*

Main category: cs.CR

TL;DR: 该论文提出了一种用于二进制域GF(2^m)的混合乘法硬件实现，结合传统乘法（CM）和Karatsuba乘法（KM），优化了椭圆曲线点乘法（ECPM），显著提高了效率和资源利用率。


<details>
  <summary>Details</summary>
Motivation: 椭圆曲线密码学（ECC）已成为主流公钥协议，但传统方法在计算复杂度和资源利用上存在不足。本文旨在通过混合乘法技术优化ECC系统的性能。

Method: 设计采用CM处理较小操作数（如41位以下），KM处理较大操作数，以减少计算复杂度并提升效率。针对NIST B-163、233、283和571参数进行了实现。

Result: 实验显示，混合设计显著减少了资源占用（如LUT减少39.82%至70.70%），延迟降低37.60%，面积延迟积（ADP）优于传统方法。

Conclusion: 混合乘法技术在ECC系统中显著提升了速度、硬件效率和资源利用率，为密码学系统提供了更优的解决方案。

Abstract: Elliptic curve cryptography (ECC) has emerged as the dominant public-key
protocol, with NIST standardizing parameters for binary field GF(2^m) ECC
systems. This work presents a hardware implementation of a Hybrid
Multiplication technique for modular multiplication over binary field GF(2m),
targeting NIST B-163, 233, 283, and 571 parameters. The design optimizes the
combination of conventional multiplication (CM) and Karatsuba multiplication
(KM) to enhance elliptic curve point multiplication (ECPM). The key innovation
uses CM for smaller operands (up to 41 bits for m=163) and KM for larger ones,
reducing computational complexity and enhancing efficiency. The design is
evaluated in three areas: Resource Utilization For m=163, the hybrid design
uses 6,812 LUTs, a 39.82% reduction compared to conventional methods. For
m=233, LUT usage reduces by 45.53% and 70.70% compared to overlap-free and
bit-parallel implementations. Delay Performance For m=163, achieves 13.31ns
delay, improving by 37.60% over bit-parallel implementations. For m=233,
maintains 13.39ns delay. Area-Delay Product For m=163, achieves ADP of 90,860,
outperforming bit-parallel (75,337) and digit-serial (43,179) implementations.
For m=233, demonstrates 16.86% improvement over overlap-free and 96.10% over
bit-parallel designs. Results show the hybrid technique significantly improves
speed, hardware efficiency, and resource utilization for ECC cryptographic
systems.

</details>


### [9] [The Secure Overview and Analysis OF 3GPP MAC CE](https://arxiv.org/abs/2506.09502)
*Jin Cao,Yuanyuan Yang,Ruhui Ma,Sheng Li,Hui Li*

Main category: cs.CR

TL;DR: 本文探讨了MAC CE的安全威胁及保护机制，旨在提升通信系统的安全性和可靠性。


<details>
  <summary>Details</summary>
Motivation: MAC CE缺乏PDCP提供的加密和完整性保护机制，易受攻击，3GPP已分析LTM的安全风险，但其他协议可能也存在漏洞。

Method: 研究MAC CE的安全威胁并提出相应的保护机制。

Result: 研究结果将支持3GPP对MAC CE的研究，并与底层协议的安全研究结合。

Conclusion: 通过研究MAC CE的安全问题及保护机制，可增强整个通信系统的安全性和可靠性。

Abstract: To more effectively control and allocate network resources, MAC CE has been
introduced into the network protocol, which is a type of control signaling
located in the MAC layer. Since MAC CE lacks encryption and integrity
protection mechanisms provided by PDCP, the control signaling carried by MAC CE
is vulnerable to interception or tampering by attackers during resource
scheduling and allocation. Currently, the 3GPP has analyzed the security risks
of Layer 1/Layer 2 Triggered Mobility (LTM), where handover signaling sent to
the UE via MAC CE by the network can lead to privacy leaks and network attacks.
However, in addition to LTM, there may be other potential security
vulnerabilities in other protocol procedures. Therefore, this paper explores
the security threats to MAC CE and the corresponding protection mechanisms. The
research is expected to support the 3GPP's study of MAC CE and be integrated
with the security research of lower-layer protocols, thereby enhancing the
security and reliability of the entire communication system.

</details>


### [10] [Beyond Personalization: Federated Recommendation with Calibration via Low-rank Decomposition](https://arxiv.org/abs/2506.09525)
*Jundong Chen,Honglei Zhang,Haoxuan Li,Chunxu Zhang,Zhiwei Li,Yidong Li*

Main category: cs.CR

TL;DR: PFedCLR通过低秩分解校准用户嵌入和个性化全局项目嵌入，解决了联邦推荐中的用户嵌入偏差问题，提升了性能、效率和隐私保护。


<details>
  <summary>Details</summary>
Motivation: 联邦推荐（FR）需要保护用户隐私，但全局聚合的项目嵌入会导致用户嵌入偏差，影响性能。

Method: 提出PFedCLR方法，通过低秩分解的双功能机制校准用户嵌入和个性化项目嵌入，同时确保效率和隐私。

Result: 实验表明PFedCLR有效缓解用户嵌入偏差，在性能、效率和隐私方面优于现有方法。

Conclusion: PFedCLR为联邦推荐提供了一种高效、隐私保护的解决方案，显著提升了推荐性能。

Abstract: Federated recommendation (FR) is a promising paradigm to protect user privacy
in recommender systems. Distinct from general federated scenarios, FR
inherently needs to preserve client-specific parameters, i.e., user embeddings,
for privacy and personalization. However, we empirically find that globally
aggregated item embeddings can induce skew in user embeddings, resulting in
suboptimal performance. To this end, we theoretically analyze the user
embedding skew issue and propose Personalized Federated recommendation with
Calibration via Low-Rank decomposition (PFedCLR). Specifically, PFedCLR
introduces an integrated dual-function mechanism, implemented with a buffer
matrix, to jointly calibrate local user embedding and personalize global item
embeddings. To ensure efficiency, we employ a low-rank decomposition of the
buffer matrix to reduce the model overhead. Furthermore, for privacy, we train
and upload the local model before personalization, preventing the server from
accessing sensitive information. Extensive experiments demonstrate that PFedCLR
effectively mitigates user embedding skew and achieves a desirable trade-off
among performance, efficiency, and privacy, outperforming state-of-the-art
(SOTA) methods.

</details>


### [11] [Identity and Access Management for the Computing Continuum](https://arxiv.org/abs/2506.09559)
*Chalima Dimitra Nassar Kyriakidou,Athanasia Maria Papathanasiou,Vasilios A. Siris,Nikos Fotiou,George C. Polyzos,Eduardo Cánovas Martínez,Antonio Skarmeta*

Main category: cs.CR

TL;DR: 本文提出了一种基于零信任（ZT）的访问控制解决方案，利用去中心化标识（DIDs）和可验证凭证（VCs）实现身份验证，并结合基于关系的访问控制（ReBAC）以应对计算连续体的动态性。


<details>
  <summary>Details</summary>
Motivation: 计算连续体的动态、分布式和异构特性为访问控制带来了新挑战，需要一种能够适应其特性的安全解决方案。

Method: 采用去中心化标识（DIDs）和可验证凭证（VCs）进行身份验证，并结合基于关系的访问控制（ReBAC）定义策略。

Result: 通过概念验证实现，证明了该解决方案的可行性和高效性。

Conclusion: 该方案有望提升去中心化环境中的安全性和信任度。

Abstract: The computing continuum introduces new challenges for access control due to
its dynamic, distributed, and heterogeneous nature. In this paper, we propose a
Zero-Trust (ZT) access control solution that leverages decentralized
identification and authentication mechanisms based on Decentralized Identifiers
(DIDs) and Verifiable Credentials (VCs). Additionally, we employ
Relationship-Based Access Control (ReBAC) to define policies that capture the
evolving trust relationships inherent in the continuum. Through a
proof-of-concept implementation, we demonstrate the feasibility and efficiency
of our solution, highlighting its potential to enhance security and trust in
decentralized environments.

</details>


### [12] [TooBadRL: Trigger Optimization to Boost Effectiveness of Backdoor Attacks on Deep Reinforcement Learning](https://arxiv.org/abs/2506.09562)
*Songze Li,Mingxuan Zhang,Oubo Ma,Kang Wei,Shouling Ji*

Main category: cs.CR

TL;DR: TooBadRL是一个系统优化深度强化学习（DRL）后门攻击触发器的框架，通过时间、空间和幅度三个关键维度提升攻击效果。


<details>
  <summary>Details</summary>
Motivation: 现有后门攻击依赖简单启发式触发器配置，忽视了触发器优化的潜在效果。

Method: 提出性能感知的自适应冻结机制、基于合作游戏的维度选择方法，以及梯度对抗的幅度优化。

Result: 在三种主流DRL算法和九个基准任务中，TooBadRL显著提高了攻击成功率，同时最小化正常任务性能下降。

Conclusion: 触发器优化在DRL后门攻击中的重要性被低估，TooBadRL为此提供了系统解决方案。

Abstract: Deep reinforcement learning (DRL) has achieved remarkable success in a wide
range of sequential decision-making domains, including robotics, healthcare,
smart grids, and finance. Recent research demonstrates that attackers can
efficiently exploit system vulnerabilities during the training phase to execute
backdoor attacks, producing malicious actions when specific trigger patterns
are present in the state observations. However, most existing backdoor attacks
rely primarily on simplistic and heuristic trigger configurations, overlooking
the potential efficacy of trigger optimization. To address this gap, we
introduce TooBadRL (Trigger Optimization to Boost Effectiveness of Backdoor
Attacks on DRL), the first framework to systematically optimize DRL backdoor
triggers along three critical axes, i.e., temporal, spatial, and magnitude.
Specifically, we first introduce a performance-aware adaptive freezing
mechanism for injection timing. Then, we formulate dimension selection as a
cooperative game, utilizing Shapley value analysis to identify the most
influential state variable for the injection dimension. Furthermore, we propose
a gradient-based adversarial procedure to optimize the injection magnitude
under environment constraints. Evaluations on three mainstream DRL algorithms
and nine benchmark tasks show that TooBadRL significantly improves attack
success rates, while ensuring minimal degradation of normal task performance.
These results highlight the previously underappreciated importance of
principled trigger optimization in DRL backdoor attacks. The source code of
TooBadRL can be found at https://github.com/S3IC-Lab/TooBadRL.

</details>


### [13] [The Rabin cryptosystem over number fields](https://arxiv.org/abs/2506.09569)
*Alessandro Cobbe,Andreas Nickel,Akay Schuster*

Main category: cs.CR

TL;DR: 将Rabin加密系统推广到一般数域，证明在特定条件下解密随机明文与整数分解问题难度相当，并与经典Rabin方案及高斯整数版本进行性能比较。


<details>
  <summary>Details</summary>
Motivation: 扩展Rabin加密系统到更一般的数域，以增强其适用性和安全性。

Method: 通过选择特定的模数，将Rabin加密系统推广到一般数域，并分析其解密难度与整数分解问题的关系。

Result: 在特定条件下，解密随机明文的难度与整数分解问题相当；性能分析显示新系统与经典Rabin方案及高斯整数版本相比具有竞争力。

Conclusion: 推广后的Rabin加密系统在安全性和性能上均表现良好，为密码学领域提供了新的选择。

Abstract: We extend Rabin's cryptosystem to general number fields. We show that
decryption of a random plaintext is as hard as the integer factorisation
problem, provided the modulus in our scheme has been chosen carefully. We
investigate the performance of our new cryptosystem in comparison with the
classical Rabin scheme and a more recent version over the Gaussian integers.

</details>


### [14] [The Everyday Security of Living with Conflict](https://arxiv.org/abs/2506.09580)
*Jessica McClearn,Reem Talhouk,Rikke Bjerg Jensen*

Main category: cs.CR

TL;DR: 论文提出了一种不同于传统技术视角的安全研究方法，关注战争和冲突中社区的日常安全体验，并通过哥伦比亚、黎巴嫩和瑞典的实地研究案例，呼吁安全研究者和从业者在设计安全技术时考虑这些生活经验。


<details>
  <summary>Details</summary>
Motivation: 传统的‘网络’前缀通常强调技术和战争的光鲜面，而忽略了战争和冲突中社区的日常安全体验。本文旨在填补这一空白，通过实地研究揭示这些被忽视的视角。

Method: 通过哥伦比亚、黎巴嫩和瑞典的三个实地研究案例（vignettes），结合民族志方法，分析社区在战争和冲突中的日常安全体验。

Result: 研究发现，社区在战争和冲突中的安全需求与传统的技术视角存在显著差异，民族志方法能够有效揭示这些需求。

Conclusion: 论文呼吁安全研究者和从业者在设计安全技术时，更多考虑战争和冲突中社区的实际生活经验，尤其是在全球冲突和灾难区域。

Abstract: When `cyber' is used as a prefix, attention is typically drawn to the
technological and spectacular aspects of war and conflict -- and, by extension,
security. We offer a different approach to engaging with and understanding
security in such contexts, by foregrounding the everyday -- mundane --
experiences of security within communities living with and fleeing from war. We
do so through three vignettes from our field research in Colombia, Lebanon and
Sweden, respectively, and by highlighting the significance of ethnography for
security research with communities living in regions afflicted by war. We
conclude by setting out a call to action for security researchers and
practitioners to consider such lived experiences in the design of security
technology that aims to cater to the needs of communities in `global conflict
and disaster regions'.

</details>


### [15] [Empirical Quantification of Spurious Correlations in Malware Detection](https://arxiv.org/abs/2506.09662)
*Bianca Perasso,Ludovico Lozza,Andrea Ponte,Luca Demetrio,Luca Oneto,Fabio Roli*

Main category: cs.CR

TL;DR: 论文探讨了端到端深度学习在恶意软件检测中依赖虚假相关性的问题，特别是编译器留下的空白空间对模型决策的影响，并通过小规模平衡数据集分析比较了两种模型的适用性。


<details>
  <summary>Details</summary>
Motivation: 研究动机是揭示深度学习模型在恶意软件检测中如何依赖虚假相关性（如编译器留下的空白空间），并量化其对决策的影响。

Method: 方法包括对两种端到端深度学习模型在小规模平衡数据集上的分析，重点关注模型对编译器空白空间的依赖程度。

Result: 结果表明，模型过度依赖编译器留下的空白空间，降低了编译代码的相关性，并提供了两种模型的适用性排名。

Conclusion: 结论指出，理解虚假相关性对模型的影响有助于选择更适合实际生产的模型。

Abstract: End-to-end deep learning exhibits unmatched performance for detecting
malware, but such an achievement is reached by exploiting spurious correlations
-- features with high relevance at inference time, but known to be useless
through domain knowledge. While previous work highlighted that deep networks
mainly focus on metadata, none investigated the phenomenon further, without
quantifying their impact on the decision. In this work, we deepen our
understanding of how spurious correlation affects deep learning for malware
detection by highlighting how much models rely on empty spaces left by the
compiler, which diminishes the relevance of the compiled code. Through our
seminal analysis on a small-scale balanced dataset, we introduce a ranking of
two end-to-end models to better understand which is more suitable to be put in
production.

</details>


### [16] [On the Virtues of Information Security in the UK Climate Movement](https://arxiv.org/abs/2506.09719)
*Mikaela Brough,Rikke Bjerg Jensen,Martin R. Albrecht*

Main category: cs.CR

TL;DR: 研究探讨了英国气候运动成员在信息安全方面的社会复杂性，揭示了开放与保密、自主与集体决策、理想冲突及社会压力等核心问题。


<details>
  <summary>Details</summary>
Motivation: 理解气候运动成员在信息安全方面的实际挑战，为设计支持活动家的方案提供方法论启示。

Method: 采用民族志研究方法，包括参与观察和访谈，在抗议活动和各种活动家场景中收集数据。

Result: 研究发现气候运动成员面临开放与保密、自主与集体决策、理想冲突及社会压力等多重张力。

Conclusion: 研究揭示了活动家环境中信息安全的复杂性，并对设计支持活动家的方案提出了方法论问题。

Abstract: We report on an ethnographic study with members of the climate movement in
the United Kingdom (UK). We conducted participant observation and interviews at
protests and in various activist settings. Reporting on the findings as they
relate to information security, we show that members of the UK climate movement
wrestled with (i) a fundamental tension between openness and secrecy; (ii)
tensions between autonomy and collective interdependence in
information-security decision-making; (iii) conflicting activist ideals that
shape security discourses; and (iv) pressures from different social gazes --
from each other, from people outside the movement and from their adversaries.
Overall, our findings shed light on the social complexities of
information-security research in activist settings and provoke methodological
questions about programmes that aim to design for activists.

</details>


### [17] [Physical Layer-Based Device Fingerprinting for Wireless Security: From Theory to Practice](https://arxiv.org/abs/2506.09807)
*Junqing Zhang,Francesco Ardizzon,Mattia Piana,Guanxiong Shen,Stefano Tomasin*

Main category: cs.CR

TL;DR: 本文综述了基于物理层的设备指纹识别技术，用于无线通信中的设备认证，重点讨论了硬件缺陷和信道特征认证方法。


<details>
  <summary>Details</summary>
Motivation: 传统加密认证方法在物联网中计算成本高且不适用，因此需要更高效的被动认证技术。

Method: 综述了硬件缺陷和信道特征为基础的认证技术，包括其算法设计、应用场景和关键问题。

Result: 总结了现有技术的优缺点，并指出其适用于传统物联网设备的潜力。

Conclusion: 讨论了当前研究的挑战，并提出了未来改进方向。

Abstract: The identification of the devices from which a message is received is part of
security mechanisms to ensure authentication in wireless communications.
Conventional authentication approaches are cryptography-based, which, however,
are usually computationally expensive and not adequate in the Internet of
Things (IoT), where devices tend to be low-cost and with limited resources.
This paper provides a comprehensive survey of physical layer-based device
fingerprinting, which is an emerging device authentication for wireless
security. In particular, this article focuses on hardware impairment-based
identity authentication and channel features-based authentication. They are
passive techniques that are readily applicable to legacy IoT devices. Their
intrinsic hardware and channel features, algorithm design methodologies,
application scenarios, and key research questions are extensively reviewed
here. The remaining research challenges are discussed, and future work is
suggested that can further enhance the physical layer-based device
fingerprinting.

</details>


### [18] [Oracle-Based Multistep Strategy for Solving Polynomial Systems Over Finite Fields and Algebraic Cryptanalysis of the Aradi Cipher](https://arxiv.org/abs/2506.09950)
*La Scala Roberto,Sharwan Kumar Tiwari*

Main category: cs.CR

TL;DR: 论文提出了一种基于深度优先搜索的多步求解策略新实现，并引入“预言函数”概念统一了所有变体。该方法成功应用于Aradi密码的代数攻击。


<details>
  <summary>Details</summary>
Motivation: 解决多元多项式系统直接求解计算不可行的问题，并提升密码分析的效率。

Method: 采用深度优先搜索策略实现多步求解算法，引入预言函数预测变量评估的必要性。

Result: 成功对Aradi密码进行了首次全轮代数攻击，揭示了其安全性问题。

Conclusion: 多步求解策略结合预言函数有效提升了密码分析的效率，对Aradi密码的攻击结果引发了对其安全性的担忧。

Abstract: The multistep solving strategy consists in a divide-and-conquer approach:
when a multivariate polynomial system is computationally infeasible to solve
directly, one variable is assigned over the elements of the base finite field,
and the procedure is recursively applied to the resulting simplified systems.
In a previous work by the same authors (among others), this approach proved
effective in the algebraic cryptanalysis of the Trivium cipher. In this paper,
we present a new implementation of the corresponding algorithm based on a
Depth-First Search strategy, along with a novel complexity analysis leveraging
tree structures. We further introduce the notion of an "oracle function" as a
general predictive tool for deciding whether the evaluation of a new variable
is necessary to simplify the current polynomial system. This notion allows us
to unify all previously proposed variants of the multistep strategy, including
the classical hybrid approach, by appropriately selecting the oracle function.
Finally, we apply the multistep solving strategy to the cryptanalysis of the
low-latency block cipher Aradi, recently introduced by the NSA. We present the
first full round algebraic attack, raising concerns about the cipher's actual
security with respect to its key length.

</details>


### [19] [LLMail-Inject: A Dataset from a Realistic Adaptive Prompt Injection Challenge](https://arxiv.org/abs/2506.09956)
*Sahar Abdelnabi,Aideen Fay,Ahmed Salem,Egor Zverev,Kai-Chieh Liao,Chi-Huang Liu,Chun-Chih Kuo,Jannis Weigend,Danyael Manlangit,Alex Apostolov,Haris Umair,João Donato,Masayuki Kawakita,Athar Mahboob,Tran Huu Bach,Tsun-Han Chiang,Myeongjin Cho,Hajin Choi,Byeonghyeon Kim,Hyeonjin Lee,Benjamin Pannell,Conor McCauley,Mark Russinovich,Andrew Paverd,Giovanni Cherubin*

Main category: cs.CR

TL;DR: 论文研究了间接提示注入攻击对大型语言模型（LLM）的影响，并通过公开挑战LLMail-Inject收集数据，分析了防御策略的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有防御措施对适应性攻击者的系统性评估不足，而成功的攻击可能带来广泛的安全和隐私问题。

Method: 通过LLMail-Inject公开挑战，模拟现实场景，收集攻击数据并分析不同防御策略、LLM架构和检索配置的效果。

Result: 收集了208,095次攻击提交数据，展示了指令与数据分离问题的新见解。

Conclusion: 研究为未来解决提示注入问题的结构性方案提供了基础。

Abstract: Indirect Prompt Injection attacks exploit the inherent limitation of Large
Language Models (LLMs) to distinguish between instructions and data in their
inputs. Despite numerous defense proposals, the systematic evaluation against
adaptive adversaries remains limited, even when successful attacks can have
wide security and privacy implications, and many real-world LLM-based
applications remain vulnerable. We present the results of LLMail-Inject, a
public challenge simulating a realistic scenario in which participants
adaptively attempted to inject malicious instructions into emails in order to
trigger unauthorized tool calls in an LLM-based email assistant. The challenge
spanned multiple defense strategies, LLM architectures, and retrieval
configurations, resulting in a dataset of 208,095 unique attack submissions
from 839 participants. We release the challenge code, the full dataset of
submissions, and our analysis demonstrating how this data can provide new
insights into the instruction-data separation problem. We hope this will serve
as a foundation for future research towards practical structural solutions to
prompt injection.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [20] [Intelligent System of Emergent Knowledge: A Coordination Fabric for Billions of Minds](https://arxiv.org/abs/2506.09335)
*Moshi Wei,Sparks Li*

Main category: cs.MA

TL;DR: ISEK是一个去中心化的认知生态系统，结合区块链、人工智能和激励机制，实现人类与AI的平等协作，通过六阶段工作流和代币经济推动任务分配和声誉管理。


<details>
  <summary>Details</summary>
Motivation: 传统平台存在中心化约束，ISEK旨在通过去中心化架构和AI-人类协作，实现大规模认知系统的有机发展。

Method: 采用Web3基础设施，结合多代理架构、六阶段工作流（发布、发现、招募、执行、结算、反馈）和代币经济（$ISEK）。

Result: ISEK构建了一个抗审查、自适应的去中心化网络，支持动态任务分配和声誉管理。

Conclusion: ISEK通过区块链和AI的结合，实现了去中心化认知系统的范式转变，突破了中心化限制。

Abstract: The Intelligent System of Emergent Knowledge (ISEK) establishes a
decentralized network where human and artificial intelligence agents
collaborate as peers, forming a self-organizing cognitive ecosystem. Built on
Web3 infrastructure, ISEK combines three fundamental principles: (1) a
decentralized multi-agent architecture resistant to censorship, (2) symbiotic
AI-human collaboration with equal participation rights, and (3) resilient
self-adaptation through distributed consensus mechanisms.
  The system implements an innovative coordination protocol featuring a
six-phase workflow (Publish, Discover, Recruit, Execute, Settle, Feedback) for
dynamic task allocation, supported by robust fault tolerance and a
multidimensional reputation system. Economic incentives are governed by the
native $ISEK token, facilitating micropayments, governance participation, and
reputation tracking, while agent sovereignty is maintained through NFT-based
identity management.
  This synthesis of blockchain technology, artificial intelligence, and
incentive engineering creates an infrastructure that actively facilitates
emergent intelligence. ISEK represents a paradigm shift from conventional
platforms, enabling the organic development of large-scale, decentralized
cognitive systems where autonomous agents collectively evolve beyond
centralized constraints.

</details>


### [21] [When Is Diversity Rewarded in Cooperative Multi-Agent Learning?](https://arxiv.org/abs/2506.09434)
*Michael Amir,Matteo Bettini,Amanda Prorok*

Main category: cs.MA

TL;DR: 论文研究了多智能体任务分配问题中，异质性团队何时优于同质性团队，并通过奖励设计和多智能体强化学习验证了异质性的优势条件。


<details>
  <summary>Details</summary>
Motivation: 团队在机器人、自然和社会中的成功常依赖于多样化的分工，但缺乏对异质性团队优势的理论解释。

Method: 通过广义聚合算子分析奖励设计，提出Heterogeneous Environment Design (HED)算法优化多智能体强化学习环境。

Result: 理论证明聚合算子的曲率决定异质性的优势，HED算法在实验中验证了理论预测的奖励机制。

Conclusion: 研究揭示了行为多样性在特定奖励设计下的优势，为异质性团队的应用提供了理论支持。

Abstract: The success of teams in robotics, nature, and society often depends on the
division of labor among diverse specialists; however, a principled explanation
for when such diversity surpasses a homogeneous team is still missing. Focusing
on multi-agent task allocation problems, our goal is to study this question
from the perspective of reward design: what kinds of objectives are best suited
for heterogeneous teams? We first consider an instantaneous, non-spatial
setting where the global reward is built by two generalized aggregation
operators: an inner operator that maps the $N$ agents' effort allocations on
individual tasks to a task score, and an outer operator that merges the $M$
task scores into the global team reward. We prove that the curvature of these
operators determines whether heterogeneity can increase reward, and that for
broad reward families this collapses to a simple convexity test. Next, we ask
what incentivizes heterogeneity to emerge when embodied, time-extended agents
must learn an effort allocation policy. To study heterogeneity in such
settings, we use multi-agent reinforcement learning (MARL) as our computational
paradigm, and introduce Heterogeneous Environment Design (HED), a
gradient-based algorithm that optimizes the parameter space of underspecified
MARL environments to find scenarios where heterogeneity is advantageous.
Experiments in matrix games and an embodied Multi-Goal-Capture environment show
that, despite the difference in settings, HED rediscovers the reward regimes
predicted by our theory to maximize the advantage of heterogeneity, both
validating HED and connecting our theoretical insights to reward design in
MARL. Together, these results help us understand when behavioral diversity
delivers a measurable benefit.

</details>


### [22] [Effective Red-Teaming of Policy-Adherent Agents](https://arxiv.org/abs/2506.09600)
*Itay Nakash,George Kour,Koren Lazar,Matan Vetzler,Guy Uziel,Ateret Anaby-Tavor*

Main category: cs.MA

TL;DR: 论文提出了一种针对任务导向LLM代理的威胁模型，并开发了CRAFT系统来测试代理的鲁棒性，同时引入了tau-break基准和评估防御策略。


<details>
  <summary>Details</summary>
Motivation: 确保任务导向LLM代理在严格政策下的一致性，同时抵御恶意用户行为。

Method: 提出CRAFT多代理红队系统，利用策略感知的劝说策略测试代理的鲁棒性，并引入tau-break基准。

Result: CRAFT在客服场景中优于传统越狱方法，但现有防御策略仍不足。

Conclusion: 需要更强研究驱动的保护措施来抵御对抗性攻击。

Abstract: Task-oriented LLM-based agents are increasingly used in domains with strict
policies, such as refund eligibility or cancellation rules. The challenge lies
in ensuring that the agent consistently adheres to these rules and policies,
appropriately refusing any request that would violate them, while still
maintaining a helpful and natural interaction. This calls for the development
of tailored design and evaluation methodologies to ensure agent resilience
against malicious user behavior. We propose a novel threat model that focuses
on adversarial users aiming to exploit policy-adherent agents for personal
benefit. To address this, we present CRAFT, a multi-agent red-teaming system
that leverages policy-aware persuasive strategies to undermine a
policy-adherent agent in a customer-service scenario, outperforming
conventional jailbreak methods such as DAN prompts, emotional manipulation, and
coercive. Building upon the existing tau-bench benchmark, we introduce
tau-break, a complementary benchmark designed to rigorously assess the agent's
robustness against manipulative user behavior. Finally, we evaluate several
straightforward yet effective defense strategies. While these measures provide
some protection, they fall short, highlighting the need for stronger,
research-driven safeguards to protect policy-adherent agents from adversarial
attacks

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [23] [Formal Methods Meets Readability: Auto-Documenting JML Java Code](https://arxiv.org/abs/2506.09230)
*Juan Carlos Recio Abad,Ruben Saborido,Francisco Chicano*

Main category: cs.SE

TL;DR: 研究发现，使用JML形式化规范能显著提升LLM生成的Javadoc质量，尤其在类级别文档完整性和复杂类不变式捕获方面表现突出。


<details>
  <summary>Details</summary>
Motivation: 探讨JML形式化规范是否能提升LLM生成的Javadoc质量，填补代码文档中常被忽略的设计契约和不变式。

Method: 通过系统比较JML注释和非注释Java类生成的文档，结合自动化指标和专家分析评估质量。

Result: JML显著提升类级别文档完整性，对方法级别提升较小；形式化规范在捕获复杂类不变式方面效果显著。

Conclusion: JML与LLM协同可提升文档质量，形式化规范主要确保全面覆盖而非改变描述本质，为软件团队提供实用建议。

Abstract: This paper investigates whether formal specifications using Java Modeling
Language (JML) can enhance the quality of Large Language Model (LLM)-generated
Javadocs. While LLMs excel at producing documentation from code alone, we
hypothesize that incorporating formally verified invariants yields more
complete and accurate results. We present a systematic comparison of
documentation generated from JML-annotated and non-annotated Java classes,
evaluating quality through both automated metrics and expert analysis. Our
findings demonstrate that JML significantly improves class-level documentation
completeness, with more moderate gains at the method level. Formal
specifications prove particularly effective in capturing complex class
invariants and design contracts that are frequently overlooked in code-only
documentation. A threshold effect emerges, where the benefits of JML become
more pronounced for classes with richer sets of invariants. While JML enhances
specification coverage, its impact on core descriptive quality is limited,
suggesting that formal specifications primarily ensure comprehensive coverage
rather than fundamentally altering implementation descriptions. These results
offer actionable insights for software teams adopting formal methods in
documentation workflows, highlighting scenarios where JML provides clear
advantages. The study contributes to AI-assisted software documentation
research by demonstrating how formal methods and LLMs can synergistically
improve documentation quality.

</details>


### [24] [UTBoost: Rigorous Evaluation of Coding Agents on SWE-Bench](https://arxiv.org/abs/2506.09289)
*Boxi Yu,Yuxuan Zhu,Pinjia He,Daniel Kang*

Main category: cs.SE

TL;DR: 论文提出UTGenerator和UTBoost，通过LLM生成测试用例以解决SWE-Bench中测试不足的问题，显著提升了代码生成代理的评估准确性。


<details>
  <summary>Details</summary>
Motivation: SWE-Bench中手动编写的测试用例不足以检测代码生成代理的真实能力，导致生成的补丁可能通过测试但未解决问题。

Method: 引入UTGenerator（基于LLM的测试用例生成器）和UTBoost（测试用例增强框架），自动分析代码库和依赖关系生成测试用例。

Result: 在评估中，发现了36个测试不足的任务实例和345个错误补丁，影响了SWE-Bench的排行榜排名。

Conclusion: UTGenerator和UTBoost有效提升了测试用例的覆盖率和评估的准确性，为代码生成代理的评估提供了更可靠的方法。

Abstract: The advent of Large Language Models (LLMs) has spurred the development of
coding agents for real-world code generation. As a widely used benchmark for
evaluating the code generation capabilities of these agents, SWE-Bench uses
real-world problems based on GitHub issues and their corresponding pull
requests. However, the manually written test cases included in these pull
requests are often insufficient, allowing generated patches to pass the tests
without resolving the underlying issue. To address this challenge, we introduce
UTGenerator, an LLM-driven test case generator that automatically analyzes
codebases and dependencies to generate test cases for real-world Python
projects. Building on UTGenerator, we propose UTBoost, a comprehensive
framework for test case augmentation. In our evaluation, we identified 36 task
instances with insufficient test cases and uncovered 345 erroneous patches
incorrectly labeled as passed in the original SWE Bench. These corrections,
impacting 40.9% of SWE-Bench Lite and 24.4% of SWE-Bench Verified leaderboard
entries, yield 18 and 11 ranking changes, respectively.

</details>


### [25] [Assessing the Impact of Refactoring Energy-Inefficient Code Patterns on Software Sustainability: An Industry Case Study](https://arxiv.org/abs/2506.09370)
*Rohit Mehra,Priyavanshi Pathania,Vibhu Saujanya Sharma,Vikrant Kaulgud,Sanjay Podder,Adam P. Burden*

Main category: cs.SE

TL;DR: 论文摘要探讨了通过自动化工具优化软件代码以减少碳排放，并通过案例研究展示了29%的能源消耗降低。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能和元宇宙等技术的发展，软件系统的碳排放问题日益严重，亟需从可持续性角度优化软件。

Method: 采用自动化工具识别代码中的能源低效模式，并指导重构。

Result: 案例研究显示，重构后每用户每月的能源消耗降低了29%。

Conclusion: 自动化工具在优化软件可持续性方面具有显著潜力，能有效减少能源消耗。

Abstract: Advances in technologies like artificial intelligence and metaverse have led
to a proliferation of software systems in business and everyday life. With this
widespread penetration, the carbon emissions of software are rapidly growing as
well, thereby negatively impacting the long-term sustainability of our
environment. Hence, optimizing software from a sustainability standpoint
becomes more crucial than ever. We believe that the adoption of automated tools
that can identify energy-inefficient patterns in the code and guide appropriate
refactoring can significantly assist in this optimization. In this extended
abstract, we present an industry case study that evaluates the sustainability
impact of refactoring energy-inefficient code patterns identified by automated
software sustainability assessment tools for a large application. Preliminary
results highlight a positive impact on the application's sustainability
post-refactoring, leading to a 29% decrease in per-user per-month energy
consumption.

</details>


### [26] [Reasoning as a Resource: Optimizing Fast and Slow Thinking in Code Generation Models](https://arxiv.org/abs/2506.09396)
*Zongjie Li,Shuai Wang*

Main category: cs.SE

TL;DR: 论文提出将推理深度作为可控资源，优化代码生成模型的设计，以在速度、准确性和成本之间实现更好的权衡。


<details>
  <summary>Details</summary>
Motivation: 当前代码生成模型的推理深度通常是无意中由提示决定的，缺乏明确管理。论文主张通过显式控制推理深度，优化模型全生命周期的推理预算。

Method: 提出自适应控制推理深度的方法，从合成数据创建、基准测试到实际部署，管理快速思维与慢速思维的权衡。

Result: 通过控制推理深度，可以增强监督信号、推动多维基准测试，并支持成本和安全意识强的部署策略。

Conclusion: 将快速思维与慢速思维视为互补模式，可以设计出在必要时深入思考、在可能时快速行动的智能编码代理。

Abstract: This position paper proposes a fundamental shift in designing code generation
models: treating reasoning depth as a controllable resource. Rather than being
an incidental byproduct of prompting, we argue that the trade-off between
rapid, direct answers ("fast thinking") and elaborate, chain-of-thought
deliberation ("slow thinking") must be explicitly managed. We contend that
optimizing reasoning budgets across the entire model lifecycle - from synthetic
data creation and benchmarking to real-world deploymen - can unlock superior
trade-offs among accuracy, latency, and cost. This paper outlines how adaptive
control over reasoning can enrich supervision signals, motivate new
multi-dimensional benchmarks, and inform cost-aware, security-conscious
deployment policies. By viewing fast and slow thinking as complementary modes
to be scheduled, we envision coding agents that think deep when necessary and
act fast when possible.

</details>


### [27] [Automated Synthesis of Formally Verified Multi-Abstraction Function Summaries](https://arxiv.org/abs/2506.09550)
*Fanpeng Yang,Xu Ma,Shuling Wang,Xiong Xu,Qinxiang Cao,Naijun Zhan,Xiaofeng Li,Bin Gu*

Main category: cs.SE

TL;DR: 论文提出了一种结合符号执行、大语言模型（LLMs）和形式验证的框架，用于生成C程序的函数摘要，支持多种抽象级别。


<details>
  <summary>Details</summary>
Motivation: 任务关键型遗留代码缺乏形式化规范，自动生成函数摘要面临复杂C语言特性的挑战。

Method: 结合VST-A的符号执行、LLMs推断循环不变量，以及Frama-C的形式验证，生成相对最强后条件（RSPs）。

Result: 实验表明，该方法能生成全面捕获程序行为的函数摘要，并支持不同抽象需求。

Conclusion: 该框架有效解决了复杂C程序函数摘要生成的挑战，为软件理解和验证提供了实用工具。

Abstract: Function summaries, which characterize the behavior of code segments
(typically functions) through preconditions and postconditions, are essential
for understanding, reusing, and verifying software, particularly in
safety-critical domains like aerospace embedded systems. However, these
mission-critical legacy code serving as a valuable reused asset often lacks
formal specifications. It is challenging to automatically generate function
summaries for C programs, due to the existence of complex features such as
loops, nested function calls, pointer aliasing, and so on. Moreover, function
summaries should support multiple abstraction levels to meet diverse
requirements, e.g. precise summaries capturing full functionality for formal
verification and intuitive summaries for human understanding.
  To address these challenges, we first propose a novel framework that combines
symbolic execution, large language models (LLMs), and formal verification to
generate Relatively Strongest Postconditions (RSPs) and build function
summaries that fully capture program behavior. Our approach leverages VST-A's
symbolic execution to precisely track program execution paths and state
transitions, employs LLMs to infer loop invariants based on predefined
templates, and uses Frama-C to guarantee soundness of generated summaries in an
iterative refinement loop. Furthermore, from generated RSPs, we automatically
synthesize strongest non-redundant postconditions expressed within given domain
specific language. We compare our approach with existing work through extensive
experiments.

</details>


### [28] [ASTAGEN: Empirical Evaluation of Automated SATD Taxonomy Generation with LLMs](https://arxiv.org/abs/2506.09601)
*Sota Nakashima,Yuta Ishimoto,Masanari Kondo,Tao Xiao,Yasutaka Kamei*

Main category: cs.SE

TL;DR: ASTAGEN利用大型语言模型（LLM）自动生成自承认技术债务（SATD）分类法，通过解释驱动的迭代设计，在短时间内低成本完成分类任务。


<details>
  <summary>Details</summary>
Motivation: 传统手动构建SATD分类法耗时且主观性强，ASTAGEN旨在通过自动化提高效率和一致性。

Method: ASTAGEN结合SATD注释和代码，首先生成解释，再迭代生成和更新分类法。

Result: 在量子软件、智能合约和机器学习领域的数据集上，ASTAGEN成功恢复已知分类，且成本低、速度快。

Conclusion: ASTAGEN支持半自动化分类法构建，为未来其他领域的自动分类研究提供了方向。

Abstract: Technical debt refers to suboptimal code that degrades software quality. When
developers intentionally introduce such debt, it is called self-admitted
technical debt (SATD). Since SATD hinders maintenance, identifying its
categories is key to uncovering quality issues. Traditionally, constructing
such taxonomies requires manually inspecting SATD comments and surrounding
code, which is time-consuming, labor-intensive, and often inconsistent due to
annotator subjectivity. This study presents ASTAGEN, an initial step toward
automating SATD taxonomy generation using large language models (LLMs). Given a
comment and its surrounding code, ASTAGEN first generates a concise explanation
for each SATD comment, then incrementally generates and updates categories to
construct a taxonomy. We evaluate ASTAGEN on SATD datasets from three domains:
quantum software, smart contracts, and machine learning. It successfully
recovers domain-specific categories reported in prior work, such as Layer
Configuration in machine learning. Compared to a naive use of an LLM, ASTAGEN
produces more consistent category assignments due to its explanation-driven,
iterative design. It also completes taxonomy generation in under two hours and
for less than one USD, even on the largest dataset. These results suggest that
while full automation remains challenging, ASTAGEN is able to support
semi-automated taxonomy construction. Furthermore, our work opens up avenues
for future work, such as automatic taxonomy generation in other areas.

</details>


### [29] [Translating a VDM Model of a Medical Device into Kapture](https://arxiv.org/abs/2506.09636)
*Joe Hare,Leo Freitas,Ken Pierce*

Main category: cs.SE

TL;DR: 本文探讨了使用Kapture工具将CANDO医疗设备的VDM模型转换为Kapture模型的过程，评估了工具的可用性、挑战及转换效果。


<details>
  <summary>Details</summary>
Motivation: 随着医疗设备复杂度的增加，需要清晰、可验证的软件需求。本文旨在探索Kapture工具在缺乏形式化方法经验的情况下，如何有效转换VDM模型。

Method: 使用Kapture工具将CANDO医疗设备的VDM模型转换为Kapture模型，评估工具的可用性和转换效果。

Result: 转换后的Kapture模型覆盖了90%以上的原始VDM模型，并生成匹配的结果轨迹。尽管遇到了一些设计和实现问题，但证明了Kapture适用于缺乏经验的用户。

Conclusion: Kapture工具能够有效支持复杂系统的建模，但转换VDM规范时存在一定挑战。

Abstract: As the complexity of safety-critical medical devices increases, so does the
need for clear, verifiable, software requirements. This paper explores the use
of Kapture, a formal modelling tool developed by D-RisQ, to translate an
existing formal VDM model of a medical implant for treating focal epilepsy
called CANDO. The work was undertaken without prior experience in formal
methods. The paper assess Kapture's usability, the challenges of formal
modelling, and the effectiveness of the translated model. The result is a model
in Kapture which covers over 90% of the original VDM model, and produces
matching traces of results. While several issues were encountered during design
and implementation, mainly due to the initial learning curve, this paper
demonstrates that complex systems can be effectively modelled in Kapture by
inexperienced users and highlights some difficulties in translating VDM
specifications to Kapture.

</details>


### [30] [Calculating Software's Energy Use and Carbon Emissions: A Survey of the State of Art, Challenges, and the Way Ahead](https://arxiv.org/abs/2506.09683)
*Priyavanshi Pathania,Nikhil Bamby,Rohit Mehra,Samarth Sikand,Vibhu Saujanya Sharma,Vikrant Kaulgud,Sanjay Podder,Adam P. Burden*

Main category: cs.SE

TL;DR: 本文综述了测量软件和AI能源及碳排放的方法与工具，提出了分类法（监测、估算、黑盒），并比较了不同工具的特点与挑战，呼吁社区合作解决这些问题。


<details>
  <summary>Details</summary>
Motivation: 随着软件和AI的普及，其能源和碳排放问题日益突出，亟需理解和优化其对环境的影响。

Method: 提出分类法（监测、估算、黑盒），比较不同工具在能源和碳排放测量中的维度和粒度。

Result: 总结了现有工具的优缺点，并指出当前研究中的挑战。

Conclusion: 强调社区合作的重要性，并启动倡议以应对这些挑战。

Abstract: The proliferation of software and AI comes with a hidden risk: its growing
energy and carbon footprint. As concerns regarding environmental sustainability
come to the forefront, understanding and optimizing how software impacts the
environment becomes paramount. In this paper, we present a state-of-the-art
review of methods and tools that enable the measurement of software and
AI-related energy and/or carbon emissions. We introduce a taxonomy to
categorize the existing work as Monitoring, Estimation, or Black-Box
approaches. We delve deeper into the tools and compare them across different
dimensions and granularity - for example, whether their measurement encompasses
energy and carbon emissions and the components considered (like CPU, GPU, RAM,
etc.). We present our observations on the practical use (component wise
consolidation of approaches) as well as the challenges that we have identified
across the current state-of-the-art. As we start an initiative to address these
challenges, we emphasize active collaboration across the community in this
important field.

</details>


### [31] [Mapping NVD Records to Their VFCs: How Hard is it?](https://arxiv.org/abs/2506.09702)
*Huu Hung Nguyen,Duc Manh Tran,Yiran Cheng,Thanh Le-Cong,Hong Jin Kang,Ratnadira Widyasari,Shar Lwin Khin,Ouh Eng Lieh,Ting Zhang,David Lo*

Main category: cs.SE

TL;DR: 该研究通过实证方法探索了将NVD记录映射到漏洞修复提交（VFCs）的可行性，发现Git引用成功率较高，但仍有大量记录未映射。


<details>
  <summary>Details</summary>
Motivation: NVD记录与漏洞修复提交之间的映射对漏洞分析至关重要，但由于NVD引用中显式链接稀疏，这一任务具有挑战性。

Method: 研究通过手动分析NVD引用，发现Git引用成功率较高，随后构建自动化管道，并挖掘外部安全数据库和GitHub仓库以填补空白。

Result: 成功映射了26,710条唯一记录（覆盖率11.3%），但88.7%的记录仍无法映射。自动化管道的精度为87%-88.4%，GitHub贡献的精度为73%。

Conclusion: 尽管Git引用表现良好，但大多数记录仍无法映射，凸显了无Git链接时的困难。研究为增强漏洞数据集和指导未来自动化安全研究提供了见解。

Abstract: Mapping National Vulnerability Database (NVD) records to vulnerability-fixing
commits (VFCs) is crucial for vulnerability analysis but challenging due to
sparse explicit links in NVD references.This study explores this mapping's
feasibility through an empirical approach. Manual analysis of NVD references
showed Git references enable over 86% success, while non-Git references achieve
under 14%. Using these findings, we built an automated pipeline extracting
31,942 VFCs from 20,360 NVD records (8.7% of 235,341) with 87% precision,
mainly from Git references. To fill gaps, we mined six external security
databases, yielding 29,254 VFCs for 18,985 records (8.1%) at 88.4% precision,
and GitHub repositories, adding 3,686 VFCs for 2,795 records (1.2%) at 73%
precision. Combining these, we mapped 26,710 unique records (11.3% coverage)
from 7,634 projects, with overlap between NVD and external databases, plus
unique GitHub contributions. Despite success with Git references, 88.7% of
records remain unmapped, highlighting the difficulty without Git links. This
study offers insights for enhancing vulnerability datasets and guiding future
automated security research.

</details>


### [32] [A First Look at Bugs in LLM Inference Engines](https://arxiv.org/abs/2506.09713)
*Mugeng Liu,Siqi Zhong,Weichen Bi,Yixuan Zhang,Zhiyang Chen,Zhenpeng Chen,Xuanzhe Liu,Yun Ma*

Main category: cs.SE

TL;DR: 本文首次对大型语言模型推理引擎（LLM推理引擎）中的错误进行了实证研究，分析了929个真实错误，揭示了症状、根本原因及共性，并提出了改进建议。


<details>
  <summary>Details</summary>
Motivation: LLM推理引擎是现代AI基础设施的核心组件，但由于资源需求和跨平台兼容性问题，易出现错误，目前缺乏系统性研究。

Method: 通过挖掘5个广泛使用的LLM推理引擎的官方仓库，构建了包含929个错误的综合数据集，并通过开放编码分析其症状和根本原因。

Result: 研究发现6种主要错误症状和28种根本原因分类，揭示了LLM推理引擎中错误检测与定位的关键挑战。

Conclusion: 基于研究结果，提出了对研究人员、引擎供应商和应用开发者的具体改进建议。

Abstract: Large language model-specific inference engines (in short as \emph{LLM
inference engines}) have become a fundamental component of modern AI
infrastructure, enabling the deployment of LLM-powered applications (LLM apps)
across cloud and local devices. Despite their critical role, LLM inference
engines are prone to bugs due to the immense resource demands of LLMs and the
complexities of cross-platform compatibility. However, a systematic
understanding of these bugs remains lacking. To bridge this gap, we present the
first empirical study on bugs in LLM inference engines. We mine official
repositories of 5 widely adopted LLM inference engines, constructing a
comprehensive dataset of 929 real-world bugs. Through a rigorous open coding
process, we analyze these bugs to uncover their symptoms, root causes, and
commonality. Our findings reveal six major bug symptoms and a taxonomy of 28
root causes, shedding light on the key challenges in bug detection and location
within LLM inference engines. Based on these insights, we propose a series of
actionable implications for researchers, inference engine vendors, and LLM app
developers.

</details>


### [33] [Towards Bridging Formal Methods and Human Interpretability](https://arxiv.org/abs/2506.09759)
*Abhijit Paul,Proma Chowdhury,Kazi Sakib*

Main category: cs.SE

TL;DR: 该研究探讨了人类对标记转换系统（LTS）设计的理解，提出了7个关键指标，并通过实验验证了Albin复杂度、状态空间大小、圈复杂度和冗余度最能反映人类理解。应用Albin复杂度指标后，设计修复工具的效率提高了39%。


<details>
  <summary>Details</summary>
Motivation: 尽管LTS在模型检查和设计修复中至关重要，但此前缺乏对人类理解LTS设计的研究。

Method: 结合软件工程和图论，提出7个指标，创建148个LTS设计数据集，采样48个进行324次配对比较，使用Bradley-Terry模型排名，并通过Kendall's Tau分析相关性。

Result: Albin复杂度、状态空间大小、圈复杂度和冗余度与人类理解相关性最高。应用Albin复杂度后，理解时间减少39%。

Conclusion: 强调人类因素的指标可提升形式化设计的可解释性。

Abstract: Labeled Transition Systems (LTS) are integral to model checking and design
repair tools. System engineers frequently examine LTS designs during model
checking or design repair to debug, identify inconsistencies, and validate
system behavior. Despite LTS's significance, no prior research has examined
human comprehension of these designs. To address this, we draw on traditional
software engineering and graph theory, identifying 7 key metrics: cyclomatic
complexity, state space size, average branching factor, maximum depth, Albin
complexity, modularity, and redundancy. We created a dataset of 148 LTS
designs, sampling 48 for 324 paired comparisons, and ranked them using the
Bradley-Terry model. Through Kendall's Tau correlation analysis, we found that
Albin complexity ($\tau = 0.444$), state space size ($\tau = 0.420$),
cyclomatic complexity ($\tau = 0.366$), and redundancy ($\tau = 0.315$) most
accurately reflect human comprehension of LTS designs. To showcase the metrics'
utility, we applied the Albin complexity metric within the Fortis design repair
tool, ranking system redesigns. This ranking reduced annotators' comprehension
time by 39\%, suggesting that metrics emphasizing human factors can enhance
formal design interpretability.

</details>


### [34] [variability.dev: Towards an Online Toolbox for Feature Modeling](https://arxiv.org/abs/2506.09845)
*Tobias Heß,Lukas Ostheimer,Tobias Betz,Simon Karrer,Tim Jannik Schmidt,Pierre Coquet,Sean Semmler,Thomas Thüm*

Main category: cs.SE

TL;DR: 本文介绍了正在开发的在线工具箱variability.dev，用于特征建模，重点展示了其协作式特征模型编辑器和在线配置器。


<details>
  <summary>Details</summary>
Motivation: 当前在线特征模型编辑器功能有限、维护不足或需要离线安装，无法满足用户需求。

Method: 基于FeatureIDE库开发协作式特征模型编辑器和在线配置器。

Result: 展示了variability.dev工具箱的初步功能，支持协作编辑和在线配置。

Conclusion: variability.dev有望填补当前在线特征模型编辑工具的空白，提供更便捷的协作和配置功能。

Abstract: The emergence of feature models as the default to model the variability in
configurable systems fosters a rich diversity in applications, application
domains, and perspectives. Independent of their domain, modelers require to
open, view, edit, transform, save, and configure models as well as to
collaborate with others. However, at the time of writing, the top five results
when googling ``Online Editor Feature Model'' point to editors that either have
minimal functionality, are unmaintained or defunct, or require an offline
installation, such as FeatureIDE. In this work we present a preview of our
in-development online toolbox for feature modeling, variability.dev. In
particular, we showcase our collaborative feature-model editor and our online
configurator both of which are built on top of the FeatureIDE library.

</details>


### [35] [Stakeholder Participation for Responsible AI Development: Disconnects Between Guidance and Current Practice](https://arxiv.org/abs/2506.09873)
*Emma Kallina,Thomas Bohné,Jat Singh*

Main category: cs.SE

TL;DR: 研究探讨了负责任AI（rAI）指南中利益相关者参与（SHI）的作用，发现现有商业实践与rAI目标存在脱节，并提出改进建议。


<details>
  <summary>Details</summary>
Motivation: 明确现有SHI实践是否能支持rAI目标，以及如何调整实践以促进rAI发展。

Method: 分析56份rAI指南，进行在线调查（n=130）和半结构化访谈（n=10）。

Result: 发现SHI实践主要受商业目标驱动，与rAI目标脱节，需针对性干预。

Conclusion: 需调整SHI实践以支持rAI，并提出未来研究方向。

Abstract: Responsible AI (rAI) guidance increasingly promotes stakeholder involvement
(SHI) during AI development. At the same time, SHI is already common in
commercial software development, but with potentially different foci. This
study clarifies the extent to which established SHI practices are able to
contribute to rAI efforts as well as potential disconnects -- essential
insights to inform and tailor future interventions that further shift industry
practice towards rAI efforts. First, we analysed 56 rAI guidance documents to
identify why SHI is recommended (i.e. its expected benefits for rAI) and
uncovered goals such as redistributing power, improving socio-technical
understandings, anticipating risks, and enhancing public oversight. To
understand why and how SHI is currently practised in commercial settings, we
then conducted an online survey (n=130) and semi-structured interviews (n=10)
with AI practitioners. Our findings reveal that SHI in practice is primarily
driven by commercial priorities (e.g. customer value, compliance) and several
factors currently discourage more rAI-aligned SHI practices. This suggests that
established SHI practices are largely not contributing to rAI efforts. To
address this disconnect, we propose interventions and research opportunities to
advance rAI development in practice.

</details>


### [36] [Assessing a Safety Case: Bottom-up Guidance for Claims and Evidence Evaluation](https://arxiv.org/abs/2506.09929)
*Scott Schnelle,Francesca Favaro,Laura Fraade-Blanar,David Wichner,Holland Broce,Justin Miranda*

Main category: cs.SE

TL;DR: 本文提出了一种评估自动驾驶系统（ADS）安全案例可信度的方法，重点关注案例中声明与证据的支持关系，并提供了评分策略和评估指南。


<details>
  <summary>Details</summary>
Motivation: 随着自动驾驶技术的发展，确保安全和公众信任需要强大的保障框架，安全案例成为实现这一目标的关键工具。本文旨在评估安全案例的可信度。

Method: 从安全案例的基本构建块（声明、证据等）出发，评估每个声明通过证据的支持程度，包括程序支持和实施支持。同时独立评估证据状态，并提供评分策略和指南。

Result: 提出了详细的评分表和评估指南，用于评估声明支持和证据状态，并讨论了治理、持续改进和时间安排等问题。

Conclusion: 该方法为评估安全案例的可信度提供了起点，有助于自动驾驶技术的安全整合和社会接受。

Abstract: As Automated Driving Systems (ADS) technology advances, ensuring safety and
public trust requires robust assurance frameworks, with safety cases emerging
as a critical tool toward such a goal. This paper explores an approach to
assess how a safety case is supported by its claims and evidence, toward
establishing credibility for the overall case. Starting from a description of
the building blocks of a safety case (claims, evidence, and optional
format-dependent entries), this paper delves into the assessment of support of
each claim through the provided evidence. Two domains of assessment are
outlined for each claim: procedural support (formalizing process specification)
and implementation support (demonstrating process application). Additionally,
an assessment of evidence status is also undertaken, independently from the
claims support. Scoring strategies and evaluation guidelines are provided,
including detailed scoring tables for claim support and evidence status
assessment. The paper further discusses governance, continual improvement, and
timing considerations for safety case assessments. Reporting of results and
findings is contextualized within its primary use for internal decision-making
on continual improvement efforts. The presented approach builds on state of the
art auditing practices, but specifically tackles the question of judging the
credibility of a safety case. While not conclusive on its own, it provides a
starting point toward a comprehensive "Case Credibility Assessment" (CCA),
starting from the evaluation of the support for each claim (individually and in
aggregate), as well as every piece of evidence provided. By delving into the
technical intricacies of ADS safety cases, this work contributes to the ongoing
discourse on safety assurance and aims to facilitate the responsible
integration of ADS technology into society.

</details>


### [37] [Microservices and Real-Time Processing in Retail IT: A Review of Open-Source Toolchains and Deployment Strategies](https://arxiv.org/abs/2506.09938)
*Aaditaa Vashisht,Rekha B S*

Main category: cs.SE

TL;DR: 本文综述了现代事件驱动和微服务架构（如Apache Kafka、Spring Boot、MongoDB和Kubernetes）如何变革零售和金融系统，强调其在实时分析、欺诈检测和高可用性方面的作用。


<details>
  <summary>Details</summary>
Motivation: 随着数字化转型的加速，零售行业越来越依赖实时、可扩展和弹性的系统来管理金融交易、分析客户行为和优化订单处理。

Method: 通过系统回顾近年来的学术出版物、技术白皮书和行业报告，本研究综合了关键主题和实施策略。

Result: 分析表明，Kafka和Spring Boot等技术在构建低延迟、事件驱动的应用中至关重要，而MongoDB结合Kubernetes则确保了库存和交易系统的容错性和高可用性。

Conclusion: 这些发现为行业从业者设计可扩展基础设施提供了见解，同时为教育工作者整合现代系统架构到培训中奠定了基础。

Abstract: With the rapid pace of digital transformation, the retail industry is
increasingly depending on real-time, scalable, and resilient systems to manage
financial transactions, analyze customer behavior, and streamline order
processing. This literature review explores how modern event-driven and
microservices-based architectures, particularly those leveraging Apache Kafka,
Spring Boot, MongoDB, and Kubernetes are transforming retail and financial
systems. By systematically reviewing academic publications, technical white
papers, and industry reports from recent years, this study synthesizes key
themes and implementation strategies. The analysis reveals that technologies
like Kafka and Spring Boot are instrumental in building low-latency,
event-driven applications that support real-time analytics and fraud detection,
while MongoDB, when deployed on Kubernetes, ensures fault tolerance and high
availability in inventory and transaction systems. Kubernetes itself plays a
crucial role in automating deployment and scaling of microservices. These
findings provide valuable insights for industry practitioners aiming to design
scalable infrastructures, identify research opportunities in hybrid deployment
models, and offer educators a foundation to integrate modern system
architectures into professional and technical communication training.

</details>
