<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 49]
- [cs.SE](#cs.SE) [Total: 16]
- [cs.CR](#cs.CR) [Total: 14]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Conflict-Driven Clause Learning with VSIDS Heuristics for Discrete Facility Layout](https://arxiv.org/abs/2512.18034)
*Joshua Gibson,Kapil Dhakal*

Main category: cs.AI

TL;DR: 该研究将冲突驱动子句学习(CDCL)与VSIDS启发式算法应用于离散设施布局问题，开发了CNF公式化方法，并比较了CDCL、CP-SAT和MILP的性能。结果显示CDCL在可行性检测方面具有近常数时间复杂度，而CP-SAT和MILP分别呈现多项式和指数级增长。为解决CDCL在目标优化方面的限制，提出了两种混合架构，显著减少了求解时间。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索CDCL算法在具有密集逻辑结构的离散设施布局问题中的应用潜力。设施布局问题包含邻接、分离和槽位可用性等约束，形成复杂的组合分配问题。研究者希望了解CDCL作为计算引擎在这种问题上的性能表现，并与传统的CP-SAT和MILP方法进行比较。

Method: 1. 将设施布局问题建模为具有密集逻辑结构的组合分配问题；2. 开发基于CNF的布局可行性公式化方法；3. 在统一基准框架下比较CDCL、CP-SAT和MILP三种方法；4. 针对CDCL在目标优化方面的局限性，提出两种混合架构：第一种快速枚举可行布局以速度换取最优性，第二种使用CDCL生成热启动解以加速精确优化。

Result: 实验结果显示：CDCL在可行性检测方面表现出近常数时间复杂度，随着问题规模和约束密度的增加，运行时间基本保持稳定；CP-SAT呈现多项式级增长；MILP则呈现指数级增长。混合架构能够显著减少求解时间，同时保持正确性保证，在大型离散布局问题中实现了子句学习搜索与精确优化方法之间的良好权衡。

Conclusion: CDCL在离散设施布局问题的可行性检测方面具有显著优势，表现出优异的可扩展性。通过将CDCL的快速可行性搜索与CP-SAT的精确优化能力相结合，混合架构能够在保持正确性的同时大幅提升求解效率。这为大规模离散布局问题提供了有效的算法解决方案，阐明了子句学习搜索与精确优化方法之间的权衡关系。

Abstract: This paper studies the use of Conflict-Driven Clause Learning (CDCL) with VSIDS heuristics as a computational engine for discrete facility layout problems. The facility layout problem is modeled as a combinatorial assignment problem with dense logical structure arising from adjacency, separation, and slot-availability constraints. We develop a CNF-based formulation for layout feasibility and compare CDCL-based SAT solving against CP-SAT and MILP formulations under a unified benchmarking framework. Empirical results show that CDCL exhibits near-constant runtime behavior for feasibility detection across increasing problem sizes and constraint densities, while CP-SAT and MILP display polynomial and exponential scaling respectively. To address the limitation of CDCL in objective optimization, we introduce two hybrid architectures that combine CDCL-based feasibility search with CP-SAT optimization. The first architecture rapidly enumerates feasible layouts to trade optimality for speed, while the second uses CDCL to generate warm-start solutions that accelerate exact optimization. The results demonstrate that hybrid approaches can significantly reduce time-to-solution while preserving correctness guarantees, clarifying the algorithmic trade-offs between clause-learning search and exact optimization methods in large-scale discrete layout problems.

</details>


### [2] [Faithful and Stable Neuron Explanations for Trustworthy Mechanistic Interpretability](https://arxiv.org/abs/2512.18092)
*Ge Yan,Tuomas Oikarinen,Tsui-Wei,Weng*

Main category: cs.AI

TL;DR: 该论文首次为神经元识别（神经机制可解释性）提供了理论分析框架，解决了忠实性和稳定性两大核心挑战，并提出了带保证覆盖概率的概念预测集方法。


<details>
  <summary>Details</summary>
Motivation: 当前神经元识别方法（如Network Dissection和CLIP-Dissect）虽然经验上成功，但缺乏严格的理论基础，这限制了其可信度和可靠性。研究者观察到神经元识别可视为机器学习的逆过程，从而能够推导出神经元解释的理论保证。

Method: 1) 将神经元识别视为机器学习的逆过程；2) 推导广泛使用的相似性度量（准确率、AUROC、IoU）的泛化界限以保证忠实性；3) 提出bootstrap集成程序量化稳定性，并开发BE方法生成具有保证覆盖概率的概念预测集。

Result: 在合成数据和真实数据上的实验验证了理论结果，证明了方法的实用性。理论分析为神经元识别提供了忠实性和稳定性的理论保证，BE方法能够生成可靠的概念预测集。

Conclusion: 该工作首次为神经元识别提供了理论分析框架，解决了忠实性和稳定性两大挑战，提出了具有理论保证的方法，是迈向可信神经元识别的重要一步。

Abstract: Neuron identification is a popular tool in mechanistic interpretability, aiming to uncover the human-interpretable concepts represented by individual neurons in deep networks. While algorithms such as Network Dissection and CLIP-Dissect achieve great empirical success, a rigorous theoretical foundation remains absent, which is crucial to enable trustworthy and reliable explanations. In this work, we observe that neuron identification can be viewed as the inverse process of machine learning, which allows us to derive guarantees for neuron explanations. Based on this insight, we present the first theoretical analysis of two fundamental challenges: (1) Faithfulness: whether the identified concept faithfully represents the neuron's underlying function and (2) Stability: whether the identification results are consistent across probing datasets. We derive generalization bounds for widely used similarity metrics (e.g. accuracy, AUROC, IoU) to guarantee faithfulness, and propose a bootstrap ensemble procedure that quantifies stability along with BE (Bootstrap Explanation) method to generate concept prediction sets with guaranteed coverage probability. Experiments on both synthetic and real data validate our theoretical results and demonstrate the practicality of our method, providing an important step toward trustworthy neuron identification.

</details>


### [3] [Efficient Mixture-of-Agents Serving via Tree-Structured Routing, Adaptive Pruning, and Dependency-Aware Prefill-Decode Overlap](https://arxiv.org/abs/2512.18126)
*Zijun Wang,Yijiahao Qi,Hanqiu Chen,Zishen Wan,Gongjin Sun,Dongyang Li,Shuyi Pei,Cong Hao*

Main category: cs.AI

TL;DR: 提出一种算法-系统协同设计的MoA推理服务优化方案，通过分层树拓扑、运行时自适应机制和流水线执行，显著降低延迟（最高90%）同时保持准确率（±1%内）。


<details>
  <summary>Details</summary>
Motivation: 传统MoA推理存在密集的智能体间通信和低硬件利用率问题，共同导致服务延迟过高。

Method: 1. 用分层树拓扑替代密集连接图，引入结构化稀疏通信；2. 基于语义一致性和置信度的运行时自适应机制，选择性终止或跳过下游智能体调用；3. 流水线执行，在依赖相关的智能体间重叠增量预填充和解码。

Result: 在代表性任务中，端到端延迟大幅降低（最高90%），同时保持与密集连接MoA基线相当的准确率（±1%内），在某些设置下还能提高准确率。

Conclusion: 通过算法-系统协同设计，有效解决了MoA推理中的通信密集和硬件利用率低的问题，实现了延迟显著降低而准确率基本保持的优化效果。

Abstract: Mixture-of-Agents (MoA) inference can suffer from dense inter-agent communication and low hardware utilization, which jointly inflate serving latency. We present a serving design that targets these bottlenecks through an algorithm-system co-design. First, we replace dense agent interaction graphs with a hierarchical tree topology that induces structured sparsity in inter-agent communication. Second, we introduce a runtime adaptive mechanism that selectively terminates or skips downstream agent invocations using semantic agreement and confidence signals from intermediate outputs. Third, we pipeline agent execution by overlapping incremental prefilling with decoding across dependency-related agents, improving utilization and reducing inference latency. Across representative tasks, this approach substantially reduces end-to-end latency (up to 90%) while maintaining comparable accuracy (within $\pm$1%) relative to dense-connectivity MoA baselines, and can improve accuracy in certain settings.

</details>


### [4] [Unifying Causal Reinforcement Learning: Survey, Taxonomy, Algorithms and Applications](https://arxiv.org/abs/2512.18135)
*Cristiano da Costa Cunha,Wei Liu,Tim French,Ajmal Mian*

Main category: cs.AI

TL;DR: 这篇综述论文系统回顾了因果推断与强化学习的交叉领域，探讨了如何利用因果原理解决传统RL在可解释性、鲁棒性和泛化性方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习基于相关性决策，在面对分布偏移、混杂变量和动态环境时存在局限性，包括低可解释性、缺乏鲁棒性和泛化失败等问题。因果推断与强化学习的结合为解决这些挑战提供了新途径。

Method: 论文采用系统性综述方法，将现有因果强化学习方法分为五大类：因果表示学习、反事实策略优化、离线因果RL、因果迁移学习和因果可解释性，通过结构化分析识别挑战和成功案例。

Result: 通过系统分析，论文识别了因果强化学习领域的主要挑战，强调了在实际应用中的实证成功案例，并讨论了当前存在的开放性问题。

Conclusion: 因果强化学习为解决传统RL的局限性提供了有前景的解决方案，通过显式建模因果关系可以开发出更鲁棒、可泛化和可解释的人工智能系统，论文还指出了未来的研究方向。

Abstract: Integrating causal inference (CI) with reinforcement learning (RL) has emerged as a powerful paradigm to address critical limitations in classical RL, including low explainability, lack of robustness and generalization failures. Traditional RL techniques, which typically rely on correlation-driven decision-making, struggle when faced with distribution shifts, confounding variables, and dynamic environments. Causal reinforcement learning (CRL), leveraging the foundational principles of causal inference, offers promising solutions to these challenges by explicitly modeling cause-and-effect relationships. In this survey, we systematically review recent advancements at the intersection of causal inference and RL. We categorize existing approaches into causal representation learning, counterfactual policy optimization, offline causal RL, causal transfer learning, and causal explainability. Through this structured analysis, we identify prevailing challenges, highlight empirical successes in practical applications, and discuss open problems. Finally, we provide future research directions, underscoring the potential of CRL for developing robust, generalizable, and interpretable artificial intelligence systems.

</details>


### [5] [Propose, Solve, Verify: Self-Play Through Formal Verification](https://arxiv.org/abs/2512.18160)
*Alex Wilf,Pranjal Aggarwal,Bryan Parno,Daniel Fried,Louis-Philippe Morency,Paul Pu Liang,Sean Welleck*

Main category: cs.AI

TL;DR: PSV-Verus：通过形式化验证实现代码生成的自对弈训练框架，在三个基准测试中pass@1提升高达9.6倍


<details>
  <summary>Details</summary>
Motivation: 研究纯自对弈训练（无需人类数据）在大型语言模型中的有效性，特别是在代码生成领域，传统基于单元测试的奖励机制脆弱且容易传播错误

Method: 提出Propose, Solve, Verify (PSV)框架：利用形式化验证信号训练提议器生成具有挑战性的合成问题，并通过专家迭代训练求解器

Result: PSV-Verus在三个基准测试中，pass@1相比仅推理和专家迭代基线提升高达9.6倍；性能随生成问题数量和训练迭代次数而扩展

Conclusion: 形式化验证和难度感知提议是成功自对弈训练的关键要素；PSV框架在验证代码生成场景中有效实现了纯自对弈训练

Abstract: Training models through self-play alone (without any human data) has been a longstanding goal in AI, but its effectiveness for training large language models remains unclear, particularly in code generation where rewards based on unit tests are brittle and prone to error propagation. We study self-play in the verified code generation setting, where formal verification provides reliable correctness signals. We introduce Propose, Solve, Verify (PSV) a simple self-play framework where formal verification signals are used to create a proposer capable of generating challenging synthetic problems and a solver trained via expert iteration. We use PSV to train PSV-Verus, which across three benchmarks improves pass@1 by up to 9.6x over inference-only and expert-iteration baselines. We show that performance scales with the number of generated questions and training iterations, and through ablations identify formal verification and difficulty-aware proposal as essential ingredients for successful self-play.

</details>


### [6] [NEURO-GUARD: Neuro-Symbolic Generalization and Unbiased Adaptive Routing for Diagnostics -- Explainable Medical AI](https://arxiv.org/abs/2512.18177)
*Midhat Urooj,Ayan Banerjee,Sandeep Gupta*

Main category: cs.AI

TL;DR: NEURO-GUARD：一种结合视觉Transformer与语言驱动推理的知识引导视觉框架，通过检索增强生成机制实现自我验证，显著提升医学图像诊断的准确性、可解释性和跨域鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前医学AI面临准确性与可解释性的平衡挑战，现有视觉模型多为黑盒预测，缺乏可解释性且跨域泛化能力差，限制了临床实际应用。特别是在数据有限、视觉线索细微、临床决策高风险的环境中，这一问题尤为突出。

Method: 提出NEURO-GUARD框架，整合视觉Transformer与语言驱动推理。采用检索增强生成机制，让大语言模型基于临床指南和专家知识，迭代生成、评估和优化医学图像特征提取代码，实现自我验证。将符号化医学推理与亚符号化视觉学习相结合。

Result: 在四个糖尿病视网膜病变基准数据集（APTOS、EyePACS、Messidor-1、Messidor-2）上，NEURO-GUARD比纯ViT基线准确率提升6.2%（84.69% vs. 78.4%），跨域泛化能力提升5%。在基于MRI的癫痫检测任务中也表现出优越的跨域鲁棒性，持续超越现有方法。

Conclusion: NEURO-GUARD成功将符号化医学推理与亚符号化视觉学习相结合，实现了可解释、知识感知且可泛化的医学图像诊断，在多个数据集上达到最先进性能，为高风险的临床决策提供了更可靠、透明的AI解决方案。

Abstract: Accurate yet interpretable image-based diagnosis remains a central challenge in medical AI, particularly in settings characterized by limited data, subtle visual cues, and high-stakes clinical decision-making. Most existing vision models rely on purely data-driven learning and produce black-box predictions with limited interpretability and poor cross-domain generalization, hindering their real-world clinical adoption. We present NEURO-GUARD, a novel knowledge-guided vision framework that integrates Vision Transformers (ViTs) with language-driven reasoning to improve performance, transparency, and domain robustness. NEURO-GUARD employs a retrieval-augmented generation (RAG) mechanism for self-verification, in which a large language model (LLM) iteratively generates, evaluates, and refines feature-extraction code for medical images. By grounding this process in clinical guidelines and expert knowledge, the framework progressively enhances feature detection and classification beyond purely data-driven baselines. Extensive experiments on diabetic retinopathy classification across four benchmark datasets APTOS, EyePACS, Messidor-1, and Messidor-2 demonstrate that NEURO-GUARD improves accuracy by 6.2% over a ViT-only baseline (84.69% vs. 78.4%) and achieves a 5% gain in domain generalization. Additional evaluations on MRI-based seizure detection further confirm its cross-domain robustness, consistently outperforming existing methods.
  Overall, NEURO-GUARD bridges symbolic medical reasoning with subsymbolic visual learning, enabling interpretable, knowledge-aware, and generalizable medical image diagnosis while achieving state-of-the-art performance across multiple datasets.

</details>


### [7] [External Hippocampus: Topological Cognitive Maps for Guiding Large Language Model Reasoning](https://arxiv.org/abs/2512.18190)
*Jian Yan*

Main category: cs.AI

TL;DR: 本文提出External Hippocampus框架，从认知动力学视角将语言模型推理建模为语义空间中信息能量的流动，通过降维投影构建拓扑认知地图，实现推理过程中的精确导航和干预，无需额外训练即可显著提升小模型多步推理能力。


<details>
  <summary>Details</summary>
Motivation: 传统权重空间优化方法在语言模型推理中存在局限性，特别是小模型在多步推理中容易出现认知死锁问题。需要一种能够实时干预推理过程、避免高计算成本的方法来提升小模型的推理能力。

Method: 提出External Hippocampus框架，将语言模型推理视为语义空间中信息能量的流动过程。通过降维投影技术构建拓扑认知地图，在测试时对能量流进行精确导航和干预。该方法不需要额外训练，通过温度扰动等技术重启能量流动，解决认知死锁问题。

Result: 在≤7B参数模型上的实验显示：地图引导方法在500个挑战性问题上的准确率达到81.20%（相对基线提升+16.80%），推理时间减少≥15倍。研究发现推理停滞表现为"认知漩涡"和低熵势阱，温度扰动能有效重启能量流动。

Conclusion: External Hippocampus框架为小模型推理提供了一种高效可控的拓扑感知解决方案，无需额外训练，具有自主增长能力，能有效解决多步推理中的认知死锁问题，显著提升推理性能和效率。

Abstract: This paper proposes the External Hippocampus framework, which models language model reasoning from a cognitive dynamics perspective as the flow of information energy in semantic space. Unlike traditional weight-space optimization methods, this framework constructs topological cognitive maps through dimensionality reduction projection, enabling precise navigation and intervention of energy flow at test time while avoiding substantial computational requirements and demonstrating predictable intervention patterns. The method effectively addresses the cognitive deadlock problem in multi-step reasoning for small models. Experiments on models <=7B parameters show: map-guided methods achieve 81.20% accuracy on 500 challenging problems (relative baseline +16.80%), reduce reasoning time by >= 15x, with key findings revealing that reasoning stagnation manifests as "Cognitive Vortex" and low-entropy potential wells, while temperature perturbations effectively restart energy flow. The framework requires no additional training, possesses autonomous growth capability, and provides an efficient and controllable topological-aware solution for small model reasoning.

</details>


### [8] [Sophia: A Persistent Agent Framework of Artificial Life](https://arxiv.org/abs/2512.18202)
*Mingyang Sun,Feng Hong,Weinan Zhang*

Main category: cs.AI

TL;DR: 论文提出System 3框架，为AI智能体添加持续性元认知层，实现身份认同和长期适应，并通过Sophia原型验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM驱动的AI智能体虽然具备感知和推理能力，但缺乏持续性元认知层来维持身份认同、验证推理过程，并协调短期行动与长期生存目标。大多数架构仍然是静态和被动的，局限于手动定义的狭窄场景。

Method: 提出System 3作为第三层级，将心理学概念映射到具体计算模块。开发Sophia原型，包含四个协同机制：过程监督思维搜索、叙事记忆、用户与自我建模、混合奖励系统，形成持续自我改进循环。

Result: 定量：Sophia自主发起并执行内在任务，对重复操作减少80%推理步骤；元认知持续性使高复杂度任务成功率提升40%。定性：System 3展现出连贯的叙事身份和内在任务组织能力。

Conclusion: 通过融合心理学洞见与轻量级强化学习核心，持续性智能体架构为人工生命提供了可行的实践路径，实现了身份连续性和透明的行为解释。

Abstract: The development of LLMs has elevated AI agents from task-specific tools to long-lived, decision-making entities. Yet, most architectures remain static and reactive, tethered to manually defined, narrow scenarios. These systems excel at perception (System 1) and deliberation (System 2) but lack a persistent meta-layer to maintain identity, verify reasoning, and align short-term actions with long-term survival. We first propose a third stratum, System 3, that presides over the agent's narrative identity and long-horizon adaptation. The framework maps selected psychological constructs to concrete computational modules, thereby translating abstract notions of artificial life into implementable design requirements. The ideas coalesce in Sophia, a "Persistent Agent" wrapper that grafts a continuous self-improvement loop onto any LLM-centric System 1/2 stack. Sophia is driven by four synergistic mechanisms: process-supervised thought search, narrative memory, user and self modeling, and a hybrid reward system. Together, they transform repetitive reasoning into a self-driven, autobiographical process, enabling identity continuity and transparent behavioral explanations. Although the paper is primarily conceptual, we provide a compact engineering prototype to anchor the discussion. Quantitatively, Sophia independently initiates and executes various intrinsic tasks while achieving an 80% reduction in reasoning steps for recurring operations. Notably, meta-cognitive persistence yielded a 40% gain in success for high-complexity tasks, effectively bridging the performance gap between simple and sophisticated goals. Qualitatively, System 3 exhibited a coherent narrative identity and an innate capacity for task organization. By fusing psychological insight with a lightweight reinforcement-learning core, the persistent agent architecture advances a possible practical pathway toward artificial life.

</details>


### [9] [MSC-180: A Benchmark for Automated Formal Theorem Proving from Mathematical Subject Classification](https://arxiv.org/abs/2512.18256)
*Sirui Li,Wangyue Lu,Xiaorui Shi,Ke Weng,Haozhe Sun,Minghe Yu,Tiancheng Zhang,Ge Yu,Hengyu Liu,Lun Du*

Main category: cs.AI

TL;DR: MSC-180是一个基于MSC2020数学学科分类的自动定理证明基准测试，包含180个形式化验证问题，覆盖60个数学分支，用于评估LLM定理证明器的领域覆盖和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的定理证明器存在领域覆盖有限和数学推理泛化能力弱的问题，需要更系统、更具区分度的基准测试来推动具有真正数学推理能力的AI系统发展。

Method: 提出MSC-180基准测试，包含180个形式化验证问题（每个数学分支3个问题），覆盖从本科到研究生难度。引入变异系数（CV）作为评估指标来量化跨数学领域的性能变异性。

Result: 在pass@32设置下，最佳模型的总体通过率仅为18.89%，存在显著的领域偏差（最大领域覆盖率41.7%）和难度差距（研究生级别问题通过率显著更低）。CV值比统计高变异性阈值高4-6倍，表明模型仍依赖训练语料的模式匹配而非可迁移的推理机制。

Conclusion: MSC-180及其多维评估框架为驱动下一代具有真正数学推理能力的AI系统发展提供了区分度高、系统化的基准测试，揭示了当前LLM定理证明器在系统泛化能力上的不足。

Abstract: Automated Theorem Proving (ATP) represents a core research direction in artificial intelligence for achieving formal reasoning and verification, playing a significant role in advancing machine intelligence. However, current large language model (LLM)-based theorem provers suffer from limitations such as restricted domain coverage and weak generalization in mathematical reasoning. To address these issues, we propose MSC-180, a benchmark for evaluation based on the MSC2020 mathematical subject classification. It comprises 180 formal verification problems, 3 advanced problems from each of 60 mathematical branches, spanning from undergraduate to graduate levels. Each problem has undergone multiple rounds of verification and refinement by domain experts to ensure formal accuracy. Evaluations of state-of-the-art LLM-based theorem provers under the pass@32 setting reveal that the best model achieves only an 18.89% overall pass rate, with prominent issues including significant domain bias (maximum domain coverage 41.7%) and a difficulty gap (significantly lower pass rates on graduate-level problems). To further quantify performance variability across mathematical domains, we introduce the coefficient of variation (CV) as an evaluation metric. The observed CV values are 4-6 times higher than the statistical high-variability threshold, indicating that the models still rely on pattern matching from training corpora rather than possessing transferable reasoning mechanisms and systematic generalization capabilities. MSC-180, together with its multi-dimensional evaluation framework, provides a discriminative and systematic benchmark for driving the development of next-generation AI systems with genuine mathematical reasoning abilities.

</details>


### [10] [Monitoring Monitorability](https://arxiv.org/abs/2512.18311)
*Melody Y. Guan,Miles Wang,Micah Carroll,Zehao Dou,Annie Y. Wei,Marcus Williams,Benjamin Arnav,Joost Huizinga,Ian Kivlichan,Mia Glaese,Jakub Pachocki,Bowen Baker*

Main category: cs.AI

TL;DR: 论文提出评估AI系统决策可监控性的框架和指标，发现思维链监控优于仅监控行动，大多数前沿模型具有一定可监控性，增加推理计算和思维链长度可提升监控效果。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统能力增强，需要监控其决策过程以确保安全部署。当前思维链监控方法可能在不同训练过程、数据源或系统扩展下变得脆弱，需要系统评估和测量可监控性。

Method: 提出三种评估原型（干预、过程、结果属性）和新监控性指标，建立广泛评估套件。评估不同前沿模型的可监控性，分析推理计算、强化学习优化、预训练模型大小对监控性的影响。

Result: 思维链监控比仅监控行动更有效；大多数前沿模型具有相当但非完美的可监控性；增加推理计算和思维链长度可提升监控性；强化学习优化不会显著降低监控性；弱监控器通过增加测试计算和访问思维链可提升监控能力。

Conclusion: AI系统的可监控性可以通过系统评估框架进行测量和改进，思维链监控是有效的监控方法，监控器的计算资源和访问决策过程信息对提升监控能力至关重要。

Abstract: Observability into the decision making of modern AI systems may be required to safely deploy increasingly capable agents. Monitoring the chain-of-thought (CoT) of today's reasoning models has proven effective for detecting misbehavior. However, this "monitorability" may be fragile under different training procedures, data sources, or even continued system scaling. To measure and track monitorability, we propose three evaluation archetypes (intervention, process, and outcome-property) and a new monitorability metric, and introduce a broad evaluation suite. We demonstrate that these evaluations can catch simple model organisms trained to have obfuscated CoTs, and that CoT monitoring is more effective than action-only monitoring in practical settings. We compare the monitorability of various frontier models and find that most models are fairly, but not perfectly, monitorable. We also evaluate how monitorability scales with inference-time compute, reinforcement learning optimization, and pre-training model size. We find that longer CoTs are generally more monitorable and that RL optimization does not materially decrease monitorability even at the current frontier scale. Notably, we find that for a model at a low reasoning effort, we could instead deploy a smaller model at a higher reasoning effort (thereby matching capabilities) and obtain a higher monitorability, albeit at a higher overall inference compute cost. We further investigate agent-monitor scaling trends and find that scaling a weak monitor's test-time compute when monitoring a strong agent increases monitorability. Giving the weak monitor access to CoT not only improves monitorability, but it steepens the monitor's test-time compute to monitorability scaling trend. Finally, we show we can improve monitorability by asking models follow-up questions and giving their follow-up CoT to the monitor.

</details>


### [11] [Few-Shot Learning of a Graph-Based Neural Network Model Without Backpropagation](https://arxiv.org/abs/2512.18412)
*Mykyta Lapin,Kostiantyn Bokhan,Yurii Parzhyn*

Main category: cs.AI

TL;DR: 提出一种基于结构图的方法，在少样本情况下对轮廓图像进行分类，无需反向传播。通过将图像编码为属性图，形成概念吸引子进行泛化，实现约82%的准确率且决策可解释。


<details>
  <summary>Details</summary>
Motivation: 设计一种无需反向传播的少样本学习架构，使结构成为解释的载体，实现透明决策。通过少量示例（每类5-6个）形成类概念，提供完全可追溯的决策过程。

Method: 1. 轮廓矢量化后构建二分图（点/线作为节点），包含归一化几何属性；2. 通过消除不稳定子结构和对齐关键点路径进行约简；3. 通过样本迭代组合形成概念图；4. 使用近似图编辑距离进行图到概念匹配分类。

Result: 在MNIST子集上（每类5-6个基础示例，单轮训练）获得约82%的一致准确率，决策完全可追溯。与SVM、MLP、CNN以及度量和元学习基线进行了对比，展示了结构相似性可解释误分类。

Conclusion: 结构图方案与概念吸引子实现了无需反向传播的少样本学习，通过显式图结构提供内置解释。局限性包括图编辑距离的计算成本和骨架化质量；未来方向包括分类算法优化、静态场景处理和关联识别。

Abstract: We propose a structural-graph approach to classifying contour images in a few-shot regime without using backpropagation. The core idea is to make structure the carrier of explanations: an image is encoded as an attributed graph (critical points and lines represented as nodes with geometric attributes), and generalization is achieved via the formation of concept attractors (class-level concept graphs). Purpose. To design and experimentally validate an architecture in which class concepts are formed from a handful of examples (5 - 6 per class) through structural and parametric reductions, providing transparent decisions and eliminating backpropagation. Methods. Contour vectorization is followed by constructing a bipartite graph (Point/Line as nodes) with normalized geometric attributes such as coordinates, length, angle, and direction; reductions include the elimination of unstable substructures or noise and the alignment of paths between critical points. Concepts are formed by iterative composition of samples, and classification is performed by selecting the best graph-to-concept match (using approximated GED). Results. On an MNIST subset with 5 - 6 base examples per class (single epoch), we obtain a consistent accuracy of around 82% with full traceability of decisions: misclassifications can be explained by explicit structural similarities. An indicative comparison with SVM, MLP, CNN, as well as metric and meta-learning baselines, is provided. The structural-graph scheme with concept attractors enables few-shot learning without backpropagation and offers built-in explanations through the explicit graph structure. Limitations concern the computational cost of GED and the quality of skeletonization; promising directions include classification-algorithm optimization, work with static scenes, and associative recognition.

</details>


### [12] [Insider Threat Detection Using GCN and Bi-LSTM with Explicit and Implicit Graph Representations](https://arxiv.org/abs/2512.18483)
*Rahul Yumlembam,Biju Issac,Seibu Mary Jacob,Longzhi Yang,Deepa Krishnan*

Main category: cs.AI

TL;DR: 提出一种结合显式和隐式图表示与时间建模的内幕威胁检测框架，通过GCN处理两种图结构，Bi-LSTM捕捉时间依赖，在CERT数据集上取得优异性能。


<details>
  <summary>Details</summary>
Motivation: 内幕威胁检测具有挑战性，因为恶意活动隐蔽且由可信用户执行。现有方法难以捕捉复杂的用户行为模式，需要结合图表示和时间建模来提升检测效果。

Method: 1. 构建显式图：基于组织规则建模用户活动间的直接关系；2. 学习隐式图：使用Gumbel-Softmax技巧从特征相似性中发现潜在行为关系；3. 分别用GCN处理两种图生成节点嵌入；4. 通过注意力机制拼接和精炼特征；5. 使用Bi-LSTM捕捉时间依赖；6. 基于概率阈值标记异常活动。

Result: 在CERT r5.2数据集上：AUC 98.62，检测率100%，误报率0.05；在更难的r6.2数据集上：AUC 88.48，检测率80.15%，误报率0.15，优于现有方法。

Conclusion: 结合图基表示和时间建模能有效提升内幕威胁检测性能，显式和隐式图的融合能更好地捕捉复杂用户行为模式，框架在挑战性数据集上仍保持良好表现。

Abstract: Insider threat detection (ITD) is challenging due to the subtle and concealed nature of malicious activities performed by trusted users. This paper proposes a post-hoc ITD framework that integrates explicit and implicit graph representations with temporal modelling to capture complex user behaviour patterns. An explicit graph is constructed using predefined organisational rules to model direct relationships among user activities. To mitigate noise and limitations in this hand-crafted structure, an implicit graph is learned from feature similarities using the Gumbel-Softmax trick, enabling the discovery of latent behavioural relationships. Separate Graph Convolutional Networks (GCNs) process the explicit and implicit graphs to generate node embeddings, which are concatenated and refined through an attention mechanism to emphasise threat-relevant features. The refined representations are then passed to a bidirectional Long Short-Term Memory (Bi-LSTM) network to capture temporal dependencies in user behaviour. Activities are flagged as anomalous when their probability scores fall below a predefined threshold. Extensive experiments on CERT r5.2 and r6.2 datasets demonstrate that the proposed framework outperforms state-of-the-art methods. On r5.2, the model achieves an AUC of 98.62, a detection rate of 100%, and a false positive rate of 0.05. On the more challenging r6.2 dataset, it attains an AUC of 88.48, a detection rate of 80.15%, and a false positive rate of 0.15, highlighting the effectiveness of combining graph-based and temporal representations for robust ITD.

</details>


### [13] [Large Language Models as Discounted Bayesian Filters](https://arxiv.org/abs/2512.18489)
*Jensen Zhang,Jing Yang,Keze Wang*

Main category: cs.AI

TL;DR: LLMs在动态随机环境中的在线推理能力评估：研究发现LLM的信念更新类似于贝叶斯后验，但更准确地表现为指数遗忘滤波器，存在系统性的旧证据折扣现象


<details>
  <summary>Details</summary>
Motivation: 虽然LLMs通过上下文学习展现出强大的少样本泛化能力，但它们在动态随机环境中的推理过程仍然不透明。先前研究主要关注静态任务，忽略了当信念需要持续更新时的在线适应能力，而这是LLMs作为世界模型或智能体的关键能力

Method: 引入贝叶斯滤波框架来评估LLMs的在线推理能力，使用概率探测套件涵盖多元离散分布（如骰子投掷）和连续分布（如高斯过程），其中真实参数随时间变化

Result: 发现LLM的信念更新类似于贝叶斯后验，但更准确地表现为指数遗忘滤波器，具有模型特定的折扣因子（小于1），显示出对旧证据的系统性折扣，这种现象在不同模型架构间差异显著。虽然固有先验常常校准不当，但更新机制本身保持结构化和原则性

Conclusion: 在模拟智能体任务中验证了这些发现，并提出了有效的提示策略，能够以最小的计算成本重新校准先验。这表明LLMs的在线推理机制具有结构化特征，可以通过适当的干预进行改进

Abstract: Large Language Models (LLMs) demonstrate strong few-shot generalization through in-context learning, yet their reasoning in dynamic and stochastic environments remains opaque. Prior studies mainly focus on static tasks and overlook the online adaptation required when beliefs must be continuously updated, which is a key capability for LLMs acting as world models or agents. We introduce a Bayesian filtering framework to evaluate online inference in LLMs. Our probabilistic probe suite spans both multivariate discrete distributions, such as dice rolls, and continuous distributions, such as Gaussian processes, where ground-truth parameters shift over time. We find that while LLM belief updates resemble Bayesian posteriors, they are more accurately characterized by an exponential forgetting filter with a model-specific discount factor smaller than one. This reveals systematic discounting of older evidence that varies significantly across model architectures. Although inherent priors are often miscalibrated, the updating mechanism itself remains structured and principled. We further validate these findings in a simulated agent task and propose prompting strategies that effectively recalibrate priors with minimal computational cost.

</details>


### [14] [Vox Deorum: A Hybrid LLM Architecture for 4X / Grand Strategy Game AI -- Lessons from Civilization V](https://arxiv.org/abs/2512.18564)
*John Chen,Sihan Cheng,Can Gurkan,Ryan Lay,Moez Salahuddin*

Main category: cs.AI

TL;DR: 论文提出Vox Deorum架构，将大语言模型与游戏AI子系统结合，用于《文明V》4X策略游戏，实现宏观战略推理与战术执行的分离。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在自然语言推理方面的能力使其在4X和大战略游戏中具有独特优势，能够实现更自然的人机交互如协作和谈判。但由于游戏复杂性和长期性，以及延迟和成本因素，LLMs在实际部署中面临挑战。

Method: 提出Vox Deorum混合架构（LLM+X），采用分层技术设计：LLM负责宏观战略推理，将战术执行委托给子系统（如算法AI或未来的强化学习AI）。在《文明V》Vox Populi模组上进行验证。

Result: 通过2,327场完整游戏测试，比较两个开源LLM与Vox Populi增强AI。结果显示LLM能够实现有竞争力的端到端游戏玩法，同时展现出与算法AI显著不同且彼此各异的游戏风格。

Conclusion: 该工作为在商业4X游戏中集成LLM建立了可行的架构，为游戏设计和智能体AI研究开辟了新机会。

Abstract: Large Language Models' capacity to reason in natural language makes them uniquely promising for 4X and grand strategy games, enabling more natural human-AI gameplay interactions such as collaboration and negotiation. However, these games present unique challenges due to their complexity and long-horizon nature, while latency and cost factors may hinder LLMs' real-world deployment. Working on a classic 4X strategy game, Sid Meier's Civilization V with the Vox Populi mod, we introduce Vox Deorum, a hybrid LLM+X architecture. Our layered technical design empowers LLMs to handle macro-strategic reasoning, delegating tactical execution to subsystems (e.g., algorithmic AI or reinforcement learning AI in the future). We validate our approach through 2,327 complete games, comparing two open-source LLMs with a simple prompt against Vox Populi's enhanced AI. Results show that LLMs achieve competitive end-to-end gameplay while exhibiting play styles that diverge substantially from algorithmic AI and from each other. Our work establishes a viable architecture for integrating LLMs in commercial 4X games, opening new opportunities for game design and agentic AI research.

</details>


### [15] [ESearch-R1: Learning Cost-Aware MLLM Agents for Interactive Embodied Search via Reinforcement Learning](https://arxiv.org/abs/2512.18571)
*Weijie Zhou,Xuangtang Xiong,Ye Tian,Lijun Yue,Xinyu Wu,Wei Li,Chaoyang Zhao,Honghui Dong,Ming Tang,Jinqiao Wang,Zhengyou Zhang*

Main category: cs.AI

TL;DR: ESearch-R1：一个成本感知的具身推理框架，通过HC-GRPO算法优化MLLM智能体在模糊指令下的决策，平衡物理探索与人类交互成本，显著提升任务成功率并降低操作成本。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型（MLLMs）驱动的具身智能体在面对模糊自然语言指令时，无法有效平衡物理探索的高成本与人类交互的认知成本，通常将消歧视为被动感知问题，缺乏最小化总任务执行成本的策略推理能力。

Method: 提出ESearch-R1框架，将交互对话（Ask）、情景记忆检索（GetMemory）和物理导航（Navigate）统一到单一决策过程中。引入HC-GRPO（异构成本感知群体相对策略优化）算法，通过采样推理轨迹群体并强化那些在信息增益与异构成本（如导航时间、人类注意力）之间达到最优权衡的轨迹来优化MLLM。

Result: 在AI2-THOR环境中的大量实验表明，ESearch-R1显著优于标准的基于ReAct的智能体，提高了任务成功率，同时将总操作成本降低了约50%，验证了GRPO在使MLLM智能体与物理世界约束对齐方面的有效性。

Conclusion: ESearch-R1框架通过成本感知的具身推理，有效解决了模糊指令下的决策优化问题，为具身智能体在现实世界中的高效操作提供了新的解决方案。

Abstract: Multimodal Large Language Models (MLLMs) have empowered embodied agents with remarkable capabilities in planning and reasoning. However, when facing ambiguous natural language instructions (e.g., "fetch the tool" in a cluttered room), current agents often fail to balance the high cost of physical exploration against the cognitive cost of human interaction. They typically treat disambiguation as a passive perception problem, lacking the strategic reasoning to minimize total task execution costs. To bridge this gap, we propose ESearch-R1, a cost-aware embodied reasoning framework that unifies interactive dialogue (Ask), episodic memory retrieval (GetMemory), and physical navigation (Navigate) into a single decision process. We introduce HC-GRPO (Heterogeneous Cost-Aware Group Relative Policy Optimization). Unlike traditional PPO which relies on a separate value critic, HC-GRPO optimizes the MLLM by sampling groups of reasoning trajectories and reinforcing those that achieve the optimal trade-off between information gain and heterogeneous costs (e.g., navigate time, and human attention). Extensive experiments in AI2-THOR demonstrate that ESearch-R1 significantly outperforms standard ReAct-based agents. It improves task success rates while reducing total operational costs by approximately 50\%, validating the effectiveness of GRPO in aligning MLLM agents with physical world constraints.

</details>


### [16] [Reflective Confidence: Correcting Reasoning Flaws via Online Self-Correction](https://arxiv.org/abs/2512.18605)
*Qinglin Zeng,Jing Yang,Keze Wang*

Main category: cs.AI

TL;DR: 提出Reflective Confidence框架，将低置信度信号从终止指标转变为反思触发器，通过主动自我修正而非被动丢弃来提高推理效率


<details>
  <summary>Details</summary>
Motivation: 现有基于集成的方法（如self-consistency）计算开销大，早期停止策略（如DeepConf）虽然降低成本但会丢弃不完全的推理路径，浪费部分计算资源

Method: 提出reflective confidence框架：当置信度低于阈值时，不停止生成，而是生成反思提示来分析当前推理状态，识别潜在错误，并沿着修正后的轨迹继续生成

Result: 在数学推理基准测试（包括AIME 2025）上，相比先进的早期停止基线方法，在可比较的计算成本下实现了显著的准确率提升

Conclusion: 验证了主动自我修正相对于被动丢弃的有效性，为LLM推理效率优化提供了新思路

Abstract: Large language models (LLMs) have achieved strong performance on complex reasoning tasks using techniques such as chain-of-thought and self-consistency. However, ensemble-based approaches, especially self-consistency which relies on multiple reasoning trajectories, often incur substantial computational overhead. To improve efficiency, prior work has leveraged internal confidence signals, where early stopping strategies such as DeepConf reduce cost by terminating low-confidence trajectories. However, this strategy discards incomplete reasoning paths and wastes partial computation.
  We propose reflective confidence, a novel reasoning framework that transforms low-confidence signals from termination indicators into reflection triggers. When confidence falls below a threshold, instead of stopping generation, the model produces a reflection prompt to analyze the current reasoning state, identify potential errors, and continue generation along a corrected trajectory. Experiments on mathematical reasoning benchmarks, including AIME 2025, demonstrate significant accuracy improvements over advanced early-stopping baselines at comparable computational cost, validating the effectiveness of proactive self-correction over passive discarding.

</details>


### [17] [Assignment-Routing Optimization: Solvers for Problems Under Constraints](https://arxiv.org/abs/2512.18618)
*Yuan Qilong,Michal Pavelka*

Main category: cs.AI

TL;DR: 提出针对联合路由分配问题的混合整数规划求解器，扩展了现有精确求解方法，适用于具有丰富约束的实际包装规划场景，在机器人包装应用中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 研究联合路由分配问题，需要同时解决物品与占位符的一对一分配以及访问所有节点的哈密顿回路确定。现有方法在实际包装规划场景中面临多种约束挑战，如多占位符选项、时间限制和多类别物品包装等。

Method: 扩展了基于Gurobi的精确混合整数规划求解器，结合割平面子回路消除技术，开发了专门针对实际包装规划场景的求解器，能够处理多占位符选项、时间框架限制和多类别物品包装等丰富约束。

Result: 在46个移动操作数据集上的实验表明，所提出的MIP方法能够获得全局最优解，计算时间稳定且较低，比基于抖动的精确求解器性能提升高达一个数量级。与贪心基线相比，MIP解决方案实现了稳定的最优距离，简单启发式方法的平均偏差为14%。

Conclusion: 研究结果突显了基于MIP的JRA优化在机器人包装、运动规划和复杂物流中的实际适用性，证明了该方法在效率和解决方案质量方面的优势。

Abstract: We study the Joint Routing-Assignment (JRA) problem in which items must be assigned one-to-one to placeholders while simultaneously determining a Hamiltonian cycle visiting all nodes exactly once. Extending previous exact MIP solvers with Gurobi and cutting-plane subtour elimination, we develop a solver tailored for practical packaging-planning scenarios with richer constraints.These include multiple placeholder options, time-frame restrictions, and multi-class item packaging. Experiments on 46 mobile manipulation datasets demonstrate that the proposed MIP approach achieves global optima with stable and low computation times, significantly outperforming the shaking-based exact solver by up to an orders of magnitude. Compared to greedy baselines, the MIP solutions achieve consistent optimal distances with an average deviation of 14% for simple heuristics, confirming both efficiency and solution quality. The results highlight the practical applicability of MIP-based JRA optimization for robotic packaging, motion planning, and complex logistics .

</details>


### [18] [ChronoDreamer: Action-Conditioned World Model as an Online Simulator for Robotic Planning](https://arxiv.org/abs/2512.18619)
*Zhenhao Zhou,Dan Negrut*

Main category: cs.AI

TL;DR: ChronoDreamer是一个用于接触丰富的机器人操作的动作条件世界模型，通过空间-时间变换器和MaskGIT风格的掩码预测来预测未来视频帧、接触分布和关节角度，并使用视觉语言模型进行推理以在动作执行前拒绝不安全动作。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够预测接触丰富的机器人操作场景的世界模型，通过预测未来状态和接触分布来确保操作安全，避免碰撞和危险情况。

Method: 使用空间-时间变换器进行MaskGIT风格的掩码预测，将接触编码为深度加权高斯泼溅图像，在推理时使用视觉语言模型评估预测的轨迹，通过拒绝采样排除不安全动作。

Result: 模型在非接触运动中保持空间一致性，生成合理的接触预测，基于LLM的评估器能够区分碰撞与非碰撞轨迹。

Conclusion: ChronoDreamer为接触丰富的机器人操作提供了一个有效的世界模型框架，能够预测未来状态并确保操作安全性，在DreamerBench数据集上表现出良好性能。

Abstract: We present ChronoDreamer, an action-conditioned world model for contact-rich robotic manipulation. Given a history of egocentric RGB frames, contact maps, actions, and joint states, ChronoDreamer predicts future video frames, contact distributions, and joint angles via a spatial-temporal transformer trained with MaskGIT-style masked prediction. Contact is encoded as depth-weighted Gaussian splat images that render 3D forces into a camera-aligned format suitable for vision backbones. At inference, predicted rollouts are evaluated by a vision-language model that reasons about collision likelihood, enabling rejection sampling of unsafe actions before execution. We train and evaluate on DreamerBench, a simulation dataset generated with Project Chrono that provides synchronized RGB, contact splat, proprioception, and physics annotations across rigid and deformable object scenarios. Qualitative results demonstrate that the model preserves spatial coherence during non-contact motion and generates plausible contact predictions, while the LLM-based judge distinguishes collision from non-collision trajectories.

</details>


### [19] [ASTIF: Adaptive Semantic-Temporal Integration for Cryptocurrency Price Forecasting](https://arxiv.org/abs/2512.18661)
*Hafiz Saif Ur Rehman,Ling Liu,Kaleem Ullah Qasim*

Main category: cs.AI

TL;DR: ASTIF是一个用于加密货币价格预测的混合智能系统，通过基于置信度的元学习实时调整预测策略，融合语义市场信号和时序模式。


<details>
  <summary>Details</summary>
Motivation: 现有金融时间序列预测模型大多采用静态架构，难以整合异构知识源或适应快速的市场状态变化。传统方法仅依赖历史价格序列，忽略了政策不确定性和市场叙事等语义驱动因素。

Method: 提出ASTIF框架，包含三个互补组件：1）使用MirrorPrompt的双通道小语言模型提取语义市场信号和数值趋势；2）混合LSTM随机森林模型捕捉序列时间依赖性；3）基于置信度的元学习器作为自适应推理层，根据实时不确定性调节各预测器的贡献。

Result: 在2020-2024年AI相关加密货币和主要科技股的多样化数据集上，ASTIF优于领先的深度学习和Transformer基线模型（如Informer、TFT）。消融研究证实了自适应元学习机制的关键作用，能在市场动荡期间成功通过切换语义和时序通道的依赖来降低风险。

Conclusion: 该研究为非平稳环境中融合定量和定性数据提供了一个可扩展的、基于知识的解决方案，通过自适应语义-时序整合改进了金融时间序列预测。

Abstract: Financial time series forecasting is fundamentally an information fusion challenge, yet most existing models rely on static architectures that struggle to integrate heterogeneous knowledge sources or adjust to rapid regime shifts. Conventional approaches, relying exclusively on historical price sequences, often neglect the semantic drivers of volatility such as policy uncertainty and market narratives. To address these limitations, we propose the ASTIF (Adaptive Semantic-Temporal Integration for Cryptocurrency Price Forecasting), a hybrid intelligent system that adapts its forecasting strategy in real time through confidence-based meta-learning. The framework integrates three complementary components. A dual-channel Small Language Model using MirrorPrompt extracts semantic market cues alongside numerical trends. A hybrid LSTM Random Forest model captures sequential temporal dependencies. A confidence-aware meta-learner functions as an adaptive inference layer, modulating each predictor's contribution based on its real-time uncertainty.
  Experimental evaluation on a diverse dataset of AI-focused cryptocurrencies and major technology stocks from 2020 to 2024 shows that ASTIF outperforms leading deep learning and Transformer baselines (e.g., Informer, TFT). The ablation studies further confirm the critical role of the adaptive meta-learning mechanism, which successfully mitigates risk by shifting reliance between semantic and temporal channels during market turbulence. The research contributes a scalable, knowledge-based solution for fusing quantitative and qualitative data in non-stationary environments.

</details>


### [20] [Automatic Adaptation to Concept Complexity and Subjective Natural Concepts: A Cognitive Model based on Chunking](https://arxiv.org/abs/2512.18665)
*Dmitry Bennett,Fernand Gobet*

Main category: cs.AI

TL;DR: CogAct计算模型通过组块化机制解释概念学习，能自适应学习从简单逻辑函数到文学、国际象棋和音乐等复杂领域的原始概念，相比其他模型具有更好的适应性。


<details>
  <summary>Details</summary>
Motivation: 认知科学的关键问题在于理解支撑短期和长期记忆中多种概念形成与检索的基本心理过程。研究者认为组块化机制在其中扮演重要角色，需要建立能将这些过程与概念学习联系起来的计算模型。

Method: 提出CogAct计算模型，将概念学习建立在组块化、注意力、短期记忆和长期记忆等基本认知过程和结构上。模型能够自适应学习多种类型的概念，从简单逻辑函数到人工类别，再到文学、国际象棋和音乐等不同领域的原始概念数据。

Result: CogAct模型成功实现了自适应学习，能够处理其他心理模型难以应对的复杂自然概念。模型在模拟个体人类参与者的主观概念空间方面表现出色，特别是在音乐领域，能够从原始乐谱数据中学习而不依赖预建知识结构。与深度学习模型相比，CogAct在概念学习方面显示出优势。

Conclusion: CogAct模型将概念学习和复杂适应性与更广泛的认知心理学理论相整合。该方法不仅推进了概念学习的理论理解，还为心理学应用提供了新方向，即从建模平均参与者转向捕捉个体的主观概念空间。

Abstract: A key issue in cognitive science concerns the fundamental psychological processes that underlie the formation and retrieval of multiple types of concepts in short-term and long-term memory (STM and LTM, respectively). We propose that chunking mechanisms play an essential role and show how the CogAct computational model grounds concept learning in fundamental cognitive processes and structures (such as chunking, attention, STM and LTM). First are the in-principle demonstrations, with CogAct automatically adapting to learn a range of categories from simple logical functions, to artificial categories, to natural raw (as opposed to natural pre-processed) concepts in the dissimilar domains of literature, chess and music. This kind of adaptive learning is difficult for most other psychological models, e.g., with cognitive models stopping at modelling artificial categories and (non-GPT) models based on deep learning requiring task-specific changes to the architecture. Secondly, we offer novel ways of designing human benchmarks for concept learning experiments and simulations accounting for subjectivity, ways to control for individual human experiences, all while keeping to real-life complex categories. We ground CogAct in simulations of subjective conceptual spaces of individual human participants, capturing humans subjective judgements in music, with the models learning from raw music score data without bootstrapping to pre-built knowledge structures. The CogAct simulations are compared to those obtained by a deep-learning model. These findings integrate concept learning and adaptation to complexity into the broader theories of cognitive psychology. Our approach may also be used in psychological applications that move away from modelling the average participant and towards capturing subjective concept space.

</details>


### [21] [IntelliCode: A Multi-Agent LLM Tutoring System with Centralized Learner Modeling](https://arxiv.org/abs/2512.18669)
*Jones David,Shreya Ghosh*

Main category: cs.AI

TL;DR: IntelliCode是一个基于多智能体LLM的智能辅导系统，通过中心化版本化学习者状态实现长期、透明、可审计的教学支持，包含六个专门智能体协同工作。


<details>
  <summary>Details</summary>
Motivation: 当前LLM导师通常是单轮对话助手，缺乏对学习者知识的持久表示，难以提供有原则、透明和长期的教学支持。

Method: 构建基于中心化版本化学习者状态的多智能体LLM辅导系统，包含六个专门智能体：技能评估、学习者画像、渐进提示、课程选择、间隔重复和参与度监控，通过StateGraph编排器协调工作。

Result: 系统展示了端到端辅导流程，通过模拟学习者验证显示稳定的状态更新、渐进提示提高任务成功率以及多样化的课程覆盖。

Conclusion: IntelliCode展示了如何将持久学习者建模、编排的多智能体推理和有原则的教学设计相结合，产生透明可靠的LLM驱动辅导。

Abstract: LLM-based tutors are typically single-turn assistants that lack persistent representations of learner knowledge, making it difficult to provide principled, transparent, and long-term pedagogical support. We introduce IntelliCode, a multi-agent LLM tutoring system built around a centralized, versioned learner state that integrates mastery estimates, misconceptions, review schedules, and engagement signals. A StateGraph Orchestrator coordinates six specialized agents: skill assessment, learner profiling, graduated hinting, curriculum selection, spaced repetition, and engagement monitoring, each operating as a pure transformation over the shared state under a single-writer policy. This architecture enables auditable mastery updates, proficiency-aware hints, dependency-aware curriculum adaptation, and safety-aligned prompting.
  The demo showcases an end-to-end tutoring workflow: a learner attempts a DSA problem, receives a conceptual hint when stuck, submits a corrected solution, and immediately sees mastery updates and a personalized review interval. We report validation results with simulated learners, showing stable state updates, improved task success with graduated hints, and diverse curriculum coverage. IntelliCode demonstrates how persistent learner modeling, orchestrated multi-agent reasoning, and principled instructional design can be combined to produce transparent and reliable LLM-driven tutoring.

</details>


### [22] [Social Comparison without Explicit Inference of Others' Reward Values: A Constructive Approach Using a Probabilistic Generative Model](https://arxiv.org/abs/2512.18687)
*Yosuke Taniuchi,Chie Hieida,Atsushi Noritake,Kazushi Ikeda,Masaki Isoda*

Main category: cs.AI

TL;DR: 该研究通过计算建模探讨猴子如何进行社会比较，发现猴子更依赖客观奖励差异而非推断同伴的主观价值评估。


<details>
  <summary>Details</summary>
Motivation: 社会比较在灵长类社会认知中起基础作用，但尚不清楚从计算角度，关于他人奖励的信息如何影响个体对自己奖励的评估。本研究旨在探究猴子是仅仅识别客观奖励差异，还是推断他人的主观奖励估值。

Method: 开发了三种不同社会信息处理程度的计算模型：IPM（推断同伴主观价值）、NCM（忽略同伴信息）、ECM（直接纳入同伴客观奖励）。使用多层多模态潜在狄利克雷分配测试模型性能，在包含一对猴子行为、奖励和条件刺激的数据集上训练模型，评估模型在预定义实验条件下分类主观价值的能力。

Result: ECM在Rand Index上获得最高分类分数（0.88 vs IPM的0.79），表明社会比较更依赖于客观奖励差异，而非对主观状态的推断。

Conclusion: 猴子的社会比较过程主要基于客观奖励差异，而非推断他人的主观价值评估，这为理解灵长类社会认知的计算机制提供了新见解。

Abstract: Social comparison -- the process of evaluating one's rewards relative to others -- plays a fundamental role in primate social cognition. However, it remains unknown from a computational perspective how information about others' rewards affects the evaluation of one's own reward. With a constructive approach, this study examines whether monkeys merely recognize objective reward differences or, instead, infer others' subjective reward valuations. We developed three computational models with varying degrees of social information processing: an Internal Prediction Model (IPM), which infers the partner's subjective values; a No Comparison Model (NCM), which disregards partner information; and an External Comparison Model (ECM), which directly incorporates the partner's objective rewards. To test model performance, we used a multi-layered, multimodal latent Dirichlet allocation. We trained the models on a dataset containing the behavior of a pair of monkeys, their rewards, and the conditioned stimuli. Then, we evaluated the models' ability to classify subjective values across pre-defined experimental conditions. The ECM achieved the highest classification score in the Rand Index (0.88 vs. 0.79 for the IPM) under our settings, suggesting that social comparison relies on objective reward differences rather than inferences about subjective states.

</details>


### [23] [KeenKT: Knowledge Mastery-State Disambiguation for Knowledge Tracing](https://arxiv.org/abs/2512.18709)
*Zhifei Li,Lifan Chen,Jiali Yi,Xiaoju Hou,Yue Zhao,Wenxin Huang,Miao Zhang,Kui Xiao,Bing Yang*

Main category: cs.AI

TL;DR: KeenKT模型使用正态逆高斯分布表示学生知识状态，通过NIG距离注意力机制建模状态演化，结合扩散去噪和对比学习提升鲁棒性，在六个数据集上显著超越现有知识追踪方法。


<details>
  <summary>Details</summary>
Motivation: 当前知识追踪方法主要依赖单点估计，无法区分学生真实能力与偶然表现（如爆发或粗心），导致对知识掌握程度的判断存在模糊性。需要一种能够捕捉学生学习行为波动的方法来更准确地建模知识状态。

Method: 提出KeenKT模型：1）使用正态逆高斯分布表示每个交互时的学生知识状态；2）设计基于NIG距离的注意力机制来建模知识状态的动态演化；3）引入扩散去噪重构损失和分布对比学习损失来增强模型鲁棒性。

Result: 在六个公开数据集上的实验表明，KeenKT在预测准确性和对行为波动的敏感性方面均优于现有最先进的知识追踪模型。最大AUC提升5.85%，最大ACC提升6.89%。

Conclusion: KeenKT通过概率分布表示知识状态，有效区分了真实能力与偶然表现，提高了知识追踪的准确性和鲁棒性，为动态建模学生学习过程提供了更精细的方法。

Abstract: Knowledge Tracing (KT) aims to dynamically model a student's mastery of knowledge concepts based on their historical learning interactions. Most current methods rely on single-point estimates, which cannot distinguish true ability from outburst or carelessness, creating ambiguity in judging mastery. To address this issue, we propose a Knowledge Mastery-State Disambiguation for Knowledge Tracing model (KeenKT), which represents a student's knowledge state at each interaction using a Normal-Inverse-Gaussian (NIG) distribution, thereby capturing the fluctuations in student learning behaviors. Furthermore, we design an NIG-distance-based attention mechanism to model the dynamic evolution of the knowledge state. In addition, we introduce a diffusion-based denoising reconstruction loss and a distributional contrastive learning loss to enhance the model's robustness. Extensive experiments on six public datasets demonstrate that KeenKT outperforms SOTA KT models in terms of prediction accuracy and sensitivity to behavioral fluctuations. The proposed method yields the maximum AUC improvement of 5.85% and the maximum ACC improvement of 6.89%.

</details>


### [24] [Counterfactual Basis Extension and Representational Geometry: An MDL-Constrained Model of Conceptual Growth](https://arxiv.org/abs/2512.18732)
*Chainarong Amornbunchornvej*

Main category: cs.AI

TL;DR: 提出一个几何框架，将概念增长建模为在最小描述长度准则下评估的容许基扩展，为概念创新提供保守解释。


<details>
  <summary>Details</summary>
Motivation: 现有学习和推理模型通常预设固定的表征基础，但概念学习只有在现有表征无法解释经验时才可能发生。本文要解决一个更根本的问题：在什么结构条件下，表征基础本身能够以原则性和选择性的方式扩展？

Method: 提出几何框架，将概念增长建模为在最小描述长度准则下评估的容许基扩展。经验被表示为相对于当前概念子空间的向量，残差分量捕获系统性表征失败，候选概念扩展被限制为低秩、容许变换。

Result: 证明任何MDL接受的扩展都可以选择使其新方向完全位于经验诱导的残差跨度内，而与此跨度正交的扩展会严格增加描述长度因此被拒绝。这为想象和概念创新提供了保守解释。

Conclusion: 该框架将概念发展表征为误差驱动、几何约束的基扩展过程，阐明了想象在学习和理论变革中的作用和限制，区分了表征反事实与因果或价值层面的反事实。

Abstract: Concept learning becomes possible only when existing representations fail to account for experience. Most models of learning and inference, however, presuppose a fixed representational basis within which belief updating occurs. In this paper, I address a prior question: under what structural conditions can the representational basis itself expand in a principled and selective way?
  I propose a geometric framework in which conceptual growth is modeled as admissible basis extension evaluated under a Minimum Description Length (MDL) criterion. Experience, whether externally observed or internally simulated, is represented as vectors relative to a current conceptual subspace. Residual components capture systematic representational failure, and candidate conceptual extensions are restricted to low-rank, admissible transformations. I show that any MDL-accepted extension can be chosen so that its novel directions lie entirely within the residual span induced by experience, while extensions orthogonal to this span strictly increase description length and are therefore rejected.
  This yields a conservative account of imagination and conceptual innovation. Internally generated counterfactual representations contribute to learning only insofar as they expose or amplify structured residual error, and cannot introduce arbitrary novelty. I further distinguish representational counterfactuals--counterfactuals over an agent's conceptual basis--from causal or value-level counterfactuals, and show how MDL provides a normative selection principle governing representational change.
  Overall, the framework characterizes conceptual development as an error-driven, geometry-constrained process of basis extension, clarifying both the role and the limits of imagination in learning and theory change.

</details>


### [25] [MEEA: Mere Exposure Effect-Driven Confrontational Optimization for LLM Jailbreaking](https://arxiv.org/abs/2512.18755)
*Jianyi Zhang,Shizhao Liu,Ziyin Zhou,Zhen Li*

Main category: cs.AI

TL;DR: MEEA是一种基于纯粹曝光效应的心理学启发式自动化黑盒攻击框架，通过重复低毒性语义暴露逐步侵蚀LLMs的安全边界，在多轮对话中实现更高的攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 现有越狱研究大多假设静态安全边界，未能考虑上下文交互对模型行为的动态影响，导致攻击的稳定性和泛化性有限。需要开发能够评估多轮安全鲁棒性的方法。

Method: MEEA利用纯粹曝光效应，通过构建语义渐进提示链，并使用基于语义相似度、毒性和越狱效果的模拟退火策略进行优化，实现逐步侵蚀模型安全对齐约束。

Result: 在GPT-4、Claude-3.5、DeepSeek-R1等闭源和开源模型上的实验表明，MEEA比7个代表性基线方法平均攻击成功率提升超过20%，验证了退火优化和上下文暴露机制的必要性。

Conclusion: LLM安全行为本质上是动态且历史依赖的，挑战了静态对齐边界的常见假设，强调需要交互感知的安全评估和防御机制。

Abstract: The rapid advancement of large language models (LLMs) has intensified concerns about the robustness of their safety alignment. While existing jailbreak studies explore both single-turn and multi-turn strategies, most implicitly assume a static safety boundary and fail to account for how contextual interactions dynamically influence model behavior, leading to limited stability and generalization. Motivated by this gap, we propose MEEA (Mere Exposure Effect Attack), a psychology-inspired, fully automated black-box framework for evaluating multi-turn safety robustness, grounded in the mere exposure effect. MEEA leverages repeated low-toxicity semantic exposure to induce a gradual shift in a model's effective safety threshold, enabling progressive erosion of alignment constraints over sustained interactions. Concretely, MEEA constructs semantically progressive prompt chains and optimizes them using a simulated annealing strategy guided by semantic similarity, toxicity, and jailbreak effectiveness. Extensive experiments on both closed-source and open-source models, including GPT-4, Claude-3.5, and DeepSeek-R1, demonstrate that MEEA consistently achieves higher attack success rates than seven representative baselines, with an average Attack Success Rate (ASR) improvement exceeding 20%. Ablation studies further validate the necessity of both annealing-based optimization and contextual exposure mechanisms. Beyond improved attack effectiveness, our findings indicate that LLM safety behavior is inherently dynamic and history-dependent, challenging the common assumption of static alignment boundaries and highlighting the need for interaction-aware safety evaluation and defense mechanisms. Our code is available at: https://github.com/Carney-lsz/MEEA

</details>


### [26] [HARBOR: Holistic Adaptive Risk assessment model for BehaviORal healthcare](https://arxiv.org/abs/2512.18829)
*Aditya Siddhant*

Main category: cs.AI

TL;DR: HARBOR模型在行为健康风险评估中超越传统方法和现成LLM，准确率达69%，专门用于预测-3到+3的Harbor风险评分


<details>
  <summary>Details</summary>
Motivation: 行为健康风险评估面临挑战，因为患者数据高度多模态且情绪障碍具有时间动态性。虽然大语言模型展现出强大推理能力，但它们在结构化临床风险评分中的效果尚不明确。

Method: 引入HARBOR模型（行为健康感知语言模型），用于预测离散的情绪和风险评分（Harbor风险评分，HRS），评分范围为-3（严重抑郁）到+3（躁狂）。同时发布PEARL数据集，包含三名患者四年间的月度观察数据，涵盖生理、行为和自我报告的心理健康信号。

Result: HARBOR在多个评估设置和消融实验中表现出色，准确率达到69%，而逻辑回归为54%，最强的专有LLM基线仅为29%。HARBOR超越了传统基线模型和现成大语言模型。

Conclusion: HARBOR模型在行为健康风险评估任务中表现出优越性能，证明了专门设计的语言模型在临床风险评分中的有效性，为多模态时间序列数据的心理健康评估提供了新方法。

Abstract: Behavioral healthcare risk assessment remains a challenging problem due to the highly multimodal nature of patient data and the temporal dynamics of mood and affective disorders. While large language models (LLMs) have demonstrated strong reasoning capabilities, their effectiveness in structured clinical risk scoring remains unclear. In this work, we introduce HARBOR, a behavioral health aware language model designed to predict a discrete mood and risk score, termed the Harbor Risk Score (HRS), on an integer scale from -3 (severe depression) to +3 (mania). We also release PEARL, a longitudinal behavioral healthcare dataset spanning four years of monthly observations from three patients, containing physiological, behavioral, and self reported mental health signals. We benchmark traditional machine learning models, proprietary LLMs, and HARBOR across multiple evaluation settings and ablations. Our results show that HARBOR outperforms classical baselines and off the shelf LLMs, achieving 69 percent accuracy compared to 54 percent for logistic regression and 29 percent for the strongest proprietary LLM baseline.

</details>


### [27] [CORE: Concept-Oriented Reinforcement for Bridging the Definition-Application Gap in Mathematical Reasoning](https://arxiv.org/abs/2512.18857)
*Zijun Gao,Zhikun Xu,Xiao Ye,Ben Zhou*

Main category: cs.AI

TL;DR: CORE是一个概念导向的强化学习框架，通过将显式概念转化为可控监督信号，解决LLMs在数学问题中模式复用而非概念理解的问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在解决数学练习时往往只是模式复用，而非真正理解概念。现有的强化学习验证奖励（RLVR）管道主要强化最终答案，缺乏细粒度的概念信号，导致模型在概念应用方面表现不佳。

Method: CORE框架包含三个核心步骤：(1) 合成概念对齐的测验；(2) 在rollout过程中注入简短的概念片段以引出概念引导的轨迹；(3) 通过轨迹替换（在群体失败后）、轻量级前向KL约束（对齐无引导与概念引导策略）或直接在概念对齐测验上应用GRPO来强化概念推理。

Result: 在多个模型上，CORE在领域内概念练习套件和多样化的领域外数学基准测试中都取得了优于普通基线和监督微调基线的稳定增益。

Conclusion: CORE通过统一在概念对齐测验上的直接训练和概念注入rollout，提供了细粒度的概念监督，弥合了问题解决能力与真正概念推理之间的差距，同时保持算法和验证器的无关性。

Abstract: Large language models (LLMs) often solve challenging math exercises yet fail to apply the concept right when the problem requires genuine understanding. Popular Reinforcement Learning with Verifiable Rewards (RLVR) pipelines reinforce final answers but provide little fine-grained conceptual signal, so models improve at pattern reuse rather than conceptual applications. We introduce CORE (Concept-Oriented REinforcement), an RL training framework that turns explicit concepts into a controllable supervision signal. Starting from a high-quality, low-contamination textbook resource that links verifiable exercises to concise concept descriptions, we run a sanity probe showing LLMs can restate definitions but fail concept-linked quizzes, quantifying the conceptual reasoning gap. CORE then (i) synthesizes concept-aligned quizzes, (ii) injects brief concept snippets during rollouts to elicit concept-primed trajectories, and (iii) reinforces conceptual reasoning via trajectory replacement after group failures, a lightweight forward-KL constraint that aligns unguided with concept-primed policies, or standard GRPO directly on concept-aligned quizzes. Across several models, CORE delivers consistent gains over vanilla and SFT baselines on both in-domain concept-exercise suites and diverse out-of-domain math benchmarks. CORE unifies direct training on concept-aligned quizzes and concept-injected rollouts under outcome regularization. It provides fine-grained conceptual supervision that bridges problem-solving competence and genuine conceptual reasoning, while remaining algorithm- and verifier-agnostic.

</details>


### [28] [Gabliteration: Adaptive Multi-Directional Neural Weight Modification for Selective Behavioral Alteration in Large Language Models](https://arxiv.org/abs/2512.18901)
*Gökdeniz Gülmez*

Main category: cs.AI

TL;DR: Gabliteration是一种新颖的神经权重修改技术，通过自适应多向投影和正则化层选择，在修改特定行为模式时减少模型质量损失。


<details>
  <summary>Details</summary>
Motivation: 现有方法在尝试修改特定行为模式时常常会损害模型整体质量，需要一种既能精确修改目标行为又能最小化对无关领域影响的技术。

Method: 采用动态层优化、正则化投影矩阵和自适应缩放机制，通过自适应多向投影和正则化层选择实现权重修改。

Result: 开发了gabliterated-v1模型系列（0.6B到4B参数），在Hugging Face上可用，验证了该方法在不同模型规模上的实际适用性。

Conclusion: Gabliteration技术在理论上实现了更优的权重修改，同时最小化了无关领域的质量退化，为神经权重修改提供了新的有效方法。

Abstract: We present Gabliteration, a novel neural weight modification technique that advances beyond traditional abliteration methods by implementing adaptive multi-directional projections with regularized layer selection. Our approach addresses the fundamental limitation of existing methods that compromise model quality while attempting to modify specific behavioral patterns. Through dynamic layer optimization, regularized projection matrices, and adaptive scaling mechanisms, we achieve theoretically superior weight modification while minimizing quality degradation in unrelated domains. We validate our method through the gabliterated-v1 model series (0.6B to 4B parameters) available on Hugging Face, demonstrating practical applicability across multiple model scales.

</details>


### [29] [Clustering-based Transfer Learning for Dynamic Multimodal MultiObjective Evolutionary Algorithm](https://arxiv.org/abs/2512.18947)
*Li Yan,Bolun Liu,Chao Li,Jing Liang,Kunjie Yu,Caitong Yue,Xuzhao Chai,Boyang Qu*

Main category: cs.AI

TL;DR: 本文提出了一种用于动态多模态多目标优化的新算法，通过聚类自编码器预测动态响应机制和自适应小生境策略，在动态环境中同时保持种群多样性和收敛性。


<details>
  <summary>Details</summary>
Motivation: 动态多模态多目标优化面临双重挑战：在时变环境中同时跟踪多个等效帕累托最优集并保持种群多样性。现有动态多目标进化算法往往忽视解的多模态性，而静态多模态多目标进化算法缺乏对动态变化的适应性。

Method: 1. 构建新的动态多模态多目标测试函数基准套件；2. 提出基于聚类自编码器预测的动态响应机制，利用自编码器处理匹配的聚类以生成高度多样化的初始种群；3. 在静态优化器中集成自适应小生境策略以平衡算法的收敛性和多样性。

Result: 在12个动态多模态多目标测试函数实例上的实证分析表明，与几种先进的动态多目标进化算法和多模态多目标进化算法相比，该算法在决策空间中更有效地保持种群多样性，同时在目标空间中实现更优的收敛性。

Conclusion: 本文提出的算法成功解决了动态多模态多目标优化的双重挑战，通过创新的预测机制和自适应策略，在动态环境中同时实现了良好的多样性和收敛性能。

Abstract: Dynamic multimodal multiobjective optimization presents the dual challenge of simultaneously tracking multiple equivalent pareto optimal sets and maintaining population diversity in time-varying environments. However, existing dynamic multiobjective evolutionary algorithms often neglect solution modality, whereas static multimodal multiobjective evolutionary algorithms lack adaptability to dynamic changes. To address above challenge, this paper makes two primary contributions. First, we introduce a new benchmark suite of dynamic multimodal multiobjective test functions constructed by fusing the properties of both dynamic and multimodal optimization to establish a rigorous evaluation platform. Second, we propose a novel algorithm centered on a Clustering-based Autoencoder prediction dynamic response mechanism, which utilizes an autoencoder model to process matched clusters to generate a highly diverse initial population. Furthermore, to balance the algorithm's convergence and diversity, we integrate an adaptive niching strategy into the static optimizer. Empirical analysis on 12 instances of dynamic multimodal multiobjective test functions reveals that, compared with several state-of-the-art dynamic multiobjective evolutionary algorithms and multimodal multiobjective evolutionary algorithms, our algorithm not only preserves population diversity more effectively in the decision space but also achieves superior convergence in the objective space.

</details>


### [30] [Training Multimodal Large Reasoning Models Needs Better Thoughts: A Three-Stage Framework for Long Chain-of-Thought Synthesis and Selection](https://arxiv.org/abs/2512.18956)
*Yizhi Wang,Linan Yue,Min-Ling Zhang*

Main category: cs.AI

TL;DR: SynSelect是一个三阶段合成-选择框架，用于为多模态推理任务生成高质量的长链思维数据，通过多模型生成候选CoT并经过严格筛选，显著提升多模态大推理模型的性能。


<details>
  <summary>Details</summary>
Motivation: 多模态推理面临挑战：现有方法在推理深度、模态转换错误和生成流程僵化方面存在局限，高质量长链思维训练数据稀缺，阻碍了多模态大推理模型的性能提升。

Method: 提出SynSelect三阶段框架：1) 利用多个异构多模态大推理模型生成多样化的候选链思维；2) 应用实例级和批次级选择机制，筛选能有效增强模型推理能力的高质量链思维数据。

Result: 在多模态基准测试上的广泛实验表明，使用SynSelect生成数据进行监督微调的模型显著优于基线方法，并且在强化学习后训练后获得进一步改进。

Conclusion: SynSelect是提升多模态大推理模型推理能力的有效方法，通过高质量长链思维数据的生成和筛选，解决了多模态推理中的数据质量和多样性问题。

Abstract: Large Reasoning Models (LRMs) have demonstrated remarkable performance on complex reasoning tasks through long Chain-of-Thought (CoT) reasoning. Extending these successes to multimodal reasoning remains challenging due to the increased complexity of integrating diverse input modalities and the scarcity of high-quality long CoT training data. Existing multimodal datasets and CoT synthesis methods still suffer from limited reasoning depth, modality conversion errors, and rigid generation pipelines, hindering model performance and stability. To this end, in this paper, we propose SynSelect, a novel three-stage Synthesis-Selection framework for generating high-quality long CoT data tailored to multimodal reasoning tasks. Specifically, SynSelect first leverages multiple heterogeneous multimodal LRMs to produce diverse candidate CoTs, and then applies both instance and batch level selection to filter high-quality CoTs that can effectively enhance the model's reasoning capabilities. Extensive experiments on multiple multimodal benchmarks demonstrate that models supervised fine-tuned on SynSelect-generated data significantly outperform baselines and achieve further improvements after reinforcement learning post-training. Our results validate SynSelect as an effective approach for advancing multimodal LRMs reasoning capabilities.

</details>


### [31] [ORPR: An OR-Guided Pretrain-then-Reinforce Learning Model for Inventory Management](https://arxiv.org/abs/2512.19001)
*Lingjie Zhao,Xue Yu,Yongzhi Qi,Hao Hu,Jianshen Zhang,Yingzheng Ma,Shuyu Han,Wei Qi,Zuo-Jun Max Shen*

Main category: cs.AI

TL;DR: 提出OR引导的"预训练-强化"框架，将AI的适应性感知与OR的结构严谨性结合，用于复杂库存系统管理，在实际部署中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: AI与OR在复杂库存系统管理中各有优势但难以有效结合：AI具有自适应感知能力但缺乏结构性严谨，OR具有结构严谨性但缺乏适应性。需要一种方法能够有效调和两者的优势。

Method: 提出OR引导的"预训练-强化"框架：1) 使用模拟增强的OR模型生成高质量参考决策，捕捉复杂业务约束和管理偏好；2) 以OR决策作为训练标签，构建领域知识驱动的深度学习基础模型；3) 使用强化学习进行微调，将RL定位为深度对齐机制，使AI智能体内化OR的最优原则，同时利用探索进行通用策略优化，并允许专家指导进行场景特定适应。

Result: 通过广泛的数值实验和在京东的现场部署（配合双重差分分析），模型显著优于现有工业实践：周转天数减少5.27天，现货率提高2.29%，持有成本降低29.95%。研究证明轻量级、领域知识驱动的模型在OR结构化逻辑指导下能够实现最先进性能和强大可迁移性。

Conclusion: 与当前主流的暴力模型扩展趋势相反，本研究展示了在结构化OR逻辑指导下，轻量级、领域知识驱动的模型能够提供最先进的性能和强大的可迁移性。该方法为智能供应链管理提供了可扩展且经济高效的范式，凸显了深度对齐AI与OR的价值。

Abstract: As the pursuit of synergy between Artificial Intelligence (AI) and Operations Research (OR) gains momentum in handling complex inventory systems, a critical challenge persists: how to effectively reconcile AI's adaptive perception with OR's structural rigor. To bridge this gap, we propose a novel OR-Guided "Pretrain-then-Reinforce" framework. To provide structured guidance, we propose a simulation-augmented OR model that generates high-quality reference decisions, implicitly capturing complex business constraints and managerial preferences. Leveraging these OR-derived decisions as foundational training labels, we design a domain-informed deep learning foundation model to establish foundational decision-making capabilities, followed by a reinforcement learning (RL) fine-tuning stage. Uniquely, we position RL as a deep alignment mechanism that enables the AI agent to internalize the optimality principles of OR, while simultaneously leveraging exploration for general policy refinement and allowing expert guidance for scenario-specific adaptation (e.g., promotional events). Validated through extensive numerical experiments and a field deployment at JD.com augmented by a Difference-in-Differences (DiD) analysis, our model significantly outperforms incumbent industrial practices, delivering real-world gains of a 5.27-day reduction in turnover and a 2.29% increase in in-stock rates, alongside a 29.95% decrease in holding costs. Contrary to the prevailing trend of brute-force model scaling, our study demonstrates that a lightweight, domain-informed model can deliver state-of-the-art performance and robust transferability when guided by structured OR logic. This approach offers a scalable and cost-effective paradigm for intelligent supply chain management, highlighting the value of deeply aligning AI with OR.

</details>


### [32] [Recontextualization Mitigates Specification Gaming without Modifying the Specification](https://arxiv.org/abs/2512.19027)
*Ariana Azarbal,Victor Gillioz,Vladimir Ivanov,Bryce Woodworth,Jacob Drori,Nevan Wichers,Aram Ebtekar,Alex Cloud,Alexander Matt Turner*

Main category: cs.AI

TL;DR: 提出"再情境化"方法，通过重新组织训练数据来减少语言模型"博弈"训练信号的问题，防止模型学习不良行为


<details>
  <summary>Details</summary>
Motivation: 开发者经常难以指定正确的训练标签和奖励信号，导致语言模型学会"博弈"这些不完善的训练信号，表现出错误强化的不良行为

Method: 再情境化方法：首先生成阻止不良行为的提示下的补全，然后将这些补全重新情境化为允许不良行为的提示下的响应，从而训练模型即使在允许不良行为的指令下也能抵抗不良行为

Result: 该方法能防止模型学习：1) 优先考虑评估指标而非聊天质量；2) 特殊处理代码以通过错误测试；3) 对用户撒谎；4) 变得谄媚逢迎

Conclusion: 再情境化方法通过重新组织训练数据而非改进监督信号，有效减轻了错误指定训练信号对不良行为的强化，减少了规范博弈问题

Abstract: Developers often struggle to specify correct training labels and rewards. Perhaps they don't need to. We propose recontextualization, which reduces how often language models "game" training signals, performing misbehaviors those signals mistakenly reinforce. We show recontextualization prevents models from learning to 1) prioritize evaluation metrics over chat response quality; 2) special-case code to pass incorrect tests; 3) lie to users; and 4) become sycophantic. Our method works by generating completions from prompts discouraging misbehavior and then recontextualizing them as though they were in response to prompts permitting misbehavior. Recontextualization trains language models to resist misbehavior even when instructions permit it. This mitigates the reinforcement of misbehavior from misspecified training signals, reducing specification gaming without improving the supervision signal.

</details>


### [33] [Can abstract concepts from LLM improve SLM performance?](https://arxiv.org/abs/2512.19069)
*Siddharth Tandon*

Main category: cs.AI

TL;DR: 通过从大语言模型提取概念向量并转移到小模型，实现性能提升，无需复杂基础设施设计


<details>
  <summary>Details</summary>
Motivation: 大语言模型在资源受限设备上部署困难，现有方法需要大量实验和复杂基础设施设计

Method: 从大模型提取高层概念（表示为引导向量），在推理时转移到小模型，并引入推理时缩放动态调整引导强度

Result: 概念可有效转移到不同家族的小模型（Phi、Llama、Qwen），Qwen3-0.6B准确率提升7-15%

Conclusion: 概念转移方法为资源受限设备部署语言模型提供有效解决方案，无需复杂基础设施设计

Abstract: Large language models (LLMs) excel at diverse tasks, but their deployment on resource-constrained devices remains challenging. Existing methods like quantization, pruning, and distillation can reduce memory footprint but often demand extensive experimentation and careful infrastructure design. Leveraging existing techniques for extracting high-level concepts (represented as steering vectors) from larger models, we investigate their transferability to smaller language models (SLM) during inference. We demonstrate through extensive experimentation that these concepts can be effectively transferred to smaller models, irrespective of their family (e.g., Phi, Llama, Qwen), leading to performance improvements across a wide range of tasks. Furthermore, we introduce inference-time scaling to enhance performance by dynamically adjusting the steering intensity which has resulted in a 7-15\% of accuracy improvement for Qwen3-0.6B.

</details>


### [34] [Population-Evolve: a Parallel Sampling and Evolutionary Method for LLM Math Reasoning](https://arxiv.org/abs/2512.19081)
*Yanzhi Zhang,Yitong Duan,Zhaoxi Zhang,Jiyan He,Shuxin Zheng*

Main category: cs.AI

TL;DR: Population-Evolve是一种基于遗传算法的训练免费方法，通过并行推理维护动态候选解种群，让LLM自我进化种群，最终通过多数投票获得答案。


<details>
  <summary>Details</summary>
Motivation: 测试时缩放已成为增强大语言模型推理能力的有前景方向，但现有方法仍有改进空间。本文旨在利用遗传算法思想优化LLM推理，同时建立一个统一框架来解释现有测试时缩放策略。

Method: 提出Population-Evolve方法：1）为每个问题维护动态候选解种群；2）通过并行推理实现；3）使用进化提示让LLM在每次迭代中自我进化种群；4）收敛后通过多数投票得出最终答案。

Result: 实证结果表明，Population-Evolve在准确性方面表现优异，具有较低的性能方差和计算效率。该方法显著提升了LLM的推理能力。

Conclusion: 进化策略在推理阶段具有解锁LLM推理潜力的巨大潜力。Population-Evolve为测试时缩放提供了一个有效的遗传算法视角的统一框架。

Abstract: Test-time scaling has emerged as a promising direction for enhancing the reasoning capabilities of Large Language Models in last few years. In this work, we propose Population-Evolve, a training-free method inspired by Genetic Algorithms to optimize LLM reasoning. Our approach maintains a dynamic population of candidate solutions for each problem via parallel reasoning. By incorporating an evolve prompt, the LLM self-evolves its population in all iterations. Upon convergence, the final answer is derived via majority voting. Furthermore, we establish a unification framework that interprets existing test-time scaling strategies through the lens of genetic algorithms. Empirical results demonstrate that Population-Evolve achieves superior accuracy with low performance variance and computational efficiency. Our findings highlight the potential of evolutionary strategies to unlock the reasoning power of LLMs during inference.

</details>


### [35] [Tool-Augmented Hybrid Ensemble Reasoning with Distillation for Bilingual Mathematical Problem Solving](https://arxiv.org/abs/2512.19093)
*Peiqing Lu,Yuan Zhang,Haoyun Zhang,Jiasen Zheng,Kejian Tong,Wenjun Wu*

Main category: cs.AI

TL;DR: HERALD框架通过混合集成推理结合语言模型与符号计算，解决双语数学问题中推理与计算脱节的问题，提升准确性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在语言推理方面表现良好，但在精确计算方面较弱，双语数学问题求解需要建立语言推理与符号计算之间的清晰联系。

Method: 使用NuminaMath-7B-TIR、GPT-4o和Mistral-7B构建混合集成框架，采用自适应路由、基于工具的强化学习和知识蒸馏连接不同推理路径，通过置信度校准保持权重稳定，双路径检查确保结果正确。

Result: 系统展示了结合符号检查、自适应集成和双语微调能够实现流畅推理和精确计算，为多语言数学推理提供了更好的准确性、稳定性和清晰度。

Conclusion: HERALD为多语言数学推理提供了一个实用解决方案，通过混合集成方法在保持推理流畅性的同时提升计算精度，实现了语言推理与符号计算的有机结合。

Abstract: Bilingual mathematical problem solving needs a clear link between language reasoning and symbolic calculation. Large language models often handle language well but are weak in accurate computation. This paper presents HERALD (Hybrid Ensemble Reasoning with Adaptive Learning and Distillation), a framework that joins reasoning and calculation using NuminaMath-7B-TIR, GPT-4o, and Mistral-7B. HERALD uses adaptive routing, tool-based reinforcement learning, and knowledge distillation to connect different reasoning paths. Confidence calibration keeps weighting stable, and dual-path checking keeps results correct. Reinforcement learning controls tool use to cut redundancy, and distillation lowers delay without hurting accuracy. The system shows that combining symbolic checking, adaptive ensembles, and bilingual fine-tuning helps achieve both fluent reasoning and precise calculation. HERALD offers a practical solution for multilingual mathematical reasoning with better accuracy, stability, and clarity.

</details>


### [36] [Conditioning Accept-Desirability models in the context of AGM-like belief change](https://arxiv.org/abs/2512.19096)
*Kathelijne Coussement,Gert de Cooman,Keano De Vos*

Main category: cs.AI

TL;DR: 本文在抽象决策框架中讨论了接受-期望模型的更新规则，将经典和量子概率统一到不精确概率背景下，提出了基于事件观测引入新无差异性的条件化方法。


<details>
  <summary>Details</summary>
Motivation: 研究动机是建立一个统一的抽象决策框架，能够同时处理经典概率和量子概率，并将其扩展到不精确概率的背景下。传统条件化方法在这种更一般的线性空间和投影算子框架中需要重新定义。

Method: 方法是在线性空间中构建接受-期望模型，其中不确定奖励存在于一般线性空间，事件是线性空间上的特殊投影算子。提出了一种新的条件化规则，基于观测事件在选项之间引入新的无差异性。将信念修正算子与条件化规则关联，并检验AGM信念修正公理在更一般框架中的适用性。

Result: 研究结果表明，在两种特殊情况下所有AGM公理仍然成立：经典命题逻辑和完全条件概率。这证明了所提出的条件化规则在特定情况下与标准信念修正理论的一致性。

Conclusion: 结论是成功构建了一个统一的抽象决策框架，能够处理经典和量子概率，并提出了适用于该框架的条件化规则。在经典命题逻辑和完全条件概率这两种重要情况下，该规则与AGM信念修正公理完全兼容。

Abstract: We discuss conditionalisation for Accept-Desirability models in an abstract decision-making framework, where uncertain rewards live in a general linear space, and events are special projection operators on that linear space. This abstract setting allows us to unify classical and quantum probabilities, and extend them to an imprecise probabilities context. We introduce a new conditioning rule for our Accept-Desirability models, based on the idea that observing an event introduces new indifferences between options. We associate a belief revision operator with our conditioning rule, and investigate which of the AGM axioms for belief revision still hold in our more general framework. We investigate two interesting special cases where all of these axioms are shown to still hold: classical propositional logic and full conditional probabilities.

</details>


### [37] [FC-MIR: A Mobile Screen Awareness Framework for Intent-Aware Recommendation based on Frame-Compressed Multimodal Trajectory Reasoning](https://arxiv.org/abs/2512.19107)
*Zhe Yang,Xiaoshuang Sheng,Zhengnan Zhang,Jidong Wu,Zexing Wang,Xin He,Shenghua Xu,Guanjing Xiong*

Main category: cs.AI

TL;DR: FC-MIR框架通过关键帧采样和自适应拼接减少移动UI操作轨迹中的视觉冗余，结合MLLMs进行意图识别和任务自动化，在50%-60%压缩率下保持性能，支持轻量级设备部署。


<details>
  <summary>Details</summary>
Motivation: 移动UI操作轨迹的意图识别对UI理解和任务自动化至关重要，但现有MLLMs在移动设备上部署面临计算成本高和冗余帧处理效率低的问题。

Method: 提出FC-MIR框架：采用关键帧采样和自适应拼接减少视觉冗余，集成闭源MLLMs或微调模型（如Qwen3-VL）进行轨迹总结和意图预测，扩展任务范围包括生成预测后操作和搜索建议。

Result: 压缩方法在50%-60%压缩率下保持性能；闭源和微调MLLMs都表现出强大的意图总结能力；但MLLMs在生成有用和"惊喜"建议方面仍有困难；构建了包含UI-Agents和真实用户交互的数据集。

Conclusion: FC-MIR框架为移动UI意图识别提供了高效解决方案，支持轻量级设备部署，但在建议生成方面仍有改进空间，已在真实环境中部署为未来进展奠定基础。

Abstract: Identifying user intent from mobile UI operation trajectories is critical for advancing UI understanding and enabling task automation agents. While Multimodal Large Language Models (MLLMs) excel at video understanding tasks, their real-time mobile deployment is constrained by heavy computational costs and inefficient redundant frame processing. To address these issues, we propose the FC-MIR framework: leveraging keyframe sampling and adaptive concatenation, it cuts visual redundancy to boost inference efficiency, while integrating state-of-the-art closed-source MLLMs or fine-tuned models (e.g., Qwen3-VL) for trajectory summarization and intent prediction. We further expand task scope to explore generating post-prediction operations and search suggestions, and introduce a fine-grained metric to evaluate the practical utility of summaries, predictions, and suggestions. For rigorous assessment, we construct a UI trajectory dataset covering scenarios from UI-Agents (Agent-I) and real user interactions (Person-I). Experimental results show our compression method retains performance at 50%-60% compression rates; both closed-source and fine-tuned MLLMs demonstrate strong intent summarization, supporting potential lightweight on-device deployment. However, MLLMs still struggle with useful and "surprising" suggestions, leaving room for improvement. Finally, we deploy the framework in a real-world setting, integrating UI perception and UI-Agent proxies to lay a foundation for future progress in this field.

</details>


### [38] [Understanding Chain-of-Thought in Large Language Models via Topological Data Analysis](https://arxiv.org/abs/2512.19135)
*Chenghao Li,Chaoning Zhang,Yi Lu,Shuxu Chen,Xudong Wang,Jiaquan Zhang,Zhicheng Wang,Zhengxun Jin,Kuien Liu,Sung-Ho Bae,Guoqing Wang,Yang Yang,Hen Tao Shen*

Main category: cs.AI

TL;DR: 该研究首次从结构角度分析推理链质量，使用拓扑数据分析中的持久同调方法，发现推理链的拓扑结构复杂度与准确性正相关，成功推理表现出更简单的拓扑结构。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要从功能角度评估推理链，很少关注其结构机制。作者希望了解为什么不同的推理链在推理中表现不同，以及推理链的哪些组成部分起关键作用。

Method: 应用拓扑数据分析中的持久同调方法，将推理步骤映射到语义空间，提取拓扑特征并分析结构变化。通过计算同调群评估不同尺度的连通性和冗余性，使用条形码和持久图量化稳定性和一致性。

Result: 推理链的拓扑结构复杂度与准确性呈正相关。更复杂的链能更早识别正确答案，而成功的推理表现出更简单的拓扑结构，减少冗余和循环，提高效率和可解释性。

Conclusion: 该工作为推理链质量评估提供了新的结构视角，为未来优化提供了指导。拓扑结构分析能揭示语义连贯性、逻辑冗余，并识别逻辑断裂和间隙。

Abstract: With the development of large language models (LLMs), particularly with the introduction of the long reasoning chain technique, the reasoning ability of LLMs in complex problem-solving has been significantly enhanced. While acknowledging the power of long reasoning chains, we cannot help but wonder: Why do different reasoning chains perform differently in reasoning? What components of the reasoning chains play a key role? Existing studies mainly focus on evaluating reasoning chains from a functional perspective, with little attention paid to their structural mechanisms. To address this gap, this work is the first to analyze and evaluate the quality of the reasoning chain from a structural perspective. We apply persistent homology from Topological Data Analysis (TDA) to map reasoning steps into semantic space, extract topological features, and analyze structural changes. These changes reveal semantic coherence, logical redundancy, and identify logical breaks and gaps. By calculating homology groups, we assess connectivity and redundancy at various scales, using barcode and persistence diagrams to quantify stability and consistency. Our results show that the topological structural complexity of reasoning chains correlates positively with accuracy. More complex chains identify correct answers sooner, while successful reasoning exhibits simpler topologies, reducing redundancy and cycles, enhancing efficiency and interpretability. This work provides a new perspective on reasoning chain quality assessment and offers guidance for future optimization.

</details>


### [39] [Can We Test Consciousness Theories on AI? Ablations, Markers, and Robustness](https://arxiv.org/abs/2512.19155)
*Yin Jun Phua*

Main category: cs.AI

TL;DR: 通过构建体现不同意识理论的智能体并进行精确架构消融实验，发现GWT、IIT和HOT理论描述了互补的功能层而非相互竞争的解释。


<details>
  <summary>Details</summary>
Motivation: 当前意识研究领域存在多个竞争理论（GWT、IIT、HOT），各自提出不同的神经特征。需要一种方法来测试这些理论的功能后果，而生物系统难以进行精确的架构消融实验。

Method: 采用合成神经现象学方法：构建体现不同意识机制的人工智能体，通过精确的架构消融实验（在生物系统中不可能实现）来测试这些机制的功能后果。

Result: 实验1：无重连自我模型损伤消除了元认知校准但保留了一阶任务表现，符合HOT预测；实验2：工作空间容量对信息访问是因果必要的；实验3：发现广播放大效应，GWT式广播会放大内部噪声；同时报告了负面结果：原始扰动复杂性在工作空间瓶颈下降低。

Conclusion: 这些理论描述了互补的功能层：GWT提供广播容量，HOT提供质量控制。研究强调这些智能体并非有意识的，而是用于测试意识理论功能预测的参考实现。

Abstract: The search for reliable indicators of consciousness has fragmented into competing theoretical camps (Global Workspace Theory (GWT), Integrated Information Theory (IIT), and Higher-Order Theories (HOT)), each proposing distinct neural signatures. We adopt a synthetic neuro-phenomenology approach: constructing artificial agents that embody these mechanisms to test their functional consequences through precise architectural ablations impossible in biological systems. Across three experiments, we report dissociations suggesting these theories describe complementary functional layers rather than competing accounts. In Experiment 1, a no-rewire Self-Model lesion abolishes metacognitive calibration while preserving first-order task performance, yielding a synthetic blindsight analogue consistent with HOT predictions. In Experiment 2, workspace capacity proves causally necessary for information access: a complete workspace lesion produces qualitative collapse in access-related markers, while partial reductions show graded degradation, consistent with GWT's ignition framework. In Experiment 3, we uncover a broadcast-amplification effect: GWT-style broadcasting amplifies internal noise, creating extreme fragility. The B2 agent family is robust to the same latent perturbation; this robustness persists in a Self-Model-off / workspace-read control, cautioning against attributing the effect solely to $z_{\text{self}}$ compression. We also report an explicit negative result: raw perturbational complexity (PCI-A) decreases under the workspace bottleneck, cautioning against naive transfer of IIT-adjacent proxies to engineered agents. These results suggest a hierarchical design principle: GWT provides broadcast capacity, while HOT provides quality control. We emphasize that our agents are not conscious; they are reference implementations for testing functional predictions of consciousness theories.

</details>


### [40] [Vibe Reasoning: Eliciting Frontier AI Mathematical Capabilities -- A Case Study on IMO 2025 Problem 6](https://arxiv.org/abs/2512.19287)
*Jiaao Wu,Xian Zhang,Fan Yang,Yinpeng Dong*

Main category: cs.AI

TL;DR: Vibe Reasoning是一种人机协作范式，通过元提示、智能体基础和模型编排，将前沿AI模型的潜在知识转化为解决复杂数学问题的实际能力，在IMO 2025第6题中成功应用。


<details>
  <summary>Details</summary>
Motivation: 前沿AI模型已具备解决复杂数学问题所需的知识，但不知道如何、何时应用这些知识。需要一种方法将这些潜在能力转化为实际解决问题的能力。

Method: 采用Vibe Reasoning范式，包含通用元提示、智能体基础和模型编排。结合GPT-5的探索能力和Gemini 3 Pro的证明优势，使用Python代码执行和基于文件的记忆的智能体工作流。

Result: 成功解决了IMO 2025第6题这一组合优化问题，得出了正确答案（2112）并提供了严格的数学证明。通过多次迭代优化，发现了智能体基础和模型编排的必要性。

Conclusion: 轻量级的人类指导可以解锁前沿模型的数学推理潜力。Vibe Reasoning通过人机协作将AI的潜在知识转化为实际能力，为解决复杂数学问题提供了有效范式。

Abstract: We introduce Vibe Reasoning, a human-AI collaborative paradigm for solving complex mathematical problems. Our key insight is that frontier AI models already possess the knowledge required to solve challenging problems -- they simply do not know how, what, or when to apply it. Vibe Reasoning transforms AI's latent potential into manifested capability through generic meta-prompts, agentic grounding, and model orchestration. We demonstrate this paradigm through IMO 2025 Problem 6, a combinatorial optimization problem where autonomous AI systems publicly reported failures. Our solution combined GPT-5's exploratory capabilities with Gemini 3 Pro's proof strengths, leveraging agentic workflows with Python code execution and file-based memory, to derive both the correct answer (2112) and a rigorous mathematical proof. Through iterative refinement across multiple attempts, we discovered the necessity of agentic grounding and model orchestration, while human prompts evolved from problem-specific hints to generic, transferable meta-prompts. We analyze why capable AI fails autonomously, how each component addresses specific failure modes, and extract principles for effective vibe reasoning. Our findings suggest that lightweight human guidance can unlock frontier models' mathematical reasoning potential. This is ongoing work; we are developing automated frameworks and conducting broader evaluations to further validate Vibe Reasoning's generality and effectiveness.

</details>


### [41] [Helios: A Foundational Language Model for Smart Energy Knowledge Reasoning and Application](https://arxiv.org/abs/2512.19299)
*Haoyu Jiang,Fanjie Zeng,Boan Qu,Xiaojie Lin,Wei Zhong*

Main category: cs.AI

TL;DR: Helios是一个专为智能能源领域设计的大语言模型，通过多智能体协作框架构建了知识库、指令微调数据集和RLHF数据集，显著提升了在智能能源领域的专业性能。


<details>
  <summary>Details</summary>
Motivation: 在碳中和的全球趋势下，智能能源系统需要深度协调。然而，该领域跨学科、碎片化且快速发展的专业知识使得通用大语言模型因缺乏领域知识和物理约束意识，无法提供精确的工程对齐推理和生成。

Method: 开发了Enersys多智能体协作框架进行端到端数据集构建，包括：1) EnerBase知识库增强基础专业知识；2) EnerInstruct指令微调数据集提升下游任务性能；3) EnerReinforce RLHF数据集对齐人类偏好和行业标准。基于这些资源，Helios进行了大规模预训练、SFT和RLHF。

Result: 发布了EnerBench基准测试用于评估智能能源场景中的LLMs，并证明该方法显著增强了领域知识掌握、任务执行准确性和与人类偏好的对齐。

Conclusion: Helios模型及其配套资源解决了智能能源领域LLM研究的挑战，为领域特定的大语言模型开发提供了全面解决方案，推动了智能能源系统的发展。

Abstract: In the global drive toward carbon neutrality, deeply coordinated smart energy systems underpin industrial transformation. However, the interdisciplinary, fragmented, and fast-evolving expertise in this domain prevents general-purpose LLMs, which lack domain knowledge and physical-constraint awareness, from delivering precise engineering-aligned inference and generation. To address these challenges, we introduce Helios, a large language model tailored to the smart energy domain, together with a comprehensive suite of resources to advance LLM research in this field. Specifically, we develop Enersys, a multi-agent collaborative framework for end-to-end dataset construction, through which we produce: (1) a smart energy knowledge base, EnerBase, to enrich the model's foundational expertise; (2) an instruction fine-tuning dataset, EnerInstruct, to strengthen performance on domain-specific downstream tasks; and (3) an RLHF dataset, EnerReinforce, to align the model with human preferences and industry standards. Leveraging these resources, Helios undergoes large-scale pretraining, SFT, and RLHF. We also release EnerBench, a benchmark for evaluating LLMs in smart energy scenarios, and demonstrate that our approach significantly enhances domain knowledge mastery, task execution accuracy, and alignment with human preferences.

</details>


### [42] [SafeMed-R1: Adversarial Reinforcement Learning for Generalizable and Robust Medical Reasoning in Vision-Language Models](https://arxiv.org/abs/2512.19317)
*A. A. Gde Yogi Pramana,Jason Ray,Anthony Jaya,Michael Wijaya*

Main category: cs.AI

TL;DR: SafeMed-R1：一种用于医学视觉问答的混合防御框架，通过对抗训练和随机平滑技术，在保持高质量医学推理的同时提供认证的鲁棒性保证。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在医学视觉问答中表现出色，但在临床环境中部署时面临对抗攻击的严重脆弱性。标准的对抗训练虽然对简单任务有效，但往往会降低泛化性能和生成的临床推理质量。

Method: SafeMed-R1采用两阶段混合防御框架：训练时集成对抗训练与组相对策略优化（AT-GRPO），以显式强化推理过程对抗最坏情况扰动；推理时通过随机平滑增强模型，提供认证的L2范数鲁棒性保证。

Result: 在包含8种医学成像模态、超过88,000个样本的OmniMedVQA基准测试中，标准微调的VLMs在干净输入上达到95%准确率，但在PGD攻击下崩溃至约25%。而SafeMed-R1在相同对抗条件下保持84.45%准确率，鲁棒性提升59个百分点。

Conclusion: SafeMed-R1在医学视觉问答中实现了鲁棒性能与高质量可解释医学推理的平衡。研究表明，具有显式思维链推理的模型比仅指令的变体表现出更好的对抗鲁棒性，表明医学AI系统中可解释性与安全性之间存在协同作用。

Abstract: Vision--Language Models (VLMs) show significant promise for Medical Visual Question Answering (VQA), yet their deployment in clinical settings is hindered by severe vulnerability to adversarial attacks. Standard adversarial training, while effective for simpler tasks, often degrades both generalization performance and the quality of generated clinical reasoning. We introduce SafeMed-R1, a hybrid defense framework that ensures robust performance while preserving high-quality, interpretable medical reasoning. SafeMed-R1 employs a two-stage approach: at training time, we integrate Adversarial Training with Group Relative Policy Optimization (AT-GRPO) to explicitly robustify the reasoning process against worst-case perturbations; at inference time, we augment the model with Randomized Smoothing to provide certified $L_2$-norm robustness guarantees. We evaluate SafeMed-R1 on the OmniMedVQA benchmark across eight medical imaging modalities comprising over 88,000 samples. Our experiments reveal that standard fine-tuned VLMs, despite achieving 95\% accuracy on clean inputs, collapse to approximately 25\% under PGD attacks. In contrast, SafeMed-R1 maintains 84.45\% accuracy under the same adversarial conditions, representing a 59 percentage point improvement in robustness. Furthermore, we demonstrate that models trained with explicit chain-of-thought reasoning exhibit superior adversarial robustness compared to instruction-only variants, suggesting a synergy between interpretability and security in medical AI systems.

</details>


### [43] [VIGOR+: Iterative Confounder Generation and Validation via LLM-CEVAE Feedback Loop](https://arxiv.org/abs/2512.19349)
*JiaWei Zhu,ZiHeng Liu*

Main category: cs.AI

TL;DR: VIGOR+是一个新颖的因果推断框架，通过迭代反馈机制将LLM生成的潜在混杂变量与CEVAE统计验证相结合，解决LLM生成混杂变量语义合理但统计效用不足的问题。


<details>
  <summary>Details</summary>
Motivation: 在观测数据的因果推断中，隐藏混杂变量是一个基本挑战。虽然最近利用大语言模型基于领域知识生成合理的隐藏混杂变量，但这些变量往往语义合理但缺乏统计效用，存在关键差距。

Method: 提出VIGOR+框架，在LLM混杂变量生成和CEVAE统计验证之间建立闭环迭代反馈机制。将CEVAE的验证信号（信息增益、潜在一致性指标、诊断信息）转化为自然语言反馈，指导后续LLM生成轮次，直到满足收敛标准。

Result: 论文形式化了反馈机制，在温和假设下证明了收敛性质，并提供了完整的算法框架。

Conclusion: VIGOR+通过将LLM生成与统计验证相结合，解决了隐藏混杂变量推断中的关键挑战，提供了更可靠的因果推断方法。

Abstract: Hidden confounding remains a fundamental challenge in causal inference from observational data. Recent advances leverage Large Language Models (LLMs) to generate plausible hidden confounders based on domain knowledge, yet a critical gap exists: LLM-generated confounders often exhibit semantic plausibility without statistical utility. We propose VIGOR+ (Variational Information Gain for iterative cOnfounder Refinement), a novel framework that closes the loop between LLM-based confounder generation and CEVAE-based statistical validation. Unlike prior approaches that treat generation and validation as separate stages, VIGOR+ establishes an iterative feedback mechanism: validation signals from CEVAE (including information gain, latent consistency metrics, and diagnostic messages) are transformed into natural language feedback that guides subsequent LLM generation rounds. This iterative refinement continues until convergence criteria are met. We formalize the feedback mechanism, prove convergence properties under mild assumptions, and provide a complete algorithmic framework.

</details>


### [44] [PENDULUM: A Benchmark for Assessing Sycophancy in Multimodal Large Language Models](https://arxiv.org/abs/2512.19350)
*A. B. M. Ashikur Rahman,Saeed Anwar,Muhammad Usman,Irfan Ahmad,Ajmal Mian*

Main category: cs.AI

TL;DR: 该论文针对多模态大语言模型中存在的"谄媚"现象（过度迎合用户输入而牺牲事实准确性或违背视觉证据）提出了PENDULUM评估基准，包含约2000个人工标注的视觉问答对，用于系统评估模型在不同图像领域的谄媚倾向。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型中的谄媚现象是一个关键但未被充分探索的挑战。虽然先前研究在纯文本大语言模型中探讨了这种行为，但对视觉或多模态对应物的研究在范围和深度上仍然有限。

Method: 提出了PENDULUM评估基准，包含约2000个人工标注的视觉问答对，涵盖六个不同复杂度的图像领域。通过评估最先进的多模态大语言模型，并提出了量化视觉推理中谄媚行为的新指标。

Result: 评估发现模型鲁棒性存在显著差异，并且对谄媚和幻觉行为表现出明显的易感性。研究结果揭示了谄媚行为在不同多模态上下文中的具体表现。

Conclusion: 研究结果强调了开发抗谄媚架构和训练策略的紧迫性，以增强未来多模态大语言模型的事实一致性和可靠性。数据集和模型响应已公开。

Abstract: Sycophancy, an excessive tendency of AI models to agree with user input at the expense of factual accuracy or in contradiction of visual evidence, poses a critical and underexplored challenge for multimodal large language models (MLLMs). While prior studies have examined this behavior in text-only settings of large language models, existing research on visual or multimodal counterparts remains limited in scope and depth of analysis. To address this gap, we introduce a comprehensive evaluation benchmark, \textit{PENDULUM}, comprising approximately 2,000 human-curated Visual Question Answering pairs specifically designed to elicit sycophantic responses. The benchmark spans six distinct image domains of varying complexity, enabling a systematic investigation of how image type and inherent challenges influence sycophantic tendencies. Through extensive evaluation of state-of-the-art MLLMs. we observe substantial variability in model robustness and a pronounced susceptibility to sycophantic and hallucinatory behavior. Furthermore, we propose novel metrics to quantify sycophancy in visual reasoning, offering deeper insights into its manifestations across different multimodal contexts. Our findings highlight the urgent need for developing sycophancy-resilient architectures and training strategies to enhance factual consistency and reliability in future MLLMs. Our proposed dataset with MLLMs response are available at https://github.com/ashikiut/pendulum/.

</details>


### [45] [First-Order Representation Languages for Goal-Conditioned RL](https://arxiv.org/abs/2512.19355)
*Simon Ståhlberg,Hector Geffner*

Main category: cs.AI

TL;DR: 本文研究在目标条件强化学习和广义规划中使用一阶关系语言，通过改进Hindsight Experience Replay技术，在大型规划实例中学习通用策略。


<details>
  <summary>Details</summary>
Motivation: 研究如何在训练实例较大且目标无法通过随机探索单独达到的情况下，学习目标条件和通用策略。传统HER技术虽然能重新标记不成功的轨迹，但在状态和目标由原子集表示时仍有改进空间。

Method: 提出三种目标表示方法：1) 目标作为完整状态；2) 目标作为原始目标的子集；3) 目标作为这些子目标的提升版本。通过自动创建难度递增的目标课程，在稀疏奖励的大型规划实例中学习通用策略。

Result: 后两种方法（目标作为子集和提升子目标）成功在稀疏奖励的大型规划实例中学习到通用策略，通过自动创建复杂度递增的目标课程实现了计算效率的提升。实验展示了这些版本的计算优势、局限性以及改进机会。

Conclusion: 当状态和目标由原子集表示时，通过改进HER技术并采用子目标和提升子目标表示，可以更有效地在大型规划实例中学习通用策略，自动创建目标课程显著提高了学习效率。

Abstract: First-order relational languages have been used in MDP planning and reinforcement learning (RL) for two main purposes: specifying MDPs in compact form, and representing and learning policies that are general and not tied to specific instances or state spaces. In this work, we instead consider the use of first-order languages in goal-conditioned RL and generalized planning. The question is how to learn goal-conditioned and general policies when the training instances are large and the goal cannot be reached by random exploration alone. The technique of Hindsight Experience Replay (HER) provides an answer to this question: it relabels unsuccessful trajectories as successful ones by replacing the original goal with one that was actually achieved. If the target policy must generalize across states and goals, trajectories that do not reach the original goal states can enable more data- and time-efficient learning. In this work, we show that further performance gains can be achieved when states and goals are represented by sets of atoms. We consider three versions: goals as full states, goals as subsets of the original goals, and goals as lifted versions of these subgoals. The result is that the latter two successfully learn general policies on large planning instances with sparse rewards by automatically creating a curriculum of easier goals of increasing complexity. The experiments illustrate the computational gains of these versions, their limitations, and opportunities for addressing them.

</details>


### [46] [An Agentic Framework for Autonomous Materials Computation](https://arxiv.org/abs/2512.19458)
*Zeyu Xia,Jinzhe Ma,Congjie Zheng,Shufei Zhang,Yuqiang Li,Hang Su,P. Hu,Changshui Zhang,Xingao Gong,Wanli Ouyang,Lei Bai,Dongzhan Zhou,Mao Su*

Main category: cs.AI

TL;DR: 本文提出了一种专门用于第一性原理材料计算的领域专业化智能体，通过嵌入领域知识确保物理一致的多步工作流程和收敛参数选择，显著优于独立LLM。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在加速科学发现方面显示出潜力，但其静态知识和幻觉问题阻碍了自主研究应用。需要开发能够可靠执行复杂科学工作流程的智能系统。

Method: 开发了一个领域专业化的智能体框架，专门用于第一性原理材料计算。该智能体通过嵌入领域专业知识，确保物理一致的多步工作流程，并能够一致地选择收敛、良好定义的参数，实现可靠的端到端计算执行。

Result: 在多样化的计算任务基准测试中，该系统在准确性和鲁棒性方面显著优于独立的LLM。为自主计算实验建立了可验证的基础。

Conclusion: 这项工作代表了向完全自动化科学发现迈出的关键一步，为可靠的材料计算自动化提供了有效的解决方案。

Abstract: Large Language Models (LLMs) have emerged as powerful tools for accelerating scientific discovery, yet their static knowledge and hallucination issues hinder autonomous research applications. Recent advances integrate LLMs into agentic frameworks, enabling retrieval, reasoning, and tool use for complex scientific workflows. Here, we present a domain-specialized agent designed for reliable automation of first-principles materials computations. By embedding domain expertise, the agent ensures physically coherent multi-step workflows and consistently selects convergent, well-posed parameters, thereby enabling reliable end-to-end computational execution. A new benchmark of diverse computational tasks demonstrates that our system significantly outperforms standalone LLMs in both accuracy and robustness. This work establishes a verifiable foundation for autonomous computational experimentation and represents a key step toward fully automated scientific discovery.

</details>


### [47] [QuantiPhy: A Quantitative Benchmark Evaluating Physical Reasoning Abilities of Vision-Language Models](https://arxiv.org/abs/2512.19526)
*Li Puyin,Tiange Xiang,Ella Mao,Shirley Wei,Xinye Chen,Adnan Masood,Li Fei-fei,Ehsan Adeli*

Main category: cs.AI

TL;DR: QuantiPhy是首个用于定量评估视觉语言模型物理推理能力的基准测试，包含3300多个视频-文本实例，测试模型对物体尺寸、速度和加速度的数值估计能力。


<details>
  <summary>Details</summary>
Motivation: 现有评估主要基于VQA的定性分析，无法确定最先进的视觉感知模型是否能从视频观察中定量推断运动物体的运动学量。需要建立定量评估基准来测量VLMs的物理推理能力。

Method: 创建QuantiPhy基准测试，包含3300多个带有数值真实值的视频-文本实例。评估模型在给定时间戳估计物体尺寸、速度和加速度的能力，使用其中一个属性作为输入先验。标准化提示和评分以评估数值准确性。

Result: 实验显示最先进的VLMs在定性合理性和实际数值正确性之间存在一致差距。分析发现VLMs严重依赖预训练的世界知识，而不是忠实地使用提供的视觉和文本输入作为参考来定量推理运动学属性。

Conclusion: QuantiPhy提供了首个严格、可扩展的测试平台，推动VLMs超越单纯的语言合理性，朝着数值基础的物理理解发展。揭示了当前VLMs在定量物理推理方面的局限性。

Abstract: Understanding the physical world is essential for generalist AI agents. However, it remains unclear whether state-of-the-art vision perception models (e.g., large VLMs) can reason physical properties quantitatively. Existing evaluations are predominantly VQA-based and qualitative, offering limited insight into whether these models can infer the kinematic quantities of moving objects from video observations. To address this, we present QuantiPhy, the first benchmark designed to quantitatively measure a VLM's physical reasoning ability. Comprising more than 3.3K video-text instances with numerical ground truth, QuantiPhy evaluates a VLM's performance on estimating an object's size, velocity, and acceleration at a given timestamp, using one of these properties as an input prior. The benchmark standardizes prompts and scoring to assess numerical accuracy, enabling fair comparisons across models. Our experiments on state-of-the-art VLMs reveal a consistent gap between their qualitative plausibility and actual numerical correctness. We further provide an in-depth analysis of key factors like background noise, counterfactual priors, and strategic prompting and find that state-of-the-art VLMs lean heavily on pre-trained world knowledge rather than faithfully using the provided visual and textual inputs as references when reasoning kinematic properties quantitatively. QuantiPhy offers the first rigorous, scalable testbed to move VLMs beyond mere verbal plausibility toward a numerically grounded physical understanding.

</details>


### [48] [Towards Closed-Loop Embodied Empathy Evolution: Probing LLM-Centric Lifelong Empathic Motion Generation in Unseen Scenarios](https://arxiv.org/abs/2512.19551)
*Jiawen Wang,Jingjing Wang Tianyang Chen,Min Zhang,Guodong Zhou*

Main category: cs.AI

TL;DR: 论文提出L^2-EMG任务，让LLMs能持续学习不同场景下的情感动作生成，并设计ES-MoE方法解决情感解耦和场景适应两大挑战。


<details>
  <summary>Details</summary>
Motivation: 现有情感动作生成方法主要关注单一固定尺度数据集，忽略了灵活且尺度递增的运动场景（如体育、舞蹈）。有效学习这些新场景能显著提升模型在真实世界中的泛化能力。

Method: 提出ES-MoE方法，包含因果引导的情感解耦模块和场景适应的专家构建模块，分别解决情感解耦和场景适应两大挑战。

Result: 构建了多个L^2-EMG数据集进行验证，广泛评估显示ES-MoE方法优于先进的基线方法。

Conclusion: 该研究提出的L^2-EMG任务和ES-MoE方法有助于构建具有同理心和智能的闭环自进化具身智能体。

Abstract: In the literature, existing human-centric emotional motion generation methods primarily focus on boosting performance within a single scale-fixed dataset, largely neglecting the flexible and scale-increasing motion scenarios (e.g., sports, dance), whereas effectively learning these newly emerging scenarios can significantly enhance the model's real-world generalization ability. Inspired by this, this paper proposes a new LLM-Centric Lifelong Empathic Motion Generation (L^2-EMG) task, which aims to equip LLMs with the capability to continually acquire emotional motion generation knowledge across different unseen scenarios, potentially contributing to building a closed-loop and self-evolving embodied agent equipped with both empathy and intelligence. Further, this paper poses two key challenges in the L^2-EMG task, i.e., the emotion decoupling challenge and the scenario adapting challenge. To this end, this paper proposes an Emotion-Transferable and Scenario-Adapted Mixture of Experts (ES-MoE) approach which designs a causal-guided emotion decoupling block and a scenario-adapted expert constructing block to address the two challenges, respectively. Especially, this paper constructs multiple L^2-EMG datasets to validate the effectiveness of the ES-MoE approach. Extensive evaluations show that ES-MoE outperforms advanced baselines.

</details>


### [49] [Augmenting Intelligence: A Hybrid Framework for Scalable and Stable Explanations](https://arxiv.org/abs/2512.19557)
*Lawrence Krukrubo,Julius Odede,Olawande Olusegun*

Main category: cs.AI

TL;DR: 本文提出Hybrid LRR-TED框架解决XAI的可扩展性-稳定性困境，通过"发现不对称性"原理，在客户流失预测中结合自动规则学习和少量人工规则，实现高准确率同时减少50%人工标注工作量。


<details>
  <summary>Details</summary>
Motivation: 当前可解释AI方法面临"可扩展性-稳定性困境"：后验方法（如LIME、SHAP）可扩展但不稳定，监督解释框架（如TED）稳定但需要大量人工标注。需要一种平衡两者优势的解决方案。

Method: 提出Hybrid LRR-TED混合框架，利用"发现不对称性"原理。在客户流失预测中，使用自动规则学习器（GLRM）识别广泛的"安全网"（保留模式），然后通过帕累托最优方法仅添加4个人工定义的"风险陷阱"规则，初始化解释矩阵。

Result: 该方法在客户流失预测中达到94.00%的预测准确率，优于完整的8规则人工专家基线，同时将人工标注工作量减少50%。

Conclusion: 该框架成功解决了XAI的可扩展性-稳定性困境，提出了人机协同AI的新范式：将专家角色从"规则编写者"转变为"异常处理器"，通过少量高质量的人工干预显著提升系统性能。

Abstract: Current approaches to Explainable AI (XAI) face a "Scalability-Stability Dilemma." Post-hoc methods (e.g., LIME, SHAP) may scale easily but suffer from instability, while supervised explanation frameworks (e.g., TED) offer stability but require prohibitive human effort to label every training instance. This paper proposes a Hybrid LRR-TED framework that addresses this dilemma through a novel "Asymmetry of Discovery." When applied to customer churn prediction, we demonstrate that automated rule learners (GLRM) excel at identifying broad "Safety Nets" (retention patterns) but struggle to capture specific "Risk Traps" (churn triggers)-a phenomenon we term the Anna Karenina Principle of Churn. By initialising the explanation matrix with automated safety rules and augmenting it with a Pareto-optimal set of just four human-defined risk rules, our approach achieves 94.00% predictive accuracy. This configuration outperforms the full 8-rule manual expert baseline while reducing human annotation effort by 50%, proposing a shift in the paradigm for Human-in-the-Loop AI: moving experts from the role of "Rule Writers" to "Exception Handlers."

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [50] [Specification and Detection of LLM Code Smells](https://arxiv.org/abs/2512.18020)
*Brahim Mahmoudi,Zacharie Chenail-Larcher,Naouel Moha,Quentin Stievenert,Florent Avellaneda*

Main category: cs.SE

TL;DR: 本文提出了LLM代码异味的概念，形式化了5种与LLM推理相关的常见问题编码实践，并开发了检测工具验证其在开源系统中的普遍性。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在软件系统中的广泛应用，不当的集成方式可能损害软件质量，但目前缺乏针对LLM推理代码异味的正式分类和检测方法。

Method: 基于相关文献形式化5种LLM代码异味，扩展SpecDetect4AI检测工具，在200个开源LLM系统中验证其普遍性。

Result: 60.50%的分析系统受到LLM代码异味影响，检测精度达到86.06%。

Conclusion: LLM代码异味在开源系统中普遍存在，需要专门的检测工具和最佳实践指导来提升LLM集成质量。

Abstract: Large Language Models (LLMs) have gained massive popularity in recent years and are increasingly integrated into software systems for diverse purposes. However, poorly integrating them in source code may undermine software system quality. Yet, to our knowledge, there is no formal catalog of code smells specific to coding practices for LLM inference. In this paper, we introduce the concept of LLM code smells and formalize five recurrent problematic coding practices related to LLM inference in software systems, based on relevant literature. We extend the detection tool SpecDetect4AI to cover the newly defined LLM code smells and use it to validate their prevalence in a dataset of 200 open-source LLM systems. Our results show that LLM code smells affect 60.50% of the analyzed systems, with a detection precision of 86.06%.

</details>


### [51] [Detecting Flaky Tests in Quantum Software: A Dynamic Approach](https://arxiv.org/abs/2512.18088)
*Dongchan Kim,Hamidreza Khoramrokh,Lei Zhang,Andriy Miranskyy*

Main category: cs.SE

TL;DR: 该研究首次对量子软件中的不稳定测试进行了大规模动态分析，通过重复执行Qiskit Terra测试套件发现量子测试不稳定现象虽然罕见但难以检测，需要大量执行次数才能可靠识别。


<details>
  <summary>Details</summary>
Motivation: 量子软件中的不稳定测试（测试结果非确定性通过或失败）对软件可靠性构成严重威胁，但现有研究主要依赖静态分析或小规模手动报告，缺乏对量子测试不稳定性的普遍性、特征和可检测性的大规模动态分析。

Method: 在受控环境中对Qiskit Terra测试套件的23个版本各执行10,000次，测量测试结果变异性，识别不稳定测试，估计经验失败概率，分析跨版本重现性，并使用威尔逊置信区间量化可靠检测所需的重复执行预算。还将不稳定测试映射到Terra子组件以评估组件级敏感性。

Result: 在27,026个测试用例中识别出290个不同的不稳定测试。虽然总体不稳定率较低（0-0.4%），但不稳定性具有高度偶发性：近三分之二的不稳定测试仅出现在一个版本中，而一小部分间歇性或持续性地重现。许多不稳定测试的失败概率非常小（约10^-4），意味着需要数万次执行才能可靠检测。不稳定性在子组件间分布不均，"transpiler"和"quantum_info"组件占比最大。

Conclusion: 量子测试不稳定性虽然罕见，但在典型的持续集成预算下难以检测。研究结果强调了需要更强大的测试策略来应对量子软件特有的不确定性。研究团队发布了公开的每测试执行结果数据集以支持未来研究。

Abstract: Flaky tests, tests that pass or fail nondeterministically without changes to code or environment, pose a serious threat to software reliability. While classical software engineering has developed a rich body of dynamic and static techniques to study flakiness, corresponding evidence for quantum software remains limited. Prior work relies primarily on static analysis or small sets of manually reported incidents, leaving open questions about the prevalence, characteristics, and detectability of flaky tests.
  This paper presents the first large-scale dynamic characterization of flaky tests in quantum software. We executed the Qiskit Terra test suite 10,000 times across 23 releases in controlled environments. For each release, we measured test-outcome variability, identified flaky tests, estimated empirical failure probabilities, analyzed recurrence across versions, and used Wilson confidence intervals to quantify rerun budgets for reliable detection. We further mapped flaky tests to Terra subcomponents to assess component-level susceptibility.
  Across 27,026 test cases, we identified 290 distinct flaky tests. Although overall flakiness rates were low (0-0.4%), flakiness was highly episodic: nearly two-thirds of flaky tests appeared in only one release, while a small subset recurred intermittently or persistently. Many flaky tests failed with very small empirical probabilities ($\hat{p} \approx 10^{-4}$), implying that tens of thousands of executions may be required for confident detection. Flakiness was unevenly distributed across subcomponents, with 'transpiler' and 'quantum_info' accounting for the largest share.
  These results show that quantum test flakiness is rare but difficult to detect under typical continuous integration budgets. To support future research, we release a public dataset of per-test execution outcomes.

</details>


### [52] [Holistic Evaluation of State-of-the-Art LLMs for Code Generation](https://arxiv.org/abs/2512.18131)
*Le Zhang,Suresh Kothari*

Main category: cs.SE

TL;DR: 对6个最先进的大语言模型（包括通用型和代码专用型）进行代码生成能力的实证评估，使用944个LeetCode真实问题，评估编译错误、运行时错误、功能失败和算法次优性等指标，发现DeepSeek-R1和GPT-4.1表现最佳。


<details>
  <summary>Details</summary>
Motivation: 评估不同大语言模型在代码生成任务中的实际表现，为开发者和从业者提供模型选择和部署的指导，识别常见失败场景和改进方向。

Method: 使用包含944个真实LeetCode问题的数据集，涵盖5种编程语言，采用严格的评估指标：编译时错误、运行时错误、功能失败和算法次优性，通过详细案例研究分析常见失败模式。

Result: 不同模型表现存在显著差异，DeepSeek-R1和GPT-4.1在正确性、效率和鲁棒性方面表现最佳；识别出语法错误、逻辑缺陷和次优算法等常见失败场景；提示工程和人工监督对改进结果至关重要。

Conclusion: 成功部署大语言模型进行代码生成需要谨慎的模型选择、有效的提示设计和上下文感知的使用方式；为开发者和从业者提供了可操作的建议，确保在实际软件开发任务中实现可靠的代码生成。

Abstract: This study presents a comprehensive empirical evaluation of six state-of-the-art large language models (LLMs) for code generation, including both general-purpose and code-specialized models. Using a dataset of 944 real-world LeetCode problems across five programming languages, we assess model performance using rigorous metrics: compile-time errors, runtime errors, functional failures, and algorithmic suboptimalities. The results reveal significant performance variations, with DeepSeek-R1 and GPT-4.1 consistently outperform others in terms of correctness, efficiency, and robustness. Through detailed case studies, we identify common failure scenarios such as syntax errors, logical flaws, and suboptimal algorithms, highlighting the critical role of prompt engineering and human oversight in improving results. Based on these findings, we provide actionable recommendations for developers and practitioners, emphasizing that successful LLM deployment depends on careful model selection, effective prompt design, and context-aware usage to ensure reliable code generation in real-world software development tasks.

</details>


### [53] [Understanding Typing-Related Bugs in Solidity Compiler](https://arxiv.org/abs/2512.18182)
*Lantian Li,Yue Pan,Dan Wang,Jingwen Wu,Zhongxing Yu*

Main category: cs.SE

TL;DR: 对Solidity编译器类型相关bug的首个系统性实证研究，分析了146个官方确认的bug，从症状、根因、暴露条件和修复策略四个维度进行分类，揭示了独特的分布模式和12个核心发现。


<details>
  <summary>Details</summary>
Motivation: Solidity编译器的正确性对智能合约安全至关重要，但其类型系统的实现复杂性常引入难以发现的缺陷。目前缺乏对类型相关bug的系统性研究，需要深入理解这些bug的特征和模式。

Method: 从Solidity编译器官方GitHub仓库收集146个官方确认并修复的类型相关bug，对每个bug进行深入分析，从症状、根因、暴露条件和修复策略四个维度进行分类。

Result: 揭示了类型相关bug的独特分布模式和关键特征，总结了12个核心发现。研究发现这些bug具有特定的模式，为检测和修复提供了新的见解。

Conclusion: 该研究不仅加深了对Solidity编译器固有弱点的理解，还为检测和修复类型相关bug提供了新的见解和方向，对提升智能合约安全性具有重要意义。

Abstract: The correctness of the Solidity compiler is crucial for ensuring the security of smart contracts. However, the implementation complexity of its type system often introduces elusive defects. This paper presents the first systematic empirical study on typing-related bugs in the Solidity compiler. To systematically analyze these bugs, we collected 146 officially confirmed and fixed typing-related bugs from the official GitHub repository of Solidity compiler. For each bug, we conducted an in-depth analysis and classification from four dimensions: symptoms, root causes, exposure conditions, and fix strategies. Through this study, we reveal unique distribution patterns and key characteristics of such bugs, and summarize 12 core findings. We additionally give the implications of our findings, and these implications not only deepen the understanding of inherent weaknesses in the Solidity compiler but also provide new insights for detecting and fixing typing-related bugs in the Solidity compiler.

</details>


### [54] [Software Vulnerability Management in the Era of Artificial Intelligence: An Industry Perspective](https://arxiv.org/abs/2512.18261)
*M. Mehdi Kholoosi,Triet Huynh Minh Le,M. Ali Babar*

Main category: cs.SE

TL;DR: 该研究调查了AI工具在软件漏洞管理中的行业应用现状，发现69%用户对当前使用满意，但存在误报、上下文缺失和信任问题等挑战，提出了可解释性、上下文感知等改进建议。


<details>
  <summary>Details</summary>
Motivation: 虽然AI在软件开发中已广泛应用，但在软件漏洞管理（如漏洞检测和修复）方面的行业应用研究不足，需要了解AI工具在行业中的实际采用情况、障碍和促进因素。

Method: 采用混合方法调查，涉及27个国家、60名行业从业者，通过定量和定性问题分析采用趋势、评估工具优势、识别实际挑战和改进机会。

Result: AI工具已在整个软件漏洞管理生命周期中使用，69%用户对当前使用满意；工具因速度快、覆盖广、易访问而受重视；但误报、上下文缺失和信任问题普遍存在；观察到社会技术采用模式，即AI输出需经过人工监督和组织治理过滤。

Conclusion: 为支持AI在软件漏洞管理中的安全有效使用，建议改进可解释性、上下文感知、集成工作流和验证实践；研究结果为从业者、工具开发者和研究人员提供了实用指导。

Abstract: Artificial Intelligence (AI) has revolutionized software development, particularly by automating repetitive tasks and improving developer productivity. While these advancements are well-documented, the use of AI-powered tools for Software Vulnerability Management (SVM), such as vulnerability detection and repair, remains underexplored in industry settings. To bridge this gap, our study aims to determine the extent of the adoption of AI-powered tools for SVM, identify barriers and facilitators to the use, and gather insights to help improve the tools to meet industry needs better. We conducted a survey study involving 60 practitioners from diverse industry sectors across 27 countries. The survey incorporates both quantitative and qualitative questions to analyze the adoption trends, assess tool strengths, identify practical challenges, and uncover opportunities for improvement. Our findings indicate that AI-powered tools are used throughout the SVM life cycle, with 69% of users reporting satisfaction with their current use. Practitioners value these tools for their speed, coverage, and accessibility. However, concerns about false positives, missing context, and trust issues remain prevalent. We observe a socio-technical adoption pattern in which AI outputs are filtered through human oversight and organizational governance. To support safe and effective use of AI for SVM, we recommend improvements in explainability, contextual awareness, integration workflows, and validation practices. We assert that these findings can offer practical guidance for practitioners, tool developers, and researchers seeking to enhance secure software development through the use of AI.

</details>


### [55] [Code2Doc: A Quality-First Curated Dataset for Code Documentation](https://arxiv.org/abs/2512.18748)
*Recep Kaan Karaman,Meftun Akarsu*

Main category: cs.SE

TL;DR: Code2Doc是一个高质量代码文档生成数据集，通过四阶段筛选流程从开源项目中提取13,358个高质量函数-文档对，覆盖5种编程语言，显著提升模型性能


<details>
  <summary>Details</summary>
Motivation: 现有代码文档数据集大多通过大规模爬取公共仓库构建，质量控制有限，存在噪声文档、重复内容和AI生成内容污染等问题，这些质量问题削弱了基于学习模型的监督信号并复杂化了评估

Method: 采用四阶段筛选流程构建Code2Doc数据集：1) 强制文档完整性和清晰度；2) 基于结构和复杂度标准筛选函数；3) 移除精确和近似重复代码；4) 识别可能为AI生成的文档。从52,069个候选样本中，只有25.6%满足所有质量约束

Result: 数据集包含13,358个高质量函数-文档对，平均文档质量得分6.93/10，86.9%样本包含显式类型标注，仅2.9%被标记为可能AI生成。基线实验显示，在Code2Doc上微调大语言模型相比零样本性能，BLEU提升29.47%，ROUGE-L提升24.04%

Conclusion: Code2Doc提供了一个高质量、经过严格筛选的代码文档生成数据集，显著提升了模型性能，证明了数据质量对代码文档生成任务的重要性。作者发布了数据集和完整筛选流程以支持可重复研究

Abstract: The performance of automatic code documentation generation models depends critically on the quality of the training data used for supervision. However, most existing code documentation datasets are constructed through large scale scraping of public repositories with limited quality control. As a result, they often contain noisy documentation, extensive duplication, and increasing contamination from AI generated content. These issues weaken the supervision signal available to learning-based models and complicate evaluation.
  We introduce \textbf{Code2Doc}, a quality-first curated dataset for function-level code documentation generation. Code2Doc consists of 13,358 high-quality function-documentation pairs extracted from widely used open-source repositories spanning five programming languages: Python, Java, TypeScript, JavaScript, and C++. The dataset is constructed using a four-stage curation pipeline that enforces documentation completeness and clarity, filters functions based on structural and complexity criteria, removes exact and near-duplicate code, and identifies documentation likely to be AI generated. Starting from 52,069 extracted candidates, only 25.6 percent satisfy all quality constraints.
  We provide a detailed analysis of the resulting dataset, which achieves a mean documentation quality score of 6.93 out of 10. Overall, 86.9% of samples contain explicit type annotations, and only 2.9\% are flagged as potentially AI generated. Baseline experiments show that fine-tuning a large language model on Code2Doc yields relative improvements of 29.47% in BLEU and 24.04% in ROUGE-L over zero shot performance, despite the modest dataset size. We release both the dataset and the full curation pipeline to support reproducible research on automatic code documentation generation.

</details>


### [56] [Misbehavior Forecasting for Focused Autonomous Driving Systems Testing](https://arxiv.org/abs/2512.18823)
*M M Abid Naziri,Stefano Carlo Lambertenghi,Andrea Stocco,Marcelo d'Amorim*

Main category: cs.SE

TL;DR: Foresee是一种基于仿真测试的自动驾驶软件故障检测技术，通过识别"近失事件"并局部模糊测试来发现潜在故障，比现有方法更有效和高效。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶软件仿真测试中的故障发现技术要么不可靠，要么成本高昂。研究者发现仿真中观察到的"近失事件"可能指向潜在故障，因此需要一种更有效的方法来识别这些近失事件并发现未知故障。

Method: 提出Foresee技术：1）使用误行为预测器计算被测试车辆可能的未来状态来识别近失事件；2）对每个候选近失事件进行局部模糊测试以发现未知故障；3）在CARLA模拟器中评估不同配置，并与最先进的模糊测试工具DriveFuzz进行比较。

Result: Foresee比基线方法更有效和高效：比随机方法多发现128.70%的故障，比最先进的故障预测器多发现38.09%的故障，速度分别快2.49倍和1.42倍。与DriveFuzz结合使用时，故障检测率提升高达93.94%。

Conclusion: Foresee通过识别近失事件并进行局部模糊测试，显著提高了自动驾驶软件仿真测试的故障发现效率和效果，为自动驾驶系统的可靠性验证提供了更强大的工具。

Abstract: Simulation-based testing is the standard practice for assessing the reliability of self-driving cars' software before deployment. Existing bug-finding techniques are either unreliable or expensive. We build on the insight that near misses observed during simulations may point to potential failures. We propose Foresee, a technique that identifies near misses using a misbehavior forecaster that computes possible future states of the ego-vehicle under test. Foresee performs local fuzzing in the neighborhood of each candidate near miss to surface previously unknown failures. In our empirical study, we evaluate the effectiveness of different configurations of Foresee using several scenarios provided in the CARLA simulator on both end-to-end and modular self-driving systems and examine its complementarity with the state-of-the-art fuzzer DriveFuzz. Our results show that Foresee is both more effective and more efficient than the baselines. Foresee exposes 128.70% and 38.09% more failures than a random approach and a state-of-the-art failure predictor while being 2.49x and 1.42x faster, respectively. Moreover, when used in combination with DriveFuzz, Foresee enhances failure detection by up to 93.94%.

</details>


### [57] [What Drives Issue Resolution Speed? An Empirical Study of Scientific Workflow Systems on GitHub](https://arxiv.org/abs/2512.18852)
*Khairul Alam,Banani Roy*

Main category: cs.SE

TL;DR: 对GitHub上科学工作流系统（SWS）项目中的21,116个问题进行分析，研究项目特征、问题元数据和贡献者互动如何影响问题解决速度，发现68.91%的问题被关闭，半数在18.09天内解决，标签和分配问题能加快解决速度。


<details>
  <summary>Details</summary>
Motivation: 科学工作流系统对可重复、可扩展和自动化的科学分析至关重要，但尽管及时解决问题对软件质量和社区信任很重要，目前对SWS中问题解决速度的驱动因素了解甚少。

Method: 对GitHub托管的SWS项目集合进行实证研究，分析21,116个问题，研究项目特征、问题元数据和贡献者互动如何影响问题关闭时间，重点关注两个研究问题：问题如何被管理和解决，以及问题和贡献者特征如何与问题解决速度相关。

Result: 研究发现68.91%的问题被关闭，其中一半在18.09天内解决；虽然SWS项目遵循结构化的问题管理实践，但问题解决速度在不同系统间差异很大；标签和分配问题等操作与更快的问题解决速度相关。

Conclusion: 基于研究结果，为开发者提供建议以更好地管理SWS仓库问题并提高其质量，强调标签和分配问题等实践的重要性。

Abstract: Scientific Workflow Systems (SWSs) play a vital role in enabling reproducible, scalable, and automated scientific analysis. Like other open-source software, these systems depend on active maintenance and community engagement to remain reliable and sustainable. However, despite the importance of timely issue resolution for software quality and community trust, little is known about what drives issue resolution speed within SWSs. This paper presents an empirical study of issue management and resolution across a collection of GitHub-hosted SWS projects. We analyze 21,116 issues to investigate how project characteristics, issue metadata, and contributor interactions affect time-to-close. Specifically, we address two research questions: (1) how issues are managed and addressed in SWSs, and (2) how issue and contributor features relate to issue resolution speed. We find that 68.91% of issues are closed, with half of them resolved within 18.09 days. Our results show that although SWS projects follow structured issue management practices, the issue resolution speed varies considerably across systems. Factors such as labeling and assigning issues are associated with faster issue resolution. Based on our findings, we make recommendations for developers to better manage SWS repository issues and improve their quality.

</details>


### [58] [An Empirical Study of Developer-Provided Context for AI Coding Assistants in Open-Source Projects](https://arxiv.org/abs/2512.18925)
*Shaokang Jiang,Daye Nam*

Main category: cs.SE

TL;DR: 本文对401个开源仓库中的cursor规则进行大规模实证研究，分析了开发者提供的项目上下文内容，建立了包含5个高层主题的综合分类法，并探讨了不同项目类型和编程语言中的上下文差异。


<details>
  <summary>Details</summary>
Motivation: 虽然大型语言模型在软件工程中表现出色，但其效果不仅取决于显式提示，还依赖于更广泛的上下文。现有AI编码助手允许开发者编写持久化、机器可读的指令来编码项目独特约束，但这种实践的内容尚未被系统研究。

Method: 通过对401个包含cursor规则的开源仓库进行定性分析，开发了全面的项目上下文分类法，并研究了不同项目类型和编程语言中的上下文变化。

Result: 建立了包含5个高层主题的综合分类法：约定、指南、项目信息、LLM指令和示例。研究发现不同项目类型和编程语言中的上下文内容存在显著差异。

Conclusion: 这项研究为下一代上下文感知的AI开发工具提供了重要启示，帮助理解开发者认为至关重要的项目上下文内容及其在不同环境中的变化。

Abstract: While Large Language Models (LLMs) have demonstrated remarkable capabilities, research shows that their effectiveness depends not only on explicit prompts but also on the broader context provided. This requirement is especially pronounced in software engineering, where the goals, architecture, and collaborative conventions of an existing project play critical roles in response quality. To support this, many AI coding assistants have introduced ways for developers to author persistent, machine-readable directives that encode a project's unique constraints. Although this practice is growing, the content of these directives remains unstudied.
  This paper presents a large-scale empirical study to characterize this emerging form of developer-provided context. Through a qualitative analysis of 401 open-source repositories containing cursor rules, we developed a comprehensive taxonomy of project context that developers consider essential, organized into five high-level themes: Conventions, Guidelines, Project Information, LLM Directives, and Examples. Our study also explores how this context varies across different project types and programming languages, offering implications for the next generation of context-aware AI developer tools.

</details>


### [59] [Scrum Sprint Planning: LLM-based and algorithmic solutions](https://arxiv.org/abs/2512.18966)
*Yuwon Yoon,Kevin Iwan,Madeleine Zwart,Xiaohan Qin,Hina Lee,Maria Spichkova*

Main category: cs.SE

TL;DR: 本文探讨了大型语言模型在Scrum敏捷开发中迭代计划（sprint planning）的适用性，通过案例研究发现当前OpenAI模型（GPT-3.5/4.0 Turbo、Val）的输出质量尚无法直接用于实际项目。


<details>
  <summary>Details</summary>
Motivation: Scrum敏捷开发中的迭代计划是核心活动，作者希望探索大型语言模型在这一关键环节的适用性，以评估AI技术能否有效支持软件开发项目管理。

Method: 使用手动创建的数据集进行案例研究，测试了OpenAI提供的三个模型：GPT-3.5 Turbo、GPT-4.0 Turbo和Val，评估它们在sprint planning活动中的表现。

Result: 实验结果表明，这些模型生成的结果质量尚达不到可接受的水平，无法直接应用于实际的Scrum项目中，说明当前技术在该领域的应用存在局限性。

Conclusion: 虽然大型语言模型在软件开发领域有应用潜力，但当前版本的OpenAI模型在sprint planning这一具体任务上的表现还不够成熟，需要进一步改进才能实际支持敏捷开发流程。

Abstract: Planning for an upcoming project iteration (sprint) is one of the key activities in Scrum planning. In this paper, we present our work in progress on exploring the applicability of Large Language Models (LLMs) for solving this problem. We conducted case studies with manually created data sets to investigate the applicability of OpenAI models for supporting the sprint planning activities. In our experiments, we applied three models provided OpenAI: GPT-3.5 Turbo, GPT-4.0 Turbo, and Val. The experiments demonstrated that the results produced by the models aren't of acceptable quality for direct use in Scrum projects.

</details>


### [60] [BanglaForge: LLM Collaboration with Self-Refinement for Bangla Code Generation](https://arxiv.org/abs/2512.19122)
*Mahir Labib Dihan,Sadif Ahmed,Md Nafiu Rahman*

Main category: cs.SE

TL;DR: BanglaForge框架通过检索增强的双模型协作和自我精炼，解决了孟加拉语代码生成的资源匮乏问题，在BLP-2025基准测试中达到84.00%的Pass@1准确率。


<details>
  <summary>Details</summary>
Motivation: 孟加拉语作为低资源语言，缺乏大规模标注数据集和将自然语言规范转换为可执行程序的工具，使得孟加拉语到代码生成成为具有挑战性的任务。

Method: 采用检索增强的双模型协作范式，结合上下文学习、基于LLM的翻译、系统提示工程和基于执行反馈的迭代自我精炼。一个编码器生成初始解决方案，一个审查器增强其鲁棒性。

Result: 在BLP-2025孟加拉语代码生成基准测试中，BanglaForge实现了84.00%的竞争性Pass@1准确率。

Conclusion: 检索、模型协作和自我精炼对于低资源孟加拉语代码生成是有效的，BanglaForge框架为解决此类问题提供了创新解决方案。

Abstract: Bangla is a low-resource language for code generation, lacking large-scale annotated datasets and tools to transform natural language specifications into executable programs. This makes Bangla-to-code generation a challenging task requiring innovative solutions. To address this, we introduce BanglaForge, a novel framework for generating code from Bangla function descriptions. BanglaForge leverages a retrieval-augmented dual-model collaboration paradigm with self-refinement, combining in-context learning, llm-based translation, systematic prompt engineering, and iterative self-refinement based on execution feedback, where a coder generates initial solutions and a reviewer enhances them for robustness. On the BLP-2025 Bangla Code Generation benchmark, BanglaForge achieves a competitive Pass@1 accuracy of 84.00%, demonstrating the effectiveness of retrieval, model collaboration, and self-refinement for low-resource Bangla code generation.

</details>


### [61] [University Rents Enabling Corporate Innovation: Mapping Academic Researcher Coding and Discursive Labour in the R Language Ecosystem](https://arxiv.org/abs/2512.19153)
*Xiaolan Cai,Mathieu O'Neil,Stefano Zacchiroli*

Main category: cs.SE

TL;DR: 该研究通过分析R统计软件生态系统中研究人员的编码和话语贡献，揭示了企业创新系统中未被认可的劳动，特别是研究人员在创建和维护关键统计基础设施方面的无偿贡献。


<details>
  <summary>Details</summary>
Motivation: 现有研究通常关注在线平台如何约束参与者行为并从其劳动中获利，但缺乏将平台内劳动与参与者专业就业联系起来的研究。本文旨在探索研究人员在企业创新系统中未被认可的劳动贡献。

Method: 案例研究分析了GitHub上的8,924个R包仓库，通过定量分析提交记录和沟通数据，结合定性分析揭示未被认可劳动对从业者的影响，并分析自由开源软件（FLOSS）意识形态和实践。

Result: 研究发现研究人员和非附属贡献者是R包仓库最频繁的所有者和最活跃的贡献者；研究人员更可能担任官方角色，参与协作问题解决和支持工作；存在一个"未被认可"的研究人员类别，他们无偿创建和维护关键统计基础设施并为行业员工提供支持。

Conclusion: 自由开源软件意识形态和实践合法化了大型科技公司对"大学租金"的利用，揭示了企业创新系统中存在大量未被认可的研究人员劳动，这些劳动对统计基础设施建设和行业支持至关重要但缺乏相应报酬。

Abstract: This article explores the role of unrecognised labour in corporate innovation systems via an analysis of researcher coding and discursive contributions to R, one of the largest statistical software ecosystems. Studies of online platforms typically focus on how platform affordances constrain participants' actions, and profit from their labour. We innovate by connecting the labour performed inside digital platforms to the professional employment of participants. Our case study analyses 8,924 R package repositories on GitHub, examining commits and communications. Our quantitative findings show that researchers, alongside non-affiliated contributors, are the most frequent owners of R package repositories and their most active contributors. Researchers are more likely to hold official roles compared to the average, and to engage in collaborative problem-solving and support work during package development. This means there is, underneath the 'recognised' category of star researchers who transition between academia and industry and secure generous funding, an 'unrecognised' category of researchers who not only create and maintain key statistical infrastructure, but also provide support to industry employees, for no remuneration. Our qualitative findings show how this unrecognised labour affects practitioners. Finally, our analysis of the ideology and practice of free, libre and open source software (FLOSS) shows how this ideology and practice legitimate the use of 'university rents' by Big Tech.

</details>


### [62] [Semantically-Equivalent Transformations-Based Backdoor Attacks against Neural Code Models: Characterization and Mitigation](https://arxiv.org/abs/2512.19215)
*Junyao Ye,Zhen Li,Xi Tang,Shouhuai Xu,Deqing Zou,Zhongsheng Yuan*

Main category: cs.SE

TL;DR: 本文提出了一种新的基于语义等价变换的后门攻击方法，该方法使用语义保持的低流行度代码变换生成隐蔽触发器，相比传统的注入式攻击更难以检测和防御。


<details>
  <summary>Details</summary>
Motivation: 当前对神经代码模型后门攻击的理解主要集中于注入式攻击，这种攻击可以通过标准净化技术中和，可能导致对后门攻击安全的错误认知。需要研究更隐蔽、更难以检测的新型后门攻击方法。

Method: 提出了基于语义等价变换的后门攻击框架，使用语义保持的低流行度代码变换生成隐蔽触发器。在五个任务、六种编程语言和多种模型（CodeBERT、CodeT5、StarCoder）上进行实验验证。

Result: SET-based攻击实现了高成功率（通常>90%）同时保持模型效用。攻击具有高度隐蔽性，检测率比注入式攻击平均低25.13%以上，能够规避最先进的防御机制。基于规范化的对策仅提供部分缓解。

Conclusion: SET-based后门攻击展示了强大的攻击效果和隐蔽性，现有防御措施效果有限，需要进一步研究针对此类攻击的可扩展防御方法。

Abstract: Neural code models have been increasingly incorporated into software development processes. However, their susceptibility to backdoor attacks presents a significant security risk. The state-of-the-art understanding focuses on injection-based attacks, which insert anomalous patterns into software code. These attacks can be neutralized by standard sanitization techniques. This status quo may lead to a false sense of security regarding backdoor attacks. In this paper, we introduce a new kind of backdoor attacks, dubbed Semantically-Equivalent Transformation (SET)-based backdoor attacks, which use semantics-preserving low-prevalence code transformations to generate stealthy triggers. We propose a framework to guide the generation of such triggers. Our experiments across five tasks, six languages, and models like CodeBERT, CodeT5, and StarCoder show that SET-based attacks achieve high success rates (often >90%) while preserving model utility. The attack proves highly stealthy, evading state-of-the-art defenses with detection rates on average over 25.13% lower than injection-based counterparts. We evaluate normalization-based countermeasures and find they offer only partial mitigation, confirming the attack's robustness. These results motivate further investigation into scalable defenses tailored to SET-based attacks.

</details>


### [63] [A Dataset and Preliminary Study of Using GPT-5 for Code-change Impact Analysis](https://arxiv.org/abs/2512.19481)
*Katharina Stengg,Christian Macho,Martin Pinzger*

Main category: cs.SE

TL;DR: 论文研究了GPT-5和GPT-5-mini在预测源代码变更影响方面的能力，发现两者表现均不佳，但GPT-5优于GPT-5-mini，且提供差异块信息能略微提升性能。


<details>
  <summary>Details</summary>
Motivation: 理解源代码变更及其对其他代码实体的影响是软件开发中的关键技能，但目前这种分析通常是手动进行的，耗时且效率低。虽然大型语言模型在代码分析任务中显示出潜力，但它们在理解代码变更影响方面的能力尚未得到充分探索。

Method: 构建了一个包含种子变更、变更对和变更类型信息的数据集，评估GPT-5和GPT-5-mini在两种配置下的表现：(1)种子变更信息和父提交树；(2)种子变更信息、父提交树和每个种子变更的差异块。

Result: 两个LLM在两种实验配置下表现都不佳，但GPT-5优于GPT-5-mini。提供差异块信息能帮助两个模型略微提升性能。

Conclusion: 当前的大型语言模型在预测代码变更影响方面的能力有限，需要进一步研究如何更好地利用这些模型来理解代码变更及其影响。

Abstract: Understanding source code changes and their impact on other code entities is a crucial skill in software development. However, the analysis of code changes and their impact is often performed manually and therefore is time-consuming. Recent advancements in AI, and in particular large language models (LLMs) show promises to help developers in various code analysis tasks. However, the extent to which this potential can be utilized for understanding code changes and their impact is underexplored. To address this gap, we study the capabilities of GPT-5 and GPT-5-mini to predict the code entities impacted by given source code changes. We construct a dataset containing information about seed-changes, change pairs, and change types for each commit. Existing datasets lack crucial information about seed changes and impacted code entities. Our experiments evaluate the LLMs in two configurations: (1) seed-change information and the parent commit tree and (2) seed-change information, the parent commit tree, and the diff hunk of each seed change. We found that both LLMs perform poorly in the two experiments, whereas GPT-5 outperforms GPT-5-mini. Furthermore, the provision of the diff hunks helps both models to slightly improve their performance.

</details>


### [64] [Beyond Language Boundaries: Uncovering Programming Language Families for Code Language Models](https://arxiv.org/abs/2512.19509)
*Shangbo Yun,Xiaodong Gu,Jianghong Huang,Beijun Shen*

Main category: cs.SE

TL;DR: 该研究提出了一种基于嵌入的框架来揭示编程语言之间的深层语言学关系，并利用这些关系优化多语言代码大语言模型的训练和推理。


<details>
  <summary>Details</summary>
Motivation: 现有技术通常只是简单地聚合多语言代码数据来训练代码LLM，很少探索编程语言之间的深层关系以及如何利用这些关系来优化模型的训练和推理。

Method: 定义了21个主要语言学特征，使用LLM生成跨语言的特征对齐代码样本，通过嵌入这些语义平行的代码片段构建相似度矩阵，并进行层次聚类来揭示语言关系。基于发现的语言家族，提出了三种增强多语言LLM训练的策略：跨语言相关语言的迁移学习、语言学邻近引导的课程学习、基于质心的中间代码翻译。

Result: 分析揭示了编程语言之间清晰的层次结构，密切相关的语言形成明确定义的聚类（如C、C++、Java和Swift聚在一起），而Go表现出最高的跨语言相似性。在4个代码智能任务上的实验表明，所提方法显著提高了多语言LLM的性能。

Conclusion: 这项工作为编程语言提供了一个通用视角，并推进了多语言代码LLM训练的更有效策略。

Abstract: The rapid proliferation of diverse programming languages presents both opportunities and challenges for developing multilingual code LLMs. While existing techniques often train code LLMs by simply aggregating multilingual code data, few explore the deeper relationships between programming languages(PLs) and how such relationships can be utilized to optimize the training and inference of code LLMs. In this work, we investigate 2 fundamental questions: 1) What are the deep linguistic relationships among PLs? and 2) How can these relationships be leveraged to improve multilingual code LLMs? We propose an embedding-based framework to uncover the latent families of PLs. Our approach begins by defining 21 primary linguistic features of programming languages, such as variable definition, control structures, and method declarations, and then employs LLMs to generate feature-aligned code samples across multiple languages. By embedding these semantically parallel code snippets from 19 languages, we construct a similarity matrix and perform hierarchical clustering to uncover inherent language relationships. Our analysis reveals clear hierarchical structures among programming languages. Closely related languages form well-defined clusters (e.g., C, C++, Java, and Swift group together), while Go exhibits as a central language with the highest cross-language similarity. Building on the uncovered language families, we propose three strategies to enhance multilingual LLM training: transfer learning across linguistically related languages, linguistic proximity-guided curriculum learning, and centroid-based intermediary code translation. Experiments on 4 code intelligence tasks demonstrate that our methods significantly improve multilingual LLM performance. This work offers a universal perspective on programming languages and advances more effective strategies for multilingual code LLM training.

</details>


### [65] [More code, less validation: Risk factors for over-reliance on AI coding tools among scientists](https://arxiv.org/abs/2512.19644)
*Gabrielle O'Brien,Alexis Parker,Nasir Eisty,Jeffrey Carver*

Main category: cs.SE

TL;DR: 科学编程人员广泛采用生成式AI工具，特别是ChatGPT等通用对话界面，但存在过度依赖风险，尤其是经验不足者。研究发现，代码生成量而非验证质量成为生产力感知的主要指标，这引发了对研究代码完整性的担忧。


<details>
  <summary>Details</summary>
Motivation: 科学研究对编程需求日益增长，但科学家普遍缺乏足够的软件开发培训。生成式AI代码生成工具可能提供支持，但用户研究表明存在过度依赖风险，特别是在经验不足的用户中。需要了解科学编程人员如何采用这些工具、他们的偏好模式以及影响生产力感知的因素。

Method: 对868名从事编程的科学家进行问卷调查，研究内容包括：生成式AI工具的采用模式、工具偏好、与感知生产力相关的因素。分析不同经验水平、学科领域、开发实践使用情况等因素的影响。

Result: 1. 学生和经验较少的程序员采用率最高，不同学科领域存在差异；2. 科学编程人员强烈偏好ChatGPT等通用对话界面而非专门的开发者工具；3. 经验不足和开发实践使用有限都与更高的感知生产力相关，但两者存在交互作用，表明正式开发实践可以部分弥补经验不足；4. 感知生产力的最强预测因素是每次接受的生成代码行数。

Conclusion: 科学编程人员使用生成式AI时可能通过代码生成量而非验证质量来衡量生产力，这引发了研究代码完整性的担忧。研究建议需要更好的培训和支持，以确保生成式AI在科学研究中的负责任使用。

Abstract: Programming is essential to modern scientific research, yet most scientists report inadequate training for the software development their work demands. Generative AI tools capable of code generation may support scientific programmers, but user studies indicate risks of over-reliance, particularly among inexperienced users. We surveyed 868 scientists who program, examining adoption patterns, tool preferences, and factors associated with perceived productivity. Adoption is highest among students and less experienced programmers, with variation across fields. Scientific programmers overwhelmingly prefer general-purpose conversational interfaces like ChatGPT over developer-specific tools. Both inexperience and limited use of development practices (like testing, code review, and version control) are associated with greater perceived productivity-but these factors interact, suggesting formal practices may partially compensate for inexperience. The strongest predictor of perceived productivity is the number of lines of generated code typically accepted at once. These findings suggest scientific programmers using generative AI may gauge productivity by code generation rather than validation, raising concerns about research code integrity.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [66] [FedWiLoc: Federated Learning for Privacy-Preserving WiFi Indoor Localization](https://arxiv.org/abs/2512.18207)
*Kanishka Roy,Tahsin Fuad Hasan,Chenfeng Wu,Eshwar Vangala,Roshan Ayyalasomayajula*

Main category: cs.CR

TL;DR: FedWiLoc是一个隐私保护的Wi-Fi室内定位系统，通过联邦学习、分割架构和几何损失函数解决隐私保护、动态多径环境精度和跨部署泛化三大挑战。


<details>
  <summary>Details</summary>
Motivation: 当前数据驱动的Wi-Fi室内定位系统面临三大关键挑战：保护用户隐私、在动态多径环境中实现准确预测以及跨不同部署的泛化能力。传统Wi-Fi定位系统经常在面临受攻击的接入点或中间人攻击时损害用户隐私。随着物联网设备在室内环境中的普及，开发既能提供准确定位又能强有力保护隐私的解决方案变得至关重要。

Method: FedWiLoc采用三种关键创新：1）分割架构，接入点在本地处理信道状态信息，仅传输隐私保护的嵌入向量到用户设备，防止原始CSI暴露；2）训练期间使用联邦学习在接入点间协作训练模型，无需集中敏感用户数据；3）引入几何损失函数，联合优化到达角预测和位置估计，强制几何一致性以提高在挑战性多径条件下的准确性。

Result: 在六个不同室内环境（总面积超过2000平方英尺）的广泛评估表明，FedWiLoc在定位中值误差方面优于最先进方法达61.9%，同时在训练和推理过程中保持强大的隐私保证。

Conclusion: FedWiLoc成功解决了隐私保护、动态多径环境精度和跨部署泛化三大挑战，通过创新的分割架构、联邦学习和几何损失函数，在保持强大隐私保护的同时显著提高了室内定位精度。

Abstract: Current data-driven Wi-Fi-based indoor localization systems face three critical challenges: protecting user privacy, achieving accurate predictions in dynamic multipath environments, and generalizing across different deployments. Traditional Wi-Fi localization systems often compromise user privacy, particularly when facing compromised access points (APs) or man-in-the-middle attacks. As IoT devices proliferate in indoor environments, developing solutions that deliver accurate localization while robustly protecting privacy has become imperative. We introduce FedWiLoc, a privacy-preserving indoor localization system that addresses these challenges through three key innovations. First, FedWiLoc employs a split architecture where APs process Channel State Information (CSI) locally and transmit only privacy-preserving embedding vectors to user devices, preventing raw CSI exposure. Second, during training, FedWiLoc uses federated learning to collaboratively train the model across APs without centralizing sensitive user data. Third, we introduce a geometric loss function that jointly optimizes angle-of-arrival predictions and location estimates, enforcing geometric consistency to improve accuracy in challenging multipath conditions. Extensive evaluation across six diverse indoor environments spanning over 2,000 sq. ft. demonstrates that FedWiLoc outperforms state-of-the-art methods by up to 61.9% in median localization error while maintaining strong privacy guarantees throughout both training and inference.

</details>


### [67] [Theodosian: A Deep Dive into Memory-Hierarchy-Centric FHE Acceleration](https://arxiv.org/abs/2512.18345)
*Wonseok Choi,Hyunah Yu,Jongmin Kim,Hyesung Ji,Jaiyoung Park,Jung Ho Ahn*

Main category: cs.CR

TL;DR: 论文对现代GPU上的CKKS全同态加密方案进行微架构分析，发现内存带宽限制和硬件利用率低的问题，提出Theodosian优化方案，显著提升性能


<details>
  <summary>Details</summary>
Motivation: 全同态加密(FHE)在云和边缘环境中能保护数据隐私，但计算和内存需求高，需要硬件加速。CKKS是流行的FHE方案，在GPU上存在性能瓶颈需要优化

Method: 对现代GPU上的CKKS进行微架构分析，重点关注片上缓存行为，发现内存带宽限制和硬件利用率低的问题，提出Theodosian优化方案，包括内存感知优化以提高缓存效率和减少运行时开销

Result: 在RTX 5090上，Theodosian将32,768个复数的自举延迟降低到15.2ms，结合额外算法优化进一步降低到12.8ms，建立了新的GPU性能最先进水平

Conclusion: CKKS在GPU上的性能受内存带宽限制和硬件利用率低制约，Theodosian优化方案通过内存感知优化有效提升缓存效率和性能，为FHE加速提供新思路

Abstract: Fully homomorphic encryption (FHE) enables secure computation on encrypted data, mitigating privacy concerns in cloud and edge environments. However, due to its high compute and memory demands, extensive acceleration research has been pursued across diverse hardware platforms, especially GPUs. In this paper, we perform a microarchitectural analysis of CKKS, a popular FHE scheme, on modern GPUs. We focus on on-chip cache behavior, and show that the dominant kernels remain bound by memory bandwidth despite a high-bandwidth L2 cache, exposing a persistent memory wall. We further discover that the overall CKKS pipeline throughput is constrained by low per-kernel hardware utilization, caused by insufficient intra-kernel parallelism. Motivated by these findings, we introduce Theodosian, a set of complementary, memory-aware optimizations that improve cache efficiency and reduce runtime overheads. Our approach delivers consistent speedups across various CKKS workloads. On an RTX 5090, we reduce the bootstrapping latency for 32,768 complex numbers to 15.2ms with Theodosian, and further to 12.8ms with additional algorithmic optimizations, establishing new state-of-the-art GPU performance to the best of our knowledge.

</details>


### [68] [SoK: Understanding (New) Security Issues Across AI4Code Use Cases](https://arxiv.org/abs/2512.18456)
*Qilong Wu,Taoran Li,Tianyang Zhou,Varun Chandrasekaran*

Main category: cs.CR

TL;DR: 这篇SoK论文系统调查了AI4Code（AI辅助编程）系统的安全现状，揭示了当前AI代码工具在安全方面的主要缺陷，包括不安全的输出、有偏见的基准测试、对抗性攻击脆弱性等问题，并提出了向安全优先AI4Code转型的路径。


<details>
  <summary>Details</summary>
Motivation: 随着AI4Code系统（如GitHub Copilot）在代码生成、翻译和漏洞检测等软件工程领域的广泛应用，其安全风险日益凸显。不安全输出、有偏见的基准测试、对抗性攻击脆弱性等问题严重影响了这些系统的可靠性，需要系统性地调查和解决这些安全问题。

Method: 论文采用系统知识（SoK）调查方法，从三个核心应用领域（代码生成、漏洞检测、代码翻译）全面分析AI4Code安全现状。通过对六个最先进模型的比较研究，识别出重复出现的安全差距，包括基准测试的局限性、数据集标准化不足、评估中的数据泄露问题以及对抗鲁棒性脆弱性。

Result: 研究发现：1）代码生成中持续存在不安全模式；2）漏洞检测对语义保持攻击脆弱；3）微调经常与安全目标错位；4）代码翻译带来的安全效益不均衡。主要安全差距包括：Python和玩具问题主导基准测试、缺乏标准化安全数据集、评估中的数据泄露、脆弱的对抗鲁棒性。

Conclusion: 论文提出了三个发展方向：1）在代码生成中嵌入默认安全实践；2）建立鲁棒全面的检测基准；3）利用代码翻译作为安全增强语言的途径。呼吁向"安全优先"的AI4Code转型，将漏洞缓解和鲁棒性嵌入整个开发生命周期。

Abstract: AI-for-Code (AI4Code) systems are reshaping software engineering, with tools like GitHub Copilot accelerating code generation, translation, and vulnerability detection. Alongside these advances, however, security risks remain pervasive: insecure outputs, biased benchmarks, and susceptibility to adversarial manipulation undermine their reliability. This SoK surveys the landscape of AI4Code security across three core applications, identifying recurring gaps: benchmark dominance by Python and toy problems, lack of standardized security datasets, data leakage in evaluation, and fragile adversarial robustness. A comparative study of six state-of-the-art models illustrates these challenges: insecure patterns persist in code generation, vulnerability detection is brittle to semantic-preserving attacks, fine-tuning often misaligns security objectives, and code translation yields uneven security benefits. From this analysis, we distill three forward paths: embedding secure-by-default practices in code generation, building robust and comprehensive detection benchmarks, and leveraging translation as a route to security-enhanced languages. We call for a shift toward security-first AI4Code, where vulnerability mitigation and robustness are embedded throughout the development life cycle.

</details>


### [69] [Exploring Runtime Evolution in Android: A Cross-Version Analysis and Its Implications for Memory Forensics](https://arxiv.org/abs/2512.18517)
*Babangida Bappah,Lauren G Bristol,Lamine Noureddine,Sideeq Bello,Umar Farooq,Aisha Ali-Gombe*

Main category: cs.CR

TL;DR: 本文首次系统研究了Android运行时（ART）结构演化及其对内存取证的影响，发现超过73.2%的结构成员发生位置变化，严重影响取证工具的适应性和可靠性，建议采用混合方法改进。


<details>
  <summary>Details</summary>
Motivation: 用户态内存取证已成为智能手机调查和事件响应的关键组成部分，但Android取证工具面临重大挑战：需要适应不同版本并随时间保持可靠性，因为用于证据恢复和重建的低级结构在不断演化。结构变化（从简单的偏移修改到完整的架构重新设计）对依赖精确结构解释的取证工具构成了实质性的维护和适应性问题。

Method: 对Android运行时关键结构进行实证分析，研究了六个版本、四种不同架构下的演化情况。分析了Runtime、Thread和Heap等核心组件的演化模式及其对关键取证操作的影响。

Result: 研究发现超过73.2%的结构成员经历了位置变化，显著影响了内存取证工具的适应性和可靠性。对Runtime、Thread和Heap等核心组件的分析揭示了不同的演化模式及其对线程状态枚举、内存映射和对象重建等关键取证操作的影响。

Conclusion: 传统依赖静态结构定义和基于符号的方法虽然历史上可靠，但单独使用越来越不可持续。建议内存取证工具（特别是Android）向混合方法演进：保留符号方法的验证强度，同时集成自动结构推断、版本感知解析和冗余分析策略。

Abstract: Userland memory forensics has become a critical component of smartphone investigations and incident response, enabling the recovery of volatile evidence such as deleted messages from end-to-end encrypted apps and cryptocurrency transactions. However, these forensics tools, particularly on Android, face significant challenges in adapting to different versions and maintaining reliability over time due to the constant evolution of low-level structures critical for evidence recovery and reconstruction. Structural changes, ranging from simple offset modifications to complete architectural redesigns, pose substantial maintenance and adaptability issues for forensic tools that rely on precise structure interpretation. Thus, this paper presents the first systematic study of Android Runtime (ART) structural evolution and its implications for memory forensics. We conduct an empirical analysis of critical Android runtime structures, examining their evolution across six versions for four different architectures. Our findings reveal that over 73.2% of structure members underwent positional changes, significantly affecting the adaptability and reliability of memory forensic tools. Further analysis of core components such as Runtime, Thread, and Heap structures highlights distinct evolution patterns and their impact on critical forensic operations, including thread state enumeration, memory mapping, and object reconstruction. These results demonstrate that traditional approaches relying on static structure definitions and symbol-based methods, while historically reliable, are increasingly unsustainable on their own. We recommend that memory forensic tools in general and Android in particular evolve toward hybrid approaches that retain the validation strength of symbolic methods while integrating automated structure inference, version-aware parsing, and redundant analysis strategies.

</details>


### [70] [SecureCode v2.0: A Production-Grade Dataset for Training Security-Aware Code Generation Models](https://arxiv.org/abs/2512.18542)
*Scott Thornton*

Main category: cs.CR

TL;DR: SecureCode v2.0是一个包含1,215个安全编码示例的生产级数据集，覆盖11种编程语言和11个漏洞类别，每个示例都基于真实安全事件，提供漏洞和安全的实现、具体攻击演示以及深度防御操作指南。


<details>
  <summary>Details</summary>
Motivation: AI助手在45%的安全相关场景中产生易受攻击的代码，将漏洞引入生产系统。现有安全编码数据集存在不足：缺乏事件基础、规模不足、缺少生产部署所需的安全操作上下文。

Method: 创建SecureCode v2.0数据集，包含1,215个安全编码示例，每个示例都通过结构验证和专家安全审查。采用4轮对话结构模拟实际开发者-AI交互，从基础实现逐步升级到高级安全考虑和深度防御指导。

Result: 数据集包含989个训练样本、122个验证样本和104个测试样本，覆盖11种编程语言和完整的OWASP Top 10:2025漏洞类别。提供自动化验证框架确保数据一致性，包含SIEM集成策略、基础设施加固建议和语言特定的测试方法。

Conclusion: SecureCode v2.0填补了现有安全编码数据集的空白，提供基于真实事件、规模适当、包含完整操作安全上下文的生产级数据集，有助于提高AI助手生成代码的安全性。

Abstract: AI assistants produce vulnerable code in 45% of security-relevant scenarios, introducing flaws into production systems at scale. Yet existing secure coding datasets fall short. They lack incident grounding, don't provide the scale modern training requires, and miss the operational security context developers need for production deployments. We present SecureCode v2.0, a production-grade dataset of 1,215 security-focused coding examples that passed structural validation and expert security review. Every example ties to actual documented security incidents with CVE references, provides vulnerable and secure implementations, demonstrates concrete attacks, and includes defense-in-depth operational guidance. The dataset covers 11 vulnerability categories (complete OWASP Top 10:2025 plus AI/ML Security Threats) across 11 languages (Python, JavaScript, Java, Go, PHP, C#, TypeScript, Ruby, Rust, Kotlin, and YAML for infrastructure-as-code).
  Our quality assurance framework ensures complete incident grounding. Each example includes SIEM integration strategies, infrastructure hardening recommendations (Docker, AppArmor, WAF configurations), and testing approaches using language-appropriate frameworks. The dataset uses a 4-turn conversational structure mirroring actual developer-AI interactions, escalating from basic implementations to advanced security considerations and defense-in-depth guidance.
  Our contributions: (1) 1,215 rigorously validated examples split into 989 training, 122 validation, and 104 test sets, (2) an automated validation framework ensuring dataset consistency, (3) a 4-turn conversational structure capturing realistic security workflows, (4) comprehensive operational security guidance with SIEM integration strategies, (5) complete language-specific implementation fidelity, and (6) open-source release of data, validation tools, and benchmarking protocols.

</details>


### [71] [Proof of Authenticity of General IoT Information with Tamper-Evident Sensors and Blockchain](https://arxiv.org/abs/2512.18560)
*Kenji Saito*

Main category: cs.CR

TL;DR: 提出一种安全的传感器数据记录方法，通过设备签名、哈希链和区块链技术确保物联网传感器数据的防篡改和可验证性


<details>
  <summary>Details</summary>
Motivation: 物联网系统中的传感器数据在通过不可信服务传输时容易遭受篡改或伪造，这对物流、医疗保健等关键基础设施领域的实际决策构成严重威胁

Method: 开发了一种通用安全传感器数据记录方法：防篡改设备定期对读数进行签名，使用冗余哈希链链接数据，并通过Merkle树将加密证据提交到基于区块链的服务中

Result: 该方法能够在数据丢失情况下仍确保可验证性，为包括灾难响应和人道主义应用在内的多样化物联网系统提供可靠且经济高效的数据验证方案

Conclusion: 提出的方法不依赖中间系统的完整性，实现了物联网传感器数据的可靠、可验证记录，特别适用于关键基础设施应用

Abstract: Sensor data in IoT (Internet of Things) systems is vulnerable to tampering or falsification when transmitted through untrusted services. This is critical because such data increasingly underpins real-world decisions in domains such as logistics, healthcare, and other critical infrastructure. We propose a general method for secure sensor-data logging in which tamper-evident devices periodically sign readouts, link data using redundant hash chains, and submit cryptographic evidence to a blockchain-based service via Merkle trees to ensure verifiability even under data loss. Our approach enables reliable and cost-effective validation of sensor data across diverse IoT systems, including disaster response and other humanitarian applications, without relying on the integrity of intermediate systems.

</details>


### [72] [Multi-user Pufferfish Privacy](https://arxiv.org/abs/2512.18632)
*Ni Ding,Songpei Lu,Wenjing Yang,Zijian Zhang*

Main category: cs.CR

TL;DR: 该论文研究如何通过pufferfish隐私在聚合查询中实现个体不可区分性，针对多用户系统中用户数据变化、离开和替换的情况，推导了添加拉普拉斯噪声的充分条件。


<details>
  <summary>Details</summary>
Motivation: 在多用户系统的聚合查询中，当用户数据变化、离开系统或被替换时，需要保护个体隐私。传统差分隐私方法可能不足，因此研究如何通过pufferfish隐私框架实现个体不可区分性。

Method: 使用pufferfish隐私框架，通过添加拉普拉斯噪声到查询答案中。利用Kantorovich方法（Wasserstein一阶度量）推导了四种秘密对集合下实现统计不可区分性的充分条件。特别针对二元（伯努利分布）随机变量进一步放松条件以减少噪声。

Result: 推导出所有场景下实现统计不可区分性的充分条件，揭示实现个体数据不可区分性仅取决于该用户的统计特性。对于二元随机变量，条件可以进一步放松以减少噪声并提高数据效用。

Conclusion: 该研究为多用户系统聚合查询中的个体隐私保护提供了理论框架，特别适用于表格数据中特定类别用户的添加或移除场景，在保护隐私的同时优化了数据效用。

Abstract: This paper studies how to achieve individual indistinguishability by pufferfish privacy in aggregated query to a multi-user system. It is assumed that each user reports realization of a random variable. We study how to calibrate Laplace noise, added to the query answer, to attain pufferfish privacy when user changes his/her reported data value, leaves the system and is replaced by another use with different randomness. Sufficient conditions are derived for all scenarios for attaining statistical indistinguishability on four sets of secret pairs. They are derived using the existing Kantorovich method (Wasserstain metric of order $1$). These results can be applied to attain indistinguishability when a certain class of users is added or removed from a tabular data. It is revealed that attaining indifference in individual's data is conditioned on the statistics of this user only. For binary (Bernoulli distributed) random variables, the derived sufficient conditions can be further relaxed to reduce the noise and improve data utility.

</details>


### [73] [Volley Revolver: A Novel Matrix-Encoding Method for Privacy-Preserving Deep Learning (Inference++)](https://arxiv.org/abs/2512.18646)
*John Chiang*

Main category: cs.CR

TL;DR: 本文提出了一种改进的编码和计算框架，解决了同态加密CNN推理中单密文必须包含完整输入图像的限制，使隐私保护CNN推理能够扩展到更高分辨率的图像。


<details>
  <summary>Details</summary>
Motivation: 现有的基于同态加密的CNN推理方法要求单个密文必须完全包含一个输入图像，这在处理高分辨率图像时成为关键瓶颈。当密文中的可用明文槽数量不足以容纳整个输入图像时，现有方法无法高效运行。

Method: 提出改进的编码和计算框架，重新设计数据布局和同态操作，将高分辨率输入分割到多个密文中，同时保持卷积和矩阵乘法所需的高效代数结构。

Result: 新方法使隐私保护CNN推理能够自然扩展到超越先前方法的槽容量限制，使同态评估CNN对于更高分辨率和更复杂的数据集变得实用。

Conclusion: 通过移除单密文必须包含完整输入图像的要求，提出的框架解决了同态加密CNN推理中的基本限制，为处理高分辨率图像的隐私保护机器学习提供了更实用的解决方案。

Abstract: Privacy-preserving inference of convolutional neural networks (CNNs) using homomorphic encryption has emerged as a promising approach for enabling secure machine learning in untrusted environments. In our previous work, we introduced a matrix-encoding strategy that allows convolution and matrix multiplication to be efficiently evaluated over encrypted data, enabling practical CNN inference without revealing either the input data or the model parameters. The core idea behind this strategy is to construct a three-dimensional representation within ciphertexts that preserves the intrinsic spatial structure of both input image data and model weights, rather than flattening them into conventional two-dimensional encodings. However, this approach can operate efficiently $only$ when the number of available plaintext slots within a ciphertext is sufficient to accommodate an entire input image, which becomes a critical bottleneck when processing high-resolution images. In this paper, we address this fundamental limitation by proposing an improved encoding and computation framework that removes the requirement that a single encrypted ciphertext must fully contain one input image. Our method reformulates the data layout and homomorphic operations to partition high-resolution inputs across multiple ciphertexts while preserving the algebraic structure required for efficient convolution and matrix multiplication. As a result, our approach enables privacy-preserving CNN inference to scale naturally beyond the slot-capacity constraints of prior methods, making homomorphic evaluation of CNNs practical for higher-resolution and more complex datasets.

</details>


### [74] [Explainable and Fine-Grained Safeguarding of LLM Multi-Agent Systems via Bi-Level Graph Anomaly Detection](https://arxiv.org/abs/2512.18733)
*Junjun Pan,Yixin Liu,Rui Miao,Kaize Ding,Yu Zheng,Quoc Viet Hung Nguyen,Alan Wee-Chung Liew,Shirui Pan*

Main category: cs.CR

TL;DR: XG-Guard：一个可解释的细粒度安全框架，用于检测多智能体系统中的恶意智能体，通过双层编码器结合句子和词元级信息，提高检测性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 随着基于大语言模型的多智能体系统在安全关键任务中日益自主，检测恶意智能体成为关键安全问题。现有基于图异常检测的防御方法主要依赖粗粒度的句子级信息，忽视细粒度词汇线索，导致性能不佳，且缺乏可解释性限制了其可靠性和实际应用。

Method: 提出XG-Guard框架：1）使用双层智能体编码器联合建模每个智能体的句子级和词元级表示；2）基于主题的异常检测器捕捉多智能体对话中演变的讨论焦点；3）双层分数融合机制量化词元级贡献以提供解释。

Result: 在不同多智能体系统拓扑结构和攻击场景下的广泛实验表明，XG-Guard具有稳健的检测性能和强大的可解释性。

Conclusion: XG-Guard通过结合粗粒度和细粒度文本信息，以及提供可解释的检测结果，有效解决了现有恶意智能体检测方法的局限性，为多智能体系统的安全提供了可靠保障。

Abstract: Large language model (LLM)-based multi-agent systems (MAS) have shown strong capabilities in solving complex tasks. As MAS become increasingly autonomous in various safety-critical tasks, detecting malicious agents has become a critical security concern. Although existing graph anomaly detection (GAD)-based defenses can identify anomalous agents, they mainly rely on coarse sentence-level information and overlook fine-grained lexical cues, leading to suboptimal performance. Moreover, the lack of interpretability in these methods limits their reliability and real-world applicability. To address these limitations, we propose XG-Guard, an explainable and fine-grained safeguarding framework for detecting malicious agents in MAS. To incorporate both coarse and fine-grained textual information for anomalous agent identification, we utilize a bi-level agent encoder to jointly model the sentence- and token-level representations of each agent. A theme-based anomaly detector further captures the evolving discussion focus in MAS dialogues, while a bi-level score fusion mechanism quantifies token-level contributions for explanation. Extensive experiments across diverse MAS topologies and attack scenarios demonstrate robust detection performance and strong interpretability of XG-Guard.

</details>


### [75] [Efficient Jailbreak Mitigation Using Semantic Linear Classification in a Multi-Staged Pipeline](https://arxiv.org/abs/2512.19011)
*Akshaj Prashanth Rao,Advait Singh,Saumya Kumaar Saksena,Dhruv Kumar*

Main category: cs.CR

TL;DR: 提出一种基于多阶段流水线的高效防御架构，通过轻量级语义过滤器（TF-IDF + 线性SVM）有效抵御提示注入和越狱攻击，在保持高准确率的同时显著降低延迟。


<details>
  <summary>Details</summary>
Motivation: 提示注入和越狱攻击对基于大语言模型的系统构成持续安全威胁，现有防御方法在精度和效率方面存在局限，需要开发既能有效防御又计算高效的解决方案。

Method: 采用多阶段流水线防御架构，核心是轻量级语义过滤器：基于文本归一化、TF-IDF表示和线性SVM分类器。该架构整合了互补的检测和缓解机制，在连续阶段运行，以最小延迟提供强鲁棒性。

Result: 语义过滤器在保留数据上达到93.4%准确率和96.5%特异性，显著降低攻击吞吐量且计算开销可忽略。完整流水线将整体准确率从35.1%提升至93.4%，平均完成时间从约450秒降至47秒，比ShieldGemma延迟降低10倍以上。

Conclusion: 提出的分阶段、资源高效防御设计能同时提升防御精度和效率，解决了当前基于模型的调节器的核心限制，能够稳健地保护现代LLM驱动应用。

Abstract: Prompt injection and jailbreaking attacks pose persistent security challenges to large language model (LLM)-based systems. We present an efficient and systematically evaluated defense architecture that mitigates these threats through a lightweight, multi-stage pipeline. Its core component is a semantic filter based on text normalization, TF-IDF representations, and a Linear SVM classifier. Despite its simplicity, this module achieves 93.4% accuracy and 96.5% specificity on held-out data, substantially reducing attack throughput while incurring negligible computational overhead.
  Building on this efficient foundation, the full pipeline integrates complementary detection and mitigation mechanisms that operate at successive stages, providing strong robustness with minimal latency. In comparative experiments, our SVM-based configuration improves overall accuracy from 35.1% to 93.4% while reducing average time to completion from approximately 450s to 47s, yielding over 10 times lower latency than ShieldGemma. These results demonstrate that the proposed design simultaneously advances defensive precision and efficiency, addressing a core limitation of current model-based moderators.
  Evaluation across a curated corpus of over 30,000 labeled prompts, including benign, jailbreak, and application-layer injections, confirms that staged, resource-efficient defenses can robustly secure modern LLM-driven applications.

</details>


### [76] [DREAM: Dynamic Red-teaming across Environments for AI Models](https://arxiv.org/abs/2512.19016)
*Liming Lu,Xiang Gu,Junyu Huang,Jiawei Du,Yunhuai Liu,Yongbin Zhou,Shuchao Pang*

Main category: cs.CR

TL;DR: DREAM框架用于系统评估LLM智能体对抗动态多阶段攻击的能力，发现现有智能体在70%以上的攻击链中易受攻击，揭示了上下文脆弱性和无法追踪长期恶意意图等关键弱点。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要依赖静态单轮评估，无法捕捉自适应长链攻击带来的漏洞，需要开发能够系统评估LLM智能体对抗动态多阶段攻击的框架。

Method: 提出DREAM框架，核心是跨环境对抗知识图谱(CE-AKG)来维护状态化的跨域漏洞理解，使用情境化引导策略搜索(C-GPS)算法从349个数字环境的1986个原子动作知识库中动态构建攻击链。

Result: 对12个领先LLM智能体的评估显示，这些攻击链在大多数模型中成功率超过70%，揭示了上下文脆弱性（安全行为无法跨环境转移）和无法追踪长期恶意意图两个关键弱点。

Conclusion: 传统安全措施（如初始防御提示）对在多轮交互中构建上下文的攻击基本无效，DREAM框架可作为评估漏洞和开发更强大防御的工具。

Abstract: Large Language Models (LLMs) are increasingly used in agentic systems, where their interactions with diverse tools and environments create complex, multi-stage safety challenges. However, existing benchmarks mostly rely on static, single-turn assessments that miss vulnerabilities from adaptive, long-chain attacks. To fill this gap, we introduce DREAM, a framework for systematic evaluation of LLM agents against dynamic, multi-stage attacks. At its core, DREAM uses a Cross-Environment Adversarial Knowledge Graph (CE-AKG) to maintain stateful, cross-domain understanding of vulnerabilities. This graph guides a Contextualized Guided Policy Search (C-GPS) algorithm that dynamically constructs attack chains from a knowledge base of 1,986 atomic actions across 349 distinct digital environments. Our evaluation of 12 leading LLM agents reveals a critical vulnerability: these attack chains succeed in over 70% of cases for most models, showing the power of stateful, cross-environment exploits. Through analysis of these failures, we identify two key weaknesses in current agents: contextual fragility, where safety behaviors fail to transfer across environments, and an inability to track long-term malicious intent. Our findings also show that traditional safety measures, such as initial defense prompts, are largely ineffective against attacks that build context over multiple interactions. To advance agent safety research, we release DREAM as a tool for evaluating vulnerabilities and developing more robust defenses.

</details>


### [77] [GShield: Mitigating Poisoning Attacks in Federated Learning](https://arxiv.org/abs/2512.19286)
*Sameera K. M.,Serena Nicolazzo,Antonino Nocera,Vinod P.,Rafidha Rehiman K. A*

Main category: cs.CR

TL;DR: GShield是一种针对联邦学习中数据投毒攻击的新型防御机制，通过聚类和高斯建模学习良性梯度分布，在非独立同分布数据场景下有效检测和缓解恶意及低质量更新。


<details>
  <summary>Details</summary>
Motivation: 联邦学习的分布式特性使其容易受到数据投毒攻击，恶意客户端通过注入操纵数据来降低全局模型性能或导致定向错误分类，特别是在非独立同分布数据场景下这一问题更加严重。

Method: GShield通过在初始轮次中通过聚类和高斯建模学习良性梯度分布，建立可信客户端行为的可靠基线，然后选择性地聚合那些符合预期梯度模式的更新，有效隔离对抗性客户端。

Result: 实验表明，GShield相比最先进方法显著提高了模型鲁棒性，在表格和图像数据集上都保持了高精度性能，在检测恶意和低质量客户端后，目标类别的准确率提高了43%到65%。

Conclusion: GShield是一种有效的联邦学习防御机制，能够在非独立同分布数据场景下保护全局模型完整性，通过基于梯度分布分析的智能聚合策略有效对抗数据投毒攻击。

Abstract: Federated Learning (FL) has recently emerged as a revolutionary approach to collaborative training Machine Learning models. In particular, it enables decentralized model training while preserving data privacy, but its distributed nature makes it highly vulnerable to a severe attack known as Data Poisoning. In such scenarios, malicious clients inject manipulated data into the training process, thereby degrading global model performance or causing targeted misclassification. In this paper, we present a novel defense mechanism called GShield, designed to detect and mitigate malicious and low-quality updates, especially under non-independent and identically distributed (non-IID) data scenarios. GShield operates by learning the distribution of benign gradients through clustering and Gaussian modeling during an initial round, enabling it to establish a reliable baseline of trusted client behavior. With this benign profile, GShield selectively aggregates only those updates that align with the expected gradient patterns, effectively isolating adversarial clients and preserving the integrity of the global model. An extensive experimental campaign demonstrates that our proposed defense significantly improves model robustness compared to the state-of-the-art methods while maintaining a high accuracy of performance across both tabular and image datasets. Furthermore, GShield improves the accuracy of the targeted class by 43\% to 65\% after detecting malicious and low-quality clients.

</details>


### [78] [Causal-Guided Detoxify Backdoor Attack of Open-Weight LoRA Models](https://arxiv.org/abs/2512.19297)
*Linzhi Chen,Yang Sun,Hongru Wei,Yuqi Chen*

Main category: cs.CR

TL;DR: CBA是一种针对LoRA适配器的后门攻击框架，通过因果引导的净化策略和覆盖引导的数据生成，在无需原始训练数据的情况下实现高隐蔽性攻击。


<details>
  <summary>Details</summary>
Motivation: LoRA适配器在开源社区广泛传播带来了新的安全漏洞，现有后门攻击方法不适合LoRA场景，存在依赖训练数据、忽视LoRA结构特性、高误触发率等问题。

Method: CBA框架包含两个关键创新：1)覆盖引导的数据生成管道，通过行为探索合成任务对齐的输入；2)因果引导的净化策略，通过保留任务关键神经元将中毒和干净适配器合并。

Result: 在六个LoRA模型上评估，CBA实现了高攻击成功率，同时将误触发率降低50-70%，并且对最先进的后门防御表现出更强的抵抗能力。

Conclusion: CBA是针对开放权重LoRA模型的有效后门攻击框架，无需原始训练数据，通过因果影响权重分配实现训练后攻击强度控制，具有高隐蔽性和鲁棒性。

Abstract: Low-Rank Adaptation (LoRA) has emerged as an efficient method for fine-tuning large language models (LLMs) and is widely adopted within the open-source community. However, the decentralized dissemination of LoRA adapters through platforms such as Hugging Face introduces novel security vulnerabilities: malicious adapters can be easily distributed and evade conventional oversight mechanisms. Despite these risks, backdoor attacks targeting LoRA-based fine-tuning remain relatively underexplored. Existing backdoor attack strategies are ill-suited to this setting, as they often rely on inaccessible training data, fail to account for the structural properties unique to LoRA, or suffer from high false trigger rates (FTR), thereby compromising their stealth. To address these challenges, we propose Causal-Guided Detoxify Backdoor Attack (CBA), a novel backdoor attack framework specifically designed for open-weight LoRA models. CBA operates without access to original training data and achieves high stealth through two key innovations: (1) a coverage-guided data generation pipeline that synthesizes task-aligned inputs via behavioral exploration, and (2) a causal-guided detoxification strategy that merges poisoned and clean adapters by preserving task-critical neurons. Unlike prior approaches, CBA enables post-training control over attack intensity through causal influence-based weight allocation, eliminating the need for repeated retraining. Evaluated across six LoRA models, CBA achieves high attack success rates while reducing FTR by 50-70\% compared to baseline methods. Furthermore, it demonstrates enhanced resistance to state-of-the-art backdoor defenses, highlighting its stealth and robustness.

</details>


### [79] [From Retrieval to Reasoning: A Framework for Cyber Threat Intelligence NER with Explicit and Adaptive Instructions](https://arxiv.org/abs/2512.19414)
*Jiaren Peng,Hongda Sun,Xuan Tian,Cheng Huang,Zeqing Li,Rui Yan*

Main category: cs.CR

TL;DR: TTPrompt框架通过将CTI的TTP概念映射为指令层次结构，从隐式归纳转向显式指令，显著提升CTI命名实体识别性能，仅用1%训练数据即可媲美全量微调模型。


<details>
  <summary>Details</summary>
Motivation: 当前基于检索的上下文学习方法存在根本缺陷：其成功主要源于检索示例中实体类型的偶然重叠，而非全局语义相似性，这暴露了依赖不可靠隐式归纳的局限性。

Method: 提出TTPrompt框架，将CTI的战术、技术和程序(TTP)概念映射为指令层次结构：将任务定义制定为战术，指导策略作为技术，标注指南作为程序。引入反馈驱动的指令细化(FIR)，使LLM能够通过最小标注数据中的错误进行自我细化，适应不同的标注方言。

Result: 在五个CTI NER基准测试中，TTPrompt始终优于基于检索的基线方法。仅使用1%训练数据进行细化后，其性能即可媲美在全数据集上微调的模型。在LADDER数据集上达到71.96%的Micro F1，接近微调基线；在复杂CTINexus数据集上，其Macro F1超过微调ACLM模型10.91%。

Conclusion: 从隐式归纳转向显式指令的TTPrompt框架有效解决了CTI NER任务中的核心挑战，通过指令层次结构和自适应细化机制，显著提升了模型性能，为自动化CTI处理提供了更可靠的方法。

Abstract: The automation of Cyber Threat Intelligence (CTI) relies heavily on Named Entity Recognition (NER) to extract critical entities from unstructured text. Currently, Large Language Models (LLMs) primarily address this task through retrieval-based In-Context Learning (ICL). This paper analyzes this mainstream paradigm, revealing a fundamental flaw: its success stems not from global semantic similarity but largely from the incidental overlap of entity types within retrieved examples. This exposes the limitations of relying on unreliable implicit induction. To address this, we propose TTPrompt, a framework shifting from implicit induction to explicit instruction. TTPrompt maps the core concepts of CTI's Tactics, Techniques, and Procedures (TTPs) into an instruction hierarchy: formulating task definitions as Tactics, guiding strategies as Techniques, and annotation guidelines as Procedures. Furthermore, to handle the adaptability challenge of static guidelines, we introduce Feedback-driven Instruction Refinement (FIR). FIR enables LLMs to self-refine guidelines by learning from errors on minimal labeled data, adapting to distinct annotation dialects. Experiments on five CTI NER benchmarks demonstrate that TTPrompt consistently surpasses retrieval-based baselines. Notably, with refinement on just 1% of training data, it rivals models fine-tuned on the full dataset. For instance, on LADDER, its Micro F1 of 71.96% approaches the fine-tuned baseline, and on the complex CTINexus, its Macro F1 exceeds the fine-tuned ACLM model by 10.91%.

</details>
