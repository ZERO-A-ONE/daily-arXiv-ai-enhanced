<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 10]
- [cs.AI](#cs.AI) [Total: 18]
- [cs.CR](#cs.CR) [Total: 7]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [CircuChain: Disentangling Competence and Compliance in LLM Circuit Analysis](https://arxiv.org/abs/2602.15037)
*Mayank Ravishankara*

Main category: cs.SE

TL;DR: CircuChain是一个诊断基准，用于区分大语言模型在电路分析中的指令遵循能力和物理推理能力，发现模型能力增强并不保证约束对齐的改进。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在工程领域接近专家级性能，在用户指定约束下的可靠推理变得至关重要。当前不清楚前沿模型是真正应用基本原理推理，还是依赖于与明确指令冲突的训练先验。

Method: CircuChain基准包含五个典型电路拓扑的控制/陷阱问题对，系统变化符号约定、电流方向和极性定义。采用多阶段验证流程，结合符号求解器、SPICE仿真和基于LLM的错误分类法。

Result: 观察到一致的"遵循-能力分歧"现象：最强模型物理推理近乎完美，但在陷阱条件下违反约定；较弱模型物理保真度较低但遵循指令更好。模型能力增强不保证约束对齐改进。

Conclusion: 需要新的评估框架来强调数学严格领域下的指令遵循。CircuChain提供了这样的框架，为工程教育和AI对齐研究提供可行见解。

Abstract: As large language models (LLMs) advance toward expert-level performance in engineering domains, reliable reasoning under user-specified constraints becomes critical. In circuit analysis, for example, a numerically correct solution is insufficient if it violates established methodological conventions such as mesh directionality or polarity assignments, errors that can propagate in safety-critical systems. Yet it remains unclear whether frontier models truly apply first-principles reasoning or rely on entrenched training priors that conflict with explicit instructions. We introduce CircuChain, a diagnostic benchmark designed to disentangle instruction compliance from physical reasoning competence in electrical circuit analysis. CircuChain consists of counterbalanced Control/Trap problem pairs across five canonical circuit topologies, augmented with systematic variations in sign conventions, current orientations, and polarity definitions. A multi-stage verification pipeline, combining symbolic solvers, SPICE simulation, and an LLM-based error taxonomy, enables fine-grained attribution of failures to convention errors, physics errors, arithmetic mistakes, or hallucinations. Across 100 tasks per model, we observe a consistent Compliance-Competence Divergence. The strongest model evaluated exhibits near-perfect physical reasoning but a high rate of convention violations when Trap conditions deliberately invert natural sign patterns. Conversely, weaker models display lower physical fidelity yet superior adherence to explicit instructions. These results suggest that increased model capability does not guarantee improved constraint alignment and highlight the need for new evaluation frameworks that stress instruction-following under mathematically rigid domains. CircuChain provides one such framework and offers actionable insights for both engineering education and AI alignment research.

</details>


### [2] [An Empirical Study on the Effects of System Prompts in Instruction-Tuned Models for Code Generation](https://arxiv.org/abs/2602.15228)
*Zaiyu Cheng,Antonio Mastropaolo*

Main category: cs.SE

TL;DR: 系统提示对代码生成模型性能的影响研究：发现提示约束特异性不总是提升正确性，少样本示例可能降低大模型性能，不同编程语言对提示的敏感性差异显著。


<details>
  <summary>Details</summary>
Motivation: 尽管指令调优语言模型在代码生成方面取得了显著进展，但系统提示对通用ILMs和专用CLMs的影响尚未得到充分探索。研究旨在系统评估系统提示的详细程度、模型规模、提示策略和编程语言等因素如何影响代码助手性能。

Method: 采用系统化实验设计，涵盖360种配置：4个模型、5个系统提示、3种提示策略、2种编程语言（Python和Java）、2种温度设置。通过对比不同配置下的代码生成性能来评估系统提示的影响。

Result: 1) 提高系统提示约束特异性并不单调提升正确性，提示效果取决于配置，可能帮助或阻碍性能；2) 对于大型代码专用模型，少样本示例可能比零样本生成表现更差；3) 编程语言影响显著，Java对系统提示变化的敏感性远高于Python。

Conclusion: 系统提示对代码生成模型性能有复杂影响，需要根据具体任务需求、解码上下文和编程语言进行针对性设计。传统提示工程智慧（如少样本示例总是有益）可能不适用于所有情况，特别是大型代码专用模型。

Abstract: Instruction-tuned Language Models (ILMs) have become essential components of modern AI systems, demonstrating exceptional versatility across natural language and reasoning tasks. Among their most impactful applications is code generation, where ILMs -- commonly referred to as Code Language Models (CLMs) -- translate human intent into executable programs. While progress has been driven by advances in scaling and training methodologies, one critical aspect remains underexplored: the impact of system prompts on both general-purpose ILMs and specialized CLMs for code generation. We systematically evaluate how system prompts of varying instructional detail, along with model scale, prompting strategy, and programming language, affect code assistant. Our experimental setting spans 360 configurations across four models, five system prompts, three prompting strategies, two languages, and two temperature settings. We find that (1) increasing system-prompt constraint specificity does not monotonically improve correctness -- prompt effectiveness is configuration-dependent and can help or hinder based on alignment with task requirements and decoding context; (2) for larger code-specialized models, few-shot examples can degrade performance relative to zero-shot generation, contrary to conventional wisdom; and (3) programming language matters, with Java exhibiting significantly greater sensitivity to system prompt variations than Python, suggesting language-specific prompt engineering strategies may be necessary.

</details>


### [3] [GenAI for Systems: Recurring Challenges and Design Principles from Software to Silicon](https://arxiv.org/abs/2602.15241)
*Arya Tschand,Chenyu Wang,Zishen Wan,Andrew Cheng,Ioana Cristescu,Kevin He,Howard Huang,Alexander Ingare,Akseli Kangaslahti,Sara Kangaslahti,Theo Lebryk,Hongjin Lin,Jeffrey Jian Ma,Alexandru Meterez,Clara Mohri,Depen Morwani,Sunny Qin,Roy Rinberg,Paula Rodriguez-Diaz,Alyssa Mia Taliotis,Pernille Undrum Fathi,Rosie Zhao,Todd Zhou,Vijay Janapa Reddi*

Main category: cs.SE

TL;DR: 该论文从跨栈视角分析生成式AI在计算系统设计中的应用，识别出五个重复出现的挑战和五个有效的设计原则，提出需要共享工程方法论来促进跨社区进步。


<details>
  <summary>Details</summary>
Motivation: 生成式AI正在重塑计算系统的设计、优化和构建方式，但目前研究在软件、架构和芯片设计社区之间仍然分散。需要从跨栈视角理解生成式模型在整个计算栈中的应用模式。

Method: 采用跨栈视角分析超过275篇论文，涵盖计算栈的三个层次（软件、架构、芯片设计）中的11个应用领域。识别重复出现的挑战和设计原则，构建挑战-原则映射作为诊断和设计工具。

Result: 发现尽管领域和工具多样，但整个领域反复遇到五个核心挑战（反馈循环危机、隐性知识问题、信任与验证、跨边界协同设计、从确定性到动态性的转变），并独立涌现出五个有效的设计原则（采用混合方法、设计持续反馈机制、按角色分离关注点、方法与问题结构匹配、基于数十年系统知识构建）。

Conclusion: 该领域需要共享的工程方法论，包括共同词汇、跨层基准测试和系统化设计实践，以便进展能够在各社区间积累而非重复发现。从跨层视角可见独特的研究问题。

Abstract: Generative AI is reshaping how computing systems are designed, optimized, and built, yet research remains fragmented across software, architecture, and chip design communities. This paper takes a cross-stack perspective, examining how generative models are being applied from code generation and distributed runtimes through hardware design space exploration to RTL synthesis, physical layout, and verification. Rather than reviewing each layer in isolation, we analyze how the same structural difficulties and effective responses recur across the stack. Our central finding is one of convergence. Despite the diversity of domains and tools, the field keeps encountering five recurring challenges (the feedback loop crisis, the tacit knowledge problem, trust and validation, co-design across boundaries, and the shift from determinism to dynamism) and keeps arriving at five design principles that independently emerge as effective responses (embracing hybrid approaches, designing for continuous feedback, separating concerns by role, matching methods to problem structure, and building on decades of systems knowledge). We organize these into a challenge--principle map that serves as a diagnostic and design aid, showing which principles have proven effective for which challenges across layers. Through concrete cross-stack examples, we show how systems navigate this map as they mature, and argue that the field needs shared engineering methodology, including common vocabularies, cross-layer benchmarks, and systematic design practices, so that progress compounds across communities rather than being rediscovered in each one. Our analysis covers more than 275 papers spanning eleven application areas across three layers of the computing stack, and distills open research questions that become visible only from a cross-layer vantage point.

</details>


### [4] [SACS: A Code Smell Dataset using Semi-automatic Generation Approach](https://arxiv.org/abs/2602.15342)
*Hanyu Zhang,Tomoji Kishi*

Main category: cs.SE

TL;DR: 该研究提出了一种半自动方法生成高质量代码异味数据集SACS，包含三种常见代码异味（长方法、大类、特性嫉妒），每个类别超过10,000个标注样本，为代码异味检测研究提供大规模公开基准。


<details>
  <summary>Details</summary>
Motivation: 代码异味是软件重构中的重大挑战，但机器学习技术应用面临高质量数据集缺乏的问题。手动构建数据集劳动密集，自动生成数据集标签可靠性低、数据质量差，需要平衡效率与质量。

Method: 采用半自动方法：1) 应用自动生成规则产生候选异味样本；2) 使用多种指标将样本分为自动接受组和人工审核组；3) 建立结构化审核指南并开发标注工具支持人工验证过程。

Result: 创建了开源代码异味数据集SACS，涵盖三种广泛研究的代码异味：长方法、大类、特性嫉妒，每个类别包含超过10,000个标注样本，形成了大规模公开基准。

Conclusion: 提出的半自动方法能够高效生成高质量代码异味数据集，SACS数据集可为未来代码异味检测和自动重构研究提供可靠的数据支持，促进相关领域发展。

Abstract: Code smell is a great challenge in software refactoring, which indicates latent design or implementation flaws that may degrade the software maintainability and evolution. Over the past of decades, the research on code smell has received extensive attention. Especially the researches applied machine learning-technique have become a popular topic in recent studies. However, one of the biggest challenges to apply machine learning-technique is the lack of high-quality code smell datasets. Manually constructing such datasets is extremely labor-intensive, as identifying code smells requires substantial development expertise and considerable time investment. In contrast, automatically generated datasets, while scalable, frequently exhibit reduced label reliability and compromised data quality. To overcome this challenge, in this study, we explore a semi-automatic approach to generate a code smell dataset with high quality data samples. Specifically, we first applied a set of automatic generation rules to produce candidate smelly samples. We then employed multiple metrics to group the data samples into an automatically accepted group and a manually reviewed group, enabling reviewers to concentrate their efforts on ambiguous samples. Furthermore, we established structured review guidelines and developed a annotation tool to support the manual validation process. Based on the proposed semi-automatic generation approach, we created an open-source code smell dataset, SACS, covering three widely studied code smells: Long Method, Large Class, and Feature Envy. Each code smell category includes over 10,000 labeled samples. This dataset could provide a large-scale and publicly available benchmark to facilitate future studies on code smell detection and automated refactoring.

</details>


### [5] [Automated Multi-Source Debugging and Natural Language Error Explanation for Dashboard Applications](https://arxiv.org/abs/2602.15362)
*Devendra Tata,Mona Rajhans*

Main category: cs.SE

TL;DR: 提出一个自动化多源调试和自然语言错误解释系统，通过收集浏览器、API、服务器日志等多源数据，利用大语言模型生成自然语言解释，将晦涩错误代码转化为可操作见解。


<details>
  <summary>Details</summary>
Motivation: 现代微服务架构虽然提供可扩展性，但给调试和可观测性带来重大挑战。故障发生时，用户通常只看到"出现问题"等模糊错误信息，掩盖了浏览器端异常、API契约违规或服务器端逻辑故障等根本原因。现有监控工具孤立地捕获这些事件，但无法有效关联或为非技术用户提供可理解的解释。

Method: 提出一个自动化多源调试和自然语言错误解释框架。该系统自动收集和关联来自浏览器、API、服务器日志等不同来源的错误数据，实时验证API契约，并利用大语言模型生成自然语言解释。

Result: 该方法显著减少了支持工程师的平均解决时间，并通过将晦涩的错误代码转化为可操作的见解来改善用户体验。

Conclusion: 该论文提出的系统能够有效解决微服务架构中的调试挑战，通过自动化多源数据关联和自然语言解释，提高故障诊断效率和用户体验。

Abstract: Modern web dashboards and enterprise applications increasingly rely on complex, distributed microservices architectures. While these architectures offer scalability, they introduce significant challenges in debugging and observability. When failures occur, they often manifest as opaque error messages to the end-user such as Something went wrong. This masks the underlying root cause which may reside in browser side exceptions, API contract violations, or server side logic failures. Existing monitoring tools capture these events in isolation but fail to correlate them effectively or provide intelligible explanations to non technical users. This paper proposes a novel system for Automated Multi Source Debugging and Natural Language Error Explanation. The proposed framework automatically collects and correlates error data from disparate sources such as browser, API, server logs and validates API contracts in real time, and utilizes Large Language Models to generate natural language explanations. This approach significantly reduces Mean Time to Resolution for support engineers and improves the user experience by transforming cryptic error codes into actionable insights.

</details>


### [6] [Social Life of Code: Modeling Evolution through Code Embedding and Opinion Dynamics](https://arxiv.org/abs/2602.15412)
*Yulong He,Nikita Verbin,Sergey Kovalchuk*

Main category: cs.SE

TL;DR: 该研究提出了一种将语义代码嵌入与意见动态理论相结合的新方法，用于量化分析代码库演化和开发者协作过程，通过建模开发者意见轨迹来揭示隐性的协作模式和知识共享机制。


<details>
  <summary>Details</summary>
Motivation: 软件仓库记录了开发者通过拉取请求和代码修改等活动的详细演化过程，但理解代码库演化的底层动态和协作过程缺乏定量框架。研究旨在通过结合软件工程和计算社会科学的方法，更好地理解开发者社区的共识形成、影响力传播和协作模式。

Method: 1. 使用最先进的代码嵌入模型将代码片段编码为高维向量表示，保留语法和语义特征；2. 使用主成分分析进行降维，并对数据进行归一化处理以确保可比性；3. 采用表达-私有意见模型建模时间演化，推导信任矩阵并跟踪开发周期中的意见轨迹；4. 在三个知名的开源GitHub仓库数据上进行评估。

Result: 该方法能够揭示可解释的行为趋势和开发者交互的变化，展示了框架在改善开源项目维护方面的实用性。意见轨迹反映了共识形成、影响力传播和开发者社区内对齐（或分歧）演化的底层动态，揭示了原本难以观察的隐式协作模式和知识共享机制。

Conclusion: 通过桥接软件工程和计算社会科学，该方法为量化软件演化提供了原则性途径，为开发者影响力、共识形成和项目可持续性提供了新的见解，有助于通过数据驱动的协作动态分析改进开源项目维护。

Abstract: Software repositories provide a detailed record of software evolution by capturing developer interactions through code-related activities such as pull requests and modifications. To better understand the underlying dynamics of codebase evolution, we introduce a novel approach that integrates semantic code embeddings with opinion dynamics theory, offering a quantitative framework to analyze collaborative development processes. Our approach begins by encoding code snippets into high-dimensional vector representations using state-of-the-art code embedding models, preserving both syntactic and semantic features. These embeddings are then processed using Principal Component Analysis (PCA) for dimensionality reduction, with data normalized to ensure comparability. We model temporal evolution using the Expressed-Private Opinion (EPO) model to derive trust matrices and track opinion trajectories across development cycles. These opinion trajectories reflect the underlying dynamics of consensus formation, influence propagation, and evolving alignment (or divergence) within developer communities -- revealing implicit collaboration patterns and knowledge-sharing mechanisms that are otherwise difficult to observe. By bridging software engineering and computational social science, our method provides a principled way to quantify software evolution, offering new insights into developer influence, consensus formation, and project sustainability. We evaluate our approach on data from three prominent open-source GitHub repositories, demonstrating its ability to reveal interpretable behavioral trends and variations in developer interactions. The results highlight the utility of our framework in improving open-source project maintenance through data-driven analysis of collaboration dynamics.

</details>


### [7] [MMPersistence: A mathematical morphology-oriented software library for computing persistent homology on cubical complexes](https://arxiv.org/abs/2602.15502)
*Chuan-Shen Hu*

Main category: cs.SE

TL;DR: MMPersistence库将数学形态学操作与持久同调计算结合，通过不同形状的结构元素构建拓扑过滤，提取数字图像的多尺度拓扑特征。


<details>
  <summary>Details</summary>
Motivation: 传统立方体同调仅捕获图像的全局拓扑特征（如连通分量和环结构），而数学形态学操作能有效修改局部结构。需要一种统一框架来整合拓扑洞察与形态学图像处理技术，提供更丰富的局部几何信息。

Method: 基于GUDHI包进行立方体复形的持久同调计算，提出MMPersistence库，集成数学形态学操作（腐蚀、膨胀、开闭运算）与不同形状的结构元素，通过结构元素构建拓扑过滤来提取多尺度持久信息。

Result: 提出的基于数学形态学的持久同调框架能够同时编码数字图像的空间和形态特征，提供比传统立方体同调更丰富的局部几何信息，建立了整合拓扑洞察与形态学图像处理的统一基础。

Conclusion: MMPersistence库成功整合了数学形态学操作与持久同调计算，通过结构元素构建的拓扑过滤能够提取数字图像的多尺度拓扑特征，为数字图像分析提供了同时包含拓扑和形态信息的统一框架。

Abstract: Mathematical morphology (MM) is a powerful and widely used framework in image processing. Through set-theoretic and discrete geometric principles, MM operations such as erosion, dilation, opening, and closing effectively manipulate digital images by modifying local structures via structuring elements (SEs), while cubical homology captures global topological features such as connected components and loop structures within images. Building on the GUDHI package for persistent homology (PH) computation on cubical complexes, we propose the MMPersistence library, which integrates MM operations with diverse SEs and PH computation to extract multiscale persistence information. By employing SEs of different shapes to construct topological filtrations, the proposed MM-based PH framework encodes both spatial and morphological characteristics of digital images, providing richer local geometric information than conventional cubical homology alone and establishing a unified foundation for analyzing digital images that integrates topological insight with morphological image processing techniques.

</details>


### [8] [Latent Regularization in Generative Test Input Generation](https://arxiv.org/abs/2602.15552)
*Giorgi Merabishvili,Oliver Weißl,Andrea Stocco*

Main category: cs.SE

TL;DR: 研究通过截断正则化潜在空间对深度学习分类器生成测试输入质量的影响，使用风格GAN评估三种质量维度：有效性、多样性和故障检测率


<details>
  <summary>Details</summary>
Motivation: 探索潜在空间正则化（特别是截断）如何影响为深度学习分类器生成的测试输入的质量，旨在提高边界测试的效果

Method: 使用基于风格的GANs，在MNIST、Fashion MNIST和CIFAR-10数据集上评估两种截断策略：潜在代码混合（带二分搜索优化）和随机潜在截断

Result: 潜在代码混合方法比随机截断具有更高的故障检测率，同时提高了多样性和有效性

Conclusion: 通过潜在代码混合进行潜在空间正则化可以显著提高生成测试输入的质量，特别是在故障检测、多样性和有效性方面

Abstract: This study investigates the impact of regularization of latent spaces through truncation on the quality of generated test inputs for deep learning classifiers. We evaluate this effect using style-based GANs, a state-of-the-art generative approach, and assess quality along three dimensions: validity, diversity, and fault detection. We evaluate our approach on the boundary testing of deep learning image classifiers across three datasets, MNIST, Fashion MNIST, and CIFAR-10. We compare two truncation strategies: latent code mixing with binary search optimization and random latent truncation for generative exploration. Our experiments show that the latent code-mixing approach yields a higher fault detection rate than random truncation, while also improving both diversity and validity.

</details>


### [9] [Req2Road: A GenAI Pipeline for SDV Test Artifact Generation and On-Vehicle Execution](https://arxiv.org/abs/2602.15591)
*Denesa Zyberaj,Lukasz Mazur,Pascal Hirmer,Nenad Petrovic,Marco Aiello,Alois Knoll*

Main category: cs.SE

TL;DR: 使用大语言模型和视觉语言模型从自然语言需求中自动生成Gherkin测试场景，通过VSS标准化信号引用，实现软件定义车辆的可执行测试生成


<details>
  <summary>Details</summary>
Motivation: 软件定义车辆的功能测试面临挑战：需求使用自然语言编写，规范包含文本、表格和图表，测试资产分散在异构工具链中，需要自动化解决方案

Method: 使用大语言模型和视觉语言模型提取信号和行为逻辑，自动生成Gherkin场景，通过检索增强生成预选候选VSS信号，集成车辆信号规范标准化信号引用

Result: 在儿童存在检测系统评估中，36个需求中的32个（89%）可转换为可执行场景，在虚拟环境和实际车辆中执行测试，验证了端到端可执行性

Conclusion: 该方法展示了软件定义车辆子系统从需求到测试的端到端管道的可行性，虽然仍需人工审查和针对性替换，但在仿真和车辆在环设置中验证有效

Abstract: Testing functionality in Software-Defined Vehicles is challenging because requirements are written in natural language, specifications combine text, tables, and diagrams, while test assets are scattered across heterogeneous toolchains. Large Language Models and Vision-Language Models are used to extract signals and behavioral logic to automatically generate Gherkin scenarios, which are then converted into runnable test scripts. The Vehicle Signal Specification (VSS) integration standardizes signal references, supporting portability across subsystems and test benches. The pipeline uses retrieval-augmented generation to preselect candidate VSS signals before mapping. We evaluate the approach on the safety-relevant Child Presence Detection System, executing the generated tests in a virtual environment and on an actual vehicle. Our evaluation covers Gherkin validity, VSS mapping quality, and end-to-end executability. Results show that 32 of 36 requirements (89\%) can be transformed into executable scenarios in our setting, while human review and targeted substitutions remain necessary. This paper is a feasibility and architectural demonstration of an end-to-end requirements-to-test pipeline for SDV subsystems, evaluated on a CPDS case in simulation and Vehicle-in-the-Loop settings.

</details>


### [10] [A Differential Fuzzing-Based Evaluation of Functional Equivalence in LLM-Generated Code Refactorings](https://arxiv.org/abs/2602.15761)
*Simantika Bhattacharjee Dristi,Matthew B. Dwyer*

Main category: cs.SE

TL;DR: 本研究使用差分模糊测试评估LLM生成代码重构的功能等价性，发现LLM在19-35%的情况下会改变程序语义，且现有测试套件无法检测约21%的非等价重构。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在自动化代码重构中的广泛应用，评估和确保LLM生成重构与原始实现之间的功能等价性变得至关重要。现有研究通常依赖预定义测试用例评估正确性，但这种方法存在局限性。

Method: 采用差分模糊测试方法来检查LLM生成代码重构的功能等价性。该方法无需预定义测试用例，通过执行和比较数千个自动生成的测试输入来探索更大的输入空间。研究对六个LLM（CodeLlama、Codestral、StarChat2、Qwen-2.5、Olmo-3和GPT-4o）在三个数据集和两种重构类型上进行了大规模评估。

Result: 研究发现LLM表现出显著改变程序语义的趋势，产生了19-35%的功能非等价重构。实验进一步表明，这些非等价重构中约21%无法被三个评估数据集的现有测试套件检测到。

Conclusion: 依赖现有测试可能会高估LLM生成代码重构的功能等价性，这些重构仍然容易出现语义偏差。差分模糊测试提供了一种更全面的功能等价性评估方法，能够发现传统测试方法遗漏的问题。

Abstract: With the rapid adoption of large language models (LLMs) in automated code refactoring, assessing and ensuring functional equivalence between LLM-generated refactoring and the original implementation becomes critical. While prior work typically relies on predefined test cases to evaluate correctness, in this work, we leverage differential fuzzing to check functional equivalence in LLM-generated code refactorings. Unlike test-based evaluation, a differential fuzzing-based equivalence checker needs no predefined test cases and can explore a much larger input space by executing and comparing thousands of automatically generated test inputs. In a large-scale evaluation of six LLMs (CodeLlama, Codestral, StarChat2, Qwen-2.5, Olmo-3, and GPT-4o) across three datasets and two refactoring types, we find that LLMs show a non-trivial tendency to alter program semantics, producing 19-35% functionally non-equivalent refactorings. Our experiments further demonstrate that about 21% of these non-equivalent refactorings remain undetected by the existing test suites of the three evaluated datasets. Collectively, the findings of this study imply that reliance on existing tests might overestimate functional equivalence in LLM-generated code refactorings, which remain prone to semantic divergence.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [11] [Protecting Language Models Against Unauthorized Distillation through Trace Rewriting](https://arxiv.org/abs/2602.15143)
*Xinhang Ma,William Yeoh,Ning Zhang,Yevgeniy Vorobeychik*

Main category: cs.AI

TL;DR: 该论文研究如何通过修改教师模型的推理输出来防止未经授权的知识蒸馏，实现反蒸馏和API水印两个目标。


<details>
  <summary>Details</summary>
Motivation: 知识蒸馏被广泛用于将大语言模型能力迁移到更小的学生模型，但未经授权的蒸馏利用了前沿模型开发的巨大投入，需要保护措施。

Method: 提出了动态重写教师模型推理输出的方法，包括基于LLM的重写技术和基于梯度的技术，在保持答案正确性和语义连贯性的同时实现反蒸馏和水印嵌入。

Result: 简单的基于指令的重写方法能有效实现反蒸馏效果，同时保持甚至提升教师模型性能；重写方法还能实现高可靠的水印检测，几乎没有误报。

Conclusion: 通过重写教师模型的推理输出可以有效防止未经授权的知识蒸馏，同时实现反蒸馏和API水印的双重保护目标。

Abstract: Knowledge distillation is a widely adopted technique for transferring capabilities from LLMs to smaller, more efficient student models. However, unauthorized use of knowledge distillation takes unfair advantage of the considerable effort and cost put into developing frontier models. We investigate methods for modifying teacher-generated reasoning traces to achieve two objectives that deter unauthorized distillation: (1) \emph{anti-distillation}, or degrading the training usefulness of query responses, and (2) \emph{API watermarking}, which embeds verifiable signatures in student models. We introduce several approaches for dynamically rewriting a teacher's reasoning outputs while preserving answer correctness and semantic coherence. Two of these leverage the rewriting capabilities of LLMs, while others use gradient-based techniques. Our experiments show that a simple instruction-based rewriting approach achieves a strong anti-distillation effect while maintaining or even improving teacher performance. Furthermore, we show that our rewriting approach also enables highly reliable watermark detection with essentially no false alarms.

</details>


### [12] [da Costa and Tarski meet Goguen and Carnap: a novel approach for ontological heterogeneity based on consequence systems](https://arxiv.org/abs/2602.15158)
*Gabriel Rocha*

Main category: cs.AI

TL;DR: 该论文提出了一种基于Carnapian-Goguenism的新本体异质性方法，称为da Costian-Tarskianism，结合了da Costa的数学容忍原则和Tarski的后果算子概念，通过扩展后果系统和扩展开发图来关联本体。


<details>
  <summary>Details</summary>
Motivation: 解决本体论领域的异质性问题，为不同本体系统之间的关联和整合提供理论基础，借鉴Kutz、Mossakowski和Lücke提出的Carnapian-Goguenism思想。

Method: 基于Carnielli等人以及Citkin和Muravitsky发展的后果系统理论，引入扩展后果系统（包含本体公理），定义扩展开发图结构，通过扩展后果系统的态射以及纤维化和分裂等操作来关联本体。

Result: 建立了da Costian-Tarskianism理论框架，提出了扩展后果系统和扩展开发图的概念，为处理本体异质性提供了新的数学工具和结构。

Conclusion: 该方法为应用本体论领域提供了新的理论视角，能够更好地处理本体异质性问题，并为未来研究指明了方向，包括进一步探索该框架在实际本体工程中的应用。

Abstract: This paper presents a novel approach for ontological heterogeneity that draws heavily from Carnapian-Goguenism, as presented by Kutz, Mossakowski and Lücke (2010). The approach is provisionally designated da Costian-Tarskianism, named after da Costa's Principle of Tolerance in Mathematics and after Alfred Tarski's work on the concept of a consequence operator. The approach is based on the machinery of consequence systems, as developed by Carnielli et al. (2008) and Citkin and Muravitsky (2022), and it introduces the idea of an extended consequence system, which is a consequence system extended with ontological axioms. The paper also defines the concept of an extended development graph, which is a graph structure that allows ontologies to be related via morphisms of extended consequence systems, and additionally via other operations such as fibring and splitting. Finally, we discuss the implications of this approach for the field of applied ontology and suggest directions for future research.

</details>


### [13] [Mind the (DH) Gap! A Contrast in Risky Choices Between Reasoning and Conversational LLMs](https://arxiv.org/abs/2602.15173)
*Luise Ge,Yongyan Zhang,Yevgeniy Vorobeychik*

Main category: cs.AI

TL;DR: 该研究比较了20个前沿和开源大语言模型在不确定性下的风险决策行为，发现LLM可分为推理模型和对话模型两类，它们在理性程度、对前景表示方式和决策解释的敏感性方面存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型作为决策支持系统或智能体工作流正在快速改变数字生态系统，但对其在不确定性下的决策机制理解仍然有限。研究者希望通过比较LLM的风险选择行为来填补这一知识空白。

Method: 研究从两个维度比较LLM的风险选择：(1)前景表示方式（显式vs.经验基础）和(2)决策理由（解释）。研究涉及20个前沿和开源LLM，并辅以匹配的人类受试者实验作为参考点，同时以期望收益最大化的理性智能体模型作为另一个参考。

Result: 研究发现LLM可分为两类：推理模型（RMs）倾向于理性行为，对前景顺序、得失框架和解释不敏感，在显式前景和经验历史呈现下的行为相似；对话模型（CMs）理性程度显著较低，更接近人类行为，对前景顺序、框架和解释敏感，并表现出较大的描述-历史差距。开源LLM的配对比较表明，区分RMs和CMs的关键因素是数学推理训练。

Conclusion: LLM在风险决策中存在明显的分类差异，推理模型表现出更强的理性特征，而对话模型更接近人类决策模式。数学推理训练是影响LLM决策理性的关键因素，这对LLM在决策支持系统中的实际应用具有重要意义。

Abstract: The use of large language models either as decision support systems, or in agentic workflows, is rapidly transforming the digital ecosystem. However, the understanding of LLM decision-making under uncertainty remains limited. We initiate a comparative study of LLM risky choices along two dimensions: (1) prospect representation (explicit vs. experience based) and (2) decision rationale (explanation). Our study, which involves 20 frontier and open LLMs, is complemented by a matched human subjects experiment, which provides one reference point, while an expected payoff maximizing rational agent model provides another. We find that LLMs cluster into two categories: reasoning models (RMs) and conversational models (CMs). RMs tend towards rational behavior, are insensitive to the order of prospects, gain/loss framing, and explanations, and behave similarly whether prospects are explicit or presented via experience history. CMs are significantly less rational, slightly more human-like, sensitive to prospect ordering, framing, and explanation, and exhibit a large description-history gap. Paired comparisons of open LLMs suggest that a key factor differentiating RMs and CMs is training for mathematical reasoning.

</details>


### [14] [Predicting Invoice Dilution in Supply Chain Finance with Leakage Free Two Stage XGBoost, KAN (Kolmogorov Arnold Networks), and Ensemble Models](https://arxiv.org/abs/2602.15248)
*Pavel Koptev,Vishnu Kumar,Konstantin Malkov,George Shapiro,Yury Vikhanov*

Main category: cs.AI

TL;DR: 本文提出AI/机器学习框架补充确定性算法，预测供应链金融中的发票稀释风险，使用实时动态信用限额替代传统不可撤销付款承诺


<details>
  <summary>Details</summary>
Motivation: 发票稀释（批准金额与实际收款差额）是供应链金融中非信用风险和利润损失的重要来源。传统依赖买方不可撤销付款承诺（IPU）的方法阻碍了供应链金融的采纳，特别是对于次级投资级买方。需要更灵活的数据驱动方法。

Method: 提出AI/机器学习框架，补充确定性算法预测发票稀释。使用实时动态信用限额方法，基于九个关键交易字段的广泛生产数据集，为每个买方-供应商对实时预测稀释风险。

Result: 论文评估了AI/机器学习框架如何补充确定性算法来预测发票稀释，但具体结果未在摘要中提供。

Conclusion: AI/机器学习框架能够有效补充传统确定性算法，为供应链金融提供更灵活、数据驱动的发票稀释风险管理方案，有助于促进供应链金融的采纳。

Abstract: Invoice or payment dilution is the gap between the approved invoice amount and the actual collection is a significant source of non credit risk and margin loss in supply chain finance. Traditionally, this risk is managed through the buyer's irrevocable payment undertaking (IPU), which commits to full payment without deductions. However, IPUs can hinder supply chain finance adoption, particularly among sub-invested grade buyers. A newer, data-driven methods use real-time dynamic credit limits, projecting dilution for each buyer-supplier pair in real-time. This paper introduces an AI, machine learning framework and evaluates how that can supplement a deterministic algorithm to predict invoice dilution using extensive production dataset across nine key transaction fields.

</details>


### [15] [When Remembering and Planning are Worth it: Navigating under Change](https://arxiv.org/abs/2602.15274)
*Omid Madani,J. Brian Burns,Reza Eghbali,Thomas L. Dean*

Main category: cs.AI

TL;DR: 研究探索在动态不确定环境中，不同类型和用途的记忆如何帮助空间导航。在简单的觅食任务中，智能体需要在障碍物和食物位置每日变化、感知受限的情况下，从家找到食物。研究发现，结合多种策略的架构能有效处理探索和规划等不同性质的任务。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决在动态不确定环境中的空间导航问题。环境具有非平稳性（障碍物和食物位置每日变化）、感知不确定性（位置信息有限且不确定）等挑战。需要开发能够快速学习并适应这些变化的鲁棒导航策略。

Method: 研究比较了从简单到复杂的多种策略，包括不同的记忆使用和学习方式。主要方法包括：使用非平稳概率学习技术持续更新情景记忆；利用这些记忆构建即时地图（不完美地图，受限于智能体经验且包含噪声）；基于记忆进行路径规划；以及结合多种策略的架构来处理探索和搜索等不同性质的任务。

Result: 研究发现：1）需要能够结合多种策略的架构来处理不同性质的任务，特别是当食物位置未知时的探索搜索任务，以及对记忆中的（可能）食物位置进行路径规划；2）使用非平稳概率学习技术持续更新情景记忆，并利用这些记忆构建即时地图进行规划的智能体，在任务难度（如目标距离）增加时，能显著比简单（最小记忆）智能体更高效；3）这种优势的前提是定位和环境变化带来的不确定性不能太大。

Conclusion: 结论表明，在动态不确定环境中，结合多种记忆使用策略的智能体架构能够显著提高空间导航效率。通过非平稳概率学习持续更新情景记忆，并利用这些记忆构建即时地图进行规划的方法，在任务复杂度增加时表现出明显优势，但需要控制不确定性水平以保证有效性。

Abstract: We explore how different types and uses of memory can aid spatial navigation in changing uncertain environments. In the simple foraging task we study, every day, our agent has to find its way from its home, through barriers, to food. Moreover, the world is non-stationary: from day to day, the location of the barriers and food may change, and the agent's sensing such as its location information is uncertain and very limited. Any model construction, such as a map, and use, such as planning, needs to be robust against these challenges, and if any learning is to be useful, it needs to be adequately fast. We look at a range of strategies, from simple to sophisticated, with various uses of memory and learning. We find that an architecture that can incorporate multiple strategies is required to handle (sub)tasks of a different nature, in particular for exploration and search, when food location is not known, and for planning a good path to a remembered (likely) food location. An agent that utilizes non-stationary probability learning techniques to keep updating its (episodic) memories and that uses those memories to build maps and plan on the fly (imperfect maps, i.e. noisy and limited to the agent's experience) can be increasingly and substantially more efficient than the simpler (minimal-memory) agents, as the task difficulties such as distance to goal are raised, as long as the uncertainty, from localization and change, is not too large.

</details>


### [16] [Improving LLM Reliability through Hybrid Abstention and Adaptive Detection](https://arxiv.org/abs/2602.15391)
*Ankit Sharma,Nachiket Tapas,Jyotiprakash Patra*

Main category: cs.AI

TL;DR: 本文提出了一种自适应弃权系统，通过动态调整安全阈值和层级级联机制，在保持高性能的同时平衡LLM的安全性和实用性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM部署面临安全性与实用性的根本权衡：严格的过滤机制会阻止良性查询，而宽松的控制则可能生成不安全内容。传统的基于静态规则或固定置信度阈值的防护措施通常缺乏上下文敏感性且计算成本高，导致高延迟和用户体验下降。

Method: 引入自适应弃权系统，基于实时上下文信号（如领域和用户历史）动态调整安全阈值。该框架集成了由五个并行检测器组成的多维检测架构，通过层级级联机制结合以优化速度和精度。级联设计通过逐步过滤查询减少不必要的计算。

Result: 在混合和特定领域工作负载上的广泛评估显示，假阳性显著减少，特别是在医疗建议和创意写作等敏感领域。系统在严格操作模式下保持高安全精度和接近完美的召回率。与非级联模型和外部防护系统相比，实现了显著的延迟改进。

Conclusion: 上下文感知的弃权框架有效平衡了安全性和实用性，同时保持了性能，为可靠的LLM部署提供了可扩展的解决方案。

Abstract: Large Language Models (LLMs) deployed in production environments face a fundamental safety-utility trade-off either a strict filtering mechanisms prevent harmful outputs but often block benign queries or a relaxed controls risk unsafe content generation. Conventional guardrails based on static rules or fixed confidence thresholds are typically context-insensitive and computationally expensive, resulting in high latency and degraded user experience. To address these limitations, we introduce an adaptive abstention system that dynamically adjusts safety thresholds based on real-time contextual signals such as domain and user history. The proposed framework integrates a multi-dimensional detection architecture composed of five parallel detectors, combined through a hierarchical cascade mechanism to optimize both speed and precision. The cascade design reduces unnecessary computation by progressively filtering queries, achieving substantial latency improvements compared to non-cascaded models and external guardrail systems. Extensive evaluation on mixed and domain-specific workloads demonstrates significant reductions in false positives, particularly in sensitive domains such as medical advice and creative writing. The system maintains high safety precision and near-perfect recall under strict operating modes. Overall, our context-aware abstention framework effectively balances safety and utility while preserving performance, offering a scalable solution for reliable LLM deployment.

</details>


### [17] [GenAI-LA: Generative AI and Learning Analytics Workshop (LAK 2026), April 27--May 1, 2026, Bergen, Norway](https://arxiv.org/abs/2602.15531)
*Javier Irigoyen,Roberto Daza,Aythami Morales,Julian Fierrez,Francisco Jurado,Alvaro Ortigosa,Ruben Tolosana*

Main category: cs.AI

TL;DR: EduEVAL-DB是一个基于教师角色的数据集，用于评估和训练自动教学评估器和AI导师的教学解释能力，包含854个解释，涵盖科学、语言和社会科学K-12年级内容，并提出了教学风险评估框架。


<details>
  <summary>Details</summary>
Motivation: 为了支持自动教学评估器和AI导师的评估与训练，需要专门的数据集来评估教学解释的质量。现有数据集缺乏对教学风险的全面评估框架，特别是在真实教育实践中观察到的教学风格和缺陷方面。

Method: 基于ScienceQA基准的精选子集构建数据集，包含139个问题的854个解释。每个问题提供1个人类教师解释和6个由LLM模拟的教师角色生成的解释。这些角色基于真实教育实践中的教学风格和缺陷，通过提示工程实现。提出了包含五个维度的教学风险评估框架：事实正确性、解释深度和完整性、焦点和相关性、学生水平适宜性、意识形态偏见。采用半自动流程和专家教师评审进行二元风险标注。

Result: 创建了包含854个解释的EduEVAL-DB数据集，涵盖K-12年级的科学、语言和社会科学内容。通过初步验证实验，评估了数据集在评估方面的适用性，比较了Gemini 2.5 Pro和轻量级本地Llama 3.1 8B模型，并探索了在EduEVAL-DB上进行监督微调是否支持可在消费级硬件上部署的模型进行教学风险检测。

Conclusion: EduEVAL-DB为自动教学评估器和AI导师的评估与训练提供了有价值的资源，提出的教学风险框架为评估教学解释质量提供了系统方法。数据集支持在消费级硬件上部署的模型进行教学风险检测，为教育AI系统的开发和评估提供了实用工具。

Abstract: This work introduces EduEVAL-DB, a dataset based on teacher roles designed to support the evaluation and training of automatic pedagogical evaluators and AI tutors for instructional explanations. The dataset comprises 854 explanations corresponding to 139 questions from a curated subset of the ScienceQA benchmark, spanning science, language, and social science across K-12 grade levels. For each question, one human-teacher explanation is provided and six are generated by LLM-simulated teacher roles. These roles are inspired by instructional styles and shortcomings observed in real educational practice and are instantiated via prompt engineering. We further propose a pedagogical risk rubric aligned with established educational standards, operationalizing five complementary risk dimensions: factual correctness, explanatory depth and completeness, focus and relevance, student-level appropriateness, and ideological bias. All explanations are annotated with binary risk labels through a semi-automatic process with expert teacher review. Finally, we present preliminary validation experiments to assess the suitability of EduEVAL-DB for evaluation. We benchmark a state-of-the-art education-oriented model (Gemini 2.5 Pro) against a lightweight local Llama 3.1 8B model and examine whether supervised fine-tuning on EduEVAL-DB supports pedagogical risk detection using models deployable on consumer hardware.

</details>


### [18] [Quantifying construct validity in large language model evaluations](https://arxiv.org/abs/2602.15532)
*Ryan Othniel Kearns*

Main category: cs.AI

TL;DR: 论文提出结构化能力模型，首次从大量LLM基准测试结果中提取可解释且可泛化的能力，解决了现有方法在构造效度方面的不足。


<details>
  <summary>Details</summary>
Motivation: 当前LLM社区常将基准测试结果等同于模型的一般能力，但基准测试存在测试集污染、标注错误等问题。现有方法（潜在因子模型和缩放定律）都无法有效分离基准测试结果与实际能力，无法可靠评估LLM的构造效度。

Method: 提出结构化能力模型，结合缩放定律和潜在因子模型的优势：模型规模应影响能力（如缩放定律），而这些能力应在考虑测量误差的情况下影响观察结果（如潜在因子模型）。在OpenLLM排行榜的大规模结果样本上拟合该模型及其两种替代方案。

Result: 结构化能力模型在简约拟合指数上优于潜在因子模型，在分布外基准预测上优于缩放定律。该模型能够提取可解释且可泛化的能力，在解释和预测能力方面都表现更好。

Conclusion: 结构化能力模型通过适当分离模型规模与能力，结合了缩放定律和潜在因子模型的见解，为量化LLM评估中的构造效度提供了更好的解释和预测能力。

Abstract: The LLM community often reports benchmark results as if they are synonymous with general model capabilities. However, benchmarks can have problems that distort performance, like test set contamination and annotator error. How can we know that a benchmark is a reliable indicator of some capability that we want to measure? This question concerns the construct validity of LLM benchmarks, and it requires separating benchmark results from capabilities when we model and predict LLM performance.
  Both social scientists and computer scientists propose formal models - latent factor models and scaling laws - for identifying the capabilities underlying benchmark scores. However, neither technique is satisfactory for construct validity. Latent factor models ignore scaling laws, and as a result, the capabilities they extract often proxy model size. Scaling laws ignore measurement error, and as a result, the capabilities they extract are both uninterpretable and overfit to the observed benchmarks.
  This thesis presents the structured capabilities model, the first model to extract interpretable and generalisable capabilities from a large collection of LLM benchmark results. I fit this model and its two alternatives on a large sample of results from the OpenLLM Leaderboard. Structured capabilities outperform latent factor models on parsimonious fit indices, and exhibit better out-of-distribution benchmark prediction than scaling laws. These improvements are possible because neither existing approach separates model scale from capabilities in the appropriate way. Model scale should inform capabilities, as in scaling laws, and these capabilities should inform observed results up to measurement error, as in latent factor models. In combining these two insights, structured capabilities demonstrate better explanatory and predictive power for quantifying construct validity in LLM evaluations.

</details>


### [19] [RUVA: Personalized Transparent On-Device Graph Reasoning](https://arxiv.org/abs/2602.15553)
*Gabriele Conte,Alessio Mattiace,Gianni Carmosino,Potito Aghilar,Giovanni Servedio,Francesco Musicco,Vito Walter Anelli,Tommaso Di Noia,Francesco Maria Donini*

Main category: cs.AI

TL;DR: Ruva提出首个"玻璃盒"架构，用于人类参与的记忆管理，通过个人知识图谱实现AI记忆的可检查和精确删除，解决传统向量检索缺乏可解释性和隐私保护的问题。


<details>
  <summary>Details</summary>
Motivation: 当前个人AI领域被"黑盒"检索增强生成主导，传统向量数据库缺乏可解释性：当AI产生幻觉或检索敏感数据时，用户无法检查原因或纠正错误。更严重的是，从向量空间中"删除"概念在数学上不精确，会留下概率性的"幽灵"，违反真正的隐私保护。

Method: Ruva采用"玻璃盒"架构，将个人AI建立在个人知识图谱基础上，实现从向量匹配到图谱推理的范式转变。该架构支持用户检查AI知道的内容，并执行特定事实的精确删除，确保"被遗忘权"。

Result: Ruva实现了人类参与的记忆管理，用户成为自己生活的编辑者。该架构解决了传统向量检索缺乏可解释性和隐私保护的根本问题，提供了可检查和可控制的AI记忆系统。

Conclusion: Ruva通过个人知识图谱架构，为个人AI提供了可解释性和隐私保护的新范式，将记忆管理权交还给用户，实现了真正的"被遗忘权"。

Abstract: The Personal AI landscape is currently dominated by "Black Box" Retrieval-Augmented Generation. While standard vector databases offer statistical matching, they suffer from a fundamental lack of accountability: when an AI hallucinates or retrieves sensitive data, the user cannot inspect the cause nor correct the error. Worse, "deleting" a concept from a vector space is mathematically imprecise, leaving behind probabilistic "ghosts" that violate true privacy. We propose Ruva, the first "Glass Box" architecture designed for Human-in-the-Loop Memory Curation. Ruva grounds Personal AI in a Personal Knowledge Graph, enabling users to inspect what the AI knows and to perform precise redaction of specific facts. By shifting the paradigm from Vector Matching to Graph Reasoning, Ruva ensures the "Right to be Forgotten." Users are the editors of their own lives; Ruva hands them the pen. The project and the demo video are available at http://sisinf00.poliba.it/ruva/.

</details>


### [20] [How Vision Becomes Language: A Layer-wise Information-Theoretic Analysis of Multimodal Reasoning](https://arxiv.org/abs/2602.15580)
*Hongxuan Wu,Yukun Zhang,Xueqing Zhou*

Main category: cs.AI

TL;DR: 该研究提出PID Flow框架，通过信息分解分析多模态Transformer中各层的信息流动模式，发现视觉信息早期达到峰值后衰减，语言信息在深层主导预测（约82%），跨模态协同作用始终低于2%。


<details>
  <summary>Details</summary>
Motivation: 研究多模态Transformer在回答视觉问题时，预测是基于视觉证据、语言推理还是真正的跨模态计算，以及这种结构如何在不同层间演化。

Method: 提出基于部分信息分解（PID）的层间分析框架，结合降维、正态化流高斯化和闭式高斯PID估计的PID Flow流程，应用于LLaVA-1.5-7B和LLaVA-1.6-7B模型在六个GQA推理任务上。

Result: 发现一致的模态转换模式：视觉独特信息早期达到峰值后衰减，语言独特信息在深层激增（约占最终预测的82%），跨模态协同作用始终低于2%。该模式在不同模型变体间高度稳定（层间相关性>0.96），但任务依赖性很强。通过注意力敲除实验验证了因果关系。

Conclusion: 研究提供了多模态Transformer中视觉信息如何转换为语言信息的信息论因果解释，并为识别模态特定信息丢失的架构瓶颈提供了定量指导。

Abstract: When a multimodal Transformer answers a visual question, is the prediction driven by visual evidence, linguistic reasoning, or genuinely fused cross-modal computation -- and how does this structure evolve across layers? We address this question with a layer-wise framework based on Partial Information Decomposition (PID) that decomposes the predictive information at each Transformer layer into redundant, vision-unique, language-unique, and synergistic components. To make PID tractable for high-dimensional neural representations, we introduce \emph{PID Flow}, a pipeline combining dimensionality reduction, normalizing-flow Gaussianization, and closed-form Gaussian PID estimation. Applying this framework to LLaVA-1.5-7B and LLaVA-1.6-7B across six GQA reasoning tasks, we uncover a consistent \emph{modal transduction} pattern: visual-unique information peaks early and decays with depth, language-unique information surges in late layers to account for roughly 82\% of the final prediction, and cross-modal synergy remains below 2\%. This trajectory is highly stable across model variants (layer-wise correlations $>$0.96) yet strongly task-dependent, with semantic redundancy governing the detailed information fingerprint. To establish causality, we perform targeted Image$\rightarrow$Question attention knockouts and show that disrupting the primary transduction pathway induces predictable increases in trapped visual-unique information, compensatory synergy, and total information cost -- effects that are strongest in vision-dependent tasks and weakest in high-redundancy tasks. Together, these results provide an information-theoretic, causal account of how vision becomes language in multimodal Transformers, and offer quantitative guidance for identifying architectural bottlenecks where modality-specific information is lost.

</details>


### [21] [On inferring cumulative constraints](https://arxiv.org/abs/2602.15635)
*Konstantin Sidorov*

Main category: cs.AI

TL;DR: 提出一种预处理方法，通过推断额外的累积约束来捕捉多资源交互，提升调度问题的求解性能


<details>
  <summary>Details</summary>
Motivation: 传统约束编程中累积约束通常单独传播，忽略了多资源间的交互作用，导致在某些基准测试中性能严重下降

Method: 将累积约束解释为占用向量的线性不等式，通过发现不能并行运行的任务集合（覆盖集），对覆盖不等式进行提升强化，并将生成的约束注入调度问题实例

Result: 在标准RCPSP和RCPSP/max测试集上，推断的约束改善了搜索性能并收紧目标边界，发现了25个新的下界和5个新的最优解，其中8个下界直接来自推断的约束

Conclusion: 提出的预处理方法能有效捕捉多资源交互，提升调度问题的求解效率，在有利实例上显著改善性能，在不利实例上仅带来轻微性能下降

Abstract: Cumulative constraints are central in scheduling with constraint programming, yet propagation is typically performed per constraint, missing multi-resource interactions and causing severe slowdowns on some benchmarks. I present a preprocessing method for inferring additional cumulative constraints that capture such interactions without search-time probing. This approach interprets cumulative constraints as linear inequalities over occupancy vectors and generates valid inequalities by (i) discovering covers, the sets of tasks that cannot run in parallel, (ii) strengthening the cover inequalities for the discovered sets with lifting, and (iii) injecting the resulting constraints back into the scheduling problem instance. Experiments on standard RCPSP and RCPSP/max test suites show that these inferred constraints improve search performance and tighten objective bounds on favorable instances, while incurring little degradation on unfavorable ones. Additionally, these experiments discover 25 new lower bounds and five new best solutions; eight of the lower bounds are obtained directly from the inferred constraints.

</details>


### [22] [CARE Drive A Framework for Evaluating Reason-Responsiveness of Vision Language Models in Automated Driving](https://arxiv.org/abs/2602.15645)
*Lucas Elbert Suryana,Farah Bierenga,Sanne van Buuren,Pepijn Kooij,Elsefien Tulleners,Federico Scari,Simeon Calvert,Bart van Arem,Arkady Zgonnikov*

Main category: cs.AI

TL;DR: 提出CARE Drive框架，用于评估自动驾驶中视觉语言模型是否基于人类相关原因进行决策，而非事后合理化解释


<details>
  <summary>Details</summary>
Motivation: 现有评估方法主要关注结果性能（如安全性、轨迹精度），但无法确定模型决策是否真正反映了人类相关考虑因素，这在安全关键领域可能造成虚假信心

Method: 提出CARE Drive框架，通过比较基准模型和原因增强模型在受控上下文变化下的决策，评估人类原因是否因果影响决策行为。采用两阶段评估：提示校准确保稳定输出，系统上下文扰动测量决策对人类原因（如安全裕度、社会压力、效率约束）的敏感性

Result: 在自行车超车场景中，明确的人类原因显著影响模型决策，改善了与专家推荐行为的一致性。但响应性随上下文因素变化，表明对不同类型原因敏感性不均

Conclusion: CARE Drive提供了经验证据，表明无需修改模型参数即可系统评估基础模型中的原因响应性，为安全关键应用中更可靠的模型评估提供了框架

Abstract: Foundation models, including vision language models, are increasingly used in automated driving to interpret scenes, recommend actions, and generate natural language explanations. However, existing evaluation methods primarily assess outcome based performance, such as safety and trajectory accuracy, without determining whether model decisions reflect human relevant considerations. As a result, it remains unclear whether explanations produced by such models correspond to genuine reason responsive decision making or merely post hoc rationalizations. This limitation is especially significant in safety critical domains because it can create false confidence. To address this gap, we propose CARE Drive, Context Aware Reasons Evaluation for Driving, a model agnostic framework for evaluating reason responsiveness in vision language models applied to automated driving. CARE Drive compares baseline and reason augmented model decisions under controlled contextual variation to assess whether human reasons causally influence decision behavior. The framework employs a two stage evaluation process. Prompt calibration ensures stable outputs. Systematic contextual perturbation then measures decision sensitivity to human reasons such as safety margins, social pressure, and efficiency constraints. We demonstrate CARE Drive in a cyclist overtaking scenario involving competing normative considerations. Results show that explicit human reasons significantly influence model decisions, improving alignment with expert recommended behavior. However, responsiveness varies across contextual factors, indicating uneven sensitivity to different types of reasons. These findings provide empirical evidence that reason responsiveness in foundation models can be systematically evaluated without modifying model parameters.

</details>


### [23] [PERSONA: Dynamic and Compositional Inference-Time Personality Control via Activation Vector Algebra](https://arxiv.org/abs/2602.15669)
*Xiachong Feng,Liang Zhao,Weihong Zhong,Yichong Huang,Yuxuan Gu,Lingpeng Kong,Xiaocheng Feng,Bing Qin*

Main category: cs.AI

TL;DR: PERSONA框架通过激活空间中的向量操作实现LLM人格控制，无需训练即可达到微调级别性能，揭示人格特质在表示空间中具有可提取、近似正交的方向特性。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型人格控制方法依赖静态提示或昂贵的微调，无法捕捉人类特质的动态性和组合性，需要一种更灵活、高效且可解释的人格控制框架。

Method: 提出PERSONA训练免费框架，包含三个阶段：Persona-Base通过对比激活分析提取正交特质向量；Persona-Algebra通过向量算术实现精确控制（标量乘法调节强度，加法组合，减法抑制）；Persona-Flow在推理时动态组合向量实现上下文感知适应。

Result: 在PersonalityBench上获得9.60的平均分，接近监督微调上限9.61；在Persona-Evolve动态人格适应基准上，在不同模型家族中达到高达91%的胜率。

Conclusion: LLM的人格方面具有数学可操作性，为可解释和高效的行为控制开辟了新方向，证明人格特质在表示空间中表现为可提取、近似正交的方向。

Abstract: Current methods for personality control in Large Language Models rely on static prompting or expensive fine-tuning, failing to capture the dynamic and compositional nature of human traits. We introduce PERSONA, a training-free framework that achieves fine-tuning level performance through direct manipulation of personality vectors in activation space. Our key insight is that personality traits appear as extractable, approximately orthogonal directions in the model's representation space that support algebraic operations. The framework operates through three stages: Persona-Base extracts orthogonal trait vectors via contrastive activation analysis; Persona-Algebra enables precise control through vector arithmetic (scalar multiplication for intensity, addition for composition, subtraction for suppression); and Persona-Flow achieves context-aware adaptation by dynamically composing these vectors during inference. On PersonalityBench, our approach achieves a mean score of 9.60, nearly matching the supervised fine-tuning upper bound of 9.61 without any gradient updates. On our proposed Persona-Evolve benchmark for dynamic personality adaptation, we achieve up to 91% win rates across diverse model families. These results provide evidence that aspects of LLM personality are mathematically tractable, opening new directions for interpretable and efficient behavioral control.

</details>


### [24] [Recursive Concept Evolution for Compositional Reasoning in Large Language Models](https://arxiv.org/abs/2602.15725)
*Sarim Chaudhry*

Main category: cs.AI

TL;DR: RCE框架让预训练语言模型在推理时动态修改内部表示几何，通过生成低秩概念子空间来构建新抽象，显著提升组合推理能力


<details>
  <summary>Details</summary>
Motivation: 现有方法通过扩展token级搜索来改进推理，但保持模型的潜在表示空间固定。当所需抽象未编码在该空间中时，性能会急剧下降。需要让模型在推理时动态修改内部表示几何

Method: 提出递归概念演化(RCE)框架：检测到表示不足时生成动态低秩概念子空间，通过最小描述长度准则选择，在协同时合并，通过约束优化进行整合以保持稳定性

Result: 在Mistral-7B上集成RCE，在组合推理基准测试中：ARC-AGI-2提升12-18点，GPQA和BBH提升8-14点，MATH和HLE上深度诱导错误持续减少

Conclusion: RCE使预训练语言模型能够在推理时构建新抽象而非仅重组现有抽象，显著提升组合推理能力，为解决复杂推理任务中的表示不足问题提供了新途径

Abstract: Large language models achieve strong performance on many complex reasoning tasks, yet their accuracy degrades sharply on benchmarks that require compositional reasoning, including ARC-AGI-2, GPQA, MATH, BBH, and HLE. Existing methods improve reasoning by expanding token-level search through chain-of-thought prompting, self-consistency, or reinforcement learning, but they leave the model's latent representation space fixed. When the required abstraction is not already encoded in this space, performance collapses. We propose Recursive Concept Evolution (RCE), a framework that enables pretrained language models to modify their internal representation geometry during inference. RCE introduces dynamically generated low-rank concept subspaces that are spawned when representational inadequacy is detected, selected through a minimum description length criterion, merged when synergistic, and consolidated via constrained optimization to preserve stability. This process allows the model to construct new abstractions rather than recombining existing ones. We integrate RCE with Mistral-7B and evaluate it across compositional reasoning benchmarks. RCE yields 12-18 point gains on ARC-AGI-2, 8-14 point improvements on GPQA and BBH, and consistent reductions in depth-induced error on MATH and HLE.

</details>


### [25] [GlobeDiff: State Diffusion Process for Partial Observability in Multi-Agent Systems](https://arxiv.org/abs/2602.15776)
*Yiqin Yang,Xu Yang,Yuhua Jiang,Ni Mu,Hao Hu,Runpeng Xie,Ziyou Zhang,Siyuan Li,Yuan-Hua Ni,Qianchuan Zhao,Bo Xu*

Main category: cs.AI

TL;DR: GlobeDiff：一种基于扩散模型的多模态全局状态推断算法，用于解决多智能体系统中的部分可观测性问题


<details>
  <summary>Details</summary>
Motivation: 在多智能体系统中，部分可观测性是有效协调和决策的关键障碍。现有的信念状态估计和智能体间通信方法存在局限：信念方法主要依赖过去经验而未能充分利用全局信息，通信方法缺乏有效利用辅助信息的鲁棒模型。

Method: 提出Global State Diffusion Algorithm (GlobeDiff)，将状态推断过程建模为多模态扩散过程，基于局部观测推断全局状态。该方法通过扩散模型克服状态估计中的模糊性，同时实现高保真度的全局状态推断。

Result: 理论证明GlobeDiff在单模态和多模态分布下的估计误差均有界。大量实验结果表明，GlobeDiff在性能上优于现有方法，能够准确推断全局状态。

Conclusion: GlobeDiff通过将状态推断建模为扩散过程，有效解决了多智能体系统中的部分可观测性问题，在理论和实验上都表现出优越性能。

Abstract: In the realm of multi-agent systems, the challenge of \emph{partial observability} is a critical barrier to effective coordination and decision-making. Existing approaches, such as belief state estimation and inter-agent communication, often fall short. Belief-based methods are limited by their focus on past experiences without fully leveraging global information, while communication methods often lack a robust model to effectively utilize the auxiliary information they provide. To solve this issue, we propose Global State Diffusion Algorithm~(GlobeDiff) to infer the global state based on the local observations. By formulating the state inference process as a multi-modal diffusion process, GlobeDiff overcomes ambiguities in state estimation while simultaneously inferring the global state with high fidelity. We prove that the estimation error of GlobeDiff under both unimodal and multi-modal distributions can be bounded. Extensive experimental results demonstrate that GlobeDiff achieves superior performance and is capable of accurately inferring the global state.

</details>


### [26] [This human study did not involve human subjects: Validating LLM simulations as behavioral evidence](https://arxiv.org/abs/2602.15785)
*Jessica Hullman,David Broska,Huaman Sun,Aaron Shaw*

Main category: cs.AI

TL;DR: 论文探讨了在社会科学实验中使用大语言模型作为合成参与者的有效性，对比了两种获取有效因果效应估计的策略：启发式方法和统计校准方法


<details>
  <summary>Details</summary>
Motivation: 随着越来越多的研究使用大语言模型作为合成参与者来生成成本效益高且几乎即时的响应，但缺乏关于何时这种模拟能够有效推断人类行为的指导

Method: 对比两种策略：1) 启发式方法通过提示工程、模型微调等修复策略使模拟与观察的人类行为可互换；2) 统计校准方法结合辅助人类数据和统计调整来考虑观察与模拟响应之间的差异

Result: 启发式方法适用于探索性任务但缺乏验证性研究所需的正式统计保证；统计校准在明确假设下保持有效性，并能以更低成本提供比仅依赖人类参与者的实验更精确的因果效应估计

Conclusion: 两种方法的潜力都取决于大语言模型对相关人群的近似程度，研究人员不应仅仅狭隘地关注用大语言模型替代人类参与者，而应考虑被忽视的机会

Abstract: A growing literature uses large language models (LLMs) as synthetic participants to generate cost-effective and nearly instantaneous responses in social science experiments. However, there is limited guidance on when such simulations support valid inference about human behavior. We contrast two strategies for obtaining valid estimates of causal effects and clarify the assumptions under which each is suitable for exploratory versus confirmatory research. Heuristic approaches seek to establish that simulated and observed human behavior are interchangeable through prompt engineering, model fine-tuning, and other repair strategies designed to reduce LLM-induced inaccuracies. While useful for many exploratory tasks, heuristic approaches lack the formal statistical guarantees typically required for confirmatory research. In contrast, statistical calibration combines auxiliary human data with statistical adjustments to account for discrepancies between observed and simulated responses. Under explicit assumptions, statistical calibration preserves validity and provides more precise estimates of causal effects at lower cost than experiments that rely solely on human participants. Yet the potential of both approaches depends on how well LLMs approximate the relevant populations. We consider what opportunities are overlooked when researchers focus myopically on substituting LLMs for human participants in a study.

</details>


### [27] [Enhancing Building Semantics Preservation in AI Model Training with Large Language Model Encodings](https://arxiv.org/abs/2602.15791)
*Suhyung Jang,Ghang Lee,Jaekun Lee,Hyunjun Lee*

Main category: cs.AI

TL;DR: 该研究提出使用大语言模型嵌入作为编码方法，以保留建筑语义中更精细的区分，相比传统one-hot编码能更好地捕捉相关子类型之间的细微关系。


<details>
  <summary>Details</summary>
Motivation: 在AECO行业中，准确表示建筑语义（包括通用对象类型和特定子类型）对AI模型训练至关重要。传统编码方法（如one-hot）往往无法传达密切相关的子类型之间的细微关系，限制了AI的语义理解能力。

Method: 提出使用大语言模型嵌入（如OpenAI GPT和Meta LLaMA）作为编码来保留建筑语义的精细区分。通过训练GraphSAGE模型对5个高层住宅BIM中的42个建筑对象子类型进行分类，测试了不同嵌入维度，包括原始高维LLM嵌入和通过Matryoshka表示模型生成的1024维压缩嵌入。

Result: LLM编码优于传统的one-hot基线，其中llama-3（压缩）嵌入的加权平均F1分数达到0.8766，而one-hot编码为0.8475。结果表明LLM编码能有效提升AI对复杂领域特定建筑语义的解释能力。

Conclusion: 利用基于LLM的编码方法在增强AI解释复杂领域特定建筑语义方面具有显著潜力。随着LLM和降维技术的不断发展，该方法在AECO行业的语义精细化任务中具有广泛的应用前景。

Abstract: Accurate representation of building semantics, encompassing both generic object types and specific subtypes, is essential for effective AI model training in the architecture, engineering, construction, and operation (AECO) industry. Conventional encoding methods (e.g., one-hot) often fail to convey the nuanced relationships among closely related subtypes, limiting AI's semantic comprehension. To address this limitation, this study proposes a novel training approach that employs large language model (LLM) embeddings (e.g., OpenAI GPT and Meta LLaMA) as encodings to preserve finer distinctions in building semantics. We evaluated the proposed method by training GraphSAGE models to classify 42 building object subtypes across five high-rise residential building information models (BIMs). Various embedding dimensions were tested, including original high-dimensional LLM embeddings (1,536, 3,072, or 4,096) and 1,024-dimensional compacted embeddings generated via the Matryoshka representation model. Experimental results demonstrated that LLM encodings outperformed the conventional one-hot baseline, with the llama-3 (compacted) embedding achieving a weighted average F1-score of 0.8766, compared to 0.8475 for one-hot encoding. The results underscore the promise of leveraging LLM-based encodings to enhance AI's ability to interpret complex, domain-specific building semantics. As the capabilities of LLMs and dimensionality reduction techniques continue to evolve, this approach holds considerable potential for broad application in semantic elaboration tasks throughout the AECO industry.

</details>


### [28] [Developing AI Agents with Simulated Data: Why, what, and how?](https://arxiv.org/abs/2602.15816)
*Xiaoran Liu,Istvan David*

Main category: cs.AI

TL;DR: 本章介绍基于仿真的合成数据生成技术，用于解决AI训练中数据不足和质量问题，提出了数字孪生AI仿真解决方案的参考框架。


<details>
  <summary>Details</summary>
Motivation: 现代符号AI应用面临的主要障碍是数据量不足和数据质量不高，因此对合成数据生成技术有强烈需求。仿真提供了一种系统化的方法来生成多样化的合成数据。

Method: 本章介绍了基于仿真的合成数据生成的关键概念、优势和挑战，并提出了一个参考框架，用于描述、设计和分析基于数字孪生的AI仿真解决方案。

Result: 提出了一个系统化的参考框架，帮助研究人员和从业者更好地理解和应用基于数字孪生的仿真技术来生成AI训练所需的合成数据。

Conclusion: 基于仿真的合成数据生成是解决AI训练数据不足问题的有效方法，数字孪生技术为此提供了系统化的解决方案框架，有助于推动现代AI技术的实际应用。

Abstract: As insufficient data volume and quality remain the key impediments to the adoption of modern subsymbolic AI, techniques of synthetic data generation are in high demand. Simulation offers an apt, systematic approach to generating diverse synthetic data. This chapter introduces the reader to the key concepts, benefits, and challenges of simulation-based synthetic data generation for AI training purposes, and to a reference framework to describe, design, and analyze digital twin-based AI simulation solutions.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [29] [Unforgeable Watermarks for Language Models via Robust Signatures](https://arxiv.org/abs/2602.15323)
*Huijia Lin,Kameron Shahabi,Min Jae Song*

Main category: cs.CR

TL;DR: 该论文提出了一种新的水印方案，通过引入不可伪造性和可恢复性两个新保证来加强内容溯源的安全性，防止虚假归属问题。


<details>
  <summary>Details</summary>
Motivation: 语言模型生成的文本越来越难以与人类写作区分，需要强大的工具来验证内容来源。现有水印方案主要关注模型质量保持和鲁棒检测，但对虚假归属的保护有限。

Method: 构建了第一个不可检测、鲁棒、不可伪造且可恢复的水印方案，关键技术是一种称为鲁棒（或可恢复）数字签名的新型密码学原语，允许验证接近已签名消息的消息，同时防止伪造远离所有先前签名消息的消息。

Result: 提出的方案能够将内容与其生成模型独家链接，实现安全归属和细粒度可追溯性，防止对手制作虚假阳性（远离水印模型输出但被标记为有水印的文本）。

Conclusion: 通过引入不可伪造性和可恢复性保证，该研究加强了水印方案的内容所有权保护，为语言模型生成内容的可信溯源提供了更安全的解决方案。

Abstract: Language models now routinely produce text that is difficult to distinguish from human writing, raising the need for robust tools to verify content provenance. Watermarking has emerged as a promising countermeasure, with existing work largely focused on model quality preservation and robust detection. However, current schemes provide limited protection against false attribution. We strengthen the notion of soundness by introducing two novel guarantees: unforgeability and recoverability. Unforgeability prevents adversaries from crafting false positives, texts that are far from any output from the watermarked model but are nonetheless flagged as watermarked. Recoverability provides an additional layer of protection: whenever a watermark is detected, the detector identifies the source text from which the flagged content was derived. Together, these properties strengthen content ownership by linking content exclusively to its generating model, enabling secure attribution and fine-grained traceability. We construct the first undetectable watermarking scheme that is robust, unforgeable, and recoverable with respect to substitutions (i.e., perturbations in Hamming metric). The key technical ingredient is a new cryptographic primitive called robust (or recoverable) digital signatures, which allow verification of messages that are close to signed ones, while preventing forgery of messages that are far from all previously signed messages. We show that any standard digital signature scheme can be boosted to a robust one using property-preserving hash functions (Boyle, LaVigne, and Vaikuntanathan, ITCS 2019).

</details>


### [30] [A Unified Evaluation of Learning-Based Similarity Techniques for Malware Detection](https://arxiv.org/abs/2602.15376)
*Udbhav Prasad,Aniesh Chawla*

Main category: cs.CR

TL;DR: 本文系统比较了基于学习的分类和相似性方法在安全应用中的表现，发现没有单一方法在所有维度都表现良好，需要结合互补技术


<details>
  <summary>Details</summary>
Motivation: 传统加密摘要（如MD5、SHA-256）虽然能提供精确身份验证，但对微小变化的输入会产生完全不同的哈希值，限制了在威胁狩猎、恶意软件分析和数字取证等实际安全任务中的应用。现有相似性技术（如ssdeep、sdhash、TLSH）和基于机器学习的方法缺乏统一的评估框架和比较

Method: 采用统一的实验框架和行业接受的指标，在大规模公开数据集上系统比较基于学习的分类和相似性方法，包括相似性摘要、局部敏感哈希和基于文件特征的机器学习嵌入方法

Result: 研究结果表明，没有单一方法在所有维度都表现良好，每种方法都有明显的权衡取舍。这意味着有效的恶意软件分析和威胁狩猎平台需要结合互补的分类和相似性技术，而不是依赖单一方法

Conclusion: 这是第一个可复现的研究，为现实世界安全工作负载中多样化的基于学习的相似性技术提供了基准测试。研究强调了在安全应用中结合多种互补技术的重要性，而不是寻找"一刀切"的解决方案

Abstract: Cryptographic digests (e.g., MD5, SHA-256) are designed to provide exact identity. Any single-bit change in the input produces a completely different hash, which is ideal for integrity verification but limits their usefulness in many real-world tasks like threat hunting, malware analysis and digital forensics, where adversaries routinely introduce minor transformations. Similarity-based techniques address this limitation by enabling approximate matching, allowing related byte sequences to produce measurably similar fingerprints. Modern enterprises manage tens of thousands of endpoints with billions of files, making the effectiveness and scalability of the proposed techniques more important than ever in security applications. Security researchers have proposed a range of approaches, including similarity digests and locality-sensitive hashes (e.g., ssdeep, sdhash, TLSH), as well as more recent machine-learning-based methods that generate embeddings from file features. However, these techniques have largely been evaluated in isolation, using disparate datasets and evaluation criteria. This paper presents a systematic comparison of learning-based classification and similarity methods using large, publicly available datasets. We evaluate each method under a unified experimental framework with industry-accepted metrics. To our knowledge, this is the first reproducible study to benchmark these diverse learning-based similarity techniques side by side for real-world security workloads. Our results show that no single approach performs well across all dimensions; instead, each exhibits distinct trade-offs, indicating that effective malware analysis and threat-hunting platforms must combine complementary classification and similarity techniques rather than rely on a single method.

</details>


### [31] [MEV in Binance Builder](https://arxiv.org/abs/2602.15395)
*Qin Wang,Ruiqiang Li,Guangsheng Yu,Vincent Gramoli,Shiping Chen*

Main category: cs.CR

TL;DR: 该研究分析了BNB智能链上建设者驱动的MEV套利活动，发现由于白名单机制、短区块间隔和私有订单流等设计特征，导致MEV利润高度集中在少数建设者手中，加剧了中心化和公平性问题。


<details>
  <summary>Details</summary>
Motivation: 研究动机是验证社区对BNB智能链PBS机制中心化问题的担忧。BSC采用白名单建设者、短区块间隔和私有订单流绕过公共内存池的设计，这些特征引发了关于中心化和公平性的长期关注。

Method: 采用实证研究方法，追踪2025年5月至11月期间BNB智能链上两个主导建设者（48Club和Blockrazor）的套利活动。分析包括利润分布、套利路径特征以及区块构建的集中度趋势。

Result: 研究发现：1）48Club和Blockrazor生产了超过96%的区块，捕获了约92%的MEV利润；2）利润集中在短路径、低跳数的包装代币和稳定币套利中；3）区块构建迅速向垄断收敛；4）短区块间隔和白名单机制压缩了MEV竞争的可竞争窗口，放大了延迟优势，排除了较慢的建设者和搜索者。

Conclusion: BNB智能链上的MEV提取不仅比以太坊更加中心化，而且在结构上更容易受到审查和公平性削弱的影响。短区块间隔和白名单PBS设计加剧了中心化问题，使MEV竞争更加不平等。

Abstract: We study the builder-driven MEV arbitrage on BNB Smart Chain (BSC). BSC's Proposer--Builder Separation (PBS) adopts a leaner design: only whitelisted builders can participate, blocks are produced at shorter intervals, and private order flow bypasses the public mempool. These features have long raised community concerns over centralization, which we empirically confirm by tracing arbitrage activity of the two dominant builders from May to November 2025. Within months, 48Club and Blockrazor produced over 96\% of blocks and captured about 92\% of MEV profits.
  We find that profits concentrate in short, low-hop arbitrage routes over wrapped tokens and stablecoins, and that block construction rapidly converges toward monopoly. Beyond concentration alone, our analysis reveals a structural source of inequality: BSC's short block interval and whitelisted PBS collapse the contestable window for MEV competition, amplifying latency advantages and excluding slower builders and searchers. MEV extraction on BSC is not only more centralized than on Ethereum, but also structurally more vulnerable to censorship and weakened fairness.

</details>


### [32] [SecCodeBench-V2 Technical Report](https://arxiv.org/abs/2602.15485)
*Longfei Chen,Ji Zhao,Lanxiao Cui,Tong Su,Xingbo Pan,Ziyang Li,Yongxing Wu,Qijiang Cao,Qiyao Cai,Jing Zhang,Yuandong Ni,Junyao He,Zeyu Zhang,Chao Ge,Xuhuai Lu,Zeyu Gao,Yuxin Cui,Weisen Chen,Yuxuan Peng,Shengping Wang,Qi Li,Yukai Huang,Yukun Liu,Tuo Zhou,Terry Yue Zhuo,Junyang Lin,Chao Zhang*

Main category: cs.CR

TL;DR: SecCodeBench-V2是一个用于评估大语言模型生成安全代码能力的公开基准测试，包含98个基于阿里巴巴工业生产的生成和修复场景，涵盖5种编程语言和22种CWE安全漏洞类别。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏能够全面评估LLM代码助手安全编码能力的标准化基准测试，特别是在工业级安全漏洞场景下的评估。现有基准测试往往无法准确反映模型在实际生产环境中生成安全代码的能力。

Method: 采用函数级任务设计，每个场景提供完整的项目脚手架，要求模型在固定接口和依赖下实现或修复目标函数。通过动态执行评估，在隔离环境中编译运行模型生成的代码，并使用专家编写的PoC测试用例进行功能和安全性验证。对于无法用确定性测试判断的场景，采用LLM-as-a-judge方法。

Result: 建立了包含98个场景的全面基准测试，涵盖Java、C、Python、Go、JavaScript五种语言和22种CWE安全漏洞类别。提供了统一的评估流程和基于Pass@K的评分协议，支持对模型安全编码能力的全面、可比较评估。

Conclusion: SecCodeBench-V2为评估AI代码助手的安全态势提供了严谨、可复现的基础，填补了工业级安全代码生成评估的空白，有助于推动更安全的AI编程助手发展。

Abstract: We introduce SecCodeBench-V2, a publicly released benchmark for evaluating Large Language Model (LLM) copilots' capabilities of generating secure code. SecCodeBench-V2 comprises 98 generation and fix scenarios derived from Alibaba Group's industrial productions, where the underlying security issues span 22 common CWE (Common Weakness Enumeration) categories across five programming languages: Java, C, Python, Go, and JavaScript. SecCodeBench-V2 adopts a function-level task formulation: each scenario provides a complete project scaffold and requires the model to implement or patch a designated target function under fixed interfaces and dependencies. For each scenario, SecCodeBench-V2 provides executable proof-of-concept (PoC) test cases for both functional validation and security verification. All test cases are authored and double-reviewed by security experts, ensuring high fidelity, broad coverage, and reliable ground truth. Beyond the benchmark itself, we build a unified evaluation pipeline that assesses models primarily via dynamic execution. For most scenarios, we compile and run model-generated artifacts in isolated environments and execute PoC test cases to validate both functional correctness and security properties. For scenarios where security issues cannot be adjudicated with deterministic test cases, we additionally employ an LLM-as-a-judge oracle. To summarize performance across heterogeneous scenarios and difficulty levels, we design a Pass@K-based scoring protocol with principled aggregation over scenarios and severity, enabling holistic and comparable evaluation across models. Overall, SecCodeBench-V2 provides a rigorous and reproducible foundation for assessing the security posture of AI coding assistants, with results and artifacts released at https://alibaba.github.io/sec-code-bench. The benchmark is publicly available at https://github.com/alibaba/sec-code-bench.

</details>


### [33] [Onto-DP: Constructing Neighborhoods for Differential Privacy on Ontological Databases](https://arxiv.org/abs/2602.15614)
*Yasmine Hayder,Adrien Boiret,Cédric Eichler,Benjamin Nguyen*

Main category: cs.CR

TL;DR: 论文提出Onto-DP（本体感知差分隐私），通过增强语义意识来保护数据库免受基于推理规则的攻击，弥补了传统差分隐私模型的不足。


<details>
  <summary>Details</summary>
Motivation: 攻击者可以利用推理规则从数据库中挖掘敏感信息，而现有的差分隐私模型在应对此类攻击时存在不足，需要更有效的保护机制。

Method: 提出Onto-DP（本体感知差分隐私），作为经典差分隐私模型的扩展，通过增强语义意识来防范了解推理规则的攻击者。

Result: Onto-DP被证明是充分条件，能够有效保护数据库免受基于推理规则的信息泄露攻击。

Conclusion: Onto-DP通过引入语义意识扩展了差分隐私范式，为对抗推理规则攻击提供了更强大的保护机制。

Abstract: In this paper, we investigate how attackers can discover sensitive information embedded within databases by exploiting inference rules. We demonstrate the inadequacy of naively applied existing state of the art differential privacy (DP) models in safeguarding against such attacks. We introduce ontology aware differential privacy (Onto-DP), a novel extension of differential privacy paradigms built on top of any classical DP model by enriching it with semantic awareness. We show that this extension is a sufficient condition to adequately protect against attackers aware of inference rules.

</details>


### [34] [Revisiting Backdoor Threat in Federated Instruction Tuning from a Signal Aggregation Perspective](https://arxiv.org/abs/2602.15671)
*Haodong Zhao,Jinming Hu,Gongshen Liu*

Main category: cs.CR

TL;DR: 联邦学习中存在一种新型后门威胁：良性客户端数据集中分布的低浓度中毒数据，即使只有不到10%的训练数据中毒，攻击成功率仍可超过85%，且现有防御机制对此完全无效。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习安全研究主要关注少数恶意客户端的后门攻击，但本文挑战这一范式，研究更普遍且隐蔽的威胁：良性客户端数据集中分布的低浓度中毒数据带来的后门漏洞。这在依赖未经验证的第三方和众包数据的联邦指令调优语言模型中尤为常见。

Method: 1) 分析两种后门数据形式：自然触发（固有特征作为隐式触发器）和对手注入触发；2) 从信号聚合角度建模后门植入过程，提出后门信噪比来量化分布式后门信号的动态特性；3) 通过大量实验验证威胁严重性。

Result: 实验结果显示：仅不到10%的训练数据中毒并分布在客户端中，攻击成功率超过85%，而主要任务性能基本不受影响。更重要的是，针对恶意客户端攻击设计的最先进后门防御机制对这种威胁完全无效。

Conclusion: 研究结果突显了针对现代去中心化数据生态系统现实情况开发新防御机制的紧迫需求。这种分布式低浓度中毒数据威胁比传统恶意客户端攻击更隐蔽且难以防御。

Abstract: Federated learning security research has predominantly focused on backdoor threats from a minority of malicious clients that intentionally corrupt model updates. This paper challenges this paradigm by investigating a more pervasive and insidious threat: \textit{backdoor vulnerabilities from low-concentration poisoned data distributed across the datasets of benign clients.} This scenario is increasingly common in federated instruction tuning for language models, which often rely on unverified third-party and crowd-sourced data. We analyze two forms of backdoor data through real cases: 1) \textit{natural trigger (inherent features as implicit triggers)}; 2) \textit{adversary-injected trigger}. To analyze this threat, we model the backdoor implantation process from signal aggregation, proposing the Backdoor Signal-to-Noise Ratio to quantify the dynamics of the distributed backdoor signal. Extensive experiments reveal the severity of this threat: With just less than 10\% of training data poisoned and distributed across clients, the attack success rate exceeds 85\%, while the primary task performance remains largely intact. Critically, we demonstrate that state-of-the-art backdoor defenses, designed for attacks from malicious clients, are fundamentally ineffective against this threat. Our findings highlight an urgent need for new defense mechanisms tailored to the realities of modern, decentralized data ecosystems.

</details>


### [35] [Natural Privacy Filters Are Not Always Free: A Characterization of Free Natural Filters](https://arxiv.org/abs/2602.15815)
*Matthew Regehr,Bingshan Hu,Ethan Leeman,Pasin Manurangsi,Pierre Tholoniat,Mathias Lécuyer*

Main category: cs.CR

TL;DR: 本文研究了自然隐私过滤器，它能够精确组合具有自适应选择隐私特性的差分隐私机制。与仅考虑简单隐私参数的传统过滤器不同，自然过滤器考虑了每个查询的完整隐私配置文件，有望在给定隐私预算下提供更好的效用。


<details>
  <summary>Details</summary>
Motivation: 现有隐私过滤器通常只考虑简单的隐私参数（如Rényi-DP或高斯DP参数），无法充分利用每个查询的完整隐私配置文件信息。自然隐私过滤器旨在通过考虑完整的隐私配置文件来提高隐私机制的效用，但需要研究其是否像其他形式的DP那样可以"免费"获得。

Method: 研究自然隐私过滤器的理论特性，分析其在组合差分隐私机制时的行为。特别关注自然隐私过滤器是否能够"免费"获得（即不增加额外隐私成本），并确定哪些类型的隐私机制家族在组合时允许免费的自然隐私过滤器。

Result: 研究发现，与直觉相反，自然隐私过滤器通常不是免费的。只有当隐私机制家族在组合时具有良好排序特性时，才存在免费的自然隐私过滤器。这意味着并非所有类型的隐私机制都能在不增加额外隐私成本的情况下实现自然隐私过滤。

Conclusion: 自然隐私过滤器虽然能够通过考虑完整的隐私配置文件来提高效用，但通常需要付出额外的隐私成本。只有在隐私机制家族具有良好排序特性的特定条件下，才能实现免费的自然隐私过滤。这一发现对设计高效隐私保护系统具有重要意义。

Abstract: We study natural privacy filters, which enable the exact composition of differentially private (DP) mechanisms with adaptively chosen privacy characteristics. Earlier privacy filters consider only simple privacy parameters such as Rényi-DP or Gaussian DP parameters. Natural filters account for the entire privacy profile of every query, promising greater utility for a given privacy budget. We show that, contrary to other forms of DP, natural privacy filters are not free in general. Indeed, we show that only families of privacy mechanisms that are well-ordered when composed admit free natural privacy filters.

</details>
