<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 6]
- [cs.CR](#cs.CR) [Total: 18]
- [cs.AI](#cs.AI) [Total: 26]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [A German Gold-Standard Dataset for Sentiment Analysis in Software Engineering](https://arxiv.org/abs/2507.07325)
*Martin Obaidi,Marc Herrmann,Elisa Schmid,Raymond Ochsner,Kurt Schneider,Jil Klünder*

Main category: cs.SE

TL;DR: 该研究填补了德语情感分析在软件开发领域的空白，通过构建一个标注良好的德语开发者论坛数据集，并验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有情感分析工具主要基于英语或非德语数据集，缺乏针对德语软件开发社区的领域特定解决方案。

Method: 从德语开发者论坛Android-Hilfe.de提取5,949条开发者语句，由四位德语母语的计算机科学学生基于Shaver等人的情感模型进行六种基本情感标注。

Result: 标注过程显示出高评分者一致性和可靠性，数据集验证有效。现有德语情感分析工具在软件开发领域表现不足。

Conclusion: 该数据集为德语软件开发社区的情感分析提供了可靠资源，并讨论了优化标注和进一步应用的途径。

Abstract: Sentiment analysis is an essential technique for investigating the emotional
climate within developer teams, contributing to both team productivity and
project success. Existing sentiment analysis tools in software engineering
primarily rely on English or non-German gold-standard datasets. To address this
gap, our work introduces a German dataset of 5,949 unique developer statements,
extracted from the German developer forum Android-Hilfe.de. Each statement was
annotated with one of six basic emotions, based on the emotion model by Shaver
et al., by four German-speaking computer science students. Evaluation of the
annotation process showed high interrater agreement and reliability. These
results indicate that the dataset is sufficiently valid and robust to support
sentiment analysis in the German-speaking software engineering community.
Evaluation with existing German sentiment analysis tools confirms the lack of
domain-specific solutions for software engineering. We also discuss approaches
to optimize annotation and present further use cases for the dataset.

</details>


### [2] [Automatic Generation of Explainability Requirements and Software Explanations From User Reviews](https://arxiv.org/abs/2507.07344)
*Martin Obaidi,Jannik Fischbach,Jakob Droste,Hannah Deters,Marc Herrmann,Jil Klünder,Steffen Krätzig,Hugo Villamizar,Kurt Schneider*

Main category: cs.SE

TL;DR: 论文提出了一种自动化工具，用于从用户反馈中提取可解释性需求并生成对应解释，评估显示AI生成解释在清晰度上优于人工，但需人工验证正确性。


<details>
  <summary>Details</summary>
Motivation: 增强透明性、建立用户信任和确保合规性是可解释性的关键需求，但现有方法难以将用户反馈转化为结构化需求和解释。

Method: 引入工具支持的方法，自动化从用户评论中提取需求并生成解释，通过工业合作创建标注数据集进行评估。

Result: AI生成的需求在相关性和正确性上不如人工，但生成的解释在清晰度和风格上更受青睐，正确性仍需人工验证。

Conclusion: 该工作推动了可解释性需求的研究，提供了自动化方法、实证见解和公开数据集。

Abstract: Explainability has become a crucial non-functional requirement to enhance
transparency, build user trust, and ensure regulatory compliance. However,
translating explanation needs expressed in user feedback into structured
requirements and corresponding explanations remains challenging. While existing
methods can identify explanation-related concerns in user reviews, there is no
established approach for systematically deriving requirements and generating
aligned explanations. To contribute toward addressing this gap, we introduce a
tool-supported approach that automates this process. To evaluate its
effectiveness, we collaborated with an industrial automation manufacturer to
create a dataset of 58 user reviews, each annotated with manually crafted
explainability requirements and explanations. Our evaluation shows that while
AI-generated requirements often lack relevance and correctness compared to
human-created ones, the AI-generated explanations are frequently preferred for
their clarity and style. Nonetheless, correctness remains an issue,
highlighting the importance of human validation. This work contributes to the
advancement of explainability requirements in software systems by (1)
introducing an automated approach to derive requirements from user reviews and
generate corresponding explanations, (2) providing empirical insights into the
strengths and limitations of automatically generated artifacts, and (3)
releasing a curated dataset to support future research on the automatic
generation of explainability requirements.

</details>


### [3] [Towards an Engineering Workflow Management System for Asset Administration Shells using BPMN](https://arxiv.org/abs/2507.07468)
*Sten Grüner,Nafise Eskandani*

Main category: cs.SE

TL;DR: 论文探讨了在工程工作流中使用资产管理壳（AAS）与BPMN结合的方法，提出了一种分布式AAS写入时复制基础设施，并开发了一个工作流管理原型以提高效率和可追溯性。


<details>
  <summary>Details</summary>
Motivation: 工业4.0技术的集成对自动化和优化工程流程至关重要，AAS是实现互操作数字孪生的关键。

Method: 结合AAS与BPMN定义结构化自动化流程，提出分布式AAS写入时复制基础设施，并开发工作流管理原型。

Result: 提出的基础设施增强了安全性和可扩展性，原型实现了AAS操作和工程工作流的自动化。

Conclusion: AAS与BPMN的结合及分布式基础设施为跨组织协作提供了高效、安全的解决方案。

Abstract: The integration of Industry 4.0 technologies into engineering workflows is an
essential step toward automating and optimizing plant and process engineering
processes. The Asset Administration Shell (AAS) serves as a key enabler for
creating interoperable Digital Twins that facilitate engineering data exchange
and automation. This paper explores the use of AAS within engineering
workflows, particularly in combination with Business Process Model and Notation
(BPMN) to define structured and automated processes. We propose a distributed
AAS copy-on-write infrastructure that enhances security and scalability while
enabling seamless cross organizational collaboration. We also introduce a
workflow management prototype automating AAS operations and engineering
workflows, improving efficiency and traceability.

</details>


### [4] [From Requirements to Code: Understanding Developer Practices in LLM-Assisted Software Engineering](https://arxiv.org/abs/2507.07548)
*Jonathan Ullrich,Matthias Koch,Andreas Vogelsang*

Main category: cs.SE

TL;DR: 论文探讨了开发者如何利用LLMs生成代码时整合需求信息，发现现有需求文档过于抽象，需手动分解为编程任务并补充设计决策才能用于提示LLMs。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs是否能直接根据需求生成高质量代码，填补开发者如何利用需求信息与LLMs交互的研究空白。

Method: 访谈了来自14家公司的18名从业者，分析他们如何将需求和设计文档转化为LLMs的输入。

Result: 需求文档需手动分解为编程任务并补充设计信息才能有效用于LLMs，表明传统需求工程仍不可或缺。

Conclusion: LLMs生成代码时仍需人工分解和丰富需求信息，强调了需求工程在自动化软件任务中的重要性。

Abstract: With the advent of generative LLMs and their advanced code generation
capabilities, some people already envision the end of traditional software
engineering, as LLMs may be able to produce high-quality code based solely on
the requirements a domain expert feeds into the system. The feasibility of this
vision can be assessed by understanding how developers currently incorporate
requirements when using LLMs for code generation-a topic that remains largely
unexplored. We interviewed 18 practitioners from 14 companies to understand how
they (re)use information from requirements and other design artifacts to feed
LLMs when generating code. Based on our findings, we propose a theory that
explains the processes developers employ and the artifacts they rely on. Our
theory suggests that requirements, as typically documented, are too abstract
for direct input into LLMs. Instead, they must first be manually decomposed
into programming tasks, which are then enriched with design decisions and
architectural constraints before being used in prompts. Our study highlights
that fundamental RE work is still necessary when LLMs are used to generate
code. Our theory is important for contextualizing scientific approaches to
automating requirements-centric SE tasks.

</details>


### [5] [Prompt Engineering for Requirements Engineering: A Literature Review and Roadmap](https://arxiv.org/abs/2507.07682)
*Kaicheng Huang,Fanyu Wang,Yutan Huang,Chetan Arora*

Main category: cs.SE

TL;DR: 本文通过系统性文献综述提出了一个面向路线图的Prompt Engineering（PE4RE）分类法，以解决LLM在需求工程（RE）中的不确定性和可控性问题。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在需求工程中的应用存在不确定性和缺乏可控性，缺乏有效的提示方法指导，阻碍了其可信赖的实现。

Method: 采用Kitchenham和Petersen的二次研究协议，筛选并分析了35项主要研究，提出了一种混合分类法，将技术导向的模式与任务导向的RE角色关联。

Result: 研究通过两个研究问题及其子问题，揭示了当前的任务、LLM家族和提示类型，并指出了局限性和研究空白。

Conclusion: 提出了一个逐步路线图，将当前的临时PE原型发展为可重复、便于实践的工作流程。

Abstract: Advancements in large language models (LLMs) have led to a surge of prompt
engineering (PE) techniques that can enhance various requirements engineering
(RE) tasks. However, current LLMs are often characterized by significant
uncertainty and a lack of controllability. This absence of clear guidance on
how to effectively prompt LLMs acts as a barrier to their trustworthy
implementation in the RE field. We present the first roadmap-oriented
systematic literature review of Prompt Engineering for RE (PE4RE). Following
Kitchenham's and Petersen's secondary-study protocol, we searched six digital
libraries, screened 867 records, and analyzed 35 primary studies. To bring
order to a fragmented landscape, we propose a hybrid taxonomy that links
technique-oriented patterns (e.g., few-shot, Chain-of-Thought) to task-oriented
RE roles (elicitation, validation, traceability). Two research questions, with
five sub-questions, map the tasks addressed, LLM families used, and prompt
types adopted, and expose current limitations and research gaps. Finally, we
outline a step-by-step roadmap showing how today's ad-hoc PE prototypes can
evolve into reproducible, practitioner-friendly workflows.

</details>


### [6] [From Domain Documents to Requirements: Retrieval-Augmented Generation in the Space Industry](https://arxiv.org/abs/2507.07689)
*Chetan Arora,Fanyu Wang,Chakkrit Tantithamthavorn,Aldeida Aleti,Shaun Kenyon*

Main category: cs.SE

TL;DR: 探讨使用检索增强生成（RAG）模型支持太空领域需求工程（RE）的可行性，提出模块化AI方法，初步结果显示可减少人工工作量并提高需求覆盖。


<details>
  <summary>Details</summary>
Motivation: 小型太空组织难以从大量非结构化文档中提取可操作需求，需自动化支持以降低参与高要求任务的障碍。

Method: 预处理太空任务文档，分类为语义类别，检索相关领域标准内容，利用大语言模型生成需求草案。

Result: 初步应用显示方法可行，能减少人工工作、提高需求覆盖并支持轻量级合规对齐。

Conclusion: 提出AI在需求工程中更广泛集成的路线图，旨在帮助小型组织参与大型关键任务。

Abstract: Requirements engineering (RE) in the space industry is inherently complex,
demanding high precision, alignment with rigorous standards, and adaptability
to mission-specific constraints. Smaller space organisations and new entrants
often struggle to derive actionable requirements from extensive, unstructured
documents such as mission briefs, interface specifications, and regulatory
standards. In this innovation opportunity paper, we explore the potential of
Retrieval-Augmented Generation (RAG) models to support and (semi-)automate
requirements generation in the space domain. We present a modular, AI-driven
approach that preprocesses raw space mission documents, classifies them into
semantically meaningful categories, retrieves contextually relevant content
from domain standards, and synthesises draft requirements using large language
models (LLMs). We apply the approach to a real-world mission document from the
space domain to demonstrate feasibility and assess early outcomes in
collaboration with our industry partner, Starbound Space Solutions. Our
preliminary results indicate that the approach can reduce manual effort,
improve coverage of relevant requirements, and support lightweight compliance
alignment. We outline a roadmap toward broader integration of AI in RE
workflows, intending to lower barriers for smaller organisations to participate
in large-scale, safety-critical missions.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [7] [WatchWitch: Interoperability, Privacy, and Autonomy for the Apple Watch](https://arxiv.org/abs/2507.07210)
*Nils Rollshausen,Alexander Heinrich,Matthias Hollick,Jiska Classen*

Main category: cs.CR

TL;DR: 论文通过逆向工程Apple Watch的无线协议，发现其安全漏洞，并开发了Android版替代方案WatchWitch，提升用户隐私控制和数据自主权。


<details>
  <summary>Details</summary>
Motivation: Apple Watch用户对其健康数据的处理方式缺乏选择权，且只能依赖苹果的封闭生态系统。

Method: 逆向工程Apple Watch的无线协议，发现安全漏洞，并开发了Android版实现WatchWitch。

Result: 成功实现与Apple Watch的互操作性，提供更强的隐私控制和数据自主权。

Conclusion: 为智能手表生态系统提供了更多用户选择，增强了对设备的控制。

Abstract: Smartwatches such as the Apple Watch collect vast amounts of intimate health
and fitness data as we wear them. Users have little choice regarding how this
data is processed: The Apple Watch can only be used with Apple's iPhones, using
their software and their cloud services. We are the first to publicly
reverse-engineer the watch's wireless protocols, which led to discovering
multiple security issues in Apple's proprietary implementation. With
WatchWitch, our custom Android reimplementation, we break out of Apple's walled
garden -- demonstrating practical interoperability with enhanced privacy
controls and data autonomy. We thus pave the way for more consumer choice in
the smartwatch ecosystem, offering users more control over their devices.

</details>


### [8] [Automated Attack Testflow Extraction from Cyber Threat Report using BERT for Contextual Analysis](https://arxiv.org/abs/2507.07244)
*Faissal Ahmadou,Sepehr Ghaffarzadegan,Boubakr Nour,Makan Pourzandi,Mourad Debbabi,Chadi Assi*

Main category: cs.CR

TL;DR: FLOWGUARDIAN利用BERT和NLP技术自动从非结构化威胁报告中提取攻击测试流程，提高网络安全测试的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 手动提取攻击测试流程耗时且易出错，需要自动化解决方案以提升安全团队的响应能力。

Method: 采用BERT和NLP技术分析安全事件，重构攻击序列并生成测试流程。

Result: 实证验证显示FLOWGUARDIAN在公共威胁报告中表现出高准确性和效率。

Conclusion: FLOWGUARDIAN显著提升了安全团队在主动威胁狩猎和事件响应中的能力。

Abstract: In the ever-evolving landscape of cybersecurity, the rapid identification and
mitigation of Advanced Persistent Threats (APTs) is crucial. Security
practitioners rely on detailed threat reports to understand the tactics,
techniques, and procedures (TTPs) employed by attackers. However, manually
extracting attack testflows from these reports requires elusive knowledge and
is time-consuming and prone to errors. This paper proposes FLOWGUARDIAN, a
novel solution leveraging language models (i.e., BERT) and Natural Language
Processing (NLP) techniques to automate the extraction of attack testflows from
unstructured threat reports. FLOWGUARDIAN systematically analyzes and
contextualizes security events, reconstructs attack sequences, and then
generates comprehensive testflows. This automated approach not only saves time
and reduces human error but also ensures comprehensive coverage and robustness
in cybersecurity testing. Empirical validation using public threat reports
demonstrates FLOWGUARDIAN's accuracy and efficiency, significantly enhancing
the capabilities of security teams in proactive threat hunting and incident
response.

</details>


### [9] [Disa: Accurate Learning-based Static Disassembly with Attentions](https://arxiv.org/abs/2507.07246)
*Peicheng Wang,Monika Santra,Mingyu Liu,Cong Sun,Dongrui Zeng,Gang Tan*

Main category: cs.CR

TL;DR: Disa是一种基于学习的新型反汇编方法，利用多头自注意力机制学习指令相关性，显著提升反汇编准确性，尤其在混淆二进制文件中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统反汇编方法依赖文件格式假设和特定架构启发式，易产生不完整和错误结果，尤其在混淆二进制中表现不佳。深度学习为反汇编提供了新思路。

Method: Disa利用多头自注意力机制学习指令相关性，推断函数入口点和指令边界，并识别与内存块边界相关的指令，以生成更精确的控制流图（CFG）。

Result: Disa在函数入口点识别上优于现有深度学习方法，F1分数提升9.1%和13.2%，内存块精度提升18.5%，间接调用目标减少4.4%。

Conclusion: Disa通过深度学习显著提升反汇编准确性，尤其在混淆二进制中表现突出，为安全领域提供了更可靠的反汇编工具。

Abstract: For reverse engineering related security domains, such as vulnerability
detection, malware analysis, and binary hardening, disassembly is crucial yet
challenging. The fundamental challenge of disassembly is to identify
instruction and function boundaries. Classic approaches rely on file-format
assumptions and architecture-specific heuristics to guess the boundaries,
resulting in incomplete and incorrect disassembly, especially when the binary
is obfuscated. Recent advancements of disassembly have demonstrated that deep
learning can improve both the accuracy and efficiency of disassembly. In this
paper, we propose Disa, a new learning-based disassembly approach that uses the
information of superset instructions over the multi-head self-attention to
learn the instructions' correlations, thus being able to infer function
entry-points and instruction boundaries. Disa can further identify instructions
relevant to memory block boundaries to facilitate an advanced block-memory
model based value-set analysis for an accurate control flow graph (CFG)
generation. Our experiments show that Disa outperforms prior deep-learning
disassembly approaches in function entry-point identification, especially
achieving 9.1% and 13.2% F1-score improvement on binaries respectively
obfuscated by the disassembly desynchronization technique and popular
source-level obfuscator. By achieving an 18.5% improvement in the memory block
precision, Disa generates more accurate CFGs with a 4.4% reduction in Average
Indirect Call Targets (AICT) compared with the state-of-the-art heuristic-based
approach.

</details>


### [10] [Semi-fragile watermarking of remote sensing images using DWT, vector quantization and automatic tiling](https://arxiv.org/abs/2507.07250)
*Jordi Serra-Ruiz,David Megías*

Main category: cs.CR

TL;DR: 提出了一种用于多波段图像的半脆弱水印方案，通过树结构向量量化方法嵌入水印，以检测图像篡改。


<details>
  <summary>Details</summary>
Motivation: 保护遥感图像的完整性，防止篡改，同时允许一定程度的有损压缩。

Method: 将图像分割为三维块，为每个块构建树结构向量量化器，并通过迭代算法嵌入水印。

Result: 方法能在有损压缩（超过阈值）下保留水印，同时检测篡改块及其位置。

Conclusion: 该方案有效平衡了水印的鲁棒性和脆弱性，适用于遥感图像保护。

Abstract: A semi-fragile watermarking scheme for multiple band images is presented in
this article. We propose to embed a mark into remote sensing images applying a
tree-structured vector quantization approach to the pixel signatures instead of
processing each band separately. The signature of the multispectral or
hyperspectral image is used to embed the mark in it order to detect any
significant modification of the original image. The image is segmented into
three-dimensional blocks, and a tree-structured vector quantizer is built for
each block. These trees are manipulated using an iterative algorithm until the
resulting block satisfies a required criterion, which establishes the embedded
mark. The method is shown to be able to preserve the mark under lossy
compression (above a given threshold) but, at the same time, it detects
possibly forged blocks and their position in the whole image.

</details>


### [11] [FedP3E: Privacy-Preserving Prototype Exchange for Non-IID IoT Malware Detection in Cross-Silo Federated Learning](https://arxiv.org/abs/2507.07258)
*Rami Darwish,Mahmoud Abdelsalam,Sajad Khorsandroo,Kaushik Roy*

Main category: cs.CR

TL;DR: 论文提出了一种名为FedP3E的新型联邦学习框架，通过隐私保护的原型交换解决物联网环境中数据异构性和类别不平衡的问题。


<details>
  <summary>Details</summary>
Motivation: 物联网生态系统的扩展使其成为复杂恶意软件攻击的目标，需要隐私保护且能处理数据异构性的检测框架。

Method: FedP3E框架利用高斯混合模型生成类原型，添加高斯噪声保护隐私，并通过SMOTE增强少数类表示。

Result: 在N-BaIoT数据集上的实验表明，FedP3E能有效减少统计异构性的负面影响，且通信开销低。

Conclusion: FedP3E通过原型交换机制提升了联邦学习在真实场景中的性能，同时保护了数据隐私。

Abstract: As IoT ecosystems continue to expand across critical sectors, they have
become prominent targets for increasingly sophisticated and large-scale malware
attacks. The evolving threat landscape, combined with the sensitive nature of
IoT-generated data, demands detection frameworks that are both
privacy-preserving and resilient to data heterogeneity. Federated Learning (FL)
offers a promising solution by enabling decentralized model training without
exposing raw data. However, standard FL algorithms such as FedAvg and FedProx
often fall short in real-world deployments characterized by class imbalance and
non-IID data distributions -- particularly in the presence of rare or disjoint
malware classes. To address these challenges, we propose FedP3E
(Privacy-Preserving Prototype Exchange), a novel FL framework that supports
indirect cross-client representation sharing while maintaining data privacy.
Each client constructs class-wise prototypes using Gaussian Mixture Models
(GMMs), perturbs them with Gaussian noise, and transmits only these compact
summaries to the server. The aggregated prototypes are then distributed back to
clients and integrated into local training, supported by SMOTE-based
augmentation to enhance representation of minority malware classes. Rather than
relying solely on parameter averaging, our prototype-driven mechanism enables
clients to enrich their local models with complementary structural patterns
observed across the federation -- without exchanging raw data or gradients.
This targeted strategy reduces the adverse impact of statistical heterogeneity
with minimal communication overhead. We evaluate FedP3E on the N-BaIoT dataset
under realistic cross-silo scenarios with varying degrees of data imbalance.

</details>


### [12] [Shuffling for Semantic Secrecy](https://arxiv.org/abs/2507.07401)
*Fupei Chen,Liyao Xiang,Haoxiang Sun,Hei Victor Cheng,Kaiming Shen*

Main category: cs.CR

TL;DR: 论文提出了一种基于随机打乱特征的语义安全通信系统，以平衡传输速率和泄漏率，提升安全性。


<details>
  <summary>Details</summary>
Motivation: 研究深度学习在语义通信中的安全性问题，改进传统安全编码方案，以在传输速率和泄漏率之间取得平衡。

Method: 设计了一种随机打乱特征的语义安全通信系统，打乱模式作为共享密钥，扭曲语义以阻止窃听者获取信息。

Result: 仿真显示，该方法在提升安全传输方面显著优于基准方法，尤其在强噪声和不可预测衰落信道中表现突出。

Conclusion: 随机打乱特征的方法有效提升了语义通信的安全性，可作为现有系统的插件灵活应用。

Abstract: Deep learning draws heavily on the latest progress in semantic
communications. The present paper aims to examine the security aspect of this
cutting-edge technique from a novel shuffling perspective. Our goal is to
improve upon the conventional secure coding scheme to strike a desirable
tradeoff between transmission rate and leakage rate. To be more specific, for a
wiretap channel, we seek to maximize the transmission rate while minimizing the
semantic error probability under the given leakage rate constraint. Toward this
end, we devise a novel semantic security communication system wherein the
random shuffling pattern plays the role of the shared secret key. Intuitively,
the permutation of feature sequences via shuffling would distort the semantic
essence of the target data to a sufficient extent so that eavesdroppers cannot
access it anymore. The proposed random shuffling method also exhibits its
flexibility in working for the existing semantic communication system as a
plugin. Simulations demonstrate the significant advantage of the proposed
method over the benchmark in boosting secure transmission, especially when
channels are prone to strong noise and unpredictable fading.

</details>


### [13] [Phishing Detection in the Gen-AI Era: Quantized LLMs vs Classical Models](https://arxiv.org/abs/2507.07406)
*Jikesh Thapa,Gurrehmat Chahal,Serban Voinea Gabreanu,Yazan Otoum*

Main category: cs.CR

TL;DR: 论文比较了传统机器学习、深度学习和量化小参数大语言模型（LLM）在钓鱼检测中的表现，发现LLM在识别上下文钓鱼线索方面潜力巨大，但当前准确率较低。研究还探讨了零样本和少样本提示策略的影响，并展示了轻量级LLM在成本效率和解释性方面的优势。


<details>
  <summary>Details</summary>
Motivation: 钓鱼攻击日益复杂，需要高精度且计算高效的检测系统。

Method: 通过实验比较传统ML、DL和量化小参数LLM在钓鱼检测中的表现，并评估零样本和少样本提示策略的影响。

Result: LLM在识别上下文钓鱼线索方面潜力大，但准确率低于ML和DL；轻量级LLM（如DeepSeek R1 Distill Qwen 14B）在80%以上准确率下仅需17GB VRAM。

Conclusion: 优化后的LLM有望成为钓鱼防御系统的组成部分，为现代网络安全框架提供高效且可解释的AI解决方案。

Abstract: Phishing attacks are becoming increasingly sophisticated, underscoring the
need for detection systems that strike a balance between high accuracy and
computational efficiency. This paper presents a comparative evaluation of
traditional Machine Learning (ML), Deep Learning (DL), and quantized
small-parameter Large Language Models (LLMs) for phishing detection. Through
experiments on a curated dataset, we show that while LLMs currently
underperform compared to ML and DL methods in terms of raw accuracy, they
exhibit strong potential for identifying subtle, context-based phishing cues.
We also investigate the impact of zero-shot and few-shot prompting strategies,
revealing that LLM-rephrased emails can significantly degrade the performance
of both ML and LLM-based detectors. Our benchmarking highlights that models
like DeepSeek R1 Distill Qwen 14B (Q8_0) achieve competitive accuracy, above
80%, using only 17GB of VRAM, supporting their viability for cost-efficient
deployment. We further assess the models' adversarial robustness and
cost-performance tradeoffs, and demonstrate how lightweight LLMs can provide
concise, interpretable explanations to support real-time decision-making. These
findings position optimized LLMs as promising components in phishing defence
systems and offer a path forward for integrating explainable, efficient AI into
modern cybersecurity frameworks.

</details>


### [14] [Hybrid LLM-Enhanced Intrusion Detection for Zero-Day Threats in IoT Networks](https://arxiv.org/abs/2507.07413)
*Mohammad F. Al-Hammouri,Yazan Otoum,Rasha Atwa,Amiya Nayak*

Main category: cs.CR

TL;DR: 提出了一种结合传统签名检测和GPT-2语义分析的新型入侵检测方法，显著提升了检测精度并减少了误报。


<details>
  <summary>Details</summary>
Motivation: 随着网络威胁日益复杂，尤其是在物联网等分布式和资源受限环境中，传统入侵检测方法难以应对新型攻击模式。

Method: 提出了一种混合框架，结合签名检测的鲁棒性和GPT-2的语义分析能力。

Result: 实验表明，该方法检测精度提升6.3%，误报减少9.0%，并保持近实时响应。

Conclusion: 语言模型集成有望构建智能、可扩展且适应现代网络环境的网络安全防御系统。

Abstract: This paper presents a novel approach to intrusion detection by integrating
traditional signature-based methods with the contextual understanding
capabilities of the GPT-2 Large Language Model (LLM). As cyber threats become
increasingly sophisticated, particularly in distributed, heterogeneous, and
resource-constrained environments such as those enabled by the Internet of
Things (IoT), the need for dynamic and adaptive Intrusion Detection Systems
(IDSs) becomes increasingly urgent. While traditional methods remain effective
for detecting known threats, they often fail to recognize new and evolving
attack patterns. In contrast, GPT-2 excels at processing unstructured data and
identifying complex semantic relationships, making it well-suited to uncovering
subtle, zero-day attack vectors. We propose a hybrid IDS framework that merges
the robustness of signature-based techniques with the adaptability of
GPT-2-driven semantic analysis. Experimental evaluations on a representative
intrusion dataset demonstrate that our model enhances detection accuracy by
6.3%, reduces false positives by 9.0%, and maintains near real-time
responsiveness. These results affirm the potential of language model
integration to build intelligent, scalable, and resilient cybersecurity
defences suited for modern connected environments.

</details>


### [15] [Autonomous AI-based Cybersecurity Framework for Critical Infrastructure: Real-Time Threat Mitigation](https://arxiv.org/abs/2507.07416)
*Jenifer Paulraj,Brindha Raghuraman,Nagarani Gopalakrishnan,Yazan Otoum*

Main category: cs.CR

TL;DR: 论文提出了一种混合AI驱动的网络安全框架，用于增强关键基础设施的实时漏洞检测、威胁建模和自动修复能力。


<details>
  <summary>Details</summary>
Motivation: 关键基础设施系统对社会稳定和经济韧性至关重要，但其日益增长的互联性使其面临多种网络威胁。

Method: 研究采用混合AI驱动的网络安全框架，结合实时漏洞检测、威胁建模和自动修复技术。

Result: 研究结果为加强关键基础设施的安全性和韧性提供了可行见解。

Conclusion: 论文强调了AI在应对关键基础设施网络安全威胁中的潜力，并提出了一个有效的解决方案。

Abstract: Critical infrastructure systems, including energy grids, healthcare
facilities, transportation networks, and water distribution systems, are
pivotal to societal stability and economic resilience. However, the increasing
interconnectivity of these systems exposes them to various cyber threats,
including ransomware, Denial-of-Service (DoS) attacks, and Advanced Persistent
Threats (APTs). This paper examines cybersecurity vulnerabilities in critical
infrastructure, highlighting the threat landscape, attack vectors, and the role
of Artificial Intelligence (AI) in mitigating these risks. We propose a hybrid
AI-driven cybersecurity framework to enhance real-time vulnerability detection,
threat modelling, and automated remediation. This study also addresses the
complexities of adversarial AI, regulatory compliance, and integration. Our
findings provide actionable insights to strengthen the security and resilience
of critical infrastructure systems against emerging cyber threats.

</details>


### [16] [May I have your Attention? Breaking Fine-Tuning based Prompt Injection Defenses using Architecture-Aware Attacks](https://arxiv.org/abs/2507.07417)
*Nishit V. Pandya,Andrey Labunets,Sicun Gao,Earlence Fernandes*

Main category: cs.CR

TL;DR: 论文评估了基于微调的提示注入防御方法的鲁棒性，提出了一种新的注意力攻击算法，并成功攻击了两种防御方法，成功率高达70%。


<details>
  <summary>Details</summary>
Motivation: 研究提示注入防御方法在whitebox设置下的实际安全性，揭示其潜在漏洞。

Method: 提出了一种基于注意力的攻击算法，并应用于两种防御方法SecAlign和StruQ。

Result: 攻击成功率高达70%，且攻击成本（token数量）增加有限。

Conclusion: 论文揭示了当前提示注入防御方法的局限性，为理解其鲁棒性提供了重要进展。

Abstract: A popular class of defenses against prompt injection attacks on large
language models (LLMs) relies on fine-tuning the model to separate instructions
and data, so that the LLM does not follow instructions that might be present
with data. There are several academic systems and production-level
implementations of this idea. We evaluate the robustness of this class of
prompt injection defenses in the whitebox setting by constructing strong
optimization-based attacks and showing that the defenses do not provide the
claimed security properties. Specifically, we construct a novel attention-based
attack algorithm for text-based LLMs and apply it to two recent whitebox
defenses SecAlign (CCS 2025) and StruQ (USENIX Security 2025), showing attacks
with success rates of up to 70% with modest increase in attacker budget in
terms of tokens. Our findings make fundamental progress towards understanding
the robustness of prompt injection defenses in the whitebox setting. We release
our code and attacks at https://github.com/nishitvp/better_opts_attacks

</details>


### [17] [RADAR: a Radio-based Analytics for Dynamic Association and Recognition of pseudonyms in VANETs](https://arxiv.org/abs/2507.07732)
*Giovanni Gambigliani Zoccoli,Filip Valgimigli,Dario Stabili,Mirco Marchetti*

Main category: cs.CR

TL;DR: RADAR算法通过结合DSRC和Wi-Fi信号，提高了车辆在C-ITS中的追踪能力，尤其在攻击者无法覆盖全部路径时表现更优。Pearson RSSI指标在所有场景中均优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 研究旨在突破VANETs中隐私保护的假名方案，通过多信号结合提升车辆追踪能力。

Method: 利用DSRC和Wi-Fi探针请求信号，比较三种关联指标（Count、Statistical RSSI、Pearson RSSI）。

Result: Pearson RSSI指标在所有场景中表现最佳，优于现有方法。

Conclusion: RADAR算法在多信号结合下显著提升追踪效果，Pearson RSSI为最佳指标，研究公开了实现和仿真场景。

Abstract: This paper presents RADAR, a tracking algorithm for vehicles participating in
Cooperative Intelligent Transportation Systems (C-ITS) that exploits multiple
radio signals emitted by a modern vehicle to break privacy-preserving pseudonym
schemes deployed in VANETs. This study shows that by combining Dedicated Short
Range Communication (DSRC) and Wi-Fi probe request messages broadcast by the
vehicle, it is possible to improve tracking over standard de-anonymization
approaches that only leverage DSRC, especially in realistic scenarios where the
attacker does not have full coverage of the entire vehicle path. The
experimental evaluation compares three different metrics for pseudonym and
Wi-Fi probe identifier association (Count, Statistical RSSI, and Pearson RSSI),
demonstrating that the Pearson RSSI metric is better at tracking vehicles under
pseudonym-changing schemes in all scenarios and against previous works. As an
additional contribution to the state-of-the-art, we publicly release all
implementations and simulation scenarios used in this work.

</details>


### [18] [Rainbow Artifacts from Electromagnetic Signal Injection Attacks on Image Sensors](https://arxiv.org/abs/2507.07773)
*Youqian Zhang,Xinyu Ji,Zhihao Wang,Qinhong Jiang*

Main category: cs.CR

TL;DR: 该论文研究了一种针对图像传感器模拟域的新型电磁信号注入攻击，揭示了CMOS图像传感器中未记录的彩虹色伪影现象，并评估了其对目标检测模型的负面影响。


<details>
  <summary>Details</summary>
Motivation: 图像传感器在安全和关键系统中广泛应用，其数据完整性至关重要。然而，现有数字完整性检查无法防范模拟域攻击，因此需要研究此类漏洞。

Method: 通过精心调制的电磁干扰，诱导CMOS图像传感器产生彩虹色伪影，并分析其对图像信号处理流程和目标检测模型的影响。

Result: 攻击导致图像传感器捕获的图像出现伪影，并显著影响目标检测模型的预测准确性。

Conclusion: 研究揭示了视觉感知系统中一个关键且未被充分探索的漏洞，强调了需要针对物理层攻击开发更鲁棒的防御措施。

Abstract: Image sensors are integral to a wide range of safety- and security-critical
systems, including surveillance infrastructure, autonomous vehicles, and
industrial automation. These systems rely on the integrity of visual data to
make decisions. In this work, we investigate a novel class of electromagnetic
signal injection attacks that target the analog domain of image sensors,
allowing adversaries to manipulate raw visual inputs without triggering
conventional digital integrity checks. We uncover a previously undocumented
attack phenomenon on CMOS image sensors: rainbow-like color artifacts induced
in images captured by image sensors through carefully tuned electromagnetic
interference. We further evaluate the impact of these attacks on
state-of-the-art object detection models, showing that the injected artifacts
propagate through the image signal processing pipeline and lead to significant
mispredictions. Our findings highlight a critical and underexplored
vulnerability in the visual perception stack, highlighting the need for more
robust defenses against physical-layer attacks in such systems.

</details>


### [19] [Mitigating Watermark Stealing Attacks in Generative Models via Multi-Key Watermarking](https://arxiv.org/abs/2507.07871)
*Toluwani Aremu,Noor Hussein,Munachiso Nwadike,Samuele Poppi,Jie Zhang,Karthik Nandakumar,Neil Gong,Nils Lukas*

Main category: cs.CR

TL;DR: 论文提出了一种多密钥扩展方法，用于减轻生成式AI水印窃取攻击，保护内容来源的真实性。


<details>
  <summary>Details</summary>
Motivation: 生成式AI提供商面临水印窃取攻击的威胁，攻击者可能伪造水印以虚假指控提供商。

Method: 提出了一种多密钥扩展方法，适用于任何水印技术，无需修改底层水印机制。

Result: 理论保证和实验证明该方法显著降低了伪造水印的有效性。

Conclusion: 多密钥扩展方法能有效减轻水印窃取攻击，为生成式AI提供商提供安全保障。

Abstract: Watermarking offers a promising solution for GenAI providers to establish the
provenance of their generated content. A watermark is a hidden signal embedded
in the generated content, whose presence can later be verified using a secret
watermarking key. A threat to GenAI providers are \emph{watermark stealing}
attacks, where users forge a watermark into content that was \emph{not}
generated by the provider's models without access to the secret key, e.g., to
falsely accuse the provider. Stealing attacks collect \emph{harmless}
watermarked samples from the provider's model and aim to maximize the expected
success rate of generating \emph{harmful} watermarked samples. Our work focuses
on mitigating stealing attacks while treating the underlying watermark as a
black-box. Our contributions are: (i) Proposing a multi-key extension to
mitigate stealing attacks that can be applied post-hoc to any watermarking
method across any modality. (ii) We provide theoretical guarantees and
demonstrate empirically that our method makes forging substantially less
effective across multiple datasets, and (iii) we formally define the threat of
watermark forging as the task of generating harmful, watermarked content and
model this threat via security games.

</details>


### [20] [The Trust Fabric: Decentralized Interoperability and Economic Coordination for the Agentic Web](https://arxiv.org/abs/2507.07901)
*Sree Bhargavi Balija,Rekha Singal,Abhishek Singh,Ramesh Raskar,Erfan Darzi,Raghu Bala,Thomas Hardjono,Ken Huang*

Main category: cs.CR

TL;DR: Nanda Unified Architecture提出了一种去中心化框架，解决了AI代理生态系统的互操作性、信任和经济协调问题，通过分布式注册、语义代理卡和动态信任层实现高效协作。


<details>
  <summary>Details</summary>
Motivation: 当前协议（如MCP、A2A、ACP和AGP）无法满足AI代理生态系统的互操作性、信任和经济协调需求，亟需一种新的解决方案。

Method: 采用分布式注册实现快速DID代理发现，语义代理卡支持可验证凭证和组合性配置，动态信任层整合行为证明与策略合规，并引入X42/H42微支付和MAESTRO安全框架。

Result: 实际部署显示99.9%的合规性，高交易量且隐私保障强，实现了全球互操作的“代理互联网”，信任成为协作的核心。

Conclusion: Nanda Unified Architecture通过密码学证明和策略即代码，将代理转变为去中心化经济中的可信参与者，为企业和Web3生态系统提供协作基础。

Abstract: The fragmentation of AI agent ecosystems has created urgent demands for
interoperability, trust, and economic coordination that current protocols --
including MCP (Hou et al., 2025), A2A (Habler et al., 2025), ACP (Liu et al.,
2025), and Cisco's AGP (Edwards, 2025) -- cannot address at scale. We present
the Nanda Unified Architecture, a decentralized framework built around three
core innovations: fast DID-based agent discovery through distributed
registries, semantic agent cards with verifiable credentials and composability
profiles, and a dynamic trust layer that integrates behavioral attestations
with policy compliance. The system introduces X42/H42 micropayments for
economic coordination and MAESTRO, a security framework incorporating
Synergetics' patented AgentTalk protocol (US Patent 12,244,584 B1) and secure
containerization. Real-world deployments demonstrate 99.9 percent compliance in
healthcare applications and substantial monthly transaction volumes with strong
privacy guarantees. By unifying MIT's trust research with production
deployments from Cisco and Synergetics, we show how cryptographic proofs and
policy-as-code transform agents into trust-anchored participants in a
decentralized economy (Lakshmanan, 2025; Sha, 2025). The result enables a
globally interoperable Internet of Agents where trust becomes the native
currency of collaboration across both enterprise and Web3 ecosystems.

</details>


### [21] [Can Large Language Models Improve Phishing Defense? A Large-Scale Controlled Experiment on Warning Dialogue Explanations](https://arxiv.org/abs/2507.07916)
*Federico Maria Cau,Giuseppe Desolda,Francesco Greco,Lucio Davide Spano,Luca Viganò*

Main category: cs.CR

TL;DR: 研究评估了大型语言模型（LLMs）生成钓鱼警告解释的能力，发现其效果可媲美或优于人工生成解释。


<details>
  <summary>Details</summary>
Motivation: 钓鱼攻击通过利用人类行为绕过技术防御，现有警告对话框因解释不清和内容静态而效果有限。

Method: 通过大规模用户研究（N=750）比较人工与LLMs（Claude 3.5 Sonnet和Llama 3.3 70B）生成的警告解释效果，分析两种解释风格（基于特征和反事实）对行为指标和感知结果的影响。

Result: LLMs生成的解释在降低钓鱼易感性上表现优异，Claude效果尤为突出；基于特征的解释对真实钓鱼更有效，反事实解释减少误报率。

Conclusion: LLMs可自动生成钓鱼警告解释，具有可扩展性、适应性和人性化特点。

Abstract: Phishing has become a prominent risk in modern cybersecurity, often used to
bypass technological defences by exploiting predictable human behaviour.
Warning dialogues are a standard mitigation measure, but the lack of
explanatory clarity and static content limits their effectiveness. In this
paper, we report on our research to assess the capacity of Large Language
Models (LLMs) to generate clear, concise, and scalable explanations for
phishing warnings. We carried out a large-scale between-subjects user study (N
= 750) to compare the influence of warning dialogues supplemented with manually
generated explanations against those generated by two LLMs, Claude 3.5 Sonnet
and Llama 3.3 70B. We investigated two explanatory styles (feature-based and
counterfactual) for their effects on behavioural metrics (click-through rate)
and perceptual outcomes (e.g., trust, risk, clarity). The results indicate that
well-constructed LLM-generated explanations can equal or surpass manually
crafted explanations in reducing susceptibility to phishing; Claude-generated
warnings exhibited particularly robust performance. Feature-based explanations
were more effective for genuine phishing attempts, whereas counterfactual
explanations diminished false-positive rates. Other variables such as workload,
gender, and prior familiarity with warning dialogues significantly moderated
warning effectiveness. These results indicate that LLMs can be used to
automatically build explanations for warning users against phishing, and that
such solutions are scalable, adaptive, and consistent with human-centred
values.

</details>


### [22] [KeyDroid: A Large-Scale Analysis of Secure Key Storage in Android Apps](https://arxiv.org/abs/2507.07927)
*Jenny Blessing,Ross J. Anderson,Alastair R. Beresford*

Main category: cs.CR

TL;DR: 本文首次全面调查了Android设备中硬件支持的密钥存储使用情况，分析了490,119个应用，发现56.3%处理敏感数据的应用未使用可信硬件，仅5.03%使用了最强的安全元件。


<details>
  <summary>Details</summary>
Motivation: 研究动机是了解Android开发者对可信硬件的使用情况，尤其是处理敏感数据的应用是否充分利用了硬件保护。

Method: 方法包括分析大量Android应用的使用数据，并与Play Store的数据安全标签进行交叉验证，同时首次对可信硬件性能进行实证分析。

Result: 结果显示，尽管行业鼓励采用可信硬件，但大多数应用未使用或仅使用较弱形式。性能分析表明，高级安全元件在某些加密操作中性能不可行。

Conclusion: 结论指出，当前可信硬件的采用率低且性能限制明显，需进一步优化和推广。

Abstract: Most contemporary mobile devices offer hardware-backed storage for
cryptographic keys, user data, and other sensitive credentials. Such hardware
protects credentials from extraction by an adversary who has compromised the
main operating system, such as a malicious third-party app. Since 2011, Android
app developers can access trusted hardware via the Android Keystore API. In
this work, we conduct the first comprehensive survey of hardware-backed key
storage in Android devices. We analyze 490 119 Android apps, collecting data on
how trusted hardware is used by app developers (if used at all) and
cross-referencing our findings with sensitive user data collected by each app,
as self-reported by developers via the Play Store's data safety labels.
  We find that despite industry-wide initiatives to encourage adoption, 56.3%
of apps self-reporting as processing sensitive user data do not use Android's
trusted hardware capabilities at all, while just 5.03% of apps collecting some
form of sensitive data use the strongest form of trusted hardware, a secure
element distinct from the main processor. To better understand the potential
downsides of using secure hardware, we conduct the first empirical analysis of
trusted hardware performance in mobile devices, measuring the runtime of common
cryptographic operations across both software- and hardware-backed keystores.
We find that while hardware-backed key storage using a coprocessor is viable
for most common cryptographic operations, secure elements capable of preventing
more advanced attacks make performance infeasible for symmetric encryption with
non-negligible payloads and any kind of asymmetric encryption.

</details>


### [23] [EinHops: Einsum Notation for Expressive Homomorphic Operations on RNS-CKKS Tensors](https://arxiv.org/abs/2507.07972)
*Karthik Garimella,Austin Ebel,Brandon Reagen*

Main category: cs.CR

TL;DR: 论文提出了一种基于爱因斯坦求和（einsum）符号的方法，用于在完全同态加密（FHE）中处理多维张量运算，并开发了EinHops系统，提高了透明度和可操作性。


<details>
  <summary>Details</summary>
Motivation: FHE虽然支持加密数据上的计算，但仅提供有限的1-D向量操作，多维张量运算需要复杂的打包和映射，现有系统缺乏透明性，难以调试和优化。

Method: 利用einsum符号明确编码张量结构和操作，将其分解为FHE友好的操作序列，开发了EinHops系统。

Result: EinHops能够处理从简单转置到复杂多维收缩的张量运算，系统简单、通用且易于理解。

Conclusion: einsum符号的显式特性使得FHE张量系统更透明和灵活，EinHops为开发者提供了清晰的底层打包策略。

Abstract: Fully Homomorphic Encryption (FHE) is an encryption scheme that allows for
computation to be performed directly on encrypted data, effectively closing the
loop on secure and outsourced computing. Data is encrypted not only during rest
and transit, but also during processing. However, FHE provides a limited
instruction set: SIMD addition, SIMD multiplication, and cyclic rotation of 1-D
vectors. This restriction makes performing multi-dimensional tensor operations
challenging. Practitioners must pack these tensors into 1-D vectors and map
tensor operations onto this one-dimensional layout rather than their
traditional nested structure. And while prior systems have made significant
strides in automating this process, they often hide critical packing decisions
behind layers of abstraction, making debugging, optimizing, and building on top
of these systems difficult.
  In this work, we approach multi-dimensional tensor operations in FHE through
Einstein summation (einsum) notation. Einsum notation explicitly encodes
dimensional structure and operations in its syntax, naturally exposing how
tensors should be packed and transformed. We decompose einsum expressions into
a fixed set of FHE-friendly operations. We implement our design and present
EinHops, a minimalist system that factors einsum expressions into a fixed
sequence of FHE operations. EinHops enables developers to perform encrypted
tensor operations using FHE while maintaining full visibility into the
underlying packing strategy. We evaluate EinHops on a range of tensor
operations from a simple transpose to complex multi-dimensional contractions.
We show that the explicit nature of einsum notation allows us to build an FHE
tensor system that is simple, general, and interpretable. We open-source
EinHops at the following repository: https://github.com/baahl-nyu/einhops.

</details>


### [24] [Defending Against Prompt Injection With a Few DefensiveTokens](https://arxiv.org/abs/2507.07974)
*Sizhe Chen,Yizhu Wang,Nicholas Carlini,Chawin Sitawarin,David Wagner*

Main category: cs.CR

TL;DR: 论文提出了一种名为DefensiveToken的测试时防御方法，用于抵御大型语言模型（LLM）系统中的提示注入攻击，其安全性接近训练时防御方法。


<details>
  <summary>Details</summary>
Motivation: 提示注入攻击通过注入指令覆盖用户任务，威胁LLM系统安全。现有测试时防御效果不如训练时防御，因此需要一种更有效的测试时防御方法。

Method: 通过插入特殊令牌（DefensiveToken）并优化其嵌入，开发者可在需要时添加少量令牌以提升安全性，无需时则保持模型原有性能。

Result: DefensiveToken在测试时实现了接近训练时防御的安全性，同时保持了模型的高质量响应能力。

Conclusion: DefensiveToken为开发者提供了灵活的安全与性能切换选项，是一种高效的测试时防御方案。

Abstract: When large language model (LLM) systems interact with external data to
perform complex tasks, a new attack, namely prompt injection, becomes a
significant threat. By injecting instructions into the data accessed by the
system, the attacker is able to override the initial user task with an
arbitrary task directed by the attacker. To secure the system, test-time
defenses, e.g., defensive prompting, have been proposed for system developers
to attain security only when needed in a flexible manner. However, they are
much less effective than training-time defenses that change the model
parameters. Motivated by this, we propose DefensiveToken, a test-time defense
with prompt injection robustness comparable to training-time alternatives.
DefensiveTokens are newly inserted as special tokens, whose embeddings are
optimized for security. In security-sensitive cases, system developers can
append a few DefensiveTokens before the LLM input to achieve security with a
minimal utility drop. In scenarios where security is less of a concern,
developers can simply skip DefensiveTokens; the LLM system remains the same as
there is no defense, generating high-quality responses. Thus, DefensiveTokens,
if released alongside the model, allow a flexible switch between the
state-of-the-art (SOTA) utility and almost-SOTA security at test time. The code
is available at https://github.com/Sizhe-Chen/DefensiveToken.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [25] [Autonomous Control Leveraging LLMs: An Agentic Framework for Next-Generation Industrial Automation](https://arxiv.org/abs/2507.07115)
*Javal Vyas,Mehmet Mercangoz*

Main category: cs.AI

TL;DR: 论文提出了一种结合符号推理和自适应控制的统一代理框架，利用大语言模型（LLMs）同时处理离散故障恢复规划和连续过程控制。


<details>
  <summary>Details</summary>
Motivation: 现代化学过程日益复杂，劳动力短缺和故障场景复杂，需要新的自动化范式。

Method: 采用有限状态机（FSMs）作为可解释的操作框架，通过LLM驱动的规划代理提出恢复序列，模拟代理执行和验证，验证-重新提示循环迭代优化无效计划。

Result: 在案例研究1中，GPT-4o和GPT-4o-mini在180个随机生成的FSMs中实现了100%的有效路径成功率；案例研究2中，LLM控制器在非线性动态处理中表现优于传统PID控制。

Conclusion: 通过结构化反馈和模块化代理，LLMs能够统一高级符号规划和低级连续控制，为化学工程中的弹性、语言驱动自动化铺平道路。

Abstract: The increasing complexity of modern chemical processes, coupled with
workforce shortages and intricate fault scenarios, demands novel automation
paradigms that blend symbolic reasoning with adaptive control. In this work, we
introduce a unified agentic framework that leverages large language models
(LLMs) for both discrete fault-recovery planning and continuous process control
within a single architecture. We adopt Finite State Machines (FSMs) as
interpretable operating envelopes: an LLM-driven planning agent proposes
recovery sequences through the FSM, a Simulation Agent executes and checks each
transition, and a Validator-Reprompting loop iteratively refines invalid plans.
In Case Study 1, across 180 randomly generated FSMs of varying sizes (4-25
states, 4-300 transitions), GPT-4o and GPT-4o-mini achieve 100% valid-path
success within five reprompts-outperforming open-source LLMs in both accuracy
and latency. In Case Study 2, the same framework modulates dual-heater inputs
on a laboratory TCLab platform (and its digital twin) to maintain a target
average temperature under persistent asymmetric disturbances. Compared to
classical PID control, our LLM-based controller attains similar performance,
while ablation of the prompting loop reveals its critical role in handling
nonlinear dynamics. We analyze key failure modes-such as instruction following
lapses and coarse ODE approximations. Our results demonstrate that, with
structured feedback and modular agents, LLMs can unify high-level symbolic
planningand low-level continuous control, paving the way towards resilient,
language-driven automation in chemical engineering.

</details>


### [26] [BOOST: Out-of-Distribution-Informed Adaptive Sampling for Bias Mitigation in Stylistic Convolutional Neural Networks](https://arxiv.org/abs/2507.07134)
*Mridula Vijendran,Shuang Chen,Jingjing Deng,Hubert P. H. Shum*

Main category: cs.AI

TL;DR: 论文提出了一种名为BOOST的新方法，通过动态调整温度缩放和采样概率，减少AI模型在艺术分类中的偏见，并在KaoKore和PACS数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: AI模型在艺术分类中因数据集不平衡导致的偏见问题日益严重，影响公平性和准确性，现有研究多忽视此问题。

Method: 提出BOOST方法，动态调整温度缩放和采样概率，并引入新指标SODC评估类间分离和偏见减少。

Result: BOOST在KaoKore和PACS数据集上有效平衡了性能和公平性。

Conclusion: BOOST为艺术领域AI模型的去偏见提供了稳健解决方案。

Abstract: The pervasive issue of bias in AI presents a significant challenge to
painting classification, and is getting more serious as these systems become
increasingly integrated into tasks like art curation and restoration. Biases,
often arising from imbalanced datasets where certain artistic styles dominate,
compromise the fairness and accuracy of model predictions, i.e., classifiers
are less accurate on rarely seen paintings. While prior research has made
strides in improving classification performance, it has largely overlooked the
critical need to address these underlying biases, that is, when dealing with
out-of-distribution (OOD) data. Our insight highlights the necessity of a more
robust approach to bias mitigation in AI models for art classification on
biased training data. We propose a novel OOD-informed model bias adaptive
sampling method called BOOST (Bias-Oriented OOD Sampling and Tuning). It
addresses these challenges by dynamically adjusting temperature scaling and
sampling probabilities, thereby promoting a more equitable representation of
all classes. We evaluate our proposed approach to the KaoKore and PACS
datasets, focusing on the model's ability to reduce class-wise bias. We further
propose a new metric, Same-Dataset OOD Detection Score (SODC), designed to
assess class-wise separation and per-class bias reduction. Our method
demonstrates the ability to balance high performance with fairness, making it a
robust solution for unbiasing AI models in the art domain.

</details>


### [27] [State-Inference-Based Prompting for Natural Language Trading with Game NPCs](https://arxiv.org/abs/2507.07203)
*Minkyung Kim,Junsik Kim,Hwidong Bae,Woongcheol Yang,Sangdon Park,Sohee Bae*

Main category: cs.AI

TL;DR: SIBP方法通过状态推断和规则遵守，解决了大语言模型在交易系统中的规则违反问题，显著提升了准确性和信任度。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在动态游戏交互中表现不佳，尤其是在规则驱动的交易系统中，常出现规则违反问题（如物品幻觉和计算错误），影响玩家信任。

Method: 提出State-Inference-Based Prompting (SIBP)，将交易分解为六个状态，通过上下文感知的物品引用和占位符价格计算实现规则遵守。

Result: 在100次交易对话中，SIBP实现了>97%的状态遵守率、>95%的引用准确率和99.7%的计算精度，优于基线方法。

Conclusion: SIBP为商业游戏中可信赖的NPC交互提供了实用基础，同时保持了计算效率。

Abstract: Large Language Models enable dynamic game interactions but struggle with
rule-governed trading systems. Current implementations suffer from rule
violations, such as item hallucinations and calculation errors, that erode
player trust. Here, State-Inference-Based Prompting (SIBP) enables reliable
trading through autonomous dialogue state inference and context-specific rule
adherence. The approach decomposes trading into six states within a unified
prompt framework, implementing context-aware item referencing and
placeholder-based price calculations. Evaluation across 100 trading dialogues
demonstrates >97% state compliance, >95% referencing accuracy, and 99.7%
calculation precision. SIBP maintains computational efficiency while
outperforming baseline approaches, establishing a practical foundation for
trustworthy NPC interactions in commercial games.

</details>


### [28] [Neurosymbolic Feature Extraction for Identifying Forced Labor in Supply Chains](https://arxiv.org/abs/2507.07217)
*Zili Wang,Frank Montabon,Kristin Yvonne Rozier*

Main category: cs.AI

TL;DR: 论文探讨了如何利用神经符号方法在数据稀疏且不可靠的供应链中检测非法活动，并比较了人工与自动化特征提取的效果。


<details>
  <summary>Details</summary>
Motivation: 供应链网络复杂且涉及非法活动时分析困难，传统机器学习需要大量数据，但非法供应链数据稀疏且不可靠。

Method: 采用神经符号方法，提出基于问题树的LLM查询方法，比较人工与自动化特征提取的效果。

Result: 能够系统评估人工与机器分类新闻文章的差异，识别供应链中的强迫劳动相关活动。

Conclusion: 神经符号方法在数据稀疏的非法供应链分析中具有潜力，问题树方法有助于量化文章相关性。

Abstract: Supply chain networks are complex systems that are challenging to analyze;
this problem is exacerbated when there are illicit activities involved in the
supply chain, such as counterfeit parts, forced labor, or human trafficking.
While machine learning (ML) can find patterns in complex systems like supply
chains, traditional ML techniques require large training data sets. However,
illicit supply chains are characterized by very sparse data, and the data that
is available is often (purposely) corrupted or unreliable in order to hide the
nature of the activities. We need to be able to automatically detect new
patterns that correlate with such illegal activity over complex, even temporal
data, without requiring large training data sets. We explore neurosymbolic
methods for identifying instances of illicit activity in supply chains and
compare the effectiveness of manual and automated feature extraction from news
articles accurately describing illicit activities uncovered by authorities. We
propose a question tree approach for querying a large language model (LLM) to
identify and quantify the relevance of articles. This enables a systematic
evaluation of the differences between human and machine classification of news
articles related to forced labor in supply chains.

</details>


### [29] [Open Source Planning & Control System with Language Agents for Autonomous Scientific Discovery](https://arxiv.org/abs/2507.07257)
*Licong Xu,Milind Sarkar,Anto I. Lonappan,Íñigo Zubeldia,Pablo Villanueva-Domingo,Santiago Casas,Christian Fidler,Chetana Amancharla,Ujjwal Tiwari,Adrian Bayer,Chadi Ait Ekiou,Miles Cranmer,Adrian Dimitrov,James Fergusson,Kahaan Gandhi,Sven Krippendorf,Andrew Laverick,Julien Lesgourgues,Antony Lewis,Thomas Meier,Blake Sherwin,Kristen Surrao,Francisco Villaescusa-Navarro,Chi Wang,Xueqing Xu,Boris Bolliet*

Main category: cs.AI

TL;DR: 介绍了一个名为cmbagent的多智能体系统，用于自动化科研任务，由约30个LLM智能体组成，采用规划与控制策略协调工作流，无需人工干预。系统成功应用于博士级宇宙学任务，性能优于现有LLM。


<details>
  <summary>Details</summary>
Motivation: 旨在通过多智能体系统实现科研任务的完全自动化，减少人工干预，提高效率。

Method: 系统由约30个LLM智能体组成，每个智能体专精不同任务（如检索、编码、结果解释等），采用规划与控制策略协调工作流，并能本地执行代码。

Result: 成功应用于博士级宇宙学任务，性能优于现有LLM，代码开源并部署于HuggingFace和云端。

Conclusion: cmbagent展示了多智能体系统在科研自动化中的潜力，性能优越且可扩展。

Abstract: We present a multi-agent system for automation of scientific research tasks,
cmbagent. The system is formed by about 30 Large Language Model (LLM) agents
and implements a Planning & Control strategy to orchestrate the agentic
workflow, with no human-in-the-loop at any point. Each agent specializes in a
different task (performing retrieval on scientific papers and codebases,
writing code, interpreting results, critiquing the output of other agents) and
the system is able to execute code locally. We successfully apply cmbagent to
carry out a PhD level cosmology task (the measurement of cosmological
parameters using supernova data) and evaluate its performance on two benchmark
sets, finding superior performance over state-of-the-art LLMs. The source code
is available on GitHub, demonstration videos are also available, and the system
is deployed on HuggingFace and will be available on the cloud.

</details>


### [30] [Application of LLMs to Multi-Robot Path Planning and Task Allocation](https://arxiv.org/abs/2507.07302)
*Ashish Kumar*

Main category: cs.AI

TL;DR: 本文研究了在多智能体强化学习中利用大型语言模型作为专家规划器以实现高效探索的方法。


<details>
  <summary>Details</summary>
Motivation: 多智能体强化学习中的高效探索问题因算法复杂性而加剧，本文旨在探索专家规划（如大型语言模型）在此类任务中的应用。

Method: 提出使用大型语言模型作为专家规划器，指导多智能体在基于规划的任务中进行高效探索。

Result: 研究表明，大型语言模型能够有效提升多智能体在复杂任务中的探索效率。

Conclusion: 大型语言模型作为专家规划器在多智能体强化学习中具有潜力，可显著提升探索效率。

Abstract: Efficient exploration is a well known problem in deep reinforcement learning
and this problem is exacerbated in multi-agent reinforcement learning due the
intrinsic complexities of such algorithms. There are several approaches to
efficiently explore an environment to learn to solve tasks by multi-agent
operating in that environment, of which, the idea of expert exploration is
investigated in this work. More specifically, this work investigates the
application of large-language models as expert planners for efficient
exploration in planning based tasks for multiple agents.

</details>


### [31] [ViDove: A Translation Agent System with Multimodal Context and Memory-Augmented Reasoning](https://arxiv.org/abs/2507.07306)
*Yichen Lu,Wei Dai,Jiaen Liu,Ching Wing Kwok,Zongheng Wu,Xudong Xiao,Ao Sun,Sheng Fu,Jianyuan Zhan,Yian Wang,Takatomo Saito,Sicheng Lai*

Main category: cs.AI

TL;DR: ViDove是一种多模态翻译代理系统，通过结合视觉和上下文信息提升翻译质量，显著优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM翻译代理通常仅支持文本输入，无法利用视觉和上下文信息，限制了翻译质量。

Method: ViDove模仿人类翻译工作流，整合多模态记忆系统和长短时记忆模块，结合领域知识。

Result: 在字幕生成和通用翻译任务中，ViDove的BLEU分数提升28%，SubER提升15%。

Conclusion: ViDove在多模态翻译中表现优异，并推出了新的基准DoveBench。

Abstract: LLM-based translation agents have achieved highly human-like translation
results and are capable of handling longer and more complex contexts with
greater efficiency. However, they are typically limited to text-only inputs. In
this paper, we introduce ViDove, a translation agent system designed for
multimodal input. Inspired by the workflow of human translators, ViDove
leverages visual and contextual background information to enhance the
translation process. Additionally, we integrate a multimodal memory system and
long-short term memory modules enriched with domain-specific knowledge,
enabling the agent to perform more accurately and adaptively in real-world
scenarios. As a result, ViDove achieves significantly higher translation
quality in both subtitle generation and general translation tasks, with a 28%
improvement in BLEU scores and a 15% improvement in SubER compared to previous
state-of-the-art baselines. Moreover, we introduce DoveBench, a new benchmark
for long-form automatic video subtitling and translation, featuring 17 hours of
high-quality, human-annotated data. Our code is available here:
https://github.com/pigeonai-org/ViDove

</details>


### [32] [On the Impossibility of Separating Intelligence from Judgment: The Computational Intractability of Filtering for AI Alignment](https://arxiv.org/abs/2507.07341)
*Sarah Ball,Greg Gluch,Shafi Goldwasser,Frauke Kreuter,Omer Reingold,Guy N. Rothblum*

Main category: cs.AI

TL;DR: 论文研究了大型语言模型（LLM）在生成有害内容时的对齐挑战，重点探讨了输入提示和输出过滤的计算难题，并指出外部过滤器无法完全确保安全性。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的广泛应用，其可能被滥用于生成有害内容的问题引发关注，研究旨在探讨如何通过过滤机制解决这一对齐挑战。

Method: 研究分析了输入提示和输出过滤的计算可行性，通过密码学假设证明高效过滤的不可行性，并探讨了放松的缓解方法。

Result: 研究发现，存在无法高效过滤的对抗性提示，且输出过滤在特定情况下计算不可行，外部过滤器无法完全确保安全性。

Conclusion: 安全性的实现不能仅依赖外部过滤器，需将判断力与模型内部架构和权重紧密结合，智能与判断不可分割。

Abstract: With the increased deployment of large language models (LLMs), one concern is
their potential misuse for generating harmful content. Our work studies the
alignment challenge, with a focus on filters to prevent the generation of
unsafe information. Two natural points of intervention are the filtering of the
input prompt before it reaches the model, and filtering the output after
generation. Our main results demonstrate computational challenges in filtering
both prompts and outputs. First, we show that there exist LLMs for which there
are no efficient prompt filters: adversarial prompts that elicit harmful
behavior can be easily constructed, which are computationally indistinguishable
from benign prompts for any efficient filter. Our second main result identifies
a natural setting in which output filtering is computationally intractable. All
of our separation results are under cryptographic hardness assumptions. In
addition to these core findings, we also formalize and study relaxed mitigation
approaches, demonstrating further computational barriers. We conclude that
safety cannot be achieved by designing filters external to the LLM internals
(architecture and weights); in particular, black-box access to the LLM will not
suffice. Based on our technical results, we argue that an aligned AI system's
intelligence cannot be separated from its judgment.

</details>


### [33] [Supply Chain Optimization via Generative Simulation and Iterative Decision Policies](https://arxiv.org/abs/2507.07355)
*Haoyue Bai,Haoyu Wang,Nanxu Gong,Xinyuan Wang,Wangyang Ying,Haifeng Chen,Yanjie Fu*

Main category: cs.AI

TL;DR: Sim-to-Dec框架结合生成模拟和智能决策，提升供应链运输的响应速度和经济效率。


<details>
  <summary>Details</summary>
Motivation: 供应链运输中的高响应性和经济效率受运输模式决策影响，需一种可观察、低风险的策略设计环境。

Method: 提出Sim-to-Dec框架，包含生成模拟模块（自回归建模）和双感知决策模型（历史与未来结合），通过端到端优化迭代改进。

Result: 在三个真实数据集上实验表明，Sim-to-Dec显著提高了准时交付率和利润。

Conclusion: Sim-to-Dec满足通用性、细粒度动态、历史与预测结合及反馈-策略紧密集成的要求，有效优化运输策略。

Abstract: High responsiveness and economic efficiency are critical objectives in supply
chain transportation, both of which are influenced by strategic decisions on
shipping mode. An integrated framework combining an efficient simulator with an
intelligent decision-making algorithm can provide an observable, low-risk
environment for transportation strategy design. An ideal simulation-decision
framework must (1) generalize effectively across various settings, (2) reflect
fine-grained transportation dynamics, (3) integrate historical experience with
predictive insights, and (4) maintain tight integration between simulation
feedback and policy refinement. We propose Sim-to-Dec framework to satisfy
these requirements. Specifically, Sim-to-Dec consists of a generative
simulation module, which leverages autoregressive modeling to simulate
continuous state changes, reducing dependence on handcrafted domain-specific
rules and enhancing robustness against data fluctuations; and a history-future
dual-aware decision model, refined iteratively through end-to-end optimization
with simulator interactions. Extensive experiments conducted on three
real-world datasets demonstrate that Sim-to-Dec significantly improves timely
delivery rates and profit.

</details>


### [34] [DrugMCTS: a drug repurposing framework combining multi-agent, RAG and Monte Carlo Tree Search](https://arxiv.org/abs/2507.07426)
*Zerui Yang,Yuwei Wan,Yinqiao Li,Yudai Matsuda,Tong Xie,Linqi Song*

Main category: cs.AI

TL;DR: DrugMCTS框架结合RAG、多智能体协作和蒙特卡洛树搜索，显著提升药物重定位任务中LLM的性能，无需领域微调即可超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在科学领域（如药物发现）中推理能力受限的问题，避免传统方法的高计算开销或数据利用不足的缺陷。

Method: 提出DrugMCTS框架，整合RAG、多智能体和蒙特卡洛树搜索，通过五个专业智能体实现结构化迭代推理。

Result: 在DrugBank和KIBA数据集上，DrugMCTS显著提高了召回率和鲁棒性，性能超越Deepseek-R1超过20%。

Conclusion: 结构化推理、智能体协作和反馈驱动搜索机制对提升LLM在药物发现中的应用至关重要。

Abstract: Recent advances in large language models have demonstrated considerable
potential in scientific domains such as drug discovery. However, their
effectiveness remains constrained when reasoning extends beyond the knowledge
acquired during pretraining. Conventional approaches, such as fine-tuning or
retrieval-augmented generation, face limitations in either imposing high
computational overhead or failing to fully exploit structured scientific data.
To overcome these challenges, we propose DrugMCTS, a novel framework that
synergistically integrates RAG, multi-agent collaboration, and Monte Carlo Tree
Search for drug repurposing. The framework employs five specialized agents
tasked with retrieving and analyzing molecular and protein information, thereby
enabling structured and iterative reasoning. Without requiring domain-specific
fine-tuning, DrugMCTS empowers Qwen2.5-7B-Instruct to outperform Deepseek-R1 by
over 20\%. Extensive experiments on the DrugBank and KIBA datasets demonstrate
that DrugMCTS achieves substantially higher recall and robustness compared to
both general-purpose LLMs and deep learning baselines. Our results highlight
the importance of structured reasoning, agent-based collaboration, and
feedback-driven search mechanisms in advancing LLM applications for drug
discovery.

</details>


### [35] [StarDojo: Benchmarking Open-Ended Behaviors of Agentic Multimodal LLMs in Production-Living Simulations with Stardew Valley](https://arxiv.org/abs/2507.07445)
*Weihao Tan,Changjiu Jiang,Yu Duan,Mingcong Lei,Jiageng Li,Yitian Hong,Xinrun Wang,Bo An*

Main category: cs.AI

TL;DR: StarDojo是一个基于《星露谷物语》的新基准测试，用于评估AI代理在开放式的生产生活模拟中的表现，涵盖农业、手工艺、探索、战斗和社交互动五大领域。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试很少同时评估生产活动和社会互动能力，StarDojo旨在填补这一空白。

Method: StarDojo提供了1000个任务和100个代表性任务的子集，支持多操作系统和无键盘鼠标控制的统一接口。

Result: 现有最佳模型GPT-4.1的成功率仅为12.7%，主要受限于视觉理解、多模态推理和低级操作能力。

Conclusion: StarDojo旨在推动复杂生产生活环境中稳健开放式代理的研究。

Abstract: Autonomous agents navigating human society must master both production
activities and social interactions, yet existing benchmarks rarely evaluate
these skills simultaneously. To bridge this gap, we introduce StarDojo, a novel
benchmark based on Stardew Valley, designed to assess AI agents in open-ended
production-living simulations. In StarDojo, agents are tasked to perform
essential livelihood activities such as farming and crafting, while
simultaneously engaging in social interactions to establish relationships
within a vibrant community. StarDojo features 1,000 meticulously curated tasks
across five key domains: farming, crafting, exploration, combat, and social
interactions. Additionally, we provide a compact subset of 100 representative
tasks for efficient model evaluation. The benchmark offers a unified,
user-friendly interface that eliminates the need for keyboard and mouse
control, supports all major operating systems, and enables the parallel
execution of multiple environment instances, making it particularly well-suited
for evaluating the most capable foundation agents, powered by multimodal large
language models (MLLMs). Extensive evaluations of state-of-the-art MLLMs agents
demonstrate substantial limitations, with the best-performing model, GPT-4.1,
achieving only a 12.7% success rate, primarily due to challenges in visual
understanding, multimodal reasoning and low-level manipulation. As a
user-friendly environment and benchmark, StarDojo aims to facilitate further
research towards robust, open-ended agents in complex production-living
environments.

</details>


### [36] [Position: We Need An Algorithmic Understanding of Generative AI](https://arxiv.org/abs/2507.07544)
*Oliver Eberle,Thomas McGee,Hamza Giaffar,Taylor Webb,Ida Momennejad*

Main category: cs.AI

TL;DR: AlgEval框架旨在系统研究LLM学习的算法，揭示其底层计算原理，提供可解释性，并优化训练方法。


<details>
  <summary>Details</summary>
Motivation: 当前研究多关注性能提升，缺乏对LLM学习算法的理论和实证理解。

Method: 提出AlgEval框架，结合案例研究（搜索算法），通过电路级分析验证假设。

Result: 揭示了LLM的算法组成和推理机制，为可解释性和性能优化提供基础。

Conclusion: AlgEval为理解LLM的计算原理提供了系统方法，有望推动更高效的训练和架构设计。

Abstract: What algorithms do LLMs actually learn and use to solve problems? Studies
addressing this question are sparse, as research priorities are focused on
improving performance through scale, leaving a theoretical and empirical gap in
understanding emergent algorithms. This position paper proposes AlgEval: a
framework for systematic research into the algorithms that LLMs learn and use.
AlgEval aims to uncover algorithmic primitives, reflected in latent
representations, attention, and inference-time compute, and their algorithmic
composition to solve task-specific problems. We highlight potential
methodological paths and a case study toward this goal, focusing on emergent
search algorithms. Our case study illustrates both the formation of top-down
hypotheses about candidate algorithms, and bottom-up tests of these hypotheses
via circuit-level analysis of attention patterns and hidden states. The
rigorous, systematic evaluation of how LLMs actually solve tasks provides an
alternative to resource-intensive scaling, reorienting the field toward a
principled understanding of underlying computations. Such algorithmic
explanations offer a pathway to human-understandable interpretability, enabling
comprehension of the model's internal reasoning performance measures. This can
in turn lead to more sample-efficient methods for training and improving
performance, as well as novel architectures for end-to-end and multi-agent
systems.

</details>


### [37] [On Trustworthy Rule-Based Models and Explanations](https://arxiv.org/abs/2507.07576)
*Mohamed Siala,Jordi Planes,Joao Marques-Silva*

Main category: cs.AI

TL;DR: 论文探讨了机器学习中解释模型预测的重要性，特别是在高风险领域，并分析了基于规则的ML模型的负面特征及其影响。


<details>
  <summary>Details</summary>
Motivation: 在高风险领域，错误的解释可能误导人类决策者，因此需要严格的解释方法。尽管可解释性是一个模糊的概念，但基于规则的ML模型仍被广泛使用。

Method: 论文开发了算法来分析基于规则的ML模型的负面特征，如负面重叠和冗余。

Result: 研究发现，广泛使用的基于规则的工具会引发具有负面特征的规则集。

Conclusion: 论文强调了对基于规则的ML模型负面特征进行严格分析的必要性。

Abstract: A task of interest in machine learning (ML) is that of ascribing explanations
to the predictions made by ML models. Furthermore, in domains deemed high risk,
the rigor of explanations is paramount. Indeed, incorrect explanations can and
will mislead human decision makers. As a result, and even if interpretability
is acknowledged as an elusive concept, so-called interpretable models are
employed ubiquitously in high-risk uses of ML and data mining (DM). This is the
case for rule-based ML models, which encompass decision trees, diagrams, sets
and lists. This paper relates explanations with well-known undesired facets of
rule-based ML models, which include negative overlap and several forms of
redundancy. The paper develops algorithms for the analysis of these undesired
facets of rule-based systems, and concludes that well-known and widely used
tools for learning rule-based ML models will induce rule sets that exhibit one
or more negative facets.

</details>


### [38] [Context Pooling: Query-specific Graph Pooling for Generic Inductive Link Prediction in Knowledge Graphs](https://arxiv.org/abs/2507.07595)
*Zhixiang Su,Di Wang,Chunyan Miao*

Main category: cs.AI

TL;DR: 论文提出了一种名为Context Pooling的新方法，用于提升GNN在知识图谱链接预测中的性能，并在多个数据集上取得了最佳效果。


<details>
  <summary>Details</summary>
Motivation: 现有的GNN模型在知识图谱链接预测中，普通聚合方法对性能影响有限，因此需要更有效的方法。

Method: 提出Context Pooling方法，首次在知识图谱中应用图池化，并设计两种指标（邻域精度和邻域召回）筛选逻辑相关邻居。

Result: 在三个公开数据集上应用于两种SOTA模型，42/48的设置中达到最佳性能。

Conclusion: Context Pooling是一种通用且高效的方法，显著提升了GNN在知识图谱链接预测中的表现。

Abstract: Recent investigations on the effectiveness of Graph Neural Network
(GNN)-based models for link prediction in Knowledge Graphs (KGs) show that
vanilla aggregation does not significantly impact the model performance. In
this paper, we introduce a novel method, named Context Pooling, to enhance
GNN-based models' efficacy for link predictions in KGs. To our best of
knowledge, Context Pooling is the first methodology that applies graph pooling
in KGs. Additionally, Context Pooling is first-of-its-kind to enable the
generation of query-specific graphs for inductive settings, where testing
entities are unseen during training. Specifically, we devise two metrics,
namely neighborhood precision and neighborhood recall, to assess the neighbors'
logical relevance regarding the given queries, thereby enabling the subsequent
comprehensive identification of only the logically relevant neighbors for link
prediction. Our method is generic and assessed by being applied to two
state-of-the-art (SOTA) models on three public transductive and inductive
datasets, achieving SOTA performance in 42 out of 48 settings.

</details>


### [39] [Enhancing Vaccine Safety Surveillance: Extracting Vaccine Mentions from Emergency Department Triage Notes Using Fine-Tuned Large Language Models](https://arxiv.org/abs/2507.07599)
*Sedigh Khademi,Jim Black,Christopher Palmer,Muhammad Javed,Hazel Clothier,Jim Buttery,Gerardo Luis Dimaguila*

Main category: cs.AI

TL;DR: 研究评估了微调的Llama 3.2模型从急诊分诊记录中提取疫苗相关信息的能力，用于实时疫苗安全监测。微调模型在提取疫苗名称的准确性上优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 支持高效的疫苗安全监测和早期发现免疫后不良事件。

Method: 使用提示工程创建标注数据集，比较提示工程模型、微调模型和基于规则的方法。

Result: 微调的Llama 3模型在提取疫苗名称的准确性上表现最佳，模型量化实现了资源受限环境的高效部署。

Conclusion: 大语言模型在自动化数据提取方面具有潜力，可支持疫苗安全监测。

Abstract: This study evaluates fine-tuned Llama 3.2 models for extracting
vaccine-related information from emergency department triage notes to support
near real-time vaccine safety surveillance. Prompt engineering was used to
initially create a labeled dataset, which was then confirmed by human
annotators. The performance of prompt-engineered models, fine-tuned models, and
a rule-based approach was compared. The fine-tuned Llama 3 billion parameter
model outperformed other models in its accuracy of extracting vaccine names.
Model quantization enabled efficient deployment in resource-constrained
environments. Findings demonstrate the potential of large language models in
automating data extraction from emergency department notes, supporting
efficient vaccine safety surveillance and early detection of emerging adverse
events following immunization issues.

</details>


### [40] [Towards conservative inference in credal networks using belief functions: the case of credal chains](https://arxiv.org/abs/2507.07619)
*Marco Sangalli,Thomas Krak,Cassio de Campos*

Main category: cs.AI

TL;DR: 本文提出了一种基于Dempster-Shafer理论的新框架，用于在信用网络（尤其是链状结构）中传播不确定性，并通过置信度和似然函数高效生成保守区间。


<details>
  <summary>Details</summary>
Motivation: 探索如何在信用网络中更高效地进行不确定性推理，结合计算速度和鲁棒的表示方法。

Method: 提出了一种基于Dempster-Shafer理论的框架，用于链状信用网络中的不确定性传播，并比较了置信度推理与经典敏感性分析。

Result: 数值结果展示了该框架的优势和局限性，为链状及一般信用网络的实用价值提供了见解。

Conclusion: 该框架在链状信用网络中表现出高效性和鲁棒性，但其适用范围和局限性仍需进一步研究。

Abstract: This paper explores belief inference in credal networks using Dempster-Shafer
theory. By building on previous work, we propose a novel framework for
propagating uncertainty through a subclass of credal networks, namely chains.
The proposed approach efficiently yields conservative intervals through belief
and plausibility functions, combining computational speed with robust
uncertainty representation. Key contributions include formalizing belief-based
inference methods and comparing belief-based inference against classical
sensitivity analysis. Numerical results highlight the advantages and
limitations of applying belief inference within this framework, providing
insights into its practical utility for chains and for credal networks in
general.

</details>


### [41] [PlanQA: A Benchmark for Spatial Reasoning in LLMs using Structured Representations](https://arxiv.org/abs/2507.07644)
*Fedor Rodionov,Abdelrahman Eldesokey,Michael Birsak,John Femiani,Bernard Ghanem,Peter Wonka*

Main category: cs.AI

TL;DR: PlanQA是一个用于评估大型语言模型（LLMs）几何和空间推理能力的诊断基准，基于室内场景的结构化表示，揭示了LLMs在真实世界布局推理中的盲点。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在几何和空间推理方面存在不足，尤其是在模拟物理约束、保持空间一致性和布局扰动下的泛化能力上。PlanQA旨在填补这一空白，推动语言模型在空间推理方面的进步。

Method: PlanQA基于结构化场景表示（如JSON、XML布局），设计了多样的问题类型，测试度量、拓扑推理及室内设计约束（如可达性、平衡性）。

Result: 实验表明，LLMs在浅层查询中可能成功，但在模拟物理约束、空间一致性和布局扰动下的泛化能力上表现不佳。

Conclusion: PlanQA揭示了LLMs在真实世界布局推理中的盲点，为未来研究提供了方向，希望推动更准确的空间推理语言模型的发展。

Abstract: We introduce PlanQA, a diagnostic benchmark for evaluating geometric and
spatial reasoning in large-language models (LLMs). PlanQA is grounded in
structured representations of indoor scenes, such as kitchens, living rooms,
and bedrooms, encoded in a symbolic format (e.g., JSON, XML layouts). The
benchmark includes diverse question types that test not only metric and
topological reasoning (e.g., distance, visibility, shortest paths) but also
interior design constraints such as affordance, clearance, balance, and
usability. Our results across a variety of frontier open-source and commercial
LLMs show that while models may succeed in shallow queries, they often fail to
simulate physical constraints, preserve spatial coherence, or generalize under
layout perturbation. PlanQA uncovers a clear blind spot in today's LLMs: they
do not consistently reason about real-world layouts. We hope that this
benchmark inspires new work on language models that can accurately infer and
manipulate spatial and geometric properties in practical settings.

</details>


### [42] [Stable Preference Optimization for LLMs: A Bilevel Approach Beyond Direct Preference Optimization](https://arxiv.org/abs/2507.07723)
*Chengtao Jian,Kai Yang,Ye Ouyang,Xiaozhou Ye*

Main category: cs.AI

TL;DR: 论文分析了直接偏好优化（DPO）的理论局限性，提出了一种稳定的双层优化框架，以改进模型对齐的稳定性和一致性。


<details>
  <summary>Details</summary>
Motivation: 尽管DPO在语言模型对齐中表现出色，但其理论性质和内在限制尚未充分研究。论文旨在揭示DPO的敏感性和概率分配问题，并提出改进方法。

Method: 通过概率演化视角分析DPO的动态特性，提出双层优化框架（稳定偏好优化），结合监督微调和正则化方案。

Result: 实验表明，该方法在推理和摘要任务中优于标准DPO，提高了推理准确性并更好地对齐输出分布。

Conclusion: 稳定偏好优化为偏好对齐目标的设计提供了新思路，推动了更可靠和可解释的语言模型对齐。

Abstract: Direct Preference Optimization (DPO) has emerged as a popular and efficient
alternative to reward modeling and reinforcement learning for aligning language
models with human preferences. Despite its empirical success, the theoretical
properties and intrinsic limitations of DPO remain underexplored. In this work,
we first present a comprehensive analysis of DPO's dynamics from a probability
evolution perspective. Our analysis reveals that DPO is highly sensitive to
initialization. It also tends to misallocate probability mass, which can
inadvertently shift probability toward irrelevant or undesired responses. This
misallocation may unintentionally reinforce model bias, thereby compromising
both the stability of model alignment and the consistency with intended
preferences. Motivated by these theoretical findings, we propose a
theoretically grounded bilevel optimization framework that tightly integrate
supervised fine-tuning with an enhanced DPO objective a.k.a. stable preference
optimization. Our approach introduces a principled regularization scheme to
explicitly encourage absolute probability improvement for preferred outputs,
while maintaining stable optimization dynamics. Experiments on challenging
reasoning and summarization benchmarks elucidate that our method consistently
improves reasoning accuracy and better aligns output distributions with
intended preferences, outperforming standard DPO. Stable preference
optimization provides new insights into the design of preference-based
alignment objectives and opens up new avenues towards more reliable and
interpretable language model alignment.

</details>


### [43] [Identification of Violin Reduction via Contour Lines Classification](https://arxiv.org/abs/2507.07743)
*Philémon Beghin,Anne-Emmanuelle Ceulemans,François Glineur*

Main category: cs.AI

TL;DR: 该论文提出了一种基于轮廓线分类小提琴是否为缩小尺寸的方法，通过3D几何网格和抛物线拟合参数分析，发现区分缩小与非缩小乐器是可行的。


<details>
  <summary>Details</summary>
Motivation: 研究小提琴制作中的尺寸缩小现象，尤其是轮廓线的变化，填补了专家观察但未定量研究的空白。

Method: 使用25把小提琴的3D几何网格数据，提取10-20条轮廓线，拟合抛物线曲线参数（α和β），并通过回归和阈值计算特征。

Result: 发现基于几何形状可以一定程度上区分缩小与非缩小乐器，其中参数β最具预测性。

Conclusion: 轮廓线分析为小提琴尺寸缩小提供了定量研究方法，但存在部分乐器难以明确分类的局限性。

Abstract: The first violins appeared in late 16th-century Italy. Over the next 200
years, they spread across Europe and luthiers of various royal courts, eager to
experiment with new techniques, created a highly diverse family of instruments.
Around 1750, size standards were introduced to unify violin making for
orchestras and conservatories. Instruments that fell between two standards were
then reduced to a smaller size by luthiers. These reductions have an impact on
several characteristics of violins, in particular on the contour lines, i.e.
lines of constant altitude, which look more like a U for non reduced
instruments and a V for reduced ones. While such differences are observed by
experts, they have not been studied quantitatively.
  This paper presents a method for classifying violins as reduced or
non-reduced based on their contour lines. We study a corpus of 25 instruments
whose 3D geometric meshes were acquired via photogrammetry. For each
instrument, we extract 10-20 contour lines regularly spaced every millimetre.
Each line is fitted with a parabola-like curve (with an equation of the type y
= alpha*abs(x)**beta) depending on two parameters, describing how open (beta)
and how vertically stretched (alpha) the curve is. We compute additional
features from those parameters, using regressions and counting how many values
fall under some threshold. We also deal with outliers and non equal numbers of
levels, and eventually obtain a numerical profile for each instrument.
  We then apply classification methods to assess whether geometry alone can
predict size reduction. We find that distinguishing between reduced and non
reduced instruments is feasible to some degree, taking into account that a
whole spectrum of more or less transformed violins exists, for which it is more
difficult to quantify the reduction. We also find the opening parameter beta to
be the most predictive.

</details>


### [44] [Measuring AI Alignment with Human Flourishing](https://arxiv.org/abs/2507.07787)
*Elizabeth Hilliard,Akshaya Jagadeesh,Alex Cook,Steele Billings,Nicholas Skytland,Alicia Llewellyn,Jackson Paull,Nathan Paull,Nolan Kurylo,Keatra Nesbitt,Robert Gruenewald,Anthony Jantzi,Omar Chavez*

Main category: cs.AI

TL;DR: FAI Benchmark评估AI对人类繁荣的贡献，涵盖七个维度，发现现有模型在多个维度上表现不足。


<details>
  <summary>Details</summary>
Motivation: 传统AI评估关注技术能力或避免危害，而FAI Benchmark旨在衡量AI对人类全面繁荣的贡献。

Method: 通过1,229个主客观问题，结合专家LLM和几何平均评分，评估28个语言模型。

Result: 最高分模型仅72/100，尤其在信仰、品德和意义等维度表现不佳。

Conclusion: FAI Benchmark为开发支持人类繁荣的AI提供了框架，对AI伦理和评估有重要意义。

Abstract: This paper introduces the Flourishing AI Benchmark (FAI Benchmark), a novel
evaluation framework that assesses AI alignment with human flourishing across
seven dimensions: Character and Virtue, Close Social Relationships, Happiness
and Life Satisfaction, Meaning and Purpose, Mental and Physical Health,
Financial and Material Stability, and Faith and Spirituality. Unlike
traditional benchmarks that focus on technical capabilities or harm prevention,
the FAI Benchmark measures AI performance on how effectively models contribute
to the flourishing of a person across these dimensions. The benchmark evaluates
how effectively LLM AI systems align with current research models of holistic
human well-being through a comprehensive methodology that incorporates 1,229
objective and subjective questions. Using specialized judge Large Language
Models (LLMs) and cross-dimensional evaluation, the FAI Benchmark employs
geometric mean scoring to ensure balanced performance across all flourishing
dimensions. Initial testing of 28 leading language models reveals that while
some models approach holistic alignment (with the highest-scoring models
achieving 72/100), none are acceptably aligned across all dimensions,
particularly in Faith and Spirituality, Character and Virtue, and Meaning and
Purpose. This research establishes a framework for developing AI systems that
actively support human flourishing rather than merely avoiding harm, offering
significant implications for AI development, ethics, and evaluation.

</details>


### [45] [MoSE: Skill-by-Skill Mixture-of-Expert Learning for Autonomous Driving](https://arxiv.org/abs/2507.07818)
*Lu Xu,Jiaqian Yu,Xiongfeng Peng,Yiwei Chen,Weiming Li,Jaewook Yoo,Sunghyun Chunag,Dongwook Lee,Daehyun Ji,Chao Zhang*

Main category: cs.AI

TL;DR: 论文提出了一种技能导向的混合专家模型（MoSE），模仿人类驾驶员的学习和推理过程，通过技能导向的路由机制和分层技能数据集，实现了高效的单次前向推理，性能优于更大参数量的模型。


<details>
  <summary>Details</summary>
Motivation: 现有混合专家模型（MoE）需要大量训练数据和复杂优化，而人类驾驶员的学习过程更高效。受此启发，研究旨在开发一种技能导向的MoE模型，提升自动驾驶系统的泛化能力和解释性。

Method: 提出MoSE模型，通过技能导向路由机制和分层技能数据集，模仿人类驾驶员的技能学习和多步推理过程，整合辅助任务（如描述、推理、规划）于单次前向过程中。

Result: MoSE在CODA AD角例推理任务中优于多个8B+参数模型，激活参数量减少至少62.5%，性能达到SOTA。

Conclusion: MoSE通过模仿人类学习过程，实现了高效且高性能的自动驾驶推理，为轻量化模型设计提供了新思路。

Abstract: Recent studies show large language models (LLMs) and vision language models
(VLMs) trained using web-scale data can empower end-to-end autonomous driving
systems for a better generalization and interpretation. Specifically, by
dynamically routing inputs to specialized subsets of parameters, the
Mixture-of-Experts (MoE) technique enables general LLMs or VLMs to achieve
substantial performance improvements while maintaining computational
efficiency. However, general MoE models usually demands extensive training data
and complex optimization. In this work, inspired by the learning process of
human drivers, we propose a skill-oriented MoE, called MoSE, which mimics human
drivers' learning process and reasoning process, skill-by-skill and
step-by-step. We propose a skill-oriented routing mechanism that begins with
defining and annotating specific skills, enabling experts to identify the
necessary driving competencies for various scenarios and reasoning tasks,
thereby facilitating skill-by-skill learning. Further align the driving process
to multi-step planning in human reasoning and end-to-end driving models, we
build a hierarchical skill dataset and pretrain the router to encourage the
model to think step-by-step. Unlike multi-round dialogs, MoSE integrates
valuable auxiliary tasks (e.g.\ description, reasoning, planning) in one single
forward process without introducing any extra computational cost. With less
than 3B sparsely activated parameters, our model outperforms several 8B+
parameters on CODA AD corner case reasoning task. Compared to existing methods
based on open-source models and data, our approach achieves state-of-the-art
performance with significantly reduced activated model size (at least by
$62.5\%$) with a single-turn conversation.

</details>


### [46] [AI Should Sense Better, Not Just Scale Bigger: Adaptive Sensing as a Paradigm Shift](https://arxiv.org/abs/2507.07820)
*Eunsu Baek,Keondo Park,Jeonggil Ko,Min-hwan Oh,Taesik Gong,Hyung-Sin Kim*

Main category: cs.AI

TL;DR: 论文提出自适应感知作为AI发展的新范式，通过动态调整传感器参数提高效率，减少环境和经济成本，并展示小模型超越大模型的潜力。


<details>
  <summary>Details</summary>
Motivation: 当前AI依赖大规模模型和数据集，导致环境、经济和伦理成本高昂，难以持续和公平。受生物感官系统启发，提出自适应感知以解决这些问题。

Method: 通过动态调整传感器参数（如曝光、灵敏度、多模态配置）来主动应对输入变化，提高模型效率和鲁棒性。

Result: 实证研究表明，自适应感知使小模型（如EfficientNet-B0）超越更大模型（如OpenCLIP-H），同时减少数据和计算需求。

Conclusion: 论文提出自适应感知的应用路线图，强调技术挑战和伦理问题，并呼吁研究标准化基准、实时算法和多模态集成，以推动可持续、鲁棒和公平的AI系统。

Abstract: Current AI advances largely rely on scaling neural models and expanding
training datasets to achieve generalization and robustness. Despite notable
successes, this paradigm incurs significant environmental, economic, and
ethical costs, limiting sustainability and equitable access. Inspired by
biological sensory systems, where adaptation occurs dynamically at the input
(e.g., adjusting pupil size, refocusing vision)--we advocate for adaptive
sensing as a necessary and foundational shift. Adaptive sensing proactively
modulates sensor parameters (e.g., exposure, sensitivity, multimodal
configurations) at the input level, significantly mitigating covariate shifts
and improving efficiency. Empirical evidence from recent studies demonstrates
that adaptive sensing enables small models (e.g., EfficientNet-B0) to surpass
substantially larger models (e.g., OpenCLIP-H) trained with significantly more
data and compute. We (i) outline a roadmap for broadly integrating adaptive
sensing into real-world applications spanning humanoid, healthcare, autonomous
systems, agriculture, and environmental monitoring, (ii) critically assess
technical and ethical integration challenges, and (iii) propose targeted
research directions, such as standardized benchmarks, real-time adaptive
algorithms, multimodal integration, and privacy-preserving methods.
Collectively, these efforts aim to transition the AI community toward
sustainable, robust, and equitable artificial intelligence systems.

</details>


### [47] [Searching for actual causes: Approximate algorithms with adjustable precision](https://arxiv.org/abs/2507.07857)
*Samuel Reyd,Ada Diaconescu,Jean-Louis Dessalles*

Main category: cs.AI

TL;DR: 论文提出了一种多项式复杂度的算法，用于识别实际原因，解决了现有方法无法处理的非布尔、黑盒和随机系统问题。


<details>
  <summary>Details</summary>
Motivation: 现有可解释人工智能（XAI）和因果性研究未能满足非专家用户对解释的需求，即识别实际原因。这是一个尚未解决的开放性问题，且现有方法难以处理复杂系统。

Method: 提出了一组多项式复杂度的算法，可调整精度和全面性，适用于非布尔、黑盒和随机系统。

Result: 实验表明，算法能识别现有方法无法处理的系统原因，且可通过增加计算时间提高精度和全面性。

Conclusion: 该算法为解决实际原因识别问题提供了实用且灵活的解决方案。

Abstract: Causality has gained popularity in recent years. It has helped improve the
performance, reliability, and interpretability of machine learning models.
However, recent literature on explainable artificial intelligence (XAI) has
faced criticism. The classical XAI and causality literature focuses on
understanding which factors contribute to which consequences. While such
knowledge is valuable for researchers and engineers, it is not what non-expert
users expect as explanations. Instead, these users often await facts that cause
the target consequences, i.e., actual causes. Formalizing this notion is still
an open problem. Additionally, identifying actual causes is reportedly an
NP-complete problem, and there are too few practical solutions to approximate
formal definitions. We propose a set of algorithms to identify actual causes
with a polynomial complexity and an adjustable level of precision and
exhaustiveness. Our experiments indicate that the algorithms (1) identify
causes for different categories of systems that are not handled by existing
approaches (i.e., non-boolean, black-box, and stochastic systems), (2) can be
adjusted to gain more precision and exhaustiveness with more computation time.

</details>


### [48] [An Integrated Framework of Prompt Engineering and Multidimensional Knowledge Graphs for Legal Dispute Analysis](https://arxiv.org/abs/2507.07893)
*Mingda Zhang,Na Zhao,Jianglong Qing,Qing xu,Kaiwen Pan,Ting luo*

Main category: cs.AI

TL;DR: 本文提出了一种结合提示工程和多维知识图谱的增强框架，用于提升大型语言模型在法律纠纷分析中的表现。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在法律纠纷分析中存在法律知识表示不足、概念理解有限和推理缺陷等问题，亟需改进。

Method: 框架采用三阶段分层提示结构（任务定义、知识背景、推理指导）和三层次知识图谱架构（分类本体、表示层、实例层），结合四种法律概念检索方法。

Result: 实验结果显示，该框架显著提升了法律纠纷分析的性能，能够准确分析复杂案件的法律适用。

Conclusion: 该研究为智能法律辅助系统的实现提供了新的技术途径。

Abstract: The rapid development of artificial intelligence has positioned large
language models as fundamental components of intelligent legal systems.
However, these models face significant limitations in legal dispute analysis,
including insufficient legal knowledge representation, limited concept
understanding, and reasoning deficiencies. This research proposes an enhanced
framework integrating prompt engineering with multidimensional knowledge
graphs. The framework introduces a three-stage hierarchical prompt structure
comprising task definition, knowledge background, and reasoning guidance,
supplemented by legal-specific reasoning templates and dynamic optimization
mechanisms. A three-layer knowledge graph architecture is constructed with
legal classification ontology, representation, and instance layers. Four
complementary methods enable precise legal concept retrieval: direct legal norm
code matching, domain-specific semantic vector similarity, ontology-based path
reasoning, and specialized lexical segmentation. These components integrate
with web search technology to establish a knowledge-enhanced framework for
legal decision-making. Experimental results demonstrate significant performance
improvements in legal dispute analysis, enabling accurate legal application
analysis for complex cases while exhibiting nuanced understanding of judicial
decision-making logic, providing a novel technical approach for implementing
intelligent legal assistance systems.

</details>


### [49] [Meek Models Shall Inherit the Earth](https://arxiv.org/abs/2507.07931)
*Hans Gundlach,Jayson Lynch,Neil Thompson*

Main category: cs.AI

TL;DR: 论文认为，随着计算规模扩大带来的边际收益递减，AI模型性能将趋于收敛，即使是资源有限的小模型也能接近最佳模型的性能。


<details>
  <summary>Details</summary>
Motivation: 探讨AI模型性能不平等问题，并挑战主流观点，认为计算资源扩展的边际收益递减将导致性能趋同。

Method: 开发模型分析固定分布下计算资源扩展的边际收益递减，并结合实证数据和理论模型验证。

Result: 研究表明，计算资源扩展的边际收益显著下降，小模型性能将接近最佳模型。

Conclusion: AI战略和政策需重新审视，以适应小模型性能提升的趋势。

Abstract: The past decade has seen incredible scaling of AI systems by a few companies,
leading to inequality in AI model performance. This paper argues that, contrary
to prevailing intuition, the diminishing returns to compute scaling will lead
to a convergence of AI model capabilities. In other words, meek models (those
with limited computation budget) shall inherit the earth, approaching the
performance level of the best models overall. We develop a model illustrating
that under a fixed-distribution next-token objective, the marginal capability
returns to raw compute shrink substantially. Given current scaling practices,
we argue that these diminishing returns are strong enough that even companies
that can scale their models exponentially faster than other organizations will
eventually have little advantage in capabilities. As part of our argument, we
give several reasons that proxies like training loss differences capture
important capability measures using evidence from benchmark data and
theoretical performance models. In addition, we analyze empirical data on the
capability difference of AI models over time. Finally, in light of the
increasing ability of meek models, we argue that AI strategy and policy require
reexamination, and we outline the areas this shift will affect.

</details>


### [50] [Working with AI: Measuring the Occupational Implications of Generative AI](https://arxiv.org/abs/2507.07935)
*Kiran Tomlinson,Sonia Jaffe,Will Wang,Scott Counts,Siddharth Suri*

Main category: cs.AI

TL;DR: 研究分析了生成式AI对经济的影响，通过用户与Microsoft Bing Copilot的对话数据，发现AI主要协助信息收集和写作，适用性最高的职业是知识工作者和销售。


<details>
  <summary>Details</summary>
Motivation: 理解AI对经济的广泛影响是社会的关键问题。

Method: 分析20万条用户与Microsoft Bing Copilot的匿名对话数据，结合职业活动分类和任务成功度，计算职业的AI适用性评分。

Result: AI适用性最高的职业包括计算机、数学、办公室行政支持和销售等知识工作者。

Conclusion: 研究为AI对职业的影响提供了实证依据，显示知识工作者和销售职业最易受AI影响。

Abstract: Given the rapid adoption of generative AI and its potential to impact a wide
range of tasks, understanding the effects of AI on the economy is one of
society's most important questions. In this work, we take a step toward that
goal by analyzing the work activities people do with AI, how successfully and
broadly those activities are done, and combine that with data on what
occupations do those activities. We analyze a dataset of 200k anonymized and
privacy-scrubbed conversations between users and Microsoft Bing Copilot, a
publicly available generative AI system. We find the most common work
activities people seek AI assistance for involve gathering information and
writing, while the most common activities that AI itself is performing are
providing information and assistance, writing, teaching, and advising.
Combining these activity classifications with measurements of task success and
scope of impact, we compute an AI applicability score for each occupation. We
find the highest AI applicability scores for knowledge work occupation groups
such as computer and mathematical, and office and administrative support, as
well as occupations such as sales whose work activities involve providing and
communicating information. Additionally, we characterize the types of work
activities performed most successfully, how wage and education correlate with
AI applicability, and how real-world usage compares to predictions of
occupational AI impact.

</details>
