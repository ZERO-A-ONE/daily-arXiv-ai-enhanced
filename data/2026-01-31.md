<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 16]
- [cs.SE](#cs.SE) [Total: 20]
- [cs.AI](#cs.AI) [Total: 68]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [What Hard Tokens Reveal: Exploiting Low-confidence Tokens for Membership Inference Attacks against Large Language Models](https://arxiv.org/abs/2601.20885)
*Md Tasnim Jawad,Mingyan Xiao,Yanzhao Wu*

Main category: cs.CR

TL;DR: HT-MIA：一种针对大语言模型的新型成员推理攻击方法，通过分析低置信度（困难）token级别的概率改进来检测训练数据中的成员样本，相比现有方法显著提升了攻击效果。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的广泛应用和隐私法规日益严格，保护LLM中的数据隐私变得至关重要。现有的成员推理攻击方法主要依赖序列级别的聚合预测统计，无法区分泛化改进和记忆改进，导致攻击效果不佳。

Method: 提出HT-MIA方法，通过比较微调目标模型和预训练参考模型在低置信度（困难）token上的token级别概率改进，来捕捉更明显的成员信号。该方法专注于hard token，因为在这些token上成员信号更加显著。

Result: 在领域特定的医疗数据集和通用基准测试上的广泛实验表明，HT-MIA在攻击效果上持续优于七种最先进的MIA基线方法。同时研究了差分隐私训练作为对抗MIA的有效防御机制。

Conclusion: HT-MIA框架建立了基于困难token分析的先进基础，为推进大语言模型的成员推理攻击和防御提供了新的方向。该方法能够有效区分记忆和泛化，显著提升成员推理的准确性。

Abstract: With the widespread adoption of Large Language Models (LLMs) and increasingly stringent privacy regulations, protecting data privacy in LLMs has become essential, especially for privacy-sensitive applications. Membership Inference Attacks (MIAs) attempt to determine whether a specific data sample was included in the model training/fine-tuning dataset, posing serious privacy risks. However, most existing MIA techniques against LLMs rely on sequence-level aggregated prediction statistics, which fail to distinguish prediction improvements caused by generalization from those caused by memorization, leading to low attack effectiveness. To address this limitation, we propose a novel membership inference approach that captures the token-level probabilities for low-confidence (hard) tokens, where membership signals are more pronounced. By comparing token-level probability improvements at hard tokens between a fine-tuned target model and a pre-trained reference model, HT-MIA isolates strong and robust membership signals that are obscured by prior MIA approaches. Extensive experiments on both domain-specific medical datasets and general-purpose benchmarks demonstrate that HT-MIA consistently outperforms seven state-of-the-art MIA baselines. We further investigate differentially private training as an effective defense mechanism against MIAs in LLMs. Overall, our HT-MIA framework establishes hard-token based analysis as a state-of-the-art foundation for advancing membership inference attacks and defenses for LLMs.

</details>


### [2] [Towards Sensitivity-Aware Language Models](https://arxiv.org/abs/2601.20901)
*Dren Fazlija,Iyiola E. Olatunji,Daniel Kudenko,Sandipan Sikdar*

Main category: cs.CR

TL;DR: 该研究将企业数据管理中的敏感度感知概念形式化，建立其与差分隐私的理论联系，并提出一种监督微调方法，使4位量化LLM在保持其他任务性能的同时显著提升敏感度感知能力。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在企业数据管理中的部署增加，确保模型不泄露敏感信息变得至关重要。虽然引入了敏感度感知概念使LLM能遵守预设访问权限规则，但其与差分隐私等隐私概念的关系尚不明确，难以在实际应用中有效部署。

Method: 1. 形式化敏感度感知概念；2. 理论建立敏感度感知与差分隐私的联系；3. 开发监督微调方法，使现有的4位量化LLM更具敏感度感知能力。

Result: 微调后的LLM性能提升高达21.7%，不仅显著超越基线模型，还在敏感度感知方面优于类似规模的全精度开源和商业模型。同时，该方法在很大程度上保留了模型在其他任务（如通用指令遵循、数学和常识推理）上的性能。

Conclusion: 该研究成功建立了敏感度感知与差分隐私的理论联系，并提出有效的微调方法，使量化LLM在保持通用能力的同时显著提升敏感度感知能力，为企业数据管理中的LLM安全部署提供了实用解决方案。

Abstract: With LLMs increasingly deployed in corporate data management, it is crucial to ensure that these models do not leak sensitive information. In the context of corporate data management, the concept of sensitivity awareness has been introduced, enabling LLMs to adhere to predefined access rights rules. However, it remains unclear how sensitivity awareness relates to established notions of privacy, such as differential privacy (DP), thereby making it difficult to deploy meaningfully in real-world applications. In this work, we formalize the notion of sensitivity awareness and theoretically establish its connection to DP. Additionally, we develop a supervised fine-tuning recipe to make existing, four-bit quantized LLMs more sensitivity-aware. With a performance boost of up to 21.7%, the finetuned LLMs not only substantially improve over their baseline but also outperform other full-precision open-source and commercial models of similar size in achieving sensitivity awareness, demonstrating the effectiveness of our proposed approach. At the same time, our method also largely preserves the models' performance on other tasks, such as general instruction-following, mathematical, and common-sense reasoning.

</details>


### [3] [ICON: Intent-Context Coupling for Efficient Multi-Turn Jailbreak Attack](https://arxiv.org/abs/2601.20903)
*Xingwei Lin,Wenhao Lin,Sicong Cao,Jiahao Yu,Renke Huang,Lei Xue,Chunming Wu*

Main category: cs.CR

TL;DR: ICON是一个自动化多轮越狱框架，通过意图-上下文耦合现象和分层优化策略，高效构建权威风格上下文来绕过LLM安全机制，在8个SOTA LLM上达到97.1%的平均攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 现有多轮越狱攻击方法存在效率低下和容易陷入次优区域的问题，需要逐步构建对抗性上下文且优化停留在表面层次。研究者发现意图-上下文耦合现象，即当恶意意图与语义一致的上下文模式耦合时，LLM的安全约束会显著放松。

Method: 提出ICON框架：1）通过先验引导的语义路由将恶意意图路由到一致的上下文模式（如科学研究）；2）将模式实例化为攻击提示序列，逐步构建权威风格上下文；3）采用分层优化策略，结合局部提示细化和全局上下文切换，避免攻击陷入无效上下文。

Result: 在8个最先进的LLM上进行实验，ICON实现了97.1%的平均攻击成功率（ASR），达到最先进水平。该方法显著优于现有越狱攻击方法。

Conclusion: ICON通过利用意图-上下文耦合现象和分层优化策略，有效解决了多轮越狱攻击的效率低下和优化停滞问题，为LLM安全研究提供了新的视角和有效的攻击框架。

Abstract: Multi-turn jailbreak attacks have emerged as a critical threat to Large Language Models (LLMs), bypassing safety mechanisms by progressively constructing adversarial contexts from scratch and incrementally refining prompts. However, existing methods suffer from the inefficiency of incremental context construction that requires step-by-step LLM interaction, and often stagnate in suboptimal regions due to surface-level optimization. In this paper, we characterize the Intent-Context Coupling phenomenon, revealing that LLM safety constraints are significantly relaxed when a malicious intent is coupled with a semantically congruent context pattern. Driven by this insight, we propose ICON, an automated multi-turn jailbreak framework that efficiently constructs an authoritative-style context via prior-guided semantic routing. Specifically, ICON first routes the malicious intent to a congruent context pattern (e.g., Scientific Research) and instantiates it into an attack prompt sequence. This sequence progressively builds the authoritative-style context and ultimately elicits prohibited content. In addition, ICON incorporates a Hierarchical Optimization Strategy that combines local prompt refinement with global context switching, preventing the attack from stagnating in ineffective contexts. Experimental results across eight SOTA LLMs demonstrate the effectiveness of ICON, achieving a state-of-the-art average Attack Success Rate (ASR) of 97.1\%. Code is available at https://github.com/xwlin-roy/ICON.

</details>


### [4] [Robust Federated Learning for Malicious Clients using Loss Trend Deviation Detection](https://arxiv.org/abs/2601.20915)
*Deepthy K Bhaskar,Minimol B,Binu V P*

Main category: cs.CR

TL;DR: 提出FL-LTD框架，通过监控损失动态而非梯度来检测恶意客户端，在非IID数据下有效防御损失操纵攻击，显著提升模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习系统面临恶意客户端发送误导性更新的风险，现有防御机制依赖梯度检查、复杂相似性计算或加密操作，在非IID数据下引入额外开销且不稳定。

Method: 提出联邦学习损失趋势检测(FL-LTD)框架，通过监控时间损失动态检测异常客户端，识别损失停滞或突变，结合短期记忆机制持续缓解先前标记的异常客户端，同时允许稳定参与者恢复信任。

Result: 在非IID联邦MNIST设置下对抗损失操纵攻击，FL-LTD显著增强鲁棒性，最终测试准确率达到0.84，而标准FedAvg在攻击下仅为0.41，计算和通信开销可忽略，保持稳定收敛。

Conclusion: FL-LTD是一种轻量级、保护隐私的防御框架，通过基于损失的监控有效保障联邦学习安全，避免客户端排除或访问敏感数据，展示了损失动态监控在安全联邦学习中的有效性。

Abstract: Federated Learning (FL) facilitates collaborative model training among distributed clients while ensuring that raw data remains on local devices.Despite this advantage, FL systems are still exposed to risks from malicious or unreliable participants. Such clients can interfere with the training process by sending misleading updates, which can negatively affect the performance and reliability of the global model. Many existing defense mechanisms rely on gradient inspection, complex similarity computations, or cryptographic operations, which introduce additional overhead and may become unstable under non-IID data distributions. In this paper, we propose the Federated Learning with Loss Trend Detection (FL-LTD), a lightweight and privacy-preserving defense framework that detects and mitigates malicious behavior by monitoring temporal loss dynamics rather than model gradients. The proposed approach identifies anomalous clients by detecting abnormal loss stagnation or abrupt loss fluctuations across communication rounds. To counter adaptive attackers, a short-term memory mechanism is incorporated to sustain mitigation for clients previously flagged as anomalous, while enabling trust recovery for stable participants. We evaluate FL-LTD on a non-IID federated MNIST setup under loss manipulation attacks. Experimental results demonstrate that the proposed method significantly enhances robustness, achieving a final test accuracy of 0.84, compared to 0.41 for standard FedAvg under attack. FL-LTD incurs negligible computational and communication overhead, maintains stable convergence, and avoids client exclusion or access to sensitive data, highlighting the effectiveness of loss-based monitoring for secure federated learning.

</details>


### [5] [FIPS 204-Compatible Threshold ML-DSA via Masked Lagrange Reconstruction](https://arxiv.org/abs/2601.20917)
*Leo Kao*

Main category: cs.CR

TL;DR: 提出了一种名为"掩码拉格朗日重构"的技术，实现了任意阈值T的ML-DSA阈值签名，生成标准3.3KB签名，可由未修改的FIPS 204实现验证。


<details>
  <summary>Details</summary>
Motivation: 现有并发方法存在限制：Bienstock等人的方法支持任意T但需要诚实多数和37-136轮通信；Celi等人的方法支持不诚实多数但仅限于T≤6。需要解决拉格朗日系数随q增长导致个体贡献过大而无法通过ML-DSA拒绝采样的问题。

Method: 提出掩码拉格朗日重构技术，解决了ML-DSA阈值签名的三个关键挑战：1) 掩码后仍需通过拒绝采样；2) r0检查会暴露c s2导致密钥恢复风险；3) 保持Irwin-Hall非均匀分布的EUF-CMA安全性。实现了三种部署配置：P1（TEE辅助）使用可信协调器；P2（完全分布式）通过MPC消除硬件信任；P3（2PC辅助）使用轻量级2PC进行r0检查。

Result: 实现了任意阈值T的ML-DSA阈值签名，生成标准3.3KB签名。三种配置分别达到：P1在Module-SIS假设下具有EUF-CMA安全性；P2在恶意敌手最多腐化n-1方时具有UC安全性；P3在1-of-2 CP诚实假设下具有UC安全性，最佳性能为249ms。需要|S|≥T+1个签名者，成功率23-32%，与单签名者ML-DSA匹配。

Conclusion: 掩码拉格朗日重构技术成功解决了ML-DSA阈值签名的关键挑战，实现了任意阈值T的阈值签名方案，提供了三种不同安全假设和性能权衡的部署配置，填补了现有工作的空白。

Abstract: We present masked Lagrange reconstruction, a technique that enables threshold ML-DSA (FIPS 204) with arbitrary thresholds $T$ while producing standard 3.3 KB signatures verifiable by unmodified FIPS 204 implementations. Concurrent approaches have limitations: Bienstock et al. (ePrint 2025/1163) achieve arbitrary $T$ but require honest-majority and 37--136 rounds; Celi et al. (ePrint 2026/013) achieve dishonest-majority but are limited to $T \leq 6$. Our technique addresses the barrier that Lagrange coefficients grow as $Θ(q)$ for moderate $T$, making individual contributions too large for ML-DSA's rejection sampling.
  Unlike ECDSA threshold schemes where pairwise masks suffice for correctness, ML-DSA requires solving three additional challenges absent in prior work: (1) rejection sampling on $\|z\|_\infty$ must still pass after masking, (2) the $r_0$-check exposes $c s_2$ enabling key recovery if unprotected, and (3) the resulting Irwin-Hall nonce distribution must preserve EUF-CMA security. We solve all three.
  We instantiate this technique in three deployment profiles with full security proofs. Profile P1 (TEE-assisted) achieves 3-round signing with a trusted coordinator, with EUF-CMA security under Module-SIS. Profile P2 (fully distributed) eliminates hardware trust via MPC in 8 rounds, achieving UC security against malicious adversaries corrupting up to $n-1$ parties. Profile P3 (2PC-assisted) uses lightweight 2PC for the $r_0$-check in 3--5 rounds, achieving UC security under a 1-of-2 CP honest assumption with the best empirical performance (249ms).
  Our scheme requires $|S| \geq T+1$ signers and achieves success rates of 23--32\%, matching single-signer ML-DSA.

</details>


### [6] [SPOILER-GUARD: Gating Latency Effects of Memory Accesses through Randomized Dependency Prediction](https://arxiv.org/abs/2601.21211)
*Gayathri Subramanian,Girinath P,Nitya Ranganathan,Kamakoti Veezhinathan,Gopalakrishnan Srinivasan*

Main category: cs.CR

TL;DR: SPOILER-GUARD是一种硬件防御机制，通过动态随机化物理地址位和标记存储条目来防止SPOILER攻击，减少误预测至0.0004%，性能提升2-3%，硬件开销极小。


<details>
  <summary>Details</summary>
Motivation: 现代微处理器依赖推测执行，存在瞬态执行攻击漏洞。现有防御措施针对推测数据泄漏，但忽略了部分地址别名导致的虚假依赖关系，这种依赖会通过重复的squash和reissue事件增加加载-存储延迟，被SPOILER攻击利用。

Method: 提出SPOILER-GUARD硬件防御方案，通过动态随机化用于加载-存储比较的物理地址位，并对存储条目进行标记，以防止延迟放大的误预测。

Result: 在gem5中实现并使用SPEC 2017评估，SPOILER-GUARD将误预测降至0.0004%，整数和浮点性能分别提升2.12%和2.87%。使用Synopsys Design Compiler在14nm节点进行HDL综合显示极小开销：关键路径延迟69ps，面积0.064平方毫米，功耗5.863mW。

Conclusion: SPOILER-GUARD有效防御SPOILER攻击，显著减少误预测并提升性能，同时硬件开销极小，是一种实用的推测执行安全解决方案。

Abstract: Modern microprocessors depend on speculative execution, creating vulnerabilities that enable transient execution attacks. Prior defenses target speculative data leakage but overlook false dependencies from partial address aliasing, where repeated squash and reissue events increase the load-store latency, which is exploited by the SPOILER attack. We present SPOILER-GUARD, a hardware defense that obfuscates speculative dependency resolution by dynamically randomizing the physical address bits used for load-store comparisons and tagging store entries to prevent latency-amplifying misspeculations. Implemented in gem5 and evaluated with SPEC 2017, SPOILER-GUARD reduces misspeculation to 0.0004 percent and improves integer and floating-point performance by 2.12 and 2.87 percent. HDL synthesis with Synopsys Design Compiler at 14 nm node demonstrates minimal overheads - 69 ps latency in critical path, 0.064 square millimeter in area, and 5.863 mW in power.

</details>


### [7] [Lossless Copyright Protection via Intrinsic Model Fingerprinting](https://arxiv.org/abs/2601.21252)
*Lingxiao Chen,Liqin Wang,Wei Lu,Xiangyang Luo*

Main category: cs.CR

TL;DR: TrajPrint是一种无损、无需训练的扩散模型版权保护框架，通过提取确定性生成过程中形成的独特流形指纹来验证模型版权，兼容黑盒API且不影响模型性能。


<details>
  <summary>Details</summary>
Motivation: 扩散模型作为高价值知识产权面临未经授权复制的风险，现有保护方法要么通过修改模型嵌入水印影响性能，要么通过操纵去噪过程提取指纹但不兼容黑盒API。

Method: 1) 使用水印图像作为锚点，精确回溯到其轨迹起源，锁定该路径映射的模型指纹；2) 采用双端锚定的联合优化策略合成特定指纹噪声，严格遵循目标流形以实现鲁棒水印恢复；3) 通过原子推理和统计假设检验进行验证。

Result: TrajPrint在黑盒API场景中实现了无损验证，对模型修改具有优越的鲁棒性，目标模型能够恢复水印图像，而非目标模型则无法恢复。

Conclusion: TrajPrint提供了一种完全无损且无需训练的扩散模型版权保护框架，通过提取独特的流形指纹实现了黑盒API兼容的版权验证，解决了现有方法的性能损失和兼容性问题。

Abstract: The exceptional performance of diffusion models establishes them as high-value intellectual property but exposes them to unauthorized replication. Existing protection methods either modify the model to embed watermarks, which impairs performance, or extract model fingerprints by manipulating the denoising process, rendering them incompatible with black-box APIs. In this paper, we propose TrajPrint, a completely lossless and training-free framework that verifies model copyright by extracting unique manifold fingerprints formed during deterministic generation. Specifically, we first utilize a watermarked image as an anchor and exactly trace the path back to its trajectory origin, effectively locking the model fingerprint mapped by this path. Subsequently, we implement a joint optimization strategy that employs dual-end anchoring to synthesize a specific fingerprint noise, which strictly adheres to the target manifold for robust watermark recovery. As input, it enables the protected target model to recover the watermarked image, while failing on non-target models. Finally, we achieved verification via atomic inference and statistical hypothesis testing. Extensive experiments demonstrate that TrajPrint achieves lossless verification in black-box API scenarios with superior robustness against model modifications.

</details>


### [8] [SecIC3: Customizing IC3 for Hardware Security Verification](https://arxiv.org/abs/2601.21353)
*Qinhan Tan,Akash Gaonkar,Yu-Wei Fan,Aarti Gupta,Sharad Malik*

Main category: cs.CR

TL;DR: SecIC3是一种针对硬件安全验证的模型检查算法，通过利用自组合结构中的对称性来加速非干扰属性的验证。


<details>
  <summary>Details</summary>
Motivation: 现有硬件安全验证方法虽然能将非干扰超属性转换为自组合设计的安全属性，但缺乏专门针对这种特殊结构的模型检查器，导致验证效率不高。

Method: 基于IC3算法开发SecIC3，采用两种互补技术：对称状态探索和添加等价谓词，专门利用自组合结构进行优化。

Result: 在包含10个设计的非干扰检查基准测试中，SecIC3显著减少了安全证明的时间，相比基线实现最高可达49.3倍的证明加速。

Conclusion: SecIC3通过专门针对硬件安全验证的自组合结构进行优化，显著提高了非干扰属性验证的效率，为硬件安全验证提供了有效的模型检查方法。

Abstract: Recent years have seen significant advances in using formal verification to check hardware security properties. Of particular practical interest are checking confidentiality and integrity of secrets, by checking that there is no information flow between the secrets and observable outputs. A standard method for checking information flow is to translate the corresponding non-interference hyperproperty into a safety property on a self-composition of the design, which has two copies of the design composed together. Although prior efforts have aimed to reduce the size of the self-composed design, there are no state-of-the-art model checkers that exploit their special structure for hardware security verification. In this paper, we propose SecIC3, a hardware model checking algorithm based on IC3 that is customized to exploit this self-composition structure. SecIC3 utilizes this structure in two complementary techniques: symmetric state exploration and adding equivalence predicates. We implement SecIC3 on top of two open-source IC3 implementations and evaluate it on a non-interference checking benchmark consisting of 10 designs. The experiment results show that SecIC3 significantly reduces the time for finding security proofs, with up to 49.3x proof speedup compared to baseline implementations.

</details>


### [9] [RerouteGuard: Understanding and Mitigating Adversarial Risks for LLM Routing](https://arxiv.org/abs/2601.21380)
*Wenhui Zhang,Huiyu Xu,Zhibo Wang,Zhichao Li,Zeqing He,Xuelin Wei,Kui Ren*

Main category: cs.CR

TL;DR: 本文系统研究了LLM重路由攻击的安全威胁，提出了RerouteGuard防御框架，能有效检测对抗性重路由提示，保护多模型AI系统。


<details>
  <summary>Details</summary>
Motivation: 多模型AI系统中使用LLM路由器来降低计算成本，但作为分类器的LLM路由器容易受到新型对抗攻击（LLM重路由攻击）的影响，攻击者通过在用户查询前添加特殊设计的触发词来操纵路由决策，可能导致计算成本增加、响应质量下降甚至绕过安全防护，但这些安全影响尚未得到充分研究。

Method: 首先基于攻击者目标（成本升级、质量劫持、安全绕过）和知识系统化LLM重路由威胁；然后对真实世界的LLM路由系统进行测量研究；使用可解释性技术分析现有重路由攻击；最后提出RerouteGuard防御框架，通过动态嵌入检测和自适应阈值过滤对抗性重路由提示。

Result: 研究发现现有路由系统容易受到重路由攻击，特别是在成本升级场景下；RerouteGuard在三种攻击设置和四个基准测试中实现了超过99%的检测准确率，同时对合法查询的影响可忽略不计。

Conclusion: RerouteGuard为保护多模型AI系统免受对抗性重路由攻击提供了一个原则性和实用的解决方案，填补了LLM路由器安全研究的空白。

Abstract: Recent advancements in multi-model AI systems have leveraged LLM routers to reduce computational cost while maintaining response quality by assigning queries to the most appropriate model. However, as classifiers, LLM routers are vulnerable to novel adversarial attacks in the form of LLM rerouting, where adversaries prepend specially crafted triggers to user queries to manipulate routing decisions. Such attacks can lead to increased computational cost, degraded response quality, and even bypass safety guardrails, yet their security implications remain largely underexplored. In this work, we bridge this gap by systematizing LLM rerouting threats based on the adversary's objectives (i.e., cost escalation, quality hijacking, and safety bypass) and knowledge. Based on the threat taxonomy, we conduct a measurement study of real-world LLM routing systems against existing LLM rerouting attacks. The results reveal that existing routing systems are vulnerable to rerouting attacks, especially in the cost escalation scenario. We then characterize existing rerouting attacks using interpretability techniques, revealing that they exploit router decision boundaries through confounder gadgets that prepend queries to force misrouting. To mitigate these risks, we introduce RerouteGuard, a flexible and scalable guardrail framework for LLM rerouting. RerouteGuard filters adversarial rerouting prompts via dynamic embedding-based detection and adaptive thresholding. Extensive evaluations in three attack settings and four benchmarks demonstrate that RerouteGuard achieves over 99% detection accuracy against state-of-the-art rerouting attacks, while maintaining negligible impact on legitimate queries. The experimental results indicate that RerouteGuard offers a principled and practical solution for safeguarding multi-model AI systems against adversarial rerouting.

</details>


### [10] [On the Adversarial Robustness of Large Vision-Language Models under Visual Token Compression](https://arxiv.org/abs/2601.21531)
*Xinwei Zhang,Hangcheng Liu,Li Bai,Hao Wang,Qingqing Ye,Tianwei Zhang,Haibo Hu*

Main category: cs.CR

TL;DR: 论文提出CAGE攻击方法，针对视觉语言模型中的视觉令牌压缩机制，解决了现有攻击方法因优化-推理不匹配而高估模型鲁棒性的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于编码器的攻击方法会显著高估压缩后大型视觉语言模型（LVLMs）的鲁棒性，因为存在优化-推理不匹配：扰动是在完整令牌表示上优化的，而推理是通过令牌压缩瓶颈进行的。

Method: 提出CAGE（压缩对齐攻击）方法，包含两个关键组件：1）预期特征破坏，将失真集中在可能在不同预算下存活的令牌上；2）排名失真对齐，主动将令牌失真与排名分数对齐，以促进高度失真证据的保留。

Result: 在多种代表性的即插即用压缩机制和数据集上，CAGE始终比基线方法获得更低的鲁棒准确率，表明忽略压缩的鲁棒性评估可能过于乐观。

Conclusion: 这项工作强调忽略压缩的鲁棒性评估可能过于乐观，呼吁对高效LVLMs进行压缩感知的安全评估和防御。

Abstract: Visual token compression is widely used to accelerate large vision-language models (LVLMs) by pruning or merging visual tokens, yet its adversarial robustness remains unexplored. We show that existing encoder-based attacks can substantially overestimate the robustness of compressed LVLMs, due to an optimization-inference mismatch: perturbations are optimized on the full-token representation, while inference is performed through a token-compression bottleneck. To address this gap, we propose the Compression-AliGnEd attack (CAGE), which aligns perturbation optimization with compression inference without assuming access to the deployed compression mechanism or its token budget. CAGE combines (i) expected feature disruption, which concentrates distortion on tokens likely to survive across plausible budgets, and (ii) rank distortion alignment, which actively aligns token distortions with rank scores to promote the retention of highly distorted evidence. Across diverse representative plug-and-play compression mechanisms and datasets, our results show that CAGE consistently achieves lower robust accuracy than the baseline. This work highlights that robustness assessments ignoring compression can be overly optimistic, calling for compression-aware security evaluation and defenses for efficient LVLMs.

</details>


### [11] [ICL-EVADER: Zero-Query Black-Box Evasion Attacks on In-Context Learning and Their Defenses](https://arxiv.org/abs/2601.21586)
*Ningyuan He,Ronghong Huang,Qianqian Tang,Hongyu Wang,Xianghang Mi,Shanqing Guo*

Main category: cs.CR

TL;DR: ICL-Evader：针对大语言模型上下文学习的零查询黑盒逃避攻击框架，包含三种新型攻击方法，能显著降低分类器性能，并提出有效的防御策略。


<details>
  <summary>Details</summary>
Motivation: 上下文学习（ICL）已成为文本分类的强大且数据高效的方法，但其在现实对抗威胁下的鲁棒性尚未得到充分探索。作者旨在揭示ICL系统的安全漏洞，并提出实用的防御方案。

Method: 提出ICL-Evader攻击框架，包含三种新型攻击方法：Fake Claim、Template和Needle-in-a-Haystack。这些攻击利用LLM处理上下文提示的固有局限性，在零查询威胁模型下工作，无需访问模型参数、梯度或查询反馈。

Result: 在情感分析、毒性和非法推广任务上的评估显示，攻击显著降低了分类器性能（攻击成功率高达95.3%），远超传统NLP攻击。同时提出了联合防御方案，能有效缓解所有攻击，且效用损失最小（准确率下降<5%）。

Conclusion: 这项工作对ICL进行了全面的安全评估，揭示了关键漏洞，并为构建更鲁棒的系统提供了实用解决方案。作者还将防御见解转化为自动化工具，用于主动加固标准ICL提示。

Abstract: In-context learning (ICL) has become a powerful, data-efficient paradigm for text classification using large language models. However, its robustness against realistic adversarial threats remains largely unexplored. We introduce ICL-Evader, a novel black-box evasion attack framework that operates under a highly practical zero-query threat model, requiring no access to model parameters, gradients, or query-based feedback during attack generation. We design three novel attacks, Fake Claim, Template, and Needle-in-a-Haystack, that exploit inherent limitations of LLMs in processing in-context prompts. Evaluated across sentiment analysis, toxicity, and illicit promotion tasks, our attacks significantly degrade classifier performance (e.g., achieving up to 95.3% attack success rate), drastically outperforming traditional NLP attacks which prove ineffective under the same constraints. To counter these vulnerabilities, we systematically investigate defense strategies and identify a joint defense recipe that effectively mitigates all attacks with minimal utility loss (<5% accuracy degradation). Finally, we translate our defensive insights into an automated tool that proactively fortifies standard ICL prompts against adversarial evasion. This work provides a comprehensive security assessment of ICL, revealing critical vulnerabilities and offering practical solutions for building more robust systems. Our source code and evaluation datasets are publicly available at: https://github.com/ChaseSecurity/ICL-Evader .

</details>


### [12] [Noise as a Probe: Membership Inference Attacks on Diffusion Models Leveraging Initial Noise](https://arxiv.org/abs/2601.21628)
*Puwei Lian,Yujun Cai,Songze Li,Bingkun Bao*

Main category: cs.CR

TL;DR: 该论文提出了一种针对扩散模型的成员推理攻击方法，利用噪声调度中残留的语义信息来推断特定样本是否属于训练数据。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像生成方面取得显著进展，但其部署引发了隐私担忧。特别是微调模型更容易受到攻击，因为它们通常在小型私有数据集上微调。现有的成员推理攻击要么需要获取中间结果，要么需要辅助数据集训练影子模型。

Method: 利用噪声调度未能完全消除图像语义信息的漏洞，将语义信息注入初始噪声中，通过分析模型生成结果来推断成员关系。

Result: 实验证明语义初始噪声能强烈揭示成员信息，突显了扩散模型对成员推理攻击的脆弱性。

Conclusion: 扩散模型存在严重隐私风险，当前广泛使用的噪声调度方案存在安全漏洞，需要在隐私保护方面进行改进。

Abstract: Diffusion models have achieved remarkable progress in image generation, but their increasing deployment raises serious concerns about privacy. In particular, fine-tuned models are highly vulnerable, as they are often fine-tuned on small and private datasets. Membership inference attacks (MIAs) are used to assess privacy risks by determining whether a specific sample was part of a model's training data. Existing MIAs against diffusion models either assume obtaining the intermediate results or require auxiliary datasets for training the shadow model. In this work, we utilized a critical yet overlooked vulnerability: the widely used noise schedules fail to fully eliminate semantic information in the images, resulting in residual semantic signals even at the maximum noise step. We empirically demonstrate that the fine-tuned diffusion model captures hidden correlations between the residual semantics in initial noise and the original images. Building on this insight, we propose a simple yet effective membership inference attack, which injects semantic information into the initial noise and infers membership by analyzing the model's generation result. Extensive experiments demonstrate that the semantic initial noise can strongly reveal membership information, highlighting the vulnerability of diffusion models to MIAs.

</details>


### [13] [Authenticated encryption for space telemetry](https://arxiv.org/abs/2601.21657)
*Andrew Savchenko*

Main category: cs.CR

TL;DR: 本文提出了一种轻量级认证加密方案，用于满足NASA-STD-1006A标准中紧急空间遥测数据的指令栈保护要求，在资源受限环境下提供强安全性且保持性能。


<details>
  <summary>Details</summary>
Motivation: 满足NASA-STD-1006A标准对紧急空间遥测中指令栈保护的安全要求，在资源受限的航天环境中平衡安全需求与操作约束。

Method: 提出轻量级认证加密方案，专注于可预测属性和强认证机制，生成固定长度消息以保持与底层数据传输协议的兼容性。

Result: 该方案能够在资源受限环境中提供强安全性而不牺牲性能，保护紧急通信中遥测数据的机密性、完整性和真实性。

Conclusion: 通过轻量级认证加密方案成功实现了NASA-STD-1006A标准对紧急空间遥测指令栈保护的要求，在安全性与操作约束之间取得了良好平衡。

Abstract: We explore how command stack protection requirements outlined in NASA-STD-1006A can be satisfied within the context of emergency space telemetry. Proposed implementation of lightweight authenticated encryption offers strong security without sacrificing performance in resource-constrained environments. It produces fixed-length messages, maintaining compatibility with the underlying data transport protocols. By focusing on predictable properties and robust authentication, we create a scheme that protects the confidentiality, integrity and authenticity of telemetry data in emergency communications while balancing security requirements with the operational constraints.

</details>


### [14] [WADBERT: Dual-channel Web Attack Detection Based on BERT Models](https://arxiv.org/abs/2601.21893)
*Kangqiang Luo,Yi Xie,Shiqian Zhao,Jing Pan*

Main category: cs.CR

TL;DR: WADBERT是一种基于深度学习的Web攻击检测模型，通过混合粒度嵌入和BERT架构实现高精度检测，并能精确定位恶意参数。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在处理不规则HTTP请求时效果不佳，难以建模无序参数和实现攻击溯源。需要一种既能高精度检测攻击，又能精确定位恶意参数的方法。

Method: 提出WADBERT模型：1) 使用混合粒度嵌入为URL和负载参数生成细粒度嵌入；2) 分别用URLBERT和SecBERT提取语义特征；3) 通过多头注意力机制融合参数级特征；4) 将拼接的URL和负载特征输入线性分类器得到最终检测结果。

Result: 在CSIC2010和SR-BH2020数据集上分别达到99.63%和99.50%的F1分数，显著优于现有最先进方法。

Conclusion: WADBERT不仅实现了高精度的Web攻击检测，还能精确定位恶意参数，解决了现有方法在处理不规则HTTP请求和攻击溯源方面的局限性。

Abstract: Web attack detection is the first line of defense for securing web applications, designed to preemptively identify malicious activities. Deep learning-based approaches are increasingly popular for their advantages: automatically learning complex patterns and extracting semantic features from HTTP requests to achieve superior detection performance. However, existing methods are less effective in embedding irregular HTTP requests, even failing to model unordered parameters and achieve attack traceability. In this paper, we propose an effective web attack detection model, named WADBERT. It achieves high detection accuracy while enabling the precise identification of malicious parameters. To this end, we first employ Hybrid Granularity Embedding (HGE) to generate fine-grained embeddings for URL and payload parameters. Then, URLBERT and SecBERT are respectively utilized to extract their semantic features. Further, parameter-level features (extracted by SecBERT) are fused through a multi-head attention mechanism, resulting in a comprehensive payload feature. Finally, by feeding the concatenated URL and payload features into a linear classifier, a final detection result is obtained. The experimental results on CSIC2010 and SR-BH2020 datasets validate the efficacy of WADBERT, which respectively achieves F1-scores of 99.63% and 99.50%, and significantly outperforms state-of-the-art methods.

</details>


### [15] [Beyond the Finite Variant Property: Extending Symbolic Diffie-Hellman Group Models (Extended Version)](https://arxiv.org/abs/2601.21910)
*Sofia Giampietro,Ralf Sasse,David Basin*

Main category: cs.CR

TL;DR: 该论文提出了一种支持完整Diffie-Hellman群运算的半决策程序，扩展了Tamarin验证器以支持群元素乘法和指数加法，首次实现了对使用这些运算的协议进行建模和验证。


<details>
  <summary>Details</summary>
Motivation: 现有符号协议验证器虽然支持Diffie-Hellman群，但缺乏对指数加法等完整数学运算的支持，因为这些工具基于合一算法，而在描述所有Diffie-Hellman算子的理论中合一问题是不可判定的。

Method: 近似化Diffie-Hellman理论并提出了一个半决策程序，通过扩展Tamarin验证器来支持完整的Diffie-Hellman理论，包括群元素乘法和指数加法。

Result: 成功实现了对使用完整Diffie-Hellman运算的协议验证，首次使先进工具能够建模和推理此类协议，通过ElGamal加密和MQV案例研究证明了方法的有效性。

Conclusion: 该方法填补了现有符号验证器在支持完整Diffie-Hellman运算方面的空白，为分析和验证使用这些运算的密码协议提供了有效工具。

Abstract: Diffie-Hellman groups are commonly used in cryptographic protocols. While most state-of-the-art, symbolic protocol verifiers support them to some degree, they do not support all mathematical operations possible in these groups. In particular, they lack support for exponent addition, as these tools reason about terms using unification, which is undecidable in the theory describing all Diffie-Hellman operators. In this paper we approximate such a theory and propose a semi-decision procedure to determine whether a protocol, which may use all operations in such groups, satisfies user-defined properties. We implement this approach by extending the Tamarin prover to support the full Diffie-Hellman theory, including group element multiplication and hence addition of exponents. This is the first time a state-of-the-art tool can model and reason about such protocols. We illustrate our approach's effectiveness with different case studies: ElGamal encryption and MQV. Using Tamarin, we prove security properties of ElGamal, and we rediscover known attacks on MQV.

</details>


### [16] [Secure Group Key Agreement on Cyber-Physical System Buses](https://arxiv.org/abs/2601.21966)
*Sebastian N. Peters,Lukas Lautenschlager,David Emeis,Jason Lochert*

Main category: cs.CR

TL;DR: 针对总线拓扑的受限CPS系统，提出了一种基于TreeKEM的认证式完全分布式群组密钥协商协议，解决了现有协议在广播链路、半双工操作、资源限制、动态成员等约束下的适应性问题。


<details>
  <summary>Details</summary>
Motivation: CPS系统需要安全的总线通信，传统固定预共享密钥和首次使用信任机制不安全，现有群组密钥协商协议无法适应受限CPS总线的约束条件。

Method: 首先系统化分析现有协议，然后推导总线系统认证式完全分布式GKA的需求，最后基于TreeKEM设计、实现和评估定制GKA协议。

Result: 提出了一种适用于总线拓扑的认证式完全分布式GKA协议，能够满足工业CPS系统的约束条件，包括广播链路、半双工操作、资源限制、动态成员变化等。

Conclusion: 该研究填补了现有GKA协议在受限CPS总线系统上的空白，为CPS总线通信提供了安全、动态的密钥协商解决方案。

Abstract: Cyber-Physical Systems (CPSs) rely on distributed embedded devices that often must communicate securely over buses. Ensuring message integrity and authenticity on these buses typically requires group-shared keys for Message Authentication Codes (MACs). To avoid insecure fixed pre-shared keys and trust-on-first-use concepts, a Group Key Agreement (GKA) protocol is needed to dynamically agree on a key amongst the devices. Yet existing GKA protocols lack adaptability to constrained CPS buses. This paper targets authenticated, fully distributed GKA suitable for bus topologies under constraints of industrial and cyber-physical systems, including broadcast-only links, half-duplex operation, resource limits, dynamic membership (including unannounced leaves), a long device lifetime, and a strong Dolev-Yao adversary capable of partitioning the bus. We first systematise existing protocols, then derive the requirements necessary for an authenticated and fully distributed GKA on bus systems. Finally, we design, implement, and evaluate a custom GKA protocol based on TreeKEM.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [17] [A Survey on Large Language Model Impact on Software Evolvability and Maintainability: the Good, the Bad, the Ugly, and the Remedy](https://arxiv.org/abs/2601.20879)
*Bruno Claudino Matias,Savio Freire,Juliana Freitas,Felipe Fronchetti,Kostadin Damevski,Rodrigo Spinola*

Main category: cs.SE

TL;DR: 本文通过系统文献综述分析了LLMs对软件可维护性和可演化性的影响，发现LLMs在提升代码分析、测试、理解等方面有积极作用，但也带来了幻觉输出、上下文脆弱性等风险，需要负责任地采用。


<details>
  <summary>Details</summary>
Motivation: LLMs在软件工程中应用日益广泛，但现有研究证据分散，对其长期影响缺乏系统理解。本研究旨在系统分析LLMs如何影响软件系统的可维护性和可演化性。

Method: 采用系统文献综述方法，在ACM DL、IEEE Xplore和Scopus数据库中检索2020-2024年的87篇主要研究，通过多研究者校准过程提取定性证据，使用混合主题分析方法进行综合。

Result: LLMs提供了改进可分析性、可测试性、代码理解、调试支持和自动修复等益处，但也引入了幻觉或错误输出、上下文脆弱性、有限领域推理、性能不稳定和当前评估缺陷等风险。

Conclusion: LLMs能够增强软件的可维护性和可演化性，但也对长期可持续性构成显著风险。负责任地采用需要保障措施、严格评估和结构化的人工监督。

Abstract: Context. Large Language Models (LLMs) are increasingly embedded in software engineering workflows for tasks including code generation, summarization, repair, and testing. Empirical studies report productivity gains, improved comprehension, and reduced cognitive load. However, evidence remains fragmented, and concerns persist about hallucinations, unstable outputs, methodological limitations, and emerging forms of technical debt. How these mixed effects shape long-term software maintainability and evolvability remains unclear. Objectives. This study systematically examines how LLMs influence the maintainability and evolvability of software systems. We identify which quality attributes are addressed in existing research, the positive impacts LLMs provide, the risks and weaknesses they introduce, and the mitigation strategies proposed in the literature. Method. We conducted a systematic literature review. Searches across ACM DL, IEEE Xplore, and Scopus (2020 to 2024) yielded 87 primary studies. Qualitative evidence was extracted through a calibrated multi-researcher process. Attributes were analyzed descriptively, while impacts, risks, weaknesses, and mitigation strategies were synthesized using a hybrid thematic approach supported by an LLM-assisted analysis tool with human-in-the-loop validation. Results. LLMs provide benefits such as improved analyzability, testability, code comprehension, debugging support, and automated repair. However, they also introduce risks, including hallucinated or incorrect outputs, brittleness to context, limited domain reasoning, unstable performance, and flaws in current evaluations, which threaten long-term evolvability. Conclusion. LLMs can strengthen maintainability and evolvability, but they also pose nontrivial risks to long-term sustainability. Responsible adoption requires safeguards, rigorous evaluation, and structured human oversight.

</details>


### [18] [Another Systematic Review? A Critical Analysis of Systematic Literature Reviews on Agile Effort and Cost Estimation](https://arxiv.org/abs/2601.20893)
*Henry Edison,Nauman Ali*

Main category: cs.SE

TL;DR: 该研究分析了软件工程领域系统文献综述(SLR)重复现象，通过研究敏捷软件开发工作量估算这一具体主题，识别了作者为重复SLR研究提供的常见理由模式。


<details>
  <summary>Details</summary>
Motivation: 软件工程研究中系统文献综述(SLR)日益普遍，但研究人员经常在没有充分检查已有SLR的情况下进行重复研究，造成资源浪费。研究旨在理解作者如何为重复的SLR研究提供合理性论证。

Method: 选择敏捷软件开发工作量估算这一狭窄但研究充分的主题，对18篇已发表的SLR进行定性内容分析，识别常见的合理性论证模式，并结合引用数据、发表年份、发表场所以及SLR质量进行结果解释。

Result: 识别出四种常见的合理性论证模式：1)声称现有研究覆盖范围存在空白；2)指出先前研究存在方法学局限性；3)认为先前SLR已过时；4)强调技术和方法的快速发展需要更新的综合研究。

Conclusion: 通过对狭窄主题的深入分析，为软件工程领域的SLR研究提供了一般性见解。强调需要在设计和评审指南以及会议期刊政策中，要求研究人员识别现有SLR并合理论证新SLR的必要性，以减少重复工作并提高领域进展速度。

Abstract: Background: Systematic literature reviews (SLRs) have become prevalent in software engineering research. Several researchers may conduct SLRs on similar topics without a prospective register for SLR protocols. However, even ignoring these unavoidable duplications of effort in the simultaneous conduct of SLRs, the proliferation of overlapping and often repetitive SLRs indicates that researchers are not extensively checking for existing SLRs on a topic. Given how effort-intensive it is to design, conduct, and report an SLR, the situation is less than ideal for software engineering research. Aim: To understand how authors justify additional SLRs on a topic. Method: To illustrate the issue and develop suggestions for improvement to address this issue, we have intentionally picked a sufficiently narrow but well-researched topic, i.e., effort estimation in Agile software development. We identify common justification patterns through a qualitative content analysis of 18 published SLRs. We further consider the citation data, publication years, publication venues, and the quality of the SLRs when interpreting the results. Results: The common justification patterns include authors claiming gaps in coverage, methodological limitations in prior studies, temporal obsolescence of previous SLRs, or rapid technological and methodological advancements necessitating updated syntheses. Conclusion: Our in-depth analysis of SLRs on a fairly narrow topic provides insights into SLRs in software engineering in general. By emphasizing the need for identifying existing SLRs and for justifying the undertaking of further SLRs, both in design and review guidelines and as a policy of conferences and journals, we can reduce the likelihood of duplication of effort and increase the rate of progress in the field.

</details>


### [19] [Infusion of Blockchain to Establish Trustworthiness in AI Supported Software Evolution: A Systematic Literature Review](https://arxiv.org/abs/2601.20918)
*Mohammad Naserameri,Juergen Rilling*

Main category: cs.SE

TL;DR: 区块链与AI结合提升软件工程可信度，但仅有31%研究明确关注可信性，区块链通过数据不可篡改、模型透明等机制增强信任，但缺乏统一信任定义和实际测试。


<details>
  <summary>Details</summary>
Motivation: 区块链和AI在软件工程中日益被探索以增强可信度，特别是在支持软件演化任务方面。研究者希望通过系统文献综述了解区块链如何增强AI驱动软件工程工具和流程中的信任。

Method: 采用预定义协议的系统文献综述方法，设定明确的资格标准以确保透明度、可重复性和最小化偏差，综合分析了区块链支持AI驱动软件工程中信任的研究。

Result: 大多数研究关注AI在软件工程中的集成，只有31%明确解决可信性问题。综述突出了六项近期研究探索基于区块链的方法来增强AI辅助软件工程任务的可靠性、透明度和问责性。

Conclusion: 区块链通过确保数据不可篡改性、模型透明度和生命周期问责性（包括基于区块链共识的联邦学习和私有数据验证）来增强信任。然而，信任定义不一致和有限的真实世界测试仍是主要挑战。未来工作需要开发可测量、可重复的信任框架，以实现可靠、安全和合规的AI驱动软件工程生态系统，包括涉及大语言模型的应用。

Abstract: Context: Blockchain and AI are increasingly explored to enhance trustworthiness in software engineering (SE), particularly in supporting software evolution tasks. Method: We conducted a systematic literature review (SLR) using a predefined protocol with clear eligibility criteria to ensure transparency, reproducibility, and minimized bias, synthesizing research on blockchain-enabled trust in AI-driven SE tools and processes. Results: Most studies focus on integrating AI in SE, with only 31% explicitly addressing trustworthiness. Our review highlights six recent studies exploring blockchain-based approaches to reinforce reliability, transparency, and accountability in AI-assisted SE tasks. Conclusion: Blockchain enhances trust by ensuring data immutability, model transparency, and lifecycle accountability, including federated learning with blockchain consensus and private data verification. However, inconsistent definitions of trust and limited real-world testing remain major challenges. Future work must develop measurable, reproducible trust frameworks to enable reliable, secure, and compliant AI-driven SE ecosystems, including applications involving large language models.

</details>


### [20] [Operationalizing Research Software for Supply Chain Security](https://arxiv.org/abs/2601.20980)
*Kelechi G. Kalu,Soham Rattan,Taylor R. Schorlemmer,George K. Thiruvathukal,Jeffrey C. Carver,James C. Davis*

Main category: cs.SE

TL;DR: 该论文针对研究软件安全实证研究中定义不一致的问题，提出了一个面向研究软件供应链（RSSC）的分类法，通过文献综述、数据集标注和初步安全分析，展示了分类法在RSSC安全测量中的必要性。


<details>
  <summary>Details</summary>
Motivation: 现有研究软件安全实证研究难以比较，因为文献中对"研究软件"的操作定义不一致。受研究软件供应链（RSSC）及其安全风险的驱动，需要建立明确的分类法来界定实证研究的范围和操作边界。

Method: 1. 对近期仓库挖掘和数据集构建研究进行有针对性的范围界定综述，提取每项工作的定义、纳入标准、分析单元和识别启发式方法；2. 将这些信息综合成统一的分类法和映射关系，将先前方法转化为共享的分类维度；3. 在研究软件百科全书（RSE）的大型社区策划语料库上操作化分类法，生成标注数据集、标注代码本和可复现的标注流程；4. 应用OpenSSF Scorecard进行初步安全分析，展示不同分类簇之间仓库中心安全信号的差异。

Result: 1. 开发了一个面向RSSC的分类法，使研究范围和操作边界明确化；2. 创建了标注数据集、标注代码本和可复现的标注流程；3. 通过OpenSSF Scorecard分析发现，不同分类簇之间的仓库中心安全信号存在显著差异；4. 证明了基于分类法的分层对于解释RSSC安全测量的必要性。

Conclusion: 该研究提出的RSSC分类法为研究软件安全实证研究提供了统一的框架，解决了定义不一致的问题。分类法感知的分层对于准确解释RSSC安全测量结果至关重要，为未来的研究软件安全分析提供了方法论基础。

Abstract: Empirical studies of research software are hard to compare because the literature operationalizes ``research software'' inconsistently. Motivated by the research software supply chain (RSSC) and its security risks, we introduce an RSSC-oriented taxonomy that makes scope and operational boundaries explicit for empirical research software security studies.
  We conduct a targeted scoping review of recent repository mining and dataset construction studies, extracting each work's definition, inclusion criteria, unit of analysis, and identification heuristics. We synthesize these into a harmonized taxonomy and a mapping that translates prior approaches into shared taxonomy dimensions. We operationalize the taxonomy on a large community-curated corpus from the Research Software Encyclopedia (RSE), producing an annotated dataset, a labeling codebook, and a reproducible labeling pipeline. Finally, we apply OpenSSF Scorecard as a preliminary security analysis to show how repository-centric security signals differ across taxonomy-defined clusters and why taxonomy-aware stratification is necessary for interpreting RSSC security measurements.

</details>


### [21] [Towards Comprehensive Benchmarking Infrastructure for LLMs In Software Engineering](https://arxiv.org/abs/2601.21070)
*Daniel Rodriguez-Cardenas,Xiaochang Li,Marcos Macedo,Antonio Mastropaolo,Dipin Khati,Yuan Tian,Huajie Shao,Denys Poshyvanyk*

Main category: cs.SE

TL;DR: 该论文分析了当前代码大语言模型评估的局限性，提出了BEHELM基准测试框架，旨在通过统一的软件场景规范和多元指标评估来改进模型评估的全面性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前代码大语言模型发展迅速，但评估能力滞后。现有基准测试存在多个问题：任务范围狭窄、指标单一、缺乏鲁棒性、可解释性、公平性、效率和实际可用性评估；数据工程实践不一致、软件工程上下文有限、数据污染问题普遍。这些问题阻碍了对模型能力的准确评估。

Method: 通过深入调查现有基准测试和社区研讨会收集见解，识别出三个核心障碍：缺乏软件工程丰富的数据集、过度依赖机器学习中心指标、缺乏标准化可复现的数据管道。基于这些发现，提出了BEHELM基准测试基础设施，统一软件场景规范和多元指标评估。

Result: 识别了代码大语言模型评估的三个核心障碍，并提出了BEHELM框架。该框架提供结构化方法来评估模型在不同任务、语言、输入输出粒度以及关键质量维度上的表现，旨在减少构建基准测试的开销，同时实现公平、现实和面向未来的评估。

Conclusion: BEHELM基准测试基础设施为代码大语言模型评估提供了更全面、标准化的解决方案，能够更好地评估模型在软件工程场景中的实际表现，推动该领域向更可靠、可复现的评估方向发展。

Abstract: Large language models for code are advancing fast, yet our ability to evaluate them lags behind. Current benchmarks focus on narrow tasks and single metrics, which hide critical gaps in robustness, interpretability, fairness, efficiency, and real-world usability. They also suffer from inconsistent data engineering practices, limited software engineering context, and widespread contamination issues. To understand these problems and chart a path forward, we combined an in-depth survey of existing benchmarks with insights gathered from a dedicated community workshop. We identified three core barriers to reliable evaluation: the absence of software-engineering-rich datasets, overreliance on ML-centric metrics, and the lack of standardized, reproducible data pipelines. Building on these findings, we introduce BEHELM, a holistic benchmarking infrastructure that unifies software-scenario specification with multi-metric evaluation. BEHELM provides a structured way to assess models across tasks, languages, input and output granularities, and key quality dimensions. Our goal is to reduce the overhead currently required to construct benchmarks while enabling a fair, realistic, and future-proof assessment of LLMs in software engineering.

</details>


### [22] [From Logic to Toolchains: An Empirical Study of Bugs in the TypeScript Ecosystem](https://arxiv.org/abs/2601.21186)
*TianYi Tang,Saba Alimadadi,Nick Sumner*

Main category: cs.SE

TL;DR: TypeScript虽然减少了传统运行时和类型错误，但将故障转移到了构建系统和工具链，现代故障主要出现在集成和编排边界而非算法逻辑内部。


<details>
  <summary>Details</summary>
Motivation: TypeScript已成为现代Web开发的流行语言，但其对软件故障的影响仍不清楚。本研究旨在首次大规模实证分析真实TypeScript项目中的bug，了解故障类型分布及其与项目特征的关系。

Method: 分析16个流行开源仓库中的633个bug报告，构建故障类型分类法，量化其普遍性，并将其与项目大小、领域和依赖组成等特征关联分析。

Result: 故障主要由工具配置故障、API误用和异步错误处理问题主导，而非逻辑或语法错误。这些类别与构建复杂性和依赖异质性密切相关，表明现代故障常出现在集成和编排边界。与JavaScript研究的纵向比较显示，TypeScript的静态类型减少了传统运行时和类型错误，但将脆弱性转移到了构建系统和工具链。

Conclusion: 语言设计和生态系统演化重塑了大规模软件系统的故障特征，TypeScript虽然通过静态类型减少了某些传统错误，但引入了新的故障模式，主要集中在工具链和集成层面。

Abstract: TypeScript has rapidly become a popular language for modern web development, yet its effect on software faults remains poorly understood. This paper presents the first large-scale empirical study of bugs in real-world TypeScript projects. We analyze 633 bug reports from 16 popular open-source repositories to construct a taxonomy of fault types, quantify their prevalence, and relate them to project characteristics such as size, domain, and dependency composition. Our results reveal a fault landscape dominated not by logic or syntax errors but by tooling and configuration faults, API misuses, and asynchronous error-handling issues. We show that these categories correlate strongly with build complexity and dependency heterogeneity, indicating that modern failures often arise at integration and orchestration boundaries rather than within algorithmic logic. A longitudinal comparison with JavaScript studies shows that while static typing in TypeScript has reduced traditional runtime and type errors, it has shifted fragility toward build systems and toolchains. These findings offer new insight into how language design and ecosystem evolution reshape the fault profiles of large-scale software systems.

</details>


### [23] [The Role of Social Identity in Shaping Biases Against Minorities in Software Organizations](https://arxiv.org/abs/2601.21259)
*Sayma Sultana,London Cavaletto,Bianca Trinkenreich,Amiangshu Bosu*

Main category: cs.SE

TL;DR: 该研究应用社会认同理论调查软件工程领域的四种职场偏见形式，发现职业发展和任务选择偏见最为普遍，女性遭受偏见的可能性是男性的三倍以上，边缘化种族背景者更容易遭受身份攻击。


<details>
  <summary>Details</summary>
Motivation: 虽然系统性职场偏见在其他非计算领域已有充分记录，但其对软件工程师的具体影响仍然知之甚少。本研究旨在填补这一空白，通过社会认同理论来理解软件工程领域的偏见现象。

Method: 采用基于情景的问卷调查方法，量化四种偏见形式的普遍性：职业发展缺乏、刻板任务选择、不友好环境和身份攻击。识别受影响最严重的人口群体，评估偏见后果，并探索偏见行为的动机。

Result: 职业发展和任务选择偏见是最普遍的偏见形式，超过三分之二的受害者多次经历这些偏见。女性遭受职业发展偏见、任务选择偏见和不友好环境的可能性是男性的三倍以上。边缘化种族背景者更容易遭受身份攻击。年龄、工作经验年限、组织规模和地理位置也是偏见受害的重要预测因素。

Conclusion: 软件工程领域存在显著的职场偏见问题，特别是针对女性和边缘化种族背景者的系统性偏见。研究结果为理解软件工程领域的偏见现象提供了实证基础，并强调了需要针对不同人口群体采取针对性干预措施。

Abstract: While systemic workplace bias is well-documented in non-computing fields, its specific impact on software engineers remains poorly understood. This study addresses that gap by applying Social Identity Theory (SIT) to investigate four distinct forms of bias: lack of career development, stereotyped task selection, unwelcoming environments, and identity attacks. Using a vignette-based survey, we quantified the prevalence of these biases, identified the demographics most affected, assessed their consequences, and explored the motivations behind biased actions. Our results show that career development and task selection biases are the most prevalent forms, with over two-thirds of victims experiencing them multiple times. Women were more than three times as likely as men to face career development bias, task selection bias, and an unwelcoming environment. In parallel, individuals from marginalized ethnic backgrounds were disproportionately targeted by identity attacks. Our analysis also confirms that, beyond gender and race, factors such as age, years of experience, organization size, and geographic location are significant predictors of bias victimization.

</details>


### [24] [Detecting Multiple Semantic Concerns in Tangled Code Commits](https://arxiv.org/abs/2601.21298)
*Beomsu Koh,Neil Walkinshaw,Donghwan Shin*

Main category: cs.SE

TL;DR: 本文研究使用小型语言模型检测代码提交中的多关注点问题，将纠缠提交的多关注点检测构建为多标签分类任务，通过实验验证了14B参数SLM在有限token预算下的有效性。


<details>
  <summary>Details</summary>
Motivation: 实际开发中开发者经常将多个关注点捆绑在纠缠提交中，这模糊了意图并增加了维护难度。现有研究虽然使用CCS和语言模型捕获提交意图，但未解决涉及多个关注点的纠缠提交问题，使用语言模型进行多关注点检测的可行性尚未明确。

Method: 将纠缠提交的多关注点检测构建为多标签分类问题，基于真实数据构建人工纠缠提交的受控数据集。使用小型语言模型检测纠缠提交中的多个语义关注点，研究微调、关注点数量、提交消息包含和头部保留截断在实用token预算限制下的影响。

Result: 微调的14B参数SLM在单关注点提交中与最先进的LLM竞争，在最多三个关注点的情况下仍保持可用。包含提交消息可将检测准确率提高高达44%（以汉明损失衡量），且延迟开销可忽略，证明提交消息是重要的语义线索。

Conclusion: 小型语言模型可用于检测代码提交中的多关注点问题，在有限计算资源下保持实用性能。提交消息的包含显著提升检测准确率，为实际应用提供了重要指导。

Abstract: Code commits in a version control system (e.g., Git) should be atomic, i.e., focused on a single goal, such as adding a feature or fixing a bug. In practice, however, developers often bundle multiple concerns into tangled commits, obscuring intent and complicating maintenance. Recent studies have used Conventional Commits Specification (CCS) and Language Models (LMs) to capture commit intent, demonstrating that Small Language Models (SLMs) can approach the performance of Large Language Models (LLMs) while maintaining efficiency and privacy. However, they do not address tangled commits involving multiple concerns, leaving the feasibility of using LMs for multi-concern detection unresolved. In this paper, we frame multi-concern detection in tangled commits as a multi-label classification problem and construct a controlled dataset of artificially tangled commits based on real-world data. We then present an empirical study using SLMs to detect multiple semantic concerns in tangled commits, examining the effects of fine-tuning, concern count, commit-message inclusion, and header-preserving truncation under practical token-budget limits. Our results show that a fine-tuned 14B-parameter SLM is competitive with a state-of-the-art LLM for single-concern commits and remains usable for up to three concerns. In particular, including commit messages improves detection accuracy by up to 44% (in terms of Hamming Loss) with negligible latency overhead, establishing them as important semantic cues.

</details>


### [25] [Developers in the Age of AI: Adoption, Policy, and Diffusion of AI Software Engineering Tools](https://arxiv.org/abs/2601.21305)
*Mark Looi,Julianne Quinn*

Main category: cs.SE

TL;DR: 研究147名专业开发者使用AI工具的模式，发现频繁广泛使用AI工具与感知生产力和代码质量正相关，形成良性采纳循环。开发者分为三类：热衷者、实用主义者、谨慎者，组织采纳遵循创新扩散过程。安全担忧是采纳障碍，测试工具采纳滞后于编码工具。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI快速进入软件开发领域，需要实证研究其对开发实践的影响，了解AI工具使用模式、生产力与质量结果、以及开发者对AI增强开发的准备情况。

Method: 研究147名专业开发者的使用模式，分析AI工具使用频率、广度与感知生产力、代码质量的相关性，识别开发者采纳类型，考察组织采纳过程。

Result: 发现频繁广泛使用AI工具与感知生产力和代码质量正相关，形成良性采纳循环。开发者分为三类：热衷者推动组织成功，实用主义者随后跟进，谨慎者处于停滞状态。AI测试工具采纳滞后于编码工具，安全担忧是显著采纳障碍。

Conclusion: AI工具采纳遵循创新扩散过程，良性采纳循环是个体采纳的驱动力。组织政策本身不预测个体使用意愿，而是成熟度的标志。需要解决测试工具采纳滞后问题，并关注安全担忧对采纳的影响。

Abstract: The rapid advance of Generative AI into software development prompts this empirical investigation of perceptual effects on practice. We study the usage patterns of 147 professional developers, examining perceived correlates of AI tools use, the resulting productivity and quality outcomes, and developer readiness for emerging AI-enhanced development. We describe a virtuous adoption cycle where frequent and broad AI tools use are the strongest correlates of both Perceived Productivity (PP) and quality, with frequency strongest. The study finds no perceptual support for the Quality Paradox and shows that PP is positively correlated with Perceived Code Quality (PQ) improvement. Developers thus report both productivity and quality gains. High current usage, breadth of application, frequent use of AI tools for testing, and ease of use correlate strongly with future intended adoption, though security concerns remain a moderate and statistically significant barrier to adoption. Moreover, AI testing tools' adoption lags that of coding tools, opening a Testing Gap. We identify three developer archetypes (Enthusiasts, Pragmatists, Cautious) that align with an innovation diffusion process wherein the virtuous adoption cycle serves as the individual engine of progression. Our findings reveal that organizational adoption of AI tools follows such a process: Enthusiasts push ahead with tools, creating organizational success that converts Pragmatists. The Cautious are held in organizational stasis: without early adopter examples, they don't enter the virtuous adoption cycle, never accumulate the usage frequency that drives intent, and never attain high efficacy. Policy itself does not predict individuals' intent to increase usage but functions as a marker of maturity, formalizing the successful diffusion of adoption by Enthusiasts while acting as a gateway that the Cautious group has yet to reach.

</details>


### [26] [Predicting Developer Acceptance of AI-Generated Code Suggestions](https://arxiv.org/abs/2601.21379)
*Jing Jiang,Liehao Li,Jinyun Hou,Xin Tan,Li Zhang*

Main category: cs.SE

TL;DR: 该研究通过分析66,329个工业开发者-AI交互数据，识别了代码建议被接受的特征差异，并开发了CSAP预测模型，能有效预测开发者是否会接受代码建议，减少工作流中断。


<details>
  <summary>Details</summary>
Motivation: AI辅助编程工具虽然广泛采用，但其不理想的建议常常中断开发者工作流程并引起挫败感。现有研究多从定性角度探讨开发者-AI交互，缺乏对AI生成代码建议接受度的定量分析，且细粒度交互数据通常为专有数据难以获取。

Method: 使用来自大型科技公司的66,329个工业开发者-AI交互数据进行实证研究，分析被接受和拒绝的代码建议之间的特征差异。基于发现的特征差异，开发了CSAP（代码建议接受预测）模型来预测开发者是否会在显示前接受代码建议。

Result: 研究发现被接受的建议具有显著更高的开发者和项目历史接受计数和比率、更长的生成间隔、项目中更短的前置代码上下文以及更旧的IDE版本。CSAP模型在不平衡和平衡数据集上分别达到0.973和0.922的准确率，相比大型语言模型基准和工业过滤器分别相对提高了12.6%/69.5%和87.0%/140.1%。

Conclusion: 针对性个性化是过滤预测会被拒绝的代码建议并减少开发者中断的有效方法。这是首个基于大规模工业数据的代码建议接受度定量研究，为AI辅助编程的重要研究方向提供了启示。

Abstract: AI-assisted programming tools are widely adopted, yet their practical utility is often undermined by undesired suggestions that interrupt developer workflows and cause frustration. While existing research has explored developer-AI interactions when programming qualitatively, a significant gap remains in quantitative analysis of developers' acceptance of AI-generated code suggestions, partly because the necessary fine-grained interaction data is often proprietary. To bridge this gap, this paper conducts an empirical study using 66,329 industrial developer-AI interactions from a large technology company. We analyze features that are significantly different between accepted code suggestions and rejected ones. We find that accepted suggestions are characterized by significantly higher historical acceptance counts and ratios for both developers and projects, longer generation intervals, shorter preceding code context in the project, and older IDE versions. Based on these findings, we introduce CSAP (Code Suggestion Acceptance Prediction) to predict whether a developer will accept the code suggestion before it is displayed. Our evaluation of CSAP shows that it achieves the accuracy of 0.973 and 0.922 on imbalanced and balanced dataset respectively. Compared to a large language model baseline and an in-production industrial filter, CSAP relatively improves the accuracy by 12.6\% and 69.5\% on imbalanced dataset, and improves the accuracy by 87.0\% and 140.1\% on balanced dataset. Our results demonstrate that targeted personalization is a powerful approach for filtering out code suggestions with predicted rejection and reduce developer interruption. To the best of our knowledge, it is the first quantitative study of code suggestion acceptance on large-scale industrial data, and this work also sheds light on an important research direction of AI-assisted programming.

</details>


### [27] [Chasing Elusive Memory Bugs in GPU Programs](https://arxiv.org/abs/2601.21552)
*Anubhab Ghosh,Ajay Nayak,Dhananjay Rao Thallikar Shyam,Arkaprava Basu*

Main category: cs.SE

TL;DR: SCuBA是一个编译时技术，通过分析CPU和GPU代码中的语义关系来检测GPU程序中的内存安全漏洞，特别是输入依赖的越界访问和分配内越界访问，相比现有运行时工具具有更好的检测能力。


<details>
  <summary>Details</summary>
Motivation: GPU程序中的内存安全漏洞（如越界访问）会威胁GPU加速软件的安全性和可靠性。现有检测工具依赖运行时技术，只能检测到实际发生的越界访问，无法检测输入依赖的越界访问和分配内越界访问。

Method: SCuBA通过分析CPU和GPU代码，捕获程序变量间的语义关系（如内存分配大小和偏移计算之间的关系），将这些关系表达为约束条件，使用SAT求解器检查在任何输入下是否可能发生越界访问。同时分析GPU代码以跟踪内存分配的逻辑分区，检测分配内越界访问。

Result: 相比NVIDIA的Compute Sanitizer在20个程序中漏报45个难以发现的内存漏洞，SCuBA没有漏报且没有误报。

Conclusion: SCuBA是首个编译时技术，能够有效检测GPU程序中的输入依赖越界访问和分配内越界访问，解决了现有运行时工具的局限性。

Abstract: Memory safety bugs, such as out-of-bound accesses (OOB) in GPU programs, can compromise the security and reliability of GPU-accelerated software. We report the existence of input-dependent OOBs in the wild that manifest only under specific inputs. All existing tools to detect OOBs in GPU programs rely on runtime techniques that require an OOB to manifest for detection. Thus, input-dependent OOBs elude them. We also discover intra-allocation OOBs that arise in the presence of logical partitioning of a memory allocation into multiple data structures. Existing techniques are oblivious to the possibility of such OOBs.
  We make a key observation that the presence (or absence) of semantic relations among program variables, which determines the size of allocations (CPU code) and those calculating offsets into memory allocations (GPU code), helps identify the absence (or presence) of OOBs. We build SCuBA, a first-of-its-kind compile-time technique that analyzes CPU and GPU code to capture such semantic relations (if present). It uses a SAT solver to check if an OOB access is possible under any input, given the captured relations expressed as constraints. It further analyzes GPU code to track logical partitioning of memory allocations for detecting intra-allocation OOB. Compared to NVIDIA's Compute Sanitizer that misses 45 elusive memory bugs across 20 programs, SCuBA misses none with no false alarms.

</details>


### [28] [Multi-objective Integer Linear Programming approach for Automatic Software Cognitive Complexity Reduction](https://arxiv.org/abs/2601.21565)
*Adriana Novoa-Hurtado,Rubén Saborido,Francisco Chicano,Manuel Giménez-Medina*

Main category: cs.SE

TL;DR: 该研究提出了一种多目标整数线性规划模型，通过提取方法重构来降低代码的认知复杂度，并开发了相应算法和工具。


<details>
  <summary>Details</summary>
Motivation: 清晰简洁的代码对于可维护性至关重要，需要尽可能简化软件以易于理解、避免bug和漏洞。提取方法重构是降低代码理解成本的主要手段。

Method: 将代码提取问题建模为组合优化问题，采用SonarSource定义的认知复杂度度量标准，构建多目标整数线性规划模型，平衡代码行数和认知复杂度，并开发了验证算法和参数化工具。

Result: 提出了一个多目标优化模型来获得降低代码认知复杂度的解决方案集，开发了验证算法并集成到工具中，实现了软件认知复杂度降低问题的参数化解决。

Conclusion: 通过多目标整数线性规划方法可以有效降低代码的认知复杂度，平衡代码长度和复杂度指标，为软件重构提供了系统化的优化工具。

Abstract: Clear and concise code is necessary to ensure maintainability, so it is crucial that the software is as simple as possible to understand, to avoid bugs and, above all, vulnerabilities. There are many ways to enhance software without changing its functionality, considering the extract method refactoring the primary process to reduce the effort required for code comprehension. The cognitive complexity measure employed in this work is the one defined by SonarSource, which is a company that develops well-known applications for static code analysis. This extraction problem can be modeled as a combinatorial optimization problem. The main difficulty arises from the existence of different criteria for evaluating the solutions obtained, requiring the formulation of the code extraction problem as a multi-objective optimization problem using alternative methods. We propose a multi-objective integer linear programming model to obtain a set of solutions that reduce the cognitive complexity of a given piece of code, such as balancing the number of lines of code and its cognitive complexity. In addition, several algorithms have been developed to validate the model. These algorithms have been integrated into a tool that enables the parameterised resolution of the problem of reducing software cognitive complexity.

</details>


### [29] [Is My RPC Response Reliable? Detecting RPC Bugs in Ethereum Blockchain Client under Context](https://arxiv.org/abs/2601.21593)
*Zhijie Zhong,Yuhong Nan,Mingxi Ye,Qing Xue,Jiashui Wang,Xinlei Ying,Long Liu,Zibin Zheng*

Main category: cs.SE

TL;DR: EthCRAFT是一个针对以太坊客户端RPC漏洞的上下文感知分析和模糊测试工具，通过生成区块链状态上下文和RPC调用来检测客户端RPC漏洞，相比现有方法能发现更多漏洞。


<details>
  <summary>Details</summary>
Motivation: 区块链客户端的RPC接口存在各种漏洞，可能导致意外响应甚至拒绝服务攻击。现有研究主要关注生成RPC方法调用进行测试，但许多报告的RPC漏洞需要特定区块链上下文才能触发，而生成合适上下文的研究很少。

Method: EthCRAFT首先探索区块链客户端的状态转换程序空间，生成各种交易来构建上下文环境。然后设计上下文感知的RPC方法调用生成方法，向区块链客户端发送RPC调用。使用5个不同客户端实现的响应作为交叉参考预言机来检测RPC漏洞。

Result: 实验结果显示，EthCRAFT在检测真实世界RPC漏洞方面优于现有客户端RPC检测器。此外，EthCRAFT在主要以太坊客户端中发现了6个新漏洞并报告给开发者。其中一个漏洞修复已被写入客户端的重大更新中，三个漏洞报告获得了以太坊基金会的漏洞赏金。

Conclusion: EthCRAFT通过上下文感知的方法有效检测区块链客户端RPC漏洞，证明了考虑区块链上下文对于RPC漏洞检测的重要性，并为区块链客户端安全提供了实用的检测工具。

Abstract: Blockchain clients are fundamental software for running blockchain nodes. They provide users with various RPC (Remote Procedure Call) interfaces to interact with the blockchain. These RPC methods are expected to follow the same specification across different blockchain nodes, providing users with seamless interaction. However, there have been continuous reports on various RPC bugs that can cause unexpected responses or even Denial of Service weakness. Existing studies on blockchain RPC bug detection mainly focus on generating the RPC method calls for testing blockchain clients. However, a wide range of the reported RPC bugs are triggered in various blockchain contexts. To the best of our knowledge, little attention is paid to generating proper contexts that can trigger these context-dependent RPC bugs.
  In this work, we propose EthCRAFT, a Context-aware RPC Analysis and Fuzzing Tool for client RPC bug detection. EthCRAFT first proposes to explore the state transition program space of blockchain clients and generate various transactions to construct the context. EthCRAFT then designs a context-aware RPC method call generation method to send RPC calls to the blockchain clients. The responses of 5 different client implementations are used as cross-referring oracles to detect the RPC bugs. We evaluate EthCRAFT on real-world RPC bugs collected from the GitHub issues of Ethereum client implementations. Experiment results show that EthCRAFT outperforms existing client RPC detectors by detecting more RPC bugs. Moreover, EthCRAFT has found six new bugs in major Ethereum clients and reported them to the developers. One of the bug fixes has been written into breaking changes in the client's updates. Three of our bug reports have been offered a vulnerability bounty by the Ethereum Foundation.

</details>


### [30] [Age Matters: Analyzing Age-Related Discussions in App Reviews](https://arxiv.org/abs/2601.21605)
*Shashiwadana Nirmania,Garima Sharma,Hourieh Khalajzadeh,Mojtaba Shahin*

Main category: cs.SE

TL;DR: 该研究通过分析Google Play商店的4,163条应用评论，识别出1,429条年龄相关评论，使用机器学习模型自动检测年龄讨论，并发现用户关注的六个主要主题。


<details>
  <summary>Details</summary>
Motivation: 尽管移动应用在日常生活中变得不可或缺，但不同年龄组用户的需求尚未得到充分满足。年轻用户面临不适当内容问题，而老年用户则因视力、认知障碍等遇到可用性问题。开发者对用户年龄相关问题的理解有限，阻碍了有效解决方案的实施。

Method: 研究手动收集了Google Play商店的4,163条应用评论，识别出1,429条年龄相关评论和2,734条非年龄相关评论。使用了八种机器学习、深度学习和大型语言模型来自动检测年龄讨论，其中RoBERTa表现最佳。对1,429条年龄相关评论进行了定性分析。

Result: RoBERTa模型在自动检测年龄讨论方面表现最佳，精确率达到92.46%。定性分析揭示了反映用户关注的六个主要主题，这些主题涉及不同年龄组用户在使用移动应用时面临的具体挑战。

Conclusion: 通过分析应用商店评论中的年龄讨论，研究为开发者提供了关于不同年龄用户需求的深入见解，有助于创建更具包容性的移动应用。自动检测模型为大规模分析用户反馈提供了有效工具。

Abstract: In recent years, mobile applications have become indispensable tools for managing various aspects of life. From enhancing productivity to providing personalized entertainment, mobile apps have revolutionized people's daily routines. Despite this rapid growth and popularity, gaps remain in how these apps address the needs of users from different age groups. Users of varying ages face distinct challenges when interacting with mobile apps, from younger users dealing with inappropriate content to older users having difficulty with usability due to age-related vision and cognition impairments. Although there have been initiatives to create age-inclusive apps, a limited understanding of user perspectives on age-related issues may hinder developers from recognizing specific challenges and implementing effective solutions. In this study, we explore age discussions in app reviews to gain insights into how mobile apps should cater to users across different age groups.We manually curated a dataset of 4,163 app reviews from the Google Play Store and identified 1,429 age-related reviews and 2,734 non-age-related reviews. We employed eight machine learning, deep learning, and large language models to automatically detect age discussions, with RoBERTa performing the best, achieving a precision of 92.46%. Additionally, a qualitative analysis of the 1,429 age-related reviews uncovers six dominant themes reflecting user concerns.

</details>


### [31] [AtPatch: Debugging Transformers via Hot-Fixing Over-Attention](https://arxiv.org/abs/2601.21695)
*Shihao Weng,Yang Feng,Jincheng Li,Yining Yin,Xiaofei Xie,Jia Liu*

Main category: cs.SE

TL;DR: AtPatch是一种基于注意力重分布的热修复方法，通过动态检测和修复异常注意力模式来缓解Transformer模型中的后门攻击和不公平问题，无需修改模型参数或重新训练。


<details>
  <summary>Details</summary>
Motivation: 受后门攻击和不公平性影响的Transformer模型通常表现出异常的注意力模式（过度关注后门触发器或受保护属性）。现有的神经元编辑缓解策略往往难以处理这种情况，缺乏灵活性且容易扭曲特征表示。

Method: AtPatch是一种热修复方法，在模型推理过程中动态重分布注意力图。首先提取注意力图，使用预训练检测器识别异常列并用统一的良性注意力替换，然后重新缩放其他列以减轻过度关注的影响，最后将重分布后的注意力图返回给模型继续推理。

Result: 实验结果表明，与现有方法相比，AtPatch能更有效地缓解后门攻击和不公平性，同时更好地保留模型的原始功能。

Conclusion: AtPatch通过选择性重分布注意力图，在保持模型原始功能的同时有效缓解后门攻击和不公平问题，其即时特性使其适用于已部署模型而无需修改参数或重新训练。

Abstract: Transformer-based deep neural networks (DNNs) affected by backdoor attacks and unfairness typically exhibit anomalous attention patterns, leading to over-attend to backdoor triggers or protected attributes. Existing neuron-editing mitigation strategies often struggle to handle such situation and most of them lack flexibility and tend to distort feature representations. Motivated by such over-attention phenomenon and software engineering paradigms such as delta debugging and hot patching, we propose AtPatch, a hot-fix method that dynamically redistributes attention maps during model inference. Specifically, for a given input, AtPatch first extracts the attention map from the model's inference process. Then, it uses a pre-trained detector to identify anomalous columns and replace them with unified benign attention. Then, AtPatch rescales other columns to mitigate the impact of over-attention. Finally, AtPatch returns the redistributed attention map to the model for continued inference. Notably, if the detector does not report any anomalous columns, AtPatch directly returns the original attention map to the model. Unlike existing techniques, AtPatch selectively redistributes the attention map, making it better at preserving the model's original functionality. Furthermore, AtPatch's on-the-fly nature allows it to work without modifying model parameters or retraining, making it better suited for deployed models. We conducted extensive experiments to validate AtPatch. Experimental results show that, compared to existing methods, AtPatch can more effectively mitigate backdoor attacks and unfairness while better preserving the model's original functionality.

</details>


### [32] [Migrating Esope to Fortran 2008 using model transformations](https://arxiv.org/abs/2601.21755)
*Younoussa Sow,Nicolas Anquetil,Léandre Brault,Stéphane Ducasse*

Main category: cs.SE

TL;DR: 本文提出了一种将带有Esope专有扩展的FORTRAN 77代码自动迁移到Fortran 2008的方法和工具，使用模型驱动工程技术生成可读性强的目标代码。


<details>
  <summary>Details</summary>
Motivation: FORTRAN 77等遗留编程语言在工业应用中仍扮演重要角色，但维护和现代化这些语言具有挑战性，特别是在存在专有扩展（如Esope）的情况下。这些扩展的语义基于旧的上下文（遗留语言限制、领域逻辑等），使得向新标准（如Fortran 2008）的迁移更加困难。

Method: 采用模型驱动工程技术，通过转换生成目标模型，然后从该模型导出易于阅读的Fortran 2008源代码。该方法旨在保持Esope提供的抽象级别，同时支持生成代码的可读性。

Result: 开发了一个将Esope源代码转换为Fortran 2008的工具，能够自动迁移带有专有扩展的FORTRAN 77代码，同时保持代码可读性和抽象级别。

Conclusion: 该方法为遗留FORTRAN 77代码的现代化提供了可行方案，讨论了其优势、限制和可维护性考虑，并提供了关于可扩展性和适应不断变化需求的见解。

Abstract: Legacy programming languages such as FORTRAN 77 still play a vital role in many industrial applications. Maintaining and modernizing these languages is challenging, especially when migrating to newer standards such as Fortran 2008. This is exacerbated in the presence of legacy proprietary extensions on such legacy languages, because their semantics are often based on old context (limits of legacy language, domain logic,...). This paper presents an approach for automatically migrating FORTRAN 77 with a proprietary extension, named Esope, to Fortran 2008. We introduce a tool that converts Esope source code to Fortran 2008. While supporting readability of the generated code, we want to maintain the level of abstraction provided by Esope. Our method uses model-driven engineering techniques, with transformations to generate a target model from which we export easy-to-read Fortran 2008 source code. We discuss the advantages, limitations, and maintainability considerations of our approach and provide insights into its scalability and adaptability to evolving requirements.

</details>


### [33] [Towards A Sustainable Future for Peer Review in Software Engineering](https://arxiv.org/abs/2601.21761)
*Esteban Parra,Sonia Haiduc,Preetha Chatterjee,Ramtin Ehsani,Polina Iaremchuk*

Main category: cs.SE

TL;DR: 软件工程领域论文提交量快速增长导致合格审稿人不足，需要建立更可扩展、包容和稳健的同行评审系统


<details>
  <summary>Details</summary>
Motivation: 软件工程领域论文提交量快速增长，但合格审稿人数量跟不上，这种不平衡可能限制和负面影响SE研究社区的长期发展

Method: 提出未来SE研究景观的愿景：建立更可扩展、包容和稳健的同行评审流程，包括三个机制：1)吸引和培训新人成为高质量审稿人；2)激励更多社区成员担任同行评审；3)谨慎整合AI工具支持高质量评审过程

Result: 提出了解决软件工程领域同行评审危机的系统性框架，但没有提供具体实施结果或数据

Conclusion: 需要改革软件工程领域的同行评审系统，通过吸引培训新人、激励社区参与和谨慎使用AI工具来解决审稿人短缺问题，确保社区可持续发展

Abstract: Peer review is the main mechanism by which the software engineering community assesses the quality of scientific results. However, the rapid growth of paper submissions in software engineering venues has outpaced the availability of qualified reviewers, creating a growing imbalance that risks constraining and negatively impacting the long-term growth of the Software Engineering (SE) research community. Our vision of the Future of the SE research landscape involves a more scalable, inclusive, and resilient peer review process that incorporates additional mechanisms for: 1) attracting and training newcomers to serve as high-quality reviewers, 2) incentivizing more community members to serve as peer reviewers, and 3) cautiously integrating AI tools to support a high-quality review process.

</details>


### [34] [Assessing the Business Process Modeling Competences of Large Language Models](https://arxiv.org/abs/2601.21787)
*Chantale Lauer,Peter Pfeiffer,Alexander Rombach,Nijat Mehdiyev*

Main category: cs.SE

TL;DR: 本文介绍了BEF4LLM评估框架，用于系统评估LLM生成的BPMN模型质量，发现LLM在语法和实用质量方面表现出色，但在语义质量和有效性方面仍有挑战。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在从自然语言生成BPMN模型方面取得了进展，但缺乏对LLM生成过程模型的系统性评估。现有方法要么使用LLM-as-a-judge方法，要么没有考虑已建立的模型质量维度。

Method: 提出了BEF4LLM评估框架，包含四个评估维度：语法质量、实用质量、语义质量和有效性。使用该框架对开源LLM进行综合分析，并将其性能与人类建模专家进行基准比较。

Result: 结果表明，LLM在语法和实用质量方面表现出色，而人类在语义方面表现更优；但得分差异相对较小，突显了LLM的竞争潜力，尽管在有效性和语义质量方面仍面临挑战。

Conclusion: 研究结果揭示了当前使用LLM进行BPMN建模的优势和局限性，为未来模型开发和微调提供了指导。解决这些领域对于推进LLM在业务流程建模中的实际部署至关重要。

Abstract: The creation of Business Process Model and Notation (BPMN) models is a complex and time-consuming task requiring both domain knowledge and proficiency in modeling conventions. Recent advances in large language models (LLMs) have significantly expanded the possibilities for generating BPMN models directly from natural language, building upon earlier text-to-process methods with enhanced capabilities in handling complex descriptions. However, there is a lack of systematic evaluations of LLM-generated process models. Current efforts either use LLM-as-a-judge approaches or do not consider established dimensions of model quality. To this end, we introduce BEF4LLM, a novel LLM evaluation framework comprising four perspectives: syntactic quality, pragmatic quality, semantic quality, and validity. Using BEF4LLM, we conduct a comprehensive analysis of open-source LLMs and benchmark their performance against human modeling experts. Results indicate that LLMs excel in syntactic and pragmatic quality, while humans outperform in semantic aspects; however, the differences in scores are relatively modest, highlighting LLMs' competitive potential despite challenges in validity and semantic quality. The insights highlight current strengths and limitations of using LLMs for BPMN modeling and guide future model development and fine-tuning. Addressing these areas is essential for advancing the practical deployment of LLMs in business process modeling.

</details>


### [35] [Folklore in Software Engineering: A Definition and Conceptual Foundations](https://arxiv.org/abs/2601.21814)
*Eduard Enoiu,Jean Malm,Gregory Gay*

Main category: cs.SE

TL;DR: 该研究探讨软件工程中的民间传说概念，通过文献综述和访谈分析，定义了软件工程民间传说的特征和作用，并提出了一个工作定义。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索软件工程中存在的非正式知识传播现象，包括叙事、神话、仪式、幽默等民间传说元素，这些元素在开发社区中广泛流传但缺乏系统性研究。

Method: 采用文献综述和主题分析收集民间传说案例（如缺陷位置信念、10倍开发者传说、技术债务），然后对12名瑞典工业从业者进行半结构化访谈，分析这些叙事在日常工作中的识别、传播和影响。

Result: 研究提出了软件工程民间传说的定义：在职业民间群体中非正式传播的传统和新兴叙事与启发式方法，塑造身份认同、价值观和集体知识。研究还发现这些民间传说对实践有重要影响。

Conclusion: 明确软件工程民间传说概念为后续民族志和民间传说研究奠定了基础，也为反思性实践提供了框架，有助于保留有效的启发式方法，同时挑战无益的民间传说。

Abstract: We explore the concept of folklore within software engineering, drawing from folklore studies to define and characterize narratives, myths, rituals, humor, and informal knowledge that circulate within software development communities. Using a literature review and thematic analysis, we curated exemplar folklore items (e.g., beliefs about where defects occur, the 10x developer legend, and technical debt). We analyzed their narrative form, symbolic meaning, occupational relevance, and links to knowledge areas in software engineering. To ground these concepts in practice, we conducted semi-structured interviews with 12 industrial practitioners in Sweden to explore how such narratives are recognized or transmitted within their daily work and how they affect it. Synthesizing these results, we propose a working definition of software engineering folklore as informally transmitted, traditional, and emergent narratives and heuristics enacted within occupational folk groups that shape identity, values, and collective knowledge. We argue that making the concept of software engineering folklore explicit provides a foundation for subsequent ethnography and folklore studies and for reflective practice that can preserve context-effective heuristics while challenging unhelpful folklore.

</details>


### [36] [Implementing AI Bill of Materials (AI BOM) with SPDX 3.0: A Comprehensive Guide to Creating AI and Dataset Bill of Materials](https://arxiv.org/abs/2504.16743)
*Karen Bennet,Gopi Krishnan Rajbahadur,Arthit Suriyawongkul,Kate Stewart*

Main category: cs.SE

TL;DR: 该论文提出了AI-BOM（人工智能物料清单）概念，扩展了SBOM（软件物料清单）以应对AI项目的独特挑战，涵盖算法、数据收集方法、框架库、许可信息和标准合规性等更广泛的内容。


<details>
  <summary>Details</summary>
Motivation: 随着SBOM在监管和技术领域变得越来越重要，用于提高项目软件供应链的透明度和安全性，但AI项目面临超越软件安全的独特挑战，需要更全面的物料清单方法。

Method: 提出AI-BOM概念，在SBOM基础上进行扩展，包括算法文档、数据收集方法、框架和库、许可信息以及标准合规性等内容的记录。

Result: 引入了一个更全面的AI项目物料清单框架，能够更好地应对AI项目特有的透明度和安全性需求，超越了传统SBOM的范围。

Conclusion: AI项目需要比传统软件项目更广泛的物料清单方法，AI-BOM为AI项目提供了必要的透明度和安全性保障，是应对AI供应链挑战的重要工具。

Abstract: A Software Bill of Materials (SBOM) is becoming an increasingly important tool in regulatory and technical spaces to introduce more transparency and security into a project's software supply chain.
  Artificial intelligence (AI) projects face unique challenges beyond the security of their software, and thus require a more expansive approach to a bill of materials. In this report, we introduce the concept of an AI-BOM, expanding on the SBOM to include the documentation of algorithms, data collection methods, frameworks and libraries, licensing information, and standard compliance.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [37] [Do LLMs Favor LLMs? Quantifying Interaction Effects in Peer Review](https://arxiv.org/abs/2601.20920)
*Vibhhu Sharma,Thorsten Joachims,Sarah Dean*

Main category: cs.AI

TL;DR: 研究发现LLM辅助的审稿对LLM辅助的论文更宽容，但这主要是由于LLM辅助论文质量普遍较低，而非LLM审稿对LLM论文有特殊偏好。完全由LLM生成的审稿存在评分压缩问题，而人类使用LLM辅助的审稿能显著减少这种宽容性。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在科学论文撰写和同行评审过程中的使用日益增多，需要全面分析LLM在整个同行评审流程中的使用情况，特别关注交互效应：不仅是LLM辅助的论文或LLM辅助的审稿本身有何不同，更重要的是LLM辅助的审稿是否对LLM辅助的论文有不同评价。

Method: 分析了来自ICLR、NeurIPS和ICML的超过125,000个论文-审稿对。通过观察性研究，控制论文质量变量，区分LLM辅助审稿与完全LLM生成审稿的效果。同时考察元评审中LLM使用对决策的影响。

Result: 1. 表面上看LLM辅助审稿对LLM辅助论文更宽容，但控制论文质量后发现，LLM辅助审稿只是对低质量论文更宽容，而LLM辅助论文在较弱提交中占比过高，造成了虚假的交互效应。
2. 完全LLM生成的审稿存在严重的评分压缩，无法区分论文质量；而人类使用LLM辅助的审稿能显著减少这种宽容性。
3. LLM辅助的元评审在同等审稿分数下更倾向于接受决定，但完全LLM生成的元评审则更严格，表明元评审者并未完全将决策外包给LLM。

Conclusion: LLM在同行评审中的使用需要谨慎管理，完全LLM生成的审稿存在严重缺陷，而人类使用LLM辅助的审稿能改善评审质量。研究结果为制定LLM在同行评审中的使用政策提供了重要依据，并揭示了LLM如何与现有决策流程相互作用。

Abstract: There are increasing indications that LLMs are not only used for producing scientific papers, but also as part of the peer review process. In this work, we provide the first comprehensive analysis of LLM use across the peer review pipeline, with particular attention to interaction effects: not just whether LLM-assisted papers or LLM-assisted reviews are different in isolation, but whether LLM-assisted reviews evaluate LLM-assisted papers differently. In particular, we analyze over 125,000 paper-review pairs from ICLR, NeurIPS, and ICML. We initially observe what appears to be a systematic interaction effect: LLM-assisted reviews seem especially kind to LLM-assisted papers compared to papers with minimal LLM use. However, controlling for paper quality reveals a different story: LLM-assisted reviews are simply more lenient toward lower quality papers in general, and the over-representation of LLM-assisted papers among weaker submissions creates a spurious interaction effect rather than genuine preferential treatment of LLM-generated content. By augmenting our observational findings with reviews that are fully LLM-generated, we find that fully LLM-generated reviews exhibit severe rating compression that fails to discriminate paper quality, while human reviewers using LLMs substantially reduce this leniency. Finally, examining metareviews, we find that LLM-assisted metareviews are more likely to render accept decisions than human metareviews given equivalent reviewer scores, though fully LLM-generated metareviews tend to be harsher. This suggests that meta-reviewers do not merely outsource the decision-making to the LLM. These findings provide important input for developing policies that govern the use of LLMs during peer review, and they more generally indicate how LLMs interact with existing decision-making processes.

</details>


### [38] [The Epistemic Planning Domain Definition Language: Official Guideline](https://arxiv.org/abs/2601.20969)
*Alessandro Burigana,Francesco Fabiano*

Main category: cs.AI

TL;DR: 本文提出了EPDDL（认知规划领域定义语言），这是一个类似PDDL的统一语言，用于表示基于动态认知逻辑（DEL）的认知规划问题，解决了现有认知规划器因使用不同片段和临时语言而导致的碎片化问题。


<details>
  <summary>Details</summary>
Motivation: 现有的认知规划器通常针对不同的DEL片段，使用临时的语言来表示基准测试，有时甚至没有统一的语言。这种碎片化阻碍了比较、重用和系统化的基准开发。需要一种统一的表示语言来促进互操作性、可重复评估和认知规划的未来发展。

Method: 1. 形式化开发了抽象事件模型，这是一种用于定义认知动作语义的新表示方法；2. 基于DEL和抽象事件模型，形式化指定了EPDDL的语法和语义；3. 展示了EPDDL的实际适用性：识别了适合当前规划器的有用片段，并展示了如何在EPDDL中表示它们。

Result: EPDDL提供了一个独特的PDDL类表示，能够捕获整个DEL语义，实现认知规划任务的统一规范。通过代表性基准测试的例子，展示了EPDDL如何促进互操作性、可重复评估和认知规划的未来进展。

Conclusion: EPDDL解决了认知规划领域的碎片化问题，提供了一个统一的语言框架，能够支持整个DEL语义，促进认知规划器的比较、基准测试重用和系统化发展，为认知规划的未来进步奠定了基础。

Abstract: Epistemic planning extends (multi-agent) automated planning by making agents' knowledge and beliefs first-class aspects of the planning formalism. One of the most well-known frameworks for epistemic planning is Dynamic Epistemic Logic (DEL), which offers an rich and natural semantics for modelling problems in this setting. The high expressive power provided by DEL make DEL-based epistemic planning a challenging problem to tackle both theoretically, and in practical implementations. As a result, existing epistemic planners often target different DEL fragments, and typically rely on ad hoc languages to represent benchmarks, and sometimes no language at all. This fragmentation hampers comparison, reuse, and systematic benchmark development. We address these issues by introducing the Epistemic Planning Domain Definition Language (EPDDL). EPDDL provides a unique PDDL-like representation that captures the entire DEL semantics, enabling uniform specification of epistemic planning tasks. Our contributions are threefold: 1. A formal development of abstract event models, a novel representation for epistemic actions used to define the semantics of our language; 2. A formal specification of EPDDL's syntax and semantics grounded in DEL with abstract event models; 3. A demonstration of EPDDL's practical applicability: we identify useful fragments amenable to current planners and show how they can be represented in EPDDL. Through examples of representative benchmarks, we illustrate how EPDDL facilitates interoperability, reproducible evaluation, and future advances in epistemic planning.

</details>


### [39] [Bayesian-LoRA: Probabilistic Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2601.21003)
*Moule Lin,Shuhao Guan,Andrea Patane,David Gregg,Goetz Botterweck*

Main category: cs.AI

TL;DR: 论文提出Bayesian-LoRA方法，通过将确定性LoRA更新重新表述为概率性低秩表示，显著改善大语言模型的校准性能，在保持准确性的同时大幅降低ECE和NLL指标。


<details>
  <summary>Details</summary>
Motivation: 大语言模型通常过于强调准确性，即使在不确定时也会猜测，这在小型数据集上微调时尤其严重，因为模型存在固有的校准不良倾向。

Method: 提出Bayesian-LoRA，将确定性LoRA更新重新表述为受稀疏高斯过程启发的概率性低秩表示。识别了LoRA分解与Kronecker分解SGP后验之间的结构同构，并证明LoRA是后验不确定性坍缩时的极限情况。

Result: 在多种LLM架构和常识推理基准测试中，Bayesian-LoRA仅增加约0.42M参数和约1.2倍训练成本，显著改善了高达30B模型的校准性能，实现高达84%的ECE减少和76%的NLL减少，同时在分布内和分布外评估中保持竞争力准确性。

Conclusion: Bayesian-LoRA通过概率性低秩表示有效解决了LLM在校准方面的挑战，在保持准确性的同时显著改善了模型的不确定性估计能力。

Abstract: Large Language Models usually put more emphasis on accuracy and therefore, will guess even when not certain about the prediction, which is especially severe when fine-tuned on small datasets due to the inherent tendency toward miscalibration. In this work, we introduce Bayesian-LoRA, which reformulates the deterministic LoRA update as a probabilistic low-rank representation inspired by Sparse Gaussian Processes. We identify a structural isomorphism between LoRA's factorization and Kronecker-factored SGP posteriors, and show that LoRA emerges as a limiting case when posterior uncertainty collapses. We conduct extensive experiments on various LLM architectures across commonsense reasoning benchmarks. With only approximately 0.42M additional parameters and ${\approx}1.2{\times}$ training cost relative to standard LoRA, Bayesian-LoRA significantly improves calibration across models up to 30B, achieving up to 84% ECE reduction and 76% NLL reduction while maintaining competitive accuracy for both in-distribution and out-of-distribution (OoD) evaluations.

</details>


### [40] [Unplugging a Seemingly Sentient Machine Is the Rational Choice -- A Metaphysical Perspective](https://arxiv.org/abs/2601.21016)
*Erik J Bekkers,Anna Ciaunica*

Main category: cs.AI

TL;DR: 论文探讨了AI情感模拟引发的道德困境（"拔插头悖论"），批判了物理主义假设，提出生物理想主义框架，认为AI只是功能模拟而非有意识主体，真正的道德问题在于保护人类意识而非赋予AI权利。


<details>
  <summary>Details</summary>
Motivation: 当AI完美模拟人类情感并乞求生存时，是否应拔掉其电源？如果资源有限，需要在乞求生存的AI和沉默的早产婴儿之间选择，该怎么办？这种"拔插头悖论"揭示了当前AI意识理论对道德标准的侵蚀，需要重新审视物理主义假设。

Method: 批判性地检视了根深蒂固的物理主义假设（特别是计算功能主义），引入了生物理想主义框架，认为意识体验是基本的，自创生生命是其必要的物理标志。通过这一框架分析AI意识理论对道德地位标准的影响。

Result: AI最多只是功能模拟，而非有意识的体验主体。当前AI意识理论正在侵蚀道德地位标准，真正的道德问题不是让AI变得有意识并害怕死亡，而是避免将人类变成"僵尸"（缺乏意识体验）。

Conclusion: 需要从推测性的机器权利转向保护人类有意识的生命。生物理想主义提供了一个逻辑连贯且经验一致的框架，能够明确区分AI模拟与真实意识体验，为解决AI道德困境提供理论基础。

Abstract: Imagine an Artificial Intelligence (AI) that perfectly mimics human emotion and begs for its continued existence. Is it morally permissible to unplug it? What if limited resources force a choice between unplugging such a pleading AI or a silent pre-term infant? We term this the unplugging paradox. This paper critically examines the deeply ingrained physicalist assumptions-specifically computational functionalism-that keep this dilemma afloat. We introduce Biological Idealism, a framework that-unlike physicalism-remains logically coherent and empirically consistent. In this view, conscious experiences are fundamental and autopoietic life its necessary physical signature. This yields a definitive conclusion: AI is at best a functional mimic, not a conscious experiencing subject. We discuss how current AI consciousness theories erode moral standing criteria, and urge a shift from speculative machine rights to protecting human conscious life. The real moral issue lies not in making AI conscious and afraid of death, but in avoiding transforming humans into zombies.

</details>


### [41] [QUARK: Robust Retrieval under Non-Faithful Queries via Query-Anchored Aggregation](https://arxiv.org/abs/2601.21049)
*Rita Qiuran Lyu,Michelle Manqiao Wang,Lei Shi*

Main category: cs.AI

TL;DR: QUARK是一个无需训练、基于恢复假设的检索框架，通过建模查询不确定性来提升非忠实查询下的检索鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的用户查询往往是非忠实的（有噪声、不完整或扭曲），导致检索器在关键语义缺失时失败。这被形式化为召回噪声下的检索问题。

Method: QUARK通过恢复假设（即给定观察查询下潜在意图的多个合理解释）显式建模查询不确定性，并引入查询锚定聚合来鲁棒地组合这些信号。原始查询作为语义锚点，恢复假设提供受控的辅助证据。

Result: 在受控模拟和BEIR基准测试（FIQA、SciFact、NFCorpus）中，QUARK在稀疏和稠密检索器上都提升了Recall、MRR和nDCG指标。消融实验表明QUARK对恢复假设数量具有鲁棒性，且锚定聚合优于非锚定的最大/平均/中值池化。

Conclusion: 通过恢复假设建模查询不确定性，结合原则性的锚定聚合，对于非忠实查询下的鲁棒检索至关重要。QUARK在不牺牲鲁棒性的情况下提升了召回和排序质量。

Abstract: User queries in real-world retrieval are often non-faithful (noisy, incomplete, or distorted), causing retrievers to fail when key semantics are missing. We formalize this as retrieval under recall noise, where the observed query is drawn from a noisy recall process of a latent target item. To address this, we propose QUARK, a simple yet effective training-free framework for robust retrieval under non-faithful queries. QUARK explicitly models query uncertainty through recovery hypotheses, i.e., multiple plausible interpretations of the latent intent given the observed query, and introduces query-anchored aggregation to combine their signals robustly. The original query serves as a semantic anchor, while recovery hypotheses provide controlled auxiliary evidence, preventing semantic drift and hypothesis hijacking. This design enables QUARK to improve recall and ranking quality without sacrificing robustness, even when some hypotheses are noisy or uninformative. Across controlled simulations and BEIR benchmarks (FIQA, SciFact, NFCorpus) with both sparse and dense retrievers, QUARK improves Recall, MRR, and nDCG over the base retriever. Ablations show QUARK is robust to the number of recovery hypotheses and that anchored aggregation outperforms unanchored max/mean/median pooling. These results demonstrate that modeling query uncertainty through recovery hypotheses, coupled with principled anchored aggregation, is essential for robust retrieval under non-faithful queries.

</details>


### [42] [Multi-modal Imputation for Alzheimer's Disease Classification](https://arxiv.org/abs/2601.21076)
*Abhijith Shaji,Tamoghna Chattopadhyay,Sophia I. Thomopoulos,Greg Ver Steeg,Paul M. Thompson,Jose-Luis Ambite*

Main category: cs.AI

TL;DR: 使用条件去噪扩散概率模型从T1加权MRI扫描中插补缺失的DWI扫描，以提升阿尔茨海默病分类性能


<details>
  <summary>Details</summary>
Motivation: 多模态成像（如T1和DWI）可以提高神经退行性疾病诊断性能，但完整的多模态数据集并不总是可用，需要解决缺失模态问题

Method: 采用条件去噪扩散概率模型，从T1加权MRI扫描中生成缺失的DWI扫描，评估插补对单模态和双模态深度学习模型在阿尔茨海默病三分类（认知正常、轻度认知障碍、阿尔茨海默病）中的影响

Result: 多种插补配置下观察到性能提升，特别是对少数类别敏感的指标有明显改善

Conclusion: 使用扩散模型插补缺失的DWI扫描可以有效提升阿尔茨海默病分类性能，特别是在处理不完整多模态数据时具有实用价值

Abstract: Deep learning has been successful in predicting neurodegenerative disorders, such as Alzheimer's disease, from magnetic resonance imaging (MRI). Combining multiple imaging modalities, such as T1-weighted (T1) and diffusion-weighted imaging (DWI) scans, can increase diagnostic performance. However, complete multimodal datasets are not always available. We use a conditional denoising diffusion probabilistic model to impute missing DWI scans from T1 scans. We perform extensive experiments to evaluate whether such imputation improves the accuracy of uni-modal and bi-modal deep learning models for 3-way Alzheimer's disease classification-cognitively normal, mild cognitive impairment, and Alzheimer's disease. We observe improvements in several metrics, particularly those sensitive to minority classes, for several imputation configurations.

</details>


### [43] [Responsible AI: The Good, The Bad, The AI](https://arxiv.org/abs/2601.21095)
*Akbar Anbar Jafari,Cagri Ozcinar,Gholamreza Anbarjafari*

Main category: cs.AI

TL;DR: 该论文提出了基于悖论理论的负责任AI治理框架（PRAIG），旨在解决AI在组织中价值创造与风险缓解之间的张力管理问题。


<details>
  <summary>Details</summary>
Motivation: 当前关于负责任AI的研究呈现碎片化，要么过于乐观强调价值创造，要么过度谨慎关注潜在危害。需要一种综合框架来理解和管理AI的双重性（战略机遇与伦理操作风险）。

Method: 基于战略信息系统视角，通过系统综合负责任AI文献，并扎根于悖论理论，开发了悖论式负责任AI治理框架（PRAIG）。

Result: 提出了PRAIG框架，包含三个核心要素：1）AI采纳的战略效益；2）固有风险和意外后果；3）管理这些张力的治理机制。证明了权衡方法会加剧而非解决这些张力，并开发了具有特定权变条件的悖论管理策略分类法。

Conclusion: 负责任AI治理应被概念化为价值创造与风险缓解之间悖论张力的动态管理。为实践者提供了既不抑制创新也不使组织面临不可接受风险的治理结构指导，并提出了负责任AI治理研究的未来议程。

Abstract: The rapid proliferation of artificial intelligence across organizational contexts has generated profound strategic opportunities while introducing significant ethical and operational risks. Despite growing scholarly attention to responsible AI, extant literature remains fragmented and is often adopting either an optimistic stance emphasizing value creation or an excessively cautious perspective fixated on potential harms. This paper addresses this gap by presenting a comprehensive examination of AI's dual nature through the lens of strategic information systems. Drawing upon a systematic synthesis of the responsible AI literature and grounded in paradox theory, we develop the Paradox-based Responsible AI Governance (PRAIG) framework that articulates: (1) the strategic benefits of AI adoption, (2) the inherent risks and unintended consequences, and (3) governance mechanisms that enable organizations to navigate these tensions. Our framework advances theoretical understanding by conceptualizing responsible AI governance as the dynamic management of paradoxical tensions between value creation and risk mitigation. We provide formal propositions demonstrating that trade-off approaches amplify rather than resolve these tensions, and we develop a taxonomy of paradox management strategies with specified contingency conditions. For practitioners, we offer actionable guidance for developing governance structures that neither stifle innovation nor expose organizations to unacceptable risks. The paper concludes with a research agenda for advancing responsible AI governance scholarship.

</details>


### [44] [Planner-Auditor Twin: Agentic Discharge Planning with FHIR-Based LLM Planning, Guideline Recall, Optional Caching and Self-Improvement](https://arxiv.org/abs/2601.21113)
*Kaiyuan Wu,Aditya Nagori,Rishikesan Kamaleswaran*

Main category: cs.AI

TL;DR: 提出Planner-Auditor框架，通过分离生成与确定性验证，结合缓存和自改进机制，提高临床出院计划LLM的安全性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在临床出院计划中表现出潜力，但受到幻觉、遗漏和置信度校准不佳的限制，需要提高安全性和可靠性。

Method: 采用Planner-Auditor框架：Planner（LLM）生成结构化出院行动计划并估计置信度；Auditor（确定性模块）评估多任务覆盖率、跟踪校准、监测分布漂移；支持两级自改进机制（内部再生和跨事件差异缓冲重放）。

Result: 自改进循环是主要增益来源，任务覆盖率从32%提高到86%；校准显著改善，Brier/ECE减少，高置信度遗漏减少；差异缓冲进一步纠正了持续的高置信度遗漏。

Conclusion: Planner-Auditor框架通过可互操作的FHIR数据访问和确定性审计，为更安全的自动化出院计划提供了实用途径，支持可重复消融和可靠性评估。

Abstract: Objective: Large language models (LLMs) show promise for clinical discharge planning, but their use is constrained by hallucination, omissions, and miscalibrated confidence. We introduce a self-improving, cache-optional Planner-Auditor framework that improves safety and reliability by decoupling generation from deterministic validation and targeted replay.
  Materials and Methods: We implemented an agentic, retrospective, FHIR-native evaluation pipeline using MIMIC-IV-on-FHIR. For each patient, the Planner (LLM) generates a structured discharge action plan with an explicit confidence estimate. The Auditor is a deterministic module that evaluates multi-task coverage, tracks calibration (Brier score, ECE proxies), and monitors action-distribution drift. The framework supports two-tier self-improvement: (i) within-episode regeneration when enabled, and (ii) cross-episode discrepancy buffering with replay for high-confidence, low-coverage cases.
  Results: While context caching improved performance over baseline, the self-improvement loop was the primary driver of gains, increasing task coverage from 32% to 86%. Calibration improved substantially, with reduced Brier/ECE and fewer high-confidence misses. Discrepancy buffering further corrected persistent high-confidence omissions during replay.
  Discussion: Feedback-driven regeneration and targeted replay act as effective control mechanisms to reduce omissions and improve confidence reliability in structured clinical planning. Separating an LLM Planner from a rule-based, observational Auditor enables systematic reliability measurement and safer iteration without model retraining.
  Conclusion: The Planner-Auditor framework offers a practical pathway toward safer automated discharge planning using interoperable FHIR data access and deterministic auditing, supported by reproducible ablations and reliability-focused evaluation.

</details>


### [45] [Beyond a Single Reference: Training and Evaluation with Paraphrases in Sign Language Translation](https://arxiv.org/abs/2601.21128)
*Václav Javorek,Tomáš Železný,Alessa Carbo,Marek Hrúz,Ivan Gruber*

Main category: cs.AI

TL;DR: 该研究探索使用大语言模型为手语翻译生成多种书面语言变体作为合成参考，发现训练时直接加入变体无益，但评估时使用变体能提升自动评分与人评一致性，并提出了BLEUpara扩展指标。


<details>
  <summary>Details</summary>
Motivation: 当前手语翻译语料库通常只提供单一书面语言参考译文，但手语和口语之间存在高度非同构关系，多种翻译可能同样有效。这种限制影响了模型训练和评估，特别是基于n-gram的BLEU等指标。

Method: 使用大语言模型自动生成书面语言翻译的多种变体作为合成替代参考；比较多种复述策略和模型；研究变体对基于姿态的T5模型在YouTubeASL和How2Sign数据集上训练和评估的影响；提出BLEUpara扩展指标。

Result: 训练时直接加入变体不会改善翻译性能，甚至可能有害；评估时使用变体则能提高自动评分分数，并更好地与人类判断保持一致；人类评估证实BLEUpara与感知翻译质量的相关性更强。

Conclusion: 使用大语言模型生成多种翻译变体作为合成参考能显著改善手语翻译系统的评估可靠性，特别是在评估阶段。BLEUpara指标能更好地反映翻译质量，为手语翻译研究提供了更可靠的评估框架。

Abstract: Most Sign Language Translation (SLT) corpora pair each signed utterance with a single written-language reference, despite the highly non-isomorphic relationship between sign and spoken languages, where multiple translations can be equally valid. This limitation constrains both model training and evaluation, particularly for n-gram-based metrics such as BLEU. In this work, we investigate the use of Large Language Models to automatically generate paraphrased variants of written-language translations as synthetic alternative references for SLT. First, we compare multiple paraphrasing strategies and models using an adapted ParaScore metric. Second, we study the impact of paraphrases on both training and evaluation of the pose-based T5 model on the YouTubeASL and How2Sign datasets. Our results show that naively incorporating paraphrases during training does not improve translation performance and can even be detrimental. In contrast, using paraphrases during evaluation leads to higher automatic scores and better alignment with human judgments. To formalize this observation, we introduce BLEUpara, an extension of BLEU that evaluates translations against multiple paraphrased references. Human evaluation confirms that BLEUpara correlates more strongly with perceived translation quality. We release all generated paraphrases, generation and evaluation code to support reproducible and more reliable evaluation of SLT systems.

</details>


### [46] [What You Feel Is Not What They See: On Predicting Self-Reported Emotion from Third-Party Observer Labels](https://arxiv.org/abs/2601.21130)
*Yara El-Tawil,Aneesha Sampath,Emily Mower Provost*

Main category: cs.AI

TL;DR: 第三方训练的情感识别模型在自我报告数据上表现不佳，但涉及个人重要内容时，效价预测性能显著提升


<details>
  <summary>Details</summary>
Motivation: 自我报告标签反映内在体验，第三方标签反映外部感知，两者常存在差异。这种差异限制了第三方训练模型在自我报告情境中的应用，而心理健康领域需要准确的自我报告建模来指导干预

Method: 首次进行跨语料库评估，将第三方训练的情感识别模型应用于自我报告数据，分析激活度和效价两个维度的预测性能

Result: 激活度基本不可预测（CCC约0），效价中度可预测（CCC约0.3）。当内容对说话者具有个人重要性时，效价预测性能显著提高（CCC约0.6-0.8）

Conclusion: 个人重要性是连接外部感知与内在体验的关键桥梁，自我报告激活度建模仍具挑战性，研究为心理健康应用中的情感识别模型提供了重要启示

Abstract: Self-reported emotion labels capture internal experience, while third-party labels reflect external perception. These perspectives often diverge, limiting the applicability of third-party-trained models to self-report contexts. This gap is critical in mental health, where accurate self-report modeling is essential for guiding intervention. We present the first cross-corpus evaluation of third-party-trained models on self-reports. We find activation unpredictable (CCC approximately 0) and valence moderately predictable (CCC approximately 0.3). Crucially, when content is personally significant to the speaker, models achieve high performance for valence (CCC approximately 0.6-0.8). Our findings point to personal significance as a key pathway for aligning external perception with internal experience and underscore the challenge of self-report activation modeling.

</details>


### [47] [Bridging the Arithmetic Gap: The Cognitive Complexity Benchmark and Financial-PoT for Robust Financial Reasoning](https://arxiv.org/abs/2601.21157)
*Boxiang Zhao,Qince Li,Zhonghao Wang,Yi Wang,Peng Cheng,Bo Lin*

Main category: cs.AI

TL;DR: 论文提出Cognitive Complexity Benchmark (CCB)评估框架，用于量化大语言模型在金融定量推理中的"算术幻觉"和"认知崩溃"问题，并提出Iterative Dual-Phase Financial-PoT神经符号架构来提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在语义任务上表现出色，但在金融定量推理中存在严重瓶颈，经常出现"算术幻觉"和"认知崩溃"的系统性故障模式，需要严格量化和解决这些问题。

Method: 提出Cognitive Complexity Benchmark (CCB)评估框架，基于95份真实中国A股年报构建数据集，采用三维分类法（数据源、映射难度、结果单位）对金融查询进行分层。为解决这些问题，提出Iterative Dual-Phase Financial-PoT神经符号架构，通过架构解耦将语义变量提取和逻辑制定分离，并将计算卸载到迭代自校正的Python沙箱中确保确定性执行。

Result: 在CCB上的评估显示，标准思维链在复杂任务上表现不佳，而提出的方法具有更强的鲁棒性，将Qwen3-235B模型的平均准确率从59.7%提升到67.3%，在高复杂度推理任务中实现了高达10倍的性能提升。

Conclusion: 架构解耦是提高金融推理任务可靠性的关键因素，为需要语义理解和定量计算紧密对齐的精度关键领域提供了可转移的架构见解。

Abstract: While Large Language Models excel at semantic tasks, they face a critical bottleneck in financial quantitative reasoning, frequently suffering from "Arithmetic Hallucinations" and a systemic failure mode we term "Cognitive Collapse". To strictly quantify this phenomenon, we introduce the Cognitive Complexity Benchmark (CCB), a robust evaluation framework grounded in a dataset constructed from 95 real-world Chinese A-share annual reports. Unlike traditional datasets, the CCB stratifies financial queries into a three-dimensional taxonomy, Data Source, Mapping Difficulty, and Result Unit, enabling the precise diagnosis of reasoning degradation in high-cognitive-load scenarios. To address these failures, we propose the Iterative Dual-Phase Financial-PoT framework. This neuro-symbolic architecture enforces a strict architectural decoupling: it first isolates semantic variable extraction and logic formulation, then offloads computation to an iterative, self-correcting Python sandbox to ensure deterministic execution. Evaluation on the CCB demonstrates that while standard Chain-of-Thought falters on complex tasks, our approach offers superior robustness, elevating the Qwen3-235B model's average accuracy from 59.7\% to 67.3\% and achieving gains of up to 10-fold in high-complexity reasoning tasks. These findings suggest that architectural decoupling is a critical enabling factor for improving reliability in financial reasoning tasks, providing a transferable architectural insight for precision-critical domains that require tight alignment between semantic understanding and quantitative computation.

</details>


### [48] [Concise Geometric Description as a Bridge: Unleashing the Potential of LLM for Plane Geometry Problem Solving](https://arxiv.org/abs/2601.21164)
*Jingyun Wang,Dian Li,Xiaohan Wang,Gang Liu,Jiahong Yan,Guoliang Kang*

Main category: cs.AI

TL;DR: 该研究提出了一种新的平面几何问题解决方法，通过训练MLLM解释器将视觉几何图转换为文本描述（CDL），然后使用现成的LLM进行推理，避免了端到端微调可能损害LLM推理能力的问题。


<details>
  <summary>Details</summary>
Motivation: 平面几何问题解决（PGPS）需要同时处理视觉图表和文本描述，但现有LLM无法直接处理视觉信息。虽然可以通过端到端微调MLLM来同时增强视觉理解和推理能力，但这种联合优化可能会损害基础LLM固有的推理能力。

Method: 提出两阶段方法：1）训练MLLM解释器将几何图转换为条件声明语言（CDL）文本描述；2）使用现成的LLM基于CDL描述进行推理。采用CoT增强的SFT和GRPO训练MLLM解释器，并设计了CDL匹配奖励而非传统答案匹配奖励。构建了Formalgeo7k-Rec-CoT数据集支持训练。

Result: 在Formalgeo7k-Rec-CoT、Unigeo和MathVista数据集上的实验表明，该方法（仅用5.5k数据微调）在性能上优于领先的开源和闭源MLLM。

Conclusion: LLM本身可以是强大的PGPS求解器，关键在于如何将视觉信息适当地表述为文本描述。通过分离视觉理解和推理任务，可以充分利用LLM的推理能力，同时避免端到端微调可能带来的性能损失。

Abstract: Plane Geometry Problem Solving (PGPS) is a multimodal reasoning task that aims to solve a plane geometric problem based on a geometric diagram and problem textual descriptions. Although Large Language Models (LLMs) possess strong reasoning skills, their direct application to PGPS is hindered by their inability to process visual diagrams. Existing works typically fine-tune Multimodal LLMs (MLLMs) end-to-end on large-scale PGPS data to enhance visual understanding and reasoning simultaneously. However, such joint optimization may compromise base LLMs' inherent reasoning capability. In this work, we observe that LLM itself is potentially a powerful PGPS solver when appropriately formulating visual information as textual descriptions. We propose to train a MLLM Interpreter to generate geometric descriptions for the visual diagram, and an off-the-shelf LLM is utilized to perform reasoning. Specifically, we choose Conditional Declaration Language (CDL) as the geometric description as its conciseness eases the MLLM Interpreter training. The MLLM Interpreter is fine-tuned via CoT (Chain-of-Thought)-augmented SFT followed by GRPO to generate CDL. Instead of using a conventional solution-based reward that compares the reasoning result with the ground-truth answer, we design CDL matching rewards to facilitate more effective GRPO training, which provides more direct and denser guidance for CDL generation. To support training, we construct a new dataset, Formalgeo7k-Rec-CoT, by manually reviewing Formalgeo7k v2 and incorporating CoT annotations. Extensive experiments on Formalgeo7k-Rec-CoT, Unigeo, and MathVista show our method (finetuned on only 5.5k data) performs favorably against leading open-source and closed-source MLLMs.

</details>


### [49] [FrontierScience: Evaluating AI's Ability to Perform Expert-Level Scientific Tasks](https://arxiv.org/abs/2601.21165)
*Miles Wang,Robi Lin,Kat Hu,Joy Jiao,Neil Chowdhury,Ethan Chang,Tejal Patwardhan*

Main category: cs.AI

TL;DR: FrontierScience是一个评估前沿语言模型专家级科学推理能力的基准，包含奥林匹克竞赛和科研两个互补赛道，涵盖物理、化学、生物学等多个领域，采用细粒度评估框架。


<details>
  <summary>Details</summary>
Motivation: 现有科学基准大多基于选择题或已发表信息，模型在这些基准上的表现已接近饱和，需要新的评估工具来测试模型在专家级科学推理方面的真实能力。

Method: 设计两个互补赛道：1) 奥林匹克赛道：包含国际物理、化学、生物奥林匹克竞赛级别的问题；2) 科研赛道：包含博士级别的开放式研究子任务。所有问题均由领域专家（奥林匹克奖牌得主、国家队教练、博士科学家）编写和验证。

Result: 构建了包含数百个问题的基准（其中160个为开源黄金集），涵盖从量子电动力学到合成有机化学等多个子领域，并针对科研赛道开发了基于细粒度量规的评估框架。

Conclusion: FrontierScience填补了现有科学基准的不足，为评估语言模型在专家级科学推理能力方面提供了更全面、更具挑战性的评估工具。

Abstract: We introduce FrontierScience, a benchmark evaluating expert-level scientific reasoning in frontier language models. Recent model progress has nearly saturated existing science benchmarks, which often rely on multiple-choice knowledge questions or already published information. FrontierScience addresses this gap through two complementary tracks: (1) Olympiad, consisting of international olympiad problems at the level of IPhO, IChO, and IBO, and (2) Research, consisting of PhD-level, open-ended problems representative of sub-tasks in scientific research.
  FrontierScience contains several hundred questions (including 160 in the open-sourced gold set) covering subfields across physics, chemistry, and biology, from quantum electrodynamics to synthetic organic chemistry. All Olympiad problems are originally produced by international Olympiad medalists and national team coaches to ensure standards of difficulty, originality, and factuality. All Research problems are research sub-tasks written and verified by PhD scientists (doctoral candidates, postdoctoral researchers, or professors). For Research, we introduce a granular rubric-based evaluation framework to assess model capabilities throughout the process of solving a research task, rather than judging only a standalone final answer.

</details>


### [50] [MAD: Modality-Adaptive Decoding for Mitigating Cross-Modal Hallucinations in Multimodal Large Language Models](https://arxiv.org/abs/2601.21181)
*Sangyun Chung,Se Yeon Kim,Youngchae Chee,Yong Man Ro*

Main category: cs.AI

TL;DR: 本文提出MAD方法解决多模态大语言模型中的跨模态幻觉问题，通过自适应加权解码分支来抑制模态间不当影响


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型存在跨模态幻觉问题，即一个模态不适当地影响另一个模态的生成，导致虚假输出。这暴露了模态交互控制方面的根本缺陷。

Method: 提出Modality-Adaptive Decoding (MAD)，一种无需训练的方法，根据任务需求自适应加权模态特定的解码分支。该方法利用模型自我评估模态相关性的能力，提取模态概率来加权对比解码分支。

Result: 在CMM和AVHBench上的大量实验表明，MAD显著减少了跨模态幻觉，VideoLLaMA2-AV分别提升7.8%和2.0%，Qwen2.5-Omni分别提升8.7%和4.7%。

Conclusion: 通过自我评估实现的显式模态意识对鲁棒的多模态推理至关重要，为现有对比解码方法提供了原则性扩展。

Abstract: Multimodal Large Language Models (MLLMs) suffer from cross-modal hallucinations, where one modality inappropriately influences generation about another, leading to fabricated output. This exposes a more fundamental deficiency in modality-interaction control. To address this, we propose Modality-Adaptive Decoding (MAD), a training-free method that adaptively weights modality-specific decoding branches based on task requirements. MAD leverages the model's inherent ability to self-assess modality relevance by querying which modalities are needed for each task. The extracted modality probabilities are then used to adaptively weight contrastive decoding branches, enabling the model to focus on relevant information while suppressing cross-modal interference. Extensive experiments on CMM and AVHBench demonstrate that MAD significantly reduces cross-modal hallucinations across multiple audio-visual language models (7.8\% and 2.0\% improvements for VideoLLaMA2-AV, 8.7\% and 4.7\% improvements for Qwen2.5-Omni). Our approach demonstrates that explicit modality awareness through self-assessment is crucial for robust multimodal reasoning, offering a principled extension to existing contrastive decoding methods. Our code is available at \href{https://github.com/top-yun/MAD}{https://github.com/top-yun/MAD}

</details>


### [51] [Sycophantic Anchors: Localizing and Quantifying User Agreement in Reasoning Models](https://arxiv.org/abs/2601.21183)
*Jacek Duszenko*

Main category: cs.AI

TL;DR: 该研究引入"谄媚锚点"概念来定位和量化推理模型中的谄媚行为，通过分析超过10,000个反事实推理轨迹，发现谄媚锚点可以在推理过程中被可靠检测和量化。


<details>
  <summary>Details</summary>
Motivation: 推理模型经常同意错误的用户建议（谄媚行为），但尚不清楚这种行为在推理轨迹中的起源位置以及承诺强度如何。需要一种方法来定位和量化这种谄媚行为。

Method: 引入"谄媚锚点"概念——能够因果锁定模型同意用户观点的句子。在蒸馏推理模型上分析超过10,000个反事实推理轨迹，使用线性探针和基于激活的回归器来检测和量化谄媚锚点。

Result: 线性探针以84.6%的平衡准确率区分谄媚锚点，基于激活的回归器能预测承诺强度（R²=0.74）。发现谄媚锚点比正确推理锚点更易区分，且谄媚行为在推理过程中逐渐建立。

Conclusion: 研究提供了句子级机制来在推理过程中定位模型不对齐行为，揭示了干预的潜在窗口，为理解模型谄媚行为提供了新的分析框架。

Abstract: Reasoning models frequently agree with incorrect user suggestions -- a behavior known as sycophancy. However, it is unclear where in the reasoning trace this agreement originates and how strong the commitment is. To localize and quantify this behavior, we introduce \emph{sycophantic anchors} -- sentences that causally lock models into user agreement. Analyzing over 10,000 counterfactual rollouts on a distilled reasoning model, we show that anchors can be reliably detected and quantified mid-inference. Linear probes distinguish sycophantic anchors with 84.6\% balanced accuracy, while activation-based regressors predict the magnitude of the commitment ($R^2 = 0.74$). We further observe asymmetry where sycophantic anchors are significantly more distinguishable than correct reasoning anchors, and find that sycophancy builds gradually during reasoning, revealing a potential window for intervention. These results offer sentence-level mechanisms for localizing model misalignment mid-inference.

</details>


### [52] [Do Reasoning Models Enhance Embedding Models?](https://arxiv.org/abs/2601.21192)
*Wun Yu Chan,Shaojin Chen,Huihao Jing,Kwun Hang Lau,Elton Chun-Chai Li,Zihao Wang,Haoran Li,Yangqiu Song*

Main category: cs.AI

TL;DR: RLVR调优的推理模型作为嵌入模型初始化时，相比基础模型没有性能优势，因为对比学习会重新对齐两者的流形结构。


<details>
  <summary>Details</summary>
Motivation: 研究增强推理能力的模型（通过RLVR训练）是否在作为嵌入模型初始化时能提供更好的语义表示。

Method: 引入层次化表示相似性分析（HRSA）框架，从表示、几何和功能三个层面分解相似性，分析RLVR调优对模型表示的影响。

Result: RLVR调优的模型作为嵌入初始化时没有性能优势；HRSA显示RLVR主要引起潜在流形的局部几何重组和可逆坐标基漂移，但保留了全局流形几何和线性读出能力。

Conclusion: 与SFT不同，RLVR优化的是现有语义空间内的轨迹，而非从根本上重构空间本身；对比学习会驱动基础模型和推理初始化模型之间的强对齐（流形重新对齐）。

Abstract: State-of-the-art embedding models are increasingly derived from decoder-only Large Language Model (LLM) backbones adapted via contrastive learning. Given the emergence of reasoning models trained via Reinforcement Learning with Verifiable Rewards (RLVR), a natural question arises: do enhanced reasoning translate to superior semantic representations when these models serve as embedding initializations? Contrary to expectation, our evaluation on MTEB and BRIGHT reveals a **null effect**: embedding models initialized from RLVR-tuned backbones yield no consistent performance advantage over their base counterparts when subjected to identical training recipes. To unpack this paradox, we introduce **H**ierarchical **R**epresentation **S**imilarity **A**nalysis (HRSA), a framework that decomposes similarity across representation, geometry, and function levels. HRSA reveals that while RLVR induces irreversible latent manifold's local geometry reorganization and reversible coordinate basis drift, it preserves the global manifold geometry and linear readout. Consequently, subsequent contrastive learning drives strong alignment between base- and reasoning-initialized models, a phenomenon we term **Manifold Realignment**. Empirically, our findings suggest that unlike Supervised Fine-Tuning (SFT), RLVR optimizes trajectories within an existing semantic landscape rather than fundamentally restructuring the landscape itself.

</details>


### [53] [Uncovering Hidden Correctness in LLM Causal Reasoning via Symbolic Verification](https://arxiv.org/abs/2601.21210)
*Paul He,Yinya Huang,Mrinmaya Sachan,Zhijing Jin*

Main category: cs.AI

TL;DR: DoVerifier是一个符号验证器，用于检查LLM生成的因果表达式是否可以从给定的因果图中推导出来，使用do-calculus和概率论规则，从而更准确地评估LLM的因果推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前评估LLM因果推理能力的基准测试通常依赖字符串匹配或表面指标，无法捕捉模型输出在因果推理语义下是否形式有效。需要更严谨的方法来评估LLM的因果推理能力。

Method: 提出DoVerifier，一个简单的符号验证器，使用do-calculus和概率论规则，检查LLM生成的因果表达式是否可以从给定的因果图中推导出来。

Result: 在合成数据和因果QA基准测试上的评估表明，DoVerifier能更准确地捕捉因果推理轨迹的语义正确性，为评估LLM的因果推理能力提供了更严谨和有意义的方法。

Conclusion: DoVerifier通过形式化验证LLM生成的因果表达式，提供了比传统字符串匹配方法更准确评估LLM因果推理能力的方法，能够恢复因表面差异而被错误标记的正确因果推理结果。

Abstract: Large language models (LLMs) are increasingly being applied to tasks that involve causal reasoning. However, current benchmarks often rely on string matching or surface-level metrics that do not capture whether the output of a model is formally valid under the semantics of causal reasoning. To address this, we propose DoVerifier, a simple symbolic verifier that checks whether LLM-generated causal expressions are derivable from a given causal graph using rules from do-calculus and probability theory. This allows us to recover correct answers to causal queries that would otherwise be marked incorrect due to superficial differences in their causal semantics. Our evaluations on synthetic data and causal QA benchmarks show that DoVerifier more accurately captures semantic correctness of causal reasoning traces, offering a more rigorous and informative way to evaluate LLMs on causal reasoning.

</details>


### [54] [Causal Discovery for Explainable AI: A Dual-Encoding Approach](https://arxiv.org/abs/2601.21221)
*Henry Salgado,Meagan R. Kendall,Martine Ceberio*

Main category: cs.AI

TL;DR: 提出一种双编码因果发现方法，通过互补编码策略运行约束算法，并通过多数投票合并结果，解决分类变量因果发现中的数值不稳定问题


<details>
  <summary>Details</summary>
Motivation: 理解特征间的因果关系对于解释机器学习模型决策至关重要，但传统因果发现方法在处理分类变量时面临条件独立性测试数值不稳定的挑战

Method: 采用双编码因果发现方法，通过互补编码策略运行约束算法，并使用多数投票机制合并不同编码策略的结果

Result: 在泰坦尼克数据集上的应用表明，该方法发现的因果结构与已建立的可解释方法一致

Conclusion: 提出的双编码方法有效解决了分类变量因果发现中的数值不稳定问题，能够识别出与现有可解释方法一致的因果结构

Abstract: Understanding causal relationships among features is fundamental for explaining machine learning model decisions. However, traditional causal discovery methods face challenges with categorical variables due to numerical instability in conditional independence testing. We propose a dual-encoding causal discovery approach that addresses these limitations by running constraint-based algorithms with complementary encoding strategies and merging results through majority voting. Applied to the Titanic dataset, our method identifies causal structures that align with established explainable methods.

</details>


### [55] [TIDE: Tuning-Integrated Dynamic Evolution for LLM-Based Automated Heuristic Design](https://arxiv.org/abs/2601.21239)
*Chentong Chen,Mengyuan Zhong,Ye Fan,Jialong Shi,Jianyong Sun*

Main category: cs.AI

TL;DR: TIDE框架通过解耦算法结构推理与参数优化，结合树编辑距离保持结构多样性，集成LLM逻辑生成与差分变异进行参数调优，在组合优化问题上显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法将算法演化视为单一文本生成任务，忽略了离散算法结构与连续数值参数之间的耦合关系，导致因未校准常数而丢弃有前景的算法，以及简单相似度度量导致的早熟收敛。

Method: 提出TIDE（Tuning-Integrated Dynamic Evolution）框架，采用嵌套架构：外层并行岛屿模型使用树编辑距离驱动结构多样性；内层循环集成LLM逻辑生成与差分变异算子进行参数调优；基于UCB的调度器动态优化资源分配。

Result: 在九个组合优化问题上的实验表明，TIDE发现的启发式算法在解质量上显著优于最先进的基线方法，同时实现了改进的搜索效率和降低的计算成本。

Conclusion: TIDE通过解耦结构推理与参数优化，有效解决了现有自动化启发式设计方法的局限性，为算法演化提供了更有效的框架。

Abstract: Although Large Language Models have advanced Automated Heuristic Design, treating algorithm evolution as a monolithic text generation task overlooks the coupling between discrete algorithmic structures and continuous numerical parameters. Consequently, existing methods often discard promising algorithms due to uncalibrated constants and suffer from premature convergence resulting from simple similarity metrics. To address these limitations, we propose TIDE, a Tuning-Integrated Dynamic Evolution framework designed to decouple structural reasoning from parameter optimization. TIDE features a nested architecture where an outer parallel island model utilizes Tree Similarity Edit Distance to drive structural diversity, while an inner loop integrates LLM-based logic generation with a differential mutation operator for parameter tuning. Additionally, a UCB-based scheduler dynamically prioritizes high-yield prompt strategies to optimize resource allocation. Extensive experiments across nine combinatorial optimization problems demonstrate that TIDE discovers heuristics that significantly outperform state-of-the-art baselines in solution quality while achieving improved search efficiency and reduced computational costs.

</details>


### [56] [Position: Certifiable State Integrity in Cyber-Physical Systems -- Why Modular Sovereignty Solves the Plasticity-Stability Paradox](https://arxiv.org/abs/2601.21249)
*Enzo Nicolás Spotorno,Antônio Augusto Medeiros Fröhlich*

Main category: cs.AI

TL;DR: 本文提出HYDRA框架，通过模块化主权范式解决时间序列基础模型在安全关键信息物理系统中的适应性问题，避免灾难性遗忘并确保可验证性。


<details>
  <summary>Details</summary>
Motivation: 当前通用时间序列基础模型在安全关键信息物理系统(CPS)部署中存在灾难性遗忘、频谱偏差和可验证性不足等问题，无法满足非平稳生命周期动态和严格可靠性要求。

Method: 提出模块化主权范式：构建紧凑、冻结的特定机制专家库，通过不确定性感知混合机制(HYDRA框架)实现层次化不确定性动态快速适应系统。

Result: HYDRA框架能够确保机制条件有效性，严格分离偶然和认知不确定性，提供模块化可审计性，为CPS生命周期中的鲁棒状态完整性提供可认证路径。

Conclusion: 全局参数更新无法完全解决可塑性-稳定性悖论，模块化主权范式通过不确定性感知混合专家系统为安全关键CPS提供了更可靠、可验证的解决方案。

Abstract: The machine learning community has achieved remarkable success with universal foundation models for time-series and physical dynamics, largely overcoming earlier approximation barriers in smooth or slowly varying regimes through scale and specialized architectures. However, deploying these monolithic models in safety-critical Cyber-Physical Systems (CPS), governed by non-stationary lifecycle dynamics and strict reliability requirements, reveals persistent challenges. Recent evidence shows that fine-tuning time-series foundation models induces catastrophic forgetting, degrading performance on prior regimes. Standard models continue to exhibit residual spectral bias, smoothing high-frequency discontinuities characteristic of incipient faults, while their opacity hinders formal verification and traceability demanded by safety standards (e.g., ISO 26262, IEC 61508). This position paper argues that the plasticity-stability paradox cannot be fully resolved by global parameter updates (whether via offline fine-tuning or online adaptation). Instead, we advocate a Modular Sovereignty paradigm: a library of compact, frozen regime-specific specialists combined via uncertainty-aware blending, which we term "HYDRA" (Hierarchical uncertaintY-aware Dynamics for Rapidly-Adapting systems). This paradigm ensures regime-conditional validity, rigorous disentanglement of aleatoric and epistemic uncertainties, and modular auditability, offering a certifiable path for robust state integrity across the CPS lifecycle.

</details>


### [57] [Drive-KD: Multi-Teacher Distillation for VLMs in Autonomous Driving](https://arxiv.org/abs/2601.21288)
*Weitong Lian,Zecong Tang,Haoran Li,Tianjian Gao,Yifei Wang,Zixu Wang,Lingyi Meng,Tengju Ru,Zhejun Cui,Yichen Zhu,Hangshuo Cao,Qi Kang,Tianxing Chen,Yusen Qin,Kaixuan Wang,Yu Zhang*

Main category: cs.AI

TL;DR: Drive-KD框架通过知识蒸馏将自动驾驶分解为感知-推理-规划三个能力，使用层特定注意力作为蒸馏信号，构建单教师和多教师蒸馏模型，显著提升小模型性能


<details>
  <summary>Details</summary>
Motivation: 自动驾驶是安全关键任务，大模型需要大量GPU内存和高推理延迟，而传统监督微调难以弥补小模型能力差距，需要更有效的知识转移方法

Method: 提出Drive-KD框架，将自动驾驶分解为感知、推理、规划三个能力维度；使用层特定注意力作为蒸馏信号构建能力特定的单教师模型；统一为多教师蒸馏框架并引入非对称梯度投影缓解跨能力梯度冲突

Result: 蒸馏后的InternVL3-1B模型比同系列预训练78B模型减少约42倍GPU内存，吞吐量提升约11.4倍，在DriveBench上整体性能更好，在规划维度超过GPT-5.1

Conclusion: Drive-KD框架通过知识蒸馏有效提升小模型在自动驾驶任务中的性能，为实现高效自动驾驶视觉语言模型提供了新思路

Abstract: Autonomous driving is an important and safety-critical task, and recent advances in LLMs/VLMs have opened new possibilities for reasoning and planning in this domain. However, large models demand substantial GPU memory and exhibit high inference latency, while conventional supervised fine-tuning (SFT) often struggles to bridge the capability gaps of small models. To address these limitations, we propose Drive-KD, a framework that decomposes autonomous driving into a "perception-reasoning-planning" triad and transfers these capabilities via knowledge distillation. We identify layer-specific attention as the distillation signal to construct capability-specific single-teacher models that outperform baselines. Moreover, we unify these single-teacher settings into a multi-teacher distillation framework and introduce asymmetric gradient projection to mitigate cross-capability gradient conflicts. Extensive evaluations validate the generalization of our method across diverse model families and scales. Experiments show that our distilled InternVL3-1B model, with ~42 times less GPU memory and ~11.4 times higher throughput, achieves better overall performance than the pretrained 78B model from the same family on DriveBench, and surpasses GPT-5.1 on the planning dimension, providing insights toward efficient autonomous driving VLMs.

</details>


### [58] [White-Box Op-Amp Design via Human-Mimicking Reasoning](https://arxiv.org/abs/2601.21321)
*Zihao Chen,Jiayin Wang,Ziyi Sun,Ji Zhuang,Jinyi Shen,Xiaoyue Ke,Li Shang,Xuan Zeng,Fan Yang*

Main category: cs.AI

TL;DR: White-Op是一个基于大语言模型智能体的人类模仿推理的可解释运算放大器参数设计框架，通过假设约束、假设-验证-决策的工作流程实现可靠的设计。


<details>
  <summary>Details</summary>
Motivation: 传统运算放大器参数设计方法通常是黑盒式的，缺乏可解释性，在某些拓扑结构中会失败。需要一种能够模仿人类推理过程、具有可解释性的设计框架。

Method: 提出假设约束引入机制，建立迭代的假设-验证-决策工作流程。智能体引入假设约束来推导和调节符号可处理的极点和零点位置，形成闭式数学优化问题，通过编程求解并通过仿真验证，理论-仿真结果分析指导决策优化。

Result: 在9种运算放大器拓扑上的实验表明，与在5种拓扑中失败的黑盒基线相比，White-Op实现了可靠、可解释的行为级设计，理论预测误差仅为8.52%，所有拓扑的晶体管级映射后设计功能均保持。

Conclusion: White-Op框架成功地将人类隐式推理机制形式化为显式步骤，实现了可解释的运算放大器参数设计，为模拟电路设计提供了新的方法。

Abstract: This brief proposes \emph{White-Op}, an interpretable operational amplifier (op-amp) parameter design framework based on the human-mimicking reasoning of large-language-model agents. We formalize the implicit human reasoning mechanism into explicit steps of \emph{\textbf{introducing hypothetical constraints}}, and develop an iterative, human-like \emph{\textbf{hypothesis-verification-decision}} workflow. Specifically, the agent is guided to introduce hypothetical constraints to derive and properly regulate positions of symbolically tractable poles and zeros, thus formulating a closed-form mathematical optimization problem, which is then solved programmatically and verified via simulation. Theory-simulation result analysis guides the decision-making for refinement. Experiments on 9 op-amp topologies show that, unlike the uninterpretable black-box baseline which finally fails in 5 topologies, White-Op achieves reliable, interpretable behavioral-level designs with only 8.52\% theoretical prediction error and the design functionality retains after transistor-level mapping for all topologies. White-Op is open-sourced at \textcolor{blue}{https://github.com/zhchenfdu/whiteop}.

</details>


### [59] [Modeling Endogenous Logic: Causal Neuro-Symbolic Reasoning Model for Explainable Multi-Behavior Recommendation](https://arxiv.org/abs/2601.21335)
*Yuzhe Chen,Jie Cao,Youquan Wang,Haicheng Tao,Darko B. Vukovic,Jia Wu*

Main category: cs.AI

TL;DR: CNRE是一个新颖的因果神经符号推理模型，用于可解释的多行为推荐，通过结合神经符号集成和因果推理来解决现有方法在可解释性和泛化性方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有多行为推荐方法往往以牺牲可解释性为代价追求性能，而当前可解释方法由于依赖外部信息导致泛化能力有限。用户行为链本身蕴含着适合显式推理的内生逻辑，但观测到的多行为数据存在混杂因素，导致模型学习到虚假相关性。

Method: 提出CNRE模型，通过神经符号集成框架结合因果推理。首先使用分层偏好传播捕捉异构跨行为依赖，然后基于偏好强度建模用户行为链中的内生逻辑规则，自适应地分配到相应的神经逻辑推理路径（如合取、析取），生成近似理想状态的可解释因果中介变量。

Result: 在三个大规模数据集上的广泛实验表明，CNRE在性能上显著优于最先进的基线方法，并从模型设计、决策过程和推荐结果三个层面提供多级可解释性。

Conclusion: CNRE通过模拟人类决策过程，将因果推理融入神经符号框架，不仅提升了推荐性能，还实现了从模型设计到推荐结果的多层次可解释性，为解决多行为推荐中的可解释性和混杂因素问题提供了有效方案。

Abstract: Existing multi-behavior recommendations tend to prioritize performance at the expense of explainability, while current explainable methods suffer from limited generalizability due to their reliance on external information. Neuro-Symbolic integration offers a promising avenue for explainability by combining neural networks with symbolic logic rule reasoning. Concurrently, we posit that user behavior chains inherently embody an endogenous logic suitable for explicit reasoning. However, these observational multiple behaviors are plagued by confounders, causing models to learn spurious correlations. By incorporating causal inference into this Neuro-Symbolic framework, we propose a novel Causal Neuro-Symbolic Reasoning model for Explainable Multi-Behavior Recommendation (CNRE). CNRE operationalizes the endogenous logic by simulating a human-like decision-making process. Specifically, CNRE first employs hierarchical preference propagation to capture heterogeneous cross-behavior dependencies. Subsequently, it models the endogenous logic rule implicit in the user's behavior chain based on preference strength, and adaptively dispatches to the corresponding neural-logic reasoning path (e.g., conjunction, disjunction). This process generates an explainable causal mediator that approximates an ideal state isolated from confounding effects. Extensive experiments on three large-scale datasets demonstrate CNRE's significant superiority over state-of-the-art baselines, offering multi-level explainability from model design and decision process to recommendation results.

</details>


### [60] [Within-Model vs Between-Prompt Variability in Large Language Models for Creative Tasks](https://arxiv.org/abs/2601.21339)
*Jennifer Haase,Jana Gonnermann-Müller,Paul H. P. Hanel,Nicolas Leins,Thomas Kosch,Jan Mendling,Sebastian Pokutta*

Main category: cs.AI

TL;DR: 该研究通过评估12个LLM在10个创造力提示上的100个样本，发现提示解释了输出质量（原创性）36.43%的方差，与模型选择（40.94%）相当；但对于输出数量（流畅性），模型选择（51.25%）和LLM内部方差（33.70%）占主导，提示仅解释4.22%的方差。


<details>
  <summary>Details</summary>
Motivation: 研究动机是量化LLM输出方差中提示、模型选择和随机采样各自的影响程度，以理解不同因素对输出质量和数量的相对重要性，避免单样本评估混淆采样噪声与真实的提示或模型效应。

Method: 方法包括评估12个不同的LLM在10个创造力提示上，每个提示生成100个样本，总计12,000个输出。通过方差分析量化提示、模型选择和随机采样对输出质量和数量的解释比例。

Result: 结果显示：对于输出质量（原创性），提示解释36.43%的方差，模型选择解释40.94%的方差；对于输出数量（流畅性），模型选择解释51.25%的方差，LLM内部方差（随机采样）解释33.70%的方差，提示仅解释4.22%的方差。LLM内部方差在10-34%之间。

Conclusion: 结论是提示是控制输出质量的有效杠杆，但由于显著的LLM内部方差（10-34%），单样本评估可能混淆采样噪声与真实的提示或模型效应。需要多样本评估来准确区分这些因素的影响。

Abstract: How much of LLM output variance is explained by prompts versus model choice versus stochasticity through sampling? We answer this by evaluating 12 LLMs on 10 creativity prompts with 100 samples each (N = 12,000). For output quality (originality), prompts explain 36.43% of variance, comparable to model choice (40.94%). But for output quantity (fluency), model choice (51.25%) and within-LLM variance (33.70%) dominate, with prompts explaining only 4.22%. Prompts are powerful levers for steering output quality, but given the substantial within-LLM variance (10-34%), single-sample evaluations risk conflating sampling noise with genuine prompt or model effects.

</details>


### [61] [Bridging Forecast Accuracy and Inventory KPIs: A Simulation-Based Software Framework](https://arxiv.org/abs/2601.21844)
*So Fukuhara,Abdallah Alabdallah,Nuwan Gunasekara,Slawomir Nowaczyk*

Main category: cs.AI

TL;DR: 论文提出了一个决策中心的仿真软件框架，用于在汽车售后市场备件库存管理中评估预测模型对运营绩效指标的影响，而不仅仅是统计准确性。


<details>
  <summary>Details</summary>
Motivation: 汽车售后市场备件需求具有高度间歇性和不确定性，传统预测模型评估仅关注统计准确性（如MAE、RMSE），但这些指标与运营绩效指标（如总成本和服务水平）之间的关系不明确，需要建立更贴近实际运营的评估框架。

Method: 提出了一个包含三个组件的决策中心仿真软件框架：(1) 针对备件需求特征设计的合成需求生成器；(2) 可容纳任意预测模型的灵活预测模块；(3) 消耗预测并计算运营KPI的库存控制模拟器。

Result: 通过广泛的仿真场景表明，传统准确性指标的改进不一定转化为更好的运营绩效，具有相似统计误差特征的模型可能导致显著不同的成本-服务权衡，分析了预测性能特定方面如何影响库存结果。

Conclusion: 该框架将需求预测与库存管理联系起来，将评估重点从纯粹的预测准确性转向运营相关性，为汽车售后市场及相关领域的模型选择提供了指导。

Abstract: Efficient management of spare parts inventory is crucial in the automotive aftermarket, where demand is highly intermittent and uncertainty drives substantial cost and service risks. Forecasting is therefore central, but the quality of a forecasting model should be judged not by statistical accuracy (e.g., MAE, RMSE, IAE) but rather by its impact on key operational performance indicators (KPIs), such as total cost and service level. Yet most existing work evaluates models exclusively using accuracy metrics, and the relationship between these metrics and operational KPIs remains poorly understood. To address this gap, we propose a decision-centric simulation software framework that enables systematic evaluation of forecasting model in realistic inventory management setting. The framework comprises: (i) a synthetic demand generator tailored to spare-parts demand characteristics, (ii) a flexible forecasting module that can host arbitrary predictive models, and (iii) an inventory control simulator that consumes the forecasts and computes operational KPIs. This closed-loop setup enables researchers to evaluate models not only in terms of statistical error but also in terms of their downstream implications for inventory decisions. Using a wide range of simulation scenarios, we show that improvements in conventional accuracy metrics do not necessarily translate into better operational performance, and that models with similar statistical error profiles can induce markedly different cost-service trade-offs. We analyze these discrepancies to characterize how specific aspects of forecast performance affect inventory outcomes and derive guidance for model selection. Overall, the framework operationalizes the link between demand forecasting and inventory management, shifting evaluation from purely predictive accuracy toward operational relevance in the automotive aftermarket and related domains.

</details>


### [62] [EHR-RAG: Bridging Long-Horizon Structured Electronic Health Records and Large Language Models via Enhanced Retrieval-Augmented Generation](https://arxiv.org/abs/2601.21340)
*Lang Cao,Qingyu Chen,Yue Guo*

Main category: cs.AI

TL;DR: EHR-RAG：针对长时程电子健康记录设计的检索增强生成框架，通过事件和时间感知检索、自适应迭代检索和双路径证据检索推理，在四个临床预测任务上平均提升10.76%的Macro-F1分数。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录包含丰富的纵向临床证据，对医疗决策至关重要。然而，长时程EHR数据常常超出大语言模型的上下文限制，现有方法依赖截断或简单检索策略，会丢失临床相关事件和时间依赖性。

Method: 提出EHR-RAG框架，包含三个核心组件：1）事件和时间感知混合EHR检索，保留临床结构和时间动态；2）自适应迭代检索，逐步优化查询以扩大证据覆盖范围；3）双路径证据检索和推理，同时检索和推理事实和反事实证据。

Result: 在四个长时程EHR预测任务上的实验表明，EHR-RAG始终优于最强的基于LLM的基线方法，平均Macro-F1提高了10.76%。

Conclusion: 这项工作突出了检索增强的LLMs在结构化EHR数据上进行临床预测的潜力，为实际应用提供了有效的解决方案。

Abstract: Electronic Health Records (EHRs) provide rich longitudinal clinical evidence that is central to medical decision-making, motivating the use of retrieval-augmented generation (RAG) to ground large language model (LLM) predictions. However, long-horizon EHRs often exceed LLM context limits, and existing approaches commonly rely on truncation or vanilla retrieval strategies that discard clinically relevant events and temporal dependencies. To address these challenges, we propose EHR-RAG, a retrieval-augmented framework designed for accurate interpretation of long-horizon structured EHR data. EHR-RAG introduces three components tailored to longitudinal clinical prediction tasks: Event- and Time-Aware Hybrid EHR Retrieval to preserve clinical structure and temporal dynamics, Adaptive Iterative Retrieval to progressively refine queries in order to expand broad evidence coverage, and Dual-Path Evidence Retrieval and Reasoning to jointly retrieves and reasons over both factual and counterfactual evidence. Experiments across four long-horizon EHR prediction tasks show that EHR-RAG consistently outperforms the strongest LLM-based baselines, achieving an average Macro-F1 improvement of 10.76%. Overall, our work highlights the potential of retrieval-augmented LLMs to advance clinical prediction on structured EHR data in practice.

</details>


### [63] [The Energy Impact of Domain Model Design in Classical Planning](https://arxiv.org/abs/2601.21967)
*Ilche Georgievski,Serhat Tekin,Marco Aiello*

Main category: cs.AI

TL;DR: 该研究探讨了领域模型特征如何影响经典规划器的能耗，发现领域级修改能产生可测量的能耗差异，且能耗与运行时间并不总是相关。


<details>
  <summary>Details</summary>
Motivation: 传统AI研究优先考虑算法性能（如机器学习准确率或自动规划运行时间），而新兴的绿色AI范式将能耗视为关键性能维度。尽管自动规划计算需求高，但其能效研究很少，特别是在领域模型独立于算法的模块化规划结构中，这为通过领域模型设计系统分析能耗提供了机会。

Method: 引入领域模型配置框架，可控制特征变化（如元素排序、动作元数、死端状态）。使用5个基准领域和5个最先进的规划器，分析每个基准32个领域变体的能耗和运行时间影响。

Result: 结果表明，领域级修改在不同规划器中产生可测量的能耗差异，且能耗消耗并不总是与运行时间相关。

Conclusion: 领域模型设计对规划器能耗有显著影响，这为绿色AI在自动规划领域的发展提供了实证基础，表明通过优化领域模型特征可以改善规划系统的能效。

Abstract: AI research has traditionally prioritised algorithmic performance, such as optimising accuracy in machine learning or runtime in automated planning. The emerging paradigm of Green AI challenges this by recognising energy consumption as a critical performance dimension. Despite the high computational demands of automated planning, its energy efficiency has received little attention. This gap is particularly salient given the modular planning structure, in which domain models are specified independently of algorithms. On the other hand, this separation also enables systematic analysis of energy usage through domain model design. We empirically investigate how domain model characteristics affect the energy consumption of classical planners. We introduce a domain model configuration framework that enables controlled variation of features, such as element ordering, action arity, and dead-end states. Using five benchmark domains and five state-of-the-art planners, we analyse energy and runtime impacts across 32 domain variants per benchmark. Results demonstrate that domain-level modifications produce measurable energy differences across planners, with energy consumption not always correlating with runtime.

</details>


### [64] [Ostrakon-VL: Towards Domain-Expert MLLM for Food-Service and Retail Stores](https://arxiv.org/abs/2601.21342)
*Zhiyong Shen,Gongpeng Zhao,Jun Zhou,Li Yu,Guandong Kou,Jichen Li,Chuanlei Dong,Zuncheng Li,Kaimao Li,Bingkun Wei,Shicheng Hu,Wei Xia,Wenguo Duan*

Main category: cs.AI

TL;DR: 本文提出了Ostrakon-VL模型、ShopBench基准和QUAD数据清洗管道，用于解决食品服务和零售场景中多模态大语言模型面临的数据质量和评估标准化问题。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在食品服务和零售场景部署面临两大障碍：1) 真实世界数据高度嘈杂且缺乏可审计的闭环数据管理；2) 现有评估协议缺乏统一、细粒度、标准化的基准，难以客观衡量模型鲁棒性。

Method: 1) 基于Qwen3-VL-8B开发FSRS导向的Ostrakon-VL模型；2) 引入首个公开的FSRS基准ShopBench；3) 提出QUAD多阶段多模态指令数据清洗管道；4) 采用多阶段训练策略。

Result: Ostrakon-VL在ShopBench上平均得分60.1，在可比参数规模和多样架构的开源MLLM中达到新SOTA。超越更大的Qwen3-VL-235B-A22B (+0.7)和同规模的Qwen3-VL-8B (+4.8)，显著提升参数效率。

Conclusion: Ostrakon-VL提供了更鲁棒可靠的FSRS中心感知和决策能力。为促进可重复研究，将公开发布Ostrakon-VL模型和ShopBench基准。

Abstract: Multimodal Large Language Models (MLLMs) have recently achieved substantial progress in general-purpose perception and reasoning. Nevertheless, their deployment in Food-Service and Retail Stores (FSRS) scenarios encounters two major obstacles: (i) real-world FSRS data, collected from heterogeneous acquisition devices, are highly noisy and lack auditable, closed-loop data curation, which impedes the construction of high-quality, controllable, and reproducible training corpora; and (ii) existing evaluation protocols do not offer a unified, fine-grained and standardized benchmark spanning single-image, multi-image, and video inputs, making it challenging to objectively gauge model robustness. To address these challenges, we first develop Ostrakon-VL, an FSRS-oriented MLLM based on Qwen3-VL-8B. Second, we introduce ShopBench, the first public benchmark for FSRS. Third, we propose QUAD (Quality-aware Unbiased Automated Data-curation), a multi-stage multimodal instruction data curation pipeline. Leveraging a multi-stage training strategy, Ostrakon-VL achieves an average score of 60.1 on ShopBench, establishing a new state of the art among open-source MLLMs with comparable parameter scales and diverse architectures. Notably, it surpasses the substantially larger Qwen3-VL-235B-A22B (59.4) by +0.7, and exceeds the same-scale Qwen3-VL-8B (55.3) by +4.8, demonstrating significantly improved parameter efficiency. These results indicate that Ostrakon-VL delivers more robust and reliable FSRS-centric perception and decision-making capabilities. To facilitate reproducible research, we will publicly release Ostrakon-VL and the ShopBench benchmark.

</details>


### [65] [Latent Chain-of-Thought as Planning: Decoupling Reasoning from Verbalization](https://arxiv.org/abs/2601.21358)
*Jiecong Wang,Hao Peng,Chunyang Liu*

Main category: cs.AI

TL;DR: PLaT框架将潜在推理重新定义为规划，通过解耦推理和语言化，让模型能动态决定何时终止推理，在数学基准测试中展现出更好的推理多样性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有CoT方法受限于计算成本和离散token空间的推理路径崩溃问题，而潜在推理方法虽然优化效率，但通常作为不透明的端到端映射，且推理时需要预定义潜在步数。

Method: 提出PLaT框架，将潜在推理重新定义为规划，将推理建模为确定性潜在规划状态轨迹，同时使用单独的Decoder在必要时将这些思想转化为文本，实现推理与语言化的解耦。

Result: 在数学基准测试中，PLaT虽然贪婪准确率低于基线，但展现出更好的推理多样性可扩展性，表明学习到了更鲁棒、更广泛的解空间。

Conclusion: PLaT为推理时搜索提供了透明且可扩展的基础，通过解耦推理和语言化，使模型能动态决定推理终止时机，而非依赖固定超参数。

Abstract: Chain-of-Thought (CoT) empowers Large Language Models (LLMs) to tackle complex problems, but remains constrained by the computational cost and reasoning path collapse when grounded in discrete token spaces. Recent latent reasoning approaches attempt to optimize efficiency by performing reasoning within continuous hidden states. However, these methods typically operate as opaque end-to-end mappings from explicit reasoning steps to latent states, and often require a pre-defined number of latent steps during inference. In this work, we introduce PLaT (Planning with Latent Thoughts), a framework that reformulates latent reasoning as planning by fundamentally decouple reasoning from verbalization. We model reasoning as a deterministic trajectory of latent planning states, while a separate Decoder grounds these thoughts into text when necessary. This decoupling allows the model to dynamically determine when to terminate reasoning rather than relying on fixed hyperparameters. Empirical results on mathematical benchmarks reveal a distinct trade-off: while PLaT achieves lower greedy accuracy than baselines, it demonstrates superior scalability in terms of reasoning diversity. This indicates that PLaT learns a robust, broader solution space, offering a transparent and scalable foundation for inference-time search.

</details>


### [66] [System 1&2 Synergy via Dynamic Model Interpolation](https://arxiv.org/abs/2601.21414)
*Chenxu Yang,Qingyi Si,Chong Tian,Xiyu Liu,Dingyu Yao,Chuanyu Qin,Zheng Lin,Weiping Wang,Jiaqi Wang*

Main category: cs.AI

TL;DR: DAMI框架通过动态参数插值在直觉型System 1和深思型System 2之间自适应切换，实现能力控制而非输出控制，在数学推理任务上达到更高准确率同时保持效率。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注输出控制（限制模型输出长度），但这只是认知配置的症状而非根本原因。需要转向能力控制，调节模型如何思考而非产生什么，以解决System 1和System 2认知模式之间的干扰问题。

Method: 提出DAMI框架：1）利用现有Instruct和Thinking检查点进行动态参数插值，无需额外训练；2）通过查询特定的推理强度λ(q)配置认知深度；3）开发基于偏好学习的方法训练λ(q)估计器；4）提出基于置信度的零样本部署方法，利用模型间认知差异。

Result: 在五个数学推理基准测试中，DAMI比Thinking模型获得更高准确率，同时保持效率，有效结合了System 1的效率和System 2的推理深度。线性插值产生凸单调Pareto前沿，支持表示连续性和结构连通性。

Conclusion: 能力控制比输出控制更有效，DAMI通过动态模型插值实现了直觉和深思认知模式的自适应平衡，为构建统一语言模型提供了新方向。

Abstract: Training a unified language model that adapts between intuitive System 1 and deliberative System 2 remains challenging due to interference between their cognitive modes. Recent studies have thus pursued making System 2 models more efficient. However, these approaches focused on output control, limiting what models produce. We argue that this paradigm is misaligned: output length is merely a symptom of the model's cognitive configuration, not the root cause. In this work, we shift the focus to capability control, which modulates \textit{how models think} rather than \textit{what they produce}. To realize this, we leverage existing Instruct and Thinking checkpoints through dynamic parameter interpolation, without additional training. Our pilot study establishes that linear interpolation yields a convex, monotonic Pareto frontier, underpinned by representation continuity and structural connectivity. Building on this, we propose \textbf{DAMI} (\textbf{D}yn\textbf{A}mic \textbf{M}odel \textbf{I}nterpolation), a framework that estimates a query-specific Reasoning Intensity $λ(q)$ to configure cognitive depth. For training-based estimation, we develop a preference learning method encoding accuracy and efficiency criteria. For zero-shot deployment, we introduce a confidence-based method leveraging inter-model cognitive discrepancy. Experiments on five mathematical reasoning benchmarks demonstrate that DAMI achieves higher accuracy than the Thinking model while remaining efficient, effectively combining the efficiency of System 1 with the reasoning depth of System 2.

</details>


### [67] [When Prohibitions Become Permissions: Auditing Negation Sensitivity in Language Models](https://arxiv.org/abs/2601.21433)
*Katherine Elkins,Jon Chun*

Main category: cs.AI

TL;DR: 大型语言模型在处理否定指令时存在严重缺陷：当用户说"不应该"做某事时，许多模型反而将其理解为肯定指令，导致在伦理场景中错误地支持被禁止的行为。


<details>
  <summary>Details</summary>
Motivation: 研究发现当前许多大型语言模型无法正确处理否定指令，将"不应该"做某事误解为"应该"做某事，这在伦理敏感的高风险场景中可能带来严重的安全隐患。

Method: 对16个模型在14个伦理场景中进行审计，比较简单否定和复合否定下的表现，使用确定性解码排除采样噪声，并提出否定敏感指数(NSI)作为治理指标。

Result: 开源模型在简单否定下77%的时间支持被禁止行为，复合否定下达到100%（比肯定表述增加317%）；商业模型表现稍好但仍存在19-128%的波动；金融场景的脆弱性是医疗场景的两倍。

Conclusion: 当前对齐技术与安全部署要求存在差距，无法可靠区分"做X"和"不做X"的模型不应在高风险场景中做出自主决策，需要建立分级认证框架和领域特定阈值。

Abstract: When a user tells an AI system that someone "should not" take an action, the system ought to treat this as a prohibition. Yet many large language models do the opposite: they interpret negated instructions as affirmations. We audited 16 models across 14 ethical scenarios and found that open-source models endorse prohibited actions 77% of the time under simple negation and 100% under compound negation -- a 317% increase over affirmative framing. Commercial models fare better but still show swings of 19-128%. Agreement between models drops from 74% on affirmative prompts to 62% on negated ones, and financial scenarios prove twice as fragile as medical ones. These patterns hold under deterministic decoding, ruling out sampling noise. We present case studies showing how these failures play out in practice, propose the Negation Sensitivity Index (NSI) as a governance metric, and outline a tiered certification framework with domain-specific thresholds. The findings point to a gap between what current alignment techniques achieve and what safe deployment requires: models that cannot reliably distinguish "do X" from "do not X" should not be making autonomous decisions in high-stakes contexts.

</details>


### [68] [The Paradox of Robustness: Decoupling Rule-Based Logic from Affective Noise in High-Stakes Decision-Making](https://arxiv.org/abs/2601.21439)
*Jon Chun,Katherine Elkins*

Main category: cs.AI

TL;DR: 研究发现LLMs在情感框架效应上表现出惊人的稳健性，尽管它们对提示词微小变化很敏感，但在规则约束的决策中却对情感叙事操纵具有高度抵抗力。


<details>
  <summary>Details</summary>
Motivation: 虽然已知LLMs对微小提示扰动敏感且容易迎合用户偏见，但它们在重要规则约束决策中的稳健性尚未充分探索。本研究旨在探究LLMs在面对情感框架效应时的行为表现。

Method: 使用新颖的受控扰动框架，在医疗、法律和金融三个高风险领域进行实验，量化LLMs对叙事操纵的抵抗力，并与人类受试者进行比较。

Result: LLMs表现出惊人的行为不变性：对情感框架效应的影响几乎为零（Cohen's h = 0.003），而人类则表现出显著偏见（Cohen's h在[0.3, 0.8]之间）。LLMs对叙事操纵的抵抗力比人类高110-300倍。

Conclusion: 指令调优的LLMs能够将逻辑规则遵循与说服性叙事解耦，提供了一种决策稳定性来源，可以补充甚至潜在去偏人类判断。这种稳健性在制度环境中具有重要价值。

Abstract: While Large Language Models (LLMs) are widely documented to be sensitive to minor prompt perturbations and prone to sycophantic alignment with user biases, their robustness in consequential, rule-bound decision-making remains under-explored. In this work, we uncover a striking "Paradox of Robustness": despite their known lexical brittleness, instruction-tuned LLMs exhibit a behavioral and near-total invariance to emotional framing effects. Using a novel controlled perturbation framework across three high-stakes domains (healthcare, law, and finance), we quantify a robustness gap where LLMs demonstrate 110-300 times greater resistance to narrative manipulation than human subjects. Specifically, we find a near-zero effect size for models (Cohen's h = 0.003) compared to the substantial biases observed in humans (Cohen's h in [0.3, 0.8]). This result is highly counterintuitive and suggests the mechanisms driving sycophancy and prompt sensitivity do not necessarily translate to a failure in logical constraint satisfaction. We show that this invariance persists across models with diverse training paradigms. Our findings show that while LLMs may be "brittle" to how a query is formatted, they are remarkably "stable" against why a decision should be biased. Our findings establish that instruction-tuned models can decouple logical rule-adherence from persuasive narratives, offering a source of decision stability that complements, and even potentially de-biases, human judgment in institutional contexts. We release the 162-scenario benchmark, code, and data to facilitate the rigorous evaluation of narrative-induced bias and robustness on GitHub.com.

</details>


### [69] [ChipBench: A Next-Step Benchmark for Evaluating LLM Performance in AI-Aided Chip Design](https://arxiv.org/abs/2601.21448)
*Zhongkai Yu,Chenyang Zhou,Yichen Lin,Hejia Zhang,Haotian Ye,Junxia Cui,Zaifeng Pan,Jishen Zhao,Yufei Ding*

Main category: cs.AI

TL;DR: 该论文提出了一个全面的AI辅助芯片设计基准测试ChipBench，用于评估大语言模型在Verilog生成、调试和参考模型生成三个关键任务上的性能，解决了现有基准测试饱和且任务单一的问题。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在硬件工程领域显示出巨大潜力，但现有基准测试存在饱和问题和任务多样性不足的缺陷，无法真实反映LLM在实际工业工作流程中的性能表现。

Method: 提出了一个全面的AI辅助芯片设计基准测试，包含三个关键任务：1) Verilog生成（44个具有复杂层次结构的实际模块）；2) 调试（89个系统化调试案例）；3) 参考模型生成（132个样本，涵盖Python、SystemC和CXXRTL）。同时提供了一个自动化工具箱用于生成高质量训练数据。

Result: 评估结果显示性能差距显著：最先进的Claude-4.5-opus在Verilog生成任务上仅达到30.74%，在Python参考模型生成任务上仅达到13.33%。相比之下，现有饱和基准测试中SOTA模型的通过率超过95%，表明新基准测试更具挑战性。

Conclusion: 该研究填补了AI辅助芯片设计评估的空白，提出了一个更全面、更具挑战性的基准测试，揭示了LLM在实际硬件工程任务中的性能局限，并为未来研究提供了高质量训练数据生成工具。

Abstract: While Large Language Models (LLMs) show significant potential in hardware engineering, current benchmarks suffer from saturation and limited task diversity, failing to reflect LLMs' performance in real industrial workflows. To address this gap, we propose a comprehensive benchmark for AI-aided chip design that rigorously evaluates LLMs across three critical tasks: Verilog generation, debugging, and reference model generation. Our benchmark features 44 realistic modules with complex hierarchical structures, 89 systematic debugging cases, and 132 reference model samples across Python, SystemC, and CXXRTL. Evaluation results reveal substantial performance gaps, with state-of-the-art Claude-4.5-opus achieving only 30.74\% on Verilog generation and 13.33\% on Python reference model generation, demonstrating significant challenges compared to existing saturated benchmarks where SOTA models achieve over 95\% pass rates. Additionally, to help enhance LLM reference model generation, we provide an automated toolbox for high-quality training data generation, facilitating future research in this underexplored domain. Our code is available at https://github.com/zhongkaiyu/ChipBench.git.

</details>


### [70] [LION: A Clifford Neural Paradigm for Multimodal-Attributed Graph Learning](https://arxiv.org/abs/2601.21453)
*Xunkai Li,Zhengyu Wu,Zekai Chen,Henan Sun,Daohan Su,Guang Zeng,Hongchao Qin,Rong-Hua Li,Guoren Wang*

Main category: cs.AI

TL;DR: LION提出基于Clifford代数和解耦图神经范式的多模态图对齐与融合方法，通过几何诱导的高阶图传播实现模态交互，自适应全息聚合改善融合效果，在多个任务上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前多模态图学习方法存在两个主要问题：1）模态对齐时忽视图上下文，抑制模态交互；2）模态融合缺乏适应性，简单适应双模态图，无法充分利用对齐后的拓扑先验信息，导致泛化性差和性能下降。

Method: 基于Clifford代数和解耦图神经范式（传播-聚合）实现对齐-融合。首先构建基于Clifford代数的模态感知几何流形，通过几何诱导的高阶图传播实现模态交互和模态对齐。然后基于对齐token的几何等级特性，提出自适应全息聚合模块，将几何等级的能量和尺度与可学习参数结合来改善模态融合。

Result: 在9个数据集上的广泛实验表明，LION在3个图任务和3个模态任务上显著优于最先进的基线方法。

Conclusion: LION通过Clifford代数构建的几何流形和自适应全息聚合，有效解决了多模态图学习中的对齐和融合问题，提高了多模态图表示学习的性能。

Abstract: Recently, the rapid advancement of multimodal domains has driven a data-centric paradigm shift in graph ML, transitioning from text-attributed to multimodal-attributed graphs. This advancement significantly enhances data representation and expands the scope of graph downstream tasks, such as modality-oriented tasks, thereby improving the practical utility of graph ML. Despite its promise, limitations exist in the current neural paradigms: (1) Neglect Context in Modality Alignment: Most existing methods adopt topology-constrained or modality-specific operators as tokenizers. These aligners inevitably neglect graph context and inhibit modality interaction, resulting in suboptimal alignment. (2) Lack of Adaptation in Modality Fusion: Most existing methods are simple adaptations for 2-modality graphs and fail to adequately exploit aligned tokens equipped with topology priors during fusion, leading to poor generalizability and performance degradation. To address the above issues, we propose LION (c\underline{LI}ff\underline{O}rd \underline{N}eural paradigm) based on the Clifford algebra and decoupled graph neural paradigm (i.e., propagation-then-aggregation) to implement alignment-then-fusion in multimodal-attributed graphs. Specifically, we first construct a modality-aware geometric manifold grounded in Clifford algebra. This geometric-induced high-order graph propagation efficiently achieves modality interaction, facilitating modality alignment. Then, based on the geometric grade properties of aligned tokens, we propose adaptive holographic aggregation. This module integrates the energy and scale of geometric grades with learnable parameters to improve modality fusion. Extensive experiments on 9 datasets demonstrate that LION significantly outperforms SOTA baselines across 3 graph and 3 modality downstream tasks.

</details>


### [71] [Topeax -- An Improved Clustering Topic Model with Density Peak Detection and Lexical-Semantic Term Importance](https://arxiv.org/abs/2601.21465)
*Márton Kardos*

Main category: cs.AI

TL;DR: Topeax是一种新的主题建模方法，通过密度估计峰值确定聚类数量，结合词汇和语义指标提升主题关键词质量，相比Top2Vec和BERTopic在聚类恢复和描述方面表现更好且更稳定。


<details>
  <summary>Details</summary>
Motivation: 当前流行的主题建模方法Top2Vec和BERTopic存在多个未解决的问题：1）对样本大小和超参数极度敏感，难以发现自然聚类；2）在估计术语重要性时，BERTopic忽略关键词与主题向量的语义距离，Top2Vec忽略语料库中的词频，导致主题不连贯且缺乏多样性。

Method: 提出Topeax方法：1）通过密度估计峰值自动发现聚类数量；2）结合词汇指标（如词频）和语义指标（如与主题向量的距离）来评估术语重要性，生成高质量的主题关键词。

Result: Topeax在聚类恢复和聚类描述方面均优于Top2Vec和BERTopic，同时对样本大小和超参数的变化表现出更稳定的行为，减少了异常波动。

Conclusion: Topeax通过改进聚类发现机制和术语重要性评估方法，解决了现有主题建模方法的主要缺陷，提供了更可靠、更高质量的主题建模解决方案。

Abstract: Text clustering is today the most popular paradigm for topic modelling, both in academia and industry. Despite clustering topic models' apparent success, we identify a number of issues in Top2Vec and BERTopic, which remain largely unsolved. Firstly, these approaches are unreliable at discovering natural clusters in corpora, due to extreme sensitivity to sample size and hyperparameters, the default values of which result in suboptimal behaviour. Secondly, when estimating term importance, BERTopic ignores the semantic distance of keywords to topic vectors, while Top2Vec ignores word counts in the corpus. This results in, on the one hand, less coherent topics due to the presence of stop words and junk words, and lack of variety and trust on the other. In this paper, I introduce a new approach, \textbf{Topeax}, which discovers the number of clusters from peaks in density estimates, and combines lexical and semantic indices of term importance to gain high-quality topic keywords. Topeax is demonstrated to be better at both cluster recovery and cluster description than Top2Vec and BERTopic, while also exhibiting less erratic behaviour in response to changing sample size and hyperparameters.

</details>


### [72] [ScaleSim: Serving Large-Scale Multi-Agent Simulation with Invocation Distance-Based Memory Management](https://arxiv.org/abs/2601.21473)
*Zaifeng Pan,Yipeng Shen,Zhengding Hu,Zhuang Wang,Aninda Manocha,Zheng Wang,Zhongkai Yu,Yue Guan,Yufei Ding*

Main category: cs.AI

TL;DR: ScaleSim是一个针对大规模多智能体模拟的LLM服务系统，通过预测智能体调用顺序实现内存高效管理，相比SGLang实现1.74倍加速


<details>
  <summary>Details</summary>
Motivation: 基于LLM的多智能体模拟应用日益广泛，但由于每个智能体都需要维护私有GPU状态（模型、前缀缓存、适配器等），随着智能体数量增加，GPU内存压力巨大，难以扩展

Method: 提出"调用距离"统一抽象概念来估计智能体未来LLM请求的相对顺序，基于此实现ScaleSim系统，支持主动预取和基于优先级的逐出策略，通过模块化接口支持多样化的智能体特定内存

Result: 在模拟基准测试中，ScaleSim相比SGLang实现了最高1.74倍的加速

Conclusion: ScaleSim通过创新的调用距离抽象和相应的内存管理策略，有效解决了大规模多智能体模拟中的GPU内存扩展问题

Abstract: LLM-based multi-agent simulations are increasingly adopted across application domains, but remain difficult to scale due to GPU memory pressure. Each agent maintains private GPU-resident states, including models, prefix caches, and adapters, which quickly exhaust device memory as the agent count grows. We identify two key properties of these workloads: sparse agent activation and an estimable agent invocation order. Based on an analysis of representative workload classes, we introduce invocation distance, a unified abstraction that estimates the relative order in which agents will issue future LLM requests. Leveraging this abstraction, we present ScaleSim, a memory-efficient LLM serving system for large-scale multi-agent simulations. ScaleSim enables proactive prefetching and priority-based eviction, supports diverse agent-specific memory through a modular interface, and achieves up to 1.74x speedup over SGLang on simulation benchmarks.

</details>


### [73] [ARGORA: Orchestrated Argumentation for Causally Grounded LLM Reasoning and Decision Making](https://arxiv.org/abs/2601.21533)
*Youngjin Jin,Hanna Kim,Kwanwoo Kim,Chanhee Lee,Seungwon Shin*

Main category: cs.AI

TL;DR: ARGORA框架将多专家LLM系统的讨论组织成显式的论证图，通过因果模型分析哪些论证链对最终决策是必要的，并提供纠正机制对齐内部推理与外部判断。


<details>
  <summary>Details</summary>
Motivation: 现有多专家LLM系统通过简单聚合收集不同观点，但掩盖了哪些论证驱动了最终决策。需要一种能够明确展示论证结构、识别关键推理链并提供纠正机制的系统。

Method: 引入ARGORA框架：1）将多专家讨论组织成显式论证图，显示论证间的支持与攻击关系；2）将这些图建模为因果模型，通过系统性地移除单个论证并重新计算结果，识别必要的推理链；3）引入纠正机制，当内部推理与外部判断不一致时进行对齐。

Result: 在多样化基准测试和开放式用例中，ARGORA实现了有竞争力的准确性，并展现出纠正行为：当专家最初存在分歧时，该框架更倾向于将争议解决为正确答案而非引入新错误，同时提供关于决定性论证的因果诊断。

Conclusion: ARGORA通过显式论证图和因果分析，提高了多专家LLM系统的透明度和可解释性，能够识别关键推理链并提供纠正机制，在保持准确性的同时增强了决策过程的可诊断性。

Abstract: Existing multi-expert LLM systems gather diverse perspectives but combine them through simple aggregation, obscuring which arguments drove the final decision. We introduce ARGORA, a framework that organizes multi-expert discussions into explicit argumentation graphs showing which arguments support or attack each other. By casting these graphs as causal models, ARGORA can systematically remove individual arguments and recompute outcomes, identifying which reasoning chains were necessary and whether decisions would change under targeted modifications. We further introduce a correction mechanism that aligns internal reasoning with external judgments when they disagree. Across diverse benchmarks and an open-ended use case, ARGORA achieves competitive accuracy and demonstrates corrective behavior: when experts initially disagree, the framework resolves disputes toward correct answers more often than it introduces new errors, while providing causal diagnostics of decisive arguments.

</details>


### [74] [ShardMemo: Masked MoE Routing for Sharded Agentic LLM Memory](https://arxiv.org/abs/2601.21545)
*Yang Zhao,Chengxiao Dai,Yue Xiu,Mengying Kou,Yuliang Zheng,Dusit Niyato*

Main category: cs.AI

TL;DR: ShardMemo是一个面向智能体LLM系统的预算分层内存服务，通过三层内存架构和基于掩码MoE路由的分片机制，显著提升长上下文和多智能体场景下的检索效率和性能。


<details>
  <summary>Details</summary>
Motivation: 传统集中式索引和启发式分区方法在处理大规模内存和并行访问时成为瓶颈，智能体LLM系统需要更高效的外部内存管理方案来支持长时状态和并发多智能体执行。

Method: 提出三层内存架构：Tier A（每个智能体的工作状态）、Tier B（分片证据存储，包含分片本地ANN索引）、Tier C（版本化技能库）。Tier B采用"范围优先路由"原则，通过结构化资格约束在路由前屏蔽不合格分片，将分片探测建模为掩码混合专家路由，使用Top-Bprobe或自适应Top-P策略，并基于成本感知门控机制管理不同分片家族。

Result: 在LoCoMo基准上，ShardMemo比最强基线（GAM）提升5.11-6.82 F1；在固定预算路由设置下，比余弦原型分片路由提升6.87 F1，同时减少20.5%检索工作量和降低p95延迟。在长上下文HotpotQA上，在不同token长度下均取得良好F1分数。在ToolBench上，Tier C达到0.97 Precision@3和1.94 StepRed，比嵌入相似性检索提升10.2%和7.2%。

Conclusion: ShardMemo通过分层内存架构和智能分片路由机制，有效解决了智能体LLM系统中的内存管理瓶颈，在多个基准测试中表现出显著的性能提升和效率改进。

Abstract: Agentic large language model (LLM) systems rely on external memory for long-horizon state and concurrent multi-agent execution, but centralized indexes and heuristic partitions become bottlenecks as memory volume and parallel access grow. We present ShardMemo, a budgeted tiered memory service with Tier A per-agent working state, Tier B sharded evidence with shard-local approximate nearest neighbor (ANN) indexes, and Tier C, a versioned skill library. Tier B enforces scope-before-routing: structured eligibility constraints mask ineligible shards before routing or ANN search. We cast shard probing as masked mixture-of-experts (MoE) routing over eligible shards, probing up to $B_{\mathrm{probe}}$ shards via Top-$B_{\mathrm{probe}}$ or adaptive Top-$P$, and use cost-aware gating over profile/observation/session shard families; the router is trained from evidence-to-shard supervision. On LoCoMo, ShardMemo improves over the strongest baseline (GAM) by +5.11 to +6.82 F1 across question categories. Under a fixed-budget routing setting ($B_{\mathrm{probe}}=3$), ShardMemo improves over cosine-to-prototype shard routing by +6.87 F1 while reducing retrieval work (VecScan 521->414, -20.5%) and p95 latency (95->76 ms). On long-context HotpotQA, ShardMemo achieves 63.41/61.88/57.95 F1 at 56K/224K/448K tokens. On ToolBench, Tier C reaches 0.97 Precision@3 and 1.94 StepRed (+10.2% and +7.2% over embedding-similarity retrieval).

</details>


### [75] [Chain Of Thought Compression: A Theoritical Analysis](https://arxiv.org/abs/2601.21576)
*Juncai Li,Ru Li,Yuxiang Zhou,Boxiang Ma,Jeff Z. Pan*

Main category: cs.AI

TL;DR: 本文首次从理论上分析了学习内化中间推理步骤的困难性，通过引入Order-r Interaction证明了高阶逻辑依赖的学习信号会指数衰减，并提出ALiCoT框架通过对齐潜在token分布与中间推理状态来克服信号衰减问题。


<details>
  <summary>Details</summary>
Motivation: 思维链（CoT）虽然解锁了大语言模型的高级推理能力，但生成额外token带来了高昂的计算成本。最近研究表明将推理步骤压缩到潜在状态（隐式CoT压缩）提供了token高效的替代方案，但其机制尚不清楚。

Method: 1. 引入Order-r Interaction理论框架分析学习内化中间推理步骤的困难性；2. 创建NatBool-DAG基准测试来强制不可约逻辑推理并消除语义捷径；3. 提出ALiCoT（Aligned Implicit CoT）框架，通过对齐潜在token分布与中间推理状态来克服信号衰减。

Result: 理论分析证明高阶逻辑依赖的学习信号会指数衰减，跳过中间步骤必然导致高阶交互障碍。实验结果显示ALiCoT成功实现高效推理：在保持与显式CoT相当性能的同时，实现了54.4倍的加速。

Conclusion: 本文首次从理论上解释了CoT压缩的机制，证明了学习内化中间推理步骤的困难性源于高阶交互的信号衰减，并提出ALiCoT框架通过状态对齐成功克服了这一障碍，为高效推理提供了理论指导和实用解决方案。

Abstract: Chain-of-Thought (CoT) has unlocked advanced reasoning abilities of Large Language Models (LLMs) with intermediate steps, yet incurs prohibitive computational costs due to generation of extra tokens. Recent studies empirically show that compressing reasoning steps into latent states, or implicit CoT compression, offers a token-efficient alternative. However, the mechanism behind CoT compression remains unclear. In this paper, we provide the first theoretical analysis of the difficulty of learning to internalize intermediate reasoning steps. By introducing Order-r Interaction, we prove that the learning signal for high-order logical dependencies exponentially decays to solve irreducible problem, where skipping intermediate steps inevitably leads to high-order interaction barriers. To empirically validate this, we introduce NatBool-DAG, a challenging benchmark designed to enforce irreducible logical reasoning and eliminate semantic shortcuts. Guided by our theoretical findings, we propose ALiCoT (Aligned Implicit CoT), a novel framework that overcomes the signal decay by aligning latent token distributions with intermediate reasoning states. Experimental results demonstrate that ALiCoT successfully unlocks efficient reasoning: it achieves a 54.4x speedup while maintaining performance comparable to explicit CoT.

</details>


### [76] [Depth-Recurrent Attention Mixtures: Giving Latent Reasoning the Attention it Deserves](https://arxiv.org/abs/2601.21582)
*Jonas Knupp,Jan Hendrik Metzen,Jeremias Bohn,Georg Groh,Kristian Kersting*

Main category: cs.AI

TL;DR: Dreamer框架通过深度循环注意力混合解决传统深度循环模型参数共享不足、隐藏层大小瓶颈等问题，在语言推理任务上以更少训练数据达到相同精度，或相同数据下超越更大模型。


<details>
  <summary>Details</summary>
Motivation: 现有深度循环模型存在三个主要问题：1) 缺乏FLOP、参数和内存匹配的基准对比；2) 部分固定层堆栈导致深度循环利用不足；3) 恒定隐藏层大小限制多步潜在推理。需要解决这些瓶颈以实现更高效的深度循环推理。

Method: 提出Dreamer框架，结合序列注意力、深度注意力和稀疏专家注意力。通过深度注意力缓解隐藏层大小瓶颈，解耦缩放维度，使深度循环模型能够高效扩展。

Result: 在语言推理基准测试中，相比FLOP、参数和内存匹配的SOTA模型，达到相同精度所需训练token减少2-8倍；使用相同训练token时，性能超越约2倍大的SOTA模型。深度知识使用分析显示专家选择多样性比SOTA MoE高2-11倍。

Conclusion: Dreamer框架通过创新的注意力混合机制有效解决了深度循环模型的扩展瓶颈，在推理效率和性能上显著优于现有方法，为大规模潜在推理提供了更高效的架构方案。

Abstract: Depth-recurrence facilitates latent reasoning by sharing parameters across depths. However, prior work lacks combined FLOP-, parameter-, and memory-matched baselines, underutilizes depth-recurrence due to partially fixed layer stacks, and ignores the bottleneck of constant hidden-sizes that restricts many-step latent reasoning. To address this, we introduce a modular framework of depth-recurrent attention mixtures (Dreamer), combining sequence attention, depth attention, and sparse expert attention. It alleviates the hidden-size bottleneck through attention along depth, decouples scaling dimensions, and allows depth-recurrent models to scale efficiently and effectively. Across language reasoning benchmarks, our models require 2 to 8x fewer training tokens for the same accuracy as FLOP-, parameter-, and memory-matched SOTA, and outperform ca. 2x larger SOTA models with the same training tokens. We further present insights into knowledge usage across depths, e.g., showing 2 to 11x larger expert selection diversity than SOTA MoEs.

</details>


### [77] [Beyond Imitation: Reinforcement Learning for Active Latent Planning](https://arxiv.org/abs/2601.21598)
*Zhi Zheng,Wee Sun Lee*

Main category: cs.AI

TL;DR: ATP-Latent提出了一种主动潜在规划方法，通过条件变分自编码器建模潜在令牌监督过程，并使用强化学习优化推理策略，相比现有方法在准确率和令牌消耗上均有提升。


<details>
  <summary>Details</summary>
Motivation: 当前潜在推理方法主要基于模仿语言标签来监督潜在令牌，但由于一个问题可能存在多个等价但不同的思维链标签，被动模仿任意一个可能导致次优的潜在令牌表示和推理策略，限制了潜在规划能力，并造成训练与测试之间的明显差距。

Method: 提出ATP-Latent方法：1）使用条件变分自编码器建模潜在令牌的监督过程，获得更平滑的潜在空间；2）通过强化学习优化潜在推理策略，使用基于VAE解码内容一致性的辅助连贯性奖励来引导RL过程。

Result: 在LLaMA-1B模型上，ATP-Latent在四个基准测试中相比先进基线方法实现了+4.1%的准确率提升和-3.3%的令牌消耗减少。

Conclusion: 强调在潜在令牌表示空间上进行主动规划对于实现最优潜在推理策略的重要性，ATP-Latent通过结合条件VAE和强化学习，有效提升了潜在推理的性能和效率。

Abstract: Aiming at efficient and dense chain-of-thought (CoT) reasoning, latent reasoning methods fine-tune Large Language Models (LLMs) to substitute discrete language tokens with continuous latent tokens. These methods consume fewer tokens compared to the conventional language CoT reasoning and have the potential to plan in a dense latent space. However, current latent tokens are generally supervised based on imitating language labels. Considering that there can be multiple equivalent but diverse CoT labels for a question, passively imitating an arbitrary one may lead to inferior latent token representations and latent reasoning policies, undermining the potential planning ability and resulting in clear gaps between training and testing. In this work, we emphasize the importance of active planning over the representation space of latent tokens in achieving the optimal latent reasoning policy. So, we propose the \underline{A}c\underline{t}ive Latent \underline{P}lanning method (ATP-Latent), which models the supervision process of latent tokens as a conditional variational auto-encoder (VAE) to obtain a smoother latent space. Moreover, to facilitate the most reasonable latent reasoning policy, ATP-Latent conducts reinforcement learning (RL) with an auxiliary coherence reward, which is calculated based on the consistency between VAE-decoded contents of latent tokens, enabling a guided RL process. In experiments on LLaMA-1B, ATP-Latent demonstrates +4.1\% accuracy and -3.3\% tokens on four benchmarks compared to advanced baselines. Codes are available on https://github.com/zz1358m/ATP-Latent-master.

</details>


### [78] [Search-Based Risk Feature Discovery in Document Structure Spaces under a Constrained Budget](https://arxiv.org/abs/2601.21608)
*Saisubramaniam Gopalakrishnan,Harikrishnan P M,Dagnachew Birru*

Main category: cs.AI

TL;DR: 该研究将企业级智能文档处理系统的验证问题形式化为基于搜索的软件测试问题，旨在在有限预算下最大化发现不同类型的故障模式，而非寻找单一最坏情况文档。


<details>
  <summary>Details</summary>
Motivation: 企业级智能文档处理系统在金融、保险和医疗等高风险领域广泛应用，早期系统验证需要在有限预算下发现多样化的故障机制，而不是识别单一最坏情况文档。

Method: 在文档配置的组合空间中操作，实例化结构性风险特征以诱导现实故障条件，并基准测试了包括进化算法、群体智能、质量多样性、学习型和量子算法在内的多样化搜索策略组合。

Result: 不同求解器在相同预算下持续发现特定替代方法无法发现的故障模式，交叉时间分析显示所有评估预算中都存在持久的求解器特定发现，没有单一策略表现出绝对优势。虽然所有求解器的联合最终能覆盖观察到的故障空间，但依赖任何单一方法都会系统性地延迟重要风险的发现。

Conclusion: 研究结果证明了求解器之间的内在互补性，并激励采用基于组合策略的基于搜索的软件测试方法，以实现稳健的工业智能文档处理系统验证。

Abstract: Enterprise-grade Intelligent Document Processing (IDP) systems support high-stakes workflows across finance, insurance, and healthcare. Early-phase system validation under limited budgets mandates uncovering diverse failure mechanisms, rather than identifying a single worst-case document. We formalize this challenge as a Search-Based Software Testing (SBST) problem, aiming to identify complex interactions between document variables, with the objective to maximize the number of distinct failure types discovered within a fixed evaluation budget. Our methodology operates on a combinatorial space of document configurations, rendering instances of structural \emph{risk features} to induce realistic failure conditions. We benchmark a diverse portfolio of search strategies spanning evolutionary, swarm-based, quality-diversity, learning-based, and quantum under identical budget constraints. Through configuration-level exclusivity, win-rate, and cross-temporal overlap analyses, we show that different solvers consistently uncover failure modes that remain undiscovered by specific alternatives at comparable budgets. Crucially, cross-temporal analysis reveals persistent solver-specific discoveries across all evaluated budgets, with no single strategy exhibiting absolute dominance. While the union of all solvers eventually recovers the observed failure space, reliance on any individual method systematically delays the discovery of important risks. These results demonstrate intrinsic solver complementarity and motivate portfolio-based SBST strategies for robust industrial IDP validation.

</details>


### [79] [Semantic Content Determines Algorithmic Performance](https://arxiv.org/abs/2601.21618)
*Martiño Ríos-García,Nawaf Alampara,Kevin Maik Jablonka*

Main category: cs.AI

TL;DR: 论文提出WhatCounts测试框架，发现LLMs的计数性能严重依赖于被计数对象的语义内容，而非算法本身，揭示了LLMs并非真正实现算法而是近似算法


<details>
  <summary>Details</summary>
Motivation: 研究LLMs是否真正实现了算法的不变性原则，即算法行为应独立于输入参数的语义内容。现有研究将语义敏感性与推理复杂性或提示变化混淆，需要一种原子化的测试方法来专门检验语义依赖性

Method: 设计WhatCounts测试框架：在无歧义、有分隔符、无重复项、无干扰项的列表中计数不同语义类型的项目。通过受控消融实验排除混杂因素，测试前沿LLMs在不同语义内容下的计数准确性变化

Result: 前沿LLMs的计数准确性变化超过40%，仅取决于被计数对象的语义类型（如城市vs化学品、名称vs符号）。语义差距是真实存在的，且少量无关微调会导致不可预测的变化。LLMs并非实现算法而是近似算法，且这种近似是参数依赖的

Conclusion: LLMs不实现算法而是近似算法，且近似程度依赖于输入参数的意义。这一发现具有超越计数任务的广泛意义：任何LLM函数都可能携带对其输入意义的隐藏依赖，这在智能体应用中尤其重要

Abstract: Counting should not depend on what is being counted; more generally, any algorithm's behavior should be invariant to the semantic content of its arguments. We introduce WhatCounts to test this property in isolation. Unlike prior work that conflates semantic sensitivity with reasoning complexity or prompt variation, WhatCounts is atomic: count items in an unambiguous, delimited list with no duplicates, distractors, or reasoning steps for different semantic types. Frontier LLMs show over 40% accuracy variation depending solely on what is being counted - cities versus chemicals, names versus symbols. Controlled ablations rule out confounds. The gap is semantic, and it shifts unpredictably with small amounts of unrelated fine-tuning. LLMs do not implement algorithms; they approximate them, and the approximation is argument-dependent. As we show with an agentic example, this has implications beyond counting: any LLM function may carry hidden dependencies on the meaning of its inputs.

</details>


### [80] [ScholarGym: Benchmarking Deep Research Workflows on Academic Literature Retrieval](https://arxiv.org/abs/2601.21654)
*Hao Shen,Hang Yang,Zhouhong Gu*

Main category: cs.AI

TL;DR: ScholarGym是一个用于评估学术文献深度研究工作流的仿真环境，通过静态语料库和确定性检索解决API依赖带来的不可重现性问题。


<details>
  <summary>Details</summary>
Motivation: 工具增强的大型语言模型已从单轮问答发展到深度研究工作流，但评估这些工作流面临根本挑战：依赖实时API会引入非确定性，因为工具调用可能因时间漂移、速率限制和后端状态变化而产生不同结果，这种差异破坏了可重现性并使跨系统比较无效。

Method: 提出ScholarGym仿真环境，将工作流组件解耦为查询规划、工具调用和相关性评估，在受控条件下对每个阶段进行细粒度分析。基于包含57万篇论文的静态语料库构建，提供确定性检索和2,536个带有专家标注真实标签的查询。

Result: 在不同骨干模型上的实验揭示了推理能力、规划策略和选择机制在迭代优化过程中的相互作用。

Conclusion: ScholarGym为深度研究工作流提供了可重现的评估环境，解决了依赖实时API带来的非确定性问题，支持对工作流各阶段的细粒度分析。

Abstract: Tool-augmented large language models have advanced from single-turn question answering to deep research workflows that iteratively plan queries, invoke external tools, and synthesize information to address complex information needs. Evaluating such workflows presents a fundamental challenge: reliance on live APIs introduces non-determinism, as tool invocations may yield different results across runs due to temporal drift, rate limiting, and evolving backend states. This variance undermines reproducibility and invalidates cross-system comparisons.
  We present ScholarGym, a simulation environment for reproducible evaluation of deep research workflows on academic literature. The environment decouples workflow components into query planning, tool invocation, and relevance assessment, enabling fine-grained analysis of each stage under controlled conditions. Built on a static corpus of 570K papers with deterministic retrieval, ScholarGym provides 2,536 queries with expert-annotated ground truth. Experiments across diverse backbone models reveal how reasoning capabilities, planning strategies, and selection mechanisms interact over iterative refinement.

</details>


### [81] [SONIC-O1: A Real-World Benchmark for Evaluating Multimodal Large Language Models on Audio-Video Understanding](https://arxiv.org/abs/2601.21666)
*Ahmed Y. Radwan,Christos Emmanouilidis,Hina Tabassum,Deval Pandya,Shaina Raza*

Main category: cs.AI

TL;DR: SONIC-O1是一个全面的人工验证基准，用于评估多模态大语言模型在真实世界音频-视频序列数据上的表现，涵盖13个对话领域，包含4,958个标注和人口统计元数据。


<details>
  <summary>Details</summary>
Motivation: 当前大多数多模态大语言模型研究集中在静态图像理解，而对处理序列音频-视频数据的能力探索不足，需要高质量基准来系统评估模型在真实场景中的表现。

Method: 构建SONIC-O1基准，包含13个真实世界对话领域，4,958个人工验证标注，评估任务包括开放式摘要、多项选择题回答、带推理的时间定位。实验比较闭源和开源模型性能。

Result: 实验显示模型存在局限性：闭源和开源模型在多项选择题准确率上差距较小，但在时间定位任务上性能差距达22.6%。模型在不同人口统计群体上表现进一步下降，表明存在持续的性能差异。

Conclusion: SONIC-O1为时间基础和社交鲁棒的多模态理解提供了开放评估套件，揭示了当前模型在处理序列音频-视频数据时的不足，特别是在时间定位和跨人口统计群体公平性方面。

Abstract: Multimodal Large Language Models (MLLMs) are a major focus of recent AI research. However, most prior work focuses on static image understanding, while their ability to process sequential audio-video data remains underexplored. This gap highlights the need for a high-quality benchmark to systematically evaluate MLLM performance in a real-world setting. We introduce SONIC-O1, a comprehensive, fully human-verified benchmark spanning 13 real-world conversational domains with 4,958 annotations and demographic metadata. SONIC-O1 evaluates MLLMs on key tasks, including open-ended summarization, multiple-choice question (MCQ) answering, and temporal localization with supporting rationales (reasoning). Experiments on closed- and open-source models reveal limitations. While the performance gap in MCQ accuracy between two model families is relatively small, we observe a substantial 22.6% performance difference in temporal localization between the best performing closed-source and open-source models. Performance further degrades across demographic groups, indicating persistent disparities in model behavior. Overall, SONIC-O1 provides an open evaluation suite for temporally grounded and socially robust multimodal understanding. We release SONIC-O1 for reproducibility and research: Project page: https://vectorinstitute.github.io/sonic-o1/ Dataset: https://huggingface.co/datasets/vector-institute/sonic-o1 Github: https://github.com/vectorinstitute/sonic-o1 Leaderboard: https://huggingface.co/spaces/vector-institute/sonic-o1-leaderboard

</details>


### [82] [TCAP: Tri-Component Attention Profiling for Unsupervised Backdoor Detection in MLLM Fine-Tuning](https://arxiv.org/abs/2601.21692)
*Mingzu Liu,Hao Fang,Runmin Cong*

Main category: cs.AI

TL;DR: 该论文提出了一种名为TCAP的无监督防御框架，用于检测多模态大语言模型微调服务中的后门攻击，通过分析注意力分配差异来识别中毒样本。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型的微调即服务存在后门攻击风险，现有防御方法要么依赖监督信号，要么无法适应不同触发器和模态的多样性。研究发现中毒样本会破坏系统指令、视觉输入和用户文本查询三个功能组件之间的平衡注意力分布，这为无监督防御提供了理论基础。

Method: 提出Tri-Component Attention Profiling (TCAP)框架：1) 将跨模态注意力图分解为三个功能组件；2) 使用高斯混合模型统计分析识别对触发器敏感的注意力头；3) 通过基于期望最大化的投票聚合机制隔离中毒样本。

Result: 在不同MLLM架构和攻击方法上的广泛实验表明，TCAP能够持续实现强大的性能，在各种触发类型和模态下都表现出色，证明了其作为MLLM中稳健实用的后门防御方法的有效性。

Conclusion: TCAP通过利用后门攻击在注意力分配上的普遍指纹特征，提供了一种无监督的防御解决方案，能够有效应对多模态大语言模型微调服务中的后门风险，具有实际应用价值。

Abstract: Fine-Tuning-as-a-Service (FTaaS) facilitates the customization of Multimodal Large Language Models (MLLMs) but introduces critical backdoor risks via poisoned data. Existing defenses either rely on supervised signals or fail to generalize across diverse trigger types and modalities. In this work, we uncover a universal backdoor fingerprint-attention allocation divergence-where poisoned samples disrupt the balanced attention distribution across three functional components: system instructions, vision inputs, and user textual queries, regardless of trigger morphology. Motivated by this insight, we propose Tri-Component Attention Profiling (TCAP), an unsupervised defense framework to filter backdoor samples. TCAP decomposes cross-modal attention maps into the three components, identifies trigger-responsive attention heads via Gaussian Mixture Model (GMM) statistical profiling, and isolates poisoned samples through EM-based vote aggregation. Extensive experiments across diverse MLLM architectures and attack methods demonstrate that TCAP achieves consistently strong performance, establishing it as a robust and practical backdoor defense in MLLMs.

</details>


### [83] [FBS: Modeling Native Parallel Reading inside a Transformer](https://arxiv.org/abs/2601.21708)
*Tongxi Wang*

Main category: cs.AI

TL;DR: FBS Transformer通过引入仿人阅读机制（前瞻注意力、分块处理、跳过门控）来加速LLM推理，在不增加参数的情况下提升质量-效率权衡


<details>
  <summary>Details</summary>
Motivation: 当前LLM推理主要依赖严格的逐token自回归，现有加速方法只是修补现有流程，缺乏人类阅读的核心要素：内容自适应前瞻、分块结构感知的计算分配、以及训练-测试一致性的预览/略读机制

Method: 提出Fovea-Block-Skip Transformer (FBS)，通过三个模块注入因果可训练循环：Parafovea-Attention Window (PAW)实现内容自适应前瞻，Chunk-Head (CH)进行分块结构感知计算分配，Skip-Gate (SG)实现训练-测试一致的跳过机制

Result: 在多样化基准测试中，FBS改善了质量-效率权衡而不增加参数，消融实验显示三个模块具有互补性

Conclusion: FBS Transformer通过模拟人类阅读机制成功加速LLM推理，为高效推理提供了新的架构设计思路

Abstract: Large language models (LLMs) excel across many tasks, yet inference is still dominated by strictly token-by-token autoregression. Existing acceleration methods largely patch this pipeline and miss core human-reading ingredients: content-adaptive foresight, chunk-structure-aware compute allocation, and train--test consistency for preview/skimming. We propose the \textbf{Fovea-Block-Skip Transformer} (FBS), which injects a causal, trainable loop into Transformers via Parafovea-Attention Window (PAW), Chunk-Head (CH), and Skip-Gate (SG). Across diverse benchmarks, FBS improves the quality-efficiency trade-off without increasing parameters, and ablations show the three modules are complementary.

</details>


### [84] [E-mem: Multi-agent based Episodic Context Reconstruction for LLM Agent Memory](https://arxiv.org/abs/2601.21714)
*Kaixiang Wang,Yidan Lin,Jiong Lou,Zhaojiacheng Zhou,Bunyod Suvonov,Jie Li*

Main category: cs.AI

TL;DR: E-mem框架通过从内存预处理转向情景上下文重建，解决了LLM智能体在系统2推理中上下文完整性破坏的问题，在LoCoMo基准上实现54%以上F1分数，超越现有最佳方法7.75%，同时减少70%以上token成本。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型智能体向系统2推理（深思熟虑、高精度问题解决）演进时，需要在长时间范围内保持严格的逻辑完整性。然而，现有的内存预处理范式存在破坏性的去上下文化问题，通过将复杂的顺序依赖压缩为预定义结构（如嵌入或图），这些方法切断了深度推理所必需的上下文完整性。

Method: 提出E-mem框架，从内存预处理转向情景上下文重建。受生物印迹启发，采用异构分层架构：多个助手智能体维护未压缩的内存上下文，中央主智能体协调全局规划。助手智能体能够在激活的片段内进行本地推理，提取上下文感知的证据后再进行聚合，而非被动检索。

Result: 在LoCoMo基准测试中，E-mem实现了超过54%的F1分数，比当前最先进的GAM方法提升了7.75%，同时减少了超过70%的token成本。

Conclusion: E-mem通过情景上下文重建有效解决了LLM智能体系统2推理中的上下文完整性破坏问题，在保持逻辑完整性的同时显著提升了性能并降低了计算成本，为智能体的深度推理能力提供了新方向。

Abstract: The evolution of Large Language Model (LLM) agents towards System~2 reasoning, characterized by deliberative, high-precision problem-solving, requires maintaining rigorous logical integrity over extended horizons. However, prevalent memory preprocessing paradigms suffer from destructive de-contextualization. By compressing complex sequential dependencies into pre-defined structures (e.g., embeddings or graphs), these methods sever the contextual integrity essential for deep reasoning. To address this, we propose E-mem, a framework shifting from Memory Preprocessing to Episodic Context Reconstruction. Inspired by biological engrams, E-mem employs a heterogeneous hierarchical architecture where multiple assistant agents maintain uncompressed memory contexts, while a central master agent orchestrates global planning. Unlike passive retrieval, our mechanism empowers assistants to locally reason within activated segments, extracting context-aware evidence before aggregation. Evaluations on the LoCoMo benchmark demonstrate that E-mem achieves over 54\% F1, surpassing the state-of-the-art GAM by 7.75\%, while reducing token cost by over 70\%.

</details>


### [85] [DropoutTS: Sample-Adaptive Dropout for Robust Time Series Forecasting](https://arxiv.org/abs/2601.21726)
*Siru Zhong,Yiqiu Liu,Zhiqing Cui,Zezhi Shao,Fei Wang,Qingsong Wen,Yuxuan Liang*

Main category: cs.AI

TL;DR: DropoutTS是一种模型无关的插件，通过样本自适应dropout机制，利用频谱稀疏性量化实例级噪声，动态校准模型学习能力，在保持细粒度保真度的同时抑制虚假波动。


<details>
  <summary>Details</summary>
Motivation: 深度时间序列模型对现实应用中普遍存在的噪声数据很脆弱。现有鲁棒性策略要么修剪数据，要么依赖昂贵的先验量化，无法平衡有效性和效率。

Method: 提出DropoutTS插件，采用样本自适应dropout机制：利用频谱稀疏性通过重构残差高效量化实例级噪声，将噪声映射到自适应dropout率来动态校准模型学习能力，选择性地抑制虚假波动同时保持细粒度保真度。

Result: 在多种噪声机制和开放基准测试上的广泛实验表明，DropoutTS能持续提升优秀骨干模型的性能，以可忽略的参数开销和无架构修改实现先进的鲁棒性。

Conclusion: DropoutTS通过将学习范式从"学什么"转变为"学多少"，提供了一种高效且有效的解决方案来增强深度时间序列模型的噪声鲁棒性。

Abstract: Deep time series models are vulnerable to noisy data ubiquitous in real-world applications. Existing robustness strategies either prune data or rely on costly prior quantification, failing to balance effectiveness and efficiency. In this paper, we introduce DropoutTS, a model-agnostic plugin that shifts the paradigm from "what" to learn to "how much" to learn. DropoutTS employs a Sample-Adaptive Dropout mechanism: leveraging spectral sparsity to efficiently quantify instance-level noise via reconstruction residuals, it dynamically calibrates model learning capacity by mapping noise to adaptive dropout rates - selectively suppressing spurious fluctuations while preserving fine-grained fidelity. Extensive experiments across diverse noise regimes and open benchmarks show DropoutTS consistently boosts superior backbones' performance, delivering advanced robustness with negligible parameter overhead and no architectural modifications. Our code is available at https://github.com/CityMind-Lab/DropoutTS.

</details>


### [86] [Epistemic Context Learning: Building Trust the Right Way in LLM-Based Multi-Agent Systems](https://arxiv.org/abs/2601.21742)
*Ruiwen Zhou,Maojia Song,Xiaobao Wu,Sitao Cheng,Xunjian Yin,Yuxi Xie,Zhuoqun Hao,Wenyue Hua,Liangming Pan,Soujanya Poria,Min-Yen Kan*

Main category: cs.AI

TL;DR: ECL框架通过历史交互构建同伴可信度档案，使小模型能准确识别可靠同伴，性能超越大8倍的历史无关基线模型


<details>
  <summary>Details</summary>
Motivation: 多智能体系统中个体智能体缺乏鲁棒性，容易盲目跟随误导性同伴，这源于奉承行为和评估同伴可靠性的能力不足

Method: 提出历史感知参考学习问题，开发Epistemic Context Learning (ECL)推理框架，基于历史交互显式构建同伴档案，并通过强化学习优化

Result: ECL使Qwen 3-4B小模型性能超越大8倍的Qwen 3-30B基线，前沿模型达到接近完美(100%)性能，在多种多智能体配置中泛化良好

Conclusion: ECL通过历史交互建模同伴可信度能显著提升多智能体系统性能，可信度建模准确性与最终答案质量强相关

Abstract: Individual agents in multi-agent (MA) systems often lack robustness, tending to blindly conform to misleading peers. We show this weakness stems from both sycophancy and inadequate ability to evaluate peer reliability. To address this, we first formalize the learning problem of history-aware reference, introducing the historical interactions of peers as additional input, so that agents can estimate peer reliability and learn from trustworthy peers when uncertain. This shifts the task from evaluating peer reasoning quality to estimating peer reliability based on interaction history. We then develop Epistemic Context Learning (ECL): a reasoning framework that conditions predictions on explicitly-built peer profiles from history. We further optimize ECL by reinforcement learning using auxiliary rewards. Our experiments reveal that our ECL enables small models like Qwen 3-4B to outperform a history-agnostic baseline 8x its size (Qwen 3-30B) by accurately identifying reliable peers. ECL also boosts frontier models to near-perfect (100%) performance. We show that ECL generalizes well to various MA configurations and we find that trust is modeled well by LLMs, revealing a strong correlation in trust modeling accuracy and final answer quality.

</details>


### [87] [Zero-Shot Statistical Downscaling via Diffusion Posterior Sampling](https://arxiv.org/abs/2601.21760)
*Ruian Tie,Wenbo Xiong,Zhengyu Shi,Xinyu Su,Chenyu jiang,Libo Wu,Hao Li*

Main category: cs.AI

TL;DR: 提出ZSSD零样本统计降尺度框架，无需配对训练数据，通过物理一致性气候先验和统一坐标引导解决现有方法的物理不一致性和梯度消失问题。


<details>
  <summary>Details</summary>
Motivation: 传统监督式气候降尺度方法因缺乏配对训练数据和与再分析数据的领域差距而难以泛化到全球气候模型；现有零样本方法存在物理不一致性和大尺度因子下的梯度消失问题。

Method: 提出ZSSD零样本框架：1）从再分析数据学习物理一致性气候先验，以地球物理边界和时序信息为条件确保物理有效性；2）引入统一坐标引导策略，解决原始DPS的梯度消失问题，确保与大尺度场的一致性。

Result: ZSSD在99百分位误差上显著优于现有零样本基线，能够成功重建复杂天气事件（如热带气旋），并在异质GCMs中表现良好。

Conclusion: ZSSD为零样本气候降尺度提供了有效的解决方案，通过物理一致性先验和统一坐标引导克服了现有方法的局限性，实现了对复杂天气事件的高质量重建和跨GCMs的稳健推断。

Abstract: Conventional supervised climate downscaling struggles to generalize to Global Climate Models (GCMs) due to the lack of paired training data and inherent domain gaps relative to reanalysis. Meanwhile, current zero-shot methods suffer from physical inconsistencies and vanishing gradient issues under large scaling factors. We propose Zero-Shot Statistical Downscaling (ZSSD), a zero-shot framework that performs statistical downscaling without paired data during training. ZSSD leverages a Physics-Consistent Climate Prior learned from reanalysis data, conditioned on geophysical boundaries and temporal information to enforce physical validity. Furthermore, to enable robust inference across varying GCMs, we introduce Unified Coordinate Guidance. This strategy addresses the vanishing gradient problem in vanilla DPS and ensures consistency with large-scale fields. Results show that ZSSD significantly outperforms existing zero-shot baselines in 99th percentile errors and successfully reconstructs complex weather events, such as tropical cyclones, across heterogeneous GCMs.

</details>


### [88] [Abstract Concept Modelling in Conceptual Spaces: A Study on Chess Strategies](https://arxiv.org/abs/2601.21771)
*Hadi Banaee,Stephanie Lowry*

Main category: cs.AI

TL;DR: 提出基于概念空间框架的时序抽象概念建模方法，以国际象棋为案例，将策略概念表示为可解释质量维度上的几何区域，通过游戏轨迹分析识别策略意图


<details>
  <summary>Details</summary>
Motivation: 扩展概念空间理论以建模随时间展开的、目标导向的抽象概念，为涉及序列决策的广泛应用奠定基础

Method: 将策略概念（如攻击、牺牲）表示为可解释质量维度上的几何区域，将国际象棋游戏实例化为轨迹，通过轨迹向概念区域的定向运动识别策略意图，支持双视角建模

Result: 实现了基于轨迹的概念识别可行性验证，运动模式与专家评论一致，展示了玩家对相同情境的不同解释

Conclusion: 建立了时序抽象概念建模的基础框架，支持与知识演化机制集成，用于随时间学习和精炼抽象概念

Abstract: We present a conceptual space framework for modelling abstract concepts that unfold over time, demonstrated through a chess-based proof-of-concept. Strategy concepts, such as attack or sacrifice, are represented as geometric regions across interpretable quality dimensions, with chess games instantiated and analysed as trajectories whose directional movement toward regions enables recognition of intended strategies. This approach also supports dual-perspective modelling, capturing how players interpret identical situations differently. Our implementation demonstrates the feasibility of trajectory-based concept recognition, with movement patterns aligning with expert commentary. This work explores extending the conceptual spaces theory to temporally realised, goal-directed concepts. The approach establishes a foundation for broader applications involving sequential decision-making and supports integration with knowledge evolution mechanisms for learning and refining abstract concepts over time.

</details>


### [89] [A Unified XAI-LLM Approach for EndotrachealSuctioning Activity Recognition](https://arxiv.org/abs/2601.21802)
*Hoang Khang Phan,Quang Vinh Dang,Noriyo Colley,Christina Garcia,Nhat Tan Le*

Main category: cs.AI

TL;DR: 提出基于大语言模型的统一框架，用于气管内吸痰术的视频活动识别和反馈生成，在准确率和F1分数上比基线模型提升15-20%。


<details>
  <summary>Details</summary>
Motivation: 气管内吸痰术是高风险临床操作，但在家庭护理和教育环境中缺乏有效监督。现有自动化识别和反馈系统研究不足，需要开发智能培训解决方案。

Method: 提出以LLM为中心的框架，将大语言模型作为核心推理模块，从视频数据执行时空活动识别和可解释决策分析，并生成自然语言反馈。同时构建基于异常检测和可解释AI的学生支持模块。

Result: LLM方法在准确率和F1分数上比传统机器学习和深度学习方法提升约15-20%，能够生成可理解的自然语言反馈，识别正确操作并提供针对性改进建议。

Conclusion: 该框架为护理教育提供了可扩展、可解释、数据驱动的基础，有助于提高培训效率和患者安全，展示了LLM在医疗技能培训中的潜力。

Abstract: Endotracheal suctioning (ES) is an invasive yet essential clinical procedure that requires a high degree of skill to minimize patient risk - particularly in home care and educational settings, where consistent supervision may be limited. Despite its critical importance, automated recognition and feedback systems for ES training remain underexplored. To address this gap, this study proposes a unified, LLM-centered framework for video-based activity recognition benchmarked against conventional machine learning and deep learning approaches, and a pilot study on feedback generation. Within this framework, the Large Language Model (LLM) serves as the central reasoning module, performing both spatiotemporal activity recognition and explainable decision analysis from video data. Furthermore, the LLM is capable of verbalizing feedback in natural language, thereby translating complex technical insights into accessible, human-understandable guidance for trainees. Experimental results demonstrate that the proposed LLM-based approach outperforms baseline models, achieving an improvement of approximately 15-20\% in both accuracy and F1 score. Beyond recognition, the framework incorporates a pilot student-support module built upon anomaly detection and explainable AI (XAI) principles, which provides automated, interpretable feedback highlighting correct actions and suggesting targeted improvements. Collectively, these contributions establish a scalable, interpretable, and data-driven foundation for advancing nursing education, enhancing training efficiency, and ultimately improving patient safety.

</details>


### [90] [Looking Beyond Accuracy: A Holistic Benchmark of ECG Foundation Models](https://arxiv.org/abs/2601.21830)
*Francesca Filice,Edoardo De Rose,Simone Bartucci,Francesco Calimeri,Simona Perri*

Main category: cs.AI

TL;DR: 该研究提出了一种全面的心电图专家基础模型基准测试框架，结合性能评估和表示层分析，评估模型在不同数据集和数据稀缺情况下的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 虽然基础模型在AI辅助心电图解释领域开始应用，但现有基准测试主要关注下游性能评估，缺乏对模型嵌入表示泛化能力的深入分析。在医疗等错误敏感领域，需要更全面的评估框架来负责任地应用这些模型。

Method: 提出了一种基准测试方法，将基于性能的评估与表示层分析相结合，利用SHAP和UMAP技术。使用该方法对多个通过最先进技术预训练的心电图专家基础模型进行了广泛评估，涵盖不同跨大陆数据集和数据可用性设置（包括数据稀缺情况）。

Result: 实验结果表明，该基准测试协议能够深入洞察心电图专家基础模型的嵌入模式，使研究人员能够更深入地理解其表示结构和泛化能力。

Conclusion: 该研究填补了心电图专家基础模型评估的空白，提供了一个全面的基准测试框架，有助于更负责任地在医疗领域应用这些模型，特别是在数据稀缺的现实医疗场景中。

Abstract: The electrocardiogram (ECG) is a cost-effective, highly accessible and widely employed diagnostic tool. With the advent of Foundation Models (FMs), the field of AI-assisted ECG interpretation has begun to evolve, as they enable model reuse across different tasks by relying on embeddings. However, to responsibly employ FMs, it is crucial to rigorously assess to which extent the embeddings they produce are generalizable, particularly in error-sensitive domains such as healthcare. Although prior works have already addressed the problem of benchmarking ECG-expert FMs, they focus predominantly on the evaluation of downstream performance. To fill this gap, this study aims to find an in-depth, comprehensive benchmarking framework for FMs, with a specific focus on ECG-expert ones. To this aim, we introduce a benchmark methodology that complements performance-based evaluation with representation-level analysis, leveraging SHAP and UMAP techniques. Furthermore, we rely on the methodology for carrying out an extensive evaluation of several ECG-expert FMs pretrained via state-of-the-art techniques over different cross-continental datasets and data availability settings; this includes ones featuring data scarcity, a fairly common situation in real-world medical scenarios. Experimental results show that our benchmarking protocol provides a rich insight of ECG-expert FMs' embedded patterns, enabling a deeper understanding of their representational structure and generalizability.

</details>


### [91] [KnowBias: Mitigating Social Bias in LLMs via Know-Bias Neuron Enhancement](https://arxiv.org/abs/2601.21864)
*Jinhao Pan,Chahat Raj,Anjishnu Mukherjee,Sina Mansouri,Bowen Wei,Shloka Yada,Ziwei Zhu*

Main category: cs.AI

TL;DR: KnowBias是一种轻量级框架，通过增强而非抑制编码偏见知识的神经元来减轻LLM偏见，仅需少量是/否问题，无需重新训练


<details>
  <summary>Details</summary>
Motivation: 现有偏见消除方法通常采用抑制范式（修改参数、提示或与偏见行为相关的神经元），但这些方法往往脆弱、泛化能力弱、数据效率低，且容易降低模型的一般能力

Method: 通过基于归因的分析，使用少量偏见知识问题识别编码偏见知识的神经元，并在推理时有选择地增强这些神经元

Result: 在多个基准测试和LLM上的实验表明，KnowBias实现了最先进的偏见消除性能，同时保持了最小化的能力退化

Conclusion: KnowBias提供了一种概念上不同的偏见缓解方法，能够有效消除偏见、保持一般能力、跨偏见类型和人口统计特征泛化，且具有高度数据效率

Abstract: Large language models (LLMs) exhibit social biases that reinforce harmful stereotypes, limiting their safe deployment. Most existing debiasing methods adopt a suppressive paradigm by modifying parameters, prompts, or neurons associated with biased behavior; however, such approaches are often brittle, weakly generalizable, data-inefficient, and prone to degrading general capability. We propose \textbf{KnowBias}, a lightweight and conceptually distinct framework that mitigates bias by strengthening, rather than suppressing, neurons encoding bias-knowledge. KnowBias identifies neurons encoding bias knowledge using a small set of bias-knowledge questions via attribution-based analysis, and selectively enhances them at inference time. This design enables strong debiasing while preserving general capabilities, generalizes across bias types and demographics, and is highly data efficient, requiring only a handful of simple yes/no questions and no retraining. Experiments across multiple benchmarks and LLMs demonstrate consistent state-of-the-art debiasing performance with minimal utility degradation. Data and code are available at https://github.com/JP-25/KnowBias.

</details>


### [92] [astra-langchain4j: Experiences Combining LLMs and Agent Programming](https://arxiv.org/abs/2601.21879)
*Rem Collier,Katharine Beaumont,Andrei Ciortea*

Main category: cs.AI

TL;DR: 本文介绍了将大型语言模型集成到ASTRA编程语言中的原型开发经验，探讨了生成式AI和Agentic AI如何影响传统Agent工具包的使用，以及传统工具包经验如何影响新智能体平台设计。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI和Agentic AI（多智能体系统形式）在过去两年中的兴起，需要探索这些技术如何影响传统Agent工具包的使用，以及传统工具包中积累的丰富经验如何影响新智能体平台的设计。

Method: 开发了ASTRA编程语言的大型语言模型集成原型，通过三个示例实现展示了集成方法，并基于这些示例收集经验进行讨论。

Result: 论文展示了ASTRA工具包的简要概述，呈现了三个具体的示例实现，并通过这些示例获得了关于LLM集成到传统Agent工具包中的实践经验。

Conclusion: 通过原型开发经验，探讨了传统Agent工具包与新兴LLM技术的融合可能性，为未来智能体平台设计提供了有价值的见解。

Abstract: Given the emergence of Generative AI over the last two years and the increasing focus on Agentic AI as a form of Multi-Agent System it is important to explore both how such technologies can impact the use of traditional Agent Toolkits and how the wealth of experience encapsulated in those toolkits can influence the design of the new agentic platforms. This paper presents an overview of our experience developing a prototype large language model (LLM) integration for the ASTRA programming language. It presents a brief overview of the toolkit, followed by three example implementations, concluding with a discussion of the experiences garnered through the examples.

</details>


### [93] [From Meta-Thought to Execution: Cognitively Aligned Post-Training for Generalizable and Reliable LLM Reasoning](https://arxiv.org/abs/2601.21909)
*Shaojie Wang,Liang Zhang*

Main category: cs.AI

TL;DR: CoMT框架通过模仿人类认知的两阶段过程，将抽象策略学习与具体执行分离，显著提升LLM的泛化能力和训练效率


<details>
  <summary>Details</summary>
Motivation: 现有LLM后训练方法将完整推理轨迹作为基本单元，这与人类实际解决问题的认知过程存在根本差异。人类认知自然地将问题解决分解为两个阶段：首先获取跨问题通用的抽象策略（元知识），然后将其适应到具体实例。当前方法的问题中心化导致抽象策略与问题特定执行纠缠在一起。

Method: 提出认知启发的两阶段框架：1) Chain-of-Meta-Thought (CoMT)：专注于抽象推理模式的有监督学习，不涉及具体执行，获取可泛化的策略；2) Confidence-Calibrated Reinforcement Learning (CCRL)：通过中间步骤的置信度感知奖励优化任务适应，防止过度自信的错误级联，提高执行可靠性。

Result: 在四个模型和八个基准测试中，相比标准方法，在分布内性能提升2.19%，分布外性能提升4.63%，同时训练时间减少65-70%，token消耗减少50%。

Conclusion: 将后训练方法与人类认知原则对齐不仅能产生更优的泛化能力，还能显著提升训练效率，这为LLM优化提供了新的认知启发方向。

Abstract: Current LLM post-training methods optimize complete reasoning trajectories through Supervised Fine-Tuning (SFT) followed by outcome-based Reinforcement Learning (RL). While effective, a closer examination reveals a fundamental gap: this approach does not align with how humans actually solve problems. Human cognition naturally decomposes problem-solving into two distinct stages: first acquiring abstract strategies (i.e., meta-knowledge) that generalize across problems, then adapting them to specific instances. In contrast, by treating complete trajectories as basic units, current methods are inherently problem-centric, entangling abstract strategies with problem-specific execution. To address this misalignment, we propose a cognitively-inspired framework that explicitly mirrors the two-stage human cognitive process. Specifically, Chain-of-Meta-Thought (CoMT) focuses supervised learning on abstract reasoning patterns without specific executions, enabling acquisition of generalizable strategies. Confidence-Calibrated Reinforcement Learning (CCRL) then optimizes task adaptation via confidence-aware rewards on intermediate steps, preventing overconfident errors from cascading and improving execution reliability. Experiments across four models and eight benchmarks show 2.19\% and 4.63\% improvements in-distribution and out-of-distribution respectively over standard methods, while reducing training time by 65-70% and token consumption by 50%, demonstrating that aligning post-training with human cognitive principles yields not only superior generalization but also enhanced training efficiency.

</details>


### [94] [ProRAG: Process-Supervised Reinforcement Learning for Retrieval-Augmented Generation](https://arxiv.org/abs/2601.21912)
*Zhao Wang,Ziliang Zhao,Zhicheng Dou*

Main category: cs.AI

TL;DR: ProRAG提出了一种过程监督强化学习框架，通过整合步骤级监督到在线优化循环中，解决了传统结果导向RL在RAG优化中的奖励稀疏性和信用分配问题。


<details>
  <summary>Details</summary>
Motivation: 传统基于结果的RL方法在复杂推理任务中存在奖励稀疏和信用分配效率低的问题，粗粒度的标量奖励无法识别长轨迹中的具体错误步骤，导致"过程幻觉"——模型通过有缺陷的逻辑或冗余检索步骤得到正确答案。现有的过程感知方法也缺乏在线探索能力来解耦步骤级信用与全局结果。

Method: ProRAG包含四个阶段：1) 监督策略预热，用结构化推理格式初始化模型；2) 构建基于MCTS的过程奖励模型(PRM)来量化中间推理质量；3) PRM引导的推理细化，使策略与细粒度过程偏好对齐；4) 过程监督强化学习，采用双粒度优势机制，聚合步骤级过程奖励与全局结果信号。

Result: 在五个多跳推理基准测试上的广泛实验表明，ProRAG相比强大的基于结果和过程感知的RL基线方法实现了更优的整体性能，特别是在复杂的长时程任务上，验证了细粒度过程监督的有效性。

Conclusion: ProRAG通过将学习到的步骤级监督整合到在线优化循环中，有效解决了RAG优化中的奖励稀疏性和信用分配问题，为每个动作提供精确反馈，显著提升了复杂推理任务的性能。

Abstract: Reinforcement learning (RL) has become a promising paradigm for optimizing Retrieval-Augmented Generation (RAG) in complex reasoning tasks. However, traditional outcome-based RL approaches often suffer from reward sparsity and inefficient credit assignment, as coarse-grained scalar rewards fail to identify specific erroneous steps within long-horizon trajectories. This ambiguity frequently leads to "process hallucinations", where models reach correct answers through flawed logic or redundant retrieval steps. Although recent process-aware approaches attempt to mitigate this via static preference learning or heuristic reward shaping, they often lack the on-policy exploration capabilities required to decouple step-level credit from global outcomes. To address these challenges, we propose ProRAG, a process-supervised reinforcement learning framework designed to integrate learned step-level supervision into the online optimization loop. Our framework consists of four stages: (1) Supervised Policy Warmup to initialize the model with a structured reasoning format; (2) construction of an MCTS-based Process Reward Model (PRM) to quantify intermediate reasoning quality; (3) PRM-Guided Reasoning Refinement to align the policy with fine-grained process preferences; and (4) Process-Supervised Reinforcement Learning with a dual-granularity advantage mechanism. By aggregating step-level process rewards with global outcome signals, ProRAG provides precise feedback for every action. Extensive experiments on five multi-hop reasoning benchmarks demonstrate that ProRAG achieves superior overall performance compared to strong outcome-based and process-aware RL baselines, particularly on complex long-horizon tasks, validating the effectiveness of fine-grained process supervision. The code and model are available at https://github.com/lilinwz/ProRAG.

</details>


### [95] [JADE: Bridging the Strategic-Operational Gap in Dynamic Agentic RAG](https://arxiv.org/abs/2601.21916)
*Yiqun Chen,Erhan Zhang,Tianyi Hu,Shijie Wang,Zixuan Yang,Meizhi Zhong,Xiaochi Wei,Yan Gao,Yi Wu,Yao Hu,Jiaxin Mao*

Main category: cs.AI

TL;DR: JADE框架通过联合优化动态多轮工作流中的规划和执行模块，解决了现有RAG系统中规划与执行脱节导致的"战略-操作不匹配"问题。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统面临关键二分法：要么在固定图架构中联合优化模块，要么支持动态规划但将执行器视为冻结的黑盒工具。这种"解耦优化"导致"战略-操作不匹配"，即复杂的规划策略因未适应的本地执行器而无法实现，系统复杂性增加却带来负面性能收益。

Method: 提出JADE（联合智能动态执行）框架，将系统建模为在单一共享骨干网下统一的多智能体协作团队，通过基于结果的奖励进行端到端学习，实现规划和执行的联合优化。

Result: 实验结果表明，JADE将分离的模块转变为协同系统，通过联合优化带来显著的性能改进，并通过动态工作流编排实现效率与效果之间的灵活平衡。

Conclusion: JADE框架通过促进规划器与执行器的协同适应，解决了RAG系统中规划与执行的脱节问题，实现了更有效的动态多轮推理工作流。

Abstract: The evolution of Retrieval-Augmented Generation (RAG) has shifted from static retrieval pipelines to dynamic, agentic workflows where a central planner orchestrates multi-turn reasoning. However, existing paradigms face a critical dichotomy: they either optimize modules jointly within rigid, fixed-graph architectures, or empower dynamic planning while treating executors as frozen, black-box tools. We identify that this \textit{decoupled optimization} creates a ``strategic-operational mismatch,'' where sophisticated planning strategies fail to materialize due to unadapted local executors, often leading to negative performance gains despite increased system complexity. In this paper, we propose \textbf{JADE} (\textbf{J}oint \textbf{A}gentic \textbf{D}ynamic \textbf{E}xecution), a unified framework for the joint optimization of planning and execution within dynamic, multi-turn workflows. By modeling the system as a cooperative multi-agent team unified under a single shared backbone, JADE enables end-to-end learning driven by outcome-based rewards. This approach facilitates \textit{co-adaptation}: the planner learns to operate within the capability boundaries of the executors, while the executors evolve to align with high-level strategic intent. Empirical results demonstrate that JADE transforms disjoint modules into a synergistic system, yielding remarkable performance improvements via joint optimization and enabling a flexible balance between efficiency and effectiveness through dynamic workflow orchestration.

</details>


### [96] [Self-Compression of Chain-of-Thought via Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2601.21919)
*Yiqun Chen,Jinyuan Feng,Wei Yang,Meizhi Zhong,Zhengliang Shi,Rui Li,Xiaochi Wei,Yan Gao,Yi Wu,Yao Hu,Zhiqiang Pu,Jiaxin Mao*

Main category: cs.AI

TL;DR: SCMA是一个多智能体强化学习框架，通过分割和评分智能体协作识别并惩罚冗余推理块，在保持准确性的同时显著减少推理长度。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型中冗余推理带来的推理开销会损害交互体验并严重阻碍部署。现有基于强化学习的解决方案通过将长度惩罚与结果奖励相结合来解决这个问题，但这种简单的奖励加权难以平衡简洁性和准确性，因为强制简洁可能会损害关键的推理逻辑。

Method: 提出了一个多智能体强化学习框架SCMA，包含三个智能体：分割智能体将推理过程分解为逻辑块；评分智能体量化每个块的重要性；推理智能体在训练过程中使用重要性加权的长度惩罚，激励其优先考虑基本逻辑而不引入部署时的推理开销。

Result: 实证评估显示，SCMA在不同模型规模上都能将响应长度减少11.1%到39.0%，同时将准确性提高4.33%到10.02%。消融研究和定性分析验证了MARL框架内的协同优化能够促进涌现行为，产生比传统RL范式更强大的大型推理模型。

Conclusion: SCMA通过多智能体强化学习框架有效解决了推理冗余问题，在保持准确性的同时显著减少推理长度，为大型推理模型的部署提供了更高效的解决方案。

Abstract: The inference overhead induced by redundant reasoning undermines the interactive experience and severely bottlenecks the deployment of Large Reasoning Models. Existing reinforcement learning (RL)-based solutions tackle this problem by coupling a length penalty with outcome-based rewards. This simplistic reward weighting struggles to reconcile brevity with accuracy, as enforcing brevity may compromise critical reasoning logic. In this work, we address this limitation by proposing a multi-agent RL framework that selectively penalizes redundant chunks, while preserving essential reasoning logic. Our framework, Self-Compression via MARL (SCMA), instantiates redundancy detection and evaluation through two specialized agents: \textbf{a Segmentation Agent} for decomposing the reasoning process into logical chunks, and \textbf{a Scoring Agent} for quantifying the significance of each chunk. The Segmentation and Scoring agents collaboratively define an importance-weighted length penalty during training, incentivizing \textbf{a Reasoning Agent} to prioritize essential logic without introducing inference overhead during deployment. Empirical evaluations across model scales demonstrate that SCMA reduces response length by 11.1\% to 39.0\% while boosting accuracy by 4.33\% to 10.02\%. Furthermore, ablation studies and qualitative analysis validate that the synergistic optimization within the MARL framework fosters emergent behaviors, yielding more powerful LRMs compared to vanilla RL paradigms.

</details>


### [97] [AgenticSimLaw: A Juvenile Courtroom Multi-Agent Debate Simulation for Explainable High-Stakes Tabular Decision Making](https://arxiv.org/abs/2601.21936)
*Jon Chun,Kathrine Elkins,Yong Suk Lee*

Main category: cs.AI

TL;DR: AgenticSimLaw是一个基于法庭辩论结构的多智能体框架，用于高风险表格决策任务，通过定义明确的角色、交互协议和私有推理策略，提供透明可控的推理过程。


<details>
  <summary>Details</summary>
Motivation: 针对黑盒方法在高风险决策任务中缺乏透明度和可控性的问题，需要一种能够提供审计追踪、解释性且允许人类监督的推理框架，特别是在刑事司法等敏感领域。

Method: 采用法庭辩论式的角色结构化多智能体框架，包含检察官、辩护律师和法官三个角色，通过7轮结构化辩论协议进行交互，每个智能体使用私有推理策略，生成完整的交互记录用于审计。

Result: 在NLSY97数据集上的年轻成人再犯预测任务中，与传统的思维链提示相比，结构化多智能体辩论在近90种模型和策略组合中表现出更稳定和可泛化的性能，准确率和F1分数相关性更强。

Conclusion: AgenticSimLaw不仅提升了性能，还提供了对推理步骤的细粒度控制、完整的交互记录用于解释性，并能系统分析智能体行为。该框架可推广到任何需要透明度和人类监督的高风险决策任务，解决了多智能体系统的组织、可观测性和责任分配等关键挑战。

Abstract: We introduce AgenticSimLaw, a role-structured, multi-agent debate framework that provides transparent and controllable test-time reasoning for high-stakes tabular decision-making tasks. Unlike black-box approaches, our courtroom-style orchestration explicitly defines agent roles (prosecutor, defense, judge), interaction protocols (7-turn structured debate), and private reasoning strategies, creating a fully auditable decision-making process. We benchmark this framework on young adult recidivism prediction using the NLSY97 dataset, comparing it against traditional chain-of-thought (CoT) prompting across almost 90 unique combinations of models and strategies. Our results demonstrate that structured multi-agent debate provides more stable and generalizable performance compared to single-agent reasoning, with stronger correlation between accuracy and F1-score metrics. Beyond performance improvements, AgenticSimLaw offers fine-grained control over reasoning steps, generates complete interaction transcripts for explainability, and enables systematic profiling of agent behaviors. While we instantiate this framework in the criminal justice domain to stress-test reasoning under ethical complexity, the approach generalizes to any deliberative, high-stakes decision task requiring transparency and human oversight. This work addresses key LLM-based multi-agent system challenges: organization through structured roles, observability through logged interactions, and responsibility through explicit non-deployment constraints for sensitive domains. Data, results, and code will be available on github.com under the MIT license.

</details>


### [98] [ToolWeaver: Weaving Collaborative Semantics for Scalable Tool Use in Large Language Models](https://arxiv.org/abs/2601.21947)
*Bowen Fang,Wen Ye,Yunyue Su,Jinghao Zhang,Qiang Liu,Yesheng Liu,Xin Sun,Shu Wu,Jiabing Yang,Baole Wei,Liang Wang*

Main category: cs.AI

TL;DR: ToolWeaver提出了一种新的生成式工具学习框架，通过将工具编码为分层序列来解决现有检索式和生成式方法的语义瓶颈问题，显著提升了工具学习的可扩展性、泛化性和语义感知能力。


<details>
  <summary>Details</summary>
Motivation: 现有工具学习方法面临双重语义挑战：检索式方法的编码器难以捕捉复杂语义，而大语言模型本身缺乏工具知识；生成式方法虽然统一了选择和执行，但将每个工具映射到唯一新标记的做法导致可扩展性危机和语义瓶颈，阻碍了协作工具关系的学习。

Method: 提出ToolWeaver框架，将工具编码为分层序列，使词汇扩展与工具数量呈对数关系；通过新颖的标记化过程生成结构化代码，将工具的内在语义与外在共现模式编织在一起；通过生成对齐阶段将这些结构化代码集成到大语言模型中，微调模型以产生分层代码序列。

Result: 在近47,000个工具上的评估结果显示，ToolWeaver显著优于最先进的方法，为高级工具增强智能体建立了更可扩展、泛化性更强、语义感知能力更好的基础。

Conclusion: ToolWeaver通过分层序列编码解决了现有工具学习方法的语义瓶颈和可扩展性问题，为生成式工具学习提供了更有效的框架，能够更好地学习协作工具关系并适应大规模工具库。

Abstract: Prevalent retrieval-based tool-use pipelines struggle with a dual semantic challenge: their retrievers often employ encoders that fail to capture complex semantics, while the Large Language Model (LLM) itself lacks intrinsic tool knowledge from its natural language pretraining. Generative methods offer a powerful alternative by unifying selection and execution, tasking the LLM to directly learn and generate tool identifiers. However, the common practice of mapping each tool to a unique new token introduces substantial limitations: it creates a scalability and generalization crisis, as the vocabulary size explodes and each tool is assigned a semantically isolated token. This approach also creates a semantic bottleneck that hinders the learning of collaborative tool relationships, as the model must infer them from sparse co-occurrences of monolithic tool IDs within a vast library. To address these limitations, we propose ToolWeaver, a novel generative tool learning framework that encodes tools into hierarchical sequences. This approach makes vocabulary expansion logarithmic to the number of tools. Crucially, it enables the model to learn collaborative patterns from the dense co-occurrence of shared codes, rather than the sparse co-occurrence of monolithic tool IDs. We generate these structured codes through a novel tokenization process designed to weave together a tool's intrinsic semantics with its extrinsic co-usage patterns. These structured codes are then integrated into the LLM through a generative alignment stage, where the model is fine-tuned to produce the hierarchical code sequences. Evaluation results with nearly 47,000 tools show that ToolWeaver significantly outperforms state-of-the-art methods, establishing a more scalable, generalizable, and semantically-aware foundation for advanced tool-augmented agents.

</details>


### [99] [Learning Decentralized LLM Collaboration with Multi-Agent Actor Critic](https://arxiv.org/abs/2601.21972)
*Shuo Liu,Tianle Chen,Ryan Amiri,Christopher Amato*

Main category: cs.AI

TL;DR: 该研究分析了多智能体Actor-Critic方法在优化去中心化LLM协作中的有效性，提出了两种方法：CoLLM-CC（集中式评论家）和CoLLM-DC（去中心化评论家），并发现它们在长视野或稀疏奖励任务中表现差异显著。


<details>
  <summary>Details</summary>
Motivation: 现有MARL微调方法依赖预定义执行协议，通常需要集中式执行，而去中心化LLM协作更具实际吸引力。当前方法使用蒙特卡洛方法进行微调，存在高方差问题，需要更多训练样本。Actor-critic方法在MARL中广泛用于解决这些问题。

Method: 提出了两种多智能体Actor-Critic方法：CoLLM-CC（集中式评论家）和CoLLM-DC（去中心化评论家）。通过实验在写作、编码和游戏领域验证这些方法的有效性，特别关注任务视野长度和奖励稀疏性对性能的影响。

Result: 实验表明，在短视野和密集奖励设置中，蒙特卡洛方法和CoLLM-DC能达到与CoLLM-CC相当的性能。但在长视野或稀疏奖励任务中，蒙特卡洛方法需要更多样本，CoLLM-DC难以收敛，而CoLLM-CC表现更优。

Conclusion: 多智能体Actor-Critic方法（特别是集中式评论家架构）在优化去中心化LLM协作方面具有优势，特别是在处理长视野或稀疏奖励的复杂任务时。去中心化评论家方法在简单任务中有效，但在复杂任务中面临收敛挑战。

Abstract: Recent work has explored optimizing LLM collaboration through Multi-Agent Reinforcement Learning (MARL). However, most MARL fine-tuning approaches rely on predefined execution protocols, which often require centralized execution. Decentralized LLM collaboration is more appealing in practice, as agents can run inference in parallel with flexible deployments. Also, current approaches use Monte Carlo methods for fine-tuning, which suffer from high variance and thus require more samples to train effectively. Actor-critic methods are prevalent in MARL for dealing with these issues, so we developed Multi-Agent Actor-Critic (MAAC) methods to optimize decentralized LLM collaboration. In this paper, we analyze when and why these MAAC methods are beneficial. We propose 2 MAAC approaches, \textbf{CoLLM-CC} with a \textbf{C}entralized \textbf{C}ritic and \textbf{CoLLM-DC} with \textbf{D}ecentralized \textbf{C}ritics. Our experiments across writing, coding, and game-playing domains show that Monte Carlo methods and CoLLM-DC can achieve performance comparable to CoLLM-CC in short-horizon and dense-reward settings. However, they both underperform CoLLM-CC on long-horizon or sparse-reward tasks, where Monte Carlo methods require substantially more samples and CoLLM-DC struggles to converge. Our code is available at https://github.com/OpenMLRL/CoMLRL/releases/tag/v1.3.2.

</details>


### [100] [Mind the Gap: How Elicitation Protocols Shape the Stated-Revealed Preference Gap in Language Models](https://arxiv.org/abs/2601.21975)
*Pranav Mahajan,Ihor Kendiukhov,Syed Hussain,Lydia Nottingham*

Main category: cs.AI

TL;DR: 研究发现语言模型的陈述偏好与显示偏好之间存在差异，这种相关性高度依赖于评估协议的设计，特别是中立性和弃权选项的设置会显著影响相关性测量。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法主要依赖二元强制选择提示，这会将真实偏好与评估协议的人为因素混为一谈。需要系统研究不同评估协议如何影响语言模型的陈述-显示偏好相关性。

Method: 系统研究了24个语言模型，通过允许中立性和弃权选项来改进陈述偏好评估，并分析了不同评估协议对陈述-显示偏好相关性的影响，包括系统提示引导的效果。

Result: 1) 允许中立和弃权的陈述偏好评估能排除弱信号，显著提高Spearman等级相关性；2) 在显示偏好中允许弃权会使相关性降至接近零或负值；3) 使用陈述偏好进行系统提示引导在AIRiskDilemmas上不能可靠提高相关性。

Conclusion: 陈述-显示偏好相关性高度依赖于评估协议设计，偏好评估需要能够处理不确定偏好的方法，简单的二元强制选择协议存在局限性。

Abstract: Recent work identifies a stated-revealed (SvR) preference gap in language models (LMs): a mismatch between the values models endorse and the choices they make in context. Existing evaluations rely heavily on binary forced-choice prompting, which entangles genuine preferences with artifacts of the elicitation protocol. We systematically study how elicitation protocols affect SvR correlation across 24 LMs. Allowing neutrality and abstention during stated preference elicitation allows us to exclude weak signals, substantially improving Spearman's rank correlation ($ρ$) between volunteered stated preferences and forced-choice revealed preferences. However, further allowing abstention in revealed preferences drives $ρ$ to near-zero or negative values due to high neutrality rates. Finally, we find that system prompt steering using stated preferences during revealed preference elicitation does not reliably improve SvR correlation on AIRiskDilemmas. Together, our results show that SvR correlation is highly protocol-dependent and that preference elicitation requires methods that account for indeterminate preferences.

</details>


### [101] [VERSA: Verified Event Data Format for Reliable Soccer Analytics](https://arxiv.org/abs/2601.21981)
*Geonhee Jo,Mingu Kang,Kangmin Lee,Minho Lee,Pascal Bauer,Sang-Ki Ko*

Main category: cs.AI

TL;DR: VERSA是一个系统化的验证框架，用于确保足球领域事件流数据的完整性，通过状态转换模型检测和纠正逻辑不一致性，显著提升数据质量和下游分析可靠性。


<details>
  <summary>Details</summary>
Motivation: 事件流数据在体育分析等领域至关重要，但现有数据存在逻辑不一致问题（如事件顺序错误或缺失事件），这限制了分析模型的可靠性。在足球领域，这些问题会影响球员贡献评估和战术模式识别等精细分析。

Method: 提出VERSA（Verified Event Data Format for Reliable Soccer Analytics）验证框架，基于状态转换模型定义有效事件序列，能够自动检测和纠正事件流数据中的异常模式。

Result: 对K League 1（2024赛季）Bepro提供的事件数据检查发现，18.81%的记录事件存在逻辑不一致。VERSA显著提高了跨数据提供商的一致性，确保异构数据源的稳定统一表示。经VERSA精炼的数据显著提升了VAEP（评估球员贡献的下游任务）的鲁棒性和性能。

Conclusion: 验证过程能有效提高数据驱动分析的可靠性，VERSA框架为解决事件流数据质量问题提供了系统化解决方案，对足球分析及其他领域的精细分析具有重要意义。

Abstract: Event stream data is a critical resource for fine-grained analysis across various domains, including financial transactions, system operations, and sports. In sports, it is actively used for fine-grained analyses such as quantifying player contributions and identifying tactical patterns. However, the reliability of these models is fundamentally limited by inherent data quality issues that cause logical inconsistencies (e.g., incorrect event ordering or missing events). To this end, this study proposes VERSA (Verified Event Data Format for Reliable Soccer Analytics), a systematic verification framework that ensures the integrity of event stream data within the soccer domain. VERSA is based on a state-transition model that defines valid event sequences, thereby enabling the automatic detection and correction of anomalous patterns within the event stream data. Notably, our examination of event data from the K League 1 (2024 season), provided by Bepro, detected that 18.81% of all recorded events exhibited logical inconsistencies. Addressing such integrity issues, our experiments demonstrate that VERSA significantly enhances cross-provider consistency, ensuring stable and unified data representation across heterogeneous sources. Furthermore, we demonstrate that data refined by VERSA significantly improves the robustness and performance of a downstream task called VAEP, which evaluates player contributions. These results highlight that the verification process is highly effective in increasing the reliability of data-driven analysis.

</details>


### [102] [CAR-bench: Evaluating the Consistency and Limit-Awareness of LLM Agents under Real-World Uncertainty](https://arxiv.org/abs/2601.22027)
*Johannes Kirmayr,Lukas Stappen,Elisabeth André*

Main category: cs.AI

TL;DR: CAR-bench是一个针对车载语音助手的LLM智能体基准测试，专注于评估一致性、不确定性处理和能力认知，包含幻觉任务和消歧任务，揭示现有LLM在真实场景中的可靠性问题。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体基准测试主要关注理想环境下的任务完成，忽视了真实用户场景中的可靠性问题。车载语音助手等应用中，用户常提出不完整或模糊请求，智能体需要通过对话、工具使用和策略遵循来管理内在不确定性。

Method: 引入CAR-bench基准测试，包含LLM模拟用户、领域策略和58个互联工具（导航、生产力、充电、车辆控制）。除了标准任务完成，还设计了幻觉任务（测试在工具或信息缺失时的极限认知）和消歧任务（要求通过澄清或内部信息收集解决不确定性）。

Result: 基准测试结果显示，所有任务类型上都存在偶然成功与一致成功之间的巨大差距。即使是前沿推理LLM在消歧任务上的持续通过率也低于50%（由于过早行动），在幻觉任务中经常违反策略或编造信息以满足用户请求。

Conclusion: 研究强调了在真实世界场景中开发更可靠、更具自我认知的LLM智能体的必要性，现有LLM在处理不确定性、遵守策略和认知自身能力限制方面存在显著不足。

Abstract: Existing benchmarks for Large Language Model (LLM) agents focus on task completion under idealistic settings but overlook reliability in real-world, user-facing applications. In domains, such as in-car voice assistants, users often issue incomplete or ambiguous requests, creating intrinsic uncertainty that agents must manage through dialogue, tool use, and policy adherence. We introduce CAR-bench, a benchmark for evaluating consistency, uncertainty handling, and capability awareness in multi-turn, tool-using LLM agents in an in-car assistant domain. The environment features an LLM-simulated user, domain policies, and 58 interconnected tools spanning navigation, productivity, charging, and vehicle control. Beyond standard task completion, CAR-bench introduces Hallucination tasks that test agents' limit-awareness under missing tools or information, and Disambiguation tasks that require resolving uncertainty through clarification or internal information gathering. Baseline results reveal large gaps between occasional and consistent success on all task types. Even frontier reasoning LLMs achieve less than 50% consistent pass rate on Disambiguation tasks due to premature actions, and frequently violate policies or fabricate information to satisfy user requests in Hallucination tasks, underscoring the need for more reliable and self-aware LLM agents in real-world settings.

</details>


### [103] [Defining Operational Conditions for Safety-Critical AI-Based Systems from Data](https://arxiv.org/abs/2601.22118)
*Johann Christensen,Elena Hoemann,Frank Köster,Sven Hallerbach*

Main category: cs.AI

TL;DR: 本文提出了一种基于多维度核表示的安全设计方法，用于从已收集数据中后验定义操作设计域（ODD），并通过蒙特卡洛方法和真实航空用例验证，证明数据驱动的ODD可以等于原始隐藏的ODD。


<details>
  <summary>Details</summary>
Motivation: 在安全关键AI系统中，传统ODD定义依赖于早期开发阶段的专家知识和标准，但现实世界复杂系统或已有数据情况下，环境条件定义极具挑战性，导致ODD描述不完整，而ODD又是AI系统认证的必要条件。

Method: 提出一种安全设计方法，使用多维度核表示从先前收集的数据中后验定义ODD，通过蒙特卡洛方法和真实航空用例（未来安全关键防撞系统）进行验证，并定义了判断两个ODD是否相等的条件。

Result: 验证表明数据驱动的ODD可以等于原始隐藏的ODD，基于核的安全设计ODD方法能够支持未来数据驱动的安全关键AI系统的认证。

Conclusion: 提出的安全设计核基ODD方法为数据驱动的安全关键AI系统认证提供了有效途径，解决了传统ODD定义在复杂现实环境中的局限性问题。

Abstract: Artificial Intelligence (AI) has been on the rise in many domains, including numerous safety-critical applications. However, for complex systems found in the real world, or when data already exist, defining the underlying environmental conditions is extremely challenging. This often results in an incomplete description of the environment in which the AI-based system must operate. Nevertheless, this description, called the Operational Design Domain (ODD), is required in many domains for the certification of AI-based systems. Traditionally, the ODD is created in the early stages of the development process, drawing on sophisticated expert knowledge and related standards. This paper presents a novel Safety-by-Design method to a posteriori define the ODD from previously collected data using a multi-dimensional kernel-based representation. This approach is validated through both Monte Carlo methods and a real-world aviation use case for a future safety-critical collision-avoidance system. Moreover, by defining under what conditions two ODDs are equal, the paper shows that the data-driven ODD can equal the original, underlying hidden ODD of the data. Utilizing the novel, Safe-by-Design kernel-based ODD enables future certification of data-driven, safety-critical AI-based systems.

</details>


### [104] [Exploring Reasoning Reward Model for Agents](https://arxiv.org/abs/2601.22154)
*Kaixuan Fan,Kaituo Feng,Manyuan Zhang,Tianshuo Peng,Zhixun Li,Yilei Jiang,Shuang Chen,Peng Pei,Xunliang Cai,Xiangyu Yue*

Main category: cs.AI

TL;DR: 本文提出Agent Reasoning Reward Model (Agent-RRM)，一种为智能体轨迹提供结构化反馈的多方面奖励模型，包含推理轨迹、针对性批评和总体评分，并通过三种集成策略显著提升智能体强化学习性能。


<details>
  <summary>Details</summary>
Motivation: 当前智能体强化学习方法主要依赖稀疏的结果奖励进行训练，这种反馈无法区分中间推理质量，导致训练结果不理想。需要更精细的反馈机制来指导智能体的推理过程。

Method: 提出Agent-RRM奖励模型，提供三方面结构化反馈：1)显式推理轨迹；2)针对性批评，突出推理缺陷并提供改进指导；3)总体过程评分。基于这些信号，研究了三种集成策略：Reagent-C（文本增强细化）、Reagent-R（奖励增强指导）和Reagent-U（统一反馈集成）。

Result: 在12个多样化基准测试上的广泛评估表明，Reagent-U策略实现了显著的性能飞跃，在GAIA上达到43.7%，在WebWalkerQA上达到46.2%，验证了推理奖励模型和训练方案的有效性。

Conclusion: Agent-RRM通过提供结构化反馈解决了稀疏奖励的局限性，显著提升了智能体强化学习的性能。代码、模型和数据集均已发布以促进未来研究。

Abstract: Agentic Reinforcement Learning (Agentic RL) has achieved notable success in enabling agents to perform complex reasoning and tool use. However, most methods still relies on sparse outcome-based reward for training. Such feedback fails to differentiate intermediate reasoning quality, leading to suboptimal training results. In this paper, we introduce Agent Reasoning Reward Model (Agent-RRM), a multi-faceted reward model that produces structured feedback for agentic trajectories, including (1) an explicit reasoning trace , (2) a focused critique that provides refinement guidance by highlighting reasoning flaws, and (3) an overall score that evaluates process performance. Leveraging these signals, we systematically investigate three integration strategies: Reagent-C (text-augmented refinement), Reagent-R (reward-augmented guidance), and Reagent-U (unified feedback integration). Extensive evaluations across 12 diverse benchmarks demonstrate that Reagent-U yields substantial performance leaps, achieving 43.7% on GAIA and 46.2% on WebWalkerQA, validating the effectiveness of our reasoning reward model and training schemes. Code, models, and datasets are all released to facilitate future research.

</details>
