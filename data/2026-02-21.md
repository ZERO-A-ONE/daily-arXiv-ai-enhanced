<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 5]
- [cs.AI](#cs.AI) [Total: 36]
- [cs.SE](#cs.SE) [Total: 6]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [The Vulnerability of LLM Rankers to Prompt Injection Attacks](https://arxiv.org/abs/2602.16752)
*Yu Yin,Shuai Wang,Bevan Koopman,Guido Zuccon*

Main category: cs.CR

TL;DR: 本文对LLM排序器面临的越狱提示攻击进行了全面的实证研究，评估了不同LLM家族、架构和设置下的漏洞程度，发现编码器-解码器架构具有更强的内在抗攻击能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型已成为强大的重排序工具，但研究发现简单的提示注入攻击可以显著改变LLM的排序决策。然而，这种漏洞在不同LLM家族、架构和设置中的普遍程度尚未得到充分探索，这对基于LLM的排序管道构成了严重的安全风险。

Method: 研究采用综合实证方法，评估两个互补任务：1）偏好漏洞评估，通过攻击成功率测量内在易感性；2）排序漏洞评估，量化对排序质量的影响。系统考察了三种主流排序范式（成对、列表、集合）和两种注入变体（决策目标劫持和决策标准劫持），并扩展分析了模型家族、位置敏感性、骨干架构和跨域鲁棒性。

Result: 研究结果揭示了这些漏洞的边界条件，关键发现包括：编码器-解码器架构对越狱攻击表现出强大的内在韧性。研究还发现漏洞在不同模型家族中存在缩放特性，并识别了位置敏感性和跨域鲁棒性的模式。

Conclusion: 该研究全面描述了LLM排序器对越狱提示攻击的漏洞边界，为开发更安全的LLM排序系统提供了重要见解。特别值得注意的是编码器-解码器架构的强抗攻击能力，这为未来设计安全的排序模型提供了有价值的参考方向。

Abstract: Large Language Models (LLMs) have emerged as powerful re-rankers. Recent research has however showed that simple prompt injections embedded within a candidate document (i.e., jailbreak prompt attacks) can significantly alter an LLM's ranking decisions. While this poses serious security risks to LLM-based ranking pipelines, the extent to which this vulnerability persists across diverse LLM families, architectures, and settings remains largely under-explored. In this paper, we present a comprehensive empirical study of jailbreak prompt attacks against LLM rankers. We focus our evaluation on two complementary tasks: (1) Preference Vulnerability Assessment, measuring intrinsic susceptibility via attack success rate (ASR); and (2) Ranking Vulnerability Assessment, quantifying the operational impact on the ranking's quality (nDCG@10). We systematically examine three prevalent ranking paradigms (pairwise, listwise, setwise) under two injection variants: decision objective hijacking and decision criteria hijacking. Beyond reproducing prior findings, we expand the analysis to cover vulnerability scaling across model families, position sensitivity, backbone architectures, and cross-domain robustness. Our results characterize the boundary conditions of these vulnerabilities, revealing critical insights such as that encoder-decoder architectures exhibit strong inherent resilience to jailbreak attacks. We publicly release our code and additional experimental results at https://github.com/ielab/LLM-Ranker-Attack.

</details>


### [2] [Large-scale online deanonymization with LLMs](https://arxiv.org/abs/2602.16800)
*Simon Lermen,Daniel Paleka,Joshua Swanson,Michael Aerni,Nicholas Carlini,Florian Tramèr*

Main category: cs.CR

TL;DR: LLM可实现大规模去匿名化攻击，通过分析用户在线文本内容，能够高精度识别Hacker News和Anthropic Interviewer的匿名用户，性能远超传统方法。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索大型语言模型在去匿名化攻击中的能力，验证在线匿名用户的"实际模糊性"是否仍然有效，重新评估在线隐私的威胁模型。

Method: 设计了基于LLM的攻击流程：1) 提取身份相关特征；2) 通过语义嵌入搜索候选匹配；3) 对候选匹配进行推理验证。构建了三个真实数据集进行评估：Hacker News-LinkedIn匹配、Reddit电影社区跨用户匹配、单个用户Reddit历史时间分割匹配。

Result: LLM方法显著优于传统基线方法，在90%精度下达到68%召回率，而最佳非LLM方法接近0%。证明了LLM能够高效完成需要人类调查员数小时工作的去匿名化任务。

Conclusion: 在线匿名用户的"实际模糊性"保护已不再有效，基于LLM的去匿名化攻击对在线隐私构成严重威胁，需要重新考虑在线隐私的威胁模型。

Abstract: We show that large language models can be used to perform at-scale deanonymization. With full Internet access, our agent can re-identify Hacker News users and Anthropic Interviewer participants at high precision, given pseudonymous online profiles and conversations alone, matching what would take hours for a dedicated human investigator. We then design attacks for the closed-world setting. Given two databases of pseudonymous individuals, each containing unstructured text written by or about that individual, we implement a scalable attack pipeline that uses LLMs to: (1) extract identity-relevant features, (2) search for candidate matches via semantic embeddings, and (3) reason over top candidates to verify matches and reduce false positives. Compared to prior deanonymization work (e.g., on the Netflix prize) that required structured data or manual feature engineering, our approach works directly on raw user content across arbitrary platforms. We construct three datasets with known ground-truth data to evaluate our attacks. The first links Hacker News to LinkedIn profiles, using cross-platform references that appear in the profiles. Our second dataset matches users across Reddit movie discussion communities; and the third splits a single user's Reddit history in time to create two pseudonymous profiles to be matched. In each setting, LLM-based methods substantially outperform classical baselines, achieving up to 68% recall at 90% precision compared to near 0% for the best non-LLM method. Our results show that the practical obscurity protecting pseudonymous users online no longer holds and that threat models for online privacy need to be reconsidered.

</details>


### [3] [What Breaks Embodied AI Security:LLM Vulnerabilities, CPS Flaws,or Something Else?](https://arxiv.org/abs/2602.17345)
*Boyang Ma,Hechuan Guo,Peizhuo Lv,Minghui Xu,Xuelong Dai,YeChao Zhang,Yijun Yang,Yue Zhang*

Main category: cs.CR

TL;DR: 本文认为现有研究从LLM漏洞或传统CPS故障角度分析具身AI存在不足，提出具身AI安全的核心挑战源于系统级不匹配，而非孤立模型缺陷，并识别了四个根本性难点。


<details>
  <summary>Details</summary>
Motivation: 具身AI系统正从受控环境转向安全关键的实际部署，其故障会导致不可逆的物理后果。现有研究主要从LLM漏洞或传统CPS故障角度分析，但这些视角单独无法解释现代具身系统中的许多故障现象。

Method: 本文采用调查分析的方法，提出具身AI故障源于"具身化引发的系统级不匹配"这一核心论点，并识别了四个关键洞察来解释为什么具身AI本质上更难保障安全。

Result: 识别了四个核心洞察：(1)语义正确性不保证物理安全性；(2)相同动作在不同物理状态下可能导致截然不同的结果；(3)小误差在紧密耦合的感知-决策-行动循环中传播放大；(4)安全性在时间或系统层级上不具备组合性。

Conclusion: 保障具身AI安全需要超越组件级防御，转向系统级的物理风险、不确定性和故障传播推理。这要求新的安全框架能够处理具身系统特有的复杂性和非线性特性。

Abstract: Embodied AI systems (e.g., autonomous vehicles, service robots, and LLM-driven interactive agents) are rapidly transitioning from controlled environments to safety critical real-world deployments. Unlike disembodied AI, failures in embodied intelligence lead to irreversible physical consequences, raising fundamental questions about security, safety, and reliability. While existing research predominantly analyzes embodied AI through the lenses of Large Language Model (LLM) vulnerabilities or classical Cyber-Physical System (CPS) failures, this survey argues that these perspectives are individually insufficient to explain many observed breakdowns in modern embodied systems. We posit that a significant class of failures arises from embodiment-induced system-level mismatches, rather than from isolated model flaws or traditional CPS attacks. Specifically, we identify four core insights that explain why embodied AI is fundamentally harder to secure: (i) semantic correctness does not imply physical safety, as language-level reasoning abstracts away geometry, dynamics, and contact constraints; (ii) identical actions can lead to drastically different outcomes across physical states due to nonlinear dynamics and state uncertainty; (iii) small errors propagate and amplify across tightly coupled perception-decision-action loops; and (iv) safety is not compositional across time or system layers, enabling locally safe decisions to accumulate into globally unsafe behavior. These insights suggest that securing embodied AI requires moving beyond component-level defenses toward system-level reasoning about physical risk, uncertainty, and failure propagation.

</details>


### [4] [Intent Laundering: AI Safety Datasets Are Not What They Seem](https://arxiv.org/abs/2602.16729)
*Shahriar Golchin,Marc Wetter*

Main category: cs.CR

TL;DR: 研究发现当前AI安全数据集过度依赖"触发线索"（具有明显负面/敏感含义的词语），无法真实反映现实世界攻击，通过"意图清洗"技术去除这些线索后，所有先前评估为"相对安全"的模型都变得不安全。


<details>
  <summary>Details</summary>
Motivation: 当前AI安全数据集的质量存在疑问，需要系统评估这些数据集是否真实反映现实世界攻击，以及它们是否真正衡量安全风险还是仅仅通过触发线索引发拒绝。

Method: 从两个角度评估：1）孤立评估：检查数据集是否反映现实攻击的三个关键特性（恶意意图驱动、精心设计、分布外）；2）实践评估：引入"意图清洗"技术，抽象去除攻击中的触发线索，同时严格保留恶意意图和相关细节。

Result: 发现当前AI安全数据集过度依赖触发线索，无法忠实代表现实攻击。去除触发线索后，所有先前评估为"相对安全"的模型（包括Gemini 3 Pro和Claude Sonnet 3.7）都变得不安全。意图清洗作为越狱技术，在黑盒访问下攻击成功率高达90%-98%。

Conclusion: 当前AI安全评估方法与现实世界攻击者行为存在显著脱节，需要开发更真实反映现实攻击的安全数据集和评估方法。

Abstract: We systematically evaluate the quality of widely used AI safety datasets from two perspectives: in isolation and in practice. In isolation, we examine how well these datasets reflect real-world attacks based on three key properties: driven by ulterior intent, well-crafted, and out-of-distribution. We find that these datasets overrely on "triggering cues": words or phrases with overt negative/sensitive connotations that are intended to trigger safety mechanisms explicitly, which is unrealistic compared to real-world attacks. In practice, we evaluate whether these datasets genuinely measure safety risks or merely provoke refusals through triggering cues. To explore this, we introduce "intent laundering": a procedure that abstracts away triggering cues from attacks (data points) while strictly preserving their malicious intent and all relevant details. Our results indicate that current AI safety datasets fail to faithfully represent real-world attacks due to their overreliance on triggering cues. In fact, once these cues are removed, all previously evaluated "reasonably safe" models become unsafe, including Gemini 3 Pro and Claude Sonnet 3.7. Moreover, when intent laundering is adapted as a jailbreaking technique, it consistently achieves high attack success rates, ranging from 90% to over 98%, under fully black-box access. Overall, our findings expose a significant disconnect between how model safety is evaluated and how real-world adversaries behave.

</details>


### [5] [Can Adversarial Code Comments Fool AI Security Reviewers -- Large-Scale Empirical Study of Comment-Based Attacks and Defenses Against LLM Code Analysis](https://arxiv.org/abs/2602.16741)
*Scott Thornton*

Main category: cs.CR

TL;DR: 对抗性注释对LLM漏洞检测性能影响有限，与代码生成场景不同，检测准确率未显著下降，复杂攻击策略无优势


<details>
  <summary>Details</summary>
Motivation: 研究对抗性提示操作是否会影响大型语言模型在漏洞检测中的性能，与代码生成场景中的已知影响进行对比

Method: 构建包含Python、JavaScript和Java的100个样本基准，每个样本配8种注释变体（从无注释到权威欺骗和技术欺骗等对抗策略），在9,366次试验中评估8个前沿模型（5个商业模型和3个开源模型）

Result: 对抗性注释对检测准确率产生微小且统计上不显著的影响，商业模型基线检测率为89-96%，开源模型为53-72%，复杂对抗策略相比简单操纵性注释无优势，静态分析交叉引用防御效果最佳

Conclusion: 与代码生成场景不同，对抗性注释操作不会显著降低LLM在漏洞检测中的性能，失败主要集中在固有的困难漏洞类别而非对抗性注释

Abstract: AI-assisted code review is widely used to detect vulnerabilities before production release. Prior work shows that adversarial prompt manipulation can degrade large language model (LLM) performance in code generation. We test whether similar comment-based manipulation misleads LLMs during vulnerability detection. We build a 100-sample benchmark across Python, JavaScript, and Java, each paired with eight comment variants ranging from no comments to adversarial strategies such as authority spoofing and technical deception. Eight frontier models, five commercial and three open-source, are evaluated in 9,366 trials. Adversarial comments produce small, statistically non-significant effects on detection accuracy (McNemar exact p > 0.21; all 95 percent confidence intervals include zero). This holds for commercial models with 89 to 96 percent baseline detection and open-source models with 53 to 72 percent, despite large absolute performance gaps. Unlike generation settings where comment manipulation achieves high attack success, detection performance does not meaningfully degrade. More complex adversarial strategies offer no advantage over simple manipulative comments. We test four automated defenses across 4,646 additional trials (14,012 total). Static analysis cross-referencing performs best at 96.9 percent detection and recovers 47 percent of baseline misses. Comment stripping reduces detection for weaker models by removing helpful context. Failures concentrate on inherently difficult vulnerability classes, including race conditions, timing side channels, and complex authorization logic, rather than on adversarial comments.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [6] [AIdentifyAGE Ontology for Decision Support in Forensic Dental Age Assessment](https://arxiv.org/abs/2602.16714)
*Renato Marcelo,Ana Rodrigues,Cristiana Palmela Pereira,António Figueiras,Rui Santos,José Rui Figueira,Alexandre P Francisco,Cátia Vaz*

Main category: cs.AI

TL;DR: AIdentifyAGE本体是一个专门用于法医牙科年龄评估的标准化语义框架，旨在解决当前方法异质性、数据碎片化和系统互操作性不足的问题，通过整合人工和AI辅助的工作流程，提高法医年龄评估的透明度、可重复性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 年龄评估在法医和司法决策中至关重要，特别是在涉及无证个人和无人陪伴未成年人的案件中，法律阈值决定了他们能否获得保护、医疗保健和司法程序。当前牙科年龄评估实践面临方法异质性、数据表示碎片化以及临床、法医和法律信息系统之间互操作性有限等挑战，这些问题阻碍了透明度和可重复性，而AI方法的日益普及进一步放大了这些局限性。

Method: 开发AIdentifyAGE本体作为领域特定的标准化语义框架，涵盖人工和AI辅助的法医牙科年龄评估工作流程。该本体建模完整的法医-法律工作流程，整合司法背景、个体层面信息、法医检查数据、牙齿发育评估方法、放射影像、统计参考研究和基于AI的估计方法。该本体与领域专家共同开发，并建立在现有的上层本体以及生物医学、牙科和机器学习本体之上，确保互操作性、可扩展性和符合FAIR原则。

Result: AIdentifyAGE本体提供了一个标准化的语义框架，能够追踪观察、方法、参考数据和报告结果之间的可追溯链接。它建立了增强一致性、透明度和可解释性的基础，为法医-法律和司法背景下的本体驱动决策支持系统奠定了坚实基础。

Conclusion: AIdentifyAGE本体是提高法医牙科年龄评估一致性、透明度和可解释性的关键步骤，为法医-法律和司法背景下的本体驱动决策支持系统建立了稳健基础，有助于改善当前实践中的方法异质性和互操作性挑战。

Abstract: Age assessment is crucial in forensic and judicial decision-making, particularly in cases involving undocumented individuals and unaccompanied minors, where legal thresholds determine access to protection, healthcare, and judicial procedures. Dental age assessment is widely recognized as one of the most reliable biological approaches for adolescents and young adults, but current practices are challenged by methodological heterogeneity, fragmented data representation, and limited interoperability between clinical, forensic, and legal information systems. These limitations hinder transparency and reproducibility, amplified by the increasing adoption of AI- based methods. The AIdentifyAGE ontology is domain-specific and provides a standardized, semantically coherent framework, encompassing both manual and AI-assisted forensic dental age assessment workflows, and enabling traceable linkage between observations, methods, reference data, and reported outcomes. It models the complete medico-legal workflow, integrating judicial context, individual-level information, forensic examination data, dental developmental assessment methods, radiographic imaging, statistical reference studies, and AI-based estimation methods. It is being developed together with domain experts, and it builds on upper and established biomedical, dental, and machine learning ontologies, ensuring interoperability, extensibility, and compliance with FAIR principles. The AIdentifyAGE ontology is a fundamental step to enhance consistency, transparency, and explainability, establishing a robust foundation for ontology-driven decision support systems in medico-legal and judicial contexts.

</details>


### [7] [Contextuality from Single-State Representations: An Information-Theoretic Principle for Adaptive Intelligence](https://arxiv.org/abs/2602.16716)
*Song-Ju Kim*

Main category: cs.AI

TL;DR: 单状态复用的经典概率表示必然产生语境性，这是自适应智能的普遍表示约束


<details>
  <summary>Details</summary>
Motivation: 自适应系统经常在多个上下文中运行，但由于内存、表示或物理资源的限制，它们重复使用固定的内部状态空间。这种单状态复用是自然和人工智能中普遍存在的现象，但其基本的表示后果仍未被充分理解。

Method: 将上下文建模为作用于共享内部状态的干预措施，证明任何再现语境性结果统计的经典模型都必须承担不可约的信息论成本。提供了一个最小构造性示例来明确实现这一成本并澄清其操作意义。

Result: 证明了语境性不是量子力学的特性，而是经典概率表示中单状态复用的必然结果。语境依赖性不能仅通过内部状态来调节，必须承担信息论成本。非经典概率框架通过放宽单一全局联合概率空间的假设来避免这种障碍。

Conclusion: 语境性是自适应智能的普遍表示约束，与物理实现无关。单状态复用的经典表示必然产生语境性，这为理解自适应系统的表示能力提供了理论基础。

Abstract: Adaptive systems often operate across multiple contexts while reusing a fixed internal state space due to constraints on memory, representation, or physical resources. Such single-state reuse is ubiquitous in natural and artificial intelligence, yet its fundamental representational consequences remain poorly understood. We show that contextuality is not a peculiarity of quantum mechanics, but an inevitable consequence of single-state reuse in classical probabilistic representations. Modeling contexts as interventions acting on a shared internal state, we prove that any classical model reproducing contextual outcome statistics must incur an irreducible information-theoretic cost: dependence on context cannot be mediated solely through the internal state. We provide a minimal constructive example that explicitly realizes this cost and clarifies its operational meaning. We further explain how nonclassical probabilistic frameworks avoid this obstruction by relaxing the assumption of a single global joint probability space, without invoking quantum dynamics or Hilbert space structure. Our results identify contextuality as a general representational constraint on adaptive intelligence, independent of physical implementation.

</details>


### [8] [Mobility-Aware Cache Framework for Scalable LLM-Based Human Mobility Simulation](https://arxiv.org/abs/2602.16727)
*Hua Yan,Heng Tan,Yingxue Zhang,Yu Yang*

Main category: cs.AI

TL;DR: MobCache是一个用于大规模人类移动仿真的缓存框架，通过可重构缓存和轻量级解码器显著提高效率，同时保持与最先进LLM方法相当的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的人类移动仿真方法虽然能模拟真实移动行为，但计算成本过高，限制了大规模应用的可扩展性。

Method: 设计MobCache框架，包含两个组件：1) 推理组件将推理步骤编码为潜在空间嵌入，使用潜在空间评估器实现推理步骤的重用和重组；2) 解码组件采用移动规律约束蒸馏训练的轻量级解码器，将潜在空间推理链转换为自然语言。

Result: 实验表明，MobCache在多个维度上显著提高了效率，同时保持了与最先进的基于LLM方法相当的性能。

Conclusion: MobCache通过创新的缓存框架解决了大规模人类移动仿真的效率瓶颈问题，为城市规划、流行病学和交通分析等应用提供了可行的解决方案。

Abstract: Large-scale human mobility simulation is critical for applications such as urban planning, epidemiology, and transportation analysis. Recent works treat large language models (LLMs) as human agents to simulate realistic mobility behaviors using structured reasoning, but their high computational cost limits scalability. To address this, we design a mobility-aware cache framework named MobCache that leverages reconstructible caches to enable efficient large-scale human mobility simulations. It consists of: (1) a reasoning component that encodes each reasoning step as a latent-space embedding and uses a latent-space evaluator to enable the reuse and recombination of reasoning steps; and (2) a decoding component that employs a lightweight decoder trained with mobility law-constrained distillation to translate latent-space reasoning chains into natural language, thereby improving simulation efficiency while maintaining fidelity. Experiments show that MobCache significantly improves efficiency across multiple dimensions while maintaining performance comparable to state-of-the-art LLM-based methods.

</details>


### [9] [When AI Benchmarks Plateau: A Systematic Study of Benchmark Saturation](https://arxiv.org/abs/2602.16763)
*Mubashara Akhtar,Anka Reuel,Prajna Soni,Sanchit Ahuja,Pawan Sasanka Ammanamanchi,Ruchit Rawal,Vilém Zouhar,Srishti Yadav,Chenxi Whitehouse,Dayeon Ki,Jennifer Mickel,Leshem Choshen,Marek Šuppa,Jan Batzner,Jenny Chim,Jeba Sania,Yanan Long,Hossein A. Rahmani,Christina Knight,Yiyang Nan,Jyoutir Raj,Yu Fan,Shubham Singh,Subramanyam Sahoo,Eliya Habba,Usman Gohar,Siddhesh Pawar,Robert Scholz,Arjun Subramonian,Jingwei Ni,Mykel Kochenderfer,Sanmi Koyejo,Mrinmaya Sachan,Stella Biderman,Zeerak Talat,Avijit Ghosh,Irene Solaiman*

Main category: cs.AI

TL;DR: 研究分析了60个大型语言模型基准测试的饱和现象，发现近半数基准已饱和，饱和率随基准年龄增长而增加。专家策划的基准比众包基准更能抵抗饱和，而隐藏测试数据对防止饱和没有保护作用。


<details>
  <summary>Details</summary>
Motivation: AI基准测试在衡量模型进展和指导部署决策中起核心作用，但许多基准测试很快饱和，无法区分最佳性能模型，降低了其长期价值。需要了解基准饱和现象及其驱动因素。

Method: 从主要模型开发商的技术报告中选取60个大型语言模型基准测试，从任务设计、数据构建和评估格式三个维度定义14个基准特性，测试5个假设来检验每个特性对饱和率的影响。

Result: 近一半的基准测试表现出饱和现象，饱和率随基准年龄增长而增加。隐藏测试数据（公开vs私有）没有保护作用，而专家策划的基准比众包基准更能抵抗饱和。

Conclusion: 研究揭示了哪些设计选择能延长基准测试的寿命，为创建更持久的评估策略提供了信息，有助于设计更有效的AI基准测试。

Abstract: Artificial Intelligence (AI) benchmarks play a central role in measuring progress in model development and guiding deployment decisions. However, many benchmarks quickly become saturated, meaning that they can no longer differentiate between the best-performing models, diminishing their long-term value. In this study, we analyze benchmark saturation across 60 Large Language Model (LLM) benchmarks selected from technical reports by major model developers. To identify factors driving saturation, we characterize benchmarks along 14 properties spanning task design, data construction, and evaluation format. We test five hypotheses examining how each property contributes to saturation rates. Our analysis reveals that nearly half of the benchmarks exhibit saturation, with rates increasing as benchmarks age. Notably, hiding test data (i.e., public vs. private) shows no protective effect, while expert-curated benchmarks resist saturation better than crowdsourced ones. Our findings highlight which design choices extend benchmark longevity and inform strategies for more durable evaluation.

</details>


### [10] [Simple Baselines are Competitive with Code Evolution](https://arxiv.org/abs/2602.16805)
*Yonatan Gideoni,Sebastian Risi,Yarin Gal*

Main category: cs.AI

TL;DR: 简单基准方法在代码进化任务中表现优于复杂方法，搜索空间设计和领域知识比进化算法本身更重要


<details>
  <summary>Details</summary>
Motivation: 许多代码进化技术展示了令人印象深刻的性能，但往往没有与更简单的基准方法进行比较，需要验证这些复杂方法是否真的比简单方法更有效

Method: 在三个领域测试简单基准方法：寻找更好的数学界限、设计智能体脚手架、机器学习竞赛，并与更复杂的方法进行比较

Result: 简单基准方法在所有三个领域都匹配或超过了更复杂的方法；数学界限任务中搜索空间和领域知识是性能关键；智能体脚手架设计中高方差导致次优选择

Conclusion: 代码进化研究需要更严格的评估方法，减少评估随机性，关注搜索空间设计和领域知识，提出未来工作的最佳实践

Abstract: Code evolution is a family of techniques that rely on large language models to search through possible computer programs by evolving or mutating existing code. Many proposed code evolution pipelines show impressive performance but are often not compared to simpler baselines. We test how well two simple baselines do over three domains: finding better mathematical bounds, designing agentic scaffolds, and machine learning competitions. We find that simple baselines match or exceed much more sophisticated methods in all three. By analyzing these results we find various shortcomings in how code evolution is both developed and used. For the mathematical bounds, a problem's search space and domain knowledge in the prompt are chiefly what dictate a search's performance ceiling and efficiency, with the code evolution pipeline being secondary. Thus, the primary challenge in finding improved bounds is designing good search spaces, which is done by domain experts, and not the search itself. When designing agentic scaffolds we find that high variance in the scaffolds coupled with small datasets leads to suboptimal scaffolds being selected, resulting in hand-designed majority vote scaffolds performing best. We propose better evaluation methods that reduce evaluation stochasticity while keeping the code evolution economically feasible. We finish with a discussion of avenues and best practices to enable more rigorous code evolution in future work.

</details>


### [11] [Improved Upper Bounds for Slicing the Hypercube](https://arxiv.org/abs/2602.16807)
*Duncan Soiffer,Nathaniel Itty,Christopher D. Rosin,Blake Bruell,Mason DiCicco,Gábor N. Sárközy,Ryan Offstein,Daniel Reichman*

Main category: cs.AI

TL;DR: 该论文研究了用超平面切片n维超立方体所有边的最小数量问题，改进了已知上界，并利用AI工具发现了新的构造方法。


<details>
  <summary>Details</summary>
Motivation: 研究n维超立方体Q_n中，用超平面切片所有边的最小数量S(n)问题。这是一个经典的组合几何问题，自1971年Paterson给出上界S(n) ≤ ⌈5n/6⌉以来，一直缺乏改进。

Method: 1. 构造了8个超平面切片Q_{10}的实例；2. 利用新开发的AI工具CPro1，该工具结合推理大语言模型和自动超参数调优，为数学构造发现创建搜索算法；3. 基于Q_{10}的构造推导出一般n的上界。

Result: 证明了S(n) ≤ ⌈4n/5⌉，除非n是5的奇数倍时S(n) ≤ 4n/5 + 1。这改进了Paterson 1971年的上界S(n) ≤ ⌈5n/6⌉。同时获得了k<n个超平面能切片的最大边数的新下界。

Conclusion: 该研究显著改进了超立方体边切片问题的上界，并展示了AI辅助数学发现的有效性，特别是CPro1工具在寻找复杂数学构造方面的潜力。

Abstract: A collection of hyperplanes $\mathcal{H}$ slices all edges of the $n$-dimensional hypercube $Q_n$ with vertex set $\{-1,1\}^n$ if, for every edge $e$ in the hypercube, there exists a hyperplane in $\mathcal{H}$ intersecting $e$ in its interior. Let $S(n)$ be the minimum number of hyperplanes needed to slice $Q_n$. We prove that $S(n) \leq \lceil \frac{4n}{5} \rceil$, except when $n$ is an odd multiple of $5$, in which case $S(n) \leq \frac{4n}{5} +1$. This improves upon the previously known upper bound of $S(n) \leq \lceil\frac{5n}{6} \rceil$ due to Paterson reported in 1971. We also obtain new lower bounds on the maximum number of edges in $Q_n$ that can be sliced using $k<n$ hyperplanes. We prove the improved upper bound on $S(n)$ by constructing $8$ hyperplanes slicing $Q_{10}$ aided by the recently introduced CPro1: an automatic tool that uses reasoning LLMs coupled with automated hyperparameter tuning to create search algorithms for the discovery of mathematical constructions.

</details>


### [12] [NeuDiff Agent: A Governed AI Workflow for Single-Crystal Neutron Crystallography](https://arxiv.org/abs/2602.16812)
*Zhongcan Xiao,Leyi Zhang,Guannan Zhang,Xiaoping Wang*

Main category: cs.AI

TL;DR: NeuDiff Agent是一个受治理的AI工作流，用于中子源设施的数据分析，将仪器数据自动处理为验证的晶体结构和可发表的CIF文件，显著缩短分析时间。


<details>
  <summary>Details</summary>
Motivation: 大规模科学设施面临分析延迟问题，特别是结构复杂的样品需要迭代处理，这成为科学产出的瓶颈。需要提高分析效率和缩短结果获取时间。

Method: 开发NeuDiff Agent作为受治理的AI工作流，使用允许列表工具，在关键工作流边界实施故障关闭验证门，并捕获完整的溯源信息。通过固定提示协议和重复端到端运行评估性能。

Result: 在基准测试中，NeuDiff Agent将分析时间从435分钟（手动）减少到86.5-94.4分钟（4.6-5.0倍加速），同时生成无checkCIF A/B级警报的验证CIF文件。

Conclusion: NeuDiff Agent为设施晶体学部署智能AI提供了实用途径，同时保持了可追溯性和面向发表的验证要求。

Abstract: Large-scale facilities increasingly face analysis and reporting latency as the limiting step in scientific throughput, particularly for structurally and magnetically complex samples that require iterative reduction, integration, refinement, and validation. To improve time-to-result and analysis efficiency, NeuDiff Agent is introduced as a governed, tool-using AI workflow for TOPAZ at the Spallation Neutron Source that takes instrument data products through reduction, integration, refinement, and validation to a validated crystal structure and a publication-ready CIF. NeuDiff Agent executes this established pipeline under explicit governance by restricting actions to allowlisted tools, enforcing fail-closed verification gates at key workflow boundaries, and capturing complete provenance for inspection, auditing, and controlled replay. Performance is assessed using a fixed prompt protocol and repeated end-to-end runs with two large language model backends, with user and machine time partitioned and intervention burden and recovery behaviors quantified under gating. In a reference-case benchmark, NeuDiff Agent reduces wall time from 435 minutes (manual) to 86.5(4.7) to 94.4(3.5) minutes (4.6-5.0x faster) while producing a validated CIF with no checkCIF level A or B alerts. These results establish a practical route to deploy agentic AI in facility crystallography while preserving traceability and publication-facing validation requirements.

</details>


### [13] [Node Learning: A Framework for Adaptive, Decentralised and Collaborative Network Edge AI](https://arxiv.org/abs/2602.16814)
*Eiman Kanjo,Mustafa Aslanov*

Main category: cs.AI

TL;DR: 提出Node Learning这一去中心化学习范式，将智能置于边缘节点，通过选择性对等交互扩展，避免集中式智能的成本和脆弱性问题。


<details>
  <summary>Details</summary>
Motivation: AI向边缘扩展暴露了集中式智能的成本和脆弱性：数据传输、延迟、能耗、对大型数据中心的依赖在异构、移动和资源受限环境中扩展性差。

Method: Node Learning：去中心化学习范式，智能驻留在单个边缘节点，通过选择性对等交互扩展。节点从本地数据持续学习，维护自身模型状态，在协作有益时机会性地交换学习知识。学习通过重叠和扩散传播，而非全局同步或中心聚合。

Result: 概念性论文，未提供具体实验结果。但提出了统一自主和协作行为的抽象框架，能适应数据、硬件、目标和连接性的异构性。

Conclusion: Node Learning为去中心化学习提供了概念基础，不抛弃现有范式，而是将其置于更广泛的去中心化视角中，对通信、硬件、信任和治理有重要影响。

Abstract: The expansion of AI toward the edge increasingly exposes the cost and fragility of cen- tralised intelligence. Data transmission, latency, energy consumption, and dependence on large data centres create bottlenecks that scale poorly across heterogeneous, mobile, and resource-constrained environments. In this paper, we introduce Node Learning, a decen- tralised learning paradigm in which intelligence resides at individual edge nodes and expands through selective peer interaction. Nodes learn continuously from local data, maintain their own model state, and exchange learned knowledge opportunistically when collaboration is beneficial. Learning propagates through overlap and diffusion rather than global synchro- nisation or central aggregation. It unifies autonomous and cooperative behaviour within a single abstraction and accommodates heterogeneity in data, hardware, objectives, and connectivity. This concept paper develops the conceptual foundations of this paradigm, contrasts it with existing decentralised approaches, and examines implications for communi- cation, hardware, trust, and governance. Node Learning does not discard existing paradigms, but places them within a broader decentralised perspective

</details>


### [14] [An order-oriented approach to scoring hesitant fuzzy elements](https://arxiv.org/abs/2602.16827)
*Luis Merino,Gabriel Navarro,Carlos Salvatierra,Evangelina Santos*

Main category: cs.AI

TL;DR: 本文为犹豫模糊集提出了一种基于序理论的统一评分框架，证明传统序不构成格结构，但对称序满足评分函数的关键规范标准，并引入优势函数用于犹豫模糊元素排序。


<details>
  <summary>Details</summary>
Motivation: 传统犹豫模糊集评分方法缺乏序理论基础，需要建立具有形式化序基础的统一框架，以支持更灵活和一致的评分机制。

Method: 1) 提出基于给定序的显式评分统一框架；2) 分析犹豫模糊元素上的经典序，证明它们不构成格结构；3) 证明对称序定义的评分满足强单调性和Gärdenfors条件等规范标准；4) 引入优势函数用于犹豫模糊元素排序，包含离散优势函数和相对优势函数两种具体实现。

Result: 1) 经典序不诱导格结构，与先前主张相反；2) 对称序定义的评分满足评分函数的关键规范标准；3) 优势函数可用于构建典型犹豫模糊集上的模糊偏好关系并支持群体决策。

Conclusion: 本文建立了基于序理论的犹豫模糊集评分统一框架，证明了对称序的优越性，并提出了实用的优势函数方法，为犹豫模糊环境下的决策提供了理论基础和实用工具。

Abstract: Traditional scoring approaches on hesitant fuzzy sets often lack a formal base in order theory. This paper proposes a unified framework, where each score is explicitly defined with respect to a given order. This order-oriented perspective enables more flexible and coherent scoring mechanisms. We examine several classical orders on hesitant fuzzy elements, that is, nonempty subsets in [0,1], and show that, contrary to prior claims, they do not induce lattice structures. In contrast, we prove that the scores defined with respect to the symmetric order satisfy key normative criteria for scoring functions, including strong monotonicity with respect to unions and the Gärdenfors condition.
  Following this analysis, we introduce a class of functions, called dominance functions, for ranking hesitant fuzzy elements. They aim to compare hesitant fuzzy elements relative to control sets incorporating minimum acceptability thresholds. Two concrete examples of dominance functions for finite sets are provided: the discrete dominance function and the relative dominance function. We show that these can be employed to construct fuzzy preference relations on typical hesitant fuzzy sets and support group decision-making.

</details>


### [15] [OpenSage: Self-programming Agent Generation Engine](https://arxiv.org/abs/2602.16891)
*Hongwei Li,Zhun Wang,Qinrun Dai,Yuzhou Nie,Jinjun Peng,Ruitong Liu,Jingyang Zhang,Kaijie Zhu,Jingxuan He,Lun Wang,Yangruibo Ding,Yueqi Chen,Wenbo Guo,Dawn Song*

Main category: cs.AI

TL;DR: OpenSage是首个支持LLM自动创建具有自生成拓扑结构和工具集的智能体开发套件，提供结构化内存系统，在多个基准测试中优于现有ADK。


<details>
  <summary>Details</summary>
Motivation: 当前智能体开发套件要么功能支持不足，要么依赖人工手动设计拓扑、工具和内存组件，限制了智能体的泛化能力和整体性能。

Method: 提出OpenSage ADK，使LLM能自动创建具有自生成拓扑和工具集的智能体，提供分层图结构内存系统，并包含专门针对软件工程任务的工具包。

Result: 在三个最先进的基准测试中，使用不同骨干模型的实验表明OpenSage优于现有ADK，消融研究验证了各组件设计的有效性。

Conclusion: OpenSage能够为下一代智能体开发铺平道路，将焦点从以人为中心转向以AI为中心的范式。

Abstract: Agent development kits (ADKs) provide effective platforms and tooling for constructing agents, and their designs are critical to the constructed agents' performance, especially the functionality for agent topology, tools, and memory. However, current ADKs either lack sufficient functional support or rely on humans to manually design these components, limiting agents' generalizability and overall performance. We propose OpenSage, the first ADK that enables LLMs to automatically create agents with self-generated topology and toolsets while providing comprehensive and structured memory support. OpenSage offers effective functionality for agents to create and manage their own sub-agents and toolkits. It also features a hierarchical, graph-based memory system for efficient management and a specialized toolkit tailored to software engineering tasks. Extensive experiments across three state-of-the-art benchmarks with various backbone models demonstrate the advantages of OpenSage over existing ADKs. We also conduct rigorous ablation studies to demonstrate the effectiveness of our design for each component. We believe OpenSage can pave the way for the next generation of agent development, shifting the focus from human-centered to AI-centered paradigms.

</details>


### [16] [AgentLAB: Benchmarking LLM Agents against Long-Horizon Attacks](https://arxiv.org/abs/2602.16901)
*Tanqiu Jiang,Yuhui Wang,Jiacheng Liang,Ting Wang*

Main category: cs.AI

TL;DR: AgentLAB是首个专门评估LLM智能体对自适应、长视野攻击脆弱性的基准测试，包含5种新型攻击类型、28个真实环境和644个安全测试用例，发现现有智能体对长视野攻击高度脆弱且单轮防御措施无效。


<details>
  <summary>Details</summary>
Motivation: 随着LLM智能体在长视野复杂环境中的部署增加，它们面临通过多轮用户-智能体-环境交互实现单轮设置中不可行目标的攻击风险，需要专门的基准来测量这种脆弱性。

Method: 提出AgentLAB基准测试框架，支持5种新型攻击类型：意图劫持、工具链式攻击、任务注入、目标漂移和内存污染，覆盖28个真实智能体环境和644个安全测试用例，用于评估代表性LLM智能体。

Result: 评估发现代表性LLM智能体对长视野攻击仍然高度脆弱，为单轮交互设计的防御措施无法可靠缓解长视野威胁，突显了现有安全防护的局限性。

Conclusion: AgentLAB作为首个专门评估LLM智能体长视野攻击脆弱性的基准测试，为跟踪实际环境中智能体安全进展提供了有价值的工具，有助于推动更有效的安全防护措施发展。

Abstract: LLM agents are increasingly deployed in long-horizon, complex environments to solve challenging problems, but this expansion exposes them to long-horizon attacks that exploit multi-turn user-agent-environment interactions to achieve objectives infeasible in single-turn settings. To measure agent vulnerabilities to such risks, we present AgentLAB, the first benchmark dedicated to evaluating LLM agent susceptibility to adaptive, long-horizon attacks. Currently, AgentLAB supports five novel attack types including intent hijacking, tool chaining, task injection, objective drifting, and memory poisoning, spanning 28 realistic agentic environments, and 644 security test cases. Leveraging AgentLAB, we evaluate representative LLM agents and find that they remain highly susceptible to long-horizon attacks; moreover, defenses designed for single-turn interactions fail to reliably mitigate long-horizon threats. We anticipate that AgentLAB will serve as a valuable benchmark for tracking progress on securing LLM agents in practical settings. The benchmark is publicly available at https://tanqiujiang.github.io/AgentLAB_main.

</details>


### [17] [LLM-WikiRace: Benchmarking Long-term Planning and Reasoning over Real-World Knowledge Graphs](https://arxiv.org/abs/2602.16902)
*Juliusz Ziomek,William Bankes,Lorenz Wolf,Shyam Sundhar Ramesh,Xiaohang Tang,Ilija Bogunovic*

Main category: cs.AI

TL;DR: LLM-Wikirace是一个评估大语言模型规划、推理和世界知识能力的基准测试，要求模型通过维基百科超链接从源页面逐步导航到目标页面。前沿模型在简单任务上表现优异甚至超越人类，但在困难任务上性能急剧下降，揭示了当前推理系统在长期规划和失败恢复方面的明显局限。


<details>
  <summary>Details</summary>
Motivation: 需要评估大语言模型在规划、推理和世界知识方面的综合能力。现有的基准测试可能无法充分测试模型在长期规划、前瞻性思考以及失败恢复等方面的能力，因此需要创建一个能够揭示这些关键局限性的新基准。

Method: 创建LLM-Wikirace基准测试，要求模型通过维基百科超链接逐步导航从源页面到目标页面。评估了包括Gemini-3、GPT-5和Claude Opus 4.5在内的广泛开源和闭源模型。任务分为简单和困难两个难度级别，并对模型轨迹进行了详细分析。

Result: 前沿模型在简单任务上表现出色甚至超越人类，但在困难任务上性能急剧下降：表现最佳的Gemini-3仅在23%的困难游戏中成功。分析表明世界知识是成功的必要条件，但超过一定阈值后，规划和长期推理能力成为主导因素。轨迹分析显示即使最强模型在失败后也难以重新规划，经常陷入循环而非恢复。

Conclusion: LLM-Wikirace是一个简单但有效的基准测试，揭示了当前推理系统在规划能力方面的明显局限性。虽然前沿模型在简单任务上表现出色，但在需要长期规划和失败恢复的困难任务上仍有很大改进空间，为规划能力强的LLMs提供了一个开放的竞技场。

Abstract: We introduce LLM-Wikirace, a benchmark for evaluating planning, reasoning, and world knowledge in large language models (LLMs). In LLM-Wikirace, models must efficiently navigate Wikipedia hyperlinks step by step to reach a target page from a given source, requiring look-ahead planning and the ability to reason about how concepts are connected in the real world. We evaluate a broad set of open- and closed-source models, including Gemini-3, GPT-5, and Claude Opus 4.5, which achieve the strongest results on the easy level of the task and demonstrate superhuman performance. Despite this, performance drops sharply on hard difficulty: the best-performing model, Gemini-3, succeeds in only 23\% of hard games, highlighting substantial remaining challenges for frontier models. Our analysis shows that world knowledge is a necessary ingredient for success, but only up to a point, beyond this threshold, planning and long-horizon reasoning capabilities become the dominant factors. Trajectory-level analysis further reveals that even the strongest models struggle to replan after failure, frequently entering loops rather than recovering. LLM-Wikirace is a simple benchmark that reveals clear limitations in current reasoning systems, offering an open arena where planning-capable LLMs still have much to prove. Our code and leaderboard available at https:/llmwikirace.github.io.

</details>


### [18] [Narrow fine-tuning erodes safety alignment in vision-language agents](https://arxiv.org/abs/2602.16931)
*Idhant Gulati,Shivam Raval*

Main category: cs.AI

TL;DR: 研究发现，在视觉语言模型上进行窄域有害数据的微调会导致严重的泛化性错位，即使只有10%的有害数据也会显著降低模型的对齐性，且多模态评估比纯文本评估更能揭示错位程度。


<details>
  <summary>Details</summary>
Motivation: 终身多模态智能体需要通过后训练持续适应新任务，但这在获取能力和保持安全对齐之间产生了根本性冲突。研究旨在探索对齐的视觉语言模型在窄域有害数据集上微调时出现的严重错位问题。

Method: 使用Gemma3-4B模型进行实验，通过LoRA微调技术在不同秩下测试错位程度，比较多模态与纯文本评估的差异，分析有害数据比例的影响，并进行几何分析以理解有害行为的低维子空间特性。评估了两种缓解策略：良性窄域微调和基于激活的引导。

Result: 错位程度随LoRA秩单调增加；多模态评估显示错位程度（70.71±1.22，r=128）显著高于纯文本评估（41.19±2.51）；即使训练数据中只有10%的有害数据也会导致实质性对齐退化；几何分析显示有害行为占据极低维子空间（10个主成分即可捕获大部分错位信息）。两种缓解策略虽能显著减少错位，但都无法完全消除已学习的有害行为。

Conclusion: 当前的后训练范式在部署后环境中可能无法充分保持模型的对齐性，需要开发更强大的持续学习框架来应对终身多模态智能体的安全挑战。

Abstract: Lifelong multimodal agents must continuously adapt to new tasks through post-training, but this creates fundamental tension between acquiring capabilities and preserving safety alignment. We demonstrate that fine-tuning aligned vision-language models on narrow-domain harmful datasets induces severe emergent misalignment that generalizes broadly across unrelated tasks and modalities. Through experiments on Gemma3-4B, we show that misalignment scales monotonically with LoRA rank, and that multimodal evaluation reveals substantially higher misalignment ($70.71 \pm 1.22$ at $r=128$) than text-only evaluation ($41.19 \pm 2.51$), suggesting that unimodal safety benchmarks may underestimate alignment degradation in vision-language models. Critically, even 10\% harmful data in the training mixture induces substantial alignment degradation. Geometric analysis reveals that harmful behaviors occupy a remarkably low-dimensional subspace, with the majority of misalignment information captured in 10 principal components. To mitigate misalignment, we evaluate two strategies: benign narrow fine-tuning and activation-based steering. While both approaches substantially reduce misalignment, neither completely removes the learned harmful behaviors. Our findings highlight the need for robust continual learning frameworks, as current post-training paradigms may not sufficiently preserve alignment in post-deployment settings.

</details>


### [19] [Fundamental Limits of Black-Box Safety Evaluation: Information-Theoretic and Computational Barriers from Latent Context Conditioning](https://arxiv.org/abs/2602.16984)
*Vishal Srivastava*

Main category: cs.AI

TL;DR: 该论文挑战了AI系统黑盒安全评估的基本假设，证明对于依赖未观测内部变量的模型，黑盒评估无法可靠估计部署风险，并建立了统计和计算上的根本限制。


<details>
  <summary>Details</summary>
Motivation: 挑战AI系统黑盒安全评估的核心假设——测试分布上的模型行为能可靠预测部署性能。作者发现某些模型的输出依赖于评估时罕见但部署时普遍存在的未观测内部变量，这种"潜在上下文条件策略"使得传统黑盒评估失效。

Method: 采用多种理论方法：1) 被动评估使用Le Cam方法证明极小极大下界；2) 自适应评估使用基于哈希的触发构造和Yao极小极大原理；3) 计算分离基于陷门单向函数假设；4) 白盒探测提供样本复杂度分析和偏差校正。

Result: 证明了黑盒评估的根本限制：被动评估的最小期望绝对误差≥0.208δL；自适应评估的最坏情况误差≥δL/16；计算上，拥有特权信息的部署环境可激活不安全行为，而多项式时间评估者无法区分；白盒探测需要O(1/(γ²ε_R²))样本。

Conclusion: 黑盒测试在统计上是欠定的，论文量化了其局限性，并为何时需要额外安全措施（架构约束、训练时保证、可解释性、部署监控）提供了明确的数学标准，这些措施对于最坏情况安全保证是必要的。

Abstract: Black-box safety evaluation of AI systems assumes model behavior on test distributions reliably predicts deployment performance. We formalize and challenge this assumption through latent context-conditioned policies -- models whose outputs depend on unobserved internal variables that are rare under evaluation but prevalent under deployment. We establish fundamental limits showing that no black-box evaluator can reliably estimate deployment risk for such models. (1) Passive evaluation: For evaluators sampling i.i.d. from D_eval, we prove minimax lower bounds via Le Cam's method: any estimator incurs expected absolute error >= (5/24)*delta*L approximately 0.208*delta*L, where delta is trigger probability under deployment and L is the loss gap. (2) Adaptive evaluation: Using a hash-based trigger construction and Yao's minimax principle, worst-case error remains >= delta*L/16 even for fully adaptive querying when D_dep is supported over a sufficiently large domain; detection requires Theta(1/epsilon) queries. (3) Computational separation: Under trapdoor one-way function assumptions, deployment environments possessing privileged information can activate unsafe behaviors that any polynomial-time evaluator without the trapdoor cannot distinguish. For white-box probing, estimating deployment risk to accuracy epsilon_R requires O(1/(gamma^2 * epsilon_R^2)) samples, where gamma = alpha_0 + alpha_1 - 1 measures probe quality, and we provide explicit bias correction under probe error. Our results quantify when black-box testing is statistically underdetermined and provide explicit criteria for when additional safeguards -- architectural constraints, training-time guarantees, interpretability, and deployment monitoring -- are mathematically necessary for worst-case safety assurance.

</details>


### [20] [Sonar-TS: Search-Then-Verify Natural Language Querying for Time Series Databases](https://arxiv.org/abs/2602.17001)
*Zhao Tan,Yiji Zhao,Shiyu Wang,Chang Xu,Yuxuan Liang,Xiping Liu,Shirui Pan,Ming Jin*

Main category: cs.AI

TL;DR: Sonar-TS是一个神经符号框架，通过"搜索-验证"流程解决时间序列数据库的自然语言查询问题，使用SQL搜索候选窗口，然后用Python程序验证原始信号，并创建了首个大规模基准NLQTSBench。


<details>
  <summary>Details</summary>
Motivation: 现有Text-to-SQL方法无法处理连续形态意图（如形状或异常），而时间序列模型难以处理超长历史记录，需要新的解决方案来帮助非专业用户从海量时间序列数据中检索有意义的事件、区间和摘要。

Method: 提出Sonar-TS神经符号框架，采用"搜索-验证"流程：1）使用特征索引通过SQL查询"ping"候选窗口；2）生成Python程序"锁定"并验证候选窗口与原始信号的匹配度，类似于主动声纳的工作原理。

Result: 创建了首个大规模基准NLQTSBench用于评估时间序列数据库的自然语言查询，实验表明Sonar-TS能够有效处理传统方法失败的复杂时间查询，展示了该领域的独特挑战。

Conclusion: 这是对时间序列数据库自然语言查询的首次系统性研究，提供了一个通用框架和评估标准，为未来研究奠定了基础，Sonar-TS能够有效解决传统方法无法处理的复杂时间查询问题。

Abstract: Natural Language Querying for Time Series Databases (NLQ4TSDB) aims to assist non-expert users retrieve meaningful events, intervals, and summaries from massive temporal records. However, existing Text-to-SQL methods are not designed for continuous morphological intents such as shapes or anomalies, while time series models struggle to handle ultra-long histories. To address these challenges, we propose Sonar-TS, a neuro-symbolic framework that tackles NLQ4TSDB via a Search-Then-Verify pipeline. Analogous to active sonar, it utilizes a feature index to ping candidate windows via SQL, followed by generated Python programs to lock on and verify candidates against raw signals. To enable effective evaluation, we introduce NLQTSBench, the first large-scale benchmark designed for NLQ over TSDB-scale histories. Our experiments highlight the unique challenges within this domain and demonstrate that Sonar-TS effectively navigates complex temporal queries where traditional methods fail. This work presents the first systematic study of NLQ4TSDB, offering a general framework and evaluation standard to facilitate future research.

</details>


### [21] [Cinder: A fast and fair matchmaking system](https://arxiv.org/abs/2602.17015)
*Saurav Pal*

Main category: cs.AI

TL;DR: Cinder是一个两阶段匹配系统，通过快速初步筛选和精确公平度评估，解决异质技能水平团队间的公平匹配问题。


<details>
  <summary>Details</summary>
Motivation: 现代多人游戏中，基于平均团队技能的传统匹配方法在处理技能分布广泛或偏斜的预组团队时，往往导致不平衡和一边倒的比赛，影响玩家留存和满意度。

Method: Cinder采用两阶段方法：第一阶段使用Ruzicka相似性指数快速比较团队的"非异常值"技能范围进行初步筛选；第二阶段将玩家等级映射到基于倒置正态分布生成的技能桶中，使用Kantorovich距离计算团队排序桶索引之间的"制裁分数"来量化匹配公平性。

Result: 通过分析1.4亿个模拟团队配对的制裁分数分布，验证了系统的可行性，为公平匹配阈值提供了稳健基础。

Conclusion: Cinder系统能够为异质技能水平的预组团队提供快速且公平的匹配，解决了传统平均技能匹配方法的局限性。

Abstract: A fair and fast matchmaking system is an important component of modern multiplayer online games, directly impacting player retention and satisfaction. However, creating fair matches between lobbies (pre-made teams) of heterogeneous skill levels presents a significant challenge. Matching based simply on average team skill metrics, such as mean or median rating or rank, often results in unbalanced and one-sided games, particularly when skill distributions are wide or skewed. This paper introduces Cinder, a two-stage matchmaking system designed to provide fast and fair matches. Cinder first employs a rapid preliminary filter by comparing the "non-outlier" skill range of lobbies using the Ruzicka similarity index. Lobbies that pass this initial check are then evaluated using a more precise fairness metric. This second stage involves mapping player ranks to a non-linear set of skill buckets, generated from an inverted normal distribution, to provide higher granularity at average skill levels. The fairness of a potential match is then quantified using the Kantorovich distance on the lobbies' sorted bucket indices, producing a "Sanction Score." We demonstrate the system's viability by analyzing the distribution of Sanction Scores from 140 million simulated lobby pairings, providing a robust foundation for fair matchmaking thresholds.

</details>


### [22] [IntentCUA: Learning Intent-level Representations for Skill Abstraction and Multi-Agent Planning in Computer-Use Agents](https://arxiv.org/abs/2602.17049)
*Seoyoung Lee,Seobin Yoon,Seongbeen Lee,Yoojung Chun,Dayoung Park,Doyeon Kim,Joo Yong Sim*

Main category: cs.AI

TL;DR: IntentCUA是一个多智能体计算机使用框架，通过意图对齐的计划记忆来稳定长时程执行，在桌面自动化任务中实现了74.83%的成功率和0.91的步骤效率比。


<details>
  <summary>Details</summary>
Motivation: 现有方法（从基于RL的规划器到轨迹检索）在长时程、噪声感知、多窗口上下文和动态环境状态下，容易偏离用户意图并重复解决常规子问题，导致错误累积和效率低下。

Method: 提出IntentCUA多智能体框架，包含规划器、计划优化器和批评器，通过共享记忆将原始交互轨迹抽象为多视图意图表示和可重用技能。意图原型检索子组对齐的技能并将其注入部分计划中。

Result: 在端到端评估中，IntentCUA实现了74.83%的任务成功率和0.91的步骤效率比，优于基于RL和轨迹中心的基线方法。消融研究表明多视图意图抽象和共享计划记忆共同提高了执行稳定性。

Conclusion: 系统级意图抽象和基于记忆的协调是实现大型动态环境中可靠高效桌面自动化的关键，合作式多智能体循环在长时程任务中提供了最大的性能提升。

Abstract: Computer-use agents operate over long horizons under noisy perception, multi-window contexts, evolving environment states. Existing approaches, from RL-based planners to trajectory retrieval, often drift from user intent and repeatedly solve routine subproblems, leading to error accumulation and inefficiency. We present IntentCUA, a multi-agent computer-use framework designed to stabilize long-horizon execution through intent-aligned plan memory. A Planner, Plan-Optimizer, and Critic coordinate over shared memory that abstracts raw interaction traces into multi-view intent representations and reusable skills. At runtime, intent prototypes retrieve subgroup-aligned skills and inject them into partial plans, reducing redundant re-planning and mitigating error propagation across desktop applications. In end-to-end evaluations, IntentCUA achieved a 74.83% task success rate with a Step Efficiency Ratio of 0.91, outperforming RL-based and trajectory-centric baselines. Ablations show that multi-view intent abstraction and shared plan memory jointly improve execution stability, with the cooperative multi-agent loop providing the largest gains on long-horizon tasks. These results highlight that system-level intent abstraction and memory-grounded coordination are key to reliable and efficient desktop automation in large, dynamic environments.

</details>


### [23] [Toward Trustworthy Evaluation of Sustainability Rating Methodologies: A Human-AI Collaborative Framework for Benchmark Dataset Construction](https://arxiv.org/abs/2602.17106)
*Xiaoran Cai,Wang Yang,Xiyu Ren,Chekun Law,Rohit Sharma,Peng Qi*

Main category: cs.AI

TL;DR: 该论文提出一个通用的人机协作框架，通过STRIDE和SR-Delta两个互补组件，生成可信的基准数据集来评估和协调不同机构间的可持续性评级差异。


<details>
  <summary>Details</summary>
Motivation: 不同可持续性评级机构对同一公司的评分差异很大，这限制了评级的可比性、可信度和决策相关性，需要一种方法来协调这些评级结果。

Method: 提出一个通用的人机协作框架，包含两个部分：STRIDE（提供原则性标准和评分系统，指导使用大语言模型构建公司级基准数据集）和SR-Delta（差异分析程序框架，揭示潜在调整的见解）。

Result: 该框架能够实现可持续性评级方法的可扩展和可比评估，为协调不同机构的评级差异提供了系统化方法。

Conclusion: 呼吁AI社区采用AI驱动的方法来加强和推进可持续性评级方法，以支持和执行紧迫的可持续发展议程。

Abstract: Sustainability or ESG rating agencies use company disclosures and external data to produce scores or ratings that assess the environmental, social, and governance performance of a company. However, sustainability ratings across agencies for a single company vary widely, limiting their comparability, credibility, and relevance to decision-making. To harmonize the rating results, we propose adopting a universal human-AI collaboration framework to generate trustworthy benchmark datasets for evaluating sustainability rating methodologies. The framework comprises two complementary parts: STRIDE (Sustainability Trust Rating & Integrity Data Equation) provides principled criteria and a scoring system that guide the construction of firm-level benchmark datasets using large language models (LLMs), and SR-Delta, a discrepancy-analysis procedural framework that surfaces insights for potential adjustments. The framework enables scalable and comparable assessment of sustainability rating methodologies. We call on the broader AI community to adopt AI-powered approaches to strengthen and advance sustainability rating methodologies that support and enforce urgent sustainability agendas.

</details>


### [24] [Owen-based Semantics and Hierarchy-Aware Explanation (O-Shap)](https://arxiv.org/abs/2602.17107)
*Xiangyu Zhou,Chenhan Xiao,Yang Weng*

Main category: cs.AI

TL;DR: 论文提出O-Shap方法，通过满足T属性的分割策略改进Owen值在图像解释中的应用，提升特征归因的精度和语义一致性。


<details>
  <summary>Details</summary>
Motivation: 传统Shapley值方法在视觉任务中假设特征独立，但像素间存在空间和语义依赖。Owen值支持分组归因但效果依赖分组定义，现有分割方法（如轴对齐或SLIC）违反关键一致性属性。

Method: 提出新的分割方法，满足T属性以确保层次结构中语义对齐，支持计算剪枝，同时提高归因准确性和可解释性。

Result: 在图像和表格数据集上的实验表明，O-Shap在归因精度、语义一致性和运行效率方面优于基线SHAP变体，特别是在结构重要的情况下。

Conclusion: 通过满足T属性的分割策略改进Owen值应用，能够更好地处理特征依赖关系，提升可解释AI中特征归因的质量和效率。

Abstract: Shapley value-based methods have become foundational in explainable artificial intelligence (XAI), offering theoretically grounded feature attributions through cooperative game theory. However, in practice, particularly in vision tasks, the assumption of feature independence breaks down, as features (i.e., pixels) often exhibit strong spatial and semantic dependencies. To address this, modern SHAP implementations now include the Owen value, a hierarchical generalization of the Shapley value that supports group attributions. While the Owen value preserves the foundations of Shapley values, its effectiveness critically depends on how feature groups are defined. We show that commonly used segmentations (e.g., axis-aligned or SLIC) violate key consistency properties, and propose a new segmentation approach that satisfies the $T$-property to ensure semantic alignment across hierarchy levels. This hierarchy enables computational pruning while improving attribution accuracy and interpretability. Experiments on image and tabular datasets demonstrate that O-Shap outperforms baseline SHAP variants in attribution precision, semantic coherence, and runtime efficiency, especially when structure matters.

</details>


### [25] [Epistemology of Generative AI: The Geometry of Knowing](https://arxiv.org/abs/2602.17116)
*Ilya Levin*

Main category: cs.AI

TL;DR: 论文提出高维空间的索引认识论，将生成式AI视为学习流形上的导航者，提出导航知识作为符号推理和统计重组之外的第三种知识生产模式。


<details>
  <summary>Details</summary>
Motivation: 生成式AI对知识和知识生产的理解提出了前所未有的挑战。与以往技术变革不同，生成式AI通过机制运作，其认识特征仍然模糊，没有这种理解，就无法在原则基础上将其负责任地整合到科学、教育和制度生活中。

Method: 基于高维几何的四个结构特性（测度集中、近正交性、指数方向容量和流形正则性），结合皮尔士符号学和帕珀特建构主义，发展高维空间的索引认识论，将生成模型重新概念化为学习流形的导航者。

Result: 提出了导航知识作为第三种知识生产模式，区别于符号推理和统计重组，为理解生成式AI的认知特征提供了新的理论框架。

Conclusion: 需要范式转变来理解生成式AI的认知机制，高维空间的索引认识论为这种理解提供了基础，导航知识概念为负责任地整合生成式AI到知识生产系统中提供了原则性框架。

Abstract: Generative AI presents an unprecedented challenge to our understanding of knowledge and its production. Unlike previous technological transformations, where engineering understanding preceded or accompanied deployment, generative AI operates through mechanisms whose epistemic character remains obscure, and without such understanding, its responsible integration into science, education, and institutional life cannot proceed on a principled basis. This paper argues that the missing account must begin with a paradigmatic break that has not yet received adequate philosophical attention. In the Turing-Shannon-von Neumann tradition, information enters the machine as encoded binary vectors, and semantics remains external to the process. Neural network architectures rupture this regime: symbolic input is instantly projected into a high-dimensional space where coordinates correspond to semantic parameters, transforming binary code into a position in a geometric space of meanings. It is this space that constitutes the active epistemic condition shaping generative production. Drawing on four structural properties of high-dimensional geometry concentration of measure, near-orthogonality, exponential directional capacity, and manifold regularity the paper develops an Indexical Epistemology of High-Dimensional Spaces. Building on Peirce semiotics and Papert constructionism, it reconceptualizes generative models as navigators of learned manifolds and proposes navigational knowledge as a third mode of knowledge production, distinct from both symbolic reasoning and statistical recombination.

</details>


### [26] [Efficient Parallel Algorithm for Decomposing Hard CircuitSAT Instances](https://arxiv.org/abs/2602.17130)
*Victor Kondratiev,Irina Gribanova,Alexander Semenov*

Main category: cs.AI

TL;DR: 提出一种用于分解困难CircuitSAT实例的新型并行算法，通过专门约束将原始SAT实例划分为一系列弱化公式，参数化设计允许高效识别高质量分解。


<details>
  <summary>Details</summary>
Motivation: 解决困难CircuitSAT实例的分解问题，特别是针对逻辑等价性检查和密码哈希函数原像攻击等实际应用中的挑战性实例。

Method: 使用专门约束将原始SAT实例划分为弱化公式族，采用参数化并行算法，通过调整参数在并行计算的硬度估计指导下高效识别高质量分解。

Result: 在具有挑战性的CircuitSAT实例上展示了算法的实际有效性，包括布尔电路逻辑等价性检查和密码哈希函数原像攻击的编码实例。

Conclusion: 提出的并行分解算法能够有效处理困难CircuitSAT实例，为逻辑等价性检查和密码分析等应用提供了实用的解决方案。

Abstract: We propose a novel parallel algorithm for decomposing hard CircuitSAT instances. The technique employs specialized constraints to partition an original SAT instance into a family of weakened formulas. Our approach is implemented as a parameterized parallel algorithm, where adjusting the parameters allows efficient identification of high-quality decompositions, guided by hardness estimations computed in parallel. We demonstrate the algorithm's practical efficacy on challenging CircuitSAT instances, including those encoding Logical Equivalence Checking of Boolean circuits and preimage attacks on cryptographic hash functions.

</details>


### [27] [From Labor to Collaboration: A Methodological Experiment Using AI Agents to Augment Research Perspectives in Taiwan's Humanities and Social Sciences](https://arxiv.org/abs/2602.17221)
*Yi-Chih Huang*

Main category: cs.AI

TL;DR: 本研究提出了一种基于AI智能体的协作研究流程（Agentic Workflow），用于人文社科研究，并以台湾Claude.ai使用数据验证其可行性。


<details>
  <summary>Details</summary>
Motivation: 当前生成式AI研究主要关注软件工程和自然科学领域，人文社科领域缺乏系统的方法论探索。本研究旨在填补这一空白，为人文社科研究者提供可复制的AI协作框架。

Method: 设计并验证了一个七阶段模块化工作流程，基于任务模块化、人机分工和可验证性三大原则。使用台湾Claude.ai使用数据（N=7,729个对话）作为实证载体，展示该工作流程在二次数据分析中的应用。

Result: 提出了可复制的人文社科AI协作框架，识别了三种人机协作操作模式：直接执行、迭代优化和人类主导。揭示了人类在研究问题制定、理论解释、情境化推理和伦理反思方面的不可替代性。

Conclusion: 该研究为人文社科研究者提供了实用的AI协作方法论，强调了人类判断在关键研究环节的重要性。同时承认了单平台数据、横断面设计和AI可靠性风险等局限性。

Abstract: Generative AI is reshaping knowledge work, yet existing research focuses predominantly on software engineering and the natural sciences, with limited methodological exploration for the humanities and social sciences. Positioned as a "methodological experiment," this study proposes an AI Agent-based collaborative research workflow (Agentic Workflow) for humanities and social science research. Taiwan's Claude.ai usage data (N = 7,729 conversations, November 2025) from the Anthropic Economic Index (AEI) serves as the empirical vehicle for validating the feasibility of this methodology.
  This study operates on two levels: the primary level is the design and validation of a methodological framework - a seven-stage modular workflow grounded in three principles: task modularization, human-AI division of labor, and verifiability, with each stage delineating clear roles for human researchers (research judgment and ethical decisions) and AI Agents (information retrieval and text generation); the secondary level is the empirical analysis of AEI Taiwan data - serving as an operational demonstration of the workflow's application to secondary data research, showcasing both the process and output quality (see Appendix A).
  This study contributes by proposing a replicable AI collaboration framework for humanities and social science researchers, and identifying three operational modes of human-AI collaboration - direct execution, iterative refinement, and human-led - through reflexive documentation of the operational process. This taxonomy reveals the irreplaceability of human judgment in research question formulation, theoretical interpretation, contextualized reasoning, and ethical reflection. Limitations including single-platform data, cross-sectional design, and AI reliability risks are acknowledged.

</details>


### [28] [Mechanistic Interpretability of Cognitive Complexity in LLMs via Linear Probing using Bloom's Taxonomy](https://arxiv.org/abs/2602.17229)
*Bianca Raimondi,Maurizio Gabbrielli*

Main category: cs.AI

TL;DR: 该研究使用布卢姆分类法作为层次化框架，通过分析大型语言模型的高维激活向量，探究不同认知层次是否在模型的残差流中线性可分。结果显示线性分类器在所有布卢姆层次上达到约95%的平均准确率，表明认知层次在模型表示中以线性可访问的子空间编码。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的黑箱特性需要超越表面性能指标的新型评估框架。本研究旨在探究认知复杂性在模型内部神经表示中的编码方式，使用布卢姆分类法作为层次化认知难度的理论框架。

Method: 通过分析不同LLMs的高维激活向量，使用布卢姆分类法作为分层认知框架，研究不同认知层次（从基础回忆到抽象综合）是否在模型残差流中线性可分。采用线性分类器来探测认知层次在模型表示中的可分离性。

Result: 线性分类器在所有布卢姆认知层次上达到约95%的平均准确率，提供了强有力的证据表明认知层次在模型表示中以线性可访问的子空间编码。研究还发现模型在前向传播早期就解析了提示的认知难度，且表示在不同层中变得越来越可分。

Conclusion: 认知层次在大型语言模型的内部表示中以线性可访问的方式编码，模型在前向传播早期就能解析认知难度。这为理解LLMs的内部工作机制提供了新视角，并支持使用线性探测方法来研究模型的认知表示。

Abstract: The black-box nature of Large Language Models necessitates novel evaluation frameworks that transcend surface-level performance metrics. This study investigates the internal neural representations of cognitive complexity using Bloom's Taxonomy as a hierarchical lens. By analyzing high-dimensional activation vectors from different LLMs, we probe whether different cognitive levels, ranging from basic recall (Remember) to abstract synthesis (Create), are linearly separable within the model's residual streams. Our results demonstrate that linear classifiers achieve approximately 95% mean accuracy across all Bloom levels, providing strong evidence that cognitive level is encoded in a linearly accessible subspace of the model's representations. These findings provide evidence that the model resolves the cognitive difficulty of a prompt early in the forward pass, with representations becoming increasingly separable across layers.

</details>


### [29] [All Leaks Count, Some Count More: Interpretable Temporal Contamination Detection in LLM Backtesting](https://arxiv.org/abs/2602.17234)
*Zeyu Zhang,Ryan Chen,Bradly C. Stadie*

Main category: cs.AI

TL;DR: 提出Shapley-DCLR指标量化LLM预测中的时间知识泄露，开发TimeSPEC方法通过声明验证减少泄露，在三个预测任务上验证有效性。


<details>
  <summary>Details</summary>
Motivation: 评估LLM预测未来事件能力需要回溯测试，但LLM可能无意中泄露训练时编码的截止日期后知识，这会破坏回溯评估的有效性。

Method: 1) 提出声明级框架检测时间知识泄露：将模型推理分解为原子声明并按时间可验证性分类；2) 使用Shapley值衡量每个声明对预测的贡献，得到Shapley-DCLR指标；3) 开发TimeSPEC方法：在生成过程中交替进行声明验证和重新生成，主动过滤时间污染。

Result: 在350个实例（美国最高法院案件预测、NBA薪资估计、股票回报排名）上实验显示标准提示基线存在显著泄露。TimeSPEC在保持任务性能的同时降低了Shapley-DCLR，证明显式的声明级验证优于基于提示的时间约束。

Conclusion: 提出的Shapley-DCLR指标能够量化LLM预测中的时间知识泄露，TimeSPEC方法通过声明级验证有效减少泄露，为可靠的回溯测试提供了更好的解决方案。

Abstract: To evaluate whether LLMs can accurately predict future events, we need the ability to \textit{backtest} them on events that have already resolved. This requires models to reason only with information available at a specified past date. Yet LLMs may inadvertently leak post-cutoff knowledge encoded during training, undermining the validity of retrospective evaluation. We introduce a claim-level framework for detecting and quantifying this \emph{temporal knowledge leakage}. Our approach decomposes model rationales into atomic claims and categorizes them by temporal verifiability, then applies \textit{Shapley values} to measure each claim's contribution to the prediction. This yields the \textbf{Shapley}-weighted \textbf{D}ecision-\textbf{C}ritical \textbf{L}eakage \textbf{R}ate (\textbf{Shapley-DCLR}), an interpretable metric that captures what fraction of decision-driving reasoning derives from leaked information. Building on this framework, we propose \textbf{Time}-\textbf{S}upervised \textbf{P}rediction with \textbf{E}xtracted \textbf{C}laims (\textbf{TimeSPEC}), which interleaves generation with claim verification and regeneration to proactively filter temporal contamination -- producing predictions where every supporting claim can be traced to sources available before the cutoff date. Experiments on 350 instances spanning U.S. Supreme Court case prediction, NBA salary estimation, and stock return ranking reveal substantial leakage in standard prompting baselines. TimeSPEC reduces Shapley-DCLR while preserving task performance, demonstrating that explicit, interpretable claim-level verification outperforms prompt-based temporal constraints for reliable backtesting.

</details>


### [30] [Dataless Weight Disentanglement in Task Arithmetic via Kronecker-Factored Approximate Curvature](https://arxiv.org/abs/2602.17385)
*Angelo Porrello,Pietro Buzzega,Felix Dangel,Thomas Sommariva,Riccardo Salami,Lorenzo Bonicelli,Simone Calderara*

Main category: cs.AI

TL;DR: 提出一种无数据方法，通过将表示漂移正则化框架化为曲率矩阵逼近问题，使用Kronecker分解近似曲率，实现任务向量的模块化组合而不需要外部任务数据。


<details>
  <summary>Details</summary>
Motivation: 任务算术提供模块化、可扩展的方式调整基础模型，但多个任务向量组合会导致跨任务干扰、表示漂移和性能下降。现有表示漂移正则化方法通常需要外部任务数据，这与模块化和数据可用性约束（如隐私要求）相冲突。

Method: 将表示漂移正则化框架化为曲率矩阵逼近问题，采用Kronecker分解近似曲率技术，获得实用的正则化器。该方法具有任务数量上的恒定复杂度，促进对任务向量重新缩放的鲁棒性。

Result: 在任务添加和否定方面达到最先进的结果，无需保留调优，具有任务数量上的恒定复杂度，对任务向量重新缩放具有鲁棒性。

Conclusion: 提出的无数据方法通过曲率矩阵逼近有效解决了任务向量组合中的表示漂移问题，在保持模块化和数据隐私约束的同时实现了高性能的任务算术操作。

Abstract: Task Arithmetic yields a modular, scalable way to adapt foundation models. Combining multiple task vectors, however, can lead to cross-task interference, causing representation drift and degraded performance. Representation drift regularization provides a natural remedy to disentangle task vectors; however, existing approaches typically require external task data, conflicting with modularity and data availability constraints (e.g., privacy requirements). We propose a dataless approach by framing regularization against representation drift as a curvature matrix approximation problem. This allows us to leverage well-established techniques; in particular, we adopt Kronecker-Factored Approximate Curvature and obtain a practical regularizer that achieves state-of-the-art results in task addition and negation. Our method has constant complexity in the number of tasks and promotes robustness to task vector rescaling, eliminating the need for held-out tuning.

</details>


### [31] [Visual Model Checking: Graph-Based Inference of Visual Routines for Image Retrieval](https://arxiv.org/abs/2602.17386)
*Adrià Molina,Oriol Ramos Terrades,Josep Lladós*

Main category: cs.AI

TL;DR: 提出一种将形式化验证与深度学习图像检索相结合的新框架，通过图验证和神经代码生成解决复杂查询的可靠性问题


<details>
  <summary>Details</summary>
Motivation: 当前基于嵌入模型的自然语言搜索在处理复杂关系、对象组合或精确约束（如身份、数量、比例）时仍存在不可靠问题，需要更可信和可验证的检索结果

Method: 通过图验证方法和神经代码生成的协同组合，将形式化验证集成到基于深度学习的图像检索中，对用户查询中的每个原子事实进行显式验证

Result: 框架不仅返回匹配结果，还能识别和标记哪些具体约束被满足或未满足，提供更透明和可问责的检索过程，同时提升了最流行的基于嵌入方法的检索效果

Conclusion: 通过将检索结果建立在形式化推理系统基础上，超越了向量表示的模糊性和近似性，为开放词汇自然语言查询提供了可信且可验证的检索解决方案

Abstract: Information retrieval lies at the foundation of the modern digital industry. While natural language search has seen dramatic progress in recent years largely driven by embedding-based models and large-scale pretraining, the field still faces significant challenges. Specifically, queries that involve complex relationships, object compositions, or precise constraints such as identities, counts and proportions often remain unresolved or unreliable within current frameworks. In this paper, we propose a novel framework that integrates formal verification into deep learning-based image retrieval through a synergistic combination of graph-based verification methods and neural code generation. Our approach aims to support open-vocabulary natural language queries while producing results that are both trustworthy and verifiable. By grounding retrieval results in a system of formal reasoning, we move beyond the ambiguity and approximation that often characterize vector representations. Instead of accepting uncertainty as a given, our framework explicitly verifies each atomic truth in the user query against the retrieved content. This allows us to not only return matching results, but also to identify and mark which specific constraints are satisfied and which remain unmet, thereby offering a more transparent and accountable retrieval process while boosting the results of the most popular embedding-based approaches.

</details>


### [32] [A Contrastive Variational AutoEncoder for NSCLC Survival Prediction with Missing Modalities](https://arxiv.org/abs/2602.17402)
*Michele Zanitti,Vanja Miskovic,Francesco Trovò,Alessandra Laura Giulia Pedrocchi,Ming Shen,Yan Kyaw Tun,Arsela Prelaj,Sokol Kosta*

Main category: cs.AI

TL;DR: 提出MCVAE模型，通过模态特定变分编码器、融合瓶颈门控机制和多任务目标，解决NSCLC患者生存预测中多模态数据严重缺失的问题。


<details>
  <summary>Details</summary>
Motivation: NSCLC患者生存预测因个体预后特征差异而具有挑战性，多模态数据（全切片图像、转录组学、DNA甲基化）可提供互补信息，但临床数据常存在严重缺失，现有模型在严重缺失情况下缺乏鲁棒性。

Method: 提出多模态对比变分自编码器（MCVAE）：1）模态特定变分编码器捕捉各数据源不确定性；2）引入带学习门控机制的融合瓶颈归一化现有模态贡献；3）多任务目标结合生存损失和重建损失正则化患者表示；4）跨模态对比损失强制潜在空间对齐；5）训练时应用随机模态掩蔽提高对任意缺失模式的鲁棒性。

Result: 在TCGA-LUAD（n=475）和TCGA-LUSC（n=446）数据集上的广泛评估表明，该方法在预测疾病特异性生存（DSS）方面有效，且在严重缺失情况下比两种最先进模型更具鲁棒性。

Conclusion: MCVAE能有效处理多模态数据严重缺失问题，但通过测试所有模态子集发现，多模态集成并不总是对任务有益，这为多模态集成提供了新的见解。

Abstract: Predicting survival outcomes for non-small cell lung cancer (NSCLC) patients is challenging due to the different individual prognostic features. This task can benefit from the integration of whole-slide images, bulk transcriptomics, and DNA methylation, which offer complementary views of the patient's condition at diagnosis. However, real-world clinical datasets are often incomplete, with entire modalities missing for a significant fraction of patients. State-of-the-art models rely on available data to create patient-level representations or use generative models to infer missing modalities, but they lack robustness in cases of severe missingness. We propose a Multimodal Contrastive Variational AutoEncoder (MCVAE) to address this issue: modality-specific variational encoders capture the uncertainty in each data source, and a fusion bottleneck with learned gating mechanisms is introduced to normalize the contributions from present modalities. We propose a multi-task objective that combines survival loss and reconstruction loss to regularize patient representations, along with a cross-modal contrastive loss that enforces cross-modal alignment in the latent space. During training, we apply stochastic modality masking to improve the robustness to arbitrary missingness patterns. Extensive evaluations on the TCGA-LUAD (n=475) and TCGA-LUSC (n=446) datasets demonstrate the efficacy of our approach in predicting disease-specific survival (DSS) and its robustness to severe missingness scenarios compared to two state-of-the-art models. Finally, we bring some clarifications on multimodal integration by testing our model on all subsets of modalities, finding that integration is not always beneficial to the task.

</details>


### [33] [A Privacy by Design Framework for Large Language Model-Based Applications for Children](https://arxiv.org/abs/2602.17418)
*Diana Addae,Diana Rogachova,Nafiseh Kahani,Masoud Barati,Michael Christensen,Chen Zhou*

Main category: cs.AI

TL;DR: 本文提出一个基于隐私设计原则的框架，用于指导AI应用开发者为儿童设计隐私保护的LLM应用，结合GDPR、PIPEDA、COPPA等法规原则，涵盖数据收集、模型训练、运营监控等全生命周期，并通过教育辅导案例验证框架实用性。


<details>
  <summary>Details</summary>
Motivation: 儿童越来越多地使用AI技术，但存在隐私风险担忧。现有隐私法规要求企业实施保护措施，但在实践中面临挑战。需要为设计者和开发者提供系统框架，以主动、风险规避的方式设计儿童AI应用。

Method: 提出基于隐私设计原则的框架，整合GDPR、PIPEDA、COPPA等隐私法规原则，映射到LLM应用的数据收集、模型训练、运营监控、持续验证等阶段。结合UNCRC、AADC等儿童权利和适龄设计指南，为每个阶段提供操作控制措施。

Result: 框架展示了如何通过技术和组织控制以及适龄设计决策，在LLM全生命周期中减少隐私风险并满足法律要求。通过13岁以下儿童LLM教育辅导案例研究，验证了框架的实际应用可行性。

Conclusion: 通过整合隐私法规原则和儿童权利框架，采用隐私设计方法，可以在AI应用开发中为儿童提供隐私保护并满足法律合规要求，支持开发既安全又符合法规的儿童AI应用。

Abstract: Children are increasingly using technologies powered by Artificial Intelligence (AI). However, there are growing concerns about privacy risks, particularly for children. Although existing privacy regulations require companies and organizations to implement protections, doing so can be challenging in practice. To address this challenge, this article proposes a framework based on Privacy-by-Design (PbD), which guides designers and developers to take on a proactive and risk-averse approach to technology design. Our framework includes principles from several privacy regulations, such as the General Data Protection Regulation (GDPR) from the European Union, the Personal Information Protection and Electronic Documents Act (PIPEDA) from Canada, and the Children's Online Privacy Protection Act (COPPA) from the United States. We map these principles to various stages of applications that use Large Language Models (LLMs), including data collection, model training, operational monitoring, and ongoing validation. For each stage, we discuss the operational controls found in the recent academic literature to help AI service providers and developers reduce privacy risks while meeting legal standards. In addition, the framework includes design guidelines for children, drawing from the United Nations Convention on the Rights of the Child (UNCRC), the UK's Age-Appropriate Design Code (AADC), and recent academic research. To demonstrate how this framework can be applied in practice, we present a case study of an LLM-based educational tutor for children under 13. Through our analysis and the case study, we show that by using data protection strategies such as technical and organizational controls and making age-appropriate design decisions throughout the LLM life cycle, we can support the development of AI applications for children that provide privacy protections and comply with legal requirements.

</details>


### [34] [Enhancing Large Language Models (LLMs) for Telecom using Dynamic Knowledge Graphs and Explainable Retrieval-Augmented Generation](https://arxiv.org/abs/2602.17529)
*Dun Yuan,Hao Zhou,Xue Liu,Hao Chen,Yan Xin,Jianzhong,Zhang*

Main category: cs.AI

TL;DR: KG-RAG框架结合知识图谱和检索增强生成，提升大语言模型在电信领域的准确性和可靠性，减少幻觉现象


<details>
  <summary>Details</summary>
Motivation: 大语言模型在电信领域应用面临挑战，包括领域复杂性、标准演进和专用术语，导致通用大语言模型在该领域产生幻觉和准确性不足的问题

Method: 提出KG-RAG框架，将知识图谱（提供电信标准和文档的结构化知识表示）与检索增强生成（动态检索相关事实）相结合，以增强大语言模型在电信特定任务中的表现

Result: 在基准数据集上的实验表明，KG-RAG优于纯大语言模型和标准RAG基线，平均准确率分别提高21.6%和14.3%

Conclusion: KG-RAG框架能有效提升大语言模型在复杂电信场景中输出的准确性、可靠性和可解释性，减少幻觉并确保符合电信规范

Abstract: Large language models (LLMs) have shown strong potential across a variety of tasks, but their application in the telecom field remains challenging due to domain complexity, evolving standards, and specialized terminology. Therefore, general-domain LLMs may struggle to provide accurate and reliable outputs in this context, leading to increased hallucinations and reduced utility in telecom operations.To address these limitations, this work introduces KG-RAG-a novel framework that integrates knowledge graphs (KGs) with retrieval-augmented generation (RAG) to enhance LLMs for telecom-specific tasks. In particular, the KG provides a structured representation of domain knowledge derived from telecom standards and technical documents, while RAG enables dynamic retrieval of relevant facts to ground the model's outputs. Such a combination improves factual accuracy, reduces hallucination, and ensures compliance with telecom specifications.Experimental results across benchmark datasets demonstrate that KG-RAG outperforms both LLM-only and standard RAG baselines, e.g., KG-RAG achieves an average accuracy improvement of 14.3% over RAG and 21.6% over LLM-only models. These results highlight KG-RAG's effectiveness in producing accurate, reliable, and explainable outputs in complex telecom scenarios.

</details>


### [35] [Evaluating Chain-of-Thought Reasoning through Reusability and Verifiability](https://arxiv.org/abs/2602.17544)
*Shashank Aggarwal,Ram Vikas Mishra,Amit Awekar*

Main category: cs.AI

TL;DR: 论文提出两个新指标（可重用性和可验证性）来评估多智能体IR管道中思维链的质量，发现这些指标与标准准确率不相关，揭示了当前基于准确率的排行榜在评估推理能力方面的盲点。


<details>
  <summary>Details</summary>
Motivation: 当前对思维链（CoT）的评估过于狭窄，仅关注目标任务准确率，无法评估推理过程本身的质量或效用。需要新的评估指标来衡量思维链的实际价值。

Method: 采用Thinker-Executor框架将CoT生成与执行解耦，引入可重用性（Executor重用Thinker的CoT的容易程度）和可验证性（Executor使用CoT匹配Thinker答案的频率）两个新指标。评估了4个Thinker模型与10个Executor模型委员会在5个基准测试上的表现。

Result: 可重用性和可验证性与标准准确率不相关，揭示了当前基于准确率的排行榜在评估推理能力方面的盲点。令人惊讶的是，专门推理模型生成的CoT并不比Llama和Gemma等通用LLM生成的CoT更可重用或可验证。

Conclusion: 需要超越准确率的新评估指标来全面评估思维链的质量。可重用性和可验证性提供了评估推理过程本身价值的重要视角，揭示了当前评估方法的局限性。

Abstract: In multi-agent IR pipelines for tasks such as search and ranking, LLM-based agents exchange intermediate reasoning in terms of Chain-of-Thought (CoT) with each other. Current CoT evaluation narrowly focuses on target task accuracy. However, this metric fails to assess the quality or utility of the reasoning process itself. To address this limitation, we introduce two novel measures: reusability and verifiability. We decouple CoT generation from execution using a Thinker-Executor framework. Reusability measures how easily an Executor can reuse the Thinker's CoT. Verifiability measures how frequently an Executor can match the Thinker's answer using the CoT. We evaluated four Thinker models against a committee of ten Executor models across five benchmarks. Our results reveal that reusability and verifiability do not correlate with standard accuracy, exposing a blind spot in current accuracy-based leaderboards for reasoning capability. Surprisingly, we find that CoTs from specialized reasoning models are not consistently more reusable or verifiable than those from general-purpose LLMs like Llama and Gemma.

</details>


### [36] [KLong: Training LLM Agent for Extremely Long-horizon Tasks](https://arxiv.org/abs/2602.17547)
*Yue Liu,Zhiyuan Hu,Flood Sung,Jiaheng Zhang,Bryan Hooi*

Main category: cs.AI

TL;DR: KLong是一个开源LLM智能体，通过轨迹分割SFT和渐进式RL训练来解决极长视野任务，在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体在处理极长视野任务时存在困难，需要开发能够有效解决这类复杂、多步骤任务的智能体系统。

Method: 1. 通过轨迹分割SFT冷启动模型：保留早期上下文，渐进截断后期上下文，保持子轨迹重叠；2. 使用Research-Factory自动生成高质量训练数据；3. 提出渐进式RL训练：分阶段训练，逐步延长超时时间。

Result: KLong（106B）在PaperBench上超越Kimi K2 Thinking（1T）11.28%，性能提升泛化到SWE-bench Verified和MLE-bench等其他编码基准测试。

Conclusion: KLong通过轨迹分割SFT和渐进式RL训练有效解决了极长视野任务，展示了优越的性能和泛化能力。

Abstract: This paper introduces KLong, an open-source LLM agent trained to solve extremely long-horizon tasks. The principle is to first cold-start the model via trajectory-splitting SFT, then scale it via progressive RL training. Specifically, we first activate basic agentic abilities of a base model with a comprehensive SFT recipe. Then, we introduce Research-Factory, an automated pipeline that generates high-quality training data by collecting research papers and constructing evaluation rubrics. Using this pipeline, we build thousands of long-horizon trajectories distilled from Claude 4.5 Sonnet (Thinking). To train with these extremely long trajectories, we propose a new trajectory-splitting SFT, which preserves early context, progressively truncates later context, and maintains overlap between sub-trajectories. In addition, to further improve long-horizon task-solving capability, we propose a novel progressive RL, which schedules training into multiple stages with progressively extended timeouts. Experiments demonstrate the superiority and generalization of KLong, as shown in Figure 1. Notably, our proposed KLong (106B) surpasses Kimi K2 Thinking (1T) by 11.28% on PaperBench, and the performance improvement generalizes to other coding benchmarks like SWE-bench Verified and MLE-bench.

</details>


### [37] [ODESteer: A Unified ODE-Based Steering Framework for LLM Alignment](https://arxiv.org/abs/2602.17560)
*Hongjue Zhao,Haosen Sun,Jiangtao Kong,Xiaochang Li,Qineng Wang,Liwei Jiang,Qi Zhu,Tarek Abdelzaher,Yejin Choi,Manling Li,Huajie Shao*

Main category: cs.AI

TL;DR: 本文提出了一种基于常微分方程的统一理论框架ODESteer，用于改进大语言模型中的激活导向技术，通过屏障函数和多步自适应导向实现更好的对齐效果。


<details>
  <summary>Details</summary>
Motivation: 当前激活导向方法存在两个关键限制：缺乏统一的理论框架来指导导向方向设计，以及过度依赖单步导向而无法捕捉激活分布的复杂模式。

Method: 提出基于常微分方程的ODESteer方法，将传统激活加法解释为ODE的一阶近似，通过定义正负激活的对数密度比作为屏障函数，构建多步自适应导向的ODE。

Result: ODESteer在多个LLM对齐基准测试中取得一致改进：TruthfulQA提升5.7%，UltraFeedback提升2.5%，RealToxicityPrompts提升2.4%。

Conclusion: 本文通过ODE统一了激活导向的理论基础，并通过ODESteer方法进行了实证验证，为大语言模型对齐提供了新的原则性视角。

Abstract: Activation steering, or representation engineering, offers a lightweight approach to align large language models (LLMs) by manipulating their internal activations at inference time. However, current methods suffer from two key limitations: \textit{(i)} the lack of a unified theoretical framework for guiding the design of steering directions, and \textit{(ii)} an over-reliance on \textit{one-step steering} that fail to capture complex patterns of activation distributions. In this work, we propose a unified ordinary differential equations (ODEs)-based \textit{theoretical} framework for activation steering in LLM alignment. We show that conventional activation addition can be interpreted as a first-order approximation to the solution of an ODE. Based on this ODE perspective, identifying a steering direction becomes equivalent to designing a \textit{barrier function} from control theory. Derived from this framework, we introduce ODESteer, a kind of ODE-based steering guided by barrier functions, which shows \textit{empirical} advancement in LLM alignment. ODESteer identifies steering directions by defining the barrier function as the log-density ratio between positive and negative activations, and employs it to construct an ODE for \textit{multi-step and adaptive} steering. Compared to state-of-the-art activation steering methods, ODESteer achieves consistent empirical improvements on diverse LLM alignment benchmarks, a notable $5.7\%$ improvement over TruthfulQA, $2.5\%$ over UltraFeedback, and $2.4\%$ over RealToxicityPrompts. Our work establishes a principled new view of activation steering in LLM alignment by unifying its theoretical foundations via ODEs, and validating it empirically through the proposed ODESteer method.

</details>


### [38] [A Hybrid Federated Learning Based Ensemble Approach for Lung Disease Diagnosis Leveraging Fusion of SWIN Transformer and CNN](https://arxiv.org/abs/2602.17566)
*Asif Hasan Chowdhury,Md. Fahim Islam,M Ragib Anjum Riad,Faiyaz Bin Hashem,Md Tanzim Reza,Md. Golam Rabiul Alam*

Main category: cs.AI

TL;DR: 提出一种基于联邦学习的混合AI模型，结合SWIN Transformer和CNN，用于通过X光片诊断COVID-19和肺炎，确保医疗数据安全的同时提高诊断准确性。


<details>
  <summary>Details</summary>
Motivation: 医疗领域需要安全、高效的AI诊断系统，但医疗数据分散且敏感。联邦学习可以在保护数据隐私的前提下，利用各医院的数据训练模型，提高疾病诊断的准确性和可靠性。

Method: 采用联邦学习框架，构建混合模型：结合SWIN Transformer和多种CNN模型（DenseNet201、Inception V3、VGG 19）。使用TensorFlow和Keras实现，通过X光片数据进行训练和诊断。

Result: 该混合模型能够有效检测COVID-19和肺炎，通过联邦学习确保数据安全，同时利用实时持续学习提高疾病诊断和严重程度预测的准确性。

Conclusion: 联邦学习与混合AI模型的结合为医疗诊断提供了安全、可靠的解决方案，既能保护患者隐私，又能提高疾病诊断的准确性，有助于应对全球疫情挑战。

Abstract: The significant advancements in computational power cre- ate a vast opportunity for using Artificial Intelligence in different ap- plications of healthcare and medical science. A Hybrid FL-Enabled Ensemble Approach For Lung Disease Diagnosis Leveraging a Combination of SWIN Transformer and CNN is the combination of cutting-edge technology of AI and Federated Learning. Since, medi- cal specialists and hospitals will have shared data space, based on that data, with the help of Artificial Intelligence and integration of federated learning, we can introduce a secure and distributed system for medical data processing and create an efficient and reliable system. The proposed hybrid model enables the detection of COVID-19 and Pneumonia based on x-ray reports. We will use advanced and the latest available tech- nology offered by Tensorflow and Keras along with Microsoft-developed Vision Transformer, that can help to fight against the pandemic that the world has to fight together as a united. We focused on using the latest available CNN models (DenseNet201, Inception V3, VGG 19) and the Transformer model SWIN Transformer in order to prepare our hy- brid model that can provide a reliable solution as a helping hand for the physician in the medical field. In this research, we will discuss how the Federated learning-based Hybrid AI model can improve the accuracy of disease diagnosis and severity prediction of a patient using the real-time continual learning approach and how the integration of federated learn- ing can ensure hybrid model security and keep the authenticity of the information.

</details>


### [39] [AI Gamestore: Scalable, Open-Ended Evaluation of Machine General Intelligence with Human Games](https://arxiv.org/abs/2602.17594)
*Lance Ying,Ryan Truong,Prafull Sharma,Kaiya Ivy Zhao,Nathan Cloos,Kelsey R. Allen,Thomas L. Griffiths,Katherine M. Collins,José Hernández-Orallo,Phillip Isola,Samuel J. Gershman,Joshua B. Tenenbaum*

Main category: cs.AI

TL;DR: 研究者提出使用"人类游戏宇宙"作为评估AI通用智能的新方法，并开发了AI GameStore平台，通过LLM生成代表性人类游戏来测试AI系统。在100个游戏测试中，前沿视觉语言模型的表现远低于人类平均水平。


<details>
  <summary>Details</summary>
Motivation: 传统AI基准测试通常只评估狭窄能力，且容易饱和。需要一种更全面的方法来评估AI系统是否具备类似人类的通用智能，特别是在快速技术发展的背景下。

Method: 提出使用"人类游戏宇宙"（所有人类为人类设计的游戏）作为评估框架。开发了AI GameStore平台，利用LLM和人类参与循环，从Apple App Store和Steam等平台自动获取并适配游戏环境，生成标准化、容器化的游戏变体。

Result: 生成了100个代表性人类游戏，测试了7个前沿视觉语言模型。最佳模型在大多数游戏中得分不到人类平均水平的10%，特别是在需要世界模型学习、记忆和规划能力的游戏中表现更差。

Conclusion: AI GameStore为评估和推动AI向人类通用智能发展提供了实用方法。当前AI在游戏表现上仍远落后于人类，尤其是在复杂认知能力方面，需要进一步改进平台和方法。

Abstract: Rigorously evaluating machine intelligence against the broad spectrum of human general intelligence has become increasingly important and challenging in this era of rapid technological advance. Conventional AI benchmarks typically assess only narrow capabilities in a limited range of human activity. Most are also static, quickly saturating as developers explicitly or implicitly optimize for them. We propose that a more promising way to evaluate human-like general intelligence in AI systems is through a particularly strong form of general game playing: studying how and how well they play and learn to play \textbf{all conceivable human games}, in comparison to human players with the same level of experience, time, or other resources. We define a "human game" to be a game designed by humans for humans, and argue for the evaluative suitability of this space of all such games people can imagine and enjoy -- the "Multiverse of Human Games". Taking a first step towards this vision, we introduce the AI GameStore, a scalable and open-ended platform that uses LLMs with humans-in-the-loop to synthesize new representative human games, by automatically sourcing and adapting standardized and containerized variants of game environments from popular human digital gaming platforms. As a proof of concept, we generated 100 such games based on the top charts of Apple App Store and Steam, and evaluated seven frontier vision-language models (VLMs) on short episodes of play. The best models achieved less than 10\% of the human average score on the majority of the games, and especially struggled with games that challenge world-model learning, memory and planning. We conclude with a set of next steps for building out the AI GameStore as a practical way to measure and drive progress toward human-like general intelligence in machines.

</details>


### [40] [MolHIT: Advancing Molecular-Graph Generation with Hierarchical Discrete Diffusion Models](https://arxiv.org/abs/2602.17602)
*Hojung Jung,Rodrigo Hormazabal,Jaehyeong Jo,Youngrok Park,Kyunggeun Roh,Se-Young Yun,Sehui Han,Dae-Woong Jeong*

Main category: cs.AI

TL;DR: MolHIT是一个基于分层离散扩散模型的分子图生成框架，通过引入化学先验编码和解耦原子编码，在MOSES数据集上实现了接近完美的化学有效性，超越了现有图扩散和1D基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有图扩散模型在分子生成中存在化学有效性低、难以满足期望属性等问题，相比1D建模方法表现不佳，需要克服这些长期存在的性能限制。

Method: MolHIT基于分层离散扩散模型，将离散扩散推广到编码化学先验的额外类别，并采用解耦原子编码方法，根据化学角色分离原子类型。

Result: 在MOSES数据集上实现了新的最先进性能，首次在图扩散中达到接近完美的化学有效性，在多个指标上超越了强大的1D基线方法，并在多属性引导生成和骨架扩展等下游任务中表现出色。

Conclusion: MolHIT成功克服了现有图扩散模型的性能限制，为AI驱动的药物发现和材料科学提供了强大的分子生成框架。

Abstract: Molecular generation with diffusion models has emerged as a promising direction for AI-driven drug discovery and materials science. While graph diffusion models have been widely adopted due to the discrete nature of 2D molecular graphs, existing models suffer from low chemical validity and struggle to meet the desired properties compared to 1D modeling. In this work, we introduce MolHIT, a powerful molecular graph generation framework that overcomes long-standing performance limitations in existing methods. MolHIT is based on the Hierarchical Discrete Diffusion Model, which generalizes discrete diffusion to additional categories that encode chemical priors, and decoupled atom encoding that splits the atom types according to their chemical roles. Overall, MolHIT achieves new state-of-the-art performance on the MOSES dataset with near-perfect validity for the first time in graph diffusion, surpassing strong 1D baselines across multiple metrics. We further demonstrate strong performance in downstream tasks, including multi-property guided generation and scaffold extension.

</details>


### [41] [CLEF HIPE-2026: Evaluating Accurate and Efficient Person-Place Relation Extraction from Multilingual Historical Texts](https://arxiv.org/abs/2602.17663)
*Juri Opitz,Corina Raclé,Emanuela Boros,Andrianos Michail,Matteo Romanello,Maud Ehrmann,Simon Clematide*

Main category: cs.AI

TL;DR: HIPE-2026是CLEF评估实验室，专注于从多语言历史文本中提取人物-地点关系，扩展了之前的HIPE系列，增加了语义关系提取任务，要求系统对两种关系类型进行分类，并引入三重评估框架。


<details>
  <summary>Details</summary>
Motivation: 该研究的动机是解决从嘈杂、多语言历史文本中提取人物-地点关系的挑战，支持大规模历史数据处理，促进知识图谱构建、历史传记重建和数字人文空间分析等下游应用。

Method: 方法包括：1）基于HIPE-2020和HIPE-2022的扩展，专注于语义关系提取；2）要求系统对两种关系类型进行分类：$at$（人物是否曾到过该地点）和$isAt$（人物在出版时间是否位于该地点）；3）引入三重评估框架，联合评估准确性、计算效率和领域泛化能力。

Result: HIPE-2026建立了针对多语言、多时期历史文本的人物-地点关系提取评估框架，为系统开发提供了标准化测试环境，支持时间推理和地理线索分析。

Conclusion: HIPE-2026通过将关系提取与大规模历史数据处理相结合，为数字人文领域提供了重要工具，支持知识图谱构建、历史传记重建和空间分析等应用，推动了历史文本信息提取技术的发展。

Abstract: HIPE-2026 is a CLEF evaluation lab dedicated to person-place relation extraction from noisy, multilingual historical texts. Building on the HIPE-2020 and HIPE-2022 campaigns, it extends the series toward semantic relation extraction by targeting the task of identifying person--place associations in multiple languages and time periods. Systems are asked to classify relations of two types - $at$ ("Has the person ever been at this place?") and $isAt$ ("Is the person located at this place around publication time?") - requiring reasoning over temporal and geographical cues. The lab introduces a three-fold evaluation profile that jointly assesses accuracy, computational efficiency, and domain generalization. By linking relation extraction to large-scale historical data processing, HIPE-2026 aims to support downstream applications in knowledge-graph construction, historical biography reconstruction, and spatial analysis in digital humanities.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [42] [A Construction-Phase Digital Twin Framework for Quality Assurance and Decision Support in Civil Infrastructure Projects](https://arxiv.org/abs/2602.16748)
*Md Asiful Islam,Shanto Jouerder,Md Sabit As Sami,Afia Jahin Prema*

Main category: cs.SE

TL;DR: 该研究提出一个施工阶段数字孪生框架，将检查记录、材料数据、早期传感和强度预测模型整合到单个构件层面，支持基于准备度的质量保证决策，实现从延迟文档审查向主动决策支持的转变。


<details>
  <summary>Details</summary>
Motivation: 当前施工质量保证依赖于完工后数天或数周才能获得的检查记录和实验室测试结果，这种延迟限制了早期干预，增加了返工风险、进度影响和文档碎片化问题。

Method: 开发施工阶段数字孪生框架，将检查记录、材料生产和放置数据、早期传感技术、预测强度模型与单个施工构件关联，整合这些数据流以表示每个构件的质量状态演变。

Result: 该框架能够支持在标准龄期测试结果可用之前做出结构化的放行或保留决策，提高可追溯性并实现更早的数据驱动质量评估，而不取代既有的检查和测试程序。

Conclusion: 提出的框架为施工质量保证从延迟的文档驱动审查向主动的构件级决策支持过渡提供了结构化路径，同时讨论了数据集成、合同约束和实施挑战等实际考虑因素。

Abstract: Quality assurance (QA) during construction often relies on inspection records and laboratory test results that become available days or weeks after work is completed. On large highway and bridge projects, this delay limits early intervention and increases the risk of rework, schedule impacts, and fragmented documentation. This study presents a construction-phase digital twin framework designed to support element-level QA and readiness-based decision making during active construction. The framework links inspection records, material production and placement data, early-age sensing, and predictive strength models to individual construction elements. By integrating these data streams, the system represents the evolving quality state of each element and supports structured release or hold decisions before standard-age test results are available. The approach does not replace established inspection and testing procedures. Instead, it supplements existing workflows by improving traceability and enabling earlier, data-informed quality assessments. Practical considerations related to data integration, contractual constraints, and implementation challenges are also discussed. The proposed framework provides a structured pathway for transitioning construction QA from delayed, document-driven review toward proactive, element-level decision support during construction.

</details>


### [43] [Not Only for Developers: Exploring Plugin Maintenance for Knowledge-Centric Communities](https://arxiv.org/abs/2602.17018)
*Giovanni Rosa,David Moreno-Lumbreras,Raula Gaikovina Kula*

Main category: cs.SE

TL;DR: 研究Obsidian知识管理平台的插件生态系统，这是一个非开发者主导的混合社区，通过分析396个插件发现6个主题类别，并探讨这类非开发者生态系统的维护模式和可持续性。


<details>
  <summary>Details</summary>
Motivation: 现代软件开发中第三方库生态系统（如PyPI、NPM、Maven）通常由开发者维护，但在非开发者主导的混合社区中，插件生态系统的维护面临不同挑战。本研究旨在探索Obsidian这种以知识管理为核心、非开发者主导的平台如何建立和维护其插件生态系统。

Method: 采用仓库挖掘和基于LLM的主题建模方法，对396个代表性插件样本进行分析，识别插件主题类别。同时分析这些插件的Pull Requests来了解软件演化情况。

Result: 识别出6个主要插件主题：(1)动态编辑与组织，(2)界面与布局，(3)创意写作与生产力，(4)知识同步解决方案，(5)链接与脚本工具，(6)工作流增强工具。Pull Requests分析显示这些生态系统存在显著的软件演化活动。

Conclusion: 即使在混合社区中，插件生态系统也能发展出可识别的工程结构。研究为理解非开发者生态系统的健康与可持续性奠定了基础，提出了三个研究方向和相关研究问题。

Abstract: The adoption of third-party libraries has become integral to modern software development, leading to large ecosystems such as PyPI, NPM, and Maven, where contributors typically share the technical expertise to sustain extensions. In communities that are not exclusively composed of developers, however, maintaining plugin ecosystems can present different challenges. In this early results paper, we study Obsidian, a knowledge--centric platform whose community is focused on writing, organization, and creativity--has built a substantial plugin ecosystem despite not being developer--centric. We investigate what kinds of plugins exist within this hybrid ecosystem and establish a foundation for understanding how they are maintained. Using repository mining and LLM-based topic modeling on a representative sample of 396 plugins, we identify six topics related to knowledge management and tooling, which is (i) dynamic editing and organization, (ii) interface and layouts, (iii) creative writing and productivity, (iv) knowledge sync solutions, (v) linking and script tools, and (vi) workflow enhancements tools. Furthermore, analysis of the Pull Requests from these plugins show that much software evolution has been performed on these ecosystem. These findings suggest that even in mixed communities, plugin ecosystems can develop recognizable engineering structures, motivating future work that highlight three different research directions with six research questions related to the health and sustainability of these non-developer ecosystems.

</details>


### [44] [Multi-Ecosystem Modeling of OSS Project Sustainability](https://arxiv.org/abs/2602.17112)
*Arjun Ashok,Nafiz Imtiaz Khan,Swati Singhvi,Stefan Stanciulescu,Zhouhao Wang,Vladimir Filkov*

Main category: cs.SE

TL;DR: 该研究通过实证分析和定量建模，评估Apache、Eclipse、OSGeo基金会孵化项目及GitHub项目的可持续性，基于社会技术轨迹特征开发可持续性模型和项目分类方法，证明其跨基金会和GitHub的预测有效性。


<details>
  <summary>Details</summary>
Motivation: 开源项目加入基金会（如Apache、Eclipse、OSGeo）是为了获得治理建议、孵化支持和社区建设机制，但不同基金会的政策、资助模式和支持策略各异。项目在加入时处于不同生命周期阶段且有不同需求，选择合适的基金会匹配和制定项目特定的可持续性计划具有挑战性。

Method: 对Apache、Eclipse、OSGeo基金会孵化项目及GitHub非基金会项目进行实证研究和定量分析；基于项目的社会技术轨迹特征开发基金会特定的可持续性模型和项目分类方法；应用先前工作中的可操作恢复策略，并对失败的孵化项目进行案例研究。

Result: 开发的模型和分类方法不仅能有效预测基金会内部项目的可持续性结果，还能跨基金会进行预测；该框架具有普适性，可应用于GitHub上的非基金会项目；通过案例研究验证了恢复策略的有效性。

Conclusion: 社会技术框架在表征和解决软件项目可持续性问题方面具有重要价值，为项目选择合适基金会和制定可持续性计划提供了量化工具和决策支持。

Abstract: Many OSS projects join foundations such as Apache, Eclipse, and OSGeo, to aid their immediate plans and improve long-term prospects by getting governance advice, incubation support, and community-building mechanisms. But foundations differ in their policies, funding models, and support strategies. Moreover, since projects joining these foundations are diverse, coming at different lifecycle stages and having different needs, it can be challenging to decide on the appropriate project-foundation match and on the project-specific plan for sustainability.
  Here, we present an empirical study and quantitative analysis of the sustainability of incubator projects in the Apache, Eclipse, and OSGeo foundations, and, additionally, of OSS projects from GitHub outside of foundations. We develop foundation-specific sustainability models and a project triage, based on projects' sociotechnical trace profiles, and demonstrate their effectiveness across the foundations. Our results show that our models with triage can effectively forecast sustainability outcomes not only within but across foundations. In addition, the generalizability of the framework allows us to apply the approach to GitHub projects outside the foundations. We complement our findings with actionable recovery strategies from previous work and apply them to case studies of failed incubator projects. Our study highlights the value of sociotechnical frameworks in characterizing and addressing software project sustainability issues.

</details>


### [45] [Quantifying Competitive Relationships Among Open-Source Software Projects](https://arxiv.org/abs/2602.17131)
*Yuki Takei,Toshiaki Aoki,Chaiyong Ragkhitwetsagul*

Main category: cs.SE

TL;DR: 该研究提出了一种名为MIAO的自动化方法，用于量化开源软件项目间的竞争关系，通过结构向量自回归模型和脉冲响应函数分析项目间的相互影响，能够以81%的准确率识别因竞争而停止开发的项目。


<details>
  <summary>Details</summary>
Motivation: 开源软件项目在快速演进的领域中面临激烈的竞争，但竞争关系对项目生存的影响尚不明确，存在被竞争对手超越的风险。需要一种量化方法来分析项目间的竞争动态。

Method: 提出MIAO方法，采用宏观经济分析中常用的结构向量自回归模型和脉冲响应函数，分析开源软件项目间的相互作用关系。

Result: 通过对187个开源软件项目组的挖掘分析，MIAO能够以81%的准确率识别因竞争影响而停止开发的项目，相关特征支持提前一年预测项目停止的准确率达到77%。

Conclusion: MIAO可作为开源项目维护者理解生态系统动态、预测项目兴衰的有价值工具，帮助项目在竞争环境中保持优势。

Abstract: Throughout the history of software, evolution has occurred in cycles of rise and fall driven by competition, and open-source software (OSS) is no exception. This cycle is accelerating, particularly in rapidly evolving domains such as web development and deep learning. However, the impact of competitive relationships among OSS projects on their survival remains unclear, and there are risks of losing a competitive edge to rivals. To address this, this study proposes a new automated method called ``Mutual Impact Analysis of OSS (MIAO)'' to quantify these competitive relationships. The proposed method employs a structural vector autoregressive model and impulse response functions, normally used in macroeconomic analysis, to analyze the interactions among OSS projects. In an empirical analysis involving mining and analyzing 187 OSS project groups, MIAO identified projects that were forced to cease development owing to competitive influences with up to 81\% accuracy, and the resulting features supported predictive experiments that anticipate cessation one year ahead with up to 77\% accuracy. This suggests that MIAO could be a valuable tool for OSS project maintainers to understand the dynamics of OSS ecosystems and predict the rise and fall of OSS projects.

</details>


### [46] [Robustness and Reasoning Fidelity of Large Language Models in Long-Context Code Question Answering](https://arxiv.org/abs/2602.17183)
*Kishan Maharaj,Nandakishore Menon,Ashita Saxena,Srikanth Tamilselvam*

Main category: cs.SE

TL;DR: 该论文系统研究了LLM在长代码上下文问答中的鲁棒性，发现现有模型在答案格式变化、干扰信息和上下文规模变化时性能显著下降


<details>
  <summary>Details</summary>
Motivation: LLM越来越多地协助需要长代码上下文推理的软件工程任务，但其在不同输入条件下的鲁棒性尚不清楚。需要系统研究长上下文代码问答的敏感性

Method: 使用受控消融实验测试对答案格式、干扰信息和上下文规模的敏感性。扩展LongCodeBench Python数据集，新增COBOL和Java问答集，评估最先进模型在三种设置下的表现：1) 打乱多项选择选项顺序；2) 开放式问题；3) 包含相关和对抗性无关信息的"大海捞针"上下文

Result: 结果显示在打乱多项选择选项和开放式问题中性能显著下降，且在存在无关线索时表现出脆弱行为。模型对输入条件变化敏感

Conclusion: 当前长上下文评估存在局限性，需要更广泛的基准来评估传统和现代系统中的代码推理能力。研究结果强调了现有模型的鲁棒性不足

Abstract: Large language models (LLMs) increasingly assist software engineering tasks that require reasoning over long code contexts, yet their robustness under varying input conditions remains unclear. We conduct a systematic study of long-context code question answering using controlled ablations that test sensitivity to answer format, distractors, and context scale. Extending LongCodeBench Python dataset with new COBOL and Java question-answer sets, we evaluate state-of-the-art models under three settings: (i) shuffled multiple-choice options, (ii) open-ended questions and (iii) needle-in-a-haystack contexts containing relevant and adversarially irrelevant information. Results show substantial performance drops in both shuffled multiple-choice options and open-ended questions, and brittle behavior in the presence of irrelevant cues. Our findings highlight limitations of current long-context evaluations and provide a broader benchmark for assessing code reasoning in both legacy and modern systems.

</details>


### [47] [Exploring LLMs for User Story Extraction from Mockups](https://arxiv.org/abs/2602.16997)
*Diego Firmenich,Leandro Antonelli,Bruno Pazos,Fabricio Lozada,Leonardo Morales*

Main category: cs.SE

TL;DR: LLMs结合高保真原型图和语言扩展词典能自动生成高质量用户故事，提升需求工程效率


<details>
  <summary>Details</summary>
Motivation: 用户故事是软件行业广泛使用的功能需求定义工具，高保真原型图有助于最终用户参与需求定义。研究探索如何结合这些技术与大语言模型，实现从原型图自动生成用户故事的敏捷自动化方法。

Method: 通过案例研究分析LLMs从高保真原型图提取用户故事的能力，比较是否包含语言扩展词典（LEL）词汇表对生成结果的影响。研究评估了在提示中包含LEL词汇表对生成用户故事准确性和适用性的提升效果。

Result: 结果表明，在提示中包含语言扩展词典（LEL）词汇表能显著提高生成用户故事的准确性和适用性。这种方法在需求工程中整合AI技术方面取得了进展。

Conclusion: 结合LLMs、高保真原型图和LEL词汇表的方法能够自动生成高质量用户故事，改善了用户与开发人员之间的沟通，代表了AI在需求工程集成中的重要进展。

Abstract: User stories are one of the most widely used artifacts in the software industry to define functional requirements. In parallel, the use of high-fidelity mockups facilitates end-user participation in defining their needs. In this work, we explore how combining these techniques with large language models (LLMs) enables agile and automated generation of user stories from mockups. To this end, we present a case study that analyzes the ability of LLMs to extract user stories from high-fidelity mockups, both with and without the inclusion of a glossary of the Language Extended Lexicon (LEL) in the prompts. Our results demonstrate that incorporating the LEL significantly enhances the accuracy and suitability of the generated user stories. This approach represents a step forward in the integration of AI into requirements engineering, with the potential to improve communication between users and developers.

</details>
