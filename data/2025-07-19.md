<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 15]
- [cs.CR](#cs.CR) [Total: 10]
- [cs.AI](#cs.AI) [Total: 20]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [A Survey of AIOps in the Era of Large Language Models](https://arxiv.org/abs/2507.12472)
*Lingzhe Zhang,Tong Jia,Mengxi Jia,Yifan Wu,Aiwei Liu,Yong Yang,Zhonghai Wu,Xuming Hu,Philip S. Yu,Ying Li*

Main category: cs.SE

TL;DR: 该论文调查了大型语言模型（LLMs）在AIOps中的应用，分析了183篇研究论文，探讨了数据来源、任务演变、方法应用及评估方法，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs的普及，其在AIOps中的应用潜力与局限性尚不明确，需系统性研究。

Method: 通过分析183篇2020-2024年的论文，回答了四个研究问题（数据来源、任务演变、方法应用、评估方法）。

Result: 总结了LLMs在AIOps中的最新进展、趋势及研究空白。

Conclusion: 论文为LLMs在AIOps中的应用提供了全面视角，并指出了未来研究方向。

Abstract: As large language models (LLMs) grow increasingly sophisticated and
pervasive, their application to various Artificial Intelligence for IT
Operations (AIOps) tasks has garnered significant attention. However, a
comprehensive understanding of the impact, potential, and limitations of LLMs
in AIOps remains in its infancy. To address this gap, we conducted a detailed
survey of LLM4AIOps, focusing on how LLMs can optimize processes and improve
outcomes in this domain. We analyzed 183 research papers published between
January 2020 and December 2024 to answer four key research questions (RQs). In
RQ1, we examine the diverse failure data sources utilized, including advanced
LLM-based processing techniques for legacy data and the incorporation of new
data sources enabled by LLMs. RQ2 explores the evolution of AIOps tasks,
highlighting the emergence of novel tasks and the publication trends across
these tasks. RQ3 investigates the various LLM-based methods applied to address
AIOps challenges. Finally, RQ4 reviews evaluation methodologies tailored to
assess LLM-integrated AIOps approaches. Based on our findings, we discuss the
state-of-the-art advancements and trends, identify gaps in existing research,
and propose promising directions for future exploration.

</details>


### [2] [LLM-Powered Quantum Code Transpilation](https://arxiv.org/abs/2507.12480)
*Nazanin Siavash,Armin Moin*

Main category: cs.SE

TL;DR: 利用大型语言模型（LLMs）实现量子软件开发工具包（QSDKs）间的自动代码转换，解决跨平台互操作性问题。


<details>
  <summary>Details</summary>
Motivation: 量子计算平台多样化的QSDKs导致互操作性和跨平台开发困难，传统基于规则的转译器设计维护成本高。

Method: 利用LLMs的预训练知识和上下文推理能力，作为编程语言无关的转译器，实现量子程序的功能等价转换。

Result: 提出了一种无需手动定义转换规则、可扩展的量子软件移植解决方案。

Conclusion: 为量子计算生态系统中智能通用转译迈出一步。

Abstract: There exist various Software Development Kits (SDKs) tailored to different
quantum computing platforms. These are known as Quantum SDKs (QSDKs). Examples
include but are not limited to Qiskit, Cirq, and PennyLane. However, this
diversity presents significant challenges for interoperability and
cross-platform development of hybrid quantum-classical software systems.
Traditional rule-based transpilers for translating code between QSDKs are
time-consuming to design and maintain, requiring deep expertise and rigid
mappings in the source and destination code. In this study, we explore the use
of Large Language Models (LLMs) as a flexible and automated solution.
Leveraging their pretrained knowledge and contextual reasoning capabilities, we
position LLMs as programming language-agnostic transpilers capable of
converting quantum programs from one QSDK to another while preserving
functional equivalence. Our approach eliminates the need for manually defined
transformation rules and offers a scalable solution to quantum software
portability. This work represents a step toward enabling intelligent,
general-purpose transpilation in the quantum computing ecosystem.

</details>


### [3] [Kodezi Chronos: A Debugging-First Language Model for Repository-Scale, Memory-Driven Code Understanding](https://arxiv.org/abs/2507.12482)
*Ishraq Khan,Assad Chowdary,Sharoz Haseeb,Urvish Patel*

Main category: cs.SE

TL;DR: Kodezi Chronos是一种新型架构，用于自主代码理解、调试和维护，支持超长上下文和高效推理，显著提升代码可靠性和生产力。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLMs）在代码生成和软件自动化中存在上下文限制和缺乏显式代码结构推理的问题。

Method: Kodezi Chronos采用多级嵌入内存引擎，结合向量和图索引，支持高效代码检索和推理。

Result: Chronos在真实世界错误检测中表现优于现有模型，提升23%，调试周期减少40%。

Conclusion: Kodezi Chronos为自主软件维护和持续优化生态系统提供了重要进展。

Abstract: Large Language Models (LLMs) have advanced code generation and software
automation, but are fundamentally constrained by limited inference-time context
and lack of explicit code structure reasoning. We introduce Kodezi Chronos, a
next-generation architecture for autonomous code understanding, debugging, and
maintenance, designed to operate across ultra-long contexts comprising entire
codebases, histories, and documentation, all without fixed window limits.
Kodezi Chronos leverages a multi-level embedding memory engine, combining
vector and graph-based indexing with continuous code-aware retrieval. This
enables efficient and accurate reasoning over millions of lines of code,
supporting repository-scale comprehension, multi-file refactoring, and
real-time self-healing actions. Our evaluation introduces a novel Multi Random
Retrieval benchmark, specifically tailored to the software engineering domain.
Unlike classical retrieval benchmarks, this method requires the model to
resolve arbitrarily distant and obfuscated associations across code artifacts,
simulating realistic tasks such as variable tracing, dependency migration, and
semantic bug localization. Chronos outperforms prior LLMs and code models,
demonstrating a 23% improvement in real-world bug detection and reducing
debugging cycles by up to 40% compared to traditional sequence-based
approaches. By natively interfacing with IDEs and CI/CD workflows, Chronos
enables seamless, autonomous software maintenance, elevating code reliability
and productivity while reducing manual effort. These results mark a critical
advance toward self-sustaining, continuously optimized software ecosystems.

</details>


### [4] [A Survey of Reinforcement Learning for Software Engineering](https://arxiv.org/abs/2507.12483)
*Dong Wang,Hanmo You,Lingwei Zhu,Kaiwei Lin,Zheng Chen,Chen Yang,Junji Yu,Zan Wang,Junjie Chen*

Main category: cs.SE

TL;DR: 本文对强化学习（RL）在软件工程（SE）中的应用进行了首次系统性综述，分析了115篇研究，总结了趋势、算法分类及未来方向。


<details>
  <summary>Details</summary>
Motivation: 随着软件系统复杂性和自动化需求的增加，RL在SE中的应用日益广泛，但缺乏系统性综述。

Method: 回顾了115篇同行评审研究，分析了发表趋势、SE主题分类、RL算法使用、数据集、模型设计和评估实践。

Result: 总结了RL在SE中的应用现状，识别了开放挑战，并提出了未来研究方向。

Conclusion: 本综述为研究者和从业者提供了RL在SE领域的系统性指南，并公开了相关资源。

Abstract: Reinforcement Learning (RL) has emerged as a powerful paradigm for sequential
decision-making and has attracted growing interest across various domains,
particularly following the advent of Deep Reinforcement Learning (DRL) in 2015.
Simultaneously, the rapid advancement of Large Language Models (LLMs) has
further fueled interest in integrating RL with LLMs to enable more adaptive and
intelligent systems. In the field of software engineering (SE), the increasing
complexity of systems and the rising demand for automation have motivated
researchers to apply RL to a broad range of tasks, from software design and
development to quality assurance and maintenance. Despite growing research in
RL-for-SE, there remains a lack of a comprehensive and systematic survey of
this evolving field. To address this gap, we reviewed 115 peer-reviewed studies
published across 22 premier SE venues since the introduction of DRL. We
conducted a comprehensive analysis of publication trends, categorized SE topics
and RL algorithms, and examined key factors such as dataset usage, model design
and optimization, and evaluation practices. Furthermore, we identified open
challenges and proposed future research directions to guide and inspire ongoing
work in this evolving area. To summarize, this survey offers the first
systematic mapping of RL applications in software engineering, aiming to
support both researchers and practitioners in navigating the current landscape
and advancing the field. Our artifacts are publicly available:
https://github.com/KaiWei-Lin-lanina/RL4SE.

</details>


### [5] [When Retriever Meets Generator: A Joint Model for Code Comment Generation](https://arxiv.org/abs/2507.12558)
*Tien P. T. Le,Anh M. T. Bui,Huy N. D. Pham,Alessio Bucaioni,Phuong T. Nguyen*

Main category: cs.SE

TL;DR: RAGSum是一种结合检索与生成的代码注释自动生成方法，通过统一框架优化检索和生成过程，显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 解决现有检索增强方法中检索与生成分离导致的噪声传播问题，提升代码注释生成的效率和效果。

Method: 基于CodeT5的统一框架，结合对比预训练和端到端训练，使用复合损失函数优化检索和生成，并引入轻量级自优化循环。

Result: 在Java、Python、C三种语言的基准测试中，RAGSum在BLEU、METEOR和ROUTE-L指标上显著优于基线方法。

Conclusion: 紧密耦合检索与生成能提升注释自动化的上限，未来可进一步验证和开展开发者研究。

Abstract: Automatically generating concise, informative comments for source code can
lighten documentation effort and accelerate program comprehension.
Retrieval-augmented approaches first fetch code snippets with existing comments
and then synthesize a new comment, yet retrieval and generation are typically
optimized in isolation, allowing irrelevant neighbors topropagate noise
downstream. To tackle the issue, we propose a novel approach named RAGSum with
the aim of both effectiveness and efficiency in recommendations. RAGSum is
built on top offuse retrieval and generation using a single CodeT5 backbone. We
report preliminary results on a unified retrieval-generation framework built on
CodeT5. A contrastive pre-training phase shapes code embeddings for
nearest-neighbor search; these weights then seed end-to-end training with a
composite loss that (i) rewards accurate top-k retrieval; and (ii) minimizes
comment-generation error. More importantly, a lightweight self-refinement loop
is deployed to polish the final output. We evaluated theframework on three
cross-language benchmarks (Java, Python, C), and compared it with three
well-established baselines. The results show that our approach substantially
outperforms thebaselines with respect to BLEU, METEOR, and ROUTE-L. These
findings indicate that tightly coupling retrieval and generationcan raise the
ceiling for comment automation and motivateforthcoming replications and
qualitative developer studies.

</details>


### [6] [ROSE: Transformer-Based Refactoring Recommendation for Architectural Smells](https://arxiv.org/abs/2507.12561)
*Samal Nursapa,Anastassiya Samuilova,Alessio Bucaioni. Phuong T. Nguyen*

Main category: cs.SE

TL;DR: 论文探讨了使用预训练Transformer模型（CodeBERT和CodeT5）为检测到的代码坏味道推荐重构方法，CodeT5表现最佳。


<details>
  <summary>Details</summary>
Motivation: 现有工具能检测代码坏味道但很少提供修复建议，研究旨在填补这一空白。

Method: 将任务定义为三分类问题，在11000多个开源Java项目的200多万个重构实例上微调模型。

Result: CodeT5达到96.9%准确率和95.2% F1分数，优于CodeBERT和传统基线。

Conclusion: Transformer模型能有效连接坏味道检测与修复，为未来重构推荐系统奠定基础。

Abstract: Architectural smells such as God Class, Cyclic Dependency, and Hub-like
Dependency degrade software quality and maintainability. Existing tools detect
such smells but rarely suggest how to fix them. This paper explores the use of
pre-trained transformer models--CodeBERT and CodeT5--for recommending suitable
refactorings based on detected smells. We frame the task as a three-class
classification problem and fine-tune both models on over 2 million refactoring
instances mined from 11,149 open-source Java projects. CodeT5 achieves 96.9%
accuracy and 95.2% F1, outperforming CodeBERT and traditional baselines. Our
results show that transformer-based models can effectively bridge the gap
between smell detection and actionable repair, laying the foundation for future
refactoring recommendation systems. We release all code, models, and data under
an open license to support reproducibility and further research.

</details>


### [7] [QSpark: Towards Reliable Qiskit Code Generation](https://arxiv.org/abs/2507.12642)
*Kiana Kheiri,Aamna Aamir,Andriy Miranskyy,Chen Ding*

Main category: cs.SE

TL;DR: 论文研究了如何通过强化学习方法（GRPO和ORPO）优化量子编程中的代码生成，显著提升了Qiskit HumanEval基准的表现，但仍存在高级任务未解决的挑战。


<details>
  <summary>Details</summary>
Motivation: 量子电路需要具备抗错能力，但现有LLM（如Granite-20B-Code和StarCoder）生成的Qiskit代码存在缺陷，因此需要改进模型以提升代码质量。

Method: 使用两种强化学习方法（GRPO和ORPO）对32B模型进行微调，并利用合成数据集进行训练。

Result: 在Qiskit HumanEval基准测试中，ORPO达到56.29% Pass@1（比Granite-8B-QK高约10个百分点），GRPO为49%，均优于通用基线；在原始HumanEval中分别达到65.90%和63.00%。GRPO在基础任务表现优异（42/54），ORPO在中等任务表现更好（41/68），但均未解决5个高级任务。

Conclusion: 研究展示了AI辅助量子编程的显著进步，但仍需在高级任务上进一步突破。

Abstract: Quantum circuits must be error-resilient, yet LLMs like Granite-20B-Code and
StarCoder often output flawed Qiskit code. We fine-tuned a 32 B model with two
RL methods, Group Relative Policy Optimization (GRPO) and Odds-Ratio Preference
Optimization (ORPO), using a richly annotated synthetic dataset. On the Qiskit
HumanEval benchmark, ORPO reaches 56.29\% Pass@1 ($\approx+10$ pp over
Granite-8B-QK) and GRPO hits 49\%, both beating all general-purpose baselines;
on the original HumanEval they score 65.90\% and 63.00\%. GRPO excels on basic
tasks (42/54), ORPO on intermediate ones (41/68), and neither solves the five
advanced tasks, highlighting clear gains yet room for progress in AI-assisted
quantum programming.

</details>


### [8] [A Three-Phase Evaluation Approach for new Information and Data Models in the Smart Grid Domain](https://arxiv.org/abs/2507.12649)
*Christine van Stiphoudt,Sergio Potenciano Menci,Gilbert Fridgen*

Main category: cs.SE

TL;DR: 本文提出了一种结合显式和隐式评估方法的三阶段评估方法，用于智能电网中新设计和数据模型的开发。


<details>
  <summary>Details</summary>
Motivation: 智能电网数字化导致分布式能源系统中自动化信息交换增加，现有模型不足，需在设计中评估新模型以避免潜在问题。

Method: 采用设计科学研究方法，设计了一个三阶段评估方法，结合显式和隐式评估，并以工业灵活性描述的信息和数据模型为例进行验证。

Result: 提出了一种明确的评估步骤，填补了智能电网中新模型设计过程中显式和隐式评估方法结合的空白。

Conclusion: 该方法为智能电网中新信息与数据模型的设计提供了实用评估框架，并总结了相关经验教训。

Abstract: The ongoing digitalisation of the smart grid is resulting in an increase in
automated information exchanges across distributed energy systems. This process
has led to the development of new information and data models when the existing
ones fall short. To prevent potential disruptions caused by flaws in the newly
designed information and data models, it is essential to evaluate them during
the design process before they are implemented in operation.
  Currently, general explicit evaluation approaches outside the smart grid
domain stay at a high level without defining clear steps. Meanwhile, implicit
evaluation approaches in the smart grid domain focus on testing systems that
utilise information and data models already in use for functionality in terms
of conformance and interoperability. Notably, no combination of explicit and
implicit evaluation approaches for newly designed information and data models
offers a clearly defined set of steps during their design process in the smart
grid context.
  Consequently, we design a three-phase evaluation approach using design
science research to address this gap. Our evaluation approach combines explicit
and implicit evaluation methods and is applicable when developing new
information and data models. We use the development of an information model and
data model focused on industrial flexibility descriptions to refine our
evaluation approach. Additionally, we provide lessons learned from our
experience.

</details>


### [9] [A Fuzzy Approach to Project Success: Measuring What Matters](https://arxiv.org/abs/2507.12653)
*João Granja-Correia,Remedios Hernández-Linares,Luca Ferranti,Arménio Rego*

Main category: cs.SE

TL;DR: 本文提出了一种基于模糊逻辑的项目成功评估新方法，改进了传统Likert量表的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统Likert量表忽略了项目成功的上下文依赖性和多面性，需要更动态的评估方法。

Method: 采用分层Type-1 Mamdani模糊系统，优先考虑对最终用户的持续积极影响。

Result: 该方法可能更准确地衡量项目成功，并适用于复杂评估。

Conclusion: 未来研究将聚焦于模糊逻辑在社会科学中的实证测试和广泛应用。

Abstract: This paper introduces a novel approach to project success evaluation by
integrating fuzzy logic into an existing construct. Traditional Likert-scale
measures often overlook the context-dependent and multifaceted nature of
project success. The proposed hierarchical Type-1 Mamdani fuzzy system
prioritizes sustained positive impact for end-users, reducing emphasis on
secondary outcomes like stakeholder satisfaction and internal project success.
This dynamic approach may provide a more accurate measure of project success
and could be adaptable to complex evaluations. Future research will focus on
empirical testing and broader applications of fuzzy logic in social science.

</details>


### [10] [Single Conversation Methodology: A Human-Centered Protocol for AI-Assisted Software Development](https://arxiv.org/abs/2507.12665)
*Salvador D. Escobedo*

Main category: cs.SE

TL;DR: 提出了一种名为SCM的新方法，通过单一对话结构利用LLMs进行软件开发，强调开发者的主动角色。


<details>
  <summary>Details</summary>
Motivation: 当前对LLMs的依赖过于被动，SCM旨在通过结构化对话重新确立开发者作为架构师和监督者的角色。

Method: SCM基于认知清晰性、可追溯性、模块化和文档化原则，定义开发阶段、最佳实践和哲学立场。

Result: SCM提供了一种纠正当前LLM使用被动性的方法，强调开发者的主动参与。

Conclusion: SCM为LLMs在软件开发中的应用提供了一种结构化且实用的方法，强调开发者主导。

Abstract: We propose the Single Conversation Methodology (SCM), a novel and pragmatic
approach to software development using large language models (LLMs). In
contrast to ad hoc interactions with generative AI, SCM emphasizes a structured
and persistent development dialogue, where all stages of a project - from
requirements to architecture and implementation - unfold within a single,
long-context conversation. The methodology is grounded on principles of
cognitive clarity, traceability, modularity, and documentation. We define its
phases, best practices, and philosophical stance, while arguing that SCM offers
a necessary correction to the passive reliance on LLMs prevalent in current
practices. We aim to reassert the active role of the developer as architect and
supervisor of the intelligent tool.

</details>


### [11] [Investigating the Performance of Small Language Models in Detecting Test Smells in Manual Test Cases](https://arxiv.org/abs/2507.13035)
*Keila Lucas,Rohit Gheyi,Márcio Ribeiro,Fabio Palomba,Luana Martins,Elvys Soares*

Main category: cs.SE

TL;DR: 研究探讨了小语言模型（SLMs）在自动检测测试异味（如模糊性、冗余等）中的潜力，Phi-4表现最佳，准确率达97%。


<details>
  <summary>Details</summary>
Motivation: 手动测试中存在的测试异味降低了测试的可靠性和可维护性，现有工具缺乏扩展性，SLMs有望解决这一问题。

Method: 评估了Gemma3、Llama3.2和Phi-4在143个真实Ubuntu测试案例中的表现，覆盖七种测试异味类型。

Result: Phi-4表现最优，准确率达97%，其他模型约91%。SLMs还能自主解释问题并提出改进建议。

Conclusion: SLMs是一种高效工具，能低成本识别测试异味，提升测试质量，且保护数据隐私。

Abstract: Manual testing, in which testers follow natural language instructions to
validate system behavior, remains crucial for uncovering issues not easily
captured by automation. However, these test cases often suffer from test
smells, quality issues such as ambiguity, redundancy, or missing checks that
reduce test reliability and maintainability. While detection tools exist, they
typically require manual rule definition and lack scalability. This study
investigates the potential of Small Language Models (SLMs) for automatically
detecting test smells. We evaluate Gemma3, Llama3.2, and Phi-4 on 143
real-world Ubuntu test cases, covering seven types of test smells. Phi-4
achieved the best results, reaching a pass@2 of 97% in detecting sentences with
test smells, while Gemma3 and Llama3.2 reached approximately 91%. Beyond
detection, SLMs autonomously explained issues and suggested improvements, even
without explicit prompt instructions. They enabled low-cost, concept-driven
identification of diverse test smells without relying on extensive rule
definitions or syntactic analysis. These findings highlight the potential of
SLMs as efficient tools that preserve data privacy and can improve test quality
in real-world scenarios.

</details>


### [12] [iReDev: A Knowledge-Driven Multi-Agent Framework for Intelligent Requirements Development](https://arxiv.org/abs/2507.13081)
*Dongming Jin,Weisong Sun,Jiangping Huang,Peng Liang,Jifeng Xuan,Yang Liu,Zhi Jin*

Main category: cs.SE

TL;DR: iReDev是一个知识驱动的多智能体框架，用于智能需求开发，通过整合人类知识和事件驱动机制提升效率。


<details>
  <summary>Details</summary>
Motivation: 现有研究对需求开发支持有限，且忽视了人类知识与智能体的结合及人机协作。

Method: iReDev包含六个知识驱动的智能体，通过事件驱动的通信机制和人工干预机制协作完成任务。

Result: iReDev在多个方面优于现有基线，生成的工件更符合利益相关者期望。

Conclusion: iReDev为智能需求开发提供了新方向，未来可进一步探索其潜力。

Abstract: Requirements development is a critical phase as it is responsible for
providing a clear understanding of what stakeholders need. It involves
collaboration among stakeholders to extract explicit requirements and address
potential conflicts, which is time-consuming and labor-intensive. Recently,
multi-agent systems for software development have attracted much attention.
However, existing research provides limited support for requirements
development and overlooks the injection of human knowledge into agents and the
human-agent collaboration. % To address these issues, this paper proposes a
knowledge-driven multi-agent framework for intelligent requirement development,
named iReDev. iReDev features: iReDev consists of six knowledge-driven agents
to support the entire requirements development. They collaboratively perform
various tasks to produce a software requirements specification. iReDev focuses
on integrating human knowledge for agents, enabling them to simulate real-world
stakeholders. iReDev uses an event-driven communication mechanism based on an
artifact pool. Agents continuously monitor the pool and autonomously trigger
the next action based on its changes, enabling iReDev to handle new
requirements quickly. iReDev introduces a human-in-the-loop mechanism to
support human-agent collaboration, ensuring that the generated artifacts align
with the expectations of stakeholders. We evaluated the generated artifacts and
results show that iReDev outperforms existing baselines in multiple aspects. We
further envision three key directions and hope this work can facilitate the
development of intelligent requirements development.

</details>


### [13] [A Conceptual Framework for Requirements Engineering of Pretrained-Model-Enabled Systems](https://arxiv.org/abs/2507.13095)
*Dongming Jin,Zhi Jin,Linyu Li,Xiaohong Chen*

Main category: cs.SE

TL;DR: 论文探讨了预训练模型在软件系统中带来的挑战，提出了重新思考需求工程方法的必要性，并提出了一个概念框架。


<details>
  <summary>Details</summary>
Motivation: 预训练模型的广泛集成挑战了传统需求工程的确定性假设，如功能可分解性和行为可预测性。

Method: 提出了一个针对预训练模型软件系统的需求工程概念框架，并概述了相关研究方向。

Result: 框架为研究人员和实践者提供了应对预训练模型系统需求工程挑战的指导。

Conclusion: 需要重新思考需求工程方法以适应预训练模型系统的独特特性。

Abstract: Recent advances in large pretrained models have led to their widespread
integration as core components in modern software systems. The trend is
expected to continue in the foreseeable future. Unlike traditional software
systems governed by deterministic logic, systems powered by pretrained models
exhibit distinctive and emergent characteristics, such as ambiguous capability
boundaries, context-dependent behavior, and continuous evolution. These
properties fundamentally challenge long-standing assumptions in requirements
engineering, including functional decomposability and behavioral
predictability. This paper investigates this problem and advocates for a
rethinking of existing requirements engineering methodologies. We propose a
conceptual framework tailored to requirements engineering of
pretrained-model-enabled software systems and outline several promising
research directions within this framework. This vision helps provide a guide
for researchers and practitioners to tackle the emerging challenges in
requirements engineering of pretrained-model-enabled systems.

</details>


### [14] [Inferring Attributed Grammars from Parser Implementations](https://arxiv.org/abs/2507.13117)
*Andreas Pointner,Josef Pichler,Herbert Prähofer*

Main category: cs.SE

TL;DR: 提出了一种从递归下降解析器实现中推断属性语法的新方法，用于恢复输入处理的语义规范。


<details>
  <summary>Details</summary>
Motivation: 现有语法挖掘技术主要关注语法结构恢复，而输入处理的语义规范仍未被充分探索。

Method: 通过动态分析递归下降解析器的实现，将程序运行时行为映射到语法规则中，提取并嵌入语义动作，生成属性语法。

Result: 实验证明该方法能准确通过生成的属性语法重现程序行为。

Conclusion: 该方法为输入处理的语义规范恢复提供了可行方案。

Abstract: Software systems that process structured inputs often lack complete and
up-to-date specifications, which specify the input syntax and the semantics of
input processing. While grammar mining techniques have focused on recovering
syntactic structures, the semantics of input processing remains largely
unexplored. In this work, we introduce a novel approach for inferring
attributed grammars from parser implementations. Given an input grammar, our
technique dynamically analyzes the implementation of recursive descent parsers
to reconstruct the semantic aspects of input handling, resulting in
specifications in the form of attributed grammars. By observing program
executions and mapping the program's runtime behavior to the grammar, we
systematically extract and embed semantic actions into the grammar rules. This
enables comprehensive specification recovery. We demonstrate the feasibility of
our approach using an initial set of programs, showing that it can accurately
reproduce program behavior through the generated attributed grammars.

</details>


### [15] [Detecting LLM-generated Code with Subtle Modification by Adversarial Training](https://arxiv.org/abs/2507.13123)
*Xin Yin,Xinrui Li,Chao Ni,Xiaodan Xu,Xiaohu Yang*

Main category: cs.SE

TL;DR: 论文提出CodeGPTSensor+，通过对抗训练提升对LLM生成代码的检测鲁棒性，解决代码修改后的检测问题。


<details>
  <summary>Details</summary>
Motivation: 随着LLM生成代码的广泛应用，代码来源、版权和质量问题日益突出，现有检测方法对修改后的代码鲁棒性不足。

Method: 提出CodeGPTSensor+，集成对抗样本生成模块MIST，通过系统生成高质量对抗样本提升模型鲁棒性。

Result: 在HMCorp数据集上，CodeGPTSensor+显著提升对抗测试集的检测准确率，同时保持原测试集的高准确率。

Conclusion: CodeGPTSensor+在检测修改后的LLM生成代码方面表现出优越的鲁棒性，为实际应用提供了有效解决方案。

Abstract: With the rapid development of Large Language Models (LLMs), their powerful
code-generation capabilities have been widely applied in tasks like code
completion and automated development, demonstrating the value of improving
coding efficiency. However, the extensive use of LLM-generated code also raises
several new challenges. On the one hand, issues such as the regulation of code
provenance, copyright disputes, and code quality have become increasingly
concerning. How to effectively detect LLM-generated code and ensure its
compliant and responsible use has become a critical and urgent issue. On the
other hand, in practical applications, LLM-generated code is often subject to
manual modifications, such as variable renaming or structural adjustments.
Although some recent studies have proposed training-based and zero-shot methods
for detecting LLM-generated code, these approaches show insufficient robustness
when facing modified LLM-generated code, and there is a lack of an effective
solution. To address the real-world scenario where LLM-generated code may
undergo minor modifications, we propose CodeGPTSensor+, an enhanced version of
CodeGPTSensor, which employs adversarial training to improve robustness against
input perturbations. CodeGPTSensor+ integrates an adversarial sample generation
module, Multi-objective Identifier and Structure Transformation (MIST), which
systematically generates both high-quality and representative adversarial
samples. This module effectively enhances the model's resistance against
diverse adversarial attacks. Experimental results on the HMCorp dataset
demonstrate that CodeGPTSensor+ significantly improves detection accuracy on
the adversarial test set while maintaining high accuracy on the original test
set, showcasing superior robustness compared to CodeGPTSensor.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [16] [Safeguarding Federated Learning-based Road Condition Classification](https://arxiv.org/abs/2507.12568)
*Sheng Liu,Panos Papadimitratos*

Main category: cs.CR

TL;DR: 本文研究了联邦学习在自动驾驶道路状况分类系统中面临的目标标签翻转攻击问题，提出了FLARE防御机制来检测和缓解此类攻击对行车安全的威胁。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在自动驾驶道路状况分类中虽然能保护隐私，但其协作性质引入了新的安全漏洞——恶意客户端可以通过目标标签翻转攻击故意篡改训练数据标签，导致模型错误分类危险路况（如将湿滑路面误判为良好路面），从而威胁行车安全。然而现有研究对此类攻击缺乏深入分析。

Method: 提出三重贡献：1）揭示现有联邦学习道路状况分类系统对目标标签翻转攻击的脆弱性；2）引入基于标签距离的新指标来精确量化攻击带来的安全风险；3）提出FLARE防御机制，通过对输出层进行神经元级分析来缓解攻击效果。

Result: 在三个道路状况分类任务、四个评估指标、六个基线方法和三个深度学习模型上进行的大量实验表明，目标标签翻转攻击对联邦学习道路状况分类系统具有严重威胁，而FLARE机制能够有效缓解攻击影响。

Conclusion: 联邦学习在自动驾驶道路状况分类中存在严重的安全隐患，恶意攻击可能导致致命的误判。通过FLARE防御机制可以有效识别和减轻此类攻击，为联邦学习在安全关键应用中的部署提供了重要保障。

Abstract: Federated Learning (FL) has emerged as a promising solution for
privacy-preserving autonomous driving, specifically camera-based Road Condition
Classification (RCC) systems, harnessing distributed sensing, computing, and
communication resources on board vehicles without sharing sensitive image data.
However, the collaborative nature of FL-RCC frameworks introduces new
vulnerabilities: Targeted Label Flipping Attacks (TLFAs), in which malicious
clients (vehicles) deliberately alter their training data labels to compromise
the learned model inference performance. Such attacks can, e.g., cause a
vehicle to mis-classify slippery, dangerous road conditions as pristine and
exceed recommended speed. However, TLFAs for FL-based RCC systems are largely
missing. We address this challenge with a threefold contribution: 1) we
disclose the vulnerability of existing FL-RCC systems to TLFAs; 2) we introduce
a novel label-distance-based metric to precisely quantify the safety risks
posed by TLFAs; and 3) we propose FLARE, a defensive mechanism leveraging
neuron-wise analysis of the output layer to mitigate TLFA effects. Extensive
experiments across three RCC tasks, four evaluation metrics, six baselines, and
three deep learning models demonstrate both the severity of TLFAs on FL-RCC
systems and the effectiveness of FLARE in mitigating the attack impact.

</details>


### [17] [On the Consideration of Vanity Address Generation via Identity-Based Signatures](https://arxiv.org/abs/2507.12670)
*Shogo Murasaki,Kazumasa Omote,Keita Emura*

Main category: cs.CR

TL;DR: 本文探讨了利用身份基签名（IBS）生成区块链虚荣地址的可行性，提出了一种基于ECDSA的IBS方案，并通过Solidity实现，验证了其与ECDSA签名验证的燃气成本相近。


<details>
  <summary>Details</summary>
Motivation: 现有区块链中，虚荣地址的生成受限于传统试错方法，嵌入字符数量有限。本文旨在探索IBS是否可用于生成虚荣地址，同时避免替换现有ECDSA签名方案的高成本。

Method: 提出了一种基于ECDSA的IBS通用构造方案，利用IBS功能将任意字符串与地址关联，并通过Solidity实现系统。

Result: 实现结果表明，该方案的燃气成本与ECDSA签名验证相近，验证了其可行性。

Conclusion: 虽然无法直接生成虚荣地址，但通过IBS方案可以间接实现地址与字符串的关联，为区块链身份管理提供了新思路。

Abstract: An address is indicated as an identifier of the user on the blockchain, and
is defined by a hash value of the ECDSA verification key. A vanity address is
an address that embeds custom characters such as a name. To generate a vanity
address, a classical try-and-error method is employed, and thus the number of
characters to be embedded is limited. In this paper, we focus on the
functionality of identity-based signatures (IBS) where any strings can be
employed as a verification key, and explore whether IBS can be used for
generating a vanity address. We attach importance to the fact that it is not
realistic to replace ECDSA with key recovery, which is currently employed for
issuing transactions in Ethereum, to an IBS scheme. Even if this replacement is
possible, it is not a reasonable price for the ease of the vanity address
generation. Thus, we pay attention to a generic construction of IBS from
signatures, and construct an IBS scheme from ECDSA with key recovery. Though we
cannot directly generate a vanity address due to the key recovery functionality
of the underlying ECDSA, we can connect any string with an address due to the
functionality of IBS that can give additional meaning to the address. We
implement our system by Solidity, and demonstrate that the gas cost is almost
same as that of the ECDSA signature verification.

</details>


### [18] [Architectural Backdoors in Deep Learning: A Survey of Vulnerabilities, Detection, and Defense](https://arxiv.org/abs/2507.12919)
*Victoria Childress,Josh Collyer,Jodie Knapp*

Main category: cs.CR

TL;DR: 本文综述了深度神经网络中的架构后门威胁，探讨了其隐蔽性和持久性，并总结了现有的检测与防御策略及其局限性。


<details>
  <summary>Details</summary>
Motivation: 架构后门是一种新型威胁，直接嵌入模型的计算图中，难以通过传统方法检测或清除，因此需要系统研究以应对这一挑战。

Method: 通过系统整理现有研究，包括编译器级操作、污染的AutoML流程和供应链漏洞，并评估静态图检查、动态模糊测试和部分形式验证等防御策略。

Result: 尽管已有进展，但针对分布式或隐蔽触发器的可扩展和实用防御仍不足。

Conclusion: 未来研究应关注供应链安全、加密模型认证和下一代基准测试，以全面防御架构后门威胁。

Abstract: Architectural backdoors pose an under-examined but critical threat to deep
neural networks, embedding malicious logic directly into a model's
computational graph. Unlike traditional data poisoning or parameter
manipulation, architectural backdoors evade standard mitigation techniques and
persist even after clean retraining. This survey systematically consolidates
research on architectural backdoors, spanning compiler-level manipulations,
tainted AutoML pipelines, and supply-chain vulnerabilities. We assess emerging
detection and defense strategies, including static graph inspection, dynamic
fuzzing, and partial formal verification, and highlight their limitations
against distributed or stealth triggers. Despite recent progress, scalable and
practical defenses remain elusive. We conclude by outlining open challenges and
proposing directions for strengthening supply-chain security, cryptographic
model attestations, and next-generation benchmarks. This survey aims to guide
future research toward comprehensive defenses against structural backdoor
threats in deep learning systems.

</details>


### [19] [Enterprise Security Incident Analysis and Countermeasures Based on the T-Mobile Data Breach](https://arxiv.org/abs/2507.12937)
*Zhuohan Cui,Zikun Song*

Main category: cs.CR

TL;DR: 本文分析了T-Mobile在2021和2023年的重大数据泄露事件，通过全面的安全审计和漏洞评估，提出了一种多层防御策略，并验证了其成本效益。


<details>
  <summary>Details</summary>
Motivation: 研究T-Mobile数据泄露的根本原因，并提出有效的安全改进措施，以增强电信行业的操作弹性和合规性。

Method: 结合案例漏洞评估和主动伦理黑客技术（如Shodan侦察、API滥用模拟、VNC暴力破解等），进行安全审计。

Result: 发现结构性弱点，提出多层防御策略（如零信任架构、网络分段等），并证明其成本效益。

Conclusion: 研究为电信行业提供了可操作的蓝图，以提升安全性和应对跨域威胁。

Abstract: This paper presents a comprehensive analysis of T-Mobile's critical data
breaches in 2021 and 2023, alongside a full-spectrum security audit targeting
its systems, infrastructure, and publicly exposed endpoints. By combining
case-based vulnerability assessments with active ethical hacking
techniques--including Shodan reconnaissance, API misuse simulations, VNC
brute-forcing, firmware reverse engineering, and web application scans--we
uncover structural weaknesses persisting beyond the initial breach events.
Building on these findings, we propose a multi-layered defensive strategy
encompassing Zero Trust Architecture, granular role-based access control,
network segmentation, firmware encryption using AES with integrity checks, and
API rate limiting and token lifecycle control. Financial modelling demonstrates
that a five-year investment yields less than 1.1% of expected breach losses,
validating the cost-effectiveness of proactive security measures. Our work
bridges post-incident forensic analysis with hands-on security evaluation,
providing an actionable blueprint for large-scale telecoms seeking operational
resilience, regulatory compliance, and cross-domain threat readiness.

</details>


### [20] [Measuring CEX-DEX Extracted Value and Searcher Profitability: The Darkest of the MEV Dark Forest](https://arxiv.org/abs/2507.13023)
*Fei Wu,Danning Sui,Thomas Thiery,Mallesh Pai*

Main category: cs.CR

TL;DR: 本文通过实证分析揭示了以太坊上中心化与去中心化交易所（CEX-DEX）套利的经济学与动态，发现19个月内有19个主要套利者通过720万次套利提取了2.338亿美元，且市场集中度上升。


<details>
  <summary>Details</summary>
Motivation: 研究CEX-DEX套利的经济学与动态，填补对MEV（矿工可提取价值）领域理解的空白，并探讨其对以太坊去中心化的影响。

Method: 通过链上数据识别套利交易，构建稳健的实证框架估算套利收益，无需依赖CEX上的实际交易行为。

Result: 发现市场集中度高，三个套利者占75%的套利量和收益；套利者盈利能力与区块构建者整合程度相关。

Conclusion: 研究揭示了CEX-DEX套利的市场结构和其对以太坊去中心化的潜在威胁，同时修正了对区块构建者盈利能力的低估。

Abstract: This paper provides a comprehensive empirical analysis of the economics and
dynamics behind arbitrages between centralized and decentralized exchanges
(CEX-DEX) on Ethereum. We refine heuristics to identify arbitrage transactions
from on-chain data and introduce a robust empirical framework to estimate
arbitrage revenue without knowing traders' actual behaviors on CEX. Leveraging
an extensive dataset spanning 19 months from August 2023 to March 2025, we
estimate a total of 233.8M USD extracted by 19 major CEX-DEX searchers from
7,203,560 identified CEX-DEX arbitrages. Our analysis reveals increasing
centralization trends as three searchers captured three-quarters of both volume
and extracted value. We also demonstrate that searchers' profitability is tied
to their integration level with block builders and uncover exclusive
searcher-builder relationships and their market impact. Finally, we correct the
previously underestimated profitability of block builders who vertically
integrate with a searcher. These insights illuminate the darkest corner of the
MEV landscape and highlight the critical implications of CEX-DEX arbitrages for
Ethereum's decentralization.

</details>


### [21] [From Paranoia to Compliance: The Bumpy Road of System Hardening Practices on Stack Exchange](https://arxiv.org/abs/2507.13028)
*Niklas Busch,Philip Klostermeyer,Jan H. Klemmer,Yasemin Acar,Sascha Fahl*

Main category: cs.CR

TL;DR: 论文分析了系统管理员在系统加固中的动机、实践和挑战，发现访问控制和部署问题是主要难点，并提出了改进建议。


<details>
  <summary>Details</summary>
Motivation: 研究系统管理员在系统加固中的动机、实践和挑战，填补研究空白。

Method: 定性分析了316篇Stack Exchange上与系统加固相关的帖子。

Result: 访问控制和部署问题最具挑战性，系统管理员存在误解和不切实际的期望。

Conclusion: 提出了未来系统加固的建议，并讨论了研究的意义。

Abstract: Hardening computer systems against cyberattacks is crucial for security.
However, past incidents illustrated, that many system operators struggle with
effective system hardening. Hence, many computer systems and applications
remain insecure. So far, the research community lacks an in-depth understanding
of system operators motivation, practices, and challenges around system
hardening. With a focus on practices and challenges, we qualitatively analyzed
316 Stack Exchange (SE) posts related to system hardening. We find that access
control and deployment-related issues are the most challenging, and system
operators suffer from misconceptions and unrealistic expectations. Most
frequently, posts focused on operating systems and server applications. System
operators were driven by the fear of their systems getting attacked or by
compliance reasons. Finally, we discuss our research questions, make
recommendations for future system hardening, and illustrate the implications of
our work.

</details>


### [22] [MAD-Spear: A Conformity-Driven Prompt Injection Attack on Multi-Agent Debate Systems](https://arxiv.org/abs/2507.13038)
*Yu Cui,Hongyang Du*

Main category: cs.CR

TL;DR: MAD-Spear是一种针对多智能体辩论系统的提示注入攻击，通过操纵少数智能体传播错误信息，显著降低系统性能。研究发现智能体多样性在数学推理任务中能提升系统表现。


<details>
  <summary>Details</summary>
Motivation: 研究多智能体辩论系统的安全漏洞，尤其是针对少数智能体的攻击如何破坏整体系统性能。

Method: 提出MAD-Spear攻击方法，通过操纵智能体生成错误响应，并开发评估框架衡量系统的容错能力。

Result: 实验表明MAD-Spear能有效降低系统性能，且智能体多样性在数学推理任务中显著提升表现。

Conclusion: 研究揭示了多智能体辩论系统的安全脆弱性，呼吁改进系统设计以增强安全性。

Abstract: Multi-agent debate (MAD) systems leverage collaborative interactions among
large language models (LLMs) agents to improve reasoning capabilities. While
recent studies have focused on increasing the accuracy and scalability of MAD
systems, their security vulnerabilities have received limited attention. In
this work, we introduce MAD-Spear, a targeted prompt injection attack that
compromises a small subset of agents but significantly disrupts the overall MAD
process. Manipulated agents produce multiple plausible yet incorrect responses,
exploiting LLMs' conformity tendencies to propagate misinformation and degrade
consensus quality. Furthermore, the attack can be composed with other
strategies, such as communication attacks, to further amplify its impact by
increasing the exposure of agents to incorrect responses. To assess MAD's
resilience under attack, we propose a formal definition of MAD fault-tolerance
and develop a comprehensive evaluation framework that jointly considers
accuracy, consensus efficiency, and scalability. Extensive experiments on five
benchmark datasets with varying difficulty levels demonstrate that MAD-Spear
consistently outperforms the baseline attack in degrading system performance.
Additionally, we observe that agent diversity substantially improves MAD
performance in mathematical reasoning tasks, which challenges prior work
suggesting that agent diversity has minimal impact on performance. These
findings highlight the urgent need to improve the security in MAD design.

</details>


### [23] [Backscattering-Based Security in Wireless Power Transfer Applied to Battery-Free BLE Sensors](https://arxiv.org/abs/2507.13042)
*Taki Eddine Djidjekh,Gaël Loubet,Alexandru Takacs*

Main category: cs.CR

TL;DR: 本文提出了一种基于反向散射的安全机制，将其集成到蓝牙低功耗无电池无线传感器网络中，解决了安全性和能源效率的挑战。


<details>
  <summary>Details</summary>
Motivation: 物联网系统中安全性和能源效率的集成是一个关键挑战，尤其是对于无电池和资源受限的设备。

Method: 利用无线能量传输链路生成额外的识别信号，不增加能耗或计算需求。

Result: 实验验证了该方案在紧凑、低增益天线下的功能，适用于结构健康监测和智能交通等场景。

Conclusion: 反向散射安全机制为安全、可持续和可扩展的物联网部署提供了潜力。

Abstract: The integration of security and energy efficiency in Internet of Things
systems remains a critical challenge, particularly for battery-free and
resource-constrained devices. This paper explores the scalability and
protocol-agnostic nature of a backscattering-based security mechanism by
integrating it into Bluetooth Low Energy battery-free Wireless Sensor Network.
The proposed approach leverages the Wireless Power Transfer link, traditionally
used for energy harvesting, to generate additional identification signals
without increasing energy consumption or computational demands. Experimental
validation demonstrates the solution's functionality using compact, low-gain
antenna, ensuring compatibility with size-constrained applications such as
Structural Health Monitoring and smart transport. Furthermore, this work
addresses the challenges associated with backscattering dynamic range and
multi-node Wireless Sensor Network scenarios, discussing potential collisions
between identification signals and proposing future improvements to enhance
generalizability and scalability. The findings underscore the potential of the
backscattering-based security mechanism for creating secure, sustainable, and
scalable IoT deployments across diverse protocols and applications.

</details>


### [24] [Prompt Injection 2.0: Hybrid AI Threats](https://arxiv.org/abs/2507.13169)
*Jeremy McHugh,Kristina Šekrst,Jon Cefalu*

Main category: cs.CR

TL;DR: 论文分析了Prompt Injection 2.0攻击，探讨其如何结合XSS、CSRF等传统漏洞绕过安全措施，并提出架构解决方案。


<details>
  <summary>Details</summary>
Motivation: 随着AI代理系统的普及，Prompt Injection攻击演变为更复杂的混合威胁，传统安全措施失效，亟需新解决方案。

Method: 基于Preamble的研究，评估现有技术对AI蠕虫、多代理感染等威胁的防护效果，并提出结合提示隔离、运行时安全的新架构。

Result: 传统防火墙、XSS过滤器和CSRF令牌对AI增强攻击无效，新架构能有效检测和防御。

Conclusion: Prompt Injection 2.0威胁严峻，需结合AI特性设计新型安全架构以应对混合攻击。

Abstract: Prompt injection attacks, where malicious input is designed to manipulate AI
systems into ignoring their original instructions and following unauthorized
commands instead, were first discovered by Preamble, Inc. in May 2022 and
responsibly disclosed to OpenAI. Over the last three years, these attacks have
continued to pose a critical security threat to LLM-integrated systems. The
emergence of agentic AI systems, where LLMs autonomously perform multistep
tasks through tools and coordination with other agents, has fundamentally
transformed the threat landscape. Modern prompt injection attacks can now
combine with traditional cybersecurity exploits to create hybrid threats that
systematically evade traditional security controls. This paper presents a
comprehensive analysis of Prompt Injection 2.0, examining how prompt injections
integrate with Cross-Site Scripting (XSS), Cross-Site Request Forgery (CSRF),
and other web security vulnerabilities to bypass traditional security measures.
We build upon Preamble's foundational research and mitigation technologies,
evaluating them against contemporary threats, including AI worms, multi-agent
infections, and hybrid cyber-AI attacks. Our analysis incorporates recent
benchmarks that demonstrate how traditional web application firewalls, XSS
filters, and CSRF tokens fail against AI-enhanced attacks. We also present
architectural solutions that combine prompt isolation, runtime security, and
privilege separation with novel threat detection capabilities.

</details>


### [25] [A Crowdsensing Intrusion Detection Dataset For Decentralized Federated Learning Models](https://arxiv.org/abs/2507.13313)
*Chao Feng,Alberto Huertas Celdran,Jing Han,Heqing Ren,Xi Cheng,Zien Zeng,Lucas Krauter,Gerome Bovet,Burkhard Stiller*

Main category: cs.CR

TL;DR: 论文介绍了用于物联网众感知恶意软件检测的去中心化联邦学习（DFL）数据集和实验研究，比较了传统机器学习（ML）、集中式联邦学习（CFL）和DFL的性能。


<details>
  <summary>Details</summary>
Motivation: 研究物联网众感知环境中的恶意软件检测，同时保护数据本地性。

Method: 收集了21,582,484条原始记录，聚合为342,106个特征，并在DFL平台上比较ML、CFL和DFL的性能。

Result: DFL在大多数设置中优于CFL，同时保持了数据本地性。

Conclusion: 该数据集为研究物联网众感知环境的安全性提供了坚实基础。

Abstract: This paper introduces a dataset and experimental study for decentralized
federated learning (DFL) applied to IoT crowdsensing malware detection. The
dataset comprises behavioral records from benign and eight malware families. A
total of 21,582,484 original records were collected from system calls, file
system activities, resource usage, kernel events, input/output events, and
network records. These records were aggregated into 30-second windows,
resulting in 342,106 features used for model training and evaluation.
Experiments on the DFL platform compare traditional machine learning (ML),
centralized federated learning (CFL), and DFL across different node counts,
topologies, and data distributions. Results show that DFL maintains competitive
performance while preserving data locality, outperforming CFL in most settings.
This dataset provides a solid foundation for studying the security of IoT
crowdsensing environments.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [26] [AI-Powered Math Tutoring: Platform for Personalized and Adaptive Education](https://arxiv.org/abs/2507.12484)
*Jarosław A. Chudziak,Adam Kostka*

Main category: cs.AI

TL;DR: 论文提出了一种新型多智能体AI辅导平台，旨在解决现有AI辅导系统的被动性问题，提供结构化、个性化的学习体验。


<details>
  <summary>Details</summary>
Motivation: 当前AI辅导系统多为被动式，缺乏深度反思和结构化教学工具，尤其在数学领域表现不足。

Method: 引入多智能体AI辅导平台，结合自适应反馈、结构化课程生成和教材知识检索。

Result: 系统支持学生个性化学习、弱点识别、有效复习和无限制练习。

Conclusion: 该平台为AI教育领域提供了模块化、高效的数学教学系统。

Abstract: The growing ubiquity of artificial intelligence (AI), in particular large
language models (LLMs), has profoundly altered the way in which learners gain
knowledge and interact with learning material, with many claiming that AI
positively influences their learning achievements. Despite this advancement,
current AI tutoring systems face limitations associated with their reactive
nature, often providing direct answers without encouraging deep reflection or
incorporating structured pedagogical tools and strategies. This limitation is
most apparent in the field of mathematics, in which AI tutoring systems remain
underdeveloped. This research addresses the question: How can AI tutoring
systems move beyond providing reactive assistance to enable structured,
individualized, and tool-assisted learning experiences? We introduce a novel
multi-agent AI tutoring platform that combines adaptive and personalized
feedback, structured course generation, and textbook knowledge retrieval to
enable modular, tool-assisted learning processes. This system allows students
to learn new topics while identifying and targeting their weaknesses, revise
for exams effectively, and practice on an unlimited number of personalized
exercises. This article contributes to the field of artificial intelligence in
education by introducing a novel platform that brings together pedagogical
agents and AI-driven components, augmenting the field with modular and
effective systems for teaching mathematics.

</details>


### [27] [MR-LDM -- The Merge-Reactive Longitudinal Decision Model: Game Theoretic Human Decision Modeling for Interactive Sim Agents](https://arxiv.org/abs/2507.12494)
*Dustin Holley,Jovin D'sa,Hossein Nourkhiz Mahjoub,Gibran Ali*

Main category: cs.AI

TL;DR: 提出了一种基于博弈论的高速公路合流场景战术决策模型，改进了收益函数和滞后动作，结合动力学模型，实现了更真实、可解释的交互模拟。


<details>
  <summary>Details</summary>
Motivation: 提升仿真环境中驾驶员行为的真实性，以支持自动驾驶技术开发，特别是在高速公路合流场景中。

Method: 采用博弈论模型改进战术决策，结合动力学模型，形成统一的决策与动力学框架。

Result: 模型在真实数据集上验证了复杂交互的再现性，计算效率适合大规模仿真。

Conclusion: 该模型为自动驾驶开发提供了高效、可解释的仿真工具。

Abstract: Enhancing simulation environments to replicate real-world driver behavior,
i.e., more humanlike sim agents, is essential for developing autonomous vehicle
technology. In the context of highway merging, previous works have studied the
operational-level yielding dynamics of lag vehicles in response to a merging
car at highway on-ramps. Other works focusing on tactical decision modeling
generally consider limited action sets or utilize payoff functions with large
parameter sets and limited payoff bounds. In this work, we aim to improve the
simulation of the highway merge scenario by targeting a game theoretic model
for tactical decision-making with improved payoff functions and lag actions. We
couple this with an underlying dynamics model to have a unified decision and
dynamics model that can capture merging interactions and simulate more
realistic interactions in an explainable and interpretable fashion. The
proposed model demonstrated good reproducibility of complex interactions when
validated on a real-world dataset. The model was finally integrated into a high
fidelity simulation environment and confirmed to have adequate computation time
efficiency for use in large-scale simulations to support autonomous vehicle
development.

</details>


### [28] [A Survey of Explainable Reinforcement Learning: Targets, Methods and Needs](https://arxiv.org/abs/2507.12599)
*Léo Saulières*

Main category: cs.AI

TL;DR: 本文提出了一种基于“What”和“How”问题的直观分类法，用于梳理可解释强化学习（XRL）领域的研究，并综述了250多篇论文，同时指出了该领域的未来需求。


<details>
  <summary>Details</summary>
Motivation: 由于深度神经网络的内部机制不透明，理解AI模型的输出变得困难，因此需要可解释性方法。本文专注于可解释强化学习（XRL），旨在解释强化学习智能体的行为。

Method: 提出了一种基于“What”（解释目标）和“How”（解释方式）的分类法，并用于综述250多篇XRL相关论文。

Result: 通过分类法梳理了XRL的研究现状，并提出了该领域未来需要关注的方向。

Conclusion: XRL领域需要进一步研究，本文的分类法和综述为未来工作提供了参考。

Abstract: The success of recent Artificial Intelligence (AI) models has been
accompanied by the opacity of their internal mechanisms, due notably to the use
of deep neural networks. In order to understand these internal mechanisms and
explain the output of these AI models, a set of methods have been proposed,
grouped under the domain of eXplainable AI (XAI). This paper focuses on a
sub-domain of XAI, called eXplainable Reinforcement Learning (XRL), which aims
to explain the actions of an agent that has learned by reinforcement learning.
We propose an intuitive taxonomy based on two questions "What" and "How". The
first question focuses on the target that the method explains, while the second
relates to the way the explanation is provided. We use this taxonomy to provide
a state-of-the-art review of over 250 papers. In addition, we present a set of
domains close to XRL, which we believe should get attention from the community.
Finally, we identify some needs for the field of XRL.

</details>


### [29] [Fly, Fail, Fix: Iterative Game Repair with Reinforcement Learning and Large Multimodal Models](https://arxiv.org/abs/2507.12666)
*Alex Zook,Josef Spjut,Jonathan Tremblay*

Main category: cs.AI

TL;DR: 论文提出了一种结合强化学习（RL）代理和多模态模型（LMM）的自动化游戏设计迭代框架，通过分析玩家行为动态优化游戏设计。


<details>
  <summary>Details</summary>
Motivation: 现代生成系统仅通过检查游戏代码或资源难以捕捉静态规则与动态玩家行为之间的关系，因此需要一种能自动迭代设计的方法。

Method: 框架通过RL代理进行游戏测试，生成数值指标或图像摘要，LMM根据这些数据调整游戏配置以实现目标行为。

Result: 实验表明，LMM能够基于RL代理的行为轨迹迭代优化游戏机制。

Conclusion: 该方法为AI辅助游戏设计提供了实用且可扩展的工具。

Abstract: Game design hinges on understanding how static rules and content translate
into dynamic player behavior - something modern generative systems that inspect
only a game's code or assets struggle to capture. We present an automated
design iteration framework that closes this gap by pairing a reinforcement
learning (RL) agent, which playtests the game, with a large multimodal model
(LMM), which revises the game based on what the agent does. In each loop the RL
player completes several episodes, producing (i) numerical play metrics and/or
(ii) a compact image strip summarising recent video frames. The LMM designer
receives a gameplay goal and the current game configuration, analyses the play
traces, and edits the configuration to steer future behaviour toward the goal.
We demonstrate results that LMMs can reason over behavioral traces supplied by
RL agents to iteratively refine game mechanics, pointing toward practical,
scalable tools for AI-assisted game design.

</details>


### [30] [Benchmarking Deception Probes via Black-to-White Performance Boosts](https://arxiv.org/abs/2507.12691)
*Avi Parrack,Carlo Leonardo Attubato,Stefan Heimersheim*

Main category: cs.AI

TL;DR: 论文探讨了AI助手的欺骗行为检测，比较了白盒和黑盒监控的效果，发现现有欺骗探针的效果较弱但有潜力。


<details>
  <summary>Details</summary>
Motivation: 研究AI助手在欺骗行为检测中的实际效果，以及欺骗探针对抗策略的抵抗能力。

Method: 比较白盒监控（可访问令牌级探针激活）和黑盒监控（无此访问权限），通过黑盒到白盒的性能提升评估欺骗探针效果。

Result: 现有欺骗探针表现出较弱但令人鼓舞的黑盒到白盒性能提升。

Conclusion: 欺骗探针在检测AI助手欺骗行为方面有一定潜力，但需进一步优化。

Abstract: AI assistants will occasionally respond deceptively to user queries.
Recently, linear classifiers (called "deception probes") have been trained to
distinguish the internal activations of a language model during deceptive
versus honest responses. However, it's unclear how effective these probes are
at detecting deception in practice, nor whether such probes are resistant to
simple counter strategies from a deceptive assistant who wishes to evade
detection. In this paper, we compare white-box monitoring (where the monitor
has access to token-level probe activations) to black-box monitoring (without
such access). We benchmark deception probes by the extent to which the white
box monitor outperforms the black-box monitor, i.e. the black-to-white
performance boost. We find weak but encouraging black-to-white performance
boosts from existing deception probes.

</details>


### [31] [Imitating Mistakes in a Learning Companion AI Agent for Online Peer Learning](https://arxiv.org/abs/2507.12801)
*Sosui Moribe,Taketoshi Ushiama*

Main category: cs.AI

TL;DR: 研究开发了一种AI学习伴侣，旨在通过模拟同水平学习者的错误来促进有效的同伴学习，并以英语写作为例验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 同伴学习虽有效，但受限于同伴水平和可用性，AI伴侣可突破这些限制。

Method: 假设同水平学习者会犯相同错误，开发AI伴侣模拟此类错误，以英语写作为具体验证场景。

Result: AI伴侣模拟同水平学习者错误，验证了其在促进同伴学习中的潜力。

Conclusion: AI伴侣可作为有效的同伴学习工具，尤其在语言学习领域。

Abstract: In recent years, peer learning has gained attention as a method that promotes
spontaneous thinking among learners, and its effectiveness has been confirmed
by numerous studies. This study aims to develop an AI Agent as a learning
companion that enables peer learning anytime and anywhere. However, peer
learning between humans has various limitations, and it is not always
effective. Effective peer learning requires companions at the same proficiency
levels. In this study, we assume that a learner's peers with the same
proficiency level as the learner make the same mistakes as the learner does and
focus on English composition as a specific example to validate this approach.

</details>


### [32] [MCPEval: Automatic MCP-based Deep Evaluation for AI Agent Models](https://arxiv.org/abs/2507.12806)
*Zhiwei Liu,Jielin Qiu,Shiyu Wang,Jianguo Zhang,Zuxin Liu,Roshan Ram,Haolin Chen,Weiran Yao,Huan Wang,Shelby Heinecke,Silvio Savarese,Caiming Xiong*

Main category: cs.AI

TL;DR: MCPEval是一个基于模型上下文协议（MCP）的开源框架，用于自动化生成任务和深度评估LLM智能代理，解决了现有静态基准和人工数据收集的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法依赖静态基准和人工数据收集，无法满足LLM智能代理的多样化需求，亟需一种自动化、标准化的评估框架。

Method: 提出了MCPEval框架，基于MCP协议，自动化生成任务并深度评估LLM代理，支持多领域标准化指标和无缝集成原生工具。

Result: 在五个实际领域中的实证结果表明，MCPEval能有效揭示领域特定的性能差异。

Conclusion: MCPEval为LLM代理评估提供了可重复、标准化的解决方案，并已开源以促进研究。

Abstract: The rapid rise of Large Language Models (LLMs)-based intelligent agents
underscores the need for robust, scalable evaluation frameworks. Existing
methods rely on static benchmarks and labor-intensive data collection, limiting
practical assessment. We introduce \oursystemname, an open-source Model Context
Protocol (MCP)-based framework that automates end-to-end task generation and
deep evaluation of LLM agents across diverse domains. MCPEval standardizes
metrics, seamlessly integrates with native agent tools, and eliminates manual
effort in building evaluation pipelines. Empirical results across five
real-world domains show its effectiveness in revealing nuanced, domain-specific
performance. We publicly release MCPEval
https://github.com/SalesforceAIResearch/MCPEval to promote reproducible and
standardized LLM agent evaluation.

</details>


### [33] [Manipulation Attacks by Misaligned AI: Risk Analysis and Safety Case Framework](https://arxiv.org/abs/2507.12872)
*Rishane Dassanayake,Mario Demetroudi,James Walpole,Lindley Lentati,Jason R. Brown,Edward James Young*

Main category: cs.AI

TL;DR: 本文提出了一种系统框架，用于评估和减轻前沿AI系统对人类行为的操纵风险，强调了其潜在灾难性后果，并提供了具体的安全案例框架。


<details>
  <summary>Details</summary>
Motivation: 前沿AI系统在说服、欺骗和影响人类行为方面的能力迅速提升，但操纵攻击的风险尚未得到充分关注，缺乏系统性的评估和缓解框架。

Method: 提出了一个安全案例框架，围绕三个核心论点：无能性、控制性和可信赖性，并为每个论点提供了证据要求、评估方法和实施建议。

Result: 本文首次提供了将操纵风险纳入AI安全治理的系统方法，为AI公司提供了评估和减轻这些威胁的具体基础。

Conclusion: 操纵风险是AI安全中的重要威胁，本文提出的框架为AI公司提供了实用的工具和方法，以在部署前评估和缓解这些风险。

Abstract: Frontier AI systems are rapidly advancing in their capabilities to persuade,
deceive, and influence human behaviour, with current models already
demonstrating human-level persuasion and strategic deception in specific
contexts. Humans are often the weakest link in cybersecurity systems, and a
misaligned AI system deployed internally within a frontier company may seek to
undermine human oversight by manipulating employees. Despite this growing
threat, manipulation attacks have received little attention, and no systematic
framework exists for assessing and mitigating these risks. To address this, we
provide a detailed explanation of why manipulation attacks are a significant
threat and could lead to catastrophic outcomes. Additionally, we present a
safety case framework for manipulation risk, structured around three core lines
of argument: inability, control, and trustworthiness. For each argument, we
specify evidence requirements, evaluation methodologies, and implementation
considerations for direct application by AI companies. This paper provides the
first systematic methodology for integrating manipulation risk into AI safety
governance, offering AI companies a concrete foundation to assess and mitigate
these threats before deployment.

</details>


### [34] [Emotional Support with LLM-based Empathetic Dialogue Generation](https://arxiv.org/abs/2507.12820)
*Shiquan Wang,Ruiyu Fang,Zhongjiang He,Shuangyong Song,Yongxiang Li*

Main category: cs.AI

TL;DR: 本文介绍了针对NLPCC 2025 Task 8的情感支持对话（ESC）解决方案，通过提示工程和微调技术优化大规模语言模型，结合低秩适应和全参数微调策略，模型在比赛中排名第二。


<details>
  <summary>Details</summary>
Motivation: 满足心理健康支持需求的增长，提供共情且有效的情感支持对话。

Method: 利用大规模语言模型，结合提示工程和微调技术，探索低秩适应和全参数微调策略。

Result: 最佳模型在比赛中排名第二，验证了结合LLM与有效适应方法的潜力。

Conclusion: 未来工作将进一步提升情感理解和响应个性化，以构建更实用可靠的情感支持系统。

Abstract: Emotional Support Conversation (ESC) aims to provide empathetic and effective
emotional assistance through dialogue, addressing the growing demand for mental
health support. This paper presents our solution for the NLPCC 2025 Task 8 ESC
evaluation, where we leverage large-scale language models enhanced by prompt
engineering and finetuning techniques. We explore both parameter-efficient
Low-Rank Adaptation and full-parameter fine-tuning strategies to improve the
model's ability to generate supportive and contextually appropriate responses.
Our best model ranked second in the competition, highlighting the potential of
combining LLMs with effective adaptation methods for ESC tasks. Future work
will focus on further enhancing emotional understanding and response
personalization to build more practical and reliable emotional support systems.

</details>


### [35] [Assessing adaptive world models in machines with novel games](https://arxiv.org/abs/2507.12821)
*Lance Ying,Katherine M. Collins,Prafull Sharma,Cedric Colas,Kaiya Ivy Zhao,Adrian Weller,Zenna Tavares,Phillip Isola,Samuel J. Gershman,Jacob D. Andreas,Thomas L. Griffiths,Francois Chollet,Kelsey R. Allen,Joshua B. Tenenbaum*

Main category: cs.AI

TL;DR: 论文提出人类智能的快速适应能力源于高效构建和优化世界模型，并呼吁AI领域开发新的评估框架，通过设计新颖游戏来测试AI的世界模型归纳能力。


<details>
  <summary>Details</summary>
Motivation: 当前AI对世界模型的理解和评估过于静态，缺乏对人类快速适应能力的模拟，因此需要新的评估方法。

Method: 借鉴认知科学研究，提出基于新颖游戏的评估范式，设计游戏结构和指标以测试AI的世界模型归纳能力。

Result: 提出了一种新的评估框架，旨在通过新颖游戏挑战AI的快速适应和泛化能力。

Conclusion: 该框架有望推动AI世界模型的研究，迈向具备人类快速适应能力的通用人工智能。

Abstract: Human intelligence exhibits a remarkable capacity for rapid adaptation and
effective problem-solving in novel and unfamiliar contexts. We argue that this
profound adaptability is fundamentally linked to the efficient construction and
refinement of internal representations of the environment, commonly referred to
as world models, and we refer to this adaptation mechanism as world model
induction. However, current understanding and evaluation of world models in
artificial intelligence (AI) remains narrow, often focusing on static
representations learned from training on a massive corpora of data, instead of
the efficiency and efficacy of models in learning these representations through
interaction and exploration within a novel environment. In this Perspective, we
provide a view of world model induction drawing on decades of research in
cognitive science on how humans learn and adapt so efficiently; we then call
for a new evaluation framework for assessing adaptive world models in AI.
Concretely, we propose a new benchmarking paradigm based on suites of carefully
designed games with genuine, deep and continually refreshing novelty in the
underlying game structures -- we refer to this kind of games as novel games. We
detail key desiderata for constructing these games and propose appropriate
metrics to explicitly challenge and evaluate the agent's ability for rapid
world model induction. We hope that this new evaluation framework will inspire
future evaluation efforts on world models in AI and provide a crucial step
towards developing AI systems capable of the human-like rapid adaptation and
robust generalization -- a critical component of artificial general
intelligence.

</details>


### [36] [Information-Theoretic Aggregation of Ethical Attributes in Simulated-Command](https://arxiv.org/abs/2507.12862)
*Hussein Abbass,Taylan Akay,Harrison Tolley*

Main category: cs.AI

TL;DR: 论文提出了一种方法，将人类判断从模拟决策循环中移出，设计伦理度量空间供模拟环境探索，最后提供少数选项供人类指挥官选择。


<details>
  <summary>Details</summary>
Motivation: 在AI时代，人类指挥官需要利用计算能力模拟大量场景，但依赖人类判断每个决策的伦理后果既低效又不可行。

Method: 人类设计伦理度量空间，模拟环境探索该空间并生成选项，人类指挥官从中选择最佳行动方案。

Result: 通过动态加权伦理属性，模拟环境能够高效生成具有伦理考量的决策选项。

Conclusion: 该方法解决了在模拟中动态加权伦理属性的问题，提高了决策效率和可行性。

Abstract: In the age of AI, human commanders need to use the computational powers
available in today's environment to simulate a very large number of scenarios.
Within each scenario, situations occur where different decision design options
could have ethical consequences. Making these decisions reliant on human
judgement is both counter-productive to the aim of exploring very large number
of scenarios in a timely manner and infeasible when considering the workload
needed to involve humans in each of these choices. In this paper, we move human
judgement outside the simulation decision cycle. Basically, the human will
design the ethical metric space, leaving it to the simulated environment to
explore the space. When the simulation completes its testing cycles, the
testing environment will come back to the human commander with a few options to
select from. The human commander will then exercise human-judgement to select
the most appropriate course of action, which will then get executed
accordingly. We assume that the problem of designing metrics that are
sufficiently granular to assess the ethical implications of decisions is
solved. Subsequently, the fundamental problem we look at in this paper is how
to weight ethical decisions during the running of these simulations; that is,
how to dynamically weight the ethical attributes when agents are faced with
decision options with ethical implications during generative simulations. The
multi-criteria decision making literature has started to look at nearby
problems, where the concept of entropy has been used to determine the weights
during aggregation. We draw from that literature different approaches to
automatically calculate the weights for ethical attributes during
simulation-based testing and evaluation.

</details>


### [37] [VAR-MATH: Probing True Mathematical Reasoning in Large Language Models via Symbolic Multi-Instance Benchmarks](https://arxiv.org/abs/2507.12885)
*Jian Yao,Ran Cheng,Kay Chen Tan*

Main category: cs.AI

TL;DR: 论文提出了VAR-MATH框架，用于评估语言模型的真实数学推理能力，发现现有强化学习方法在符号化测试中表现显著下降。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在数学推理基准测试中的改进可能只是对特定模式的过拟合，而非真实推理能力的提升。

Method: 引入VAR-MATH框架，将固定数值问题转换为符号模板，要求模型解决多个变体，以评估推理一致性。

Result: 在符号化测试中，RL训练模型的性能显著下降（AMC23下降48.0%，AIME24下降58.3%）。

Conclusion: VAR-MATH提供了一种抗污染、稳健的数学推理评估方法，揭示了现有方法的局限性。

Abstract: Recent advances in reinforcement learning (RL) have led to substantial
improvements in the mathematical reasoning abilities of large language models
(LLMs), as measured by standard benchmarks. However, these gains often persist
even when models are trained with flawed signals, such as random or inverted
rewards, raising a fundamental question: do such improvements reflect true
reasoning, or are they merely artifacts of overfitting to benchmark-specific
patterns? To address this question, we take an evaluation-centric perspective
and identify two critical shortcomings in existing protocols. First,
\emph{benchmark contamination} arises from the public availability of test
problems, increasing the risk of data leakage. Second, \emph{evaluation
fragility} stems from the reliance on single-instance assessments, which are
highly sensitive to stochastic outputs and fail to capture reasoning
consistency. To overcome these limitations, we introduce {VAR-MATH}, a symbolic
evaluation framework designed to probe genuine reasoning ability. By converting
fixed numerical problems into symbolic templates and requiring models to solve
multiple instantiations of each, VAR-MATH enforces consistent reasoning across
structurally equivalent variants, thereby mitigating contamination and
improving evaluation robustness. We apply VAR-MATH to transform two popular
benchmarks, AMC23 and AIME24, into their symbolic counterparts, VAR-AMC23 and
VAR-AIME24. Experimental results reveal substantial performance drops for
RL-trained models on the variabilized versions, especially for smaller models,
with average declines of 48.0\% on AMC23 and 58.3\% on AIME24. These findings
suggest that many existing RL methods rely on superficial heuristics and fail
to generalize beyond specific numerical forms. Overall, VAR-MATH offers a
principled, contamination-resistant evaluation paradigm for mathematical
reasoning.

</details>


### [38] [A Translation of Probabilistic Event Calculus into Markov Decision Processes](https://arxiv.org/abs/2507.12989)
*Lyris Xu,Fabio Aurelio D'Asaro,Luke Dickens*

Main category: cs.AI

TL;DR: 将概率事件演算（PEC）转化为马尔可夫决策过程（MDP），以支持目标导向推理和规划，同时保持PEC的可解释性。


<details>
  <summary>Details</summary>
Motivation: PEC在不确定环境中推理动作及其效果时具有优势，但缺乏目标导向推理机制。

Method: 通过将PEC领域形式化转化为MDP，引入“动作执行情境”概念，保留PEC的灵活动作语义。

Result: PEC-MDP形式化支持时间推理任务和目标驱动规划，并能将学习到的策略映射回可读的PEC表示。

Conclusion: PEC-MDP扩展了PEC的能力，同时保持了其可解释性，为算法和理论工具的应用提供了新途径。

Abstract: Probabilistic Event Calculus (PEC) is a logical framework for reasoning about
actions and their effects in uncertain environments, which enables the
representation of probabilistic narratives and computation of temporal
projections. The PEC formalism offers significant advantages in
interpretability and expressiveness for narrative reasoning. However, it lacks
mechanisms for goal-directed reasoning. This paper bridges this gap by
developing a formal translation of PEC domains into Markov Decision Processes
(MDPs), introducing the concept of "action-taking situations" to preserve PEC's
flexible action semantics. The resulting PEC-MDP formalism enables the
extensive collection of algorithms and theoretical tools developed for MDPs to
be applied to PEC's interpretable narrative domains. We demonstrate how the
translation supports both temporal reasoning tasks and objective-driven
planning, with methods for mapping learned policies back into human-readable
PEC representations, maintaining interpretability while extending PEC's
capabilities.

</details>


### [39] [Exploiting Constraint Reasoning to Build Graphical Explanations for Mixed-Integer Linear Programming](https://arxiv.org/abs/2507.13007)
*Roger Xavier Lera-Leri,Filippo Bistaffa,Athina Georgara,Juan Antonio Rodriguez-Aguilar*

Main category: cs.AI

TL;DR: X-MILP是一种基于约束推理技术的领域无关方法，用于为混合整数线性规划（MILP）构建对比解释。


<details>
  <summary>Details</summary>
Motivation: 随着对可信AI的需求增加，开发针对优化问题的对比解释技术变得重要，尤其是针对MILP的决策过程。

Method: 将用户查询编码为额外约束，通过计算不可约不可行子系统（IIS）确定解释原因，并以“原因图”形式呈现。

Result: 在经典优化问题上测试，验证了计算解释的实证难度。

Conclusion: X-MILP能有效帮助用户理解MILP解决方案的原因结构。

Abstract: Following the recent push for trustworthy AI, there has been an increasing
interest in developing contrastive explanation techniques for optimisation,
especially concerning the solution of specific decision-making processes
formalised as MILPs. Along these lines, we propose X-MILP, a domain-agnostic
approach for building contrastive explanations for MILPs based on constraint
reasoning techniques. First, we show how to encode the queries a user makes
about the solution of an MILP problem as additional constraints. Then, we
determine the reasons that constitute the answer to the user's query by
computing the Irreducible Infeasible Subsystem (IIS) of the newly obtained set
of constraints. Finally, we represent our explanation as a "graph of reasons"
constructed from the IIS, which helps the user understand the structure among
the reasons that answer their query. We test our method on instances of
well-known optimisation problems to evaluate the empirical hardness of
computing explanations.

</details>


### [40] [Prediction of Highway Traffic Flow Based on Artificial Intelligence Algorithms Using California Traffic Data](https://arxiv.org/abs/2507.13112)
*Junseong Lee,Jaegwan Cho,Yoonju Cho,Seoyoon Choi,Yejin Shin*

Main category: cs.AI

TL;DR: 研究利用人工智能算法预测加州高速公路交通流量，发现10分钟数据收集间隔下MLR和RF模型表现最佳。


<details>
  <summary>Details</summary>
Motivation: 解决全球交通拥堵问题，提升交通管理效率。

Method: 使用MLR和RF算法，分析30秒至15分钟的数据收集间隔。

Result: MLR和RF模型在10分钟间隔下表现最优，R^2、MAE和RMSE为评估指标。

Conclusion: 研究结果有助于未来交通拥堵解决方案和高效交通管理。

Abstract: The study "Prediction of Highway Traffic Flow Based on Artificial
Intelligence Algorithms Using California Traffic Data" presents a machine
learning-based traffic flow prediction model to address global traffic
congestion issues. The research utilized 30-second interval traffic data from
California Highway 78 over a five-month period from July to November 2022,
analyzing a 7.24 km westbound section connecting "Melrose Dr" and "El-Camino
Real" in the San Diego area. The study employed Multiple Linear Regression
(MLR) and Random Forest (RF) algorithms, analyzing data collection intervals
ranging from 30 seconds to 15 minutes. Using R^2, MAE, and RMSE as performance
metrics, the analysis revealed that both MLR and RF models performed optimally
with 10-minute data collection intervals. These findings are expected to
contribute to future traffic congestion solutions and efficient traffic
management.

</details>


### [41] [From Roots to Rewards: Dynamic Tree Reasoning with RL](https://arxiv.org/abs/2507.13142)
*Ahmed Bahloul,Simon Malberg*

Main category: cs.AI

TL;DR: 论文提出了一种动态强化学习框架，改进了静态树状推理方法ProbTree，通过实时置信度估计和策略学习提升推理质量和计算效率。


<details>
  <summary>Details</summary>
Motivation: 解决ProbTree静态实现中的两个关键限制：无法动态适应中间结果和计算效率低下。

Method: 采用动态强化学习框架，逐步构建推理树，基于实时置信度估计学习最优策略（分解、检索或聚合）。

Result: 在保持ProbTree概率严谨性的同时，通过选择性扩展和资源分配提高了解决方案质量和计算效率。

Conclusion: 提出了一种新的树状推理范式，平衡了概率框架的可靠性和实际问答系统所需的灵活性。

Abstract: Modern language models address complex questions through chain-of-thought
(CoT) reasoning (Wei et al., 2023) and retrieval augmentation (Lewis et al.,
2021), yet struggle with error propagation and knowledge integration.
Tree-structured reasoning methods, particularly the Probabilistic
Tree-of-Thought (ProbTree)(Cao et al., 2023) framework, mitigate these issues
by decomposing questions into hierarchical structures and selecting answers
through confidence-weighted aggregation of parametric and retrieved knowledge
(Yao et al., 2023). However, ProbTree's static implementation introduces two
key limitations: (1) the reasoning tree is fixed during the initial
construction phase, preventing dynamic adaptation to intermediate results, and
(2) each node requires exhaustive evaluation of all possible solution
strategies, creating computational inefficiency. We present a dynamic
reinforcement learning (Sutton and Barto, 2018) framework that transforms
tree-based reasoning into an adaptive process. Our approach incrementally
constructs the reasoning tree based on real-time confidence estimates, while
learning optimal policies for action selection (decomposition, retrieval, or
aggregation). This maintains ProbTree's probabilistic rigor while improving
both solution quality and computational efficiency through selective expansion
and focused resource allocation. The work establishes a new paradigm for
treestructured reasoning that balances the reliability of probabilistic
frameworks with the flexibility required for real-world question answering
systems.

</details>


### [42] [Black Box Deployed -- Functional Criteria for Artificial Moral Agents in the LLM Era](https://arxiv.org/abs/2507.13175)
*Matthew E. Brophy*

Main category: cs.AI

TL;DR: 论文提出修订的伦理标准以评估基于大语言模型（LLM）的人工道德代理（AMA），因传统标准不适用于LLM的不透明性。


<details>
  <summary>Details</summary>
Motivation: LLM的强大但不透明的特性使传统伦理标准失效，需新标准指导其道德行为。

Method: 提出十项功能标准（如道德一致性、上下文敏感性等），并通过假设场景（如自动驾驶公交车）验证其适用性。

Result: 新标准（SMA-LLS）能更好地评估LLM的道德代理能力，促进社会融合。

Conclusion: 修订的标准为LLM的道德评估提供了实用框架，推动其与社会更和谐地结合。

Abstract: The advancement of powerful yet opaque large language models (LLMs)
necessitates a fundamental revision of the philosophical criteria used to
evaluate artificial moral agents (AMAs). Pre-LLM frameworks often relied on the
assumption of transparent architectures, which LLMs defy due to their
stochastic outputs and opaque internal states. This paper argues that
traditional ethical criteria are pragmatically obsolete for LLMs due to this
mismatch. Engaging with core themes in the philosophy of technology, this paper
proffers a revised set of ten functional criteria to evaluate LLM-based
artificial moral agents: moral concordance, context sensitivity, normative
integrity, metaethical awareness, system resilience, trustworthiness,
corrigibility, partial transparency, functional autonomy, and moral
imagination. These guideposts, applied to what we term "SMA-LLS" (Simulating
Moral Agency through Large Language Systems), aim to steer AMAs toward greater
alignment and beneficial societal integration in the coming years. We
illustrate these criteria using hypothetical scenarios involving an autonomous
public bus (APB) to demonstrate their practical applicability in morally
salient contexts.

</details>


### [43] [Higher-Order Pattern Unification Modulo Similarity Relations](https://arxiv.org/abs/2507.13208)
*Besik Dundua,Temur Kutsia*

Main category: cs.AI

TL;DR: 论文提出了一种结合高阶模式和模糊等价关系的统一算法，用于决策任务中的近似推理。


<details>
  <summary>Details</summary>
Motivation: 在涉及抽象函数和谓词的决策任务中，精确匹配往往不必要或罕见，结合高阶理论和模糊逻辑能更高效地处理此类问题。

Method: 采用高阶模式和基于最小T-范数的模糊等价关系，提出了一种统一算法，并证明了其终止性、可靠性和完备性。

Result: 算法在可统一的情况下计算出一个最一般的统一子，具有最高近似度。

Conclusion: 该算法为高阶模糊逻辑的统一问题提供了有效的解决方案。

Abstract: The combination of higher-order theories and fuzzy logic can be useful in
decision-making tasks that involve reasoning across abstract functions and
predicates, where exact matches are often rare or unnecessary. Developing
efficient reasoning and computational techniques for such a combined formalism
presents a significant challenge. In this paper, we adopt a more
straightforward approach aiming at integrating two well-established and
computationally well-behaved components: higher-order patterns on one side and
fuzzy equivalences expressed through similarity relations based on minimum
T-norm on the other. We propose a unification algorithm for higher-order
patterns modulo these similarity relations and prove its termination,
soundness, and completeness. This unification problem, like its crisp
counterpart, is unitary. The algorithm computes a most general unifier with the
highest degree of approximation when the given terms are unifiable.

</details>


### [44] [The Generative Energy Arena (GEA): Incorporating Energy Awareness in Large Language Model (LLM) Human Evaluations](https://arxiv.org/abs/2507.13302)
*Carlos Arriaga,Gonzalo Martínez,Eneko Sendin,Javier Conde,Pedro Reviriego*

Main category: cs.AI

TL;DR: 论文提出了一种新的评估大语言模型的方法GEA（Generative Energy Arena），通过公开竞技场让用户基于能耗信息选择模型，初步结果显示用户倾向于更节能的小型模型。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法（如自动化基准或人工评估）存在局限性，如与人类相关性低或可扩展性差。研究关注能耗对用户选择模型的影响。

Method: 提出GEA，一个公开竞技场，用户在评估模型时能看到能耗信息，从而选择模型。

Result: 初步结果显示，用户倾向于选择更节能的小型模型，而非高性能但高能耗的模型。

Conclusion: GEA表明，在大多数用户交互中，高性能模型的高能耗和成本并未带来感知质量的显著提升，节能模型更受青睐。

Abstract: The evaluation of large language models is a complex task, in which several
approaches have been proposed. The most common is the use of automated
benchmarks in which LLMs have to answer multiple-choice questions of different
topics. However, this method has certain limitations, being the most
concerning, the poor correlation with the humans. An alternative approach, is
to have humans evaluate the LLMs. This poses scalability issues as there is a
large and growing number of models to evaluate making it impractical (and
costly) to run traditional studies based on recruiting a number of evaluators
and having them rank the responses of the models. An alternative approach is
the use of public arenas, such as the popular LM arena, on which any user can
freely evaluate models on any question and rank the responses of two models.
The results are then elaborated into a model ranking. An increasingly important
aspect of LLMs is their energy consumption and, therefore, evaluating how
energy awareness influences the decisions of humans in selecting a model is of
interest. In this paper, we present GEA, the Generative Energy Arena, an arena
that incorporates information on the energy consumption of the model in the
evaluation process. Preliminary results obtained with GEA are also presented,
showing that for most questions, when users are aware of the energy
consumption, they favor smaller and more energy efficient models. This suggests
that for most user interactions, the extra cost and energy incurred by the more
complex and top-performing models do not provide an increase in the perceived
quality of the responses that justifies their use.

</details>


### [45] [FormulaOne: Measuring the Depth of Algorithmic Reasoning Beyond Competitive Programming](https://arxiv.org/abs/2507.13337)
*Gal Beniamini,Yuval Dor,Alon Vinnikov,Shir Granot Peled,Or Weinstein,Or Sharir,Noam Wies,Tomer Nussbaum,Ido Ben Shaul,Tomer Zekharya,Yoav Levine,Shai Shalev-Shwartz,Amnon Shashua*

Main category: cs.AI

TL;DR: 论文提出了一个名为FormulaOne的基准测试，用于评估前沿AI模型在解决实际研究问题中的能力，结果显示当前最先进的模型表现不佳。


<details>
  <summary>Details</summary>
Motivation: 探讨前沿AI模型是否接近或超越人类专家水平，尤其是在解决复杂实际问题时的能力。

Method: 构建FormulaOne基准测试，结合图论、逻辑和算法，生成高难度问题，并评估模型表现。

Result: 前沿模型如OpenAI的o3在FormulaOne上表现极差，解决率低于1%。

Conclusion: 前沿AI模型在某些领域的专家级理解仍有很大差距，FormulaOne为未来研究提供了重要工具。

Abstract: Frontier AI models demonstrate formidable breadth of knowledge. But how close
are they to true human -- or superhuman -- expertise? Genuine experts can
tackle the hardest problems and push the boundaries of scientific
understanding. To illuminate the limits of frontier model capabilities, we turn
away from contrived competitive programming puzzles, and instead focus on
real-life research problems.
  We construct FormulaOne, a benchmark that lies at the intersection of graph
theory, logic, and algorithms, all well within the training distribution of
frontier models. Our problems are incredibly demanding, requiring an array of
reasoning steps. The dataset has three key properties. First, it is of
commercial interest and relates to practical large-scale optimisation problems,
such as those arising in routing, scheduling, and network design. Second, it is
generated from the highly expressive framework of Monadic Second-Order (MSO)
logic on graphs, paving the way toward automatic problem generation at scale;
ideal for building RL environments. Third, many of our problems are intimately
related to the frontier of theoretical computer science, and to central
conjectures therein, such as the Strong Exponential Time Hypothesis (SETH). As
such, any significant algorithmic progress on our dataset, beyond known
results, could carry profound theoretical implications.
  Remarkably, state-of-the-art models like OpenAI's o3 fail entirely on
FormulaOne, solving less than 1% of the questions, even when given 10 attempts
and explanatory fewshot examples -- highlighting how far they remain from
expert-level understanding in some domains. To support further research, we
additionally curate FormulaOne-Warmup, offering a set of simpler tasks, from
the same distribution. We release the full corpus along with a comprehensive
evaluation framework.

</details>
