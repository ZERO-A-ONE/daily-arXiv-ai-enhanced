{"id": "2506.15084", "categories": ["cs.SE", "cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2506.15084", "abs": "https://arxiv.org/abs/2506.15084", "authors": ["Weiqi Lu", "Yongqiang Tian", "Xiaohan Zhong", "Haoyang Ma", "Zhenyang Xu", "Shing-Chi Cheung", "Chengnian Sun"], "title": "An Empirical Study of Bugs in Data Visualization Libraries", "comment": "Proc. ACM Softw. Eng. 2, FSE", "summary": "Data visualization (DataViz) libraries play a crucial role in presentation,\ndata analysis, and application development, underscoring the importance of\ntheir accuracy in transforming data into visual representations. Incorrect\nvisualizations can adversely impact user experience, distort information\nconveyance, and influence user perception and decision-making processes. Visual\nbugs in these libraries can be particularly insidious as they may not cause\nobvious errors like crashes, but instead mislead users of the underlying data\ngraphically, resulting in wrong decision making. Consequently, a good\nunderstanding of the unique characteristics of bugs in DataViz libraries is\nessential for researchers and developers to detect and fix bugs in DataViz\nlibraries.\n  This study presents the first comprehensive analysis of bugs in DataViz\nlibraries, examining 564 bugs collected from five widely-used libraries. Our\nstudy systematically analyzes their symptoms and root causes, and provides a\ndetailed taxonomy. We found that incorrect/inaccurate plots are pervasive in\nDataViz libraries and incorrect graphic computation is the major root cause,\nwhich necessitates further automated testing methods for DataViz libraries.\nMoreover, we identified eight key steps to trigger such bugs and two test\noracles specific to DataViz libraries, which may inspire future research in\ndesigning effective automated testing techniques. Furthermore, with the recent\nadvancements in Vision Language Models (VLMs), we explored the feasibility of\napplying these models to detect incorrect/inaccurate plots. The results show\nthat the effectiveness of VLMs in bug detection varies from 29% to 57%,\ndepending on the prompts, and adding more information in prompts does not\nnecessarily increase the effectiveness. More findings can be found in our\nmanuscript."}
{"id": "2506.15088", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.15088", "abs": "https://arxiv.org/abs/2506.15088", "authors": ["Miao Miao"], "title": "Program Feature-based Fuzzing Benchmarking", "comment": null, "summary": "Fuzzing is a powerful software testing technique renowned for its\neffectiveness in identifying software vulnerabilities. Traditional fuzzing\nevaluations typically focus on overall fuzzer performance across a set of\ntarget programs, yet few benchmarks consider how fine-grained program features\ninfluence fuzzing effectiveness. To bridge this gap, we introduce a novel\nbenchmark designed to generate programs with configurable, fine-grained program\nfeatures to enhance fuzzing evaluations. We reviewed 25 recent grey-box fuzzing\nstudies, extracting 7 program features related to control-flow and data-flow\nthat can impact fuzzer performance. Using these features, we generated a\nbenchmark consisting of 153 programs controlled by 10 fine-grained configurable\nparameters. We evaluated 11 popular fuzzers using this benchmark. The results\nindicate that fuzzer performance varies significantly based on the program\nfeatures and their strengths, highlighting the importance of incorporating\nprogram characteristics into fuzzing evaluations."}
{"id": "2506.15098", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.15098", "abs": "https://arxiv.org/abs/2506.15098", "authors": ["Haosheng Zuo", "Feifei Niu", "Chuanyi Li"], "title": "Enhancement Report Approval Prediction: A Comparative Study of Large Language Models", "comment": null, "summary": "Enhancement reports (ERs) serve as a critical communication channel between\nusers and developers, capturing valuable suggestions for software improvement.\nHowever, manually processing these reports is resource-intensive, leading to\ndelays and potential loss of valuable insights. To address this challenge,\nenhancement report approval prediction (ERAP) has emerged as a research focus,\nleveraging machine learning techniques to automate decision-making. While\ntraditional approaches have employed feature-based classifiers and deep\nlearning models, recent advancements in large language models (LLM) present new\nopportunities for enhancing prediction accuracy. This study systematically\nevaluates 18 LLM variants (including BERT, RoBERTa, DeBERTa-v3, ELECTRA, and\nXLNet for encoder models; GPT-3.5-turbo, GPT-4o-mini, Llama 3.1 8B, Llama 3.1\n8B Instruct and DeepSeek-V3 for decoder models) against traditional methods\n(CNN/LSTM-BERT/GloVe). Our experiments reveal two key insights: (1)\nIncorporating creator profiles increases unfine-tuned decoder-only models'\noverall accuracy by 10.8 percent though it may introduce bias; (2) LoRA\nfine-tuned Llama 3.1 8B Instruct further improve performance, reaching 79\npercent accuracy and significantly enhancing recall for approved reports (76.1\npercent vs. LSTM-GLOVE's 64.1 percent), outperforming traditional methods by 5\npercent under strict chronological evaluation and effectively addressing class\nimbalance issues. These findings establish LLM as a superior solution for ERAP,\ndemonstrating their potential to streamline software maintenance workflows and\nimprove decision-making in real-world development environments. We also\ninvestigated and summarized the ER cases where the large models underperformed,\nproviding valuable directions for future research."}
{"id": "2506.14913", "categories": ["cs.CR", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.14913", "abs": "https://arxiv.org/abs/2506.14913", "authors": ["Wassim Bouaziz", "Mathurin Videau", "Nicolas Usunier", "El-Mahdi El-Mhamdi"], "title": "Winter Soldier: Backdooring Language Models at Pre-Training with Indirect Data Poisoning", "comment": "18 pages, 12 figures", "summary": "The pre-training of large language models (LLMs) relies on massive text\ndatasets sourced from diverse and difficult-to-curate origins. Although\nmembership inference attacks and hidden canaries have been explored to trace\ndata usage, such methods rely on memorization of training data, which LM\nproviders try to limit. In this work, we demonstrate that indirect data\npoisoning (where the targeted behavior is absent from training data) is not\nonly feasible but also allow to effectively protect a dataset and trace its\nuse. Using gradient-based optimization prompt-tuning, we make a model learn\narbitrary secret sequences: secret responses to secret prompts that are absent\nfrom the training corpus. We validate our approach on language models\npre-trained from scratch and show that less than 0.005% of poisoned tokens are\nsufficient to covertly make a LM learn a secret and detect it with extremely\nhigh confidence ($p < 10^{-55}$) with a theoretically certifiable scheme.\nCrucially, this occurs without performance degradation (on LM benchmarks) and\ndespite secrets never appearing in the training set."}
{"id": "2506.15135", "categories": ["cs.SE", "cs.LO", "cs.PL", "F.3.1; F.1.2"], "pdf": "https://arxiv.org/pdf/2506.15135", "abs": "https://arxiv.org/abs/2506.15135", "authors": ["Zhengqun Koo"], "title": "Towards Bug-Free Distributed Go Programs", "comment": "Version 1. B.Comp. Dissertation", "summary": "Programmers of distributed systems need to reason about concurrency to avoid\nraces. However, reasoning about concurrency is difficult, and unexpected races\nshow up as bugs. Data race detection in shared memory systems is well-studied\n(dynamic data race detection [13], behavioral types [15], dynamic race\ndetection [31]). Similar to how a data race consists of reads and writes not\nrelated by happens-before at a shared memory location, a communication race\nconsists of receives and sends not related by happens-before on a shared\nchannel. Communication races are problematic: a receiver expects a specific\nmessage from a specific sender, but with a communication race, the receiver can\nreceive a message meant for another receiver, or not receive anything at all.\nIn this work, we describe a verification framework that can prove the absence\nof communication races for distributed programs that use a subset of the Go\nprogramming language, where synchronization is mainly achieved via message\npassing. We statically reason about how a distributed program executes, using a\nhappens-before order, extended to buffered and unbuffered channels."}
{"id": "2506.14944", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2506.14944", "abs": "https://arxiv.org/abs/2506.14944", "authors": ["Majid Khabbazian"], "title": "Fair Data Exchange with Constant-Time Proofs", "comment": "13 pages, 0 figures", "summary": "The Fair Data Exchange (FDE) protocol introduced at CCS 2024 offers atomic\npay-per-file transfers with constant-size proofs, but its prover and verifier\nruntimes still scale linearly with the file length n. We collapse these costs\nto essentially constant by viewing the file as a rate-1 Reed-Solomon (RS)\ncodeword, extending it to a lower-rate RS code with constant redundancy,\nencrypting this extended vector, and then proving correctness for only a small\nrandom subset of the resulting ciphertexts; RS decoding repairs any corrupted\nsymbols with negligible failure probability. Our protocol preserves full\nclient- and server-fairness, and adds only a tunable communication redundancy\noverhead.\n  Finally, we patch the elliptic-curve mismatch in the Bitcoin instantiation of\nFDE with a compact zk-SNARK, enabling the entire exchange to run off-chain and\nfalling back to just two on-chain transactions when channels are unavailable."}
{"id": "2506.15172", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.15172", "abs": "https://arxiv.org/abs/2506.15172", "authors": ["Maria Spichkova", "Kevin Iwan", "Madeleine Zwart", "Hina Lee", "Yuwon Yoon", "Xiaohan Qin"], "title": "Advanced approach for Agile/Scrum Process: RetroAI++", "comment": "Preprint. Accepted to the 29th International Conference on\n  Knowledge-Based and Intelligent Information & Engineering Systems (KES 2025).\n  Final version to be published by Elsevier (In Press)", "summary": "In Agile/Scrum software development, sprint planning and retrospective\nanalysis are the key elements of project management. The aim of our work is to\nsupport software developers in these activities. In this paper, we present our\nprototype tool RetroAI++, based on emerging intelligent technologies. In our\nRetroAI++ prototype, we aim to automate and refine the practical application of\nAgile/Scrum processes within Sprint Planning and Retrospectives. Leveraging AI\ninsights, our prototype aims to automate and refine the many processes involved\nin the Sprint Planning, Development and Retrospective stages of Agile/Scrum\ndevelopment projects, offering intelligent suggestions for sprint organisation\nas well as meaningful insights for retrospective reflection."}
{"id": "2506.14964", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2506.14964", "abs": "https://arxiv.org/abs/2506.14964", "authors": ["Filip Rezabek", "Jonathan Passerat-Palmbach", "Moe Mahhouk", "Frieder Erdmann", "Andrew Miller"], "title": "Narrowing the Gap between TEEs Threat Model and Deployment Strategies", "comment": null, "summary": "Confidential Virtual Machines (CVMs) provide isolation guarantees for data in\nuse, but their threat model does not include physical level protection and\nside-channel attacks. Therefore, current deployments rely on trusted cloud\nproviders to host the CVMs' underlying infrastructure. However, TEE\nattestations do not provide information about the operator hosting a CVM.\nWithout knowing whether a Trusted Execution Environment (TEE) runs within a\nprovider's infrastructure, a user cannot accurately assess the risks of\nphysical attacks. We observe a misalignment in the threat model where the\nworkloads are protected against other tenants but do not offer end-to-end\nsecurity assurances to external users without relying on cloud providers. The\nattestation should be extended to bind the CVM with the provider. A possible\nsolution can rely on the Protected Platform Identifier (PPID), a unique CPU\nidentifier. However, the implementation details of various TEE manufacturers,\nattestation flows, and providers vary. This makes verification of attestations,\nease of migration, and building applications without relying on a trusted party\nchallenging, highlighting a key limitation that must be addressed for the\nadoption of CVMs. We discuss two points focusing on hardening and extensions of\nTEEs' attestation."}
{"id": "2506.15227", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.15227", "abs": "https://arxiv.org/abs/2506.15227", "authors": ["Quanjun Zhang", "Chunrong Fang", "Siqi Gu", "Ye Shang", "Zhenyu Chen", "Liang Xiao"], "title": "Large Language Models for Unit Testing: A Systematic Literature Review", "comment": null, "summary": "Unit testing is a fundamental practice in modern software engineering, with\nthe aim of ensuring the correctness, maintainability, and reliability of\nindividual software components. Very recently, with the advances in Large\nLanguage Models (LLMs), a rapidly growing body of research has leveraged LLMs\nto automate various unit testing tasks, demonstrating remarkable performance\nand significantly reducing manual effort. However, due to ongoing explorations\nin the LLM-based unit testing field, it is challenging for researchers to\nunderstand existing achievements, open challenges, and future opportunities.\nThis paper presents the first systematic literature review on the application\nof LLMs in unit testing until March 2025. We analyze \\numpaper{} relevant\npapers from the perspectives of both unit testing and LLMs. We first categorize\nexisting unit testing tasks that benefit from LLMs, e.g., test generation and\noracle generation. We then discuss several critical aspects of integrating LLMs\ninto unit testing research, including model usage, adaptation strategies, and\nhybrid approaches. We further summarize key challenges that remain unresolved\nand outline promising directions to guide future research in this area.\nOverall, our paper provides a systematic overview of the research landscape to\nthe unit testing community, helping researchers gain a comprehensive\nunderstanding of achievements and promote future research. Our artifacts are\npublicly available at the GitHub repository:\nhttps://github.com/iSEngLab/AwesomeLLM4UT."}
{"id": "2506.15018", "categories": ["cs.CR", "cs.DS", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.15018", "abs": "https://arxiv.org/abs/2506.15018", "authors": ["Ben Jacobsen", "Kassem Fawaz"], "title": "Private Continual Counting of Unbounded Streams", "comment": "12 pages, 2 figures", "summary": "We study the problem of differentially private continual counting in the\nunbounded setting where the input size $n$ is not known in advance. Current\nstate-of-the-art algorithms based on optimal instantiations of the matrix\nmechanism cannot be directly applied here because their privacy guarantees only\nhold when key parameters are tuned to $n$. Using the common `doubling trick'\navoids knowledge of $n$ but leads to suboptimal and non-smooth error. We solve\nthis problem by introducing novel matrix factorizations based on logarithmic\nperturbations of the function $\\frac{1}{\\sqrt{1-z}}$ studied in prior works,\nwhich may be of independent interest. The resulting algorithm has smooth error,\nand for any $\\alpha > 0$ and $t\\leq n$ it is able to privately estimate the sum\nof the first $t$ data points with $O(\\log^{2+2\\alpha}(t))$ variance. It\nrequires $O(t)$ space and amortized $O(\\log t)$ time per round, compared to\n$O(\\log(n)\\log(t))$ variance, $O(n)$ space and $O(n \\log n)$ pre-processing\ntime for the nearly-optimal bounded-input algorithm of Henzinger et al. (SODA\n2023). Empirically, we find that our algorithm's performance is also comparable\nto theirs in absolute terms: our variance is less than $1.5\\times$ theirs for\n$t$ as large as $2^{24}$."}
{"id": "2506.15453", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.15453", "abs": "https://arxiv.org/abs/2506.15453", "authors": ["Yusuf Sulistyo Nugroho", "Farah Danisha Salam", "Brittany Reid", "Raula Gaikovina Kula", "Kazumasa Shimari", "Kenichi Matsumoto"], "title": "Uncovering Intention through LLM-Driven Code Snippet Description Generation", "comment": "6 pages, 3 figures, 4 tables, conference paper", "summary": "Documenting code snippets is essential to pinpoint key areas where both\ndevelopers and users should pay attention. Examples include usage examples and\nother Application Programming Interfaces (APIs), which are especially important\nfor third-party libraries. With the rise of Large Language Models (LLMs), the\nkey goal is to investigate the kinds of description developers commonly use and\nevaluate how well an LLM, in this case Llama, can support description\ngeneration. We use NPM Code Snippets, consisting of 185,412 packages with\n1,024,579 code snippets. From there, we use 400 code snippets (and their\ndescriptions) as samples. First, our manual classification found that the\nmajority of original descriptions (55.5%) highlight example-based usage. This\nfinding emphasizes the importance of clear documentation, as some descriptions\nlacked sufficient detail to convey intent. Second, the LLM correctly identified\nthe majority of original descriptions as \"Example\" (79.75%), which is identical\nto our manual finding, showing a propensity for generalization. Third, compared\nto the originals, the produced description had an average similarity score of\n0.7173, suggesting relevance but room for improvement. Scores below 0.9\nindicate some irrelevance. Our results show that depending on the task of the\ncode snippet, the intention of the document may differ from being instructions\nfor usage, installations, or descriptive learning examples for any user of a\nlibrary."}
{"id": "2506.15028", "categories": ["cs.CR", "cs.ET", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.15028", "abs": "https://arxiv.org/abs/2506.15028", "authors": ["Gargi Mitra", "Mohammadreza Hallajiyan", "Inji Kim", "Athish Pranav Dharmalingam", "Mohammed Elnawawy", "Shahrear Iqbal", "Karthik Pattabiraman", "Homa Alemzadeh"], "title": "Systems-Theoretic and Data-Driven Security Analysis in ML-enabled Medical Devices", "comment": "32 pages, 6 figures, 6 tables", "summary": "The integration of AI/ML into medical devices is rapidly transforming\nhealthcare by enhancing diagnostic and treatment facilities. However, this\nadvancement also introduces serious cybersecurity risks due to the use of\ncomplex and often opaque models, extensive interconnectivity, interoperability\nwith third-party peripheral devices, Internet connectivity, and vulnerabilities\nin the underlying technologies. These factors contribute to a broad attack\nsurface and make threat prevention, detection, and mitigation challenging.\nGiven the highly safety-critical nature of these devices, a cyberattack on\nthese devices can cause the ML models to mispredict, thereby posing significant\nsafety risks to patients. Therefore, ensuring the security of these devices\nfrom the time of design is essential. This paper underscores the urgency of\naddressing the cybersecurity challenges in ML-enabled medical devices at the\npre-market phase. We begin by analyzing publicly available data on device\nrecalls and adverse events, and known vulnerabilities, to understand the threat\nlandscape of AI/ML-enabled medical devices and their repercussions on patient\nsafety. Building on this analysis, we introduce a suite of tools and techniques\ndesigned by us to assist security analysts in conducting comprehensive\npremarket risk assessments. Our work aims to empower manufacturers to embed\ncybersecurity as a core design principle in AI/ML-enabled medical devices,\nthereby making them safe for patients."}
{"id": "2506.15655", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.15655", "abs": "https://arxiv.org/abs/2506.15655", "authors": ["Yilin Zhang", "Xinran Zhao", "Zora Zhiruo Wang", "Chenyang Yang", "Jiayi Wei", "Tongshuang Wu"], "title": "cAST: Enhancing Code Retrieval-Augmented Generation with Structural Chunking via Abstract Syntax Tree", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has become essential for large-scale\ncode generation, grounding predictions in external code corpora to improve\nactuality. However, a critical yet underexplored aspect of RAG pipelines is\nchunking -- the process of dividing documents into retrievable units. Existing\nline-based chunking heuristics often break semantic structures, splitting\nfunctions or merging unrelated code, which can degrade generation quality. We\npropose chunking via Abstract Syntax Trees (\\ourwork), a structure-aware method\nthat recursively breaks large AST nodes into smaller chunks and merges sibling\nnodes while respecting size limits. This approach generates self-contained,\nsemantically coherent units across programming languages and tasks, improving\nperformance on diverse code generation tasks, e.g., boosting Recall@5 by 4.3\npoints on RepoEval retrieval and Pass@1 by 2.67 points on SWE-bench generation.\nOur work highlights the importance of structure-aware chunking for scaling\nretrieval-enhanced code intelligence."}
{"id": "2506.15034", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2506.15034", "abs": "https://arxiv.org/abs/2506.15034", "authors": ["Pratama Derry", "Laksmono Agus Mahardika Ari", "Iqbal Muhammad", "Howon Kim"], "title": "MECHA: Multithreaded and Efficient Cryptographic Hardware Access", "comment": "4 Page", "summary": "This paper presents a multithread and efficient cryptographic hardware access\n(MECHA) for efficient and fast cryptographic operations that eliminates the\nneed for context switching. Utilizing a UNIX domain socket, MECHA manages\nmultiple requests from multiple applications simultaneously, resulting in\nfaster processing and improved efficiency. We comprise several key components,\nincluding the Server thread, Client thread, Transceiver thread, and a pair of\nSender and Receiver queues. MECHA design is portable and can be used with any\ncommunication protocol, with experimental results demonstrating a 83% increase\nin the speed of concurrent cryptographic requests compared to conventional\ninterface design. MECHA architecture has significant potential in the field of\nsecure communication applications ranging from cloud computing to the IoT,\noffering a faster and more efficient solution for managing multiple\ncryptographic operation requests concurrently."}
{"id": "2506.15648", "categories": ["cs.CR", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2506.15648", "abs": "https://arxiv.org/abs/2506.15648", "authors": ["Georgios Androutsopoulos", "Antonio Bianchi"], "title": "deepSURF: Detecting Memory Safety Vulnerabilities in Rust Through Fuzzing LLM-Augmented Harnesses", "comment": null, "summary": "Although Rust ensures memory safety by default, it also permits the use of\nunsafe code, which can introduce memory safety vulnerabilities if misused.\nUnfortunately, existing tools for detecting memory bugs in Rust typically\nexhibit limited detection capabilities, inadequately handle Rust-specific\ntypes, or rely heavily on manual intervention.\n  To address these limitations, we present deepSURF, a tool that integrates\nstatic analysis with Large Language Model (LLM)-guided fuzzing harness\ngeneration to effectively identify memory safety vulnerabilities in Rust\nlibraries, specifically targeting unsafe code. deepSURF introduces a novel\napproach for handling generics by substituting them with custom types and\ngenerating tailored implementations for the required traits, enabling the\nfuzzer to simulate user-defined behaviors within the fuzzed library.\nAdditionally, deepSURF employs LLMs to augment fuzzing harnesses dynamically,\nfacilitating exploration of complex API interactions and significantly\nincreasing the likelihood of exposing memory safety vulnerabilities. We\nevaluated deepSURF on 27 real-world Rust crates, successfully rediscovering 20\nknown memory safety bugs and uncovering 6 previously unknown vulnerabilities,\ndemonstrating clear improvements over state-of-the-art tools."}
{"id": "2506.15043", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.15043", "abs": "https://arxiv.org/abs/2506.15043", "authors": ["Amir Hossein Baradaran"], "title": "Advanced Prediction of Hypersonic Missile Trajectories with CNN-LSTM-GRU Architectures", "comment": null, "summary": "Advancements in the defense industry are paramount for ensuring the safety\nand security of nations, providing robust protection against emerging threats.\nAmong these threats, hypersonic missiles pose a significant challenge due to\ntheir extreme speeds and maneuverability, making accurate trajectory prediction\na critical necessity for effective countermeasures. This paper addresses this\nchallenge by employing a novel hybrid deep learning approach, integrating\nConvolutional Neural Networks (CNNs), Long Short-Term Memory (LSTM) networks,\nand Gated Recurrent Units (GRUs). By leveraging the strengths of these\narchitectures, the proposed method successfully predicts the complex\ntrajectories of hypersonic missiles with high accuracy, offering a significant\ncontribution to defense strategies and missile interception technologies. This\nresearch demonstrates the potential of advanced machine learning techniques in\nenhancing the predictive capabilities of defense systems."}
{"id": "2506.15070", "categories": ["cs.CR", "cs.ET"], "pdf": "https://arxiv.org/pdf/2506.15070", "abs": "https://arxiv.org/abs/2506.15070", "authors": ["Rasha Karakchi", "Rye Stahle-Smith", "Nishant Chinnasami", "Tiffany Yu"], "title": "Toward a Lightweight, Scalable, and Parallel Secure Encryption Engine", "comment": "This is submitted to the ACM/IEEE Symposium on Edge Computing (SEC\n  2025)", "summary": "The exponential growth of Internet of Things (IoT) applications has\nintensified the demand for efficient, high-throughput, and energy-efficient\ndata processing at the edge. Conventional CPU-centric encryption methods suffer\nfrom performance bottlenecks and excessive data movement, especially in\nlatency-sensitive and resource-constrained environments. In this paper, we\npresent SPiME, a lightweight, scalable, and FPGA-compatible Secure\nProcessor-in-Memory Encryption architecture that integrates the Advanced\nEncryption Standard (AES-128) directly into a Processing-in-Memory (PiM)\nframework. SPiME is designed as a modular array of parallel PiM units, each\ncombining an AES core with a minimal control unit to enable distributed\nin-place encryption with minimal overhead. The architecture is fully\nimplemented in Verilog and tested on multiple AMD UltraScale and UltraScale+\nFPGAs. Evaluation results show that SPiME can scale beyond 4,000 parallel units\nwhile maintaining less than 5\\% utilization of key FPGA resources on high-end\ndevices. It delivers over 25~Gbps in sustained encryption throughput with\npredictable, low-latency performance. The design's portability,\nconfigurability, and resource efficiency make it a compelling solution for\nsecure edge computing, embedded cryptographic systems, and customizable\nhardware accelerators."}
{"id": "2506.15075", "categories": ["cs.CR", "cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2506.15075", "abs": "https://arxiv.org/abs/2506.15075", "authors": ["Samhita Kuili", "Mohammadreza Amini", "Burak Kantarci"], "title": "CWGAN-GP Augmented CAE for Jamming Detection in 5G-NR in Non-IID Datasets", "comment": "6 pages, 5 figures, Accepted to IEEE International Symposium on\n  Personal, Indoor and Mobile Radio Communications (PIMRC) 2025", "summary": "In the ever-expanding domain of 5G-NR wireless cellular networks,\nover-the-air jamming attacks are prevalent as security attacks, compromising\nthe quality of the received signal. We simulate a jamming environment by\nincorporating additive white Gaussian noise (AWGN) into the real-world In-phase\nand Quadrature (I/Q) OFDM datasets. A Convolutional Autoencoder (CAE) is\nexploited to implement a jamming detection over various characteristics such as\nheterogenous I/Q datasets; extracting relevant information on Synchronization\nSignal Blocks (SSBs), and fewer SSB observations with notable class imbalance.\nGiven the characteristics of datasets, balanced datasets are acquired by\nemploying a Conv1D conditional Wasserstein Generative Adversarial\nNetwork-Gradient Penalty(CWGAN-GP) on both majority and minority SSB\nobservations. Additionally, we compare the performance and detection ability of\nthe proposed CAE model on augmented datasets with benchmark models:\nConvolutional Denoising Autoencoder (CDAE) and Convolutional Sparse Autoencoder\n(CSAE). Despite the complexity of data heterogeneity involved across all\ndatasets, CAE depicts the robustness in detection performance of jammed signal\nby achieving average values of 97.33% precision, 91.33% recall, 94.08%\nF1-score, and 94.35% accuracy over CDAE and CSAE."}
{"id": "2506.15093", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2506.15093", "abs": "https://arxiv.org/abs/2506.15093", "authors": ["James Petrie", "Onni Aarne", "Nora Ammann", "David Dalrymple"], "title": "Flexible Hardware-Enabled Guarantees for AI Compute", "comment": null, "summary": "As artificial intelligence systems become increasingly powerful, they pose\ngrowing risks to international security, creating urgent coordination\nchallenges that current governance approaches struggle to address without\ncompromising sensitive information or national security. We propose flexible\nhardware-enabled guarantees (flexHEGs), that could be integrated with AI\naccelerators to enable trustworthy, privacy-preserving verification and\nenforcement of claims about AI development. FlexHEGs consist of an auditable\nguarantee processor that monitors accelerator usage and a secure enclosure\nproviding physical tamper protection. The system would be fully open source\nwith flexible, updateable verification capabilities. FlexHEGs could enable\ndiverse governance mechanisms including privacy-preserving model evaluations,\ncontrolled deployment, compute limits for training, and automated safety\nprotocol enforcement. In this first part of a three part series, we provide a\ncomprehensive introduction of the flexHEG system, including an overview of the\ngovernance and security capabilities it offers, its potential development and\nadoption paths, and the remaining challenges and limitations it faces. While\ntechnically challenging, flexHEGs offer an approach to address emerging\nregulatory and international security challenges in frontier AI development."}
{"id": "2506.15100", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2506.15100", "abs": "https://arxiv.org/abs/2506.15100", "authors": ["Onni Aarne", "James Petrie"], "title": "International Security Applications of Flexible Hardware-Enabled Guarantees", "comment": null, "summary": "As AI capabilities advance rapidly, flexible hardware-enabled guarantees\n(flexHEGs) offer opportunities to address international security challenges\nthrough comprehensive governance frameworks. This report examines how flexHEGs\ncould enable internationally trustworthy AI governance by establishing\nstandardized designs, robust ecosystem defenses, and clear operational\nparameters for AI-relevant chips. We analyze four critical international\nsecurity applications: limiting proliferation to address malicious use,\nimplementing safety norms to prevent loss of control, managing risks from\nmilitary AI systems, and supporting strategic stability through\nbalance-of-power mechanisms while respecting national sovereignty. The report\nexplores both targeted deployments for specific high-risk facilities and\ncomprehensive deployments covering all AI-relevant compute. We examine two\nprimary governance models: verification-based agreements that enable\ntransparent compliance monitoring, and ruleset-based agreements that\nautomatically enforce international rules through cryptographically-signed\nupdates. Through game-theoretic analysis, we demonstrate that comprehensive\nflexHEG agreements could remain stable under reasonable assumptions about state\npreferences and catastrophic risks. The report addresses critical\nimplementation challenges including technical thresholds for AI-relevant chips,\nmanagement of existing non-flexHEG hardware, and safeguards against abuse of\ngovernance power. While requiring significant international coordination,\nflexHEGs could provide a technical foundation for managing AI risks at the\nscale and speed necessary to address emerging threats to international security\nand stability."}
{"id": "2506.15102", "categories": ["cs.CR", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2506.15102", "abs": "https://arxiv.org/abs/2506.15102", "authors": ["Shizhao Peng", "Shoumo Li", "Tianle Tao"], "title": "EVA-S2PMLP: Secure and Scalable Two-Party MLP via Spatial Transformation", "comment": null, "summary": "Privacy-preserving neural network training in vertically partitioned\nscenarios is vital for secure collaborative modeling across institutions. This\npaper presents \\textbf{EVA-S2PMLP}, an Efficient, Verifiable, and Accurate\nSecure Two-Party Multi-Layer Perceptron framework that introduces spatial-scale\noptimization for enhanced privacy and performance. To enable reliable\ncomputation under real-number domain, EVA-S2PMLP proposes a secure\ntransformation pipeline that maps scalar inputs to vector and matrix spaces\nwhile preserving correctness. The framework includes a suite of atomic\nprotocols for linear and non-linear secure computations, with modular support\nfor secure activation, matrix-vector operations, and loss evaluation.\nTheoretical analysis confirms the reliability, security, and asymptotic\ncomplexity of each protocol. Extensive experiments show that EVA-S2PMLP\nachieves high inference accuracy and significantly reduced communication\noverhead, with up to $12.3\\times$ improvement over baselines. Evaluation on\nbenchmark datasets demonstrates that the framework maintains model utility\nwhile ensuring strict data confidentiality, making it a practical solution for\nprivacy-preserving neural network training in finance, healthcare, and\ncross-organizational AI applications."}
{"id": "2506.15112", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2506.15112", "abs": "https://arxiv.org/abs/2506.15112", "authors": ["Xiangman Li", "Xiaodong Wu", "Jianbing Ni", "Mohamed Mahmoud", "Maazen Alsabaan"], "title": "PDLRecover: Privacy-preserving Decentralized Model Recovery with Machine Unlearning", "comment": null, "summary": "Decentralized learning is vulnerable to poison attacks, where malicious\nclients manipulate local updates to degrade global model performance. Existing\ndefenses mainly detect and filter malicious models, aiming to prevent a limited\nnumber of attackers from corrupting the global model. However, restoring an\nalready compromised global model remains a challenge. A direct approach is to\nremove malicious clients and retrain the model using only the benign clients.\nYet, retraining is time-consuming, computationally expensive, and may\ncompromise model consistency and privacy.\n  We propose PDLRecover, a novel method to recover a poisoned global model\nefficiently by leveraging historical model information while preserving\nprivacy. The main challenge lies in protecting shared historical models while\nenabling parameter estimation for model recovery. By exploiting the linearity\nof approximate Hessian matrix computation, we apply secret sharing to protect\nhistorical updates, ensuring local models are not leaked during transmission or\nreconstruction. PDLRecover introduces client-side preparation, periodic\nrecovery updates, and a final exact update to ensure robustness and convergence\nof the recovered model. Periodic updates maintain accurate curvature\ninformation, and the final step ensures high-quality convergence. Experiments\nshow that the recovered global model achieves performance comparable to a fully\nretrained model but with significantly reduced computation and time cost.\nMoreover, PDLRecover effectively prevents leakage of local model parameters,\nensuring both accuracy and privacy in recovery."}
{"id": "2506.15117", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2506.15117", "abs": "https://arxiv.org/abs/2506.15117", "authors": ["Ming Nie", "Zhixiong Yang", "Bingsheng Wei"], "title": "CipherMind: The Longest Codebook in the World", "comment": null, "summary": "In recent years, the widespread application of large language models has\ninspired us to consider using inference for communication encryption. We\ntherefore propose CipherMind, which utilizes intermediate results from\ndeterministic fine-tuning of large model inferences as transmission content.\nThe semantic parameters of large models exhibit characteristics like opaque\nunderlying implementations and weak interpretability, thus enabling their use\nas an encryption method for data transmission. This communication paradigm can\nbe applied in scenarios like intra-gateway transmission, and theoretically, it\ncan be implemented using any large model as its foundation."}
{"id": "2506.15170", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2506.15170", "abs": "https://arxiv.org/abs/2506.15170", "authors": ["Yanxu Mao", "Tiehan Cui", "Peipei Liu", "Datao You", "Hongsong Zhu"], "title": "From LLMs to MLLMs to Agents: A Survey of Emerging Paradigms in Jailbreak Attacks and Defenses within LLM Ecosystem", "comment": null, "summary": "Large language models (LLMs) are rapidly evolving from single-modal systems\nto multimodal LLMs and intelligent agents, significantly expanding their\ncapabilities while introducing increasingly severe security risks. This paper\npresents a systematic survey of the growing complexity of jailbreak attacks and\ncorresponding defense mechanisms within the expanding LLM ecosystem. We first\ntrace the developmental trajectory from LLMs to MLLMs and Agents, highlighting\nthe core security challenges emerging at each stage. Next, we categorize\nmainstream jailbreak techniques from both the attack impact and visibility\nperspectives, and provide a comprehensive analysis of representative attack\nmethods, related datasets, and evaluation metrics. On the defense side, we\norganize existing strategies based on response timing and technical approach,\noffering a structured understanding of their applicability and implementation.\nFurthermore, we identify key limitations in existing surveys, such as\ninsufficient attention to agent-specific security issues, the absence of a\nclear taxonomy for hybrid jailbreak methods, a lack of detailed analysis of\nexperimental setups, and outdated coverage of recent advancements. To address\nthese limitations, we provide an updated synthesis of recent work and outline\nfuture research directions in areas such as dataset construction, evaluation\nframework optimization, and strategy generalization. Our study seeks to enhance\nthe understanding of jailbreak mechanisms and facilitate the advancement of\nmore resilient and adaptive defense strategies in the context of ever more\ncapable LLMs."}
{"id": "2506.15212", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2506.15212", "abs": "https://arxiv.org/abs/2506.15212", "authors": ["Madjid G. Tehrani", "Eldar Sultanow", "William J. Buchanan", "Mahkame Houmani", "Christel H. Djaha Fodja"], "title": "LLM vs. SAST: A Technical Analysis on Detecting Coding Bugs of GPT4-Advanced Data Analysis", "comment": null, "summary": "With the rapid advancements in Natural Language Processing (NLP), large\nlanguage models (LLMs) like GPT-4 have gained significant traction in diverse\napplications, including security vulnerability scanning. This paper\ninvestigates the efficacy of GPT-4 in identifying software vulnerabilities\ncompared to traditional Static Application Security Testing (SAST) tools.\nDrawing from an array of security mistakes, our analysis underscores the potent\ncapabilities of GPT-4 in LLM-enhanced vulnerability scanning. We unveiled that\nGPT-4 (Advanced Data Analysis) outperforms SAST by an accuracy of 94% in\ndetecting 32 types of exploitable vulnerabilities. This study also addresses\nthe potential security concerns surrounding LLMs, emphasising the imperative of\nsecurity by design/default and other security best practices for AI."}
{"id": "2506.15224", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2506.15224", "abs": "https://arxiv.org/abs/2506.15224", "authors": ["Kevin Pfisterer", "Quentin Hillebrand", "Vorapong Suppakitpaisarn"], "title": "Facility Location Problem under Local Differential Privacy without Super-set Assumption", "comment": "accepted at DBSec 2025", "summary": "In this paper, we introduce an adaptation of the facility location problem\nand analyze it within the framework of local differential privacy (LDP). Under\nthis model, we ensure the privacy of client presence at specific locations.\nWhen n is the number of points, Gupta et al. established a lower bound of\n$\\Omega(\\sqrt{n})$ on the approximation ratio for any differentially private\nalgorithm applied to the original facility location problem. As a result,\nsubsequent works have adopted the super-set assumption, which may, however,\ncompromise user privacy. We show that this lower bound does not apply to our\nadaptation by presenting an LDP algorithm that achieves a constant\napproximation ratio with a relatively small additive factor. Additionally, we\nprovide experimental results demonstrating that our algorithm outperforms the\nstraightforward approach on both synthetically generated and real-world\ndatasets."}
{"id": "2506.15253", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.15253", "abs": "https://arxiv.org/abs/2506.15253", "authors": ["Yuchuan Fu", "Xiaohan Yuan", "Dongxia Wang"], "title": "RAS-Eval: A Comprehensive Benchmark for Security Evaluation of LLM Agents in Real-World Environments", "comment": "12 pages, 8 figures", "summary": "The rapid deployment of Large language model (LLM) agents in critical domains\nlike healthcare and finance necessitates robust security frameworks. To address\nthe absence of standardized evaluation benchmarks for these agents in dynamic\nenvironments, we introduce RAS-Eval, a comprehensive security benchmark\nsupporting both simulated and real-world tool execution. RAS-Eval comprises 80\ntest cases and 3,802 attack tasks mapped to 11 Common Weakness Enumeration\n(CWE) categories, with tools implemented in JSON, LangGraph, and Model Context\nProtocol (MCP) formats. We evaluate 6 state-of-the-art LLMs across diverse\nscenarios, revealing significant vulnerabilities: attacks reduced agent task\ncompletion rates (TCR) by 36.78% on average and achieved an 85.65% success rate\nin academic settings. Notably, scaling laws held for security capabilities,\nwith larger models outperforming smaller counterparts. Our findings expose\ncritical risks in real-world agent deployments and provide a foundational\nframework for future security research. Code and data are available at\nhttps://github.com/lanzer-tree/RAS-Eval."}
{"id": "2506.15388", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.15388", "abs": "https://arxiv.org/abs/2506.15388", "authors": ["Florian Rokohl", "Alexander Lehnert", "Marc Reichenbach"], "title": "Evaluation Pipeline for systematically searching for Anomaly Detection Systems", "comment": "Submitted to 18th HiPEAC Workshop on Reconfigurable Computing\n  (WRC'2024)", "summary": "Digitalization in the medical world provides major benefits while making it a\ntarget for attackers and thus hard to secure. To deal with network intruders we\npropose an anomaly detection system on hardware to detect malicious clients in\nreal-time. We meet real-time and power restrictions using FPGAs. Overall system\nperformance is achieved via the presented holistic system evaluation."}
{"id": "2506.15417", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2506.15417", "abs": "https://arxiv.org/abs/2506.15417", "authors": ["Alessandro Palumbo", "Ruben Salvador"], "title": "Detecting Hardware Trojans in Microprocessors via Hardware Error Correction Code-based Modules", "comment": "To appear at the 31st IEEE International Symposium on On-Line Testing\n  and Robust System Design (IOLTS) 2025, 7 pages, 5 figures,", "summary": "Software-exploitable Hardware Trojans (HTs) enable attackers to execute\nunauthorized software or gain illicit access to privileged operations. This\nmanuscript introduces a hardware-based methodology for detecting runtime HT\nactivations using Error Correction Codes (ECCs) on a RISC-V microprocessor.\nSpecifically, it focuses on HTs that inject malicious instructions, disrupting\nthe normal execution flow by triggering unauthorized programs. To counter this\nthreat, the manuscript introduces a Hardware Security Checker (HSC) leveraging\nHamming Single Error Correction (HSEC) architectures for effective HT\ndetection. Experimental results demonstrate that the proposed solution achieves\na 100% detection rate for potential HT activations, with no false positives or\nundetected attacks. The implementation incurs minimal overhead, requiring only\n72 #LUTs, 24 #FFs, and 0.5 #BRAM while maintaining the microprocessor's\noriginal operating frequency and introducing no additional time delay."}
{"id": "2506.15432", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2506.15432", "abs": "https://arxiv.org/abs/2506.15432", "authors": ["Guillaume Lomet", "Ruben Salvador", "Brice Colombier", "Vincent Grosso", "Olivier Sentieys", "Cedric Killian"], "title": "Side-Channel Extraction of Dataflow AI Accelerator Hardware Parameters", "comment": "To appear at the 31st IEEE International Symposium on On-Line Testing\n  and Robust System Design (IOLTS) 2025, 7 pages, 4 figures, 1 algorithm", "summary": "Dataflow neural network accelerators efficiently process AI tasks on FPGAs,\nwith deployment simplified by ready-to-use frameworks and pre-trained models.\nHowever, this convenience makes them vulnerable to malicious actors seeking to\nreverse engineer valuable Intellectual Property (IP) through Side-Channel\nAttacks (SCA). This paper proposes a methodology to recover the hardware\nconfiguration of dataflow accelerators generated with the FINN framework.\nThrough unsupervised dimensionality reduction, we reduce the computational\noverhead compared to the state-of-the-art, enabling lightweight classifiers to\nrecover both folding and quantization parameters. We demonstrate an attack\nphase requiring only 337 ms to recover the hardware parameters with an accuracy\nof more than 95% and 421 ms to fully recover these parameters with an averaging\nof 4 traces for a FINN-based accelerator running a CNN, both using a random\nforest classifier on side-channel traces, even with the accelerator dataflow\nfully loaded. This approach offers a more realistic attack scenario than\nexisting methods, and compared to SoA attacks based on tsfresh, our method\nrequires 940x and 110x less time for preparation and attack phases,\nrespectively, and gives better results even without averaging traces."}
{"id": "2506.15547", "categories": ["cs.CR", "cs.CC", "quant-ph"], "pdf": "https://arxiv.org/pdf/2506.15547", "abs": "https://arxiv.org/abs/2506.15547", "authors": ["Cameron Foreman", "Lewis Wooltorton", "Kevin Milner", "Florian J. Curchod"], "title": "An efficient construction of Raz's two-source randomness extractor with improved parameters", "comment": "12 + 11 pages. Comments welcome!", "summary": "Randomness extractors are algorithms that distill weak random sources into\nnear-perfect random numbers. Two-source extractors enable this distillation\nprocess by combining two independent weak random sources. Raz's extractor (STOC\n'05) was the first to achieve this in a setting where one source has linear\nmin-entropy (i.e., proportional to its length), while the other has only\nlogarithmic min-entropy in its length. However, Raz's original construction is\nimpractical due to a polynomial computation time of at least degree 4. Our work\nsolves this problem by presenting an improved version of Raz's extractor with\nquasi-linear computation time, as well as a new analytic theorem with reduced\nentropy requirements. We provide comprehensive analytical and numerical\ncomparisons of our construction with others in the literature, and we derive\nstrong and quantum-proof versions of our efficient Raz extractor. Additionally,\nwe offer an easy-to-use, open-source code implementation of the extractor and a\nnumerical parameter calculation module."}
{"id": "2506.15648", "categories": ["cs.CR", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2506.15648", "abs": "https://arxiv.org/abs/2506.15648", "authors": ["Georgios Androutsopoulos", "Antonio Bianchi"], "title": "deepSURF: Detecting Memory Safety Vulnerabilities in Rust Through Fuzzing LLM-Augmented Harnesses", "comment": null, "summary": "Although Rust ensures memory safety by default, it also permits the use of\nunsafe code, which can introduce memory safety vulnerabilities if misused.\nUnfortunately, existing tools for detecting memory bugs in Rust typically\nexhibit limited detection capabilities, inadequately handle Rust-specific\ntypes, or rely heavily on manual intervention.\n  To address these limitations, we present deepSURF, a tool that integrates\nstatic analysis with Large Language Model (LLM)-guided fuzzing harness\ngeneration to effectively identify memory safety vulnerabilities in Rust\nlibraries, specifically targeting unsafe code. deepSURF introduces a novel\napproach for handling generics by substituting them with custom types and\ngenerating tailored implementations for the required traits, enabling the\nfuzzer to simulate user-defined behaviors within the fuzzed library.\nAdditionally, deepSURF employs LLMs to augment fuzzing harnesses dynamically,\nfacilitating exploration of complex API interactions and significantly\nincreasing the likelihood of exposing memory safety vulnerabilities. We\nevaluated deepSURF on 27 real-world Rust crates, successfully rediscovering 20\nknown memory safety bugs and uncovering 6 previously unknown vulnerabilities,\ndemonstrating clear improvements over state-of-the-art tools."}
{"id": "2506.15656", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2506.15656", "abs": "https://arxiv.org/abs/2506.15656", "authors": ["Wenhao Li", "Selvakumar Manickam", "Yung-wey Chong", "Shankar Karuppayah"], "title": "PhishDebate: An LLM-Based Multi-Agent Framework for Phishing Website Detection", "comment": null, "summary": "Phishing websites continue to pose a significant cybersecurity threat, often\nleveraging deceptive structures, brand impersonation, and social engineering\ntactics to evade detection. While recent advances in large language models\n(LLMs) have enabled improved phishing detection through contextual\nunderstanding, most existing approaches rely on single-agent classification\nfacing the risks of hallucination and lack interpretability or robustness. To\naddress these limitations, we propose PhishDebate, a modular multi-agent\nLLM-based debate framework for phishing website detection. PhishDebate employs\nfour specialized agents to independently analyze different textual aspects of a\nwebpage--URL structure, HTML composition, semantic content, and brand\nimpersonation--under the coordination of a Moderator and a final Judge. Through\nstructured debate and divergent thinking, the framework delivers more accurate\nand interpretable decisions. Extensive evaluations on commercial LLMs\ndemonstrate that PhishDebate achieves 98.2% recall and 98.2% True Positive Rate\n(TPR) on a real-world phishing dataset, and outperforms single-agent and Chain\nof Thought (CoT) baselines. Additionally, its modular design allows agent-level\nconfigurability, enabling adaptation to varying resource and application\nrequirements."}
