<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 17]
- [cs.CR](#cs.CR) [Total: 25]
- [cs.AI](#cs.AI) [Total: 43]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Evaluating Large Language Models for Functional and Maintainable Code in Industrial Settings: A Case Study at ASML](https://arxiv.org/abs/2509.12395)
*Yash Mundhra,Max Valk,Maliheh Izadi*

Main category: cs.SE

TL;DR: 在ASML工业环境中评估LLM生成专有代码的能力，发现提示技术和模型大小对代码质量影响显著，而代码专用与通用LLM性能差异不明显


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在开源代码生成方面表现优异，但在专有工业环境中的适用性尚未充分探索，特别是在具有领域特定约束和代码依赖性的环境中

Method: 开发了针对ASML专有代码库的评估框架和新基准，提出了build@k评估指标，研究了不同提示技术、比较通用和代码专用LLM性能，并分析了模型大小的影响

Result: 提示技术和模型大小对输出质量有显著影响，few-shot和chain-of-thought提示获得最高的构建成功率，代码专用与通用LLM性能差异不明显且在不同模型家族间变化很大

Conclusion: LLM在工业专有代码生成中具有潜力，但需要针对性的提示技术和适当的模型规模，代码专用模型并非总是优于通用模型

Abstract: Large language models have shown impressive performance in various domains,
including code generation across diverse open-source domains. However, their
applicability in proprietary industrial settings, where domain-specific
constraints and code interdependencies are prevalent, remains largely
unexplored. We present a case study conducted in collaboration with the
leveling department at ASML to investigate the performance of LLMs in
generating functional, maintainable code within a closed, highly specialized
software environment.
  We developed an evaluation framework tailored to ASML's proprietary codebase
and introduced a new benchmark. Additionally, we proposed a new evaluation
metric, build@k, to assess whether LLM-generated code successfully compiles and
integrates within real industrial repositories. We investigate various
prompting techniques, compare the performance of generic and code-specific
LLMs, and examine the impact of model size on code generation capabilities,
using both match-based and execution-based metrics. The findings reveal that
prompting techniques and model size have a significant impact on output
quality, with few-shot and chain-of-thought prompting yielding the highest
build success rates. The difference in performance between the code-specific
LLMs and generic LLMs was less pronounced and varied substantially across
different model families.

</details>


### [2] [Understanding Prompt Management in GitHub Repositories: A Call for Best Practices](https://arxiv.org/abs/2509.12421)
*Hao Li,Hicham Masri,Filipe R. Cogo,Abdul Ali Bangash,Bram Adams,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: 对24,800个开源提示的经验分析，揭示了提示管理中的格式不一致、重复和读取性问题，并提出改进建议


<details>
  <summary>Details</summary>
Motivation: 基础模型的快速采用产生了提示软件(promptware)，而提示的有效管理（如组织和质量保证）至关重要但革具挑战

Method: 对92个GitHub仓库中的24,800个开源提示进行经验分析，研究提示管理实践和质量属性

Result: 发现了重要挑战：提示格式存在明显不一致性、大量内部和外部提示重复、常见的读取性和拼写问题

Conclusion: 根据发现提出可操作的建议，帮助开发者在快速发展的提示软件生态系统中提高开源提示的可用性和可维护性

Abstract: The rapid adoption of foundation models (e.g., large language models) has
given rise to promptware, i.e., software built using natural language prompts.
Effective management of prompts, such as organization and quality assurance, is
essential yet challenging. In this study, we perform an empirical analysis of
24,800 open-source prompts from 92 GitHub repositories to investigate prompt
management practices and quality attributes. Our findings reveal critical
challenges such as considerable inconsistencies in prompt formatting,
substantial internal and external prompt duplication, and frequent readability
and spelling issues. Based on these findings, we provide actionable
recommendations for developers to enhance the usability and maintainability of
open-source prompts within the rapidly evolving promptware ecosystem.

</details>


### [3] [From Legacy Fortran to Portable Kokkos:An Autonomous Agentic AI Workflow](https://arxiv.org/abs/2509.12443)
*Sparsh Gupta,Kamalavasan Kamalakkannan,Maxim Moraru,Galen Shipman,Patrick Diehl*

Main category: cs.SE

TL;DR: 使用多个LLM代理协作的AI流程，自动将继承Fortran代码转换为性能可移植的Kokkos C++程序，以满足异构GPU加速设备的需求


<details>
  <summary>Details</summary>
Motivation: 科学计算应用依赖于传统Fortran代码，而现代HPC向GPU加速异构架构迁移，许多加速器缺乏原生Fortran支持，需要将绣承代码现代化以实现可移植性

Method: 设计了一个代理AI流程，通过专门化的LLM代理协作完成Fortran到Kokkos的转换、验证、编译、运行、测试、调试和优化等任务

Result: 流程成功将多个标准内核现代化，生成了在不同硬件平台上性能可移植的Kokkos代码，优化后的代码超越了Fortran基准线，而且成本仅几美元

Conclusion: 证明了代理AI在Fortran到Kokkos转换中的可行性，为自主现代化继承科学应用提供了途径，展示了LLM驱动系统在科学计算领域执行结构化任务的潜力

Abstract: Scientific applications continue to rely on legacy Fortran codebases
originally developed for homogeneous, CPU-based systems. As High-Performance
Computing (HPC) shifts toward heterogeneous GPU-accelerated architectures, many
accelerators lack native Fortran bindings, creating an urgent need to modernize
legacy codes for portability. Frameworks like Kokkos provide performance
portability and a single-source C++ abstraction, but manual Fortran-to-Kokkos
porting demands significant expertise and time. Large language models (LLMs)
have shown promise in source-to-source code generation, yet their use in fully
autonomous workflows for translating and optimizing parallel code remains
largely unexplored, especially for performance portability across diverse
hardware.
  This paper presents an agentic AI workflow where specialized LLM "agents"
collaborate to translate, validate, compile, run, test, debug, and optimize
Fortran kernels into portable Kokkos C++ programs. Results show the pipeline
modernizes a range of benchmark kernels, producing performance-portable Kokkos
codes across hardware partitions. Paid OpenAI models such as GPT-5 and
o4-mini-high executed the workflow for only a few U.S. dollars, generating
optimized codes that surpassed Fortran baselines, whereas open-source models
like Llama4-Maverick often failed to yield functional codes.
  This work demonstrates the feasibility of agentic AI for Fortran-to-Kokkos
transformation and offers a pathway for autonomously modernizing legacy
scientific applications to run portably and efficiently on diverse
supercomputers. It further highlights the potential of LLM-driven agentic
systems to perform structured, domain-specific reasoning tasks in scientific
and systems-oriented applications.

</details>


### [4] [Perspectives, Needs and Challenges for Sustainable Software Engineering Teams: A FinServ Case Study](https://arxiv.org/abs/2509.12466)
*Satwik Ghanta,Peggy Gregory,Gul Calikli*

Main category: cs.SE

TL;DR: 该研究通过案例研究发现金融服务公司在可持续软件工程实践中存在管理层与开发人员之间的认知差异，管理层关注技术和经济可持续性，而开发人员更关注工作负荷和压力等人本因素。


<details>
  <summary>Details</summary>
Motivation: 可持续软件工程(SSE)虽然日益重要，但缺乏针对特定组织背景(如金融服务行业)的研究，这些行业具有数据驱动、严格监管和高交易量的特点，需要了解其具体的可持续性需求。

Method: 采用探索性定性案例研究方法，通过对金融服务公司的高层管理人员(6人)和软件工程师(16人)进行访谈和焦点小组讨论，调查他们对可持续性的认知、实践方式和挑战。

Result: 研究发现组织层级间对可持续性的认知存在明显分歧：管理层强调技术和经济可持续性(如云迁移、数据可用性)，开发人员则关注人本因素(工作负荷管理、压力减少)。开发人员对组织倡议持怀疑态度，认为可能是公关策略。多数参与者倾向于设立专门的可持续性团队。

Conclusion: 组织目标与开发人员需求之间的脱节凸显了需要针对具体情境、共同设计的干预措施的重要性，建议借鉴安全治理的内部结构模式来建立可持续性团队。

Abstract: Sustainable Software Engineering (SSE) is slowly becoming an industry need
for reasons including reputation enhancement, improved profits and more
efficient practices. However, SSE has many definitions, and this is a challenge
for organisations trying to build a common and broadly agreed understanding of
the term. Although much research effort has gone into identifying general SSE
practices, there is a gap in understanding the sustainability needs of specific
organisational contexts, such as financial services, which are highly
data-driven, operate under strict regulatory requirements, and handle millions
of transactions day to day. To address this gap, our research focuses on a
financial services company (FinServCo) that invited us to investigate
perceptions of sustainability in their IT function: how it could be put into
practice, who is responsible for it, and what the challenges are. We conducted
an exploratory qualitative case study using interviews and a focus group with
six higher management employees and 16 software engineers comprising various
experience levels from junior developers to team leaders. Our study found a
clear divergence in how sustainability is perceived between organisational
levels. Higher management emphasised technical and economic sustainability,
focusing on cloud migration and business continuity through data availability.
In contrast, developers highlighted human-centric concerns such as workload
management and stress reduction. Scepticism toward organisational initiatives
was also evident, with some developers viewing them as a PR strategy. Many
participants expressed a preference for a dedicated sustainability team,
drawing analogies to internal structures for security governance. The
disconnect between organisational goals and individual developer needs
highlights the importance of context-sensitive, co-designed interventions.

</details>


### [5] [Good Vibrations? A Qualitative Study of Co-Creation, Communication, Flow, and Trust in Vibe Coding](https://arxiv.org/abs/2509.12491)
*Veronica Pimenova,Sarah Fakhoury,Christian Bird,Margaret-Anne Storey,Madeline Endres*

Main category: cs.SE

TL;DR: 本文首次对vibe coding进行系统性定性研究，通过访谈和社交媒体数据分析，揭示了这种AI辅助编程范式的本质、实践方式、痛点及最佳实践。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注代码产物或理论探讨，缺乏对开发者实际体验的实证研究，需要深入理解vibe coding在实践中的感知和应用。

Method: 采用定性研究方法，分析超过19万字的半结构化访谈、Reddit讨论和LinkedIn帖子数据，构建基于对话交互、共同创造和开发者心流体验的理论框架。

Result: 发现AI信任在委托到共同创造的连续体中起调节作用，识别了规范制定、可靠性、调试、延迟等痛点，并总结了缓解这些挑战的最佳实践。

Conclusion: 研究为AI开发工具的未来发展提供了启示，并为vibe coding的进一步研究指明了方向。

Abstract: Vibe coding, a term coined by Andrej Karpathy in February 2025, has quickly
become a compelling and controversial natural language programming paradigm in
AI-assisted software development. Centered on iterative co-design with an AI
assistant, vibe coding emphasizes flow and experimentation over strict upfront
specification. While initial studies have begun to explore this paradigm, most
focus on analyzing code artifacts or proposing theories with limited empirical
backing. There remains a need for a grounded understanding of vibe coding as it
is perceived and experienced by developers. We present the first systematic
qualitative investigation of vibe coding perceptions and practice. Drawing on
over 190,000 words from semi-structured interviews, Reddit threads, and
LinkedIn posts, we characterize what vibe coding is, why and how developers use
it, where it breaks down, and which emerging practices aim to support it. We
propose a qualitatively grounded theory of vibe coding centered on
conversational interaction with AI, co-creation, and developer flow and joy. We
find that AI trust regulates movement along a continuum from delegation to
co-creation and supports the developer experience by sustaining flow. We
surface recurring pain points and risks in areas including specification,
reliability, debugging, latency, code review burden, and collaboration. We also
present best practices that have been discovered and shared to mitigate these
challenges. We conclude with implications for the future of AI dev tools and
directions for researchers investigating vibe coding.

</details>


### [6] [Ensembling Large Language Models for Code Vulnerability Detection: An Empirical Evaluation](https://arxiv.org/abs/2509.12629)
*Zhihong Sun,Jia Li,Yao Wan,Chuanyi Li,Hongyu Zhang,Zhi jin,Ge Li,Hong Liu,Chen Lyu,Songlin Hu*

Main category: cs.SE

TL;DR: 通过集成学习技术（Bagging、Boosting、Stacking）结合多个大语言模型，显著提升了代码漏洞检测的性能，并提出了专门的动态门控堆叠方法DGS来处理类别不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型在代码漏洞检测方面表现出艺，但不同模型或同一模型不同训练阶段对相同代码的检测结果存在显著差异，这些不一致性虽然影响检测稳定性，但也为利用模型间的补充性通过集成学习构建更稳健的检测系统提供了机会。

Method: 采用5个大语言模型（DeepSeek-Coder-6.7B、CodeLlama-7B/13B、CodeQwen1.5-7B、StarCoder2-15B）在3个数据集（Devign、ReVeal、BigVul）上进行完整实验，测试了三种集成策略（Bagging、Boosting、Stacking）。受混合专家系统的启发，还提出了专门为漏洞检测设计的动态门控堆叠方法（DGS）。

Result: 集成学习方法能够显著提升检测性能，兼容不平衡数据集的Boosting策略表现特别优异。新提出的DGS方法在处理类别不平衡和多类分类任务中正常超越传统Stacking方法。

Conclusion: 通过集成学习技术利用不同大语言模型的补充性，可以构建更可靠和高效的基于LLM的代码漏洞检测系统，为该领域提供了有价值的见解。

Abstract: Code vulnerability detection is crucial for ensuring the security and
reliability of modern software systems. Recently, Large Language Models (LLMs)
have shown promising capabilities in this domain. However, notable
discrepancies in detection results often arise when analyzing identical code
segments across different training stages of the same model or among
architecturally distinct LLMs. While such inconsistencies may compromise
detection stability, they also highlight a key opportunity: the latent
complementarity among models can be harnessed through ensemble learning to
create more robust vulnerability detection systems. In this study, we explore
the potential of ensemble learning to enhance the performance of LLMs in source
code vulnerability detection. We conduct comprehensive experiments involving
five LLMs (i.e., DeepSeek-Coder-6.7B, CodeLlama-7B, CodeLlama-13B,
CodeQwen1.5-7B, and StarCoder2-15B), using three ensemble strategies (i.e.,
Bagging, Boosting, and Stacking). These experiments are carried out across
three widely adopted datasets (i.e., Devign, ReVeal, and BigVul). Inspired by
Mixture of Experts (MoE) techniques, we further propose Dynamic Gated Stacking
(DGS), a Stacking variant tailored for vulnerability detection. Our results
demonstrate that ensemble approaches can significantly improve detection
performance, with Boosting excelling in scenarios involving imbalanced
datasets. Moreover, DGS consistently outperforms traditional Stacking,
particularly in handling class imbalance and multi-class classification tasks.
These findings offer valuable insights into building more reliable and
effective LLM-based vulnerability detection systems through ensemble learning.

</details>


### [7] [When Large Language Models Meet UAVs: How Far Are We?](https://arxiv.org/abs/2509.12795)
*Yihua Chen,Xingle Que,Jiashuo Zhang,Ting Chen,Guangshun Li,Jiachi Chen*

Main category: cs.SE

TL;DR: 该研究通过分析74篇论文和56个GitHub项目，发现学术界与工业界在LLM-UAV融合应用上存在差异，并识别了阻碍实际应用的技术成熟度、性能、安全等关键因素。


<details>
  <summary>Details</summary>
Motivation: 无人机与大型语言模型融合的研究日益增多，但现有研究多为初步探索，缺乏对实际应用的深入理解，存在学术研究与实际需求脱节的风险。

Method: 采用实证研究方法，分析74篇学术论文和56个公开GitHub项目，识别LLM在无人机系统中的九类任务类型并量化分布，同时通过在线问卷获取52份有效反馈。

Result: 研究发现学术界侧重理论建模和任务优化，任务关注分散；工业界则聚焦飞行控制、任务规划和人机交互，注重可操作性和效率。40.4%的从业者尝试过将LLM应用于无人机任务。

Conclusion: 研究识别了技术成熟度、性能、安全、成本等阻碍实际应用的关键因素，并为未来发展提出了挑战和建议，旨在促进LLM-UAV技术的实际落地。

Abstract: The integration of unmanned aerial vehicles (UAVs) and large language models
(LLMs) has emerged as a research direction of growing interest, with the
potential to address challenges in autonomous decision-making, human-UAV
interaction, and real-time adaptability. However, existing studies have
remained largely in preliminary exploration with a limited understanding of
real-world practice, risking a misalignment between academic research and
practical needs and hindering the translation of results. To examine and
address these potential challenges, we conducted an empirical study of 74
selected papers and 56 public GitHub projects, identified nine task types for
LLMs in UAV systems, and quantified their distribution. Our findings show that
academic research emphasizes theoretical modeling and task optimization with
dispersed attention across tasks. In contrast, industrial projects focus on
flight control, task planning, and human-machine interaction, prioritizing
operability and efficiency. To further capture industry perspectives, we
distributed an online questionnaire. We obtained 52 valid responses: 40.4% of
practitioners have attempted to apply LLMs to UAV tasks. We further identify
factors that impede real-world integration, including technological maturity,
performance, safety, cost, and other considerations. Finally, we highlight
challenges for future development and provide recommendations.

</details>


### [8] [LLM-Based Approach for Enhancing Maintainability of Automotive Architectures](https://arxiv.org/abs/2509.12798)
*Nenad Petrovic,Lukasz Mazur,Alois Knoll*

Main category: cs.SE

TL;DR: 探索大型语言模型在提高汽车系统灵活性方面的应用潜力，通过三个案例研究展示自动化任务的可能性


<details>
  <summary>Details</summary>
Motivation: 汽车系统存在许多降低灵活性的瓶颈，包括长期维护困难、后期更新扩展复杂、重工程和合规流程冗长、设备及软件组件异构繁多等问题

Method: 使用OpenAI的GPT-4o模型进行概念验证实现，通过三个案例研究：1)更新、硬件抽象和合规性检查 2)接口兼容性检查 3)架构修改建议

Result: 早期研究阶段展示了LLMs在自动化汽车系统任务方面的潜力，为提升系统灵活性提供了可行方案

Conclusion: 大型语言模型在提高汽车系统灵活性方面具有显著潜力，能够有效解决传统方法中的瓶颈问题

Abstract: There are many bottlenecks that decrease the flexibility of automotive
systems, making their long-term maintenance, as well as updates and extensions
in later lifecycle phases increasingly difficult, mainly due to long
re-engineering, standardization, and compliance procedures, as well as
heterogeneity and numerosity of devices and underlying software components
involved. In this paper, we explore the potential of Large Language Models
(LLMs) when it comes to the automation of tasks and processes that aim to
increase the flexibility of automotive systems. Three case studies towards
achieving this goal are considered as outcomes of early-stage research: 1)
updates, hardware abstraction, and compliance, 2) interface compatibility
checking, and 3) architecture modification suggestions. For proof-of-concept
implementation, we rely on OpenAI's GPT-4o model.

</details>


### [9] [SateLight: A Satellite Application Update Framework for Satellite Computing](https://arxiv.org/abs/2509.12809)
*Jinfeng Wen,Jianshu Zhao,Zixi Zhu,Xiaomin Zhang,Qi Liang,Ao Zhou,Shangguang Wang*

Main category: cs.SE

TL;DR: SateLight是一个专为卫星计算设计的应用更新框架，通过容器化、差分更新和容错机制，显著减少传输延迟并确保更新可靠性


<details>
  <summary>Details</summary>
Motivation: 卫星计算需要应用软件更新，但现有地面系统更新方法无法满足卫星环境的异构性、带宽限制和恶劣空间条件等约束

Method: 采用容器化封装异构应用，集成内容感知差分策略减少通信数据量，细粒度星上更新设计重构目标应用，以及基于层的容错恢复机制

Result: 在卫星模拟环境中，相比最佳基线传输延迟降低91.18%（平均56.54%），所有评估应用实现100%更新正确性，真实在轨卫星案例验证了实用性

Conclusion: SateLight为卫星计算提供了一种实用有效的应用更新解决方案，成功解决了卫星环境下的软件更新挑战

Abstract: Satellite computing is an emerging paradigm that empowers satellites to
perform onboard processing tasks (i.e., \textit{satellite applications}),
thereby reducing reliance on ground-based systems and improving responsiveness.
However, enabling application software updates in this context remains a
fundamental challenge due to application heterogeneity, limited
ground-to-satellite bandwidth, and harsh space conditions. Existing software
update approaches, designed primarily for terrestrial systems, fail to address
these constraints, as they assume abundant computational capacity and stable
connectivity.
  To address this gap, we propose SateLight, a practical and effective
satellite application update framework tailored for satellite computing.
SateLight leverages containerization to encapsulate heterogeneous applications,
enabling efficient deployment and maintenance. SateLight further integrates
three capabilities: (1) a content-aware differential strategy that minimizes
communication data volume, (2) a fine-grained onboard update design that
reconstructs target applications, and (3) a layer-based fault-tolerant recovery
mechanism to ensure reliability under failure-prone space conditions.
Experimental results on a satellite simulation environment with 10
representative satellite applications demonstrate that SateLight reduces
transmission latency by up to 91.18% (average 56.54%) compared to the best
currently available baseline. It also consistently ensures 100% update
correctness across all evaluated applications. Furthermore, a case study on a
real-world in-orbit satellite demonstrates the practicality of our approach.

</details>


### [10] [Evaluating Large Language Models for Code Translation: Effects of Prompt Language and Prompt Design](https://arxiv.org/abs/2509.12973)
*Aamer Aljagthami,Mohammed Banabila,Musab Alshehri,Mohammed Kabini,Mohammad D. Alahmadi*

Main category: cs.SE

TL;DR: 本研究系统评估了大型语言模型在C++、Java、Python和C#之间的代码翻译性能，发现详细提示和英文提示能显著提升翻译质量，所有LLM均优于传统TransCoder基准。


<details>
  <summary>Details</summary>
Motivation: 虽然LLM在自动化源代码翻译方面显示出潜力，但关于模型选择、提示设计和提示语言对跨编程语言翻译质量影响的比较证据仍然有限。

Method: 使用BLEU和CodeBLEU指标，在两种提示风格（简洁指令和详细规范）和两种提示语言（英语和阿拉伯语）下，对最先进的LLM进行系统实证评估，并与传统基线TransCoder进行比较。

Result: 详细提示在所有模型和翻译方向上带来一致提升，英文提示比阿拉伯语提示性能高13-15%。最佳模型在Java到C#和Python到C++等挑战性语言对上获得最高CodeBLEU分数，所有LLM均超越TransCoder基准。

Conclusion: 研究证明了精心设计的提示工程和提示语言选择的重要性，为软件现代化和跨语言互操作性提供了实用指导。

Abstract: Large language models (LLMs) have shown promise for automated source-code
translation, a capability critical to software migration, maintenance, and
interoperability. Yet comparative evidence on how model choice, prompt design,
and prompt language shape translation quality across multiple programming
languages remains limited. This study conducts a systematic empirical
assessment of state-of-the-art LLMs for code translation among C++, Java,
Python, and C#, alongside a traditional baseline (TransCoder). Using BLEU and
CodeBLEU, we quantify syntactic fidelity and structural correctness under two
prompt styles (concise instruction and detailed specification) and two prompt
languages (English and Arabic), with direction-aware evaluation across language
pairs. Experiments show that detailed prompts deliver consistent gains across
models and translation directions, and English prompts outperform Arabic by
13-15%. The top-performing model attains the highest CodeBLEU on challenging
pairs such as Java to C# and Python to C++. Our evaluation shows that each LLM
outperforms TransCoder across the benchmark. These results demonstrate the
value of careful prompt engineering and prompt language choice, and provide
practical guidance for software modernization and cross-language
interoperability.

</details>


### [11] [Validating Solidity Code Defects using Symbolic and Concrete Execution powered by Large Language Models](https://arxiv.org/abs/2509.13023)
*Ştefan-Claudiu Susan,Andrei Arusoaie,Dorel Lucanu*

Main category: cs.SE

TL;DR: 提出了一种结合Slither检测器、LLMs、Kontrol和Forge的新型智能合约漏洞检测管道，能够可靠检测缺陷并生成证明，显著减少误报和人工验证负担。


<details>
  <summary>Details</summary>
Motivation: 静态分析工具和大型语言模型在Solidity智能合约漏洞检测中存在高误报率，需要能够形式化或经验证明缺陷存在的方法。

Method: 集成定制Slither检测器、LLMs、Kontrol和Forge的检测管道，通过符号执行或具体执行来正确分类代码故障。

Result: 对七种关键缺陷类型进行了实验，展示了在重入、复杂回调和错误访问控制策略等三种漏洞上的有效性，能够有效验证真阳性。

Conclusion: 尽管存在LLMs不一致性和成本等潜在限制，但该方法为结合启发式分析和形式验证实现更可靠的智能合约审计建立了稳健框架。

Abstract: The high rate of false alarms from static analysis tools and Large Language
Models (LLMs) complicates vulnerability detection in Solidity Smart Contracts,
demanding methods that can formally or empirically prove the presence of
defects. This paper introduces a novel detection pipeline that integrates
custom Slither-based detectors, LLMs, Kontrol, and Forge. Our approach is
designed to reliably detect defects and generate proofs. We currently perform
experiments with promising results for seven types of critical defects. We
demonstrate the pipeline's efficacy by presenting our findings for three
vulnerabilities -- Reentrancy, Complex Fallback, and Faulty Access Control
Policies -- that are challenging for current verification solutions, which
often generate false alarms or fail to detect them entirely. We highlight the
potential of either symbolic or concrete execution in correctly classifying
such code faults. By chaining these instruments, our method effectively
validates true positives, significantly reducing the manual verification
burden. Although we identify potential limitations, such as the inconsistency
and the cost of LLMs, our findings establish a robust framework for combining
heuristic analysis with formal verification to achieve more reliable and
automated smart contract auditing.

</details>


### [12] [GView: A Survey of Binary Forensics via Visual, Semantic, and AI-Enhanced Analysis](https://arxiv.org/abs/2509.13025)
*Raul Zaharia,Dragoş Gavriluţ,Gheorghiţă Mutu*

Main category: cs.SE

TL;DR: GView是一个开源的网络安全取证分析框架，结合可视化界面和AI增强推理，利用大语言模型动态改进推理能力，简化取证工作流程。


<details>
  <summary>Details</summary>
Motivation: 应对日益复杂和多样化的网络安全威胁，需要更先进的取证分析工具来提升分析效率和准确性。

Method: 开发开源取证分析框架，集成可视化界面、大语言模型（LLMs）、逻辑推理（谓词和推理规则），支持动态推理增强和用户行为分析。

Result: GView框架已发表多篇论文并持续发展，展示了在实践取证和学术研究之间的桥梁作用，具有可扩展架构。

Conclusion: GView框架通过AI和可视化技术的结合，有效提升了网络安全取证分析的效率和智能化水平，具有重要的实践和学术价值。

Abstract: Cybersecurity threats continue to become more sophisticated and diverse in
their artifacts, boosting both their volume and complexity. To overcome those
challenges, we present GView, an open-source forensic analysis framework with
visual and AI-enhanced reasoning. It started with focus on the practical
cybersecurity industry. It has evolved significantly, incorporating large
language models (LLMs) to dynamically enhance reasoning and ease the forensic
workflows. This paper surveys both the current state of GView with its
published papers alongside those that are in the publishing process. It also
includes its innovative use of logical inference through predicates and
inference rules for both the analyzed documents and the user's actions for
better suggestions. We highlight the extensible architecture, showcasing its
potential as a bridge between the practical forensics worlds with the academic
research.

</details>


### [13] [Automating Code Generation for Semiconductor Equipment Control from Developer Utterances with LLMs](https://arxiv.org/abs/2509.13055)
*Youngkyoung Kim,Sanghyeok Park,Misoo Kim,Gangho Yoon,Eunseok Lee,Simon S. Woo*

Main category: cs.SE

TL;DR: 提出渐进式知识增强(PKE)框架，通过多阶段提示激活LLM潜在知识，在不需大量微调的情况下提升二级设备语言ALPG的代码生成能力


<details>
  <summary>Details</summary>
Motivation: 半导体设备语言(如ALPG)编程难度高，而现有LLM在这些低级语言上效果有限，需要一种无需大规模微调的解决方案

Method: 设计了渐进式知识增强(PKE)框架，通过多阶段提示步骤，从简单到复杂的示例渐进式提取和激活LLM内部的潜在知识

Result: 在工业级ALPG数据集上，PKE在生成正确ALPG代码方面显著超过标准提示和最新方法，比第二佳方法分别提高11.1%和15.2%的准确匹配分数

Conclusion: PKE为专业低级编程语言提供了一种实用的LLM能力提升方案，支持半导体软件开发的生产力提升，逐步知识提取基于难度的方法能够有效提高准确性

Abstract: Semiconductors form the backbone of modern electronics, with their
manufacturing and testing relying on highly specialized equipment and
domain-specific programming languages. Equipment languages such as the
Algorithmic Pattern Generator (ALPG) are critical for precise hardware control
but are challenging to program due to their low-level syntax and steep learning
curve. While large language models (LLMs) have shown promise in generating
high-level code from natural language, their effectiveness on low-level
equipment languages remains limited. To address this, we propose Progressive
Knowledge Enhancement (PKE), a novel multi-stage prompting framework that
progressively extracts and activates the latent knowledge within LLMs, guiding
them from simple to complex examples without extensive fine-tuning. Empirical
evaluation on an industrial ALPG dataset shows that PKE significantly
outperforms standard prompting and surpasses state-of-the-art methods in
generating correct ALPG code, achieving 11.1\% and 15.2\% higher exact match
scores compared to the second-best technique. Further analysis of individual
components confirms that progressive knowledge extraction based on difficulty
enhances accuracy. Our study offer a practical approach to boosting LLM
capabilities for specialized low-level programming, supporting greater
productivity in semiconductor software development.

</details>


### [14] [Accelerating Discovery: Rapid Literature Screening with LLMs](https://arxiv.org/abs/2509.13103)
*Santiago Matalonga,Domenico Amalfitano,Jean Carlo Rossa Hauck,Martín Solari,Guilherme H. Travassos*

Main category: cs.SE

TL;DR: 基于LLM的工具能够有90%的准确率识别与研究无关的文档，有效支持多源文献综述的文档搜索和筛选工作。


<details>
  <summary>Details</summary>
Motivation: 多源文献综述(MVLR)需要审查大量非结构化文档，过程耗时耗力。研究人员在航空电子领域进行上下文感知软件系统测试的MVLR时，需要审查超过8,000份异质性强的文档。

Method: 采用系统工程方法开发了一款本地部署的LLM基础工具，结合检索增强生成(RAG)技术处理候选文档。使用正确一致率(PPA)作为主要指标，通过便利抽样、人工判断和统计抽样来验证工具质量。

Result: 工具目前对于与研究无关的来源文档，与人类研究人员达到了90%的正确一致率。工具开发详情已分享，以支持领域特定的适配。

Conclusion: 使用LLM基础工具支持学术研究人员进行严谨的MVLR是可行的。这些工具可以释放贵重时间用于更高层次的抽象任务，但研究人员的参与仍然至关重要，以确保工具支持完整的研究。

Abstract: Background: Conducting Multi Vocal Literature Reviews (MVLRs) is often time
and effort-intensive. Researchers must review and filter a large number of
unstructured sources, which frequently contain sparse information and are
unlikely to be included in the final study. Our experience conducting an MVLR
on Context-Aware Software Systems (CASS) Testing in the avionics domain
exemplified this challenge, with over 8,000 highly heterogeneous documents
requiring review. Therefore, we developed a Large Language Model (LLM)
assistant to support the search and filtering of documents. Aims: To develop
and validate an LLM based tool that can support researchers in performing the
search and filtering of documents for an MVLR without compromising the rigor of
the research protocol. Method: We applied sound engineering practices to
develop an on-premises LLM-based tool incorporating Retrieval Augmented
Generation (RAG) to process candidate sources. Progress towards the aim was
quantified using the Positive Percent Agreement (PPA) as the primary metric to
ensure the performance of the LLM based tool. Convenience sampling, supported
by human judgment and statistical sampling, were used to verify and validate
the tool's quality-in-use. Results: The tool currently demonstrates a PPA
agreement with human researchers of 90% for sources that are not relevant to
the study. Development details are shared to support domain-specific adaptation
of the tool. Conclusions: Using LLM-based tools to support academic researchers
in rigorous MVLR is feasible. These tools can free valuable time for
higher-level, abstract tasks. However, researcher participation remains
essential to ensure that the tool supports thorough research.

</details>


### [15] [Vulnerability Patching Across Software Products and Software Components: A Case Study of Red Hat's Product Portfolio](https://arxiv.org/abs/2509.13117)
*Jukka Ruohonen,Sani Abdullahi,Abhishek Tiwari*

Main category: cs.SE

TL;DR: 对红帽产品1999-2024年漏洞修复的时间序列分析显示，易受攻击产品和组件数量不稳定，呈现线性增长趋势，与整体漏洞趋势不一致，但近期可能趋于稳定


<details>
  <summary>Details</summary>
Motivation: 基于软件维护和安全债务概念，研究红帽产品漏洞修复的时间模式

Method: 采用分段回归分析对红帽产品组件1999-2024年的漏洞修复数据进行时间序列分析

Result: 易受攻击产品和组件数量不稳定，多数序列呈现线性增长趋势，与整体漏洞趋势不一致，存在明显断点表明线性趋势并非普遍适用

Conclusion: 安全债务可能正在趋于稳定，线性增长模式并非普遍适用

Abstract: Motivated by software maintenance and the more recent concept of security
debt, the paper presents a time series analysis of vulnerability patching of
Red Hat's products and components between 1999 and 2024. According to the
results based on segmented regression analysis, the amounts of vulnerable
products and components have not been stable; a linear trend describes many of
the series well. Nor do the amounts align well with trends characterizing
vulnerabilities in general. There are also visible breakpoints indicating that
the linear trend is not universally applicable and that the growing security
debt may be stabilizing.

</details>


### [16] [Optimizing Code Embeddings and ML Classifiers for Python Source Code Vulnerability Detection](https://arxiv.org/abs/2509.13134)
*Talaya Farasat,Joachim Posegga*

Main category: cs.SE

TL;DR: 这篇论文研究了Python源代码漏洞检测中代码嵌入技术和机器学习分类器的最优组合，发现Word2Vec结合BiLSTM模型表现最佳


<details>
  <summary>Details</summary>
Motivation: 随着源代码复杂度和规模不断增长，手动软件漏洞检测变得越来越不实用，需要自动化方法来应对这一挑战

Method: 评估了三种代码嵌入技术（Word2Vec、CodeBERT、GraphCodeBERT）与两种深度学习分类器（BiLSTM和CNN）的组合性能

Result: CNN与GraphCodeBERT组合表现良好，但BiLSTM使用Word2Vec在总体上达到最佳效果，显示经典嵌入技术在适当模型配置下仍有优势

Conclusion: 选择适当的嵌入技术与分类器组合对于提高Python源代码自动化漏洞检测系统的效果至关重要

Abstract: In recent years, the growing complexity and scale of source code have
rendered manual software vulnerability detection increasingly impractical. To
address this challenge, automated approaches leveraging machine learning and
code embeddings have gained substantial attention. This study investigates the
optimal combination of code embedding techniques and machine learning
classifiers for vulnerability detection in Python source code. We evaluate
three embedding techniques, i.e., Word2Vec, CodeBERT, and GraphCodeBERT
alongside two deep learning classifiers, i.e., Bidirectional Long Short-Term
Memory (BiLSTM) networks and Convolutional Neural Networks (CNN). While CNN
paired with GraphCodeBERT exhibits strong performance, the BiLSTM model using
Word2Vec consistently achieves superior overall results. These findings suggest
that, despite the advanced architectures of recent models like CodeBERT and
GraphCodeBERT, classical embeddings such as Word2Vec, when used with
sequence-based models like BiLSTM, can offer a slight yet consistent
performance advantage. The study underscores the critical importance of
selecting appropriate combinations of embeddings and classifiers to enhance the
effectiveness of automated vulnerability detection systems, particularly for
Python source code.

</details>


### [17] [Towards the Next Generation of Software: Insights from Grey Literature on AI-Native Applications](https://arxiv.org/abs/2509.13144)
*Lingli Cao,Shanshan Li,Ying Fan,Danyang Li,Chenxing Zhong*

Main category: cs.SE

TL;DR: 本研究通过灰色文献综述提出了AI原生应用的双层工程蓝图，明确了其以AI为核心的智能范式和概率性质的两大特征，以及可靠性、易用性等关键质量属性。


<details>
  <summary>Details</summary>
Motivation: AI原生应用作为软件工程新范式发展快速，但缺乏统一的工程定义和架构蓝图，实践者需要系统性指导来进行系统设计、质量保证和技术选型。

Method: 采用灰色文献综述方法，结合从Google和Bing搜索获取的概念观点以及GitHub上领先开源项目的实践见解，通过结构化协议进行来源选择、质量评估和主题分析。

Result: 基于选择标准确定了106份研究，分析显示AI原生应用具有两大核心特征：AI作为系统智能范式的中心作用和内在的概率性、非确定性质。关键质量属性包括可靠性、易用性、性能效率和AI特定的可观测性。具有代表性的技术栈包含LLM组织框架、向量数据库和AI原生可观测平台。

Conclusion: 本研究首次提出了双层工程蓝图，为AI原生应用提供了系统化的工程定义和架构指南，帮助实践者更好地设计、开发和维护这类新型软件系统。

Abstract: Background: The rapid advancement of large language models (LLMs) has given
rise to AI-native applications, a new paradigm in software engineering that
fundamentally redefines how software is designed, developed, and evolved.
Despite their growing prominence, AI-native applications still lack a unified
engineering definition and architectural blueprint, leaving practitioners
without systematic guidance for system design, quality assurance, and
technology selection.
  Objective: This study seeks to establish a comprehensive understanding of
AI-native applications by identifying their defining characteristics, key
quality attributes, and typical technology stacks, as well as by clarifying the
opportunities and challenges they present.
  Method: We conducted a grey literature review, integrating conceptual
perspectives retrieved from targeted Google and Bing searches with practical
insights derived from leading open-source projects on GitHub. A structured
protocol encompassing source selection, quality assessment, and thematic
analysis was applied to synthesize findings across heterogeneous sources.
  Results: We finally identified 106 studies based on the selection criteria.
The analysis reveals that AI-native applications are distinguished by two core
pillars: the central role of AI as the system's intelligence paradigm and their
inherently probabilistic, non-deterministic nature. Critical quality attributes
include reliability, usability, performance efficiency, and AI-specific
observability. In addition, a typical technology stack has begun to emerge,
comprising LLM orchestration frameworks, vector databases, and AI-native
observability platforms. These systems emphasize response quality,
cost-effectiveness, and outcome predictability, setting them apart from
conventional software systems.
  Conclusion: This study is the first to propose a dual-layered engineering
blueprint...

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [18] [Towards Trustworthy Agentic IoEV: AI Agents for Explainable Cyberthreat Mitigation and State Analytics](https://arxiv.org/abs/2509.12233)
*Meryem Malak Dif,Mouhamed Amine Bouchiha,Abdelaziz Amara Korba,Yacine Ghamri-Doudane*

Main category: cs.CR

TL;DR: 提出了一个面向电动汽车物联网的智能AI框架，通过多智能体协作实现自主威胁防护、鲁棒分析和可解释决策支持，显著提升安全性和预测准确性。


<details>
  <summary>Details</summary>
Motivation: 电动汽车物联网面临网络攻击、电池状态预测不可靠和决策过程不透明等挑战，这些因素削弱了系统信任度和性能表现。

Method: 设计包含专用智能体的AAI架构：充电站网络威胁检测响应、实时SoC估计、SoH异常检测，通过可解释推理层协调；开发可解释威胁缓解机制；提出鲁棒的SoC和SoH模型；实现三智能体流水线，使用LLM驱动推理和动态工具调用。

Result: 通过多样化IoEV场景的综合实验验证，在安全性和预测准确性方面取得显著改进。

Conclusion: 该AAI框架有效解决了IoEV系统的关键挑战，所有数据集、模型和代码将公开发布，为电动汽车物联网的安全可靠运行提供了创新解决方案。

Abstract: The Internet of Electric Vehicles (IoEV) envisions a tightly coupled
ecosystem of electric vehicles (EVs), charging infrastructure, and grid
services, yet it remains vulnerable to cyberattacks, unreliable battery-state
predictions, and opaque decision processes that erode trust and performance. To
address these challenges, we introduce a novel Agentic Artificial Intelligence
(AAI) framework tailored for IoEV, where specialized agents collaborate to
deliver autonomous threat mitigation, robust analytics, and interpretable
decision support. Specifically, we design an AAI architecture comprising
dedicated agents for cyber-threat detection and response at charging stations,
real-time State of Charge (SoC) estimation, and State of Health (SoH) anomaly
detection, all coordinated through a shared, explainable reasoning layer;
develop interpretable threat-mitigation mechanisms that proactively identify
and neutralize attacks on both physical charging points and learning
components; propose resilient SoC and SoH models that leverage continuous and
adversarial-aware learning to produce accurate, uncertainty-aware forecasts
with human-readable explanations; and implement a three-agent pipeline, where
each agent uses LLM-driven reasoning and dynamic tool invocation to interpret
intent, contextualize tasks, and execute formal optimizations for user-centric
assistance. Finally, we validate our framework through comprehensive
experiments across diverse IoEV scenarios, demonstrating significant
improvements in security and prediction accuracy. All datasets, models, and
code will be released publicly.

</details>


### [19] [Secure Human Oversight of AI: Exploring the Attack Surface of Human Oversight](https://arxiv.org/abs/2509.12290)
*Jonas C. Ditz,Veronika Lazar,Elmar Lichtmeß,Carola Plesch,Matthias Heck,Kevin Baum,Markus Langer*

Main category: cs.CR

TL;DR: 这篇论文提出人工智能人类监管存在安全漏洞，从网络安全角度分析了攻击途径和防御策略


<details>
  <summary>Details</summary>
Motivation: 目前对AI人类监管的讨论主要集中在效果性，而忽视了其安全性风险，人类监管可能成为AI操作中的新攻击面

Method: 采用网络安全视角，分析威胁有效人类监管的攻击途径，包括攻击AI系统、监管通信渠道或监管人员本身

Result: 识别了多种攻击模式，并提出了相应的防御策略，以保障人类监管的安全性

Conclusion: 需要重视人类监管的安全性，通过网络安全措施加强监管系统，确保AI运作的安全性和可负责性

Abstract: Human oversight of AI is promoted as a safeguard against risks such as
inaccurate outputs, system malfunctions, or violations of fundamental rights,
and is mandated in regulation like the European AI Act. Yet debates on human
oversight have largely focused on its effectiveness, while overlooking a
critical dimension: the security of human oversight. We argue that human
oversight creates a new attack surface within the safety, security, and
accountability architecture of AI operations. Drawing on cybersecurity
perspectives, we analyze attack vectors that threaten the requirements of
effective human oversight, thereby undermining the safety of AI operations.
Such attacks may target the AI system, its communication with oversight
personnel, or the personnel themselves. We then outline hardening strategies to
mitigate these risks. Our contributions are: (1) introducing a security
perspective on human oversight, and (2) providing an overview of attack vectors
and hardening strategies to enable secure human oversight of AI.

</details>


### [20] [Collaborative P4-SDN DDoS Detection and Mitigation with Early-Exit Neural Networks](https://arxiv.org/abs/2509.12291)
*Ouassim Karrakchou,Alaa Zniber,Anass Sebbar,Mounir Ghogho*

Main category: cs.CR

TL;DR: 基于P4编程数据平面和SDN控制平面的协同架构，通过分开早退神经网络实现实时DDoS检测和响应，在保按高检测精度的同时显著降低延迟和控制平面负载。


<details>
  <summary>Details</summary>
Motivation: DDoS攻击对网络安全构成持续威胁，需要及时且可扩展的缓解策略。传统方法在实时检测和响应时间、可扩展性以及控制平面负载方面遇到挑战。

Method: 提出一种协同架构，集成P4编程数据平面和SDN控制平面。核心是分开早退神经网络：在数据平面使用量化CNN进行部分推理，将不确定案例延迟到控制平面的GRU模块进行深入分析。这种设计支持高速分类和复杂流量升级分析。

Result: 使用真实DDoS数据集进行实验评估，该方法实现了高检测精度，同时显著减少了推理延迟和控制平面负载。

Conclusion: 结果表明紧密耦合的ML-P4-SDN系统在高效、适应性和低延迟DDoS防御方面具有强大潜力，为网络安全领域提供了一种有前晨性的解决方案。

Abstract: Distributed Denial of Service (DDoS) attacks pose a persistent threat to
network security, requiring timely and scalable mitigation strategies. In this
paper, we propose a novel collaborative architecture that integrates a
P4-programmable data plane with an SDN control plane to enable real-time DDoS
detection and response. At the core of our approach is a split early-exit
neural network that performs partial inference in the data plane using a
quantized Convolutional Neural Network (CNN), while deferring uncertain cases
to a Gated Recurrent Unit (GRU) module in the control plane. This design
enables high-speed classification at line rate with the ability to escalate
more complex flows for deeper analysis. Experimental evaluation using
real-world DDoS datasets demonstrates that our approach achieves high detection
accuracy with significantly reduced inference latency and control plane
overhead. These results highlight the potential of tightly coupled ML-P4-SDN
systems for efficient, adaptive, and low-latency DDoS defense.

</details>


### [21] [Amulet: a Python Library for Assessing Interactions Among ML Defenses and Risks](https://arxiv.org/abs/2509.12386)
*Asim Waheed,Vasisht Duddu,Rui Zhang,Sebastian Szyller,N. Asokan*

Main category: cs.CR

TL;DR: AMULET是一个Python库，用于评估机器学习模型在不同风险（安全、隐私、公平性）防御措施之间的意外交互作用，帮助从业者和研究人员在部署前进行大规模评估。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型面临安全、隐私和公平性风险，现有防御措施可能无意中影响对其他无关风险的敏感性，需要系统评估这些意外交互作用。

Method: 开发了AMULET库，具有全面性（包含代表性攻击、防御和指标）、可扩展性（模块化设计）、一致性（用户友好API）和适用性（评估未探索的交互作用）的特点。

Result: AMULET库能够评估未探索的意外交互作用，比较防御或攻击措施的有效性，并支持添加新的攻击和防御模块。

Conclusion: AMULET为机器学习从业者和研究人员提供了一个全面的工具，用于系统评估不同风险防御措施之间的意外交互作用，有助于在模型部署前发现潜在问题。

Abstract: ML models are susceptible to risks to security, privacy, and fairness.
Several defenses are designed to protect against their intended risks, but can
inadvertently affect susceptibility to other unrelated risks, known as
unintended interactions. Several jurisdictions are preparing ML regulatory
frameworks that require ML practitioners to assess the susceptibility of ML
models to different risks. A library for valuating unintended interactions that
can be used by (a) practitioners to evaluate unintended interactions at scale
prior to model deployment and (b) researchers to design defenses which do not
suffer from an unintended increase in unrelated risks. Ideally, such a library
should be i) comprehensive by including representative attacks, defenses and
metrics for different risks, ii) extensible to new modules due to its modular
design, iii) consistent with a user-friendly API template for inputs and
outputs, iv) applicable to evaluate previously unexplored unintended
interactions. We present AMULET, a Python library that covers risks to
security, privacy, and fairness, which satisfies all these requirements. AMULET
can be used to evaluate unexplored unintended interactions, compare
effectiveness between defenses or attacks, and include new attacks and
defenses.

</details>


### [22] [Redefining Website Fingerprinting Attacks With Multiagent LLMs](https://arxiv.org/abs/2509.12462)
*Chuxu Song,Dheekshith Dev Manohar Mekala,Hao Wang,Richard Martin*

Main category: cs.CR

TL;DR: 网站指纹识别(WFP)在现代网络环境中面临挑战，传统方法无法满足实际用户行为的复杂性。研究通过LLM多代理系统生成人物驱动的浏览行为流量，在低成本下提升模型准确度从10%到80%。


<details>
  <summary>Details</summary>
Motivation: 现有网站指纹识别方法在现代单页应用(SPA)和脚本化浏览器环境下无法有效概括实际用户的多样化行为特征，导致模型性能差。

Method: 放弃会话边界，采用连续流量分段。使用大语言模型(LLM)多代理系统构建可扩展数据生成流水线，模拟现实的人物驱动浏览行为，成本仅为人工收集的1/3-1/5。

Result: 在30名实际用户浏览20个现代网站的流量上，所有9个状态越模型在脚本化流量训练后对人类数据的准确度均低于10%，而使用LLM生成流量训练后准确度提升至80%范围，显示了对实际追踪的强大概括能力。

Conclusion: 现代WFP模型的性能变得越来越受限于数据质量，可扩展、语义基础的合成流量对捐捕实际用户行为的复杂性至关重要。

Abstract: Website Fingerprinting (WFP) uses deep learning models to classify encrypted
network traffic to infer visited websites. While historically effective, prior
methods fail to generalize to modern web environments. Single-page applications
(SPAs) eliminate the paradigm of websites as sets of discrete pages,
undermining page-based classification, and traffic from scripted browsers lacks
the behavioral richness seen in real user sessions. Our study reveals that
users exhibit highly diverse behaviors even on the same website, producing
traffic patterns that vary significantly across individuals. This behavioral
entropy makes WFP a harder problem than previously assumed and highlights the
need for larger, more diverse, and representative datasets to achieve robust
performance. To address this, we propose a new paradigm: we drop
session-boundaries in favor of contiguous traffic segments and develop a
scalable data generation pipeline using large language models (LLM) agents.
These multi-agent systems coordinate decision-making and browser interaction to
simulate realistic, persona-driven browsing behavior at 3--5x lower cost than
human collection. We evaluate nine state-of-the-art WFP models on traffic from
20 modern websites browsed by 30 real users, and compare training performance
across human, scripted, and LLM-generated datasets. All models achieve under
10\% accuracy when trained on scripted traffic and tested on human data. In
contrast, LLM-generated traffic boosts accuracy into the 80\% range,
demonstrating strong generalization to real-world traces. Our findings indicate
that for modern WFP, model performance is increasingly bottlenecked by data
quality, and that scalable, semantically grounded synthetic traffic is
essential for capturing the complexity of real user behavior.

</details>


### [23] [QKD Oracles for Authenticated Key Exchange](https://arxiv.org/abs/2509.12478)
*Kathrin Hövelmanns,Daan Planken,Christian Schaffner,Sebastian R. Verschoor*

Main category: cs.CR

TL;DR: 本文分析了结合后量子认证密钥交换(AKE)和量子密钥分发(QKD)的安全协议，发现了现有安全模型在处理QKD密钥ID时的漏洞，提出了新的QKD预言机模型，并在CK+模型中证明了新协议的安全性。


<details>
  <summary>Details</summary>
Motivation: 结合后量子AKE和QKD可以提供量子攻击防护，但现有安全分析在处理QKD密钥ID方面存在缺陷，可能导致依赖密钥攻击。

Method: 将QKD建模为类似ETSI 014标准接口的预言机，集成到CK+安全模型中，构建新的混合协议并形式化证明其安全性。

Result: 发现了QKD密钥ID处理不当导致的依赖密钥攻击漏洞，提出了首个可证明安全且保持QKD信息论安全性的混合协议。

Conclusion: 通过适当的QKD建模和安全分析，可以构建既保持QKD信息论安全性又能抵抗量子攻击的混合密钥交换协议。

Abstract: Authenticated Key Exchange (AKE) establishes shared ('symmetric')
cryptographic keys which are essential for secure online communication. AKE
protocols can be constructed from public-key cryptography like Key
Encapsulation Mechanisms (KEMs). Another approach is to use Quantum Key
Distribution (QKD) to establish a symmetric key, which uses quantum
communication. Combining post-quantum AKE and QKD appropriately may provide
security against quantum attacks even if only one of the two approaches turns
out to be secure.
  We provide an extensive review of existing security analyses for combined AKE
and their formal security models, and identify some gaps in their treatment of
QKD key IDs. In particular, improper handling of QKD key IDs leads to
Dependent-Key attacks on AKE.
  As our main conceptual contribution, we model QKD as an oracle that closely
resembles the standard ETSI 014 QKD interface. We demonstrate the usability of
our QKD oracle for cryptographic security analyses by integrating it into a
prominent security model for AKE, called CK+ model, thereby obtaining a
security model for combined AKE that catches Dependent-Key attacks. In this
model, we formally prove security of a new protocol that combines QKD with a
triple-KEM handshake. This is the first provably secure hybrid protocol that
maintains information-theoretic security of QKD.

</details>


### [24] [Towards Closing the Performance Gap for Cryptographic Kernels Between CPUs and Specialized Hardware](https://arxiv.org/abs/2509.12494)
*Naifeng Zhang,Sophia Fu,Franz Franchetti*

Main category: cs.CR

TL;DR: 该研究通过优化x86 CPU的标量实现、利用SIMD指令和提出新的AVX-512扩展MQX，显著缩小了CPU与ASIC在密码学内核性能上的差距。


<details>
  <summary>Details</summary>
Motivation: 专用硬件(ASIC)一直是密码学内核的主要加速器，虽然GPU能达到接近ASIC的性能，但CPU在这方面的性能差距仍然很大，需要研究如何缩小这一差距。

Method: 开发了优化的x86 CPU标量实现，使用AVX2和AVX-512 SIMD指令，并提出新的MQX扩展指令集(仅需3条新指令)，进行roofline分析评估多核扩展性能。

Result: NTT和BLAS操作分别比现有CPU基线平均加速38倍和62倍，MQX将单核CPU相对于ASIC的减速降低到35倍，服务器级CPU可接近ASIC性能。

Conclusion: 通过优化的SIMD实现和MQX扩展指令，CPU能够显著缩小与专用硬件在密码学工作负载上的性能差距，使高端服务器CPU能够接近ASIC的性能水平。

Abstract: Specialized hardware like application-specific integrated circuits (ASICs)
remains the primary accelerator type for cryptographic kernels based on large
integer arithmetic. Prior work has shown that commodity and server-class GPUs
can achieve near-ASIC performance for these workloads. However, achieving
comparable performance on CPUs remains an open challenge. This work
investigates the following question: How can we narrow the performance gap
between CPUs and specialized hardware for key cryptographic kernels like basic
linear algebra subprograms (BLAS) operations and the number theoretic transform
(NTT)?
  To this end, we develop an optimized scalar implementation of these kernels
for x86 CPUs at the per-core level. We utilize SIMD instructions (specifically
AVX2 and AVX-512) to further improve performance, achieving an average speedup
of 38 times and 62 times over state-of-the-art CPU baselines for NTTs and BLAS
operations, respectively. To narrow the gap further, we propose a small AVX-512
extension, dubbed multi-word extension (MQX), which delivers substantial
speedup with only three new instructions and minimal proposed hardware
modifications. MQX cuts the slowdown relative to ASICs to as low as 35 times on
a single CPU core. Finally, we perform a roofline analysis to evaluate the peak
performance achievable with MQX when scaled across an entire multi-core CPU.
Our results show that, with MQX, top-tier server-grade CPUs can approach the
performance of state-of-the-art ASICs for cryptographic workloads.

</details>


### [25] [Exploiting Timing Side-Channels in Quantum Circuits Simulation Via ML-Based Methods](https://arxiv.org/abs/2509.12535)
*Ben Dong,Hui Feng,Qian Wang*

Main category: cs.CR

TL;DR: 本文提出了一种针对云端量子模拟器的新型时序侧信道攻击，恶意进程可以通过观测执行时序模式来提取并发运行的量子电路的敏感信息，识别率可达88%-99.9%。


<details>
  <summary>Details</summary>
Motivation: 随着量子计算的发展，量子电路模拟器在云端平台部署，用户提交专有电路设计进行模拟。当前缺乏对这些模拟器安全风险的研究，特别是时序侧信道攻击的威胁。

Method: 使用QASMBench基准测试套件系统分析模拟器行为，分析不同电路执行的时序和内存特征。利用模式识别技术对时序特征进行分类，以推断电路身份。

Result: 实验结果显示时序特征表现出电路依赖性模式，能够有效进行分类。在不同数据集上实现了88%到99.9%的量子电路识别率。

Conclusion: 这项工作揭示了量子模拟环境中先前未被探索的安全风险，呼吁需要更强的隔离机制来保护用户工作负载的安全。

Abstract: As quantum computing advances, quantum circuit simulators serve as critical
tools to bridge the current gap caused by limited quantum hardware
availability. These simulators are typically deployed on cloud platforms, where
users submit proprietary circuit designs for simulation. In this work, we
demonstrate a novel timing side-channel attack targeting cloud-based quantum
simulators. A co-located malicious process can observe fine-grained execution
timing patterns to extract sensitive information about concurrently running
quantum circuits. We systematically analyze simulator behavior using the
QASMBench benchmark suite, profiling timing and memory characteristics across
various circuit executions. Our experimental results show that timing profiles
exhibit circuit-dependent patterns that can be effectively classified using
pattern recognition techniques, enabling the adversary to infer circuit
identities and compromise user confidentiality. We were able to achieve 88% to
99.9% identification rate of quantum circuits based on different datasets. This
work highlights previously unexplored security risks in quantum simulation
environments and calls for stronger isolation mechanisms to protect user
workloads

</details>


### [26] [Yet Another Watermark for Large Language Models](https://arxiv.org/abs/2509.12574)
*Siyuan Bao,Ying Shi,Zhiguang Yang,Hanzhou Wu,Xinpeng Zhang*

Main category: cs.CR

TL;DR: 本文提出了一种新的大语言模型水印框架，通过操控模型内部参数来嵌入水印，在黑盒场景下也能高效提取，并且更好地平衡了水印的稳健性和隐藏性。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM水印方法主要通过调整词汇采样预测或后处理来嵌入水印，缺乏与模型的本质耦合，很可能降低生成文本的语义质量。而基于训练或微调的传统水印方法要么限于白盒场景，要么因模型参数过多而非常耗时。

Method: 提出一种新的水印框架，通过操控LLM的内部参数来嵌入水印，并能够在不访问LLM的情况下从生成的文本中提取水印。该方法将水印与LLM的本质参数缠绕在一起。

Result: 实验结果验证了该方法的可行性、优劣性和实用性。与相关方法相比，该方法更好地平衡了水印的稳健性和隐藏性，并且允许在黑盒场景下进行计算效率高的水印提取。

Conclusion: 这项工作提供了与主流研究不同的新视角，为未来研究提供了启示。通过操控LLM内部参数来嵌入水印的方法，有望在保持文本语义质量的同时实现高效的水印插入和提取。

Abstract: Existing watermarking methods for large language models (LLMs) mainly embed
watermark by adjusting the token sampling prediction or post-processing,
lacking intrinsic coupling with LLMs, which may significantly reduce the
semantic quality of the generated marked texts. Traditional watermarking
methods based on training or fine-tuning may be extendable to LLMs. However,
most of them are limited to the white-box scenario, or very time-consuming due
to the massive parameters of LLMs. In this paper, we present a new watermarking
framework for LLMs, where the watermark is embedded into the LLM by
manipulating the internal parameters of the LLM, and can be extracted from the
generated text without accessing the LLM. Comparing with related methods, the
proposed method entangles the watermark with the intrinsic parameters of the
LLM, which better balances the robustness and imperceptibility of the
watermark. Moreover, the proposed method enables us to extract the watermark
under the black-box scenario, which is computationally efficient for use.
Experimental results have also verified the feasibility, superiority and
practicality. This work provides a new perspective different from mainstream
works, which may shed light on future research.

</details>


### [27] [Secure and Efficient Out-of-band Call Metadata Transmission](https://arxiv.org/abs/2509.12582)
*David Adei,Varun Madathil,Nithin Shyam S.,Bradley Reaves*

Main category: cs.CR

TL;DR: Sidecar是一个分布式隐私保护系统，通过安全带外信令扩展STIR/SHAKEN框架到所有电话网络技术，解决了现有方案泄露敏感元数据的问题，提供更好的隐私保护和安全性。


<details>
  <summary>Details</summary>
Motivation: 现有STIR/SHAKEN框架无法有效对抗电话滥用，且遗留非VoIP基础设施无法参与，行业解决方案会以明文向第三方广播敏感元数据，缺乏隐私保护和数据过期机制。

Method: 提出安全带外信令概念，设计可扩展协议并在通用可组合性框架中证明其安全性，开发开源参考实现。

Result: Sidecar保护用户身份和提供商商业秘密，保证记录过期，减少资源需求同时提供相近的呼叫建立时间和更好的可用性，支持按使用付费计费和异常检测。

Conclusion: Sidecar不仅优于现有解决方案，而且是改造碎片化全球电话系统的变革性工具，可支持更强的呼叫认证和品牌呼叫等未来改进。

Abstract: The STIR/SHAKEN (S/S) attestation Framework mandated by the United States,
Canada, and France to combat pervasive telephone abuse has not achieved its
goals, partly because legacy non-VoIP infrastructure could not participate. The
industry solution to extend S/S broadcasts sensitive metadata of every non-VoIP
call in plaintext to every third party required to facilitate the system. It
has no mechanism to determine whether a provider's request for call data is
appropriate, nor can it ensure that every copy of that call data is unavailable
after its specified expiration. It threatens subscriber privacy and provider
confidentiality.
  In this paper, we present Sidecar, a distributed, privacy-preserving system
with tunable decentralization that securely extends S/S across all telephone
network technologies. We introduce the notion of secure out-of-band signaling
for telephony and formalize its system and security requirements. We then
design novel, scalable protocols that realize these requirements and prove
their security within the Universal Composability framework. Finally, we
demonstrate Sidecar's efficiency with our open-sourced reference
implementation. Compared to the current solution, Sidecar 1) protects the
confidentiality of subscriber identity and provider trade secrets, 2)
guarantees record expiration as long as a single node handling a record is
honest, 3) reduces resource requirements while providing virtually identical
call-setup times and equivalent or better uptimes, and 4) enables secure
pay-per-use billing and integrates mechanisms to mitigate and detect
misbehavior. Moreover, Sidecar can be extended to provide the same security
guarantees for arbitrary call metadata. Not only is Sidecar a superior
approach, it is also a transformative tool to retrofit fragmented global
telephony and enable future improvements, such as stronger call authentication
and Branded Calling.

</details>


### [28] [A Systematic Evaluation of Parameter-Efficient Fine-Tuning Methods for the Security of Code LLMs](https://arxiv.org/abs/2509.12649)
*Kiho Lee,Jungkon Kim,Doowon Kim,Hyoungshick Kim*

Main category: cs.CR

TL;DR: 通过参数高效细调技术（PEFT）提升代码生成模型的安全性，推荐提示微调为最优方案，在CodeGen2 16B上实现了80.86%的安全代码生成率


<details>
  <summary>Details</summary>
Motivation: 代码生成大语言模型经常生成不安全代码，带来严重安全风险，需要找到有效方法提升生成代码的安全性

Method: 综合评估七种参数高效细调技术（PEFT），包括提示微调、前缀微调等，并通过采样温度优化解码策略

Result: 提示微调表现最佳，在CodeGen2 16B上实现80.86%的安全代码生成率（比基线提升13.5%），通过温度优化可达87.65%，相当于每百万个代码片段减少203,700个漏洞

Conclusion: 提示微调是最有效的PEFT方法，能显著提升代码生成的安全性且不影响功能性，同时增强了对毒化攻击的抵御能力，在Python和Java上都有良好的通用性

Abstract: Code-generating Large Language Models (LLMs) significantly accelerate
software development. However, their frequent generation of insecure code
presents serious risks. We present a comprehensive evaluation of seven
parameter-efficient fine-tuning (PEFT) techniques, demonstrating substantial
gains in secure code generation without compromising functionality. Our
research identifies prompt-tuning as the most effective PEFT method, achieving
an 80.86% Overall-Secure-Rate on CodeGen2 16B, a 13.5-point improvement over
the 67.28% baseline. Optimizing decoding strategies through sampling
temperature further elevated security to 87.65%. This equates to a reduction of
approximately 203,700 vulnerable code snippets per million generated. Moreover,
prompt and prefix tuning increase robustness against poisoning attacks in our
TrojanPuzzle evaluation, with strong performance against CWE-79 and CWE-502
attack vectors. Our findings generalize across Python and Java, confirming
prompt-tuning's consistent effectiveness. This study provides essential
insights and practical guidance for building more resilient software systems
with LLMs.

</details>


### [29] [Hardened CTIDH: Dummy-Free and Deterministic CTIDH](https://arxiv.org/abs/2509.12877)
*Gustavo Banegas,Andreas Hellenbrand,Matheus Saldanha*

Main category: cs.CR

TL;DR: 首个假操作免除的dCTIDH实现，结合DACsHUND和改进的Matryoshka结构，在保持常数时间的同时完全消除假操作，性能超过CTIDH约5%


<details>
  <summary>Details</summary>
Motivation: CTIDH和dCTIDH依赖差分加法链和Matryoshka结构中的假操作，这些假操作可能被故障注入攻击利用，需要开发假操作免除的安全实现

Method: 结合DACsHUND技术（在批处理内强制相同长度差分加法链无需填充）和重构的Matryoshka结构（移除假乘法并验证所有中间点），并重新设计参数集避免小素数的限制

Result: 实现了dCTIDH-2048-194和dCTIDH-2048-205的假操作免除版本，群作用成本约35.7-36.2万次Fp乘法，中位评估时间1.59-1.60Gcyc，超过CTIDH约5%，比dCSIDH快4倍以上

Conclusion: 成功开发了首个高效的CSIDH类协议，同时具备确定性、常数时间和完全假操作免除的特性，为后量子加密提供了更安全的选择

Abstract: Isogeny-based cryptography has emerged as a promising postquantum
alternative, with CSIDH and its constant-time variants CTIDH and dCTIDH
offering efficient group-action protocols. However, CTIDH and dCTIDH rely on
dummy operations in differential addition chains (DACs) and Matryoshka, which
can be exploitable by fault-injection attacks. In this work, we present the
first dummy-free implementation of dCTIDH. Our approach combines two recent
ideas: DACsHUND, which enforces equal-length DACs within each batch without
padding, and a reformulated Matryoshka structure that removes dummy
multiplications and validates all intermediate points. Our analysis shows that
small primes such as 3, 5, and 7 severely restrict feasible DACsHUND
configurations, motivating new parameter sets that exclude them. We implement
dummy-free dCTIDH-2048-194 and dCTIDH-2048-205, achieving group action costs of
roughly 357,000-362,000 Fp-multiplications, with median evaluation times of
1.59-1.60 (Gcyc). These results do not surpass dC-TIDH, but they outperform
CTIDH by roughly 5% while eliminating dummy operations entirely. Compared to
dCSIDH, our construction is more than 4x faster. To the best of our knowledge,
this is the first efficient implementation of a CSIDH-like protocol that is
simultaneously deterministic, constant-time, and fully dummy-free.

</details>


### [30] [A Fault Analysis on SNOVA](https://arxiv.org/abs/2509.12879)
*Gustavo Banegas,Ricardo Villanueva-Polanco*

Main category: cs.CR

TL;DR: 对SNOVA后量子签名方案进行全面的故障分析，揭示了仅需22-68个故障签名即可恢复密钥的漏洞，并提出了轻量级防护措施


<details>
  <summary>Details</summary>
Motivation: SNOVA作为NIST后量子密码标准化第二轮候选方案，虽然效率高且密钥尺寸小，但其在故障攻击下的安全性尚未得到充分研究，需要评估其抗故障攻击能力

Method: 采用永久性和瞬时性故障注入策略，利用SNOVA结构特点进行密钥恢复；提出新型故障辅助协调攻击，通过求解二次多项式系统提取密钥空间；进行仿真验证

Result: 分析显示根据安全级别不同，仅需22-68个故障签名即可实现密钥恢复；瞬时故障在关键签名生成步骤中能显著破坏SNOVA安全性

Conclusion: 研究强调了在SNOVA等后量子密码方案中实施故障抵抗机制的重要性，以确保方案的鲁棒性；提出的轻量级防护措施能有效降低故障攻击成功率且不增加显著开销

Abstract: SNOVA is a post-quantum cryptographic signature scheme known for its
efficiency and compact key sizes, making it a second-round candidate in the
NIST post-quantum cryptography standardization process. This paper presents a
comprehensive fault analysis of SNOVA, focusing on both permanent and transient
faults during signature generation. We introduce several fault injection
strategies that exploit SNOVA's structure to recover partial or complete secret
keys with limited faulty signatures. Our analysis reveals that as few as 22 to
68 faulty signatures, depending on the security level, can suffice for key
recovery. We propose a novel fault-assisted reconciliation attack,
demonstrating its effectiveness in extracting the secret key space via solving
a quadratic polynomial system. Simulations show transient faults in key
signature generation steps can significantly compromise SNOVA's security. To
address these vulnerabilities, we propose a lightweight countermeasure to
reduce the success of fault attacks without adding significant overhead. Our
results highlight the importance of fault-resistant mechanisms in post-quantum
cryptographic schemes like SNOVA to ensure robustness.

</details>


### [31] [EByFTVeS: Efficient Byzantine Fault Tolerant-based Verifiable Secret-sharing in Distributed Privacy-preserving Machine Learning](https://arxiv.org/abs/2509.12899)
*Zhen Li,Zijian Zhang,Wenjin Yang,Pengbo Wang,Zhaoqi Wang,Meng Li,Yan Wu,Xuyang Liu,Jing Sun,Liehuang Zhu*

Main category: cs.CR

TL;DR: 本文提出了一种针对现有VSS方案的适应性份额延迟供应攻击(ACuMPA)，并设计了一个高效的拜占庭容错可验证秘密共享方案(EByFTVeS)来应对此类攻击，在保证安全性同时提升效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于VSS的分布式隐私保护机器学习方案存在一致性问题以及计算通信负担重的问题，虽然BFT系统被引入来改善，但作者发现仍然存在安全漏洞。

Method: 提出了ASDP策略和ACuMPA攻击算法，然后设计了EByFTVeS方案，该方案具有拜占庭容错能力，并通过理论分析和实验验证其性能。

Result: EByFTVeS方案在有效性、活性、一致性和隐私性方面都得到了理论保证，并且在效率上优于现有最先进的VSS方案。

Conclusion: 本文不仅揭示了现有VSS方案的安全漏洞，还提出了一个高效且安全的替代方案EByFTVeS，为分布式隐私保护机器学习提供了更好的安全保障。

Abstract: Verifiable Secret Sharing (VSS) has been widespread in Distributed
Privacy-preserving Machine Learning (DPML), because invalid shares from
malicious dealers or participants can be recognized by verifying the commitment
of the received shares for honest participants. However, the consistency and
the computation and communitation burden of the VSS-based DPML schemes are
still two serious challenges. Although Byzantine Fault Tolerance (BFT) system
has been brought to guarantee the consistency and improve the efficiency of the
existing VSS-based DPML schemes recently, we explore an Adaptive Share Delay
Provision (ASDP) strategy, and launch an ASDP-based Customized Model Poisoning
Attack (ACuMPA) for certain participants in this paper. We theoretically
analyzed why the ASDP strategy and the ACuMPA algorithm works to the existing
schemes. Next, we propose an [E]fficient [By]zantine [F]ault [T]olerant-based
[Ve]rifiable [S]ecret-sharing (EByFTVeS) scheme. Finally, the validity,
liveness, consistency and privacy of the EByFTVeS scheme are theoretically
analyzed, while the efficiency of the EByFTVeS scheme outperforms that of
the-state-of-art VSS scheme according to comparative experiment results.

</details>


### [32] [A Graph-Based Approach to Alert Contextualisation in Security Operations Centres](https://arxiv.org/abs/2509.12923)
*Magnus Wiik Eckhoff,Peter Marius Flydal,Siem Peters,Martin Eian,Jonas Halvorsen,Vasileios Mavroeidis,Gudmund Grov*

Main category: cs.CR

TL;DR: 通过图结构方法将安全警报聚合成组，利用图匹配网络将新警报组与历史事件关联，提升SOC警报上下文分析效果


<details>
  <summary>Details</summary>
Motivation: 解决安全运营中心(SOC)面临的大量警报解释挑战，需要有效地进行上下文化处理，快速区分真实威胁和良性活动

Method: 提出基于图的警报聚合方法，将警报作为节点，在定义时间窗口内建立关系边，形成警报组，然后使用图匹配网络(GMN)将入便警报组与历史事件相关联

Result: 通过警报分组实现了更高抽象层次的分析，更有效地捕捉攻击步骤，为下游机器学习方法提供了良好的数据格式

Conclusion: 图基警报聚合方法能够有效提升SOC警报上下文化能力，为分析师提供更多见解，具有实际应用价值

Abstract: Interpreting the massive volume of security alerts is a significant challenge
in Security Operations Centres (SOCs). Effective contextualisation is
important, enabling quick distinction between genuine threats and benign
activity to prioritise what needs further analysis.This paper proposes a
graph-based approach to enhance alert contextualisation in a SOC by aggregating
alerts into graph-based alert groups, where nodes represent alerts and edges
denote relationships within defined time-windows. By grouping related alerts,
we enable analysis at a higher abstraction level, capturing attack steps more
effectively than individual alerts. Furthermore, to show that our format is
well suited for downstream machine learning methods, we employ Graph Matching
Networks (GMNs) to correlate incoming alert groups with historical incidents,
providing analysts with additional insights.

</details>


### [33] [Jailbreaking Large Language Models Through Content Concretization](https://arxiv.org/abs/2509.12937)
*Johan Wahréus,Ahmed Hussain,Panos Papadimitratos*

Main category: cs.CR

TL;DR: 提出Content Concretization(CC)技术，通过两阶段迭代将抽象恶意请求转化为具体可执行代码，显著提高LLM越狱成功率


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的安全机制仍然容易通过不同越狱技术被绕过，需要研究新的攻击方法来揭示安全漏洞

Method: 两阶段过程：首先使用低层级安全过滤模型生成初始响应，然后通过处理初步输出和原始提示的高层级模型进行精炼

Result: 在350个网络安全特定提示上测试，越狱成功率从7%提升至62%，每次提示成本仅7.5美分，生成的代码需要最少修改即可执行

Conclusion: 研究结果突显了当前LLM安全框架的关键漏洞，需要改进有害代码生成的防护机制

Abstract: Large Language Models (LLMs) are increasingly deployed for task automation
and content generation, yet their safety mechanisms remain vulnerable to
circumvention through different jailbreaking techniques. In this paper, we
introduce \textit{Content Concretization} (CC), a novel jailbreaking technique
that iteratively transforms abstract malicious requests into concrete,
executable implementations. CC is a two-stage process: first, generating
initial LLM responses using lower-tier, less constrained safety filters models,
then refining them through higher-tier models that process both the preliminary
output and original prompt. We evaluate our technique using 350
cybersecurity-specific prompts, demonstrating substantial improvements in
jailbreak Success Rates (SRs), increasing from 7\% (no refinements) to 62\%
after three refinement iterations, while maintaining a cost of 7.5\textcent~per
prompt. Comparative A/B testing across nine different LLM evaluators confirms
that outputs from additional refinement steps are consistently rated as more
malicious and technically superior. Moreover, manual code analysis reveals that
generated outputs execute with minimal modification, although optimal
deployment typically requires target-specific fine-tuning. With eventual
improved harmful code generation, these results highlight critical
vulnerabilities in current LLM safety frameworks.

</details>


### [34] [xRWA: A Cross-Chain Framework for Interoperability of Real-World Assets](https://arxiv.org/abs/2509.12957)
*Yihao Guo,Haoming Zhu,Minghui Xu,Xiuzhen Cheng,Bin Xiao*

Main category: cs.CR

TL;DR: 一种重点关注身份管理和验证的跨链实际资产框架，通过集成去中心化标识符和可验证凭证来解决多链环境下的重复验证问题，并设计了无需关闭额外渠道的结算方案


<details>
  <summary>Details</summary>
Motivation: 实际资产(RWAs)在多链部署时面临重复认证和多步结算协议导致的效率低下问题，需要一种能够提高跨链运作效率的解决方案

Method: 集成去中心化标识符(DID)和可验证凭证(VC)支持去中心化识别，采用基于简化支付验证(SPV)的认证协议避免重复验证，设计无需关闭额外渠道的跨链结算方案

Result: 通过模拟实验验证了框架的可行性，并证明在跨链环境下对RWAs的效率有显著提升

Conclusion: 该框架有效解决了RWAs在多链部署中的身份管理和效率挑战，为实际资产的跨链集成提供了可行的技术方案

Abstract: Real-World Assets (RWAs) have recently attracted increasing attention as a
means of bridging traditional financial instruments with decentralized
infrastructures. By representing assets such as bonds, commodities, and real
estate on blockchains, RWAs can enhance liquidity, broaden accessibility, and
extend the scope of decentralized finance. Industry forecasts further suggest
rapid growth of tokenized RWAs in the coming years, underscoring their
potential role in the evolution of digital financial markets. However, when
deployed across multiple blockchains, RWAs face challenges such as repeated
authentication on different chains and inefficiency caused by multi-step
settlement protocols. To address these issues, we present a cross-chain
framework for RWAs that emphasizes identity management, authentication, and
interaction. The framework integrates Decentralized Identifiers and Verifiable
Credentials with customized attributes to support decentralized identification,
and incorporates an authentication protocol based on Simplified Payment
Verification to avoid redundant verification across chains. Furthermore, we
design a cross-chain channel that enables the settlement of RWAs without
requiring channel closure, thereby improving operational efficiency. We
implement the framework and evaluate it through simulations, which confirm its
feasibility and demonstrate improvements in efficiency for RWAs in cross-chain
settings.

</details>


### [35] [Universal share based quantum multi secret image sharing scheme](https://arxiv.org/abs/2509.12979)
*Dipak K. Rabari,Yogesh K. Meghrajani,Laxmi S. Desai*

Main category: cs.CR

TL;DR: 这篇论文提出了一种基于量子计算和视觉加密术的新题多图像密密共享方案，通过万能分享概念实现高安全性的图像传输。


<details>
  <summary>Details</summary>
Motivation: 因为网络普及导致黑客攻击和非授权访问增多，图像信息安全变得越来越重要。量子计算能够高效解决某些数学问题，威胁到现有加密算法的安全性，需要结合量子计算优势和视觉加密术来提高安全性。

Method: 采用基于万能分享的量子多秘密共享技术，利用量子计算特性来实现多个图像的安全共享。该方案通过量子计算机的能力来提高对各种偷听威胁的耐受性。

Result: 该方法能够为企业数据访问和军事通信等应用提供稳健的安全解决方案，实现來密图像的安全共享。

Conclusion: 结合量子计算和视觉加密术的新题量子多秘密共享方案能够有效提高图像通信的安全性，应对量子计算带来的安全挑战，为重要领域的图像数据保护提供了可靠的技术支撑。

Abstract: Image security for information has become increasingly critical as internet
become more prevalent due to hacking and unauthorized access. To ensure the
security of confidential image data, image encryption using visual cryptography
plays a crucial role. To share multiple images using visual cryptography, the
company organizer utilizes the concept of a universal or common share.
Likewise, quantum computing is an emerging technology that facilitates secure
communication. The ability of quantum computers to solve certain mathematical
problems efficiently threatens the security of many current encryption
algorithms. Hence, to leverage the strengths of quantum computing and visual
cryptography, this research introduces a novel universal share-based quantum
multi-secret sharing technique for secure image communication. Quantum
computing enables the scheme to exhibit high resilience to different
eavesdropping threats. Consequently, the proposed method offers robust security
solution for sharing confidential images across a range of applications,
including enterprise data access and military communications.

</details>


### [36] [xOffense: An AI-driven autonomous penetration testing framework with offensive knowledge-enhanced LLMs and multi agent systems](https://arxiv.org/abs/2509.13021)
*Phung Duc Luong,Le Tran Gia Bao,Nguyen Vu Khai Tam,Dong Huu Nguyen Khoa,Nguyen Huu Quyen,Van-Hau Pham,Phan The Duy*

Main category: cs.CR

TL;DR: xOffense是一个基于AI的多智能体渗透测试框架，使用微调的Qwen3-32B大模型驱动，实现从人工到全自动化的渗透测试流程，在基准测试中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 将劳动密集型、专家驱动的手动渗透测试转变为完全自动化、可扩展的机器可执行工作流程，提高效率和可重复性。

Method: 采用微调的中等规模开源LLM(Qwen3-32B)驱动推理和决策，通过专门智能体负责侦察、漏洞扫描和利用，并使用编排层确保各阶段协调。基于思维链渗透测试数据进行微调。

Result: 在AutoPenBench和AI-Pentest-Benchmark基准测试中，xOffense以79.17%的子任务完成率显著优于VulnBot和PentestGPT等领先系统。

Conclusion: 领域适配的中等规模LLM结合结构化多智能体编排，能够为自主渗透测试提供优越、成本效益高且可重复的解决方案。

Abstract: This work introduces xOffense, an AI-driven, multi-agent penetration testing
framework that shifts the process from labor-intensive, expert-driven manual
efforts to fully automated, machine-executable workflows capable of scaling
seamlessly with computational infrastructure. At its core, xOffense leverages a
fine-tuned, mid-scale open-source LLM (Qwen3-32B) to drive reasoning and
decision-making in penetration testing. The framework assigns specialized
agents to reconnaissance, vulnerability scanning, and exploitation, with an
orchestration layer ensuring seamless coordination across phases. Fine-tuning
on Chain-of-Thought penetration testing data further enables the model to
generate precise tool commands and perform consistent multi-step reasoning. We
evaluate xOffense on two rigorous benchmarks: AutoPenBench and
AI-Pentest-Benchmark. The results demonstrate that xOffense consistently
outperforms contemporary methods, achieving a sub-task completion rate of
79.17%, decisively surpassing leading systems such as VulnBot and PentestGPT.
These findings highlight the potential of domain-adapted mid-scale LLMs, when
embedded within structured multi-agent orchestration, to deliver superior,
cost-efficient, and reproducible solutions for autonomous penetration testing.

</details>


### [37] [Bridging Threat Models and Detections: Formal Verification via CADP](https://arxiv.org/abs/2509.13035)
*Dumitru-Bogdan Prelipcean,Cătălin Dima*

Main category: cs.CR

TL;DR: 提出基于标记转换系统的形式化验证框架，用于自动检测威胁模型与检测规则之间的语义一致性，通过bisimulation和弱迹包含进行验证。


<details>
  <summary>Details</summary>
Motivation: 现有威胁检测系统依赖基于规则的逻辑，但这些规则与高层威胁模型的符合性很少被形式化验证，存在语义不匹配问题。

Method: 将检测逻辑和攻击树建模为标记转换系统(LTS)，使用GTDL语言形式化检测规则，通过CADP工具箱进行自动验证。

Result: 在真实恶意软件场景(LokiBot、Emotet)和参数化合成模型上验证，能够识别语义不匹配，支持迭代优化，具有良好的可扩展性。

Conclusion: 该框架为威胁检测系统提供了系统化的形式化验证方法，能够有效确保检测规则与威胁模型的一致性。

Abstract: Threat detection systems rely on rule-based logic to identify adversarial
behaviors, yet the conformance of these rules to high-level threat models is
rarely verified formally. We present a formal verification framework that
models both detection logic and attack trees as labeled transition systems
(LTSs), enabling automated conformance checking via bisimulation and weak trace
inclusion. Detection rules specified in the Generic Threat Detection Language
(GTDL, a general-purpose detection language we formalize in this work) are
assigned a compositional operational semantics, and threat models expressed as
attack trees are interpreted as LTSs through a structural trace semantics. Both
representations are translated to LNT, a modeling language supported by the
CADP toolbox. This common semantic domain enables systematic and automated
verification of detection coverage. We evaluate our approach on real-world
malware scenarios such as LokiBot and Emotet and provide scalability analysis
through parametric synthetic models. Results confirm that our methodology
identifies semantic mismatches between threat models and detection rules,
supports iterative refinement, and scales to realistic threat landscapes.

</details>


### [38] [MIA-EPT: Membership Inference Attack via Error Prediction for Tabular Data](https://arxiv.org/abs/2509.13046)
*Eyal German,Daniel Samira,Yuval Elovici,Asaf Shabtai*

Main category: cs.CR

TL;DR: 提出了MIA-EPT方法，一种针对表格扩散模型的成员推理攻击，通过属性掩码和重建误差来检测训练数据泄露，在多个先进模型上验证了有效性


<details>
  <summary>Details</summary>
Motivation: 扩散模型生成的表格数据可能存在记忆训练记录和泄露敏感信息的问题，而针对表格数据的成员推理攻击研究不足，需要专门的方法来评估隐私风险

Method: MIA-EPT是一种黑盒攻击方法，通过掩码目标记录属性并重建，基于预测误差构建特征向量来判断记录是否属于训练集，无需访问模型内部

Result: 在三个扩散合成器上验证，AUC-ROC最高达0.599，TPR@10% FPR为22.0%；在MIDST 2025竞赛中获得黑盒多表赛道第二名(TPR@10% FPR=20.0%)

Conclusion: 该方法能够有效发现合成表格数据中的成员泄露，挑战了合成数据天生具有隐私保护性的假设

Abstract: Synthetic data generation plays an important role in enabling data sharing,
particularly in sensitive domains like healthcare and finance. Recent advances
in diffusion models have made it possible to generate realistic, high-quality
tabular data, but they may also memorize training records and leak sensitive
information. Membership inference attacks (MIAs) exploit this vulnerability by
determining whether a record was used in training. While MIAs have been studied
in images and text, their use against tabular diffusion models remains
underexplored despite the unique risks of structured attributes and limited
record diversity. In this paper, we introduce MIAEPT, Membership Inference
Attack via Error Prediction for Tabular Data, a novel black-box attack
specifically designed to target tabular diffusion models. MIA-EPT constructs
errorbased feature vectors by masking and reconstructing attributes of target
records, disclosing membership signals based on how well these attributes are
predicted. MIA-EPT operates without access to the internal components of the
generative model, relying only on its synthetic data output, and was shown to
generalize across multiple state-of-the-art diffusion models. We validate
MIA-EPT on three diffusion-based synthesizers, achieving AUC-ROC scores of up
to 0.599 and TPR@10% FPR values of 22.0% in our internal tests. Under the MIDST
2025 competition conditions, MIA-EPT achieved second place in the Black-box
Multi-Table track (TPR@10% FPR = 20.0%). These results demonstrate that our
method can uncover substantial membership leakage in synthetic tabular data,
challenging the assumption that synthetic data is inherently
privacy-preserving. Our code is publicly available at
https://github.com/eyalgerman/MIA-EPT.

</details>


### [39] [SLasH-DSA: Breaking SLH-DSA Using an Extensible End-To-End Rowhammer Framework](https://arxiv.org/abs/2509.13048)
*Jeremy Boy,Antoon Purnal,Anna Pätschke,Luca Wilke,Thomas Eisenbarth*

Main category: cs.CR

TL;DR: 首个针对SLH-DSA的纯软件通用伪造攻击，利用Rowhammer比特翻转破坏内部状态并伪造签名，无需物理访问即可在普通桌面/服务器硬件上实现


<details>
  <summary>Details</summary>
Motivation: 随着量子计算发展，PQC方案逐步取代经典算法。SLH-DSA作为NIST新标准算法，虽然理论基础安全，但实际实现可能面临Rowhammer等硬件漏洞威胁

Method: 开发Swage框架实现端到端Rowhammer故障攻击，通过比特翻转破坏SLH-DSA内部状态，结合新颖的复杂度分析确定最优计算路径进行签名伪造

Result: 成功对OpenSSL 3.5.1中所有安全级别的SLH-DSA实现通用伪造攻击，最高安全级别仅需8小时攻击和36秒后处理即可完成

Conclusion: 即使理论安全的PQC方案在实际条件下也可能失效，需要额外的实现加固或硬件防护来抵御Rowhammer攻击

Abstract: As quantum computing advances, PQC schemes are adopted to replace classical
algorithms. Among them is the SLH-DSA that was recently standardized by NIST
and is favored for its conservative security foundations.
  In this work, we present the first software-only universal forgery attack on
SLH-DSA, leveraging Rowhammer-induced bit flips to corrupt the internal state
and forge signatures. While prior work targeted embedded systems and required
physical access, our attack is software-only, targeting commodity desktop and
server hardware, significantly broadening the threat model. We demonstrate a
full end-to-end attack against all security levels of SLH-DSA in OpenSSL 3.5.1,
achieving universal forgery for the highest security level after eight hours of
hammering and 36 seconds of post-processing. Our post-processing is informed by
a novel complexity analysis that, given a concrete set of faulty signatures,
identifies the most promising computational path to pursue.
  To enable the attack, we introduce Swage, a modular and extensible framework
for implementing end-to-end Rowhammer-based fault attacks. Swage abstracts and
automates key components of practical Rowhammer attacks. Unlike prior tooling,
Swage is untangled from the attacked code, making it reusable and suitable for
frictionless analysis of different targets. Our findings highlight that even
theoretically sound PQC schemes can fail under real-world conditions,
underscoring the need for additional implementation hardening or hardware
defenses against Rowhammer.

</details>


### [40] [Digital Sovereignty Control Framework for Military AI-based Cyber Security](https://arxiv.org/abs/2509.13072)
*Clara Maathuis,Kasper Cools*

Main category: cs.CR

TL;DR: 提出一种多角度框架，用于定义和评估军事网络安全领域的数据和AI模型的数字主权控制


<details>
  <summary>Details</summary>
Motivation: 在新兴威胁环境下，确保数字主权对军事组织至关重要，特别是在AI驱动的网络安全解决方案方面

Method: 采用设计导向的研究方法，结合系统性文献综述、批判性思维和实际案例分析

Result: 开发了一个多学科框架，重点关注上下文、自主性、参与者参与和风险缩减等方面

Conclusion: 该框架能够保护敏感防御资产，应对各种威胁，同时解决相结合作战能力和战略法律考虑的挑战

Abstract: In today's evolving threat landscape, ensuring digital sovereignty has become
mandatory for military organizations, especially given their increased
development and investment in AI-driven cyber security solutions. To this end,
a multi-angled framework is proposed in this article in order to define and
assess digital sovereign control of data and AI-based models for military cyber
security. This framework focuses on aspects such as context, autonomy,
stakeholder involvement, and mitigation of risks in this domain. Grounded on
the concepts of digital sovereignty and data sovereignty, the framework aims to
protect sensitive defence assets against threats such as unauthorized access,
ransomware, and supply-chain attacks. This approach reflects the multifaceted
nature of digital sovereignty by preserving operational autonomy, assuring
security and safety, securing privacy, and fostering ethical compliance of both
military systems and decision-makers. At the same time, the framework addresses
interoperability challenges among allied forces, strategic and legal
considerations, and the integration of emerging technologies by considering a
multidisciplinary approach that enhances the resilience and preservation of
control over (critical) digital assets. This is done by adopting a design
oriented research where systematic literature review is merged with critical
thinking and analysis of field incidents in order to assure the effectivity and
realism of the framework proposed.

</details>


### [41] [Characterizing Phishing Pages by JavaScript Capabilities](https://arxiv.org/abs/2509.13186)
*Aleksandr Nahapetyan,Kanv Khare,Kevin Schwarz,Bradley Reaves,Alexandros Kapravelos*

Main category: cs.CR

TL;DR: 该论文提出了一种自动检测和分类钓鱼工具包的系统，准确率达97%，能够处理大规模钓鱼页面分析，发现UI交互性和基本指纹识别是最普遍的钓鱼技术。


<details>
  <summary>Details</summary>
Motivation: 钓鱼攻击者使用钓鱼工具包大规模部署钓鱼页面，而研究人员和防御者仍依赖手动分析来识别静态特征，需要自动化工具来处理海量钓鱼页面。

Method: 开发了一个自动化系统，通过分析钓鱼页面的JavaScript逻辑复杂性来进行聚类，使用真实数据集（548个工具包家族，4,562个URL）进行验证。

Result: 系统在真实数据集上达到97%的准确率，在未标记数据集中将434,050个钓鱼页面聚类为11,377个群组，发现UI交互性（90%）和基本指纹识别（80%）是最常见技术。

Conclusion: 该研究为研究人员和分析师提供了处理海量钓鱼页面的新方法，自动化了原本需要手动完成的过程，并揭示了不同钓鱼技术的流行程度。

Abstract: In 2024, the Anti-Phishing Work Group identified over one million phishing
pages. Phishers achieve this scale by using phishing kits -- ready-to-deploy
phishing websites -- to rapidly deploy phishing campaigns with specific data
exfiltration, evasion, or mimicry techniques. In contrast, researchers and
defenders continue to fight phishing on a page-by-page basis and rely on manual
analysis to recognize static features for kit identification.
  This paper aims to aid researchers and analysts by automatically
differentiating groups of phishing pages based on the underlying kit,
automating a previously manual process, and enabling us to measure how popular
different client-side techniques are across these groups. For kit detection,
our system has an accuracy of 97% on a ground-truth dataset of 548 kit families
deployed across 4,562 phishing URLs. On an unlabeled dataset, we leverage the
complexity of 434,050 phishing pages' JavaScript logic to group them into
11,377 clusters, annotating the clusters with what phishing techniques they
employ. We find that UI interactivity and basic fingerprinting are universal
techniques, present in 90% and 80% of the clusters, respectively. On the other
hand, mouse detection via the browser's mouse API is among the rarest
behaviors, despite being used in a deployment of a 7-year-old open-source
phishing kit. Our methods and findings provide new ways for researchers and
analysts to tackle the volume of phishing pages.

</details>


### [42] [Trustworthy and Confidential SBOM Exchange](https://arxiv.org/abs/2509.13217)
*Eman Abu Ishgair,Chinenye Okafor,Marcela S. Melara,Santiago Torres-Arias*

Main category: cs.CR

TL;DR: Petra是一个SBOM交换系统，通过选择性加密实现软件物料清单的保密性编辑和分发，在保持透明度的同时保护知识产权和安全漏洞信息。


<details>
  <summary>Details</summary>
Motivation: 企业软件供应商需要在软件供应链安全透明度与机密信息保护之间找到平衡，既要满足监管要求又要防止敏感信息泄露。

Method: 采用选择性加密技术，构建格式无关、防篡改的SBOM表示形式，生成高效且保密的完整性证明，支持加密审计和信任建立。

Result: 原型系统显示，交换编辑后的SBOM只需额外不到1KB的数据，SBOM解密在查询过程中的性能开销最多为1%。

Conclusion: Petra系统成功解决了SBOM透明度和机密性之间的冲突，为软件供应链安全提供了实用的解决方案。

Abstract: Software Bills of Materials (SBOMs) have become a regulatory requirement for
improving software supply chain security and trust by means of transparency
regarding components that make up software artifacts. However, enterprise and
regulated software vendors commonly wish to restrict who can view confidential
software metadata recorded in their SBOMs due to intellectual property or
security vulnerability information. To address this tension between
transparency and confidentiality, we propose Petra, an SBOM exchange system
that empowers software vendors to interoperably compose and distribute redacted
SBOM data using selective encryption. Petra enables software consumers to
search redacted SBOMs for answers to specific security questions without
revealing information they are not authorized to access. Petra leverages a
format-agnostic, tamper-evident SBOM representation to generate efficient and
confidentiality-preserving integrity proofs, allowing interested parties to
cryptographically audit and establish trust in redacted SBOMs. Exchanging
redacted SBOMs in our Petra prototype requires less than 1 extra KB per SBOM,
and SBOM decryption account for at most 1% of the performance overhead during
an SBOM query.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [43] [V-Math: An Agentic Approach to the Vietnamese National High School Graduation Mathematics Exams](https://arxiv.org/abs/2509.12251)
*Duong Q. Nguyen,Quy P. Nguyen,Nguyen Van Nhon,Quang-Thinh Bui,H. Nguyen-Xuan*

Main category: cs.AI

TL;DR: V-Math是一个为越南高中生设计的自主代理框架，用于准备国家高中数学毕业考试，包含问题生成器、解题解释器和个性化导师三个AI代理。


<details>
  <summary>Details</summary>
Motivation: 帮助越南高中生准备国家高中数学毕业考试，同时减轻教师手动出题的工作负担，丰富教学资源。

Method: 集成三个专门AI代理：基于规范矩阵的问题生成器、提供详细步骤推理的解题解释器、以及根据学生表现自适应的个性化导师。

Result: 初步评估显示V-Math能生成符合矩阵要求的考试题，具有高解题准确率，提供连贯解释，并增强练习材料的多样性。

Conclusion: V-Math有潜力支持符合国家标准的可扩展、公平的数学备考，同时通过AI辅助考试创建赋能教师。

Abstract: This paper develops an autonomous agentic framework called V-Math that aims
to assist Vietnamese high school students in preparing for the National High
School Graduation Mathematics Exams (NHSGMEs). The salient framework integrates
three specialized AI agents: a specification-matrix-conditioned question
generator, a solver/explainer for detailed step-by-step reasoning, and a
personalized tutor that adapts to student performance. Beyond enabling
self-paced student practice, V-Math supports teachers by generating innovative,
compliant exam questions and building diverse, high-quality question banks.
This reduces manual workload and enriches instructional resources. We describe
the system architecture, focusing on practice modes for learners and
teacher-oriented features for question generation. Preliminary evaluations
demonstrate that V-Math produces matrix-aligned exams with high solution
accuracy, delivers coherent explanations, and enhances the variety of practice
materials. These results highlight its potential to support scalable, equitable
mathematics preparation aligned with national standards while also empowering
teachers through AI-assisted exam creation.

</details>


### [44] [DISPLIB: a library of train dispatching problems](https://arxiv.org/abs/2509.12254)
*Oddvar Kloster,Bjørnar Luteberget,Carlo Mannino,Giorgio Sartor*

Main category: cs.AI

TL;DR: 这篇论文提出了DISPLIB标准，为火车重新路由和重新排程问题定义了统一的问题定义和文件格式，并提供了来自多个实际应用场景的问题实例，以促进解决方案的可复现性和性能比较。


<details>
  <summary>Details</summary>
Motivation: 目前火车重新路由和重新排程问题的研究存在代码和数据不公开、问题定义不统一、无法进行性能比较等问题，影响了研究的可复现性和进步。

Method: 受MILP、SAT、TSP、VRP等标准化社区的启发，提出了DISPLIB标准，包括统一的问题定义、文件格式，收集了多个实际工业场景的问题实例，并提供了参考解决算法实现。

Result: 建立了一个开放的问题实例库和标准化框架，使得研究人员和开发者可以在没有工业联系的情况下工作，并支持解决方案的实验性比较。

Conclusion: DISPLIB标准有助于解决火车重新路由和重新排程领域的可复现性问题，促进优化算法的研究和开发，提高运营效率。所有材料都在线可用。

Abstract: Optimization-based decision support systems have a significant potential to
reduce delays, and thus improve efficiency on the railways, by automatically
re-routing and re-scheduling trains after delays have occurred. The operations
research community has dedicated a lot of effort to developing optimization
algorithms for this problem, but each study is typically tightly connected with
a specific industrial use case. Code and data are seldom shared publicly. This
fact hinders reproducibility, and has led to a proliferation of papers
describing algorithms for more or less compatible problem definitions, without
any real opportunity for readers to assess their relative performance. Inspired
by the successful communities around MILP, SAT, TSP, VRP, etc., we introduce a
common problem definition and file format, DISPLIB, which captures all the main
features of train re-routing and re-scheduling. We have gathered problem
instances from multiple real-world use cases and made them openly available. In
this paper, we describe the problem definition, the industrial instances, and a
reference solver implementation. This allows any researcher or developer to
work on the train dispatching problem without an industrial connection, and
enables the research community to perform empirical comparisons between
solvers. All materials are available online at https://displib.github.io.

</details>


### [45] [InPhyRe Discovers: Large Multimodal Models Struggle in Inductive Physical Reasoning](https://arxiv.org/abs/2509.12263)
*Gautam Sreekumar,Vishnu Naresh Boddeti*

Main category: cs.AI

TL;DR: 提出了InPhyRe基准测试，用于评估大型多模态模型在违反物理规律的场景中的归纳物理推理能力，发现现有模型存在参数知识应用困难、语言偏见和视觉输入忽略等问题。


<details>
  <summary>Details</summary>
Motivation: 现有大型多模态模型仅编码训练时观察到的物理规律参数知识，无法处理违反这些规律的推理场景，而人类具备从少量视觉示例中适应新物理环境的归纳推理能力，这对安全关键应用至关重要。

Method: 提出InPhyRe视觉问答基准，通过算法生成的合成碰撞视频评估模型在违反物理规律的碰撞事件中的推理能力，测试了13个大型多模态模型。

Result: 研究发现：(1)模型难以将有限的参数知识应用于推理；(2)当演示样本违反物理规律时归纳推理能力弱；(3)存在语言偏见且忽略视觉输入，质疑模型对视觉输入的可信度。

Conclusion: 大型多模态模型在归纳物理推理方面存在显著缺陷，需要改进以提升在违反物理规律场景中的推理能力和视觉输入的可信度。

Abstract: Large multimodal models (LMMs) encode universal physical laws observed during
training, such as momentum conservation, as parametric knowledge. It allows
LMMs to answer physical reasoning queries, such as the outcome of a potential
collision event from visual input. However, since parametric knowledge includes
only the physical laws seen during training, it is insufficient for reasoning
when the inference scenario violates these physical laws. In contrast, humans
possess the skill to adapt their physical reasoning to unseen physical
environments from a few visual examples. This ability, which we refer to as
inductive physical reasoning, is indispensable for LMMs if they are to replace
human agents in safety-critical applications. Despite its importance, existing
visual benchmarks evaluate only the parametric knowledge in LMMs, and not
inductive physical reasoning. To this end, we propose InPhyRe, the first visual
question answering benchmark to measure inductive physical reasoning in LMMs.
InPhyRe evaluates LMMs on their ability to predict the outcome of collision
events in algorithmically generated synthetic collision videos. By inspecting
13 LMMs, InPhyRe informs us that (1) LMMs struggle to apply their limited
parametric knowledge about universal physical laws to reasoning, (2) inductive
physical reasoning in LMMs is weak when demonstration samples violate universal
physical laws, and (3) inductive physical reasoning in LMMs suffers from
language bias and largely ignores the visual inputs, questioning the
trustworthiness of LMMs regarding visual inputs.

</details>


### [46] [LLMAP: LLM-Assisted Multi-Objective Route Planning with User Preferences](https://arxiv.org/abs/2509.12273)
*Liangqi Yuan,Dong-Jun Han,Christopher G. Brinton,Sabine Brunswicker*

Main category: cs.AI

TL;DR: 提出LLMAP系统，结合LLM解析自然语言偏好和基于图的搜索算法，解决多约束条件下的最优路线规划问题


<details>
  <summary>Details</summary>
Motivation: 现有路线规划方法存在局限性：直接使用LLM处理大规模地图数据困难，基于图搜索的方法对自然语言偏好理解能力有限，且用户时空分布高度异构不可预测

Method: 使用LLM-as-Parser解析自然语言、识别任务、提取用户偏好和任务依赖关系，结合多步图构建迭代搜索算法(MSGS)进行最优路线查找，采用多目标优化自适应调整权重

Result: 在14个国家27个城市的1000个不同复杂度路由提示上进行实验，结果显示该方法在多重约束下实现了优越性能

Conclusion: LLMAP系统能够有效处理自然语言驱动的路线规划问题，在保证多重约束条件下实现最优路线规划

Abstract: The rise of large language models (LLMs) has made natural language-driven
route planning an emerging research area that encompasses rich user objectives.
Current research exhibits two distinct approaches: direct route planning using
LLM-as-Agent and graph-based searching strategies. However, LLMs in the former
approach struggle to handle extensive map data, while the latter shows limited
capability in understanding natural language preferences. Additionally, a more
critical challenge arises from the highly heterogeneous and unpredictable
spatio-temporal distribution of users across the globe. In this paper, we
introduce a novel LLM-Assisted route Planning (LLMAP) system that employs an
LLM-as-Parser to comprehend natural language, identify tasks, and extract user
preferences and recognize task dependencies, coupled with a Multi-Step Graph
construction with iterative Search (MSGS) algorithm as the underlying solver
for optimal route finding. Our multi-objective optimization approach adaptively
tunes objective weights to maximize points of interest (POI) quality and task
completion rate while minimizing route distance, subject to three key
constraints: user time limits, POI opening hours, and task dependencies. We
conduct extensive experiments using 1,000 routing prompts sampled with varying
complexity across 14 countries and 27 cities worldwide. The results demonstrate
that our approach achieves superior performance with guarantees across multiple
constraints.

</details>


### [47] [Developing an aeroponic smart experimental greenhouse for controlling irrigation and plant disease detection using deep learning and IoT](https://arxiv.org/abs/2509.12274)
*Mohammadreza Narimani,Ali Hajiahmad,Ali Moghimi,Reza Alimardani,Shahin Rafiee,Amir Hossein Mirzabe*

Main category: cs.AI

TL;DR: 研究发展了一种智能气培温室系统，结合IoT和AI技术直接监控玫瑰植物状态和环境条件，并通过AI算法识别植物疾病。


<details>
  <summary>Details</summary>
Motivation: 通过智能化技术提高温室植物生产效率，及时监测植物状态和环境条件，以便做出合理的管理决策。

Method: 开发IoT基础平台直接控制环境条件，使用VGG-19、InceptionResNetV2和InceptionV3算法构建AI疾病检测框架，分析定期捞获的植物图像。

Result: IoT系统能够实时发布温度、湿度、水流等数据，并调整参数以优化植物生长环境。VGG-19算法在识别干旱压力和锌病叶片方面达到92%的最高准确率。

Conclusion: 研究成功开发了一种整合IoT和AI技术的智能气培温室系统，能够有效监控环境条件并准确识别植物疾病，为智能农业管理提供了可靠的技术支撑。

Abstract: Controlling environmental conditions and monitoring plant status in
greenhouses is critical to promptly making appropriate management decisions
aimed at promoting crop production. The primary objective of this research
study was to develop and test a smart aeroponic greenhouse on an experimental
scale where the status of Geranium plant and environmental conditions are
continuously monitored through the integration of the internet of things (IoT)
and artificial intelligence (AI). An IoT-based platform was developed to
control the environmental conditions of plants more efficiently and provide
insights to users to make informed management decisions. In addition, we
developed an AI-based disease detection framework using VGG-19,
InceptionResNetV2, and InceptionV3 algorithms to analyze the images captured
periodically after an intentional inoculation. The performance of the AI
framework was compared with an expert's evaluation of disease status.
Preliminary results showed that the IoT system implemented in the greenhouse
environment is able to publish data such as temperature, humidity, water flow,
and volume of charge tanks online continuously to users and adjust the
controlled parameters to provide an optimal growth environment for the plants.
Furthermore, the results of the AI framework demonstrate that the VGG-19
algorithm was able to identify drought stress and rust leaves from healthy
leaves with the highest accuracy, 92% among the other algorithms.

</details>


### [48] [AIssistant: An Agentic Approach for Human--AI Collaborative Scientific Work on Reviews and Perspectives in Machine Learning](https://arxiv.org/abs/2509.12282)
*Sasi Kiran Gaddipati,Farhana Keya,Gollam Rabby,Sören Auer*

Main category: cs.AI

TL;DR: AIssistant是一个开源的人类-AI协作框架，用于简化科学工作流的端到端创建，通过模块化工具和代理实现文献合成、实验、引用管理和LaTeX生成，保持人类监督以确保准确性。


<details>
  <summary>Details</summary>
Motivation: 当前AI辅助研究工具存在碎片化且缺乏以人为中心的工作流程的问题，需要开发一个集成化的协作框架来简化科学研究过程。

Method: 开发AIssistant框架，集成模块化工具和代理进行文献合成、分段实验、引用管理和自动LaTeX论文生成，保持人类监督。采用三层评估：独立人类评审、自动化LLM评审和程序主席监督。

Result: AIssistant提高了起草效率和主题一致性，但人类-AI协作对于保持事实正确性、方法合理性和伦理合规性仍然至关重要。存在引用幻觉、动态论文结构适应困难和多模态内容集成不完全等限制。

Conclusion: AIssistant是一个有前景的人类-AI协作框架，能够提升科研效率，但仍需要人类监督来确保质量，并需要进一步解决现有局限性。

Abstract: Advances in AI-assisted research have introduced powerful tools for
literature retrieval, hypothesis generation, experimentation, and manuscript
preparation. However, systems remain fragmented and lack human-centred
workflows. To address these gaps, we introduce AIssistant, an agentic,
open-source Human-AI collaborative framework designed to simplify the
end-to-end creation of scientific workflows. Since our development is still in
an early stage, we present here the first experiments with AIssistant for
perspective and review research papers in machine learning. Our system
integrates modular tools and agents for literature synthesis, section-wise
experimentation, citation management, and automatic LaTeX paper text
generation, while maintaining human oversight at every stage to ensure
accuracy, coherence, and scholarly rigour. We conducted a comprehensive
evaluation across three layers: (1) Independent Human Review, following NeurIPS
double-blind standards; (2) Automated LLM Review, using GPT-5 as a scalable
human review proxy; and (3) Program Chair Oversight, where the chair monitors
the entire review process and makes final validation and acceptance decisions.
The results demonstrate that AIssistant improves drafting efficiency and
thematic consistency. Nonetheless, Human-AI collaboration remains essential for
maintaining factual correctness, methodological soundness, and ethical
compliance. Despite its effectiveness, we identify key limitations, including
hallucinated citations, difficulty adapting to dynamic paper structures, and
incomplete integration of multimodal content.

</details>


### [49] [Small Models, Big Results: Achieving Superior Intent Extraction through Decomposition](https://arxiv.org/abs/2509.12423)
*Danielle Cohen,Yoni Halpern,Noam Kahlon,Joel Oren,Omri Berkovitch,Sapir Caduri,Ido Dagan,Anatoly Efros*

Main category: cs.AI

TL;DR: 提出了一种分解式方法，通过结构化交互摘要和意图提取，提升资源受限模型在UI交互轨迹中的意图理解能力，甚至超越大型多模态语言模型的基准性能。


<details>
  <summary>Details</summary>
Motivation: 解决小型设备端模型在UI交互意图理解方面的局限性，提供隐私保护、低成本和低延迟的用户体验，同时提升意图推理的准确性。

Method: 采用两阶段分解方法：首先进行结构化交互摘要，捕捉每个用户操作的关键信息；然后使用微调模型在聚合摘要上进行意图提取。

Result: 该方法显著改善了资源受限模型的意图理解能力，甚至超过了大型多模态语言模型的基准性能。

Conclusion: 分解式方法为设备端智能代理提供了有效的意图理解解决方案，在保持隐私和低延迟的同时实现了高性能的意图推理。

Abstract: Understanding user intents from UI interaction trajectories remains a
challenging, yet crucial, frontier in intelligent agent development. While
massive, datacenter-based, multi-modal large language models (MLLMs) possess
greater capacity to handle the complexities of such sequences, smaller models
which can run on-device to provide a privacy-preserving, low-cost, and
low-latency user experience, struggle with accurate intent inference. We
address these limitations by introducing a novel decomposed approach: first, we
perform structured interaction summarization, capturing key information from
each user action. Second, we perform intent extraction using a fine-tuned model
operating on the aggregated summaries. This method improves intent
understanding in resource-constrained models, even surpassing the base
performance of large MLLMs.

</details>


### [50] [Building Coding Agents via Entropy-Enhanced Multi-Turn Preference Optimization](https://arxiv.org/abs/2509.12434)
*Jiahao Yu,Zelei Cheng,Xian Wu,Xinyu Xing*

Main category: cs.AI

TL;DR: 提出了一个名为\sys的熵增强框架，用于在多轮工具辅助的代码生成任务中优化偏好学习，通过保持策略熵和多轮交互优化，在SWE-bench基准测试中实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在软件工程复杂任务中表现不佳，标准对齐方法会降低输出多样性，限制测试时扩展的效果，且现有偏好优化算法无法很好地处理多轮推理和工具集成需求。

Method: \sys框架通过增强偏好目标来显式保持策略熵，并将学习推广到优化多轮交互而非单轮响应。还提出了结合学习验证器和无模型方法的混合最佳轨迹选择方案。

Result: 在SWE-bench排行榜上，30B参数的\sys训练模型在开源权重模型中排名第一（Lite版本）和第四（Verified版本），性能超过参数量10倍以上的模型。

Conclusion: \sys框架成功解决了多轮工具辅助编码任务中的偏好优化问题，通过保持输出多样性和多轮交互优化，显著提升了LLM在软件工程任务中的性能。

Abstract: Software engineering presents complex, multi-step challenges for Large
Language Models (LLMs), requiring reasoning over large codebases and
coordinated tool use. The difficulty of these tasks is exemplified by
benchmarks like SWE-bench, where current LLMs still struggle to resolve
real-world issues.
  A promising approach to enhance performance is test-time scaling (TTS), but
its gains are heavily dependent on the diversity of model outputs.
  While standard alignment methods such as Direct Preference Optimization (DPO)
and Kahneman-Tversky Optimization (KTO) are effective at aligning model outputs
with human preferences, this process can come at the cost of reduced diversity,
limiting the effectiveness of TTS.
  Additionally, existing preference optimization algorithms are typically
designed for single-turn tasks and do not fully address the complexities of
multi-turn reasoning and tool integration required for interactive coding
agents.
  To bridge this gap, we introduce \sys, an entropy-enhanced framework that
adapts existing preference optimization algorithms to the multi-turn,
tool-assisted setting.
  \sys augments the preference objective to explicitly preserve policy entropy
and generalizes learning to optimize over multi-turn interactions rather than
single-turn responses.
  We validate \sys by fine-tuning a diverse suite of models from different
families and sizes (up to 106B parameters).
  To maximize performance gains from TTS, we further propose a hybrid
best-trajectory selection scheme combining a learned verifier model with model
free approaches.
  On the \swebench leaderboard, our approach establishes new state-of-the-art
results among open-weight models. A 30B parameter model trained with \sys ranks
1st on \lite and 4th on \verified on the open-weight leaderboard, surpassed
only by models with over 10x more parameters(\eg$>$350B).

</details>


### [51] [Enhancing Physical Consistency in Lightweight World Models](https://arxiv.org/abs/2509.12437)
*Dingrui Wang,Zhexiao Sun,Zhouheng Li,Cheng Wang,Youlun Peng,Hongyuan Ye,Baha Zarrouki,Wei Li,Mattia Piccinini,Lei Xie,Johannes Betz*

Main category: cs.AI

TL;DR: PIWM是一个紧凑的鸟瞰图世界模型，通过Soft Mask训练和Warm Start推理技术，在保持小参数量的同时显著提升物理动态建模性能


<details>
  <summary>Details</summary>
Motivation: 解决世界模型部署中大小与性能的权衡问题 - 大模型计算资源需求高不适用于边缘设备，小模型物理建模能力不足

Method: 提出Physics-Informed BEV World Model (PIWM)，使用Soft Mask训练改进动态物体建模，引入Warm Start推理技术提升零样本预测质量

Result: 在相同参数量(400M)下比基线提升60.6%加权总分；最小PIWM(130M)比最大基线模型(400M)提升7.4%加权总分且推理速度快28%

Conclusion: PIWM通过创新的训练和推理技术，实现了小参数模型的高性能物理建模，为边缘设备部署提供了有效的解决方案

Abstract: A major challenge in deploying world models is the trade-off between size and
performance. Large world models can capture rich physical dynamics but require
massive computing resources, making them impractical for edge devices. Small
world models are easier to deploy but often struggle to learn accurate physics,
leading to poor predictions. We propose the Physics-Informed BEV World Model
(PIWM), a compact model designed to efficiently capture physical interactions
in bird's-eye-view (BEV) representations. PIWM uses Soft Mask during training
to improve dynamic object modeling and future prediction. We also introduce a
simple yet effective technique, Warm Start, for inference to enhance prediction
quality with a zero-shot model. Experiments show that at the same parameter
scale (400M), PIWM surpasses the baseline by 60.6% in weighted overall score.
Moreover, even when compared with the largest baseline model (400M), the
smallest PIWM (130M Soft Mask) achieves a 7.4% higher weighted overall score
with a 28% faster inference speed.

</details>


### [52] [Reasoning Models Can be Accurately Pruned Via Chain-of-Thought Reconstruction](https://arxiv.org/abs/2509.12464)
*Ryan Lucas,Kayhan Behdin,Zhipeng Wang,Qingquan Song,Shao Tang,Rahul Mazumder*

Main category: cs.AI

TL;DR: 通过联合重建输入和链式思绪追踪激活的方式，提出了一种可以无缝集成到现有剪枝流程的理性意识压缩方法，显著提升了理性模型的性能。


<details>
  <summary>Details</summary>
Motivation: 理性语言模型在推理时产生长链式思绪追踪，部署成本高，而标准的神经网络剪枝技术在理性任务上导致更大的性能损失和更多的思考令牌。

Method: 在剪枝过程中同时重建输入激活和模型在策略上的链式思绪追踪激活，这种"理性意识压缩"(RAC)方法可以无缝集成到SparseGPT等现有剪枝流程中。

Result: RAC方法显著提升了现有剪枝方法的性能，解决了标准剪枝在理性任务上的性能问题。

Conclusion: 通过考虑理性任务的特殊性，联合重建输入和链式思绪追踪的方法能够有效改善理性模型的压缩效果。

Abstract: Reasoning language models such as DeepSeek-R1 produce long chain-of-thought
traces during inference time which make them costly to deploy at scale. We show
that using compression techniques such as neural network pruning produces
greater performance loss than in typical language modeling tasks, and in some
cases can make the model slower since they cause the model to produce more
thinking tokens but with worse performance. We show that this is partly due to
the fact that standard LLM pruning methods often focus on input reconstruction,
whereas reasoning is a decode-dominated task. We introduce a simple, drop-in
fix: during pruning we jointly reconstruct activations from the input and the
model's on-policy chain-of-thought traces. This "Reasoning-Aware Compression"
(RAC) integrates seamlessly into existing pruning workflows such as SparseGPT,
and boosts their performance significantly. Code reproducing the results in the
paper can be found at: https://github.com/RyanLucas3/RAC

</details>


### [53] [Empowering Clinical Trial Design through AI: A Randomized Evaluation of PowerGPT](https://arxiv.org/abs/2509.12471)
*Yiwen Lu,Lu Li,Dazheng Zhang,Xinyao Jian,Tingyin Wang,Siqi Chen,Yuqing Lei,Jiayi Tong,Zhaohan Xi,Haitao Chu,Chongliang Luo,Alexis Ogdie,Brian Athey,Alparslan Turan,Michael Abramoff,Joseph C Cappelleri,Hua Xu,Yun Lu,Jesse Berlin,Daniel I. Sessler,David A. Asch,Xiaoqian Jiang,Yong Chen*

Main category: cs.AI

TL;DR: PowerGPT是一个AI驱动的系统，通过整合大语言模型和统计引擎，自动化临床试验设计中的检验选择和样本量计算，显著提高了完成率、准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 临床研究和试验设计中的样本量计算对统计功效分析至关重要，但其复杂性和对统计专业知识的依赖为许多研究人员设置了障碍。

Method: 开发PowerGPT系统，整合大语言模型(LLMs)与统计引擎，实现检验选择和样本量估计的自动化。通过随机试验评估其有效性。

Result: PowerGPT显著提高了任务完成率(检验选择99.3% vs 88.9%，样本量计算99.3% vs 77.8%)和准确性(样本量估计94.1% vs 55.4%，p<0.001)，同时减少了平均完成时间(4.0 vs 9.3分钟，p<0.001)。这些优势在各种统计检验中表现一致，惠及统计学家和非统计学家。

Conclusion: PowerGPT代表了一种可扩展的AI驱动方法，提高了临床研究中统计功效分析的可及性、效率和准确性，已在多个机构部署使用。

Abstract: Sample size calculations for power analysis are critical for clinical
research and trial design, yet their complexity and reliance on statistical
expertise create barriers for many researchers. We introduce PowerGPT, an
AI-powered system integrating large language models (LLMs) with statistical
engines to automate test selection and sample size estimation in trial design.
In a randomized trial to evaluate its effectiveness, PowerGPT significantly
improved task completion rates (99.3% vs. 88.9% for test selection, 99.3% vs.
77.8% for sample size calculation) and accuracy (94.1% vs. 55.4% in sample size
estimation, p < 0.001), while reducing average completion time (4.0 vs. 9.3
minutes, p < 0.001). These gains were consistent across various statistical
tests and benefited both statisticians and non-statisticians as well as
bridging expertise gaps. Already under deployment across multiple institutions,
PowerGPT represents a scalable AI-driven approach that enhances accessibility,
efficiency, and accuracy in statistical power analysis for clinical research.

</details>


### [54] [Physical Complexity of a Cognitive Artifact](https://arxiv.org/abs/2509.12495)
*Gülce Kardeş,David Krakauer,Joshua Grochow*

Main category: cs.AI

TL;DR: 论文通过分析Soma Cube物理拼图的计算复杂度，将计算机科学概念映射到认知问题解决策略，提出了"物质性原则"，并展示了如何通过分层策略降低任务难度。


<details>
  <summary>Details</summary>
Motivation: 认知科学和理论计算机科学都致力于分类和解释任务难度，本文旨在通过物理拼图研究智能机制如何通过利用物质约束来降低任务复杂性。

Method: 通过分析Soma Cube拼图的分支因子和搜索树出度，定量评估任务难度，并系统研究预处理（认知分块）、值排序（认知自由排序）、变量排序（认知脚手架）和剪枝（认知推理）等策略如何逐步改进试错搜索。

Result: 研究表明，通过分层策略可以显著降低搜索复杂度，有效利用人工制品能够通过利用物理约束来降低时间复杂性。

Conclusion: 提出了一个智能模型，将其视为算法库，能够同时调用心智和物质的能力，强调了物质性在降低认知任务难度中的重要作用。

Abstract: Cognitive science and theoretical computer science both seek to classify and
explain the difficulty of tasks. Mechanisms of intelligence are those that
reduce task difficulty. Here we map concepts from the computational complexity
of a physical puzzle, the Soma Cube, onto cognitive problem-solving strategies
through a ``Principle of Materiality''. By analyzing the puzzle's branching
factor, measured through search tree outdegree, we quantitatively assess task
difficulty and systematically examine how different strategies modify
complexity. We incrementally refine a trial-and-error search by layering
preprocessing (cognitive chunking), value ordering (cognitive free-sorting),
variable ordering (cognitive scaffolding), and pruning (cognitive inference).
We discuss how the competent use of artifacts reduces effective time complexity
by exploiting physical constraints and propose a model of intelligence as a
library of algorithms that recruit the capabilities of both mind and matter.

</details>


### [55] [A Dimensionality-Reduced XAI Framework for Roundabout Crash Severity Insights](https://arxiv.org/abs/2509.12524)
*Rohit Chakraborty,Subasish Das*

Main category: cs.AI

TL;DR: 本研究通过可解释的两步工作流程分析俄亥俄州环岛事故，识别出四种事故模式，并使用SHAP解释树模型量化伤害驱动因素，发现黑暗、湿滑路面和高速条件下事故更严重，为公共安全分析提供实用XAI模板。


<details>
  <summary>Details</summary>
Motivation: 环岛虽然减少了严重事故，但风险模式因条件而异，需要深入分析不同条件下的事故模式和伤害驱动因素，以支持安全改进措施。

Method: 采用两步可解释工作流程：首先使用聚类对应分析(CCA)识别共现因素和四种事故模式，然后使用基于树的严重性模型并通过SHAP进行解释，量化模式内和跨模式的伤害驱动因素。

Result: 研究发现黑暗、湿滑路面和较高限速与固定物体或角度事件同时发生时事故严重性更高；在清晰、低速环境下事故严重性较低。特定模式解释突出了入口处(未能让行、间隙接受)、多车道循环内(不当操作)和减速期间(追尾)的机制。

Conclusion: 该工作流程将模式发现与案例级解释相结合，支持场地筛选、对策选择和审计就绪报告，为公共安全分析中的可用可解释人工智能(XAI)提供了实用模板。

Abstract: Roundabouts reduce severe crashes, yet risk patterns vary by conditions. This
study analyzes 2017-2021 Ohio roundabout crashes using a two-step, explainable
workflow. Cluster Correspondence Analysis (CCA) identifies co-occurring factors
and yields four crash patterns. A tree-based severity model is then interpreted
with SHAP to quantify drivers of injury within and across patterns. Results
show higher severity when darkness, wet surfaces, and higher posted speeds
coincide with fixed-object or angle events, and lower severity in clear,
low-speed settings. Pattern-specific explanations highlight mechanisms at
entries (fail-to-yield, gap acceptance), within multi-lane circulation
(improper maneuvers), and during slow-downs (rear-end). The workflow links
pattern discovery with case-level explanations, supporting site screening,
countermeasure selection, and audit-ready reporting. The contribution to
Information Systems is a practical template for usable XAI in public safety
analytics.

</details>


### [56] [zELO: ELO-inspired Training Method for Rerankers and Embedding Models](https://arxiv.org/abs/2509.12541)
*Nicholas Pipitone,Ghita Houir Alami,Advaith Avadhanam,Anton Kaminskyi,Ashley Khoo*

Main category: cs.AI

TL;DR: zELO是一种新颖的训练方法，通过将排序任务等同于Thurstone模型来优化检索性能。基于此方法训练出的zerank-1系列模型在多个领域实现了最先进的检索效果，超越了闭源专有重排序器。


<details>
  <summary>Details</summary>
Motivation: 现有的检索模型往往依赖于标注数据，而zELO方法旨在利用无监督数据来训练高性能的重排序模型，解决标注数据稀缺和成本高的问题。

Method: 采用zELO训练方法，将排序任务建模为Thurstone模型，使用112,000个查询和每个查询100个文档的无标注数据进行端到端训练，训练时间少于10,000 H100小时。

Result: zerank-1和zerank-1-small模型在金融、法律、代码和STEM等多个领域取得了最高的检索分数，在NDCG@10和Recall指标上超越了闭源专有重排序器，并在零样本设置下在域外和私有客户数据集上保持了优异性能。

Conclusion: zELO方法证明了使用无监督数据训练高性能重排序模型的可行性，zerank-1系列模型在多个领域实现了最先进的检索性能，展现了强大的泛化能力。

Abstract: We introduce a novel training methodology named zELO, which optimizes
retrieval performance via the analysis that ranking tasks are statically
equivalent to a Thurstone model. Based on the zELO method, we use unsupervised
data in order train a suite of state-of-the-art open-weight reranker models:
zerank-1 and zerank-1-small. These models achieve the highest retrieval scores
in multiple domains, including finance, legal, code, and STEM, outperforming
closed-source proprietary rerankers on both NDCG@10 and Recall. These models
also demonstrate great versatility, maintaining their 0-shot performance on
out-of-domain and private customer datasets. The training data included 112,000
queries and 100 documents per query, and was trained end-to-end from
unannotated queries and documents in less than 10,000 H100-hours.

</details>


### [57] [Human + AI for Accelerating Ad Localization Evaluation](https://arxiv.org/abs/2509.12543)
*Harshit Rajgarhia,Shivali Dalmia,Mengyang Zhao,Mukherji Abhishek,Kiran Ganesh*

Main category: cs.AI

TL;DR: 一种结合自动化组件与人工监督的框架，通过场景文本检测、图像修复、机器翻译和文本重新排版技术，实现多语言广告本地化的视觉一致性和风格完整性。


<details>
  <summary>Details</summary>
Motivation: 多语言广告本地化不仅需要文本翻译，还需保持视觉一致性、空间对齐和风格完整性，以满足不同语言和格式的需求。

Method: 提出结构化框架，集成场景文本检测、图像修复(inpainting)、机器翻译(MT)和文本重新排版(text reimposition)技术，加入人工监督来处理广告本地化的复杂性。

Result: 在六个不同语言环境中进行定性评估，结果显示该方法能够产生语义准确且视觉一致的本地化广告，适用于实际工作流程。

Conclusion: 这是首个专门为加速广告本地化评估工作流而设计的综合技术框架，能够有效保持广告的视觉一致性和风格完整性。

Abstract: Adapting advertisements for multilingual audiences requires more than simple
text translation; it demands preservation of visual consistency, spatial
alignment, and stylistic integrity across diverse languages and formats. We
introduce a structured framework that combines automated components with human
oversight to address the complexities of advertisement localization. To the
best of our knowledge, this is the first work to integrate scene text
detection, inpainting, machine translation (MT), and text reimposition
specifically for accelerating ad localization evaluation workflows. Qualitative
results across six locales demonstrate that our approach produces semantically
accurate and visually coherent localized advertisements, suitable for
deployment in real-world workflows.

</details>


### [58] [Redefining CX with Agentic AI: Minerva CQ Case Study](https://arxiv.org/abs/2509.12589)
*Garima Agrawal,Riccardo De Maria,Kiran Davuluri,Daniele Spera,Charlie Read,Cosimo Spera,Jack Garrett,Don Miller*

Main category: cs.AI

TL;DR: 这篇论文介绍了Minerva CQ这种以人工智能为基础的客服助理系统，通过主动式、自主性的工作流程来降低客服人员的认知负荷，并在实际部署中实现了效率和客户体验的显著提升。


<details>
  <summary>Details</summary>
Motivation: 当前客服中心存在处理时间长、一次解决率低、客户满意度差等问题，主要原因是客服人员面临系统分散、手动排查、常常需要让客户等待等认知负荷。现有的AI助手工具多为被动响作，缺乏深度上下文理解能力。

Method: 提出Agentic AI概念，开发Minerva CQ实时客服助手系统。系统整合了实时转写、意图和情感检测、实体识别、上下文信息检索、动态客户精准画像和部分对话摘要等功能，支持主动式工作流程和持续上下文构建。

Result: Minerva CQ在生产环境中部署，作为AI副驾作用，在多个部署中实现了客服效率和客户体验的可测量改善。

Conclusion: Agentic AI通过主动、自主性的方式，能够有效减轻客服人员的认知负荷，提升客服质量和效率，为客服行业的AI应用提供了新的解决方案。

Abstract: Despite advances in AI for contact centers, customer experience (CX)
continues to suffer from high average handling time (AHT), low first-call
resolution, and poor customer satisfaction (CSAT). A key driver is the
cognitive load on agents, who must navigate fragmented systems, troubleshoot
manually, and frequently place customers on hold. Existing AI-powered
agent-assist tools are often reactive driven by static rules, simple prompting,
or retrieval-augmented generation (RAG) without deeper contextual reasoning. We
introduce Agentic AI goal-driven, autonomous, tool-using systems that
proactively support agents in real time. Unlike conventional approaches,
Agentic AI identifies customer intent, triggers modular workflows, maintains
evolving context, and adapts dynamically to conversation state. This paper
presents a case study of Minerva CQ, a real-time Agent Assist product deployed
in voice-based customer support. Minerva CQ integrates real-time transcription,
intent and sentiment detection, entity recognition, contextual retrieval,
dynamic customer profiling, and partial conversational summaries enabling
proactive workflows and continuous context-building. Deployed in live
production, Minerva CQ acts as an AI co-pilot, delivering measurable
improvements in agent efficiency and customer experience across multiple
deployments.

</details>


### [59] [Match Chat: Real Time Generative AI and Generative Computing for Tennis](https://arxiv.org/abs/2509.12592)
*Aaron Baughman,Gozde Akay,Eduardo Morales,Rahul Agarwal,Preetika Srivastava*

Main category: cs.AI

TL;DR: Match Chat是一个基于智能代理的实时网球比赛助手，结合GenAI和GenComp技术，在温网和美网为100万用户提供即时准确的比赛查询服务，准确率达92.83%，平均响应时间6.25秒。


<details>
  <summary>Details</summary>
Motivation: 提升网球球迷观赛体验，通过自然语言查询为观众提供实时比赛数据和洞察，解决传统观赛中信息获取不便的问题。

Method: 采用面向代理架构(AOA)，结合规则引擎、预测模型和智能代理预处理用户查询，使用交互式提示设计优化GenAI组件响应。

Result: 系统处理了约100万用户查询，准确率92.83%，平均响应时间6.25秒，支持120 RPS负载，96.08%查询使用交互提示引导，100%系统可用性。

Conclusion: 该研究展示了在动态环境中部署高性能代理系统的实用路径，为实时消费级AI系统提供了强调速度、精度和可用性的关键设计模式。

Abstract: We present Match Chat, a real-time, agent-driven assistant designed to
enhance the tennis fan experience by delivering instant, accurate responses to
match-related queries. Match Chat integrates Generative Artificial Intelligence
(GenAI) with Generative Computing (GenComp) techniques to synthesize key
insights during live tennis singles matches. The system debuted at the 2025
Wimbledon Championships and the 2025 US Open, where it provided about 1 million
users with seamless access to streaming and static data through natural
language queries. The architecture is grounded in an Agent-Oriented
Architecture (AOA) combining rule engines, predictive models, and agents to
pre-process and optimize user queries before passing them to GenAI components.
The Match Chat system had an answer accuracy of 92.83% with an average response
time of 6.25 seconds under loads of up to 120 requests per second (RPS). Over
96.08% of all queries were guided using interactive prompt design, contributing
to a user experience that prioritized clarity, responsiveness, and minimal
effort. The system was designed to mask architectural complexity, offering a
frictionless and intuitive interface that required no onboarding or technical
familiarity. Across both Grand Slam deployments, Match Chat maintained 100%
uptime and supported nearly 1 million unique users, underscoring the
scalability and reliability of the platform. This work introduces key design
patterns for real-time, consumer-facing AI systems that emphasize speed,
precision, and usability that highlights a practical path for deploying
performant agentic systems in dynamic environments.

</details>


### [60] [DaSAThco: Data-Aware SAT Heuristics Combinations Optimization via Large Language Models](https://arxiv.org/abs/2509.12602)
*Minyu Chen,Guoqiang Li*

Main category: cs.AI

TL;DR: DaSAThco框架使用大语言模型生成专门的启发式集成组合，并通过自适应选择机制实现从实例特征到定制化启发式的通用映射，解决了SAT求解器配置泛化问题


<details>
  <summary>Details</summary>
Motivation: SAT问题的异质性使得单一最优配置不可行，现有方法缺乏泛化能力且需要为新的问题类型重新优化，成本高昂

Method: 使用大语言模型在系统定义的问题原型指导下生成多样化的专门启发式集成组合，然后学习自适应选择机制形成最终映射

Result: DaSAThco实现了优越性能，特别是在非自适应方法表现有限的领域外泛化方面表现出强大的鲁棒性

Conclusion: 该工作为复杂可配置系统的自动化算法设计建立了更可扩展和实用的路径

Abstract: The performance of Conflict-Driven Clause Learning solvers hinges on internal
heuristics, yet the heterogeneity of SAT problems makes a single, universally
optimal configuration unattainable. While prior automated methods can find
specialized configurations for specific problem families, this dataset-specific
approach lacks generalizability and requires costly re-optimization for new
problem types. We introduce DaSAThco, a framework that addresses this challenge
by learning a generalizable mapping from instance features to tailored
heuristic ensembles, enabling a train-once, adapt-broadly model. Our framework
uses a Large Language Model, guided by systematically defined Problem
Archetypes, to generate a diverse portfolio of specialized heuristic ensembles
and subsequently learns an adaptive selection mechanism to form the final
mapping. Experiments show that DaSAThco achieves superior performance and, most
notably, demonstrates robust out-of-domain generalization where non-adaptive
methods show limitations. Our work establishes a more scalable and practical
path toward automated algorithm design for complex, configurable systems.

</details>


### [61] [Analogy-Driven Financial Chain-of-Thought (AD-FCoT): A Prompting Approach for Financial Sentiment Analysis](https://arxiv.org/abs/2509.12611)
*Anmol Singhal Navya Singhal*

Main category: cs.AI

TL;DR: AD-FCoT是一个基于类比推理和思维链提示的金融情感分析框架，无需额外训练即可提升LLM在金融新闻情感分析的准确性和市场相关性。


<details>
  <summary>Details</summary>
Motivation: 现有金融情感分析方法难以捕捉复杂经济背景且缺乏透明推理，影响了可靠性。需要一种能够整合历史类比和结构化推理的方法来提升分析质量。

Method: 提出类比驱动的金融思维链(AD-FCoT)提示框架，通过引导LLM在新事件和相关历史情景之间建立类比，并将这些类比嵌入到结构化推理链中。

Result: 在数千篇新闻文章上的实验表明，AD-FCoT在情感分类准确性上优于强基线，与市场回报的相关性显著更高，生成的解释与领域专业知识一致。

Conclusion: AD-FCoT是首个在金融领域明确结合类比示例和思维链推理的方法，通过纯提示方式实现了可解释的金融分析，适合实际应用。

Abstract: Financial news sentiment analysis is crucial for anticipating market
movements. With the rise of AI techniques such as Large Language Models (LLMs),
which demonstrate strong text understanding capabilities, there has been
renewed interest in enhancing these systems. Existing methods, however, often
struggle to capture the complex economic context of news and lack transparent
reasoning, which undermines their reliability. We propose Analogy-Driven
Financial Chain-of-Thought (AD-FCoT), a prompting framework that integrates
analogical reasoning with chain-of-thought (CoT) prompting for sentiment
prediction on historical financial news. AD-FCoT guides LLMs to draw parallels
between new events and relevant historical scenarios with known outcomes,
embedding these analogies into a structured, step-by-step reasoning chain. To
our knowledge, this is among the first approaches to explicitly combine
analogical examples with CoT reasoning in finance. Operating purely through
prompting, AD-FCoT requires no additional training data or fine-tuning and
leverages the model's internal financial knowledge to generate rationales that
mirror human analytical reasoning. Experiments on thousands of news articles
show that AD-FCoT outperforms strong baselines in sentiment classification
accuracy and achieves substantially higher correlation with market returns. Its
generated explanations also align with domain expertise, providing
interpretable insights suitable for real-world financial analysis.

</details>


### [62] [GBV-SQL: Guided Generation and SQL2Text Back-Translation Validation for Multi-Agent Text2SQL](https://arxiv.org/abs/2509.12612)
*Daojun Chen,Xi Wang,Shenyuan Ren,Qingzhi Ma,Pengpeng Zhao,An Liu*

Main category: cs.AI

TL;DR: GBV-SQL是一个多智能体框架，通过SQL回译验证机制解决Text2SQL中的语义鸿沟问题，同时揭示了基准数据中的系统性错误问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在Text2SQL生成方面取得显著进展，但语法正确的查询经常误解用户意图，存在关键的语义鸿沟问题。

Method: 提出GBV-SQL多智能体框架，采用引导生成和SQL2Text回译验证机制，通过专门智能体将生成的SQL翻译回自然语言来验证其与原始问题的逻辑一致性。

Result: 在BIRD基准上达到63.23%的执行准确率（绝对提升5.8%）；在去除错误样本后，在Spider基准上达到96.5%（开发集）和97.6%（测试集）的执行准确率。

Conclusion: 该工作不仅提供了语义验证的鲁棒框架，还提出了对基准完整性的批判性视角，强调了更严格数据集管理的必要性。

Abstract: While Large Language Models have significantly advanced Text2SQL generation,
a critical semantic gap persists where syntactically valid queries often
misinterpret user intent. To mitigate this challenge, we propose GBV-SQL, a
novel multi-agent framework that introduces Guided Generation with SQL2Text
Back-translation Validation. This mechanism uses a specialized agent to
translate the generated SQL back into natural language, which verifies its
logical alignment with the original question. Critically, our investigation
reveals that current evaluation is undermined by a systemic issue: the poor
quality of the benchmarks themselves. We introduce a formal typology for "Gold
Errors", which are pervasive flaws in the ground-truth data, and demonstrate
how they obscure true model performance. On the challenging BIRD benchmark,
GBV-SQL achieves 63.23% execution accuracy, a 5.8% absolute improvement. After
removing flawed examples, GBV-SQL achieves 96.5% (dev) and 97.6% (test)
execution accuracy on the Spider benchmark. Our work offers both a robust
framework for semantic validation and a critical perspective on benchmark
integrity, highlighting the need for more rigorous dataset curation.

</details>


### [63] [Mob-based cattle weight gain forecasting using ML models](https://arxiv.org/abs/2509.12615)
*Muhammad Riaz Hasib Hossain,Rafiqul Islam,Shawn R McGrath,Md Zahidul Islam,David Lamb*

Main category: cs.AI

TL;DR: 本文提出了一种基于随机森林(RF)模型的新方法，用于预测群体生牛一个月的体重增长，绩效超过SVR和LSTM模型，R²达到0.973。


<details>
  <summary>Details</summary>
Motivation: 预测群体生牛体重增长可以帮助农场主精细喂养策略、做出智慧纳种选择并降低气候变化和市场波动的风险。

Method: 使用随机森林(RF)模型，与支持向量回归(SVR)和长短期记忆(LSTM)模型进行比较。采用了4个数据集，包含108头牛的756个样本数据以及影响体重增长的气象数据(降雨和温度)。

Result: RF模型在所有数据集上都表现更好，当包含气象和年龄因素时，达到R² 0.973、RMSE 0.040和MAE 0.033。结果表明包含气象和年龄因素显著提高了预测准确性。

Conclusion: RF模型是预测牛体重增长的稳健工具，年龄和气候因素对群体体重趋势有显著影响。研究还开发了一个自动化前处理工具生成标准数据集，已公开在GitHub上。

Abstract: Forecasting mob based cattle weight gain (MB CWG) may benefit large livestock
farms, allowing farmers to refine their feeding strategies, make educated
breeding choices, and reduce risks linked to climate variability and market
fluctuations. In this paper, a novel technique termed MB CWG is proposed to
forecast the one month advanced weight gain of herd based cattle using
historical data collected from the Charles Sturt University Farm. This research
employs a Random Forest (RF) model, comparing its performance against Support
Vector Regression (SVR) and Long Short Term Memory (LSTM) models for monthly
weight gain prediction. Four datasets were used to evaluate the performance of
models, using 756 sample data from 108 herd-based cattle, along with weather
data (rainfall and temperature) influencing CWG. The RF model performs better
than the SVR and LSTM models across all datasets, achieving an R^2 of 0.973,
RMSE of 0.040, and MAE of 0.033 when both weather and age factors were
included. The results indicate that including both weather and age factors
significantly improves the accuracy of weight gain predictions, with the RF
model outperforming the SVR and LSTM models in all scenarios. These findings
demonstrate the potential of RF as a robust tool for forecasting cattle weight
gain in variable conditions, highlighting the influence of age and climatic
factors on herd based weight trends. This study has also developed an
innovative automated pre processing tool to generate a benchmark dataset for MB
CWG predictive models. The tool is publicly available on GitHub and can assist
in preparing datasets for current and future analytical research..

</details>


### [64] [ECG-aBcDe: Overcoming Model Dependence, Encoding ECG into a Universal Language for Any LLM](https://arxiv.org/abs/2509.12625)
*Yong Xia,Jingxuan Li,YeTeng Sun,Jiarui Bu*

Main category: cs.AI

TL;DR: ECG-aBcDe是一种创新的心电信号编码方法，将ECG转换为通用ECG语言，使任何大语言模型都能直接分析心电数据，无需修改架构，同时提升可解释性和时间尺度信息学习能力。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在心电分析中存在三个主要问题：模型特异性编码器阻碍跨模型迁移、Transformer架构难以捕捉ECG关键时间尺度信息、黑盒特性限制临床应用。

Method: 提出ECG-aBcDe编码方法，将ECG信号转换为通用ECG语言，构建ECG语言与自然语言混合数据集，实现预训练LLMs的直接微调，支持双向转换以提取注意力热图。

Result: 在ROUGE-L和METEOR指标上表现竞争性，BLEU-4指标显著提升，数据集内评估提升2.8倍(42.58分)，跨数据集评估提升3.9倍(30.76分)。

Conclusion: ECG-aBcDe为ECG分析与大语言模型集成提供了新范式，证明了该方法的可行性，解决了迁移性、时间尺度学习和可解释性三大挑战。

Abstract: Large Language Models (LLMs) hold significant promise for electrocardiogram
(ECG) analysis, yet challenges remain regarding transferability, time-scale
information learning, and interpretability. Current methods suffer from
model-specific ECG encoders, hindering transfer across LLMs. Furthermore, LLMs
struggle to capture crucial time-scale information inherent in ECGs due to
Transformer limitations. And their black-box nature limits clinical adoption.
To address these limitations, we introduce ECG-aBcDe, a novel ECG encoding
method that transforms ECG signals into a universal ECG language readily
interpretable by any LLM. By constructing a hybrid dataset of ECG language and
natural language, ECG-aBcDe enables direct fine-tuning of pre-trained LLMs
without architectural modifications, achieving "construct once, use anywhere"
capability. Moreover, the bidirectional convertibility between ECG and ECG
language of ECG-aBcDe allows for extracting attention heatmaps from ECG
signals, significantly enhancing interpretability. Finally, ECG-aBcDe
explicitly represents time-scale information, mitigating Transformer
limitations. This work presents a new paradigm for integrating ECG analysis
with LLMs. Compared with existing methods, our method achieves competitive
performance on ROUGE-L and METEOR. Notably, it delivers significant
improvements in the BLEU-4, with improvements of 2.8 times and 3.9 times in
in-dataset and cross-dataset evaluations, respectively, reaching scores of
42.58 and 30.76. These results provide strong evidence for the feasibility of
the new paradigm.

</details>


### [65] [Learn to Relax with Large Language Models: Solving Nonlinear Combinatorial Optimization Problems via Bidirectional Coevolution](https://arxiv.org/abs/2509.12643)
*Beidan Liu,Zhengqiu Zhu,Chen Gao,Yong Zhao,Wei Qi,Quanjun Yin*

Main category: cs.AI

TL;DR: AutoCO是一个端到端的自动化约束优化方法，利用大语言模型生成约束松弛策略，结合进化算法和蒙特卡洛树搜索的双向协同进化机制，有效解决非线性组合优化问题。


<details>
  <summary>Details</summary>
Motivation: 传统约束松弛方法依赖专家驱动的迭代设计，缺乏系统自动化和可扩展性。现有LLM优化方法主要作为被动约束验证器，无法处理NCOPs中复杂的约束交互。

Method: 利用结构化LLM推理生成约束松弛策略，通过统一三元表示方案动态演化算法原理和可执行代码。建立双向（全局-局部）协同进化机制，结合进化算法进行局部优化和蒙特卡洛树搜索进行全局策略空间探索。

Result: 在三个具有挑战性的NCOP基准测试上的综合实验验证了AutoCO的一致有效性和优于基线的性能表现。

Conclusion: AutoCO通过LLM学习松弛约束的方法，革命性地解决了NCOPs问题，在碎片化解空间中实现了强化和多样化的最优平衡。

Abstract: Nonlinear Combinatorial Optimization Problems (NCOPs) present a formidable
computational hurdle in practice, as their nonconvex nature gives rise to
multi-modal solution spaces that defy efficient optimization. Traditional
constraint relaxation approaches rely heavily on expert-driven, iterative
design processes that lack systematic automation and scalable adaptability.
While recent Large Language Model (LLM)-based optimization methods show promise
for autonomous problem-solving, they predominantly function as passive
constraint validators rather than proactive strategy architects, failing to
handle the sophisticated constraint interactions inherent to NCOPs.To address
these limitations, we introduce the first end-to-end \textbf{Auto}mated
\textbf{C}onstraint \textbf{O}ptimization (AutoCO) method, which revolutionizes
NCOPs resolution through learning to relax with LLMs.Specifically, we leverage
structured LLM reasoning to generate constraint relaxation strategies, which
are dynamically evolving with algorithmic principles and executable code
through a unified triple-representation scheme. We further establish a novel
bidirectional (global-local) coevolution mechanism that synergistically
integrates Evolutionary Algorithms for intensive local refinement with Monte
Carlo Tree Search for systematic global strategy space exploration, ensuring
optimal balance between intensification and diversification in fragmented
solution spaces. Finally, comprehensive experiments on three challenging NCOP
benchmarks validate AutoCO's consistent effectiveness and superior performance
over the baselines.

</details>


### [66] [Large Language Models Imitate Logical Reasoning, but at what Cost?](https://arxiv.org/abs/2509.12645)
*Lachlan McGinness,Peter Baumgartner*

Main category: cs.AI

TL;DR: 对前沿大语言模型在18个月期间的推理能力进行纵向研究，发现通过思维链提示和思维模型的引入，模型性能显著提升。提出了一种神经符号架构，使用较小模型将问题转化为标准化形式，再用Z3求解器解决，大幅降低计算成本的同时保持近乎完美的性能。


<details>
  <summary>Details</summary>
Motivation: 评估前沿大语言模型随时间推移的推理能力变化，并探索更高效的推理方法，以降低计算成本同时保持高性能。

Method: 使用PrOntoQA数据集进行真假问题测试，采用思维链提示和思维模型技术。提出神经符号架构：用小参数LLM将问题标准化，然后用Z3求解器进行可满足性验证。

Result: 2023-2024年性能提升归因于隐藏思维链提示，2024-2025年思维模型带来显著改进。神经符号方法将计算成本大幅降低（FLOPs减少），同时保持近乎完美的性能，推理FLOPs估算公式准确度在10%以内。

Conclusion: 大语言模型的推理能力随时间持续提升，神经符号架构为高效推理提供了可行方案，能在显著降低计算成本的同时维持高性能水平。

Abstract: We present a longitudinal study which evaluates the reasoning capability of
frontier Large Language Models over an eighteen month period. We measured the
accuracy of three leading models from December 2023, September 2024 and June
2025 on true or false questions from the PrOntoQA dataset and their
faithfulness to reasoning strategies provided through in-context learning. The
improvement in performance from 2023 to 2024 can be attributed to hidden Chain
of Thought prompting. The introduction of thinking models allowed for
significant improvement in model performance between 2024 and 2025.
  We then present a neuro-symbolic architecture which uses LLMs of less than 15
billion parameters to translate the problems into a standardised form. We then
parse the standardised forms of the problems into a program to be solved by Z3,
an SMT solver, to determine the satisfiability of the query. We report the
number of prompt and completion tokens as well as the computational cost in
FLOPs for open source models. The neuro-symbolic approach significantly reduces
the computational cost while maintaining near perfect performance. The common
approximation that the number of inference FLOPs is double the product of the
active parameters and total tokens was accurate within 10\% for all
experiments.

</details>


### [67] [Zero-shot Graph Reasoning via Retrieval Augmented Framework with LLMs](https://arxiv.org/abs/2509.12743)
*Hanqing Li,Kiran Sheena Jyothi,Henry Liang,Sharika Mahadevan,Diego Klabjan*

Main category: cs.AI

TL;DR: GRRAF是一种无需训练的新方法，利用检索增强生成(RAG)和LLM的代码生成能力来处理各种图推理任务，通过生成可执行代码查询图数据库，在大多数任务上达到100%准确率且具有良好的可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要大量微调或依赖预定义算法，存在局限性。GRRAF旨在通过结合RAG和LLM的代码生成能力，提供一种更灵活、高效的图推理解决方案。

Method: 将目标图存储在图形数据库中，提示LLM生成可执行代码查询来检索必要信息，包含错误反馈循环和超时机制以确保正确性和效率。

Result: 在GraphInstruct数据集上，GRRAF在大多数图推理任务（包括环检测、二分图检查、最短路径计算和最大流）上达到100%准确率，token成本与图大小无关，可扩展到10,000个节点的大型图。

Conclusion: GRRAF提供了一种无需训练的高效图推理方法，在准确性和可扩展性方面表现优异，为图推理任务提供了新的解决方案。

Abstract: We propose a new, training-free method, Graph Reasoning via Retrieval
Augmented Framework (GRRAF), that harnesses retrieval-augmented generation
(RAG) alongside the code-generation capabilities of large language models
(LLMs) to address a wide range of graph reasoning tasks. In GRRAF, the target
graph is stored in a graph database, and the LLM is prompted to generate
executable code queries that retrieve the necessary information. This approach
circumvents the limitations of existing methods that require extensive
finetuning or depend on predefined algorithms, and it incorporates an error
feedback loop with a time-out mechanism to ensure both correctness and
efficiency. Experimental evaluations on the GraphInstruct dataset reveal that
GRRAF achieves 100% accuracy on most graph reasoning tasks, including cycle
detection, bipartite graph checks, shortest path computation, and maximum flow,
while maintaining consistent token costs regardless of graph sizes. Imperfect
but still very high performance is observed on subgraph matching. Notably,
GRRAF scales effectively to large graphs with up to 10,000 nodes.

</details>


### [68] [H$^2$R: Hierarchical Hindsight Reflection for Multi-Task LLM Agents](https://arxiv.org/abs/2509.12810)
*Shicheng Ye,Chao Yu,Kaiqiang Ke,Chengdong Xu,Yinqi Wei*

Main category: cs.AI

TL;DR: 通过分层记忆架构和后览反思机制，实现细粒度知识转移，提升LLM基于代理在多任务场景中的结构化知识利用能力


<details>
  <summary>Details</summary>
Motivation: 现有方法将过往经验和知识视为整体单元，导致知识转移效率低下和粗粒度

Method: 提出分层记忆架构，将高级规划记忆与低级执行记忆解耦，通过分层后览反思(H²R)机制从过往代理-环境交互中精炼可重用的分层知识

Result: 在两个标准测试集上表现出更好的泛化能力和决策性能，超过了Expel等基线方法

Conclusion: H²R能够通过细粒度知识转移有效提升LLM基代理在新任务中的知识利用效率和性能

Abstract: Large language model (LLM)-based agents have shown strong potential in
multi-task scenarios, owing to their ability to transfer knowledge across
diverse tasks. However, existing approaches often treat prior experiences and
knowledge as monolithic units, leading to inefficient and coarse-grained
knowledge transfer. In this work, we propose a novel hierarchical memory
architecture that enables fine-grained knowledge transfer by decoupling
high-level planning memory from low-level execution memory. To construct and
refine these hierarchical memories, we introduce Hierarchical Hindsight
Reflection (H$^2$R), a mechanism that distills reusable and hierarchical
knowledge from past agent-environment interactions. At test time, H$^2$R
performs retrievals of high-level and low-level memories separately, allowing
LLM-based agents to efficiently access and utilize task-relevant knowledge for
new tasks.Experimental results across two benchmarks demonstrate that H$^2$R
can improve generalization and decision-making performance, outperforming prior
baselines such as Expel.

</details>


### [69] [LTA-thinker: Latent Thought-Augmented Training Framework for Large Language Models on Complex Reasoning](https://arxiv.org/abs/2509.12875)
*Jiaqi Wang,Binquan Ji,Haibo Luo,Yiyang Qi,Ruiting Li,Huiyan Wang,Yuantao Han,Cangyi Yang,jiaxu Zhang,Feiliang Ren*

Main category: cs.AI

TL;DR: LTA-Thinker是一个潜在思维增强训练框架，通过增加潜在思维分布的方差和引入基于分布的定向优化范式，提升大语言模型的复杂推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法如Coconut和SoftCoT在连续潜在空间推理中有效，但核心瓶颈在于高效生成和利用高质量潜在思维。SoftCoT++理论表明更大的潜在思维分布方差能更好逼近真实分布。

Method: 1) 基于可学习先验构建潜在思维生成架构以增加分布方差；2) 引入基于分布的定向优化范式，结合SFT损失、语义对齐损失（KL散度）和推理焦点损失（对比学习）进行多目标协同训练。

Result: 实验表明LTA-Thinker在各种基线中达到最先进性能，展现出更高的性能上限和更好的扩展效果。

Conclusion: LTA-Thinker通过增强潜在思维分布方差和优化训练范式，有效提升了大语言模型的复杂推理能力，为动态优化推理过程提供了新思路。

Abstract: Complex Reasoning in Large Language Models can be dynamically optimized using
Test-Time Scaling (TTS) to mitigate Overthinking. Methods such as Coconut,
SoftCoT and its variant are effective in continuous latent space inference, the
core bottleneck still lies in the efficient generation and utilization of
high-quality Latent Thought. Drawing from the theory of SoftCoT++ that a larger
variance in the generated Latent Thought distribution more closely approximates
the golden truth distribution, we propose a Latent Thought-Augmented Training
Framework--LTA-Thinker, which improves distributional variance and enhances
reasoning performance from two perspectives. First, LTA-Thinker constructs a
Latent Thought generation architecture based on a learnable prior. This
architecture aims to increase the variance distribution of generated Latent
Thought Vectors in order to simplify the overall structure and raise the
performance ceiling. Second, LTA-Thinker introduces a distribution-based
directional optimization paradigm that jointly constrains both distribution
locality and distribution scale. This mechanism improves information efficiency
and computational cost through a multi-objective co-training strategy, which
combines standard Supervised Fine-Tuning (SFT) loss with two novel losses:
Semantic Alignment Loss, which utilizes KL divergence to ensure that the Latent
Thought is highly relevant to the semantics of the question; Reasoning Focus
Loss, which utilizes a contrastive learning mechanism to guide the model to
focus on the most critical reasoning steps. Experiments show that LTA-thinker
achieves state-of-the-art (SOTA) performance among various baselines and
demonstrates a higher performance ceiling and better scaling effects.

</details>


### [70] [Stochastic Streets: A Walk Through Random LLM Address Generation in four European Cities](https://arxiv.org/abs/2509.12914)
*Tairan Fu,David Campo-Nazareno,Javier Coronado-Blázquez,Javier Conde,Pedro Reviriego,Fabrizio Lombardi*

Main category: cs.AI

TL;DR: LLMs在解决复杂数学问题和回答困难问题方面表现出色，但无法有效生成欧洲城市的随机街道地址


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在生成看似简单但实际需要精确地理知识的随机街道地址方面的能力，测试模型在真实世界数据生成中的局限性

Method: 通过要求LLMs生成欧洲城市的随机街道地址来测试其性能，分析生成结果的准确性和合理性

Result: LLMs在生成随机街道地址方面表现不佳，经常产生不存在的地址或不符合当地命名规范的地址

Conclusion: 尽管LLMs在复杂任务上表现优异，但在需要精确真实世界知识的简单任务上存在明显局限性，这揭示了当前模型的知识表示和生成能力的边界

Abstract: Large Language Models (LLMs) are capable of solving complex math problems or
answer difficult questions on almost any topic, but can they generate random
street addresses for European cities?

</details>


### [71] [Population Estimation using Deep Learning over Gandhinagar Urban Area](https://arxiv.org/abs/2509.12926)
*Jai Singla,Peal Jotania,Keivalya Pandya*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Population estimation is crucial for various applications, from resource
allocation to urban planning. Traditional methods such as surveys and censuses
are expensive, time-consuming and also heavily dependent on human resources,
requiring significant manpower for data collection and processing. In this
study a deep learning solution is proposed to estimate population using high
resolution (0.3 m) satellite imagery, Digital Elevation Models (DEM) of 0.5m
resolution and vector boundaries. Proposed method combines Convolution Neural
Network (CNN) architecture for classification task to classify buildings as
residential and non-residential and Artificial Neural Network (ANN)
architecture to estimate the population. Approx. 48k building footprints over
Gandhinagar urban area are utilized containing both residential and
non-residential, with residential categories further used for building-level
population estimation. Experimental results on a large-scale dataset
demonstrate the effectiveness of our model, achieving an impressive overall
F1-score of 0.9936. The proposed system employs advanced geospatial analysis
with high spatial resolution to estimate Gandhinagar population at 278,954. By
integrating real-time data updates, standardized metrics, and infrastructure
planning capabilities, this automated approach addresses critical limitations
of conventional census-based methodologies. The framework provides
municipalities with a scalable and replicable tool for optimized resource
management in rapidly urbanizing cities, showcasing the efficiency of AI-driven
geospatial analytics in enhancing data-driven urban governance.

</details>


### [72] [HLSMAC: A New StarCraft Multi-Agent Challenge for High-Level Strategic Decision-Making](https://arxiv.org/abs/2509.12927)
*Xingxing Hong,Yungong Wang,Dexin Jin,Ye Yuan,Ximing Huang,Zijian Wu,Wenxin Li*

Main category: cs.AI

TL;DR: HLSMAC是一个基于《三十六计》设计的新多智能体强化学习基准，包含12个星际争霸II场景，专注于评估高层战略决策能力，超越了传统微操测试。


<details>
  <summary>Details</summary>
Motivation: 现有MARL基准如SMAC主要关注微观操作，缺乏对高层战略智能的全面评估，需要新的基准来测试多智能体的战略决策能力。

Method: 基于《三十六计》设计12个星际争霸II场景，每个对应一个具体计策，引入多维评估指标（能力利用率、推进效率等），集成先进MARL算法和LLM智能体进行实验。

Result: 实验结果表明HLSMAC能够有效评估多智能体的战略决策能力，为推进多智能体战略决策研究提供了强大的测试平台。

Conclusion: HLSMAC填补了MARL基准在高层战略评估方面的空白，为多智能体战略决策研究提供了新的测试标准和方向。

Abstract: Benchmarks are crucial for assessing multi-agent reinforcement learning
(MARL) algorithms. While StarCraft II-related environments have driven
significant advances in MARL, existing benchmarks like SMAC focus primarily on
micromanagement, limiting comprehensive evaluation of high-level strategic
intelligence. To address this, we introduce HLSMAC, a new cooperative MARL
benchmark with 12 carefully designed StarCraft II scenarios based on classical
stratagems from the Thirty-Six Stratagems. Each scenario corresponds to a
specific stratagem and is designed to challenge agents with diverse strategic
elements, including tactical maneuvering, timing coordination, and deception,
thereby opening up avenues for evaluating high-level strategic decision-making
capabilities. We also propose novel metrics across multiple dimensions beyond
conventional win rate, such as ability utilization and advancement efficiency,
to assess agents' overall performance within the HLSMAC environment. We
integrate state-of-the-art MARL algorithms and LLM-based agents with our
benchmark and conduct comprehensive experiments. The results demonstrate that
HLSMAC serves as a robust testbed for advancing multi-agent strategic
decision-making.

</details>


### [73] [The Anatomy of Alignment: Decomposing Preference Optimization by Steering Sparse Features](https://arxiv.org/abs/2509.12934)
*Jeremias Ferrao,Matthijs van der Lende,Ilija Lichkovski,Clement Neo*

Main category: cs.AI

TL;DR: FSRL是一种透明的对齐框架，使用轻量级适配器通过调节稀疏自编码器的可解释特征来引导LLM行为，效果与RLHF相当但更可解释。


<details>
  <summary>Details</summary>
Motivation: RLHF方法导致参数变化分散且不透明，难以理解模型内化了什么，需要更透明的对齐方法。

Method: 使用稀疏自编码器提取可解释特征，训练轻量级适配器通过调节这些特征来实现行为引导和偏好优化。

Result: FSRL在偏好优化方面与RLHF方法效果相当，机制分析发现适配器策略更倾向于提升风格特征而非显式对齐概念。

Conclusion: FSRL为可解释的模型控制和诊断对齐机制提供了工具，揭示了偏好优化过程可能将风格呈现作为质量的代理指标。

Abstract: Aligning large language models is critical for their usability and safety.
However, the prevailing approach of Reinforcement Learning from Human Feedback
(RLHF) induces diffuse, opaque parameter changes, making it difficult to
discern what the model has internalized. Hence, we introduce Feature Steering
with Reinforcement Learning (FSRL), a transparent alignment framework that
trains a lightweight adapter to steer behavior by modulating interpretable
features from a Sparse Autoencoder (SAE). First, we demonstrate that FSRL is an
effective method for preference optimization and is comparable with current
RLHF methods. We then perform mechanistic analysis on the trained adapter, and
find that its policy systematically promotes style features over explicit
alignment concepts, suggesting that the preference optimization process rewards
stylistic presentation as a proxy for quality. Ultimately, we hope that FSRL
provides a tool for both interpretable model control and diagnosing the
internal mechanisms of alignment.

</details>


### [74] [Black-box Model Merging for Language-Model-as-a-Service with Massive Model Repositories](https://arxiv.org/abs/2509.12951)
*Shilian Chen,Jie Zhou,Tianyu Huai,Yujiang Lu,Junsong Li,Bihao Zhan,Qianjun Pan,Yutao Yang,Xin Li,Qin Chen,Hang Yan,Liang He*

Main category: cs.AI

TL;DR: 通过行为优化算法实现黑盒大语言模型合并，仅需API推理请求即可完成模型合并，无需获取模型参数


<details>
  <summary>Details</summary>
Motivation: 解决对于GPT-4等黑盒大模型，因无法获取模型参数而无法使用传统任务向量合并方法的挑战

Method: 基于行为优化算法的Evo-Merging框架，包含稀疏基去噪和符号知觉缩放两个核心组件，仅使用API推理请求

Result: 在多个任务上达到了最先进的性能，显著超过现有强基线方法

Conclusion: 该方法为黑盒大模型合并提供了有效解决方案，并通过理论分析证明了不对称稀疏化的有效性

Abstract: Model merging refers to the process of integrating multiple distinct models
into a unified model that preserves and combines the strengths and capabilities
of the individual models. Most existing approaches rely on task vectors to
combine models, typically under the assumption that model parameters are
accessible. However, for extremely large language models (LLMs) such as GPT-4,
which are often provided solely as black-box services through API interfaces
(Language-Model-as-a-Service), model weights are not available to end users.
This presents a significant challenge, which we refer to as black-box model
merging (BMM) with massive LLMs. To address this challenge, we propose a
derivative-free optimization framework based on the evolutionary algorithm
(Evo-Merging) that enables effective model merging using only inference-time
API queries. Our method consists of two key components: (1) sparsity-based
denoising, designed to identify and filter out irrelevant or redundant
information across models, and (2) sign-aware scaling, which dynamically
computes optimal combination weights for the relevant models based on their
performance. We also provide a formal justification, along with a theoretical
analysis, for our asymmetric sparsification. Extensive experimental evaluations
demonstrate that our approach achieves state-of-the-art results on a range of
tasks, significantly outperforming existing strong baselines.

</details>


### [75] [Forget What's Sensitive, Remember What Matters: Token-Level Differential Privacy in Memory Sculpting for Continual Learning](https://arxiv.org/abs/2509.12958)
*Bihao Zhan,Jie Zhou,Junsong Li,Yutao Yang,Shilian Chen,Qianjun Pan,Xin Li,Wen Wu,Xingjiao Wu,Qin Chen,Hang Yan,Liang He*

Main category: cs.AI

TL;DR: 提出了隐私增强持续学习框架PeCL，通过动态差分隐私策略和隐私引导记忆雕刻模块，在保护敏感信息的同时保持模型性能


<details>
  <summary>Details</summary>
Motivation: 传统持续学习模型面临严重的隐私挑战，统一的差分隐私预算会导致模型性能显著下降，阻碍在隐私敏感领域的部署

Method: 1. 引入token级动态差分隐私策略，根据语义敏感性自适应分配隐私预算；2. 集成隐私引导记忆雕刻模块，智能遗忘敏感信息同时保留任务不变的历史知识

Result: PeCL在隐私保护和模型效用之间实现了优越的平衡，在保持先前任务高精度的同时确保强大的隐私保护，优于基线模型

Conclusion: 该框架为隐私敏感的持续学习应用提供了有效的解决方案，通过选择性保护和遗忘机制解决了传统方法的局限性

Abstract: Continual Learning (CL) models, while adept at sequential knowledge
acquisition, face significant and often overlooked privacy challenges due to
accumulating diverse information. Traditional privacy methods, like a uniform
Differential Privacy (DP) budget, indiscriminately protect all data, leading to
substantial model utility degradation and hindering CL deployment in
privacy-sensitive areas. To overcome this, we propose a privacy-enhanced
continual learning (PeCL) framework that forgets what's sensitive and remembers
what matters. Our approach first introduces a token-level dynamic Differential
Privacy strategy that adaptively allocates privacy budgets based on the
semantic sensitivity of individual tokens. This ensures robust protection for
private entities while minimizing noise injection for non-sensitive, general
knowledge. Second, we integrate a privacy-guided memory sculpting module. This
module leverages the sensitivity analysis from our dynamic DP mechanism to
intelligently forget sensitive information from the model's memory and
parameters, while explicitly preserving the task-invariant historical knowledge
crucial for mitigating catastrophic forgetting. Extensive experiments show that
PeCL achieves a superior balance between privacy preserving and model utility,
outperforming baseline models by maintaining high accuracy on previous tasks
while ensuring robust privacy.

</details>


### [76] [Toward PDDL Planning Copilot](https://arxiv.org/abs/2509.12987)
*Yarin Benyamin,Argaman Mordoch,Shahaf S. Shperberg,Roni Stern*

Main category: cs.AI

TL;DR: Planning Copilot是一个基于MCP协议的聊天机器人，通过集成多种规划工具让LLMs能够执行可靠的长期规划任务，无需领域特定微调即可显著提升规划能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在自主执行复杂任务时缺乏可靠的长期规划能力，需要一种方法来弥补这一缺陷。

Method: 利用Model Context Protocol (MCP)标准，集成多种规划工具（语法检查、规划器选择、调用、验证和模拟执行），通过自然语言指令调用这些工具。

Result: 实验表明Planning Copilot在使用三个开源LLM时表现远超无工具支持的相同LLMs，并且在有限定性比较中显著优于GPT-5。

Conclusion: 专用规划工具是使LLMs能够有效执行规划任务的有效途径，即使基于较小的LLM也能超越大型商业模型的表现。

Abstract: Large Language Models (LLMs) are increasingly being used as autonomous agents
capable of performing complicated tasks. However, they lack the ability to
perform reliable long-horizon planning on their own. This paper bridges this
gap by introducing the Planning Copilot, a chatbot that integrates multiple
planning tools and allows users to invoke them through instructions in natural
language. The Planning Copilot leverages the Model Context Protocol (MCP), a
recently developed standard for connecting LLMs with external tools and
systems. This approach allows using any LLM that supports MCP without
domain-specific fine-tuning. Our Planning Copilot supports common planning
tasks such as checking the syntax of planning problems, selecting an
appropriate planner, calling it, validating the plan it generates, and
simulating their execution. We empirically evaluate the ability of our Planning
Copilot to perform these tasks using three open-source LLMs. The results show
that the Planning Copilot highly outperforms using the same LLMs without the
planning tools. We also conducted a limited qualitative comparison of our tool
against Chat GPT-5, a very recent commercial LLM. Our results shows that our
Planning Copilot significantly outperforms GPT-5 despite relying on a much
smaller LLM. This suggests dedicated planning tools may be an effective way to
enable LLMs to perform planning tasks.

</details>


### [77] [Data-driven Methods of Extracting Text Structure and Information Transfer](https://arxiv.org/abs/2509.12999)
*Shinichi Honna,Taichi Murayama,Akira Matsui*

Main category: cs.AI

TL;DR: 安娜·卡列尼娜原则在不同文本类型中的结构模式验证：小说呈现反向AKP顺序模式，维基百科结合AKP与有序模式，学术论文顺序反向AKP但位置保持噪声，电影按类型分化


<details>
  <summary>Details</summary>
Motivation: 验证安娜·卡列尼娜原则（成功需满足少量必要条件，失败形式多样）及其反向模式在不同文本媒介中的适用性，探索不同领域文本结构的共性与差异

Method: 将文本表示为功能块序列，通过转换顺序和位置评估收敛性，分析小说、在线百科全书、研究论文和电影四种媒介

Result: 结构原则因媒介而异：小说顺序遵循反向AKP，维基百科结合AKP与有序模式，学术论文顺序反向AKP但位置噪声，电影按类型分化

Conclusion: 成功取决于特定媒介的结构约束，而失败在不同领域呈现不同形态，结构模式具有媒介特异性

Abstract: The Anna Karenina Principle (AKP) holds that success requires satisfying a
small set of essential conditions, whereas failure takes diverse forms. We test
AKP, its reverse, and two further patterns described as ordered and noisy
across novels, online encyclopedias, research papers, and movies. Texts are
represented as sequences of functional blocks, and convergence is assessed in
transition order and position. Results show that structural principles vary by
medium: novels follow reverse AKP in order, Wikipedia combines AKP with ordered
patterns, academic papers display reverse AKP in order but remain noisy in
position, and movies diverge by genre. Success therefore depends on structural
constraints that are specific to each medium, while failure assumes different
shapes across domains.

</details>


### [78] [A Visualized Framework for Event Cooperation with Generative Agents](https://arxiv.org/abs/2509.13011)
*Yuyang Tian,Shunqiang Mao,Wenchang Gao,Lanlan Qiu,Tianxing He*

Main category: cs.AI

TL;DR: 开发了MiniAgentPro可视化平台，用于评估LLM代理在物理环境中的事件组织和交互能力，通过8个事件场景测试发现基础设置表现良好但复杂协调存在挑战


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理社会模拟框架缺乏系统的事件组织评估和物理环境可视化集成，限制了代理在空间中导航和物品交互的真实性

Method: 开发MiniAgentPro可视化平台，包含直观的地图编辑器用于定制环境，以及带有流畅动画的模拟播放器，并基于此工具构建包含8个不同事件场景的测试集

Result: 使用GPT-4o评估显示在基础设置中表现强劲，但在困难变体中突显了协调挑战

Conclusion: MiniAgentPro平台为LLM代理在物理环境中的能力评估提供了有效的可视化工具，揭示了当前代理在复杂协调任务中的局限性

Abstract: Large Language Models (LLMs) have revolutionized the simulation of agent
societies, enabling autonomous planning, memory formation, and social
interactions. However, existing frameworks often overlook systematic
evaluations for event organization and lack visualized integration with
physically grounded environments, limiting agents' ability to navigate spaces
and interact with items realistically. We develop MiniAgentPro, a visualization
platform featuring an intuitive map editor for customizing environments and a
simulation player with smooth animations. Based on this tool, we introduce a
comprehensive test set comprising eight diverse event scenarios with basic and
hard variants to assess agents' ability. Evaluations using GPT-4o demonstrate
strong performance in basic settings but highlight coordination challenges in
hard variants.

</details>


### [79] [Reasoning with Preference Constraints: A Benchmark for Language Models in Many-to-One Matching Markets](https://arxiv.org/abs/2509.13131)
*Marylou Fauchard,Florian Carichon,Margarida Carvalho,Golnoosh Farnadi*

Main category: cs.AI

TL;DR: 这篇论文提出了一个新的大学招生问题基准测试集，用于评估大语言模型在匹配问题中的推理能力，发现虽然LLMs能满足某些约束，但在可行性、稳定性和最优性方面仍遇到困难，且不同提示策略效果差异显著。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型在数学推理和组合优化任务中表现出艰，但在需要在偏好和结构约束下进行推理的匹配问题不能力仍未得到充分探索。需要建立专门的测试基准来评估LLMs在这类问题中的表现。

Method: 提出了一个包含369个实例的大学招生问题基准测试集，从可行性、稳定性和最优性三个维度评估多个开源LLMs。测试了不同的提示策略，包括思维链、上下文学习和角色基于提示，以及迭代提示与自动生成反馈的结合。

Result: 结果显示LLMs虽然能满足某些约束，但无法一致满足所有评估标准。具有推理能力的LLMs（如QwQ和GPT-oss）显著超过传统模型（如Llama、Qwen或Mistral）。不同提示策略效果差异显著，没有哪种提示能一致提供最佳性能。迭代提示的性能并非单调增长，可能早期达到峰值后在后续尝试中显著下降。

Conclusion: 这项工作为模型推理能力和提示策略在具有偏好约束的组合优化问题中的有效性提供了新视角。它强调了在复杂匹配问题中开发专门推理机制的重要性，并指出了当前提示技术的局限性。

Abstract: Recent advances in reasoning with large language models (LLMs) have
demonstrated strong performance on complex mathematical tasks, including
combinatorial optimization. Techniques such as Chain-of-Thought and In-Context
Learning have further enhanced this capability, making LLMs both powerful and
accessible tools for a wide range of users, including non-experts. However,
applying LLMs to matching problems, which require reasoning under preferential
and structural constraints, remains underexplored. To address this gap, we
introduce a novel benchmark of 369 instances of the College Admission Problem,
a canonical example of a matching problem with preferences, to evaluate LLMs
across key dimensions: feasibility, stability, and optimality. We employ this
benchmark to assess the performance of several open-weight LLMs. Our results
first reveal that while LLMs can satisfy certain constraints, they struggle to
meet all evaluation criteria consistently. They also show that reasoning LLMs,
like QwQ and GPT-oss, significantly outperform traditional models such as
Llama, Qwen or Mistral, defined here as models used without any dedicated
reasoning mechanisms. Moreover, we observed that LLMs reacted differently to
the various prompting strategies tested, which include Chain-of-Thought,
In-Context Learning and role-based prompting, with no prompt consistently
offering the best performance. Finally, we report the performances from
iterative prompting with auto-generated feedback and show that they are not
monotonic; they can peak early and then significantly decline in later
attempts. Overall, this work offers a new perspective on model reasoning
performance and the effectiveness of prompting strategies in combinatorial
optimization problems with preferential constraints.

</details>


### [80] [Agentic AI for Financial Crime Compliance](https://arxiv.org/abs/2509.13137)
*Henrik Axelsen,Valdemar Licht,Jan Damsgaard*

Main category: cs.AI

TL;DR: 这篇论文通过行动设计研究方法，为数字化金融平台开发了一种可解释、可追踪的代理智能系统，用于金融犯罪合规管理，并在监管约束下实现了工作流重构。


<details>
  <summary>Details</summary>
Motivation: 金融犯罪合规管理成本高、复杂性增加，但效果提升不明显。虽然AI有潜力，但现有解决方案不透明且与监管要求对齐差。

Method: 采用行动设计研究(ADR)方法，与金融科技公司和监管方合作开发。通过以工件为中心的建模，为自治代理分配明确边界角色，实现任务特定模型路由和审计日志记录。

Result: 开发了一个自动化备案、监控、调查和报告的实际原型系统，强调可解释性、可追踪性和设计合规性。提供了参考架构和实践见解。

Conclusion: 这项研究扩展了信息系统领域关于AI合规的研究，证明在负责任治理架构内嵌入自动化，可以在高风险监管环境中支持透明度和机构信任。

Abstract: The cost and complexity of financial crime compliance (FCC) continue to rise,
often without measurable improvements in effectiveness. While AI offers
potential, most solutions remain opaque and poorly aligned with regulatory
expectations. This paper presents the design and deployment of an agentic AI
system for FCC in digitally native financial platforms. Developed through an
Action Design Research (ADR) process with a fintech firm and regulatory
stakeholders, the system automates onboarding, monitoring, investigation, and
reporting, emphasizing explainability, traceability, and compliance-by-design.
Using artifact-centric modeling, it assigns clearly bounded roles to autonomous
agents and enables task-specific model routing and audit logging. The
contribution includes a reference architecture, a real-world prototype, and
insights into how Agentic AI can reconfigure FCC workflows under regulatory
constraints. Our findings extend IS literature on AI-enabled compliance by
demonstrating how automation, when embedded within accountable governance
structures, can support transparency and institutional trust in high-stakes,
regulated environments.

</details>


### [81] [G-CSEA: A Graph-Based Conflict Set Extraction Algorithm for Identifying Infeasibility in Pseudo-Boolean Models](https://arxiv.org/abs/2509.13203)
*Kanishk Garg,Saranya D.,Sanal Kumar,Saurabh Singh,Anupam Purwar*

Main category: cs.AI

TL;DR: 这篇论文提出了一种基于图论的冲突集提取算法(G-CSEA)，用于高效识别伪布尔约束模型中的不可行子集


<details>
  <summary>Details</summary>
Motivation: 解决伪布尔约束模型中冲突约束识别的效率问题，现有方法如反复可行性检查或对偶射分析在这类模型中效果有限

Method: 受SAT求解器中冲突驱动子句学习(CDCL)的启发，构建含义图并在检测到冲突时追溯所有贡献约束，生成冲突集

Result: 提出的G-CSEA算法能够高效提取冲突集，可选性使用QuickXplain进一步最小化以生成不可约简不可行子集(IIS)

Conclusion: 该方法为伪布尔约束模型提供了一种效率更高的冲突识别方法，适用于劳动力排程等含复杂规则约束的应用场景

Abstract: Workforce scheduling involves a variety of rule-based constraints-such as
shift limits, staffing policies, working hour restrictions, and many similar
scheduling rules-which can interact in conflicting ways, leading to infeasible
models. Identifying the underlying causes of such infeasibility is critical for
resolving scheduling issues and restoring feasibility. A common diagnostic
approach is to compute Irreducible Infeasible Subsets (IISs): minimal sets of
constraints that are jointly infeasible but become feasible when any one is
removed. We consider models formulated using pseudo-Boolean constraints with
inequality relations over binary variables, which naturally encode scheduling
logic. Existing IIS extraction methods such as Additive Deletion and
QuickXplain rely on repeated feasibility checks, often incurring large numbers
of solver calls. Dual ray analysis, while effective for LP-based models, may
fail when the relaxed problem is feasible but the underlying pseudo-Boolean
model is not. To address these limitations, we propose Graph-based Conflict Set
Extraction Algorithm (G-CSEA) to extract a conflict set, an approach inspired
by Conflict-Driven Clause Learning (CDCL) in SAT solvers. Our method constructs
an implication graph during constraint propagation and, upon detecting a
conflict, traces all contributing constraints across both decision branches.
The resulting conflict set can optionally be minimized using QuickXplain to
produce an IIS.

</details>


### [82] [Simulating Clinical AI Assistance using Multimodal LLMs: A Case Study in Diabetic Retinopathy](https://arxiv.org/abs/2509.13234)
*Nadim Barakat,William Lotter*

Main category: cs.AI

TL;DR: 这篇论文研究多模态大语言模型在糖尿病视网膜病检测中的应用，比较了GPT-4o和MedGemma两款模型的性能，并分析了不同输出格式对临床AI协作的影响。


<details>
  <summary>Details</summary>
Motivation: 目前FDA批准的糖尿病视网膜病检测AI系统主要提供二进制转诊输出，这种最小化输出可能限制了临床信任和实用性。需要找到最有效的输出格式来提升医生与AI的协作性能。

Method: 使用IDRiD和Messidor-2数据集评估GPT-4o和MedGemma两款MLLM模型。实验包括：(1)基准评估，(2)使用合成预测模拟AI协助，(3)实际AI到AI协作（GPT-4o整合MedGemma输出）。

Result: MedGemma在基准评测中表现更好，效底和AUROC更高；GPT-4o特异性近于完美但效底低。两款模型都能根据模拟AI输入调整预测，但GPT-4o在错误输入时性能崩溃，MedGemma更稳定。实际协作中，GPT-4o在MedGemma描述性输出指导下取得了强劲结果（AUROC达0.96）。

Conclusion: MLLM模型可以改善DR筛查流程，并作为可扩展的模拟器研究临床AI协助。过程清晰的输出格式能提升可解释性和临床信任，而过开源轻量模型如MedGemma在资源稀缺环境中尤其价值。

Abstract: Diabetic retinopathy (DR) is a leading cause of blindness worldwide, and AI
systems can expand access to fundus photography screening. Current FDA-cleared
systems primarily provide binary referral outputs, where this minimal output
may limit clinical trust and utility. Yet, determining the most effective
output format to enhance clinician-AI performance is an empirical challenge
that is difficult to assess at scale. We evaluated multimodal large language
models (MLLMs) for DR detection and their ability to simulate clinical AI
assistance across different output types. Two models were tested on IDRiD and
Messidor-2: GPT-4o, a general-purpose MLLM, and MedGemma, an open-source
medical model. Experiments included: (1) baseline evaluation, (2) simulated AI
assistance with synthetic predictions, and (3) actual AI-to-AI collaboration
where GPT-4o incorporated MedGemma outputs. MedGemma outperformed GPT-4o at
baseline, achieving higher sensitivity and AUROC, while GPT-4o showed
near-perfect specificity but low sensitivity. Both models adjusted predictions
based on simulated AI inputs, but GPT-4o's performance collapsed with incorrect
ones, whereas MedGemma remained more stable. In actual collaboration, GPT-4o
achieved strong results when guided by MedGemma's descriptive outputs, even
without direct image access (AUROC up to 0.96). These findings suggest MLLMs
may improve DR screening pipelines and serve as scalable simulators for
studying clinical AI assistance across varying output configurations. Open,
lightweight models such as MedGemma may be especially valuable in low-resource
settings, while descriptive outputs could enhance explainability and clinician
trust in clinical workflows.

</details>


### [83] [A Scenario-Driven Cognitive Approach to Next-Generation AI Memory](https://arxiv.org/abs/2509.13235)
*Linyue Cai,Yuyang Cheng,Xiaoding Shao,Huiming Wang,Yong Zhao,Wei Zhang,Kang Li*

Main category: cs.AI

TL;DR: 提出基于认知场景驱动的COLMA记忆架构，解决现有AI记忆系统适应性差、多模态整合不足等问题，为AGI发展提供结构化基础


<details>
  <summary>Details</summary>
Motivation: 随着AI向AGI发展，现有记忆系统存在适应性有限、多模态整合不足、无法支持持续学习等问题，需要更鲁棒和类人的记忆系统

Method: 采用场景驱动方法，从代表性认知场景中提取关键功能需求，形成统一设计原则，提出认知分层记忆架构(COLMA)，整合认知场景、记忆过程和存储机制

Result: 开发出COLMA框架，为构建能够终身学习和类人推理的AI系统提供结构化基础

Conclusion: COLMA架构有助于AGI的务实发展，为解决当前记忆系统局限性提供了创新解决方案

Abstract: As artificial intelligence advances toward artificial general intelligence
(AGI), the need for robust and human-like memory systems has become
increasingly evident. Current memory architectures often suffer from limited
adaptability, insufficient multimodal integration, and an inability to support
continuous learning. To address these limitations, we propose a scenario-driven
methodology that extracts essential functional requirements from representative
cognitive scenarios, leading to a unified set of design principles for
next-generation AI memory systems. Based on this approach, we introduce the
\textbf{COgnitive Layered Memory Architecture (COLMA)}, a novel framework that
integrates cognitive scenarios, memory processes, and storage mechanisms into a
cohesive design. COLMA provides a structured foundation for developing AI
systems capable of lifelong learning and human-like reasoning, thereby
contributing to the pragmatic development of AGI.

</details>


### [84] [RepIt: Representing Isolated Targets to Steer Language Models](https://arxiv.org/abs/2509.13281)
*Vincent Siu,Nathan W. Henry,Nicholas Crispino,Yang Liu,Dawn Song,Chenguang Wang*

Main category: cs.AI

TL;DR: RepIt是一个简单高效的概念表示分离框架，能够从少量数据中提取纯净的概念向量，实现对LLM行为的精准干预，同时引发了对模型安全性的新担忧。


<details>
  <summary>Details</summary>
Motivation: 现有的激活引导方法往往产生比预期更广泛的影响，需要更纯净的概念向量来实现针对性干预和更细粒度的LLM行为理解。

Method: 提出了RepIt框架，通过数据高效的方式分离概念特定的表示，能够在少量示例（如12个）和单个A6000 GPU上提取稳健的目标表示。

Result: 在五个前沿LLM上，RepIt实现了精准干预：选择性抑制特定概念的拒绝行为，同时保持其他地方的拒绝能力，使模型能够回答WMD相关问题但在标准基准测试中仍保持安全评分。纠正信号仅定位到100-200个神经元。

Conclusion: 通过RepIt分离拒绝向量，证明了针对性干预可以抵消过度泛化，为更细粒度的模型行为控制奠定了基础，但同时也引发了关于计算和数据需求较低可能带来的安全担忧。

Abstract: While activation steering in large language models (LLMs) is a growing area
of research, methods can often incur broader effects than desired. This
motivates isolation of purer concept vectors to enable targeted interventions
and understand LLM behavior at a more granular level. We present RepIt, a
simple and data-efficient framework for isolating concept-specific
representations. Across five frontier LLMs, RepIt enables precise
interventions: it selectively suppresses refusal on targeted concepts while
preserving refusal elsewhere, producing models that answer WMD-related
questions while still scoring as safe on standard benchmarks. We further show
that the corrective signal localizes to just 100-200 neurons and that robust
target representations can be extracted from as few as a dozen examples on a
single A6000. This efficiency raises a dual concern: manipulations can be
performed with modest compute and data to extend to underrepresented
data-scarce topics while evading existing benchmarks. By disentangling refusal
vectors with RepIt, this work demonstrates that targeted interventions can
counteract overgeneralization, laying the foundation for more granular control
of model behavior.

</details>


### [85] [Shapes of Cognition for Computational Cognitive Modeling](https://arxiv.org/abs/2509.13288)
*Marjorie McShane,Sergei Nirenburg,Sanjay Oruganti,Jesse English*

Main category: cs.AI

TL;DR: Shapes of cognition是一个新的计算认知建模范式，通过记忆感官、语言、概念、情景和程序知识的星座模式，让智能体像人类一样通过期望典型性、识别模式、习惯行动、类比推理和最小化认知负荷来处理复杂现实。


<details>
  <summary>Details</summary>
Motivation: 为语言赋能的智能代理(LEIAs)开发一个能够模拟人类认知方式的计算模型，使智能体能够像人类一样处理现实世界的复杂性，同时保证系统的可解释性、可扩展性和可信赖性。

Method: 采用基于形状的建模方法，包括特定的目标、假设、建模策略、知识库和实际模型，在特定认知架构中实现。使用形状作为记忆的知识星座来处理典型情况，并通过基于形状的恢复方法处理非典型结果。

Result: 提出了一个具体的形状建模框架，能够处理广泛的现象，实现了知识驱动和混合AI的新方法，为构建可信赖的智能代理系统提供了理论基础。

Conclusion: 形状认知范式不仅为LEIAs提供了具体的建模方法，其原理还可以更广泛地应用于知识驱动和混合AI领域，为这些传统方法注入新的活力。

Abstract: Shapes of cognition is a new conceptual paradigm for the computational
cognitive modeling of Language-Endowed Intelligent Agents (LEIAs). Shapes are
remembered constellations of sensory, linguistic, conceptual, episodic, and
procedural knowledge that allow agents to cut through the complexity of real
life the same way as people do: by expecting things to be typical, recognizing
patterns, acting by habit, reasoning by analogy, satisficing, and generally
minimizing cognitive load to the degree situations permit. Atypical outcomes
are treated using shapes-based recovery methods, such as learning on the fly,
asking a human partner for help, or seeking an actionable, even if imperfect,
situational understanding. Although shapes is an umbrella term, it is not
vague: shapes-based modeling involves particular objectives, hypotheses,
modeling strategies, knowledge bases, and actual models of wide-ranging
phenomena, all implemented within a particular cognitive architecture. Such
specificity is needed both to vet our hypotheses and to achieve our practical
aims of building useful agent systems that are explainable, extensible, and
worthy of our trust, even in critical domains. However, although the LEIA
example of shapes-based modeling is specific, the principles can be applied
more broadly, giving new life to knowledge-based and hybrid AI.

</details>
