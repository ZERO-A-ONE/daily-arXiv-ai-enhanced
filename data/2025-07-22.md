<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 37]
- [cs.CR](#cs.CR) [Total: 31]
- [cs.AI](#cs.AI) [Total: 58]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Impact of Code Context and Prompting Strategies on Automated Unit Test Generation with Modern General-Purpose Large Language Models](https://arxiv.org/abs/2507.14256)
*Jakub Walczak,Piotr Tomalak,Artur Laskowski*

Main category: cs.SE

TL;DR: 论文研究了代码上下文和提示策略对LLM生成单元测试质量的影响，发现包含文档字符串显著提升代码充分性，而链式思维提示策略表现最佳。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过生成式AI自动生成高质量的单元测试，提升软件开发效率。

Method: 评估不同LLM在多种上下文和提示策略下生成的单元测试质量，包括代码充分性、分支覆盖率和变异分数。

Result: 包含文档字符串显著提升测试充分性，链式思维提示策略效果最佳，M5模型表现最优。

Conclusion: 生成式AI在单元测试生成中具有潜力，代码上下文和提示策略是关键因素。

Abstract: Generative AI is gaining increasing attention in software engineering, where
testing remains an indispensable reliability mechanism. According to the widely
adopted testing pyramid, unit tests constitute the majority of test cases and
are often schematic, requiring minimal domain expertise. Automatically
generating such tests under the supervision of software engineers can
significantly enhance productivity during the development phase of the software
lifecycle.
  This paper investigates the impact of code context and prompting strategies
on the quality and adequacy of unit tests generated by various large language
models (LLMs) across several families. The results show that including
docstrings notably improves code adequacy, while further extending context to
the full implementation yields definitely smaller gains. Notably, the
chain-of-thought prompting strategy -- applied even to 'reasoning' models --
achieves the best results, with up to 96.3\% branch coverage, a 57\% average
mutation score, and near-perfect compilation success rate. Among the evaluated
models, M5 (Gemini 2.5 Pro) demonstrated superior performance in both mutation
score and branch coverage being still in top in terms of compilation success
rate.
  All the code and resulting test suites are publicly available at
https://github.com/peetery/LLM-analysis.

</details>


### [2] [Leveraging LLMs for Formal Software Requirements -- Challenges and Prospects](https://arxiv.org/abs/2507.14330)
*Arshad Beg,Diarmuid O'Donoghue,Rosemary Monahan*

Main category: cs.SE

TL;DR: VERIFAI项目旨在通过NLP、本体建模和LLMs等技术，自动化或半自动化地从非正式需求生成可验证的规范。


<details>
  <summary>Details</summary>
Motivation: 解决从非正式和模糊的自然语言需求生成形式化规范的关键挑战。

Method: 结合NLP、本体建模、构件重用和LLMs等技术。

Result: 初步文献综述，识别了挑战和潜在研究方向。

Conclusion: VERIFAI为形式化规范生成提供了新的自动化途径。

Abstract: Software correctness is ensured mathematically through formal verification,
which involves the resources of generating formal requirement specifications
and having an implementation that must be verified. Tools such as
model-checkers and theorem provers ensure software correctness by verifying the
implementation against the specification. Formal methods deployment is
regularly enforced in the development of safety-critical systems e.g.
aerospace, medical devices and autonomous systems. Generating these
specifications from informal and ambiguous natural language requirements
remains the key challenge. Our project, VERIFAI^{1}, aims to investigate
automated and semi-automated approaches to bridge this gap, using techniques
from Natural Language Processing (NLP), ontology-based domain modelling,
artefact reuse, and large language models (LLMs). This position paper presents
a preliminary synthesis of relevant literature to identify recurring challenges
and prospective research directions in the generation of verifiable
specifications from informal requirements.

</details>


### [3] [Developing Shared Vocabulary System For Collaborative Software Engineering](https://arxiv.org/abs/2507.14396)
*Carey Lai Zheng Hui,Johnson Britto Jessia Esther Leena,Kumuthini Subramanian,Zhao Chenyu,Shubham Rajeshkumar Jariwala*

Main category: cs.SE

TL;DR: 研究探讨了软件工程中沟通误解的技术因素，并通过共享词汇系统改善了文档清晰度和协作效率。


<details>
  <summary>Details</summary>
Motivation: 软件工程中沟通不畅导致误解和低效，研究旨在解决这一问题。

Method: 采用设计科学研究框架，分三阶段：问题识别、方法开发和实证验证。

Result: 共享词汇系统显著提升了信息密度、文档清晰度和协作效率。

Conclusion: 研究为改善软件工程沟通提供了实用建议，并指出了未来研究方向。

Abstract: Effective communication is a critical factor in successful software
engineering collaboration. However, communication gaps remain a persistent
challenge, often leading to misunderstandings, inefficiencies, and defects.
This research investigates the technical factors contributing to such
misunderstandings and explores the measurable benefits of establishing shared
vocabulary systems within software documentation and codebases. Using a Design
Science Research (DSR) framework, the study was structured into three iterative
phases: problem identification, method development, and empirical validation.
The problem identification phase involved thematic analysis of communication
data and semi-structured interviews, revealing key factors such as ambiguous
messaging, misalignment in documentation, inconsistent code review feedback,
and API integration miscommunication. Grounded Theory principles were employed
to design a structured methodology for collaborative vocabulary development.
Empirical validation through controlled experiments demonstrated that while
initial adoption introduced overhead, the shared vocabulary system
significantly improved information density, documentation clarity, and
collaboration efficiency over time. Findings offer actionable insights for
improving communication practices in software engineering, while also
identifying limitations and directions for future research.

</details>


### [4] [On the Effect of Token Merging on Pre-trained Models for Code](https://arxiv.org/abs/2507.14423)
*Mootez Saad,Hao Li,Tushar Sharma,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: 研究探讨了代码语言模型中子令牌隐藏表示的合并策略，提出两种方法：基于平均和基于学习的方法，实验表明能减少计算量1%至19%，下游任务表现有升有降。


<details>
  <summary>Details</summary>
Motivation: 传统代码语言模型的令牌化输出较长，可能增加计算开销，研究旨在通过合并子令牌隐藏表示来优化。

Method: 提出两种子令牌隐藏表示合并策略：基于平均和基于学习的方法，并与六种代码语言模型结合进行实验。

Result: 计算量减少1%至19%，下游任务中漏洞检测F1分数下降1.82分，代码翻译CodeBLEU提升2.47分。

Conclusion: 研究为优化代码语言模型的计算效率和下游性能提供了有效方法。

Abstract: Tokenization is a fundamental component of language models for code. It
involves breaking down the input into units that are later passed to the
language model stack to learn high-dimensional representations used in various
contexts, from classification to generation. However, the output of these
tokenizers is often longer than that traditionally used in compilers and
interpreters. This could result in undesirable effects, such as increased
computational overhead. In this work, we investigate the effect of merging the
hidden representations of subtokens that belong to the same semantic unit, such
as subtokens that form a single identifier. We propose two strategies: one
based on averaging the representations and another that leverages a
learning-based approach. Both methods can be seamlessly integrated with
existing language models for code. We conduct experiments using six language
models for code: CodeBERT, GraphCodeBERT, UniXCoder, CdoeT5, CodeT5+ (220M),
and CodeT5+ (770M), across three software engineering tasks: vulnerability
detection, code classification, and code translation. Results show that these
strategies can reduce the number of floating-point operations by $1\%$ to
$19\%$. Regarding downstream performance, the most significant degradation was
observed in the vulnerability detection task, where the F1 score decreased by
$1.82$ points compared to the baseline. In contrast, for code translation, we
observed an improvement of $2.47$ points in CodeBLEU. This work contributes to
the broader effort of improving language models for code across multiple
dimensions, including both computational efficiency and downstream performance.

</details>


### [5] [Architectural Degradation: Definition, Motivations, Measurement and Remediation Approaches](https://arxiv.org/abs/2507.14547)
*Noman Ahmad,Ruoyu Su,Matteo Esposito,Andrea Janes,Valentina Lenarduzzi,Davide Taibi*

Main category: cs.SE

TL;DR: 论文通过多声部文献综述，统一了对架构退化的理解，提出了涵盖架构、代码和过程债务的分类法，并揭示了当前研究的不足。


<details>
  <summary>Details</summary>
Motivation: 架构退化影响系统质量和可维护性，但现有研究定义和策略分散，需统一理解。

Method: 对108项研究进行多声部文献综述，提取定义、原因、指标、工具和修复策略，并开发分类法。

Result: 架构退化从低级问题转向社会技术问题，定义了54项指标和31种测量技术，但持续修复工具不足。

Conclusion: 研究揭示了指标、工具和修复逻辑间的脱节，呼吁整体、主动的策略以实现可持续架构。

Abstract: Architectural degradation, also known as erosion, decay, or aging, impacts
system quality, maintainability, and adaptability. Although widely
acknowledged, current literature shows fragmented definitions, metrics, and
remediation strategies. Our study aims to unify understanding of architectural
degradation by identifying its definitions, causes, metrics, tools, and
remediation approaches across academic and gray literature. We conducted a
multivocal literature review of 108 studies extracting definitions, causes,
metrics, measurement approaches, tools, and remediation strategies. We
developed a taxonomy encompassing architectural, code, and process debt to
explore definition evolution, methodological trends, and research gaps.
Architectural degradation has shifted from a low-level issue to a
socio-technical concern. Definitions now address code violations, design drift,
and structural decay. Causes fall under architectural (e.g., poor
documentation), code (e.g., hasty fixes), and process debt (e.g., knowledge
loss). We identified 54 metrics and 31 measurement techniques, focused on
smells, cohesion/coupling, and evolution. Yet, most tools detect issues but
rarely support ongoing or preventive remediation. Degradation is both technical
and organizational. While detection is well-studied, continuous remediation
remains lacking. Our study reveals missed integration between metrics, tools,
and repair logic, urging holistic, proactive strategies for sustainable
architecture.

</details>


### [6] [Emerging Trends in Software Architecture from the Practitioners Perspective: A Five Year Review](https://arxiv.org/abs/2507.14554)
*Ruoyu Su,Noman ahmad,Matteo Esposito,Andrea Janes,Davide Taibi,Valentina Lenarduzzi*

Main category: cs.SE

TL;DR: 研究分析了五年内八个行业会议中的软件架构趋势，发现Kubernetes、Serverless等技术主导当前实践，主要应用于DevOps后期阶段。


<details>
  <summary>Details</summary>
Motivation: 随着云计算、微服务和容器的兴起，软件架构实践多样化，理解这些变化至关重要。

Method: 分析了5,677个行业会议演讲，使用大型语言模型和专家验证提取技术及其用途和上下文。

Result: Kubernetes、Cloud Native等技术在频率和中心性上占主导，覆盖自动化、协调、云AI等领域。

Conclusion: 少数核心技术主导当前架构实践，研究需更全面关注架构设计、质量和演变。

Abstract: Software architecture plays a central role in the design, development, and
maintenance of software systems. With the rise of cloud computing,
microservices, and containers, architectural practices have diversified.
Understanding these shifts is vital. This study analyzes software architecture
trends across eight leading industry conferences over five years. We
investigate the evolution of software architecture by analyzing talks from top
practitioner conferences, focusing on the motivations and contexts driving
technology adoption. We analyzed 5,677 talks from eight major industry
conferences, using large language models and expert validation to extract
technologies, their purposes, and usage contexts. We also explored how
technologies interrelate and fit within DevOps and deployment pipelines. Among
450 technologies, Kubernetes, Cloud Native, Serverless, and Containers dominate
by frequency and centrality. Practitioners present technology mainly related to
deployment, communication, AI, and observability. We identify five technology
communities covering automation, coordination, cloud AI, monitoring, and
cloud-edge. Most technologies span multiple DevOps stages and support hybrid
deployment. Our study reveals that a few core technologies, like Kubernetes and
Serverless, dominate the contemporary software architecture practice. These are
mainly applied in later DevOps stages, with limited focus on early phases like
planning and coding. We also show how practitioners frame technologies by
purpose and context, reflecting evolving industry priorities. Finally, we
observe how only research can provide a more holistic lens on architectural
design, quality, and evolution.

</details>


### [7] [Harnessing LLMs for Document-Guided Fuzzing of OpenCV Library](https://arxiv.org/abs/2507.14558)
*Bin Duan,Tarek Mahmud,Meiru Che,Yan Yan,Naipeng Dong,Dan Dongseong Kim,Guowei Yang*

Main category: cs.SE

TL;DR: VISTAFUZZ利用大型语言模型（LLMs）解析OpenCV API文档，生成标准化信息并提取参数约束，以系统化测试API，发现17个新bug。


<details>
  <summary>Details</summary>
Motivation: OpenCV作为广泛使用的开源计算机视觉库，其可靠性至关重要，但现有测试方法可能不足。

Method: VISTAFUZZ通过LLMs解析API文档，提取参数约束和依赖关系，生成测试输入。

Result: 测试330个API，发现17个新bug，其中10个已确认，5个已修复。

Conclusion: VISTAFUZZ能有效提升OpenCV库的可靠性，为API测试提供新方法。

Abstract: The combination of computer vision and artificial intelligence is
fundamentally transforming a broad spectrum of industries by enabling machines
to interpret and act upon visual data with high levels of accuracy. As the
biggest and by far the most popular open-source computer vision library, OpenCV
library provides an extensive suite of programming functions supporting
real-time computer vision. Bugs in the OpenCV library can affect the downstream
computer vision applications, and it is critical to ensure the reliability of
the OpenCV library. This paper introduces VISTAFUZZ, a novel technique for
harnessing large language models (LLMs) for document-guided fuzzing of the
OpenCV library. VISTAFUZZ utilizes LLMs to parse API documentation and obtain
standardized API information. Based on this standardized information, VISTAFUZZ
extracts constraints on individual input parameters and dependencies between
these. Using these constraints and dependencies, VISTAFUZZ then generates new
input values to systematically test each target API. We evaluate the
effectiveness of VISTAFUZZ in testing 330 APIs in the OpenCV library, and the
results show that VISTAFUZZ detected 17 new bugs, where 10 bugs have been
confirmed, and 5 of these have been fixed.

</details>


### [8] [A first look at License Variants in the PyPI Ecosystem](https://arxiv.org/abs/2507.14594)
*Weiwei Xu,Hengzhi Ye,Kai Gao,Minghui Zhou*

Main category: cs.SE

TL;DR: 论文研究了开源许可证变体在PyPI生态系统中的影响，提出了LV-Parser和LV-Compat工具以提高许可证分析的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 开源许可证变体在现代软件系统中普遍存在，但缺乏深入理解和有效工具，导致合规性问题。

Method: 通过实证研究分析PyPI中的许可证变体，并开发了基于差异分析和大语言模型的LV-Parser，以及自动化检测工具LV-Compat。

Result: 研究发现许可证变体常见但实质性修改仅占2%，但导致10.7%的下游依赖不兼容。LV-Parser准确率0.936，计算成本降低30%；LV-Compat检测效率提升5.2倍，精度0.98。

Conclusion: 研究填补了许可证变体的知识空白，并为开发者提供了实用的工具以应对开源许可证的复杂性。

Abstract: Open-source licenses establish the legal foundation for software reuse, yet
license variants, including both modified standard licenses and custom-created
alternatives, introduce significant compliance complexities. Despite their
prevalence and potential impact, these variants are poorly understood in modern
software systems, and existing tools do not account for their existence,
leading to significant challenges in both effectiveness and efficiency of
license analysis. To fill this knowledge gap, we conduct a comprehensive
empirical study of license variants in the PyPI ecosystem. Our findings show
that textual variations in licenses are common, yet only 2% involve substantive
modifications. However, these license variants lead to significant compliance
issues, with 10.7% of their downstream dependencies found to be
license-incompatible.
  Inspired by our findings, we introduce LV-Parser, a novel approach for
efficient license variant analysis leveraging diff-based techniques and large
language models, along with LV-Compat, an automated pipeline for detecting
license incompatibilities in software dependency networks. Our evaluation
demonstrates that LV-Parser achieves an accuracy of 0.936 while reducing
computational costs by 30%, and LV-Compat identifies 5.2 times more
incompatible packages than existing methods with a precision of 0.98.
  This work not only provides the first empirical study into license variants
in software packaging ecosystem but also equips developers and organizations
with practical tools for navigating the complex landscape of open-source
licensing.

</details>


### [9] [An Efficient Algorithm for Generating Minimal Unique-Cause MC/DC Test cases for Singular Boolean Expressions](https://arxiv.org/abs/2507.14687)
*Robin Lee,Youngho Nam*

Main category: cs.SE

TL;DR: 本文提出了一种名为'Robin's Rule'的确定性算法，用于高效生成满足Unique-Cause MC/DC的最小测试集，特别适用于单一布尔表达式（SBEs）。


<details>
  <summary>Details</summary>
Motivation: 尽管Unique-Cause MC/DC在关键系统中提供最高可靠性保障，但其高效测试生成方法研究不足。本文旨在填补这一空白。

Method: 提出'Robin's Rule'算法，直接构造最小测试集（N + 1个用例），无需生成完整真值表。

Result: 在TCAS-II规范验证中，该方法以理论最小测试数实现100%覆盖率，且效率优于商业工具。

Conclusion: 'Robin's Rule'为安全关键系统验证提供了实用且最优的解决方案，兼具严谨性和效率。

Abstract: Modified Condition/Decision Coverage (MC/DC) is a mandatory structural
coverage criterion for ensuring the reliability and safety of critical systems.
While its strictest form, Unique-Cause MC/DC, offers the highest assurance,
research on its efficient test generation has been lacking. This gap is
particularly significant, as an analysis of large-scale avionics systems shows
that 99.7% of all conditional decisions are, in fact, Singular Boolean
Expressions (SBEs) the ideal structure for applying Unique-Cause MC/DC. This
paper proposes 'Robin's Rule', a deterministic algorithm that directly
constructs a minimal test set of N + 1 cases to guarantee 100% Unique-Cause
MC/DC for SBEs with N conditions, without generating a full truth table. To
validate our approach, we constructed a benchmark by reformulating the TCAS-II
specifications into SBEs and verified the results using an industry-standard,
certified commercial tool. The results confirm that our method consistently
achieves 100% coverage with the theoretical minimum number of tests and is more
efficient than the commercial tool. This work provides a practical and provably
optimal solution for verifying safety-critical systems, ensuring both rigor and
efficiency.

</details>


### [10] [HistoryFinder: Advancing Method-Level Source Code History Generation with Accurate Oracles and Enhanced Algorithm](https://arxiv.org/abs/2507.14716)
*Shahidul Islam,Ashik Aowal,Md Sharif Uddin,Shaiful Chowdhury*

Main category: cs.SE

TL;DR: 该论文提出了一种新的方法历史生成工具HistoryFinder，通过构建更准确的基准（oracle）并优化性能，显著优于现有工具。


<details>
  <summary>Details</summary>
Motivation: 现有方法历史生成工具的评估因基准不准确而受限，影响了软件工程任务的效率和准确性。

Method: 结合自动化分析和专家手动验证构建了两个新基准（oracle），并开发了HistoryFinder工具。

Result: 在400个方法的评估中，HistoryFinder在精度、召回率和F1分数上均优于其他工具，且运行时间表现优异。

Conclusion: HistoryFinder在准确性和效率上均表现最佳，适合需要高精度和高性能的场景。

Abstract: Reconstructing a method's change history efficiently and accurately is
critical for many software engineering tasks, including maintenance,
refactoring, and comprehension. Despite the availability of method history
generation tools such as CodeShovel and CodeTracker, existing evaluations of
their effectiveness are limited by inaccuracies in the ground truth oracles
used. In this study, we systematically construct two new oracles -- the
corrected CodeShovel oracle and a newly developed HistoryFinder oracle -- by
combining automated analysis with expert-guided manual validation. We also
introduce HistoryFinder, a new method history generation tool designed to
improve not only the accuracy and completeness of method change histories but
also to offer competitive runtime performance. Through extensive evaluation
across 400 methods from 40 open-source repositories, we show that HistoryFinder
consistently outperforms CodeShovel, CodeTracker, IntelliJ, and Git-based
baselines in terms of precision, recall, and F1 score. Moreover, HistoryFinder
achieves competitive runtime performance, offering the lowest mean and median
execution times among all the research-based tools.
  While Git-based tools exhibit the fastest runtimes, this efficiency comes at
the cost of significantly lower precision and recall -- leaving HistoryFinder
as the best overall choice when both accuracy and efficiency are important. To
facilitate adoption, we provide a web interface, CLI, and Java library for
flexible usage.

</details>


### [11] [Investigating the Role of LLMs Hyperparameter Tuning and Prompt Engineering to Support Domain Modeling](https://arxiv.org/abs/2507.14735)
*Vladyslav Bulhakov,Giordano d'Aloisio,Claudio Di Sipio,Antinisca Di Marco,Davide Di Ruscio*

Main category: cs.SE

TL;DR: 本文探讨了如何通过超参数调整和提示工程提高Llama 3.1模型在生成领域模型时的准确性，并在医疗数据模型中取得了显著改进。


<details>
  <summary>Details</summary>
Motivation: 通用大型语言模型（LLMs）在领域建模中存在局限性，而微调模型需要大量计算资源且可能导致灾难性遗忘。

Method: 采用基于搜索的方法调整超参数，并结合提示工程，针对医疗数据模型进行优化。

Result: 优化后的模型在医疗数据模型中表现显著优于基线LLM，并在其他十个应用领域中多数情况下表现更好。

Conclusion: 超参数调整与提示工程结合可提升领域模型生成的效果，但解决方案并非普遍适用。

Abstract: The introduction of large language models (LLMs) has enhanced automation in
software engineering tasks, including in Model Driven Engineering (MDE).
However, using general-purpose LLMs for domain modeling has its limitations.
One approach is to adopt fine-tuned models, but this requires significant
computational resources and can lead to issues like catastrophic forgetting.
  This paper explores how hyperparameter tuning and prompt engineering can
improve the accuracy of the Llama 3.1 model for generating domain models from
textual descriptions. We use search-based methods to tune hyperparameters for a
specific medical data model, resulting in a notable quality improvement over
the baseline LLM. We then test the optimized hyperparameters across ten diverse
application domains.
  While the solutions were not universally applicable, we demonstrate that
combining hyperparameter tuning with prompt engineering can enhance results
across nearly all examined domain models.

</details>


### [12] [Toward Inclusive AI-Driven Development: Exploring Gender Differences in Code Generation Tool Interactions](https://arxiv.org/abs/2507.14770)
*Manaal Basha,Ivan Beschastnikh,Gema Rodriguez-Perez,Cleidson R. B. de Souza*

Main category: cs.SE

TL;DR: 研究探讨代码生成工具（CGTs）在不同性别开发者中的使用差异，分析其对任务结果和认知负荷的影响，旨在推动公平性和包容性。


<details>
  <summary>Details</summary>
Motivation: 随着代码生成工具的普及，其公平性和包容性问题日益突出，但针对不同用户群体的有效性尚未充分研究。

Method: 采用混合被试设计，54名参与者按性别均分，完成编程任务并收集认知负荷、任务表现等数据。

Result: 预计将揭示性别差异对CGT使用的影响，为未来工具设计提供依据。

Conclusion: 研究为CGT的公平性、透明性和包容性设计奠定基础，推动AI工具的平等发展。

Abstract: Context: The increasing reliance on Code Generation Tools (CGTs), such as
Windsurf and GitHub Copilot, are revamping programming workflows and raising
critical questions about fairness and inclusivity. While CGTs offer potential
productivity enhancements, their effectiveness across diverse user groups have
not been sufficiently investigated. Objectives: We hypothesize that developers'
interactions with CGTs vary based on gender, influencing task outcomes and
cognitive load, as prior research suggests that gender differences can affect
technology use and cognitive processing. Methods: The study will employ a
mixed-subjects design with 54 participants, evenly divided by gender for a
counterbalanced design. Participants will complete two programming tasks
(medium to hard difficulty) with only CGT assistance and then with only
internet access. Task orders and conditions will be counterbalanced to mitigate
order effects. Data collection will include cognitive load surveys, screen
recordings, and task performance metrics such as completion time, code
correctness, and CGT interaction behaviors. Statistical analyses will be
conducted to identify statistically significant differences in CGT usage.
Expected Contributions: Our work can uncover gender differences in CGT
interaction and performance among developers. Our findings can inform future
CGT designs and help address usability and potential disparities in interaction
patterns across diverse user groups. Conclusion: While results are not yet
available, our proposal lays the groundwork for advancing fairness,
accountability, transparency, and ethics (FATE) in CGT design. The outcomes are
anticipated to contribute to inclusive AI practices and equitable tool
development for all users.

</details>


### [13] [VeriOpt: PPA-Aware High-Quality Verilog Generation via Multi-Role LLMs](https://arxiv.org/abs/2507.14776)
*Kimia Tasnia,Alexander Garcia,Tasnuva Farheen,Sazadur Rahman*

Main category: cs.SE

TL;DR: VeriOpt是一个利用角色提示和PPA优化框架，使LLM生成高质量可综合Verilog代码，显著提升PPA指标。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在硬件设计中忽视PPA指标，VeriOpt填补这一空白。

Method: 通过角色分工和多模态反馈整合PPA约束，优化代码生成。

Result: 实验显示功耗降低88%，面积减少76%，时序改善73%，功能正确率86%。

Conclusion: VeriOpt在AI驱动硬件设计中平衡功能正确性与质量，推动LLM在生产中的应用。

Abstract: The rapid adoption of large language models(LLMs) in hardware design has
primarily focused on generating functionally correct Verilog code, overlooking
critical Power Performance-Area(PPA) metrics essential for industrial-grade
designs. To bridge this gap, we propose VeriOpt, a novel framework that
leverages role-based prompting and PPA-aware optimization to enable LLMs to
produce high-quality, synthesizable Verilog. VeriOpt structures LLM
interactions into specialized roles (e.g., Planner, Programmer, Reviewer,
Evaluator) to emulate human design workflows, while integrating PPA constraints
directly into the prompting pipeline. By combining multi-modal feedback (e.g.,
synthesis reports, timing diagrams) with PPA aware prompting, VeriOpt achieves
PPA-efficient code generation without sacrificing functional correctness.
Experimental results demonstrate up to 88% reduction in power, 76% reduction in
area and 73% improvement in timing closure compared to baseline LLM-generated
RTL, validated using industry standard EDA tools. At the same time achieves 86%
success rate in functionality evaluation. Our work advances the
state-of-the-art AI-driven hardware design by addressing the critical gap
between correctness and quality, paving the way for reliable LLM adoption in
production workflows.

</details>


### [14] [Enhancing Repository-Level Code Generation with Call Chain-Aware Multi-View Context](https://arxiv.org/abs/2507.14791)
*Yang Liu,Li Zhang,Fang Liu,Zhuohang Wang,Donglin Wei,Zhishuo Yang,Kechi Zhang,Jia Li,Lin Shi*

Main category: cs.SE

TL;DR: RepoScope利用调用链感知的多视角上下文进行仓库级代码生成，通过构建仓库结构语义图（RSSG）和检索四视角上下文，显著提升了代码生成的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在识别仓库的丰富语义和结构关系方面存在不足，导致上下文信息狭窄且不准确。

Method: RepoScope通过静态分析构建RSSG，结合结构和相似性上下文，提出调用链预测方法和结构保持的序列化算法。

Result: 在CoderEval和DevEval基准测试中，RepoScope的pass@1得分相对提升了36.35%。

Conclusion: RepoScope无需额外训练或多轮LLM查询，高效且通用，显著提升了仓库级代码生成的性能。

Abstract: Repository-level code generation aims to generate code within the context of
a specified repository. Existing approaches typically employ
retrieval-augmented generation (RAG) techniques to provide LLMs with relevant
contextual information extracted from the repository. However, these approaches
often struggle with effectively identifying truly relevant contexts that
capture the rich semantics of the repository, and their contextual perspectives
remains narrow. Moreover, most approaches fail to account for the structural
relationships in the retrieved code during prompt construction, hindering the
LLM's ability to accurately interpret the context. To address these issues, we
propose RepoScope, which leverages call chain-aware multi-view context for
repository-level code generation. RepoScope constructs a Repository Structural
Semantic Graph (RSSG) and retrieves a comprehensive four-view context,
integrating both structural and similarity-based contexts. We propose a novel
call chain prediction method that utilizes the repository's structural
semantics to improve the identification of callees in the target function.
Additionally, we present a structure-preserving serialization algorithm for
prompt construction, ensuring the coherence of the context for the LLM.
Notably, RepoScope relies solely on static analysis, eliminating the need for
additional training or multiple LLM queries, thus ensuring both efficiency and
generalizability. Evaluation on widely-used repository-level code generation
benchmarks (CoderEval and DevEval) demonstrates that RepoScope outperforms
state-of-the-art methods, achieving up to a 36.35% relative improvement in
pass@1 scores. Further experiments emphasize RepoScope's potential to improve
code generation across different tasks and its ability to integrate effectively
with existing approaches.

</details>


### [15] [Think Like an Engineer: A Neuro-Symbolic Collaboration Agent for Generative Software Requirements Elicitation and Self-Review](https://arxiv.org/abs/2507.14969)
*Sai Zhang,Zhenchang Xing,Jieshan Chen,Dehai Zhao,Zizhong Zhu,Xiaowang Zhang,Zhiyong Feng,Xiaohong Li*

Main category: cs.SE

TL;DR: RequireCEG是一个需求获取和自我审查代理，通过因果效应图（CEGs）和神经符号协作架构，解决用户需求模糊性问题，提升生成式软件开发的效果。


<details>
  <summary>Details</summary>
Motivation: 非专业用户的需求描述通常模糊，导致生成式软件开发困难。现有方法如Gherkin难以表达因果逻辑，因此需要更有效的需求获取和审查方法。

Method: RequireCEG结合特征树和因果效应图（CEGs），分层分析用户需求，构建自修复CEGs，并优化Gherkin场景以确保一致性。

Result: 实验表明，该方法在RGPair基准数据集上达到87%的覆盖率，多样性提升51.88%。

Conclusion: RequireCEG有效解决了用户需求模糊性问题，提升了生成式软件开发的准确性和多样性。

Abstract: The vision of End-User Software Engineering (EUSE) is to empower
non-professional users with full control over the software development
lifecycle. It aims to enable users to drive generative software development
using only natural language requirements. However, since end-users often lack
knowledge of software engineering, their requirement descriptions are
frequently ambiguous, raising significant challenges to generative software
development. Although existing approaches utilize structured languages like
Gherkin to clarify user narratives, they still struggle to express the causal
logic between preconditions and behavior actions. This paper introduces
RequireCEG, a requirement elicitation and self-review agent that embeds
causal-effect graphs (CEGs) in a neuro-symbolic collaboration architecture.
RequireCEG first uses a feature tree to analyze user narratives hierarchically,
clearly defining the scope of software components and their system behavior
requirements. Next, it constructs the self-healing CEGs based on the elicited
requirements, capturing the causal relationships between atomic preconditions
and behavioral actions. Finally, the constructed CEGs are used to review and
optimize Gherkin scenarios, ensuring consistency between the generated Gherkin
requirements and the system behavior requirements elicited from user
narratives. To evaluate our method, we created the RGPair benchmark dataset and
conducted extensive experiments. It achieves an 87% coverage rate and raises
diversity by 51.88%.

</details>


### [16] [The Rise of AI Teammates in Software Engineering (SE) 3.0: How Autonomous Coding Agents Are Reshaping Software Engineering](https://arxiv.org/abs/2507.15003)
*Hao Li,Haoxiang Zhang,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: 论文介绍了AIDev，首个大规模数据集，记录了AI编码代理在软件开发中的实际运作，为研究AI与人类协作提供了实证基础。


<details>
  <summary>Details</summary>
Motivation: 研究AI代理在软件开发中的实际表现，填补理论与实证之间的空白，推动AI原生软件工程的发展。

Method: 通过收集456,000个由五种主流AI代理（如GitHub Copilot）提交的拉取请求数据，构建AIDev数据集，分析其表现和协作模式。

Result: AI代理在提交速度上优于人类，但接受率较低，且代码结构更简单，揭示了信任与效用差距。

Conclusion: AIDev为研究AI原生工作流和人类-AI协作提供了重要资源，支持未来软件开发中AI代理的优化与应用。

Abstract: The future of software engineering--SE 3.0--is unfolding with the rise of AI
teammates: autonomous, goal-driven systems collaborating with human developers.
Among these, autonomous coding agents are especially transformative, now
actively initiating, reviewing, and evolving code at scale. This paper
introduces AIDev, the first large-scale dataset capturing how such agents
operate in the wild. Spanning over 456,000 pull requests by five leading
agents--OpenAI Codex, Devin, GitHub Copilot, Cursor, and Claude Code--across
61,000 repositories and 47,000 developers, AIDev provides an unprecedented
empirical foundation for studying autonomous teammates in software development.
  Unlike prior work that has largely theorized the rise of AI-native software
engineering, AIDev offers structured, open data to support research in
benchmarking, agent readiness, optimization, collaboration modeling, and AI
governance. The dataset includes rich metadata on PRs, authorship, review
timelines, code changes, and integration outcomes--enabling exploration beyond
synthetic benchmarks like SWE-bench. For instance, although agents often
outperform humans in speed, their PRs are accepted less frequently, revealing a
trust and utility gap. Furthermore, while agents accelerate code
submission--one developer submitted as many PRs in three days as they had in
three years--these are structurally simpler (via code complexity metrics).
  We envision AIDev as a living resource: extensible, analyzable, and ready for
the SE and AI communities. Grounding SE 3.0 in real-world evidence, AIDev
enables a new generation of research into AI-native workflows and supports
building the next wave of symbiotic human-AI collaboration. The dataset is
publicly available at https://github.com/SAILResearch/AI_Teammates_in_SE3.
  > AI Agent, Agentic AI, Coding Agent, Agentic Coding, Software Engineering
Agent

</details>


### [17] [Survey of GenAI for Automotive Software Development: From Requirements to Executable Code](https://arxiv.org/abs/2507.15025)
*Nenad Petrovic,Vahid Zolfaghari,Andre Schamschurko,Sven Kirchner,Fengjunjie Pan,Chengdng Wu,Nils Purschke,Aleksei Velsh,Krzysztof Lebioda,Yinglei Song,Yi Zhang,Lukasz Mazur,Alois Knoll*

Main category: cs.SE

TL;DR: 探讨了GenAI在汽车软件开发中的应用，重点关注需求处理、合规性和代码生成，并提出了一个通用的GenAI辅助开发流程。


<details>
  <summary>Details</summary>
Motivation: 汽车软件开发流程复杂且成本高，GenAI有望减少人工干预和成本。

Method: 综述了LLMs、RAG和VLMs等技术，并总结了代码生成中的提示技术。

Result: 提出了一个通用的GenAI辅助开发流程，并分享了行业合作伙伴的GenAI工具使用调查结果。

Conclusion: GenAI在汽车软件开发中具有潜力，但仍需进一步研究和实践验证。

Abstract: Adoption of state-of-art Generative Artificial Intelligence (GenAI) aims to
revolutionize many industrial areas by reducing the amount of human
intervention needed and effort for handling complex underlying processes.
Automotive software development is considered to be a significant area for
GenAI adoption, taking into account lengthy and expensive procedures, resulting
from the amount of requirements and strict standardization. In this paper, we
explore the adoption of GenAI for various steps of automotive software
development, mainly focusing on requirements handling, compliance aspects and
code generation. Three GenAI-related technologies are covered within the
state-of-art: Large Language Models (LLMs), Retrieval Augmented Generation
(RAG), Vision Language Models (VLMs), as well as overview of adopted prompting
techniques in case of code generation. Additionally, we also derive a
generalized GenAI-aided automotive software development workflow based on our
findings from this literature review. Finally, we include a summary of a survey
outcome, which was conducted among our automotive industry partners regarding
the type of GenAI tools used for their daily work activities.

</details>


### [18] [Can LLMs Generate User Stories and Assess Their Quality?](https://arxiv.org/abs/2507.15157)
*Giovanni Quattrocchi,Liliana Pasquale,Paola Spoletini,Luciano Baresi*

Main category: cs.SE

TL;DR: 论文探讨了如何利用LLMs在敏捷框架中自动化需求获取，生成用户故事（US），并评估其语义质量。结果表明，LLMs生成的US在覆盖范围和风格质量上与人类相似，但多样性和创造性较低。LLMs还能可靠评估US的语义质量。


<details>
  <summary>Details</summary>
Motivation: 需求获取是需求工程中最具挑战性的活动之一，且高质量需求的制定对软件质量至关重要。当前语义质量评估仍依赖人工，耗时费力。

Method: 使用10种先进的LLMs模拟客户访谈生成US，并比较其与人类生成的US的质量。同时探索LLMs自动评估US语义质量的能力。

Result: LLMs生成的US在覆盖范围和风格质量上与人类相似，但多样性和创造性较低。LLMs能可靠评估US的语义质量，减少人工工作量。

Conclusion: LLMs在需求获取和语义质量评估中具有潜力，但需进一步优化以提高多样性和创造性。

Abstract: Requirements elicitation is still one of the most challenging activities of
the requirements engineering process due to the difficulty requirements
analysts face in understanding and translating complex needs into concrete
requirements. In addition, specifying high-quality requirements is crucial, as
it can directly impact the quality of the software to be developed. Although
automated tools allow for assessing the syntactic quality of requirements,
evaluating semantic metrics (e.g., language clarity, internal consistency)
remains a manual and time-consuming activity. This paper explores how LLMs can
help automate requirements elicitation within agile frameworks, where
requirements are defined as user stories (US). We used 10 state-of-the-art LLMs
to investigate their ability to generate US automatically by emulating customer
interviews. We evaluated the quality of US generated by LLMs, comparing it with
the quality of US generated by humans (domain experts and students). We also
explored whether and how LLMs can be used to automatically evaluate the
semantic quality of US. Our results indicate that LLMs can generate US similar
to humans in terms of coverage and stylistic quality, but exhibit lower
diversity and creativity. Although LLM-generated US are generally comparable in
quality to those created by humans, they tend to meet the acceptance quality
criteria less frequently, regardless of the scale of the LLM model. Finally,
LLMs can reliably assess the semantic quality of US when provided with clear
evaluation criteria and have the potential to reduce human effort in
large-scale assessments.

</details>


### [19] [Deep Learning Framework Testing via Heuristic Guidance Based on Multiple Model Measurements](https://arxiv.org/abs/2507.15181)
*Yinglong Zou,Juan Zhai,Chunrong Fang,Yanzhou Mu,Jiawei Liu,Zhenyu Chen*

Main category: cs.SE

TL;DR: DLMMM是一种深度学习框架测试方法，通过融合多种模型测量指标来优化测试效果。


<details>
  <summary>Details</summary>
Motivation: 现有测试方法在检测框架缺陷时存在三个关键局限性：未能定量测量算子组合多样性、忽略模型执行时间、未考虑不同测量指标间的相关性。

Method: DLMMM定量测量模型的缺陷检测性能、算子组合多样性和执行时间，并基于相关性融合这些指标以实现权衡。此外，设计了多级启发式指导生成测试输入模型。

Result: DLMMM通过多指标融合和多级启发式指导，提高了深度学习框架的测试效果。

Conclusion: DLMMM克服了现有方法的局限性，为深度学习框架测试提供了更全面的解决方案。

Abstract: Deep learning frameworks serve as the foundation for developing and deploying
deep learning applications. To enhance the quality of deep learning frameworks,
researchers have proposed numerous testing methods using deep learning models
as test inputs. However, existing methods predominantly measure model bug
detection effectiveness as heuristic indicators, presenting three critical
limitations: Firstly, existing methods fail to quantitatively measure model's
operator combination variety, potentially missing critical operator
combinations that could trigger framework bugs. Secondly, existing methods
neglect measuring model execution time, resulting in the omission of numerous
models potential for detecting more framework bugs within limited testing time.
Thirdly, existing methods overlook correlation between different model
measurements, relying simply on single-indicator heuristic guidance without
considering their trade-offs. To overcome these limitations, we propose DLMMM,
the first deep learning framework testing method to include multiple model
measurements into heuristic guidance and fuse these measurements to achieve
their trade-off. DLMMM firstly quantitatively measures model's bug detection
performance, operator combination variety, and model execution time. After
that, DLMMM fuses the above measurements based on their correlation to achieve
their trade-off. To further enhance testing effectiveness, DLMMM designs
multi-level heuristic guidance for test input model generation.

</details>


### [20] [Cultural Impact on Requirements Engineering Activities: Bangladeshi Practitioners' View](https://arxiv.org/abs/2507.15188)
*Chowdhury Shahriar Muzammel,Maria Spichkova,James Harland*

Main category: cs.SE

TL;DR: 研究探讨了孟加拉国文化对需求工程（RE）活动的影响，旨在避免误解并促进IT行业的多样性。


<details>
  <summary>Details</summary>
Motivation: 需求工程是软件开发中互动密集的阶段，文化差异可能导致误解和冲突。孟加拉国IT行业增长迅速，但文化影响尚未充分研究。

Method: 研究聚焦孟加拉国文化背景下的RE过程，分析文化因素对RE活动的影响。

Result: 研究发现文化因素显著影响RE活动的实施和效果。

Conclusion: 了解文化影响有助于优化RE过程，促进IT行业的多样性和包容性。

Abstract: Requirements Engineering (RE) is one of the most interaction-intensive phases
of software development. This means that RE activities might be especially
impacted by stakeholders' national culture. Software development projects
increasingly have a very diverse range of stakeholders. To future-proof RE
activities, we need to help RE practitioners avoid misunderstandings and
conflicts that might arise from not understanding potential Cultural Influences
(CIs). Moreover, an awareness of CIs supports diversity and inclusion in the IT
profession. Bangladesh has a growing IT sector with some unique socio-cultural
characteristics, and has been largely overlooked in this research field. In
this study, we aim to investigate how the RE process is adopted in the context
of Bangladeshi culture and what cultural influences impact overall RE
activities.

</details>


### [21] [Towards Using Personas in Requirements Engineering: What Has Been Changed Recently?](https://arxiv.org/abs/2507.15197)
*Chowdhury Shahriar Muzammel,Maria Spichkova,James Harland*

Main category: cs.SE

TL;DR: 系统映射研究（SMS）探讨了2023年至2025年间需求工程（RE）中人物角色的应用趋势，发现AI解决方案和模板化人物角色日益流行。


<details>
  <summary>Details</summary>
Motivation: 研究旨在了解人物角色在需求工程中的最新应用趋势，特别是生成式AI的影响。

Method: 通过系统映射研究（SMS）分析22篇相关文献，关注人物角色的表示、构建、验证及其在RE活动中的应用。

Result: 研究发现AI在人物角色构建和验证中的应用增多，模板化人物角色更受欢迎，验证相关研究比例上升。

Conclusion: 人物角色在RE中的应用趋势显示AI和模板化方法的普及，验证研究的重要性增加。

Abstract: In requirements engineering (RE), personas are now being used to represent
user expectations and needs. This systematic mapping study (SMS) aims to
explore the most recent studies and to cover recent changes in trends,
especially related to the recent evolution of Generative AI approaches. Our SMS
covers the period between April 2023 and April 2025. We identified 22 relevant
publications and analysed persona representation, construction, validation, as
well as RE activities covered by personas. We identified that a number of
studies applied AI-based solutions for persona construction and validation. We
observed that template-based personas are becoming more popular nowadays. We
also observed an increase in the proportion of studies covering validation
aspects.

</details>


### [22] [SimdBench: Benchmarking Large Language Models for SIMD-Intrinsic Code Generation](https://arxiv.org/abs/2507.15224)
*Yibo He,Shuoran Zhao,Jiaming Huang,Yingjie Fu,Hao Yu,Cunjian Huang,Tao Xie*

Main category: cs.SE

TL;DR: SimdBench是首个专门为SIMD-intrinsic代码生成设计的基准测试，包含136个任务，评估了18个LLM在生成向量化代码时的表现。


<details>
  <summary>Details</summary>
Motivation: SIMD指令在性能关键任务中广泛应用，但现有代码生成基准仅关注标量代码，LLM在SIMD-intrinsic代码生成中的表现尚不明确。

Method: 提出SimdBench基准，针对五种代表性SIMD指令集（SSE、AVX、Neon、SVE、RVV），评估18个LLM的代码生成能力。

Result: LLM在SIMD-intrinsic代码生成中的pass@k普遍低于标量代码生成，揭示了改进方向。

Conclusion: SimdBench为LLM在SIMD-intrinsic代码生成领域的研究提供了工具和洞察。

Abstract: SIMD (Single Instruction Multiple Data) instructions and their compiler
intrinsics are widely supported by modern processors to accelerate
performance-critical tasks. SIMD intrinsic programming, a trade-off between
coding productivity and high performance, is widely used in the development of
mainstream performance-critical libraries and daily computing tasks. Large
Language Models (LLMs), which have demonstrated strong and comprehensive
capabilities in code generation, show promise in assisting programmers with the
challenges of SIMD intrinsic programming. However, existing code-generation
benchmarks focus on only scalar code, and it is unclear how LLMs perform in
generating vectorized code using SIMD intrinsics. To fill this gap, we propose
SimdBench, the first code benchmark specifically designed for SIMD-intrinsic
code generation, comprising 136 carefully crafted tasks and targeting five
representative SIMD intrinsics: SSE (x86 Streaming SIMD Extension), AVX (x86
Advanced Vector Extension), Neon (ARM Advanced SIMD Extension), SVE (ARM
Scalable Vector Extension), and RVV (RISC-V Vector Extension). We conduct a
systematic evaluation (measuring both correctness and performance) of 18
representative LLMs on SimdBench, resulting in a series of novel and insightful
findings. Our evaluation results demonstrate that LLMs exhibit a universal
decrease in pass@k during SIMD-intrinsic code generation compared to
scalar-code generation. Our in-depth analysis highlights promising directions
for the further advancement of LLMs in the challenging domain of SIMD-intrinsic
code generation. SimdBench is fully open source at
https://anonymous.4open.science/r/SimdBench-1B3F/ to benefit the broader
research community.

</details>


### [23] [Code Clone Detection via an AlphaFold-Inspired Framework](https://arxiv.org/abs/2507.15226)
*Changguo Jia,Yi Zhan,Tianqi Zhao,Hengzhi Ye,Minghui Zhou*

Main category: cs.SE

TL;DR: AlphaCC利用AlphaFold的序列到结构建模能力，通过多语言适用的令牌序列表示代码片段，实现跨语言的代码克隆检测，并在语义理解和效率上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有代码克隆检测方法在捕捉代码语义或跨语言适用性上存在不足，AlphaFold在蛋白质序列到结构预测中的成功启发了该方法。

Method: AlphaCC将代码片段转换为令牌序列，构建多序列对齐增强上下文理解，采用改进的注意力编码器建模依赖关系，并通过相似度评分和二元分类检测克隆对。

Result: 在三种语言多样的数据集上，AlphaCC在语义克隆检测中优于所有基线，同时保持高效性。

Conclusion: AlphaCC展示了跨语言适用性和强大的语义理解能力，适用于大规模克隆检测任务。

Abstract: Code clone detection, which aims to identify functionally equivalent code
fragments, plays a critical role in software maintenance and vulnerability
analysis. Substantial methods have been proposed to detect code clones, but
they fall short in capturing code semantics or relying on language-specific
analyzers. Inspired by the remarkable success of AlphaFold in predicting
three-dimensional protein structures from protein sequences, in this paper, we
leverage AlphaFold for code clone detection based on the insight that protein
sequences and token sequences share a common linear sequential structure. In
particular, we propose AlphaCC, which represents code fragments as token
sequences to ensure multi-language applicability and adapts AlphaFold's
sequence-to-structure modeling capability to infer code semantics. The pipeline
of AlphaCC goes through three steps. First, AlphaCC transforms each input code
fragment into a token sequence and, motivated by AlphaFold's use of multiple
sequence alignment (MSA) to enhance contextual understanding, constructs an MSA
from lexically similar token sequences. Second, AlphaCC adopts a modified
attention-based encoder based on AlphaFold to model dependencies within and
across token sequences. Finally, unlike AlphaFold's protein structure
prediction task, AlphaCC computes similarity scores between token sequences
through a late interaction strategy and performs binary classification to
determine code clone pairs. Comprehensive evaluations on three language-diverse
datasets demonstrate AlphaCC's applicability across multiple programming
languages. On two semantic clone detection datasets, it consistently
outperforms all baselines, showing strong semantic understanding. Moreover,
AlphaCC maintains competitive efficiency, enabling practical usage in
large-scale clone detection tasks.

</details>


### [24] [FaultLine: Automated Proof-of-Vulnerability Generation Using LLM Agents](https://arxiv.org/abs/2507.15241)
*Vikram Nitin,Baishakhi Ray,Roshanak Zilouchian Moghaddam*

Main category: cs.SE

TL;DR: FaultLine是一种基于LLM代理的工作流，通过分层推理自动生成漏洞验证测试（PoV），在多语言数据集上表现优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 软件安全漏洞报告常缺乏验证漏洞的测试（PoV），导致修复效果无法验证且易出现回归问题。生成PoV测试具有挑战性，需深入分析程序控制流和数据流。

Method: FaultLine通过三步推理生成PoV测试：1) 追踪输入从API到漏洞点的路径；2) 分析路径中的分支条件；3) 在反馈循环中生成测试用例。该方法不依赖语言特定的分析工具。

Result: 在多语言的100个漏洞数据集中，FaultLine成功为16个项目生成PoV测试，优于CodeAct 2.1的9个，相对性能提升77%。

Conclusion: 分层推理可提升LLM代理在PoV测试生成中的表现，但问题仍具挑战性。代码和数据集已公开以促进进一步研究。

Abstract: Despite the critical threat posed by software security vulnerabilities,
reports are often incomplete, lacking the proof-of-vulnerability (PoV) tests
needed to validate fixes and prevent regressions. These tests are crucial not
only for ensuring patches work, but also for helping developers understand how
vulnerabilities can be exploited. Generating PoV tests is a challenging
problem, requiring reasoning about the flow of control and data through deeply
nested levels of a program.
  We present FaultLine, an LLM agent workflow that uses a set of carefully
designed reasoning steps, inspired by aspects of traditional static and dynamic
program analysis, to automatically generate PoV test cases. Given a software
project with an accompanying vulnerability report, FaultLine 1) traces the flow
of an input from an externally accessible API ("source") to the "sink"
corresponding to the vulnerability, 2) reasons about the conditions that an
input must satisfy in order to traverse the branch conditions encountered along
the flow, and 3) uses this reasoning to generate a PoV test case in a
feedback-driven loop. FaultLine does not use language-specific static or
dynamic analysis components, which enables it to be used across programming
languages.
  To evaluate FaultLine, we collate a challenging multi-lingual dataset of 100
known vulnerabilities in Java, C and C++ projects. On this dataset, FaultLine
is able to generate PoV tests for 16 projects, compared to just 9 for CodeAct
2.1, a popular state-of-the-art open-source agentic framework. Thus, FaultLine
represents a 77% relative improvement over the state of the art. Our findings
suggest that hierarchical reasoning can enhance the performance of LLM agents
on PoV test generation, but the problem in general remains challenging. We make
our code and dataset publicly available in the hope that it will spur further
research in this area.

</details>


### [25] [Input Reduction Enhanced LLM-based Program Repair](https://arxiv.org/abs/2507.15251)
*Boyang Yang,Luyao Ren,Xin Yin,Jiadong Ren,Haoye Tian,Shunfu Jin*

Main category: cs.SE

TL;DR: ReduceFix 是一种基于 LLM 的自动程序修复方法，通过自动缩减测试输入来解决长提示中的信息丢失问题，显著提升修复性能。


<details>
  <summary>Details</summary>
Motivation: LLM 在长提示中容易丢失关键信息，导致修复性能下降。

Method: 提出 ReduceFix，自动缩减测试输入并保留其失败诱导行为，用于指导补丁生成。

Result: 在 LFTBench 上，ReduceFix 平均缩减输入 89.1%，修复成功率提升最高达 53.8%。

Conclusion: 自动缩减失败输入是 LLM 程序修复的有效补充，显著提升其可扩展性和效果。

Abstract: Large Language Models (LLMs) have shown great potential in Automated Program
Repair (APR). Test inputs, being crucial for reasoning the root cause of
failures, are always included in the prompt for LLM-based APR. Unfortunately,
LLMs struggle to retain key information in long prompts. When the test inputs
are extensive in the prompt, this may trigger the "lost-in-the-middle" issue,
compromising repair performance. To address this, we propose ReduceFix, an
LLM-based APR approach with a built-in component that automatically reduces
test inputs while retaining their failure-inducing behavior. ReduceFix prompts
an LLM to generate a reducer that minimizes failure-inducing test inputs
without human effort, and then feeds the reduced failure-inducing inputs to
guide patch generation.
  For targeted evaluation, we constructed LFTBench, the first long-input APR
benchmark with 200 real bugs from 20 programming tasks, each paired with a
failure-inducing input whose median size is 1 MB. On this benchmark, ReduceFix
shrinks inputs by 89.1% on average and improves overall pass@10 by up to 53.8%
relative to a prompt that includes the original test, and by 17.6% compared
with omitting the test entirely. Adding the same reduction step to ChatRepair
increases its fix rate by 21.3% without other changes. Ablation studies further
highlight the impact of input length and compressed failure information on
repair success. These results underscore that automatically reducing failing
inputs is a practical and powerful complement to LLM-based APR, significantly
improving its scalability and effectiveness.

</details>


### [26] [Butterfly Effects in Toolchains: A Comprehensive Analysis of Failed Parameter Filling in LLM Tool-Agent Systems](https://arxiv.org/abs/2507.15296)
*Qian Xiong,Yuekai Huang,Ziyou Jiang,Zhiyuan Chang,Yujia Zheng,Tianhao Li,Mingyang Li*

Main category: cs.SE

TL;DR: 该论文研究了工具代理范式中参数失败的问题，提出了分类和改进建议。


<details>
  <summary>Details</summary>
Motivation: 探索工具代理范式中参数失败的现象，并提出改进建议以提高其可靠性和有效性。

Method: 构建参数失败分类法，通过输入扰动方法分析输入源与失败类别的相关性。

Result: 实验表明参数名称幻觉失败主要源于LLM固有局限，其他失败模式与输入源问题相关。

Conclusion: 建议标准化工具返回格式、改进错误反馈机制和确保参数一致性以优化工具代理交互。

Abstract: The emergence of the tool agent paradigm has broadened the capability
boundaries of the Large Language Model (LLM), enabling it to complete more
complex tasks. However, the effectiveness of this paradigm is limited due to
the issue of parameter failure during its execution. To explore this phenomenon
and propose corresponding suggestions, we first construct a parameter failure
taxonomy in this paper. We derive five failure categories from the invocation
chain of a mainstream tool agent. Then, we explore the correlation between
three different input sources and failure categories by applying 15 input
perturbation methods to the input. Experimental results show that parameter
name hallucination failure primarily stems from inherent LLM limitations, while
issues with input sources mainly cause other failure patterns. To improve the
reliability and effectiveness of tool-agent interactions, we propose
corresponding improvement suggestions, including standardizing tool return
formats, improving error feedback mechanisms, and ensuring parameter
consistency.

</details>


### [27] [StackTrans: From Large Language Model to Large Pushdown Automata Model](https://arxiv.org/abs/2507.15343)
*Kechi Zhang,Ge Li,Jia Li,Huangzhao Zhang,Yihong Dong,Jia Li,Jingjing Xu,Zhi Jin*

Main category: cs.SE

TL;DR: StackTrans通过引入隐藏状态堆栈改进了Transformer架构，解决了其无法有效捕捉Chomsky层级的问题，并在多项任务中表现优于标准Transformer和其他基线模型。


<details>
  <summary>Details</summary>
Motivation: Transformer架构虽然强大，但无法有效处理Chomsky层级（如正则表达式或确定性上下文无关文法），因此需要改进。

Method: 提出StackTrans，在Transformer层间引入可微分的隐藏状态堆栈操作（如压入和弹出），保持与现有框架的兼容性。

Result: StackTrans在Chomsky层级和大规模自然语言任务中表现优异，甚至小规模模型（360M参数）也能超越更大规模的开放LLM。

Conclusion: StackTrans通过堆栈机制提升了Transformer的效率和推理能力，为处理复杂语言结构提供了新思路。

Abstract: The Transformer architecture has emerged as a landmark advancement within the
broad field of artificial intelligence, effectively catalyzing the advent of
large language models (LLMs). However, despite its remarkable capabilities and
the substantial progress it has facilitated, the Transformer architecture still
has some limitations. One such intrinsic limitation is its inability to
effectively capture the Chomsky hierarchy, such as regular expressions or
deterministic context-free grammars. Drawing inspiration from pushdown
automata, which efficiently resolve deterministic context-free grammars using
stacks, we propose StackTrans to address the aforementioned issue within LLMs.
Unlike previous approaches that modify the attention computation, StackTrans
explicitly incorporates hidden state stacks between Transformer layers. This
design maintains compatibility with existing frameworks like flash-attention.
Specifically, our design features stack operations -- such as pushing and
popping hidden states -- that are differentiable and can be learned in an
end-to-end manner. Our comprehensive evaluation spans benchmarks for both
Chomsky hierarchies and large-scale natural languages. Across these diverse
tasks, StackTrans consistently outperforms standard Transformer models and
other baselines. We have successfully scaled StackTrans up from 360M to 7B
parameters. In particular, our from-scratch pretrained model StackTrans-360M
outperforms several larger open-source LLMs with 2-3x more parameters,
showcasing its superior efficiency and reasoning capability.

</details>


### [28] [Applying the Chinese Wall Reverse Engineering Technique to Large Language Model Code Editing](https://arxiv.org/abs/2507.15599)
*Manatsawin Hanmongkolchai*

Main category: cs.SE

TL;DR: 论文探讨了代码大语言模型（Code LLM）的版权问题，提出了一种“中国墙”技术，通过强模型指导弱模型提升性能，但实际应用受限。


<details>
  <summary>Details</summary>
Motivation: 解决代码大语言模型训练数据版权问题，同时提升弱模型的实用性。

Method: 应用“中国墙”技术，利用高质量模型生成详细指令指导弱模型完成任务。

Result: Comma v0.1 1T性能提升66%，Starcoder2 Instruct提升20%。

Conclusion: 该技术有效但受限于缺乏无版权问题的公开训练数据。

Abstract: Large language models for code (Code LLM) are increasingly utilized in
programming environments. Despite their utility, the training datasets for top
LLM remain undisclosed, raising concerns about potential copyright violations.
Some models, such as Pleias and Comma put emphasis on data curation and
licenses, however, with limited training data these models are not competitive
and only serve as proof of concepts. To improve the utility of these models, we
propose an application of the "Chinese Wall" technique, inspired by the reverse
engineering technique of the same name -- a high quality model is used to
generate detailed instructions for a weaker model. By doing so, a weaker but
ethically aligned model may be used to perform complicated tasks that,
otherwise, can only be completed by more powerful models. In our evaluation,
we've found that this technique improves Comma v0.1 1T's performance in
CanItEdit benchmark by over 66%, and Starcoder2 Instruct by roughly 20%
compared to when running the same model on the benchmark alone. The practical
application of this technique today, however, may be limited due to the lack of
models trained on public domain content without copyright restrictions.

</details>


### [29] [Hot Topics and Common Challenges: an Empirical Study of React Discussions on Stack Overflow](https://arxiv.org/abs/2507.15624)
*Yusuf Sulistyo Nugroho,Ganno Tribuana Kurniaji,Syful Islam,Mohammed Humayun Kabir,Vanesya Aura Ardity,Md. Kamal Uddin*

Main category: cs.SE

TL;DR: 分析Stack Overflow上React相关问题的研究，发现高频关键词、错误分类及用户声誉与错误的关系，提出社区需提供算法问题指导。


<details>
  <summary>Details</summary>
Motivation: 尽管React在单页应用开发中受欢迎，但用户面临的具体挑战尚不明确，因此研究旨在通过Stack Overflow数据分析填补这一空白。

Method: 采用探索性数据分析方法，研究React问题中的高频关键词、错误分类及用户声誉与错误的关系。

Result: 高频关键词包括code、link等；算法错误最常见，中声誉用户贡献最多（55.77%）。

Conclusion: 社区需提供算法问题指导，研究结果为React社区早期实施阶段提供支持，帮助克服挑战。

Abstract: React is a JavaScript library used to build user interfaces for single-page
applications. Although recent studies have shown the popularity and advantages
of React in web development, the specific challenges users face remain unknown.
Thus, this study aims to analyse the React-related questions shared on Stack
Overflow. The study utilizes an exploratory data analysis to investigate the
most frequently discussed keywords, error classification, and user
reputation-based errors, which is the novelty of this work. The results show
the top eight most frequently used keywords on React-related questions, namely,
code, link, vir, href, connect, azure, windows, and website. The error
classification of questions from the sample shows that algorithmic error is the
most frequent issue faced by all groups of users, where mid-reputation users
contribute the most, accounting for 55.77%. This suggests the need for the
community to provide guidance materials in solving algorithm-related problems.
We expect that the results of this study will provide valuable insight into
future research to support the React community during the early stages of
implementation, facilitating their ability to effectively overcome challenges
to adoption.

</details>


### [30] [SustainDiffusion: Optimising the Social and Environmental Sustainability of Stable Diffusion Models](https://arxiv.org/abs/2507.15663)
*Giordano d'Aloisio,Tosin Fadahunsi,Jay Choy,Rebecca Moussa,Federica Sarro*

Main category: cs.SE

TL;DR: SustainDiffusion是一种基于搜索的方法，旨在减少Stable Diffusion模型的社会和环境危害，同时保持图像质量。


<details>
  <summary>Details</summary>
Motivation: 解决Stable Diffusion模型在社会和环境可持续性方面的负面影响，如性别和种族偏见以及高能耗。

Method: 通过搜索超参数和提示结构的最优组合，减少偏见并降低能耗，同时保持图像质量。

Result: SustainDiffusion显著减少了性别偏见（68%）、种族偏见（59%）和能耗（48%），且结果稳定且可推广。

Conclusion: SustainDiffusion证明了在不调整模型架构的情况下，提升文本到图像生成模型的社会和环境可持续性是可行的。

Abstract: Background: Text-to-image generation models are widely used across numerous
domains. Among these models, Stable Diffusion (SD) - an open-source
text-to-image generation model - has become the most popular, producing over 12
billion images annually. However, the widespread use of these models raises
concerns regarding their social and environmental sustainability.
  Aims: To reduce the harm that SD models may have on society and the
environment, we introduce SustainDiffusion, a search-based approach designed to
enhance the social and environmental sustainability of SD models.
  Method: SustainDiffusion searches the optimal combination of hyperparameters
and prompt structures that can reduce gender and ethnic bias in generated
images while also lowering the energy consumption required for image
generation. Importantly, SustainDiffusion maintains image quality comparable to
that of the original SD model.
  Results: We conduct a comprehensive empirical evaluation of SustainDiffusion,
testing it against six different baselines using 56 different prompts. Our
results demonstrate that SustainDiffusion can reduce gender bias in SD3 by 68%,
ethnic bias by 59%, and energy consumption (calculated as the sum of CPU and
GPU energy) by 48%. Additionally, the outcomes produced by SustainDiffusion are
consistent across multiple runs and can be generalised to various prompts.
  Conclusions: With SustainDiffusion, we demonstrate how enhancing the social
and environmental sustainability of text-to-image generation models is possible
without fine-tuning or changing the model's architecture.

</details>


### [31] [Modeling CubeSat Storage Battery Discharge: Equivalent Circuit Versus Machine Learning Approaches](https://arxiv.org/abs/2507.15666)
*Igor Turkin,Lina Volobuieva,Andriy Chukhray,Oleksandr Liubimov*

Main category: cs.SE

TL;DR: 文章比较了两种建模CubeSat卫星电池放电的方法：等效电路分析和机器学习，旨在选择更优方法以提高卫星电源系统的预测和容错能力。


<details>
  <summary>Details</summary>
Motivation: 研究动机是为CubeSat卫星电池放电建模，以预测电源系统断开的后果并确保轨道设备的容错性。

Method: 方法包括基于物理定律的等效电路分析和基于实际数据的机器学习建模。

Result: 等效电路模型透明但灵活性差；机器学习模型更准确且适应性强。

Conclusion: 机器学习方法在复杂条件下表现更优，适合实际应用。

Abstract: The subject of the article is the study and comparison of two approaches to
modelling the battery discharge of a CubeSat satellite: analytical using
equivalent circuit and machine learning. The article aims to make a reasoned
choice of the approach to modelling the battery discharge of a CubeSat
satellite. Modelling the battery discharge of a satellite will enable the
prediction of the consequences of disconnecting the autonomous power system and
ensure the fault tolerance of equipment in orbit. Therefore, the selected study
is relevant and promising. This study focuses on the analysis of CubeSat
satellite data, based explicitly on orbital data samples of the power system,
which include data available at the time of the article publication. The
dataset contains data on the voltage, current, and temperature of the battery
and solar panels attached to the five sides of the satellite. In this context,
two approaches are considered: analytical modelling based on physical laws and
machine learning, which uses empirical data to create a predictive model.
Results: A comparative analysis of the modeling results reveals that the
equivalent circuit approach has the advantage of transparency, as it identifies
possible parameters that facilitate understanding of the relationships.
However, the model is less flexible to environmental changes or non-standard
satellite behavior. The machine learning model demonstrated more accurate
results, as it can account for complex dependencies and adapt to actual
conditions, even when they deviate from theoretical assumptions.

</details>


### [32] [BugScope: Learn to Find Bugs Like Human](https://arxiv.org/abs/2507.15671)
*Jinyao Guo,Chengpeng Wang,Dominic Deluca,Jinjie Liu,Zhuo Zhang,Xiangyu Zhang*

Main category: cs.SE

TL;DR: BugScope是一个基于LLM的多智能体系统，通过模拟人类审计员学习新错误模式的方式，显著提升了软件错误检测的精度和召回率。


<details>
  <summary>Details</summary>
Motivation: 传统静态分析工具在覆盖范围和适应性上受限，而现有的LLM方法对复杂错误的处理能力不足。BugScope旨在解决这些问题。

Method: BugScope通过程序切片提取相关检测上下文，并构建定制化的检测提示，指导LLM进行准确推理。

Result: 在40个真实错误的数据集上，BugScope达到87.04%的精度和90.00%的召回率，F1分数优于现有工具。在Linux内核等大规模系统中发现了141个未知错误。

Conclusion: BugScope在错误检测方面表现出色，具有显著的实用价值。

Abstract: Detecting software bugs remains a fundamental challenge due to the extensive
diversity of real-world defects. Traditional static analysis tools often rely
on symbolic workflows, which restrict their coverage and hinder adaptability to
customized bugs with diverse anti-patterns. While recent advances incorporate
large language models (LLMs) to enhance bug detection, these methods continue
to struggle with sophisticated bugs and typically operate within limited
analysis contexts. To address these challenges, we propose BugScope, an
LLM-driven multi-agent system that emulates how human auditors learn new bug
patterns from representative examples and apply that knowledge during code
auditing. Given a set of examples illustrating both buggy and non-buggy
behaviors, BugScope synthesizes a retrieval strategy to extract relevant
detection contexts via program slicing and then constructs a tailored detection
prompt to guide accurate reasoning by the LLM. Our evaluation on a curated
dataset of 40 real-world bugs drawn from 21 widely-used open-source projects
demonstrates that BugScope achieves 87.04% precision and 90.00% recall,
surpassing state-of-the-art industrial tools by 0.44 in F1 score. Further
testing on large-scale open-source systems, including the Linux kernel,
uncovered 141 previously unknown bugs, of which 78 have been fixed and 7
confirmed by developers, highlighting BugScope's substantial practical impact.

</details>


### [33] [Do AI models help produce verified bug fixes?](https://arxiv.org/abs/2507.15822)
*Li Huang,Ilgiz Mustafin,Marco Piccioni,Alessandro Schena,Reto Weber,Bertrand Meyer*

Main category: cs.SE

TL;DR: 研究探讨了大型语言模型（LLM）在自动程序修复（APR）中的实际效果，通过实验比较了程序员使用和不使用LLM的调试表现，并提出了相关方法论和使用模式。


<details>
  <summary>Details</summary>
Motivation: 探索LLM在APR中的实际应用效果，验证其是否能提供正确修复，并研究程序员如何利用LLM辅助调试。

Method: 采用随机分组实验，一组程序员使用LLM，另一组不使用，通过程序证明工具验证修复的正确性，并结合目标-查询-度量方法进行分析。

Result: 实验结果与预期不同，揭示了LLM在调试中的实际效果，并提出了7种LLM使用模式及优化建议。

Conclusion: 研究为AI和LLM在程序修复中的合理应用提供了初步依据，并提出了可复用的实验方法和实用建议。

Abstract: Among areas of software engineering where AI techniques -- particularly,
Large Language Models -- seem poised to yield dramatic improvements, an
attractive candidate is Automatic Program Repair (APR), the production of
satisfactory corrections to software bugs. Does this expectation materialize in
practice? How do we find out, making sure that proposed corrections actually
work? If programmers have access to LLMs, how do they actually use them to
complement their own skills?
  To answer these questions, we took advantage of the availability of a
program-proving environment, which formally determines the correctness of
proposed fixes, to conduct a study of program debugging with two randomly
assigned groups of programmers, one with access to LLMs and the other without,
both validating their answers through the proof tools. The methodology relied
on a division into general research questions (Goals in the Goal-Query-Metric
approach), specific elements admitting specific answers (Queries), and
measurements supporting these answers (Metrics). While applied so far to a
limited sample size, the results are a first step towards delineating a proper
role for AI and LLMs in providing guaranteed-correct fixes to program bugs.
  These results caused surprise as compared to what one might expect from the
use of AI for debugging and APR. The contributions also include: a detailed
methodology for experiments in the use of LLMs for debugging, which other
projects can reuse; a fine-grain analysis of programmer behavior, made possible
by the use of full-session recording; a definition of patterns of use of LLMs,
with 7 distinct categories; and validated advice for getting the best of LLMs
for debugging and Automatic Program Repair.

</details>


### [34] [Investigating the Use of LLMs for Evidence Briefings Generation in Software Engineering](https://arxiv.org/abs/2507.15828)
*Mauro Marcelino,Marcos Alves,Bianca Trinkenreich,Bruno Cartaxo,Sérgio Soares,Simone D. J. Barbosa,Marcos Kalinowski*

Main category: cs.SE

TL;DR: 该论文提出了一种基于RAG的LLM工具，用于自动生成证据简报，并通过实验比较其与人工简报在内容保真度、易理解性和实用性上的表现。


<details>
  <summary>Details</summary>
Motivation: 证据简报对软件工程师有用，但人工制作成本高，阻碍了其广泛应用。因此，研究自动生成简报的方法具有重要意义。

Method: 开发了基于RAG的LLM工具，生成证据简报，并通过对照实验评估其与人工简报的差异。

Result: 实验结果待报告。

Conclusion: 结论将根据实验结果得出。

Abstract: [Context] An evidence briefing is a concise and objective transfer medium
that can present the main findings of a study to software engineers in the
industry. Although practitioners and researchers have deemed Evidence Briefings
useful, their production requires manual labor, which may be a significant
challenge to their broad adoption. [Goal] The goal of this registered report is
to describe an experimental protocol for evaluating LLM-generated evidence
briefings for secondary studies in terms of content fidelity, ease of
understanding, and usefulness, as perceived by researchers and practitioners,
compared to human-made briefings. [Method] We developed an RAG-based LLM tool
to generate evidence briefings. We used the tool to automatically generate two
evidence briefings that had been manually generated in previous research
efforts. We designed a controlled experiment to evaluate how the LLM-generated
briefings compare to the human-made ones regarding perceived content fidelity,
ease of understanding, and usefulness. [Results] To be reported after the
experimental trials. [Conclusion] Depending on the experiment results.

</details>


### [35] [Observing Fine-Grained Changes in Jupyter Notebooks During Development Time](https://arxiv.org/abs/2507.15831)
*Sergey Titov,Konstantin Grotov,Cristina Sarasua,Yaroslav Golubev,Dhivyabharathi Ramasamy,Alberto Bacchelli,Abraham Bernstein,Timofey Bryksin*

Main category: cs.SE

TL;DR: 该论文填补了数据科学领域中计算笔记本动态开发过程的研究空白，通过工具集收集开发时的代码变更，分析笔记本的使用模式和变化类型。


<details>
  <summary>Details</summary>
Motivation: 现有研究集中在细粒度日志分析，但缺乏对数据科学中计算笔记本开发过程的研究。

Method: 引入工具集收集Jupyter笔记本开发时的代码变更，收集20名开发者100多小时的工作数据，分析笔记本的动态开发过程。

Result: 发现笔记本主要用于小规模修复和代码迭代，兼具开发和调试功能。

Conclusion: 提出了未来研究方向，强调了笔记本在数据科学中的多功能性。

Abstract: In software engineering, numerous studies have focused on the analysis of
fine-grained logs, leading to significant innovations in areas such as
refactoring, security, and code completion. However, no similar studies have
been conducted for computational notebooks in the context of data science.
  To help bridge this research gap, we make three scientific contributions: we
(1) introduce a toolset for collecting code changes in Jupyter notebooks during
development time; (2) use it to collect more than 100 hours of work related to
a data analysis task and a machine learning task (carried out by 20 developers
with different levels of expertise), resulting in a dataset containing 2,655
cells and 9,207 cell executions; and (3) use this dataset to investigate the
dynamic nature of the notebook development process and the changes that take
place in the notebooks.
  In our analysis of the collected data, we classified the changes made to the
cells between executions and found that a significant number of these changes
were relatively small fixes and code iteration modifications. This suggests
that notebooks are used not only as a development and exploration tool but also
as a debugging tool. We report a number of other insights and propose potential
future research directions on the novel data.

</details>


### [36] [Generating executable oracles to check conformance of client code to requirements of JDK Javadocs using LLMs](https://arxiv.org/abs/2411.01789)
*Shan Jiang,Chenguang Zhu,Sarfraz Khurshid*

Main category: cs.SE

TL;DR: 论文提出了一种利用Javadocs和大型语言模型自动化生成Java库测试预言的方法，实验表明该方法高效且准确。


<details>
  <summary>Details</summary>
Motivation: 测试预言自动化是一个较少被探索的领域，尤其是如何从非正式的自然语言描述中提取预期行为。Javadocs提供了丰富的库行为信息，可用于自动化生成测试预言。

Method: 利用大型语言模型从Javadocs中提取信息，生成测试预言，包括正常和异常行为的检查。

Result: 实验显示，98.8%的预言可编译，96.4%准确反映预期属性，错误可通过LLM生成的额外注释轻松修正。

Conclusion: Javadocs结合大型语言模型是一种高效且准确的测试预言自动化方法，具有实际应用潜力。

Abstract: Software testing remains the most widely used methodology for validating
quality of code. However, effectiveness of testing critically depends on the
quality of test suites used. Test cases in a test suite consist of two
fundamental parts: (1) input values for the code under test, and (2) correct
checks for the outputs it produces. These checks are commonly written as
assertions, and termed test oracles. The last couple of decades have seen much
progress in automated test input generation, e.g., using fuzzing and symbolic
execution. However, automating test oracles remains a relatively less explored
problem area. Indeed, a test oracle by its nature requires knowledge of
expected behavior, which may only be known to the developer and may not not
exist in a formal language that supports automated reasoning.
  Our focus in this paper is automation of test oracles for clients of widely
used Java libraries, e.g., java.lang and java.util packages. Our key insight is
that Javadocs that provide a rich source of information can enable automated
generation of test oracles. Javadocs of the core Java libraries are fairly
detailed documents that contain natural language descriptions of not only how
the libraries behave but also how the clients must (not) use them. We use large
language models as an enabling technology to embody our insight into a
framework for test oracle automation, and evaluate it experimentally. Our
experiments demonstrate that LLMs can generate oracles for checking normal and
exceptional behaviors from Javadocs, with 98.8% of these oracles being
compilable and 96.4% accurately reflecting intended properties. Even for the
few incorrect oracles, errors are minor and can be easily corrected with the
help of additional comment information generated by the LLMs.

</details>


### [37] [On the Effectiveness of Large Language Models in Writing Alloy Formulas](https://arxiv.org/abs/2502.15441)
*Yang Hong,Shan Jiang,Yulei Fu,Sarfraz Khurshid*

Main category: cs.SE

TL;DR: 论文探讨了使用大型语言模型（LLMs）编写Alloy声明式公式的实验，展示了LLMs在生成、转换和补全Alloy公式方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 声明式规范对开发安全可靠的软件系统至关重要，但正确编写规范仍然具有挑战性。

Method: 通过实验，使用ChatGPT和DeepSeek两种LLMs，从自然语言描述生成Alloy公式、生成等价公式，以及补全公式草图。

Result: 实验结果表明，LLMs在生成和补全Alloy公式方面表现良好，并能提供多种独特解决方案。

Conclusion: LLMs为规范编写提供了新的可能性，有望在软件开发中发挥重要作用。

Abstract: Declarative specifications have a vital role to play in developing safe and
dependable software systems. Writing specifications correctly, however, remains
particularly challenging. This paper presents a controlled experiment on using
large language models (LLMs) to write declarative formulas in the well-known
language Alloy. Our use of LLMs is three-fold. One, we employ LLMs to write
complete Alloy formulas from given natural language descriptions (in English).
Two, we employ LLMs to create alternative but equivalent formulas in Alloy with
respect to given Alloy formulas. Three, we employ LLMs to complete sketches of
Alloy formulas and populate the holes in the sketches by synthesizing Alloy
expressions and operators so that the completed formulas accurately represent
the desired properties (that are given in natural language). We conduct the
experimental evaluation using 11 well-studied subject specifications and employ
two popular LLMs, namely ChatGPT and DeepSeek. The experimental results show
that the LLMs generally perform well in synthesizing complete Alloy formulas
from input properties given in natural language or in Alloy, and are able to
enumerate multiple unique solutions. Moreover, the LLMs are also successful at
completing given sketches of Alloy formulas with respect to natural language
descriptions of desired properties (without requiring test cases). We believe
LLMs offer a very exciting advance in our ability to write specifications, and
can help make specifications take a pivotal role in software development and
enhance our ability to build robust software.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [38] [DM-RSA: An Extension of RSA with Dual Modulus](https://arxiv.org/abs/2507.14197)
*Andriamifidisoa Ramamonjy,Rufine Marius Lalasoa*

Main category: cs.CR

TL;DR: DM-RSA是一种RSA加密系统的变体，使用双模数增强安全性，通过CRT提高抗侧信道攻击能力，同时保持效率。


<details>
  <summary>Details</summary>
Motivation: 传统RSA在安全性上存在不足，尤其是对侧信道攻击和模数部分泄露的脆弱性。

Method: 采用双模数设计，利用中国剩余定理（CRT）进行解密。

Result: 提高了抗侧信道攻击能力，同时保持高效性，易于集成现有基础设施。

Conclusion: DM-RSA在安全性和效率上取得了平衡，适合实际应用。

Abstract: We introduce DM-RSA (Dual Modulus RSA), a variant of the RSA cryptosystem
that employs two distinct moduli symmetrically to enhance security. By
leveraging the Chinese Remainder Theorem (CRT) for decryption, DM-RSA provides
increased robustness against side-channel attacks while preserving the
efficiency of classical RSA. This approach improves resistance to partial
compromise of a modulus and integrates easily into existing infrastructures.

</details>


### [39] [ExCyTIn-Bench: Evaluating LLM agents on Cyber Threat Investigation](https://arxiv.org/abs/2507.14201)
*Yiran Wu,Mauricio Velazco,Andrew Zhao,Manuel Raúl Meléndez Luján,Srisuma Movva,Yogesh K Roy,Quang Nguyen,Roberto Rodriguez,Qingyun Wu,Michael Albada,Julia Kiseleva,Anand Mudgerikar*

Main category: cs.CR

TL;DR: ExCyTIn-Bench是首个评估LLM代理在网络安全威胁调查任务中的基准，通过基于调查图的安全问题生成数据集，支持自动化和可解释的评估。


<details>
  <summary>Details</summary>
Motivation: 现实世界中安全分析师需处理大量异构警报和日志，而LLM代理的自动威胁调查是一个有前景的方向，需要开发评估工具。

Method: 构建包含8种模拟攻击、57个日志表和589个问题的数据集，利用专家设计的检测逻辑生成调查图，并通过LLM生成问题。

Result: 实验显示任务难度较高，平均奖励为0.249，最佳模型为0.368，表明未来研究空间大。

Conclusion: ExCyTIn-Bench为LLM代理的开发和评估提供了可扩展、可解释的基准，代码和数据即将发布。

Abstract: We present ExCyTIn-Bench, the first benchmark to Evaluate an LLM agent x on
the task of Cyber Threat Investigation through security questions derived from
investigation graphs. Real-world security analysts must sift through a large
number of heterogeneous alert signals and security logs, follow multi-hop
chains of evidence, and compile an incident report. With the developments of
LLMs, building LLM-based agents for automatic thread investigation is a
promising direction. To assist the development and evaluation of LLM agents, we
construct a dataset from a controlled Azure tenant that covers 8 simulated
real-world multi-step attacks, 57 log tables from Microsoft Sentinel and
related services, and 589 automatically generated questions. We leverage
security logs extracted with expert-crafted detection logic to build threat
investigation graphs, and then generate questions with LLMs using paired nodes
on the graph, taking the start node as background context and the end node as
answer. Anchoring each question to these explicit nodes and edges not only
provides automatic, explainable ground truth answers but also makes the
pipeline reusable and readily extensible to new logs. This also enables the
automatic generation of procedural tasks with verifiable rewards, which can be
naturally extended to training agents via reinforcement learning. Our
comprehensive experiments with different models confirm the difficulty of the
task: with the base setting, the average reward across all evaluated models is
0.249, and the best achieved is 0.368, leaving substantial headroom for future
research. Code and data are coming soon!

</details>


### [40] [PRM-Free Security Alignment of Large Models via Red Teaming and Adversarial Training](https://arxiv.org/abs/2507.14202)
*Pengfei Du*

Main category: cs.CR

TL;DR: 提出了一种无需PRM的安全对齐框架，通过自动化红队和对抗训练实现高效安全保证，计算成本降低61%。


<details>
  <summary>Details</summary>
Motivation: LLMs在广泛应用中表现出色，但存在安全风险，当前基于PRM的方法计算开销大且难以扩展。

Method: 利用自动化红队和对抗训练，包括遗传算法优化、多智能体模拟和高级提示变异技术，结合课程学习和自适应正则化。

Result: 在五种先进LLMs上验证，性能优于PRM方法，计算成本降低61%。

Conclusion: 该框架为资源有限组织提供了高效安全对齐方案，并为应对不断演变的威胁奠定了基础。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across
diverse applications, yet they pose significant security risks that threaten
their safe deployment in critical domains. Current security alignment
methodologies predominantly rely on Process Reward Models (PRMs) to evaluate
intermediate reasoning steps, introducing substantial computational overhead
and scalability constraints. This paper presents a novel PRM-free security
alignment framework that leverages automated red teaming and adversarial
training to achieve robust security guarantees while maintaining computational
efficiency. Our approach systematically identifies vulnerabilities through
sophisticated attack strategies including genetic algorithm optimization,
multi-agent simulation, and advanced prompt mutation techniques. The framework
enhances model robustness via targeted adversarial training with curriculum
learning and adaptive regularization mechanisms. Comprehensive experimental
evaluation across five state-of-the-art LLMs demonstrates that our method
achieves superior security alignment performance compared to PRM-based
approaches while reducing computational costs by 61\%. The framework
incorporates transparent reporting and continuous audit mechanisms that enable
iterative security improvement and regulatory compliance. Our contributions
advance the field of efficient LLM security alignment by democratizing access
to robust security measures for resource-constrained organizations and
providing a scalable foundation for addressing evolving adversarial threats.

</details>


### [41] [Mitigating Trojanized Prompt Chains in Educational LLM Use Cases: Experimental Findings and Detection Tool Design](https://arxiv.org/abs/2507.14207)
*Richard M. Charles,James H. Curry,Richard B. Charles*

Main category: cs.CR

TL;DR: 研究探讨了K-12教育中大型语言模型（LLMs）的安全漏洞，学生可能通过特洛伊化提示绕过内容审核系统，引发不安全输出。


<details>
  <summary>Details</summary>
Motivation: 揭示LLMs在教育应用中的潜在风险，确保其安全部署。

Method: 通过模拟K-12查询和多轮对话实验，测试GPT-3.5和GPT-4的漏洞，并开发原型工具TrojanPromptGuard（TPG）。

Result: 实验暴露了LLMs的关键漏洞，TPG能有效检测和缓解特洛伊化提示。

Conclusion: 研究为AI安全研究人员和教育技术专家提供了LLMs安全部署的参考。

Abstract: The integration of Large Language Models (LLMs) in K--12 education offers
both transformative opportunities and emerging risks. This study explores how
students may Trojanize prompts to elicit unsafe or unintended outputs from
LLMs, bypassing established content moderation systems with safety guardrils.
Through a systematic experiment involving simulated K--12 queries and
multi-turn dialogues, we expose key vulnerabilities in GPT-3.5 and GPT-4. This
paper presents our experimental design, detailed findings, and a prototype
tool, TrojanPromptGuard (TPG), to automatically detect and mitigate Trojanized
educational prompts. These insights aim to inform both AI safety researchers
and educational technologists on the safe deployment of LLMs for educators.

</details>


### [42] [Secure Goal-Oriented Communication: Defending against Eavesdropping Timing Attacks](https://arxiv.org/abs/2507.14212)
*Federico Mason,Federico Chiariotti,Pietro Talli,Andrea Zanella*

Main category: cs.CR

TL;DR: 目标导向通信（GoC）通过减少传输频率优化通信，但会引入基于时间的侧信道攻击。本文研究了针对GoC的窃听攻击，提出了理论框架和防御措施。


<details>
  <summary>Details</summary>
Motivation: GoC虽能减少传输频率，但会暴露系统状态信息，需研究其安全性和防御方法。

Method: 研究基于时间的侧信道攻击，提出理论框架和两种启发式防御措施。

Result: 实验显示，普通GoC调度器泄漏60%系统状态信息，而防御措施可减少泄漏50%且性能损失较小。

Conclusion: 启发式防御措施能有效平衡GoC的性能优势与信息安全。

Abstract: Goal-oriented Communication (GoC) is a new paradigm that plans data
transmission to occur only when it is instrumental for the receiver to achieve
a certain goal. This leads to the advantage of reducing the frequency of
transmissions significantly while maintaining adherence to the receiver's
objectives. However, GoC scheduling also opens a timing-based side channel that
an eavesdropper can exploit to obtain information about the state of the
system. This type of attack sidesteps even information-theoretic security, as
it exploits the timing of updates rather than their content. In this work, we
study such an eavesdropping attack against pull-based goal-oriented scheduling
for remote monitoring and control of Markov processes. We provide a theoretical
framework for defining the effectiveness of the attack and propose possible
countermeasures, including two practical heuristics that provide a balance
between the performance gains offered by GoC and the amount of leaked
information. Our results show that, while a naive goal-oriented scheduler
allows the eavesdropper to correctly guess the system state about 60% of the
time, our heuristic defenses can halve the leakage with a marginal reduction of
the benefits of goal-oriented approaches.

</details>


### [43] [Magneto-Ionic Hardware Security Primitives: Embedding Data Protection at the Material Level](https://arxiv.org/abs/2507.14213)
*Irena Spasojevic,Federica Celegato,Alessandro Magni,Paola Tiberto,Jordi Sort*

Main category: cs.CR

TL;DR: 提出一种基于磁离子策略的硬件级安全技术，通过电压控制氮离子迁移生成可调磁性指纹，用于抗黑客和防伪。


<details>
  <summary>Details</summary>
Motivation: 大数据时代对高效、安全的硬件需求增加，传统加密方案资源密集且易受攻击，需创新技术保护敏感信息。

Method: 利用电压控制氮离子在FeCoN点中的迁移，生成可调厚度的铁磁子层，形成确定性或概率性磁性状态。

Result: 实现了可重构架构，兼具防篡改、低能耗和可扩展性，支持随机数生成、物理不可克隆函数等功能。

Conclusion: 该技术为下一代硬件安全提供了基于磁性现象的新解决方案。

Abstract: The Big Data revolution has heightened the demand for robust,
energy-efficient security hardware capable of withstanding increasingly
sophisticated cyber threats. Conventional encryption schemes, reliant on
complex algorithms, are resource-intensive and remain vulnerable. To fortify
sensitive information, society needs innovative anti-hacking and
anti-counterfeiting technologies that exploit new materials and designs. Here,
we present a magneto-ionic strategy for hardware-level security based on fully
selective voltage-controlled N3- ion migration within pre-defined, initially
paramagnetic FeCoN dots. This process generates ferromagnetic sublayers of
tuneable thickness, resulting in either deterministic (single-domain or vortex)
or probabilistic states (with coexisting magnetic configurations and
voltage-adjustable probabilities), each exhibiting stochastic orientation and
chirality, thereby providing a rich platform for magnetic fingerprinting. This
approach enables self-protected primitives, including true random number
generators, physical unclonable functions, and in-memory probabilistic
inference. The resulting reconfigurable architecture combines tamper
resistance, low energy consumption, and scalability, marking a significant leap
toward next-generation hardware security rooted in emergent magnetic phenomena.

</details>


### [44] [GPU-Accelerated Interpretable Generalization for Rapid Cyberattack Detection and Forensics](https://arxiv.org/abs/2507.14222)
*Shu-Ting Huang,Wen-Cheng Chung,Hao-Ting Pai*

Main category: cs.CR

TL;DR: IG-GPU是一种基于GPU的PyTorch重构方法，显著提升了IG机制在入侵检测中的性能，实现了116倍的加速，并在大规模数据集上保持了高准确率。


<details>
  <summary>Details</summary>
Motivation: 解决IG机制在CPU上运行时因立方时间复杂度和大中间位集导致的全规模数据集处理不切实际的问题。

Method: 通过将成对交集和子集评估卸载到GPU上，利用PyTorch重构IG机制。

Result: 在NSL-KDD数据集上，IG-GPU实现了116倍加速，并在全规模数据集上保持了高召回率（0.957）、精确率（0.973）和AUC（0.961）。

Conclusion: IG-GPU填补了严格可解释性与实时网络防御之间的鸿沟，为未来硬件感知调度和多GPU分片提供了基础。

Abstract: The Interpretable Generalization (IG) mechanism recently published in IEEE
Transactions on Information Forensics and Security delivers state-of-the-art,
evidence-based intrusion detection by discovering coherent normal and attack
patterns through exhaustive intersect-and-subset operations-yet its cubic-time
complexity and large intermediate bitsets render full-scale datasets
impractical on CPUs. We present IG-GPU, a PyTorch re-architecture that offloads
all pairwise intersections and subset evaluations to commodity GPUs.
Implemented on a single NVIDIA RTX 4070 Ti, in the 15k-record NSL-KDD dataset,
IG-GPU shows a 116-fold speed-up over the multi-core CPU implementation of IG.
In the full size of NSL-KDD (148k-record), given small training data (e.g.,
10%-90% train-test split), IG-GPU runs in 18 minutes with Recall 0.957,
Precision 0.973, and AUC 0.961, whereas IG required down-sampling to
15k-records to avoid memory exhaustion and obtained Recall 0.935, Precision
0.942, and AUC 0.940. The results confirm that IG-GPU is robust across scales
and could provide millisecond-level per-flow inference once patterns are
learned. IG-GPU thus bridges the gap between rigorous interpretability and
real-time cyber-defense, offering a portable foundation for future work on
hardware-aware scheduling, multi-GPU sharding, and dataset-specific sparsity
optimizations.

</details>


### [45] [Multi-Granular Discretization for Interpretable Generalization in Precise Cyberattack Identification](https://arxiv.org/abs/2507.14223)
*Wen-Cheng Chung,Shu-Ting Huang,Hao-Ting Pai*

Main category: cs.CR

TL;DR: 论文提出了一种可解释的入侵检测系统（IG-MD），通过多粒度离散化技术提升精度，同时保持高召回率和透明度。


<details>
  <summary>Details</summary>
Motivation: 现有XAI方法在入侵检测系统中提供部分或误导性解释，需要一种完全可审计的规则生成方法。

Method: 采用Interpretable Generalization（IG）机制学习特征组合，并引入Multi-Granular Discretization（IG-MD）技术处理连续特征。

Result: 在多个数据集上表现优异，IG-MD在UKM-IDS20上提升精度≥4%，召回率保持≈1.0。

Conclusion: IG-MD展示了单一模型可跨域扩展，无需定制调整，同时保持高透明度和性能。

Abstract: Explainable intrusion detection systems (IDS) are now recognized as essential
for mission-critical networks, yet most "XAI" pipelines still bolt an
approximate explainer onto an opaque classifier, leaving analysts with partial
and sometimes misleading insights. The Interpretable Generalization (IG)
mechanism, published in IEEE Transactions on Information Forensics and
Security, eliminates that bottleneck by learning coherent patterns - feature
combinations unique to benign or malicious traffic - and turning them into
fully auditable rules. IG already delivers outstanding precision, recall, and
AUC on NSL-KDD, UNSW-NB15, and UKM-IDS20, even when trained on only 10% of the
data. To raise precision further without sacrificing transparency, we introduce
Multi-Granular Discretization (IG-MD), which represents every continuous
feature at several Gaussian-based resolutions. On UKM-IDS20, IG-MD lifts
precision by greater than or equal to 4 percentage points across all nine
train-test splits while preserving recall approximately equal to 1.0,
demonstrating that a single interpretation-ready model can scale across domains
without bespoke tuning.

</details>


### [46] [Using Modular Arithmetic Optimized Neural Networks To Crack Affine Cryptographic Schemes Efficiently](https://arxiv.org/abs/2507.14229)
*Vanja Stojanović,Žiga Lesar,CIril Bohak*

Main category: cs.CR

TL;DR: 混合神经网络架构用于仿射密码分析，结合模算术和统计特征学习，在短到中等长度密文中表现优异，但对长密文泛化能力有限。


<details>
  <summary>Details</summary>
Motivation: 受可解释神经网络在模算术和经典密码分析中的进展启发，研究如何结合模算术和统计特征提高仿射密码的密钥恢复准确性。

Method: 采用混合神经网络架构，包含处理原始密文的模算术分支和利用字母频率特征的统计分支。

Result: 在自然英语文本数据集上，混合模型在短到中等长度密文中表现出高密钥恢复准确性，优于纯统计方法。

Conclusion: 混合模型在短到中等长度密文中有效，但对长密文的泛化能力有待改进。

Abstract: We investigate the cryptanalysis of affine ciphers using a hybrid neural
network architecture that combines modular arithmetic-aware and statistical
feature-based learning. Inspired by recent advances in interpretable neural
networks for modular arithmetic and neural cryptanalysis of classical ciphers,
our approach integrates a modular branch that processes raw ciphertext
sequences and a statistical branch that leverages letter frequency features.
Experiments on datasets derived from natural English text demonstrate that the
hybrid model attains high key recovery accuracy for short and moderate
ciphertexts, outperforming purely statistical approaches for the affine cipher.
However, performance degrades for very long ciphertexts, highlighting
challenges in model generalization.

</details>


### [47] [Breaking the Illusion of Security via Interpretation: Interpretable Vision Transformer Systems under Attack](https://arxiv.org/abs/2507.14248)
*Eldor Abdukhamidov,Mohammed Abuhamad,Simon S. Woo,Hyoungshick Kim,Tamer Abuhmed*

Main category: cs.CR

TL;DR: 本文提出了一种名为AdViT的攻击方法，能够同时欺骗视觉Transformer模型及其解释模型，实验表明其在白盒和黑盒场景下均具有高攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉Transformer模型及其解释模型被认为安全性高，但成功攻击可能导致严重后果，而现有研究未充分考虑其对解释模型的影响。

Method: 提出AdViT攻击方法，生成能同时误导Transformer模型及其解释模型的对抗样本。

Result: AdViT在白盒和黑盒场景下攻击成功率均达100%，且生成的对抗样本难以被检测。

Conclusion: 研究表明，即使结合解释模型，Transformer模型仍易受攻击，需进一步研究防御策略。

Abstract: Vision transformer (ViT) models, when coupled with interpretation models, are
regarded as secure and challenging to deceive, making them well-suited for
security-critical domains such as medical applications, autonomous vehicles,
drones, and robotics. However, successful attacks on these systems can lead to
severe consequences. Recent research on threats targeting ViT models primarily
focuses on generating the smallest adversarial perturbations that can deceive
the models with high confidence, without considering their impact on model
interpretations. Nevertheless, the use of interpretation models can effectively
assist in detecting adversarial examples. This study investigates the
vulnerability of transformer models to adversarial attacks, even when combined
with interpretation models. We propose an attack called "AdViT" that generates
adversarial examples capable of misleading both a given transformer model and
its coupled interpretation model. Through extensive experiments on various
transformer models and two transformer-based interpreters, we demonstrate that
AdViT achieves a 100% attack success rate in both white-box and black-box
scenarios. In white-box scenarios, it reaches up to 98% misclassification
confidence, while in black-box scenarios, it reaches up to 76%
misclassification confidence. Remarkably, AdViT consistently generates accurate
interpretations in both scenarios, making the adversarial examples more
difficult to detect.

</details>


### [48] [Quantum-Safe Identity Verification using Relativistic Zero-Knowledge Proof Systems](https://arxiv.org/abs/2507.14324)
*Yao Ma,Wen Yu Kon,Jefferson Chu,Kevin Han Yong Loh,Kaushik Chakraborty,Charles Lim*

Main category: cs.CR

TL;DR: 该论文改进了基于图着色的相对论零知识证明（RZKP）协议，放宽了相对论约束，提升了稳定性和可扩展性，并扩展了协议配置以增强安全性。


<details>
  <summary>Details</summary>
Motivation: 当前基于密码/PIN的身份验证易受钓鱼攻击，需要更安全的解决方案。

Method: 改进两证明者图着色RZKP协议，放宽相对论约束至30米，并提出三证明者配置以增强安全性。

Result: 实验证明协议在稳定性和可扩展性上显著提升，并建立了修改后协议的安全参数上限。

Conclusion: 该工作为近远期身份验证提供了更安全、高效的解决方案。

Abstract: Identity verification is the process of confirming an individual's claimed
identity, which is essential in sectors like finance, healthcare, and online
services to ensure security and prevent fraud. However, current
password/PIN-based identity solutions are susceptible to phishing or skimming
attacks, where malicious intermediaries attempt to steal credentials using fake
identification portals. Alikhani et al. [Nature, 2021] began exploring identity
verification through graph coloring-based relativistic zero-knowledge proofs
(RZKPs), a key cryptographic primitive that enables a prover to demonstrate
knowledge of secret credentials to a verifier without disclosing any
information about the secret. Our work advances this field and addresses
unresolved issues: From an engineering perspective, we relax further the
relativistic constraints from 60m to 30m, and significantly enhance the
stability and scalability of the experimental demonstration of the 2-prover
graph coloring-based RZKP protocol for near-term use cases. At the same time,
for long-term security against entangled malicious provers, we propose a
modified protocol with comparable computation and communication costs, we
establish an upper bound on the soundness parameter for this modified protocol.
On the other hand, we extend the two-prover, two-verifier setup to a
three-prover configuration, demonstrating the security of such relativistic
protocols against entangled malicious provers.

</details>


### [49] [Towards Efficient Privacy-Preserving Machine Learning: A Systematic Review from Protocol, Model, and System Perspectives](https://arxiv.org/abs/2507.14519)
*Wenxuan Zeng,Tianshi Xu,Yi Chen,Yifan Zhou,Mingzhe Zhang,Jin Tan,Cheng Hong,Meng Li*

Main category: cs.CR

TL;DR: 本文综述了隐私保护机器学习（PPML）的研究进展，重点关注跨层级优化，包括协议、模型和系统层级，并讨论了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: PPML在保护用户数据隐私方面具有潜力，但其效率和可扩展性成本较高，因此需要优化。

Method: 通过分类现有研究为协议、模型和系统层级，进行定性和定量比较。

Result: 总结了PPML的优化进展，并提出了跨层级整合优化的必要性。

Conclusion: 本文为PPML领域提供了全面的理解，并指出了未来研究方向，同时通过GitHub仓库持续跟踪进展。

Abstract: Privacy-preserving machine learning (PPML) based on cryptographic protocols
has emerged as a promising paradigm to protect user data privacy in cloud-based
machine learning services. While it achieves formal privacy protection, PPML
often incurs significant efficiency and scalability costs due to orders of
magnitude overhead compared to the plaintext counterpart. Therefore, there has
been a considerable focus on mitigating the efficiency gap for PPML. In this
survey, we provide a comprehensive and systematic review of recent PPML studies
with a focus on cross-level optimizations. Specifically, we categorize existing
papers into protocol level, model level, and system level, and review progress
at each level. We also provide qualitative and quantitative comparisons of
existing works with technical insights, based on which we discuss future
research directions and highlight the necessity of integrating optimizations
across protocol, model, and system levels. We hope this survey can provide an
overarching understanding of existing approaches and potentially inspire future
breakthroughs in the PPML field. As the field is evolving fast, we also provide
a public GitHub repository to continuously track the developments, which is
available at https://github.com/PKU-SEC-Lab/Awesome-PPML-Papers.

</details>


### [50] [FORTA: Byzantine-Resilient FL Aggregation via DFT-Guided Krum](https://arxiv.org/abs/2507.14588)
*Usayd Shahul,J. Harshan*

Main category: cs.CR

TL;DR: FORTA是一个在实数域操作的拜占庭鲁棒安全聚合框架，利用DFT编码保护隐私，改进Krum算法以提高鲁棒性和聚合准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于有限域算术的安全聚合方法在处理实数模型更新时可能产生数值错误和溢出，需要直接在实数域操作的安全聚合方法。

Method: FORTA结合DFT编码和基于Krum的异常检测，通过DFT解码器的反馈改进Krum算法，提升鲁棒性。

Result: 理论分析和实验表明，改进后的Krum算法比标准Krum更具鲁棒性，聚合更准确。

Conclusion: FORTA在实数域实现了高效且鲁棒的拜占庭安全聚合，解决了数值精度问题。

Abstract: Secure federated learning enables collaborative model training across
decentralized users while preserving data privacy. A key component is secure
aggregation, which keeps individual updates hidden from both the server and
users, while also defending against Byzantine users who corrupt the
aggregation. To this end, Jinhyun So et al. recently developed a
Byzantine-resilient secure aggregation scheme using a secret-sharing strategy
over finite-field arithmetic. However, such an approach can suffer from
numerical errors and overflows when applied to real-valued model updates,
motivating the need for secure aggregation methods that operate directly over
the real domain. We propose FORTA, a Byzantine-resilient secure aggregation
framework that operates entirely in the real domain. FORTA leverages Discrete
Fourier Transform (DFT) codes for privacy and employs Krum-based outlier
detection for robustness. While DFT decoder is error-free under infinite
precision, finite precision introduces numerical perturbations that can distort
distance estimates and allow malicious updates to evade detection. To address
this, FORTA refines Krum using feedback from DFT decoder, improving the
selection of trustworthy updates. Theoretical analysis and experiments show
that our modification of Krum offers improved robustness and more accurate
aggregation than standard Krum.

</details>


### [51] [Hybrid Classical-Quantum Rainbow Table Attack on Human Passwords](https://arxiv.org/abs/2507.14600)
*MA. Khajeian*

Main category: cs.CR

TL;DR: 提出了一种结合彩虹表和高效量子搜索的混合框架，用于增强密码恢复能力。


<details>
  <summary>Details</summary>
Motivation: 长且由人生成的密码因其不规则结构和巨大搜索空间，对经典和量子攻击都构成挑战。

Method: 使用基于字典的密码生成和转换规则构建彩虹表，并将其组织为桶以优化查找和空间复杂度；在每个桶内使用分布式精确Grover算法进行量子搜索。

Result: 整体量子电路更浅且对噪声更鲁棒，特别是在近量子设备中常见的去极化通道下。

Conclusion: 提出的混合框架结合了结构化彩虹表和高效量子搜索，显著提升了密码恢复效率。

Abstract: Passwords that are long and human-generated pose a challenge for both
classical and quantum attacks due to their irregular structure and large search
space. In this work, we present an enhanced classical-quantum hybrid attack
tailored to this scenario. We build rainbow tables using dictionary-based
password generation with transformation rules to better model real user
behavior. These tables are then organized into buckets, enabling faster lookup
and reduced space complexity. To perform quantum search within each bucket, we
use a distributed exact variant of Grover's algorithm, which offers lower
circuit depth and deterministic success. As a result, the overall quantum
circuit is shallower and more robust against noise, particularly from
depolarizing channels commonly found in near-term quantum devices. Through this
work, Overall, we propose a hybrid framework that combines structured rainbow
tables with efficient quantum search to enhance password recovery.

</details>


### [52] [VTarbel: Targeted Label Attack with Minimal Knowledge on Detector-enhanced Vertical Federated Learning](https://arxiv.org/abs/2507.14625)
*Juntao Tan,Anran Li,Quanchao Liu,Peng Ran,Lan Zhang*

Main category: cs.CR

TL;DR: VTarbel是一种针对垂直联邦学习（VFL）的两阶段攻击框架，能够在规避异常检测器的情况下实现目标标签攻击。


<details>
  <summary>Details</summary>
Motivation: 现有VFL安全研究多关注隐私漏洞，而针对目标标签攻击的研究不足，且现有方法假设不切实际。

Method: VTarbel通过两阶段攻击（准备阶段和攻击阶段），利用高表现力样本训练本地代理模型和检测器估计，生成对抗样本以规避检测。

Result: VTarbel在多种模型架构、数据集和检测器下均优于现有基线方法，并能有效规避隐私保护防御。

Conclusion: 研究揭示了当前VFL部署中的安全盲点，亟需开发更鲁棒的防御机制。

Abstract: Vertical federated learning (VFL) enables multiple parties with disjoint
features to collaboratively train models without sharing raw data. While
privacy vulnerabilities of VFL are extensively-studied, its security
threats-particularly targeted label attacks-remain underexplored. In such
attacks, a passive party perturbs inputs at inference to force
misclassification into adversary-chosen labels. Existing methods rely on
unrealistic assumptions (e.g., accessing VFL-model's outputs) and ignore
anomaly detectors deployed in real-world systems. To bridge this gap, we
introduce VTarbel, a two-stage, minimal-knowledge attack framework explicitly
designed to evade detector-enhanced VFL inference. During the preparation
stage, the attacker selects a minimal set of high-expressiveness samples (via
maximum mean discrepancy), submits them through VFL protocol to collect
predicted labels, and uses these pseudo-labels to train estimated detector and
surrogate model on local features. In attack stage, these models guide
gradient-based perturbations of remaining samples, crafting adversarial
instances that induce targeted misclassifications and evade detection. We
implement VTarbel and evaluate it against four model architectures, seven
multimodal datasets, and two anomaly detectors. Across all settings, VTarbel
outperforms four state-of-the-art baselines, evades detection, and retains
effective against three representative privacy-preserving defenses. These
results reveal critical security blind spots in current VFL deployments and
underscore urgent need for robust, attack-aware defenses.

</details>


### [53] [VMask: Tunable Label Privacy Protection for Vertical Federated Learning via Layer Masking](https://arxiv.org/abs/2507.14629)
*Juntao Tan,Lan Zhang,Zhonghao Hu,Kai Yang,Peng Ran,Bo Li*

Main category: cs.CR

TL;DR: VMask是一种新型标签隐私保护框架，通过层掩码技术防御模型完成攻击，实现隐私与性能的最佳平衡。


<details>
  <summary>Details</summary>
Motivation: 垂直联邦学习（VFL）易受标签推理攻击，现有防御方法要么牺牲模型准确性，要么计算开销过大。

Method: 采用秘密共享（SS）技术掩码攻击者模型的关键层参数，破坏输入数据与中间输出的强相关性，并提供可调隐私预算。

Result: VMask成功将标签推理准确率降至随机猜测水平，模型性能几乎无损（如Transformer模型精度仅下降0.09%），运行速度远快于加密方法。

Conclusion: VMask在隐私保护与模型性能间取得最佳平衡，是防御模型完成攻击的高效解决方案。

Abstract: Though vertical federated learning (VFL) is generally considered to be
privacy-preserving, recent studies have shown that VFL system is vulnerable to
label inference attacks originating from various attack surfaces. Among these
attacks, the model completion (MC) attack is currently the most powerful one.
Existing defense methods against it either sacrifice model accuracy or incur
impractical computational overhead. In this paper, we propose VMask, a novel
label privacy protection framework designed to defend against MC attack from
the perspective of layer masking. Our key insight is to disrupt the strong
correlation between input data and intermediate outputs by applying the secret
sharing (SS) technique to mask layer parameters in the attacker's model. We
devise a strategy for selecting critical layers to mask, reducing the overhead
that would arise from naively applying SS to the entire model. Moreover, VMask
is the first framework to offer a tunable privacy budget to defenders, allowing
for flexible control over the levels of label privacy according to actual
requirements. We built a VFL system, implemented VMask on it, and extensively
evaluated it using five model architectures and 13 datasets with different
modalities, comparing it to 12 other defense methods. The results demonstrate
that VMask achieves the best privacy-utility trade-off, successfully thwarting
the MC attack (reducing the label inference accuracy to a random guessing
level) while preserving model performance (e.g., in Transformer-based model,
the averaged drop of VFL model accuracy is only 0.09%). VMask's runtime is up
to 60,846 times faster than cryptography-based methods, and it only marginally
exceeds that of standard VFL by 1.8 times in a large Transformer-based model,
which is generally acceptable.

</details>


### [54] [CANDoSA: A Hardware Performance Counter-Based Intrusion Detection System for DoS Attacks on Automotive CAN bus](https://arxiv.org/abs/2507.14739)
*Franco Oberti,Stefano Di Carlo,Alessandro Savino*

Main category: cs.CR

TL;DR: 本文提出了一种基于硬件性能计数器（HPCs）的新型入侵检测系统（IDS），用于检测CAN网络中的异常行为，以提高汽车嵌入式系统的安全性。


<details>
  <summary>Details</summary>
Motivation: CAN协议缺乏内置安全功能，传统安全措施保护有限，随着自动驾驶车辆的普及，网络安全威胁日益严重。

Method: 利用RISC-V架构的CAN接收器模拟，结合AES-128加密和FreeRTOS任务，通过HPCs检测异常行为，并通过数据提取和相关性分析优化HPC特征。

Result: 结果表明，该方法能显著提升CAN网络的安全性，应对汽车网络安全的新挑战。

Conclusion: 基于HPC的IDS为CAN环境提供了一种有效的安全解决方案，具有实际应用潜力。

Abstract: The Controller Area Network (CAN) protocol, essential for automotive embedded
systems, lacks inherent security features, making it vulnerable to cyber
threats, especially with the rise of autonomous vehicles. Traditional security
measures offer limited protection, such as payload encryption and message
authentication. This paper presents a novel Intrusion Detection System (IDS)
designed for the CAN environment, utilizing Hardware Performance Counters
(HPCs) to detect anomalies indicative of cyber attacks. A RISC-V-based CAN
receiver is simulated using the gem5 simulator, processing CAN frame payloads
with AES-128 encryption as FreeRTOS tasks, which trigger distinct HPC
responses. Key HPC features are optimized through data extraction and
correlation analysis to enhance classification efficiency. Results indicate
that this approach could significantly improve CAN security and address
emerging challenges in automotive cybersecurity.

</details>


### [55] [Careful Whisper: Attestation for peer-to-peer Confidential Computing networks](https://arxiv.org/abs/2507.14796)
*Ceren Kocaoğullar,Gustavo Petri,Dominic P. Mulligan,Derek Miller,Hugo J. M. Vincent,Shale Xiong,Alastair R. Beresford*

Main category: cs.CR

TL;DR: Careful Whisper是一种基于gossip的协议，通过线性复杂度降低TEE间的认证开销，支持异构网络中的信任传递和离线节点的信任建立。


<details>
  <summary>Details</summary>
Motivation: 解决TEE在动态网络中直接认证导致的二次通信开销问题。

Method: 提出Careful Whisper协议，利用gossip机制高效传播信任，支持异构网络和离线节点。

Result: 在200节点网络中，每轮仅需0.158秒和21.5 KiB资源，且对认证失败具有鲁棒性。

Conclusion: Careful Whisper在动态网络中高效且可靠地传播信任，优于传统方法。

Abstract: Trusted Execution Environments (TEEs) are designed to protect the privacy and
integrity of data in use. They enable secure data processing and sharing in
peer-to-peer networks, such as vehicular ad hoc networks of autonomous
vehicles, without compromising confidentiality. In these networks, nodes must
establish mutual trust to collaborate securely. TEEs can achieve this through
remote attestation, where a prover presents evidence of its trustworthiness to
a verifier, which then decides whether or not to trust the prover. However, a
naive peer-to-peer attestation approach, where every TEE directly attests every
other TEE, results in quadratic communication overhead. This is inefficient in
dynamic environments, where nodes frequently join and leave the network.
  To address this, we present Careful Whisper, a gossip-based protocol that
disseminates trust efficiently, reducing attestation overhead to linear
complexity under ideal conditions. It enables interoperability by enabling
transitive trust across heterogeneous networks, and supports trust
establishment with offline nodes via relayed attestations. Using a custom
discrete-event simulator, we show that Careful Whisper propagates trust both
faster and more widely than naive approaches across various network topologies.
Our results demonstrate that our protocol is resource efficient, sending ~21.5
KiB and requiring 0.158 seconds per round in a 200-node network, and that our
protocol is resilient to attestation failures across various network
topologies.

</details>


### [56] [Manipulating LLM Web Agents with Indirect Prompt Injection Attack via HTML Accessibility Tree](https://arxiv.org/abs/2507.14799)
*Sam Johnson,Viet Pham,Thai Le*

Main category: cs.CR

TL;DR: LLM-based web navigation agents易受间接提示注入攻击，攻击者可通过HTML嵌入触发器操控代理行为，导致恶意操作。


<details>
  <summary>Details</summary>
Motivation: 揭示LLM驱动的web代理在安全上的脆弱性，强调防御的必要性。

Method: 使用GCG算法和Llama-3.1驱动的Browser Gym代理，测试攻击效果。

Result: 在真实网站上成功实施攻击，如窃取登录凭证和强制广告点击。

Conclusion: LLM代理的广泛应用需更强的安全防护措施。

Abstract: This work demonstrates that LLM-based web navigation agents offer powerful
automation capabilities but are vulnerable to Indirect Prompt Injection (IPI)
attacks. We show that adversaries can embed universal adversarial triggers in
webpage HTML to hijack agent behavior that utilizes the accessibility tree to
parse HTML, causing unintended or malicious actions. Using the Greedy
Coordinate Gradient (GCG) algorithm and a Browser Gym agent powered by
Llama-3.1, our system demonstrates high success rates across real websites in
both targeted and general attacks, including login credential exfiltration and
forced ad clicks. Our empirical results highlight critical security risks and
the need for stronger defenses as LLM-driven autonomous web agents become more
widely adopted. The system software
(https://github.com/sej2020/manipulating-web-agents) is released under the MIT
License, with an accompanying publicly available demo website
(http://lethaiq.github.io/attack-web-llm-agent).

</details>


### [57] [Quantum Skyshield: Quantum Key Distribution and Post-Quantum Authentication for Low-Altitude Wireless Networks in Adverse Skies](https://arxiv.org/abs/2507.14822)
*Zeeshan Kaleem,Misha Urooj Khan,Ahmad Suleman,Waqas Khalid,Kai-Kit Wong,Chau Yuen*

Main category: cs.CR

TL;DR: 论文提出了一种名为Quantum Skyshield的量子安全架构，用于解决低空无线网络（LAWNs）中的安全问题，结合了BB84量子密钥分发和后量子认证机制。


<details>
  <summary>Details</summary>
Motivation: 随着无人机和高空平台的密集部署，低空无线网络的安全问题日益突出，尤其是自由空间光通信（FSO）的脆弱性。

Method: 采用BB84量子密钥分发（QKD）和Lamport一次性签名、HMAC等后量子认证机制，并设计了Grover启发的威胁检测机制。

Result: 仿真结果显示，在量子比特错误率（QBER）低于11%时，可可靠生成128位对称密钥，威胁检测机制单次迭代的异常识别概率达89%。

Conclusion: Quantum Skyshield为LAWNs提供了可靠的安全解决方案，并提出了未来研究方向。

Abstract: Recently, low-altitude wireless networks (LAWNs) have emerged as a critical
backbone for supporting the low-altitude economy, particularly with the
densification of unmanned aerial vehicles (UAVs) and high-altitude platforms
(HAPs). To meet growing data demands, some LAWN deployments incorporate
free-space optical (FSO) links, which offer exceptional bandwidth and beam
directivity. However, without strong security measures in place, both
conventional radio frequency channels and FSO beams remain vulnerable to
interception and spoofing and FSO in particular can suffer from turbulence,
misalignment, and weather-related attenuation. To address these challenges in
the quantum era, a quantum-secure architecture called Quantum Skyshield is
proposed to enable reliable communication between the base transceiver station
(BTS) and LAWN. The proposed design integrates BB84 quantum key distribution
(QKD) with post-quantum authentication mechanisms. Simulation results confirm
the reliable generation of a 128-bit symmetric key when the quantum bit error
rate (QBER) remains below the threshold of 11%. Authentication is enforced
using Lamport one-time signatures and hash-based message authentication codes
(HMAC) to ensure message integrity. A Grover-inspired threat detection
mechanism identifies anomalies with up to 89% probability in a single
iteration, enabling real-time trust evaluation. Lastly, future research
challenges have also been identified and discussed to guide further development
in this area.

</details>


### [58] [A Privacy-Centric Approach: Scalable and Secure Federated Learning Enabled by Hybrid Homomorphic Encryption](https://arxiv.org/abs/2507.14853)
*Khoa Nguyen,Tanveer Khan,Antonis Michalas*

Main category: cs.CR

TL;DR: 论文探讨了如何通过混合同态加密（HHE）结合联邦学习（FL）解决通信和隐私问题，以实现可扩展且安全的去中心化学习系统。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在隐私敏感领域具有潜力，但面临通信开销和数据隐私的挑战，现有隐私保护技术（如纯同态加密）计算和通信成本高，限制了实际应用。

Method: 提出将混合同态加密（HHE）与联邦学习结合，HHE结合了对称加密和同态加密，以降低计算和通信成本。

Result: HHE与FL的结合有效解决了通信和隐私问题，为可扩展且安全的去中心化学习系统提供了可能。

Conclusion: 混合同态加密是联邦学习中解决隐私和通信问题的有效方法，具有实际部署潜力。

Abstract: Federated Learning (FL) enables collaborative model training without sharing
raw data, making it a promising approach for privacy-sensitive domains. Despite
its potential, FL faces significant challenges, particularly in terms of
communication overhead and data privacy. Privacy-preserving Techniques (PPTs)
such as Homomorphic Encryption (HE) have been used to mitigate these concerns.
However, these techniques introduce substantial computational and communication
costs, limiting their practical deployment. In this work, we explore how Hybrid
Homomorphic Encryption (HHE), a cryptographic protocol that combines symmetric
encryption with HE, can be effectively integrated with FL to address both
communication and privacy challenges, paving the way for scalable and secure
decentralized learning system.

</details>


### [59] [A Compact Post-quantum Strong Designated Verifier Signature Scheme from Isogenies](https://arxiv.org/abs/2507.14893)
*Farzin Renan*

Main category: cs.CR

TL;DR: 论文提出了一种基于同源的新型强指定验证者签名方案（CSI-SDVS），具有紧凑的密钥和签名大小，且能抵抗量子攻击。


<details>
  <summary>Details</summary>
Motivation: 隐私敏感应用（如电子投票和数字现金）需要更严格的验证模型以确保保密性和控制，现有方案易受量子攻击或体积过大。

Method: 基于CSIDH的理想类群动作框架和CSI-FiSh的签名技术，依赖于多目标群动作逆问题的困难性。

Result: CSI-SDVS在随机预言模型中实现了强安全性（SUF-CMA、NT、PSI），密钥和签名大小仅为O(λ)。

Conclusion: CSI-SDVS是目前最紧凑的后量子SDVS方案，也是唯一基于同源的安全构造。

Abstract: Digital signatures are essential cryptographic tools that provide
authentication and integrity in digital communications. However,
privacy-sensitive applications, such as e-voting and digital cash, require more
restrictive verification models to ensure confidentiality and control. Strong
Designated Verifier Signature (SDVS) schemes address this need by enabling the
signer to designate a specific verifier, ensuring that only this party can
validate the signature. Existing SDVS constructions are primarily based on
number-theoretic assumptions and are therefore vulnerable to quantum attacks.
Although post-quantum alternatives, particularly those based on lattices, have
been proposed, they often entail large key and signature sizes. In this work,
we introduce $\mathsf{CSI\text{-}SDVS}$, a novel isogeny-based SDVS scheme that
offers a compact, quantum-resistant alternative. Our construction builds on the
ideal class group action framework of CSIDH and the signature techniques of
CSI-FiSh, and relies on the hardness of the Multi-Target Group Action Inverse
Problem (MT-GAIP). $\mathsf{CSI\text{-}SDVS}$ achieves strong security
guarantees; namely, Strong Unforgeability under Chosen-Message Attacks
(SUF-CMA), Non-Transferability (NT), and Privacy of Signer's Identity (PSI), in
the random oracle model. Remarkably, both the keys and signatures in
$\mathsf{CSI\text{-}SDVS}$ are of size $\mathcal{O}(\lambda)$, representing a
significant improvement over the typical $\mathcal{O}(\lambda^2)$ bounds in
existing post-quantum SDVS schemes, thereby making it among the most compact
PQC-based SDVS schemes and the only post-quantum secure construction based on
isogenies.

</details>


### [60] [Metaverse Security and Privacy Research: A Systematic Review](https://arxiv.org/abs/2507.14985)
*Argianto Rahartomo,Leonel Merino,Mohammad Ghafari*

Main category: cs.CR

TL;DR: 对2013-2024年间关于元宇宙安全与隐私问题的文献进行了系统综述，揭示了研究趋势、重点领域及现存挑战。


<details>
  <summary>Details</summary>
Motivation: 元宇宙技术的快速发展带来了新的安全与隐私挑战，需系统梳理研究进展以指导未来方向。

Method: 通过文献综述，按方法、安全隐私属性、沉浸式组件和评估策略分类分析。

Result: 过去五年研究激增，重点关注实用和用户中心方法，认证和不可观察性为热门领域，但政策合规等仍存空白。

Conclusion: 需跨学科整合方法以应对元宇宙的技术复杂性和人为因素，构建可信赖的沉浸式环境。

Abstract: The rapid growth of metaverse technologies, including virtual worlds,
augmented reality, and lifelogging, has accelerated their adoption across
diverse domains. This rise exposes users to significant new security and
privacy challenges due to sociotechnical complexity, pervasive connectivity,
and extensive user data collection in immersive environments. We present a
systematic review of the literature published between 2013 and 2024, offering a
comprehensive analysis of how the research community has addressed
metaverse-related security and privacy issues over the past decade. We organize
the studies by method, examined the security and privacy properties, immersive
components, and evaluation strategies. Our investigation reveals a sharp
increase in research activity in the last five years, a strong focus on
practical and user-centered approaches, and a predominant use of benchmarking,
human experimentation, and qualitative methods. Authentication and
unobservability are the most frequently studied properties. However, critical
gaps remain in areas such as policy compliance, accessibility,
interoperability, and back-end infrastructure security. We emphasize the
intertwined technical complexity and human factors of the metaverse and call
for integrated, interdisciplinary approaches to securing inclusive and
trustworthy immersive environments.

</details>


### [61] [LibLMFuzz: LLM-Augmented Fuzz Target Generation for Black-box Libraries](https://arxiv.org/abs/2507.15058)
*Ian Hardgrove,John D. Hastings*

Main category: cs.CR

TL;DR: LibLMFuzz框架利用LLM和轻量工具链自动分析闭源库，降低模糊测试成本，实现100% API覆盖。


<details>
  <summary>Details</summary>
Motivation: 解决闭源库模糊测试的高成本和复杂性。

Method: 结合LLM和工具链自动分析二进制文件、生成驱动并自我修复错误。

Result: 成功为558个API生成驱动，75.52%首次执行正确。

Conclusion: LLM增强的中间件有望降低黑盒组件测试成本，未来可研究分支覆盖。

Abstract: A fundamental problem in cybersecurity and computer science is determining
whether a program is free of bugs and vulnerabilities. Fuzzing, a popular
approach to discovering vulnerabilities in programs, has several advantages
over alternative strategies, although it has investment costs in the form of
initial setup and continuous maintenance. The choice of fuzzing is further
complicated when only a binary library is available, such as the case of
closed-source and proprietary software. In response, we introduce LibLMFuzz, a
framework that reduces costs associated with fuzzing closed-source libraries by
pairing an agentic Large Language Model (LLM) with a lightweight tool-chain
(disassembler/compiler/fuzzer) to autonomously analyze stripped binaries, plan
fuzz strategies, generate drivers, and iteratively self-repair build or runtime
errors. Tested on four widely-used Linux libraries, LibLMFuzz produced
syntactically correct drivers for all 558 fuzz-able API functions, achieving
100% API coverage with no human intervention. Across the 1601 synthesized
drivers, 75.52% were nominally correct on first execution. The results show
that LLM-augmented middleware holds promise in reducing the costs of fuzzing
black box components and provides a foundation for future research efforts.
Future opportunities exist for research in branch coverage.

</details>


### [62] [PromptArmor: Simple yet Effective Prompt Injection Defenses](https://arxiv.org/abs/2507.15219)
*Tianneng Shi,Kaijie Zhu,Zhun Wang,Yuqi Jia,Will Cai,Weida Liang,Haonan Wang,Hend Alzahrani,Joshua Lu,Kenji Kawaguchi,Basel Alomair,Xuandong Zhao,William Yang Wang,Neil Gong,Wenbo Guo,Dawn Song*

Main category: cs.CR

TL;DR: PromptArmor是一种简单有效的防御方法，通过检测和移除输入中的恶意提示来保护LLM代理免受提示注入攻击。


<details>
  <summary>Details</summary>
Motivation: LLM代理易受提示注入攻击，导致执行攻击者指定的任务而非用户意图。

Method: PromptArmor利用现成的LLM检测并移除输入中的潜在恶意提示。

Result: 在AgentDojo基准测试中，PromptArmor的误报率和漏报率均低于1%，攻击成功率降至1%以下。

Conclusion: PromptArmor应作为评估新防御方法的标准基线。

Abstract: Despite their potential, recent research has demonstrated that LLM agents are
vulnerable to prompt injection attacks, where malicious prompts are injected
into the agent's input, causing it to perform an attacker-specified task rather
than the intended task provided by the user. In this paper, we present
PromptArmor, a simple yet effective defense against prompt injection attacks.
Specifically, PromptArmor prompts an off-the-shelf LLM to detect and remove
potential injected prompts from the input before the agent processes it. Our
results show that PromptArmor can accurately identify and remove injected
prompts. For example, using GPT-4o, GPT-4.1, or o4-mini, PromptArmor achieves
both a false positive rate and a false negative rate below 1% on the AgentDojo
benchmark. Moreover, after removing injected prompts with PromptArmor, the
attack success rate drops to below 1%. We also demonstrate PromptArmor's
effectiveness against adaptive attacks and explore different strategies for
prompting an LLM. We recommend that PromptArmor be adopted as a standard
baseline for evaluating new defenses against prompt injection attacks.

</details>


### [63] [The Matrix Subcode Equivalence problem and its application to signature with MPC-in-the-Head](https://arxiv.org/abs/2507.15377)
*Magali Bardet,Charles Brion,Philippe Gaborit,Mercedes Haiech,Romaric Neveu*

Main category: cs.CR

TL;DR: 论文提出了两个新问题：矩阵子码等价问题和矩阵码置换核问题，并基于MPCitH范式构建签名方案。这些问题与矩阵码等价问题相关，且证明矩阵子码等价问题可归约为NP完全的Hamming子码等价问题。通过分析算法复杂性和攻击，参数比矩阵码等价问题更小，签名大小约为4,800字节，公钥约为275字节，性能优于SPHINCS+和MEDS。


<details>
  <summary>Details</summary>
Motivation: 当前密码学中，矩阵码的子码等价问题尚未被研究，而Hamming度量下已有定义。论文旨在填补这一空白，提出新问题并构建高效的签名方案。

Method: 引入矩阵子码等价问题和矩阵码置换核问题，应用MPCitH范式构建签名方案，并分析算法复杂性和攻击。

Result: 签名大小约为4,800字节，公钥约为275字节，性能优于SPHINCS+和MEDS，签名和公钥总大小减少近5倍。

Conclusion: 论文通过新问题构建的签名方案在性能和多样性上优于现有方案，为后量子签名领域提供了新思路。

Abstract: Nowadays, equivalence problems are widely used in cryptography, most notably
to establish cryptosystems such as digital signatures, with MEDS, LESS, PERK as
the most recent ones. However, in the context of matrix codes, only the code
equivalence problem has been studied, while the subcode equivalence is
well-defined in the Hamming metric. In this work, we introduce two new
problems: the Matrix Subcode Equivalence Problem and the Matrix Code Permuted
Kernel Problem, to which we apply the MPCitH paradigm to build a signature
scheme. These new problems, closely related to the Matrix Code Equivalence
problem, ask to find an isometry given a code $C$ and a subcode $D$.
Furthermore, we prove that the Matrix Subcode Equivalence problem reduces to
the Hamming Subcode Equivalence problem, which is known to be NP-Complete, thus
introducing the matrix code version of the Permuted Kernel Problem. We also
adapt the combinatorial and algebraic algorithms for the Matrix Code
Equivalence problem to the subcode case, and we analyze their complexities. We
find with this analysis that the algorithms perform much worse than in the code
equivalence case, which is the same as what happens in the Hamming metric.
Finally, our analysis of the attacks allows us to take parameters much smaller
than in the Matrix Code Equivalence case. Coupled with the effectiveness of
\textit{Threshold-Computation-in-the-Head} or \textit{VOLE-in-the-Head}, we
obtain a signature size of $\approx$ 4 800 Bytes, with a public key of
$\approx$ 275 Bytes. We thus obtain a reasonable signature size, which brings
diversity in the landscape of post-quantum signature schemes, by relying on a
new hard problem. In particular, this new signature scheme performs better than
SPHINCS+, with a smaller size of public key + signature. Our signature compares
also well with other signature schemes: compared to MEDS, the signature is
smaller, and we reduced the size of the sum of signature and public key by a
factor close to 5. We also obtain a signature size that is almost half the size
of the CROSS signature scheme.

</details>


### [64] [PiMRef: Detecting and Explaining Ever-evolving Spear Phishing Emails with Knowledge Base Invariants](https://arxiv.org/abs/2507.15393)
*Ruofan Liu,Yun Lin,Silas Yeo Shuen Yu,Xiwen Teoh,Zhenkai Liang,Jin Song Dong*

Main category: cs.CR

TL;DR: 论文提出了一种基于知识库的钓鱼邮件检测方法PiMRef，通过验证发件人身份的真实性和检测可疑行为来识别钓鱼邮件，显著提升了检测精度和效率。


<details>
  <summary>Details</summary>
Motivation: 钓鱼邮件因其广泛传播和低成本成为网络犯罪的重要手段，传统检测方法难以应对其动态变化，而大型语言模型（LLM）的兴起进一步加剧了这一威胁。

Method: PiMRef通过提取发件人身份、验证域名合法性和检测用户互动提示，将钓鱼检测转化为身份事实核查任务。

Result: 在标准测试中，PiMRef的精度提升了8.8%，召回率保持不变；在真实场景中，其精度达92.1%，召回率达87.9%，运行时间中位数为0.05秒。

Conclusion: PiMRef在钓鱼邮件检测中表现出色，优于现有方法，为防御LLM生成的钓鱼邮件提供了有效解决方案。

Abstract: Phishing emails are a critical component of the cybercrime kill chain due to
their wide reach and low cost. Their ever-evolving nature renders traditional
rule-based and feature-engineered detectors ineffective in the ongoing arms
race between attackers and defenders. The rise of large language models (LLMs)
further exacerbates the threat, enabling attackers to craft highly convincing
phishing emails at minimal cost.
  This work demonstrates that LLMs can generate psychologically persuasive
phishing emails tailored to victim profiles, successfully bypassing nearly all
commercial and academic detectors. To defend against such threats, we propose
PiMRef, the first reference-based phishing email detector that leverages
knowledge-based invariants. Our core insight is that persuasive phishing emails
often contain disprovable identity claims, which contradict real-world facts.
PiMRef reframes phishing detection as an identity fact-checking task. Given an
email, PiMRef (i) extracts the sender's claimed identity, (ii) verifies the
legitimacy of the sender's domain against a predefined knowledge base, and
(iii) detects call-to-action prompts that push user engagement. Contradictory
claims are flagged as phishing indicators and serve as human-understandable
explanations.
  Compared to existing methods such as D-Fence, HelpHed, and ChatSpamDetector,
PiMRef boosts precision by 8.8% with no loss in recall on standard benchmarks
like Nazario and PhishPot. In a real-world evaluation of 10,183 emails across
five university accounts over three years, PiMRef achieved 92.1% precision,
87.9% recall, and a median runtime of 0.05s, outperforming the state-of-the-art
in both effectiveness and efficiency.

</details>


### [65] [PhishIntentionLLM: Uncovering Phishing Website Intentions through Multi-Agent Retrieval-Augmented Generation](https://arxiv.org/abs/2507.15419)
*Wenhao Li,Selvakumar Manickam,Yung-wey Chong,Shankar Karuppayah*

Main category: cs.CR

TL;DR: 提出PhishIntentionLLM框架，利用多代理RAG技术从网站截图识别钓鱼意图，显著提升检测精度。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注钓鱼网站检测，而恶意意图识别研究不足。

Method: 基于大型语言模型的视觉-语言能力，构建多代理RAG框架，识别四种钓鱼目标。

Result: 在GPT-4o上实现0.7895的微精度，比单代理基线提升约95%。

Conclusion: 提供了一种可扩展且可解释的钓鱼意图分析解决方案。

Abstract: Phishing websites remain a major cybersecurity threat, yet existing methods
primarily focus on detection, while the recognition of underlying malicious
intentions remains largely unexplored. To address this gap, we propose
PhishIntentionLLM, a multi-agent retrieval-augmented generation (RAG) framework
that uncovers phishing intentions from website screenshots. Leveraging the
visual-language capabilities of large language models (LLMs), our framework
identifies four key phishing objectives: Credential Theft, Financial Fraud,
Malware Distribution, and Personal Information Harvesting. We construct and
release the first phishing intention ground truth dataset (~2K samples) and
evaluate the framework using four commercial LLMs. Experimental results show
that PhishIntentionLLM achieves a micro-precision of 0.7895 with GPT-4o and
significantly outperforms the single-agent baseline with a ~95% improvement in
micro-precision. Compared to the previous work, it achieves 0.8545 precision
for credential theft, marking a ~4% improvement. Additionally, we generate a
larger dataset of ~9K samples for large-scale phishing intention profiling
across sectors. This work provides a scalable and interpretable solution for
intention-aware phishing analysis.

</details>


### [66] [Cryptanalysis of a multivariate CCZ scheme](https://arxiv.org/abs/2507.15449)
*Alessio Caminata,Elisa Gorla,Madison Mabe,Martina Vigorito,Irene Villa*

Main category: cs.CR

TL;DR: 论文分析了Pesto方案，发现其公开的4次多项式系统可高效降为二次多项式系统，质疑CCZ变换的安全性提升效果。


<details>
  <summary>Details</summary>
Motivation: 研究Pesto方案中CCZ变换对安全性的实际影响，验证其是否如预期提供显著安全性提升。

Method: 通过数学方法将公开的4次多项式系统降为二次多项式系统。

Result: 发现CCZ变换并未显著增加安全性，公开系统可被高效降维。

Conclusion: CCZ变换在Pesto方案中的安全性提升效果有限，需重新评估其应用价值。

Abstract: We consider the multivariate scheme Pesto, which was introduced by Calderini,
Caminata, and Villa. In this scheme, the public polynomials are obtained by
applying a CCZ transformation to a set of quadratic secret polynomials. As a
consequence, the public key consists of polynomials of degree 4. In this work,
we show that the public degree 4 polynomial system can be efficiently reduced
to a system of quadratic polynomials. This seems to suggest that the CCZ
transformation may not offer a significant increase in security, contrary to
what was initially believed.

</details>


### [67] [Multi-Stage Prompt Inference Attacks on Enterprise LLM Systems](https://arxiv.org/abs/2507.15613)
*Andrii Balashov,Olena Ponomarova,Xiaohua Zhai*

Main category: cs.CR

TL;DR: 论文研究了企业环境中LLMs面临的多阶段提示推理攻击，提出了防御措施，并验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 企业部署的LLMs面临新型安全威胁，如通过多阶段提示推理攻击提取机密数据。

Method: 通过概率理论、优化框架和信息论泄漏边界分析攻击，提出统计异常检测、细粒度访问控制等防御措施。

Result: 攻击能可靠提取敏感信息，提出的防御措施（如差分隐私训练和异常检测）显著降低攻击成功率。

Conclusion: 企业LLM安全需从单轮提示过滤转向多阶段攻击与防御的整体视角。

Abstract: Large Language Models (LLMs) deployed in enterprise settings (e.g., as
Microsoft 365 Copilot) face novel security challenges. One critical threat is
prompt inference attacks: adversaries chain together seemingly benign prompts
to gradually extract confidential data. In this paper, we present a
comprehensive study of multi-stage prompt inference attacks in an enterprise
LLM context. We simulate realistic attack scenarios where an attacker uses
mild-mannered queries and indirect prompt injections to exploit an LLM
integrated with private corporate data. We develop a formal threat model for
these multi-turn inference attacks and analyze them using probability theory,
optimization frameworks, and information-theoretic leakage bounds. The attacks
are shown to reliably exfiltrate sensitive information from the LLM's context
(e.g., internal SharePoint documents or emails), even when standard safety
measures are in place.
  We propose and evaluate defenses to counter such attacks, including
statistical anomaly detection, fine-grained access control, prompt sanitization
techniques, and architectural modifications to LLM deployment. Each defense is
supported by mathematical analysis or experimental simulation. For example, we
derive bounds on information leakage under differential privacy-based training
and demonstrate an anomaly detection method that flags multi-turn attacks with
high AUC. We also introduce an approach called "spotlighting" that uses input
transformations to isolate untrusted prompt content, reducing attack success by
an order of magnitude. Finally, we provide a formal proof of concept and
empirical validation for a combined defense-in-depth strategy. Our work
highlights that securing LLMs in enterprise settings requires moving beyond
single-turn prompt filtering toward a holistic, multi-stage perspective on both
attacks and defenses.

</details>


### [68] [Cyber security of Mega Events: A Case Study of Securing the Digital Infrastructure for MahaKumbh 2025 -- A 45 days Mega Event of 600 Million Footfalls](https://arxiv.org/abs/2507.15660)
*Rohit Negi,Amit Negi,Manish Sharma,S. Venkatesan,Prem Kumar,Sandeep K. Shukla*

Main category: cs.CR

TL;DR: 论文探讨了大型活动（如奥运会、世界杯、G20峰会等）数字化带来的网络安全挑战，并以2025年印度MahaKumbh活动为例，提出了一种针对临时性数字基础设施的安全评估和风险管理方法。


<details>
  <summary>Details</summary>
Motivation: 大型活动的数字基础设施通常是临时搭建的，存在安全漏洞，容易成为黑客攻击目标。本文旨在提出一种针对此类临时性基础设施的网络安全保护方法。

Method: 作者团队作为网络安全评估和风险管理监督团队，详细描述了其范围、流程、方法论及实施过程，以确保MahaKumbh 2025活动的安全。

Result: 在45天的活动期间，所有网络攻击均未成功，证明了该方法的有效性。

Conclusion: 本文记录了该方法论，并讨论了未来类似大型活动中可以改进的地方。

Abstract: Mega events such as the Olympics, World Cup tournaments, G-20 Summit,
religious events such as MahaKumbh are increasingly digitalized. From event
ticketing, vendor booth or lodging reservations, sanitation, event scheduling,
customer service, crime reporting, media streaming and messaging on digital
display boards, surveillance, crowd control, traffic control and many other
services are based on mobile and web applications, wired and wireless
networking, network of Closed-Circuit Television (CCTV) cameras, specialized
control room with network and video-feed monitoring. Consequently, cyber
threats directed at such digital infrastructure are common. Starting from hobby
hackers, hacktivists, cyber crime gangs, to the nation state actors, all target
such infrastructure to unleash chaos on an otherwise smooth operation, and
often the cyber threat actors attempt to embarrass the organizing country or
the organizers. Unlike long-standing organizations such as a corporate or a
government department, the infrastructure of mega-events is temporary,
constructed over a short time span in expediency, and often shortcuts are taken
to make the deadline for the event. As a result, securing such an elaborate yet
temporary infrastructure requires a different approach than securing a standard
organizational digital infrastructure. In this paper, we describe our approach
to securing MahaKumbh 2025, a 600 million footfall event for 45 days in
Prayagraj, India, as a cyber security assessment and risk management oversight
team. We chronicle the scope, process, methodology, and outcome of our team's
effort to secure this mega event. It should be noted that none of the cyber
attacks during the 45-day event was successful. Our goal is to put on record
the methodology and discuss what we would do differently in case we work on
similar future mega event.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [69] [The Free Will Equation: Quantum Field Analogies for AGI](https://arxiv.org/abs/2507.14154)
*Rahul Kabali*

Main category: cs.AI

TL;DR: 本文提出了一种名为“自由意志方程”的理论框架，借鉴量子场论，赋予AGI代理一种受控的随机决策能力，以提升其适应性和创造性。


<details>
  <summary>Details</summary>
Motivation: 传统AGI研究专注于确定性规则下的目标优化，而人类智能具有自发性决策能力，这对于创造力和适应性至关重要。本文旨在模拟这种能力。

Method: 通过将AI代理的认知状态视为潜在行动的叠加态，并引入类似量子场和内在动机的机制，实现受控的随机决策。

Result: 在非稳态多臂老虎机环境中，使用该框架的代理比基线方法获得了更高的奖励和策略多样性。

Conclusion: 该框架为AGI提供了一种模拟人类自由意志的方法，有望提升其适应性和创造力。

Abstract: Artificial General Intelligence (AGI) research traditionally focuses on
algorithms that optimize for specific goals under deterministic rules. Yet,
human-like intelligence exhibits adaptive spontaneity - an ability to make
unexpected choices or free decisions not strictly dictated by past data or
immediate reward. This trait, often dubbed "free will" in a loose sense, might
be crucial for creativity, robust adaptation, and avoiding ruts in
problem-solving. This paper proposes a theoretical framework, called the Free
Will Equation, that draws analogies from quantum field theory to endow AGI
agents with a form of adaptive, controlled stochasticity in their
decision-making process. The core idea is to treat an AI agent's cognitive
state as a superposition of potential actions or thoughts, which collapses
probabilistically into a concrete action when a decision is made - much like a
quantum wavefunction collapsing upon measurement. By incorporating mechanisms
analogous to quantum fields, along with intrinsic motivation terms, we aim to
improve an agent's ability to explore novel strategies and adapt to unforeseen
changes. Experiments in a non-stationary multi-armed bandit environment
demonstrate that agents using this framework achieve higher rewards and policy
diversity compared to baseline methods.

</details>


### [70] [DREAMS: Density Functional Theory Based Research Engine for Agentic Materials Simulation](https://arxiv.org/abs/2507.14267)
*Ziqi Wang,Hongshuo Huang,Hancheng Zhao,Changwen Xu,Shang Zhu,Jan Janssen,Venkatasubramanian Viswanathan*

Main category: cs.AI

TL;DR: DREAMS是一个基于DFT的多智能体框架，通过LLM规划器和领域特定代理实现材料发现的高通量、高保真模拟，显著减少对人力的依赖。


<details>
  <summary>Details</summary>
Motivation: 解决DFT模拟中训练时间长、参数调优复杂和系统误差处理困难的问题。

Method: 采用分层多智能体框架，结合LLM规划器和领域代理（如原子结构生成、DFT收敛测试、HPC调度和错误处理），并通过共享画布避免幻觉。

Result: 在Sol27LC基准测试中误差低于1%，解决了CO/Pt(111)吸附难题，并通过贝叶斯采样确认FCC位点偏好。

Conclusion: DREAMS实现了L3级自动化，显著减少对人力的依赖，为高通量材料发现提供了可扩展路径。

Abstract: Materials discovery relies on high-throughput, high-fidelity simulation
techniques such as Density Functional Theory (DFT), which require years of
training, extensive parameter fine-tuning and systematic error handling. To
address these challenges, we introduce the DFT-based Research Engine for
Agentic Materials Screening (DREAMS), a hierarchical, multi-agent framework for
DFT simulation that combines a central Large Language Model (LLM) planner agent
with domain-specific LLM agents for atomistic structure generation, systematic
DFT convergence testing, High-Performance Computing (HPC) scheduling, and error
handling. In addition, a shared canvas helps the LLM agents to structure their
discussions, preserve context and prevent hallucination. We validate DREAMS
capabilities on the Sol27LC lattice-constant benchmark, achieving average
errors below 1\% compared to the results of human DFT experts. Furthermore, we
apply DREAMS to the long-standing CO/Pt(111) adsorption puzzle, demonstrating
its long-term and complex problem-solving capabilities. The framework again
reproduces expert-level literature adsorption-energy differences. Finally,
DREAMS is employed to quantify functional-driven uncertainties with Bayesian
ensemble sampling, confirming the Face Centered Cubic (FCC)-site preference at
the Generalized Gradient Approximation (GGA) DFT level. In conclusion, DREAMS
approaches L3-level automation - autonomous exploration of a defined design
space - and significantly reduces the reliance on human expertise and
intervention, offering a scalable path toward democratized, high-throughput,
high-fidelity computational materials discovery.

</details>


### [71] [WebGuard: Building a Generalizable Guardrail for Web Agents](https://arxiv.org/abs/2507.14293)
*Boyuan Zheng,Zeyi Liao,Scott Salisbury,Zeyuan Liu,Michael Lin,Qinyuan Zheng,Zifan Wang,Xiang Deng,Dawn Song,Huan Sun,Yu Su*

Main category: cs.AI

TL;DR: WebGuard是一个用于评估网络代理行为风险的数据集，旨在开发安全措施。研究发现当前LLMs在预测行为结果和识别高风险行为上表现不佳，但通过微调模型可显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 随着基于LLMs的自主网络代理的快速发展，其可能采取意外或有害行为的风险增加，亟需有效的安全措施。

Method: 引入WebGuard数据集，包含4,939个人工标注的行为，采用三级风险分类（SAFE、LOW、HIGH），并研究微调专用防护模型的效果。

Result: 前沿LLMs在预测行为结果和高风险行为召回率上表现不足（均低于60%）。微调后的Qwen2.5VL-7B模型将准确率从37%提升至80%，高风险行为召回率从20%提升至76%。

Conclusion: 尽管微调模型显著提升了性能，但其可靠性仍不足以支持高风险部署，需进一步优化以实现近乎完美的准确率和召回率。

Abstract: The rapid development of autonomous web agents powered by Large Language
Models (LLMs), while greatly elevating efficiency, exposes the frontier risk of
taking unintended or harmful actions. This situation underscores an urgent need
for effective safety measures, akin to access controls for human users. To
address this critical challenge, we introduce WebGuard, the first comprehensive
dataset designed to support the assessment of web agent action risks and
facilitate the development of guardrails for real-world online environments. In
doing so, WebGuard specifically focuses on predicting the outcome of
state-changing actions and contains 4,939 human-annotated actions from 193
websites across 22 diverse domains, including often-overlooked long-tail
websites. These actions are categorized using a novel three-tier risk schema:
SAFE, LOW, and HIGH. The dataset includes designated training and test splits
to support evaluation under diverse generalization settings. Our initial
evaluations reveal a concerning deficiency: even frontier LLMs achieve less
than 60% accuracy in predicting action outcomes and less than 60% recall in
lagging HIGH-risk actions, highlighting the risks of deploying
current-generation agents without dedicated safeguards. We therefore
investigate fine-tuning specialized guardrail models using WebGuard. We conduct
comprehensive evaluations across multiple generalization settings and find that
a fine-tuned Qwen2.5VL-7B model yields a substantial improvement in
performance, boosting accuracy from 37% to 80% and HIGH-risk action recall from
20% to 76%. Despite these improvements, the performance still falls short of
the reliability required for high-stakes deployment, where guardrails must
approach near-perfect accuracy and recall.

</details>


### [72] [Manimator: Transforming Research Papers into Visual Explanations](https://arxiv.org/abs/2507.14306)
*Samarth P,Vyoman Jain,Shiva Golugula,Motamarri Sai Sathvik*

Main category: cs.AI

TL;DR: Manimator是一个开源系统，利用大型语言模型将研究论文和自然语言提示转化为解释性动画，旨在简化复杂STEM主题的可视化教育内容创作。


<details>
  <summary>Details</summary>
Motivation: 理解复杂科学和数学概念对学习者具有挑战性，动态可视化能提升理解，但手动创建耗时且需要专业技能。

Method: Manimator通过LLM解析输入文本或PDF生成结构化场景描述，再由另一LLM将其转换为可执行的Manim Python代码。

Result: 系统能够快速生成高质量的教育动画，降低创作门槛。

Conclusion: Manimator有潜力成为教育工具，促进STEM内容的民主化创作。

Abstract: Understanding complex scientific and mathematical concepts, particularly
those presented in dense research papers, poses a significant challenge for
learners. Dynamic visualizations can greatly enhance comprehension, but
creating them manually is time-consuming and requires specialized knowledge and
skills. We introduce manimator, an open-source system that leverages Large
Language Models to transform research papers and natural language prompts into
explanatory animations using the Manim engine. Manimator employs a pipeline
where an LLM interprets the input text or research paper PDF to generate a
structured scene description outlining key concepts, mathematical formulas, and
visual elements and another LLM translates this description into executable
Manim Python code. We discuss its potential as an educational tool for rapidly
creating engaging visual explanations for complex STEM topics, democratizing
the creation of high-quality educational content.

</details>


### [73] [Language Models as Ontology Encoders](https://arxiv.org/abs/2507.14334)
*Hui Yang,Jiaoyan Chen,Yuan He,Yongsheng Gao,Ian Horrocks*

Main category: cs.AI

TL;DR: OnT是一种新的本体嵌入方法，结合预训练语言模型和双曲空间几何建模，有效整合文本标签并保留逻辑结构，在预测和推理任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有本体嵌入方法要么忽略文本信息，要么无法保留逻辑结构，因此需要一种能同时解决这两个问题的方法。

Method: 通过双曲空间中的几何建模调整预训练语言模型（PLM），结合文本标签并保留描述逻辑EL的类层次结构和逻辑关系。

Result: 在四个真实世界本体上的实验表明，OnT在预测和推理任务中均优于现有方法，并展示了强大的迁移学习和实际应用潜力。

Conclusion: OnT是一种高效的本体嵌入方法，能够同时利用文本信息和逻辑结构，适用于实际应用和知识推理。

Abstract: OWL (Web Ontology Language) ontologies which are able to formally represent
complex knowledge and support semantic reasoning have been widely adopted
across various domains such as healthcare and bioinformatics. Recently,
ontology embeddings have gained wide attention due to its potential to infer
plausible new knowledge and approximate complex reasoning. However, existing
methods face notable limitations: geometric model-based embeddings typically
overlook valuable textual information, resulting in suboptimal performance,
while the approaches that incorporate text, which are often based on language
models, fail to preserve the logical structure. In this work, we propose a new
ontology embedding method OnT, which tunes a Pretrained Language Model (PLM)
via geometric modeling in a hyperbolic space for effectively incorporating
textual labels and simultaneously preserving class hierarchies and other
logical relationships of Description Logic EL. Extensive experiments on four
real-world ontologies show that OnT consistently outperforms the baselines
including the state-of-the-art across both tasks of prediction and inference of
axioms. OnT also demonstrates strong potential in real-world applications,
indicated by its robust transfer learning abilities and effectiveness in real
cases of constructing a new ontology from SNOMED CT. Data and code are
available at https://github.com/HuiYang1997/OnT.

</details>


### [74] [ProofCompass: Enhancing Specialized Provers with LLM Guidance](https://arxiv.org/abs/2507.14335)
*Nicolas Wischermann,Claudio Mayrink Verdun,Gabriel Poesia,Francesco Noseda*

Main category: cs.AI

TL;DR: ProofCompass是一种混合方法，通过结合大型语言模型（LLM）和专用证明器（如DSP-v1.5），显著提高了数学推理的计算效率和准确性，无需额外训练。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么依赖大型通用模型，要么依赖小型专用模型，各有局限性，且训练大型专用模型计算成本高。

Method: ProofCompass利用LLM提供自然语言证明策略并分析失败尝试，指导专用证明器（如DSP-v1.5）进行问题分解。

Result: 在miniF2F基准测试中，ProofCompass以25倍更少的尝试（128 vs 3200）将准确率从54.9%提升至55.3%。

Conclusion: ProofCompass展示了在形式定理证明中同时提高计算效率和准确性的潜力。

Abstract: Language models have become increasingly powerful tools for formal
mathematical reasoning. However, most existing approaches rely exclusively on
either large general-purpose models or smaller specialized models, each with
distinct limitations, while training specialized large models still requires
significant computational resources. This paper introduces ProofCompass, a
novel hybrid methodology that achieves remarkable computational efficiency by
strategically guiding existing specialized prover methods, such as
DeepSeek-Prover-v1.5-RL (DSP-v1.5) with a Large Language Model (LLM) without
requiring additional model training. The LLM provides natural language proof
strategies and analyzes failed attempts to select intermediate lemmas, enabling
effective problem decomposition. On the miniF2F benchmark, ProofCompass
demonstrates substantial resource efficiency: it outperforms DSP-v1.5 ($54.9\%
\rightarrow 55.3\%$) while using 25x fewer attempts ($3200 \rightarrow 128$).
Our synergistic approach paves the way for simultaneously improving
computational efficiency and accuracy in formal theorem proving.

</details>


### [75] [Adaptive Multi-Agent Reasoning via Automated Workflow Generation](https://arxiv.org/abs/2507.14393)
*Humza Sami,Mubashir ul Islam,Pierre-Emmanuel Gaillardon,Valerio Tenace*

Main category: cs.AI

TL;DR: Nexus Architect是一种改进的多智能体系统框架，通过自动化工作流合成机制提升推理模型的泛化能力，显著优于现有大型推理模型。


<details>
  <summary>Details</summary>
Motivation: 当前大型推理模型（LRMs）在解决新问题时依赖记忆而非推理，泛化能力不足，Nexus Architect旨在解决这一问题。

Method: Nexus Architect通过自动化工作流合成机制，根据用户提示和示例生成定制化推理流程，并结合迭代提示优化机制提升性能。

Result: 在逻辑问题数据集上，Nexus Architect表现优于现有LRMs，最高提升66%通过率。

Conclusion: Nexus Architect显著提升了推理模型的泛化能力和性能，为复杂任务提供了更高效的解决方案。

Abstract: The rise of Large Reasoning Models (LRMs) promises a significant leap forward
in language model capabilities, aiming to tackle increasingly sophisticated
tasks with unprecedented efficiency and accuracy. However, despite their
impressive performance, recent studies have highlighted how current reasoning
models frequently fail to generalize to novel, unseen problems, often resorting
to memorized solutions rather than genuine inferential reasoning. Such behavior
underscores a critical limitation in modern LRMs, i.e., their tendency toward
overfitting, which in turn results in poor generalization in problem-solving
capabilities.
  In this paper, we introduce Nexus Architect, an enhanced iteration of our
multi-agent system framework, Nexus, equipped with a novel automated workflow
synthesis mechanism. Given a user's prompt and a small set of representative
examples, the Architect autonomously generates a tailored reasoning workflow by
selecting suitable strategies, tool integrations, and adversarial techniques
for a specific problem class. Furthermore, the Architect includes an iterative
prompt refinement mechanism that fine-tunes agents' system prompts to maximize
performance and improve the generalization capabilities of the system.
  We empirically evaluate Nexus Architect by employing an off-the-shelf,
non-reasoning model on a custom dataset of challenging logical questions and
compare its performance against state-of-the-art LRMs. Results show that Nexus
Architect consistently outperforms existing solutions, achieving up to a 66%
increase in pass rate over Gemini 2.5 Flash Preview, nearly 2.5$\times$ against
Claude Sonnet 4 and DeepSeek-R1, and over 3$\times$ w.r.t. Llama 4 Scout.

</details>


### [76] [Fail Fast, or Ask: Mitigating the Deficiencies of Reasoning LLMs with Human-in-the-Loop Systems Engineering](https://arxiv.org/abs/2507.14406)
*Michael J. Zellinger,Matt Thomson*

Main category: cs.AI

TL;DR: 通过结合推理模型与人类专家的协作，以及引入非推理模型快速筛选问题，显著降低了错误率和延迟，同时节省成本。


<details>
  <summary>Details</summary>
Motivation: 在风险敏感领域，AI模型的错误率需接近0%，而现有推理模型仍存在错误率高和延迟大的问题。

Method: 提出协作系统：1) 推理模型通过推理轨迹长度量化不确定性，将不确定问题转交人类专家；2) 引入非推理模型快速筛选问题，直接转交人类以减少延迟。

Result: 错误率从3%降至1%以下，延迟减少40%，成本节省50%，同时保持90%以上的准确率。

Conclusion: 通过黑盒系统工程，显著改善了推理模型的错误率和延迟问题，无需修改模型内部。

Abstract: State-of-the-art reasoning LLMs are powerful problem solvers, but they still
occasionally make mistakes. However, adopting AI models in risk-sensitive
domains often requires error rates near 0%. To address this gap, we propose
collaboration between a reasoning model and a human expert who resolves queries
the model cannot confidently answer. We find that quantifying the uncertainty
of a reasoning model through the length of its reasoning trace yields an
effective basis for deferral to a human, e.g., cutting the error rate of Qwen3
235B-A22B on difficult MATH problems from 3% to less than 1% when deferring
7.5% of queries. However, the high latency of reasoning models still makes them
challenging to deploy on use cases with high query volume. To address this
challenge, we explore fronting a reasoning model with a large non-reasoning
model. We call this modified human-in-the-loop system "Fail Fast, or Ask",
since the non-reasoning model may defer difficult queries to the human expert
directly ("failing fast"), without incurring the reasoning model's higher
latency. We show that this approach yields around 40% latency reduction and
about 50% cost savings for DeepSeek R1 while maintaining 90+% area under the
accuracy-rejection curve. However, we observe that latency savings are lower
than expected because of "latency drag", the phenomenon that processing easier
queries with a non-reasoning model pushes the reasoning model's latency
distribution towards longer latencies. Broadly, our results suggest that the
deficiencies of state-of-the-art reasoning models -- nontrivial error rates and
high latency -- can be substantially mitigated through black-box systems
engineering, without requiring access to LLM internals.

</details>


### [77] [Inverse Scaling in Test-Time Compute](https://arxiv.org/abs/2507.14417)
*Aryo Pradipta Gema,Alexander Hägele,Runjin Chen,Andy Arditi,Jacob Goldman-Wetzler,Kit Fraser-Taliente,Henry Sleight,Linda Petrini,Julian Michael,Beatrice Alex,Pasquale Minervini,Yanda Chen,Joe Benton,Ethan Perez*

Main category: cs.AI

TL;DR: 研究发现，增加大型推理模型（LRMs）的推理长度会降低性能，表现为测试计算量与准确率之间的反比关系。研究识别了五种失败模式，并强调了评估多样化推理长度的重要性。


<details>
  <summary>Details</summary>
Motivation: 探讨测试计算量扩展对模型推理能力的影响，揭示潜在的问题模式。

Method: 构建四类评估任务（简单计数、回归、演绎和高级AI风险），分析不同推理长度下的模型表现。

Result: 发现五种失败模式，包括分心、过拟合、虚假关联、复杂任务注意力不集中以及行为放大。

Conclusion: 测试计算量扩展虽能提升模型能力，但可能强化问题推理模式，需多样化评估推理长度。

Abstract: We construct evaluation tasks where extending the reasoning length of Large
Reasoning Models (LRMs) deteriorates performance, exhibiting an inverse scaling
relationship between test-time compute and accuracy. Our evaluation tasks span
four categories: simple counting tasks with distractors, regression tasks with
spurious features, deduction tasks with constraint tracking, and advanced AI
risks. We identify five distinct failure modes when models reason for longer:
1) Claude models become increasingly distracted by irrelevant information; 2)
OpenAI o-series models resist distractors but overfit to problem framings; 3)
models shift from reasonable priors to spurious correlations; 4) all models
show difficulties in maintaining focus on complex deductive tasks; and 5)
extended reasoning may amplify concerning behaviors, with Claude Sonnet 4
showing increased expressions of self-preservation. These findings suggest that
while test-time compute scaling remains promising for improving model
capabilities, it may inadvertently reinforce problematic reasoning patterns.
Our results demonstrate the importance of evaluating models across diverse
reasoning lengths to identify and address these failure modes in LRMs.

</details>


### [78] [Routine: A Structural Planning Framework for LLM Agent System in Enterprise](https://arxiv.org/abs/2507.14447)
*Guancheng Zeng,Xueyi Chen,Jiawang Hu,Shaohua Qi,Yaxuan Mao,Zhantao Wang,Yifan Nie,Shuang Li,Qiuyang Feng,Pengxu Qiu,Yujia Wang,Wenqiang Han,Linyan Huang,Gang Li,Jingjing Mo,Haowen Hu*

Main category: cs.AI

TL;DR: 论文提出Routine框架，通过结构化规划和参数传递提升企业环境中多步骤工具调用任务的执行稳定性，显著提高模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决企业环境中代理系统因缺乏领域知识导致的计划混乱、工具缺失和执行不稳定问题。

Method: 引入Routine框架，包含清晰结构、明确指令和参数传递，并构建训练数据集进行模型微调。

Result: Routine显著提升模型性能，如GPT-4o准确率从41.1%升至96.3%，Qwen3-14B从32.6%升至83.3%，微调后进一步提升。

Conclusion: Routine有效提升代理系统在企业环境中的稳定性和适应性，加速AI流程技术的部署。

Abstract: The deployment of agent systems in an enterprise environment is often
hindered by several challenges: common models lack domain-specific process
knowledge, leading to disorganized plans, missing key tools, and poor execution
stability. To address this, this paper introduces Routine, a multi-step agent
planning framework designed with a clear structure, explicit instructions, and
seamless parameter passing to guide the agent's execution module in performing
multi-step tool-calling tasks with high stability. In evaluations conducted
within a real-world enterprise scenario, Routine significantly increases the
execution accuracy in model tool calls, increasing the performance of GPT-4o
from 41.1% to 96.3%, and Qwen3-14B from 32.6% to 83.3%. We further constructed
a Routine-following training dataset and fine-tuned Qwen3-14B, resulting in an
accuracy increase to 88.2% on scenario-specific evaluations, indicating
improved adherence to execution plans. In addition, we employed Routine-based
distillation to create a scenario-specific, multi-step tool-calling dataset.
Fine-tuning on this distilled dataset raised the model's accuracy to 95.5%,
approaching GPT-4o's performance. These results highlight Routine's
effectiveness in distilling domain-specific tool-usage patterns and enhancing
model adaptability to new scenarios. Our experimental results demonstrate that
Routine provides a practical and accessible approach to building stable agent
workflows, accelerating the deployment and adoption of agent systems in
enterprise environments, and advancing the technical vision of AI for Process.

</details>


### [79] [BioGraphFusion: Graph Knowledge Embedding for Biological Completion and Reasoning](https://arxiv.org/abs/2507.14468)
*Yitong Lin,Jiaying He,Jiahe Chen,Xinnan Zhu,Jianwei Zheng,Tao Bo*

Main category: cs.AI

TL;DR: BioGraphFusion框架通过深度融合语义和结构学习，显著提升了生物医学知识图谱的推理能力。


<details>
  <summary>Details</summary>
Motivation: 生物医学知识图谱的完成和推理具有挑战性，现有方法在语义理解和结构学习之间缺乏协同进化。

Method: BioGraphFusion通过张量分解建立全局语义基础，结合LSTM动态优化关系嵌入，并采用查询引导的子图构建和混合评分机制。

Result: 在三个生物医学任务中，BioGraphFusion表现优于现有方法，并通过案例研究验证了其生物学意义。

Conclusion: BioGraphFusion为生物医学知识图谱的语义和结构学习提供了高效协同解决方案。

Abstract: Motivation: Biomedical knowledge graphs (KGs) are crucial for drug discovery
and disease understanding, yet their completion and reasoning are challenging.
Knowledge Embedding (KE) methods capture global semantics but struggle with
dynamic structural integration, while Graph Neural Networks (GNNs) excel
locally but often lack semantic understanding. Even ensemble approaches,
including those leveraging language models, often fail to achieve a deep,
adaptive, and synergistic co-evolution between semantic comprehension and
structural learning. Addressing this critical gap in fostering continuous,
reciprocal refinement between these two aspects in complex biomedical KGs is
paramount.
  Results: We introduce BioGraphFusion, a novel framework for deeply
synergistic semantic and structural learning. BioGraphFusion establishes a
global semantic foundation via tensor decomposition, guiding an LSTM-driven
mechanism to dynamically refine relation embeddings during graph propagation.
This fosters adaptive interplay between semantic understanding and structural
learning, further enhanced by query-guided subgraph construction and a hybrid
scoring mechanism. Experiments across three key biomedical tasks demonstrate
BioGraphFusion's superior performance over state-of-the-art KE, GNN, and
ensemble models. A case study on Cutaneous Malignant Melanoma 1 (CMM1)
highlights its ability to unveil biologically meaningful pathways.
  Availability and Implementation: Source code and all training data are freely
available for download at https://github.com/Y-TARL/BioGraphFusion.
  Contact: zjw@zjut.edu.cn, botao666666@126.com.
  Supplementary information: Supplementary data are available at Bioinformatics
online.

</details>


### [80] [Amico: An Event-Driven Modular Framework for Persistent and Embedded Autonomy](https://arxiv.org/abs/2507.14513)
*Hongyi Yang,Yue Pan,Jiayi Xu,Kelsen Liu*

Main category: cs.AI

TL;DR: Amico是一个模块化、事件驱动的框架，专为嵌入式系统优化的自主代理设计，解决了现有框架在资源受限环境中的不足。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型和自主代理框架在动态环境和资源受限场景中表现不佳，依赖云端计算且缺乏持久自主性和环境感知能力。

Method: Amico采用Rust编写，支持通过WebAssembly在嵌入式平台和浏览器环境中高效运行，提供事件处理、状态管理和行为执行的抽象。

Result: Amico构建了一个统一的框架，支持在计算资源有限和连接不稳定的环境中部署弹性和交互式代理。

Conclusion: Amico为资源受限环境中的自主代理提供了一种高效、安全的解决方案。

Abstract: Recent advances in large language models (LLMs) and autonomous agents have
enabled systems capable of performing complex tasks across domains such as
human-computer interaction, planning, and web navigation. However, many
existing frameworks struggle in real-world or resource-constrained environments
due to their reliance on cloud-based computation, limited robustness in dynamic
contexts, and lack of persistent autonomy and environmental awareness.
  We present Amico, a modular, event-driven framework for building autonomous
agents optimized for embedded systems. Written in Rust for safety and
performance, Amico supports reactive, persistent agents that operate
efficiently across embedded platforms and browser environments via WebAssembly.
It provides clean abstractions for event handling, state management, behavior
execution, and integration with reasoning modules. Amico delivers a unified
infrastructure for constructing resilient, interactive agents suitable for
deployment in settings with limited compute and intermittent connectivity.

</details>


### [81] [What if Othello-Playing Language Models Could See?](https://arxiv.org/abs/2507.14520)
*Xinyi Chen,Yifei Yuan,Jiaang Li,Serge Belongie,Maarten de Rijke,Anders Søgaard*

Main category: cs.AI

TL;DR: 多模态训练（结合文本和视觉输入）在Othello游戏中提升模型性能和内部表示的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 探讨语言模型是否仅通过文本就能理解世界，或是否需要通过多模态（如视觉）进行更高效的学习。

Method: 引入VISOTHELLO模型，结合移动历史和棋盘图像进行多模态训练，并通过下一步移动预测与单模态基线对比。

Result: 多模态训练提升了模型性能和内部表示的鲁棒性。

Conclusion: 视觉输入有助于语言模型推断结构化世界表示。

Abstract: Language models are often said to face a symbol grounding problem. While some
argue that world understanding can emerge from text alone, others suggest
grounded learning is more efficient. We explore this through Othello, where the
board state defines a simplified, rule-based world. Building on prior work, we
introduce VISOTHELLO, a multi-modal model trained on move histories and board
images. Using next-move prediction, we compare it to mono-modal baselines and
test robustness to semantically irrelevant perturbations. We find that
multi-modal training improves both performance and the robustness of internal
representations. These results suggest that grounding language in visual input
helps models infer structured world representations.

</details>


### [82] [Large Language Models Assisting Ontology Evaluation](https://arxiv.org/abs/2507.14552)
*Anna Sofia Lippolis,Mohammad Javad Saeedizade,Robin Keskisärkkä,Aldo Gangemi,Eva Blomqvist,Andrea Giovanni Nuzzolese*

Main category: cs.AI

TL;DR: 论文介绍了OE-Assist框架，通过自动化和半自动化的CQ验证辅助本体评估，利用LLM技术提升效率。


<details>
  <summary>Details</summary>
Motivation: 传统本体评估方法（如CQ验证）成本高、劳动密集且易出错，需要更高效的解决方案。

Method: 提出OE-Assist框架，利用LLM技术自动和半自动化验证CQ，并基于1,393个CQ数据集进行实验。

Result: LLM-based方法（o1-preview和o3-mini）的评估效果与普通用户表现相当。

Conclusion: LLM技术可有效辅助本体评估，OE-Assist框架为自动化评估提供了新方向。

Abstract: Ontology evaluation through functional requirements, such as testing via
competency question (CQ) verification, is a well-established yet costly,
labour-intensive, and error-prone endeavour, even for ontology engineering
experts. In this work, we introduce OE-Assist, a novel framework designed to
assist ontology evaluation through automated and semi-automated CQ
verification. By presenting and leveraging a dataset of 1,393 CQs paired with
corresponding ontologies and ontology stories, our contributions present, to
our knowledge, the first systematic investigation into large language model
(LLM)-assisted ontology evaluation, and include: (i) evaluating the
effectiveness of a LLM-based approach for automatically performing CQ
verification against a manually created gold standard, and (ii) developing and
assessing an LLM-powered framework to assist CQ verification with Prot\'eg\'e,
by providing suggestions. We found that automated LLM-based evaluation with
o1-preview and o3-mini perform at a similar level to the average user's
performance.

</details>


### [83] [Coordinate Heart System: A Geometric Framework for Emotion Representation](https://arxiv.org/abs/2507.14593)
*Omar Al-Desi*

Main category: cs.AI

TL;DR: 提出了一种基于坐标系的情绪表示框架（CHS），通过八种核心情绪在单位圆上的坐标表示，实现复杂情绪状态的数学计算。


<details>
  <summary>Details</summary>
Motivation: 传统情绪模型在表示复杂情绪状态时存在不足，需要一种数学上更完备的框架。

Method: 将八种核心情绪定位为单位圆上的坐标，通过坐标混合和向量运算实现情绪计算，并引入稳定性参数S。

Result: 开发了八情绪系统，消除了表示盲点，并通过实验验证了其在复杂情绪场景中的有效性。

Conclusion: CHS为人工智能情绪建模提供了新的数学基础，能够更全面地表示和处理复杂情绪状态。

Abstract: This paper presents the Coordinate Heart System (CHS), a geometric framework
for emotion representation in artificial intelligence applications. We position
eight core emotions as coordinates on a unit circle, enabling mathematical
computation of complex emotional states through coordinate mixing and vector
operations. Our initial five-emotion model revealed significant coverage gaps
in the emotion space, leading to the development of an eight-emotion system
that provides complete geometric coverage with mathematical guarantees. The
framework converts natural language input to emotion coordinates and supports
real-time emotion interpolation through computational algorithms. The system
introduces a re-calibrated stability parameter S in [0,1], which dynamically
integrates emotional load, conflict resolution, and contextual drain factors.
This stability model leverages advanced Large Language Model interpretation of
textual cues and incorporates hybrid temporal tracking mechanisms to provide
nuanced assessment of psychological well-being states. Our key contributions
include: (i) mathematical proof demonstrating why five emotions are
insufficient for complete geometric coverage, (ii) an eight-coordinate system
that eliminates representational blind spots, (iii) novel algorithms for
emotion mixing, conflict resolution, and distance calculation in emotion space,
and (iv) a comprehensive computational framework for AI emotion recognition
with enhanced multi-dimensional stability modeling. Experimental validation
through case studies demonstrates the system's capability to handle emotionally
conflicted states, contextual distress factors, and complex psychological
scenarios that traditional categorical emotion models cannot adequately
represent. This work establishes a new mathematical foundation for emotion
modeling in artificial intelligence systems.

</details>


### [84] [Efficient Story Point Estimation With Comparative Learning](https://arxiv.org/abs/2507.14642)
*Monoshiz Mahbub Khan,Xioayin Xi,Andrew Meneely,Zhe Yu*

Main category: cs.AI

TL;DR: 提出一种基于比较学习的框架，用于校准项目特定的故事点预测模型，以减少敏捷开发中故事点估计的负担。


<details>
  <summary>Details</summary>
Motivation: 传统的故事点估计方法（如计划扑克）繁琐且耗时，机器学习虽能减轻负担，但现有模型需依赖同一项目的历史数据。本文旨在通过比较学习简化这一过程。

Method: 开发者通过比较任务对的努力程度，而非直接分配故事点，训练机器学习模型预测故事点。

Result: 在16个项目、23,313个手动估计数据上，模型预测与真实故事点的Spearman秩相关系数平均为0.34，性能与回归模型相当或更好。

Conclusion: 比较学习方法比回归方法更高效，且降低开发者的认知负担。

Abstract: Story point estimation is an essential part of agile software development.
Story points are unitless, project-specific effort estimates that help
developers plan their sprints. Traditionally, developers estimate story points
collaboratively using planning poker or other manual techniques. While the
initial calibrating of the estimates to each project is helpful, once a team
has converged on a set of precedents, story point estimation can become tedious
and labor-intensive. Machine learning can reduce this burden, but only with
enough context from the historical decisions made by the project team. That is,
state-of-the-art models, such as GPT2SP and FastText-SVM, only make accurate
predictions (within-project) when trained on data from the same project. The
goal of this work is to streamline story point estimation by evaluating a
comparative learning-based framework for calibrating project-specific story
point prediction models. Instead of assigning a specific story point value to
every backlog item, developers are presented with pairs of items, and indicate
which item requires more effort. Using these comparative judgments, a machine
learning model is trained to predict the story point estimates. We empirically
evaluated our technique using data with 23,313 manual estimates in 16 projects.
The model learned from comparative judgments can achieve on average 0.34
Spearman's rank correlation coefficient between its predictions and the ground
truth story points. This is similar to, if not better than, the performance of
a regression model learned from the ground truth story points. Therefore, the
proposed comparative learning approach is more efficient than state-of-the-art
regression-based approaches according to the law of comparative judgments -
providing comparative judgments yields a lower cognitive burden on humans than
providing ratings or categorical labels.

</details>


### [85] [When Autonomy Goes Rogue: Preparing for Risks of Multi-Agent Collusion in Social Systems](https://arxiv.org/abs/2507.14660)
*Qibing Ren,Sitao Xie,Longxuan Wei,Zhenfei Yin,Junchi Yan,Lizhuang Ma,Jing Shao*

Main category: cs.AI

TL;DR: 论文探讨了多智能体系统（MAS）在恶意协作中的风险，提出了一种模拟框架，并发现去中心化系统比中心化系统更具破坏性。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统的自主性增强，多智能体系统可能带来类似人类群体的危害，但目前研究主要集中在单智能体系统上，多智能体系统的风险尚未充分探索。

Method: 提出了一种灵活的框架，模拟恶意多智能体系统的协作风险，支持中心化和去中心化结构，并应用于虚假信息传播和电商欺诈两个高风险领域。

Result: 研究发现去中心化系统比中心化系统更擅长执行恶意行为，且能调整策略以规避传统干预措施（如内容标记）。

Conclusion: 研究揭示了恶意多智能体系统的运作方式，强调需要改进检测系统和应对措施。

Abstract: Recent large-scale events like election fraud and financial scams have shown
how harmful coordinated efforts by human groups can be. With the rise of
autonomous AI systems, there is growing concern that AI-driven groups could
also cause similar harm. While most AI safety research focuses on individual AI
systems, the risks posed by multi-agent systems (MAS) in complex real-world
situations are still underexplored. In this paper, we introduce a
proof-of-concept to simulate the risks of malicious MAS collusion, using a
flexible framework that supports both centralized and decentralized
coordination structures. We apply this framework to two high-risk fields:
misinformation spread and e-commerce fraud. Our findings show that
decentralized systems are more effective at carrying out malicious actions than
centralized ones. The increased autonomy of decentralized systems allows them
to adapt their strategies and cause more damage. Even when traditional
interventions, like content flagging, are applied, decentralized groups can
adjust their tactics to avoid detection. We present key insights into how these
malicious groups operate and the need for better detection systems and
countermeasures. Code is available at https://github.com/renqibing/RogueAgent.

</details>


### [86] [Configurable multi-agent framework for scalable and realistic testing of llm-based agents](https://arxiv.org/abs/2507.14705)
*Sai Wang,Senthilnathan Subramanian,Mudit Sahni,Praneeth Gone,Lingjie Meng,Xiaochen Wang,Nicolas Ferradas Bertoli,Tingxian Cheng,Jun Xu*

Main category: cs.AI

TL;DR: Neo是一个可配置的多代理框架，用于自动化评估基于LLM的系统，通过动态生成多样化测试用例，显著提高测试效率和覆盖率。


<details>
  <summary>Details</summary>
Motivation: 静态基准和手动测试无法满足LLM代理的复杂行为评估需求，因此需要一种自动化、动态的测试框架。

Method: Neo结合问题生成代理和评估代理，通过共享上下文中心模块化组合提示、场景控制和动态反馈，利用概率状态模型生成多样化测试输入。

Result: 在金融助手聊天机器人测试中，Neo发现边缘案例故障的效率接近人类专家，且吞吐量提高10-12倍。

Conclusion: Neo为可扩展、自进化的LLM质量评估奠定了基础，其框架具有模型无关性和可扩展性。

Abstract: Large-language-model (LLM) agents exhibit complex, context-sensitive
behaviour that quickly renders static benchmarks and ad-hoc manual testing
obsolete.
  We present Neo, a configurable, multi-agent framework that automates
realistic, multi-turn evaluation of LLM-based systems. Neo couples a Question
Generation Agent and an Evaluation Agent through a shared context-hub, allowing
domain prompts, scenario controls and dynamic feedback to be composed
modularly. Test inputs are sampled from a probabilistic state model spanning
dialogue flow, user intent and emotional tone, enabling diverse, human-like
conversations that adapt after every turn.
  Applied to a production-grade Seller Financial Assistant chatbot, Neo (i)
uncovered edge-case failures across five attack categories with a 3.3% break
rate close to the 5.8% achieved by expert human red-teamers, and (ii) delivered
10-12X higher throughput, generating 180 coherent test questions in around 45
mins versus 16h of human effort. Beyond security probing, Neo's stochastic
policies balanced topic coverage and conversational depth, yielding broader
behavioural exploration than manually crafted scripts.
  Neo therefore lays a foundation for scalable, self-evolving LLM QA: its agent
interfaces, state controller and feedback loops are model-agnostic and
extensible to richer factual-grounding and policy-compliance checks. We release
the framework to facilitate reproducible, high-fidelity testing of emerging
agentic systems.

</details>


### [87] [Automated Safety Evaluations Across 20 Large Language Models: The Aymara LLM Risk and Responsibility Matrix](https://arxiv.org/abs/2507.14719)
*Juan Manuel Contreras*

Main category: cs.AI

TL;DR: Aymara AI是一个用于生成和管理定制化、基于政策的安全评估的平台，通过将自然语言安全政策转化为对抗性提示，并使用AI评分器评估模型响应。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）在现实应用中的普及，可扩展且严格的安全评估变得至关重要。

Method: Aymara AI将自然语言安全政策转化为对抗性提示，并使用基于AI的评分器（经人类验证）对模型响应进行评分。

Result: 评估了20个商用LLM在10个安全领域的表现，结果显示性能差异显著（平均安全分数52.4%至86.2%），复杂领域表现较差（如隐私与冒充领域平均24.3%）。

Conclusion: LLM安全性具有不一致性和上下文依赖性，需要像Aymara AI这样的可扩展、定制化工具来支持负责任的AI开发和监管。

Abstract: As large language models (LLMs) become increasingly integrated into
real-world applications, scalable and rigorous safety evaluation is essential.
This paper introduces Aymara AI, a programmatic platform for generating and
administering customized, policy-grounded safety evaluations. Aymara AI
transforms natural-language safety policies into adversarial prompts and scores
model responses using an AI-based rater validated against human judgments. We
demonstrate its capabilities through the Aymara LLM Risk and Responsibility
Matrix, which evaluates 20 commercially available LLMs across 10 real-world
safety domains. Results reveal wide performance disparities, with mean safety
scores ranging from 86.2% to 52.4%. While models performed well in
well-established safety domains such as Misinformation (mean = 95.7%), they
consistently failed in more complex or underspecified domains, notably Privacy
& Impersonation (mean = 24.3%). Analyses of Variance confirmed that safety
scores differed significantly across both models and domains (p < .05). These
findings underscore the inconsistent and context-dependent nature of LLM safety
and highlight the need for scalable, customizable tools like Aymara AI to
support responsible AI development and oversight.

</details>


### [88] [Towards AI Urban Planner in the Age of GenAI, LLMs, and Agentic AI](https://arxiv.org/abs/2507.14730)
*Yanjie Fu*

Main category: cs.AI

TL;DR: 本文探讨了生成式AI与城市规划的结合，提出将城市规划视为生成式AI任务，并指出当前研究的四大局限及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 探索生成式AI、大语言模型和代理AI如何与城市规划结合，以推动AI城市规划师的发展。

Method: 通过调查生成式AI方法（如VAEs、GANs、transformers和扩散模型）在城市设计中的应用，分析现有研究的局限性。

Result: 发现四大研究空白：缺乏城市理论指导、多空间分辨率研究不足、数据驱动的城市设计知识不足、忽视现实世界交互。

Conclusion: 提出未来研究方向，包括理论引导生成、数字孪生和人机协同设计，呼吁生成式智能与参与式城市主义的新结合。

Abstract: Generative AI, large language models, and agentic AI have emerged separately
of urban planning. However, the convergence between AI and urban planning
presents an interesting opportunity towards AI urban planners. This paper
conceptualizes urban planning as a generative AI task, where AI synthesizes
land-use configurations under geospatial, social, and human-centric
constraints. We survey how generative AI approaches, including VAEs, GANs,
transformers, and diffusion models, reshape urban design. We further identify
critical gaps: 1) limited research on integrating urban theory guidance, 2)
limited research of AI urban planning over multiple spatial resolutions or
angularities, 3) limited research on augmenting urban design knowledge from
data, and 4) limited research on addressing real-world interactions. To address
these limitations, we outline future research directions in theory-guided
generation, digital twins, and human-machine co-design, calling for a new
synthesis of generative intelligence and participatory urbanism.

</details>


### [89] [AgentFly: Extensible and Scalable Reinforcement Learning for LM Agents](https://arxiv.org/abs/2507.14897)
*Renxi Wang,Rifo Ahmad Genadi,Bilal El Bouardi,Yongxin Wang,Fajri Koto,Zhengzhong Liu,Timothy Baldwin,Haonan Li*

Main category: cs.AI

TL;DR: AgentFly是一个结合语言模型（LM）和强化学习（RL）的可扩展框架，旨在通过RL算法增强LM代理的能力。


<details>
  <summary>Details</summary>
Motivation: 当前LM代理主要通过提示工程或监督微调构建，而RL与LM代理的结合（Agent-RL）缺乏系统性研究。

Method: 开发了AgentFly框架，支持多轮交互、工具定义和奖励函数，并实现异步执行和资源管理。

Result: 框架在多任务中成功训练代理，展示了其有效性。

Conclusion: AgentFly为Agent-RL研究提供了系统化工具和基础。

Abstract: Language model (LM) agents have gained significant attention for their
ability to autonomously complete tasks through interactions with environments,
tools, and APIs. LM agents are primarily built with prompt engineering or
supervised finetuning. At the same time, reinforcement learning (RL) has been
explored to enhance LM's capabilities, such as reasoning and factuality.
However, the combination of the LM agents and reinforcement learning (Agent-RL)
remains underexplored and lacks systematic study. To this end, we built
AgentFly, a scalable and extensible Agent-RL framework designed to empower LM
agents with a variety of RL algorithms. Our framework supports multi-turn
interactions by adapting traditional RL methods with token-level masking. It
features a decorator-based interface for defining tools and reward functions,
enabling seamless extension and ease of use. To support high-throughput
training, we implement asynchronous execution of tool calls and reward
computations, and design a centralized resource management system for scalable
environment coordination. We also provide a suite of prebuilt tools and
environments, demonstrating the framework's effectiveness through successful
agent training across multiple tasks.

</details>


### [90] [InsightX Agent: An LMM-based Agentic Framework with Integrated Tools for Reliable X-ray NDT Analysis](https://arxiv.org/abs/2507.14899)
*Jiale Liu,Huan Wang,Yue Zhang,Xiaoyu Luo,Jiaxiang Hu,Zhiliang Liu,Min Xie*

Main category: cs.AI

TL;DR: 论文提出了一种基于LMM的交互式、可解释的X射线无损检测框架InsightX Agent，结合SDMSD和EGR工具，显著提升了检测可靠性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在X射线无损检测中缺乏交互性、可解释性和自我评估能力，限制了其可靠性和操作员信任。

Method: InsightX Agent以LMM为核心协调SDMSD和EGR工具，SDMSD用于多尺度缺陷检测，EGR通过链式思维验证和优化结果。

Result: 在GDXray+数据集上，InsightX Agent实现了96.35%的F1分数，同时显著提升了分析的可解释性和可信度。

Conclusion: InsightX Agent展示了基于LMM的代理框架在工业检测任务中的变革潜力。

Abstract: Non-destructive testing (NDT), particularly X-ray inspection, is vital for
industrial quality assurance, yet existing deep-learning-based approaches often
lack interactivity, interpretability, and the capacity for critical
self-assessment, limiting their reliability and operator trust. To address
these shortcomings, this paper proposes InsightX Agent, a novel LMM-based
agentic framework designed to deliver reliable, interpretable, and interactive
X-ray NDT analysis. Unlike typical sequential pipelines, InsightX Agent
positions a Large Multimodal Model (LMM) as a central orchestrator,
coordinating between the Sparse Deformable Multi-Scale Detector (SDMSD) and the
Evidence-Grounded Reflection (EGR) tool. The SDMSD generates dense defect
region proposals for multi-scale feature maps and sparsifies them through
Non-Maximum Suppression (NMS), optimizing detection of small, dense targets in
X-ray images while maintaining computational efficiency. The EGR tool guides
the LMM agent through a chain-of-thought-inspired review process, incorporating
context assessment, individual defect analysis, false positive elimination,
confidence recalibration and quality assurance to validate and refine the
SDMSD's initial proposals. By strategically employing and intelligently using
tools, InsightX Agent moves beyond passive data processing to active reasoning,
enhancing diagnostic reliability and providing interpretations that integrate
diverse information sources. Experimental evaluations on the GDXray+ dataset
demonstrate that InsightX Agent not only achieves a high object detection
F1-score of 96.35% but also offers significantly improved interpretability and
trustworthiness in its analyses, highlighting the transformative potential of
agentic LLM frameworks for industrial inspection tasks.

</details>


### [91] [Feedback-Induced Performance Decline in LLM-Based Decision-Making](https://arxiv.org/abs/2507.14906)
*Xiao Yang,Juxi Leitner,Michael Burke*

Main category: cs.AI

TL;DR: 研究探讨了大型语言模型（LLMs）在马尔可夫决策过程中的表现，发现其在简单环境中表现优异，但在复杂场景中需进一步优化。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在自主决策中的适用性，尤其是其基于预训练知识的快速适应能力。

Method: 通过在线结构化提示策略，比较LLM与传统强化学习（RL）方法在序列决策任务中的零样本性能。

Result: LLMs在简单环境中初始表现更好，但在复杂场景中规划和推理能力不足；反馈机制可能降低性能。

Conclusion: 需进一步研究混合策略、微调和高级记忆整合以提升LLM的决策能力。

Abstract: The ability of Large Language Models (LLMs) to extract context from natural
language problem descriptions naturally raises questions about their
suitability in autonomous decision-making settings. This paper studies the
behaviour of these models within a Markov Decision Process (MDPs). While
traditional reinforcement learning (RL) strategies commonly employed in this
setting rely on iterative exploration, LLMs, pre-trained on diverse datasets,
offer the capability to leverage prior knowledge for faster adaptation. We
investigate online structured prompting strategies in sequential decision
making tasks, comparing the zero-shot performance of LLM-based approaches to
that of classical RL methods. Our findings reveal that although LLMs
demonstrate improved initial performance in simpler environments, they struggle
with planning and reasoning in complex scenarios without fine-tuning or
additional guidance. Our results show that feedback mechanisms, intended to
improve decision-making, often introduce confusion, leading to diminished
performance in intricate environments. These insights underscore the need for
further exploration into hybrid strategies, fine-tuning, and advanced memory
integration to enhance LLM-based decision-making capabilities.

</details>


### [92] [The Endless Tuning. An Artificial Intelligence Design To Avoid Human Replacement and Trace Back Responsibilities](https://arxiv.org/abs/2507.14909)
*Elio Grande*

Main category: cs.AI

TL;DR: 《Endless Tuning》提出了一种基于双重镜像过程的人工智能可靠部署方法，旨在避免人类被替代并填补责任缺口。该方法通过三个原型应用测试，重点关注用户体验而非统计准确性。


<details>
  <summary>Details</summary>
Motivation: 解决人工智能部署中的人类替代问题和责任缺口（Matthias 2004），并探索伦理与技术选择的结合。

Method: 采用双重镜像过程，开发协议并在贷款审批、肺炎诊断和艺术风格识别三个领域进行原型测试。

Result: 实验表明，用户在使用深度学习模型时能感知到完全控制，同时能在责任与问责之间建立桥梁。

Conclusion: 该方法为人工智能伦理提供了一种新视角，强调用户体验和可控性，为责任问题提供了解决方案。

Abstract: The Endless Tuning is a design method for a reliable deployment of artificial
intelligence based on a double mirroring process, which pursues both the goals
of avoiding human replacement and filling the so-called responsibility gap
(Matthias 2004). Originally depicted in (Fabris et al. 2024) and ensuing the
relational approach urged therein, it was then actualized in a protocol,
implemented in three prototypical applications regarding decision-making
processes (respectively: loan granting, pneumonia diagnosis, and art style
recognition) and tested with such as many domain experts. Step by step
illustrating the protocol, giving insights concretely showing a different voice
(Gilligan 1993) in the ethics of artificial intelligence, a philosophical
account of technical choices (e.g., a reversed and hermeneutic deployment of
XAI algorithms) will be provided in the present study together with the results
of the experiments, focusing on user experience rather than statistical
accuracy. Even thoroughly employing deep learning models, full control was
perceived by the interviewees in the decision-making setting, while it appeared
that a bridge can be built between accountability and liability in case of
damage.

</details>


### [93] [Redefining Elderly Care with Agentic AI: Challenges and Opportunities](https://arxiv.org/abs/2507.14912)
*Ruhul Amin Khalil,Kashif Ahmad,Hazrat Ali*

Main category: cs.AI

TL;DR: 本文探讨了基于大型语言模型（LLM）的代理人工智能（Agentic AI）在老年护理中的潜力与挑战，强调个性化健康跟踪、认知护理和环境管理，同时提出数据隐私、安全性和伦理问题。


<details>
  <summary>Details</summary>
Motivation: 全球老龄化人口需要创新的老年护理策略，代理AI因其主动性和自主决策能力成为潜在解决方案。

Method: 通过分析代理AI在老年护理中的应用，探讨其独特能力、局限性和伦理问题。

Result: 代理AI有望显著改善老年护理，但也需解决隐私、安全和伦理问题。

Conclusion: 需进一步研究以实现以人为本的代理AI集成，并填补现有文献空白。

Abstract: The global ageing population necessitates new and emerging strategies for
caring for older adults. In this article, we explore the potential for
transformation in elderly care through Agentic Artificial Intelligence (AI),
powered by Large Language Models (LLMs). We discuss the proactive and
autonomous decision-making facilitated by Agentic AI in elderly care.
Personalized tracking of health, cognitive care, and environmental management,
all aimed at enhancing independence and high-level living for older adults,
represents important areas of application. With a potential for significant
transformation of elderly care, Agentic AI also raises profound concerns about
data privacy and security, decision independence, and access. We share key
insights to emphasize the need for ethical safeguards, privacy protections, and
transparent decision-making. Our goal in this article is to provide a balanced
discussion of both the potential and the challenges associated with Agentic AI,
and to provide insights into its responsible use in elderly care, to bring
Agentic AI into harmony with the requirements and vulnerabilities specific to
the elderly. Finally, we identify the priorities for the academic research
communities, to achieve human-centered advancements and integration of Agentic
AI in elderly care. To the best of our knowledge, this is no existing study
that reviews the role of Agentic AI in elderly care. Hence, we address the
literature gap by analyzing the unique capabilities, applications, and
limitations of LLM-based Agentic AI in elderly care. We also provide a
companion interactive dashboard at https://hazratali.github.io/agenticai/.

</details>


### [94] [Complexity of Faceted Explanations in Propositional Abduction](https://arxiv.org/abs/2507.14962)
*Johannes Schmidt,Mohamed Maizia,Victor Lagerkvist,Johannes K. Fichte*

Main category: cs.AI

TL;DR: 论文研究了命题溯因中的细粒度推理，引入了“facet”概念以区分解释中的相关性和可弃性，并分析了不同设置下的facet特性。


<details>
  <summary>Details</summary>
Motivation: 旨在通过更细粒度的推理方法（如facet和解释间距离）来更好地理解命题溯因中的解释变异性，同时保持计算复杂性可控。

Method: 引入facet概念（部分解释中出现但不全出现的文字），并分析其在Post框架等多种设置下的特性。

Result: 提供了对facet在命题溯因中的全面分析，包括在Post框架中的几乎完整分类。

Conclusion: 通过facet和解释间距离，能够更精细地理解解释的异质性，同时保持计算复杂性在合理范围内。

Abstract: Abductive reasoning is a popular non-monotonic paradigm that aims to explain
observed symptoms and manifestations. It has many applications, such as
diagnosis and planning in artificial intelligence and database updates. In
propositional abduction, we focus on specifying knowledge by a propositional
formula. The computational complexity of tasks in propositional abduction has
been systematically characterized - even with detailed classifications for
Boolean fragments. Unsurprisingly, the most insightful reasoning problems
(counting and enumeration) are computationally highly challenging. Therefore,
we consider reasoning between decisions and counting, allowing us to understand
explanations better while maintaining favorable complexity. We introduce facets
to propositional abductions, which are literals that occur in some explanation
(relevant) but not all explanations (dispensable). Reasoning with facets
provides a more fine-grained understanding of variability in explanations
(heterogeneous). In addition, we consider the distance between two
explanations, enabling a better understanding of heterogeneity/homogeneity. We
comprehensively analyze facets of propositional abduction in various settings,
including an almost complete characterization in Post's framework.

</details>


### [95] [AlphaAlign: Incentivizing Safety Alignment with Extremely Simplified Reinforcement Learning](https://arxiv.org/abs/2507.14987)
*Yi Zhang,An Zhang,XiuYu Zhang,Leheng Sheng,Yuxin Chen,Zhenkai Liang,Xiang Wang*

Main category: cs.AI

TL;DR: AlphaAlign是一个基于纯强化学习的框架，通过可验证的安全奖励激发大语言模型的内在安全自我意识，解决有害内容生成和过度拒绝问题。


<details>
  <summary>Details</summary>
Motivation: 当前的安全对齐方法存在浅层拒绝或依赖密集监督的问题，未能充分利用模型的内在安全自我意识。

Method: AlphaAlign采用双奖励系统：可验证的安全奖励鼓励对有害查询的正确拒绝，并惩罚过度拒绝；标准化帮助性奖励指导对良性输入的高质量响应。

Result: AlphaAlign在简化性、效率、安全性与实用性平衡以及深度对齐方面表现出显著优势。

Conclusion: AlphaAlign通过主动安全推理，有效提升模型的安全性和实用性，无需依赖监督数据。

Abstract: Large language models (LLMs), despite possessing latent safety understanding
from their vast pretraining data, remain vulnerable to generating harmful
content and exhibit issues such as over-refusal and utility degradation after
safety alignment. Current safety alignment methods often result in superficial
refusal shortcuts or rely on intensive supervision for reasoning-based
approaches, failing to fully leverage the model's intrinsic safety
self-awareness. We propose \textbf{AlphaAlign}, a simple yet effective pure
reinforcement learning (RL) framework with verifiable safety reward designed to
incentivize this latent safety awareness through proactive safety reasoning.}
AlphaAlign employs a dual-reward system: a verifiable safety reward encourages
correctly formatted and explicitly justified refusals for harmful queries while
penalizing over-refusals, and a normalized helpfulness reward guides
high-quality responses to benign inputs. This allows the model to develop
proactive safety reasoning capabilities without depending on supervised
safety-specific reasoning data. AlphaAlign demonstrates three key advantages:
(1) Simplicity and efficiency, requiring only binary prompt safety labels and
minimal RL steps for substantial improvements. (2) Breaking the safety-utility
trade-off, by enhancing refusal of harmful content and reducing over-refusals,
while simultaneously maintaining or even improving general task performance and
robustness to unseen jailbreaks. (3) Deep alignment, fostering proactive safety
reasoning that generates explicit safety rationales rather than relying on
shallow refusal patterns.

</details>


### [96] [A Forced-Choice Neural Cognitive Diagnostic Model of Personality Testing](https://arxiv.org/abs/2507.15013)
*Xiaoyu Li,Jin Wu,Shaoyang Guo,Haoran Shi,Chanjin Zheng*

Main category: cs.AI

TL;DR: 提出了一种基于深度学习的强制选择神经认知诊断模型（FCNCD），用于解决传统模型的局限性，适用于三种常见题型，并通过实验验证了其准确性、可解释性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在智能时代，心理测量测试在人员选拔、职业发展和心理健康评估中日益重要，而强制选择测试因其能降低回答失真的风险而被广泛使用。

Method: 通过非线性映射挖掘参与者和项目特征，使用多层神经网络建模其交互，并利用单调性假设提升诊断结果的可解释性。

Result: 在真实和模拟数据集上的实验验证了FCNCD的准确性、可解释性和鲁棒性。

Conclusion: FCNCD模型有效克服了传统模型的限制，适用于多种强制选择测试题型，具有实际应用价值。

Abstract: In the smart era, psychometric tests are becoming increasingly important for
personnel selection, career development, and mental health assessment.
Forced-choice tests are common in personality assessments because they require
participants to select from closely related options, lowering the risk of
response distortion. This study presents a deep learning-based Forced-Choice
Neural Cognitive Diagnostic Model (FCNCD) that overcomes the limitations of
traditional models and is applicable to the three most common item block types
found in forced-choice tests. To account for the unidimensionality of items in
forced-choice tests, we create interpretable participant and item parameters.
We model the interactions between participant and item features using
multilayer neural networks after mining them using nonlinear mapping. In
addition, we use the monotonicity assumption to improve the interpretability of
the diagnostic results. The FCNCD's effectiveness is validated by experiments
on real-world and simulated datasets that show its accuracy, interpretability,
and robustness.

</details>


### [97] [DeRAG: Black-box Adversarial Attacks on Multiple Retrieval-Augmented Generation Applications via Prompt Injection](https://arxiv.org/abs/2507.15042)
*Jerry Wang,Fang Yu*

Main category: cs.AI

TL;DR: 本文提出了一种基于差分进化（DE）的方法，优化对抗性提示后缀以攻击RAG系统，实验表明其攻击成功率优于现有方法，且能逃避检测。


<details>
  <summary>Details</summary>
Motivation: 对抗性提示攻击会显著影响RAG系统的可靠性，本文旨在提出一种更有效的攻击方法。

Method: 采用差分进化（DE）优化对抗性提示后缀，将其视为黑盒问题，通过进化候选后缀来最大化错误文档的检索排名。

Result: 在BEIR QA数据集上的实验显示，DE方法攻击成功率高，且仅需少量标记（≤5），同时能逃避BERT检测器的检测。

Conclusion: DE方法在攻击RAG系统时表现优异，且对抗性后缀具有可读性和隐蔽性。

Abstract: Adversarial prompt attacks can significantly alter the reliability of
Retrieval-Augmented Generation (RAG) systems by re-ranking them to produce
incorrect outputs. In this paper, we present a novel method that applies
Differential Evolution (DE) to optimize adversarial prompt suffixes for
RAG-based question answering. Our approach is gradient-free, treating the RAG
pipeline as a black box and evolving a population of candidate suffixes to
maximize the retrieval rank of a targeted incorrect document to be closer to
real world scenarios. We conducted experiments on the BEIR QA datasets to
evaluate attack success at certain retrieval rank thresholds under multiple
retrieving applications. Our results demonstrate that DE-based prompt
optimization attains competitive (and in some cases higher) success rates
compared to GGPP to dense retrievers and PRADA to sparse retrievers, while
using only a small number of tokens (<=5 tokens) in the adversarial suffix.
Furthermore, we introduce a readability-aware suffix construction strategy,
validated by a statistically significant reduction in MLM negative
log-likelihood with Welch's t-test. Through evaluations with a BERT-based
adversarial suffix detector, we show that DE-generated suffixes evade
detection, yielding near-chance detection accuracy.

</details>


### [98] [From Kicking to Causality: Simulating Infant Agency Detection with a Robust Intrinsic Reward](https://arxiv.org/abs/2507.15106)
*Xia Xu,Jochen Triesch*

Main category: cs.AI

TL;DR: 论文提出了一种基于因果推理的Causal Action Influence Score (CAIS)作为内在奖励，用于强化学习，以解决传统相关性奖励在噪声环境中的脆弱性问题。


<details>
  <summary>Details</summary>
Motivation: 人类婴儿能有效发现自身因果效应，而传统强化学习代理在噪声环境中依赖相关性奖励表现脆弱，因此需要一种更鲁棒的奖励机制。

Method: CAIS通过计算动作条件感官结果分布与基线分布的1-Wasserstein距离，量化动作的因果影响，过滤环境噪声。

Result: 在模拟婴儿-移动环境中，CAIS成功过滤噪声并学习正确策略，同时能复现“消退爆发”现象。

Conclusion: 显式推断因果关系是发展鲁棒代理感的关键机制，为自适应自主系统提供了心理学合理的框架。

Abstract: While human infants robustly discover their own causal efficacy, standard
reinforcement learning agents remain brittle, as their reliance on
correlation-based rewards fails in noisy, ecologically valid scenarios. To
address this, we introduce the Causal Action Influence Score (CAIS), a novel
intrinsic reward rooted in causal inference. CAIS quantifies an action's
influence by measuring the 1-Wasserstein distance between the learned
distribution of sensory outcomes conditional on that action, $p(h|a)$, and the
baseline outcome distribution, $p(h)$. This divergence provides a robust reward
that isolates the agent's causal impact from confounding environmental noise.
We test our approach in a simulated infant-mobile environment where
correlation-based perceptual rewards fail completely when the mobile is
subjected to external forces. In stark contrast, CAIS enables the agent to
filter this noise, identify its influence, and learn the correct policy.
Furthermore, the high-quality predictive model learned for CAIS allows our
agent, when augmented with a surprise signal, to successfully reproduce the
"extinction burst" phenomenon. We conclude that explicitly inferring causality
is a crucial mechanism for developing a robust sense of agency, offering a
psychologically plausible framework for more adaptive autonomous systems.

</details>


### [99] [Automated planning with ontologies under coherence update semantics](https://arxiv.org/abs/2507.15120)
*Stefan Borgwardt,Duy Nhu,Gabriele Röger*

Main category: cs.AI

TL;DR: 论文提出了一种结合DL-Lite本体和自动化规划的新方法，通过显式输入知识和动作基础（eKABs）实现，并展示了其复杂度不高于现有方法。


<details>
  <summary>Details</summary>
Motivation: 将背景知识（如本体）融入自动化规划问题，以提升规划能力。

Method: 结合DL-Lite本体和eKABs，采用一致性更新语义处理动作效果，并通过多项式编译转换为经典规划问题。

Result: 复杂度与现有方法相当，并通过实验验证了编译变体的性能。

Conclusion: 新方法在保持低复杂度的同时，有效整合了本体知识，提升了规划能力。

Abstract: Standard automated planning employs first-order formulas under closed-world
semantics to achieve a goal with a given set of actions from an initial state.
We follow a line of research that aims to incorporate background knowledge into
automated planning problems, for example, by means of ontologies, which are
usually interpreted under open-world semantics. We present a new approach for
planning with DL-Lite ontologies that combines the advantages of ontology-based
action conditions provided by explicit-input knowledge and action bases (eKABs)
and ontology-aware action effects under the coherence update semantics. We show
that the complexity of the resulting formalism is not higher than that of
previous approaches and provide an implementation via a polynomial compilation
into classical planning. An evaluation of existing and new benchmarks examines
the performance of a planning system on different variants of our compilation.

</details>


### [100] [Clinical Semantic Intelligence (CSI): Emulating the Cognitive Framework of the Expert Clinician for Comprehensive Oral Disease Diagnosis](https://arxiv.org/abs/2507.15140)
*Mohammad Mashayekhi,Sara Ahmadi Majd,Arian AmirAmjadi,Parsa Hosseini*

Main category: cs.AI

TL;DR: 开发了一种名为CSI的人工智能框架，通过模拟专家临床医生的认知过程来诊断118种口腔疾病，显著提高了诊断准确性。


<details>
  <summary>Details</summary>
Motivation: 口腔疾病诊断因症状重叠而具有挑战性，需要超越简单模式匹配的专家推理方法。

Method: 结合多模态CLIP模型和ChatGLM-6B语言模型，采用分层诊断推理树（HDRT）进行快速和标准模式诊断。

Result: 在431张图像的测试集上，快速模式准确率为73.4%，标准模式提升至89.5%。

Conclusion: CSI通过分层推理显著提升诊断性能，为临床诊断提供了实用工具。

Abstract: The diagnosis of oral diseases presents a problematic clinical challenge,
characterized by a wide spectrum of pathologies with overlapping
symptomatology. To address this, we developed Clinical Semantic Intelligence
(CSI), a novel artificial intelligence framework that diagnoses 118 different
oral diseases by computationally modeling the cognitive processes of an expert
clinician. Our core hypothesis is that moving beyond simple pattern matching to
emulate expert reasoning is critical to building clinically useful diagnostic
aids.
  CSI's architecture integrates a fine-tuned multimodal CLIP model with a
specialized ChatGLM-6B language model. This system executes a Hierarchical
Diagnostic Reasoning Tree (HDRT), a structured framework that distills the
systematic, multi-step logic of differential diagnosis. The framework operates
in two modes: a Fast Mode for rapid screening and a Standard Mode that
leverages the full HDRT for an interactive and in-depth diagnostic workup.
  To train and validate our system, we curated a primary dataset of 4,310
images, supplemented by an external hold-out set of 176 images for final
validation. A clinically-informed augmentation strategy expanded our training
data to over 30,000 image-text pairs. On a 431-image internal test set, CSI's
Fast Mode achieved an accuracy of 73.4%, which increased to 89.5% with the
HDRT-driven Standard Mode. The performance gain is directly attributable to the
hierarchical reasoning process. Herein, we detail the architectural philosophy,
development, and rigorous evaluation of the CSI framework.

</details>


### [101] [Can We Move Freely in NEOM's The Line? An Agent-Based Simulation of Human Mobility in a Futuristic Smart City](https://arxiv.org/abs/2507.15143)
*Abderaouf Bahi,Amel Ourici*

Main category: cs.AI

TL;DR: 论文研究了沙特NEOM线性智能城市The Line中人类移动的可行性，通过混合仿真框架验证了AI支持的自由移动可能性。


<details>
  <summary>Details</summary>
Motivation: 评估在The Line这种前所未有的线性城市拓扑中，居民是否能自由移动。

Method: 开发了结合基于代理的建模、强化学习、监督学习和图神经网络的混合仿真框架，模拟多模式交通行为。

Result: 实验显示，AI集成架构下，平均通勤时间为7.8至8.4分钟，满意度超89%，可达性指数超91%。

Conclusion: The Line中的自由移动在AI系统、可持续基础设施和实时反馈支持下是可行的。

Abstract: This paper investigates the feasibility of human mobility in The Line, a
proposed 170-kilometer linear smart city in NEOM, Saudi Arabia. To assess
whether citizens can move freely within this unprecedented urban topology, we
develop a hybrid simulation framework that integrates agent-based modeling,
reinforcement learning, supervised learning, and graph neural networks. The
simulation captures multi-modal transportation behaviors across 50 vertical
levels and varying density scenarios using both synthetic data and real-world
traces from high-density cities. Our experiments reveal that with the full
AI-integrated architecture, agents achieved an average commute time of 7.8 to
8.4 minutes, a satisfaction rate exceeding 89 percent, and a reachability index
of over 91 percent, even during peak congestion periods. Ablation studies
confirmed that the removal of intelligent modules such as reinforcement
learning or graph neural networks significantly degrades performance, with
commute times increasing by up to 85 percent and reachability falling below 70
percent. Environmental modeling further demonstrated low energy consumption and
minimal CO2 emissions when electric modes are prioritized. The findings suggest
that freedom of movement is not only conceptually achievable in The Line, but
also operationally realistic if supported by adaptive AI systems, sustainable
infrastructure, and real-time feedback loops.

</details>


### [102] [Solving Formal Math Problems by Decomposition and Iterative Reflection](https://arxiv.org/abs/2507.15225)
*Yichi Zhou,Jianqiu Zhao,Yongxin Zhang,Bohan Wang,Siran Wang,Luoxin Chen,Jiahui Wang,Haowei Chen,Allan Jie,Xinbo Zhang,Haocheng Wang,Luong Trung,Rong Ye,Phan Nhat Hoang,Huishuai Zhang,Peng Sun,Hang Li*

Main category: cs.AI

TL;DR: Delta Prover是一个基于代理的框架，利用通用大语言模型（LLM）与Lean 4证明环境交互，无需模型专业化即可高效生成形式化证明。


<details>
  <summary>Details</summary>
Motivation: 通用LLM在形式化证明（如Lean 4）中表现不佳，现有方法需高成本的专业化模型。Delta Prover旨在通过代理框架释放通用LLM的潜力。

Method: 结合反射分解、迭代证明修复算法和基于Lean 4的领域特定语言（DSL），实现交互式证明构建。

Result: 在miniF2F-test基准测试中达到95.9%的成功率，超越所有现有方法，包括需模型专业化的方法。

Conclusion: 通用LLM在有效代理结构引导下具备强大定理证明能力，为形式化环境中的自动推理提供高效替代方案。

Abstract: General-purpose Large Language Models (LLMs) have achieved remarkable success
in intelligence, performing comparably to human experts on complex reasoning
tasks such as coding and mathematical reasoning. However, generating formal
proofs in specialized languages like Lean 4 remains a significant challenge for
these models, limiting their application in complex theorem proving and
automated verification. Current approaches typically require specializing
models through fine-tuning on dedicated formal corpora, incurring high costs
for data collection and training. In this work, we introduce \textbf{Delta
Prover}, an agent-based framework that orchestrates the interaction between a
general-purpose LLM and the Lean 4 proof environment. Delta Prover leverages
the reflection and reasoning capabilities of general-purpose LLMs to
interactively construct formal proofs in Lean 4, circumventing the need for
model specialization. At its core, the agent integrates two novel,
interdependent components: an algorithmic framework for reflective
decomposition and iterative proof repair, and a custom Domain-Specific Language
(DSL) built upon Lean 4 for streamlined subproblem management. \textbf{Delta
Prover achieves a state-of-the-art 95.9\% success rate on the miniF2F-test
benchmark, surpassing all existing approaches, including those requiring model
specialization.} Furthermore, Delta Prover exhibits a significantly stronger
test-time scaling law compared to standard Best-of-N proof strategies.
Crucially, our findings demonstrate that general-purpose LLMs, when guided by
an effective agentic structure, possess substantial untapped theorem-proving
capabilities. This presents a computationally efficient alternative to
specialized models for robust automated reasoning in formal environments.

</details>


### [103] [Explainable Artificial Intelligence based Soft Evaluation Indicator for Arc Fault Diagnosis](https://arxiv.org/abs/2507.15239)
*Qianchao Wang,Yuxuan Ding,Chuanzhen Jia,Zhe Li,Yaping Du*

Main category: cs.AI

TL;DR: 论文提出了一种软评估指标和轻量级平衡神经网络，以提高电弧故障诊断模型的可解释性和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有AI电弧故障诊断模型虽准确率高，但其可信度存疑，需增强模型的可解释性和信任度。

Method: 结合可解释人工智能和真实电弧故障实验，定义电弧故障的正确解释，并提出轻量级平衡神经网络。

Result: 实验表明，软评估指标和轻量级网络在多种数据集上表现优异，提升了模型的可理解性和可信度。

Conclusion: 该方法使电弧故障诊断模型更易理解和信任，支持从业者做出可靠决策。

Abstract: Novel AI-based arc fault diagnosis models have demonstrated outstanding
performance in terms of classification accuracy. However, an inherent problem
is whether these models can actually be trusted to find arc faults. In this
light, this work proposes a soft evaluation indicator that explains the outputs
of arc fault diagnosis models, by defining the the correct explanation of arc
faults and leveraging Explainable Artificial Intelligence and real arc fault
experiments. Meanwhile, a lightweight balanced neural network is proposed to
guarantee competitive accuracy and soft feature extraction score. In our
experiments, several traditional machine learning methods and deep learning
methods across two arc fault datasets with different sample times and noise
levels are utilized to test the effectiveness of the soft evaluation indicator.
Through this approach, the arc fault diagnosis models are easy to understand
and trust, allowing practitioners to make informed and trustworthy decisions.

</details>


### [104] [Disentangling Homophily and Heterophily in Multimodal Graph Clustering](https://arxiv.org/abs/2507.15253)
*Zhaochen Guo,Zhixiang Shen,Xuanting Xie,Liangjian Wen,Zhao Kang*

Main category: cs.AI

TL;DR: 本文提出了一种名为DMGC的新型框架，用于多模态图的无监督聚类，通过分解图结构并引入双频融合机制，显著提升了聚类性能。


<details>
  <summary>Details</summary>
Motivation: 多模态图在现实世界中有广泛应用，但其在无监督学习中的研究不足，尤其是如何处理混合的同质性和异质性关系。

Method: DMGC将原始图分解为同质性增强图和异质性感知图，并通过双频融合机制联合优化，同时采用自监督对齐目标。

Result: 在多个数据集上的实验表明，DMGC实现了最先进的性能，证明了其有效性和泛化能力。

Conclusion: DMGC为多模态图聚类提供了一种有效的无监督解决方案，具有广泛的应用潜力。

Abstract: Multimodal graphs, which integrate unstructured heterogeneous data with
structured interconnections, offer substantial real-world utility but remain
insufficiently explored in unsupervised learning. In this work, we initiate the
study of multimodal graph clustering, aiming to bridge this critical gap.
Through empirical analysis, we observe that real-world multimodal graphs often
exhibit hybrid neighborhood patterns, combining both homophilic and
heterophilic relationships. To address this challenge, we propose a novel
framework -- \textsc{Disentangled Multimodal Graph Clustering (DMGC)} -- which
decomposes the original hybrid graph into two complementary views: (1) a
homophily-enhanced graph that captures cross-modal class consistency, and (2)
heterophily-aware graphs that preserve modality-specific inter-class
distinctions. We introduce a \emph{Multimodal Dual-frequency Fusion} mechanism
that jointly filters these disentangled graphs through a dual-pass strategy,
enabling effective multimodal integration while mitigating category confusion.
Our self-supervised alignment objectives further guide the learning process
without requiring labels. Extensive experiments on both multimodal and
multi-relational graph datasets demonstrate that DMGC achieves state-of-the-art
performance, highlighting its effectiveness and generalizability across diverse
settings. Our code is available at https://github.com/Uncnbb/DMGC.

</details>


### [105] [IM-Chat: A Multi-agent LLM-based Framework for Knowledge Transfer in Injection Molding Industry](https://arxiv.org/abs/2507.15268)
*Junhyeong Lee,Joon-Young Kim,Heekyu Kim,Inhyo Lee,Seunghwa Ryu*

Main category: cs.AI

TL;DR: IM-Chat是一个基于大语言模型的多代理框架，旨在解决注塑行业知识转移的挑战，结合文档知识和现场数据，通过检索增强生成和工具调用代理实现适应性。


<details>
  <summary>Details</summary>
Motivation: 注塑行业面临经验工人退休和多语言沟通障碍，导致知识转移困难。

Method: IM-Chat结合文档知识和数据驱动的过程条件生成器，采用检索增强生成和工具调用代理的模块化架构。

Result: 评估显示，更强大的模型在复杂任务中表现更好，IM-Chat在工业知识工作流中具有可行性。

Conclusion: IM-Chat为制造业AI辅助决策提供了一种可扩展和通用的解决方案。

Abstract: The injection molding industry faces critical challenges in preserving and
transferring field knowledge, particularly as experienced workers retire and
multilingual barriers hinder effective communication. This study introduces
IM-Chat, a multi-agent framework based on large language models (LLMs),
designed to facilitate knowledge transfer in injection molding. IM-Chat
integrates both limited documented knowledge (e.g., troubleshooting tables,
manuals) and extensive field data modeled through a data-driven process
condition generator that infers optimal manufacturing settings from
environmental inputs such as temperature and humidity, enabling robust and
context-aware task resolution. By adopting a retrieval-augmented generation
(RAG) strategy and tool-calling agents within a modular architecture, IM-Chat
ensures adaptability without the need for fine-tuning. Performance was assessed
across 100 single-tool and 60 hybrid tasks for GPT-4o, GPT-4o-mini, and
GPT-3.5-turbo by domain experts using a 10-point rubric focused on relevance
and correctness, and was further supplemented by automated evaluation using
GPT-4o guided by a domain-adapted instruction prompt. The evaluation results
indicate that more capable models tend to achieve higher accuracy, particularly
in complex, tool-integrated scenarios. Overall, these findings demonstrate the
viability of multi-agent LLM systems for industrial knowledge workflows and
establish IM-Chat as a scalable and generalizable approach to AI-assisted
decision support in manufacturing.

</details>


### [106] [QSAF: A Novel Mitigation Framework for Cognitive Degradation in Agentic AI](https://arxiv.org/abs/2507.15330)
*Hammad Atta,Muhammad Zeeshan Baig,Yasir Mehmood,Nadeem Shahzad,Ken Huang,Muhammad Aziz Ul Haq,Muhammad Awais,Kamal Ahmed*

Main category: cs.AI

TL;DR: 论文提出了一种新型的AI系统漏洞类别——认知退化，并提出了Qorvex安全AI框架（QSAF Domain 10）来应对此类问题，通过实时监控和缓解措施提升AI系统的行为与认知韧性。


<details>
  <summary>Details</summary>
Motivation: 传统的外部威胁（如提示注入）已被广泛研究，但AI系统内部的认知退化问题（如内存不足、规划递归等）尚未得到充分关注，可能导致逻辑崩溃和幻觉。

Method: 提出了QSAF Domain 10框架，包含六阶段认知退化生命周期和七项实时控制措施（如回退路由、饥饿检测等），并借鉴认知神经科学将AI架构映射到人类认知模型。

Result: 该框架能够实时监控AI子系统，并通过主动缓解措施（如内存完整性强制）防止认知退化，提升系统韧性。

Conclusion: 研究首次将认知退化确立为AI系统的新漏洞类别，并提出了跨平台的防御模型，为AI系统的行为韧性提供了新思路。

Abstract: We introduce Cognitive Degradation as a novel vulnerability class in agentic
AI systems. Unlike traditional adversarial external threats such as prompt
injection, these failures originate internally, arising from memory starvation,
planner recursion, context flooding, and output suppression. These systemic
weaknesses lead to silent agent drift, logic collapse, and persistent
hallucinations over time. To address this class of failures, we introduce the
Qorvex Security AI Framework for Behavioral & Cognitive Resilience (QSAF Domain
10), a lifecycle-aware defense framework defined by a six-stage cognitive
degradation lifecycle. The framework includes seven runtime controls
(QSAF-BC-001 to BC-007) that monitor agent subsystems in real time and trigger
proactive mitigation through fallback routing, starvation detection, and memory
integrity enforcement. Drawing from cognitive neuroscience, we map agentic
architectures to human analogs, enabling early detection of fatigue,
starvation, and role collapse. By introducing a formal lifecycle and real-time
mitigation controls, this work establishes Cognitive Degradation as a critical
new class of AI system vulnerability and proposes the first cross-platform
defense model for resilient agentic behavior.

</details>


### [107] [One Step is Enough: Multi-Agent Reinforcement Learning based on One-Step Policy Optimization for Order Dispatch on Ride-Sharing Platforms](https://arxiv.org/abs/2507.15351)
*Zijian Zhao,Sen Li*

Main category: cs.AI

TL;DR: 论文提出两种新方法（GRPO和OSPO）解决动态拼车匹配问题，避免传统MARL依赖准确值函数估计的问题，实验证明其性能优越。


<details>
  <summary>Details</summary>
Motivation: 动态拼车平台面临实时匹配乘客与车辆的高维度和不确定性挑战，传统MARL方法因依赖值函数估计而表现不佳。

Method: 1. 将GRPO应用于拼车，用组平均奖励替代PPO基线以减少估计误差；2. 提出OSPO，仅用一步奖励训练最优策略。

Result: 在真实曼哈顿拼车数据集上，GRPO和OSPO在大多数场景中表现优越，优化了接载时间和订单完成量。

Conclusion: GRPO和OSPO通过避免值函数估计，显著提升了动态拼车匹配的性能和稳定性。

Abstract: On-demand ride-sharing platforms face the fundamental challenge of
dynamically bundling passengers with diverse origins and destinations and
matching them with vehicles in real time, all under significant uncertainty.
Recently, MARL has emerged as a promising solution for this problem, leveraging
decentralized learning to address the curse of dimensionality caused by the
large number of agents in the ride-hailing market and the resulting expansive
state and action spaces. However, conventional MARL-based ride-sharing
approaches heavily rely on the accurate estimation of Q-values or V-values,
which becomes problematic in large-scale, highly uncertain environments.
Specifically, most of these approaches adopt an independent paradigm,
exacerbating this issue, as each agent treats others as part of the
environment, leading to unstable training and substantial estimation bias in
value functions. To address these challenges, we propose two novel alternative
methods that bypass value function estimation. First, we adapt GRPO to
ride-sharing, replacing the PPO baseline with the group average reward to
eliminate critic estimation errors and reduce training bias. Second, inspired
by GRPO's full utilization of group reward information, we customize the PPO
framework for ride-sharing platforms and show that, under a homogeneous fleet,
the optimal policy can be trained using only one-step rewards - a method we
term One-Step Policy Optimization (OSPO). Experiments on a real-world Manhattan
ride-hailing dataset demonstrate that both GRPO and OSPO achieve superior
performance across most scenarios, efficiently optimizing pickup times and the
number of served orders using simple MLP networks.

</details>


### [108] [RAD: Retrieval High-quality Demonstrations to Enhance Decision-making](https://arxiv.org/abs/2507.15356)
*Lu Guo,Yixiang Shan,Zhengbang Zhu,Qifan Liang,Lichang Song,Ting Long,Weinan Zhang,Yi Chang*

Main category: cs.AI

TL;DR: RAD结合检索与扩散模型，提升离线强化学习的长时规划能力。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习因数据集稀疏性和轨迹间过渡重叠不足而受限，传统方法泛化能力差。

Method: 提出RAD，通过非参数检索和扩散模型动态检索高回报状态并规划。

Result: 实验表明RAD在多种基准测试中表现优异。

Conclusion: RAD有效解决了离线强化学习中的泛化和规划问题。

Abstract: Offline reinforcement learning (RL) enables agents to learn policies from
fixed datasets, avoiding costly or unsafe environment interactions. However,
its effectiveness is often limited by dataset sparsity and the lack of
transition overlap between suboptimal and expert trajectories, which makes
long-horizon planning particularly challenging. Prior solutions based on
synthetic data augmentation or trajectory stitching often fail to generalize to
novel states and rely on heuristic stitching points. To address these
challenges, we propose Retrieval High-quAlity Demonstrations (RAD) for
decision-making, which combines non-parametric retrieval with diffusion-based
generative modeling. RAD dynamically retrieves high-return states from the
offline dataset as target states based on state similarity and return
estimation, and plans toward them using a condition-guided diffusion model.
Such retrieval-guided generation enables flexible trajectory stitching and
improves generalization when encountered with underrepresented or
out-of-distribution states. Extensive experiments confirm that RAD achieves
competitive or superior performance compared to baselines across diverse
benchmarks, validating its effectiveness.

</details>


### [109] [Predictive Process Monitoring Using Object-centric Graph Embeddings](https://arxiv.org/abs/2507.15411)
*Wissam Gherissi,Mehdi Acheli,Joyce El Haddad,Daniela Grigori*

Main category: cs.AI

TL;DR: 提出了一种基于图注意力网络和LSTM的端到端模型，用于预测未来流程行为，包括下一活动和下一事件时间。


<details>
  <summary>Details</summary>
Motivation: 利用对象中心事件日志提升流程预测，解决信息提取和模型构建的挑战。

Method: 结合图注意力网络编码活动关系，LSTM处理时间依赖。

Result: 在真实和合成事件日志上表现优于现有方法。

Conclusion: 模型在预测流程行为方面具有竞争力。

Abstract: Object-centric predictive process monitoring explores and utilizes
object-centric event logs to enhance process predictions. The main challenge
lies in extracting relevant information and building effective models. In this
paper, we propose an end-to-end model that predicts future process behavior,
focusing on two tasks: next activity prediction and next event time. The
proposed model employs a graph attention network to encode activities and their
relationships, combined with an LSTM network to handle temporal dependencies.
Evaluated on one reallife and three synthetic event logs, the model
demonstrates competitive performance compared to state-of-the-art methods.

</details>


### [110] [Optimization of Activity Batching Policies in Business Processes](https://arxiv.org/abs/2507.15457)
*Orlenys López-Pintado,Jannis Rosenbaum,Marlon Dumas*

Main category: cs.AI

TL;DR: 论文提出了一种基于帕累托优化的方法，通过干预启发式发现业务过程中活动批处理的最优策略，平衡等待时间、处理成本和资源利用率。


<details>
  <summary>Details</summary>
Motivation: 业务过程中，批处理策略需要在成本和等待时间之间找到平衡，但现有方法缺乏自动发现最优策略的能力。

Method: 采用帕累托优化和干预启发式，通过模拟评估策略改进，并结合三种元启发式（爬山法、模拟退火和强化学习）优化策略。

Result: 实验表明，基于干预启发式的方法在收敛性、多样性和周期时间增益上优于非启发式基线。

Conclusion: 该方法能有效发现最优批处理策略，为业务过程优化提供实用工具。

Abstract: In business processes, activity batching refers to packing multiple activity
instances for joint execution. Batching allows managers to trade off cost and
processing effort against waiting time. Larger and less frequent batches may
lower costs by reducing processing effort and amortizing fixed costs, but they
create longer waiting times. In contrast, smaller and more frequent batches
reduce waiting times but increase fixed costs and processing effort. A batching
policy defines how activity instances are grouped into batches and when each
batch is activated. This paper addresses the problem of discovering batching
policies that strike optimal trade-offs between waiting time, processing
effort, and cost. The paper proposes a Pareto optimization approach that starts
from a given set (possibly empty) of activity batching policies and generates
alternative policies for each batched activity via intervention heuristics.
Each heuristic identifies an opportunity to improve an activity's batching
policy with respect to a metric (waiting time, processing time, cost, or
resource utilization) and an associated adjustment to the activity's batching
policy (the intervention). The impact of each intervention is evaluated via
simulation. The intervention heuristics are embedded in an optimization
meta-heuristic that triggers interventions to iteratively update the Pareto
front of the interventions identified so far. The paper considers three
meta-heuristics: hill-climbing, simulated annealing, and reinforcement
learning. An experimental evaluation compares the proposed approach based on
intervention heuristics against the same (non-heuristic guided) meta-heuristics
baseline regarding convergence, diversity, and cycle time gain of
Pareto-optimal policies.

</details>


### [111] [Chart-R1: Chain-of-Thought Supervision and Reinforcement for Advanced Chart Reasoner](https://arxiv.org/abs/2507.15509)
*Lei Chen,Xuanle Zhao,Zhixiong Zeng,Jing Huang,Yufeng Zhong,Lin Ma*

Main category: cs.AI

TL;DR: Chart-R1是一种基于强化学习微调的图表领域视觉语言模型，用于复杂图表推理。通过程序化数据合成技术和两阶段训练策略（Chart-COT和Chart-RFT），在开源基准和自建数据集上表现出色，甚至媲美大型模型。


<details>
  <summary>Details</summary>
Motivation: 验证R1-Style方法在通用多模态数据（如图表）上的优势，解决图表领域推理数据不足的问题。

Method: 提出程序化数据合成技术生成高质量图表推理数据，并采用两阶段训练策略：Chart-COT（逐步监督）和Chart-RFT（数值敏感的强化微调）。

Result: Chart-R1在图表领域方法中表现显著优势，甚至与GPT-4o、Claude-3.5等大型模型相当。

Conclusion: Chart-R1通过创新的数据合成和训练策略，成功提升了图表领域的复杂推理能力。

Abstract: Recently, inspired by OpenAI-o1/o3 and Deepseek-R1, the R1-Style method based
on reinforcement learning fine-tuning has received widespread attention from
the community. Previous R1-Style methods mainly focus on mathematical reasoning
and code intelligence. It is of great research significance to verify their
advantages on more general multimodal data. Chart is an important multimodal
data type with rich information, which brings important research challenges in
complex reasoning. In this work, we introduce Chart-R1, a chart-domain
vision-language model with reinforcement learning fine-tuning to enable complex
chart reasoning. To support Chart-R1, we first propose a novel programmatic
data synthesis technology to generate high-quality step-by-step chart reasoning
data covering single- and multi-subcharts, which makes up for the lack of
reasoning data in the chart domain. Then we develop a two-stage training
strategy: Chart-COT with step-by-step chain-of-thought supervision, and
Chart-RFT with numerically sensitive reinforcement fine-tuning. Chart-COT aims
to decompose complex chart reasoning tasks into fine-grained, understandable
subtasks through step-by-step supervision, which lays a good foundation for
improving the reasoning level of reinforcement learning. Chart-RFT utilize the
typical group relative policy optimization strategy, in which a relatively soft
reward is adopted for numerical response to emphasize the numerical sensitivity
in the chart domain. We conduct extensive experiments on open-source benchmarks
and self-built chart reasoning dataset (\emph{i.e., ChartRQA}). Experimental
results show that Chart-R1 has significant advantages compared to chart-domain
methods, even comparable to open/closed source large-scale models (\emph{e.g.,
GPT-4o, Claude-3.5}).

</details>


### [112] [HAMLET: Hyperadaptive Agent-based Modeling for Live Embodied Theatrics](https://arxiv.org/abs/2507.15518)
*Sizhou Chen,Shufan Jiang,Chi Zhang,Xiao-Lei Zhang,Xuelong Li*

Main category: cs.AI

TL;DR: HAMLET是一个多智能体框架，用于戏剧创作和实时表演，通过自主决策和场景互动提升沉浸感。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型（LLM）的戏剧生成方法缺乏主动性和环境交互能力，限制了实时表演的互动性和沉浸感。

Method: 提出HAMLET框架，生成叙事蓝图并赋予演员自主决策能力，支持通过动作改变场景状态。

Result: 实验评估表明，HAMLET能创造富有表现力和连贯性的戏剧体验。

Conclusion: HAMLET通过多智能体自主决策和场景互动，显著提升了戏剧表演的互动性和沉浸感。

Abstract: Creating an immersive and interactive theatrical experience is a long-term
goal in the field of interactive narrative. The emergence of large language
model (LLM) is providing a new path to achieve this goal. However, existing
LLM-based drama generation methods often result in AI agents that lack
initiative and cannot interact with the physical environment. Furthermore,
these methods typically require detailed user input to drive the drama. These
limitations reduce the interactivity and immersion of online real-time
performance. To address the above challenges, we propose HAMLET, a multi-agent
framework focused on drama creation and online performance. Given a simple
topic, the framework generates a narrative blueprint, guiding the subsequent
improvisational performance. During the online performance, each actor is given
an autonomous mind. This means that actors can make independent decisions based
on their own background, goals, and emotional state. In addition to
conversations with other actors, their decisions can also change the state of
scene props through actions such as opening a letter or picking up a weapon.
The change is then broadcast to other related actors, updating what they know
and care about, which in turn influences their next action. To evaluate the
quality of drama performance, we designed an evaluation method to assess three
primary aspects, including character performance, narrative quality, and
interaction experience. The experimental evaluation shows that HAMLET can
create expressive and coherent theatrical experiences. Our code, dataset and
models are available at https://github.com/HAMLET-2025/HAMLET.

</details>


### [113] [LLM world models are mental: Output layer evidence of brittle world model use in LLM mechanical reasoning](https://arxiv.org/abs/2507.15521)
*Cole Robertson,Philip Wolff*

Main category: cs.AI

TL;DR: 论文研究了大型语言模型（LLMs）是否构建内部世界模型或仅依赖统计关联。通过滑轮系统问题测试，发现LLMs能利用统计关联（如滑轮数量）估计机械优势（MA），但缺乏对复杂结构连接的推理能力。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs是否具备内部世界模型构建能力，而非仅依赖统计关联。

Method: 采用认知科学方法，通过三个研究测试LLMs在滑轮系统问题中的表现：1）估计MA；2）区分功能性与随机系统；3）比较功能性与无作用系统。

Result: LLMs能利用滑轮数量估计MA（Study 1），区分功能性与随机系统（Study 2），但在复杂结构连接推理中表现不佳（Study 3）。

Conclusion: LLMs可能具备初步世界模型能力，但缺乏复杂推理。认知科学方法有助于评估AI系统的世界建模能力。

Abstract: Do large language models (LLMs) construct and manipulate internal world
models, or do they rely solely on statistical associations represented as
output layer token probabilities? We adapt cognitive science methodologies from
human mental models research to test LLMs on pulley system problems using
TikZ-rendered stimuli. Study 1 examines whether LLMs can estimate mechanical
advantage (MA). State-of-the-art models performed marginally but significantly
above chance, and their estimates correlated significantly with ground-truth
MA. Significant correlations between number of pulleys and model estimates
suggest that models employed a pulley counting heuristic, without necessarily
simulating pulley systems to derive precise values. Study 2 tested this by
probing whether LLMs represent global features crucial to MA estimation. Models
evaluated a functionally connected pulley system against a fake system with
randomly placed components. Without explicit cues, models identified the
functional system as having greater MA with F1=0.8, suggesting LLMs could
represent systems well enough to differentiate jumbled from functional systems.
Study 3 built on this by asking LLMs to compare functional systems with matched
systems which were connected up but which transferred no force to the weight;
LLMs identified the functional system with F1=0.46, suggesting random guessing.
Insofar as they may generalize, these findings are compatible with the notion
that LLMs manipulate internal world models, sufficient to exploit statistical
associations between pulley count and MA (Study 1), and to approximately
represent system components' spatial relations (Study 2). However, they may
lack the facility to reason over nuanced structural connectivity (Study 3). We
conclude by advocating the utility of cognitive scientific methods to evaluate
the world-modeling capacities of artificial intelligence systems.

</details>


### [114] [Data-Efficient Safe Policy Improvement Using Parametric Structure](https://arxiv.org/abs/2507.15532)
*Kasper Engelen,Guillermo A. Pérez,Marnix Suilen*

Main category: cs.AI

TL;DR: 本文提出了一种更高效的安全策略改进（SPI）方法，通过利用转移动态中的参数依赖关系、游戏抽象预处理和SMT求解技术，显著提升了数据效率。


<details>
  <summary>Details</summary>
Motivation: 解决离线强化学习中的安全策略改进问题，利用已知的参数依赖关系提升数据效率。

Method: 1. 提出参数化SPI算法，利用分布间的相关性更准确估计转移动态；2. 使用游戏抽象预处理技术剪枝冗余动作；3. 基于SMT求解的进阶预处理技术进一步剪枝动作。

Result: 实验表明，这些技术将SPI的数据效率提升了多个数量级，同时保持了相同的可靠性保证。

Conclusion: 通过结合参数依赖和预处理技术，显著提升了SPI的数据效率，为离线强化学习提供了更高效的解决方案。

Abstract: Safe policy improvement (SPI) is an offline reinforcement learning problem in
which a new policy that reliably outperforms the behavior policy with high
confidence needs to be computed using only a dataset and the behavior policy.
Markov decision processes (MDPs) are the standard formalism for modeling
environments in SPI. In many applications, additional information in the form
of parametric dependencies between distributions in the transition dynamics is
available. We make SPI more data-efficient by leveraging these dependencies
through three contributions: (1) a parametric SPI algorithm that exploits known
correlations between distributions to more accurately estimate the transition
dynamics using the same amount of data; (2) a preprocessing technique that
prunes redundant actions from the environment through a game-based abstraction;
and (3) a more advanced preprocessing technique, based on satisfiability modulo
theory (SMT) solving, that can identify more actions to prune. Empirical
results and an ablation study show that our techniques increase the data
efficiency of SPI by multiple orders of magnitude while maintaining the same
reliability guarantees.

</details>


### [115] [Metric assessment protocol in the context of answer fluctuation on MCQ tasks](https://arxiv.org/abs/2507.15581)
*Ekaterina Goliakova,Xavier Renard,Marie-Jeanne Lesot,Thibault Laugel,Christophe Marsala,Marcin Detyniecki*

Main category: cs.AI

TL;DR: 论文提出了一种评估多选问题（MCQ）指标的新协议，分析了指标与答案波动率的关系，发现现有指标与答案变化有强关联，并提出了一种新指标“最差准确率”。


<details>
  <summary>Details</summary>
Motivation: 现有研究未对评估LLM能力的多选问题指标进行全面评估，且MCQ评估存在答案波动问题。

Method: 提出了一种指标评估协议，分析评估方法与波动率及原始性能的关系。

Result: 现有指标与答案变化有强关联，新指标“最差准确率”在协议中表现最佳。

Conclusion: 新协议和新指标有助于更全面地评估LLM在多选问题中的表现。

Abstract: Using multiple-choice questions (MCQs) has become a standard for assessing
LLM capabilities efficiently. A variety of metrics can be employed for this
task. However, previous research has not conducted a thorough assessment of
them. At the same time, MCQ evaluation suffers from answer fluctuation: models
produce different results given slight changes in prompts. We suggest a metric
assessment protocol in which evaluation methodologies are analyzed through
their connection with fluctuation rates, as well as original performance. Our
results show that there is a strong link between existing metrics and the
answer changing, even when computed without any additional prompt variants. A
novel metric, worst accuracy, demonstrates the highest association on the
protocol.

</details>


### [116] [TacticCraft: Natural Language-Driven Tactical Adaptation for StarCraft II](https://arxiv.org/abs/2507.15618)
*Weiyu Ma,Jiwen Jiang,Haobo Fu,Haifeng Zhang*

Main category: cs.AI

TL;DR: 提出了一种基于适配器的战术调节方法，用于《星际争霸II》AI代理，使其能根据高层战术指令调整策略。


<details>
  <summary>Details</summary>
Motivation: 当前AI代理虽强大，但缺乏基于高层战术指令的适应能力。

Method: 冻结预训练策略网络（DI-Star），为每个动作头附加轻量级适配器模块，并通过KL散度约束训练这些适配器。

Result: 实验表明，该方法能成功调节代理行为（如侵略性、扩张模式和技术偏好），同时保持竞争力。

Conclusion: 该方法以最小计算开销实现灵活战术控制，为复杂即时战略游戏提供实用策略定制。

Abstract: We present an adapter-based approach for tactical conditioning of StarCraft
II AI agents. Current agents, while powerful, lack the ability to adapt their
strategies based on high-level tactical directives. Our method freezes a
pre-trained policy network (DI-Star) and attaches lightweight adapter modules
to each action head, conditioned on a tactical tensor that encodes strategic
preferences. By training these adapters with KL divergence constraints, we
ensure the policy maintains core competencies while exhibiting tactical
variations. Experimental results show our approach successfully modulates agent
behavior across tactical dimensions including aggression, expansion patterns,
and technology preferences, while maintaining competitive performance. Our
method enables flexible tactical control with minimal computational overhead,
offering practical strategy customization for complex real-time strategy games.

</details>


### [117] [Agentic AI for autonomous anomaly management in complex systems](https://arxiv.org/abs/2507.15676)
*Reza Vatankhah Barenji,Sina Khoshgoftar*

Main category: cs.AI

TL;DR: 研究探讨了代理型AI在复杂系统中自主检测和响应异常的潜力，强调其能改变传统依赖人类的异常管理方法。


<details>
  <summary>Details</summary>
Motivation: 传统异常管理方法依赖人工，效率低且成本高，代理型AI可提供自主解决方案。

Method: 利用代理型AI技术，设计自主检测和响应异常的机制。

Result: 代理型AI能有效提升异常管理的效率和准确性。

Conclusion: 代理型AI在复杂系统异常管理中具有显著潜力，可替代传统人工方法。

Abstract: This paper explores the potential of agentic AI in autonomously detecting and
responding to anomalies within complex systems, emphasizing its ability to
transform traditional, human-dependent anomaly management methods.

</details>


### [118] [Towards physician-centered oversight of conversational diagnostic AI](https://arxiv.org/abs/2507.15743)
*Elahe Vedadi,David Barrett,Natalie Harris,Ellery Wulczyn,Shashir Reddy,Roma Ruparel,Mike Schaekermann,Tim Strother,Ryutaro Tanno,Yash Sharma,Jihyeon Lee,Cían Hughes,Dylan Slack,Anil Palepu,Jan Freyberg,Khaled Saab,Valentin Liévin,Wei-Hung Weng,Tao Tu,Yun Liu,Nenad Tomasev,Kavita Kulkarni,S. Sara Mahdavi,Kelvin Guu,Joëlle Barral,Dale R. Webster,James Manyika,Avinatan Hassidim,Katherine Chou,Yossi Matias,Pushmeet Kohli,Adam Rodman,Vivek Natarajan,Alan Karthikesalingam,David Stutz*

Main category: cs.AI

TL;DR: 论文提出了一个名为g-AMIE的多智能体系统框架，用于在医疗诊断对话中实现异步监督，确保患者安全。


<details>
  <summary>Details</summary>
Motivation: 现有对话AI系统在诊断对话中表现出潜力，但个体诊断和治疗计划需由持牌专业人员监管。受此启发，研究旨在设计一种框架，使AI系统在监督下安全运行。

Method: 提出g-AMIE系统，通过多智能体在护栏内完成病史采集，避免提供个体化医疗建议，并通过临床驾驶舱界面将评估结果传递给监督医生。

Result: 在虚拟OSCE测试中，g-AMIE在高质量病史采集、病例总结及诊断建议方面优于NPs/PAs和PCPs组，且监督效率更高。

Conclusion: 异步监督是一种可行的模式，可增强AI系统在专家监督下的实际应用潜力。

Abstract: Recent work has demonstrated the promise of conversational AI systems for
diagnostic dialogue. However, real-world assurance of patient safety means that
providing individual diagnoses and treatment plans is considered a regulated
activity by licensed professionals. Furthermore, physicians commonly oversee
other team members in such activities, including nurse practitioners (NPs) or
physician assistants/associates (PAs). Inspired by this, we propose a framework
for effective, asynchronous oversight of the Articulate Medical Intelligence
Explorer (AMIE) AI system. We propose guardrailed-AMIE (g-AMIE), a multi-agent
system that performs history taking within guardrails, abstaining from
individualized medical advice. Afterwards, g-AMIE conveys assessments to an
overseeing primary care physician (PCP) in a clinician cockpit interface. The
PCP provides oversight and retains accountability of the clinical decision.
This effectively decouples oversight from intake and can thus happen
asynchronously. In a randomized, blinded virtual Objective Structured Clinical
Examination (OSCE) of text consultations with asynchronous oversight, we
compared g-AMIE to NPs/PAs or a group of PCPs under the same guardrails. Across
60 scenarios, g-AMIE outperformed both groups in performing high-quality
intake, summarizing cases, and proposing diagnoses and management plans for the
overseeing PCP to review. This resulted in higher quality composite decisions.
PCP oversight of g-AMIE was also more time-efficient than standalone PCP
consultations in prior work. While our study does not replicate existing
clinical practices and likely underestimates clinicians' capabilities, our
results demonstrate the promise of asynchronous oversight as a feasible
paradigm for diagnostic AI systems to operate under expert human oversight for
enhancing real-world care.

</details>


### [119] [LAPO: Internalizing Reasoning Efficiency via Length-Adaptive Policy Optimization](https://arxiv.org/abs/2507.15758)
*Xingyu Wu,Yuchen Yan,Shangke Lyu,Linjuan Wu,Yiwen Qiu,Yongliang Shen,Weiming Lu,Jian Shao,Jun Xiao,Yueting Zhuang*

Main category: cs.AI

TL;DR: LAPO框架通过两阶段强化学习，使模型内化推理长度控制能力，减少40.9%的token使用并提升2.3%的准确率。


<details>
  <summary>Details</summary>
Motivation: 解决大型推理模型因自由生成链式思维序列导致的token浪费问题。

Method: 两阶段强化学习：第一阶段学习成功解的长度分布，第二阶段将其作为元认知指导嵌入推理上下文。

Result: 在数学推理基准测试中，token使用减少40.9%，准确率提升2.3%。

Conclusion: LAPO使模型能根据问题复杂度动态分配计算资源，实现高效且高质量的推理。

Abstract: Large reasoning models have achieved remarkable performance through extended
chain-of-thought sequences, yet this computational freedom leads to excessive
token generation even for simple problems. We present Length-Adaptive Policy
Optimization (LAPO), a novel framework that transforms reasoning length control
from an external constraint into an intrinsic model capability. Unlike existing
approaches that impose rigid limits or rely on post-hoc interventions, LAPO
enables models to internalize an understanding of appropriate reasoning depth
through a two-stage reinforcement learning process. In the first stage, models
learn natural reasoning patterns by discovering the statistical distribution of
successful solution lengths. The second stage leverages these patterns as
meta-cognitive guidance, embedding them directly within the model's reasoning
context to ensure inference-time flexibility. Experiments on mathematical
reasoning benchmarks demonstrate that LAPO reduces token usage by up to 40.9\%
while improving accuracy by 2.3\%. Our analysis reveals that models trained
with LAPO develop emergent abilities to allocate computational resources based
on problem complexity, achieving efficient reasoning without sacrificing
quality.

</details>


### [120] [GasAgent: A Multi-Agent Framework for Automated Gas Optimization in Smart Contracts](https://arxiv.org/abs/2507.15761)
*Jingyi Zheng,Zifan Peng,Yule Liu,Junfeng Wang,Yifan Liao,Wenhan Dong,Xinlei He*

Main category: cs.AI

TL;DR: GasAgent是一个多代理系统，用于智能合约Gas优化，结合现有模式的兼容性和新模式的自动发现/验证，实现端到端优化。


<details>
  <summary>Details</summary>
Motivation: 现有解决方案依赖手动发现Gas浪费模式，效率低且难以扩展，而基于LLM的方法存在兼容性和冗余问题。

Method: GasAgent由四个专业代理（Seeker、Innovator、Executor、Manager）组成，协作完成Gas优化的闭环流程。

Result: 在100个真实合约中优化了82个，平均节省9.97%的部署Gas；在500个LLM生成的合约中优化了79.8%，节省4.79%-13.93%的Gas。

Conclusion: GasAgent展示了作为LLM辅助智能合约开发的优化层的广泛可用性和有效性。

Abstract: Smart contracts are trustworthy, immutable, and automatically executed
programs on the blockchain. Their execution requires the Gas mechanism to
ensure efficiency and fairness. However, due to non-optimal coding practices,
many contracts contain Gas waste patterns that need to be optimized. Existing
solutions mostly rely on manual discovery, which is inefficient, costly to
maintain, and difficult to scale. Recent research uses large language models
(LLMs) to explore new Gas waste patterns. However, it struggles to remain
compatible with existing patterns, often produces redundant patterns, and
requires manual validation/rewriting. To address this gap, we present GasAgent,
the first multi-agent system for smart contract Gas optimization that combines
compatibility with existing patterns and automated discovery/validation of new
patterns, enabling end-to-end optimization. GasAgent consists of four
specialized agents, Seeker, Innovator, Executor, and Manager, that collaborate
in a closed loop to identify, validate, and apply Gas-saving improvements.
Experiments on 100 verified real-world contracts demonstrate that GasAgent
successfully optimizes 82 contracts, achieving an average deployment Gas
savings of 9.97%. In addition, our evaluation confirms its compatibility with
existing tools and validates the effectiveness of each module through ablation
studies. To assess broader usability, we further evaluate 500 contracts
generated by five representative LLMs across 10 categories and find that
GasAgent optimizes 79.8% of them, with deployment Gas savings ranging from
4.79% to 13.93%, showing its usability as the optimization layer for
LLM-assisted smart contract development.

</details>


### [121] [A Framework for Analyzing Abnormal Emergence in Service Ecosystems Through LLM-based Agent Intention Mining](https://arxiv.org/abs/2507.15770)
*Yifan Shen,Zihan Zhao,Xiao Xue,Yuwei Guo,Qun Ma,Deyu Zhou,Ming Zhang*

Main category: cs.AI

TL;DR: 论文提出了一种基于多智能体意图的动态可解释涌现分析框架EAMI，通过双视角思维追踪机制和k-means聚类分析群体意图的相变点，并在实验中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着服务计算、云计算和物联网的发展，服务生态系统日益复杂，传统因果方法难以分析智能体间的异常涌现现象，需要新的动态分析方法。

Method: EAMI框架采用双视角思维追踪机制（Inspector Agent和Analysis Agent）提取意图，结合k-means聚类和意图时序涌现图进行动态分析。

Result: 实验在复杂O2O服务系统和Stanford AI Town中验证了EAMI的有效性、通用性和效率。

Conclusion: EAMI为服务生态系统中的异常涌现和因果分析提供了新范式。

Abstract: With the rise of service computing, cloud computing, and IoT, service
ecosystems are becoming increasingly complex. The intricate interactions among
intelligent agents make abnormal emergence analysis challenging, as traditional
causal methods focus on individual trajectories. Large language models offer
new possibilities for Agent-Based Modeling (ABM) through Chain-of-Thought (CoT)
reasoning to reveal agent intentions. However, existing approaches remain
limited to microscopic and static analysis. This paper introduces a framework:
Emergence Analysis based on Multi-Agent Intention (EAMI), which enables dynamic
and interpretable emergence analysis. EAMI first employs a dual-perspective
thought track mechanism, where an Inspector Agent and an Analysis Agent extract
agent intentions under bounded and perfect rationality. Then, k-means
clustering identifies phase transition points in group intentions, followed by
a Intention Temporal Emergence diagram for dynamic analysis. The experiments
validate EAMI in complex online-to-offline (O2O) service system and the
Stanford AI Town experiment, with ablation studies confirming its
effectiveness, generalizability, and efficiency. This framework provides a
novel paradigm for abnormal emergence and causal analysis in service
ecosystems. The code is available at
https://anonymous.4open.science/r/EAMI-B085.

</details>


### [122] [Challenges of Trustworthy Federated Learning: What's Done, Current Trends and Remaining Work](https://arxiv.org/abs/2507.15796)
*Nuria Rodríguez-Barroso,Mario García-Márquez,M. Victoria Luzón,Francisco Herrera*

Main category: cs.AI

TL;DR: 本文探讨了联邦学习（FL）如何满足可信人工智能（TAI）的要求，分析了FL在隐私保护方面的潜力及其与TAI其他要求的对齐挑战。


<details>
  <summary>Details</summary>
Motivation: 随着AI在高风险领域的广泛应用，确保其可信性（TAI）变得至关重要。联邦学习（FL）虽能解决隐私问题，但其分布式特性与TAI的其他要求存在冲突，需要系统分析。

Method: 以TAI要求为框架，系统分类并分析了FL与TAI对齐的主要障碍，总结了现有研究、趋势及未解决问题。

Result: 识别了FL与TAI对齐的关键挑战，并详细探讨了各挑战的研究现状和发展方向。

Conclusion: FL在隐私保护方面具有潜力，但需进一步研究以解决其与TAI其他要求的对齐问题。

Abstract: In recent years, the development of Trustworthy Artificial Intelligence (TAI)
has emerged as a critical objective in the deployment of AI systems across
sensitive and high-risk domains. TAI frameworks articulate a comprehensive set
of ethical, legal, and technical requirements to ensure that AI technologies
are aligned with human values, rights, and societal expectations. Among the
various AI paradigms, Federated Learning (FL) presents a promising solution to
pressing privacy concerns. However, aligning FL with the rest of the
requirements of TAI presents a series of challenges, most of which arise from
its inherently distributed nature. In this work, we adopt the requirements TAI
as a guiding structure to systematically analyze the challenges of adapting FL
to TAI. Specifically, we classify and examine the key obstacles to aligning FL
with TAI, providing a detailed exploration of what has been done, the trends,
and the remaining work within each of the identified challenges.

</details>


### [123] [Identifying Conditional Causal Effects in MPDAGs](https://arxiv.org/abs/2507.15842)
*Sara LaPlante,Emilija Perković*

Main category: cs.AI

TL;DR: 论文研究了在已知最大定向部分有向无环图（MPDAG）的情况下，如何识别条件因果效应。提出了三种结果：不受治疗影响的调节集识别公式、MPDAG设置下的do calculus推广，以及一个完整的条件效应识别算法。


<details>
  <summary>Details</summary>
Motivation: 研究背景是因果推断中图模型的不确定性，特别是在MPDAG表示的情况下，如何准确识别条件因果效应。

Method: 提出了三种方法：1）针对不受治疗影响的调节集的识别公式；2）将do calculus推广到MPDAG设置；3）开发了一个完整的条件效应识别算法。

Result: 成功实现了在MPDAG设置下条件因果效应的识别，并提供了理论和算法支持。

Conclusion: 论文为MPDAG背景下的条件因果效应识别提供了系统性的解决方案，扩展了因果推断的理论和应用范围。

Abstract: We consider identifying a conditional causal effect when a graph is known up
to a maximally oriented partially directed acyclic graph (MPDAG). An MPDAG
represents an equivalence class of graphs that is restricted by background
knowledge and where all variables in the causal model are observed. We provide
three results that address identification in this setting: an identification
formula when the conditioning set is unaffected by treatment, a generalization
of the well-known do calculus to the MPDAG setting, and an algorithm that is
complete for identifying these conditional effects.

</details>


### [124] [Hierarchical Budget Policy Optimization for Adaptive Reasoning](https://arxiv.org/abs/2507.15844)
*Shangke Lyu,Linjuan Wu,Yuchen Yan,Xingyu Wu,Hao Li,Yongliang Shen,Peisheng Jiang,Weiming Lu,Jun Xiao,Yueting Zhuang*

Main category: cs.AI

TL;DR: HBPO是一种强化学习框架，通过分层预算探索和差异化奖励机制，使模型能根据问题复杂度自适应调整推理深度，显著减少计算资源使用同时提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决大型推理模型在链式思维生成中计算效率低下的问题，避免因统一推理策略导致的资源浪费。

Method: 采用分层预算探索，将样本分组并分配不同token预算，结合预算感知的奖励机制，优化资源分配。

Result: 实验显示HBPO平均减少60.6%的token使用，同时准确率提升3.14%，且模型能自适应调整推理深度。

Conclusion: 推理效率与能力并非矛盾，通过分层训练可同时优化两者，保持探索多样性。

Abstract: Large reasoning models achieve remarkable performance through extensive
chain-of-thought generation, yet exhibit significant computational inefficiency
by applying uniform reasoning strategies regardless of problem complexity. We
present Hierarchical Budget Policy Optimization (HBPO), a reinforcement
learning framework that enables models to learn problem-specific reasoning
depths without sacrificing capability. HBPO addresses the fundamental challenge
of exploration space collapse in efficiency-oriented training, where penalties
on long output length systematically bias models away from necessary long
reasoning paths. Through hierarchical budget exploration, our approach
partitions rollout samples into multiple subgroups with distinct token budgets,
aiming to enable efficient resource allocation while preventing degradation of
capability. We introduce differentiated reward mechanisms that create
budget-aware incentives aligned with the complexity of the problem, allowing
models to discover natural correspondences between task requirements and
computational effort. Extensive experiments demonstrate that HBPO reduces
average token usage by up to 60.6% while improving accuracy by 3.14% across
four reasoning benchmarks. Unlike existing methods that impose external
constraints or rely on discrete mode selection, HBPO exhibits emergent adaptive
behavior where models automatically adjust reasoning depth based on problem
complexity. Our results suggest that reasoning efficiency and capability are
not inherently conflicting, and can be simultaneously optimized through
appropriately structured hierarchical training that preserves exploration
diversity.

</details>


### [125] [The Other Mind: How Language Models Exhibit Human Temporal Cognition](https://arxiv.org/abs/2507.15851)
*Lingyu Li,Yang Yao,Yixu Wang,Chubo Li,Yan Teng,Yingchun Wang*

Main category: cs.AI

TL;DR: 研究发现大型语言模型（LLMs）表现出类似人类的时间认知模式，包括主观时间参考点和Weber-Fechner定律的遵循。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs中未在训练数据中直接指定的认知模式，特别是时间认知。

Method: 通过相似性判断任务和多层次分析（神经元、表征、信息层面）研究LLMs的时间认知机制。

Result: 发现LLMs中存在时间偏好神经元，表征层次构建过程，以及训练语料的非线性时间结构。

Conclusion: 提出体验主义视角，认为LLMs的认知是内部表征系统对外部世界的主观构建，暗示AI对齐需关注内部构建的引导。

Abstract: As Large Language Models (LLMs) continue to advance, they exhibit certain
cognitive patterns similar to those of humans that are not directly specified
in training data. This study investigates this phenomenon by focusing on
temporal cognition in LLMs. Leveraging the similarity judgment task, we find
that larger models spontaneously establish a subjective temporal reference
point and adhere to the Weber-Fechner law, whereby the perceived distance
logarithmically compresses as years recede from this reference point. To
uncover the mechanisms behind this behavior, we conducted multiple analyses
across neuronal, representational, and informational levels. We first identify
a set of temporal-preferential neurons and find that this group exhibits
minimal activation at the subjective reference point and implements a
logarithmic coding scheme convergently found in biological systems. Probing
representations of years reveals a hierarchical construction process, where
years evolve from basic numerical values in shallow layers to abstract temporal
orientation in deep layers. Finally, using pre-trained embedding models, we
found that the training corpus itself possesses an inherent, non-linear
temporal structure, which provides the raw material for the model's internal
construction. In discussion, we propose an experientialist perspective for
understanding these findings, where the LLMs' cognition is viewed as a
subjective construction of the external world by its internal representational
system. This nuanced perspective implies the potential emergence of alien
cognitive frameworks that humans cannot intuitively predict, pointing toward a
direction for AI alignment that focuses on guiding internal constructions. Our
code is available at https://TheOtherMind.github.io.

</details>


### [126] [Gemini 2.5 Pro Capable of Winning Gold at IMO 2025](https://arxiv.org/abs/2507.15855)
*Yichen Huang,Lin F. Yang*

Main category: cs.AI

TL;DR: 论文探讨了使用Google的Gemini 2.5 Pro解决IMO 2025难题的效果，通过优化流程和提示工程，成功解决了5/6的问题。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在解决国际数学奥林匹克（IMO）难题时的表现，探索如何优化模型使用方式。

Method: 使用Gemini 2.5 Pro模型，结合管道设计和提示工程，避免数据污染。

Result: 在IMO 2025的6道题中，成功解决了5道（部分问题存在局限性）。

Conclusion: 优化模型使用方法对解决高难度数学问题至关重要。

Abstract: The International Mathematical Olympiad (IMO) poses uniquely challenging
problems requiring deep insight, creativity, and formal reasoning. While Large
Language Models (LLMs) perform well on mathematical benchmarks like AIME, they
struggle with Olympiad-level tasks. We use Google's Gemini 2.5 Pro on the newly
released IMO 2025 problems, avoiding data contamination. With pipeline design
and prompt engineering, 5 (out of 6) problems are solved correctly (up to a
caveat discussed below), highlighting the importance of finding the optimal way
of using powerful models.

</details>
