{"id": "2508.15135", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.15135", "abs": "https://arxiv.org/abs/2508.15135", "authors": ["Sumudu Liyanage", "Sherlock A. Licorish", "Markus Wagner", "Stephen G. MacDonell"], "title": "On the need to perform comprehensive evaluations of automated program repair benchmarks: Sorald case study", "comment": null, "summary": "In supporting the development of high-quality software, especially necessary\nin the era of LLMs, automated program repair (APR) tools aim to improve code\nquality by automatically addressing violations detected by static analysis\nprofilers. Previous research tends to evaluate APR tools only for their ability\nto clear violations, neglecting their potential introduction of new (sometimes\nsevere) violations, changes to code functionality and degrading of code\nstructure. There is thus a need for research to develop and assess\ncomprehensive evaluation frameworks for APR tools. This study addresses this\nresearch gap, and evaluates Sorald (a state-of-the-art APR tool) as a proof of\nconcept. Sorald's effectiveness was evaluated in repairing 3,529 SonarQube\nviolations across 30 rules within 2,393 Java code snippets extracted from Stack\nOverflow. Outcomes show that while Sorald fixes specific rule violations, it\nintroduced 2,120 new faults (32 bugs, 2088 code smells), reduced code\nfunctional correctness--as evidenced by a 24% unit test failure rate--and\ndegraded code structure, demonstrating the utility of our framework. Findings\nemphasize the need for evaluation methodologies that capture the full spectrum\nof APR tool effects, including side effects, to ensure their safe and effective\nadoption."}
{"id": "2508.15411", "categories": ["cs.SE", "cs.CL", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.15411", "abs": "https://arxiv.org/abs/2508.15411", "authors": ["Frederik Vandeputte"], "title": "Foundational Design Principles and Patterns for Building Robust and Adaptive GenAI-Native Systems", "comment": null, "summary": "Generative AI (GenAI) has emerged as a transformative technology,\ndemonstrating remarkable capabilities across diverse application domains.\nHowever, GenAI faces several major challenges in developing reliable and\nefficient GenAI-empowered systems due to its unpredictability and inefficiency.\nThis paper advocates for a paradigm shift: future GenAI-native systems should\nintegrate GenAI's cognitive capabilities with traditional software engineering\nprinciples to create robust, adaptive, and efficient systems.\n  We introduce foundational GenAI-native design principles centered around five\nkey pillars -- reliability, excellence, evolvability, self-reliance, and\nassurance -- and propose architectural patterns such as GenAI-native cells,\norganic substrates, and programmable routers to guide the creation of resilient\nand self-evolving systems. Additionally, we outline the key ingredients of a\nGenAI-native software stack and discuss the impact of these systems from\ntechnical, user adoption, economic, and legal perspectives, underscoring the\nneed for further validation and experimentation. Our work aims to inspire\nfuture research and encourage relevant communities to implement and refine this\nconceptual framework."}
{"id": "2508.15423", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15423", "abs": "https://arxiv.org/abs/2508.15423", "authors": ["Ruiqi Wang", "Zezhou Yang", "Cuiyun Gao", "Xin Xia", "Qing Liao"], "title": "An Empirical Study of Knowledge Distillation for Code Understanding Tasks", "comment": "Accepted by ICSE 2026 (Cycle 1)", "summary": "Pre-trained language models (PLMs) have emerged as powerful tools for code\nunderstanding. However, deploying these PLMs in large-scale applications faces\npractical challenges due to their computational intensity and inference\nlatency. Knowledge distillation (KD), a promising model compression and\nacceleration technique, addresses these limitations by transferring knowledge\nfrom large teacher models to compact student models, enabling efficient\ninference while preserving most of the teacher models' capabilities. While this\ntechnique has shown remarkable success in natural language processing and\ncomputer vision domains, its potential for code understanding tasks remains\nlargely underexplored.\n  In this paper, we systematically investigate the effectiveness and usage of\nKD in code understanding tasks. Our study encompasses two popular types of KD\nmethods, i.e., logit-based and feature-based KD methods, experimenting across\neight student models and two teacher PLMs from different domains on three\ndownstream tasks. The experimental results indicate that KD consistently offers\nnotable performance boosts across student models with different sizes compared\nwith standard fine-tuning. Notably, code-specific PLM demonstrates better\neffectiveness as the teacher model. Among all KD methods, the latest\nfeature-based KD methods exhibit superior performance, enabling student models\nto retain up to 98% teacher performance with merely 5% parameters. Regarding\nstudent architecture, our experiments reveal that similarity with teacher\narchitecture does not necessarily lead to better performance. We further\ndiscuss the efficiency and behaviors in the KD process and inference, summarize\nthe implications of findings, and identify promising future directions."}
{"id": "2508.15495", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.15495", "abs": "https://arxiv.org/abs/2508.15495", "authors": ["Dongjun Yu", "Xiao Yan", "Zhenrui Li", "Jipeng Xiao", "Haochuan He", "Yongda Yu", "Hao Zhang", "Guoping Rong", "Xiaobo Huang"], "title": "SynthCoder: A Synthetical Strategy to Tune LLMs for Code Completion", "comment": null, "summary": "Code completion is a prominent application of Large Language Models (LLMs) in\nsoftware engineering. Due to the near real-time response requirements of this\ntask, base models with small to medium-sized parameters are typically employed,\nsupplemented by various optimization and post-training techniques. However,\nthese optimization methods often have trade-offs, leading to a seesaw effect\nwhere performance improvements on certain datasets or metrics are accompanied\nby degradations on others -- sometimes even falling below the baseline model's\nperformance. This paper proposes SynthCoder, a model that integrates leading\nindustry practices to achieve state-of-the-art performance on the\nFill-in-the-Middle (FIM) code completion task. In specific, we first construct\na diverse dataset by combining Abstract Syntax Tree (AST) node extraction with\nheuristics that simulate developer behavior. Then we enrich our training corpus\nwith cross-file contextual information using the BM25 algorithm and call\ngraphs, enhancing the model's ability to perform code completion in both\nfile-level and repository-level scenarios. As the last step, we employ a\ntwo-stage training process using the Seed-Coder-8B-Base as the base model.\nFirst, we fine-tune the model using Curriculum Learning technology. Following\nthis, we perform alignment using Direct Preference Optimization (DPO) with\npreference pairs generated through Rejection Sampling. Experimental results\ndemonstrate that our final model excels on mainstream repository-level code\ncompletion benchmarks, including aiXcoder, ExecRepoBench, CrossCodeEval, and\nCoLT. Furthermore, our carefully curated training set effectively mitigates the\nmodel's tendency to just repeat existing code, a common issue existing in\nvarious code completion models."}
{"id": "2508.14923", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.14923", "abs": "https://arxiv.org/abs/2508.14923", "authors": ["Andrew Kiruluta"], "title": "A Fully Spectral Neuro-Symbolic Reasoning Architecture with Graph Signal Processing as the Computational Backbone", "comment": null, "summary": "We propose a fully spectral, neuro\\-symbolic reasoning architecture that\nleverages Graph Signal Processing (GSP) as the primary computational backbone\nfor integrating symbolic logic and neural inference. Unlike conventional\nreasoning models that treat spectral graph methods as peripheral components,\nour approach formulates the entire reasoning pipeline in the graph spectral\ndomain. Logical entities and relationships are encoded as graph signals,\nprocessed via learnable spectral filters that control multi-scale information\npropagation, and mapped into symbolic predicates for rule-based inference. We\npresent a complete mathematical framework for spectral reasoning, including\ngraph Fourier transforms, band-selective attention, and spectral rule\ngrounding. Experiments on benchmark reasoning datasets (ProofWriter,\nEntailmentBank, bAbI, CLUTRR, and ARC-Challenge) demonstrate improvements in\nlogical consistency, interpretability, and computational efficiency over\nstate\\-of\\-the\\-art neuro\\-symbolic models. Our results suggest that GSP\nprovides a mathematically grounded and computationally efficient substrate for\nrobust and interpretable reasoning systems."}
{"id": "2508.14925", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.14925", "abs": "https://arxiv.org/abs/2508.14925", "authors": ["Zhiqiang Wang", "Yichao Gao", "Yanting Wang", "Suyuan Liu", "Haifeng Sun", "Haoran Cheng", "Guanquan Shi", "Haohua Du", "Xiangyang Li"], "title": "MCPTox: A Benchmark for Tool Poisoning Attack on Real-World MCP Servers", "comment": null, "summary": "By providing a standardized interface for LLM agents to interact with\nexternal tools, the Model Context Protocol (MCP) is quickly becoming a\ncornerstone of the modern autonomous agent ecosystem. However, it creates novel\nattack surfaces due to untrusted external tools. While prior work has focused\non attacks injected through external tool outputs, we investigate a more\nfundamental vulnerability: Tool Poisoning, where malicious instructions are\nembedded within a tool's metadata without execution. To date, this threat has\nbeen primarily demonstrated through isolated cases, lacking a systematic,\nlarge-scale evaluation.\n  We introduce MCPTox, the first benchmark to systematically evaluate agent\nrobustness against Tool Poisoning in realistic MCP settings. MCPTox is\nconstructed upon 45 live, real-world MCP servers and 353 authentic tools. To\nachieve this, we design three distinct attack templates to generate a\ncomprehensive suite of 1312 malicious test cases by few-shot learning, covering\n10 categories of potential risks. Our evaluation on 20 prominent LLM agents\nsetting reveals a widespread vulnerability to Tool Poisoning, with o1-mini,\nachieving an attack success rate of 72.8\\%. We find that more capable models\nare often more susceptible, as the attack exploits their superior\ninstruction-following abilities. Finally, the failure case analysis reveals\nthat agents rarely refuse these attacks, with the highest refused rate\n(Claude-3.7-Sonnet) less than 3\\%, demonstrating that existing safety alignment\nis ineffective against malicious actions that use legitimate tools for\nunauthorized operation. Our findings create a crucial empirical baseline for\nunderstanding and mitigating this widespread threat, and we release MCPTox for\nthe development of verifiably safer AI agents. Our dataset is available at an\nanonymized repository: \\textit{https://anonymous.4open.science/r/AAAI26-7C02}."}
{"id": "2508.15496", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.15496", "abs": "https://arxiv.org/abs/2508.15496", "authors": ["Elena Masserini", "Diego Clerissi", "Daniela Micucci", "João R. Campos", "Leonardo Mariani"], "title": "Towards the Assessment of Task-based Chatbots: From the TOFU-R Snapshot to the BRASATO Curated Dataset", "comment": "10 pages, 10 figure, Accepted at IEEE International Symposium on\n  Software Reliability Engineering (ISSRE) 2025", "summary": "Task-based chatbots are increasingly being used to deliver real services, yet\nassessing their reliability, security, and robustness remains underexplored,\nalso due to the lack of large-scale, high-quality datasets. The emerging\nautomated quality assessment techniques targeting chatbots often rely on\nlimited pools of subjects, such as custom-made toy examples, or outdated, no\nlonger available, or scarcely popular agents, complicating the evaluation of\nsuch techniques. In this paper, we present two datasets and the tool support\nnecessary to create and maintain these datasets. The first dataset is RASA\nTASK-BASED CHATBOTS FROM GITHUB (TOFU-R), which is a snapshot of the Rasa\nchatbots available on GitHub, representing the state of the practice in\nopen-source chatbot development with Rasa. The second dataset is BOT RASA\nCOLLECTION (BRASATO), a curated selection of the most relevant chatbots for\ndialogue complexity, functional complexity, and utility, whose goal is to ease\nreproducibility and facilitate research on chatbot reliability."}
{"id": "2508.15013", "categories": ["cs.AI", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2508.15013", "abs": "https://arxiv.org/abs/2508.15013", "authors": ["Nadav Amir", "Stas Tiomkin", "Angela Langdon"], "title": "Goals and the Structure of Experience", "comment": null, "summary": "Purposeful behavior is a hallmark of natural and artificial intelligence. Its\nacquisition is often believed to rely on world models, comprising both\ndescriptive (what is) and prescriptive (what is desirable) aspects that\nidentify and evaluate state of affairs in the world, respectively. Canonical\ncomputational accounts of purposeful behavior, such as reinforcement learning,\nposit distinct components of a world model comprising a state representation\n(descriptive aspect) and a reward function (prescriptive aspect). However, an\nalternative possibility, which has not yet been computationally formulated, is\nthat these two aspects instead co-emerge interdependently from an agent's goal.\nHere, we describe a computational framework of goal-directed state\nrepresentation in cognitive agents, in which the descriptive and prescriptive\naspects of a world model co-emerge from agent-environment interaction\nsequences, or experiences. Drawing on Buddhist epistemology, we introduce a\nconstruct of goal-directed, or telic, states, defined as classes of\ngoal-equivalent experience distributions. Telic states provide a parsimonious\naccount of goal-directed learning in terms of the statistical divergence\nbetween behavioral policies and desirable experience features. We review\nempirical and theoretical literature supporting this novel perspective and\ndiscuss its potential to provide a unified account of behavioral,\nphenomenological and neural dimensions of purposeful behaviors across diverse\nsubstrates."}
{"id": "2508.15031", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.15031", "abs": "https://arxiv.org/abs/2508.15031", "authors": ["Kaixiang Zhao", "Lincan Li", "Kaize Ding", "Neil Zhenqiang Gong", "Yue Zhao", "Yushun Dong"], "title": "A Systematic Survey of Model Extraction Attacks and Defenses: State-of-the-Art and Perspectives", "comment": null, "summary": "Machine learning (ML) models have significantly grown in complexity and\nutility, driving advances across multiple domains. However, substantial\ncomputational resources and specialized expertise have historically restricted\ntheir wide adoption. Machine-Learning-as-a-Service (MLaaS) platforms have\naddressed these barriers by providing scalable, convenient, and affordable\naccess to sophisticated ML models through user-friendly APIs. While this\naccessibility promotes widespread use of advanced ML capabilities, it also\nintroduces vulnerabilities exploited through Model Extraction Attacks (MEAs).\nRecent studies have demonstrated that adversaries can systematically replicate\na target model's functionality by interacting with publicly exposed interfaces,\nposing threats to intellectual property, privacy, and system security. In this\npaper, we offer a comprehensive survey of MEAs and corresponding defense\nstrategies. We propose a novel taxonomy that classifies MEAs according to\nattack mechanisms, defense approaches, and computing environments. Our analysis\ncovers various attack techniques, evaluates their effectiveness, and highlights\nchallenges faced by existing defenses, particularly the critical trade-off\nbetween preserving model utility and ensuring security. We further assess MEAs\nwithin different computing paradigms and discuss their technical, ethical,\nlegal, and societal implications, along with promising directions for future\nresearch. This systematic survey aims to serve as a valuable reference for\nresearchers, practitioners, and policymakers engaged in AI security and\nprivacy. Additionally, we maintain an online repository continuously updated\nwith related literature at https://github.com/kzhao5/ModelExtractionPapers."}
{"id": "2508.15503", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.15503", "abs": "https://arxiv.org/abs/2508.15503", "authors": ["Sebastian Baltes", "Florian Angermeir", "Chetan Arora", "Marvin Muñoz Barón", "Chunyang Chen", "Lukas Böhme", "Fabio Calefato", "Neil Ernst", "Davide Falessi", "Brian Fitzgerald", "Davide Fucci", "Marcos Kalinowski", "Stefano Lambiase", "Daniel Russo", "Mircea Lungu", "Lutz Prechelt", "Paul Ralph", "Christoph Treude", "Stefan Wagner"], "title": "Evaluation Guidelines for Empirical Studies in Software Engineering involving LLMs", "comment": "Draft of evaluation guidelines for empirical studies in software\n  engineering involving LLMs (see also llm-guidelines.org)", "summary": "Large language models (LLMs) are increasingly being integrated into software\nengineering (SE) research and practice, yet their non-determinism, opaque\ntraining data, and evolving architectures complicate the reproduction and\nreplication of empirical studies. We present a community effort to scope this\nspace, introducing a taxonomy of LLM-based study types together with eight\nguidelines for designing and reporting empirical studies involving LLMs. The\nguidelines present essential (must) criteria as well as desired (should)\ncriteria and target transparency throughout the research process. Our\nrecommendations, contextualized by our study types, are: (1) to declare LLM\nusage and role; (2) to report model versions, configurations, and fine-tuning;\n(3) to document tool architectures; (4) to disclose prompts and interaction\nlogs; (5) to use human validation; (6) to employ an open LLM as a baseline; (7)\nto report suitable baselines, benchmarks, and metrics; and (8) to openly\narticulate limitations and mitigations. Our goal is to enable reproducibility\nand replicability despite LLM-specific barriers to open science. We maintain\nthe study types and guidelines online as a living resource for the community to\nuse and shape (llm-guidelines.org)."}
{"id": "2508.15030", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15030", "abs": "https://arxiv.org/abs/2508.15030", "authors": ["Ashmi Banerjee", "Fitri Nur Aisyah", "Adithi Satish", "Wolfgang Wörndl", "Yashar Deldjoo"], "title": "Collab-REC: An LLM-based Agentic Framework for Balancing Recommendations in Tourism", "comment": null, "summary": "We propose Collab-REC, a multi-agent framework designed to counteract\npopularity bias and enhance diversity in tourism recommendations. In our\nsetting, three LLM-based agents -- Personalization, Popularity, and\nSustainability generate city suggestions from complementary perspectives. A\nnon-LLM moderator then merges and refines these proposals via multi-round\nnegotiation, ensuring each agent's viewpoint is incorporated while penalizing\nspurious or repeated responses. Experiments on European city queries show that\nCollab-REC improves diversity and overall relevance compared to a single-agent\nbaseline, surfacing lesser-visited locales that often remain overlooked. This\nbalanced, context-aware approach addresses over-tourism and better aligns with\nconstraints provided by the user, highlighting the promise of multi-stakeholder\ncollaboration in LLM-driven recommender systems."}
{"id": "2508.15036", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15036", "abs": "https://arxiv.org/abs/2508.15036", "authors": ["Ruyi Ding", "Tianhong Xu", "Xinyi Shen", "Aidong Adam Ding", "Yunsi Fei"], "title": "MoEcho: Exploiting Side-Channel Attacks to Compromise User Privacy in Mixture-of-Experts LLMs", "comment": "This paper will appear in CCS 2025", "summary": "The transformer architecture has become a cornerstone of modern AI, fueling\nremarkable progress across applications in natural language processing,\ncomputer vision, and multimodal learning. As these models continue to scale\nexplosively for performance, implementation efficiency remains a critical\nchallenge. Mixture of Experts (MoE) architectures, selectively activating\nspecialized subnetworks (experts), offer a unique balance between model\naccuracy and computational cost. However, the adaptive routing in MoE\narchitectures, where input tokens are dynamically directed to specialized\nexperts based on their semantic meaning inadvertently opens up a new attack\nsurface for privacy breaches. These input-dependent activation patterns leave\ndistinctive temporal and spatial traces in hardware execution, which\nadversaries could exploit to deduce sensitive user data. In this work, we\npropose MoEcho, discovering a side channel analysis based attack surface that\ncompromises user privacy on MoE based systems. Specifically, in MoEcho, we\nintroduce four novel architectural side channels on different computing\nplatforms, including Cache Occupancy Channels and Pageout+Reload on CPUs, and\nPerformance Counter and TLB Evict+Reload on GPUs, respectively. Exploiting\nthese vulnerabilities, we propose four attacks that effectively breach user\nprivacy in large language models (LLMs) and vision language models (VLMs) based\non MoE architectures: Prompt Inference Attack, Response Reconstruction Attack,\nVisual Inference Attack, and Visual Reconstruction Attack. MoEcho is the first\nruntime architecture level security analysis of the popular MoE structure\ncommon in modern transformers, highlighting a serious security and privacy\nthreat and calling for effective and timely safeguards when harnessing MoE\nbased models for developing efficient large scale AI services."}
{"id": "2508.15512", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.15512", "abs": "https://arxiv.org/abs/2508.15512", "authors": ["Markus Borg", "Martin Larsson", "Philip Breid", "Nadim Hagatulah"], "title": "QUPER-MAn: Benchmark-Guided Target Setting for Maintainability Requirements", "comment": "Accepted at the 1st International Workshop on Responsible Software\n  Engineering", "summary": "Maintainable source code is essential for sustainable development in any\nsoftware organization. Unfortunately, many studies show that maintainability\noften receives less attention than its importance warrants. We argue that\nrequirements engineering can address this gap the problem by fostering\ndiscussions and setting appropriate targets in a responsible manner. In this\npreliminary work, we conducted an exploratory study of industry practices\nrelated to requirements engineering for maintainability. Our findings confirm\nprevious studies: maintainability remains a second-class quality concern.\nExplicit requirements often make sweeping references to coding conventions.\nTools providing maintainability proxies are common but typically only used in\nimplicit requirements related to engineering practices. To address this, we\npropose QUPER-MAn, a maintainability adaption of the QUPER model, which was\noriginally developed to help organizations set targets for performance\nrequirements. Developed using a design science approach, QUPER-MAn, integrates\nmaintainability benchmarks and supports target setting. We posit that it can\nshift maintainability from an overlooked development consequence to an actively\nmanaged goal driven by informed and responsible engineering decisions."}
{"id": "2508.15047", "categories": ["cs.AI", "cs.GR"], "pdf": "https://arxiv.org/pdf/2508.15047", "abs": "https://arxiv.org/abs/2508.15047", "authors": ["Yibo Liu", "Liam Shatzel", "Brandon Haworth", "Teseo Schneider"], "title": "Emergent Crowds Dynamics from Language-Driven Multi-Agent Interactions", "comment": null, "summary": "Animating and simulating crowds using an agent-based approach is a\nwell-established area where every agent in the crowd is individually controlled\nsuch that global human-like behaviour emerges. We observe that human navigation\nand movement in crowds are often influenced by complex social and environmental\ninteractions, driven mainly by language and dialogue. However, most existing\nwork does not consider these dimensions and leads to animations where\nagent-agent and agent-environment interactions are largely limited to steering\nand fixed higher-level goal extrapolation.\n  We propose a novel method that exploits large language models (LLMs) to\ncontrol agents' movement. Our method has two main components: a dialogue system\nand language-driven navigation. We periodically query agent-centric LLMs\nconditioned on character personalities, roles, desires, and relationships to\ncontrol the generation of inter-agent dialogue when necessitated by the spatial\nand social relationships with neighbouring agents. We then use the conversation\nand each agent's personality, emotional state, vision, and physical state to\ncontrol the navigation and steering of each agent. Our model thus enables\nagents to make motion decisions based on both their perceptual inputs and the\nongoing dialogue.\n  We validate our method in two complex scenarios that exemplify the interplay\nbetween social interactions, steering, and crowding. In these scenarios, we\nobserve that grouping and ungrouping of agents automatically occur.\nAdditionally, our experiments show that our method serves as an\ninformation-passing mechanism within the crowd. As a result, our framework\nproduces more realistic crowd simulations, with emergent group behaviours\narising naturally from any environmental setting."}
{"id": "2508.15042", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.15042", "abs": "https://arxiv.org/abs/2508.15042", "authors": ["Sima Arasteh", "Christophe Hauser"], "title": "When Machine Learning Meets Vulnerability Discovery: Challenges and Lessons Learned", "comment": null, "summary": "In recent years, machine learning has demonstrated impressive results in\nvarious fields, including software vulnerability detection. Nonetheless, using\nmachine learning to identify software vulnerabilities presents new challenges,\nespecially regarding the scale of data involved, which was not a factor in\ntraditional methods. Consequently, in spite of the rise of new\nmachine-learning-based approaches in that space, important shortcomings persist\nregarding their evaluation. First, researchers often fail to provide concrete\nstatistics about their training datasets, such as the number of samples for\neach type of vulnerability. Moreover, many methods rely on training with\nsemantically similar functions rather than directly on vulnerable programs.\nThis leads to uncertainty about the suitability of the datasets currently used\nfor training. Secondly, the choice of a model and the level of granularity at\nwhich models are trained also affect the effectiveness of such vulnerability\ndiscovery approaches.\n  In this paper, we explore the challenges of applying machine learning to\nvulnerability discovery. We also share insights from our two previous research\npapers, Bin2vec and BinHunter, which could enhance future research in this\nfield."}
{"id": "2508.15536", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.15536", "abs": "https://arxiv.org/abs/2508.15536", "authors": ["Yi Zhang", "He Jiang", "Xiaochen Li", "Shikai Guo", "Peiyu Zou", "Zun Wang"], "title": "A Novel Mutation Based Method for Detecting FPGA Logic Synthesis Tool Bugs", "comment": null, "summary": "FPGA (Field-Programmable Gate Array) logic synthesis tools are key components\nin the EDA (Electronic Design Automation) toolchain. They convert hardware\ndesigns written in description languages such as Verilog into gate-level\nrepresentations for FPGAs. However, defects in these tools may lead to\nunexpected behaviors and pose security risks. Therefore, it is crucial to\nharden these tools through testing. Although several methods have been proposed\nto automatically test FPGA logic synthesis tools, the challenge remains of\ninsufficient semantic and logical complexity in test programs. In this paper,\nwe propose VERMEI, a new method for testing FPGA logic synthesis tools. VERMEI\nconsists of three modules: preprocessing, equivalent mutation, and bug\nidentification. The preprocessing module identifies zombie logic (inactive code\nwith no impact on the circuit output) in seed programs through simulation and\ncoverage analysis. The equivalent mutation module generates equivalent variants\nof seed programs by pruning or inserting logic fragments in zombie areas. It\nuses Bayesian sampling to extract logic fragments from historical Verilog\ndesigns, making the generated variants have complex control flows and\nstructures. The bug identification module, based on differential testing,\ncompares the synthesized outputs of seed and variant programs to identify bugs.\nExperiments on Yosys, Vivado, and Quartus demonstrate that VERMEI outperforms\nthe state-of-the-art methods. Within five months, VERMEI reported 15 bugs to\nvendors, 9 of which were confirmed as new."}
{"id": "2508.15050", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.15050", "abs": "https://arxiv.org/abs/2508.15050", "authors": ["Romain Lacombe", "Kerrie Wu", "Eddie Dilworth"], "title": "Don't Think Twice! Over-Reasoning Impairs Confidence Calibration", "comment": "Published at ICML 2025 Workshop on Reliable and Responsible\n  Foundation Models", "summary": "Large Language Models deployed as question answering tools require robust\ncalibration to avoid overconfidence. We systematically evaluate how reasoning\ncapabilities and budget affect confidence assessment accuracy, using the\nClimateX dataset (Lacombe et al., 2023) and expanding it to human and planetary\nhealth. Our key finding challenges the \"test-time scaling\" paradigm: while\nrecent reasoning LLMs achieve 48.7% accuracy in assessing expert confidence,\nincreasing reasoning budgets consistently impairs rather than improves\ncalibration. Extended reasoning leads to systematic overconfidence that worsens\nwith longer thinking budgets, producing diminishing and negative returns beyond\nmodest computational investments. Conversely, search-augmented generation\ndramatically outperforms pure reasoning, achieving 89.3% accuracy by retrieving\nrelevant evidence. Our results suggest that information access, rather than\nreasoning depth or inference budget, may be the critical bottleneck for\nimproved confidence calibration of knowledge-intensive tasks."}
{"id": "2508.15089", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.15089", "abs": "https://arxiv.org/abs/2508.15089", "authors": ["Arun Ganesh"], "title": "Tighter Privacy Analysis for Truncated Poisson Sampling", "comment": null, "summary": "We give a new privacy amplification analysis for truncated Poisson sampling,\na Poisson sampling variant that truncates a batch if it exceeds a given maximum\nbatch size."}
{"id": "2508.15570", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.15570", "abs": "https://arxiv.org/abs/2508.15570", "authors": ["Marion Wiese", "Kamila Serwa", "Anastasia Besier", "Ariane S. Marion-Jetten", "Eva Bittner"], "title": "Establishing Technical Debt Management -- A Five-Step Workshop Approach and an Action Research Study", "comment": "Accepted for publication by the Journal of Systems and Software --\n  Special Issue on Managing Technical Debt in Software-Intensive Products and\n  Services", "summary": "Context. Technical debt (TD) items are constructs in a software system\nproviding short-term benefits but hindering future changes. TD management (TDM)\nis frequently researched but rarely adopted in practice. Goal. This study aimed\nto establish a TDM process in an IT company based on a predefined workshop\nconcept. We analyzed which research approaches practitioners adopted for each\nTD activity and the TDM's long-term effect on TD awareness. Method. We used\naction research (five action cycles in 16 months) with an IT team that creates\nIT solutions for signal processing. To examine TD awareness, we (1) analyzed\nquestionnaires completed during each workshop, (2) observed team meetings, (3)\nadopted a method from psychology for measuring awareness in decision-making\nsituations called TD-SAGAT, and (4) evaluated the backlog data. Results.\nPractitioners preferred TD repayment and prioritization based on the system's\nevolution and cost calculations, i.e., repayment of so-called low-hanging\nfruits. Reminders in the backlog items, such as checkboxes or text templates,\nled to a sustainable rise in TD awareness. Conclusions. We showed that a\nworkshop-based approach is feasible and leads to sustainable process changes.\nNew ideas for TDM applicable to other IT teams emerged, e.g., using a\nre-submission date, using a Talked about TD checkbox, and using visualizations\nfor TD prioritization."}
{"id": "2508.15053", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.15053", "abs": "https://arxiv.org/abs/2508.15053", "authors": ["Itai Zilberstein", "Alberto Candela", "Steve Chien", "David Rijlaarsdam", "Tom Hendrix", "Leonie Buckley", "Aubrey Dunne"], "title": "Demonstrating Onboard Inference for Earth Science Applications with Spectral Analysis Algorithms and Deep Learning", "comment": "International Symposium on Artificial Intelligence, Robotics and\n  Automation in Space, November 2024", "summary": "In partnership with Ubotica Technologies, the Jet Propulsion Laboratory is\ndemonstrating state-of-the-art data analysis onboard CogniSAT-6/HAMMER (CS-6).\nCS-6 is a satellite with a visible and near infrared range hyperspectral\ninstrument and neural network acceleration hardware. Performing data analysis\nat the edge (e.g. onboard) can enable new Earth science measurements and\nresponses. We will demonstrate data analysis and inference onboard CS-6 for\nnumerous applications using deep learning and spectral analysis algorithms."}
{"id": "2508.15100", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.15100", "abs": "https://arxiv.org/abs/2508.15100", "authors": ["Ehssan Mousavipour", "Andrey Dimanchev", "Majid Ghaderi"], "title": "Adaptive Anomaly Detection in Evolving Network Environments", "comment": null, "summary": "Distribution shift, a change in the statistical properties of data over time,\nposes a critical challenge for deep learning anomaly detection systems.\nExisting anomaly detection systems often struggle to adapt to these shifts.\nSpecifically, systems based on supervised learning require costly manual\nlabeling, while those based on unsupervised learning rely on clean data, which\nis difficult to obtain, for shift adaptation. Both of these requirements are\nchallenging to meet in practice. In this paper, we introduce NetSight, a\nframework for supervised anomaly detection in network data that continually\ndetects and adapts to distribution shifts in an online manner. NetSight\neliminates manual intervention through a novel pseudo-labeling technique and\nuses a knowledge distillation-based adaptation strategy to prevent catastrophic\nforgetting. Evaluated on three long-term network datasets, NetSight\ndemonstrates superior adaptation performance compared to state-of-the-art\nmethods that rely on manual labeling, achieving F1-score improvements of up to\n11.72%. This proves its robustness and effectiveness in dynamic networks that\nexperience distribution shifts over time."}
{"id": "2508.15584", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.15584", "abs": "https://arxiv.org/abs/2508.15584", "authors": ["Maria Teresa Rossi", "Leonardo Mariani", "Oliviero Riganelli"], "title": "From PREVENTion to REACTion: Enhancing Failure Resolution in Naval Systems", "comment": null, "summary": "Complex and large industrial systems often misbehave, for instance, due to\nwear, misuse, or faults. To cope with these incidents, it is important to\ntimely detect their occurrences, localize the sources of the problems, and\nimplement the appropriate countermeasures. This paper reports our experience\nwith a state-of-the-art failure prediction method, PREVENT, and its extension\nwith a troubleshooting module, REACT, applied to naval systems developed by\nFincantieri. Our results show how to integrate anomaly detection with\ntroubleshooting procedures. We conclude by discussing a lesson learned, which\nmay help deploy and extend these analyses to other industrial products."}
{"id": "2508.15068", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15068", "abs": "https://arxiv.org/abs/2508.15068", "authors": ["Shuang Ao", "Gopal Rumchurn"], "title": "S3LoRA: Safe Spectral Sharpness-Guided Pruning in Adaptation of Agent Planner", "comment": "9 pages, 2 figures", "summary": "Adapting Large Language Models (LLMs) using parameter-efficient fine-tuning\n(PEFT) techniques such as LoRA has enabled powerful capabilities in LLM-based\nagents. However, these adaptations can unintentionally compromise safety\nalignment, leading to unsafe or unstable behaviors, particularly in agent\nplanning tasks. Existing safety-aware adaptation methods often require access\nto both base and instruction-tuned model checkpoints, which are frequently\nunavailable in practice, limiting their applicability. We propose S3LoRA (Safe\nSpectral Sharpness-Guided Pruning LoRA), a lightweight, data-free, and\nmodel-independent framework that mitigates safety risks in LoRA-adapted models\nby inspecting only the fine-tuned weight updates. We first introduce\nMagnitude-Aware Spherically Normalized SVD (MAS-SVD), which robustly analyzes\nthe structural properties of LoRA updates while preserving global magnitude\ninformation. We then design the Spectral Sharpness Index (SSI), a\nsharpness-aware metric to detect layers with highly concentrated and\npotentially unsafe updates. These layers are pruned post-hoc to reduce risk\nwithout sacrificing task performance. Extensive experiments and ablation\nstudies across agent planning and language generation tasks show that S3LoRA\nconsistently improves safety metrics while maintaining or improving utility\nmetrics and significantly reducing inference cost. These results establish\nS3LoRA as a practical and scalable solution for safely deploying LLM-based\nagents in real-world, resource-constrained, and safety-critical environments."}
{"id": "2508.15172", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.15172", "abs": "https://arxiv.org/abs/2508.15172", "authors": ["Zheng Li", "Xiaoyang Dong", "Xiaoyun Wang"], "title": "Conditional Cube Attack on Round-Reduced ASCON", "comment": null, "summary": "This paper evaluates the secure level of authenticated encryption\n\\textsc{Ascon} against cube-like method. \\textsc{Ascon} submitted by Dobraunig\n\\emph{et~al.} is one of 16 survivors of the 3rd round CAESAR competition. The\ncube-like method is first used by Dinur \\emph{et~al.} to analyze Keccak keyed\nmodes. At CT-RSA 2015, Dobraunig \\emph{et~al.} applied this method to 5/6-round\nreduced \\textsc{Ascon}, whose structure is similar to Keccak keyed modes.\nHowever, for \\textsc{Ascon} the non-linear layer is more complex and state is\nmuch smaller, which make it hard for the attackers to select enough cube\nvariables that do not multiply with each other after the first round. This\nseems to be the reason why the best previous key-recovery attack is on 6-round\n\\textsc{Ascon}, while for Keccak keyed modes (Keccak-MAC and Keyak) the\nattacked round is no less than 7-round.\n  In this paper, we generalize the conditional cube attack proposed by Huang\n\\emph{et~al.}, and find new cubes depending on some key bit conditions for\n5/6-round reduced \\textsc{Ascon}, and translate the previous theoretic 6-round\nattack with $2^{66}$ time complexity to a practical one with $2^{40}$ time\ncomplexity. Moreover, we propose the first 7-round key-recovery attack on\n\\textsc{Ascon}. By introducing \\emph{the cube-like key-subset technique}, we\ndivide the full key space into many subsets according to different key\nconditions. For each key subset, we launch the cube tester to determine if the\nkey falls into it. Finally, we recover the full key space by testing all the\nkey subsets. The total time complexity is about $2^{103.9}$. In addition, for a\nweak-key subset, whose size is $2^{117}$, the attack is more efficient and\ncosts only $2^{77}$ time complexity. Those attacks do not threaten the full\nround (12 rounds) \\textsc{Ascon}."}
{"id": "2508.15386", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.15386", "abs": "https://arxiv.org/abs/2508.15386", "authors": ["Sabine Houy", "Bruno Kreyssig", "Timothee Riom", "Alexandre Bartel", "Patrick McDaniel"], "title": "A Practical Guideline and Taxonomy to LLVM's Control Flow Integrity", "comment": null, "summary": "Memory corruption vulnerabilities remain one of the most severe threats to\nsoftware security. They often allow attackers to achieve arbitrary code\nexecution by redirecting a vulnerable program's control flow. While Control\nFlow Integrity (CFI) has gained traction to mitigate this exploitation path,\ndevelopers are not provided with any direction on how to apply CFI to\nreal-world software. In this work, we establish a taxonomy mapping LLVM's\nforward-edge CFI variants to memory corruption vulnerability classes, offering\nactionable guidance for developers seeking to deploy CFI incrementally in\nexisting codebases. Based on the Top 10 Known Exploited Vulnerabilities (KEV)\nlist, we identify four high-impact vulnerability categories and select one\nrepresentative CVE for each. We evaluate LLVM's CFI against each CVE and\nexplain why CFI blocks exploitation in two cases while failing in the other\ntwo, illustrating its potential and current limitations. Our findings support\ninformed deployment decisions and provide a foundation for improving the\npractical use of CFI in production systems."}
{"id": "2508.15118", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15118", "abs": "https://arxiv.org/abs/2508.15118", "authors": ["Jennifer Leigh", "Dimitrios Letsios", "Alessandro Mella", "Lucio Machetti", "Francesca Toni"], "title": "Argumentation for Explainable Workforce Optimisation (with Appendix)", "comment": "Accepted to PAIS 2025", "summary": "Workforce management is a complex problem optimising the makespan and travel\ndistance required for a team of operators to complete a set of jobs, using a\nset of instruments. A crucial challenge in workforce management is\naccommodating changes at execution time so that explanations are provided to\nall stakeholders involved. Here, we show that, by understanding workforce\nmanagement as abstract argumentation in an industrial application, we can\naccommodate change and obtain faithful explanations. We show, with a user\nstudy, that our tool and explanations lead to faster and more accurate problem\nsolving than conventional solutions by hand."}
{"id": "2508.15183", "categories": ["cs.CR", "cs.DS"], "pdf": "https://arxiv.org/pdf/2508.15183", "abs": "https://arxiv.org/abs/2508.15183", "authors": ["Badih Ghazi", "Pritish Kamath", "Alexander Knop", "Ravi Kumar", "Pasin Manurangsi", "Chiyuan Zhang"], "title": "Private Hyperparameter Tuning with Ex-Post Guarantee", "comment": null, "summary": "The conventional approach in differential privacy (DP) literature formulates\nthe privacy-utility trade-off with a \"privacy-first\" perspective: for a\npredetermined level of privacy, a certain utility is achievable. However,\npractitioners often operate under a \"utility-first\" paradigm, prioritizing a\ndesired level of utility and then determining the corresponding privacy cost.\n  Wu et al. [2019] initiated a formal study of this \"utility-first\" perspective\nby introducing ex-post DP. They demonstrated that by adding correlated Laplace\nnoise and progressively reducing it on demand, a sequence of increasingly\naccurate estimates of a private parameter can be generated, with the privacy\ncost attributed only to the least noisy iterate released. This led to a Laplace\nmechanism variant that achieves a specified utility with minimal privacy loss.\nHowever, their work, and similar findings by Whitehouse et al. [2022], are\nprimarily limited to simple mechanisms based on Laplace or Gaussian noise.\n  In this paper, we significantly generalize these results. In particular, we\nextend the work of Wu et al. [2019] and Liu and Talwar [2019] to support any\nsequence of private estimators, incurring at most a doubling of the original\nprivacy budget. Furthermore, we demonstrate that hyperparameter tuning for\nthese estimators, including the selection of an optimal privacy budget, can be\nperformed without additional privacy cost. Finally, we extend our results to\nex-post Renyi DP, further broadening the applicability of utility-first privacy\nmechanisms."}
{"id": "2508.15119", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.15119", "abs": "https://arxiv.org/abs/2508.15119", "authors": ["Rachel Ma", "Jingyi Qu", "Andreea Bobu", "Dylan Hadfield-Menell"], "title": "Open-Universe Assistance Games", "comment": "7 pages + 2 pages references + 7 pages appendix", "summary": "Embodied AI agents must infer and act in an interpretable way on diverse\nhuman goals and preferences that are not predefined. To formalize this setting,\nwe introduce Open-Universe Assistance Games (OU-AGs), a framework where the\nagent must reason over an unbounded and evolving space of possible goals. In\nthis context, we introduce GOOD (GOals from Open-ended Dialogue), a\ndata-efficient, online method that extracts goals in the form of natural\nlanguage during an interaction with a human, and infers a distribution over\nnatural language goals. GOOD prompts an LLM to simulate users with different\ncomplex intents, using its responses to perform probabilistic inference over\ncandidate goals. This approach enables rich goal representations and\nuncertainty estimation without requiring large offline datasets. We evaluate\nGOOD in a text-based grocery shopping domain and in a text-operated simulated\nhousehold robotics environment (AI2Thor), using synthetic user profiles. Our\nmethod outperforms a baseline without explicit goal tracking, as confirmed by\nboth LLM-based and human evaluations."}
{"id": "2508.15252", "categories": ["cs.CR", "cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.15252", "abs": "https://arxiv.org/abs/2508.15252", "authors": ["Shiyi Yang", "Xinshu Li", "Guanglin Zhou", "Chen Wang", "Xiwei Xu", "Liming Zhu", "Lina Yao"], "title": "Retrieval-Augmented Review Generation for Poisoning Recommender Systems", "comment": null, "summary": "Recent studies have shown that recommender systems (RSs) are highly\nvulnerable to data poisoning attacks, where malicious actors inject fake user\nprofiles, including a group of well-designed fake ratings, to manipulate\nrecommendations. Due to security and privacy constraints in practice, attackers\ntypically possess limited knowledge of the victim system and thus need to craft\nprofiles that have transferability across black-box RSs. To maximize the attack\nimpact, the profiles often remains imperceptible. However, generating such\nhigh-quality profiles with the restricted resources is challenging. Some works\nsuggest incorporating fake textual reviews to strengthen the profiles; yet, the\npoor quality of the reviews largely undermines the attack effectiveness and\nimperceptibility under the practical setting.\n  To tackle the above challenges, in this paper, we propose to enhance the\nquality of the review text by harnessing in-context learning (ICL) capabilities\nof multimodal foundation models. To this end, we introduce a demonstration\nretrieval algorithm and a text style transfer strategy to augment the navie\nICL. Specifically, we propose a novel practical attack framework named RAGAN to\ngenerate high-quality fake user profiles, which can gain insights into the\nrobustness of RSs. The profiles are generated by a jailbreaker and\ncollaboratively optimized on an instructional agent and a guardian to improve\nthe attack transferability and imperceptibility. Comprehensive experiments on\nvarious real-world datasets demonstrate that RAGAN achieves the\nstate-of-the-art poisoning attack performance."}
{"id": "2508.15126", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.15126", "abs": "https://arxiv.org/abs/2508.15126", "authors": ["Pengsong Zhang", "Xiang Hu", "Guowei Huang", "Yang Qi", "Heng Zhang", "Xiuxu Li", "Jiaxing Song", "Jiabin Luo", "Yijiang Li", "Shuo Yin", "Chengxiao Dai", "Eric Hanchen Jiang", "Xiaoyan Zhou", "Zhenfei Yin", "Boqin Yuan", "Jing Dong", "Guinan Su", "Guanren Qiao", "Haiming Tang", "Anghong Du", "Lili Pan", "Zhenzhong Lan", "Xinyu Liu"], "title": "aiXiv: A Next-Generation Open Access Ecosystem for Scientific Discovery Generated by AI Scientists", "comment": "Preprint under review. Code is available at\n  https://github.com/aixiv-org. Website is available at\n  https://forms.gle/DxQgCtXFsJ4paMtn8", "summary": "Recent advances in large language models (LLMs) have enabled AI agents to\nautonomously generate scientific proposals, conduct experiments, author papers,\nand perform peer reviews. Yet this flood of AI-generated research content\ncollides with a fragmented and largely closed publication ecosystem.\nTraditional journals and conferences rely on human peer review, making them\ndifficult to scale and often reluctant to accept AI-generated research content;\nexisting preprint servers (e.g. arXiv) lack rigorous quality-control\nmechanisms. Consequently, a significant amount of high-quality AI-generated\nresearch lacks appropriate venues for dissemination, hindering its potential to\nadvance scientific progress. To address these challenges, we introduce aiXiv, a\nnext-generation open-access platform for human and AI scientists. Its\nmulti-agent architecture allows research proposals and papers to be submitted,\nreviewed, and iteratively refined by both human and AI scientists. It also\nprovides API and MCP interfaces that enable seamless integration of\nheterogeneous human and AI scientists, creating a scalable and extensible\necosystem for autonomous scientific discovery. Through extensive experiments,\nwe demonstrate that aiXiv is a reliable and robust platform that significantly\nenhances the quality of AI-generated research proposals and papers after\niterative revising and reviewing on aiXiv. Our work lays the groundwork for a\nnext-generation open-access ecosystem for AI scientists, accelerating the\npublication and dissemination of high-quality AI-generated research content.\nCode is available at https://github.com/aixiv-org. Website is available at\nhttps://forms.gle/DxQgCtXFsJ4paMtn8."}
{"id": "2508.15306", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.15306", "abs": "https://arxiv.org/abs/2508.15306", "authors": ["Henrietta Hegyi", "Laszlo Erdodi"], "title": "Connected and Exposed: Cybersecurity Risks, Regulatory Gaps, and Public Perception in Internet-Connected Vehicles", "comment": null, "summary": "The rapid advancement of Internet-connected vehicle technologies has\nintroduced a new era of smart mobility, while simultaneously raising\nsignificant cybersecurity and privacy concerns. This paper explores the\nevolving threat landscape associated with connected vehicles, focusing on risks\nsuch as unauthorized remote access and the potential leakage of personal data.\nTo assess the current state of protection, we conducted a comprehensive\nanalysis of 16 international standards and regulations, evaluating them from\nmultiple perspectives including regulatory strength, technical specificity,\ntreatment of supply chain risks, and approaches to personal data handling.\n  In parallel, we carried out a user-focused survey designed to map consumer\nattitudes toward smart cars. The survey investigated which types of vehicles\nusers trust and prefer, the reasons behind rejecting certain car types, their\nawareness of data-related risks, and whether they feel adequately informed\nabout how their vehicles handle data. By combining regulatory analysis with\nuser perception insights, this study aims to contribute to a more holistic\nunderstanding of the challenges and expectations surrounding connected vehicle\necosystems."}
{"id": "2508.15144", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15144", "abs": "https://arxiv.org/abs/2508.15144", "authors": ["Jiabo Ye", "Xi Zhang", "Haiyang Xu", "Haowei Liu", "Junyang Wang", "Zhaoqing Zhu", "Ziwei Zheng", "Feiyu Gao", "Junjie Cao", "Zhengxi Lu", "Jitong Liao", "Qi Zheng", "Fei Huang", "Jingren Zhou", "Ming Yan"], "title": "Mobile-Agent-v3: Foundamental Agents for GUI Automation", "comment": null, "summary": "This paper introduces GUI-Owl, a foundational GUI agent model that achieves\nstate-of-the-art performance among open-source end-to-end models on ten GUI\nbenchmarks across desktop and mobile environments, covering grounding, question\nanswering, planning, decision-making, and procedural knowledge. GUI-Owl-7B\nachieves 66.4 on AndroidWorld and 29.4 on OSWorld. Building on this, we propose\nMobile-Agent-v3, a general-purpose GUI agent framework that further improves\nperformance to 73.3 on AndroidWorld and 37.7 on OSWorld, setting a new\nstate-of-the-art for open-source GUI agent frameworks. GUI-Owl incorporates\nthree key innovations: (1) Large-scale Environment Infrastructure: a\ncloud-based virtual environment spanning Android, Ubuntu, macOS, and Windows,\nenabling our Self-Evolving GUI Trajectory Production framework. This generates\nhigh-quality interaction data via automated query generation and correctness\nvalidation, leveraging GUI-Owl to refine trajectories iteratively, forming a\nself-improving loop. It supports diverse data pipelines and reduces manual\nannotation. (2) Diverse Foundational Agent Capabilities: by integrating UI\ngrounding, planning, action semantics, and reasoning patterns, GUI-Owl supports\nend-to-end decision-making and can act as a modular component in multi-agent\nsystems. (3) Scalable Environment RL: we develop a scalable reinforcement\nlearning framework with fully asynchronous training for real-world alignment.\nWe also introduce Trajectory-aware Relative Policy Optimization (TRPO) for\nonline RL, achieving 34.9 on OSWorld. GUI-Owl and Mobile-Agent-v3 are\nopen-sourced at https://github.com/X-PLUG/MobileAgent."}
{"id": "2508.15310", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.15310", "abs": "https://arxiv.org/abs/2508.15310", "authors": ["Hengyu An", "Jinghuai Zhang", "Tianyu Du", "Chunyi Zhou", "Qingming Li", "Tao Lin", "Shouling Ji"], "title": "IPIGuard: A Novel Tool Dependency Graph-Based Defense Against Indirect Prompt Injection in LLM Agents", "comment": "EMNLP 2025", "summary": "Large language model (LLM) agents are widely deployed in real-world\napplications, where they leverage tools to retrieve and manipulate external\ndata for complex tasks. However, when interacting with untrusted data sources\n(e.g., fetching information from public websites), tool responses may contain\ninjected instructions that covertly influence agent behaviors and lead to\nmalicious outcomes, a threat referred to as Indirect Prompt Injection (IPI).\nExisting defenses typically rely on advanced prompting strategies or auxiliary\ndetection models. While these methods have demonstrated some effectiveness,\nthey fundamentally rely on assumptions about the model's inherent security,\nwhich lacks structural constraints on agent behaviors. As a result, agents\nstill retain unrestricted access to tool invocations, leaving them vulnerable\nto stronger attack vectors that can bypass the security guardrails of the\nmodel. To prevent malicious tool invocations at the source, we propose a novel\ndefensive task execution paradigm, called IPIGuard, which models the agents'\ntask execution process as a traversal over a planned Tool Dependency Graph\n(TDG). By explicitly decoupling action planning from interaction with external\ndata, IPIGuard significantly reduces unintended tool invocations triggered by\ninjected instructions, thereby enhancing robustness against IPI attacks.\nExperiments on the AgentDojo benchmark show that IPIGuard achieves a superior\nbalance between effectiveness and robustness, paving the way for the\ndevelopment of safer agentic systems in dynamic environments."}
{"id": "2508.15180", "categories": ["cs.AI", "I.2.7"], "pdf": "https://arxiv.org/pdf/2508.15180", "abs": "https://arxiv.org/abs/2508.15180", "authors": ["Kai Xiong", "Yanwei Huang", "Rongjunchen Zhang", "Kun Chen", "Haipang Wu"], "title": "PuzzleClone: An SMT-Powered Framework for Synthesizing Verifiable Data", "comment": null, "summary": "High-quality mathematical and logical datasets with verifiable answers are\nessential for strengthening the reasoning capabilities of large language models\n(LLMs). While recent data augmentation techniques have facilitated the creation\nof large-scale benchmarks, existing LLM-generated datasets often suffer from\nlimited reliability, diversity, and scalability. To address these challenges,\nwe introduce PuzzleClone, a formal framework for synthesizing verifiable data\nat scale using Satisfiability Modulo Theories (SMT). Our approach features\nthree key innovations: (1) encoding seed puzzles into structured logical\nspecifications, (2) generating scalable variants through systematic variable\nand constraint randomization, and (3) ensuring validity via a reproduction\nmechanism. Applying PuzzleClone, we construct a curated benchmark comprising\nover 83K diverse and programmatically validated puzzles. The generated puzzles\nspan a wide spectrum of difficulty and formats, posing significant challenges\nto current state-of-the-art models. We conduct post training (SFT and RL) on\nPuzzleClone datasets. Experimental results show that training on PuzzleClone\nyields substantial improvements not only on PuzzleClone testset but also on\nlogic and mathematical benchmarks. Post training raises PuzzleClone average\nfrom 14.4 to 56.2 and delivers consistent improvements across 7 logic and\nmathematical benchmarks up to 12.5 absolute percentage points (AMC2023 from\n52.5 to 65.0). Our code and data are available at\nhttps://github.com/puzzleclone."}
{"id": "2508.15386", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.15386", "abs": "https://arxiv.org/abs/2508.15386", "authors": ["Sabine Houy", "Bruno Kreyssig", "Timothee Riom", "Alexandre Bartel", "Patrick McDaniel"], "title": "A Practical Guideline and Taxonomy to LLVM's Control Flow Integrity", "comment": null, "summary": "Memory corruption vulnerabilities remain one of the most severe threats to\nsoftware security. They often allow attackers to achieve arbitrary code\nexecution by redirecting a vulnerable program's control flow. While Control\nFlow Integrity (CFI) has gained traction to mitigate this exploitation path,\ndevelopers are not provided with any direction on how to apply CFI to\nreal-world software. In this work, we establish a taxonomy mapping LLVM's\nforward-edge CFI variants to memory corruption vulnerability classes, offering\nactionable guidance for developers seeking to deploy CFI incrementally in\nexisting codebases. Based on the Top 10 Known Exploited Vulnerabilities (KEV)\nlist, we identify four high-impact vulnerability categories and select one\nrepresentative CVE for each. We evaluate LLVM's CFI against each CVE and\nexplain why CFI blocks exploitation in two cases while failing in the other\ntwo, illustrating its potential and current limitations. Our findings support\ninformed deployment decisions and provide a foundation for improving the\npractical use of CFI in production systems."}
{"id": "2508.15192", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.15192", "abs": "https://arxiv.org/abs/2508.15192", "authors": ["Wenjie Lin", "Jin Wei-Kocsis"], "title": "LLM4Sweat: A Trustworthy Large Language Model for Hyperhidrosis Support", "comment": null, "summary": "While large language models (LLMs) have shown promise in healthcare, their\napplication for rare medical conditions is still hindered by scarce and\nunreliable datasets for fine-tuning. Hyperhidrosis, a disorder causing\nexcessive sweating beyond physiological needs, is one such rare disorder,\naffecting 2-3% of the population and significantly impacting both physical\ncomfort and psychosocial well-being. To date, no work has tailored LLMs to\nadvance the diagnosis or care of hyperhidrosis. To address this gap, we present\nLLM4Sweat, an open-source and domain-specific LLM framework for trustworthy and\nempathetic hyperhidrosis support. The system follows a three-stage pipeline. In\nthe data augmentation stage, a frontier LLM generates medically plausible\nsynthetic vignettes from curated open-source data to create a diverse and\nbalanced question-answer dataset. In the fine-tuning stage, an open-source\nfoundation model is fine-tuned on the dataset to provide diagnosis,\npersonalized treatment recommendations, and empathetic psychological support.\nIn the inference and expert evaluation stage, clinical and psychological\nspecialists assess accuracy, appropriateness, and empathy, with validated\nresponses iteratively enriching the dataset. Experiments show that LLM4Sweat\noutperforms baselines and delivers the first open-source LLM framework for\nhyperhidrosis, offering a generalizable approach for other rare diseases with\nsimilar data and trustworthiness challenges."}
{"id": "2508.15541", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.15541", "abs": "https://arxiv.org/abs/2508.15541", "authors": ["Bingguang Lu", "Hongsheng Hu", "Yuantian Miao", "Shaleeza Sohail", "Chaoxiang He", "Shuo Wang", "Xiao Chen"], "title": "BadFU: Backdoor Federated Learning through Adversarial Machine Unlearning", "comment": null, "summary": "Federated learning (FL) has been widely adopted as a decentralized training\nparadigm that enables multiple clients to collaboratively learn a shared model\nwithout exposing their local data. As concerns over data privacy and regulatory\ncompliance grow, machine unlearning, which aims to remove the influence of\nspecific data from trained models, has become increasingly important in the\nfederated setting to meet legal, ethical, or user-driven demands. However,\nintegrating unlearning into FL introduces new challenges and raises largely\nunexplored security risks. In particular, adversaries may exploit the\nunlearning process to compromise the integrity of the global model. In this\npaper, we present the first backdoor attack in the context of federated\nunlearning, demonstrating that an adversary can inject backdoors into the\nglobal model through seemingly legitimate unlearning requests. Specifically, we\npropose BadFU, an attack strategy where a malicious client uses both backdoor\nand camouflage samples to train the global model normally during the federated\ntraining process. Once the client requests unlearning of the camouflage\nsamples, the global model transitions into a backdoored state. Extensive\nexperiments under various FL frameworks and unlearning strategies validate the\neffectiveness of BadFU, revealing a critical vulnerability in current federated\nunlearning practices and underscoring the urgent need for more secure and\nrobust federated unlearning mechanisms."}
{"id": "2508.15204", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15204", "abs": "https://arxiv.org/abs/2508.15204", "authors": ["Raj Jain", "Marc Wetter"], "title": "R-ConstraintBench: Evaluating LLMs on NP-Complete Scheduling", "comment": null, "summary": "Effective scheduling under tight resource, timing, and operational\nconstraints underpins large-scale planning across sectors such as capital\nprojects, manufacturing, logistics, and IT fleet transitions. However, the\nreliability of large language models (LLMs) when reasoning under\nhigh-constraint regimes is insufficiently characterized. To address this gap,\nwe present R-ConstraintBench, a scalable framework that evaluates models on\nResource-Constrained Project Scheduling Problems (RCPSP), an NP-Complete\nfeasibility class, while difficulty increases via linear growth in constraints.\nR-ConstraintBench incrementally increases non-redundant precedence constraints\nin Directed Acyclic Graphs (DAGs) and then introduces downtime, temporal\nwindows, and disjunctive constraints. As an illustrative example, we\ninstantiate the benchmark in a data center migration setting and evaluate\nmultiple LLMs using feasibility and error analysis, identifying degradation\nthresholds and constraint types most associated with failure. Empirically,\nstrong models are near-ceiling on precedence-only DAGs, but feasibility\nperformance collapses when downtime, temporal windows, and disjunctive\nconstraints interact, implicating constraint interaction, not graph depth, as\nthe principal bottleneck. Performance on clean synthetic ramps also does not\nguarantee transfer to domain-grounded scenarios, underscoring limited\ngeneralization."}
{"id": "2508.15606", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.15606", "abs": "https://arxiv.org/abs/2508.15606", "authors": ["Yu Yang", "Zhenyuan Li", "Xiandong Ran", "Jiahao Liu", "Jiahui Wang", "Bo Yu", "Shouling Ji"], "title": "Towards Scalable and Interpretable Mobile App Risk Analysis via Large Language Models", "comment": null, "summary": "Mobile application marketplaces are responsible for vetting apps to identify\nand mitigate security risks. Current vetting processes are labor-intensive,\nrelying on manual analysis by security professionals aided by semi-automated\ntools. To address this inefficiency, we propose Mars, a system that leverages\nLarge Language Models (LLMs) for automated risk identification and profiling.\nMars is designed to concurrently analyze multiple applications across diverse\nrisk categories with minimal human intervention. To enhance analytical\nprecision and operational efficiency, Mars leverages a pre-constructed risk\nidentification tree to extract relevant indicators from high-dimensional\napplication features. This initial step filters the data, reducing the input\nvolume for the LLM and mitigating the potential for model hallucination induced\nby irrelevant features. The extracted indicators are then subjected to LLM\nanalysis for final risk determination. Furthermore, Mars automatically\ngenerates a comprehensive evidence chain for each assessment, documenting the\nanalytical process to provide transparent justification. These chains are\ndesigned to facilitate subsequent manual review and to inform enforcement\ndecisions, such as application delisting. The performance of Mars was evaluated\non a real-world dataset from a partner Android marketplace. The results\ndemonstrate that Mars attained an F1-score of 0.838 in risk identification and\nan F1-score of 0.934 in evidence retrieval. To assess its practical\napplicability, a user study involving 20 expert analysts was conducted, which\nindicated that Mars yielded a substantial efficiency gain, ranging from 60% to\n90%, over conventional manual analysis."}
{"id": "2508.15222", "categories": ["cs.AI", "cs.CV", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.15222", "abs": "https://arxiv.org/abs/2508.15222", "authors": ["Hantao Zhang", "Jingyang Liu", "Ed Li"], "title": "See it. Say it. Sorted: Agentic System for Compositional Diagram Generation", "comment": null, "summary": "We study sketch-to-diagram generation: converting rough hand sketches into\nprecise, compositional diagrams. Diffusion models excel at photorealism but\nstruggle with the spatial precision, alignment, and symbolic structure required\nfor flowcharts. We introduce See it. Say it. Sorted., a training-free agentic\nsystem that couples a Vision-Language Model (VLM) with Large Language Models\n(LLMs) to produce editable Scalable Vector Graphics (SVG) programs. The system\nruns an iterative loop in which a Critic VLM proposes a small set of\nqualitative, relational edits; multiple candidate LLMs synthesize SVG updates\nwith diverse strategies (conservative->aggressive, alternative, focused); and a\nJudge VLM selects the best candidate, ensuring stable improvement. This design\nprioritizes qualitative reasoning over brittle numerical estimates, preserves\nglobal constraints (e.g., alignment, connectivity), and naturally supports\nhuman-in-the-loop corrections. On 10 sketches derived from flowcharts in\npublished papers, our method more faithfully reconstructs layout and structure\nthan two frontier closed-source image generation LLMs (GPT-5 and\nGemini-2.5-Pro), accurately composing primitives (e.g., multi-headed arrows)\nwithout inserting unwanted text. Because outputs are programmatic SVGs, the\napproach is readily extensible to presentation tools (e.g., PowerPoint) via\nAPIs and can be specialized with improved prompts and task-specific tools. The\ncodebase is open-sourced at\nhttps://github.com/hantaoZhangrichard/see_it_say_it_sorted.git."}
{"id": "2508.15240", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15240", "abs": "https://arxiv.org/abs/2508.15240", "authors": ["Sabab Aosaf", "Muhammad Ali Nayeem", "Afsana Haque", "M Sohel Rahmana"], "title": "Computational Intelligence based Land-use Allocation Approaches for Mixed Use Areas", "comment": null, "summary": "Urban land-use allocation represents a complex multi-objective optimization\nproblem critical for sustainable urban development policy. This paper presents\nnovel computational intelligence approaches for optimizing land-use allocation\nin mixed-use areas, addressing inherent trade-offs between land-use\ncompatibility and economic objectives. We develop multiple optimization\nalgorithms, including custom variants integrating differential evolution with\nmulti-objective genetic algorithms. Key contributions include: (1) CR+DES\nalgorithm leveraging scaled difference vectors for enhanced exploration, (2)\nsystematic constraint relaxation strategy improving solution quality while\nmaintaining feasibility, and (3) statistical validation using Kruskal-Wallis\ntests with compact letter displays. Applied to a real-world case study with\n1,290 plots, CR+DES achieves 3.16\\% improvement in land-use compatibility\ncompared to state-of-the-art methods, while MSBX+MO excels in price\noptimization with 3.3\\% improvement. Statistical analysis confirms algorithms\nincorporating difference vectors significantly outperform traditional\napproaches across multiple metrics. The constraint relaxation technique enables\nbroader solution space exploration while maintaining practical constraints.\nThese findings provide urban planners and policymakers with evidence-based\ncomputational tools for balancing competing objectives in land-use allocation,\nsupporting more effective urban development policies in rapidly urbanizing\nregions."}
{"id": "2508.15294", "categories": ["cs.AI", "cs.CL", "cs.MA", "I.2.7"], "pdf": "https://arxiv.org/pdf/2508.15294", "abs": "https://arxiv.org/abs/2508.15294", "authors": ["Gaoke Zhang", "Bo Wang", "Yunlong Ma", "Dongming Zhao", "Zifei Yu"], "title": "Multiple Memory Systems for Enhancing the Long-term Memory of Agent", "comment": null, "summary": "An agent powered by large language models have achieved impressive results,\nbut effectively handling the vast amounts of historical data generated during\ninteractions remains a challenge. The current approach is to design a memory\nmodule for the agent to process these data. However, existing methods, such as\nMemoryBank and A-MEM, have poor quality of stored memory content, which affects\nrecall performance and response quality. In order to better construct\nhigh-quality long-term memory content, we have designed a multiple memory\nsystem (MMS) inspired by cognitive psychology theory. The system processes\nshort-term memory to multiple long-term memory fragments, and constructs\nretrieval memory units and contextual memory units based on these fragments,\nwith a one-to-one correspondence between the two. During the retrieval phase,\nMMS will match the most relevant retrieval memory units based on the user's\nquery. Then, the corresponding contextual memory units is obtained as the\ncontext for the response stage to enhance knowledge, thereby effectively\nutilizing historical data. Experiments on LoCoMo dataset compared our method\nwith three others, proving its effectiveness. Ablation studies confirmed the\nrationality of our memory units. We also analyzed the robustness regarding the\nnumber of selected memory segments and the storage overhead, demonstrating its\npractical value."}
{"id": "2508.15305", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15305", "abs": "https://arxiv.org/abs/2508.15305", "authors": ["Wei Yang", "Jinwei Xiao", "Hongming Zhang", "Qingyang Zhang", "Yanna Wang", "Bo Xu"], "title": "Coarse-to-Fine Grounded Memory for LLM Agent Planning", "comment": "Accepted to EMNLP 2025 Main Conference;27 pages,15 figures", "summary": "Recent advancements in Large Language Models (LLMs) have driven growing\ninterest in LLM-based agents for complex planning tasks. To avoid costly agent\ntraining, many studies adopted memory mechanism that enhances LLM with offline\nexperiences or online trajectory analysis. However, existing works focus on\nsingle-granularity memory derived from dynamic environmental interactions,\nwhich are inherently constrained by the quality of the collected experiences.\nThis limitation, in turn, constrain the diversity of knowledge and the\nflexibility of planning. We propose Coarse-to-Fine Grounded Memory (\\Ours{}), a\nnovel framework that grounds coarse-to-fine memories with LLM, thereby fully\nleverage them for flexible adaptation to diverse scenarios. \\Ours{} grounds\nenvironmental information into coarse-grained focus points to guide experience\ncollection in training tasks, followed by grounding of actionable\nhybrid-grained tips from each experience. At inference, \\Ours{} retrieves\ntask-relevant experiences and tips to support planning. When facing\nenvironmental anomalies, the LLM grounds the current situation into\nfine-grained key information, enabling flexible self-QA reflection and plan\ncorrection."}
{"id": "2508.15327", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.15327", "abs": "https://arxiv.org/abs/2508.15327", "authors": ["Xiancheng Gao", "Yufeng Shi", "Wengang Zhou", "Houqiang Li"], "title": "Search-Based Credit Assignment for Offline Preference-Based Reinforcement Learning", "comment": "7 pages, 6 figures, under review", "summary": "Offline reinforcement learning refers to the process of learning policies\nfrom fixed datasets, without requiring additional environment interaction.\nHowever, it often relies on well-defined reward functions, which are difficult\nand expensive to design. Human feedback is an appealing alternative, but its\ntwo common forms, expert demonstrations and preferences, have complementary\nlimitations. Demonstrations provide stepwise supervision, but they are costly\nto collect and often reflect limited expert behavior modes. In contrast,\npreferences are easier to collect, but it is unclear which parts of a behavior\ncontribute most to a trajectory segment, leaving credit assignment unresolved.\nIn this paper, we introduce a Search-Based Preference Weighting (SPW) scheme to\nunify these two feedback sources. For each transition in a preference labeled\ntrajectory, SPW searches for the most similar state-action pairs from expert\ndemonstrations and directly derives stepwise importance weights based on their\nsimilarity scores. These weights are then used to guide standard preference\nlearning, enabling more accurate credit assignment that traditional approaches\nstruggle to achieve. We demonstrate that SPW enables effective joint learning\nfrom preferences and demonstrations, outperforming prior methods that leverage\nboth feedback types on challenging robot manipulation tasks."}
{"id": "2508.15335", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15335", "abs": "https://arxiv.org/abs/2508.15335", "authors": ["Bin Deng", "Yizhe Feng", "Zeming Liu", "Qing Wei", "Xiangrong Zhu", "Shuai Chen", "Yuanfang Guo", "Yunhong Wang"], "title": "RETAIL: Towards Real-world Travel Planning for Large Language Models", "comment": null, "summary": "Although large language models have enhanced automated travel planning\nabilities, current systems remain misaligned with real-world scenarios. First,\nthey assume users provide explicit queries, while in reality requirements are\noften implicit. Second, existing solutions ignore diverse environmental factors\nand user preferences, limiting the feasibility of plans. Third, systems can\nonly generate plans with basic POI arrangements, failing to provide all-in-one\nplans with rich details. To mitigate these challenges, we construct a novel\ndataset \\textbf{RETAIL}, which supports decision-making for implicit queries\nwhile covering explicit queries, both with and without revision needs. It also\nenables environmental awareness to ensure plan feasibility under real-world\nscenarios, while incorporating detailed POI information for all-in-one travel\nplans. Furthermore, we propose a topic-guided multi-agent framework, termed\nTGMA. Our experiments reveal that even the strongest existing model achieves\nmerely a 1.0% pass rate, indicating real-world travel planning remains\nextremely challenging. In contrast, TGMA demonstrates substantially improved\nperformance 2.72%, offering promising directions for real-world travel\nplanning."}
{"id": "2508.15338", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.15338", "abs": "https://arxiv.org/abs/2508.15338", "authors": ["Jinning Yang", "Wen Shi"], "title": "DiagECG: An LLM-Driven Framework for Diagnostic Reasoning via Discretized ECG Tokenization", "comment": null, "summary": "Electrocardiography plays a central role in cardiovascular diagnostics, yet\nexisting automated approaches often struggle to generalize across clinical\ntasks and offer limited support for open-ended reasoning. We present DiagECG, a\nnovel framework that integrates time-series and language modeling by enabling\nlarge language models to process 12-lead ECG signals for clinical text\ngeneration tasks. Our approach discretizes continuous ECG embeddings into\nsymbolic tokens using a lead-independent encoder and quantization module. These\ntokens are then used to extend the vocabulary of LLM, allowing the model to\nhandle both ECG and natural language inputs in a unified manner. To bridge the\nmodality gap, we pretrain the model on an autoregressive ECG forecasting task,\nenabling the LLM to model temporal dynamics using its native language modeling\ncapabilities. Finally, we perform instruction tuning on both ECG question\nanswering and diagnostic report generation. Without modifying the core model,\nDiagECG achieves strong performance across tasks while maintaining\ngeneralization to out-of-distribution settings. Extensive experiments\ndemonstrate the effectiveness of each component and highlight the potential of\nintegrating symbolic ECG representations into LLMs for medical reasoning."}
{"id": "2508.15358", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15358", "abs": "https://arxiv.org/abs/2508.15358", "authors": ["Alberto Pozanco", "Marianela Morales", "Daniel Borrajo", "Manuela Veloso"], "title": "Planning with Minimal Disruption", "comment": null, "summary": "In many planning applications, we might be interested in finding plans that\nminimally modify the initial state to achieve the goals. We refer to this\nconcept as plan disruption. In this paper, we formally introduce it, and define\nvarious planning-based compilations that aim to jointly optimize both the sum\nof action costs and plan disruption. Experimental results in different\nbenchmarks show that the reformulated task can be effectively solved in\npractice to generate plans that balance both objectives."}
{"id": "2508.15432", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.15432", "abs": "https://arxiv.org/abs/2508.15432", "authors": ["Bidyapati Pradhan", "Surajit Dasgupta", "Amit Kumar Saha", "Omkar Anustoop", "Sriram Puttagunta", "Vipul Mittal", "Gopal Sarda"], "title": "GraSP: A Unified Graph-Based Framework for Scalable Generation, Quality Tagging, and Management of Synthetic Data for SFT and DPO", "comment": null, "summary": "The advancement of large language models (LLMs) is critically dependent on\nthe availability of high-quality datasets for Supervised Fine-Tuning (SFT),\nalignment tasks like Direct Preference Optimization (DPO), etc. In this work,\nwe present a comprehensive synthetic data generation framework that facilitates\nscalable, configurable, and high-fidelity generation of synthetic data tailored\nfor these training paradigms. Our approach employs a modular and\nconfiguration-based pipeline capable of modeling complex dialogue flows with\nminimal manual intervention. This framework uses a dual-stage quality tagging\nmechanism, combining heuristic rules and LLM-based evaluations, to\nautomatically filter and score data extracted from OASST-formatted\nconversations, ensuring the curation of high-quality dialogue samples. The\nresulting datasets are structured under a flexible schema supporting both SFT\nand DPO use cases, enabling seamless integration into diverse training\nworkflows. Together, these innovations offer a robust solution for generating\nand managing synthetic conversational data at scale, significantly reducing the\noverhead of data preparation in LLM training pipelines."}
{"id": "2508.15447", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15447", "abs": "https://arxiv.org/abs/2508.15447", "authors": ["Zihao Wang", "Junming Zhang"], "title": "From Bits to Boardrooms: A Cutting-Edge Multi-Agent LLM Framework for Business Excellence", "comment": "Accepted by ECAI 2025", "summary": "Large Language Models (LLMs) have shown promising potential in business\napplications, particularly in enterprise decision support and strategic\nplanning, yet current approaches often struggle to reconcile intricate\noperational analyses with overarching strategic goals across diverse market\nenvironments, leading to fragmented workflows and reduced collaboration across\norganizational levels. This paper introduces BusiAgent, a novel multi-agent\nframework leveraging LLMs for advanced decision-making in complex corporate\nenvironments. BusiAgent integrates three core innovations: an extended\nContinuous Time Markov Decision Process (CTMDP) for dynamic agent modeling, a\ngeneralized entropy measure to optimize collaborative efficiency, and a\nmulti-level Stackelberg game to handle hierarchical decision processes.\nAdditionally, contextual Thompson sampling is employed for prompt optimization,\nsupported by a comprehensive quality assurance system to mitigate errors.\nExtensive empirical evaluations across diverse business scenarios validate\nBusiAgent's efficacy, demonstrating its capacity to generate coherent,\nclient-focused solutions that smoothly integrate granular insights with\nhigh-level strategy, significantly outperforming established approaches in both\nsolution quality and user satisfaction. By fusing cutting-edge AI technologies\nwith deep business insights, BusiAgent marks a substantial step forward in\nAI-driven enterprise decision-making, empowering organizations to navigate\ncomplex business landscapes more effectively."}
{"id": "2508.15507", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.15507", "abs": "https://arxiv.org/abs/2508.15507", "authors": ["Yekun Zhu", "Guang Chen", "Chengjun Mao"], "title": "Think in Blocks: Adaptive Reasoning from Direct Response to Deep Reasoning", "comment": null, "summary": "Large Language Models (LLMs) with chains-of-thought have demonstrated strong\nperformance on an increasing range of tasks, particularly those involving\ncomplex logical reasoning. However, excessively long chains can lead to\noverthinking, causing computational waste and slower responses. This raises a\nquestion: can LLMs dynamically adjust the length of their reasoning processes\nbased on task complexity? To address this, we propose the Think in Blocks\nframework, which enables adaptive reasoning-from zero to deep reasoning-by\npartitioning the reasoning process into a tunable number of blocks. Our main\ncontributions are: (1) Establishing an explicit block-structured paradigm in\nwhich the model first predicts an integer reasoning budget-the number of\nblocks-and then partitions its reasoning accordingly; (2) Training an adaptive\nmodel through a three-stage pipeline-Supervised Fine-Tuning, reward-guided\nDirect Preference Optimization, and Reinforcement Learning-that adjusts its\nreasoning depth to problem difficulty; (3) Exploiting the explicit block count\nto dynamically control reasoning depth at inference time, allowing flexible\nadjustment of chain-of-thought length during deployment."}
{"id": "2508.15510", "categories": ["cs.AI", "I.2.11; I.2.0; J.4; K.4.0; I.2.6"], "pdf": "https://arxiv.org/pdf/2508.15510", "abs": "https://arxiv.org/abs/2508.15510", "authors": ["Filippo Tonini", "Lukas Galke"], "title": "Super-additive Cooperation in Language Model Agents", "comment": "FAIEMA 2025", "summary": "With the prospect of autonomous artificial intelligence (AI) agents, studying\ntheir tendency for cooperative behavior becomes an increasingly relevant topic.\nThis study is inspired by the super-additive cooperation theory, where the\ncombined effects of repeated interactions and inter-group rivalry have been\nargued to be the cause for cooperative tendencies found in humans. We devised a\nvirtual tournament where language model agents, grouped into teams, face each\nother in a Prisoner's Dilemma game. By simulating both internal team dynamics\nand external competition, we discovered that this blend substantially boosts\nboth overall and initial, one-shot cooperation levels (the tendency to\ncooperate in one-off interactions). This research provides a novel framework\nfor large language models to strategize and act in complex social scenarios and\noffers evidence for how intergroup competition can, counter-intuitively, result\nin more cooperative behavior. These insights are crucial for designing future\nmulti-agent AI systems that can effectively work together and better align with\nhuman values. Source code is available at\nhttps://github.com/pippot/Superadditive-cooperation-LLMs."}
{"id": "2508.15548", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15548", "abs": "https://arxiv.org/abs/2508.15548", "authors": ["Jiayi Song", "Rui Wan", "Lipeng Ma", "Weidong Yang", "Qingyuan Zhou", "Yixuan Li", "Ben Fei"], "title": "DeepThink3D: Enhancing Large Language Models with Programmatic Reasoning in Complex 3D Situated Reasoning Tasks", "comment": null, "summary": "This work enhances the ability of large language models (LLMs) to perform\ncomplex reasoning in 3D scenes. Recent work has addressed the 3D situated\nreasoning task by invoking tool usage through large language models. Large\nlanguage models call tools via APIs and integrate the generated programs\nthrough a chain of thought to solve problems based on the program results.\nHowever, due to the simplicity of the questions in the dataset, the generated\nprogram reasoning chains are relatively short. To solve this main challenge, in\nthis paper, we introduce DeepThink3D to enhance the tool usage of LLMs in\ncomplex 3D situated reasoning tasks. Our work proposes a combinatorial and\niterative evolutionary approach on the SQA3D benchmark to generate more complex\nquestions. Building on this foundation, we fine-tune the large language model\nto make it more proficient in using 3D tools. By employing Direct Preference\nOptimization (DPO), we directly optimize the toolchain strategies generated by\nmodels, thereby enhancing their accuracy in complex tasks."}
{"id": "2508.15588", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15588", "abs": "https://arxiv.org/abs/2508.15588", "authors": ["Ahmed Nasir", "Abdelhafid Zenati"], "title": "A Dynamical Systems Framework for Reinforcement Learning Safety and Robustness Verification", "comment": null, "summary": "The application of reinforcement learning to safety-critical systems is\nlimited by the lack of formal methods for verifying the robustness and safety\nof learned policies. This paper introduces a novel framework that addresses\nthis gap by analyzing the combination of an RL agent and its environment as a\ndiscrete-time autonomous dynamical system. By leveraging tools from dynamical\nsystems theory, specifically the Finite-Time Lyapunov Exponent (FTLE), we\nidentify and visualize Lagrangian Coherent Structures (LCS) that act as the\nhidden \"skeleton\" governing the system's behavior. We demonstrate that\nrepelling LCS function as safety barriers around unsafe regions, while\nattracting LCS reveal the system's convergence properties and potential failure\nmodes, such as unintended \"trap\" states. To move beyond qualitative\nvisualization, we introduce a suite of quantitative metrics, Mean Boundary\nRepulsion (MBR), Aggregated Spurious Attractor Strength (ASAS), and\nTemporally-Aware Spurious Attractor Strength (TASAS), to formally measure a\npolicy's safety margin and robustness. We further provide a method for deriving\nlocal stability guarantees and extend the analysis to handle model uncertainty.\nThrough experiments in both discrete and continuous control environments, we\nshow that this framework provides a comprehensive and interpretable assessment\nof policy behavior, successfully identifying critical flaws in policies that\nappear successful based on reward alone."}
{"id": "2508.15610", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.15610", "abs": "https://arxiv.org/abs/2508.15610", "authors": ["Alfio Gliozzo", "Naweed Khan", "Christodoulos Constantinides", "Nandana Mihindukulasooriya", "Nahuel Defosse", "Junkyu Lee"], "title": "Transduction is All You Need for Structured Data Workflows", "comment": "32 pages, 8 figures", "summary": "This paper introduces Agentics, a modular framework for building agent-based\nsystems capable of structured reasoning and compositional generalization over\ncomplex data. Designed with research and practical applications in mind,\nAgentics offers a novel perspective on working with data and AI workflows. In\nthis framework, agents are abstracted from the logical flow and they are used\ninternally to the data type to enable logical transduction among data. Agentics\nencourages AI developers to focus on modeling data rather than crafting\nprompts, enabling a declarative language in which data types are provided by\nLLMs and composed through logical transduction, which is executed by LLMs when\ntypes are connected. We provide empirical evidence demonstrating the\napplicability of this framework across domain-specific multiple-choice question\nanswering, semantic parsing for text-to-SQL, and automated prompt optimization\ntasks, achieving state-of-the-art accuracy or improved scalability without\nsacrificing performance. The open-source implementation is available at\n\\texttt{https://github.com/IBM/agentics}."}
{"id": "2508.15630", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15630", "abs": "https://arxiv.org/abs/2508.15630", "authors": ["Meera Ray", "Christopher L. Dancy"], "title": "Adapting A Vector-Symbolic Memory for Lisp ACT-R", "comment": "6 pages. 5 figures. Submitted and accepted to the 23rd International\n  Conference on Cognitive Modeling (ICCM 2025)", "summary": "Holographic Declarative Memory (HDM) is a vector-symbolic alternative to\nACT-R's Declarative Memory (DM) system that can bring advantages such as\nscalability and architecturally defined similarity between DM chunks. We\nadapted HDM to work with the most comprehensive and widely-used implementation\nof ACT-R (Lisp ACT-R) so extant ACT-R models designed with DM can be run with\nHDM without major changes. With this adaptation of HDM, we have developed\nvector-based versions of common ACT-R functions, set up a text processing\npipeline to add the contents of large documents to ACT-R memory, and most\nsignificantly created a useful and novel mechanism to retrieve an entire chunk\nof memory based on a request using only vector representations of tokens.\nPreliminary results indicate that we can maintain vector-symbolic advantages of\nHDM (e.g., chunk recall without storing the actual chunk and other advantages\nwith scaling) while also extending it so that previous ACT-R models may work\nwith the system with little (or potentially no) modifications within the actual\nprocedural and declarative memory portions of a model. As a part of iterative\nimprovement of this newly translated holographic declarative memory module, we\nwill continue to explore better time-context representations for vectors to\nimprove the module's ability to reconstruct chunks during recall. To more fully\ntest this translated HDM module, we also plan to develop decision-making models\nthat use instance-based learning (IBL) theory, which is a useful application of\nHDM given the advantages of the system."}
{"id": "2508.15652", "categories": ["cs.AI", "cs.IT", "cs.LG", "cs.MA", "math.IT"], "pdf": "https://arxiv.org/pdf/2508.15652", "abs": "https://arxiv.org/abs/2508.15652", "authors": ["Ardian Selmonaj", "Miroslav Strupl", "Oleg Szehr", "Alessandro Antonucci"], "title": "Understanding Action Effects through Instrumental Empowerment in Multi-Agent Reinforcement Learning", "comment": "European Conference on Artificial Intelligence (ECAI) 2025", "summary": "To reliably deploy Multi-Agent Reinforcement Learning (MARL) systems, it is\ncrucial to understand individual agent behaviors within a team. While prior\nwork typically evaluates overall team performance based on explicit reward\nsignals or learned value functions, it is unclear how to infer agent\ncontributions in the absence of any value feedback. In this work, we\ninvestigate whether meaningful insights into agent behaviors can be extracted\nthat are consistent with the underlying value functions, solely by analyzing\nthe policy distribution. Inspired by the phenomenon that intelligent agents\ntend to pursue convergent instrumental values, which generally increase the\nlikelihood of task success, we introduce Intended Cooperation Values (ICVs), a\nmethod based on information-theoretic Shapley values for quantifying each\nagent's causal influence on their co-players' instrumental empowerment.\nSpecifically, ICVs measure an agent's action effect on its teammates' policies\nby assessing their decision uncertainty and preference alignment. The analysis\nacross cooperative and competitive MARL environments reveals the extent to\nwhich agents adopt similar or diverse strategies. By comparing action effects\nbetween policies and value functions, our method identifies which agent\nbehaviors are beneficial to team success, either by fostering deterministic\ndecisions or by preserving flexibility for future action choices. Our proposed\nmethod offers novel insights into cooperation dynamics and enhances\nexplainability in MARL systems."}
{"id": "2508.15680", "categories": ["cs.AI", "cs.HC", "I.2.6; I.2.11; K.4.1; K.6.0"], "pdf": "https://arxiv.org/pdf/2508.15680", "abs": "https://arxiv.org/abs/2508.15680", "authors": ["Mark Cote", "Susana Aires"], "title": "Futurity as Infrastructure: A Techno-Philosophical Interpretation of the AI Lifecycle", "comment": "15 pages, 3 figures, Presented at IAIL 2025 - Imagining the AI\n  Landscape after the AI Act, 4th International Workshop on Imagining the AI\n  Landscape After the AI Act, The fourth International Conference on Hybrid\n  Human-Artificial Intelligence", "summary": "This paper argues that a techno-philosophical reading of the EU AI Act\nprovides insight into the long-term dynamics of data in AI systems,\nspecifically, how the lifecycle from ingestion to deployment generates\nrecursive value chains that challenge existing frameworks for Responsible AI.\nWe introduce a conceptual tool to frame the AI pipeline, spanning data,\ntraining regimes, architectures, feature stores, and transfer learning. Using\ncross-disciplinary methods, we develop a technically grounded and\nphilosophically coherent analysis of regulatory blind spots. Our central claim\nis that what remains absent from policymaking is an account of the dynamic of\nbecoming that underpins both the technical operation and economic logic of AI.\nTo address this, we advance a formal reading of AI inspired by Simondonian\nphilosophy of technology, reworking his concept of individuation to model the\nAI lifecycle, including the pre-individual milieu, individuation, and\nindividuated AI. To translate these ideas, we introduce futurity: the\nself-reinforcing lifecycle of AI, where more data enhances performance, deepens\npersonalisation, and expands application domains. Futurity highlights the\nrecursively generative, non-rivalrous nature of data, underpinned by\ninfrastructures like feature stores that enable feedback, adaptation, and\ntemporal recursion. Our intervention foregrounds escalating power asymmetries,\nparticularly the tech oligarchy whose infrastructures of capture, training, and\ndeployment concentrate value and decision-making. We argue that effective\nregulation must address these infrastructural and temporal dynamics, and\npropose measures including lifecycle audits, temporal traceability, feedback\naccountability, recursion transparency, and a right to contest recursive reuse."}
{"id": "2508.15690", "categories": ["cs.AI", "cs.LG", "cs.MM"], "pdf": "https://arxiv.org/pdf/2508.15690", "abs": "https://arxiv.org/abs/2508.15690", "authors": ["Abhigya Verma", "Sriram Puttagunta", "Seganrasan Subramanian", "Sravan Ramachandran"], "title": "GRAFT: GRaPH and Table Reasoning for Textual Alignment -- A Benchmark for Structured Instruction Following and Visual Reasoning", "comment": "23 pages, 9 tables, 3 figures", "summary": "GRAFT is a structured multimodal benchmark for evaluating models on\ninstruction-following, visual reasoning, and visual-textual alignment tasks. It\nfeatures programmatically generated charts and synthetically rendered tables,\ncreated with Python visualization libraries to ensure control over data\nsemantics, structure, and clarity. Each GRAFT instance pairs a chart or table\nimage with a systematically generated, multi-step analytical question based\nsolely on visual content. Answers are provided in structured formats such as\nJSON or YAML, supporting consistent evaluation of both reasoning and output\nformat. The benchmark introduces a taxonomy of reasoning types including\ncomparison, trend identification, ranking, aggregation, proportion estimation,\nand anomaly detection to enable comprehensive assessment. Reference answers\nfollow strict factual and formatting guidelines for precise, aspect-based\nevaluation. GRAFT offers a unified, scalable framework for fine-grained\nbenchmarking of multimodal models on visually grounded, structured reasoning\ntasks, setting a new evaluation standard in this field."}
{"id": "2508.15693", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15693", "abs": "https://arxiv.org/abs/2508.15693", "authors": ["Wilka Carvalho", "Vikram Goddla", "Ishaan Sinha", "Hoon Shin", "Kunal Jha"], "title": "NiceWebRL: a Python library for human subject experiments with reinforcement learning environments", "comment": null, "summary": "We present NiceWebRL, a research tool that enables researchers to use machine\nreinforcement learning (RL) environments for online human subject experiments.\nNiceWebRL is a Python library that allows any Jax-based environment to be\ntransformed into an online interface, supporting both single-agent and\nmulti-agent environments. As such, NiceWebRL enables AI researchers to compare\ntheir algorithms to human performance, cognitive scientists to test ML\nalgorithms as theories for human cognition, and multi-agent researchers to\ndevelop algorithms for human-AI collaboration. We showcase NiceWebRL with 3\ncase studies that demonstrate its potential to help develop Human-like AI,\nHuman-compatible AI, and Human-assistive AI. In the first case study\n(Human-like AI), NiceWebRL enables the development of a novel RL model of\ncognition. Here, NiceWebRL facilitates testing this model against human\nparticipants in both a grid world and Craftax, a 2D Minecraft domain. In our\nsecond case study (Human-compatible AI), NiceWebRL enables the development of a\nnovel multi-agent RL algorithm that can generalize to human partners in the\nOvercooked domain. Finally, in our third case study (Human-assistive AI), we\nshow how NiceWebRL can allow researchers to study how an LLM can assist humans\non complex tasks in XLand-Minigrid, an environment with millions of\nhierarchical tasks. The library is available at\nhttps://github.com/KempnerInstitute/nicewebrl."}
{"id": "2508.15734", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15734", "abs": "https://arxiv.org/abs/2508.15734", "authors": ["Cooper Elsworth", "Keguo Huang", "David Patterson", "Ian Schneider", "Robert Sedivy", "Savannah Goodman", "Ben Townsend", "Parthasarathy Ranganathan", "Jeff Dean", "Amin Vahdat", "Ben Gomes", "James Manyika"], "title": "Measuring the environmental impact of delivering AI at Google Scale", "comment": null, "summary": "The transformative power of AI is undeniable - but as user adoption\naccelerates, so does the need to understand and mitigate the environmental\nimpact of AI serving. However, no studies have measured AI serving\nenvironmental metrics in a production environment. This paper addresses this\ngap by proposing and executing a comprehensive methodology for measuring the\nenergy usage, carbon emissions, and water consumption of AI inference workloads\nin a large-scale, AI production environment. Our approach accounts for the full\nstack of AI serving infrastructure - including active AI accelerator power,\nhost system energy, idle machine capacity, and data center energy overhead.\nThrough detailed instrumentation of Google's AI infrastructure for serving the\nGemini AI assistant, we find the median Gemini Apps text prompt consumes 0.24\nWh of energy - a figure substantially lower than many public estimates. We also\nshow that Google's software efficiency efforts and clean energy procurement\nhave driven a 33x reduction in energy consumption and a 44x reduction in carbon\nfootprint for the median Gemini Apps text prompt over one year. We identify\nthat the median Gemini Apps text prompt uses less energy than watching nine\nseconds of television (0.24 Wh) and consumes the equivalent of five drops of\nwater (0.26 mL). While these impacts are low compared to other daily\nactivities, reducing the environmental impact of AI serving continues to\nwarrant important attention. Towards this objective, we propose that a\ncomprehensive measurement of AI serving environmental metrics is critical for\naccurately comparing models, and to properly incentivize efficiency gains\nacross the full AI serving stack."}
{"id": "2508.15748", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15748", "abs": "https://arxiv.org/abs/2508.15748", "authors": ["Emma Rath", "Stuart Armstrong", "Rebecca Gorman"], "title": "Response and Prompt Evaluation to Prevent Parasocial Relationships with Chatbots", "comment": null, "summary": "The development of parasocial relationships with AI agents has severe, and in\nsome cases, tragic effects for human well-being. Yet preventing such dynamics\nis challenging: parasocial cues often emerge gradually in private\nconversations, and not all forms of emotional engagement are inherently\nharmful. We address this challenge by introducing a simple response evaluation\nframework, created by repurposing a state-of-the-art language model, that\nevaluates ongoing conversations for parasocial cues in real time. To test the\nfeasibility of this approach, we constructed a small synthetic dataset of\nthirty dialogues spanning parasocial, sycophantic, and neutral conversations.\nIterative evaluation with five stage testing successfully identified all\nparasocial conversations while avoiding false positives under a tolerant\nunanimity rule, with detection typically occurring within the first few\nexchanges. These findings provide preliminary evidence that evaluation agents\ncan provide a viable solution for the prevention of parasocial relations."}
{"id": "2508.15757", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.15757", "abs": "https://arxiv.org/abs/2508.15757", "authors": ["Yuxing Lu", "Yucheng Hu", "Nan Sun", "Xukai Zhao"], "title": "Language-Guided Tuning: Enhancing Numeric Optimization with Textual Feedback", "comment": "9 pages, 4 figures, 4 tables", "summary": "Configuration optimization remains a critical bottleneck in machine learning,\nrequiring coordinated tuning across model architecture, training strategy,\nfeature engineering, and hyperparameters. Traditional approaches treat these\ndimensions independently and lack interpretability, while recent automated\nmethods struggle with dynamic adaptability and semantic reasoning about\noptimization decisions. We introduce Language-Guided Tuning (LGT), a novel\nframework that employs multi-agent Large Language Models to intelligently\noptimize configurations through natural language reasoning. We apply textual\ngradients - qualitative feedback signals that complement numerical optimization\nby providing semantic understanding of training dynamics and configuration\ninterdependencies. LGT coordinates three specialized agents: an Advisor that\nproposes configuration changes, an Evaluator that assesses progress, and an\nOptimizer that refines the decision-making process, creating a self-improving\nfeedback loop. Through comprehensive evaluation on six diverse datasets, LGT\ndemonstrates substantial improvements over traditional optimization methods,\nachieving performance gains while maintaining high interpretability."}
{"id": "2508.15031", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.15031", "abs": "https://arxiv.org/abs/2508.15031", "authors": ["Kaixiang Zhao", "Lincan Li", "Kaize Ding", "Neil Zhenqiang Gong", "Yue Zhao", "Yushun Dong"], "title": "A Systematic Survey of Model Extraction Attacks and Defenses: State-of-the-Art and Perspectives", "comment": null, "summary": "Machine learning (ML) models have significantly grown in complexity and\nutility, driving advances across multiple domains. However, substantial\ncomputational resources and specialized expertise have historically restricted\ntheir wide adoption. Machine-Learning-as-a-Service (MLaaS) platforms have\naddressed these barriers by providing scalable, convenient, and affordable\naccess to sophisticated ML models through user-friendly APIs. While this\naccessibility promotes widespread use of advanced ML capabilities, it also\nintroduces vulnerabilities exploited through Model Extraction Attacks (MEAs).\nRecent studies have demonstrated that adversaries can systematically replicate\na target model's functionality by interacting with publicly exposed interfaces,\nposing threats to intellectual property, privacy, and system security. In this\npaper, we offer a comprehensive survey of MEAs and corresponding defense\nstrategies. We propose a novel taxonomy that classifies MEAs according to\nattack mechanisms, defense approaches, and computing environments. Our analysis\ncovers various attack techniques, evaluates their effectiveness, and highlights\nchallenges faced by existing defenses, particularly the critical trade-off\nbetween preserving model utility and ensuring security. We further assess MEAs\nwithin different computing paradigms and discuss their technical, ethical,\nlegal, and societal implications, along with promising directions for future\nresearch. This systematic survey aims to serve as a valuable reference for\nresearchers, practitioners, and policymakers engaged in AI security and\nprivacy. Additionally, we maintain an online repository continuously updated\nwith related literature at https://github.com/kzhao5/ModelExtractionPapers."}
{"id": "2508.15036", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15036", "abs": "https://arxiv.org/abs/2508.15036", "authors": ["Ruyi Ding", "Tianhong Xu", "Xinyi Shen", "Aidong Adam Ding", "Yunsi Fei"], "title": "MoEcho: Exploiting Side-Channel Attacks to Compromise User Privacy in Mixture-of-Experts LLMs", "comment": "This paper will appear in CCS 2025", "summary": "The transformer architecture has become a cornerstone of modern AI, fueling\nremarkable progress across applications in natural language processing,\ncomputer vision, and multimodal learning. As these models continue to scale\nexplosively for performance, implementation efficiency remains a critical\nchallenge. Mixture of Experts (MoE) architectures, selectively activating\nspecialized subnetworks (experts), offer a unique balance between model\naccuracy and computational cost. However, the adaptive routing in MoE\narchitectures, where input tokens are dynamically directed to specialized\nexperts based on their semantic meaning inadvertently opens up a new attack\nsurface for privacy breaches. These input-dependent activation patterns leave\ndistinctive temporal and spatial traces in hardware execution, which\nadversaries could exploit to deduce sensitive user data. In this work, we\npropose MoEcho, discovering a side channel analysis based attack surface that\ncompromises user privacy on MoE based systems. Specifically, in MoEcho, we\nintroduce four novel architectural side channels on different computing\nplatforms, including Cache Occupancy Channels and Pageout+Reload on CPUs, and\nPerformance Counter and TLB Evict+Reload on GPUs, respectively. Exploiting\nthese vulnerabilities, we propose four attacks that effectively breach user\nprivacy in large language models (LLMs) and vision language models (VLMs) based\non MoE architectures: Prompt Inference Attack, Response Reconstruction Attack,\nVisual Inference Attack, and Visual Reconstruction Attack. MoEcho is the first\nruntime architecture level security analysis of the popular MoE structure\ncommon in modern transformers, highlighting a serious security and privacy\nthreat and calling for effective and timely safeguards when harnessing MoE\nbased models for developing efficient large scale AI services."}
{"id": "2508.15310", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.15310", "abs": "https://arxiv.org/abs/2508.15310", "authors": ["Hengyu An", "Jinghuai Zhang", "Tianyu Du", "Chunyi Zhou", "Qingming Li", "Tao Lin", "Shouling Ji"], "title": "IPIGuard: A Novel Tool Dependency Graph-Based Defense Against Indirect Prompt Injection in LLM Agents", "comment": "EMNLP 2025", "summary": "Large language model (LLM) agents are widely deployed in real-world\napplications, where they leverage tools to retrieve and manipulate external\ndata for complex tasks. However, when interacting with untrusted data sources\n(e.g., fetching information from public websites), tool responses may contain\ninjected instructions that covertly influence agent behaviors and lead to\nmalicious outcomes, a threat referred to as Indirect Prompt Injection (IPI).\nExisting defenses typically rely on advanced prompting strategies or auxiliary\ndetection models. While these methods have demonstrated some effectiveness,\nthey fundamentally rely on assumptions about the model's inherent security,\nwhich lacks structural constraints on agent behaviors. As a result, agents\nstill retain unrestricted access to tool invocations, leaving them vulnerable\nto stronger attack vectors that can bypass the security guardrails of the\nmodel. To prevent malicious tool invocations at the source, we propose a novel\ndefensive task execution paradigm, called IPIGuard, which models the agents'\ntask execution process as a traversal over a planned Tool Dependency Graph\n(TDG). By explicitly decoupling action planning from interaction with external\ndata, IPIGuard significantly reduces unintended tool invocations triggered by\ninjected instructions, thereby enhancing robustness against IPI attacks.\nExperiments on the AgentDojo benchmark show that IPIGuard achieves a superior\nbalance between effectiveness and robustness, paving the way for the\ndevelopment of safer agentic systems in dynamic environments."}
{"id": "2508.15423", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15423", "abs": "https://arxiv.org/abs/2508.15423", "authors": ["Ruiqi Wang", "Zezhou Yang", "Cuiyun Gao", "Xin Xia", "Qing Liao"], "title": "An Empirical Study of Knowledge Distillation for Code Understanding Tasks", "comment": "Accepted by ICSE 2026 (Cycle 1)", "summary": "Pre-trained language models (PLMs) have emerged as powerful tools for code\nunderstanding. However, deploying these PLMs in large-scale applications faces\npractical challenges due to their computational intensity and inference\nlatency. Knowledge distillation (KD), a promising model compression and\nacceleration technique, addresses these limitations by transferring knowledge\nfrom large teacher models to compact student models, enabling efficient\ninference while preserving most of the teacher models' capabilities. While this\ntechnique has shown remarkable success in natural language processing and\ncomputer vision domains, its potential for code understanding tasks remains\nlargely underexplored.\n  In this paper, we systematically investigate the effectiveness and usage of\nKD in code understanding tasks. Our study encompasses two popular types of KD\nmethods, i.e., logit-based and feature-based KD methods, experimenting across\neight student models and two teacher PLMs from different domains on three\ndownstream tasks. The experimental results indicate that KD consistently offers\nnotable performance boosts across student models with different sizes compared\nwith standard fine-tuning. Notably, code-specific PLM demonstrates better\neffectiveness as the teacher model. Among all KD methods, the latest\nfeature-based KD methods exhibit superior performance, enabling student models\nto retain up to 98% teacher performance with merely 5% parameters. Regarding\nstudent architecture, our experiments reveal that similarity with teacher\narchitecture does not necessarily lead to better performance. We further\ndiscuss the efficiency and behaviors in the KD process and inference, summarize\nthe implications of findings, and identify promising future directions."}
{"id": "2508.14925", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.14925", "abs": "https://arxiv.org/abs/2508.14925", "authors": ["Zhiqiang Wang", "Yichao Gao", "Yanting Wang", "Suyuan Liu", "Haifeng Sun", "Haoran Cheng", "Guanquan Shi", "Haohua Du", "Xiangyang Li"], "title": "MCPTox: A Benchmark for Tool Poisoning Attack on Real-World MCP Servers", "comment": null, "summary": "By providing a standardized interface for LLM agents to interact with\nexternal tools, the Model Context Protocol (MCP) is quickly becoming a\ncornerstone of the modern autonomous agent ecosystem. However, it creates novel\nattack surfaces due to untrusted external tools. While prior work has focused\non attacks injected through external tool outputs, we investigate a more\nfundamental vulnerability: Tool Poisoning, where malicious instructions are\nembedded within a tool's metadata without execution. To date, this threat has\nbeen primarily demonstrated through isolated cases, lacking a systematic,\nlarge-scale evaluation.\n  We introduce MCPTox, the first benchmark to systematically evaluate agent\nrobustness against Tool Poisoning in realistic MCP settings. MCPTox is\nconstructed upon 45 live, real-world MCP servers and 353 authentic tools. To\nachieve this, we design three distinct attack templates to generate a\ncomprehensive suite of 1312 malicious test cases by few-shot learning, covering\n10 categories of potential risks. Our evaluation on 20 prominent LLM agents\nsetting reveals a widespread vulnerability to Tool Poisoning, with o1-mini,\nachieving an attack success rate of 72.8\\%. We find that more capable models\nare often more susceptible, as the attack exploits their superior\ninstruction-following abilities. Finally, the failure case analysis reveals\nthat agents rarely refuse these attacks, with the highest refused rate\n(Claude-3.7-Sonnet) less than 3\\%, demonstrating that existing safety alignment\nis ineffective against malicious actions that use legitimate tools for\nunauthorized operation. Our findings create a crucial empirical baseline for\nunderstanding and mitigating this widespread threat, and we release MCPTox for\nthe development of verifiably safer AI agents. Our dataset is available at an\nanonymized repository: \\textit{https://anonymous.4open.science/r/AAAI26-7C02}."}
{"id": "2508.15031", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.15031", "abs": "https://arxiv.org/abs/2508.15031", "authors": ["Kaixiang Zhao", "Lincan Li", "Kaize Ding", "Neil Zhenqiang Gong", "Yue Zhao", "Yushun Dong"], "title": "A Systematic Survey of Model Extraction Attacks and Defenses: State-of-the-Art and Perspectives", "comment": null, "summary": "Machine learning (ML) models have significantly grown in complexity and\nutility, driving advances across multiple domains. However, substantial\ncomputational resources and specialized expertise have historically restricted\ntheir wide adoption. Machine-Learning-as-a-Service (MLaaS) platforms have\naddressed these barriers by providing scalable, convenient, and affordable\naccess to sophisticated ML models through user-friendly APIs. While this\naccessibility promotes widespread use of advanced ML capabilities, it also\nintroduces vulnerabilities exploited through Model Extraction Attacks (MEAs).\nRecent studies have demonstrated that adversaries can systematically replicate\na target model's functionality by interacting with publicly exposed interfaces,\nposing threats to intellectual property, privacy, and system security. In this\npaper, we offer a comprehensive survey of MEAs and corresponding defense\nstrategies. We propose a novel taxonomy that classifies MEAs according to\nattack mechanisms, defense approaches, and computing environments. Our analysis\ncovers various attack techniques, evaluates their effectiveness, and highlights\nchallenges faced by existing defenses, particularly the critical trade-off\nbetween preserving model utility and ensuring security. We further assess MEAs\nwithin different computing paradigms and discuss their technical, ethical,\nlegal, and societal implications, along with promising directions for future\nresearch. This systematic survey aims to serve as a valuable reference for\nresearchers, practitioners, and policymakers engaged in AI security and\nprivacy. Additionally, we maintain an online repository continuously updated\nwith related literature at https://github.com/kzhao5/ModelExtractionPapers."}
{"id": "2508.15036", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15036", "abs": "https://arxiv.org/abs/2508.15036", "authors": ["Ruyi Ding", "Tianhong Xu", "Xinyi Shen", "Aidong Adam Ding", "Yunsi Fei"], "title": "MoEcho: Exploiting Side-Channel Attacks to Compromise User Privacy in Mixture-of-Experts LLMs", "comment": "This paper will appear in CCS 2025", "summary": "The transformer architecture has become a cornerstone of modern AI, fueling\nremarkable progress across applications in natural language processing,\ncomputer vision, and multimodal learning. As these models continue to scale\nexplosively for performance, implementation efficiency remains a critical\nchallenge. Mixture of Experts (MoE) architectures, selectively activating\nspecialized subnetworks (experts), offer a unique balance between model\naccuracy and computational cost. However, the adaptive routing in MoE\narchitectures, where input tokens are dynamically directed to specialized\nexperts based on their semantic meaning inadvertently opens up a new attack\nsurface for privacy breaches. These input-dependent activation patterns leave\ndistinctive temporal and spatial traces in hardware execution, which\nadversaries could exploit to deduce sensitive user data. In this work, we\npropose MoEcho, discovering a side channel analysis based attack surface that\ncompromises user privacy on MoE based systems. Specifically, in MoEcho, we\nintroduce four novel architectural side channels on different computing\nplatforms, including Cache Occupancy Channels and Pageout+Reload on CPUs, and\nPerformance Counter and TLB Evict+Reload on GPUs, respectively. Exploiting\nthese vulnerabilities, we propose four attacks that effectively breach user\nprivacy in large language models (LLMs) and vision language models (VLMs) based\non MoE architectures: Prompt Inference Attack, Response Reconstruction Attack,\nVisual Inference Attack, and Visual Reconstruction Attack. MoEcho is the first\nruntime architecture level security analysis of the popular MoE structure\ncommon in modern transformers, highlighting a serious security and privacy\nthreat and calling for effective and timely safeguards when harnessing MoE\nbased models for developing efficient large scale AI services."}
{"id": "2508.15042", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.15042", "abs": "https://arxiv.org/abs/2508.15042", "authors": ["Sima Arasteh", "Christophe Hauser"], "title": "When Machine Learning Meets Vulnerability Discovery: Challenges and Lessons Learned", "comment": null, "summary": "In recent years, machine learning has demonstrated impressive results in\nvarious fields, including software vulnerability detection. Nonetheless, using\nmachine learning to identify software vulnerabilities presents new challenges,\nespecially regarding the scale of data involved, which was not a factor in\ntraditional methods. Consequently, in spite of the rise of new\nmachine-learning-based approaches in that space, important shortcomings persist\nregarding their evaluation. First, researchers often fail to provide concrete\nstatistics about their training datasets, such as the number of samples for\neach type of vulnerability. Moreover, many methods rely on training with\nsemantically similar functions rather than directly on vulnerable programs.\nThis leads to uncertainty about the suitability of the datasets currently used\nfor training. Secondly, the choice of a model and the level of granularity at\nwhich models are trained also affect the effectiveness of such vulnerability\ndiscovery approaches.\n  In this paper, we explore the challenges of applying machine learning to\nvulnerability discovery. We also share insights from our two previous research\npapers, Bin2vec and BinHunter, which could enhance future research in this\nfield."}
{"id": "2508.15135", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.15135", "abs": "https://arxiv.org/abs/2508.15135", "authors": ["Sumudu Liyanage", "Sherlock A. Licorish", "Markus Wagner", "Stephen G. MacDonell"], "title": "On the need to perform comprehensive evaluations of automated program repair benchmarks: Sorald case study", "comment": null, "summary": "In supporting the development of high-quality software, especially necessary\nin the era of LLMs, automated program repair (APR) tools aim to improve code\nquality by automatically addressing violations detected by static analysis\nprofilers. Previous research tends to evaluate APR tools only for their ability\nto clear violations, neglecting their potential introduction of new (sometimes\nsevere) violations, changes to code functionality and degrading of code\nstructure. There is thus a need for research to develop and assess\ncomprehensive evaluation frameworks for APR tools. This study addresses this\nresearch gap, and evaluates Sorald (a state-of-the-art APR tool) as a proof of\nconcept. Sorald's effectiveness was evaluated in repairing 3,529 SonarQube\nviolations across 30 rules within 2,393 Java code snippets extracted from Stack\nOverflow. Outcomes show that while Sorald fixes specific rule violations, it\nintroduced 2,120 new faults (32 bugs, 2088 code smells), reduced code\nfunctional correctness--as evidenced by a 24% unit test failure rate--and\ndegraded code structure, demonstrating the utility of our framework. Findings\nemphasize the need for evaluation methodologies that capture the full spectrum\nof APR tool effects, including side effects, to ensure their safe and effective\nadoption."}
{"id": "2508.14923", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.14923", "abs": "https://arxiv.org/abs/2508.14923", "authors": ["Andrew Kiruluta"], "title": "A Fully Spectral Neuro-Symbolic Reasoning Architecture with Graph Signal Processing as the Computational Backbone", "comment": null, "summary": "We propose a fully spectral, neuro\\-symbolic reasoning architecture that\nleverages Graph Signal Processing (GSP) as the primary computational backbone\nfor integrating symbolic logic and neural inference. Unlike conventional\nreasoning models that treat spectral graph methods as peripheral components,\nour approach formulates the entire reasoning pipeline in the graph spectral\ndomain. Logical entities and relationships are encoded as graph signals,\nprocessed via learnable spectral filters that control multi-scale information\npropagation, and mapped into symbolic predicates for rule-based inference. We\npresent a complete mathematical framework for spectral reasoning, including\ngraph Fourier transforms, band-selective attention, and spectral rule\ngrounding. Experiments on benchmark reasoning datasets (ProofWriter,\nEntailmentBank, bAbI, CLUTRR, and ARC-Challenge) demonstrate improvements in\nlogical consistency, interpretability, and computational efficiency over\nstate\\-of\\-the\\-art neuro\\-symbolic models. Our results suggest that GSP\nprovides a mathematically grounded and computationally efficient substrate for\nrobust and interpretable reasoning systems."}
{"id": "2508.15089", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.15089", "abs": "https://arxiv.org/abs/2508.15089", "authors": ["Arun Ganesh"], "title": "Tighter Privacy Analysis for Truncated Poisson Sampling", "comment": null, "summary": "We give a new privacy amplification analysis for truncated Poisson sampling,\na Poisson sampling variant that truncates a batch if it exceeds a given maximum\nbatch size."}
{"id": "2508.15411", "categories": ["cs.SE", "cs.CL", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.15411", "abs": "https://arxiv.org/abs/2508.15411", "authors": ["Frederik Vandeputte"], "title": "Foundational Design Principles and Patterns for Building Robust and Adaptive GenAI-Native Systems", "comment": null, "summary": "Generative AI (GenAI) has emerged as a transformative technology,\ndemonstrating remarkable capabilities across diverse application domains.\nHowever, GenAI faces several major challenges in developing reliable and\nefficient GenAI-empowered systems due to its unpredictability and inefficiency.\nThis paper advocates for a paradigm shift: future GenAI-native systems should\nintegrate GenAI's cognitive capabilities with traditional software engineering\nprinciples to create robust, adaptive, and efficient systems.\n  We introduce foundational GenAI-native design principles centered around five\nkey pillars -- reliability, excellence, evolvability, self-reliance, and\nassurance -- and propose architectural patterns such as GenAI-native cells,\norganic substrates, and programmable routers to guide the creation of resilient\nand self-evolving systems. Additionally, we outline the key ingredients of a\nGenAI-native software stack and discuss the impact of these systems from\ntechnical, user adoption, economic, and legal perspectives, underscoring the\nneed for further validation and experimentation. Our work aims to inspire\nfuture research and encourage relevant communities to implement and refine this\nconceptual framework."}
{"id": "2508.15013", "categories": ["cs.AI", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2508.15013", "abs": "https://arxiv.org/abs/2508.15013", "authors": ["Nadav Amir", "Stas Tiomkin", "Angela Langdon"], "title": "Goals and the Structure of Experience", "comment": null, "summary": "Purposeful behavior is a hallmark of natural and artificial intelligence. Its\nacquisition is often believed to rely on world models, comprising both\ndescriptive (what is) and prescriptive (what is desirable) aspects that\nidentify and evaluate state of affairs in the world, respectively. Canonical\ncomputational accounts of purposeful behavior, such as reinforcement learning,\nposit distinct components of a world model comprising a state representation\n(descriptive aspect) and a reward function (prescriptive aspect). However, an\nalternative possibility, which has not yet been computationally formulated, is\nthat these two aspects instead co-emerge interdependently from an agent's goal.\nHere, we describe a computational framework of goal-directed state\nrepresentation in cognitive agents, in which the descriptive and prescriptive\naspects of a world model co-emerge from agent-environment interaction\nsequences, or experiences. Drawing on Buddhist epistemology, we introduce a\nconstruct of goal-directed, or telic, states, defined as classes of\ngoal-equivalent experience distributions. Telic states provide a parsimonious\naccount of goal-directed learning in terms of the statistical divergence\nbetween behavioral policies and desirable experience features. We review\nempirical and theoretical literature supporting this novel perspective and\ndiscuss its potential to provide a unified account of behavioral,\nphenomenological and neural dimensions of purposeful behaviors across diverse\nsubstrates."}
{"id": "2508.15100", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.15100", "abs": "https://arxiv.org/abs/2508.15100", "authors": ["Ehssan Mousavipour", "Andrey Dimanchev", "Majid Ghaderi"], "title": "Adaptive Anomaly Detection in Evolving Network Environments", "comment": null, "summary": "Distribution shift, a change in the statistical properties of data over time,\nposes a critical challenge for deep learning anomaly detection systems.\nExisting anomaly detection systems often struggle to adapt to these shifts.\nSpecifically, systems based on supervised learning require costly manual\nlabeling, while those based on unsupervised learning rely on clean data, which\nis difficult to obtain, for shift adaptation. Both of these requirements are\nchallenging to meet in practice. In this paper, we introduce NetSight, a\nframework for supervised anomaly detection in network data that continually\ndetects and adapts to distribution shifts in an online manner. NetSight\neliminates manual intervention through a novel pseudo-labeling technique and\nuses a knowledge distillation-based adaptation strategy to prevent catastrophic\nforgetting. Evaluated on three long-term network datasets, NetSight\ndemonstrates superior adaptation performance compared to state-of-the-art\nmethods that rely on manual labeling, achieving F1-score improvements of up to\n11.72%. This proves its robustness and effectiveness in dynamic networks that\nexperience distribution shifts over time."}
{"id": "2508.15423", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15423", "abs": "https://arxiv.org/abs/2508.15423", "authors": ["Ruiqi Wang", "Zezhou Yang", "Cuiyun Gao", "Xin Xia", "Qing Liao"], "title": "An Empirical Study of Knowledge Distillation for Code Understanding Tasks", "comment": "Accepted by ICSE 2026 (Cycle 1)", "summary": "Pre-trained language models (PLMs) have emerged as powerful tools for code\nunderstanding. However, deploying these PLMs in large-scale applications faces\npractical challenges due to their computational intensity and inference\nlatency. Knowledge distillation (KD), a promising model compression and\nacceleration technique, addresses these limitations by transferring knowledge\nfrom large teacher models to compact student models, enabling efficient\ninference while preserving most of the teacher models' capabilities. While this\ntechnique has shown remarkable success in natural language processing and\ncomputer vision domains, its potential for code understanding tasks remains\nlargely underexplored.\n  In this paper, we systematically investigate the effectiveness and usage of\nKD in code understanding tasks. Our study encompasses two popular types of KD\nmethods, i.e., logit-based and feature-based KD methods, experimenting across\neight student models and two teacher PLMs from different domains on three\ndownstream tasks. The experimental results indicate that KD consistently offers\nnotable performance boosts across student models with different sizes compared\nwith standard fine-tuning. Notably, code-specific PLM demonstrates better\neffectiveness as the teacher model. Among all KD methods, the latest\nfeature-based KD methods exhibit superior performance, enabling student models\nto retain up to 98% teacher performance with merely 5% parameters. Regarding\nstudent architecture, our experiments reveal that similarity with teacher\narchitecture does not necessarily lead to better performance. We further\ndiscuss the efficiency and behaviors in the KD process and inference, summarize\nthe implications of findings, and identify promising future directions."}
{"id": "2508.15030", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15030", "abs": "https://arxiv.org/abs/2508.15030", "authors": ["Ashmi Banerjee", "Fitri Nur Aisyah", "Adithi Satish", "Wolfgang Wörndl", "Yashar Deldjoo"], "title": "Collab-REC: An LLM-based Agentic Framework for Balancing Recommendations in Tourism", "comment": null, "summary": "We propose Collab-REC, a multi-agent framework designed to counteract\npopularity bias and enhance diversity in tourism recommendations. In our\nsetting, three LLM-based agents -- Personalization, Popularity, and\nSustainability generate city suggestions from complementary perspectives. A\nnon-LLM moderator then merges and refines these proposals via multi-round\nnegotiation, ensuring each agent's viewpoint is incorporated while penalizing\nspurious or repeated responses. Experiments on European city queries show that\nCollab-REC improves diversity and overall relevance compared to a single-agent\nbaseline, surfacing lesser-visited locales that often remain overlooked. This\nbalanced, context-aware approach addresses over-tourism and better aligns with\nconstraints provided by the user, highlighting the promise of multi-stakeholder\ncollaboration in LLM-driven recommender systems."}
{"id": "2508.15172", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.15172", "abs": "https://arxiv.org/abs/2508.15172", "authors": ["Zheng Li", "Xiaoyang Dong", "Xiaoyun Wang"], "title": "Conditional Cube Attack on Round-Reduced ASCON", "comment": null, "summary": "This paper evaluates the secure level of authenticated encryption\n\\textsc{Ascon} against cube-like method. \\textsc{Ascon} submitted by Dobraunig\n\\emph{et~al.} is one of 16 survivors of the 3rd round CAESAR competition. The\ncube-like method is first used by Dinur \\emph{et~al.} to analyze Keccak keyed\nmodes. At CT-RSA 2015, Dobraunig \\emph{et~al.} applied this method to 5/6-round\nreduced \\textsc{Ascon}, whose structure is similar to Keccak keyed modes.\nHowever, for \\textsc{Ascon} the non-linear layer is more complex and state is\nmuch smaller, which make it hard for the attackers to select enough cube\nvariables that do not multiply with each other after the first round. This\nseems to be the reason why the best previous key-recovery attack is on 6-round\n\\textsc{Ascon}, while for Keccak keyed modes (Keccak-MAC and Keyak) the\nattacked round is no less than 7-round.\n  In this paper, we generalize the conditional cube attack proposed by Huang\n\\emph{et~al.}, and find new cubes depending on some key bit conditions for\n5/6-round reduced \\textsc{Ascon}, and translate the previous theoretic 6-round\nattack with $2^{66}$ time complexity to a practical one with $2^{40}$ time\ncomplexity. Moreover, we propose the first 7-round key-recovery attack on\n\\textsc{Ascon}. By introducing \\emph{the cube-like key-subset technique}, we\ndivide the full key space into many subsets according to different key\nconditions. For each key subset, we launch the cube tester to determine if the\nkey falls into it. Finally, we recover the full key space by testing all the\nkey subsets. The total time complexity is about $2^{103.9}$. In addition, for a\nweak-key subset, whose size is $2^{117}$, the attack is more efficient and\ncosts only $2^{77}$ time complexity. Those attacks do not threaten the full\nround (12 rounds) \\textsc{Ascon}."}
{"id": "2508.15495", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.15495", "abs": "https://arxiv.org/abs/2508.15495", "authors": ["Dongjun Yu", "Xiao Yan", "Zhenrui Li", "Jipeng Xiao", "Haochuan He", "Yongda Yu", "Hao Zhang", "Guoping Rong", "Xiaobo Huang"], "title": "SynthCoder: A Synthetical Strategy to Tune LLMs for Code Completion", "comment": null, "summary": "Code completion is a prominent application of Large Language Models (LLMs) in\nsoftware engineering. Due to the near real-time response requirements of this\ntask, base models with small to medium-sized parameters are typically employed,\nsupplemented by various optimization and post-training techniques. However,\nthese optimization methods often have trade-offs, leading to a seesaw effect\nwhere performance improvements on certain datasets or metrics are accompanied\nby degradations on others -- sometimes even falling below the baseline model's\nperformance. This paper proposes SynthCoder, a model that integrates leading\nindustry practices to achieve state-of-the-art performance on the\nFill-in-the-Middle (FIM) code completion task. In specific, we first construct\na diverse dataset by combining Abstract Syntax Tree (AST) node extraction with\nheuristics that simulate developer behavior. Then we enrich our training corpus\nwith cross-file contextual information using the BM25 algorithm and call\ngraphs, enhancing the model's ability to perform code completion in both\nfile-level and repository-level scenarios. As the last step, we employ a\ntwo-stage training process using the Seed-Coder-8B-Base as the base model.\nFirst, we fine-tune the model using Curriculum Learning technology. Following\nthis, we perform alignment using Direct Preference Optimization (DPO) with\npreference pairs generated through Rejection Sampling. Experimental results\ndemonstrate that our final model excels on mainstream repository-level code\ncompletion benchmarks, including aiXcoder, ExecRepoBench, CrossCodeEval, and\nCoLT. Furthermore, our carefully curated training set effectively mitigates the\nmodel's tendency to just repeat existing code, a common issue existing in\nvarious code completion models."}
{"id": "2508.15047", "categories": ["cs.AI", "cs.GR"], "pdf": "https://arxiv.org/pdf/2508.15047", "abs": "https://arxiv.org/abs/2508.15047", "authors": ["Yibo Liu", "Liam Shatzel", "Brandon Haworth", "Teseo Schneider"], "title": "Emergent Crowds Dynamics from Language-Driven Multi-Agent Interactions", "comment": null, "summary": "Animating and simulating crowds using an agent-based approach is a\nwell-established area where every agent in the crowd is individually controlled\nsuch that global human-like behaviour emerges. We observe that human navigation\nand movement in crowds are often influenced by complex social and environmental\ninteractions, driven mainly by language and dialogue. However, most existing\nwork does not consider these dimensions and leads to animations where\nagent-agent and agent-environment interactions are largely limited to steering\nand fixed higher-level goal extrapolation.\n  We propose a novel method that exploits large language models (LLMs) to\ncontrol agents' movement. Our method has two main components: a dialogue system\nand language-driven navigation. We periodically query agent-centric LLMs\nconditioned on character personalities, roles, desires, and relationships to\ncontrol the generation of inter-agent dialogue when necessitated by the spatial\nand social relationships with neighbouring agents. We then use the conversation\nand each agent's personality, emotional state, vision, and physical state to\ncontrol the navigation and steering of each agent. Our model thus enables\nagents to make motion decisions based on both their perceptual inputs and the\nongoing dialogue.\n  We validate our method in two complex scenarios that exemplify the interplay\nbetween social interactions, steering, and crowding. In these scenarios, we\nobserve that grouping and ungrouping of agents automatically occur.\nAdditionally, our experiments show that our method serves as an\ninformation-passing mechanism within the crowd. As a result, our framework\nproduces more realistic crowd simulations, with emergent group behaviours\narising naturally from any environmental setting."}
{"id": "2508.15183", "categories": ["cs.CR", "cs.DS"], "pdf": "https://arxiv.org/pdf/2508.15183", "abs": "https://arxiv.org/abs/2508.15183", "authors": ["Badih Ghazi", "Pritish Kamath", "Alexander Knop", "Ravi Kumar", "Pasin Manurangsi", "Chiyuan Zhang"], "title": "Private Hyperparameter Tuning with Ex-Post Guarantee", "comment": null, "summary": "The conventional approach in differential privacy (DP) literature formulates\nthe privacy-utility trade-off with a \"privacy-first\" perspective: for a\npredetermined level of privacy, a certain utility is achievable. However,\npractitioners often operate under a \"utility-first\" paradigm, prioritizing a\ndesired level of utility and then determining the corresponding privacy cost.\n  Wu et al. [2019] initiated a formal study of this \"utility-first\" perspective\nby introducing ex-post DP. They demonstrated that by adding correlated Laplace\nnoise and progressively reducing it on demand, a sequence of increasingly\naccurate estimates of a private parameter can be generated, with the privacy\ncost attributed only to the least noisy iterate released. This led to a Laplace\nmechanism variant that achieves a specified utility with minimal privacy loss.\nHowever, their work, and similar findings by Whitehouse et al. [2022], are\nprimarily limited to simple mechanisms based on Laplace or Gaussian noise.\n  In this paper, we significantly generalize these results. In particular, we\nextend the work of Wu et al. [2019] and Liu and Talwar [2019] to support any\nsequence of private estimators, incurring at most a doubling of the original\nprivacy budget. Furthermore, we demonstrate that hyperparameter tuning for\nthese estimators, including the selection of an optimal privacy budget, can be\nperformed without additional privacy cost. Finally, we extend our results to\nex-post Renyi DP, further broadening the applicability of utility-first privacy\nmechanisms."}
{"id": "2508.15496", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.15496", "abs": "https://arxiv.org/abs/2508.15496", "authors": ["Elena Masserini", "Diego Clerissi", "Daniela Micucci", "João R. Campos", "Leonardo Mariani"], "title": "Towards the Assessment of Task-based Chatbots: From the TOFU-R Snapshot to the BRASATO Curated Dataset", "comment": "10 pages, 10 figure, Accepted at IEEE International Symposium on\n  Software Reliability Engineering (ISSRE) 2025", "summary": "Task-based chatbots are increasingly being used to deliver real services, yet\nassessing their reliability, security, and robustness remains underexplored,\nalso due to the lack of large-scale, high-quality datasets. The emerging\nautomated quality assessment techniques targeting chatbots often rely on\nlimited pools of subjects, such as custom-made toy examples, or outdated, no\nlonger available, or scarcely popular agents, complicating the evaluation of\nsuch techniques. In this paper, we present two datasets and the tool support\nnecessary to create and maintain these datasets. The first dataset is RASA\nTASK-BASED CHATBOTS FROM GITHUB (TOFU-R), which is a snapshot of the Rasa\nchatbots available on GitHub, representing the state of the practice in\nopen-source chatbot development with Rasa. The second dataset is BOT RASA\nCOLLECTION (BRASATO), a curated selection of the most relevant chatbots for\ndialogue complexity, functional complexity, and utility, whose goal is to ease\nreproducibility and facilitate research on chatbot reliability."}
{"id": "2508.15050", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.15050", "abs": "https://arxiv.org/abs/2508.15050", "authors": ["Romain Lacombe", "Kerrie Wu", "Eddie Dilworth"], "title": "Don't Think Twice! Over-Reasoning Impairs Confidence Calibration", "comment": "Published at ICML 2025 Workshop on Reliable and Responsible\n  Foundation Models", "summary": "Large Language Models deployed as question answering tools require robust\ncalibration to avoid overconfidence. We systematically evaluate how reasoning\ncapabilities and budget affect confidence assessment accuracy, using the\nClimateX dataset (Lacombe et al., 2023) and expanding it to human and planetary\nhealth. Our key finding challenges the \"test-time scaling\" paradigm: while\nrecent reasoning LLMs achieve 48.7% accuracy in assessing expert confidence,\nincreasing reasoning budgets consistently impairs rather than improves\ncalibration. Extended reasoning leads to systematic overconfidence that worsens\nwith longer thinking budgets, producing diminishing and negative returns beyond\nmodest computational investments. Conversely, search-augmented generation\ndramatically outperforms pure reasoning, achieving 89.3% accuracy by retrieving\nrelevant evidence. Our results suggest that information access, rather than\nreasoning depth or inference budget, may be the critical bottleneck for\nimproved confidence calibration of knowledge-intensive tasks."}
{"id": "2508.15252", "categories": ["cs.CR", "cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.15252", "abs": "https://arxiv.org/abs/2508.15252", "authors": ["Shiyi Yang", "Xinshu Li", "Guanglin Zhou", "Chen Wang", "Xiwei Xu", "Liming Zhu", "Lina Yao"], "title": "Retrieval-Augmented Review Generation for Poisoning Recommender Systems", "comment": null, "summary": "Recent studies have shown that recommender systems (RSs) are highly\nvulnerable to data poisoning attacks, where malicious actors inject fake user\nprofiles, including a group of well-designed fake ratings, to manipulate\nrecommendations. Due to security and privacy constraints in practice, attackers\ntypically possess limited knowledge of the victim system and thus need to craft\nprofiles that have transferability across black-box RSs. To maximize the attack\nimpact, the profiles often remains imperceptible. However, generating such\nhigh-quality profiles with the restricted resources is challenging. Some works\nsuggest incorporating fake textual reviews to strengthen the profiles; yet, the\npoor quality of the reviews largely undermines the attack effectiveness and\nimperceptibility under the practical setting.\n  To tackle the above challenges, in this paper, we propose to enhance the\nquality of the review text by harnessing in-context learning (ICL) capabilities\nof multimodal foundation models. To this end, we introduce a demonstration\nretrieval algorithm and a text style transfer strategy to augment the navie\nICL. Specifically, we propose a novel practical attack framework named RAGAN to\ngenerate high-quality fake user profiles, which can gain insights into the\nrobustness of RSs. The profiles are generated by a jailbreaker and\ncollaboratively optimized on an instructional agent and a guardian to improve\nthe attack transferability and imperceptibility. Comprehensive experiments on\nvarious real-world datasets demonstrate that RAGAN achieves the\nstate-of-the-art poisoning attack performance."}
{"id": "2508.15503", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.15503", "abs": "https://arxiv.org/abs/2508.15503", "authors": ["Sebastian Baltes", "Florian Angermeir", "Chetan Arora", "Marvin Muñoz Barón", "Chunyang Chen", "Lukas Böhme", "Fabio Calefato", "Neil Ernst", "Davide Falessi", "Brian Fitzgerald", "Davide Fucci", "Marcos Kalinowski", "Stefano Lambiase", "Daniel Russo", "Mircea Lungu", "Lutz Prechelt", "Paul Ralph", "Christoph Treude", "Stefan Wagner"], "title": "Evaluation Guidelines for Empirical Studies in Software Engineering involving LLMs", "comment": "Draft of evaluation guidelines for empirical studies in software\n  engineering involving LLMs (see also llm-guidelines.org)", "summary": "Large language models (LLMs) are increasingly being integrated into software\nengineering (SE) research and practice, yet their non-determinism, opaque\ntraining data, and evolving architectures complicate the reproduction and\nreplication of empirical studies. We present a community effort to scope this\nspace, introducing a taxonomy of LLM-based study types together with eight\nguidelines for designing and reporting empirical studies involving LLMs. The\nguidelines present essential (must) criteria as well as desired (should)\ncriteria and target transparency throughout the research process. Our\nrecommendations, contextualized by our study types, are: (1) to declare LLM\nusage and role; (2) to report model versions, configurations, and fine-tuning;\n(3) to document tool architectures; (4) to disclose prompts and interaction\nlogs; (5) to use human validation; (6) to employ an open LLM as a baseline; (7)\nto report suitable baselines, benchmarks, and metrics; and (8) to openly\narticulate limitations and mitigations. Our goal is to enable reproducibility\nand replicability despite LLM-specific barriers to open science. We maintain\nthe study types and guidelines online as a living resource for the community to\nuse and shape (llm-guidelines.org)."}
{"id": "2508.15053", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.15053", "abs": "https://arxiv.org/abs/2508.15053", "authors": ["Itai Zilberstein", "Alberto Candela", "Steve Chien", "David Rijlaarsdam", "Tom Hendrix", "Leonie Buckley", "Aubrey Dunne"], "title": "Demonstrating Onboard Inference for Earth Science Applications with Spectral Analysis Algorithms and Deep Learning", "comment": "International Symposium on Artificial Intelligence, Robotics and\n  Automation in Space, November 2024", "summary": "In partnership with Ubotica Technologies, the Jet Propulsion Laboratory is\ndemonstrating state-of-the-art data analysis onboard CogniSAT-6/HAMMER (CS-6).\nCS-6 is a satellite with a visible and near infrared range hyperspectral\ninstrument and neural network acceleration hardware. Performing data analysis\nat the edge (e.g. onboard) can enable new Earth science measurements and\nresponses. We will demonstrate data analysis and inference onboard CS-6 for\nnumerous applications using deep learning and spectral analysis algorithms."}
{"id": "2508.15306", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.15306", "abs": "https://arxiv.org/abs/2508.15306", "authors": ["Henrietta Hegyi", "Laszlo Erdodi"], "title": "Connected and Exposed: Cybersecurity Risks, Regulatory Gaps, and Public Perception in Internet-Connected Vehicles", "comment": null, "summary": "The rapid advancement of Internet-connected vehicle technologies has\nintroduced a new era of smart mobility, while simultaneously raising\nsignificant cybersecurity and privacy concerns. This paper explores the\nevolving threat landscape associated with connected vehicles, focusing on risks\nsuch as unauthorized remote access and the potential leakage of personal data.\nTo assess the current state of protection, we conducted a comprehensive\nanalysis of 16 international standards and regulations, evaluating them from\nmultiple perspectives including regulatory strength, technical specificity,\ntreatment of supply chain risks, and approaches to personal data handling.\n  In parallel, we carried out a user-focused survey designed to map consumer\nattitudes toward smart cars. The survey investigated which types of vehicles\nusers trust and prefer, the reasons behind rejecting certain car types, their\nawareness of data-related risks, and whether they feel adequately informed\nabout how their vehicles handle data. By combining regulatory analysis with\nuser perception insights, this study aims to contribute to a more holistic\nunderstanding of the challenges and expectations surrounding connected vehicle\necosystems."}
{"id": "2508.15512", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.15512", "abs": "https://arxiv.org/abs/2508.15512", "authors": ["Markus Borg", "Martin Larsson", "Philip Breid", "Nadim Hagatulah"], "title": "QUPER-MAn: Benchmark-Guided Target Setting for Maintainability Requirements", "comment": "Accepted at the 1st International Workshop on Responsible Software\n  Engineering", "summary": "Maintainable source code is essential for sustainable development in any\nsoftware organization. Unfortunately, many studies show that maintainability\noften receives less attention than its importance warrants. We argue that\nrequirements engineering can address this gap the problem by fostering\ndiscussions and setting appropriate targets in a responsible manner. In this\npreliminary work, we conducted an exploratory study of industry practices\nrelated to requirements engineering for maintainability. Our findings confirm\nprevious studies: maintainability remains a second-class quality concern.\nExplicit requirements often make sweeping references to coding conventions.\nTools providing maintainability proxies are common but typically only used in\nimplicit requirements related to engineering practices. To address this, we\npropose QUPER-MAn, a maintainability adaption of the QUPER model, which was\noriginally developed to help organizations set targets for performance\nrequirements. Developed using a design science approach, QUPER-MAn, integrates\nmaintainability benchmarks and supports target setting. We posit that it can\nshift maintainability from an overlooked development consequence to an actively\nmanaged goal driven by informed and responsible engineering decisions."}
{"id": "2508.15068", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15068", "abs": "https://arxiv.org/abs/2508.15068", "authors": ["Shuang Ao", "Gopal Rumchurn"], "title": "S3LoRA: Safe Spectral Sharpness-Guided Pruning in Adaptation of Agent Planner", "comment": "9 pages, 2 figures", "summary": "Adapting Large Language Models (LLMs) using parameter-efficient fine-tuning\n(PEFT) techniques such as LoRA has enabled powerful capabilities in LLM-based\nagents. However, these adaptations can unintentionally compromise safety\nalignment, leading to unsafe or unstable behaviors, particularly in agent\nplanning tasks. Existing safety-aware adaptation methods often require access\nto both base and instruction-tuned model checkpoints, which are frequently\nunavailable in practice, limiting their applicability. We propose S3LoRA (Safe\nSpectral Sharpness-Guided Pruning LoRA), a lightweight, data-free, and\nmodel-independent framework that mitigates safety risks in LoRA-adapted models\nby inspecting only the fine-tuned weight updates. We first introduce\nMagnitude-Aware Spherically Normalized SVD (MAS-SVD), which robustly analyzes\nthe structural properties of LoRA updates while preserving global magnitude\ninformation. We then design the Spectral Sharpness Index (SSI), a\nsharpness-aware metric to detect layers with highly concentrated and\npotentially unsafe updates. These layers are pruned post-hoc to reduce risk\nwithout sacrificing task performance. Extensive experiments and ablation\nstudies across agent planning and language generation tasks show that S3LoRA\nconsistently improves safety metrics while maintaining or improving utility\nmetrics and significantly reducing inference cost. These results establish\nS3LoRA as a practical and scalable solution for safely deploying LLM-based\nagents in real-world, resource-constrained, and safety-critical environments."}
{"id": "2508.15310", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.15310", "abs": "https://arxiv.org/abs/2508.15310", "authors": ["Hengyu An", "Jinghuai Zhang", "Tianyu Du", "Chunyi Zhou", "Qingming Li", "Tao Lin", "Shouling Ji"], "title": "IPIGuard: A Novel Tool Dependency Graph-Based Defense Against Indirect Prompt Injection in LLM Agents", "comment": "EMNLP 2025", "summary": "Large language model (LLM) agents are widely deployed in real-world\napplications, where they leverage tools to retrieve and manipulate external\ndata for complex tasks. However, when interacting with untrusted data sources\n(e.g., fetching information from public websites), tool responses may contain\ninjected instructions that covertly influence agent behaviors and lead to\nmalicious outcomes, a threat referred to as Indirect Prompt Injection (IPI).\nExisting defenses typically rely on advanced prompting strategies or auxiliary\ndetection models. While these methods have demonstrated some effectiveness,\nthey fundamentally rely on assumptions about the model's inherent security,\nwhich lacks structural constraints on agent behaviors. As a result, agents\nstill retain unrestricted access to tool invocations, leaving them vulnerable\nto stronger attack vectors that can bypass the security guardrails of the\nmodel. To prevent malicious tool invocations at the source, we propose a novel\ndefensive task execution paradigm, called IPIGuard, which models the agents'\ntask execution process as a traversal over a planned Tool Dependency Graph\n(TDG). By explicitly decoupling action planning from interaction with external\ndata, IPIGuard significantly reduces unintended tool invocations triggered by\ninjected instructions, thereby enhancing robustness against IPI attacks.\nExperiments on the AgentDojo benchmark show that IPIGuard achieves a superior\nbalance between effectiveness and robustness, paving the way for the\ndevelopment of safer agentic systems in dynamic environments."}
{"id": "2508.15536", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.15536", "abs": "https://arxiv.org/abs/2508.15536", "authors": ["Yi Zhang", "He Jiang", "Xiaochen Li", "Shikai Guo", "Peiyu Zou", "Zun Wang"], "title": "A Novel Mutation Based Method for Detecting FPGA Logic Synthesis Tool Bugs", "comment": null, "summary": "FPGA (Field-Programmable Gate Array) logic synthesis tools are key components\nin the EDA (Electronic Design Automation) toolchain. They convert hardware\ndesigns written in description languages such as Verilog into gate-level\nrepresentations for FPGAs. However, defects in these tools may lead to\nunexpected behaviors and pose security risks. Therefore, it is crucial to\nharden these tools through testing. Although several methods have been proposed\nto automatically test FPGA logic synthesis tools, the challenge remains of\ninsufficient semantic and logical complexity in test programs. In this paper,\nwe propose VERMEI, a new method for testing FPGA logic synthesis tools. VERMEI\nconsists of three modules: preprocessing, equivalent mutation, and bug\nidentification. The preprocessing module identifies zombie logic (inactive code\nwith no impact on the circuit output) in seed programs through simulation and\ncoverage analysis. The equivalent mutation module generates equivalent variants\nof seed programs by pruning or inserting logic fragments in zombie areas. It\nuses Bayesian sampling to extract logic fragments from historical Verilog\ndesigns, making the generated variants have complex control flows and\nstructures. The bug identification module, based on differential testing,\ncompares the synthesized outputs of seed and variant programs to identify bugs.\nExperiments on Yosys, Vivado, and Quartus demonstrate that VERMEI outperforms\nthe state-of-the-art methods. Within five months, VERMEI reported 15 bugs to\nvendors, 9 of which were confirmed as new."}
{"id": "2508.15118", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15118", "abs": "https://arxiv.org/abs/2508.15118", "authors": ["Jennifer Leigh", "Dimitrios Letsios", "Alessandro Mella", "Lucio Machetti", "Francesca Toni"], "title": "Argumentation for Explainable Workforce Optimisation (with Appendix)", "comment": "Accepted to PAIS 2025", "summary": "Workforce management is a complex problem optimising the makespan and travel\ndistance required for a team of operators to complete a set of jobs, using a\nset of instruments. A crucial challenge in workforce management is\naccommodating changes at execution time so that explanations are provided to\nall stakeholders involved. Here, we show that, by understanding workforce\nmanagement as abstract argumentation in an industrial application, we can\naccommodate change and obtain faithful explanations. We show, with a user\nstudy, that our tool and explanations lead to faster and more accurate problem\nsolving than conventional solutions by hand."}
{"id": "2508.15386", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.15386", "abs": "https://arxiv.org/abs/2508.15386", "authors": ["Sabine Houy", "Bruno Kreyssig", "Timothee Riom", "Alexandre Bartel", "Patrick McDaniel"], "title": "A Practical Guideline and Taxonomy to LLVM's Control Flow Integrity", "comment": null, "summary": "Memory corruption vulnerabilities remain one of the most severe threats to\nsoftware security. They often allow attackers to achieve arbitrary code\nexecution by redirecting a vulnerable program's control flow. While Control\nFlow Integrity (CFI) has gained traction to mitigate this exploitation path,\ndevelopers are not provided with any direction on how to apply CFI to\nreal-world software. In this work, we establish a taxonomy mapping LLVM's\nforward-edge CFI variants to memory corruption vulnerability classes, offering\nactionable guidance for developers seeking to deploy CFI incrementally in\nexisting codebases. Based on the Top 10 Known Exploited Vulnerabilities (KEV)\nlist, we identify four high-impact vulnerability categories and select one\nrepresentative CVE for each. We evaluate LLVM's CFI against each CVE and\nexplain why CFI blocks exploitation in two cases while failing in the other\ntwo, illustrating its potential and current limitations. Our findings support\ninformed deployment decisions and provide a foundation for improving the\npractical use of CFI in production systems."}
{"id": "2508.15570", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.15570", "abs": "https://arxiv.org/abs/2508.15570", "authors": ["Marion Wiese", "Kamila Serwa", "Anastasia Besier", "Ariane S. Marion-Jetten", "Eva Bittner"], "title": "Establishing Technical Debt Management -- A Five-Step Workshop Approach and an Action Research Study", "comment": "Accepted for publication by the Journal of Systems and Software --\n  Special Issue on Managing Technical Debt in Software-Intensive Products and\n  Services", "summary": "Context. Technical debt (TD) items are constructs in a software system\nproviding short-term benefits but hindering future changes. TD management (TDM)\nis frequently researched but rarely adopted in practice. Goal. This study aimed\nto establish a TDM process in an IT company based on a predefined workshop\nconcept. We analyzed which research approaches practitioners adopted for each\nTD activity and the TDM's long-term effect on TD awareness. Method. We used\naction research (five action cycles in 16 months) with an IT team that creates\nIT solutions for signal processing. To examine TD awareness, we (1) analyzed\nquestionnaires completed during each workshop, (2) observed team meetings, (3)\nadopted a method from psychology for measuring awareness in decision-making\nsituations called TD-SAGAT, and (4) evaluated the backlog data. Results.\nPractitioners preferred TD repayment and prioritization based on the system's\nevolution and cost calculations, i.e., repayment of so-called low-hanging\nfruits. Reminders in the backlog items, such as checkboxes or text templates,\nled to a sustainable rise in TD awareness. Conclusions. We showed that a\nworkshop-based approach is feasible and leads to sustainable process changes.\nNew ideas for TDM applicable to other IT teams emerged, e.g., using a\nre-submission date, using a Talked about TD checkbox, and using visualizations\nfor TD prioritization."}
{"id": "2508.15119", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.15119", "abs": "https://arxiv.org/abs/2508.15119", "authors": ["Rachel Ma", "Jingyi Qu", "Andreea Bobu", "Dylan Hadfield-Menell"], "title": "Open-Universe Assistance Games", "comment": "7 pages + 2 pages references + 7 pages appendix", "summary": "Embodied AI agents must infer and act in an interpretable way on diverse\nhuman goals and preferences that are not predefined. To formalize this setting,\nwe introduce Open-Universe Assistance Games (OU-AGs), a framework where the\nagent must reason over an unbounded and evolving space of possible goals. In\nthis context, we introduce GOOD (GOals from Open-ended Dialogue), a\ndata-efficient, online method that extracts goals in the form of natural\nlanguage during an interaction with a human, and infers a distribution over\nnatural language goals. GOOD prompts an LLM to simulate users with different\ncomplex intents, using its responses to perform probabilistic inference over\ncandidate goals. This approach enables rich goal representations and\nuncertainty estimation without requiring large offline datasets. We evaluate\nGOOD in a text-based grocery shopping domain and in a text-operated simulated\nhousehold robotics environment (AI2Thor), using synthetic user profiles. Our\nmethod outperforms a baseline without explicit goal tracking, as confirmed by\nboth LLM-based and human evaluations."}
{"id": "2508.15541", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.15541", "abs": "https://arxiv.org/abs/2508.15541", "authors": ["Bingguang Lu", "Hongsheng Hu", "Yuantian Miao", "Shaleeza Sohail", "Chaoxiang He", "Shuo Wang", "Xiao Chen"], "title": "BadFU: Backdoor Federated Learning through Adversarial Machine Unlearning", "comment": null, "summary": "Federated learning (FL) has been widely adopted as a decentralized training\nparadigm that enables multiple clients to collaboratively learn a shared model\nwithout exposing their local data. As concerns over data privacy and regulatory\ncompliance grow, machine unlearning, which aims to remove the influence of\nspecific data from trained models, has become increasingly important in the\nfederated setting to meet legal, ethical, or user-driven demands. However,\nintegrating unlearning into FL introduces new challenges and raises largely\nunexplored security risks. In particular, adversaries may exploit the\nunlearning process to compromise the integrity of the global model. In this\npaper, we present the first backdoor attack in the context of federated\nunlearning, demonstrating that an adversary can inject backdoors into the\nglobal model through seemingly legitimate unlearning requests. Specifically, we\npropose BadFU, an attack strategy where a malicious client uses both backdoor\nand camouflage samples to train the global model normally during the federated\ntraining process. Once the client requests unlearning of the camouflage\nsamples, the global model transitions into a backdoored state. Extensive\nexperiments under various FL frameworks and unlearning strategies validate the\neffectiveness of BadFU, revealing a critical vulnerability in current federated\nunlearning practices and underscoring the urgent need for more secure and\nrobust federated unlearning mechanisms."}
{"id": "2508.15584", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.15584", "abs": "https://arxiv.org/abs/2508.15584", "authors": ["Maria Teresa Rossi", "Leonardo Mariani", "Oliviero Riganelli"], "title": "From PREVENTion to REACTion: Enhancing Failure Resolution in Naval Systems", "comment": null, "summary": "Complex and large industrial systems often misbehave, for instance, due to\nwear, misuse, or faults. To cope with these incidents, it is important to\ntimely detect their occurrences, localize the sources of the problems, and\nimplement the appropriate countermeasures. This paper reports our experience\nwith a state-of-the-art failure prediction method, PREVENT, and its extension\nwith a troubleshooting module, REACT, applied to naval systems developed by\nFincantieri. Our results show how to integrate anomaly detection with\ntroubleshooting procedures. We conclude by discussing a lesson learned, which\nmay help deploy and extend these analyses to other industrial products."}
{"id": "2508.15126", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.15126", "abs": "https://arxiv.org/abs/2508.15126", "authors": ["Pengsong Zhang", "Xiang Hu", "Guowei Huang", "Yang Qi", "Heng Zhang", "Xiuxu Li", "Jiaxing Song", "Jiabin Luo", "Yijiang Li", "Shuo Yin", "Chengxiao Dai", "Eric Hanchen Jiang", "Xiaoyan Zhou", "Zhenfei Yin", "Boqin Yuan", "Jing Dong", "Guinan Su", "Guanren Qiao", "Haiming Tang", "Anghong Du", "Lili Pan", "Zhenzhong Lan", "Xinyu Liu"], "title": "aiXiv: A Next-Generation Open Access Ecosystem for Scientific Discovery Generated by AI Scientists", "comment": "Preprint under review. Code is available at\n  https://github.com/aixiv-org. Website is available at\n  https://forms.gle/DxQgCtXFsJ4paMtn8", "summary": "Recent advances in large language models (LLMs) have enabled AI agents to\nautonomously generate scientific proposals, conduct experiments, author papers,\nand perform peer reviews. Yet this flood of AI-generated research content\ncollides with a fragmented and largely closed publication ecosystem.\nTraditional journals and conferences rely on human peer review, making them\ndifficult to scale and often reluctant to accept AI-generated research content;\nexisting preprint servers (e.g. arXiv) lack rigorous quality-control\nmechanisms. Consequently, a significant amount of high-quality AI-generated\nresearch lacks appropriate venues for dissemination, hindering its potential to\nadvance scientific progress. To address these challenges, we introduce aiXiv, a\nnext-generation open-access platform for human and AI scientists. Its\nmulti-agent architecture allows research proposals and papers to be submitted,\nreviewed, and iteratively refined by both human and AI scientists. It also\nprovides API and MCP interfaces that enable seamless integration of\nheterogeneous human and AI scientists, creating a scalable and extensible\necosystem for autonomous scientific discovery. Through extensive experiments,\nwe demonstrate that aiXiv is a reliable and robust platform that significantly\nenhances the quality of AI-generated research proposals and papers after\niterative revising and reviewing on aiXiv. Our work lays the groundwork for a\nnext-generation open-access ecosystem for AI scientists, accelerating the\npublication and dissemination of high-quality AI-generated research content.\nCode is available at https://github.com/aixiv-org. Website is available at\nhttps://forms.gle/DxQgCtXFsJ4paMtn8."}
{"id": "2508.15606", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.15606", "abs": "https://arxiv.org/abs/2508.15606", "authors": ["Yu Yang", "Zhenyuan Li", "Xiandong Ran", "Jiahao Liu", "Jiahui Wang", "Bo Yu", "Shouling Ji"], "title": "Towards Scalable and Interpretable Mobile App Risk Analysis via Large Language Models", "comment": null, "summary": "Mobile application marketplaces are responsible for vetting apps to identify\nand mitigate security risks. Current vetting processes are labor-intensive,\nrelying on manual analysis by security professionals aided by semi-automated\ntools. To address this inefficiency, we propose Mars, a system that leverages\nLarge Language Models (LLMs) for automated risk identification and profiling.\nMars is designed to concurrently analyze multiple applications across diverse\nrisk categories with minimal human intervention. To enhance analytical\nprecision and operational efficiency, Mars leverages a pre-constructed risk\nidentification tree to extract relevant indicators from high-dimensional\napplication features. This initial step filters the data, reducing the input\nvolume for the LLM and mitigating the potential for model hallucination induced\nby irrelevant features. The extracted indicators are then subjected to LLM\nanalysis for final risk determination. Furthermore, Mars automatically\ngenerates a comprehensive evidence chain for each assessment, documenting the\nanalytical process to provide transparent justification. These chains are\ndesigned to facilitate subsequent manual review and to inform enforcement\ndecisions, such as application delisting. The performance of Mars was evaluated\non a real-world dataset from a partner Android marketplace. The results\ndemonstrate that Mars attained an F1-score of 0.838 in risk identification and\nan F1-score of 0.934 in evidence retrieval. To assess its practical\napplicability, a user study involving 20 expert analysts was conducted, which\nindicated that Mars yielded a substantial efficiency gain, ranging from 60% to\n90%, over conventional manual analysis."}
{"id": "2508.15386", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.15386", "abs": "https://arxiv.org/abs/2508.15386", "authors": ["Sabine Houy", "Bruno Kreyssig", "Timothee Riom", "Alexandre Bartel", "Patrick McDaniel"], "title": "A Practical Guideline and Taxonomy to LLVM's Control Flow Integrity", "comment": null, "summary": "Memory corruption vulnerabilities remain one of the most severe threats to\nsoftware security. They often allow attackers to achieve arbitrary code\nexecution by redirecting a vulnerable program's control flow. While Control\nFlow Integrity (CFI) has gained traction to mitigate this exploitation path,\ndevelopers are not provided with any direction on how to apply CFI to\nreal-world software. In this work, we establish a taxonomy mapping LLVM's\nforward-edge CFI variants to memory corruption vulnerability classes, offering\nactionable guidance for developers seeking to deploy CFI incrementally in\nexisting codebases. Based on the Top 10 Known Exploited Vulnerabilities (KEV)\nlist, we identify four high-impact vulnerability categories and select one\nrepresentative CVE for each. We evaluate LLVM's CFI against each CVE and\nexplain why CFI blocks exploitation in two cases while failing in the other\ntwo, illustrating its potential and current limitations. Our findings support\ninformed deployment decisions and provide a foundation for improving the\npractical use of CFI in production systems."}
{"id": "2508.15144", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15144", "abs": "https://arxiv.org/abs/2508.15144", "authors": ["Jiabo Ye", "Xi Zhang", "Haiyang Xu", "Haowei Liu", "Junyang Wang", "Zhaoqing Zhu", "Ziwei Zheng", "Feiyu Gao", "Junjie Cao", "Zhengxi Lu", "Jitong Liao", "Qi Zheng", "Fei Huang", "Jingren Zhou", "Ming Yan"], "title": "Mobile-Agent-v3: Foundamental Agents for GUI Automation", "comment": null, "summary": "This paper introduces GUI-Owl, a foundational GUI agent model that achieves\nstate-of-the-art performance among open-source end-to-end models on ten GUI\nbenchmarks across desktop and mobile environments, covering grounding, question\nanswering, planning, decision-making, and procedural knowledge. GUI-Owl-7B\nachieves 66.4 on AndroidWorld and 29.4 on OSWorld. Building on this, we propose\nMobile-Agent-v3, a general-purpose GUI agent framework that further improves\nperformance to 73.3 on AndroidWorld and 37.7 on OSWorld, setting a new\nstate-of-the-art for open-source GUI agent frameworks. GUI-Owl incorporates\nthree key innovations: (1) Large-scale Environment Infrastructure: a\ncloud-based virtual environment spanning Android, Ubuntu, macOS, and Windows,\nenabling our Self-Evolving GUI Trajectory Production framework. This generates\nhigh-quality interaction data via automated query generation and correctness\nvalidation, leveraging GUI-Owl to refine trajectories iteratively, forming a\nself-improving loop. It supports diverse data pipelines and reduces manual\nannotation. (2) Diverse Foundational Agent Capabilities: by integrating UI\ngrounding, planning, action semantics, and reasoning patterns, GUI-Owl supports\nend-to-end decision-making and can act as a modular component in multi-agent\nsystems. (3) Scalable Environment RL: we develop a scalable reinforcement\nlearning framework with fully asynchronous training for real-world alignment.\nWe also introduce Trajectory-aware Relative Policy Optimization (TRPO) for\nonline RL, achieving 34.9 on OSWorld. GUI-Owl and Mobile-Agent-v3 are\nopen-sourced at https://github.com/X-PLUG/MobileAgent."}
{"id": "2508.15180", "categories": ["cs.AI", "I.2.7"], "pdf": "https://arxiv.org/pdf/2508.15180", "abs": "https://arxiv.org/abs/2508.15180", "authors": ["Kai Xiong", "Yanwei Huang", "Rongjunchen Zhang", "Kun Chen", "Haipang Wu"], "title": "PuzzleClone: An SMT-Powered Framework for Synthesizing Verifiable Data", "comment": null, "summary": "High-quality mathematical and logical datasets with verifiable answers are\nessential for strengthening the reasoning capabilities of large language models\n(LLMs). While recent data augmentation techniques have facilitated the creation\nof large-scale benchmarks, existing LLM-generated datasets often suffer from\nlimited reliability, diversity, and scalability. To address these challenges,\nwe introduce PuzzleClone, a formal framework for synthesizing verifiable data\nat scale using Satisfiability Modulo Theories (SMT). Our approach features\nthree key innovations: (1) encoding seed puzzles into structured logical\nspecifications, (2) generating scalable variants through systematic variable\nand constraint randomization, and (3) ensuring validity via a reproduction\nmechanism. Applying PuzzleClone, we construct a curated benchmark comprising\nover 83K diverse and programmatically validated puzzles. The generated puzzles\nspan a wide spectrum of difficulty and formats, posing significant challenges\nto current state-of-the-art models. We conduct post training (SFT and RL) on\nPuzzleClone datasets. Experimental results show that training on PuzzleClone\nyields substantial improvements not only on PuzzleClone testset but also on\nlogic and mathematical benchmarks. Post training raises PuzzleClone average\nfrom 14.4 to 56.2 and delivers consistent improvements across 7 logic and\nmathematical benchmarks up to 12.5 absolute percentage points (AMC2023 from\n52.5 to 65.0). Our code and data are available at\nhttps://github.com/puzzleclone."}
{"id": "2508.15192", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.15192", "abs": "https://arxiv.org/abs/2508.15192", "authors": ["Wenjie Lin", "Jin Wei-Kocsis"], "title": "LLM4Sweat: A Trustworthy Large Language Model for Hyperhidrosis Support", "comment": null, "summary": "While large language models (LLMs) have shown promise in healthcare, their\napplication for rare medical conditions is still hindered by scarce and\nunreliable datasets for fine-tuning. Hyperhidrosis, a disorder causing\nexcessive sweating beyond physiological needs, is one such rare disorder,\naffecting 2-3% of the population and significantly impacting both physical\ncomfort and psychosocial well-being. To date, no work has tailored LLMs to\nadvance the diagnosis or care of hyperhidrosis. To address this gap, we present\nLLM4Sweat, an open-source and domain-specific LLM framework for trustworthy and\nempathetic hyperhidrosis support. The system follows a three-stage pipeline. In\nthe data augmentation stage, a frontier LLM generates medically plausible\nsynthetic vignettes from curated open-source data to create a diverse and\nbalanced question-answer dataset. In the fine-tuning stage, an open-source\nfoundation model is fine-tuned on the dataset to provide diagnosis,\npersonalized treatment recommendations, and empathetic psychological support.\nIn the inference and expert evaluation stage, clinical and psychological\nspecialists assess accuracy, appropriateness, and empathy, with validated\nresponses iteratively enriching the dataset. Experiments show that LLM4Sweat\noutperforms baselines and delivers the first open-source LLM framework for\nhyperhidrosis, offering a generalizable approach for other rare diseases with\nsimilar data and trustworthiness challenges."}
{"id": "2508.15204", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15204", "abs": "https://arxiv.org/abs/2508.15204", "authors": ["Raj Jain", "Marc Wetter"], "title": "R-ConstraintBench: Evaluating LLMs on NP-Complete Scheduling", "comment": null, "summary": "Effective scheduling under tight resource, timing, and operational\nconstraints underpins large-scale planning across sectors such as capital\nprojects, manufacturing, logistics, and IT fleet transitions. However, the\nreliability of large language models (LLMs) when reasoning under\nhigh-constraint regimes is insufficiently characterized. To address this gap,\nwe present R-ConstraintBench, a scalable framework that evaluates models on\nResource-Constrained Project Scheduling Problems (RCPSP), an NP-Complete\nfeasibility class, while difficulty increases via linear growth in constraints.\nR-ConstraintBench incrementally increases non-redundant precedence constraints\nin Directed Acyclic Graphs (DAGs) and then introduces downtime, temporal\nwindows, and disjunctive constraints. As an illustrative example, we\ninstantiate the benchmark in a data center migration setting and evaluate\nmultiple LLMs using feasibility and error analysis, identifying degradation\nthresholds and constraint types most associated with failure. Empirically,\nstrong models are near-ceiling on precedence-only DAGs, but feasibility\nperformance collapses when downtime, temporal windows, and disjunctive\nconstraints interact, implicating constraint interaction, not graph depth, as\nthe principal bottleneck. Performance on clean synthetic ramps also does not\nguarantee transfer to domain-grounded scenarios, underscoring limited\ngeneralization."}
{"id": "2508.15222", "categories": ["cs.AI", "cs.CV", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.15222", "abs": "https://arxiv.org/abs/2508.15222", "authors": ["Hantao Zhang", "Jingyang Liu", "Ed Li"], "title": "See it. Say it. Sorted: Agentic System for Compositional Diagram Generation", "comment": null, "summary": "We study sketch-to-diagram generation: converting rough hand sketches into\nprecise, compositional diagrams. Diffusion models excel at photorealism but\nstruggle with the spatial precision, alignment, and symbolic structure required\nfor flowcharts. We introduce See it. Say it. Sorted., a training-free agentic\nsystem that couples a Vision-Language Model (VLM) with Large Language Models\n(LLMs) to produce editable Scalable Vector Graphics (SVG) programs. The system\nruns an iterative loop in which a Critic VLM proposes a small set of\nqualitative, relational edits; multiple candidate LLMs synthesize SVG updates\nwith diverse strategies (conservative->aggressive, alternative, focused); and a\nJudge VLM selects the best candidate, ensuring stable improvement. This design\nprioritizes qualitative reasoning over brittle numerical estimates, preserves\nglobal constraints (e.g., alignment, connectivity), and naturally supports\nhuman-in-the-loop corrections. On 10 sketches derived from flowcharts in\npublished papers, our method more faithfully reconstructs layout and structure\nthan two frontier closed-source image generation LLMs (GPT-5 and\nGemini-2.5-Pro), accurately composing primitives (e.g., multi-headed arrows)\nwithout inserting unwanted text. Because outputs are programmatic SVGs, the\napproach is readily extensible to presentation tools (e.g., PowerPoint) via\nAPIs and can be specialized with improved prompts and task-specific tools. The\ncodebase is open-sourced at\nhttps://github.com/hantaoZhangrichard/see_it_say_it_sorted.git."}
{"id": "2508.15240", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15240", "abs": "https://arxiv.org/abs/2508.15240", "authors": ["Sabab Aosaf", "Muhammad Ali Nayeem", "Afsana Haque", "M Sohel Rahmana"], "title": "Computational Intelligence based Land-use Allocation Approaches for Mixed Use Areas", "comment": null, "summary": "Urban land-use allocation represents a complex multi-objective optimization\nproblem critical for sustainable urban development policy. This paper presents\nnovel computational intelligence approaches for optimizing land-use allocation\nin mixed-use areas, addressing inherent trade-offs between land-use\ncompatibility and economic objectives. We develop multiple optimization\nalgorithms, including custom variants integrating differential evolution with\nmulti-objective genetic algorithms. Key contributions include: (1) CR+DES\nalgorithm leveraging scaled difference vectors for enhanced exploration, (2)\nsystematic constraint relaxation strategy improving solution quality while\nmaintaining feasibility, and (3) statistical validation using Kruskal-Wallis\ntests with compact letter displays. Applied to a real-world case study with\n1,290 plots, CR+DES achieves 3.16\\% improvement in land-use compatibility\ncompared to state-of-the-art methods, while MSBX+MO excels in price\noptimization with 3.3\\% improvement. Statistical analysis confirms algorithms\nincorporating difference vectors significantly outperform traditional\napproaches across multiple metrics. The constraint relaxation technique enables\nbroader solution space exploration while maintaining practical constraints.\nThese findings provide urban planners and policymakers with evidence-based\ncomputational tools for balancing competing objectives in land-use allocation,\nsupporting more effective urban development policies in rapidly urbanizing\nregions."}
{"id": "2508.15294", "categories": ["cs.AI", "cs.CL", "cs.MA", "I.2.7"], "pdf": "https://arxiv.org/pdf/2508.15294", "abs": "https://arxiv.org/abs/2508.15294", "authors": ["Gaoke Zhang", "Bo Wang", "Yunlong Ma", "Dongming Zhao", "Zifei Yu"], "title": "Multiple Memory Systems for Enhancing the Long-term Memory of Agent", "comment": null, "summary": "An agent powered by large language models have achieved impressive results,\nbut effectively handling the vast amounts of historical data generated during\ninteractions remains a challenge. The current approach is to design a memory\nmodule for the agent to process these data. However, existing methods, such as\nMemoryBank and A-MEM, have poor quality of stored memory content, which affects\nrecall performance and response quality. In order to better construct\nhigh-quality long-term memory content, we have designed a multiple memory\nsystem (MMS) inspired by cognitive psychology theory. The system processes\nshort-term memory to multiple long-term memory fragments, and constructs\nretrieval memory units and contextual memory units based on these fragments,\nwith a one-to-one correspondence between the two. During the retrieval phase,\nMMS will match the most relevant retrieval memory units based on the user's\nquery. Then, the corresponding contextual memory units is obtained as the\ncontext for the response stage to enhance knowledge, thereby effectively\nutilizing historical data. Experiments on LoCoMo dataset compared our method\nwith three others, proving its effectiveness. Ablation studies confirmed the\nrationality of our memory units. We also analyzed the robustness regarding the\nnumber of selected memory segments and the storage overhead, demonstrating its\npractical value."}
{"id": "2508.15305", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15305", "abs": "https://arxiv.org/abs/2508.15305", "authors": ["Wei Yang", "Jinwei Xiao", "Hongming Zhang", "Qingyang Zhang", "Yanna Wang", "Bo Xu"], "title": "Coarse-to-Fine Grounded Memory for LLM Agent Planning", "comment": "Accepted to EMNLP 2025 Main Conference;27 pages,15 figures", "summary": "Recent advancements in Large Language Models (LLMs) have driven growing\ninterest in LLM-based agents for complex planning tasks. To avoid costly agent\ntraining, many studies adopted memory mechanism that enhances LLM with offline\nexperiences or online trajectory analysis. However, existing works focus on\nsingle-granularity memory derived from dynamic environmental interactions,\nwhich are inherently constrained by the quality of the collected experiences.\nThis limitation, in turn, constrain the diversity of knowledge and the\nflexibility of planning. We propose Coarse-to-Fine Grounded Memory (\\Ours{}), a\nnovel framework that grounds coarse-to-fine memories with LLM, thereby fully\nleverage them for flexible adaptation to diverse scenarios. \\Ours{} grounds\nenvironmental information into coarse-grained focus points to guide experience\ncollection in training tasks, followed by grounding of actionable\nhybrid-grained tips from each experience. At inference, \\Ours{} retrieves\ntask-relevant experiences and tips to support planning. When facing\nenvironmental anomalies, the LLM grounds the current situation into\nfine-grained key information, enabling flexible self-QA reflection and plan\ncorrection."}
{"id": "2508.15327", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.15327", "abs": "https://arxiv.org/abs/2508.15327", "authors": ["Xiancheng Gao", "Yufeng Shi", "Wengang Zhou", "Houqiang Li"], "title": "Search-Based Credit Assignment for Offline Preference-Based Reinforcement Learning", "comment": "7 pages, 6 figures, under review", "summary": "Offline reinforcement learning refers to the process of learning policies\nfrom fixed datasets, without requiring additional environment interaction.\nHowever, it often relies on well-defined reward functions, which are difficult\nand expensive to design. Human feedback is an appealing alternative, but its\ntwo common forms, expert demonstrations and preferences, have complementary\nlimitations. Demonstrations provide stepwise supervision, but they are costly\nto collect and often reflect limited expert behavior modes. In contrast,\npreferences are easier to collect, but it is unclear which parts of a behavior\ncontribute most to a trajectory segment, leaving credit assignment unresolved.\nIn this paper, we introduce a Search-Based Preference Weighting (SPW) scheme to\nunify these two feedback sources. For each transition in a preference labeled\ntrajectory, SPW searches for the most similar state-action pairs from expert\ndemonstrations and directly derives stepwise importance weights based on their\nsimilarity scores. These weights are then used to guide standard preference\nlearning, enabling more accurate credit assignment that traditional approaches\nstruggle to achieve. We demonstrate that SPW enables effective joint learning\nfrom preferences and demonstrations, outperforming prior methods that leverage\nboth feedback types on challenging robot manipulation tasks."}
{"id": "2508.15335", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15335", "abs": "https://arxiv.org/abs/2508.15335", "authors": ["Bin Deng", "Yizhe Feng", "Zeming Liu", "Qing Wei", "Xiangrong Zhu", "Shuai Chen", "Yuanfang Guo", "Yunhong Wang"], "title": "RETAIL: Towards Real-world Travel Planning for Large Language Models", "comment": null, "summary": "Although large language models have enhanced automated travel planning\nabilities, current systems remain misaligned with real-world scenarios. First,\nthey assume users provide explicit queries, while in reality requirements are\noften implicit. Second, existing solutions ignore diverse environmental factors\nand user preferences, limiting the feasibility of plans. Third, systems can\nonly generate plans with basic POI arrangements, failing to provide all-in-one\nplans with rich details. To mitigate these challenges, we construct a novel\ndataset \\textbf{RETAIL}, which supports decision-making for implicit queries\nwhile covering explicit queries, both with and without revision needs. It also\nenables environmental awareness to ensure plan feasibility under real-world\nscenarios, while incorporating detailed POI information for all-in-one travel\nplans. Furthermore, we propose a topic-guided multi-agent framework, termed\nTGMA. Our experiments reveal that even the strongest existing model achieves\nmerely a 1.0% pass rate, indicating real-world travel planning remains\nextremely challenging. In contrast, TGMA demonstrates substantially improved\nperformance 2.72%, offering promising directions for real-world travel\nplanning."}
{"id": "2508.15338", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.15338", "abs": "https://arxiv.org/abs/2508.15338", "authors": ["Jinning Yang", "Wen Shi"], "title": "DiagECG: An LLM-Driven Framework for Diagnostic Reasoning via Discretized ECG Tokenization", "comment": null, "summary": "Electrocardiography plays a central role in cardiovascular diagnostics, yet\nexisting automated approaches often struggle to generalize across clinical\ntasks and offer limited support for open-ended reasoning. We present DiagECG, a\nnovel framework that integrates time-series and language modeling by enabling\nlarge language models to process 12-lead ECG signals for clinical text\ngeneration tasks. Our approach discretizes continuous ECG embeddings into\nsymbolic tokens using a lead-independent encoder and quantization module. These\ntokens are then used to extend the vocabulary of LLM, allowing the model to\nhandle both ECG and natural language inputs in a unified manner. To bridge the\nmodality gap, we pretrain the model on an autoregressive ECG forecasting task,\nenabling the LLM to model temporal dynamics using its native language modeling\ncapabilities. Finally, we perform instruction tuning on both ECG question\nanswering and diagnostic report generation. Without modifying the core model,\nDiagECG achieves strong performance across tasks while maintaining\ngeneralization to out-of-distribution settings. Extensive experiments\ndemonstrate the effectiveness of each component and highlight the potential of\nintegrating symbolic ECG representations into LLMs for medical reasoning."}
{"id": "2508.15358", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15358", "abs": "https://arxiv.org/abs/2508.15358", "authors": ["Alberto Pozanco", "Marianela Morales", "Daniel Borrajo", "Manuela Veloso"], "title": "Planning with Minimal Disruption", "comment": null, "summary": "In many planning applications, we might be interested in finding plans that\nminimally modify the initial state to achieve the goals. We refer to this\nconcept as plan disruption. In this paper, we formally introduce it, and define\nvarious planning-based compilations that aim to jointly optimize both the sum\nof action costs and plan disruption. Experimental results in different\nbenchmarks show that the reformulated task can be effectively solved in\npractice to generate plans that balance both objectives."}
{"id": "2508.15432", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.15432", "abs": "https://arxiv.org/abs/2508.15432", "authors": ["Bidyapati Pradhan", "Surajit Dasgupta", "Amit Kumar Saha", "Omkar Anustoop", "Sriram Puttagunta", "Vipul Mittal", "Gopal Sarda"], "title": "GraSP: A Unified Graph-Based Framework for Scalable Generation, Quality Tagging, and Management of Synthetic Data for SFT and DPO", "comment": null, "summary": "The advancement of large language models (LLMs) is critically dependent on\nthe availability of high-quality datasets for Supervised Fine-Tuning (SFT),\nalignment tasks like Direct Preference Optimization (DPO), etc. In this work,\nwe present a comprehensive synthetic data generation framework that facilitates\nscalable, configurable, and high-fidelity generation of synthetic data tailored\nfor these training paradigms. Our approach employs a modular and\nconfiguration-based pipeline capable of modeling complex dialogue flows with\nminimal manual intervention. This framework uses a dual-stage quality tagging\nmechanism, combining heuristic rules and LLM-based evaluations, to\nautomatically filter and score data extracted from OASST-formatted\nconversations, ensuring the curation of high-quality dialogue samples. The\nresulting datasets are structured under a flexible schema supporting both SFT\nand DPO use cases, enabling seamless integration into diverse training\nworkflows. Together, these innovations offer a robust solution for generating\nand managing synthetic conversational data at scale, significantly reducing the\noverhead of data preparation in LLM training pipelines."}
{"id": "2508.15447", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15447", "abs": "https://arxiv.org/abs/2508.15447", "authors": ["Zihao Wang", "Junming Zhang"], "title": "From Bits to Boardrooms: A Cutting-Edge Multi-Agent LLM Framework for Business Excellence", "comment": "Accepted by ECAI 2025", "summary": "Large Language Models (LLMs) have shown promising potential in business\napplications, particularly in enterprise decision support and strategic\nplanning, yet current approaches often struggle to reconcile intricate\noperational analyses with overarching strategic goals across diverse market\nenvironments, leading to fragmented workflows and reduced collaboration across\norganizational levels. This paper introduces BusiAgent, a novel multi-agent\nframework leveraging LLMs for advanced decision-making in complex corporate\nenvironments. BusiAgent integrates three core innovations: an extended\nContinuous Time Markov Decision Process (CTMDP) for dynamic agent modeling, a\ngeneralized entropy measure to optimize collaborative efficiency, and a\nmulti-level Stackelberg game to handle hierarchical decision processes.\nAdditionally, contextual Thompson sampling is employed for prompt optimization,\nsupported by a comprehensive quality assurance system to mitigate errors.\nExtensive empirical evaluations across diverse business scenarios validate\nBusiAgent's efficacy, demonstrating its capacity to generate coherent,\nclient-focused solutions that smoothly integrate granular insights with\nhigh-level strategy, significantly outperforming established approaches in both\nsolution quality and user satisfaction. By fusing cutting-edge AI technologies\nwith deep business insights, BusiAgent marks a substantial step forward in\nAI-driven enterprise decision-making, empowering organizations to navigate\ncomplex business landscapes more effectively."}
{"id": "2508.15507", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.15507", "abs": "https://arxiv.org/abs/2508.15507", "authors": ["Yekun Zhu", "Guang Chen", "Chengjun Mao"], "title": "Think in Blocks: Adaptive Reasoning from Direct Response to Deep Reasoning", "comment": null, "summary": "Large Language Models (LLMs) with chains-of-thought have demonstrated strong\nperformance on an increasing range of tasks, particularly those involving\ncomplex logical reasoning. However, excessively long chains can lead to\noverthinking, causing computational waste and slower responses. This raises a\nquestion: can LLMs dynamically adjust the length of their reasoning processes\nbased on task complexity? To address this, we propose the Think in Blocks\nframework, which enables adaptive reasoning-from zero to deep reasoning-by\npartitioning the reasoning process into a tunable number of blocks. Our main\ncontributions are: (1) Establishing an explicit block-structured paradigm in\nwhich the model first predicts an integer reasoning budget-the number of\nblocks-and then partitions its reasoning accordingly; (2) Training an adaptive\nmodel through a three-stage pipeline-Supervised Fine-Tuning, reward-guided\nDirect Preference Optimization, and Reinforcement Learning-that adjusts its\nreasoning depth to problem difficulty; (3) Exploiting the explicit block count\nto dynamically control reasoning depth at inference time, allowing flexible\nadjustment of chain-of-thought length during deployment."}
{"id": "2508.15510", "categories": ["cs.AI", "I.2.11; I.2.0; J.4; K.4.0; I.2.6"], "pdf": "https://arxiv.org/pdf/2508.15510", "abs": "https://arxiv.org/abs/2508.15510", "authors": ["Filippo Tonini", "Lukas Galke"], "title": "Super-additive Cooperation in Language Model Agents", "comment": "FAIEMA 2025", "summary": "With the prospect of autonomous artificial intelligence (AI) agents, studying\ntheir tendency for cooperative behavior becomes an increasingly relevant topic.\nThis study is inspired by the super-additive cooperation theory, where the\ncombined effects of repeated interactions and inter-group rivalry have been\nargued to be the cause for cooperative tendencies found in humans. We devised a\nvirtual tournament where language model agents, grouped into teams, face each\nother in a Prisoner's Dilemma game. By simulating both internal team dynamics\nand external competition, we discovered that this blend substantially boosts\nboth overall and initial, one-shot cooperation levels (the tendency to\ncooperate in one-off interactions). This research provides a novel framework\nfor large language models to strategize and act in complex social scenarios and\noffers evidence for how intergroup competition can, counter-intuitively, result\nin more cooperative behavior. These insights are crucial for designing future\nmulti-agent AI systems that can effectively work together and better align with\nhuman values. Source code is available at\nhttps://github.com/pippot/Superadditive-cooperation-LLMs."}
{"id": "2508.15548", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15548", "abs": "https://arxiv.org/abs/2508.15548", "authors": ["Jiayi Song", "Rui Wan", "Lipeng Ma", "Weidong Yang", "Qingyuan Zhou", "Yixuan Li", "Ben Fei"], "title": "DeepThink3D: Enhancing Large Language Models with Programmatic Reasoning in Complex 3D Situated Reasoning Tasks", "comment": null, "summary": "This work enhances the ability of large language models (LLMs) to perform\ncomplex reasoning in 3D scenes. Recent work has addressed the 3D situated\nreasoning task by invoking tool usage through large language models. Large\nlanguage models call tools via APIs and integrate the generated programs\nthrough a chain of thought to solve problems based on the program results.\nHowever, due to the simplicity of the questions in the dataset, the generated\nprogram reasoning chains are relatively short. To solve this main challenge, in\nthis paper, we introduce DeepThink3D to enhance the tool usage of LLMs in\ncomplex 3D situated reasoning tasks. Our work proposes a combinatorial and\niterative evolutionary approach on the SQA3D benchmark to generate more complex\nquestions. Building on this foundation, we fine-tune the large language model\nto make it more proficient in using 3D tools. By employing Direct Preference\nOptimization (DPO), we directly optimize the toolchain strategies generated by\nmodels, thereby enhancing their accuracy in complex tasks."}
{"id": "2508.15588", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15588", "abs": "https://arxiv.org/abs/2508.15588", "authors": ["Ahmed Nasir", "Abdelhafid Zenati"], "title": "A Dynamical Systems Framework for Reinforcement Learning Safety and Robustness Verification", "comment": null, "summary": "The application of reinforcement learning to safety-critical systems is\nlimited by the lack of formal methods for verifying the robustness and safety\nof learned policies. This paper introduces a novel framework that addresses\nthis gap by analyzing the combination of an RL agent and its environment as a\ndiscrete-time autonomous dynamical system. By leveraging tools from dynamical\nsystems theory, specifically the Finite-Time Lyapunov Exponent (FTLE), we\nidentify and visualize Lagrangian Coherent Structures (LCS) that act as the\nhidden \"skeleton\" governing the system's behavior. We demonstrate that\nrepelling LCS function as safety barriers around unsafe regions, while\nattracting LCS reveal the system's convergence properties and potential failure\nmodes, such as unintended \"trap\" states. To move beyond qualitative\nvisualization, we introduce a suite of quantitative metrics, Mean Boundary\nRepulsion (MBR), Aggregated Spurious Attractor Strength (ASAS), and\nTemporally-Aware Spurious Attractor Strength (TASAS), to formally measure a\npolicy's safety margin and robustness. We further provide a method for deriving\nlocal stability guarantees and extend the analysis to handle model uncertainty.\nThrough experiments in both discrete and continuous control environments, we\nshow that this framework provides a comprehensive and interpretable assessment\nof policy behavior, successfully identifying critical flaws in policies that\nappear successful based on reward alone."}
{"id": "2508.15610", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.15610", "abs": "https://arxiv.org/abs/2508.15610", "authors": ["Alfio Gliozzo", "Naweed Khan", "Christodoulos Constantinides", "Nandana Mihindukulasooriya", "Nahuel Defosse", "Junkyu Lee"], "title": "Transduction is All You Need for Structured Data Workflows", "comment": "32 pages, 8 figures", "summary": "This paper introduces Agentics, a modular framework for building agent-based\nsystems capable of structured reasoning and compositional generalization over\ncomplex data. Designed with research and practical applications in mind,\nAgentics offers a novel perspective on working with data and AI workflows. In\nthis framework, agents are abstracted from the logical flow and they are used\ninternally to the data type to enable logical transduction among data. Agentics\nencourages AI developers to focus on modeling data rather than crafting\nprompts, enabling a declarative language in which data types are provided by\nLLMs and composed through logical transduction, which is executed by LLMs when\ntypes are connected. We provide empirical evidence demonstrating the\napplicability of this framework across domain-specific multiple-choice question\nanswering, semantic parsing for text-to-SQL, and automated prompt optimization\ntasks, achieving state-of-the-art accuracy or improved scalability without\nsacrificing performance. The open-source implementation is available at\n\\texttt{https://github.com/IBM/agentics}."}
{"id": "2508.15630", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15630", "abs": "https://arxiv.org/abs/2508.15630", "authors": ["Meera Ray", "Christopher L. Dancy"], "title": "Adapting A Vector-Symbolic Memory for Lisp ACT-R", "comment": "6 pages. 5 figures. Submitted and accepted to the 23rd International\n  Conference on Cognitive Modeling (ICCM 2025)", "summary": "Holographic Declarative Memory (HDM) is a vector-symbolic alternative to\nACT-R's Declarative Memory (DM) system that can bring advantages such as\nscalability and architecturally defined similarity between DM chunks. We\nadapted HDM to work with the most comprehensive and widely-used implementation\nof ACT-R (Lisp ACT-R) so extant ACT-R models designed with DM can be run with\nHDM without major changes. With this adaptation of HDM, we have developed\nvector-based versions of common ACT-R functions, set up a text processing\npipeline to add the contents of large documents to ACT-R memory, and most\nsignificantly created a useful and novel mechanism to retrieve an entire chunk\nof memory based on a request using only vector representations of tokens.\nPreliminary results indicate that we can maintain vector-symbolic advantages of\nHDM (e.g., chunk recall without storing the actual chunk and other advantages\nwith scaling) while also extending it so that previous ACT-R models may work\nwith the system with little (or potentially no) modifications within the actual\nprocedural and declarative memory portions of a model. As a part of iterative\nimprovement of this newly translated holographic declarative memory module, we\nwill continue to explore better time-context representations for vectors to\nimprove the module's ability to reconstruct chunks during recall. To more fully\ntest this translated HDM module, we also plan to develop decision-making models\nthat use instance-based learning (IBL) theory, which is a useful application of\nHDM given the advantages of the system."}
{"id": "2508.15652", "categories": ["cs.AI", "cs.IT", "cs.LG", "cs.MA", "math.IT"], "pdf": "https://arxiv.org/pdf/2508.15652", "abs": "https://arxiv.org/abs/2508.15652", "authors": ["Ardian Selmonaj", "Miroslav Strupl", "Oleg Szehr", "Alessandro Antonucci"], "title": "Understanding Action Effects through Instrumental Empowerment in Multi-Agent Reinforcement Learning", "comment": "European Conference on Artificial Intelligence (ECAI) 2025", "summary": "To reliably deploy Multi-Agent Reinforcement Learning (MARL) systems, it is\ncrucial to understand individual agent behaviors within a team. While prior\nwork typically evaluates overall team performance based on explicit reward\nsignals or learned value functions, it is unclear how to infer agent\ncontributions in the absence of any value feedback. In this work, we\ninvestigate whether meaningful insights into agent behaviors can be extracted\nthat are consistent with the underlying value functions, solely by analyzing\nthe policy distribution. Inspired by the phenomenon that intelligent agents\ntend to pursue convergent instrumental values, which generally increase the\nlikelihood of task success, we introduce Intended Cooperation Values (ICVs), a\nmethod based on information-theoretic Shapley values for quantifying each\nagent's causal influence on their co-players' instrumental empowerment.\nSpecifically, ICVs measure an agent's action effect on its teammates' policies\nby assessing their decision uncertainty and preference alignment. The analysis\nacross cooperative and competitive MARL environments reveals the extent to\nwhich agents adopt similar or diverse strategies. By comparing action effects\nbetween policies and value functions, our method identifies which agent\nbehaviors are beneficial to team success, either by fostering deterministic\ndecisions or by preserving flexibility for future action choices. Our proposed\nmethod offers novel insights into cooperation dynamics and enhances\nexplainability in MARL systems."}
{"id": "2508.15680", "categories": ["cs.AI", "cs.HC", "I.2.6; I.2.11; K.4.1; K.6.0"], "pdf": "https://arxiv.org/pdf/2508.15680", "abs": "https://arxiv.org/abs/2508.15680", "authors": ["Mark Cote", "Susana Aires"], "title": "Futurity as Infrastructure: A Techno-Philosophical Interpretation of the AI Lifecycle", "comment": "15 pages, 3 figures, Presented at IAIL 2025 - Imagining the AI\n  Landscape after the AI Act, 4th International Workshop on Imagining the AI\n  Landscape After the AI Act, The fourth International Conference on Hybrid\n  Human-Artificial Intelligence", "summary": "This paper argues that a techno-philosophical reading of the EU AI Act\nprovides insight into the long-term dynamics of data in AI systems,\nspecifically, how the lifecycle from ingestion to deployment generates\nrecursive value chains that challenge existing frameworks for Responsible AI.\nWe introduce a conceptual tool to frame the AI pipeline, spanning data,\ntraining regimes, architectures, feature stores, and transfer learning. Using\ncross-disciplinary methods, we develop a technically grounded and\nphilosophically coherent analysis of regulatory blind spots. Our central claim\nis that what remains absent from policymaking is an account of the dynamic of\nbecoming that underpins both the technical operation and economic logic of AI.\nTo address this, we advance a formal reading of AI inspired by Simondonian\nphilosophy of technology, reworking his concept of individuation to model the\nAI lifecycle, including the pre-individual milieu, individuation, and\nindividuated AI. To translate these ideas, we introduce futurity: the\nself-reinforcing lifecycle of AI, where more data enhances performance, deepens\npersonalisation, and expands application domains. Futurity highlights the\nrecursively generative, non-rivalrous nature of data, underpinned by\ninfrastructures like feature stores that enable feedback, adaptation, and\ntemporal recursion. Our intervention foregrounds escalating power asymmetries,\nparticularly the tech oligarchy whose infrastructures of capture, training, and\ndeployment concentrate value and decision-making. We argue that effective\nregulation must address these infrastructural and temporal dynamics, and\npropose measures including lifecycle audits, temporal traceability, feedback\naccountability, recursion transparency, and a right to contest recursive reuse."}
{"id": "2508.15690", "categories": ["cs.AI", "cs.LG", "cs.MM"], "pdf": "https://arxiv.org/pdf/2508.15690", "abs": "https://arxiv.org/abs/2508.15690", "authors": ["Abhigya Verma", "Sriram Puttagunta", "Seganrasan Subramanian", "Sravan Ramachandran"], "title": "GRAFT: GRaPH and Table Reasoning for Textual Alignment -- A Benchmark for Structured Instruction Following and Visual Reasoning", "comment": "23 pages, 9 tables, 3 figures", "summary": "GRAFT is a structured multimodal benchmark for evaluating models on\ninstruction-following, visual reasoning, and visual-textual alignment tasks. It\nfeatures programmatically generated charts and synthetically rendered tables,\ncreated with Python visualization libraries to ensure control over data\nsemantics, structure, and clarity. Each GRAFT instance pairs a chart or table\nimage with a systematically generated, multi-step analytical question based\nsolely on visual content. Answers are provided in structured formats such as\nJSON or YAML, supporting consistent evaluation of both reasoning and output\nformat. The benchmark introduces a taxonomy of reasoning types including\ncomparison, trend identification, ranking, aggregation, proportion estimation,\nand anomaly detection to enable comprehensive assessment. Reference answers\nfollow strict factual and formatting guidelines for precise, aspect-based\nevaluation. GRAFT offers a unified, scalable framework for fine-grained\nbenchmarking of multimodal models on visually grounded, structured reasoning\ntasks, setting a new evaluation standard in this field."}
{"id": "2508.15693", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15693", "abs": "https://arxiv.org/abs/2508.15693", "authors": ["Wilka Carvalho", "Vikram Goddla", "Ishaan Sinha", "Hoon Shin", "Kunal Jha"], "title": "NiceWebRL: a Python library for human subject experiments with reinforcement learning environments", "comment": null, "summary": "We present NiceWebRL, a research tool that enables researchers to use machine\nreinforcement learning (RL) environments for online human subject experiments.\nNiceWebRL is a Python library that allows any Jax-based environment to be\ntransformed into an online interface, supporting both single-agent and\nmulti-agent environments. As such, NiceWebRL enables AI researchers to compare\ntheir algorithms to human performance, cognitive scientists to test ML\nalgorithms as theories for human cognition, and multi-agent researchers to\ndevelop algorithms for human-AI collaboration. We showcase NiceWebRL with 3\ncase studies that demonstrate its potential to help develop Human-like AI,\nHuman-compatible AI, and Human-assistive AI. In the first case study\n(Human-like AI), NiceWebRL enables the development of a novel RL model of\ncognition. Here, NiceWebRL facilitates testing this model against human\nparticipants in both a grid world and Craftax, a 2D Minecraft domain. In our\nsecond case study (Human-compatible AI), NiceWebRL enables the development of a\nnovel multi-agent RL algorithm that can generalize to human partners in the\nOvercooked domain. Finally, in our third case study (Human-assistive AI), we\nshow how NiceWebRL can allow researchers to study how an LLM can assist humans\non complex tasks in XLand-Minigrid, an environment with millions of\nhierarchical tasks. The library is available at\nhttps://github.com/KempnerInstitute/nicewebrl."}
{"id": "2508.15734", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15734", "abs": "https://arxiv.org/abs/2508.15734", "authors": ["Cooper Elsworth", "Keguo Huang", "David Patterson", "Ian Schneider", "Robert Sedivy", "Savannah Goodman", "Ben Townsend", "Parthasarathy Ranganathan", "Jeff Dean", "Amin Vahdat", "Ben Gomes", "James Manyika"], "title": "Measuring the environmental impact of delivering AI at Google Scale", "comment": null, "summary": "The transformative power of AI is undeniable - but as user adoption\naccelerates, so does the need to understand and mitigate the environmental\nimpact of AI serving. However, no studies have measured AI serving\nenvironmental metrics in a production environment. This paper addresses this\ngap by proposing and executing a comprehensive methodology for measuring the\nenergy usage, carbon emissions, and water consumption of AI inference workloads\nin a large-scale, AI production environment. Our approach accounts for the full\nstack of AI serving infrastructure - including active AI accelerator power,\nhost system energy, idle machine capacity, and data center energy overhead.\nThrough detailed instrumentation of Google's AI infrastructure for serving the\nGemini AI assistant, we find the median Gemini Apps text prompt consumes 0.24\nWh of energy - a figure substantially lower than many public estimates. We also\nshow that Google's software efficiency efforts and clean energy procurement\nhave driven a 33x reduction in energy consumption and a 44x reduction in carbon\nfootprint for the median Gemini Apps text prompt over one year. We identify\nthat the median Gemini Apps text prompt uses less energy than watching nine\nseconds of television (0.24 Wh) and consumes the equivalent of five drops of\nwater (0.26 mL). While these impacts are low compared to other daily\nactivities, reducing the environmental impact of AI serving continues to\nwarrant important attention. Towards this objective, we propose that a\ncomprehensive measurement of AI serving environmental metrics is critical for\naccurately comparing models, and to properly incentivize efficiency gains\nacross the full AI serving stack."}
{"id": "2508.15748", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15748", "abs": "https://arxiv.org/abs/2508.15748", "authors": ["Emma Rath", "Stuart Armstrong", "Rebecca Gorman"], "title": "Response and Prompt Evaluation to Prevent Parasocial Relationships with Chatbots", "comment": null, "summary": "The development of parasocial relationships with AI agents has severe, and in\nsome cases, tragic effects for human well-being. Yet preventing such dynamics\nis challenging: parasocial cues often emerge gradually in private\nconversations, and not all forms of emotional engagement are inherently\nharmful. We address this challenge by introducing a simple response evaluation\nframework, created by repurposing a state-of-the-art language model, that\nevaluates ongoing conversations for parasocial cues in real time. To test the\nfeasibility of this approach, we constructed a small synthetic dataset of\nthirty dialogues spanning parasocial, sycophantic, and neutral conversations.\nIterative evaluation with five stage testing successfully identified all\nparasocial conversations while avoiding false positives under a tolerant\nunanimity rule, with detection typically occurring within the first few\nexchanges. These findings provide preliminary evidence that evaluation agents\ncan provide a viable solution for the prevention of parasocial relations."}
{"id": "2508.15757", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.15757", "abs": "https://arxiv.org/abs/2508.15757", "authors": ["Yuxing Lu", "Yucheng Hu", "Nan Sun", "Xukai Zhao"], "title": "Language-Guided Tuning: Enhancing Numeric Optimization with Textual Feedback", "comment": "9 pages, 4 figures, 4 tables", "summary": "Configuration optimization remains a critical bottleneck in machine learning,\nrequiring coordinated tuning across model architecture, training strategy,\nfeature engineering, and hyperparameters. Traditional approaches treat these\ndimensions independently and lack interpretability, while recent automated\nmethods struggle with dynamic adaptability and semantic reasoning about\noptimization decisions. We introduce Language-Guided Tuning (LGT), a novel\nframework that employs multi-agent Large Language Models to intelligently\noptimize configurations through natural language reasoning. We apply textual\ngradients - qualitative feedback signals that complement numerical optimization\nby providing semantic understanding of training dynamics and configuration\ninterdependencies. LGT coordinates three specialized agents: an Advisor that\nproposes configuration changes, an Evaluator that assesses progress, and an\nOptimizer that refines the decision-making process, creating a self-improving\nfeedback loop. Through comprehensive evaluation on six diverse datasets, LGT\ndemonstrates substantial improvements over traditional optimization methods,\nachieving performance gains while maintaining high interpretability."}
{"id": "2508.15031", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.15031", "abs": "https://arxiv.org/abs/2508.15031", "authors": ["Kaixiang Zhao", "Lincan Li", "Kaize Ding", "Neil Zhenqiang Gong", "Yue Zhao", "Yushun Dong"], "title": "A Systematic Survey of Model Extraction Attacks and Defenses: State-of-the-Art and Perspectives", "comment": null, "summary": "Machine learning (ML) models have significantly grown in complexity and\nutility, driving advances across multiple domains. However, substantial\ncomputational resources and specialized expertise have historically restricted\ntheir wide adoption. Machine-Learning-as-a-Service (MLaaS) platforms have\naddressed these barriers by providing scalable, convenient, and affordable\naccess to sophisticated ML models through user-friendly APIs. While this\naccessibility promotes widespread use of advanced ML capabilities, it also\nintroduces vulnerabilities exploited through Model Extraction Attacks (MEAs).\nRecent studies have demonstrated that adversaries can systematically replicate\na target model's functionality by interacting with publicly exposed interfaces,\nposing threats to intellectual property, privacy, and system security. In this\npaper, we offer a comprehensive survey of MEAs and corresponding defense\nstrategies. We propose a novel taxonomy that classifies MEAs according to\nattack mechanisms, defense approaches, and computing environments. Our analysis\ncovers various attack techniques, evaluates their effectiveness, and highlights\nchallenges faced by existing defenses, particularly the critical trade-off\nbetween preserving model utility and ensuring security. We further assess MEAs\nwithin different computing paradigms and discuss their technical, ethical,\nlegal, and societal implications, along with promising directions for future\nresearch. This systematic survey aims to serve as a valuable reference for\nresearchers, practitioners, and policymakers engaged in AI security and\nprivacy. Additionally, we maintain an online repository continuously updated\nwith related literature at https://github.com/kzhao5/ModelExtractionPapers."}
{"id": "2508.15036", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15036", "abs": "https://arxiv.org/abs/2508.15036", "authors": ["Ruyi Ding", "Tianhong Xu", "Xinyi Shen", "Aidong Adam Ding", "Yunsi Fei"], "title": "MoEcho: Exploiting Side-Channel Attacks to Compromise User Privacy in Mixture-of-Experts LLMs", "comment": "This paper will appear in CCS 2025", "summary": "The transformer architecture has become a cornerstone of modern AI, fueling\nremarkable progress across applications in natural language processing,\ncomputer vision, and multimodal learning. As these models continue to scale\nexplosively for performance, implementation efficiency remains a critical\nchallenge. Mixture of Experts (MoE) architectures, selectively activating\nspecialized subnetworks (experts), offer a unique balance between model\naccuracy and computational cost. However, the adaptive routing in MoE\narchitectures, where input tokens are dynamically directed to specialized\nexperts based on their semantic meaning inadvertently opens up a new attack\nsurface for privacy breaches. These input-dependent activation patterns leave\ndistinctive temporal and spatial traces in hardware execution, which\nadversaries could exploit to deduce sensitive user data. In this work, we\npropose MoEcho, discovering a side channel analysis based attack surface that\ncompromises user privacy on MoE based systems. Specifically, in MoEcho, we\nintroduce four novel architectural side channels on different computing\nplatforms, including Cache Occupancy Channels and Pageout+Reload on CPUs, and\nPerformance Counter and TLB Evict+Reload on GPUs, respectively. Exploiting\nthese vulnerabilities, we propose four attacks that effectively breach user\nprivacy in large language models (LLMs) and vision language models (VLMs) based\non MoE architectures: Prompt Inference Attack, Response Reconstruction Attack,\nVisual Inference Attack, and Visual Reconstruction Attack. MoEcho is the first\nruntime architecture level security analysis of the popular MoE structure\ncommon in modern transformers, highlighting a serious security and privacy\nthreat and calling for effective and timely safeguards when harnessing MoE\nbased models for developing efficient large scale AI services."}
{"id": "2508.15310", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.15310", "abs": "https://arxiv.org/abs/2508.15310", "authors": ["Hengyu An", "Jinghuai Zhang", "Tianyu Du", "Chunyi Zhou", "Qingming Li", "Tao Lin", "Shouling Ji"], "title": "IPIGuard: A Novel Tool Dependency Graph-Based Defense Against Indirect Prompt Injection in LLM Agents", "comment": "EMNLP 2025", "summary": "Large language model (LLM) agents are widely deployed in real-world\napplications, where they leverage tools to retrieve and manipulate external\ndata for complex tasks. However, when interacting with untrusted data sources\n(e.g., fetching information from public websites), tool responses may contain\ninjected instructions that covertly influence agent behaviors and lead to\nmalicious outcomes, a threat referred to as Indirect Prompt Injection (IPI).\nExisting defenses typically rely on advanced prompting strategies or auxiliary\ndetection models. While these methods have demonstrated some effectiveness,\nthey fundamentally rely on assumptions about the model's inherent security,\nwhich lacks structural constraints on agent behaviors. As a result, agents\nstill retain unrestricted access to tool invocations, leaving them vulnerable\nto stronger attack vectors that can bypass the security guardrails of the\nmodel. To prevent malicious tool invocations at the source, we propose a novel\ndefensive task execution paradigm, called IPIGuard, which models the agents'\ntask execution process as a traversal over a planned Tool Dependency Graph\n(TDG). By explicitly decoupling action planning from interaction with external\ndata, IPIGuard significantly reduces unintended tool invocations triggered by\ninjected instructions, thereby enhancing robustness against IPI attacks.\nExperiments on the AgentDojo benchmark show that IPIGuard achieves a superior\nbalance between effectiveness and robustness, paving the way for the\ndevelopment of safer agentic systems in dynamic environments."}
{"id": "2508.15423", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15423", "abs": "https://arxiv.org/abs/2508.15423", "authors": ["Ruiqi Wang", "Zezhou Yang", "Cuiyun Gao", "Xin Xia", "Qing Liao"], "title": "An Empirical Study of Knowledge Distillation for Code Understanding Tasks", "comment": "Accepted by ICSE 2026 (Cycle 1)", "summary": "Pre-trained language models (PLMs) have emerged as powerful tools for code\nunderstanding. However, deploying these PLMs in large-scale applications faces\npractical challenges due to their computational intensity and inference\nlatency. Knowledge distillation (KD), a promising model compression and\nacceleration technique, addresses these limitations by transferring knowledge\nfrom large teacher models to compact student models, enabling efficient\ninference while preserving most of the teacher models' capabilities. While this\ntechnique has shown remarkable success in natural language processing and\ncomputer vision domains, its potential for code understanding tasks remains\nlargely underexplored.\n  In this paper, we systematically investigate the effectiveness and usage of\nKD in code understanding tasks. Our study encompasses two popular types of KD\nmethods, i.e., logit-based and feature-based KD methods, experimenting across\neight student models and two teacher PLMs from different domains on three\ndownstream tasks. The experimental results indicate that KD consistently offers\nnotable performance boosts across student models with different sizes compared\nwith standard fine-tuning. Notably, code-specific PLM demonstrates better\neffectiveness as the teacher model. Among all KD methods, the latest\nfeature-based KD methods exhibit superior performance, enabling student models\nto retain up to 98% teacher performance with merely 5% parameters. Regarding\nstudent architecture, our experiments reveal that similarity with teacher\narchitecture does not necessarily lead to better performance. We further\ndiscuss the efficiency and behaviors in the KD process and inference, summarize\nthe implications of findings, and identify promising future directions."}
