<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 5]
- [cs.CR](#cs.CR) [Total: 11]
- [cs.AI](#cs.AI) [Total: 20]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Socio-Technical Smell Dynamics in Code Samples: A Multivocal Review on Emergence, Evolution, and Co-Occurrence](https://arxiv.org/abs/2507.13481)
*Arthur Bueno,Bruno Cafeo,Maria Cagnin,Awdren Fontão*

Main category: cs.SE

TL;DR: 研究探讨了开源生态系统中代码样本的技术异味（如大类、模块化差）和社区异味（如单一贡献者、沟通碎片化）的共现与演变，发现社区异味常预示或加剧技术退化。


<details>
  <summary>Details</summary>
Motivation: 代码样本在开源生态中至关重要，但缺乏正式管理，导致技术和社会问题交织。目前对两者如何相互影响的研究不足。

Method: 采用多声文献综述方法，分析30篇同行评审论文和17篇实践导向文献（2013-2024），通过主题合成识别异味动态的社会技术模式。

Result: 识别出9种模式，显示社区异味常先于或加剧技术退化，如“无线电静默”和集中所有权与结构异常相关。

Conclusion: 开源生态中，社区层面的问题常预示代码样本的可维护性下降，需针对共享教学工件设计轻量级治理机制和社会技术质量指标。

Abstract: Code samples play a pivotal role in open-source ecosystems (OSSECO), serving
as lightweight artifacts that support knowledge transfer, onboarding, and
framework adoption. Despite their instructional relevance, these samples are
often governed informally, with minimal review and unclear ownership, which
increases their exposure to socio-technical degradation. In this context, the
co-occurrence and longitudinal interplay of code smells (e.g., large classes,
poor modularity) and community smells (e.g., lone contributors, fragmented
communication) become particularly critical. While each type of smell has been
studied in isolation, little is known about how community-level dysfunctions
anticipate or exacerbate technical anomalies in code samples over time. This
study investigates how code and community smells emerge, co-occur, and evolve
within code samples maintained in OSSECOs. A Multivocal Literature Review
protocol was applied, encompassing 30 peer-reviewed papers and 17
practitioner-oriented sources (2013-2024). Thematic synthesis was conducted to
identify recurring socio-technical patterns related to smell dynamics. Nine
patterns were identified, showing that community smells often precede or
reinforce technical degradation in code samples. Symptoms such as "radio
silence" and centralized ownership were frequently associated with persistent
structural anomalies. Additionally, limited onboarding, the absence of
continuous refactoring, and informal collaboration emerged as recurring
conditions for smell accumulation. Conclusion: In OSSECOs, particularly within
code samples, community-level dysfunctions not only correlate with but often
signal maintainability decay. These findings underscore the need for
socio-technical quality indicators and lightweight governance mechanisms
tailored to shared instructional artifacts.

</details>


### [2] [AI-Assisted Fixes to Code Review Comments at Scale](https://arxiv.org/abs/2507.13499)
*Chandra Maddila,Negar Ghorbani,James Saindon,Parth Thakkar,Vijayaraghavan Murali,Rui Abreu,Jingyue Shen,Brian Zhou,Nachiappan Nagappan,Peter C. Rigby*

Main category: cs.SE

TL;DR: Meta开发了MetaMateCR工具，通过AI辅助修复代码审查意见，并在生产环境中大规模应用。通过微调Llama模型，其性能优于GPT-4o，但需通过安全试验确保不影响审查效率。


<details>
  <summary>Details</summary>
Motivation: 解决Meta每周数万条代码审查意见的处理效率问题，提供AI辅助的自动化修复方案。

Method: 基于64k数据点微调Llama模型，进行离线测试后投入生产，并通过随机对照试验和全生产实验确保安全性。

Result: LargeLSFT模型离线测试中修复准确率达68%，优于GPT-4o 9个百分点；生产环境中应用率提升9.2个百分点。

Conclusion: MetaMateCR成功展示了AI辅助修复的潜力，同时强调了安全试验对避免效率损失的重要性。

Abstract: Aim. There are 10s of thousands of code review comments each week at Meta. We
developed Metamate for Code Review (MetaMateCR) that provides AI-assisted fixes
for reviewer comments in production at scale.
  Method. We developed an internal benchmark of 64k <review comment, patch>
data points to fine-tune Llama models. Once our models achieve reasonable
offline results, we roll them into production. To ensure that our AI-assisted
fixes do not negatively impact the time it takes to do code reviews, we conduct
randomized controlled safety trials as well as full production experiments.
  Offline Results. As a baseline, we compare GPT-4o to our small and large
Llama models. In offline results, our LargeLSFT model creates an exact match
patch 68% of the time outperforming GPT-4o by 9 percentage points (pp). The
internal models also use more modern Hack functions when compared to the PHP
functions suggested by GPT-4o.
  Safety Trial. When we roll MetaMateCR into production in a safety trial that
compares no AI patches with AI patch suggestions, we see a large regression
with reviewers taking over 5% longer to conduct reviews. After investigation,
we modify the UX to only show authors the AI patches, and see no regressions in
the time for reviews.
  Production. When we roll LargeLSFT into production, we see an
ActionableToApplied rate of 19.7%, which is a 9.2pp improvement over GPT-4o.
Our results illustrate the importance of safety trials in ensuring that AI does
not inadvertently slow down engineers, and a successful review comment to AI
patch product running at scale.

</details>


### [3] [Towards Better Requirements from the Crowd: Developer Engagement with Feature Requests in Open Source Software](https://arxiv.org/abs/2507.13553)
*Pragyan K C,Rambod Ghandiparsi,Thomas Herron,John Heaps,Mitra Bokaei Hosseini*

Main category: cs.SE

TL;DR: 研究探讨了开源软件中功能请求的模糊性和不完整性，以及开发者如何通过澄清对话处理这些问题。


<details>
  <summary>Details</summary>
Motivation: 功能请求常因自然语言表达模糊或不完整导致误解和实施错误，影响软件质量。研究旨在了解开发者如何处理这些问题。

Method: 通过分析开源软件平台上的功能请求和开发者澄清对话，研究其动态和模式。

Result: 发现功能请求普遍存在模糊性和不完整性，开发者更关注项目目标而非文本澄清，澄清时侧重用户意图和可行性。

Conclusion: 研究揭示了澄清对话的模式，为改善用户与开发者协作和处理功能请求提供了实践建议。

Abstract: As user demands evolve, effectively incorporating feature requests is crucial
for maintaining software relevance and user satisfaction. Feature requests,
typically expressed in natural language, often suffer from ambiguity or
incomplete information due to communication gaps or the requester's limited
technical expertise. These issues can lead to misinterpretation, faulty
implementation, and reduced software quality. While seeking clarification from
requesters is a common strategy to mitigate these risks, little is known about
how developers engage in this clarification process in practice-how they
formulate clarifying questions, seek technical or contextual details, align on
goals and use cases, or decide to close requests without attempting
clarification. This study investigates how feature requests are prone to NL
defects (i.e. ambiguous or incomplete) and the conversational dynamics of
clarification in open-source software (OSS) development, aiming to understand
how developers handle ambiguous or incomplete feature requests. Our findings
suggest that feature requests published on the OSS platforms do possess
ambiguity and incompleteness, and in some cases, both. We also find that
explicit clarification for the resolution of these defects is uncommon;
developers usually focus on aligning with project goals rather than resolving
unclear text. When clarification occurs, it emphasizes understanding user
intent/goal and feasibility, rather than technical details. By characterizing
the dynamics of clarification in open-source issue trackers, this work
identifies patterns that can improve user-developer collaboration and inform
best practices for handling feature requests effectively.

</details>


### [4] [Demystifying Feature Requests: Leveraging LLMs to Refine Feature Requests in Open-Source Software](https://arxiv.org/abs/2507.13555)
*Pragyan K C,Rambod Ghandiparsi,Thomas Herron,John Heaps,Mitra Bokaei Hosseini*

Main category: cs.SE

TL;DR: 论文提出了一种利用大语言模型（LLM）检测和改进自然语言（NL）功能请求中的缺陷的方法，以提高其清晰度和完整性。


<details>
  <summary>Details</summary>
Motivation: 随着软件应用的普及和需求的快速变化，用户提供的功能请求常存在模糊和不完整的问题，传统验证方法在开源环境中不实用。

Method: 利用LLM自动识别模糊和不完整的请求，并生成澄清问题（CQs）以改进请求。

Result: 方法在真实开源功能请求上进行了评估，并与人工标注进行了比较，同时通过开发者访谈深入了解了缺陷的影响。

Conclusion: 该方法能有效提升功能请求的质量，对下游软件工程任务有积极影响。

Abstract: The growing popularity and widespread use of software applications (apps)
across various domains have driven rapid industry growth. Along with this
growth, fast-paced market changes have led to constantly evolving software
requirements. Such requirements are often grounded in feature requests and
enhancement suggestions, typically provided by users in natural language (NL).
However, these requests often suffer from defects such as ambiguity and
incompleteness, making them challenging to interpret. Traditional validation
methods (e.g., interviews and workshops) help clarify such defects but are
impractical in decentralized environments like open-source software (OSS),
where change requests originate from diverse users on platforms like GitHub.
This paper proposes a novel approach leveraging Large Language Models (LLMs) to
detect and refine NL defects in feature requests. Our approach automates the
identification of ambiguous and incomplete requests and generates clarification
questions (CQs) to enhance their usefulness for developers. To evaluate its
effectiveness, we apply our method to real-world OSS feature requests and
compare its performance against human annotations. In addition, we conduct
interviews with GitHub developers to gain deeper insights into their
perceptions of NL defects, the strategies they use to address these defects,
and the impact of defects on downstream software engineering (SE) tasks.

</details>


### [5] [Testing Autonomous Driving Systems -- What Really Matters and What Doesn't](https://arxiv.org/abs/2507.13661)
*Changwen Li,Joseph Sifakis,Rongjie Yan,Jian Zhang*

Main category: cs.SE

TL;DR: 本文提出了一种评估自动驾驶系统测试方法的框架，指出当前多数方法在有效性和有效性验证上存在不足，并强调测试结果受自动驾驶设计的影响。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶系统测试领域缺乏统一的技术评估标准，导致测试方法分散且效果不佳。

Method: 提出一个框架，比较现有测试方法的内在有效性和有效性验证，并分析自动驾驶设计对测试结果的影响。

Result: 多数测试方法未能满足有效性和有效性验证要求，且测试结果高度依赖自动驾驶的设计原则。

Conclusion: 当前技术无法为自动驾驶提供足够强的保证，建议开发时注重合理性和确定性。

Abstract: Despite extensive research, the testing of autonomous driving systems (ADS)
landscape remains fragmented, and there is currently no basis for an informed
technical assessment of the importance and contribution of the current state of
the art. This paper attempts to address this problem by exploring two
complementary aspects.
  First, it proposes a framework for comparing existing test methods in terms
of their intrinsic effectiveness and validity. It shows that many methods do
not meet both of these requirements. Either because they are based on criteria
that do not allow for rapid, inexpensive, and comprehensive detection of
failures, or because the degree of validity of the properties tested cannot be
accurately estimated. In particular, it is shown that most critical test
methods do not take into account the nominal operational capabilities of
autopilots and generate scenarios that are impossible for the tested vehicles
to handle, resulting in unjustified rejections.
  Secondly, the paper shows that test effectiveness and validity are highly
dependent on how autopilots are designed: how they choose between different
control policies to perform maneuvers, as well as on the reproducibility of the
results. In fact, most test methods take for granted two principles underlying
traditional methods, but do not generally apply to ADS. We maintain that the
absence of rationality and determinacy significantly impairs the effectiveness
and validity of test methods, and provide test results on eight open
autopilots, in which most do not satisfy these properties, thereby illustrating
this fact.
  We conclude that under the current state of the art, it is impossible to
obtain strong enough guarantees for essential autopilot properties and
recommend that autopilots be developed with a view to both rationality and
determinacy.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [6] [A Novel APVD Steganography Technique Incorporating Pseudorandom Pixel Selection for Robust Image Security](https://arxiv.org/abs/2507.13367)
*Mehrab Hosain,Rajiv Kapoor*

Main category: cs.CR

TL;DR: 本文提出了一种结合自适应像素值差分（APVD）和伪随机像素选择的新型隐写方法，解决了传统APVD中的“未使用块”问题，显著提升了安全性、嵌入容量和图像质量。


<details>
  <summary>Details</summary>
Motivation: 传统APVD隐写方法存在“未使用块”问题，导致安全性降低、嵌入容量受限和图像质量下降。

Method: 结合APVD与伪随机像素选择，优化隐写策略。

Result: 新方法在安全性、数据隐藏容量和图像质量（如PSNR、UIQ、SSIM）方面优于现有技术。

Conclusion: 该方法适用于多种图像类型，确保安全传输的同时保持图像美学质量。

Abstract: Steganography is the process of embedding secret information discreetly
within a carrier, ensuring secure exchange of confidential data. The Adaptive
Pixel Value Differencing (APVD) steganography method, while effective,
encounters certain challenges like the "unused blocks" issue. This problem can
cause a decrease in security, compromise the embedding capacity, and lead to
lower visual quality. This research presents a novel steganographic strategy
that integrates APVD with pseudorandom pixel selection to effectively mitigate
these issues. The results indicate that the new method outperforms existing
techniques in aspects of security, data hiding capacity, and the preservation
of image quality. Empirical results reveal that the combination of APVD with
pseudorandom pixel selection significantly enhances key image quality metrics
such as Peak Signal-to-Noise Ratio (PSNR), Universal Image Quality Index (UIQ),
and Structural Similarity Index (SSIM), surpassing other contemporary methods
in performance. The newly proposed method is versatile, able to handle a
variety of cover and secret images in both color and grayscale, thereby
ensuring secure data transmission without compromising the aesthetic quality of
the image.

</details>


### [7] [PHASE: Passive Human Activity Simulation Evaluation](https://arxiv.org/abs/2507.13505)
*Steven Lamp,Jason D. Hiser,Anh Nguyen-Tuong,Jack W. Davidson*

Main category: cs.CR

TL;DR: PHASE是一个机器学习框架，用于通过分析Zeek连接日志区分人类与非人类活动，准确率超过90%，并提出了一种新的DNS标签分类方法。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏定量评估合成用户行为真实性的方法，影响了网络安全模拟环境的有效性。

Method: PHASE框架被动分析Zeek日志，利用DNS记录分类流量，并应用SHAP分析揭示人类行为特征。

Result: PHASE能准确识别非人类行为模式，并通过改进配置显著提升合成用户行为的真实性。

Conclusion: PHASE为网络安全模拟环境提供了高效的行为真实性评估工具，提升了合成用户行为的逼真度。

Abstract: Cybersecurity simulation environments, such as cyber ranges, honeypots, and
sandboxes, require realistic human behavior to be effective, yet no
quantitative method exists to assess the behavioral fidelity of synthetic user
personas. This paper presents PHASE (Passive Human Activity Simulation
Evaluation), a machine learning framework that analyzes Zeek connection logs
and distinguishes human from non-human activity with over 90\% accuracy. PHASE
operates entirely passively, relying on standard network monitoring without any
user-side instrumentation or visible signs of surveillance. All network
activity used for machine learning is collected via a Zeek network appliance to
avoid introducing unnecessary network traffic or artifacts that could disrupt
the fidelity of the simulation environment. The paper also proposes a novel
labeling approach that utilizes local DNS records to classify network traffic,
thereby enabling machine learning analysis. Furthermore, we apply SHAP (SHapley
Additive exPlanations) analysis to uncover temporal and behavioral signatures
indicative of genuine human users. In a case study, we evaluate a synthetic
user persona and identify distinct non-human patterns that undermine behavioral
realism. Based on these insights, we develop a revised behavioral configuration
that significantly improves the human-likeness of synthetic activity yielding a
more realistic and effective synthetic user persona.

</details>


### [8] [FuSeFL: Fully Secure and Scalable Cross-Silo Federated Learning](https://arxiv.org/abs/2507.13591)
*Sahar Ghoflsaz Ghinani,Elaheh Sadredini*

Main category: cs.CR

TL;DR: FuSeFL是一种完全安全且可扩展的联邦学习方案，专为跨机构场景设计，通过轻量级安全多方计算（MPC）实现高效训练，保护数据和模型隐私。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习方法在隐私保护方面存在高计算、通信或内存开销问题，且忽视全局模型的保密性，限制了其实际应用。

Method: FuSeFL采用轻量级MPC技术，将训练分散到客户端对，服务器仅负责安全聚合，避免数据卸载和服务器瓶颈。

Result: FuSeFL显著降低了通信延迟（95%）和服务器内存使用（50%），同时提高了模型准确性。

Conclusion: FuSeFL在安全性和效率上表现优异，适用于大规模跨机构联邦学习场景。

Abstract: Federated Learning (FL) enables collaborative model training without
centralizing client data, making it attractive for privacy-sensitive domains.
While existing approaches employ cryptographic techniques such as homomorphic
encryption, differential privacy, or secure multiparty computation to mitigate
inference attacks-including model inversion, membership inference, and gradient
leakage-they often suffer from high computational, communication, or memory
overheads. Moreover, many methods overlook the confidentiality of the global
model itself, which may be proprietary and sensitive. These challenges limit
the practicality of secure FL, especially in cross-silo deployments involving
large datasets and strict compliance requirements.
  We present FuSeFL, a fully secure and scalable FL scheme designed for
cross-silo settings. FuSeFL decentralizes training across client pairs using
lightweight secure multiparty computation (MPC), while confining the server's
role to secure aggregation. This design eliminates server bottlenecks, avoids
data offloading, and preserves full confidentiality of data, model, and updates
throughout training. FuSeFL defends against inference threats, achieves up to
95% lower communication latency and 50% lower server memory usage, and improves
accuracy over prior secure FL solutions, demonstrating strong security and
efficiency at scale.

</details>


### [9] [GIFT: Gradient-aware Immunization of diffusion models against malicious Fine-Tuning with safe concepts retention](https://arxiv.org/abs/2507.13598)
*Amro Abdalla,Ismail Shaheen,Dan DeGenaro,Rupayan Mallick,Bogdan Raita,Sarah Adel Bargal*

Main category: cs.CR

TL;DR: GIFT是一种梯度感知免疫技术，用于防御扩散模型免受恶意微调攻击，同时保留其生成安全内容的能力。


<details>
  <summary>Details</summary>
Motivation: 现有安全机制（如安全检查器）易被绕过，概念擦除方法在对抗性微调下失效，因此需要一种更鲁棒的防御方法。

Method: 将免疫问题建模为双层优化问题：上层目标通过表示噪声和最大化降低模型对有害概念的表示能力，下层目标保留对安全数据的性能。

Result: 实验表明，GIFT显著削弱模型重新学习有害概念的能力，同时保持对安全内容的生成质量。

Conclusion: GIFT为创建抗对抗性微调攻击的固有安全生成模型提供了有前景的方向。

Abstract: We present GIFT: a {G}radient-aware {I}mmunization technique to defend
diffusion models against malicious {F}ine-{T}uning while preserving their
ability to generate safe content. Existing safety mechanisms like safety
checkers are easily bypassed, and concept erasure methods fail under
adversarial fine-tuning. GIFT addresses this by framing immunization as a
bi-level optimization problem: the upper-level objective degrades the model's
ability to represent harmful concepts using representation noising and
maximization, while the lower-level objective preserves performance on safe
data. GIFT achieves robust resistance to malicious fine-tuning while
maintaining safe generative quality. Experimental results show that our method
significantly impairs the model's ability to re-learn harmful concepts while
maintaining performance on safe content, offering a promising direction for
creating inherently safer generative models resistant to adversarial
fine-tuning attacks.

</details>


### [10] [Large Language Models in Cybersecurity: Applications, Vulnerabilities, and Defense Techniques](https://arxiv.org/abs/2507.13629)
*Niveen O. Jaffal,Mohammed Alkhanafseh,David Mohaisen*

Main category: cs.CR

TL;DR: LLMs在网络安全中的应用及自身漏洞的综述，涵盖关键领域整合与漏洞缓解策略。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs如何通过智能化和自动化提升网络安全，并解决其自身的安全问题。

Method: 综述LLMs在网络安全中的应用，分析其整合领域及自身漏洞，提出缓解策略。

Result: LLMs在网络安全中表现优异，但需关注其自身漏洞，并提供了实用建议。

Conclusion: LLMs为网络安全带来革新，未来需平衡其优势与潜在风险。

Abstract: Large Language Models (LLMs) are transforming cybersecurity by enabling
intelligent, adaptive, and automated approaches to threat detection,
vulnerability assessment, and incident response. With their advanced language
understanding and contextual reasoning, LLMs surpass traditional methods in
tackling challenges across domains such as IoT, blockchain, and hardware
security. This survey provides a comprehensive overview of LLM applications in
cybersecurity, focusing on two core areas: (1) the integration of LLMs into key
cybersecurity domains, and (2) the vulnerabilities of LLMs themselves, along
with mitigation strategies. By synthesizing recent advancements and identifying
key limitations, this work offers practical insights and strategic
recommendations for leveraging LLMs to build secure, scalable, and future-ready
cyber defense systems.

</details>


### [11] [TopicAttack: An Indirect Prompt Injection Attack via Topic Transition](https://arxiv.org/abs/2507.13686)
*Yulin Chen,Haoran Li,Yuexin Li,Yue Liu,Yangqiu Song,Bryan Hooi*

Main category: cs.CR

TL;DR: 论文提出TopicAttack方法，通过平滑的话题转换提高间接提示注入攻击的成功率，实验显示其攻击成功率超过90%。


<details>
  <summary>Details</summary>
Motivation: 现有攻击方法因指令注入突兀而效果有限，需改进攻击的平滑性和隐蔽性。

Method: 提出TopicAttack，生成伪造的对话过渡提示，逐步将话题转向注入指令。

Result: TopicAttack在大多数情况下攻击成功率超过90%，且能抵御多种防御方法。

Conclusion: TopicAttack通过提高注入指令的注意力比例，显著提升了攻击成功率。

Abstract: Large language models (LLMs) have shown remarkable performance across a range
of NLP tasks. However, their strong instruction-following capabilities and
inability to distinguish instructions from data content make them vulnerable to
indirect prompt injection attacks. In such attacks, instructions with malicious
purposes are injected into external data sources, such as web documents. When
LLMs retrieve this injected data through tools, such as a search engine and
execute the injected instructions, they provide misled responses. Recent attack
methods have demonstrated potential, but their abrupt instruction injection
often undermines their effectiveness. Motivated by the limitations of existing
attack methods, we propose TopicAttack, which prompts the LLM to generate a
fabricated conversational transition prompt that gradually shifts the topic
toward the injected instruction, making the injection smoother and enhancing
the plausibility and success of the attack. Through comprehensive experiments,
TopicAttack achieves state-of-the-art performance, with an attack success rate
(ASR) over 90\% in most cases, even when various defense methods are applied.
We further analyze its effectiveness by examining attention scores. We find
that a higher injected-to-original attention ratio leads to a greater success
probability, and our method achieves a much higher ratio than the baseline
methods.

</details>


### [12] [Quantum Blockchain Survey: Foundations, Trends, and Gaps](https://arxiv.org/abs/2507.13720)
*Saurav Ghosh*

Main category: cs.CR

TL;DR: 综述分析了量子计算对区块链的威胁及应对策略，包括后量子区块链和量子区块链的研究进展。


<details>
  <summary>Details</summary>
Motivation: 量子计算威胁传统区块链的加密基础，需探索新的安全解决方案。

Method: 综述了后量子区块链（量子抗性算法）和量子区块链（量子特性利用）的研究，分析其密码学基础、架构设计和实现挑战。

Result: 提供了技术提案的比较，强调安全性、可扩展性和部署的权衡，并指出硬件、共识和网络设计中的开放问题。

Conclusion: 为量子时代推进安全区块链系统提供了结构化、全面的参考。

Abstract: Quantum computing poses fundamental risks to classical blockchain systems by
undermining widely used cryptographic primitives. In response, two major
research directions have emerged: post-quantum blockchains, which integrate
quantum-resistant algorithms, and quantum blockchains, which leverage quantum
properties such as entanglement and quantum key distribution. This survey
reviews key developments in both areas, analyzing their cryptographic
foundations, architectural designs, and implementation challenges. This work
provides a comparative overview of technical proposals, highlight trade-offs in
security, scalability, and deployment, and identify open research problems
across hardware, consensus, and network design. The goal is to offer a
structured and comprehensive reference for advancing secure blockchain systems
in the quantum era.

</details>


### [13] [Developers Insight On Manifest v3 Privacy and Security Webextensions](https://arxiv.org/abs/2507.13926)
*Libor Polčák,Giorgio Maone,Michael McMahon,Martin Bednář*

Main category: cs.CR

TL;DR: 本文研究了Chrome浏览器Manifest v3的挑战与机遇，发现其对不同类型的webextensions影响不一，部分项目需放弃功能或拒绝更新。


<details>
  <summary>Details</summary>
Motivation: 探讨Manifest v3对webextensions隐私、安全和用户体验的影响，以及API变更带来的问题。

Method: 采用深入的定性研究方法，分析Manifest v3的挑战与机遇。

Result: 部分项目观察到积极效果，但多数对用户利益有限、关键API移除或需寻找替代方案表示担忧。

Conclusion: Manifest v3对不同类型webextensions影响各异，部分功能可能丢失，需关注关键API的缺失。

Abstract: Webextensions can improve web browser privacy, security, and user experience.
The APIs offered by the browser to webextensions affect possible functionality.
Currently, Chrome transitions to a modified set of APIs called Manifest v3.
This paper studies the challenges and opportunities of Manifest v3 with an
in-depth structured qualitative research. Even though some projects observed
positive effects, a majority expresses concerns over limited benefits to users,
removal of crucial APIs, or the need to find workarounds. Our findings indicate
that the transition affects different types of webextensions differently; some
can migrate without losing functionality, while other projects remove
functionality or decline to update. The respondents identified several critical
missing APIs, including reliable APIs to inject content scripts, APIs for
storing confidential content, and others.

</details>


### [14] [Chain Table: Protecting Table-Level Data Integrity by Digital Ledger Technology](https://arxiv.org/abs/2507.13932)
*Feng Yu,Ryan Laird*

Main category: cs.CR

TL;DR: 本文提出了一种名为“链表”的数据库内设计，用于在不依赖完整区块链系统的情况下保护数据完整性。


<details>
  <summary>Details</summary>
Motivation: 区块链技术虽能保护数据完整性，但实现完整的区块链系统技术复杂且繁琐。

Method: 设计了一种名为“链表”的数据库内结构，并提出了一套数据写入原则以实现严格的数据安全。

Result: 链表与数据写入原则结合，能够保证灵活的表级数据完整性（TDI）。

Conclusion: 链表设计简洁，技术门槛低且存储开销小，为数据完整性保护提供了高效解决方案。

Abstract: The rise of blockchain and Digital Ledger Technology (DLT) has gained wide
traction. Instead of relying on a traditional centralized data authority, a
blockchain system consists of digitally entangled block data shared across a
distributed network. The specially designed chain data structure and its
consensus mechanism protect blockchain data from being tampered by unauthorized
adversaries. However, implementing a full-fledged blockchain system to protect
a database can be technically cumbersome. In this work, we introduce an
in-database design, named chain table, to protect data integrity without the
need for a blockchain system. It features a succinct design without significant
technology barriers or storage overhead. To realize rigorous data security, we
also propose a set of data writing principles for the chain table. We prove
that the chain table, together with the data writing principles, will guarantee
flexible data integrity, named table-level data integrity (TDI).

</details>


### [15] [The CryptoNeo Threat Modelling Framework (CNTMF): Securing Neobanks and Fintech in Integrated Blockchain Ecosystems](https://arxiv.org/abs/2507.14007)
*Serhan W. Bahar*

Main category: cs.CR

TL;DR: 论文提出CryptoNeo威胁建模框架（CNTMF），用于应对区块链、加密货币和Web3技术融合带来的风险。


<details>
  <summary>Details</summary>
Motivation: 传统金融系统与去中心化技术的结合带来了新的安全风险，如预言机操纵和跨链攻击，需要新的威胁建模方法。

Method: CNTMF扩展了STRIDE、OWASP Top 10等方法，引入混合层分析、CRYPTOQ助记符和AI增强反馈循环。

Result: 基于2025年真实数据，CNTMF帮助减少损失（上半年损失达24.7亿美元）。

Conclusion: CNTMF通过资产映射、风险分析和迭代反馈，有效应对如国家支持攻击等新兴威胁。

Abstract: The rapid integration of blockchain, cryptocurrency, and Web3 technologies
into digital banks and fintech operations has created an integrated environment
blending traditional financial systems with decentralised elements. This paper
introduces the CryptoNeo Threat Modelling Framework (CNTMF), a proposed
framework designed to address the risks in these ecosystems, such as oracle
manipulation and cross-chain exploits. CNTMF represents a proposed extension of
established methodologies like STRIDE, OWASP Top 10, NIST frameworks, LINDDUN,
and PASTA, while incorporating tailored components including Hybrid Layer
Analysis, the CRYPTOQ mnemonic for cryptocurrency-specific risks, and an
AI-Augmented Feedback Loop. Drawing on real-world data from 2025 incidents,
CNTMF supports data-driven mitigation to reduce losses, which totalled
approximately $2.47 billion in the first half of 2025 across 344 security
events (CertiK via GlobeNewswire, 2025; Infosecurity Magazine, 2025). Its
phases guide asset mapping, risk profiling, prioritisation, mitigation, and
iterative feedback. This supports security against evolving risks like
state-sponsored attacks.

</details>


### [16] [An Adversarial-Driven Experimental Study on Deep Learning for RF Fingerprinting](https://arxiv.org/abs/2507.14109)
*Xinyu Cao,Bimal Adhikari,Shangqing Zhao,Jingxian Wu,Yanjun Pan*

Main category: cs.CR

TL;DR: 论文探讨了基于深度学习的射频指纹识别系统的安全漏洞，发现其在域偏移下存在一致的误分类行为，可能被利用为后门攻击。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注系统在无线环境中的鲁棒性，而忽略了深度学习方法的潜在安全风险。

Method: 通过对抗驱动的实验分析，系统研究了深度学习模型的误分类行为及其成因。

Result: 实验表明，模型在域偏移下会误分类设备，且训练原始信号会导致指纹与环境特征纠缠，增加攻击面。

Conclusion: 仅通过后处理安全方法无法完全缓解这些漏洞，需重新设计模型训练方法。

Abstract: Radio frequency (RF) fingerprinting, which extracts unique hardware
imperfections of radio devices, has emerged as a promising physical-layer
device identification mechanism in zero trust architectures and beyond 5G
networks. In particular, deep learning (DL) methods have demonstrated
state-of-the-art performance in this domain. However, existing approaches have
primarily focused on enhancing system robustness against temporal and spatial
variations in wireless environments, while the security vulnerabilities of
these DL-based approaches have often been overlooked. In this work, we
systematically investigate the security risks of DL-based RF fingerprinting
systems through an adversarial-driven experimental analysis. We observe a
consistent misclassification behavior for DL models under domain shifts, where
a device is frequently misclassified as another specific one. Our analysis
based on extensive real-world experiments demonstrates that this behavior can
be exploited as an effective backdoor to enable external attackers to intrude
into the system. Furthermore, we show that training DL models on raw received
signals causes the models to entangle RF fingerprints with environmental and
signal-pattern features, creating additional attack vectors that cannot be
mitigated solely through post-processing security methods such as confidence
thresholds.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [17] [GraphTrafficGPT: Enhancing Traffic Management Through Graph-Based AI Agent Coordination](https://arxiv.org/abs/2507.13511)
*Nabil Abdelaziz Ferhat Taleb,Abdolazim Rezaei,Raj Atulkumar Patel,Mehdi Sookhak*

Main category: cs.AI

TL;DR: GraphTrafficGPT是一种基于图的架构，用于优化LLM驱动的交通管理任务，解决了传统链式系统的高效性问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于链式的交通管理系统（如TrafficGPT）存在任务执行顺序固定、令牌使用率高和扩展性差的问题，无法适应复杂的现实场景。

Method: 提出GraphTrafficGPT，将任务及其依赖关系表示为有向图中的节点和边，通过Brain Agent分解用户查询并协调多个专业代理，实现并行执行和动态资源分配。

Result: 实验表明，GraphTrafficGPT比TrafficGPT减少50.2%的令牌消耗和19.0%的平均响应延迟，同时支持多查询并行执行，效率提升23.0%。

Conclusion: GraphTrafficGPT通过图结构和并行处理显著提升了交通管理系统的效率和可扩展性。

Abstract: Large Language Models (LLMs) offer significant promise for intelligent
traffic management; however, current chain-based systems like TrafficGPT are
hindered by sequential task execution, high token usage, and poor scalability,
making them inefficient for complex, real-world scenarios. To address these
limitations, we propose GraphTrafficGPT, a novel graph-based architecture,
which fundamentally redesigns the task coordination process for LLM-driven
traffic applications. GraphTrafficGPT represents tasks and their dependencies
as nodes and edges in a directed graph, enabling efficient parallel execution
and dynamic resource allocation. The main idea behind the proposed model is a
Brain Agent that decomposes user queries, constructs optimized dependency
graphs, and coordinates a network of specialized agents for data retrieval,
analysis, visualization, and simulation. By introducing advanced context-aware
token management and supporting concurrent multi-query processing, the proposed
architecture handles interdependent tasks typical of modern urban mobility
environments. Experimental results demonstrate that GraphTrafficGPT reduces
token consumption by 50.2% and average response latency by 19.0% compared to
TrafficGPT, while supporting simultaneous multi-query execution with up to
23.0% improvement in efficiency.

</details>


### [18] [PrefPalette: Personalized Preference Modeling with Latent Attributes](https://arxiv.org/abs/2507.13541)
*Shuyue Stella Li,Melanie Sclar,Hunter Lang,Ansong Ni,Jacqueline He,Puxin Xu,Andrew Cohen,Chan Young Park,Yulia Tsvetkov,Asli Celikyilmaz*

Main category: cs.AI

TL;DR: PrefPalette通过分解偏好为属性维度，并基于社交社区动态调整权重，显著提升了偏好预测的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 当前偏好模型将人类判断视为黑箱，无法理解偏好背后的原因，PrefPalette旨在解决这一问题。

Method: PrefPalette采用多属性决策原则，包括生成合成数据以隔离属性效果，以及基于注意力的偏好建模。

Result: 在Reddit的45个社区中，PrefPalette的平均预测准确率比GPT-4o高46.6%，并揭示了社区特定的偏好特征。

Conclusion: PrefPalette不仅提升了预测性能，还提供了透明的、可解释的洞察，为更可信的个性化应用奠定了基础。

Abstract: Personalizing AI systems requires understanding not just what users prefer,
but the reasons that underlie those preferences - yet current preference models
typically treat human judgment as a black box. We introduce PrefPalette, a
framework that decomposes preferences into attribute dimensions and tailors its
preference prediction to distinct social community values in a
human-interpretable manner. PrefPalette operationalizes a cognitive science
principle known as multi-attribute decision making in two ways: (1) a scalable
counterfactual attribute synthesis step that involves generating synthetic
training data to isolate for individual attribute effects (e.g., formality,
humor, cultural values), and (2) attention-based preference modeling that
learns how different social communities dynamically weight these attributes.
This approach moves beyond aggregate preference modeling to capture the diverse
evaluation frameworks that drive human judgment. When evaluated on 45 social
communities from the online platform Reddit, PrefPalette outperforms GPT-4o by
46.6% in average prediction accuracy. Beyond raw predictive improvements,
PrefPalette also shed light on intuitive, community-specific profiles:
scholarly communities prioritize verbosity and stimulation, conflict-oriented
communities value sarcasm and directness, and support-based communities
emphasize empathy. By modeling the attribute-mediated structure of human
judgment, PrefPalette delivers both superior preference modeling and
transparent, interpretable insights, and serves as a first step toward more
trustworthy, value-aware personalized applications.

</details>


### [19] [GOFAI meets Generative AI: Development of Expert Systems by means of Large Language Models](https://arxiv.org/abs/2507.13550)
*Eduardo C. Garrido-Merchán,Cristina Puente*

Main category: cs.AI

TL;DR: 论文提出了一种结合大语言模型（LLMs）和符号系统的新方法，通过结构化提示生成可验证的Prolog知识表示，解决了LLMs的幻觉问题，并提升了专家系统的可靠性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在生成知识时存在幻觉或不可验证的问题，限制了其在敏感领域的应用。

Method: 通过限制领域和结构化提示，生成Prolog符号表示，并由人类专家验证和修正。

Result: 实验表明，该方法在事实准确性和语义连贯性上表现优异，结合了LLMs的召回能力和符号系统的精确性。

Conclusion: 该方法为敏感领域的可靠AI应用奠定了基础，兼具透明性、可扩展性和可靠性。

Abstract: The development of large language models (LLMs) has successfully transformed
knowledge-based systems such as open domain question nswering, which can
automatically produce vast amounts of seemingly coherent information. Yet,
those models have several disadvantages like hallucinations or confident
generation of incorrect or unverifiable facts. In this paper, we introduce a
new approach to the development of expert systems using LLMs in a controlled
and transparent way. By limiting the domain and employing a well-structured
prompt-based extraction approach, we produce a symbolic representation of
knowledge in Prolog, which can be validated and corrected by human experts.
This approach also guarantees interpretability, scalability and reliability of
the developed expert systems. Via quantitative and qualitative experiments with
Claude Sonnet 3.7 and GPT-4.1, we show strong adherence to facts and semantic
coherence on our generated knowledge bases. We present a transparent hybrid
solution that combines the recall capacity of LLMs with the precision of
symbolic systems, thereby laying the foundation for dependable AI applications
in sensitive domains.

</details>


### [20] [Why Isn't Relational Learning Taking Over the World?](https://arxiv.org/abs/2507.13558)
*David Poole*

Main category: cs.AI

TL;DR: 论文探讨了AI应关注实体及其关系，而非仅建模像素和文字，并分析了关系学习未普及的原因及改进方向。


<details>
  <summary>Details</summary>
Motivation: 当前AI主要建模像素和文字，但世界由实体及其关系构成，应直接建模这些实体。

Method: 分析关系学习的现状，探讨其未普及的原因，并提出改进方向。

Result: 关系学习仅在有限场景中应用，需进一步研究以提升其重要性。

Conclusion: 关系学习应成为AI的核心方向，需更多研究以实现其潜力。

Abstract: AI seems to be taking over the world with systems that model pixels, words,
and phonemes. The world is arguably made up, not of pixels, words, and phonemes
but of entities (objects, things, including events) with properties and
relations among them. Surely we should model these, not the perception or
description of them. You might suspect that concentrating on modeling words and
pixels is because all of the (valuable) data in the world is in terms of text
and images. If you look into almost any company you will find their most
valuable data is in spreadsheets, databases and other relational formats. These
are not the form that are studied in introductory machine learning, but are
full of product numbers, student numbers, transaction numbers and other
identifiers that can't be interpreted naively as numbers. The field that
studies this sort of data has various names including relational learning,
statistical relational AI, and many others. This paper explains why relational
learning is not taking over the world -- except in a few cases with restricted
relations -- and what needs to be done to bring it to it's rightful prominence.

</details>


### [21] [BifrostRAG: Bridging Dual Knowledge Graphs for Multi-Hop Question Answering in Construction Safety](https://arxiv.org/abs/2507.13625)
*Yuxin Zhang,Xi Wang,Mo Hu,Zhenyu Zhang*

Main category: cs.AI

TL;DR: BifrostRAG是一种双图RAG集成系统，通过实体网络图和文档导航图建模语言关系和文档结构，显著提升多跳问题回答的准确性。


<details>
  <summary>Details</summary>
Motivation: 解决安全法规信息检索和多跳问题回答中因文本复杂性和结构性问题导致的传统RAG系统性能不足。

Method: 引入双图架构（实体网络图和文档导航图），结合图遍历和向量语义搜索的混合检索机制。

Result: 在多跳问题数据集上，BifrostRAG达到92.8%精确率、85.5%召回率和87.3% F1分数，显著优于单模态RAG基线。

Conclusion: BifrostRAG为LLM驱动的合规检查提供了强大的知识引擎，其双图混合检索机制可推广至其他复杂技术文档领域。

Abstract: Information retrieval and question answering from safety regulations are
essential for automated construction compliance checking but are hindered by
the linguistic and structural complexity of regulatory text. Many
compliance-related queries are multi-hop, requiring synthesis of information
across interlinked clauses. This poses a challenge for traditional
retrieval-augmented generation (RAG) systems. To overcome this, we introduce
BifrostRAG: a dual-graph RAG-integrated system that explicitly models both
linguistic relationships (via an Entity Network Graph) and document structure
(via a Document Navigator Graph). This architecture powers a hybrid retrieval
mechanism that combines graph traversal with vector-based semantic search,
enabling large language models to reason over both the meaning and the
structure of the text. Evaluation on a multi-hop question dataset shows that
BifrostRAG achieves 92.8 percent precision, 85.5 percent recall, and an F1
score of 87.3 percent. These results significantly outperform vector-only and
graph-only RAG baselines that represent current leading approaches. Error
analysis further highlights the comparative advantages of our hybrid method
over single-modality RAGs. These findings establish BifrostRAG as a robust
knowledge engine for LLM-driven compliance checking. Its dual-graph, hybrid
retrieval mechanism offers a transferable blueprint for navigating complex
technical documents across knowledge-intensive engineering domains.

</details>


### [22] [Buggy rule diagnosis for combined steps through final answer evaluation in stepwise tasks](https://arxiv.org/abs/2507.13651)
*Gerben van der Hoek,Johan Jeuring,Rogier Bos*

Main category: cs.AI

TL;DR: 论文提出了一种基于最终答案的自动错误诊断方法，用于解决学生在智能辅导系统中组合多步骤任务时的错误诊断难题，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 学生在智能辅导系统中组合多步骤任务时，可能的路径组合爆炸导致错误诊断困难，而基于最终答案的诊断可以缓解这一问题。

Method: 设计了一种服务，通过自动完成中间输入并根据任务解决方案策略诊断错误，利用最终答案进行错误诊断。

Result: 在二次方程求解任务的数据集（n=1939）中，该方法成功诊断了29.4%的步骤，且与教师诊断的一致性达到97%（n=115）。

Conclusion: 基于最终答案的错误诊断方法具有潜力，可作为进一步研究的基础。

Abstract: Many intelligent tutoring systems can support a student in solving a stepwise
task. When a student combines several steps in one step, the number of possible
paths connecting consecutive inputs may be very large. This combinatorial
explosion makes error diagnosis hard. Using a final answer to diagnose a
combination of steps can mitigate the combinatorial explosion, because there
are generally fewer possible (erroneous) final answers than (erroneous)
solution paths. An intermediate input for a task can be diagnosed by
automatically completing it according to the task solution strategy and
diagnosing this solution. This study explores the potential of automated error
diagnosis based on a final answer. We investigate the design of a service that
provides a buggy rule diagnosis when a student combines several steps. To
validate the approach, we apply the service to an existing dataset (n=1939) of
unique student steps when solving quadratic equations, which could not be
diagnosed by a buggy rule service that tries to connect consecutive inputs with
a single rule. Results show that final answer evaluation can diagnose 29,4% of
these steps. Moreover, a comparison of the generated diagnoses with teacher
diagnoses on a subset (n=115) shows that the diagnoses align in 97% of the
cases. These results can be considered a basis for further exploration of the
approach.

</details>


### [23] [Combining model tracing and constraint-based modeling for multistep strategy diagnoses](https://arxiv.org/abs/2507.13652)
*Gerben van der Hoek,Johan Jeuring,Rogier Bos*

Main category: cs.AI

TL;DR: 提出了一种结合模型追踪和基于约束建模的方法，用于诊断学生在分步任务中的输入，并在多步策略诊断中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的模型追踪和基于约束建模方法各有局限性，前者适用于连续步骤的诊断，后者适用于合并步骤的诊断。结合两者可以更全面地诊断学生输入。

Method: 通过定义约束作为学生输入与策略步骤的共同属性，设计了一个多步策略诊断系统，并在解二次方程的步骤数据集上进行验证。

Result: 系统诊断与教师编码在所有140个学生步骤中完全一致，证明了方法的有效性。

Conclusion: 结合两种方法的多步策略诊断系统能够准确诊断学生输入，即使学生合并了多个步骤。

Abstract: Model tracing and constraint-based modeling are two approaches to diagnose
student input in stepwise tasks. Model tracing supports identifying consecutive
problem-solving steps taken by a student, whereas constraint-based modeling
supports student input diagnosis even when several steps are combined into one
step. We propose an approach that merges both paradigms. By defining
constraints as properties that a student input has in common with a step of a
strategy, it is possible to provide a diagnosis when a student deviates from a
strategy even when the student combines several steps. In this study we explore
the design of a system for multistep strategy diagnoses, and evaluate these
diagnoses. As a proof of concept, we generate diagnoses for an existing dataset
containing steps students take when solving quadratic equations (n=2136). To
compare with human diagnoses, two teachers coded a random sample of deviations
(n=70) and applications of the strategy (n=70). Results show that that the
system diagnosis aligned with the teacher coding in all of the 140 student
steps.

</details>


### [24] [DailyLLM: Context-Aware Activity Log Generation Using Multi-Modal Sensors and LLMs](https://arxiv.org/abs/2507.13737)
*Ye Tian,Xiaoyuan Ren,Zihao Wang,Onat Gungor,Xiaofan Yu,Tajana Rosing*

Main category: cs.AI

TL;DR: DailyLLM是一个基于轻量级LLM的框架，通过整合位置、运动、环境和生理四个维度的上下文信息，显著提升了活动日志生成的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有活动日志生成方法在准确性、效率和语义丰富性方面存在不足，DailyLLM旨在解决这些问题。

Method: DailyLLM采用轻量级LLM框架，结合结构化提示和高效特征提取，实现高级活动理解。

Result: 实验表明，DailyLLM在BERTScore精度上比70B参数的SOTA基线提升17%，推理速度快10倍，且可在个人电脑和Raspberry Pi上高效部署。

Conclusion: DailyLLM为活动日志生成提供了更高效、准确的解决方案，适用于资源有限的设备。

Abstract: Rich and context-aware activity logs facilitate user behavior analysis and
health monitoring, making them a key research focus in ubiquitous computing.
The remarkable semantic understanding and generation capabilities of Large
Language Models (LLMs) have recently created new opportunities for activity log
generation. However, existing methods continue to exhibit notable limitations
in terms of accuracy, efficiency, and semantic richness. To address these
challenges, we propose DailyLLM. To the best of our knowledge, this is the
first log generation and summarization system that comprehensively integrates
contextual activity information across four dimensions: location, motion,
environment, and physiology, using only sensors commonly available on
smartphones and smartwatches. To achieve this, DailyLLM introduces a
lightweight LLM-based framework that integrates structured prompting with
efficient feature extraction to enable high-level activity understanding.
Extensive experiments demonstrate that DailyLLM outperforms state-of-the-art
(SOTA) log generation methods and can be efficiently deployed on personal
computers and Raspberry Pi. Utilizing only a 1.5B-parameter LLM model, DailyLLM
achieves a 17% improvement in log generation BERTScore precision compared to
the 70B-parameter SOTA baseline, while delivering nearly 10x faster inference
speed.

</details>


### [25] [OntView: What you See is What you Meant](https://arxiv.org/abs/2507.13759)
*Carlos Bobed,Carlota Quintana,Eduardo Mena,Jorge Bobed,Fernando Bobillo*

Main category: cs.AI

TL;DR: OntView是一个直观的本体查看器，通过用户友好的界面展示本体概念及其形式化定义，支持GCI可视化并提供简化视图功能。


<details>
  <summary>Details</summary>
Motivation: 现有本体可视化工具无法有效展示本体结构，限制了用户对大型本体框架的理解。

Method: OntView基于DL推理器，采用“所见即所得”范式，支持GCI可视化，并提供三种简化视图方式。

Result: OntView成功实现了直观的本体可视化，并开源发布。

Conclusion: OntView解决了现有工具在可视化方面的不足，提升了用户对本体的理解能力。

Abstract: In the field of knowledge management and computer science, ontologies provide
a structured framework for modeling domain-specific knowledge by defining
concepts and their relationships. However, the lack of tools that provide
effective visualization is still a significant challenge. While numerous
ontology editors and viewers exist, most of them fail to graphically represent
ontology structures in a meaningful and non-overwhelming way, limiting users'
ability to comprehend dependencies and properties within large ontological
frameworks.
  In this paper, we present OntView, an ontology viewer that is designed to
provide users with an intuitive visual representation of ontology concepts and
their formal definitions through a user-friendly interface. Building on the use
of a DL reasoner, OntView follows a "What you see is what you meant" paradigm,
showing the actual inferred knowledge. One key aspect for this is its ability
to visualize General Concept Inclusions (GCI), a feature absent in existing
visualization tools. Moreover, to avoid a possible information overload,
OntView also offers different ways to show a simplified view of the ontology
by: 1) creating ontology summaries by assessing the importance of the concepts
(according to different available algorithms), 2) focusing the visualization on
the existing TBox elements between two given classes and 3) allowing to
hide/show different branches in a dynamic way without losing the semantics.
OntView has been released with an open-source license for the whole community.

</details>


### [26] [From Extraction to Synthesis: Entangled Heuristics for Agent-Augmented Strategic Reasoning](https://arxiv.org/abs/2507.13768)
*Renato Ghisellini,Remo Pareschi,Marco Pedroni,Giovanni Battista Raggi*

Main category: cs.AI

TL;DR: 提出了一种结合启发式提取、语义激活和组合合成的混合架构，用于增强代理的战略推理能力。


<details>
  <summary>Details</summary>
Motivation: 传统决策引擎通常选择最佳规则，而本文旨在通过语义交互建模和修辞框架，将冲突的启发式方法融合为连贯且上下文敏感的叙述。

Method: 结合经典军事理论和现代企业战略，通过语义相互依赖的过程激活和组合多个启发式方法。

Result: 通过Meta与FTC的案例研究展示了该框架，并通过语义指标进行了初步验证。

Conclusion: 讨论了局限性（如动态干扰调整）和未来扩展方向。

Abstract: We present a hybrid architecture for agent-augmented strategic reasoning,
combining heuristic extraction, semantic activation, and compositional
synthesis. Drawing on sources ranging from classical military theory to
contemporary corporate strategy, our model activates and composes multiple
heuristics through a process of semantic interdependence inspired by research
in quantum cognition. Unlike traditional decision engines that select the best
rule, our system fuses conflicting heuristics into coherent and
context-sensitive narratives, guided by semantic interaction modeling and
rhetorical framing. We demonstrate the framework via a Meta vs. FTC case study,
with preliminary validation through semantic metrics. Limitations and
extensions (e.g., dynamic interference tuning) are discussed.

</details>


### [27] [When Speed meets Accuracy: an Efficient and Effective Graph Model for Temporal Link Prediction](https://arxiv.org/abs/2507.13825)
*Haoyang Li,Yuming Xu,Yiming Li,Hanmo Liu,Darian Li,Chen Jason Zhang,Lei Chen,Qing Li*

Main category: cs.AI

TL;DR: EAGLE是一个轻量级框架，用于动态图中的时间链接预测，通过结合短期时间新近性和长期全局结构模式，显著提高了效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有T-GNNs在建模时间和结构依赖时存在计算开销高的问题，影响了可扩展性和效率。

Method: EAGLE包含时间感知模块和结构感知模块，采用自适应权重机制动态调整两者贡献，无需复杂多跳消息传递或内存密集型机制。

Result: 在七个真实世界时间图上，EAGLE在效果和效率上均优于现有T-GNNs，速度提升超过50倍。

Conclusion: EAGLE通过轻量级设计，有效解决了T-GNNs的效率和可扩展性问题，同时保持了高性能。

Abstract: Temporal link prediction in dynamic graphs is a critical task with
applications in diverse domains such as social networks, recommendation
systems, and e-commerce platforms. While existing Temporal Graph Neural
Networks (T-GNNs) have achieved notable success by leveraging complex
architectures to model temporal and structural dependencies, they often suffer
from scalability and efficiency challenges due to high computational overhead.
In this paper, we propose EAGLE, a lightweight framework that integrates
short-term temporal recency and long-term global structural patterns. EAGLE
consists of a time-aware module that aggregates information from a node's most
recent neighbors to reflect its immediate preferences, and a structure-aware
module that leverages temporal personalized PageRank to capture the influence
of globally important nodes. To balance these attributes, EAGLE employs an
adaptive weighting mechanism to dynamically adjust their contributions based on
data characteristics. Also, EAGLE eliminates the need for complex multi-hop
message passing or memory-intensive mechanisms, enabling significant
improvements in efficiency. Extensive experiments on seven real-world temporal
graphs demonstrate that EAGLE consistently achieves superior performance
against state-of-the-art T-GNNs in both effectiveness and efficiency,
delivering more than a 50x speedup over effective transformer-based T-GNNs.

</details>


### [28] [Causal Knowledge Transfer for Multi-Agent Reinforcement Learning in Dynamic Environments](https://arxiv.org/abs/2507.13846)
*Kathrin Korte,Christian Medeiros Adriano,Sona Ghahremani,Holger Giese*

Main category: cs.AI

TL;DR: 论文提出了一种因果知识转移框架，帮助多智能体强化学习（MARL）在非平稳环境中共享因果表示，无需重新训练即可适应新环境。


<details>
  <summary>Details</summary>
Motivation: 解决传统知识转移方法在非平稳环境中泛化能力不足的问题，减少智能体适应新环境时的重新训练成本。

Method: 通过建模碰撞为因果干预，生成恢复动作宏（macro），并在智能体之间在线转移这些宏，实现零样本适应。

Result: 实验表明，该方法能使智能体在新环境中填补随机探索与完全重新训练策略之间约一半的差距，且效果受环境复杂性和智能体目标异质性的影响。

Conclusion: 因果知识转移框架为MARL在非平稳环境中的适应性提供了有效解决方案，尤其在环境变化和智能体目标多样时表现突出。

Abstract: [Context] Multi-agent reinforcement learning (MARL) has achieved notable
success in environments where agents must learn coordinated behaviors. However,
transferring knowledge across agents remains challenging in non-stationary
environments with changing goals. [Problem] Traditional knowledge transfer
methods in MARL struggle to generalize, and agents often require costly
retraining to adapt. [Approach] This paper introduces a causal knowledge
transfer framework that enables RL agents to learn and share compact causal
representations of paths within a non-stationary environment. As the
environment changes (new obstacles), agents' collisions require adaptive
recovery strategies. We model each collision as a causal intervention
instantiated as a sequence of recovery actions (a macro) whose effect
corresponds to a causal knowledge of how to circumvent the obstacle while
increasing the chances of achieving the agent's goal (maximizing cumulative
reward). This recovery action macro is transferred online from a second agent
and is applied in a zero-shot fashion, i.e., without retraining, just by
querying a lookup model with local context information (collisions). [Results]
Our findings reveal two key insights: (1) agents with heterogeneous goals were
able to bridge about half of the gap between random exploration and a fully
retrained policy when adapting to new environments, and (2) the impact of
causal knowledge transfer depends on the interplay between environment
complexity and agents' heterogeneous goals.

</details>


### [29] [Large Language Models as Innovators: A Framework to Leverage Latent Space Exploration for Novelty Discovery](https://arxiv.org/abs/2507.13874)
*Mateusz Bystroński,Mikołaj Hołysz,Grzegorz Piotrowski,Nitesh V. Chawla,Tomasz Kajdanowicz*

Main category: cs.AI

TL;DR: 提出了一种模型无关的潜在空间创意框架，通过导航连续嵌入空间实现可控、可扩展的创造力。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型（LLMs）在生成新颖且相关内容时的局限性，避免依赖领域特定启发式或结构化提示。

Method: 提出了一种模型无关的潜在空间创意框架，无需手工规则，适应不同领域和任务。

Result: 初步结果显示该框架具有作为通用人机协作创意工具的潜力。

Conclusion: 该框架为可控创意生成提供了新方向，有望成为人机协作的有力工具。

Abstract: Innovative idea generation remains a core challenge in AI, as large language
models (LLMs) often struggle to produce outputs that are both novel and
relevant. Despite their fluency, LLMs tend to replicate patterns seen during
training, limiting their ability to diverge creatively without extensive prompt
engineering. Prior work has addressed this through domain-specific heuristics
and structured prompting pipelines, but such solutions are brittle and
difficult to generalize. In this paper, we propose a model-agnostic
latent-space ideation framework that enables controlled, scalable creativity by
navigating the continuous embedding space of ideas. Unlike prior methods, our
framework requires no handcrafted rules and adapts easily to different domains,
input formats, and creative tasks. This paper introduces an early-stage
prototype of our method, outlining the conceptual framework and preliminary
results highlighting its potential as a general-purpose co-ideator for human-AI
collaboration.

</details>


### [30] [Cross-modal Causal Intervention for Alzheimer's Disease Prediction](https://arxiv.org/abs/2507.13956)
*Yutao Jin,Haowen Xiao,Jielei Chu,Fengmao Lv,Yuxiao Li,Tianrui Li*

Main category: cs.AI

TL;DR: 提出了一种名为ADPC的新型视觉-语言因果干预框架，用于辅助诊断阿尔茨海默病（AD），通过消除混杂因素提升分类性能。


<details>
  <summary>Details</summary>
Motivation: 早期识别和干预轻度认知障碍（MCI）可延缓AD发展，但诊断中存在数据选择偏差和变量复杂关系的挑战。

Method: 结合MRI、fMRI图像和LLM生成的文本数据，通过因果干预消除混杂因素，分类CN/MCI/AD。

Result: 实验表明ADPC在区分CN/MCI/AD上表现优异，达到SOTA指标。

Conclusion: 展示了因果推理与多模态学习结合在神经疾病诊断中的潜力。

Abstract: Mild Cognitive Impairment (MCI) serves as a prodromal stage of Alzheimer's
Disease (AD), where early identification and intervention can effectively slow
the progression to dementia. However, diagnosing AD remains a significant
challenge in neurology due to the confounders caused mainly by the selection
bias of multimodal data and the complex relationships between variables. To
address these issues, we propose a novel visual-language causal intervention
framework named Alzheimer's Disease Prediction with Cross-modal Causal
Intervention (ADPC) for diagnostic assistance. Our ADPC employs large language
model (LLM) to summarize clinical data under strict templates, maintaining
structured text outputs even with incomplete or unevenly distributed datasets.
The ADPC model utilizes Magnetic Resonance Imaging (MRI), functional MRI (fMRI)
images and textual data generated by LLM to classify participants into
Cognitively Normal (CN), MCI, and AD categories. Because of the presence of
confounders, such as neuroimaging artifacts and age-related biomarkers,
non-causal models are likely to capture spurious input-output correlations,
generating less reliable results. Our framework implicitly eliminates
confounders through causal intervention. Experimental results demonstrate the
outstanding performance of our method in distinguishing CN/MCI/AD cases,
achieving state-of-the-art (SOTA) metrics across most evaluation metrics. The
study showcases the potential of integrating causal reasoning with multi-modal
learning for neurological disease diagnosis.

</details>


### [31] [Towards Constraint Temporal Answer Set Programming](https://arxiv.org/abs/2507.13958)
*Pedro Cabalar,Martín Diéguez,François Olivier,Torsten Schaub,Igor Stéphan*

Main category: cs.AI

TL;DR: 提出了一种新的基于时间和约束的逻辑扩展方法，用于解决动态系统中的高分辨率时序和数值推理问题。


<details>
  <summary>Details</summary>
Motivation: 解决逻辑方法（如ASP）在动态系统中高分辨率时序和数值推理的挑战。

Method: 结合线性时间逻辑和约束逻辑，扩展了Here-and-There逻辑及其非单调平衡扩展。

Result: 实现了首个专门为ASP设计的非单调时序约束推理系统。

Conclusion: 为ASP范式下处理复杂动态系统提供了基础逻辑框架。

Abstract: Reasoning about dynamic systems with a fine-grained temporal and numeric
resolution presents significant challenges for logic-based approaches like
Answer Set Programming (ASP). To address this, we introduce and elaborate upon
a novel temporal and constraint-based extension of the logic of Here-and-There
and its nonmonotonic equilibrium extension, representing, to the best of our
knowledge, the first approach to nonmonotonic temporal reasoning with
constraints specifically tailored for ASP. This expressive system is achieved
by a synergistic combination of two foundational ASP extensions: the
linear-time logic of Here-and-There, providing robust nonmonotonic temporal
reasoning capabilities, and the logic of Here-and-There with constraints,
enabling the direct integration and manipulation of numeric constraints, among
others. This work establishes the foundational logical framework for tackling
complex dynamic systems with high resolution within the ASP paradigm.

</details>


### [32] [KROMA: Ontology Matching with Knowledge Retrieval and Large Language Models](https://arxiv.org/abs/2507.14032)
*Lam Nguyen,Erika Barcelos,Roger French,Yinghui Wu*

Main category: cs.AI

TL;DR: KROMA是一个基于大型语言模型（LLMs）和检索增强生成（RAG）的新型本体匹配框架，通过动态丰富语义上下文和优化性能，显著提升了本体匹配的效果。


<details>
  <summary>Details</summary>
Motivation: 现有本体匹配系统依赖手工规则或专用模型，适应性有限，KROMA旨在通过LLMs和RAG解决这一问题。

Method: KROMA结合了双相似度概念匹配和轻量级本体细化步骤，以减少LLMs的通信开销，并通过检索增强生成动态丰富语义上下文。

Result: 实验表明，KROMA在多个基准数据集上优于传统系统和前沿LLM方法，同时保持低通信开销。

Conclusion: KROMA证明了知识检索、提示增强和本体细化等优化技术在大规模本体匹配中的可行性和优势。

Abstract: Ontology Matching (OM) is a cornerstone task of semantic interoperability,
yet existing systems often rely on handcrafted rules or specialized models with
limited adaptability. We present KROMA, a novel OM framework that harnesses
Large Language Models (LLMs) within a Retrieval-Augmented Generation (RAG)
pipeline to dynamically enrich the semantic context of OM tasks with
structural, lexical, and definitional knowledge. To optimize both performance
and efficiency, KROMA integrates a bisimilarity-based concept matching and a
lightweight ontology refinement step, which prune candidate concepts and
substantially reduce the communication overhead from invoking LLMs. Through
experiments on multiple benchmark datasets, we show that integrating knowledge
retrieval with context-augmented LLMs significantly enhances ontology matching,
outperforming both classic OM systems and cutting-edge LLM-based approaches
while keeping communication overhead comparable. Our study highlights the
feasibility and benefit of the proposed optimization techniques (targeted
knowledge retrieval, prompt enrichment, and ontology refinement) for ontology
matching at scale.

</details>


### [33] [Glucose-ML: A collection of longitudinal diabetes datasets for development of robust AI solutions](https://arxiv.org/abs/2507.14077)
*Temiloluwa Prioleau,Baiying Lu,Yanjun Cui*

Main category: cs.AI

TL;DR: Glucose-ML是一个包含10个公开糖尿病数据集的集合，旨在加速透明、可重复和稳健的AI解决方案开发。


<details>
  <summary>Details</summary>
Motivation: 解决高质量糖尿病数据集获取困难的问题，推动AI在糖尿病管理中的应用。

Method: 收集并整合了10个公开数据集，包含超过30万天的连续血糖监测数据，并进行比较分析和案例研究。

Result: 研究发现同一算法在不同数据集上的预测结果差异显著，为AI开发提供了基准和建议。

Conclusion: Glucose-ML为糖尿病领域的AI研究提供了丰富资源和实用指导，推动了稳健AI解决方案的发展。

Abstract: Artificial intelligence (AI) algorithms are a critical part of
state-of-the-art digital health technology for diabetes management. Yet, access
to large high-quality datasets is creating barriers that impede development of
robust AI solutions. To accelerate development of transparent, reproducible,
and robust AI solutions, we present Glucose-ML, a collection of 10 publicly
available diabetes datasets, released within the last 7 years (i.e., 2018 -
2025). The Glucose-ML collection comprises over 300,000 days of continuous
glucose monitor (CGM) data with a total of 38 million glucose samples collected
from 2500+ people across 4 countries. Participants include persons living with
type 1 diabetes, type 2 diabetes, prediabetes, and no diabetes. To support
researchers and innovators with using this rich collection of diabetes
datasets, we present a comparative analysis to guide algorithm developers with
data selection. Additionally, we conduct a case study for the task of blood
glucose prediction - one of the most common AI tasks within the field. Through
this case study, we provide a benchmark for short-term blood glucose prediction
across all 10 publicly available diabetes datasets within the Glucose-ML
collection. We show that the same algorithm can have significantly different
prediction results when developed/evaluated with different datasets. Findings
from this study are then used to inform recommendations for developing robust
AI solutions within the diabetes or broader health domain. We provide direct
links to each longitudinal diabetes dataset in the Glucose-ML collection and
openly provide our code.

</details>


### [34] [Generative AI-Driven High-Fidelity Human Motion Simulation](https://arxiv.org/abs/2507.14097)
*Hari Iyer,Neel Macwan,Atharva Jitendra Hude,Heejin Jeong,Shenghan Guo*

Main category: cs.AI

TL;DR: G-AI-HMS结合生成式AI提升工业任务中的人体运动模拟质量，通过文本到运动和计算机视觉验证，显著减少误差。


<details>
  <summary>Details</summary>
Motivation: 现有人体运动模拟方法运动保真度低，需提高模拟质量以更准确评估工人行为、安全和生产力。

Method: 集成文本到文本和文本到运动模型，利用大语言模型和MotionGPT生成运动感知语言，并通过计算机视觉验证运动真实性。

Result: 在八项任务中，AI增强的运动在空间准确性、姿态对齐和时间相似性上优于人工描述，显著减少误差。

Conclusion: G-AI-HMS通过生成式AI显著提升运动模拟质量，为工业任务提供更可靠的评估工具。

Abstract: Human motion simulation (HMS) supports cost-effective evaluation of worker
behavior, safety, and productivity in industrial tasks. However, existing
methods often suffer from low motion fidelity. This study introduces
Generative-AI-Enabled HMS (G-AI-HMS), which integrates text-to-text and
text-to-motion models to enhance simulation quality for physical tasks.
G-AI-HMS tackles two key challenges: (1) translating task descriptions into
motion-aware language using Large Language Models aligned with MotionGPT's
training vocabulary, and (2) validating AI-enhanced motions against real human
movements using computer vision. Posture estimation algorithms are applied to
real-time videos to extract joint landmarks, and motion similarity metrics are
used to compare them with AI-enhanced sequences. In a case study involving
eight tasks, the AI-enhanced motions showed lower error than human created
descriptions in most scenarios, performing better in six tasks based on spatial
accuracy, four tasks based on alignment after pose normalization, and seven
tasks based on overall temporal similarity. Statistical analysis showed that
AI-enhanced prompts significantly (p $<$ 0.0001) reduced joint error and
temporal misalignment while retaining comparable posture accuracy.

</details>


### [35] [Automated Interpretation of Non-Destructive Evaluation Contour Maps Using Large Language Models for Bridge Condition Assessment](https://arxiv.org/abs/2507.14107)
*Viraj Nishesh Darji,Callie C. Liao,Duoduo Liao*

Main category: cs.AI

TL;DR: 该研究探讨了大型语言模型（LLMs）在解释无损评估（NDE）轮廓图中的应用，展示了LLMs如何提升桥梁维护的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 桥梁维护和安全至关重要，但NDE数据分析耗时且需要专业知识，LLMs的进步为自动化分析提供了新途径。

Method: 研究设计了特定提示词，评估了多个LLMs在解释NDE轮廓图时的表现，包括生成描述、识别缺陷和提供建议。

Result: 九个模型中有四个表现更优，尤其是ChatGPT-4和Claude 3.5 Sonnet，能生成更有效的总结。

Conclusion: LLMs在桥梁维护中具有潜力，能显著提升效率和准确性，为基础设施管理提供创新支持。

Abstract: Bridge maintenance and safety are essential for transportation authorities,
and Non-Destructive Evaluation (NDE) techniques are critical to assessing
structural integrity. However, interpreting NDE data can be time-consuming and
requires expertise, potentially delaying decision-making. Recent advancements
in Large Language Models (LLMs) offer new ways to automate and improve this
analysis. This pilot study introduces a holistic assessment of LLM capabilities
for interpreting NDE contour maps and demonstrates the effectiveness of LLMs in
providing detailed bridge condition analyses. It establishes a framework for
integrating LLMs into bridge inspection workflows, indicating that LLM-assisted
analysis can enhance efficiency without compromising accuracy. In this study,
several LLMs are explored with prompts specifically designed to enhance the
quality of image descriptions, which are applied to interpret five different
NDE contour maps obtained through technologies for assessing bridge conditions.
Each LLM model is evaluated based on its ability to produce detailed
descriptions, identify defects, provide actionable recommendations, and
demonstrate overall accuracy. The research indicates that four of the nine
models provide better image descriptions, effectively covering a wide range of
topics related to the bridge's condition. The outputs from these four models
are summarized using five different LLMs to form a comprehensive overview of
the bridge. Notably, LLMs ChatGPT-4 and Claude 3.5 Sonnet generate more
effective summaries. The findings suggest that LLMs have the potential to
significantly improve efficiency and accuracy. This pilot study presents an
innovative approach that leverages LLMs for image captioning in parallel and
summarization, enabling faster decision-making in bridge maintenance and
enhancing infrastructure management and safety assessments.

</details>


### [36] [CUDA-L1: Improving CUDA Optimization via Contrastive Reinforcement Learning](https://arxiv.org/abs/2507.14111)
*Xiaoya Li,Xiaofei Sun,Albert Wang,Jiwei Li,Chris Shum*

Main category: cs.AI

TL;DR: CUDA-L1是一种基于强化学习的自动化CUDA优化框架，显著提升了CUDA内核的性能，并展示了跨GPU架构的优异可移植性。


<details>
  <summary>Details</summary>
Motivation: 由于大型语言模型的快速发展，对GPU计算资源的需求激增，亟需自动化CUDA优化策略。现有模型在CUDA优化上成功率低，因此开发了CUDA-L1。

Method: CUDA-L1采用强化学习框架，通过速度提升奖励信号训练模型，无需人工干预或领域知识。

Result: 在NVIDIA A100上训练后，CUDA-L1在KernelBench的250个CUDA内核上平均提速17.7倍，峰值达449倍，且在其他GPU架构上也表现优异。

Conclusion: CUDA-L1展示了强化学习在自动化CUDA优化中的潜力，有望显著提升GPU效率并缓解计算资源压力。

Abstract: The exponential growth in demand for GPU computing resources, driven by the
rapid advancement of Large Language Models, has created an urgent need for
automated CUDA optimization strategies. While recent advances in LLMs show
promise for code generation, current SOTA models (e.g. R1, o1) achieve low
success rates in improving CUDA speed. In this paper, we introduce CUDA-L1, an
automated reinforcement learning framework for CUDA optimization.
  CUDA-L1 achieves performance improvements on the CUDA optimization task:
trained on NVIDIA A100, it delivers an average speedup of x17.7 across all 250
CUDA kernels of KernelBench, with peak speedups reaching x449. Furthermore, the
model also demonstrates excellent portability across GPU architectures,
achieving average speedups of x17.8 on H100, x19.0 on RTX 3090, x16.5 on L40,
x14.7 on H800, and x13.9 on H20 despite being optimized specifically for A100.
Beyond these benchmark results, CUDA-L1 demonstrates several remarkable
properties: 1) Discovers a variety of CUDA optimization techniques and learns
to combine them strategically to achieve optimal performance; 2) Uncovers
fundamental principles of CUDA optimization; 3) Identifies non-obvious
performance bottlenecks and rejects seemingly beneficial optimizations that
harm performance.
  The capabilities of CUDA-L1 demonstrate that reinforcement learning can
transform an initially poor-performing LLM into an effective CUDA optimizer
through speedup-based reward signals alone, without human expertise or domain
knowledge. More importantly, the trained RL model extend the acquired reasoning
abilities to new kernels. This paradigm opens possibilities for automated
optimization of CUDA operations, and holds promise to substantially promote GPU
efficiency and alleviate the rising pressure on GPU computing resources.

</details>
