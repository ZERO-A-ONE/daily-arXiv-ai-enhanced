<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 13]
- [cs.CR](#cs.CR) [Total: 20]
- [cs.AI](#cs.AI) [Total: 25]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Pattern-Based File and Data Access with Python Glob: A Comprehensive Guide for Computational Research](https://arxiv.org/abs/2509.08843)
*Sidney Shapiro*

Main category: cs.SE

TL;DR: 本文介绍了Python glob模块作为数据科学、商业分析和AI应用中文件模式匹配的基础工具，通过具体案例展示了其在数据摄取、分析和可重复研究中的实用价值。


<details>
  <summary>Details</summary>
Motivation: 文件模式访问是计算研究中的基础但常被忽视的方面，需要为研究者和实践者提供一个简洁的参考，将基础概念与应用实践相连接。

Method: 通过具体的Python示例和使用pandas、scikit-learn、matplotlib等流行库，展示glob如何促进高效的文件遍历和分析管道集成。

Result: 展示了glob在大规模数据摄取、组织数据分析、AI数据集构建和可重复研究实践中的多种用例，证明了其作为方法构建块的作用。

Conclusion: glob模块应成为Python研究工作流中文件模式匹配的默认引用工具，为跨学科的可扩展工作流程提供简单而强大的支持。

Abstract: Pattern-based file access is a fundamental but often under-documented aspect
of computational research. The Python glob module provides a simple yet
powerful way to search, filter, and ingest files using wildcard patterns,
enabling scalable workflows across disciplines. This paper introduces glob as a
versatile tool for data science, business analytics, and artificial
intelligence applications. We demonstrate use cases including large-scale data
ingestion, organizational data analysis, AI dataset construction, and
reproducible research practices. Through concrete Python examples with widely
used libraries such as pandas,scikit-learn, and matplotlib, we show how glob
facilitates efficient file traversal and integration with analytical pipelines.
By situating glob within the broader context of reproducible research and data
engineering, we highlight its role as a methodological building block. Our goal
is to provide researchers and practitioners with a concise reference that
bridges foundational concepts and applied practice, making glob a default
citation for file pattern matching in Python-based research workflows.

</details>


### [2] [A Systematic Mapping Study on Chatbots in Programming Education](https://arxiv.org/abs/2509.08857)
*Marcelino Garcia,Renato Garcia,Arthur Parizotto,Andre Mendes,Pedro Valle,Ricardo Vilela,Renato Balancieri,Williamson Silva*

Main category: cs.SE

TL;DR: 这是一份关于教育聊天机器在编程教育中应用的系统性地图研究，分析了54项研究的趋势和空白


<details>
  <summary>Details</summary>
Motivation: 调查教育聊天机器在编程教育中的开发和应用情况，以支持初学者的编程学习

Method: 进行系统性地图研究(SMS)，从3,216篇公开出版物中选取54项研究，基于5个研究子问题进行分析

Result: 发现以Python教学为主的聊天机器占主导地位，重点关注基础编程概念，采用多种教学方法和技术架构

Conclusion: 识别了文献中的趋势和空白，为开发新的编程教育工具提供了见解

Abstract: Educational chatbots have gained prominence as support tools for teaching
programming, particularly in introductory learning contexts. This paper
presents a Systematic Mapping Study (SMS) that investigated how such agents
have been developed and applied in programming education. From an initial set
of 3,216 publications, 54 studies were selected and analyzed based on five
research subquestions, addressing chatbot types, programming languages used,
educational content covered, interaction models, and application contexts. The
results reveal a predominance of chatbots designed for Python instruction,
focusing on fundamental programming concepts, and employing a wide variety of
pedagogical approaches and technological architectures. In addition to
identifying trends and gaps in the literature, this study provides insights to
inform the development of new educational tools for programming instruction.

</details>


### [3] [GeoJSON Agents:A Multi-Agent LLM Architecture for Geospatial Analysis-Function Calling vs Code Generation](https://arxiv.org/abs/2509.08863)
*Qianqian Luo,Liuchang Xu,Qingming Lin,Sensen Wu,Ruichen Mao,Chao Wang,Hailin Feng,Bo Huang,Zhenhong Du*

Main category: cs.SE

TL;DR: 这篇论文提出了GeoJSON Agents多段代理架构，通过函数调用和代码生成两种方法处理空间数据，显著提升了LLM在GIS自动化任务中的性能


<details>
  <summary>Details</summary>
Motivation: 解决LLM缺乏GIS专业知识在空间数据处理任务中的限制，提高GIS自动化的性能和可扩展性

Method: 设计了三部件架构：任务解析、代理协作和结果集成。Planner代理将自然语言任务转换为GeoJSON命令，Worker代理通过函数调用或代码生成执行空间分析

Result: 在70个任务的测试中，函数调用方法达到85.71%准确率，代码生成方法达到97.14%准确率，都显著超过通用模型(48.57%)

Conclusion: 该研究首次引入了用于GeoJSON数据的LLM多代理框架，并对比了两种主流LLM增强方法的优势，为提升GeoAI系统性能提供了新视角

Abstract: LLMs have made substantial progress in task automation and natural language
understanding.However,without expertise in GIS,they continue to encounter
limitations.To address these issues, we propose GeoJSON Agents-a multi-agent
LLM architecture.This framework transforms natural language tasks into
structured GeoJSON operation commands and processes spatial data using two
widely adopted LLM enhancement techniques:Function Calling and Code
Generation.The architecture consists of three components-task parsing,agent
collaboration,and result integration-aimed at enhancing both the performance
and scalability of GIS automation.The Planner agent interprets natural language
tasks into structured GeoJSON commands.Then,specialized Worker agents
collaborate according to assigned roles to perform spatial data processing and
analysis,either by invoking predefined function APIs or by dynamically
generating and executing Python-based spatial analysis code.Finally,the system
integrates the outputs from multiple execution rounds into
reusable,standards-compliant GeoJSON files.To systematically evaluate the
performance of the two approaches,we constructed a benchmark dataset of 70
tasks with varying complexity and conducted experiments using OpenAI's GPT-4o
as the core model.Results indicate that the Function Calling-based GeoJSON
Agent achieved an accuracy of 85.71%,while the Code Generation-based agent
reached 97.14%,both significantly outperforming the best-performing
general-purpose model (48.57%).Further analysis reveals that the Code
Generation provides greater flexibility,whereas the Function Calling approach
offers more stable execution.This study is the first to introduce an LLM
multi-agent framework for GeoJSON data and to compare the strengths and
limitations of two mainstream LLM enhancement methods,offering new perspectives
for improving GeoAI system performance.

</details>


### [4] [TraceRAG: A LLM-Based Framework for Explainable Android Malware Detection and Behavior Analysis](https://arxiv.org/abs/2509.08865)
*Guangyu Zhang,Xixuan Wang,Shiyu Sun,Peiyan Xiao,Kun Sun,Yanhai Xiong*

Main category: cs.SE

TL;DR: TraceRAG是一个基于检索增强生成(RAG)的Android恶意软件分析框架，通过自然语言查询和Java代码分析，提供可解释的恶意行为检测和报告生成。


<details>
  <summary>Details</summary>
Motivation: 传统分析方法难以检测深度隐藏的恶意行为且缺乏可解释性，而大语言模型的发展为构建可解释的恶意软件分析框架提供了新机遇。

Method: 首先生成方法级代码片段的摘要并建立向量索引，然后通过行为相关查询检索语义相关片段进行深度分析，最后基于多轮分析结果生成包含恶意行为和对应代码实现的可读报告。

Result: 实验结果显示该方法达到96%的恶意软件检测准确率和83.81%的行为识别准确率，基于更新的VirusTotal扫描和人工验证。专家评估确认了生成报告的实际效用。

Conclusion: TraceRAG框架成功地将自然语言处理技术应用于Android恶意软件分析，提供了高准确性和可解释性的检测解决方案，具有重要的实际应用价值。

Abstract: Sophisticated evasion tactics in malicious Android applications, combined
with their intricate behavioral semantics, enable attackers to conceal
malicious logic within legitimate functions, underscoring the critical need for
robust and in-depth analysis frameworks. However, traditional analysis
techniques often fail to recover deeply hidden behaviors or provide
human-readable justifications for their decisions. Inspired by advances in
large language models (LLMs), we introduce TraceRAG, a retrieval-augmented
generation (RAG) framework that bridges natural language queries and Java code
to deliver explainable malware detection and analysis. First, TraceRAG
generates summaries of method-level code snippets, which are indexed in a
vector database. At query time, behavior-focused questions retrieve the most
semantically relevant snippets for deeper inspection. Finally, based on the
multi-turn analysis results, TraceRAG produces human-readable reports that
present the identified malicious behaviors and their corresponding code
implementations. Experimental results demonstrate that our method achieves 96\%
malware detection accuracy and 83.81\% behavior identification accuracy based
on updated VirusTotal (VT) scans and manual verification. Furthermore, expert
evaluation confirms the practical utility of the reports generated by TraceRAG.

</details>


### [5] [Benchmarking Energy Efficiency of Large Language Models Using vLLM](https://arxiv.org/abs/2509.08867)
*K. Pronk,Q. Zhao*

Main category: cs.SE

TL;DR: 作者开发了一个仿真实世界使用场景的LLM能源效率测试基准，使用vLLM服务器评估模型大小、架构和并发请求对能源效率的影响。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的普及，其部署和使用需要大量能源，对气候产生负面影响。现有的能源效率评测基准往往无法反映真实生产环境的情况。

Method: 使用vLLM高吞吐量服务器构建仿真实世界使用场景的测试基准，分析模型大小、架构设计和并发请求数量对推理能源效率的影响。

Result: 证明了可以创建更好反映实际部署条件的能源效率测试基准，为开发者提供了有价值的可持续性见解。

Conclusion: 该研究为开发更可持续的人工智能系统提供了重要的测试工具和见解，有助于降低LLM部署的环境影响。

Abstract: The prevalence of Large Language Models (LLMs) is having an growing impact on
the climate due to the substantial energy required for their deployment and
use. To create awareness for developers who are implementing LLMs in their
products, there is a strong need to collect more information about the energy
efficiency of LLMs. While existing research has evaluated the energy efficiency
of various models, these benchmarks often fall short of representing realistic
production scenarios. In this paper, we introduce the LLM Efficiency Benchmark,
designed to simulate real-world usage conditions. Our benchmark utilizes vLLM,
a high-throughput, production-ready LLM serving backend that optimizes model
performance and efficiency. We examine how factors such as model size,
architecture, and concurrent request volume affect inference energy efficiency.
Our findings demonstrate that it is possible to create energy efficiency
benchmarks that better reflect practical deployment conditions, providing
valuable insights for developers aiming to build more sustainable AI systems.

</details>


### [6] [CLARA: A Developer's Companion for Code Comprehension and Analysis](https://arxiv.org/abs/2509.09072)
*Ahmed Adnan,Mushfiqur Rahman,Saad Sakib Noor,Kazi Sakib*

Main category: cs.SE

TL;DR: CLARA是一个基于先进推理模型的浏览器扩展工具，帮助开发者和研究人员进行代码理解、重构和质量检测，通过用户研究证明其有用性和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有代码分析工具需要项目设置、缺乏上下文感知且需要大量手动操作，CLARA旨在解决这些问题。

Method: 开发浏览器扩展工具，使用最先进的推理模型，通过定性评估和10名用户的综合研究来验证工具。

Result: 研究表明CLARA在代码理解和分析任务中具有实用性、准确性和实用性。

Conclusion: CLARA是一个开源工具，能够有效辅助代码理解和分析工作，解决了现有工具的局限性。

Abstract: Code comprehension and analysis of open-source project codebases is a task
frequently performed by developers and researchers. However, existing tools
that practitioners use for assistance with such tasks often require prior
project setup, lack context-awareness, and involve significant manual effort.
To address this, we present CLARA, a browser extension that utilizes a
state-of-the-art inference model to assist developers and researchers in: (i)
comprehending code files and code fragments, (ii) code refactoring, and (iii)
code quality attribute detection. We qualitatively evaluated CLARA's inference
model using existing datasets and methodology, and performed a comprehensive
user study with 10 developers and academic researchers to assess its usability
and usefulness. The results show that CLARA is useful, accurate, and practical
in code comprehension and analysis tasks. CLARA is an open-source tool
available at https://github.com/SaadNoor555/CLARA_tool_demo. A video showing
the full capabilities of CLARA can be found at
https://youtu.be/VDKVXvIH41Q?si=qBFsmS_Y4m_9x3YH.

</details>


### [7] [Probing Pre-trained Language Models on Code Changes: Insights from ReDef, a High-Confidence Just-in-Time Defect Prediction Dataset](https://arxiv.org/abs/2509.09192)
*Doha Nam,Taehyoun Kim,Duksan Ryu,Jongmoon Baik*

Main category: cs.SE

TL;DR: 这篇论文提出了ReDef数据集，通过revert commits和GPT辅助筛选构建了高信度的软件缺陷预测数据集，并系统评估了预训练语言模型在代码修改理解方面的能力和局限性。


<details>
  <summary>Details</summary>
Motivation: 解决现有JIT-SDP数据集标签噪声大、精确度低的问题，构建高信度的软件缺陷预测数据集，并研究预训练语言模型对代码修改的理解能力。

Method: 使用revert commits定义缺陷修改，通过历史检查验证清洁修改，采用GPT辅助的多票审核过程筛选模糊实例，构建了3,164个缺陷修改和10,268个清洁修改。对CodeBERT、CodeT5+、UniXcoder进行微调，使用5种编码策略，并通过反事实扰动探测模型敏感性。

Result: diff式编码在所有PLM中都比整体函数格式表现更好，统计测试证明了模型独立的大效应。但反事实测试显示性能减退很少，表明模型依赖表面线索而非真正语义理解。

Conclusion: 当前的预训练语言模型在代码修改理解方面仍有限，无法真正理解代码修改的语义，而是依赖表面特征进行预测。

Abstract: Just-in-Time software defect prediction (JIT-SDP) plays a critical role in
prioritizing risky code changes during code review and continuous integration.
However, existing datasets often suffer from noisy labels and low precision in
identifying bug-inducing commits. To address this, we present ReDef
(Revert-based Defect dataset), a high-confidence benchmark of function-level
modifications curated from 22 large-scale C/C++ projects. Defective cases are
anchored by revert commits, while clean cases are validated through post-hoc
history checks. Ambiguous instances are conservatively filtered out via a
GPT-assisted triage process involving multiple votes and audits. This pipeline
yields 3,164 defective and 10,268 clean modifications, offering substantially
more reliable labels than prior existing resources. Beyond dataset
construction, we provide the first systematic evaluation of how pre-trained
language models (PLMs) reason about code modifications -- specifically, which
input encodings most effectively expose change information, and whether models
genuinely capture edit semantics. We fine-tune CodeBERT, CodeT5+, and UniXcoder
under five encoding strategies, and further probe their sensitivity through
counterfactual perturbations that swap added/deleted blocks, invert diff
polarity, or inject spurious markers. Our results show that compact diff-style
encodings consistently outperform whole-function formats across all PLMs, with
statistical tests confirming large, model-independent effects. However, under
counterfactual tests, performance degrades little or not at all -- revealing
that what appears to be robustness in fact reflects reliance on superficial
cues rather than true semantic understanding. These findings indicate that,
unlike in snapshot-based tasks, current PLMs remain limited in their ability to
genuinely comprehend code modifications.

</details>


### [8] [On Integrating Large Language Models and Scenario-Based Programming for Improving Software Reliability](https://arxiv.org/abs/2509.09194)
*Ayelet Berzack,Guy Katz*

Main category: cs.SE

TL;DR: 提出了一种将大语言模型与传统软件工程技术（特别是基于场景的编程范式）相结合的方法，以提高软件开发可靠性并减少错误。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型能显著减少开发时间并生成组织良好的代码，但它们经常引入严重错误并以说服性自信呈现错误代码，可能误导开发者接受有缺陷的解决方案。

Method: 采用基于场景的编程（SBP）范式，这是一种事件驱动、基于场景的软件工程方法，允许人类开发者将专业知识输入LLM，并检查和验证其输出。

Result: 通过案例研究设计并实现了Connect4游戏，结合LLM和SBP创建了能够击败各种强大现有代理的高能力代理，在某些情况下还能正式验证代理的正确性。

Conclusion: 该方法展示了将LLM与传统软件工程技术结构化结合的可行性，能够提高开发流程效率、减少错误，并为用户提供更高的程序属性验证信心。

Abstract: Large Language Models (LLMs) are fast becoming indispensable tools for
software developers, assisting or even partnering with them in crafting complex
programs. The advantages are evident -- LLMs can significantly reduce
development time, generate well-organized and comprehensible code, and
occasionally suggest innovative ideas that developers might not conceive on
their own. However, despite their strengths, LLMs will often introduce
significant errors and present incorrect code with persuasive confidence,
potentially misleading developers into accepting flawed solutions.
  In order to bring LLMs into the software development cycle in a more reliable
manner, we propose a methodology for combining them with ``traditional''
software engineering techniques in a structured way, with the goal of
streamlining the development process, reducing errors, and enabling users to
verify crucial program properties with increased confidence. Specifically, we
focus on the Scenario-Based Programming (SBP) paradigm -- an event-driven,
scenario-based approach for software engineering -- to allow human developers
to pour their expert knowledge into the LLM, as well as to inspect and verify
its outputs.
  To evaluate our methodology, we conducted a significant case study, and used
it to design and implement the Connect4 game. By combining LLMs and SBP we were
able to create a highly-capable agent, which could defeat various strong
existing agents. Further, in some cases, we were able to formally verify the
correctness of our agent. Finally, our experience reveals interesting insights
regarding the ease-of-use of our proposed approach. The full code of our
case-study will be made publicly available with the final version of this
paper.

</details>


### [9] [Altered Histories in Version Control System Repositories: Evidence from the Trenches](https://arxiv.org/abs/2509.09294)
*Solal Rapaport,Laurent Pautet,Samuel Tardieu,Stefano Zacchiroli*

Main category: cs.SE

TL;DR: 首次大规模研究Git公开仓库历史修改，发现1.22M仓库存在8.7M个重写历史，包括许可证和密钥删除等问题，并提出自动化工具GitHistorian进行检测。


<details>
  <summary>Details</summary>
Motivation: 调查Git公共分支中的历史修改行为，这些修改导致push/pull流程失效、仓库完整性受损，为供应链攻击提供可乘之机。

Method: 分析Software Heritage归档的111M个仓库，识别和分类历史修改行为，进行两个具体案例研究（许可证更改和密钥删除）。

Result: 发现1.22M仓库存在8.7M个重写历史，常见问题包括违规更改许可证和漏洞删除密钥等。

Conclusion: 历史修改带来严重风险，需要开发者关注。GitHistorian工具能够自动检测和描述公开仓库中的历史修改行为。

Abstract: Version Control Systems (VCS) like Git allow developers to locally rewrite
recorded history, e.g., to reorder and suppress commits or specific data in
them. These alterations have legitimate use cases, but become problematic when
performed on public branches that have downstream users: they break push/pull
workflows, challenge the integrity and reproducibility of repositories, and
create opportunities for supply chain attackers to sneak into them nefarious
changes. We conduct the first large-scale investigation of Git history
alterations in public code repositories. We analyze 111 M (millions)
repositories archived by Software Heritage, which preserves VCS histories even
across alterations. We find history alterations in 1.22 M repositories, for a
total of 8.7 M rewritten histories. We categorize changes by where they happen
(which repositories, which branches) and what is changed in them (files or
commit metadata). Conducting two targeted case studies we show that altered
histories recurrently change licenses retroactively, or are used to remove
''secrets'' (e.g., private keys) committed by mistake. As these behaviors
correspond to bad practices-in terms of project governance or security
management, respectively-that software recipients might want to avoid, we
introduce GitHistorian, an automated tool, that developers can use to spot and
describe history alterations in public Git repositories.

</details>


### [10] [Cross-Domain Evaluation of Transformer-Based Vulnerability Detection on Open & Industry Data](https://arxiv.org/abs/2509.09313)
*Moritz Mock,Thomas Forrer,Barbara Russo*

Main category: cs.SE

TL;DR: 这篇论文研究了深度学习在工业环境中的漏洞检测应用，开发了AI-DO系统并通过调查验证其实用性。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习漏洞检测技术从学术界向产业界转移时遇到的挑战，包括信任问题、继承系统、数字素养缺乏和学业产业知识差距等。

Method: 首先评估CodeBERT在工业和开源软件中检测漏洞函数的性能，分析其跨域泛化能力，并探索处理类不平衡的策略。然后开发AI-DO系统，这是一个集成到CI/CD流程中的推荐系统，使用细调的CodeBERT在代码审查时检测和定位漏洞。最后通过调查评估工具的实用性。

Result: 结果显示，基于工业数据训练的模型在同一域内准确检测漏洞，但在开源代码上性能下降。而基于开源数据细调的深度学习模型，通过适当的下采样技术，能够改善漏洞检测效果。

Conclusion: 研究证明了深度学习漏洞检测技术在工业环境中的可行性，开发的AI-DO系统能够在不打断工作流程的情况下有效检测漏洞，为学术技术向产业转移提供了实践路径。

Abstract: Deep learning solutions for vulnerability detection proposed in academic
research are not always accessible to developers, and their applicability in
industrial settings is rarely addressed. Transferring such technologies from
academia to industry presents challenges related to trustworthiness, legacy
systems, limited digital literacy, and the gap between academic and industrial
expertise. For deep learning in particular, performance and integration into
existing workflows are additional concerns. In this work, we first evaluate the
performance of CodeBERT for detecting vulnerable functions in industrial and
open-source software. We analyse its cross-domain generalisation when
fine-tuned on open-source data and tested on industrial data, and vice versa,
also exploring strategies for handling class imbalance. Based on these results,
we develop AI-DO(Automating vulnerability detection Integration for Developers'
Operations), a Continuous Integration-Continuous Deployment (CI/CD)-integrated
recommender system that uses fine-tuned CodeBERT to detect and localise
vulnerabilities during code review without disrupting workflows. Finally, we
assess the tool's perceived usefulness through a survey with the company's IT
professionals. Our results show that models trained on industrial data detect
vulnerabilities accurately within the same domain but lose performance on
open-source code, while a deep learner fine-tuned on open data, with
appropriate undersampling techniques, improves the detection of
vulnerabilities.

</details>


### [11] [ORCA: Unveiling Obscure Containers In The Wild](https://arxiv.org/abs/2509.09322)
*Jacopo Bufalino,Agathe Blaise,Stefano Secci*

Main category: cs.SE

TL;DR: 这篇论文研究容器安全分析工具在遇到不完整容器图像时的限制，提出了一种抹除强化的分析方法ORCA，能够在文件覆盖率上较现有工具提升40%。


<details>
  <summary>Details</summary>
Motivation: 现代软件开发依赖开源库和第三方组件，容器化环境中容器文件系统的无意修改会导致不完整图像，影响SCA工具的可靠性。

Method: 分析600个流行容器的不完整情况，提出抹除强化的容器分析方法，并开源实现为ORCA工具。

Result: ORCA能有效检测不完整容器的内容，在文件覆盖率上比Docker Scout和Syft提升了中位数40%。

Conclusion: 当前常见的SCA工具在分析不完整容器时存在显著限制，ORCA提供了更可靠的解决方案。

Abstract: Modern software development increasingly depends on open-source libraries and
third-party components, which are often encapsulated into containerized
environments. While improving the development and deployment of applications,
this approach introduces security risks, particularly when outdated or
vulnerable components are inadvertently included in production environments.
Software Composition Analysis (SCA) is a critical process that helps identify
and manage packages and dependencies inside a container. However, unintentional
modifications to the container filesystem can lead to incomplete container
images, which compromise the reliability of SCA tools. In this paper, we
examine the limitations of both cloud-based and open-source SCA tools when
faced with such obscure images. An analysis of 600 popular containers revealed
that obscure containers exist in well-known registries and trusted images and
that many tools fail to analyze such containers. To mitigate these issues, we
propose an obscuration-resilient methodology for container analysis and
introduce ORCA (Obscuration-Resilient Container Analyzer), its open-source
implementation. We reported our findings to all vendors using their appropriate
channels. Our results demonstrate that ORCA effectively detects the content of
obscure containers and achieves a median 40% improvement in file coverage
compared to Docker Scout and Syft.

</details>


### [12] [LoCoBench: A Benchmark for Long-Context Large Language Models in Complex Software Engineering](https://arxiv.org/abs/2509.09614)
*Jielin Qiu,Zuxin Liu,Zhiwei Liu,Rithesh Murthy,Jianguo Zhang,Haolin Chen,Shiyu Wang,Ming Zhu,Liangwei Yang,Juntao Tan,Zhepeng Cen,Cheng Qian,Shelby Heinecke,Weiran Yao,Silvio Savarese,Caiming Xiong,Huan Wang*

Main category: cs.SE

TL;DR: LoCoBench是一个专门评估长上下文LLM在复杂软件开发场景中表现的基准测试，包含8,000个评估场景，覆盖10种编程语言，上下文长度从10K到1M tokens，包含8个任务类别和17个评估指标。


<details>
  <summary>Details</summary>
Motivation: 随着支持百万token上下文窗口的语言模型出现，需要专门的基准测试来评估这些模型在真实复杂软件开发场景中的长上下文理解能力，填补现有代码评估基准在长上下文能力评估方面的空白。

Method: 通过5阶段流水线系统生成多样化的高质量评估场景，包含8个任务类别：架构理解、跨文件重构、多会话开发、bug调查、功能实现、代码理解、集成测试和安全分析。引入包含17个指标的全面评估框架，包括8个新指标，并组合成LoCoBench Score (LCBS)。

Result: 对最先进的长上下文模型评估显示存在显著的性能差距，表明在复杂软件开发中的长上下文理解仍然是一个重要的未解决挑战。

Conclusion: LoCoBench揭示了长上下文模型在复杂软件开发场景中的局限性，强调了这一领域需要更多关注和研究，为未来模型开发提供了重要的评估工具。

Abstract: The emergence of long-context language models with context windows extending
to millions of tokens has created new opportunities for sophisticated code
understanding and software development evaluation. We propose LoCoBench, a
comprehensive benchmark specifically designed to evaluate long-context LLMs in
realistic, complex software development scenarios. Unlike existing code
evaluation benchmarks that focus on single-function completion or short-context
tasks, LoCoBench addresses the critical evaluation gap for long-context
capabilities that require understanding entire codebases, reasoning across
multiple files, and maintaining architectural consistency across large-scale
software systems. Our benchmark provides 8,000 evaluation scenarios
systematically generated across 10 programming languages, with context lengths
spanning 10K to 1M tokens, a 100x variation that enables precise assessment of
long-context performance degradation in realistic software development
settings. LoCoBench introduces 8 task categories that capture essential
long-context capabilities: architectural understanding, cross-file refactoring,
multi-session development, bug investigation, feature implementation, code
comprehension, integration testing, and security analysis. Through a 5-phase
pipeline, we create diverse, high-quality scenarios that challenge LLMs to
reason about complex codebases at unprecedented scale. We introduce a
comprehensive evaluation framework with 17 metrics across 4 dimensions,
including 8 new evaluation metrics, combined in a LoCoBench Score (LCBS). Our
evaluation of state-of-the-art long-context models reveals substantial
performance gaps, demonstrating that long-context understanding in complex
software development represents a significant unsolved challenge that demands
more attention. LoCoBench is released at:
https://github.com/SalesforceAIResearch/LoCoBench.

</details>


### [13] [I Know Who Clones Your Code: Interpretable Smart Contract Similarity Detection](https://arxiv.org/abs/2509.09630)
*Zhenguang Liu,Lixun Ma,Zhongzheng Mu,Chengkun Wei,Xiaojun Xu,Yingying Jiao,Kui Ren*

Main category: cs.SE

TL;DR: SmartDetector是一种新颖的智能合约函数相似性检测方法，通过将AST分解为语句树并进行细粒度比较，在三个真实数据集上平均F1分数达到95.88%，比现有方法提升14.01%。


<details>
  <summary>Details</summary>
Motivation: 开源代码在智能合约开发中的广泛重用虽然提高了编程效率，但也显著增加了漏洞传播风险。现有的基于AST的方法难以处理复杂树结构，而深度学习方法往往忽视代码语法和可解释性，导致性能不佳。

Method: SmartDetector将智能合约函数的AST分解为一系列较小的语句树，每个语句树反映源代码的结构元素。然后使用分类器通过比较每对语句树来计算两个函数的相似度得分。为了解决分类器无限超参数空间的问题，数学推导了余弦扩散过程来高效搜索最优超参数。

Result: 在三个大型真实数据集上的广泛实验表明，SmartDetector在F1分数上平均比当前最先进方法提高了14.01%，总体平均F1分数达到95.88%。

Conclusion: SmartDetector通过细粒度的语句级相似性比较，有效解决了智能合约函数相似性检测问题，在性能和可解释性方面都表现出色，为智能合约安全分析提供了有力工具。

Abstract: Widespread reuse of open-source code in smart contract development boosts
programming efficiency but significantly amplifies bug propagation across
contracts, while dedicated methods for detecting similar smart contract
functions remain very limited. Conventional abstract-syntax-tree (AST) based
methods for smart contract similarity detection face challenges in handling
intricate tree structures, which impedes detailed semantic comparison of code.
Recent deep-learning based approaches tend to overlook code syntax and
detection interpretability, resulting in suboptimal performance.
  To fill this research gap, we introduce SmartDetector, a novel approach for
computing similarity between smart contract functions, explainable at the
fine-grained statement level. Technically, SmartDetector decomposes the AST of
a smart contract function into a series of smaller statement trees, each
reflecting a structural element of the source code. Then, SmartDetector uses a
classifier to compute the similarity score of two functions by comparing each
pair of their statement trees. To address the infinite hyperparameter space of
the classifier, we mathematically derive a cosine-wise diffusion process to
efficiently search optimal hyperparameters. Extensive experiments conducted on
three large real-world datasets demonstrate that SmartDetector outperforms
current state-of-the-art methods by an average improvement of 14.01% in
F1-score, achieving an overall average F1-score of 95.88%.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [14] [Cross-Service Token: Finding Attacks in 5G Core Networks](https://arxiv.org/abs/2509.08992)
*Anqi Chen,Riccardo Preatoni,Alessandro Brighente,Mauro Conti,Cristina Nita-Rotaru*

Main category: cs.CR

TL;DR: FivGeeFuzz是一个基于语法的模糊测试框架，用于发现5G核心网络中服务化接口的安全漏洞，在free5GC中发现了8个未知漏洞


<details>
  <summary>Details</summary>
Motivation: 5G核心网络采用服务化架构，网络功能通过HTTP API通信，云基础设施增加了内部攻击风险，需要研究安全漏洞以防止攻击者利用受损网络功能获取未授权资源访问

Method: 从3GPP API规范自动推导语法，生成畸形、意外或语义不一致的输入，集成自动化bug检测与手动验证和根因分析

Result: 在free5GC中发现了8个未知漏洞，包括运行时崩溃、错误处理不当和未授权资源访问，其中7个已修复，1个正在修复

Conclusion: FivGeeFuzz框架有效发现了5G核心网络服务化接口的安全漏洞，证明了其在保护5G网络安全方面的重要性

Abstract: 5G marks a major departure from previous cellular architectures, by
transitioning from a monolithic design of the core network to a Service-Based
Architecture (SBA) where services are modularized as Network Functions (NFs)
which communicate with each other via standard-defined HTTP-based APIs called
Service-Based Interfaces (SBIs). These NFs are deployed in private and public
cloud infrastructure, and an access control framework based on OAuth restricts
how they communicate with each other and obtain access to resources. Given the
increased vulnerabilities of clouds to insiders, it is important to study the
security of the 5G Core services for vulnerabilities that allow attackers to
use compromised NFs to obtain unauthorized access to resources.
  We present FivGeeFuzz, a grammar-based fuzzing framework designed to uncover
security flaws in 5G core SBIs. FivGeeFuzz automatically derives grammars from
3GPP API specifications to generate malformed, unexpected, or semantically
inconsistent inputs, and it integrates automated bug detection with manual
validation and root-cause analysis. We evaluate our approach on free5GC, the
only open-source 5G core implementing Release 17-compliant SBIs with an access
control mechanism. Using FivGeeFuzz, we discovered 8 previously unknown
vulnerabilities in free5GC, leading to runtime crashes, improper error
handling, and unauthorized access to resources, including a very severe attack
we call Cross-Service Token Attack. All bugs were confirmed by the free5GC
team, 7 have already been patched, and the remaining one has a patch under
development.

</details>


### [15] [When FinTech Meets Privacy: Securing Financial LLMs with Differential Private Fine-Tuning](https://arxiv.org/abs/2509.08995)
*Sichen Zhu,Hoyeung Leung,Xiaoyi Wang,Jia Wei,Honghui Xu*

Main category: cs.CR

TL;DR: DPFinLLM是一个专为金融应用设计的轻量级隐私保护大语言模型，结合差分隐私机制和精简架构，在保护敏感金融数据的同时保持高性能。


<details>
  <summary>Details</summary>
Motivation: 随着AI模型在金融边缘设备部署的增加，保护敏感金融数据的隐私成为重要挑战，需要开发既能保护隐私又保持高性能的轻量级LLM解决方案。

Method: 提出DPFinLLM模型，结合鲁棒的差分隐私机制和受先进模型启发的精简架构，专门为设备端金融应用设计。

Result: 在多个金融情感数据集上的广泛实验验证了DPFinLLM的有效性，即使在严格隐私约束下也能达到与完全微调模型相当的性能。

Conclusion: DPFinLLM成功解决了金融领域隐私保护与性能平衡的问题，为设备端金融AI应用提供了安全高效的解决方案。

Abstract: The integration of Large Language Models (LLMs) into financial technology
(FinTech) has revolutionized the analysis and processing of complex financial
data, driving advancements in real-time decision-making and analytics. With the
growing trend of deploying AI models on edge devices for financial
applications, ensuring the privacy of sensitive financial data has become a
significant challenge. To address this, we propose DPFinLLM, a
privacy-enhanced, lightweight LLM specifically designed for on-device financial
applications. DPFinLLM combines a robust differential privacy mechanism with a
streamlined architecture inspired by state-of-the-art models, enabling secure
and efficient processing of financial data. This proposed DPFinLLM can not only
safeguard user data from privacy breaches but also ensure high performance
across diverse financial tasks. Extensive experiments on multiple financial
sentiment datasets validate the effectiveness of DPFinLLM, demonstrating its
ability to achieve performance comparable to fully fine-tuned models, even
under strict privacy constraints.

</details>


### [16] [Beyond Tag Collision: Cluster-based Memory Management for Tag-based Sanitizers](https://arxiv.org/abs/2509.09089)
*Mengfei Xie,Yan Lin,Hongtao Wu,Jianming Fu,Chenke Luo,Guojun Peng*

Main category: cs.CR

TL;DR: ClusterTag是一种基于集群的内存分配器，通过将内存对象分组到独立集群中并在集群间引入随机地址间隔，有效缓解了标签冲突问题，在保持低性能开销的同时提供了确定性的安全检测结果。


<details>
  <summary>Details</summary>
Motivation: 现有的标签消毒器由于标签编码空间有限，在时间和空间维度上难以给内存对象分配唯一标签，导致标签冲突问题，可能产生误报或漏报。

Method: 设计集群化内存分配器，将内存对象分组到独立集群中限制标签冲突范围；采用集群粒度的堆随机化方案，在集群间引入随机地址间隔，打破标签空间的熵限制。

Result: 性能开销控制在1%以内；在Juliet数据集上表现出确定性检测结果（500次重复测试一致），而现有标签分配策略存在概率性漏报；在最小、平均和不可预测性三个标签冲突距离指标上均实现平衡改进。

Conclusion: ClusterTag通过集群化设计和堆随机化有效解决了标签冲突问题，为标签消毒器提供了更可靠的安全保障，同时保持了较低的性能开销。

Abstract: Tag-based sanitizers attach a small "key" to each pointer and a matching
"lock" tag to its target memory object, enabling runtime verification of
pointer-object consistency and helping developers to detect potential memory
violations. However, the limited tag encoding space challenges existing studies
in assigning distinct tags to memory objects across temporal and spatial
dimensions, leading to potential tag collisions. In this paper, we present
ClusterTag, a novel cluster-based memory allocator aimed at simultaneously
mitigating tag collisions in both temporal and spatial dimensions. The core
design of ClusterTag effectively balances the significant mismatch between tag
encoding space and memory objects: it divides memory objects into multiple
independent clusters, thereby limiting tag collisions to finite chunks within
each cluster. To mitigate tag collisions across clusters, we design a
cluster-grained heap randomization scheme. This approach introduces random
address intervals between clusters and further breaks the entropy limitation of
the tag space. ClusterTag has been implemented as an independent memory
allocator that seamlessly integrates with tag-based sanitizers such as HWASan,
and maintains comparable performance overhead (within 1%) at various
randomization densities. Security evaluations on the Juliet dataset indicate
that ClusterTag exhibits deterministic results across 500 repeated tests (5,652
reported and 1,530 missed), while the existing three types of tag assignment
strategies all exhibit probabilistic false negatives due to tag collisions.
Quantitative analysis across three tag collision distance metrics-minimum,
average, and unpredictability-demonstrates that ClusterTag achieves balanced
improvements across all three, whereas prior tag assignment schemes (random,
staggered, fixed) show significant trade-offs in at least one metric.

</details>


### [17] [Towards Confidential and Efficient LLM Inference with Dual Privacy Protection](https://arxiv.org/abs/2509.09091)
*Honglan Yu,Yibin Wang,Feifei Dai,Dong Liu,Haihui Fan,Xiaoyan Gu*

Main category: cs.CR

TL;DR: CMIF是一个保护隐私的高效模型推理框架，通过在客户端TEE中部署嵌入层，在GPU服务器上运行后续层，并优化Report-Noisy-Max机制，在保护数据隐私的同时减少推理开销。


<details>
  <summary>Details</summary>
Motivation: 解决TEE中高推理延迟和DP方法中噪声影响LLM性能的问题，需要在保护隐私的同时保持模型效率和语义理解能力。

Method: 1) 在客户端TEE中部署嵌入层，在GPU服务器上运行后续层；2) 优化Report-Noisy-Max机制来保护敏感输入；3) 减少TEE与GPU之间的通信开销。

Result: 在Llama系列模型上的实验表明，CMIF显著减少了TEE中的额外推理开销，同时有效保护了用户数据隐私。

Conclusion: CMIF框架成功解决了TEE和DP方法在LLM隐私保护中的局限性，实现了隐私保护与推理效率的平衡。

Abstract: CPU-based trusted execution environments (TEEs) and differential privacy (DP)
have gained wide applications for private inference. Due to high inference
latency in TEEs, researchers use partition-based approaches that offload linear
model components to GPUs. However, dense nonlinear layers of large language
models (LLMs) result in significant communication overhead between TEEs and
GPUs. DP-based approaches apply random noise to protect data privacy, but this
compromises LLM performance and semantic understanding. To overcome the above
drawbacks, this paper proposes CMIF, a Confidential and efficient Model
Inference Framework. CMIF confidentially deploys the embedding layer in the
client-side TEE and subsequent layers on GPU servers. Meanwhile, it optimizes
the Report-Noisy-Max mechanism to protect sensitive inputs with a slight
decrease in model performance. Extensive experiments on Llama-series models
demonstrate that CMIF reduces additional inference overhead in TEEs while
preserving user data privacy.

</details>


### [18] [DP-FedLoRA: Privacy-Enhanced Federated Fine-Tuning for On-Device Large Language Models](https://arxiv.org/abs/2509.09097)
*Honghui Xu,Shiva Shrestha,Wei Chen,Zhiyuan Li,Zhipeng Cai*

Main category: cs.CR

TL;DR: DP-FedLoRA是一个隐私增强的联邦微调框架，将LoRA适配与差分隐私结合，在边缘设备上实现隐私保护的LLM微调


<details>
  <summary>Details</summary>
Motivation: 随着设备端大语言模型的普及，联邦微调需要处理敏感的用户数据，存在严重的隐私问题，需要设计既能保护隐私又高效的解决方案

Method: 提出DP-FedLoRA框架，客户端使用高斯噪声对LoRA矩阵进行裁剪和扰动，满足(ε,δ)-差分隐私，并提供理论分析证明更新的无偏性和噪声方差界限

Result: 在主流基准测试中，DP-FedLoRA在提供强隐私保证的同时保持了竞争性的性能表现

Conclusion: 该框架为设备端环境中可扩展且隐私保护的LLM部署铺平了道路

Abstract: As on-device large language model (LLM) systems become increasingly
prevalent, federated fine-tuning enables advanced language understanding and
generation directly on edge devices; however, it also involves processing
sensitive, user-specific data, raising significant privacy concerns within the
federated learning framework. To address these challenges, we propose
DP-FedLoRA, a privacy-enhanced federated fine-tuning framework that integrates
LoRA-based adaptation with differential privacy in a communication-efficient
setting. Each client locally clips and perturbs its LoRA matrices using
Gaussian noise to satisfy ($\epsilon$, $\delta$)-differential privacy. We
further provide a theoretical analysis demonstrating the unbiased nature of the
updates and deriving bounds on the variance introduced by noise, offering
practical guidance for privacy-budget calibration. Experimental results across
mainstream benchmarks show that DP-FedLoRA delivers competitive performance
while offering strong privacy guarantees, paving the way for scalable and
privacy-preserving LLM deployment in on-device environments.

</details>


### [19] [AgriSentinel: Privacy-Enhanced Embedded-LLM Crop Disease Alerting System](https://arxiv.org/abs/2509.09103)
*Chanti Raju Mylay,Bobin Deng,Zhipeng Cai,Honghui Xu*

Main category: cs.CR

TL;DR: AgriSentinel是一个基于嵌入式LLM的隐私增强型作物病害预警系统，通过差分隐私保护敏感数据，在移动设备上实现轻量级病害分类，并提供可操作的病害管理建议。


<details>
  <summary>Details</summary>
Motivation: 现有作物病害预警系统忽视数据隐私、市场定价权和农民友好性等问题，使农民面临隐私泄露和经济剥削风险。

Method: 采用差分隐私机制保护作物图像数据，开发轻量级深度学习分类模型优化移动设备部署，并利用微调的本地大型语言模型提供具体病害管理建议。

Result: 实验验证系统能有效保护数据隐私，保持高分类性能，并提供实用的病害管理策略。

Conclusion: AgriSentinel为自动化作物病害预警和管理提供了强大、农民友好的解决方案，有助于改善农业决策和提高作物生产力。

Abstract: Crop diseases pose significant threats to global food security, agricultural
productivity, and sustainable farming practices, directly affecting farmers'
livelihoods and economic stability. To address the growing need for effective
crop disease management, AI-based disease alerting systems have emerged as
promising tools by providing early detection and actionable insights for timely
intervention. However, existing systems often overlook critical aspects such as
data privacy, market pricing power, and farmer-friendly usability, leaving
farmers vulnerable to privacy breaches and economic exploitation. To bridge
these gaps, we propose AgriSentinel, the first Privacy-Enhanced Embedded-LLM
Crop Disease Alerting System. AgriSentinel incorporates a differential privacy
mechanism to protect sensitive crop image data while maintaining classification
accuracy. Its lightweight deep learning-based crop disease classification model
is optimized for mobile devices, ensuring accessibility and usability for
farmers. Additionally, the system includes a fine-tuned, on-device large
language model (LLM) that leverages a curated knowledge pool to provide farmers
with specific, actionable suggestions for managing crop diseases, going beyond
simple alerting. Comprehensive experiments validate the effectiveness of
AgriSentinel, demonstrating its ability to safeguard data privacy, maintain
high classification performance, and deliver practical, actionable disease
management strategies. AgriSentinel offers a robust, farmer-friendly solution
for automating crop disease alerting and management, ultimately contributing to
improved agricultural decision-making and enhanced crop productivity.

</details>


### [20] [CryptGNN: Enabling Secure Inference for Graph Neural Networks](https://arxiv.org/abs/2509.09107)
*Pritam Sen,Yao Ma,Cristian Borcea*

Main category: cs.CR

TL;DR: CryptGNN是一个安全的图神经网络推理解决方案，使用多方安全计算技术保护客户端数据和模型参数隐私


<details>
  <summary>Details</summary>
Motivation: 解决云环境中第三方GNN模型推理时的隐私保护问题，防止云服务提供商和模型所有者获取客户端数据和图结构信息，同时保护模型参数不被泄露

Method: 采用分布式安全多方计算(SMPC)技术实现安全的消息传递和特征变换层，支持任意数量的SMPC参与方，无需可信服务器，可抵抗P-1个参与方的合谋攻击

Result: 理论分析和实验验证表明CryptGNN具有安全性和高效性

Conclusion: CryptGNN为云环境中的GNN模型推理提供了有效的隐私保护解决方案，能够在保护各方隐私的同时实现安全的模型推理服务

Abstract: We present CryptGNN, a secure and effective inference solution for
third-party graph neural network (GNN) models in the cloud, which are accessed
by clients as ML as a service (MLaaS). The main novelty of CryptGNN is its
secure message passing and feature transformation layers using distributed
secure multi-party computation (SMPC) techniques. CryptGNN protects the
client's input data and graph structure from the cloud provider and the
third-party model owner, and it protects the model parameters from the cloud
provider and the clients. CryptGNN works with any number of SMPC parties, does
not require a trusted server, and is provably secure even if P-1 out of P
parties in the cloud collude. Theoretical analysis and empirical experiments
demonstrate the security and efficiency of CryptGNN.

</details>


### [21] [Character-Level Perturbations Disrupt LLM Watermarks](https://arxiv.org/abs/2509.09112)
*Zhaoxi Zhang,Xiaomei Zhang,Yanjun Zhang,He Zhang,Shirui Pan,Bo Liu,Asif Qumer Gill,Leo Yu Zhang*

Main category: cs.CR

TL;DR: 本文研究发现字符级扰动比传统方法更有效去除LLM水印，提出基于遗传算法的引导攻击方法，并揭示了现有水印方案的重大漏洞


<details>
  <summary>Details</summary>
Motivation: 现有水印去除方法通常效果不佳，造成需要大扰动或强大对手才能有效去除的误解，需要研究在实际威胁模型下的有效去除方法

Method: 首先形式化LLM水印系统模型，分析不同扰动类型的攻击范围，发现字符级扰动能通过干扰分词过程同时影响多个token。提出基于遗传算法的引导去除攻击，使用参考检测器进行优化

Result: 实验证实字符级扰动在限制性威胁模型下显著更有效，GA方法在有限黑盒查询条件下表现出强大的去除性能。自适应复合字符级攻击能有效击败防御

Conclusion: 现有LLM水印方案存在重大漏洞，迫切需要开发新的鲁棒机制

Abstract: Large Language Model (LLM) watermarking embeds detectable signals into
generated text for copyright protection, misuse prevention, and content
detection. While prior studies evaluate robustness using watermark removal
attacks, these methods are often suboptimal, creating the misconception that
effective removal requires large perturbations or powerful adversaries.
  To bridge the gap, we first formalize the system model for LLM watermark, and
characterize two realistic threat models constrained on limited access to the
watermark detector. We then analyze how different types of perturbation vary in
their attack range, i.e., the number of tokens they can affect with a single
edit. We observe that character-level perturbations (e.g., typos, swaps,
deletions, homoglyphs) can influence multiple tokens simultaneously by
disrupting the tokenization process. We demonstrate that character-level
perturbations are significantly more effective for watermark removal under the
most restrictive threat model. We further propose guided removal attacks based
on the Genetic Algorithm (GA) that uses a reference detector for optimization.
Under a practical threat model with limited black-box queries to the watermark
detector, our method demonstrates strong removal performance. Experiments
confirm the superiority of character-level perturbations and the effectiveness
of the GA in removing watermarks under realistic constraints. Additionally, we
argue there is an adversarial dilemma when considering potential defenses: any
fixed defense can be bypassed by a suitable perturbation strategy. Motivated by
this principle, we propose an adaptive compound character-level attack.
Experimental results show that this approach can effectively defeat the
defenses. Our findings highlight significant vulnerabilities in existing LLM
watermark schemes and underline the urgency for the development of new robust
mechanisms.

</details>


### [22] [IoTFuzzSentry: A Protocol Guided Mutation Based Fuzzer for Automatic Vulnerability Testing in Commercial IoT Devices](https://arxiv.org/abs/2509.09158)
*Priyanka Rushikesh Chaudhary,Rajib Ranjan Maiti*

Main category: cs.CR

TL;DR: IoTFuzzSentry是一个基于变异的协议模糊测试工具，用于发现物联网设备中的安全漏洞，已在商业设备中发现4类漏洞并披露了2个CVE。


<details>
  <summary>Details</summary>
Motivation: 物联网设备在运行阶段常运行轻量级服务器处理用户交互，传输层或应用层安全机制的实现缺陷可能导致未经授权访问和数据泄露等威胁。

Method: 开发了名为IoTFuzzSentry的变异式模糊测试工具，向物联网通信中注入精心构造的传输层和应用层数据包，并集成到Cotopaxi测试工具中。

Result: 在商用物联网设备（IP摄像头和智能插座）中发现4类漏洞：IoT访问凭证泄露、偷窥实时视频流、窃取实时图像、IoT命令注入，已披露2个CVE，还有一个待发布。

Conclusion: IoTFuzzSentry有潜力发现非常规安全威胁，帮助物联网厂商以可忽略的开销自动加强商业化物联网设备的安全性。

Abstract: Protocol fuzzing is a scalable and cost-effective technique for identifying
security vulnerabilities in deployed Internet of Things devices. During their
operational phase, IoT devices often run lightweight servers to handle user
interactions, such as video streaming or image capture in smart cameras.
Implementation flaws in transport or application-layer security mechanisms can
expose IoT devices to a range of threats, including unauthorized access and
data leakage. This paper addresses the challenge of uncovering such
vulnerabilities by leveraging protocol fuzzing techniques that inject crafted
transport and application-layer packets into IoT communications. We present a
mutation-based fuzzing tool, named IoTFuzzSentry, to identify specific
non-trivial vulnerabilities in commercial IoT devices. We further demonstrate
how these vulnerabilities can be exploited in real-world scenarios. We
integrated our fuzzing tool into a well-known testing tool Cotopaxi and
evaluated it with commercial-off-the-shelf IoT devices such as IP cameras and
Smart Plug. Our evaluation revealed vulnerabilities categorized into 4 types
(IoT Access Credential Leakage, Sneak IoT Live Video Stream, Creep IoT Live
Image, IoT Command Injection) and we show their exploits using three IoT
devices. We have responsibly disclosed all these vulnerabilities to the
respective vendors. So far, we have published two CVEs, CVE-2024-41623 and
CVE-2024-42531, and one is awaiting. To extend the applicability, we have
investigated the traffic of six additional IoT devices and our analysis shows
that these devices can have similar vulnerabilities, due to the presence of a
similar set of application protocols. We believe that IoTFuzzSentry has the
potential to discover unconventional security threats and allow IoT vendors to
strengthen the security of their commercialized IoT devices automatically with
negligible overhead.

</details>


### [23] [Enhancing Cyber Threat Hunting -- A Visual Approach with the Forensic Visualization Toolkit](https://arxiv.org/abs/2509.09185)
*Jihane Najar,Marinos Tsantekidis,Aris Sotiropoulos,Vassilis Prevelakis*

Main category: cs.CR

TL;DR: 提出了Forensic Visualization Toolkit (FVT)工具，用于增强网络安全态势感知和威胁狩猎能力


<details>
  <summary>Details</summary>
Motivation: 应对动态网络威胁环境，需要主动识别传统安全措施可能遗漏的高级威胁，而不是等待自动化系统报警

Method: 开发了FVT工具，提供数字取证调查、数字证据分析和高级可视化功能，支持交互式威胁搜索

Result: FVT已集成到多个欧盟资助研究项目中并持续改进，通过实际场景验证能显著增强网络安全专业人员的能力

Conclusion: FVT是一个强大的工具，能够有效帮助安全分析师识别、分析和应对网络威胁，提升网络安全防御水平

Abstract: In today's dynamic cyber threat landscape, organizations must take proactive
steps to bolster their cybersecurity defenses. Cyber threat hunting is a
proactive and iterative process aimed at identifying and mitigating advanced
threats that may go undetected by traditional security measures. Rather than
waiting for automated security systems to flag potential threats, threat
hunting involves actively searching for signs of malicious activity within an
organization's network. In this paper, we present the Forensic Visualization
Toolkit, a powerful tool designed for digital forensics investigations,
analysis of digital evidence, and advanced visualizations to enhance
cybersecurity situational awareness and risk management and empower security
analysts with an intuitive and interactive tool. Through practical, real-world
scenarios, we demonstrate how FVT significantly amplifies the capabilities of
cybersecurity professionals, enabling them to effectively identify, analyze,
and respond to threats. Furthermore, it is important to highlight that FVT has
been integrated into, utilized, and continually enhanced within various
EU-funded research projects over recent years.

</details>


### [24] [Shell or Nothing: Real-World Benchmarks and Memory-Activated Agents for Automated Penetration Testing](https://arxiv.org/abs/2509.09207)
*Wuyuao Mai,Geng Hong,Qi Liu,Jinsong Chen,Jiarun Dai,Xudong Pan,Yuan Zhang,Min Yang*

Main category: cs.CR

TL;DR: 提出了首个真实世界渗透测试基准TermiBench和新型多代理框架TermiAgent，解决了现有AI渗透测试在简化CTF环境中性能虚高的问题，在真实环境下显著提升了渗透测试能力。


<details>
  <summary>Details</summary>
Motivation: 传统渗透测试成本高、耗时长且依赖专家人力，现有AI驱动的渗透测试代理在过度简化的CTF环境中评估，性能估计远离真实世界实践，需要建立真实世界的基准测试框架。

Method: 提出TermiBench基准测试（510个主机、25种服务、30个CVE）和TermiAgent多代理框架，采用定位记忆激活机制解决长上下文遗忘问题，通过结构化代码理解构建可靠的漏洞利用库。

Result: 现有系统在真实条件下几乎无法获得系统shell，而TermiAgent在评估中优于最先进代理，表现出更强的渗透测试能力，减少执行时间和财务成本，甚至在笔记本电脑规模部署中也具有实用性。

Conclusion: 这项工作提供了首个开源的真实世界自主渗透测试基准和新型代理框架，为AI驱动的渗透测试建立了里程碑，解决了现有方法的局限性并展示了实际应用价值。

Abstract: Penetration testing is critical for identifying and mitigating security
vulnerabilities, yet traditional approaches remain expensive, time-consuming,
and dependent on expert human labor. Recent work has explored AI-driven
pentesting agents, but their evaluation relies on oversimplified
capture-the-flag (CTF) settings that embed prior knowledge and reduce
complexity, leading to performance estimates far from real-world practice. We
close this gap by introducing the first real-world, agent-oriented pentesting
benchmark, TermiBench, which shifts the goal from 'flag finding' to achieving
full system control. The benchmark spans 510 hosts across 25 services and 30
CVEs, with realistic environments that require autonomous reconnaissance,
discrimination between benign and exploitable services, and robust exploit
execution. Using this benchmark, we find that existing systems can hardly
obtain system shells under realistic conditions.
  To address these challenges, we propose TermiAgent, a multi-agent penetration
testing framework. TermiAgent mitigates long-context forgetting with a Located
Memory Activation mechanism and builds a reliable exploit arsenal via
structured code understanding rather than naive retrieval. In evaluations, our
work outperforms state-of-the-art agents, exhibiting stronger penetration
testing capability, reducing execution time and financial cost, and
demonstrating practicality even on laptop-scale deployments. Our work delivers
both the first open-source benchmark for real-world autonomous pentesting and a
novel agent framework that establishes a milestone for AI-driven penetration
testing.

</details>


### [25] [A Cyber-Twin Based Honeypot for Gathering Threat Intelligence](https://arxiv.org/abs/2509.09222)
*Muhammad Azmi Umer,Zhan Xuna,Yan Lin Aung,Aditya P. Mathur,Jianying Zhou*

Main category: cs.CR

TL;DR: 基于水处理厂网络双生的鬼网终结系统，用于吸引和记录网络攻击，为关键基础设施提供威胁情报


<details>
  <summary>Details</summary>
Motivation: 关键基础设施易受网络攻击，需要有效的防护技术来应对这些威胁

Method: 开发了一种基于水处理厂网络双生的鬼网终结系统，作为实际水处理厂的现实复制品

Result: 该鬼网终结已经运行并多次受到攻击，包括详细描述的劫持磁盘攻击，收集到了有价值的攻击数据

Conclusion: 通过鬼网终结系统获得的威胁情报可以分享给水处理厂管理方，用于改善厂网保护系统，提高关键基础设施的网络安全防护能力

Abstract: Critical Infrastructure (CI) is prone to cyberattacks. Several techniques
have been developed to protect CI against such attacks. In this work, we
describe a honeypot based on a cyber twin for a water treatment plant. The
honeypot is intended to serve as a realistic replica of a water treatment plant
that attracts potential attackers. The attacks launched on the honeypot are
recorded and analyzed for threat intelligence. The intelligence so obtained is
shared with the management of water treatment plants, who in turn may use it to
improve plant protection systems. The honeypot used here is operational and has
been attacked on several occasions using, for example, a ransomware attack that
is described in detail.

</details>


### [26] [What You Code Is What We Prove: Translating BLE App Logic into Formal Models with LLMs for Vulnerability Detection](https://arxiv.org/abs/2509.09291)
*Biwei Yan,Yue Zhang,Minghui Xu,Runyu Pan,Jinku Li,Xiuzhen Cheng*

Main category: cs.CR

TL;DR: 通过LLM将BLE应用程序代码转换为可验证的形式模型，实现了大规模的安全性验证，发现1050个Android BLE应用中仅50%以上缺少核心安全保护


<details>
  <summary>Details</summary>
Motivation: BLE应用层安全漏洞逐渐增多，开发者常忽略加密、认证等关键保护。形式验证虽可检查这些属性，但手动建模耗时耗力，无法进行大规模分析

Method: 将BLE安全分析重构为语义翻译问题，利用LLM作为翻译器将BLE特定代码转换为ProVerif可验证的过程模型，实现VerifiaBLE系统，结合静态分析、提示导向的LLM翻译和符号验证

Result: 在1,050个Android BLE应用中：仅10.2%实现了加密、随机性和认证三项保护，53.9%完全缺少这些保护，显示了系统性的安全弱点

Conclusion: 使用LLM作为结构化翻译器可以降低形式方法的门槛，开启在安全关键领域的可扩展验证

Abstract: The application layer of Bluetooth Low Energy (BLE) is a growing source of
security vulnerabilities, as developers often neglect to implement critical
protections such as encryption, authentication, and freshness. While formal
verification offers a principled way to check these properties, the manual
effort of constructing formal models makes it impractical for large-scale
analysis. This paper introduces a key insight: BLE application security
analysis can be reframed as a semantic translation problem, i.e., from
real-world code to formal models. We leverage large language models (LLMs) not
to directly detect vulnerabilities, but to serve as translators that convert
BLE-specific code into process models verifiable by tools like ProVerif. We
implement this idea in VerifiaBLE, a system that combines static analysis,
prompt-guided LLM translation, and symbolic verification to check three core
security features: encryption, randomness, and authentication. Applied to 1,050
Android BLE apps, VerifiaBLE uncovers systemic weaknesses: only 10.2\% of apps
implement all three protections, while 53.9\% omit them entirely. Our work
demonstrates that using LLMs as structured translators can lower the barrier to
formal methods, unlocking scalable verification across security-critical
domains.

</details>


### [27] [On the Security of SSH Client Signatures](https://arxiv.org/abs/2509.09331)
*Fabian Bäumer,Marcus Brinkmann,Maximilian Radoy,Jörg Schwenk,Juraj Somorovsky*

Main category: cs.CR

TL;DR: 该研究通过收集和分析SSH客户端公钥，发现ECDSA确定性nonce漏洞，导致PuTTY客户端私钥可从58个签名中恢复。


<details>
  <summary>Details</summary>
Motivation: 填补SSH客户端安全性的研究空白，因为与服务器不同，SSH客户端无法通过互联网扫描进行测量。

Method: 1) 从GitHub和GitLab等开放平台收集SSH客户端公钥并进行安全测试；2) 对24个流行SSH客户端的签名算法实现进行黑盒实验分析。

Result: 收集了31,622,338个密钥，发现98个短密钥、139个弱随机性密钥、149个公因子密钥。首次证明ECDSA确定性nonce可导致PuTTY私钥从58个签名中恢复(CVE-2024-31497)。

Conclusion: 虽然大多数密钥安全，但ECDSA确定性nonce存在严重漏洞，需要引起重视。PuTTY已修复该漏洞。

Abstract: Administrators and developers use SSH client keys and signatures for
authentication, for example, to access internet backbone servers or to commit
new code on platforms like GitHub. However, unlike servers, SSH clients cannot
be measured through internet scans. We close this gap in two steps. First, we
collect SSH client public keys. Such keys are regularly published by their
owners on open development platforms like GitHub and GitLab. We systematize
previous non-academic work by subjecting these keys to various security tests
in a longitudinal study. Second, in a series of black-box lab experiments, we
analyze the implementations of algorithms for SSH client signatures in 24
popular SSH clients for Linux, Windows, and macOS.
  We extracted 31,622,338 keys from three public sources in two scans. Compared
to previous work, we see a clear tendency to abandon RSA signatures in favor of
EdDSA signatures. Still, in January 2025, we found 98 broken short keys, 139
keys generated from weak randomness, and 149 keys with common or small
factors-the large majority of the retrieved keys exposed no weakness.
  Weak randomness can not only compromise a secret key through its public key,
but also through signatures. It is well-known that a bias in random nonces in
ECDSA can reveal the secret key through public signatures. For the first time,
we show that the use of deterministic nonces in ECDSA can also be dangerous:
The private signing key of a PuTTY client can be recovered from just 58 valid
signatures if ECDSA with NIST curve P-521 is used. PuTTY acknowledged our
finding in CVE-2024-31497, and they subsequently replaced the nonce generation
algorithm.

</details>


### [28] [[Extended] Ethics in Computer Security Research: A Data-Driven Assessment of the Past, the Present, and the Possible Future](https://arxiv.org/abs/2509.09351)
*Harshini Sri Ramulu,Helen Schmitt,Bogdan Rerich,Rachel Gonzalez Rodriguez,Tadayoshi Kohno,Yasemin Acar*

Main category: cs.CR

TL;DR: 这篇论文分析了计算机安全领域的论理问题状况，通过对顶级论文的审查和专家访谈，发现了论理报告不一致、缺乏平衡权衡的问题，并提出改进建议。


<details>
  <summary>Details</summary>
Motivation: 计算机安全领域经常讨论论理问题，但研究人员缺乏明确的论理决策指南，特别是在道德上不明确的情况下。

Method: 审查了2024年1154篇顶级安全论文的论理报告情况，进行了半结构化访谈研究（访谈24位安全和隐私研究人员）。

Result: 发现论理报告水平不一致，重点集中在机构批准、人体保护和负责任暴露上，缺乏对害处与改善的平衡讨论。访谈显示研究人员期待进行论理研究但价值观、框架和决策缺乏一致性。

Conclusion: 提供了当前计算机安全研究论理状况的概览，并提出改进该领域论理研究状况的建议。

Abstract: Ethical questions are discussed regularly in computer security. Still,
researchers in computer security lack clear guidance on how to make, document,
and assess ethical decisions in research when what is morally right or
acceptable is not clear-cut. In this work, we give an overview of the
discussion of ethical implications in current published work in computer
security by reviewing all 1154 top-tier security papers published in 2024,
finding inconsistent levels of ethics reporting with a strong focus of
reporting institutional or ethics board approval, human subjects protection,
and responsible disclosure, and a lack of discussion of balancing harms and
benefits. We further report on the results of a semi-structured interview study
with 24 computer security and privacy researchers (among whom were also:
reviewers, ethics committee members, and/or program chairs) and their ethical
decision-making both as authors and during peer review, finding a strong desire
for ethical research, but a lack of consistency in considered values, ethical
frameworks (if articulated), decision-making, and outcomes. We present an
overview of the current state of the discussion of ethics and current de-facto
standards in computer security research, and contribute suggestions to improve
the state of ethics in computer security research.

</details>


### [29] [ENSI: Efficient Non-Interactive Secure Inference for Large Language Models](https://arxiv.org/abs/2509.09424)
*Zhiyu He,Maojiang Wang,Xinwen Gao,Yuchuan Luo,Lin Liu,Shaojing Fu*

Main category: cs.CR

TL;DR: ENSI是一个新颖的非交互式安全推理框架，通过密码协议与LLM架构的协同设计，显著提升大语言模型的安全推理效率


<details>
  <summary>Details</summary>
Motivation: 密码协议与大规模语言模型集成存在显著挑战，协议复杂性和LLM参数规模限制了实际可用性

Method: 采用优化的编码策略整合CKKS方案与轻量级BitNet模型，集成sigmoid注意力机制替代softmax，在RMSNorm中嵌入Bootstrapping操作

Result: 矩阵乘法加速约8倍，softmax推理速度提升2.6倍，bootstrapping比例降至1%

Conclusion: ENSI框架通过架构协同设计有效解决了LLM安全推理的计算效率问题

Abstract: Secure inference enables privacy-preserving machine learning by leveraging
cryptographic protocols that support computations on sensitive user data
without exposing it. However, integrating cryptographic protocols with large
language models (LLMs) presents significant challenges, as the inherent
complexity of these protocols, together with LLMs' massive parameter scale and
sophisticated architectures, severely limits practical usability. In this work,
we propose ENSI, a novel non-interactive secure inference framework for LLMs,
based on the principle of co-designing the cryptographic protocols and LLM
architecture. ENSI employs an optimized encoding strategy that seamlessly
integrates CKKS scheme with a lightweight LLM variant, BitNet, significantly
reducing the computational complexity of encrypted matrix multiplications. In
response to the prohibitive computational demands of softmax under homomorphic
encryption (HE), we pioneer the integration of the sigmoid attention mechanism
with HE as a seamless, retraining-free alternative. Furthermore, by embedding
the Bootstrapping operation within the RMSNorm process, we efficiently refresh
ciphertexts while markedly decreasing the frequency of costly bootstrapping
invocations. Experimental evaluations demonstrate that ENSI achieves
approximately an 8x acceleration in matrix multiplications and a 2.6x speedup
in softmax inference on CPU compared to state-of-the-art method, with the
proportion of bootstrapping is reduced to just 1%.

</details>


### [30] [Prompt Pirates Need a Map: Stealing Seeds helps Stealing Prompts](https://arxiv.org/abs/2509.09488)
*Felix Mächtle,Ashwath Shetty,Jonas Sander,Nils Loose,Sören Pirk,Thomas Eisenbarth*

Main category: cs.CR

TL;DR: 本文揭示了扩散模型中基于数值优化的提示词恢复方法存在根本性限制，发现并利用了噪声生成漏洞(CWE-339)，开发了SeedSnitch种子恢复工具和PromptPirate提示词窃取方法，显著提升了攻击效果，并提出了有效的防御措施。


<details>
  <summary>Details</summary>
Motivation: 扩散模型生成的图像具有重要的知识产权和经济价值，提示词窃取对安全和隐私构成严重威胁。现有提示词恢复方法存在根本性技术限制，需要研究更有效的攻击方法和防御措施。

Method: 1) 识别PyTorch在CPU上生成初始随机噪声时种子值范围限制为2^32的漏洞；2) 开发SeedSnitch工具进行大规模种子暴力破解；3) 提出基于遗传算法的PromptPirate优化方法进行提示词窃取

Result: 1) 在CivitAI平台上约95%的图像种子可在140分钟内被破解；2) PromptPirate相比现有最佳方法(PromptStealer、P2HP、CLIP-Interrogator)在LPIPS相似度上提升8-11%；3) 提出了有效的防御措施使种子窃取和基于优化的提示词窃取失效

Conclusion: 扩散模型存在严重的提示词窃取安全漏洞，通过利用噪声生成限制可以高效恢复种子并窃取提示词。研究提出了有效的攻击方法和防御措施，已与开发者合作进行协调修复。

Abstract: Diffusion models have significantly advanced text-to-image generation,
enabling the creation of highly realistic images conditioned on textual prompts
and seeds. Given the considerable intellectual and economic value embedded in
such prompts, prompt theft poses a critical security and privacy concern. In
this paper, we investigate prompt-stealing attacks targeting diffusion models.
We reveal that numerical optimization-based prompt recovery methods are
fundamentally limited as they do not account for the initial random noise used
during image generation. We identify and exploit a noise-generation
vulnerability (CWE-339), prevalent in major image-generation frameworks,
originating from PyTorch's restriction of seed values to a range of $2^{32}$
when generating the initial random noise on CPUs. Through a large-scale
empirical analysis conducted on images shared via the popular platform CivitAI,
we demonstrate that approximately 95% of these images' seed values can be
effectively brute-forced in 140 minutes per seed using our seed-recovery tool,
SeedSnitch. Leveraging the recovered seed, we propose PromptPirate, a genetic
algorithm-based optimization method explicitly designed for prompt stealing.
PromptPirate surpasses state-of-the-art methods, i.e., PromptStealer, P2HP, and
CLIP-Interrogator, achieving an 8-11% improvement in LPIPS similarity.
Furthermore, we introduce straightforward and effective countermeasures that
render seed stealing, and thus optimization-based prompt stealing, ineffective.
We have disclosed our findings responsibly and initiated coordinated mitigation
efforts with the developers to address this critical vulnerability.

</details>


### [31] [What Does Normal Even Mean? Evaluating Benign Traffic in Intrusion Detection Datasets](https://arxiv.org/abs/2509.09564)
*Meghan Wilkinson,Robert H Thomson*

Main category: cs.CR

TL;DR: 本文分析了几个常用入侵检测数据集中良性流量的结构，发现它们包含有意义的子类别，并使用无监督聚类技术来提高多类分类性能。


<details>
  <summary>Details</summary>
Motivation: 监督学习技术依赖标签数据，但现有入侵检测数据集将所有非攻击流量归为单一大的良性类，这可能没有抓住数据结构中的有意义差异。

Method: 使用HDBSCAN、Mean Shift等无监督聚类技术分析NSL-KDD、UNSW-NB15和CIC-IDS 2017数据集中良性流量的结构，识别其中的子类别。

Result: 研究发现良性流量中存在有意义的子类别，这些子类别可以提高多类分类算法的性能。

Conclusion: 不应将入侵检测数据集中的良性流量简单地归为单一类别，而应使用无监督聚类技术识别其中的子结构以获得更好的分类效果。

Abstract: Supervised machine learning techniques rely on labeled data to achieve high
task performance, but this requires the labels to capture some meaningful
differences in the underlying data structure. For training network intrusion
detection algorithms, most datasets contain a series of attack classes and a
single large benign class which captures all non-attack network traffic. A
review of intrusion detection papers and guides that explicitly state their
data preprocessing steps identified that the majority took the labeled
categories of the dataset at face value when training their algorithms. The
present paper evaluates the structure of benign traffic in several common
intrusion detection datasets (NSL-KDD, UNSW-NB15, and CIC-IDS 2017) and
determines whether there are meaningful sub-categories within this traffic
which may improve overall multi-classification performance using common machine
learning techniques. We present an overview of some unsupervised clustering
techniques (e.g., HDBSCAN, Mean Shift Clustering) and show how they
differentially cluster the benign traffic space.

</details>


### [32] [Bridging the Gap in Phishing Detection: A Comprehensive Phishing Dataset Collector](https://arxiv.org/abs/2509.09592)
*Aditya Kulkarni,Shahil Manishbhai Patel,Shivam Pradip Tirmare,Vivek Balachandran,Tamal Das*

Main category: cs.CR

TL;DR: 这篇论文提出了一种新的资源收集工具，用于采集浏览器资源、截图等该资源以构建暴力浏览器数据集，解决现有数据集中资源不全面、偏差较大的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的浏览器数据集库很少，且没有一个库能够全面整合URL、源代码、截图和相关资源。数据集中资源的不全面导致模型偏差，影响浏览器检测效果。

Method: 设计了一种资源收集工具，利用PhishTank作为主要浏览器URL来源，收集CSS、Javascript、favicons、图片和截图等多种资源，比PyWebCopy库收集更多资源类型。

Result: 使用该工具生成了包含4,056个合法URL和5,666个浏览器URL的样本数据集，包含相关资源。识别出了数据集中与浏览器标签最相关的特征。

Conclusion: 该工具提供了全面的资源集，能够帮助研究人员开发更有效的浏览器检测方法，解决了现有数据集资源不全面的问题。

Abstract: To combat phishing attacks -- aimed at luring web users to divulge their
sensitive information -- various phishing detection approaches have been
proposed. As attackers focus on devising new tactics to bypass existing
detection solutions, researchers have adapted by integrating machine learning
and deep learning into phishing detection. Phishing dataset collection is vital
to developing effective phishing detection approaches, which highly depend on
the diversity of the gathered datasets. The lack of diversity in the dataset
results in a biased model. Since phishing websites are often short-lived,
collecting them is also a challenge. Consequently, very few phishing webpage
dataset repositories exist to date. No single repository comprehensively
consolidates all phishing elements corresponding to a phishing webpage, namely,
URL, webpage source code, screenshot, and related webpage resources. This paper
introduces a resource collection tool designed to gather various resources
associated with a URL, such as CSS, Javascript, favicons, webpage images, and
screenshots. Our tool leverages PhishTank as the primary source for obtaining
active phishing URLs. Our tool fetches several additional webpage resources
compared to PyWebCopy Python library, which provides webpage content for a
given URL. Additionally, we share a sample dataset generated using our tool
comprising 4,056 legitimate and 5,666 phishing URLs along with their associated
resources. We also remark on the top correlated phishing features with their
associated class label found in our dataset. Our tool offers a comprehensive
resource set that can aid researchers in developing effective phishing
detection approaches.

</details>


### [33] [CryptoGuard: An AI-Based Cryptojacking Detection Dashboard Prototype](https://arxiv.org/abs/2509.09638)
*Amitabh Chakravorty,Jess Kropczynski,Nelly Elsayed*

Main category: cs.CR

TL;DR: 提出了CryptoGuard AI安全仪表板前端原型，通过用户中心设计帮助加密货币钱包用户监控登录和交易活动，检测可疑行为并采取行动


<details>
  <summary>Details</summary>
Motivation: 随着加密货币的广泛采用，加密劫持已成为钱包用户的重要安全威胁，需要开发用户友好的安全工具

Method: 采用用户中心设计流程，基于Figma构建高保真可点击原型，模拟关键用户交互，优先考虑非技术用户的直观体验

Result: 开发了具有可视化警报和报告功能的概念原型，虽然AI功能是概念性的，但展示了风险沟通和用户赋能的界面设计

Conclusion: 实用的安全工具不仅需要强大的后端功能，还需要用户中心的设计来有效沟通风险并支持用户在真实威胁下快速决策

Abstract: With the widespread adoption of cryptocurrencies, cryptojacking has become a
significant security threat to crypto wallet users. This paper presents a
front-end prototype of an AI-powered security dashboard, namely, CryptoGuard.
Developed through a user-centered design process, the prototype was constructed
as a high-fidelity, click-through model from Figma mockups to simulate key user
interactions. It is designed to assist users in monitoring their login and
transaction activity, identifying any suspicious behavior, and enabling them to
take action directly within the wallet interface. The dashboard is designed for
a general audience, prioritizing an intuitive user experience for non-technical
individuals. Although its AI functionality is conceptual, the prototype
demonstrates features like visual alerts and reporting. This work is positioned
explicitly as a design concept, bridging cryptojacking detection research with
human-centered interface design. This paper also demonstrates how usability
heuristics can directly inform a tool's ability to support rapid and confident
decision-making under real-world threats. This paper argues that practical
security tools require not only robust backend functionality but also a
user-centric design that communicates risk and empowers users to take
meaningful action.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [34] [An Interval Type-2 Version of Bayes Theorem Derived from Interval Probability Range Estimates Provided by Subject Matter Experts](https://arxiv.org/abs/2509.08834)
*John T. Rickard,William A. Dembski,James Rickards*

Main category: cs.AI

TL;DR: 本文提出了区间二型贝叶斯定理，通过保守方法处理输入区间的不一致性，并开发了将专家提供的区间编码为二型模糊隶属函数的新算法。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯推断假设精确输入值，但现实应用中专家通常提供区间范围估计，需要扩展贝叶斯定理来处理区间不确定性。

Method: 开发了区间二型贝叶斯定理，采用保守方法避免输入不一致性；提出了将专家区间编码为二型模糊隶属函数的新算法。

Result: 成功扩展了贝叶斯定理到区间二型版本，能够处理区间输入概率，避免了传统方法可能产生的无效输出结果。

Conclusion: 提出的区间二型贝叶斯定理和编码算法为处理现实世界中的不确定性提供了更实用的框架，扩展了贝叶斯推断的应用范围。

Abstract: Bayesian inference is widely used in many different fields to test hypotheses
against observations. In most such applications, an assumption is made of
precise input values to produce a precise output value. However, this is
unrealistic for real-world applications. Often the best available information
from subject matter experts (SMEs) in a given field is interval range estimates
of the input probabilities involved in Bayes Theorem. This paper provides two
key contributions to extend Bayes Theorem to an interval type-2 (IT2) version.
First, we develop an IT2 version of Bayes Theorem that uses a novel and
conservative method to avoid potential inconsistencies in the input IT2 MFs
that otherwise might produce invalid output results. We then describe a novel
and flexible algorithm for encoding SME-provided intervals into IT2 fuzzy
membership functions (MFs), which we can use to specify the input probabilities
in Bayes Theorem. Our algorithm generalizes and extends previous work on this
problem that primarily addressed the encoding of intervals into word MFs for
Computing with Words applications.

</details>


### [35] [Automated Unity Game Template Generation from GDDs via NLP and Multi-Modal LLMs](https://arxiv.org/abs/2509.08847)
*Amna Hassan*

Main category: cs.AI

TL;DR: 一种基于NLP和多模态LLM的自动化游戏模板生成框架，能够将游戏设计文档转换为Unity游戏原型，显著提高了代码生成质量和设计遵循度。


<details>
  <summary>Details</summary>
Motivation: 解决AI辅助游戏开发中从设计到实现的转换问题，简化游戏开发流程，提高生产力。

Method: 使用细调的LLaMA-3模型专门用于Unity代码生成，结合自定制Unity集成包，构建了一个从解析GDD到生成Unity C#代码的端到端系统。

Result: 在编译成功率、GDD遵循度、最佳实践采用和代码模块性指标上显著优于基线模型，平均得分4.8/5.0分，多游戏类型都表现出高度遵循设计规范。

Conclusion: 该系统有效填补了AI辅助游戏开发的关键空白，使LLM成为流程化从游戏设计到实现过程的价值工具。

Abstract: This paper presents a novel framework for automated game template generation
by transforming Game Design Documents (GDDs) into functional Unity game
prototypes using Natural Language Processing (NLP) and multi-modal Large
Language Models (LLMs). We introduce an end-to-end system that parses GDDs,
extracts structured game specifications, and synthesizes Unity-compatible C#
code that implements the core mechanics, systems, and architecture defined in
the design documentation. Our approach combines a fine-tuned LLaMA-3 model
specialized for Unity code generation with a custom Unity integration package
that streamlines the implementation process. Evaluation results demonstrate
significant improvements over baseline models, with our fine-tuned model
achieving superior performance (4.8/5.0 average score) compared to
state-of-the-art LLMs across compilation success, GDD adherence, best practices
adoption, and code modularity metrics. The generated templates demonstrate high
adherence to GDD specifications across multiple game genres. Our system
effectively addresses critical gaps in AI-assisted game development,
positioning LLMs as valuable tools in streamlining the transition from game
design to implementation.

</details>


### [36] [Global Constraint LLM Agents for Text-to-Model Translation](https://arxiv.org/abs/2509.08970)
*Junyang Cai,Serdar Kadioglu,Bistra Dilkina*

Main category: cs.AI

TL;DR: 提出了一个基于多智能体LLM的框架，将MiniZinc建模任务按全局约束类型分解，每个智能体专门处理特定类型的约束检测和代码生成，最后通过组装智能体整合成完整模型。


<details>
  <summary>Details</summary>
Motivation: 自然语言描述优化或满足问题难以准确转换为MiniZinc模型，需要逻辑推理和约束编程专业知识，传统方法面临挑战。

Method: 采用多智能体方法：多个专门的LLM智能体按全局约束类型分解建模任务，每个智能体专注于检测和生成特定类别全局约束的代码，最后由组装智能体整合成完整MiniZinc模型。

Result: 初步实验显示，相比单次提示和思维链提示等基线方法，该框架在多个LLM上表现更好。

Conclusion: 通过将复杂问题分解为更小的子任务，该方法降低了整体复杂性，为自然语言到MiniZinc模型的自动转换提供了有效解决方案，并提出了未来改进的路线图。

Abstract: Natural language descriptions of optimization or satisfaction problems are
challenging to translate into correct MiniZinc models, as this process demands
both logical reasoning and constraint programming expertise. We introduce a
framework that addresses this challenge with an agentic approach: multiple
specialized large language model (LLM) agents decompose the modeling task by
global constraint type. Each agent is dedicated to detecting and generating
code for a specific class of global constraint, while a final assembler agent
integrates these constraint snippets into a complete MiniZinc model. By
dividing the problem into smaller, well-defined sub-tasks, each LLM handles a
simpler reasoning challenge, potentially reducing overall complexity. We
conduct initial experiments with several LLMs and show better performance
against baselines such as one-shot prompting and chain-of-thought prompting.
Finally, we outline a comprehensive roadmap for future work, highlighting
potential enhancements and directions for improvement.

</details>


### [37] [ForTIFAI: Fending Off Recursive Training Induced Failure for AI Models](https://arxiv.org/abs/2509.08972)
*Soheil Zibakhsh Shabgahi,Pedram Aghazadeh,Azalia Mirhosseini,Farinaz Koushanfar*

Main category: cs.AI

TL;DR: 本文提出了一种截断交叉熵（TCE）损失函数来缓解生成式AI模型在合成数据上重复训练导致的模型崩溃问题，通过降低对高置信度预测的权重来显著延迟模型崩溃。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI模型对合成数据的依赖增加，重复训练会导致模型崩溃现象，现有缓解策略有限。研究发现模型对其自生成数据的过度自信是崩溃的关键驱动因素。

Method: 提出置信度感知的损失函数Truncated Cross Entropy (TCE)，在训练过程中降低高置信度预测的权重，构建模型无关的框架将损失函数设计与模型崩溃缓解联系起来。

Result: TCE显著延迟了递归训练中的模型崩溃，可将模型崩溃前的保真度间隔延长2.3倍以上，且该方法在不同模态上具有通用性。

Conclusion: 损失函数设计为在合成数据时代保持生成模型质量提供了简单而强大的工具，置信度感知方法能有效缓解模型崩溃问题。

Abstract: The increasing reliance on generative AI models has accelerated the
generation rate of synthetic data, with some projections suggesting that most
available new data for training could be machine-generated by 2030. This shift
to a mainly synthetic content presents a critical challenge: repeated training
in synthetic data leads to a phenomenon known as model collapse, where model
performance degrades over generations of training, eventually rendering the
models ineffective. Although prior studies have explored the causes and
detection of model collapse, existing mitigation strategies remain limited.
  In this paper, we identify model overconfidence in their self-generated data
as a key driver of collapse. Building on this observation, we propose a
confidence-aware loss function that downweights high-confidence predictions
during training. We introduce a novel loss function we call Truncated Cross
Entropy (TCE). We demonstrate that TCE significantly delays model collapse in
recursive training.
  We provide a model-agnostic framework that links the loss function design to
model collapse mitigation and validate our approach both theoretically and
empirically, showing that it can extend the model's fidelity interval before
collapse by more than 2.3x. Finally, we show that our method generalizes across
modalities. These findings suggest that the design of loss functions provides a
simple yet powerful tool for preserving the quality of generative models in the
era of increasing synthetic data.

</details>


### [38] [Uncertainty Awareness and Trust in Explainable AI- On Trust Calibration using Local and Global Explanations](https://arxiv.org/abs/2509.08989)
*Carina Newen,Daniel Bodemer,Sonja Glantz,Emmanuel Müller,Magdalena Wischnewski,Lenka Schnaubert*

Main category: cs.AI

TL;DR: 本文研究可解释AI中的不确定性解释和全局解释，通过测试算法在信任校准方面的能力，探讨复杂但直观的可视化方法是否能提高用户满意度和可解释性。


<details>
  <summary>Details</summary>
Motivation: 尽管可解释AI(XAI)已被广泛研究，但不确定性解释和全局解释这两个领域相对较少受到关注。研究者希望构建XAI方案的通用指南，并特别关注这些被忽视的方面。

Method: 选择了一个能够同时涵盖不确定性、鲁棒性和全局XAI等多个概念的算法，测试其在信任校准方面的能力。同时检验了旨在提供更直观视觉理解的算法，尽管其理解复杂度较高。

Result: 研究发现，尽管算法本身可能比较复杂，但能够提供直观视觉理解的算法确实能够带来更高的用户满意度和更好的人类可解释性。

Conclusion: 在可解释AI设计中，即使算法复杂度较高，通过提供直观的可视化解释仍然能够有效提升用户体验和解释效果，这对构建XAI通用指南具有重要启示。

Abstract: Explainable AI has become a common term in the literature, scrutinized by
computer scientists and statisticians and highlighted by psychological or
philosophical researchers. One major effort many researchers tackle is
constructing general guidelines for XAI schemes, which we derived from our
study. While some areas of XAI are well studied, we focus on uncertainty
explanations and consider global explanations, which are often left out. We
chose an algorithm that covers various concepts simultaneously, such as
uncertainty, robustness, and global XAI, and tested its ability to calibrate
trust. We then checked whether an algorithm that aims to provide more of an
intuitive visual understanding, despite being complicated to understand, can
provide higher user satisfaction and human interpretability.

</details>


### [39] [Instructional Prompt Optimization for Few-Shot LLM-Based Recommendations on Cold-Start Users](https://arxiv.org/abs/2509.09066)
*Haowei Yang,Yushang Zhao,Sitao Min,Bo Su,Chao Yao,Wei Xu*

Main category: cs.AI

TL;DR: 本文提出了一种基于上下文条件提示的方法来优化大语言模型在冷启动推荐任务中的表现，通过最优样本注入和指令结构化显著提升了few-shot场景下的推荐精度。


<details>
  <summary>Details</summary>
Motivation: 解决推荐系统中冷启动用户因缺乏历史行为信息而导致推荐效果不佳的问题，探索如何通过提示工程优化大语言模型在低数据环境下的推荐性能。

Method: 采用上下文条件提示公式P(u, Ds)→R̂，其中u是冷启动用户画像，Ds是精选支持集，R̂是预测的项目排序列表。使用基于transformer的自回归大语言模型（BioGPT、LLaMA-2、GPT-4），结合token级对齐和嵌入空间正则化技术。

Result: 实验证明最优样本注入和指令结构化能显著提高precision@k和NDCG分数，提示组合不仅影响语法结构，还能直接控制注意力规模和解码器推理行为。

Conclusion: 基于提示的适配方法可以视为解决基于大语言模型的推荐系统中冷启动问题的一种有效途径，特别是在低数据设置下表现出色。

Abstract: The cold-start user issue further compromises the effectiveness of
recommender systems in limiting access to the historical behavioral
information. It is an effective pipeline to optimize instructional prompts on a
few-shot large language model (LLM) used in recommender tasks. We introduce a
context-conditioned prompt formulation method P(u,\ Ds)\ \rightarrow\
R\widehat, where u is a cold-start user profile, Ds is a curated support set,
and R\widehat is the predicted ranked list of items. Based on systematic
experimentation with transformer-based autoregressive LLMs (BioGPT, LLaMA-2,
GPT-4), we provide empirical evidence that optimal exemplar injection and
instruction structuring can significantly improve the precision@k and NDCG
scores of such models in low-data settings. The pipeline uses token-level
alignments and embedding space regularization with a greater semantic fidelity.
Our findings not only show that timely composition is not merely syntactic but
also functional as it is in direct control of attention scales and decoder
conduct through inference. This paper shows that prompt-based adaptation may be
considered one of the ways to address cold-start recommendation issues in
LLM-based pipelines.

</details>


### [40] [Understanding Economic Tradeoffs Between Human and AI Agents in Bargaining Games](https://arxiv.org/abs/2509.09071)
*Crystal Qian,Kehang Zhu,John Horton,Benjamin S. Manning,Vivian Tsai,James Wexler,Nithum Thain*

Main category: cs.AI

TL;DR: 本文比较了人类、大语言模型和贝叶斯统计模型在动态协商任务中的表现，发现虽然绩效相似但行为模式存在显著差异，指出仅考虑性能平等会忽视过程和对齐性的重要性。


<details>
  <summary>Details</summary>
Motivation: 随着协调任务趋向由自主组件执行，需要评估这些组件在动态多组件环境中的协商过程，而不仅仅是最终性能。传统统计模型和大语言模型各有优势，需要在相同条件下进行直接比较。

Method: 在动态协商设置中比较人类（N = 216）、LLMs（GPT-4o, Gemini 1.5 Pro）和贝叶斯组件的表现，捕获结果和行为动态。

Result: 贝叶斯组件通过敏锐优化获得最高绩效，但抱潜交易拒绝率高。人类和LLMs能达到类似总体绩效，但行为差异显著：LLMs偏向保守、透款式交易，人类则采用更多战略、冒险和公平导向行为。

Conclusion: 性能平等作为常见评测标准，可能潜藏了过程和对齐性方面的根本性差异，这些差异对于实际部署在真实世界协调任务中至关重要。

Abstract: Coordination tasks traditionally performed by humans are increasingly being
delegated to autonomous agents. As this pattern progresses, it becomes critical
to evaluate not only these agents' performance but also the processes through
which they negotiate in dynamic, multi-agent environments. Furthermore,
different agents exhibit distinct advantages: traditional statistical agents,
such as Bayesian models, may excel under well-specified conditions, whereas
large language models (LLMs) can generalize across contexts. In this work, we
compare humans (N = 216), LLMs (GPT-4o, Gemini 1.5 Pro), and Bayesian agents in
a dynamic negotiation setting that enables direct, identical-condition
comparisons across populations, capturing both outcomes and behavioral
dynamics. Bayesian agents extract the highest surplus through aggressive
optimization, at the cost of frequent trade rejections. Humans and LLMs can
achieve similar overall surplus, but through distinct behaviors: LLMs favor
conservative, concessionary trades with few rejections, while humans employ
more strategic, risk-taking, and fairness-oriented behaviors. Thus, we find
that performance parity -- a common benchmark in agent evaluation -- can
conceal fundamental differences in process and alignment, which are critical
for practical deployment in real-world coordination tasks.

</details>


### [41] [Anti-Money Laundering Machine Learning Pipelines; A Technical Analysis on Identifying High-risk Bank Clients with Supervised Learning](https://arxiv.org/abs/2509.09127)
*Khashayar Namdar,Pin-Chien Wang,Tushar Raju,Steven Zheng,Fiona Li,Safwat Tahmin Khan*

Main category: cs.AI

TL;DR: 本文提出了一种用于反洗钱风险识别的机器学习管道，在IMI大数据与人工智能竞赛中获得第二名，AUROC达到0.961。


<details>
  <summary>Details</summary>
Motivation: 金融机构的反洗钱措施是优先事项，机器学习在此领域具有巨大潜力。本文旨在开发一个系统化的ML管道来识别高风险银行客户。

Method: 采用16步设计和统计分析流程，构建SQLite数据库进行特征工程，连接预训练模型，并提供可解释AI模块分析特征重要性。

Result: 管道在包含195,789个客户ID的数据集上表现优异，平均AUROC为0.961（标准差0.005），在竞赛中获得第二名。

Conclusion: 提出的综合方法成功开发了稳健的ML管道，为金融机构的反洗钱风险识别提供了有效的解决方案。

Abstract: Anti-money laundering (AML) actions and measurements are among the priorities
of financial institutions, for which machine learning (ML) has shown to have a
high potential. In this paper, we propose a comprehensive and systematic
approach for developing ML pipelines to identify high-risk bank clients in a
dataset curated for Task 1 of the University of Toronto 2023-2024 Institute for
Management and Innovation (IMI) Big Data and Artificial Intelligence
Competition. The dataset included 195,789 customer IDs, and we employed a
16-step design and statistical analysis to ensure the final pipeline was
robust. We also framed the data in a SQLite database, developed SQL-based
feature engineering algorithms, connected our pre-trained model to the
database, and made it inference-ready, and provided explainable artificial
intelligence (XAI) modules to derive feature importance. Our pipeline achieved
a mean area under the receiver operating characteristic curve (AUROC) of 0.961
with a standard deviation (SD) of 0.005. The proposed pipeline achieved second
place in the competition.

</details>


### [42] [Mind Meets Space: Rethinking Agentic Spatial Intelligence from a Neuroscience-inspired Perspective](https://arxiv.org/abs/2509.09154)
*Bui Duc Manh,Soumyaratna Debnath,Zetong Zhang,Shriram Damodaran,Arvind Kumar,Yueyi Zhang,Lu Mi,Erik Cambria,Lin Wang*

Main category: cs.AI

TL;DR: 这篇论文提出了一种基于神经科学原理的计算框架，用于提升自主AI系统的空间推理能力，包含六个核心模块，以突破当前系统在无结构环境中的空间智能限制。


<details>
  <summary>Details</summary>
Motivation: 当前自主AI系统在空间推理能力上存在显著缺口，主要限于符号化和序列处理，而人类空间智能基于多感知觉、空间记忆和认知地图，能够在无结构环境中进行灵活的上下文感知决策。缩小这个差距对于推进自主空间智能至关重要。

Method: 从计算神经科学角度审视空间神经模型，提出了基于神经科学原理的新计算框架。该框架将核心生物功能映射到六个重要计算模块：生物受侵多模态感知、多感官整合、自我中心-客观中心转换、人工认知地图、空间记忆和空间推理。

Result: 论文建立了一个包含六个核心模块的综合计算框架，为虚拟和物理环境中的自主空间推理能力提供了全局视角。通过框架导向的分析，评估了现有方法与每个模块的相关性，识别了阻碍更神经科学基础空间推理模块发展的关键空白。

Conclusion: 该研究为自主空间智能领域提供了基于神经科学的视角和结构化路径，提出的计算框架有望推动空间推理能力在动态或无结构环境中的普适性发展，并在虚拟系统和体现系统（如机器人学）等应用领域具有广阔前景。

Abstract: Recent advances in agentic AI have led to systems capable of autonomous task
execution and language-based reasoning, yet their spatial reasoning abilities
remain limited and underexplored, largely constrained to symbolic and
sequential processing. In contrast, human spatial intelligence, rooted in
integrated multisensory perception, spatial memory, and cognitive maps, enables
flexible, context-aware decision-making in unstructured environments.
Therefore, bridging this gap is critical for advancing Agentic Spatial
Intelligence toward better interaction with the physical 3D world. To this end,
we first start from scrutinizing the spatial neural models as studied in
computational neuroscience, and accordingly introduce a novel computational
framework grounded in neuroscience principles. This framework maps core
biological functions to six essential computation modules: bio-inspired
multimodal sensing, multi-sensory integration, egocentric-allocentric
conversion, an artificial cognitive map, spatial memory, and spatial reasoning.
Together, these modules form a perspective landscape for agentic spatial
reasoning capability across both virtual and physical environments. On top, we
conduct a framework-guided analysis of recent methods, evaluating their
relevance to each module and identifying critical gaps that hinder the
development of more neuroscience-grounded spatial reasoning modules. We further
examine emerging benchmarks and datasets and explore potential application
domains ranging from virtual to embodied systems, such as robotics. Finally, we
outline potential research directions, emphasizing the promising roadmap that
can generalize spatial reasoning across dynamic or unstructured environments.
We hope this work will benefit the research community with a
neuroscience-grounded perspective and a structured pathway. Our project page
can be found at Github.

</details>


### [43] [ProgD: Progressive Multi-scale Decoding with Dynamic Graphs for Joint Multi-agent Motion Forecasting](https://arxiv.org/abs/2509.09210)
*Xing Gao,Zherui Huang,Weiyao Lin,Xiao Sun*

Main category: cs.AI

TL;DR: 提出ProgD方法，通过动态异构图建模和渐进多尺度解码策略，解决多智能体运动预测中交互关系动态演化的问题，在多个基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽略了多智能体交互关系的动态演化特性，无法有效捕捉未来场景中不断变化的社会交互，限制了预测准确性。

Method: 采用动态异构图进行场景建模，设计渐进式建模策略处理时空依赖关系，结合多尺度解码逐步消除未来运动的不确定性。

Result: 在INTERACTION多智能体预测基准中排名第一，在Argoverse 2多世界预测基准上也达到最优性能。

Conclusion: ProgD方法通过显式建模动态交互关系，有效提升了多智能体运动预测的准确性和一致性，为自动驾驶安全规划提供了更可靠的预测能力。

Abstract: Accurate motion prediction of surrounding agents is crucial for the safe
planning of autonomous vehicles. Recent advancements have extended prediction
techniques from individual agents to joint predictions of multiple interacting
agents, with various strategies to address complex interactions within future
motions of agents. However, these methods overlook the evolving nature of these
interactions. To address this limitation, we propose a novel progressive
multi-scale decoding strategy, termed ProgD, with the help of dynamic
heterogeneous graph-based scenario modeling. In particular, to explicitly and
comprehensively capture the evolving social interactions in future scenarios,
given their inherent uncertainty, we design a progressive modeling of scenarios
with dynamic heterogeneous graphs. With the unfolding of such dynamic
heterogeneous graphs, a factorized architecture is designed to process the
spatio-temporal dependencies within future scenarios and progressively
eliminate uncertainty in future motions of multiple agents. Furthermore, a
multi-scale decoding procedure is incorporated to improve on the future
scenario modeling and consistent prediction of agents' future motion. The
proposed ProgD achieves state-of-the-art performance on the INTERACTION
multi-agent prediction benchmark, ranking $1^{st}$, and the Argoverse 2
multi-world forecasting benchmark.

</details>


### [44] [Enabling Regulatory Multi-Agent Collaboration: Architecture, Challenges, and Solutions](https://arxiv.org/abs/2509.09215)
*Qinnan Hu,Yuntao Wang,Yuan Gao,Zhou Su,Linkang Du*

Main category: cs.AI

TL;DR: 基于区块链的多自治理架构，通过行为追踪、动态声誉评估和恶意行为预测三个核心模块，解决大语言模型自主组件的可靠性和可负责性挑战


<details>
  <summary>Details</summary>
Motivation: 大语言模型自主组件在金融、医疗、智能制造等领域带来重大机遇，但其不可预测行为和异构能力造成了重大的治理和可负责性挑战

Method: 提出一种区块链启用的层状监管自主组件协作架构，包含自主组件层、区块链数据层和监管应用层，设计了行为追踪仲裁、动态声誉评估和恶意行为预测三个核心模块

Result: 为大规模自主组件生态系统建立了可信、弹性和可扩展监管机制的系统性基础

Conclusion: 该方法有效解决了自主组件协作中的可靠性和可负责性问题，为多自主组件系统的区块链监管框架提供了未来研究方向

Abstract: Large language models (LLMs)-empowered autonomous agents are transforming
both digital and physical environments by enabling adaptive, multi-agent
collaboration. While these agents offer significant opportunities across
domains such as finance, healthcare, and smart manufacturing, their
unpredictable behaviors and heterogeneous capabilities pose substantial
governance and accountability challenges. In this paper, we propose a
blockchain-enabled layered architecture for regulatory agent collaboration,
comprising an agent layer, a blockchain data layer, and a regulatory
application layer. Within this framework, we design three key modules: (i) an
agent behavior tracing and arbitration module for automated accountability,
(ii) a dynamic reputation evaluation module for trust assessment in
collaborative scenarios, and (iii) a malicious behavior forecasting module for
early detection of adversarial activities. Our approach establishes a
systematic foundation for trustworthy, resilient, and scalable regulatory
mechanisms in large-scale agent ecosystems. Finally, we discuss the future
research directions for blockchain-enabled regulatory frameworks in multi-agent
systems.

</details>


### [45] [Jupiter: Enhancing LLM Data Analysis Capabilities via Notebook and Inference-Time Value-Guided Search](https://arxiv.org/abs/2509.09245)
*Shuocheng Li,Yihao Liu,Silin Du,Wenxuan Zeng,Zhe Xu,Mengyu Zhou,Yeye He,Haoyu Dong,Shi Han,Dongmei Zhang*

Main category: cs.AI

TL;DR: 通过从真实Jupyter笔记本提取高质量数据分析任务构建NbQA数据集，并使用MCTS搜索框架Jupiter提升LLM在数据科学中的多步推理和工具使用能力


<details>
  <summary>Details</summary>
Motivation: 解决现有大语言模型在数据科学工作流中多步推理和工具使用方面的困难，提高复杂数据分析任务的效果

Method: 构建标准化的NbQA数据集，开发Jupiter框架将数据分析模式化为搜索问题，使用蒙特卡罗树搜索(MCTS)生成多样化解决方案轨迹用于价值模型学习

Result: Qwen2.5-7B和14B-Instruct模型在NbQA上分别解决77.82%和86.38%的任务，跟GPT-4o和先进代理框架相当或更优，在多步推理任务中显示出更好的泛化能力和工具使用推理能力

Conclusion: 通过真实数据分析任务的标准化和MCTS搜索框架的结合，有效提升了LLM在数据科学工作流中的多步推理和工具使用能力

Abstract: Large language models (LLMs) have shown great promise in automating data
science workflows, but existing models still struggle with multi-step reasoning
and tool use, which limits their effectiveness on complex data analysis tasks.
To address this, we propose a scalable pipeline that extracts high-quality,
tool-based data analysis tasks and their executable multi-step solutions from
real-world Jupyter notebooks and associated data files. Using this pipeline, we
introduce NbQA, a large-scale dataset of standardized task-solution pairs that
reflect authentic tool-use patterns in practical data science scenarios. To
further enhance multi-step reasoning, we present Jupiter, a framework that
formulates data analysis as a search problem and applies Monte Carlo Tree
Search (MCTS) to generate diverse solution trajectories for value model
learning. During inference, Jupiter combines the value model and node visit
counts to efficiently collect executable multi-step plans with minimal search
steps. Experimental results show that Qwen2.5-7B and 14B-Instruct models on
NbQA solve 77.82% and 86.38% of tasks on InfiAgent-DABench,
respectively-matching or surpassing GPT-4o and advanced agent frameworks.
Further evaluations demonstrate improved generalization and stronger tool-use
reasoning across diverse multi-step reasoning tasks.

</details>


### [46] [Fusing Knowledge and Language: A Comparative Study of Knowledge Graph-Based Question Answering with LLMs](https://arxiv.org/abs/2509.09272)
*Vaibhav Chaudhary,Neha Soni,Narotam Singh,Amita Kapoor*

Main category: cs.AI

TL;DR: 这篇论文对比了三种知识图构建方法（spaCy、Stanford CoreNLP-OpenIE、GraphRAG）在问答系统中的表现，发现OpenIE覆盖最全面，GraphRAG推理能力最强。


<details>
  <summary>Details</summary>
Motivation: 传统RAG方法在处理复杂文本的主题性和整体理解方面有限，需要知识图来提升问答系统的深度分析能力。

Method: 进行技术对比研究，分析三种开源知识图构建方法的能力、发展状态和对LLM问答性能的影响。

Result: 实验结果显示OpenIE能生成最全面的三元组，而GraphRAG在推理能力方面表现最优。

Conclusion: 论文讨论了各方法的优缺点，并为知识图基于问答的未来改进提供了见解。

Abstract: Knowledge graphs, a powerful tool for structuring information through
relational triplets, have recently become the new front-runner in enhancing
question-answering systems. While traditional Retrieval Augmented Generation
(RAG) approaches are proficient in fact-based and local context-based
extraction from concise texts, they encounter limitations when addressing the
thematic and holistic understanding of complex, extensive texts, requiring a
deeper analysis of both text and context. This paper presents a comprehensive
technical comparative study of three different methodologies for constructing
knowledge graph triplets and integrating them with Large Language Models (LLMs)
for question answering: spaCy, Stanford CoreNLP-OpenIE, and GraphRAG, all
leveraging open source technologies. We evaluate the effectiveness,
feasibility, and adaptability of these methods by analyzing their capabilities,
state of development, and their impact on the performance of LLM-based question
answering. Experimental results indicate that while OpenIE provides the most
comprehensive coverage of triplets, GraphRAG demonstrates superior reasoning
abilities among the three. We conclude with a discussion on the strengths and
limitations of each method and provide insights into future directions for
improving knowledge graph-based question answering.

</details>


### [47] [Tree-OPO: Off-policy Monte Carlo Tree-Guided Advantage Optimization for Multistep Reasoning](https://arxiv.org/abs/2509.09284)
*Bingning Huang,Tu Nguyen,Matthieu Zimmer*

Main category: cs.AI

TL;DR: 将MCTS轨迹重新用于基于偏好的强化学习策略优化，提出分阶段GRPO训练范式，利用树结构优势估计改善策略学习效果


<details>
  <summary>Details</summary>
Motivation: 受MCTS在LLM推理中生成高质量中间轨迹的启发，探索如何将传统用于训练价值模型的MCTS轨迹重新用于改进基于偏好的强化学习策略优化

Method: 提出分阶段GRPO训练范式，使用部分揭示的MCTS rollout生成补全结果，引入新颖的树结构优势估计设置，产生前缀条件奖励信号

Result: 结构化优势估计可以稳定更新并更好地反映组合推理质量，但仍面临优势饱和和奖励信号崩溃等挑战

Conclusion: 提出了启发式和统计解决方案来缓解这些问题，并讨论了在分阶段或树状奖励结构下学习的开放挑战

Abstract: Recent advances in reasoning with large language models (LLMs) have shown the
effectiveness of Monte Carlo Tree Search (MCTS) for generating high-quality
intermediate trajectories, particularly in math and symbolic domains. Inspired
by this, we explore how MCTS-derived trajectories, traditionally used for
training value or reward models, can be repurposed to improve policy
optimization in preference-based reinforcement learning (RL). Specifically, we
focus on Group Relative Policy Optimization (GRPO), a recent algorithm that
enables preference-consistent policy learning without value networks. We
propose a staged GRPO training paradigm where completions are derived from
partially revealed MCTS rollouts, introducing a novel tree-structured setting
for advantage estimation. This leads to a rich class of prefix-conditioned
reward signals, which we analyze theoretically and empirically. Our initial
results indicate that while structured advantage estimation can stabilize
updates and better reflect compositional reasoning quality, challenges such as
advantage saturation and reward signal collapse remain. We propose heuristic
and statistical solutions to mitigate these issues and discuss open challenges
for learning under staged or tree-like reward structures.

</details>


### [48] [LightAgent: Production-level Open-source Agentic AI Framework](https://arxiv.org/abs/2509.09292)
*Weige Cai,Tong Zhu,Jinyi Niu,Ruiqi Hu,Lingyao Li,Tenglong Wang,Xiaowu Dai,Weining Shen,Liwen Zhang*

Main category: cs.AI

TL;DR: LightAgent是一个轻量级但功能强大的多智能体框架，解决了现有框架在灵活性和简单性之间的权衡问题，集成了记忆、工具和思维树等核心功能。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的快速发展，多智能体系统在各种应用场景中取得了显著进展，但在设计通用、鲁棒和高效的智能体部署平台方面仍存在重大挑战。

Method: 提出了LightAgent框架，集成Memory (mem0)、Tools和Tree of Thought (ToT)等核心功能，同时保持极轻量级结构，是一个完全开源的解决方案。

Result: 该框架能够无缝集成到主流聊天平台，使开发者能够轻松构建自学习智能体。

Conclusion: LightAgent有效解决了现有框架在灵活性和简单性之间的权衡问题，为多智能体系统提供了一个轻量级但功能强大的部署平台。

Abstract: With the rapid advancement of large language models (LLMs), Multi-agent
Systems (MAS) have achieved significant progress in various application
scenarios. However, substantial challenges remain in designing versatile,
robust, and efficient platforms for agent deployment. To address these
limitations, we propose \textbf{LightAgent}, a lightweight yet powerful agentic
framework, effectively resolving the trade-off between flexibility and
simplicity found in existing frameworks. LightAgent integrates core
functionalities such as Memory (mem0), Tools, and Tree of Thought (ToT), while
maintaining an extremely lightweight structure. As a fully open-source
solution, it seamlessly integrates with mainstream chat platforms, enabling
developers to easily build self-learning agents. We have released LightAgent at
\href{https://github.com/wxai-space/LightAgent}{https://github.com/wxai-space/LightAgent}

</details>


### [49] [Explaining Tournament Solutions with Minimal Supports](https://arxiv.org/abs/2509.09312)
*Clément Contet,Umberto Grandi,Jérôme Mengin*

Main category: cs.AI

TL;DR: 该论文研究如何为锦标赛规则下的获胜者提供认证解释，通过识别最小支持子锦标赛来解释为什么某个候选人是必然获胜者。


<details>
  <summary>Details</summary>
Motivation: 锦标赛模型广泛应用于表示候选者之间的成对优势关系，需要为获胜者提供形式化的可解释AI解释，回答"为什么获胜者会赢"的问题。

Method: 识别最小支持子锦标赛（候选人在其中是必然获胜者的最小子锦标赛），针对多种常见锦标赛规则（top cycle、uncovered set、Copeland、Borda、maximin、weighted uncovered set）进行分析。

Result: 确定了各种规则下最小支持子锦标赛的大小，提出了多项式时间算法来计算除weighted uncovered set外的所有规则的最小支持，后者是NP完全问题。

Conclusion: 最小支持子锦标赛能够提供紧凑、认证且直观的解释，为锦标赛获胜者的解释提供了有效的形式化框架。

Abstract: Tournaments are widely used models to represent pairwise dominance between
candidates, alternatives, or teams. We study the problem of providing certified
explanations for why a candidate appears among the winners under various
tournament rules. To this end, we identify minimal supports, minimal
sub-tournaments in which the candidate is guaranteed to win regardless of how
the rest of the tournament is completed (that is, the candidate is a necessary
winner of the sub-tournament). This notion corresponds to an abductive
explanation for the question,"Why does the winner win the tournament", a
central concept in formal explainable AI. We focus on common tournament
solutions: the top cycle, the uncovered set, the Copeland rule, the Borda rule,
the maximin rule, and the weighted uncovered set. For each rule we determine
the size of the smallest minimal supports, and we present polynomial-time
algorithms to compute them for all but the weighted uncovered set, for which
the problem is NP-complete. Finally, we show how minimal supports can serve to
produce compact, certified, and intuitive explanations.

</details>


### [50] [Measuring Implicit Spatial Coordination in Teams: Effects on Collective Intelligence and Performance](https://arxiv.org/abs/2509.09314)
*Thuy Ngoc Nguyen,Anita Williams Woolley,Cleotilde Gonzalez*

Main category: cs.AI

TL;DR: 研究探讨了空间协调的三个维度（探索多样性、移动专门化、适应性空间距离）在隐式协调任务中对团队表现的影响


<details>
  <summary>Details</summary>
Motivation: 传统研究主要关注同地或知识工作团队的隐式协调，而这项研究重点关注需要在物理空间中协调移动但缺乏视觉线索或明确沟通的团队（如消防员、军队等）

Method: 分析34个四人团队（136名参与者）在搜救任务中的数据，测量空间距离、分布模式和移动对齐等关系性团队工作指标

Result: 空间专门化正向预测表现，适应性空间距离呈边际倒U型关系（中等适应最优），高低表现团队在时间动态上存在显著差异

Conclusion: 研究揭示了角色基础团队中隐式空间协调的重要性，强调平衡的适应策略，对培训和AI辅助团队支持系统具有重要意义

Abstract: Coordinated teamwork is essential in fast-paced decision-making environments
that require dynamic adaptation, often without an opportunity for explicit
communication. Although implicit coordination has been extensively considered
in the existing literature, the majority of work has focused on co-located,
synchronous teamwork (such as sports teams) or, in distributed teams, primarily
on coordination of knowledge work. However, many teams (firefighters, military,
law enforcement, emergency response) must coordinate their movements in
physical space without the benefit of visual cues or extensive explicit
communication. This paper investigates how three dimensions of spatial
coordination, namely exploration diversity, movement specialization, and
adaptive spatial proximity, influence team performance in a collaborative
online search and rescue task where explicit communication is restricted and
team members rely on movement patterns to infer others' intentions and
coordinate actions. Our metrics capture the relational aspects of teamwork by
measuring spatial proximity, distribution patterns, and alignment of movements
within shared environments. We analyze data from 34 four-person teams (136
participants) assigned to specialized roles in a search and rescue task.
Results show that spatial specialization positively predicts performance, while
adaptive spatial proximity exhibits a marginal inverted U-shaped relationship,
suggesting moderate levels of adaptation are optimal. Furthermore, the temporal
dynamics of these metrics differentiate high- from low-performing teams over
time. These findings provide insights into implicit spatial coordination in
role-based teamwork and highlight the importance of balanced adaptive
strategies, with implications for training and AI-assisted team support
systems.

</details>


### [51] [Towards Adaptive ML Benchmarks: Web-Agent-Driven Construction, Domain Expansion, and Metric Optimization](https://arxiv.org/abs/2509.09321)
*Hangyi Jia,Yuxi Qian,Hanwen Tong,Xinhui Wu,Lin Chen,Feng Wei*

Main category: cs.AI

TL;DR: TAM Bench是一个用于评估基于LLM的端到端机器学习工作流代理的多样化、现实化基准测试，包含150个AutoML任务，具有浏览器自动化采集、基于排行榜的难度建模和多维度评估框架三大创新。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试在任务覆盖、领域多样性、难度建模和评估严谨性方面存在局限，无法充分评估LLM代理在真实场景中的端到端ML工作流自动化能力。

Method: 1) 基于浏览器自动化和LLM的任务采集系统从Kaggle等平台自动收集结构化ML挑战；2) 基于排行榜的难度建模机制使用参与人数和分数离散度估计任务复杂度；3) 包含性能、格式合规性、约束遵循和任务泛化能力的多维度评估框架。

Result: 构建了包含150个AutoML任务的基准测试，提供Lite(18任务)、Medium和Full三个不同规模的子集，其中Lite版本在模态和难度级别上具有平衡覆盖，适合日常基准测试和比较研究。

Conclusion: TAM Bench为解决现有基准测试局限性提供了多样化、现实化和结构化的解决方案，能够更好地评估LLM代理在端到端ML工作流自动化中的能力。

Abstract: Recent advances in large language models (LLMs) have enabled the emergence of
general-purpose agents for automating end-to-end machine learning (ML)
workflows, including data analysis, feature engineering, model training, and
competition solving. However, existing benchmarks remain limited in task
coverage, domain diversity, difficulty modeling, and evaluation rigor, failing
to capture the full capabilities of such agents in realistic settings. We
present TAM Bench, a diverse, realistic, and structured benchmark for
evaluating LLM-based agents on end-to-end ML tasks. TAM Bench features three
key innovations: (1) A browser automation and LLM-based task acquisition system
that automatically collects and structures ML challenges from platforms such as
Kaggle, AIcrowd, and Biendata, spanning multiple task types and data modalities
(e.g., tabular, text, image, graph, audio); (2) A leaderboard-driven difficulty
modeling mechanism that estimates task complexity using participant counts and
score dispersion, enabling scalable and objective task calibration; (3) A
multi-dimensional evaluation framework incorporating performance, format
compliance, constraint adherence, and task generalization. Based on 150 curated
AutoML tasks, we construct three benchmark subsets of different sizes -- Lite,
Medium, and Full -- designed for varying evaluation scenarios. The Lite
version, with 18 tasks and balanced coverage across modalities and difficulty
levels, serves as a practical testbed for daily benchmarking and comparative
studies.

</details>


### [52] [Curriculum-Based Multi-Tier Semantic Exploration via Deep Reinforcement Learning](https://arxiv.org/abs/2509.09356)
*Abdel Hakim Drid,Vincenzo Suriani,Daniele Nardi,Abderrezzak Debilou*

Main category: cs.AI

TL;DR: 一种新的深度强化学习架构，通过视觉-语言模型和层状奖励函数集成普通常识，实现资源效率的语义探索


<details>
  <summary>Details</summary>
Motivation: 解决传统强化学习方法在语义探索中效率与认知能力的平衡问题，提升体现代理的高级认知能力和环境理解能力

Method: 集成视觉-语言模型普通常识到层状奖励函数，将VLM查询模型化为专门动作，结合课程学习策略确保稳定学习

Result: 显著提高了物体发现率，学会了有效导航到语义丰富区域，掌握了战略性地使用外部环境信息查询

Conclusion: 提供了一种实用可扩展的方法，将普通常识语义推理嵌入自主代理，为完全智能自主探索提供新方向

Abstract: Navigating and understanding complex and unknown environments autonomously
demands more than just basic perception and movement from embodied agents.
Truly effective exploration requires agents to possess higher-level cognitive
abilities, the ability to reason about their surroundings, and make more
informed decisions regarding exploration strategies. However, traditional RL
approaches struggle to balance efficient exploration and semantic understanding
due to limited cognitive capabilities embedded in the small policies for the
agents, leading often to human drivers when dealing with semantic exploration.
In this paper, we address this challenge by presenting a novel Deep
Reinforcement Learning (DRL) architecture that is specifically designed for
resource efficient semantic exploration. A key methodological contribution is
the integration of a Vision-Language Model (VLM) common-sense through a layered
reward function. The VLM query is modeled as a dedicated action, allowing the
agent to strategically query the VLM only when deemed necessary for gaining
external guidance, thereby conserving resources. This mechanism is combined
with a curriculum learning strategy designed to guide learning at different
levels of complexity to ensure robust and stable learning. Our experimental
evaluation results convincingly demonstrate that our agent achieves
significantly enhanced object discovery rates and develops a learned capability
to effectively navigate towards semantically rich regions. Furthermore, it also
shows a strategic mastery of when to prompt for external environmental
information. By demonstrating a practical and scalable method for embedding
common-sense semantic reasoning with autonomous agents, this research provides
a novel approach to pursuing a fully intelligent and self-guided exploration in
robotics.

</details>


### [53] [TORSO: Template-Oriented Reasoning Towards General Tasks](https://arxiv.org/abs/2509.09448)
*Minhyuk Kim,Seungyoon Lee,Heuiseok Lim*

Main category: cs.AI

TL;DR: TORSO方法通过引导LLM利用内部推理能力生成响应，无需手动构建few-shot示例，在多个基准测试中取得优异性能


<details>
  <summary>Details</summary>
Motivation: 现有few-shot提示方法过度依赖提供示例，限制了模型内在推理能力的发挥，且任务特定的提示构建成本高且存在不一致性问题

Method: 提出Template-Oriented Reasoning (TORSO)方法，激发模型利用内部推理能力生成适当响应，无需手动构建few-shot示例

Result: 实验结果表明TORSO在多样化LLM基准测试中实现了强大性能，并生成合理的推理过程

Conclusion: TORSO提供了一种有效的方法来引导LLM发挥其内在推理能力，无需依赖成本高昂的手动few-shot提示构建

Abstract: The approaches that guide Large Language Models (LLMs) to emulate human
reasoning during response generation have emerged as an effective method for
enabling them to solve complex problems in a step-by-step manner, thereby
achieving superior performance. However, most existing approaches using
few-shot prompts to generate responses heavily depend on the provided examples,
limiting the utilization of the model's inherent reasoning capabilities.
Moreover, constructing task-specific few-shot prompts is often costly and may
lead to inconsistencies across different tasks. In this work, we introduce
Template-Oriented Reasoning (TORSO), which elicits the model to utilize
internal reasoning abilities to generate proper responses across various tasks
without the need for manually crafted few-shot examples. Our experimental
results demonstrate that TORSO achieves strong performance on diverse LLMs
benchmarks with reasonable rationales.

</details>


### [54] [Inteligencia Artificial jurídica y el desafío de la veracidad: análisis de alucinaciones, optimización de RAG y principios para una integración responsable](https://arxiv.org/abs/2509.09467)
*Alex Dantart*

Main category: cs.AI

TL;DR: 论文分析了大语言模型在法律领域中的幻觉问题，考察了RAG策略的效果和局限性，建议采用以验证性和可追溯性为核心的"咨询式AI"范式


<details>
  <summary>Details</summary>
Motivation: 解决LLM在法律应用中产生假信息（幻觉）的挑战，探索有效的缓解策略和优化方案

Method: 分析幻觉现象的原因和表现形式，评估RAG（查询扩展生成）策略的效果和局限性，提出整体优化建议

Result: 发现RAG策略存在局限性，需要更全面的优化方案，人类监督在法律应用中具有无可替代的作用

Conclusion: 解决方案不在于氐次改进生成模型，而是应采用"咨询式AI"范式，以验证性和可追溯性为核心，作为扩大专业判断的工具而非替代人类判断

Abstract: This technical report analyzes the challenge of "hallucinations" (false
information) in LLMs applied to law. It examines their causes, manifestations,
and the effectiveness of the RAG mitigation strategy, highlighting its
limitations and proposing holistic optimizations. The paper explores the
ethical and regulatory implications, emphasizing human oversight as an
irreplaceable role. It concludes that the solution lies not in incrementally
improving generative models, but in adopting a "consultative" AI paradigm that
prioritizes veracity and traceability, acting as a tool to amplify, not
replace, professional judgment.
  --
  Este informe t\'ecnico analiza el desaf\'io de las "alucinaciones"
(informaci\'on falsa) en los LLMs aplicados al derecho. Se examinan sus causas,
manifestaciones y la efectividad de la estrategia de mitigaci\'on RAG,
exponiendo sus limitaciones y proponiendo optimizaciones hol\'isticas. Se
exploran las implicaciones \'eticas y regulatorias, enfatizando la
supervisi\'on humana como un rol insustituible. El documento concluye que la
soluci\'on no reside en mejorar incrementalmente los modelos generativos, sino
en adoptar un paradigma de IA "consultiva" que priorice la veracidad y la
trazabilidad, actuando como una herramienta para amplificar, y no sustituir, el
juicio profesional.

</details>


### [55] [SEDM: Scalable Self-Evolving Distributed Memory for Agents](https://arxiv.org/abs/2509.09498)
*Haoran Xu,Jiacong Hu,Ke Zhang,Lei Yu,Yuxin Tang,Xinyuan Song,Yiqun Duan,Lynn Ai,Bill Shi*

Main category: cs.AI

TL;DR: SEDM是一个自演进分布式内存框架，通过可验证写入、动态内存调度和跨域知识扩散来解决多智能体系统中的内存管理问题，提高推理准确性并降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 长期多智能体系统产生大量轨迹和交互数据，现有内存管理方法存在噪声累积、内存膨胀和跨域泛化能力有限等问题，需要更高效的内存管理机制。

Method: SEDM框架包含三个核心组件：基于可重现回放的可验证写入准入、根据经验效用动态排序和整合条目的自调度内存控制器、以及抽象可重用见解支持跨异构任务迁移的跨域知识扩散。

Result: 在基准数据集上的评估显示，SEDM相比强基线方法提高了推理准确性，同时减少了token开销，并且能够将从事实验证中提取的知识用于增强多跳推理能力。

Conclusion: SEDM为开放式多智能体协作提供了一个可扩展且可持续的内存机制，将内存从被动存储库转变为主动自优化的组件。

Abstract: Long-term multi-agent systems inevitably generate vast amounts of
trajectories and historical interactions, which makes efficient memory
management essential for both performance and scalability. Existing methods
typically depend on vector retrieval and hierarchical storage, yet they are
prone to noise accumulation, uncontrolled memory expansion, and limited
generalization across domains. To address these challenges, we present SEDM,
Self-Evolving Distributed Memory, a verifiable and adaptive framework that
transforms memory from a passive repository into an active, self-optimizing
component. SEDM integrates verifiable write admission based on reproducible
replay, a self-scheduling memory controller that dynamically ranks and
consolidates entries according to empirical utility, and cross-domain knowledge
diffusion that abstracts reusable insights to support transfer across
heterogeneous tasks. Evaluations on benchmark datasets demonstrate that SEDM
improves reasoning accuracy while reducing token overhead compared with strong
memory baselines, and further enables knowledge distilled from fact
verification to enhance multi-hop reasoning. The results highlight SEDM as a
scalable and sustainable memory mechanism for open-ended multi-agent
collaboration. The code will be released in the later stage of this project.

</details>


### [56] [Compositional Concept Generalization with Variational Quantum Circuits](https://arxiv.org/abs/2509.09541)
*Hala Hawashin,Mina Abbaszadeh,Nicholas Joseph,Beth Pearson,Martha Lewis,Mehrnoosh sadrzadeh*

Main category: cs.AI

TL;DR: 量子模型在组合泛化任务中表现优于经典组合模型，特别是在使用多热编码时取得了良好的概念验证结果


<details>
  <summary>Details</summary>
Motivation: 组合泛化是人类认知的关键能力，但当前AI工具如视觉语言模型缺乏这种能力。量子模型训练效率更高，有望提升组合泛化性能

Method: 在希尔伯特空间中解释组合张量模型的表示，使用变分量子电路学习这些表示。采用两种图像编码技术：多热编码和CLIP模型的角/幅度编码

Result: 使用噪声多热编码获得了良好的概念验证结果。在CLIP图像向量上的表现较为复杂，但仍优于经典组合模型

Conclusion: 量子模型在组合泛化任务中展现出潜力，特别是在特定编码方式下能够超越经典方法，为量子AI在组合推理方面的应用提供了初步证据

Abstract: Compositional generalization is a key facet of human cognition, but lacking
in current AI tools such as vision-language models. Previous work examined
whether a compositional tensor-based sentence semantics can overcome the
challenge, but led to negative results. We conjecture that the increased
training efficiency of quantum models will improve performance in these tasks.
We interpret the representations of compositional tensor-based models in
Hilbert spaces and train Variational Quantum Circuits to learn these
representations on an image captioning task requiring compositional
generalization. We used two image encoding techniques: a multi-hot encoding
(MHE) on binary image vectors and an angle/amplitude encoding on image vectors
taken from the vision-language model CLIP. We achieve good proof-of-concept
results using noisy MHE encodings. Performance on CLIP image vectors was more
mixed, but still outperformed classical compositional models.

</details>


### [57] [Boosting Embodied AI Agents through Perception-Generation Disaggregation and Asynchronous Pipeline Execution](https://arxiv.org/abs/2509.09560)
*Shulai Zhang,Ao Xu,Quan Chen,Han Zhao,Weihao Cui,Ningxin Zheng,Haibin Lin,Xin Liu,Minyi Guo*

Main category: cs.AI

TL;DR: Auras是一个算法-系统协同设计的推理框架，通过解耦感知和生成模块并提供受控的流水线并行，显著提升具身AI系统的推理频率和吞吐量，同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 具身AI系统在动态环境中需要处理高频输入输出需求，传统顺序计算模式在保证准确性的同时难以达到实际应用所需的"思考"频率。

Method: Auras框架解耦感知和生成模块，提供受控的流水线并行机制，并建立共享的公共上下文来解决数据陈旧性问题。

Result: 实验结果显示Auras平均提升2.54倍吞吐量，同时达到原始准确性的102.7%。

Conclusion: Auras有效克服了顺序计算的限制，为具身AI系统提供了高吞吐量的推理能力。

Abstract: Embodied AI systems operate in dynamic environments, requiring seamless
integration of perception and generation modules to process high-frequency
input and output demands. Traditional sequential computation patterns, while
effective in ensuring accuracy, face significant limitations in achieving the
necessary "thinking" frequency for real-world applications. In this work, we
present Auras, an algorithm-system co-designed inference framework to optimize
the inference frequency of embodied AI agents. Auras disaggregates the
perception and generation and provides controlled pipeline parallelism for them
to achieve high and stable throughput. Faced with the data staleness problem
that appears when the parallelism is increased, Auras establishes a public
context for perception and generation to share, thereby promising the accuracy
of embodied agents. Experimental results show that Auras improves throughput by
2.54x on average while achieving 102.7% of the original accuracy, demonstrating
its efficacy in overcoming the constraints of sequential computation and
providing high throughput.

</details>


### [58] [The Illusion of Diminishing Returns: Measuring Long Horizon Execution in LLMs](https://arxiv.org/abs/2509.09677)
*Akshit Sinha,Arvindh Arun,Shashwat Goel,Steffen Staab,Jonas Geiping*

Main category: cs.AI

TL;DR: 研究发现大语言模型在长任务执行中存在自我条件效应，模型规模扩大并不能解决执行错误累积问题，而思维模型能显著提升长任务执行能力


<details>
  <summary>Details</summary>
Motivation: 探讨大语言模型规模扩展是否带来边际收益递减，以及为什么模型在简单任务变长时会失败，重点关注执行能力而非推理能力

Method: 通过提供明确的知识和计划来隔离执行能力，测试不同规模模型在多步任务中的表现，分析错误模式和自我条件效应

Result: 大模型能正确执行更多步骤，但单步准确率随步骤增加而下降；发现自我条件效应（模型在包含先前错误的上下文中更容易犯错）；思维模型不会自我条件且能执行更长任务

Conclusion: 关注执行能力可以解释LLM在复杂推理和简单长任务中的表现差异，模型规模和顺序测试时计算对长视野任务有巨大益处

Abstract: Does continued scaling of large language models (LLMs) yield diminishing
returns? Real-world value often stems from the length of task an agent can
complete. We start this work by observing the simple but counterintuitive fact
that marginal gains in single-step accuracy can compound into exponential
improvements in the length of a task a model can successfully complete. Then,
we argue that failures of LLMs when simple tasks are made longer arise from
mistakes in execution, rather than an inability to reason. We propose isolating
execution capability, by explicitly providing the knowledge and plan needed to
solve a long-horizon task. We find that larger models can correctly execute
significantly more turns even when small models have 100\% single-turn
accuracy. We observe that the per-step accuracy of models degrades as the
number of steps increases. This is not just due to long-context limitations --
curiously, we observe a self-conditioning effect -- models become more likely
to make mistakes when the context contains their errors from prior turns.
Self-conditioning does not reduce by just scaling the model size. In contrast,
recent thinking models do not self-condition, and can also execute much longer
tasks in a single turn. We conclude by benchmarking frontier thinking models on
the length of task they can execute in a single turn. Overall, by focusing on
the ability to execute, we hope to reconcile debates on how LLMs can solve
complex reasoning problems yet fail at simple tasks when made longer, and
highlight the massive benefits of scaling model size and sequential test-time
compute for long-horizon tasks.

</details>
