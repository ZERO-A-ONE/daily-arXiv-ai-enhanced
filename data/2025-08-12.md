<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 15]
- [cs.CR](#cs.CR) [Total: 20]
- [cs.AI](#cs.AI) [Total: 65]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Refactoring-Aware Patch Integration Across Structurally Divergent Java Forks](https://arxiv.org/abs/2508.06718)
*Daniel Ogenrwot,John Businge*

Main category: cs.SE

TL;DR: 论文研究了GitHub上长期分叉（变体）的补丁集成问题，提出了RePatch系统，能够通过语义对齐成功集成更多补丁。


<details>
  <summary>Details</summary>
Motivation: 长期分叉的代码变体因结构漂移（如重构）导致补丁集成困难，现有工具（如Git cherry-pick）失败率高。

Method: 提出RePatch系统，扩展RefMerge框架，支持非对称补丁传输，通过反转重构对齐补丁上下文。

Result: 在478个补丁请求中，Git cherry-pick失败率64.4%，而RePatch成功集成了52.8%的失败案例。

Conclusion: 基于语义推理的RePatch优于语法工具，解决了变体补丁传播的挑战。

Abstract: While most forks on platforms like GitHub are short-lived and used for social
collaboration, a smaller but impactful subset evolve into long-lived forks,
referred to here as variants, that maintain independent development
trajectories. Integrating bug-fix patches across such divergent variants poses
challenges due to structural drift, including refactorings that rename,
relocate, or reorganize code elements and obscure semantic correspondence. This
paper presents an empirical study of patch integration failures in 14 divergent
pair of variants and introduces RePatch, a refactoring-aware integration system
for Java repositories. RePatch extends the RefMerge framework, originally
designed for symmetric merges, by supporting asymmetric patch transfer. RePatch
inverts refactorings in both the source and target to realign the patch
context, applies the patch, and replays the transformations to preserve the
intent of the variant. In our evaluation of 478 bug-fix pull requests, Git
cherry-pick fails in 64.4% of cases due to structural misalignments, while
RePatch successfully integrates 52.8% of the previously failing patches. These
results highlight the limitations of syntax-based tools and the need for
semantic reasoning in variant-aware patch propagation.

</details>


### [2] [Quo Vadis, Code Review? Exploring the Future of Code Review](https://arxiv.org/abs/2508.06879)
*Michael Dorner,Andreas Bauer,Darja Šmite,Lukas Thode,Daniel Mendez,Ricardo Britto,Stephan Lukasczyk,Ehsan Zabardast,Michael Kormann*

Main category: cs.SE

TL;DR: 研究探讨了当前开发者对代码审查的反思及未来预期变化，并分析了这些变化对代码审查长期发展的潜在风险。


<details>
  <summary>Details</summary>
Motivation: 了解开发者对代码审查的现状反思及未来预期，以评估其对协作软件工程的影响。

Method: 通过调研和分析开发者的观点，探讨代码审查的现状和未来趋势。

Result: 识别了代码审查未来可能的变化及其潜在风险。

Conclusion: 代码审查的未来变化可能带来长期风险，需谨慎评估其对协作软件工程的影响。

Abstract: Code review has long been a core practice in collaborative software
engineering. In this research, we explore how practitioners reflect on code
review today and what changes they anticipate in the near future. We then
discuss the potential long-term risks of these anticipated changes for the
evolution of code review and its role in collaborative software engineering.

</details>


### [3] [Multi-Modal Requirements Data-based Acceptance Criteria Generation using LLMs](https://arxiv.org/abs/2508.06888)
*Fanyu Wang,Chetan Arora,Yonghui Liu,Kaicheng Huang,Chakkrit Tantithamthavorn,Aldeida Aleti,Dishan Sambathkumar,David Lo*

Main category: cs.SE

TL;DR: 论文提出了一种名为RAGcceptance M2RE的新方法，利用检索增强生成（RAG）从多模态需求数据中生成验收标准（ACs），显著提升了ACs的相关性、正确性和可理解性。


<details>
  <summary>Details</summary>
Motivation: 手动创建准确、全面且无歧义的验收标准在用户界面密集型应用中具有挑战性，因为依赖领域知识和视觉上下文，而文本需求往往无法完全捕捉这些信息。

Method: 采用检索增强生成（RAG）技术，结合文本文档和视觉UI信息的多模态数据，生成验收标准。

Result: 工业案例研究表明，多模态信息集成显著提升了生成ACs的质量，减少了人工工作量，并捕捉了容易被忽视的利益相关者意图。

Conclusion: 多模态RAG技术在优化软件验证流程和提高开发效率方面具有显著潜力，研究还公开了实现和数据集。

Abstract: Acceptance criteria (ACs) play a critical role in software development by
clearly defining the conditions under which a software feature satisfies
stakeholder expectations. However, manually creating accurate, comprehensive,
and unambiguous acceptance criteria is challenging, particularly in user
interface-intensive applications, due to the reliance on domain-specific
knowledge and visual context that is not always captured by textual
requirements alone. To address these challenges, we propose RAGcceptance M2RE,
a novel approach that leverages Retrieval-Augmented Generation (RAG) to
generate acceptance criteria from multi-modal requirements data, including both
textual documentation and visual UI information. We systematically evaluated
our approach in an industrial case study involving an education-focused
software system used by approximately 100,000 users. The results indicate that
integrating multi-modal information significantly enhances the relevance,
correctness, and comprehensibility of the generated ACs. Moreover, practitioner
evaluations confirm that our approach effectively reduces manual effort,
captures nuanced stakeholder intent, and provides valuable criteria that domain
experts may overlook, demonstrating practical utility and significant potential
for industry adoption. This research underscores the potential of multi-modal
RAG techniques in streamlining software validation processes and improving
development efficiency. We also make our implementation and a dataset
available.

</details>


### [4] [Integrating Rules and Semantics for LLM-Based C-to-Rust Translation](https://arxiv.org/abs/2508.06926)
*Feng Luo,Kexing Ji,Cuiyun Gao,Shuzheng Gao,Jia Feng,Kui Liu,Xin Xia,Michael R. Lyu*

Main category: cs.SE

TL;DR: IRENE是一个基于LLM的框架，通过集成规则和语义提升C到Rust的代码翻译质量，解决了现有方法在语法规则和语义一致性上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有C到Rust的翻译方法依赖静态规则或LLM的直接提示策略，存在语法规则适应性和语义捕获不足的问题。

Method: IRENE包含三个模块：规则增强检索模块、结构化摘要模块和错误驱动翻译模块，分别提升规则处理、语义理解和翻译迭代优化。

Result: 在两个数据集（xCodeEval和HW-Bench）和八个LLM上评估，IRENE在翻译准确性和安全性上表现优异。

Conclusion: IRENE通过结合规则和语义，显著提升了C到Rust代码翻译的质量和安全性。

Abstract: Automated translation of legacy C code into Rust aims to ensure memory safety
while reducing the burden of manual migration. Early approaches in code
translation rely on static rule-based methods, but they suffer from limited
coverage due to dependence on predefined rule patterns. Recent works regard the
task as a sequence-to-sequence problem by leveraging large language models
(LLMs). Although these LLM-based methods are capable of reducing unsafe code
blocks, the translated code often exhibits issues in following Rust rules and
maintaining semantic consistency. On one hand, existing methods adopt a direct
prompting strategy to translate the C code, which struggles to accommodate the
syntactic rules between C and Rust. On the other hand, this strategy makes it
difficult for LLMs to accurately capture the semantics of complex code. To
address these challenges, we propose IRENE, an LLM-based framework that
Integrates RulEs aNd sEmantics to enhance translation. IRENE consists of three
modules: 1) a rule-augmented retrieval module that selects relevant translation
examples based on rules generated from a static analyzer developed by us,
thereby improving the handling of Rust rules; 2) a structured summarization
module that produces a structured summary for guiding LLMs to enhance the
semantic understanding of C code; 3) an error-driven translation module that
leverages compiler diagnostics to iteratively refine translations. We evaluate
IRENE on two datasets (xCodeEval, a public dataset, and HW-Bench, an industrial
dataset provided by Huawei) and eight LLMs, focusing on translation accuracy
and safety.

</details>


### [5] [When Prompt Engineering Meets Software Engineering: CNL-P as Natural and Robust "APIs'' for Human-AI Interaction](https://arxiv.org/abs/2508.06942)
*Zhenchang Xing,Yang Liu,Zhuo Cheng,Qing Huang,Dehai Zhao,Daniel Sun,Chenhua Liu*

Main category: cs.SE

TL;DR: 提出了一种名为CNL-P的受控自然语言提示方法，结合了提示工程和软件工程原则，以减少自然语言的歧义并提高LLM输出的质量。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLM）能力的增强，其在智能客服、代码生成等领域的应用日益广泛，但自然语言提示的歧义性限制了其效果。

Method: CNL-P通过引入精确的语法结构和严格的语义规范，结合提示工程和软件工程原则，并开发了NL2CNL-P转换工具和语法检查工具。

Result: 实验表明，CNL-P显著提升了LLM响应的质量，实现了提示工程与软件工程的有机融合。

Conclusion: CNL-P为以自然语言为中心的新编程范式奠定了基础，弥合了提示工程与传统软件工程之间的差距。

Abstract: With the growing capabilities of large language models (LLMs), they are
increasingly applied in areas like intelligent customer service, code
generation, and knowledge management. Natural language (NL) prompts act as the
``APIs'' for human-LLM interaction. To improve prompt quality, best practices
for prompt engineering (PE) have been developed, including writing guidelines
and templates. Building on this, we propose Controlled NL for Prompt (CNL-P),
which not only incorporates PE best practices but also draws on key principles
from software engineering (SE). CNL-P introduces precise grammar structures and
strict semantic norms, further eliminating NL's ambiguity, allowing for a
declarative but structured and accurate expression of user intent. This helps
LLMs better interpret and execute the prompts, leading to more consistent and
higher-quality outputs. We also introduce an NL2CNL-P conversion tool based on
LLMs, enabling users to write prompts in NL, which are then transformed into
CNL-P format, thus lowering the learning curve of CNL-P. In particular, we
develop a linting tool that checks CNL-P prompts for syntactic and semantic
accuracy, applying static analysis techniques to NL for the first time.
Extensive experiments demonstrate that CNL-P enhances the quality of LLM
responses through the novel and organic synergy of PE and SE. We believe that
CNL-P can bridge the gap between emerging PE and traditional SE, laying the
foundation for a new programming paradigm centered around NL.

</details>


### [6] [An Empirical Study on Method-Level Performance Evolution in Open-Source Java Projects](https://arxiv.org/abs/2508.07084)
*Kaveh Shahedi,Nana Gyambrah,Heng Li,Maxime Lamothe,Foutse Khomh*

Main category: cs.SE

TL;DR: 该论文通过大规模实证研究分析了Java项目中方法级代码变更对性能的影响，发现32.7%的变更会导致性能变化，且性能退化比改进更常见。研究挑战了传统观念，并提出了自动化性能测试的建议。


<details>
  <summary>Details</summary>
Motivation: 性能是软件开发中的关键质量属性，但方法级代码变更对性能演化的影响缺乏细粒度的实证研究。开发者常凭直觉假设变更类型与性能的关系，但这些假设缺乏验证。

Method: 研究分析了15个成熟开源Java项目的739个提交中的1,499个方法级变更，使用JMH进行性能测量和统计量化，并通过字节码插桩捕获执行指标。

Result: 32.7%的变更导致性能变化，性能退化比改进多1.3倍。算法变更改进潜力最大但风险也高。资深开发者变更更稳定，代码复杂度增加退化风险。

Conclusion: 研究为将自动化性能测试集成到持续集成流程提供了实证依据，并挑战了传统的风险分层开发策略。

Abstract: Performance is a critical quality attribute in software development, yet the
impact of method-level code changes on performance evolution remains poorly
understood. While developers often make intuitive assumptions about which types
of modifications are likely to cause performance regressions or improvements,
these beliefs lack empirical validation at a fine-grained level. We conducted a
large-scale empirical study analyzing performance evolution in 15 mature
open-source Java projects hosted on GitHub. Our analysis encompassed 739
commits containing 1,499 method-level code changes, using Java Microbenchmark
Harness (JMH) for precise performance measurement and rigorous statistical
analysis to quantify both the significance and magnitude of performance
variations. We employed bytecode instrumentation to capture method-specific
execution metrics and systematically analyzed four key aspects: temporal
performance patterns, code change type correlations, developer and complexity
factors, and domain-size interactions. Our findings reveal that 32.7% of
method-level changes result in measurable performance impacts, with regressions
occurring 1.3 times more frequently than improvements. Contrary to conventional
wisdom, we found no significant differences in performance impact distributions
across code change categories, challenging risk-stratified development
strategies. Algorithmic changes demonstrate the highest improvement potential
but carry substantial regression risk. Senior developers produce more stable
changes with fewer extreme variations, while code complexity correlates with
increased regression likelihood. Domain-size interactions reveal significant
patterns, with web server + small projects exhibiting the highest performance
instability. Our study provides empirical evidence for integrating automated
performance testing into continuous integration pipelines.

</details>


### [7] [From Noise to Knowledge: Interactive Summaries for Developer Alerts](https://arxiv.org/abs/2508.07169)
*Burak Yetiştiren,Hong Jin Kang,Miryung Kim*

Main category: cs.SE

TL;DR: CLARITY是一个交互式工具，通过总结规则和主动反馈帮助程序员更高效地理解和分类静态分析工具生成的警告。


<details>
  <summary>Details</summary>
Motivation: 程序员通常需要逐个检查静态分析工具生成的警告，而识别重复主题和关系可以提升认知效率。

Method: CLARITY通过交互式查询和规则推断算法，支持用户自定义分组警告，并根据用户反馈动态调整规则。

Result: 在用户研究中，CLARITY显著提高了用户识别警告根本原因的速度和信心，且模拟显示其反馈机制减少了所需交互次数。

Conclusion: CLARITY通过主动学习驱动的总结方法，有效提升了警告理解的交互性和效率。

Abstract: Programmers using bug-finding tools often review their reported warnings one
by one. Based on the insight that identifying recurring themes and
relationships can enhance the cognitive process of sensemaking, we propose
CLARITY, which supports interpreting tool-generated warnings through
interactive inquiry. CLARITY derives summary rules for custom grouping of
related warnings with active feedback. As users mark warnings as interesting or
uninteresting, CLARITY's rule inference algorithm surfaces common symptoms,
highlighting structural similarities in containment, subtyping, invoked
methods, accessed fields, and expressions.
  We demonstrate CLARITY on Infer and SpotBugs warnings across two mature Java
projects. In a within-subject user study with 14 participants, users
articulated root causes for similar uninteresting warnings faster and with more
confidence using CLARITY. We observed significant individual variation in
desired grouping, reinforcing the need for customizable sensemaking. Simulation
shows that with rule-level feedback, only 11.8 interactions are needed on
average to align all inferred rules with a simulated user's labels (vs. 17.8
without). Our evaluation suggests that CLARITY's active learning-based
summarization enhances interactive warning sensemaking.

</details>


### [8] [Dynamic Benchmark Construction for Evaluating Large Language Models on Real-World Codes](https://arxiv.org/abs/2508.07180)
*Zhe Zhang,Runlin Liu,Aishan Liu,Xingyu Liu,Xiang Gao,Hailong Sun*

Main category: cs.SE

TL;DR: CODE2BENCH是一个动态构建无污染、鲁棒的代码生成基准的端到端管道，用于评估大语言模型（LLMs）在真实世界任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有基准存在数据污染和测试不足的问题，无法有效揭示模型缺陷。

Method: 通过自动化动态更新、依赖分析和属性测试构建基准。

Result: 评估16个LLMs显示，模型在复杂逻辑和跨语言任务上表现较差，但在Python库任务上较强。

Conclusion: CODE2BENCH为LLMs在真实软件开发任务中的评估提供了无污染、语言无关的方法。

Abstract: As large language models LLMs) become increasingly integrated into software
development workflows, rigorously evaluating their performance on complex,
real-world code generation tasks has become essential. However, existing
benchmarks often suffer from data contamination and limited test rigor,
constraining their ability to reveal model failures effectively. To address
these, we present CODE2BENCH, a end-to-end pipeline for dynamically
constructing robust and contamination-resistant benchmarks from real-world
GitHub repositories. Specifically, CODE2BENCH introduces three key innovations:
(1) Automated Dynamism, achieved through periodic ingestion of recent code to
minimize training data contamination; (2) Scope Graph-based dependency
analysis, which enables structured classification of functions into benchmark
instances with controlled dependency levels (distinguishing between
Self-Contained (SC) tasks for cross-language evaluation and Weakly
Self-Contained (WSC) tasks involving permitted library usage); and (3)
Property-Based Testing (PBT) for the automated synthesis of rigorous test
suites to enable thorough functional verification. Using this pipeline, we
construct CODE2BENCH-2505, the first benchmark derived from 880 recent Python
projects spanning diverse domains, comprising 1,163 code generation tasks with
100% average branch coverage on ground-truth implementations. Extensive
evaluation of 16 LLMs using CODE2BENCH-2505 reveals that models consistently
struggle with SC tasks requiring complex, non-standard logic and cross-language
transfer, while showing relatively stronger performance on WSC tasks in Python.
Our work introduces a contamination-resistant, language-agnostic methodology
for dynamic benchmark construction, offering a principled foundation for the
comprehensive and realistic evaluation of LLMs on real-world software
development tasks.

</details>


### [9] [TraceLens: Question-Driven Debugging for Taint Flow Understanding](https://arxiv.org/abs/2508.07198)
*Burak Yetiştiren,Hong Jin Kang,Miryung Kim*

Main category: cs.SE

TL;DR: TraceLens是一种新型的污点分析调试工具，通过问答式界面帮助用户分析数据流问题，显著提高了分析准确性和用户体验。


<details>
  <summary>Details</summary>
Motivation: 现有污点分析工具缺乏对数据流问题的调试能力，用户难以理解异常数据流的原因或缺失预期数据流的情况。

Method: 提出TraceLens，支持用户通过问答形式（如为什么、为什么不、假设问题）调试数据流，并进行假设性分析。

Result: 用户研究表明，TraceLens比CodeQL平均准确率提高21%，减少45%的认知负担，并提升用户信心。

Conclusion: TraceLens通过问答式调试界面显著提升了污点分析的效果和用户体验。

Abstract: Taint analysis is a security analysis technique used to track the flow of
potentially dangerous data through an application and its dependent libraries.
Investigating why certain unexpected flows appear and why expected flows are
missing is an important sensemaking process during end-user taint analysis.
Existing taint analysis tools often do not provide this end-user debugging
capability, where developers can ask why, why-not, and what-if questions about
dataflows and reason about the impact of configuring sources and sinks, and
models of 3rd-party libraries that abstract permissible and impermissible data
flows. Furthermore, a tree-view or a list-view used in existing
taint-analyzer's visualization makes it difficult to reason about the global
impact on connectivity between multiple sources and sinks.
  Inspired by the insight that sensemaking tool-generated results can be
significantly improved by a QA inquiry process, we propose TraceLens, a first
end-user question-answer style debugging interface for taint analysis. It
enables a user to ask why, why-not, and what-if questions to investigate the
existence of suspicious flows, the non-existence of expected flows, and the
global impact of third-party library models. TraceLens performs speculative
what-if analysis, to help a user in debugging how different connectivity
assumptions affect overall results. A user study with 12 participants shows
that participants using TraceLens achieved 21% higher accuracy on average,
compared to CodeQL. They also reported a 45% reduction in mental demand
(NASA-TLX) and rated higher confidence in identifying relevant flows using
TraceLens.

</details>


### [10] [AutoAssert 1: A LoRA Fine-Tuned LLM Model for Efficient Automated Assertion Generation](https://arxiv.org/abs/2508.07371)
*Yi Zhong,Hongchao Liu,Di ZHao*

Main category: cs.SE

TL;DR: 提出了一种基于硬件描述语言（HDL）的断言生成方法，结合轻量级可调参数的大语言模型（LLM）和Unsloth平台，自动生成测试用例，显著降低训练成本且不牺牲准确性。


<details>
  <summary>Details</summary>
Motivation: 随着软件系统复杂性增加，对自动化测试和维护工具的需求急剧增长。

Method: 结合轻量级可调参数的LLM与Unsloth平台，自动生成测试用例。

Result: 经验证，该方法能高效生成严格符合硬件逻辑的断言。

Conclusion: 该框架为现代软件测试和维护提供了强大而灵活的解决方案。

Abstract: As the complexity of software systems continues to increase, the demand for
automated testing and maintenance tools is growing exponentially. To meet this
urgent need, we propose a new assertion generation method based on Hardware
Description Language (HDL). This method combines a lightweight,
parameter-adjustable large language model (LLM) with the Unsloth platform to
automatically generate test cases, thereby significantly reducing training
costs without sacrificing accuracy or generalization performance. Empirical
evaluation shows that our method can efficiently generate assertions that
strictly conform to the hardware logic. This framework provides a robust and
flexible solution to modern software testing and maintenance challenges.
https://github.com/liusu-orange/AutoAssert-1 and
https://gitee.com/OpenBPU/auto-assert1 are the locations of the source code.

</details>


### [11] [Extracting Overlapping Microservices from Monolithic Code via Deep Semantic Embeddings and Graph Neural Network-Based Soft Clustering](https://arxiv.org/abs/2508.07486)
*Morteza Ziabakhsh,Kiyan Rezaee,Sadegh Eskandari,Seyed Amir Hossein Tabatabaei,Mohammad M. Ghassemi*

Main category: cs.SE

TL;DR: Mo2oM框架通过软聚类方法将单体架构转换为重叠微服务，显著提升了结构模块化和通信效率。


<details>
  <summary>Details</summary>
Motivation: 现有微服务提取方法采用硬聚类，导致服务间耦合增加和服务内内聚降低，Mo2oM旨在解决这一问题。

Method: 结合深度语义嵌入和结构依赖，使用图神经网络软聚类算法生成微服务。

Result: 在四个开源基准测试中，Mo2oM在结构模块化、通信开销、接口数量和服务规模平衡方面均有显著提升。

Conclusion: Mo2oM通过软聚类方法有效优化了微服务提取，优于现有技术。

Abstract: Modern software systems are increasingly shifting from monolithic
architectures to microservices to enhance scalability, maintainability, and
deployment flexibility. Existing microservice extraction methods typically rely
on hard clustering, assigning each software component to a single microservice.
This approach often increases inter-service coupling and reduces intra-service
cohesion. We propose Mo2oM (Monolithic to Overlapping Microservices), a
framework that formulates microservice extraction as a soft clustering problem,
allowing components to belong probabilistically to multiple microservices. This
approach is inspired by expert-driven decompositions, where practitioners
intentionally replicate certain software components across services to reduce
communication overhead. Mo2oM combines deep semantic embeddings with structural
dependencies extracted from methodcall graphs to capture both functional and
architectural relationships. A graph neural network-based soft clustering
algorithm then generates the final set of microservices. We evaluate Mo2oM on
four open-source monolithic benchmarks and compare it against eight
state-of-the-art baselines. Our results demonstrate that Mo2oM achieves
improvements of up to 40.97% in structural modularity (balancing cohesion and
coupling), 58% in inter-service call percentage (communication overhead),
26.16% in interface number (modularity and decoupling), and 38.96% in
non-extreme distribution (service size balance) across all benchmarks.

</details>


### [12] [Adopting Road-Weather Open Data in Route Recommendation Engine](https://arxiv.org/abs/2508.07881)
*Henna Tammia,Benjamin Kämä,Ella Peltonen*

Main category: cs.SE

TL;DR: 论文探讨了如何高效利用芬兰DigiTraffic开放道路数据接口的大规模道路天气和交通数据，提出了一种基于简单路由应用的个性化道路推荐引擎方法。


<details>
  <summary>Details</summary>
Motivation: DigiTraffic提供了大量实时道路传感器数据，但高效利用这些数据需要深入理解数据质量、预处理和机器学习工具。

Method: 通过分析DigiTraffic的道路天气相关属性，提出了一种高效数据利用的方法，并开发了基于简单路由应用的个性化道路推荐引擎。

Result: 基于真实数据验证，该方法能有效识别并为三种不同驾驶者档案推荐个性化路线。

Conclusion: 研究展示了如何利用大规模道路数据开发实用应用，为个性化路线推荐提供了可行方案。

Abstract: Digitraffic, Finland's open road data interface, provides access to
nationwide road sensors with more than 2,300 real-time attributes from 1,814
stations. However, efficiently utilizing such a versatile data API for a
practical application requires a deeper understanding of the data qualities,
preprocessing phases, and machine learning tools. This paper discusses the
challenges of large-scale road weather and traffic data. We go through the
road-weather-related attributes from DigiTraffic as a practical example of
processes required to work with such a dataset. In addition, we provide a
methodology for efficient data utilization for the target application, a
personalized road recommendation engine based on a simple routing application.
We validate our solution based on real-world data, showing we can efficiently
identify and recommend personalized routes for three different driver profiles.

</details>


### [13] [SHIELDA: Structured Handling of Exceptions in LLM-Driven Agentic Workflows](https://arxiv.org/abs/2508.07935)
*Jingwen Zhou,Jieshan Chen,Qinghua Lu,Dehai Zhao,Liming Zhu*

Main category: cs.SE

TL;DR: 论文提出SHIELDA框架，用于结构化处理LLM驱动的工作流中的异常，通过分类和执行预定义模式实现跨阶段恢复。


<details>
  <summary>Details</summary>
Motivation: 现有异常处理方案对异常处理浅显且恢复逻辑脆弱，无法追踪异常的根本原因。

Method: 提出SHIELDA框架，包含异常分类器和结构化执行器，实现异常根因链接和可组合策略。

Result: 通过AutoPR代理案例验证SHIELDA能有效实现跨阶段异常恢复。

Conclusion: SHIELDA为LLM代理工作流提供了一种模块化、结构化的异常处理解决方案。

Abstract: Large Language Model (LLM) agentic systems are software systems powered by
LLMs that autonomously reason, plan, and execute multi-step workflows to
achieve human goals, rather than merely executing predefined steps. During
execution, these workflows frequently encounter exceptions. Existing exception
handling solutions often treat exceptions superficially, failing to trace
execution-phase exceptions to their reasoning-phase root causes. Furthermore,
their recovery logic is brittle, lacking structured escalation pathways when
initial attempts fail. To tackle these challenges, we first present a
comprehensive taxonomy of 36 exception types across 12 agent artifacts.
Building on this, we propose SHIELDA (Structured Handling of Exceptions in
LLM-Driven Agentic Workflows), a modular runtime exception handling framework
for LLM agentic workflows. SHIELDA uses an exception classifier to select a
predefined exception handling pattern from a handling pattern registry. These
patterns are then executed via a structured handling executor, comprising local
handling, flow control, and state recovery, to enable phase-aware recovery by
linking exceptions to their root causes and facilitating composable strategies.
We validate SHIELDA's effectiveness through a case study on the AutoPR agent,
demonstrating effective, cross-phase recovery from a reasoning-induced
exception.

</details>


### [14] [Exploring the Challenges and Opportunities of AI-assisted Codebase Generation](https://arxiv.org/abs/2508.07966)
*Philipp Eibl,Sadra Sabouri,Souti Chattopadhyay*

Main category: cs.SE

TL;DR: 论文研究了代码库AI助手（CBAs）的开发者和用户互动情况，发现尽管CBAs能生成完整代码库，但用户满意度较低，主要问题包括功能不足、代码质量差和沟通问题。


<details>
  <summary>Details</summary>
Motivation: 探索开发者如何与CBAs互动，以及CBAs未能满足开发者需求的原因，以改进其设计和实用性。

Method: 通过用户研究（n=16）和访谈，分析开发者在编码任务中使用CBAs的行为和反馈。

Result: 用户满意度低（平均2.8分），主要不满源于功能不足（77%）、代码质量差（42%）和沟通问题（25%）。

Conclusion: 提出了六项CBAs面临的挑战和五项工作流程障碍，并基于21个商业CBAs的调查，提出了改进设计的机会。

Abstract: Recent AI code assistants have significantly improved their ability to
process more complex contexts and generate entire codebases based on a textual
description, compared to the popular snippet-level generation. These codebase
AI assistants (CBAs) can also extend or adapt codebases, allowing users to
focus on higher-level design and deployment decisions. While prior work has
extensively studied the impact of snippet-level code generation, this new class
of codebase generation models is relatively unexplored. Despite initial
anecdotal reports of excitement about these agents, they remain less frequently
adopted compared to snippet-level code assistants. To utilize CBAs better, we
need to understand how developers interact with CBAs, and how and why CBAs fall
short of developers' needs. In this paper, we explored these gaps through a
counterbalanced user study and interview with (n = 16) students and developers
working on coding tasks with CBAs. We found that participants varied the
information in their prompts, like problem description (48% of prompts),
required functionality (98% of prompts), code structure (48% of prompts), and
their prompt writing process. Despite various strategies, the overall
satisfaction score with generated codebases remained low (mean = 2.8, median =
3, on a scale of one to five). Participants mentioned functionality as the most
common factor for dissatisfaction (77% of instances), alongside poor code
quality (42% of instances) and communication issues (25% of instances). We
delve deeper into participants' dissatisfaction to identify six underlying
challenges that participants faced when using CBAs, and extracted five barriers
to incorporating CBAs into their workflows. Finally, we surveyed 21 commercial
CBAs to compare their capabilities with participant challenges and present
design opportunities for more efficient and useful CBAs.

</details>


### [15] [PyVeritas: On Verifying Python via LLM-Based Transpilation and Bounded Model Checking for C](https://arxiv.org/abs/2508.08171)
*Pedro Orvalho,Marta Kwiatkowska*

Main category: cs.SE

TL;DR: PyVeritas利用大型语言模型（LLMs）将Python代码转换为C代码，结合模型检查和MaxSAT技术，实现了Python程序的验证和错误定位。


<details>
  <summary>Details</summary>
Motivation: Python缺乏成熟的正式验证工具，而C语言已有成熟的模型检查工具（如CBMC）。PyVeritas旨在填补这一空白，利用LLMs实现高效的代码转换和验证。

Method: 通过LLMs将Python代码转换为C代码，然后使用边界模型检查和MaxSAT技术进行验证和错误定位。

Result: 实验表明，LLM转换的准确率可达80-90%，支持对小型Python程序的断言验证和错误诊断。

Conclusion: PyVeritas为Python程序提供了一种有效的正式验证方法，填补了现有工具的不足。

Abstract: Python has become the dominant language for general-purpose programming, yet
it lacks robust tools for formal verification. In contrast, programmers working
in languages such as C benefit from mature model checkers, for example CBMC,
which enable exhaustive symbolic reasoning and fault localisation. The inherent
complexity of Python, coupled with the verbosity and low-level nature of
existing transpilers (e.g., Cython), have historically limited the
applicability of formal verification to Python programs.
  In this paper, we propose PyVeritas, a novel framework that leverages Large
Language Models (LLMs) for high-level transpilation from Python to C, followed
by bounded model checking and MaxSAT-based fault localisation in the generated
C code. PyVeritas enables verification and bug localisation for Python code
using existing model checking tools for C. Our empirical evaluation on two
Python benchmarks demonstrates that LLM-based transpilation can achieve a high
degree of accuracy, up to 80--90% for some LLMs, enabling effective development
environment that supports assertion-based verification and interpretable fault
diagnosis for small yet non-trivial Python programs.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [16] [Symbolic Execution in Practice: A Survey of Applications in Vulnerability, Malware, Firmware, and Protocol Analysis](https://arxiv.org/abs/2508.06643)
*Joshua Bailey,Charles Nicholas*

Main category: cs.CR

TL;DR: 本文提出了符号执行的系统分类法，将其分为范围缩减和引导启发式两种主要策略，并探讨了其在多个领域的应用及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 符号执行虽强大，但路径爆炸问题限制了其实际应用，因此需要系统分类和优化策略。

Method: 提出分类法，将策略分为范围缩减和引导启发式，并综述其在多个领域的应用。

Result: 分类法为符号执行提供了清晰的优化方向，并展示了其广泛的应用潜力。

Conclusion: 未来研究可关注实时操作系统和类型安全语言中的符号执行应用。

Abstract: Symbolic execution is a powerful program analysis technique that allows for
the systematic exploration of all program paths. Path explosion, where the
number of states to track becomes unwieldy, is one of the biggest challenges
hindering symbolic execution's practical application. To combat this,
researchers have employed various strategies to enable symbolic execution on
complex software systems. This paper introduces a systematic taxonomy of these
strategies, categorizing them into two primary approaches: Scope Reduction,
which aims to reduce the scope of symbolic execution to manageable portions of
code, and Guidance Heuristics, which steer the symbolic execution engine toward
promising paths. Using this taxonomy as a lens, we survey applications of
symbolic executions in several domains such as vulnerability analysis, malware
analysis, firmware re-hosting, and network protocol analysis. Finally, we
identify promising directions for future research, including the application of
symbolic execution to real-time operating systems and modern, type-safe
languages.

</details>


### [17] [Mitigating Distribution Shift in Graph-Based Android Malware Classification via Function Metadata and LLM Embeddings](https://arxiv.org/abs/2508.06734)
*Ngoc N. Tran,Anwar Said,Waseem Abbas,Tyler Derr,Xenofon D. Koutsoukos*

Main category: cs.CR

TL;DR: 现有基于图的恶意软件分类器在标准数据集上准确率超过94%，但在同一家族未见变体上准确率下降高达45%。本文提出语义增强框架，提升模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法在模型架构和结构表示上未能捕捉深层语义模式，导致泛化能力不足。

Method: 提出语义增强框架，结合上下文特征（如函数级元数据和代码嵌入）增强函数调用图。

Result: 实验表明，该方法在分布偏移下提升分类性能8%，并增强鲁棒性。

Conclusion: 该方法为构建适应威胁环境变化的恶意软件检测系统提供了实用路径。

Abstract: Graph-based malware classifiers can achieve over 94% accuracy on standard
Android datasets, yet we find they suffer accuracy drops of up to 45% when
evaluated on previously unseen malware variants from the same family - a
scenario where strong generalization would typically be expected. This
highlights a key limitation in existing approaches: both the model
architectures and their structure-only representations often fail to capture
deeper semantic patterns. In this work, we propose a robust semantic enrichment
framework that enhances function call graphs with contextual features,
including function-level metadata and, when available, code embeddings derived
from large language models. The framework is designed to operate under
real-world constraints where feature availability is inconsistent, and supports
flexible integration of semantic signals. To evaluate generalization under
realistic domain and temporal shifts, we introduce two new benchmarks:
MalNet-Tiny-Common and MalNet-Tiny-Distinct, constructed using malware family
partitioning to simulate cross-family generalization and evolving threat
behavior. Experiments across multiple graph neural network backbones show that
our method improves classification performance by up to 8% under distribution
shift and consistently enhances robustness when integrated with
adaptation-based methods. These results offer a practical path toward building
resilient malware detection systems in evolving threat environments.

</details>


### [18] [Label Inference Attacks against Federated Unlearning](https://arxiv.org/abs/2508.06789)
*Wei Wang,Xiangyun Tang,Yajie Wang,Yijing Lin,Tao Zhang,Meng Shen,Dusit Niyato,Liehuang Zhu*

Main category: cs.CR

TL;DR: 论文提出了一种针对联邦遗忘（FU）的新型标签推断攻击ULIA，通过模型参数变化推断遗忘数据的标签，实验表明攻击成功率高达100%。


<details>
  <summary>Details</summary>
Motivation: 解决联邦遗忘中模型参数变化暴露用户数据隐私的问题，填补标签推断攻击研究的空白。

Method: 设计了梯度-标签映射机制，通过模型梯度变化推断遗忘数据的标签，并在IID和非IID设置下评估。

Result: 在IID设置下，ULIA攻击成功率达100%；即使仅遗忘1%数据，攻击成功率仍为93%至62.3%。

Conclusion: ULIA攻击揭示了联邦遗忘中的隐私风险，需进一步研究防御措施。

Abstract: Federated Unlearning (FU) has emerged as a promising solution to respond to
the right to be forgotten of clients, by allowing clients to erase their data
from global models without compromising model performance. Unfortunately,
researchers find that the parameter variations of models induced by FU expose
clients' data information, enabling attackers to infer the label of unlearning
data, while label inference attacks against FU remain unexplored. In this
paper, we introduce and analyze a new privacy threat against FU and propose a
novel label inference attack, ULIA, which can infer unlearning data labels
across three FU levels. To address the unique challenges of inferring labels
via the models variations, we design a gradient-label mapping mechanism in ULIA
that establishes a relationship between gradient variations and unlearning
labels, enabling inferring labels on accumulated model variations. We evaluate
ULIA on both IID and non-IID settings. Experimental results show that in the
IID setting, ULIA achieves a 100% Attack Success Rate (ASR) under both
class-level and client-level unlearning. Even when only 1% of a user's local
data is forgotten, ULIA still attains an ASR ranging from 93% to 62.3%.

</details>


### [19] [Towards Practical Data-Dependent Memory-Hard Functions with Optimal Sustained Space Trade-offs in the Parallel Random Oracle Model](https://arxiv.org/abs/2508.06795)
*Jeremiah Blocki,Blake Holman*

Main category: cs.CR

TL;DR: 论文提出了一种新的内存硬函数（MHF）EGSample，旨在解决现有MHF在实践中的不足，并提供更强的SSC/CMC权衡保证。


<details>
  <summary>Details</summary>
Motivation: 现有MHF在理论构造上依赖昂贵的组合图，且动态铺砖游戏在MHF分析中的启发式使用缺乏形式化证明，可能导致并行随机预言模型（PROM）中的高效攻击。

Method: 开发了新的MHF EGSample，避免依赖昂贵的组合构造，并在动态铺砖模型和PROM中证明了其SSC/CMC权衡的等效性。

Result: 在动态铺砖模型中，EGSample的SSC/CMC权衡为Ω(N)或Ω(N^{3−ϵ})；在PROM中，权衡为Ω(N)或Ω(N^{2.5−ϵ})。

Conclusion: EGSample是一种实用的MHF，具有可证明的强SSC/CMC权衡，适用于实际部署。

Abstract: Memory-Hard Functions (MHF) are a useful cryptographic primitive to build
egalitarian proofs-of-work and to help protect low entropy secrets (e.g., user
passwords) against brute-forces attacks. Ideally, we would like for a MHF to
have the property that (1) an honest party can evaluate the function in
sequential time $\Omega(N)$, and (2) any parallel party that evaluates the
function is forced to lockup $\Omega(N)$ memory for $\Omega(N)$ sequential
steps. Unfortunately, this goal is not quite achievable, so prior work of
Blocki and Holman [BH22] focused on designing MHFs with strong tradeoff
guarantees between sustained-space complexity (SSC) and cumulative memory costs
(CMC). However, their theoretical construction is not suitable for practical
deployment due to the reliance on expensive constructions of combinatorial
graphs. Furthermore, there is no formal justification for the heuristic use of
the dynamic pebbling game in MHF analysis so we cannot rule out the possibility
that there are more efficient attacks in the Parallel Random Oracle Model
(PROM). Towards the goal of developing a practical MHF with provably strong
SSC/CMC tradeoffs we develop a new MHF called EGSample which does not rely on
expensive combinatorial constructions like [BH22]. In the dynamic pebbling
model, we prove equivalent SSC/CMC tradeoffs for EGSample i.e., any the dynamic
pebbling strategy either (1) locks up $\Omega(N)$ memory for $\Omega(N)$ steps,
or (2) incurs cumulative memory cost at least $\Omega(N^{3-\epsilon})$. We also
develop new techniques to directly establish SSC/CMC tradeoffs in the parallel
random oracle model. In particular, we prove that {\em any} PROM algorithm
evaluating our MHF either (1) locks up $\Omega(N)$ blocks of memory for
$\Omega(N)$ steps or (2) incurs cumulative memory cost at least
$\Omega(N^{2.5-\epsilon})$.

</details>


### [20] [Towards Effective Prompt Stealing Attack against Text-to-Image Diffusion Models](https://arxiv.org/abs/2508.06837)
*Shiqian Zhao,Chong Wang,Yiming Li,Yihao Huang,Wenjie Qu,Siew-Kei Lam,Yi Xie,Kangjie Chen,Jie Zhang,Tianwei Zhang*

Main category: cs.CR

TL;DR: 论文提出了一种名为Prometheus的训练无关、基于搜索的提示窃取攻击方法，通过动态修饰符和上下文匹配算法，成功从多个平台提取高质量提示。


<details>
  <summary>Details</summary>
Motivation: 现有提示窃取攻击方法适应性有限，无法有效应对多样化的展示图像和扩散模型。

Method: 提出动态修饰符补充静态修饰符，设计上下文匹配算法减少搜索空间，并通过本地代理模型进行贪婪搜索优化提示。

Result: Prometheus在多个平台上成功提取提示，攻击成功率提升25.0%，且对潜在防御措施具有抵抗力。

Conclusion: Prometheus展示了提示窃取攻击的实际严重性，为未来防御研究提供了方向。

Abstract: Text-to-Image (T2I) models, represented by DALL$\cdot$E and Midjourney, have
gained huge popularity for creating realistic images. The quality of these
images relies on the carefully engineered prompts, which have become valuable
intellectual property. While skilled prompters showcase their AI-generated art
on markets to attract buyers, this business incidentally exposes them to
\textit{prompt stealing attacks}. Existing state-of-the-art attack techniques
reconstruct the prompts from a fixed set of modifiers (i.e., style
descriptions) with model-specific training, which exhibit restricted
adaptability and effectiveness to diverse showcases (i.e., target images) and
diffusion models.
  To alleviate these limitations, we propose Prometheus, a training-free,
proxy-in-the-loop, search-based prompt-stealing attack, which reverse-engineers
the valuable prompts of the showcases by interacting with a local proxy model.
It consists of three innovative designs. First, we introduce dynamic modifiers,
as a supplement to static modifiers used in prior works. These dynamic
modifiers provide more details specific to the showcases, and we exploit NLP
analysis to generate them on the fly. Second, we design a contextual matching
algorithm to sort both dynamic and static modifiers. This offline process helps
reduce the search space of the subsequent step. Third, we interact with a local
proxy model to invert the prompts with a greedy search algorithm. Based on the
feedback guidance, we refine the prompt to achieve higher fidelity. The
evaluation results show that Prometheus successfully extracts prompts from
popular platforms like PromptBase and AIFrog against diverse victim models,
including Midjourney, Leonardo.ai, and DALL$\cdot$E, with an ASR improvement of
25.0\%. We also validate that Prometheus is resistant to extensive potential
defenses, further highlighting its severity in practice.

</details>


### [21] [SPARE: Securing Progressive Web Applications Against Unauthorized Replications](https://arxiv.org/abs/2508.07053)
*Sajib Talukder,Nur Imtiazul Haque,Khandakar Ashrafi Akbar*

Main category: cs.CR

TL;DR: 论文提出了一种基于查询参数的安全解决方案，防止恶意开发者复制PWA链接创建假冒原生应用，并通过嵌入Unix时间戳和设备标识符来验证防御策略的有效性。


<details>
  <summary>Details</summary>
Motivation: WebView和PWA技术被广泛用于移动应用，但恶意开发者可能利用其创建假冒应用，损害用户和原开发者的利益，因此需要有效的安全措施。

Method: 提出基于查询参数的安全方案，结合Unix时间戳和设备标识符，并通过模拟攻击和Zipfian分布的用户行为数据集验证其有效性。

Result: 开发了原型系统，验证了防御策略在高级攻击场景下的有效性，并提出了改进措施。

Conclusion: 提出的安全框架能有效防止PWA的非法复制，为移动应用安全提供了实用解决方案。

Abstract: WebView applications are widely used in mobile applications to display web
content directly within the app, enhancing user engagement by eliminating the
need to open an external browser and providing a seamless experience.
Progressive Web Applications (PWAs) further improve usability by combining the
accessibility of web apps with the speed, offline capabilities, and
responsiveness of native applications. However, malicious developers can
exploit this technology by duplicating PWA web links to create counterfeit
native apps, monetizing through user diversion. This unethical practice poses
significant risks to users and the original application developers,
underscoring the need for robust security measures to prevent unauthorized
replication. Considering the one-way communication of Trusted Web Activity (a
method for integrating web content into Android applications) and PWAs, we
propose a query parameter-based practical security solution to defend against
or mitigate such attacks. We analyze the vulnerabilities of our proposed
security solution to assess its effectiveness and introduce advanced measures
to address any identified weaknesses, presenting a comprehensive defense
framework. As part of our work, we developed a prototype web application that
secures PWAs from replication by embedding a combination of Unix timestamps and
device identifiers into the query parameters. We evaluate the effectiveness of
this defense strategy by simulating an advanced attack scenario. Additionally,
we created a realistic dataset reflecting mobile app user behavior, modeled
using a Zipfian distribution, to validate our framework.

</details>


### [22] [ScamDetect: Towards a Robust, Agnostic Framework to Uncover Threats in Smart Contracts](https://arxiv.org/abs/2508.07094)
*Pasquale De Rosa,Pascal Felber,Valerio Schiavoni*

Main category: cs.CR

TL;DR: 论文提出ScamDetect框架，通过静态字节码分析和图神经网络（GNN）检测智能合约中的恶意行为，解决现有方法面临的混淆技术和区块链环境异质性问题。


<details>
  <summary>Details</summary>
Motivation: 智能合约的广泛应用和金融重要性吸引了复杂威胁，传统检测方法存在隐私和安全问题，静态分析虽有效但面临混淆技术和环境异质性挑战。

Method: 分两阶段开发ScamDetect：1. 使用GNN分析控制流图（CFG）检测混淆的EVM字节码；2. 扩展至WASM等新兴运行时。

Result: PhishingHook框架已实现约90%的检测准确率，ScamDetect旨在进一步提升检测能力。

Conclusion: ScamDetect旨在为去中心化生态系统提供主动、可扩展的安全解决方案。

Abstract: Smart contracts have transformed decentralized finance by enabling
programmable, trustless transactions. However, their widespread adoption and
growing financial significance have attracted persistent and sophisticated
threats, such as phishing campaigns and contract-level exploits. Traditional
transaction-based threat detection methods often expose sensitive user data and
interactions, raising privacy and security concerns. In response, static
bytecode analysis has emerged as a proactive mitigation strategy, identifying
malicious contracts before they execute harmful actions.Building on this
approach, we introduced PhishingHook, the first machine-learning-based
framework for detecting phishing activities in smart contracts via static
bytecode and opcode analysis, achieving approximately 90% detection accuracy.
Nevertheless, two pressing challenges remain: (1) the increasing use of
sophisticated bytecode obfuscation techniques designed to evade static
analysis, and (2) the heterogeneity of blockchain environments requiring
platform-agnostic solutions.This paper presents a vision for ScamDetect (Smart
Contract Agnostic Malware Detector), a robust, modular, and platform-agnostic
framework for smart contract malware detection. Over the next 2.5 years,
ScamDetect will evolve in two stages: first, by tackling obfuscated Ethereum
Virtual Machine (EVM) bytecode through graph neural network (GNN) analysis of
control flow graphs (CFGs), leveraging GNNs' ability to capture complex
structural patterns beyond opcode sequences; and second, by generalizing
detection capabilities to emerging runtimes such as WASM. ScamDetect aims to
enable proactive, scalable security for the future of decentralized ecosystems.

</details>


### [23] [A Real-Time, Self-Tuning Moderator Framework for Adversarial Prompt Detection](https://arxiv.org/abs/2508.07139)
*Ivan Zhang*

Main category: cs.CR

TL;DR: 论文提出了一种实时自调节（RTST）框架，用于防御LLM的对抗攻击和越狱，同时保持轻量级训练。


<details>
  <summary>Details</summary>
Motivation: 随着AI模型在社会中的广泛应用，确保LLM的对齐性对信息安全至关重要。现有防御方法难以快速适应新攻击，或会降低模型对良性提示的响应能力。

Method: 引入实时自调节（RTST）框架，轻量级训练，适应性强。

Result: 在Google的Gemini模型上实证评估，显示其对现代越狱攻击的有效防御。

Conclusion: RTST框架在防御越狱攻击方面优于传统的微调或分类器模型，具有适应性和低侵入性。

Abstract: Ensuring LLM alignment is critical to information security as AI models
become increasingly widespread and integrated in society. Unfortunately, many
defenses against adversarial attacks and jailbreaking on LLMs cannot adapt
quickly to new attacks, degrade model responses to benign prompts, or introduce
significant barriers to scalable implementation. To mitigate these challenges,
we introduce a real-time, self-tuning (RTST) moderator framework to defend
against adversarial attacks while maintaining a lightweight training footprint.
We empirically evaluate its effectiveness using Google's Gemini models against
modern, effective jailbreaks. Our results demonstrate the advantages of an
adaptive, minimally intrusive framework for jailbreak defense over traditional
fine-tuning or classifier models.

</details>


### [24] [Understanding NFTs from EIP Standards](https://arxiv.org/abs/2508.07190)
*Minfeng Qi,Qin Wang,Guangsheng Yu,Ruiqiang Li,Victor Zhou,Shiping Chen*

Main category: cs.CR

TL;DR: 本文首次通过以太坊改进提案（EIPs）视角系统研究NFT技术基础，揭示其标准问题与安全风险。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注NFT市场动态和用户行为，缺乏对其技术标准的系统性分析。

Method: 对191个NFT相关EIPs和10K+社区讨论进行多维度分析，包括Solidity接口解析、继承结构建模等。

Result: 发现基础标准与新兴标准差异、跨版本互操作性差，功能复杂性增加安全风险。

Conclusion: NFT技术基础亟需系统性研究，以应对标准不统一和安全挑战。

Abstract: We argue that the technical foundations of non-fungible tokens (NFTs) remain
inadequately understood. Prior research has focused on market dynamics, user
behavior, and isolated security incidents, yet systematic analysis of the
standards underpinning NFT functionality is largely absent.
  We present the first study of NFTs through the lens of Ethereum Improvement
Proposals (EIPs). We conduct a large-scale empirical analysis of 191
NFT-related EIPs and 10K+ Ethereum Magicians discussions (as of July, 2025). We
integrate multi-dimensional analyses including the automated parsing of
Solidity interfaces, graph-based modeling of inheritance structures,
contributor profiling, and mining of community discussion data. We distinguish
foundational from emerging standards, expose poor cross-version
interoperability, and show that growing functional complexity heightens
security risks.

</details>


### [25] [Fading the Digital Ink: A Universal Black-Box Attack Framework for 3DGS Watermarking Systems](https://arxiv.org/abs/2508.07263)
*Qingyuan Zeng,Shu Jiang,Jiajing Lin,Zhenzhong Wang,Kay Chen Tan,Min Jiang*

Main category: cs.CR

TL;DR: 本文提出了一种针对3D高斯泼溅（3DGS）水印技术的通用黑盒攻击框架GMEA，通过多目标优化平衡水印移除与视觉质量，成功挑战现有水印系统的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着3DGS水印技术的普及，其对抗攻击的鲁棒性尚未充分研究，本文旨在填补这一空白。

Method: 提出GMEA框架，将攻击建模为大规模多目标优化问题，采用基于组的优化策略处理3DGS模型的搜索空间。

Result: 实验表明GMEA能有效移除主流3DGS水印方法中的1D和2D水印，同时保持高视觉保真度。

Conclusion: 揭示了现有3DGS版权保护方案的脆弱性，呼吁开发更鲁棒的水印系统。

Abstract: With the rise of 3D Gaussian Splatting (3DGS), a variety of digital
watermarking techniques, embedding either 1D bitstreams or 2D images, are used
for copyright protection. However, the robustness of these watermarking
techniques against potential attacks remains underexplored. This paper
introduces the first universal black-box attack framework, the Group-based
Multi-objective Evolutionary Attack (GMEA), designed to challenge these
watermarking systems. We formulate the attack as a large-scale multi-objective
optimization problem, balancing watermark removal with visual quality. In a
black-box setting, we introduce an indirect objective function that blinds the
watermark detector by minimizing the standard deviation of features extracted
by a convolutional network, thus rendering the feature maps uninformative. To
manage the vast search space of 3DGS models, we employ a group-based
optimization strategy to partition the model into multiple, independent
sub-optimization problems. Experiments demonstrate that our framework
effectively removes both 1D and 2D watermarks from mainstream 3DGS watermarking
methods while maintaining high visual fidelity. This work reveals critical
vulnerabilities in existing 3DGS copyright protection schemes and calls for the
development of more robust watermarking systems.

</details>


### [26] [SRAM-based Physically Unclonable Function using Lightweight Hamming-Code Fuzzy Extractor for Energy Harvesting Beat Sensors](https://arxiv.org/abs/2508.07510)
*Hoang-Long Pham,Duy-Hieu Bui,Xuan-Tu Tran,Orazio Aiello*

Main category: cs.CR

TL;DR: 本文提出了一种基于SRAM的物理不可克隆函数（PUF）结合高可靠性比特选择算法和轻量级纠错码，为无电池能量收集物联网传感器节点（如心跳传感器）生成可靠的加密密钥，以解决其缺乏安全机制的问题。


<details>
  <summary>Details</summary>
Motivation: 无电池能量收集物联网传感器节点（如心跳传感器）虽然成本低且无需维护，但缺乏安全机制保护用户数据。数据加密和认证需要可靠的加密密钥，但生成此类密钥具有挑战性。

Method: 提出了一种基于SRAM的PUF，结合高可靠性比特选择算法和轻量级纠错码，利用心跳传感器微控制器开关电源的特性，满足SRAM断电读取随机值的要求。

Result: 系统已在STM32 Cortex M0+微控制器上评估，并成功应用于保护心跳传感器的重要数据。

Conclusion: 该方法为无电池物联网传感器节点提供了一种可靠且安全的加密密钥生成方案。

Abstract: Batteryless energy harvesting IoT sensor nodes such as beat sensors can be
deployed in millions without the need to replace batteries. They are
ultra-low-power and cost-effective wireless sensor nodes without the
maintenance cost and can work for 24 hours/365 days. However, they were not
equipped with security mechanisms to protect user data. Data encryption and
authentication can be used to secure beat sensor applications, but generating a
secure cryptographic key is challenging. In this paper, we proposed an
SRAM-based Physically Unclonable Function (PUF) combining a high-reliability
bit selection algorithm with a lightweight error-correcting code to generate
reliable secure keys for data encryption. The system employs a feature of beat
sensors, in which the microcontroller is powered on to transmit the ID signals
and then powered off. This fits the SRAM-based PUF requirement, which needs the
SRAM to be powered off to read out its random values. The proposed system has
been evaluated on STM32 Cortex M0+ microcontrollers and has been implemented to
protect important data on beat sensors.

</details>


### [27] [Chimera: Harnessing Multi-Agent LLMs for Automatic Insider Threat Simulation](https://arxiv.org/abs/2508.07745)
*Jiongchi Yu,Xiaofei Xie,Qiang Hu,Yuhan Ma,Ziming Zhao*

Main category: cs.CR

TL;DR: 提出Chimera框架，利用LLM多代理模拟企业内外部行为，生成高质量数据集ChimeraLog，提升内部威胁检测研究。


<details>
  <summary>Details</summary>
Motivation: 解决内部威胁检测研究中高质量数据稀缺的问题，现有公开数据集规模有限或缺乏真实语义。

Method: 采用基于LLM的多代理框架，模拟员工角色行为，集成会议、互动和调度模块，覆盖15种攻击类型。

Result: 生成数据集ChimeraLog，经评估显示多样性和真实性，现有检测方法F1分数显著低于CERT数据集。

Conclusion: ChimeraLog为内部威胁检测研究提供了更高质量的数据支持，推动了该领域的进展。

Abstract: Insider threats, which can lead to severe losses, remain a major security
concern. While machine learning-based insider threat detection (ITD) methods
have shown promising results, their progress is hindered by the scarcity of
high-quality data. Enterprise data is sensitive and rarely accessible, while
publicly available datasets, when limited in scale due to cost, lack sufficient
real-world coverage; and when purely synthetic, they fail to capture rich
semantics and realistic user behavior. To address this, we propose Chimera, the
first large language model (LLM)-based multi-agent framework that automatically
simulates both benign and malicious insider activities and collects diverse
logs across diverse enterprise environments. Chimera models each employee with
agents that have role-specific behavior and integrates modules for group
meetings, pairwise interactions, and autonomous scheduling, capturing realistic
organizational dynamics. It incorporates 15 types of insider attacks (e.g., IP
theft, system sabotage) and has been deployed to simulate activities in three
sensitive domains: technology company, finance corporation, and medical
institution, producing a new dataset, ChimeraLog. We assess ChimeraLog via
human studies and quantitative analysis, confirming its diversity, realism, and
presence of explainable threat patterns. Evaluations of existing ITD methods
show an average F1-score of 0.83, which is significantly lower than 0.99 on the
CERT dataset, demonstrating ChimeraLog's higher difficulty and utility for
advancing ITD research.

</details>


### [28] [A Comparative Analysis of Lightweight Hash Functions Using AVR ATXMega128 and ChipWhisperer](https://arxiv.org/abs/2508.07840)
*Mohsin Khan,Dag Johansen,Håvard Dagenborg*

Main category: cs.CR

TL;DR: 本文比较了22种轻量级哈希函数的性能，包括SHA-3决赛算法，使用AVR微控制器和ChipWhisperer平台评估速度、内存占用和能耗。


<details>
  <summary>Details</summary>
Motivation: 轻量级哈希函数在嵌入式系统和物联网中至关重要，但缺乏全面的性能比较。

Method: 采用AVR ATXMega128微控制器和ChipWhisperer平台，结合E-RANK指标，评估哈希函数的速度、内存和能耗。

Result: 提供了各哈希函数在速度、内存和能耗方面的详细比较数据。

Conclusion: 通过E-RANK指标，为开发者选择哈希函数提供了新的权衡视角。

Abstract: Lightweight hash functions have become important building blocks for security
in embedded and IoT systems. A plethora of algorithms have been proposed and
standardized, providing a wide range of performance trade-off options for
developers to choose from. This paper presents a comparative analysis of 22 key
software-based lightweight hash functions, including the finalist from the
SHA-3 competition. We use a novel benchmark methodology that combines an AVR
ATXMega128 microcontroller with the ChipWhisperer cryptanalysis platform and
evaluate and compare the various hash functions along several dimensions,
including execution speed, % measured in Cycles per Byte (CpB), memory
footprint, and energy consumption. Using the composite E-RANK metric, we
provide new insight into the various trade-offs each hash function offers to
system developers.

</details>


### [29] [EFU: Enforcing Federated Unlearning via Functional Encryption](https://arxiv.org/abs/2508.07873)
*Samaneh Mohammadi,Vasileios Tsouvalas,Iraklis Symeonidis,Ali Balador,Tanir Ozcelebi,Francesco Flammini,Nirvana Meratnia*

Main category: cs.CR

TL;DR: EFU是一个加密强制的联邦遗忘框架，允许客户端在不暴露遗忘意图的情况下从协作训练模型中移除其数据影响。


<details>
  <summary>Details</summary>
Motivation: 现有联邦遗忘方法依赖服务器端合作，可能泄露客户意图和身份，损害隐私和自主性。

Method: EFU利用功能加密绑定加密更新到特定聚合函数，防止服务器检测或跳过遗忘请求，并结合对抗样本和参数重要性正则化掩盖行为变化。

Result: 实验表明，EFU在遗忘数据上接近随机准确率，同时保持与完全重新训练相当的性能，且隐藏遗忘意图。

Conclusion: EFU是一种通用、安全且可验证的联邦遗忘框架，适用于任何客户端遗忘机制。

Abstract: Federated unlearning (FU) algorithms allow clients in federated settings to
exercise their ''right to be forgotten'' by removing the influence of their
data from a collaboratively trained model. Existing FU methods maintain data
privacy by performing unlearning locally on the client-side and sending
targeted updates to the server without exposing forgotten data; yet they often
rely on server-side cooperation, revealing the client's intent and identity
without enforcement guarantees - compromising autonomy and unlearning privacy.
In this work, we propose EFU (Enforced Federated Unlearning), a
cryptographically enforced FU framework that enables clients to initiate
unlearning while concealing its occurrence from the server. Specifically, EFU
leverages functional encryption to bind encrypted updates to specific
aggregation functions, ensuring the server can neither perform unauthorized
computations nor detect or skip unlearning requests. To further mask behavioral
and parameter shifts in the aggregated model, we incorporate auxiliary
unlearning losses based on adversarial examples and parameter importance
regularization. Extensive experiments show that EFU achieves near-random
accuracy on forgotten data while maintaining performance comparable to full
retraining across datasets and neural architectures - all while concealing
unlearning intent from the server. Furthermore, we demonstrate that EFU is
agnostic to the underlying unlearning algorithm, enabling secure,
function-hiding, and verifiable unlearning for any client-side FU mechanism
that issues targeted updates.

</details>


### [30] [Robust Anomaly Detection in O-RAN: Leveraging LLMs against Data Manipulation Attacks](https://arxiv.org/abs/2508.08029)
*Thusitha Dayaratne,Ngoc Duy Pham,Viet Vo,Shangqi Lai,Sharif Abuadbba,Hajime Suzuki,Xingliang Yuan,Carsten Rudolph*

Main category: cs.CR

TL;DR: 5G和O-RAN架构的灵活性带来了新的安全挑战，如恶意xApps通过Unicode篡改数据攻击SDL。传统ML方法易受攻击崩溃，而LLMs能稳健处理篡改数据且延迟低，适合Near-RT RIC部署。


<details>
  <summary>Details</summary>
Motivation: 5G和O-RAN架构的开放性和复杂性引入了新的安全威胁，如恶意xApps对SDL的数据篡改攻击，传统ML方法无法有效应对。

Method: 研究使用LLMs进行异常检测，评估其在处理篡改数据时的稳健性和延迟表现。

Result: LLMs能稳健处理篡改数据且延迟低（<0.07秒），适合Near-RT RIC部署，但初始检测精度需提升。

Conclusion: LLMs在对抗Unicode篡改攻击中表现稳健，适合O-RAN架构，未来可通过提示工程进一步提升精度。

Abstract: The introduction of 5G and the Open Radio Access Network (O-RAN) architecture
has enabled more flexible and intelligent network deployments. However, the
increased complexity and openness of these architectures also introduce novel
security challenges, such as data manipulation attacks on the semi-standardised
Shared Data Layer (SDL) within the O-RAN platform through malicious xApps. In
particular, malicious xApps can exploit this vulnerability by introducing
subtle Unicode-wise alterations (hypoglyphs) into the data that are being used
by traditional machine learning (ML)-based anomaly detection methods. These
Unicode-wise manipulations can potentially bypass detection and cause failures
in anomaly detection systems based on traditional ML, such as AutoEncoders,
which are unable to process hypoglyphed data without crashing. We investigate
the use of Large Language Models (LLMs) for anomaly detection within the O-RAN
architecture to address this challenge. We demonstrate that LLM-based xApps
maintain robust operational performance and are capable of processing
manipulated messages without crashing. While initial detection accuracy
requires further improvements, our results highlight the robustness of LLMs to
adversarial attacks such as hypoglyphs in input data. There is potential to use
their adaptability through prompt engineering to further improve the accuracy,
although this requires further research. Additionally, we show that LLMs
achieve low detection latency (under 0.07 seconds), making them suitable for
Near-Real-Time (Near-RT) RIC deployments.

</details>


### [31] [IPBA: Imperceptible Perturbation Backdoor Attack in Federated Self-Supervised Learning](https://arxiv.org/abs/2508.08031)
*Jiayao Wang,Yang Song,Zhendong Zhao,Jiale Zhang,Qilin Wu,Junwu Zhu,Dongfang Zhao*

Main category: cs.CR

TL;DR: 本文提出了一种针对联邦自监督学习（FSSL）的隐蔽且有效的后门攻击方法IPBA，解决了现有方法在隐蔽性和实用性上的不足。


<details>
  <summary>Details</summary>
Motivation: FSSL结合了去中心化建模和无监督表示学习的优势，但研究表明其易受后门攻击，现有方法隐蔽性不足。

Method: IPBA通过解耦后门样本和增强样本的特征分布，并引入Sliced-Wasserstein距离优化触发生成过程。

Result: 实验表明，IPBA在多个FSSL场景和数据集中显著优于现有后门攻击方法，且对各种防御机制表现出强鲁棒性。

Conclusion: IPBA为FSSL中的后门攻击提供了更隐蔽和实用的解决方案。

Abstract: Federated self-supervised learning (FSSL) combines the advantages of
decentralized modeling and unlabeled representation learning, serving as a
cutting-edge paradigm with strong potential for scalability and privacy
preservation. Although FSSL has garnered increasing attention, research
indicates that it remains vulnerable to backdoor attacks. Existing methods
generally rely on visually obvious triggers, which makes it difficult to meet
the requirements for stealth and practicality in real-world deployment. In this
paper, we propose an imperceptible and effective backdoor attack method against
FSSL, called IPBA. Our empirical study reveals that existing imperceptible
triggers face a series of challenges in FSSL, particularly limited
transferability, feature entanglement with augmented samples, and
out-of-distribution properties. These issues collectively undermine the
effectiveness and stealthiness of traditional backdoor attacks in FSSL. To
overcome these challenges, IPBA decouples the feature distributions of backdoor
and augmented samples, and introduces Sliced-Wasserstein distance to mitigate
the out-of-distribution properties of backdoor samples, thereby optimizing the
trigger generation process. Our experimental results on several FSSL scenarios
and datasets show that IPBA significantly outperforms existing backdoor attack
methods in performance and exhibits strong robustness under various defense
mechanisms.

</details>


### [32] [False Reality: Uncovering Sensor-induced Human-VR Interaction Vulnerability](https://arxiv.org/abs/2508.08043)
*Yancheng Jiang,Yan Jiang,Ruochen Zhou,Yi-Chao Chen,Xiaoyu Ji,Wenyuan Xu*

Main category: cs.CR

TL;DR: 论文首次系统分析了针对VR系统的物理攻击，提出了一种名为“False Reality”的新型攻击威胁，无需修改软件即可干扰VR服务，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: VR技术广泛应用于多个领域，但现有攻击通常需要高权限和专业知识。本文旨在探索无需软件修改即可实施的物理攻击，以提升VR系统的安全性。

Method: 通过篡改传感器数据干扰VR服务，利用感知和心理效应欺骗用户，提出攻击路径框架并通过实验验证。

Result: 在五种商用VR设备上验证了三种代表性攻击路径，证明False Reality能诱导用户产生有害行为。

Conclusion: 研究为未来VR系统的安全性和韧性提供了重要见解，并提出了一种防御原型。

Abstract: Virtual Reality (VR) techniques, serving as the bridge between the real and
virtual worlds, have boomed and are widely used in manufacturing, remote
healthcare, gaming, etc. Specifically, VR systems offer users immersive
experiences that include both perceptions and actions. Various studies have
demonstrated that attackers can manipulate VR software to influence users'
interactions, including perception and actions. However, such attacks typically
require strong access and specialized expertise. In this paper, we are the
first to present a systematic analysis of physical attacks against VR systems
and introduce False Reality, a new attack threat to VR devices without
requiring access to or modification of their software. False Reality disturbs
VR system services by tampering with sensor measurements, and further spoofing
users' perception even inducing harmful actions, e.g., inducing dizziness or
causing users to crash into obstacles, by exploiting perceptual and
psychological effects. We formalize these threats through an attack pathway
framework and validate three representative pathways via physical experiments
and user studies on five commercial VR devices. Finally, we further propose a
defense prototype to mitigate such threats. Our findings shall provide valuable
insights for enhancing the security and resilience of future VR systems.

</details>


### [33] [Fully-Fluctuating Participation in Sleepy Consensus](https://arxiv.org/abs/2508.08068)
*Yuval Efron,Joachim Neu,Toniann Pitassi*

Main category: cs.CR

TL;DR: 论文提出了一种新的对手模型——外部对手模型，解决了现有睡眠模型中协议无法应对极端参与波动的问题，同时保持了效率和抗腐蚀能力。


<details>
  <summary>Details</summary>
Motivation: 现有睡眠模型中的协议无法像比特币那样在极端参与波动下保持安全性和鲁棒性，因此需要一种新的对手模型来解决这一问题。

Method: 提出外部对手模型，假设腐败节点不会泄露其私钥信息，从而在睡眠模型中实现安全性和效率的平衡。

Result: 在该模型下，睡眠模型中的协议能够在不牺牲效率或抗腐蚀能力的情况下，应对完全波动的参与水平。

Conclusion: 外部对手模型不仅自然且理论上有吸引力，还绕过了先前研究中提出的障碍，为睡眠模型的安全性提供了新的解决方案。

Abstract: Proof-of-work allows Bitcoin to boast security amidst arbitrary fluctuations
in participation of miners throughout time, so long as, at any point in time, a
majority of hash power is honest. In recent years, however, the pendulum has
shifted in favor of proof-of-stake-based consensus protocols. There, the sleepy
model is the most prominent model for handling fluctuating participation of
nodes. However, to date, no protocol in the sleepy model rivals Bitcoin in its
robustness to drastic fluctuations in participation levels, with
state-of-the-art protocols making various restrictive assumptions. In this
work, we present a new adversary model, called external adversary. Intuitively,
in our model, corrupt nodes do not divulge information about their secret keys.
In this model, we show that protocols in the sleepy model can meaningfully
claim to remain secure against fully fluctuating participation, without
compromising efficiency or corruption resilience. Our adversary model is quite
natural, and arguably naturally captures the process via which malicious
behavior arises in protocols, as opposed to traditional worst-case modeling. On
top of which, the model is also theoretically appealing, circumventing a
barrier established in a recent work of Malkhi, Momose, and Ren.

</details>


### [34] [Differential Privacy for Regulatory Compliance in Cyberattack Detection on Critical Infrastructure Systems](https://arxiv.org/abs/2508.08190)
*Paritosh Ramanan,H. M. Mohaimanul Islam,Abhiram Reddy Alugula*

Main category: cs.CR

TL;DR: 论文提出了一种基于差分隐私假设测试的网络攻击检测框架，旨在增强监管信心并缓解关键基础设施网络（CIN）利益相关者的隐私问题。


<details>
  <summary>Details</summary>
Motivation: 随着网络攻击风险的增加，关键基础设施系统面临更严格的监管合规要求，但利益相关者担心数据隐私泄露。

Method: 采用两阶段隐私保护方案，保护协方差及相关传感器驱动测试统计数据的隐私。

Result: 理论证明该方法在非差分隐私情况下具有可比的误分类率，并提供强隐私保证；实际数据集验证了其可靠性。

Conclusion: 该框架在保护隐私的同时，有效检测网络攻击，适用于多利益相关者的关键基础设施系统。

Abstract: Industrial control systems are a fundamental component of critical
infrastructure networks (CIN) such as gas, water and power. With the growing
risk of cyberattacks, regulatory compliance requirements are also increasing
for large scale critical infrastructure systems comprising multiple utility
stakeholders. The primary goal of regulators is to ensure overall system
stability with recourse to trustworthy stakeholder attack detection. However,
adhering to compliance requirements requires stakeholders to also disclose
sensor and control data to regulators raising privacy concerns. In this paper,
we present a cyberattack detection framework that utilizes differentially
private (DP) hypothesis tests geared towards enhancing regulatory confidence
while alleviating privacy concerns of CIN stakeholders. The hallmark of our
approach is a two phase privacy scheme that protects the privacy of covariance,
as well as the associated sensor driven test statistics computed as a means to
generate alarms. Theoretically, we show that our method induces a
misclassification error rate comparable to the non-DP cases while delivering
robust privacy guarantees. With the help of real-world datasets, we show the
reliability of our DP-detection outcomes for a wide variety of attack scenarios
for interdependent stakeholders.

</details>


### [35] [AuthPrint: Fingerprinting Generative Models Against Malicious Model Providers](https://arxiv.org/abs/2508.05691)
*Kai Yao,Marc Juarez*

Main category: cs.CR

TL;DR: 论文提出了一种对抗性模型指纹技术，用于验证生成模型输出的来源，即使在模型提供者可能对抗的情况下也能有效工作。


<details>
  <summary>Details</summary>
Motivation: 当前生成模型在高风险领域应用广泛，但缺乏验证输出来源的机制，尤其是在模型提供者可能对抗的情况下。

Method: 通过可信验证器从模型输出空间中提取秘密指纹，并训练模型预测和验证这些指纹，以对抗模型提供者的潜在对抗行为。

Result: 实验表明，该方法在GAN和扩散模型上实现了接近零的FPR@95%TPR，即使对原始架构和训练数据进行小修改也能保持鲁棒性。

Conclusion: 该方法为生成模型的来源验证提供了有效的解决方案，即使在对抗性环境下也能保持高准确性和鲁棒性。

Abstract: Generative models are increasingly adopted in high-stakes domains, yet
current deployments offer no mechanisms to verify the origin of model outputs.
We address this gap by extending model fingerprinting techniques beyond the
traditional collaborative setting to one where the model provider may act
adversarially. To our knowledge, this is the first work to evaluate
fingerprinting for provenance attribution under such a threat model. The
methods rely on a trusted verifier that extracts secret fingerprints from the
model's output space, unknown to the provider, and trains a model to predict
and verify them. Our empirical evaluation shows that our methods achieve
near-zero FPR@95%TPR for instances of GAN and diffusion models, even when
tested on small modifications to the original architecture and training data.
Moreover, the methods remain robust against adversarial attacks that actively
modify the outputs to bypass detection. Source codes are available at
https://github.com/PSMLab/authprint.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [36] [Solving Pasur Using GPU-Accelerated Counterfactual Regret Minimization](https://arxiv.org/abs/2508.06559)
*Sina Baghal*

Main category: cs.AI

TL;DR: 本文介绍了一个基于CUDA加速的计算框架，用于模拟Pasur纸牌游戏，并通过Counterfactual Regret Minimization（CFR）计算近纳什均衡。框架通过高效内存管理和游戏树分解解决了复杂规则和大规模游戏树的挑战。


<details>
  <summary>Details</summary>
Motivation: Pasur游戏的复杂规则和大规模游戏树带来了独特的计算挑战，需要高效的内存管理和计算优化。

Method: 使用PyTorch CUDA张量处理规则复杂性，将游戏树分解为实际游戏状态和继承分数，并通过逐轮反向训练策略管理计算复杂度。

Result: 构建了包含超过10^9个节点的完整游戏树，并训练了一个基于树的模型来预测策略。通过大规模自对弈估计每副牌的公平价值。

Conclusion: 该框架可扩展到其他强化学习算法，适用于多轮分解的动作树场景，如回合制策略游戏或金融市场中的顺序交易决策。

Abstract: Pasur is a fishing card game played over six rounds and is played similarly
to games such as Cassino and Scopa, and Bastra. This paper introduces a
CUDA-accelerated computational framework for simulating Pasur, emphasizing
efficient memory management. We use our framework to compute near-Nash
equilibria via Counterfactual Regret Minimization (CFR), a well-known algorithm
for solving large imperfect-information games.
  Solving Pasur presents unique challenges due to its intricate rules and the
large size of its game tree. We handle rule complexity using PyTorch CUDA
tensors and to address the memory-intensive nature of the game, we decompose
the game tree into two key components: (1) actual game states, and (2)
inherited scores from previous rounds. We construct the Full Game Tree by
pairing card states with accumulated scores in the Unfolding Process. This
design reduces memory overhead by storing only essential strategy values and
node connections. To further manage computational complexity, we apply a
round-by-round backward training strategy, starting from the final round and
recursively propagating average utilities to earlier stages. Our approach
constructs the complete game tree, which on average consists of over $10^9$
nodes. We provide detailed implementation snippets.
  After computing a near-Nash equilibrium strategy, we train a tree-based model
to predict these strategies for use during gameplay. We then estimate the fair
value of each deck through large-scale self-play between equilibrium strategies
by simulating, for instance, 10,000 games per matchup, executed in parallel
using GPU acceleration.
  Similar frameworks can be extended to other reinforcement learning algorithms
where the action tree naturally decomposes into multiple rounds such as
turn-based strategy games or sequential trading decisions in financial markets.

</details>


### [37] [Operationalizing Serendipity: Multi-Agent AI Workflows for Enhanced Materials Characterization with Theory-in-the-Loop](https://arxiv.org/abs/2508.06569)
*Lance Yao,Suman Samantray,Ayana Ghosh,Kevin Roccapriore,Libor Kovarik,Sarah Allec,Maxim Ziatdinov*

Main category: cs.AI

TL;DR: SciLink是一个开源的多智能体AI框架，旨在通过自动化链接实验观察、新颖性评估和理论模拟，在材料研究中实现偶然发现。


<details>
  <summary>Details</summary>
Motivation: 现代自主实验室虽能加速假设验证，但效率优化可能忽略偶然发现。SciLink旨在填补这一空白，促进材料研究中的意外发现。

Method: 采用混合AI策略，结合机器学习模型进行定量分析和大型语言模型进行高级推理，将原始数据转化为可验证的科学主张，并根据文献评估新颖性。

Result: SciLink在多种研究场景中表现出色，包括原子分辨率和超光谱数据分析，并能整合实时专家指导和提出后续实验建议。

Conclusion: SciLink不仅提高效率，还主动培育偶然发现的环境，弥合自动化实验与开放式科学探索之间的差距。

Abstract: The history of science is punctuated by serendipitous discoveries, where
unexpected observations, rather than targeted hypotheses, opened new fields of
inquiry. While modern autonomous laboratories excel at accelerating hypothesis
testing, their optimization for efficiency risks overlooking these crucial,
unplanned findings. To address this gap, we introduce SciLink, an open-source,
multi-agent artificial intelligence framework designed to operationalize
serendipity in materials research by creating a direct, automated link between
experimental observation, novelty assessment, and theoretical simulations. The
framework employs a hybrid AI strategy where specialized machine learning
models perform quantitative analysis of experimental data, while large language
models handle higher-level reasoning. These agents autonomously convert raw
data from materials characterization techniques into falsifiable scientific
claims, which are then quantitatively scored for novelty against the published
literature. We demonstrate the framework's versatility across diverse research
scenarios, showcasing its application to atomic-resolution and hyperspectral
data, its capacity to integrate real-time human expert guidance, and its
ability to close the research loop by proposing targeted follow-up experiments.
By systematically analyzing all observations and contextualizing them, SciLink
provides a practical framework for AI-driven materials research that not only
enhances efficiency but also actively cultivates an environment ripe for
serendipitous discoveries, thereby bridging the gap between automated
experimentation and open-ended scientific exploration.

</details>


### [38] [IRL-VLA: Training an Vision-Language-Action Policy via Reward World Model](https://arxiv.org/abs/2508.06571)
*Anqing Jiang,Yu Gao,Yiru Wang,Zhigang Sun,Shuo Wang,Yuwen Heng,Hao Sun,Shichen Tang,Lijuan Zhu,Jinhao Chai,Jijun Wang,Zichong Gu,Hao Jiang,Li Sun*

Main category: cs.AI

TL;DR: 论文提出IRL-VLA框架，通过逆强化学习奖励世界模型和自建VLA方法解决自动驾驶中VLA模型的闭环训练问题，并在NAVSIM v2和CVPR2025竞赛中取得优异表现。


<details>
  <summary>Details</summary>
Motivation: 现有VLA架构基于开环模仿学习，性能受限；闭环训练依赖高保真传感器模拟，存在领域差距和计算效率问题。

Method: 三阶段框架：1) 预训练VLA策略；2) 通过逆强化学习构建轻量级奖励世界模型；3) 使用PPO优化奖励模型指导的强化学习。

Result: 在NAVSIM v2端到端驾驶基准和CVPR2025竞赛中取得领先表现。

Conclusion: IRL-VLA框架为闭环自动驾驶中的VLA研究提供了高效解决方案。

Abstract: Vision-Language-Action (VLA) models have demonstrated potential in autonomous
driving. However, two critical challenges hinder their development: (1)
Existing VLA architectures are typically based on imitation learning in
open-loop setup which tends to capture the recorded behaviors in the dataset,
leading to suboptimal and constrained performance, (2) Close-loop training
relies heavily on high-fidelity sensor simulation, where domain gaps and
computational inefficiencies pose significant barriers. In this paper, we
introduce IRL-VLA, a novel close-loop Reinforcement Learning via
\textbf{I}nverse \textbf{R}einforcement \textbf{L}earning reward world model
with a self-built VLA approach. Our framework proceeds in a three-stage
paradigm: In the first stage, we propose a VLA architecture and pretrain the
VLA policy via imitation learning. In the second stage, we construct a
lightweight reward world model via inverse reinforcement learning to enable
efficient close-loop reward computation. To further enhance planning
performance, finally, we design specialized reward world model guidence
reinforcement learning via PPO(Proximal Policy Optimization) to effectively
balance the safety incidents, comfortable driving, and traffic efficiency. Our
approach achieves state-of-the-art performance in NAVSIM v2 end-to-end driving
benchmark, 1st runner up in CVPR2025 Autonomous Grand Challenge. We hope that
our framework will accelerate VLA research in close-loop autonomous driving.

</details>


### [39] [CountQA: How Well Do MLLMs Count in the Wild?](https://arxiv.org/abs/2508.06585)
*Jayant Sravan Tamarapalli,Rynaa Grover,Nilay Pande,Sahiti Yerramilli*

Main category: cs.AI

TL;DR: 多模态大语言模型（MLLMs）在视觉场景理解上表现流畅，但在对象计数能力上存在严重缺陷。CountQA是一个新基准，用于评估和改善这一缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在对象计数能力上表现不佳，限制了其实际应用。现有基准测试无法真实反映复杂场景下的计数能力。

Method: 引入CountQA基准，包含1500多个问答对，涵盖高密度、遮挡和杂乱的现实图像。评估了15种主流MLLMs。

Result: 表现最佳的模型准确率仅为42.9%，且随着对象数量增加，性能下降。

Conclusion: CountQA为改进MLLMs的计数能力提供了基准，推动其在实际应用中的可靠性。

Abstract: Multimodal Large Language Models (MLLMs) demonstrate remarkable fluency in
understanding visual scenes, yet they exhibit a critical lack in a fundamental
cognitive skill: object counting. This blind spot severely limits their
reliability in real-world applications. To date, this capability has been
largely unevaluated in complex scenarios, as existing benchmarks either feature
sparse object densities or are confined to specific visual domains, failing to
test models under realistic conditions. Addressing this gap, we introduce
CountQA, a challenging new benchmark designed to probe this deficiency.
Comprising over 1,500 question-answer pairs, CountQA features real-world images
with high object density, clutter, and occlusion. We investigate this weakness
by evaluating 15 prominent MLLMs on the CountQA benchmark and reveal that the
top-performing model achieves a mere 42.9% accuracy, with performance declining
as object counts rise. By providing a dedicated benchmark to diagnose and
rectify this core weakness, CountQA paves the way for a new generation of MLLMs
that are not only descriptively fluent but also numerically grounded and
spatially aware. We will open-source the dataset and code upon paper acceptance
to foster further research.

</details>


### [40] [Formal Concept Analysis: a Structural Framework for Variability Extraction and Analysis](https://arxiv.org/abs/2508.06668)
*Jessie Galasso*

Main category: cs.AI

TL;DR: 本文总结了形式概念分析（FCA）在变异性分析中的关键属性及其应用方法。


<details>
  <summary>Details</summary>
Motivation: FCA在知识表示和发现中具有潜力，但其数学基础文献使其在变异性任务中的应用不够直观。本文旨在填补这一空白。

Method: 通过筛选FCA框架中与变异性分析相关的关键属性，并解释其在概念结构中的应用。

Result: 明确了FCA中可用于变异性分析的关键属性及其解释方法。

Conclusion: 本文为FCA在变异性分析中的应用提供了实用指导。

Abstract: Formal Concept Analysis (FCA) is a mathematical framework for knowledge
representation and discovery. It performs a hierarchical clustering over a set
of objects described by attributes, resulting in conceptual structures in which
objects are organized depending on the attributes they share. These conceptual
structures naturally highlight commonalities and variabilities among similar
objects by categorizing them into groups which are then arranged by similarity,
making it particularly appropriate for variability extraction and analysis.
Despite the potential of FCA, determining which of its properties can be
leveraged for variability-related tasks (and how) is not always
straightforward, partly due to the mathematical orientation of its foundational
literature. This paper attempts to bridge part of this gap by gathering a
selection of properties of the framework which are essential to variability
analysis, and how they can be used to interpret diverse variability information
within the resulting conceptual structures.

</details>


### [41] [Zero-Shot Cellular Trajectory Map Matching](https://arxiv.org/abs/2508.06674)
*Weijie Shi,Yue Cui,Hao Chen,Jiaming Li,Mengze Li,Jia Zhu,Jiajie Xu,Xiaofang Zhou*

Main category: cs.AI

TL;DR: 提出了一种基于像素的轨迹校准辅助方法，用于零样本CTMM，通过迁移地理空间知识校准轨迹，并在路网层面引导路径查找。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖ID特征和区域特定数据，限制了在未探索区域的适应性，需要一种无需额外训练的高精度CTMM方法。

Method: 结合高斯混合模型和VAE提取场景自适应特征，设计时空感知模块捕获序列特征和位置不确定性，使用约束路径查找算法重建路网ID序列。

Result: 实验表明，模型在零样本CTMM中性能优于现有方法16.8%。

Conclusion: 该方法通过知识迁移和路径优化，显著提升了零样本CTMM的准确性和适应性。

Abstract: Cellular Trajectory Map-Matching (CTMM) aims to align cellular location
sequences to road networks, which is a necessary preprocessing in
location-based services on web platforms like Google Maps, including navigation
and route optimization. Current approaches mainly rely on ID-based features and
region-specific data to learn correlations between cell towers and roads,
limiting their adaptability to unexplored areas. To enable high-accuracy CTMM
without additional training in target regions, Zero-shot CTMM requires to
extract not only region-adaptive features, but also sequential and location
uncertainty to alleviate positioning errors in cellular data. In this paper, we
propose a pixel-based trajectory calibration assistant for zero-shot CTMM,
which takes advantage of transferable geospatial knowledge to calibrate
pixelated trajectory, and then guide the path-finding process at the road
network level. To enhance knowledge sharing across similar regions, a Gaussian
mixture model is incorporated into VAE, enabling the identification of
scenario-adaptive experts through soft clustering. To mitigate high positioning
errors, a spatial-temporal awareness module is designed to capture sequential
features and location uncertainty, thereby facilitating the inference of
approximate user positions. Finally, a constrained path-finding algorithm is
employed to reconstruct the road ID sequence, ensuring topological validity
within the road network. This process is guided by the calibrated trajectory
while optimizing for the shortest feasible path, thus minimizing unnecessary
detours. Extensive experiments demonstrate that our model outperforms existing
methods in zero-shot CTMM by 16.8\%.

</details>


### [42] [Probabilistic Circuits for Knowledge Graph Completion with Reduced Rule Sets](https://arxiv.org/abs/2508.06706)
*Jaikrishna Manojkumar Patil,Nathaniel Lee,Al Mehdi Saadat Chowdhury,YooJung Choi,Paulo Shakarian*

Main category: cs.AI

TL;DR: 论文提出了一种基于规则上下文和概率电路的知识图谱补全方法，显著减少了规则数量，同时保持高性能。


<details>
  <summary>Details</summary>
Motivation: 规则补全方法需要大量规则才能达到高性能，但过多的规则会降低可解释性。

Method: 从训练数据中发现规则上下文，并利用概率电路对这些上下文建模，以减少规则数量并提升性能。

Result: 规则数量减少70-96%，性能提升31倍，保留基线91%的峰值性能。

Conclusion: 该方法在8个标准数据集上验证有效，为基于规则的推理提供了新思路。

Abstract: Rule-based methods for knowledge graph completion provide explainable results
but often require a significantly large number of rules to achieve competitive
performance. This can hinder explainability due to overwhelmingly large rule
sets. We discover rule contexts (meaningful subsets of rules that work
together) from training data and use learned probability distribution (i.e.
probabilistic circuits) over these rule contexts to more rapidly achieve
performance of the full rule set. Our approach achieves a 70-96% reduction in
number of rules used while outperforming baseline by up to 31$\times$ when
using equivalent minimal number of rules and preserves 91% of peak baseline
performance even when comparing our minimal rule sets against baseline's full
rule sets. We show that our framework is grounded in well-known semantics of
probabilistic logic, does not require independence assumptions, and that our
tractable inference procedure provides both approximate lower bounds and exact
probability of a given query. The efficacy of our method is validated by
empirical studies on 8 standard benchmark datasets where we show competitive
performance by using only a fraction of the rules required by AnyBURL's
standard inference method, the current state-of-the-art for rule-based
knowledge graph completion. This work may have further implications for general
probabilistic reasoning over learned sets of rules.

</details>


### [43] [GLIDR: Graph-Like Inductive Logic Programming with Differentiable Reasoning](https://arxiv.org/abs/2508.06716)
*Blair Johnson,Clayton Kerce,Faramarz Fekri*

Main category: cs.AI

TL;DR: GLIDR是一种可微分的规则学习方法，通过更丰富的语法和推理算法提升知识图谱任务的性能与可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有链式规则结构限制了性能与可解释性，需要更灵活的规则学习方法。

Method: GLIDR使用可微分消息传递推理算法，支持分支和循环等复杂规则结构。

Result: GLIDR在知识图谱补全任务中显著优于现有方法，甚至可与嵌入方法竞争。

Conclusion: GLIDR具有高性能、鲁棒性，并能与其他深度学习模型结合进行端到端优化。

Abstract: Differentiable inductive logic programming (ILP) techniques have proven
effective at finding approximate rule-based solutions to link prediction and
node classification problems on knowledge graphs; however, the common
assumption of chain-like rule structure can hamper the performance and
interpretability of existing approaches. We introduce GLIDR, a differentiable
rule learning method that models the inference of logic rules with more
expressive syntax than previous methods. GLIDR uses a differentiable message
passing inference algorithm that generalizes previous chain-like rule learning
methods to allow rules with features like branches and cycles. GLIDR has a
simple and expressive rule search space which is parameterized by a limit on
the maximum number of free variables that may be included in a rule. Explicit
logic rules can be extracted from the weights of a GLIDR model for use with
symbolic solvers. We demonstrate that GLIDR can significantly outperform
existing rule learning methods on knowledge graph completion tasks and even
compete with embedding methods despite the inherent disadvantage of being a
structure-only prediction method. We show that rules extracted from GLIDR
retain significant predictive performance, and that GLIDR is highly robust to
training data noise. Finally, we demonstrate that GLIDR can be chained with
deep neural networks and optimized end-to-end for rule learning on arbitrary
data modalities.

</details>


### [44] [ParBalans: Parallel Multi-Armed Bandits-based Adaptive Large Neighborhood Search](https://arxiv.org/abs/2508.06736)
*Alican Yilmaz,Junyang Cai,Serdar Kadioglu,Bistra Dilkina*

Main category: cs.AI

TL;DR: ParBalans通过并行化扩展Balans，提升混合整数规划问题的求解效率，表现优于商业求解器Gurobi。


<details>
  <summary>Details</summary>
Motivation: 混合整数规划问题计算资源需求高，并行化是加速求解的关键策略。

Method: 提出ParBalans，结合求解器级和算法级并行化，优化Balans的性能。

Result: 实验显示ParBalans在复杂问题上表现优于Gurobi。

Conclusion: ParBalans为混合整数规划问题提供了一种高效的并行化解决方案。

Abstract: Solving Mixed-Integer Programming (MIP) problems often requires substantial
computational resources due to their combinatorial nature. Parallelization has
emerged as a critical strategy to accelerate solution times and enhance
scalability to tackle large, complex instances. This paper investigates the
parallelization capabilities of Balans, a recently proposed multi-armed
bandits-based adaptive large neighborhood search for MIPs. While Balans's
modular architecture inherently supports parallel exploration of diverse
parameter configurations, this potential has not been thoroughly examined. To
address this gap, we introduce ParBalans, an extension that leverages both
solver-level and algorithmic-level parallelism to improve performance on
challenging MIP instances. Our experimental results demonstrate that ParBalans
exhibits competitive performance compared to the state-of-the-art commercial
solver Gurobi, particularly on hard optimization benchmarks.

</details>


### [45] [Topology Generation of UAV Covert Communication Networks: A Graph Diffusion Approach with Incentive Mechanism](https://arxiv.org/abs/2508.06746)
*Xin Tang,Qian Chen,Fengshun Li,Youchun Gong,Yinqiu Liu,Wen Tian,Shaowen Qin,Xiaohuan Li*

Main category: cs.AI

TL;DR: 论文提出了一种结合图扩散策略优化（GDPO）和Stackelberg博弈（SG）激励机制的自组织无人机网络框架，以解决动态移动性和暴露风险带来的挑战。


<details>
  <summary>Details</summary>
Motivation: 随着无人机网络在敏感应用中的需求增长，确保可靠连接和隐蔽通信变得至关重要。动态移动性和暴露风险是主要挑战。

Method: 采用GDPO方法生成稀疏但连接良好的拓扑结构，并结合SG激励机制引导无人机选择支持合作和隐蔽通信的行为。

Result: 实验验证了框架在模型收敛性、拓扑生成质量和隐蔽通信性能提升方面的有效性。

Conclusion: 提出的框架能有效应对无人机网络的动态性和隐蔽通信需求。

Abstract: With the growing demand for Uncrewed Aerial Vehicle (UAV) networks in
sensitive applications, such as urban monitoring, emergency response, and
secure sensing, ensuring reliable connectivity and covert communication has
become increasingly vital. However, dynamic mobility and exposure risks pose
significant challenges. To tackle these challenges, this paper proposes a
self-organizing UAV network framework combining Graph Diffusion-based Policy
Optimization (GDPO) with a Stackelberg Game (SG)-based incentive mechanism. The
GDPO method uses generative AI to dynamically generate sparse but
well-connected topologies, enabling flexible adaptation to changing node
distributions and Ground User (GU) demands. Meanwhile, the Stackelberg Game
(SG)-based incentive mechanism guides self-interested UAVs to choose relay
behaviors and neighbor links that support cooperation and enhance covert
communication. Extensive experiments are conducted to validate the
effectiveness of the proposed framework in terms of model convergence, topology
generation quality, and enhancement of covert communication performance.

</details>


### [46] [Pushing the Envelope of LLM Inference on AI-PC](https://arxiv.org/abs/2508.06753)
*Evangelos Georganas,Dhiraj Kalamkar,Alexander Heinecke*

Main category: cs.AI

TL;DR: 论文提出了一种针对超低比特（1/1.58/2-bit）LLM模型的优化推理运行时，通过设计高效微内核并在PyTorch-TPP框架中实现，显著提升了推理性能。


<details>
  <summary>Details</summary>
Motivation: 超低比特LLM模型在资源受限环境（如边缘设备和AI PC）中具有潜力，但现有推理运行时的计算效率尚未充分探索。

Method: 设计并实现了针对现代CPU优化的1-bit和2-bit微内核，并将其集成到PyTorch-TPP框架中。

Result: 2-bit模型推理性能比当前SOTA运行时bitnet.cpp快2.2倍，比16-bit模型推理快7倍。

Conclusion: 优化后的运行时为超低比特LLM模型的高效部署铺平了道路，适用于AI PC和边缘设备。

Abstract: The advent of ultra-low-bit LLM models (1/1.58/2-bit), which match the
perplexity and end-task performance of their full-precision counterparts using
the same model size, is ushering in a new era of LLM inference for
resource-constrained environments such as edge devices and AI PCs. While these
quantization advances promise models that are more cost-effective in terms of
latency, memory, throughput, and energy consumption, the computational
efficiency of state-of-the-art (SOTA) inference runtimes (e.g., bitnet.cpp)
used to deploy them remains underexplored. In this work, we take a bottom-up
approach: we first design and implement 1-bit and 2-bit microkernels optimized
for modern CPUs, achieving peak computational efficiency across a variety of
CPU platforms. We integrate these microkernels into a state-of-the-art LLM
inference framework, namely PyTorch-TPP, and present end-to-end inference
results with 2-bit models that outperform the current SOTA runtime bitnet.cpp
by up to 2.2x, and deliver up to 7x speedup compared to the 16-bit model
inference. Our optimized runtime advances the state of LLM inference on AI PCs
and edge devices, paving the way for efficient deployment of ultra-low-bit LLM
models.

</details>


### [47] [A Fuzzy Logic Prompting Framework for Large Language Models in Adaptive and Uncertain Tasks](https://arxiv.org/abs/2508.06754)
*Vanessa Figueiredo*

Main category: cs.AI

TL;DR: 提出了一种模块化提示框架，支持在大语言模型（LLMs）中更安全、更自适应地处理动态用户任务。


<details>
  <summary>Details</summary>
Motivation: 基于人类学习理论（如最近发展区ZPD），旨在提升LLMs在动态任务中的适应性和安全性。

Method: 结合自然语言边界提示与控制模式，采用模糊支架逻辑和适应规则，无需微调或外部协调。

Result: 在模拟智能辅导环境中，框架显著提升了支架质量、适应性和教学对齐性，优于标准提示方法。

Conclusion: 该框架不仅适用于教育领域，还可扩展到其他交互密集型任务，为不确定或动态场景中的LLM行为提供了可重用方法。

Abstract: We introduce a modular prompting framework that supports safer and more
adaptive use of large language models (LLMs) across dynamic, user-centered
tasks. Grounded in human learning theory, particularly the Zone of Proximal
Development (ZPD), our method combines a natural language boundary prompt with
a control schema encoded with fuzzy scaffolding logic and adaptation rules.
This architecture enables LLMs to modulate behavior in response to user state
without requiring fine-tuning or external orchestration. In a simulated
intelligent tutoring setting, the framework improves scaffolding quality,
adaptivity, and instructional alignment across multiple models, outperforming
standard prompting baselines. Evaluation is conducted using rubric-based LLM
graders at scale. While initially developed for education, the framework has
shown promise in other interaction-heavy domains, such as procedural content
generation for games. Designed for safe deployment, it provides a reusable
methodology for structuring interpretable, goal-aligned LLM behavior in
uncertain or evolving contexts.

</details>


### [48] [Natural Language-Driven Viewpoint Navigation for Volume Exploration via Semantic Block Representation](https://arxiv.org/abs/2508.06823)
*Xuan Zhao,Jun Tao*

Main category: cs.AI

TL;DR: 提出了一种基于自然语言交互的框架，用于优化体数据探索中的视点选择，结合CLIP Score和强化学习提升导航效率和可解释性。


<details>
  <summary>Details</summary>
Motivation: 体数据探索对科学数据集解释至关重要，但缺乏领域知识或3D导航经验的用户难以选择最佳视点。

Method: 框架将体数据块编码以区分结构，结合CLIP Score提供语义信息，并通过强化学习搜索符合用户意图的视点。

Result: 自动化视点选择提高了体数据导航效率，并增强了复杂科学现象的可解释性。

Conclusion: 通过自然语言交互和语义引导，该方法显著改善了体数据探索的效率和用户体验。

Abstract: Exploring volumetric data is crucial for interpreting scientific datasets.
However, selecting optimal viewpoints for effective navigation can be
challenging, particularly for users without extensive domain expertise or
familiarity with 3D navigation. In this paper, we propose a novel framework
that leverages natural language interaction to enhance volumetric data
exploration. Our approach encodes volumetric blocks to capture and
differentiate underlying structures. It further incorporates a CLIP Score
mechanism, which provides semantic information to the blocks to guide
navigation. The navigation is empowered by a reinforcement learning framework
that leverage these semantic cues to efficiently search for and identify
desired viewpoints that align with the user's intent. The selected viewpoints
are evaluated using CLIP Score to ensure that they best reflect the user
queries. By automating viewpoint selection, our method improves the efficiency
of volumetric data navigation and enhances the interpretability of complex
scientific phenomena.

</details>


### [49] [CP-Agent: Agentic Constraint Programming](https://arxiv.org/abs/2508.07468)
*Stefan Szeider*

Main category: cs.AI

TL;DR: 论文提出了一种基于纯代理策略的新方法，用于将自然语言问题描述转化为形式化约束模型，无需固定流程，成功解决了CP-Bench基准集中的所有问题。


<details>
  <summary>Details</summary>
Motivation: 传统方法使用固定流程，无法解决大量基准问题，因此需要一种更灵活的方法。

Method: 采用基于ReAct原则的通用Python编码代理，通过精心设计的项目提示注入领域知识，结合文件操作和代码执行工具动态测试和调试。

Result: 该方法成功解决了CP-Bench基准集中的全部101个问题。

Conclusion: 约束建模任务需要结合通用编码工具和提示编码的领域知识，而非专用代理架构或预定义流程。

Abstract: Translating natural language problem descriptions into formal constraint
models remains a fundamental challenge in constraint programming, requiring
deep expertise in both the problem domain and modeling frameworks. Previous
approaches to automating this translation have employed fixed workflows with
predetermined modeling steps, failing on a significant number of benchmark
problems. We present a new approach using a pure agentic strategy without any
fixed pipeline. We developed a general-purpose Python coding agent based on the
ReAct (Reason and Act) principle, utilizing a persistent IPython kernel for
stateful code execution and iterative development. Rather than embedding
constraint programming logic into the agent architecture, domain-specific
expertise is injected solely through a carefully crafted project prompt. The
agent combines this prompt-encoded knowledge with access to file operations and
code execution tools, enabling it to test hypotheses, debug failures, and
verify solutions dynamically. Implemented in just a few hundred lines of code,
this architecture successfully solves all 101 problems of the CP-Bench
constraint programming benchmark set. The results suggest that constraint
modeling tasks require the combination of general coding tools and domain
expertise encoded in prompts, rather than specialized agent architectures or
predefined workflows.

</details>


### [50] [Remote Sensing Image Intelligent Interpretation with the Language-Centered Perspective: Principles, Methods and Challenges](https://arxiv.org/abs/2508.06832)
*Haifeng Li,Wang Guo,Haiyang Wu,Mengwei Wu,Jipeng Zhang,Qing Zhu,Yu Liu,Xin Huang,Chao Tao*

Main category: cs.AI

TL;DR: 论文提出从视觉中心转向语言中心的遥感图像解释范式，借鉴全球工作空间理论，以大型语言模型为核心，整合感知、任务、知识和行动空间，实现统一理解、推理和决策。


<details>
  <summary>Details</summary>
Motivation: 现有视觉中心模型在多模态推理、语义抽象和交互决策方面存在局限，缺乏统一理论框架解释语言在认知中的作用。

Method: 提出语言中心框架，将大型语言模型作为认知中枢，整合多模态表示、知识关联及推理决策，构建全局工作空间驱动的解释机制。

Result: 总结了语言中心解决方案如何应对多模态统一表示、知识关联和推理决策等核心挑战，并提出了未来研究方向。

Conclusion: 为下一代遥感解释系统提供概念基础，建立认知驱动智能地理空间分析的路线图。

Abstract: The mainstream paradigm of remote sensing image interpretation has long been
dominated by vision-centered models, which rely on visual features for semantic
understanding. However, these models face inherent limitations in handling
multi-modal reasoning, semantic abstraction, and interactive decision-making.
While recent advances have introduced Large Language Models (LLMs) into remote
sensing workflows, existing studies primarily focus on downstream applications,
lacking a unified theoretical framework that explains the cognitive role of
language. This review advocates a paradigm shift from vision-centered to
language-centered remote sensing interpretation. Drawing inspiration from the
Global Workspace Theory (GWT) of human cognition, We propose a
language-centered framework for remote sensing interpretation that treats LLMs
as the cognitive central hub integrating perceptual, task, knowledge and action
spaces to enable unified understanding, reasoning, and decision-making. We
first explore the potential of LLMs as the central cognitive component in
remote sensing interpretation, and then summarize core technical challenges,
including unified multimodal representation, knowledge association, and
reasoning and decision-making. Furthermore, we construct a global
workspace-driven interpretation mechanism and review how language-centered
solutions address each challenge. Finally, we outline future research
directions from four perspectives: adaptive alignment of multimodal data, task
understanding under dynamic knowledge constraints, trustworthy reasoning, and
autonomous interaction. This work aims to provide a conceptual foundation for
the next generation of remote sensing interpretation systems and establish a
roadmap toward cognition-driven intelligent geospatial analysis.

</details>


### [51] [Multi-level Advantage Credit Assignment for Cooperative Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2508.06836)
*Xutong Zhao,Yaqi Xie*

Main category: cs.AI

TL;DR: 论文提出了一种多级优势信用分配方法（MACA），用于解决多智能体强化学习中的信用分配问题，通过多级优势函数和注意力机制捕捉不同层次的智能体贡献。


<details>
  <summary>Details</summary>
Motivation: 多智能体强化学习中信用分配的挑战在于如何评估每个智能体对共享奖励的贡献，尤其是在任务多样性和智能体协作层次不同的情况下。

Method: MACA方法通过多级优势公式进行显式反事实推理，结合注意力框架识别智能体相关性，构建多级优势函数指导策略学习。

Result: 在Starcraft v1&v2任务上的实验表明，MACA在复杂信用分配场景中表现优异。

Conclusion: MACA通过多级优势信用分配有效解决了多智能体协作中的信用分配问题，适用于多种协作层次的任务。

Abstract: Cooperative multi-agent reinforcement learning (MARL) aims to coordinate
multiple agents to achieve a common goal. A key challenge in MARL is credit
assignment, which involves assessing each agent's contribution to the shared
reward. Given the diversity of tasks, agents may perform different types of
coordination, with rewards attributed to diverse and often overlapping agent
subsets. In this work, we formalize the credit assignment level as the number
of agents cooperating to obtain a reward, and address scenarios with multiple
coexisting levels. We introduce a multi-level advantage formulation that
performs explicit counterfactual reasoning to infer credits across distinct
levels. Our method, Multi-level Advantage Credit Assignment (MACA), captures
agent contributions at multiple levels by integrating advantage functions that
reason about individual, joint, and correlated actions. Utilizing an
attention-based framework, MACA identifies correlated agent relationships and
constructs multi-level advantages to guide policy learning. Comprehensive
experiments on challenging Starcraft v1\&v2 tasks demonstrate MACA's superior
performance, underscoring its efficacy in complex credit assignment scenarios.

</details>


### [52] [MDK12-Bench: A Comprehensive Evaluation of Multimodal Large Language Models on Multidisciplinary Exams](https://arxiv.org/abs/2508.06851)
*Pengfei Zhou,Xiaopeng Peng,Fanrui Zhang,Zhaopan Xu,Jiaxin Ai,Yansheng Qiu,Chuanhao Li,Zhen Li,Ming Li,Yukang Feng,Jianwen Sun,Haoquan Zhang,Zizhen Li,Xiaofeng Mao,Zekai Li,Wangbo Zhao,Kai Wang,Xiaojun Chang,Wenqi Shao,Yang You,Kaipeng Zhang*

Main category: cs.AI

TL;DR: MDK12-Bench是一个基于K-12考试的大规模多学科基准测试，用于全面评估多模态大语言模型（MLLMs）的性能，并提出动态评估框架和知识增强生成方法。


<details>
  <summary>Details</summary>
Motivation: 当前MLLMs的基准测试存在规模小、覆盖窄、知识无结构化等问题，无法全面评估模型性能。

Method: 构建MDK12-Bench基准，包含141K实例和6,225个知识点的六层分类法，并提出动态评估框架和KP-RAG方法。

Result: 发现当前MLLMs在多方面存在局限性，并提供了增强模型鲁棒性和可解释性的指导。

Conclusion: MDK12-Bench为MLLMs的评估和改进提供了重要工具，尤其在AI辅助教育领域。

Abstract: Multimodal large language models (MLLMs), which integrate language and visual
cues for problem-solving, are crucial for advancing artificial general
intelligence (AGI). However, current benchmarks for measuring the intelligence
of MLLMs suffer from limited scale, narrow coverage, and unstructured
knowledge, offering only static and undifferentiated evaluations. To bridge
this gap, we introduce MDK12-Bench, a large-scale multidisciplinary benchmark
built from real-world K-12 exams spanning six disciplines with 141K instances
and 6,225 knowledge points organized in a six-layer taxonomy. Covering five
question formats with difficulty and year annotations, it enables comprehensive
evaluation to capture the extent to which MLLMs perform over four dimensions:
1) difficulty levels, 2) temporal (cross-year) shifts, 3) contextual shifts,
and 4) knowledge-driven reasoning. We propose a novel dynamic evaluation
framework that introduces unfamiliar visual, textual, and question form shifts
to challenge model generalization while improving benchmark objectivity and
longevity by mitigating data contamination. We further evaluate knowledge-point
reference-augmented generation (KP-RAG) to examine the role of knowledge in
problem-solving. Key findings reveal limitations in current MLLMs in multiple
aspects and provide guidance for enhancing model robustness, interpretability,
and AI-assisted education.

</details>


### [53] [MeteorPred: A Meteorological Multimodal Large Model and Dataset for Severe Weather Event Prediction](https://arxiv.org/abs/2508.06859)
*Shuo Tang,Jian Xu,Jiadong Zhang,Yi Chen,Qizhao Jin,Lingdong Shen,Chenglin Liu,Shiming Xiang*

Main category: cs.AI

TL;DR: 论文提出了一种基于AI的端到端天气预警系统，通过构建大规模多模态数据集MP-Bench和开发气象多模态大模型（MMLM），解决了现有系统依赖人工、数据对齐不足等问题。


<details>
  <summary>Details</summary>
Motivation: 当前天气预警系统依赖专家手动解读，存在主观性和操作负担。AI技术的发展为自动化天气预测提供了新方向，但面临数据稀缺、高维数据对齐困难等挑战。

Method: 构建MP-Bench数据集（421,363对气象数据与文本标注），开发MMLM模型，支持4D气象数据输入，并设计了动态特征提取模块。

Result: 实验表明MMLM在多种任务中表现优异，验证了其在天气预警中的有效性。

Conclusion: MMLM为自动化AI天气预测系统迈出了关键一步，代码和数据集将公开。

Abstract: Timely and accurate severe weather warnings are critical for disaster
mitigation. However, current forecasting systems remain heavily reliant on
manual expert interpretation, introducing subjectivity and significant
operational burdens. With the rapid development of AI technologies, the
end-to-end "AI weather station" is gradually emerging as a new trend in
predicting severe weather events. Three core challenges impede the development
of end-to-end AI severe weather system: (1) scarcity of severe weather event
samples; (2) imperfect alignment between high-dimensional meteorological data
and textual warnings; (3) existing multimodal language models are unable to
handle high-dimensional meteorological data and struggle to fully capture the
complex dependencies across temporal sequences, vertical pressure levels, and
spatial dimensions. To address these challenges, we introduce MP-Bench, the
first large-scale temporal multimodal dataset for severe weather events
prediction, comprising 421,363 pairs of raw multi-year meteorological data and
corresponding text caption, covering a wide range of severe weather scenarios
across China. On top of this dataset, we develop a meteorology multimodal large
model (MMLM) that directly ingests 4D meteorological inputs. In addition, it is
designed to accommodate the unique characteristics of 4D meteorological data
flow, incorporating three plug-and-play adaptive fusion modules that enable
dynamic feature extraction and integration across temporal sequences, vertical
pressure layers, and spatial dimensions. Extensive experiments on MP-Bench
demonstrate that MMLM performs exceptionally well across multiple tasks,
highlighting its effectiveness in severe weather understanding and marking a
key step toward realizing automated, AI-driven weather forecasting systems. Our
source code and dataset will be made publicly available.

</details>


### [54] [Pushdown Reward Machines for Reinforcement Learning](https://arxiv.org/abs/2508.06894)
*Giovanni Varricchione,Toryn Q. Klassen,Natasha Alechina,Mehdi Dastani,Brian Logan,Sheila A. McIlraith*

Main category: cs.AI

TL;DR: 论文提出了推下奖励机（pdRMs），基于确定性下推自动机扩展了奖励机（RMs），能够识别和奖励确定性上下文无关语言表示的行为，提升了表达能力。


<details>
  <summary>Details</summary>
Motivation: 扩展奖励机的表达能力，使其能够处理更复杂的非马尔可夫奖励函数，从而提高强化学习的样本效率。

Method: 引入两种基于pdRM的策略，一种可以访问整个堆栈，另一种仅能访问堆栈顶部的$k$个符号，并提出了检查两种策略最优性的方法。

Result: 理论证明了pdRMs的表达能力，并提供了空间复杂度分析。实验表明pdRMs可以训练代理执行确定性上下文无关语言表示的任务。

Conclusion: pdRMs比RMs更具表达力，能够有效处理更复杂的任务，为强化学习提供了新的工具。

Abstract: Reward machines (RMs) are automata structures that encode (non-Markovian)
reward functions for reinforcement learning (RL). RMs can reward any behaviour
representable in regular languages and, when paired with RL algorithms that
exploit RM structure, have been shown to significantly improve sample
efficiency in many domains. In this work, we present pushdown reward machines
(pdRMs), an extension of reward machines based on deterministic pushdown
automata. pdRMs can recognize and reward temporally extended behaviours
representable in deterministic context-free languages, making them more
expressive than reward machines. We introduce two variants of pdRM-based
policies, one which has access to the entire stack of the pdRM, and one which
can only access the top $k$ symbols (for a given constant $k$) of the stack. We
propose a procedure to check when the two kinds of policies (for a given
environment, pdRM, and constant $k$) achieve the same optimal expected reward.
We then provide theoretical results establishing the expressive power of pdRMs,
and space complexity results about the proposed learning problems. Finally, we
provide experimental results showing how agents can be trained to perform tasks
representable in deterministic context-free languages using pdRMs.

</details>


### [55] [GDBA Revisited: Unleashing the Power of Guided Local Search for Distributed Constraint Optimization](https://arxiv.org/abs/2508.06899)
*Yanchen Deng,Xinrun Wang,Bo An*

Main category: cs.AI

TL;DR: 论文提出了一种名为DGLS的新方法，通过改进GDBA的缺陷，显著提升了分布式约束优化问题的局部搜索性能。


<details>
  <summary>Details</summary>
Motivation: GDBA在解决分布式约束优化问题时容易陷入局部最优，且性能提升有限。论文旨在解决其三个主要问题：过度激进的约束违反条件、无限制的惩罚累积和不协调的惩罚更新。

Method: 提出了DGLS框架，包括自适应违反条件、惩罚蒸发机制和同步惩罚更新方案。

Result: DGLS在标准基准测试中表现优异，尤其在结构化问题上显著优于现有方法（3.77%--66.3%）。

Conclusion: DGLS通过理论分析和实验验证，证明了其在解决分布式约束优化问题中的高效性和优越性。

Abstract: Local search is an important class of incomplete algorithms for solving
Distributed Constraint Optimization Problems (DCOPs) but it often converges to
poor local optima. While GDBA provides a comprehensive rule set to escape
premature convergence, its empirical benefits remain marginal on general-valued
problems. In this work, we systematically examine GDBA and identify three
factors that potentially lead to its inferior performance, i.e.,
over-aggressive constraint violation conditions, unbounded penalty
accumulation, and uncoordinated penalty updates. To address these issues, we
propose Distributed Guided Local Search (DGLS), a novel GLS framework for DCOPs
that incorporates an adaptive violation condition to selectively penalize
constraints with high cost, a penalty evaporation mechanism to control the
magnitude of penalization, and a synchronization scheme for coordinated penalty
updates. We theoretically show that the penalty values are bounded, and agents
play a potential game in our DGLS. Our extensive empirical results on various
standard benchmarks demonstrate the great superiority of DGLS over
state-of-the-art baselines. Particularly, compared to Damped Max-sum with high
damping factors (e.g., 0.7 or 0.9), our DGLS achieves competitive performance
on general-valued problems, and outperforms it by significant margins
(\textbf{3.77\%--66.3\%}) on structured problems in terms of anytime results.

</details>


### [56] [DSperse: A Framework for Targeted Verification in Zero-Knowledge Machine Learning](https://arxiv.org/abs/2508.06972)
*Dan Ivanov,Tristan Freiberg,Haruna Isah*

Main category: cs.AI

TL;DR: DSperse是一个模块化框架，用于分布式机器学习推理，通过战略性的加密验证实现高效和灵活的信任最小化。


<details>
  <summary>Details</summary>
Motivation: 解决分布式零知识机器学习中全模型电路化的高成本和僵化问题，通过局部验证关键子计算实现高效验证。

Method: 采用模块化设计，支持对推理管道中的部分或全部子计算（“切片”）进行验证，结合审计、复制或经济激励确保全局一致性。

Result: 实验评估了多种证明系统，展示了切片与非切片配置下的内存使用、运行时间和电路行为表现。

Conclusion: DSperse通过灵活的验证边界设计，支持可扩展的、针对性的验证策略，适应多样化的部署需求。

Abstract: DSperse is a modular framework for distributed machine learning inference
with strategic cryptographic verification. Operating within the emerging
paradigm of distributed zero-knowledge machine learning, DSperse avoids the
high cost and rigidity of full-model circuitization by enabling targeted
verification of strategically chosen subcomputations. These verifiable
segments, or "slices", may cover part or all of the inference pipeline, with
global consistency enforced through audit, replication, or economic incentives.
This architecture supports a pragmatic form of trust minimization, localizing
zero-knowledge proofs to the components where they provide the greatest value.
We evaluate DSperse using multiple proving systems and report empirical results
on memory usage, runtime, and circuit behavior under sliced and unsliced
configurations. By allowing proof boundaries to align flexibly with the model's
logical structure, DSperse supports scalable, targeted verification strategies
suited to diverse deployment needs.

</details>


### [57] [Automated Formalization via Conceptual Retrieval-Augmented LLMs](https://arxiv.org/abs/2508.06931)
*Wangyue Lu,Lun Du,Sirui Li,Ke Weng,Haozhe Sun,Hengyu Liu,Minghe Yu,Tiancheng Zhang,Ge Yu*

Main category: cs.AI

TL;DR: CRAMF是一个概念驱动的检索增强数学形式化框架，通过检索核心数学概念的形式化定义，提升基于LLM的自动形式化能力，解决了模型幻觉和语义鸿沟问题。


<details>
  <summary>Details</summary>
Motivation: 手动形式化在交互式定理证明器中劳动密集且需要专业知识，自动形式化面临模型幻觉和语义鸿沟的挑战。

Method: CRAMF通过从Mathlib4构建概念定义知识库，结合上下文查询增强和双通道混合检索策略，提升检索精度。

Result: 在多个基准测试中，CRAMF显著提升了翻译准确率，最高达62.1%，平均提升29.9%。

Conclusion: CRAMF为自动形式化提供了有效解决方案，显著提升了基于LLM的自动形式化性能。

Abstract: Interactive theorem provers (ITPs) require manual formalization, which is
labor-intensive and demands expert knowledge. While automated formalization
offers a potential solution, it faces two major challenges: model hallucination
(e.g., undefined predicates, symbol misuse, and version incompatibility) and
the semantic gap caused by ambiguous or missing premises in natural language
descriptions. To address these issues, we propose CRAMF, a Concept-driven
Retrieval-Augmented Mathematical Formalization framework. CRAMF enhances
LLM-based autoformalization by retrieving formal definitions of core
mathematical concepts, providing contextual grounding during code generation.
However, applying retrieval-augmented generation (RAG) in this setting is
non-trivial due to the lack of structured knowledge bases, the polymorphic
nature of mathematical concepts, and the high precision required in formal
retrieval. We introduce a framework for automatically constructing a
concept-definition knowledge base from Mathlib4, the standard mathematical
library for the Lean 4 theorem prover, indexing over 26,000 formal definitions
and 1,000+ core mathematical concepts. To address conceptual polymorphism, we
propose contextual query augmentation with domain- and application-level
signals. In addition, we design a dual-channel hybrid retrieval strategy with
reranking to ensure accurate and relevant definition retrieval. Experiments on
miniF2F, ProofNet, and our newly proposed AdvancedMath benchmark show that
CRAMF can be seamlessly integrated into LLM-based autoformalizers, yielding
consistent improvements in translation accuracy, achieving up to 62.1% and an
average of 29.9% relative improvement.

</details>


### [58] [Intrinsic Explainability of Multimodal Learning for Crop Yield Prediction](https://arxiv.org/abs/2508.06939)
*Hiba Najjar,Deepak Pathak,Marlon Nuske,Andreas Dengel*

Main category: cs.AI

TL;DR: 该研究利用Transformer模型的可解释性，通过注意力机制分析多模态学习网络，用于子田块级别的作物产量预测，并提出了新的模态归因方法。


<details>
  <summary>Details</summary>
Motivation: 多模态学习在农业中有广泛应用，但模型复杂性和可解释性常被忽视。本研究旨在通过Transformer模型提高多模态学习的可解释性。

Method: 使用自注意力机制，采用Attention Rollout (AR)和Generic Attention (GA)两种方法估计特征归因，并提出了Weighted Modality Activation (WMA)方法评估模态归因。

Result: Transformer模型在子田块和田间级别的R2分数分别比卷积和循环网络高0.10和0.04；AR在时间归因上表现更稳健。

Conclusion: Transformer模型在多模态学习中表现优异，AR方法提供了更可靠的解释，模态归因方法揭示了不同模式的差异。

Abstract: Multimodal learning enables various machine learning tasks to benefit from
diverse data sources, effectively mimicking the interplay of different factors
in real-world applications, particularly in agriculture. While the
heterogeneous nature of involved data modalities may necessitate the design of
complex architectures, the model interpretability is often overlooked. In this
study, we leverage the intrinsic explainability of Transformer-based models to
explain multimodal learning networks, focusing on the task of crop yield
prediction at the subfield level. The large datasets used cover various crops,
regions, and years, and include four different input modalities: multispectral
satellite and weather time series, terrain elevation maps and soil properties.
Based on the self-attention mechanism, we estimate feature attributions using
two methods, namely the Attention Rollout (AR) and Generic Attention (GA), and
evaluate their performance against Shapley-based model-agnostic estimations,
Shapley Value Sampling (SVS). Additionally, we propose the Weighted Modality
Activation (WMA) method to assess modality attributions and compare it with SVS
attributions. Our findings indicate that Transformer-based models outperform
other architectures, specifically convolutional and recurrent networks,
achieving R2 scores that are higher by 0.10 and 0.04 at the subfield and field
levels, respectively. AR is shown to provide more robust and reliable temporal
attributions, as confirmed through qualitative and quantitative evaluation,
compared to GA and SVS values. Information about crop phenology stages was
leveraged to interpret the explanation results in the light of established
agronomic knowledge. Furthermore, modality attributions revealed varying
patterns across the two methods compared.[...]

</details>


### [59] [Large Language Models Do Not Simulate Human Psychology](https://arxiv.org/abs/2508.06950)
*Sarah Schröder,Thekla Morgenroth,Ulrike Kuhl,Valerie Vaquet,Benjamin Paaßen*

Main category: cs.AI

TL;DR: 论文警告不要用LLMs模拟人类心理学，指出其与人类反应的显著差异。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs是否能替代人类参与者进行心理学研究。

Method: 通过概念论证和实证研究，比较LLMs与人类对微小语言变化的反应差异。

Result: LLMs与人类反应存在显著差异，且不同模型间反应不一致。

Conclusion: LLMs不能模拟人类心理学，需在每次应用中验证其可靠性。

Abstract: Large Language Models (LLMs),such as ChatGPT, are increasingly used in
research, ranging from simple writing assistance to complex data annotation
tasks. Recently, some research has suggested that LLMs may even be able to
simulate human psychology and can, hence, replace human participants in
psychological studies. We caution against this approach. We provide conceptual
arguments against the hypothesis that LLMs simulate human psychology. We then
present empiric evidence illustrating our arguments by demonstrating that
slight changes to wording that correspond to large changes in meaning lead to
notable discrepancies between LLMs' and human responses, even for the recent
CENTAUR model that was specifically fine-tuned on psychological responses.
Additionally, different LLMs show very different responses to novel items,
further illustrating their lack of reliability. We conclude that LLMs do not
simulate human psychology and recommend that psychological researchers should
treat LLMs as useful but fundamentally unreliable tools that need to be
validated against human responses for every new application.

</details>


### [60] [DatasetResearch: Benchmarking Agent Systems for Demand-Driven Dataset Discovery](https://arxiv.org/abs/2508.06960)
*Keyu Li,Mohan Jiang,Dayuan Fu,Yunze Wu,Xiangkun Hu,Dequan Wang,Pengfei Liu*

Main category: cs.AI

TL;DR: 论文提出了DatasetResearch基准，评估AI代理在发现和合成数据集方面的能力，揭示了当前技术与完美数据集发现之间的巨大差距。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的快速发展，数据可用性成为AI开发的瓶颈，而许多有价值的数据集分散在各处。论文旨在探索AI代理是否能超越传统搜索，实现自主需求驱动的数据整理。

Method: 引入DatasetResearch基准，评估AI代理在208个真实需求中的表现，采用三维评估框架分析知识密集型和推理密集型任务。

Result: 结果显示，即使是先进的深度研究系统，在挑战性子集上的得分仅为22%，暴露了当前能力的不足。搜索代理在知识任务中表现优异，而合成代理在推理任务中占优，但两者在“极端案例”中表现不佳。

Conclusion: 研究为数据集发现代理建立了首个严格基准，指明了实现AI系统发现任何数据集的路径，为下一代自改进AI系统奠定了基础。

Abstract: The rapid advancement of large language models has fundamentally shifted the
bottleneck in AI development from computational power to data availability-with
countless valuable datasets remaining hidden across specialized repositories,
research appendices, and domain platforms. As reasoning capabilities and deep
research methodologies continue to evolve, a critical question emerges: can AI
agents transcend conventional search to systematically discover any dataset
that meets specific user requirements, enabling truly autonomous demand-driven
data curation? We introduce DatasetResearch, the first comprehensive benchmark
evaluating AI agents' ability to discover and synthesize datasets from 208
real-world demands across knowledge-intensive and reasoning-intensive tasks.
Our tri-dimensional evaluation framework reveals a stark reality: even advanced
deep research systems achieve only 22% score on our challenging
DatasetResearch-pro subset, exposing the vast gap between current capabilities
and perfect dataset discovery. Our analysis uncovers a fundamental
dichotomy-search agents excel at knowledge tasks through retrieval breadth,
while synthesis agents dominate reasoning challenges via structured
generation-yet both catastrophically fail on "corner cases" outside existing
distributions. These findings establish the first rigorous baseline for dataset
discovery agents and illuminate the path toward AI systems capable of finding
any dataset in the digital universe. Our benchmark and comprehensive analysis
provide the foundation for the next generation of self-improving AI systems and
are publicly available at https://github.com/GAIR-NLP/DatasetResearch.

</details>


### [61] [MASteer: Multi-Agent Adaptive Steer Strategy for End-to-End LLM Trustworthiness Repair](https://arxiv.org/abs/2508.06963)
*Changqing Li,Tianlin Li,Xiaohan Zhang,Aishan Liu,Li Pan*

Main category: cs.AI

TL;DR: MASteer是一个基于表示工程的端到端框架，用于修复大型语言模型的信任问题，通过自动生成样本和自适应策略选择，显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有修复方法（如SFT和RLHF）成本高且速度慢，而提示工程缺乏鲁棒性和扩展性，需要更轻量、自动化的解决方案。

Method: MASteer结合AutoTester（多智能体生成样本）和AutoRepairer（自适应策略选择），实现自动化信任修复。

Result: 在标准任务中，MASteer在LLaMA-3.1-8B-Chat和Qwen-3-8B-Chat上分别提升15.36%和4.21%的性能。

Conclusion: MASteer展示了高效、可扩展的信任修复能力，具有强鲁棒性和实用价值。

Abstract: Large Language Models (LLMs) face persistent and evolving trustworthiness
issues, motivating developers to seek automated and flexible repair methods
that enable convenient deployment across diverse scenarios. Existing repair
methods like supervised fine-tuning (SFT) and reinforcement learning with human
feedback (RLHF) are costly and slow, while prompt engineering lacks robustness
and scalability. Representation engineering, which steers model behavior by
injecting targeted concept vectors during inference, offers a lightweight,
training-free alternative. However, current approaches depend on manually
crafted samples and fixed steering strategies, limiting automation and
adaptability. To overcome these challenges, we propose MASteer, the first
end-to-end framework for trustworthiness repair in LLMs based on representation
engineering. MASteer integrates two core components: AutoTester, a multi-agent
system that generates diverse, high-quality steer samples tailored to developer
needs; and AutoRepairer, which constructs adaptive steering strategies with
anchor vectors for automated, context-aware strategy selection during
inference. Experiments on standard and customized trustworthiness tasks show
MASteer consistently outperforms baselines, improving metrics by 15.36% on
LLaMA-3.1-8B-Chat and 4.21% on Qwen-3-8B-Chat, while maintaining general model
capabilities. MASteer demonstrates strong robustness, generalization, and
practical value for scalable, efficient trustworthiness repair.

</details>


### [62] [Simulating Biological Intelligence: Active Inference with Experiment-Informed Generative Model](https://arxiv.org/abs/2508.06980)
*Aswin Paul,Moein Khajehnejad,Forough Habibollahi,Brett J. Kagan,Adeel Razi*

Main category: cs.AI

TL;DR: 提出了一种基于主动推理的框架，用于建模具身代理的决策过程，结合生物神经元网络，探索了记忆学习和预测规划在智能决策中的作用。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能的快速发展，理解自主代理的有目的行为基础对开发安全高效系统至关重要。生物神经元网络可能提供更高效率和可解释性。

Method: 采用主动推理理论，结合实验启发的生成模型，在模拟游戏环境中模拟决策过程。

Result: 结果表明代理能够学习，揭示了记忆学习和预测规划在智能决策中的重要性。

Conclusion: 该工作为可解释AI领域提供了基于生物学的可扩展方法，有助于理解代理的有目的行为。

Abstract: With recent and rapid advancements in artificial intelligence (AI),
understanding the foundation of purposeful behaviour in autonomous agents is
crucial for developing safe and efficient systems. While artificial neural
networks have dominated the path to AI, recent studies are exploring the
potential of biologically based systems, such as networks of living biological
neuronal networks. Along with promises of high power and data efficiency, these
systems may also inform more explainable and biologically plausible models. In
this work, we propose a framework rooted in active inference, a general theory
of behaviour, to model decision-making in embodied agents. Using
experiment-informed generative models, we simulate decision-making processes in
a simulated game-play environment, mirroring experimental setups that use
biological neurons. Our results demonstrate learning in these agents, providing
insights into the role of memory-based learning and predictive planning in
intelligent decision-making. This work contributes to the growing field of
explainable AI by offering a biologically grounded and scalable approach to
understanding purposeful behaviour in agents.

</details>


### [63] [Efficient and Reliable Hitting-Set Computations for the Implicit Hitting Set Approach](https://arxiv.org/abs/2508.07015)
*Hannes Ihalainen,Dieter Vandesande,André Schidler,Jeremias Berg,Bart Bogaerts,Matti Järvisalo*

Main category: cs.AI

TL;DR: 论文探讨了隐式命中集（IHS）框架中替代整数规划的优化方法，包括伪布尔推理和随机局部搜索，评估了其在实际应用中的可行性和效率与可靠性的权衡。


<details>
  <summary>Details</summary>
Motivation: 研究IHS框架中替代整数规划的优化方法，以解决数值不稳定性和提高计算正确性。

Method: 采用伪布尔推理和随机局部搜索作为替代优化技术，并与商业整数规划求解器进行比较。

Result: 商业整数规划求解器效率最高但存在数值不稳定性问题，伪布尔推理在正确性方面表现优异且具有竞争力。

Conclusion: 伪布尔推理可作为IHS计算的有效替代方法，提供正确性证明，适用于任何IHS实例化。

Abstract: The implicit hitting set (IHS) approach offers a general framework for
solving computationally hard combinatorial optimization problems declaratively.
IHS iterates between a decision oracle used for extracting sources of
inconsistency and an optimizer for computing so-called hitting sets (HSs) over
the accumulated sources of inconsistency. While the decision oracle is
language-specific, the optimizers is usually instantiated through integer
programming.
  We explore alternative algorithmic techniques for hitting set optimization
based on different ways of employing pseudo-Boolean (PB) reasoning as well as
stochastic local search. We extensively evaluate the practical feasibility of
the alternatives in particular in the context of pseudo-Boolean (0-1 IP)
optimization as one of the most recent instantiations of IHS. Highlighting a
trade-off between efficiency and reliability, while a commercial IP solver
turns out to remain the most effective way to instantiate HS computations, it
can cause correctness issues due to numerical instability; in fact, we show
that exact HS computations instantiated via PB reasoning can be made
competitive with a numerically exact IP solver. Furthermore, the use of PB
reasoning as a basis for HS computations allows for obtaining certificates for
the correctness of IHS computations, generally applicable to any IHS
instantiation in which reasoning in the declarative language at hand can be
captured in the PB-based proof format we employ.

</details>


### [64] [MultiMedEdit: A Scenario-Aware Benchmark for Evaluating Knowledge Editing in Medical VQA](https://arxiv.org/abs/2508.07022)
*Shengtao Wen,Haodong Chen,Yadong Wang,Zhongying Pan,Xiang Chen,Yu Tian,Bo Qian,Dong Liang,Sheng-Jun Huang*

Main category: cs.AI

TL;DR: MultiMedEdit是首个针对临床多模态任务的知识编辑（KE）基准，填补了多模态医学场景中KE研究的空白，并揭示了当前方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有知识编辑研究主要集中在通用领域和医学QA任务，而多模态医学场景中的KE研究不足，需要结合视觉推理支持临床决策。

Method: 提出MultiMedEdit框架，涵盖理解和推理任务类型，定义三维度量标准（可靠性、通用性、局部性），并支持跨范式比较。

Result: 实验表明当前方法在泛化和长尾推理方面表现不佳，尤其在复杂临床工作流中。效率分析揭示了实际部署中的权衡。

Conclusion: MultiMedEdit为未来开发临床鲁棒的知识编辑技术奠定了基础，并揭示了当前方法的不足。

Abstract: Knowledge editing (KE) provides a scalable approach for updating factual
knowledge in large language models without full retraining. While previous
studies have demonstrated effectiveness in general domains and medical QA
tasks, little attention has been paid to KE in multimodal medical scenarios.
Unlike text-only settings, medical KE demands integrating updated knowledge
with visual reasoning to support safe and interpretable clinical decisions. To
address this gap, we propose MultiMedEdit, the first benchmark tailored to
evaluating KE in clinical multimodal tasks. Our framework spans both
understanding and reasoning task types, defines a three-dimensional metric
suite (reliability, generality, and locality), and supports cross-paradigm
comparisons across general and domain-specific models. We conduct extensive
experiments under single-editing and lifelong-editing settings. Results suggest
that current methods struggle with generalization and long-tail reasoning,
particularly in complex clinical workflows. We further present an efficiency
analysis (e.g., edit latency, memory footprint), revealing practical trade-offs
in real-world deployment across KE paradigms. Overall, MultiMedEdit not only
reveals the limitations of current approaches but also provides a solid
foundation for developing clinically robust knowledge editing techniques in the
future.

</details>


### [65] [K-Dense Analyst: Towards Fully Automated Scientific Analysis](https://arxiv.org/abs/2508.07043)
*Orion Li,Vinayak Agarwal,Summer Zhou,Ashwin Gopinath,Timothy Kassis*

Main category: cs.AI

TL;DR: K-Dense Analyst是一种分层多代理系统，通过双循环架构实现自主生物信息学分析，性能超越最佳语言模型GPT-5。


<details>
  <summary>Details</summary>
Motivation: 现代生物信息学分析的复杂性导致数据生成与科学洞察之间存在关键差距，现有语言模型在真实分析流程中存在局限。

Method: 采用分层多代理系统，通过双循环架构将复杂目标分解为可执行、可验证的任务。

Result: 在BixBench基准测试中，K-Dense Analyst准确率达29.2%，比GPT-5高6.3个百分点。

Conclusion: 自主科学推理需要专门构建的系统，而不仅仅是增强的语言模型。

Abstract: The complexity of modern bioinformatics analysis has created a critical gap
between data generation and developing scientific insights. While large
language models (LLMs) have shown promise in scientific reasoning, they remain
fundamentally limited when dealing with real-world analytical workflows that
demand iterative computation, tool integration and rigorous validation. We
introduce K-Dense Analyst, a hierarchical multi-agent system that achieves
autonomous bioinformatics analysis through a dual-loop architecture. K-Dense
Analyst, part of the broader K-Dense platform, couples planning with validated
execution using specialized agents to decompose complex objectives into
executable, verifiable tasks within secure computational environments. On
BixBench, a comprehensive benchmark for open-ended biological analysis, K-Dense
Analyst achieves 29.2% accuracy, surpassing the best-performing language model
(GPT-5) by 6.3 percentage points, representing nearly 27% improvement over what
is widely considered the most powerful LLM available. Remarkably, K-Dense
Analyst achieves this performance using Gemini 2.5 Pro, which attains only
18.3% accuracy when used directly, demonstrating that our architectural
innovations unlock capabilities far beyond the underlying model's baseline
performance. Our insights demonstrate that autonomous scientific reasoning
requires more than enhanced language models, it demands purpose-built systems
that can bridge the gap between high-level scientific objectives and low-level
computational execution. These results represent a significant advance toward
fully autonomous computational biologists capable of accelerating discovery
across the life sciences.

</details>


### [66] [Towards Safer AI Moderation: Evaluating LLM Moderators Through a Unified Benchmark Dataset and Advocating a Human-First Approach](https://arxiv.org/abs/2508.07063)
*Naseem Machlovi,Maryam Saleki,Innocent Ababio,Ruhul Amin*

Main category: cs.AI

TL;DR: 论文探讨了大型语言模型（LLMs）在内容审核中的局限性，提出了一种实验框架和SafePhi模型，以改进对情感、攻击性语言和偏见的检测。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统在日常生活中的广泛应用，对更安全、可靠的内容审核需求日益增长，但LLMs在道德推理和偏见检测方面仍存在不足。

Method: 开发了一个基于SOTA模型的实验框架，并引入统一基准数据集和SafePhi（QLoRA微调的Phi-4版本）进行测试。

Result: SafePhi在Macro F1得分上达到0.89，优于OpenAI Moderator（0.77）和Llama Guard（0.74），但在某些领域仍表现不佳。

Conclusion: 研究强调了LLMs在内容审核中的局限性，建议引入更多异构数据和人类参与，以提高模型的鲁棒性和可解释性。

Abstract: As AI systems become more integrated into daily life, the need for safer and
more reliable moderation has never been greater. Large Language Models (LLMs)
have demonstrated remarkable capabilities, surpassing earlier models in
complexity and performance. Their evaluation across diverse tasks has
consistently showcased their potential, enabling the development of adaptive
and personalized agents. However, despite these advancements, LLMs remain prone
to errors, particularly in areas requiring nuanced moral reasoning. They
struggle with detecting implicit hate, offensive language, and gender biases
due to the subjective and context-dependent nature of these issues. Moreover,
their reliance on training data can inadvertently reinforce societal biases,
leading to inconsistencies and ethical concerns in their outputs. To explore
the limitations of LLMs in this role, we developed an experimental framework
based on state-of-the-art (SOTA) models to assess human emotions and offensive
behaviors. The framework introduces a unified benchmark dataset encompassing 49
distinct categories spanning the wide spectrum of human emotions, offensive and
hateful text, and gender and racial biases. Furthermore, we introduced SafePhi,
a QLoRA fine-tuned version of Phi-4, adapting diverse ethical contexts and
outperforming benchmark moderators by achieving a Macro F1 score of 0.89, where
OpenAI Moderator and Llama Guard score 0.77 and 0.74, respectively. This
research also highlights the critical domains where LLM moderators consistently
underperformed, pressing the need to incorporate more heterogeneous and
representative data with human-in-the-loop, for better model robustness and
explainability.

</details>


### [67] [Designing a Feedback-Driven Decision Support System for Dynamic Student Intervention](https://arxiv.org/abs/2508.07107)
*Timothy Oluwapelumi Adeyemi,Nadiah Fahad AlOtaibi*

Main category: cs.AI

TL;DR: 论文提出了一种反馈驱动的决策支持系统（DSS），通过闭环架构实现持续模型优化，提升学生成绩预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习模型多为静态，无法适应新数据（如干预后结果），限制了预测的及时性和准确性。

Method: 系统结合LightGBM回归器和增量训练，支持实时输入更新数据并自动触发模型更新，采用Flask界面和SHAP解释工具。

Result: 实验显示，经过重新训练后RMSE降低10.7%，干预学生的预测分数持续上调。

Conclusion: 该框架将静态预测器转变为自优化系统，推动了教育分析向以人为本、数据驱动和响应式AI的发展。

Abstract: Accurate prediction of student performance is essential for timely academic
intervention. However, most machine learning models in education are static and
cannot adapt when new data, such as post-intervention outcomes, become
available. To address this limitation, we propose a Feedback-Driven Decision
Support System (DSS) with a closed-loop architecture that enables continuous
model refinement. The system integrates a LightGBM-based regressor with
incremental retraining, allowing educators to input updated student results,
which automatically trigger model updates. This adaptive mechanism improves
prediction accuracy by learning from real-world academic progress. The platform
features a Flask-based web interface for real-time interaction and incorporates
SHAP for explainability, ensuring transparency. Experimental results show a
10.7\% reduction in RMSE after retraining, with consistent upward adjustments
in predicted scores for intervened students. By transforming static predictors
into self-improving systems, our approach advances educational analytics toward
human-centered, data-driven, and responsive AI. The framework is designed for
integration into LMS and institutional dashboards.

</details>


### [68] [Multi-Dimensional Summarization Agents with Context-Aware Reasoning over Enterprise Tables](https://arxiv.org/abs/2508.07186)
*Amit Dhanda*

Main category: cs.AI

TL;DR: 提出了一种基于LLM的多智能体框架，用于跨维度结构化企业数据摘要，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统表格到文本模型在跨层次结构和上下文感知差异方面能力不足，难以满足商业报告需求。

Method: 采用多智能体管道，包括数据切片、方差检测、上下文构建和LLM生成。

Result: 框架在数据忠实度（83%）、显著变化覆盖率和决策关键洞察相关性（4.4/5）上表现优异。

Conclusion: 该框架在Kaggle数据集上验证，显著提升了摘要的忠实性、相关性和洞察质量。

Abstract: We propose a novel framework for summarizing structured enterprise data
across multiple dimensions using large language model (LLM)-based agents.
Traditional table-to-text models often lack the capacity to reason across
hierarchical structures and context-aware deltas, which are essential in
business reporting tasks. Our method introduces a multi-agent pipeline that
extracts, analyzes, and summarizes multi-dimensional data using agents for
slicing, variance detection, context construction, and LLM-based generation.
Our results show that the proposed framework outperforms traditional
approaches, achieving 83\% faithfulness to underlying data, superior coverage
of significant changes, and high relevance scores (4.4/5) for decision-critical
insights. The improvements are especially pronounced in categories involving
subtle trade-offs, such as increased revenue due to price changes amid
declining unit volumes, which competing methods either overlook or address with
limited specificity. We evaluate the framework on Kaggle datasets and
demonstrate significant improvements in faithfulness, relevance, and insight
quality over baseline table summarization approaches.

</details>


### [69] [EndoAgent: A Memory-Guided Reflective Agent for Intelligent Endoscopic Vision-to-Decision Reasoning](https://arxiv.org/abs/2508.07292)
*Yi Tang,Kaini Wang,Yang Chen,Guangquan Zhou*

Main category: cs.AI

TL;DR: EndoAgent是一种基于记忆引导的AI代理，用于内窥镜图像分析，结合了迭代推理和自适应工具选择，显著优于现有多模态模型。


<details>
  <summary>Details</summary>
Motivation: 现有基于大规模预训练的方法在多任务协调和复杂临床流程处理上表现不足，AI代理在内窥镜领域的潜力尚未充分挖掘。

Method: 提出EndoAgent，采用双记忆设计（短期动作跟踪和长期经验学习），集成专家工具，并引入EndoAgentBench基准测试。

Result: 实验表明EndoAgent在视觉理解和语言生成任务上优于通用及医学多模态模型。

Conclusion: EndoAgent展示了强大的灵活性和推理能力，为内窥镜诊断AI系统提供了新方向。

Abstract: Developing general artificial intelligence (AI) systems to support endoscopic
image diagnosis is an emerging research priority. Existing methods based on
large-scale pretraining often lack unified coordination across tasks and
struggle to handle the multi-step processes required in complex clinical
workflows. While AI agents have shown promise in flexible instruction parsing
and tool integration across domains, their potential in endoscopy remains
underexplored. To address this gap, we propose EndoAgent, the first
memory-guided agent for vision-to-decision endoscopic analysis that integrates
iterative reasoning with adaptive tool selection and collaboration. Built on a
dual-memory design, it enables sophisticated decision-making by ensuring
logical coherence through short-term action tracking and progressively
enhancing reasoning acuity through long-term experiential learning. To support
diverse clinical tasks, EndoAgent integrates a suite of expert-designed tools
within a unified reasoning loop. We further introduce EndoAgentBench, a
benchmark of 5,709 visual question-answer pairs that assess visual
understanding and language generation capabilities in realistic scenarios.
Extensive experiments show that EndoAgent consistently outperforms both general
and medical multimodal models, exhibiting its strong flexibility and reasoning
capabilities.

</details>


### [70] [Hallucination as a Computational Boundary: A Hierarchy of Inevitability and the Oracle Escape](https://arxiv.org/abs/2508.07334)
*Quan Shi,Wang Xi,Zenghui Ding,Jianqing Gao,Xianjun Yang*

Main category: cs.AI

TL;DR: 本文通过形式化大语言模型为概率图灵机，证明了幻觉现象在计算必要性层次上的不可避免性，并提出了两种解决方案：检索增强生成（RAG）和持续学习。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型（LLMs）幻觉现象的核心障碍，为其可靠部署提供理论基础。

Method: 构建“计算必要性层次”，证明幻觉的不可避免性；提出RAG作为预言机模型和持续学习作为“内部化预言机”机制。

Result: 证明了幻觉现象的理论边界，并提供了两种有效的解决方案。

Conclusion: 通过形式化方法和新理论框架，为LLMs的可靠部署提供了理论支持和实践路径。

Abstract: The illusion phenomenon of large language models (LLMs) is the core obstacle
to their reliable deployment. This article formalizes the large language model
as a probabilistic Turing machine by constructing a "computational necessity
hierarchy", and for the first time proves the illusions are inevitable on
diagonalization, incomputability, and information theory boundaries supported
by the new "learner pump lemma". However, we propose two "escape routes": one
is to model Retrieval Enhanced Generations (RAGs) as oracle machines, proving
their absolute escape through "computational jumps", providing the first formal
theory for the effectiveness of RAGs; The second is to formalize continuous
learning as an "internalized oracle" mechanism and implement this path through
a novel neural game theory framework.Finally, this article proposes a

</details>


### [71] [Rethinking Domain-Specific LLM Benchmark Construction: A Comprehensiveness-Compactness Approach](https://arxiv.org/abs/2508.07353)
*Rubing Chen,Jiaxin Wu,Jian Wang,Xulu Zhang,Wenqi Fan,Chenghua Lin,Xiao-Yong Wei,Qing Li*

Main category: cs.AI

TL;DR: 论文提出了一种基于全面性-紧凑性原则的迭代式基准测试框架Comp-Comp，用于构建领域特定的大型语言模型（LLM）基准测试，挑战了传统的扩展法则。


<details>
  <summary>Details</summary>
Motivation: 现有领域特定基准测试主要依赖扩展法则，但语料库和问答集设计对模型精度和召回率的影响尚未被研究。

Method: 提出Comp-Comp框架，通过全面性确保语义召回，紧凑性提升精度，指导语料库和问答集构建。

Result: 通过案例研究构建了XUBench，一个大规模、全面的封闭领域基准测试。

Conclusion: Comp-Comp框架不仅适用于学术领域，还可扩展到其他领域，为基准测试构建提供新思路。

Abstract: Numerous benchmarks have been built to evaluate the domain-specific abilities
of large language models (LLMs), highlighting the need for effective and
efficient benchmark construction. Existing domain-specific benchmarks primarily
focus on the scaling law, relying on massive corpora for supervised fine-tuning
or generating extensive question sets for broad coverage. However, the impact
of corpus and question-answer (QA) set design on the precision and recall of
domain-specific LLMs remains unexplored. In this paper, we address this gap and
demonstrate that the scaling law is not always the optimal principle for
benchmark construction in specific domains. Instead, we propose Comp-Comp, an
iterative benchmarking framework based on a comprehensiveness-compactness
principle. Here, comprehensiveness ensures semantic recall of the domain, while
compactness enhances precision, guiding both corpus and QA set construction. To
validate our framework, we conducted a case study in a well-renowned
university, resulting in the creation of XUBench, a large-scale and
comprehensive closed-domain benchmark. Although we use the academic domain as
the case in this work, our Comp-Comp framework is designed to be extensible
beyond academia, providing valuable insights for benchmark construction across
various domains.

</details>


### [72] [Pentest-R1: Towards Autonomous Penetration Testing Reasoning Optimized via Two-Stage Reinforcement Learning](https://arxiv.org/abs/2508.07382)
*He Kong,Die Hu,Jingguo Ge,Liangxiong Li,Hui Li,Tong Li*

Main category: cs.AI

TL;DR: Pentest-R1是一个通过两阶段强化学习优化LLM在渗透测试中推理能力的框架，显著提升了任务成功率。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在渗透测试中存在错误处理、推理效率和复杂任务自主执行能力不足的问题，Pentest-R1旨在解决这些限制。

Method: 采用两阶段强化学习：离线RL学习基础攻击逻辑，在线RL在CTF环境中通过环境反馈微调模型。

Result: 在AutoPenBench上成功率24.2%，Cybench上15.0%，性能接近顶级专有模型。

Conclusion: 两阶段训练协同作用对Pentest-R1的成功至关重要。

Abstract: Automating penetration testing is crucial for enhancing cybersecurity, yet
current Large Language Models (LLMs) face significant limitations in this
domain, including poor error handling, inefficient reasoning, and an inability
to perform complex end-to-end tasks autonomously. To address these challenges,
we introduce Pentest-R1, a novel framework designed to optimize LLM reasoning
capabilities for this task through a two-stage reinforcement learning pipeline.
We first construct a dataset of over 500 real-world, multi-step walkthroughs,
which Pentest-R1 leverages for offline reinforcement learning (RL) to instill
foundational attack logic. Subsequently, the LLM is fine-tuned via online RL in
an interactive Capture The Flag (CTF) environment, where it learns directly
from environmental feedback to develop robust error self-correction and
adaptive strategies. Our extensive experiments on the Cybench and AutoPenBench
benchmarks demonstrate the framework's effectiveness. On AutoPenBench,
Pentest-R1 achieves a 24.2\% success rate, surpassing most state-of-the-art
models and ranking second only to Gemini 2.5 Flash. On Cybench, it attains a
15.0\% success rate in unguided tasks, establishing a new state-of-the-art for
open-source LLMs and matching the performance of top proprietary models.
Ablation studies confirm that the synergy of both training stages is critical
to its success.

</details>


### [73] [Invert4TVG: A Temporal Video Grounding Framework with Inversion Tasks for Enhanced Action Understanding](https://arxiv.org/abs/2508.07388)
*Zhaoyu Chen,Hongnan Lin,Yongwei Nie,Fei Ma,Xuemiao Xu,Fei Yu,Chengjiang Long*

Main category: cs.AI

TL;DR: Invert4TVG框架通过三个反转任务（动词补全、动作识别和视频描述）增强视频片段定位和语义理解，显著提升了定位准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法过度优化时间IoU，牺牲了语义动作理解，影响了TVG的鲁棒性。

Method: 提出Invert4TVG框架，结合三个反转任务和强化学习，平衡定位和语义优化。

Result: 在Charades-STA数据集上，R1@0.7指标比Time-R1提升了7.1%。

Conclusion: 通过反转任务增强语义理解，显著提高了定位准确性的上限。

Abstract: Temporal Video Grounding (TVG) seeks to localize video segments matching a
given textual query. Current methods, while optimizing for high temporal
Intersection-over-Union (IoU), often overfit to this metric, compromising
semantic action understanding in the video and query, a critical factor for
robust TVG. To address this, we introduce Inversion Tasks for TVG (Invert4TVG),
a novel framework that enhances both localization accuracy and action
understanding without additional data. Our approach leverages three inversion
tasks derived from existing TVG annotations: (1) Verb Completion, predicting
masked action verbs in queries from video segments; (2) Action Recognition,
identifying query-described actions; and (3) Video Description, generating
descriptions of video segments that explicitly embed query-relevant actions.
These tasks, integrated with TVG via a reinforcement learning framework with
well-designed reward functions, ensure balanced optimization of localization
and semantics. Experiments show our method outperforms state-of-the-art
approaches, achieving a 7.1\% improvement in R1@0.7 on Charades-STA for a 3B
model compared to Time-R1. By inverting TVG to derive query-related actions
from segments, our approach strengthens semantic understanding, significantly
raising the ceiling of localization accuracy.

</details>


### [74] [Generative AI for Strategic Plan Development](https://arxiv.org/abs/2508.07405)
*Jesse Ponnock*

Main category: cs.AI

TL;DR: 论文提出了一种利用生成式人工智能（GAI）为大型政府组织制定战略计划的模块化模型，并评估了BERTopic和NMF在主题建模中的表现。结果显示，这些技术能生成与战略计划愿景元素相似的主题，BERTopic表现最佳。


<details>
  <summary>Details</summary>
Motivation: 随着生成式人工智能（GAI）和大语言模型（LLMs）的突破，越来越多的专业服务通过AI增强。本文旨在探索如何利用GAI为政府组织开发战略计划。

Method: 使用BERTopic和NMF模型对政府问责办公室（GAO）的大量报告进行主题建模，生成与战略计划愿景元素相似的主题，并比较其性能。

Result: BERTopic和NMF能生成与100%的愿景元素相似的主题，其中BERTopic表现更优，超过一半的主题达到“中等”或“强”相关性。

Conclusion: GAI支持的战略计划开发对数十亿美元的行业有重要影响，并帮助联邦政府满足关键公共利益法规。未来工作将聚焦于模型的实际应用和剩余模块的可行性。

Abstract: Given recent breakthroughs in Generative Artificial Intelligence (GAI) and
Large Language Models (LLMs), more and more professional services are being
augmented through Artificial Intelligence (AI), which once seemed impossible to
automate. This paper presents a modular model for leveraging GAI in developing
strategic plans for large scale government organizations and evaluates leading
machine learning techniques in their application towards one of the identified
modules. Specifically, the performance of BERTopic and Non-negative Matrix
Factorization (NMF) are evaluated in their ability to use topic modeling to
generate themes representative of Vision Elements within a strategic plan. To
accomplish this, BERTopic and NMF models are trained using a large volume of
reports from the Government Accountability Office (GAO). The generated topics
from each model are then scored for similarity against the Vision Elements of a
published strategic plan and the results are compared. Our results show that
these techniques are capable of generating themes similar to 100% of the
elements being evaluated against. Further, we conclude that BERTopic performs
best in this application with more than half of its correlated topics achieving
a "medium" or "strong" correlation. A capability of GAI-enabled strategic plan
development impacts a multi-billion dollar industry and assists the federal
government in overcoming regulatory requirements which are crucial to the
public good. Further work will focus on the operationalization of the concept
proven in this study as well as viability of the remaining modules in the
proposed model for GAI-generated strategic plans.

</details>


### [75] [A Comprehensive Survey of Self-Evolving AI Agents: A New Paradigm Bridging Foundation Models and Lifelong Agentic Systems](https://arxiv.org/abs/2508.07407)
*Jinyuan Fang,Yanwen Peng,Xi Zhang,Yingxu Wang,Xinhao Yi,Guibin Zhang,Yi Xu,Bin Wu,Siwei Liu,Zihao Li,Zhaochun Ren,Nikos Aletras,Xi Wang,Han Zhou,Zaiqiao Meng*

Main category: cs.AI

TL;DR: 综述了自进化AI代理系统的现有技术，提出了统一的概念框架，并探讨了评估、安全与伦理问题。


<details>
  <summary>Details</summary>
Motivation: 现有代理系统依赖静态配置，无法适应动态环境，需研究自进化技术以提升适应性。

Method: 提出包含四个关键组件的统一框架，系统回顾针对不同组件的自进化技术，并探讨领域专用策略。

Result: 总结了多种自进化技术及其应用领域，强调了评估、安全与伦理的重要性。

Conclusion: 为开发更自适应、自主和终身代理系统提供了系统化基础。

Abstract: Recent advances in large language models have sparked growing interest in AI
agents capable of solving complex, real-world tasks. However, most existing
agent systems rely on manually crafted configurations that remain static after
deployment, limiting their ability to adapt to dynamic and evolving
environments. To this end, recent research has explored agent evolution
techniques that aim to automatically enhance agent systems based on interaction
data and environmental feedback. This emerging direction lays the foundation
for self-evolving AI agents, which bridge the static capabilities of foundation
models with the continuous adaptability required by lifelong agentic systems.
In this survey, we provide a comprehensive review of existing techniques for
self-evolving agentic systems. Specifically, we first introduce a unified
conceptual framework that abstracts the feedback loop underlying the design of
self-evolving agentic systems. The framework highlights four key components:
System Inputs, Agent System, Environment, and Optimisers, serving as a
foundation for understanding and comparing different strategies. Based on this
framework, we systematically review a wide range of self-evolving techniques
that target different components of the agent system. We also investigate
domain-specific evolution strategies developed for specialised fields such as
biomedicine, programming, and finance, where optimisation objectives are
tightly coupled with domain constraints. In addition, we provide a dedicated
discussion on the evaluation, safety, and ethical considerations for
self-evolving agentic systems, which are critical to ensuring their
effectiveness and reliability. This survey aims to provide researchers and
practitioners with a systematic understanding of self-evolving AI agents,
laying the foundation for the development of more adaptive, autonomous, and
lifelong agentic systems.

</details>


### [76] [Grounding Natural Language for Multi-agent Decision-Making with Multi-agentic LLMs](https://arxiv.org/abs/2508.07466)
*Dom Huh,Prasant Mohapatra*

Main category: cs.AI

TL;DR: 论文提出了一种系统性框架，将大型语言模型（LLMs）与多智能体决策算法结合，以提升协作与推理能力。


<details>
  <summary>Details</summary>
Motivation: 语言是协作与推理的基础，建立共同语言有助于智能体间的清晰沟通与协调。

Method: 提出多智能体LLMs的设计框架，包括高级提示工程、记忆架构、多模态信息处理和微调对齐策略。

Result: 通过在经典游戏设置中的消融实验验证了设计选择的有效性。

Conclusion: 该框架为多智能体LLMs的设计提供了实用指导，增强了协作与决策能力。

Abstract: Language is a ubiquitous tool that is foundational to reasoning and
collaboration, ranging from everyday interactions to sophisticated
problem-solving tasks. The establishment of a common language can serve as a
powerful asset in ensuring clear communication and understanding amongst
agents, facilitating desired coordination and strategies. In this work, we
extend the capabilities of large language models (LLMs) by integrating them
with advancements in multi-agent decision-making algorithms. We propose a
systematic framework for the design of multi-agentic large language models
(LLMs), focusing on key integration practices. These include advanced prompt
engineering techniques, the development of effective memory architectures,
multi-modal information processing, and alignment strategies through
fine-tuning algorithms. We evaluate these design choices through extensive
ablation studies on classic game settings with significant underlying social
dilemmas and game-theoretic considerations.

</details>


### [77] [Democratizing Diplomacy: A Harness for Evaluating Any Large Language Model on Full-Press Diplomacy](https://arxiv.org/abs/2508.07485)
*Alexander Duffy,Samuel J Paech,Ishana Shastri,Elizabeth Karpinski,Baptiste Alloui-Cros,Tyler Marques,Matthew Lyle Olson*

Main category: cs.AI

TL;DR: 开发了一个无需微调即可让本地大型语言模型（LLM）玩完整版《外交》游戏的评估工具，解决了游戏状态复杂和信息密度高的问题。


<details>
  <summary>Details</summary>
Motivation: 《外交》游戏的高复杂性和信息密度使得研究困难，需要前沿LLM或微调。本文旨在消除这些限制，使更多LLM能够参与研究。

Method: 通过数据驱动的迭代优化文本游戏状态表示，开发工具支持假设测试和统计分析，并引入关键状态分析协议。

Result: 实验表明，较大模型表现最佳，但较小模型也能胜任。工具成功实现了无需微调的评估。

Conclusion: 该工具降低了战略推理评估的门槛，揭示了LLM自然具备的能力，代码已开源。

Abstract: We present the first evaluation harness that enables any out-of-the-box,
local, Large Language Models (LLMs) to play full-press Diplomacy without
fine-tuning or specialized training. Previous work required frontier LLMs, or
fine-tuning, due to the high complexity and information density of Diplomacy's
game state. Combined with the high variance of matches, these factors made
Diplomacy prohibitive for study. In this work, we used data-driven iteration to
optimize a textual game state representation such that a 24B model can reliably
complete matches without any fine tuning. We develop tooling to facilitate
hypothesis testing and statistical analysis, and we present case studies on
persuasion, aggressive playstyles, and performance across a range of models. We
conduct a variety of experiments across many popular LLMs, finding the larger
models perform the best, but the smaller models still play adequately. We also
introduce Critical State Analysis: an experimental protocol for rapidly
iterating and analyzing key moments in a game at depth. Our harness
democratizes the evaluation of strategic reasoning in LLMs by eliminating the
need for fine-tuning, and it provides insights into how these capabilities
emerge naturally from widely used LLMs. Our code is available in the supplement
and will be open sourced.

</details>


### [78] [MCPToolBench++: A Large Scale AI Agent Model Context Protocol MCP Tool Use Benchmark](https://arxiv.org/abs/2508.07575)
*Shiqing Fan,Xichen Ding,Liang Zhang,Linjian Mo*

Main category: cs.AI

TL;DR: 论文提出了MCPToolBench++，一个用于评估LLMs调用MCP工具性能的大规模多领域基准测试。


<details>
  <summary>Details</summary>
Motivation: 现有评估LLMs和AI Agents使用MCP工具的能力存在数据集不全面、响应格式多样、成功率不稳定等问题。

Method: 构建了一个基于4000多个MCP服务器的大规模基准测试，涵盖单步和多步工具调用。

Result: 评估了具有代理能力的SOTA LLMs，并报告了结果。

Conclusion: MCPToolBench++为评估LLMs在MCP工具调用中的性能提供了有效解决方案。

Abstract: LLMs' capabilities are enhanced by using function calls to integrate various
data sources or API results into the context window. Typical tools include
search, web crawlers, maps, financial data, file systems, and browser usage,
etc. Integrating these data sources or functions requires a standardized
method. The Model Context Protocol (MCP) provides a standardized way to supply
context to LLMs. However, the evaluation of LLMs and AI Agents' MCP tool use
abilities suffer from several issues. First, there's a lack of comprehensive
datasets or benchmarks to evaluate various MCP tools. Second, the diverse
formats of response from MCP tool call execution further increase the
difficulty of evaluation. Additionally, unlike existing tool-use benchmarks
with high success rates in functions like programming and math functions, the
success rate of real-world MCP tool is not guaranteed and varies across
different MCP servers. Furthermore, the LLMs' context window also limits the
number of available tools that can be called in a single run, because the
textual descriptions of tool and the parameters have long token length for an
LLM to process all at once. To help address the challenges of evaluating LLMs'
performance on calling MCP tools, we propose MCPToolBench++, a large-scale,
multi-domain AI Agent tool use benchmark. As of July 2025, this benchmark is
build upon marketplace of over 4k MCP servers from more than 40 categories,
collected from the MCP marketplaces and GitHub communities. The datasets
consist of both single-step and multi-step tool calls across different
categories. We evaluated SOTA LLMs with agentic abilities on this benchmark and
reported the results.

</details>


### [79] [Optimization of Private Semantic Communication Performance: An Uncooperative Covert Communication Method](https://arxiv.org/abs/2508.07586)
*Wenjing Zhang,Ye Hu,Tao Luo,Zhilong Zhang,Mingzhe Chen*

Main category: cs.AI

TL;DR: 提出了一种新颖的隐蔽语义通信框架，通过友好干扰器和时间槽优化来保护语义信息传输，避免被攻击者窃取。


<details>
  <summary>Details</summary>
Motivation: 解决语义信息传输中易被攻击者窃取的问题，提升隐私保护和传输质量。

Method: 采用优先采样辅助的双延迟深度确定性策略梯度算法，联合优化语义信息和传输功率。

Result: 仿真结果显示，隐私保护和传输质量分别提升了77.8%和14.3%。

Conclusion: 所提算法能有效提升隐蔽语义通信的安全性和性能。

Abstract: In this paper, a novel covert semantic communication framework is
investigated. Within this framework, a server extracts and transmits the
semantic information, i.e., the meaning of image data, to a user over several
time slots. An attacker seeks to detect and eavesdrop the semantic transmission
to acquire details of the original image. To avoid data meaning being
eavesdropped by an attacker, a friendly jammer is deployed to transmit jamming
signals to interfere the attacker so as to hide the transmitted semantic
information. Meanwhile, the server will strategically select time slots for
semantic information transmission. Due to limited energy, the jammer will not
communicate with the server and hence the server does not know the transmit
power of the jammer. Therefore, the server must jointly optimize the semantic
information transmitted at each time slot and the corresponding transmit power
to maximize the privacy and the semantic information transmission quality of
the user. To solve this problem, we propose a prioritised sampling assisted
twin delayed deep deterministic policy gradient algorithm to jointly determine
the transmitted semantic information and the transmit power per time slot
without the communications between the server and the jammer. Compared to
standard reinforcement learning methods, the propose method uses an additional
Q network to estimate Q values such that the agent can select the action with a
lower Q value from the two Q networks thus avoiding local optimal action
selection and estimation bias of Q values. Simulation results show that the
proposed algorithm can improve the privacy and the semantic information
transmission quality by up to 77.8% and 14.3% compared to the traditional
reinforcement learning methods.

</details>


### [80] [HGMF: A Hierarchical Gaussian Mixture Framework for Scalable Tool Invocation within the Model Context Protocol](https://arxiv.org/abs/2508.07602)
*Wenpeng Xing,Zhipeng Chen,Changting Lin,Meng Han*

Main category: cs.AI

TL;DR: HGMF是一种概率剪枝方法，通过分层高斯混合模型（GMM）聚类和过滤工具库，显著提高了工具选择的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在大规模、层次化工具库中选择正确工具的挑战，避免低准确性和高计算成本。

Method: HGMF将查询和工具描述映射到统一语义空间，分两阶段进行GMM聚类和基于查询似然的过滤，生成高相关性候选集。

Result: 实验表明HGMF显著提高了工具选择准确性并降低了推理延迟。

Conclusion: HGMF为大规模工具库提供了一种可扩展且高效的解决方案。

Abstract: Invoking external tools enables Large Language Models (LLMs) to perform
complex, real-world tasks, yet selecting the correct tool from large,
hierarchically-structured libraries remains a significant challenge. The
limited context windows of LLMs and noise from irrelevant options often lead to
low selection accuracy and high computational costs. To address this, we
propose the Hierarchical Gaussian Mixture Framework (HGMF), a probabilistic
pruning method for scalable tool invocation. HGMF first maps the user query and
all tool descriptions into a unified semantic space. The framework then
operates in two stages: it clusters servers using a Gaussian Mixture Model
(GMM) and filters them based on the query's likelihood. Subsequently, it
applies the same GMM-based clustering and filtering to the tools associated
with the selected servers. This hierarchical process produces a compact,
high-relevance candidate set, simplifying the final selection task for the LLM.
Experiments on a public dataset show that HGMF significantly improves tool
selection accuracy while reducing inference latency, confirming the framework's
scalability and effectiveness for large-scale tool libraries.

</details>


### [81] [ThinkTuning: Instilling Cognitive Reflections without Distillation](https://arxiv.org/abs/2508.07616)
*Aswin RRV,Jacob Dineen,Divij Handa,Md Nayem Uddin,Mihir Parmar,Chitta Baral,Ben Zhou*

Main category: cs.AI

TL;DR: 论文提出ThinkTuning方法，通过教师模型的反馈指导学生模型，提升其推理能力，实验显示在多任务基准上有显著改进。


<details>
  <summary>Details</summary>
Motivation: 现有RL方法仅能激发预训练模型的潜在行为，无法真正培养新的推理能力，因此需要一种新方法训练不具备此类能力的模型。

Method: 采用GRPO框架，通过教师模型提供反馈，逐步指导学生模型改进其推理过程。

Result: ThinkTuning在多个基准测试中平均提升3.85%，在MATH-500、AIME和GPQA-Diamond上分别提升2.08%、2.23%和3.99%。

Conclusion: ThinkTuning通过教师模型的隐式监督有效提升了学生模型的推理能力，为训练不具备初始推理能力的模型提供了新思路。

Abstract: Recent advances in test-time scaling have led to the emergence of thinking
LLMs that exhibit self-reflective behaviors and multi-step reasoning. While RL
drives this self-improvement paradigm, a recent study (Gandhi et al., 2025)
shows that RL alone does not truly instill these new reasoning abilities - it
merely draws out behaviors already present in the base models. This raises a
question: How can we train the models that don't exhibit such thinking behavior
to develop it in the first place? To this end, we propose ThinkTuning, a
GRPO-based interactive training approach where we augment the rollouts of a
student model with the guidance from a teacher model. A simple idea from
classroom practice inspires our method: a teacher poses a problem, lets the
student try an answer, then gives corrective feedback -- enough to point the
mind in the right direction and then show the solution. Each piece of feedback
reshapes the student's thoughts, leading them to arrive at the correct
solution. Similarly, we find that this type of implicit supervision through
feedback from a teacher model of the same size improves the reasoning
capabilities of the student model. In particular, on average, our method shows
a 3.85% improvement over zero-shot baselines across benchmarks, and on
MATH-500, AIME and GPQA-Diamond it shows 2.08%, 2.23% and 3.99% improvements
over the vanilla-GRPO baseline. Source code is available at
https://github.com/3rdAT/ThinkTuning.

</details>


### [82] [Multimodal AI Systems for Enhanced Laying Hen Welfare Assessment and Productivity Optimization](https://arxiv.org/abs/2508.07628)
*Daniel Essien,Suresh Neethirajan*

Main category: cs.AI

TL;DR: 论文提出利用多模态AI技术改进家禽福利监测，通过整合视觉、声音、环境和生理数据，实现更精准的福利评估。研究发现特征级融合策略在性能和鲁棒性上表现最佳，并提出了解决实际部署障碍的新工具和框架。


<details>
  <summary>Details</summary>
Motivation: 传统家禽福利监测依赖主观人工观察和单一传感器数据，无法全面反映现代农场中蛋鸡的多维度福利需求。多模态AI技术为解决这一问题提供了突破性方案。

Method: 采用多模态AI技术，整合视觉、声学、环境和生理数据流，通过特征级融合策略优化模型性能。提出领域转移评分（DTS）和数据可靠性指数（DRI）作为评估工具，并设计模块化部署框架。

Result: 特征级融合策略在真实农场条件下表现出最佳平衡性，提出的DTS和DRI工具有效解决了模型适应性和数据质量问题。

Conclusion: 多模态AI技术为家禽福利监测提供了从被动单模态监测向主动精准化系统的转变基础，结合了生产效率和科学伦理。

Abstract: The future of poultry production depends on a paradigm shift replacing
subjective, labor-intensive welfare checks with data-driven, intelligent
monitoring ecosystems. Traditional welfare assessments-limited by human
observation and single-sensor data-cannot fully capture the complex,
multidimensional nature of laying hen welfare in modern farms. Multimodal
Artificial Intelligence (AI) offers a breakthrough, integrating visual,
acoustic, environmental, and physiological data streams to reveal deeper
insights into avian welfare dynamics. This investigation highlights multimodal
As transformative potential, showing that intermediate (feature-level) fusion
strategies achieve the best balance between robustness and performance under
real-world poultry conditions, and offer greater scalability than early or late
fusion approaches. Key adoption barriers include sensor fragility in harsh farm
environments, high deployment costs, inconsistent behavioral definitions, and
limited cross-farm generalizability. To address these, we introduce two novel
evaluation tools - the Domain Transfer Score (DTS) to measure model
adaptability across diverse farm settings, and the Data Reliability Index (DRI)
to assess sensor data quality under operational constraints. We also propose a
modular, context-aware deployment framework designed for laying hen
environments, enabling scalable and practical integration of multimodal
sensing. This work lays the foundation for a transition from reactive, unimodal
monitoring to proactive, precision-driven welfare systems that unite
productivity with ethical, science based animal care.

</details>


### [83] [Breaking Down and Building Up: Mixture of Skill-Based Vision-and-Language Navigation Agents](https://arxiv.org/abs/2508.07642)
*Tianyi Ma,Yue Zhang,Zehao Wang,Parisa Kordjamshidi*

Main category: cs.AI

TL;DR: SkillNav是一个模块化框架，通过将导航任务分解为可解释的原子技能，并利用基于VLM的路由器动态选择适合的代理，显著提升了VLN任务的性能。


<details>
  <summary>Details</summary>
Motivation: 当前VLN方法在复杂空间和时间推理场景中泛化能力不足，需要更结构化的方法。

Method: 提出SkillNav框架，将导航分解为原子技能，每个技能由专门代理处理，并引入VLM路由器动态选择代理。

Result: 在R2R和GSA-R2R基准测试中达到新SOTA，展示了强泛化能力。

Conclusion: SkillNav通过模块化和技能分解，显著提升了VLN任务的性能和泛化能力。

Abstract: Vision-and-Language Navigation (VLN) poses significant challenges in enabling
agents to interpret natural language instructions and navigate complex 3D
environments. While recent progress has been driven by large-scale pre-training
and data augmentation, current methods still struggle to generalize to unseen
scenarios, particularly when complex spatial and temporal reasoning is
required. In this work, we propose SkillNav, a modular framework that
introduces structured, skill-based reasoning into Transformer-based VLN agents.
Our method decomposes navigation into a set of interpretable atomic skills
(e.g., Vertical Movement, Area and Region Identification, Stop and Pause), each
handled by a specialized agent. We then introduce a novel zero-shot
Vision-Language Model (VLM)-based router, which dynamically selects the most
suitable agent at each time step by aligning sub-goals with visual observations
and historical actions. SkillNav achieves a new state-of-the-art performance on
the R2R benchmark and demonstrates strong generalization to the GSA-R2R
benchmark that includes novel instruction styles and unseen environments.

</details>


### [84] [Disentangling Multiplex Spatial-Temporal Transition Graph Representation Learning for Socially Enhanced POI Recommendation](https://arxiv.org/abs/2508.07649)
*Jie Li,Haoye Dong,Zhengyang Wu,Zetao Zheng,Mingrong Lin*

Main category: cs.AI

TL;DR: DiMuST模型通过解耦多时空图表示学习，提升POI推荐性能。


<details>
  <summary>Details</summary>
Motivation: 现有POI推荐模型将时空转换分开建模，导致节点表示不匹配，增加冗余信息和模型不确定性。

Method: 提出DiMuST模型，采用解耦变分多图自动编码器（DAE），分离共享与私有分布，并通过PoE机制融合共享特征，对比约束去噪私有特征。

Result: 在两个数据集上，DiMuST在多项指标上显著优于现有方法。

Conclusion: DiMuST有效捕捉POI的时空转换表示，同时保持其时空关系的内在相关性。

Abstract: Next Point-of-Interest (POI) recommendation is a research hotspot in business
intelligence, where users' spatial-temporal transitions and social
relationships play key roles. However, most existing works model spatial and
temporal transitions separately, leading to misaligned representations of the
same spatial-temporal key nodes. This misalignment introduces redundant
information during fusion, increasing model uncertainty and reducing
interpretability. To address this issue, we propose DiMuST, a socially enhanced
POI recommendation model based on disentangled representation learning over
multiplex spatial-temporal transition graphs. The model employs a novel
Disentangled variational multiplex graph Auto-Encoder (DAE), which first
disentangles shared and private distributions using a multiplex
spatial-temporal graph strategy. It then fuses the shared features via a
Product of Experts (PoE) mechanism and denoises the private features through
contrastive constraints. The model effectively captures the spatial-temporal
transition representations of POIs while preserving the intrinsic correlation
of their spatial-temporal relationships. Experiments on two challenging
datasets demonstrate that our DiMuST significantly outperforms existing methods
across multiple metrics.

</details>


### [85] [1-2-3 Check: Enhancing Contextual Privacy in LLM via Multi-Agent Reasoning](https://arxiv.org/abs/2508.07667)
*Wenkai Li,Liwen Sun,Zhenxiang Guan,Xuhui Zhou,Maarten Sap*

Main category: cs.AI

TL;DR: 论文提出了一种多智能体框架，通过分解隐私推理任务来减少隐私信息泄漏，实验表明其优于单智能体基线。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在多源信息处理中的上下文隐私问题。

Method: 引入多智能体框架，将隐私推理分解为专门子任务（提取、分类），并通过信息流拓扑分析隐私错误传播。

Result: 在ConfAIde和PrivacyLens基准测试中，多智能体配置显著减少隐私泄漏（GPT-4o下分别降低18%和19%）。

Conclusion: 多智能体系统的信息流设计在上下文隐私保护中具有潜力。

Abstract: Addressing contextual privacy concerns remains challenging in interactive
settings where large language models (LLMs) process information from multiple
sources (e.g., summarizing meetings with private and public information). We
introduce a multi-agent framework that decomposes privacy reasoning into
specialized subtasks (extraction, classification), reducing the information
load on any single agent while enabling iterative validation and more reliable
adherence to contextual privacy norms. To understand how privacy errors emerge
and propagate, we conduct a systematic ablation over information-flow
topologies, revealing when and why upstream detection mistakes cascade into
downstream leakage. Experiments on the ConfAIde and PrivacyLens benchmark with
several open-source and closed-sourced LLMs demonstrate that our best
multi-agent configuration substantially reduces private information leakage
(\textbf{18\%} on ConfAIde and \textbf{19\%} on PrivacyLens with GPT-4o) while
preserving the fidelity of public content, outperforming single-agent
baselines. These results highlight the promise of principled information-flow
design in multi-agent systems for contextual privacy with LLMs.

</details>


### [86] [EMPATHIA: Multi-Faceted Human-AI Collaboration for Refugee Integration](https://arxiv.org/abs/2508.07671)
*Mohamed Rayan Barhdadi,Mehmet Tuncel,Erchin Serpedin,Hasan Kurban*

Main category: cs.AI

TL;DR: EMPATHIA是一个多智能体框架，旨在解决难民整合中的文化、情感和伦理维度问题，通过模块化设计实现透明和可解释的决策。


<details>
  <summary>Details</summary>
Motivation: 当前AI在难民整合中仅关注就业等狭窄目标，忽视了文化、情感和伦理等长期成功的关键因素。

Method: EMPATHIA基于Kegan的建构发展理论，分为SEED、RISE和THRIVE三个模块，采用多智能体架构进行透明决策。

Result: 在UN Kakuma数据集上验证，EMPATHIA达到87.4%的收敛率，并在五个接收国实现了可解释的评估。

Conclusion: EMPATHIA通过平衡多价值系统和支持人机协作，为AI驱动的分配任务提供了通用框架。

Abstract: Current AI approaches to refugee integration optimize narrow objectives such
as employment and fail to capture the cultural, emotional, and ethical
dimensions critical for long-term success. We introduce EMPATHIA (Enriched
Multimodal Pathways for Agentic Thinking in Humanitarian Immigrant Assistance),
a multi-agent framework addressing the central Creative AI question: how do we
preserve human dignity when machines participate in life-altering decisions?
Grounded in Kegan's Constructive Developmental Theory, EMPATHIA decomposes
integration into three modules: SEED (Socio-cultural Entry and Embedding
Decision) for initial placement, RISE (Rapid Integration and Self-sufficiency
Engine) for early independence, and THRIVE (Transcultural Harmony and
Resilience through Integrated Values and Engagement) for sustained outcomes.
SEED employs a selector-validator architecture with three specialized agents -
emotional, cultural, and ethical - that deliberate transparently to produce
interpretable recommendations. Experiments on the UN Kakuma dataset (15,026
individuals, 7,960 eligible adults 15+ per ILO/UNHCR standards) and
implementation on 6,359 working-age refugees (15+) with 150+ socioeconomic
variables achieved 87.4% validation convergence and explainable assessments
across five host countries. EMPATHIA's weighted integration of cultural,
emotional, and ethical factors balances competing value systems while
supporting practitioner-AI collaboration. By augmenting rather than replacing
human expertise, EMPATHIA provides a generalizable framework for AI-driven
allocation tasks where multiple values must be reconciled.

</details>


### [87] [Ethics2vec: aligning automatic agents and human preferences](https://arxiv.org/abs/2508.07673)
*Gianluca Bontempi*

Main category: cs.AI

TL;DR: 论文提出了一种名为Ethics2Vec的方法，通过向量化AI代理的决策策略，评估其与人类价值观的一致性。


<details>
  <summary>Details</summary>
Motivation: 解决AI系统与人类价值观对齐的挑战，尤其是在涉及不可衡量或不可比较的伦理价值时。

Method: 扩展Anything2vec方法，将代理的决策策略映射为多元向量表示，用于比较和评估与人类价值观的对齐。

Result: 提出了Ethics2Vec方法，适用于二元决策和自动控制场景（如自动驾驶）。

Conclusion: Ethics2Vec为AI伦理对齐提供了一种可行的量化方法。

Abstract: Though intelligent agents are supposed to improve human experience (or make
it more efficient), it is hard from a human perspective to grasp the ethical
values which are explicitly or implicitly embedded in an agent behaviour. This
is the well-known problem of alignment, which refers to the challenge of
designing AI systems that align with human values, goals and preferences. This
problem is particularly challenging since most human ethical considerations
refer to \emph{incommensurable} (i.e. non-measurable and/or incomparable)
values and criteria. Consider, for instance, a medical agent prescribing a
treatment to a cancerous patient. How could it take into account (and/or weigh)
incommensurable aspects like the value of a human life and the cost of the
treatment? Now, the alignment between human and artificial values is possible
only if we define a common space where a metric can be defined and used. This
paper proposes to extend to ethics the conventional Anything2vec approach,
which has been successful in plenty of similar and hard-to-quantify domains
(ranging from natural language processing to recommendation systems and graph
analysis). This paper proposes a way to map an automatic agent decision-making
(or control law) strategy to a multivariate vector representation, which can be
used to compare and assess the alignment with human values. The Ethics2Vec
method is first introduced in the case of an automatic agent performing binary
decision-making. Then, a vectorisation of an automatic control law (like in the
case of a self-driving car) is discussed to show how the approach can be
extended to automatic control settings.

</details>


### [88] [Symmetry-Aware Transformer Training for Automated Planning](https://arxiv.org/abs/2508.07743)
*Markus Fritzsche,Elliot Gestrin,Jendrik Seipp*

Main category: cs.AI

TL;DR: 提出了一种新颖的对比学习目标，使Transformer能够感知对称性，从而弥补其在自动规划领域的局限性。


<details>
  <summary>Details</summary>
Motivation: Transformer在自动规划领域的应用受限，主要由于问题对称性导致组合爆炸，现有方法（如PlanGPT）难以从等效表示中高效学习。

Method: 结合对比学习目标和架构改进，使Transformer能够感知对称性，适用于规划生成或启发式预测。

Result: 在多个规划领域的实验表明，该方法有效且高效地解决了PlanGPT的局限性。

Conclusion: 对称感知训练显著提升了Transformer在自动规划中的性能。

Abstract: While transformers excel in many settings, their application in the field of
automated planning is limited. Prior work like PlanGPT, a state-of-the-art
decoder-only transformer, struggles with extrapolation from easy to hard
planning problems. This in turn stems from problem symmetries: planning tasks
can be represented with arbitrary variable names that carry no meaning beyond
being identifiers. This causes a combinatorial explosion of equivalent
representations that pure transformers cannot efficiently learn from. We
propose a novel contrastive learning objective to make transformers
symmetry-aware and thereby compensate for their lack of inductive bias.
Combining this with architectural improvements, we show that transformers can
be efficiently trained for either plan-generation or heuristic-prediction. Our
results across multiple planning domains demonstrate that our symmetry-aware
training effectively and efficiently addresses the limitations of PlanGPT.

</details>


### [89] [Best-Effort Policies for Robust Markov Decision Processes](https://arxiv.org/abs/2508.07790)
*Alessandro Abate,Thom Badings,Giuseppe De Giacomo,Francesco Fabiano*

Main category: cs.AI

TL;DR: 论文研究了鲁棒马尔可夫决策过程（RMDPs）中的最优策略选择问题，提出了一种新的策略选择标准——最优鲁棒最佳努力（ORBE）策略，以在非完全对抗性概率下最大化期望回报。


<details>
  <summary>Details</summary>
Motivation: 在RMDPs中，存在多个最优鲁棒策略，这些策略在最坏情况下表现相同，但在非对抗性概率下期望回报不同。因此，需要一种更精细的策略选择标准。

Method: 提出ORBE策略，结合了博弈论中的支配和最佳努力概念，要求策略不仅在最坏情况下最优，还需在非对抗性概率下最大化期望回报。并设计了计算ORBE策略的算法。

Result: 证明了ORBE策略的存在性，并分析了其结构。实验验证了方法的可行性。

Conclusion: ORBE策略为RMDPs中的最优策略选择提供了理论依据和实用算法，解决了传统方法中的策略等价性问题。

Abstract: We study the common generalization of Markov decision processes (MDPs) with
sets of transition probabilities, known as robust MDPs (RMDPs). A standard goal
in RMDPs is to compute a policy that maximizes the expected return under an
adversarial choice of the transition probabilities. If the uncertainty in the
probabilities is independent between the states, known as s-rectangularity,
such optimal robust policies can be computed efficiently using robust value
iteration. However, there might still be multiple optimal robust policies,
which, while equivalent with respect to the worst-case, reflect different
expected returns under non-adversarial choices of the transition probabilities.
Hence, we propose a refined policy selection criterion for RMDPs, drawing
inspiration from the notions of dominance and best-effort in game theory.
Instead of seeking a policy that only maximizes the worst-case expected return,
we additionally require the policy to achieve a maximal expected return under
different (i.e., not fully adversarial) transition probabilities. We call such
a policy an optimal robust best-effort (ORBE) policy. We prove that ORBE
policies always exist, characterize their structure, and present an algorithm
to compute them with a small overhead compared to standard robust value
iteration. ORBE policies offer a principled tie-breaker among optimal robust
policies. Numerical experiments show the feasibility of our approach.

</details>


### [90] [KIRETT: Knowledge-Graph-Based Smart Treatment Assistant for Intelligent Rescue Operations](https://arxiv.org/abs/2508.07834)
*Mubaris Nadeem,Johannes Zenkert,Lisa Bender,Christian Weber,Madjid Fathi*

Main category: cs.AI

TL;DR: 论文提出了一种基于知识图谱和人工智能的创新知识管理系统，旨在为急救人员提供实时、智能的治疗建议。


<details>
  <summary>Details</summary>
Motivation: 全球救援需求迅速增长，急救人员在紧急情况下需要快速提供个性化医疗，但缺乏实时知识支持。

Method: 采用知识图谱作为核心知识表示，结合人工智能预识别技术，为急救人员提供智能治疗建议。

Result: 系统能够实时计算、评估和处理知识，优化急救人员的治疗决策。

Conclusion: 该知识管理系统显著提升了急救效率和质量，为紧急医疗提供了智能化支持。

Abstract: Over the years, the need for rescue operations throughout the world has
increased rapidly. Demographic changes and the resulting risk of injury or
health disorders form the basis for emergency calls. In such scenarios, first
responders are in a rush to reach the patient in need, provide first aid, and
save lives. In these situations, they must be able to provide personalized and
optimized healthcare in the shortest possible time and estimate the patients
condition with the help of freshly recorded vital data in an emergency
situation. However, in such a timedependent situation, first responders and
medical experts cannot fully grasp their knowledge and need assistance and
recommendation for further medical treatments. To achieve this, on the spot
calculated, evaluated, and processed knowledge must be made available to
improve treatments by first responders. The Knowledge Graph presented in this
article as a central knowledge representation provides first responders with an
innovative knowledge management that enables intelligent treatment
recommendations with an artificial intelligence-based pre-recognition of the
situation.

</details>


### [91] [\(X\)-evolve: Solution space evolution powered by large language models](https://arxiv.org/abs/2508.07932)
*Yi Zhai,Zhiqiang Wei,Ruohan Li,Keyu Pan,Shuo Liu,Lu Zhang,Jianmin Ji,Wuyang Zhang,Yu Zhang,Yanyong Zhang*

Main category: cs.AI

TL;DR: 论文提出了一种名为X-evolve的新方法，通过进化解空间而非单个解，显著减少了大型语言模型（LLM）的调用成本，并提高了搜索效率。


<details>
  <summary>Details</summary>
Motivation: 当前结合LLM和进化算法（EA）的方法通常进化单个解，导致LLM调用成本高昂。X-evolve旨在通过进化解空间来降低成本并提升效率。

Method: X-evolve通过LLM生成可调程序，其中特定代码片段作为参数定义可调解空间。基于分数的搜索算法高效探索这一参数化空间。

Result: 在三个优化问题中验证了X-evolve的有效性：在cap set问题中发现了更大的部分可容许集；在信息论中找到了更大的独立集；在在线装箱问题中生成了优于标准策略的启发式方法。

Conclusion: X-evolve通过进化解空间显著提升了搜索效率，解决了高维问题的计算难题。

Abstract: While combining large language models (LLMs) with evolutionary algorithms
(EAs) shows promise for solving complex optimization problems, current
approaches typically evolve individual solutions, often incurring high LLM call
costs. We introduce \(X\)-evolve, a paradigm-shifting method that instead
evolves solution spaces \(X\) (sets of individual solutions) - subsets of the
overall search space \(S\). In \(X\)-evolve, LLMs generate tunable programs
wherein certain code snippets, designated as parameters, define a tunable
solution space. A score-based search algorithm then efficiently explores this
parametrically defined space, guided by feedback from objective function
scores. This strategy enables broader and more efficient exploration, which can
potentially accelerate convergence at a much lower search cost, requiring up to
two orders of magnitude fewer LLM calls than prior leading methods. We
demonstrate \(X\)-evolve's efficacy across three distinct hard optimization
problems. For the cap set problem, we discover a larger partial admissible set,
establishing a new tighter asymptotic lower bound for the cap set constant (\(C
\ge 2.2203\)). In information theory, we uncover a larger independent set for
the 15-vertex cycle graph (\(\mathcal{C}_{15}^{\boxtimes 5}\), size 19,946),
thereby raising the known lower bound on its Shannon capacity. Furthermore, for
the NP-hard online bin packing problem, we generate heuristics that
consistently outperform standard strategies across established benchmarks. By
evolving solution spaces, our method considerably improves search
effectiveness, making it possible to tackle high-dimensional problems that were
previously computationally prohibitive.

</details>


### [92] [Deep Reinforcement Learning with anticipatory reward in LSTM for Collision Avoidance of Mobile Robots](https://arxiv.org/abs/2508.07941)
*Olivier Poulet,Frédéric Guinand,François Guérin*

Main category: cs.AI

TL;DR: 提出了一种基于短期预测的碰撞风险预测方法，使用LSTM预测机器人位置，并通过动态调整DQN的奖励来减少碰撞。


<details>
  <summary>Details</summary>
Motivation: 在无通信或标识的受限环境中，减少机器人碰撞并提高稳定性。

Method: 使用LSTM预测机器人位置，结合DQN动态调整奖励。

Result: 在1Hz采样频率下，碰撞次数显著减少，稳定性提高。

Conclusion: 该方法计算成本低，适合嵌入式系统实现。

Abstract: This article proposes a collision risk anticipation method based on
short-term prediction of the agents position. A Long Short-Term Memory (LSTM)
model, trained on past trajectories, is used to estimate the next position of
each robot. This prediction allows us to define an anticipated collision risk
by dynamically modulating the reward of a Deep Q-Learning Network (DQN) agent.
The approach is tested in a constrained environment, where two robots move
without communication or identifiers. Despite a limited sampling frequency (1
Hz), the results show a significant decrease of the collisions number and a
stability improvement. The proposed method, which is computationally
inexpensive, appears particularly attractive for implementation on embedded
systems.

</details>


### [93] [FEAT: A Multi-Agent Forensic AI System with Domain-Adapted Large Language Model for Automated Cause-of-Death Analysis](https://arxiv.org/abs/2508.07950)
*Chen Shen,Wanqing Zhang,Kehan Li,Erwen Huang,Haitao Bi,Aiying Fan,Yiwen Shen,Hongmei Dong,Ji Zhang,Yuming Shao,Zengjia Liu,Xinshe Liu,Tao Li,Chunxia Yan,Shuanliang Fan,Di Wu,Jianhua Ma,Bin Cong,Zhenyuan Wang,Chunfeng Lian*

Main category: cs.AI

TL;DR: FEAT是一个基于多智能体AI框架的自动化法医死因分析系统，通过大型语言模型优化法医工作流程，提高诊断一致性和效率。


<details>
  <summary>Details</summary>
Motivation: 解决法医死因鉴定中的人力短缺和诊断差异问题，特别是在中国等案件量大的法医体系中。

Method: FEAT采用多智能体架构，包括任务分解的中央规划器、证据分析的专业局部求解器、迭代优化的记忆与反思模块，以及结论合成的全局求解器。结合工具增强推理、分层检索增强生成和法医调优的LLM。

Result: 在多样化中国案例中，FEAT优于现有AI系统，表现稳健且专家一致性高，能检测细微证据。

Conclusion: FEAT是首个基于LLM的法医AI系统，结合AI效率与人类监督，可提升法医服务的可及性和可靠性。

Abstract: Forensic cause-of-death determination faces systemic challenges, including
workforce shortages and diagnostic variability, particularly in high-volume
systems like China's medicolegal infrastructure. We introduce FEAT (ForEnsic
AgenT), a multi-agent AI framework that automates and standardizes death
investigations through a domain-adapted large language model. FEAT's
application-oriented architecture integrates: (i) a central Planner for task
decomposition, (ii) specialized Local Solvers for evidence analysis, (iii) a
Memory & Reflection module for iterative refinement, and (iv) a Global Solver
for conclusion synthesis. The system employs tool-augmented reasoning,
hierarchical retrieval-augmented generation, forensic-tuned LLMs, and
human-in-the-loop feedback to ensure legal and medical validity. In evaluations
across diverse Chinese case cohorts, FEAT outperformed state-of-the-art AI
systems in both long-form autopsy analyses and concise cause-of-death
conclusions. It demonstrated robust generalization across six geographic
regions and achieved high expert concordance in blinded validations. Senior
pathologists validated FEAT's outputs as comparable to those of human experts,
with improved detection of subtle evidentiary nuances. To our knowledge, FEAT
is the first LLM-based AI agent system dedicated to forensic medicine, offering
scalable, consistent death certification while maintaining expert-level rigor.
By integrating AI efficiency with human oversight, this work could advance
equitable access to reliable medicolegal services while addressing critical
capacity constraints in forensic systems.

</details>


### [94] [Interpreting Fedspeak with Confidence: A LLM-Based Uncertainty-Aware Framework Guided by Monetary Policy Transmission Paths](https://arxiv.org/abs/2508.08001)
*Rui Yao,Qi Chai,Jinhai Yao,Siyuan Li,Junhao Chen,Qi Zhang,Hao Wang*

Main category: cs.AI

TL;DR: 本文提出了一种基于LLM的、不确定性感知的框架，用于解析美联储的Fedspeak并分类其货币政策立场，结合领域特定推理和动态不确定性解码模块，显著提升了分类准确性和模型可靠性。


<details>
  <summary>Details</summary>
Motivation: 美联储的Fedspeak是一种隐含政策信号的语言，自动解析其内容对金融预测和算法交易具有重要意义。

Method: 采用LLM框架，结合货币政策传导机制的领域特定推理，并引入动态不确定性解码模块评估预测置信度。

Result: 实验表明，该框架在政策立场分析任务中达到最先进性能，且感知不确定性与模型错误率显著正相关。

Conclusion: 该框架有效提升了Fedspeak的解析能力，为金融和政策分析提供了可靠工具。

Abstract: "Fedspeak", the stylized and often nuanced language used by the U.S. Federal
Reserve, encodes implicit policy signals and strategic stances. The Federal
Open Market Committee strategically employs Fedspeak as a communication tool to
shape market expectations and influence both domestic and global economic
conditions. As such, automatically parsing and interpreting Fedspeak presents a
high-impact challenge, with significant implications for financial forecasting,
algorithmic trading, and data-driven policy analysis. In this paper, we propose
an LLM-based, uncertainty-aware framework for deciphering Fedspeak and
classifying its underlying monetary policy stance. Technically, to enrich the
semantic and contextual representation of Fedspeak texts, we incorporate
domain-specific reasoning grounded in the monetary policy transmission
mechanism. We further introduce a dynamic uncertainty decoding module to assess
the confidence of model predictions, thereby enhancing both classification
accuracy and model reliability. Experimental results demonstrate that our
framework achieves state-of-the-art performance on the policy stance analysis
task. Moreover, statistical analysis reveals a significant positive correlation
between perceptual uncertainty and model error rates, validating the
effectiveness of perceptual uncertainty as a diagnostic signal.

</details>


### [95] [Fitting Description Logic Ontologies to ABox and Query Examples](https://arxiv.org/abs/2508.08007)
*Maurice Funk,Marvin Grosser,Carsten Lutz*

Main category: cs.AI

TL;DR: 研究基于本体介导查询的拟合问题，确定是否存在满足正负例的本体，并分析计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 解决本体介导查询中拟合问题的存在性和计算复杂度，为实际应用提供理论支持。

Method: 使用描述逻辑ALC和ALCI作为本体语言，结合多种查询语言（AQs、CQs、UCQs），分析拟合问题的有效表征和计算复杂度。

Result: 发现拟合问题对AQs和完整CQs是CONP复杂度，对CQs和UCQs是2EXPTIME完全复杂度。

Conclusion: 研究为不同查询语言下的本体拟合问题提供了理论框架和复杂度界限，适用于ALC和ALCI。

Abstract: We study a fitting problem inspired by ontology-mediated querying: given a
collection
  of positive and negative examples of
  the form $(\mathcal{A},q)$ with
  $\mathcal{A}$ an ABox and $q$ a Boolean query, we seek
  an ontology $\mathcal{O}$ that satisfies $\mathcal{A} \cup \mathcal{O} \vDash
q$ for all positive examples and $\mathcal{A} \cup \mathcal{O}\not\vDash q$ for
all negative examples.
  We consider the description logics $\mathcal{ALC}$ and $\mathcal{ALCI}$ as
ontology languages and
  a range of query languages that
  includes atomic queries (AQs), conjunctive queries (CQs), and unions thereof
(UCQs).
  For all of the resulting fitting problems,
  we provide
  effective characterizations and determine the computational complexity
  of deciding whether a fitting ontology exists. This problem turns out to be
${\small CO}NP$ for AQs and full CQs
  and $2E{\small XP}T{\small IME}$-complete for CQs and UCQs.
  These results hold for both $\mathcal{ALC}$ and $\mathcal{ALCI}$.

</details>


### [96] [AdaptFlow: Adaptive Workflow Optimization via Meta-Learning](https://arxiv.org/abs/2508.08053)
*Runchuan Zhu,Bowen Jiang,Lingrui Mei,Fangkai Yang,Lu Wang,Haoxiang Gao,Fengshuo Bai,Pu Zhao,Qingwei Lin,Saravan Rajmohan,Dongmei Zhang*

Main category: cs.AI

TL;DR: AdaptFlow是一个基于自然语言的元学习框架，通过双层优化实现任务级别的快速适应，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型（LLM）工作流通常依赖静态模板或手动设计，限制了适应性和可扩展性。

Method: AdaptFlow采用双层优化：内层通过LLM反馈优化子任务工作流，外层更新共享初始化以实现跨任务泛化。

Result: 在问答、代码生成和数学推理任务中，AdaptFlow表现优于手动和自动基线，达到SOTA。

Conclusion: AdaptFlow通过语言引导的修改实现泛化，为复杂任务提供了高效的工作流适应方案。

Abstract: Recent advances in large language models (LLMs) have sparked growing interest
in agentic workflows, which are structured sequences of LLM invocations
intended to solve complex tasks. However, existing approaches often rely on
static templates or manually designed workflows, which limit adaptability to
diverse tasks and hinder scalability. We propose AdaptFlow, a natural
language-based meta-learning framework inspired by model-agnostic meta-learning
(MAML). AdaptFlow learns a generalizable workflow initialization that enables
rapid subtask-level adaptation. It employs a bi-level optimization scheme: the
inner loop refines the workflow for a specific subtask using LLM-generated
feedback, while the outer loop updates the shared initialization to perform
well across tasks. This setup allows AdaptFlow to generalize effectively to
unseen tasks by adapting the initialized workflow through language-guided
modifications. Evaluated across question answering, code generation, and
mathematical reasoning benchmarks, AdaptFlow consistently outperforms both
manually crafted and automatically searched baselines, achieving
state-of-the-art results with strong generalization across tasks and models.
The source code and data are available at
https://github.com/microsoft/DKI_LLM/tree/AdaptFlow/AdaptFlow.

</details>


### [97] [FNBT: Full Negation Belief Transformation for Open-World Information Fusion Based on Dempster-Shafer Theory of Evidence](https://arxiv.org/abs/2508.08075)
*Meishen He,Wenjun Ma,Jiao Wang,Huijun Yue,Xiaoma Fan*

Main category: cs.AI

TL;DR: 提出了一种基于Dempster-Shafer理论的开放世界信息融合方法FNBT，解决了异构框架下证据融合的问题。


<details>
  <summary>Details</summary>
Motivation: 现实场景中，数据或模型常来自不同区域或组织，导致异构框架，传统融合方法效果不佳。

Method: 引入标准判断开放世界任务，扩展框架以容纳异构元素，采用全否定机制转换质量函数，应用现有组合规则。

Result: 理论满足质量函数不变性、遗传性和本质冲突消除；实证在分类任务中表现优异，解决了Zadeh反例。

Conclusion: FNBT方法在理论和实践中均有效，适用于开放世界信息融合。

Abstract: The Dempster-Shafer theory of evidence has been widely applied in the field
of information fusion under uncertainty. Most existing research focuses on
combining evidence within the same frame of discernment. However, in real-world
scenarios, trained algorithms or data often originate from different regions or
organizations, where data silos are prevalent. As a result, using different
data sources or models to generate basic probability assignments may lead to
heterogeneous frames, for which traditional fusion methods often yield
unsatisfactory results. To address this challenge, this study proposes an
open-world information fusion method, termed Full Negation Belief
Transformation (FNBT), based on the Dempster-Shafer theory. More specially, a
criterion is introduced to determine whether a given fusion task belongs to the
open-world setting. Then, by extending the frames, the method can accommodate
elements from heterogeneous frames. Finally, a full negation mechanism is
employed to transform the mass functions, so that existing combination rules
can be applied to the transformed mass functions for such information fusion.
Theoretically, the proposed method satisfies three desirable properties, which
are formally proven: mass function invariance, heritability, and essential
conflict elimination. Empirically, FNBT demonstrates superior performance in
pattern classification tasks on real-world datasets and successfully resolves
Zadeh's counterexample, thereby validating its practical effectiveness.

</details>


### [98] [TeamMedAgents: Enhancing Medical Decision-Making of LLMs Through Structured Teamwork](https://arxiv.org/abs/2508.08115)
*Pranav Pushkar Mishra,Mohammad Arvan,Mohan Zalake*

Main category: cs.AI

TL;DR: TeamMedAgents通过将人类团队合作的心理学模型应用于多代理医疗决策系统，显著提升了医疗基准测试的表现。


<details>
  <summary>Details</summary>
Motivation: 将人类团队合作的心理学理论（如Salas的“Big Five”模型）应用于AI多代理系统，以提升医疗决策的协作效果。

Method: 实现并评估六个团队合作核心组件（如团队领导力、共享心智模型等），通过模块化机制在多代理架构中验证其效果。

Result: 在8个医疗基准测试中，7个表现显著提升，且不同任务和领域需要不同的团队合作配置。

Conclusion: TeamMedAgents为关键决策领域的多代理系统设计提供了基于证据的方法，推动了协作AI的发展。

Abstract: We present TeamMedAgents, a novel multi-agent approach that systematically
integrates evidence-based teamwork components from human-human collaboration
into medical decision-making with large language models (LLMs). Our approach
validates an organizational psychology teamwork model from human collaboration
to computational multi-agent medical systems by operationalizing six core
teamwork components derived from Salas et al.'s "Big Five" model: team
leadership, mutual performance monitoring, team orientation, shared mental
models, closed-loop communication, and mutual trust. We implement and evaluate
these components as modular, configurable mechanisms within an adaptive
collaboration architecture while assessing the effect of the number of agents
involved based on the task's requirements and domain. Systematic evaluation of
computational implementations of teamwork behaviors across eight medical
benchmarks (MedQA, MedMCQA, MMLU-Pro Medical, PubMedQA, DDXPlus, MedBullets,
Path-VQA, and PMC-VQA) demonstrates consistent improvements across 7 out of 8
evaluated datasets. Controlled ablation studies conducted on 50 questions per
configuration across 3 independent runs provide mechanistic insights into
individual component contributions, revealing optimal teamwork configurations
that vary by reasoning task complexity and domain-specific requirements. Our
ablation analyses reveal dataset-specific optimal teamwork configurations,
indicating that different medical reasoning modalities benefit from distinct
collaborative patterns. TeamMedAgents represents an advancement in
collaborative AI by providing a systematic translation of established teamwork
theories from human collaboration into agentic collaboration, establishing a
foundation for evidence-based multi-agent system design in critical
decision-making domains.

</details>


### [99] [BlindGuard: Safeguarding LLM-based Multi-Agent Systems under Unknown Attacks](https://arxiv.org/abs/2508.08127)
*Rui Miao,Yixin Liu,Yili Wang,Xu Shen,Yue Tan,Yiwei Dai,Shirui Pan,Xin Wang*

Main category: cs.AI

TL;DR: BlindGuard是一种无监督防御方法，用于检测多智能体系统中的恶意代理，无需攻击标签或先验知识。


<details>
  <summary>Details</summary>
Motivation: 现有监督防御方法依赖标记数据，不适用于实际场景，需开发更通用的无监督防御方法。

Method: 通过分层代理编码器捕捉个体、邻域和全局交互模式，结合噪声注入和对比学习训练检测模型。

Result: 实验表明BlindGuard能有效检测多种攻击类型，且泛化能力优于监督基线方法。

Conclusion: BlindGuard为多智能体系统提供了一种实用且通用的无监督防御方案。

Abstract: The security of LLM-based multi-agent systems (MAS) is critically threatened
by propagation vulnerability, where malicious agents can distort collective
decision-making through inter-agent message interactions. While existing
supervised defense methods demonstrate promising performance, they may be
impractical in real-world scenarios due to their heavy reliance on labeled
malicious agents to train a supervised malicious detection model. To enable
practical and generalizable MAS defenses, in this paper, we propose BlindGuard,
an unsupervised defense method that learns without requiring any
attack-specific labels or prior knowledge of malicious behaviors. To this end,
we establish a hierarchical agent encoder to capture individual, neighborhood,
and global interaction patterns of each agent, providing a comprehensive
understanding for malicious agent detection. Meanwhile, we design a
corruption-guided detector that consists of directional noise injection and
contrastive learning, allowing effective detection model training solely on
normal agent behaviors. Extensive experiments show that BlindGuard effectively
detects diverse attack types (i.e., prompt injection, memory poisoning, and
tool attack) across MAS with various communication patterns while maintaining
superior generalizability compared to supervised baselines. The code is
available at: https://github.com/MR9812/BlindGuard.

</details>


### [100] [From Natural Language to Solver-Ready Power System Optimization: An LLM-Assisted, Validation-in-the-Loop Framework](https://arxiv.org/abs/2508.08147)
*Yunkai Hu,Tianqiao Zhao,Meng Yue*

Main category: cs.AI

TL;DR: 论文提出了一种基于大型语言模型（LLMs）的代理，将电力系统优化场景的自然语言描述自动转换为紧凑、可求解的数学公式，并生成解决方案。该方法避免了直接使用LLMs生成解决方案的不可行或次优问题，通过验证和迭代修复确保可行性。


<details>
  <summary>Details</summary>
Motivation: 直接使用LLMs生成优化问题的解决方案常导致不可行或次优结果，因其缺乏数值精度和约束处理能力。本文旨在通过结合LLMs与传统优化求解器，实现高效可靠的决策支持。

Method: 提出了一种结合LLMs和优化求解器的流程，包括领域感知提示、系统验证和迭代修复，生成可求解的数学模型。以机组组合问题为例验证方法。

Result: 实验表明，该方法能生成最优或接近最优的调度方案及目标成本，验证了结合任务特定验证的求解器显著提升解决方案的可靠性。

Conclusion: 结合AI与传统优化框架，能够高效地将高层问题描述转化为可执行的数学模型，提升能源系统决策效率。

Abstract: This paper introduces a novel Large Language Models (LLMs)-assisted agent
that automatically converts natural-language descriptions of power system
optimization scenarios into compact, solver-ready formulations and generates
corresponding solutions. In contrast to approaches that rely solely on LLM to
produce solutions directly, the proposed method focuses on discovering a
mathematically compatible formulation that can be efficiently solved by
off-the-shelf optimization solvers. Directly using LLMs to produce solutions
often leads to infeasible or suboptimal results, as these models lack the
numerical precision and constraint-handling capabilities of established
optimization solvers. The pipeline integrates a domain-aware prompt and schema
with an LLM, enforces feasibility through systematic validation and iterative
repair, and returns both solver-ready models and user-facing results. Using the
unit commitment problem as a representative case study, the agent produces
optimal or near-optimal schedules along with the associated objective costs.
Results demonstrate that coupling the solver with task-specific validation
significantly enhances solution reliability. This work shows that combining AI
with established optimization frameworks bridges high-level problem
descriptions and executable mathematical models, enabling more efficient
decision-making in energy systems

</details>
