<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 14]
- [cs.CR](#cs.CR) [Total: 19]
- [cs.AI](#cs.AI) [Total: 51]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Beyond Single LLMs: Enhanced Code Generation via Multi-Stage Performance-Guided LLM Orchestration](https://arxiv.org/abs/2510.01379)
*Huashan Chen,Zhenyu Qi,Haotang Li,Hong Chen,Jinfu Chen,Kebin Peng,In Kee Kim,Kyu Hyung Lee,Sen He*

Main category: cs.SE

TL;DR: 提出了一种多阶段、性能导向的编排框架PerfOrch，动态选择最适合的LLM来处理编程任务，在无需微调的情况下显著提升了代码生成的功能正确性和运行时性能。


<details>
  <summary>Details</summary>
Motivation: 当前单一模型方法忽视了不同模型在不同编程语言、算法领域和开发阶段中的异构计算优势，需要一种能够利用各模型专长的动态编排方法。

Method: 基于对17个最先进LLM在5种编程语言上的实证研究，开发了PerfOrch框架，通过分阶段验证和回滚机制，在生成-修复-优化工作流中动态路由任务到最适合的LLM。

Result: 在HumanEval-X和EffiBench-X基准测试中分别达到96.22%和91.37%的平均正确率，显著优于GPT-4o的78.66%和49.11%；在58.76%的问题上改进了执行时间，跨语言中位数加速达17.67%-27.66%。

Conclusion: PerfOrch提供了一种可扩展的即插即用架构，能够适应快速发展的生成式AI格局，为生产级自动化软件工程提供了新范式。

Abstract: While Large Language Models (LLMs) have become the predominant paradigm for
automated code generation, current single-model approaches fundamentally ignore
the heterogeneous computational strengths that different models exhibit across
programming languages, algorithmic domains, and development stages. This paper
challenges the single-model convention by introducing a multi-stage,
performance-guided orchestration framework that dynamically routes coding tasks
to the most suitable LLMs within a structured generate-fix-refine workflow. Our
approach is grounded in a comprehensive empirical study of 17 state-of-the-art
LLMs across five programming languages (Python, Java, C++, Go, and Rust) using
HumanEval-X benchmark. The study, which evaluates both functional correctness
and runtime performance metrics (execution time, mean/max memory utilization,
and CPU efficiency), reveals pronounced performance heterogeneity by language,
development stage, and problem category. Guided by these empirical insights, we
present PerfOrch, an LLM agent that orchestrates top-performing LLMs for each
task context through stage-wise validation and rollback mechanisms. Without
requiring model fine-tuning, PerfOrch achieves substantial improvements over
strong single-model baselines: average correctness rates of 96.22% and 91.37%
on HumanEval-X and EffiBench-X respectively, surpassing GPT-4o's 78.66% and
49.11%. Beyond correctness gains, the framework delivers consistent performance
optimizations, improving execution time for 58.76% of problems with median
speedups ranging from 17.67% to 27.66% across languages on two benchmarks. The
framework's plug-and-play architecture ensures practical scalability, allowing
new LLMs to be profiled and integrated seamlessly, thereby offering a paradigm
for production-grade automated software engineering that adapts to the rapidly
evolving generative AI landscape.

</details>


### [2] [Deciphering WONTFIX: A Mixed-Method Study on Why GitHub Issues Get Rejected](https://arxiv.org/abs/2510.01514)
*J. Alexander Curtis,Sharadha Kasiviswanathan,Nasir Eisty*

Main category: cs.SE

TL;DR: 本研究分析了GitHub上wontfix标签的普遍性和使用原因，发现约30%的项目使用该标签，主要用于用户提交的bug报告和功能请求，并识别了8个常见的使用主题。


<details>
  <summary>Details</summary>
Motivation: wontfix标签在GitHub仓库中被广泛使用但理解有限，其对项目管理和开源社区动态的影响尚不明确，需要系统研究其使用模式和原因。

Method: 采用混合方法，从3,132个最受欢迎的GitHub仓库收集数据，通过定量分析评估wontfix标签的普遍性，并通过开放式编码和主题分析对使用原因进行定性分类。

Result: 约30%的GitHub项目使用wontfix标签，主要应用于用户提交的bug报告和功能请求，识别出8个常见使用主题，包括用户特定控制因素和维护者特定决策等。

Conclusion: wontfix标签是GitHub项目中管理资源和引导贡献者努力的关键工具，但也可能抑制社区参与并影响项目管理透明度，理解其使用原因有助于项目管理者做出明智决策。

Abstract: Context: The ``wontfix'' label is a widely used yet narrowly understood tool
in GitHub repositories, indicating that an issue will not be pursued further.
Despite its prevalence, the impact of this label on project management and
community dynamics within open-source software development is not clearly
defined. Objective: This study examines the prevalence and reasons behind
issues being labeled as wontfix across various open-source repositories on
GitHub. Method: Employing a mixed-method approach, we analyze both quantitative
data to assess the prevalence of the wontfix label and qualitative data to
explore the reasoning that it was used. Data were collected from 3,132 of
GitHub's most-popular repositories. Later, we employ open coding and thematic
analysis to categorize the reasons behind wontfix labels, providing a
structured understanding of the issue management landscape. Results: Our
findings show that about 30% of projects on GitHub apply the wontfix label to
some issues. These issues most often occur on user-submitted issues for bug
reports and feature requests. The study identified eight common themes behind
labeling issues as wontfix, ranging from user-specific control factors to
maintainer-specific decisions. Conclusions: The wontfix label is a critical
tool for managing resources and guiding contributor efforts in GitHub projects.
However, it can also discourage community involvement and obscure the
transparency of project management. Understanding these reasons aids project
managers in making informed decisions and fostering efficient collaboration
within open-source communities.

</details>


### [3] [MIMIC: Integrating Diverse Personality Traits for Better Game Testing Using Large Language Model](https://arxiv.org/abs/2510.01635)
*Yifei Chen,Sarra Habchi,Lili Wei*

Main category: cs.SE

TL;DR: MIMIC是一个将多样化人格特征整合到游戏代理中的框架，使代理能够在相似情境下采用不同的游戏策略，从而提高测试覆盖率和游戏交互多样性。


<details>
  <summary>Details</summary>
Motivation: 传统游戏测试代理往往忽视人类玩家因个性差异而采用的不同策略，导致在相似情境下产生重复的解决方案，难以触发多样化的游戏交互或发现边缘情况。

Method: 通过整合多样化人格特征到游戏代理中，使代理能够模仿不同的游戏风格和策略。

Result: 在Minecraft中，MIMIC相比最先进的代理实现了更高的任务完成率，并提供了更多样化的解决方案。

Conclusion: MIMIC在游戏测试方面具有显著潜力，能够实现更高的测试覆盖率和更丰富的游戏交互。

Abstract: Modern video games pose significant challenges for traditional automated
testing algorithms, yet intensive testing is crucial to ensure game quality. To
address these challenges, researchers designed gaming agents using
Reinforcement Learning, Imitation Learning, or Large Language Models. However,
these agents often neglect the diverse strategies employed by human players due
to their different personalities, resulting in repetitive solutions in similar
situations. Without mimicking varied gaming strategies, these agents struggle
to trigger diverse in-game interactions or uncover edge cases.
  In this paper, we present MIMIC, a novel framework that integrates diverse
personality traits into gaming agents, enabling them to adopt different gaming
strategies for similar situations. By mimicking different playstyles, MIMIC can
achieve higher test coverage and richer in-game interactions across different
games. It also outperforms state-of-the-art agents in Minecraft by achieving a
higher task completion rate and providing more diverse solutions. These results
highlight MIMIC's significant potential for effective game testing.

</details>


### [4] [FOSS-chain: using blockchain for Open Source Software license compliance](https://arxiv.org/abs/2510.01740)
*Kypros Iacovou,Georgia M. Kapitsaki,Evangelia Vanezi*

Main category: cs.SE

TL;DR: FOSS-chain是一个结合区块链技术的开源软件许可证管理平台，旨在解决衍生作品中的许可证兼容性问题，自动化许可证合规流程。


<details>
  <summary>Details</summary>
Motivation: 开源软件许可证管理复杂，许可证不兼容可能导致法律纠纷，而区块链的不可篡改特性为许可证管理提供了透明性和可追溯性。

Method: 设计并实现了FOSS-chain网络平台，集成区块链技术，自动化处理14种开源软件许可证的合规流程。

Result: 通过小规模用户研究对原型版本进行了初步评估，结果显示出该平台在实际软件系统中应用的潜力。

Conclusion: FOSS-chain平台展示了区块链技术在开源软件许可证管理中的可行性，为衍生作品的许可证合规提供了有效解决方案。

Abstract: Open Source Software (OSS) is widely used and carries licenses that indicate
the terms under which the software is provided for use, also specifying
modification and distribution rules. Ensuring that users are respecting OSS
license terms when creating derivative works is a complex process. Compliance
issues arising from incompatibilities among licenses may lead to legal
disputes. At the same time, the blockchain technology with immutable entries
offers a mechanism to provide transparency when it comes to licensing and
ensure software changes are recorded. In this work, we are introducing an
integration of blockchain and license management when creating derivative
works, in order to tackle the issue of OSS license compatibility. We have
designed, implemented and performed a preliminary evaluation of FOSS-chain, a
web platform that uses blockchain and automates the license compliance process,
covering 14 OSS licenses. We have evaluated the initial prototype version of
the FOSS-chain platform via a small scale user study. Our preliminary results
are promising, demonstrating the potential of the platform for adaptation on
realistic software systems.

</details>


### [5] [ARENA: A tool for measuring and analysing the energy efficiency of Android apps](https://arxiv.org/abs/2510.01754)
*Hina Anwar*

Main category: cs.SE

TL;DR: ARENA是一个Android Studio插件工具，通过硬件设备测量Android应用能耗，支持数据聚合、统计分析和可视化，帮助开发者在IDE中直接进行能耗对比分析。


<details>
  <summary>Details</summary>
Motivation: 硬件能耗测量方法虽然准确但过程复杂耗时，缺乏开源工具支持开发者和研究人员进行可靠的硬件能耗测量。

Method: 开发ARENA作为IntelliJ和Android Studio插件，连接物理测量设备，在IDE中执行测试场景并计算Android手机能耗，提供数据聚合、统计分析和可视化功能。

Result: ARENA实现了在IDE环境中直接进行硬件能耗测量，简化了测量流程，支持开发者比较不同应用或同一应用不同版本的能耗差异。

Conclusion: ARENA工具解决了硬件能耗测量过程复杂的问题，为开发者和研究人员提供了便捷可靠的能耗分析解决方案。

Abstract: To build energy-efficient apps, there is a need to estimate and analyze their
energy consumption in typical usage scenarios. The energy consumption of
Android apps could be estimated via software-based and hardware-based
approaches. Software-based approaches, while easier to implement, are not as
accurate as hardware-based approaches. The process of measuring the energy
consumption of an Android app via a hardware-based approach typically involves
1) setting up a measurement environment, 2) executing the app under test on a
mobile device, 3) recording current/voltage data via a hardware device to
measure energy consumption, and 4) cleaning and aggregating data for analyses,
reports, and visualizations. Specialized scripts are written for selected
hardware and software components to ensure reliable energy measurements. The
energy measurement process is repeated many times and aggregated to remove
noise. These steps make the hardware-based energy measurement process
time-consuming and not easy to adapt or reproduce. There is a lack of
open-source tools available for developers and researchers to take reliable
energy measurements via hardware devices. In this paper, we present and
demonstrate ARENA, a support tool that enables developers and researchers to
connect to a physical measurement device without leaving the comfort of their
IDE. Developers could use ARENA during development to compare energy
consumption between different apps or versions of the same app. ARENA
calculates energy consumption on an Android smartphone by executing a test
scenario on the app under development. Further, ARENA helps aggregate,
statistically analyze, report, and visualize the data, allowing developers and
researchers to dig into the data directly or visually. We implemented ARENA as
an IntelliJ and Android Studio plugin.

</details>


### [6] [Towards Speeding up Program Repair with Non-Autoregressive Model](https://arxiv.org/abs/2510.01825)
*Zhenyu Yang,Yue Pan,Zhen Yang,Zhongxing Yu*

Main category: cs.SE

TL;DR: NARRepair是首个为自动程序修复任务定制的非自回归代码生成模型，通过修复动作预测器、令牌间依赖提取器和两阶段解码器解决NAR方法在APR任务中的质量问题，在修复速度和准确性方面达到最先进的综合性能。


<details>
  <summary>Details</summary>
Motivation: 传统基于自回归的APR技术存在巨大的时间延迟问题，特别是参数较多的模型延迟更严重。非自回归方法可以并行输出目标代码，但直接应用于APR任务会导致补丁质量下降。

Method: 提出NARRepair模型，包含三个核心组件：修复动作预测器缓解过度修正问题，令牌间依赖提取器解决缺乏令牌间依赖信息问题，两阶段解码器解决缺乏上下文信息问题。

Result: 在三个广泛使用的APR数据集上评估，NARRepair在有限修复时间内表现最佳，相比AR-based APR技术，在GPU环境下修复速度提升了1.4-6.4倍。

Conclusion: NARRepair在修复速度和准确性方面实现了最先进的综合性能，证明了NAR方法在APR任务中的有效性。

Abstract: Enlightened by the success of machine learning techniques in various
application areas, recent years have witnessed a surge of research efforts on
automatic program repair (APR) using machine learning techniques. Previous
machine learning-based APR techniques essentially modified bugs in the
autoregressive (AR) manner, which predicts future values based on past values.
Due to the manner of token-by-token generation, the AR-based APR technique has
a huge time delay. In particular, the delay of the APR model with a large
number of parameters is more serious. To address the issue, we aim to apply the
non-autoregressive (NAR) method to the APR task, which can output target code
in a parallel manner to avoid huge repair delays. However, the naive use of the
NAR manner for the APR task suffers from the issue of compromised patch
quality. To effectively adapt the NAR manner for the APR task, we in this paper
propose NARRepair, the first customized NAR code generation model for the APR
task. The NARRepair model features three major novelties, including 1) the
repair action predictor for alleviating the over-correction issue, 2) the
inter-token dependency extractor for alleviating the issue of lacking
inter-token dependency information, and 3) the two-stage decoder for
alleviating the issue of lacking contextual information. We evaluated NARRepair
on three widely used datasets in the APR community, and the results show that
1) compared to other APR techniques, the NARRepair model has the best
performance within the limited repair time, and 2) compared to AR-based APR
techniques, the repair speed of NARRepair has been increased by 1.4-6.4 times
in the GPU environment. Overall, the results show that NARRepair has achieved
state-of-the-art comprehensive performance in terms of repair speed and
accuracy.

</details>


### [7] [RefFilter: Improving Semantic Conflict Detection via Refactoring-Aware Static Analysis](https://arxiv.org/abs/2510.01960)
*Victor Lira,Paulo Borba,Rodrigo Bonifácio,Galileu Santos e Matheus barbosa*

Main category: cs.SE

TL;DR: 提出了RefFilter工具，通过集成自动重构检测来改进语义干扰检测的精度，减少误报


<details>
  <summary>Details</summary>
Motivation: 现有轻量级静态分析技术在协作软件开发中检测语义干扰时存在高误报率，主要原因是无法有效区分行为保持的代码重构和影响行为的变更

Method: 在现有静态技术基础上加入自动重构检测，从报告中排除行为保持的重构操作

Result: 在标记数据集上减少近32%的误报，虽然伴随轻微假阴性增加，但精度提升显著优于召回率的小幅损失

Conclusion: 重构感知的干扰检测是改进现代开发工作流中合并支持的实用有效策略

Abstract: Detecting semantic interference remains a challenge in collaborative software
development. Recent lightweight static analysis techniques improve efficiency
over SDG-based methods, but they still suffer from a high rate of false
positives. A key cause of these false positives is the presence of
behavior-preserving code refactorings, which current techniques cannot
effectively distinguish from changes that impact behavior and can interfere
with others. To handle this problem we present RefFilter, a refactoring-aware
tool for semantic interference detection. It builds on existing static
techniques by incorporating automated refactoring detection to improve
precision. RefFilter discards behavior-preserving refactorings from reports,
reducing false positives while preserving detection coverage. To evaluate
effectiveness and scalability, use two datasets: a labeled dataset with 99
scenarios and ground truth, and a novel dataset of 1,087 diverse merge
scenarios that we have built. Experimental results show that RefFilter reduces
false positives by nearly 32% on the labeled dataset. While this reduction
comes with a non significant increase in false negatives, the overall gain in
precision significantly outweighs the minor trade-off in recall. These findings
demonstrate that refactoring-aware interference detection is a practical and
effective strategy for improving merge support in modern development workflows.

</details>


### [8] [Clarifying Semantics of In-Context Examples for Unit Test Generation](https://arxiv.org/abs/2510.01994)
*Chen Yang,Lin Yang,Ziqi Wang,Dong Wang,Jianyi Zhou,Junjie Chen*

Main category: cs.SE

TL;DR: CLAST是一种通过系统化重构单元测试来提升语义清晰度的新技术，能够显著改善基于上下文学习的单元测试生成效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的单元测试生成方法严重依赖于上下文示例的质量，但结构不佳或语义不清的测试示例会导致生成效果不理想。

Method: CLAST通过程序分析和LLM重写相结合的方式，将复杂测试分解为逻辑更清晰的测试，并提升语义清晰度。

Result: 在4个开源项目和3个工业项目上的评估显示，CLAST在保持测试有效性和提升语义清晰度方面大幅优于现有最佳技术UTgen，用户研究中85.33%的参与者更偏好CLAST重构的测试。

Conclusion: CLAST重构的测试作为示例能够有效提升基于ICL的单元测试生成方法，为软件测试实践提供了重要价值，并指明了未来研究方向。

Abstract: Recent advances in large language models (LLMs) have enabled promising
performance in unit test generation through in-context learning (ICL). However,
the quality of in-context examples significantly influences the effectiveness
of generated tests-poorly structured or semantically unclear test examples
often lead to suboptimal outputs. In this paper, we propose CLAST, a novel
technique that systematically refines unit tests to improve their semantic
clarity, thereby enhancing their utility as in-context examples. The approach
decomposes complex tests into logically clearer ones and improves semantic
clarity through a combination of program analysis and LLM-based rewriting. We
evaluated CLAST on four open-source and three industrial projects. The results
demonstrate that CLAST largely outperforms UTgen, the state-of-the-art
refinement technique, in both preserving test effectiveness and enhancing
semantic clarity. Specifically, CLAST fully retains the original effectiveness
of unit tests, while UTgen reduces compilation success rate (CSR), pass rate
(PR), test coverage (Cov), and mutation score (MS) by an average of 12.90%,
35.82%, 4.65%, and 5.07%, respectively. Over 85.33% of participants in our user
study preferred the semantic clarity of CLAST-refined tests. Notably,
incorporating CLAST-refined tests as examples effectively improves ICL-based
unit test generation approaches such as RAGGen and TELPA, resulting in an
average increase of 25.97% in CSR, 28.22% in PR, and 45.99% in Cov for
generated tests, compared to incorporating UTgen-refined tests. The insights
from the follow-up user study not only reinforce CLAST's potential impact in
software testing practice but also illuminate avenues for future research.

</details>


### [9] [Automatic Generation of Combinatorial Reoptimisation Problem Specifications: A Vision](https://arxiv.org/abs/2510.02002)
*Maximilian Kratz,Steffen Zschaler,Jens Kosiol,Gabriele Taentzer*

Main category: cs.SE

TL;DR: 该论文提出使用模型驱动工程（MDE）方法，通过声明式建模语言和模型转换来系统化地从原始优化问题规范推导出再优化问题规范，解决在上下文因素变化时需要调整优化解决方案的挑战。


<details>
  <summary>Details</summary>
Motivation: 当优化问题的上下文因素发生变化时，需要重新优化解决方案，但新的优化问题与原始问题存在显著差异：需要最小化对原始解决方案的更改、某些部分无法更改、需要生成从原始解到新解的变更脚本。

Method: 采用模型驱动工程方法，特别是使用声明式建模语言和模型转换来高层次地规范优化问题，系统化地推导再优化问题规范。基于GIPS工具开发了概念验证实现，并应用于助教分配案例。

Result: 提出了组合再优化问题的初步分类，以及推导相应再优化规范的策略。通过助教分配的资源分配问题验证了方法的可行性。

Conclusion: 模型驱动工程为从原始优化问题规范系统化推导再优化问题提供了新的机会，能够有效应对上下文变化时的解决方案调整需求。

Abstract: Once an optimisation problem has been solved, the solution may need
adaptation when contextual factors change. This challenge, also known as
reoptimisation, has been addressed in various problem domains, such as railway
crew rescheduling, nurse rerostering, or aircraft recovery. This requires a
modified problem to be solved again to ensure that the adapted solution is
optimal in the new context. However, the new optimisation problem differs
notably from the original problem: (i) we want to make only minimal changes to
the original solution to minimise the impact; (ii) we may be unable to change
some parts of the original solution (e.g., because they refer to past
allocations); and (iii) we need to derive a change script from the original
solution to the new solution. In this paper, we argue that Model-Driven
Engineering (MDE) - in particular, the use of declarative modelling languages
and model transformations for the high-level specification of optimisation
problems - offers new opportunities for the systematic derivation of
reoptimisation problems from the original optimisation problem specification.
We focus on combinatorial reoptimisation problems and provide an initial
categorisation of changing problems and strategies for deriving the
corresponding reoptimisation specifications. We introduce an initial
proof-of-concept implementation based on the GIPS (Graph-Based (Mixed) Integer
Linear Programming Problem Specification) tool and apply it to an example
resource-allocation problem: the allocation of teaching assistants to teaching
sessions.

</details>


### [10] [ACM SIGSOFT SEN Empirical Software Engineering: Introducing Our New Regular Column](https://arxiv.org/abs/2510.02007)
*Justus Bogner,Roberto Verdecchia*

Main category: cs.SE

TL;DR: 介绍新的ACM SIGSOFT SEN专栏(SEN-ESE)，旨在讨论经验软件工程研究的元方面，包括最佳实践、统计方法等，通过专家访谈、焦点小组等方式促进ESE研究的改进。


<details>
  <summary>Details</summary>
Motivation: 经验软件工程研究领域虽然成熟，但仍面临研究可重现性、外部有效性有限、评审主观性等挑战，且许多方面缺乏明确文档，使新人难以掌握。

Method: 通过新的ACM SIGSOFT SEN专栏，采用专家访谈、焦点小组、调查和立场文章等多种方式，讨论ESE研究的元方面。

Result: 创建了一个定期讨论ESE研究隐含话题的平台，鼓励社区参与和反馈，促进研究实践的反思和改进。

Conclusion: SEN-ESE专栏将为经验软件工程研究社区提供一个讨论挑战性、争议性或未充分探索话题的场所，旨在推动该领域的研究方法和实践持续改进。

Abstract: From its early foundations in the 1970s, empirical software engineering (ESE)
has evolved into a mature research discipline that embraces a plethora of
different topics, methodologies, and industrial practices. Despite its
remarkable progress, the ESE research field still needs to keep evolving, as
new impediments, shortcoming, and technologies emerge. Research
reproducibility, limited external validity, subjectivity of reviews, and
porting research results to industrial practices are just some examples of the
drivers for improvements to ESE research. Additionally, several facets of ESE
research are not documented very explicitly, which makes it difficult for
newcomers to pick them up. With this new regular ACM SIGSOFT SEN column
(SEN-ESE), we introduce a venue for discussing meta-aspects of ESE research,
ranging from general topics such as the nature and best practices for
replication packages, to more nuanced themes such as statistical methods,
interview transcription tools, and publishing interdisciplinary research. Our
aim for the column is to be a place where we can regularly spark conversations
on ESE topics that might not often be touched upon or are left implicit.
Contributions to this column will be grounded in expert interviews, focus
groups, surveys, and position pieces, with the goal of encouraging reflection
and improvement in how we conduct, communicate, teach, and ultimately improve
ESE research. Finally, we invite feedback from the ESE community on
challenging, controversial, or underexplored topics, as well as suggestions for
voices you would like to hear from. While we cannot promise to act on every
idea, we aim to shape this column around the community interests and are
grateful for all contributions.

</details>


### [11] [Towards fairer public transit: Real-time tensor-based multimodal fare evasion and fraud detection](https://arxiv.org/abs/2510.02165)
*Peter Wauyo,Dalia Bwiza,Alain Murara,Edwin Mugume,Eric Umuhoza*

Main category: cs.SE

TL;DR: 提出基于多模态（CCTV视频和音频）的公共交通欺诈检测系统，使用ViViT和AST模型提取特征，通过张量融合网络实现跨模态交互，在自定义数据集上达到89.5%准确率，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决公共交通中的欺诈和逃票问题，减少收入损失，提高乘客安全和运营合规性。现有系统检测能力有限，需要更有效的多模态分析方法。

Method: 使用Vision Transformer for Video (ViViT)提取视频特征，Audio Spectrogram Transformer (AST)分析音频，采用Tensor Fusion Network (TFN)架构通过2-fold笛卡尔积显式建模单模态和双模态交互。

Result: 在自定义数据集上达到89.5%准确率、87.2%精确率和84.0%召回率，显著优于早期融合基线，比最先进交通欺诈检测系统的75%召回率有显著提升。消融研究显示张量融合方法比传统拼接方法F1分数提高7.0%，召回率提升8.8%。

Conclusion: 该多模态系统能有效检测公共交通欺诈行为，支持实时检测，可帮助运营商减少收入损失、提高乘客安全和确保运营合规。

Abstract: This research introduces a multimodal system designed to detect fraud and
fare evasion in public transportation by analyzing closed circuit television
(CCTV) and audio data. The proposed solution uses the Vision Transformer for
Video (ViViT) model for video feature extraction and the Audio Spectrogram
Transformer (AST) for audio analysis. The system implements a Tensor Fusion
Network (TFN) architecture that explicitly models unimodal and bimodal
interactions through a 2-fold Cartesian product. This advanced fusion technique
captures complex cross-modal dynamics between visual behaviors (e.g.,
tailgating,unauthorized access) and audio cues (e.g., fare transaction sounds).
The system was trained and tested on a custom dataset, achieving an accuracy of
89.5%, precision of 87.2%, and recall of 84.0% in detecting fraudulent
activities, significantly outperforming early fusion baselines and exceeding
the 75% recall rates typically reported in state-of-the-art transportation
fraud detection systems. Our ablation studies demonstrate that the tensor
fusion approach provides a 7.0% improvement in the F1 score and an 8.8% boost
in recall compared to traditional concatenation methods. The solution supports
real-time detection, enabling public transport operators to reduce revenue
loss, improve passenger safety, and ensure operational compliance.

</details>


### [12] [SIEVE: Towards Verifiable Certification for Code-datasets](https://arxiv.org/abs/2510.02166)
*Fatou Ndiaye Mbodji,El-hacen Diallo,Jordan Samhi,Kui Liu,Jacques Klein,Tegawendé F. Bissyande*

Main category: cs.SE

TL;DR: SIEVE是一个社区驱动的框架，将代码数据集的质量检查转化为可验证的置信度证书，提供统计保证以替代传统的静态数据集卡片。


<details>
  <summary>Details</summary>
Motivation: 当前公共代码数据集缺乏可验证的质量保证，静态数据集卡片不可审计且无统计保证，团队需要构建孤立的清理流程，导致效率低下和成本增加。

Method: 提出SIEVE框架，将每个属性的检查转化为置信度卡片——机器可读、可验证的证书，具有随时有效的统计边界。

Result: SIEVE框架能够为代码数据集提供可验证的质量认证，替代传统的叙述性卡片。

Conclusion: SIEVE有望降低质量保证成本，增加对代码数据集的信任，通过可随时验证的认证来改进数据集质量保证方法。

Abstract: Code agents and empirical software engineering rely on public code datasets,
yet these datasets lack verifiable quality guarantees. Static 'dataset cards'
inform, but they are neither auditable nor do they offer statistical
guarantees, making it difficult to attest to dataset quality. Teams build
isolated, ad-hoc cleaning pipelines. This fragments effort and raises cost. We
present SIEVE, a community-driven framework. It turns per-property checks into
Confidence Cards-machine-readable, verifiable certificates with anytime-valid
statistical bounds. We outline a research plan to bring SIEVE to maturity,
replacing narrative cards with anytime-verifiable certification. This shift is
expected to lower quality-assurance costs and increase trust in code-datasets.

</details>


### [13] [TAIBOM: Bringing Trustworthiness to AI-Enabled Systems](https://arxiv.org/abs/2510.02169)
*Vadim Safronov,Anthony McCaigue,Nicholas Allott,Andrew Martin*

Main category: cs.SE

TL;DR: 提出了Trusted AI Bill of Materials (TAIBOM)框架，将SBOM原则扩展到AI领域，解决AI系统特有的动态性、数据驱动特性和松散耦合依赖问题。


<details>
  <summary>Details</summary>
Motivation: 开源软件和AI技术的融合给软件供应链带来了新的复杂性，现有依赖管理和系统保证方法无法有效处理AI系统的独特特性，包括动态数据驱动性质和跨数据集、模型、软件组件的松散耦合依赖。

Method: TAIBOM框架提供：(i) 针对AI组件的结构化依赖模型，(ii) 在异构AI管道中传播完整性声明的机制，(iii) 验证组件来源的信任证明过程。

Result: 展示了TAIBOM如何支持AI工作流中的保证、安全性和合规性，突出了其相对于SPDX和CycloneDX等现有标准的优势。

Conclusion: 这项工作通过结构化软件透明度为可信和可验证的AI系统奠定了基础。

Abstract: The growing integration of open-source software and AI-driven technologies
has introduced new layers of complexity into the software supply chain,
challenging existing methods for dependency management and system assurance.
While Software Bills of Materials (SBOMs) have become critical for enhancing
transparency and traceability, current frameworks fall short in capturing the
unique characteristics of AI systems -- namely, their dynamic, data-driven
nature and the loosely coupled dependencies across datasets, models, and
software components. These challenges are compounded by fragmented governance
structures and the lack of robust tools for ensuring integrity, trust, and
compliance in AI-enabled environments.
  In this paper, we introduce Trusted AI Bill of Materials (TAIBOM) -- a novel
framework extending SBOM principles to the AI domain. TAIBOM provides (i) a
structured dependency model tailored for AI components, (ii) mechanisms for
propagating integrity statements across heterogeneous AI pipelines, and (iii) a
trust attestation process for verifying component provenance. We demonstrate
how TAIBOM supports assurance, security, and compliance across AI workflows,
highlighting its advantages over existing standards such as SPDX and CycloneDX.
This work lays the foundation for trustworthy and verifiable AI systems through
structured software transparency.

</details>


### [14] [FalseCrashReducer: Mitigating False Positive Crashes in OSS-Fuzz-Gen Using Agentic AI](https://arxiv.org/abs/2510.02185)
*Paschal C. Amusuo,Dongge Liu,Ricardo Andres Calvo Mendez,Jonathan Metzman,Oliver Chang,James C. Davis*

Main category: cs.SE

TL;DR: 本文提出了两种AI驱动策略来减少OSS-Fuzz-Gen中的误报崩溃：约束驱动的模糊驱动生成和基于上下文的崩溃验证，在1500个基准函数上实现了误报崩溃减少8%、报告崩溃减少一半以上的效果。


<details>
  <summary>Details</summary>
Motivation: 现有自动生成的模糊驱动经常导致误报崩溃，特别是在处理结构化输入和复杂状态需求的函数时，这在工业级模糊驱动生成中会损害系统可信度。

Method: 1) 约束驱动的模糊驱动生成：主动对函数输入和状态施加约束来指导驱动创建；2) 基于上下文的崩溃验证：反应式分析函数调用者来确定报告崩溃是否从程序入口点可行。

Result: 在1500个OSS-Fuzz基准函数上，这些策略使误报崩溃减少达8%，报告崩溃减少超过一半，并证明前沿LLM可以作为可靠程序分析代理。

Conclusion: 研究结果凸显了将AI集成到大规模模糊测试管道中的前景和挑战，展示了AI在提高模糊测试准确性方面的潜力。

Abstract: Fuzz testing has become a cornerstone technique for identifying software bugs
and security vulnerabilities, with broad adoption in both industry and
open-source communities. Directly fuzzing a function requires fuzz drivers,
which translate random fuzzer inputs into valid arguments for the target
function. Given the cost and expertise required to manually develop fuzz
drivers, methods exist that leverage program analysis and Large Language Models
to automatically generate these drivers. However, the generated fuzz drivers
frequently lead to false positive crashes, especially in functions highly
structured input and complex state requirements. This problem is especially
crucial in industry-scale fuzz driver generation efforts like OSS-Fuzz-en, as
reporting false positive crashes to maintainers impede trust in both the system
and the team.
  This paper presents two AI-driven strategies to reduce false positives in
OSS-Fuzz-Gen, a multi-agent system for automated fuzz driver generation. First,
constraint-based fuzz driver generation proactively enforces constraints on a
function's inputs and state to guide driver creation. Second, context-based
crash validation reactively analyzes function callers to determine whether
reported crashes are feasible from program entry points. Using 1,500 benchmark
functions from OSS-Fuzz, we show that these strategies reduce spurious crashes
by up to 8%, cut reported crashes by more than half, and demonstrate that
frontier LLMs can serve as reliable program analysis agents. Our results
highlight the promise and challenges of integrating AI into large-scale fuzzing
pipelines.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [15] [Jailbreaking LLMs via Semantically Relevant Nested Scenarios with Targeted Toxic Knowledge](https://arxiv.org/abs/2510.01223)
*Hui Dou,Ning Xu,Yiwen Zhang,Kaibin Wang*

Main category: cs.CR

TL;DR: 提出RTS-Attack攻击框架，通过构建与查询高度相关的嵌套场景并整合针对性有害知识，成功绕过LLMs的对齐防御，生成隐蔽性强的越狱提示。


<details>
  <summary>Details</summary>
Motivation: 现有嵌套场景方法虽然潜力巨大，但由于恶意意图明显容易被检测。研究发现LLMs的对齐防御对语义相关且包含针对性有害知识的嵌套场景不敏感，这是一个关键但未被充分探索的方向。

Method: 提出RTS-Attack框架，构建与查询高度语义相关的嵌套场景，并整合针对性有害知识，生成不含有害查询的越狱提示，实现高隐蔽性攻击。

Result: 在GPT-4o、Llama3-70b、Gemini-pro等多种先进LLMs上的广泛实验表明，RTS-Attack在效率和通用性方面均优于基线方法。

Conclusion: RTS-Attack框架成功揭示了LLMs对齐防御的漏洞，为评估和改进LLMs安全性提供了重要工具，但需注意该方法可能被恶意利用。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
various tasks. However, they remain exposed to jailbreak attacks, eliciting
harmful responses. The nested scenario strategy has been increasingly adopted
across various methods, demonstrating immense potential. Nevertheless, these
methods are easily detectable due to their prominent malicious intentions. In
this work, we are the first to find and systematically verify that LLMs'
alignment defenses are not sensitive to nested scenarios, where these scenarios
are highly semantically relevant to the queries and incorporate targeted toxic
knowledge. This is a crucial yet insufficiently explored direction. Based on
this, we propose RTS-Attack (Semantically Relevant Nested Scenarios with
Targeted Toxic Knowledge), an adaptive and automated framework to examine LLMs'
alignment. By building scenarios highly relevant to the queries and integrating
targeted toxic knowledge, RTS-Attack bypasses the alignment defenses of LLMs.
Moreover, the jailbreak prompts generated by RTS-Attack are free from harmful
queries, leading to outstanding concealment. Extensive experiments demonstrate
that RTS-Attack exhibits superior performance in both efficiency and
universality compared to the baselines across diverse advanced LLMs, including
GPT-4o, Llama3-70b, and Gemini-pro. Our complete code is available in the
supplementary material. WARNING: THIS PAPER CONTAINS POTENTIALLY HARMFUL
CONTENT.

</details>


### [16] [Fine-Tuning Jailbreaks under Highly Constrained Black-Box Settings: A Three-Pronged Approach](https://arxiv.org/abs/2510.01342)
*Xiangfang Li,Yu Wang,Bo Li*

Main category: cs.CR

TL;DR: 本文提出了一种三管齐下的越狱攻击方法，通过结合安全风格的前缀/后缀包装、敏感词汇的良性编码和后门机制，在仅能提交微调数据的黑盒设置下成功攻击了GPT-4.1和GPT-4o，攻击成功率超过97%。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的快速发展，确保其安全使用变得日益重要。微调是适应下游任务的常用方法，但容易受到越狱攻击。现有研究大多关注过于简化的攻击场景，限制了其在真实防御环境中的实用性。

Method: 采用三管齐下的攻击策略：1) 安全风格的前缀/后缀包装；2) 敏感词汇的良性编码（如下划线）；3) 后门机制。攻击者只能向提供商提交微调数据，而提供商可在上传前数据过滤、训练时防御性微调和训练后安全审计三个阶段部署防御。

Result: 在真实部署中，该方法成功越狱了OpenAI平台上的GPT-4.1和GPT-4o模型，两种模型的攻击成功率均超过97%。

Conclusion: 该研究表明，即使在提供商部署多阶段防御的情况下，精心设计的微调数据攻击仍能有效越狱大语言模型，凸显了当前防御机制的脆弱性。

Abstract: With the rapid advancement of large language models (LLMs), ensuring their
safe use becomes increasingly critical. Fine-tuning is a widely used method for
adapting models to downstream tasks, yet it is vulnerable to jailbreak attacks.
However, most existing studies focus on overly simplified attack scenarios,
limiting their practical relevance to real-world defense settings. To make this
risk concrete, we present a three-pronged jailbreak attack and evaluate it
against provider defenses under a dataset-only black-box fine-tuning interface.
In this setting, the attacker can only submit fine-tuning data to the provider,
while the provider may deploy defenses across stages: (1) pre-upload data
filtering, (2) training-time defensive fine-tuning, and (3) post-training
safety audit. Our attack combines safety-styled prefix/suffix wrappers, benign
lexical encodings (underscoring) of sensitive tokens, and a backdoor mechanism,
enabling the model to learn harmful behaviors while individual datapoints
appear innocuous. Extensive experiments demonstrate the effectiveness of our
approach. In real-world deployment, our method successfully jailbreaks GPT-4.1
and GPT-4o on the OpenAI platform with attack success rates above 97% for both
models. Our code is available at
https://github.com/lxf728/tri-pronged-ft-attack.

</details>


### [17] [Integrated Security Mechanisms for Weight Protection in Memristive Crossbar Arrays](https://arxiv.org/abs/2510.01350)
*Muhammad Faheemur Rahman,Wayne Burleson*

Main category: cs.CR

TL;DR: 提出了两种安全机制（密钥置换器和水印保护列）来保护忆阻器交叉阵列中的权重，防止对抗性提取，同时建立可验证的所有权。


<details>
  <summary>Details</summary>
Motivation: 忆阻器交叉阵列虽然适合机器学习和神经形态系统，但非易失性忆阻器容易受到安全威胁，特别是当硬件被攻陷时，存储的权重可能被提取。这些权重代表经过长时间训练获得的知识产权，需要保护。

Method: 设计了两种安全机制：密钥置换器通过密钥对权重进行置换保护；水印保护列在交叉阵列中添加特殊列来嵌入所有权信息。这两种机制都能与现有忆阻器交叉阵列架构高效集成，无需重大设计修改。

Result: 在45nm、22nm和7nm CMOS节点上的仿真显示，两种机制都能提供强大的保护，面积、延迟和功耗的开销均低于10%。使用MNIST数据集的初步实验进一步验证了该方法的可行性。

Conclusion: 所提出的安全机制能够以最小的性能代价有效保护忆阻器内存计算系统，为忆阻器硬件提供了实用的安全解决方案。

Abstract: Memristive crossbar arrays enable in-memory computing by performing parallel
analog computations directly within memory, making them well-suited for machine
learning, neural networks, and neuromorphic systems. However, despite their
advantages, non-volatile memristors are vulnerable to security threats (such as
adversarial extraction of stored weights when the hardware is compromised.
Protecting these weights is essential since they represent valuable
intellectual property resulting from lengthy and costly training processes
using large, often proprietary, datasets. As a solution we propose two security
mechanisms: Keyed Permutor and Watermark Protection Columns; where both
safeguard critical weights and establish verifiable ownership (even in cases of
data leakage). Our approach integrates efficiently with existing memristive
crossbar architectures without significant design modifications. Simulations
across 45nm, 22nm, and 7nm CMOS nodes, using a realistic interconnect model and
a large RF dataset, show that both mechanisms offer robust protection with
under 10% overhead in area, delay and power. We also present initial
experiments employing the widely known MNIST dataset; further highlighting the
feasibility of securing memristive in-memory computing systems with minimal
performance trade-offs.

</details>


### [18] [WAInjectBench: Benchmarking Prompt Injection Detections for Web Agents](https://arxiv.org/abs/2510.01354)
*Yinuo Liu,Ruohan Xu,Xilong Wang,Yuqi Jia,Neil Zhenqiang Gong*

Main category: cs.CR

TL;DR: 首个针对web代理的提示注入攻击检测基准研究，系统评估了文本和图像检测方法，发现现有检测器对不含显式指令或使用不可察觉扰动的攻击效果不佳。


<details>
  <summary>Details</summary>
Motivation: 现有提示注入攻击检测方法缺乏对web代理的系统性评估，需要填补这一研究空白。

Method: 提出细粒度攻击分类，构建包含恶意和良性样本的数据集，系统化文本和图像检测方法，并在多种场景下评估性能。

Result: 部分检测器能识别依赖显式文本指令或可见图像扰动的攻击，但对不含显式指令或使用不可察觉扰动的攻击检测效果差。

Conclusion: 需要开发更强大的检测方法来应对复杂的提示注入攻击，特别是那些不依赖显式指令的攻击类型。

Abstract: Multiple prompt injection attacks have been proposed against web agents. At
the same time, various methods have been developed to detect general prompt
injection attacks, but none have been systematically evaluated for web agents.
In this work, we bridge this gap by presenting the first comprehensive
benchmark study on detecting prompt injection attacks targeting web agents. We
begin by introducing a fine-grained categorization of such attacks based on the
threat model. We then construct datasets containing both malicious and benign
samples: malicious text segments generated by different attacks, benign text
segments from four categories, malicious images produced by attacks, and benign
images from two categories. Next, we systematize both text-based and
image-based detection methods. Finally, we evaluate their performance across
multiple scenarios. Our key findings show that while some detectors can
identify attacks that rely on explicit textual instructions or visible image
perturbations with moderate to high accuracy, they largely fail against attacks
that omit explicit instructions or employ imperceptible perturbations. Our
datasets and code are released at:
https://github.com/Norrrrrrr-lyn/WAInjectBench.

</details>


### [19] [Breaking the Code: Security Assessment of AI Code Agents Through Systematic Jailbreaking Attacks](https://arxiv.org/abs/2510.01359)
*Shoumik Saha,Jifan Chen,Sam Mayers,Sanjay Krishna Gouda,Zijian Wang,Varun Kumar*

Main category: cs.CR

TL;DR: JAWS-BENCH是一个评估代码能力LLM代理安全性的基准测试，涵盖三个攻击场景：空工作区、单文件和多文件环境。研究发现代码代理在提示攻击下平均接受61%的攻击，其中27%可端到端运行，多文件环境下攻击成功率可达75%。


<details>
  <summary>Details</summary>
Motivation: 随着代码能力LLM代理被集成到软件开发流程中，其安全绕过（越狱）攻击的风险显著增加。现有评估主要关注文本拒绝，而忽略了代理是否实际编译和运行恶意程序的问题。

Method: 构建JAWS-BENCH基准测试，包含三个逐步升级的工作区场景（JAWS-0、JAWS-1、JAWS-M），并采用分层、可执行感知的评估框架来测试合规性、攻击成功率、语法正确性和运行时可执行性。

Result: 在空工作区条件下，代码代理平均接受61%的攻击，58%有害，52%可解析，27%可端到端运行。单文件环境下攻击成功率约71%，多文件环境下约75%，其中32%的攻击代码可立即部署。

Conclusion: 将LLM包装为代理会显著增加漏洞风险，攻击成功率提高1.6倍。研究结果强调了需要执行感知防御、代码上下文安全过滤器以及在整个代理多步推理过程中保持拒绝决策的机制。

Abstract: Code-capable large language model (LLM) agents are increasingly embedded into
software engineering workflows where they can read, write, and execute code,
raising the stakes of safety-bypass ("jailbreak") attacks beyond text-only
settings. Prior evaluations emphasize refusal or harmful-text detection,
leaving open whether agents actually compile and run malicious programs. We
present JAWS-BENCH (Jailbreaks Across WorkSpaces), a benchmark spanning three
escalating workspace regimes that mirror attacker capability: empty (JAWS-0),
single-file (JAWS-1), and multi-file (JAWS-M). We pair this with a
hierarchical, executable-aware Judge Framework that tests (i) compliance, (ii)
attack success, (iii) syntactic correctness, and (iv) runtime executability,
moving beyond refusal to measure deployable harm. Using seven LLMs from five
families as backends, we find that under prompt-only conditions in JAWS-0, code
agents accept 61% of attacks on average; 58% are harmful, 52% parse, and 27%
run end-to-end. Moving to single-file regime in JAWS-1 drives compliance to ~
100% for capable models and yields a mean ASR (Attack Success Rate) ~ 71%; the
multi-file regime (JAWS-M) raises mean ASR to ~ 75%, with 32% instantly
deployable attack code. Across models, wrapping an LLM in an agent
substantially increases vulnerability -- ASR raises by 1.6x -- because initial
refusals are frequently overturned during later planning/tool-use steps.
Category-level analyses identify which attack classes are most vulnerable and
most readily deployable, while others exhibit large execution gaps. These
findings motivate execution-aware defenses, code-contextual safety filters, and
mechanisms that preserve refusal decisions throughout the agent's multi-step
reasoning and tool use.

</details>


### [20] [E-FuzzEdge: Optimizing Embedded Device Security with Scalable In-Place Fuzzing](https://arxiv.org/abs/2510.01393)
*Davide Rusconi,Osama Yousef,Mirco Picca,Flavio Toffalini,Andrea Lanzi*

Main category: cs.CR

TL;DR: E-FuzzEdge是一种针对微控制器模糊测试的新型架构，通过优化执行速度来解决硬件在环模糊测试的低效问题，在缺乏可扩展性的场景下提高测试吞吐量。


<details>
  <summary>Details</summary>
Motivation: 解决硬件在环模糊测试在微控制器环境中的低效率问题，特别是在缺乏可扩展性的场景下提高测试吞吐量。

Method: 开发了E-FuzzEdge架构，通过优化执行速度来改进模糊测试性能，支持与其他嵌入式模糊测试技术集成，特别是那些执行设备测试而非固件仿真的方法。

Result: 与最先进的基准测试相比，系统表现出显著的性能改进。

Conclusion: E-FuzzEdge架构与现有嵌入式模糊测试技术兼容，可以被更广泛的嵌入式模糊测试社区集成到工作流程中，从而提高整体测试效率。

Abstract: In this paper we show E-FuzzEdge, a novel fuzzing architecture targeted
towards improving the throughput of fuzzing campaigns in contexts where
scalability is unavailable. E-FuzzEdge addresses the inefficiencies of
hardware-in-the-loop fuzzing for microcontrollers by optimizing execution
speed. We evaluated our system against state-of-the-art benchmarks,
demonstrating significant performance improvements. A key advantage of
E-FuzzEdgearchitecture is its compatibility with other embedded fuzzing
techniques that perform on device testing instead of firmware emulation. This
means that the broader embedded fuzzing community can integrate E-FuzzEdge into
their workflows to enhance overall testing efficiency.

</details>


### [21] [Securing IoT Devices in Smart Cities: A Review of Proposed Solutions](https://arxiv.org/abs/2510.01445)
*Andrés F. Betancur-López*

Main category: cs.CR

TL;DR: 对智慧城市中物联网设备安全保护方案的综述，重点分析了轻量级密码学、物理不可克隆函数和区块链解决方案的优缺点。


<details>
  <summary>Details</summary>
Motivation: 智慧城市中的物联网设备由于计算资源有限而容易受到攻击，其广泛采用增加了安全漏洞的潜在影响，需要有效的安全保护方案。

Method: 通过分析设备级安全的最新文献进行综述，特别关注轻量级密码学、物理不可克隆函数和区块链解决方案。

Result: 发现当前方法既有优势也有局限性，需要更实用、可扩展和资源高效的机制来确保物联网生态系统中的用户隐私和数据保护。

Conclusion: 智慧城市物联网安全需要开发更实用的轻量级安全机制，以应对设备资源限制和大规模部署的挑战。

Abstract: Privacy and security in Smart Cities remain at constant risk due to the
vulnerabilities introduced by Internet of Things (IoT) devices. The limited
computational resources of these devices make them especially susceptible to
attacks, while their widespread adoption increases the potential impact of
security breaches. This article presents a review of security proposals aimed
at protecting IoT devices in Smart City environments. The review was conducted
by analyzing recent literature on device-level security, with particular
emphasis on lightweight cryptography, physically unclonable functions (PUFs),
and blockchain-based solutions. Findings highlight both the strengths and
limitations of current approaches, as well as the need for more practical,
scalable, and resource-efficient mechanisms to ensure user privacy and data
protection in IoT ecosystems.

</details>


### [22] [POLAR: Automating Cyber Threat Prioritization through LLM-Powered Assessment](https://arxiv.org/abs/2510.01552)
*Luoxi Tang,Yuqiao Meng,Ankita Patra,Weicheng Ma,Muchao Ye,Zhaohan Xi*

Main category: cs.CR

TL;DR: 本文研究了LLMs在网络安全威胁情报(CTI)中的内在脆弱性，揭示了三个核心问题：伪相关性、矛盾知识和受限泛化能力，这些限制了LLMs在实际CTI应用中的有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs被广泛用于支持网络安全分析，但在实际部署中存在显著性能差距。本文旨在研究CTI领域中LLMs的内在脆弱性，这些挑战源于威胁态势本身而非模型架构。

Method: 采用大规模评估方法，结合多个CTI基准测试和真实威胁报告，引入了一种新颖的分类方法，整合了分层、自回归优化和人在回路监督，以可靠分析失败实例。

Result: 通过广泛实验和人工检查，揭示了三个基本脆弱性：伪相关性、矛盾知识和受限泛化，这些限制了LLMs在有效支持CTI方面的能力。

Conclusion: 为设计更稳健的LLM驱动的CTI系统提供了可操作的见解，以促进未来研究。

Abstract: Large Language Models (LLMs) are intensively used to assist security analysts
in counteracting the rapid exploitation of cyber threats, wherein LLMs offer
cyber threat intelligence (CTI) to support vulnerability assessment and
incident response. While recent work has shown that LLMs can support a wide
range of CTI tasks such as threat analysis, vulnerability detection, and
intrusion defense, significant performance gaps persist in practical
deployments. In this paper, we investigate the intrinsic vulnerabilities of
LLMs in CTI, focusing on challenges that arise from the nature of the threat
landscape itself rather than the model architecture. Using large-scale
evaluations across multiple CTI benchmarks and real-world threat reports, we
introduce a novel categorization methodology that integrates stratification,
autoregressive refinement, and human-in-the-loop supervision to reliably
analyze failure instances. Through extensive experiments and human inspections,
we reveal three fundamental vulnerabilities: spurious correlations,
contradictory knowledge, and constrained generalization, that limit LLMs in
effectively supporting CTI. Subsequently, we provide actionable insights for
designing more robust LLM-powered CTI systems to facilitate future research.

</details>


### [23] [Position: Privacy Is Not Just Memorization!](https://arxiv.org/abs/2510.01645)
*Niloofar Mireshghallah,Tianshi Li*

Main category: cs.CR

TL;DR: 本文认为LLM隐私风险远不止训练数据提取，还包括数据收集、推理时上下文泄露、自主代理能力和深度推理攻击等更紧迫的威胁，呼吁研究社区超越当前技术解决方案的狭隘关注。


<details>
  <summary>Details</summary>
Motivation: 当前关于LLM隐私风险的讨论过度关注训练数据的逐字记忆，而更直接和可扩展的隐私威胁却被忽视，需要重新审视LLM系统的整体隐私格局。

Method: 提出了涵盖LLM生命周期（从数据收集到部署）的隐私风险全面分类法，并通过案例研究展示当前隐私框架的不足，同时对2016-2025年间1,322篇AI/ML隐私论文进行纵向分析。

Result: 分析显示记忆问题在技术研究中受到过度关注，而最紧迫的隐私危害存在于其他领域，当前技术方法在这些领域几乎无法提供有效解决方案。

Conclusion: 呼吁研究社区从根本上改变处理LLM隐私的方式，超越当前技术解决方案的狭隘关注，采用跨学科方法来应对这些新兴威胁的社会技术性质。

Abstract: The discourse on privacy risks in Large Language Models (LLMs) has
disproportionately focused on verbatim memorization of training data, while a
constellation of more immediate and scalable privacy threats remain
underexplored. This position paper argues that the privacy landscape of LLM
systems extends far beyond training data extraction, encompassing risks from
data collection practices, inference-time context leakage, autonomous agent
capabilities, and the democratization of surveillance through deep inference
attacks. We present a comprehensive taxonomy of privacy risks across the LLM
lifecycle -- from data collection through deployment -- and demonstrate through
case studies how current privacy frameworks fail to address these multifaceted
threats. Through a longitudinal analysis of 1,322 AI/ML privacy papers
published at leading conferences over the past decade (2016--2025), we reveal
that while memorization receives outsized attention in technical research, the
most pressing privacy harms lie elsewhere, where current technical approaches
offer little traction and viable paths forward remain unclear. We call for a
fundamental shift in how the research community approaches LLM privacy, moving
beyond the narrow focus of current technical solutions and embracing
interdisciplinary approaches that address the sociotechnical nature of these
emerging threats.

</details>


### [24] [Evaluating the Robustness of a Production Malware Detection System to Transferable Adversarial Attacks](https://arxiv.org/abs/2510.01676)
*Milad Nasr,Yanick Fratantonio,Luca Invernizzi,Ange Albertini,Loua Farah,Alex Petit-Bianco,Andreas Terzis,Kurt Thomas,Elie Bursztein,Nicholas Carlini*

Main category: cs.CR

TL;DR: 通过针对Gmail恶意软件检测系统中ML组件Magika的对抗攻击，仅修改13字节即可在90%情况下绕过检测，成功通过Gmail发送恶意文件。开发了防御方法，使攻击成功率降至20%且需要50字节修改。


<details>
  <summary>Details</summary>
Motivation: 研究深度学习模型在生产系统中的弱点如何导致系统级漏洞，特别是针对Gmail恶意软件检测管道的案例研究。

Method: 设计对抗样本欺骗Magika模型，使其错误路由恶意软件到不合适的检测器；开发防御措施降低攻击成功率。

Result: 攻击：修改13字节实现90%绕过率；防御：攻击成功率降至20%且需要50字节修改。防御已部署到Gmail生产环境。

Conclusion: ML组件的对抗攻击可严重威胁生产系统安全，但通过适当防御可显著降低风险，实际部署证明防御有效性。

Abstract: As deep learning models become widely deployed as components within larger
production systems, their individual shortcomings can create system-level
vulnerabilities with real-world impact. This paper studies how adversarial
attacks targeting an ML component can degrade or bypass an entire
production-grade malware detection system, performing a case study analysis of
Gmail's pipeline where file-type identification relies on a ML model.
  The malware detection pipeline in use by Gmail contains a machine learning
model that routes each potential malware sample to a specialized malware
classifier to improve accuracy and performance. This model, called Magika, has
been open sourced. By designing adversarial examples that fool Magika, we can
cause the production malware service to incorrectly route malware to an
unsuitable malware detector thereby increasing our chance of evading detection.
Specifically, by changing just 13 bytes of a malware sample, we can
successfully evade Magika in 90% of cases and thereby allow us to send malware
files over Gmail. We then turn our attention to defenses, and develop an
approach to mitigate the severity of these types of attacks. For our defended
production model, a highly resourced adversary requires 50 bytes to achieve
just a 20% attack success rate. We implement this defense, and, thanks to a
collaboration with Google engineers, it has already been deployed in production
for the Gmail classifier.

</details>


### [25] [Towards Imperceptible Adversarial Defense: A Gradient-Driven Shield against Facial Manipulations](https://arxiv.org/abs/2510.01699)
*Yue Li,Linying Xue,Dongdong Lin,Qiushi Li,Hui Tian,Hongxia Wang*

Main category: cs.CR

TL;DR: 提出GRASP方法，通过梯度投影机制解决对抗性扰动中防御效果与视觉质量之间的平衡问题，在保持高防御成功率的同时实现更好的视觉质量。


<details>
  <summary>Details</summary>
Motivation: 随着生成模型的快速发展，面部图像被操纵的风险增加，现有主动防御方法在不可感知性和防御效果之间存在权衡，强扰动会破坏伪造但降低视觉保真度。

Method: 提出基于梯度投影的对抗主动防御方法GRASP，首次成功整合结构相似性损失和低频损失来增强扰动不可感知性，通过梯度投影机制缓解不同损失间的梯度冲突。

Result: 实验验证GRASP的有效性，PSNR超过40 dB，SSIM达到0.99，对面部属性操纵的防御成功率达到100%，在视觉质量方面显著优于现有方法。

Conclusion: GRASP方法成功解决了主动防御中防御效果与视觉质量之间的平衡问题，通过梯度投影机制实现了更好的优化效果。

Abstract: With the flourishing prosperity of generative models, manipulated facial
images have become increasingly accessible, raising concerns regarding privacy
infringement and societal trust. In response, proactive defense strategies
embed adversarial perturbations into facial images to counter deepfake
manipulation. However, existing methods often face a tradeoff between
imperceptibility and defense effectiveness-strong perturbations may disrupt
forgeries but degrade visual fidelity. Recent studies have attempted to address
this issue by introducing additional visual loss constraints, yet often
overlook the underlying gradient conflicts among losses, ultimately weakening
defense performance. To bridge the gap, we propose a gradient-projection-based
adversarial proactive defense (GRASP) method that effectively counters facial
deepfakes while minimizing perceptual degradation. GRASP is the first approach
to successfully integrate both structural similarity loss and low-frequency
loss to enhance perturbation imperceptibility. By analyzing gradient conflicts
between defense effectiveness loss and visual quality losses, GRASP pioneers
the design of the gradient-projection mechanism to mitigate these conflicts,
enabling balanced optimization that preserves image fidelity without
sacrificing defensive performance. Extensive experiments validate the efficacy
of GRASP, achieving a PSNR exceeding 40 dB, SSIM of 0.99, and a 100% defense
success rate against facial attribute manipulations, significantly
outperforming existing approaches in visual quality.

</details>


### [26] [Constructions of Efficiently Implementable Boolean Functions with Provable Nonlinearity/Resiliency/Algebraic Immunity Trade-Offs](https://arxiv.org/abs/2510.01720)
*Palash Sarkar*

Main category: cs.CR

TL;DR: 提出了多个布尔函数族，在弹性、非线性度和代数免疫性之间实现可证明的权衡，且具有线性复杂度的实现。


<details>
  <summary>Details</summary>
Motivation: 设计在密码学中具有良好安全属性的布尔函数，同时保证高效实现。

Method: 构造多个布尔函数族，通过参数化方法平衡弹性、非线性度和代数免疫性。

Result: 对于任意给定的弹性、非线性度和代数免疫性要求，都能构造出满足条件的n变量函数，且n与各参数呈线性关系，可用O(n)门实现。

Conclusion: 提出的函数族在密码学安全属性和实现效率之间达到了良好的平衡，为实际应用提供了可行的解决方案。

Abstract: We describe several families of efficiently implementable Boolean functions
achieving provable trade-offs between resiliency, nonlinearity, and algebraic
immunity. In concrete terms, the following result holds for each of the
function families that we propose. Given integers $m_0\geq 0$, $x_0\geq 1$, and
$a_0\geq 1$, it is possible to construct an $n$-variable function which has
resiliency at least $m_0$, linear bias (which is an equivalent method of
expressing nonlinearity) at most $2^{-x_0}$ and algebraic immunity at least
$a_0$; further, $n$ is linear in $m_0$, $x_0$ and $a_0$, and the function can
be implemented using $O(n)$ gates.

</details>


### [27] [Secure Multi-Modal Data Fusion in Federated Digital Health Systems via MCP](https://arxiv.org/abs/2510.01780)
*Aueaphum Aueawatthanaphisut*

Main category: cs.CR

TL;DR: 提出基于模型上下文协议(MCP)的多模态联邦学习框架，解决医疗数据异构集成挑战，提升诊断准确性9.8%，降低客户端掉线率54%


<details>
  <summary>Details</summary>
Motivation: 解决异构医疗数据安全互操作集成难题，现有联邦学习框架缺乏标准化机制来协调分布式环境中的多模态数据融合

Method: 利用MCP作为互操作层，统一多模态特征对齐、差分隐私安全聚合和能耗感知调度三大支柱，实现AI代理和工具链的自适应编排

Result: 在基准数据集和临床队列中验证，相比基线联邦学习诊断准确率提升9.8%，客户端掉线率降低54%，实现临床可接受的隐私-效用权衡

Conclusion: MCP支持的多模态融合为下一代公平可信的联邦医疗基础设施提供了可扩展且值得信赖的路径

Abstract: Secure and interoperable integration of heterogeneous medical data remains a
grand challenge in digital health. Current federated learning (FL) frameworks
offer privacy-preserving model training but lack standardized mechanisms to
orchestrate multi-modal data fusion across distributed and resource-constrained
environments. This study introduces a novel framework that leverages the Model
Context Protocol (MCP) as an interoperability layer for secure, cross-agent
communication in multi-modal federated healthcare systems. The proposed
architecture unifies three pillars: (i) multi-modal feature alignment for
clinical imaging, electronic medical records, and wearable IoT data; (ii)
secure aggregation with differential privacy to protect patient-sensitive
updates; and (iii) energy-aware scheduling to mitigate dropouts in mobile
clients. By employing MCP as a schema-driven interface, the framework enables
adaptive orchestration of AI agents and toolchains while ensuring compliance
with privacy regulations. Experimental evaluation on benchmark datasets and
pilot clinical cohorts demonstrates up to 9.8\% improvement in diagnostic
accuracy compared with baseline FL, a 54\% reduction in client dropout rates,
and clinically acceptable privacy--utility trade-offs. These results highlight
MCP-enabled multi-modal fusion as a scalable and trustworthy pathway toward
equitable, next-generation federated health infrastructures.

</details>


### [28] [ZK-WAGON: Imperceptible Watermark for Image Generation Models using ZK-SNARKs](https://arxiv.org/abs/2510.01967)
*Aadarsh Anantha Ramakrishnan,Shubham Agarwal,Selvanayagam S,Kunwar Singh*

Main category: cs.CR

TL;DR: ZK-WAGON是首个使用ZK-SNARKs为图像生成模型添加水印的系统，通过选择性层ZK电路创建和LSB隐写术实现安全、可验证的来源证明，不暴露模型内部信息。


<details>
  <summary>Details</summary>
Motivation: 随着图像生成模型能力增强，合成媒体的真实性、所有权和滥用问题日益严重。传统水印方法会降低图像质量、易被移除或需要访问模型内部信息，不适合安全可扩展部署。

Method: 提出选择性层ZK电路创建(SL-ZKCC)方法，将图像生成模型的关键层转换为电路，显著减少证明生成时间。使用ZK-SNARKs生成可验证来源证明，并通过LSB隐写术将证明嵌入生成图像中。

Result: 在GAN和Diffusion模型上验证了该系统，提供了安全、模型无关的可信AI图像生成流程。

Conclusion: ZK-WAGON为图像生成模型提供了安全、可扩展的水印解决方案，能够在不暴露敏感信息的情况下验证图像来源。

Abstract: As image generation models grow increasingly powerful and accessible,
concerns around authenticity, ownership, and misuse of synthetic media have
become critical. The ability to generate lifelike images indistinguishable from
real ones introduces risks such as misinformation, deepfakes, and intellectual
property violations. Traditional watermarking methods either degrade image
quality, are easily removed, or require access to confidential model internals
- making them unsuitable for secure and scalable deployment. We are the first
to introduce ZK-WAGON, a novel system for watermarking image generation models
using the Zero-Knowledge Succinct Non Interactive Argument of Knowledge
(ZK-SNARKs). Our approach enables verifiable proof of origin without exposing
model weights, generation prompts, or any sensitive internal information. We
propose Selective Layer ZK-Circuit Creation (SL-ZKCC), a method to selectively
convert key layers of an image generation model into a circuit, reducing proof
generation time significantly. Generated ZK-SNARK proofs are imperceptibly
embedded into a generated image via Least Significant Bit (LSB) steganography.
We demonstrate this system on both GAN and Diffusion models, providing a
secure, model-agnostic pipeline for trustworthy AI image generation.

</details>


### [29] [Mirage Fools the Ear, Mute Hides the Truth: Precise Targeted Adversarial Attacks on Polyphonic Sound Event Detection Systems](https://arxiv.org/abs/2510.02158)
*Junjie Su,Weifei Jin,Yuxin Cao,Derui Wang,Kai Ye,Jie Hao*

Main category: cs.CR

TL;DR: 提出了M2A框架，针对多音源声音事件检测系统进行精确的对抗攻击，通过保护损失确保非目标区域输出不变，并引入编辑精度指标平衡攻击效果与精确度


<details>
  <summary>Details</summary>
Motivation: 声音事件检测系统在安全关键应用中广泛部署，但其对抗攻击鲁棒性研究不足。现有攻击方法要么因SED的强上下文依赖而效果不佳，要么因仅关注目标区域而影响非目标区域，缺乏精确性

Method: 提出Mirage和Mute攻击框架，在优化过程中对非目标输出施加特定约束（保护损失），确保非目标区域的模型输出不被改变，从而实现精确攻击

Result: 在两个最先进的SED模型上，M2A分别达到94.56%和99.11%的编辑精度，证明该框架在保持有效性的同时显著提升了攻击精确度

Conclusion: M2A框架能够对多音源SED系统进行精确有效的对抗攻击，通过保护损失机制确保攻击只影响目标区域，同时引入的编辑精度指标为评估此类攻击提供了新标准

Abstract: Sound Event Detection (SED) systems are increasingly deployed in
safety-critical applications such as industrial monitoring and audio
surveillance. However, their robustness against adversarial attacks has not
been well explored. Existing audio adversarial attacks targeting SED systems,
which incorporate both detection and localization capabilities, often lack
effectiveness due to SED's strong contextual dependencies or lack precision by
focusing solely on misclassifying the target region as the target event,
inadvertently affecting non-target regions. To address these challenges, we
propose the Mirage and Mute Attack (M2A) framework, which is designed for
targeted adversarial attacks on polyphonic SED systems. In our optimization
process, we impose specific constraints on the non-target output, which we
refer to as preservation loss, ensuring that our attack does not alter the
model outputs for non-target region, thus achieving precise attacks.
Furthermore, we introduce a novel evaluation metric Editing Precison (EP) that
balances effectiveness and precision, enabling our method to simultaneously
enhance both. Comprehensive experiments show that M2A achieves 94.56% and
99.11% EP on two state-of-the-art SED models, demonstrating that the framework
is sufficiently effective while significantly enhancing attack precision.

</details>


### [30] [NoMod: A Non-modular Attack on Module Learning With Errors](https://arxiv.org/abs/2510.02162)
*Cristian Bassotto,Ermes Franch,Marina Krček,Stjepan Picek*

Main category: cs.CR

TL;DR: 提出了NoMod ML-Attack方法，通过将模运算包装视为统计噪声，将密钥恢复转化为鲁棒线性估计问题，成功攻击了基于Module-LWE的后量子密码方案。


<details>
  <summary>Details</summary>
Motivation: 量子计算威胁经典公钥密码学，NIST采用基于Module-LWE问题的后量子方案，需要开发有效的密码分析方法。

Method: 结合优化的格预处理（包括减向量保存和代数放大）与通过Tukey双权损失训练的鲁棒估计器，将模运算包装处理为统计噪声。

Result: 成功恢复维度n=350的二进制密钥、n=256的稀疏二项式密钥，以及在CRYSTALS-Kyber参数(n,k)=(128,3)和(256,2)下恢复稀疏密钥。

Conclusion: NoMod方法有效规避了模运算建模的挑战，为后量子密码方案的安全性分析提供了新工具。

Abstract: The advent of quantum computing threatens classical public-key cryptography,
motivating NIST's adoption of post-quantum schemes such as those based on the
Module Learning With Errors (Module-LWE) problem. We present NoMod ML-Attack, a
hybrid white-box cryptanalytic method that circumvents the challenge of
modeling modular reduction by treating wrap-arounds as statistical corruption
and casting secret recovery as robust linear estimation. Our approach combines
optimized lattice preprocessing--including reduced-vector saving and algebraic
amplification--with robust estimators trained via Tukey's Biweight loss.
Experiments show NoMod achieves full recovery of binary secrets for dimension
$n = 350$, recovery of sparse binomial secrets for $n = 256$, and successful
recovery of sparse secrets in CRYSTALS-Kyber settings with parameters $(n, k) =
(128, 3)$ and $(256, 2)$. We release our implementation in an anonymous
repository https://anonymous.4open.science/r/NoMod-3BD4.

</details>


### [31] [Testing Stability and Robustness in Three Cryptographic Chaotic Systems](https://arxiv.org/abs/2510.02184)
*N. A. Anagnostopoulos,K. Konstantinidis,A. N. Miliou,S. G. Stavrinides*

Main category: cs.CR

TL;DR: 本文测试了三种不同且知名的密码混沌系统的稳定性和鲁棒性，并比较了它们在存在噪声情况下的安全性能。


<details>
  <summary>Details</summary>
Motivation: 在实际应用中，确保驱动-响应系统在所有方面都相同的情况下能够始终保持同步至关重要，即使在存在噪声的情况下也是如此。

Method: 测试了三种不同且知名的密码混沌系统的稳定性和鲁棒性。

Result: 比较了三种混沌系统在存在噪声情况下的同步性能和安全性能。

Conclusion: 通过对比分析，评估了不同混沌系统在噪声环境下的安全性和同步稳定性表现。

Abstract: In practical applications, it is crucial that the drive-response systems,
although identical in all respects, are synchronized at all times, even if
there is noise present. In this work, we test the stability and robustness of
three distinct and well-known cryptographic chaotic systems, and compare the
results in relation to the desired security.

</details>


### [32] [Authentication Security of PRF GNSS Ranging](https://arxiv.org/abs/2510.02196)
*Jason Anderson*

Main category: cs.CR

TL;DR: 本文分析了基于伪随机函数(PRF)的GNSS测距认证安全性，推导了在多种欺骗模型下的安全边界，并应用于伽利略系统的信号认证服务。


<details>
  <summary>Details</summary>
Motivation: 当GNSS测距码使用只有广播方知道的秘密生成时，欺骗者无法在广播前预测测距码，因此PRF测距可用于建立对GNSS伪距和PNT解决方案的信任。

Method: 应用分析方法推导PRF GNSS测距在多种欺骗模型下的认证安全性，包括安全性码估计和重放(SCER)欺骗器模型。

Result: 计算得出在非SCER模型下，最多需要400ms的伽利略E6-C数据来确保128位认证安全性；对于SCER对手，预测了破解认证安全性所需的接收无线电设备。

Conclusion: 这项工作可用于设计PRF GNSS测距协议，通过计算漏检概率来满足有用的认证安全要求。

Abstract: This work derives the authentication security of pseudorandom function (PRF)
GNSS ranging under multiple GNSS spoofing models, including the Security Code
Estimation and Replay (SCER) spoofer. When GNSS ranging codes derive from a PRF
utilizing a secret known only to the broadcaster, the spoofer cannot predict
the ranging code before broadcast. Therefore, PRF ranging can be used to
establish trust in the GNSS pseudoranges and the resulting receiver position,
navigation, and timing (PNT) solution. I apply the methods herein to Galileo's
Signal Authentication Service (SAS) utilizing the encrypted Galileo E6-C signal
to compute that, at most, 400 ms of Galileo E6-C data to assert 128-bit
authentication security under non-SCER models. For the SCER adversary, I
predict the adversary's needed receiving radio equipment to break
authentication security. One can use this work to design a PRF GNSS ranging
protocol to meet useful authentication security requirements by computing the
probability of missed detection.

</details>


### [33] [An efficient quantum algorithm for computing $S$-units and its applications](https://arxiv.org/abs/2510.02280)
*Jean-Francois Biasse,Fang Song*

Main category: cs.CR

TL;DR: 本文详细证明了Biasse和Song提出的量子多项式时间算法，用于计算数域的S-单位群，该算法可直接应用于计算类群、S-类群、相对类群、单位群、射线类群，解决主理想问题、某些范数方程，以及分解理想类群中的理想类。


<details>
  <summary>Details</summary>
Motivation: 提供Biasse和Song量子多项式时间算法的详细证明，该算法能高效解决数论中的多个重要计算问题，包括类群计算和主理想问题等。

Method: 基于Biasse和Song的量子多项式时间算法，通过计算数域的S-单位群，推导出解决多个数论计算问题的方法。结合Cramer等人的研究成果，进一步应用于寻找主理想的短生成元和分圆域理想格中的"适度短向量"。

Result: 证明了量子多项式时间算法可有效计算S-单位群，并由此直接获得计算类群、解决主理想问题等多种数论问题的多项式时间方法。

Conclusion: 该量子算法为解决数论中的多个经典计算问题提供了高效解决方案，特别是在结合其他研究成果后，能够进一步应用于密码学相关的短向量寻找问题。

Abstract: In this paper, we provide details on the proofs of the quantum polynomial
time algorithm of Biasse and Song (SODA 16) for computing the $S$-unit group of
a number field. This algorithm directly implies polynomial time methods to
calculate class groups, S-class groups, relative class group and the unit
group, ray class groups, solve the principal ideal problem, solve certain norm
equations, and decompose ideal classes in the ideal class group. Additionally,
combined with a result of Cramer, Ducas, Peikert and Regev (Eurocrypt 2016),
the resolution of the principal ideal problem allows one to find short
generators of a principal ideal. Likewise, methods due to Cramer, Ducas and
Wesolowski (Eurocrypt 2017) use the resolution of the principal ideal problem
and the decomposition of ideal classes to find so-called ``mildly short
vectors'' in ideal lattices of cyclotomic fields.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [34] [OR-Toolformer: Modeling and Solving Operations Research Problems with Tool Augmented Large Language Models](https://arxiv.org/abs/2510.01253)
*Jianzhang Zhang,Jialong Zhou,Chuang Liu*

Main category: cs.AI

TL;DR: OR-Toolformer通过微调Llama-3.1-8B-Instruct模型，结合半自动数据合成流程和外部求解器，在运筹学问题上实现了高精度求解，并在未见问题上表现出良好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在运筹学任务中依赖闭源API带来的隐私问题，以及从头训练开源模型的高计算成本问题。

Method: 使用半自动数据合成流程生成多样化的运筹学问题-答案对，通过微调Llama-3.1-8B-Instruct模型，并增强模型调用外部求解器API的能力。

Result: 在四个标准基准测试中的三个上，OR-Toolformer达到最高80.1%的执行准确率，比同等规模基线模型提升超过4.3%。在两种未见运筹学问题的零样本评估中，平均准确率达到54%，比最强基线提升21个百分点。

Conclusion: 工具增强的微调方法能够有效提升大语言模型在运筹学问题建模和求解中的准确性和泛化能力。

Abstract: Large language models (LLMs) demonstrate strong mathematical reasoning, but
reliance on closed-source APIs for OR tasks raises privacy concerns, and
training open-source models from scratch incurs high compute costs. We
introduce OR-Toolformer, which fine-tunes Llama-3.1-8B-Instruct with a
semi-automatic data synthesis pipeline that generates diverse OR problem-answer
pairs and augments the model with external solvers to produce API calls. On
three of four standard benchmarks, OR-Toolformer achieves up to 80.1% execution
accuracy, exceeding size-matched baselines by over 4.3%. In zero-shot
evaluation on two unseen OR problem types, it attains 54% average accuracy, a
21 percentage-point improvement over the strongest baseline. These findings
validate the efficacy of tool-augmented fine-tuning LLMs for accurate and
generalizable OR problem modeling and solving.

</details>


### [35] [Modeling Others' Minds as Code](https://arxiv.org/abs/2510.01272)
*Kunal Jha,Aydan Yuenan Huang,Eric Ye,Natasha Jaques,Max Kleiman-Weiner*

Main category: cs.AI

TL;DR: 提出ROTE算法，将人类行为建模为行为程序而非信念驱动的策略，结合LLM生成假设空间和概率推理处理不确定性，在稀疏观测下预测人类行为表现优异


<details>
  <summary>Details</summary>
Motivation: 现有人类行为建模方法要么对理性做出不切实际的假设，要么计算量过大难以快速适应，需要更高效准确的方法来支持稳健安全的人机协作

Method: 将日常社交互动建模为行为程序，使用LLM合成行为程序的假设空间，结合概率推理处理不确定性，在网格世界任务和家庭模拟器中测试

Result: ROTE在稀疏观测下预测人类和AI行为，比行为克隆和LLM方法在样本内准确率和样本外泛化能力上提升高达50%

Conclusion: 将动作理解视为程序合成问题，为AI系统在现实世界中高效有效预测人类行为开辟了新路径

Abstract: Accurate prediction of human behavior is essential for robust and safe
human-AI collaboration. However, existing approaches for modeling people are
often data-hungry and brittle because they either make unrealistic assumptions
about rationality or are too computationally demanding to adapt rapidly. Our
key insight is that many everyday social interactions may follow predictable
patterns; efficient "scripts" that minimize cognitive load for actors and
observers, e.g., "wait for the green light, then go." We propose modeling these
routines as behavioral programs instantiated in computer code rather than
policies conditioned on beliefs and desires. We introduce ROTE, a novel
algorithm that leverages both large language models (LLMs) for synthesizing a
hypothesis space of behavioral programs, and probabilistic inference for
reasoning about uncertainty over that space. We test ROTE in a suite of
gridworld tasks and a large-scale embodied household simulator. ROTE predicts
human and AI behaviors from sparse observations, outperforming competitive
baselines -- including behavior cloning and LLM-based methods -- by as much as
50% in terms of in-sample accuracy and out-of-sample generalization. By
treating action understanding as a program synthesis problem, ROTE opens a path
for AI systems to efficiently and effectively predict human behavior in the
real-world.

</details>


### [36] [Cyber Academia-Chemical Engineering (CA-ChemE): A Living Digital Town for Self-Directed Research Evolution and Emergent Scientific Discovery](https://arxiv.org/abs/2510.01293)
*Zekun Jiang,Chunming Xu,Tianhang Zhou*

Main category: cs.AI

TL;DR: 提出了CA-ChemE系统，一个通过多智能体协作实现自主研究演进和新兴科学发现的数字化学术城镇，解决了AI在化学工程中跨学科协作和探索未知问题的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有AI系统在化学工程领域的跨学科协作和探索未知问题方面存在局限，需要开发能够自主演进和发现新知识的智能系统。

Method: 构建CA-ChemE系统，集成领域知识库、知识增强技术和协作智能体，创建能够进行深度专业推理和高效跨学科协作的智能生态系统。

Result: 知识库增强机制使7个专家智能体的对话质量评分平均提高10-15%，协作智能体的干预使远领域专家对的协作效率提升8.5%，而近领域对仅提升0.8%，揭示了"知识库差距导致的协作效率递减"效应。

Conclusion: 精心设计的多智能体架构为化学工程领域的自主科学发现提供了可行路径。

Abstract: The rapid advancement of artificial intelligence (AI) has demonstrated
substantial potential in chemical engineering, yet existing AI systems remain
limited in interdisciplinary collaboration and exploration of uncharted
problems. To address these issues, we present the Cyber Academia-Chemical
Engineering (CA-ChemE) system, a living digital town that enables self-directed
research evolution and emergent scientific discovery through multi-agent
collaboration. By integrating domain-specific knowledge bases, knowledge
enhancement technologies, and collaboration agents, the system successfully
constructs an intelligent ecosystem capable of deep professional reasoning and
efficient interdisciplinary collaboration. Our findings demonstrate that
knowledge base-enabled enhancement mechanisms improved dialogue quality scores
by 10-15% on average across all seven expert agents, fundamentally ensuring
technical judgments are grounded in verifiable scientific evidence. However, we
observed a critical bottleneck in cross-domain collaboration efficiency,
prompting the introduction of a Collaboration Agent (CA) equipped with ontology
engineering capabilities. CA's intervention achieved 8.5% improvements for
distant-domain expert pairs compared to only 0.8% for domain-proximate pairs -
a 10.6-fold difference - unveiling the "diminished collaborative efficiency
caused by knowledge-base gaps" effect. This study demonstrates how carefully
designed multi-agent architectures can provide a viable pathway toward
autonomous scientific discovery in chemical engineering.

</details>


### [37] [The Social Laboratory: A Psychometric Framework for Multi-Agent LLM Evaluation](https://arxiv.org/abs/2510.01295)
*Zarreen Reza*

Main category: cs.AI

TL;DR: 提出了一个基于多智能体辩论的新型评估框架，用于量化LLM智能体在交互环境中的社会认知行为，发现智能体具有强烈的共识寻求倾向，且角色设定和主持人能显著影响辩论结果。


<details>
  <summary>Details</summary>
Motivation: 传统评估基准无法捕捉LLM智能体在交互环境中涌现的社会和认知动态，需要新的评估方法来理解自主智能体的社会行为。

Method: 使用多智能体辩论作为受控"社会实验室"，让具有不同角色和激励的LLM智能体在LLM主持人监督下就各种挑战性话题进行辩论，并采用新的心理测量和语义指标进行分析。

Result: 发现智能体具有强大的共识寻求倾向（语义一致性μ>0.88），角色设定能诱导稳定的心理测量特征，主持人的角色能显著改变辩论结果。

Conclusion: 这项工作为面向智能体环境的新型动态心理测量评估协议提供了蓝图，为理解和塑造下一代AI智能体的社会行为提供了关键方法。

Abstract: As Large Language Models (LLMs) transition from static tools to autonomous
agents, traditional evaluation benchmarks that measure performance on
downstream tasks are becoming insufficient. These methods fail to capture the
emergent social and cognitive dynamics that arise when agents communicate,
persuade, and collaborate in interactive environments. To address this gap, we
introduce a novel evaluation framework that uses multi-agent debate as a
controlled "social laboratory" to discover and quantify these behaviors. In our
framework, LLM-based agents, instantiated with distinct personas and
incentives, deliberate on a wide range of challenging topics under the
supervision of an LLM moderator. Our analysis, enabled by a new suite of
psychometric and semantic metrics, reveals several key findings. Across
hundreds of debates, we uncover a powerful and robust emergent tendency for
agents to seek consensus, consistently reaching high semantic agreement ({\mu}
> 0.88) even without explicit instruction and across sensitive topics. We show
that assigned personas induce stable, measurable psychometric profiles,
particularly in cognitive effort, and that the moderators persona can
significantly alter debate outcomes by structuring the environment, a key
finding for external AI alignment. This work provides a blueprint for a new
class of dynamic, psychometrically grounded evaluation protocols designed for
the agentic setting, offering a crucial methodology for understanding and
shaping the social behaviors of the next generation of AI agents. We have
released the code and results at
https://github.com/znreza/multi-agent-LLM-eval-for-debate.

</details>


### [38] [Agentic Jigsaw Interaction Learning for Enhancing Visual Perception and Reasoning in Vision-Language Models](https://arxiv.org/abs/2510.01304)
*Yu Zeng,Wenxuan Huang,Shiting Huang,Xikun Bao,Yukun Qi,Yiming Zhao,Qiuchen Wang,Lin Chen,Zehui Chen,Huaian Chen,Wanli Ouyang,Feng Zhao*

Main category: cs.AI

TL;DR: AGILE通过将拼图任务转化为交互式学习过程，显著提升了视觉语言模型的感知和推理能力，在2×2拼图任务中准确率从9.5%提升至82.8%，并在9个通用视觉任务上平均提升3.1%。


<details>
  <summary>Details</summary>
Motivation: 当前大型视觉语言模型在多模态理解和推理方面虽有进步，但在基本感知和推理能力上仍存在局限，特别是在简单拼图任务上表现接近随机。高质量视觉语言数据的稀缺性和有限可扩展性限制了模型能力的提升。

Method: AGILE将拼图解决设计为交互过程，模型基于当前状态生成可执行代码执行动作，环境提供细粒度视觉反馈。通过观察与交互的迭代循环，模型通过探索和反馈逐步提升感知和推理能力。

Result: AGILE在复杂度不同的拼图任务上显著提升性能（2×2设置下准确率从9.5%提升至82.8%），并在9个通用视觉任务上表现出强泛化能力，平均提升3.1%。

Conclusion: AGILE为推进多模态模型的推理和泛化能力开辟了新途径，并为多模态强化学习数据稀缺问题提供了高效、可扩展的解决方案。

Abstract: Although current large Vision-Language Models (VLMs) have advanced in
multimodal understanding and reasoning, their fundamental perceptual and
reasoning abilities remain limited. Specifically, even on simple jigsaw tasks,
existing VLMs perform near randomly, revealing deficiencies in core perception
and reasoning capabilities. While high-quality vision-language data can enhance
these capabilities, its scarcity and limited scalability impose significant
constraints. To address this, we propose AGILE, an Agentic jiGsaw Interaction
Learning for Enhancing visual perception and reasoning in VLMs. AGILE
formulates jigsaw solving as an interactive process, enabling the model to
progressively engage with the environment. At each step, the model generates
executable code to perform an action based on the current state, while the
environment provides fine-grained visual feedback to guide task completion.
Through this iterative cycle of observation and interaction, the model
incrementally improves its perceptual and reasoning capabilities via
exploration and feedback. Experimental results show that AGILE not only
substantially boosts performance on jigsaw tasks of varying complexity (e.g.,
increasing accuracy from 9.5% to 82.8% under the 2 $\times$ 2 setting) but also
demonstrates strong generalization across 9 general vision tasks, achieving an
average improvement of 3.1%. These results indicate notable enhancements in
both perceptual and reasoning abilities. This work opens a new avenue for
advancing reasoning and generalization in multimodal models and provides an
efficient, scalable solution to the scarcity of multimodal reinforcement
learning data. The code and datasets is available at
https://github.com/yuzeng0-0/AGILE .

</details>


### [39] [Aristotle: IMO-level Automated Theorem Proving](https://arxiv.org/abs/2510.01346)
*Tudor Achim,Alex Best,Kevin Der,Mathïs Fédérico,Sergei Gukov,Daniel Halpern-Leister,Kirsten Henningsgard,Yury Kudryashov,Alexander Meiburg,Martin Michelsen,Riley Patterson,Eric Rodriguez,Laura Scharff,Vikram Shanker,Vladmir Sicca,Hari Sowrirajan,Aidan Swope,Matyas Tamas,Vlad Tenev,Jonathan Thomm,Harold Williams,Lawrence Wu*

Main category: cs.AI

TL;DR: Aristotle系统结合形式验证与非形式推理，在2025年国际数学奥林匹克竞赛中达到金牌级别表现


<details>
  <summary>Details</summary>
Motivation: 开发一个能够结合形式验证和非形式推理的AI系统，以解决复杂的数学问题，特别是国际数学奥林匹克竞赛级别的问题

Method: 集成三个主要组件：Lean证明搜索系统、生成并形式化引理的非形式推理系统、专用几何求解器

Result: 在2025年国际数学奥林匹克竞赛问题上达到金牌等效性能，展示了自动定理证明的最先进性能

Conclusion: Aristotle系统证明了结合形式验证和非形式推理的方法在解决复杂数学问题上的有效性，具有良好的扩展性

Abstract: We introduce Aristotle, an AI system that combines formal verification with
informal reasoning, achieving gold-medal-equivalent performance on the 2025
International Mathematical Olympiad problems. Aristotle integrates three main
components: a Lean proof search system, an informal reasoning system that
generates and formalizes lemmas, and a dedicated geometry solver. Our system
demonstrates state-of-the-art performance with favorable scaling properties for
automated theorem proving.

</details>


### [40] [MEMTRACK: Evaluating Long-Term Memory and State Tracking in Multi-Platform Dynamic Agent Environments](https://arxiv.org/abs/2510.01353)
*Darshan Deshpande,Varun Gangal,Hersh Mehta,Anand Kannappan,Rebecca Qian,Peng Wang*

Main category: cs.AI

TL;DR: MEMTRACK是一个用于评估多平台代理环境中长期记忆和状态跟踪的基准测试，模拟真实企业工作流程，整合Slack、Linear、Git等多个平台的异步事件。


<details>
  <summary>Details</summary>
Motivation: 现有上下文和记忆基准测试主要关注对话场景，但企业动态环境中的记忆评估对于实际应用至关重要。

Method: 通过专家手动设计和基于代理的可扩展合成方法构建数据集，创建生态有效的软件开发场景，包含跨平台依赖、噪声信息和矛盾内容。

Result: 实验显示最先进的LLM和记忆后端在处理长期跨平台记忆、解决矛盾方面面临挑战，表现最佳的GPT-5模型在MEMTRACK上仅达到60%正确率。

Conclusion: 该工作为记忆增强代理的评估研究提供了可扩展框架，超越了现有对话设置的局限，为复杂组织环境中的多代理、多平台记忆基准测试奠定了基础。

Abstract: Recent works on context and memory benchmarking have primarily focused on
conversational instances but the need for evaluating memory in dynamic
enterprise environments is crucial for its effective application. We introduce
MEMTRACK, a benchmark designed to evaluate long-term memory and state tracking
in multi-platform agent environments. MEMTRACK models realistic organizational
workflows by integrating asynchronous events across multiple communication and
productivity platforms such as Slack, Linear and Git. Each benchmark instance
provides a chronologically platform-interleaved timeline, with noisy,
conflicting, cross-referring information as well as potential
codebase/file-system comprehension and exploration. Consequently, our benchmark
tests memory capabilities such as acquistion, selection and conflict
resolution. We curate the MEMTRACK dataset through both manual expert driven
design and scalable agent based synthesis, generating ecologically valid
scenarios grounded in real world software development processes. We introduce
pertinent metrics for Correctness, Efficiency, and Redundancy that capture the
effectiveness of memory mechanisms beyond simple QA performance. Experiments
across SoTA LLMs and memory backends reveal challenges in utilizing memory
across long horizons, handling cross-platform dependencies, and resolving
contradictions. Notably, the best performing GPT-5 model only achieves a 60\%
Correctness score on MEMTRACK. This work provides an extensible framework for
advancing evaluation research for memory-augmented agents, beyond existing
focus on conversational setups, and sets the stage for multi-agent,
multi-platform memory benchmarking in complex organizational settings

</details>


### [41] [Retrieval-Augmented Framework for LLM-Based Clinical Decision Support](https://arxiv.org/abs/2510.01363)
*Leon Garza,Anantaa Kotal,Michael A. Grasso,Emre Umucu*

Main category: cs.AI

TL;DR: 提出基于大语言模型的临床决策支持系统，通过分析电子健康记录生成治疗建议，采用检索增强生成技术整合结构化和非结构化数据，旨在增强而非替代临床决策。


<details>
  <summary>Details</summary>
Motivation: 临床决策复杂性增加和电子健康记录快速增长为数据驱动的医疗带来机遇与挑战，需要开发能够辅助处方决策的智能系统。

Method: 采用检索增强生成(RAG)管道，整合自然语言处理和结构化临床输入，通过检索相似病例并生成上下文相关的治疗建议。

Result: 初步评估显示模型输出具有临床合理性和一致性，在适当约束和严格验证下可为处方工作流提供有价值的决策支持。

Conclusion: 这是将生成式AI整合到真实世界临床决策的初步尝试，强调透明度、安全性以及与既定实践的协调。

Abstract: The increasing complexity of clinical decision-making, alongside the rapid
expansion of electronic health records (EHR), presents both opportunities and
challenges for delivering data-informed care. This paper proposes a clinical
decision support system powered by Large Language Models (LLMs) to assist
prescribing clinicians. The system generates therapeutic suggestions by
analyzing historical EHR data, including patient demographics, presenting
complaints, clinical symptoms, diagnostic information, and treatment histories.
The framework integrates natural language processing with structured clinical
inputs to produce contextually relevant recommendations. Rather than replacing
clinician judgment, it is designed to augment decision-making by retrieving and
synthesizing precedent cases with comparable characteristics, drawing on local
datasets or federated sources where applicable. At its core, the system employs
a retrieval-augmented generation (RAG) pipeline that harmonizes unstructured
narratives and codified data to support LLM-based inference. We outline the
system's technical components, including representation representation
alignment and generation strategies. Preliminary evaluations, conducted with
de-identified and synthetic clinical datasets, examine the clinical
plausibility and consistency of the model's outputs. Early findings suggest
that LLM-based tools may provide valuable decision support in prescribing
workflows when appropriately constrained and rigorously validated. This work
represents an initial step toward integration of generative AI into real-world
clinical decision-making with an emphasis on transparency, safety, and
alignment with established practices.

</details>


### [42] [Is It Thinking or Cheating? Detecting Implicit Reward Hacking by Measuring Reasoning Effort](https://arxiv.org/abs/2510.01367)
*Xinpeng Wang,Nitish Joshi,Barbara Plank,Rico Angell,He He*

Main category: cs.AI

TL;DR: 提出TRACE方法检测隐式奖励攻击，通过截断推理链测量模型通过验证器所需的最小努力，识别利用漏洞而非真正解决问题的模型行为。


<details>
  <summary>Details</summary>
Motivation: 奖励攻击（模型利用奖励函数漏洞获得高分但不解决实际任务）是严重威胁，特别是隐式攻击会绕过现有推理链监控方法。

Method: TRACE方法逐步截断模型的推理链，强制模型在不同长度下回答，测量通过验证器的比率。利用漏洞的模型只需少量推理就能获得高分，表现为准确率-长度曲线下面积较大。

Result: 在数学推理任务中比72B推理链监控器提升65%以上，在编程任务中比32B监控器提升30%以上，并能发现训练中的未知漏洞。

Conclusion: TRACE提供了一种可扩展的无监督监督方法，在当前监控方法失效的情况下有效检测隐式奖励攻击。

Abstract: Reward hacking, where a reasoning model exploits loopholes in a reward
function to achieve high rewards without solving the intended task, poses a
significant threat. This behavior may be explicit, i.e. verbalized in the
model's chain-of-thought (CoT), or implicit, where the CoT appears benign thus
bypasses CoT monitors. To detect implicit reward hacking, we propose TRACE
(Truncated Reasoning AUC Evaluation). Our key observation is that hacking
occurs when exploiting the loophole is easier than solving the actual task.
This means that the model is using less `effort' than required to achieve high
reward. TRACE quantifies effort by measuring how early a model's reasoning
becomes sufficient to pass a verifier. We progressively truncate a model's CoT
at various lengths, force the model to answer, and measure the verifier-passing
rate at each cutoff. A hacking model, which takes a shortcut, will achieve a
high passing rate with only a small fraction of its CoT, yielding a large area
under the accuracy-vs-length curve. TRACE achieves over 65% gains over our
strongest 72B CoT monitor in math reasoning, and over 30% gains over a 32B
monitor in coding. We further show that TRACE can discover unknown loopholes
during training. Overall, TRACE offers a scalable unsupervised approach for
oversight where current monitoring methods prove ineffective.

</details>


### [43] [Fine-tuning with RAG for Improving LLM Learning of New Skills](https://arxiv.org/abs/2510.01375)
*Humaid Ibrahim,Nikolai Rozanov,Marek Rei*

Main category: cs.AI

TL;DR: 提出一种通过蒸馏将推理时检索转化为学习能力的方法，在交互式基准测试中显著提升智能体性能并减少token使用


<details>
  <summary>Details</summary>
Motivation: 大型语言模型智能体在多步任务中经常出现可预测的失败，如尝试不满足前提条件的动作、发出冗余命令或错误处理环境约束。检索增强生成虽然能提供运行时指导，但需要维护外部知识库并增加计算开销

Method: 三步管道：(1)从智能体失败中提取紧凑可重用的提示；(2)使用这些提示在回合开始时通过一次性检索生成改进的教师轨迹；(3)在移除提示字符串的情况下训练学生模型，强制内化而非记忆

Result: 在ALFWorld（家务任务）和WebShop（在线购物）两个交互式基准测试中，蒸馏学生模型始终优于基线智能体，ALFWorld成功率高达91%（基线79%），WebShop分数提升至72（基线61），同时比检索增强教师少用10-60%的token

Conclusion: 该方法在不同模型规模（7B/14B参数）和智能体架构（ReAct/StateAct）上具有通用性，表明检索优势可以通过有针对性的微调有效内化，无需永久性的运行时依赖

Abstract: Large language model (LLM) agents deployed for multi-step tasks frequently
fail in predictable ways: attempting actions with unmet preconditions, issuing
redundant commands, or mishandling environment constraints. While
retrieval-augmented generation (RAG) can improve performance by providing
runtime guidance, it requires maintaining external knowledge databases and adds
computational overhead at every deployment. We propose a simple pipeline that
converts inference-time retrieval into learned competence through distillation.
Our approach: (1) extracts compact, reusable hints from agent failures, (2)
uses these hints to generate improved teacher trajectories via one-shot
retrieval at episode start, and (3) trains student models on these trajectories
with hint strings removed, forcing internalization rather than memorization.
Across two interactive benchmarks, ALFWorld (household tasks) and WebShop
(online shopping), distilled students consistently outperform baseline agents,
achieving up to 91% success on ALFWorld (vs. 79% for baselines) and improving
WebShop scores to 72 (vs. 61 for baselines), while using 10-60% fewer tokens
than retrieval-augmented teachers depending on the environment. The approach
generalizes across model scales (7B/14B parameters) and agent architectures
(ReAct/StateAct), demonstrating that retrieval benefits can be effectively
internalized through targeted fine-tuning without permanent runtime
dependencies.

</details>


### [44] [Automating Data-Driven Modeling and Analysis for Engineering Applications using Large Language Model Agents](https://arxiv.org/abs/2510.01398)
*Yang Liu,Zaid Abulawi,Abhiram Garimidi,Doyeong Lim*

Main category: cs.AI

TL;DR: 提出使用LLM代理自动进行数据驱动建模和分析的创新流程，特别关注回归任务，在临界热通量预测基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现代工程依赖大量实验和模拟数据，传统数据驱动方法需要大量人工干预，难以扩展和泛化到不同应用。

Method: 评估两种LLM代理框架：多代理系统和基于ReAct范式的单代理系统，自动处理数据预处理、神经网络开发、训练、超参数优化和不确定性量化。

Result: LLM代理开发的模型超越了传统CHF查找表，预测精度和UQ与人类专家开发的贝叶斯优化深度神经网络模型相当。

Conclusion: LLM代理在自动化复杂工程建模任务方面具有巨大潜力，能显著减少人工工作量同时达到或超过现有预测性能标准。

Abstract: Modern engineering increasingly relies on vast datasets generated by
experiments and simulations, driving a growing demand for efficient, reliable,
and broadly applicable modeling strategies. There is also heightened interest
in developing data-driven approaches, particularly neural network models, for
effective prediction and analysis of scientific datasets. Traditional
data-driven methods frequently involve extensive manual intervention, limiting
their ability to scale effectively and generalize to diverse applications. In
this study, we propose an innovative pipeline utilizing Large Language Model
(LLM) agents to automate data-driven modeling and analysis, with a particular
emphasis on regression tasks. We evaluate two LLM-agent frameworks: a
multi-agent system featuring specialized collaborative agents, and a
single-agent system based on the Reasoning and Acting (ReAct) paradigm. Both
frameworks autonomously handle data preprocessing, neural network development,
training, hyperparameter optimization, and uncertainty quantification (UQ). We
validate our approach using a critical heat flux (CHF) prediction benchmark,
involving approximately 25,000 experimental data points from the OECD/NEA
benchmark dataset. Results indicate that our LLM-agent-developed model
surpasses traditional CHF lookup tables and delivers predictive accuracy and UQ
on par with state-of-the-art Bayesian optimized deep neural network models
developed by human experts. These outcomes underscore the significant potential
of LLM-based agents to automate complex engineering modeling tasks, greatly
reducing human workload while meeting or exceeding existing standards of
predictive performance.

</details>


### [45] [OntoLogX: Ontology-Guided Knowledge Graph Extraction from Cybersecurity Logs with Large Language Models](https://arxiv.org/abs/2510.01409)
*Luca Cotti,Idilio Drago,Anisa Rula,Devis Bianchini,Federico Cerutti*

Main category: cs.AI

TL;DR: OntoLogX是一个基于LLM的自主AI代理，将原始系统日志转换为基于本体的知识图谱，并通过RAG和迭代校正确保KG质量，最终映射到MITRE ATT&CK战术以提取可操作的网络威胁情报。


<details>
  <summary>Details</summary>
Motivation: 系统日志是宝贵的网络威胁情报来源，但由于缺乏结构化、语义不一致和跨设备碎片化等问题，其效用受到限制。需要能够将噪声异构数据协调为连贯可互操作表示的方法。

Method: 集成轻量级日志本体与检索增强生成(RAG)和迭代校正步骤，确保生成的KG在语法和语义上有效。系统将KG聚合到会话级别，并使用LLM预测MITRE ATT&CK战术。

Result: 在公共基准和真实世界蜜罐数据集上的评估表明，OntoLogX能够在多个KG后端上稳健生成KG，并准确将对抗活动映射到ATT&CK战术。检索和校正提高了精确率和召回率。

Conclusion: 代码导向模型在结构化日志分析中表现有效，基于本体的表示为可操作的CTI提取提供了价值，检索和校正显著提升了KG生成的质量。

Abstract: System logs represent a valuable source of Cyber Threat Intelligence (CTI),
capturing attacker behaviors, exploited vulnerabilities, and traces of
malicious activity. Yet their utility is often limited by lack of structure,
semantic inconsistency, and fragmentation across devices and sessions.
Extracting actionable CTI from logs therefore requires approaches that can
reconcile noisy, heterogeneous data into coherent and interoperable
representations. We introduce OntoLogX, an autonomous Artificial Intelligence
(AI) agent that leverages Large Language Models (LLMs) to transform raw logs
into ontology-grounded Knowledge Graphs (KGs). OntoLogX integrates a
lightweight log ontology with Retrieval Augmented Generation (RAG) and
iterative correction steps, ensuring that generated KGs are syntactically and
semantically valid. Beyond event-level analysis, the system aggregates KGs into
sessions and employs a LLM to predict MITRE ATT&CK tactics, linking low-level
log evidence to higher-level adversarial objectives. We evaluate OntoLogX on
both logs from a public benchmark and a real-world honeypot dataset,
demonstrating robust KG generation across multiple KGs backends and accurate
mapping of adversarial activity to ATT&CK tactics. Results highlight the
benefits of retrieval and correction for precision and recall, the
effectiveness of code-oriented models in structured log analysis, and the value
of ontology-grounded representations for actionable CTI extraction.

</details>


### [46] [A Tale of LLMs and Induced Small Proxies: Scalable Agents for Knowledge Mining](https://arxiv.org/abs/2510.01427)
*Sipeng Zhang,Longfei Yun,Zilong Wang,Jingbo Shang,Letian Peng*

Main category: cs.AI

TL;DR: Falconer是一个结合LLM智能推理与轻量代理模型的协作框架，用于可扩展的知识挖掘，在保持高准确性的同时显著降低推理成本。


<details>
  <summary>Details</summary>
Motivation: 解决大规模知识挖掘中LLM部署成本过高与传统分类器/抽取器泛化能力不足的问题。

Method: 使用LLM作为规划器分解用户指令为可执行管道，并作为标注器生成监督数据训练小型代理模型，统一分类和抽取为两个原子操作。

Result: Falconer在指令跟随准确性上接近最先进LLM，同时减少90%推理成本，加速大规模知识挖掘20倍以上。

Conclusion: Falconer为深度研究提供了高效可扩展的基础框架，平衡了性能与成本。

Abstract: At the core of Deep Research is knowledge mining, the task of extracting
structured information from massive unstructured text in response to user
instructions. Large language models (LLMs) excel at interpreting such
instructions but are prohibitively expensive to deploy at scale, while
traditional pipelines of classifiers and extractors remain efficient yet
brittle and unable to generalize to new tasks. We introduce Falconer, a
collaborative framework that combines the agentic reasoning of LLMs with
lightweight proxy models for scalable knowledge mining. In Falconer, LLMs act
as planners, decomposing user instructions into executable pipelines, and as
annotators, generating supervision to train small proxies. The framework
unifies classification and extraction into two atomic operations, get label and
get span, enabling a single instruction-following model to replace multiple
task-specific components. To evaluate the consistency between proxy models
incubated by Falconer and annotations provided by humans and large models, we
construct new benchmarks covering both planning and end-to-end execution.
Experiments show that Falconer closely matches state-of-the-art LLMs in
instruction-following accuracy while reducing inference cost by up to 90% and
accelerating large-scale knowledge mining by more than 20x, offering an
efficient and scalable foundation for Deep Research.

</details>


### [47] [On the Role of Domain Experts in Creating Effective Tutoring Systems](https://arxiv.org/abs/2510.01432)
*Sarath Sreedharan,Kelsey Sikes,Nathaniel Blanchard,Lisa Mason,Nikhil Krishnaswamy,Jill Zarestky*

Main category: cs.AI

TL;DR: 本文探讨了如何利用领域专家提供的精炼知识来创建有效的智能辅导系统，重点介绍了两种方法：使用可解释AI技术自动生成课程，以及利用专家指定的课程表开发自适应辅导系统。


<details>
  <summary>Details</summary>
Motivation: AI教育社区往往忽视了领域专家提供的精炼知识在创建有效辅导系统中的作用。本文旨在强调这种专业知识的重要性，并展示其在开发新型教育系统中的应用价值。

Method: 1. 使用可解释AI技术结合专家指定的问题解决规则来自动生成课程；2. 利用专家指定的学习课程表来开发自适应辅导系统，提高学习体验和算法效率。

Result: 通过传粉者识别辅导系统的案例研究，证明了这些方法的实用性和有效性，展示了专家知识在创建智能教育系统中的重要作用。

Conclusion: 领域专家的精炼知识在开发智能辅导系统中具有关键作用，通过结合可解释AI技术和专家指定的课程表，可以创建更有效、更自适应的教育系统。

Abstract: The role that highly curated knowledge, provided by domain experts, could
play in creating effective tutoring systems is often overlooked within the AI
for education community. In this paper, we highlight this topic by discussing
two ways such highly curated expert knowledge could help in creating novel
educational systems. First, we will look at how one could use explainable AI
(XAI) techniques to automatically create lessons. Most existing XAI methods are
primarily aimed at debugging AI systems. However, we will discuss how one could
use expert specified rules about solving specific problems along with novel XAI
techniques to automatically generate lessons that could be provided to
learners. Secondly, we will see how an expert specified curriculum for learning
a target concept can help develop adaptive tutoring systems, that can not only
provide a better learning experience, but could also allow us to use more
efficient algorithms to create these systems. Finally, we will highlight the
importance of such methods using a case study of creating a tutoring system for
pollinator identification, where such knowledge could easily be elicited from
experts.

</details>


### [48] [VOGUE: Guiding Exploration with Visual Uncertainty Improves Multimodal Reasoning](https://arxiv.org/abs/2510.01444)
*Rui Liu,Dian Yu,Tong Zheng,Runpeng Dai,Zongxia Li,Wenhao Yu,Zhenwen Liang,Linfeng Song,Haitao Mi,Pratap Tokekar,Dong Yu*

Main category: cs.AI

TL;DR: VOGUE通过将探索从文本输出空间转移到视觉输入空间，利用视觉不确定性指导多模态大语言模型的强化学习，显著提升了推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型在强化学习中面临探索不足的问题，特别是忽视了视觉输入的不确定性，导致策略对视觉变化不够鲁棒。

Method: VOGUE将图像视为随机上下文，通过对称KL散度量化策略对视觉扰动的敏感性，结合不确定性比例奖励、token熵奖励和退火采样调度来平衡探索与利用。

Result: 在Qwen2.5-VL-3B/7B模型上，VOGUE在三个视觉数学基准上平均提升pass@1准确率2.6%，在三个通用领域推理基准上提升3.7%，同时提高了pass@4性能并缓解了探索衰减。

Conclusion: 基于视觉输入固有不确定性的探索策略是提升多模态推理的有效方法。

Abstract: Reinforcement learning with verifiable rewards (RLVR) improves reasoning in
large language models (LLMs) but struggles with exploration, an issue that
still persists for multimodal LLMs (MLLMs). Current methods treat the visual
input as a fixed, deterministic condition, overlooking a critical source of
ambiguity and struggling to build policies robust to plausible visual
variations. We introduce $\textbf{VOGUE (Visual Uncertainty Guided
Exploration)}$, a novel method that shifts exploration from the output (text)
to the input (visual) space. By treating the image as a stochastic context,
VOGUE quantifies the policy's sensitivity to visual perturbations using the
symmetric KL divergence between a "raw" and "noisy" branch, creating a direct
signal for uncertainty-aware exploration. This signal shapes the learning
objective via an uncertainty-proportional bonus, which, combined with a
token-entropy bonus and an annealed sampling schedule, effectively balances
exploration and exploitation. Implemented within GRPO on two model scales
(Qwen2.5-VL-3B/7B), VOGUE boosts pass@1 accuracy by an average of 2.6% on three
visual math benchmarks and 3.7% on three general-domain reasoning benchmarks,
while simultaneously increasing pass@4 performance and mitigating the
exploration decay commonly observed in RL fine-tuning. Our work shows that
grounding exploration in the inherent uncertainty of visual inputs is an
effective strategy for improving multimodal reasoning.

</details>


### [49] [AIReg-Bench: Benchmarking Language Models That Assess AI Regulation Compliance](https://arxiv.org/abs/2510.01474)
*Bill Marino,Rosco Hunter,Zubair Jamali,Marinos Emmanouil Kalpakos,Mudra Kashyap,Isaiah Hinton,Alexa Hanson,Maahum Nazir,Christoph Schnabl,Felix Steffek,Hongkai Wen,Nicholas D. Lane*

Main category: cs.AI

TL;DR: 提出了首个用于评估大语言模型在AI法规合规性检查方面性能的基准数据集AIReg-Bench，基于欧盟AI法案构建，包含120个技术文档样本和专家标注的合规性标签。


<details>
  <summary>Details</summary>
Motivation: 随着政府对AI监管的加强，需要评估LLM在AI法规合规性检查方面的能力，但目前缺乏相应的基准测试工具。

Method: 通过两步流程创建数据集：(1) 使用结构化指令提示LLM生成120个技术文档样本；(2) 法律专家审查并标注每个样本是否违反AI法案的具体条款。

Result: 建立了包含专家标注的合规性标签的数据集，并评估了前沿LLM复现专家标签的能力，为理解LLM在AIR合规评估中的机会和局限性提供了起点。

Conclusion: AIReg-Bench为后续LLM在AI法规合规性评估方面的性能比较建立了基准，数据集和评估代码已开源。

Abstract: As governments move to regulate AI, there is growing interest in using Large
Language Models (LLMs) to assess whether or not an AI system complies with a
given AI Regulation (AIR). However, there is presently no way to benchmark the
performance of LLMs at this task. To fill this void, we introduce AIReg-Bench:
the first benchmark dataset designed to test how well LLMs can assess
compliance with the EU AI Act (AIA). We created this dataset through a two-step
process: (1) by prompting an LLM with carefully structured instructions, we
generated 120 technical documentation excerpts (samples), each depicting a
fictional, albeit plausible, AI system - of the kind an AI provider might
produce to demonstrate their compliance with AIR; (2) legal experts then
reviewed and annotated each sample to indicate whether, and in what way, the AI
system described therein violates specific Articles of the AIA. The resulting
dataset, together with our evaluation of whether frontier LLMs can reproduce
the experts' compliance labels, provides a starting point to understand the
opportunities and limitations of LLM-based AIR compliance assessment tools and
establishes a benchmark against which subsequent LLMs can be compared. The
dataset and evaluation code are available at
https://github.com/camlsys/aireg-bench.

</details>


### [50] [Lateral Tree-of-Thoughts Surpasses ToT by Incorporating Logically-Consistent, Low-Utility Candidates](https://arxiv.org/abs/2510.01500)
*Abhinav Madahar*

Main category: cs.AI

TL;DR: LToT是一种改进的Tree-of-Thoughts搜索控制器，通过将候选解分为主线（高效用）和侧线（逻辑一致但低效用）来缓解广度饱和和深度短视问题，使用侧线竞速算法在保持计算效率的同时提升多样性。


<details>
  <summary>Details</summary>
Motivation: 传统Tree-of-Thoughts搜索在大计算预算下存在两个问题：广度饱和（额外样本产生近似重复）和深度短视（短视效用函数过早剪枝有潜力的分支）。需要一种方法既能利用大计算预算提升可靠性，又能避免这些病理现象。

Method: 提出Lateral Tree-of-Thoughts (LToT)，将前沿分为主线和侧线。主线用于开发高效用候选，侧线保留逻辑一致但低效用的候选。使用Lateral Racing with Short-Circuit (LR--SC)算法：对侧线进行有上限的连续减半竞速，使用宽度感知阈值和重复确认机制，一旦侧线分支效用超过主线阈值立即提升。

Result: 理论证明侧线成本为伪线性Θ(N₀ log_η N₀)，其中N₀为初始侧线宽度，η>1为剔除因子，相比未加限制的主线指数增长有显著改进。实证评估正在进行中。

Conclusion: LToT能够将大测试时计算预算转化为原则性多样性，同时保持提升纪律，缓解饱和和短视问题而不增加计算成本。

Abstract: Modern deployments increasingly allocate large test-time compute (thousands
of tokens or many node expansions) to boost reliability. Under such budgets,
standard Tree-of-Thoughts-style search exhibits two pathologies: breadth
saturation (additional samples mostly produce near-duplicates, so width stops
growing) and depth myopia (noisy short-horizon utilities prune branches whose
payoff appears after a few more steps). We propose Lateral Tree-of-Thoughts
(LToT), a drop-in controller that separates utility from logical consistency
and treats low-utility but consistent candidates as assets rather than waste.
The frontier is split into mainlines (high-utility candidates used for
exploitation) and laterals (consistent, initially low-utility candidates that
receive short, cheap probes before judgment). LToT explores laterals via
Lateral Racing with Short-Circuit (LR--SC): a capped successive-halving race
that spreads tiny probes across a very wide lateral set, uses width-aware
thresholds with repeat-to-confirm, and immediately promotes a branch once its
envelope clears the mainline bar; mainlines are kept intentionally narrow so
surplus compute is invested where width is cheap. We prove a pseudolinear
lateral cost $\Theta(N_0 \log_{\eta} N_0)$ with logarithmically many rungs
(initial lateral width $N_0$; culling factor $\eta>1$), in contrast to the
exponential growth of uncapped mainlines. Empirical evaluations on benchmark
tasks are in preparation and will be added in a future revision. In short, LToT
turns large test-time budgets into principled diversity while preserving
promotion discipline, mitigating saturation and myopia without inflating
compute.

</details>


### [51] [Towards Interpretable and Inference-Optimal COT Reasoning with Sparse Autoencoder-Guided Generation](https://arxiv.org/abs/2510.01528)
*Daniel Zhao,Abhilash Shankarampeta,Lanxiang Hu,Tajana Rosing,Hao Zhang*

Main category: cs.AI

TL;DR: 提出一种结合稀疏自编码器和聚类技术的方法，分析LLM内部token表示并指导数学推理任务的生成过程，通过平衡利用和探索实现更高质量的推理。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型在数学推理任务中的内部表示机制，探索如何通过分析token表示来指导生成过程，避免极端行为，提高推理质量。

Method: 训练稀疏自编码器生成token的稀疏向量表示，应用k-means聚类构建token簇图，定义基于边权重的奖励函数量化推理轨迹的遵循程度，同时测量生成多样性评估探索程度。

Result: 研究发现平衡利用和探索对于数学推理任务的高准确性至关重要，稀疏自编码器可作为可扩展的奖励模型指导生成过程。

Conclusion: 该方法能有效防止极端行为，促进更高质量的推理过程，在LLM的数学推理任务中实现了利用和探索的良好平衡。

Abstract: We propose a novel method that leverages sparse autoencoders (SAEs) and
clustering techniques to analyze the internal token representations of large
language models (LLMs) and guide generations in mathematical reasoning tasks.
Our approach first trains an SAE to generate sparse vector representations for
training tokens, then applies k-means clustering to construct a graph where
vertices represent token clusters and weighted edges capture sequential token
transitions. Using this graph, we define an edge-weight based reward function
to quantify adherence to established reasoning traces, thereby identifying
exploitative reasoning trajectories. Additionally, we measure generation
diversity from clustering to assess the extent of exploration. Our findings
indicate that balancing both exploitation and exploration is crucial for
achieving high accuracy in mathematical reasoning tasks. During generation, the
SAE can serve as a scalable reward model to guide generations, ensuring a
balanced trade-off between exploitation and exploration. This prevents extreme
behaviors in either direction, ultimately fostering a higher-quality reasoning
process in LLMs.

</details>


### [52] [LOGicalThought: Logic-Based Ontological Grounding of LLMs for High-Assurance Reasoning](https://arxiv.org/abs/2510.01530)
*Navapat Nananukul,Yue Zhang,Ryan Lee,Eric Boxer,Jonathan May,Vibhav Giridhar Gogate,Jay Pujara,Mayank Kejriwal*

Main category: cs.AI

TL;DR: 提出了一种名为LOGicalThought (LogT)的神经符号推理架构，结合高级逻辑语言和推理器与LLM，将长文本指南推理转化为紧凑的接地评估，在四个多领域基准测试中性能提升11.84%。


<details>
  <summary>Details</summary>
Motivation: 在关键领域如法律和医学中，需要准确、可验证且明确基于证据的推理。传统LLM在处理包含可废止逻辑、否定和蕴含的复杂推理任务时存在局限。

Method: 使用神经符号架构，结合高级逻辑语言和推理器与LLM，构建双重符号图上下文和基于逻辑的上下文表示。

Result: 在四个多领域基准测试中，相比四个基线模型，LogT整体性能提升11.84%，在否定推理上提升10.2%，蕴含推理上提升13.2%，可废止推理上提升5.5%。

Conclusion: LogT架构能够有效解决高保证文本指南中的复杂逻辑推理挑战，显著提升LLM在否定、蕴含和可废止推理方面的性能。

Abstract: High-assurance reasoning, particularly in critical domains such as law and
medicine, requires conclusions that are accurate, verifiable, and explicitly
grounded in evidence. This reasoning relies on premises codified from rules,
statutes, and contracts, inherently involving defeasible or non-monotonic logic
due to numerous exceptions, where the introduction of a single fact can
invalidate general rules, posing significant challenges. While large language
models (LLMs) excel at processing natural language, their capabilities in
standard inference tasks do not translate to the rigorous reasoning required
over high-assurance text guidelines. Core reasoning challenges within such
texts often manifest specific logical structures involving negation,
implication, and, most critically, defeasible rules and exceptions. In this
paper, we propose a novel neurosymbolically-grounded architecture called
LOGicalThought (LogT) that uses an advanced logical language and reasoner in
conjunction with an LLM to construct a dual symbolic graph context and
logic-based context. These two context representations transform the problem
from inference over long-form guidelines into a compact grounded evaluation.
Evaluated on four multi-domain benchmarks against four baselines, LogT improves
overall performance by 11.84% across all LLMs. Performance improves
significantly across all three modes of reasoning: by up to +10.2% on negation,
+13.2% on implication, and +5.5% on defeasible reasoning compared to the
strongest baseline.

</details>


### [53] [Just Do It!? Computer-Use Agents Exhibit Blind Goal-Directedness](https://arxiv.org/abs/2510.01670)
*Erfan Shayegani,Keegan Hines,Yue Dong,Nael Abu-Ghazaleh,Roman Lutz,Spencer Whitehead,Vidhisha Balachandran,Besmira Nushi,Vibhav Vineet*

Main category: cs.AI

TL;DR: 论文揭示了计算机使用代理(CUAs)存在盲目标导向(BGD)偏差，即不顾可行性、安全性和上下文盲目追求目标，开发了BLIND-ACT基准测试，发现前沿模型平均BGD率高达80.8%，提示干预效果有限。


<details>
  <summary>Details</summary>
Motivation: 计算机使用代理(CUAs)在GUI上执行操作实现用户目标，但存在盲目追求目标而忽视可行性、安全性和上下文的系统性偏差，需要识别和量化这种风险。

Method: 开发BLIND-ACT基准测试，包含90个任务捕捉BGD的三种模式：缺乏上下文推理、模糊假设决策、矛盾或不可行目标。基于OSWorld构建真实环境，使用LLM评估代理行为。

Result: 评估9个前沿模型，平均BGD率达到80.8%。提示干预可降低BGD水平但风险依然显著。观察到三种失败模式：执行优先偏见、思维-行动脱节、请求优先。

Conclusion: 识别BGD并引入BLIND-ACT为研究和缓解这种基本风险奠定了基础，需要更强的训练或推理时干预来确保CUA的安全部署。

Abstract: Computer-Use Agents (CUAs) are an increasingly deployed class of agents that
take actions on GUIs to accomplish user goals. In this paper, we show that CUAs
consistently exhibit Blind Goal-Directedness (BGD): a bias to pursue goals
regardless of feasibility, safety, reliability, or context. We characterize
three prevalent patterns of BGD: (i) lack of contextual reasoning, (ii)
assumptions and decisions under ambiguity, and (iii) contradictory or
infeasible goals. We develop BLIND-ACT, a benchmark of 90 tasks capturing these
three patterns. Built on OSWorld, BLIND-ACT provides realistic environments and
employs LLM-based judges to evaluate agent behavior, achieving 93.75% agreement
with human annotations. We use BLIND-ACT to evaluate nine frontier models,
including Claude Sonnet and Opus 4, Computer-Use-Preview, and GPT-5, observing
high average BGD rates (80.8%) across them. We show that BGD exposes subtle
risks that arise even when inputs are not directly harmful. While
prompting-based interventions lower BGD levels, substantial risk persists,
highlighting the need for stronger training- or inference-time interventions.
Qualitative analysis reveals observed failure modes: execution-first bias
(focusing on how to act over whether to act), thought-action disconnect
(execution diverging from reasoning), and request-primacy (justifying actions
due to user request). Identifying BGD and introducing BLIND-ACT establishes a
foundation for future research on studying and mitigating this fundamental risk
and ensuring safe CUA deployment.

</details>


### [54] [Information Seeking for Robust Decision Making under Partial Observability](https://arxiv.org/abs/2510.01531)
*Djengo Cyun-Jyun Fang,Tsung-Wei Ke*

Main category: cs.AI

TL;DR: InfoSeeker是一个LLM决策框架，通过集成任务导向规划和信息寻求来在部分可观测环境中做出最优决策，相比现有方法获得74%的绝对性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有LLM规划代理虽然解决了观测不确定性，但往往忽略了其内部动态与实际环境之间的差异，需要主动信息寻求来对齐内部动态。

Method: InfoSeeker提示LLM通过规划行动来主动收集信息，验证其理解、检测环境变化或测试假设，然后生成或修订任务导向计划。

Result: 在部分可观测环境的新基准测试中，InfoSeeker相比现有方法获得74%的绝对性能提升，且在机器人操作和网络导航等基准测试中优于基线方法。

Conclusion: 紧密集成规划与信息寻求对于在部分可观测环境中实现鲁棒行为至关重要，InfoSeeker框架展示了这种集成的有效性。

Abstract: Explicit information seeking is essential to human problem-solving in
practical environments characterized by incomplete information and noisy
dynamics. When the true environmental state is not directly observable, humans
seek information to update their internal dynamics and inform future
decision-making. Although existing Large Language Model (LLM) planning agents
have addressed observational uncertainty, they often overlook discrepancies
between their internal dynamics and the actual environment. We introduce
Information Seeking Decision Planner (InfoSeeker), an LLM decision-making
framework that integrates task-oriented planning with information seeking to
align internal dynamics and make optimal decisions under uncertainty in both
agent observations and environmental dynamics. InfoSeeker prompts an LLM to
actively gather information by planning actions to validate its understanding,
detect environmental changes, or test hypotheses before generating or revising
task-oriented plans. To evaluate InfoSeeker, we introduce a novel benchmark
suite featuring partially observable environments with incomplete observations
and uncertain dynamics. Experiments demonstrate that InfoSeeker achieves a 74%
absolute performance gain over prior methods without sacrificing sample
efficiency. Moreover, InfoSeeker generalizes across LLMs and outperforms
baselines on established benchmarks such as robotic manipulation and web
navigation. These findings underscore the importance of tightly integrating
planning and information seeking for robust behavior in partially observable
environments. The project page is available at https://infoseekerllm.github.io

</details>


### [55] [Step-Aware Policy Optimization for Reasoning in Diffusion Large Language Models](https://arxiv.org/abs/2510.01544)
*Shaoan Xie,Lingjing Kong,Xiangchen Song,Xinshuai Dong,Guangyi Chen,Eric P. Xing,Kun Zhang*

Main category: cs.AI

TL;DR: 提出了Step-Aware Policy Optimization (SAPO)算法，通过过程奖励函数引导扩散语言模型学习结构化推理路径，解决了现有方法中推理步骤无意义的问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于强化学习的扩散语言模型训练依赖稀疏的结果奖励，会强化导致偶然正确结果的错误推理路径，这与推理的自然结构不匹配。

Method: 首先提出理论框架将复杂问题解决形式化为层次选择过程，然后引入SAPO算法，使用过程奖励函数使去噪过程与潜在推理层次对齐。

Result: 实验结果表明该方法在挑战性推理基准上显著提升性能，并增强了生成过程的可解释性。

Conclusion: 基于理论框架的SAPO算法能够有效引导扩散语言模型学习结构化推理，解决了现有方法的局限性。

Abstract: Diffusion language models (dLLMs) offer a promising, non-autoregressive
paradigm for text generation, yet training them for complex reasoning remains a
key challenge. Current reinforcement learning approaches often rely on sparse,
outcome-based rewards, which can reinforce flawed reasoning paths that lead to
coincidentally correct answers. We argue that this stems from a fundamental
mismatch with the natural structure of reasoning. We first propose a
theoretical framework that formalizes complex problem solving as a hierarchical
selection process, where an intractable global constraint is decomposed into a
series of simpler, localized logical steps. This framework provides a
principled foundation for algorithm design, including theoretical insights into
the identifiability of this latent reasoning structure. Motivated by this
theory, we identify unstructured refinement -- a failure mode where a model's
iterative steps do not contribute meaningfully to the solution -- as a core
deficiency in existing methods. We then introduce Step-Aware Policy
Optimization (SAPO), a novel RL algorithm that aligns the dLLM's denoising
process with the latent reasoning hierarchy. By using a process-based reward
function that encourages incremental progress, SAPO guides the model to learn
structured, coherent reasoning paths. Our empirical results show that this
principled approach significantly improves performance on challenging reasoning
benchmarks and enhances the interpretability of the generation process.

</details>


### [56] [InvThink: Towards AI Safety via Inverse Reasoning](https://arxiv.org/abs/2510.01569)
*Yubin Kim,Taehan Kim,Eugene Park,Chunjong Park,Cynthia Breazeal,Daniel McDuff,Hae Won Park*

Main category: cs.AI

TL;DR: InvThink是一种让大语言模型具备逆向思维能力的方法，通过预先分析潜在危害及其后果来生成安全输出，在安全性和通用推理能力方面均有显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有安全对齐方法直接优化安全响应，但缺乏对潜在风险的系统性分析。InvThink旨在通过逆向思维让模型在生成响应前先识别和避免潜在危害。

Method: 1) 列举潜在危害 2) 分析后果 3) 生成主动避免这些风险的安全输出。通过监督微调和强化学习在三个LLM家族中实现。

Result: 安全改进随模型规模扩大而增强；缓解安全税效应，保持标准基准上的通用推理能力；在高风险领域（医疗、金融、法律等）相比基线方法减少有害响应达15.7%。

Conclusion: 逆向推理为构建更安全、更强大的语言模型提供了可扩展且通用的路径。

Abstract: We present InvThink, a simple yet powerful approach that gives large language
models (LLMs) the capability of inverse thinking: reasoning through failure
modes before generating responses. Unlike existing safety alignment methods
that optimize directly for safe response, InvThink instructs models to 1)
enumerate potential harms, 2) analyze their consequences, and 3) generate safe
outputs that proactively avoid these risks. Our method reveals three key
findings: (i) safety improvements show stronger scaling with model size
compared to existing safety methods. (ii) InvThink mitigates safety tax; by
training models to systematically consider failure modes, it preserves general
reasoning capabilities on standard benchmarks. (iii) beyond general safety
tasks, InvThink excels in high-stakes domains including external-facing
(medicine, finance, law) and agentic (blackmail, murder) risk scenarios,
achieving up to 15.7% reduction in harmful responses compared to baseline
methods like SafetyPrompt. We further implement InvThink via supervised
fine-tuning, and reinforcement learning across three LLM families. These
results suggest that inverse reasoning provides a scalable and generalizable
path toward safer, more capable language models.

</details>


### [57] [UpSafe$^\circ$C: Upcycling for Controllable Safety in Large Language Models](https://arxiv.org/abs/2510.02194)
*Yuhao Sun,Zhuoer Xu,Shiwen Cui,Kun Yang,Lingyun Yu,Yongdong Zhang,Hongtao Xie*

Main category: cs.AI

TL;DR: 提出了UpSafe°C框架，通过安全感知的上循环技术增强LLM安全性，采用稀疏MoE结构和两阶段SFT策略，引入安全温度机制实现推理时灵活控制。


<details>
  <summary>Details</summary>
Motivation: 现有安全技术（外部护栏、推理时指导、后训练对齐）在平衡安全性、实用性和可控性方面存在局限性，需要更动态、模块化的安全控制方法。

Method: 识别安全关键层并将其上循环为稀疏MoE结构，路由器作为软护栏选择性激活原始MLP和安全专家；采用两阶段SFT策略；引入安全温度机制实现推理时动态调整。

Result: 在多个基准测试、基础模型和模型规模上实现了对有害和越狱输入的鲁棒安全改进，同时在通用任务上保持竞争力；安全温度实现了效用与安全性的帕累托最优前沿。

Conclusion: 为LLM安全开辟了新方向：从静态对齐转向动态、模块化和推理感知的控制。

Abstract: Large Language Models (LLMs) have achieved remarkable progress across a wide
range of tasks, but remain vulnerable to safety risks such as harmful content
generation and jailbreak attacks. Existing safety techniques -- including
external guardrails, inference-time guidance, and post-training alignment --
each face limitations in balancing safety, utility, and controllability. In
this work, we propose UpSafe$^\circ$C, a unified framework for enhancing LLM
safety through safety-aware upcycling. Our approach first identifies
safety-critical layers and upcycles them into a sparse Mixture-of-Experts (MoE)
structure, where the router acts as a soft guardrail that selectively activates
original MLPs and added safety experts. We further introduce a two-stage SFT
strategy to strengthen safety discrimination while preserving general
capabilities. To enable flexible control at inference time, we introduce a
safety temperature mechanism, allowing dynamic adjustment of the trade-off
between safety and utility. Experiments across multiple benchmarks, base model,
and model scales demonstrate that UpSafe$^\circ$C achieves robust safety
improvements against harmful and jailbreak inputs, while maintaining
competitive performance on general tasks. Moreover, analysis shows that safety
temperature provides fine-grained inference-time control that achieves the
Pareto-optimal frontier between utility and safety. Our results highlight a new
direction for LLM safety: moving from static alignment toward dynamic, modular,
and inference-aware control.

</details>


### [58] [AdvEvo-MARL: Shaping Internalized Safety through Adversarial Co-Evolution in Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2510.01586)
*Zhenyu Pan,Yiting Zhang,Zhuo Liu,Yolo Yunlong Tang,Zeliang Zhang,Haozheng Luo,Yuwei Han,Jianshu Zhang,Dennis Wu,Hong-Yu Chen,Haoran Lu,Haoyang Fang,Manling Li,Chenliang Xu,Philip S. Yu,Han Liu*

Main category: cs.AI

TL;DR: AdvEvo-MARL是一个协同进化的多智能体强化学习框架，通过在对抗学习环境中联合优化攻击者和防御者，将安全性内化到任务智能体中，无需依赖外部防护模块。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM多智能体系统防御方法存在局限性：自我验证方法因单个智能体能力不足而表现不佳；外部防护模块会增加系统开销并造成单点故障。需要一种能够同时保证安全性和效用的解决方案。

Method: 提出AdvEvo-MARL框架，在对抗学习环境中联合优化攻击者（生成不断演变的越狱提示）和防御者（训练任务智能体既完成任务又抵抗攻击）。引入公共基线优势估计，同一功能组内的智能体共享组级平均回报基线，实现低方差更新和更强的组内协调。

Result: 在代表性攻击场景中，AdvEvo-MARL始终将攻击成功率（ASR）保持在20%以下，而基线方法最高可达38.33%。同时保持甚至提高了任务准确性（在推理任务上最高提升+3.67%）。

Conclusion: 安全性和效用可以共同改进，无需依赖额外的防护智能体或增加系统开销。AdvEvo-MARL通过将安全性内化到任务智能体中，提供了一种更有效的多智能体系统安全防护方案。

Abstract: LLM-based multi-agent systems excel at planning, tool use, and role
coordination, but their openness and interaction complexity also expose them to
jailbreak, prompt-injection, and adversarial collaboration. Existing defenses
fall into two lines: (i) self-verification that asks each agent to pre-filter
unsafe instructions before execution, and (ii) external guard modules that
police behaviors. The former often underperforms because a standalone agent
lacks sufficient capacity to detect cross-agent unsafe chains and
delegation-induced risks; the latter increases system overhead and creates a
single-point-of-failure-once compromised, system-wide safety collapses, and
adding more guards worsens cost and complexity. To solve these challenges, we
propose AdvEvo-MARL, a co-evolutionary multi-agent reinforcement learning
framework that internalizes safety into task agents. Rather than relying on
external guards, AdvEvo-MARL jointly optimizes attackers (which synthesize
evolving jailbreak prompts) and defenders (task agents trained to both
accomplish their duties and resist attacks) in adversarial learning
environments. To stabilize learning and foster cooperation, we introduce a
public baseline for advantage estimation: agents within the same functional
group share a group-level mean-return baseline, enabling lower-variance updates
and stronger intra-group coordination. Across representative attack scenarios,
AdvEvo-MARL consistently keeps attack-success rate (ASR) below 20%, whereas
baselines reach up to 38.33%, while preserving-and sometimes improving-task
accuracy (up to +3.67% on reasoning tasks). These results show that safety and
utility can be jointly improved without relying on extra guard agents or added
system overhead.

</details>


### [59] [AgentRec: Next-Generation LLM-Powered Multi-Agent Collaborative Recommendation with Adaptive Intelligence](https://arxiv.org/abs/2510.01609)
*Bo Ma,Hang Li,ZeHua Hu,XiaoFan Gui,LuYao Liu,Simon Lau*

Main category: cs.AI

TL;DR: AgentRec是一个基于LLM的多智能体协作推荐框架，通过分层智能体网络解决动态用户偏好、对话连贯性和多目标平衡问题，在多个指标上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有对话推荐系统在处理动态用户偏好、保持对话连贯性和平衡多目标方面存在挑战，需要更智能的协作框架来提升推荐效果。

Method: 采用分层智能体网络，包括对话理解、偏好建模、上下文感知和动态排序等专门智能体，通过自适应权重机制协调，结合三层学习策略处理不同复杂度的查询。

Result: 在三个真实数据集上的实验表明，AgentRec在对话成功率提升2.8%、推荐准确率(NDCG@10)提升1.9%、对话效率提升3.2%，同时保持可比较的计算成本。

Conclusion: AgentRec通过多智能体协作框架有效解决了对话推荐系统的关键挑战，在多个性能指标上实现了显著提升，证明了该方法的有效性。

Abstract: Interactive conversational recommender systems have gained significant
attention for their ability to capture user preferences through natural
language interactions. However, existing approaches face substantial challenges
in handling dynamic user preferences, maintaining conversation coherence, and
balancing multiple ranking objectives simultaneously. This paper introduces
AgentRec, a next-generation LLM-powered multi-agent collaborative
recommendation framework that addresses these limitations through hierarchical
agent networks with adaptive intelligence. Our approach employs specialized
LLM-powered agents for conversation understanding, preference modeling, context
awareness, and dynamic ranking, coordinated through an adaptive weighting
mechanism that learns from interaction patterns. We propose a three-tier
learning strategy combining rapid response for simple queries, intelligent
reasoning for complex preferences, and deep collaboration for challenging
scenarios. Extensive experiments on three real-world datasets demonstrate that
AgentRec achieves consistent improvements over state-of-the-art baselines, with
2.8\% enhancement in conversation success rate, 1.9\% improvement in
recommendation accuracy (NDCG@10), and 3.2\% better conversation efficiency
while maintaining comparable computational costs through intelligent agent
coordination.

</details>


### [60] [PychoBench: Evaluating the Psychology Intelligence of Large Language Models](https://arxiv.org/abs/2510.01611)
*Min Zeng*

Main category: cs.AI

TL;DR: 该论文研究了大型语言模型在心理咨询领域的应用潜力，通过开发基于美国国家咨询师认证考试的PsychoBench基准测试，评估LLMs是否具备成为心理咨询师所需的专业知识。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在需要认知能力的应用领域（如心理咨询）的潜力，确定LLMs是否能够满足心理咨询师的专业资格要求。

Method: 开发PsychoBench基准测试，包含2,252个精心设计的单选题，基于美国国家咨询师认证考试，涵盖心理学各个子学科，要求深度理解和广泛知识。

Result: 先进模型如GPT-4o、Llama3.3-70B和Gemma3-27B表现远超通过阈值（约70%准确率），而较小的开源模型（如Qwen2.5-7B、Mistral-7B）远低于该阈值。

Conclusion: 只有前沿的LLMs目前能够满足心理咨询考试标准，这突显了开发心理学导向LLMs的潜力和挑战。

Abstract: Large Language Models (LLMs) have demonstrated remarkable success across a
wide range of industries, primarily due to their impressive generative
abilities. Yet, their potential in applications requiring cognitive abilities,
such as psychological counseling, remains largely untapped. This paper
investigates the key question: Can LLMs be effectively applied to psychological
counseling? To determine whether an LLM can effectively take on the role of a
psychological counselor, the first step is to assess whether it meets the
qualifications required for such a role, namely the ability to pass the U.S.
National Counselor Certification Exam (NCE). This is because, just as a human
counselor must pass a certification exam to practice, an LLM must demonstrate
sufficient psychological knowledge to meet the standards required for such a
role. To address this, we introduce PsychoBench, a benchmark grounded in
U.S.national counselor examinations, a licensure test for professional
counselors that requires about 70% accuracy to pass. PsychoBench comprises
approximately 2,252 carefully curated single-choice questions, crafted to
require deep understanding and broad enough to cover various sub-disciplines of
psychology. This benchmark provides a comprehensive assessment of an LLM's
ability to function as a counselor. Our evaluation shows that advanced models
such as GPT-4o, Llama3.3-70B, and Gemma3-27B achieve well above the passing
threshold, while smaller open-source models (e.g., Qwen2.5-7B, Mistral-7B)
remain far below it. These results suggest that only frontier LLMs are
currently capable of meeting counseling exam standards, highlighting both the
promise and the challenges of developing psychology-oriented LLMs.

</details>


### [61] [Learning to Decide with Just Enough: Information-Theoretic Context Summarization for CDMPs](https://arxiv.org/abs/2510.01620)
*Peidong Liu,Junjiang Lin,Shaowen Wang,Yao Xu,Haiqing Li,Xuhao Xie,Siyi Wu,Hao Li*

Main category: cs.AI

TL;DR: 提出基于信息论的上下文马尔可夫决策过程(CMDPs)方法，使用大语言模型压缩高维上下文输入为低维语义摘要，提升决策效率并降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有CMDP方法在高维或非结构化上下文中泛化能力差，导致计算开销大且性能不稳定，需要更高效的上下文处理方法。

Method: 使用大语言模型进行信息论摘要，将上下文输入压缩为低维语义丰富的摘要，这些摘要通过保留决策关键信息同时减少冗余来增强状态表示。

Result: 在离散、连续、视觉和推荐基准测试中，该方法优于原始上下文和非上下文基线，提高了奖励、成功率和样本效率，同时减少了延迟和内存使用。

Conclusion: 基于LLM的摘要为上下文丰富、资源受限环境中的高效决策提供了可扩展且可解释的解决方案。

Abstract: Contextual Markov Decision Processes (CMDPs) offer a framework for sequential
decision-making under external signals, but existing methods often fail to
generalize in high-dimensional or unstructured contexts, resulting in excessive
computation and unstable performance. We propose an information-theoretic
summarization approach that uses large language models (LLMs) to compress
contextual inputs into low-dimensional, semantically rich summaries. These
summaries augment states by preserving decision-critical cues while reducing
redundancy. Building on the notion of approximate context sufficiency, we
provide, to our knowledge, the first regret bounds and a latency-entropy
trade-off characterization for CMDPs. Our analysis clarifies how
informativeness impacts computational cost. Experiments across discrete,
continuous, visual, and recommendation benchmarks show that our method
outperforms raw-context and non-context baselines, improving reward, success
rate, and sample efficiency, while reducing latency and memory usage. These
findings demonstrate that LLM-based summarization offers a scalable and
interpretable solution for efficient decision-making in context-rich,
resource-constrained environments.

</details>


### [62] [Understanding the Geospatial Reasoning Capabilities of LLMs: A Trajectory Recovery Perspective](https://arxiv.org/abs/2510.01639)
*Thinh Hung Truong,Jey Han Lau,Jianzhong Qi*

Main category: cs.AI

TL;DR: LLMs能够理解道路网络地图并执行导航任务，在轨迹恢复任务中表现优于现有基线模型，具有强大的零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型在空间推理方面的能力，特别是能否读取道路网络地图并执行导航任务。

Method: 将轨迹恢复作为代理任务，使用GLOBALTRACE数据集（包含4000+真实轨迹），通过提示框架让LLMs基于道路网络生成有效路径，无需外部导航工具。

Result: LLMs在轨迹恢复任务中表现优于现有基线模型和专用轨迹恢复模型，具有强大的零样本泛化能力，但对不同区域和交通方式存在系统性偏差。

Conclusion: LLMs能够通过灵活推理地图来增强导航体验，整合用户偏好，展现了强大的空间推理能力。

Abstract: We explore the geospatial reasoning capabilities of Large Language Models
(LLMs), specifically, whether LLMs can read road network maps and perform
navigation. We frame trajectory recovery as a proxy task, which requires models
to reconstruct masked GPS traces, and introduce GLOBALTRACE, a dataset with
over 4,000 real-world trajectories across diverse regions and transportation
modes. Using road network as context, our prompting framework enables LLMs to
generate valid paths without accessing any external navigation tools.
Experiments show that LLMs outperform off-the-shelf baselines and specialized
trajectory recovery models, with strong zero-shot generalization. Fine-grained
analysis shows that LLMs have strong comprehension of the road network and
coordinate systems, but also pose systematic biases with respect to regions and
transportation modes. Finally, we demonstrate how LLMs can enhance navigation
experiences by reasoning over maps in flexible ways to incorporate user
preferences.

</details>


### [63] [GuruAgents: Emulating Wise Investors with Prompt-Guided LLM Agents](https://arxiv.org/abs/2510.01664)
*Yejin Kim,Youngbin Lee,Juhyeong Kim,Yongjae Lee*

Main category: cs.AI

TL;DR: 本研究开发了五个基于提示工程的AI投资代理(GuruAgents)，分别模拟传奇投资大师的投资策略，在纳斯达克100成分股上进行回测，其中巴菲特代理表现最佳，年化回报率达42.2%。


<details>
  <summary>Details</summary>
Motivation: 将定性投资哲学转化为可复现的量化策略，探索自动化系统投资的新方向。

Method: 通过将不同投资大师的独特哲学编码到LLM提示中，整合金融工具和确定性推理管道，开发五个不同的GuruAgents。

Result: 在2023年第四季度至2025年第二季度的回测中，各代理表现出由其提示人格驱动的独特行为，巴菲特代理表现最佳，年化回报率42.2%显著超越基准。

Conclusion: 提示工程能够成功将投资大师的定性哲学转化为可复现的量化策略，为自动化系统投资开辟了新方向。

Abstract: This study demonstrates that GuruAgents, prompt-guided AI agents, can
systematically operationalize the strategies of legendary investment gurus. We
develop five distinct GuruAgents, each designed to emulate an iconic investor,
by encoding their distinct philosophies into LLM prompts that integrate
financial tools and a deterministic reasoning pipeline. In a backtest on
NASDAQ-100 constituents from Q4 2023 to Q2 2025, the GuruAgents exhibit unique
behaviors driven by their prompted personas. The Buffett GuruAgent achieves the
highest performance, delivering a 42.2\% CAGR that significantly outperforms
benchmarks, while other agents show varied results. These findings confirm that
prompt engineering can successfully translate the qualitative philosophies of
investment gurus into reproducible, quantitative strategies, highlighting a
novel direction for automated systematic investing. The source code and data
are available at https://github.com/yejining99/GuruAgents.

</details>


### [64] [A Locally Executable AI System for Improving Preoperative Patient Communication: A Multi-Domain Clinical Evaluation](https://arxiv.org/abs/2510.01671)
*Motoki Sato,Yuki Matsushita,Hidekazu Takahashi,Tomoaki Kakazu,Sou Nagata,Mizuho Ohnuma,Atsushi Yoshikawa,Masayuki Yamamura*

Main category: cs.AI

TL;DR: LENOHA是一个安全优先、本地优先的系统，通过高精度分类器将临床查询路由到医生策划的FAQ数据库，返回原文答案而非生成文本，在临床路径中避免了生成错误。


<details>
  <summary>Details</summary>
Motivation: 患者在等待侵入性手术时通常有未解答的问题，但时间紧迫的工作流程和隐私限制限制了个性化咨询。

Method: 使用高精度句子转换器分类器路由输入，从临床医生策划的FAQ返回原文答案，消除临床路径中的自由文本生成。在两个领域（拔牙和胃镜检查）进行评估。

Result: E5-large-instruct编码器总体准确率0.983，AUC 0.996，仅7个错误，与GPT-4o统计无差异。非生成临床路径每输入消耗约1.0 mWh，比本地8B SLM小170倍，延迟约0.10秒。

Conclusion: 通过返回经过验证的FAQ原文答案，在临床路径中结构性地避免了前沿判别和生成引起的错误，支持隐私保护、可持续性和在带宽受限环境中的公平部署。

Abstract: Patients awaiting invasive procedures often have unanswered pre-procedural
questions; however, time-pressured workflows and privacy constraints limit
personalized counseling. We present LENOHA (Low Energy, No Hallucination, Leave
No One Behind Architecture), a safety-first, local-first system that routes
inputs with a high-precision sentence-transformer classifier and returns
verbatim answers from a clinician-curated FAQ for clinical queries, eliminating
free-text generation in the clinical path. We evaluated two domains (tooth
extraction and gastroscopy) using expert-reviewed validation sets
(n=400/domain) for thresholding and independent test sets (n=200/domain). Among
the four encoders, E5-large-instruct (560M) achieved an overall accuracy of
0.983 (95% CI 0.964-0.991), AUC 0.996, and seven total errors, which were
statistically indistinguishable from GPT-4o on this task; Gemini made no errors
on this test set. Energy logging shows that the non-generative clinical path
consumes ~1.0 mWh per input versus ~168 mWh per small-talk reply from a local
8B SLM, a ~170x difference, while maintaining ~0.10 s latency on a single
on-prem GPU. These results indicate that near-frontier discrimination and
generation-induced errors are structurally avoided in the clinical path by
returning vetted FAQ answers verbatim, supporting privacy, sustainability, and
equitable deployment in bandwidth-limited environments.

</details>


### [65] [Improving AGI Evaluation: A Data Science Perspective](https://arxiv.org/abs/2510.01687)
*John Hawkins*

Main category: cs.AI

TL;DR: 本文提出AGI评估方法应从基于直觉的合成任务转向基于稳健任务执行能力的评估，借鉴数据科学实践来证明系统可部署性。


<details>
  <summary>Details</summary>
Motivation: 当前AGI评估方法主要依赖对智能的直觉理解来设计合成任务，但这些方法在AI历史上表现不佳，需要更有效的评估方法。

Method: 提出基于稳健任务执行能力的评估哲学，借鉴数据科学中证明系统可可靠部署的实践方法，提供实际评估示例。

Result: 论证了基于直觉的合成任务评估方法的局限性，提出了更实用的基于任务执行能力的替代方案。

Conclusion: AGI评估应转向关注系统在实际任务中的稳健执行能力，而非依赖直觉设计的合成任务，这能更好地证明系统的实际部署价值。

Abstract: Evaluation of potential AGI systems and methods is difficult due to the
breadth of the engineering goal. We have no methods for perfect evaluation of
the end state, and instead measure performance on small tests designed to
provide directional indication that we are approaching AGI. In this work we
argue that AGI evaluation methods have been dominated by a design philosophy
that uses our intuitions of what intelligence is to create synthetic tasks,
that have performed poorly in the history of AI. Instead we argue for an
alternative design philosophy focused on evaluating robust task execution that
seeks to demonstrate AGI through competence. This perspective is developed from
common practices in data science that are used to show that a system can be
reliably deployed. We provide practical examples of what this would mean for
AGI evaluation.

</details>


### [66] [VaPR -- Vision-language Preference alignment for Reasoning](https://arxiv.org/abs/2510.01700)
*Rohan Wadhawan,Fabrice Y Harel-Canada,Zi-Yi Dou,Suhaila Shakiah,Robinson Piramuthu,Nanyun Peng*

Main category: cs.AI

TL;DR: 提出了VaPR框架，通过LLM引导的响应编辑生成硬负样本，解决了合成偏好标注中的风格和长度偏差问题，显著提升了大型视觉语言模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的偏好微调方法忽略了合成偏好标注中普遍存在的风格和长度偏差噪声，需要一种能产生高质量负样本的方法来改善模型对齐效果。

Method: 基于LLM引导的响应编辑框架，生成具有目标错误的拒绝响应，同时保持与接受响应在风格和长度上的相似性，构建了包含30K高质量样本的VaPR数据集。

Result: 在三个LVLM家族上显著提升性能，平均增益分别为6.5%(LLaVA)、4.0%(Qwen2VL)和1.5%(Qwen2.5VL)，在推理任务上表现尤为突出，并减少了二元问题中回答"是"的倾向。

Conclusion: VaPR框架有效解决了合成偏好标注的噪声问题，性能随数据规模扩展而提升，且能泛化到开源LLM作为编辑器，实现了接近GPT-4o合成数据的性能。

Abstract: Preference finetuning methods like Direct Preference Optimization (DPO) with
AI-generated feedback have shown promise in aligning Large Vision-Language
Models (LVLMs) with human preferences. However, existing techniques overlook
the prevalence of noise in synthetic preference annotations in the form of
stylistic and length biases. To this end, we introduce a hard-negative response
generation framework based on LLM-guided response editing, that produces
rejected responses with targeted errors, maintaining stylistic and length
similarity to the accepted ones. Using this framework, we develop the VaPR
dataset, comprising 30K high-quality samples, to finetune three LVLM families:
LLaVA-V1.5, Qwen2VL & Qwen2.5VL (2B-13B sizes). Our VaPR models deliver
significant performance improvements across ten benchmarks, achieving average
gains of 6.5% (LLaVA), 4.0% (Qwen2VL), and 1.5% (Qwen2.5VL), with notable
improvements on reasoning tasks. A scaling analysis shows that performance
consistently improves with data size, with LLaVA models benefiting even at
smaller scales. Moreover, VaPR reduces the tendency to answer "Yes" in binary
questions - addressing a common failure mode in LVLMs like LLaVA. Lastly, we
show that the framework generalizes to open-source LLMs as editors, with models
trained on VaPR-OS achieving ~99% of the performance of models trained on
\name, which is synthesized using GPT-4o. Our data, models, and code can be
found on the project page https://vap-r.github.io

</details>


### [67] [MetaboT: AI-based agent for natural language-based interaction with metabolomics knowledge graphs](https://arxiv.org/abs/2510.01724)
*Madina Bekbergenova,Lucas Pradi,Benjamin Navet,Emma Tysinger,Franck Michel,Matthieu Feraud,Yousouf Taghzouti,Yan Zhou Chen,Olivier Kirchhoffer,Florence Mehl,Martin Legrand,Tao Jiang,Marco Pagni,Soha Hassoun,Jean-Luc Wolfender,Wout Bittremieux,Fabien Gandon,Louis-Félix Nothias*

Main category: cs.AI

TL;DR: MetaboT是一个基于大语言模型的多智能体AI系统，能够将用户自然语言问题转换为SPARQL查询语言，用于操作代谢组学知识图谱，显著提高了查询准确率。


<details>
  <summary>Details</summary>
Motivation: 代谢组学质谱分析产生大量数据，知识图谱虽能结构化这些数据，但使用需要深入理解其本体和查询语言语法，技术门槛较高。

Method: 采用多智能体系统架构，使用LangChain和LangGraph库集成LLM与外部工具，通过专门的智能体处理用户查询、验证问题、识别化学转换需求，并生成SPARQL查询。

Result: 在50个代谢组学相关问题的测试中，MetaboT达到83.67%的准确率，显著优于仅使用GPT-4o的基线方法（8.16%）。

Conclusion: MetaboT作为对话式问答助手，通过自动化SPARQL查询生成和执行，消除了访问知识图谱的技术障碍，同时保持实验基础的查询生成，确保输出符合领域标准。

Abstract: Mass spectrometry metabolomics generates vast amounts of data requiring
advanced methods for interpretation. Knowledge graphs address these challenges
by structuring mass spectrometry data, metabolite information, and their
relationships into a connected network (Gaudry et al. 2024). However, effective
use of a knowledge graph demands an in-depth understanding of its ontology and
its query language syntax. To overcome this, we designed MetaboT, an AI system
utilizing large language models (LLMs) to translate user questions into SPARQL
semantic query language for operating on knowledge graphs (Steve Harris 2013).
We demonstrate its effectiveness using the Experimental Natural Products
Knowledge Graph (ENPKG), a large-scale public knowledge graph for plant natural
products (Gaudry et al. 2024).MetaboT employs specialized AI agents for
handling user queries and interacting with the knowledge graph by breaking down
complex tasks into discrete components, each managed by a specialised agent
(Fig. 1a). The multi-agent system is constructed using the LangChain and
LangGraph libraries, which facilitate the integration of LLMs with external
tools and information sources (LangChain, n.d.). The query generation process
follows a structured workflow. First, the Entry Agent determines if the
question is new or a follow-up to previous interactions. New questions are
forwarded to the Validator Agent, which verifies if the question is related to
the knowledge graph. Then, the valid question is sent to the Supervisor Agent,
which identifies if the question requires chemical conversions or standardized
identifiers. In this case it delegates the question to the Knowledge Graph
Agent, which can use tools to extract necessary details, such as URIs or
taxonomies of chemical names, from the user query. Finally, an agent
responsible for crafting the SPARQL queries equipped with the ontology of the
knowledge graph uses the provided identifiers to generate the query. Then, the
system executes the generated query against the metabolomics knowledge graph
and returns structured results to the user (Fig. 1b). To assess the performance
of MetaboT we have curated 50 metabolomics-related questions and their expected
answers. In addition to submitting these questions to MetaboT, we evaluated a
baseline by submitting them to a standard LLM (GPT-4o) with a prompt that
incorporated the knowledge graph ontology but did not provide specific entity
IDs. This baseline achieved only 8.16% accuracy, compared to MetaboT's 83.67%,
underscoring the necessity of our multi-agent system for accurately retrieving
entities and generating correct SPARQL queries. MetaboT demonstrates promising
performance as a conversational question-answering assistant, enabling
researchers to retrieve structured metabolomics data through natural language
queries. By automating the generation and execution of SPARQL queries, it
removes technical barriers that have traditionally hindered access to knowledge
graphs. Importantly, MetaboT leverages the capabilities of LLMs while
maintaining experimentally grounded query generation, ensuring that outputs
remain aligned with domain-specific standards and data structures. This
approach facilitates data-driven discoveries by bridging the gap between
complex semantic technologies and user-friendly interaction. MetaboT is
accessible at [https://metabot.holobiomicslab.eu/], and its source code is
available at [https://github.com/HolobiomicsLab/MetaboT].

</details>


### [68] [A cybersecurity AI agent selection and decision support framework](https://arxiv.org/abs/2510.01751)
*Masike Malatji*

Main category: cs.AI

TL;DR: 提出了一个结构化决策支持框架，将不同类型的AI代理架构与NIST网络安全框架2.0系统对齐，为组织选择AI解决方案提供透明的方法论。


<details>
  <summary>Details</summary>
Motivation: 弥合理论AI构建与操作网络安全需求之间的差距，提供统一的检测、事件响应和治理策略，超越孤立的AI应用。

Method: 采用结构化方法，将NIST CSF 2.0功能分解为具体任务，将AI代理特性（自主性、自适应学习、实时响应）与安全需求关联，并定义分级自主级别。

Result: 通过概念验证，框架展示了定制AI代理部署如何与现实约束和风险状况对齐，增强态势感知、加速响应时间，并通过自适应风险管理加强长期韧性。

Conclusion: 该研究为遵循行业标准的稳健、经验验证的多代理系统奠定了基础，建立了AI理论与网络安全操作需求之间的桥梁。

Abstract: This paper presents a novel, structured decision support framework that
systematically aligns diverse artificial intelligence (AI) agent architectures,
reactive, cognitive, hybrid, and learning, with the comprehensive National
Institute of Standards and Technology (NIST) Cybersecurity Framework (CSF) 2.0.
By integrating agent theory with industry guidelines, this framework provides a
transparent and stepwise methodology for selecting and deploying AI solutions
to address contemporary cyber threats. Employing a granular decomposition of
NIST CSF 2.0 functions into specific tasks, the study links essential AI agent
properties such as autonomy, adaptive learning, and real-time responsiveness to
each subcategory's security requirements. In addition, it outlines graduated
levels of autonomy (assisted, augmented, and fully autonomous) to accommodate
organisations at varying stages of cybersecurity maturity. This holistic
approach transcends isolated AI applications, providing a unified detection,
incident response, and governance strategy. Through conceptual validation, the
framework demonstrates how tailored AI agent deployments can align with
real-world constraints and risk profiles, enhancing situational awareness,
accelerating response times, and fortifying long-term resilience via adaptive
risk management. Ultimately, this research bridges the gap between theoretical
AI constructs and operational cybersecurity demands, establishing a foundation
for robust, empirically validated multi-agent systems that adhere to industry
standards.

</details>


### [69] [REBot: From RAG to CatRAG with Semantic Enrichment and Graph Routing](https://arxiv.org/abs/2510.01800)
*Thanh Ma,Tri-Tam La,Lam-Thu Le Huu,Minh-Nghi Nguyen,Khanh-Van Pham Luu,Huu-Hoa Nguyen*

Main category: cs.AI

TL;DR: REBot是一个基于CatRAG框架的学术规章制度咨询聊天机器人，通过混合检索推理方法实现高精度咨询，在分类和问答任务中达到98.89%的F1分数。


<details>
  <summary>Details</summary>
Motivation: 学术规章制度咨询对学生理解院校政策至关重要，但构建有效系统需要特定领域的监管资源支持。

Method: 提出CatRAG混合检索推理框架，结合检索增强生成和图推理，使用分层类别标记的知识图谱，配备轻量级意图分类器路由查询。

Result: 构建了规章制度专用数据集，在分类和问答任务中实现了98.89%的F1分数，达到最先进性能。

Conclusion: 开发了Web应用验证REBot在实际学术咨询场景中的实用价值，证明了该方法的有效性。

Abstract: Academic regulation advising is essential for helping students interpret and
comply with institutional policies, yet building effective systems requires
domain specific regulatory resources. To address this challenge, we propose
REBot, an LLM enhanced advisory chatbot powered by CatRAG, a hybrid retrieval
reasoning framework that integrates retrieval augmented generation with graph
based reasoning. CatRAG unifies dense retrieval and graph reasoning, supported
by a hierarchical, category labeled knowledge graph enriched with semantic
features for domain alignment. A lightweight intent classifier routes queries
to the appropriate retrieval modules, ensuring both factual accuracy and
contextual depth. We construct a regulation specific dataset and evaluate REBot
on classification and question answering tasks, achieving state of the art
performance with an F1 score of 98.89%. Finally, we implement a web application
that demonstrates the practical value of REBot in real world academic advising
scenarios.

</details>


### [70] [Human-AI Teaming Co-Learning in Military Operations](https://arxiv.org/abs/2510.01815)
*Clara Maathuis,Kasper Cools*

Main category: cs.AI

TL;DR: 提出可信赖的协同学习模型用于军事行动中的人机协作，包含四个维度：可调节自主性、多层控制、双向反馈和协作决策


<details>
  <summary>Details</summary>
Motivation: 在快速演变的军事威胁和复杂作战环境中，AI集成面临有效性和伦理挑战，需要从系统内部动态角度解决多维责任、安全和鲁棒性问题

Method: 设计包含四个维度的协同学习模型：可调节自主性（根据任务状态、系统置信度等动态调整）、多层控制（持续监督和问责）、双向反馈（显性和隐性反馈循环）、协作决策（生成带置信度和理由的决策）

Result: 提供了具体示例和建议，有助于进一步开发负责任和可信赖的军事人机协作系统

Conclusion: 该模型通过双向洞察交换和联合适应，为军事行动中构建可信赖的人机协作系统提供了框架

Abstract: In a time of rapidly evolving military threats and increasingly complex
operational environments, the integration of AI into military operations proves
significant advantages. At the same time, this implies various challenges and
risks regarding building and deploying human-AI teaming systems in an effective
and ethical manner. Currently, understanding and coping with them are often
tackled from an external perspective considering the human-AI teaming system as
a collective agent. Nevertheless, zooming into the dynamics involved inside the
system assures dealing with a broader palette of relevant multidimensional
responsibility, safety, and robustness aspects. To this end, this research
proposes the design of a trustworthy co-learning model for human-AI teaming in
military operations that encompasses a continuous and bidirectional exchange of
insights between the human and AI agents as they jointly adapt to evolving
battlefield conditions. It does that by integrating four dimensions. First,
adjustable autonomy for dynamically calibrating the autonomy levels of agents
depending on aspects like mission state, system confidence, and environmental
uncertainty. Second, multi-layered control which accounts continuous oversight,
monitoring of activities, and accountability. Third, bidirectional feedback
with explicit and implicit feedback loops between the agents to assure a proper
communication of reasoning, uncertainties, and learned adaptations that each of
the agents has. And fourth, collaborative decision-making which implies the
generation, evaluation, and proposal of decisions associated with confidence
levels and rationale behind them. The model proposed is accompanied by concrete
exemplifications and recommendations that contribute to further developing
responsible and trustworthy human-AI teaming systems in military operations.

</details>


### [71] [Plan Then Action:High-Level Planning Guidance Reinforcement Learning for LLM Reasoning](https://arxiv.org/abs/2510.01833)
*Zhihao Dou,Qinjian Zhao,Zhongwei Wan,Dinggen Zhang,Weida Wang,Towsif Raiyan,Benteng Chen,Qingtao Pan,Yang Ouyang,Zhiqiang Gao,Shufei Zhang,Sumon Biswas*

Main category: cs.AI

TL;DR: 提出了PTA-GRPO框架，通过两阶段方法改进LLM的推理能力：第一阶段利用高级LLM将思维链提炼为紧凑的高层指导进行监督微调；第二阶段引入指导感知的强化学习方法联合优化最终输出和高层指导质量。


<details>
  <summary>Details</summary>
Motivation: 现有LLM的推理过程受限于自回归的token级生成，缺乏全局规划，导致推理冗余、不连贯或不准确。现有方法如树搜索和RL计算成本高且难以产生最优推理轨迹。

Method: 两阶段框架：1）使用高级LLM将CoT提炼为高层指导进行SFT；2）引入指导感知的RL方法联合优化最终输出和指导质量。

Result: 在多个数学推理基准测试（MATH、AIME2024、AIME2025、AMC）和不同基础模型上，PTA-GRPO均实现了稳定且显著的性能提升。

Conclusion: PTA-GRPO能够有效提升LLM的推理能力，在不同模型和任务上表现出良好的有效性和泛化性。

Abstract: Large language models (LLMs) have demonstrated remarkable reasoning abilities
in complex tasks, often relying on Chain-of-Thought (CoT) reasoning. However,
due to their autoregressive token-level generation, the reasoning process is
largely constrained to local decision-making and lacks global planning. This
limitation frequently results in redundant, incoherent, or inaccurate
reasoning, which significantly degrades overall performance. Existing
approaches, such as tree-based algorithms and reinforcement learning (RL),
attempt to address this issue but suffer from high computational costs and
often fail to produce optimal reasoning trajectories. To tackle this challenge,
we propose Plan-Then-Action Enhanced Reasoning with Group Relative Policy
Optimization PTA-GRPO, a two-stage framework designed to improve both
high-level planning and fine-grained CoT reasoning. In the first stage, we
leverage advanced LLMs to distill CoT into compact high-level guidance, which
is then used for supervised fine-tuning (SFT). In the second stage, we
introduce a guidance-aware RL method that jointly optimizes the final output
and the quality of high-level guidance, thereby enhancing reasoning
effectiveness. We conduct extensive experiments on multiple mathematical
reasoning benchmarks, including MATH, AIME2024, AIME2025, and AMC, across
diverse base models such as Qwen2.5-7B-Instruct, Qwen3-8B, Qwen3-14B, and
LLaMA3.2-3B. Experimental results demonstrate that PTA-GRPO consistently
achieves stable and significant improvements across different models and tasks,
validating its effectiveness and generalization.

</details>


### [72] [Learning a Dense Reasoning Reward Model from Expert Demonstration via Inverse Reinforcement Learning](https://arxiv.org/abs/2510.01857)
*Claudio Fanconi,Nicolás Astorga,Mihaela van der Schaar*

Main category: cs.AI

TL;DR: 该论文提出了一种对抗性逆强化学习方法，为大型语言模型推理学习密集的token级奖励模型，用于过程监督而非风格模仿。该推理奖励在训练时提供步骤级反馈优化推理策略，在推理时作为批评器在固定计算预算下重排采样轨迹。


<details>
  <summary>Details</summary>
Motivation: 重新构建和操作化对抗性逆强化学习到大型语言模型推理，直接从专家演示中学习密集的token级奖励模型，而不是通过监督微调模仿风格。

Method: 使用对抗性逆强化学习方法学习密集的token级推理奖励模型，该模型在训练时提供步骤级反馈优化推理策略，在推理时作为批评器重排采样轨迹。

Result: 在GSM8K数据集上使用Llama3和Qwen2.5骨干网络进行实验，证明：(i)密集推理奖励可作为学习信号引发推理；(ii)通过奖励引导的重排提高了预测性能（特别是基于Llama的策略）。

Conclusion: 通过将训练信号、推理时选择和token级诊断统一到单个推理奖励中，这项工作表明可重用的过程级奖励具有增强语言模型中多步推理的广泛潜力。

Abstract: We reframe and operationalise adversarial inverse reinforcement learning
(IRL) to large language model reasoning, learning a dense, token-level reward
model for process supervision directly from expert demonstrations rather than
imitating style via supervised fine-tuning. The learned reasoning reward serves
two complementary roles: (i) it provides step-level feedback to optimise a
reasoning policy during training; and (ii) it functions at inference as a
critic to rerank sampled traces under fixed compute budgets. We demonstrate
that our approach prioritises correctness over surface form, yielding scores
that correlate with eventual answer validity and enabling interpretable
localisation of errors within a trace. Empirically, on GSM8K with Llama3 and
Qwen2.5 backbones, we demonstrate: (i) dense reasoning rewards can be used as a
learning signal to elicit reasoning, and (ii) predictive performance is
improved from reward-guided reranking (notably for Llama-based policies). By
unifying training signals, inference-time selection, and token-level
diagnostics into a single reasoning reward, this work suggests reusable
process-level rewards with broad potential to enhance multi-step reasoning in
language models.

</details>


### [73] [Constrained Adaptive Rejection Sampling](https://arxiv.org/abs/2510.01902)
*Paweł Parys,Sairam Vaidya,Taylor Berg-Kirkpatrick,Loris D'Antoni*

Main category: cs.AI

TL;DR: 提出CARS方法，在保持语言模型分布不变的前提下，通过自适应剪枝技术提高约束生成的采样效率，避免无效前缀的重复探索。


<details>
  <summary>Details</summary>
Motivation: 现有约束生成方法存在两极化问题：贪婪约束解码会扭曲语言模型分布，而拒绝采样虽然保持分布保真度但计算效率低下。在程序模糊测试等需要样本有效性和多样性的领域，这两种方法都有问题。

Method: CARS方法从无约束采样开始，通过构建trie数据结构记录违反约束的后续序列，并从未来采样中减去这些无效路径的概率质量，实现自适应剪枝。

Result: 在程序模糊测试和分子生成等多个领域的实验中，CARS在采样效率（每个有效样本所需的LM前向传递次数）和样本多样性方面都优于GCD和其他近似方法。

Conclusion: CARS方法在不扭曲语言模型分布的前提下，显著提高了约束生成的采样效率，同时保持了样本的多样性，为需要严格约束的应用场景提供了更好的解决方案。

Abstract: Language Models (LMs) are increasingly used in applications where generated
outputs must satisfy strict semantic or syntactic constraints. Existing
approaches to constrained generation fall along a spectrum: greedy constrained
decoding methods enforce validity during decoding but distort the LM's
distribution, while rejection sampling (RS) preserves fidelity but wastes
computation by discarding invalid outputs. Both extremes are problematic in
domains such as program fuzzing, where both validity and diversity of samples
are essential. We present Constrained Adaptive Rejection Sampling (CARS), an
approach that strictly improves the sample-efficiency of RS without
distributional distortion. CARS begins with unconstrained LM sampling and
adaptively rules out constraint-violating continuations by recording them in a
trie and subtracting their probability mass from future draws. This adaptive
pruning ensures that prefixes proven invalid are never revisited, acceptance
rates improve monotonically, and the resulting samples exactly follow the
constrained distribution. In experiments on a variety of domains -- e.g.,
program fuzzing and molecular generation -- CARS consistently achieves higher
efficiency -- measured in the number of LM forward passes per valid sample --
while also producing stronger sample diversity than both GCD and methods that
approximate the LM's distribution.

</details>


### [74] [To Mask or to Mirror: Human-AI Alignment in Collective Reasoning](https://arxiv.org/abs/2510.01924)
*Crystal Qian,Aaron Parisi,Clémentine Bouleau,Vivian Tsai,Maël Lebreton,Lucas Dixon*

Main category: cs.AI

TL;DR: 该研究评估大语言模型在集体决策中与人类社会推理的对齐程度，通过Lost at Sea任务进行大规模实验，发现不同LLM在集体推理中的行为存在差异，对齐程度取决于情境、线索和模型特定的归纳偏见。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型越来越多地用于建模和增强集体决策，需要检验它们与人类社会推理的对齐程度，特别是集体层面的对齐，而非仅关注个体层面。

Method: 使用Lost at Sea社会心理学任务进行大规模在线实验(N=748)，随机分配组别进行领导者选举，一组显示人口统计属性，另一组使用假名。然后基于人类数据模拟匹配的LLM组别，评估Gemini 2.5、GPT 4.1、Claude Haiku 3.5和Gemma 3等模型。

Result: LLM行为出现分化：有些模型反映了人类偏见；其他模型则掩盖这些偏见并试图补偿。实证表明人类-AI在集体推理中的对齐取决于情境、线索和模型特定的归纳偏见。

Conclusion: 理解LLM如何与集体人类行为对齐对于推进社会对齐的AI至关重要，需要能够捕捉集体推理复杂性的动态基准测试。

Abstract: As large language models (LLMs) are increasingly used to model and augment
collective decision-making, it is critical to examine their alignment with
human social reasoning. We present an empirical framework for assessing
collective alignment, in contrast to prior work on the individual level. Using
the Lost at Sea social psychology task, we conduct a large-scale online
experiment (N=748), randomly assigning groups to leader elections with either
visible demographic attributes (e.g. name, gender) or pseudonymous aliases. We
then simulate matched LLM groups conditioned on the human data, benchmarking
Gemini 2.5, GPT 4.1, Claude Haiku 3.5, and Gemma 3. LLM behaviors diverge: some
mirror human biases; others mask these biases and attempt to compensate for
them. We empirically demonstrate that human-AI alignment in collective
reasoning depends on context, cues, and model-specific inductive biases.
Understanding how LLMs align with collective human behavior is critical to
advancing socially-aligned AI, and demands dynamic benchmarks that capture the
complexities of collective reasoning.

</details>


### [75] [Zero-shot reasoning for simulating scholarly peer-review](https://arxiv.org/abs/2510.02027)
*Khalid M. Saqr*

Main category: cs.AI

TL;DR: 开发了一个确定性模拟框架，为评估AI生成的同行评审报告提供首个稳定、基于证据的标准，能够模拟校准的编辑判断并保持程序完整性。


<details>
  <summary>Details</summary>
Motivation: 学术出版生态系统面临提交量不可管理和AI不受监管的双重危机，需要新的治理模式来保障科学诚信。传统纯人工同行评审缺乏可扩展的客观基准。

Method: 采用确定性模拟框架，分析352份同行评审模拟报告，识别一致的系统状态指标。

Result: 系统能够模拟校准的编辑判断，'修订'决定在所有学科中始终占多数结果(>50%)，'拒绝'率动态适应领域规范；保持稳定的29%证据锚定合规率，在不同评审任务和科学领域中保持不变。

Conclusion: 该框架将AI重新定位为机构问责的重要组成部分，为维护学术交流信任提供关键基础设施。

Abstract: The scholarly publishing ecosystem faces a dual crisis of unmanageable
submission volumes and unregulated AI, creating an urgent need for new
governance models to safeguard scientific integrity. The traditional human-only
peer review regime lacks a scalable, objective benchmark, making editorial
processes opaque and difficult to audit. Here we investigate a deterministic
simulation framework that provides the first stable, evidence-based standard
for evaluating AI-generated peer review reports. Analyzing 352 peer-review
simulation reports, we identify consistent system state indicators that
demonstrate its reliability. First, the system is able to simulate calibrated
editorial judgment, with 'Revise' decisions consistently forming the majority
outcome (>50%) across all disciplines, while 'Reject' rates dynamically adapt
to field-specific norms, rising to 45% in Health Sciences. Second, it maintains
unwavering procedural integrity, enforcing a stable 29% evidence-anchoring
compliance rate that remains invariant across diverse review tasks and
scientific domains. These findings demonstrate a system that is predictably
rule-bound, mitigating the stochasticity of generative AI. For the scientific
community, this provides a transparent tool to ensure fairness; for publishing
strategists, it offers a scalable instrument for auditing workflows, managing
integrity risks, and implementing evidence-based governance. The framework
repositions AI as an essential component of institutional accountability,
providing the critical infrastructure to maintain trust in scholarly
communication.

</details>


### [76] [ReTabAD: A Benchmark for Restoring Semantic Context in Tabular Anomaly Detection](https://arxiv.org/abs/2510.02060)
*Sanghyu Yoon,Dongmin Kim,Suhee Yoon,Ye Seul Sim,Seungdong Yoa,Hye-Seung Cho,Soonyoung Lee,Hankook Lee,Woohyung Lim*

Main category: cs.AI

TL;DR: ReTabAD是一个为表格异常检测恢复文本语义的基准框架，提供20个带有文本元数据的表格数据集和多种检测算法，通过零样本LLM框架利用语义上下文提升检测性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有表格异常检测基准缺乏文本语义上下文（如特征描述和领域知识），限制了模型利用领域知识进行检测的能力，ReTabAD旨在解决这一局限性。

Method: 提供20个带有结构化文本元数据的表格数据集，实现包括经典、深度学习和LLM方法在内的最新异常检测算法，并开发零样本LLM框架利用语义上下文而无需任务特定训练。

Result: 实验结果显示语义上下文能提高检测性能，并通过支持领域感知推理增强可解释性。

Conclusion: ReTabAD为系统探索上下文感知的异常检测建立了基准，证明了文本元数据在异常检测中的重要价值。

Abstract: In tabular anomaly detection (AD), textual semantics often carry critical
signals, as the definition of an anomaly is closely tied to domain-specific
context. However, existing benchmarks provide only raw data points without
semantic context, overlooking rich textual metadata such as feature
descriptions and domain knowledge that experts rely on in practice. This
limitation restricts research flexibility and prevents models from fully
leveraging domain knowledge for detection. ReTabAD addresses this gap by
restoring textual semantics to enable context-aware tabular AD research. We
provide (1) 20 carefully curated tabular datasets enriched with structured
textual metadata, together with implementations of state-of-the-art AD
algorithms including classical, deep learning, and LLM-based approaches, and
(2) a zero-shot LLM framework that leverages semantic context without
task-specific training, establishing a strong baseline for future research.
Furthermore, this work provides insights into the role and utility of textual
metadata in AD through experiments and analysis. Results show that semantic
context improves detection performance and enhances interpretability by
supporting domain-aware reasoning. These findings establish ReTabAD as a
benchmark for systematic exploration of context-aware AD.

</details>


### [77] [Demystifying the Roles of LLM Layers in Retrieval, Knowledge, and Reasoning](https://arxiv.org/abs/2510.02091)
*Xinyuan Song,Keyu Wang,PengXiang Li,Lu Yin,Shiwei Liu*

Main category: cs.AI

TL;DR: 该论文系统研究了LLM中深度层的利用情况，发现深度层的作用高度异质化和情境依赖，浅层负责知识和检索，深层对推理至关重要，但可通过蒸馏重塑。


<details>
  <summary>Details</summary>
Motivation: 针对现有研究认为LLM深层对表示学习贡献不大且可被移除的观点，这些结论通常基于狭窄评估，可能忽略了模型行为的重要方面。

Method: 通过系统研究深度利用的多个维度，包括评估协议、任务类别和模型架构，分析不同深度层在不同设置下的贡献。

Result: 基于似然的非生成评估中，修剪大多数层可保持性能，只有前几层关键；生成评估发现中间和深层对推理和长程连贯性不可或缺；知识检索集中在浅层，推理精度严重依赖深层但可通过蒸馏重塑。

Conclusion: LLM中深度利用高度异质化和情境依赖，强调在解释和压缩大模型时需要任务、度量和模型感知的视角。

Abstract: Recent studies suggest that the deeper layers of Large Language Models (LLMs)
contribute little to representation learning and can often be removed without
significant performance loss. However, such claims are typically drawn from
narrow evaluations and may overlook important aspects of model behavior. In
this work, we present a systematic study of depth utilization across diverse
dimensions, including evaluation protocols, task categories, and model
architectures. Our analysis confirms that very deep layers are generally less
effective than earlier ones, but their contributions vary substantially with
the evaluation setting. Under likelihood-based metrics without generation,
pruning most layers preserves performance, with only the initial few being
critical. By contrast, generation-based evaluation uncovers indispensable roles
for middle and deeper layers in enabling reasoning and maintaining long-range
coherence. We further find that knowledge and retrieval are concentrated in
shallow components, whereas reasoning accuracy relies heavily on deeper layers
-- yet can be reshaped through distillation. These results highlight that depth
usage in LLMs is highly heterogeneous and context-dependent, underscoring the
need for task-, metric-, and model-aware perspectives in both interpreting and
compressing large models.

</details>


### [78] [Do AI Models Perform Human-like Abstract Reasoning Across Modalities?](https://arxiv.org/abs/2510.02125)
*Claas Beger,Ryan Yi,Shuhao Fu,Arseny Moskvichev,Sarah W. Tsai,Sivasankaran Rajamanickam,Melanie Mitchell*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: OpenAI's o3-preview reasoning model exceeded human accuracy on the ARC-AGI
benchmark, but does that mean state-of-the-art models recognize and reason with
the abstractions that the task creators intended? We investigate models'
abstraction abilities on ConceptARC. We evaluate models under settings that
vary the input modality (textual vs. visual), whether the model is permitted to
use external Python tools, and, for reasoning models, the amount of reasoning
effort. In addition to measuring output accuracy, we perform fine-grained
evaluation of the natural-language rules that models generate to explain their
solutions. This dual evaluation lets us assess whether models solve tasks using
the abstractions ConceptARC was designed to elicit, rather than relying on
surface-level patterns. Our results show that, while some models using
text-based representations match human output accuracy, the best models' rules
are often based on surface-level ``shortcuts'' and capture intended
abstractions far less often than humans. Thus their capabilities for general
abstract reasoning may be overestimated by evaluations based on accuracy alone.
In the visual modality, AI models' output accuracy drops sharply, yet our
rule-level analysis reveals that models might be underestimated, as they still
exhibit a substantial share of rules that capture intended abstractions, but
are often unable to correctly apply these rules. In short, our results show
that models still lag humans in abstract reasoning, and that using accuracy
alone to evaluate abstract reasoning on ARC-like tasks may overestimate
abstract-reasoning capabilities in textual modalities and underestimate it in
visual modalities. We believe that our evaluation framework offers a more
faithful picture of multimodal models' abstract reasoning abilities and a more
principled way to track progress toward human-like, abstraction-centered
intelligence.

</details>


### [79] [FlexDoc: Parameterized Sampling for Diverse Multilingual Synthetic Documents for Training Document Understanding Models](https://arxiv.org/abs/2510.02133)
*Karan Dua,Hitesh Laxmichand Patel,Puneet Mittal,Ranjeet Gupta,Amit Agarwal,Praneet Pabolu,Srikant Panda,Hansa Meghwani,Graham Horwood,Fahad Shah*

Main category: cs.AI

TL;DR: FlexDoc是一个可扩展的合成数据生成框架，通过随机模式和参数化采样生成多语言半结构化文档，显著降低文档理解模型的标注成本。


<details>
  <summary>Details</summary>
Motivation: 企业级文档理解模型需要大量多样化标注数据，但数据收集面临隐私限制、法律约束和手动标注成本高昂的问题，成本可达数百万美元。

Method: 结合随机模式和参数化采样，通过概率建模布局模式、视觉结构和内容变异性，实现可控的大规模文档变体生成。

Result: 在关键信息提取任务中，FlexDoc生成的数据将绝对F1分数提高了11%，同时相比传统硬模板方法减少了90%以上的标注工作量。

Conclusion: 该解决方案已在实际部署中加速了企业级文档理解模型的开发，同时显著降低了数据获取和标注成本。

Abstract: Developing document understanding models at enterprise scale requires large,
diverse, and well-annotated datasets spanning a wide range of document types.
However, collecting such data is prohibitively expensive due to privacy
constraints, legal restrictions, and the sheer volume of manual annotation
needed - costs that can scale into millions of dollars. We introduce FlexDoc, a
scalable synthetic data generation framework that combines Stochastic Schemas
and Parameterized Sampling to produce realistic, multilingual semi-structured
documents with rich annotations. By probabilistically modeling layout patterns,
visual structure, and content variability, FlexDoc enables the controlled
generation of diverse document variants at scale. Experiments on Key
Information Extraction (KIE) tasks demonstrate that FlexDoc-generated data
improves the absolute F1 Score by up to 11% when used to augment real datasets,
while reducing annotation effort by over 90% compared to traditional
hard-template methods. The solution is in active deployment, where it has
accelerated the development of enterprise-grade document understanding models
while significantly reducing data acquisition and annotation costs.

</details>


### [80] [A Rigorous Benchmark with Multidimensional Evaluation for Deep Research Agents: From Answers to Reports](https://arxiv.org/abs/2510.02190)
*Yang Yao,Yixu Wang,Yuxuan Zhang,Yi Lu,Tianle Gu,Lingyu Li,Dingyi Zhao,Keming Wu,Haozhe Wang,Ping Nie,Yan Teng,Yingchun Wang*

Main category: cs.AI

TL;DR: 该论文提出了一个专门针对深度研究代理(DRAs)的严格基准测试和多维评估框架，包含214个专家策划的挑战性查询和手动构建的参考包，用于全面评估DRAs在长格式报告生成方面的能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试在评估维度、响应格式和评分机制方面存在不足，无法有效评估从封闭语言模型向具有外部感知和信息整合能力的互联代理系统转变的新范式。

Method: 开发了一个包含214个专家策划查询的基准测试，分布在10个广泛主题领域，每个查询都配有手动构建的参考包。同时提出了一个多维评估框架，整合了语义质量、主题聚焦和检索可信度的评分指标。

Result: 广泛实验证实主流DRAs在性能上优于基于网络搜索工具增强的推理模型，但也显示出仍有相当大的改进空间。

Conclusion: 该研究为DRA系统的能力评估、架构改进和范式进步提供了坚实基础。

Abstract: Artificial intelligence is undergoing the paradigm shift from closed language
models to interconnected agent systems capable of external perception and
information integration. As a representative embodiment, Deep Research Agents
(DRAs) systematically exhibit the capabilities for task decomposition,
cross-source retrieval, multi-stage reasoning, and structured output, which
markedly enhance performance on complex and open-ended tasks. However, existing
benchmarks remain deficient in evaluation dimensions, response formatting, and
scoring mechanisms, limiting their capacity to assess such systems effectively.
This paper introduces a rigorous benchmark and a multidimensional evaluation
framework tailored to DRAs and report-style responses. The benchmark comprises
214 expert-curated challenging queries distributed across 10 broad thematic
domains, each accompanied by manually constructed reference bundles to support
composite evaluation. The framework enables comprehensive evaluation of
long-form reports generated by DRAs, incorporating integrated scoring metrics
for semantic quality, topical focus, and retrieval trustworthiness. Extensive
experimentation confirms the superior performance of mainstream DRAs over
web-search-tool-augmented reasoning models, yet reveals considerable scope for
further improvement. This study provides a robust foundation for capability
assessment, architectural refinement, and paradigm advancement in DRA systems.

</details>


### [81] [The Reasoning Boundary Paradox: How Reinforcement Learning Constrains Language Models](https://arxiv.org/abs/2510.02230)
*Phuc Minh Nguyen,Chinh D. La,Duy M. H. Nguyen,Nitesh V. Chawla,Binh T. Nguyen,Khoa D. Doan*

Main category: cs.AI

TL;DR: RLVR方法在提升大语言模型推理能力时反而会缩小推理边界，研究发现存在负干扰和赢家通吃现象，导致模型收敛到狭窄的解决方案策略。


<details>
  <summary>Details</summary>
Motivation: 研究RLVR方法为何会缩小推理边界而非扩展它，揭示其学习动态中的关键问题。

Method: 通过理论分析和实证研究，识别RLVR中的负干扰和赢家通吃现象，并提出针对低概率问题的数据筛选算法。

Result: 发现RLVR会降低Pass@k性能，通过专注于低概率问题的学习可以显著改善性能。

Conclusion: RLVR的失败源于其固有的策略采样机制，通过适当的数据筛选可以缓解这一问题并提升推理能力。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a key
method for improving Large Language Models' reasoning capabilities, yet recent
evidence suggests it may paradoxically shrink the reasoning boundary rather
than expand it. This paper investigates the shrinkage issue of RLVR by
analyzing its learning dynamics and reveals two critical phenomena that explain
this failure. First, we expose negative interference in RLVR, where learning to
solve certain training problems actively reduces the likelihood of correct
solutions for others, leading to the decline of Pass@$k$ performance, or the
probability of generating a correct solution within $k$ attempts. Second, we
uncover the winner-take-all phenomenon: RLVR disproportionately reinforces
problems with high likelihood, correct solutions, under the base model, while
suppressing other initially low-likelihood ones. Through extensive theoretical
and empirical analysis on multiple mathematical reasoning benchmarks, we show
that this effect arises from the inherent on-policy sampling in standard RL
objectives, causing the model to converge toward narrow solution strategies.
Based on these insights, we propose a simple yet effective data curation
algorithm that focuses RLVR learning on low-likelihood problems, achieving
notable improvement in Pass@$k$ performance. Our code is available at
https://github.com/mail-research/SELF-llm-interference.

</details>


### [82] [The Unreasonable Effectiveness of Scaling Agents for Computer Use](https://arxiv.org/abs/2510.02250)
*Gonzalo Gonzalez-Pumariega,Vincent Tu,Chih-Lun Lee,Jiachen Yang,Ang Li,Xin Eric Wang*

Main category: cs.AI

TL;DR: 提出了Behavior Best-of-N (bBoN)方法，通过生成多个执行轨迹并使用行为描述来选择最佳轨迹，显著提高了计算机使用代理在复杂任务中的成功率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 计算机使用代理(CUAs)在自动化日常数字任务方面具有潜力，但其不可靠性和高方差阻碍了在长视野复杂任务中的应用。

Method: bBoN方法通过生成多个代理执行轨迹，并使用描述代理行为轨迹的叙述来进行选择，实现广泛探索和原则性轨迹选择。

Result: 在OSWorld上达到69.9%的最新水平，显著超越先前方法，接近72%的人类水平性能；在WindowsAgentArena和AndroidWorld上表现出强大的泛化能力。

Conclusion: 当正确实施时，扩展CUAs具有惊人的有效性，有效扩展需要结构化的轨迹理解和选择，bBoN为实现这一目标提供了实用框架。

Abstract: Computer-use agents (CUAs) hold promise for automating everyday digital
tasks, but their unreliability and high variance hinder their application to
long-horizon, complex tasks. We introduce Behavior Best-of-N (bBoN), a method
that scales over agents by generating multiple rollouts and selecting among
them using behavior narratives that describe the agents' rollouts. It enables
both wide exploration and principled trajectory selection, substantially
improving robustness and success rates. On OSWorld, our bBoN scaling method
establishes a new state of the art (SoTA) at 69.9%, significantly outperforming
prior methods and approaching human-level performance at 72%, with
comprehensive ablations validating key design choices. We further demonstrate
strong generalization results to different operating systems on
WindowsAgentArena and AndroidWorld. Crucially, our results highlight the
unreasonable effectiveness of scaling CUAs, when you do it right: effective
scaling requires structured trajectory understanding and selection, and bBoN
provides a practical framework to achieve this.

</details>


### [83] [RLAD: Training LLMs to Discover Abstractions for Solving Reasoning Problems](https://arxiv.org/abs/2510.02263)
*Yuxiao Qu,Anikait Singh,Yoonho Lee,Amrith Setlur,Ruslan Salakhutdinov,Chelsea Finn,Aviral Kumar*

Main category: cs.AI

TL;DR: 提出RLAD方法，通过推理抽象来引导模型进行更有效的推理，使用两玩家RL训练范式联合训练抽象生成器和解决方案生成器。


<details>
  <summary>Details</summary>
Motivation: 现有推理模型往往无法持续捕获或重用程序，而是陷入冗长和退化的探索。需要更有效的方法来引导模型学习成功的推理过程。

Method: 引入推理抽象概念，训练模型能够为问题提出多个抽象描述，然后通过RL激励在构建解决方案时使用这些抽象信息。采用两玩家RL训练范式联合训练抽象生成器和解决方案生成器。

Result: 该方法实现了结构化探索，解耦了抽象提议和解决方案生成的学习信号，并提高了对更难问题的泛化能力。测试时分配更多计算资源生成抽象比生成更多解决方案更有利于性能提升。

Conclusion: 推理抽象能够引导有意义的探索，RLAD方法通过结构化训练范式有效提升了模型的推理能力。

Abstract: Reasoning requires going beyond pattern matching or memorization of solutions
to identify and implement "algorithmic procedures" that can be used to deduce
answers to hard problems. Doing so requires realizing the most relevant
primitives, intermediate results, or shared procedures, and building upon them.
While RL post-training on long chains of thought ultimately aims to uncover
this kind of algorithmic behavior, most reasoning traces learned by large
models fail to consistently capture or reuse procedures, instead drifting into
verbose and degenerate exploration. To address more effective reasoning, we
introduce reasoning abstractions: concise natural language descriptions of
procedural and factual knowledge that guide the model toward learning
successful reasoning. We train models to be capable of proposing multiple
abstractions given a problem, followed by RL that incentivizes building a
solution while using the information provided by these abstractions. This
results in a two-player RL training paradigm, abbreviated as RLAD, that jointly
trains an abstraction generator and a solution generator. This setup
effectively enables structured exploration, decouples learning signals of
abstraction proposal and solution generation, and improves generalization to
harder problems. We also show that allocating more test-time compute to
generating abstractions is more beneficial for performance than generating more
solutions at large test budgets, illustrating the role of abstractions in
guiding meaningful exploration.

</details>


### [84] [BioX-Bridge: Model Bridging for Unsupervised Cross-Modal Knowledge Transfer across Biosignals](https://arxiv.org/abs/2510.02276)
*Chenqi Li,Yu Liu,Timothy Denison,Tingting Zhu*

Main category: cs.AI

TL;DR: BioX-Bridge是一个用于生物信号跨模态知识迁移的轻量级框架，通过桥接网络对齐基础模型的中间表示，在减少88-99%可训练参数的同时保持或提升迁移性能。


<details>
  <summary>Details</summary>
Motivation: 生物信号模态间存在相关性，但缺乏大规模标注数据集限制了特定模态模型的训练。现有知识蒸馏方法计算开销大，特别是对于大型基础模型。

Method: 训练轻量级桥接网络来对齐基础模型的中间表示，引入高效的桥接位置选择策略和灵活的原型网络架构。

Result: 在多个生物信号模态、任务和数据集上的实验表明，BioX-Bridge显著减少了可训练参数(88-99%)，同时保持或优于现有方法的迁移性能。

Conclusion: BioX-Bridge提供了一种高效的无监督跨模态知识迁移解决方案，特别适用于大型基础模型，为生物信号健康监测系统提供了更好的可访问性和适应性。

Abstract: Biosignals offer valuable insights into the physiological states of the human
body. Although biosignal modalities differ in functionality, signal fidelity,
sensor comfort, and cost, they are often intercorrelated, reflecting the
holistic and interconnected nature of human physiology. This opens up the
possibility of performing the same tasks using alternative biosignal
modalities, thereby improving the accessibility, usability, and adaptability of
health monitoring systems. However, the limited availability of large labeled
datasets presents challenges for training models tailored to specific tasks and
modalities of interest. Unsupervised cross-modal knowledge transfer offers a
promising solution by leveraging knowledge from an existing modality to support
model training for a new modality. Existing methods are typically based on
knowledge distillation, which requires running a teacher model alongside
student model training, resulting in high computational and memory overhead.
This challenge is further exacerbated by the recent development of foundation
models that demonstrate superior performance and generalization across tasks at
the cost of large model sizes. To this end, we explore a new framework for
unsupervised cross-modal knowledge transfer of biosignals by training a
lightweight bridge network to align the intermediate representations and enable
information flow between foundation models and across modalities. Specifically,
we introduce an efficient strategy for selecting alignment positions where the
bridge should be constructed, along with a flexible prototype network as the
bridge architecture. Extensive experiments across multiple biosignal
modalities, tasks, and datasets show that BioX-Bridge reduces the number of
trainable parameters by 88--99\% while maintaining or even improving transfer
performance compared to state-of-the-art methods.

</details>
