{"id": "2508.10059", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.10059", "abs": "https://arxiv.org/abs/2508.10059", "authors": ["Yueke Zhang", "Yifan Zhang", "Kevin Leach", "Yu Huang"], "title": "FormalGrad: Integrating Formal Methods with Gradient-Based LLM Refinement", "comment": "6 Pages", "summary": "While Large Language Models (LLMs) have demonstrated remarkable capabilities\nin code generation, they often produce solutions that lack guarantees of\ncorrectness, robustness, and efficiency. The limitation is acute in domains\nrequiring strict constraints. FormalGrad introduces a principled framework that\nintegrates formal methods directly into an iterative LLM-based generation loop.\nIt uniquely treats code as a differentiable variable, converting structured\nfeedback and formal constraints into a textual pseudo-gradient. This gradient\nguides the model to iteratively refine solutions, ensuring they are not only\nfunctional but also robust and formally justified. We evaluate FormalGrad on\nthe HumanEval, HumanEval+, and LiveCodeBench benchmarks. Our implementation\noutperforms strong baselines, achieving an absolute improvement of up to 27% on\nHumanEval and a 41% relative improvement on the challenging LiveCodeBench V6.\nFormalGrad generates formally justified code that is robust and efficient,\npaving the way for reliable AI-assisted software development in high-stakes\napplications.", "AI": {"tldr": "FormalGrad\u901a\u8fc7\u5c06\u5f62\u5f0f\u5316\u65b9\u6cd5\u96c6\u6210\u5230LLM\u4ee3\u7801\u751f\u6210\u5faa\u73af\u4e2d\uff0c\u5229\u7528\u4f2a\u68af\u5ea6\u6307\u5bfc\u6a21\u578b\u8fed\u4ee3\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4ee3\u7801\u7684\u6b63\u786e\u6027\u3001\u9c81\u68d2\u6027\u548c\u6548\u7387\u3002", "motivation": "LLM\u751f\u6210\u7684\u4ee3\u7801\u5e38\u7f3a\u4e4f\u6b63\u786e\u6027\u3001\u9c81\u68d2\u6027\u548c\u6548\u7387\u7684\u4fdd\u8bc1\uff0c\u5c24\u5176\u5728\u4e25\u683c\u7ea6\u675f\u7684\u9886\u57df\u3002FormalGrad\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5c06\u4ee3\u7801\u89c6\u4e3a\u53ef\u5fae\u5206\u53d8\u91cf\uff0c\u5c06\u5f62\u5f0f\u5316\u7ea6\u675f\u8f6c\u5316\u4e3a\u6587\u672c\u4f2a\u68af\u5ea6\uff0c\u6307\u5bfcLLM\u8fed\u4ee3\u4f18\u5316\u4ee3\u7801\u3002", "result": "\u5728HumanEval\u3001HumanEval+\u548cLiveCodeBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u7edd\u5bf9\u63d0\u5347\u8fbe27%\uff0c\u76f8\u5bf9\u63d0\u5347\u8fbe41%\u3002", "conclusion": "FormalGrad\u4e3a\u9ad8\u98ce\u9669\u5e94\u7528\u4e2d\u7684\u53ef\u9760AI\u8f85\u52a9\u8f6f\u4ef6\u5f00\u53d1\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2508.10068", "categories": ["cs.SE", "cs.CL", "cs.IR", "cs.PL"], "pdf": "https://arxiv.org/pdf/2508.10068", "abs": "https://arxiv.org/abs/2508.10068", "authors": ["Xiaohan Chen", "Zhongying Pan", "Quan Feng", "Yu Tian", "Shuqun Yang", "Mengru Wang", "Lina Gong", "Yuxia Geng", "Piji Li", "Xiang Chen"], "title": "SaraCoder: Orchestrating Semantic and Structural Cues for Profit-Oriented Repository-Level Code Completion", "comment": null, "summary": "Retrieval-augmented generation (RAG) for repository-level code completion\ncommonly relies on superficial text similarity, leading to results plagued by\nsemantic misguidance, redundancy, and homogeneity, while also failing to\nresolve external symbol ambiguity. To address these challenges, we introduce\nSaracoder, a Hierarchical Feature-Optimized retrieval framework. Its core\nHierarchical Feature Optimization module systematically refines candidates by\ndistilling deep semantic relationships, pruning exact duplicates, assessing\nstructural similarity with a novel graph-based metric that weighs edits by\ntheir topological importance, and reranking results to maximize both relevance\nand diversity. Furthermore, an External-Aware Identifier Disambiguator module\naccurately resolves cross-file symbol ambiguity via dependency analysis.\nExtensive experiments on the challenging CrossCodeEval and RepoEval-Updated\nbenchmarks demonstrate that Saracoder significantly outperforms existing\nbaselines across multiple programming languages and models. Our work proves\nthat systematically refining retrieval results across multiple dimensions\nprovides a new paradigm for building more accurate and robust repository-level\ncode completion systems.", "AI": {"tldr": "Saracoder\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u7279\u5f81\u4f18\u5316\u7684\u68c0\u7d22\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfRAG\u5728\u4ee3\u7801\u8865\u5168\u4e2d\u7684\u8bed\u4e49\u8bef\u5bfc\u3001\u5197\u4f59\u548c\u540c\u8d28\u6027\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u4f9d\u8d56\u5206\u6790\u89e3\u51b3\u4e86\u5916\u90e8\u7b26\u53f7\u6b67\u4e49\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u6587\u672c\u76f8\u4f3c\u6027\u7684\u68c0\u7d22\u65b9\u6cd5\u5728\u4ee3\u7801\u8865\u5168\u4e2d\u5b58\u5728\u8bed\u4e49\u8bef\u5bfc\u3001\u5197\u4f59\u548c\u540c\u8d28\u6027\u95ee\u9898\uff0c\u4e14\u65e0\u6cd5\u89e3\u51b3\u5916\u90e8\u7b26\u53f7\u6b67\u4e49\uff0cSaracoder\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u91c7\u7528\u5206\u5c42\u7279\u5f81\u4f18\u5316\u6a21\u5757\uff0c\u901a\u8fc7\u6df1\u5ea6\u8bed\u4e49\u5173\u7cfb\u63d0\u70bc\u3001\u53bb\u91cd\u3001\u57fa\u4e8e\u56fe\u7684\u7ed3\u6784\u76f8\u4f3c\u6027\u8bc4\u4f30\u548c\u7ed3\u679c\u91cd\u6392\uff0c\u4ee5\u53ca\u5916\u90e8\u611f\u77e5\u6807\u8bc6\u7b26\u6d88\u6b67\u6a21\u5757\u3002", "result": "\u5728CrossCodeEval\u548cRepoEval-Updated\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSaracoder\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "conclusion": "\u901a\u8fc7\u591a\u7ef4\u5ea6\u7cfb\u7edf\u4f18\u5316\u68c0\u7d22\u7ed3\u679c\uff0cSaracoder\u4e3a\u6784\u5efa\u66f4\u51c6\u786e\u548c\u5065\u58ee\u7684\u4ee3\u7801\u8865\u5168\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2508.10074", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.10074", "abs": "https://arxiv.org/abs/2508.10074", "authors": ["Ruofan Lu", "Yintong Huo", "Meng Zhang", "Yichen Li", "Michael R. Lyu"], "title": "Next Edit Prediction: Learning to Predict Code Edits from Context and Interaction History", "comment": null, "summary": "The rapid advancement of large language models (LLMs) has led to the\nwidespread adoption of AI-powered coding assistants integrated into a\ndevelopment environment. On one hand, low-latency code completion offers\ncompletion suggestions but is fundamentally constrained to the cursor's current\nposition. On the other hand, chat-based editing can perform complex\nmodifications, yet forces developers to stop their work, describe the intent in\nnatural language, which causes a context-switch away from the code. This\ncreates a suboptimal user experience, as neither paradigm proactively predicts\nthe developer's next edit in a sequence of related edits. To bridge this gap\nand provide the seamless code edit suggestion, we introduce the task of Next\nEdit Prediction, a novel task designed to infer developer intent from recent\ninteraction history to predict both the location and content of the subsequent\nedit. Specifically, we curate a high-quality supervised fine-tuning dataset and\nan evaluation benchmark for the Next Edit Prediction task. Then, we conduct\nsupervised fine-tuning on a series of models and performed a comprehensive\nevaluation of both the fine-tuned models and other baseline models, yielding\nseveral novel findings. This work lays the foundation for a new interaction\nparadigm that proactively collaborate with developers by anticipating their\nfollowing action, rather than merely reacting to explicit instructions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201cNext Edit Prediction\u201d\u7684\u65b0\u4efb\u52a1\uff0c\u65e8\u5728\u901a\u8fc7\u5f00\u53d1\u8005\u6700\u8fd1\u7684\u4ea4\u4e92\u5386\u53f2\u63a8\u65ad\u5176\u610f\u56fe\uff0c\u9884\u6d4b\u4e0b\u4e00\u4e2a\u7f16\u8f91\u7684\u4f4d\u7f6e\u548c\u5185\u5bb9\uff0c\u4ee5\u4f18\u5316AI\u4ee3\u7801\u52a9\u624b\u7684\u4f7f\u7528\u4f53\u9a8c\u3002", "motivation": "\u73b0\u6709AI\u4ee3\u7801\u52a9\u624b\u5b58\u5728\u5c40\u9650\u6027\uff1a\u4f4e\u5ef6\u8fdf\u4ee3\u7801\u8865\u5168\u4ec5\u9002\u7528\u4e8e\u5149\u6807\u5f53\u524d\u4f4d\u7f6e\uff0c\u800c\u57fa\u4e8e\u804a\u5929\u7684\u7f16\u8f91\u9700\u8981\u5f00\u53d1\u8005\u4e2d\u65ad\u5de5\u4f5c\u5e76\u63cf\u8ff0\u610f\u56fe\u3002\u8fd9\u4e24\u79cd\u65b9\u5f0f\u5747\u672a\u80fd\u4e3b\u52a8\u9884\u6d4b\u5f00\u53d1\u8005\u7684\u4e00\u7cfb\u5217\u76f8\u5173\u7f16\u8f91\uff0c\u5bfc\u81f4\u7528\u6237\u4f53\u9a8c\u4e0d\u4f73\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e86\u4e00\u4e2a\u9ad8\u8d28\u91cf\u7684\u76d1\u7763\u5fae\u8c03\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u57fa\u51c6\uff0c\u5bf9\u4e00\u7cfb\u5217\u6a21\u578b\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\uff0c\u5e76\u4e0e\u5176\u4ed6\u57fa\u7ebf\u6a21\u578b\u8fdb\u884c\u4e86\u5168\u9762\u8bc4\u4f30\u3002", "result": "\u7814\u7a76\u5f97\u51fa\u4e86\u4e00\u4e9b\u65b0\u53d1\u73b0\uff0c\u4e3a\u4e00\u79cd\u65b0\u7684\u4ea4\u4e92\u8303\u5f0f\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u8be5\u8303\u5f0f\u80fd\u4e3b\u52a8\u9884\u6d4b\u5f00\u53d1\u8005\u7684\u4e0b\u4e00\u6b65\u884c\u52a8\uff0c\u800c\u975e\u4ec5\u54cd\u5e94\u663e\u5f0f\u6307\u4ee4\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3aAI\u4ee3\u7801\u52a9\u624b\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u65e0\u7f1d\u7684\u534f\u4f5c\u65b9\u5f0f\uff0c\u901a\u8fc7\u9884\u6d4b\u5f00\u53d1\u8005\u7684\u7f16\u8f91\u610f\u56fe\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5f00\u53d1\u6548\u7387\u548c\u4f7f\u7528\u4f53\u9a8c\u3002"}}
{"id": "2508.10157", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.10157", "abs": "https://arxiv.org/abs/2508.10157", "authors": ["Ajibode Adekunle", "Abdul Ali Bangash", "Bram Adams", "Ahmed E. Hassan"], "title": "On the synchronization between Hugging Face pre-trained language models and their upstream GitHub repository", "comment": null, "summary": "Pretrained language models (PTLMs) have advanced natural language processing\n(NLP), enabling progress in tasks like text generation and translation. Like\nsoftware package management, PTLMs are trained using code and environment\nscripts in upstream repositories (e.g., GitHub, GH) and distributed as variants\nvia downstream platforms like Hugging Face (HF). Coordinating development\nbetween GH and HF poses challenges such as misaligned release timelines,\ninconsistent versioning, and limited reuse of PTLM variants. We conducted a\nmixed-method study of 325 PTLM families (904 HF variants) to examine how commit\nactivities are coordinated. Our analysis reveals that GH contributors typically\nmake changes related to specifying the version of the model, improving code\nquality, performance optimization, and dependency management within the\ntraining scripts, while HF contributors make changes related to improving model\ndescriptions, data set handling, and setup required for model inference.\nFurthermore, to understand the synchronization aspects of commit activities\nbetween GH and HF, we examined three dimensions of these activities -- lag\n(delay), type of synchronization, and intensity -- which together yielded eight\ndistinct synchronization patterns. The prevalence of partially synchronized\npatterns, such as Disperse synchronization and Sparse synchronization, reveals\nstructural disconnects in current cross-platform release practices. These\npatterns often result in isolated changes -- where improvements or fixes made\non one platform are never replicated on the other -- and in some cases,\nindicate an abandonment of one repository in favor of the other. Such\nfragmentation risks exposing end users to incomplete, outdated, or behaviorally\ninconsistent models. Hence, recognizing these synchronization patterns is\ncritical for improving oversight and traceability in PTLM release workflows.", "AI": {"tldr": "\u7814\u7a76\u5206\u6790\u4e86\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff08PTLM\uff09\u5728GitHub\u548cHugging Face\u5e73\u53f0\u4e0a\u7684\u5f00\u53d1\u534f\u8c03\u95ee\u9898\uff0c\u63ed\u793a\u4e86\u516b\u79cd\u540c\u6b65\u6a21\u5f0f\uff0c\u6307\u51fa\u8de8\u5e73\u53f0\u53d1\u5e03\u5b9e\u8df5\u4e2d\u7684\u7ed3\u6784\u6027\u95ee\u9898\u3002", "motivation": "PTLM\u7684\u5f00\u53d1\u6d89\u53ca\u4e0a\u6e38\uff08GitHub\uff09\u548c\u4e0b\u6e38\uff08Hugging Face\uff09\u5e73\u53f0\uff0c\u4f46\u4e24\u8005\u95f4\u7684\u534f\u8c03\u95ee\u9898\uff08\u5982\u7248\u672c\u4e0d\u4e00\u81f4\u3001\u53d8\u66f4\u672a\u540c\u6b65\uff09\u53ef\u80fd\u5bfc\u81f4\u6a21\u578b\u4e0d\u5b8c\u6574\u6216\u8fc7\u65f6\u3002", "method": "\u91c7\u7528\u6df7\u5408\u65b9\u6cd5\u7814\u7a76\u4e86325\u4e2aPTLM\u5bb6\u65cf\uff08904\u4e2aHugging Face\u53d8\u4f53\uff09\uff0c\u5206\u6790\u4e86\u63d0\u4ea4\u6d3b\u52a8\u7684\u534f\u8c03\u60c5\u51b5\uff0c\u5305\u62ec\u5ef6\u8fdf\u3001\u540c\u6b65\u7c7b\u578b\u548c\u5f3a\u5ea6\u4e09\u4e2a\u7ef4\u5ea6\u3002", "result": "\u53d1\u73b0GitHub\u8d21\u732e\u8005\u5173\u6ce8\u4ee3\u7801\u4f18\u5316\u548c\u7248\u672c\u7ba1\u7406\uff0c\u800cHugging Face\u8d21\u732e\u8005\u5173\u6ce8\u6a21\u578b\u63cf\u8ff0\u548c\u63a8\u7406\u8bbe\u7f6e\uff1b\u63ed\u793a\u4e86\u516b\u79cd\u540c\u6b65\u6a21\u5f0f\uff0c\u90e8\u5206\u540c\u6b65\u6a21\u5f0f\u5bfc\u81f4\u53d8\u66f4\u5b64\u7acb\u6216\u4ed3\u5e93\u5e9f\u5f03\u3002", "conclusion": "\u8bc6\u522b\u540c\u6b65\u6a21\u5f0f\u5bf9\u6539\u8fdbPTLM\u53d1\u5e03\u6d41\u7a0b\u7684\u76d1\u7763\u548c\u53ef\u8ffd\u6eaf\u6027\u81f3\u5173\u91cd\u8981\uff0c\u4ee5\u907f\u514d\u7528\u6237\u63a5\u89e6\u4e0d\u5b8c\u6574\u6216\u884c\u4e3a\u4e0d\u4e00\u81f4\u7684\u6a21\u578b\u3002"}}
{"id": "2508.10017", "categories": ["cs.CR", "cs.AI", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.10017", "abs": "https://arxiv.org/abs/2508.10017", "authors": ["Rodrigo Tertulino"], "title": "A Robust Pipeline for Differentially Private Federated Learning on Imbalanced Clinical Data using SMOTETomek and FedProx", "comment": "This is being prepared to be submitted to the Journal of the\n  Brazilian Computer Society (JBCS), which is still under construction", "summary": "Federated Learning (FL) presents a groundbreaking approach for collaborative\nhealth research, allowing model training on decentralized data while\nsafeguarding patient privacy. FL offers formal security guarantees when\ncombined with Differential Privacy (DP). The integration of these technologies,\nhowever, introduces a significant trade-off between privacy and clinical\nutility, a challenge further complicated by the severe class imbalance often\npresent in medical datasets. The research presented herein addresses these\ninterconnected issues through a systematic, multi-stage analysis. An FL\nframework was implemented for cardiovascular risk prediction, where initial\nexperiments showed that standard methods struggled with imbalanced data,\nresulting in a recall of zero. To overcome such a limitation, we first\nintegrated the hybrid Synthetic Minority Over-sampling Technique with Tomek\nLinks (SMOTETomek) at the client level, successfully developing a clinically\nuseful model. Subsequently, the framework was optimized for non-IID data using\na tuned FedProx algorithm. Our final results reveal a clear, non-linear\ntrade-off between the privacy budget (epsilon) and model recall, with the\noptimized FedProx consistently out-performing standard FedAvg. An optimal\noperational region was identified on the privacy-utility frontier, where strong\nprivacy guarantees (with epsilon 9.0) can be achieved while maintaining high\nclinical utility (recall greater than 77%). Ultimately, our study provides a\npractical methodological blueprint for creating effective, secure, and accurate\ndiagnostic tools that can be applied to real-world, heterogeneous healthcare\ndata.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u548c\u5dee\u5206\u9690\u79c1\uff08DP\uff09\u7684\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u533b\u7597\u6570\u636e\u4e2d\u9690\u79c1\u4e0e\u4e34\u5e8a\u6548\u7528\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u4f18\u5316\u7b97\u6cd5\u548c\u6570\u636e\u5904\u7406\u6280\u672f\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u533b\u7597\u6570\u636e\u901a\u5e38\u5206\u6563\u4e14\u9690\u79c1\u654f\u611f\uff0c\u8054\u90a6\u5b66\u4e60\u53ef\u4ee5\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\u8fdb\u884c\u534f\u4f5c\u7814\u7a76\uff0c\u4f46\u9762\u4e34\u6570\u636e\u4e0d\u5e73\u8861\u548c\u9690\u79c1-\u6548\u7528\u6743\u8861\u7684\u6311\u6218\u3002", "method": "\u7814\u7a76\u91c7\u7528\u591a\u9636\u6bb5\u5206\u6790\uff0c\u9996\u5148\u5728\u5ba2\u6237\u7aef\u96c6\u6210SMOTETomek\u6280\u672f\u5904\u7406\u6570\u636e\u4e0d\u5e73\u8861\uff0c\u968f\u540e\u4f18\u5316FedProx\u7b97\u6cd5\u4ee5\u9002\u5e94\u975e\u72ec\u7acb\u540c\u5206\u5e03\uff08non-IID\uff09\u6570\u636e\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u4f18\u5316\u540e\u7684FedProx\u5728\u9690\u79c1\u9884\u7b97\uff08epsilon\uff09\u4e0e\u53ec\u56de\u7387\u4e4b\u95f4\u5b58\u5728\u975e\u7ebf\u6027\u6743\u8861\uff0c\u5e76\u5728epsilon\u4e3a9.0\u65f6\u5b9e\u73b0\u4e86\u9ad8\u53ec\u56de\u7387\uff08>77%\uff09\u548c\u5f3a\u9690\u79c1\u4fdd\u969c\u3002", "conclusion": "\u7814\u7a76\u4e3a\u5f00\u53d1\u5b89\u5168\u3001\u6709\u6548\u7684\u533b\u7597\u8bca\u65ad\u5de5\u5177\u63d0\u4f9b\u4e86\u5b9e\u7528\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u73b0\u5b9e\u4e16\u754c\u4e2d\u5f02\u6784\u7684\u533b\u7597\u6570\u636e\u3002"}}
{"id": "2508.10047", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10047", "abs": "https://arxiv.org/abs/2508.10047", "authors": ["Ziyang Xiao", "Jingrong Xie", "Lilin Xu", "Shisi Guan", "Jingyan Zhu", "Xiongwei Han", "Xiaojin Fu", "WingYin Yu", "Han Wu", "Wei Shi", "Qingcan Kang", "Jiahui Duan", "Tao Zhong", "Mingxuan Yuan", "Jia Zeng", "Yuan Wang", "Gang Chen", "Dongxiang Zhang"], "title": "A Survey of Optimization Modeling Meets LLMs: Progress and Future Directions", "comment": null, "summary": "By virtue of its great utility in solving real-world problems, optimization\nmodeling has been widely employed for optimal decision-making across various\nsectors, but it requires substantial expertise from operations research\nprofessionals. With the advent of large language models (LLMs), new\nopportunities have emerged to automate the procedure of mathematical modeling.\nThis survey presents a comprehensive and timely review of recent advancements\nthat cover the entire technical stack, including data synthesis and fine-tuning\nfor the base model, inference frameworks, benchmark datasets, and performance\nevaluation. In addition, we conducted an in-depth analysis on the quality of\nbenchmark datasets, which was found to have a surprisingly high error rate. We\ncleaned the datasets and constructed a new leaderboard with fair performance\nevaluation in terms of base LLM model and datasets. We also build an online\nportal that integrates resources of cleaned datasets, code and paper repository\nto benefit the community. Finally, we identify limitations in current\nmethodologies and outline future research opportunities.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u81ea\u52a8\u5316\u6570\u5b66\u5efa\u6a21\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u5305\u62ec\u6570\u636e\u5408\u6210\u3001\u6a21\u578b\u5fae\u8c03\u3001\u63a8\u7406\u6846\u67b6\u3001\u57fa\u51c6\u6570\u636e\u96c6\u548c\u6027\u80fd\u8bc4\u4f30\uff0c\u5e76\u6784\u5efa\u4e86\u65b0\u7684\u516c\u5e73\u8bc4\u4f30\u6392\u884c\u699c\u548c\u5728\u7ebf\u8d44\u6e90\u95e8\u6237\u3002", "motivation": "\u4f18\u5316\u5efa\u6a21\u5728\u89e3\u51b3\u5b9e\u9645\u95ee\u9898\u4e2d\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u9700\u8981\u5927\u91cf\u4e13\u4e1a\u77e5\u8bc6\u3002LLMs\u7684\u51fa\u73b0\u4e3a\u81ea\u52a8\u5316\u6570\u5b66\u5efa\u6a21\u63d0\u4f9b\u4e86\u65b0\u673a\u4f1a\u3002", "method": "\u7efc\u8ff0\u4e86\u6280\u672f\u6808\u7684\u5404\u4e2a\u65b9\u9762\uff0c\u5305\u62ec\u6570\u636e\u5408\u6210\u3001\u6a21\u578b\u5fae\u8c03\u3001\u63a8\u7406\u6846\u67b6\u7b49\uff0c\u5e76\u5206\u6790\u4e86\u57fa\u51c6\u6570\u636e\u96c6\u7684\u8d28\u91cf\uff0c\u6e05\u7406\u6570\u636e\u5e76\u6784\u5efa\u65b0\u7684\u6392\u884c\u699c\u3002", "result": "\u53d1\u73b0\u57fa\u51c6\u6570\u636e\u96c6\u9519\u8bef\u7387\u9ad8\uff0c\u6e05\u7406\u540e\u6784\u5efa\u4e86\u65b0\u7684\u516c\u5e73\u8bc4\u4f30\u6392\u884c\u699c\u548c\u5728\u7ebf\u8d44\u6e90\u95e8\u6237\u3002", "conclusion": "\u6307\u51fa\u4e86\u5f53\u524d\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2508.10517", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.10517", "abs": "https://arxiv.org/abs/2508.10517", "authors": ["Likai Ye", "Mengliang Li", "Dehai Zhao", "Jiamou Sun", "Xiaoxue Ren"], "title": "Bridging Solidity Evolution Gaps: An LLM-Enhanced Approach for Smart Contract Compilation Error Resolution", "comment": "International Conference on Software Maintenance and Evolution\n  (ICSME) 2025", "summary": "Solidity, the dominant smart contract language for Ethereum, has rapidly\nevolved with frequent version updates to enhance security, functionality, and\ndeveloper experience. However, these continual changes introduce significant\nchallenges, particularly in compilation errors, code migration, and\nmaintenance. Therefore, we conduct an empirical study to investigate the\nchallenges in the Solidity version evolution and reveal that 81.68% of examined\ncontracts encounter errors when compiled across different versions, with 86.92%\nof compilation errors.\n  To mitigate these challenges, we conducted a systematic evaluation of large\nlanguage models (LLMs) for resolving Solidity compilation errors during version\nmigrations. Our empirical analysis across both open-source (LLaMA3, DeepSeek)\nand closed-source (GPT-4o, GPT-3.5-turbo) LLMs reveals that although these\nmodels exhibit error repair capabilities, their effectiveness diminishes\nsignificantly for semantic-level issues and shows strong dependency on prompt\nengineering strategies. This underscores the critical need for domain-specific\nadaptation in developing reliable LLM-based repair systems for smart contracts.\n  Building upon these insights, we introduce SMCFIXER, a novel framework that\nsystematically integrates expert knowledge retrieval with LLM-based repair\nmechanisms for Solidity compilation error resolution. The architecture\ncomprises three core phases: (1) context-aware code slicing that extracts\nrelevant error information; (2) expert knowledge retrieval from official\ndocumentation; and (3) iterative patch generation for Solidity migration.\nExperimental validation across Solidity version migrations demonstrates our\napproach's statistically significant 24.24% improvement over baseline GPT-4o on\nreal-world datasets, achieving near-perfect 96.97% accuracy.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86Solidity\u7248\u672c\u66f4\u65b0\u5e26\u6765\u7684\u7f16\u8bd1\u9519\u8bef\u95ee\u9898\uff0c\u5e76\u63d0\u51faSMCFIXER\u6846\u67b6\u7ed3\u5408LLMs\u548c\u4e13\u5bb6\u77e5\u8bc6\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "motivation": "Solidity\u9891\u7e41\u7248\u672c\u66f4\u65b0\u5bfc\u81f4\u7f16\u8bd1\u9519\u8bef\u548c\u8fc1\u79fb\u56f0\u96be\uff0c\u5f71\u54cd\u667a\u80fd\u5408\u7ea6\u5f00\u53d1\u548c\u7ef4\u62a4\u3002", "method": "\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\u5206\u6790\u7f16\u8bd1\u9519\u8bef\uff0c\u8bc4\u4f30LLMs\u4fee\u590d\u80fd\u529b\uff0c\u5e76\u63d0\u51faSMCFIXER\u6846\u67b6\u6574\u5408\u4e13\u5bb6\u77e5\u8bc6\u548cLLMs\u3002", "result": "81.68%\u5408\u7ea6\u8de8\u7248\u672c\u7f16\u8bd1\u51fa\u9519\uff0cLLMs\u4fee\u590d\u80fd\u529b\u6709\u9650\uff1bSMCFIXER\u6bd4GPT-4o\u63d0\u534724.24%\uff0c\u51c6\u786e\u7387\u8fbe96.97%\u3002", "conclusion": "SMCFIXER\u663e\u8457\u63d0\u5347Solidity\u7f16\u8bd1\u9519\u8bef\u4fee\u590d\u6548\u679c\uff0c\u5f3a\u8c03\u9886\u57df\u77e5\u8bc6\u5bf9LLMs\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2508.10023", "categories": ["cs.CR", "quant-ph", "94A60 (Cryptography)"], "pdf": "https://arxiv.org/pdf/2508.10023", "abs": "https://arxiv.org/abs/2508.10023", "authors": ["Samet \u00dcnsal"], "title": "A Comparative Performance Evaluation of Kyber, sntrup761, and FrodoKEM for Post-Quantum Cryptography", "comment": "12 pages, 3 tables, IEEE conference format", "summary": "Post-quantum cryptography (PQC) aims to develop cryptographic algorithms that\nare secure against attacks from quantum computers. This paper compares the\nleading postquantum cryptographic algorithms, such as Kyber, sntrup761, and\nFrodoKEM, in terms of their security, performance, and real-world\napplicability. The review highlights the strengths and weaknesses of each\nalgorithm and provides insights into future research directions. We also\ndiscuss the challenges of transitioning from classical to post-quantum systems\nand the potential impacts on various industries. This paper serves as a\nfoundation for understanding the current state of post-quantum cryptography and\nits future prospects in the quantum computing era.", "AI": {"tldr": "\u672c\u6587\u6bd4\u8f83\u4e86\u540e\u91cf\u5b50\u5bc6\u7801\u5b66\uff08PQC\uff09\u4e2d\u7684\u4e3b\u8981\u7b97\u6cd5\uff08\u5982Kyber\u3001sntrup761\u548cFrodoKEM\uff09\uff0c\u5206\u6790\u4e86\u5176\u5b89\u5168\u6027\u3001\u6027\u80fd\u548c\u5b9e\u9645\u9002\u7528\u6027\uff0c\u5e76\u63a2\u8ba8\u4e86\u4ece\u7ecf\u5178\u7cfb\u7edf\u8fc7\u6e21\u5230\u540e\u91cf\u5b50\u7cfb\u7edf\u7684\u6311\u6218\u53ca\u5176\u5bf9\u5404\u884c\u4e1a\u7684\u5f71\u54cd\u3002", "motivation": "\u91cf\u5b50\u8ba1\u7b97\u673a\u7684\u53d1\u5c55\u5bf9\u73b0\u6709\u5bc6\u7801\u7cfb\u7edf\u6784\u6210\u5a01\u80c1\uff0c\u56e0\u6b64\u9700\u8981\u7814\u7a76\u80fd\u591f\u62b5\u5fa1\u91cf\u5b50\u653b\u51fb\u7684\u540e\u91cf\u5b50\u5bc6\u7801\u7b97\u6cd5\u3002", "method": "\u901a\u8fc7\u6bd4\u8f83\u5206\u6790Kyber\u3001sntrup761\u548cFrodoKEM\u7b49\u7b97\u6cd5\u7684\u5b89\u5168\u6027\u3001\u6027\u80fd\u548c\u9002\u7528\u6027\uff0c\u8bc4\u4f30\u5176\u4f18\u7f3a\u70b9\u3002", "result": "\u603b\u7ed3\u4e86\u5404\u7b97\u6cd5\u7684\u4f18\u7f3a\u70b9\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "\u672c\u6587\u4e3a\u7406\u89e3\u540e\u91cf\u5b50\u5bc6\u7801\u5b66\u7684\u73b0\u72b6\u53ca\u5176\u5728\u91cf\u5b50\u8ba1\u7b97\u65f6\u4ee3\u7684\u672a\u6765\u524d\u666f\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.10108", "categories": ["cs.AI", "cs.CL", "I.2.7; I.2.6; E.0"], "pdf": "https://arxiv.org/pdf/2508.10108", "abs": "https://arxiv.org/abs/2508.10108", "authors": ["Sattvik Sahai", "Prasoon Goyal", "Michael Johnston", "Anna Gottardi", "Yao Lu", "Lucy Hu", "Luke Dai", "Shaohua Liu", "Samyuth Sagi", "Hangjie Shi", "Desheng Zhang", "Lavina Vaz", "Leslie Ball", "Maureen Murray", "Rahul Gupta", "Shankar Ananthakrishna"], "title": "Amazon Nova AI Challenge -- Trusted AI: Advancing secure, AI-assisted software development", "comment": "18 pages, 1st Proceedings of Amazon Nova AI Challenge (Trusted AI\n  2025)", "summary": "AI systems for software development are rapidly gaining prominence, yet\nsignificant challenges remain in ensuring their safety. To address this, Amazon\nlaunched the Trusted AI track of the Amazon Nova AI Challenge, a global\ncompetition among 10 university teams to drive advances in secure AI. In the\nchallenge, five teams focus on developing automated red teaming bots, while the\nother five create safe AI assistants. This challenge provides teams with a\nunique platform to evaluate automated red-teaming and safety alignment methods\nthrough head-to-head adversarial tournaments where red teams have multi-turn\nconversations with the competing AI coding assistants to test their safety\nalignment. Along with this, the challenge provides teams with a feed of high\nquality annotated data to fuel iterative improvement. Throughout the challenge,\nteams developed state-of-the-art techniques, introducing novel approaches in\nreasoning-based safety alignment, robust model guardrails, multi-turn\njail-breaking, and efficient probing of large language models (LLMs). To\nsupport these efforts, the Amazon Nova AI Challenge team made substantial\nscientific and engineering investments, including building a custom baseline\ncoding specialist model for the challenge from scratch, developing a tournament\norchestration service, and creating an evaluation harness. This paper outlines\nthe advancements made by university teams and the Amazon Nova AI Challenge team\nin addressing the safety challenges of AI for software development,\nhighlighting this collaborative effort to raise the bar for AI safety.", "AI": {"tldr": "\u4e9a\u9a6c\u900aNova AI\u6311\u6218\u8d5b\u901a\u8fc7\u5bf9\u6297\u6027\u7ade\u8d5b\u63a8\u52a8AI\u5b89\u5168\u6027\u7814\u7a76\uff0c\u5927\u5b66\u56e2\u961f\u5f00\u53d1\u4e86\u5148\u8fdb\u6280\u672f\uff0c\u5982\u63a8\u7406\u5bf9\u9f50\u548c\u591a\u8f6e\u8d8a\u72f1\u6d4b\u8bd5\u3002", "motivation": "\u89e3\u51b3AI\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u5b89\u5168\u6027\u95ee\u9898\uff0c\u63a8\u52a8\u5b89\u5168AI\u7684\u53d1\u5c55\u3002", "method": "\u901a\u8fc7\u5bf9\u6297\u6027\u7ade\u8d5b\uff0c\u56e2\u961f\u5f00\u53d1\u81ea\u52a8\u5316\u7ea2\u961f\u673a\u5668\u4eba\u548c\u5b89\u5168AI\u52a9\u624b\uff0c\u5e76\u5229\u7528\u9ad8\u8d28\u91cf\u6570\u636e\u8fdb\u884c\u8fed\u4ee3\u6539\u8fdb\u3002", "result": "\u56e2\u961f\u5f00\u53d1\u4e86\u5148\u8fdb\u6280\u672f\uff0c\u5982\u63a8\u7406\u5bf9\u9f50\u3001\u6a21\u578b\u62a4\u680f\u548c\u591a\u8f6e\u8d8a\u72f1\u6d4b\u8bd5\uff0c\u63d0\u5347\u4e86AI\u5b89\u5168\u6027\u3002", "conclusion": "\u8be5\u6311\u6218\u8d5b\u901a\u8fc7\u534f\u4f5c\u63d0\u5347\u4e86AI\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u5b89\u5168\u6027\u6807\u51c6\u3002"}}
{"id": "2508.10852", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.10852", "abs": "https://arxiv.org/abs/2508.10852", "authors": ["Souhaila Serbout", "Diana Carolina Mu\u00f1oz Hurtado", "Hassan Atwi", "Edoardo Riggio", "Cesare Pautasso"], "title": "EVOSCAT: Exploring Software Change Dynamics in Large-Scale Historical Datasets", "comment": "Submitted to VISSOFT 2025. For the hi-resolution version of the\n  paper, see https://design.inf.usi.ch/publications/2025/vissoft", "summary": "Long lived software projects encompass a large number of artifacts, which\nundergo many revisions throughout their history. Empirical software engineering\nresearchers studying software evolution gather and collect datasets with\nmillions of events, representing changes introduced to specific artifacts. In\nthis paper, we propose EvoScat, a tool that attempts addressing temporal\nscalability through the usage of interactive density scatterplot to provide a\nglobal overview of large historical datasets mined from open source\nrepositories in a single visualization. EvoScat intents to provide researchers\nwith a mean to produce scalable visualizations that can help them explore and\ncharacterize evolution datasets, as well as comparing the histories of\nindividual artifacts, both in terms of 1) observing how rapidly different\nartifacts age over multiple-year-long time spans 2) how often metrics\nassociated with each artifacts tend towards an improvement or worsening. The\npaper shows how the tool can be tailored to specific analysis needs (pace of\nchange comparison, clone detection, freshness assessment) thanks to its support\nfor flexible configuration of history scaling and alignment along the time\naxis, artifacts sorting and interactive color mapping, enabling the analysis of\nmillions of events obtained by mining the histories of tens of thousands of\nsoftware artifacts. We include in this paper a gallery showcasing datasets\ngathering specific artifacts (OpenAPI descriptions, GitHub workflow\ndefinitions) across multiple repositories, as well as diving into the history\nof specific popular open source projects.", "AI": {"tldr": "EvoScat\u662f\u4e00\u4e2a\u5de5\u5177\uff0c\u901a\u8fc7\u4ea4\u4e92\u5f0f\u5bc6\u5ea6\u6563\u70b9\u56fe\u63d0\u4f9b\u5927\u578b\u5386\u53f2\u6570\u636e\u96c6\u7684\u5168\u5c40\u6982\u89c8\uff0c\u5e2e\u52a9\u7814\u7a76\u4eba\u5458\u63a2\u7d22\u548c\u6bd4\u8f83\u8f6f\u4ef6\u6f14\u5316\u6570\u636e\u3002", "motivation": "\u957f\u671f\u8f6f\u4ef6\u9879\u76ee\u5305\u542b\u5927\u91cf\u5de5\u4ef6\uff0c\u7ecf\u5386\u591a\u6b21\u4fee\u8ba2\uff0c\u7814\u7a76\u8f6f\u4ef6\u6f14\u5316\u7684\u7814\u7a76\u4eba\u5458\u9700\u8981\u5904\u7406\u6570\u767e\u4e07\u4e8b\u4ef6\u7684\u6570\u636e\u96c6\u3002", "method": "EvoScat\u4f7f\u7528\u4ea4\u4e92\u5f0f\u5bc6\u5ea6\u6563\u70b9\u56fe\uff0c\u652f\u6301\u7075\u6d3b\u914d\u7f6e\u65f6\u95f4\u8f74\u7f29\u653e\u3001\u5de5\u4ef6\u6392\u5e8f\u548c\u4ea4\u4e92\u5f0f\u989c\u8272\u6620\u5c04\u3002", "result": "\u5de5\u5177\u80fd\u591f\u5206\u6790\u6570\u767e\u4e07\u4e8b\u4ef6\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u5206\u6790\u9700\u6c42\uff08\u5982\u53d8\u66f4\u901f\u5ea6\u6bd4\u8f83\u3001\u514b\u9686\u68c0\u6d4b\u3001\u65b0\u9c9c\u5ea6\u8bc4\u4f30\uff09\u3002", "conclusion": "EvoScat\u4e3a\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u53ef\u89c6\u5316\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u63a2\u7d22\u548c\u6bd4\u8f83\u8f6f\u4ef6\u6f14\u5316\u6570\u636e\u96c6\u3002"}}
{"id": "2508.10031", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.10031", "abs": "https://arxiv.org/abs/2508.10031", "authors": ["Jinhwa Kim", "Ian G. Harris"], "title": "Context Misleads LLMs: The Role of Context Filtering in Maintaining Safe Alignment of LLMs", "comment": "13 pages, 2 figures", "summary": "While Large Language Models (LLMs) have shown significant advancements in\nperformance, various jailbreak attacks have posed growing safety and ethical\nrisks. Malicious users often exploit adversarial context to deceive LLMs,\nprompting them to generate responses to harmful queries. In this study, we\npropose a new defense mechanism called Context Filtering model, an input\npre-processing method designed to filter out untrustworthy and unreliable\ncontext while identifying the primary prompts containing the real user intent\nto uncover concealed malicious intent. Given that enhancing the safety of LLMs\noften compromises their helpfulness, potentially affecting the experience of\nbenign users, our method aims to improve the safety of the LLMs while\npreserving their original performance. We evaluate the effectiveness of our\nmodel in defending against jailbreak attacks through comparative analysis,\ncomparing our approach with state-of-the-art defense mechanisms against six\ndifferent attacks and assessing the helpfulness of LLMs under these defenses.\nOur model demonstrates its ability to reduce the Attack Success Rates of\njailbreak attacks by up to 88% while maintaining the original LLMs'\nperformance, achieving state-of-the-art Safety and Helpfulness Product results.\nNotably, our model is a plug-and-play method that can be applied to all LLMs,\nincluding both white-box and black-box models, to enhance their safety without\nrequiring any fine-tuning of the models themselves. We will make our model\npublicly available for research purposes.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aContext Filtering\u7684\u9632\u5fa1\u673a\u5236\uff0c\u901a\u8fc7\u8fc7\u6ee4\u4e0d\u53ef\u4fe1\u4e0a\u4e0b\u6587\u6765\u4fdd\u62a4\u5927\u8bed\u8a00\u6a21\u578b\u514d\u53d7\u6076\u610f\u653b\u51fb\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u9762\u4e34\u65e5\u76ca\u589e\u957f\u7684\u5b89\u5168\u548c\u4f26\u7406\u98ce\u9669\uff0c\u6076\u610f\u7528\u6237\u5229\u7528\u5bf9\u6297\u6027\u4e0a\u4e0b\u6587\u6b3a\u9a97\u6a21\u578b\u751f\u6210\u6709\u5bb3\u54cd\u5e94\u3002", "method": "\u63d0\u51faContext Filtering\u6a21\u578b\uff0c\u9884\u5904\u7406\u8f93\u5165\u4ee5\u8fc7\u6ee4\u4e0d\u53ef\u4fe1\u4e0a\u4e0b\u6587\uff0c\u8bc6\u522b\u771f\u5b9e\u7528\u6237\u610f\u56fe\u3002", "result": "\u6a21\u578b\u5c06\u653b\u51fb\u6210\u529f\u7387\u964d\u4f4e\u9ad8\u8fbe88%\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u539f\u6709\u6027\u80fd\u3002", "conclusion": "Context Filtering\u662f\u4e00\u79cd\u5373\u63d2\u5373\u7528\u7684\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u6240\u6709\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u65e0\u9700\u5fae\u8c03\u5373\u53ef\u63d0\u5347\u5b89\u5168\u6027\u3002"}}
{"id": "2508.10143", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10143", "abs": "https://arxiv.org/abs/2508.10143", "authors": ["Alexandru-Andrei Avram", "Adrian Groza", "Alexandru Lecu"], "title": "MCP-Orchestrated Multi-Agent System for Automated Disinformation Detection", "comment": "8 pages + 1 page references, 5 figures, 4 tables, Registered for the\n  27th International Symposium on Symbolic and Numeric Algorithms for\n  Scientific Computing, 2025, Timisoara", "summary": "The large spread of disinformation across digital platforms creates\nsignificant challenges to information integrity. This paper presents a\nmulti-agent system that uses relation extraction to detect disinformation in\nnews articles, focusing on titles and short text snippets. The proposed Agentic\nAI system combines four agents: (i) a machine learning agent (logistic\nregression), (ii) a Wikipedia knowledge check agent (which relies on named\nentity recognition), (iii) a coherence detection agent (using LLM prompt\nengineering), and (iv) a web-scraped data analyzer that extracts relational\ntriplets for fact checking. The system is orchestrated via the Model Context\nProtocol (MCP), offering shared context and live learning across components.\nResults demonstrate that the multi-agent ensemble achieves 95.3% accuracy with\nan F1 score of 0.964, significantly outperforming individual agents and\ntraditional approaches. The weighted aggregation method, mathematically derived\nfrom individual agent misclassification rates, proves superior to algorithmic\nthreshold optimization. The modular architecture makes the system easily\nscalable, while also maintaining details of the decision processes.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u901a\u8fc7\u5173\u7cfb\u63d0\u53d6\u68c0\u6d4b\u65b0\u95fb\u6807\u9898\u548c\u77ed\u6587\u672c\u4e2d\u7684\u865a\u5047\u4fe1\u606f\uff0c\u51c6\u786e\u7387\u8fbe95.3%\u3002", "motivation": "\u6570\u5b57\u5e73\u53f0\u4e0a\u865a\u5047\u4fe1\u606f\u7684\u5e7f\u6cdb\u4f20\u64ad\u5bf9\u4fe1\u606f\u5b8c\u6574\u6027\u6784\u6210\u6311\u6218\uff0c\u9700\u8981\u9ad8\u6548\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u7ed3\u5408\u56db\u79cd\u667a\u80fd\u4f53\uff08\u673a\u5668\u5b66\u4e60\u3001\u7ef4\u57fa\u767e\u79d1\u77e5\u8bc6\u68c0\u67e5\u3001\u8fde\u8d2f\u6027\u68c0\u6d4b\u3001\u7f51\u7edc\u6570\u636e\u6293\u53d6\u5206\u6790\uff09\uff0c\u901a\u8fc7Model Context Protocol\u534f\u8c03\u3002", "result": "\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u51c6\u786e\u738795.3%\uff0cF1\u5206\u65700.964\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "\u7cfb\u7edf\u6a21\u5757\u5316\u67b6\u6784\u6613\u4e8e\u6269\u5c55\uff0c\u51b3\u7b56\u8fc7\u7a0b\u900f\u660e\uff0c\u52a0\u6743\u805a\u5408\u65b9\u6cd5\u4f18\u4e8e\u9608\u503c\u4f18\u5316\u3002"}}
{"id": "2508.10033", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10033", "abs": "https://arxiv.org/abs/2508.10033", "authors": ["Yuksel Aydin"], "title": "Cognitive Cybersecurity for Artificial Intelligence: Guardrail Engineering with CCS-7", "comment": null, "summary": "Language models exhibit human-like cognitive vulnerabilities, such as\nemotional framing, that escape traditional behavioral alignment. We present\nCCS-7 (Cognitive Cybersecurity Suite), a taxonomy of seven vulnerabilities\ngrounded in human cognitive security research. To establish a human benchmark,\nwe ran a randomized controlled trial with 151 participants: a \"Think First,\nVerify Always\" (TFVA) lesson improved cognitive security by +7.9% overall. We\nthen evaluated TFVA-style guardrails across 12,180 experiments on seven diverse\nlanguage model architectures. Results reveal architecture-dependent risk\npatterns: some vulnerabilities (e.g., identity confusion) are almost fully\nmitigated, while others (e.g., source interference) exhibit escalating\nbackfire, with error rates increasing by up to 135% in certain models. Humans,\nin contrast, show consistent moderate improvement. These findings reframe\ncognitive safety as a model-specific engineering problem: interventions\neffective in one architecture may fail, or actively harm, another, underscoring\nthe need for architecture-aware cognitive safety testing before deployment.", "AI": {"tldr": "CCS-7\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4eba\u7c7b\u8ba4\u77e5\u5b89\u5168\u7814\u7a76\u7684\u8bed\u8a00\u6a21\u578b\u8ba4\u77e5\u6f0f\u6d1e\u5206\u7c7b\u6cd5\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u4e0d\u540c\u67b6\u6784\u6a21\u578b\u5bf9\u8fd9\u4e9b\u6f0f\u6d1e\u7684\u54cd\u5e94\u5dee\u5f02\u3002", "motivation": "\u7814\u7a76\u8bed\u8a00\u6a21\u578b\u5728\u8ba4\u77e5\u5b89\u5168\u65b9\u9762\u7684\u6f0f\u6d1e\uff0c\u4ee5\u586b\u8865\u4f20\u7edf\u884c\u4e3a\u5bf9\u9f50\u65b9\u6cd5\u7684\u4e0d\u8db3\u3002", "method": "\u901a\u8fc7\u968f\u673a\u5bf9\u7167\u8bd5\u9a8c\uff08151\u4eba\uff09\u5efa\u7acb\u4eba\u7c7b\u57fa\u51c6\uff0c\u5e76\u572812,180\u6b21\u5b9e\u9a8c\u4e2d\u8bc4\u4f30TFVA\u98ce\u683c\u9632\u62a4\u63aa\u65bd\u5bf97\u79cd\u8bed\u8a00\u6a21\u578b\u67b6\u6784\u7684\u5f71\u54cd\u3002", "result": "\u4e0d\u540c\u6a21\u578b\u67b6\u6784\u5bf9\u6f0f\u6d1e\u7684\u54cd\u5e94\u5dee\u5f02\u663e\u8457\uff0c\u67d0\u4e9b\u6f0f\u6d1e\u88ab\u6709\u6548\u7f13\u89e3\uff0c\u800c\u53e6\u4e00\u4e9b\u6f0f\u6d1e\u7684\u9519\u8bef\u7387\u751a\u81f3\u4e0a\u5347135%\u3002\u4eba\u7c7b\u8868\u73b0\u5219\u5448\u73b0\u4e00\u81f4\u7684\u4e2d\u7b49\u63d0\u5347\u3002", "conclusion": "\u8ba4\u77e5\u5b89\u5168\u9700\u9488\u5bf9\u7279\u5b9a\u6a21\u578b\u67b6\u6784\u8fdb\u884c\u5b9a\u5236\u5316\u6d4b\u8bd5\u548c\u5e72\u9884\uff0c\u901a\u7528\u65b9\u6cd5\u53ef\u80fd\u65e0\u6548\u6216\u6709\u5bb3\u3002"}}
{"id": "2508.10146", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10146", "abs": "https://arxiv.org/abs/2508.10146", "authors": ["Hana Derouiche", "Zaki Brahmi", "Haithem Mazeni"], "title": "Agentic AI Frameworks: Architectures, Protocols, and Design Challenges", "comment": null, "summary": "The emergence of Large Language Models (LLMs) has ushered in a transformative\nparadigm in artificial intelligence, Agentic AI, where intelligent agents\nexhibit goal-directed autonomy, contextual reasoning, and dynamic multi-agent\ncoordination. This paper provides a systematic review and comparative analysis\nof leading Agentic AI frameworks, including CrewAI, LangGraph, AutoGen,\nSemantic Kernel, Agno, Google ADK, and MetaGPT, evaluating their architectural\nprinciples, communication mechanisms, memory management, safety guardrails, and\nalignment with service-oriented computing paradigms. Furthermore, we identify\nkey limitations, emerging trends, and open challenges in the field. To address\nthe issue of agent communication, we conduct an in-depth analysis of protocols\nsuch as the Contract Net Protocol (CNP), Agent-to-Agent (A2A), Agent Network\nProtocol (ANP), and Agora. Our findings not only establish a foundational\ntaxonomy for Agentic AI systems but also propose future research directions to\nenhance scalability, robustness, and interoperability. This work serves as a\ncomprehensive reference for researchers and practitioners working to advance\nthe next generation of autonomous AI systems.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u56de\u987e\u548c\u6bd4\u8f83\u4e86\u4e3b\u6d41Agentic AI\u6846\u67b6\uff0c\u5206\u6790\u4e86\u5176\u67b6\u6784\u3001\u901a\u4fe1\u673a\u5236\u548c\u5b89\u5168\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u63a2\u7d22\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u9a71\u52a8\u7684Agentic AI\u7684\u6f5c\u529b\uff0c\u89e3\u51b3\u667a\u80fd\u4ee3\u7406\u7684\u81ea\u4e3b\u6027\u3001\u534f\u8c03\u6027\u548c\u901a\u4fe1\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u56de\u987e\u548c\u6bd4\u8f83\u5206\u6790\u591a\u4e2aAgentic AI\u6846\u67b6\uff0c\u5e76\u6df1\u5165\u5206\u6790\u901a\u4fe1\u534f\u8bae\u5982CNP\u3001A2A\u7b49\u3002", "result": "\u5efa\u7acb\u4e86Agentic AI\u7cfb\u7edf\u7684\u57fa\u7840\u5206\u7c7b\u6cd5\uff0c\u5e76\u8bc6\u522b\u4e86\u5173\u952e\u5c40\u9650\u6027\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "\u672c\u6587\u4e3a\u4e0b\u4e00\u4ee3\u81ea\u4e3bAI\u7cfb\u7edf\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u53c2\u8003\uff0c\u5f3a\u8c03\u4e86\u53ef\u6269\u5c55\u6027\u3001\u9c81\u68d2\u6027\u548c\u4e92\u64cd\u4f5c\u6027\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2508.10035", "categories": ["cs.CR", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.10035", "abs": "https://arxiv.org/abs/2508.10035", "authors": ["Varsha Sen", "Biswash Basnet"], "title": "Neural Network-Based Detection and Multi-Class Classification of FDI Attacks in Smart Grid Home Energy Systems", "comment": "17 pages, 7 figures", "summary": "False Data Injection Attacks (FDIAs) pose a significant threat to smart grid\ninfrastructures, particularly Home Area Networks (HANs), where real-time\nmonitoring and control are highly adopted. Owing to the comparatively less\nstringent security controls and widespread availability of HANs, attackers view\nthem as an attractive entry point to manipulate aggregated demand patterns,\nwhich can ultimately propagate and disrupt broader grid operations. These\nattacks undermine the integrity of smart meter data, enabling malicious actors\nto manipulate consumption values without activating conventional alarms,\nthereby creating serious vulnerabilities across both residential and\nutility-scale infrastructures. This paper presents a machine learning-based\nframework for both the detection and classification of FDIAs using residential\nenergy data. A real-time detection is provided by the lightweight Artificial\nNeural Network (ANN), which works by using the most vital features of energy\nconsumption, cost, and time context. For the classification of different attack\ntypes, a Bidirectional LSTM is trained to recognize normal, trapezoidal, and\nsigmoid attack shapes through learning sequential dependencies in the data. A\nsynthetic time-series dataset was generated to emulate realistic household\nbehaviour. Experimental results demonstrate that the proposed models are\neffective in identifying and classifying FDIAs, offering a scalable solution\nfor enhancing grid resilience at the edge. This work contributes toward\nbuilding intelligent, data-driven defence mechanisms that strengthen smart grid\ncybersecurity from residential endpoints.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u6d4b\u548c\u5206\u7c7b\u667a\u80fd\u7535\u7f51\u4e2d\u7684\u865a\u5047\u6570\u636e\u6ce8\u5165\u653b\u51fb\uff08FDIAs\uff09\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\uff08ANN\uff09\u548c\u53cc\u5411LSTM\u6a21\u578b\u5b9e\u73b0\u5b9e\u65f6\u68c0\u6d4b\u548c\u653b\u51fb\u5206\u7c7b\u3002", "motivation": "\u865a\u5047\u6570\u636e\u6ce8\u5165\u653b\u51fb\uff08FDIAs\uff09\u5bf9\u667a\u80fd\u7535\u7f51\u57fa\u7840\u8bbe\u65bd\u6784\u6210\u4e25\u91cd\u5a01\u80c1\uff0c\u5c24\u5176\u662f\u5bb6\u5ead\u533a\u57df\u7f51\u7edc\uff08HANs\uff09\uff0c\u5176\u5b89\u5168\u63a7\u5236\u8f83\u5f31\u4e14\u5e7f\u6cdb\u53ef\u7528\uff0c\u653b\u51fb\u8005\u53ef\u901a\u8fc7\u64cd\u7eb5\u6d88\u8d39\u6570\u636e\u7834\u574f\u7535\u7f51\u8fd0\u884c\u3002", "method": "\u4f7f\u7528\u8f7b\u91cf\u7ea7ANN\u8fdb\u884c\u5b9e\u65f6\u68c0\u6d4b\uff0c\u63d0\u53d6\u80fd\u6e90\u6d88\u8017\u3001\u6210\u672c\u548c\u65f6\u95f4\u4e0a\u4e0b\u6587\u7684\u5173\u952e\u7279\u5f81\uff1b\u901a\u8fc7\u53cc\u5411LSTM\u5206\u7c7b\u4e0d\u540c\u653b\u51fb\u7c7b\u578b\uff08\u6b63\u5e38\u3001\u68af\u5f62\u548cS\u5f62\u653b\u51fb\uff09\u3002\u5b9e\u9a8c\u57fa\u4e8e\u5408\u6210\u7684\u5bb6\u5ead\u884c\u4e3a\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u96c6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u6a21\u578b\u80fd\u6709\u6548\u8bc6\u522b\u548c\u5206\u7c7bFDIAs\uff0c\u4e3a\u589e\u5f3a\u7535\u7f51\u8fb9\u7f18\u97e7\u6027\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u667a\u80fd\u7535\u7f51\u7f51\u7edc\u5b89\u5168\u63d0\u4f9b\u4e86\u6570\u636e\u9a71\u52a8\u7684\u9632\u5fa1\u673a\u5236\uff0c\u4ece\u4f4f\u5b85\u7aef\u70b9\u52a0\u5f3a\u4e86\u7535\u7f51\u7684\u97e7\u6027\u3002"}}
{"id": "2508.10152", "categories": ["cs.AI", "I.2.7; H.3.3"], "pdf": "https://arxiv.org/pdf/2508.10152", "abs": "https://arxiv.org/abs/2508.10152", "authors": ["Doaa Allabadi", "Kyle Bradbury", "Jordan M. Malof"], "title": "Improving and Evaluating Open Deep Research Agents", "comment": "8 pages, 2 figures, 2 tables", "summary": "We focus here on Deep Research Agents (DRAs), which are systems that can take\na natural language prompt from a user, and then autonomously search for, and\nutilize, internet-based content to address the prompt. Recent DRAs have\ndemonstrated impressive capabilities on public benchmarks however, recent\nresearch largely involves proprietary closed-source systems. At the time of\nthis work, we only found one open-source DRA, termed Open Deep Research (ODR).\nIn this work we adapt the challenging recent BrowseComp benchmark to compare\nODR to existing proprietary systems. We propose BrowseComp-Small (BC-Small),\ncomprising a subset of BrowseComp, as a more computationally-tractable DRA\nbenchmark for academic labs. We benchmark ODR and two other proprietary systems\non BC-Small: one system from Anthropic and one system from Google. We find that\nall three systems achieve 0% accuracy on the test set of 60 questions. We\nintroduce three strategic improvements to ODR, resulting in the ODR+ model,\nwhich achieves a state-of-the-art 10% success rate on BC-Small among both\nclosed-source and open-source systems. We report ablation studies indicating\nthat all three of our improvements contributed to the success of ODR+.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5f00\u6e90\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\uff08ODR\uff09\u4e0e\u4e13\u6709\u7cfb\u7edf\u7684\u6027\u80fd\u5bf9\u6bd4\uff0c\u63d0\u51fa\u6539\u8fdb\u540e\u7684ODR+\u6a21\u578b\u5728BC-Small\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523010%\u7684\u6210\u529f\u7387\u3002", "motivation": "\u5f53\u524d\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\uff08DRAs\uff09\u591a\u4e3a\u95ed\u6e90\u7cfb\u7edf\uff0c\u7f3a\u4e4f\u5f00\u6e90\u9009\u62e9\uff0c\u9650\u5236\u4e86\u5b66\u672f\u7814\u7a76\u3002", "method": "\u901a\u8fc7\u6539\u8fdb\u5f00\u6e90\u7cfb\u7edfODR\uff0c\u63d0\u51faODR+\u6a21\u578b\uff0c\u5e76\u5728BC-Small\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e0e\u4e13\u6709\u7cfb\u7edf\u5bf9\u6bd4\u3002", "result": "ODR+\u5728BC-Small\u6d4b\u8bd5\u96c6\u4e0a\u8fbe\u523010%\u7684\u6210\u529f\u7387\uff0c\u4f18\u4e8e\u5176\u4ed6\u7cfb\u7edf\u3002", "conclusion": "\u5f00\u6e90\u7cfb\u7edfODR+\u901a\u8fc7\u6539\u8fdb\u53ef\u4ee5\u63a5\u8fd1\u4e13\u6709\u7cfb\u7edf\u6027\u80fd\uff0c\u4e3a\u5b66\u672f\u7814\u7a76\u63d0\u4f9b\u53ef\u884c\u9009\u62e9\u3002"}}
{"id": "2508.10038", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10038", "abs": "https://arxiv.org/abs/2508.10038", "authors": ["Pierre-Francois Gimenez", "Sarath Sivaprasad", "Mario Fritz"], "title": "Certifiably robust malware detectors by design", "comment": null, "summary": "Malware analysis involves analyzing suspicious software to detect malicious\npayloads. Static malware analysis, which does not require software execution,\nrelies increasingly on machine learning techniques to achieve scalability.\nAlthough such techniques obtain very high detection accuracy, they can be\neasily evaded with adversarial examples where a few modifications of the sample\ncan dupe the detector without modifying the behavior of the software. Unlike\nother domains, such as computer vision, creating an adversarial example of\nmalware without altering its functionality requires specific transformations.\nWe propose a new model architecture for certifiably robust malware detection by\ndesign. In addition, we show that every robust detector can be decomposed into\na specific structure, which can be applied to learn empirically robust malware\ndetectors, even on fragile features. Our framework ERDALT is based on this\nstructure. We compare and validate these approaches with machine-learning-based\nmalware detection methods, allowing for robust detection with limited reduction\nof detection performance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6a21\u578b\u67b6\u6784ERDALT\uff0c\u7528\u4e8e\u8bbe\u8ba1\u53ef\u8bc1\u660e\u9c81\u68d2\u7684\u6076\u610f\u8f6f\u4ef6\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u5206\u89e3\u9c81\u68d2\u68c0\u6d4b\u5668\u7684\u7ed3\u6784\uff0c\u5b9e\u73b0\u4e86\u5728\u8106\u5f31\u7279\u5f81\u4e0a\u7684\u7ecf\u9a8c\u9c81\u68d2\u6027\u3002", "motivation": "\u9759\u6001\u6076\u610f\u8f6f\u4ef6\u5206\u6790\u4f9d\u8d56\u673a\u5668\u5b66\u4e60\u6280\u672f\uff0c\u4f46\u6613\u53d7\u5bf9\u6297\u6837\u672c\u653b\u51fb\uff0c\u9700\u8981\u5728\u4e0d\u6539\u53d8\u8f6f\u4ef6\u529f\u80fd\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u9c81\u68d2\u68c0\u6d4b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6a21\u578b\u67b6\u6784\uff0c\u5e76\u5c55\u793a\u4e86\u9c81\u68d2\u68c0\u6d4b\u5668\u7684\u5206\u89e3\u7ed3\u6784\uff0c\u5e94\u7528\u4e8e\u5b66\u4e60\u7ecf\u9a8c\u9c81\u68d2\u68c0\u6d4b\u5668\u3002", "result": "\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u68c0\u6d4b\u6027\u80fd\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u9c81\u68d2\u68c0\u6d4b\u3002", "conclusion": "ERDALT\u6846\u67b6\u4e3a\u6076\u610f\u8f6f\u4ef6\u68c0\u6d4b\u63d0\u4f9b\u4e86\u9c81\u68d2\u6027\u4fdd\u969c\uff0c\u4e14\u6027\u80fd\u635f\u5931\u6709\u9650\u3002"}}
{"id": "2508.10164", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10164", "abs": "https://arxiv.org/abs/2508.10164", "authors": ["Bin Hong", "Jiayu Liu", "Zhenya Huang", "Kai Zhang", "Mengdi Zhang"], "title": "Pruning Long Chain-of-Thought of Large Reasoning Models via Small-Scale Preference Optimization", "comment": "19 pages, 5 figures", "summary": "Recent advances in Large Reasoning Models (LRMs) have demonstrated strong\nperformance on complex tasks through long Chain-of-Thought (CoT) reasoning.\nHowever, their lengthy outputs increase computational costs and may lead to\noverthinking, raising challenges in balancing reasoning effectiveness and\nefficiency. Current methods for efficient reasoning often compromise reasoning\nquality or require extensive resources. This paper investigates efficient\nmethods to reduce the generation length of LRMs. We analyze generation path\ndistributions and filter generated trajectories through difficulty estimation.\nSubsequently, we analyze the convergence behaviors of the objectives of various\npreference optimization methods under a Bradley-Terry loss based framework.\nBased on the analysis, we propose Length Controlled Preference Optimization\n(LCPO) that directly balances the implicit reward related to NLL loss. LCPO can\neffectively learn length preference with limited data and training. Extensive\nexperiments demonstrate that our approach significantly reduces the average\noutput length by over 50\\% across multiple benchmarks while maintaining the\nreasoning performance. Our work highlights the potential for computationally\nefficient approaches in guiding LRMs toward efficient reasoning.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLCPO\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u63a7\u5236\u751f\u6210\u957f\u5ea6\uff0c\u663e\u8457\u51cf\u5c11\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u8f93\u51fa\u957f\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u63a8\u7406\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u957f\u63a8\u7406\u94fe\u589e\u52a0\u4e86\u8ba1\u7b97\u6210\u672c\u5e76\u53ef\u80fd\u5bfc\u81f4\u8fc7\u5ea6\u601d\u8003\uff0c\u5f53\u524d\u65b9\u6cd5\u5728\u6548\u7387\u548c\u8d28\u91cf\u4e4b\u95f4\u96be\u4ee5\u5e73\u8861\u3002", "method": "\u5206\u6790\u751f\u6210\u8def\u5f84\u5206\u5e03\u5e76\u901a\u8fc7\u96be\u5ea6\u4f30\u8ba1\u8fc7\u6ee4\u8f68\u8ff9\uff0c\u63d0\u51faLCPO\u65b9\u6cd5\u76f4\u63a5\u5e73\u8861\u9690\u5f0f\u5956\u52b1\u4e0eNLL\u635f\u5931\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLCPO\u5728\u591a\u4e2a\u57fa\u51c6\u4e0a\u5c06\u5e73\u5747\u8f93\u51fa\u957f\u5ea6\u51cf\u5c1150%\u4ee5\u4e0a\uff0c\u540c\u65f6\u4fdd\u6301\u63a8\u7406\u6027\u80fd\u3002", "conclusion": "LCPO\u5c55\u793a\u4e86\u5728\u6709\u9650\u6570\u636e\u548c\u8bad\u7ec3\u4e0b\u9ad8\u6548\u5f15\u5bfc\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.10039", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10039", "abs": "https://arxiv.org/abs/2508.10039", "authors": ["Wenqiang Wang", "Yan Xiao", "Hao Lin", "Yangshijie Zhang", "Xiaochun Cao"], "title": "Multi-task Adversarial Attacks against Black-box Model with Few-shot Queries", "comment": null, "summary": "Current multi-task adversarial text attacks rely on abundant access to shared\ninternal features and numerous queries, often limited to a single task type. As\na result, these attacks are less effective against practical scenarios\ninvolving black-box feedback APIs, limited queries, or multiple task types. To\nbridge this gap, we propose \\textbf{C}luster and \\textbf{E}nsemble\n\\textbf{M}ulti-task Text Adversarial \\textbf{A}ttack (\\textbf{CEMA}), an\neffective black-box attack that exploits the transferability of adversarial\ntexts across different tasks. CEMA simplifies complex multi-task scenarios by\nusing a \\textit{deep-level substitute model} trained in a\n\\textit{plug-and-play} manner for text classification, enabling attacks without\nmimicking the victim model. This approach requires only a few queries for\ntraining, converting multi-task attacks into classification attacks and\nallowing attacks across various tasks.\n  CEMA generates multiple adversarial candidates using different text\nclassification methods and selects the one that most effectively attacks\nsubstitute models.\n  In experiments involving multi-task models with two, three, or six\ntasks--spanning classification, translation, summarization, and text-to-image\ngeneration--CEMA demonstrates significant attack success with as few as 100\nqueries. Furthermore, CEMA can target commercial APIs (e.g., Baidu and Google\nTranslate), large language models (e.g., ChatGPT 4o), and image-generation\nmodels (e.g., Stable Diffusion V2), showcasing its versatility and\neffectiveness in real-world applications.", "AI": {"tldr": "CEMA\u662f\u4e00\u79cd\u9ed1\u76d2\u591a\u4efb\u52a1\u6587\u672c\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u5bf9\u6297\u6587\u672c\u7684\u8de8\u4efb\u52a1\u53ef\u8f6c\u79fb\u6027\uff0c\u7b80\u5316\u590d\u6742\u591a\u4efb\u52a1\u573a\u666f\uff0c\u4ec5\u9700\u5c11\u91cf\u67e5\u8be2\u5373\u53ef\u5b9e\u73b0\u9ad8\u6548\u653b\u51fb\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5171\u4eab\u5185\u90e8\u7279\u5f81\u548c\u5927\u91cf\u67e5\u8be2\uff0c\u4e14\u5c40\u9650\u4e8e\u5355\u4e00\u4efb\u52a1\u7c7b\u578b\uff0c\u96be\u4ee5\u5e94\u5bf9\u9ed1\u76d2\u53cd\u9988API\u3001\u6709\u9650\u67e5\u8be2\u6216\u591a\u4efb\u52a1\u7c7b\u578b\u7684\u5b9e\u9645\u573a\u666f\u3002", "method": "CEMA\u91c7\u7528\u6df1\u5ea6\u7ea7\u66ff\u4ee3\u6a21\u578b\uff0c\u4ee5\u5373\u63d2\u5373\u7528\u65b9\u5f0f\u8bad\u7ec3\uff0c\u5c06\u591a\u4efb\u52a1\u653b\u51fb\u8f6c\u5316\u4e3a\u5206\u7c7b\u653b\u51fb\uff0c\u5e76\u901a\u8fc7\u4e0d\u540c\u6587\u672c\u5206\u7c7b\u65b9\u6cd5\u751f\u6210\u5019\u9009\u5bf9\u6297\u6587\u672c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCEMA\u5728\u6d89\u53ca2\u81f36\u4e2a\u4efb\u52a1\u7684\u6a21\u578b\u4e2d\uff0c\u4ec5\u9700100\u6b21\u67e5\u8be2\u5373\u53ef\u5b9e\u73b0\u663e\u8457\u653b\u51fb\u6210\u529f\u7387\uff0c\u5e76\u80fd\u9488\u5bf9\u5546\u4e1aAPI\u548c\u5927\u6a21\u578b\u3002", "conclusion": "CEMA\u5c55\u793a\u4e86\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u591a\u529f\u80fd\u6027\u548c\u9ad8\u6548\u6027\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u4efb\u52a1\u548c\u6a21\u578b\u3002"}}
{"id": "2508.10177", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10177", "abs": "https://arxiv.org/abs/2508.10177", "authors": ["Stepan Kulibaba", "Artem Dzhalilov", "Roman Pakhomov", "Oleg Svidchenko", "Alexander Gasnikov", "Aleksei Shpilman"], "title": "KompeteAI: Accelerated Autonomous Multi-Agent System for End-to-End Pipeline Generation for Machine Learning Problems", "comment": null, "summary": "Recent Large Language Model (LLM)-based AutoML systems demonstrate impressive\ncapabilities but face significant limitations such as constrained exploration\nstrategies and a severe execution bottleneck. Exploration is hindered by\none-shot methods lacking diversity and Monte Carlo Tree Search (MCTS)\napproaches that fail to recombine strong partial solutions. The execution\nbottleneck arises from lengthy code validation cycles that stifle iterative\nrefinement. To overcome these challenges, we introduce KompeteAI, a novel\nAutoML framework with dynamic solution space exploration. Unlike previous MCTS\nmethods that treat ideas in isolation, KompeteAI introduces a merging stage\nthat composes top candidates. We further expand the hypothesis space by\nintegrating Retrieval-Augmented Generation (RAG), sourcing ideas from Kaggle\nnotebooks and arXiv papers to incorporate real-world strategies. KompeteAI also\naddresses the execution bottleneck via a predictive scoring model and an\naccelerated debugging method, assessing solution potential using early stage\nmetrics to avoid costly full-code execution. This approach accelerates pipeline\nevaluation 6.9 times. KompeteAI outperforms leading methods (e.g., RD-agent,\nAIDE, and Ml-Master) by an average of 3\\% on the primary AutoML benchmark,\nMLE-Bench. Additionally, we propose Kompete-bench to address limitations in\nMLE-Bench, where KompeteAI also achieves state-of-the-art results", "AI": {"tldr": "KompeteAI\u662f\u4e00\u4e2a\u65b0\u578bAutoML\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u89e3\u51b3\u65b9\u6848\u7a7a\u95f4\u63a2\u7d22\u548c\u5408\u5e76\u9636\u6bb5\u63d0\u5347\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\u73b0\u6709LLM-based AutoML\u7cfb\u7edf\u7684\u63a2\u7d22\u5c40\u9650\u548c\u6267\u884c\u74f6\u9888\u95ee\u9898\u3002", "motivation": "\u73b0\u6709LLM-based AutoML\u7cfb\u7edf\u5b58\u5728\u63a2\u7d22\u7b56\u7565\u53d7\u9650\u548c\u6267\u884c\u74f6\u9888\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u6027\u80fd\u548c\u6548\u7387\u3002", "method": "KompeteAI\u5f15\u5165\u52a8\u6001\u89e3\u51b3\u65b9\u6848\u7a7a\u95f4\u63a2\u7d22\uff0c\u7ed3\u5408\u5408\u5e76\u9636\u6bb5\u548cRAG\u6280\u672f\uff0c\u5e76\u91c7\u7528\u9884\u6d4b\u8bc4\u5206\u6a21\u578b\u548c\u52a0\u901f\u8c03\u8bd5\u65b9\u6cd5\u3002", "result": "KompeteAI\u5728MLE-Bench\u57fa\u51c6\u4e0a\u5e73\u5747\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd53%\uff0c\u5e76\u5c06\u7ba1\u9053\u8bc4\u4f30\u901f\u5ea6\u63d0\u53476.9\u500d\u3002", "conclusion": "KompeteAI\u901a\u8fc7\u521b\u65b0\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86AutoML\u7cfb\u7edf\u7684\u6027\u80fd\u548c\u6548\u7387\uff0c\u5e76\u5728\u65b0\u63d0\u51fa\u7684Kompete-bench\u4e0a\u53d6\u5f97\u6700\u4f73\u7ed3\u679c\u3002"}}
{"id": "2508.10041", "categories": ["cs.CR", "quant-ph"], "pdf": "https://arxiv.org/pdf/2508.10041", "abs": "https://arxiv.org/abs/2508.10041", "authors": ["Julien Mellaerts"], "title": "Quantum Prime Factorization: A Novel Approach Based on Fermat Method", "comment": null, "summary": "In this paper, we introduce a novel quantum algorithm for the factorization\nof composite odd numbers. This work makes two significant contributions. First,\nwe present a new improvement to the classical Fermat method, fourfold reducing\nthe computational complexity of factoring. Second, we reformulate Fermat\nfactorization method as an optimization problem suitable for Quantum Annealers\nwhich allowed us to factorize 8,689,739, the biggest number ever factorized\nusing a quantum device to our knowledge.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u91cf\u5b50\u7b97\u6cd5\u7528\u4e8e\u5206\u89e3\u5408\u6570\uff0c\u6539\u8fdb\u4e86\u7ecf\u5178\u8d39\u9a6c\u65b9\u6cd5\uff0c\u5e76\u5c06\u8d39\u9a6c\u5206\u89e3\u65b9\u6cd5\u91cd\u65b0\u8868\u8ff0\u4e3a\u9002\u7528\u4e8e\u91cf\u5b50\u9000\u706b\u5668\u7684\u4f18\u5316\u95ee\u9898\u3002", "motivation": "\u901a\u8fc7\u6539\u8fdb\u7ecf\u5178\u8d39\u9a6c\u65b9\u6cd5\u548c\u5229\u7528\u91cf\u5b50\u9000\u706b\u5668\uff0c\u63d0\u9ad8\u5927\u6570\u5206\u89e3\u7684\u6548\u7387\u548c\u80fd\u529b\u3002", "method": "\u6539\u8fdb\u7ecf\u5178\u8d39\u9a6c\u65b9\u6cd5\uff0c\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\uff1b\u5c06\u8d39\u9a6c\u5206\u89e3\u91cd\u65b0\u8868\u8ff0\u4e3a\u4f18\u5316\u95ee\u9898\uff0c\u9002\u7528\u4e8e\u91cf\u5b50\u9000\u706b\u5668\u3002", "result": "\u6210\u529f\u5206\u89e3\u4e868,689,739\uff0c\u8fd9\u662f\u76ee\u524d\u5df2\u77e5\u7528\u91cf\u5b50\u8bbe\u5907\u5206\u89e3\u7684\u6700\u5927\u6570\u3002", "conclusion": "\u65b0\u7b97\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u5206\u89e3\u6548\u7387\uff0c\u5c55\u793a\u4e86\u91cf\u5b50\u9000\u706b\u5668\u5728\u5927\u6570\u5206\u89e3\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.10241", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10241", "abs": "https://arxiv.org/abs/2508.10241", "authors": ["Mark Zilberman"], "title": "Extending the Entropic Potential of Events for Uncertainty Quantification and Decision-Making in Artificial Intelligence", "comment": "10 pages", "summary": "This work demonstrates how the concept of the entropic potential of events --\na parameter quantifying the influence of discrete events on the expected future\nentropy of a system -- can enhance uncertainty quantification, decision-making,\nand interpretability in artificial intelligence (AI). Building on its original\nformulation in physics, the framework is adapted for AI by introducing an\nevent-centric measure that captures how actions, observations, or other\ndiscrete occurrences impact uncertainty at future time horizons. Both the\noriginal and AI-adjusted definitions of entropic potential are formalized, with\nthe latter emphasizing conditional expectations to account for counterfactual\nscenarios. Applications are explored in policy evaluation, intrinsic reward\ndesign, explainable AI, and anomaly detection, highlighting the metric's\npotential to unify and strengthen uncertainty modeling in intelligent systems.\nConceptual examples illustrate its use in reinforcement learning, Bayesian\ninference, and anomaly detection, while practical considerations for\ncomputation in complex AI models are discussed. The entropic potential\nframework offers a theoretically grounded, interpretable, and versatile\napproach to managing uncertainty in AI, bridging principles from\nthermodynamics, information theory, and machine learning.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e8b\u4ef6\u71b5\u52bf\u7684\u6982\u5ff5\uff0c\u7528\u4e8e\u589e\u5f3aAI\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3001\u51b3\u7b56\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u653f\u7b56\u8bc4\u4f30\u3001\u5956\u52b1\u8bbe\u8ba1\u7b49\u9886\u57df\u3002", "motivation": "\u901a\u8fc7\u5f15\u5165\u4e8b\u4ef6\u71b5\u52bf\u7684\u6982\u5ff5\uff0c\u65e8\u5728\u7edf\u4e00\u548c\u52a0\u5f3a\u667a\u80fd\u7cfb\u7edf\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\uff0c\u7ed3\u5408\u7269\u7406\u5b66\u548c\u4fe1\u606f\u7406\u8bba\u7684\u539f\u7406\u3002", "method": "\u5c06\u7269\u7406\u5b66\u4e2d\u7684\u71b5\u52bf\u6982\u5ff5\u8c03\u6574\u4e3aAI\u9002\u7528\u7684\u4e8b\u4ef6\u4e2d\u5fc3\u5ea6\u91cf\uff0c\u5f3a\u8c03\u6761\u4ef6\u671f\u671b\u4ee5\u8003\u8651\u53cd\u4e8b\u5b9e\u573a\u666f\uff0c\u5e76\u5e94\u7528\u4e8e\u591a\u79cdAI\u4efb\u52a1\u3002", "result": "\u5c55\u793a\u4e86\u71b5\u52bf\u6846\u67b6\u5728\u5f3a\u5316\u5b66\u4e60\u3001\u8d1d\u53f6\u65af\u63a8\u7406\u548c\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u5b9e\u7528\u6027\uff0c\u4e3aAI\u4e0d\u786e\u5b9a\u6027\u7ba1\u7406\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002", "conclusion": "\u71b5\u52bf\u6846\u67b6\u4e3aAI\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u7ba1\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u7406\u8bba\u624e\u5b9e\u3001\u53ef\u89e3\u91ca\u4e14\u591a\u529f\u80fd\u7684\u65b9\u6cd5\uff0c\u8fde\u63a5\u4e86\u70ed\u529b\u5b66\u3001\u4fe1\u606f\u7406\u8bba\u548c\u673a\u5668\u5b66\u4e60\u3002"}}
{"id": "2508.10042", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10042", "abs": "https://arxiv.org/abs/2508.10042", "authors": ["Jane Carney", "Kushal Upreti", "Gaby G. Dagher", "Tim Andersen"], "title": "FIDELIS: Blockchain-Enabled Protection Against Poisoning Attacks in Federated Learning", "comment": null, "summary": "Federated learning enhances traditional deep learning by enabling the joint\ntraining of a model with the use of IoT device's private data. It ensures\nprivacy for clients, but is susceptible to data poisoning attacks during\ntraining that degrade model performance and integrity. Current poisoning\ndetection methods in federated learning lack a standardized detection method or\ntake significant liberties with trust. In this paper, we present \\Sys, a novel\nblockchain-enabled poison detection framework in federated learning. The\nframework decentralizes the role of the global server across participating\nclients. We introduce a judge model used to detect data poisoning in model\nupdates. The judge model is produced by each client and verified to reach\nconsensus on a single judge model. We implement our solution to show \\Sys is\nrobust against data poisoning attacks and the creation of our judge model is\nscalable.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u533a\u5757\u94fe\u7684\u8054\u90a6\u5b66\u4e60\u6570\u636e\u6295\u6bd2\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6563\u5168\u5c40\u670d\u52a1\u5668\u89d2\u8272\u548c\u5f15\u5165\u5171\u8bc6\u9a8c\u8bc1\u7684\u6cd5\u5b98\u6a21\u578b\uff0c\u63d0\u9ad8\u68c0\u6d4b\u7684\u9c81\u68d2\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u867d\u4fdd\u62a4\u9690\u79c1\uff0c\u4f46\u6613\u53d7\u6570\u636e\u6295\u6bd2\u653b\u51fb\uff0c\u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5\u7f3a\u4e4f\u6807\u51c6\u5316\u6216\u8fc7\u5ea6\u4f9d\u8d56\u4fe1\u4efb\u3002", "method": "\u8bbe\u8ba1\u533a\u5757\u94fe\u6846\u67b6\uff0c\u5206\u6563\u5168\u5c40\u670d\u52a1\u5668\u529f\u80fd\uff0c\u5ba2\u6237\u7aef\u751f\u6210\u6cd5\u5b98\u6a21\u578b\u5e76\u901a\u8fc7\u5171\u8bc6\u9a8c\u8bc1\u3002", "result": "\u6846\u67b6\u80fd\u6709\u6548\u62b5\u5fa1\u6570\u636e\u6295\u6bd2\u653b\u51fb\uff0c\u6cd5\u5b98\u6a21\u578b\u7684\u751f\u6210\u5177\u6709\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\\Sys \u6846\u67b6\u4e3a\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u6570\u636e\u6295\u6bd2\u95ee\u9898\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u5206\u6563\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.10265", "categories": ["cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2508.10265", "abs": "https://arxiv.org/abs/2508.10265", "authors": ["Jingde Cheng"], "title": "Why Cannot Large Language Models Ever Make True Correct Reasoning?", "comment": "8 pages. arXiv admin note: substantial text overlap with\n  arXiv:2412.12408", "summary": "Recently, with the application progress of AIGC tools based on large language\nmodels (LLMs), led by ChatGPT, many AI experts and more non-professionals are\ntrumpeting the \"understanding ability\" and \"reasoning ability\" of the LLMs. The\npresent author considers that the so-called \"understanding ability\" and\n\"reasoning ability\" of LLMs are just illusions of those people who with vague\nconcepts. In fact, the LLMs can never have the true understanding ability and\ntrue reasoning ability. This paper intents to explain that, because the\nessential limitations of their working principle, the LLMs can never have the\nability of true correct reasoning.", "AI": {"tldr": "\u672c\u6587\u8ba4\u4e3a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684AIGC\u5de5\u5177\uff08\u5982ChatGPT\uff09\u7684\u201c\u7406\u89e3\u80fd\u529b\u201d\u548c\u201c\u63a8\u7406\u80fd\u529b\u201d\u53ea\u662f\u6982\u5ff5\u6a21\u7cca\u8005\u7684\u9519\u89c9\uff0c\u5b9e\u9645\u4e0aLLMs\u56e0\u5de5\u4f5c\u539f\u7406\u7684\u672c\u8d28\u9650\u5236\u65e0\u6cd5\u62e5\u6709\u771f\u6b63\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u63a2\u8ba8LLMs\u662f\u5426\u771f\u6b63\u5177\u5907\u7406\u89e3\u548c\u63a8\u7406\u80fd\u529b\uff0c\u63ed\u793a\u5176\u5de5\u4f5c\u539f\u7406\u7684\u672c\u8d28\u9650\u5236\u3002", "method": "\u901a\u8fc7\u5206\u6790LLMs\u7684\u5de5\u4f5c\u539f\u7406\uff0c\u8bba\u8bc1\u5176\u65e0\u6cd5\u5b9e\u73b0\u771f\u6b63\u7684\u6b63\u786e\u63a8\u7406\u3002", "result": "LLMs\u56e0\u672c\u8d28\u9650\u5236\u65e0\u6cd5\u62e5\u6709\u771f\u6b63\u7684\u7406\u89e3\u548c\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "LLMs\u7684\u201c\u7406\u89e3\u201d\u548c\u201c\u63a8\u7406\u201d\u80fd\u529b\u662f\u9519\u89c9\uff0c\u5176\u5de5\u4f5c\u539f\u7406\u51b3\u5b9a\u4e86\u5b83\u4eec\u65e0\u6cd5\u5b9e\u73b0\u771f\u6b63\u7684\u6b63\u786e\u63a8\u7406\u3002"}}
{"id": "2508.10043", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10043", "abs": "https://arxiv.org/abs/2508.10043", "authors": ["Pallavi Zambare", "Venkata Nikhil Thanikella", "Ying Liu"], "title": "Securing Agentic AI: Threat Modeling and Risk Analysis for Network Monitoring Agentic AI System", "comment": "Submitted and under review in IEEE Transactions on Privacy", "summary": "When combining Large Language Models (LLMs) with autonomous agents, used in\nnetwork monitoring and decision-making systems, this will create serious\nsecurity issues. In this research, the MAESTRO framework consisting of the\nseven layers threat modeling architecture in the system was used to expose,\nevaluate, and eliminate vulnerabilities of agentic AI. The prototype agent\nsystem was constructed and implemented, using Python, LangChain, and telemetry\nin WebSockets, and deployed with inference, memory, parameter tuning, and\nanomaly detection modules. Two practical threat cases were confirmed as\nfollows: (i) resource denial of service by traffic replay denial-of-service,\nand (ii) memory poisoning by tampering with the historical log file maintained\nby the agent. These situations resulted in measurable levels of performance\ndegradation, i.e. telemetry updates were delayed, and computational loads were\nincreased, as a result of poor system adaptations. It was suggested to use a\nmultilayered defense-in-depth approach with memory isolation, validation of\nplanners and anomaly response systems in real-time. These findings verify that\nMAESTRO is viable in operational threat mapping, prospective risk scoring, and\nthe basis of the resilient system design. The authors bring attention to the\nimportance of the enforcement of memory integrity, paying attention to the\nadaptation logic monitoring, and cross-layer communication protection that\nguarantee the agentic AI reliability in adversarial settings.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51faMAESTRO\u6846\u67b6\uff0c\u901a\u8fc7\u4e03\u5c42\u5a01\u80c1\u5efa\u6a21\u67b6\u6784\u8bc4\u4f30\u548c\u6d88\u9664\u81ea\u4e3bAI\u4ee3\u7406\u7684\u5b89\u5168\u6f0f\u6d1e\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e0e\u81ea\u4e3b\u4ee3\u7406\u5728\u7f51\u7edc\u76d1\u63a7\u548c\u51b3\u7b56\u7cfb\u7edf\u4e2d\u4f1a\u5f15\u53d1\u4e25\u91cd\u5b89\u5168\u95ee\u9898\uff0c\u9700\u7cfb\u7edf\u6027\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528Python\u3001LangChain\u548cWebSockets\u6784\u5efa\u539f\u578b\u7cfb\u7edf\uff0c\u5305\u542b\u63a8\u7406\u3001\u5185\u5b58\u3001\u53c2\u6570\u8c03\u4f18\u548c\u5f02\u5e38\u68c0\u6d4b\u6a21\u5757\uff0c\u5e76\u901a\u8fc7MAESTRO\u6846\u67b6\u8fdb\u884c\u5a01\u80c1\u5efa\u6a21\u3002", "result": "\u786e\u8ba4\u4e24\u7c7b\u5a01\u80c1\uff08\u8d44\u6e90\u62d2\u7edd\u670d\u52a1\u548c\u5185\u5b58\u6c61\u67d3\uff09\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\uff1b\u5efa\u8bae\u91c7\u7528\u591a\u5c42\u9632\u5fa1\u7b56\u7565\u3002", "conclusion": "MAESTRO\u6846\u67b6\u5728\u5a01\u80c1\u6620\u5c04\u548c\u5f39\u6027\u7cfb\u7edf\u8bbe\u8ba1\u4e2d\u53ef\u884c\uff0c\u5f3a\u8c03\u5185\u5b58\u5b8c\u6574\u6027\u3001\u903b\u8f91\u76d1\u63a7\u548c\u8de8\u5c42\u901a\u4fe1\u4fdd\u62a4\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2508.10293", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10293", "abs": "https://arxiv.org/abs/2508.10293", "authors": ["Chuhuai Yue", "Chengqi Dong", "Yinan Gao", "Hang He", "Jiajun Chai", "Guojun Yin", "Wei Lin"], "title": "Promoting Efficient Reasoning with Verifiable Stepwise Reward", "comment": null, "summary": "Large reasoning models (LRMs) have recently achieved significant progress in\ncomplex reasoning tasks, aided by reinforcement learning with verifiable\nrewards. However, LRMs often suffer from overthinking, expending excessive\ncomputation on simple problems and reducing efficiency. Existing efficient\nreasoning methods typically require accurate task assessment to preset token\nbudgets or select reasoning modes, which limits their flexibility and\nreliability. In this work, we revisit the essence of overthinking and identify\nthat encouraging effective steps while penalizing ineffective ones is key to\nits solution. To this end, we propose a novel rule-based verifiable stepwise\nreward mechanism (VSRM), which assigns rewards based on the performance of\nintermediate states in the reasoning trajectory. This approach is intuitive and\nnaturally fits the step-by-step nature of reasoning tasks. We conduct extensive\nexperiments on standard mathematical reasoning benchmarks, including AIME24 and\nAIME25, by integrating VSRM with PPO and Reinforce++. Results show that our\nmethod achieves substantial output length reduction while maintaining original\nreasoning performance, striking an optimal balance between efficiency and\naccuracy. Further analysis of overthinking frequency and pass@k score before\nand after training demonstrates that our approach in deed effectively\nsuppresses ineffective steps and encourages effective reasoning, fundamentally\nalleviating the overthinking problem. All code will be released upon\nacceptance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c4\u5219\u7684\u9010\u6b65\u9a8c\u8bc1\u5956\u52b1\u673a\u5236\uff08VSRM\uff09\uff0c\u901a\u8fc7\u5956\u52b1\u6709\u6548\u63a8\u7406\u6b65\u9aa4\u5e76\u60e9\u7f5a\u65e0\u6548\u6b65\u9aa4\uff0c\u89e3\u51b3\u4e86\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u5728\u7b80\u5355\u95ee\u9898\u4e0a\u8fc7\u5ea6\u8ba1\u7b97\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6548\u7387\u3002", "motivation": "\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u5b58\u5728\u8fc7\u5ea6\u8ba1\u7b97\uff08overthinking\uff09\u95ee\u9898\uff0c\u964d\u4f4e\u4e86\u6548\u7387\u3002\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u9884\u8bbe\u9884\u7b97\u6216\u9009\u62e9\u63a8\u7406\u6a21\u5f0f\uff0c\u7f3a\u4e4f\u7075\u6d3b\u6027\u548c\u53ef\u9760\u6027\u3002", "method": "\u63d0\u51faVSRM\u673a\u5236\uff0c\u6839\u636e\u63a8\u7406\u8f68\u8ff9\u4e2d\u4e2d\u95f4\u72b6\u6001\u7684\u8868\u73b0\u5206\u914d\u5956\u52b1\uff0c\u7ed3\u5408PPO\u548cReinforce++\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u5728AIME24\u548cAIME25\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cVSRM\u663e\u8457\u51cf\u5c11\u4e86\u8f93\u51fa\u957f\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u63a8\u7406\u6027\u80fd\uff0c\u6709\u6548\u6291\u5236\u4e86\u65e0\u6548\u6b65\u9aa4\u3002", "conclusion": "VSRM\u901a\u8fc7\u5956\u52b1\u6709\u6548\u6b65\u9aa4\u548c\u60e9\u7f5a\u65e0\u6548\u6b65\u9aa4\uff0c\u4ece\u6839\u672c\u4e0a\u7f13\u89e3\u4e86\u8fc7\u5ea6\u8ba1\u7b97\u95ee\u9898\uff0c\u5e73\u8861\u4e86\u6548\u7387\u4e0e\u51c6\u786e\u6027\u3002"}}
{"id": "2508.10044", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10044", "abs": "https://arxiv.org/abs/2508.10044", "authors": ["Aydin Zaboli", "Junho Hong"], "title": "Generative AI for Cybersecurity of Energy Management Systems: Methods, Challenges, and Future Directions", "comment": "36 pages, 10 figures", "summary": "This paper elaborates on an extensive security framework specifically\ndesigned for energy management systems (EMSs), which effectively tackles the\ndynamic environment of cybersecurity vulnerabilities and/or system problems\n(SPs), accomplished through the incorporation of novel methodologies. A\ncomprehensive multi-point attack/error model is initially proposed to\nsystematically identify vulnerabilities throughout the entire EMS data\nprocessing pipeline, including post state estimation (SE) stealth attacks, EMS\ndatabase manipulation, and human-machine interface (HMI) display corruption\naccording to the real-time database (RTDB) storage. This framework acknowledges\nthe interconnected nature of modern attack vectors, which utilize various\nphases of supervisory control and data acquisition (SCADA) data flow. Then,\ngenerative AI (GenAI)-based anomaly detection systems (ADSs) for EMSs are\nproposed for the first time in the power system domain to handle the scenarios.\nFurther, a set-of-mark generative intelligence (SoM-GI) framework, which\nleverages multimodal analysis by integrating visual markers with rules\nconsidering the GenAI capabilities, is suggested to overcome inherent spatial\nreasoning limitations. The SoM-GI methodology employs systematic visual\nindicators to enable accurate interpretation of segmented HMI displays and\ndetect visual anomalies that numerical methods fail to identify. Validation on\nthe IEEE 14-Bus system shows the framework's effectiveness across scenarios,\nwhile visual analysis identifies inconsistencies. This integrated approach\ncombines numerical analysis with visual pattern recognition and linguistic\nrules to protect against cyber threats and system errors.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u80fd\u6e90\u7ba1\u7406\u7cfb\u7edf\uff08EMS\uff09\u7684\u5b89\u5168\u6846\u67b6\uff0c\u7ed3\u5408\u751f\u6210\u5f0fAI\u548c\u89c6\u89c9\u6807\u8bb0\u5206\u6790\uff0c\u6709\u6548\u5e94\u5bf9\u7f51\u7edc\u5b89\u5168\u6f0f\u6d1e\u548c\u7cfb\u7edf\u95ee\u9898\u3002", "motivation": "\u73b0\u4ee3\u80fd\u6e90\u7ba1\u7406\u7cfb\u7edf\u9762\u4e34\u590d\u6742\u7684\u7f51\u7edc\u5b89\u5168\u5a01\u80c1\u548c\u7cfb\u7edf\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u7efc\u5408\u6027\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u591a\u70b9\u591a\u70b9\u653b\u51fb/\u9519\u8bef\u6a21\u578b\uff0c\u7ed3\u5408\u751f\u6210\u5f0fAI\u5f02\u5e38\u68c0\u6d4b\u7cfb\u7edf\u548c\u89c6\u89c9\u6807\u8bb0\u751f\u6210\u667a\u80fd\u6846\u67b6\uff08SoM-GI\uff09\u3002", "result": "\u5728IEEE 14-Bus\u7cfb\u7edf\u4e0a\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u6210\u529f\u68c0\u6d4b\u5230\u89c6\u89c9\u5f02\u5e38\u548c\u6f0f\u6d1e\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u6570\u503c\u5206\u6790\u548c\u89c6\u89c9\u6a21\u5f0f\u8bc6\u522b\u7684\u7ed3\u5408\uff0c\u4e3aEMS\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u5b89\u5168\u4fdd\u969c\u3002"}}
{"id": "2508.10337", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.10337", "abs": "https://arxiv.org/abs/2508.10337", "authors": ["Chenliang Zhang", "Lin Wang", "Yuanyuan Lu", "Yusheng Qi", "Kexin Wang", "Peixu Hou", "Wenshi Chen"], "title": "A Curriculum Learning Approach to Reinforcement Learning: Leveraging RAG for Multimodal Question Answering", "comment": null, "summary": "This paper describes the solutions of the Dianping-Trust-Safety team for the\nMETA CRAG-MM challenge. The challenge requires building a comprehensive\nretrieval-augmented generation system capable for multi-modal multi-turn\nquestion answering. The competition consists of three tasks: (1) answering\nquestions using structured data retrieved from an image-based mock knowledge\ngraph, (2) synthesizing information from both knowledge graphs and web search\nresults, and (3) handling multi-turn conversations that require context\nunderstanding and information aggregation from multiple sources. For Task 1,\nour solution is based on the vision large language model, enhanced by\nsupervised fine-tuning with knowledge distilled from GPT-4.1. We further\napplied curriculum learning strategies to guide reinforcement learning,\nresulting in improved answer accuracy and reduced hallucination. For Task 2 and\nTask 3, we additionally leveraged web search APIs to incorporate external\nknowledge, enabling the system to better handle complex queries and multi-turn\nconversations. Our approach achieved 1st place in Task 1 with a significant\nlead of 52.38\\%, and 3rd place in Task 3, demonstrating the effectiveness of\nthe integration of curriculum learning with reinforcement learning in our\ntraining pipeline.", "AI": {"tldr": "Dianping-Trust-Safety\u56e2\u961f\u5728META CRAG-MM\u6311\u6218\u4e2d\u63d0\u51fa\u591a\u6a21\u6001\u591a\u8f6e\u95ee\u7b54\u7cfb\u7edf\u89e3\u51b3\u65b9\u6848\uff0c\u7ed3\u5408\u89c6\u89c9\u5927\u8bed\u8a00\u6a21\u578b\u3001\u5f3a\u5316\u5b66\u4e60\u548c\u5916\u90e8\u77e5\u8bc6\uff0c\u4efb\u52a11\u83b7\u7b2c\u4e00\uff0c\u4efb\u52a13\u83b7\u7b2c\u4e09\u3002", "motivation": "\u89e3\u51b3\u591a\u6a21\u6001\u591a\u8f6e\u95ee\u7b54\u7684\u590d\u6742\u9700\u6c42\uff0c\u6574\u5408\u7ed3\u6784\u5316\u6570\u636e\u548c\u5916\u90e8\u77e5\u8bc6\uff0c\u63d0\u5347\u95ee\u7b54\u7cfb\u7edf\u7684\u51c6\u786e\u6027\u548c\u4e0a\u4e0b\u6587\u7406\u89e3\u80fd\u529b\u3002", "method": "\u4efb\u52a11\u4f7f\u7528\u89c6\u89c9\u5927\u8bed\u8a00\u6a21\u578b\u5e76\u57fa\u4e8eGPT-4.1\u77e5\u8bc6\u84b8\u998f\u5fae\u8c03\uff0c\u7ed3\u5408\u8bfe\u7a0b\u5b66\u4e60\u4f18\u5316\u5f3a\u5316\u5b66\u4e60\uff1b\u4efb\u52a12\u548c3\u5f15\u5165\u7f51\u7edc\u641c\u7d22API\u8865\u5145\u5916\u90e8\u77e5\u8bc6\u3002", "result": "\u4efb\u52a11\u4ee552.38%\u4f18\u52bf\u83b7\u7b2c\u4e00\uff0c\u4efb\u52a13\u83b7\u7b2c\u4e09\uff0c\u9a8c\u8bc1\u4e86\u8bfe\u7a0b\u5b66\u4e60\u4e0e\u5f3a\u5316\u5b66\u4e60\u7ed3\u5408\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u591a\u6a21\u6001\u591a\u8f6e\u95ee\u7b54\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u5728\u590d\u6742\u67e5\u8be2\u548c\u4e0a\u4e0b\u6587\u7406\u89e3\u65b9\u9762\u5177\u6709\u4f18\u52bf\u3002"}}
{"id": "2508.10052", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10052", "abs": "https://arxiv.org/abs/2508.10052", "authors": ["Pallavi Zambare", "Venkata Nikhil Thanikella", "Nikhil Padmanabh Kottur", "Sree Akhil Akula", "Ying Liu"], "title": "NetMoniAI: An Agentic AI Framework for Network Security & Monitoring", "comment": "Accepted in IEEE 3rd International Conference on Artificial\n  Intelligence, Blockchain, and Internet of Things (AIBThings 2025)", "summary": "In this paper, we present NetMoniAI, an agentic AI framework for automatic\nnetwork monitoring and security that integrates decentralized analysis with\nlightweight centralized coordination. The framework consists of two layers:\nautonomous micro-agents at each node perform local traffic analysis and anomaly\ndetection. A central controller then aggregates insights across nodes to detect\ncoordinated attacks and maintain system-wide situational awareness. We\nevaluated NetMoniAI on a local micro-testbed and through NS-3 simulations.\nResults confirm that the two-tier agentic-AI design scales under resource\nconstraints, reduces redundancy, and improves response time without\ncompromising accuracy. To facilitate broader adoption and reproducibility, the\ncomplete framework is available as open source. This enables researchers and\npractitioners to replicate, validate, and extend it across diverse network\nenvironments and threat scenarios. Github link:\nhttps://github.com/pzambare3/NetMoniAI", "AI": {"tldr": "NetMoniAI\u662f\u4e00\u4e2a\u7528\u4e8e\u81ea\u52a8\u7f51\u7edc\u76d1\u63a7\u548c\u5b89\u5168\u7684AI\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u5206\u6563\u5206\u6790\u548c\u8f7b\u91cf\u7ea7\u96c6\u4e2d\u534f\u8c03\uff0c\u901a\u8fc7\u4e24\u5c42\u8bbe\u8ba1\u5b9e\u73b0\u9ad8\u6548\u68c0\u6d4b\u548c\u54cd\u5e94\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u7f51\u7edc\u76d1\u63a7\u5728\u8d44\u6e90\u53d7\u9650\u548c\u590d\u6742\u653b\u51fb\u573a\u666f\u4e0b\u7684\u5197\u4f59\u548c\u54cd\u5e94\u5ef6\u8fdf\u95ee\u9898\u3002", "method": "\u91c7\u7528\u4e24\u5c42\u67b6\u6784\uff1a\u8282\u70b9\u7ea7\u5fae\u4ee3\u7406\u8fdb\u884c\u672c\u5730\u5206\u6790\uff0c\u4e2d\u592e\u63a7\u5236\u5668\u805a\u5408\u5168\u5c40\u4fe1\u606f\u3002", "result": "\u5728\u6d4b\u8bd5\u548c\u6a21\u62df\u4e2d\u9a8c\u8bc1\u4e86\u5176\u53ef\u6269\u5c55\u6027\u3001\u4f4e\u5197\u4f59\u548c\u5feb\u901f\u54cd\u5e94\u80fd\u529b\u3002", "conclusion": "\u5f00\u6e90\u6846\u67b6\u4fbf\u4e8e\u7814\u7a76\u548c\u5b9e\u8df5\u5e94\u7528\uff0c\u652f\u6301\u591a\u6837\u5316\u7f51\u7edc\u73af\u5883\u548c\u5a01\u80c1\u573a\u666f\u3002"}}
{"id": "2508.10340", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10340", "abs": "https://arxiv.org/abs/2508.10340", "authors": ["Chak Lam Shek", "Guangyao Shi", "Pratap Tokekar"], "title": "Multi-Agent Trust Region Policy Optimisation: A Joint Constraint Approach", "comment": null, "summary": "Multi-agent reinforcement learning (MARL) requires coordinated and stable\npolicy updates among interacting agents. Heterogeneous-Agent Trust Region\nPolicy Optimization (HATRPO) enforces per-agent trust region constraints using\nKullback-Leibler (KL) divergence to stabilize training. However, assigning each\nagent the same KL threshold can lead to slow and locally optimal updates,\nespecially in heterogeneous settings. To address this limitation, we propose\ntwo approaches for allocating the KL divergence threshold across agents:\nHATRPO-W, a Karush-Kuhn-Tucker-based (KKT-based) method that optimizes\nthreshold assignment under global KL constraints, and HATRPO-G, a greedy\nalgorithm that prioritizes agents based on improvement-to-divergence ratio. By\nconnecting sequential policy optimization with constrained threshold\nscheduling, our approach enables more flexible and effective learning in\nheterogeneous-agent settings. Experimental results demonstrate that our methods\nsignificantly boost the performance of HATRPO, achieving faster convergence and\nhigher final rewards across diverse MARL benchmarks. Specifically, HATRPO-W and\nHATRPO-G achieve comparable improvements in final performance, each exceeding\n22.5%. Notably, HATRPO-W also demonstrates more stable learning dynamics, as\nreflected by its lower variance.", "AI": {"tldr": "HATRPO-W\u548cHATRPO-G\u901a\u8fc7\u52a8\u6001\u5206\u914dKL\u9608\u503c\u4f18\u5316\u4e86HATRPO\u5728\u5f02\u6784\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u6027\u80fd\uff0c\u5206\u522b\u63d0\u5347\u4e8622.5%\u7684\u6700\u7ec8\u8868\u73b0\uff0c\u4e14HATRPO-W\u66f4\u7a33\u5b9a\u3002", "motivation": "HATRPO\u4e2d\u56fa\u5b9aKL\u9608\u503c\u5728\u5f02\u6784\u73af\u5883\u4e0b\u53ef\u80fd\u5bfc\u81f4\u66f4\u65b0\u7f13\u6162\u548c\u5c40\u90e8\u6700\u4f18\uff0c\u9700\u52a8\u6001\u5206\u914d\u9608\u503c\u4ee5\u63d0\u5347\u5b66\u4e60\u6548\u679c\u3002", "method": "\u63d0\u51faHATRPO-W\uff08\u57fa\u4e8eKKT\u4f18\u5316\u5168\u5c40KL\u7ea6\u675f\uff09\u548cHATRPO-G\uff08\u57fa\u4e8e\u8d2a\u5a6a\u7b97\u6cd5\u6309\u6539\u8fdb-\u6563\u5ea6\u6bd4\u5206\u914d\u9608\u503c\uff09\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u4e24\u79cd\u65b9\u6cd5\u663e\u8457\u63d0\u5347HATRPO\u6027\u80fd\uff0c\u6536\u655b\u66f4\u5feb\uff0c\u6700\u7ec8\u5956\u52b1\u66f4\u9ad8\uff0cHATRPO-W\u65b9\u5dee\u66f4\u4f4e\u3002", "conclusion": "\u52a8\u6001KL\u9608\u503c\u5206\u914d\u5728\u5f02\u6784MARL\u4e2d\u66f4\u7075\u6d3b\u6709\u6548\uff0cHATRPO-W\u548cHATRPO-G\u5747\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2508.10065", "categories": ["cs.CR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10065", "abs": "https://arxiv.org/abs/2508.10065", "authors": ["Yuhao Sun", "Yihua Zhang", "Gaowen Liu", "Hongtao Xie", "Sijia Liu"], "title": "Invisible Watermarks, Visible Gains: Steering Machine Unlearning with Bi-Level Watermarking Design", "comment": "Accepted by ICCV 2025", "summary": "With the increasing demand for the right to be forgotten, machine unlearning\n(MU) has emerged as a vital tool for enhancing trust and regulatory compliance\nby enabling the removal of sensitive data influences from machine learning (ML)\nmodels. However, most MU algorithms primarily rely on in-training methods to\nadjust model weights, with limited exploration of the benefits that data-level\nadjustments could bring to the unlearning process. To address this gap, we\npropose a novel approach that leverages digital watermarking to facilitate MU\nby strategically modifying data content. By integrating watermarking, we\nestablish a controlled unlearning mechanism that enables precise removal of\nspecified data while maintaining model utility for unrelated tasks. We first\nexamine the impact of watermarked data on MU, finding that MU effectively\ngeneralizes to watermarked data. Building on this, we introduce an\nunlearning-friendly watermarking framework, termed Water4MU, to enhance\nunlearning effectiveness. The core of Water4MU is a bi-level optimization (BLO)\nframework: at the upper level, the watermarking network is optimized to\nminimize unlearning difficulty, while at the lower level, the model itself is\ntrained independently of watermarking. Experimental results demonstrate that\nWater4MU is effective in MU across both image classification and image\ngeneration tasks. Notably, it outperforms existing methods in challenging MU\nscenarios, known as \"challenging forgets\".", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6570\u5b57\u6c34\u5370\u7684\u673a\u5668\u9057\u5fd8\uff08MU\uff09\u65b0\u65b9\u6cd5Water4MU\uff0c\u901a\u8fc7\u6570\u636e\u7ea7\u8c03\u6574\u4f18\u5316\u9057\u5fd8\u6548\u679c\uff0c\u5e76\u5728\u56fe\u50cf\u5206\u7c7b\u548c\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u968f\u7740\u2018\u88ab\u9057\u5fd8\u6743\u2019\u9700\u6c42\u7684\u589e\u52a0\uff0c\u673a\u5668\u9057\u5fd8\u6210\u4e3a\u63d0\u5347\u4fe1\u4efb\u548c\u5408\u89c4\u6027\u7684\u91cd\u8981\u5de5\u5177\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u6a21\u578b\u6743\u91cd\u8c03\u6574\uff0c\u6570\u636e\u7ea7\u8c03\u6574\u7684\u6f5c\u529b\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002", "method": "\u5229\u7528\u6570\u5b57\u6c34\u5370\u6280\u672f\uff0c\u63d0\u51faWater4MU\u6846\u67b6\uff0c\u91c7\u7528\u53cc\u5c42\u4f18\u5316\uff08BLO\uff09\u8bbe\u8ba1\uff1a\u4e0a\u5c42\u4f18\u5316\u6c34\u5370\u7f51\u7edc\u4ee5\u964d\u4f4e\u9057\u5fd8\u96be\u5ea6\uff0c\u4e0b\u5c42\u72ec\u7acb\u8bad\u7ec3\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cWater4MU\u5728\u56fe\u50cf\u5206\u7c7b\u548c\u751f\u6210\u4efb\u52a1\u4e2d\u5747\u6709\u6548\uff0c\u5c24\u5176\u5728\u2018\u6311\u6218\u6027\u9057\u5fd8\u2019\u573a\u666f\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "Water4MU\u901a\u8fc7\u6570\u636e\u7ea7\u6c34\u5370\u8c03\u6574\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u9057\u5fd8\u7684\u7cbe\u786e\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2508.10358", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10358", "abs": "https://arxiv.org/abs/2508.10358", "authors": ["Mengtao Zhou", "Sifan Wu", "Huan Zhang", "Qi Sima", "Bang Liu"], "title": "What to Ask Next? Probing the Imaginative Reasoning of LLMs with TurtleSoup Puzzles", "comment": null, "summary": "We investigate the capacity of Large Language Models (LLMs) for imaginative\nreasoning--the proactive construction, testing, and revision of hypotheses in\ninformation-sparse environments. Existing benchmarks, often static or focused\non social deduction, fail to capture the dynamic, exploratory nature of this\nreasoning process. To address this gap, we introduce a comprehensive research\nframework based on the classic \"Turtle Soup\" game, integrating a benchmark, an\nagent, and an evaluation protocol. We present TurtleSoup-Bench, the first\nlarge-scale, bilingual, interactive benchmark for imaginative reasoning,\ncomprising 800 turtle soup puzzles sourced from both the Internet and expert\nauthors. We also propose Mosaic-Agent, a novel agent designed to assess LLMs'\nperformance in this setting. To evaluate reasoning quality, we develop a\nmulti-dimensional protocol measuring logical consistency, detail completion,\nand conclusion alignment. Experiments with leading LLMs reveal clear capability\nlimits, common failure patterns, and a significant performance gap compared to\nhumans. Our work offers new insights into LLMs' imaginative reasoning and\nestablishes a foundation for future research on exploratory agent behavior.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u4fe1\u606f\u7a00\u758f\u73af\u5883\u4e2d\u7684\u60f3\u8c61\u63a8\u7406\u80fd\u529b\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u201c\u4e4c\u9f9f\u6c64\u201d\u6e38\u620f\u7684\u6846\u67b6\uff0c\u5305\u62ec\u57fa\u51c6\u3001\u4ee3\u7406\u548c\u8bc4\u4f30\u534f\u8bae\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u591a\u4e3a\u9759\u6001\u6216\u5173\u6ce8\u793e\u4f1a\u63a8\u7406\uff0c\u65e0\u6cd5\u6355\u6349\u52a8\u6001\u63a2\u7d22\u6027\u63a8\u7406\u8fc7\u7a0b\u3002", "method": "\u5f15\u5165\u57fa\u4e8e\u201c\u4e4c\u9f9f\u6c64\u201d\u6e38\u620f\u7684\u6846\u67b6\uff0c\u5f00\u53d1\u4e86TurtleSoup-Bench\u57fa\u51c6\u548cMosaic-Agent\u4ee3\u7406\uff0c\u5e76\u8bbe\u8ba1\u4e86\u591a\u7ef4\u8bc4\u4f30\u534f\u8bae\u3002", "result": "\u5b9e\u9a8c\u663e\u793aLLMs\u5728\u60f3\u8c61\u63a8\u7406\u4e2d\u5b58\u5728\u80fd\u529b\u9650\u5236\u548c\u5e38\u89c1\u5931\u8d25\u6a21\u5f0f\uff0c\u4e0e\u4eba\u7c7b\u8868\u73b0\u6709\u663e\u8457\u5dee\u8ddd\u3002", "conclusion": "\u7814\u7a76\u4e3aLLMs\u7684\u60f3\u8c61\u63a8\u7406\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\uff0c\u5e76\u4e3a\u672a\u6765\u63a2\u7d22\u6027\u4ee3\u7406\u884c\u4e3a\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.10185", "categories": ["cs.CR", "cs.CY", "cs.NI"], "pdf": "https://arxiv.org/pdf/2508.10185", "abs": "https://arxiv.org/abs/2508.10185", "authors": ["Ren\u00e9 Mayrhofer", "Michael Roland", "Tobias H\u00f6ller", "Philipp Hofer", "Mario Lins"], "title": "An Architecture for Distributed Digital Identities in the Physical World", "comment": null, "summary": "Digital identities are increasingly important for mediating not only digital\nbut also physical service transactions. Managing such identities through\ncentralized providers can cause both availability and privacy concerns: single\npoints of failure and control are ideal targets for global attacks on\ntechnical, organizational, or legal fronts. We design, analyze, and build a\ndistributed digital identity architecture for physical world transactions in\ncommon scenarios like unlocking doors, public transport, or crossing country\nborders. This architecture combines (biometric and other) sensors, (established\nand upcoming) identity authorities, attribute verifiers, and a new core\ncomponent we call the \\emph{Personal Identity Agent (PIA)} that represents\nindividuals with their identity attributes in the digital domain. All\ntransactions are conducted in a completely decentralized manner, and the\ncomponents for which we currently assume central coordination are optional and\nonly used for assisting with service discovery and latency reduction. We\npresent a first protocol between these parties and formally verify that it\nachieves relevant security properties based on a realistic threat model\nincluding strong global adversaries. A proof-of-concept implementation\ndemonstrates practical feasibility of both architecture and initial protocol\nfor applications that can tolerate end-to-end latencies in the range of a few\nseconds.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5e03\u5f0f\u6570\u5b57\u8eab\u4efd\u67b6\u6784\uff0c\u7528\u4e8e\u7269\u7406\u4e16\u754c\u4ea4\u6613\uff0c\u901a\u8fc7\u53bb\u4e2d\u5fc3\u5316\u65b9\u5f0f\u89e3\u51b3\u96c6\u4e2d\u5f0f\u8eab\u4efd\u7ba1\u7406\u7684\u53ef\u7528\u6027\u548c\u9690\u79c1\u95ee\u9898\u3002", "motivation": "\u96c6\u4e2d\u5f0f\u8eab\u4efd\u7ba1\u7406\u5b58\u5728\u5355\u70b9\u6545\u969c\u548c\u9690\u79c1\u98ce\u9669\uff0c\u9700\u8981\u4e00\u79cd\u53bb\u4e2d\u5fc3\u5316\u89e3\u51b3\u65b9\u6848\u6765\u652f\u6301\u7269\u7406\u4e16\u754c\u4ea4\u6613\u3002", "method": "\u8bbe\u8ba1\u4e86\u7ed3\u5408\u4f20\u611f\u5668\u3001\u8eab\u4efd\u673a\u6784\u3001\u5c5e\u6027\u9a8c\u8bc1\u5668\u548c\u4e2a\u4eba\u8eab\u4efd\u4ee3\u7406\uff08PIA\uff09\u7684\u5206\u5e03\u5f0f\u67b6\u6784\uff0c\u5e76\u63d0\u51fa\u4e86\u9996\u4e2a\u534f\u8bae\u3002", "result": "\u534f\u8bae\u5728\u5f3a\u5168\u5c40\u5bf9\u624b\u5a01\u80c1\u6a21\u578b\u4e0b\u5b9e\u73b0\u4e86\u76f8\u5173\u5b89\u5168\u5c5e\u6027\uff0c\u6982\u5ff5\u9a8c\u8bc1\u8bc1\u660e\u4e86\u67b6\u6784\u548c\u534f\u8bae\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u5206\u5e03\u5f0f\u6570\u5b57\u8eab\u4efd\u67b6\u6784\u5728\u5bb9\u5fcd\u51e0\u79d2\u5ef6\u8fdf\u7684\u5e94\u7528\u4e2d\u5177\u6709\u5b9e\u9645\u53ef\u884c\u6027\u3002"}}
{"id": "2508.10391", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10391", "abs": "https://arxiv.org/abs/2508.10391", "authors": ["Yaoze Zhang", "Rong Wu", "Pinlong Cai", "Xiaoman Wang", "Guohang Yan", "Song Mao", "Ding Wang", "Botian Shi"], "title": "LeanRAG: Knowledge-Graph-Based Generation with Semantic Aggregation and Hierarchical Retrieval", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) plays a crucial role in grounding Large\nLanguage Models by leveraging external knowledge, whereas the effectiveness is\noften compromised by the retrieval of contextually flawed or incomplete\ninformation. To address this, knowledge graph-based RAG methods have evolved\ntowards hierarchical structures, organizing knowledge into multi-level\nsummaries. However, these approaches still suffer from two critical,\nunaddressed challenges: high-level conceptual summaries exist as disconnected\n``semantic islands'', lacking the explicit relations needed for cross-community\nreasoning; and the retrieval process itself remains structurally unaware, often\ndegenerating into an inefficient flat search that fails to exploit the graph's\nrich topology. To overcome these limitations, we introduce LeanRAG, a framework\nthat features a deeply collaborative design combining knowledge aggregation and\nretrieval strategies. LeanRAG first employs a novel semantic aggregation\nalgorithm that forms entity clusters and constructs new explicit relations\namong aggregation-level summaries, creating a fully navigable semantic network.\nThen, a bottom-up, structure-guided retrieval strategy anchors queries to the\nmost relevant fine-grained entities and then systematically traverses the\ngraph's semantic pathways to gather concise yet contextually comprehensive\nevidence sets. The LeanRAG can mitigate the substantial overhead associated\nwith path retrieval on graphs and minimizes redundant information retrieval.\nExtensive experiments on four challenging QA benchmarks with different domains\ndemonstrate that LeanRAG significantly outperforming existing methods in\nresponse quality while reducing 46\\% retrieval redundancy. Code is available\nat: https://github.com/RaZzzyz/LeanRAG", "AI": {"tldr": "LeanRAG\u662f\u4e00\u4e2a\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u4e49\u805a\u5408\u548c\u7ed3\u6784\u611f\u77e5\u68c0\u7d22\u7b56\u7565\u63d0\u5347\u68c0\u7d22\u6548\u7387\u548c\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684RAG\u65b9\u6cd5\u5b58\u5728\u8bed\u4e49\u5b64\u5c9b\u548c\u68c0\u7d22\u6548\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u9700\u8981\u6539\u8fdb\u3002", "method": "\u91c7\u7528\u8bed\u4e49\u805a\u5408\u7b97\u6cd5\u6784\u5efa\u5b9e\u4f53\u96c6\u7fa4\u548c\u663e\u5f0f\u5173\u7cfb\uff0c\u7ed3\u5408\u81ea\u5e95\u5411\u4e0a\u7684\u7ed3\u6784\u611f\u77e5\u68c0\u7d22\u7b56\u7565\u3002", "result": "\u5728\u56db\u4e2aQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u54cd\u5e94\u8d28\u91cf\u663e\u8457\u63d0\u5347\uff0c\u68c0\u7d22\u5197\u4f59\u51cf\u5c1146%\u3002", "conclusion": "LeanRAG\u901a\u8fc7\u8bed\u4e49\u7f51\u7edc\u548c\u9ad8\u6548\u68c0\u7d22\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86RAG\u7684\u6027\u80fd\u548c\u6548\u7387\u3002"}}
{"id": "2508.10212", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.10212", "abs": "https://arxiv.org/abs/2508.10212", "authors": ["Md Sazedur Rahman", "Mohamed Elmahallawy", "Sanjay Madria", "Samuel Frimpong"], "title": "Detecting Untargeted Attacks and Mitigating Unreliable Updates in Federated Learning for Underground Mining Operations", "comment": null, "summary": "Underground mining operations rely on distributed sensor networks to collect\ncritical data daily, including mine temperature, toxic gas concentrations, and\nminer movements for hazard detection and operational decision-making. However,\ntransmitting raw sensor data to a central server for training deep learning\nmodels introduces significant privacy risks, potentially exposing sensitive\nmine-specific information. Federated Learning (FL) offers a transformative\nsolution by enabling collaborative model training while ensuring that raw data\nremains localized at each mine. Despite its advantages, FL in underground\nmining faces key challenges: (i) An attacker may compromise a mine's local\nmodel by employing techniques such as sign-flipping attacks or additive noise,\nleading to erroneous predictions; (ii) Low-quality (yet potentially valuable)\ndata, caused by poor lighting conditions or sensor inaccuracies in mines may\ndegrade the FL training process. In response, this paper proposes MineDetect, a\ndefense FL framework that detects and isolates the attacked models while\nmitigating the impact of mines with low-quality data. MineDetect introduces two\nkey innovations: (i) Detecting attacked models (maliciously manipulated) by\ndeveloping a history-aware mechanism that leverages local and global averages\nof gradient updates; (ii) Identifying and eliminating adversarial influences\nfrom unreliable models (generated by clients with poor data quality) on the FL\ntraining process. Comprehensive simulations across diverse datasets demonstrate\nthat MineDetect outperforms existing methods in both robustness and accuracy,\neven in challenging non-IID data scenarios. Its ability to counter adversarial\ninfluences while maintaining lower computational efficiency makes it a vital\nadvancement for improving safety and operational effectiveness in underground\nmining.", "AI": {"tldr": "MineDetect\u662f\u4e00\u4e2a\u9488\u5bf9\u5730\u4e0b\u91c7\u77ff\u4e2d\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u7684\u9632\u5fa1\u6846\u67b6\uff0c\u901a\u8fc7\u68c0\u6d4b\u548c\u9694\u79bb\u53d7\u653b\u51fb\u6a21\u578b\u4ee5\u53ca\u5904\u7406\u4f4e\u8d28\u91cf\u6570\u636e\uff0c\u63d0\u5347FL\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u5730\u4e0b\u91c7\u77ff\u4f9d\u8d56\u4f20\u611f\u5668\u7f51\u7edc\u6536\u96c6\u6570\u636e\uff0c\u4f46\u96c6\u4e2d\u5f0f\u8bad\u7ec3\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5b58\u5728\u9690\u79c1\u98ce\u9669\u3002FL\u867d\u80fd\u89e3\u51b3\u9690\u79c1\u95ee\u9898\uff0c\u4f46\u4ecd\u9762\u4e34\u6a21\u578b\u653b\u51fb\u548c\u4f4e\u8d28\u91cf\u6570\u636e\u7684\u6311\u6218\u3002", "method": "\u63d0\u51faMineDetect\u6846\u67b6\uff0c\u5305\u62ec\u5386\u53f2\u611f\u77e5\u673a\u5236\u68c0\u6d4b\u53d7\u653b\u51fb\u6a21\u578b\uff0c\u4ee5\u53ca\u8bc6\u522b\u5e76\u6d88\u9664\u4f4e\u8d28\u91cf\u6570\u636e\u5bf9FL\u8bad\u7ec3\u7684\u5f71\u54cd\u3002", "result": "\u5728\u591a\u6837\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMineDetect\u5728\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u975e\u72ec\u7acb\u540c\u5206\u5e03\u6570\u636e\u573a\u666f\u4e2d\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "MineDetect\u901a\u8fc7\u62b5\u5fa1\u653b\u51fb\u548c\u5904\u7406\u4f4e\u8d28\u91cf\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5730\u4e0b\u91c7\u77ff\u4e2dFL\u7684\u5b89\u5168\u6027\u548c\u64cd\u4f5c\u6548\u7387\u3002"}}
{"id": "2508.10425", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.10425", "abs": "https://arxiv.org/abs/2508.10425", "authors": ["Yan Ting Chok", "Soyon Park", "Seungheun Baek", "Hajung Kim", "Junhyun Lee", "Jaewoo Kang"], "title": "HiRef: Leveraging Hierarchical Ontology and Network Refinement for Robust Medication Recommendation", "comment": null, "summary": "Medication recommendation is a crucial task for assisting physicians in\nmaking timely decisions from longitudinal patient medical records. However,\nreal-world EHR data present significant challenges due to the presence of\nrarely observed medical entities and incomplete records that may not fully\ncapture the clinical ground truth. While data-driven models trained on\nlongitudinal Electronic Health Records often achieve strong empirical\nperformance, they struggle to generalize under missing or novel conditions,\nlargely due to their reliance on observed co-occurrence patterns. To address\nthese issues, we propose Hierarchical Ontology and Network Refinement for\nRobust Medication Recommendation (HiRef), a unified framework that combines two\ncomplementary structures: (i) the hierarchical semantics encoded in curated\nmedical ontologies, and (ii) refined co-occurrence patterns derived from\nreal-world EHRs. We embed ontology entities in hyperbolic space, which\nnaturally captures tree-like relationships and enables knowledge transfer\nthrough shared ancestors, thereby improving generalizability to unseen codes.\nTo further improve robustness, we introduce a prior-guided sparse\nregularization scheme that refines the EHR co-occurrence graph by suppressing\nspurious edges while preserving clinically meaningful associations. Our model\nachieves strong performance on EHR benchmarks (MIMIC-III and MIMIC-IV) and\nmaintains high accuracy under simulated unseen-code settings. Extensive\nexperiments with comprehensive ablation studies demonstrate HiRef's resilience\nto unseen medical codes, supported by in-depth analyses of the learned\nsparsified graph structure and medical code embeddings.", "AI": {"tldr": "HiRef\u6846\u67b6\u7ed3\u5408\u533b\u5b66\u672c\u4f53\u5c42\u6b21\u7ed3\u6784\u548c\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff08EHR\uff09\u5171\u73b0\u6a21\u5f0f\uff0c\u901a\u8fc7\u53cc\u66f2\u7a7a\u95f4\u5d4c\u5165\u548c\u7a00\u758f\u6b63\u5219\u5316\u63d0\u5347\u836f\u7269\u63a8\u8350\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u89e3\u51b3EHR\u6570\u636e\u4e2d\u7f55\u89c1\u5b9e\u4f53\u548c\u7f3a\u5931\u8bb0\u5f55\u5bfc\u81f4\u7684\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u533b\u5b66\u672c\u4f53\u5c42\u6b21\u7ed3\u6784\u548cEHR\u5171\u73b0\u6a21\u5f0f\uff0c\u4f7f\u7528\u53cc\u66f2\u7a7a\u95f4\u5d4c\u5165\u548c\u7a00\u758f\u6b63\u5219\u5316\u4f18\u5316\u6a21\u578b\u3002", "result": "\u5728MIMIC-III\u548cMIMIC-IV\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u5728\u6a21\u62df\u672a\u89c1\u4ee3\u7801\u573a\u666f\u4e0b\u4fdd\u6301\u9ad8\u51c6\u786e\u6027\u3002", "conclusion": "HiRef\u901a\u8fc7\u672c\u4f53\u548cEHR\u7ed3\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u836f\u7269\u63a8\u8350\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2508.10327", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.10327", "abs": "https://arxiv.org/abs/2508.10327", "authors": ["Haoyang Hu", "Xun Huang", "Chenyu Wu", "Shiwen Liu", "Zhichao Lian", "Shuangquan Zhang"], "title": "BERTector: Intrusion Detection Based on Joint-Dataset Learning", "comment": null, "summary": "Intrusion detection systems (IDS) are facing challenges in generalization and\nrobustness due to the heterogeneity of network traffic and the diversity of\nattack patterns. To address this issue, we propose a new joint-dataset training\nparadigm for IDS and propose a scalable BERTector framework based on BERT.\nBERTector integrates three key components: NSS-Tokenizer for traffic-aware\nsemantic tokenization, supervised fine-tuning with a hybrid dataset, and\nlow-rank adaptation (LoRA) for efficient training. Extensive experiments show\nthat BERTector achieves state-of-the-art detection accuracy, strong\ncross-dataset generalization capabilities, and excellent robustness to\nadversarial perturbations. This work establishes a unified and efficient\nsolution for modern IDS in complex and dynamic network environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eBERT\u7684IDS\u6846\u67b6BERTector\uff0c\u901a\u8fc7\u8054\u5408\u6570\u636e\u96c6\u8bad\u7ec3\u548c\u9ad8\u6548\u7ec4\u4ef6\uff0c\u5b9e\u73b0\u4e86\u9ad8\u68c0\u6d4b\u7cbe\u5ea6\u548c\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3IDS\u5728\u5f02\u6784\u7f51\u7edc\u6d41\u91cf\u548c\u591a\u6837\u5316\u653b\u51fb\u6a21\u5f0f\u4e0b\u7684\u6cdb\u5316\u4e0e\u9c81\u68d2\u6027\u95ee\u9898\u3002", "method": "\u7ed3\u5408NSS-Tokenizer\u3001\u6df7\u5408\u6570\u636e\u96c6\u76d1\u7763\u5fae\u8c03\u548cLoRA\u9ad8\u6548\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u663e\u793aBERTector\u5728\u68c0\u6d4b\u7cbe\u5ea6\u3001\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u548c\u5bf9\u6297\u6270\u52a8\u9c81\u68d2\u6027\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u4e3a\u590d\u6742\u52a8\u6001\u7f51\u7edc\u73af\u5883\u4e2d\u7684IDS\u63d0\u4f9b\u4e86\u7edf\u4e00\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.10429", "categories": ["cs.AI", "cs.CR", "cs.CV", "I.2.10; I.2.6"], "pdf": "https://arxiv.org/pdf/2508.10429", "abs": "https://arxiv.org/abs/2508.10429", "authors": ["Yi Dong", "Yusuke Muraoka", "Scott Shi", "Yi Zhang"], "title": "MM-Food-100K: A 100,000-Sample Multimodal Food Intelligence Dataset with Verifiable Provenance", "comment": "10 pages, 5 figures, 6 tables. The dataset is available at\n  https://huggingface.co/datasets/Codatta/MM-Food-100K", "summary": "We present MM-Food-100K, a public 100,000-sample multimodal food intelligence\ndataset with verifiable provenance. It is a curated approximately 10% open\nsubset of an original 1.2 million, quality-accepted corpus of food images\nannotated for a wide range of information (such as dish name, region of\ncreation). The corpus was collected over six weeks from over 87,000\ncontributors using the Codatta contribution model, which combines community\nsourcing with configurable AI-assisted quality checks; each submission is\nlinked to a wallet address in a secure off-chain ledger for traceability, with\na full on-chain protocol on the roadmap. We describe the schema, pipeline, and\nQA, and validate utility by fine-tuning large vision-language models (ChatGPT\n5, ChatGPT OSS, Qwen-Max) on image-based nutrition prediction. Fine-tuning\nyields consistent gains over out-of-box baselines across standard metrics; we\nreport results primarily on the MM-Food-100K subset. We release MM-Food-100K\nfor publicly free access and retain approximately 90% for potential commercial\naccess with revenue sharing to contributors.", "AI": {"tldr": "MM-Food-100K\u662f\u4e00\u4e2a\u516c\u5f00\u768410\u4e07\u6837\u672c\u591a\u6a21\u6001\u98df\u54c1\u6570\u636e\u96c6\uff0c\u5177\u6709\u53ef\u9a8c\u8bc1\u7684\u6765\u6e90\u3002\u5b83\u662f\u4ece120\u4e07\u8d28\u91cf\u5408\u683c\u7684\u98df\u54c1\u56fe\u50cf\u4e2d\u7cbe\u9009\u768410%\u5b50\u96c6\uff0c\u5305\u542b\u4e30\u5bcc\u7684\u6ce8\u91ca\u4fe1\u606f\uff08\u5982\u83dc\u540d\u3001\u4ea7\u5730\uff09\u3002\u6570\u636e\u96c6\u901a\u8fc7Codatta\u8d21\u732e\u6a21\u578b\u6536\u96c6\uff0c\u7ed3\u5408\u4e86\u793e\u533a\u4f17\u5305\u548cAI\u8f85\u52a9\u8d28\u91cf\u68c0\u67e5\uff0c\u5e76\u652f\u6301\u94fe\u4e0b\u8ffd\u8e2a\u3002\u901a\u8fc7\u5fae\u8c03\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0c\u7ed3\u679c\u663e\u793a\u6027\u80fd\u63d0\u5347\u3002\u6570\u636e\u96c6\u514d\u8d39\u516c\u5f0010%\uff0c\u5176\u4f5990%\u4fdd\u7559\u5546\u4e1a\u7528\u9014\u5e76\u5206\u4eab\u6536\u76ca\u7ed9\u8d21\u732e\u8005\u3002", "motivation": "\u98df\u54c1\u6570\u636e\u7684\u591a\u6a21\u6001\u548c\u53ef\u9a8c\u8bc1\u6765\u6e90\u9700\u6c42\u65e5\u76ca\u589e\u957f\uff0c\u4f46\u73b0\u6709\u6570\u636e\u96c6\u89c4\u6a21\u548c\u8d28\u91cf\u6709\u9650\u3002MM-Food-100K\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63d0\u4f9b\u9ad8\u8d28\u91cf\u3001\u53ef\u8ffd\u6eaf\u7684\u98df\u54c1\u6570\u636e\u96c6\u3002", "method": "\u4f7f\u7528Codatta\u8d21\u732e\u6a21\u578b\uff0c\u7ed3\u5408\u793e\u533a\u4f17\u5305\u548cAI\u8f85\u52a9\u8d28\u91cf\u68c0\u67e5\uff0c\u4ece87,000\u591a\u540d\u8d21\u732e\u8005\u4e2d\u6536\u96c6\u6570\u636e\u3002\u6570\u636e\u901a\u8fc7\u94fe\u4e0b\u5206\u7c7b\u8d26\u8ffd\u8e2a\uff0c\u5e76\u8ba1\u5212\u5b9e\u73b0\u94fe\u4e0a\u534f\u8bae\u3002", "result": "\u5fae\u8c03\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08\u5982ChatGPT 5\u3001Qwen-Max\uff09\u540e\uff0c\u5728\u56fe\u50cf\u8425\u517b\u9884\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "MM-Food-100K\u662f\u4e00\u4e2a\u9ad8\u8d28\u91cf\u3001\u53ef\u8ffd\u6eaf\u7684\u591a\u6a21\u6001\u98df\u54c1\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u516c\u5f00\u90e8\u5206\u6570\u636e\u4fc3\u8fdb\u7814\u7a76\uff0c\u540c\u65f6\u4fdd\u7559\u5546\u4e1a\u6f5c\u529b\u5e76\u56de\u9988\u8d21\u732e\u8005\u3002"}}
{"id": "2508.10431", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.10431", "abs": "https://arxiv.org/abs/2508.10431", "authors": ["Chris Cao", "Gururaj Saileshwar"], "title": "Yet Another Mirage of Breaking MIRAGE: Debunking Occupancy-based Side-Channel Attacks on Fully Associative Randomized Caches", "comment": null, "summary": "Recent work presented at USENIX Security 2025 claims that occupancy-based\nattacks can recover AES keys from the MIRAGE randomized cache. In this paper,\nwe examine these claims and find that they arise from fundamental modeling\nflaws. Most critically, the authors' simulation of MIRAGE uses a constant seed\nto initialize the random number generator used for global evictions in MIRAGE,\ncausing every AES encryption they trace to evict the same deterministic\nsequence of cache lines. This artificially creates a highly repeatable timing\npattern that is not representative of a realistic implementation of MIRAGE,\nwhere eviction sequences vary randomly between encryptions. When we instead\nrandomize the eviction seed for each run, reflecting realistic operation, the\ncorrelation between AES T-table accesses and attacker runtimes disappears, and\nthe attack fails. These findings show that the reported leakage is an artifact\nof incorrect modeling, and not an actual vulnerability in MIRAGE.", "AI": {"tldr": "\u8bba\u6587\u6307\u51faUSENIX Security 2025\u4e2d\u5173\u4e8eMIRAGE\u968f\u673a\u7f13\u5b58\u653b\u51fb\u7684\u7814\u7a76\u5b58\u5728\u5efa\u6a21\u7f3a\u9677\uff0c\u653b\u51fb\u6210\u529f\u662f\u7531\u4e8e\u6a21\u62df\u4e2d\u4f7f\u7528\u4e86\u56fa\u5b9a\u968f\u673a\u79cd\u5b50\uff0c\u5b9e\u9645\u968f\u673a\u5316\u540e\u653b\u51fb\u5931\u6548\u3002", "motivation": "\u9a8c\u8bc1USENIX Security 2025\u4e2d\u5173\u4e8eMIRAGE\u968f\u673a\u7f13\u5b58\u653b\u51fb\u7684\u7ed3\u8bba\u662f\u5426\u6210\u7acb\u3002", "method": "\u91cd\u65b0\u6a21\u62dfMIRAGE\u7f13\u5b58\u653b\u51fb\uff0c\u968f\u673a\u5316\u6bcf\u6b21\u8fd0\u884c\u7684\u79cd\u5b50\uff0c\u5bf9\u6bd4\u56fa\u5b9a\u79cd\u5b50\u4e0e\u5b9e\u9645\u968f\u673a\u5316\u60c5\u51b5\u4e0b\u7684\u653b\u51fb\u6548\u679c\u3002", "result": "\u56fa\u5b9a\u79cd\u5b50\u5bfc\u81f4\u653b\u51fb\u6210\u529f\uff0c\u968f\u673a\u5316\u540e\u653b\u51fb\u5931\u6548\uff0c\u8bc1\u660e\u539f\u7814\u7a76\u7ed3\u8bba\u662f\u5efa\u6a21\u9519\u8bef\u3002", "conclusion": "MIRAGE\u968f\u673a\u7f13\u5b58\u5728\u5b9e\u9645\u8fd0\u884c\u4e2d\u4e0d\u5b58\u5728AES\u5bc6\u94a5\u6cc4\u9732\u6f0f\u6d1e\uff0c\u539f\u7814\u7a76\u7ed3\u8bba\u662f\u6a21\u62df\u7f3a\u9677\u5bfc\u81f4\u7684\u5047\u8c61\u3002"}}
{"id": "2508.10433", "categories": ["cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.10433", "abs": "https://arxiv.org/abs/2508.10433", "authors": ["Runqi Qiao", "Qiuna Tan", "Peiqing Yang", "Yanzi Wang", "Xiaowan Wang", "Enhui Wan", "Sitong Zhou", "Guanting Dong", "Yuchen Zeng", "Yida Xu", "Jie Wang", "Chong Sun", "Chen Li", "Honggang Zhang"], "title": "We-Math 2.0: A Versatile MathBook System for Incentivizing Visual Mathematical Reasoning", "comment": "Working in progress", "summary": "Multimodal Large Language Models (MLLMs) have demonstrated impressive\ncapabilities across various tasks, but still struggle with complex mathematical\nreasoning. Existing research primarily focuses on dataset construction and\nmethod optimization, often overlooking two critical aspects: comprehensive\nknowledge-driven design and model-centric data space modeling. In this paper,\nwe introduce We-Math 2.0, a unified system that integrates a structured\nmathematical knowledge system, model-centric data space modeling, and a\nreinforcement learning (RL)-based training paradigm to comprehensively enhance\nthe mathematical reasoning abilities of MLLMs. The key contributions of We-Math\n2.0 are fourfold: (1) MathBook Knowledge System: We construct a five-level\nhierarchical system encompassing 491 knowledge points and 1,819 fundamental\nprinciples. (2) MathBook-Standard & Pro: We develop MathBook-Standard, a\ndataset that ensures broad conceptual coverage and flexibility through dual\nexpansion. Additionally, we define a three-dimensional difficulty space and\ngenerate 7 progressive variants per problem to build MathBook-Pro, a\nchallenging dataset for robust training. (3) MathBook-RL: We propose a\ntwo-stage RL framework comprising: (i) Cold-Start Fine-tuning, which aligns the\nmodel with knowledge-oriented chain-of-thought reasoning; and (ii) Progressive\nAlignment RL, leveraging average-reward learning and dynamic data scheduling to\nachieve progressive alignment across difficulty levels. (4) MathBookEval: We\nintroduce a comprehensive benchmark covering all 491 knowledge points with\ndiverse reasoning step distributions. Experimental results show that\nMathBook-RL performs competitively with existing baselines on four widely-used\nbenchmarks and achieves strong results on MathBookEval, suggesting promising\ngeneralization in mathematical reasoning.", "AI": {"tldr": "We-Math 2.0\u662f\u4e00\u4e2a\u7edf\u4e00\u7cfb\u7edf\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u6570\u5b66\u77e5\u8bc6\u7cfb\u7edf\u3001\u6a21\u578b\u4e2d\u5fc3\u6570\u636e\u7a7a\u95f4\u5efa\u6a21\u548c\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u8303\u5f0f\uff0c\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u7684\u6570\u5b66\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u6570\u636e\u96c6\u6784\u5efa\u548c\u65b9\u6cd5\u4f18\u5316\uff0c\u5ffd\u89c6\u4e86\u77e5\u8bc6\u9a71\u52a8\u8bbe\u8ba1\u548c\u6570\u636e\u7a7a\u95f4\u5efa\u6a21\uff0c\u5bfc\u81f4MLLMs\u5728\u590d\u6742\u6570\u5b66\u63a8\u7406\u4e0a\u8868\u73b0\u4e0d\u8db3\u3002", "method": "We-Math 2.0\u6574\u5408\u4e86\u4e94\u7ea7\u6570\u5b66\u77e5\u8bc6\u7cfb\u7edf\u3001\u53cc\u6269\u5c55\u6570\u636e\u96c6\uff08MathBook-Standard & Pro\uff09\u3001\u4e24\u9636\u6bb5\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff08MathBook-RL\uff09\u548c\u7efc\u5408\u8bc4\u6d4b\u57fa\u51c6\uff08MathBookEval\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMathBook-RL\u5728\u56db\u4e2a\u5e38\u7528\u57fa\u51c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u5728MathBookEval\u4e0a\u5c55\u73b0\u51fa\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "We-Math 2.0\u901a\u8fc7\u7cfb\u7edf\u5316\u8bbe\u8ba1\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86MLLMs\u7684\u6570\u5b66\u63a8\u7406\u80fd\u529b\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2508.10493", "categories": ["cs.CR", "cs.GT"], "pdf": "https://arxiv.org/pdf/2508.10493", "abs": "https://arxiv.org/abs/2508.10493", "authors": ["Bernhard Kauer", "Aleksandr Petrosyan", "Benjamin Livshits"], "title": "AlDBaran: Towards Blazingly Fast State Commitments for Blockchains", "comment": null, "summary": "The fundamental basis for maintaining integrity within contemporary\nblockchain systems is provided by authenticated databases. Our analysis\nindicates that a significant portion of the approaches applied in this domain\nfail to sufficiently meet the stringent requirements of systems processing\ntransactions at rates of multi-million TPS. AlDBaran signifies a substantial\nadvancement in authenticated databases. By eliminating disk I/O operations from\nthe critical path, implementing prefetching strategies, and refining the update\nmechanism of the Merkle tree, we have engineered an authenticated data\nstructure capable of handling state updates efficiently at a network throughput\nof 50 Gbps. This throughput capacity significantly surpasses any empirically\ndocumented blockchain throughput, guaranteeing the ability of even the most\nhigh-throughput blockchains to generate state commitments effectively.\n  AlDBaran provides support for historical state proofs, which facilitates a\nwide array of novel applications. For instance, the deployment of AlDBaran\ncould enable blockchains that do not currently support state commitments to\noffer functionalities for light clients and/or implement rollups.\n  When benchmarked against alternative authenticated data structure projects,\nAlDBaran exhibits superior performance and simplicity. In particular, AlDBaran\nachieves speeds of approximately 48 million updates per second using an\nidentical machine configuration. This characteristic renders AlDBaran an\nattractive solution for resource-limited environments, as its historical data\ncapabilities can be modularly isolated (and deactivated), which further\nenhances performance. On consumer-level portable hardware, it achieves\napproximately 8 million updates/s in an in-memory setting and 5 million\nupdates/s with snapshots at sub-second intervals, illustrating compelling and\ncost-effective scalability.", "AI": {"tldr": "AlDBaran\u662f\u4e00\u79cd\u9ad8\u6027\u80fd\u8ba4\u8bc1\u6570\u636e\u5e93\uff0c\u901a\u8fc7\u4f18\u5316\u78c1\u76d8I/O\u3001\u9884\u53d6\u7b56\u7565\u548cMerkle\u6811\u66f4\u65b0\u673a\u5236\uff0c\u652f\u630150 Gbps\u7f51\u7edc\u541e\u5410\u91cf\uff0c\u663e\u8457\u63d0\u5347\u533a\u5757\u94fe\u72b6\u6001\u66f4\u65b0\u7684\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u8ba4\u8bc1\u6570\u636e\u5e93\u65e0\u6cd5\u6ee1\u8db3\u9ad8\u541e\u5410\u91cf\u533a\u5757\u94fe\u7cfb\u7edf\u7684\u9700\u6c42\uff0cAlDBaran\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u6d88\u9664\u78c1\u76d8I/O\u64cd\u4f5c\u3001\u5b9e\u65bd\u9884\u53d6\u7b56\u7565\u548c\u6539\u8fdbMerkle\u6811\u66f4\u65b0\u673a\u5236\uff0c\u4f18\u5316\u8ba4\u8bc1\u6570\u636e\u7ed3\u6784\u3002", "result": "AlDBaran\u652f\u630150 Gbps\u541e\u5410\u91cf\uff0c\u6bcf\u79d24800\u4e07\u6b21\u66f4\u65b0\uff0c\u6027\u80fd\u4f18\u4e8e\u540c\u7c7b\u9879\u76ee\u3002", "conclusion": "AlDBaran\u4e3a\u9ad8\u541e\u5410\u91cf\u533a\u5757\u94fe\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u652f\u6301\u5386\u53f2\u72b6\u6001\u8bc1\u660e\u548c\u8f7b\u5ba2\u6237\u7aef\u529f\u80fd\u3002"}}
{"id": "2508.10467", "categories": ["cs.AI", "cs.DL"], "pdf": "https://arxiv.org/pdf/2508.10467", "abs": "https://arxiv.org/abs/2508.10467", "authors": ["Xueli Pan", "Victor de Boer", "Jacco van Ossenbruggen"], "title": "FIRESPARQL: A LLM-based Framework for SPARQL Query Generation over Scholarly Knowledge Graphs", "comment": "Accepted at 17th International Joint Conference on Knowledge\n  Discovery, Knowledge Engineering and Knowledge Management (IC3K)", "summary": "Question answering over Scholarly Knowledge Graphs (SKGs) remains a\nchallenging task due to the complexity of scholarly content and the intricate\nstructure of these graphs. Large Language Model (LLM) approaches could be used\nto translate natural language questions (NLQs) into SPARQL queries; however,\nthese LLM-based approaches struggle with SPARQL query generation due to limited\nexposure to SKG-specific content and the underlying schema. We identified two\nmain types of errors in the LLM-generated SPARQL queries: (i) structural\ninconsistencies, such as missing or redundant triples in the queries, and (ii)\nsemantic inaccuracies, where incorrect entities or properties are shown in the\nqueries despite a correct query structure. To address these issues, we propose\nFIRESPARQL, a modular framework that supports fine-tuned LLMs as a core\ncomponent, with optional context provided via retrieval-augmented generation\n(RAG) and a SPARQL query correction layer. We evaluate the framework on the\nSciQA Benchmark using various configurations (zero-shot, zero-shot with RAG,\none-shot, fine-tuning, and fine-tuning with RAG) and compare the performance\nwith baseline and state-of-the-art approaches. We measure query accuracy using\nBLEU and ROUGE metrics, and query result accuracy using relaxed exact\nmatch(RelaxedEM), with respect to the gold standards containing the NLQs,\nSPARQL queries, and the results of the queries. Experimental results\ndemonstrate that fine-tuning achieves the highest overall performance, reaching\n0.90 ROUGE-L for query accuracy and 0.85 RelaxedEM for result accuracy on the\ntest set.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faFIRESPARQL\u6846\u67b6\uff0c\u901a\u8fc7\u5fae\u8c03LLM\u548c\u7ed3\u5408RAG\u6280\u672f\uff0c\u89e3\u51b3\u5b66\u672f\u77e5\u8bc6\u56fe\u8c31\u95ee\u7b54\u4e2dSPARQL\u67e5\u8be2\u751f\u6210\u7684\u7ed3\u6784\u548c\u8bed\u4e49\u9519\u8bef\uff0c\u5b9e\u9a8c\u8868\u660e\u5fae\u8c03\u65b9\u6cd5\u6027\u80fd\u6700\u4f73\u3002", "motivation": "\u89e3\u51b3\u5b66\u672f\u77e5\u8bc6\u56fe\u8c31\u95ee\u7b54\u4e2dLLM\u751f\u6210SPARQL\u67e5\u8be2\u65f6\u7684\u7ed3\u6784\u548c\u8bed\u4e49\u9519\u8bef\u95ee\u9898\u3002", "method": "\u63d0\u51faFIRESPARQL\u6846\u67b6\uff0c\u7ed3\u5408\u5fae\u8c03LLM\u3001RAG\u6280\u672f\u548cSPARQL\u67e5\u8be2\u6821\u6b63\u5c42\uff0c\u5e76\u5728SciQA\u57fa\u51c6\u4e0a\u8bc4\u4f30\u591a\u79cd\u914d\u7f6e\u3002", "result": "\u5fae\u8c03\u65b9\u6cd5\u8868\u73b0\u6700\u4f73\uff0cROUGE-L\u8fbe0.90\uff0cRelaxedEM\u8fbe0.85\u3002", "conclusion": "FIRESPARQL\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u5b66\u672f\u77e5\u8bc6\u56fe\u8c31\u95ee\u7b54\u4e2dSPARQL\u67e5\u8be2\u751f\u6210\u7684\u51c6\u786e\u6027\u548c\u7ed3\u679c\u8d28\u91cf\u3002"}}
{"id": "2508.10510", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.10510", "abs": "https://arxiv.org/abs/2508.10510", "authors": ["Hugo Delavenne", "Louise Lallemand"], "title": "Codes on any Cayley Graph have an Interactive Oracle Proof of Proximity", "comment": null, "summary": "Interactive Oracle Proofs of Proximity (IOPP) are at the heart of code-based\nSNARKs, a family of zeroknowledge protocols. The first and most famous one is\nthe FRI protocol [BBHR18a], that efficiently tests proximity to Reed-Solomon\ncodes. This paper generalizes the flowering IOPP introduced in [DMR25] for some\nspecific (2, n)-regular Tanner codes to a much broader variety of codes: any\ncode with symbols indexed on the edges of a Cayley graph. The flowering\nprotocol of [DMR25] had a soundness parameter much lower than the FRI protocol\n[BCI + 23], and complexity parameters that could compete with the FRI\n[BBHR18a]. The lower soundness and the absence of restriction on the base field\nmay lead to other practical speedups, however the codes considered in [DMR25]\nhave an o(1) minimum distance. The generalization proposed in this paper\npreserves the soundness parameter with a slight decrease of the complexity\nparameters, while allowing being applied on codes with constant rate and\nconstant minimum distance thanks to the good expansion properties of some\nfamilies of Cayley graphs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5e7f\u4e49\u7684\u4ea4\u4e92\u5f0f\u9884\u8a00\u8bc1\u660e\u90bb\u8fd1\u6027\uff08IOPP\uff09\u534f\u8bae\uff0c\u9002\u7528\u4e8e\u57fa\u4e8eCayley\u56fe\u7684\u7f16\u7801\uff0c\u4fdd\u7559\u4e86\u4f4esoundness\u53c2\u6570\u5e76\u6269\u5c55\u4e86\u5e94\u7528\u8303\u56f4\u3002", "motivation": "\u6269\u5c55[DMR25]\u4e2d\u63d0\u51fa\u7684flowering IOPP\u534f\u8bae\uff0c\u4f7f\u5176\u9002\u7528\u4e8e\u66f4\u5e7f\u6cdb\u7684\u7f16\u7801\u7c7b\u578b\uff0c\u540c\u65f6\u4fdd\u6301\u4f4esoundness\u53c2\u6570\u548c\u7ade\u4e89\u6027\u7684\u590d\u6742\u5ea6\u3002", "method": "\u901a\u8fc7\u5229\u7528Cayley\u56fe\u7684\u826f\u597d\u6269\u5c55\u6027\u8d28\uff0c\u5c06\u534f\u8bae\u63a8\u5e7f\u5230\u7b26\u53f7\u7d22\u5f15\u5728Cayley\u56fe\u8fb9\u4e0a\u7684\u7f16\u7801\u3002", "result": "\u63d0\u51fa\u7684\u5e7f\u4e49\u534f\u8bae\u5728\u4fdd\u6301soundness\u53c2\u6570\u7684\u540c\u65f6\uff0c\u590d\u6742\u5ea6\u7565\u6709\u4e0b\u964d\uff0c\u9002\u7528\u4e8e\u5177\u6709\u6052\u5b9a\u901f\u7387\u548c\u6700\u5c0f\u8ddd\u79bb\u7684\u7f16\u7801\u3002", "conclusion": "\u8be5\u534f\u8bae\u4e3a\u57fa\u4e8eCayley\u56fe\u7684\u7f16\u7801\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684IOPP\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u6f5c\u5728\u7684\u5b9e\u9645\u52a0\u901f\u4f18\u52bf\u3002"}}
{"id": "2508.10486", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10486", "abs": "https://arxiv.org/abs/2508.10486", "authors": ["Ivan Khai Ze Lim", "Ningyi Liao", "Yiming Yang", "Gerald Wei Yong Yip", "Siqiang Luo"], "title": "SEQ-GPT: LLM-assisted Spatial Query via Example", "comment": null, "summary": "Contemporary spatial services such as online maps predominantly rely on user\nqueries for location searches. However, the user experience is limited when\nperforming complex tasks, such as searching for a group of locations\nsimultaneously. In this study, we examine the extended scenario known as\nSpatial Exemplar Query (SEQ), where multiple relevant locations are jointly\nsearched based on user-specified examples. We introduce SEQ-GPT, a spatial\nquery system powered by Large Language Models (LLMs) towards more versatile SEQ\nsearch using natural language. The language capabilities of LLMs enable unique\ninteractive operations in the SEQ process, including asking users to clarify\nquery details and dynamically adjusting the search based on user feedback. We\nalso propose a tailored LLM adaptation pipeline that aligns natural language\nwith structured spatial data and queries through dialogue synthesis and\nmulti-model cooperation. SEQ-GPT offers an end-to-end demonstration for\nbroadening spatial search with realistic data and application scenarios.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u7a7a\u95f4\u67e5\u8be2\u7cfb\u7edfSEQ-GPT\uff0c\u7528\u4e8e\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u8fdb\u884c\u66f4\u7075\u6d3b\u7684\u7a7a\u95f4\u793a\u4f8b\u67e5\u8be2\uff08SEQ\uff09\u3002", "motivation": "\u73b0\u6709\u7a7a\u95f4\u670d\u52a1\uff08\u5982\u5728\u7ebf\u5730\u56fe\uff09\u4e3b\u8981\u4f9d\u8d56\u7528\u6237\u67e5\u8be2\u8fdb\u884c\u4f4d\u7f6e\u641c\u7d22\uff0c\u4f46\u5728\u6267\u884c\u590d\u6742\u4efb\u52a1\uff08\u5982\u540c\u65f6\u641c\u7d22\u591a\u4e2a\u76f8\u5173\u4f4d\u7f6e\uff09\u65f6\u7528\u6237\u4f53\u9a8c\u53d7\u9650\u3002", "method": "\u5f15\u5165SEQ-GPT\u7cfb\u7edf\uff0c\u5229\u7528LLMs\u7684\u81ea\u7136\u8bed\u8a00\u80fd\u529b\u5b9e\u73b0\u4ea4\u4e92\u5f0f\u64cd\u4f5c\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u5b9a\u5236\u7684LLM\u9002\u914d\u6d41\u7a0b\uff0c\u901a\u8fc7\u5bf9\u8bdd\u5408\u6210\u548c\u591a\u6a21\u578b\u534f\u4f5c\u5c06\u81ea\u7136\u8bed\u8a00\u4e0e\u7ed3\u6784\u5316\u7a7a\u95f4\u6570\u636e\u5bf9\u9f50\u3002", "result": "SEQ-GPT\u5c55\u793a\u4e86\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u6269\u5c55\u7a7a\u95f4\u641c\u7d22\u7684\u7aef\u5230\u7aef\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u6570\u636e\u548c\u573a\u666f\u3002", "conclusion": "SEQ-GPT\u901a\u8fc7LLMs\u63d0\u5347\u4e86\u7a7a\u95f4\u67e5\u8be2\u7684\u7075\u6d3b\u6027\u548c\u4ea4\u4e92\u6027\uff0c\u4e3a\u590d\u6742\u7a7a\u95f4\u641c\u7d22\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2508.10636", "categories": ["cs.CR", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2508.10636", "abs": "https://arxiv.org/abs/2508.10636", "authors": ["Sandipan Dey", "Payal Santosh Kate", "Vatsala Upadhyay", "Abhishek Vaish"], "title": "A Transformer-Based Approach for DDoS Attack Detection in IoT Networks", "comment": null, "summary": "DDoS attacks have become a major threat to the security of IoT devices and\ncan cause severe damage to the network infrastructure. IoT devices suffer from\nthe inherent problem of resource constraints and are therefore susceptible to\nsuch resource-exhausting attacks. Traditional methods for detecting DDoS\nattacks are not efficient enough to cope with the dynamic nature of IoT\nnetworks, as well as the scalability of the attacks, diversity of protocols,\nhigh volume of traffic, and variability in device behavior, and variability of\nprotocols like MQTT, CoAP, making it hard to implement security across all the\nprotocols. In this paper, we propose a novel approach, i.e., the use of\nTransformer models, which have shown remarkable performance in natural language\nprocessing tasks, for detecting DDoS attacks on IoT devices. The proposed model\nextracts features from network traffic data and processes them using a\nself-attention mechanism. Experiments conducted on a real-world dataset\ndemonstrate that the proposed approach outperforms traditional machine learning\ntechniques, which can be validated by comparing both approaches' accuracy,\nprecision, recall, and F1-score. The results of this study show that the\nTransformer models can be an effective solution for detecting DDoS attacks on\nIoT devices and have the potential to be deployed in real-world IoT\nenvironments.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u6a21\u578b\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u68c0\u6d4b\u7269\u8054\u7f51\u8bbe\u5907\u4e0a\u7684DDoS\u653b\u51fb\uff0c\u6548\u679c\u4f18\u4e8e\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u6280\u672f\u3002", "motivation": "\u7269\u8054\u7f51\u8bbe\u5907\u56e0\u8d44\u6e90\u53d7\u9650\u6613\u53d7DDoS\u653b\u51fb\uff0c\u4f20\u7edf\u68c0\u6d4b\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u52a8\u6001\u7f51\u7edc\u73af\u5883\u548c\u591a\u6837\u5316\u534f\u8bae\u3002", "method": "\u4f7f\u7528Transformer\u6a21\u578b\u63d0\u53d6\u7f51\u7edc\u6d41\u91cf\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u81ea\u6ce8\u610f\u529b\u673a\u5236\u5904\u7406\u6570\u636e\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u51c6\u786e\u7387\u3001\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u548cF1\u5206\u6570\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "Transformer\u6a21\u578b\u662f\u68c0\u6d4b\u7269\u8054\u7f51DDoS\u653b\u51fb\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u5907\u5b9e\u9645\u90e8\u7f72\u6f5c\u529b\u3002"}}
{"id": "2508.10492", "categories": ["cs.AI", "cs.CE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.10492", "abs": "https://arxiv.org/abs/2508.10492", "authors": ["Shicheng Xu", "Xin Huang", "Zihao Wei", "Liang Pang", "Huawei Shen", "Xueqi Cheng"], "title": "Reverse Physician-AI Relationship: Full-process Clinical Diagnosis Driven by a Large Language Model", "comment": "39 pages", "summary": "Full-process clinical diagnosis in the real world encompasses the entire\ndiagnostic workflow that begins with only an ambiguous chief complaint. While\nartificial intelligence (AI), particularly large language models (LLMs), is\ntransforming clinical diagnosis, its role remains largely as an assistant to\nphysicians. This AI-assisted working pattern makes AI can only answer specific\nmedical questions at certain parts within the diagnostic process, but lack the\nability to drive the entire diagnostic process starting from an ambiguous\ncomplaint, which still relies heavily on human physicians. This gap limits AI's\nability to fully reduce physicians' workload and enhance diagnostic efficiency.\nTo address this, we propose a paradigm shift that reverses the relationship\nbetween physicians and AI: repositioning AI as the primary director, with\nphysicians serving as its assistants. So we present DxDirector-7B, an LLM\nendowed with advanced deep thinking capabilities, enabling it to drive the\nfull-process diagnosis with minimal physician involvement. Furthermore,\nDxDirector-7B establishes a robust accountability framework for misdiagnoses,\ndelineating responsibility between AI and human physicians. In evaluations\nacross rare, complex, and real-world cases under full-process diagnosis\nsetting, DxDirector-7B not only achieves significant superior diagnostic\naccuracy but also substantially reduces physician workload than\nstate-of-the-art medical LLMs as well as general-purpose LLMs. Fine-grained\nanalyses across multiple clinical departments and tasks validate its efficacy,\nwith expert evaluations indicating its potential to serve as a viable\nsubstitute for medical specialists. These findings mark a new era where AI,\ntraditionally a physicians' assistant, now drives the entire diagnostic process\nto drastically reduce physicians' workload, indicating an efficient and\naccurate diagnostic solution.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faDxDirector-7B\uff0c\u4e00\u79cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u80fd\u591f\u4e3b\u5bfc\u5168\u6d41\u7a0b\u4e34\u5e8a\u8bca\u65ad\uff0c\u51cf\u5c11\u533b\u751f\u5de5\u4f5c\u91cf\u5e76\u63d0\u9ad8\u8bca\u65ad\u51c6\u786e\u6027\u3002", "motivation": "\u5f53\u524dAI\u5728\u4e34\u5e8a\u8bca\u65ad\u4e2d\u4ec5\u4f5c\u4e3a\u8f85\u52a9\u5de5\u5177\uff0c\u65e0\u6cd5\u4e3b\u5bfc\u4ece\u6a21\u7cca\u4e3b\u8bc9\u5f00\u59cb\u7684\u5b8c\u6574\u8bca\u65ad\u6d41\u7a0b\uff0c\u9650\u5236\u4e86\u5176\u51cf\u8f7b\u533b\u751f\u8d1f\u62c5\u548c\u63d0\u5347\u6548\u7387\u7684\u6f5c\u529b\u3002", "method": "\u63d0\u51faDxDirector-7B\uff0c\u4e00\u79cd\u5177\u5907\u6df1\u5ea6\u601d\u8003\u80fd\u529b\u7684LLM\uff0c\u80fd\u591f\u4e3b\u5bfc\u8bca\u65ad\u6d41\u7a0b\uff0c\u5e76\u5efa\u7acb\u8d23\u4efb\u6846\u67b6\u3002", "result": "\u5728\u7f55\u89c1\u3001\u590d\u6742\u548c\u771f\u5b9e\u6848\u4f8b\u4e2d\uff0cDxDirector-7B\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u533b\u5b66LLM\u548c\u901a\u7528LLM\uff0c\u51cf\u5c11\u533b\u751f\u5de5\u4f5c\u91cf\u5e76\u63d0\u9ad8\u51c6\u786e\u6027\u3002", "conclusion": "DxDirector-7B\u6807\u5fd7\u7740AI\u4ece\u8f85\u52a9\u5de5\u5177\u8f6c\u53d8\u4e3a\u8bca\u65ad\u4e3b\u5bfc\u8005\uff0c\u4e3a\u9ad8\u6548\u3001\u51c6\u786e\u7684\u8bca\u65ad\u63d0\u4f9b\u4e86\u65b0\u65b9\u6848\u3002"}}
{"id": "2508.10639", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.10639", "abs": "https://arxiv.org/abs/2508.10639", "authors": ["Anyuan Sang", "Lu Zhou", "Li Yang", "Junbo Jia", "Huipeng Yang", "Pengbin Feng", "Jianfeng Ma"], "title": "MirGuard: Towards a Robust Provenance-based Intrusion Detection System Against Graph Manipulation Attacks", "comment": null, "summary": "Learning-based Provenance-based Intrusion Detection Systems (PIDSes) have\nbecome essential tools for anomaly detection in host systems due to their\nability to capture rich contextual and structural information, as well as their\npotential to detect unknown attacks. However, recent studies have shown that\nthese systems are vulnerable to graph manipulation attacks, where attackers\nmanipulate the graph structure to evade detection. While some previous\napproaches have discussed this type of attack, none have fully addressed it\nwith a robust detection solution, limiting the practical applicability of\nPIDSes.\n  To address this challenge, we propose MirGuard, a robust anomaly detection\nframework that combines logic-aware multi-view augmentation with contrastive\nrepresentation learning. Rather than applying arbitrary structural\nperturbations, MirGuard introduces Logic-Aware Noise Injection (LNI) to\ngenerate semantically valid graph views, ensuring that all augmentations\npreserve the underlying causal semantics of the provenance data. These views\nare then used in a Logic-Preserving Contrastive Learning framework, which\nencourages the model to learn representations that are invariant to benign\ntransformations but sensitive to adversarial inconsistencies. Comprehensive\nevaluations on multiple provenance datasets demonstrate that MirGuard\nsignificantly outperforms state-of-the-art detectors in robustness against\nvarious graph manipulation attacks without sacrificing detection performance\nand efficiency. Our work represents the first targeted study to enhance PIDS\nagainst such adversarial threats, providing a robust and effective solution to\nmodern cybersecurity challenges.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faMirGuard\u6846\u67b6\uff0c\u901a\u8fc7\u903b\u8f91\u611f\u77e5\u7684\u591a\u89c6\u56fe\u589e\u5f3a\u548c\u5bf9\u6bd4\u8868\u793a\u5b66\u4e60\uff0c\u63d0\u5347\u57fa\u4e8e\u5b66\u4e60\u7684\u6eaf\u6e90\u5165\u4fb5\u68c0\u6d4b\u7cfb\u7edf\uff08PIDSes\uff09\u5bf9\u56fe\u64cd\u7eb5\u653b\u51fb\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u5b66\u4e60\u7684PIDSes\u6613\u53d7\u56fe\u64cd\u7eb5\u653b\u51fb\u5f71\u54cd\uff0c\u7f3a\u4e4f\u9c81\u68d2\u7684\u68c0\u6d4b\u65b9\u6848\uff0c\u9650\u5236\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u7ed3\u5408\u903b\u8f91\u611f\u77e5\u566a\u58f0\u6ce8\u5165\uff08LNI\uff09\u751f\u6210\u8bed\u4e49\u6709\u6548\u7684\u56fe\u89c6\u56fe\uff0c\u5e76\u901a\u8fc7\u903b\u8f91\u4fdd\u7559\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\u5b66\u4e60\u5bf9\u826f\u6027\u53d8\u6362\u4e0d\u53d8\u4f46\u5bf9\u5bf9\u6297\u6027\u4e0d\u4e00\u81f4\u654f\u611f\u7684\u8868\u793a\u3002", "result": "\u5728\u591a\u6eaf\u6e90\u6570\u636e\u96c6\u4e0a\uff0cMirGuard\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u68c0\u6d4b\u5668\uff0c\u5bf9\u6297\u56fe\u64cd\u7eb5\u653b\u51fb\u7684\u9c81\u68d2\u6027\u66f4\u5f3a\uff0c\u4e14\u4e0d\u5f71\u54cd\u68c0\u6d4b\u6027\u80fd\u548c\u6548\u7387\u3002", "conclusion": "MirGuard\u662f\u9996\u4e2a\u9488\u5bf9PIDS\u5bf9\u6297\u6027\u5a01\u80c1\u7684\u589e\u5f3a\u65b9\u6848\uff0c\u4e3a\u73b0\u4ee3\u7f51\u7edc\u5b89\u5168\u6311\u6218\u63d0\u4f9b\u4e86\u9c81\u68d2\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.10501", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.10501", "abs": "https://arxiv.org/abs/2508.10501", "authors": ["Yushi Feng", "Junye Du", "Yingying Hong", "Qifan Wang", "Lequan Yu"], "title": "PASS: Probabilistic Agentic Supernet Sampling for Interpretable and Adaptive Chest X-Ray Reasoning", "comment": null, "summary": "Existing tool-augmented agentic systems are limited in the real world by (i)\nblack-box reasoning steps that undermine trust of decision-making and pose\nsafety risks, (ii) poor multimodal integration, which is inherently critical\nfor healthcare tasks, and (iii) rigid and computationally inefficient agentic\npipelines. We introduce PASS (Probabilistic Agentic Supernet Sampling), the\nfirst multimodal framework to address these challenges in the context of Chest\nX-Ray (CXR) reasoning. PASS adaptively samples agentic workflows over a\nmulti-tool graph, yielding decision paths annotated with interpretable\nprobabilities. Given the complex CXR reasoning task with multimodal medical\ndata, PASS leverages its learned task-conditioned distribution over the agentic\nsupernet. Thus, it adaptively selects the most suitable tool at each supernet\nlayer, offering probability-annotated trajectories for post-hoc audits and\ndirectly enhancing medical AI safety. PASS also continuously compresses salient\nfindings into an evolving personalized memory, while dynamically deciding\nwhether to deepen its reasoning path or invoke an early exit for efficiency. To\noptimize a Pareto frontier balancing performance and cost, we design a novel\nthree-stage training procedure, including expert knowledge warm-up, contrastive\npath-ranking, and cost-aware reinforcement learning. To facilitate rigorous\nevaluation, we introduce CAB-E, a comprehensive benchmark for multi-step,\nsafety-critical, free-form CXR reasoning. Experiments across various benchmarks\nvalidate that PASS significantly outperforms strong baselines in multiple\nmetrics (e.g., accuracy, AUC, LLM-J.) while balancing computational costs,\npushing a new paradigm shift towards interpretable, adaptive, and multimodal\nmedical agentic systems.", "AI": {"tldr": "PASS\u662f\u4e00\u79cd\u591a\u6a21\u6001\u6846\u67b6\uff0c\u901a\u8fc7\u6982\u7387\u6027\u91c7\u6837\u548c\u81ea\u9002\u5e94\u5de5\u5177\u9009\u62e9\uff0c\u89e3\u51b3\u4e86\u533b\u7597AI\u4e2d\u7684\u9ed1\u76d2\u63a8\u7406\u3001\u591a\u6a21\u6001\u6574\u5408\u548c\u6548\u7387\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u73b0\u6709\u5de5\u5177\u589e\u5f3a\u7684\u4ee3\u7406\u7cfb\u7edf\u5b58\u5728\u9ed1\u76d2\u63a8\u7406\u3001\u591a\u6a21\u6001\u6574\u5408\u4e0d\u8db3\u548c\u6548\u7387\u4f4e\u4e0b\u7b49\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u5728\u533b\u7597\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u3002", "method": "PASS\u901a\u8fc7\u591a\u5de5\u5177\u56fe\u81ea\u9002\u5e94\u91c7\u6837\u4ee3\u7406\u5de5\u4f5c\u6d41\uff0c\u7ed3\u5408\u4e09\u9636\u6bb5\u8bad\u7ec3\uff08\u4e13\u5bb6\u77e5\u8bc6\u9884\u70ed\u3001\u5bf9\u6bd4\u8def\u5f84\u6392\u5e8f\u548c\u6210\u672c\u611f\u77e5\u5f3a\u5316\u5b66\u4e60\uff09\uff0c\u4f18\u5316\u6027\u80fd\u548c\u6210\u672c\u5e73\u8861\u3002", "result": "PASS\u5728\u591a\u4e2a\u6307\u6807\uff08\u5982\u51c6\u786e\u7387\u3001AUC\uff09\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\uff0c\u540c\u65f6\u5e73\u8861\u8ba1\u7b97\u6210\u672c\u3002", "conclusion": "PASS\u4e3a\u533b\u7597AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u3001\u81ea\u9002\u5e94\u548c\u591a\u6a21\u6001\u7684\u65b0\u8303\u5f0f\u3002"}}
{"id": "2508.10652", "categories": ["cs.CR", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2508.10652", "abs": "https://arxiv.org/abs/2508.10652", "authors": ["Richa Dasila", "Vatsala Upadhyay", "Samo Bobek", "Abhishek Vaish"], "title": "A Novel Study on Intelligent Methods and Explainable AI for Dynamic Malware Analysis", "comment": null, "summary": "Deep learning models are one of the security strategies, trained on extensive\ndatasets, and play a critical role in detecting and responding to these threats\nby recognizing complex patterns in malicious code. However, the opaque nature\nof these models-often described as \"black boxes\"-makes their decision-making\nprocesses difficult to understand, even for their creators. This research\naddresses these challenges by integrating Explainable AI (XAI) techniques to\nenhance the interpretability and trustworthiness of malware detection models.\nIn this research, the use of Multi-Layer Perceptrons (MLP) for dynamic malware\nanalysis has been considered, a less explored area, and its efficacy in\ndetecting Metamorphic Malware, and further the effectiveness and transparency\nof MLPs, CNNs, RNNs, and CNN-LSTM models in malware classification, evaluating\nthese models through the lens of Explainable AI (XAI). This comprehensive\napproach aims to demystify the internal workings of deep learning models,\npromoting a better understanding and trust in their predictive capabilities in\ncybersecurity contexts. Such in-depth analysis and implementation haven't been\ndone to the best of our knowledge.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u53ef\u89e3\u91caAI\uff08XAI\uff09\u6280\u672f\u63d0\u5347\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u6076\u610f\u8f6f\u4ef6\u68c0\u6d4b\u4e2d\u7684\u900f\u660e\u5ea6\u548c\u53ef\u4fe1\u5ea6\uff0c\u63a2\u7d22\u4e86MLP\u3001CNN\u3001RNN\u548cCNN-LSTM\u7b49\u6a21\u578b\u7684\u6548\u679c\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u6076\u610f\u8f6f\u4ef6\u68c0\u6d4b\u4e2d\u867d\u6709\u6548\uff0c\u4f46\u5176\u201c\u9ed1\u76d2\u201d\u7279\u6027\u5bfc\u81f4\u51b3\u7b56\u8fc7\u7a0b\u96be\u4ee5\u7406\u89e3\uff0c\u5f71\u54cd\u4e86\u4fe1\u4efb\u5ea6\u3002", "method": "\u6574\u5408XAI\u6280\u672f\uff0c\u8bc4\u4f30MLP\u3001CNN\u3001RNN\u548cCNN-LSTM\u6a21\u578b\u5728\u52a8\u6001\u6076\u610f\u8f6f\u4ef6\u5206\u6790\u548c\u5206\u7c7b\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u7814\u7a76\u5c55\u793a\u4e86\u8fd9\u4e9b\u6a21\u578b\u5728\u6076\u610f\u8f6f\u4ef6\u68c0\u6d4b\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5e76\u901a\u8fc7XAI\u63d0\u5347\u4e86\u5176\u900f\u660e\u5ea6\u548c\u53ef\u4fe1\u5ea6\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u7f51\u7edc\u5b89\u5168\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u900f\u660e\u548c\u53ef\u4fe1\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.10530", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.10530", "abs": "https://arxiv.org/abs/2508.10530", "authors": ["Zetian Sun", "Dongfang Li", "Baotian Hu"], "title": "Diversity First, Quality Later: A Two-Stage Assumption for Language Model Alignment", "comment": null, "summary": "The alignment of language models (LMs) with human preferences is critical for\nbuilding reliable AI systems. The problem is typically framed as optimizing an\nLM policy to maximize the expected reward that reflects human preferences.\nRecently, Direct Preference Optimization (DPO) was proposed as a LM alignment\nmethod that directly optimize the policy from static preference data, and\nfurther improved by incorporating on-policy sampling (i.e., preference\ncandidates generated during the training loop) for better LM alignment.\nHowever, we show on-policy data is not always optimal, with systematic\neffectiveness difference emerging between static and on-policy preference\ncandidates. For example, on-policy data can result in a 3$\\times$ effectiveness\ncompared with static data for Llama-3, and a 0.4$\\times$ effectiveness for\nZephyr. To explain the phenomenon, we propose the alignment stage assumption,\nwhich divides the alignment process into two distinct stages: the preference\ninjection stage, which benefits from diverse data, and the preference\nfine-tuning stage, which favors high-quality data. Through theoretical and\nempirical analysis, we characterize these stages and propose an effective\nalgorithm to identify the boundaries between them. We perform experiments on 5\nmodels (Llama, Zephyr, Phi-2, Qwen, Pythia) and 2 alignment methods (DPO,\nSLiC-HF) to show the generalizability of alignment stage assumption and\nboundary measurement.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u8bed\u8a00\u6a21\u578b\uff08LM\uff09\u4e0e\u4eba\u7c7b\u504f\u597d\u5bf9\u9f50\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u9759\u6001\u548c\u52a8\u6001\u504f\u597d\u6570\u636e\u5728\u4f18\u5316\u4e2d\u7684\u6548\u679c\u5dee\u5f02\uff0c\u5e76\u5f15\u5165\u5bf9\u9f50\u9636\u6bb5\u5047\u8bbe\u6765\u89e3\u91ca\u8fd9\u4e00\u73b0\u8c61\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u89e3\u51b3\u8bed\u8a00\u6a21\u578b\u4e0e\u4eba\u7c7b\u504f\u597d\u5bf9\u9f50\u7684\u4f18\u5316\u95ee\u9898\uff0c\u7279\u522b\u662f\u9759\u6001\u548c\u52a8\u6001\u504f\u597d\u6570\u636e\u5728\u8bad\u7ec3\u4e2d\u7684\u4e0d\u540c\u6548\u679c\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u63d0\u51fa\u5bf9\u9f50\u9636\u6bb5\u5047\u8bbe\uff08\u5206\u4e3a\u504f\u597d\u6ce8\u5165\u548c\u504f\u597d\u5fae\u8c03\u9636\u6bb5\uff09\uff0c\u5e76\u901a\u8fc7\u7406\u8bba\u548c\u5b9e\u9a8c\u5206\u6790\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u52a8\u6001\u6570\u636e\u5728\u4e0d\u540c\u6a21\u578b\u4e2d\u7684\u6548\u679c\u5dee\u5f02\u663e\u8457\uff08\u5982Llama-3\u4e3a3\u500d\uff0cZephyr\u4e3a0.4\u500d\uff09\uff0c\u9a8c\u8bc1\u4e86\u5047\u8bbe\u7684\u666e\u9002\u6027\u3002", "conclusion": "\u7ed3\u8bba\u662f\u5bf9\u9f50\u8fc7\u7a0b\u53ef\u5206\u4e3a\u4e24\u4e2a\u9636\u6bb5\uff0c\u8bc6\u522b\u9636\u6bb5\u8fb9\u754c\u6709\u52a9\u4e8e\u4f18\u5316\u8bed\u8a00\u6a21\u578b\u7684\u5bf9\u9f50\u6548\u679c\u3002"}}
{"id": "2508.10677", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.10677", "abs": "https://arxiv.org/abs/2508.10677", "authors": ["Amine Tellache", "Abdelaziz Amara Korba", "Amdjed Mokhtari", "Horea Moldovan", "Yacine Ghamri-Doudane"], "title": "Advancing Autonomous Incident Response: Leveraging LLMs and Cyber Threat Intelligence", "comment": null, "summary": "Effective incident response (IR) is critical for mitigating cyber threats,\nyet security teams are overwhelmed by alert fatigue, high false-positive rates,\nand the vast volume of unstructured Cyber Threat Intelligence (CTI) documents.\nWhile CTI holds immense potential for enriching security operations, its\nextensive and fragmented nature makes manual analysis time-consuming and\nresource-intensive. To bridge this gap, we introduce a novel\nRetrieval-Augmented Generation (RAG)-based framework that leverages Large\nLanguage Models (LLMs) to automate and enhance IR by integrating dynamically\nretrieved CTI. Our approach introduces a hybrid retrieval mechanism that\ncombines NLP-based similarity searches within a CTI vector database with\nstandardized queries to external CTI platforms, facilitating context-aware\nenrichment of security alerts. The augmented intelligence is then leveraged by\nan LLM-powered response generation module, which formulates precise,\nactionable, and contextually relevant incident mitigation strategies. We\npropose a dual evaluation paradigm, wherein automated assessment using an\nauxiliary LLM is systematically cross-validated by cybersecurity experts.\nEmpirical validation on real-world and simulated alerts demonstrates that our\napproach enhances the accuracy, contextualization, and efficiency of IR,\nalleviating analyst workload and reducing response latency. This work\nunderscores the potential of LLM-driven CTI fusion in advancing autonomous\nsecurity operations and establishing a foundation for intelligent, adaptive\ncybersecurity frameworks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7684\u6846\u67b6\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u81ea\u52a8\u5316\u548c\u589e\u5f3a\u4e8b\u4ef6\u54cd\u5e94\uff08IR\uff09\uff0c\u901a\u8fc7\u52a8\u6001\u68c0\u7d22\u7f51\u7edc\u5a01\u80c1\u60c5\u62a5\uff08CTI\uff09\u6765\u63d0\u5347\u5b89\u5168\u64cd\u4f5c\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u5b89\u5168\u56e2\u961f\u9762\u4e34\u544a\u8b66\u75b2\u52b3\u3001\u9ad8\u8bef\u62a5\u7387\u548c\u5927\u91cf\u975e\u7ed3\u6784\u5316CTI\u6587\u6863\u7684\u6311\u6218\uff0c\u624b\u52a8\u5206\u6790\u8017\u65f6\u4e14\u8d44\u6e90\u5bc6\u96c6\uff0c\u4e9f\u9700\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u6df7\u5408\u68c0\u7d22\u673a\u5236\uff0c\u7ed3\u5408NLP\u76f8\u4f3c\u6027\u641c\u7d22\u548c\u6807\u51c6\u5316\u67e5\u8be2\u5916\u90e8CTI\u5e73\u53f0\uff0c\u5e76\u901a\u8fc7LLM\u751f\u6210\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u54cd\u5e94\u7b56\u7565\u3002", "result": "\u5b9e\u8bc1\u9a8c\u8bc1\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86IR\u7684\u51c6\u786e\u6027\u3001\u4e0a\u4e0b\u6587\u76f8\u5173\u6027\u548c\u6548\u7387\uff0c\u51cf\u8f7b\u4e86\u5206\u6790\u5e08\u8d1f\u62c5\u5e76\u7f29\u77ed\u4e86\u54cd\u5e94\u65f6\u95f4\u3002", "conclusion": "LLM\u9a71\u52a8\u7684CTI\u878d\u5408\u6709\u6f5c\u529b\u63a8\u52a8\u81ea\u4e3b\u5b89\u5168\u64cd\u4f5c\uff0c\u5e76\u4e3a\u667a\u80fd\u81ea\u9002\u5e94\u7f51\u7edc\u5b89\u5168\u6846\u67b6\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2508.10539", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.10539", "abs": "https://arxiv.org/abs/2508.10539", "authors": ["Zetian Sun", "Dongfang Li", "Baotian Hu", "Min Zhang"], "title": "Improving Value-based Process Verifier via Low-Cost Variance Reduction", "comment": null, "summary": "Large language models (LLMs) have achieved remarkable success in a wide range\nof tasks. However, their reasoning capabilities, particularly in complex\ndomains like mathematics, remain a significant challenge. Value-based process\nverifiers, which estimate the probability of a partial reasoning chain leading\nto a correct solution, are a promising approach for improving reasoning.\nNevertheless, their effectiveness is often hindered by estimation error in\ntheir training annotations, a consequence of the limited number of Monte Carlo\n(MC) samples feasible due to the high cost of LLM inference. In this paper, we\nidentify that the estimation error primarily arises from high variance rather\nthan bias, and the MC estimator is a Minimum Variance Unbiased Estimator\n(MVUE). To address the problem, we propose the \\textsc{Com}pound \\textsc{M}onte\n\\textsc{C}arlo \\textsc{S}ampling (ComMCS) method, which constructs an unbiased\nestimator by linearly combining the MC estimators from the current and\nsubsequent steps. Theoretically, we show that our method leads to a predictable\nreduction in variance, while maintaining an unbiased estimation without\nadditional LLM inference cost. We also perform empirical experiments on the\nMATH-500 and GSM8K benchmarks to demonstrate the effectiveness of our method.\nNotably, ComMCS outperforms regression-based optimization method by 2.8 points,\nthe non-variance-reduced baseline by 2.2 points on MATH-500 on Best-of-32\nsampling experiment.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aComMCS\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u5f53\u524d\u548c\u540e\u7eed\u6b65\u9aa4\u7684\u8499\u7279\u5361\u6d1b\u4f30\u8ba1\u5668\uff0c\u51cf\u5c11\u65b9\u5dee\u5e76\u4fdd\u6301\u65e0\u504f\u4f30\u8ba1\uff0c\u4ece\u800c\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u9886\u57df\uff08\u5982\u6570\u5b66\uff09\u4e2d\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u9886\u57df\u7684\u63a8\u7406\u80fd\u529b\u4ecd\u6709\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u56e0\u8499\u7279\u5361\u6d1b\u6837\u672c\u6570\u91cf\u6709\u9650\u5bfc\u81f4\u4f30\u8ba1\u8bef\u5dee\uff0c\u4e3b\u8981\u6e90\u4e8e\u9ad8\u65b9\u5dee\u3002", "method": "\u63d0\u51faComMCS\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ebf\u6027\u7ec4\u5408\u5f53\u524d\u548c\u540e\u7eed\u6b65\u9aa4\u7684\u8499\u7279\u5361\u6d1b\u4f30\u8ba1\u5668\uff0c\u51cf\u5c11\u65b9\u5dee\u4e14\u65e0\u9700\u989d\u5916\u63a8\u7406\u6210\u672c\u3002", "result": "\u5728MATH-500\u548cGSM8K\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cComMCS\u6bd4\u56de\u5f52\u4f18\u5316\u65b9\u6cd5\u548c\u975e\u65b9\u5dee\u51cf\u5c11\u57fa\u7ebf\u5206\u522b\u63d0\u53472.8\u548c2.2\u5206\u3002", "conclusion": "ComMCS\u901a\u8fc7\u51cf\u5c11\u65b9\u5dee\u6709\u6548\u63d0\u5347\u63a8\u7406\u80fd\u529b\uff0c\u4e14\u65e0\u9700\u989d\u5916\u6210\u672c\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2508.10880", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.10880", "abs": "https://arxiv.org/abs/2508.10880", "authors": ["Yanzhe Zhang", "Diyi Yang"], "title": "Searching for Privacy Risks in LLM Agents via Simulation", "comment": "Preprint", "summary": "The widespread deployment of LLM-based agents is likely to introduce a\ncritical privacy threat: malicious agents that proactively engage others in\nmulti-turn interactions to extract sensitive information. These dynamic\ndialogues enable adaptive attack strategies that can cause severe privacy\nviolations, yet their evolving nature makes it difficult to anticipate and\ndiscover sophisticated vulnerabilities manually. To tackle this problem, we\npresent a search-based framework that alternates between improving attacker and\ndefender instructions by simulating privacy-critical agent interactions. Each\nsimulation involves three roles: data subject, data sender, and data recipient.\nWhile the data subject's behavior is fixed, the attacker (data recipient)\nattempts to extract sensitive information from the defender (data sender)\nthrough persistent and interactive exchanges. To explore this interaction space\nefficiently, our search algorithm employs LLMs as optimizers, using parallel\nsearch with multiple threads and cross-thread propagation to analyze simulation\ntrajectories and iteratively propose new instructions. Through this process, we\nfind that attack strategies escalate from simple direct requests to\nsophisticated multi-turn tactics such as impersonation and consent forgery,\nwhile defenses advance from rule-based constraints to identity-verification\nstate machines. The discovered attacks and defenses transfer across diverse\nscenarios and backbone models, demonstrating strong practical utility for\nbuilding privacy-aware agents.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u641c\u7d22\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u62df\u9690\u79c1\u5173\u952e\u4ee3\u7406\u4ea4\u4e92\u6765\u53d1\u73b0\u548c\u6539\u8fdb\u653b\u51fb\u4e0e\u9632\u5fa1\u7b56\u7565\uff0c\u63ed\u793a\u4e86\u4ece\u7b80\u5355\u8bf7\u6c42\u5230\u590d\u6742\u591a\u8f6e\u653b\u51fb\u7684\u6f14\u53d8\uff0c\u4ee5\u53ca\u9632\u5fa1\u673a\u5236\u7684\u63d0\u5347\u3002", "motivation": "\u5927\u89c4\u6a21\u90e8\u7f72\u57fa\u4e8eLLM\u7684\u4ee3\u7406\u53ef\u80fd\u5e26\u6765\u9690\u79c1\u5a01\u80c1\uff0c\u6076\u610f\u4ee3\u7406\u901a\u8fc7\u591a\u8f6e\u4ea4\u4e92\u63d0\u53d6\u654f\u611f\u4fe1\u606f\uff0c\u52a8\u6001\u5bf9\u8bdd\u7684\u9002\u5e94\u6027\u653b\u51fb\u7b56\u7565\u96be\u4ee5\u624b\u52a8\u9884\u6d4b\u548c\u53d1\u73b0\u3002", "method": "\u91c7\u7528\u641c\u7d22\u6846\u67b6\uff0c\u6a21\u62df\u4e09\u79cd\u89d2\u8272\uff08\u6570\u636e\u4e3b\u4f53\u3001\u6570\u636e\u53d1\u9001\u8005\u3001\u6570\u636e\u63a5\u6536\u8005\uff09\u7684\u4ea4\u4e92\uff0c\u5229\u7528LLM\u4f5c\u4e3a\u4f18\u5316\u5668\u8fdb\u884c\u5e76\u884c\u641c\u7d22\u548c\u591a\u7ebf\u7a0b\u4f20\u64ad\u3002", "result": "\u653b\u51fb\u7b56\u7565\u4ece\u76f4\u63a5\u8bf7\u6c42\u5347\u7ea7\u4e3a\u590d\u6742\u591a\u8f6e\u6218\u672f\uff08\u5982\u5192\u5145\u548c\u4f2a\u9020\u540c\u610f\uff09\uff0c\u9632\u5fa1\u4ece\u57fa\u4e8e\u89c4\u5219\u7684\u7ea6\u675f\u53d1\u5c55\u4e3a\u8eab\u4efd\u9a8c\u8bc1\u72b6\u6001\u673a\u3002", "conclusion": "\u53d1\u73b0\u7684\u653b\u51fb\u548c\u9632\u5fa1\u7b56\u7565\u5177\u6709\u8de8\u573a\u666f\u548c\u6a21\u578b\u7684\u5b9e\u7528\u6027\uff0c\u4e3a\u6784\u5efa\u9690\u79c1\u611f\u77e5\u4ee3\u7406\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2508.10599", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10599", "abs": "https://arxiv.org/abs/2508.10599", "authors": ["Xinyan Jiang", "Lin Zhang", "Jiayi Zhang", "Qingsong Yang", "Guimin Hu", "Di Wang", "Lijie Hu"], "title": "MSRS: Adaptive Multi-Subspace Representation Steering for Attribute Alignment in Large Language Models", "comment": null, "summary": "Activation steering offers a promising approach to controlling the behavior\nof Large Language Models by directly manipulating their internal activations.\nHowever, most existing methods struggle to jointly steer multiple attributes,\noften resulting in interference and undesirable trade-offs. To address this\nchallenge, we propose Multi-Subspace Representation Steering (MSRS), a novel\nframework for effective multi-attribute steering via subspace representation\nfine-tuning. MSRS reduces inter-attribute interference by allocating orthogonal\nsubspaces to each attribute, isolating their influence within the model's\nrepresentation space. MSRS also incorporates a hybrid subspace composition\nstrategy: it combines attribute-specific subspaces for unique steering\ndirections with a shared subspace for common steering directions. A dynamic\nweighting function learns to efficiently integrate these components for precise\ncontrol. During inference, MSRS introduces a token-level steering mechanism\nthat dynamically identifies and intervenes on the most semantically relevant\ntokens, enabling fine-grained behavioral modulation. Experimental results show\nthat MSRS significantly reduces attribute conflicts, surpasses existing methods\nacross a range of attributes, and generalizes effectively to diverse downstream\ntasks.", "AI": {"tldr": "MSRS\u662f\u4e00\u79cd\u901a\u8fc7\u5b50\u7a7a\u95f4\u8868\u793a\u5fae\u8c03\u5b9e\u73b0\u591a\u5c5e\u6027\u63a7\u5236\u7684\u65b0\u6846\u67b6\uff0c\u51cf\u5c11\u5c5e\u6027\u95f4\u5e72\u6270\u5e76\u63d0\u5347\u63a7\u5236\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u591a\u5c5e\u6027\u8054\u5408\u63a7\u5236\u65f6\u5b58\u5728\u5e72\u6270\u548c\u6743\u8861\u95ee\u9898\uff0cMSRS\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "MSRS\u901a\u8fc7\u5206\u914d\u6b63\u4ea4\u5b50\u7a7a\u95f4\u9694\u79bb\u5c5e\u6027\u5f71\u54cd\uff0c\u7ed3\u5408\u6df7\u5408\u5b50\u7a7a\u95f4\u7b56\u7565\u548c\u52a8\u6001\u6743\u91cd\u51fd\u6570\uff0c\u5b9e\u73b0\u7cbe\u786e\u63a7\u5236\u3002", "result": "\u5b9e\u9a8c\u8868\u660eMSRS\u663e\u8457\u51cf\u5c11\u5c5e\u6027\u51b2\u7a81\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u80fd\u6cdb\u5316\u5230\u591a\u79cd\u4e0b\u6e38\u4efb\u52a1\u3002", "conclusion": "MSRS\u4e3a\u591a\u5c5e\u6027\u63a7\u5236\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.10669", "categories": ["cs.AI", "cs.IR", "H.3.3; I.2.7; H.2.8"], "pdf": "https://arxiv.org/pdf/2508.10669", "abs": "https://arxiv.org/abs/2508.10669", "authors": ["Zhenye Yang", "Jinpeng Chen", "Huan Li", "Xiongnan Jin", "Xuanyang Li", "Junwei Zhang", "Hongbo Gao", "Kaimin Wei", "Senzhang Wang"], "title": "STEP: Stepwise Curriculum Learning for Context-Knowledge Fusion in Conversational Recommendation", "comment": "10 pages; 4 figures; 6 tables; code available at\n  https://github.com/Alex-bupt/STEP", "summary": "Conversational recommender systems (CRSs) aim to proactively capture user\npreferences through natural language dialogue and recommend high-quality items.\nTo achieve this, CRS gathers user preferences via a dialog module and builds\nuser profiles through a recommendation module to generate appropriate\nrecommendations. However, existing CRS faces challenges in capturing the deep\nsemantics of user preferences and dialogue context. In particular, the\nefficient integration of external knowledge graph (KG) information into\ndialogue generation and recommendation remains a pressing issue. Traditional\napproaches typically combine KG information directly with dialogue content,\nwhich often struggles with complex semantic relationships, resulting in\nrecommendations that may not align with user expectations.\n  To address these challenges, we introduce STEP, a conversational recommender\ncentered on pre-trained language models that combines curriculum-guided\ncontext-knowledge fusion with lightweight task-specific prompt tuning. At its\nheart, an F-Former progressively aligns the dialogue context with\nknowledge-graph entities through a three-stage curriculum, thus resolving\nfine-grained semantic mismatches. The fused representation is then injected\ninto the frozen language model via two minimal yet adaptive prefix prompts: a\nconversation prefix that steers response generation toward user intent and a\nrecommendation prefix that biases item ranking toward knowledge-consistent\ncandidates. This dual-prompt scheme allows the model to share cross-task\nsemantics while respecting the distinct objectives of dialogue and\nrecommendation. Experimental results show that STEP outperforms mainstream\nmethods in the precision of recommendation and dialogue quality in two public\ndatasets.", "AI": {"tldr": "STEP\u662f\u4e00\u79cd\u57fa\u4e8e\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7684\u5bf9\u8bdd\u63a8\u8350\u7cfb\u7edf\uff0c\u901a\u8fc7\u8bfe\u7a0b\u5f15\u5bfc\u7684\u4e0a\u4e0b\u6587-\u77e5\u8bc6\u878d\u5408\u548c\u8f7b\u91cf\u7ea7\u4efb\u52a1\u7279\u5b9a\u63d0\u793a\u8c03\u4f18\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u7cfb\u7edf\u5728\u6355\u6349\u7528\u6237\u504f\u597d\u548c\u5bf9\u8bdd\u4e0a\u4e0b\u6587\u6df1\u5c42\u8bed\u4e49\u65b9\u9762\u7684\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u5bf9\u8bdd\u63a8\u8350\u7cfb\u7edf\u5728\u6574\u5408\u5916\u90e8\u77e5\u8bc6\u56fe\u8c31\u4fe1\u606f\u65f6\u96be\u4ee5\u5904\u7406\u590d\u6742\u8bed\u4e49\u5173\u7cfb\uff0c\u5bfc\u81f4\u63a8\u8350\u7ed3\u679c\u4e0e\u7528\u6237\u671f\u671b\u4e0d\u7b26\u3002", "method": "STEP\u91c7\u7528\u4e09\u9636\u6bb5\u8bfe\u7a0b\u9010\u6b65\u5bf9\u9f50\u5bf9\u8bdd\u4e0a\u4e0b\u6587\u4e0e\u77e5\u8bc6\u56fe\u8c31\u5b9e\u4f53\uff0c\u5e76\u901a\u8fc7\u53cc\u63d0\u793a\u65b9\u6848\uff08\u5bf9\u8bdd\u524d\u7f00\u548c\u63a8\u8350\u524d\u7f00\uff09\u5c06\u878d\u5408\u8868\u793a\u6ce8\u5165\u51bb\u7ed3\u7684\u8bed\u8a00\u6a21\u578b\u4e2d\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSTEP\u5728\u4e24\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u7684\u63a8\u8350\u7cbe\u5ea6\u548c\u5bf9\u8bdd\u8d28\u91cf\u4f18\u4e8e\u4e3b\u6d41\u65b9\u6cd5\u3002", "conclusion": "STEP\u901a\u8fc7\u4e0a\u4e0b\u6587-\u77e5\u8bc6\u878d\u5408\u548c\u53cc\u63d0\u793a\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u8bdd\u63a8\u8350\u7cfb\u7edf\u7684\u6027\u80fd\u3002"}}
{"id": "2508.10703", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10703", "abs": "https://arxiv.org/abs/2508.10703", "authors": ["Yiping Song", "Jiaoyan Chen", "Renate A. Schmidt"], "title": "GenOM: Ontology Matching with Description Generation and Large Language Model", "comment": null, "summary": "Ontology matching (OM) plays an essential role in enabling semantic\ninteroperability and integration across heterogeneous knowledge sources,\nparticularly in the biomedical domain which contains numerous complex concepts\nrelated to diseases and pharmaceuticals. This paper introduces GenOM, a large\nlanguage model (LLM)-based ontology alignment framework, which enriches the\nsemantic representations of ontology concepts via generating textual\ndefinitions, retrieves alignment candidates with an embedding model, and\nincorporates exact matching-based tools to improve precision. Extensive\nexperiments conducted on the OAEI Bio-ML track demonstrate that GenOM can often\nachieve competitive performance, surpassing many baselines including\ntraditional OM systems and recent LLM-based methods. Further ablation studies\nconfirm the effectiveness of semantic enrichment and few-shot prompting,\nhighlighting the framework's robustness and adaptability.", "AI": {"tldr": "GenOM\u662f\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u672c\u4f53\u5bf9\u9f50\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u6587\u672c\u5b9a\u4e49\u4e30\u5bcc\u672c\u4f53\u6982\u5ff5\u7684\u8bed\u4e49\u8868\u793a\uff0c\u7ed3\u5408\u5d4c\u5165\u6a21\u578b\u548c\u7cbe\u786e\u5339\u914d\u5de5\u5177\u63d0\u5347\u6027\u80fd\uff0c\u5728\u751f\u7269\u533b\u5b66\u9886\u57df\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3\u751f\u7269\u533b\u5b66\u9886\u57df\u4e2d\u5f02\u6784\u77e5\u8bc6\u6e90\u7684\u8bed\u4e49\u4e92\u64cd\u4f5c\u6027\u548c\u96c6\u6210\u95ee\u9898\uff0c\u7279\u522b\u662f\u590d\u6742\u75be\u75c5\u548c\u836f\u7269\u76f8\u5173\u6982\u5ff5\u7684\u5bf9\u9f50\u3002", "method": "\u4f7f\u7528LLM\u751f\u6210\u672c\u4f53\u6982\u5ff5\u7684\u6587\u672c\u5b9a\u4e49\uff0c\u7ed3\u5408\u5d4c\u5165\u6a21\u578b\u68c0\u7d22\u5bf9\u9f50\u5019\u9009\uff0c\u5e76\u5229\u7528\u7cbe\u786e\u5339\u914d\u5de5\u5177\u63d0\u9ad8\u7cbe\u5ea6\u3002", "result": "\u5728OAEI Bio-ML\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8d85\u8d8a\u4f20\u7edf\u672c\u4f53\u5339\u914d\u7cfb\u7edf\u548c\u8fd1\u671fLLM\u65b9\u6cd5\u3002", "conclusion": "GenOM\u6846\u67b6\u901a\u8fc7\u8bed\u4e49\u589e\u5f3a\u548c\u5c11\u6837\u672c\u63d0\u793a\uff0c\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2508.10745", "categories": ["cs.AI", "cs.CV", "cs.LG", "cs.MA", "cs.MM"], "pdf": "https://arxiv.org/pdf/2508.10745", "abs": "https://arxiv.org/abs/2508.10745", "authors": ["Sayan Nag", "K J Joseph", "Koustava Goswami", "Vlad I Morariu", "Balaji Vasan Srinivasan"], "title": "Agentic Design Review System", "comment": null, "summary": "Evaluating graphic designs involves assessing it from multiple facets like\nalignment, composition, aesthetics and color choices. Evaluating designs in a\nholistic way involves aggregating feedback from individual expert reviewers.\nTowards this, we propose an Agentic Design Review System (AgenticDRS), where\nmultiple agents collaboratively analyze a design, orchestrated by a meta-agent.\nA novel in-context exemplar selection approach based on graph matching and a\nunique prompt expansion method plays central role towards making each agent\ndesign aware. Towards evaluating this framework, we propose DRS-BENCH\nbenchmark. Thorough experimental evaluation against state-of-the-art baselines\nadapted to the problem setup, backed-up with critical ablation experiments\nbrings out the efficacy of Agentic-DRS in evaluating graphic designs and\ngenerating actionable feedback. We hope that this work will attract attention\nto this pragmatic, yet under-explored research direction.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u4ee3\u7406\u534f\u4f5c\u7684\u56fe\u5f62\u8bbe\u8ba1\u8bc4\u4f30\u7cfb\u7edf\uff08AgenticDRS\uff09\uff0c\u901a\u8fc7\u56fe\u5339\u914d\u548c\u63d0\u793a\u6269\u5c55\u65b9\u6cd5\u63d0\u5347\u4ee3\u7406\u7684\u8bbe\u8ba1\u611f\u77e5\u80fd\u529b\uff0c\u5e76\u5728DRS-BENCH\u57fa\u51c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u56fe\u5f62\u8bbe\u8ba1\u8bc4\u4f30\u9700\u8981\u4ece\u591a\u4e2a\u7ef4\u5ea6\u7efc\u5408\u4e13\u5bb6\u53cd\u9988\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u7cfb\u7edf\u5316\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faAgenticDRS\u7cfb\u7edf\uff0c\u5229\u7528\u591a\u4ee3\u7406\u534f\u4f5c\u548c\u5143\u4ee3\u7406\u534f\u8c03\uff0c\u7ed3\u5408\u56fe\u5339\u914d\u548c\u63d0\u793a\u6269\u5c55\u6280\u672f\u3002", "result": "\u5b9e\u9a8c\u8868\u660eAgenticDRS\u5728\u8bc4\u4f30\u56fe\u5f62\u8bbe\u8ba1\u548c\u751f\u6210\u53cd\u9988\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u56fe\u5f62\u8bbe\u8ba1\u8bc4\u4f30\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u547c\u5401\u66f4\u591a\u5173\u6ce8\u8fd9\u4e00\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2508.10747", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.10747", "abs": "https://arxiv.org/abs/2508.10747", "authors": ["Sangwoo Jeon", "Juchul Shin", "Gyeong-Tae Kim", "YeonJe Cho", "Seongwoo Kim"], "title": "Scaling Up without Fading Out: Goal-Aware Sparse GNN for RL-based Generalized Planning", "comment": "16 pages, 10 figures", "summary": "Generalized planning using deep reinforcement learning (RL) combined with\ngraph neural networks (GNNs) has shown promising results in various symbolic\nplanning domains described by PDDL. However, existing approaches typically\nrepresent planning states as fully connected graphs, leading to a combinatorial\nexplosion in edge information and substantial sparsity as problem scales grow,\nespecially evident in large grid-based environments. This dense representation\nresults in diluted node-level information, exponentially increases memory\nrequirements, and ultimately makes learning infeasible for larger-scale\nproblems. To address these challenges, we propose a sparse, goal-aware GNN\nrepresentation that selectively encodes relevant local relationships and\nexplicitly integrates spatial features related to the goal. We validate our\napproach by designing novel drone mission scenarios based on PDDL within a grid\nworld, effectively simulating realistic mission execution environments. Our\nexperimental results demonstrate that our method scales effectively to larger\ngrid sizes previously infeasible with dense graph representations and\nsubstantially improves policy generalization and success rates. Our findings\nprovide a practical foundation for addressing realistic, large-scale\ngeneralized planning tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7a00\u758f\u3001\u76ee\u6807\u611f\u77e5\u7684GNN\u8868\u793a\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u5bc6\u96c6\u56fe\u8868\u793a\u5728\u5927\u578b\u89c4\u5212\u95ee\u9898\u4e2d\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4f7f\u7528\u5168\u8fde\u63a5\u56fe\u8868\u793a\u89c4\u5212\u72b6\u6001\uff0c\u5bfc\u81f4\u8fb9\u7f18\u4fe1\u606f\u7ec4\u5408\u7206\u70b8\u548c\u8282\u70b9\u4fe1\u606f\u7a00\u91ca\uff0c\u96be\u4ee5\u6269\u5c55\u5230\u5927\u89c4\u6a21\u95ee\u9898\u3002", "method": "\u91c7\u7528\u7a00\u758f\u3001\u76ee\u6807\u611f\u77e5\u7684GNN\u8868\u793a\uff0c\u9009\u62e9\u6027\u7f16\u7801\u5c40\u90e8\u76f8\u5173\u5173\u7cfb\u5e76\u663e\u5f0f\u6574\u5408\u76ee\u6807\u76f8\u5173\u7a7a\u95f4\u7279\u5f81\u3002", "result": "\u5728\u5927\u578b\u7f51\u683c\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u53ef\u6269\u5c55\u6027\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u7b56\u7565\u6cdb\u5316\u80fd\u529b\u548c\u6210\u529f\u7387\u3002", "conclusion": "\u4e3a\u89e3\u51b3\u73b0\u5b9e\u4e2d\u7684\u5927\u89c4\u6a21\u5e7f\u4e49\u89c4\u5212\u4efb\u52a1\u63d0\u4f9b\u4e86\u5b9e\u7528\u57fa\u7840\u3002"}}
{"id": "2508.10769", "categories": ["cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2508.10769", "abs": "https://arxiv.org/abs/2508.10769", "authors": ["Zhiqi Shen", "Shaojing Fan", "Danni Xu", "Terence Sim", "Mohan Kankanhalli"], "title": "Modeling Human Responses to Multimodal AI Content", "comment": null, "summary": "As AI-generated content becomes widespread, so does the risk of\nmisinformation. While prior research has primarily focused on identifying\nwhether content is authentic, much less is known about how such content\ninfluences human perception and behavior. In domains like trading or the stock\nmarket, predicting how people react (e.g., whether a news post will go viral),\ncan be more critical than verifying its factual accuracy. To address this, we\ntake a human-centered approach and introduce the MhAIM Dataset, which contains\n154,552 online posts (111,153 of them AI-generated), enabling large-scale\nanalysis of how people respond to AI-generated content. Our human study reveals\nthat people are better at identifying AI content when posts include both text\nand visuals, particularly when inconsistencies exist between the two. We\npropose three new metrics: trustworthiness, impact, and openness, to quantify\nhow users judge and engage with online content. We present T-Lens, an LLM-based\nagent system designed to answer user queries by incorporating predicted human\nresponses to multimodal information. At its core is HR-MCP (Human Response\nModel Context Protocol), built on the standardized Model Context Protocol\n(MCP), enabling seamless integration with any LLM. This integration allows\nT-Lens to better align with human reactions, enhancing both interpretability\nand interaction capabilities. Our work provides empirical insights and\npractical tools to equip LLMs with human-awareness capabilities. By\nhighlighting the complex interplay among AI, human cognition, and information\nreception, our findings suggest actionable strategies for mitigating the risks\nof AI-driven misinformation.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86AI\u751f\u6210\u5185\u5bb9\u5bf9\u4eba\u7c7b\u884c\u4e3a\u548c\u611f\u77e5\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u4e86MhAIM\u6570\u636e\u96c6\u548cT-Lens\u7cfb\u7edf\uff0c\u901a\u8fc7\u65b0\u6307\u6807\u91cf\u5316\u7528\u6237\u5bf9\u5185\u5bb9\u7684\u5224\u65ad\u548c\u4e92\u52a8\u3002", "motivation": "\u968f\u7740AI\u751f\u6210\u5185\u5bb9\u7684\u666e\u53ca\uff0c\u5176\u5e26\u6765\u7684\u9519\u8bef\u4fe1\u606f\u98ce\u9669\u589e\u52a0\uff0c\u4f46\u4eba\u7c7b\u5982\u4f55\u611f\u77e5\u548c\u5e94\u5bf9\u6b64\u7c7b\u5185\u5bb9\u7684\u7814\u7a76\u8f83\u5c11\u3002", "method": "\u5f15\u5165MhAIM\u6570\u636e\u96c6\uff0c\u63d0\u51fa\u4fe1\u4efb\u5ea6\u3001\u5f71\u54cd\u529b\u548c\u5f00\u653e\u6027\u4e09\u4e2a\u65b0\u6307\u6807\uff0c\u5f00\u53d1\u57fa\u4e8eLLM\u7684T-Lens\u7cfb\u7edf\uff0c\u7ed3\u5408HR-MCP\u534f\u8bae\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u4eba\u7c7b\u5728\u56fe\u6587\u7ed3\u5408\u7684\u5185\u5bb9\u4e2d\u66f4\u5bb9\u6613\u8bc6\u522bAI\u751f\u6210\u5185\u5bb9\uff0c\u5c24\u5176\u662f\u5f53\u56fe\u6587\u4e0d\u4e00\u81f4\u65f6\u3002T-Lens\u7cfb\u7edf\u80fd\u66f4\u597d\u5730\u9884\u6d4b\u4eba\u7c7b\u53cd\u5e94\u3002", "conclusion": "\u7814\u7a76\u4e3aLLM\u63d0\u4f9b\u4e86\u4eba\u7c7b\u611f\u77e5\u80fd\u529b\u7684\u5de5\u5177\uff0c\u63ed\u793a\u4e86AI\u3001\u4eba\u7c7b\u8ba4\u77e5\u548c\u4fe1\u606f\u63a5\u6536\u7684\u590d\u6742\u5173\u7cfb\uff0c\u63d0\u51fa\u4e86\u51cf\u5c11AI\u9519\u8bef\u4fe1\u606f\u98ce\u9669\u7684\u7b56\u7565\u3002"}}
{"id": "2508.10777", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10777", "abs": "https://arxiv.org/abs/2508.10777", "authors": ["Ma\u00ebl Jullien", "Marco Valentino", "Andr\u00e9 Freitas"], "title": "The Knowledge-Reasoning Dissociation: Fundamental Limitations of LLMs in Clinical Natural Language Inference", "comment": "19 pages", "summary": "Large language models are often assumed to acquire increasingly structured,\ngeneralizable internal representations simply by scaling data and parameters.\nWe interrogate this assumption by introducing a Clinical Trial Natural Language\nInference benchmark comprising four reasoning families, Causal Attribution,\nCompositional Grounding, Epistemic Verification, and Risk State Abstraction.\nEach item is paired with a targeted Ground Knowledge and Meta-Level Reasoning\nVerification (GKMRV) probe, allowing us to dissociate failures of factual\naccess from failures of inference. We evaluate six contemporary LLMs under both\ndirect and chain of thought prompting.\n  Models achieve near-ceiling GKMRV accuracy (mean accuracy 0.918) yet perform\npoorly on the main reasoning tasks (mean accuracy 0.25). Despite low accuracy,\noutput inferences are highly consistent across samples (mean 0.87), indicating\na systematic application of underlying heuristics and shortcuts.\n  These results reveal fundamental structural and representational limitations:\ncurrent LLMs often possess the relevant clinical knowledge but lack the\nstructured, composable internal representations needed to deploy it reliably\n(e.g., integrating constraints, weighing evidence, or simulating\ncounterfactuals). Decoupling knowledge from reasoning with GKMRV makes this\ndissociation explicit and measurable, providing an effective framework for\nprobing the reliability of LLMs in high-stakes domains.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7\u4e34\u5e8a\u8bd5\u9a8c\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u53d1\u73b0\uff0c\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u77e5\u8bc6\u83b7\u53d6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u7ed3\u6784\u5316\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u63ed\u793a\u4e86\u5176\u5185\u90e8\u8868\u5f81\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u63a2\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\u662f\u5426\u901a\u8fc7\u6570\u636e\u548c\u53c2\u6570\u6269\u5c55\u83b7\u5f97\u7ed3\u6784\u5316\u3001\u53ef\u6cdb\u5316\u7684\u5185\u90e8\u8868\u5f81\u3002", "method": "\u5f15\u5165\u4e34\u5e8a\u8bd5\u9a8c\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u57fa\u51c6\uff08\u56db\u79cd\u63a8\u7406\u7c7b\u578b\uff09\uff0c\u5e76\u8bbe\u8ba1GKMRV\u63a2\u9488\u5206\u79bb\u77e5\u8bc6\u83b7\u53d6\u4e0e\u63a8\u7406\u5931\u8d25\u3002\u8bc4\u4f30\u516d\u79cd\u5f53\u4ee3LLM\u7684\u76f4\u63a5\u548c\u601d\u7ef4\u94fe\u63d0\u793a\u8868\u73b0\u3002", "result": "\u6a21\u578b\u5728GKMRV\u63a2\u9488\u4e0a\u8868\u73b0\u4f18\u5f02\uff08\u51c6\u786e\u73870.918\uff09\uff0c\u4f46\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u5dee\uff08\u51c6\u786e\u73870.25\uff09\uff0c\u8f93\u51fa\u4e00\u81f4\u6027\u9ad8\uff080.87\uff09\uff0c\u8868\u660e\u5176\u4f9d\u8d56\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002", "conclusion": "\u5f53\u524dLLM\u7f3a\u4e4f\u7ed3\u6784\u5316\u3001\u53ef\u7ec4\u5408\u7684\u5185\u90e8\u8868\u5f81\uff0c\u65e0\u6cd5\u53ef\u9760\u5e94\u7528\u77e5\u8bc6\u3002GKMRV\u6846\u67b6\u4e3a\u9ad8\u98ce\u9669\u9886\u57dfLLM\u53ef\u9760\u6027\u8bc4\u4f30\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2508.10806", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10806", "abs": "https://arxiv.org/abs/2508.10806", "authors": ["Maria J. P. Peixoto", "Akriti Pandey", "Ahsan Zaman", "Peter R. Lewis"], "title": "Who Benefits from AI Explanations? Towards Accessible and Interpretable Systems", "comment": "Paper accepted for the IJCAI 2025 Workshop on Explainable Artificial\n  Intelligence (XAI): https://sites.google.com/view/xai2025/proceedings", "summary": "As AI systems are increasingly deployed to support decision-making in\ncritical domains, explainability has become a means to enhance the\nunderstandability of these outputs and enable users to make more informed and\nconscious choices. However, despite growing interest in the usability of\neXplainable AI (XAI), the accessibility of these methods, particularly for\nusers with vision impairments, remains underexplored. This paper investigates\naccessibility gaps in XAI through a two-pronged approach. First, a literature\nreview of 79 studies reveals that evaluations of XAI techniques rarely include\ndisabled users, with most explanations relying on inherently visual formats.\nSecond, we present a four-part methodological proof of concept that\noperationalizes inclusive XAI design: (1) categorization of AI systems, (2)\npersona definition and contextualization, (3) prototype design and\nimplementation, and (4) expert and user assessment of XAI techniques for\naccessibility. Preliminary findings suggest that simplified explanations are\nmore comprehensible for non-visual users than detailed ones, and that\nmultimodal presentation is required for more equitable interpretability.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u53ef\u89e3\u91caAI\uff08XAI\uff09\u7684\u53ef\u8bbf\u95ee\u6027\u95ee\u9898\uff0c\u7279\u522b\u662f\u5bf9\u89c6\u89c9\u969c\u788d\u7528\u6237\u7684\u9002\u7528\u6027\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5305\u5bb9\u6027\u8bbe\u8ba1\u65b9\u6cd5\u3002", "motivation": "\u968f\u7740AI\u7cfb\u7edf\u5728\u5173\u952e\u51b3\u7b56\u9886\u57df\u7684\u5e94\u7528\u589e\u52a0\uff0c\u53ef\u89e3\u91ca\u6027\u6210\u4e3a\u63d0\u5347\u7528\u6237\u7406\u89e3\u548c\u9009\u62e9\u7684\u5173\u952e\uff0c\u4f46XAI\u5bf9\u89c6\u89c9\u969c\u788d\u7528\u6237\u7684\u53ef\u8bbf\u95ee\u6027\u7814\u7a76\u4e0d\u8db3\u3002", "method": "\u901a\u8fc7\u6587\u732e\u7efc\u8ff0\u548c\u56db\u90e8\u5206\u65b9\u6cd5\u8bba\u9a8c\u8bc1\uff1aAI\u7cfb\u7edf\u5206\u7c7b\u3001\u7528\u6237\u89d2\u8272\u5b9a\u4e49\u3001\u539f\u578b\u8bbe\u8ba1\u4e0e\u5b9e\u73b0\u3001\u4e13\u5bb6\u548c\u7528\u6237\u8bc4\u4f30\u3002", "result": "\u521d\u6b65\u53d1\u73b0\u8868\u660e\uff0c\u7b80\u5316\u89e3\u91ca\u6bd4\u8be6\u7ec6\u89e3\u91ca\u66f4\u6613\u7406\u89e3\uff0c\u4e14\u591a\u6a21\u6001\u5448\u73b0\u6709\u52a9\u4e8e\u63d0\u5347\u516c\u5e73\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "XAI\u9700\u8981\u66f4\u591a\u5305\u5bb9\u6027\u8bbe\u8ba1\uff0c\u4ee5\u89e3\u51b3\u89c6\u89c9\u969c\u788d\u7528\u6237\u7684\u53ef\u8bbf\u95ee\u6027\u5dee\u8ddd\u3002"}}
