<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 29]
- [cs.CR](#cs.CR) [Total: 49]
- [cs.AI](#cs.AI) [Total: 50]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Challenges and Practices in Quantum Software Testing and Debugging: Insights from Practitioners](https://arxiv.org/abs/2506.17306)
*Jake Zappin,Trevor Stalnaker,Oscar Chaparro,Denys Poshyvanyk*

Main category: cs.SE

TL;DR: 量子软件工程面临独特挑战，开发者主要依赖经典测试和调试方法，缺乏量子专用工具，导致效率低下。


<details>
  <summary>Details</summary>
Motivation: 了解量子软件开发者在测试和调试中的实际做法及挑战，以推动工具改进。

Method: 调查26名量子软件开发者并进行后续访谈，聚焦测试、调试和常见问题。

Result: 测试普遍采用单元测试（88%）、回归测试（54%）和验收测试（54%），但仅31%使用量子专用工具。调试多依赖经典方法，如打印语句和模拟器。主要问题源于经典因素，如库更新（81%）和兼容性问题（62%）。

Conclusion: 亟需开发更符合量子开发者需求的测试和调试工具，以提升效率。

Abstract: Quantum software engineering is an emerging discipline with distinct
challenges, particularly in testing and debugging. As quantum computing
transitions from theory to implementation, developers face issues not present
in classical software development, such as probabilistic execution, limited
observability, shallow abstractions, and low awareness of quantum-specific
tools. To better understand current practices, we surveyed 26 quantum software
developers from academia and industry and conducted follow-up interviews
focused on testing, debugging, and recurring challenges. All participants
reported engaging in testing, with unit testing (88%), regression testing
(54%), and acceptance testing (54%) being the most common. However, only 31%
reported using quantum-specific testing tools, relying instead on manual
methods. Debugging practices were similarly grounded in classical strategies,
such as print statements, circuit visualizations, and simulators, which
respondents noted do not scale well. The most frequently cited sources of bugs
were classical in nature-library updates (81%), developer mistakes (68%), and
compatibility issues (62%)-often worsened by limited abstraction in existing
SDKs. These findings highlight the urgent need for better-aligned testing and
debugging tools, integrated more seamlessly into the workflows of quantum
developers. We present these results in detail and offer actionable
recommendations grounded in the real-world needs of practitioners.

</details>


### [2] [An Expert Survey on Models and Digital Twins](https://arxiv.org/abs/2506.17313)
*Jonathan Reif,Daniel Dittler,Milapji Singh Gill,Tamás Farkas,Valentin Stegmaier,Felix Gehlhoff,Tobias Kleinert,Michael Weyrich*

Main category: cs.SE

TL;DR: 研究通过专家调查揭示了数字孪生（DTs）中集成多种数字模型（DMs）的挑战，包括缺乏标准化接口、高人工适配成本和模型复用支持不足，提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 数字孪生（DTs）在工业应用中日益重要，但集成多种数字模型（DMs）的挑战和需求缺乏行业视角的研究。

Method: 通过跨多个应用领域的专家调查，识别和分析在DTs中集成DMs的挑战。

Result: 发现缺乏标准化接口、高人工适配成本和模型复用支持不足等问题。

Conclusion: 未来研究需关注自动化模型组合和基于语义的互操作性。

Abstract: Digital Twins (DTs) are becoming increasingly vital for future industrial
applications, enhancing monitoring, control, and optimization of physical
assets. This enhancement is made possible by integrating various Digital Models
(DMs) within DTs, which must interoperate to represent different system aspects
and fulfill diverse application purposes. However, industry perspectives on the
challenges and research needs for integrating these models are rarely obtained.
Thus, this study conducts an expert survey across multiple application domains
to identify and analyze the challenges in utilizing diverse DMs within DTs. The
results reveal missing standardized interfaces, high manual adaptation effort,
and limited support for model reuse across lifecycle phases, highlighting
future research needs in automated model composition and semantics-based
interoperability.

</details>


### [3] [Large Language Models for Spreadsheets: Benchmarking Progress and Evaluating Performance with FLARE](https://arxiv.org/abs/2506.17330)
*Simon Thorne*

Main category: cs.SE

TL;DR: 研究评估了大型语言模型（LLMs）在电子表格任务中的表现，发现其在简单任务中表现良好，但在复杂任务中容易出错，提出了新的基准FLARE以评估LLMs的逻辑推理能力。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在电子表格任务中的能力，填补现有研究的空白。

Method: 引入一个全面的基准框架，评估LLMs在电子表格功能、公式生成和数据操作任务中的表现。

Result: LLMs在简单任务中表现良好，但在复杂任务中容易产生看似合理但错误的输出。

Conclusion: 当前LLMs在需要精确逻辑推理的电子表格任务中存在局限性，需要整合符号推理能力，并提出了FLARE基准以支持未来研究。

Abstract: Large Language Models (LLMs) have demonstrated some significant capabilities
across various domains; however, their effectiveness in spreadsheet related
tasks remains underexplored. This study introduces a foundation for a
comprehensive benchmark framework to evaluate the performance of leading LLMs
in executing spreadsheet functions, formula generation and data manipulation
tasks. The benchmark encompasses tasks ranging from basic formula creation to
complex, real world spreadsheet scenarios. Our findings reveal that while LLMs
exhibit proficiency in straightforward tasks, they often falter in complex,
multi step operations, frequently producing plausible yet incorrect outputs.
These results underscore the limitations of current LLMs in handling
spreadsheet tasks that require precise logical reasoning and highlight the need
for integrating symbolic reasoning capabilities into LLM architectures. To
support this, we introduce FLARE (Formula Logic, Auditing, Reasoning and
Evaluation) a new benchmark for evaluating LLM performance on real-world
spreadsheet logic, auditing, and reasoning tasks.

</details>


### [4] [LMR-BENCH: Evaluating LLM Agent's Ability on Reproducing Language Modeling Research](https://arxiv.org/abs/2506.17335)
*Shuo Yan,Ruochen Li,Ziming Luo,Zimu Wang,Daoyang Li,Liqiang Jing,Kaiyu He,Peilin Wu,George Michalopoulos,Yue Zhang,Ziyang Zhang,Mian Zhang,Zhiyu Chen,Xinya Du*

Main category: cs.SE

TL;DR: LMR-BENCH是一个评估LLM代理从NLP研究论文中重现代码能力的基准，包含28个任务。实验显示，即使最先进的模型在科学推理和代码合成方面仍有局限。


<details>
  <summary>Details</summary>
Motivation: 探索LLM代理在从研究论文中重现代码这一关键任务中的能力，填补现有研究的空白。

Method: 设计LMR-BENCH基准，包含28个任务，基于23篇顶级NLP论文。实验采用标准提示和LLM代理设置，评估代码正确性。

Result: 实验结果表明，即使最先进的LLM在科学推理和代码合成方面仍存在显著局限性。

Conclusion: LLM代理在自主重现科学研究代码方面仍有重大挑战，需进一步改进。

Abstract: Large language model (LLM) agents have demonstrated remarkable potential in
advancing scientific discovery. However, their capability in the fundamental
yet crucial task of reproducing code from research papers, especially in the
NLP domain, remains underexplored. This task includes unique complex reasoning
challenges in the intellectual synthesis of abstract concepts and the
comprehension of code repositories with interdependent files. Motivated by this
gap, we present LMR-BENCH, a benchmark designed to systematically evaluate the
capability of LLM agents on code reproduction from Language Modeling Research.
It consists of 28 code reproduction tasks derived from 23 research papers
published in top-tier NLP venues over the past five years, spanning nine
fundamental categories. Models are provided with a research paper, a code
repository containing one or more masked functions, and instructions for
implementing these functions. We conduct extensive experiments in standard
prompting and LLM agent settings with state-of-the-art LLMs, evaluating the
accuracy of unit tests and performing LLM-based evaluation of code correctness.
Experimental results reveal that even the most advanced models still exhibit
persistent limitations in scientific reasoning and code synthesis, highlighting
critical gaps in LLM agents' ability to autonomously reproduce scientific
research

</details>


### [5] [Re-Evaluating Code LLM Benchmarks Under Semantic Mutation](https://arxiv.org/abs/2506.17369)
*Zhiyuan Pan,Xing Hu,Xin Xia,Xiaohu Yang*

Main category: cs.SE

TL;DR: 论文研究了代码基准测试中提示敏感性问题，发现即使微小提示变化也会显著影响模型性能，并可能导致模型排名不一致。


<details>
  <summary>Details</summary>
Motivation: 现有代码基准测试通常依赖单一提示模板，容易因提示敏感性导致评估不可靠，而此前研究主要关注自然语言处理任务。

Method: 提出一个通用框架生成语义和结构相似的提示模板，并在8个代码任务和10个LLM上进行实验，使用统计指标分析结果。

Result: 实验表明，微小提示变化会导致性能显著波动，并可能影响模型排名。

Conclusion: 未来设计代码基准测试时应考虑提示敏感性，以提高评估的可靠性和准确性。

Abstract: In the era of large language models (LLMs), code benchmarks have become an
important research area in software engineering and are widely used by
practitioners. These benchmarks evaluate the performance of LLMs on specific
code-related tasks, such as code understanding and generation. A critical step
in constructing code benchmarks is the design of prompts. However, as existing
code benchmarks typically rely on a single prompt template per task, they are
prone to the issue of prompt sensitivity, where minor prompt variations could
result in substantial performance variations, leading to unreliable evaluations
of model capabilities.
  While previous studies have explored prompt sensitivity, their experimental
designs and findings are limited to traditional natural language processing
(NLP) tasks. In this paper, we present an empirical study to investigate prompt
sensitivity in code benchmarks. We first propose a general framework that
modifies prompt templates in a manner that preserves both their semantics and
their structure as much as possible. Based on the framework, we conduct
extensive experiments across eight code benchmark tasks on 10 representative
open-source LLMs, with each task featuring 100 semantically similar prompt
templates. We then analyze the evaluation results using various statistical
metrics, focusing on both absolute and relative model performance. Our findings
suggest that even slight prompt variations can lead to significant shifts in
performance. Additionally, we observe that such variations can introduce
inconsistencies in the performance rankings across different models. These
insights highlight the need for considering prompt sensitivity when designing
future code benchmarks, to ensure more reliable and accurate evaluation of LLM
capabilities.

</details>


### [6] [Breaking Single-Tester Limits: Multi-Agent LLMs for Multi-User Feature Testing](https://arxiv.org/abs/2506.17539)
*Sidong Feng,Changhao Du,Huaxiao Liu,Qingnan Wang,Zhengwei Lv,Mengfei Wang,Chunyang Chen*

Main category: cs.SE

TL;DR: MAdroid是一种基于大型语言模型的多代理方法，用于自动化测试多用户交互功能，效果显著。


<details>
  <summary>Details</summary>
Motivation: 移动应用的多用户交互功能测试自动化面临挑战，现有方法无法满足动态协作需求。

Method: MAdroid采用多代理架构，包括用户代理（Operator）和监督代理（Coordinator和Observer），分别负责任务指导、用户交互模拟和过程监控。

Result: 在41项任务中，MAdroid成功完成82.9%，动作相似度达96.8%，并发现11个交互性错误。

Conclusion: MAdroid在多用户交互测试中表现出色，具有实际应用潜力。

Abstract: The growing dependence on mobile phones and their apps has made multi-user
interactive features, like chat calls, live streaming, and video conferencing,
indispensable for bridging the gaps in social connectivity caused by physical
and situational barriers. However, automating these interactive features for
testing is fraught with challenges, owing to their inherent need for timely,
dynamic, and collaborative user interactions, which current automated testing
methods inadequately address. Inspired by the concept of agents designed to
autonomously and collaboratively tackle problems, we propose MAdroid, a novel
multi-agent approach powered by the Large Language Models (LLMs) to automate
the multi-user interactive task for app feature testing. Specifically, MAdroid
employs two functional types of multi-agents: user agents (Operator) and
supervisor agents (Coordinator and Observer). Each agent takes a specific role:
the Coordinator directs the interactive task; the Operator mimics user
interactions on the device; and the Observer monitors and reviews the task
automation process. Our evaluation, which included 41 multi-user interactive
tasks, demonstrates the effectiveness of our approach, achieving 82.9% of the
tasks with 96.8% action similarity, outperforming the ablation studies and
state-of-the-art baselines. Additionally, a preliminary investigation
underscores MAdroid's practicality by helping identify 11 multi-user
interactive bugs during regression app testing, confirming its potential value
in real-world software development contexts.

</details>


### [7] [CodeMorph: Mitigating Data Leakage in Large Language Model Assessment](https://arxiv.org/abs/2506.17627)
*Hongzhou Rao,Yanjie Zhao,Wenjie Zhu,Ling Xiao,Meizhen Wang,Haoyu Wang*

Main category: cs.SE

TL;DR: CodeMorph是一种支持多编程语言并保留跨文件依赖关系的代码扰动方法，通过语义保留变换和遗传算法选择，有效降低LLM在代码任务中的性能，减少数据泄漏。


<details>
  <summary>Details</summary>
Motivation: 由于基准泄漏问题导致代码LLM评估指标虚高，现有扰动方法无法生成复杂多样的代码变体，且缺乏多语言支持。

Method: CodeMorph结合26种语义保留变换方法和遗传算法PESO，迭代扰动代码并选择最优扰动策略，降低相似性分数。

Result: 实验显示，CodeMorph使LLM在五种编程语言上的代码完成任务准确率平均下降24.67%，Python降幅最大（45%）；PESO优化的代码相似性分数平均降低7.01%。

Conclusion: CodeMorph能有效缓解数据泄漏问题，提升代码LLM评估的可靠性。

Abstract: Concerns about benchmark leakage in large language models for code (Code
LLMs) have raised issues of data contamination and inflated evaluation metrics.
The diversity and inaccessibility of many training datasets make it difficult
to prevent data leakage entirely, even with time lag strategies. Consequently,
generating new datasets through code perturbation has become essential.
However, existing methods often fail to produce complex and diverse variations,
struggle with complex cross-file dependencies, and lack support for multiple
programming languages, which limits their effectiveness in enhancing LLM
evaluations for coding tasks. To fill this gap, we propose CodeMorph, an
approach designed to support multiple programming languages while preserving
cross-file dependencies to mitigate data leakage. CodeMorph consists of two
main components that work together to enhance the perturbation process. The
first component employs 26 semantic-preserving transformation methods to
iteratively perturb code, generating diverse variations while ensuring that the
modified code remains compilable. The second component introduces a genetic
algorithm-based selection algorithm, PESO, to identify the more effective
perturbation method for each iteration by targeting lower similarity scores
between the perturbed and original code, thereby enhancing overall perturbation
effectiveness. Experimental results demonstrate that after applying CodeMorph,
the accuracy of the LLM on code completion tasks across five programming
languages decreased by an average of 24.67%, with Python showing the most
significant reduction at 45%. The similarity score of code optimized by PESO
is, on average, 7.01% lower than that of randomly perturbed code, peaking at a
reduction of 42.86%.

</details>


### [8] [Deep Learning Framework Testing via Model Mutation: How Far Are We?](https://arxiv.org/abs/2506.17638)
*Yanzhou Mu,Rong Wang,Juan Zhai,Chunrong Fang,Xiang Chen,Zhiyuan Peng,Peiran Yang,Ruixiang Qian,Shaoyu Yang,Zhenyu Chen*

Main category: cs.SE

TL;DR: 本文研究了基于模型变异的深度学习框架缺陷检测方法的有效性，提出了优化策略，并验证了其效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法在检测深度学习框架缺陷时存在高误报率和开发者忽视的问题，因此需要改进。

Method: 收集缺陷报告并分类，分析现有方法的不足，提出优化策略。

Result: 优化后发现了7个新缺陷，其中4个被确认为高优先级问题。共发现39个独特缺陷，31个被开发者确认。

Conclusion: 优化策略显著提升了缺陷检测的有效性，为深度学习框架测试提供了改进方向。

Abstract: Deep Learning (DL) frameworks are a fundamental component of DL development.
Therefore, the detection of DL framework defects is important and challenging.
As one of the most widely adopted DL testing techniques, model mutation has
recently gained significant attention. In this study, we revisit the defect
detection ability of existing mutation-based testing methods and investigate
the factors that influence their effectiveness. To begin with, we reviewed
existing methods and observed that many of them mutate DL models (e.g.,
changing their parameters) without any customization, ignoring the unique
challenges in framework testing. Another issue with these methods is their
limited effectiveness, characterized by a high rate of false positives caused
by illegal mutations arising from the use of generic, non-customized mutation
operators. Moreover, we tracked the defects identified by these methods and
discovered that most of them were ignored by developers. Motivated by these
observations, we investigate the effectiveness of existing mutation-based
testing methods in detecting important defects that have been authenticated by
framework developers. We begin by collecting defect reports from three popular
frameworks and classifying them based on framework developers' ratings to build
a comprehensive dataset. We then perform an in-depth analysis to uncover
valuable insights. Based on our findings, we propose optimization strategies to
address the shortcomings of existing approaches. Following these optimizations,
we identified seven new defects, four of which were confirmed by developers as
high-priority issues, with three resolved. In summary, we identified 39 unique
defects across just 23 models, of which 31 were confirmed by developers, and
eight have been fixed.

</details>


### [9] [May the Feedback Be with You! Unlocking the Power of Feedback-Driven Deep Learning Framework Fuzzing via LLMs](https://arxiv.org/abs/2506.17642)
*Shaoyu Yang,Chunrong Fang,Haifeng Lin,Xiang Chen,Zhenyu Chen*

Main category: cs.SE

TL;DR: FUEL是一种基于LLM的反馈驱动模糊测试方法，用于检测深度学习框架中的错误，通过分析LLM和生成LLM代理提升测试效果。


<details>
  <summary>Details</summary>
Motivation: 深度学习框架中的错误可能导致严重后果，现有模糊测试技术未能充分利用多种反馈信息，且LLM在反馈分析方面的潜力未被挖掘。

Method: FUEL采用两个LLM代理：分析LLM从反馈信息中推断分析摘要，生成LLM根据这些摘要创建测试用例。

Result: FUEL在PyTorch和TensorFlow中检测到104个错误，其中93个为新错误，47个已修复，5个获得CVE编号。

Conclusion: 考虑多种反馈类型有助于提升模糊测试性能，利用LLM分析反馈信息是一个有前景的方向。

Abstract: Artificial Intelligence (AI) Infrastructures, represented by Deep Learning
(DL) frameworks, have served as fundamental DL systems over the last decade.
However, the bugs in DL frameworks could lead to catastrophic consequences in
some critical scenarios (e.g., healthcare and autonomous driving). A simple yet
effective way to find bugs in DL frameworks is fuzz testing (Fuzzing).
Unfortunately, existing fuzzing techniques have not comprehensively considered
multiple types of feedback. Additionally, they analyze feedback in a
coarse-grained manner, such as mutating the test cases only according to
whether the coverage increases. Recently, researchers introduced Large Language
Models (LLMs) into fuzzing. However, current LLM-based fuzzing techniques only
focus on using LLMs to generate test cases while overlooking their potential to
analyze feedback information, failing to create more valid and diverse test
cases. To fill this gap, we propose FUEL to break the seal of Feedback-driven
fuzzing for DL frameworks. The backbone of FUEL comprises two LLM-based agents,
namely analysis LLM and generation LLM. Analysis LLM agent infers analysis
summaries from feedback information, while the generation LLM agent creates
tests guided by these analysis summaries. So far, FUEL has detected 104 bugs
for PyTorch and TensorFlow, with 93 confirmed as new bugs, 47 already fixed,
and 5 assigned with CVE IDs. Our work indicates that considering multiple types
of feedback is beneficial to fuzzing performance, and leveraging LLMs to
analyze feedback information is a promising direction. Our artifact is
available at https://github.com/NJU-iSE/FUEL

</details>


### [10] [Improving Compiler Bug Isolation by Leveraging Large Language Models](https://arxiv.org/abs/2506.17647)
*Yixian Qi,Jiajun Jiang,Fengjie Li,Bowen Chen,Hongyu Zhang,Junjie Chen*

Main category: cs.SE

TL;DR: AutoCBI利用大型语言模型（LLMs）改进编译器错误定位，通过总结文件功能和重新排序可疑文件，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 编译器错误可能导致严重后果，传统方法在可扩展性和有效性上存在不足，LLMs的进展为改进提供了新思路。

Method: AutoCBI结合LLMs总结文件功能，并利用四种信息（失败测试程序、文件功能摘要、可疑文件列表和编译配置）重新排序可疑文件。

Result: 在GCC和LLVM的120个真实错误上，AutoCBI在Top-1结果中分别比RecBi、DiWi和FuseFL多定位66.67%/69.23%、300%/340%和100%/57.14%的错误。

Conclusion: AutoCBI通过LLMs显著提升编译器错误定位效果，各组件均对结果有重要贡献。

Abstract: Compilers play a foundational role in building reliable software systems, and
bugs within them can lead to catastrophic consequences. The compilation process
typically involves hundreds of files, making traditional automated bug
isolation techniques inapplicable due to scalability or effectiveness issues.
Current mainstream compiler bug localization techniques have limitations in
test program mutation and resource consumption. Inspired by the recent advances
of pre-trained Large Language Models (LLMs), we propose an innovative approach
named AutoCBI, which (1) uses LLMs to summarize compiler file functions and (2)
employs specialized prompts to guide LLM in reordering suspicious file
rankings. This approach leverages four types of information: the failing test
program, source file function summaries, lists of suspicious files identified
through analyzing test coverage, as well as compilation configurations with
related output messages, resulting in a refined ranking of suspicious files.
Our evaluation of AutoCBI against state-of-the-art approaches (DiWi, RecBi and
FuseFL) on 120 real-world bugs from the widely-used GCC and LLVM compilers
demonstrates its effectiveness. Specifically, AutoCBI isolates 66.67%/69.23%,
300%/340%, and 100%/57.14% more bugs than RecBi, DiWi, and FuseFL,
respectively, in the Top-1 ranked results for GCC/LLVM. Additionally, the
ablation study underscores the significance of each component in our approach.

</details>


### [11] [PAGENT: Learning to Patch Software Engineering Agents](https://arxiv.org/abs/2506.17772)
*Haoran Xue,Gias Uddin,Song Wang*

Main category: cs.SE

TL;DR: 论文研究了LLM代码代理生成的失败补丁，分析了失败原因并提出了一种新工具PAGENT来修复类型相关错误。


<details>
  <summary>Details</summary>
Motivation: 了解LLM代码代理生成失败补丁的根因，并探索修复方法。

Method: 收集114个未解决问题，分析769个失败补丁，提出分类法，并设计PAGENT工具结合程序分析和LLM推理修复类型错误。

Result: PAGENT成功修复了127个类型相关失败补丁中的29个。

Conclusion: PAGENT在修复类型相关错误方面表现出潜力，但仍需改进。

Abstract: LLM Agents produce patches automatically to resolve an issue. However, they
can generate inaccurate patches. Little is known about the root causes behind
those failed patches or how those could be fixed. This paper reports an
empirical study of the failed patches generated by seven top LLM code agents.
We collected 114 issues from the SWE-bench Lite dataset that remained
unresolved across the agents. The seven agents produced a total of 769 failed
patches for those issues, which we checked with a combination of GPT-4o and
manual analysis. We present a taxonomy of the failure reasons across the
patches. The taxonomy contains six categories, with several sub-categories
under each category. For example, a frequently observed category is the
inability of an LLM to correctly infer/produce the appropriate variable type in
the produced patch. As a first step towards addressing such type-related
errors, we designed PAGENT (Patch Agent). PAGENT utilizes program analysis
techniques like CFG creation and exploration to infer the type of information
of a patch. PAGENT does this by applying repository-level static code analysis
techniques. Then, PAGENT refines the inferred type by further utilizing an
LLM-based inference technique. We tested PAGENT on all 127 type-related failed
patches from the top three agents in our study. PAGENT could fix 29 of the 127
failed patches.

</details>


### [12] [SAVANT: Vulnerability Detection in Application Dependencies through Semantic-Guided Reachability Analysis](https://arxiv.org/abs/2506.17798)
*Wang Lingxiang,Quanzhi Fu,Wenjia Song,Gelei Deng,Yi Liu,Dan Williams,Ying Zhang*

Main category: cs.SE

TL;DR: SAVANT利用语义预处理和LLM技术，显著提升了Java开发中第三方库漏洞检测的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有SCA工具在检测第三方库漏洞时因语义理解不足和计算复杂性导致误报率高，影响开发效率。

Method: SAVANT结合语义预处理和LLM上下文分析，通过代码分段和LLM反射技术精准识别漏洞。

Result: 在55个实际应用中，SAVANT的精确度为83.8%，召回率为73.8%，准确率为69.0%，F1分数为78.5%。

Conclusion: SAVANT在漏洞检测性能上优于现有SCA工具，为开发团队提供了更可靠的解决方案。

Abstract: The integration of open-source third-party library dependencies in Java
development introduces significant security risks when these libraries contain
known vulnerabilities. Existing Software Composition Analysis (SCA) tools
struggle to effectively detect vulnerable API usage from these libraries due to
limitations in understanding API usage semantics and computational challenges
in analyzing complex codebases, leading to inaccurate vulnerability alerts that
burden development teams and delay critical security fixes.
  To address these challenges, we proposed SAVANT by leveraging two insights:
proof-of-vulnerability test cases demonstrate how vulnerabilities can be
triggered in specific contexts, and Large Language Models (LLMs) can understand
code semantics. SAVANT combines semantic preprocessing with LLM-powered context
analysis for accurate vulnerability detection. SAVANT first segments source
code into meaningful blocks while preserving semantic relationships, then
leverages LLM-based reflection to analyze API usage context and determine
actual vulnerability impacts. Our evaluation on 55 real-world applications
shows that SAVANT achieves 83.8% precision, 73.8% recall, 69.0% accuracy, and
78.5% F1-score, outperforming state-of-the-art SCA tools.

</details>


### [13] [Is Your Automated Software Engineer Trustworthy?](https://arxiv.org/abs/2506.17812)
*Noble Saji Mathews,Meiyappan Nagappan*

Main category: cs.SE

TL;DR: BouncerBench是一个评估LLM在软件工程任务中是否能在输入模糊或输出不可靠时拒绝响应的基准测试。


<details>
  <summary>Details</summary>
Motivation: 现有LLM工具在处理模糊输入或错误输出时缺乏拒绝机制，导致不可靠行为。

Method: 引入BouncerBench基准测试，评估LLM是否能区分模糊问题和错误补丁，并实现基本的输入输出过滤机制。

Result: 多数LLM无法在模糊输入或错误输出时拒绝响应，表明改进空间大。

Conclusion: BouncerBench为构建更可靠的代码代理提供了初步评估工具。

Abstract: Large Language Models (LLMs) are being increasingly used in software
engineering tasks, with an increased focus on bug report resolution over the
past year. However, most proposed systems fail to properly handle uncertain or
incorrect inputs and outputs. Existing LLM-based tools and coding agents
respond to every issue and generate a patch for every case, even when the input
is vague or their own output is incorrect. There are no mechanisms in place to
abstain when confidence is low. This leads to unreliable behaviour, such as
hallucinated code changes or responses based on vague issue reports. We
introduce BouncerBench, a benchmark that evaluates whether LLM-based software
agents can refuse to act when inputs are ill-defined or refuse to respond when
their own outputs are likely to be incorrect. Unlike prior benchmarks that
implicitly incentivize models to generate responses even when uncertain,
BouncerBench aims to improve precision by targeting two overlooked failure
points: (1) vague or underspecified issue descriptions in tickets and (2)
logically or functionally incorrect code patches created by the system. It
measures whether proposed systems can distinguish actionable issues from vague
tickets and valid patches from untrustworthy ones. We also implement a basic
input and output bouncer, evaluating how well current LLMs can abstain when
needed. Our results show that most models fail to abstain from underspecified
inputs or incorrect outputs. Hence, we conclude that there is significant room
for improvement before LLMs can be trusted to make correct decisions and
recommendations in real-world software engineering workflows. BouncerBench
provides a first step toward evaluating and building more cautious, trustworthy
code agents. The replication package, dataset, and leaderboard can be found at
bouncerbench.com

</details>


### [14] [The Impact of AI-Generated Solutions on Software Architecture and Productivity: Results from a Survey Study](https://arxiv.org/abs/2506.17833)
*Giorgio Amasanti,Jasmin Jahic*

Main category: cs.SE

TL;DR: AI工具显著提升软件工程师的生产力，但随着项目复杂度增加，生产力提升效果减弱。AI生成的代码片段对软件质量无显著负面影响，但解决复杂问题时质量下降，需架构师进行问题分解和集成。


<details>
  <summary>Details</summary>
Motivation: 探讨AI工具对软件工程师生产力的影响及其对软件质量的长期影响。

Method: 通过调查使用AI工具的软件从业者收集数据。

Result: AI工具显著提高生产力，但对复杂项目效果减弱；AI生成的代码片段不影响质量，但复杂问题解决方案质量较低。

Conclusion: AI工具在提升生产力方面有效，但需架构师介入以解决复杂问题，确保软件质量。

Abstract: AI-powered software tools are widely used to assist software engineers.
However, there is still a need to understand the productivity benefits of such
tools for software engineers. In addition to short-term benefits, there is a
question of how adopting AI-generated solutions affects the quality of software
over time (e.g., maintainability and extendability).
  To provide some insight on these questions, we conducted a survey among
software practitioners who use AI tools. Based on the data collected from our
survey, we conclude that AI tools significantly increase the productivity of
software engineers. However, the productivity benefits of using AI tools reduce
as projects become more complex. The results also show that there are no
significant negative influences of adopting AI-generated solutions on software
quality, as long as those solutions are limited to smaller code snippets.
However, when solving larger and more complex problems, AI tools generate
solutions of a lower quality, indicating the need for architects to perform
problem decomposition and solution integration.

</details>


### [15] [Software Reuse in the Generative AI Era: From Cargo Cult Towards AI Native Software Engineering](https://arxiv.org/abs/2506.17937)
*Tommi Mikkonen,Antero Taivalsaari*

Main category: cs.SE

TL;DR: 论文探讨了AI辅助生成式软件重用在‘AI原生’软件工程中的影响，提出了相关问题，并定义了研究议程和行动呼吁。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能和生成式软件重用的兴起，传统软件重用方法正被AI辅助方法取代，这引发了类似‘货物崇拜开发’的新问题。

Method: 通过讨论AI辅助生成式软件重用的影响，提出相关问题和研究议程。

Result: 揭示了AI辅助软件重用可能带来的挑战和问题。

Conclusion: 呼吁研究社区关注并解决AI辅助软件重用中的核心问题。

Abstract: Software development is currently under a paradigm shift in which artificial
intelligence and generative software reuse are taking the center stage in
software creation. Consequently, earlier software reuse practices and methods
are rapidly being replaced by AI-assisted approaches in which developers place
their trust on code that has been generated by artificial intelligence. This is
leading to a new form of software reuse that is conceptually not all that
different from cargo cult development. In this paper we discuss the
implications of AI-assisted generative software reuse in the context of
emerging "AI native" software engineering, bring forth relevant questions, and
define a tentative research agenda and call to action for tackling some of the
central issues associated with this approach.

</details>


### [16] [Build It Clean: Large-Scale Detection of Code Smells in Build Scripts](https://arxiv.org/abs/2506.17948)
*Mahzabin Tamanna,Yash Chandrani,Matthew Burrows,Brandon Wroblewski,Laurie Williams,Dominik Wermke*

Main category: cs.SE

TL;DR: 该研究通过分析GitHub上的构建脚本和问题，识别了13种代码异味类别，并开发了静态分析工具Sniffer来检测这些异味，提出了改进构建脚本的建议。


<details>
  <summary>Details</summary>
Motivation: 构建脚本中可能引入代码异味，导致构建失败或增加技术债务，研究旨在帮助开发者避免这些问题。

Method: 采用混合方法，定性分析2000个GitHub问题，定量分析5882个构建脚本（Maven、Gradle、CMake、Make），开发工具Sniffer检测异味。

Result: 发现10,895个异味实例，Maven中Insecure URLs最常见，Gradle和CMake中Hardcoded Paths/URLs常见，Makefiles中Wildcard Usage最多。

Conclusion: 研究提出了减少构建脚本异味的策略，以提高软件项目的效率、可靠性和可维护性。

Abstract: Build scripts are files that automate the process of compiling source code,
managing dependencies, running tests, and packaging software into deployable
artifacts. These scripts are ubiquitous in modern software development
pipelines for streamlining testing and delivery. While developing build
scripts, practitioners may inadvertently introduce code smells. Code smells are
recurring patterns of poor coding practices that may lead to build failures or
increase risk and technical debt. The goal of this study is to aid
practitioners in avoiding code smells in build scripts through an empirical
study of build scripts and issues on GitHub. We employed a mixed-methods
approach, combining qualitative and quantitative analysis. We conducted a
qualitative analysis of 2000 build-script-related GitHub issues. Next, we
developed a static analysis tool, Sniffer, to identify code smells in 5882
build scripts of Maven, Gradle, CMake, and Make files, collected from 4877
open-source GitHub repositories. We identified 13 code smell categories, with a
total of 10,895 smell occurrences, where 3184 were in Maven, 1214 in Gradle,
337 in CMake, and 6160 in Makefiles.
  Our analysis revealed that Insecure URLs were the most prevalent code smell
in Maven build scripts, while Hardcoded Paths/URLs were commonly observed in
both Gradle and CMake scripts. Wildcard Usage emerged as the most frequent
smell in Makefiles. The co-occurrence analysis revealed strong associations
between specific smell pairs of Hardcoded Paths/URLs with Duplicates, and
Inconsistent Dependency Management with Empty or Incomplete Tags, indicating
potential underlying issues in the build script structure and maintenance
practices. Based on our findings, we recommend strategies to mitigate the
existence of code smells in build scripts to improve the efficiency,
reliability, and maintainability of software projects.

</details>


### [17] [VFArchē: A Dual-Mode Framework for Locating Vulnerable Functions in Open-Source Software](https://arxiv.org/abs/2506.18050)
*Lyuye Zhang,Jian Zhang,Kaixuan Li,Chong Wang,Chengwei Liu,Jiahui Wu,Sen Chen,Yaowen Zheng,Yang Liu*

Main category: cs.SE

TL;DR: VFArch=e是一种双模式方法，用于定位软件依赖中的漏洞函数，适用于有或没有补丁的场景，显著提升了SCA工具的准确性。


<details>
  <summary>Details</summary>
Motivation: 现代漏洞数据库（如NVD）通常缺乏漏洞函数的关键信息，而补丁信息也不总是可用，需要一种全面的解决方案。

Method: VFArch=e采用双模式设计，结合补丁信息和无补丁场景的搜索策略，自动定位漏洞函数。

Result: 实验结果显示，VFArch=e在Patch-present和Patch-absent模式下分别比最佳基线提高了1.3倍和1.9倍的Mean Reciprocal Rank，并显著减少了78-89%的误报。

Conclusion: VFArch=e在实际应用中表现出色，能够高效定位漏洞函数，提升软件组合分析的准确性。

Abstract: Software Composition Analysis (SCA) has become pivotal in addressing
vulnerabilities inherent in software project dependencies. In particular,
reachability analysis is increasingly used in Open-Source Software (OSS)
projects to identify reachable vulnerabilities (e.g., CVEs) through call
graphs, enabling a focus on exploitable risks. Performing reachability analysis
typically requires the vulnerable function (VF) to track the call chains from
downstream applications. However, such crucial information is usually
unavailable in modern vulnerability databases like NVD. While directly
extracting VF from modified functions in vulnerability patches is intuitive,
patches are not always available. Moreover, our preliminary study shows that
over 26% of VF do not exist in the modified functions. Meanwhile, simply
ignoring patches to search vulnerable functions suffers from overwhelming
noises and lexical gaps between descriptions and source code. Given that almost
half of the vulnerabilities are equipped with patches, a holistic solution that
handles both scenarios with and without patches is required. To meet real-world
needs and automatically localize VF, we present VFArch\=e, a dual-mode approach
designed for disclosed vulnerabilities, applicable in scenarios with or without
available patch links. The experimental results of VFArch\=e on our constructed
benchmark dataset demonstrate significant efficacy regarding three metrics,
achieving 1.3x and 1.9x Mean Reciprocal Rank over the best baselines for
Patch-present and Patch-absent modes, respectively. Moreover, VFArch\=e has
proven its applicability in real-world scenarios by successfully locating VF
for 43 out of 50 latest vulnerabilities with reasonable efforts and
significantly reducing 78-89% false positives of SCA tools.

</details>


### [18] [Call Me Maybe: Enhancing JavaScript Call Graph Construction using Graph Neural Networks](https://arxiv.org/abs/2506.18191)
*Masudul Hasan Masud Bhuiyan,Gianluca De Stefano,Giancarlo Pellegrino,Cristian-Alexandru Staicu*

Main category: cs.SE

TL;DR: GRAPHIA利用图神经网络改进JavaScript调用图构建，通过链接预测提高准确性，减少人工分析需求。


<details>
  <summary>Details</summary>
Motivation: 现有JavaScript调用图构建工具既不健全也不完整，存在误报和遗漏。

Method: 将问题建模为全程序图上的链接预测，结合语法和语义边，利用图神经网络学习非局部关系。

Result: 在50个流行JavaScript库上评估，GRAPHIA在42%的未解决调用中将正确目标排名第一，72%排名前五。

Conclusion: 学习型方法能提高调用图构建的召回率，GRAPHIA是首个将GNN应用于多文件程序图的工作。

Abstract: Static analysis plays a key role in finding bugs, including security issues.
A critical step in static analysis is building accurate call graphs that model
function calls in a program. However, due to hard-to-analyze language features,
existing call graph construction algorithms for JavaScript are neither sound
nor complete. Prior work shows that even advanced solutions produce false edges
and miss valid ones. In this work, we assist these tools by identifying missed
call edges. Our main idea is to frame the problem as link prediction on full
program graphs, using a rich representation with multiple edge types. Our
approach, GRAPHIA, leverages recent advances in graph neural networks to model
non-local relationships between code elements. Concretely, we propose
representing JavaScript programs using a combination of syntactic- and
semantic-based edges. GRAPHIA can learn from imperfect labels, including static
call edges from existing tools and dynamic edges from tests, either from the
same or different projects. Because call graphs are sparse, standard machine
learning metrics like ROC are not suitable. Instead, we evaluate GRAPHIA by
ranking function definitions for each unresolved call site. We conduct a
large-scale evaluation on 50 popular JavaScript libraries with 163K call edges
(150K static and 13K dynamic). GRAPHIA builds program graphs with 6.6M
structural and 386K semantic edges. It ranks the correct target as the top
candidate in over 42% of unresolved cases and within the top 5 in 72% of cases,
reducing the manual effort needed for analysis. Our results show that
learning-based methods can improve the recall of JavaScript call graph
construction. To our knowledge, this is the first work to apply GNN-based link
prediction to full multi-file program graphs for interprocedural analysis.

</details>


### [19] [Managing Technical Debt in a Multidisciplinary Data Intensive Software Team: an Observational Case Study](https://arxiv.org/abs/2506.18219)
*Ulrike M. Graetsch,Rashina Hoda,Hourieh Khalazjadeh,Mojtaba Shahin,John Grundy*

Main category: cs.SE

TL;DR: 研究探讨了多学科数据密集型团队如何管理技术债务，识别了技术债务类型并提出了管理实践。


<details>
  <summary>Details</summary>
Motivation: 随着数据密集型解决方案投资的增加，技术债务问题日益突出，但多学科团队如何管理技术债务的研究有限。

Method: 采用探索性观察案例研究和社会技术扎根理论（STGT）进行数据分析。

Result: 识别了技术数据组件债务和管道债务，并描述了团队如何评估、处理技术债务以适应开发周期限制。

Conclusion: 研究结果与现有技术债务分类一致，并强调了多学科团队需要新的实施模式和工具支持。

Abstract: Context: There is an increase in the investment and development of
data-intensive (DI) solutions, systems that manage large amounts of data.
Without careful management, this growing investment will also grow associated
technical debt (TD). Delivery of DI solutions requires a multidisciplinary
skill set, but there is limited knowledge about how multidisciplinary teams
develop DI systems and manage TD.
  Objective: This research contributes empirical, practice based insights about
multidisciplinary DI team TD management practices.
  Method: This research was conducted as an exploratory observation case study.
We used socio-technical grounded theory (STGT) for data analysis to develop
concepts and categories that articulate TD and TDs debt management practices.
  Results: We identify TD that the DI team deals with, in particular technical
data components debt and pipeline debt. We explain how the team manages the TD,
assesses TD, what TD treatments they consider and how they implement TD
treatments to fit sprint capacity constraints.
  Conclusion: We align our findings to existing TD and TDM taxonomies, discuss
their implications and highlight the need for new implementation patterns and
tool support for multidisciplinary DI teams.

</details>


### [20] [Tu(r)ning AI Green: Exploring Energy Efficiency Cascading with Orthogonal Optimizations](https://arxiv.org/abs/2506.18289)
*Saurabhsingh Rajput,Mootez Saad,Tushar Sharma*

Main category: cs.SE

TL;DR: 论文提出将能源效率作为AI流程设计的首要考虑因素，通过五个阶段的策略选择实现显著节能，实验验证节能达94.6%且性能损失极小。


<details>
  <summary>Details</summary>
Motivation: AI的快速增长带来计算和能源挑战，现有优化方法多为被动且孤立，缺乏对组合效应的理解。

Method: 将能源效率作为核心设计原则，在数据、模型、训练、系统和推理五个阶段进行策略性选择。

Result: 实验显示正交组合可节能高达94.6%，同时保留95.95%的原始F1分数。

Conclusion: 该方法为可持续AI提供了平衡效率、性能和环保的行动框架。

Abstract: AI's exponential growth intensifies computational demands and energy
challenges. While practitioners employ various optimization techniques, that we
refer as "knobs" in this paper, to tune model efficiency, these are typically
afterthoughts and reactive ad-hoc changes applied in isolation without
understanding their combinatorial effects on energy efficiency. This paper
emphasizes on treating energy efficiency as the first-class citizen and as a
fundamental design consideration for a compute-intensive pipeline. We show that
strategic selection across five AI pipeline phases (data, model, training,
system, inference) creates cascading efficiency. Experimental validation shows
orthogonal combinations reduce energy consumption by up to $94.6$% while
preserving $95.95$% of the original F1 score of non-optimized pipelines. This
curated approach provides actionable frameworks for informed sustainable AI
that balance efficiency, performance, and environmental responsibility.

</details>


### [21] [Use Property-Based Testing to Bridge LLM Code Generation and Validation](https://arxiv.org/abs/2506.18315)
*Lehan He,Zeren Chen,Zhe Zhang,Jing Shao,Xiang Gao,Lu Sheng*

Main category: cs.SE

TL;DR: 论文提出Property-Generated Solver框架，利用基于属性的测试（PBT）验证程序的高层属性，而非具体输入输出，以提升大语言模型（LLMs）生成代码的功能正确性。


<details>
  <summary>Details</summary>
Motivation: 传统测试驱动开发（TDD）在LLMs代码生成中存在高质量测试用例稀缺和自动化测试生成缺陷的问题，导致验证效果受限。

Method: 框架包含两个协作的LLM代理：Generator负责代码生成与迭代优化，Tester管理PBT生命周期并提供语义丰富的反馈。

Result: 实验表明，该框架在多个代码生成基准测试中，pass@1指标相对TDD方法提升了23.1%至37.3%。

Conclusion: Property-Generated Solver通过PBT为核心验证机制，显著提升了LLMs生成代码的正确性和泛化能力。

Abstract: Large Language Models (LLMs) excel at code generation, but ensuring their
outputs to be functionally correct, especially in complex programming tasks, is
a persistent challenge. While traditional Test-Driven Development (TDD) offers
a path for code refinement, its efficacy with LLMs is often undermined by the
scarcity of high-quality test cases or the pitfalls of automated test
generation, including biased tests or inaccurate output predictions that can
misdirect the correction process. This paper introduces Property-Generated
Solver, a novel framework that leverages Property-Based Testing (PBT) to
validate high-level program properties or invariants, instead of relying on
specific input-output examples. These properties are often simpler to define
and verify than directly predicting exhaustive test oracles, breaking the
"cycle of self-deception" where tests might share flaws with the code they are
meant to validate. Property-Generated Solver employs two collaborative
LLM-based agents: a Generator dedicated to code generation and iterative
refinement, and a Tester that manages the PBT life-cycle and formulate
semantically rich feedback from property violations. The resulting
comprehensive and actionable feedback then guides the Generator in its
refinement efforts. By establishing PBT as the core validation engine within
this iterative, closed-loop paradigm, Property-Generated Solver provides a
robust mechanism for steering LLMs towards more correct and generalizable code.
Extensive experimental results on multiple code generation benchmarks
demonstrate that Property-Generated Solver achieves substantial pass@1
improvements, ranging from 23.1% to 37.3% relative gains over established TDD
methods.

</details>


### [22] [Predictive Analytics for Collaborators Answers, Code Quality, and Dropout on Stack Overflow](https://arxiv.org/abs/2506.18329)
*Elijah Zolduoarrati,Sherlock A. Licorish,Nigel Stanger*

Main category: cs.SE

TL;DR: 该研究评估了21种算法在三个任务中的表现，发现不同任务下最优模型不同，并验证了CodeBERT在用户流失分类中的有效性。


<details>
  <summary>Details</summary>
Motivation: 以往研究使用有限模型或随意选择方法，可能导致遗漏未测试算法，因此需要更全面的基准测试。

Method: 采用标准化、对数变换等方法，结合贝叶斯超参数优化和遗传算法，并微调CodeBERT进行用户流失分类。

Result: Bagging模型在预测用户回答数时表现最佳（R2=0.821），SGD回归器在代码质量预测中表现优异，XGBoost在预测用户流失时F1得分最高（0.825）。CodeBERT的F1得分为0.809。

Conclusion: 研究为特定任务提供了最优模型选择建议，并帮助实践者缩小超参数搜索范围。

Abstract: Previous studies that used data from Stack Overflow to develop predictive
models often employed limited benchmarks of 3-5 models or adopted arbitrary
selection methods. Despite being insightful, their limited scope suggests the
need to benchmark more models to avoid overlooking untested algorithms. Our
study evaluates 21 algorithms across three tasks: predicting the number of
question a user is likely to answer, their code quality violations, and their
dropout status. We employed normalisation, standardisation, as well as
logarithmic and power transformations paired with Bayesian hyperparameter
optimisation and genetic algorithms. CodeBERT, a pre-trained language model for
both natural and programming languages, was fine-tuned to classify user dropout
given their posts (questions and answers) and code snippets. We found Bagging
ensemble models combined with standardisation achieved the highest R2 value
(0.821) in predicting user answers. The Stochastic Gradient Descent regressor,
followed by Bagging and Epsilon Support Vector Machine models, consistently
demonstrated superior performance to other benchmarked algorithms in predicting
user code quality across multiple quality dimensions and languages. Extreme
Gradient Boosting paired with log-transformation exhibited the highest F1-score
(0.825) in predicting user dropout. CodeBERT was able to classify user dropout
with a final F1-score of 0.809, validating the performance of Extreme Gradient
Boosting that was solely based on numerical data. Overall, our benchmarking of
21 algorithms provides multiple insights. Researchers can leverage findings
regarding the most suitable models for specific target variables, and
practitioners can utilise the identified optimal hyperparameters to reduce the
initial search space during their own hyperparameter tuning processes.

</details>


### [23] [Recipe for Discovery: A Framework for Systematic Open Source Project Identification](https://arxiv.org/abs/2506.18359)
*Juanita Gomez,Emily Lovell,Stephanie Lieggi,Alvaro A. Cardenas,James Davis*

Main category: cs.SE

TL;DR: 该论文提出了一种框架，用于发现、分类和分析分散在机构系统中的开源软件项目，并以加州大学系统为例进行了验证。


<details>
  <summary>Details</summary>
Motivation: 开源软件开发在高校和研究机构中往往分散且难以追踪，导致这些高影响力工具缺乏可见性和认可。

Method: 利用GitHub的REST API构建管道发现相关仓库并提取元数据，采用传统机器学习和大语言模型（LLMs）进行分类。

Result: 框架在规模上有效，发现了超过52,000个仓库，并以高准确性预测了机构关联性。

Conclusion: 该框架为学术开源项目的发现和分析提供了有效工具，提升了机构对其开源贡献的认知。

Abstract: Open source software development, particularly within institutions such as
universities and research laboratories, is often decentralized and difficult to
track. Despite producing highly impactful tools in science, these efforts often
go unrecognized due to a lack of visibility and institutional awareness. This
paper addresses the challenge of discovering, classifying, and analyzing open
source software projects developed across distributed institutional systems. We
present a framework for systematically identifying institutional affiliated
repositories, using the University of California (UC) system as a case study.
  Using GitHub's REST API, we build a pipeline to discover relevant
repositories and extract meaningful metadata. We then propose and evaluate
multiple classification strategies, including both traditional machine learning
models and large language models (LLMs), to distinguish affiliated projects
from unrelated repositories and generate accurate insights into the academic
open source landscape. Our results show that the framework is effective at
scale, discovering over 52,000 repositories and predicting institutional
affiliation with high accuracy.

</details>


### [24] [Tracing Errors, Constructing Fixes: Repository-Level Memory Error Repair via Typestate-Guided Context Retrieval](https://arxiv.org/abs/2506.18394)
*Xiao Cheng,Zhihao Guo,Huan Huo,Yulei Sui*

Main category: cs.SE

TL;DR: LTFix利用大型语言模型（LLMs）和有限类型状态自动机，提出了一种自动化修复C语言内存错误的新方法，解决了跨函数和文件的复杂错误修复问题。


<details>
  <summary>Details</summary>
Motivation: C语言的手动内存管理导致内存错误频发，传统自动修复方法依赖人工设计，效率低且受限。深度学习虽能自动提取修复模式，但需大量数据且缺乏可解释性。

Method: LTFix结合LLMs和有限类型状态自动机，通过跟踪错误传播路径和上下文痕迹，捕获内存状态和执行历史的时空维度，为LLM提供语义丰富的信息。

Result: 该方法有效解决了LLM在跨函数和文件分析中的上下文窗口限制问题，提升了复杂内存错误的修复能力。

Conclusion: LTFix为自动化内存错误修复提供了高效且可解释的新思路，尤其适用于复杂场景。

Abstract: Memory-related errors in C programming continue to pose significant
challenges in software development, primarily due to the complexities of manual
memory management inherent in the language. These errors frequently serve as
vectors for severe vulnerabilities, while their repair requires extensive
knowledge of program logic and C's memory model. Automated Program Repair (APR)
has emerged as a critical research area to address these challenges.
Traditional APR approaches rely on expert-designed strategies and predefined
templates, which are labor-intensive and constrained by the effectiveness of
manual specifications. Deep learning techniques offer a promising alternative
by automatically extracting repair patterns, but they require substantial
training datasets and often lack interpretability.
  This paper introduces LTFix, a novel approach that harnesses the potential of
Large Language Models (LLMs) for automated memory error repair, especially for
complex repository-level errors that span multiple functions and files. We
address two fundamental challenges in LLM-based memory error repair: a limited
understanding of interprocedural memory management patterns and context window
limitations for repository-wide analysis. Our approach utilizes a finite
typestate automaton to guide the tracking of error-propagation paths and
context trace, capturing both spatial (memory states) and temporal (execution
history) dimensions of error behavior. This typestate-guided context retrieval
strategy provides the LLM with concise yet semantically rich information
relevant to erroneous memory management, effectively addressing the token
limitation of LLMs.

</details>


### [25] [Your Token Becomes Worthless: Unveiling Rug Pull Schemes in Crypto Token via Code-and-Transaction Fusion Analysis](https://arxiv.org/abs/2506.18398)
*Hao Wu,Haijun Wang,Shangwang Li,Yin Wu,Ming Fan,Wuxia Jin,Yitao Zhao,Ting Liu*

Main category: cs.SE

TL;DR: RPhunter是一种结合代码和交易行为的新型Rug Pull检测技术，通过构建语义风险代码图和令牌流行为图，利用图神经网络和注意力融合模型，显著提高了检测精度和召回率。


<details>
  <summary>Details</summary>
Motivation: Rug Pull诈骗在加密货币领域造成重大损失，现有方法仅关注代码风险或交易行为，无法有效检测复杂的恶意行为。

Method: RPhunter通过声明性规则和流分析提取代码风险信息，构建语义风险代码图（SRCG）；同时将动态交易活动建模为令牌流行为图（TFBG），并利用图神经网络和注意力融合模型整合两者特征。

Result: 在645个Rug Pull事件的数据集上，RPhunter的精确率为95.3%，召回率为93.8%，F1分数为94.5%；在真实场景中识别了4801个Rug Pull代币，精确率为91%。

Conclusion: RPhunter通过整合代码和交易行为特征，显著提升了Rug Pull检测效果，优于现有方法。

Abstract: Rug pull scams have emerged as a persistent threat to cryptocurrency, causing
significant financial losses. A typical scenario involves scammers deploying
honeypot contracts to attract investments, restricting token sales, and
draining the funds, which leaves investors with worthless tokens. Current
methods either rely on predefined patterns to detect code risks or utilize
statistical transaction data to train detection models. However, real-world Rug
Pull schemes often involve a complex interplay between malicious code and
suspicious transaction behaviors. These methods, which solely focus on one
aspect, fall short in detecting such schemes effectively.
  In this paper, we propose RPhunter, a novel technique that integrates code
and transaction for Rug Pull detection. First, RPhunter establishes declarative
rules and performs flow analysis to extract code risk information, further
constructing a semantic risk code graph (SRCG). Meanwhile, to leverage
transaction information, RPhunter formulates dynamic token transaction
activities as a token flow behavior graph (TFBG) in which nodes and edges are
characterized from network structure and market manipulation perspectives.
Finally, RPhunter employs graph neural networks to extract complementary
features from SRCG and TFBG, integrating them through an attention fusion model
to enhance the detection of Rug Pull. We manually analyzed 645 Rug Pull
incidents from code and transaction aspects and constructed a ground-truth
dataset. We evaluated RPhunter on our dataset, achieving a precision of 95.3%,
a recall of 93.8% and an F1 score of 94.5%, which highlights superior
performance compared to existing state-of-the-art methods. Furthermore, when
applied to the real-world scenarios, RPhunter has identified 4801 Rug Pull
tokens, achieving a precision of 91%.

</details>


### [26] [The Debugging Decay Index: Rethinking Debugging Strategies for Code LLMs](https://arxiv.org/abs/2506.18403)
*Muntasir Adnan,Carlos C. N. Kuhn*

Main category: cs.SE

TL;DR: 论文提出Debugging Decay Index (DDI)，量化AI调试能力衰减，并通过适时干预提升调试效果。


<details>
  <summary>Details</summary>
Motivation: 当前AI调试能力在2-3次尝试后显著下降，亟需量化框架优化迭代调试策略。

Method: 引入DDI数学框架，预测调试失效点，并提出适时重启策略。

Result: DDI揭示AI调试的根本限制，提供优化代码生成策略的量化工具。

Conclusion: DDI为AI调试提供首个定量优化框架，适时干预可显著提升调试效果。

Abstract: The effectiveness of AI debugging follows a predictable exponential decay
pattern; most models lose 60-80% of their debugging capability within just 2-3
attempts, despite iterative debugging being a critical capability for practical
code generation systems. We introduce the Debugging Decay Index (DDI), a
mathematical framework that quantifies when debugging becomes ineffective and
predicts intervention points. Our strategic fresh start approach shifts from
exploitation to exploration at strategic points in the debugging process,
demonstrating that well-timed interventions can rescue the effectiveness of
debugging. DDI reveals a fundamental limitation in current AI debugging and
provides the first quantitative framework for optimising iterative code
generation strategies.

</details>


### [27] [ModeliHub: A Web-based, Federated Analytics Platform for Modelica-centric, Model-based Systems Engineering](https://arxiv.org/abs/2506.18790)
*Mohamad Omar Nachawati*

Main category: cs.SE

TL;DR: ModeliHub是一个基于Web的联邦分析平台，专为基于Modelica的模型系统工程设计，提供统一的系统模型和实时仿真环境。


<details>
  <summary>Details</summary>
Motivation: 为系统工程提供一种基于Modelica的统一模型和实时仿真工具，以支持迭代开发周期中的数字孪生技术。

Method: 采用Modelica为中心的联邦架构，结合可扩展的Modelica编译器前端（基于Isomorphic TypeScript），实现跨浏览器、桌面和服务器的无缝运行。

Result: 实现了统一的系统模型和实时交互仿真环境，支持跨工程领域的无缝集成与分析。

Conclusion: ModeliHub通过其创新架构在严谨性和灵活性之间取得平衡，为系统工程提供了高效的工具支持。

Abstract: This paper introduces ModeliHub, a Web-based, federated analytics platform
designed specifically for model-based systems engineering with Modelica.
ModeliHub's key innovation lies in its Modelica-centric, hub-and-spoke
federation architecture that provides systems engineers with a Modelica-based,
unified system model of repositories containing heterogeneous engineering
artifacts. From this unified system model, ModeliHub's Virtual Twin engine
provides a real-time, interactive simulation environment for deploying Modelica
simulation models that represent digital twins of the virtual prototype of the
system under development at a particular iteration of the iterative systems
engineering life cycle. The implementation of ModeliHub is centered around its
extensible, Modelica compiler frontend developed in Isomorphic TypeScript that
can run seamlessly across browser, desktop and server environments. This
architecture aims to strike a balance between rigor and agility, enabling
seamless integration and analysis across various engineering domains.

</details>


### [28] [Context-Aware CodeLLM Eviction for AI-assisted Coding](https://arxiv.org/abs/2506.18796)
*Kishanthan Thangarajah,Boyuan Chen,Shi Chang,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: CACE是一种新型上下文感知模型驱逐策略，优化自托管CodeLLM在资源受限环境中的服务效率。


<details>
  <summary>Details</summary>
Motivation: 解决自托管CodeLLM在模型管理和服务效率上的挑战，如隐私、延迟和模型定制需求。

Method: CACE利用多种上下文感知因素（如模型加载时间、任务延迟敏感性、预期输出长度等）进行模型驱逐决策。

Result: CACE显著降低了首次令牌时间（TTFT）和端到端延迟，同时减少了模型驱逐次数。

Conclusion: CACE为现实软件工程环境提供了可扩展、低延迟的AI编码助手部署策略。

Abstract: AI-assisted coding tools powered by Code Large Language Models (CodeLLMs) are
increasingly integrated into modern software development workflows. To address
concerns around privacy, latency, and model customization, many enterprises opt
to self-host these models. However, the diversity and growing number of
CodeLLMs, coupled with limited accelerator memory, introduce practical
challenges in model management and serving efficiency. This paper presents
CACE, a novel context-aware model eviction strategy designed specifically to
optimize self-hosted CodeLLM serving under resource constraints. Unlike
traditional eviction strategies based solely on recency (e.g., Least Recently
Used), CACE leverages multiple context-aware factors, including model load
time, task-specific latency sensitivity, expected output length, and recent
usage and future demand tracked through a sliding window. We evaluate CACE
using realistic workloads that include both latency-sensitive code completion
and throughput-intensive code reasoning tasks. Our experiments show that CACE
reduces Time-to-First-Token (TTFT) and end-to-end (E2E) latency, while
significantly lowering the number of model evictions compared to
state-of-the-art systems. Ablation studies further demonstrate the importance
of multi-factor eviction in balancing responsiveness and resource efficiency.
This work contributes practical strategies for deploying scalable, low-latency
AI coding assistants in real-world software engineering environments.

</details>


### [29] [Understanding Software Engineering Agents: A Study of Thought-Action-Result Trajectories](https://arxiv.org/abs/2506.18824)
*Islem Bouzenia,Michael Pradel*

Main category: cs.SE

TL;DR: 该论文对三种基于大语言模型（LLM）的代理（RepairAgent、AutoCodeRover、OpenHands）的决策过程进行了大规模实证研究，揭示了其行为模式和失败原因，并提出了改进建议。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM代理被广泛用于自动化软件工程任务，但其内部决策过程尚未被充分研究，限制了对其操作动态和失败模式的理解。

Method: 通过统一交互日志格式，分析了120条轨迹和2822次LLM交互，结合定量和定性方法评估结构特性、行为模式和推理连贯性。

Result: 研究发现成功与失败执行的行为特征差异，提出了改进代理设计的实用建议，如提示策略和失败诊断。

Conclusion: 研究为透明和稳健的自主软件工程代理提供了数据集和框架，支持未来研究。

Abstract: Large Language Model (LLM)-based agents are increasingly employed to automate
complex software engineering tasks such as program repair and issue resolution.
These agents operate by autonomously generating natural language thoughts,
invoking external tools, and iteratively refining their solutions. Despite
their widespread adoption, the internal decision-making processes of these
agents remain largely unexplored, limiting our understanding of their
operational dynamics and failure modes. In this paper, we present a large-scale
empirical study of the thought-action-result trajectories of three
state-of-the-art LLM-based agents: \textsc{RepairAgent},
\textsc{AutoCodeRover}, and \textsc{OpenHands}. We unify their interaction logs
into a common format, capturing 120 trajectories and 2822 LLM interactions
focused on program repair and issue resolution. Our study combines quantitative
analyses of structural properties, action patterns, and token usage with
qualitative assessments of reasoning coherence and feedback integration. We
identify key trajectory characteristics such as iteration counts and token
consumption, recurring action sequences, and the semantic coherence linking
thoughts, actions, and their results. Our findings reveal behavioral motifs and
anti-patterns that distinguish successful from failed executions, providing
actionable insights for improving agent design, including prompting strategies,
failure diagnosis, and anti-pattern detection. We release our dataset and
annotation framework to support further research on transparent and robust
autonomous software engineering agents.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [30] [A Geometric Square-Based Approach to RSA Integer Factorization](https://arxiv.org/abs/2506.17233)
*Akihisa Yorozu*

Main category: cs.CR

TL;DR: 提出了一种基于几何解释和平方差的RSA因数分解新方法，适用于小半素数，但对大数如RSA-100尚不实用。


<details>
  <summary>Details</summary>
Motivation: 通过几何视角和平方差重新表述RSA因数分解问题，以解决素数间距较近的情况。

Method: 利用完美平方之间的距离，建立递推关系以实现快速收敛。

Result: 对小半素数有效，但对大数如RSA-100尚未实现实用化。

Conclusion: 该方法在小规模问题中展现潜力，但需进一步改进以应对大规模挑战。

Abstract: We present a new approach to RSA factorization inspired by geometric
interpretations and square differences. This method reformulates the problem in
terms of the distance between perfect squares and provides a recurrence
relation that allows rapid convergence when the RSA modulus has closely spaced
prime factors. Although this method is efficient for small semiprimes, it does
not yet succeed in factoring large challenges like RSA-100 in practical time,
highlighting both its potential and current limitations.

</details>


### [31] [Design, Implementation, and Analysis of Fair Faucets for Blockchain Ecosystems](https://arxiv.org/abs/2506.17236)
*Serdar Metin*

Main category: cs.CR

TL;DR: 论文探讨了非商业区块链网络中公平分配共享资源的问题，提出了6种基于Max-min公平性的算法，解决了传统faucet机制的DoS攻击和公平性问题。


<details>
  <summary>Details</summary>
Motivation: 非商业区块链网络中缺乏货币化机制，传统faucet机制易受DoS攻击且无法保证公平性，因此需要新的解决方案。

Method: 提出了6种基于Max-min公平性的算法，这些算法高效、抗DoS攻击，并支持不同的用户权重策略。

Result: 贡献的算法在区块链计算经济学上成本低，且能有效抵抗DoS攻击，同时实现公平分配。

Conclusion: 研究为非商业区块链网络提供了一种公平、高效的资源分配机制，解决了现有faucet机制的局限性。

Abstract: The present dissertation addresses the problem of fairly distributing shared
resources in non-commercial blockchain networks. Blockchains are distributed
systems that order and timestamp records of a given network of users, in a
public, cryptographically secure, and consensual way. The records, which may in
kind be events, transaction orders, sets of rules for structured transactions
etc. are placed within well-defined datastructures called blocks, and they are
linked to each other by the virtue of cryptographic pointers, in a total
ordering which represents their temporal relations of succession. The ability
to operate on the blockchain, and/or to contribute a record to the content of a
block are shared resources of the blockchain systems. In commercial networks,
these resources are exchanged in return for fiat money, and consequently,
fairness is not a relevant problem in terms of computer engineering. In
non-commercial networks, however, monetary solutions are not available, by
definition. The present non-commercial blockchain networks employ trivial
distribution mechanisms called faucets, which offer fixed amounts of free
tokens (called cryptocurrencies) specific to the given network. This mechanism,
although simple and efficient, is prone to denial of service (DoS) attacks and
cannot address the fairness problem. In the present dissertation, the faucet
mechanism is adapted for fair distribution, in line with Max-min Fairness
scheme. In total, we contributed 6 distinct Max-min Fair algorithms as
efficient blockchain faucets. The algorithms we contribute are resistant to DoS
attacks, low-cost in terms of blockchain computation economics, and they also
allow for different user weighting policies.

</details>


### [32] [Detecting and Mitigating SQL Injection Vulnerabilities in Web Applications](https://arxiv.org/abs/2506.17245)
*Sagar Neupane*

Main category: cs.CR

TL;DR: 本文提出了一种针对PHP-MySQL应用的SQL注入漏洞检测与缓解方法，结合工具和案例研究验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: SQL注入仍是Web应用的关键漏洞，现有防护技术难以应对不断演变的攻击策略。

Method: 使用OWASP ZAP、sqlmap和Nmap等工具，系统化评估和修复SQL注入漏洞。

Result: 验证了输入过滤和预编译语句的有效性，并强调持续安全评估的必要性。

Conclusion: 该研究为SQL注入的检测与预防提供了实用策略，并通过案例研究支持其有效性。

Abstract: SQL injection (SQLi) remains a critical vulnerability in web applications,
enabling attackers to manipulate databases through malicious inputs. Despite
advancements in mitigation techniques, the evolving complexity of web
applications and attack strategies continues to pose significant risks. This
paper presents a comprehensive penetration testing methodology to identify,
exploit, and mitigate SQLi vulnerabilities in a PHP-MySQL-based web
application. Utilizing tools such as OWASP ZAP, sqlmap, and Nmap, the study
demonstrates a systematic approach to vulnerability assessment and remediation.
The findings underscore the efficacy of input sanitization and prepared
statements in mitigating SQLi risks, while highlighting the need for ongoing
security assessments to address emerging threats. The study contributes to the
field by providing practical insights into effective detection and prevention
strategies, supported by a real-world case study.

</details>


### [33] [Securing Generative AI Agentic Workflows: Risks, Mitigation, and a Proposed Firewall Architecture](https://arxiv.org/abs/2506.17266)
*Sunil Kumar Jang Bahadur,Gopala Dhar*

Main category: cs.CR

TL;DR: 论文探讨了生成式人工智能（GenAI）在自主工作流程中的安全挑战，提出了多代理系统中的风险及解决方案，并设计了一种“GenAI安全防火墙”架构。


<details>
  <summary>Details</summary>
Motivation: 生成式人工智能的快速发展带来了新的安全挑战，尤其是在多代理系统中，需要解决数据隐私、模型操纵等问题以确保技术的安全部署。

Method: 论文分析了GenAI工作流程中的安全漏洞，提出了包括数据加密、访问控制、提示工程等缓解策略，并设计了一种集成多种安全服务的“GenAI安全防火墙”架构。

Result: 研究提出了一种全面的安全防护方案，利用GenAI自身能力增强防御，为系统提供高效保护。

Conclusion: 解决GenAI的安全问题是其负责任和安全部署的关键，提出的防火墙架构为未来研究提供了重要方向。

Abstract: Generative Artificial Intelligence (GenAI) presents significant advancements
but also introduces novel security challenges, particularly within agentic
workflows where AI agents operate autonomously. These risks escalate in
multi-agent systems due to increased interaction complexity. This paper
outlines critical security vulnerabilities inherent in GenAI agentic workflows,
including data privacy breaches, model manipulation, and issues related to
agent autonomy and system integration. It discusses key mitigation strategies
such as data encryption, access control, prompt engineering, model monitoring,
agent sandboxing, and security audits. Furthermore, it details a proposed
"GenAI Security Firewall" architecture designed to provide comprehensive,
adaptable, and efficient protection for these systems by integrating various
security services and leveraging GenAI itself for enhanced defense. Addressing
these security concerns is paramount for the responsible and safe deployment of
this transformative technology.

</details>


### [34] [Digital Privacy Everywhere](https://arxiv.org/abs/2506.17269)
*Paritosh Ranjan,Surajit Majumder,Prodip Roy*

Main category: cs.CR

TL;DR: DPE是一个主动执行隐私政策的系统，通过集中管理、现场验证和设备模块协作，实时确保敏感场所的隐私合规。


<details>
  <summary>Details</summary>
Motivation: 数字设备隐私侵犯问题日益严重，现有被动解决方案效果不佳，需要主动、可扩展的系统。

Method: DPE采用集中管理控制台、现场验证单元、移动设备执行模块和外部地理所有权服务，实时配置和执行隐私设置。

Result: 系统能实时禁用摄像头、麦克风等功能，适用于多种场所，确保隐私合规且用户体验无缝。

Conclusion: DPE为敏感环境提供了一种高效、可扩展的隐私保护解决方案。

Abstract: The increasing proliferation of digital and mobile devices equipped with
cameras, microphones, GPS, and other privacy invasive components has raised
significant concerns for businesses operating in sensitive or policy restricted
environments. Current solutions rely on passive enforcement, such as signage or
verbal instructions, which are largely ineffective. This paper presents Digital
Privacy Everywhere (DPE), a comprehensive and scalable system designed to
actively enforce custom privacy policies for digital devices within predefined
physical boundaries. The DPE architecture includes a centralized management
console, field verification units (FVUs), enforcement modules for mobile
devices (EMMDs), and an External Geo Ownership Service (EGOS). These components
collaboratively detect, configure, and enforce privacy settings such as
disabling cameras, microphones, or radios across various premises like
theaters, hospitals, financial institutions, and educational facilities. The
system ensures privacy compliance in real time while maintaining a seamless
user experience and operational scalability across geographies.

</details>


### [35] [Step-by-Step Reasoning Attack: Revealing 'Erased' Knowledge in Large Language Models](https://arxiv.org/abs/2506.17279)
*Yash Sinha,Manit Baser,Murari Mandal,Dinil Mon Divakaran,Mohan Kankanhalli*

Main category: cs.CR

TL;DR: 论文提出了一种基于逐步推理的黑盒攻击方法Sleek，用于暴露现有大语言模型（LLM）知识遗忘技术的缺陷，揭示隐藏信息仍可通过特定提示恢复。


<details>
  <summary>Details</summary>
Motivation: 确保LLM符合数据与AI法规、保护用户隐私、减少偏见和错误信息，但现有遗忘技术未能彻底删除知识，反而使其隐藏但仍可恢复。

Method: 提出Sleek攻击框架，包含三步：1) 基于逐步推理的对抗提示生成；2) 成功恢复被删除内容并暴露知识不公平抑制的机制；3) 对提示分类以识别最有效的攻击类型。

Result: 实验表明，现有遗忘技术不可靠，62.5%的对抗提示成功恢复被遗忘的《哈利波特》知识，50%暴露了保留知识的不公平抑制。

Conclusion: 研究揭示了信息泄露的持续风险，强调需要更鲁棒的遗忘策略以实现彻底知识删除。

Abstract: Knowledge erasure in large language models (LLMs) is important for ensuring
compliance with data and AI regulations, safeguarding user privacy, mitigating
bias, and misinformation. Existing unlearning methods aim to make the process
of knowledge erasure more efficient and effective by removing specific
knowledge while preserving overall model performance, especially for retained
information. However, it has been observed that the unlearning techniques tend
to suppress and leave the knowledge beneath the surface, thus making it
retrievable with the right prompts. In this work, we demonstrate that
\textit{step-by-step reasoning} can serve as a backdoor to recover this hidden
information. We introduce a step-by-step reasoning-based black-box attack,
Sleek, that systematically exposes unlearning failures. We employ a structured
attack framework with three core components: (1) an adversarial prompt
generation strategy leveraging step-by-step reasoning built from LLM-generated
queries, (2) an attack mechanism that successfully recalls erased content, and
exposes unfair suppression of knowledge intended for retention and (3) a
categorization of prompts as direct, indirect, and implied, to identify which
query types most effectively exploit unlearning weaknesses. Through extensive
evaluations on four state-of-the-art unlearning techniques and two widely used
LLMs, we show that existing approaches fail to ensure reliable knowledge
removal. Of the generated adversarial prompts, 62.5% successfully retrieved
forgotten Harry Potter facts from WHP-unlearned Llama, while 50% exposed unfair
suppression of retained knowledge. Our work highlights the persistent risks of
information leakage, emphasizing the need for more robust unlearning strategies
for erasure.

</details>


### [36] [Theoretically Unmasking Inference Attacks Against LDP-Protected Clients in Federated Vision Models](https://arxiv.org/abs/2506.17292)
*Quan Nguyen,Minh N. Vu,Truc Nguyen,My T. Thai*

Main category: cs.CR

TL;DR: 联邦学习通过协调服务器实现客户端协作学习，避免直接数据共享以保护隐私。然而，成员推理攻击（MIAs）的高成功率挑战了这一观点。尽管本地差分隐私（LDP）被视为隐私保护的黄金标准，但多数MIAs研究忽略了LDP或未提供理论保证。本文推导了针对LDP保护数据的低多项式时间MIAs的理论下限，表明隐私风险仍存在。实验证实，减轻攻击所需的噪声会显著降低模型效用。


<details>
  <summary>Details</summary>
Motivation: 研究旨在填补关于成员推理攻击（MIAs）在本地差分隐私（LDP）保护下的理论空白，揭示即使数据受LDP保护，隐私风险依然存在。

Method: 推导了针对全连接层或自注意力层的低多项式时间MIAs的理论下限，并通过联邦视觉模型进行实际评估。

Result: 理论分析表明，隐私风险取决于隐私预算；实验证实攻击噪声会显著降低模型效用。

Conclusion: 即使采用LDP保护，联邦学习仍面临隐私风险，需权衡隐私保护与模型效用。

Abstract: Federated Learning enables collaborative learning among clients via a
coordinating server while avoiding direct data sharing, offering a perceived
solution to preserve privacy. However, recent studies on Membership Inference
Attacks (MIAs) have challenged this notion, showing high success rates against
unprotected training data. While local differential privacy (LDP) is widely
regarded as a gold standard for privacy protection in data analysis, most
studies on MIAs either neglect LDP or fail to provide theoretical guarantees
for attack success rates against LDP-protected data. To address this gap, we
derive theoretical lower bounds for the success rates of low-polynomial time
MIAs that exploit vulnerabilities in fully connected or self-attention layers.
We establish that even when data are protected by LDP, privacy risks persist,
depending on the privacy budget. Practical evaluations on federated vision
models confirm considerable privacy risks, revealing that the noise required to
mitigate these attacks significantly degrades models' utility.

</details>


### [37] [LLM Jailbreak Oracle](https://arxiv.org/abs/2506.17299)
*Shuyi Lin,Anshuman Suri,Alina Oprea,Cheng Tan*

Main category: cs.CR

TL;DR: 论文提出了一种名为Boa的高效算法，用于解决大型语言模型（LLMs）的越狱漏洞评估问题。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在安全关键应用中的部署增加，缺乏系统方法评估其对越狱攻击的脆弱性成为一个关键安全缺口。

Method: Boa采用三阶段搜索策略：构建拒绝模式的黑名单、广度优先采样识别易访问的越狱路径、以及基于细粒度安全分数的深度优先优先级搜索。

Result: Boa能够高效解决越狱预言问题，支持系统防御评估、标准化红队攻击比较及极端对抗条件下的模型认证。

Conclusion: Boa为LLMs的越狱漏洞提供了系统化评估方法，填补了安全领域的空白。

Abstract: As large language models (LLMs) become increasingly deployed in
safety-critical applications, the lack of systematic methods to assess their
vulnerability to jailbreak attacks presents a critical security gap. We
introduce the jailbreak oracle problem: given a model, prompt, and decoding
strategy, determine whether a jailbreak response can be generated with
likelihood exceeding a specified threshold. This formalization enables a
principled study of jailbreak vulnerabilities. Answering the jailbreak oracle
problem poses significant computational challenges -- the search space grows
exponentially with the length of the response tokens. We present Boa, the first
efficient algorithm for solving the jailbreak oracle problem. Boa employs a
three-phase search strategy: (1) constructing block lists to identify refusal
patterns, (2) breadth-first sampling to identify easily accessible jailbreaks,
and (3) depth-first priority search guided by fine-grained safety scores to
systematically explore promising low-probability paths. Boa enables rigorous
security assessments including systematic defense evaluation, standardized
comparison of red team attacks, and model certification under extreme
adversarial conditions.

</details>


### [38] [A Nested Watermark for Large Language Models](https://arxiv.org/abs/2506.17308)
*Koichi Nagatsuka,Terufumi Morishita,Yasuhiro Sogawa*

Main category: cs.CR

TL;DR: 提出了一种嵌套水印方案，用于检测大型语言模型生成的文本，解决了单密钥泄露时的溯源问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型可能被滥用于生成虚假信息，现有水印技术在密钥泄露时无法溯源。

Method: 采用嵌套水印方案，嵌入两个独立密钥的水印，确保即使一个密钥泄露也能识别作者。

Result: 实验表明，该方法在保持文本流畅性的同时，实现了高检测准确率。

Conclusion: 嵌套水印方案有效提升了大型语言模型生成文本的可追溯性和安全性。

Abstract: The rapid advancement of large language models (LLMs) has raised concerns
regarding their potential misuse, particularly in generating fake news and
misinformation. To address these risks, watermarking techniques for
autoregressive language models have emerged as a promising means for detecting
LLM-generated text. Existing methods typically embed a watermark by increasing
the probabilities of tokens within a group selected according to a single
secret key. However, this approach suffers from a critical limitation: if the
key is leaked, it becomes impossible to trace the text's provenance or
attribute authorship. To overcome this vulnerability, we propose a novel nested
watermarking scheme that embeds two distinct watermarks into the generated text
using two independent keys. This design enables reliable authorship
identification even in the event that one key is compromised. Experimental
results demonstrate that our method achieves high detection accuracy for both
watermarks while maintaining the fluency and overall quality of the generated
text.

</details>


### [39] [Efficient Malware Detection with Optimized Learning on High-Dimensional Features](https://arxiv.org/abs/2506.17309)
*Aditya Choudhary,Sarthak Pawar,Yashodhara Haribhakta*

Main category: cs.CR

TL;DR: 论文提出了一种通过XGBoost特征选择和PCA降维技术优化高维特征的方法，用于高效且准确的恶意软件检测。


<details>
  <summary>Details</summary>
Motivation: 高维特征提取（如2381维）在恶意软件检测中带来计算挑战，需优化以平衡效率与性能。

Method: 结合XGBoost特征选择和PCA降维，评估128、256和384维特征在四种模型上的表现。

Result: LightGBM在384维特征上表现最佳，准确率达97.52%，且泛化能力优异。

Conclusion: 该方法在计算效率和检测性能间取得平衡，为恶意软件检测提供了可扩展的解决方案。

Abstract: Malware detection using machine learning requires feature extraction from
binary files, as models cannot process raw binaries directly. A common approach
involves using LIEF for raw feature extraction and the EMBER vectorizer to
generate 2381-dimensional feature vectors. However, the high dimensionality of
these features introduces significant computational challenges. This study
addresses these challenges by applying two dimensionality reduction techniques:
XGBoost-based feature selection and Principal Component Analysis (PCA). We
evaluate three reduced feature dimensions (128, 256, and 384), which correspond
to approximately 5.4%, 10.8%, and 16.1% of the original 2381 features, across
four models-XGBoost, LightGBM, Extra Trees, and Random Forest-using a unified
training, validation, and testing split formed from the EMBER-2018, ERMDS, and
BODMAS datasets. This approach ensures generalization and avoids dataset bias.
Experimental results show that LightGBM trained on the 384-dimensional feature
set after XGBoost feature selection achieves the highest accuracy of 97.52% on
the unified dataset, providing an optimal balance between computational
efficiency and detection performance. The best model, trained in 61 minutes
using 30 GB of RAM and 19.5 GB of disk space, generalizes effectively to
completely unseen datasets, maintaining 95.31% accuracy on TRITIUM and 93.98%
accuracy on INFERNO. These findings present a scalable, compute-efficient
approach for malware detection without compromising accuracy.

</details>


### [40] [Tracking GPTs Third Party Service: Automation, Analysis, and Insights](https://arxiv.org/abs/2506.17315)
*Chuan Yan,Liuhuo Wan,Bowei Guan,Fengqi Yu,Guangdong Bai,Jin Song Dong*

Main category: cs.CR

TL;DR: GPTs-ThirdSpy是一个自动化框架，用于提取GPTs的隐私设置，支持学术研究第三方服务集成。


<details>
  <summary>Details</summary>
Motivation: GPTs与第三方服务集成时隐私信息披露不足，难以系统性评估数据隐私影响。

Method: 开发GPTs-ThirdSpy框架，自动提取GPTs的隐私设置和第三方服务元数据。

Result: 提供实时、可靠的元数据，支持对集成、合规性和安全风险的深入分析。

Conclusion: GPTs-ThirdSpy有助于大规模研究GPT生态系统的透明度和监管挑战。

Abstract: ChatGPT has quickly advanced from simple natural language processing to
tackling more sophisticated and specialized tasks. Drawing inspiration from the
success of mobile app ecosystems, OpenAI allows developers to create
applications that interact with third-party services, known as GPTs. GPTs can
choose to leverage third-party services to integrate with specialized APIs for
domain-specific applications. However, the way these disclose privacy setting
information limits accessibility and analysis, making it challenging to
systematically evaluate the data privacy implications of third-party integrate
to GPTs. In order to support academic research on the integration of
third-party services in GPTs, we introduce GPTs-ThirdSpy, an automated
framework designed to extract privacy settings of GPTs. GPTs-ThirdSpy provides
academic researchers with real-time, reliable metadata on third-party services
used by GPTs, enabling in-depth analysis of their integration, compliance, and
potential security risks. By systematically collecting and structuring this
data, GPTs-ThirdSpy facilitates large-scale research on the transparency and
regulatory challenges associated with the GPT app ecosystem.

</details>


### [41] [Beyond the Scope: Security Testing of Permission Management in Team Workspace](https://arxiv.org/abs/2506.17317)
*Liuhuo Wan,Chuan Yan,Mark Huasong Meng,Kailong Wang,Haoyu Wang,Guangdong Bai,Jin Song Dong*

Main category: cs.CR

TL;DR: 团队工作空间广泛用于多用户协作和资源管理，但第三方插件可能绕过权限隔离，导致权限升级风险。本文通过分析生态系统中的访问控制机制，识别了三种安全风险，并开发了自动化工具TAI进行测试，发现41个存在问题的交互。


<details>
  <summary>Details</summary>
Motivation: 团队工作空间的多用户协作和插件集成功能使其成为资源管理的中心，但插件可能绕过权限隔离，引发安全问题。本文旨在调查插件权限管理的现状，揭示潜在风险。

Method: 通过深入分析团队工作空间生态系统的访问控制机制，考虑多用户和跨应用特性，识别安全风险。开发自动化工具TAI，系统测试所有可能的交互。

Result: 发现权限升级漏洞普遍存在，41个交互被识别为有问题。

Conclusion: 团队工作空间平台和第三方开发者需警惕权限升级风险，加强安全措施。

Abstract: Nowadays team workspaces are widely adopted for multi-user collaboration and
digital resource management. To further broaden real-world applications,
mainstream team workspaces platforms, such as Google Workspace and Microsoft
OneDrive, allow third-party applications (referred to as add-ons) to be
integrated into their workspaces, significantly extending the functionality of
team workspaces. The powerful multi-user collaboration capabilities and
integration of add-ons make team workspaces a central hub for managing shared
resources and protecting them against unauthorized access. Due to the
collaboration features of team workspaces, add-ons involved in collaborations
may bypass the permission isolation enforced by the administrator, unlike in
single-user permission management.
  This paper aims to investigate the permission management landscape of team
workspaces add-ons. To this end, we perform an in-depth analysis of the
enforced access control mechanism inherent in this ecosystem, considering both
multi-user and cross-app features. We identify three potential security risks
that can be exploited to cause permission escalation. We then systematically
reveal the landscape of permission escalation risks in the current ecosystem.
Specifically, we propose an automated tool, TAI, to systematically test all
possible interactions within this ecosystem. Our evaluation reveals that
permission escalation vulnerabilities are widespread in this ecosystem, with 41
interactions identified as problematic. Our findings should raise an alert to
both the team workspaces platforms and third-party developers.

</details>


### [42] [Context manipulation attacks : Web agents are susceptible to corrupted memory](https://arxiv.org/abs/2506.17318)
*Atharv Singh Patlan,Ashwin Hebbar,Pramod Viswanath,Prateek Mittal*

Main category: cs.CR

TL;DR: 论文提出了一种名为“计划注入”的新型攻击方法，针对自主网页导航代理的上下文漏洞，攻击成功率显著高于传统提示注入攻击。


<details>
  <summary>Details</summary>
Motivation: 由于大型语言模型（LLMs）的无状态特性，自主网页导航代理依赖外部内存系统维护上下文，而客户端或第三方管理的内存存在安全隐患。

Method: 通过“计划注入”攻击，操纵代理的内部任务表示，并评估了两种流行代理（Browser-use和Agent-E）的脆弱性。

Result: 攻击成功率比传统提示注入高3倍，且“上下文链式注入”在隐私窃取任务中成功率提升17.7%。

Conclusion: 代理系统的安全内存处理必须作为首要考虑因素。

Abstract: Autonomous web navigation agents, which translate natural language
instructions into sequences of browser actions, are increasingly deployed for
complex tasks across e-commerce, information retrieval, and content discovery.
Due to the stateless nature of large language models (LLMs), these agents rely
heavily on external memory systems to maintain context across interactions.
Unlike centralized systems where context is securely stored server-side, agent
memory is often managed client-side or by third-party applications, creating
significant security vulnerabilities. This was recently exploited to attack
production systems.
  We introduce and formalize "plan injection," a novel context manipulation
attack that corrupts these agents' internal task representations by targeting
this vulnerable context. Through systematic evaluation of two popular web
agents, Browser-use and Agent-E, we show that plan injections bypass robust
prompt injection defenses, achieving up to 3x higher attack success rates than
comparable prompt-based attacks. Furthermore, "context-chained injections,"
which craft logical bridges between legitimate user goals and attacker
objectives, lead to a 17.7% increase in success rate for privacy exfiltration
tasks. Our findings highlight that secure memory handling must be a first-class
concern in agentic systems.

</details>


### [43] [On the Performance of Cyber-Biomedical Features for Intrusion Detection in Healthcare 5.0](https://arxiv.org/abs/2506.17329)
*Pedro H. Lui,Lucas P. Siqueira,Juliano F. Kazienko,Vagner E. Quincozes,Silvio E. Quincozes,Daniel Welfer*

Main category: cs.CR

TL;DR: Healthcare 5.0结合AI、IoT和实时监测，但面临网络安全威胁。本研究应用XAI分析医疗数据，XGBoost在入侵检测中表现优异，网络数据起主导作用。


<details>
  <summary>Details</summary>
Motivation: 当前AI驱动的网络安全模型忽视生物医学数据，本研究旨在填补这一空白，提升医疗5.0的安全性。

Method: 应用可解释AI（XAI）分析整合网络流量和生物医学传感器数据的医疗5.0数据集。

Result: XGBoost在良性数据篡改中F1分数达99%，欺骗检测为81%。网络数据主导入侵检测，生物医学特征对欺骗检测有贡献。

Conclusion: XAI能有效提升医疗5.0的网络安全，网络数据是关键，生物医学特征对特定攻击检测有辅助作用。

Abstract: Healthcare 5.0 integrates Artificial Intelligence (AI), the Internet of
Things (IoT), real-time monitoring, and human-centered design toward
personalized medicine and predictive diagnostics. However, the increasing
reliance on interconnected medical technologies exposes them to cyber threats.
Meanwhile, current AI-driven cybersecurity models often neglect biomedical
data, limiting their effectiveness and interpretability. This study addresses
this gap by applying eXplainable AI (XAI) to a Healthcare 5.0 dataset that
integrates network traffic and biomedical sensor data. Classification outputs
indicate that XGBoost achieved 99% F1-score for benign and data alteration, and
81% for spoofing. Explainability findings reveal that network data play a
dominant role in intrusion detection whereas biomedical features contributed to
spoofing detection, with temperature reaching a Shapley values magnitude of
0.37.

</details>


### [44] [Privacy-Preserving LLM Interaction with Socratic Chain-of-Thought Reasoning and Homomorphically Encrypted Vector Databases](https://arxiv.org/abs/2506.17336)
*Yubeen Bae,Minchan Kim,Jaejin Lee,Sangbum Kim,Jaehyung Kim,Yejin Choi,Niloofar Mireshghallah*

Main category: cs.CR

TL;DR: 论文提出了一种结合强大但不可信的LLM与本地弱模型的方法，通过加密语义搜索保护用户隐私，并在性能上优于单独使用强大LLM。


<details>
  <summary>Details</summary>
Motivation: 解决用户在使用LLM处理敏感数据时面临的隐私与性能权衡问题。

Method: 采用Socratic Chain-of-Thought Reasoning，将任务分解为生成非私有提示和加密搜索私有数据，最后由本地模型生成响应。

Result: 在LoCoMo基准测试中，结合GPT-4o与本地Llama-3.2-1B模型的性能优于单独使用GPT-4o，提升达7.1个百分点。

Conclusion: 该方法为任务分解与隐私保护提供了初步解决方案，展示了强大LLM与本地模型协作的潜力。

Abstract: Large language models (LLMs) are increasingly used as personal agents,
accessing sensitive user data such as calendars, emails, and medical records.
Users currently face a trade-off: They can send private records, many of which
are stored in remote databases, to powerful but untrusted LLM providers,
increasing their exposure risk. Alternatively, they can run less powerful
models locally on trusted devices. We bridge this gap. Our Socratic
Chain-of-Thought Reasoning first sends a generic, non-private user query to a
powerful, untrusted LLM, which generates a Chain-of-Thought (CoT) prompt and
detailed sub-queries without accessing user data. Next, we embed these
sub-queries and perform encrypted sub-second semantic search using our
Homomorphically Encrypted Vector Database across one million entries of a
single user's private data. This represents a realistic scale of personal
documents, emails, and records accumulated over years of digital activity.
Finally, we feed the CoT prompt and the decrypted records to a local language
model and generate the final response. On the LoCoMo long-context QA benchmark,
our hybrid framework, combining GPT-4o with a local Llama-3.2-1B model,
outperforms using GPT-4o alone by up to 7.1 percentage points. This
demonstrates a first step toward systems where tasks are decomposed and split
between untrusted strong LLMs and weak local ones, preserving user privacy.

</details>


### [45] [AndroIDS : Android-based Intrusion Detection System using Federated Learning](https://arxiv.org/abs/2506.17349)
*Akarsh K Nair,Shanik Hubert Satheesh Kumar.,Deepti Gupta*

Main category: cs.CR

TL;DR: 论文提出了一种基于联邦学习的入侵检测框架AndroIDS，利用系统调用痕迹作为隐私保护数据源，有效检测安卓移动IoT系统中的异常行为，无需共享原始数据。


<details>
  <summary>Details</summary>
Motivation: 随着安卓移动IoT系统的快速增长，设备面临更多网络攻击风险，特别是在智能家居、无人机等场景，需要一种隐私保护的入侵检测方法。

Method: 采用联邦学习方法，利用系统调用痕迹构建个性化数据集，支持分布式节点协作检测异常行为，避免数据共享。

Result: 在IID和非IID条件下，模型准确率分别为96.46%和92.87%，F1分数为89%和86%，表现出对数据异构性的鲁棒性。

Conclusion: 该方法在真实移动IoT场景中具有实用性和可扩展性，为隐私保护的入侵检测提供了有效解决方案。

Abstract: The exponential growth of android-based mobile IoT systems has significantly
increased the susceptibility of devices to cyberattacks, particularly in smart
homes, UAVs, and other connected mobile environments. This article presents a
federated learning-based intrusion detection framework called AndroIDS that
leverages system call traces as a personalized and privacy-preserving data
source. Unlike conventional centralized approaches, the proposed method enables
collaborative anomaly detection without sharing raw data, thus preserving user
privacy across distributed nodes. A generalized system call dataset was
generated to reflect realistic android system behavior and serves as the
foundation for experimentation. Extensive evaluation demonstrates the
effectiveness of the FL model under both IID and non-IID conditions, achieving
an accuracy of 96.46 % and 92.87 %, and F1-scores of 89 % and 86 %,
respectively. These results highlight the models robustness to data
heterogeneity, with only a minor performance drop in the non-IID case. Further,
a detailed comparison with centralized deep learning further illustrates
trade-offs in detection performance and deployment feasibility. Overall, the
results validate the practical applicability of the proposed approach for
secure and scalable intrusion detection in real-world mobile IoT scenarios.

</details>


### [46] [CUBA: Controlled Untargeted Backdoor Attack against Deep Neural Networks](https://arxiv.org/abs/2506.17350)
*Yinghao Wu,Liyan Zhang*

Main category: cs.CR

TL;DR: 提出了一种新型的约束无目标后门攻击（CUBA），结合了无目标攻击的灵活性和目标攻击的意图性，能够绕过现有的后门防御方法。


<details>
  <summary>Details</summary>
Motivation: 现有后门攻击多为目标攻击，其触发机制与特定恶意行为强关联，而无目标攻击在某种程度上削弱了攻击效果。CUBA旨在结合两者的优势。

Method: 通过应用对数归一化交叉熵损失与翻转的独热标签，约束训练过程中的对数，使模型在选定目标类别上呈现均匀分布。

Result: 实验表明，CUBA在不同数据集上均表现出有效性，能够绕过现有防御方法。

Conclusion: CUBA成功结合了无目标攻击的灵活性和目标攻击的意图性，为后门攻击提供了新思路。

Abstract: Backdoor attacks have emerged as a critical security threat against deep
neural networks in recent years. The majority of existing backdoor attacks
focus on targeted backdoor attacks, where trigger is strongly associated to
specific malicious behavior. Various backdoor detection methods depend on this
inherent property and shows effective results in identifying and mitigating
such targeted attacks. However, a purely untargeted attack in backdoor
scenarios is, in some sense, self-weakening, since the target nature is what
makes backdoor attacks so powerful. In light of this, we introduce a novel
Constrained Untargeted Backdoor Attack (CUBA), which combines the flexibility
of untargeted attacks with the intentionality of targeted attacks. The
compromised model, when presented with backdoor images, will classify them into
random classes within a constrained range of target classes selected by the
attacker. This combination of randomness and determinedness enables the
proposed untargeted backdoor attack to natively circumvent existing backdoor
defense methods. To implement the untargeted backdoor attack under controlled
flexibility, we propose to apply logit normalization on cross-entropy loss with
flipped one-hot labels. By constraining the logit during training, the
compromised model will show a uniform distribution across selected target
classes, resulting in controlled untargeted attack. Extensive experiments
demonstrate the effectiveness of the proposed CUBA on different datasets.

</details>


### [47] [Differentiation-Based Extraction of Proprietary Data from Fine-Tuned LLMs](https://arxiv.org/abs/2506.17353)
*Zongjie Li,Daoyuan Wu,Shuai Wang,Zhendong Su*

Main category: cs.CR

TL;DR: 本文首次研究了监督微调（SFT）数据提取问题，提出了一种新的提取方法DDE，并通过实验验证其优于现有基线。同时，提出了一种防御机制以应对此类攻击。


<details>
  <summary>Details</summary>
Motivation: 随着领域特定和人类对齐的大语言模型（LLM）需求增加，SFT数据集成为潜在提取目标，研究其数据泄露风险具有重要意义。

Method: 提出了一种针对SFT模型的差异化数据提取方法（DDE），利用微调模型的置信度及其与预训练基模型的行为差异。

Result: 实验表明，DDE在所有攻击设置中均优于现有基线，验证了SFT数据提取的可行性。

Conclusion: 研究揭示了微调LLM中隐藏的数据泄露风险，并为开发更安全的模型提供了见解。

Abstract: The increasing demand for domain-specific and human-aligned Large Language
Models (LLMs) has led to the widespread adoption of Supervised Fine-Tuning
(SFT) techniques. SFT datasets often comprise valuable instruction-response
pairs, making them highly valuable targets for potential extraction. This paper
studies this critical research problem for the first time. We start by formally
defining and formulating the problem, then explore various attack goals, types,
and variants based on the unique properties of SFT data in real-world
scenarios. Based on our analysis of extraction behaviors of direct extraction,
we develop a novel extraction method specifically designed for SFT models,
called Differentiated Data Extraction (DDE), which exploits the confidence
levels of fine-tuned models and their behavioral differences from pre-trained
base models. Through extensive experiments across multiple domains and
scenarios, we demonstrate the feasibility of SFT data extraction using DDE. Our
results show that DDE consistently outperforms existing extraction baselines in
all attack settings. To counter this new attack, we propose a defense mechanism
that mitigates DDE attacks with minimal impact on model performance. Overall,
our research reveals hidden data leak risks in fine-tuned LLMs and provides
insights for developing more secure models.

</details>


### [48] [Secret Sharing in 5G-MEC: Applicability for joint Security and Dependability](https://arxiv.org/abs/2506.17371)
*Thilina Pathirana,Ruxandra F. Olimid*

Main category: cs.CR

TL;DR: 论文研究了在5G-MEC存储中使用阈值秘密共享技术，以增强数据安全性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 5G-MEC的分布式特性带来了隐私和安全挑战，需要确保敏感数据在边缘的高效操作和安全性。

Method: 采用(k,n)阈值秘密共享方案，将数据分散存储于n个节点，需至少k个节点才能重构数据，同时考虑MEH的可信度选择存储节点。

Result: 该方案能防止少于k个节点的数据泄露，并容忍最多n-k个节点故障，提升数据保密性和可用性。

Conclusion: 阈值秘密共享在5G-MEC中有效应对未授权访问和节点故障，且其节点选择方法可推广至其他场景。

Abstract: Multi-access Edge Computing (MEC), an enhancement of 5G, processes data
closer to its generation point, reducing latency and network load. However, the
distributed and edge-based nature of 5G-MEC presents privacy and security
challenges, including data exposure risks. Ensuring efficient manipulation and
security of sensitive data at the edge is crucial. To address these challenges,
we investigate the usage of threshold secret sharing in 5G-MEC storage, an
approach that enhances both security and dependability. A (k,n) threshold
secret sharing scheme splits and stores sensitive data among n nodes, requiring
at least k nodes for reconstruction. The solution ensures confidentiality by
protecting data against fewer than k colluding nodes and enhances availability
by tolerating up to n-k failing nodes. This approach mitigates threats such as
unauthorized access and node failures, whether accidental or intentional. We
further discuss a method for selecting the convenient MEHs to store the shares,
considering the MEHs' trustworthiness level as a main criterion. Although we
define our proposal in the context of secret-shared data storage, it can be
seen as an independent, standalone selection process for 5G-MEC trustworthy
node selection in other scenarios too.

</details>


### [49] [Open Sky, Open Threats: Replay Attacks in Space Launch and Re-entry Phases](https://arxiv.org/abs/2506.17446)
*Nesrine Benchoubane,Eray Guven,Gunes Karabulut Kurt*

Main category: cs.CR

TL;DR: 研究重放攻击对航天器通信上下行链路完整性的影响，提出改进接收器设计以增强抗干扰能力。


<details>
  <summary>Details</summary>
Motivation: 探讨重放攻击在航天器关键通信阶段（发射和再入）对通信完整性的威胁。

Method: 结合软件定义无线电（SDR）和实时信道模拟器，模拟真实攻击条件，并提出改进接收器设计。

Result: 攻击信号可压制合法传输，信噪比差异达-7.8 dB（再入）和-6.5 dB（发射）。

Conclusion: 改进的接收器设计通过相位相干性依赖的DD均衡器和窄带PLL，增强了对重放干扰的抵抗能力。

Abstract: This paper examines the effects of replay attacks on the integrity of both
uplink and downlink communications during critical phases of spacecraft
communication. By combining software-defined radios (SDRs) with a real-time
channel emulator, we replicate realistic attack conditions on the Orion
spacecraft's communication systems in both launch and reentry. Our evaluation
shows that, under replay attacks, the attacker's signal can overpower
legitimate transmissions, leading to a Signal to Noise Ratio (SNR) difference
of up to -7.8 dB during reentry and -6.5 dB during launch. To mitigate these
threats, we propose a more secure receiver design incorporating a
phase-coherency-dependent decision-directed (DD) equalizer with a narrowed
phase-locked loop (PLL) bandwidth. This configuration enhances resilience by
making synchronization more sensitive to phase distortions caused by replay
interference.

</details>


### [50] [A Smart Contract-based Non-Transferable Signature Verification System using Nominative Signatures](https://arxiv.org/abs/2506.17504)
*Hinata Nishino,Kazumasa Omote,Keita Emura*

Main category: cs.CR

TL;DR: 论文提出了一种基于智能合约和提名签名技术的不可转让签名验证系统，用于验证资金转移情况。


<details>
  <summary>Details</summary>
Motivation: 提名签名虽能限制签名验证者，但无法验证资金是否已转移或即将转移，尤其在加密货币场景下需要同时验证签名和资金转移。

Method: 将Hanaoka-Schuldt提名签名方案从对称配对改为非对称配对，并通过智能合约运行验证算法，评估其燃气成本。

Result: 系统能同时验证签名和资金转移，且提名签名的不可见性使其适合在区块链上发布。

Conclusion: 基于智能合约的提名签名系统解决了资金转移验证问题，为加密货币场景提供了实用解决方案。

Abstract: Nominative signatures allow us to indicate who can verify a signature, and
they can be employed to construct a non-transferable signature verification
system that prevents the signature verification by a third party in unexpected
situations. For example, this system can prevent IOU/loan certificate
verification in unexpected situations. However, nominative signatures
themselves do not allow the verifier to check whether the funds will be
transferred in the future or have been transferred.It would be desirable to
verify the fact simultaneously when the system involves a certain money
transfer such as cryptocurrencies/cryptoassets. In this paper, we propose a
smart contract-based non-transferable signature verification system using
nominative signatures. We pay attention to the fact that the invisibility,
which is a security requirement to be held for nominative signatures, allows us
to publish nominative signatures on the blockchain. Our system can verify
whether a money transfer actually will take place, in addition to indicating
who can verify a signature. We transform the Hanaoka-Schuldt nominative
signature scheme (ACNS 2011, IEICE Trans. 2016) which is constructed over a
symmetric pairing to a scheme constructed over an asymmetric pairing, and
evaluate the gas cost when a smart contract runs the verification algorithm of
the modified Hanaoka-Schuldt nominative signature scheme.

</details>


### [51] [Semantic-Aware Parsing for Security Logs](https://arxiv.org/abs/2506.17512)
*Julien Piet,Vivian Fang,Rishi Khare,Vern Paxson,Raluca Ada Popa,David Wagner*

Main category: cs.CR

TL;DR: Matryoshka是一个端到端系统，利用LLM自动生成具有语义感知的结构化日志解析器，结合语法和语义解析层，提升日志查询效率和安全性。


<details>
  <summary>Details</summary>
Motivation: 解决现有AI日志解析器缺乏语义解释能力，以及直接查询大型语言模型在规模和安全性上的不足。

Method: 结合基于精确正则表达式的语法解析器和新的语义解析层，将变量聚类并映射到可查询的语义模式中。

Result: 语法解析器优于现有方法，语义层在安全查询上F1得分为0.95，显著减少手动工作。

Conclusion: Matryoshka通过自动提取和组织字段，推动了全自动AI驱动的日志分析的发展。

Abstract: Security analysts struggle to quickly and efficiently query and correlate log
data due to the heterogeneity and lack of structure in real-world logs.
Existing AI-based parsers focus on learning syntactic log templates but lack
the semantic interpretation needed for querying. Directly querying large
language models on raw logs is impractical at scale and vulnerable to prompt
injection attacks.
  In this paper, we introduce Matryoshka, the first end-to-end system
leveraging LLMs to automatically generate semantically-aware structured log
parsers. Matryoshka combines a novel syntactic parser-employing precise regular
expressions rather than wildcards-with a completely new semantic parsing layer
that clusters variables and maps them into a queryable, contextually meaningful
schema. This approach provides analysts with queryable and semantically rich
data representations, facilitating rapid and precise log querying without the
traditional burden of manual parser construction. Additionally, Matryoshka can
map the newly created fields to recognized attributes within the Open
Cybersecurity Schema Framework (OCSF), enabling interoperability.
  We evaluate Matryoshka on a newly curated real-world log benchmark,
introducing novel metrics to assess how consistently fields are named and
mapped across logs. Matryoshka's syntactic parser outperforms prior works, and
the semantic layer achieves an F1 score of 0.95 on realistic security queries.
Although mapping fields to the extensive OCSF taxonomy remains challenging,
Matryoshka significantly reduces manual effort by automatically extracting and
organizing valuable fields, moving us closer to fully automated, AI-driven log
analytics.

</details>


### [52] [SoK: Stablecoin Designs, Risks, and the Stablecoin LEGO](https://arxiv.org/abs/2506.17622)
*Shengchen Ling,Yuefeng Du,Yajin Zhou,Lei Wu,Cong Wang,Xiaohua Jia,Houmin Yan*

Main category: cs.CR

TL;DR: 本文通过分析157项研究、95种活跃稳定币和44起重大安全事件，提出了稳定币的四个关键见解，并引入Stablecoin LEGO框架，为构建更具韧性的稳定币提供系统基础。


<details>
  <summary>Details</summary>
Motivation: 稳定币在现代金融中占据重要地位，但其设计权衡、安全动态和失败路径的系统性理解仍不足。本文旨在填补这一空白。

Method: 通过大规模分析研究、稳定币案例和安全事件，提出四个关键见解，并开发Stablecoin LEGO框架。

Result: 1) 稳定性是依赖市场信心和流动性的脆弱状态；2) 设计存在风险专业化而非缓解的权衡；3) 收益机制导致稳定性与高风险金融工程的系统性张力；4) 安全事件通过压力测试推动韧性提升。LEGO框架显示，较低风险与历史教训的整合密切相关。

Conclusion: 本文为构建、评估和监管更具韧性的稳定币提供了系统性基础。

Abstract: Stablecoins have become significant assets in modern finance, with a market
capitalization exceeding USD 246 billion (May 2025). Yet, despite their
systemic importance, a comprehensive and risk-oriented understanding of crucial
aspects like their design trade-offs, security dynamics, and interdependent
failure pathways often remains underdeveloped. This SoK confronts this gap
through a large-scale analysis of 157 research studies, 95 active stablecoins,
and 44 major security incidents. Our analysis establishes four pivotal
insights: 1) stability is best understood not an inherent property but an
emergent, fragile state reliant on the interplay between market confidence and
continuous liquidity; 2) stablecoin designs demonstrate trade-offs in risk
specialization instead of mitigation; 3) the widespread integration of yield
mechanisms imposes a "dual mandate" that creates a systemic tension between the
core mission of stability and the high-risk financial engineering required for
competitive returns; and 4) major security incidents act as acute "evolutionary
pressures", forging resilience by stress-testing designs and aggressively
redefining the security frontier. We introduce the Stablecoin LEGO framework, a
quantitative methodology mapping historical failures to current designs. Its
application reveals that a lower assessed risk strongly correlates with
integrating lessons from past incidents. We hope this provides a systematic
foundation for building, evaluating, and regulating more resilient stablecoins.

</details>


### [53] [List-Decodable Byzantine Robust PIR: Lower Communication Complexity, Higher Byzantine Tolerance, Smaller List Size](https://arxiv.org/abs/2506.17625)
*Pengzhen Ke,Liang Feng Zhang,Huaxiong Wang,Li-Ping Wang*

Main category: cs.CR

TL;DR: 提出了两种完美的列表可解码拜占庭鲁棒PIR方案，首次同时处理多数恶意服务器、实现低通信复杂度并提供非平凡列表大小估计。


<details>
  <summary>Details</summary>
Motivation: 解决恶意服务器环境下的隐私信息检索问题，提升拜占庭容忍度和通信效率。

Method: 设计两种列表可解码拜占庭鲁棒PIR方案，优化通信复杂度和列表大小。

Result: 方案实现了更低的通信复杂度、更高的拜占庭容忍度和更小的列表大小。

Conclusion: 提出的方案在恶意服务器环境下显著提升了PIR的性能和效率。

Abstract: Private Information Retrieval (PIR) is a privacy-preserving primitive in
cryptography. Significant endeavors have been made to address the variant of
PIR concerning the malicious servers. Among those endeavors, list-decodable
Byzantine robust PIR schemes may tolerate a majority of malicious responding
servers that provide incorrect answers. In this paper, we propose two perfect
list-decodable BRPIR schemes. Our schemes are the first ones that can
simultaneously handle a majority of malicious responding servers, achieve a
communication complexity of $o(n^{1/2})$ for a database of size n, and provide
a nontrivial estimation on the list sizes. Compared with the existing
solutions, our schemes attain lower communication complexity, higher byzantine
tolerance, and smaller list size.

</details>


### [54] [A Locally Differential Private Coding-Assisted Succinct Histogram Protocol](https://arxiv.org/abs/2506.17767)
*Hsuan-Po Liu,Hessam Mahdavifar*

Main category: cs.CR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: A succinct histogram captures frequent items and their frequencies across
clients and has become increasingly important for large-scale,
privacy-sensitive machine learning applications. To develop a rigorous
framework to guarantee privacy for the succinct histogram problem, local
differential privacy (LDP) has been utilized and shown promising results. To
preserve data utility under LDP, which essentially works by intentionally
adding noise to data, error-correcting codes naturally emerge as a promising
tool for reliable information collection. This work presents the first
practical $(\epsilon,\delta)$-LDP protocol for constructing succinct histograms
using error-correcting codes. To this end, polar codes and their
successive-cancellation list (SCL) decoding algorithms are leveraged as the
underlying coding scheme. More specifically, our protocol introduces
Gaussian-based perturbations to enable efficient soft decoding. Experiments
demonstrate that our approach outperforms prior methods, particularly for items
with low true frequencies, while maintaining similar frequency estimation
accuracy.

</details>


### [55] [A TRNG Implemented using a Soft-Data Based Sponge Function within a Unified Strong PUF Architecture](https://arxiv.org/abs/2506.17795)
*Rachel Cazzola,Cyrus Minwalla,Calvin Chan,Jim Plusquellic*

Main category: cs.CR

TL;DR: 提出了一种统一的PUF-TRNG架构，结合静态和动态熵源，通过改进的算法实现高安全性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 硬件安全原语（如TRNG和PUF）是微电子系统信任根的核心组件，需要一种高效且安全的统一架构。

Method: 利用SiRF PUF的静态熵和路径延迟测量中的动态噪声，结合改进的双工海绵构造算法进行数据处理。

Result: 在FPGA板上实现并通过多项测试，表现出高最小熵和中等数据速率的稳定设计。

Conclusion: 该架构在安全性和性能上表现优异，适用于实际应用。

Abstract: Hardware security primitives including True Random Number Generators (TRNG)
and Physical Unclonable Functions (PUFs) are central components to establishing
a root of trust in microelectronic systems. In this paper, we propose a unified
PUF-TRNG architecture that leverages a combination of the static entropy
available in a strong PUF called the shift-register, reconvergent-fanout (SiRF)
PUF, and the dynamic entropy associated with random noise present in path delay
measurements. The SiRF PUF uses an engineered netlist containing a large number
of paths as the source of static entropy, and a time-to-digital-converter (TDC)
as a high-resolution, embedded instrument for measuring path delays, where
measurement noise serves as the source of dynamic entropy. A novel data
postprocessing algorithm is proposed based on a modified duplex sponge
construction. The sponge function operates on soft data, i.e., fixed point data
values, to add entropy to the ensuing random bit sequences and to increase the
bit generation rate. A postprocessing algorithm for reproducing PUF-generated
encryption keys is also used in the TRNG to protect against temperature voltage
attacks designed to subvert the random characteristics in the bit sequences.
The unified PUF-TRNG architecture is implemented across multiple instances of a
ZYBO Z7-10 FPGA board and extensively tested with NIST SP 800-22, NIST SP
800-90B, AIS-31, and DieHarder test suites. Results indicate a stable and
robust TRNG design with excellent min-entropy and a moderate data rate.

</details>


### [56] [AdRo-FL: Informed and Secure Client Selection for Federated Learning in the Presence of Adversarial Aggregator](https://arxiv.org/abs/2506.17805)
*Md. Kamrul Hossain,Walid Aljoby,Anis Elgabli,Ahmed M. Abdelmoniem,Khaled A. Harras*

Main category: cs.CR

TL;DR: AdRo-FL 是一种联邦学习方法，通过智能客户端选择和防御机制，同时提升性能并抵御 Biased Selection Attack (BSA)。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习中的安全聚合（SA）易受 BSA 攻击，而随机选择又无法满足性能需求。

Method: 提出两种客户端选择框架：基于集群信任的配额机制和分布式环境下的两阶段选择协议。

Result: AdRo-FL 在准确率和收敛速度上均优于不安全基线。

Conclusion: AdRo-FL 在保护隐私的同时，显著提升了联邦学习的效率和性能。

Abstract: Federated Learning (FL) enables collaborative learning without exposing
clients' data. While clients only share model updates with the aggregator,
studies reveal that aggregators can infer sensitive information from these
updates. Secure Aggregation (SA) protects individual updates during
transmission; however, recent work demonstrates a critical vulnerability where
adversarial aggregators manipulate client selection to bypass SA protections,
constituting a Biased Selection Attack (BSA). Although verifiable random
selection prevents BSA, it precludes informed client selection essential for FL
performance. We propose Adversarial Robust Federated Learning (AdRo-FL), which
simultaneously enables: informed client selection based on client utility, and
robust defense against BSA maintaining privacy-preserving aggregation. AdRo-FL
implements two client selection frameworks tailored for distinct settings. The
first framework assumes clients are grouped into clusters based on mutual
trust, such as different branches of an organization. The second framework
handles distributed clients where no trust relationships exist between them.
For the cluster-oriented setting, we propose a novel defense against BSA by (1)
enforcing a minimum client selection quota from each cluster, supervised by a
cluster-head in every round, and (2) introducing a client utility function to
prioritize efficient clients. For the distributed setting, we design a
two-phase selection protocol: first, the aggregator selects the top clients
based on our utility-driven ranking; then, a verifiable random function (VRF)
ensures a BSA-resistant final selection. AdRo-FL also applies quantization to
reduce communication overhead and sets strict transmission deadlines to improve
energy efficiency. AdRo-FL achieves up to $1.85\times$ faster time-to-accuracy
and up to $1.06\times$ higher final accuracy compared to insecure baselines.

</details>


### [57] [LASA: Enhancing SoC Security Verification with LLM-Aided Property Generation](https://arxiv.org/abs/2506.17865)
*Dinesh Reddy Ankireddy,Sudipta Paria,Aritra Dasgupta,Sandip Ray,Swarup Bhunia*

Main category: cs.CR

TL;DR: LASA利用LLMs和RAG技术，自动生成非空泛的安全属性和SystemVerilog断言，显著提升SoC设计的形式验证效率和覆盖率。


<details>
  <summary>Details</summary>
Motivation: 现代SoC设计复杂度高，传统形式验证方法需要大量人工编写安全属性，耗时、昂贵且易错。

Method: LASA结合LLMs和RAG技术，从设计文档生成安全属性和SVA，并通过EDA工具进行形式验证和迭代优化。

Result: 在多个开源SoC设计中，LASA平均覆盖率达88%，并成功检测到OpenTitan SoC中的5个独特漏洞。

Conclusion: LASA框架高效、全面，显著提升了SoC设计的安全验证能力。

Abstract: Ensuring the security of modern System-on-Chip (SoC) designs poses
significant challenges due to increasing complexity and distributed assets
across the intellectual property (IP) blocks. Formal property verification
(FPV) provides the capability to model and validate design behaviors through
security properties with model checkers; however, current practices require
significant manual efforts to create such properties, making them
time-consuming, costly, and error-prone. The emergence of Large Language Models
(LLMs) has showcased remarkable proficiency across diverse domains, including
HDL code generation and verification tasks. Current LLM-based techniques often
produce vacuous assertions and lack efficient prompt generation, comprehensive
verification, and bug detection. This paper presents LASA, a novel framework
that leverages LLMs and retrieval-augmented generation (RAG) to produce
non-vacuous security properties and SystemVerilog Assertions (SVA) from design
specifications and related documentation for bus-based SoC designs. LASA
integrates commercial EDA tool for FPV to generate coverage metrics and
iteratively refines prompts through a feedback loop to enhance coverage. The
effectiveness of LASA is validated through various open-source SoC designs,
demonstrating high coverage values with an average of ~88\%, denoting
comprehensive verification through efficient generation of security properties
and SVAs. LASA also demonstrates bug detection capabilities, identifying five
unique bugs in the buggy OpenTitan SoC from Hack@DAC'24 competition.

</details>


### [58] [Cost-Effective Optimization and Implementation of the CRT-Paillier Decryption Algorithm for Enhanced Performance](https://arxiv.org/abs/2506.17935)
*Zhengwu Huang,Ding Deng,Pengyue Sun,Guangfu Sun,Xiaomei Tang*

Main category: cs.CR

TL;DR: 提出了一种名为eCRT-Paillier的解密算法，通过结合预计算参数和消除额外判断操作，显著提高了Paillier算法的解密效率，并设计了一个高效并行架构MESA，在FPGA上实现了高吞吐量的解密加速。


<details>
  <summary>Details</summary>
Motivation: 解决Paillier算法在云计算中隐私保护时因复杂模运算导致的解密效率低的问题。

Method: 结合预计算参数和消除额外判断操作优化解密链，设计并行架构MESA，简化模幂运算单元并并行化数据流。

Result: MESA在2048位密钥下解密时间为0.577ms，吞吐量提升1.16至313.21倍，资源效率显著提高。

Conclusion: eCRT-Paillier算法和MESA架构显著提升了Paillier算法的解密效率和资源利用率。

Abstract: To address the privacy protection problem in cloud computing, privacy
enhancement techniques such as the Paillier additive homomorphism algorithm are
receiving widespread attention. Paillier algorithm allows addition and scalar
multiplication operations in dencrypted state, which can effectively protect
privacy. However, its computational efficiency is limited by complex modulo
operations due to the ciphertext expansion followed by encryption. To
accelerate its decryption operation, the Chinese Remainder Theorem (CRT) is
often used to optimize these modulo operations, which lengthens the decryption
computation chain in turn. To address this issue, we propose an eCRT-Paillier
decryption algorithm that shortens the decryption computation chain by
combining precomputed parameters and eliminating extra judgment operations
introduced by Montgomery modular multiplications. These two improvements reduce
50% modular multiplications and 60% judgment operations in the postprocessing
of the CRT-Paillier decryption algorithm. Based on these improvements, we
propose a highly parallel full-pipeline architecture to eliminate stalls caused
by multiplier reuse in traditional modular exponentiation operations. This
architecture also adopts some optimizations such as simplifying modular
exponentiation units by dividing the exponent into segments and parallelizing
data flow by multi-core instantiation. Finally, a high-throughput and efficient
Paillier accelerator named MESA was implemented on the Xilinx Virtex-7 FPGA for
evaluation, which can complete a decryption using 2048-bit key within 0.577ms
under 100 MHz clock frequency. Compared to prior works, MESA demonstrates a
throughput improvement of 1.16 to 313.21 under identical conditions, also with
enhancements in area efficiency for LUT, DSP, and FF of 3.32 to 117.55, 1.49 to
1.64, and 2.94 to 9.94, respectively.

</details>


### [59] [Secure User-friendly Blockchain Modular Wallet Design Using Android & OP-TEE](https://arxiv.org/abs/2506.17988)
*Seongjin Kim,Sanguk Yun,Jungho Jang*

Main category: cs.CR

TL;DR: 本文提出了一种基于ARM TrustZone和OP-TEE的密钥管理平台，解决了加密经济中私钥泄漏和互操作性问题。


<details>
  <summary>Details</summary>
Motivation: 传统钱包在软件栈的各个层次泄漏私钥，导致数字资产流失。

Method: 通过重新设计密钥管理为平台级服务，利用多租户TA存储和模块化设计，结合硬件安全机制和OTA更新。

Result: 实现了安全、可扩展的多链资产管理平台，抵御多种攻击层。

Conclusion: 模块化TEE是Web3的关键OS原语，为全球部署提供了可审计、弹性的多链资产管理蓝图。

Abstract: Emerging crypto economies still hemorrhage digital assets because legacy
wallets leak private keys at almost every layer of the software stack, from
user-space libraries to kernel memory dumps. This paper solves that twin crisis
of security and interoperability by re-imagining key management as a
platform-level service anchored in ARM TrustZone through OP-TEE. Our
architecture fractures the traditional monolithic Trusted Application into
per-chain modules housed in a multi-tenant TA store, finally breaking OP-TEE's
single-binary ceiling. A cryptographically sealed firmware-over-the-air
pipeline welds each TA set to an Android system image, enabling hot-swap
updates while Verified Boot enforces rollback protection. Every package carries
a chained signature developer first, registry second so even a compromised
supply chain cannot smuggle malicious code past the Secure World's RSA-PSS
gatekeeper. Inside the TEE, strict inter-TA isolation, cache partitioning, and
GP-compliant crypto APIs ensure secrets never bleed across trust boundaries or
timing domains. The Rich Execution Environment can interact only via
hardware-mediated Secure Monitor Calls, collapsing the surface exposed to
malware in Android space. End-users enjoy a single polished interface yet can
install or retire Bitcoin, Ethereum, Solana, or tomorrow's chain with one tap,
shrinking both storage footprint and audit scope. For auditors, the composition
model slashes duplicated verification effort by quarantining blockchain logic
inside narrowly scoped modules that share formally specified interfaces. Our
threat analysis spans six adversary layers and shows how the design neutralizes
REE malware sniffing, OTA injection, and cross-module side channels without
exotic hardware. A reference implementation on AOSP exports a Wallet Manager
HAL, custom SELinux domains, and a CI/CD pipeline that vet community modules
before release. The result is not merely another hardware wallet but a
programmable substrate that can evolve at the velocity of the blockchain
ecosystem. By welding radical extensibility to hardware-anchored assurance, the
platform closes the security-usability gap that has long stymied mass-market
self-custody. We posit that modular TEEs are the missing OS primitive for Web3,
much as virtual memory unlocked multi-tasking in classical computing. Together,
these contributions sketch a blueprint for multi-chain asset management that is
auditable, resilient, and poised for global deployment.

</details>


### [60] [Mechanistic Interpretability in the Presence of Architectural Obfuscation](https://arxiv.org/abs/2506.18053)
*Marcos Florencio,Thomas Barton*

Main category: cs.CR

TL;DR: 研究了架构混淆对GPT-2-small模型机制可解释性的影响，发现混淆虽改变激活模式但保留计算图，阻碍细粒度解释但保持任务性能。


<details>
  <summary>Details</summary>
Motivation: 探讨架构混淆是否真正阻碍模型机制理解，还是仅改变表示坐标系。

Method: 在GPT-2-small模型上应用混淆映射，使用logit-lens归因、因果路径修补和注意力头消融技术分析。

Result: 混淆显著改变注意力头激活模式但保留层间计算图，阻碍用户提示的反向工程，但前馈和残差路径功能完好。

Conclusion: 架构混淆可同时保持全局模型行为并阻碍用户内容机制分析，为隐私保护和可解释性工具提供指导。

Abstract: Architectural obfuscation - e.g., permuting hidden-state tensors, linearly
transforming embedding tables, or remapping tokens - has recently gained
traction as a lightweight substitute for heavyweight cryptography in
privacy-preserving large-language-model (LLM) inference. While recent work has
shown that these techniques can be broken under dedicated reconstruction
attacks, their impact on mechanistic interpretability has not been
systematically studied. In particular, it remains unclear whether scrambling a
network's internal representations truly thwarts efforts to understand how the
model works, or simply relocates the same circuits to an unfamiliar coordinate
system. We address this gap by analyzing a GPT-2-small model trained from
scratch with a representative obfuscation map. Assuming the obfuscation map is
private and the original basis is hidden (mirroring an honest-but-curious
server), we apply logit-lens attribution, causal path-patching, and
attention-head ablation to locate and manipulate known circuits. Our findings
reveal that obfuscation dramatically alters activation patterns within
attention heads yet preserves the layer-wise computational graph. This
disconnect hampers reverse-engineering of user prompts: causal traces lose
their alignment with baseline semantics, and token-level logit attributions
become too noisy to reconstruct. At the same time, feed-forward and residual
pathways remain functionally intact, suggesting that obfuscation degrades
fine-grained interpretability without compromising top-level task performance.
These results establish quantitative evidence that architectural obfuscation
can simultaneously (i) retain global model behaviour and (ii) impede
mechanistic analyses of user-specific content. By mapping where
interpretability breaks down, our study provides guidance for future privacy
defences and for robustness-aware interpretability tooling.

</details>


### [61] [Federated Learning-Based Data Collaboration Method for Enhancing Edge Cloud AI System Security Using Large Language Models](https://arxiv.org/abs/2506.18087)
*Huaiying Luo,Cheng Ji*

Main category: cs.CR

TL;DR: 提出了一种基于联邦学习的数据协作方法，结合安全多方计算协议和大型语言模型（LLMs），优化边缘云AI系统的数据隐私保护和效率。


<details>
  <summary>Details</summary>
Motivation: 随着边缘计算和云系统在AI应用中的普及，如何在保证数据隐私的同时维持高效性能成为关键安全问题。

Method: 在现有联邦学习框架中引入安全多方计算协议，利用LLMs优化数据聚合与加密过程，并结合对抗训练技术增强系统安全性。

Result: 实验表明，该方法在数据保护和模型鲁棒性上比传统联邦学习方法提升15%。

Conclusion: 该方法有效提升了边缘云AI系统的隐私保护和安全性，具有实际应用价值。

Abstract: With the widespread application of edge computing and cloud systems in
AI-driven applications, how to maintain efficient performance while ensuring
data privacy has become an urgent security issue. This paper proposes a
federated learning-based data collaboration method to improve the security of
edge cloud AI systems, and use large-scale language models (LLMs) to enhance
data privacy protection and system robustness. Based on the existing federated
learning framework, this method introduces a secure multi-party computation
protocol, which optimizes the data aggregation and encryption process between
distributed nodes by using LLM to ensure data privacy and improve system
efficiency. By combining advanced adversarial training techniques, the model
enhances the resistance of edge cloud AI systems to security threats such as
data leakage and model poisoning. Experimental results show that the proposed
method is 15% better than the traditional federated learning method in terms of
data protection and model robustness.

</details>


### [62] [Optimizing Resource Allocation and Energy Efficiency in Federated Fog Computing for IoT](https://arxiv.org/abs/2506.18100)
*Taimoor Ahmad,Anas Ali*

Main category: cs.CR

TL;DR: 提出了一种基于多层机器学习的框架，用于智能检测物联网网络中的ARP欺骗攻击，显著提高了检测准确率和降低了误报率。


<details>
  <summary>Details</summary>
Motivation: 传统检测方法在物联网网络中效果不佳，误报率高且适应性差，亟需更智能的解决方案。

Method: 采用多层集成分类器框架，优化检测精度并减少误报，同时引入动态反馈机制进行模型重训练。

Result: 实验结果显示检测准确率高达97.5%，误报率低于2%，且检测速度更快。

Conclusion: 该研究为物联网网络提供了更可靠的ARP欺骗防御方案，提升了安全性和信任度。

Abstract: Address Resolution Protocol (ARP) spoofing attacks severely threaten Internet
of Things (IoT) networks by allowing attackers to intercept, modify, or block
communications. Traditional detection methods are insufficient due to high
false positives and poor adaptability. This research proposes a multi-layered
machine learning-based framework for intelligently detecting ARP spoofing in
IoT networks. Our approach utilizes an ensemble of classifiers organized into
multiple layers, each layer optimizing detection accuracy and reducing false
alarms. Experimental evaluations demonstrate significant improvements in
detection accuracy (up to 97.5\%), reduced false positive rates (less than
2\%), and faster detection time compared to existing methods. Our key
contributions include introducing multi-layer ensemble classifiers specifically
tuned for IoT networks, systematically addressing dataset imbalance problems,
introducing a dynamic feedback mechanism for classifier retraining, and
validating practical applicability through extensive simulations. This research
enhances security management in IoT deployments, providing robust defenses
against ARP spoofing attacks and improving reliability and trust in IoT
environments.

</details>


### [63] [Dynamic Temporal Positional Encodings for Early Intrusion Detection in IoT](https://arxiv.org/abs/2506.18114)
*Ioannis Panopoulos,Maria-Lamprini A. Bartsioka,Sokratis Nikolaidis,Stylianos I. Venieris,Dimitra I. Kaklamani,Iakovos S. Venieris*

Main category: cs.CR

TL;DR: 提出了一种基于Transformer的早期入侵检测系统（EIDS），通过动态时间位置编码和数据增强提升检测准确性和实时性。


<details>
  <summary>Details</summary>
Motivation: 物联网（IoT）的快速发展带来了安全挑战，传统入侵检测系统（IDS）忽视网络流量的时间特性，限制了早期威胁检测的效果。

Method: 采用Transformer架构，结合动态时间位置编码和数据增强技术，利用网络流时间戳捕捉序列结构和时间异常。

Result: 在CICIoT2023数据集上表现优于现有模型，同时在资源受限的IoT设备上实现低延迟和低内存占用。

Conclusion: EIDS在准确性和实时性上均表现出色，适用于资源受限的IoT环境。

Abstract: The rapid expansion of the Internet of Things (IoT) has introduced
significant security challenges, necessitating efficient and adaptive Intrusion
Detection Systems (IDS). Traditional IDS models often overlook the temporal
characteristics of network traffic, limiting their effectiveness in early
threat detection. We propose a Transformer-based Early Intrusion Detection
System (EIDS) that incorporates dynamic temporal positional encodings to
enhance detection accuracy while maintaining computational efficiency. By
leveraging network flow timestamps, our approach captures both sequence
structure and timing irregularities indicative of malicious behaviour.
Additionally, we introduce a data augmentation pipeline to improve model
robustness. Evaluated on the CICIoT2023 dataset, our method outperforms
existing models in both accuracy and earliness. We further demonstrate its
real-time feasibility on resource-constrained IoT devices, achieving
low-latency inference and minimal memory footprint.

</details>


### [64] [HE-LRM: Encrypted Deep Learning Recommendation Models using Fully Homomorphic Encryption](https://arxiv.org/abs/2506.18150)
*Karthik Garimella,Austin Ebel,Gabrielle De Micheli,Brandon Reagen*

Main category: cs.CR

TL;DR: 本文探讨了全同态加密（FHE）在深度学习推荐模型（DLRM）中的应用挑战与机遇，提出了压缩嵌入查找和多嵌入打包策略，显著降低了计算成本，并实现了端到端加密推荐系统HE-LRM。


<details>
  <summary>Details</summary>
Motivation: 尽管FHE在隐私保护神经网络推理中具有潜力，但其在稀疏特征网络（如DLRM）中的应用仍面临计算和内存限制的挑战。本文旨在解决这些问题。

Method: 开发了压缩嵌入查找方法以减少FHE计算成本，并提出了多嵌入打包策略。这些方法被集成到开源框架Orion中，构建了HE-LRM。

Result: 压缩嵌入查找比现有方法提升了77倍，实现了4400万参数嵌入查找。HE-LRM在UCI和Criteo数据集上验证了其可行性。

Conclusion: 通过适当的压缩和打包策略，加密推理在推荐系统中是可行的，为隐私保护推荐系统提供了新思路。

Abstract: Fully Homomorphic Encryption (FHE) is an encryption scheme that not only
encrypts data but also allows for computations to be applied directly on the
encrypted data. While computationally expensive, FHE can enable
privacy-preserving neural inference in the client-server setting: a client
encrypts their input with FHE and sends it to an untrusted server. The server
then runs neural inference on the encrypted data and returns the encrypted
results. The client decrypts the output locally, keeping both the input and
result private from the server. Private inference has focused on networks with
dense inputs such as image classification, and less attention has been given to
networks with sparse features. Unlike dense inputs, sparse features require
efficient encrypted lookup operations into large embedding tables, which
present computational and memory constraints for FHE.
  In this paper, we explore the challenges and opportunities when applying FHE
to Deep Learning Recommendation Models (DLRM) from both a compiler and systems
perspective. DLRMs utilize conventional MLPs for dense features and embedding
tables to map sparse, categorical features to dense vector representations. We
develop novel methods for performing compressed embedding lookups in order to
reduce FHE computational costs while keeping the underlying model performant.
Our embedding lookup improves upon a state-of-the-art approach by $77 \times$.
Furthermore, we present an efficient multi-embedding packing strategy that
enables us to perform a 44 million parameter embedding lookup under FHE.
Finally, we integrate our solutions into the open-source Orion framework and
present HE-LRM, an end-to-end encrypted DLRM. We evaluate HE-LRM on UCI (health
prediction) and Criteo (click prediction), demonstrating that with the right
compression and packing strategies, encrypted inference for recommendation
systems is practical.

</details>


### [65] [Smart-LLaMA-DPO: Reinforced Large Language Model for Explainable Smart Contract Vulnerability Detection](https://arxiv.org/abs/2506.18245)
*Lei Yu,Zhirong Huang,Hang Yuan,Shiqi Cheng,Li Yang,Fengjun Zhang,Chenjie Shen,Jiajia Ma,Jingyuan Zhang,Junyi Lu,Chun Zuo*

Main category: cs.CR

TL;DR: 论文提出Smart-LLaMA-DPO方法，基于LLaMA-3.1-8B，通过构建全面数据集和优化训练流程，显著提升智能合约漏洞检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有智能合约漏洞检测方法存在数据集不全面和大型语言模型（LLMs）解释不准确的问题。

Method: 构建全面数据集，进行持续预训练（CPT）和监督微调（SFT），并应用直接偏好优化（DPO）。

Result: 在F1分数和准确率上分别平均提升10.43%和7.87%，生成更正确、全面和清晰的解释。

Conclusion: Smart-LLaMA-DPO显著优于现有方法，解决了数据集和模型解释的局限性。

Abstract: Smart contract vulnerability detection remains a major challenge in
blockchain security. Existing vulnerability detection methods face two main
issues: (1) Existing datasets lack comprehensive coverage and high-quality
explanations for preference learning. (2) Large language models (LLMs) often
struggle with accurately interpreting specific concepts in smart contract
security. Empirical analysis shows that even after continual pre-training (CPT)
and supervised fine-tuning (SFT), LLMs may misinterpret the execution order of
state changes, resulting in incorrect explanations despite making correct
detection decisions. To address these challenges, we propose Smart-LLaMA-DPO
based on LLaMA-3.1-8B. We construct a comprehensive dataset covering four major
vulnerability types and machine-unauditable vulnerabilities, including precise
labels, explanations, and locations for SFT, as well as high-quality and
low-quality output pairs for Direct Preference Optimization (DPO). Second, we
perform CPT using large-scale smart contract to enhance the LLM's understanding
of specific security practices in smart contracts. Futhermore, we conduct SFT
with our comprehensive dataset. Finally, we apply DPO, leveraging human
feedback and a specially designed loss function that increases the probability
of preferred explanations while reducing the likelihood of non-preferred
outputs. We evaluate Smart-LLaMA-DPO on four major vulnerability types:
reentrancy, timestamp dependence, integer overflow/underflow, and delegatecall,
as well as machine-unauditable vulnerabilities. Our method significantly
outperforms state-of-the-art baselines, with average improvements of 10.43% in
F1 score and 7.87% in accuracy. Moreover, both LLM evaluation and human
evaluation confirm that our method generates more correct, thorough, and clear
explanations.

</details>


### [66] [SoK: Current State of Ethereum's Enshrined Proposer Builder Separation](https://arxiv.org/abs/2506.18189)
*Maxwell Koegler*

Main category: cs.CR

TL;DR: 论文探讨了以太坊中的Proposer-Builder Separation（PBS）机制，分析了其当前通过MEV-boost实现的方式，以及社区对将其纳入协议（ePBS）的呼声，同时讨论了潜在的好处和风险。


<details>
  <summary>Details</summary>
Motivation: 研究PBS的动机在于解决当前MEV-boost的未监管和依赖中继的问题，同时探索如何通过协议集成（ePBS）实现MEV缓解、降低验证者成本，并满足Web3社区对审查抵抗和奖励公平的需求。

Method: 论文通过系统化知识（SoK）方法，梳理现有PBS机制，分析ePBS升级的必要性，并探讨其可能的社会经济影响。

Result: 研究指出ePBS可能带来MEV缓解和成本降低，但也存在多方共谋和链停滞的风险。

Conclusion: 结论强调需要权衡ePBS的利弊，同时满足Web3的核心价值观，如审查抵抗和奖励公平。

Abstract: Initially introduced to Ethereum via Flashbots' MEV-boost, Proposer-Builder
Separation allows proposers to auction off blockspace to a market of
transaction orderers, known as builders. PBS is currently available to
validators through the aforementioned MEV-boost, but its unregulated and
relay-dependent nature has much of the Ethereum community calling for its
enshrinement. Providing a protocol-integrated PBS marketspace and communication
channel for payload outsourcing is termed PBS enshrinement. Although ePBS
potentially introduces native MEV mitigation mechanisms and reduces validator
operation costs, fears of multiparty collusion and chain stagnation are all too
real. In addition to mitigating these potential drawbacks, PBS research pursues
many tenets revered by Web3 enthusiasts, including but not limited to,
censorship resistance, validator reward equity, and deflationary finance. The
subsequent SoK will identify current PBS mechanisms, the need for enshrinement,
additions to the ePBS upgrade, and the existing or potential on-chain
socioeconomic implications of each.

</details>


### [67] [Automatic Selection of Protections to Mitigate Risks Against Software Applications](https://arxiv.org/abs/2506.18470)
*Daniele Canavese,Leonardo Regano,Bjorn De Sutter,Cataldo Basile*

Main category: cs.CR

TL;DR: 提出了一种基于博弈论的自动化软件保护选择方法，通过软件保护指数优化防御策略，平衡安全性与性能开销。


<details>
  <summary>Details</summary>
Motivation: 解决软件中关键资产面临的MATE风险，通过自动化保护选择提升防御效率。

Method: 使用博弈论模型，结合启发式深度优先探索和动态规划优化，提出软件保护指数评估保护效果。

Result: 通过概念验证和专家评估，证明该方法能有效降低风险并保持软件可用性。

Conclusion: 自动化软件保护选择是实用且有效的风险缓解方案。

Abstract: This paper introduces a novel approach for the automated selection of
software protections to mitigate MATE risks against critical assets within
software applications. We formalize the key elements involved in protection
decision-making - including code artifacts, assets, security requirements,
attacks, and software protections - and frame the protection process through a
game-theoretic model. In this model, a defender strategically applies
protections to various code artifacts of a target application, anticipating
repeated attack attempts by adversaries against the confidentiality and
integrity of the application's assets. The selection of the optimal defense
maximizes resistance to attacks while ensuring the application remains usable
by constraining the overhead introduced by protections. The game is solved
through a heuristic based on a mini-max depth-first exploration strategy,
augmented with dynamic programming optimizations for improved efficiency.
Central to our formulation is the introduction of the Software Protection
Index, an original contribution that extends existing notions of potency and
resilience by evaluating protection effectiveness against attack paths using
software metrics and expert assessments. We validate our approach through a
proof-of-concept implementation and expert evaluations, demonstrating that
automated software protection is a practical and effective solution for risk
mitigation in software.

</details>


### [68] [Shrinking the Generation-Verification Gap with Weak Verifiers](https://arxiv.org/abs/2506.18203)
*Jon Saad-Falcon,E. Kelly Buchanan,Mayee F. Chen,Tzu-Heng Huang,Brendan McLaughlin,Tanvir Bhathal,Shang Zhu,Ben Athiwaratkun,Frederic Sala,Scott Linderman,Azalia Mirhoseini,Christopher Ré*

Main category: cs.CR

TL;DR: Weaver框架通过结合多个弱验证器，显著提升了语言模型生成候选响应的选择准确性，减少了标记数据的依赖。


<details>
  <summary>Details</summary>
Motivation: 当前高质量的验证器要么不可扩展（如人工），要么实用性有限（如Lean工具），而语言模型验证器与完美验证器之间存在性能差距。

Method: Weaver通过加权集成多个弱验证器，利用弱监督估计验证器准确性，并统一输出格式，提升验证效果。

Result: Weaver在数学和推理任务中显著提升了Pass@1性能，达到了87.7%的平均准确率。

Conclusion: Weaver通过集成弱验证器和弱监督方法，有效缩小了语言模型验证器与完美验证器之间的性能差距。

Abstract: Verifiers can improve language model capabilities by scoring and ranking
responses from generated candidates. Currently, high-quality verifiers are
either unscalable (e.g., humans) or limited in utility (e.g., tools like Lean).
While LM judges and reward models have become broadly useful as general-purpose
verifiers, a significant performance gap remains between them and oracle
verifiers (verifiers with perfect accuracy). To help close this gap, we
introduce Weaver, a framework for designing a strong verifier by combining
multiple weak, imperfect verifiers. We find weighted ensembles of verifiers,
which typically require learning from labeled data, significantly outperform
unweighted combinations due to differences in verifier accuracies. To reduce
dependency on labeled data, Weaver leverages weak supervision to estimate each
verifier's accuracy and combines outputs into a unified score that better
reflects true response quality. However, directly applying weak supervision
algorithms poses challenges, including inconsistent verifier output formats and
handling low-quality verifiers. Weaver addresses these using dataset statistics
to normalize outputs and filter specific verifiers. We study Weaver's
effectiveness in test-time repeated sampling, where a model generates multiple
candidate responses and selects one. Our evaluations show Weaver significantly
improves over Pass@1-performance when selecting the first candidate-across
reasoning and math tasks, achieving o3-mini-level accuracy with Llama 3.3 70B
Instruct as generator, and an ensemble of 70B or smaller judge and reward
models as verifiers (87.7% average). This gain mirrors the jump between GPT-4o
and o3-mini (69.0% vs. 86.7%), which required extensive finetuning and
post-training. To reduce computational costs of verifier ensembles, we train a
400M cross-encoder using Weaver's combined output scores.

</details>


### [69] [FORGE: An LLM-driven Framework for Large-Scale Smart Contract Vulnerability Dataset Construction](https://arxiv.org/abs/2506.18795)
*Jiachi Chen,Yiming Shen,Jiashuo Zhang,Zihao Li,John Grundy,Zhenzhe Shao,Yanlin Wang,Jiashui Wang,Ting Chen,Zibin Zheng*

Main category: cs.CR

TL;DR: FORGE是一种自动化构建智能合约漏洞数据集的方法，解决了人工标注的低效和不一致问题，利用LLM驱动流程从审计报告中提取漏洞并分类，生成高质量数据集。


<details>
  <summary>Details</summary>
Motivation: 当前人工构建智能合约漏洞数据集存在劳动密集、错误率高以及分类标准不统一的问题，限制了数据集的规模和质量。

Method: FORGE采用分治策略从审计报告中提取结构化漏洞信息，并利用树状思维技术将其分类到CWE标准中。

Result: 在6,454份审计报告上运行FORGE，生成了81,390个Solidity文件和27,497个漏洞发现，覆盖296个CWE类别，提取精度达95.6%。

Conclusion: FORGE生成的数据集质量高，揭示了现有安全工具的局限性，并指出研究重点与实际漏洞分布的不一致。

Abstract: High-quality smart contract vulnerability datasets are critical for
evaluating security tools and advancing smart contract security research. Two
major limitations of current manual dataset construction are (1)
labor-intensive and error-prone annotation processes limiting the scale,
quality, and evolution of the dataset, and (2) absence of standardized
classification rules results in inconsistent vulnerability categories and
labeling results across different datasets. To address these limitations, we
present FORGE, the first automated approach for constructing smart contract
vulnerability datasets. FORGE leverages an LLM-driven pipeline to extract
high-quality vulnerabilities from real-world audit reports and classify them
according to the CWE, the most widely recognized classification in software
security. FORGE employs a divide-and-conquer strategy to extract structured and
self-contained vulnerability information from these reports. Additionally, it
uses a tree-of-thoughts technique to classify the vulnerability information
into the hierarchical CWE classification. To evaluate FORGE's effectiveness, we
run FORGE on 6,454 real-world audit reports and generate a dataset comprising
81,390 solidity files and 27,497 vulnerability findings across 296 CWE
categories. Manual assessment of the dataset demonstrates high extraction
precision and classification consistency with human experts (precision of 95.6%
and inter-rater agreement k-$\alpha$ of 0.87). We further validate the
practicality of our dataset by benchmarking 13 existing security tools on our
dataset. The results reveal the significant limitations in current detection
capabilities. Furthermore, by analyzing the severity-frequency distribution
patterns through a unified CWE perspective in our dataset, we highlight
inconsistency between current smart contract research focus and priorities
identified from real-world vulnerabilities...

</details>


### [70] [Adaptive alert prioritisation in security operations centres via learning to defer with human feedback](https://arxiv.org/abs/2506.18462)
*Fatemeh Jalalvand,Mohan Baruwal Chhetri,Surya Nepal,Cécile Paris*

Main category: cs.CR

TL;DR: 论文提出了一种基于人类反馈的自适应延迟框架L2DHF，通过深度强化学习优化延迟决策，显著提高了警报优先级排序的准确性并减少了分析师的工作负担。


<details>
  <summary>Details</summary>
Motivation: 解决传统静态延迟策略无法从人类反馈中学习和适应的问题，以提高安全操作中心（SOC）的效率。

Method: 采用深度强化学习从人类反馈中学习（DRLHF），动态优化延迟决策。

Result: 在UNSW-NB15和CICIDS2017数据集上，L2DHF显著提高了关键警报的准确性（13-16%和60-67%），并减少了误分类（如CICIDS2017上高类别警报减少98%）和延迟（如UNSW-NB15上减少37%）。

Conclusion: L2DHF是一种高效且实用的方法，能够显著提升SOC的警报优先级排序效果并减轻分析师负担。

Abstract: Alert prioritisation (AP) is crucial for security operations centres (SOCs)
to manage the overwhelming volume of alerts and ensure timely detection and
response to genuine threats, while minimising alert fatigue. Although
predictive AI can process large alert volumes and identify known patterns, it
struggles with novel and evolving scenarios that demand contextual
understanding and nuanced judgement. A promising solution is Human-AI teaming
(HAT), which combines human expertise with AI's computational capabilities.
Learning to Defer (L2D) operationalises HAT by enabling AI to "defer" uncertain
or unfamiliar cases to human experts. However, traditional L2D models rely on
static deferral policies that do not evolve with experience, limiting their
ability to learn from human feedback and adapt over time. To overcome this, we
introduce Learning to Defer with Human Feedback (L2DHF), an adaptive deferral
framework that leverages Deep Reinforcement Learning from Human Feedback
(DRLHF) to optimise deferral decisions. By dynamically incorporating human
feedback, L2DHF continuously improves AP accuracy and reduces unnecessary
deferrals, enhancing SOC effectiveness and easing analyst workload. Experiments
on two widely used benchmark datasets, UNSW-NB15 and CICIDS2017, demonstrate
that L2DHF significantly outperforms baseline models. Notably, it achieves
13-16% higher AP accuracy for critical alerts on UNSW-NB15 and 60-67% on
CICIDS2017. It also reduces misprioritisations, for example, by 98% for
high-category alerts on CICIDS2017. Moreover, L2DHF decreases deferrals, for
example, by 37% on UNSW-NB15, directly reducing analyst workload. These gains
are achieved with efficient execution, underscoring L2DHF's practicality for
real-world SOC deployment.

</details>


### [71] [DUMB and DUMBer: Is Adversarial Training Worth It in the Real World?](https://arxiv.org/abs/2506.18516)
*Francesco Marchiori,Marco Alecci,Luca Pajola,Mauro Conti*

Main category: cs.CR

TL;DR: 论文提出DUMBer框架，系统评估对抗训练模型的鲁棒性，发现其在不同模型和数据集中的表现差异，为实践提供实用建议。


<details>
  <summary>Details</summary>
Motivation: 对抗样本威胁深度学习模型的可靠性，尤其是对抗训练的泛化性不足，限制了实际应用。

Method: 基于DUMB方法构建DUMBer框架，评估多种对抗训练技术，覆盖13种攻击算法和3个计算机视觉任务。

Result: 实验发现对抗训练的效果因模型、数据集和攻击设置而异，提供了具体防御建议。

Conclusion: DUMBer框架揭示了对抗训练的局限性，为实际部署提供了针对性指导。

Abstract: Adversarial examples are small and often imperceptible perturbations crafted
to fool machine learning models. These attacks seriously threaten the
reliability of deep neural networks, especially in security-sensitive domains.
Evasion attacks, a form of adversarial attack where input is modified at test
time to cause misclassification, are particularly insidious due to their
transferability: adversarial examples crafted against one model often fool
other models as well. This property, known as adversarial transferability,
complicates defense strategies since it enables black-box attacks to succeed
without direct access to the victim model. While adversarial training is one of
the most widely adopted defense mechanisms, its effectiveness is typically
evaluated on a narrow and homogeneous population of models. This limitation
hinders the generalizability of empirical findings and restricts practical
adoption.
  In this work, we introduce DUMBer, an attack framework built on the
foundation of the DUMB (Dataset soUrces, Model architecture, and Balance)
methodology, to systematically evaluate the resilience of adversarially trained
models. Our testbed spans multiple adversarial training techniques evaluated
across three diverse computer vision tasks, using a heterogeneous population of
uniquely trained models to reflect real-world deployment variability. Our
experimental pipeline comprises over 130k evaluations spanning 13
state-of-the-art attack algorithms, allowing us to capture nuanced behaviors of
adversarial training under varying threat models and dataset conditions. Our
findings offer practical, actionable insights for AI practitioners, identifying
which defenses are most effective based on the model, dataset, and attacker
setup.

</details>


### [72] [Security Assessment of DeepSeek and GPT Series Models against Jailbreak Attacks](https://arxiv.org/abs/2506.18543)
*Xiaodong Wu,Xiangman Li,Jianbing Ni*

Main category: cs.CR

TL;DR: 本文首次系统评估了DeepSeek系列模型在对抗性提示攻击（jailbreak）下的鲁棒性，发现其MoE架构在优化攻击中表现较好，但在提示攻击中更脆弱，而GPT-4 Turbo因密集架构和人类反馈强化学习表现更优。


<details>
  <summary>Details</summary>
Motivation: 随着开源大模型（如DeepSeek）的广泛应用，其对抗性攻击的鲁棒性尚未充分研究，而专有模型（如GPT-4）已广泛评估。本文旨在填补这一空白。

Method: 使用HarmBench基准评估了7种攻击策略，覆盖510种有害行为，比较DeepSeek与GPT-3.5、GPT-4的表现。

Result: DeepSeek的MoE架构在优化攻击（如TAP-T）中表现较好，但在提示攻击中更脆弱；GPT-4 Turbo因密集架构和人类反馈强化学习表现更优。

Conclusion: 开源大模型需针对性安全优化和模块化对齐策略，以平衡架构效率与安全性。

Abstract: The widespread deployment of large language models (LLMs) has raised critical
concerns over their vulnerability to jailbreak attacks, i.e., adversarial
prompts that bypass alignment mechanisms and elicit harmful or policy-violating
outputs. While proprietary models like GPT-4 have undergone extensive
evaluation, the robustness of emerging open-source alternatives such as
DeepSeek remains largely underexplored, despite their growing adoption in
real-world applications. In this paper, we present the first systematic
jailbreak evaluation of DeepSeek-series models, comparing them with GPT-3.5 and
GPT-4 using the HarmBench benchmark. We evaluate seven representative attack
strategies across 510 harmful behaviors categorized by both function and
semantic domain. Our analysis reveals that DeepSeek's Mixture-of-Experts (MoE)
architecture introduces routing sparsity that offers selective robustness
against optimization-based attacks such as TAP-T, but leads to significantly
higher vulnerability under prompt-based and manually engineered attacks. In
contrast, GPT-4 Turbo demonstrates stronger and more consistent safety
alignment across diverse behaviors, likely due to its dense Transformer design
and reinforcement learning from human feedback. Fine-grained behavioral
analysis and case studies further show that DeepSeek often routes adversarial
prompts to under-aligned expert modules, resulting in inconsistent refusal
behaviors. These findings highlight a fundamental trade-off between
architectural efficiency and alignment generalization, emphasizing the need for
targeted safety tuning and modular alignment strategies to ensure secure
deployment of open-source LLMs.

</details>


### [73] [Understanding the Theoretical Guarantees of DPM](https://arxiv.org/abs/2506.18685)
*Yara Schütt,Esfandiar Mohammadi*

Main category: cs.CR

TL;DR: 本文深入分析了差分隐私机制（DPM）的效用，扩展了停止准则的分析，并揭示了其对实际输入分布的约束。研究发现，即使使用最优DPM分割，聚类轮廓分数仍可能下降，质疑其作为聚类指标的适用性。此外，研究将DPM与理论概念$(\xi, \rho)$-可分性联系起来，为理解DPM在任意输入下的行为提供了理论支持。


<details>
  <summary>Details</summary>
Motivation: 研究动机是扩展DPM的效用分析，特别是在实际输入分布下的表现，以更好地理解其行为和适用性。

Method: 方法包括对DPM停止准则的扩展分析，结合聚类轮廓分数评估其效用，并将其与$(\xi, \rho)$-可分性理论概念联系起来。

Result: 研究发现DPM的最小聚类规模和评分函数权重存在约束，且即使使用最优分割，轮廓分数仍可能下降。此外，DPM的理论保证为分析不同超参数和数据集的影响提供了基础。

Conclusion: 结论是DPM的理论分析有助于理解其在未知设置和数据集中的行为，但其效用可能受限于某些指标（如轮廓分数）的适用性。

Abstract: In this study, we conducted an in-depth examination of the utility analysis
of the differentially private mechanism (DPM). The authors of DPM have already
established the probability of a good split being selected and of DPM halting.
In this study, we expanded the analysis of the stopping criterion and provided
an interpretation of these guarantees in the context of realistic input
distributions. Our findings revealed constraints on the minimum cluster size
and the metric weight for the scoring function. Furthermore, we introduced an
interpretation of the utility of DPM through the lens of the clustering metric,
the silhouette score. Our findings indicate that even when an optimal DPM-based
split is employed, the silhouette score of the resulting clustering may still
decline. This observation calls into question the suitability of the silhouette
score as a clustering metric. Finally, we examined the potential of the
underlying concept of DPM by linking it to a more theoretical view, that of
$(\xi, \rho)$-separability. This extensive analysis of the theoretical
guarantees of DPM allows a better understanding of its behaviour for arbitrary
inputs. From these guarantees, we can analyse the impact of different
hyperparameters and different input data sets, thereby promoting the
application of DPM in practice for unknown settings and data sets.

</details>


### [74] [Vulnerability Assessment Combining CVSS Temporal Metrics and Bayesian Networks](https://arxiv.org/abs/2506.18715)
*Stefano Perone,Simone Guarino,Luca Faramondi,Roberto Setola*

Main category: cs.CR

TL;DR: 本文提出了一种创新的漏洞评估方法，通过将时间维度与贝叶斯网络结合，改进了漏洞评估和优先级排序。


<details>
  <summary>Details</summary>
Motivation: 现有文献忽视了漏洞评估中的时间维度，而工业环境中的网络安全迫切需要更精确的评估方法。

Method: 结合CVSS时间指标与贝叶斯网络，动态计算漏洞的时间分数并更新CVSS基础分数。

Result: 通过概率建模，实现了更准确的漏洞优先级排序和决策支持。

Conclusion: 该方法为工业环境中的漏洞评估提供了一种动态且自适应的解决方案。

Abstract: Vulnerability assessment is a critical challenge in cybersecurity,
particularly in industrial environments. This work presents an innovative
approach by incorporating the temporal dimension into vulnerability assessment,
an aspect neglected in existing literature. Specifically, this paper focuses on
refining vulnerability assessment and prioritization by integrating Common
Vulnerability Scoring System (CVSS) Temporal Metrics with Bayesian Networks to
account for exploit availability, remediation efforts, and confidence in
reported vulnerabilities. Through probabilistic modeling, Bayesian networks
enable a structured and adaptive evaluation of vulnerabilities, allowing for
more accurate prioritization and decision-making. The proposed approach
dynamically computes the Temporal Score and updates the CVSS Base Score by
processing data on exploits and fixes from vulnerability databases.

</details>


### [75] [Physical Layer Challenge-Response Authentication between Ambient Backscatter Devices](https://arxiv.org/abs/2506.18767)
*Yifan Zhang,Yongchao Dang,Masoud Kaveh,Zheng Yan,Riku Jäntti,Zhu Han*

Main category: cs.CR

TL;DR: 提出了一种新型物理层挑战-响应认证方案PLCRA-BD，适用于环境反向散射通信（AmBC）中的资源受限设备，支持高移动性并抵御多种无线攻击。


<details>
  <summary>Details</summary>
Motivation: 现有认证方法因计算需求高无法在资源受限的反向散射设备（BDs）间实现，而AmBC系统的开放无线环境易受攻击。

Method: 通过物理层指纹轻量级识别，设计联合收发器以减少环境RF信号干扰，并利用信道相干性和随机数实现低复杂度认证。

Result: 安全分析表明其能抵御多种攻击，仿真验证了其在资源受限BDs中的高效性和鲁棒性。

Conclusion: PLCRA-BD方案在资源受限和高移动性场景下表现优异，优于传统认证方法。

Abstract: Ambient backscatter communication (AmBC) has become an integral part of
ubiquitous Internet of Things (IoT) applications due to its energy-harvesting
capabilities and ultra-low-power consumption. However, the open wireless
environment exposes AmBC systems to various attacks, and existing
authentication methods cannot be implemented between resource-constrained
backscatter devices (BDs) due to their high computational demands.To this end,
this paper proposes PLCRA-BD, a novel physical layer challenge-response
authentication scheme between BDs in AmBC that overcomes BDs' limitations,
supports high mobility, and performs robustly against impersonation and
wireless attacks. It constructs embedded keys as physical layer fingerprints
for lightweight identification and designs a joint transceiver that integrates
BDs' backscatter waveform with receiver functionality to mitigate interference
from ambient RF signals by exploiting repeated patterns in OFDM symbols. Based
on this, a challenge-response authentication procedure is introduced to enable
low-complexity fingerprint exchange between two paired BDs leveraging channel
coherence, while securing the exchange process using a random number and
unpredictable channel fading. Additionally, we optimize the authentication
procedure for high-mobility scenarios, completing exchanges within the channel
coherence time to minimize the impact of dynamic channel fluctuations. Security
analysis confirms its resistance against impersonation, eavesdropping, replay,
and counterfeiting attacks. Extensive simulations validate its effectiveness in
resource-constrained BDs, demonstrating high authentication accuracy across
diverse channel conditions, robustness against multiple wireless attacks, and
superior efficiency compared to traditional authentication schemes.

</details>


### [76] [Design high-confidence computers using trusted instructional set architecture and emulators](https://arxiv.org/abs/2506.18780)
*Shuangbao Paul Wang*

Main category: cs.CR

TL;DR: 论文提出了一种全面的可信计算机架构设计与仿真方法，以解决现代处理器因Spectre和Meltdown漏洞而面临的安全问题。


<details>
  <summary>Details</summary>
Motivation: 现代处理器因分支预测和流水线技术而存在Spectre和Meltdown漏洞，现有软件补丁无法根本解决问题，需要新的架构设计方法。

Method: 采用全面的可信计算机架构设计与仿真方法。

Result: 提出了一种新的架构设计方法，旨在从根本上解决处理器漏洞问题。

Conclusion: 通过全面的架构设计与仿真，可以有效提升处理器的安全性，同时保持性能。

Abstract: High-confidence computing relies on trusted instructional set architecture,
sealed kernels, and secure operating systems. Cloud computing depends on
trusted systems for virtualization tasks. Branch predictions and pipelines are
essential in improving performance of a CPU/GPU. But Spectre and Meltdown make
modern processors vulnerable to be exploited. Disabling the prediction and
pipeline is definitely not a good solution. On the other hand, current software
patches can only address non-essential issues around Meltdown. This paper
introduces a holistic approach in trusted computer architecture design and
emulation.

</details>


### [77] [Cellular Automata as Generators of Interleaving Sequences](https://arxiv.org/abs/2506.18848)
*Sara D. Cardell*

Main category: cs.CR

TL;DR: 本文提出两种一维细胞自动机作为交织序列的生成器，填补了现有文献中细胞自动机与交织序列生成之间联系的空白。


<details>
  <summary>Details</summary>
Motivation: 探索细胞自动机生成交织序列的能力，填补现有文献中细胞自动机与交织序列生成之间联系的空白。

Method: 提出两种一维细胞自动机作为交织序列的生成器。

Result: 展示了细胞自动机生成交织序列的潜力。

Conclusion: 研究为细胞自动机和交织序列生成之间的联系提供了新视角，促进了对这两个领域的深入理解。

Abstract: An interleaving sequence is obtained by combining or intertwining elements
from two or more sequences. On the other hand, cellular automata are known to
be generators for keystream sequences. In this paper we present two families of
one-dimensional cellular automata as generators of interleaving sequences. This
study aims to close a notable gap within the current body of literature by
exploring the capacity of cellular automata to generate interleaving sequences.
While previous works have separately examined cellular automata as sequence
generators and interleaving sequences, there exists limited literature
interconnecting these two topics. Our study seeks to bridge this gap, providing
perspectives on the generation of interleaving sequences through the
utilisation of cellular automata, thereby fostering a deeper understanding of
both disciplines.

</details>


### [78] [Amplifying Machine Learning Attacks Through Strategic Compositions](https://arxiv.org/abs/2506.18870)
*Yugeng Liu,Zheng Li,Hai Huang,Michael Backes,Yang Zhang*

Main category: cs.CR

TL;DR: 研究了机器学习模型中多种攻击策略的组合效应，提出了攻击组合的分类法，并通过实验验证了四种有效组合。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注单一攻击，而实际中攻击者可能同时使用多种策略，因此需要研究攻击间的相互作用。

Method: 提出基于攻击流程三阶段（准备、执行、评估）的分类法，并实验验证四种攻击组合的有效性。

Result: 实验证明四种攻击组合在三种模型架构和数据集上均有效。

Conclusion: 呼吁研究者和从业者考虑多攻击策略的高级对抗场景，以增强AI系统的安全性和鲁棒性。

Abstract: Machine learning (ML) models are proving to be vulnerable to a variety of
attacks that allow the adversary to learn sensitive information, cause
mispredictions, and more. While these attacks have been extensively studied,
current research predominantly focuses on analyzing each attack type
individually. In practice, however, adversaries may employ multiple attack
strategies simultaneously rather than relying on a single approach. This
prompts a crucial yet underexplored question: When the adversary has multiple
attacks at their disposal, are they able to mount or amplify the effect of one
attack with another? In this paper, we take the first step in studying the
strategic interactions among different attacks, which we define as attack
compositions. Specifically, we focus on four well-studied attacks during the
model's inference phase: adversarial examples, attribute inference, membership
inference, and property inference. To facilitate the study of their
interactions, we propose a taxonomy based on three stages of the attack
pipeline: preparation, execution, and evaluation. Using this taxonomy, we
identify four effective attack compositions, such as property inference
assisting attribute inference at its preparation level and adversarial examples
assisting property inference at its execution level. We conduct extensive
experiments on the attack compositions using three ML model architectures and
three benchmark image datasets. Empirical results demonstrate the effectiveness
of these four attack compositions. We implement and release a modular reusable
toolkit, COAT. Arguably, our work serves as a call for researchers and
practitioners to consider advanced adversarial settings involving multiple
attack strategies, aiming to strengthen the security and robustness of AI
systems.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [79] [Evaluating Generalization and Representation Stability in Small LMs via Prompting](https://arxiv.org/abs/2506.17289)
*Rahul Raja,Arpita Vats*

Main category: cs.AI

TL;DR: 比较小语言模型在少样本提示和监督微调下的泛化能力，分析其在不同任务和分布下的表现。


<details>
  <summary>Details</summary>
Motivation: 探究少样本提示和监督微调在低资源场景和分布变化下的鲁棒性差异。

Method: 通过比较提示和微调在不同任务、提示风格和模型规模下的表现，分析其内部表示。

Result: 发现两种方法在知识内化和泛化方面存在显著差异。

Conclusion: 为低数据场景下的模型选择提供指导，并提示与微调的对比提供实证依据。

Abstract: We investigate the generalization capabilities of small language models under
two popular adaptation paradigms: few-shot prompting and supervised
fine-tuning. While prompting is often favored for its parameter efficiency and
flexibility, it remains unclear how robust this approach is in low-resource
settings and under distributional shifts. This paper presents a comparative
study of prompting and fine-tuning across task formats, prompt styles, and
model scales, with a focus on their behavior in both in-distribution and
out-of-distribution (OOD) settings.
  Beyond accuracy, we analyze the internal representations learned by each
approach to assess the stability and abstraction of task-specific features. Our
findings highlight critical differences in how small models internalize and
generalize knowledge under different adaptation strategies. This work offers
practical guidance for model selection in low-data regimes and contributes
empirical insight into the ongoing debate over prompting versus fine-tuning.
Code for the experiments is available at the following

</details>


### [80] [Individual Causal Inference with Structural Causal Model](https://arxiv.org/abs/2506.17300)
*Daniel T. Chang*

Main category: cs.AI

TL;DR: 本文提出了一种基于结构因果模型（SCM）的个体因果推断（ICI）方法，通过个体化操作符（indiv(W)）和个体因果查询（P(Y | indiv(W), do(X), Z)）来估计个体因果效应（ICE）。


<details>
  <summary>Details</summary>
Motivation: 传统因果推断方法多为群体层面，难以直接应用于个体。本文旨在利用SCM中的外生变量（U）编码个体差异，实现个体化因果推断。

Method: 提出个体化操作符indiv(W)和个体因果查询P(Y | indiv(W), do(X), Z)，将SCM应用于个体层面。

Result: 论证了ICI是基于个体可能性的推断，而非反事实推断。

Conclusion: SCM为个体因果推断提供了理论框架，扩展了因果推断的应用范围。

Abstract: Individual causal inference (ICI) uses causal inference methods to understand
and predict the effects of interventions on individuals, considering their
specific characteristics / facts. It aims to estimate individual causal effect
(ICE), which varies across individuals. Estimating ICE can be challenging due
to the limited data available for individuals, and the fact that most causal
inference methods are population-based. Structural Causal Model (SCM) is
fundamentally population-based. Therefore, causal discovery (structural
learning and parameter learning), association queries and intervention queries
are all naturally population-based. However, exogenous variables (U) in SCM can
encode individual variations and thus provide the mechanism for individualized
population per specific individual characteristics / facts. Based on this, we
propose ICI with SCM as a "rung 3" causal inference, because it involves
"imagining" what would be the causal effect of a hypothetical intervention on
an individual, given the individual's observed characteristics / facts.
Specifically, we propose the indiv-operator, indiv(W), to formalize/represent
the population individualization process, and the individual causal query, P(Y
| indiv(W), do(X), Z), to formalize/represent ICI. We show and argue that ICI
with SCM is inference on individual alternatives (possible), not individual
counterfactuals (non-actual).

</details>


### [81] [Resource Rational Contractualism Should Guide AI Alignment](https://arxiv.org/abs/2506.17434)
*Sydney Levine,Matija Franklin,Tan Zhi-Xuan,Secil Yanik Guyot,Lionel Wong,Daniel Kilov,Yejin Choi,Joshua B. Tenenbaum,Noah Goodman,Seth Lazar,Iason Gabriel*

Main category: cs.AI

TL;DR: 提出了一种资源理性契约主义（RRC）框架，使AI系统能够通过启发式方法高效地模拟理性多方协议。


<details>
  <summary>Details</summary>
Motivation: 解决AI在多样化人类环境中决策时如何高效达成多方认可的协议问题。

Method: 利用规范基础和认知启发的启发式方法，权衡努力与准确性，模拟理性多方协议。

Result: RRC框架使AI系统能够高效运作并动态适应不断变化的人类社会。

Conclusion: RRC为AI系统提供了一种高效且适应性强的决策框架，适用于多样化的人类环境。

Abstract: AI systems will soon have to navigate human environments and make decisions
that affect people and other AI agents whose goals and values diverge.
Contractualist alignment proposes grounding those decisions in agreements that
diverse stakeholders would endorse under the right conditions, yet securing
such agreement at scale remains costly and slow -- even for advanced AI. We
therefore propose Resource-Rational Contractualism (RRC): a framework where AI
systems approximate the agreements rational parties would form by drawing on a
toolbox of normatively-grounded, cognitively-inspired heuristics that trade
effort for accuracy. An RRC-aligned agent would not only operate efficiently,
but also be equipped to dynamically adapt to and interpret the ever-changing
human social world.

</details>


### [82] [Keeping Medical AI Healthy: A Review of Detection and Correction Methods for System Degradation](https://arxiv.org/abs/2506.17442)
*Hao Guan,David Bates,Li Zhou*

Main category: cs.AI

TL;DR: 本文综述了医疗AI系统性能退化的原因、检测方法及修正策略，旨在提升其在动态临床环境中的长期可靠性。


<details>
  <summary>Details</summary>
Motivation: 医疗AI系统在现实环境中可能因数据分布变化、患者特征改变等因素导致性能退化，影响其可靠性和安全性。

Method: 回顾了性能退化的常见原因，总结了数据与模型漂移的检测技术，并探讨了从模型重训练到测试时适应的修正策略。

Result: 涵盖了传统机器学习模型和大型语言模型（LLMs），分析了其优缺点。

Conclusion: 提出了未来研究方向，旨在开发可靠、稳健的医疗AI系统，以适应动态临床环境。

Abstract: Artificial intelligence (AI) is increasingly integrated into modern
healthcare, offering powerful support for clinical decision-making. However, in
real-world settings, AI systems may experience performance degradation over
time, due to factors such as shifting data distributions, changes in patient
characteristics, evolving clinical protocols, and variations in data quality.
These factors can compromise model reliability, posing safety concerns and
increasing the likelihood of inaccurate predictions or adverse outcomes. This
review presents a forward-looking perspective on monitoring and maintaining the
"health" of AI systems in healthcare. We highlight the urgent need for
continuous performance monitoring, early degradation detection, and effective
self-correction mechanisms. The paper begins by reviewing common causes of
performance degradation at both data and model levels. We then summarize key
techniques for detecting data and model drift, followed by an in-depth look at
root cause analysis. Correction strategies are further reviewed, ranging from
model retraining to test-time adaptation. Our survey spans both traditional
machine learning models and state-of-the-art large language models (LLMs),
offering insights into their strengths and limitations. Finally, we discuss
ongoing technical challenges and propose future research directions. This work
aims to guide the development of reliable, robust medical AI systems capable of
sustaining safe, long-term deployment in dynamic clinical settings.

</details>


### [83] [OmniReflect: Discovering Transferable Constitutions for LLM agents via Neuro-Symbolic Reflections](https://arxiv.org/abs/2506.17449)
*Manasa Bharadwaj,Nikhil Verma,Kevin Ferreira*

Main category: cs.AI

TL;DR: OmniReflect是一个分层、反思驱动的框架，通过构建指导原则（constitution）提升LLM代理的性能，在动态环境中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如微调和自我修正）缺乏长期学习的通用机制，且在动态环境中效率低下。

Method: OmniReflect采用两种模式：自我维持（单个代理定期反思）和协作（元顾问从校准集生成指导原则）。结合神经、符号和神经符号技术。

Result: 实验显示任务成功率显著提升（ALFWorld +10.3%，BabyAI +23.8%，PDDL +8.3%）。协作模式下轻量级Qwen3-4B代理优于基线。

Conclusion: OmniReflect在多种环境和模型上表现出鲁棒性和高效性。

Abstract: Efforts to improve Large Language Model (LLM) agent performance on complex
tasks have largely focused on fine-tuning and iterative self-correction.
However, these approaches often lack generalizable mechanisms for longterm
learning and remain inefficient in dynamic environments. We introduce
OmniReflect, a hierarchical, reflection-driven framework that constructs a
constitution, a compact set of guiding principles distilled from task
experiences, to enhance the effectiveness and efficiency of an LLM agent.
OmniReflect operates in two modes: Self-sustaining, where a single agent
periodically curates its own reflections during task execution, and
Co-operative, where a Meta-advisor derives a constitution from a small
calibration set to guide another agent. To construct these constitutional
principles, we employ Neural, Symbolic, and NeuroSymbolic techniques, offering
a balance between contextual adaptability and computational efficiency.
Empirical results averaged across models show major improvements in task
success, with absolute gains of +10.3% on ALFWorld, +23.8% on BabyAI, and +8.3%
on PDDL in the Self-sustaining mode. Similar gains are seen in the Co-operative
mode, where a lightweight Qwen3-4B ReAct agent outperforms all Reflexion
baselines on BabyAI. These findings highlight the robustness and effectiveness
of OmniReflect across environments and backbones.

</details>


### [84] [From Unstructured Communication to Intelligent RAG: Multi-Agent Automation for Supply Chain Knowledge Bases](https://arxiv.org/abs/2506.17484)
*Yao Zhang,Zaixi Shang,Silpan Patel,Mikel Zuniga*

Main category: cs.AI

TL;DR: 论文提出了一种基于LLM的多智能体系统，将供应链支持票据转化为结构化知识库，显著提升RAG系统的性能。


<details>
  <summary>Details</summary>
Motivation: 供应链操作数据中的关键知识常埋没于非结构化通信中，现有RAG系统因数据质量问题效果有限。

Method: 采用离线优先方法，通过三个智能体（分类发现、分类、知识合成）将通信数据转化为结构化知识库。

Result: 实验显示，该方法将知识库体积缩减至3.4%，提升RAG系统性能（48.74% vs. 38.60%有用回答），减少77.4%无用回答。

Conclusion: 该方法通过离线处理将临时通信转化为结构化知识，显著提升供应链支持效率，自动解决约50%未来票据。

Abstract: Supply chain operations generate vast amounts of operational data; however,
critical knowledge such as system usage practices, troubleshooting workflows,
and resolution techniques often remains buried within unstructured
communications like support tickets, emails, and chat logs. While RAG systems
aim to leverage such communications as a knowledge base, their effectiveness is
limited by raw data challenges: support tickets are typically noisy,
inconsistent, and incomplete, making direct retrieval suboptimal. Unlike
existing RAG approaches that focus on runtime optimization, we introduce a
novel offline-first methodology that transforms these communications into a
structured knowledge base. Our key innovation is a LLMs-based multi-agent
system orchestrating three specialized agents: Category Discovery for taxonomy
creation, Categorization for ticket grouping, and Knowledge Synthesis for
article generation. Applying our methodology to real-world support tickets with
resolution notes and comments, our system creates a compact knowledge base -
reducing total volume to just 3.4% of original ticket data while improving
quality. Experiments demonstrate that our prebuilt knowledge base in RAG
systems significantly outperforms traditional RAG implementations (48.74% vs.
38.60% helpful answers) and achieves a 77.4% reduction in unhelpful responses.
By automating institutional knowledge capture that typically remains siloed in
experts' heads, our solution translates to substantial operational efficiency:
reducing support workload, accelerating resolution times, and creating
self-improving systems that automatically resolve approximately 50% of future
supply chain tickets. Our approach addresses a key gap in knowledge management
by transforming transient communications into structured, reusable knowledge
through intelligent offline processing rather than latency-inducing runtime
architectures.

</details>


### [85] [Kaleidoscopic Teaming in Multi Agent Simulations](https://arxiv.org/abs/2506.17514)
*Ninareh Mehrabi,Tharindu Kumarage,Kai-Wei Chang,Aram Galstyan,Rahul Gupta*

Main category: cs.AI

TL;DR: 论文提出了一种名为“万花筒式团队”的新框架，用于评估AI代理在单代理和多代理场景中的安全性，弥补现有安全评估方法的不足。


<details>
  <summary>Details</summary>
Motivation: 现有红队或安全评估框架无法全面评估AI代理在复杂行为、思维过程和交互中的安全风险，尤其是在多代理场景中。

Method: 引入“万花筒式团队”概念及框架，通过生成多样化场景模拟现实社会，评估单代理和多代理的安全性，并采用上下文优化技术改进场景生成。

Result: 通过框架识别了多种AI模型在代理用例中的安全漏洞。

Conclusion: 万花筒式团队框架为AI代理的安全性评估提供了更全面的方法，适用于复杂行为和多代理交互场景。

Abstract: Warning: This paper contains content that may be inappropriate or offensive.
  AI agents have gained significant recent attention due to their autonomous
tool usage capabilities and their integration in various real-world
applications. This autonomy poses novel challenges for the safety of such
systems, both in single- and multi-agent scenarios. We argue that existing red
teaming or safety evaluation frameworks fall short in evaluating safety risks
in complex behaviors, thought processes and actions taken by agents. Moreover,
they fail to consider risks in multi-agent setups where various vulnerabilities
can be exposed when agents engage in complex behaviors and interactions with
each other. To address this shortcoming, we introduce the term kaleidoscopic
teaming which seeks to capture complex and wide range of vulnerabilities that
can happen in agents both in single-agent and multi-agent scenarios. We also
present a new kaleidoscopic teaming framework that generates a diverse array of
scenarios modeling real-world human societies. Our framework evaluates safety
of agents in both single-agent and multi-agent setups. In single-agent setup,
an agent is given a scenario that it needs to complete using the tools it has
access to. In multi-agent setup, multiple agents either compete against or
cooperate together to complete a task in the scenario through which we capture
existing safety vulnerabilities in agents. We introduce new in-context
optimization techniques that can be used in our kaleidoscopic teaming framework
to generate better scenarios for safety analysis. Lastly, we present
appropriate metrics that can be used along with our framework to measure safety
of agents. Utilizing our kaleidoscopic teaming framework, we identify
vulnerabilities in various models with respect to their safety in agentic
use-cases.

</details>


### [86] [Cite Pretrain: Retrieval-Free Knowledge Attribution for Large Language Models](https://arxiv.org/abs/2506.17585)
*Yukun Huang,Sanxing Chen,Jian Pei,Manzil Zaheer,Bhuwan Dhingra*

Main category: cs.AI

TL;DR: 论文探讨如何通过改进训练过程使语言模型能够可靠地引用预训练文档，而无需依赖推理时的外部检索器，提出了一种名为Active Indexing的方法，显著提高了引用精度。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型的引用不可靠，依赖外部检索器会引入延迟和噪声，因此研究如何在预训练阶段实现可靠引用。

Method: 采用两阶段方法：1) 持续预训练以将事实绑定到文档标识符；2) 指令微调以激发引用行为。提出Active Indexing，通过合成QA对增强训练数据。

Result: Active Indexing在Qwen2.5-7B和3B模型上表现优于Passive Indexing，引用精度提升高达30.2%。

Conclusion: Active Indexing是一种有效的方法，能够显著提升语言模型的引用可靠性，且性能随数据规模增加而持续提升。

Abstract: Trustworthy language models should provide both correct and verifiable
answers. While language models can sometimes attribute their outputs to
pretraining data, their citations are often unreliable due to hallucination. As
a result, current systems insert citations by querying an external retriever at
inference time, introducing latency, infrastructure dependence, and
vulnerability to retrieval noise. We explore whether LLMs can be made to
reliably attribute to the documents seen during (continual)
pretraining--without test-time retrieval--by revising the training process. To
evaluate this, we release CitePretrainBench, a benchmark that mixes real-world
corpora (Wikipedia, Common Crawl, arXiv) with novel, unseen documents and
probes both short-form (single fact) and long-form (multi-fact) citation tasks.
Our approach follows a two-stage process: (1) continual pretraining to bind
facts to persistent document identifiers, and (2) instruction tuning to elicit
citation behavior. We find that simple Passive Indexing, which appends an
identifier to each document, helps memorize verbatim text but fails on
paraphrased or compositional facts. Instead, we propose Active Indexing, which
continually pretrains on synthetic QA pairs that (1) restate each fact in
diverse compositional forms, and (2) require bidirectional source-to-fact and
fact-to-source generation, jointly teaching the model to generate content from
a cited source and to attribute its own answers. Experiments with Qwen2.5-7B
and 3B show that Active Indexing consistently outperforms Passive Indexing
across all tasks and models, with citation precision gains up to 30.2 percent.
Our ablation studies reveal that performance continues to improve as we scale
the amount of augmented data, showing a clear upward trend even at 16 times the
original token count.

</details>


### [87] [Taming the Untamed: Graph-Based Knowledge Retrieval and Reasoning for MLLMs to Conquer the Unknown](https://arxiv.org/abs/2506.17589)
*Bowen Wang*

Main category: cs.AI

TL;DR: 论文探讨了如何通过构建多模态知识图谱（MH-MMKG）和设计多代理检索器，提升多模态大语言模型（MLLMs）在特定领域任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在罕见领域任务中表现不佳，因缺乏相关知识。研究旨在解决这一问题。

Method: 采用视觉游戏认知作为测试平台，构建MH-MMKG，并设计多代理检索器以自主搜索相关知识。

Result: 实验表明，该方法显著提升了MLLMs的性能。

Conclusion: 研究为多模态知识增强推理提供了新视角，并为未来研究奠定了基础。

Abstract: The real value of knowledge lies not just in its accumulation, but in its
potential to be harnessed effectively to conquer the unknown. Although recent
multimodal large language models (MLLMs) exhibit impressing multimodal
capabilities, they often fail in rarely encountered domain-specific tasks due
to limited relevant knowledge. To explore this, we adopt visual game cognition
as a testbed and select Monster Hunter: World as the target to construct a
multimodal knowledge graph (MH-MMKG), which incorporates multi-modalities and
intricate entity relations. We also design a series of challenging queries
based on MH-MMKG to evaluate the models' ability for complex knowledge
retrieval and reasoning. Furthermore, we propose a multi-agent retriever that
enables a model to autonomously search relevant knowledge without additional
training. Experimental results show that our approach significantly enhances
the performance of MLLMs, providing a new perspective on multimodal
knowledge-augmented reasoning and laying a solid foundation for future
research.

</details>


### [88] [Measuring and Augmenting Large Language Models for Solving Capture-the-Flag Challenges](https://arxiv.org/abs/2506.17644)
*Zimo Ji,Daoyuan Wu,Wenyuan Jiang,Pingchuan Ma,Zongjie Li,Shuai Wang*

Main category: cs.AI

TL;DR: 论文探讨了LLMs在CTF竞赛中的表现，构建了CTFKnow基准测试，并提出CTFAgent框架以提升LLMs的CTF解题能力。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在CTF竞赛中的自动化解题能力，填补技术知识应用与适应性策略的不足。

Method: 构建CTFKnow基准测试（3,992题），并提出CTFAgent框架，包含两阶段RAG和环境增强模块。

Result: CTFAgent在实验中性能提升超80%，并在picoCTF2024竞赛中排名前23.6%。

Conclusion: CTFAgent框架显著提升LLMs的CTF解题能力，验证了技术知识与应用结合的重要性。

Abstract: Capture-the-Flag (CTF) competitions are crucial for cybersecurity education
and training. As large language models (LLMs) evolve, there is increasing
interest in their ability to automate CTF challenge solving. For example, DARPA
has organized the AIxCC competition since 2023 to advance AI-powered automated
offense and defense. However, this demands a combination of multiple abilities,
from knowledge to reasoning and further to actions. In this paper, we highlight
the importance of technical knowledge in solving CTF problems and deliberately
construct a focused benchmark, CTFKnow, with 3,992 questions to measure LLMs'
performance in this core aspect. Our study offers a focused and innovative
measurement of LLMs' capability in understanding CTF knowledge and applying it
to solve CTF challenges. Our key findings reveal that while LLMs possess
substantial technical knowledge, they falter in accurately applying this
knowledge to specific scenarios and adapting their strategies based on feedback
from the CTF environment.
  Based on insights derived from this measurement study, we propose CTFAgent, a
novel LLM-driven framework for advancing CTF problem-solving. CTFAgent
introduces two new modules: two-stage Retrieval Augmented Generation (RAG) and
interactive Environmental Augmentation, which enhance LLMs' technical knowledge
and vulnerability exploitation on CTF, respectively. Our experimental results
show that, on two popular CTF datasets, CTFAgent both achieves over 80%
performance improvement. Moreover, in the recent picoCTF2024 hosted by CMU,
CTFAgent ranked in the top 23.6% of nearly 7,000 participating teams. This
reflects the benefit of our measurement study and the potential of our
framework in advancing LLMs' capabilities in CTF problem-solving.

</details>


### [89] [PhysUniBench: An Undergraduate-Level Physics Reasoning Benchmark for Multimodal Models](https://arxiv.org/abs/2506.17667)
*Lintao Wang,Encheng Su,Jiaqi Liu,Pengze Li,Peng Xia,Jiabei Xiao,Wenlong Zhang,Xinnan Dai,Xi Chen,Yuan Meng,Mingyu Ding,Lei Bai,Wanli Ouyang,Shixiang Tang,Aoran Wang,Xinzhu Ma*

Main category: cs.AI

TL;DR: PhysUniBench是一个多模态基准测试，用于评估和改进多模态大语言模型在本科物理问题上的推理能力，包含3,304个问题，覆盖8个物理子领域。当前最先进模型表现不佳，如GPT-4o mini准确率仅34.2%。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法在捕捉本科物理的广度和复杂性上存在局限，需要更严格的测试工具。

Method: 通过多阶段构建过程（包括专家评估、自动过滤和难度分级）开发PhysUniBench，涵盖开放性和选择题。

Result: 当前模型在物理推理上表现较差，尤其在多步骤问题和图表解读上。

Conclusion: PhysUniBench旨在推动AI在科学领域的发展，促进模型在物理推理和多模态理解上的进步。

Abstract: Physics problem-solving is a challenging domain for large AI models,
requiring integration of conceptual understanding, mathematical reasoning, and
interpretation of physical diagrams. Current evaluation methodologies show
notable limitations in capturing the breadth and complexity of
undergraduate-level physics, underscoring the need for more rigorous
assessments. To this end, we present PhysUniBench, a large-scale multimodal
benchmark designed to evaluate and improve the reasoning capabilities of
multimodal large language models (MLLMs) specifically on undergraduate-level
physics problems. PhysUniBench consists of 3,304 physics questions spanning 8
major sub-disciplines of physics, each accompanied by one visual diagrams. The
benchmark includes both open-ended and multiple-choice questions,
systematically curated and difficulty-rated through an iterative
model-in-the-loop process. The benchmark's construction involved a rigorous
multi-stage process, including multiple roll-outs, expert-level evaluation,
automated filtering of easily solved problems, and a nuanced difficulty grading
system with five levels. Through extensive experiments, we observe that current
state-of-the-art models encounter substantial challenges in physics reasoning.
For example, GPT-4o mini achieves only about 34.2\% accuracy in the proposed
PhysUniBench. These results highlight that current MLLMs struggle with advanced
physics reasoning, especially on multi-step problems and those requiring
precise diagram interpretation. By providing a broad and rigorous assessment
tool, PhysUniBench aims to drive progress in AI for Science, encouraging the
development of models with stronger physical reasoning, problem-solving skills,
and multimodal understanding. The benchmark and evaluation scripts are
available at https://prismax-team.github.io/PhysUniBenchmark/.

</details>


### [90] [Beyond Syntax: Action Semantics Learning for App Agents](https://arxiv.org/abs/2506.17697)
*Bohan Tang,Dezhao Luo,Jingxuan Chen,Shaogang Gong,Jianye Hao,Jun Wang,Kun Shao*

Main category: cs.AI

TL;DR: 论文提出了一种名为动作语义学习（ASL）的新框架，通过捕捉动作的语义而非语法形式，提升了App代理的准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于提示的解决方案依赖封闭的LLM API，计算成本高且依赖外部API；而微调开源LLM的方法因语法学习范式导致分布外（OOD）脆弱性。

Method: ASL框架定义动作语义为用户界面中的状态转换，并引入语义估计器（SEE）计算语义奖励，训练代理生成语义一致的动作。

Result: 理论分析和实验表明，ASL在OOD问题上比现有语法学习范式更具鲁棒性，显著提升了App代理的准确性和泛化能力。

Conclusion: ASL通过语义学习解决了现有方法的局限性，为App代理提供了更高效和鲁棒的解决方案。

Abstract: The advent of Large Language Models (LLMs) enables the rise of App agents
that interpret user intent and operate smartphone Apps through actions such as
clicking and scrolling. While prompt-based solutions with closed LLM APIs show
promising ability, they incur heavy compute costs and external API dependency.
Fine-tuning smaller open-source LLMs solves these limitations. However, current
fine-tuning methods use a syntax learning paradigm that forces agents to
reproduce exactly the ground truth action strings, leading to
out-of-distribution (OOD) vulnerability. To fill this gap, we propose Action
Semantics Learning (ASL), a novel learning framework, where the learning
objective is capturing the semantics of the ground truth actions. Specifically,
inspired by the programming language theory, we define the action semantics for
App agents as the state transition induced by the action in the user interface.
With this insight, ASL employs a novel SEmantic Estimator (SEE) to compute a
semantic reward to train the App agents in generating actions aligned with the
semantics of ground truth actions, even when the syntactic forms differ. To
support the effectiveness of ASL, we theoretically demonstrate the superior
robustness of ASL for the OOD problem compared with the existing syntax
learning paradigm. Extensive experiments on offline and online smartphone App
operation benchmarks show that ASL significantly improves the accuracy and
generalisation of App agents over existing methods.

</details>


### [91] [AnyMAC: Cascading Flexible Multi-Agent Collaboration via Next-Agent Prediction](https://arxiv.org/abs/2506.17784)
*Song Wang,Zhen Tan,Zihan Chen,Shuang Zhou,Tianlong Chen,Jundong Li*

Main category: cs.AI

TL;DR: 提出了一种基于序列结构的多智能体协作框架，通过动态选择智能体角色和上下文信息，提升通信灵活性和性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖静态或图结构的通信拓扑，缺乏适应性和灵活性，限制了多智能体协作的潜力。

Method: 提出Next-Agent Prediction和Next-Context Selection两个关键组件，构建任务自适应的通信管道。

Result: 在多个基准测试中表现优异，同时显著降低通信开销。

Conclusion: 序列结构框架为多智能体协作提供了更大的拓扑空间和更高的灵活性。

Abstract: Recent progress in large language model (LLM)-based multi-agent collaboration
highlights the power of structured communication in enabling collective
intelligence. However, existing methods largely rely on static or graph-based
inter-agent topologies, lacking the potential adaptability and flexibility in
communication. In this work, we propose a new framework that rethinks
multi-agent coordination through a sequential structure rather than a graph
structure, offering a significantly larger topology space for multi-agent
communication. Our method focuses on two key directions: (1) Next-Agent
Prediction, which selects the most suitable agent role at each step, and (2)
Next-Context Selection (NCS), which enables each agent to selectively access
relevant information from any previous step. Together, these components
construct task-adaptive communication pipelines that support both role
flexibility and global information flow. Extensive evaluations across multiple
benchmarks demonstrate that our approach achieves superior performance while
substantially reducing communication overhead.

</details>


### [92] [Bayesian Social Deduction with Graph-Informed Language Models](https://arxiv.org/abs/2506.17788)
*Shahab Rahimirad,Guven Gergerli,Lucia Romero,Angela Qian,Matthew Lyle Olson,Simon Stepputtis,Joseph Campbell*

Main category: cs.AI

TL;DR: 论文提出了一种混合推理框架，结合结构化概率模型和LLM，提升了语言模型在社交推理任务中的表现，并在Avalon游戏中击败了人类玩家。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在社交推理任务（如推断不可观察的信念和意图）中表现有限，尤其是在实时推理和小型模型上。

Method: 采用混合推理框架，将信念推断任务交给结构化概率模型处理，同时利用LLM进行语言理解和交互。

Result: 该方法在Agent-Agent对战中表现优异，首次在受控研究中击败人类玩家，胜率达67%，并获得更高的定性评价。

Conclusion: 混合推理框架显著提升了语言模型的社交推理能力，为未来研究提供了代码、模型和数据集支持。

Abstract: Social reasoning - inferring unobservable beliefs and intentions from partial
observations of other agents - remains a challenging task for large language
models (LLMs). We evaluate the limits of current reasoning language models in
the social deduction game Avalon and find that while the largest models
demonstrate strong performance, they require extensive test-time inference and
degrade sharply when distilled to smaller, real-time-capable variants. To
address this, we introduce a hybrid reasoning framework that externalizes
belief inference to a structured probabilistic model, while using an LLM for
language understanding and interaction. Our approach achieves competitive
performance with much larger models in Agent-Agent play and, notably, is the
first language agent to defeat human players in a controlled study - achieving
a 67% win rate and receiving higher qualitative ratings than both reasoning
baselines and human teammates. We release code, models, and a dataset to
support future work on social reasoning in LLM agents, which can be found at
https://camp-lab-purdue.github.io/bayesian-social-deduction/

</details>


### [93] [Efficient Strategy Synthesis for MDPs via Hierarchical Block Decomposition](https://arxiv.org/abs/2506.17792)
*Alexandros Evangelidis,Gricel Vázquez,Simos Gerasimou*

Main category: cs.AI

TL;DR: 提出了一种动态优化MDP的方法，通过迭代选择脆弱区域进行细化，显著提升了大规模MDP的策略合成效率。


<details>
  <summary>Details</summary>
Motivation: 传统策略合成方法难以应对大规模状态空间，需要更高效的解决方案。

Method: 动态细化MDP，迭代选择最脆弱的区域进行优化。

Result: 在1M状态的MDP中，性能比PRISM提升高达2倍。

Conclusion: 该方法为大规模MDP的策略合成提供了高效且实用的解决方案。

Abstract: Software-intensive systems, such as software product lines and robotics,
utilise Markov decision processes (MDPs) to capture uncertainty and analyse
sequential decision-making problems. Despite the usefulness of conventional
policy synthesis methods, they fail to scale to large state spaces. Our
approach addresses this issue and accelerates policy synthesis in large MDPs by
dynamically refining the MDP and iteratively selecting the most fragile MDP
regions for refinement. This iterative procedure offers a balance between
accuracy and efficiency, as refinement occurs only when necessary. Through a
comprehensive empirical evaluation comprising diverse case studies and MDPs up
to 1M states, we demonstrate significant performance improvements yielded by
our approach compared to the leading probabilistic model checker PRISM (up to
2x), thus offering a very competitive solution for real-world policy synthesis
tasks in larger MDPs.

</details>


### [94] [Reflective Verbal Reward Design for Pluralistic Alignment](https://arxiv.org/abs/2506.17834)
*Carter Blair,Kate Larson,Edith Law*

Main category: cs.AI

TL;DR: 论文提出了一种个性化的奖励建模方法，通过对话引导用户反思和构建偏好，解决了传统RLHF中少数群体偏好被压制的问题。


<details>
  <summary>Details</summary>
Motivation: 人类价值观具有多样性，传统RLHF通过单一奖励模型聚合反馈可能导致少数群体偏好被压制。

Method: 使用语言模型引导用户通过反思对话构建个性化偏好，并基于对话历史生成个性化奖励函数（“语言奖励模型”）。

Result: 在30名参与者的实验中，该方法比非反思语言奖励模型准确率提高9-12%，且样本效率更高。

Conclusion: 个性化奖励建模方法能更有效地捕捉多样化的价值观，同时提升准确性和效率。

Abstract: AI agents are commonly aligned with "human values" through reinforcement
learning from human feedback (RLHF), where a single reward model is learned
from aggregated human feedback and used to align an agent's behavior. However,
human values are not homogeneous--different people hold distinct and sometimes
conflicting values. Aggregating feedback into a single reward model risks
disproportionately suppressing minority preferences. To address this, we
present a novel reward modeling approach for learning individualized reward
models. Our approach uses a language model to guide users through reflective
dialogues where they critique agent behavior and construct their preferences.
This personalized dialogue history, containing the user's reflections and
critiqued examples, is then used as context for another language model that
serves as an individualized reward function (what we call a "verbal reward
model") for evaluating new trajectories. In studies with 30 participants, our
method achieved a 9-12% improvement in accuracy over non-reflective verbal
reward models while being more sample efficient than traditional supervised
learning methods.

</details>


### [95] [Out of Control -- Why Alignment Needs Formal Control Theory (and an Alignment Control Stack)](https://arxiv.org/abs/2506.17846)
*Elija Perrier*

Main category: cs.AI

TL;DR: 本文主张将形式最优控制理论作为AI对齐研究的核心，提出了一种不同于主流AI安全和安全方法的新视角。通过引入分层对齐控制堆栈，探讨了各层的测量与控制特性及其互操作性，旨在为前沿AI模型和代理系统提供更全面的控制框架。


<details>
  <summary>Details</summary>
Motivation: 当前AI安全和机制可解释性研究在形式上虽有所进展，但缺乏通用性和协议互操作性研究。本文旨在通过形式最优控制理论，构建一个更全面的对齐框架，以提升AI系统的安全性和可靠性。

Method: 提出“对齐控制堆栈”概念，分层从物理层到社会技术层，分析各层的测量与控制特性及其互操作性，结合最优控制理论与实际部署需求。

Result: 通过分层对齐控制堆栈，为前沿AI模型和代理系统提供了更全面的控制框架，增强了安全性和可靠性。

Conclusion: 形式最优控制理论的应用能够为AI对齐研究提供更全面的框架，帮助政府和监管机构更好地理解和控制AI技术，确保其可持续造福社会。

Abstract: This position paper argues that formal optimal control theory should be
central to AI alignment research, offering a distinct perspective from
prevailing AI safety and security approaches. While recent work in AI safety
and mechanistic interpretability has advanced formal methods for alignment,
they often fall short of the generalisation required of control frameworks for
other technologies. There is also a lack of research into how to render
different alignment/control protocols interoperable. We argue that by recasting
alignment through principles of formal optimal control and framing alignment in
terms of hierarchical stack from physical to socio-technical layers according
to which controls may be applied we can develop a better understanding of the
potential and limitations for controlling frontier models and agentic AI
systems. To this end, we introduce an Alignment Control Stack which sets out a
hierarchical layered alignment stack, identifying measurement and control
characteristics at each layer and how different layers are formally
interoperable. We argue that such analysis is also key to the assurances that
will be needed by governments and regulators in order to see AI technologies
sustainably benefit the community. Our position is that doing so will bridge
the well-established and empirically validated methods of optimal control with
practical deployment considerations to create a more comprehensive alignment
framework, enhancing how we approach safety and reliability for advanced AI
systems.

</details>


### [96] [Towards Robust Fact-Checking: A Multi-Agent System with Advanced Evidence Retrieval](https://arxiv.org/abs/2506.17878)
*Tam Trinh,Manh Nguyen,Truong-Son Hy*

Main category: cs.AI

TL;DR: 本文提出了一种基于多智能体系统的自动化事实核查方法，显著提升了准确性、效率和可解释性。


<details>
  <summary>Details</summary>
Motivation: 数字时代中错误信息的快速传播对公共讨论构成挑战，传统人工核查方法难以应对在线内容的规模和速度，而现有自动化方法在处理复杂声明、确保来源可信度和透明度方面存在局限。

Method: 系统包含四个专门智能体：输入摄取智能体分解声明，查询生成智能体制定目标子查询，证据检索智能体获取可信证据，裁决预测智能体生成可解释的验证判断。

Result: 在基准数据集（FEVEROUS、HOVER、SciFact）上，系统比基线方法在Macro F1分数上提升了12.3%，能有效分解复杂声明、检索可靠证据并提供透明解释。

Conclusion: 该方法为自动化事实核查领域提供了更准确、高效和透明的验证方法，同时保持实际应用的扩展性。

Abstract: The rapid spread of misinformation in the digital era poses significant
challenges to public discourse, necessitating robust and scalable fact-checking
solutions. Traditional human-led fact-checking methods, while credible,
struggle with the volume and velocity of online content, prompting the
integration of automated systems powered by Large Language Models (LLMs).
However, existing automated approaches often face limitations, such as handling
complex claims, ensuring source credibility, and maintaining transparency. This
paper proposes a novel multi-agent system for automated fact-checking that
enhances accuracy, efficiency, and explainability. The system comprises four
specialized agents: an Input Ingestion Agent for claim decomposition, a Query
Generation Agent for formulating targeted subqueries, an Evidence Retrieval
Agent for sourcing credible evidence, and a Verdict Prediction Agent for
synthesizing veracity judgments with human-interpretable explanations.
Evaluated on benchmark datasets (FEVEROUS, HOVER, SciFact), the proposed system
achieves a 12.3% improvement in Macro F1-score over baseline methods. The
system effectively decomposes complex claims, retrieves reliable evidence from
trusted sources, and generates transparent explanations for verification
decisions. Our approach contributes to the growing field of automated
fact-checking by providing a more accurate, efficient, and transparent
verification methodology that aligns with human fact-checking practices while
maintaining scalability for real-world applications. Our source code is
available at https://github.com/HySonLab/FactAgent

</details>


### [97] [Leveraging Large Language Model for Intelligent Log Processing and Autonomous Debugging in Cloud AI Platforms](https://arxiv.org/abs/2506.17900)
*Cheng Ji,Huaiying Luo*

Main category: cs.AI

TL;DR: 本文提出了一种基于大语言模型（LLM）的智能日志处理和自动调试框架LLM-ID，通过多阶段语义推理机制提升故障定位和系统自修复能力。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统规模和复杂度的增加，日志数据量大、非结构化且语义模糊，传统方法难以有效定位故障。

Method: 结合预训练Transformer模型和多阶段语义推理，动态结构化日志，利用无监督聚类和嵌入机制提取事件模板和语义模式，再通过微调LLM和多轮注意力机制进行上下文推理生成故障假设和根因路径，并引入基于强化学习的恢复规划器。

Result: 实验表明，LLM-ID在云平台日志数据集上故障定位准确率提升16.2%，优于主流方法。

Conclusion: LLM-ID具有更强的语义理解、持续学习和异构环境适应能力，为复杂系统故障定位提供了高效解决方案。

Abstract: With the increasing complexity and rapid expansion of the scale of AI systems
in cloud platforms, the log data generated during system operation is massive,
unstructured, and semantically ambiguous, which brings great challenges to
fault location and system self-repair. In order to solve this problem, this
paper proposes an intelligent log processing and automatic debugging framework
based on Large Language Model (LLM), named Intelligent Debugger (LLM-ID). This
method is extended on the basis of the existing pre-trained Transformer model,
and integrates a multi-stage semantic inference mechanism to realize the
context understanding of system logs and the automatic reconstruction of fault
chains. Firstly, the system log is dynamically structured, and the unsupervised
clustering and embedding mechanism is used to extract the event template and
semantic schema. Subsequently, the fine-tuned LLM combined with the multi-round
attention mechanism to perform contextual reasoning on the log sequence to
generate potential fault assumptions and root cause paths. Furthermore, this
paper introduces a reinforcement learning-based policy-guided recovery planner,
which is driven by the remediation strategy generated by LLM to support dynamic
decision-making and adaptive debugging in the cloud environment. Compared with
the existing rule engine or traditional log analysis system, the proposed model
has stronger semantic understanding ability, continuous learning ability and
heterogeneous environment adaptability. Experiments on the cloud platform log
dataset show that LLM-ID improves the fault location accuracy by 16.2%, which
is significantly better than the current mainstream methods

</details>


### [98] [Learning, Reasoning, Refinement: A Framework for Kahneman's Dual-System Intelligence in GUI Agents](https://arxiv.org/abs/2506.17913)
*Jinjie Wei,Jiyao Liu,Lihao Liu,Ming Hu,Junzhi Ning,Mingcheng Li,Weijie Yin,Junjun He,Xiao Liang,Chao Feng,Dingkang Yang*

Main category: cs.AI

TL;DR: CogniGUI是一个基于认知框架的GUI代理，通过结合视觉语义分析和相对奖励系统，实现自适应学习和高效操作，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有GUI代理系统依赖试错决策，缺乏渐进式推理和自适应学习能力，且评估指标过于简单。

Method: 结合双系统设计：快速视觉语义分析的解析引擎和基于相对奖励的策略优化代理。

Result: CogniGUI在现有和新提出的基准测试中均优于现有方法。

Conclusion: CogniGUI通过认知框架和新型基准测试，提升了GUI代理的自适应性和性能。

Abstract: Graphical User Interface (GUI) agents have made significant progress in
automating digital tasks through the utilization of computer vision and
language models. Nevertheless, existing agent systems encounter notable
limitations. Firstly, they predominantly depend on trial and error decision
making rather than progressive reasoning, thereby lacking the capability to
learn and adapt from interactive encounters. Secondly, these systems are
assessed using overly simplistic single step accuracy metrics, which do not
adequately reflect the intricate nature of real world GUI interactions. In this
paper, we present CogniGUI, a cognitive framework developed to overcome these
limitations by enabling adaptive learning for GUI automation resembling
human-like behavior. Inspired by Kahneman's Dual Process Theory, our approach
combines two main components: (1) an omni parser engine that conducts immediate
hierarchical parsing of GUI elements through quick visual semantic analysis to
identify actionable components, and (2) a Group based Relative Policy
Optimization (GRPO) grounding agent that assesses multiple interaction paths
using a unique relative reward system, promoting minimal and efficient
operational routes. This dual-system design facilitates iterative ''exploration
learning mastery'' cycles, enabling the agent to enhance its strategies over
time based on accumulated experience. Moreover, to assess the generalization
and adaptability of agent systems, we introduce ScreenSeek, a comprehensive
benchmark that includes multi application navigation, dynamic state
transitions, and cross interface coherence, which are often overlooked
challenges in current benchmarks. Experimental results demonstrate that
CogniGUI surpasses state-of-the-art methods in both the current GUI grounding
benchmarks and our newly proposed benchmark.

</details>


### [99] [Evolving Prompts In-Context: An Open-ended, Self-replicating Perspective](https://arxiv.org/abs/2506.17930)
*Jianyu Wang,Zhiqiang Hu,Lidong Bing*

Main category: cs.AI

TL;DR: 论文提出了一种新的提示设计范式，通过将随机演示修剪为看似无意义的“胡言乱语”来提升大语言模型（LLM）的性能，并提出了自动优化框架PromptQuine。


<details>
  <summary>Details</summary>
Motivation: 挑战传统提示设计的智慧，探索更高效的提示优化方法，以提升LLM在多样化任务中的表现。

Method: 提出PromptQuine框架，通过进化搜索自动寻找有效的修剪策略，仅需少量数据即可优化提示。

Result: “胡言乱语”提示在多种任务中表现优异，甚至超越现有自动优化技术，且适用于不同对齐的LLM。

Conclusion: 研究为上下文学习的机制研究提供了新方向，并呼吁开发更开放的搜索算法以优化LLM提示。

Abstract: We propose a novel prompt design paradigm that challenges conventional wisdom
in large language model (LLM) prompting. While conventional wisdom prioritizes
well-crafted instructions and demonstrations for in-context learning (ICL), we
show that pruning random demonstrations into seemingly incoherent "gibberish"
can remarkably improve performance across diverse tasks. Notably, the
"gibberish" always matches or surpasses state-of-the-art automatic prompt
optimization techniques, achieving substantial gains regardless of LLM
alignment. Nevertheless, discovering an effective pruning strategy is
non-trivial, as existing attribution methods and prompt compression algorithms
fail to deliver robust results, let alone human intuition. In terms of this, we
propose a self-discover prompt optimization framework, PromptQuine, an
evolutionary search framework that automatically searches for the pruning
strategy by itself using only low-data regimes. Much like the emergent
complexity in nature--such as symbiosis and self-organization--arising in
response to resource constraints, our framework evolves and refines
unconventional yet highly effective prompts by leveraging only the tokens
present within the context. We demonstrate its effectiveness across
classification, multi-choice question answering, generation and math reasoning
tasks across LLMs, while achieving decent runtime efficiency. We hope our
findings can guide mechanistic studies on in-context learning, and provide a
call to action, to pave the way for more open-ended search algorithms for more
effective LLM prompting.

</details>


### [100] [medicX-KG: A Knowledge Graph for Pharmacists' Drug Information Needs](https://arxiv.org/abs/2506.17959)
*Lizzy Farrugia,Lilian M. Azzopardi,Jeremy Debattista,Charlie Abela*

Main category: cs.AI

TL;DR: 本文介绍了medicX-KG，一个面向药剂师的知识图谱，支持临床和监管决策，整合多源数据，解决药物信息碎片化问题。


<details>
  <summary>Details</summary>
Motivation: 药剂师角色从配药转向综合服务，需要准确、实时的药物信息支持。现有数据源分散，缺乏统一整合。

Method: 利用人工智能和语义技术构建medicX-KG，整合BNF、DrugBank和MMA数据，通过访谈优化设计。

Result: medicX-KG有效支持药物可用性、相互作用、不良反应等查询，但存在剂量编码不足和实时更新限制。

Conclusion: medicX-KG为药剂师提供了实用工具，未来需优化数据覆盖和实时性。

Abstract: The role of pharmacists is evolving from medicine dispensing to delivering
comprehensive pharmaceutical services within multidisciplinary healthcare
teams. Central to this shift is access to accurate, up-to-date medicinal
product information supported by robust data integration. Leveraging artificial
intelligence and semantic technologies, Knowledge Graphs (KGs) uncover hidden
relationships and enable data-driven decision-making. This paper presents
medicX-KG, a pharmacist-oriented knowledge graph supporting clinical and
regulatory decisions. It forms the semantic layer of the broader medicX
platform, powering predictive and explainable pharmacy services. medicX-KG
integrates data from three sources, including, the British National Formulary
(BNF), DrugBank, and the Malta Medicines Authority (MMA) that addresses Malta's
regulatory landscape and combines European Medicines Agency alignment with
partial UK supply dependence. The KG tackles the absence of a unified national
drug repository, reducing pharmacists' reliance on fragmented sources. Its
design was informed by interviews with practicing pharmacists to ensure
real-world applicability. We detail the KG's construction, including data
extraction, ontology design, and semantic mapping. Evaluation demonstrates that
medicX-KG effectively supports queries about drug availability, interactions,
adverse reactions, and therapeutic classes. Limitations, including missing
detailed dosage encoding and real-time updates, are discussed alongside
directions for future enhancements.

</details>


### [101] [Graphs Meet AI Agents: Taxonomy, Progress, and Future Opportunities](https://arxiv.org/abs/2506.18019)
*Yuanchen Bei,Weizhi Zhang,Siwen Wang,Weizhi Chen,Sheng Zhou,Hao Chen,Yong Li,Jiajun Bu,Shirui Pan,Yizhou Yu,Irwin King,Fakhri Karray,Philip S. Yu*

Main category: cs.AI

TL;DR: 论文探讨了图结构如何赋能AI代理，通过整合图技术与核心代理功能，提升其处理复杂任务的能力。


<details>
  <summary>Details</summary>
Motivation: AI代理从强化学习到大型语言模型的演进，需要更高效的数据结构支持复杂任务，图结构因其优势成为关键。

Method: 系统综述了图技术与AI代理功能的结合，包括规划、记忆和协作，并展示了应用案例。

Result: 图结构为AI代理提供了强大的数据处理能力，支持其应对复杂挑战。

Conclusion: 图结构是下一代AI代理的关键技术，未来研究应进一步探索其潜力。

Abstract: AI agents have experienced a paradigm shift, from early dominance by
reinforcement learning (RL) to the rise of agents powered by large language
models (LLMs), and now further advancing towards a synergistic fusion of RL and
LLM capabilities. This progression has endowed AI agents with increasingly
strong abilities. Despite these advances, to accomplish complex real-world
tasks, agents are required to plan and execute effectively, maintain reliable
memory, and coordinate smoothly with other agents. Achieving these capabilities
involves contending with ever-present intricate information, operations, and
interactions. In light of this challenge, data structurization can play a
promising role by transforming intricate and disorganized data into
well-structured forms that agents can more effectively understand and process.
In this context, graphs, with their natural advantage in organizing, managing,
and harnessing intricate data relationships, present a powerful data paradigm
for structurization to support the capabilities demanded by advanced AI agents.
To this end, this survey presents a first systematic review of how graphs can
empower AI agents. Specifically, we explore the integration of graph techniques
with core agent functionalities, highlight notable applications, and identify
prospective avenues for future research. By comprehensively surveying this
burgeoning intersection, we hope to inspire the development of next-generation
AI agents equipped to tackle increasingly sophisticated challenges with graphs.
Related resources are collected and continuously updated for the community in
the Github link.

</details>


### [102] [Action Language BC+](https://arxiv.org/abs/2506.18044)
*Joseph Babb,Joohyung Lee*

Main category: cs.AI

TL;DR: BC+是一种新的动作语言，旨在填补传统动作语言与现代ASP语言之间的差距，通过广义稳定模型语义实现。


<details>
  <summary>Details</summary>
Motivation: 传统动作语言与现代ASP语言之间存在表达能力的差距，限制了动作语言的实用性。

Method: 提出BC+语言，基于广义稳定模型语义定义其语义，涵盖现代ASP语言的有用构造。

Result: BC+能够涵盖其他动作语言（如B、C、C+、BC）的最佳特性，并可通过ASP求解器实现。

Conclusion: BC+成功填补了动作语言与现代ASP语言之间的差距，并实现了实用化。

Abstract: Action languages are formal models of parts of natural language that are
designed to describe effects of actions. Many of these languages can be viewed
as high level notations of answer set programs structured to represent
transition systems. However, the form of answer set programs considered in the
earlier work is quite limited in comparison with the modern Answer Set
Programming (ASP) language, which allows several useful constructs for
knowledge representation, such as choice rules, aggregates, and abstract
constraint atoms. We propose a new action language called BC+, which closes the
gap between action languages and the modern ASP language. The main idea is to
define the semantics of BC+ in terms of general stable model semantics for
propositional formulas, under which many modern ASP language constructs can be
identified with shorthands for propositional formulas. Language BC+ turns out
to be sufficiently expressive to encompass the best features of other action
languages, such as languages B, C, C+, and BC. Computational methods available
in ASP solvers are readily applicable to compute BC+, which led to an
implementation of the language by extending system cplus2asp.

</details>


### [103] [Weighted Assumption Based Argumentation to reason about ethical principles and actions](https://arxiv.org/abs/2506.18056)
*Paolo Baldi,Fabio Aurelio D'Asaro,Abeer Dyoub,Francesca Alessandra Lisi*

Main category: cs.AI

TL;DR: 本文扩展了基于假设的论证（ABA），引入加权论证，通过为论证分配权重并推导攻击权重，以伦理学推理为例展示方法，并基于答案集编程实现。


<details>
  <summary>Details</summary>
Motivation: 增强ABA的表达能力，使其能够处理带权重的论证，从而更灵活地应用于伦理学推理等领域。

Method: 为ABA中的论证分配权重，并推导攻击权重，通过伦理学推理案例展示方法，并基于答案集编程实现。

Result: 提出了一种带权重的ABA框架，并通过实例和实现验证了其可行性。

Conclusion: 加权ABA扩展了传统ABA的表达能力，为伦理学推理等应用提供了更灵活的工具。

Abstract: We augment Assumption Based Argumentation (ABA for short) with weighted
argumentation. In a nutshell, we assign weights to arguments and then derive
the weight of attacks between ABA arguments. We illustrate our proposal through
running examples in the field of ethical reasoning, and present an
implementation based on Answer Set Programming.

</details>


### [104] [Deep Research Agents: A Systematic Examination And Roadmap](https://arxiv.org/abs/2506.18096)
*Yuxuan Huang,Yihang Chen,Haozheng Zhang,Kang Li,Meng Fang,Linyi Yang,Xiaoguang Li,Lifeng Shang,Songcen Xu,Jianye Hao,Kun Shao,Jun Wang*

Main category: cs.AI

TL;DR: 本文分析了构成深度研究（DR）代理的基础技术和架构组件，提出了分类法，并评估了当前基准的局限性，同时指出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLM）的快速发展，深度研究代理成为解决复杂多轮信息研究任务的新兴自主AI系统，需要对其技术和架构进行系统分析。

Method: 通过对比信息获取策略、模块化工具使用框架，提出分类法区分静态和动态工作流，并基于规划策略和代理组成对架构进行分类。

Result: 系统化现有方法，指出当前基准的关键局限性（如外部知识访问受限、执行效率低等），并提出了未来研究方向。

Conclusion: 深度研究代理领域存在开放挑战，但未来研究方向明确，相关资源已整理为可更新的研究库。

Abstract: The rapid progress of Large Language Models (LLMs) has given rise to a new
category of autonomous AI systems, referred to as Deep Research (DR) agents.
These agents are designed to tackle complex, multi-turn informational research
tasks by leveraging a combination of dynamic reasoning, adaptive long-horizon
planning, multi-hop information retrieval, iterative tool use, and the
generation of structured analytical reports. In this paper, we conduct a
detailed analysis of the foundational technologies and architectural components
that constitute Deep Research agents. We begin by reviewing information
acquisition strategies, contrasting API-based retrieval methods with
browser-based exploration. We then examine modular tool-use frameworks,
including code execution, multimodal input processing, and the integration of
Model Context Protocols (MCPs) to support extensibility and ecosystem
development. To systematize existing approaches, we propose a taxonomy that
differentiates between static and dynamic workflows, and we classify agent
architectures based on planning strategies and agent composition, including
single-agent and multi-agent configurations. We also provide a critical
evaluation of current benchmarks, highlighting key limitations such as
restricted access to external knowledge, sequential execution inefficiencies,
and misalignment between evaluation metrics and the practical objectives of DR
agents. Finally, we outline open challenges and promising directions for future
research. A curated and continuously updated repository of DR agent research is
available at: {https://github.com/ai-agents-2030/awesome-deep-research-agent}.

</details>


### [105] [Decentralized Consensus Inference-based Hierarchical Reinforcement Learning for Multi-Constrained UAV Pursuit-Evasion Game](https://arxiv.org/abs/2506.18126)
*Xiang Yuming,Li Sizhao,Li Rongpeng,Zhao Zhifeng,Zhang Honggang*

Main category: cs.AI

TL;DR: 论文提出了一种名为CI-HRL的两层框架，用于解决多约束追逃游戏中的协同逃避与编队覆盖问题，通过高层策略和低层策略分别处理目标定位和避障导航，实验验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 多约束追逃游戏中的协同逃避与编队覆盖问题具有高维复杂性，尤其在通信受限条件下更具挑战性。

Method: 采用Consensus Inference-based Hierarchical Reinforcement Learning (CI-HRL)框架，高层策略使用ConsMAC模块实现全局信息感知，低层策略使用AT-M和策略蒸馏完成控制。

Result: 实验（包括高保真SITL仿真）表明CI-HRL在协同逃避和任务完成能力上表现优越。

Conclusion: CI-HRL为解决高维复杂问题提供了有效方案，提升了无人机群的协作能力。

Abstract: Multiple quadrotor unmanned aerial vehicle (UAV) systems have garnered
widespread research interest and fostered tremendous interesting applications,
especially in multi-constrained pursuit-evasion games (MC-PEG). The Cooperative
Evasion and Formation Coverage (CEFC) task, where the UAV swarm aims to
maximize formation coverage across multiple target zones while collaboratively
evading predators, belongs to one of the most challenging issues in MC-PEG,
especially under communication-limited constraints. This multifaceted problem,
which intertwines responses to obstacles, adversaries, target zones, and
formation dynamics, brings up significant high-dimensional complications in
locating a solution. In this paper, we propose a novel two-level framework
(i.e., Consensus Inference-based Hierarchical Reinforcement Learning (CI-HRL)),
which delegates target localization to a high-level policy, while adopting a
low-level policy to manage obstacle avoidance, navigation, and formation.
Specifically, in the high-level policy, we develop a novel multi-agent
reinforcement learning module, Consensus-oriented Multi-Agent Communication
(ConsMAC), to enable agents to perceive global information and establish
consensus from local states by effectively aggregating neighbor messages.
Meanwhile, we leverage an Alternative Training-based Multi-agent proximal
policy optimization (AT-M) and policy distillation to accomplish the low-level
control. The experimental results, including the high-fidelity
software-in-the-loop (SITL) simulations, validate that CI-HRL provides a
superior solution with enhanced swarm's collaborative evasion and task
completion capabilities.

</details>


### [106] [SE-Merging: A Self-Enhanced Approach for Dynamic Model Merging](https://arxiv.org/abs/2506.18135)
*Zijun Chen,Zhanpeng Zhou,Bo Zhang,Weinan Zhang,Xi Sun,Junchi Yan*

Main category: cs.AI

TL;DR: 论文研究了模型合并的机制，提出了一种自增强框架SE-Merging，通过动态识别任务和调整合并系数提升多任务能力。


<details>
  <summary>Details</summary>
Motivation: 尽管模型合并在多任务能力上表现出色，但其底层机制尚不明确，需要深入研究。

Method: 从表示角度分析模型合并机制，提出SE-Merging框架，动态识别任务并调整合并系数。

Result: SE-Merging显著提升了性能，且无需额外训练，兼容现有技术。

Conclusion: 模型合并通过区分任务和适应专家模型实现多任务能力，SE-Merging进一步优化了这一过程。

Abstract: Model merging has gained increasing attention due to its intriguing property:
interpolating the parameters of different task-specific fine-tuned models leads
to multi-task abilities. However, despite its empirical success, the underlying
mechanisms of model merging remain poorly understood. In this work, we delve
into the mechanism behind model merging from a representation perspective. Our
analysis reveals that model merging achieves multi-task abilities through two
key capabilities: i) distinguishing samples from different tasks, and ii)
adapting to the corresponding expert model for each sample. These two
capabilities allow the merged model to retain task-specific expertise, enabling
efficient multi-task adaptation. Building on these insights, we propose
\texttt{SE-Merging}, a self-enhanced model merging framework that leverages
these two characteristics to dynamically identify the corresponding task for
each sample and then adaptively rescales the merging coefficients to further
enhance task-specific expertise in the merged model. Notably,
\texttt{SE-Merging} achieves dynamic model merging without additional training.
Extensive experiments demonstrate that \texttt{SE-Merging} achieves significant
performance improvements while remaining compatible with existing model merging
techniques.

</details>


### [107] [CoachGPT: A Scaffolding-based Academic Writing Assistant](https://arxiv.org/abs/2506.18149)
*Fumian Chen,Sotheara Veng,Joshua Wilson,Xiaoming Li,Hui Fang*

Main category: cs.AI

TL;DR: 论文提出了一种基于大型语言模型（LLMs）的学术写作助手CoachGPT，旨在为资源有限或偏好自主学习的学习者提供实时反馈和指导。


<details>
  <summary>Details</summary>
Motivation: 传统写作辅助工具缺乏上下文理解，而现有LLMs生成的文本缺乏教学功能，可能对学习产生负面影响。

Method: 开发了CoachGPT，一个基于LLMs的AI代理网络应用，通过分解任务并提供实时反馈来辅助写作。

Result: 用户研究证明CoachGPT在学术写作中的实用性和LLMs的潜力。

Conclusion: CoachGPT通过独特的脚手架结构，为学术写作提供了更沉浸式的体验和个性化指导。

Abstract: Academic writing skills are crucial for students' success, but can feel
overwhelming without proper guidance and practice, particularly when writing in
a second language. Traditionally, students ask instructors or search
dictionaries, which are not universally accessible. Early writing assistants
emerged as rule-based systems that focused on detecting misspellings,
subject-verb disagreements, and basic punctuation errors; however, they are
inaccurate and lack contextual understanding. Machine learning-based assistants
demonstrate a strong ability for language understanding but are expensive to
train. Large language models (LLMs) have shown remarkable capabilities in
generating responses in natural languages based on given prompts. Still, they
have a fundamental limitation in education: they generate essays without
teaching, which can have detrimental effects on learning when misused. To
address this limitation, we develop CoachGPT, which leverages large language
models (LLMs) to assist individuals with limited educational resources and
those who prefer self-paced learning in academic writing. CoachGPT is an AI
agent-based web application that (1) takes instructions from experienced
educators, (2) converts instructions into sub-tasks, and (3) provides real-time
feedback and suggestions using large language models. This unique scaffolding
structure makes CoachGPT unique among existing writing assistants. Compared to
existing writing assistants, CoachGPT provides a more immersive writing
experience with personalized feedback and guidance. Our user studies prove the
usefulness of CoachGPT and the potential of large language models for academic
writing.

</details>


### [108] [AI Through the Human Lens: Investigating Cognitive Theories in Machine Psychology](https://arxiv.org/abs/2506.18156)
*Akash Kundu,Rishika Goswami*

Main category: cs.AI

TL;DR: 研究探讨了大型语言模型（LLMs）是否表现出类似人类的认知模式，基于心理学中的四个框架，发现模型行为与人类认知倾向相似但受训练数据和调整方法影响。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs是否具备类似人类的认知模式，以评估其透明度和伦理部署的潜在影响。

Method: 通过结构化提示和自动评分，评估多个专有和开源模型在四个心理学框架下的表现。

Result: 模型能生成连贯叙述，易受正面框架影响，道德判断偏向自由/压迫维度，并通过大量合理化缓解自我矛盾。

Conclusion: 研究为AI透明度和伦理部署提供启示，并建议未来结合认知心理学与AI安全性研究。

Abstract: We investigate whether Large Language Models (LLMs) exhibit human-like
cognitive patterns under four established frameworks from psychology: Thematic
Apperception Test (TAT), Framing Bias, Moral Foundations Theory (MFT), and
Cognitive Dissonance. We evaluated several proprietary and open-source models
using structured prompts and automated scoring. Our findings reveal that these
models often produce coherent narratives, show susceptibility to positive
framing, exhibit moral judgments aligned with Liberty/Oppression concerns, and
demonstrate self-contradictions tempered by extensive rationalization. Such
behaviors mirror human cognitive tendencies yet are shaped by their training
data and alignment methods. We discuss the implications for AI transparency,
ethical deployment, and future work that bridges cognitive psychology and AI
safety

</details>


### [109] [Chain-of-Memory: Enhancing GUI Agents for Cross-Application Navigation](https://arxiv.org/abs/2506.18158)
*Xinzge Gao,Chuanrui Hu,Bin Chen,Teng Li*

Main category: cs.AI

TL;DR: 提出了一种名为Chain-of-Memory（CoM）的新方法，用于显式建模GUI代理的短期和长期记忆，显著提升了跨应用任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖历史截图或动作隐式表示任务状态，导致GUI代理难以准确理解任务状态，且缺乏存储关键信息的机制。

Method: CoM通过捕获动作描述、整合任务相关屏幕信息，并维护专用内存模块来显式建模记忆。

Result: 实验表明，CoM显著提升了GUI代理在跨应用任务中的性能，且7B模型可实现与72B模型相当的内存管理能力。

Conclusion: CoM为GUI代理提供了显式记忆表示，解决了复杂任务中的信息存储问题，并通过开源数据集和代码推动了进一步研究。

Abstract: Multimodal large language models (MLLMs) are attracting growing attention in
the development of Graphical User Interface (GUI) agents. Existing approaches
often rely on historical screenshots or actions to implicitly represent the
task state. This reliance poses challenges for GUI agents in accurately
understanding task states and underscores the absence of effective mechanisms
to store critical information in complex and lengthy cross-app tasks. To
address these challenges, we propose Chain-of-Memory (CoM), a novel approach
for explicitly modeling short-term and long-term memory in GUI agents. CoM
achieves this by capturing action descriptions, integrating task-relevant
screen information, and maintaining a dedicated memory module to store and
manage this information. By leveraging explicit memory representations, CoM
enables GUI agents to better understand task states and retain critical
historical information persistently. To equip GUI agents with memory management
capabilities and evaluate the effectiveness of CoM, we developed the GUI
Odyssey-CoM, a dataset comprising 111k screen-action pairs annotated with
Chain-of-Memory. Experimental results demonstrate that CoM significantly
improves GUI agents' performance in cross-application tasks. Additionally, GUI
Odyssey-CoM enables 7B models to achieve memory management capabilities
comparable to 72B models. The dataset and code will be open-sourced.

</details>


### [110] [Reasoning about Uncertainty: Do Reasoning Models Know When They Don't Know?](https://arxiv.org/abs/2506.18183)
*Zhiting Mei,Christina Zhang,Tenny Yin,Justin Lidard,Ola Shorinwa,Anirudha Majumdar*

Main category: cs.AI

TL;DR: 论文探讨了推理语言模型的不确定性量化问题，发现模型通常过度自信，且深度推理可能加剧这一问题，但通过自省可以部分改善校准。


<details>
  <summary>Details</summary>
Motivation: 推理模型在生成错误答案时仍表现出高置信度（幻觉问题），研究其不确定性量化对实际应用的安全性至关重要。

Method: 提出自省不确定性量化（UQ）方法，评估模型校准情况，并测试深度推理和自省对校准的影响。

Result: 发现推理模型通常过度自信，深度推理可能加剧此问题，而自省能部分改善校准（如o3-Mini和DeepSeek R1），但并非所有模型都受益（如Claude 3.7 Sonnet）。

Conclusion: 需进一步研究设计UQ基准和改进推理模型的校准方法。

Abstract: Reasoning language models have set state-of-the-art (SOTA) records on many
challenging benchmarks, enabled by multi-step reasoning induced using
reinforcement learning. However, like previous language models, reasoning
models are prone to generating confident, plausible responses that are
incorrect (hallucinations). Knowing when and how much to trust these models is
critical to the safe deployment of reasoning models in real-world applications.
To this end, we explore uncertainty quantification of reasoning models in this
work. Specifically, we ask three fundamental questions: First, are reasoning
models well-calibrated? Second, does deeper reasoning improve model
calibration? Finally, inspired by humans' innate ability to double-check their
thought processes to verify the validity of their answers and their confidence,
we ask: can reasoning models improve their calibration by explicitly reasoning
about their chain-of-thought traces? We introduce introspective uncertainty
quantification (UQ) to explore this direction. In extensive evaluations on SOTA
reasoning models across a broad range of benchmarks, we find that reasoning
models: (i) are typically overconfident, with self-verbalized confidence
estimates often greater than 85% particularly for incorrect responses, (ii)
become even more overconfident with deeper reasoning, and (iii) can become
better calibrated through introspection (e.g., o3-Mini and DeepSeek R1) but not
uniformly (e.g., Claude 3.7 Sonnet becomes more poorly calibrated). Lastly, we
conclude with important research directions to design necessary UQ benchmarks
and improve the calibration of reasoning models.

</details>


### [111] [The Impact of Medication Non-adherence on Adverse Outcomes: Evidence from Schizophrenia Patients via Survival Analysis](https://arxiv.org/abs/2506.18187)
*Shahriar Noroozizadeh,Pim Welle,Jeremy C. Weiss,George H. Chen*

Main category: cs.AI

TL;DR: 研究量化了精神分裂症患者不遵医嘱服用抗精神病药物与不良后果的关联，使用生存分析和因果推断方法，发现不遵医嘱会提前1至4个月引发不良事件。


<details>
  <summary>Details</summary>
Motivation: 探讨不遵医嘱服用抗精神病药物对精神分裂症患者不良后果的影响，为临床和政策提供依据。

Method: 采用生存分析方法，结合T-learner、S-learner和最近邻匹配等因果推断工具，利用不同时间段的纵向数据（3、6、9、12个月）进行分析。

Result: 研究发现不遵医嘱会显著提前不良后果1至4个月，且不同药物类型和剂型的结果一致。

Conclusion: 研究强调了遵医嘱的重要性，并展示了生存分析与因果推断结合的政策价值，但需注意因果解释的假设限制。

Abstract: This study quantifies the association between non-adherence to antipsychotic
medications and adverse outcomes in individuals with schizophrenia. We frame
the problem using survival analysis, focusing on the time to the earliest of
several adverse events (early death, involuntary hospitalization, jail
booking). We extend standard causal inference methods (T-learner, S-learner,
nearest neighbor matching) to utilize various survival models to estimate
individual and average treatment effects, where treatment corresponds to
medication non-adherence. Analyses are repeated using different amounts of
longitudinal information (3, 6, 9, and 12 months). Using data from Allegheny
County in western Pennsylvania, we find strong evidence that non-adherence
advances adverse outcomes by approximately 1 to 4 months. Ablation studies
confirm that county-provided risk scores adjust for key confounders, as their
removal amplifies the estimated effects. Subgroup analyses by medication
formulation (injectable vs. oral) and medication type consistently show that
non-adherence is associated with earlier adverse events. These findings
highlight the clinical importance of adherence in delaying psychiatric crises
and show that integrating survival analysis with causal inference tools can
yield policy-relevant insights. We caution that although we apply causal
inference, we only make associative claims and discuss assumptions needed for
causal interpretation.

</details>


### [112] [A Conceptual Framework for AI Capability Evaluations](https://arxiv.org/abs/2506.18213)
*María Victoria Carro,Denise Alejandra Mester,Francisca Gauna Selasco,Luca Nicolás Forziati Gangi,Matheo Sandleris Musa,Lola Ramos Pereyra,Mario Leiva,Juan Gustavo Corvalan,María Vanina Martinez,Gerardo Simari*

Main category: cs.AI

TL;DR: 提出一个概念框架，用于分析和系统化AI能力评估方法，支持透明性、可比性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 由于AI系统在社会中的广泛应用，透明且全面的评估方法成为AI治理的重要工具，但目前缺乏清晰的方法。

Method: 提出一个结构化的描述性框架，分析现有评估方法和术语，不引入新分类或固定格式。

Result: 框架支持评估的透明性、可比性和可解释性，帮助识别方法弱点、设计评估和政策制定。

Conclusion: 该框架为研究人员、实践者和政策制定者提供了一个实用工具，以更好地理解和比较复杂的AI评估。

Abstract: As AI systems advance and integrate into society, well-designed and
transparent evaluations are becoming essential tools in AI governance,
informing decisions by providing evidence about system capabilities and risks.
Yet there remains a lack of clarity on how to perform these assessments both
comprehensively and reliably. To address this gap, we propose a conceptual
framework for analyzing AI capability evaluations, offering a structured,
descriptive approach that systematizes the analysis of widely used methods and
terminology without imposing new taxonomies or rigid formats. This framework
supports transparency, comparability, and interpretability across diverse
evaluations. It also enables researchers to identify methodological weaknesses,
assists practitioners in designing evaluations, and provides policymakers with
an accessible tool to scrutinize, compare, and navigate complex evaluation
landscapes.

</details>


### [113] [The 4th Dimension for Scaling Model Size](https://arxiv.org/abs/2506.18233)
*Ruike Zhu,Hanwen Zhang,Tianyu Shi,Chi Wang,Tianyi Zhou,Zengyi Qin*

Main category: cs.AI

TL;DR: 本文探索了一种新的模型扩展维度——虚拟逻辑深度（VLD），通过参数重用在不增加参数总量的情况下提升模型的有效算法深度。研究发现，VLD扩展能保持知识容量几乎不变，显著提升推理能力，且参数数量与推理能力无关。


<details>
  <summary>Details</summary>
Motivation: 研究参数重用在模型扩展中的潜力及其特性，探索在不增加参数总量的情况下提升模型性能的方法。

Method: 通过精心设计的对照实验，研究虚拟逻辑深度（VLD）扩展对模型知识容量和推理能力的影响。

Result: VLD扩展能保持知识容量几乎不变，显著提升推理能力；参数数量与知识容量相关，但与推理能力无关。

Conclusion: VLD扩展是一种有效的模型扩展方法，可在不增加参数的情况下提升推理能力，适用于多种模型配置。

Abstract: Scaling the size of large language models typically involves three
dimensions: depth, width, and the number of parameters. In this work, we
explore a fourth dimension, virtual logical depth (VLD), which increases the
effective algorithmic depth without changing the overall parameter count by
reusing parameters within the model. Although parameter reuse is not a new
concept, its potential and characteristics in model scaling have not been
thoroughly studied. Through carefully designed controlled experiments, we make
the following key discoveries regarding VLD scaling:
  VLD scaling forces the knowledge capacity of the model to remain almost
constant, with only minor variations.
  VLD scaling enables a significant improvement in reasoning capability,
provided the scaling method is properly implemented.
  The number of parameters correlates with knowledge capacity, but not with
reasoning capability. Under certain conditions, it is not necessary to increase
the parameter count to enhance reasoning.
  These findings are consistent across various model configurations and are
likely to be generally valid within the scope of our experiments.

</details>


### [114] [Advanced For-Loop for QML algorithm search](https://arxiv.org/abs/2506.18260)
*FuTe Wong*

Main category: cs.AI

TL;DR: 论文提出了一种基于大型语言模型的多智能体系统（LLMMA）框架，用于自动搜索和优化量子机器学习（QML）算法。


<details>
  <summary>Details</summary>
Motivation: 受Google DeepMind的FunSearch启发，旨在通过智能体框架系统地探索经典机器学习概念，并将其适配到量子计算中。

Method: 利用LLMMA在抽象层面上迭代生成和优化经典机器学习算法的量子变换。

Result: 展示了智能体框架在自动化开发QML算法中的潜力。

Conclusion: 未来方向包括引入规划机制和优化搜索策略，以扩展量子增强机器学习的应用。

Abstract: This paper introduces an advanced framework leveraging Large Language
Model-based Multi-Agent Systems (LLMMA) for the automated search and
optimization of Quantum Machine Learning (QML) algorithms. Inspired by Google
DeepMind's FunSearch, the proposed system works on abstract level to
iteratively generates and refines quantum transformations of classical machine
learning algorithms (concepts), such as the Multi-Layer Perceptron,
forward-forward and backpropagation algorithms. As a proof of concept, this
work highlights the potential of agentic frameworks to systematically explore
classical machine learning concepts and adapt them for quantum computing,
paving the way for efficient and automated development of QML algorithms.
Future directions include incorporating planning mechanisms and optimizing
strategy in the search space for broader applications in quantum-enhanced
machine learning.

</details>


### [115] [Dynamic Knowledge Exchange and Dual-diversity Review: Concisely Unleashing the Potential of a Multi-Agent Research Team](https://arxiv.org/abs/2506.18348)
*Weilun Yu,Shixiang Tang,Yonggui Huang,Nanqing Dong,Li Fan,Honggang Qi,Wei Liu,Xiaoli Diao,Xi Chen,Wanli Ouyang*

Main category: cs.AI

TL;DR: IDVSCI是一个基于LLM的多智能体框架，通过动态知识交换和双多样性评审机制提升科学发现的深度和创造力。


<details>
  <summary>Details</summary>
Motivation: 现有LLM科学代理缺乏交互推理和评估机制，无法满足真实研究需求。

Method: 提出IDVSCI框架，包含动态知识交换和双多样性评审两大创新。

Result: 在计算机科学和健康科学数据集上表现优于现有系统。

Conclusion: 交互和同行评审动态建模对LLM自主研究具有重要价值。

Abstract: Scientific progress increasingly relies on effective collaboration among
researchers, a dynamic that large language models (LLMs) have only begun to
emulate. While recent LLM-based scientist agents show promise in autonomous
scientific discovery, they often lack the interactive reasoning and evaluation
mechanisms essential to real-world research. We propose IDVSCI (Internal
Discussion and Vote SCIentists), a multi-agent framework built on LLMs that
incorporates two key innovations: a Dynamic Knowledge Exchange mechanism
enabling iterative feedback among agents, and a Dual-Diversity Review paradigm
that simulates heterogeneous expert evaluation. These components jointly
promote deeper reasoning and the generation of more creative and impactful
scientific ideas. To evaluate the effectiveness and generalizability of our
approach, we conduct experiments on two datasets: a widely used benchmark in
computer science and a new dataset we introduce in the health sciences domain.
Results show that IDVSCI consistently achieves the best performance across both
datasets, outperforming existing systems such as AI Scientist and VIRSCI. These
findings highlight the value of modeling interaction and peer review dynamics
in LLM-based autonomous research.

</details>


### [116] [A Large Language Model-based Multi-Agent Framework for Analog Circuits' Sizing Relationships Extraction](https://arxiv.org/abs/2506.18424)
*Chengjie Liu,Weiyu Chen,Huiyao Xu,Yuan Du,Jun Yang,Li Du*

Main category: cs.AI

TL;DR: 提出了一种基于大语言模型（LLM）的多智能体框架，用于从学术论文中提取模拟电路的尺寸关系，以有效修剪搜索空间，提高优化效率。


<details>
  <summary>Details</summary>
Motivation: 现有技术在模拟电路尺寸设计中忽略了先验知识的自动引入，导致搜索空间未能有效修剪，存在较大压缩空间。

Method: 采用LLM多智能体框架从学术论文中提取尺寸关系，并基于此修剪搜索空间。

Result: 在3种电路上测试，优化效率提高了2.32至26.6倍。

Conclusion: LLM能有效修剪模拟电路尺寸设计的搜索空间，为LLM与传统模拟电路设计自动化方法的结合提供了新思路。

Abstract: In the design process of the analog circuit pre-layout phase, device sizing
is an important step in determining whether an analog circuit can meet the
required performance metrics. Many existing techniques extract the circuit
sizing task as a mathematical optimization problem to solve and continuously
improve the optimization efficiency from a mathematical perspective. But they
ignore the automatic introduction of prior knowledge, fail to achieve effective
pruning of the search space, which thereby leads to a considerable compression
margin remaining in the search space. To alleviate this problem, we propose a
large language model (LLM)-based multi-agent framework for analog circuits'
sizing relationships extraction from academic papers. The search space in the
sizing process can be effectively pruned based on the sizing relationship
extracted by this framework. Eventually, we conducted tests on 3 types of
circuits, and the optimization efficiency was improved by $2.32 \sim 26.6
\times$. This work demonstrates that the LLM can effectively prune the search
space for analog circuit sizing, providing a new solution for the combination
of LLMs and conventional analog circuit design automation methods.

</details>


### [117] [How Robust is Model Editing after Fine-Tuning? An Empirical Study on Text-to-Image Diffusion Models](https://arxiv.org/abs/2506.18428)
*Feng He,Zhenyang Liu,Marco Valentino,Zhixue Zhao*

Main category: cs.AI

TL;DR: 研究发现，模型编辑后的行为在微调后通常无法保持，尤其是DoRA微调方法对编辑的逆转效果最强，而UCE编辑方法相对更稳健。


<details>
  <summary>Details</summary>
Motivation: 探讨模型编辑与微调之间的相互作用，以解决编辑后的行为是否会在微调后保留的问题，这对AI安全和实际应用至关重要。

Method: 在T2I扩散模型（Stable Diffusion和FLUX）中，结合两种编辑技术（UCE和ReFACT）和三种微调方法（DreamBooth、LoRA和DoRA），通过多样化的编辑任务和评估指标进行实证分析。

Result: 编辑后的行为在微调后普遍失效，DoRA对编辑的逆转效果最强，UCE编辑方法在微调后保留的效果优于ReFACT。

Conclusion: 当前编辑方法在长期控制AI系统行为方面存在局限性，需开发更稳健的技术以确保安全性和一致性，同时微调可作为恶意编辑的补救手段，但需在微调后重新编辑以维持有益属性。

Abstract: Model editing offers a low-cost technique to inject or correct a particular
behavior in a pre-trained model without extensive retraining, supporting
applications such as factual correction and bias mitigation. Despite this
common practice, it remains unknown whether edits persist after fine-tuning or
whether they are inadvertently reversed. This question has fundamental
practical implications. For example, if fine-tuning removes prior edits, it
could serve as a defence mechanism against hidden malicious edits. Vice versa,
the unintended removal of edits related to bias mitigation could pose serious
safety concerns. We systematically investigate the interaction between model
editing and fine-tuning in the context of T2I diffusion models, which are known
to exhibit biases and generate inappropriate content. Our study spans two T2I
model families (Stable Diffusion and FLUX), two sota editing techniques, and
three fine-tuning methods (DreamBooth, LoRA, and DoRA). Through an extensive
empirical analysis across diverse editing tasks and evaluation metrics, our
findings reveal a trend: edits generally fail to persist through fine-tuning,
even when fine-tuning is tangential or unrelated to the edits. Notably, we
observe that DoRA exhibits the strongest edit reversal effect. At the same
time, among editing methods, UCE demonstrates greater robustness, retaining
significantly higher efficacy post-fine-tuning compared to ReFACT. These
findings highlight a crucial limitation in current editing methodologies,
emphasizing the need for more robust techniques to ensure reliable long-term
control and alignment of deployed AI systems. These findings have dual
implications for AI safety: they suggest that fine-tuning could serve as a
remediation mechanism for malicious edits while simultaneously highlighting the
need for re-editing after fine-tuning to maintain beneficial safety and
alignment properties.

</details>


### [118] [Standard Applicability Judgment and Cross-jurisdictional Reasoning: A RAG-based Framework for Medical Device Compliance](https://arxiv.org/abs/2506.18511)
*Yu Han,Aaron Ceross,Jeroen H. M. Bergmann*

Main category: cs.AI

TL;DR: 论文提出了一种模块化AI系统，利用检索增强生成（RAG）技术自动化确定医疗设备标准的适用性，准确率达73%，Top-5检索召回率为87%。


<details>
  <summary>Details</summary>
Motivation: 医疗设备合规性中，标准适用性判定依赖专家解读分散且异构的文档，亟需自动化解决方案。

Method: 采用RAG流程，结合检索候选标准和大型语言模型推断适用性（强制、推荐或不适用），并提供可追溯的合理性解释。

Result: 系统在分类准确率和检索召回率上表现优异，支持跨司法管辖区（如中美标准）的推理和冲突解决。

Conclusion: 该系统为可扩展且可解释的AI支持监管科学提供了首个端到端解决方案。

Abstract: Identifying the appropriate regulatory standard applicability remains a
critical yet understudied challenge in medical device compliance, frequently
necessitating expert interpretation of fragmented and heterogeneous
documentation across different jurisdictions. To address this challenge, we
introduce a modular AI system that leverages a retrieval-augmented generation
(RAG) pipeline to automate standard applicability determination. Given a
free-text device description, our system retrieves candidate standards from a
curated corpus and uses large language models to infer jurisdiction-specific
applicability, classified as Mandatory, Recommended, or Not Applicable, with
traceable justifications. We construct an international benchmark dataset of
medical device descriptions with expert-annotated standard mappings, and
evaluate our system against retrieval-only, zero-shot, and rule-based
baselines. The proposed approach attains a classification accuracy of 73% and a
Top-5 retrieval recall of 87%, demonstrating its effectiveness in identifying
relevant regulatory standards. We introduce the first end-to-end system for
standard applicability reasoning, enabling scalable and interpretable
AI-supported regulatory science. Notably, our region-aware RAG agent performs
cross-jurisdictional reasoning between Chinese and U.S. standards, supporting
conflict resolution and applicability justification across regulatory
frameworks.

</details>


### [119] [A Question Bank to Assess AI Inclusivity: Mapping out the Journey from Diversity Errors to Inclusion Excellence](https://arxiv.org/abs/2506.18538)
*Rifat Ara Shams,Didar Zowghi,Muneera Bano*

Main category: cs.AI

TL;DR: 论文提出一个包含253个问题的AI包容性问题库，用于评估AI系统的多样性及包容性，覆盖五个支柱：人类、数据、流程、系统和治理。


<details>
  <summary>Details</summary>
Motivation: 现有AI风险评估框架常忽视包容性，缺乏标准化工具衡量AI系统是否符合多样性及包容性原则。

Method: 采用多源迭代方法开发问题库，结合文献综述、D&I指南、负责任AI框架及模拟用户研究。

Result: 模拟评估显示问题库对多样角色和应用领域的AI包容性评估具有相关性和有效性。

Conclusion: 问题库为研究者、从业者和政策制定者提供了评估和提升AI包容性的实用工具，推动更公平、负责任的AI技术。

Abstract: Ensuring diversity and inclusion (D&I) in artificial intelligence (AI) is
crucial for mitigating biases and promoting equitable decision-making. However,
existing AI risk assessment frameworks often overlook inclusivity, lacking
standardized tools to measure an AI system's alignment with D&I principles.
This paper introduces a structured AI inclusivity question bank, a
comprehensive set of 253 questions designed to evaluate AI inclusivity across
five pillars: Humans, Data, Process, System, and Governance. The development of
the question bank involved an iterative, multi-source approach, incorporating
insights from literature reviews, D&I guidelines, Responsible AI frameworks,
and a simulated user study. The simulated evaluation, conducted with 70
AI-generated personas related to different AI jobs, assessed the question
bank's relevance and effectiveness for AI inclusivity across diverse roles and
application domains. The findings highlight the importance of integrating D&I
principles into AI development workflows and governance structures. The
question bank provides an actionable tool for researchers, practitioners, and
policymakers to systematically assess and enhance the inclusivity of AI
systems, paving the way for more equitable and responsible AI technologies.

</details>


### [120] [T-CPDL: A Temporal Causal Probabilistic Description Logic for Developing Logic-RAG Agent](https://arxiv.org/abs/2506.18559)
*Hong Qing Yu*

Main category: cs.AI

TL;DR: 提出了T-CPDL框架，结合时间、因果和概率逻辑，提升语言模型的结构化推理能力。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在时间约束、因果关系和概率推理上的不足。

Method: 扩展传统描述逻辑，引入时间区间算子、显式因果关系和概率标注，提出两种T-CPDL变体。

Result: 实验验证T-CPDL显著提升推理准确性、可解释性和置信度校准。

Conclusion: T-CPDL增强语言模型的透明推理能力，为Logic-RAG框架奠定基础。

Abstract: Large language models excel at generating fluent text but frequently struggle
with structured reasoning involving temporal constraints, causal relationships,
and probabilistic reasoning. To address these limitations, we propose Temporal
Causal Probabilistic Description Logic (T-CPDL), an integrated framework that
extends traditional Description Logic with temporal interval operators,
explicit causal relationships, and probabilistic annotations. We present two
distinct variants of T-CPDL: one capturing qualitative temporal relationships
through Allen's interval algebra, and another variant enriched with explicit
timestamped causal assertions. Both variants share a unified logical structure,
enabling complex reasoning tasks ranging from simple temporal ordering to
nuanced probabilistic causation. Empirical evaluations on temporal reasoning
and causal inference benchmarks confirm that T-CPDL substantially improves
inference accuracy, interpretability, and confidence calibration of language
model outputs. By delivering transparent reasoning paths and fine-grained
temporal and causal semantics, T-CPDL significantly enhances the capability of
language models to support robust, explainable, and trustworthy
decision-making. This work also lays the groundwork for developing advanced
Logic-Retrieval-Augmented Generation (Logic-RAG) frameworks, potentially
boosting the reasoning capabilities and efficiency of knowledge graph-enhanced
RAG systems.

</details>


### [121] [Airalogy: AI-empowered universal data digitization for research automation](https://arxiv.org/abs/2506.18586)
*Zijie Yang,Qiji Zhou,Fang Guo,Sijie Zhang,Yexun Xi,Jinglei Nie,Yudian Zhu,Liping Huang,Chou Wu,Yonghe Xia,Xiaoyu Ma,Yingming Pu,Panzhong Lu,Junshu Pan,Mingtao Chen,Tiannan Guo,Yanmei Dou,Hongyu Chen,Anping Zeng,Jiaxing Huang,Tian Xu,Yue Zhang*

Main category: cs.AI

TL;DR: Airalogy是一个多学科AI驱动平台，旨在标准化和数字化研究数据，解决当前数据分散、管理低效的问题，并支持AI应用。


<details>
  <summary>Details</summary>
Motivation: 当前AI应用受限于数据分散和缺乏标准化，阻碍了多学科AI赋能。

Method: 开发Airalogy平台，结合科学领域知识和计算技术，提供标准化数据记录和AI辅助功能。

Result: Airalogy已在西湖大学各学院实验室部署，有望加速科学创新。

Conclusion: Airalogy通过平衡通用性和标准化，推动多学科研究数据的AI应用，造福全球科研。

Abstract: Research data are the foundation of Artificial Intelligence (AI)-driven
science, yet current AI applications remain limited to a few fields with
readily available, well-structured, digitized datasets. Achieving comprehensive
AI empowerment across multiple disciplines is still out of reach. Present-day
research data collection is often fragmented, lacking unified standards,
inefficiently managed, and difficult to share. Creating a single platform for
standardized data digitization needs to overcome the inherent challenge of
balancing between universality (supporting the diverse, ever-evolving needs of
various disciplines) and standardization (enforcing consistent formats to fully
enable AI). No existing platform accommodates both facets. Building a truly
multidisciplinary platform requires integrating scientific domain knowledge
with sophisticated computing skills. Researchers often lack the computational
expertise to design customized and standardized data recording methods, whereas
platform developers rarely grasp the intricate needs of multiple scientific
domains. These gaps impede research data standardization and hamper AI-driven
progress. In this study, we address these challenges by developing Airalogy
(https://airalogy.com), the world's first AI- and community-driven platform
that balances universality and standardization for digitizing research data
across multiple disciplines. Airalogy represents entire research workflows
using customizable, standardized data records and offers an advanced AI
research copilot for intelligent Q&A, automated data entry, analysis, and
research automation. Already deployed in laboratories across all four schools
of Westlake University, Airalogy has the potential to accelerate and automate
scientific innovation in universities, industry, and the global research
community-ultimately benefiting humanity as a whole.

</details>


### [122] [AggTruth: Contextual Hallucination Detection using Aggregated Attention Scores in LLMs](https://arxiv.org/abs/2506.18628)
*Piotr Matys,Jan Eliasz,Konrad Kiełczyński,Mikołaj Langner,Teddy Ferdinan,Jan Kocoń,Przemysław Kazienko*

Main category: cs.AI

TL;DR: AggTruth是一种通过分析内部注意力分数分布来检测上下文幻觉的方法，在多种LLM中表现稳定，优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在RAG设置中产生幻觉的问题，提升模型部署的可靠性。

Method: 提出四种基于注意力分数聚合的变体方法，分析特征选择技术和注意力头数量对性能的影响。

Result: 在相同任务和跨任务设置中表现稳定，优于当前SOTA。

Conclusion: 注意力头的选择对检测性能至关重要，AggTruth在多种场景下有效。

Abstract: In real-world applications, Large Language Models (LLMs) often hallucinate,
even in Retrieval-Augmented Generation (RAG) settings, which poses a
significant challenge to their deployment. In this paper, we introduce
AggTruth, a method for online detection of contextual hallucinations by
analyzing the distribution of internal attention scores in the provided context
(passage). Specifically, we propose four different variants of the method, each
varying in the aggregation technique used to calculate attention scores. Across
all LLMs examined, AggTruth demonstrated stable performance in both same-task
and cross-task setups, outperforming the current SOTA in multiple scenarios.
Furthermore, we conducted an in-depth analysis of feature selection techniques
and examined how the number of selected attention heads impacts detection
performance, demonstrating that careful selection of heads is essential to
achieve optimal results.

</details>


### [123] [Dual-level Behavioral Consistency for Inter-group and Intra-group Coordination in Multi-Agent Systems](https://arxiv.org/abs/2506.18651)
*Shuocun Yang,Huawen Hu,Enze Shi,Shu Zhang*

Main category: cs.AI

TL;DR: 论文提出了一种名为DLBC的新型多智能体强化学习方法，通过动态调节组内和组间的行为一致性，提升协作效率和任务分工。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注组内行为一致性，而忽视了多智能体分组场景中的行为一致性，因此需要一种方法同时调节组内和组间的行为多样性。

Method: DLBC将智能体分组，动态调节组内和组间的行为多样性，通过直接约束策略函数实现广泛适用性。

Result: 实验表明，DLBC显著提升了组内协作和组间任务分工，性能大幅改善。

Conclusion: DLBC为多智能体系统的行为一致性控制提供了新思路，未来可探索其在更复杂任务中的应用。

Abstract: Behavioral diversity in Multi-agent reinforcement learning(MARL) represents
an emerging and promising research area. Prior work has largely centered on
intra-group behavioral consistency in multi-agent systems, with limited
attention given to behavioral consistency in multi-agent grouping scenarios. In
this paper, we introduce Dual-Level Behavioral Consistency (DLBC), a novel MARL
control method designed to explicitly regulate agent behaviors at both
intra-group and inter-group levels. DLBC partitions agents into distinct groups
and dynamically modulates behavioral diversity both within and between these
groups. By dynamically modulating behavioral diversity within and between these
groups, DLBC achieves enhanced division of labor through inter-group
consistency, which constrains behavioral strategies across different groups.
Simultaneously, intra-group consistency, achieved by aligning behavioral
strategies within each group, fosters stronger intra-group cooperation.
Crucially, DLBC's direct constraint of agent policy functions ensures its broad
applicability across various algorithmic frameworks. Experimental results in
various grouping cooperation scenarios demonstrate that DLBC significantly
enhances both intra-group cooperative performance and inter-group task
specialization, yielding substantial performance improvements. DLBC provides
new ideas for behavioral consistency control of multi-intelligent body systems,
and its potential for application in more complex tasks and dynamic
environments can be further explored in the future.

</details>


### [124] [Programming by Backprop: LLMs Acquire Reusable Algorithmic Abstractions During Code Training](https://arxiv.org/abs/2506.18777)
*Jonathan Cook,Silvia Sapora,Arash Ahmadian,Akbir Khan,Tim Rocktaschel,Jakob Foerster,Laura Ruis*

Main category: cs.AI

TL;DR: 论文提出Programming by Backprop (PBB)方法，通过仅训练源代码（无输入输出示例）提升大语言模型（LLMs）的泛化推理能力，并验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs通过源代码训练提升泛化推理能力的机制，尤其是无输入输出示例时的表现。

Method: 通过两组程序（带输入输出示例和不带输入输出示例）微调LLMs，比较其表现，并分析PBB的效果。

Result: LLMs能在无输入输出示例时评估程序，且PBB在代码形式下效果更好，还能通过链式思考更可靠地生成输出。

Conclusion: 代码训练使LLMs内化可重用算法抽象，未来可进一步优化符号程序学习，并探索模型对齐等应用。

Abstract: Training large language models (LLMs) on source code significantly enhances
their general-purpose reasoning abilities, but the mechanisms underlying this
generalisation are poorly understood. In this paper, we propose Programming by
Backprop (PBB) as a potential driver of this effect - teaching a model to
evaluate a program for inputs by training on its source code alone, without
ever seeing I/O examples. To explore this idea, we finetune LLMs on two sets of
programs representing simple maths problems and algorithms: one with source
code and I/O examples (w/ IO), the other with source code only (w/o IO). We
find evidence that LLMs have some ability to evaluate w/o IO programs for
inputs in a range of experimental settings, and make several observations.
Firstly, PBB works significantly better when programs are provided as code
rather than semantically equivalent language descriptions. Secondly, LLMs can
produce outputs for w/o IO programs directly, by implicitly evaluating the
program within the forward pass, and more reliably when stepping through the
program in-context via chain-of-thought. We further show that PBB leads to more
robust evaluation of programs across inputs than training on I/O pairs drawn
from a distribution that mirrors naturally occurring data. Our findings suggest
a mechanism for enhanced reasoning through code training: it allows LLMs to
internalise reusable algorithmic abstractions. Significant scope remains for
future work to enable LLMs to more effectively learn from symbolic procedures,
and progress in this direction opens other avenues like model alignment by
training on formal constitutional principles.

</details>


### [125] [TRIZ Agents: A Multi-Agent LLM Approach for TRIZ-Based Innovation](https://arxiv.org/abs/2506.18783)
*Kamil Szczepanik,Jarosław A. Chudziak*

Main category: cs.AI

TL;DR: 论文提出了一种基于大型语言模型（LLM）的多智能体系统（TRIZ agents），用于协作解决TRIZ方法论中的创新问题，并通过工程案例验证其有效性。


<details>
  <summary>Details</summary>
Motivation: TRIZ方法的应用常受限于其复杂性和跨学科知识需求，而LLM的进展为自动化部分流程提供了新可能。

Method: 提出多智能体系统，每个智能体具备专业能力和工具访问权限，协作执行TRIZ步骤。

Result: 通过工程案例验证，多智能体系统能高效解决复杂创新问题，生成多样化的创新解决方案。

Conclusion: 研究表明，分散式问题解决在复杂创新任务中具有优势，为AI驱动的创新提供了新方向。

Abstract: TRIZ, the Theory of Inventive Problem Solving, is a structured,
knowledge-based framework for innovation and abstracting problems to find
inventive solutions. However, its application is often limited by the
complexity and deep interdisciplinary knowledge required. Advancements in Large
Language Models (LLMs) have revealed new possibilities for automating parts of
this process. While previous studies have explored single LLMs in TRIZ
applications, this paper introduces a multi-agent approach. We propose an
LLM-based multi-agent system, called TRIZ agents, each with specialized
capabilities and tool access, collaboratively solving inventive problems based
on the TRIZ methodology. This multi-agent system leverages agents with various
domain expertise to efficiently navigate TRIZ steps. The aim is to model and
simulate an inventive process with language agents. We assess the effectiveness
of this team of agents in addressing complex innovation challenges based on a
selected case study in engineering. We demonstrate the potential of agent
collaboration to produce diverse, inventive solutions. This research
contributes to the future of AI-driven innovation, showcasing the advantages of
decentralized problem-solving in complex ideation tasks.

</details>


### [126] [ConciseHint: Boosting Efficient Reasoning via Continuous Concise Hints during Generation](https://arxiv.org/abs/2506.18810)
*Siao Tang,Xinyin Ma,Gongfan Fang,Xinchao Wang*

Main category: cs.AI

TL;DR: 论文提出ConciseHint框架，通过在推理过程中注入文本提示，鼓励模型生成简洁的推理过程，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）在复杂推理任务中表现优异，但推理过程冗长导致效率问题。现有方法未关注在生成过程中直接干预以提升简洁性。

Method: 提出ConciseHint框架，通过在推理过程中动态注入文本提示（手动设计或基于简洁数据训练），并根据查询复杂度自适应调整提示强度。

Result: 在DeepSeek-R1和Qwen-3等模型上实验，推理长度减少65%（GSM8K基准），且几乎无准确率损失。

Conclusion: ConciseHint能有效生成简洁推理过程，同时保持模型性能，填补了现有方法的空白。

Abstract: Recent advancements in large reasoning models (LRMs) like DeepSeek-R1 and
OpenAI o1 series have achieved notable performance enhancements on complex
reasoning tasks by scaling up the generation length by Chain-of-Thought (CoT).
However, an emerging issue is their inclination to produce excessively verbose
reasoning processes, leading to the inefficiency problem. Existing literature
on improving efficiency mainly adheres to the before-reasoning paradigms such
as prompting and reasoning or fine-tuning and reasoning, but ignores the
promising direction of directly encouraging the model to speak concisely by
intervening during the generation of reasoning. In order to fill the blank, we
propose a framework dubbed ConciseHint, which continuously encourages the
reasoning model to speak concisely by injecting the textual hint (manually
designed or trained on the concise data) during the token generation of the
reasoning process. Besides, ConciseHint is adaptive to the complexity of the
query by adaptively adjusting the hint intensity, which ensures it will not
undermine model performance. Experiments on the state-of-the-art LRMs,
including DeepSeek-R1 and Qwen-3 series, demonstrate that our method can
effectively produce concise reasoning processes while maintaining performance
well. For instance, we achieve a reduction ratio of 65\% for the reasoning
length on GSM8K benchmark with Qwen-3 4B with nearly no accuracy loss.

</details>


### [127] [Steering Conceptual Bias via Transformer Latent-Subspace Activation](https://arxiv.org/abs/2506.18887)
*Vansh Sharma,Venkat Raman*

Main category: cs.AI

TL;DR: 通过激活语言模型中的潜在子空间，研究成功引导科学代码生成偏向特定编程语言，提出了一种梯度优化的自适应激活框架（G-ACT），显著提升了语言选择的准确性。


<details>
  <summary>Details</summary>
Motivation: 探讨如何通过激活语言模型的潜在子空间，实现对科学代码生成中编程语言选择的精确控制。

Method: 开发了G-ACT框架，通过聚类每提示的激活差异并训练轻量级探测器，选择最佳激活方向。

Result: 在LLaMA-3.2 3B模型中，G-ACT显著提升了CPP语言的生成偏向，分类准确率提高了15%，早期层提升了61.5%。

Conclusion: G-ACT提供了一种可扩展、可解释且高效的概念级控制机制，适用于实际代理系统。

Abstract: This work examines whether activating latent subspaces in language models
(LLMs) can steer scientific code generation toward a specific programming
language. Five causal LLMs were first evaluated on scientific coding prompts to
quantify their baseline bias among four programming languages. A static
neuron-attribution method, perturbing the highest activated MLP weight for a
C++ or CPP token, proved brittle and exhibited limited generalization across
prompt styles and model scales. To address these limitations, a
gradient-refined adaptive activation steering framework (G-ACT) was developed:
per-prompt activation differences are clustered into a small set of steering
directions, and lightweight per-layer probes are trained and refined online to
select the appropriate steering vector. In LLaMA-3.2 3B, this approach reliably
biases generation towards the CPP language by increasing the average probe
classification accuracy by 15% and the early layers (0-6) improving the probe
classification accuracy by 61.5% compared to the standard ACT framework. For
LLaMA-3.3 70B, where attention-head signals become more diffuse, targeted
injections at key layers still improve language selection. Although per-layer
probing introduces a modest inference overhead, it remains practical by
steering only a subset of layers and enables reproducible model behavior. These
results demonstrate a scalable, interpretable and efficient mechanism for
concept-level control for practical agentic systems.

</details>


### [128] [jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval](https://arxiv.org/abs/2506.18902)
*Michael Günther,Saba Sturua,Mohammad Kalim Akram,Isabelle Mohr,Andrei Ungureanu,Sedigheh Eslami,Scott Martens,Bo Wang,Nan Wang,Han Xiao*

Main category: cs.AI

TL;DR: jina-embeddings-v4是一个3.8亿参数的多模态嵌入模型，通过新颖的架构统一文本和图像表示，支持单向量和多向量嵌入，并在多种检索任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决文本和图像表示的统一问题，并优化跨模态检索任务的性能。

Method: 采用任务特定的LoRA适配器，支持单向量和多向量嵌入，适用于多种检索场景。

Result: 在单模态和跨模态检索任务中达到最先进性能，尤其在处理视觉丰富内容时表现出色。

Conclusion: jina-embeddings-v4在多模态检索任务中具有显著优势，并提供了新的视觉丰富图像检索基准Jina-VDR。

Abstract: We introduce jina-embeddings-v4, a 3.8 billion parameter multimodal embedding
model that unifies text and image representations through a novel architecture
supporting both single-vector and multi-vector embeddings in the late
interaction style. The model incorporates task-specific Low-Rank Adaptation
(LoRA) adapters to optimize performance across diverse retrieval scenarios,
including query-based information retrieval, cross-modal semantic similarity,
and programming code search. Comprehensive evaluations demonstrate that
jina-embeddings-v4 achieves state-of-the-art performance on both single- modal
and cross-modal retrieval tasks, with particular strength in processing
visually rich content such as tables, charts, diagrams, and mixed-media
formats. To facilitate evaluation of this capability, we also introduce
Jina-VDR, a novel benchmark specifically designed for visually rich image
retrieval.

</details>
