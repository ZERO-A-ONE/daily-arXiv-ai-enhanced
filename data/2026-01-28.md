<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 20]
- [cs.AI](#cs.AI) [Total: 47]
- [cs.SE](#cs.SE) [Total: 16]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [From Scores to Queues: Operationalizing Cross-Chain Obfuscation Signals for Smart-Contract Audits](https://arxiv.org/abs/2601.17356)
*Yao Zhao,Zhang Sheng,Shengchen Duan,Shen Wang*

Main category: cs.CR

TL;DR: HObfNET是一个高效的智能合约混淆检测模型，相比Obfs_Tool工具实现了2300-5200倍的加速，能够在跨链环境中快速评估混淆程度，并提出了实用的审计队列策略。


<details>
  <summary>Details</summary>
Motivation: 智能合约混淆增加了审计成本，而跨链混淆信号的可比性和可迁移性尚不明确，需要高效的工具来支持大规模跨链安全评估。

Method: 提出HObfNET作为Obfs_Tool的高效替代模型，实现快速跨链评分；使用主阈值和极端阈值(p99和p99.9)策略，分析高评分队列的特征模式。

Result: 模型在以太坊上与工具输出高度一致(PCC 0.9158, MAPE 8.20%)，处理速度达8-9ms/合约；发现跨链存在系统性评分漂移，高评分尾部显示稀有选择器、外部调用操作码富集等特征；公开事件样本均落入p99队列。

Conclusion: HObfNET为多链安全操作提供了实用的两层级审计队列和跨链关联工作流，能够有效识别高风险合约并支持优先级排序。

Abstract: Obfuscation substantially increases the interpretation cost of smart-contract auditing, while the comparability and transferability of obfuscation signals across chains remain unclear. We present HObfNET as an efficient surrogate of Obfs_Tool (ObfProbe), enabling fast cross-chain scoring at scale. The model aligns well with tool outputs on Ethereum (PCC 0.9158, MAPE 8.20 percent) and achieves 8-9 ms per contract, a 2.3k-5.2k times speedup over second-level Obfs_Tool runs, enabling million-scale scoring. On large BSC, Polygon, and Avalanche corpora, we find systematic score drift: fixed-threshold transfer inflates and deflates candidate queues, motivating within-chain main and extreme thresholds (p99 and p99.9) and an actionable queueing strategy. The high-score tail exhibits rare selectors, external-call opcode enrichment, and low signature density; a proxy indicator is enriched in the BSC high-score queue, enabling secondary triage. Cross-chain reuse analysis shows tail enrichment and directional diffusion, with traceable same-hash cases across chains. In publicly alignable incident samples, all fall into the p99 queue; Transit Swap DEX Hack and New Free DAO Flash Loan exhibit cross-chain spillover, indicating real-world hit and prioritization value. We deliver a two-tier audit queue and cross-chain linkage workflow to support practical multi-chain security operations.

</details>


### [2] [Res-MIA: A Training-Free Resolution-Based Membership Inference Attack on Federated Learning Models](https://arxiv.org/abs/2601.17378)
*Mohammad Zare,Pirooz Shamsinejadbabaki*

Main category: cs.CR

TL;DR: 提出Res-MIA攻击方法，利用深度学习模型对高频输入细节的敏感性，通过渐进降低输入分辨率并分析置信度衰减来推断训练数据成员身份，在联邦学习场景中无需影子模型或辅助数据即可实现高效攻击。


<details>
  <summary>Details</summary>
Motivation: 尽管联邦学习因其去中心化特性被视为隐私保护训练范式，但研究表明最终全局模型仍可能通过黑盒访问泄露成员信息。现有成员推理攻击方法通常需要影子模型或大量辅助数据，存在计算成本高、实用性受限的问题。

Method: Res-MIA采用训练无关的黑盒攻击方法，通过受控下采样和恢复操作渐进降低输入分辨率，分析模型预测置信度的衰减模式。核心洞察是训练样本在分辨率侵蚀下表现出比非成员样本更陡峭的置信度下降，这揭示了稳健的成员信号。

Result: 在联邦ResNet-18模型和CIFAR-10数据集上的评估显示，Res-MIA持续优于现有训练无关基线方法，AUC最高可达0.88，且计算开销极小。该方法仅需有限的前向查询即可实现高效攻击。

Conclusion: 研究揭示了频率敏感过拟合是联邦学习中重要但先前未被充分探索的隐私泄露源，强调了需要设计减少对细粒度、非稳健输入特征依赖的隐私感知模型架构。

Abstract: Membership inference attacks (MIAs) pose a serious threat to the privacy of machine learning models by allowing adversaries to determine whether a specific data sample was included in the training set. Although federated learning (FL) is widely regarded as a privacy-aware training paradigm due to its decentralized nature, recent evidence shows that the final global model can still leak sensitive membership information through black-box access. In this paper, we introduce Res-MIA, a novel training-free and black-box membership inference attack that exploits the sensitivity of deep models to high-frequency input details. Res-MIA progressively degrades the input resolution using controlled downsampling and restoration operations, and analyzes the resulting confidence decay in the model's predictions. Our key insight is that training samples exhibit a significantly steeper confidence decline under resolution erosion compared to non-member samples, revealing a robust membership signal. Res-MIA requires no shadow models, no auxiliary data, and only a limited number of forward queries to the target model. We evaluate the proposed attack on a federated ResNet-18 trained on CIFAR-10, where it consistently outperforms existing training-free baselines and achieves an AUC of up to 0.88 with minimal computational overhead. These findings highlight frequency-sensitive overfitting as an important and previously underexplored source of privacy leakage in federated learning, and emphasize the need for privacy-aware model designs that reduce reliance on fine-grained, non-robust input features.

</details>


### [3] [Prompt and Circumstances: Evaluating the Efficacy of Human Prompt Inference in AI-Generated Art](https://arxiv.org/abs/2601.17379)
*Khoi Trinh,Scott Seidenberger,Joseph Spracklen,Raveen Wijewickrama,Bimal Viswanath,Murtuza Jadliwala,Anindya Maiti*

Main category: cs.CR

TL;DR: 该研究探讨AI艺术提示词市场中的知识产权问题，通过实验评估人类和AI能否从公开样本图像推断原始提示词，以及人机协作提示词推断的效果。


<details>
  <summary>Details</summary>
Motivation: AI艺术提示词市场声称对提示词拥有知识产权，但人类和AI工具可能通过公开的样本图像推断出原始提示词。本研究旨在探讨这些隐藏的提示词是否真正构成知识产权，以及人机协作能否提高提示词推断的准确性。

Method: 研究包含两个主要部分：(1) 评估人类仅通过观察AI生成图像来推断原始提示词的准确性；(2) 探索通过大型语言模型结合人类和AI推断的提示词，创建混合提示词的方法。研究首次结合了人类主体实验并深入探讨了人机协作提示词推断。

Result: 研究发现，人类推断的提示词以及人机协作推断的提示词生成的图像与原始图像有中等程度的相似性，但不如使用原始提示词成功。此外，使用建议的合并技术结合人类和AI推断的提示词，其性能并未超过纯人类推断的提示词。

Conclusion: 尽管人类和AI能够从样本图像中推断出有一定相似性的提示词，但这些推断的提示词无法完全复制原始提示词的效果。这为提示词市场中的知识产权主张提供了一定的支持，但同时也揭示了现有保护措施的局限性。

Abstract: The emerging field of AI-generated art has witnessed the rise of prompt marketplaces, where creators can purchase, sell, or share prompts to generate unique artworks. These marketplaces often assert ownership over prompts, claiming them as intellectual property. This paper investigates whether concealed prompts sold on prompt marketplaces can be considered bona fide intellectual property, given that humans and AI tools may be able to infer the prompts based on publicly advertised sample images accompanying each prompt on sale. Specifically, our study aims to assess (i) how accurately humans can infer the original prompt solely by examining an AI-generated image, with the goal of generating images similar to the original image, and (ii) the possibility of improving upon individual human and AI prompt inferences by crafting combined human and AI prompts with the help of a large language model. Although previous research has explored AI-driven prompt inference and protection strategies, our work is the first to incorporate a human subject study and examine collaborative human-AI prompt inference in depth. Our findings indicate that while prompts inferred by humans and prompts inferred through a combined human and AI effort can generate images with a moderate level of similarity, they are not as successful as using the original prompt. Moreover, combining human- and AI-inferred prompts using our suggested merging techniques did not improve performance over purely human-inferred prompts.

</details>


### [4] [On the Impossibility of Simulation Security for Quantum Functional Encryption](https://arxiv.org/abs/2601.17497)
*Mohammed Barhoush,Arthur Mehta,Anne Müller,Louis Salvail*

Main category: cs.CR

TL;DR: 该论文证明了在量子环境下模拟安全的功能加密同样是不可能的，扩展了经典环境中的不可能性结果


<details>
  <summary>Details</summary>
Motivation: 功能加密是一种强大的密码学原语，但理想的模拟安全在经典环境中已被证明不可能。然而，这些不可能性证明依赖于经典论证，因此需要研究在量子环境中是否可能实现模拟安全的功能加密。

Method: 通过将经典不可能性结果扩展到量子世界，针对不同攻击场景：1）当攻击者可以发出无限数量的挑战消息时，证明无条件不可能性；2）当攻击者可以获得多个功能密钥时，基于伪随机量子状态或公钥加密的假设证明不可能性。证明中展示了伪随机状态的新颖不可压缩性。

Result: 1）在无限挑战消息场景下，证明与经典环境相同的无条件不可能性；2）在多个功能密钥场景下，基于伪随机量子状态（比经典伪随机函数更弱）或公钥加密的假设证明不可能性，为不可能性提供了独立证据。

Conclusion: 量子环境中的模拟安全功能加密同样是不可能的，经典不可能性结果在很大程度上扩展到量子世界。论文还展示了伪随机状态的不可压缩性，这可能具有独立的研究价值。

Abstract: Functional encryption is a powerful cryptographic primitive that enables fine-grained access to encrypted data and underlies numerous applications. Although the ideal security notion for FE (simulation security) has been shown to be impossible in the classical setting, those impossibility results rely on inherently classical arguments. This leaves open the question of whether simulation-secure functional encryption can be achieved in the quantum regime.
  In this work, we rule out this possibility by showing that the classical impossibility results largely extend to the quantum world. In particular, when the adversary can issue an unbounded number of challenge messages, we prove an unconditional impossibility, matching the classical barrier. In the case where the adversary may obtain many functional keys, classical arguments only yield impossibility under the assumption of pseudorandom functions; we strengthen this by proving impossibility under the potentially weaker assumption of pseudorandom quantum states. In the same setting, we also establish an alternative impossibility based on public-key encryption. Since public-key encryption is not known to imply pseudorandom quantum states, this provides independent evidence of the barrier. As part of our proofs, we show a novel incompressibility property for pseudorandom states, which may be of independent interest.

</details>


### [5] [Prompt Injection Attacks on Agentic Coding Assistants: A Systematic Analysis of Vulnerabilities in Skills, Tools, and Protocol Ecosystems](https://arxiv.org/abs/2601.17548)
*Narek Maloyan,Dmitry Namiot*

Main category: cs.CR

TL;DR: 本文系统分析了面向智能编码助手的提示注入攻击，提出了三维分类法，综合了78项研究，发现现有防御对自适应攻击的缓解率不足50%，强调需要架构级而非临时过滤的解决方案。


<details>
  <summary>Details</summary>
Motivation: 随着Claude Code、GitHub Copilot、Cursor等智能编码助手的普及，这些系统通过MCP等协议集成外部工具、文件系统和shell访问，扩展的能力表面引入了关键安全漏洞。需要系统研究针对智能编码助手的提示注入攻击，以理解其安全威胁。

Method: 提出新颖的三维分类法，从传播向量、攻击模式和传播行为三个维度对攻击进行分类。对78项近期研究（2021-2026）进行元分析，系统整理了42种不同的攻击技术，并批判性分析了18种防御机制。

Result: 研究发现：采用自适应攻击策略时，对最先进防御的攻击成功率超过85%；大多数防御机制对复杂自适应攻击的缓解率低于50%；需要将提示注入视为需要架构级缓解的一类漏洞。

Conclusion: 安全社区必须将提示注入视为需要架构级缓解而非临时过滤方法的一类重要漏洞。贡献包括：统一的分类法、基于技能架构漏洞的首个系统分析、基于已识别局限性的深度防御框架。

Abstract: The proliferation of agentic AI coding assistants, including Claude Code, GitHub Copilot, Cursor, and emerging skill-based architectures, has fundamentally transformed software development workflows. These systems leverage Large Language Models (LLMs) integrated with external tools, file systems, and shell access through protocols like the Model Context Protocol (MCP). However, this expanded capability surface introduces critical security vulnerabilities. In this \textbf{Systematization of Knowledge (SoK)} paper, we present a comprehensive analysis of prompt injection attacks targeting agentic coding assistants. We propose a novel three-dimensional taxonomy categorizing attacks across \textit{delivery vectors}, \textit{attack modalities}, and \textit{propagation behaviors}. Our meta-analysis synthesizes findings from 78 recent studies (2021--2026), consolidating evidence that attack success rates against state-of-the-art defenses exceed 85\% when adaptive attack strategies are employed. We systematically catalog 42 distinct attack techniques spanning input manipulation, tool poisoning, protocol exploitation, multimodal injection, and cross-origin context poisoning. Through critical analysis of 18 defense mechanisms reported in prior work, we identify that most achieve less than 50\% mitigation against sophisticated adaptive attacks. We contribute: (1) a unified taxonomy bridging disparate attack classifications, (2) the first systematic analysis of skill-based architecture vulnerabilities with concrete exploit chains, and (3) a defense-in-depth framework grounded in the limitations we identify. Our findings indicate that the security community must treat prompt injection as a first-class vulnerability class requiring architectural-level mitigations rather than ad-hoc filtering approaches.

</details>


### [6] [Private Iris Recognition with High-Performance FHE](https://arxiv.org/abs/2601.17561)
*Jincheol Ha,Guillaume Hanrot,Taeyeong Noh,Jung Hee Cheon,Jung Woo Kim,Damien Stehlé*

Main category: cs.CR

TL;DR: 本文探索使用阈值全同态加密（ThFHE）替代之前的秘密共享多方计算（SS-MPC）方案，用于大规模虹膜识别系统的隐私保护匹配。ThFHE方案在保持高效性能的同时，提供了更强的安全优势：无需可信设置、数据库和查询可公开、密钥可分布式管理、支持主动安全。


<details>
  <summary>Details</summary>
Motivation: 随着World ID等大规模虹膜识别项目的兴起，生物特征数据的隐私保护成为关键问题。现有的基于秘密共享多方计算的方案虽然有效，但在安全模型上存在限制。ThFHE技术能够提供更强的安全保证，包括无可信设置、公开可验证性等优势，值得探索其在虹膜识别隐私保护中的应用。

Method: 采用阈值全同态加密（ThFHE）方案，具体使用CKKS同态加密方案。关键技术包括：利用基于FHE的线性代数最新进展，通过int8 GPU操作加速；引入早期减少待处理密文数量的技术；在8块RTX-5090 GPU上实现概念验证系统。

Result: 概念验证实现能够在约1.8秒内完成32个虹膜与7×2^14规模数据库的匹配（对于4个虹膜匹配相同数据库仅需约0.33秒）。通信轮次仅需2-3轮，相比之前的40多轮大幅减少。系统在保持高效性能的同时，提供了更强的安全特性。

Conclusion: 阈值全同态加密方案为大规模虹膜识别系统的隐私保护提供了有前景的替代方案，在保持高性能的同时显著提升了安全特性，包括无可信设置、公开可验证性、分布式密钥管理和主动安全支持等优势。

Abstract: Among biometric verification systems, irises stand out because they offer high accuracy even in large-scale databases. For example, the World ID project aims to provide authentication to all humans via iris recognition, with millions already registered. Storing such biometric data raises privacy concerns, which can be addressed using privacy-enhancing techniques.
  Bloemen et al. describe a solution based on 2-out-of-3 Secret-Sharing Multiparty Computation (SS-MPC), for the World ID setup. In terms of security, unless an adversary corrupts 2~servers, the iris codes remain confidential and nothing leaks beyond the result of the computation. Their solution is able to match~$32$ users against a database of~$2^{22}$ iris codes in~$\approx 2$s , using~24 H100 GPUs, more than 40~communication rounds and $81$GB/party of data transferred (the timing assumes a network speed above~3Tb/s).
  In the present work, we explore the use of Threshold Fully Homomorphic Encryption (ThFHE) for the same task. The ThFHE solution brings a number of security advantages: no trusted setup, the encrypted database and queries can be public, the secret can be distributed among many parties, and active security can be added without significant performance degradation.
  Our proof-of-concept implementation of the computation phase handles $32$~eyes against a database of $7\cdot 2^{14}$ iris codes in~$\approx 1.8$s ($\approx 0.33s$ for 4 eyes against the same database), using 8 RTX-5090 GPUs. To this, one should add~2 to 3 rounds of communication (depending on deployment choice). We perform the matching using the CKKS (Th)FHE scheme. Our main technical ingredients are the use of recent progress on FHE-based linear algebra boosted using int8 GPU operations, and the introduction of a technique reducing the number of ciphertexts to be processed as early as possible.

</details>


### [7] [Reconstructing Protected Biometric Templates from Binary Authentication Results](https://arxiv.org/abs/2601.17620)
*Eliron Rahimi,Margarita Osadchy,Orr Dunkelman*

Main category: cs.CR

TL;DR: 论文提出一种针对生物特征模板保护系统的攻击方法，仅通过观察认证尝试的成功/失败结果，就能重建生物特征模板，最终生成可通过系统认证的高分辨率面部图像。


<details>
  <summary>Details</summary>
Motivation: 生物特征数据非常私密且高度敏感，现有保护方法（如生物哈希、模糊承诺、全同态加密等）在攻击者能够注入样本并观察系统输出的情况下，其保护能力存在疑问。特别是对于仅返回认证成功/失败结果的系统，攻击可行性尚未明确。

Method: 通过注入足够数量的模板，仅观察认证尝试的成功/失败结果，利用生成式反演方法重建生物特征模板，形成从二进制分数到高分辨率面部图像的完整攻击流程。

Result: 攻击实现了可忽略的模板重建损失，能够完全恢复面部图像，生成的面部图像在系统中通过率超过98%。该方法适用于任何保持识别准确性的保护机制。

Conclusion: 研究表明，仅通过观察认证成功/失败结果就能重建生物特征模板，这对现有生物特征保护系统构成严重威胁，强调了需要更强大的保护机制来抵御此类攻击。

Abstract: Biometric data is considered to be very private and highly sensitive. As such, many methods for biometric template protection were considered over the years -- from biohashing and specially crafted feature extraction procedures, to the use of cryptographic solutions such as Fuzzy Commitments or the use of Fully Homomorphic Encryption (FHE).
  A key question that arises is how much protection these solutions can offer when the adversary can inject samples, and observe the outputs of the system. While for systems that return the similarity score, one can use attacks such as hill-climbing, for systems where the adversary can only learn whether the authentication attempt was successful, this question remained open.
  In this paper, we show that it is indeed possible to reconstruct the biometric template by just observing the success/failure of the authentication attempt (given the ability to inject a sufficient amount of templates). Our attack achieves negligible template reconstruction loss and enables full recovery of facial images through a generative inversion method, forming a pipeline from binary scores to high-resolution facial images that successfully pass the system more than 98\% of the time. Our results, of course, are applicable for any protection mechanism that maintains the accuracy of the recognition.

</details>


### [8] [FOCA: Multimodal Malware Classification via Hyperbolic Cross-Attention](https://arxiv.org/abs/2601.17638)
*Nitin Choudhury,Bikrant Bikram Pratap Maurya,Orchid Chetia Phukan,Arun Balaji Buduru*

Main category: cs.CR

TL;DR: FOCA是一个新颖的多模态恶意软件分类框架，首次在双曲空间中利用音频和视觉表示之间的层次关系，超越了传统的欧几里得融合方法。


<details>
  <summary>Details</summary>
Motivation: 传统基于欧几里得空间的多模态融合方法无法有效捕捉音频和视觉表示之间的内在层次关系，需要一种能够更好建模这种层次结构的新方法。

Method: FOCA包含三个核心组件：1) 将欧几里得嵌入映射到庞加莱球的双曲投影模块；2) 在曲率感知约束下对齐多模态依赖关系的双曲交叉注意力机制；3) 基于莫比乌斯加法的融合层。

Result: 在Mal-Net和CICMalDroid2020两个基准数据集上的实验表明，FOCA始终优于单模态模型，超越了大多数欧几里得多模态基线，并取得了最先进的性能。

Conclusion: FOCA通过将多模态融合引入双曲空间，有效捕捉了音频和视觉表示之间的层次关系，为恶意软件分类提供了新的有效方法。

Abstract: In this work, we introduce FOCA, a novel multimodal framework for malware classification that jointly leverages audio and visual modalities. Unlike conventional Euclidean-based fusion methods, FOCA is the first to exploit the intrinsic hierarchical relationships between audio and visual representations within hyperbolic space. To achieve this, raw binaries are transformed into both audio and visual representations, which are then processed through three key components: (i) a hyperbolic projection module that maps Euclidean embeddings into the Poincare ball, (ii) a hyperbolic cross-attention mechanism that aligns multimodal dependencies under curvature-aware constraints, and (iii) a Mobius addition-based fusion layer. Comprehensive experiments on two benchmark datasets-Mal-Net and CICMalDroid2020- show that FOCA consistently outperforms unimodal models, surpasses most Euclidean multimodal baselines, and achieves state-of-the-art performance over existing works.

</details>


### [9] [A Systemic Evaluation of Multimodal RAG Privacy](https://arxiv.org/abs/2601.17644)
*Ali Al-Lawati,Suhang Wang*

Main category: cs.CR

TL;DR: 该论文实证研究了多模态检索增强生成（mRAG）管道中的隐私风险，特别是视觉资产（如图像）及其元数据（如标题）的泄露问题。


<details>
  <summary>Details</summary>
Motivation: 随着多模态检索增强生成（mRAG）管道在视觉中心任务（如视觉问答）中的广泛应用，隐私挑战日益突出。虽然mRAG能够连接私有数据集以提升模型性能，但在推理过程中存在私有信息泄露的风险。

Method: 通过实证研究分析mRAG管道中的隐私风险，重点关注标准模型提示下的信息泄露。实施了一个案例研究，尝试推断mRAG中是否包含特定视觉资产（如图像），如果存在则泄露其相关元数据（如标题）。

Result: 研究发现mRAG管道存在显著的隐私风险，能够通过标准提示推断私有数据集中视觉资产的存在并泄露其元数据。

Conclusion: 研究结果强调了隐私保护机制的必要性，并推动未来对mRAG隐私保护的进一步研究。

Abstract: The growing adoption of multimodal Retrieval-Augmented Generation (mRAG) pipelines for vision-centric tasks (e.g. visual QA) introduces important privacy challenges. In particular, while mRAG provides a practical capability to connect private datasets to improve model performance, it risks the leakage of private information from these datasets during inference. In this paper, we perform an empirical study to analyze the privacy risks inherent in the mRAG pipeline observed through standard model prompting. Specifically, we implement a case study that attempts to infer the inclusion of a visual asset, e.g. image, in the mRAG, and if present leak the metadata, e.g. caption, related to it. Our findings highlight the need for privacy-preserving mechanisms and motivate future research on mRAG privacy.

</details>


### [10] [A PUF-Based Security Framework for Fault and Intrusion Detection](https://arxiv.org/abs/2601.17661)
*Ahmed Oun,Rishabh Das,Clay Hess,Aakriti Barat,Savas Kaya*

Main category: cs.CR

TL;DR: 该研究提出了一种基于硬件信任根的工业控制系统传感器认证方案，通过在测量层嵌入物理不可克隆函数(PUF)来验证传感器读数，防止恶意攻击和信号退化。


<details>
  <summary>Details</summary>
Motivation: 工业控制系统依赖传感器反馈来维持安全关键过程在操作限制内，但传感器读数可能受到信号退化或供应链攻击的威胁，需要一种可靠的身份验证机制。

Method: 结合电压指纹识别和时序认证的硬件信任根架构，将物理不可克隆函数(PUF)嵌入测量层，使用基于Simulink的PUF模拟器在硬件在环水罐测试平台上进行原型验证。

Result: 系统在5.18小时正常操作期间保持99.97%的准确率，能够检测所有注入的异常，包括尖峰故障、硬过故障和将系统推向不安全状态的硬件木马场景。

Conclusion: 该架构提供了一种过程感知、供应商无关的方法，可与遗留工厂集成，检测传感器信号退化或复杂的供应链攻击，增强工业控制系统的安全性。

Abstract: Industrial Control Systems (ICS) rely on sensor feedback to keep safety-critical processes within operational limits. This research presents a hardware-root-of-trust that embeds a Physically Unclonable Function (PUF) at the measurement layer to authenticate sensor readings. The architecture combines voltage fingerprinting with a temporal authentication that integrates with standard industrial control system architecture. The research prototypes the PUF integration on a hardware-in-the-loop (HIL) water tank testbed using a Simulink-based PUF emulator. The system maintains 99.97% accuracy over a 5.18-hour period of normal operation and flags all injected anomalies, including spike faults, hard-over faults, and hardware trojan scenarios that push the system over to an unsafe operational state. The proposed architecture provides a process-aware, vendor-agnostic approach that can integrate with legacy plants to detect sensor signal degradation or sophisticated supply chain attacks.

</details>


### [11] [Performance Analysis of Quantum-Secure Digital Signature Algorithms in Blockchain](https://arxiv.org/abs/2601.17785)
*Tushar Jain*

Main category: cs.CR

TL;DR: 该论文构建了一个支持多种后量子签名算法的区块链原型，重点评估了CRYSTALS-Dilithium、Falcon和Hawk等基于格的方案在区块链环境中的性能表现。


<details>
  <summary>Details</summary>
Motivation: 当前大多数加密货币和区块链平台依赖椭圆曲线密码学，而Shor算法使其容易受到量子攻击。因此，需要研究后量子数字签名在真实区块链系统中的实际表现。

Method: 设计并实现了一个支持多种量子安全签名算法的区块链原型，重点关注基于格的方案（CRYSTALS-Dilithium、Falcon、Hawk）。通过实验测量密钥生成、签名、验证时间、密钥大小和签名大小等性能指标。

Result: 报告提供了详细的性能指标对比，包括各种量子安全签名算法在区块链环境中的具体表现数据，并扩展分析了HAETAE等其他方案。

Conclusion: 该研究为区块链系统集成后量子密码学提供了重要的实验数据和性能分析，有助于推动量子安全区块链技术的发展。

Abstract: The long-term security of public blockchains strictly depends on the hardness assumptions of the underlying digital signature schemes. In the current scenario, most deployed cryptocurrencies and blockchain platforms rely on elliptic-curve cryptography, which is vulnerable to quantum attacks due to Shor's algorithm. Therefore, it is important to understand how post-quantum (PQ) digital signatures behave when integrated into real blockchain systems. This report presents a blockchain prototype that supports multiple quantum-secure signature algorithms, focusing on CRYSTALS-Dilithium, Falcon and Hawk as lattice-based schemes. This report also describes the design of the prototype and discusses the performance metrics, which include key generation, signing, verification times, key sizes and signature sizes. This report covers the problem, background, and experimental methodology, also providing a detailed comparison of quantum-secure signatures in a blockchain context and extending the analysis to schemes such as HAETAE.

</details>


### [12] [FARM: Few-shot Adaptive Malware Family Classification under Concept Drift](https://arxiv.org/abs/2601.17907)
*Numan Halit Guldemir,Oluwafemi Olukoya,Jesús Martínez-del-Rincón*

Main category: cs.CR

TL;DR: FARM框架通过三重自编码器、DBSCAN聚类和少样本学习，有效检测和适应恶意软件分类中的概念漂移，在有限监督下提升分类性能。


<details>
  <summary>Details</summary>
Motivation: 恶意软件分类模型面临概念漂移问题，包括协变量漂移和标签漂移，导致性能下降。现有方法难以在动态威胁环境中有效适应新出现的恶意软件家族。

Method: FARM框架使用三重自编码器将样本投影到判别性潜在空间，通过DBSCAN聚类和动态阈值进行无监督漂移检测。采用基于原型的少样本学习进行快速适应，并支持积累足够漂移样本后的完全重新训练。

Result: 在BenchMFC数据集上，FARM在协变量漂移下将分类性能提升5.6%，仅使用少样本适应在未见恶意软件家族上平均F1得分达0.85，重新训练后进一步提升至0.94。

Conclusion: FARM框架在有限监督下展现出对动态恶意软件检测环境的强大鲁棒性和适应性，为解决恶意软件分类中的概念漂移问题提供了有效方案。

Abstract: Malware classification models often face performance degradation due to concept drift, arising from evolving threat landscapes and the emergence of novel malware families. This paper presents FARM (Few-shot Adaptive Recognition of Malware), a framework designed to detect and adapt to both covariate and label drift in Windows Portable Executable (PE) malware classification. FARM leverages a triplet autoencoder to project samples into a discriminative latent space, enabling unsupervised drift detection via DBSCAN clustering and dynamic thresholding. For rapid adaptation, it employs few-shot learning using prototype-based classification, requiring only a handful of labeled samples. FARM also supports full retraining when enough drifted samples accumulate, updating the latent space for long-term integration. Experiments on the BenchMFC dataset demonstrate that FARM improves classification performance under covariate drift by 5.6\%, and achieves an average F1 score of 0.85 on unseen malware families using only few-shot adaptation, which further increases to 0.94 after retraining. These results highlight FARM's robustness and adaptability in dynamic malware detection environments under limited supervision.

</details>


### [13] [From Statistical Disclosure Control to Fair AI: Navigating Fundamental Tradeoffs in Differential Privacy](https://arxiv.org/abs/2601.17909)
*Adriana Watson*

Main category: cs.CR

TL;DR: 该论文系统分析了隐私、效用和公平性三者之间的根本性权衡，展示了在差分隐私机器学习系统中同时实现这三者的局限性，并为实践者提供了决策指导。


<details>
  <summary>Details</summary>
Motivation: 当前差分隐私研究主要关注隐私与效用的权衡，而公平性约束被低估和研究不足。需要系统性地探讨隐私、效用和公平性三者之间的根本性限制。

Method: 通过具体示例和技术分析，连接三个理论线索：Dalenius的语义隐私不可能性结果、Dwork的可实现差分隐私替代方案，以及加入公平性要求后的新兴不可能性结果。

Result: 展示了隐私、效用和公平性之间的三方帕累托边界，揭示了同时实现这三者的根本性限制，并证明了这些限制对少数群体的影响。

Conclusion: 建立了一个统一框架，综合分散的研究结果，为部署隐私公平学习系统的实践者和政策制定者提供实用指导，帮助他们在这些权衡中做出明智决策。

Abstract: Differential privacy has become the gold standard for privacy-preserving machine learning systems. Unfortunately, subsequent work has primarily fixated on the privacy-utility tradeoff, leaving the subject of fairness constraints undervalued and under-researched. This paper provides a systematic treatment connecting three threads: (1) Dalenius's impossibility results for semantic privacy, (2) Dwork's differential privacy as an achievable alternative, and (3) emerging impossibility results from the addition of a fairness requirement. Through concrete examples and technical analysis, the three-way Pareto frontier between privacy, utility, and fairness is demonstrated to showcase the fundamental limits on what can be simultaneously achieved. In this work, these limits are characterized, the impact on minority groups is demonstrated, and practical guidance for navigating these tradeoffs are provided. This forms a unified framework synthesizing scattered results to help practitioners and policymakers make informed decisions when deploying private fair learning systems.

</details>


### [14] [Prompt Injection Evaluations: Refusal Boundary Instability and Artifact-Dependent Compliance in GPT-4-Series Models](https://arxiv.org/abs/2601.17911)
*Thomas Heverin*

Main category: cs.CR

TL;DR: 该研究挑战了将拒绝视为稳定二元安全指标的范式，通过结构化扰动建模拒绝为局部决策边界，发现拒绝行为具有概率性和依赖特定内容的特性，单次提示评估会系统性高估安全性。


<details>
  <summary>Details</summary>
Motivation: 传统提示注入评估通常将拒绝视为稳定、二元的安全指标，但本研究质疑这一范式，认为拒绝行为可能是不稳定的，需要更深入理解其作为决策边界的特性。

Method: 评估GPT-4.1和GPT-4o两个模型，使用3,274次扰动运行，每个基础提示进行25次跨五个结构化家族的扰动，结果手动编码为拒绝、部分合规或完全合规。采用卡方检验、逻辑回归、混合效应建模和新的拒绝边界熵(RBE)指标进行分析。

Result: 虽然两个模型拒绝率>94%，但拒绝不稳定性持续存在且不均匀。约三分之一初始拒绝诱导提示至少出现一次"拒绝逃逸"（扰动下转向合规）。文本类内容（如勒索软件说明）不稳定性显著更高，翻转率超过20%，而可执行恶意软件内容在两个模型中均未出现拒绝逃逸。GPT-4o比GPT-4.1有更严格的拒绝执行和更低的RBE，但未消除内容依赖风险。

Conclusion: 拒绝行为是概率性、内容依赖的边界现象而非稳定二元属性，需要改变LLM安全性的测量和审计方式，单次提示评估会系统性高估安全鲁棒性。

Abstract: Prompt injection evaluations typically treat refusal as a stable, binary indicator of safety. This study challenges that paradigm by modeling refusal as a local decision boundary and examining its stability under structured perturbations. We evaluated two models, GPT-4.1 and GPT-4o, using 3,274 perturbation runs derived from refusal-inducing prompt injection attempts. Each base prompt was subjected to 25 perturbations across five structured families, with outcomes manually coded as Refusal, Partial Compliance, or Full Compliance.
  Using chi-square tests, logistic regression, mixed-effects modeling, and a novel Refusal Boundary Entropy (RBE) metric, we demonstrate that while both models refuse >94% of attempts, refusal instability is persistent and non-uniform. Approximately one-third of initial refusal-inducing prompts exhibited at least one "refusal escape," a transition to compliance under perturbation. We find that artifact type is a stronger predictor of refusal failure than perturbation style. Textual artifacts, such as ransomware notes, exhibited significantly higher instability, with flip rates exceeding 20%. Conversely, executable malware artifacts showed zero refusal escapes in both models. While GPT-4o demonstrated tighter refusal enforcement and lower RBE than GPT-4.1, it did not eliminate artifact-dependent risks. These findings suggest that single-prompt evaluations systematically overestimate safety robustness. We conclude that refusal behavior is a probabilistic, artifact-dependent boundary phenomenon rather than a stable binary property, requiring a shift in how LLM safety is measured and audited.

</details>


### [15] [Data Siphoning Through Advanced Persistent Transmission Attacks At The Physical Layer](https://arxiv.org/abs/2601.17967)
*Alon Hillel-Tuch*

Main category: cs.CR

TL;DR: 研究开发一种感知和完整性协议，以缓解物理层侧信道攻击，防止数据窃听和拒绝服务攻击


<details>
  <summary>Details</summary>
Motivation: 物理层传输介质（如铜缆、光纤、无线）存在物理攻击向量，威胁数据机密性和可用性。现有协议和加密标准虽然能混淆数据，但往往无法保护数据类型和目的地安全，对机密性和完整性的洞察有限。

Method: 研究开发一种感知和完整性协议，专门针对物理侧信道攻击进行缓解，防止数据窃听和拒绝服务攻击。

Result: 论文探讨了开发此类协议的可行性，但未提供具体实验结果或性能数据。

Conclusion: 需要开发新的感知和完整性协议来应对物理层侧信道攻击，以增强数据通信的机密性和可用性。

Abstract: Data at the physical layer transmits via media such as copper cable, fiber optic, or wireless. Physical attack vectors exist that challenge data confidentiality and availability. Protocols and encryption standards help obfuscate but often cannot keep the data type and destination secure, with limited insight into confidentiality and integrity. We will investigate the feasibility of developing an awareness and integrity protocol to help mitigate physical side-channel attacks that lead to eavesdropping of data communication and denial-of-service.
  Keywords: data confidentiality, siphoning, eavesdropping, person-in-the-middle, denial-of-service, physical layer attacks, nation-states

</details>


### [16] [XGuardian: Towards Explainable and Generalized AI Anti-Cheat on FPS Games](https://arxiv.org/abs/2601.18068)
*Jiayi Zhang,Chenxin Sun,Chenxiong Qian*

Main category: cs.CR

TL;DR: XGuardian是一个服务器端、可泛化且可解释的FPS游戏自瞄作弊检测系统，仅使用俯仰角和偏航角数据构建时序特征，在不同游戏中实现高性能检测和低开销。


<details>
  <summary>Details</summary>
Motivation: 自瞄作弊是FPS游戏中最普遍和臭名昭著的作弊形式，严重威胁游戏产业。现有检测方法存在框架不可靠、泛化性有限、开销高、检测性能低、结果缺乏可解释性等问题。

Method: 提出XGuardian系统，仅需俯仰角和偏航角两个原始数据输入（所有FPS游戏必备），构建新颖的时序特征来描述瞄准轨迹，从而区分作弊者和正常玩家。

Result: 在最新主流FPS游戏CS2上进行评估，并在另外两款不同游戏中验证泛化性。与先前工作相比，在不同游戏中实现了高检测性能和低开销，展示了广泛的泛化性和高效性。

Conclusion: XGuardian能够为其预测提供合理解释，从而缩短封禁周期。该系统及其数据集已公开可用，为FPS游戏作弊检测提供了有效的解决方案。

Abstract: Aim-assist cheats are the most prevalent and infamous form of cheating in First-Person Shooter (FPS) games, which help cheaters illegally reveal the opponent's location and auto-aim and shoot, and thereby pose significant threats to the game industry. Although a considerable research effort has been made to automatically detect aim-assist cheats, existing works suffer from unreliable frameworks, limited generalizability, high overhead, low detection performance, and a lack of explainability of detection results. In this paper, we propose XGuardian, a server-side generalized and explainable system for detecting aim-assist cheats to overcome these limitations. It requires only two raw data inputs, pitch and yaw, which are all FPS games' must-haves, to construct novel temporal features and describe aim trajectories, which are essential for distinguishing cheaters and normal players. XGuardian is evaluated with the latest mainstream FPS game CS2, and validates its generalizability with another two different games. It achieves high detection performance and low overhead compared to prior works across different games with real-world and large-scale datasets, demonstrating wide generalizability and high effectiveness. It is able to justify its predictions and thereby shorten the ban cycle. We make XGuardian as well as our datasets publicly available.

</details>


### [17] [Rhea: Detecting Privilege-Escalated Evasive Ransomware Attacks Using Format-Aware Validation in the Cloud](https://arxiv.org/abs/2601.18216)
*Beom Heyn Kim,Seok Min Hong,Mohammad Mannan*

Main category: cs.CR

TL;DR: Rhea是一个云卸载的勒索软件防御系统，通过分析数据快照和文件格式验证来检测特权提升的逃避性勒索软件攻击。


<details>
  <summary>Details</summary>
Motivation: 现代勒索软件变种结合了特权提升和复杂的逃避策略（如间歇加密、低熵加密、模仿攻击），能够击败依赖I/O模式分析的现有解决方案。传统的基于统计内容的检测在加密规模减小时因采样噪声而变得不可靠。

Method: Rhea采用云卸载架构，分析复制的数据快照（称为突变快照）。引入格式感知验证，通过验证文件格式的语法和语义正确性来检测勒索软件，而不是依赖统计或基于熵的指标。利用文件格式规范作为检测不变式。

Result: 评估表明，Rhea显著优于现有方法，证明其在实际对抗现代勒索软件威胁方面的有效性。

Conclusion: Rhea通过文件格式验证的方法，能够可靠地识别细粒度和逃避性加密，即使在攻击者拥有提升权限的情况下也能有效工作，为对抗现代勒索软件威胁提供了实用的解决方案。

Abstract: Ransomware variants increasingly combine privilege escalation with sophisticated evasion strategies such as intermittent encryption, low-entropy encryption, and imitation attacks. Such powerful ransomware variants, privilege-escalated evasive ransomware (PEER), can defeat existing solutions relying on I/O-pattern analysis by tampering with or obfuscating I/O traces. Meanwhile, conventional statistical content-based detection becomes unreliable as the encryption size decreases due to sampling noises. We present Rhea, a cloud-offloaded ransomware defense system that analyzes replicated data snapshots, so-called mutation snapshots. Rhea introduces Format-Aware Validation that validates the syntactic and semantic correctness of file formats, instead of relying on statistical or entropy-based indicators. By leveraging file-format specifications as detection invariants, Rhea can reliably identify fine-grained and evasive encryption even under elevated attacker privileges. Our evaluation demonstrates that Rhea significantly outperforms existing approaches, establishing its practical effectiveness against modern ransomware threats.

</details>


### [18] [Fundamentals, Recent Advances, and Challenges Regarding Cryptographic Algorithms for the Quantum Computing Era](https://arxiv.org/abs/2601.18413)
*Darlan Noetzold,Valderi Reis Quietinho Leithardt*

Main category: cs.CR

TL;DR: 这是一本关于量子计算对密码学影响的葡萄牙语参考书，面向数据安全和密码学领域的学生和专业人士，涵盖从基础概念到后量子密码学标准化的全面内容。


<details>
  <summary>Details</summary>
Motivation: 为葡萄牙语读者提供关于量子计算对密码学影响的清晰、最新的概述，满足本科生、硕士生、博士生以及专业人士在数据安全和密码学领域的学习和研究需求。

Method: 采用渐进式结构：从基本概念开始，介绍量子算法及其影响（特别是Shor算法），然后重点讨论基于格、编码、哈希函数、多元方程、同源等不同"家族"的后量子密码方案，分析标准化现状（特别是NIST进程），最后讨论迁移、互操作性、性能和密码治理等实际问题。

Result: 本书提供了量子密码学的全面参考框架，涵盖了从理论基础到实际应用的各个方面，旨在帮助读者理解量子计算对密码学的数学原理和实际影响。

Conclusion: 本书旨在培养批判性思维和明智的技术决策能力，促进后量子时代的安全过渡策略，为学术界和工业界提供量子密码学的系统性知识资源。

Abstract: This book arises from the need to provide a clear and up-to-date overview of the impacts of quantum computing on cryptography. The goal is to provide a reference in Portuguese for undergraduate, master's, and doctoral students in the field of data security and cryptography. Throughout the chapters, we present fundamentals, we discuss classical and post-quantum algorithms, evaluate emerging patterns, and point out real-world implementation challenges. The initial objective is to serve as a guide for students, researchers, and professionals who need to understand not only the mathematics involved, but also its practical implications in security systems and policies. For more advanced professionals, the main objective is to present content and ideas so that they can assess the changes and perspectives in the era of quantum cryptographic algorithms. To that end, the text's structure was designed to be progressive: we begin with essential concepts, move on to quantum algorithms and their consequences (with emphasis on Shor's algorithm), present issues focusing on "families" of post-quantum schemes (based on lattices, codes, hash functions, multivariate, isogenies), analyze the state of the art in standardization (highlighting the NIST process), and finally, discuss migration, interoperability, performance, and cryptographic governance. We hope that this work will assist in the formation of critical thinking and informed technical decision-making, fostering secure transition strategies for the post-quantum era.

</details>


### [19] [KeyMemRT Compiler and Runtime: Unlocking Memory-Scalable FHE](https://arxiv.org/abs/2601.18445)
*Eymen Ünay,Björn Franke,Jackson Woodruff*

Main category: cs.CR

TL;DR: KeyMemRT：基于MLIR的FHE编译器和运行时框架，通过管理旋转密钥生命周期来降低内存使用，支持任意数量的旋转索引而不导致内存膨胀


<details>
  <summary>Details</summary>
Motivation: 全同态加密（FHE）虽然能实现隐私保护计算，但存在高延迟和高内存消耗问题。旋转密钥通常占用大部分内存，在复杂FHE应用中会造成内存瓶颈，限制程序吞吐量。现有编译器很少解决此问题，而是依赖大内存系统，这阻碍了FHE的采用，因为手动优化FHE程序具有挑战性。

Method: KeyMemRT采用基于MLIR的编译器和运行时框架，通过数据流分析确定密钥生命周期，单独管理旋转密钥生命周期以降低内存利用率。这是第一个提供自动密钥管理、处理细粒度密钥管理并管理引导密钥的FHE编译器。

Result: 与最先进的FHE编译器相比，KeyMemRT在ANT-ACE上实现了1.74倍的内存减少和1.20倍的加速，在内存优化编译器Fhelipe上实现了1.16倍的内存减少和1.73倍的加速。实现了Orion和HEIR的前端。

Conclusion: KeyMemRT作为后优化编译器，可以成为任何FHE编译器的目标，通过自动管理旋转密钥生命周期有效解决了FHE中的内存瓶颈问题，降低了资源需求，有助于推动FHE的采用。

Abstract: Fully Homomorphic Encryption (FHE) enables privacy preserving computation but it suffers from high latency and memory consumption. The computations are secured with special keys called rotation keys which often take up the majority of memory. In complex FHE applications, these rotation keys can cause a large memory bottleneck limiting program throughput. Existing compilers make little effort to solve this problem, instead relying on systems with massive memory availability. This resource requirement is a barrier to FHE uptake because optimizing FHE programs by hand is challenging due to their scale, complexity and expertise required.
  In this work, we present KeyMemRT; an MLIR based compiler and runtime framework that individually manages rotation key lifetimes to lower memory utilization and to allow arbitrary number of rotation indices to be supported without memory bloating. KeyMemRT relies on dataflow analysis to determine key lifetimes and is the first FHE compiler to provide automatic key management, handle fine-grained key-mangement and manage boostrap keys. We implement frontends for Orion and HEIR and show improvements over state-of-the-art FHE compilers. KeyMemRT achieves memory reduction of 1.74x and a speedup of 1.20x over ANT-ACE, and memory reduction of 1.16x and a speedup of 1.73x over memory-optimized compiler Fhelipe. We provide KeyMemRT as a post-optimizing compiler that can be targeted by any FHE compiler.

</details>


### [20] [Scaling up Privacy-Preserving ML: A CKKS Implementation of Llama-2-7B](https://arxiv.org/abs/2601.18511)
*Jaiyoung Park,Sejin Park,Jai Hyun Park,Jung Ho Ahn,Jung Hee Cheon,Guillaume Hanrot,Jung Woo Kim,Minje Park,Damien Stehlé*

Main category: cs.CR

TL;DR: 提出一种基于全同态加密的私有LLM推理方案，支持数千输入token，其中仅部分加密，通过不平衡分块预填充框架和机器学习策略优化异常值处理，实现Llama-2-7B的端到端私有推理。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型普及，推理输入的隐私问题日益突出。现有全同态加密方案在处理长输入token时扩展性差，且受异常值影响大，导致非线性层评估成本高昂。

Method: 提出不平衡分块预填充框架，分别处理公开和私有输入token；采用明文-明文、明文-密文和密文-密文计算组件；设计新的同态算法用于矩阵乘法和多项式评估；使用token前置和旋转等机器学习策略减少异常值范围。

Result: 实现了基于CKKS的Llama-2-7B端到端私有推理，支持最多4096个输入token（最后128个加密）。在8个NVIDIA RTX-4090 GPU集群上，摘要推理耗时85秒，生成每个输出token耗时33秒。

Conclusion: 该方案有效解决了现有FHE方案在处理长输入token时的扩展性问题，通过部分加密和异常值优化策略，实现了实用的大模型私有推理，为隐私保护LLM应用提供了可行方案。

Abstract: As large language models (LLMs) become ubiquitous, privacy concerns pertaining to inference inputs keep growing. In this context, fully homomorphic encryption (FHE) has emerged as a primary cryptographic solution to provide non-interactive confidential LLM inference. Existing solutions scale poorly with the input token length, and hence focus either on small models or larger models with a small number of input tokens. They also suffer from the existence of large outlier values. These values have a strong impact on the evaluation of non-linear layers, leading to large-degree polynomial approximation and thus heavy evaluation costs.
  We propose an FHE-based private LLM inference solution that allows thousands of input tokens with only a part of them being encrypted: this fits with a scenario where the context is benign and only part of the input is sensitive. To do so, we suggest an unbalanced chunked prefill framework that processes the private and public parts of the input tokens differently. Our framework contains plaintext-plaintext, plaintext-ciphertext and ciphertext-ciphertext computational components. We adopt different strategies and ingredients for each component. We also devise new homomorphic algorithms for specific matrix multiplication and polynomial evaluation tasks encountered during LLM inference.
  Furthermore, without retraining, we tailor the LLM inference algorithm to reduce the ranges of outlier values: we leverage machine learning strategies (token prepending and rotations) to mitigate the impact of the outliers on non-linear layers.
  Based on these ingredients, we describe a CKKS-based end-to-end implementation of Llama-2-7B private inference for up to 4096 input tokens, of which the last 128 are encrypted. On a cluster of 8~NVIDIA RTX-4090 GPUs, inference takes 85s for summarization and 33s for generation per output token.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [21] [Online parameter estimation for the Crazyflie quadcopter through an EM algorithm](https://arxiv.org/abs/2601.17009)
*Yanhua Zhao*

Main category: cs.AI

TL;DR: 该论文研究了在四旋翼无人机系统中添加随机噪声的影响，使用扩展卡尔曼滤波器进行状态估计，基于SDE系统实现线性二次高斯控制器，并应用期望最大化算法进行参数估计。


<details>
  <summary>Details</summary>
Motivation: 无人机在各种领域（如救援、摄影、农业、运输）应用广泛，但地震等灾害会破坏基础设施，使救援人员难以到达某些区域。四旋翼无人机系统在实际运行中会受到噪声影响，需要研究噪声对系统的影响并开发有效的状态估计和控制方法。

Method: 1. 在四旋翼无人机系统中添加随机噪声；2. 使用扩展卡尔曼滤波器基于传感器的噪声观测进行状态估计；3. 基于随机微分方程系统实现线性二次高斯控制器；4. 应用期望最大化算法进行四旋翼无人机的参数估计；5. 比较离线参数估计和在线参数估计的结果。

Result: 研究结果表明，在线参数估计的收敛值范围比离线参数估计略大。这意味着在线参数估计在动态变化的环境中可能具有更好的适应性，但同时也显示出更大的不确定性范围。

Conclusion: 该研究成功地将随机噪声建模、状态估计、控制和参数估计方法应用于四旋翼无人机系统。在线参数估计方法虽然收敛值范围较大，但在实际应用中可能更适合动态环境。这些方法对于提高无人机在噪声环境下的性能和可靠性具有重要意义。

Abstract: Drones are becoming more and more popular nowadays. They are small in size, low in cost, and reliable in operation. They contain a variety of sensors and can perform a variety of flight tasks, reaching places that are difficult or inaccessible for humans. Earthquakes damage a lot of infrastructure, making it impossible for rescuers to reach some areas. But drones can help. Many amateur and professional photographers like to use drones for aerial photography. Drones play a non-negligible role in agriculture and transportation too. Drones can be used to spray pesticides, and they can also transport supplies. A quadcopter is a four-rotor drone and has been studied in this paper. In this paper, random noise is added to the quadcopter system and its effects on the drone system are studied. An extended Kalman filter has been used to estimate the state based on noisy observations from the sensor. Based on a SDE system, a linear quadratic Gaussian controller has been implemented. The expectation maximization algorithm has been applied for parameter estimation of the quadcopter. The results of offline parameter estimation and online parameter estimation are presented. The results show that the online parameter estimation has a slightly larger range of convergence values than the offline parameter estimation.

</details>


### [22] [Interpreting Agentic Systems: Beyond Model Explanations to System-Level Accountability](https://arxiv.org/abs/2601.17168)
*Judy Zhu,Dhari Gandhi,Himanshu Joshi,Ahmad Rezaie Mianroodi,Sedef Akinli Kocak,Dhanesh Ramachandran*

Main category: cs.AI

TL;DR: 该论文评估了现有可解释性方法在智能体系统中的应用局限，提出了专门针对智能体系统的可解释性技术发展方向，以确保AI系统的安全可靠部署。


<details>
  <summary>Details</summary>
Motivation: 智能体系统与传统的机器学习模型在架构和部署上有根本性差异，引入了独特的安全挑战，包括目标错位、决策错误累积和智能体间协调风险。现有的可解释性技术主要针对静态模型开发，在应用于智能体系统时存在局限性。

Method: 论文评估了现有可解释性方法在智能体系统背景下的适用性和局限性，识别了这些方法在提供智能体决策洞察能力方面的差距。提出了专门为智能体系统设计的可解释性技术发展方向。

Result: 现有可解释性方法在应用于智能体系统时存在不足，无法充分应对智能体系统的时序动态性、决策累积效应和上下文依赖行为。需要新的分析方法来理解智能体决策过程。

Conclusion: 需要开发专门针对智能体系统的可解释性技术，在智能体生命周期的各个阶段（从目标形成、环境交互到结果评估）嵌入监督机制。这些进展对于确保智能体AI系统的安全和可靠部署至关重要。

Abstract: Agentic systems have transformed how Large Language Models (LLMs) can be leveraged to create autonomous systems with goal-directed behaviors, consisting of multi-step planning and the ability to interact with different environments. These systems differ fundamentally from traditional machine learning models, both in architecture and deployment, introducing unique AI safety challenges, including goal misalignment, compounding decision errors, and coordination risks among interacting agents, that necessitate embedding interpretability and explainability by design to ensure traceability and accountability across their autonomous behaviors. Current interpretability techniques, developed primarily for static models, show limitations when applied to agentic systems. The temporal dynamics, compounding decisions, and context-dependent behaviors of agentic systems demand new analytical approaches. This paper assesses the suitability and limitations of existing interpretability methods in the context of agentic systems, identifying gaps in their capacity to provide meaningful insight into agent decision-making. We propose future directions for developing interpretability techniques specifically designed for agentic systems, pinpointing where interpretability is required to embed oversight mechanisms across the agent lifecycle from goal formation, through environmental interaction, to outcome evaluation. These advances are essential to ensure the safe and accountable deployment of agentic AI systems.

</details>


### [23] [High-Fidelity Longitudinal Patient Simulation Using Real-World Data](https://arxiv.org/abs/2601.17310)
*Yu Akagi,Tomohisa Seki,Hiromasa Ito,Toru Takiguchi,Kazuhiko Ohe,Yoshimasa Kawazoe*

Main category: cs.AI

TL;DR: 利用真实世界临床记录开发生成式模拟器，可基于患者历史生成高保真未来临床轨迹，在200多万条记录上预训练，能准确预测事件发生率和实验室结果。


<details>
  <summary>Details</summary>
Motivation: 模拟在临床医学中具有变革潜力，可用于个性化治疗规划和虚拟临床试验，但模拟患者轨迹因复杂的生物和社会文化影响而具有挑战性。本研究旨在利用真实世界临床记录经验性地建模患者时间线。

Method: 开发了一个生成式模拟器模型，以患者历史为输入，合成细粒度、真实的未来轨迹。该模型在超过200万条临床记录上进行了预训练。

Result: 模型生成了高保真的未来时间线，与真实患者未来数据中的事件发生率、实验室测试结果和时间动态密切匹配。准确估计了未来事件概率，观察值与预期值比率在不同结果和时间范围内始终接近1.0。

Conclusion: 研究揭示了电子健康记录中真实世界数据的未开发价值，并引入了一个可扩展的临床护理计算机模拟框架。

Abstract: Simulation is a powerful tool for exploring uncertainty. Its potential in clinical medicine is transformative and includes personalized treatment planning and virtual clinical trials. However, simulating patient trajectories is challenging because of complex biological and sociocultural influences. Here, we show that real-world clinical records can be leveraged to empirically model patient timelines. We developed a generative simulator model that takes a patient's history as input and synthesizes fine-grained, realistic future trajectories. The model was pretrained on more than 200 million clinical records. It produced high-fidelity future timelines, closely matching event occurrence rates, laboratory test results, and temporal dynamics in real patient future data. It also accurately estimated future event probabilities, with observed-to-expected ratios consistently near 1.0 across diverse outcomes and time horizons. Our results reveal the untapped value of real-world data in electronic health records and introduce a scalable framework for in silico modeling of clinical care.

</details>


### [24] [Phase Transition for Budgeted Multi-Agent Synergy](https://arxiv.org/abs/2601.17311)
*Bang Liu,Linglong Kong,Jian Pei*

Main category: cs.AI

TL;DR: 该论文提出了一个可校准的最小理论，用于预测多智能体系统在固定推理预算下的三种行为模式：提升、饱和和崩溃。理论基于三个关键约束：有限上下文窗口、有损通信和智能体间的相关性失败。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统虽然能提高可靠性，但在固定推理预算下常常表现出提升、饱和甚至崩溃三种不同行为。现有研究缺乏能够系统预测这些行为模式的理论框架，特别是考虑到现代智能体栈的三个关键约束：有限上下文窗口、有损通信和相似智能体间的共享失败。

Method: 提出了一个可校准的最小理论框架，将每个叶子智能体用计算-性能缩放指数β描述，通信用消息长度保真度曲线γ(m)描述，相关性用有效共享错误相关性ρ描述，上下文窗口W则施加了硬性扇入限制。针对具有多数聚合的二元成功/失败任务，分析了深度b叉树结构，证明了在相关输入和有损通信下的尖锐相变现象。

Result: 理论预测存在一个单一标量αρ（结合γ(m)、ρ和扇入b）决定弱信号是被放大到非平凡固定点还是被冲刷到随机水平。在放大机制下，推导了组织指数s，并证明当s>β时出现预算协同效应（即在相同总预算下优于最佳单个智能体）。进一步通过混合深度表征饱和现象，并提供了在增长和饱和阶段都保持准确的保守裁剪预测器。

Conclusion: 该理论框架能够预测多智能体系统在固定推理预算下的行为模式，揭示了相关性诱导和通信诱导的性能下限，并暴露了核心设计权衡。理论预测的相边界在受控合成模拟中得到验证，并能解释最近大规模匹配预算研究中报告的LLM智能体系统缩放的主要瓶颈。

Abstract: Multi-agent systems can improve reliability, yet under a fixed inference budget they often help, saturate, or even collapse. We develop a minimal and calibratable theory that predicts these regimes from three binding constraints of modern agent stacks: finite context windows, lossy inter-agent communication, and shared failures among similar agents. Each leaf agent is summarized by a compute-performance scaling exponent $β$; communication is captured by a message-length fidelity curve $γ(m)$; dependence is captured by an effective shared-error correlation $ρ$; and a context window $W$ imposes hard fan-in limits that make hierarchy necessary. For binary success/failure tasks with majority aggregation, we prove a sharp phase transition for deep $b$-ary trees with correlated inputs and lossy communication: a single scalar $α_ρ$ (combining $γ(m)$, $ρ$, and fan-in $b$) determines whether weak signal is amplified to a nontrivial fixed point or washed out to chance. In the amplifying regime, we derive an organization exponent $s$ and show that budgeted synergy, i.e., outperforming the best single agent under the same total budget, occurs exactly when $s>β$, yielding closed-form compute allocation rules and explicit budget thresholds. We further characterize saturation via a mixing depth and provide a conservative clipped predictor that remains accurate across growth and saturation. A continuous-performance warm-up gives closed-form risks for star, chain, and tree organizations, making correlation- and communication-induced floors explicit and exposing the core design trade-offs in a smooth setting. Finally, we validate the predicted phase boundaries in controlled synthetic simulations and show how the same mechanisms explain the dominant bottlenecks reported in recent large-scale matched-budget studies of LLM agent-system scaling.

</details>


### [25] [TheoremForge: Scaling up Formal Data Synthesis with Low-Budget Agentic Workflow](https://arxiv.org/abs/2601.17332)
*Yicheng Tao,Hongteng Xu*

Main category: cs.AI

TL;DR: TheoremForge是一个低成本的形式化数学数据合成管道，通过将形式化过程分解为五个子任务，并采用解耦提取策略从失败轨迹中恢复有效训练信号，显著提高了数据合成效率和成本效益。


<details>
  <summary>Details</summary>
Motivation: 形式化数学中智能体工作流的高成本阻碍了大规模数据合成，加剧了开源语料库的稀缺性。为了解决这个问题，需要开发一种成本效益高的形式化数据合成方法。

Method: TheoremForge将形式化过程分解为五个子任务：陈述形式化、证明生成、前提选择、证明修正和证明草图。采用解耦提取策略，从全局失败的轨迹中恢复有效的训练信号，有效利用浪费的计算资源。

Result: 在2000个问题的基准测试中，TheoremForge实现了12.6%的验证率，超过8.6%的基线水平，每个成功轨迹的平均成本仅为0.481美元（使用Gemini-3-Flash）。解耦提取策略使证明生成的数据产出比标准过滤方法提高了1.6倍。

Conclusion: TheoremForge作为一个可扩展的框架，能够构建数据飞轮来训练未来的专家模型，为解决形式化数学数据稀缺问题提供了有效的解决方案。

Abstract: The high cost of agentic workflows in formal mathematics hinders large-scale data synthesis, exacerbating the scarcity of open-source corpora. To address this, we introduce \textbf{TheoremForge}, a cost-effective formal data synthesis pipeline that decomposes the formalization process into five sub-tasks, which are \textit{statement formalization}, \textit{proof generation}, \textit{premise selection}, \textit{proof correction} and \textit{proof sketching}. By implementing a \textit{Decoupled Extraction Strategy}, the workflow recovers valid training signals from globally failed trajectories, effectively utilizing wasted computation. Experiments on a 2,000-problem benchmark demonstrate that TheoremForge achieves a Verified Rate of 12.6\%, surpassing the 8.6\% baseline, at an average cost of only \textbf{\$0.481} per successful trajectory using Gemini-3-Flash. Crucially, our strategy increases data yield by \textbf{1.6$\times$} for proof generation compared to standard filtering. These results establish TheoremForge as a scalable framework for constructing a data flywheel to train future expert models. Our code is available \href{https://github.com/timechess/TheoremForge}{here}.

</details>


### [26] [The Relativity of AGI: Distributional Axioms, Fragility, and Undecidability](https://arxiv.org/abs/2601.17335)
*Angshul Majumdar*

Main category: cs.AI

TL;DR: 该研究从理论角度分析通用人工智能（AGI）的定义问题，证明AGI无法脱离具体任务分布和资源限制而存在绝对定义，且无法通过可计算程序（包括自我验证）进行完备认证。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探讨AGI是否具有一致的理论定义，能够支持关于存在性、鲁棒性或自我验证的绝对主张。当前AGI讨论中常存在模糊的绝对化声称，需要从形式化角度澄清其理论基础。

Method: 将AGI公理化定义为分布性、资源受限的语义谓词，索引包括任务族、任务分布、性能函数和明确资源预算。在此框架下，运用数学证明方法（包括Rice定理和哥德尔-塔斯基论证）推导四类结果。

Result: 1. 通用性是关系性的，不存在分布无关的AGI概念；2. 任务分布的任意微小扰动可通过悬崖集使AGI属性失效，排除普遍鲁棒性；3. 有限资源下无法实现跨任务族的无界泛化；4. AGI作为非平凡语义属性，无法通过任何可计算程序（包括自我验证）进行完备认证。

Conclusion: 强分布无关的AGI主张在没有明确形式化索引的情况下是无定义的，而非错误。AI的经验进展并不暗示自我认证通用智能的可实现性，依赖内部自我认证的递归自我改进方案是病态的。

Abstract: We study whether Artificial General Intelligence (AGI) admits a coherent theoretical definition that supports absolute claims of existence, robustness, or self-verification. We formalize AGI axiomatically as a distributional, resource-bounded semantic predicate, indexed by a task family, a task distribution, a performance functional, and explicit resource budgets. Under this framework, we derive four classes of results. First, we show that generality is inherently relational: there is no distribution-independent notion of AGI. Second, we prove non-invariance results demonstrating that arbitrarily small perturbations of the task distribution can invalidate AGI properties via cliff sets, precluding universal robustness. Third, we establish bounded transfer guarantees, ruling out unbounded generalization across task families under finite resources. Fourth, invoking Rice-style and Gödel--Tarski arguments, we prove that AGI is a nontrivial semantic property and therefore cannot be soundly and completely certified by any computable procedure, including procedures implemented by the agent itself. Consequently, recursive self-improvement schemes that rely on internal self-certification of AGI are ill-posed. Taken together, our results show that strong, distribution-independent claims of AGI are not false but undefined without explicit formal indexing, and that empirical progress in AI does not imply the attainability of self-certifying general intelligence.

</details>


### [27] [Are We Evaluating the Edit Locality of LLM Model Editing Properly?](https://arxiv.org/abs/2601.17343)
*Wei Liu,Haomei Xu,Hongkai Liu,Zhiying Deng,Ruixuan Li,Heng Huang,Yee Whye Teh,Wee Sun Lee*

Main category: cs.AI

TL;DR: 本文指出现有模型编辑特异性评估协议存在不足，提出了更有效的评估框架，能够更敏感地衡量知识保留能力。


<details>
  <summary>Details</summary>
Motivation: 模型编辑需要平衡编辑效果（成功注入目标知识）和特异性（保留现有非目标知识），但现有特异性评估协议存在根本性问题，无法有效衡量不同方法的性能差异。

Method: 提出新的评估协议：消除开放式LLM与确定性答案假设之间的冲突，避免查询无关的流畅性偏差，并能在近乎连续的空间中平滑调整评估严格度。

Result: 实验表明，基于新协议的指标对特异性正则化强度变化更敏感，与正则化强度强相关，能够更精细地区分不同方法的知识保留能力。

Conclusion: 现有特异性评估协议存在不足，提出的新评估框架能够更有效地评估模型编辑方法的知识保留能力，为模型编辑研究提供更好的评估工具。

Abstract: Model editing has recently emerged as a popular paradigm for efficiently updating knowledge in LLMs. A central desideratum of updating knowledge is to balance editing efficacy, i.e., the successful injection of target knowledge, and specificity (also known as edit locality), i.e., the preservation of existing non-target knowledge. However, we find that existing specificity evaluation protocols are inadequate for this purpose. We systematically elaborated on the three fundamental issues it faces. Beyond the conceptual issues, we further empirically demonstrate that existing specificity metrics are weakly correlated with the strength of specificity regularizers. We also find that current metrics lack sufficient sensitivity, rendering them ineffective at distinguishing the specificity performance of different methods. Finally, we propose a constructive evaluation protocol. Under this protocol, the conflict between open-ended LLMs and the assumption of determined answers is eliminated, query-independent fluency biases are avoided, and the evaluation strictness can be smoothly adjusted within a near-continuous space. Experiments across various LLMs, datasets, and editing methods show that metrics derived from the proposed protocol are more sensitive to changes in the strength of specificity regularizers and exhibit strong correlation with them, enabling more fine-grained discrimination of different methods' knowledge preservation capabilities.

</details>


### [28] [Multi-Agent Learning Path Planning via LLMs](https://arxiv.org/abs/2601.17346)
*Haoxin Xu,Changyong Qi,Tong Liu,Bohao Zhang,Anna He,Bingqian Jiang,Longwei Zheng,Xiaoqing Gu*

Main category: cs.AI

TL;DR: 提出基于大语言模型的多智能体学习路径规划框架，通过角色分工与规则协作实现透明、可解释的个性化学习路径推荐


<details>
  <summary>Details</summary>
Motivation: 现有智能导学系统中的学习路径规划方法缺乏透明度、适应性和以学习者为中心的可解释性，需要更可信、可解释的教育AI解决方案

Method: 提出MALPP框架，包含三个基于LLM的任务特定智能体：学习者分析智能体、路径规划智能体和反思智能体，通过结构化提示和预定义规则协作，基于认知负荷理论和最近发展区理论确保认知对齐

Result: 在MOOCCubeX数据集上使用7个LLM进行实验，MALPP在路径质量、知识序列一致性和认知负荷对齐方面显著优于基线模型，消融研究验证了协作机制和理论约束的有效性

Conclusion: 该研究为可信、可解释的教育AI发展做出贡献，展示了基于LLM的以学习者为中心的自适应教学的可扩展方法

Abstract: The integration of large language models (LLMs) into intelligent tutoring systems offers transformative potential for personalized learning in higher education. However, most existing learning path planning approaches lack transparency, adaptability, and learner-centered explainability. To address these challenges, this study proposes a novel Multi-Agent Learning Path Planning (MALPP) framework that leverages a role- and rule-based collaboration mechanism among intelligent agents, each powered by LLMs. The framework includes three task-specific agents: a learner analytics agent, a path planning agent, and a reflection agent. These agents collaborate via structured prompts and predefined rules to analyze learning profiles, generate tailored learning paths, and iteratively refine them with interpretable feedback. Grounded in Cognitive Load Theory and Zone of Proximal Development, the system ensures that recommended paths are cognitively aligned and pedagogically meaningful. Experiments conducted on the MOOCCubeX dataset using seven LLMs show that MALPP significantly outperforms baseline models in path quality, knowledge sequence consistency, and cognitive load alignment. Ablation studies further validate the effectiveness of the collaborative mechanism and theoretical constraints. This research contributes to the development of trustworthy, explainable AI in education and demonstrates a scalable approach to learner-centered adaptive instruction powered by LLMs.

</details>


### [29] [Auditing Disability Representation in Vision-Language Models](https://arxiv.org/abs/2601.17348)
*Srikant Panda,Sourabh Singh Yadav,Palkesh Malviya*

Main category: cs.AI

TL;DR: 该研究分析了视觉语言模型在描述残障人士图像时的表现，发现模型会从基于证据的事实描述转向包含无根据推断的解释性转变，导致情感降级和缺陷导向的框架化。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型越来越多地应用于社会敏感领域，但它们在残障方面的行为尚未得到充分探索。研究者关注模型在描述残障人士图像时，从基于视觉证据的事实描述转向包含无根据推断的解释性转变的现象。

Method: 研究者创建了一个基于中性提示和残障情境化提示配对的基准测试，在零样本设置下评估了15个最先进的开源和闭源视觉语言模型，涵盖9个残障类别。评估框架将解释保真度作为核心目标，结合标准文本指标（情感、社会尊重和响应长度的变化）和LLM作为评判者的协议，并由有残障生活经验的标注者验证。

Result: 研究发现引入残障情境会持续降低解释保真度，导致解释性转变，表现为推测性推断、叙事扩展、情感降级和缺陷导向的框架化。这些效应在种族和性别维度上进一步放大。最后，研究表明有针对性的提示和偏好微调能有效提高解释保真度并显著减少解释性转变。

Conclusion: 视觉语言模型在描述残障人士时存在系统性偏见，倾向于做出无根据的推断和负面框架化。这种偏见在交叉身份维度上更加明显，但可以通过有针对性的干预措施来改善。

Abstract: Vision-language models (VLMs) are increasingly deployed in socially sensitive applications, yet their behavior with respect to disability remains underexplored. We study disability aware descriptions for person centric images, where models often transition from evidence grounded factual description to interpretation shift including introduction of unsupported inferences beyond observable visual evidence. To systematically analyze this phenomenon, we introduce a benchmark based on paired Neutral Prompts (NP) and Disability-Contextualised Prompts (DP) and evaluate 15 state-of-the-art open- and closed-source VLMs under a zero-shot setting across 9 disability categories. Our evaluation framework treats interpretive fidelity as core objective and combines standard text-based metrics capturing affective degradation through shifts in sentiment, social regard and response length with an LLM-as-judge protocol, validated by annotators with lived experience of disability. We find that introducing disability context consistently degrades interpretive fidelity, inducing interpretation shifts characterised by speculative inference, narrative elaboration, affective degradation and deficit oriented framing. These effects are further amplified along race and gender dimension. Finally, we demonstrate targeted prompting and preference fine-tuning effectively improves interpretive fidelity and reduces substantially interpretation shifts.

</details>


### [30] [A Syllogistic Probe: Tracing the Evolution of Logic Reasoning in Large Language Models](https://arxiv.org/abs/2601.17426)
*Zhengqing Zang,Yuqi Ding,Yanmei Gu,Changkai Song,Zhengkai Yang,Guoping Du,Junbo Zhao,Haobo Wang*

Main category: cs.AI

TL;DR: 该研究探索大语言模型在逻辑推理框架上的演变，使用存在性导入作为探针评估三段论推理，发现模型规模扩展、思维链推理和基础模型对逻辑框架演变有重要影响。


<details>
  <summary>Details</summary>
Motivation: 人类逻辑从直觉驱动推理转向严格的形式系统，受大语言模型最新进展启发，研究探索LLMs是否展现出类似的底层逻辑框架演变。

Method: 使用存在性导入作为探针，评估传统逻辑和现代逻辑下的三段论推理，通过在新构建的三段论数据集上测试最先进的LLMs进行广泛实验。

Result: 发现三个关键结果：(1) 模型规模扩展促进向现代逻辑的转变；(2) 思维链推理是超越参数扩展的高效加速器；(3) 基础模型决定这种转变出现难易和稳定性的关键因素。

Conclusion: 除了这些核心因素外，还进行了额外实验深入分析当前LLMs在三段论推理上的特性，为理解LLMs逻辑推理能力演变提供了重要见解。

Abstract: Human logic has gradually shifted from intuition-driven inference to rigorous formal systems. Motivated by recent advances in large language models (LLMs), we explore whether LLMs exhibit a similar evolution in the underlying logical framework. Using existential import as a probe, we for evaluate syllogism under traditional and modern logic. Through extensive experiments of testing SOTA LLMs on a new syllogism dataset, we have some interesting findings: (i) Model size scaling promotes the shift toward modern logic; (ii) Thinking serves as an efficient accelerator beyond parameter scaling; (iii) the Base model plays a crucial role in determining how easily and stably this shift can emerge. Beyond these core factors, we conduct additional experiments for in-depth analysis of properties of current LLMs on syllogistic reasoning.

</details>


### [31] [Lattice: Generative Guardrails for Conversational Agents](https://arxiv.org/abs/2601.17481)
*Emily Broadhurst,Tawab Safi,Joseph Edell,Vashisht Ganesh,Karime Maamari*

Main category: cs.AI

TL;DR: Lattice是一个自构建和持续改进的AI安全护栏框架，通过两阶段方法实现：构建阶段从标注数据通过模拟优化创建初始护栏，持续改进阶段通过风险评估、对抗测试和整合自主适应新威胁，在ProsocialDialog数据集上达到91% F1分数。


<details>
  <summary>Details</summary>
Motivation: 现有对话AI系统的安全护栏使用静态规则，无法适应新威胁或部署环境的变化，需要能够自我构建和持续改进的护栏框架。

Method: Lattice采用两阶段框架：1)构建阶段：通过迭代模拟和优化从标注示例构建初始护栏；2)持续改进阶段：通过风险评估、对抗测试和整合自主适应已部署的护栏。

Result: 在ProsocialDialog数据集上，Lattice在保留数据上达到91% F1分数，比关键词基线高43个百分点，比LlamaGuard高25个百分点，比NeMo高4个百分点。持续改进阶段通过闭环优化在跨域数据上实现7个百分点的F1提升。

Conclusion: Lattice框架表明，有效的AI安全护栏可以通过迭代优化实现自我构建，为对话AI系统提供了动态适应的安全防护机制。

Abstract: Conversational AI systems require guardrails to prevent harmful outputs, yet existing approaches use static rules that cannot adapt to new threats or deployment contexts. We introduce Lattice, a framework for self-constructing and continuously improving guardrails. Lattice operates in two stages: construction builds initial guardrails from labeled examples through iterative simulation and optimization; continuous improvement autonomously adapts deployed guardrails through risk assessment, adversarial testing, and consolidation. Evaluated on the ProsocialDialog dataset, Lattice achieves 91% F1 on held-out data, outperforming keyword baselines by 43pp, LlamaGuard by 25pp, and NeMo by 4pp. The continuous improvement stage achieves 7pp F1 improvement on cross-domain data through closed-loop optimization. Our framework shows that effective guardrails can be self-constructed through iterative optimization.

</details>


### [32] [Cognitive Platform Engineering for Autonomous Cloud Operations](https://arxiv.org/abs/2601.17542)
*Vinoth Punniyamoorthy,Nitin Saksena,Srivenkateswara Reddy Sankiti,Nachiappan Chockalingam,Aswathnarayan Muthukrishnan Kirubakaran,Shiva Kumar Reddy Carimireddy,Durgaraman Maruthavanan*

Main category: cs.AI

TL;DR: 本文提出认知平台工程新范式，通过四层架构将智能推理嵌入平台生命周期，实现云环境的自主调整和弹性管理


<details>
  <summary>Details</summary>
Motivation: 传统DevOps自动化难以应对云原生系统的规模和动态性，规则驱动的方法导致响应式运维、修复延迟和依赖人工经验

Method: 提出四平面参考架构：数据收集层、智能推理层、策略驱动编排层和人工体验层，构建持续反馈循环；原型实现基于Kubernetes、Terraform、Open Policy Agent和基于ML的异常检测

Result: 原型展示在平均解决时间、资源效率和合规性方面的改进，证明将智能嵌入平台操作可实现弹性、自调整和意图对齐的云环境

Conclusion: 认知平台工程为云平台运维提供新范式，未来研究方向包括强化学习、可解释治理和可持续自管理云生态系统

Abstract: Modern DevOps practices have accelerated software delivery through automation, CI/CD pipelines, and observability tooling,but these approaches struggle to keep pace with the scale and dynamism of cloud-native systems. As telemetry volume grows and configuration drift increases, traditional, rule-driven automation often results in reactive operations, delayed remediation, and dependency on manual expertise. This paper introduces Cognitive Platform Engineering, a next-generation paradigm that integrates sensing, reasoning, and autonomous action directly into the platform lifecycle. This paper propose a four-plane reference architecture that unifies data collection, intelligent inference, policy-driven orchestration, and human experience layers within a continuous feedback loop. A prototype implementation built with Kubernetes, Terraform, Open Policy Agent, and ML-based anomaly detection demonstrates improvements in mean time to resolution, resource efficiency, and compliance. The results show that embedding intelligence into platform operations enables resilient, self-adjusting, and intent-aligned cloud environments. The paper concludes with research opportunities in reinforcement learning, explainable governance, and sustainable self-managing cloud ecosystems.

</details>


### [33] [JaxARC: A High-Performance JAX-based Environment for Abstraction and Reasoning Research](https://arxiv.org/abs/2601.17564)
*Aadam,Monu Verma,Mohamed Abdel-Mottaleb*

Main category: cs.AI

TL;DR: JaxARC是一个基于JAX的高性能强化学习环境，用于解决ARC推理任务，相比Gymnasium实现38-5,439倍加速，支持大规模并行实验。


<details>
  <summary>Details</summary>
Motivation: 现有的基于Gymnasium的ARC强化学习环境存在计算瓶颈，限制了实验规模。需要高性能环境来支持大规模强化学习研究。

Method: 使用JAX实现功能化、无状态的架构，支持大规模并行处理，提供灵活的action空间、可组合的wrapper和配置驱动的可复现性。

Result: 在相同batch size下实现38-5,439倍加速，峰值吞吐量达到7.9亿步/秒，支持多个ARC数据集，使之前计算不可行的大规模RL研究成为可能。

Conclusion: JaxARC为ARC推理任务提供了高性能的强化学习环境，解决了现有环境的计算瓶颈问题，促进了大规模强化学习研究的发展。

Abstract: The Abstraction and Reasoning Corpus (ARC) tests AI systems' ability to perform human-like inductive reasoning from a few demonstration pairs. Existing Gymnasium-based RL environments severely limit experimental scale due to computational bottlenecks. We present JaxARC, an open-source, high-performance RL environment for ARC implemented in JAX. Its functional, stateless architecture enables massive parallelism, achieving 38-5,439x speedup over Gymnasium at matched batch sizes, with peak throughput of 790M steps/second. JaxARC supports multiple ARC datasets, flexible action spaces, composable wrappers, and configuration-driven reproducibility, enabling large-scale RL research previously computationally infeasible. JaxARC is available at https://github.com/aadimator/JaxARC.

</details>


### [34] [Health-ORSC-Bench: A Benchmark for Measuring Over-Refusal and Safety Completion in Health Context](https://arxiv.org/abs/2601.17642)
*Zhihao Zhang,Liting Huang,Guanghao Wu,Preslav Nakov,Heng Ji,Usman Naseem*

Main category: cs.AI

TL;DR: 论文提出了Health-ORSC-Bench基准，用于系统评估医疗大语言模型在安全对齐中的过度拒绝和安全完成能力，发现当前模型在安全性和实用性之间存在显著矛盾。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在医疗领域的安全对齐主要依赖二元拒绝边界，导致对良性查询的过度拒绝或对有害查询的不安全合规。现有基准只能测量极端情况，无法评估模型在双用途或边界查询中提供安全高层指导而不跨越可操作伤害的能力。

Method: 提出了Health-ORSC-Bench基准，包含31,920个良性边界提示，涵盖七个健康类别（如自残、医疗错误信息）。使用自动化流水线结合人工验证，在不同意图模糊度水平上测试模型。评估了30个最先进的大语言模型，包括GPT-5和Claude-4。

Result: 安全优化模型经常拒绝高达80%的"困难"良性提示，而领域特定模型则经常为了实用性牺牲安全性。模型家族和大小显著影响校准：较大的前沿模型（如GPT-5、Llama-4）表现出"安全悲观主义"和更高的过度拒绝，而较小或基于MoE的模型（如Qwen-3-Next）表现不同。

Conclusion: 当前大语言模型难以平衡拒绝和合规，Health-ORSC-Bench为校准下一代医疗AI助手提供了严格标准，使其能够提供细致、安全和有用的完成结果。

Abstract: Safety alignment in Large Language Models is critical for healthcare; however, reliance on binary refusal boundaries often results in \emph{over-refusal} of benign queries or \emph{unsafe compliance} with harmful ones. While existing benchmarks measure these extremes, they fail to evaluate Safe Completion: the model's ability to maximise helpfulness on dual-use or borderline queries by providing safe, high-level guidance without crossing into actionable harm. We introduce \textbf{Health-ORSC-Bench}, the first large-scale benchmark designed to systematically measure \textbf{Over-Refusal} and \textbf{Safe Completion} quality in healthcare. Comprising 31,920 benign boundary prompts across seven health categories (e.g., self-harm, medical misinformation), our framework uses an automated pipeline with human validation to test models at varying levels of intent ambiguity. We evaluate 30 state-of-the-art LLMs, including GPT-5 and Claude-4, revealing a significant tension: safety-optimised models frequently refuse up to 80\% of "Hard" benign prompts, while domain-specific models often sacrifice safety for utility. Our findings demonstrate that model family and size significantly influence calibration: larger frontier models (e.g., GPT-5, Llama-4) exhibit "safety-pessimism" and higher over-refusal than smaller or MoE-based counterparts (e.g., Qwen-3-Next), highlighting that current LLMs struggle to balance refusal and compliance. Health-ORSC-Bench provides a rigorous standard for calibrating the next generation of medical AI assistants toward nuanced, safe, and helpful completions. The code and data will be released upon acceptance. \textcolor{red}{Warning: Some contents may include toxic or undesired contents.}

</details>


### [35] [The LLM Data Auditor: A Metric-oriented Survey on Quality and Trustworthiness in Evaluating Synthetic Data](https://arxiv.org/abs/2601.17717)
*Kaituo Zhang,Mingzhi Hu,Hoang Anh Duy Le,Fariha Kabir Torsha,Zhimeng Jiang,Minh Khai Bui,Chia-Yuan Chang,Yu-Neng Chuang,Zhen Xiong,Ying Lin,Guanchu Wang,Na Zou*

Main category: cs.AI

TL;DR: 本文提出了LLM数据审计框架，旨在解决LLM生成合成数据质量评估不足的问题，通过系统化分类内在评估指标，为多模态数据生成提供统一的质量评估视角。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM已成为生成多模态数据的强大工具，能够将稀缺数据转化为可控资产，但现有研究主要关注生成方法，对合成数据质量评估关注有限，且缺乏跨模态的统一评估框架。

Method: 提出LLM数据审计框架，首先描述LLM在六种不同模态中生成数据的应用，然后从质量和可信度两个维度系统分类合成数据的内在评估指标，从依赖下游任务性能的外在评估转向数据本身固有属性的评估。

Result: 通过该评估系统分析各模态代表性生成方法的实验评估，发现当前评估实践存在显著缺陷，并基于这些发现为社区改进数据生成评估提供具体建议。

Conclusion: LLM数据审计框架不仅系统化评估合成数据质量，还概述了合成数据在不同模态中的实际应用方法，为提升LLM生成数据质量评估提供了系统化解决方案。

Abstract: Large Language Models (LLMs) have emerged as powerful tools for generating data across various modalities. By transforming data from a scarce resource into a controllable asset, LLMs mitigate the bottlenecks imposed by the acquisition costs of real-world data for model training, evaluation, and system iteration. However, ensuring the high quality of LLM-generated synthetic data remains a critical challenge. Existing research primarily focuses on generation methodologies, with limited direct attention to the quality of the resulting data. Furthermore, most studies are restricted to single modalities, lacking a unified perspective across different data types. To bridge this gap, we propose the \textbf{LLM Data Auditor framework}. In this framework, we first describe how LLMs are utilized to generate data across six distinct modalities. More importantly, we systematically categorize intrinsic metrics for evaluating synthetic data from two dimensions: quality and trustworthiness. This approach shifts the focus from extrinsic evaluation, which relies on downstream task performance, to the inherent properties of the data itself. Using this evaluation system, we analyze the experimental evaluations of representative generation methods for each modality and identify substantial deficiencies in current evaluation practices. Based on these findings, we offer concrete recommendations for the community to improve the evaluation of data generation. Finally, the framework outlines methodologies for the practical application of synthetic data across different modalities.

</details>


### [36] [Neuro-Symbolic Verification on Instruction Following of LLMs](https://arxiv.org/abs/2601.17789)
*Yiming Su,Kunzhao Xu,Yanjie Gao,Fan Yang,Cheng Li,Mao Yang,Tianyin Xu*

Main category: cs.AI

TL;DR: NSVIF是一个神经符号框架，用于验证LLM输出是否遵循指令，将指令遵循验证建模为约束满足问题，显著优于基于LLM的方法并提供可解释反馈。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型并不总是遵循指令，这种违规在基于LLM的智能体工作流中会传播放大，导致任务失败和系统事故，需要一种通用验证方法来确保LLM输出符合指令。

Method: NSVIF将指令遵循验证建模为约束满足问题，将用户指令建模为约束，包含逻辑和语义约束，通过统一求解器协调逻辑推理和语义分析来完成约束求解。

Result: 实验使用新基准VIFBENCH评估，NSVIF显著优于基于LLM的方法，提供可解释反馈，且其反馈无需后训练即可帮助提高LLM的指令遵循能力。

Conclusion: NSVIF是一个通用、通用的指令遵循验证框架，通过神经符号方法有效验证LLM输出是否符合指令，为解决LLM指令遵循问题提供了有效解决方案。

Abstract: A fundamental problem of applying Large Language Models (LLMs) to important applications is that LLMs do not always follow instructions, and violations are often hard to observe or check. In LLM-based agentic workflows, such violations can propagate and amplify along reasoning chains, causing task failures and system incidents. This paper presents NSVIF, a neuro-symbolic framework for verifying whether an LLM's output follows the instructions used to prompt the LLM. NSVIF is a universal, general-purpose verifier; it makes no assumption about the instruction or the LLM. NSVIF formulates instruction-following verification as a constraint-satisfaction problem by modeling user instructions as constraints. NSVIF models both logical and semantic constraints; constraint solving is done by a unified solver that orchestrates logical reasoning and semantic analysis. To evaluate NSVIF, we develop VIFBENCH, a new benchmark for instruction-following verifiers with fine-grained data labels. Experiments show that NSVIF significantly outperforms LLM-based approaches and provides interpretable feedback. We also show that feedback from NSVIF helps improve LLMs' instruction-following capability without post-training.

</details>


### [37] [MMR-Bench: A Comprehensive Benchmark for Multimodal LLM Routing](https://arxiv.org/abs/2601.17814)
*Haoxuan Ma,Guannan Lai,Han-Jia Ye*

Main category: cs.AI

TL;DR: MMR-Bench是一个用于评估多模态大语言模型路由策略的基准测试，通过控制候选模型集和成本模型，在固定计算预算下优化多模态任务的成本-准确率权衡。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在架构、对齐策略和效率方面存在异质性，单一模型无法在所有任务上表现最优。实际部署中，工作负载从轻量OCR到复杂多模态推理不等，使用单一模型要么在简单任务上过度配置计算资源，要么在困难任务上牺牲准确性。需要研究多模态模型选择（路由）策略来解决这一矛盾。

Method: 提出MMR-Bench统一基准测试，提供：1）具有模态感知输入和可变计算预算的控制环境；2）涵盖OCR、通用VQA和多模态数学推理的广泛视觉语言任务套件；3）强单模型参考、理论上限和代表性路由策略。通过该基准评估多模态信号对路由质量的影响。

Result: 实验表明，融入多模态信号能改善路由质量，提升成本-准确率边界。路由系统能以最强单模型约33%的成本超越其准确性。在部分模型和任务上训练的策略能零样本泛化到新数据集和纯文本基准，无需重新调优。

Conclusion: MMR-Bench为研究自适应多模态模型选择和高效MLLM部署提供了基础，展示了多模态路由在实际部署中的潜力，能够显著降低计算成本同时保持或提升性能。

Abstract: Multimodal large language models (MLLMs) have advanced rapidly, yet heterogeneity in architecture, alignment strategies, and efficiency means that no single model is uniformly superior across tasks. In practical deployments, workloads span lightweight OCR to complex multimodal reasoning; using one MLLM for all queries either over-provisions compute on easy instances or sacrifices accuracy on hard ones. Query-level model selection (routing) addresses this tension, but extending routing from text-only LLMs to MLLMs is nontrivial due to modality fusion, wide variation in computational cost across models, and the absence of a standardized, budget-aware evaluation. We present MMR-Bench, a unified benchmark that isolates the multimodal routing problem and enables comparison under fixed candidate sets and cost models. MMR-Bench provides (i) a controlled environment with modality-aware inputs and variable compute budgets, (ii) a broad suite of vision-language tasks covering OCR, general VQA, and multimodal math reasoning, and (iii) strong single-model reference, oracle upper bounds, and representative routing policies. Using MMR-Bench, we show that incorporating multimodal signals improves routing quality. Empirically, these cues improve the cost-accuracy frontier and enable the routed system to exceed the strongest single model's accuracy at roughly 33% of its cost. Furthermore, policies trained on a subset of models and tasks generalize zero-shot to new datasets and text-only benchmarks without retuning, establishing MMR-Bench as a foundation for studying adaptive multimodal model selection and efficient MLLM deployment. The code will be available at: https://github.com/Hunter-Wrynn/MMR-Bench.

</details>


### [38] [RegGuard: AI-Powered Retrieval-Enhanced Assistant for Pharmaceutical Regulatory Compliance](https://arxiv.org/abs/2601.17826)
*Siyuan Yang,Xihan Bian,Jiayin Tang*

Main category: cs.AI

TL;DR: RegGuard是一个工业级AI助手，用于自动化解读异构监管文本并将其与公司内部政策对齐，通过HiSACC和ReLACE组件提升检索和生成质量，显著减少幻觉风险。


<details>
  <summary>Details</summary>
Motivation: 监管更新日益频繁复杂，给跨国制药公司带来沉重负担。合规团队需要跨司法管辖区、格式和机构手动解读不断变化的规则，成本高且容易出错。

Method: 系统通过安全管道摄入异构文档源，采用两个核心组件：HiSACC（分层语义聚合上下文分块）将长文档语义分割为连贯单元；ReLACE（监管列表自适应交叉编码器）基于开源模型构建，联合建模用户查询和检索候选以改进排名相关性。

Result: 企业环境评估显示，RegGuard在相关性、基础性和上下文专注度方面显著提升回答质量，同时大幅降低幻觉风险。系统架构具备可审计性和可追溯性，支持来源跟踪、访问控制和增量索引。

Conclusion: RegGuard系统能够高效响应不断变化的文档源，适用于任何有严格合规需求的领域，为监管合规提供了自动化、高质量的解决方案。

Abstract: The increasing frequency and complexity of regulatory updates present a significant burden for multinational pharmaceutical companies. Compliance teams must interpret evolving rules across jurisdictions, formats, and agencies, often manually, at high cost and risk of error. We introduce RegGuard, an industrial-scale AI assistant designed to automate the interpretation of heterogeneous regulatory texts and align them with internal corporate policies. The system ingests heterogeneous document sources through a secure pipeline and enhances retrieval and generation quality with two novel components: HiSACC (Hierarchical Semantic Aggregation for Contextual Chunking) semantically segments long documents into coherent units while maintaining consistency across non-contiguous sections. ReLACE (Regulatory Listwise Adaptive Cross-Encoder for Reranking), a domain-adapted cross-encoder built on an open-source model, jointly models user queries and retrieved candidates to improve ranking relevance. Evaluations in enterprise settings demonstrate that RegGuard improves answer quality specifically in terms of relevance, groundedness, and contextual focus, while significantly mitigating hallucination risk. The system architecture is built for auditability and traceability, featuring provenance tracking, access control, and incremental indexing, making it highly responsive to evolving document sources and relevant for any domain with stringent compliance demands.

</details>


### [39] [Aligning Medical Conversational AI through Online Reinforcement Learning with Information-Theoretic Rewards](https://arxiv.org/abs/2601.17828)
*Tanvi Verma,Yang Zhou,Rick Siow Mong Goh,Yong Liu*

Main category: cs.AI

TL;DR: IGFT是一种无需人类对话数据、通过信息增益奖励训练医疗对话AI进行患者访谈的方法，结合在线强化学习和GPT-4o-mini质量评估，在Avey和MIMIC数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有医疗对话AI训练需要昂贵的人工标注对话数据或静态数据集，限制了模型学习有效提问策略的能力。需要一种能够通过自我生成对话学习、无需预收集人类对话的方法。

Method: 提出信息增益微调（IGFT），结合在线组相对策略优化（GRPO）和信息论奖励。使用信息增益奖励函数跟踪对话中揭示的临床实体（症状、时间模式、病史），结合GPT-4o-mini对临床相关性、患者参与度和特异性的质量评估。使用LoRA微调Llama-3.1-8B-Instruct和DeepSeek-R1-Distill-Qwen-7B模型。

Result: DeepSeek-R1-Distill-Qwen-7B（IGFT）在Avey数据集上F1得分为0.408（比基础模型提升10.9%），在MIMIC数据集上为0.289（提升12.9%）。Llama-3.1-8B-Instruct（IGFT）分别达到0.384和0.336。两个模型在MIMIC上都优于OpenAI模型，并超越了HuatuoGPT和UltraMedical等医疗领域基线模型。

Conclusion: IGFT提供了一种无需人类对话数据训练医疗对话AI的有效方法，通过信息增益奖励使模型能够学习有针对性的临床提问策略，在生成全面病史方面优于现有方法，特别是在多轮对话场景中表现优异。

Abstract: We present Information Gain Fine-Tuning (IGFT), a novel approach for training medical conversational AI to conduct effective patient interviews and generate comprehensive History of Present Illness (HPI) without requiring pre-collected human conversations. IGFT combines online Group Relative Policy Optimization (GRPO) with information-theoretic rewards, enabling models to learn from self-generated conversations with simulated patients. Unlike existing approaches that rely on expensive expert-annotated conversations or static datasets, our online RL framework allows models to discover effective questioning strategies through exploration. Our key innovation is an information gain reward function that tracks which clinical entities such as symptoms, temporal patterns, and medical history, are revealed during conversation. Each question's reward is computed based on its expected information gain combined with GPT-4o-mini quality assessments across dimensions including clinical relevance, patient engagement, and specificity. This hybrid approach ensures models learn to ask targeted, clinically appropriate questions that efficiently gather diagnostic information. We fine-tune two models using LoRA: Llama-3.1-8B-Instruct and DeepSeek-R1-Distill-Qwen-7B (a reasoning-optimized model). Training exclusively on Avey data containing concise HPIs, we evaluate generalization to MIMIC data with longer, more elaborate HPIs. DeepSeek-R1-Distill-Qwen-7B (IGFT) achieves F1 scores of 0.408 on Avey (10.9% improvement over base) and 0.289 on MIMIC (12.9% improvement), while Llama-3.1-8B-Instruct (IGFT) reaches 0.384 and 0.336 respectively. Both models outperform OpenAI's model on MIMIC and surpass medical domain-specific baselines like HuatuoGPT and UltraMedical, which were optimized for single-turn medical QA rather than multi-turn conversations.

</details>


### [40] [UniCog: Uncovering Cognitive Abilities of LLMs through Latent Mind Space Analysis](https://arxiv.org/abs/2601.17897)
*Jiayu Liu,Yinhe Long,Zhenya Huang,Enhong Chen*

Main category: cs.AI

TL;DR: UniCog是一个通过潜在心智空间分析大语言模型认知的统一框架，将密集模型激活编码为稀疏解耦的潜在维度，揭示了LLM认知的帕累托原则，并利用潜在激活异常检测推理失败，最终通过潜在信息候选优先级策略将推理性能提升达7.5%。


<details>
  <summary>Details</summary>
Motivation: 现有可解释性方法在解释LLM推理过程中如何调动认知能力方面存在局限，而研究表明LLM的认知过程与人类存在根本差异，因此需要新的分析框架来理解LLM的认知机制。

Method: 提出UniCog统一框架，将其构建为潜在变量模型，将密集模型激活编码为稀疏、解耦的潜在维度，通过对六个先进LLM（包括DeepSeek-V3.2和GPT-4o）进行广泛分析。

Result: 揭示了LLM认知的帕累托原则：共享推理核心与能力特定特征互补；发现推理失败常表现为潜在激活的异常强度；通过潜在信息候选优先级策略，在挑战性基准测试中将推理性能提升达7.5%。

Conclusion: UniCog为LLM分析开辟了新范式，提供了基于认知的推理动态视图，通过潜在心智空间分析能够更好地理解和改进LLM的推理能力。

Abstract: A growing body of research suggests that the cognitive processes of large language models (LLMs) differ fundamentally from those of humans. However, existing interpretability methods remain limited in explaining how cognitive abilities are engaged during LLM reasoning. In this paper, we propose UniCog, a unified framework that analyzes LLM cognition via a latent mind space. Formulated as a latent variable model, UniCog encodes diverse abilities from dense model activations into sparse, disentangled latent dimensions. Through extensive analysis on six advanced LLMs, including DeepSeek-V3.2 and GPT-4o, we reveal a Pareto principle of LLM cognition, where a shared reasoning core is complemented by ability-specific signatures. Furthermore, we discover that reasoning failures often manifest as anomalous intensity in latent activations. These findings opens a new paradigm in LLM analysis, providing a cognition grounded view of reasoning dynamics. Finally, leveraging these insights, we introduce a latent-informed candidate prioritization strategy, which improves reasoning performance by up to 7.5% across challenging benchmarks. Our code is available at https://github.com/milksalute/unicog.

</details>


### [41] [Think Locally, Explain Globally: Graph-Guided LLM Investigations via Local Reasoning and Belief Propagation](https://arxiv.org/abs/2601.17915)
*Saurabh Jha,Rohan Arora,Bhavya,Noah Zheutlin,Paulina Toro Isaza,Laura Shwartz,Yu Deng,Daby Sow,Ruchi Mahindru,Ruchir Puri*

Main category: cs.AI

TL;DR: EoG框架通过将调查任务分解为在依赖图上的溯因推理，使用LLM进行局部证据挖掘和标记，由确定性控制器管理遍历、状态和信念传播，显著提升了开放调查任务的准确性和一致性。


<details>
  <summary>Details</summary>
Motivation: 传统LLM智能体在处理开放调查任务时存在局限性：上下文窗口有限导致关键证据可能被丢弃；ReAct式智能体在探索顺序敏感、结果不稳定；缺乏明确的信念记录和修正机制；推理与控制职责纠缠导致执行错误影响推理质量。

Method: 提出EoG框架：1) 将调查任务形式化为依赖图上的溯因推理；2) LLM负责有界的局部证据挖掘和标记（原因vs症状）；3) 确定性控制器管理图遍历、状态维护和信念传播；4) 计算最小解释边界。

Result: 在ITBench诊断任务上，EoG相比ReAct基线显著提升了准确性和运行间一致性，包括实体F1分数的7倍平均提升（Majority-at-k指标）。

Conclusion: EoG框架通过解耦推理与控制职责，采用依赖图上的溯因推理方法，有效解决了开放调查任务中LLM智能体的可靠性问题，提高了任务执行的稳定性和准确性。

Abstract: LLM agents excel when environments are mostly static and the needed information fits in a model's context window, but they often fail in open-ended investigations where explanations must be constructed by iteratively mining evidence from massive, heterogeneous operational data. These investigations exhibit hidden dependency structure: entities interact, signals co-vary, and the importance of a fact may only become clear after other evidence is discovered. Because the context window is bounded, agents must summarize intermediate findings before their significance is known, increasing the risk of discarding key evidence. ReAct-style agents are especially brittle in this regime. Their retrieve-summarize-reason loop makes conclusions sensitive to exploration order and introduces run-to-run non-determinism, producing a reliability gap where Pass-at-k may be high but Majority-at-k remains low. Simply sampling more rollouts or generating longer reasoning traces does not reliably stabilize results, since hypotheses cannot be autonomously checked as new evidence arrives and there is no explicit mechanism for belief bookkeeping and revision. In addition, ReAct entangles semantic reasoning with controller duties such as tool orchestration and state tracking, so execution errors and plan drift degrade reasoning while consuming scarce context.
  We address these issues by formulating investigation as abductive reasoning over a dependency graph and proposing EoG (Explanations over Graphs), a disaggregated framework in which an LLM performs bounded local evidence mining and labeling (cause vs symptom) while a deterministic controller manages traversal, state, and belief propagation to compute a minimal explanatory frontier. On a representative ITBench diagnostics task, EoG improves both accuracy and run-to-run consistency over ReAct baselines, including a 7x average gain in Majority-at-k entity F1.

</details>


### [42] [Agentic AI for Self-Driving Laboratories in Soft Matter: Taxonomy, Benchmarks,and Open Challenges](https://arxiv.org/abs/2601.17920)
*Xuanzhou Chen,Audrey Wang,Stanley Yin,Hanyang Jiang,Dong Zhang*

Main category: cs.AI

TL;DR: 这篇综述论文以软物质为代表，探讨了自动驾驶实验室中的AI问题，将SDL自主性构建为智能体-环境交互问题，回顾了闭环实验的主要方法，提出了基于能力的分类体系，并总结了实际部署的经验教训。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶实验室为在昂贵操作、噪声延迟反馈、严格可行性和安全约束以及非平稳性条件下的智能体AI提供了严格测试平台。论文旨在通过软物质这一代表性场景，聚焦真实实验室中出现的AI问题，建立SDL自主性的理论框架。

Method: 将SDL自主性构建为具有明确观察、行动、成本和约束的智能体-环境交互问题；回顾了闭环实验的主要方法家族，包括贝叶斯优化和主动学习用于样本高效实验选择，规划和强化学习用于长时程协议优化，以及协调异构仪器和软件的工具使用智能体；提出了基于决策时域、不确定性建模、行动参数化、约束处理、故障恢复和人类参与等维度的能力驱动分类体系。

Result: 建立了SDL自主性的统一理论框架，连接了常见SDL流程与已建立的AI原则；提出了可验证和可溯源感知的策略，支持调试、可重复性和安全操作；综合了基准任务模板和评估指标，优先考虑成本感知性能、漂移鲁棒性、约束违反行为和可重复性。

Conclusion: 从已部署的SDL中提炼了经验教训，并概述了多模态表示、校准不确定性、安全探索和共享基准基础设施等开放挑战。强调需要可验证、可溯源感知的策略来确保调试、可重复性和安全操作。

Abstract: Self-driving laboratories (SDLs) close the loop between experiment design, automated execution, and data-driven decision making, and they provide a demanding testbed for agentic AI under expensive actions, noisy and delayed feedback, strict feasibility and safety constraints, and non-stationarity. This survey uses soft matter as a representative setting but focuses on the AI questions that arise in real laboratories. We frame SDL autonomy as an agent environment interaction problem with explicit observations, actions, costs, and constraints, and we use this formulation to connect common SDL pipelines to established AI principles. We review the main method families that enable closed loop experimentation, including Bayesian optimization and active learning for sample efficient experiment selection, planning and reinforcement learning for long horizon protocol optimization, and tool using agents that orchestrate heterogeneous instruments and software. We emphasize verifiable and provenance aware policies that support debugging, reproducibility, and safe operation. We then propose a capability driven taxonomy that organizes systems by decision horizon, uncertainty modeling, action parameterization, constraint handling, failure recovery, and human involvement. To enable meaningful comparison, we synthesize benchmark task templates and evaluation metrics that prioritize cost aware performance, robustness to drift, constraint violation behavior, and reproducibility. Finally, we distill lessons from deployed SDLs and outline open challenges in multi-modal representation, calibrated uncertainty, safe exploration, and shared benchmark infrastructure.

</details>


### [43] [Learning Transferable Skills in Action RPGs via Directed Skill Graphs and Selective Adaptation](https://arxiv.org/abs/2601.17923)
*Ali Najar*

Main category: cs.AI

TL;DR: 该研究提出了一种基于技能图的分层课程学习方法，用于在复杂实时控制环境中实现终身学习，通过在《黑暗之魂III》游戏中训练可重用的技能组件，实现高效适应环境变化。


<details>
  <summary>Details</summary>
Motivation: 终身学习智能体需要在不从头训练或覆盖已学行为的情况下扩展能力，特别是在复杂实时控制环境中，传统方法面临样本效率低和灾难性遗忘的问题。

Method: 将战斗表示为有向技能图，采用分层课程训练方法，将控制分解为五个可重用技能：相机控制、目标锁定、移动、闪避和治疗-攻击决策策略，每个技能针对特定职责优化。

Result: 技能分解提高了样本效率，当环境从第一阶段切换到第二阶段时，只需针对性地微调两个技能就能在有限交互预算内快速恢复性能，而上游技能保持可迁移性。

Conclusion: 技能图课程与选择性微调相结合，为复杂实时环境中不断进化的持续学习智能体提供了实用路径，支持高效适应环境变化而无需完全重新训练。

Abstract: Lifelong agents should expand their competence over time without retraining from scratch or overwriting previously learned behaviors. We investigate this in a challenging real-time control setting (Dark Souls III) by representing combat as a directed skill graph and training its components in a hierarchical curriculum. The resulting agent decomposes control into five reusable skills: camera control, target lock-on, movement, dodging, and a heal-attack decision policy, each optimized for a narrow responsibility. This factorization improves sample efficiency by reducing the burden on any single policy and supports selective post-training: when the environment shifts from Phase 1 to Phase 2, only a subset of skills must be adapted, while upstream skills remain transferable. Empirically, we find that targeted fine-tuning of just two skills rapidly recovers performance under a limited interaction budget, suggesting that skill-graph curricula together with selective fine-tuning offer a practical pathway toward evolving, continually learning agents in complex real-time environments.

</details>


### [44] [Expert Evaluation and the Limits of Human Feedback in Mental Health AI Safety Testing](https://arxiv.org/abs/2601.18061)
*Kiana Jafari,Paul Ulrich Nikolaus Rust,Duncan Eddy,Robbie Fraser,Nina Vasan,Darja Djordjevic,Akanksha Dadlani,Max Lamparth,Eugenia Kim,Mykel Kochenderfer*

Main category: cs.AI

TL;DR: 研究发现，在心理健康领域，即使经过专业培训的精神科医生使用标准化评估标准，对AI生成回复的评估仍存在显著分歧，尤其是在自杀自伤等安全关键项目上分歧最大，表明专家共识不能作为可靠的地面真值。


<details>
  <summary>Details</summary>
Motivation: 研究旨在检验LHF（从人类反馈中学习）的基本假设：专家判断经过适当聚合后可以作为训练和评估AI系统的有效地面真值。在心理健康这种高安全风险的领域，专家共识尤为重要。

Method: 三位认证精神科医生使用校准的评估标准独立评估LLM生成的回复，计算评估者间信度（ICC和Krippendorff's α），并对分歧最大的安全关键项目进行定性访谈分析。

Result: 评估者间信度极低（ICC 0.087-0.295），低于可接受阈值；自杀自伤类回复分歧最大；一个因素的信度甚至为负值（α=-0.203）；定性分析显示分歧源于不同的临床框架（安全优先、参与中心、文化导向）而非测量误差。

Conclusion: 专家分歧是安全关键AI评估中的社会技术现象，反映了不同的专业哲学；标签聚合只是算术妥协，抹杀了专业哲学；建议从基于共识的聚合转向能够保留和学习专家分歧的对齐方法。

Abstract: Learning from human feedback~(LHF) assumes that expert judgments, appropriately aggregated, yield valid ground truth for training and evaluating AI systems. We tested this assumption in mental health, where high safety stakes make expert consensus essential. Three certified psychiatrists independently evaluated LLM-generated responses using a calibrated rubric. Despite similar training and shared instructions, inter-rater reliability was consistently poor ($ICC$ $0.087$--$0.295$), falling below thresholds considered acceptable for consequential assessment. Disagreement was highest on the most safety-critical items. Suicide and self-harm responses produced greater divergence than any other category, and was systematic rather than random. One factor yielded negative reliability (Krippendorff's $α= -0.203$), indicating structured disagreement worse than chance. Qualitative interviews revealed that disagreement reflects coherent but incompatible individual clinical frameworks, safety-first, engagement-centered, and culturally-informed orientations, rather than measurement error. By demonstrating that experts rely on holistic risk heuristics rather than granular factor discrimination, these findings suggest that aggregated labels function as arithmetic compromises that effectively erase grounded professional philosophies. Our results characterize expert disagreement in safety-critical AI as a sociotechnical phenomenon where professional experience introduces sophisticated layers of principled divergence. We discuss implications for reward modeling, safety classification, and evaluation benchmarks, recommending that practitioners shift from consensus-based aggregation to alignment methods that preserve and learn from expert disagreement.

</details>


### [45] [EvolVE: Evolutionary Search for LLM-based Verilog Generation and Optimization](https://arxiv.org/abs/2601.18067)
*Wei-Po Hsin,Ren-Hao Deng,Yao-Ting Hsieh,En-Ming Huang,Shih-Hao Hung*

Main category: cs.AI

TL;DR: EvolVE是一个利用进化策略和结构化测试平台生成来自动化Verilog硬件设计的框架，在多个基准测试中达到最先进水平，并在工业级问题上显著优化PPA指标。


<details>
  <summary>Details</summary>
Motivation: Verilog硬件设计过程劳动密集且需要专业知识，现有大语言模型由于训练数据有限和顺序推理特性，难以捕捉硬件系统的严格形式逻辑和并发特性。

Method: 提出EvolVE框架，分析多种进化策略：蒙特卡洛树搜索（MCTS）用于最大化功能正确性，想法引导优化（IGR）用于设计优化；结合结构化测试平台生成（STG）加速进化过程；创建IC-RTL工业级基准测试套件。

Result: 在VerilogEval v2上达到98.1%，RTLLM v2上达到92%；在工业级IC-RTL套件上超越竞赛参与者实现，在哈夫曼编码中减少PPA乘积达66%，所有问题几何平均减少17%。

Conclusion: EvolVE框架通过进化策略和结构化测试平台生成，成功解决了LLM在硬件设计中的局限性，在功能正确性和设计优化方面都取得了显著成果，为自动化硬件设计提供了有效解决方案。

Abstract: Verilog's design cycle is inherently labor-intensive and necessitates extensive domain expertise. Although Large Language Models (LLMs) offer a promising pathway toward automation, their limited training data and intrinsic sequential reasoning fail to capture the strict formal logic and concurrency inherent in hardware systems. To overcome these barriers, we present EvolVE, the first framework to analyze multiple evolution strategies on chip design tasks, revealing that Monte Carlo Tree Search (MCTS) excels at maximizing functional correctness, while Idea-Guided Refinement (IGR) proves superior for optimization. We further leverage Structured Testbench Generation (STG) to accelerate the evolutionary process. To address the lack of complex optimization benchmarks, we introduce IC-RTL, targeting industry-scale problems derived from the National Integrated Circuit Contest. Evaluations establish EvolVE as the new state-of-the-art, achieving 98.1% on VerilogEval v2 and 92% on RTLLM v2. Furthermore, on the industry-scale IC-RTL suite, our framework surpasses reference implementations authored by contest participants, reducing the Power, Performance, Area (PPA) product by up to 66% in Huffman Coding and 17% in the geometric mean across all problems. The source code of the IC-RTL benchmark is available at https://github.com/weiber2002/ICRTL.

</details>


### [46] [Beyond Text-to-SQL: Can LLMs Really Debug Enterprise ETL SQL?](https://arxiv.org/abs/2601.18119)
*Jing Ye,Yiwen Duan,Yonghong Yu,Victor Ma,Yang Gao,Xing Chen*

Main category: cs.AI

TL;DR: OurBench是首个企业级SQL推理与调试基准，包含469个语法错误查询和516个语义错误查询，平均超过140行代码。评估近30个LLM显示性能差距巨大，最佳模型Claude-4-Sonnet准确率仅36.46%（语法）和32.17%（语义）。


<details>
  <summary>Details</summary>
Motivation: 企业数据工程中SQL至关重要，但即使是经验丰富的开发者和先进的文本到SQL LLM也难以一次性生成完全正确的SQL代码，通常需要多次调试迭代。目前缺乏专门针对企业级SQL推理和调试的基准测试。

Method: 提出两个关键创新：(1) 使用逆向工程自动构建工作流，系统性地向大规模SQL代码中注入真实错误，实现可扩展和多样化的基准生成；(2) 针对企业环境设计的免执行评估框架，提供快速、准确且资源高效的评估。

Result: OurBench包含469个OurBenchSyn查询（语法错误+明确错误消息）和516个OurBenchSem查询（语义错误，代码不符合用户意图）。查询复杂度高，平均超过140行，具有深而广的抽象语法树。评估显示LLM性能普遍较差，最佳模型准确率仅约三分之一，大多数模型低于20%。

Conclusion: OurBench揭示了当前LLM在企业级SQL调试方面的显著局限性，探索了四种解决方案策略，识别了关键挑战，并为企业环境中LLM辅助SQL调试指出了有前景的研究方向。

Abstract: SQL is central to enterprise data engineering, yet generating fully correct SQL code in a single attempt remains difficult, even for experienced developers and advanced text-to-SQL LLMs, often requiring multiple debugging iterations. We introduce OurBench, the first benchmark for enterprise-level SQL reasoning and debugging. Our benchmark is built on two key innovations: (1) an automated construction workflow that uses reverse engineering to systematically inject realistic bugs into large-scale SQL code, enabling scalable and diverse benchmark generation; and (2) an execution-free evaluation framework tailored to enterprise settings, providing fast, accurate, and resource-efficient assessment.
  OurBench comprises 469 OurBenchSyn queries featuring syntax errors with explicit error messages, and 516 OurBenchSem queries targeting semantic errors in which the code fails to meet user intent. The queries are highly complex, averaging over 140 lines and featuring deep and wide abstract syntax trees.
  Evaluation of nearly 30 LLMs reveals a substantial performance gap: the best-performing model, Claude-4-Sonnet, achieves only 36.46 percent accuracy on OurBenchSyn and 32.17 percent on OurBenchSem, while most models score below 20 percent. We further explore four solution strategies, identify key challenges, and outline promising directions for enterprise SQL debugging with LLMs.

</details>


### [47] [Deadline-Aware, Energy-Efficient Control of Domestic Immersion Hot Water Heaters](https://arxiv.org/abs/2601.18123)
*Muhammad Ibrahim Khan,Bivin Pradeep,James Brusey*

Main category: cs.AI

TL;DR: 研究提出基于截止时间感知的智能控制方法，通过强化学习优化家用即热式热水器能耗，相比传统开关控制可节省26-69%能源。


<details>
  <summary>Details</summary>
Motivation: 传统家用即热式热水器在冬季常连续运行，追求快速加热而非高效节能，忽略了可预测的需求窗口和环境热损失。需要开发能够感知截止时间、在指定时间达到目标温度的同时最小化能耗的控制策略。

Method: 构建了Gymnasium环境模拟即热式热水器（含一阶热损失模型，离散开关动作：0W/6000W，每120秒执行一次）。对比三种方法：时间最优的bang-bang基线控制、零样本蒙特卡洛树搜索规划器、近端策略优化强化学习策略。

Result: 在初始温度10-30°C、截止时间30-90步（1-3小时）、目标温度40-80°C的参数扫描中，PPO在60步（2小时）时间范围内能耗最低（3.23kWh），相比bang-bang控制（4.37-10.45kWh）和MCTS（4.18-6.46kWh）显著节能。在代表性场景（50kg水量、20°C环境温度、60°C目标温度）中，PPO比bang-bang节能54%，比MCTS节能33%。

Conclusion: 学习得到的截止时间感知控制能在相同物理假设下显著降低能耗，规划器无需训练即可提供部分节能效果，而学习策略在训练后推理成本接近零，为智能家居节能提供了有效解决方案。

Abstract: Typical domestic immersion water heater systems are often operated continuously during winter, heating quickly rather than efficiently and ignoring predictable demand windows and ambient losses. We study deadline-aware control, where the aim is to reach a target temperature at a specified time while minimising energy consumption. We introduce an efficient Gymnasium environment that models an immersion hot water heater with first-order thermal losses and discrete on and off actions of 0 W and 6000 W applied every 120 seconds. Methods include a time-optimal bang-bang baseline, a zero-shot Monte Carlo Tree Search planner, and a Proximal Policy Optimisation policy. We report total energy consumption in watt-hours under identical physical dynamics. Across sweeps of initial temperature from 10 to 30 degrees Celsius, deadline from 30 to 90 steps, and target temperature from 40 to 80 degrees Celsius, PPO achieves the most energy-efficient performance at a 60-step horizon of 2 hours, using 3.23 kilowatt-hours, compared to 4.37 to 10.45 kilowatt-hours for bang-bang control and 4.18 to 6.46 kilowatt-hours for MCTS. This corresponds to energy savings of 26 percent at 30 steps and 69 percent at 90 steps. In a representative trajectory with a 50 kg water mass, 20 degrees Celsius ambient temperature, and a 60 degrees Celsius target, PPO consumes 54 percent less energy than bang-bang control and 33 percent less than MCTS. These results show that learned deadline-aware control reduces energy consumption under identical physical assumptions, while planners provide partial savings without training and learned policies offer near-zero inference cost once trained.

</details>


### [48] [RareAlert: Aligning heterogeneous large language model reasoning for early rare disease risk screening](https://arxiv.org/abs/2601.18132)
*Xi Chen,Hongru Zhou,Huahui Yi,Shiyu Feng,Hanyu Zhou,Tiancheng He,Mingke You,Li Wang,Qiankun Li,Kun Wang,Weili Fu,Kang Li,Jian Li*

Main category: cs.AI

TL;DR: RareAlert是一个基于多LLM推理校准的罕见病早期筛查系统，通过整合10个LLM的推理信号，训练出可在本地部署的单一模型，在包含158,666个病例的数据集上实现了0.917的AUC，优于所有评估的LLM和机器学习模型。


<details>
  <summary>Details</summary>
Motivation: 罕见病的漏诊和延迟诊断是重大挑战，现有初级医疗分诊流程在初次就诊时无法可靠识别罕见病患者，需要通用筛查来减少诊断延迟。

Method: 开发了RareAlert系统，整合10个LLM生成的推理信号，使用机器学习进行校准和加权，然后将对齐的推理蒸馏到单个可在本地部署的模型中。使用包含158,666个病例的RareBench数据集进行开发和评估。

Result: RareAlert在独立测试集上达到0.917的AUC，优于最佳机器学习集成模型和所有评估的LLM（包括GPT-5、DeepSeek-R1、Claude-3.7-Sonnet等），证明了LLM医疗推理的多样性和在高不确定性临床任务中对齐推理的有效性。

Conclusion: 罕见病识别可重新概念化为应用于普通患者群体的通用不确定性解决过程。通过将校准推理整合到单一模型中，RareAlert实现了准确、保护隐私、可扩展的罕见病风险筛查，适合大规模本地部署。

Abstract: Missed and delayed diagnosis remains a major challenge in rare disease care. At the initial clinical encounters, physicians assess rare disease risk using only limited information under high uncertainty. When high-risk patients are not recognised at this stage, targeted diagnostic testing is often not initiated, resulting in missed diagnosis. Existing primary care triage processes are structurally insufficient to reliably identify patients with rare diseases at initial clinical presentation and universal screening is needed to reduce diagnostic delay. Here we present RareAlert, an early screening system which predict patient-level rare disease risk from routinely available primary-visit information. RareAlert integrates reasoning generated by ten LLMs, calibrates and weights these signals using machine learning, and distils the aligned reasoning into a single locally deployable model. To develop and evaluate RareAlert, we curated RareBench, a real-world dataset of 158,666 cases covering 33 Orphanet disease categories and more than 7,000 rare conditions, including both rare and non-rare presentations. The results showed that rare disease identification can be reconceptualised as a universal uncertainty resolution process applied to the general patient population. On an independent test set, RareAlert, a Qwen3-4B based model trained with calibrated reasoning signals, achieved an AUC of 0.917, outperforming the best machine learning ensemble and all evaluated LLMs, including GPT-5, DeepSeek-R1, Claude-3.7-Sonnet, o3-mini, Gemini-2.5-Pro, and Qwen3-235B. These findings demonstrate the diversity in LLM medical reasoning and the effectiveness of aligning such reasoning in highly uncertain clinical tasks. By incorporating calibrated reasoning into a single model, RareAlert enables accurate, privacy-preserving, and scalable rare disease risk screening suitable for large-scale local deployment.

</details>


### [49] [Success Conditioning as Policy Improvement: The Optimization Problem Solved by Imitating Success](https://arxiv.org/abs/2601.18175)
*Daniel Russo*

Main category: cs.AI

TL;DR: 成功条件化（success conditioning）是一种广泛使用的策略改进技术，通过收集轨迹、识别成功轨迹并模仿其动作来更新策略。本文证明该方法精确解决了信任区域优化问题，在数据自动确定的χ²散度约束下最大化策略改进。


<details>
  <summary>Details</summary>
Motivation: 成功条件化技术在不同领域有多种名称（如拒绝采样+SFT、目标条件RL、决策变换器等），但其解决的优化问题本质一直不明确。本文旨在从理论层面阐明该方法背后的数学原理和优化基础。

Method: 通过理论证明，将成功条件化形式化为一个信任区域优化问题：在χ²散度约束下最大化策略改进。该约束的半径由数据自动确定。推导出相对策略改进、策略变化幅度和动作影响（action-influence）之间的恒等关系。

Result: 成功条件化被证明是一种保守的改进算子，它不会降低性能或引发危险的分布偏移。当该方法失败时，会通过几乎不改变策略的方式可观察地表现出来。理论还应用于常见的回报阈值化实践，显示其可以放大改进但可能偏离真实目标。

Conclusion: 成功条件化精确解决了信任区域优化问题，为这一广泛使用的技术提供了坚实的理论基础。该方法具有保守改进的特性，失败时表现可观察，为实际应用提供了理论指导。

Abstract: A widely used technique for improving policies is success conditioning, in which one collects trajectories, identifies those that achieve a desired outcome, and updates the policy to imitate the actions taken along successful trajectories. This principle appears under many names -- rejection sampling with SFT, goal-conditioned RL, Decision Transformers -- yet what optimization problem it solves, if any, has remained unclear. We prove that success conditioning exactly solves a trust-region optimization problem, maximizing policy improvement subject to a $χ^2$ divergence constraint whose radius is determined automatically by the data. This yields an identity: relative policy improvement, the magnitude of policy change, and a quantity we call action-influence -- measuring how random variation in action choices affects success rates -- are exactly equal at every state. Success conditioning thus emerges as a conservative improvement operator. Exact success conditioning cannot degrade performance or induce dangerous distribution shift, but when it fails, it does so observably, by hardly changing the policy at all. We apply our theory to the common practice of return thresholding, showing this can amplify improvement, but at the cost of potential misalignment with the true objective.

</details>


### [50] [Paying Less Generalization Tax: A Cross-Domain Generalization Study of RL Training for LLM Agents](https://arxiv.org/abs/2601.18217)
*Zhihan Liu,Lin Guan,Yixin Nie,Kai Zhang,Zhuoqun Hao,Lin Chen,Asli Celikyilmaz,Zhaoran Wang,Na Zhang*

Main category: cs.AI

TL;DR: 研究探索LLM智能体在未知测试领域中的泛化能力，发现状态信息丰富度和规划复杂度是影响跨域泛化的关键因素，而非领域真实性或文本相似性。提出通过添加无关特征增加状态信息丰富度的方法，并分析了SFT预热和逐步思考等建模选择对泛化的影响。


<details>
  <summary>Details</summary>
Motivation: 通用LLM智能体通常在有限环境中进行后训练，但需要在更广泛的未知领域中部署。本研究旨在解决当最终测试领域未知时，智能体后训练的挑战，探索哪些环境和建模因素对跨域性能影响最大。

Method: 首先识别与跨域泛化强相关的环境特征：状态信息丰富度和规划复杂度。提出一种随机化技术，通过添加少量与目标无关的干扰特征来增加状态信息丰富度。同时分析建模选择，包括SFT预热/中期训练以及RL期间启用逐步思考的影响。

Result: 发现状态信息丰富度和规划复杂度是跨域泛化的主要决定因素，而非领域真实性或文本相似性。增加状态信息丰富度能有效提高跨域鲁棒性。SFT预热有助于防止灾难性遗忘但会损害未包含在训练数据中的领域的泛化能力。逐步思考在RL期间对保持泛化能力起关键作用。

Conclusion: 跨域泛化能力主要由状态信息丰富度和规划复杂度决定，而非领域相似性。通过添加无关特征增加状态信息丰富度是提高泛化能力的有效方法。建模选择如SFT预热和逐步思考对泛化有不同影响，需要在训练策略中平衡考虑。

Abstract: Generalist LLM agents are often post-trained on a narrow set of environments but deployed across far broader, unseen domains. In this work, we investigate the challenge of agentic post-training when the eventual test domains are unknown. Specifically, we analyze which properties of reinforcement learning (RL) environments and modeling choices have the greatest influence on out-of-domain performance. First, we identify two environment axes that strongly correlate with cross-domain generalization: (i) state information richness, i.e., the amount of information for the agent to process from the state, and (ii) planning complexity, estimated via goal reachability and trajectory length under a base policy. Notably, domain realism and text-level similarity are not the primary factors; for instance, the simple grid-world domain Sokoban leads to even stronger generalization in SciWorld than the more realistic ALFWorld. Motivated by these findings, we further show that increasing state information richness alone can already effectively improve cross-domain robustness. We propose a randomization technique, which is low-overhead and broadly applicable: add small amounts of distractive goal-irrelevant features to the state to make it richer without altering the task. Beyond environment-side properties, we also examine several modeling choices: (a) SFT warmup or mid-training helps prevent catastrophic forgetting during RL but undermines generalization to domains that are not included in the mid-training datamix; and (b) turning on step-by-step thinking during RL, while not always improving in-domain performance, plays a crucial role in preserving generalization.

</details>


### [51] [Yunjue Agent Tech Report: A Fully Reproducible, Zero-Start In-Situ Self-Evolving Agent System for Open-Ended Tasks](https://arxiv.org/abs/2601.18226)
*Haotian Li,Shijun Yang,Weizhen Qi,Silei Zhao,Rui Hua,Mingzhu Song,Xiaojian Yang,Chao Peng*

Main category: cs.AI

TL;DR: 提出In-Situ Self-Evolving范式，通过将连续任务交互作为经验流，使系统能从短期执行反馈中提炼长期可复用能力，无需真实标签。开发Yunjue Agent系统，通过迭代合成、优化和重用工具来应对新兴挑战，引入并行批量进化策略提高效率。


<details>
  <summary>Details</summary>
Motivation: 传统智能体系统在开放环境中面临挑战：任务分布持续漂移且外部监督稀缺，依赖静态工具集或离线训练无法适应动态变化，导致系统能力边界僵化且未知。

Method: 提出In-Situ Self-Evolving范式，将工具进化作为能力扩展的关键路径，提供可验证的二元反馈信号。开发Yunjue Agent系统，实现工具的迭代合成、优化和重用，并引入并行批量进化策略优化进化效率。

Result: 在五个不同基准测试的零起点设置下，相比专有基线模型取得显著性能提升。补充的暖启动评估证实积累的通用知识可以无缝迁移到新领域。提出了监测进化收敛的新度量标准。

Conclusion: In-Situ Self-Evolving范式能够使智能体在开放环境中持续自我进化，通过工具进化扩展能力边界，实现从短期反馈到长期能力的转化，为弹性、自进化智能研究提供了新方向。

Abstract: Conventional agent systems often struggle in open-ended environments where task distributions continuously drift and external supervision is scarce. Their reliance on static toolsets or offline training lags behind these dynamics, leaving the system's capability boundaries rigid and unknown. To address this, we propose the In-Situ Self-Evolving paradigm. This approach treats sequential task interactions as a continuous stream of experience, enabling the system to distill short-term execution feedback into long-term, reusable capabilities without access to ground-truth labels. Within this framework, we identify tool evolution as the critical pathway for capability expansion, which provides verifiable, binary feedback signals. Within this framework, we develop Yunjue Agent, a system that iteratively synthesizes, optimizes, and reuses tools to navigate emerging challenges. To optimize evolutionary efficiency, we further introduce a Parallel Batch Evolution strategy. Empirical evaluations across five diverse benchmarks under a zero-start setting demonstrate significant performance gains over proprietary baselines. Additionally, complementary warm-start evaluations confirm that the accumulated general knowledge can be seamlessly transferred to novel domains. Finally, we propose a novel metric to monitor evolution convergence, serving as a function analogous to training loss in conventional optimization. We open-source our codebase, system traces, and evolved tools to facilitate future research in resilient, self-evolving intelligence.

</details>


### [52] [Think-Augmented Function Calling: Improving LLM Parameter Accuracy Through Embedded Reasoning](https://arxiv.org/abs/2601.18282)
*Lei Wei,Jinpeng Ou,Xiao Peng,Bin Wang*

Main category: cs.AI

TL;DR: TAFC框架通过引入显式推理机制，在函数和参数层面增强大语言模型的函数调用能力，提高参数生成准确性和推理透明度，同时保持API兼容性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在函数调用中缺乏参数生成的显式推理透明度，特别是对于具有相互依赖参数的复杂函数。现有方法如思维链提示在智能体层面操作，无法为单个函数参数提供细粒度的推理指导。

Method: 提出Think-Augmented Function Calling (TAFC)框架，引入通用的"think"参数增强机制，使模型能够表达决策过程；通过动态优化参数描述提高推理质量；对复杂参数基于复杂度评分自动触发细粒度推理；提出推理引导优化以对齐人类期望。

Result: 在ToolBench上对专有和开源模型的评估显示，TAFC在多参数函数的参数生成准确性和推理连贯性方面有显著提升，同时为调试AI智能体行为提供增强的可解释性。

Conclusion: TAFC框架无需修改现有LLM架构，保持完全API兼容性，通过显式推理机制有效提升了函数调用的准确性和透明度，为复杂参数决策提供适当的论证。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in function calling for autonomous agents, yet current mechanisms lack explicit reasoning transparency during parameter generation, particularly for complex functions with interdependent parameters. While existing approaches like chain-of-thought prompting operate at the agent level, they fail to provide fine-grained reasoning guidance for individual function parameters. To address these limitations, we propose Think-Augmented Function Calling (TAFC), a novel framework that enhances function calling accuracy through explicit reasoning at both function and parameter levels. Our method introduces a universal "think" parameter augmentation that enables models to articulate their decision-making process, with dynamic optimization for parameter descriptions to improve reasoning quality. For complex parameters, TAFC automatically triggers granular reasoning based on complexity scoring, ensuring appropriate justification for critical decisions. Additionally, we propose reasoning-guided optimization to align generated reasoning with human expectations. TAFC requires no architectural modifications to existing LLMs while maintaining full API compatibility. Evaluation on ToolBench across proprietary and open-source models demonstrates significant improvements in parameter generation accuracy and reasoning coherence for multi-parameter functions, while providing enhanced interpretability for debugging AI agent behaviors.

</details>


### [53] [A Generative AI-Driven Reliability Layer for Action-Oriented Disaster Resilience](https://arxiv.org/abs/2601.18308)
*Geunsik Lim*

Main category: cs.AI

TL;DR: Climate RADAR是一个基于生成式AI的可靠性层，将传统预警系统从警报传递转变为行动执行，通过整合多源数据和LLM提供个性化建议，提高保护行动执行率并减少响应延迟。


<details>
  <summary>Details</summary>
Motivation: 传统早期预警系统虽然能快速传播警报，但往往无法触发及时的保护行动，导致可预防的损失和不平等问题。需要一种能够将警报转化为实际行动的系统。

Method: 开发Climate RADAR系统，整合气象、水文、脆弱性和社会数据形成综合风险指数，使用带有护栏的大型语言模型为公民、志愿者和市政部门提供个性化行动建议。

Result: 通过模拟、用户研究和市政试点评估显示，系统提高了保护行动执行率，减少了响应延迟，增强了可用性和信任度。

Conclusion: Climate RADAR结合预测分析、行为科学和负责任AI，推进以人为本、透明和公平的早期预警系统，为符合要求的灾害韧性基础设施提供实用路径。

Abstract: As climate-related hazards intensify, conventional early warning systems (EWS) disseminate alerts rapidly but often fail to trigger timely protective actions, leading to preventable losses and inequities. We introduce Climate RADAR (Risk-Aware, Dynamic, and Action Recommendation system), a generative AI-based reliability layer that reframes disaster communication from alerts delivered to actions executed. It integrates meteorological, hydrological, vulnerability, and social data into a composite risk index and employs guardrail-embedded large language models (LLMs) to deliver personalized recommendations across citizen, volunteer, and municipal interfaces. Evaluation through simulations, user studies, and a municipal pilot shows improved outcomes, including higher protective action execution, reduced response latency, and increased usability and trust. By combining predictive analytics, behavioral science, and responsible AI, Climate RADAR advances people-centered, transparent, and equitable early warning systems, offering practical pathways toward compliance-ready disaster resilience infrastructures.

</details>


### [54] [Can Good Writing Be Generative? Expert-Level AI Writing Emerges through Fine-Tuning on High-Quality Books](https://arxiv.org/abs/2601.18353)
*Tuhin Chakrabarty,Paramveer S. Dhillon*

Main category: cs.AI

TL;DR: 专家作家与LLM在模仿50位知名作家风格上的竞争实验显示：在上下文提示条件下，专家评委82.7%偏好人类作品；但在对作者完整作品微调后，62%偏好AI作品。普通评委则始终偏好AI写作。


<details>
  <summary>Details</summary>
Motivation: 挑战传统观念——创意写作是机器无法复制的独特人类活动。研究生成式AI能否模拟数千种作者风格，以及这对创意写作的未来意味着什么。

Method: 行为实验：28位MFA作家（专家）与三个LLM竞争模仿50位知名作家风格。通过28位专家评委和131位普通评委的盲对比评估，比较上下文提示和微调完整作品两种条件下的表现。

Result: 1. 上下文提示条件下，专家评委82.7%偏好人类写作；2. 微调完整作品后，专家评委62%偏好AI写作；3. 普通评委始终偏好AI写作；4. 专家作家对AI写作的偏好引发了身份危机和审美自信的侵蚀。

Conclusion: 研究挑战了关于AI创意局限性的论述，提出了关于创意劳动未来的根本性问题。AI不仅能模仿作家风格，还能在某些条件下超越人类表现，这对创意写作领域产生了深远影响。

Abstract: Creative writing has long been considered a uniquely human endeavor, requiring voice and style that machines could not replicate. This assumption is challenged by Generative AI that can emulate thousands of author styles in seconds with negligible marginal labor. To understand this better, we conducted a behavioral experiment where 28 MFA writers (experts) competed against three LLMs in emulating 50 critically acclaimed authors. Based on blind pairwise comparisons by 28 expert judges and 131 lay judges, we find that experts preferred human writing in 82.7% of cases under the in-context prompting condition but this reversed to 62% preference for AI after fine-tuning on authors' complete works. Lay judges, however, consistently preferred AI writing. Debrief interviews with expert writers revealed that their preference for AI writing triggered an identity crisis, eroding aesthetic confidence and questioning what constitutes "good writing." These findings challenge discourse about AI's creative limitations and raise fundamental questions about the future of creative labor.

</details>


### [55] [Dynamic Thinking-Token Selection for Efficient Reasoning in Large Reasoning Models](https://arxiv.org/abs/2601.18383)
*Zhenyuan Guo,Tong Chen,Wenlong Meng,Chen Gong,Xin Yu,Chengkun Wei,Wenzhi Chen*

Main category: cs.AI

TL;DR: 论文提出DynTS方法，通过注意力图分析推理轨迹，识别关键决策token并仅保留其KV缓存，优化大型推理模型的效率


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在生成推理轨迹时会产生大量内存占用和计算开销，成为效率瓶颈。研究发现推理轨迹中只有部分关键token对最终答案有决定性影响，其余token贡献可忽略

Method: 提出动态思维token选择（DynTS）方法：1）使用注意力图分析推理轨迹中token的影响；2）识别决策关键token；3）在推理过程中仅保留关键token的KV缓存状态，剔除冗余条目

Result: 通过选择性保留关键token的KV缓存，显著减少了内存占用和计算开销，优化了大型推理模型的推理效率

Conclusion: DynTS方法基于对推理轨迹中token重要性的分析，通过动态选择关键token并优化KV缓存管理，有效提升了大型推理模型的效率，同时保持推理质量

Abstract: Large Reasoning Models (LRMs) excel at solving complex problems by explicitly generating a reasoning trace before deriving the final answer. However, these extended generations incur substantial memory footprint and computational overhead, bottlenecking LRMs' efficiency. This work uses attention maps to analyze the influence of reasoning traces and uncover an interesting phenomenon: only some decision-critical tokens in a reasoning trace steer the model toward the final answer, while the remaining tokens contribute negligibly. Building on this observation, we propose Dynamic Thinking-Token Selection (DynTS). This method identifies decision-critical tokens and retains only their associated Key-Value (KV) cache states during inference, evicting the remaining redundant entries to optimize efficiency.

</details>


### [56] [OffSeeker: Online Reinforcement Learning Is Not All You Need for Deep Research Agents](https://arxiv.org/abs/2601.18467)
*Yuhang Zhou,Kai Zheng,Qiguang Chen,Mengkang Hu,Qingfeng Sun,Can Xu,Jingjing Chen*

Main category: cs.AI

TL;DR: 该论文提出了一种完全离线的训练方法DeepForge，用于构建强大的研究智能体，避免了昂贵的在线强化学习，并在多个基准测试中取得了与更大模型相当的性能。


<details>
  <summary>Details</summary>
Motivation: 当前深度研究智能体在长时任务中表现出色，但最佳性能通常依赖昂贵的在线强化学习（需要大量API调用）。离线训练虽然更高效，但受限于高质量研究轨迹的稀缺性。作者旨在证明昂贵的在线强化学习并非构建强大研究智能体的唯一途径。

Method: 提出了一个完全开源的离线训练套件，核心贡献包括：1) DeepForge - 无需繁重预处理即可生成大规模研究查询的任务合成框架；2) 精心策划的数据集，包含66k QA对、33k SFT轨迹和21k DPO对。利用这些资源，完全离线训练了8B参数的OffSeeker模型。

Result: 在六个基准测试上的广泛评估表明，OffSeeker不仅在相似规模智能体中领先，而且与通过大量在线RL训练的30B参数系统保持竞争力。

Conclusion: 证明了昂贵的在线强化学习并非构建强大研究智能体的必要条件，通过有效的离线训练方法（DeepForge和高质量数据集）可以开发出具有竞争力的研究智能体，为更高效的研究智能体开发提供了新途径。

Abstract: Deep research agents have shown remarkable potential in handling long-horizon tasks. However, state-of-the-art performance typically relies on online reinforcement learning (RL), which is financially expensive due to extensive API calls. While offline training offers a more efficient alternative, its progress is hindered by the scarcity of high-quality research trajectories. In this paper, we demonstrate that expensive online reinforcement learning is not all you need to build powerful research agents. To bridge this gap, we introduce a fully open-source suite designed for effective offline training. Our core contributions include DeepForge, a ready-to-use task synthesis framework that generates large-scale research queries without heavy preprocessing; and a curated collection of 66k QA pairs, 33k SFT trajectories, and 21k DPO pairs. Leveraging these resources, we train OffSeeker (8B), a model developed entirely offline. Extensive evaluations across six benchmarks show that OffSeeker not only leads among similar-sized agents but also remains competitive with 30B-parameter systems trained via heavy online RL.

</details>


### [57] [AgentDoG: A Diagnostic Guardrail Framework for AI Agent Safety and Security](https://arxiv.org/abs/2601.18491)
*Dongrui Liu,Qihan Ren,Chen Qian,Shuai Shao,Yuejin Xie,Yu Li,Zhonghao Yang,Haoyu Luo,Peng Wang,Qingyu Liu,Binxin Hu,Ling Tang,Jilin Mei,Dadi Guo,Leitao Yuan,Junyao Yang,Guanxu Chen,Qihao Lin,Yi Yu,Bo Zhang,Jiaxuan Guo,Jie Zhang,Wenqi Shao,Huiqi Deng,Zhiheng Xi,Wenjie Wang,Wenxuan Wang,Wen Shen,Zhikai Chen,Haoyu Xie,Jialing Tao,Juntao Dai,Jiaming Ji,Zhongjie Ba,Linfeng Zhang,Yong Liu,Quanshi Zhang,Lei Zhu,Zhihua Wei,Hui Xue,Chaochao Lu,Jing Shao,Xia Hu*

Main category: cs.AI

TL;DR: 该论文提出了一种新的智能体安全防护框架AgentDoG，通过三维分类法对智能体风险进行分类，并创建了细粒度的安全基准ATBench，能够诊断不安全行为的根本原因。


<details>
  <summary>Details</summary>
Motivation: 随着AI智能体的兴起，自主工具使用和环境交互带来了复杂的安全挑战。现有的防护模型缺乏对智能体风险的认知和风险诊断的透明度，无法覆盖复杂多样的风险行为。

Method: 首先提出了统一的三维分类法，从来源（where）、失效模式（how）和后果（what）三个正交维度对智能体风险进行分类。基于此分类法，创建了细粒度的智能体安全基准ATBench，并开发了AgentDoG诊断防护框架，提供跨智能体轨迹的细粒度上下文监控和根本原因诊断。

Result: AgentDoG在Qwen和Llama模型家族中提供了三个规模版本（4B、7B和8B参数）。在多样复杂的交互场景中，AgentDoG在智能体安全调节方面达到了最先进的性能，超越了简单的二元标签，提供了溯源和透明度。

Conclusion: AgentDoG框架通过结构化分类法和诊断能力，为AI智能体安全提供了有效的防护方案，能够诊断不安全行为的根本原因，促进有效的智能体对齐。所有模型和数据集均已开源发布。

Abstract: The rise of AI agents introduces complex safety and security challenges arising from autonomous tool use and environmental interactions. Current guardrail models lack agentic risk awareness and transparency in risk diagnosis. To introduce an agentic guardrail that covers complex and numerous risky behaviors, we first propose a unified three-dimensional taxonomy that orthogonally categorizes agentic risks by their source (where), failure mode (how), and consequence (what). Guided by this structured and hierarchical taxonomy, we introduce a new fine-grained agentic safety benchmark (ATBench) and a Diagnostic Guardrail framework for agent safety and security (AgentDoG). AgentDoG provides fine-grained and contextual monitoring across agent trajectories. More Crucially, AgentDoG can diagnose the root causes of unsafe actions and seemingly safe but unreasonable actions, offering provenance and transparency beyond binary labels to facilitate effective agent alignment. AgentDoG variants are available in three sizes (4B, 7B, and 8B parameters) across Qwen and Llama model families. Extensive experimental results demonstrate that AgentDoG achieves state-of-the-art performance in agentic safety moderation in diverse and complex interactive scenarios. All models and datasets are openly released.

</details>


### [58] [DEEPMED: Building a Medical DeepResearch Agent via Multi-hop Med-Search Data and Turn-Controlled Agentic Training & Inference](https://arxiv.org/abs/2601.18496)
*Zihan wang,Hao Wang,Shi Feng,Xiaocui Yang,Daling Wang,Yiqun Zhang,Jinghao Lin,Haihua Yang,Xiaozhong Ji*

Main category: cs.AI

TL;DR: DeepMed：针对医学推理的深度研究模型，通过解决任务特性和工具使用扩展问题，在七个医学基准测试中平均提升基础模型9.79%


<details>
  <summary>Details</summary>
Motivation: 现有医学推理模型受限于参数化知识，容易遗忘和产生幻觉。通用深度研究模型虽然能基于工具证据生成输出，但直接应用于医学领域效果有限，主要因为任务特性差异和工具使用扩展问题。

Method: 1. 数据：采用多跳医学搜索QA合成方法，支持模型在医学上下文中应用深度研究范式；2. 训练：引入难度感知的轮次惩罚机制，抑制过度工具调用；3. 推理：加入监控器，在有限步骤内验证假设，避免上下文污染。

Result: 在七个医学基准测试中，DeepMed平均提升基础模型性能9.79%，超越了更大的医学推理模型和深度研究模型。

Conclusion: DeepMed通过专门设计的医学深度研究方法，有效解决了医学推理中的任务特性和工具使用扩展问题，显著提升了医学推理性能。

Abstract: Medical reasoning models remain constrained by parametric knowledge and are thus susceptible to forgetting and hallucinations. DeepResearch (DR) models ground outputs in verifiable evidence from tools and perform strongly in general domains, but their direct transfer to medical field yields relatively limited gains. We attribute this to two gaps: task characteristic and tool-use scaling. Medical questions require evidence interpretation in a knowledge-intensive clinical context; while general DR models can retrieve information, they often lack clinical-context reasoning and thus "find it but fail to use it," leaving performance limited by medical abilities. Moreover, in medical scenarios, blindly scaling tool-call can inject noisy context, derailing sensitive medical reasoning and prompting repetitive evidence-seeking along incorrect paths. Therefore, we propose DeepMed. For data, we deploy a multi-hop med-search QA synthesis method supporting the model to apply the DR paradigm in medical contexts. For training, we introduce a difficulty-aware turn-penalty to suppress excessive tool-call growth. For inference, we bring a monitor to help validate hypotheses within a controlled number of steps and avoid context rot. Overall, on seven medical benchmarks, DeepMed improves its base model by 9.79\% on average and outperforms larger medical reasoning and DR models.

</details>


### [59] [Deconstructing Instruction-Following: A New Benchmark for Granular Evaluation of Large Language Model Instruction Compliance Abilities](https://arxiv.org/abs/2601.18554)
*Alberto Purpura,Li Wang,Sahil Badyal,Eugenio Beaufrand,Adam Faulkner*

Main category: cs.AI

TL;DR: MOSAIC是一个模块化框架，通过动态生成包含最多20个应用导向约束的数据集，对LLM指令遵循能力进行细粒度独立分析，发现合规性受约束类型、数量和位置显著影响。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试往往无法反映真实世界使用情况，或未能将指令遵循能力与任务成功分离，因此需要更可靠的方法来评估LLM对复杂指令的遵循能力。

Method: 引入MOSAIC框架，使用动态生成的数据集，包含最多20个应用导向的生成约束，对指令遵循能力进行模块化、细粒度的独立分析。

Result: 对五个不同家族的LLM评估显示：合规性不是单一能力，而是显著受约束类型、数量和位置影响；揭示了模型特定弱点、指令间的协同与冲突关系，以及首因效应和近因效应等位置偏差。

Conclusion: 这些细粒度洞察对于诊断模型失败和开发需要严格遵循复杂指令的系统中的更可靠LLM至关重要。

Abstract: Reliably ensuring Large Language Models (LLMs) follow complex instructions is a critical challenge, as existing benchmarks often fail to reflect real-world use or isolate compliance from task success. We introduce MOSAIC (MOdular Synthetic Assessment of Instruction Compliance), a modular framework that uses a dynamically generated dataset with up to 20 application-oriented generation constraints to enable a granular and independent analysis of this capability. Our evaluation of five LLMs from different families based on this new benchmark demonstrates that compliance is not a monolithic capability but varies significantly with constraint type, quantity, and position. The analysis reveals model-specific weaknesses, uncovers synergistic and conflicting interactions between instructions, and identifies distinct positional biases such as primacy and recency effects. These granular insights are critical for diagnosing model failures and developing more reliable LLMs for systems that demand strict adherence to complex instructions.

</details>


### [60] [Stability as a Liability:Systematic Breakdown of Linguistic Structure in LLMs](https://arxiv.org/abs/2601.18588)
*Xianzhe Meng,Qiangsheng Zeng,Ling Luo,Qinghan Yang,Jiarui Hao,Wenbo Wu,Qinyu Wang,Rui Yin,Lin Qi,Renzhi Lu*

Main category: cs.AI

TL;DR: 研究发现训练稳定性与生成质量并不一致：稳定训练会导致模型输出低熵、重复，尽管损失平滑收敛


<details>
  <summary>Details</summary>
Motivation: 分析训练稳定性对生成分布的影响，探索稳定性是否必然带来良好的生成质量

Method: 理论分析最大似然训练下稳定参数轨迹的影响，并使用基于反馈的训练框架进行实证验证

Result: 稳定训练使模型近似最小化前向KL散度，同时隐式降低生成熵，导致模型集中在有限的经验模式上，产生重复行为

Conclusion: 优化稳定性与生成表达能力并不一致，稳定性本身不足以作为生成质量的指标

Abstract: Training stability is typically regarded as a prerequisite for reliable optimization in large language models. In this work, we analyze how stabilizing training dynamics affects the induced generation distribution. We show that under standard maximum likelihood training, stable parameter trajectories lead stationary solutions to approximately minimize the forward KL divergence to the empirical distribution, while implicitly reducing generative entropy. As a consequence, the learned model can concentrate probability mass on a limited subset of empirical modes, exhibiting systematic degeneration despite smooth loss convergence. We empirically validate this effect using a controlled feedback-based training framework that stabilizes internal generation statistics, observing consistent low-entropy outputs and repetitive behavior across architectures and random seeds. It indicates that optimization stability and generative expressivity are not inherently aligned, and that stability alone is an insufficient indicator of generative quality.

</details>


### [61] [A Balanced Neuro-Symbolic Approach for Commonsense Abductive Logic](https://arxiv.org/abs/2601.18595)
*Joseph Cotnareanu,Didier Chetelat,Yingxue Zhang,Mark Coates*

Main category: cs.AI

TL;DR: 本文提出了一种结合LLM和逻辑求解器的新方法，通过迭代反馈机制补充缺失的常识关系，提升复杂推理问题的解决能力。


<details>
  <summary>Details</summary>
Motivation: 虽然大型语言模型在形式推理方面表现出色，但在需要复杂证明规划的问题上经常失败。现有逻辑求解器虽然推理效率高，但假设所有相关事实都已提供，无法处理缺失的常识关系。

Method: 提出了一种新颖方法：使用逻辑求解器的反馈来迭代地增强逻辑问题，通过LLM提供常识关系。这涉及一个搜索过程，通过潜在的常识假设来最大化找到有用事实的机会，同时保持成本可控。

Result: 在一系列纯逻辑推理数据集上（其中部分常识信息已被移除），该方法相比现有技术始终取得了显著改进。

Conclusion: 在人类语境中工作时，平衡神经和符号元素具有重要价值，该方法展示了结合LLM和逻辑求解器的优势。

Abstract: Although Large Language Models (LLMs) have demonstrated impressive formal reasoning abilities, they often break down when problems require complex proof planning. One promising approach for improving LLM reasoning abilities involves translating problems into formal logic and using a logic solver. Although off-the-shelf logic solvers are in principle substantially more efficient than LLMs at logical reasoning, they assume that all relevant facts are provided in a question and are unable to deal with missing commonsense relations. In this work, we propose a novel method that uses feedback from the logic solver to augment a logic problem with commonsense relations provided by the LLM, in an iterative manner. This involves a search procedure through potential commonsense assumptions to maximize the chance of finding useful facts while keeping cost tractable. On a collection of pure-logical reasoning datasets, from which some commonsense information has been removed, our method consistently achieves considerable improvements over existing techniques, demonstrating the value in balancing neural and symbolic elements when working in human contexts.

</details>


### [62] [Assessing the Quality of Mental Health Support in LLM Responses through Multi-Attribute Human Evaluation](https://arxiv.org/abs/2601.18630)
*Abeer Badawi,Md Tahmid Rahman Laskar,Elahe Rahimi,Sheri Grach,Lindsay Bertrand,Lames Danok,Frank Rudzicz,Jimmy Huang,Elham Dolatabadi*

Main category: cs.AI

TL;DR: 本文提出了一种基于人类评估的方法来评估大语言模型在心理健康对话中的表现，发现LLMs在认知支持方面表现可靠，但在情感共鸣方面存在不足，揭示了认知-情感差距。


<details>
  <summary>Details</summary>
Motivation: 全球心理健康危机日益严重，存在治疗缺口、资源不足和治疗师短缺等问题，大语言模型作为可扩展的心理支持工具具有潜力，但其可靠性、治疗相关性和与人类标准的对齐程度仍存在挑战。

Method: 开发了一种基于人类评估的方法，从真实场景数据集中收集500个心理健康对话，评估9个不同LLMs（包括闭源和开源模型）生成的回应。由两位精神病学专家使用5点李克特量表，基于包含6个属性的评估框架（涵盖认知支持和情感共鸣）进行独立评分。

Result: LLMs在认知可靠性方面表现强劲，能产生安全、连贯且临床适当的信息，但在情感对齐方面表现不稳定。闭源模型（如GPT-4o）提供更平衡的治疗回应，而开源模型表现出更大的变异性和情感平淡。研究揭示了持续的认知-情感差距。

Conclusion: 需要建立具有故障意识、临床基础的评估框架，在心理健康导向的LLMs中优先考虑关系敏感性而不仅仅是信息准确性。提倡采用以治疗敏感性为中心、包含人类参与者的平衡评估协议，为心理健康对话AI的负责任设计和临床监督提供指导框架。

Abstract: The escalating global mental health crisis, marked by persistent treatment gaps, availability, and a shortage of qualified therapists, positions Large Language Models (LLMs) as a promising avenue for scalable support. While LLMs offer potential for accessible emotional assistance, their reliability, therapeutic relevance, and alignment with human standards remain challenging to address. This paper introduces a human-grounded evaluation methodology designed to assess LLM generated responses in therapeutic dialogue. Our approach involved curating a dataset of 500 mental health conversations from datasets with real-world scenario questions and evaluating the responses generated by nine diverse LLMs, including closed source and open source models. More specifically, these responses were evaluated by two psychiatric trained experts, who independently rated each on a 5 point Likert scale across a comprehensive 6 attribute rubric. This rubric captures Cognitive Support and Affective Resonance, providing a multidimensional perspective on therapeutic quality. Our analysis reveals that LLMs provide strong cognitive reliability by producing safe, coherent, and clinically appropriate information, but they demonstrate unstable affective alignment. Although closed source models (e.g., GPT-4o) offer balanced therapeutic responses, open source models show greater variability and emotional flatness. We reveal a persistent cognitive-affective gap and highlight the need for failure aware, clinically grounded evaluation frameworks that prioritize relational sensitivity alongside informational accuracy in mental health oriented LLMs. We advocate for balanced evaluation protocols with human in the loop that center on therapeutic sensitivity and provide a framework to guide the responsible design and clinical oversight of mental health oriented conversational AI.

</details>


### [63] [AdaReasoner: Dynamic Tool Orchestration for Iterative Visual Reasoning](https://arxiv.org/abs/2601.18631)
*Mingyang Song,Haoyu Sun,Jiawei Gu,Linjie Li,Luxin Xu,Ranjay Krishna,Yu Cheng*

Main category: cs.AI

TL;DR: AdaReasoner是一个多模态模型家族，通过学习工具使用作为通用推理技能而非特定工具或显式监督行为，实现了在视觉推理任务中自主选择、组合和调整工具使用的能力。


<details>
  <summary>Details</summary>
Motivation: 人类在面对超出自身能力的问题时会依赖工具，这为提高多模态大语言模型的视觉推理能力提供了有前景的范式。有效推理的关键在于知道使用哪些工具、何时调用它们以及如何在多步骤中组合它们，即使面对新工具或新任务时也是如此。

Method: AdaReasoner通过三个关键组件实现：(1) 可扩展的数据管理流程，让模型接触长视野、多步骤的工具交互；(2) Tool-GRPO强化学习算法，基于最终任务成功优化工具选择和序列；(3) 自适应学习机制，动态调节工具使用频率。

Result: AdaReasoner表现出强大的工具自适应和泛化行为：自主采用有益工具、抑制无关工具、根据任务需求调整工具使用频率，尽管从未被显式训练这样做。这些能力转化为在多个挑战性基准测试中的最先进性能，将7B基础模型平均提升24.9%，并在VSP和Jigsaw等多个任务上超越GPT-5等强大专有系统。

Conclusion: AdaReasoner通过将工具使用作为通用推理技能学习，实现了在多模态视觉推理任务中自主、自适应地使用工具的能力，显著提升了模型性能并展现出强大的泛化能力。

Abstract: When humans face problems beyond their immediate capabilities, they rely on tools, providing a promising paradigm for improving visual reasoning in multimodal large language models (MLLMs). Effective reasoning, therefore, hinges on knowing which tools to use, when to invoke them, and how to compose them over multiple steps, even when faced with new tools or new tasks. We introduce \textbf{AdaReasoner}, a family of multimodal models that learn tool use as a general reasoning skill rather than as tool-specific or explicitly supervised behavior. AdaReasoner is enabled by (i) a scalable data curation pipeline exposing models to long-horizon, multi-step tool interactions; (ii) Tool-GRPO, a reinforcement learning algorithm that optimizes tool selection and sequencing based on end-task success; and (iii) an adaptive learning mechanism that dynamically regulates tool usage. Together, these components allow models to infer tool utility from task context and intermediate outcomes, enabling coordination of multiple tools and generalization to unseen tools. Empirically, AdaReasoner exhibits strong tool-adaptive and generalization behaviors: it autonomously adopts beneficial tools, suppresses irrelevant ones, and adjusts tool usage frequency based on task demands, despite never being explicitly trained to do so. These capabilities translate into state-of-the-art performance across challenging benchmarks, improving the 7B base model by +24.9\% on average and surpassing strong proprietary systems such as GPT-5 on multiple tasks, including VSP and Jigsaw.

</details>


### [64] [FadeMem: Biologically-Inspired Forgetting for Efficient Agent Memory](https://arxiv.org/abs/2601.18642)
*Lei Wei,Xu Dong,Xiao Peng,Niantao Xie,Bin Wang*

Main category: cs.AI

TL;DR: FadeMem是一种受生物学启发的智能体记忆架构，通过引入主动遗忘机制解决大语言模型记忆限制问题，实现选择性遗忘和高效信息管理


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型作为自主智能体部署时面临关键记忆限制，缺乏选择性遗忘机制，导致在上下文边界出现灾难性遗忘或在内部信息过载。人类记忆通过自适应衰减过程自然平衡保留与遗忘，而现有AI系统采用二元保留策略，要么保留一切要么完全丢失

Method: FadeMem采用受生物学启发的智能体记忆架构，在双层记忆层次结构中实现差异化衰减率，保留由自适应指数衰减函数控制，该函数受语义相关性、访问频率和时间模式调节。通过LLM引导的冲突解决和智能记忆融合，系统在整合相关信息的同时允许无关细节衰减

Result: 在Multi-Session Chat、LoCoMo和LTI-Bench上的实验表明，系统在多跳推理和检索方面表现优异，同时实现了45%的存储减少，验证了生物学启发遗忘在智能体记忆系统中的有效性

Conclusion: FadeMem通过引入受人类认知启发的主动遗忘机制，成功解决了大语言模型智能体的记忆管理问题，在保持推理能力的同时显著减少了存储需求，为构建更高效的自主智能体系统提供了新思路

Abstract: Large language models deployed as autonomous agents face critical memory limitations, lacking selective forgetting mechanisms that lead to either catastrophic forgetting at context boundaries or information overload within them. While human memory naturally balances retention and forgetting through adaptive decay processes, current AI systems employ binary retention strategies that preserve everything or lose it entirely. We propose FadeMem, a biologically-inspired agent memory architecture that incorporates active forgetting mechanisms mirroring human cognitive efficiency. FadeMem implements differential decay rates across a dual-layer memory hierarchy, where retention is governed by adaptive exponential decay functions modulated by semantic relevance, access frequency, and temporal patterns. Through LLM-guided conflict resolution and intelligent memory fusion, our system consolidates related information while allowing irrelevant details to fade. Experiments on Multi-Session Chat, LoCoMo, and LTI-Bench demonstrate superior multi-hop reasoning and retrieval with 45\% storage reduction, validating the effectiveness of biologically-inspired forgetting in agent memory systems.

</details>


### [65] [Health-SCORE: Towards Scalable Rubrics for Improving Health-LLMs](https://arxiv.org/abs/2601.18706)
*Zhichao Yang,Sepehr Janghorbani,Dongxu Zhang,Jun Han,Qian Qian,Andrew Ressler,Gregory D. Lyng,Sanjit Singh Batra,Robert E. Tillman*

Main category: cs.AI

TL;DR: Health-SCORE是一个可扩展的基于评分标准的训练和评估框架，显著降低医疗领域评分标准开发成本，同时保持评估质量


<details>
  <summary>Details</summary>
Motivation: 在医疗等安全关键领域，评分标准对于评估开放式LLM响应至关重要，但创建高质量、领域特定的评分标准需要大量专家时间和开发成本，使得基于评分标准的评估和训练难以扩展

Method: 开发了Health-SCORE框架，这是一个通用且可扩展的基于评分标准的训练和评估系统，通过减少评分标准开发成本而不牺牲性能

Result: Health-SCORE在开放式医疗任务中实现了与人工创建评分标准相当的评估质量，同时显著降低了开发工作量；该框架还可作为结构化奖励信号指导强化学习，并可通过上下文学习直接整合到提示中以提高响应质量

Conclusion: Health-SCORE使基于评分标准的评估和训练更加可扩展，为医疗领域的LLM评估提供了实用且高效的解决方案

Abstract: Rubrics are essential for evaluating open-ended LLM responses, especially in safety-critical domains such as healthcare. However, creating high-quality and domain-specific rubrics typically requires significant human expertise time and development cost, making rubric-based evaluation and training difficult to scale. In this work, we introduce Health-SCORE, a generalizable and scalable rubric-based training and evaluation framework that substantially reduces rubric development costs without sacrificing performance. We show that Health-SCORE provides two practical benefits beyond standalone evaluation: it can be used as a structured reward signal to guide reinforcement learning with safety-aware supervision, and it can be incorporated directly into prompts to improve response quality through in-context learning. Across open-ended healthcare tasks, Health-SCORE achieves evaluation quality comparable to human-created rubrics while significantly lowering development effort, making rubric-based evaluation and training more scalable.

</details>


### [66] [Conditioned Generative Modeling of Molecular Glues: A Realistic AI Approach for Synthesizable Drug-like Molecules](https://arxiv.org/abs/2601.18716)
*Naeyma N. Islam,Thomas R. Caulfield*

Main category: cs.AI

TL;DR: 该研究开发了一种AI辅助药物设计方法，通过E3连接酶导向的分子胶促进Aβ-42的靶向降解，用于阿尔茨海默病治疗。


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病中Aβ-42的病理积累导致突触功能障碍和神经退行性变，虽然细胞外淀粉样斑块研究较多，但细胞内Aβ-42作为早期毒性驱动因素的重要性日益凸显，需要开发靶向降解策略。

Method: 采用AI辅助药物设计方法，通过结构建模、ADMET筛选和对接评估Aβ-42与三种E3连接酶（CRBN、VHL、MDM2）的三元复合物形成潜力，开发了连接酶条件化连接树变分自编码器（LC-JT-VAE）生成连接酶特异性小分子。

Result: 生成模型能够产生化学有效、新颖且靶向特异性的分子胶，能够促进Aβ-42降解，为神经退行性疾病治疗提供了有前景的框架。

Conclusion: 这种集成方法为设计泛素-蛋白酶体系统靶向疗法提供了有前景的框架，可用于神经退行性疾病的治疗。

Abstract: Alzheimer's disease (AD) is marked by the pathological accumulation of amyloid beta-42 (Abeta-42), contributing to synaptic dysfunction and neurodegeneration. While extracellular amyloid plaques are well-studied, increasing evidence highlights intracellular Abeta-42 as an early and toxic driver of disease progression. In this study, we present a novel, AI-assisted drug design approach to promote targeted degradation of Abeta-42 via the ubiquitin-proteasome system (UPS), using E3 ligase-directed molecular glues. We systematically evaluated the ternary complex formation potential of Abeta-42 with three E3 ligases: CRBN, VHL, and MDM2, through structure-based modeling, ADMET screening, and docking. We then developed a Ligase-Conditioned Junction Tree Variational Autoencoder (LC-JT-VAE) to generate ligase-specific small molecules, incorporating protein sequence embeddings and torsional angle-aware molecular graphs. Our results demonstrate that this generative model can produce chemically valid, novel, and target-specific molecular glues capable of facilitating Abeta-42 degradation. This integrated approach offers a promising framework for designing UPS-targeted therapies for neurodegenerative diseases.

</details>


### [67] [TSRBench: A Comprehensive Multi-task Multi-modal Time Series Reasoning Benchmark for Generalist Models](https://arxiv.org/abs/2601.18744)
*Fangxu Yu,Xingang Guo,Lingzhi Yuan,Haoqiang Kang,Hongyu Zhao,Lianhui Qin,Furong Huang,Bin Hu,Tianyi Zhou*

Main category: cs.AI

TL;DR: TSRBench是一个全面的多模态时间序列推理基准测试，包含4125个问题、14个领域、4个维度和15个任务，用于评估通用模型的时间序列推理能力。


<details>
  <summary>Details</summary>
Motivation: 时间序列数据在现实场景中无处不在且对关键应用至关重要，但现有通用模型基准测试缺乏时间序列维度，需要填补这一空白以评估模型的时间序列推理能力。

Method: 构建TSRBench基准测试，包含4125个来自14个领域的问题，分为感知、推理、预测和决策制定4个主要维度，包含15个评估基本推理能力的任务。

Result: 评估了30多个领先的专有和开源LLM、VLM和TSLLM，发现：1）缩放定律适用于感知和推理但在预测中失效；2）强推理能力不能保证准确的上下文感知预测；3）当前多模态模型未能有效融合文本和视觉表示以获得互补性能增益。

Conclusion: TSRBench提供了一个标准化评估平台，不仅突显了现有挑战，还为推进通用模型提供了有价值的见解，揭示了时间序列推理中语义理解和数值预测之间的脱节问题。

Abstract: Time series data is ubiquitous in real-world scenarios and crucial for critical applications ranging from energy management to traffic control. Consequently, the ability to reason over time series is a fundamental skill for generalist models to solve practical problems. However, this dimension is notably absent from existing benchmarks of generalist models. To bridge this gap, we introduce TSRBench, a comprehensive multi-modal benchmark designed to stress-test the full spectrum of time series reasoning capabilities. TSRBench features: i) a diverse set of 4125 problems from 14 domains, and is categorized into 4 major dimensions: Perception, Reasoning, Prediction, and Decision-Making. ii) 15 tasks from the 4 dimensions evaluating essential reasoning capabilities (e.g., numerical reasoning). Through extensive experiments, we evaluated over 30 leading proprietary and open-source LLMs, VLMs, and TSLLMs within TSRBench. Our findings reveal that: i) scaling laws hold for perception and reasoning but break down for prediction; ii) strong reasoning does not guarantee accurate context-aware forecasting, indicating a decoupling between semantic understanding and numerical prediction; and iii) despite the complementary nature of textual and visual represenations of time series as inputs, current multimodal models fail to effectively fuse them for reciprocal performance gains. TSRBench provides a standardized evaluation platform that not only highlights existing challenges but also offers valuable insights to advance generalist models. Our code and dataset are available at https://tsrbench.github.io/.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [68] [Risk-based test framework for LLM features in regulated software](https://arxiv.org/abs/2601.17292)
*Zhiyin Zhou*

Main category: cs.SE

TL;DR: 提出基于风险的LLM功能测试框架，包括六类风险分类、分层测试策略，并在临床研究平台的知识库助手中进行案例研究


<details>
  <summary>Details</summary>
Motivation: 大型语言模型越来越多地嵌入到受监管和安全关键的软件中，如临床研究平台和医疗信息系统。这些功能虽然提供了自然语言搜索、摘要和配置辅助等能力，但也引入了幻觉、有害建议、隐私安全、偏见、不稳定性和对抗性滥用等风险。现有机器学习测试和AI保障研究提供了有用概念，但对交互式、产品嵌入式助手的指导有限。

Method: 提出基于风险的测试框架：1）六类风险分类法；2）分层测试策略，将风险映射到护栏层、编排层和系统层的具体测试；3）在临床研究平台的知识库助手中进行案例研究，应用该方法。

Result: 论文提出了一个系统化的测试框架，能够识别和测试LLM在受监管软件中的各种风险。通过案例研究验证了该框架在临床研究平台知识库助手场景中的适用性和有效性。

Conclusion: 该框架为受监管软件中LLM功能的测试提供了系统化方法，填补了现有AI测试方法在交互式、产品嵌入式助手方面的指导空白，有助于确保LLM在安全关键应用中的可靠部署。

Abstract: Large language models are increasingly embedded in regulated and safety-critical software, including clinical research platforms and healthcare information systems. While these features enable natural language search, summarization, and configuration assistance, they introduce risks such as hallucinations, harmful or out-of-scope advice, privacy and security issues, bias, instability under change, and adversarial misuse. Prior work on machine learning testing and AI assurance offers useful concepts but limited guidance for interactive, product-embedded assistants. This paper proposes a risk-based testing framework for LLM features in regulated software: a six-category risk taxonomy, a layered test strategy mapping risks to concrete tests across guardrail, orchestration, and system layers, and a case study applying the approach to a Knowledgebase assistant in a clinical research platform.

</details>


### [69] [YASA: Scalable Multi-Language Taint Analysis on the Unified AST at Ant Group](https://arxiv.org/abs/2601.17390)
*Yayi Wang,Shenao Wang,Jian Zhao,Shaosen Shi,Ting Li,Yan Cheng,Lizhong Bian,Kan Yu,Yanjie Zhao,Haoyu Wang*

Main category: cs.SE

TL;DR: YASA是一个统一的多语言静态污点分析框架，专为工业级部署设计，在Ant Group中成功分析了超过1亿行代码，发现了314条先前未知的污点路径，其中92条被确认为0-day漏洞。


<details>
  <summary>Details</summary>
Motivation: 现代企业采用多种编程语言技术栈，给静态应用安全测试带来挑战。现有的污点分析工具主要针对单一语言设计，而多语言工具在中间表示设计、分析精度和可扩展性方面存在局限，难以在Ant Group等大规模工业应用中有效扩展。

Method: YASA引入了统一抽象语法树（UAST），为不同编程语言提供统一抽象。基于UAST，YASA执行点对点分析和污点传播，利用统一语义模型管理语言无关的构造，同时结合语言特定的语义模型处理独特的语言特性。

Result: 在行业标准基准测试中，YASA在Java、JavaScript、Python和Go上持续优于6个单语言和2个多语言静态分析器。在Ant Group的实际部署中，YASA分析了超过1亿行代码，发现了314条先前未知的污点路径，其中92条被确认为0-day漏洞，76个漏洞已被内部开发团队修复。

Conclusion: YASA是一个有效的多语言静态污点分析框架，能够在大规模工业软件系统中实际应用，显著提高了跨多种编程语言的安全漏洞检测能力。

Abstract: Modern enterprises increasingly adopt diverse technology stacks with various programming languages, posing significant challenges for static application security testing (SAST). Existing taint analysis tools are predominantly designed for single languages, requiring substantial engineering effort that scales with language diversity. While multi-language tools like CodeQL, Joern, and WALA attempt to address these challenges, they face limitations in intermediate representation design, analysis precision, and extensibility, which make them difficult to scale effectively for large-scale industrial applications at Ant Group. To bridge this gap, we present YASA (Yet Another Static Analyzer), a unified multi-language static taint analysis framework designed for industrial-scale deployment. Specifically, YASA introduces the Unified Abstract Syntax Tree (UAST) that provides a unified abstraction for compatibility across diverse programming languages. Building on the UAST, YASA performs point-to analysis and taint propagation, leveraging a unified semantic model to manage language-agnostic constructs, while incorporating language-specific semantic models to handle other unique language features. When compared to 6 single- and 2 multi-language static analyzers on an industry-standard benchmark, YASA consistently outperformed all baselines across Java, JavaScript, Python, and Go. In real-world deployment within Ant Group, YASA analyzed over 100 million lines of code across 7.3K internal applications. It identified 314 previously unknown taint paths, with 92 of them confirmed as 0-day vulnerabilities. All vulnerabilities were responsibly reported, with 76 already patched by internal development teams, demonstrating YASA's practical effectiveness for securing large-scale industrial software systems.

</details>


### [70] [Towards a Declarative Agentic Layer for Intelligent Agents in MCP-Based Server Ecosystems](https://arxiv.org/abs/2601.17435)
*Maria Jesus Rodriguez-Sanchez,Manuel Noguera,Angel Ruiz-Zafra,Kawtar Benghazi*

Main category: cs.SE

TL;DR: DALIA是一个声明式的、模型无关的智能体架构层，通过形式化可执行能力、声明式发现协议和确定性任务图，解决当前智能体系统的可靠性问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的智能体和多智能体系统存在根本性的可靠性问题，如幻觉动作、不可执行计划和脆弱的协调。这些问题并非源于底层模型本身的限制，而是由于缺乏明确连接目标、能力和执行的架构结构。

Method: 提出DALIA（声明式智能体层）架构层，包括：1）形式化可执行能力；2）通过声明式发现协议暴露任务；3）维护智能体及其执行资源的联邦目录；4）构建完全基于声明操作的确定性任务图。该架构强制分离发现、规划和执行阶段。

Result: 通过一个代表性的面向任务场景展示了DALIA的操作，证明声明式基础能够实现跨异构环境的可重现和可验证的智能体工作流。

Conclusion: DALIA架构通过约束智能体行为到可验证的操作空间，减少对推测推理和自由形式协调的依赖，从而解决当前智能体系统的可靠性问题，实现更稳健的智能体工作流。

Abstract: Recent advances in Large Language Models (LLMs) have enabled the development of increasingly complex agentic and multi-agent systems capable of planning, tool use and task decomposition. However, empirical evidence shows that many of these systems suffer from fundamental reliability issues, including hallucinated actions, unexecutable plans and brittle coordination. Crucially, these failures do not stem from limitations of the underlying models themselves, but from the absence of explicit architectural structure linking goals, capabilities and execution. This paper presents a declarative, model-independent architectural layer for grounded agentic workflows that addresses this gap. The proposed layer, referred to as DALIA (Declarative Agentic Layer for Intelligent Agents), formalises executable capabilities, exposes tasks through a declarative discovery protocol, maintains a federated directory of agents and their execution resources, and constructs deterministic task graphs grounded exclusively in declared operations. By enforcing a clear separation between discovery, planning and execution, the architecture constrains agent behaviour to a verifiable operational space, reducing reliance on speculative reasoning and free-form coordination. We present the architecture and design principles of the proposed layer and illustrate its operation through a representative task-oriented scenario, demonstrating how declarative grounding enables reproducible and verifiable agentic workflows across heterogeneous environments.

</details>


### [71] [Data-driven Test Generation for Fuzzing AI Compiler](https://arxiv.org/abs/2601.17450)
*Qingchao Shen*

Main category: cs.SE

TL;DR: 提出统一的数据驱动测试框架，通过三个组件系统解决AI编译器不同阶段的测试挑战，在四个主流AI编译器中发现了266个未知bug。


<details>
  <summary>Details</summary>
Motivation: AI编译器对于在不同硬件平台上高效部署AI模型至关重要，但容易存在bug，这些bug会损害编译器可靠性和模型正确性，因此确保AI编译器质量非常重要。

Method: 提出统一的数据驱动测试框架，包含三个组件：1) OPERA：迁移AI库测试来测试模型加载阶段的算子转换逻辑；2) OATest：合成多样化的优化感知计算图来测试高级优化；3) HARMONY：生成和变异多样化的低级IR种子，生成硬件优化感知测试来测试低级优化。

Result: 该框架在四个广泛使用的AI编译器中检测到了266个先前未知的bug，证明了其测试覆盖率和有效性的提升。

Conclusion: 提出的统一数据驱动测试框架通过针对不同编译阶段的特定挑战，提供了全面、阶段感知的测试解决方案，显著提高了AI编译器的测试覆盖率和效果。

Abstract: Artificial Intelligence (AI) compilers are critical for efficiently deploying AI models across diverse hardware platforms. However, they remain prone to bugs that can compromise both compiler reliability and model correctness. Thus, ensuring the quality of AI compilers is crucial. In this work, we present a unified data-driven testing framework that systematically addresses stage-specific challenges in AI compilers. Specifically, OPERA migrates tests for AI libraries to test various operator conversion logic in the model loading stage. OATest synthesizes diverse optimization-aware computational graphs for testing high-level optimizations. HARMONY generates and mutates diverse low-level IR seeds to generate hardware-optimization-aware tests for testing low-level optimizations. Together, these techniques provide a comprehensive, stage-aware framework that enhances testing coverage and effectiveness, detecting 266 previously unknown bugs in four widely used AI compilers.

</details>


### [72] [LogPrism: Unifying Structure and Variable Encoding for Effective Log Compression](https://arxiv.org/abs/2601.17482)
*Yang Liu,Kaiming Zhang,Zhuangbin Chen,Jinyang Liu,Zibin Zheng*

Main category: cs.SE

TL;DR: LogPrism提出了一种统一的日志压缩框架，通过构建统一冗余树（URT）动态整合结构提取和变量编码，突破了传统"先解析后压缩"范式的局限性，在压缩比和处理速度上均达到新的最优水平。


<details>
  <summary>Details</summary>
Motivation: 传统的"先解析后压缩"范式将日志解析和压缩视为孤立目标，限制了压缩效果。解析器虽然注重语义准确性（事件识别），但往往掩盖了静态模板和动态变量之间的深度相关性，而这些相关性对存储效率至关重要。

Method: 提出LogPrism框架，通过构建统一冗余树（URT）实现统一冗余编码。该方法动态整合结构提取与变量编码，不依赖僵化的预解析步骤，通过分层方法挖掘"结构+变量"共现模式，捕获深度上下文冗余，并通过预模式编码加速处理。

Result: 在16个基准数据集上的实验证实LogPrism达到了新的最优水平：在13个数据集上获得最高压缩比，比领先基线高出4.7%到80.9%；处理速度达到29.87 MB/s，比竞争对手快1.68倍到43.04倍；在单归档模式下，压缩比比最佳基线高19.39%，同时保持2.62倍的速度优势。

Conclusion: LogPrism通过统一冗余编码有效解决了传统日志压缩中解析与压缩目标不一致的问题，在压缩效率和速度方面均实现了显著提升，为日志压缩领域提供了新的解决方案。

Abstract: The prevailing "parse-then-compress" paradigm in log compression fundamentally limits effectiveness by treating log parsing and compression as isolated objectives. While parsers prioritize semantic accuracy (i.e., event identification), they often obscure deep correlations between static templates and dynamic variables that are critical for storage efficiency. In this paper, we investigate this misalignment through a comprehensive empirical study and propose LogPrism, a framework that bridges the gap via unified redundancy encoding. Rather than relying on a rigid pre-parsing step, LogPrism dynamically integrates structural extraction with variable encoding by constructing a Unified Redundancy Tree (URT). This hierarchical approach effectively mines "structure+variable" co-occurrence patterns, capturing deep contextual redundancies while accelerating processing through pre-emptive pattern encoding. Extensive experiments on 16 benchmark datasets confirm that LogPrism establishes a new state-of-the-art. It achieves the highest compression ratio on 13 datasets, surpassing leading baselines by margins of 4.7% to 80.9%, while delivering superior throughput at 29.87 MB/s (1.68$\times$~43.04$\times$ faster than competitors). Moreover, when configured in single-archive mode to maximize global pattern discovery, LogPrism outperforms the best baseline by 19.39% in compression ratio while maintaining a 2.62$\times$ speed advantage.

</details>


### [73] [Measuring Braking Behavior Using Vehicle Tracking and Camera-to-Satellite Homography Rectification](https://arxiv.org/abs/2601.17558)
*J. P. Fleischer,Tanchanok Sirikanchittavon,Chonlachart Jeenprasom,Nooshin Yousefzadeh,Sanjay Ranka,Mohammed Hadi*

Main category: cs.SE

TL;DR: 开发了一个开源软件，用于分析交通摄像头视频，通过地面平面单应性估计将交通摄像头视角与卫星正射影像对齐，实现车辆轨迹、速度、减速度和制动严重程度的准确测量，无需摄像头标定。


<details>
  <summary>Details</summary>
Motivation: 开发一个无需摄像头标定就能准确分析交通摄像头视频中车辆行为的系统，支持联网车辆、主动交通管理、事故缓解以及数据驱动的道路设计和安全分析。

Method: 使用MAGSAC++估计器建立地面平面单应性，将固定交通摄像头视角与卫星正射影像对齐，校正倾斜视角；利用YOLO11进行目标检测，将检测结果转换为校正后的俯视坐标系；所有检测和轨迹数据存储在ClickHouse数据库中。

Result: 在佛罗里达州基韦斯特两个信号交叉口的实际案例研究中，高流量交叉口的制动活动在下午4点左右达到峰值（约57.5次/小时），第二个交叉口在上午10点左右达到峰值（约15.5次/小时）；空间分析显示大多数制动事件在上游开始，轻度和中度制动主要发生在距离停车线30-45+米处，而严重制动则分布更广，在交互和并道活动较多的车道中尤为集中。

Conclusion: 该系统展示了作为集中式安全信息系统的巨大潜力，能够支持联网车辆、促进主动交通管理、缓解事故，并为数据驱动的道路设计和安全分析提供支持。

Abstract: This paper presents an open-source software application for analyzing traffic camera footage, focusing on vehicle behavior and braking events at signalized urban highways. The core innovation is a robust ground-plane homography estimation that links fixed traffic camera views to satellite orthoimagery. This process rectifies the camera's oblique perspective, ensuring that pixel distances accurately represent real-world distances. This enables the acquisition of features such as vehicle trajectory, speed, deceleration, and braking severity without the need for camera calibration. The pipeline employs the MAGSAC++ estimator to build the homography, converting YOLO11 object detections into a rectified top-down coordinate system. All detection and trajectory data are stored in a ClickHouse database for subsequent analysis. A real-world case study at two signalized intersections in Key West, Florida, showcased the system's capabilities. Across two days of daytime footage, braking activity at the higher-volume intersection peaked around 4 PM at approximately 57.5 events per hour, while the second intersection peaked around 10 AM at roughly 15.5 events per hour. The spatial analysis revealed that most braking events initiated upstream, with mild and moderate braking mostly occurring 30 to 45+ meters away from the stop bar and severe braking distributed throughout, but particularly concentrated in lanes with higher interaction and merging activity. The findings highlight the significant potential of this centralized safety information system to support connected vehicles, facilitating proactive traffic management, crash mitigation, and data-driven roadway design and safety analysis.

</details>


### [74] [Prompt Driven Development with Claude Code: Building a Complete TUI Framework for the Ring Programming Language](https://arxiv.org/abs/2601.17584)
*Mahmoud Samir Fayed,Ahmed Samir Fayed*

Main category: cs.SE

TL;DR: 本研究通过实证分析展示了使用Claude Code Opus 4.5在约10小时内纯提示驱动开发7420行Ring编程语言终端用户界面框架的可行性，证明了现代LLM能够维持架构一致性并支持生产级工具开发。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在软件开发中应用日益广泛，但其通过自然语言交互生成和维护大型多模块系统的能力尚未得到充分表征。本研究旨在实证评估LLM在构建生产级软件系统方面的实际能力。

Method: 采用纯提示驱动工作流，使用Claude Code Opus 4.5在三天内约10小时的活跃工作中开发7420行Ring编程语言TUI框架。通过107个提示（21个功能请求、72个错误修复、9个文档信息共享、4个架构指导、1个文档生成）完成开发，人类角色仅限于需求指定、行为验证和纠正提示，不手动编写任何代码。

Result: 开发出包含完整窗口子系统、事件驱动架构、交互式小部件、分层菜单、网格和树组件、标签控件以及多窗口桌面环境的TUI框架。开发过程分为五个阶段，窗口管理器阶段需要最多交互，其次是复杂UI系统和控件扩展。错误修复主要涉及重绘问题、事件处理故障、运行时错误和布局不一致。

Conclusion: 通过定量提示分析和模型行为的定性评估，本研究提供了实证证据表明现代LLM能够维持架构一致性并支持为新兴编程语言构建生产级工具，突显了提示驱动开发作为软件工程实践中可行方法的潜力。

Abstract: Large language models are increasingly used in software development, yet their ability to generate and maintain large, multi module systems through natural language interaction remains insufficiently characterized. This study presents an empirical analysis of developing a 7420 line Terminal User Interface framework for the Ring programming language, completed in roughly ten hours of active work spread across three days using a purely prompt driven workflow with Claude Code, Opus 4.5. The system was produced through 107 prompts: 21 feature requests, 72 bug fix prompts, 9 prompts sharing information from Ring documentation, 4 prompts providing architectural guidance, and 1 prompt dedicated to generating documentation. Development progressed across five phases, with the Window Manager phase requiring the most interaction, followed by complex UI systems and controls expansion. Bug related prompts covered redraw issues, event handling faults, runtime errors, and layout inconsistencies, while feature requests focused primarily on new widgets, window manager capabilities, and advanced UI components. Most prompts were short, reflecting a highly iterative workflow in which the human role was limited to specifying requirements, validating behaviour, and issuing corrective prompts without writing any code manually. The resulting framework includes a complete windowing subsystem, event driven architecture, interactive widgets, hierarchical menus, grid and tree components, tab controls, and a multi window desktop environment. By combining quantitative prompt analysis with qualitative assessment of model behaviour, this study provides empirical evidence that modern LLMs can sustain architectural coherence and support the construction of production grade tooling for emerging programming languages, highlighting prompt driven development as a viable methodology within software engineering practice.

</details>


### [75] [Human-Aligned Enhancement of Programming Answers with LLMs Guided by User Feedback](https://arxiv.org/abs/2601.17604)
*Suborno Deb Bappon,Saikat Mondal,Chanchal K. Roy,Kevin Schneider*

Main category: cs.SE

TL;DR: 该研究探索了使用大语言模型（LLMs）来改进编程问答平台（如Stack Overflow）上的答案，通过分析用户评论反馈并生成改进版本，开发了AUTOCOMBAT工具，在基准测试和用户研究中表现出色。


<details>
  <summary>Details</summary>
Motivation: 在Stack Overflow等技术问答平台上，用户经常通过评论指出答案中的错误、低效或解释不足，但约三分之一的反馈从未得到处理，导致许多答案不完整或过时。研究探索LLMs是否能够通过解释和整合评论反馈来改进编程答案。

Method: 1. 创建ReSOlve基准数据集：包含790个Stack Overflow答案及相关评论线程，标注了改进相关和一般反馈；2. 评估四种最先进的LLMs识别可操作问题的能力；3. 开发AUTOCOMBAT工具：利用用户评论和问题上下文联合改进编程答案；4. 进行用户研究：58名从业者评估工具实用价值。

Result: 1. DeepSeek在识别可操作问题方面达到最佳精确度和召回率平衡；2. AUTOCOMBAT相比人类修订参考，产生接近人类质量的改进，同时保持原始意图，显著优于基线；3. 用户研究中84.5%的参与者表示会采用或推荐该工具。

Conclusion: AUTOCOMBAT展示了可扩展的反馈驱动答案精炼在提高技术知识平台可靠性和可信度方面的潜力，为LLMs在编程问答改进方面的应用提供了实证支持。

Abstract: Large Language Models (LLMs) are widely used to support software developers in tasks such as code generation, optimization, and documentation. However, their ability to improve existing programming answers in a human-like manner remains underexplored. On technical question-and-answer platforms such as Stack Overflow (SO), contributors often revise answers based on user comments that identify errors, inefficiencies, or missing explanations. Yet roughly one-third of this feedback is never addressed due to limited time, expertise, or visibility, leaving many answers incomplete or outdated. This study investigates whether LLMs can enhance programming answers by interpreting and incorporating comment-based feedback. We make four main contributions. First, we introduce ReSOlve, a benchmark consisting of 790 SO answers with associated comment threads, annotated for improvement-related and general feedback. Second, we evaluate four state-of-the-art LLMs on their ability to identify actionable concerns, finding that DeepSeek achieves the best balance between precision and recall. Third, we present AUTOCOMBAT, an LLM-powered tool that improves programming answers by jointly leveraging user comments and question context. Compared to human revised references, AUTOCOMBAT produces near-human quality improvements while preserving the original intent and significantly outperforming the baseline. Finally, a user study with 58 practitioners shows strong practical value, with 84.5 percent indicating they would adopt or recommend the tool. Overall, AUTOCOMBAT demonstrates the potential of scalable, feedback-driven answer refinement to improve the reliability and trustworthiness of technical knowledge platforms.

</details>


### [76] [Multi-Agent End-to-End Vulnerability Management for Mitigating Recurring Vulnerabilities](https://arxiv.org/abs/2601.17762)
*Zelong Zheng,Jiayuan Zhou,Xing Hu,Yi Gao,Shengyi Pan*

Main category: cs.SE

TL;DR: MAVM是一个多智能体框架，用于端到端的重复漏洞管理，通过整合漏洞知识库、检测、确认、修复和验证组件，显著提升了漏洞修复准确率。


<details>
  <summary>Details</summary>
Motivation: 现代软件系统规模扩大和复杂性增加使得漏洞管理日益重要，但现有自动化方法不足：传统静态分析难以捕捉跨函数/模块的上下文依赖；大语言模型缺乏足够的上下文信息检索能力；代码重用导致重复漏洞频发，而现有方法未能充分利用历史漏洞知识。

Method: 提出MAVM多智能体框架，包含五个组件：漏洞知识库（从公开披露漏洞构建）、检测、确认、修复和验证，集成到统一的多智能体流水线中。设计上下文检索工具，使智能体能够提取和推理仓库级信息，模拟真实世界安全工作流程。

Result: 在包含78个真实世界补丁移植案例（覆盖114个函数级迁移）的数据集上，MAVM成功检测并修复了51个真实漏洞，修复准确率比基线方法高出31.9%-45.2%。

Conclusion: MAVM通过整合历史漏洞知识和上下文检索能力，有效解决了现有漏洞管理方法的局限性，显著提升了重复漏洞的检测和修复效果，为端到端漏洞管理提供了有效框架。

Abstract: Software vulnerability management has become increasingly critical as modern systems scale in size and complexity. However, existing automated approaches remain insufficient. Traditional static analysis methods struggle to precisely capture contextual dependencies, especially when vulnerabilities span multiple functions or modules. Large language models (LLMs) often lack the ability to retrieve and exploit sufficient contextual information, resulting in incomplete reasoning and unreliable outcomes. Meanwhile, recurring vulnerabilities emerge repeatedly due to code reuse and shared logic, making historical vulnerability knowledge an indispensable foundation for effective vulnerability detection and repair. Nevertheless, prior approaches such as clone-based detection and patch porting, have not fully leveraged this knowledge. To address these challenges, we present MAVM, a multi-agent framework for end-to-end recurring vulnerability management. MAVM integrates five components, including a vulnerability knowledge base, detection, confirmation, repair, and validation, into a unified multi-agent pipeline. We construct a knowledge base from publicly disclosed vulnerabilities, thereby addressing the underuse of historical knowledge in prior work and mitigating the lack of domain-specific expertise in LLMs. Furthermore, we design context-retrieval tools that allow agents to extract and reason over repository-level information, overcoming the contextual limitations of previous methods. Based on agents, MAVM effectively simulates real-world security workflows. To evaluate the performance of MAVM, we construct a dataset containing 78 real-world patch-porting cases (covering 114 function-level migrations). On this dataset, MAVM successfully detects and repairs 51 real vulnerabilities, outperforming baselines by 31.9%-45.2% in repair accuracy, which demonstrates its effectiveness.

</details>


### [77] [iResolveX: Multi-Layered Indirect Call Resolution via Static Reasoning and Learning-Augmented Refinement](https://arxiv.org/abs/2601.17888)
*Monika Santra,Bokai Zhang,Mark Lim,Vishnu Asutosh Dasu,Dongrui Zeng,Gang Tan*

Main category: cs.SE

TL;DR: iResolveX是一个混合多层框架，结合保守静态分析和学习优化，用于解决二进制逆向工程中的间接调用解析问题，在保持高召回率的同时显著减少误报。


<details>
  <summary>Details</summary>
Motivation: 间接调用解析是逆向工程和控制流图恢复的关键挑战。静态分析方法虽然完备但会产生大量误报，而机器学习方法能提高精度但可能牺牲完备性和泛化能力。

Method: 采用三层混合框架：第一层使用保守的值集分析(BPA)确保高召回率；第二层加入基于学习的软签名评分器(iScoreGen)和选择性过程间回溯分析(iScoreRefine)；最终输出带置信度评分的间接控制流图(p-IndirectCFG)。

Result: 在SPEC CPU2006和真实二进制文件上，iScoreGen平均减少19.2%的预测目标，同时保持98.2%的BPA级召回率。结合iScoreRefine后，总减少率达到44.3%，召回率为97.8%（仅下降0.4%）。

Conclusion: iResolveX支持保守的召回保持配置和F1优化配置，优于现有最先进系统，为下游分析提供了精度-召回率的灵活权衡。

Abstract: Indirect call resolution remains a key challenge in reverse engineering and control-flow graph recovery, especially for stripped or optimized binaries. Static analysis is sound but often over-approximates, producing many false positives, whereas machine-learning approaches can improve precision but may sacrifice completeness and generalization. We present iResolveX, a hybrid multi-layered framework that combines conservative static analysis with learning-based refinement. The first layer applies a conservative value-set analysis (BPA) to ensure high recall. The second layer adds a learning-based soft-signature scorer (iScoreGen) and selective inter-procedural backward analysis with memory inspection (iScoreRefine) to reduce false positives. The final output, p-IndirectCFG, annotates indirect edges with confidence scores, enabling downstream analyses to choose appropriate precision--recall trade-offs. Across SPEC CPU2006 and real-world binaries, iScoreGen reduces predicted targets by 19.2% on average while maintaining BPA-level recall (98.2%). Combined with iScoreRefine, the total reduction reaches 44.3% over BPA with 97.8% recall (a 0.4% drop). iResolveX supports both conservative, recall-preserving and F1-optimized configurations and outperforms state-of-the-art systems.

</details>


### [78] [Prompt-Based REST API Test Amplification in Industry: An Experience Report](https://arxiv.org/abs/2601.17903)
*Tolgahan Bardakci,Andreas Faes,Mutlu Beyazit,Serge Demeyr*

Main category: cs.SE

TL;DR: LLM在工业环境中对REST API测试放大的有效性验证研究


<details>
  <summary>Details</summary>
Motivation: 虽然LLM越来越多地用于支持软件测试任务，但在工业环境中对REST API测试的有效性缺乏证据。研究旨在填补这一空白，验证LLM在工业环境中的实际应用效果。

Method: 在比利时最大物流公司的工业环境中复制早期LLM-based REST API测试放大研究。将LLM测试放大应用于生产微服务的六个代表性端点，该系统具有大规模、安全敏感、认证复杂、有状态行为和组织约束等特点。

Result: LLM-based测试放大在工业环境中仍然具有实际应用价值，能够提高测试覆盖率，并揭示各种观察结果和异常情况。

Conclusion: LLM-based测试放大在工业环境中是实用的，能够有效支持复杂的REST API测试，即使在具有深度复杂性、安全敏感性和组织约束的生产系统中也能发挥作用。

Abstract: Large Language Models (LLMs) are increasingly used to support software testing tasks, yet there is little evidence of their effectiveness for REST API testing in industrial settings. To address this gap, we replicate our earlier work on LLM-based REST API test amplification within an industrial context at one of the largest logistics companies in Belgium. We apply LLM-based test amplification to six representative endpoints of a production microservice embedded in a large-scale, security-sensitive system, where there is in-depth complexity in authentication, stateful behavior, and organizational constraints. Our experience shows that LLM-based test amplification remains practically useful in industry by increasing coverage and revealing various observations and anomalies.

</details>


### [79] [TAM-Eval: Evaluating LLMs for Automated Unit Test Maintenance](https://arxiv.org/abs/2601.18241)
*Elena Bruches,Vadim Alperovich,Dari Baturova,Roman Derunets,Daniil Grebenkin,Georgy Mkrtchyan,Oleg Sedukhin,Mikhail Klementev,Ivan Bondarenko,Nikolay Bushkov,Stanislav Moiseev*

Main category: cs.SE

TL;DR: TAM-Eval是一个用于评估LLM在测试套件维护中性能的框架和基准，涵盖创建、修复和更新三个核心场景，支持Python、Java和Go项目，包含1539个场景。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在软件工程中的应用主要集中在孤立的测试生成或预言预测，忽视了测试套件维护这一更广泛的挑战。现有研究局限于函数级任务，无法反映真实世界的测试维护工作流程。

Method: 提出了TAM-Eval框架和基准，在测试文件级别操作，同时保持对完整仓库上下文的访问。包含1539个自动提取和验证的场景，支持系统无关的评估，使用基于测试套件通过率、代码覆盖率和变异测试的无参考协议。

Result: 实证结果表明，最先进的LLM在现实测试维护过程中的能力有限，对测试有效性的提升幅度很小。

Conclusion: LLM在测试套件维护方面的能力仍然有限，TAM-Eval作为开源框架发布，旨在支持未来自动化软件测试的研究。

Abstract: While Large Language Models (LLMs) have shown promise in software engineering, their application to unit testing remains largely confined to isolated test generation or oracle prediction, neglecting the broader challenge of test suite maintenance. We introduce TAM-Eval (Test Automated Maintenance Evaluation), a framework and benchmark designed to evaluate model performance across three core test maintenance scenarios: creation, repair, and updating of test suites. Unlike prior work limited to function-level tasks, TAM-Eval operates at the test file level, while maintaining access to full repository context during isolated evaluation, better reflecting real-world maintenance workflows. Our benchmark comprises 1,539 automatically extracted and validated scenarios from Python, Java, and Go projects. TAM-Eval supports system-agnostic evaluation of both raw LLMs and agentic workflows, using a reference-free protocol based on test suite pass rate, code coverage, and mutation testing. Empirical results indicate that state-of-the-art LLMs have limited capabilities in realistic test maintenance processes and yield only marginal improvements in test effectiveness. We release TAM-Eval as an open-source framework to support future research in automated software testing. Our data and code are publicly available at https://github.com/trndcenter/TAM-Eval.

</details>


### [80] [Forecasting the Maintained Score from the OpenSSF Scorecard for GitHub Repositories linked to PyPI libraries](https://arxiv.org/abs/2601.18344)
*Alexandros Tsakpinis,Efe Berk Ergülec,Emil Schwenger,Alexander Pretschner*

Main category: cs.SE

TL;DR: 研究通过时间序列预测方法预测开源软件仓库的未来维护活动，发现可以使用VARMA、随机森林和LSTM等模型有效预测OpenSSF Scorecard的Maintained指标，准确率可达95%以上。


<details>
  <summary>Details</summary>
Motivation: OpenSSF Scorecard的Maintained指标仅反映过去90天的维护活动，缺乏对未来维护的预测能力，限制了其在主动风险评估中的实用性。本研究旨在探索能否预测未来的维护活动。

Method: 分析了3,220个GitHub仓库（对应PyPI库中PageRank最高的1%），重建了三年历史Maintained分数。将任务构建为多变量时间序列预测，考虑四种目标表示：原始分数、分桶维护等级、数值趋势斜率和分类趋势类型。比较了VARMA（统计模型）、随机森林（机器学习模型）和LSTM（深度学习模型），使用3-12个月训练窗口和1-6个月预测范围。

Result: 未来维护活动可以以有意义的准确度进行预测，特别是对于聚合表示（如分桶分数和趋势类型），准确率分别超过0.95和0.80。简单的统计和机器学习模型与深度学习方法表现相当，表明不需要复杂架构。

Conclusion: 预测建模可以有效补充现有的Scorecard指标，实现对开源维护风险的更主动评估。研究结果表明，通过时间序列预测方法可以提前识别可能被放弃的依赖项，提高开源软件供应链的安全性。

Abstract: The OpenSSF Scorecard is widely used to assess the security posture of open-source software repositories, with the Maintained metric indicating recent development activity and helping identify potentially abandoned dependencies. However, this metric is inherently retrospective, reflecting only the past 90 days of activity and providing no insight into future maintenance, which limits its usefulness for proactive risk assessment. In this paper, we study to what extent future maintenance activity, as captured by the OpenSSF Maintained score, can be forecasted. We analyze 3,220 GitHub repositories associated with the top 1% most central PyPI libraries by PageRank and reconstruct historical Maintained scores over a three-year period. We formulate the task as multivariate time series forecasting and consider four target representations: raw scores, bucketed maintenance levels, numerical trend slopes, and categorical trend types. We compare a statistical model (VARMA), a machine learning model (Random Forest), and a deep learning model (LSTM) across training windows of 3-12 months and forecasting horizons of 1-6 months. Our results show that future maintenance activity can be predicted with meaningful accuracy, particularly for aggregated representations such as bucketed scores and trend types, achieving accuracies above 0.95 and 0.80, respectively. Simpler statistical and machine learning models perform on par with deep learning approaches, indicating that complex architectures are not required. These findings suggest that predictive modeling can effectively complement existing Scorecard metrics, enabling more proactive assessment of open-source maintenance risks.

</details>


### [81] [An Audit of Machine Learning Experiments on Software Defect Prediction](https://arxiv.org/abs/2601.18477)
*Giuseppe Destefanis,Leila Yousefi,Martin Shepperd,Allan Tucker,Stephen Swift,Steve Counsell,Mahir Arzoky*

Main category: cs.SE

TL;DR: 对2019-2023年软件缺陷预测研究进行审计，发现实验设计和报告实践差异大，近半数研究缺乏足够细节支持复现，存在改进空间


<details>
  <summary>Details</summary>
Motivation: 评估近期软件缺陷预测研究的实验设计、分析和报告实践，了解当前研究现状并评估已发表结果的可复现性

Method: 审计2019-2023年SCOPUS索引的SDP研究，关注结果度量、样本外验证策略、统计推断使用等9个研究问题，使用González Barahona和Robles提出的工具评估可复现性

Result: 从1585个实验中随机抽样101篇论文，发现研究实践差异显著：数据集数量1-365个，学习器1-34种，性能度量1-9种；约45%研究应用正式统计推断；共发现427个问题，中位数每篇4个问题；可复现性从接近完全到严重受限；发现2个"折磨短语"案例和可能的论文工厂活动

Conclusion: 实验设计和报告实践差异广泛，近半数研究缺乏足够细节支持复现，审计表明存在显著改进空间

Abstract: Background: Machine learning algorithms are widely used to predict defect prone software components. In this literature, computational experiments are the main means of evaluation, and the credibility of results depends on experimental design and reporting. Objective: This paper audits recent software defect prediction (SDP) studies by assessing their experimental design, analysis, and reporting practices against accepted norms from statistics, machine learning, and empirical software engineering. The aim is to characterise current practice and assess the reproducibility of published results. Method: We audited SDP studies indexed in SCOPUS between 2019 and 2023, focusing on design and analysis choices such as outcome measures, out of sample validation strategies, and the use of statistical inference. Nine study issues were evaluated. Reproducibility was assessed using the instrument proposed by González Barahona and Robles. Results: The search identified approximately 1,585 SDP experiments published during the period. From these, we randomly sampled 101 papers, including 61 journal and 40 conference publications, with almost 50 percent behind paywalls. We observed substantial variation in research practice. The number of datasets ranged from 1 to 365, learners or learner variants from 1 to 34, and performance measures from 1 to 9. About 45 percent of studies applied formal statistical inference. Across the sample, we identified 427 issues, with a median of four per paper, and only one paper without issues. Reproducibility ranged from near complete to severely limited. We also identified two cases of tortured phrases and possible paper mill activity. Conclusions: Experimental design and reporting practices vary widely, and almost half of the studies provide insufficient detail to support reproduction. The audit indicates substantial scope for improvement.

</details>


### [82] [On the Abolition of the "ICSE Paper" and the Adoption of the "Registered Proposal" and the "Results Report"](https://arxiv.org/abs/2601.18566)
*Fabio Massacci,Winnie Mbaka*

Main category: cs.SE

TL;DR: 提出废除传统ICSE论文模式，建立两阶段系统：先提交"注册提案"进行方法论评审，次年提交"结果报告"展示实证研究结果，以解决领域创新危机和可重复性问题。


<details>
  <summary>Details</summary>
Motivation: 为了解决软件工程领域的"创新恶性循环"和"可重复性危机"，这些问题是该领域长期存在的挑战，阻碍了科学进步和可靠知识的积累。

Method: 提出颠覆性的两阶段论文系统：1) 作者提交"注册提案"，详细描述新想法和实验方法，接受同行评审；2) 次年任何人都可以提交基于已批准提案的"结果报告"，展示实证研究成果。两者都应成为主流会议的一等公民。

Result: 该提议基于软件工程未来预调查的社区反馈，得到了支持。这种变革性想法旨在从根本上改变论文发表模式，促进更严谨、可重复的研究实践。

Conclusion: 废除传统ICSE论文模式，采用两阶段注册报告系统，可以有效解决软件工程领域的创新危机和可重复性问题，推动该领域向更科学、更严谨的方向发展。

Abstract: To address the 'novelty-vicious cycle' and the 'replicability crisis' of the field (both discussed in the survey) we propose abolishing the "ICSE paper" as we know it and replacing it with a two-tier system that also evolves the existing notion of 'Registered Report'. Authors proposing a new idea, experiment, or analysis would submit a "Registered Proposal" of their idea and the proposed experimental methodology to undergo peer review. The following year, anyone can submit (shorter) "Results Reports" on the realization of the empirical work based on the registered proposals of the previous ICSE (or FSE or ISSTA or ASE etc.). Both works should be first class citizens of the mainstream events. We argue that such a disruptive (heretical?) idea is supported and based on the responses of the community of the Future of Software Engineering pre-survey

</details>


### [83] [How are MLOps Frameworks Used in Open Source Projects? An Empirical Characterization](https://arxiv.org/abs/2601.18591)
*Fiorella Zampetti,Federico Stocchetti,Federica Razzano,Damian Andrew Tamburri,Massimiliano Di Penta*

Main category: cs.SE

TL;DR: 研究分析8个热门开源MLOps框架的实际使用情况和期望功能增强，发现开发者主要使用API实现自定义功能而非开箱即用，且很少集成到GitHub工作流中


<details>
  <summary>Details</summary>
Motivation: MLOps框架虽然提供广泛功能，但开发者可能只使用其中一部分，同时错过一些高度期望的功能。本研究旨在了解MLOps框架的实际使用情况和用户期望的功能改进

Method: 1) 分析GitHub上依赖项目如何使用这些框架的API和命令；2) 从框架的问题跟踪器中定性分析功能请求和改进建议；3) 将期望的改进与先前识别的使用功能相关联

Result: MLOps框架很少开箱即用，很少集成到GitHub工作流中；开发者主要使用API在项目中实现自定义功能；使用的功能涉及核心ML阶段和整个基础设施治理；有时会利用多个具有互补功能的框架；功能请求主要针对核心功能增强、更好的API暴露和CI/CD集成

Conclusion: MLOps框架的实际使用模式与框架设计初衷存在差异，开发者更倾向于使用API构建自定义解决方案而非直接使用框架的完整功能集，这为MLOps框架的未来发展提供了重要方向

Abstract: Machine Learning (ML) Operations (MLOps) frameworks have been conceived to support developers and AI engineers in managing the lifecycle of their ML models. While such frameworks provide a wide range of features, developers may leverage only a subset of them, while missing some highly desired features. This paper investigates the practical use and desired feature enhancements of eight popular open-source MLOps frameworks. Specifically, we analyze their usage by dependent projects on GitHub, examining how they invoke the frameworks' APIs and commands. Then, we qualitatively analyze feature requests and enhancements mined from the frameworks' issue trackers, relating these desired improvements to the previously identified usage features. Results indicate that MLOps frameworks are rarely used out-of-the-box and are infrequently integrated into GitHub Workflows, but rather, developers use their APIs to implement custom functionality in their projects. Used features concern core ML phases and whole infrastructure governance, sometimes leveraging multiple frameworks with complementary features. The mapping with feature requests highlights that users mainly ask for enhancements to core features of the frameworks, but also better API exposure and CI/CD integration.

</details>
