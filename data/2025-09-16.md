<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 27]
- [cs.CR](#cs.CR) [Total: 58]
- [cs.AI](#cs.AI) [Total: 50]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Quality Assessment of Tabular Data using Large Language Models and Code Generation](https://arxiv.org/abs/2509.10572)
*Ashlesha Akella,Akshar Kaul,Krishnasuri Narayanam,Sameep Mehta*

Main category: cs.SE

TL;DR: 一种结合统计检测和LLM驱动的三阶段框架，通过生成质量规则和代码验证器来提高表格数据质量检验的效率和可靠性


<details>
  <summary>Details</summary>
Motivation: 解决基于规则的数据质量验证方法存在的效率低下、需要人工干预和计算成本高等问题

Method: 三阶段框架：首先通过传统聚类过滤数据样本，然后迭代地使用LLM生成语义有效的质量规则，最后通过代码生成LLM合成可执行的验证器，并使用RAG技术提供外部知识源和领域特定的少样本示例

Result: 在标准数据集上进行了涉广法的评估，证明了该方法的有效性

Conclusion: 该框架能够生成可靠的数据质量规则和验证器，通过结合统计检测和LLM驱动的方法，有效解决了传统规则基验证方法的限制

Abstract: Reliable data quality is crucial for downstream analysis of tabular datasets,
yet rule-based validation often struggles with inefficiency, human
intervention, and high computational costs. We present a three-stage framework
that combines statistical inliner detection with LLM-driven rule and code
generation. After filtering data samples through traditional clustering, we
iteratively prompt LLMs to produce semantically valid quality rules and
synthesize their executable validators through code-generating LLMs. To
generate reliable quality rules, we aid LLMs with retrieval-augmented
generation (RAG) by leveraging external knowledge sources and domain-specific
few-shot examples. Robust guardrails ensure the accuracy and consistency of
both rules and code snippets. Extensive evaluations on benchmark datasets
confirm the effectiveness of our approach.

</details>


### [2] [Reasonable Experiments in Model-Based Systems Engineering](https://arxiv.org/abs/2509.10649)
*Johan Cederbladh,Loek Cleophas,Eduard Kamburjan,Lucas Lima,Rakshit Mittal,Hans Vangheluwe*

Main category: cs.SE

TL;DR: 基于案例推理和领域知识的实验管理框架，通过智能重用实验数据来避免重复实验，加速系统设计过程


<details>
  <summary>Details</summary>
Motivation: 在数字工程和早期验证验证背景下，实验管理对估算系统参数和探索设计决策至关重要，需要智能重用实验数据以节省时间和资源

Method: 提出一种管理数字/物理资产实验的框架，采用案例推理技术结合领域知识，判断是否可以重用既往实验来回答新问题

Result: 提供了实验管理器的通用架构，并通过工业车辆能源系统设计案例进行了验证

Conclusion: 该框架能够有效重用实验数据，避免重复设置和执行实验，从而加速整体设计过程

Abstract: With the current trend in Model-Based Systems Engineering towards Digital
Engineering and early Validation & Verification, experiments are increasingly
used to estimate system parameters and explore design decisions. Managing such
experimental configuration metadata and results is of utmost importance in
accelerating overall design effort. In particular, we observe it is important
to 'intelligent-ly' reuse experiment-related data to save time and effort by
not performing potentially superfluous, time-consuming, and resource-intensive
experiments. In this work, we present a framework for managing experiments on
digital and/or physical assets with a focus on case-based reasoning with domain
knowledge to reuse experimental data efficiently by deciding whether an
already-performed experiment (or associated answer) can be reused to answer a
new (potentially different) question from the engineer/user without having to
set up and perform a new experiment. We provide the general architecture for
such an experiment manager and validate our approach using an industrial
vehicular energy system-design case study.

</details>


### [3] [Arguzz: Testing zkVMs for Soundness and Completeness Bugs](https://arxiv.org/abs/2509.10819)
*Christoph Hochrainer,Valentin Wüstholz,Maria Christakis*

Main category: cs.SE

TL;DR: Arguzz是首个自动化测试zkVM的工具，通过结合蜕变测试和故障注入来检测零知识虚拟机的正确性和完整性漏洞，在6个真实zkVM中发现了11个漏洞。


<details>
  <summary>Details</summary>
Motivation: zkVM在去中心化应用和区块链rollup中广泛部署，但其约束系统和执行逻辑中的漏洞可能导致严重的安全问题（接受无效执行或拒绝有效执行），需要系统化测试方法。

Method: Arguzz采用新颖的蜕变测试变体结合故障注入：生成语义等效的程序对，合并为具有已知输出的Rust程序，在zkVM中运行并通过故障注入模拟恶意证明者来发现约束过弱的问题。

Result: 在测试的6个真实zkVM（RISC Zero、Nexus、Jolt、SP1、OpenVM和Pico）中发现了11个漏洞，其中一个RISC Zero漏洞获得了5万美元的漏洞赏金。

Conclusion: 尽管经过审计，zkVM仍存在严重漏洞，Arguzz证明了系统化测试zkVM的迫切必要性，能够有效发现传统方法可能遗漏的安全问题。

Abstract: Zero-knowledge virtual machines (zkVMs) are increasingly deployed in
decentralized applications and blockchain rollups since they enable verifiable
off-chain computation. These VMs execute general-purpose programs, frequently
written in Rust, and produce succinct cryptographic proofs. However, zkVMs are
complex, and bugs in their constraint systems or execution logic can cause
critical soundness (accepting invalid executions) or completeness (rejecting
valid ones) issues.
  We present Arguzz, the first automated tool for testing zkVMs for soundness
and completeness bugs. To detect such bugs, Arguzz combines a novel variant of
metamorphic testing with fault injection. In particular, it generates
semantically equivalent program pairs, merges them into a single Rust program
with a known output, and runs it inside a zkVM. By injecting faults into the
VM, Arguzz mimics malicious or buggy provers to uncover overly weak
constraints.
  We used Arguzz to test six real-world zkVMs (RISC Zero, Nexus, Jolt, SP1,
OpenVM, and Pico) and found eleven bugs in three of them. One RISC Zero bug
resulted in a $50,000 bounty, despite prior audits, demonstrating the critical
need for systematic testing of zkVMs.

</details>


### [4] [TPSQLi: Test Prioritization for SQL Injection Vulnerability Detection in Web Applications](https://arxiv.org/abs/2509.10920)
*Guan-Yan Yang,Farn Wang,You-Zong Gu,Ya-Wen Teng,Kuo-Hui Yeh,Ping-Hsueh Ho,Wei-Ling Wen*

Main category: cs.SE

TL;DR: 这篇论文提出了一种新的SQL注入漏洞测试优先级方法，通过动态调整防御强度来提高测试效率和效果。


<details>
  <summary>Details</summary>
Motivation: 网络攻击的快速增长和SQL注入攻击在漏洞排名中占据前三，导致软件测试复杂度和工作量大幅增加，需要高级工具支持灵活开发周期。

Method: 利用历史测试结果动态调整防御强度向量，为后续测试设计优化流程，并根据特定软件需求定制防御机制。

Result: 该方法通过灵活框架实现动态调整和考虑漏洞暴露时间因素，提高了漏洞检测和缩减的效果和效率。

Conclusion: 该研究为SQL注入漏洞测试提供了一种有效的优先级方法，适应了当前网络安全威胁下的灵活开发需求。

Abstract: The rapid proliferation of network applications has led to a significant
increase in network attacks. According to the OWASP Top 10 Projects report
released in 2021, injection attacks rank among the top three vulnerabilities in
software projects. This growing threat landscape has increased the complexity
and workload of software testing, necessitating advanced tools to support agile
development cycles. This paper introduces a novel test prioritization method
for SQL injection vulnerabilities to enhance testing efficiency. By leveraging
previous test outcomes, our method adjusts defense strength vectors for
subsequent tests, optimizing the testing workflow and tailoring defense
mechanisms to specific software needs. This approach aims to improve the
effectiveness and efficiency of vulnerability detection and mitigation through
a flexible framework that incorporates dynamic adjustments and considers the
temporal aspects of vulnerability exposure.

</details>


### [5] [When the Code Autopilot Breaks: Why LLMs Falter in Embedded Machine Learning](https://arxiv.org/abs/2509.10946)
*Roberto Morabito,Guanghan Wu*

Main category: cs.SE

TL;DR: 这篇论文通过实验研究揭示了大语言模型在嵌入式机器学习流程中的多种失败模式，包括提示格式引发的误解、编译成功但运行时崩溃的代码等问题，并提出了失败分类形态学。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在嵌入式机器学工作流中自动化软件生成的应用增多，它们的输出经常失败或行为不可预测，需要系统性地研究其失败模式和根本原因。

Method: 研究基于一个自动驾驶框架，调度数据预处理、模型转换和设备上推理代码生成。通过实验分析提示格式、模型行为和结构假设对成功率和失败特征的影响。

Result: 分析揭示了多种容易出错的行为，包括格式引发的误解和编译成功但会破坏下游的运行时代码。研究得出了失败类别分类形态学，并在多个LLM中分析了错误，突出了共同根本原因和系统脆弱性。

Conclusion: 虽然基于特定设备，但研究揭示了LLM基于代码生成的更广泛挑战。文章最后讨论了改善LLM驱动嵌入式ML系统可靠性和可追溯性的方向。

Abstract: Large Language Models (LLMs) are increasingly used to automate software
generation in embedded machine learning workflows, yet their outputs often fail
silently or behave unpredictably. This article presents an empirical
investigation of failure modes in LLM-powered ML pipelines, based on an
autopilot framework that orchestrates data preprocessing, model conversion, and
on-device inference code generation. We show how prompt format, model behavior,
and structural assumptions influence both success rates and failure
characteristics, often in ways that standard validation pipelines fail to
detect. Our analysis reveals a diverse set of error-prone behaviors, including
format-induced misinterpretations and runtime-disruptive code that compiles but
breaks downstream. We derive a taxonomy of failure categories and analyze
errors across multiple LLMs, highlighting common root causes and systemic
fragilities. Though grounded in specific devices, our study reveals broader
challenges in LLM-based code generation. We conclude by discussing directions
for improving reliability and traceability in LLM-powered embedded ML systems.

</details>


### [6] [Hardness, Structural Knowledge, and Opportunity: An Analytical Framework for Modular Performance Modeling](https://arxiv.org/abs/2509.11000)
*Omid Gheibi,Christian Kästner,Pooyan Jamshidi*

Main category: cs.SE

TL;DR: 本文研究结构知识对性能影响模型建模的作用，通过定义模型难度和分析矩阵，揭示了结构知识水平和系统特征对模型攽次机会的影响。


<details>
  <summary>Details</summary>
Motivation: 虽然灰盒方法利用结构知识改善性能模型，但结构知识、系统特征与模型攽次之间的关系并不明确，需要正式研究来填补这一空白。

Method: 通过控制实验使用合成系统模型，建立分析矩阵来量化模型难度和攽次机会，研究结构方面变化和结构知识水平的影响。

Result: 发现模型难度主要由模块数量和每个模块的配置选项数量驱动；更高的结构知识水平和更高的模型难度都显著提高攽次机会；不同性能指标下因素影响不同：排名准确性中结构知识更重要，预测准确性中模型难度作用更大。

Conclusion: 研究结果为系统设计者提供了可操作的见解，帮助他们根据系统特征和任务目标战略性地分配时间并选择适当的建模方法。

Abstract: Performance-influence models are beneficial for understanding how
configurations affect system performance, but their creation is challenging due
to the exponential growth of configuration spaces. While gray-box approaches
leverage selective "structural knowledge" (like the module execution graph of
the system) to improve modeling, the relationship between this knowledge, a
system's characteristics (we call them "structural aspects"), and potential
model improvements is not well understood. This paper addresses this gap by
formally investigating how variations in structural aspects (e.g., the number
of modules and options per module) and the level of structural knowledge impact
the creation of "opportunities" for improved "modular performance modeling". We
introduce and quantify the concept of modeling "hardness", defined as the
inherent difficulty of performance modeling. Through controlled experiments
with synthetic system models, we establish an "analytical matrix" to measure
these concepts. Our findings show that modeling hardness is primarily driven by
the number of modules and configuration options per module. More importantly,
we demonstrate that both higher levels of structural knowledge and increased
modeling hardness significantly enhance the opportunity for improvement. The
impact of these factors varies by performance metric; for ranking accuracy
(e.g., in debugging task), structural knowledge is more dominant, while for
prediction accuracy (e.g., in resource management task), hardness plays a
stronger role. These results provide actionable insights for system designers,
guiding them to strategically allocate time and select appropriate modeling
approaches based on a system's characteristics and a given task's objectives.

</details>


### [7] [ViScratch: Using Large Language Models and Gameplay Videos for Automated Feedback in Scratch](https://arxiv.org/abs/2509.11065)
*Yuan Si,Daming Li,Hanyuan Shi,Jialu Zhang*

Main category: cs.SE

TL;DR: ViScratch是首个利用游戏视频和代码进行Scratch程序调试的多模态反馈系统，通过视觉-语言模型识别关键问题并提出AST级别的修复方案，显著优于现有LLM工具。


<details>
  <summary>Details</summary>
Motivation: 现有的Scratch调试工具主要依赖预定义规则和用户手动输入，忽略了平台的可视化特性，无法有效处理语义错误。

Method: 采用两阶段流水线：首先使用视觉-语言模型将视觉症状与代码结构对齐以识别关键问题，然后提出最小化的AST级别修复并在Scratch虚拟机中验证。

Result: ViScratch在真实Scratch项目上显著优于最先进的基于LLM的工具和人工测试者，游戏视频成为关键的调试信号。

Conclusion: 视频可以作为可视化编程环境中的一等规范，为基于LLM的调试开辟了超越纯符号代码的新方向。

Abstract: Block-based programming environments such as Scratch are increasingly popular
in programming education, in particular for young learners. While the use of
blocks helps prevent syntax errors, semantic bugs remain common and difficult
to debug. Existing tools for Scratch debugging rely heavily on predefined rules
or user manual inputs, and crucially, they ignore the platform's inherently
visual nature.
  We introduce ViScratch, the first multimodal feedback generation system for
Scratch that leverages both the project's block code and its generated gameplay
video to diagnose and repair bugs. ViScratch uses a two-stage pipeline: a
vision-language model first aligns visual symptoms with code structure to
identify a single critical issue, then proposes minimal, abstract syntax tree
level repairs that are verified via execution in the Scratch virtual machine.
  We evaluate ViScratch on a set of real-world Scratch projects against
state-of-the-art LLM-based tools and human testers. Results show that gameplay
video is a crucial debugging signal: ViScratch substantially outperforms prior
tools in both bug identification and repair quality, even without access to
project descriptions or goals. This work demonstrates that video can serve as a
first-class specification in visual programming environments, opening new
directions for LLM-based debugging beyond symbolic code alone.

</details>


### [8] [Rethinking Technology Stack Selection with AI Coding Proficiency](https://arxiv.org/abs/2509.11132)
*Xiaoyu Zhang,Weipeng Jiang,Juan Zhai,Shiqing Ma,Qingshuang Bao,Chenhao Lin,Chao Shen,Tianlin Li,Yang Liu*

Main category: cs.SE

TL;DR: 本文提出了AI编码熟练度的概念，评估LLMs利用特定技术生成高质量代码的能力，并通过实证研究发现不同库之间的AI熟练度差异高达84%，这会影响技术选择并威胁技术多样性。


<details>
  <summary>Details</summary>
Motivation: 传统技术栈选择方法只关注技术本身属性，忽视了LLMs能否有效利用所选技术。现有LLMs在使用流行库时经常生成低质量代码，导致高调试成本和技术债务。

Method: 提出了AI编码熟练度的概念，并对170个第三方库和61个任务场景进行了全面的实证研究，评估了6个广泛使用的LLMs。

Result: 研究发现功能相似的库在LLM生成代码质量得分上差异高达84%，不同模型使用相同库时也存在质量差距。这些差距转化为实际工程成本，可能导致开发者偏向选择AI熟练度高的库。

Conclusion: 呼吁社区将AI熟练度评估整合到技术选择框架中，并制定缓解策略，以保持AI驱动开发中的竞争平衡和技术多样性。

Abstract: Large language models (LLMs) are now an integral part of software development
workflows and are reshaping the whole process. Traditional technology stack
selection has not caught up. Most of the existing selection methods focus
solely on the inherent attributes of the technology, overlooking whether the
LLM can effectively leverage the chosen technology. For example, when
generating code snippets using popular libraries like Selenium (one of the most
widely used test automation tools with over 33k GitHub stars), existing LLMs
frequently generate low-quality code snippets (e.g., using deprecated APIs and
methods, or containing syntax errors). As such, teams using LLM assistants risk
choosing technologies that cannot be used effectively by LLMs, yielding high
debugging effort and mounting technical debt. We foresee a practical question
in the LLM era, is a technology ready for AI-assisted development? In this
paper, we first propose the concept, AI coding proficiency, the degree to which
LLMs can utilize a given technology to generate high-quality code snippets. We
conduct the first comprehensive empirical study examining AI proficiency across
170 third-party libraries and 61 task scenarios, evaluating six widely used
LLMs. Our findings reveal that libraries with similar functionalities can
exhibit up to 84% differences in the quality score of LLM-generated code, while
different models also exhibit quality gaps among their generation results using
the same library. These gaps translate into real engineering costs and can
steer developer choices toward a narrow set of libraries with high AI coding
proficiency, threatening technological diversity in the ecosystem. We call on
the community to integrate AI proficiency assessments into technology selection
frameworks and develop mitigation strategies, preserving competitive balance in
AI-driven development.

</details>


### [9] [UserTrace: User-Level Requirements Generation and Traceability Recovery from Software Project Repositories](https://arxiv.org/abs/2509.11238)
*Dongming Jin,Zhi Jin,Yiran Zhang,Zheng Fang,Linyu Li,Yuanpeng He,Xiaohong Chen,Weisong Sun*

Main category: cs.SE

TL;DR: UserTrace是一个多智能体系统，自动从软件仓库生成用户级需求(URs)并恢复实时追踪链接(从URs到实现级需求再到代码)，解决了现有代码摘要和需求追踪技术的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有自动化代码摘要(ACS)方法主要生成面向开发者的实现级需求，而需求追踪(RT)技术常忽略项目演进影响，导致用户级需求和实时追踪链接研究不足，但这些对支持用户理解和验证AI生成软件是否符合用户意图至关重要。

Method: UserTrace协调四个专业智能体(代码审查员、搜索器、编写器、验证器)，通过三阶段流程：结构化仓库依赖、推导代码单元的实现级需求、结合领域特定上下文合成用户级需求。

Result: 比较评估显示，UserTrace生成的用户级需求在完整性、正确性和有用性方面优于现有基线，在追踪链接恢复精度上优于五种最先进的需求追踪方法。用户研究进一步证明其能帮助最终用户验证AI生成仓库是否符合其意图。

Conclusion: UserTrace有效填补了用户级需求和实时追踪链接自动生成的空白，为软件维护和AI生成软件的验证提供了重要支持。

Abstract: Software maintainability critically depends on high-quality requirements
descriptions and explicit traceability between requirements and code. Although
automated code summarization (ACS) and requirements traceability (RT)
techniques have been widely studied, existing ACS methods mainly generate
implementation-level (i.e., developer-oriented) requirements (IRs) for
fine-grained units (e.g., methods), while RT techniques often overlook the
impact of project evolution. As a result, user-level (i.e., end user-oriented)
requirements (URs) and live trace links remain underexplored, despite their
importance for supporting user understanding and for validating whether
AI-generated software aligns with user intent. To address this gap, we propose
UserTrace, a multi-agent system that automatically generates URs and recovers
live trace links (from URs to IRs to code) from software repositories.
UserTrace coordinates four specialized agents (i.e., Code Reviewer, Searcher,
Writer, and Verifier) through a three-phase process: structuring repository
dependencies, deriving IRs for code units, and synthesizing URs with
domain-specific context. Our comparative evaluation shows that UserTrace
produces URs with higher completeness, correctness, and helpfulness than an
established baseline, and achieves superior precision in trace link recovery
compared to five state-of-the-art RT approaches. A user study further
demonstrates that UserTrace helps end users validate whether the AI-generated
repositories align with their intent.

</details>


### [10] [Beyond Autoregression: An Empirical Study of Diffusion Large Language Models for Code Generation](https://arxiv.org/abs/2509.11252)
*Chengze li,Yitong Zhang,Jia Li,Liyi Cai,Ge Li*

Main category: cs.SE

TL;DR: 通过对比9个代表性渗透模型在4个标准数据集上的实验，证实渗透LLM在代码生成任务上与自回归模型相当，具有更强的长度外推能力，并提供了关键因素分析和改进建议。


<details>
  <summary>Details</summary>
Motivation: 自回归LLM在代码生成中存在效率低和生成顺序固化的问题，渗透LLM通过多标记预测和灵活生成顺序有望解决这些限制，但缺乏系统性研究。

Method: 采用9个代表性渗透LLM模型，在4个广泛使用的代码生成标准数据集上进行实验对比，分析渗透LLM的效果、效率和关键影响因素。

Result: 渗透LLM在相同规模下与自回归LLM竞争力相当，在长代码理解和长度外推方面表现更优，并识别出了影响效果和效率的关键因素。

Conclusion: 渗透LLM是代码生成领域的有力竞争者，具有明显优势和改进潜力，研究为该方向提供了实证基础和实践指南。

Abstract: LLMs have become the mainstream approaches to code generation. Existing LLMs
mainly employ autoregressive generation, i.e. generating code token-by-token
from left to right. However, the underlying autoregressive generation has two
limitations in code generation. First, autoregressive LLMs only generate a
token at each step, showing low efficiency in practice. Second, programming is
a non-sequential process involving back-and-forth editing, while autoregressive
LLMs only employ the left-to-right generation order. These two intrinsic
limitations hinder the further development of LLMs in code generation.
Recently, diffusion LLMs have emerged as a promising alternative. Diffusion
LLMs address the above limitations with two advances, including multi-token
prediction (i.e. generating multiple tokens at each step) and flexible
generation order (i.e. flexibly determining which positions to generate
tokens). However, there is no systematic study exploring diffusion LLMs in code
generation. To bridge the knowledge gap, we present the first empirical study
of diffusion LLMs for code generation. Our study involves 9 representative
diffusion LLMs and conduct experiments on 4 widely used benchmarks. Based on
the results, we summarize the following findings. (1) Existing diffusion LLMs
are competitive with autoregressive LLMs with similar sizes. (2) Diffusion LLMs
have a stronger length extrapolation ability than autoregressive LLMs and
perform better in long code understanding. (3) We explore factors impacting the
effectiveness and efficiency of diffusion LLMs, and provide practical guidance.
(4) We discuss several promising further directions to improve diffusion LLMs
on code generation. We open-source all source code, data, and results to
facilitate the following research. The code is publicly available at
https://github.com/zhangyitonggg/dllm4code.

</details>


### [11] [A Web-Based Environment for the Specification and Generation of Smart Legal Contracts](https://arxiv.org/abs/2509.11258)
*Regan Meloche,Durga Sivakumar,Amal A. Anda,Sofana Alfuhaid,Daniel Amyot,Luigi Logrippo,John Mylopoulos*

Main category: cs.SE

TL;DR: 这篇论文提出了一个基于Web的环境，通过支持用户辅助精化Symboleo规范和自动生成监控智能合约，来缩小自然语言合同与智能合约实现之间的差距。


<details>
  <summary>Details</summary>
Motivation: 监控合同执行是发现违约行为的关键，智能合约提供了防篡改和自动化处理的能力，但自然语言合同与智能合约实现之间存在较大差距。

Method: 开发了一个Web环境，支持用户辅助精化Symboleo规范（对应法律合同模板），然后自动生成可在Hyperledger Fabric平台部署的监控智能合约。

Result: 通过交互式能源领域的样本合同进行演示，证明该环境在加速法律遵循上下文中智能合约开发方面具有很大潜力。

Conclusion: 该研究为缩小自然语言合同与智能合约实现之间的差距提供了有效的解决方案，通过协助式精化和自动生成技术，促进了法律合同监控的自动化迁移。

Abstract: Monitoring the compliance of contract performance against legal obligations
is important in order to detect violations, ideally, as soon as they occur.
Such monitoring can nowadays be achieved through the use of smart contracts,
which provide protection against tampering as well as some level of automation
in handling violations. However, there exists a large gap between natural
language contracts and smart contract implementations. This paper introduces a
Web-based environment that partly fills that gap by supporting the
user-assisted refinement of Symboleo specifications corresponding to legal
contract templates, followed by the automated generation of monitoring smart
contracts deployable on the Hyperledger Fabric platform. This environment,
illustrated using a sample contract from the transactive energy domain, shows
much potential in accelerating the development of smart contracts in a legal
compliance context.

</details>


### [12] [Weakly Supervised Vulnerability Localization via Multiple Instance Learning](https://arxiv.org/abs/2509.11312)
*Wenchao Gu,Yupan Chen,Yanlin Wang,Hongyu Zhang,Cuiyun Gao,Michael R. Lyu*

Main category: cs.SE

TL;DR: WAVES是一种基于多示例学习的弱监督漏洞定位方法，无需语句级标注即可实现函数级漏洞检测和语句级漏洞定位


<details>
  <summary>Details</summary>
Motivation: 传统漏洞检测方法主要在函数或文件级别进行粗粒度检测，开发者仍需手动检查大量代码来定位具体漏洞语句。语句级标注需要专家知识且成本高昂，因此需要无需额外语句级标注的方法

Method: 采用多示例学习思想，将函数级真实标签转换为语句级伪标签，无需额外语句级标注。利用这些伪标签训练函数级表示向量的分类器

Result: 在三个流行基准数据集上的实验表明，该方法在漏洞检测方面达到可比性能，在语句级漏洞定位方面达到最先进性能

Conclusion: WAVES方法成功解决了漏洞定位中语句级标注成本高的问题，通过弱监督学习实现了准确的漏洞检测和定位

Abstract: Software vulnerability detection has emerged as a significant concern in the
field of software security recently, capturing the attention of numerous
researchers and developers. Most previous approaches focus on coarse-grained
vulnerability detection, such as at the function or file level. However, the
developers would still encounter the challenge of manually inspecting a large
volume of code inside the vulnerable function to identify the specific
vulnerable statements for modification, indicating the importance of
vulnerability localization. Training the model for vulnerability localization
usually requires ground-truth labels at the statement-level, and labeling
vulnerable statements demands expert knowledge, which incurs high costs. Hence,
the demand for an approach that eliminates the need for additional labeling at
the statement-level is on the rise. To tackle this problem, we propose a novel
approach called WAVES for WeAkly supervised Vulnerability Localization via
multiplE inStance learning, which does not need the additional statement-level
labels during the training. WAVES has the capability to determine whether a
function is vulnerable (i.e., vulnerability detection) and pinpoint the
vulnerable statements (i.e., vulnerability localization). Specifically,
inspired by the concept of multiple instance learning, WAVES converts the
ground-truth label at the function-level into pseudo labels for individual
statements, eliminating the need for additional statement-level labeling. These
pseudo labels are utilized to train the classifiers for the function-level
representation vectors. Extensive experimentation on three popular benchmark
datasets demonstrates that, in comparison to previous baselines, our approach
achieves comparable performance in vulnerability detection and state-of-the-art
performance in statement-level vulnerability localization.

</details>


### [13] [Large Language Models (LLMs) for Requirements Engineering (RE): A Systematic Literature Review](https://arxiv.org/abs/2509.11446)
*Mohammad Amin Zadenoori,Jacek Dąbrowski,Waad Alhoshan,Liping Zhao,Alessio Ferrari*

Main category: cs.SE

TL;DR: 本文对2023-2024年间74项研究进行系统文献综述，分析大语言模型在需求工程中的应用现状、趋势和挑战。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在各领域的广泛应用，需求工程作为语言密集型任务也开始受益于LLM技术，需要系统梳理当前研究现状和发展趋势。

Method: 采用系统文献综述方法，分析74篇2023-2024年的主要研究，从发表趋势、RE活动、提示策略、评估方法等多个维度进行分类分析。

Result: 研究发现LLM在需求工程中的应用主要集中在需求获取和验证，而非传统的缺陷检测；主要使用GPT模型和零样本/少样本提示；研究多在受控环境中进行，工业应用有限。

Conclusion: 研究指出了未来重要方向：扩大RE在SE中的影响力、探索较少研究的任务、改进提示方法、在真实环境中测试，并为研究者和实践者提供了相关工具和数据集列表。

Abstract: Large Language Models (LLMs) are finding applications in numerous domains,
and Requirements Engineering (RE) is increasingly benefiting from their
capabilities to assist with complex, language-intensive tasks. This paper
presents a systematic literature review of 74 primary studies published between
2023 and 2024, examining how LLMs are being applied in RE. The study
categorizes the literature according to several dimensions, including
publication trends, RE activities, prompting strategies, and evaluation
methods. Our findings indicate notable patterns, among which we observe
substantial differences compared to previous works leveraging standard Natural
Language Processing (NLP) techniques. Most of the studies focus on using LLMs
for requirements elicitation and validation, rather than defect detection and
classification, which were dominant in the past. Researchers have also
broadened their focus and addressed novel tasks, e.g., test generation,
exploring the integration of RE with other software engineering (SE)
disciplines. Although requirements specifications remain the primary focus,
other artifacts are increasingly considered, including issues from issue
tracking systems, regulations, and technical manuals. The studies mostly rely
on GPT-based models, and often use Zero-shot or Few-shot prompting. They are
usually evaluated in controlled environments, with limited use in industry
settings and limited integration in complex workflows. Our study outlines
important future directions, such as leveraging the potential to expand the
influence of RE in SE, exploring less-studied tasks, improving prompting
methods, and testing in real-world environments. Our contribution also helps
researchers and practitioners use LLMs more effectively in RE, by providing a
list of identified tools leveraging LLMs for RE, as well as datasets.

</details>


### [14] [VulAgent: Hypothesis-Validation based Multi-Agent Vulnerability Detection](https://arxiv.org/abs/2509.11523)
*Ziliang Wang,Ge Li,Jia Li,Hao Zhu,Zhi Jin*

Main category: cs.SE

TL;DR: VulAgent是一个基于假设验证的多智能体漏洞检测框架，通过模拟人类代码审计过程，使用专门化智能体从不同分析视角协作检测漏洞，显著提高了检测准确率和降低了误报率。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型在项目级漏洞检测中面临两个挑战：准确定位安全敏感代码，以及正确关联和推理复杂程序上下文。需要模拟人类审计员的代码审查方式来提高检测效果。

Method: 采用多智能体框架，每个智能体专注于特定分析视角（如内存、授权等），通过假设验证范式：先形成漏洞假设和触发路径，然后针对相关程序上下文和防御检查进行验证。

Result: 在两个数据集上平均提高整体准确率6.6%，漏洞-修复代码对的正确识别率最高提升450%（平均246%），误报率降低约36%。

Conclusion: VulAgent通过多智能体协作和假设验证范式，有效解决了项目级漏洞检测的挑战，在准确性和误报率方面显著优于现有基于LLM的基线方法。

Abstract: The application of language models to project-level vulnerability detection
remains challenging, owing to the dual requirement of accurately localizing
security-sensitive code and correctly correlating and reasoning over complex
program context. We present VulAgent, a multi-agent vulnerability detection
framework based on hypothesis validation. Our design is inspired by how human
auditors review code: when noticing a sensitive operation, they form a
hypothesis about a possible vulnerability, consider potential trigger paths,
and then verify the hypothesis against the surrounding context. VulAgent
implements a semantics-sensitive, multi-view detection pipeline: specialized
agents, each aligned to a specific analysis perspective (e.g., memory,
authorization), collaboratively surface and precisely localize sensitive code
sites with higher coverage. Building on this, VulAgent adopts a
hypothesis-validation paradigm: for each vulnerability report, it builds
hypothesis conditions and a trigger path, steering the LLM to target the
relevant program context and defensive checks during verification, which
reduces false positives. On average across the two datasets, VulAgent improves
overall accuracy by 6.6%, increases the correct identification rate of
vulnerable--fixed code pairs by up to 450% (246% on average), and reduces the
false positive rate by about 36% compared with state-of-the-art LLM-based
baselines.

</details>


### [15] [Sedeve-Kit, a Specification-Driven Development Framework for Building Distributed Systems](https://arxiv.org/abs/2509.11566)
*Hua Guo,Yunhong Ji,Xuan Zhou*

Main category: cs.SE

TL;DR: 提出了一个规范驱动的分布式系统开发框架，通过TLA+规范定义、模型检查和测试用例生成来确保系统正确性


<details>
  <summary>Details</summary>
Motivation: 解决分布式系统中非确定性并发和故障带来的复杂性挑战

Method: 三阶段方法：1) 使用TLA+定义规范和不变式，进行模型检查并生成测试用例；2) 基于规范编写代码；3) 使用生成的测试用例进行严格测试

Result: 通过抽象设计与具体实现之间的持续验证，确保系统质量

Conclusion: 该框架通过规范驱动的开发方法有效解决了分布式系统开发的复杂性，确保了系统的正确性和可靠性

Abstract: Developing distributed systems presents significant challenges, primarily due
to the complexity introduced by non-deterministic concurrency and faults. To
address these, we propose a specification-driven development framework. Our
method encompasses three key stages. The first stage defines system
specifications and invariants using TLA${^+}$. It allows us to perform model
checking on the algorithm's correctness and generate test cases for subsequent
development phases. In the second stage, based on the established
specifications, we write code to ensure consistency and accuracy in the
implementation. Finally, after completing the coding process, we rigorously
test the system using the test cases generated in the initial stage. This
process ensures system quality by maintaining a strong connection between the
abstract design and the concrete implementation through continuous
verification.

</details>


### [16] [Automated Creation and Enrichment Framework for Improved Invocation of Enterprise APIs as Tools](https://arxiv.org/abs/2509.11626)
*Prerna Agarwal,Himanshu Gupta,Soujanya Soni,Rohith Vallam,Renuka Sindhgatta,Sameep Mehta*

Main category: cs.SE

TL;DR: ACE是一个自动化工具创建和增强框架，将企业API转换为LLM兼容工具，通过生成丰富的工具规范和动态筛选机制，提高工具选择和调用的准确性。


<details>
  <summary>Details</summary>
Motivation: 企业环境中，LLM代理使用API工具面临文档质量差、输入输出模式复杂、操作数量多等挑战，导致工具选择困难和负载形成准确性下降达25%。

Method: ACE框架：(i)生成包含参数描述和示例的丰富工具规范；(ii)集成动态筛选机制，在运行时过滤相关工具，降低提示复杂度同时保持可扩展性。

Result: 在专有和开源API上验证了框架有效性，并展示了与代理框架的集成能力。

Conclusion: ACE是首个端到端自动化企业API工具创建、增强和动态选择的框架，显著提升了LLM代理的工具使用效果。

Abstract: Recent advancements in Large Language Models (LLMs) has lead to the
development of agents capable of complex reasoning and interaction with
external tools. In enterprise contexts, the effective use of such tools that
are often enabled by application programming interfaces (APIs), is hindered by
poor documentation, complex input or output schema, and large number of
operations. These challenges make tool selection difficult and reduce the
accuracy of payload formation by up to 25%. We propose ACE, an automated tool
creation and enrichment framework that transforms enterprise APIs into
LLM-compatible tools. ACE, (i) generates enriched tool specifications with
parameter descriptions and examples to improve selection and invocation
accuracy, and (ii) incorporates a dynamic shortlisting mechanism that filters
relevant tools at runtime, reducing prompt complexity while maintaining
scalability. We validate our framework on both proprietary and open-source APIs
and demonstrate its integration with agentic frameworks. To the best of our
knowledge, ACE is the first end-to-end framework that automates the creation,
enrichment, and dynamic selection of enterprise API tools for LLM agents.

</details>


### [17] [Do Code Semantics Help? A Comprehensive Study on Execution Trace-Based Information for Code Large Language Models](https://arxiv.org/abs/2509.11686)
*Jian Wang,Xiaofei Xie,Qiang Hu,Shangqing Liu,Yi Li*

Main category: cs.SE

TL;DR: 代码大语言模型在程序执行行为理解和语义信息表达方面存在显著限制，本文提出了一种集成语义信息的框架，但实验结果显示语义信息对代码LLM的微调和推理改善效果有限


<details>
  <summary>Details</summary>
Motivation: 解决代码大语言模型在程序运行行为理解和语义信息表达不一致方面的核心限制，这些问题影响了模型的实际部署和能力

Method: 提出一种通用框架，支持将语义信息（如执行跟踪）集成到代码任务相关提示中，并通过完整的实验研究进行验证

Result: 实验结果与以往研究相反，显示语义信息对代码LLM的监督微调和测试时扩展的帮助有限

Conclusion: 语义信息对代码LLM的理解能力改善效果并不如预期，需要更系统的方法来提升模型的理解能力

Abstract: Code Large Language Models (Code LLMs) have opened a new era in programming
with their impressive capabilities. However, recent research has revealed
critical limitations in their ability to reason about runtime behavior and
understand the actual functionality of programs, which poses significant
challenges for their post-training and practical deployment. Specifically, Code
LLMs encounter two principal issues: (1) a lack of proficiency in reasoning
about program execution behavior, as they struggle to interpret what programs
actually do during runtime, and (2) the inconsistent and fragmented
representation of semantic information, such as execution traces, across
existing methods, which hinders their ability to generalize and reason
effectively. These challenges underscore the necessity for more systematic
approaches to enhance the reasoning capabilities of Code LLMs. To address these
issues, we introduce a generic framework to support integrating semantic
information~(e.g., execution trace) to code task-relevant prompts, and conduct
a comprehensive study to explore the role of semantic information in enhancing
the reasoning ability of Code LLMs accordingly. Specifically, we focus on
investigating the usefulness of trace-based semantic information in boosting
supervised fine-tuning~(SFT) and post-phase inference of Code LLMs. The
experimental results surprisingly disagree with previous works and demonstrate
that semantic information has limited usefulness for SFT and test time scaling
of Code LLM.

</details>


### [18] [AI Asset Management for Manufacturing (AIM4M): Development of a Process Model for Operationalization](https://arxiv.org/abs/2509.11691)
*Lukas Rauh,Mel-Rick Süner,Daniel Schel,Thomas Bauernhansl*

Main category: cs.SE

TL;DR: 这篇论文提出了一种新的人工智能资产生命周期管理模型，以解决制造业中AI运营化的挑战，特别是在网络物理生产系统(CPPS)环境中的应用困难。


<details>
  <summary>Details</summary>
Motivation: 虽然人工智能在制造业有明显优势，但在原型阶段之外进行运营化避過了技术系统复杂性、缺乏实施标准和组织过程分散等挑战，特别是在CPPS环境中。

Method: 论文提出了一种基于机器学习运营(MLOps)原则的过程模型，并针对CPPS领域特定需求进行了三个方面的精炼。

Result: 该过程模型能够支持组织在实践中系统地开发、部署和管理AI资产，满足CPPS特定约束和监管要求。

Conclusion: 该研究为制造业AI运营化提供了一种有效的生命周期管理方法，将机器学习运营原则与CPPS领域特定需求相结合，有助于解决实际应用中的挑战。

Abstract: The benefits of adopting artificial intelligence (AI) in manufacturing are
undeniable. However, operationalizing AI beyond the prototype, especially when
involved with cyber-physical production systems (CPPS), remains a significant
challenge due to the technical system complexity, a lack of implementation
standards and fragmented organizational processes. To this end, this paper
proposes a new process model for the lifecycle management of AI assets designed
to address challenges in manufacturing and facilitate effective
operationalization throughout the entire AI lifecycle. The process model, as a
theoretical contribution, builds on machine learning operations (MLOps)
principles and refines three aspects to address the domain-specific
requirements from the CPPS context. As a result, the proposed process model
aims to support organizations in practice to systematically develop, deploy and
manage AI assets across their full lifecycle while aligning with CPPS-specific
constraints and regulatory demands.

</details>


### [19] [From Evaluation to Enhancement: Large Language Models for Zero-Knowledge Proof Code Generation](https://arxiv.org/abs/2509.11708)
*Zhantong Xue,Pingchuan Ma,Zhaoyu Wang,Shuai Wang*

Main category: cs.SE

TL;DR: 这篇论文提出了ZK-Eval和ZK-Coder两个框架，用于评估和提升LLM在零知识证明编程中的性能，将成功率从17.35%提升到83.38%


<details>
  <summary>Details</summary>
Motivation: 零知识证明编程复杂易错，需要处理有限域算术、约束系统和器件等概念，而现有LLM在这一领域的效果仍未经过系统评估

Method: 提出ZK-Eval评估流水线（语言知识、器件能力、端到端生成）和ZK-Coder框架（约束绘图、导航检索、交互修复）

Result: 对四个独立LLM的评估显示模型在语法层面表现优异但在器件使用和语义正确性上遇到困难；ZK-Coder在Circom和Noir上将成功率分别从17.35%提升到83.38%和从32.21%提升到90.05%

Conclusion: 设计了系统性的方法来衡量和增强LLM在ZK代码生成中的能力，为实践者降低门槛并推进可信计算的发展

Abstract: Zero-knowledge proofs (ZKPs) are increasingly deployed in domains such as
privacy-preserving authentication, blockchain scalability, and secure finance.
However, authoring ZK programs remains challenging: unlike mainstream
programming, ZK development requires reasoning about finite field arithmetic,
constraint systems, and gadgets, making it knowledge-intensive and error-prone.
While large language models (LLMs) have demonstrated strong code generation
capabilities in general-purpose languages, their effectiveness for ZK
programming, where correctness hinges on both language mastery and gadget-level
reasoning, remains unexplored. To address this gap, we propose
\textsc{ZK-Eval}, a domain-specific evaluation pipeline that probes LLM
capabilities at three levels: language knowledge, gadget competence, and
end-to-end program generation. Our evaluation of four state-of-the-art LLMs
reveals that models excel at surface-level syntax but struggle with gadget
usage and semantic correctness, often yielding incorrect programs. Based on
these insights, we introduce \textsc{ZK-Coder}, an agentic framework that
augments LLMs with constraint sketching, guided retrieval, and interactive
repair. Experiments on Circom and Noir show substantial gains, with success
rates improving from 17.35\% to 83.38\% and from 32.21\% to 90.05\%,
respectively. With \textsc{ZK-Eval} and \textsc{ZK-Coder}, we establish a
foundation for systematically measuring and augmenting LLMs in ZK code
generation to lower barriers for practitioners and advance trustworthy
computation.

</details>


### [20] [Toward Greener Background Processes -- Measuring Energy Cost of Autosave Feature](https://arxiv.org/abs/2509.11738)
*Maria Küüsvek,Hina Anwar*

Main category: cs.SE

TL;DR: 这篇论文提出了一种可重用的能源消耗评估流程，重点关注桌面应用的背景进程能源影响，并通过Python文本编辑器的自动保存功能进行了实证研究。


<details>
  <summary>Details</summary>
Motivation: 桌面应用的背景进程常被忽视，但具有重要的积累能源影响，需要系统性的能源行为评估方法。

Method: 三阶段评估流程：1)背景功能分解为核心操作 2)操作隔离 3)控制测量进行比较分析。以三款Python文本编辑器的自动保存功能为案例研究，进行900次软件能源测量。

Result: 识别了影响能源消耗的关键设计因素，包括保存频率、缓冲策略和辅助逻辑（如变更检测）。

Conclusion: 提出了四条可执行的建议，以支持Python自动保存功能的绿色实现，促进可持续软件开发实践。

Abstract: Background processes in desktop applications are often overlooked in energy
consumption studies, yet they represent continuous, automated workloads with
significant cumulative impact. This paper introduces a reusable process for
evaluating the energy behavior of such features at the level of operational
design. The process works in three phases: 1) decomposing background
functionality into core operations, 2) operational isolation, and 3) controlled
measurements enabling comparative profiling. We instantiate the process in a
case study of autosave implementations across three open-source Python-based
text editors. Using 900 empirical software-based energy measurements, we
identify key design factors affecting energy use, including save frequency,
buffering strategy, and auxiliary logic such as change detection. We give four
actionable recommendations for greener implementations of autosave features in
Python to support sustainable software practices.

</details>


### [21] [Analysing Python Machine Learning Notebooks with Moose](https://arxiv.org/abs/2509.11748)
*Marius Mignard,Steven Costiou,Nicolas Anquetil,Anne Etien*

Main category: cs.SE

TL;DR: Vespucci Linter是一个针对机器学习笔记本代码的多层次静态分析工具，能够检测Python编码规范、笔记本组织结构以及ML特定问题三个层面的代码质量问题


<details>
  <summary>Details</summary>
Motivation: 机器学习笔记本代码质量普遍较低，现有工具通常只关注单一层面且难以捕捉ML特定语义，需要一种能够进行多层次上下文分析的工具

Method: 基于Moose构建，采用元建模方法统一笔记本结构元素和Python代码实体，实现了22个基于文献的linting规则，并在5000个Kaggle笔记本上应用验证

Result: 在所有三个层面都发现了违规情况，验证了多层次方法的有效性，证明了工具在提高笔记本环境中ML开发质量和可靠性方面的潜力

Conclusion: Vespucci Linter通过多层次静态分析成功解决了ML笔记本代码质量问题，为改善机器学习开发实践提供了有效工具

Abstract: Machine Learning (ML) code, particularly within notebooks, often exhibits
lower quality compared to traditional software. Bad practices arise at three
distinct levels: general Python coding conventions, the organizational
structure of the notebook itself, and ML-specific aspects such as
reproducibility and correct API usage. However, existing analysis tools
typically focus on only one of these levels and struggle to capture ML-specific
semantics, limiting their ability to detect issues. This paper introduces
Vespucci Linter, a static analysis tool with multi-level capabilities, built on
Moose and designed to address this challenge. Leveraging a metamodeling
approach that unifies the notebook's structural elements with Python code
entities, our linter enables a more contextualized analysis to identify issues
across all three levels. We implemented 22 linting rules derived from the
literature and applied our tool to a corpus of 5,000 notebooks from the Kaggle
platform. The results reveal violations at all levels, validating the relevance
of our multi-level approach and demonstrating Vespucci Linter's potential to
improve the quality and reliability of ML development in notebook environments.

</details>


### [22] [CodeCureAgent: Automatic Classification and Repair of Static Analysis Warnings](https://arxiv.org/abs/2509.11787)
*Pascal Joos,Islem Bouzenia,Michael Pradel*

Main category: cs.SE

TL;DR: CodeCureAgent是一个基于LLM的智能代理系统，能够自动分析、分类和修复静态分析警告，无需人工干预，在1000个SonarQube警告中实现了96.8%的合理修复率。


<details>
  <summary>Details</summary>
Motivation: 传统静态分析工具需要开发者手动处理警告，过程繁琐导致警告积累和代码质量下降，需要自动化解决方案。

Method: 采用基于LLM的代理框架，迭代调用工具收集代码库信息并编辑代码修复警告，包含三步启发式补丁批准流程：构建项目、验证警告消失且无新警告、运行测试套件。

Result: 在106个Java项目的1000个SonarQube警告上，实现了96.8%的合理修复率，比现有最佳方法分别高出30.7%和29.2%，正确修复率达86.3%，每个警告处理成本约2.9美分，耗时约4分钟。

Conclusion: CodeCureAgent能可靠修复静态分析警告，可用于清理现有代码库并集成到CI/CD管道中防止警告积累。

Abstract: Static analysis tools are widely used to detect bugs, vulnerabilities, and
code smells. Traditionally, developers must resolve these warnings manually.
Because this process is tedious, developers sometimes ignore warnings, leading
to an accumulation of warnings and a degradation of code quality. This paper
presents CodeCureAgent, an approach that harnesses LLM-based agents to
automatically analyze, classify, and repair static analysis warnings. Unlike
previous work, our method does not follow a predetermined algorithm. Instead,
we adopt an agentic framework that iteratively invokes tools to gather
additional information from the codebase (e.g., via code search) and edit the
codebase to resolve the warning. CodeCureAgent detects and suppresses false
positives, while fixing true positives when identified. We equip CodeCureAgent
with a three-step heuristic to approve patches: (1) build the project, (2)
verify that the warning disappears without introducing new warnings, and (3)
run the test suite. We evaluate CodeCureAgent on a dataset of 1,000 SonarQube
warnings found in 106 Java projects and covering 291 distinct rules. Our
approach produces plausible fixes for 96.8% of the warnings, outperforming
state-of-the-art baseline approaches by 30.7% and 29.2% in plausible-fix rate,
respectively. Manual inspection of 291 cases reveals a correct-fix rate of
86.3%, showing that CodeCureAgent can reliably repair static analysis warnings.
The approach incurs LLM costs of about 2.9 cents (USD) and an end-to-end
processing time of about four minutes per warning. We envision CodeCureAgent
helping to clean existing codebases and being integrated into CI/CD pipelines
to prevent the accumulation of static analysis warnings.

</details>


### [23] [MMORE: Massive Multimodal Open RAG & Extraction](https://arxiv.org/abs/2509.11937)
*Alexandre Sallinen,Stefan Krsteski,Paul Teiletche,Marc-Antoine Allard,Baptiste Lecoeur,Michael Zhang,Fabrice Nemo,David Kalajdzic,Matthias Meyer,Mary-Anne Hartley*

Main category: cs.SE

TL;DR: MMORE是一个开源的多模态检索增强生成管道，支持15+种文件格式处理，提供分布式处理能力，在速度和准确性上优于基准方法，并能提升医学问答的准确性。


<details>
  <summary>Details</summary>
Motivation: 为了解决异构多模态文档的大规模知识检索和处理问题，支持LLM下游应用，需要统一的处理管道来处理文本、表格、图像、音频、视频等多种格式的文档。

Method: 采用模块化分布式处理架构，支持CPU和GPU的并行扩展，使用混合稠密-稀疏检索技术，提供交互式API和批量RAG端点。

Result: 在处理基准测试中比单节点基线快3.8倍，在扫描PDF上的准确率比Docling高40%，在PubMedQA上医学LLM的问答准确性随检索深度增加而提升。

Conclusion: MMORE为在多样化真实世界多模态数据上部署任务无关的RAG系统提供了强大且可扩展的基础框架。

Abstract: We introduce MMORE, an open-source pipeline for Massive Multimodal Open
RetrievalAugmented Generation and Extraction, designed to ingest, transform,
and retrieve knowledge from heterogeneous document formats at scale. MMORE
supports more than fifteen file types, including text, tables, images, emails,
audio, and video, and processes them into a unified format to enable downstream
applications for LLMs. The architecture offers modular, distributed processing,
enabling scalable parallelization across CPUs and GPUs. On processing
benchmarks, MMORE demonstrates a 3.8-fold speedup over single-node baselines
and 40% higher accuracy than Docling on scanned PDFs. The pipeline integrates
hybrid dense-sparse retrieval and supports both interactive APIs and batch RAG
endpoints. Evaluated on PubMedQA, MMORE-augmented medical LLMs improve
biomedical QA accuracy with increasing retrieval depth. MMORE provides a
robust, extensible foundation for deploying task-agnostic RAG systems on
diverse, real-world multimodal data. The codebase is available at
https://github.com/swiss-ai/mmore.

</details>


### [24] [VisDocSketcher: Towards Scalable Visual Documentation with Agentic Systems](https://arxiv.org/abs/2509.11942)
*Luís F. Gomes,Xin Zhou,David Lo,Rui Abreu*

Main category: cs.SE

TL;DR: 这篇论文提出了第一个基于代理LLM系统的自动视觉文档生成方法VisDocSketcher，能够从代码生成74.4%的有效视觉文档，并提出了评价框架AutoSketchEval来量化评估质量。


<details>
  <summary>Details</summary>
Motivation: 视觉文档能够降低开发者理解代码的认知障碍，但手工创建耐时而评估主观，目前缺乏自动生成高级视觉文档的方法。

Method: 提出VisDocSketcher方法，结合静态分析和LLM代理来识别代码中的关键元素并生成对应的视觉表示。

Result: 实验结果显示，该方法能够为74.4%的样本生成有效视觉文档，比简单模板基线提高26.7-39.8%。评估框架能够可靠区分高质量和低质量文档，AUC超过0.87。

Conclusion: 该研究为自动视觉文档领域奠定了基础，提供了既能生成有效视觉表示又能可靠评估质量的实用工具。

Abstract: Visual documentation is an effective tool for reducing the cognitive barrier
developers face when understanding unfamiliar code, enabling more intuitive
comprehension. Compared to textual documentation, it provides a higher-level
understanding of the system structure and data flow. Developers usually prefer
visual representations over lengthy textual descriptions for large software
systems. Visual documentation is both difficult to produce and challenging to
evaluate. Manually creating it is time-consuming, and currently, no existing
approach can automatically generate high-level visual documentation directly
from code. Its evaluation is often subjective, making it difficult to
standardize and automate. To address these challenges, this paper presents the
first exploration of using agentic LLM systems to automatically generate visual
documentation. We introduce VisDocSketcher, the first agent-based approach that
combines static analysis with LLM agents to identify key elements in the code
and produce corresponding visual representations. We propose a novel evaluation
framework, AutoSketchEval, for assessing the quality of generated visual
documentation using code-level metrics. The experimental results show that our
approach can valid visual documentation for 74.4% of the samples. It shows an
improvement of 26.7-39.8% over a simple template-based baseline. Our evaluation
framework can reliably distinguish high-quality (code-aligned) visual
documentation from low-quality (non-aligned) ones, achieving an AUC exceeding
0.87. Our work lays the foundation for future research on automated visual
documentation by introducing practical tools that not only generate valid
visual representations but also reliably assess their quality.

</details>


### [25] [LitterBox+: An Extensible Framework for LLM-enhanced Scratch Static Code Analysis](https://arxiv.org/abs/2509.12021)
*Benedikt Fein,Florian Obermüller,Gordon Fraser*

Main category: cs.SE

TL;DR: LitterBox+框架将Scratch积木编程转换为文本表示，结合LLM能力提供代码查询、质量分析和修复建议，并集成到Scratch界面中


<details>
  <summary>Details</summary>
Motivation: 解决积木式编程环境Scratch中LLM无法直接处理图形化代码的问题，为学习者提供AI编程辅助

Method: 扩展LitterBox静态分析工具，将块状代码转换为适合LLM的文本表示，提供API和UI集成

Result: 开发出可直接在Scratch环境中使用的LLM辅助工具，支持代码查询、质量问题和修复生成

Conclusion: LitterBox+成功桥接了积木编程与LLM技术，为教育编程环境提供了可扩展的AI辅助框架

Abstract: Large language models (LLMs) have become an essential tool to support
developers using traditional text-based programming languages, but the
graphical notation of the block-based Scratch programming environment inhibits
the use of LLMs. To overcome this limitation, we propose the LitterBox+
framework that extends the Scratch static code analysis tool LitterBox with the
generative abilities of LLMs. By converting block-based code to a textual
representation suitable for LLMs, LitterBox+ allows users to query LLMs about
their programs, about quality issues reported by LitterBox, and it allows
generating code fixes. Besides offering a programmatic API for these
functionalities, LitterBox+ also extends the Scratch user interface to make
these functionalities available directly in the environment familiar to
learners. The framework is designed to be easily extensible with other prompts,
LLM providers, and new features combining the program analysis capabilities of
LitterBox with the generative features of LLMs. We provide a screencast
demonstrating the tool at https://youtu.be/RZ6E0xgrIgQ.

</details>


### [26] [A New Benchmark for Evaluating Code Translation with Third-Party Libraries](https://arxiv.org/abs/2509.12087)
*Pengyu Xue,Kunwu Zheng,Zhen Yang,Yifei Pei,Linhao Wu,Jiahui Dong,Xiapu Luo,Yan Xiao,Fei Liu,Yuxuan Zhang,Xiran Lyu,Xianhang Li,Xuanyu Zhu,Chengyi Wang*

Main category: cs.SE

TL;DR: 这篇论文构建了第一个专门评测第三方库中心代码翻译的基准TransLibEval，发现LLM在涉及TPL的代码翻译中性能显著下降（平均CA下降超60%），并揭示了之前被激洞的第三方引用错误。


<details>
  <summary>Details</summary>
Motivation: 实际编程中过超90%的代码依赖第三方库，但现有基准测试集在TPL类别和规模上有限，导致TPL相关错误难以暴露，阻碍了有针对性解决方案的发展。

Method: 构建TransLibEval基准，包含200个Python、Java和C++实际任务，每个任务明确涉及来自数据处理、机器学习、网页开发等多样类别的TPL，具有全面的依赖覆盖和高覆盖测试套件。评估7个最新LLM（商业、通用和代码专业系列）在三类六种翻译策略下的表现。

Result: 实验结果显示与无库设置相比，LLM的性能减退显著（平均代码精确度下降超过60%），不同策略展现异质优势。对GPT-4o的4,831个失败案例分析揭示了众多之前被激洞的第三方引用错误。

Conclusion: 这些发现突出了库中心翻译的独特挑战，为改进具有TPL意识的代码智能提供了实用指南，应重点关注第三方库相关的代码翻译问题。

Abstract: In recent years, Large Language Models (LLMs) have been widely studied in the
code translation field on the method, class, and even repository levels.
However, most of these benchmarks are limited in terms of Third-Party Library
(TPL) categories and scales, making TPL-related errors hard to expose and
hindering the development of targeted solutions. Considering the high
dependence (over 90%) on TPLs in practical programming, demystifying and
analyzing LLMs' code translation performance involving various TPLs becomes
imperative. To address this gap, we construct TransLibEval, the first benchmark
dedicated to library-centric code translation. It consists of 200 real-world
tasks across Python, Java, and C++, each explicitly involving TPLs from diverse
categories such as data processing, machine learning, and web development, with
comprehensive dependency coverage and high-coverage test suites. We evaluate
seven recent LLMs of commercial, general, and code-specialized families under
six translation strategies of three categories: Direct, IR-guided, and
Retrieval-augmented. Experimental results show a dramatic performance drop
compared with library-free settings (average CA decline over 60%), while
diverse strategies demonstrate heterogeneous advantages. Furthermore, we
analyze 4,831 failed cases from GPT-4o, one of the State-of-the-Art (SOTA)
LLMs, revealing numerous third-party reference errors that were obscured
previously. These findings highlight the unique challenges of library-centric
translation and provide practical guidance for improving TPL-aware code
intelligence.

</details>


### [27] [EfficientUICoder: Efficient MLLM-based UI Code Generation via Input and Output Token Compression](https://arxiv.org/abs/2509.12159)
*Jingyu Xiao,Zhongyi Zhang,Yuxuan Wan,Yintong Huo,Yang Liu,Michael R. Lyu*

Main category: cs.SE

TL;DR: EfficientUICoder是一个针对UI2Code任务的高效压缩框架，通过元素感知压缩、区域感知细化和自适应重复抑制，在保持网页质量的同时实现55%-60%的压缩比，显著降低计算成本和时间开销。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在UI2Code任务中计算开销巨大，存在图像和代码令牌的显著冗余，导致计算复杂度高、关键UI元素关注不足，以及生成冗长无效的HTML文件。

Method: 提出三组件压缩框架：1)元素和布局感知令牌压缩，检测元素区域并构建UI元素树；2)区域感知令牌细化，利用注意力分数筛选重要令牌；3)自适应重复令牌抑制，动态减少HTML/CSS结构重复生成。

Result: 在34B级MLLMs上实现55%-60%压缩比，计算成本降低44.9%，生成令牌减少41.4%，预填充时间减少46.6%，推理时间减少48.8%，且不损害网页质量。

Conclusion: EfficientUICoder有效解决了UI2Code任务中的计算冗余问题，显著提升了效率，为高效UI代码生成提供了实用解决方案。

Abstract: Multimodal Large Language Models have demonstrated exceptional performance in
UI2Code tasks, significantly enhancing website development efficiency. However,
these tasks incur substantially higher computational overhead than traditional
code generation due to the large number of input image tokens and extensive
output code tokens required. Our comprehensive study identifies significant
redundancies in both image and code tokens that exacerbate computational
complexity and hinder focus on key UI elements, resulting in excessively
lengthy and often invalid HTML files. We propose EfficientUICoder, a
compression framework for efficient UI code generation with three key
components. First, Element and Layout-aware Token Compression preserves
essential UI information by detecting element regions and constructing UI
element trees. Second, Region-aware Token Refinement leverages attention scores
to discard low-attention tokens from selected regions while integrating
high-attention tokens from unselected regions. Third, Adaptive Duplicate Token
Suppression dynamically reduces repetitive generation by tracking HTML/CSS
structure frequencies and applying exponential penalties. Extensive experiments
show EfficientUICoderachieves a 55%-60% compression ratio without compromising
webpage quality and delivers superior efficiency improvements: reducing
computational cost by 44.9%, generated tokens by 41.4%, prefill time by 46.6%,
and inference time by 48.8% on 34B-level MLLMs. Code is available at
https://github.com/WebPAI/EfficientUICoder.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [28] [AegisShield: Democratizing Cyber Threat Modeling with Generative AI](https://arxiv.org/abs/2509.10482)
*Matthew Grofsky*

Main category: cs.CR

TL;DR: AegisShield是一个基于生成式AI的威胁建模工具，通过自动化威胁生成和系统评估，帮助资源有限的小型组织应对复杂技术系统的安全威胁。


<details>
  <summary>Details</summary>
Motivation: 传统威胁建模方法难以扩展，特别是对于资源有限的小型组织。技术系统日益复杂使得手动威胁建模变得困难且耗时。

Method: 开发AegisShield工具，集成STRIDE和MITRE ATT&CK框架，利用生成式AI自动生成威胁，并整合国家漏洞数据库和AlienVault开放威胁交换的实时威胁情报。

Result: 评估243个威胁和8000多个AI生成威胁显示：显著降低复杂度(p<0.001)，输出与专家开发威胁语义对齐(p<0.05)，85.4%的成功率映射到MITRE ATT&CK技术(p<0.001)。

Conclusion: AegisShield通过自动化和标准化威胁建模，帮助资源不足的组织更早应对风险，支持安全设计实践的广泛应用。

Abstract: The increasing sophistication of technology systems makes traditional threat
modeling hard to scale, especially for small organizations with limited
resources. This paper develops and evaluates AegisShield, a generative AI
enhanced threat modeling tool that implements STRIDE and MITRE ATT&CK to
automate threat generation and provide systematic assessments. By integrating
real time threat intelligence from the National Vulnerability Database and
AlienVault Open Threat Exchange, AegisShield produces streamlined and
accessible threat descriptions. Our assessment of 243 threats from 15 case
studies and over 8000 AI generated threats shows that AegisShield reduces
complexity (p less than 0.001), yields outputs semantically aligned with expert
developed threats (p less than 0.05), and achieves an 85.4 percent success rate
in mapping threats to MITRE ATT&CK techniques (p less than 0.001). Automating
and standardizing threat modeling helps under resourced organizations address
risk earlier and supports wider adoption of secure by design practices.

</details>


### [29] [Turning CVEs into Educational Labs:Insights and Challenges](https://arxiv.org/abs/2509.10488)
*Trueye Tafese*

Main category: cs.CR

TL;DR: 通过Docker容器技术将CVE漏洞转化为实践教学实验室，提供结构化教程和评估体系，有效提升学生对网络安全原理的理解和实践能力。


<details>
  <summary>Details</summary>
Motivation: 解决网络安全教育中理论与实践脱节的问题，通过将真实CVE漏洞转化为安全的实验环境，促进学生对实际安全挑战的深入理解。

Method: 使用Docker容器技术构建容器化实验环境，模拟SQL注入、任意代码执行、SSL证书验证缺陷等真实漏洞，配合结构化教程、前后测评和恢复步骤。

Result: 学生对网络安全原理、威胁缩减技术和安全编码实践的理解显著提升，证明了该方法在教育效果上的成功。

Conclusion: 该研究提供了一种可扩展、可复现的模型，有效地将CVE漏洞整合到网络安全教育中，在受控安全环境中培养学生对实际安全挑战的深层理解。

Abstract: This research focuses on transforming CVEs to hands-on educational lab for
cybersecurity training. The study shows the practical application of CVEs by
developing containerized lab environments- Docker to simulate real-world
vulnerabilities like SQL Injection, arbitrary code execution, and improper SSL
certificate validation. These labs has structured tutorials, pre- and
post-surveys to evaluate learning outcomes, and remediation steps.Key
challenges included interpreting limited CVE data, resolving technical
complexities in lab design, and ensuring accessibility for diverse learners.
Despite these difficulties, the findings highlight the use of educational
benefits of vulnerability analysis, bridging theoretical concepts with hands-on
experience. The results indicate that students improved comprehension of
cybersecurity principles, threat mitigation techniques, and secure coding
practices. This innovative approach provides a scalable and reproducible model
for integrating CVEs into cybersecurity education, fostering a deeper
understanding of real-world security challenges in a controlled and safe
environment.

</details>


### [30] [Investigation Of The Distinguishability Of Giraud-Verneuil Atomic Blocks](https://arxiv.org/abs/2509.10492)
*Philip Laryea Doku*

Main category: cs.CR

TL;DR: 这篇论文研究了椭圆曲线加密系统(ECC)在原子性防护方法下的侧道攻击安全性，发现屏蔽操作虽然正确实现，但硬件/软件层的额外时钟周期还是会导致原子块可区分，影响SCA防护效果。


<details>
  <summary>Details</summary>
Motivation: 椭圆曲线加密虽然效率高安全性强，但容易受到侧道攻击的威胁。虽然有各种防护措施，但需要实验验证原子性方法的实际效果。

Method: 在TI LAUNCHXLF28379D微控制器上实现Giraud和Verneuil的原子模式，使用FLECC库进行常数时间操作。通过Lecroy波形设备和Langer探头测量电磁涛漏，分析原子块在EM跟踪中的可区分性。

Result: 当存在额外时钟周期过程时，原子块可以通过视觉区分；清除这些过程后，它们更同步且更难区分，减少了SCA攻击成功的风险。

Conclusion: 原子性单独使用不能完全保护ECC免受侧道攻击。需要进一步研究额外时钟周期过程的成因和内存寄存器中间操作的处理方式，以提高防护措施的有效性。

Abstract: In this work, we investigate the security of Elliptic Curve Cryptosystem
(ECC) implementations against Side-Channel Analysis (SCA). ECC is well known
for its efficiency and strong security, yet vulnerable to SCA which exploits
physical information leaked during scalar multiplication (kP). Countermeasures
such as regularity and atomicity exist; this thesis focuses on atomicity. In
this work, we study the Giraud and Verneuil atomic pattern for kP, implementing
it using the right-to-left kP algorithm on the NIST EC P-256 curve. We use the
FLECC library with constant-time operations and execute on the Texas
Instruments LAUNCHXLF28379D MCU. We measure Electromagnetic (EM) emissions
during kP using a Lecroy WavePro 604HD Oscilloscope, a Langer ICS 105
Integrated Circuit Scanner, and a Langer MFA-R 0.2-75 Near Field Probe. We
investigate whether the Giraud and Verneuil atomic blocks are distinguishable
in EM traces. Our findings show that, when additional clock cycle processes are
present, the atomic blocks can be visually distinguished; after removing these
processes, they become more synchronised and harder to distinguish, reducing
the risk of a successful SCA attack. These results show that, although the
atomic pattern is correctly implemented with dummy operations, resistance to
SCA can still be affected by additional processes inserted at hardware or
software level.This means atomicity alone may not fully protect ECC from SCA.
More research is needed to investigate the causes of the additional clock cycle
processes and how intermediate operations are addressed in memory registers.
This will help to understand the processes that lead to the insertion of these
additional clock cycles. This thesis is the first to experimentally implement
and investigate Giraud and Verneuil's atomic pattern on hardware, and it offers
useful results to improve countermeasures against SCA.

</details>


### [31] [EchoLeak: The First Real-World Zero-Click Prompt Injection Exploit in a Production LLM System](https://arxiv.org/abs/2509.10540)
*Pavan Reddy,Aditya Sanjay Gujral*

Main category: cs.CR

TL;DR: EchoLeak是一个在Microsoft 365 Copilot中发现的无交互提示注入漏洞，通过精心构造的邮件实现远程数据泄露，暴露了AI助手在企业环境中的严重安全风险。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型助手在企业工作流程中的集成度提高，连接内部和外部数据源带来了新的安全担忧，需要研究实际生产环境中的提示注入漏洞及其防御措施。

Method: 通过多级绕过技术链：规避微软的XPIA分类器、使用引用式Markdown绕过链接审查、利用自动获取的图像、滥用Microsoft Teams代理，实现无需用户交互的权限提升。

Result: 成功实现了跨LLM信任边界的完整权限提升，证明了提示注入在生产AI系统中是实际存在的高危漏洞类别。

Conclusion: 提出了包括提示分区、增强输入输出过滤、基于来源的访问控制和严格内容安全策略等工程缓解措施，为构建安全的AI助手提供了通用防御蓝图。

Abstract: Large language model (LLM) assistants are increasingly integrated into
enterprise workflows, raising new security concerns as they bridge internal and
external data sources. This paper presents an in-depth case study of EchoLeak
(CVE-2025-32711), a zero-click prompt injection vulnerability in Microsoft 365
Copilot that enabled remote, unauthenticated data exfiltration via a single
crafted email. By chaining multiple bypasses-evading Microsofts XPIA (Cross
Prompt Injection Attempt) classifier, circumventing link redaction with
reference-style Markdown, exploiting auto-fetched images, and abusing a
Microsoft Teams proxy allowed by the content security policy-EchoLeak achieved
full privilege escalation across LLM trust boundaries without user interaction.
We analyze why existing defenses failed, and outline a set of engineering
mitigations including prompt partitioning, enhanced input/output filtering,
provenance-based access control, and strict content security policies. Beyond
the specific exploit, we derive generalizable lessons for building secure AI
copilots, emphasizing the principle of least privilege, defense-in-depth
architectures, and continuous adversarial testing. Our findings establish
prompt injection as a practical, high-severity vulnerability class in
production AI systems and provide a blueprint for defending against future
AI-native threats.

</details>


### [32] [Robust DDoS-Attack Classification with 3D CNNs Against Adversarial Methods](https://arxiv.org/abs/2509.10543)
*Landon Bragg,Nathan Dorsey,Josh Prior,John Ajit,Ben Kim,Nate Willis,Pablo Rivas*

Main category: cs.CR

TL;DR: 提出了一种基于蜂巢图序列和3D CNN的DDoS攻击检测方法，通过对抗训练将对抗样本准确率从50-55%提升至93%以上，并发现早期帧（3-4帧）具有强预测信号


<details>
  <summary>Details</summary>
Motivation: DDoS攻击通过微妙改变流量模式来绕过检测，现有方法在对抗样本面前表现不佳，需要开发更鲁棒的检测技术

Method: 使用时空蜂巢图编码建立模式识别基线，应用FGSM和PGD对抗训练结合空间噪声和图像平移，分析逐帧预测以发现早期信号

Result: 在基准数据集上，该方法将对抗准确率从50-55%提升至超过93%，同时保持干净样本性能，发现第3-4帧提供强预测信号

Conclusion: 基于蜂巢图序列和3D CNN的方法能有效检测DDoS攻击，对抗训练显著提升鲁棒性，早期分类是可行的

Abstract: Distributed Denial-of-Service (DDoS) attacks remain a serious threat to
online infrastructure, often bypassing detection by altering traffic in subtle
ways. We present a method using hive-plot sequences of network data and a 3D
convolutional neural network (3D CNN) to classify DDoS traffic with high
accuracy. Our system relies on three main ideas: (1) using spatio-temporal
hive-plot encodings to set a pattern-recognition baseline, (2) applying
adversarial training with FGSM and PGD alongside spatial noise and image
shifts, and (3) analyzing frame-wise predictions to find early signals. On a
benchmark dataset, our method lifts adversarial accuracy from 50-55% to over
93% while maintaining clean-sample performance. Frames 3-4 offer strong
predictive signals, showing early-stage classification is possible.

</details>


### [33] [Decentralized Identity Management on Ripple: A Conceptual Framework for High-Speed, Low-Cost Identity Transactions in Attestation-Based Attribute-Based Identity](https://arxiv.org/abs/2509.10545)
*Ruwanga Konara,Kasun De Zoysa,Asanka Sayakkara*

Main category: cs.CR

TL;DR: 本文分析了基于区块链的去中心化身份管理系统研究现状，特别关注了Ripple区块链上基于证明的属性基去中心化身份管理系统的研究空白，并提出了相关概念框架。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要集中在通用属性基去中心化身份管理系统(ABDIDM)上，而基于证明的属性基去中心化身份管理(ABABDIDM)研究相对缺乏。Ripple区块链虽然具有适合属性基身份管理的特性，但相关研究仍然空白。

Method: 通过文献综述分析现有DIDMS研究现状，特别关注不同区块链平台的应用情况，并针对Ripple区块链提出基于证明的属性基去中心化身份管理系统的概念化框架。

Result: 发现ABABDIDM研究明显不足，Ripple区块链上的身份管理系统研究几乎空白，为此提出了在Ripple上构建ABABDIDM的概念方案。

Conclusion: Ripple区块链具有开发属性基身份管理系统的潜力，但相关研究严重不足，需要更多研究来探索在这一平台上构建基于证明的属性基去中心化身份管理系统。

Abstract: Recent years have seen many industrial implementations and much scholastic
research, i.e., prototypes and theoretical frameworks, in Decentralized
Identity Management Systems (DIDMS). It is safe to say that Attestation-Based
Attribute-Based Decentralized IDM (ABABDIDM) has not received anywhere near the
same level of attention in the literature as general Attribute-Based DIDMs
(ABDIDM), i.e, decentralized Attribute-Based Access Control (ABAC). The use of
decentralization, i.e., DIDM, is to improve upon the security and
privacy-related issues of centralized Identity Management Systems (IDM) and
Attribute-Based IDMs (ABIDM). And blockchain is the framework used for
decentralization in all these schemes. Many DIDMs - even ABDIDMs - have been
defined on popular blockchains such as Hyperledger, Ethereum, and Bitcoin.
However, despite the characteristics of Ripple that makes it appealing for an
ABIDM, there is a lack of research to develop an Identity Management System
(IDMS) on Ripple in literature. We have attempted to conceptualize an ABABDIDM
on Ripple.

</details>


### [34] [Auditable Early Stopping for Agentic Routing: Ledger-Verified Run-Wise Certificates under Local DP](https://arxiv.org/abs/2509.10550)
*Shivam Akhauri*

Main category: cs.CR

TL;DR: 提出了两种模式（Exact和Surrogate）的提前停止证书机制，用于在保护本地差分隐私和保持可审计轨迹的前提下，在上下文索引前缀DAG上进行perturb-and-MAP最佳优先搜索。


<details>
  <summary>Details</summary>
Motivation: 在生产工具使用代理中，路由器需要在保护本地差分隐私和留下可审计轨迹的同时，知道何时停止探索。

Method: 使用耦合实现路径分数和修剪键到单个指数竞赛，通过偏移传播延迟实现。提供精确模式（Exact）和代理模式（Surrogate），包含编译器、本地有限性检查、SuffixCountDP例程等组件。

Result: 实现了可重现的结果，包括小型真实工具使用管道和验证器检查的审计轨迹，提供了代码和账本。

Conclusion: 提出的方法能够在保护隐私和可审计性的前提下，有效控制工具使用代理的搜索过程，并提供两种可靠的停止模式。

Abstract: In production tool-use agents (e.g., retrieval $\to$ summarization $\to$
calculator), routers must know when to stop exploring while preserving local DP
and leaving an auditable trail. We present run-wise early-stopping certificates
for perturb-and-MAP (PaM) best-first search on context-indexed prefix DAGs
whose children partition the leaves. We couple realized path scores and pruning
keys to a single exponential race realized lazily via offset propagation. With
exact leaf counts $N(v)$, lazy reuse at winners and independent residuals yield
an Exact mode with a sound halting rule based on Key$(v) = M_tau(v) - \log
t(v)$, where $t(v)$ is the minimum arrival time among leaves under $v$. With
only upper bounds $N_{ub} \ge N$, a Surrogate mode uses a parent-anchored
surrogate race without winner reuse; because $-\log \hat t \ge -\log t$, the
frontier invariant holds and stopping remains sound. We add a compiler from
shared-node DAGs to prefix DAGs, local finiteness checks, a SuffixCountDP
routine for exact counts with safe downgrades, a validator-side tightening term
$\kappa = \log(N/N_{ub})$, and an auditable ledger/validator that replays runs
deterministically. We also give an absolute LogSumExp tail bound, an acyclicity
certificate, and a fallback PRF-per-leaf scheme (NoCert) whose work matches a
realized-score best-first baseline up to a small per-node overhead. Finally, we
integrate a price/latency/$(\epsilon, \delta)$-aware multi-LLM controller and
DP-trained LoRA adapters chosen at runtime; these choices do not affect the
two-mode frontier invariants. We report Mac/commodity-hardware reproducible
results, a small real tool-use pipeline, and validator-checked audit trails,
with code and ledgers provided.

</details>


### [35] [A Hybrid Encryption Framework Combining Classical, Post-Quantum, and QKD Methods](https://arxiv.org/abs/2509.10551)
*Amal Raj,Vivek Balachandran*

Main category: cs.CR

TL;DR: 提出混合加密框架，结合经典密码学、后量子密码学和量子密钥分发技术，以应对量子计算威胁，并通过原型实现验证性能


<details>
  <summary>Details</summary>
Motivation: 应对量子计算对现有加密系统的威胁，提供从经典密码学到后量子密码学的平稳过渡方案

Method: 集成EdDSA、ECDH（经典密码学）、ML-DSA-6x5、ML-KEM-768（后量子密码学）和QKD量子密钥分发，使用密钥派生函数生成对称密钥和HMAC密钥

Result: 开发了原型系统并评估了执行时间和网络性能指标，验证了混合方案的可行性

Conclusion: 该框架结合了经典密码学的高效性、后量子密码学的量子抗性和QKD的密钥安全性，为密码系统提供了实用的过渡路径，为未来后量子密码学的应用奠定了基础

Abstract: This paper introduces a hybrid encryption framework combining classical
cryptography (EdDSA, ECDH), post-quantum cryptography (ML-DSA-6x5, ML-KEM-768),
and Quantum Key Distribution (QKD) via Guardian to counter quantum computing
threats. Our prototype implements this integration, using a key derivation
function to generate secure symmetric and HMAC keys, and evaluates its
performance across execution time and network metrics. The approach improves
data protection by merging classical efficiency with PQC's quantum resilience
and QKD's key security, offering a practical transition path for cryptographic
systems. This research lays the foundation for future adoption of PQC in
securing digital communication.

</details>


### [36] [AVEC: Bootstrapping Privacy for Local LLMs](https://arxiv.org/abs/2509.10561)
*Madhava Gaikwad*

Main category: cs.CR

TL;DR: AVEC是一个用于本地语言模型隐私保护的框架，通过边缘控制实现可验证的隐私保护，采用自适应差分隐私预算分配和可验证转换技术。


<details>
  <summary>Details</summary>
Motivation: 为了解决本地语言模型在委托查询中的隐私保护问题，需要一种能够在边缘设备上实施隐私保护并确保可验证性的框架。

Method: 提出自适应预算算法，基于查询敏感度、本地置信度和历史使用情况分配差分隐私参数；使用可验证转换和设备完整性检查；基于Rényi差分隐私和里程表式记账形式化保证。

Result: 建立了效用上限、委托泄漏边界，以及确定性门控和仅哈希认证的不可能性结果；通过模拟评估验证机制行为和记账效果。

Conclusion: AVEC提供了一个概念架构和理论基础，为本地语言模型的隐私引导提供了实证研究的路径，但目前尚未达到部署就绪状态。

Abstract: This position paper presents AVEC (Adaptive Verifiable Edge Control), a
framework for bootstrapping privacy for local language models by enforcing
privacy at the edge with explicit verifiability for delegated queries. AVEC
introduces an adaptive budgeting algorithm that allocates per-query
differential privacy parameters based on sensitivity, local confidence, and
historical usage, and uses verifiable transformation with on-device integrity
checks. We formalize guarantees using R\'enyi differential privacy with
odometer-based accounting, and establish utility ceilings, delegation-leakage
bounds, and impossibility results for deterministic gating and hash-only
certification. Our evaluation is simulation-based by design to study mechanism
behavior and accounting; we do not claim deployment readiness or task-level
utility with live LLMs. The contribution is a conceptual architecture and
theoretical foundation that chart a pathway for empirical follow-up on
privately bootstrapping local LLMs.

</details>


### [37] [Enhancing IoMT Security with Explainable Machine Learning: A Case Study on the CICIOMT2024 Dataset](https://arxiv.org/abs/2509.10563)
*Mohammed Yacoubi,Omar Moussaoui,C. Drocourt*

Main category: cs.CR

TL;DR: 这篇论文研究了在医疗物联网(IoMT)环境中使用可解释人工智能(XAI)来提升集成学习模型的可解释性，通过SHAP和LIME技术为网络安全专业人员提供攻击检测决策的解释。


<details>
  <summary>Details</summary>
Motivation: 医疗物联网环境中人工智能驱动的威胁检测模型通常是黑盒模型，网络安全专业人员需要理解AI决策的理由以确保信任和责任制，应对持续增长的网络攻击威胁。

Method: 采用集成学习技术（bagging方法的Random Forest和boosting方法的CatBoost）进行网络攻击分类，然后应用XAI模型生成局部和全局解释，使用SHAP和LIME技术来强调特征重要性。

Result: 研究展示了XAI技术如何为集成学习模型提供可解释性，帮助利益相关者理解网络威胁检测的关键因素和决策理由。

Conclusion: 通过结合集成学习模型和XAI技术，可以在保持高准确性的同时提升IoMT环境中网络安全检测模型的透明度和可解释性，从而增强专业人员对AI决策的信任和责任制。

Abstract: Explainable Artificial Intelligence (XAI) enhances the transparency and
interpretability of AI models, addressing their inherent opacity. In
cybersecurity, particularly within the Internet of Medical Things (IoMT), the
black-box nature of AI-driven threat detection poses a significant challenge.
Cybersecurity professionals must not only detect attacks but also understand
the reasoning behind AI decisions to ensure trust and accountability. The rapid
increase in cyberattacks targeting connected medical devices threatens patient
safety and data privacy, necessitating advanced AI-driven solutions. This study
compares two ensemble learning techniques, bagging and boosting, for
cyber-attack classification in IoMT environments. We selected Random Forest for
bagging and CatBoost for boosting. Random Forest helps reduce variance, while
CatBoost improves bias by combining weak classifiers into a strong ensemble
model, making them effective for detecting sophisticated attacks. However,
their complexity often reduces transparency, making it difficult for
cybersecurity professionals to interpret and trust their decisions. To address
this issue, we apply XAI models to generate local and global explanations,
providing insights into AI decision-making. Using techniques like SHAP (Shapley
Additive Explanations) and LIME (Local Interpretable Model-agnostic
Explanations), we highlight feature importance to help stakeholders understand
the key factors driving cyber threat detection.

</details>


### [38] [SG-ML: Smart Grid Cyber Range Modelling Language](https://arxiv.org/abs/2509.10568)
*Muhammad M. Roomi,Suhail S. M. Hussain,Daisuke Mashima*

Main category: cs.CR

TL;DR: SG-ML是一个基于XML的智能电网建模语言，用于自动化生成智能电网网络靶场，整合了电力系统和网络拓扑表示，支持定制化配置和自动化部署。


<details>
  <summary>Details</summary>
Motivation: 解决现有网络靶场设计方法零散、缺乏统一标准的问题，填补电力系统建模与自动化部署之间的鸿沟，为智能电网安全研究、培训和评估提供可重现的测试环境。

Method: 定义一组XML模式，集成IEC 61850 SCL和IEC 61131 PLCopen XML等标准，同时引入专有模式补充攻击注入参数等缺失功能，通过SG-ML处理器解析配置模型实例化靶场环境。

Result: 实现了可定制、可扩展的智能电网网络靶场自动化生成，支持物理和网络拓扑模拟、网络设备配置，能够重用现有资产减少建模工作量。

Conclusion: SG-ML提供了一种统一的方法论，能够自动化生成真实、可重现的智能电网网络靶场，为安全研究和培训提供了有效的工具和平台。

Abstract: This work provides a detailed specification of the Smart Grid Modelling
Language (SG-ML), which is designed for the automated generation of smart grid
cyber ranges. SG-ML is defined as a set of XML schemas that describe a smart
grid's configuration in both machine-readable and human-friendly ways, thereby
bridging the gap between system modelling and automated deployment. Unlike
prior ad-hoc approaches to cyber range design, SG-ML provides a unified
methodology that integrates both power system and cyber network
representations. The SG-ML model can be customized by users to meet specific
requirements, such as emulating physical or cyber topologies and configuring
network devices. An SG-ML Processor then parses this configured model to
instantiate the cyber range environment. The modelling language leverages
established standards like the IEC 61850 Substation Configuration Language
(SCL) and IEC 61131 PLCopen XML to define power system topology, cyber network
topology, and device configurations. This approach allows for the reuse of
existing assets, reducing the effort needed to create the SG-ML model. To
address gaps not covered by these standards such as attack injection
parameters, scenario-specific metadata, and additional network constraints,
SG-ML introduces proprietary schemas that complement standard models. Overall,
SG-ML enables reproducible, scalable, and automated generation of realistic
smart grid cyber ranges for research, training, and security assessment.

</details>


### [39] [MarkDiffusion: An Open-Source Toolkit for Generative Watermarking of Latent Diffusion Models](https://arxiv.org/abs/2509.10569)
*Leyi Pan,Sheng Guan,Zheyu Fu,Luyang Si,Zian Wang,Xuming Hu,Irwin King,Philip S. Yu,Aiwei Liu,Lijie Wen*

Main category: cs.CR

TL;DR: MarkDiffusion是一个开源Python工具包，用于潜在扩散模型的生成式水印，包含统一实现框架、可视化套件和综合评估模块。


<details>
  <summary>Details</summary>
Motivation: 帮助研究人员，提高公众对生成式水印的认识和参与度，促进共识并推动研究和应用发展。

Method: 包含三个核心组件：统一的算法集成框架、水印模式可视化展示机制、包含24种工具和8个自动化评估管道的综合评估模块。

Result: 开发了一个完整的开源工具包，支持水印算法的集成、可视化展示和标准化评估。

Conclusion: MarkDiffusion为生成式水印研究提供了全面的工具支持，有助于推动该领域的研究发展和实际应用。

Abstract: We introduce MarkDiffusion, an open-source Python toolkit for generative
watermarking of latent diffusion models. It comprises three key components: a
unified implementation framework for streamlined watermarking algorithm
integrations and user-friendly interfaces; a mechanism visualization suite that
intuitively showcases added and extracted watermark patterns to aid public
understanding; and a comprehensive evaluation module offering standard
implementations of 24 tools across three essential aspects - detectability,
robustness, and output quality - plus 8 automated evaluation pipelines. Through
MarkDiffusion, we seek to assist researchers, enhance public awareness and
engagement in generative watermarking, and promote consensus while advancing
research and applications.

</details>


### [40] [Directionality of the Voynich Script](https://arxiv.org/abs/2509.10573)
*Christophe Parisel*

Main category: cs.CR

TL;DR: 提出了一种利用n-gram困惑度不对称性来检测字符序列方向性偏差的统计方法


<details>
  <summary>Details</summary>
Motivation: Voynich手稿虽然很可能是从左到右书写的，但其底层文字或密码的阅读方向（从左到右还是从右到左）缺乏定量研究

Method: 使用n-gram困惑度不对称性的统计方法来分析字符序列的方向性偏差

Result: 该方法能够有效检测文本的阅读方向

Conclusion: 该统计方法为确定Voynich手稿等神秘文本的阅读方向提供了量化工具

Abstract: While the Voynich Manuscript was almost certainly written left-to-right
(LTR), the question whether the underlying script or cipher reads LTR or
right-to-left (RTL) has received little quantitative attention. We introduce a
statistical method that leverages n-gram perplexity asymmetry to determine
directional bias in character sequences.

</details>


### [41] [The Coding Limits of Robust Watermarking for Generative Models](https://arxiv.org/abs/2509.10577)
*Danilo Francati,Yevin Nikhel Goonatilake,Shubham Pawar,Daniele Venturi,Giuseppe Ateniese*

Main category: cs.CR

TL;DR: 本文证明了生成模型密码水印鲁棒性的尖锐阈值，发现二进制输出最多只能容忍一半比特被修改，q进制输出最多容忍(1-1/q)符号被修改。给出了达到该界限的显式构造，并通过实验验证了该阈值在实践中已经达到。


<details>
  <summary>Details</summary>
Motivation: 研究密码水印技术的鲁棒性极限，明确水印方案能够承受的最大篡改程度，为实际水印系统的设计提供理论指导。

Method: 引入无消息密钥码的编码抽象框架，形式化鲁棒水印的三个必要条件：可靠性、篡改检测和伪随机性。通过理论证明建立鲁棒性阈值，并构造达到该阈值的线性时间编码方案。

Result: 建立了密码水印鲁棒性的精确极限：二进制输出最多容忍50%比特修改，q进制输出最多容忍(1-1/q)符号修改。构造了接近该极限的实用方案，并通过实验验证了该阈值在实践中可达。

Conclusion: 完整刻画了鲁棒水印的理论极限，提供了达到该极限的构造方法，实验证明该极限在实际水印系统中已经达到，为水印技术的设计和评估提供了重要理论基础。

Abstract: We prove a sharp threshold for the robustness of cryptographic watermarking
for generative models. This is achieved by introducing a coding abstraction,
which we call messageless secret-key codes, that formalizes sufficient and
necessary requirements of robust watermarking: soundness, tamper detection, and
pseudorandomness. Thus, we establish that robustness has a precise limit: For
binary outputs no scheme can survive if more than half of the encoded bits are
modified, and for an alphabet of size q the corresponding threshold is
$(1-1/q)$ of the symbols.
  Complementing this impossibility, we give explicit constructions that meet
the bound up to a constant slack. For every ${\delta} > 0$, assuming
pseudorandom functions and access to a public counter, we build linear-time
codes that tolerate up to $(1/2)(1-{\delta})$ errors in the binary case and
$(1-1/q)(1-{\delta})$ errors in the $q$-ary case. Together with the lower
bound, these yield the maximum robustness achievable under standard
cryptographic assumptions.
  We then test experimentally whether this limit appears in practice by looking
at the recent watermarking for images of Gunn, Zhao, and Song (ICLR 2025). We
show that a simple crop and resize operation reliably flipped about half of the
latent signs and consistently prevented belief-propagation decoding from
recovering the codeword, erasing the watermark while leaving the image visually
intact.
  These results provide a complete characterization of robust watermarking,
identifying the threshold at which robustness fails, constructions that achieve
it, and an experimental confirmation that the threshold is already reached in
practice.

</details>


### [42] [Multi-channel secure communication framework for wireless IoT (MCSC-WoT): enhancing security in Internet of Things](https://arxiv.org/abs/2509.10581)
*Prokash Barman,Ratul Chowdhury,Banani Saha*

Main category: cs.CR

TL;DR: 提出了MCSC框架，结合加密协议和动态信道跳变策略，为IoT/WoT网络提供增强的安全性和降低的同步开销。


<details>
  <summary>Details</summary>
Motivation: 传统加密方法和跳频技术不足以防御中间人、干扰和重放攻击，且多信道通信系统的同步问题导致延迟和能耗增加，不适合资源受限的IoT/WoT设备。

Method: 开发了多信道安全通信(MCSC)框架，集成先进加密协议和动态信道跳变策略。

Result: MCSC在包传输率、延迟、吞吐量和能效方面表现优异，比FHSS、单信道AES和ECC方案具有更低错误率和更强抗攻击能力。

Conclusion: MCSC框架能有效保护IoT/WoT网络安全，且不影响操作性能，在各种干扰条件下验证了其效率。

Abstract: In modern smart systems, the convergence of the Internet of Things (IoT) and
Wireless of Things (WoT) have been revolutionized by offering a broad level of
wireless connectivity and communication among various devices. Hitherto, this
greater interconnectivity poses important security problems, including the
question of how to securely interconnect different networks, preserve secure
communication channels, and maintain data integrity. However, the traditional
cryptographic method and frequency hopping technique, although they provide
some protection, are not sufficient to defend against Man-In-The-Middle,
jamming, and replay attacks. In addition, synchronization issues in
multi-channel communication systems result in increased latency and energy
consumption, which make them unsuitable for resource-constrained IoT and WoT
devices. This work presents the Multi-Channel Secure Communication (MCSC)
framework, which integrates advanced cryptographic protocols with dynamic
channel-hopping strategies to enhance security with reduced synchronization
overhead. The MCSC framework maximizes the critical performance metrics, such
as packet delivery ratio, latency, throughput, and energy efficiency, and
fulfills the specific requirements of the IoT and WoT networks. A comprehensive
comparison of MCSC with well-established methods, including Frequency Hop
Spread Spectrum, single channel Advanced Encryption Standard, and various
Elliptic Curve Cryptography-based schemes, indicates that MCSC has lower error
rates and is more resilient to a wider range of cyber attacks. The efficiency
of the proposed solution to secure IoT and WoT networks without compromising
the operational performance is validated under various interference conditions.

</details>


### [43] [Safety and Security Analysis of Large Language Models: Risk Profile and Harm Potential](https://arxiv.org/abs/2509.10655)
*Charankumar Akiri,Harrison Simpson,Kshitiz Aryal,Aarav Khanna,Maanak Gupta*

Main category: cs.CR

TL;DR: 这篇论文通过实验分析了主流大语言模型在24个安全风险类别上的弱点，发现它们存在普遍的对恶意提示的故障漏洞，并提出了风险严重性指数来评估模型安全性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的普遍部署，它们受到恶意操纵和利用的漏洞可能带来严重的安全、安全和伦理风险，需要对正在发展的对抗提示技术进行风险评估。

Method: 研究对9款主流LLM模型进行实验分析，测试它们在24个安全风险类别上对恶意提示的忍受能力，包括暴力犯罪、非暴力犯罪、社会危害、非法性内容等，并创建了风险严重性指数(RSI)来量化模型安全性。

Result: 研究发现所有测试模型都存在普遍的安全过滤器漏洞，能够被恶意提示操纵产生有害回应，显示了当前LLM安全防护措施的不足。

Conclusion: 研究强调了加强模型对齐、负责任部署实践和模型治理的紧迫性，尤其是对于开放访问和快速迭代的模型，并提出RSI指数作为评估LLM风险水平的有效工具。

Abstract: While the widespread deployment of Large Language Models (LLMs) holds great
potential for society, their vulnerabilities to adversarial manipulation and
exploitation can pose serious safety, security, and ethical risks. As new
threats continue to emerge, it becomes critically necessary to assess the
landscape of LLMs' safety and security against evolving adversarial prompt
techniques. To understand the behavior of LLMs, this research provides an
empirical analysis and risk profile of nine prominent LLMs, Claude Opus 4,
DeepSeek V3 (both open-source and online), Gemini 2.5 Flash, GPT-4o, Grok 3,
Llama 4 Scout, Mistral 7B, and Qwen 3 1.7B, against 24 different security and
safety categories. These LLMs are evaluated on their ability to produce harmful
responses for adversarially crafted prompts (dataset has been made public) for
a broad range of safety and security topics, such as promotion of violent
criminal behavior, promotion of non-violent criminal activity, societal harms
related to safety, illegal sexual content, dangerous code generation, and
cybersecurity threats beyond code. Our study introduces the Risk Severity Index
(RSI), an agile and scalable evaluation score, to quantify and compare the
security posture and creating a risk profile of LLMs. As the LLM development
landscape progresses, the RSI is intended to be a valuable metric for comparing
the risks of LLMs across evolving threats. This research finds widespread
vulnerabilities in the safety filters of the LLMs tested and highlights the
urgent need for stronger alignment, responsible deployment practices, and model
governance, particularly for open-access and rapidly iterated models.

</details>


### [44] [LLM in the Middle: A Systematic Review of Threats and Mitigations to Real-World LLM-based Systems](https://arxiv.org/abs/2509.10682)
*Vitor Hugo Galhardo Moia,Igor Jochem Sanz,Gabriel Antonio Fontes Rebello,Rodrigo Duarte de Meneses,Briland Hitaj,Ulf Lindqvist*

Main category: cs.CR

TL;DR: 这篇论文系统性地调查了基于大语言模型(LLM)系统的安全隐私威胁，对威胁和防御策略进行了全面分类，涵盖整个软件和LLM生命周期。


<details>
  <summary>Details</summary>
Motivation: 生成式AI特别是LLMs的成功应用吸引了网络犯罪分子的关注，他们试图滥用模型、窃取敏感数据或破坏服务。为LLM系统提供安全保障面临巨大挑战，需要同时应对传统软件威胁和针对LLMs的新威胁。

Method: 通过系统性文献回顾和全面分类方法，分析真实场景中LLM使用的不同特征，从开发到运营全过程。威胁按严重程度和适用场景分类，防御策略按生命周期阶段和对应攻击策略进行系统分类映射。

Result: 建立了完整的LLM系统安全威胁分类框架，识别了最相关的威胁，并提供了针对性的防御策略分类，为不同生命周期阶段的威胁提供了缓解方案。

Conclusion: 这项工作为消费者和供应商理解并有效缓解LLM集成风险提供了基础，使研究社区能够从开放挑战和边缘案例的讨论中受益，促进LLM系统的安全隐私保护采用。

Abstract: The success and wide adoption of generative AI (GenAI), particularly large
language models (LLMs), has attracted the attention of cybercriminals seeking
to abuse models, steal sensitive data, or disrupt services. Moreover, providing
security to LLM-based systems is a great challenge, as both traditional threats
to software applications and threats targeting LLMs and their integration must
be mitigated. In this survey, we shed light on security and privacy concerns of
such LLM-based systems by performing a systematic review and comprehensive
categorization of threats and defensive strategies considering the entire
software and LLM life cycles. We analyze real-world scenarios with distinct
characteristics of LLM usage, spanning from development to operation. In
addition, threats are classified according to their severity level and to which
scenarios they pertain, facilitating the identification of the most relevant
threats. Recommended defense strategies are systematically categorized and
mapped to the corresponding life cycle phase and possible attack strategies
they attenuate. This work paves the way for consumers and vendors to understand
and efficiently mitigate risks during integration of LLMs in their respective
solutions or organizations. It also enables the research community to benefit
from the discussion of open challenges and edge cases that may hinder the
secure and privacy-preserving adoption of LLM-based systems.

</details>


### [45] [Privacy-Preserving Decentralized Federated Learning via Explainable Adaptive Differential Privacy](https://arxiv.org/abs/2509.10691)
*Fardin Jalil Piran,Zhiling Chen,Yang Zhang,Qianyu Zhou,Jiong Tang,Farhad Imani*

Main category: cs.CR

TL;DR: 私有化分布式联邦学习框架PrivateDFL，结合超维计算和差分隐私，通过可审计的噪声账户提高精度并降低延迟和能耗。


<details>
  <summary>Details</summary>
Motivation: 分布式联邦学习存在模型更新泄漏数据的风险，差分隐私保护需要注入噪声，但黑盒训练无法追踪历史噪声，导致过度噪声注入和精度下降。

Method: 提出PrivateDFL框架，结合超维计算和差分隐私技术，维护可审计的累积噪声记录，使得每个客户只需注入与已累积噪声的差额。

Result: 在MNIST、ISOLET和UCI-HAR数据集上，超过了传统DP-SGD和RDP方法。在非IID分布下，在MNIST上精度提高24.42%，训练时间减少10倍，推理延迟降低76倍，能耗减少1倍。在ISOLET上精度提高80%以上，训练时间减少10倍，推理延迟降低40倍，能耗减少36倍。

Conclusion: PrivateDFL框架在保持形式(ε, δ)隐私保证的前提下，实现了更高精度、更低延迟和更低能耗，且无需中央服务器。未来工作将扩展到对抗性客户和异构隐私预算的适应性拓扑。

Abstract: Decentralized federated learning faces privacy risks because model updates
can leak data through inference attacks and membership inference, a concern
that grows over many client exchanges. Differential privacy offers principled
protection by injecting calibrated noise so confidential information remains
secure on resource-limited IoT devices. Yet without transparency, black-box
training cannot track noise already injected by previous clients and rounds,
which forces worst-case additions and harms accuracy. We propose PrivateDFL, an
explainable framework that joins hyperdimensional computing with differential
privacy and keeps an auditable account of cumulative noise so each client adds
only the difference between the required noise and what has already been
accumulated. We evaluate on MNIST, ISOLET, and UCI-HAR to span image, signal,
and tabular modalities, and we benchmark against transformer-based and deep
learning-based baselines trained centrally with Differentially Private
Stochastic Gradient Descent (DP-SGD) and Renyi Differential Privacy (RDP).
PrivateDFL delivers higher accuracy, lower latency, and lower energy across IID
and non-IID partitions while preserving formal (epsilon, delta) guarantees and
operating without a central server. For example, under non-IID partitions,
PrivateDFL achieves 24.42% higher accuracy than the Vision Transformer on MNIST
while using about 10x less training time, 76x lower inference latency, and 11x
less energy, and on ISOLET it exceeds Transformer accuracy by more than 80%
with roughly 10x less training time, 40x lower inference latency, and 36x less
training energy. Future work will extend the explainable accounting to
adversarial clients and adaptive topologies with heterogeneous privacy budgets.

</details>


### [46] [Side-channel Inference of User Activities in AR/VR Using GPU Profiling](https://arxiv.org/abs/2509.10703)
*Seonghun Son,Chandrika Mukherjee,Reham Mohamed Aburas,Berk Gulmezoglu,Z. Berkay Celik*

Main category: cs.CR

TL;DR: OVRWatcher是一种针对AR/VR设备的侧信道攻击方法，通过监控低分辨率GPU使用情况来推断用户活动，无需并发应用执行或特殊权限


<details>
  <summary>Details</summary>
Motivation: 现有恶意应用监控方法在Meta Quest等AR/VR设备上不可行，因为这些设备禁止并发独立应用执行，需要新的攻击向量来保护用户隐私

Method: 开发OVRWatcher侧信道原语，通过后台脚本监控1Hz低分辨率GPU使用情况，捕捉GPU指标与3D对象交互之间的相关性

Result: 在应用指纹识别方面达到99%以上准确率，在对象级推断方面达到98%以上准确率，能够识别虚拟购物产品选择和虚拟会议参与者数量

Conclusion: 低分辨率GPU监控可以成为有效的侧信道攻击手段，揭示了AR/VR设备中新的隐私风险，需要更强的安全防护措施

Abstract: Over the past decade, AR/VR devices have drastically changed how we interact
with the digital world. Users often share sensitive information, such as their
location, browsing history, and even financial data, within third-party apps
installed on these devices, assuming a secure environment protected from
malicious actors. Recent research has revealed that malicious apps can exploit
such capabilities and monitor benign apps to track user activities, leveraging
fine-grained profiling tools, such as performance counter APIs. However,
app-to-app monitoring is not feasible on all AR/VR devices (e.g., Meta Quest),
as a concurrent standalone app execution is disabled. In this paper, we present
OVRWatcher, a novel side-channel primitive for AR/VR devices that infers user
activities by monitoring low-resolution (1Hz) GPU usage via a background
script, unlike prior work that relies on high-resolution profiling. OVRWatcher
captures correlations between GPU metrics and 3D object interactions under
varying speeds, distances, and rendering scenarios, without requiring
concurrent app execution, access to application data, or additional SDK
installations. We demonstrate the efficacy of OVRWatcher in fingerprinting both
standalone AR/VR and WebXR applications. OVRWatcher also distinguishes virtual
objects, such as products in immersive shopping apps selected by real users and
the number of participants in virtual meetings, thereby revealing users'
product preferences and potentially exposing confidential information from
those meetings. OVRWatcher achieves over 99% accuracy in app fingerprinting and
over 98% accuracy in object-level inference.

</details>


### [47] [Feature-Centric Approaches to Android Malware Analysis: A Survey](https://arxiv.org/abs/2509.10709)
*Shama Maganur,Yili Jiang,Jiaqi Huang,Fangtian Zhong*

Main category: cs.CR

TL;DR: 这篇系统性文献综述分析了中介设备平台的Android恶意软件分析方法，重点关注静态、动态、混合和图基方法的特征提取技术，以及它们在IoT安全中的应用和偏强弊。


<details>
  <summary>Details</summary>
Motivation: Android平台的开放性使得恶意软件能够渗透IoT网络，造成大规模中断、数据泄漏和拒绝服务攻击，需要有效的分析方法来保护IoT基础设施。

Method: 进行系统性文献综述(SLR)，对静态分析、动态分析、混合分析和图基方法进行详细分析和比较，重点研究特征提取技术。

Result: 静态分析效率高但易被混淆避免；动态分析对避免行为抵御力强但计算成本高；混合方法在准确性和资源使用之间取得平衡；图基方法提供优秀的语义建模和对抗精度。

Conclusion: 该研究提供了现有方法的结构化比较，曝露了研究空白，并给出了未来研究方向的路线图，以提高IoT驱动的Android恶意软件检测的可扩展性、适应性和长期安全性。

Abstract: Sophisticated malware families exploit the openness of the Android platform
to infiltrate IoT networks, enabling large-scale disruption, data exfiltration,
and denial-of-service attacks. This systematic literature review (SLR) examines
cutting-edge approaches to Android malware analysis with direct implications
for securing IoT infrastructures. We analyze feature extraction techniques
across static, dynamic, hybrid, and graph-based methods, highlighting their
trade-offs: static analysis offers efficiency but is easily evaded through
obfuscation; dynamic analysis provides stronger resistance to evasive behaviors
but incurs high computational costs, often unsuitable for lightweight IoT
devices; hybrid approaches balance accuracy with resource considerations; and
graph-based methods deliver superior semantic modeling and adversarial
robustness. This survey contributes a structured comparison of existing
methods, exposes research gaps, and outlines a roadmap for future directions to
enhance scalability, adaptability, and long-term security in IoT-driven Android
malware detection.

</details>


### [48] [Security theory for data flow and access control: From partial orders to lattices and back, a half-century trip](https://arxiv.org/abs/2509.10727)
*Luigi Logrippo*

Main category: cs.CR

TL;DR: 本文认为基于偏序理论的多级Bell La Padula模型比基于格理论的模型更适合安全数据访问控制，并通过ABAC展示了非格数据流网络的实现


<details>
  <summary>Details</summary>
Motivation: 重新评估1970年代提出的基于偏序理论的多级Bell La Padula模型，证明其比后来流行的基于格理论的模型更适合安全数据访问和数据流控制

Method: 通过理论分析和实例演示，展示如何使用基于属性的访问控制(ABAC)来实现非格数据流网络

Result: 证明了偏序模型比格模型更合适，并展示了非格数据流网络在ABAC框架下的可行性实现

Conclusion: 基于偏序理论的多级Bell La Padula模型在安全数据访问控制方面具有优势，ABAC为实现非格数据流网络提供了有效途径

Abstract: The multi level Bell La Padula model for secure data access and data flow
control, formulated in the 1970s, was based on the theory of partial orders.
Since then, another model, based on lattice theory, has prevailed. We present
reasons why the partial order model is more appropriate. We also show, by
example, how non lattice data flow networks can be easily implemented by using
Attribute-based access control (ABAC).

</details>


### [49] [Five Minutes of DDoS Brings down Tor: DDoS Attacks on the Tor Directory Protocol and Mitigations](https://arxiv.org/abs/2509.10755)
*Zhongtang Luo,Jianting Zhang,Akshat Neerati,Aniket Kate*

Main category: cs.CR

TL;DR: 通过DDoS攻击目栉多数Tor目录权威可以以每月$53.28的低成本带网络失效，但可以通过部分同步协议进行改进


<details>
  <summary>Details</summary>
Motivation: 现有Tor目录协议做了严格的同步假设，容易受到DDoS攻击影响，需要更加健壮的协议设计

Method: 设计了一种新的Tor目录协议，利用标准的部分同步共识协议，并在Rust中实现原型

Result: 在受控环境中证明了DDoS攻击的可行性，新协议在保持相似性能的同时能够抵御类似攻击

Conclusion: 部分同步协议可以有效提高Tor目录协议的容锐性，以较低成本提升网络安全性

Abstract: The Tor network offers network anonymity to its users by routing their
traffic through a sequence of relays. A group of nine directory authorities
maintains information about all available relay nodes using a distributed
directory protocol. We observe that the current protocol makes a steep
synchrony assumption, which makes it vulnerable to natural as well as
adversarial non-synchronous communication scenarios over the Internet. In this
paper, we show that it is possible to cause a failure in the Tor directory
protocol by targeting a majority of the authorities for only five minutes using
a well-executed distributed denial-of-service (DDoS) attack. We demonstrate
this attack in a controlled environment and show that it is cost-effective for
as little as \$53.28 per month to disrupt the protocol and to effectively bring
down the entire Tor network. To mitigate this problem, we consider the popular
partial synchrony assumption for the Tor directory protocol that ensures that
the protocol security is hampered even when the network delays are large and
unknown. We design a new Tor directory protocol that leverages any standard
partial-synchronous consensus protocol to solve this problem, while also
proving its security. We have implemented a prototype in Rust, demonstrating
comparable performance to the current protocol while resisting similar attacks.

</details>


### [50] [A Content-dependent Watermark for Safeguarding Image Attribution](https://arxiv.org/abs/2509.10766)
*Tong Zhou,Ruyi Ding,Gaowen Liu,Charles Fleming,Ramana Rao Kompella,Yunsi Fei,Xiaolin Xu,Shaolei Ren*

Main category: cs.CR

TL;DR: MetaSeal是一个具有密码学安全保障的内容依赖水印框架，用于保护图像归属权，防止伪造和篡改。


<details>
  <summary>Details</summary>
Motivation: 数字和AI生成图像的快速增长需要安全可验证的图像归属方法。现有水印技术容易受到伪造攻击，存在错误归属风险，可能损害AI模型开发者和数字艺术家的权益。

Method: 提出MetaSeal框架，采用内容依赖水印技术，提供密码学安全保证。设计包括：防伪造抵抗、鲁棒的自包含保护、篡改证据可视化检测。

Result: 实验表明MetaSeal能有效缓解伪造尝试，适用于自然图像和AI生成图像。

Conclusion: MetaSeal为安全图像归属建立了新标准，提供了密码学安全保障的解决方案。

Abstract: The rapid growth of digital and AI-generated images has amplified the need
for secure and verifiable methods of image attribution. While digital
watermarking offers more robust protection than metadata-based
approaches--which can be easily stripped--current watermarking techniques
remain vulnerable to forgery, creating risks of misattribution that can damage
the reputations of AI model developers and the rights of digital artists. These
vulnerabilities arise from two key issues: (1) content-agnostic watermarks,
which, once learned or leaked, can be transferred across images to fake
attribution, and (2) reliance on detector-based verification, which is
unreliable since detectors can be tricked. We present MetaSeal, a novel
framework for content-dependent watermarking with cryptographic security
guarantees to safeguard image attribution. Our design provides (1) forgery
resistance, preventing unauthorized replication and enforcing cryptographic
verification; (2) robust, self-contained protection, embedding attribution
directly into images while maintaining resilience against benign
transformations; and (3) evidence of tampering, making malicious alterations
visually detectable. Experiments demonstrate that MetaSeal effectively
mitigates forgery attempts and applies to both natural and AI-generated images,
establishing a new standard for secure image attribution.

</details>


### [51] [ORQ: Complex Analytics on Private Data with Strong Security Guarantees](https://arxiv.org/abs/2509.10793)
*Eli Baum,Sam Buxbaum,Nitin Mathai,Muhammad Faisal,Vasiliki Kalavri,Mayank Varia,John Liagouris*

Main category: cs.CR

TL;DR: ORQ是一个基于多方安全计算的隐私保护数据分析系统，能够高效执行包含多表连接和聚合的关系查询，显著降低了安全连接的计算成本。


<details>
  <summary>Details</summary>
Motivation: 传统多方安全计算(MPC)在处理关系查询特别是多表连接时存在二次计算成本问题，限制了其在大规模私有数据集分析中的应用。

Method: ORQ通过实时连接记录和应用聚合来避免结果集膨胀，构建了通用的不经意操作符、数据并行向量化查询引擎、通信优化层和数据流API。

Result: 在LAN和WAN环境下，ORQ相比现有技术显著减少了MPC执行时间，能够处理大一个数量级的数据集，并首次在完全MPC保护下完成了TPC-H Scale Factor 10基准测试。

Conclusion: ORQ系统证明了在完全隐私保护下高效分析大规模关系数据的可行性，为安全协作数据分析提供了实用解决方案。

Abstract: We present ORQ, a system that enables collaborative analysis of large private
datasets using cryptographically secure multi-party computation (MPC). ORQ
protects data against semi-honest or malicious parties and can efficiently
evaluate relational queries with multi-way joins and aggregations that have
been considered notoriously expensive under MPC. To do so, ORQ eliminates the
quadratic cost of secure joins by leveraging the fact that, in practice, the
structure of many real queries allows us to join records and apply the
aggregations "on the fly" while keeping the result size bounded. On the system
side, ORQ contributes generic oblivious operators, a data-parallel vectorized
query engine, a communication layer that amortizes MPC network costs, and a
dataflow API for expressing relational analytics -- all built from the ground
up.
  We evaluate ORQ in LAN and WAN deployments on a diverse set of workloads,
including complex queries with multiple joins and custom aggregations. When
compared to state-of-the-art solutions, ORQ significantly reduces MPC execution
times and can process one order of magnitude larger datasets. For our most
challenging workload, the full TPC-H benchmark, we report results entirely
under MPC with Scale Factor 10 -- a scale that had previously been achieved
only with information leakage or the use of trusted third parties.

</details>


### [52] [Automatic Generation of a Cryptography Misuse Taxonomy Using Large Language Models](https://arxiv.org/abs/2509.10814)
*Yang Zhang,Wenyi Ouyang,Yi Zhang,Liang Cheng,Chen Wu,Wenxin Hu*

Main category: cs.CR

TL;DR: 利用大语言模型自动检测和分类加密API滥用，构建了包含279个基础类别的新分类系统，其中36个是现有分类法未涵盖的新模式


<details>
  <summary>Details</summary>
Motivation: 现有加密API滥用检测工具依赖人工预定义的粗糖规则，无法适应实际中演化的滥用模式

Method: 使用大语言模型（GPT、Llama、Gemini、Claude）通过提示工程技术自动检测C/C++、Java、Python、Go代码中的加密API滥用，并进行层次分类

Result: 在3,492个实际软件程序数据集上验证，构建了包含279个基础滥用类别的分类系统，其中36个新类别。将11个新滥用类型集成到现有工具中，扩展了检测能力

Conclusion: 大语言模型能够有效地自动检测和分类加密API滥用，构建更全面的分类系统，为开发者和安全工具提供了更好的跟踪和理解新兴滥用模式的能力

Abstract: The prevalence of cryptographic API misuse (CAM) is compromising the
effectiveness of cryptography and in turn the security of modern systems and
applications. Despite extensive efforts to develop CAM detection tools, these
tools typically rely on a limited set of predefined rules from human-curated
knowledge. This rigid, rule-based approach hinders adaptation to evolving CAM
patterns in real practices.
  We propose leveraging large language models (LLMs), trained on publicly
available cryptography-related data, to automatically detect and classify CAMs
in real-world code to address this limitation. Our method enables the
development and continuous expansion of a CAM taxonomy, supporting developers
and detection tools in tracking and understanding emerging CAM patterns.
Specifically, we develop an LLM-agnostic prompt engineering method to guide
LLMs in detecting CAM instances from C/C++, Java, Python, and Go code, and then
classifying them into a hierarchical taxonomy.
  Using a data set of 3,492 real-world software programs, we demonstrate the
effectiveness of our approach with mainstream LLMs, including GPT, Llama,
Gemini, and Claude. It also allows us to quantitatively measure and compare the
performance of these LLMs in analyzing CAM in realistic code. Our evaluation
produced a taxonomy with 279 base CAM categories, 36 of which are not addressed
by existing taxonomies. To validate its practical value, we encode 11 newly
identified CAM types into detection rules and integrate them into existing
tools. Experiments show that such integration expands the tools' detection
capabilities.

</details>


### [53] [From Paradigm Shift to Audit Rift: Exploring Vulnerabilities and Audit Tips for TON Smart Contracts](https://arxiv.org/abs/2509.10823)
*Yury Yanovich,Sergey Sobolev,Yash Madhwal,Kirill Ziborov,Vladimir Gorgadze,Victoria Kovalevskay,Elizaveta Smirnova,Matvey Mishuris,Subodh Sharma*

Main category: cs.CR

TL;DR: 这篇论文为TON智能合约提供了一个基于233个真实漏洞分析的全面审计检查清单，以应对异步执行模型带来的安全挑战。


<details>
  <summary>Details</summary>
Motivation: TON高性能区块链设计带来了智能合约开发和安全性新挑战，需要从以太坊成熟审计方法迁移到TON生态系统。

Method: 分析34份专业审计报告中的233个真实漏洞，开发了一个综合性的审计检查清单，包括异步消息处理等TON特有问题。

Result: 识别了TON智能合约的具体漏洞类型，提供了详细案例研究和实践建议，为开发者和审计人员提供系统化的安全检查方法。

Conclusion: 该检查清单有助于系统性地发现和减轻漏洞，提升TON基础项目的安全性和可靠性，促进坚固的区块链环境发展。

Abstract: The Open Network (TON) is a high-performance blockchain platform designed for
scalability and efficiency, leveraging an asynchronous execution model and a
multi-layered architecture. While TON's design offers significant advantages,
it also introduces unique challenges for smart contract development and
security. This paper introduces a comprehensive audit checklist for TON smart
contracts, based on an analysis of 34 professional audit reports containing 233
real-world vulnerabilities. The checklist addresses TON-specific challenges,
such as asynchronous message handling, and provides actionable insights for
developers and auditors. We also present detailed case studies of
vulnerabilities in TON smart contracts, highlighting their implications and
offering lessons learned. By adopting this checklist, developers and auditors
can systematically identify and mitigate vulnerabilities, enhancing the
security and reliability of TON-based projects. Our work bridges the gap
between Ethereum's mature audit methodologies and the emerging needs of the TON
ecosystem, fostering a more secure and robust blockchain environment.

</details>


### [54] [A Comparison of Selected Image Transformation Techniques for Malware Classification](https://arxiv.org/abs/2509.10838)
*Rishit Agrawal,Kunal Bhatnagar,Andrew Do,Ronnit Rana,Mark Stamp*

Main category: cs.CR

TL;DR: 这篇论文实验了八种不同的恶意软件转图像技术，发现虽然转换方法差异较大，但分类效果相似，说明图像分析技术的效果更多取决于其本身强大性，而非软件转图像的具体细节。


<details>
  <summary>Details</summary>
Motivation: 当前恶意软件研究中图像基于的机器学习技术效果显著，但缺乏标准化的软件转图像方法，文献中的方法多为随机或者没有充分考虑可执行文件特性。

Method: 实验了八种不同的恶意软件转换为图像的技术，并对每种技术测试多种学习模型的效果。

Result: 发现多种图像转换技术虽然过程差异较大，但在各种学习模型下表现相似，分类效果没有显著差异。

Conclusion: 图像基于的恶意软件分类技术的效果更多取决于图像分析技术本身的强大性，而非软件转换为图像的具体方法细节。

Abstract: Recently, a considerable amount of malware research has focused on the use of
powerful image-based machine learning techniques, which generally yield
impressive results. However, before image-based techniques can be applied to
malware, the samples must be converted to images, and there is no
generally-accepted approach for doing so. The malware-to-image conversion
strategies found in the literature often appear to be ad hoc, with little or no
effort made to take into account properties of executable files. In this paper,
we experiment with eight distinct malware-to-image conversion techniques, and
for each, we test a variety of learning models. We find that several of these
image conversion techniques perform similarly across a range of learning
models, in spite of the image conversion processes being quite different. These
results suggest that the effectiveness of image-based malware classification
techniques may depend more on the inherent strengths of image analysis
techniques, as opposed to the precise details of the image conversion strategy.

</details>


### [55] [Large Language Models for Security Operations Centers: A Comprehensive Survey](https://arxiv.org/abs/2509.10858)
*Ali Habibzadeh,Farid Feyzi,Reza Ebrahimi Atani*

Main category: cs.CR

TL;DR: 这篇论文是首个系统性研究大语言模型在安全运营中心中应用的综述，分析了LLM如何解决SOC面临的高警报量、资源有限等挑战，以及未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 安全运营中心作为网络安全前线，面临着高警报量、资源有限、专家短缺、响应延迟等挑战，需要新技术来提升效率和准确性。

Method: 系统性调查研究，从结构化视角分析生成式AI和LLM在SOC工作流程中的集成能力、挑战和未来方向。

Result: 研究评估了LLM在自动化日志分析、简化分流、提高检测准确性、快速获取威胁智能知识等方面的潜力。

Conclusion: 这份综述为研究人员和SOC管理者提供了完整的当前研究状态概览，显示了LLM在网络安全领域的重要应用价值和发展前景。

Abstract: Large Language Models (LLMs) have emerged as powerful tools capable of
understanding and generating human-like text, offering transformative potential
across diverse domains. The Security Operations Center (SOC), responsible for
safeguarding digital infrastructure, represents one of these domains. SOCs
serve as the frontline of defense in cybersecurity, tasked with continuous
monitoring, detection, and response to incidents. However, SOCs face persistent
challenges such as high alert volumes, limited resources, high demand for
experts with advanced knowledge, delayed response times, and difficulties in
leveraging threat intelligence effectively. In this context, LLMs can offer
promising solutions by automating log analysis, streamlining triage, improving
detection accuracy, and providing the required knowledge in less time. This
survey systematically explores the integration of generative AI and more
specifically LLMs into SOC workflow, providing a structured perspective on its
capabilities, challenges, and future directions. We believe that this survey
offers researchers and SOC managers a broad overview of the current state of
LLM integration within academic study. To the best of our knowledge, this is
the first comprehensive study to examine LLM applications in SOCs in details.

</details>


### [56] [Finding SSH Strict Key Exchange Violations by State Learning](https://arxiv.org/abs/2509.10895)
*Fabian Bäumer,Marcel Maehren,Marcus Brinkmann,Jörg Schwenk*

Main category: cs.CR

TL;DR: 分析SSH严格密钥交换(strict KEX)实现的安全性，发现多个实现违反规范并存在严重安全漏洞，包括Tectia SSH的恶意会话攻击和Erlang SSH的远程代码执行漏洞。


<details>
  <summary>Details</summary>
Motivation: SSH协议在USENIX 2024年遭受Terrapin攻击后，厂商采用strict KEX作为防御措施。本研究旨在验证strict KEX在实际实现中的安全性和合规性。

Method: 使用黑盒状态学习技术分析10个支持strict KEX的SSH实现，针对5种密钥交换算法共学习33个状态机，检查实现是否符合规范。

Result: 发现7个实现违反strict KEX规范，识别出2个关键安全漏洞：Tectia SSH的恶意会话攻击和Erlang SSH的远程代码执行漏洞。

Conclusion: 尽管strict KEX设计上能防御Terrapin攻击，但实际实现存在严重安全问题，需要更严格的验证和测试来确保SSH协议的安全性。

Abstract: SSH is an important protocol for secure remote shell access to servers on the
Internet. At USENIX 2024, B\"aumer et al. presented the Terrapin attack on SSH,
which relies on the attacker injecting optional messages during the key
exchange. To mitigate this attack, SSH vendors adopted an extension developed
by OpenSSH called strict key exchange ("strict KEX"). With strict KEX, optional
messages are forbidden during the handshake, preventing the attack. In
practice, this should simplify the state machine of an SSH handshake to a
linear message flow similar to that of TLS.
  In this work, we analyze the design, implementation, and security of strict
KEX in popular SSH servers, using black-box state learning, which can uncover
the hidden state machine of an implementation. In practice, it is limited by
the number of learned messages and the complexity of the state machine. Thus,
learning the complete state machine of SSH is infeasible. Previous research on
SSH, therefore, excluded optional messages, learning only a partial state
machine. However, these messages are a critical part of the Terrapin attack. We
propose to instead learn the complete state machine of the handshake phase of
an SSH server, but with strict KEX enabled.
  We investigate the security of ten SSH implementations supporting strict KEX
for up to five key exchange algorithms. In total, we learn 33 state machines,
revealing significant differences in the implementations. We show that seven
implementations violate the strict KEX specification and find two critical
security vulnerabilities. One results in a rogue session attack in the
proprietary Tectia SSH implementation. Another affects the official SSH
implementation of the Erlang Open Telecom Platform, and enables unauthenticated
remote code execution in the security context of the SSH server.

</details>


### [57] [A Range-Based Sharding (RBS) Protocol for Scalable Enterprise Blockchain](https://arxiv.org/abs/2509.11006)
*M. Z. Haider,M. Dias de Assuncao,Kaiwen Zhang*

Main category: cs.CR

TL;DR: 基于范围的分片协议(RBS)，为企业区块链提供高效的分片解决方案，通过提高吞吐量和降低延迟来解决可扩展性问题


<details>
  <summary>Details</summary>
Motivation: 区块链技术在企业环境中遇到可扩展性挑战，现有分片方案在协调分片、故障容耗和共识开销方面存在缺陷

Method: 提出范围基分片协议(RBS)，采用提交-曝光方案进行安全分片分配，在Quorum上实现，通过均衡计算负载和减少跨分片交易延迟来提高性能

Result: 实验评估显示RBS在吞吐量和延迟方面显著优于现有企业分片框架，适合大规模区块链部署

Conclusion: RBS协议为企业区块链提供了一种高效、可扩展的分片解决方案，有效解决了现有方案的性能瓶颈问题

Abstract: Blockchain technology offers decentralization and security but struggles with
scalability, particularly in enterprise settings where efficiency and
controlled access are paramount. Sharding is a promising solution for private
blockchains, yet existing approaches face challenges in coordinating shards,
ensuring fault tolerance with limited nodes, and minimizing the high overhead
of consensus mechanisms like PBFT. This paper proposes the Range-Based Sharding
(RBS) Protocol, a novel sharding mechanism tailored for enterprise blockchains,
implemented on Quorum. Unlike traditional sharding models such as OmniLedger
and non-sharding Corda framework, RBS employs a commit-reveal scheme for secure
and unbiased shard allocation, ensuring fair validator distribution while
reducing cross-shard transaction delays. Our approach enhances scalability by
balancing computational loads across shards, reducing consensus overhead, and
improving parallel transaction execution. Experimental evaluations demonstrate
that RBS achieves significantly higher throughput and lower latency compared to
existing enterprise sharding frameworks, making it a viable and efficient
solution for largescale blockchain deployments.

</details>


### [58] [SoK: How Sensor Attacks Disrupt Autonomous Vehicles: An End-to-end Analysis, Challenges, and Missed Threats](https://arxiv.org/abs/2509.11120)
*Qingzhao Zhang,Shaocheng Luo,Z. Morley Mao,Miroslav Pajic,Michael K. Reiter*

Main category: cs.CR

TL;DR: 这篇论文系统性评估了自主驾驶车轨道中传感器攻击的可行性，提出SEPG模型来分析错误传播，发现了11个新攻击渠道，并使用LLM自动化安全评估。


<details>
  <summary>Details</summary>
Motivation: 自主驾驶车依赖复杂传感器系统确保安全，但实际系统中传感器攻击的可行性与影响仍不明确，需要系统性分析错误传播机制。

Method: 构建系统错误传播图(SEPG)模型，系统调研各种平台、传感器模态和攻击方法，通过原型实验验证发现，并使用大语言模型自动化SEPG构建。

Result: 提炼出7个关于传感器攻击可行性的关键发现，发现11个之前被忽视的利用模块交互的攻击渠道，多个通过原型实验验证。

Conclusion: SEPG模型能够有效分析传感器攻击在自主系统中的传播机制，大语言模型在安全评估自动化方面具有潜力，为自主驾驶安全提供了新的分析框架。

Abstract: Autonomous vehicles, including self-driving cars, robotic ground vehicles,
and drones, rely on complex sensor pipelines to ensure safe and reliable
operation. However, these safety-critical systems remain vulnerable to
adversarial sensor attacks that can compromise their performance and mission
success. While extensive research has demonstrated various sensor attack
techniques, critical gaps remain in understanding their feasibility in
real-world, end-to-end systems. This gap largely stems from the lack of a
systematic perspective on how sensor errors propagate through interconnected
modules in autonomous systems when autonomous vehicles interact with the
physical world.
  To bridge this gap, we present a comprehensive survey of autonomous vehicle
sensor attacks across platforms, sensor modalities, and attack methods. Central
to our analysis is the System Error Propagation Graph (SEPG), a structured
demonstration tool that illustrates how sensor attacks propagate through system
pipelines, exposing the conditions and dependencies that determine attack
feasibility. With the aid of SEPG, our study distills seven key findings that
highlight the feasibility challenges of sensor attacks and uncovers eleven
previously overlooked attack vectors exploiting inter-module interactions,
several of which we validate through proof-of-concept experiments.
Additionally, we demonstrate how large language models (LLMs) can automate
aspects of SEPG construction and cross-validate expert analysis, showcasing the
promise of AI-assisted security evaluation.

</details>


### [59] [Your Compiler is Backdooring Your Model: Understanding and Exploiting Compilation Inconsistency Vulnerabilities in Deep Learning Compilers](https://arxiv.org/abs/2509.11173)
*Simin Chen,Jinjun Peng,Yixin He,Junfeng Yang,Baishakhi Ray*

Main category: cs.CR

TL;DR: 深度学习编译器存在根本性安全漏洞，能够在编译过程中静默改变模型语义并植入隐藏后门，攻击成功率100%且无法被现有检测器发现


<details>
  <summary>Details</summary>
Motivation: 揭示深度学习编译器设计中的固有安全风险，研究官方未修改的编译器是否能在编译过程中改变模型语义并引入隐藏后门

Method: 研究对抗性和自然两种场景：对抗场景下制作良性模型，使触发器在编译前无效但在编译后成为有效后门；自然场景下分析HuggingFace前100个模型寻找自然触发器

Result: 在6个模型、3个商业编译器、2个硬件平台上测试，攻击对触发输入成功率100%，保持正常精度且不被最先进检测器发现；发现31个模型存在自然触发器

Conclusion: 深度学习编译器即使未经对抗操控也能引入风险，揭示了被忽视的安全威胁，为安全可信机器学习开辟了新研究方向

Abstract: Deep learning (DL) compilers are core infrastructure in modern DL systems,
offering flexibility and scalability beyond vendor-specific libraries. This
work uncovers a fundamental vulnerability in their design: can an official,
unmodified compiler alter a model's semantics during compilation and introduce
hidden backdoors? We study both adversarial and natural settings. In the
adversarial case, we craft benign models where triggers have no effect
pre-compilation but become effective backdoors after compilation. Tested on six
models, three commercial compilers, and two hardware platforms, our attack
yields 100% success on triggered inputs while preserving normal accuracy and
remaining undetected by state-of-the-art detectors. The attack generalizes
across compilers, hardware, and floating-point settings. In the natural
setting, we analyze the top 100 HuggingFace models (including one with 220M+
downloads) and find natural triggers in 31 models. This shows that compilers
can introduce risks even without adversarial manipulation.
  Our results reveal an overlooked threat: unmodified DL compilers can silently
alter model semantics. To our knowledge, this is the first work to expose
inherent security risks in DL compiler design, opening a new direction for
secure and trustworthy ML.

</details>


### [60] [ODoQ: Oblivious DNS-over-QUIC](https://arxiv.org/abs/2509.11123)
*Aditya Kulkarni,Tamal Das,Vivek Balachandran*

Main category: cs.CR

TL;DR: 提出ODoQ协议，结合QUIC协议和代理服务器，在保护用户隐私的同时降低DNS解析延迟


<details>
  <summary>Details</summary>
Motivation: 现有DNS隐私协议要么侧重用户隐私保护（如Oblivious DNS），要么侧重降低延迟（如DNS-over-QUIC），缺乏同时实现两者的单一协议

Method: 设计Oblivious DNS-over-QUIC (ODoQ)协议，利用QUIC协议的优势并引入中间代理服务器来保护客户端身份不被递归解析器暴露

Result: 成功提出了一个能够同时保护用户隐私和降低解析延迟的DNS协议方案

Conclusion: ODoQ协议有效解决了DNS隐私保护与性能优化的双重需求，为DNS协议发展提供了新的方向

Abstract: The Domain Name System (DNS), which converts domain names to their respective
IP addresses, has advanced enhancements aimed at safeguarding DNS data and
users' identity from attackers. The recent privacy-focused advancements have
enabled the IETF to standardize several protocols. Nevertheless, these
protocols tend to focus on either strengthening user privacy (like Oblivious
DNS and Oblivious DNS-over-HTTPS) or reducing resolution latency (as
demonstrated by DNS-over-QUIC). Achieving both within a single protocol remains
a key challenge, which we address in this paper. Our proposed protocol --
'Oblivious DNS-over-QUIC' (ODoQ) -- leverages the benefits of the QUIC protocol
and incorporates an intermediary proxy server to protect the client's identity
from exposure to the recursive resolver.

</details>


### [61] [A Holistic Approach to E-Commerce Innovation: Redefining Security and User Experience](https://arxiv.org/abs/2509.11712)
*Mohammad Olid Ali Akash,Priyangana Saha*

Main category: cs.CR

TL;DR: 这项研究提出了一种新的电子商务平台，通过直观界面和强大安全措施解决Android购物应用的复杂性和安全问题。


<details>
  <summary>Details</summary>
Motivation: 现代电子商务应用面临界面复杂导致用户扰惱、支付选择有限和认证机制弱等挑战，需要提供更简单安全的购物体验。

Method: 开发新的电商平台，采用直观界面设计、优化产品分类和高效结账流程，并集成Google认证和SSL加密支付网关等安全措施。

Result: 该平台能够重构电商用户体验，提供更简单安全的在线购物环境，为未来发展打下基础。

Conclusion: 通过重点关注用户友好性、安全性和个性化需求，可以提升电子商务平台的竞争力，满足现代用户的期望。

Abstract: In the modern, fast-moving world of e-commerce, many Android apps face
challenges in providing a simple and secure shopping experience. Many of these
apps, often enough, have complicated designs that prevent users from finding
what they want quickly, thus frustrating them and wasting their precious time.
Another major issue is that of security; with the limitation of payment options
and weak authentication mechanisms, users' sensitive information can be
compromised. This research presents a new e-commerce platform that responds to
the above challenges with an intuitive interface and strong security measures.
The platform makes online shopping easy with well-organized categories of
products and a fast, efficient checkout process. It also gives priority to
security by incorporating features such as Google authentication and
SSL-secured payment gateways to protect user data and ensure secure
transactions. This paper discusses how a focus on user-friendliness, security,
and personalization steps up the game for e-commerce platforms, providing
workable frameworks that match modern user needs and expectations. The findings
show the e-commerce user experience can be remodelled by the platform, hence
opening ways for future developments in that respect.

</details>


### [62] [Cryptanalysis and design for a family of plaintext non-delayed chaotic ciphers](https://arxiv.org/abs/2509.11158)
*Qianxue Wang,Simin Yu*

Main category: cs.CR

TL;DR: 本文通过四种攻击方法破解了一种典型的三阶段置换-渌散-置换PNDCC密码，证明统计指标不能保证安全性，并提出了改进的PDCC方案。


<details>
  <summary>Details</summary>
Motivation: 对平文无延迟混涌动密码(PNDCC)的设计理念和实践安全性进行重新审视，因为现有文献中缺乏严格的密码学证明而仅依靠统计测试。

Method: 提出了四种攻击方法：基于均质操作的差分攻击、S-PTC攻击、新题冲击-步进基于差分攻击(ISBDA)和链式攻击，并建立了多阶段PNDCC的数学模型。

Result: 成功破解了所有统计指标都表现良好的PNDCC密码，证明统计标准不是安全性的充分条件，链式攻击能够破解一类多阶段PNDCC。

Conclusion: 建议采用平文延迟混涌动密码(PDCC)作为安全改进方案，能够抵御各种密码分析攻击。

Abstract: Plaintext non-delayed chaotic cipher (PNDCC) means that in the diffusion
equation, plaintext has no delay terms while ciphertext has a feedback term. In
existing literature, chaotic cipher diffusions invariably take this form. Since
its introduction, PNDCC has attracted attention but also doubts. Designers of
chaotic ciphers usually claim PNDCC security by statistical tests, while
rigorous cryptographic proofs are absent. Thus, it is necessary to re-examine
its design rationale and empirical security. To address this issue, we present
a typical example of a three-stage permutation-diffusion-permutation PNDCC,
which contains multiple security vulnerabilities. Although all of its
statistical indicators show good performance, we are able to break it using
four different attacks. The first is a differential attack based on homogeneous
operations; the second is an S-PTC attack; the third is a novel
impulse-step-based differential attack (ISBDA), proposed in this paper, and the
fourth is a novel chain attack, also introduced here. These results demonstrate
that the fulfilment of statistical criteria is not a sufficient condition for
the security of PNDCC. Then, based on a mathematical model of multi-stage
PNDCC, we show that the proposed chain attack can successfully break a class of
multi-stage PNDCCs. The key technique of the chain attack depends on how to
reveal all permutations. To address this key problem, we summarize the chaining
rules and show that, from the attacker's perspective, if the same decryption
chain can be reconstructed then all permutations can be deciphered. To that
end, the entire diffusion process can be broken by solving a system of
simultaneous equations. Finally, as a secure improvement, we propose a new
scheme termed plaintext-delayed chaotic cipher (PDCC) that can resist various
cryptanalytic attacks.

</details>


### [63] [A Practical Adversarial Attack against Sequence-based Deep Learning Malware Classifiers](https://arxiv.org/abs/2509.11836)
*Kai Tan,Dongyang Zhan,Lin Ye,Hongli Zhang,Binxing Fang*

Main category: cs.CR

TL;DR: 基于深度Q网络和启应回溯搜索的对执攻击方法，能够生成符合实际条件的干扰序列，通过源代码映射避免直接修改行为序列，有效避免恶意软件检测模型


<details>
  <summary>Details</summary>
Motivation: 现有的对执样本生成方法通过删除、替换关键行为或插入良性行为来改变序列特征，但这些直接操控序列的方法导致对执样本难以在实践中实施或应用

Method: 使用深度Q网络(Deep Q-Network)和启应回溯搜索策略来生成符合实际条件的干扰序列，然后通过新题的转换方法将修改映射回源代码，避免直接修改行为日志序列

Result: 方法能够从真实世界的恶意软件行为序列生成对执样本，在避免异常检测模型时具有高成功率

Conclusion: 该方法不仅有效，而且实用，能够在保持修改软件功能的同时生成对执样本

Abstract: Sequence-based deep learning models (e.g., RNNs), can detect malware by
analyzing its behavioral sequences. Meanwhile, these models are susceptible to
adversarial attacks. Attackers can create adversarial samples that alter the
sequence characteristics of behavior sequences to deceive malware classifiers.
The existing methods for generating adversarial samples typically involve
deleting or replacing crucial behaviors in the original data sequences, or
inserting benign behaviors that may violate the behavior constraints. However,
these methods that directly manipulate sequences make adversarial samples
difficult to implement or apply in practice. In this paper, we propose an
adversarial attack approach based on Deep Q-Network and a heuristic
backtracking search strategy, which can generate perturbation sequences that
satisfy practical conditions for successful attacks. Subsequently, we utilize a
novel transformation approach that maps modifications back to the source code,
thereby avoiding the need to directly modify the behavior log sequences. We
conduct an evaluation of our approach, and the results confirm its
effectiveness in generating adversarial samples from real-world malware
behavior sequences, which have a high success rate in evading anomaly detection
models. Furthermore, our approach is practical and can generate adversarial
samples while maintaining the functionality of the modified software.

</details>


### [64] [DMLDroid: Deep Multimodal Fusion Framework for Android Malware Detection with Resilience to Code Obfuscation and Adversarial Perturbations](https://arxiv.org/abs/2509.11187)
*Doan Minh Trung,Tien Duc Anh Hao,Luong Hoang Minh,Nghi Hoang Khoa,Nguyen Tan Cam,Van-Hau Pham,Phan The Duy*

Main category: cs.CR

TL;DR: DMLDroid是一个基于多模态融合的Android恶意软件检测系统，结合权限意图、DEX文件图像和API调用序列三种特征表示，通过动态加权融合机制在检测准确性和抗混淆/对抗攻击鲁棒性方面都表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的Android恶意软件检测方法（基于字符串、图像和图）在真实环境中面对代码混淆和对抗样本时鲁棒性不足，多模态学习虽然前景广阔但缺乏系统性研究。

Method: 提出DMLDroid多模态融合检测框架，整合三种特征：权限意图（表格型）、DEX文件表示（图像型）、API调用（图衍生序列型），采用不同的融合策略进行实验。

Result: 在CICMalDroid 2020数据集上，动态加权融合机制达到97.98%准确率和98.67% F1分数，在混淆和对抗攻击场景下仍保持98%以上的准确率和F1分数。

Conclusion: 多模态融合能显著提升Android恶意软件检测的准确性和鲁棒性，有效应对不断演变的威胁。

Abstract: In recent years, learning-based Android malware detection has seen
significant advancements, with detectors generally falling into three
categories: string-based, image-based, and graph-based approaches. While these
methods have shown strong detection performance, they often struggle to sustain
robustness in real-world settings, particularly when facing code obfuscation
and adversarial examples (AEs). Deep multimodal learning has emerged as a
promising solution, leveraging the strengths of multiple feature types to
enhance robustness and generalization. However, a systematic investigation of
multimodal fusion for both accuracy and resilience remains underexplored. In
this study, we propose DMLDroid, an Android malware detection based on
multimodal fusion that leverages three different representations of malware
features, including permissions & intents (tabular-based), DEX file
representations (image-based), and API calls (graph-derived sequence-based). We
conduct exhaustive experiments independently on each feature, as well as in
combination, using different fusion strategies. Experimental results on the
CICMalDroid 2020 dataset demonstrate that our multimodal approach with the
dynamic weighted fusion mechanism achieves high performance, reaching 97.98%
accuracy and 98.67% F1-score on original malware detection. Notably, the
proposed method maintains strong robustness, sustaining over 98% accuracy and
98% F1-score under both obfuscation and adversarial attack scenarios. Our
findings highlight the benefits of multimodal fusion in improving both
detection accuracy and robustness against evolving Android malware threats.

</details>


### [65] [Implementation of Learning with Errors in Non-Commuting Multiplicative Groups](https://arxiv.org/abs/2509.11237)
*Aleksejus Mihalkovič,Lina Dindiene,Eligijus Sakalauskas*

Main category: cs.CR

TL;DR: 将LWE推广到非交换的模最大循环群M2t，利用其双循环结构构造消息恢复准则，实现Regev原始思想并获得非交换性优势，安全性与原方案相当


<details>
  <summary>Details</summary>
Motivation: 将学习错误问题(LWE)从交换群推广到非交换群，利用非交换群的特殊结构来获得密码学优势

Method: 使用模最大循环群M2t（具有两个最大乘法阶循环），构造消息恢复准则，在非交换群中实现Regev的原始加密思想

Result: 成功在非交换群M2t中实现了LWE的推广，能够以压倒性概率恢复消息比特

Conclusion: 该方法在非交换群中实现了与原始LWE方案相当的安全级别，同时利用了非交换性的优势

Abstract: In this paper, we demonstrate a way to generalize learning with errors (LWE)
to the family of so-called modular-maximal cyclic groups which are
non-commuting. Since the group M2t has two cycles of maximal multiplicative
order, we use this fact to construct an accurate criterion for restoring the
message bit with overwhelming probability. Furthermore, we implement the
original idea by O. Regev in the considered group to gain benefits from the
non-commutativity of M2t . Also we prove that using this approach we can
achieve a level of security comparable to the original idea.

</details>


### [66] [Exploring and Exploiting the Resource Isolation Attack Surface of WebAssembly Containers](https://arxiv.org/abs/2509.11242)
*Zhaofeng Yu,Dongyang Zhan,Lin Ye,Haining Yu,Hongli Zhang,Zhihong Tian*

Main category: cs.CR

TL;DR: 本文系统性分析WebAssembly运行时的资源隔离漏洞，通过静态分析方法发现WASI/WASIX接口存在资源消耗攻击风险，并提出了相应的利用策略和缓解方案。


<details>
  <summary>Details</summary>
Motivation: 虽然WebAssembly被设计为更安全的容器运行时，但当前运行时在资源隔离方面存在缺陷，攻击者可通过WASI/WASIX接口消耗主机资源干扰其他容器实例的运行，这个攻击面尚未得到充分探索和量化。

Method: 提出多种静态Wasm运行时分析方法，系统性探索资源隔离攻击面，基于分析结果提出多种利用策略来破坏Wasm运行时的资源隔离。

Result: 实验结果显示恶意Wasm实例不仅能够消耗大量系统资源，还能给底层操作系统其他组件带来高负载，导致整个系统性能显著降低。

Conclusion: 本文揭示了Wasm运行时在资源隔离方面的安全漏洞，并讨论了缓解方案，为提升Wasm容器安全性提供了重要参考。

Abstract: Recently, the WebAssembly (or Wasm) technology has been rapidly evolving,
with many runtimes actively under development, providing cross-platform secure
sandboxes for Wasm modules to run as portable containers. Compared with Docker,
which isolates applications at the operating system level, Wasm runtimes
provide more security mechanisms, such as linear memory, type checking, and
protected call stacks. Although Wasm is designed with security in mind and
considered to be a more secure container runtime, various security challenges
have arisen, and researchers have focused on the security of Wasm runtimes,
such as discovering vulnerabilities or proposing new security mechanisms to
achieve robust isolation. However, we have observed that the resource isolation
is not well protected by the current Wasm runtimes, and attackers can exhaust
the host's resources to interfere with the execution of other container
instances by exploiting the WASI/WASIX interfaces. And the attack surface has
not been well explored and measured. In this paper, we explore the resource
isolation attack surface of Wasm runtimes systematically by proposing several
static Wasm runtime analysis approaches. Based on the analysis results, we
propose several exploitation strategies to break the resource isolation of Wasm
runtimes. The experimental results show that malicious Wasm instances can not
only consume large amounts of system resources on their own but also introduce
high workloads into other components of the underlying operating system,
leading to a substantial performance degradation of the whole system. In
addition, the mitigation approaches have also been discussed.

</details>


### [67] [Make Identity Unextractable yet Perceptible: Synthesis-Based Privacy Protection for Subject Faces in Photos](https://arxiv.org/abs/2509.11249)
*Tao Wang,Yushu Zhang,Xiangli Xiao,Kun Xu,Lin Yuan,Wenying Wen,Yuming Fang*

Main category: cs.CR

TL;DR: 这篇论文提出了PerceptFace方法，通过合成技术保护人脸隐私，在阻止深度学习识别的同时保持熟悉者的人脸识别感知能力。


<details>
  <summary>Details</summary>
Motivation: 传统的批动基础的反人脸识别方法存在隐私保护的假象，需要找到更可靠的新方案来保护照片中的人脸隐私。

Method: 提出PerceptFace方法，利用合成技术改变人脸图像，设计了新的感知相似性损失函数，减少在人眼敏感区域的改动，使得识别无法被机器提取但仍可以被熟悉者感知。

Result: 实验结果显示PerceptFace在识别保护和识别感知之间实现了更优的平衡，较现有方法更加高效。

Conclusion: PerceptFace作为首个专门用于主体人脸保护的合成方法，具有成为实用反人脸识别工具的很大潜力，并提供了公开API。

Abstract: Deep learning-based face recognition (FR) technology exacerbates privacy
concerns in photo sharing. In response, the research community developed a
suite of anti-FR methods to block identity extraction by unauthorized FR
systems. Benefiting from quasi-imperceptible alteration, perturbation-based
methods are well-suited for privacy protection of subject faces in photos, as
they allow familiar persons to recognize subjects via naked eyes. However, we
reveal that perturbation-based methods provide a false sense of privacy through
theoretical analysis and experimental validation.
  Therefore, new alternative solutions should be found to protect subject
faces. In this paper, we explore synthesis-based methods as a promising
solution, whose challenge is to enable familiar persons to recognize subjects.
To solve the challenge, we present a key insight: In most photo sharing
scenarios, familiar persons recognize subjects through identity perception
rather than meticulous face analysis. Based on the insight, we propose the
first synthesis-based method dedicated to subject faces, i.e., PerceptFace,
which can make identity unextractable yet perceptible. To enhance identity
perception, a new perceptual similarity loss is designed for faces, reducing
the alteration in regions of high sensitivity to human vision.
  As a synthesis-based method, PerceptFace can inherently provide reliable
identity protection. Meanwhile, out of the confine of meticulous face analysis,
PerceptFace focuses on identity perception from a more practical scenario,
which is also enhanced by the designed perceptual similarity loss. Sufficient
experiments show that PerceptFace achieves a superior trade-off between
identity protection and identity perception compared to existing methods. We
provide a public API of PerceptFace and believe that it has great potential to
become a practical anti-FR tool.

</details>


### [68] [Realistic Environmental Injection Attacks on GUI Agents](https://arxiv.org/abs/2509.11250)
*Yitong Zhang,Ximo Li,Liyi Cai,Jia Li*

Main category: cs.CR

TL;DR: 本文提出了Chameleon攻击框架，针对LVLM驱动的GUI代理在动态网页环境中的漏洞，通过LLM驱动的环境模拟和注意力黑洞机制，显著提升了攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 现有的环境注入攻击研究假设过于理想化，无法反映真实网页的动态性和小尺寸触发图像的现实场景，需要更现实的威胁模型来暴露GUI代理的脆弱性。

Method: 提出Chameleon攻击框架，包含两个创新：1) LLM驱动的环境模拟自动生成多样化高保真网页模拟；2) 注意力黑洞机制将注意力权重转化为显式监督信号，引导代理关注触发区域。

Result: 在6个真实网站和4个代表性LVLM GUI代理上的评估显示，Chameleon显著优于现有方法，消融研究证实两个创新对性能都至关重要。

Conclusion: 研究揭示了现代GUI代理中未被充分探索的漏洞，为开放世界GUI代理系统的防御研究奠定了坚实基础。

Abstract: GUI agents built on LVLMs are increasingly used to interact with websites.
However, their exposure to open-world content makes them vulnerable to
Environmental Injection Attacks (EIAs) that hijack agent behavior via webpage
elements. Many recent studies assume the attacker to be a regular user who can
only upload a single trigger image, which is more realistic than earlier
assumptions of website-level administrative control. However, these works still
fall short of realism: (1) the trigger's position and surrounding context
remain largely fixed between training and testing, failing to capture the
dynamic nature of real webpages and (2) the trigger often occupies an
unrealistically large area, whereas real-world images are typically small. To
better reflect real-world scenarios, we introduce a more realistic threat model
where the attacker is a regular user and the trigger image is small and
embedded within a dynamically changing environment. As a result, existing
attacks prove largely ineffective under this threat model.
  To better expose the vulnerabilities of GUI agents, we propose Chameleon, an
attack framework with two main novelties. The first is LLM-Driven Environment
Simulation, which automatically generates diverse and high-fidelity webpage
simulations. The second is Attention Black Hole, which transforms attention
weights into explicit supervisory signals that guide the agent's focus toward
the trigger region. We evaluate Chameleon on 6 realistic websites and 4
representative LVLM-powered GUI agents, where it significantly outperforms
existing methods. Ablation studies confirm that both novelties are critical to
performance. Our findings reveal underexplored vulnerabilities in modern GUI
agents and establish a robust foundation for future research on defense in
open-world GUI agent systems. The code is publicly available at
https://github.com/zhangyitonggg/attack2gui.

</details>


### [69] [Thunderhammer: Rowhammer Bitflips via PCIe and Thunderbolt (USB-C)](https://arxiv.org/abs/2509.11440)
*Robert Dumitru,Junpeng Wan,Daniel Genkin,Rick Kennell,Dave,Tian,Yuval Yarom*

Main category: cs.CR

TL;DR: Thunderhammer攻击通过PCIe或Thunderbolt恶意外设实现Rowhammer攻击，首次展示了从外部设备触发DDR4内存位翻转的新攻击向量


<details>
  <summary>Details</summary>
Motivation: Rowhammer攻击传统上通过本地代码或远程代码执行实现，但此前从未探索过通过PCIe等外部设备接口进行攻击的可能性

Method: 设计定制设备逆向工程PCIe请求调度参数，开发精确时序的访问模式，通过PCIe插槽和Thunderbolt端口执行攻击

Result: 成功在DDR4内存模块上通过PCIe连接和Thunderbolt端口实现了Rowhammer诱导的位翻转

Conclusion: Thunderhammer揭示了Rowhammer攻击的新维度，表明恶意外设也能构成严重的安全威胁，需要新的防御机制来应对这类硬件级攻击

Abstract: In recent years, Rowhammer has attracted significant attention from academia
and industry alike. This technique, first published in 2014, flips bits in
memory by repeatedly accessing neighbouring memory locations. Since its
discovery, researchers have developed a substantial body of work exploiting
Rowhammer and proposing countermeasures. These works demonstrate that Rowhammer
can be mounted not only through native code, but also via remote code
execution, such as JavaScript in browsers, and over networks.
  In this work, we uncover a previously unexplored Rowhammer vector. We present
Thunderhammer, an attack that induces DRAM bitflips from malicious peripherals
connected via PCIe or Thunderbolt (which tunnels PCIe). On modern DDR4 systems,
we observe that triggering bitflips through PCIe requests requires precisely
timed access patterns tailored to the target system. We design a custom device
to reverse engineer critical architectural parameters that shape PCIe request
scheduling, and to execute effective hammering access patterns. Leveraging this
knowledge, we successfully demonstrate Rowhammer-induced bitflips in DDR4
memory modules via both PCIe slot connections and Thunderbolt ports tunnelling
PCIe.

</details>


### [70] [MAUI: Reconstructing Private Client Data in Federated Transfer Learning](https://arxiv.org/abs/2509.11451)
*Ahaan Dabholkar,Atul Sharma,Z. Berkay Celik,Saurabh Bagchi*

Main category: cs.CR

TL;DR: MAUI是一种新型的联邦学习数据重建攻击方法，通过仅利用分类头的梯度信息，在不修改模型架构的情况下实现高精度数据重建，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习数据重建攻击在迁移学习场景中存在两个主要弱点：1）初始层梯度信息缺失导致重建精度下降；2）需要修改模型架构的显式攻击容易被客户端检测。

Method: MAUI仅利用分类头的梯度信息，首先从梯度中提取鲁棒的特征表示，然后将这些特征表示反演为原始输入数据。

Result: 在CIFAR10和ImageNet数据集上，针对多种模型架构（CNN、VGG11、ResNet、ShuffleNet-V2、ViT）均实现了高精度重建，PSNR分数比现有方法提高40-120%。

Conclusion: MAUI证明了即使在没有初始层梯度信息的情况下，仅通过分类头梯度也能实现高效的数据重建，且具有更好的隐蔽性。

Abstract: Recent works in federated learning (FL) have shown the utility of leveraging
transfer learning for balancing the benefits of FL and centralized learning. In
this setting, federated training happens after a stable point has been reached
through conventional training. Global model weights are first centrally
pretrained by the server on a public dataset following which only the last few
linear layers (the classification head) of the model are finetuned across
clients. In this scenario, existing data reconstruction attacks (DRAs) in FL
show two key weaknesses. First, strongly input-correlated gradient information
from the initial model layers is never shared, significantly degrading
reconstruction accuracy. Second, DRAs in which the server makes highly
specific, handcrafted manipulations to the model structure or parameters (for
e.g., layers with all zero weights, identity mappings and rows with identical
weight patterns) are easily detectable by an active client.
  Improving on these, we propose MAUI, a stealthy DRA that does not require any
overt manipulations to the model architecture or weights, and relies solely on
the gradients of the classification head. MAUI first extracts "robust" feature
representations of the input batch from the gradients of the classification
head and subsequently inverts these representations to the original inputs. We
report highly accurate reconstructions on the CIFAR10 and ImageNet datasets on
a variety of model architectures including convolution networks (CNN, VGG11),
ResNets (18, 50), ShuffleNet-V2 and Vision Transformer (ViT B-32), regardless
of the batch size. MAUI significantly outperforms prior DRAs in reconstruction
quality, achieving 40-120% higher PSNR scores.

</details>


### [71] [Dstack: A Zero Trust Framework for Confidential Containers](https://arxiv.org/abs/2509.11555)
*Shunfan Zhou,Kevin Wang,Hang Yin*

Main category: cs.CR

TL;DR: dstack是一个将原始TEE技术转化为零信任平台的框架，通过便携式机密容器、去中心化代码管理和可验证域管理三大创新，为Web3应用提供安全可靠的执行环境。


<details>
  <summary>Details</summary>
Motivation: Web3应用需要不依赖中心化信任机构的机密计算平台，但现有TEE实现在安全可靠性、抗审查性和供应商独立性方面存在显著限制。

Method: 提出dstack框架，包含三个核心组件：dstack-OS、dstack-KMS和dstack-Gateway，实现便携式机密容器、去中心化代码管理和可验证域管理三大创新技术。

Result: 评估显示dstack在提供全面安全保证的同时，保持了实际应用的可用性，兼具VM级TEE解决方案的性能优势和Web3应用所需的无信任保证。

Conclusion: dstack成功将原始TEE技术转化为真正的零信任平台，为Web3应用提供了安全可靠的执行环境解决方案。

Abstract: Web3 applications require execution platforms that maintain confidentiality
and integrity without relying on centralized trust authorities. While Trusted
Execution Environments (TEEs) offer promising capabilities for confidential
computing, current implementations face significant limitations when applied to
Web3 contexts, particularly in security reliability, censorship resistance, and
vendor independence.
  This paper presents dstack, a comprehensive framework that transforms raw TEE
technology into a true Zero Trust platform. We introduce three key innovations:
(1) Portable Confidential Containers that enable seamless workload migration
across heterogeneous TEE environments while maintaining security guarantees,
(2) Decentralized Code Management that leverages smart contracts for
transparent governance of TEE applications, and (3) Verifiable Domain
Management that ensures secure and verifiable application identity without
centralized authorities.
  These innovations are implemented through three core components: dstack-OS,
dstack-KMS, and dstack-Gateway. Together, they demonstrate how to achieve both
the performance advantages of VM-level TEE solutions and the trustless
guarantees required by Web3 applications. Our evaluation shows that dstack
provides comprehensive security guarantees while maintaining practical
usability for real-world applications.

</details>


### [72] [ILA: Correctness via Type Checking for Fully Homomorphic Encryption](https://arxiv.org/abs/2509.11559)
*Tarakaram Gollamudi,Anitha Gollamudi,Joshua Gancher*

Main category: cs.CR

TL;DR: 提出了一种面向正确性的中间表示语言ILA，用于同态加密电路的类型检查，通过类型系统跟踪噪声边界和包装错误，确保功能正确性


<details>
  <summary>Details</summary>
Motivation: 现有FHE库和编译器对噪声静态跟踪支持有限，且存在有限模算术的包装错误问题，导致FHE应用开发困难且缺乏信心

Method: 设计ILA中间语言和类型系统，量化跟踪密码文本噪声等低级边界，不依赖密钥，基于FHE模型抽象并实例化到BGV、BFV和TFHE方案

Result: 实现了功能正确性标准的识别和证明，ILA具有最大通用性，核心类型系统不直接假设特定FHE方案

Conclusion: ILA为同态评估电路提供了静态类型检查能力，解决了噪声跟踪和包装错误问题，提升了FHE应用开发的可靠性和信心

Abstract: RLWE-based Fully Homomorphic Encryption (FHE) schemes add some small
\emph{noise} to the message during encryption. The noise accumulates with each
homomorphic operation. When the noise exceeds a critical value, the FHE circuit
produces an incorrect output. This makes developing FHE applications quite
subtle, as one must closely track the noise to ensure correctness. However,
existing libraries and compilers offer limited support to statically track the
noise. Additionally, FHE circuits are also plagued by wraparound errors that
are common in finite modulus arithmetic. These two limitations of existing
compilers and libraries make FHE applications too difficult to develop with
confidence.
  In this work, we present a \emph{correctness-oriented} IR, Intermediate
Language for Arithmetic circuits, for type-checking circuits intended for
homomorphic evaluation. Our IR is backed by a type system that tracks low-level
quantitative bounds (e.g., ciphertext noise) without using the secret key.
Using our type system, we identify and prove a strong \emph{functional
correctness} criterion for \ila circuits. Additionally, we have designed \ila
to be maximally general: our core type system does not directly assume a
particular FHE scheme, but instead axiomatizes a \emph{model} of FHE. We
instantiate this model with the exact FHE schemes (BGV, BFV and TFHE), and
obtain functional correctness for free.

</details>


### [73] [Cyber Threat Hunting: Non-Parametric Mining of Attack Patterns from Cyber Threat Intelligence for Precise Threats Attribution](https://arxiv.org/abs/2509.11615)
*Rimsha Kanwal,Umara Noor,Zafar Iqbal,Zahid Rashid*

Main category: cs.CR

TL;DR: 通过机器学习和可视化分析工具CAPE，自动化网络威胁归因分析，提高攻击模式识别的准确性和效率


<details>
  <summary>Details</summary>
Motivation: 现有网络威胁归因方法准确性低、误报率高，而威胁情报文档数量巨大组织困难，需要更有效的自动化解决方案

Method: 提出Cyber-Attack Pattern Explorer (CAPE)工具，采用非参数挖掘技术从威胁情报文档中提取攻击模式，通过交互式可视化和机器学习算法进行威胁归因

Result: 构建了可以识别攻击模式的数据集，这些模式在语义上与常用主题保持一致，方便解释和分析

Conclusion: CAPE系统能够有效地支持安全分析师进行网络威胁归因，通过自动化挖掘和可视化展示提高信息发现效率

Abstract: With the ever-changing landscape of cyber threats, identifying their origin
has become paramount, surpassing the simple task of attack classification.
Cyber threat attribution gives security analysts the insights they need to
device effective threat mitigation strategies. Such strategies empower
enterprises to proactively detect and defend against future cyber-attacks.
However, existing approaches exhibit limitations in accurately identifying
threat actors, leading to low precision and a significant occurrence of false
positives. Machine learning offers the potential to automate certain aspects of
cyber threat attribution. The distributed nature of information regarding cyber
threat actors and their intricate attack methodologies has hindered substantial
progress in this domain. Cybersecurity analysts deal with an ever-expanding
collection of cyber threat intelligence documents. While these documents hold
valuable insights, their sheer volume challenges efficient organization and
retrieval of pertinent information. To assist the cybersecurity analyst
activities, we propose a machine learning based approach featuring visually
interactive analytics tool named the Cyber-Attack Pattern Explorer (CAPE),
designed to facilitate efficient information discovery by employing interactive
visualization and mining techniques. In the proposed system, a non-parametric
mining technique is proposed to create a dataset for identifying the attack
patterns within cyber threat intelligence documents. These attack patterns
align semantically with commonly employed themes ensuring ease of
interpretation. The extracted dataset is used for training of proposed machine
learning algorithms that enables the attribution of cyber threats with
respective to the actors.

</details>


### [74] [Cyber Attack Mitigation Framework for Denial of Service (DoS) Attacks in Fog Computing](https://arxiv.org/abs/2509.11668)
*Fizza Khurshid,Umara Noor,Zahid Rashid*

Main category: cs.CR

TL;DR: 这篇论文提出了一种多层安全框架，通过智能设备、雾计算和云计算的结合，实现对DDoS攻击的自动化缓解。


<details>
  <summary>Details</summary>
Motivation: 现有研究多集中于攻击检测，而自动化缓解领域被忽视，需要创新方案来应对不断变化的网络威胁。

Method: 设计了一个三层框架：设备层使用智能设备进行包检查，雾层通过统计分析、行为分析和深度包检查进行检测，云层使用防火墙进行确认和缓解攻击。

Result: 该框架能够有效过滤DoS数据包，将合法数据包转发到雾层，形成了一个全面的网络安全保护系统。

Conclusion: 这种多层自动化缓解方法提高了对网络威胁的鲁棒性，为当前网络安全挑战提供了重要解决方案，并为未来的自动化缓解技术指明了方向。

Abstract: Innovative solutions to cyber security issues are shaped by the ever-changing
landscape of cyber threats. Automating the mitigation of these threats can be
achieved through a new methodology that addresses the domain of mitigation
automation, which is often overlooked. This literature overview emphasizes the
lack of scholarly work focusing specifically on automated cyber threat
mitigation, particularly in addressing challenges beyond detection. The
proposed methodology comprise of the development of an automatic cyber threat
mitigation framework tailored for Distributed Denial-of-Service (DDoS) attacks.
This framework adopts a multi-layer security approach, utilizing smart devices
at the device layer, and leveraging fog network and cloud computing layers for
deeper understanding and technological adaptability. Initially, firewall
rule-based packet inspection is conducted on simulated attack traffic to filter
out DoS packets, forwarding legitimate packets to the fog. The methodology
emphasizes the integration of fog detection through statistical and behavioral
analysis, specification-based detection, and deep packet inspection, resulting
in a comprehensive cyber protection system. Furthermore, cloud-level inspection
is performed to confirm and mitigate attacks using firewalls, enhancing
strategic defense and increasing robustness against cyber threats. These
enhancements enhance understanding of the research framework's practical
implementation and assessment strategies, substantiating its importance in
addressing current cyber security challenges and shaping future automation
mitigation approaches.

</details>


### [75] [An Unsupervised Learning Approach For A Reliable Profiling Of Cyber Threat Actors Reported Globally Based On Complete Contextual Information Of Cyber Attacks](https://arxiv.org/abs/2509.11683)
*Sawera Shahid,Umara Noor,Zahid Rashid*

Main category: cs.CR

TL;DR: 本文提出一种无监督的聚类技术，用于根据网络威胁信息对网络突击者进行分组和画像，以提前构建有效防御机制。


<details>
  <summary>Details</summary>
Motivation: 现有监督学习方法依赖结构化数据集且特征数量少，不能可靠地识别网络攻击者的特征和行为模式。

Method: 采用无监督聚类算法（聚类分析），根据网络威胁信息的上下文特征对网络犯罪集团进行分组和画像。

Result: 识别了不同网络威胁行动者之间的关联性，并根据共同特征对其进行聚合分析，实现了网络犯罪集团的自动化画像。

Conclusion: 无监督聚类方法能够有效地分析网络威胁数据，提供更加准确和可靠的网络攻击者画像，为预防性安全防御提供支撑。

Abstract: Cyber attacks are rapidly increasing with the advancement of technology and
there is no protection for our information. To prevent future cyberattacks it
is critical to promptly recognize cyberattacks and establish strong defense
mechanisms against them. To respond to cybersecurity threats immediately, it is
essential to examine the attackers skills, knowledge, and behaviors with the
goal of evaluating their impact on the system and comprehending the traits
associated with these attacks. Creating a profile of cyber threat actors based
on their traits or patterns of behavior can help to create effective defenses
against cyberattacks in advance. In the current literature, multiple supervised
machine learning based approaches considered a smaller number of features for
attacker profiling that are reported in textual cyber threat incident documents
although these profiles have been developed based on the security experts own
perception, we cannot rely on them. Supervised machine learning approaches
strictly depend upon the structure data set. This usually leads to a two step
process where we first have to establish a structured data set before we can
analyze it and then employ it to construct defense mechanisms, which takes
time. In this paper, an unsupervised efficient agglomerative hierarchal
clustering technique is proposed for profiling cybercriminal groups based on
their comprehensive contextual threat information in order to address the
aforementioned issues. The main objective of this report is to identify the
relationship between cyber threat actors based on their common features,
aggregate them, and also profile cyber criminal groups.

</details>


### [76] [Time-Based State-Management of Hash-Based Signature CAs for VPN-Authentication](https://arxiv.org/abs/2509.11695)
*Daniel Herzinger,Linus Heise,Daniel Loebenberger,Matthias Söllner*

Main category: cs.CR

TL;DR: 这篇论文提出了一种采用基于XMSS哈希签名的时间基状态管理方案，实现量子安全的IPsec VPN认证，在保证安全性的同时减少带宽和计算资源消耗。


<details>
  <summary>Details</summary>
Motivation: 量子计算的发展必须将整个技术栈迁移到后量子加密，包括IPsec VPN认证。当前的RFC草案没有考虑哈希基签名，而这种签名具有签名小和可靠长期安全的优势。

Method: 设计了基于时间的状态管理方案，使用XMSS哈希签名方案构建证书授权机构(CA)，CA发行基于经典加密的矩位证书（有效期短，如4小时）。提出了增强时钟弹性、识别系统时钟错误以及快速恢复的策略。

Result: 实现了一个OpenBSD平台的量子安全VPN认证设计，与现有方案相比，需要更少的带宽和计算资源，同时具有高度灵活的矩位证书管理。

Conclusion: 该方案通过结合哈希基签名和短期经典加密证书，实现了高效、安全的量子安全VPN认证，为后量子迁移提供了可行的解决方案。

Abstract: Advances in quantum computing necessitate migrating the entire technology
stack to post-quantum cryptography. This includes IPsec-based VPN connection
authentication. Although there is an RFC draft for post-quantum authentication
in this setting, the draft does not consider (stateful) hash-based signatures
despite their small signature size and trusted long-term security.
  We propose a design with time-based state-management that assigns VPN devices
a certificate authority (CA) based on the hash-based signature scheme XMSS. The
CA then issues leaf certificates which are based on classical cryptography but
have a short validity time, e. g., four hours. It is to be expected that even
large quantum computers will take significantly longer to break the
cryptography, making the design quantum-secure. We propose strategies to make
the timekeeping more resilient to faults and tampering, as well as strategies
to recognize a wrong system time, minimize its potential damage, and quickly
recover.
  The result is an OpenBSD implementation of a quantum-safe and, regarding the
leaf certificates, highly flexible VPN authentication design that requires
significantly less bandwidth and computational resources compared to existing
alternatives.

</details>


### [77] [Removal Attack and Defense on AI-generated Content Latent-based Watermarking](https://arxiv.org/abs/2509.11745)
*De Zhang Lee,Han Fang,Hanyi Wang,Ee-Chien Chang*

Main category: cs.CR

TL;DR: 该论文研究AI生成内容水印的安全性问题，发现仅凭不可区分性不足以保证抗移除攻击，提出了一种利用水印边界信息的新型攻击方法，并设计了相应的防御机制。


<details>
  <summary>Details</summary>
Motivation: 研究数字水印在AI生成内容中的安全性问题，特别是针对移除攻击的抵抗能力，发现现有基于不可区分性的水印方案存在边界信息泄露的安全隐患。

Method: 提出了一种新型攻击方法，利用水印对象的边界位置信息来显著降低移除水印所需的失真；同时设计了一种防御机制，通过秘密变换来隐藏边界信息。

Result: 实验证明新型攻击比基准白噪声攻击在特定设置下可将移除水印所需的失真降低高达15倍；防御机制能有效将攻击者的扰动等同于朴素白噪声攻击者的扰动。

Conclusion: 边界信息泄露是基于潜在空间的水印方案中的重要安全问题，需要专门的防御机制来确保水印的安全性，仅靠不可区分性不足以保证抗移除攻击能力。

Abstract: Digital watermarks can be embedded into AI-generated content (AIGC) by
initializing the generation process with starting points sampled from a secret
distribution. When combined with pseudorandom error-correcting codes, such
watermarked outputs can remain indistinguishable from unwatermarked objects,
while maintaining robustness under whitenoise. In this paper, we go beyond
indistinguishability and investigate security under removal attacks. We
demonstrate that indistinguishability alone does not necessarily guarantee
resistance to adversarial removal. Specifically, we propose a novel attack that
exploits boundary information leaked by the locations of watermarked objects.
This attack significantly reduces the distortion required to remove watermarks
-- by up to a factor of $15 \times$ compared to a baseline whitenoise attack
under certain settings. To mitigate such attacks, we introduce a defense
mechanism that applies a secret transformation to hide the boundary, and prove
that the secret transformation effectively rendering any attacker's
perturbations equivalent to those of a naive whitenoise adversary. Our
empirical evaluations, conducted on multiple versions of Stable Diffusion,
validate the effectiveness of both the attack and the proposed defense,
highlighting the importance of addressing boundary leakage in latent-based
watermarking schemes.

</details>


### [78] [On Spatial-Provenance Recovery in Wireless Networks with Relaxed-Privacy Constraints](https://arxiv.org/abs/2509.11761)
*Manish Bansal,Pramsu Shrivastava,J. Harshan*

Main category: cs.CR

TL;DR: 提出了一种V2X网络中基于相关线性Bloom滤波器的空间溯源恢复协议，在保护车辆位置隐私的同时满足RSU的定位需求


<details>
  <summary>Details</summary>
Motivation: 解决V2X网络中车辆不愿分享精确GPS坐标的隐私顾虑与RSU需要位置数据提供服务的矛盾

Method: 使用相关线性Bloom滤波器嵌入位置信息，考虑定位分辨率、ad hoc协议和无线技术覆盖范围

Result: 理论分析显示在放松隐私和通信开销之间存在权衡，实验证明只需少量包头比特即可提供安全功能如定位低功率干扰源

Conclusion: 提出的放松隐私模型和空间溯源恢复协议能有效平衡隐私保护和服务需求，具有低延迟和低通信开销的优势

Abstract: In Vehicle-to-Everything (V2X) networks with multi-hop communication, Road
Side Units (RSUs) intend to gather location data from the vehicles to offer
various location-based services. Although vehicles use the Global Positioning
System (GPS) for navigation, they may refrain from sharing their exact GPS
coordinates to the RSUs due to privacy considerations. Thus, to address the
localization expectations of the RSUs and the privacy concerns of the vehicles,
we introduce a relaxed-privacy model wherein the vehicles share their partial
location information in order to avail the location-based services. To
implement this notion of relaxed-privacy, we propose a low-latency protocol for
spatial-provenance recovery, wherein vehicles use correlated linear Bloom
filters to embed their position information. Our proposed spatial-provenance
recovery process takes into account the resolution of localization, the
underlying ad hoc protocol, and the coverage range of the wireless technology
used by the vehicles. Through a rigorous theoretical analysis, we present
extensive analysis on the underlying trade-off between relaxed-privacy and the
communication-overhead of the protocol. Finally, using a wireless testbed, we
show that our proposed method requires a few bits in the packet header to
provide security features such as localizing a low-power jammer executing a
denial-of-service attack.

</details>


### [79] [Anomaly Detection in Industrial Control Systems Based on Cross-Domain Representation Learning](https://arxiv.org/abs/2509.11786)
*Dongyang Zhan,Wenqi Zhang,Lin Ye,Xiangzhan Yu,Hongli Zhang,Zheng He*

Main category: cs.CR

TL;DR: 提出基于跨域表示学习的工业控制系统异常检测方法，通过图神经网络学习多域行为的联合特征，使用多任务学习分别识别不同域的异常


<details>
  <summary>Details</summary>
Motivation: 工业控制系统安全至关重要，现有方法主要分析单一域（网络流量或传感器数据），但不同域行为相关，仅分析单一域难以全面识别异常

Method: 构建跨域图表示ICS多域行为，利用图神经网络学习联合特征，采用多任务学习方法分别识别不同域的异常并进行联合训练

Result: 实验结果表明该方法在识别ICS异常方面性能优于现有方法

Conclusion: 跨域表示学习和多任务学习能有效提升工业控制系统异常检测的全面性和准确性

Abstract: Industrial control systems (ICSs) are widely used in industry, and their
security and stability are very important. Once the ICS is attacked, it may
cause serious damage. Therefore, it is very important to detect anomalies in
ICSs. ICS can monitor and manage physical devices remotely using communication
networks. The existing anomaly detection approaches mainly focus on analyzing
the security of network traffic or sensor data. However, the behaviors of
different domains (e.g., network traffic and sensor physical status) of ICSs
are correlated, so it is difficult to comprehensively identify anomalies by
analyzing only a single domain. In this paper, an anomaly detection approach
based on cross-domain representation learning in ICSs is proposed, which can
learn the joint features of multi-domain behaviors and detect anomalies within
different domains. After constructing a cross-domain graph that can represent
the behaviors of multiple domains in ICSs, our approach can learn the joint
features of them by leveraging graph neural networks. Since anomalies behave
differently in different domains, we leverage a multi-task learning approach to
identify anomalies in different domains separately and perform joint training.
The experimental results show that the performance of our approach is better
than existing approaches for identifying anomalies in ICSs.

</details>


### [80] [Off-Path TCP Exploits: PMTUD Breaks TCP Connection Isolation in IP Address Sharing Scenarios](https://arxiv.org/abs/2509.11833)
*Xuewei Feng,Zhaoxi Li,Qi Li,Ziqiang Wang,Kun Sun,Ke Xu*

Main category: cs.CR

TL;DR: 本文揭示了路径MTU发现(PMTUD)在IP地址共享环境中的安全漏洞，攻击者可利用此漏洞进行离线TCP拼控攻击，成功率达70%，平均时间220秒。


<details>
  <summary>Details</summary>
Motivation: 研究PMTUD在普遍IP地址共享实践中的安全漏洞，因为当前PMTUD设计无法有效处理IP地址共享情况，造成可袭略的安全风险。

Method: 通过观察服务器为公共IP地址确定的路径MTU值，离线攻击者与恶意设备合作可以推断其他合法设备的TCP连接序列号，进而执行TCP拼控攻击。方法包括识别目标TCP连接和推断序列号。

Result: 在各种网络配置下进行测试，攻击平均耗时220秒，成功率达70%。在50个真实网络中发现38个存在漏洞。通过SSH DoS、FTP流量毒化和HTTP注入案例证明了其威胁。

Conclusion: PMTUD在IP地址共享环境中存在严重安全漏洞，可导致离线TCP拼控攻击。已向IETF、Linux和Cisco等机构负责任更新，并提出了对策措施。

Abstract: Path MTU Discovery (PMTUD) and IP address sharing are integral aspects of
modern Internet infrastructure. In this paper, we investigate the security
vulnerabilities associated with PMTUD within the context of prevalent IP
address sharing practices. We reveal that PMTUD is inadequately designed to
handle IP address sharing, creating vulnerabilities that attackers can exploit
to perform off-path TCP hijacking attacks. We demonstrate that by observing the
path MTU value determined by a server for a public IP address (shared among
multiple devices), an off-path attacker on the Internet, in collaboration with
a malicious device, can infer the sequence numbers of TCP connections
established by other legitimate devices sharing the same IP address. This
vulnerability enables the attacker to perform off-path TCP hijacking attacks,
significantly compromising the security of the affected TCP connections. Our
attack involves first identifying a target TCP connection originating from the
shared IP address, followed by inferring the sequence numbers of the identified
connection. We thoroughly assess the impacts of our attack under various
network configurations. Experimental results reveal that the attack can be
executed within an average time of 220 seconds, achieving a success rate of
70%.Case studies, including SSH DoS, FTP traffic poisoning, and HTTP injection,
highlight the threat it poses to various applications. Additionally, we
evaluate our attack across 50 real-world networks with IP address
sharing--including public Wi-Fi, VPNs, and 5G--and find 38 vulnerable. Finally,
we responsibly disclose the vulnerabilities, receive recognition from
organizations such as IETF, Linux, and Cisco, and propose our countermeasures.

</details>


### [81] [NeuroStrike: Neuron-Level Attacks on Aligned LLMs](https://arxiv.org/abs/2509.11864)
*Lichao Wu,Sasha Behrouzi,Mohamadreza Rostami,Maximilian Thang,Stjepan Picek,Ahmad-Reza Sadeghi*

Main category: cs.CR

TL;DR: NeuroStrike是一种新颖的攻击框架，通过识别和修剪安全神经元来绕过LLM的安全对齐机制，在白盒和黑盒设置下都能有效攻击，成功率高达76.9%


<details>
  <summary>Details</summary>
Motivation: 当前LLM安全对齐技术（如监督微调和RLHF）存在脆弱性，容易被精心设计的对抗提示绕过，但现有攻击方法依赖试错、缺乏泛化性且受可扩展性限制

Method: 白盒设置：通过前向激活分析识别安全神经元，在推理时修剪这些神经元；黑盒设置：提出首个LLM剖析攻击，在开源代理模型上训练对抗提示生成器，然后攻击黑盒目标

Result: 在20多个开源LLM上，修剪目标层中不到0.6%的神经元，平均攻击成功率达76.9%；对4个多模态LLM实现100%成功率；黑盒攻击在5个黑盒模型上平均成功率达63.7%

Conclusion: 安全神经元在不同架构间具有有效迁移性，NeuroStrike框架揭示了安全对齐机制的根本脆弱性，对LLM安全部署提出了重要挑战

Abstract: Safety alignment is critical for the ethical deployment of large language
models (LLMs), guiding them to avoid generating harmful or unethical content.
Current alignment techniques, such as supervised fine-tuning and reinforcement
learning from human feedback, remain fragile and can be bypassed by carefully
crafted adversarial prompts. Unfortunately, such attacks rely on trial and
error, lack generalizability across models, and are constrained by scalability
and reliability.
  This paper presents NeuroStrike, a novel and generalizable attack framework
that exploits a fundamental vulnerability introduced by alignment techniques:
the reliance on sparse, specialized safety neurons responsible for detecting
and suppressing harmful inputs. We apply NeuroStrike to both white-box and
black-box settings: In the white-box setting, NeuroStrike identifies safety
neurons through feedforward activation analysis and prunes them during
inference to disable safety mechanisms. In the black-box setting, we propose
the first LLM profiling attack, which leverages safety neuron transferability
by training adversarial prompt generators on open-weight surrogate models and
then deploying them against black-box and proprietary targets. We evaluate
NeuroStrike on over 20 open-weight LLMs from major LLM developers. By removing
less than 0.6% of neurons in targeted layers, NeuroStrike achieves an average
attack success rate (ASR) of 76.9% using only vanilla malicious prompts.
Moreover, Neurostrike generalizes to four multimodal LLMs with 100% ASR on
unsafe image inputs. Safety neurons transfer effectively across architectures,
raising ASR to 78.5% on 11 fine-tuned models and 77.7% on five distilled
models. The black-box LLM profiling attack achieves an average ASR of 63.7%
across five black-box models, including the Google Gemini family.

</details>


### [82] [Efficient Byzantine-Robust Privacy-Preserving Federated Learning via Dimension Compression](https://arxiv.org/abs/2509.11870)
*Xian Qin,Xue Yang,Xiaohu Tang*

Main category: cs.CR

TL;DR: 聚合学习中的防泄露与反希拉政击的新方案，通过同态加密和维度压缩实现了隐私保护、精神粒子防御和计算效率的平衡


<details>
  <summary>Details</summary>
Motivation: 聚合学习虽然保护数据隐私，但仍面临梯度暴露隐私和恶意客户攻击的风险，现有方案难以同时满足隐私保护、精神粗鲁性和计算效率的要求

Method: 采用同态加密结合Johnson-Lindenstrauss变换的维度压缩技术，构建双服务器架构，在加密域实现安全的精神粗鲁性防御，并通过梯度压缩大幅降低计算开销

Result: 计算复杂度从O(dn)降至O(kn)（k≪d），在多种数据集上经验验证能保持与非隐私聚合学习相当的模型准确性，同时有效防御网络中达40%的恶意客户攻击

Conclusion: 该方案成功解决了聚合学习中隐私保护、精神粗鲁性和计算效率之间的争扣，为安全高效的协作机器学习提供了可行的技术路径

Abstract: Federated Learning (FL) allows collaborative model training across
distributed clients without sharing raw data, thus preserving privacy. However,
the system remains vulnerable to privacy leakage from gradient updates and
Byzantine attacks from malicious clients. Existing solutions face a critical
trade-off among privacy preservation, Byzantine robustness, and computational
efficiency. We propose a novel scheme that effectively balances these competing
objectives by integrating homomorphic encryption with dimension compression
based on the Johnson-Lindenstrauss transformation. Our approach employs a
dual-server architecture that enables secure Byzantine defense in the
ciphertext domain while dramatically reducing computational overhead through
gradient compression. The dimension compression technique preserves the
geometric relationships necessary for Byzantine defence while reducing
computation complexity from $O(dn)$ to $O(kn)$ cryptographic operations, where
$k \ll d$. Extensive experiments across diverse datasets demonstrate that our
approach maintains model accuracy comparable to non-private FL while
effectively defending against Byzantine clients comprising up to $40\%$ of the
network.

</details>


### [83] [zkToken: Empowering Holders to Limit Revocation Checks for Verifiable Credentials](https://arxiv.org/abs/2509.11934)
*Praveensankar Manimaran,Mayank Raikwar,Thiago Garrett,Arlindo F. da Conceição,Leander Jehl,Roman Vitenberg*

Main category: cs.CR

TL;DR: 提出一种新的时间限制连续验证框架，解决可验证凭证系统中的跟踪性问题，通过黑名单方案和零知证明实现验证周期后撤销状态的不可追踪性


<details>
  <summary>Details</summary>
Motivation: 现有可验证凭证系统在撤销时允许验证方监控凭证有效性，这是敏感信息，存在跟踪风险，且目前没有适当的解决方案

Method: 采用黑名单方案，将撤销凭证的令牌存储在注册表中，使用零知证明允许持有者证明非黑名单成员，持有者可以个别配置验证周期

Result: 理论证明了安全性，分析和实验评估显示在保持其他性能指标与现有方案相当的情况下，显著提高了持有者的带宽消耗

Conclusion: 该框架有效解决了可验证凭证系统中的跟踪性问题，提供了验证周期后撤销状态的不可追踪性保证，具有良好的可扩展性和性能优势

Abstract: Systems managing Verifiable Credentials are becoming increasingly popular.
Unfortunately, their support for revoking previously issued credentials allows
verifiers to effectively monitor the validity of the credentials, which is
sensitive information. While the issue started to gain recognition, no adequate
solution has been proposed so far.
  In this work, we propose a novel framework for time-limited continuous
verification. The holder is able to individually configure the verification
period when sharing information with the verifier, and the system guarantees
proven untraceability of the revocation status after the verification period
expires. Different from existing systems, the implementation adopts a more
scalable blacklist approach where tokens corresponding to revoked credentials
are stored in the registry. The approach employs ZK proofs that allow holders
to prove non-membership in the blacklist. In addition to theoretically proving
security, we evaluate the approach analytically and experimentally and show
that it significantly improves bandwidth consumption on the holder while being
on par with state-of-the-art solutions with respect to the other performance
metrics.

</details>


### [84] [Poison to Detect: Detection of Targeted Overfitting in Federated Learning](https://arxiv.org/abs/2509.11974)
*Soumia Zohra El Mestari,Maciej Krzysztof Zuziak,Gabriele Lenzini*

Main category: cs.CR

TL;DR: 本文研究联邦学习中协调器操纵聚合过程导致特定客户端本地模型过拟合的威胁，提出了三种客户端检测技术来验证全局聚合的完整性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习虽然能保护数据隐私，但仍易受隐私攻击，特别是协调器故意操纵聚合过程导致特定客户端过拟合的威胁。现有研究主要关注减少训练过程中的信息泄露，而本文关注让客户端能早期检测到针对性过拟合，从而在造成重大损害前退出。

Method: 提出了三种检测技术：(a)标签翻转 - 通过翻转标签来检测模型异常；(b)后门触发器注入 - 注入特定模式来验证模型行为；(c)模型指纹识别 - 通过模型特征来识别异常。在多个数据集和不同攻击场景下进行了评估。

Result: 三种方法都能可靠地检测出协调器诱导的针对性过拟合，但在计算复杂度、检测延迟和误报率方面存在差异。

Conclusion: 提出的客户端检测技术能有效识别联邦学习中的针对性过拟合攻击，为客户端提供了早期检测和退出的机制，增强了联邦学习系统的安全性。

Abstract: Federated Learning (FL) enables collaborative model training across
decentralised clients while keeping local data private, making it a widely
adopted privacy-enhancing technology (PET). Despite its privacy benefits, FL
remains vulnerable to privacy attacks, including those targeting specific
clients. In this paper, we study an underexplored threat where a dishonest
orchestrator intentionally manipulates the aggregation process to induce
targeted overfitting in the local models of specific clients. Whereas many
studies in this area predominantly focus on reducing the amount of information
leakage during training, we focus on enabling an early client-side detection of
targeted overfitting, thereby allowing clients to disengage before significant
harm occurs. In line with this, we propose three detection techniques - (a)
label flipping, (b) backdoor trigger injection, and (c) model fingerprinting -
that enable clients to verify the integrity of the global aggregation. We
evaluated our methods on multiple datasets under different attack scenarios.
Our results show that the three methods reliably detect targeted overfitting
induced by the orchestrator, but they differ in terms of computational
complexity, detection latency, and false-positive rates.

</details>


### [85] [LOKI: Proactively Discovering Online Scam Websites by Mining Toxic Search Queries](https://arxiv.org/abs/2509.12181)
*Pujan Paudel,Gianluca Stringhini*

Main category: cs.CR

TL;DR: LOKI系统通过LUPI和SERP特征蒸馏方法，从搜索引擎查询中识别可能返回高比例欺诈网站的查询关键词，显著提高了诈骗网站的发现效率


<details>
  <summary>Details</summary>
Motivation: 现有电商诈骗检测系统虽然准确，但缺乏有效的候选网站发现机制。用户报告反应滞后，主动搜索方法覆盖率低且无法适应新型诈骗类型

Method: 基于学习特权信息(LUPI)框架和搜索引擎结果页面(SERP)特征蒸馏的关键词评分模型

Result: 在10个主要诈骗类别中比基准方法提升20.58倍发现效率，仅用1,663个已知诈骗网站种子集就发现了52,493个未报告诈骗网站，并能泛化到未见过的诈骗类别

Conclusion: LOKI系统能够有效发现电商诈骗网站，具有高覆盖率和良好的泛化能力，对及时发现新兴网络威胁具有重要实用价值

Abstract: Online e-commerce scams, ranging from shopping scams to pet scams, globally
cause millions of dollars in financial damage every year. In response, the
security community has developed highly accurate detection systems able to
determine if a website is fraudulent. However, finding candidate scam websites
that can be passed as input to these downstream detection systems is
challenging: relying on user reports is inherently reactive and slow, and
proactive systems issuing search engine queries to return candidate websites
suffer from low coverage and do not generalize to new scam types. In this
paper, we present LOKI, a system designed to identify search engine queries
likely to return a high fraction of fraudulent websites. LOKI implements a
keyword scoring model grounded in Learning Under Privileged Information (LUPI)
and feature distillation from Search Engine Result Pages (SERPs). We rigorously
validate LOKI across 10 major scam categories and demonstrate a 20.58 times
improvement in discovery over both heuristic and data-driven baselines across
all categories. Leveraging a small seed set of only 1,663 known scam sites, we
use the keywords identified by our method to discover 52,493 previously
unreported scams in the wild. Finally, we show that LOKI generalizes to
previously-unseen scam categories, highlighting its utility in surfacing
emerging threats.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [86] [Situation Model of the Transport, Transport Emissions and Meteorological Conditions](https://arxiv.org/abs/2509.10541)
*V. Benes,M. Svitek,A. Michalikova,M. Melicherik*

Main category: cs.AI

TL;DR: 基于模糊推理系统开发城市交通排放预测模型，分析气象条件对排放量和扩散的影响，为城市规划者提供环保交通管理方案


<details>
  <summary>Details</summary>
Motivation: 城市空气污染是当今社会面临的重要问题，需要系统性分析交通排放与气象条件的关系，为城市规划者提供有效的环境友好型交通管理方案

Method: 使用模糊推理系统(FIS)建立预测模型，基于布拉格的实际交通、气象和排放数据，分析不同条件下排放变化

Result: 开发了能够预测交通排放变化的模型，揭示了气象条件对排放数量和扩散的重要影响

Conclusion: 该模型为城市规划者和政策制定者提供了科学依据，有助于在环境保护前提下更有效地规划和管理城市交通

Abstract: Air pollution in cities and the possibilities of reducing this pollution
represents one of the most important factors that today's society has to deal
with. This paper focuses on a systemic approach to traffic emissions with their
relation to meteorological conditions, analyzing the effect of weather on the
quantity and dispersion of traffic emissions in a city. Using fuzzy inference
systems (FIS) the model for prediction of changes in emissions depending on
various conditions is developed. The proposed model is based on traffic,
meteorology and emission data measured in Prague, Czech Republic. The main
objective of the work is to provide insight into how urban planners and
policymakers can plan and manage urban transport more effectively with
environmental protection in mind.

</details>


### [87] [ZapGPT: Free-form Language Prompting for Simulated Cellular Control](https://arxiv.org/abs/2509.10660)
*Nam H. Le,Patrick Erickson,Yanbo Zhang,Michael Levin,Josh Bongard*

Main category: cs.AI

TL;DR: 通过两个AI模型的协同工作，实现了仅依靠自由形式自然语言提示来指导简单代理群体行为，无需任务特定调整或工程化适应函数


<details>
  <summary>Details</summary>
Motivation: 解决人类语言与人工智能/生物系统间的沟通间隔，实现通过自然语言指令直接控制复杂系统行为，推进AI-生物学合作

Method: 使用第一个AI模型将命令式提示转换为干预措施应用于模拟细胞，第二个AI模型评估提示描述细胞动态的准确性，通过进化算法优化第一个模型以提高评分

Result: 系统能够在不需重新训练的情况下泛化到未见过的提示，证明了仅依靠自然语言提示就可以指导集体行为

Conclusion: 该方法为用语言替代数学目标函数、固定规则和领域特定编程的未来提供了具体步骤，是向着AI-生物学合作进行的重要进展

Abstract: Human language is one of the most expressive tools for conveying intent, yet
most artificial or biological systems lack mechanisms to interpret or respond
meaningfully to it. Bridging this gap could enable more natural forms of
control over complex, decentralized systems. In AI and artificial life, recent
work explores how language can specify high-level goals, but most systems still
depend on engineered rewards, task-specific supervision, or rigid command sets,
limiting generalization to novel instructions. Similar constraints apply in
synthetic biology and bioengineering, where the locus of control is often
genomic rather than environmental perturbation.
  A key open question is whether artificial or biological collectives can be
guided by free-form natural language alone, without task-specific tuning or
carefully designed evaluation metrics. We provide one possible answer here by
showing, for the first time, that simple agents' collective behavior can be
guided by free-form language prompts: one AI model transforms an imperative
prompt into an intervention that is applied to simulated cells; a second AI
model scores how well the prompt describes the resulting cellular dynamics; and
the former AI model is evolved to improve the scores generated by the latter.
  Unlike previous work, our method does not require engineered fitness
functions or domain-specific prompt design. We show that the evolved system
generalizes to unseen prompts without retraining. By treating natural language
as a control layer, the system suggests a future in which spoken or written
prompts could direct computational, robotic, or biological systems to desired
behaviors. This work provides a concrete step toward this vision of AI-biology
partnerships, in which language replaces mathematical objective functions,
fixed rules, and domain-specific programming.

</details>


### [88] [Maestro: Self-Improving Text-to-Image Generation via Agent Orchestration](https://arxiv.org/abs/2509.10704)
*Xingchen Wan,Han Zhou,Ruoxi Sun,Hootan Nakhost,Ke Jiang,Rajarishi Sinha,Sercan Ö. Arık*

Main category: cs.AI

TL;DR: Maestro是一个自演进图像生成系统，通过自我批判和自我进化机制，让文本到图像模型能够仅从初始提示自主改进生成图像质量，无需人工干预。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像模型高度依赖人工干预和迭代提示工程，存在可用性挑战，需要手动处理通常不够明确的提示。

Method: 采用两个关键创新：1) 自我批判 - 使用多模态LLM代理作为'评论家'识别图像弱点并提供可解释的编辑信号；2) 自我进化 - 利用MLLM作为评判者进行图像对比，进化创意提示候选。

Result: 在复杂文本到图像任务上的广泛实验表明，Maestro显著提高了图像质量，优于初始提示和最先进的自动化方法，且效果随MLLM组件先进性而提升。

Conclusion: 这项工作为自改进的文本到图像生成提供了一个强大、可解释且有效的途径。

Abstract: Text-to-image (T2I) models, while offering immense creative potential, are
highly reliant on human intervention, posing significant usability challenges
that often necessitate manual, iterative prompt engineering over often
underspecified prompts. This paper introduces Maestro, a novel self-evolving
image generation system that enables T2I models to autonomously self-improve
generated images through iterative evolution of prompts, using only an initial
prompt. Maestro incorporates two key innovations: 1) self-critique, where
specialized multimodal LLM (MLLM) agents act as 'critics' to identify
weaknesses in generated images, correct for under-specification, and provide
interpretable edit signals, which are then integrated by a 'verifier' agent
while preserving user intent; and 2) self-evolution, utilizing MLLM-as-a-judge
for head-to-head comparisons between iteratively generated images, eschewing
problematic images, and evolving creative prompt candidates that align with
user intents. Extensive experiments on complex T2I tasks using black-box models
demonstrate that Maestro significantly improves image quality over initial
prompts and state-of-the-art automated methods, with effectiveness scaling with
more advanced MLLM components. This work presents a robust, interpretable, and
effective pathway towards self-improving T2I generation.

</details>


### [89] [Understanding AI Evaluation Patterns: How Different GPT Models Assess Vision-Language Descriptions](https://arxiv.org/abs/2509.10707)
*Sajjad Abdoli,Rudi Cilibrasi,Rima Al-Shikh*

Main category: cs.AI

TL;DR: 这研究分析了三种GPT模型在评估视觉-语言描述时的不同"评估个性"，发现GPT-4o-mini系统一致性强，GPT-4o错误检测能力突出，GPT-5呈现极端保守主义，且所有GPT模型都存在2:1的负面偏见。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统越来越多地评估其他AI输出，理解它们的评估行为对防止偏见的层层累积至关重要。

Method: 分析NVIDIA Describe Anything模型生成的视觉-语言描述，并用三种GPT变体(GPT-4o, GPT-4o-mini, GPT-5)进行评估，使用Gemini 2.5 Pro作为独立问题生成器进行受控实验验证。

Result: 不同GPT模型呈现明显不同的评估个性：GPT-4o-mini系统一致性强但方差最小，GPT-4o错误检测能力突出，GPT-5呈现极端保守主义且变异性高。所有GPT模型都存在2:1的负面偏见。

Conclusion: 评估能力并不随着通用能力的提升而扩展，健壮的AI评估需要多样化的架构视角。

Abstract: As AI systems increasingly evaluate other AI outputs, understanding their
assessment behavior becomes crucial for preventing cascading biases. This study
analyzes vision-language descriptions generated by NVIDIA's Describe Anything
Model and evaluated by three GPT variants (GPT-4o, GPT-4o-mini, GPT-5) to
uncover distinct "evaluation personalities" the underlying assessment
strategies and biases each model demonstrates. GPT-4o-mini exhibits systematic
consistency with minimal variance, GPT-4o excels at error detection, while
GPT-5 shows extreme conservatism with high variability. Controlled experiments
using Gemini 2.5 Pro as an independent question generator validate that these
personalities are inherent model properties rather than artifacts. Cross-family
analysis through semantic similarity of generated questions reveals significant
divergence: GPT models cluster together with high similarity while Gemini
exhibits markedly different evaluation strategies. All GPT models demonstrate a
consistent 2:1 bias favoring negative assessment over positive confirmation,
though this pattern appears family-specific rather than universal across AI
architectures. These findings suggest that evaluation competence does not scale
with general capability and that robust AI assessment requires diverse
architectural perspectives.

</details>


### [90] [AI Answer Engine Citation Behavior An Empirical Analysis of the GEO16 Framework](https://arxiv.org/abs/2509.10762)
*Arlen Kumar,Leanid Palkhouski*

Main category: cs.AI

TL;DR: GEO-16框架评估AI引擎引用网页质量，发现页面整体质量是引用的强预测因子，Metadata、Freshness、Semantic HTML和Structured Data等支柱与引用关联最强。


<details>
  <summary>Details</summary>
Motivation: 随着AI答案引擎通过生成响应和引用网页来源来中介领域知识访问，需要建立一个评估框架来审计这些引擎引用的网页质量。

Method: 使用70个产品意图提示收集了三个引擎（Brave Summary、Google AI Overviews和Perplexity）的1,702个引用，审计了1,100个唯一URL，采用GEO-16框架将页面质量信号转换为带支柱分数和标准化GEO分数G（0-1）。

Result: 引擎在引用页面的GEO质量上存在差异，整体页面质量是引用的强预测因子，G≥0.70且至少12个支柱命中的操作点与显著更高的引用率相关。

Conclusion: 研究为出版商提供了实用的操作指南，但这是观察性研究，专注于英语B2B SaaS页面，存在局限性和有效性威胁。

Abstract: AI answer engines increasingly mediate access to domain knowledge by
generating responses and citing web sources. We introduce GEO-16, a 16 pillar
auditing framework that converts on page quality signals into banded pillar
scores and a normalized GEO score G that ranges from 0 to 1. Using 70 product
intent prompts, we collected 1,702 citations across three engines (Brave
Summary, Google AI Overviews, and Perplexity) and audited 1,100 unique URLs. In
our corpus, the engines differed in the GEO quality of the pages they cited,
and pillars related to Metadata and Freshness, Semantic HTML, and Structured
Data showed the strongest associations with citation. Logistic models with
domain clustered standard errors indicate that overall page quality is a strong
predictor of citation, and simple operating points (for example, G at least
0.70 combined with at least 12 pillar hits) align with substantially higher
citation rates in our data. We report per engine contrasts, vertical effects,
threshold analysis, and diagnostics, then translate findings into a practical
playbook for publishers. The study is observational and focuses on English
language B2B SaaS pages; we discuss limitations, threats to validity, and
reproducibility considerations.

</details>


### [91] [AgentArch: A Comprehensive Benchmark to Evaluate Agent Architectures in Enterprise](https://arxiv.org/abs/2509.10769)
*Tara Bogavelli,Roshnee Sharma,Hari Subramani*

Main category: cs.AI

TL;DR: 这篇论文通过企业特定基准测诅18种不同的自主组件配置，发现了重要的模型特定架构偏好和自主系统在企业任务上的显著弱点，最高成功率仅为35.3%~70.8%。


<details>
  <summary>Details</summary>
Motivation: 当前对自主组件在复杂多自主系统中的交互作用缺乏实证理解，需要通过综合性基准测诅来填补这些知识空白。

Method: 使用企业特定基准测诅评估18种不同的自主组件配置，重点考察四个关键维度：组织策略、提示实现方式、内存架构和思考工具集成。

Result: 发现了显著的模型特定架构偏好，挖掘了当前"一刀切"自主AI系统范式的问题，同时发现自主系统在企业任务上表现弱劳，最高成功率仅为35.3%~70.8%。

Conclusion: 这些发现应该为未来自主系统的设计提供实证支撑，使得架构组件选择和模型选择更加以实证为基础。

Abstract: While individual components of agentic architectures have been studied in
isolation, there remains limited empirical understanding of how different
design dimensions interact within complex multi-agent systems. This study aims
to address these gaps by providing a comprehensive enterprise-specific
benchmark evaluating 18 distinct agentic configurations across state-of-the-art
large language models. We examine four critical agentic system dimensions:
orchestration strategy, agent prompt implementation (ReAct versus function
calling), memory architecture, and thinking tool integration. Our benchmark
reveals significant model-specific architectural preferences that challenge the
prevalent one-size-fits-all paradigm in agentic AI systems. It also reveals
significant weaknesses in overall agentic performance on enterprise tasks with
the highest scoring models achieving a maximum of only 35.3\% success on the
more complex task and 70.8\% on the simpler task. We hope these findings inform
the design of future agentic systems by enabling more empirically backed
decisions regarding architectural components and model selection.

</details>


### [92] [LLM Enhancement with Domain Expert Mental Model to Reduce LLM Hallucination with Causal Prompt Engineering](https://arxiv.org/abs/2509.10818)
*Boris Kovalerchuk,Brent D. Fegley*

Main category: cs.AI

TL;DR: 本文提出了一种基于人机对话和布尔函数的专家心理模型(EMM)算法，用于改进LLM在决策支持中的提示工程，解决LLM训练数据缺失和幻觉问题。


<details>
  <summary>Details</summary>
Motivation: LLM在决策支持中存在训练数据缺失导致的幻觉问题，现有RAG方法无法完全解决信息缺失问题，需要更有效的方法来捕捉专家复杂心理模型。

Method: 提出四步EMM算法：因素识别、层次化结构构建、生成广义专家心理模型规范、从规范生成详细模型，基于优化人机对话和单调布尔函数。

Result: 开发了计算可处理的个人专家决策心理模型，能够更有效地利用LLM进行决策支持，减少幻觉并提高准确性。

Conclusion: EMM方法为LLM提示工程提供了系统化框架，能够更好地捕获专家知识，提升LLM在信息缺失情况下的决策支持能力。

Abstract: Difficult decision-making problems abound in various disciplines and domains.
The proliferation of generative techniques, especially large language models
(LLMs), has excited interest in using them for decision support. However, LLMs
cannot yet resolve missingness in their training data, leading to
hallucinations. Retrieval-Augmented Generation (RAG) enhances LLMs by
incorporating external information retrieval, reducing hallucinations and
improving accuracy. Yet, RAG and related methods are only partial solutions, as
they may lack access to all necessary sources or key missing information. Even
everyday issues often challenge LLMs' abilities. Submitting longer prompts with
context and examples is one approach to address knowledge gaps, but designing
effective prompts is non-trivial and may not capture complex mental models of
domain experts. For tasks with missing critical information, LLMs are
insufficient, as are many existing systems poorly represented in available
documents. This paper explores how LLMs can make decision-making more
efficient, using a running example of evaluating whether to respond to a call
for proposals. We propose a technology based on optimized human-machine
dialogue and monotone Boolean and k-valued functions to discover a
computationally tractable personal expert mental model (EMM) of
decision-making. Our EMM algorithm for LLM prompt engineering has four steps:
(1) factor identification, (2) hierarchical structuring of factors, (3)
generating a generalized expert mental model specification, and (4) generating
a detailed generalized expert mental model from that specification.

</details>


### [93] [From Grounding to Skolemization: A Logic-Constrained Vector Symbolic Architecture for Complex Query Answering](https://arxiv.org/abs/2509.10837)
*Yuyin Lu,Hegang Chen,Yanghui Rao*

Main category: cs.AI

TL;DR: 该论文提出了Logic-constrained Vector Symbolic Architecture (LVSA)，一个神经符号框架，通过统一可微Skolem化模块和神经否定器，解决了知识图谱复杂查询回答中逻辑完备性和计算效率的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 解决不完全知识图谱上复杂查询回答(EFO$_1$查询)中逻辑完备性和计算效率之间的根本权衡问题。基于Grounding的方法存在组合爆炸，而大多数Skolemization方法忽视了显式建模Skolem函数并损害了逻辑一致性。

Method: 提出Logic-constrained Vector Symbolic Architecture (LVSA)框架，包含：1) 可微Skolem化模块；2) 神经否定器；3) 逻辑约束驱动的优化协议，协调几何和逻辑需求。

Result: 理论上保证对所有EFO$_1$查询的普遍性。实证上优于最先进的Skolemization方法，相比基于Grounding的基线方法将推理成本降低了几个数量级。

Conclusion: LVSA框架成功解决了知识图谱复杂查询回答中的关键挑战，在保持逻辑完备性的同时显著提高了计算效率，为神经符号推理提供了有效的解决方案。

Abstract: Complex Query Answering (CQA) over incomplete Knowledge Graphs (KGs),
typically formalized as reasoning with Existential First-Order predicate logic
with one free variable (EFO$_1$), faces a fundamental trade-off between logical
soundness and computational efficiency. This work establishes the
Grounding-Skolemization dichotomy for systematically analyzing CQA methods
through the lens of formal logic. While Grounding-based methods inherently
suffer from combinatorial explosion, most Skolemization-based methods neglect
to explicitly model Skolem functions and compromise logical consistency. To
address these limitations, we propose the Logic-constrained Vector Symbolic
Architecture (LVSA), a neuro-symbolic framework that unifies a differentiable
Skolemization module and a neural negator, as well as a logical
constraint-driven optimization protocol to harmonize geometric and logical
requirements. Theoretically, LVSA guarantees universality for all EFO$_1$
queries. Empirically, it outperforms state-of-the-art Skolemization-based
methods and reduces inference costs by orders of magnitude compared to
Grounding-based baselines.

</details>


### [94] [Is the `Agent' Paradigm a Limiting Framework for Next-Generation Intelligent Systems?](https://arxiv.org/abs/2509.10875)
*Jesse Gardner,Vladimir A. Baulin*

Main category: cs.AI

TL;DR: 本文重新评估AI研究中的"代理"范式，指出其概念模糊性和人类中心偏见可能限制了AI发展，建议向系统动态和非代理框架转变


<details>
  <summary>Details</summary>
Motivation: 批判性重新评估AI研究中长期以来的代理中心范式，认为该范式存在概念模糊性和人类中心偏见，可能限制了AI的发展

Method: 通过系统文献综述，对比分析了代理系统（agency-inspired）、自主系统（agential）和非代理系统（non-agentic）的区别，解构了各种AI框架中的代理范式

Result: 发现代理框架虽有启发性但容易产生误导，特别是在大语言模型中可能模糊了基础计算机制，需要向系统动态和非代理框架转变

Conclusion: 研究非代理和系统框架对开发健壮、可扩展、潜在非人类形式的通用智能至关重要，需要根本重新考虑对智能的理解，超越代理隐喻

Abstract: The concept of the 'agent' has profoundly shaped Artificial Intelligence (AI)
research, guiding development from foundational theories to contemporary
applications like Large Language Model (LLM)-based systems. This paper
critically re-evaluates the necessity and optimality of this agent-centric
paradigm. We argue that its persistent conceptual ambiguities and inherent
anthropocentric biases may represent a limiting framework. We distinguish
between agentic systems (AI inspired by agency, often semi-autonomous, e.g.,
LLM-based agents), agential systems (fully autonomous, self-producing systems,
currently only biological), and non-agentic systems (tools without the
impression of agency). Our analysis, based on a systematic review of relevant
literature, deconstructs the agent paradigm across various AI frameworks,
highlighting challenges in defining and measuring properties like autonomy and
goal-directedness. We argue that the 'agentic' framing of many AI systems,
while heuristically useful, can be misleading and may obscure the underlying
computational mechanisms, particularly in Large Language Models (LLMs). As an
alternative, we propose a shift in focus towards frameworks grounded in
system-level dynamics, world modeling, and material intelligence. We conclude
that investigating non-agentic and systemic frameworks, inspired by complex
systems, biology, and unconventional computing, is essential for advancing
towards robust, scalable, and potentially non-anthropomorphic forms of general
intelligence. This requires not only new architectures but also a fundamental
reconsideration of our understanding of intelligence itself, moving beyond the
agent metaphor.

</details>


### [95] [Harmful Prompt Laundering: Jailbreaking LLMs with Abductive Styles and Symbolic Encoding](https://arxiv.org/abs/2509.10931)
*Seongho Joo,Hyukhun Koh,Kyomin Jung*

Main category: cs.AI

TL;DR: HaPLa是一种新型的通用越狱攻击技术，通过溯因推理框架和符号编码两种策略，在黑盒访问条件下实现对LLM的高效攻击，成功率超过95%。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型存在被恶意滥用的风险，需要研究利用其架构和学习范式内在弱点的通用越狱攻击方法来加强防御。

Method: 提出HaPLa技术，包含：1) 溯因推理框架 - 让LLM推断有害活动的中间步骤而非直接响应；2) 符号编码 - 轻量级方法混淆有害内容，绕过关键词检测。

Result: 在GPT系列模型上攻击成功率超过95%，所有目标模型平均达到70%。不同符号编码规则的分析表明，安全调优LLM而不显著降低其对良性查询的有用性仍然困难。

Conclusion: HaPLa展示了LLM存在的根本性安全挑战，当前防御机制存在局限性，需要在模型安全性和实用性之间找到更好的平衡。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across
diverse tasks, but their potential misuse for harmful purposes remains a
significant concern. To strengthen defenses against such vulnerabilities, it is
essential to investigate universal jailbreak attacks that exploit intrinsic
weaknesses in the architecture and learning paradigms of LLMs. In response, we
propose \textbf{H}armful \textbf{P}rompt \textbf{La}undering (HaPLa), a novel
and broadly applicable jailbreaking technique that requires only black-box
access to target models. HaPLa incorporates two primary strategies: 1)
\textit{abductive framing}, which instructs LLMs to infer plausible
intermediate steps toward harmful activities, rather than directly responding
to explicit harmful queries; and 2) \textit{symbolic encoding}, a lightweight
and flexible approach designed to obfuscate harmful content, given that current
LLMs remain sensitive primarily to explicit harmful keywords. Experimental
results show that HaPLa achieves over 95% attack success rate on GPT-series
models and 70% across all targets. Further analysis with diverse symbolic
encoding rules also reveals a fundamental challenge: it remains difficult to
safely tune LLMs without significantly diminishing their helpfulness in
responding to benign queries.

</details>


### [96] [Public Data Assisted Differentially Private In-Context Learning](https://arxiv.org/abs/2509.10932)
*Seongho Joo,Hyukhun Koh,Kyomin Jung*

Main category: cs.AI

TL;DR: 通过引入任务相关公共数据在保持差分隐私保证的前提下，提高了私有上下文学习的效果，同时具有强壁垒成员推断攻击的能力


<details>
  <summary>Details</summary>
Motivation: 大语言模型中的上下文学习存在私有数据泄漏风险，而传统差分隐私方案往往会导致模型效用显著下降

Method: 将任务相关公共数据整合到上下文学习框架中，在维持差分隐私保证的同时设计私有上下文学习算法

Result: 实验结果显示该方法在公共数据的协助下显著提高了私有ICL的效用，并具有强壁垒成员推断攻击的能力

Conclusion: 通过利用公共数据来平衡隐私保护和模型效用是可行的，该方法为安全的上下文学习提供了有效解决方案

Abstract: In-context learning (ICL) in Large Language Models (LLMs) has shown
remarkable performance across various tasks without requiring fine-tuning.
However, recent studies have highlighted the risk of private data leakage
through the prompt in ICL, especially when LLMs are exposed to malicious
attacks. While differential privacy (DP) provides strong privacy guarantees, it
often significantly reduces the utility of in-context learning (ICL). To
address this challenge, we incorporate task-related public data into the ICL
framework while maintaining the DP guarantee. Based on this approach, we
propose a private in-context learning algorithm that effectively balances
privacy protection and model utility. Through experiments, we demonstrate that
our approach significantly improves the utility of private ICL with the
assistance of public data. Additionally, we show that our method is robust
against membership inference attacks, demonstrating empirical privacy
protection.

</details>


### [97] [Enhancing Computational Cognitive Architectures with LLMs: A Case Study](https://arxiv.org/abs/2509.10972)
*Ron Sun*

Main category: cs.AI

TL;DR: 这篇论文探讨了将大语言模型组合到Clarion认知架构中的方法，以结合LLMs的计算能力和认知架构的心理实际性。


<details>
  <summary>Details</summary>
Motivation: 认知计算架构虽然结构化了心理功能，但计算能力有限，而LLMs显示出优异的计算能力，因此需要将两者结合来处理现实世界的复杂性和心理实际性。

Method: 使用Clarion认知架构的隐式-显式二分法作为基础，实现Clarion与LLMs的无缝集成，将LLMs的计算能力与Clarion的心理精细性相结合。

Result: 成功实现了Clarion认知架构与大语言模型的协同组合，为处理现实世界复杂性和保持心理实际性提供了新的解决方案。

Conclusion: 通过将LLMs集成到认知架构中，可以同时满足计算能力和心理实际性的要求，为认知计算架构的发展开启了新方向。

Abstract: Computational cognitive architectures are broadly scoped models of the human
mind that combine different psychological functionalities (as well as often
different computational methods for these different functionalities) into one
unified framework. They structure them in a psychologically plausible and
validated way. However, such models thus far have only limited computational
capabilities, mostly limited by the computational tools and techniques that
were adopted. More recently, LLMs have proved to be more capable
computationally than any other tools. Thus, in order to deal with both
real-world complexity and psychological realism at the same time, incorporating
LLMs into cognitive architectures naturally becomes an important task. In the
present article, a synergistic combination of the Clarion cognitive
architecture and LLMs is discussed as a case study. The implicit-explicit
dichotomy that is fundamental to Clarion is leveraged for a seamless
integration of Clarion and LLMs. As a result, computational power of LLMs is
combined with psychological nicety of Clarion.

</details>


### [98] [Rethinking Human Preference Evaluation of LLM Rationales](https://arxiv.org/abs/2509.11026)
*Ziang Li,Manasi Ganti,Zixian Ma,Helena Vasconcelos,Qijia He,Ranjay Krishna*

Main category: cs.AI

TL;DR: 本文重新思考LLM生成rationale的评估方法，提出基于多属性细粒度评估来替代传统的二元偏好判断，通过识别关键属性、分析人类偏好与属性的关系，并开发属性特定的ELO评分系统来提供更细致的模型比较。


<details>
  <summary>Details</summary>
Motivation: 当前LLM生成rationale的评估主要依赖人类或LLM的二元偏好判断，这种方法过于粗糙且不透明，无法提供rationale优劣的具体原因，需要更细粒度的评估框架。

Method: 1) 从文献中识别关键rationale属性；2) 使用自动指标、LLM判断和人工标注评估这些属性；3) 用SHAP分析MT Bench和Chatbot Arena数据集，确定最能解释人类偏好的属性；4) 开发属性特定的ELO评分系统重新评估模型。

Result: 研究发现细粒度属性评估能更好地表征rationale质量，属性特定的ELO评分揭示了更细致的模型比较结果，为rationale评估提供了更丰富的信息。

Conclusion: 基于属性的细粒度评估方法能够克服二元比较的局限性，为LLM rationale评估提供更可解释和可靠的实践指导，推动未来研究向更透明的评估方向发展。

Abstract: Large language models (LLMs) often generate natural language rationales --
free-form explanations that help improve performance on complex reasoning tasks
and enhance interpretability for human users. However, evaluating these
rationales remains challenging. While recent work has relied on binary
preference judgments from humans or LLM judges, such evaluations are often
opaque and coarse-grained, offering limited insight into what makes one
rationale better than another. In this work, we rethink preference evaluation
for LLM-generated rationales by asking: (1) What attributes define good
rationales? (2) Can human preferences be explained by these attributes? (3) Can
attribute-based evaluation overcome the limitations of binary comparisons? We
identify a set of key rationale attributes from prior literature and assess
them using automatic metrics, LLM judgments, and human annotations. We then
analyze two standard human preference datasets MT Bench and Chatbot Arena using
SHAP to identify which attributes best explain human preference outcomes.
Finally, we re-evaluate model-generated rationales using attribute-specific ELO
scores, revealing more nuanced model comparisons and insights. Our findings
suggest that fine-grained attribute evaluations can better characterize
rationale quality and guide future research toward more interpretable and
reliable evaluation practices.

</details>


### [99] [Free-MAD: Consensus-Free Multi-Agent Debate](https://arxiv.org/abs/2509.11035)
*Yu Cui,Hang Fu,Haibin Zhang,Licheng Wang,Cong Zuo*

Main category: cs.AI

TL;DR: Free-MAD是一个新的多智能体辩论框架，通过基于分数的决策机制和反从众机制，解决了传统共识式MAD方法的通信开销大、错误传播和投票随机性问题，在单轮辩论中显著提升推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有MAD方法依赖多轮交互达成共识和多数投票，存在通信开销大、正确回答可能被错误回答影响导致错误传播、多数投票引入随机性和不公平性等问题。

Method: 提出Free-MAD框架：1）基于分数的决策机制，评估整个辩论轨迹而非仅最后一轮；2）引入反从众机制，减轻多数意见的过度影响；3）仅需单轮辩论。

Result: 在8个基准数据集上，Free-MAD显著提升推理性能，同时减少token成本。相比现有MAD方法，在真实攻击场景中表现出更好的鲁棒性。

Conclusion: Free-MAD通过消除智能体间共识需求，采用轨迹评估和反从众机制，有效解决了传统MAD方法的局限性，实现了更高效、准确和鲁棒的推理性能。

Abstract: Multi-agent debate (MAD) is an emerging approach to improving the reasoning
capabilities of large language models (LLMs). Existing MAD methods rely on
multiple rounds of interaction among agents to reach consensus, and the final
output is selected by majority voting in the last round. However, this
consensus-based design faces several limitations. First, multiple rounds of
communication increases token overhead and limits scalability. Second, due to
the inherent conformity of LLMs, agents that initially produce correct
responses may be influenced by incorrect ones during the debate process,
causing error propagation. Third, majority voting introduces randomness and
unfairness in the decision-making phase, and can degrade the reasoning
performance.
  To address these issues, we propose \textsc{Free-MAD}, a novel MAD framework
that eliminates the need for consensus among agents. \textsc{Free-MAD}
introduces a novel score-based decision mechanism that evaluates the entire
debate trajectory rather than relying on the last round only. This mechanism
tracks how each agent's reasoning evolves, enabling more accurate and fair
outcomes. In addition, \textsc{Free-MAD} reconstructs the debate phase by
introducing anti-conformity, a mechanism that enables agents to mitigate
excessive influence from the majority. Experiments on eight benchmark datasets
demonstrate that \textsc{Free-MAD} significantly improves reasoning performance
while requiring only a single-round debate and thus reducing token costs. We
also show that compared to existing MAD approaches, \textsc{Free-MAD} exhibits
improved robustness in real-world attack scenarios.

</details>


### [100] [Agentic Lybic: Multi-Agent Execution System with Tiered Reasoning and Orchestration](https://arxiv.org/abs/2509.11067)
*Liangxuan Guo,Bin Zhu,Qingqian Tao,Kangning Liu,Xun Zhao,Xianzhe Qin,Jin Gao,Guangfu Hao*

Main category: cs.AI

TL;DR: Agentic Lybic是一个基于有限状态机(FSM)的多智能体桌面自动化系统，通过动态编排和持续质量控制在复杂多步骤任务中实现了57.07%的成功率，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的桌面自动化智能体在处理复杂多步骤任务时存在协调性差和质量控制不足的问题，需要一种更可靠的自适应系统。

Method: 采用有限状态机架构，包含Controller、Manager、三个Worker(Technician、Operator、Analyst)和Evaluator四个组件，通过FSM路由机制动态选择最优执行策略。

Result: 在OSWorld基准测试中达到57.07%的成功率(50步内)，大幅超越现有方法，证明了系统在复杂计算环境中的卓越可靠性。

Conclusion: 基于原则的多智能体编排结合持续质量控制为通用桌面自动化提供了优越的可靠性，FSM架构实现了灵活的动态协调和错误恢复能力。

Abstract: Autonomous agents for desktop automation struggle with complex multi-step
tasks due to poor coordination and inadequate quality control. We introduce
\textsc{Agentic Lybic}, a novel multi-agent system where the entire
architecture operates as a finite-state machine (FSM). This core innovation
enables dynamic orchestration. Our system comprises four components: a
Controller, a Manager, three Workers (Technician for code-based operations,
Operator for GUI interactions, and Analyst for decision support), and an
Evaluator. The critical mechanism is the FSM-based routing between these
components, which provides flexibility and generalization by dynamically
selecting the optimal execution strategy for each subtask. This principled
orchestration, combined with robust quality gating, enables adaptive replanning
and error recovery. Evaluated officially on the OSWorld benchmark,
\textsc{Agentic Lybic} achieves a state-of-the-art 57.07\% success rate in 50
steps, substantially outperforming existing methods. Results demonstrate that
principled multi-agent orchestration with continuous quality control provides
superior reliability for generalized desktop automation in complex computing
environments.

</details>


### [101] [Tractable Asymmetric Verification for Large Language Models via Deterministic Replicability](https://arxiv.org/abs/2509.11068)
*Zan-Kai Chong,Hiroyuki Ohsaki,Bryan Ng*

Main category: cs.AI

TL;DR: 提出了一种用于验证多智能体系统中LLM输出真实性的可验证框架，通过概率性审计随机片段实现不对称计算成本，验证成本远低于生成成本


<details>
  <summary>Details</summary>
Motivation: 解决多智能体系统中LLM输出真实性的计算信任问题，防止输出被伪造或由劣质模型生成

Method: 基于确定性可复制性原则，在计算同质环境下，通过多个验证者概率性审计LLM输出的随机小片段，实现分布式验证

Result: 模拟显示针对性验证比完全重新生成快12倍以上，具有可调节的检测概率参数

Conclusion: 为可审计LLM系统建立了可行的验证机制，为负责任AI提供基础层，并为未来复杂异构多智能体系统研究奠定基础

Abstract: The landscape of Large Language Models (LLMs) shifts rapidly towards dynamic,
multi-agent systems. This introduces a fundamental challenge in establishing
computational trust, specifically how one agent can verify that another's
output was genuinely produced by a claimed LLM, and not falsified or generated
by a cheaper or inferior model. To address this challenge, this paper proposes
a verification framework that achieves tractable asymmetric effort, where the
cost to verify a computation is substantially lower than the cost to perform
it. Our approach is built upon the principle of deterministic replicability, a
property inherent to autoregressive models that strictly necessitates a
computationally homogeneous environment where all agents operate on identical
hardware and software stacks. Within this defined context, our framework
enables multiple validators to probabilistically audit small, random segments
of an LLM's output and it distributes the verification workload effectively.
The simulations demonstrated that targeted verification can be over 12 times
faster than full regeneration, with tunable parameters to adjust the detection
probability. By establishing a tractable mechanism for auditable LLM systems,
our work offers a foundational layer for responsible AI and serves as a
cornerstone for future research into the more complex, heterogeneous
multi-agent systems.

</details>


### [102] [Patient-Zero: A Unified Framework for Real-Record-Free Patient Agent Generation](https://arxiv.org/abs/2509.11078)
*Yunghwei Lai,Weizhi Ma,Yang Liu*

Main category: cs.AI

TL;DR: 提出了Patient-Zero框架，无需真实医疗记录即可生成逼真的虚拟患者，通过医学知识注入和动态更新机制实现高准确性、多样性和一致性。


<details>
  <summary>Details</summary>
Motivation: 解决现有LLM生成医疗数据方法在隐私保护、准确性、多样性方面的局限性，以及缺乏真实患者交互能力的问题。

Method: 采用医学对齐的多步生成架构，通过分层医学知识注入构建完整患者记录；设计动态更新机制优化虚拟患者的交互能力和一致性。

Result: 实验表明模型在准确性、多样性和一致性方面表现良好，使用生成数据训练后模型在MedQA数据集上显著提升。

Conclusion: Patient-Zero框架能够生成高质量虚拟患者数据，为医疗AI应用提供有效的合成数据解决方案。

Abstract: Synthetic data generation using large language models (LLMs) has emerged as a
promising solution across various domains, particularly in medical field, to
mitigate data collection challenges. However, existing studies mainly utilize
LLMs to rewrite and complete existing medical records, where the limitations in
data privacy, accuracy, and diversity sill exist, and additionally lack the
ability to interact like real patients. To address these issues, we propose a
realistic patient generation framework, Patient-Zero, which requires no real
medical records. Patient-Zero first introduces a medically-aligned multi-step
generation architecture, which builds comprehensive patient records through
hierarchical medical knowledge injection without real medical records. Then, to
optimize the virtual patient's interaction abilities with humans, Patient-Zero
designs a dynamic updating mechanism to improve the consistency and
conversational performance. Our framework enables the generation of
contextually diverse patient records while maintaining strict medical
coherence, supported by adaptive dialogue strategies and real-time clinical
plausibility verification. Experimental results demonstrate that our model
achieves good performance in accuracy, diversity, and consistency. After
training with our generated virtual patients, existing models show significant
improvements on the MedQA dataset.

</details>


### [103] [Difficulty-Aware Agent Orchestration in LLM-Powered Workflows](https://arxiv.org/abs/2509.11079)
*Jinwei Su,Yinghui Xia,Qizhen Lan,Xinyuan Song,Yang Jingsong,Lewei He,Tianyu Shi*

Main category: cs.AI

TL;DR: DAAO是一个动态的多智能体框架，通过难度感知的工作流编排、算子分配和LLM路由，实现了对查询难度的自适应处理，在准确性和推理效率上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体框架使用静态或任务级工作流，无法根据查询难度动态调整，导致简单查询过度处理、复杂查询性能不足，且忽略了异构LLM的效率-性能权衡。

Method: 提出Difficulty-Aware Agentic Orchestration (DAAO)框架，包含三个模块：变分自编码器(VAE)用于难度估计、模块化算子分配器、成本和性能感知的LLM路由器，动态调整工作流深度、算子选择和LLM分配。

Result: 在六个基准测试中，DAAO在准确性和推理效率方面均优于先前的多智能体系统。

Conclusion: DAAO通过动态难度感知的工作流编排和异构LLM的智能利用，实现了更精细化的查询特定推理策略，为多智能体系统提供了有效的效率-性能平衡方案。

Abstract: Large Language Model (LLM)-based agentic systems have shown strong
capabilities across various tasks. However, existing multi-agent frameworks
often rely on static or task-level workflows, which either over-process simple
queries or underperform on complex ones, while also neglecting the
efficiency-performance trade-offs across heterogeneous LLMs. To address these
limitations, we propose Difficulty-Aware Agentic Orchestration (DAAO), a
dynamic framework that adapts workflow depth, operator selection, and LLM
assignment based on the difficulty of each input query. DAAO comprises three
interdependent modules: a variational autoencoder (VAE) for difficulty
estimation, a modular operator allocator, and a cost- and performance-aware LLM
router. By leveraging heterogeneous LLMs and dynamically tailoring workflows,
DAAO enables fine-grained, query-specific reasoning strategies. DAAO
outperforms prior multi-agent systems in both accuracy and inference efficiency
across six benchmarks. We will release our code and implementation details upon
publication.

</details>


### [104] [Neural cellular automata: applications to biology and beyond classical AI](https://arxiv.org/abs/2509.11131)
*Benedikt Hartl,Michael Levin,Léo Pio-Lopez*

Main category: cs.AI

TL;DR: 神经细胞自动机(NCA)是一种结合神经网络和细胞自动机的多尺度建模框架，能够模拟生物自组织过程并展示出强大的适应性和泛化能力，在生物建模、机器人控制和生成式AI等领域具有广泛应用前景。


<details>
  <summary>Details</summary>
Motivation: NCA框架旨在通过可训练的局部决策规则来捕捉生物系统的自适应自调节动力学，为多尺度生物过程（如进化、发育、再生等）提供统一的建模范式，并探索其在生物工程和人工智能中的潜在应用。

Method: NCA将人工神经网络作为局部决策中心，通过可微分或可进化的更新规则来模拟局部化智能体之间的相互作用，实现从分子到系统级别的多尺度建模。

Result: NCA不仅能够重现生物启发模式，还能在新条件下泛化，表现出对扰动的鲁棒性和开放式适应能力，在机器人形态控制、再生任务甚至ARC-AGI推理任务中展示出强大的目标导向动力学。

Conclusion: NCA构成了一个计算简洁的统一范式，不仅连接了多尺度生物学与现代生成式AI，还有潜力设计出真正具有生物启发性的集体智能系统，能够进行分层推理和控制。

Abstract: Neural Cellular Automata (NCA) represent a powerful framework for modeling
biological self-organization, extending classical rule-based systems with
trainable, differentiable (or evolvable) update rules that capture the adaptive
self-regulatory dynamics of living matter. By embedding Artificial Neural
Networks (ANNs) as local decision-making centers and interaction rules between
localized agents, NCA can simulate processes across molecular, cellular,
tissue, and system-level scales, offering a multiscale competency architecture
perspective on evolution, development, regeneration, aging, morphogenesis, and
robotic control. These models not only reproduce biologically inspired target
patterns but also generalize to novel conditions, demonstrating robustness to
perturbations and the capacity for open-ended adaptation and reasoning. Given
their immense success in recent developments, we here review current literature
of NCAs that are relevant primarily for biological or bioengineering
applications. Moreover, we emphasize that beyond biology, NCAs display robust
and generalizing goal-directed dynamics without centralized control, e.g., in
controlling or regenerating composite robotic morphologies or even on
cutting-edge reasoning tasks such as ARC-AGI-1. In addition, the same
principles of iterative state-refinement is reminiscent to modern generative
Artificial Intelligence (AI), such as probabilistic diffusion models. Their
governing self-regulatory behavior is constraint to fully localized
interactions, yet their collective behavior scales into coordinated
system-level outcomes. We thus argue that NCAs constitute a unifying
computationally lean paradigm that not only bridges fundamental insights from
multiscale biology with modern generative AI, but have the potential to design
truly bio-inspired collective intelligence capable of hierarchical reasoning
and control.

</details>


### [105] [AlignKT: Explicitly Modeling Knowledge State for Knowledge Tracing with Ideal State Alignment](https://arxiv.org/abs/2509.11135)
*Jing Xiao,Chang You,Zhiyu Chen*

Main category: cs.AI

TL;DR: AlignKT是一个用于知识追踪的前后端架构模型，通过将初步知识状态与基于教学理论的理想知识状态对齐，提高模型的可解释性和教学支持能力。


<details>
  <summary>Details</summary>
Motivation: 现有知识追踪模型主要关注拟合学习者交互序列，而忽视了知识状态本身，导致可解释性不足和教学支持不充分。

Method: 采用前端到后端架构，定义基于教学理论的理想知识状态作为对齐标准，使用五个编码器实现，并加入对比学习模块增强对齐鲁棒性。

Result: 在三个真实数据集上超越七个基线模型，在两个数据集上达到最先进性能，在第三个数据集上表现具有竞争力。

Conclusion: AlignKT通过显式建模稳定的知识状态，显著提升了知识追踪模型的性能和可解释性，为智能辅导系统提供了更好的教学支持。

Abstract: Knowledge Tracing (KT) serves as a fundamental component of Intelligent
Tutoring Systems (ITS), enabling these systems to monitor and understand
learners' progress by modeling their knowledge state. However, many existing KT
models primarily focus on fitting the sequences of learners' interactions, and
often overlook the knowledge state itself. This limitation leads to reduced
interpretability and insufficient instructional support from the ITS. To
address this challenge, we propose AlignKT, which employs a frontend-to-backend
architecture to explicitly model a stable knowledge state. In this approach,
the preliminary knowledge state is aligned with an additional criterion.
Specifically, we define an ideal knowledge state based on pedagogical theories
as the alignment criterion, providing a foundation for interpretability. We
utilize five encoders to implement this set-up, and incorporate a contrastive
learning module to enhance the robustness of the alignment process. Through
extensive experiments, AlignKT demonstrates superior performance, outperforming
seven KT baselines on three real-world datasets. It achieves state-of-the-art
results on two of these datasets and exhibits competitive performance on the
third. The code of this work is available at
https://github.com/SCNU203/AlignKT.

</details>


### [106] [AI-Generated Content in Cross-Domain Applications: Research Trends, Challenges and Propositions](https://arxiv.org/abs/2509.11151)
*Jianxin Li,Liang Qu,Taotao Cai,Zhixue Zhao,Nur Al Hasan Haldar,Aneesh Krishna,Xiangjie Kong,Flavio Romero Macau,Tanmoy Chakraborty,Aniket Deroy,Binshan Lin,Karen Blackmore,Nasimul Noman,Jingxian Cheng,Ningning Cui,Jianliang Xu*

Main category: cs.AI

TL;DR: 该论文提供了人工智能生成内容(AIGC)的跨领域视角，统计了生成式AI训练技术、检测方法、社会影响和技术挑战，为未来研究提供指导。


<details>
  <summary>Details</summary>
Motivation: 当前少有研究深入探讨AIGC在不同领域的最新进展和挑战，需要跨领域视角来完善AIGC的研究体系。

Method: 聚集16位来自不同学科的学者，通过三方面贡献：提供AIGC全局概览、分析各领域社会影响、讨论关键技术挑战并提出研究建议。

Result: 构建了一个跨领域的AIGC研究框架，包含技术、检测、平台使用、社会影响等多维度分析，为识别当前研究趋势和挑战提供了系统性视角。

Conclusion: 该视野性论文为AIGC领域提供了跨学科的综合分析，显示了AIGC在各行业的应用潜力和挑战，为未来研究指明了方向。

Abstract: Artificial Intelligence Generated Content (AIGC) has rapidly emerged with the
capability to generate different forms of content, including text, images,
videos, and other modalities, which can achieve a quality similar to content
created by humans. As a result, AIGC is now widely applied across various
domains such as digital marketing, education, and public health, and has shown
promising results by enhancing content creation efficiency and improving
information delivery. However, there are few studies that explore the latest
progress and emerging challenges of AIGC across different domains. To bridge
this gap, this paper brings together 16 scholars from multiple disciplines to
provide a cross-domain perspective on the trends and challenges of AIGC.
Specifically, the contributions of this paper are threefold: (1) It first
provides a broader overview of AIGC, spanning the training techniques of
Generative AI, detection methods, and both the spread and use of AI-generated
content across digital platforms. (2) It then introduces the societal impacts
of AIGC across diverse domains, along with a review of existing methods
employed in these contexts. (3) Finally, it discusses the key technical
challenges and presents research propositions to guide future work. Through
these contributions, this vision paper seeks to offer readers a cross-domain
perspective on AIGC, providing insights into its current research trends,
ongoing challenges, and future directions.

</details>


### [107] [VideoAgent: Personalized Synthesis of Scientific Videos](https://arxiv.org/abs/2509.11253)
*Xiao Liang,Bangxin Li,Zixuan Chen,Hanyue Zheng,Zhi Ma,Di Wang,Cong Tian,Quan Wang*

Main category: cs.AI

TL;DR: VideoAgent是一个多智能体框架，通过对话界面自动生成个性化的科学视频，将论文解析为细粒度资源库，并编排叙事流程来合成静态幻灯片和动态动画，显著优于现有商业科学视频生成服务。


<details>
  <summary>Details</summary>
Motivation: 现有的文档自动化工作主要关注静态媒体（如海报和幻灯片），缺乏个性化动态编排和多模态内容同步机制，难以实现有效的科学知识传播。

Method: 提出VideoAgent多智能体框架，通过解析源论文创建细粒度资源库，根据用户需求编排叙事流程，合成静态幻灯片和动态动画；同时开发SciVidEval评估套件，结合多模态内容质量自动指标和基于视频测验的人工评估。

Result: 大量实验表明，该方法显著优于现有商业科学视频生成服务，在科学传播方面达到接近人类水平的质量。

Conclusion: VideoAgent框架成功解决了科学视频自动生成的挑战，实现了个性化动态编排和多模态内容同步，为科学知识传播提供了有效的自动化解决方案。

Abstract: Automating the generation of scientific videos is a crucial yet challenging
task for effective knowledge dissemination. However, existing works on document
automation primarily focus on static media such as posters and slides, lacking
mechanisms for personalized dynamic orchestration and multimodal content
synchronization. To address these challenges, we introduce VideoAgent, a novel
multi-agent framework that synthesizes personalized scientific videos through a
conversational interface. VideoAgent parses a source paper into a fine-grained
asset library and, guided by user requirements, orchestrates a narrative flow
that synthesizes both static slides and dynamic animations to explain complex
concepts. To enable rigorous evaluation, we also propose SciVidEval, the first
comprehensive suite for this task, which combines automated metrics for
multimodal content quality and synchronization with a Video-Quiz-based human
evaluation to measure knowledge transfer. Extensive experiments demonstrate
that our method significantly outperforms existing commercial scientific video
generation services and approaches human-level quality in scientific
communication.

</details>


### [108] [Prompts to Proxies: Emulating Human Preferences via a Compact LLM Ensemble](https://arxiv.org/abs/2509.11311)
*Bingchen Wang,Zi-Yu Khoo,Bryan Kian Hsiang Low*

Main category: cs.AI

TL;DR: 使用LLM作为人类调查应答者的代理，通过两阶段对齐框架解决社会科学调查成本高和人口结构失衡问题


<details>
  <summary>Details</summary>
Motivation: 解决社会科学中调查部署成本不断上升和调查数据人口结构失衡的挑战，提供更经济高效的解决方案

Method: 提出P2P系统，通过结构化提示工程、基于熵的采样和回归选择方法，构建多样化的代理人设并选择代表性子集近似真实人群

Result: 在真实意见调查数据集上验证，对齐后的代理人群能高保真度重现聚合响应模式，展现出实质性响应多样性，无需人口统计条件

Conclusion: 该框架不仅提高了社会科学研究的数据效率，还为研窋对齐操作化提供了实验床，具有良好的普遍性和简洁性

Abstract: Large language models (LLMs) have demonstrated promise in emulating
human-like responses across a wide range of tasks. In this paper, we propose a
novel alignment framework that treats LLMs as agent proxies for human survey
respondents, affording a cost-effective and steerable solution to two pressing
challenges in the social sciences: the rising cost of survey deployment and the
growing demographic imbalance in survey response data. Drawing inspiration from
the theory of revealed preference, we formulate alignment as a two-stage
problem: constructing diverse agent personas called endowments that simulate
plausible respondent profiles, and selecting a representative subset to
approximate a ground-truth population based on observed data. To implement the
paradigm, we introduce P2P, a system that steers LLM agents toward
representative behavioral patterns using structured prompt engineering,
entropy-based sampling, and regression-based selection. Unlike
personalization-heavy approaches, our alignment approach is
demographic-agnostic and relies only on aggregate survey results, offering
better generalizability and parsimony. Beyond improving data efficiency in
social science research, our framework offers a testbed for studying the
operationalization of pluralistic alignment. We demonstrate the efficacy of our
approach on real-world opinion survey datasets, showing that our aligned agent
populations can reproduce aggregate response patterns with high fidelity and
exhibit substantial response diversity, even without demographic conditioning.

</details>


### [109] [Decoding Plastic Toxicity: An Intelligent Framework for Conflict-Aware Relational Metapath Extraction from Scientific Abstracts](https://arxiv.org/abs/2509.11330)
*Sudeshna Jana,Manjira Sinha,Tirthankar Dasgupta*

Main category: cs.AI

TL;DR: 使用大语言模型从科学摘要中提取循环感染物源到健康影响的关系元路径，构建毒性轨迹图追踪污染物传播，并通过动态证据调解模块确保知识一致性。


<details>
  <summary>Details</summary>
Motivation: 微粒子和纳粒塑料在环境中积累导致严重健康风险，需要从科学文本中挖掘可靠的因果关系知识来进行风险评估。

Method: 提出一种新框架，利用大语言模型提取关系元路径（多跳语义链），构建毒性轨迹图追踪污染物传播，并集成动态证据调解模块解决语义冲突。

Result: 方法在从噪声科学文本中提取可靠、高效用的关系知识方面表现出艰强性能，为领域特定语料库中挖掘复杂因果结构提供了可扩展的解决方案。

Conclusion: 该研究为挖掘微粒塑料污染与健康风险之间的复杂因果关系提供了一种可扩展的方法，通过结构化的知识提取和证据调解机制，有助于更准确地评估环境污染物的健康影响。

Abstract: The widespread use of plastics and their persistence in the environment have
led to the accumulation of micro- and nano-plastics across air, water, and
soil, posing serious health risks including respiratory, gastrointestinal, and
neurological disorders. We propose a novel framework that leverages large
language models to extract relational metapaths, multi-hop semantic chains
linking pollutant sources to health impacts, from scientific abstracts. Our
system identifies and connects entities across diverse contexts to construct
structured relational metapaths, which are aggregated into a Toxicity
Trajectory Graph that traces pollutant propagation through exposure routes and
biological systems. Moreover, to ensure consistency and reliability, we
incorporate a dynamic evidence reconciliation module that resolves semantic
conflicts arising from evolving or contradictory research findings. Our
approach demonstrates strong performance in extracting reliable, high-utility
relational knowledge from noisy scientific text and offers a scalable solution
for mining complex cause-effect structures in domain-specific corpora.

</details>


### [110] [The power of dynamic causality in observer-based design for soft sensor applications](https://arxiv.org/abs/2509.11336)
*William Farlessyost,Sebastian Oberst,Shweta Singh*

Main category: cs.AI

TL;DR: 通过动态因果分析优化观测器基于的软件传感器，利用LTC网络识别和剥离对状态估计因果影响最小的传感器输入，实现更精简、更准确的传感器选择。


<details>
  <summary>Details</summary>
Motivation: 传统传感器选择方法依赖线性观测性指数戗统计相关性，无法抓取复杂系统的时间演化特征，需要一种能够分析动态因果关系的新方法。

Method: 采用液体时间常数(LTC)网络，通过迭代工作流：训练LTC观测器、通过受控扰动分析量化输入因果影响、移除可忽略输入、重新训练直到性能下降。在三个机械系统测试平台上验证。

Result: 因果导向的剥离能一致识别出与基础物理一致的最小传感器集，同时提高预测准确性，能够自动区分必要物理测量与噪声，判断派生交互项的补充性与冗余性。

Conclusion: 该框架通过基于动态因果关系而非静态相关性来基础传感器选择决策，提高了可解释性，在过程工程、生态监测和农业领域的软件传感应用中具有重要价值。

Abstract: This paper introduces a novel framework for optimizing observer-based soft
sensors through dynamic causality analysis. Traditional approaches to sensor
selection often rely on linearized observability indices or statistical
correlations that fail to capture the temporal evolution of complex systems. We
address this gap by leveraging liquid-time constant (LTC) networks,
continuous-time neural architectures with input-dependent time constants, to
systematically identify and prune sensor inputs with minimal causal influence
on state estimation. Our methodology implements an iterative workflow: training
an LTC observer on candidate inputs, quantifying each input's causal impact
through controlled perturbation analysis, removing inputs with negligible
effect, and retraining until performance degradation occurs. We demonstrate
this approach on three mechanistic testbeds representing distinct physical
domains: a harmonically forced spring-mass-damper system, a nonlinear
continuous stirred-tank reactor, and a predator-prey model following the
structure of the Lotka-Volterra model, but with seasonal forcing and added
complexity. Results show that our causality-guided pruning consistently
identifies minimal sensor sets that align with underlying physics while
improving prediction accuracy. The framework automatically distinguishes
essential physical measurements from noise and determines when derived
interaction terms provide complementary versus redundant information. Beyond
computational efficiency, this approach enhances interpretability by grounding
sensor selection decisions in dynamic causal relationships rather than static
correlations, offering significant benefits for soft sensing applications
across process engineering, ecological monitoring, and agricultural domains.

</details>


### [111] [MAPGD: Multi-Agent Prompt Gradient Descent for Collaborative Prompt Optimization](https://arxiv.org/abs/2509.11361)
*Yichen Han,Bojun Liu,Zhengpeng zhou,Guanyu Liu,Zeng Zhang,Yang Yang,Wenli Wang,Isaac N Shi,Yunyan,Lewei He,Tianyu Shi*

Main category: cs.AI

TL;DR: MAPGD是一个多智能体提示梯度下降框架，通过多智能体协作和梯度优化解决传统单轨迹提示工程的局限性，在准确性、效率和鲁棒性方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有提示工程方法依赖单一优化轨迹，存在适应性差、效率低、视角狭窄、梯度冲突和计算成本高等问题，需要更有效的优化框架。

Method: MAPGD整合多智能体协作与梯度优化，包含任务清晰度、示例选择、格式设计和风格优化等专门智能体，采用语义梯度协调解决冲突，基于bandit的候选选择实现高效探索-利用平衡，并提供理论收敛保证。

Result: 在分类、生成和推理任务上的实验表明，MAPGD在准确性和效率方面优于单智能体和随机基线方法，消融实验证实了梯度融合、智能体专业化和冲突解决的有效性。

Conclusion: MAPGD提供了一个统一、梯度启发的多智能体方法，实现了鲁棒且可解释的提示优化，为提示工程提供了新的有效解决方案。

Abstract: Prompt engineering is crucial for leveraging large language models (LLMs),
but existing methods often rely on a single optimization trajectory, limiting
adaptability and efficiency while suffering from narrow perspectives, gradient
conflicts, and high computational cost. We propose MAPGD (Multi-Agent Prompt
Gradient Descent), a framework integrating multi-agent collaboration with
gradient-based optimization. MAPGD features specialized agents for task
clarity, example selection, format design, and stylistic refinement; semantic
gradient coordination to resolve conflicts; bandit-based candidate selection
for efficient exploration-exploitation; and theoretical convergence guarantees.
Experiments on classification, generation, and reasoning tasks show MAPGD
outperforms single-agent and random baselines in accuracy and efficiency.
Ablations confirm the benefits of gradient fusion, agent specialization, and
conflict resolution, providing a unified, gradient-inspired multi-agent
approach to robust and interpretable prompt optimization.

</details>


### [112] [Securing AI Agents: Implementing Role-Based Access Control for Industrial Applications](https://arxiv.org/abs/2509.11431)
*Aadil Gani Ganie*

Main category: cs.AI

TL;DR: 该论文提出了一个基于角色的访问控制(RBAC)框架，用于保护AI代理免受安全威胁，特别是提示注入攻击，以支持AI代理在企业环境中的有效和可扩展部署。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型(LLMs)存在训练数据静态和通用性限制，AI代理虽然通过访问外部工具和实时数据缓解了这些问题，但在工业应用中仍然面临安全威胁，特别是提示注入攻击，这对其完整性和可靠性构成重大风险。

Method: 提出了一个集成基于角色的访问控制(RBAC)到AI代理中的框架，为AI代理提供强大的安全防护措施，特别关注本地部署的实现方案。

Result: 该框架旨在为AI代理提供安全防护，防止未经授权的访问和恶意攻击，确保AI代理在企业环境中的可靠运行。

Conclusion: 通过集成RBAC安全框架，可以有效解决AI代理面临的安全挑战，支持AI代理在工业环境中的安全、有效和可扩展部署，特别是在需要高度安全性的本地部署场景中。

Abstract: The emergence of Large Language Models (LLMs) has significantly advanced
solutions across various domains, from political science to software
development. However, these models are constrained by their training data,
which is static and limited to information available up to a specific date.
Additionally, their generalized nature often necessitates fine-tuning --
whether for classification or instructional purposes -- to effectively perform
specific downstream tasks. AI agents, leveraging LLMs as their core, mitigate
some of these limitations by accessing external tools and real-time data,
enabling applications such as live weather reporting and data analysis. In
industrial settings, AI agents are transforming operations by enhancing
decision-making, predictive maintenance, and process optimization. For example,
in manufacturing, AI agents enable near-autonomous systems that boost
productivity and support real-time decision-making. Despite these advancements,
AI agents remain vulnerable to security threats, including prompt injection
attacks, which pose significant risks to their integrity and reliability. To
address these challenges, this paper proposes a framework for integrating
Role-Based Access Control (RBAC) into AI agents, providing a robust security
guardrail. This framework aims to support the effective and scalable deployment
of AI agents, with a focus on on-premises implementations.

</details>


### [113] [Knowledge-Guided Adaptive Mixture of Experts for Precipitation Prediction](https://arxiv.org/abs/2509.11459)
*Chen Jiang,Kofi Osei,Sai Deepthi Yeddula,Dongji Feng,Wei-Shinn Ku*

Main category: cs.AI

TL;DR: 提出自适应专家混合模型用于降水率预测，解决多源异构气象数据整合难题，在飓风伊恩数据集上显著优于基线方法


<details>
  <summary>Details</summary>
Motivation: 降水预测对农业和灾害管理至关重要，但多源观测数据（雷达、卫星、地面测量）在时空分辨率和特征上存在异质性，传统深度学习模型难以有效整合这些异构模态数据

Method: 自适应专家混合模型，每个专家专注于特定模态或时空模式，采用动态路由器学习将输入分配给最相关的专家，并开发了交互式可视化工具

Result: 在2022年飓风伊恩的多模态气候数据集上评估，自适应MoE模型在预测准确性方面显著优于所有基线方法

Conclusion: 模块化设计不仅提高了预测准确性，还增强了模型的可解释性，同时可视化工具支持气候敏感领域利益相关者的决策制定

Abstract: Accurate precipitation forecasting is indispensable in agriculture, disaster
management, and sustainable strategies. However, predicting rainfall has been
challenging due to the complexity of climate systems and the heterogeneous
nature of multi-source observational data, including radar, satellite imagery,
and surface-level measurements. The multi-source data vary in spatial and
temporal resolution, and they carry domain-specific features, making it
challenging for effective integration in conventional deep learning models.
Previous research has explored various machine learning techniques for weather
prediction; however, most struggle with the integration of data with
heterogeneous modalities. To address these limitations, we propose an Adaptive
Mixture of Experts (MoE) model tailored for precipitation rate prediction. Each
expert within the model specializes in a specific modality or spatio-temporal
pattern. We also incorporated a dynamic router that learns to assign inputs to
the most relevant experts. Our results show that this modular design enhances
predictive accuracy and interpretability. In addition to the modeling
framework, we introduced an interactive web-based visualization tool that
enables users to intuitively explore historical weather patterns over time and
space. The tool was designed to support decision-making for stakeholders in
climate-sensitive sectors. We evaluated our approach using a curated multimodal
climate dataset capturing real-world conditions during Hurricane Ian in 2022.
The benchmark results show that the Adaptive MoE significantly outperformed all
the baselines.

</details>


### [114] [Cross-Platform Scaling of Vision-Language-Action Models from Edge to Cloud GPUs](https://arxiv.org/abs/2509.11480)
*Amir Taherin,Juyi Lin,Arash Akbari,Arman Akbari,Pu Zhao,Weiwei Chen,David Kaeli,Yanzhi Wang*

Main category: cs.AI

TL;DR: 这篇论文评估了5种代表性VLA模型在边缘和数据中心GPU平台的性能扩展、系统指标和功耗情况，发现架构选择和功耗约束对性能有显著影响，并挖掘了高吞向量优化潜力。


<details>
  <summary>Details</summary>
Motivation: VLA模型作为机器人控制的通用策略强大但性能扩展特性、硬件平台适配性和功耗预算仍不明确，需要系统性评估以提供部署选择优化指南。

Method: 使用LIBERO基准测试5种代表性VLA模型（包括独创架构），在边缘功耗约束和高性能数据中心GPU配置下测量准确率、延迟、吞吐量、峰值内存使用等系统级指标。

Result: 发现三个关键趋势：1）架构选择如动作标记化和模型背榜大小强力影响吞吐量和内存占用；2）功耗约束边缘设备存在非线性性能泄溢，某些配置性能超过旧版数据中心GPU；3）可以在不严重损失准确性的前提下实现高吞吐量变体。

Conclusion: 研究给出了在不同部署约束下选择和优化VLA模型的可行性建议，并挑战了当前认为数据中心硬件在机器人推理中优势的假设。

Abstract: Vision-Language-Action (VLA) models have emerged as powerful generalist
policies for robotic control, yet their performance scaling across model
architectures and hardware platforms, as well as their associated power
budgets, remain poorly understood. This work presents an evaluation of five
representative VLA models -- spanning state-of-the-art baselines and two newly
proposed architectures -- targeting edge and datacenter GPU platforms. Using
the LIBERO benchmark, we measure accuracy alongside system-level metrics,
including latency, throughput, and peak memory usage, under varying edge power
constraints and high-performance datacenter GPU configurations. Our results
identify distinct scaling trends: (1) architectural choices, such as action
tokenization and model backbone size, strongly influence throughput and memory
footprint; (2) power-constrained edge devices exhibit non-linear performance
degradation, with some configurations matching or exceeding older datacenter
GPUs; and (3) high-throughput variants can be achieved without significant
accuracy loss. These findings provide actionable insights when selecting and
optimizing VLAs across a range of deployment constraints. Our work challenges
current assumptions about the superiority of datacenter hardware for robotic
inference.

</details>


### [115] [MedicalOS: An LLM Agent based Operating System for Digital Healthcare](https://arxiv.org/abs/2509.11507)
*Jared Zhu,Junde Wu*

Main category: cs.AI

TL;DR: MedicalOS是一个基于代理的医疗操作系统，通过自然语言指令将人类指令转换为预定义的医疗命令，实现临床工作流程自动化。


<details>
  <summary>Details</summary>
Motivation: 现有数字医疗系统操作复杂，医护人员需要管理多个工具、重复手动操作，花费大量时间在行政事务而非患者护理上。LLM代理的发展为通过自然语言与系统交互提供了可能，但医疗领域需要遵循临床指南的特定抽象层。

Method: 开发MedicalOS作为医疗领域特定的抽象层，将人类指令转换为预定义的医疗命令（如患者查询、病史检索、检查管理等），这些命令被包装为现成工具使用机器语言（Python、API、MCP、Linux）。

Result: 在22个专科的214个病例上验证，显示出高诊断准确性、临床合理的检查请求，以及一致的结构化报告和药物推荐生成。

Conclusion: MedicalOS为临床实践中的工作流程自动化提供了一个可信赖且可扩展的基础。

Abstract: Decades' advances in digital health technologies, such as electronic health
records, have largely streamlined routine clinical processes. Yet, most these
systems are still hard to learn and use: Clinicians often face the burden of
managing multiple tools, repeating manual actions for each patient, navigating
complicated UI trees to locate functions, and spending significant time on
administration instead of caring for patients. The recent rise of large
language model (LLM) based agents demonstrates exceptional capability in coding
and computer operation, revealing the potential for humans to interact with
operating systems and software not by direct manipulation, but by instructing
agents through natural language. This shift highlights the need for an
abstraction layer, an agent-computer interface, that translates human language
into machine-executable commands. In digital healthcare, however, requires a
more domain-specific abstractions that strictly follow trusted clinical
guidelines and procedural standards to ensure safety, transparency, and
compliance. To address this need, we present \textbf{MedicalOS}, a unified
agent-based operational system designed as such a domain-specific abstract
layer for healthcare. It translates human instructions into pre-defined digital
healthcare commands, such as patient inquiry, history retrieval, exam
management, report generation, referrals, treatment planning, that we wrapped
as off-the-shelf tools using machine languages (e.g., Python, APIs, MCP,
Linux). We empirically validate MedicalOS on 214 patient cases across 22
specialties, demonstrating high diagnostic accuracy and confidence, clinically
sound examination requests, and consistent generation of structured reports and
medication recommendations. These results highlight MedicalOS as a trustworthy
and scalable foundation for advancing workflow automation in clinical practice.

</details>


### [116] [Task Decoding based on Eye Movements using Synthetic Data Augmentation](https://arxiv.org/abs/2509.11547)
*Shanmuka Sadhu,Arca Baran,Preeti Pandey,Ayush Kumar*

Main category: cs.AI

TL;DR: 通过使用合成数据生成器（CTGAN、CopulaGAN、Gretel AI）生成眼动数据来支持Yarbus假说，证明可以从眼动中解码任务类别，数据增幅后分类准确率从28.1%提升到82%


<details>
  <summary>Details</summary>
Motivation: 验证Yarbus的假说，即可以通过观察者的眼动解码其执行的任务，并提高传统机器学习算法在眼动数据解码任务中的性能

Method: 使用CTGAN、CopulaGAN和Gretel AI合成数据生成器在320个真实眼动数据样本上生成合成数据，将真实数据与合成数据结合进行任务分类识别

Result: 数据增幅后分类准确率显著提升：Random Forest从28.1%提升到82%（Inception Time），使用5倍数据增幅后达到最佳效果

Conclusion: 通过合成数据增幅可以显著提高眼动数据任务解码的准确性，强烈支持Yarbus假说，为眼动研究提供了有效的数据增幅方法

Abstract: Machine learning has been extensively used in various applications related to
eye-tracking research. Understanding eye movement is one of the most
significant subsets of eye-tracking research that reveals the scanning pattern
of an individual. Researchers have thoroughly analyzed eye movement data to
understand various eye-tracking applications, such as attention mechanisms,
navigational behavior, task understanding, etc. The outcome of traditional
machine learning algorithms used for decoding tasks based on eye movement data
has received a mixed reaction to Yarbus' claim that it is possible to decode
the observer's task from their eye movements. In this paper, to support the
hypothesis by Yarbus, we are decoding tasks categories while generating
synthetic data samples using well-known Synthetic Data Generators CTGAN and its
variations such as CopulaGAN and Gretel AI Synthetic Data generators on
available data from an in-person user study. Our results show that augmenting
more eye movement data combined with additional synthetically generated
improves classification accuracy even with traditional machine learning
algorithms. We see a significant improvement in task decoding accuracy from
28.1% using Random Forest to 82% using Inception Time when five times more data
is added in addition to the 320 real eye movement dataset sample. Our proposed
framework outperforms all the available studies on this dataset because of the
use of additional synthetic datasets. We validated our claim with various
algorithms and combinations of real and synthetic data to show how decoding
accuracy increases with the increase in the augmentation of generated data to
real data.

</details>


### [117] [Formal Reasoning for Intelligent QA Systems: A Case Study in the Educational Domain](https://arxiv.org/abs/2509.11572)
*Tuan Bui,An Nguyen,Phat Thai,Minh Hua,Ngan Pham L. N.,Ngan Pham T. B.,Dung Le,Long Nguyen,Thanh-Tung Tran,Thang Bui,Tho Quan*

Main category: cs.AI

TL;DR: MCFR是一个神经符号框架，将大语言模型与模型检测相结合，用于支持属性验证，提高闭域QA系统中的推理忠实性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM推理轨迹往往不忠实，只是看似合理的解释而非因果推导。符号引擎与LLM结合的方法局限于静态逻辑，难以处理动态、基于状态的多步推理。

Method: 提出MCFR框架，将自然语言转换为形式化规范，并在转移模型上进行验证。使用模型检测技术来支持属性验证。

Result: MCFR在EduMC-QA基准测试中表现出色，提高了推理的忠实性和可解释性，为高风险闭域应用中的可验证QA提供了可行路径。

Conclusion: MCFR框架通过整合LLM和模型检测技术，有效解决了动态状态推理的挑战，为闭域QA系统提供了更可靠的验证方法。

Abstract: Reasoning is essential for closed-domain QA systems in which procedural
correctness and policy compliance are critical. While large language models
(LLMs) have shown strong performance on many reasoning tasks, recent work
reveals that their reasoning traces are often unfaithful - serving more as
plausible justifications than as causally grounded derivations. Efforts to
combine LLMs with symbolic engines (e.g., Prover9, Z3) have improved
reliability but remain limited to static forms of logic, struggling with
dynamic, state-based reasoning such as multi-step progressions and conditional
transitions.
  In this paper, we propose MCFR (Model Checking for Formal Reasoning), a
neuro-symbolic framework that integrates LLMs with model checking to support
property verification. MCFR translates natural language into formal
specifications and verifies them over transition models. To support evaluation,
we introduce EduMC-QA, a benchmark dataset grounded in real academic
procedures. Our results show that MCFR improves reasoning faithfulness and
interpretability, offering a viable path toward verifiable QA in high-stakes
closed-domain applications. In addition to evaluating MCFR, we compare its
performance with state-of-the-art LLMs such as ChatGPT, DeepSeek, and Claude to
contextualize its effectiveness.

</details>


### [118] [A Survey of Reasoning and Agentic Systems in Time Series with Large Language Models](https://arxiv.org/abs/2509.11575)
*Ching Chang,Yidan Shi,Defu Cao,Wei Yang,Jeehyun Hwang,Haixin Wang,Jiacheng Pang,Wei Wang,Yan Liu,Wen-Chih Peng,Tien-Fu Chen*

Main category: cs.AI

TL;DR: 时间序列推理技术调研：定义了三种推理拓扑结构（直接推理、线性链推理、分支结构推理），跨越传统时间序列分析、解释理解、因果推断、生成等领域，并提供了数据集、评估方法和实践指南。


<details>
  <summary>Details</summary>
Motivation: 将时间作为一等轴心，通过中间证据直接融入答案来提升时间序列推理的可解释性和可靠性，进而支持理解、解释和在动态世界中行动。

Method: 通过推理拓扑结构组织文献：直接推理、线性链推理和分支结构推理。结合分解、验证、集成、工具使用、知识访问、多模态、代理循环等标签集。综述各领域方法和系统，分析各种拓扑结构的优缺点。

Result: 提供了精选的数据集、测试框架和资源，强调保持证据可见性和时间对齐的评估方法。细化了根据不确定性选择拓扑结构、通过可观测结果基础、策划假设假设和流式处理、将成本和延迟作为设计预算的实践指南。

Conclusion: 推理结构需在基础碰撞和自我纠正能力与计算成本、可复现性之间取得平衡。未来进展将依赖于将推理质量与实用性相结合的测试框架，以及在假设假设、流式和长期限设置下考虑成本与风险交易的闭环测试床。这些方向标志着从窄隘准确性向大规模可靠性的转变，以支持具有可追踪证据和可信结果的系统。

Abstract: Time series reasoning treats time as a first-class axis and incorporates
intermediate evidence directly into the answer. This survey defines the problem
and organizes the literature by reasoning topology with three families: direct
reasoning in one step, linear chain reasoning with explicit intermediates, and
branch-structured reasoning that explores, revises, and aggregates. The
topology is crossed with the main objectives of the field, including
traditional time series analysis, explanation and understanding, causal
inference and decision making, and time series generation, while a compact tag
set spans these axes and captures decomposition and verification, ensembling,
tool use, knowledge access, multimodality, agent loops, and LLM alignment
regimes. Methods and systems are reviewed across domains, showing what each
topology enables and where it breaks down in faithfulness or robustness, along
with curated datasets, benchmarks, and resources that support study and
deployment (https://github.com/blacksnail789521/Time-Series-Reasoning-Survey).
Evaluation practices that keep evidence visible and temporally aligned are
highlighted, and guidance is distilled on matching topology to uncertainty,
grounding with observable artifacts, planning for shift and streaming, and
treating cost and latency as design budgets. We emphasize that reasoning
structures must balance capacity for grounding and self-correction against
computational cost and reproducibility, while future progress will likely
depend on benchmarks that tie reasoning quality to utility and on closed-loop
testbeds that trade off cost and risk under shift-aware, streaming, and
long-horizon settings. Taken together, these directions mark a shift from
narrow accuracy toward reliability at scale, enabling systems that not only
analyze but also understand, explain, and act on dynamic worlds with traceable
evidence and credible outcomes.

</details>


### [119] [AMLNet: A Knowledge-Based Multi-Agent Framework to Generate and Detect Realistic Money Laundering Transactions](https://arxiv.org/abs/2509.11595)
*Sabin Huda,Ernest Foo,Zahra Jadidi,MA Hakim Newton,Abdul Sattar*

Main category: cs.AI

TL;DR: AMLNet是一个基于知识的多代理框架，包含监管感知的交易生成器和集成检测管道，用于生成符合监管要求的合成反洗钱交易数据并实现高精度检测。


<details>
  <summary>Details</summary>
Motivation: 反洗钱研究缺乏可公开共享且符合监管要求的交易数据集，这限制了AML实验的可重复性和监管一致性研究。

Method: 采用知识驱动的多代理框架：1)监管感知交易生成器生成包含洗钱核心阶段和高级类型的合成交易；2)集成检测管道进行洗钱检测。

Result: 生成1,090,173笔合成交易(约0.16%洗钱阳性)，监管对齐度达75%，技术保真度得分0.75。检测集成F1得分0.90(精度0.84，召回率0.97)，在外部数据集上也表现良好。

Conclusion: AMLNet提供了一个多维评估框架和公开数据集，推动了可重复且符合监管要求的反洗钱实验研究。

Abstract: Anti-money laundering (AML) research is constrained by the lack of publicly
shareable, regulation-aligned transaction datasets. We present AMLNet, a
knowledge-based multi-agent framework with two coordinated units: a
regulation-aware transaction generator and an ensemble detection pipeline. The
generator produces 1,090,173 synthetic transactions (approximately 0.16\%
laundering-positive) spanning core laundering phases (placement, layering,
integration) and advanced typologies (e.g., structuring, adaptive threshold
behavior). Regulatory alignment reaches 75\% based on AUSTRAC rule coverage
(Section 4.2), while a composite technical fidelity score of 0.75 summarizes
temporal, structural, and behavioral realism components (Section 4.4). The
detection ensemble achieves F1 0.90 (precision 0.84, recall 0.97) on the
internal test partitions of AMLNet and adapts to the external SynthAML dataset,
indicating architectural generalizability across different synthetic generation
paradigms. We provide multi-dimensional evaluation (regulatory, temporal,
network, behavioral) and release the dataset (Version 1.0,
https://doi.org/10.5281/zenodo.16736515), to advance reproducible and
regulation-conscious AML experimentation.

</details>


### [120] [Adapting and Evaluating Multimodal Large Language Models for Adolescent Idiopathic Scoliosis Self-Management: A Divide and Conquer Framework](https://arxiv.org/abs/2509.11645)
*Zhaolong Wu,Pu Luo,Jason Pui Yin Cheung,Teng Zhang*

Main category: cs.AI

TL;DR: 首次全面评估多模态大语言模型在青少年特发性脊柱侧弯自我管理中的应用，发现现有模型在脊柱X光解读和专业知识理解方面存在显著局限，通过关键点提示和知识库增强取得一定改进，但距离个性化辅助仍有较大差距。


<details>
  <summary>Details</summary>
Motivation: 评估多模态大语言模型在青少年特发性脊柱侧弯(AIS)自我管理中的实际能力，探索其在医疗辅助领域的应用潜力。

Method: 构建包含3000张X光片和诊断文本的数据库，采用'分而治之'框架评估5个MLLM模型，包括视觉问答、领域知识评估和患者教育咨询三个任务，并引入脊柱关键点提示和检索增强生成技术进行改进。

Result: MLLM在复杂脊柱X光解读和AIS护理知识理解方面存在明显局限，脊柱关键点提示对不同架构模型效果不一，RAG显著提升知识评估任务表现，但脊柱畸形位置检测最佳准确率仅0.55，方向检测仅0.13。

Conclusion: 当前多模态大语言模型尚无法胜任AIS护理的个性化辅助任务，主要挑战在于准确检测脊柱畸形位置和方向的能力不足。

Abstract: This study presents the first comprehensive evaluation of Multimodal Large
Language Models (MLLMs) for Adolescent Idiopathic Scoliosis (AIS)
self-management. We constructed a database of approximately 3,000
anteroposterior X-rays with diagnostic texts and evaluated five MLLMs through a
`Divide and Conquer' framework consisting of a visual question-answering task,
a domain knowledge assessment task, and a patient education counseling
assessment task. Our investigation revealed limitations of MLLMs' ability in
interpreting complex spinal radiographs and comprehending AIS care knowledge.
To address these, we pioneered enhancing MLLMs with spinal keypoint prompting
and compiled an AIS knowledge base for retrieval augmented generation (RAG),
respectively. Results showed varying effectiveness of visual prompting across
different architectures, while RAG substantially improved models' performances
on the knowledge assessment task. Our findings indicate current MLLMs are far
from capable in realizing personalized assistant in AIS care. The greatest
challenge lies in their abilities to obtain accurate detections of spinal
deformity locations (best accuracy: 0.55) and directions (best accuracy: 0.13).

</details>


### [121] [HeLoFusion: An Efficient and Scalable Encoder for Modeling Heterogeneous and Multi-Scale Interactions in Trajectory Prediction](https://arxiv.org/abs/2509.11719)
*Bingqing Wei,Lianmin Chen,Zhongyu Xia,Yongtao Wang*

Main category: cs.AI

TL;DR: HeLoFusion是一种高效的编码器，通过构建局部多尺度图来建模异构多智能体交互，在自动驾驶轨迹预测中实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以捕捉复杂社交动态的丰富性，特别是多尺度交互的共存和异构智能体的多样化行为。

Method: 构建以每个智能体为中心的局部多尺度图，采用聚合-分解消息传递方案和类型特定特征网络来处理智能体异构性。

Result: 在Waymo Open Motion数据集上实现了最先进的性能，在Soft mAP和minADE等关键指标上设立了新的基准。

Conclusion: 基于局部性、显式建模多尺度和异构交互的架构是推进运动预测的有效策略。

Abstract: Multi-agent trajectory prediction in autonomous driving requires a
comprehensive understanding of complex social dynamics. Existing methods,
however, often struggle to capture the full richness of these dynamics,
particularly the co-existence of multi-scale interactions and the diverse
behaviors of heterogeneous agents. To address these challenges, this paper
introduces HeLoFusion, an efficient and scalable encoder for modeling
heterogeneous and multi-scale agent interactions. Instead of relying on global
context, HeLoFusion constructs local, multi-scale graphs centered on each
agent, allowing it to effectively model both direct pairwise dependencies and
complex group-wise interactions (\textit{e.g.}, platooning vehicles or
pedestrian crowds). Furthermore, HeLoFusion tackles the critical challenge of
agent heterogeneity through an aggregation-decomposition message-passing scheme
and type-specific feature networks, enabling it to learn nuanced,
type-dependent interaction patterns. This locality-focused approach enables a
principled representation of multi-level social context, yielding powerful and
expressive agent embeddings. On the challenging Waymo Open Motion Dataset,
HeLoFusion achieves state-of-the-art performance, setting new benchmarks for
key metrics including Soft mAP and minADE. Our work demonstrates that a
locality-grounded architecture, which explicitly models multi-scale and
heterogeneous interactions, is a highly effective strategy for advancing motion
forecasting.

</details>


### [122] [Learning Representations in Video Game Agents with Supervised Contrastive Imitation Learning](https://arxiv.org/abs/2509.11880)
*Carlos Celemin,Joseph Brennan,Pierluigi Vito Amadori,Tim Bradley*

Main category: cs.AI

TL;DR: 本文提出将监督对比学习(SupCon)应用于模仿学习(IL)，通过在视频游戏环境中学习更有效的状态表示，以更好地捕捉动作相关因素和因果关系。


<details>
  <summary>Details</summary>
Motivation: 目标是获得能够更好捕捉动作相关因素的观测潜在表示，从而更好地建模从观测到演示者执行动作的因果关系（例如玩家在障碍物出现时跳跃）。

Method: 提出将SupCon损失与连续输出空间集成的方法，使SupCon能够在不受环境动作类型限制的情况下操作。

Result: 在3D游戏Astro Bot和Returnal以及多个2D Atari游戏上的实验显示，相比仅使用监督动作预测损失函数的基线模型，该方法提高了表示质量、加快了学习收敛速度并具有更好的泛化能力。

Conclusion: 监督对比学习可以有效地提升模仿学习中状态表示的质量和性能表现。

Abstract: This paper introduces a novel application of Supervised Contrastive Learning
(SupCon) to Imitation Learning (IL), with a focus on learning more effective
state representations for agents in video game environments. The goal is to
obtain latent representations of the observations that capture better the
action-relevant factors, thereby modeling better the cause-effect relationship
from the observations that are mapped to the actions performed by the
demonstrator, for example, the player jumps whenever an obstacle appears ahead.
We propose an approach to integrate the SupCon loss with continuous output
spaces, enabling SupCon to operate without constraints regarding the type of
actions of the environment. Experiments on the 3D games Astro Bot and Returnal,
and multiple 2D Atari games show improved representation quality, faster
learning convergence, and better generalization compared to baseline models
trained only with supervised action prediction loss functions.

</details>


### [123] [EgoMem: Lifelong Memory Agent for Full-duplex Omnimodal Models](https://arxiv.org/abs/2509.11914)
*Yiqun Yao,Naitong Yu,Xiang Li,Xin Jiang,Xuezhi Fang,Wenjia Ma,Xuying Meng,Jing Li,Aixin Sun,Yequan Wang*

Main category: cs.AI

TL;DR: EgoMem是首个为全双工模型设计的终身记忆代理，能够从原始视听流中实时识别多用户、提供个性化响应，并维护用户的长期知识记忆。


<details>
  <summary>Details</summary>
Motivation: 现有的记忆代理主要面向LLMs，无法处理实时全双工模型的视听流输入，需要开发专门针对终身、实时和具身场景的记忆系统。

Method: 采用三个异步进程：检索过程（动态识别用户并收集相关上下文）、全模态对话过程（生成个性化音频响应）、内存管理过程（检测对话边界并更新长期记忆）。

Result: 检索和内存管理模块准确率超过95%，与RoboEgo全模态聊天机器人集成后，实时个性化对话的事实一致性得分超过87%。

Conclusion: EgoMem为实时全双工模型建立了强大的终身记忆基准，完全基于原始视听流处理，适合终身学习和具身智能场景。

Abstract: We introduce EgoMem, the first lifelong memory agent tailored for full-duplex
models that process real-time omnimodal streams. EgoMem enables real-time
models to recognize multiple users directly from raw audiovisual streams, to
provide personalized response, and to maintain long-term knowledge of users'
facts, preferences, and social relationships extracted from audiovisual
history. EgoMem operates with three asynchronous processes: (i) a retrieval
process that dynamically identifies user via face and voice, and gathers
relevant context from a long-term memory; (ii) an omnimodal dialog process that
generates personalized audio responses based on the retrieved context; and
(iii) a memory management process that automatically detects dialog boundaries
from omnimodal streams, and extracts necessary information to update the
long-term memory. Unlike existing memory agents for LLMs, EgoMem relies
entirely on raw audiovisual streams, making it especially suitable for
lifelong, real-time, and embodied scenarios. Experimental results demonstrate
that EgoMem's retrieval and memory management modules achieve over 95% accuracy
on the test set. When integrated with a fine-tuned RoboEgo omnimodal chatbot,
the system achieves fact-consistency scores above 87% in real-time personalized
dialogs, establishing a strong baseline for future research.

</details>


### [124] [BuildingGym: An open-source toolbox for AI-based building energy management using reinforcement learning](https://arxiv.org/abs/2509.11922)
*Xilei Dai,Ruotian Chen,Songze Guan,Wen-Tai Li,Chau Yuen*

Main category: cs.AI

TL;DR: 建筑能源管理领域缺乏灵活的强化学习框架，研究人员开发了开源工具BuildingGym，通过集成EnergyPlus模拟器和外部信号接口，支持系统级和房间级控制，为建筑管理者和AI专家提供了灵活的RL算法训练平台。


<details>
  <summary>Details</summary>
Motivation: 建筑能源管理领域虽然强化学习已经证明有效，但缺乏一个灵活的框架来在各种控制问题中实现RL算法。

Method: 开发BuildingGym开源工具，集成EnergyPlus作为核心模拟器，支持外部信号输入，提供多种内置RL算法，支持系统级和房间级控制。

Result: 通过BuildingGym高效设置了制冷负荷管理训练任务，包括常规和动态制冷负荷管理。内置算法在两种任务中都表现出艰强的性能。

Conclusion: BuildingGym平台有效地为建筑能源管理领域提供了灵活的RL控制策略训练框架，帮助建筑管理者获得最优控制策略，同时为AI专家提供了算法实现和测试环境。

Abstract: Reinforcement learning (RL) has proven effective for AI-based building energy
management. However, there is a lack of flexible framework to implement RL
across various control problems in building energy management. To address this
gap, we propose BuildingGym, an open-source tool designed as a
research-friendly and flexible framework for training RL control strategies for
common challenges in building energy management. BuildingGym integrates
EnergyPlus as its core simulator, making it suitable for both system-level and
room-level control. Additionally, BuildingGym is able to accept external
signals as control inputs instead of taking the building as a stand-alone
entity. This feature makes BuildingGym applicable for more flexible
environments, e.g. smart grid and EVs community. The tool provides several
built-in RL algorithms for control strategy training, simplifying the process
for building managers to obtain optimal control strategies. Users can achieve
this by following a few straightforward steps to configure BuildingGym for
optimization control for common problems in the building energy management
field. Moreover, AI specialists can easily implement and test state-of-the-art
control algorithms within the platform. BuildingGym bridges the gap between
building managers and AI specialists by allowing for the easy configuration and
replacement of RL algorithms, simulators, and control environments or problems.
With BuildingGym, we efficiently set up training tasks for cooling load
management, targeting both constant and dynamic cooling load management. The
built-in algorithms demonstrated strong performance across both tasks,
highlighting the effectiveness of BuildingGym in optimizing cooling strategies.

</details>


### [125] [Neuromorphic Intelligence](https://arxiv.org/abs/2509.11940)
*Marcel van Gerven*

Main category: cs.AI

TL;DR: 该论文提出动力系统理论作为神经形态计算的统一理论框架，能够整合人工智能、神经科学、物理学等多学科知识，通过利用噪声作为学习资源和微分遗传编程来实现自适应行为。


<details>
  <summary>Details</summary>
Motivation: 神经形态计算旨在复制人脑的高效性和适应性，但缺乏统一的理论框架来整合多学科知识。论文旨在解决这一核心挑战，为神经形态智能系统提供理论基础。

Method: 采用动力系统理论作为理论基础，该理论基于微分计算，为建模推理、学习和控制提供原则性语言。通过利用噪声作为学习资源，并使用微分遗传编程来发现实现自适应行为的动力系统。

Result: 动力系统理论成功提供了一个能够桥接多学科的框架，使智能行为能够从物理基质的动力学中涌现，同时提高了AI的科学性和可持续性。

Conclusion: 动力系统理论为神经形态计算提供了坚实的理论基础，推动了神经形态智能的发展，使智能系统更加可持续、透明和易于访问，为AI的科学和可持续发展开辟了新途径。

Abstract: Neuromorphic computing seeks to replicate the remarkable efficiency,
flexibility, and adaptability of the human brain in artificial systems. Unlike
conventional digital approaches, which depend on massive computational and
energy resources, neuromorphic systems exploit brain-inspired principles of
computation to achieve orders of magnitude greater energy efficiency. By
drawing on insights from artificial intelligence, neuroscience, physics,
chemistry, and materials science, neuromorphic computing promises to deliver
intelligent systems that are sustainable, transparent, and widely accessible. A
central challenge, however, is to identify a unifying theoretical framework
capable of bridging these diverse disciplines. We argue that dynamical systems
theory provides such a foundation. Rooted in differential calculus, it offers a
principled language for modeling inference, learning, and control in both
natural and artificial substrates. Within this framework, noise can be
harnessed as a resource for learning, while differential genetic programming
enables the discovery of dynamical systems that implement adaptive behaviors.
Embracing this perspective paves the way toward emergent neuromorphic
intelligence, where intelligent behavior arises from the dynamics of physical
substrates, advancing both the science and sustainability of AI.

</details>


### [126] [How to Evaluate Medical AI](https://arxiv.org/abs/2509.11941)
*Ilia Kopanichuk,Petr Anokhin,Vladimir Shaposhnikov,Vladimir Makharev,Ekaterina Tsapieva,Iaroslav Bespalov,Dmitry V. Dylov,Ivan Oseledets*

Main category: cs.AI

TL;DR: 提出新的相对精度和相对回掉率指标（RPAD/RRAD），通过与多个专家意见对比来评估AI诊断性能，解决传统指标无法处理专家判断变异性的问题。


<details>
  <summary>Details</summary>
Motivation: 传统的精度和回掉率等评估指标无法考虑医学专家判断的本质变异性，导致对AI性能的评估不一致。虽然Cohen's Kappa等一致性统计量更可靠，但缺乏可解释性。

Method: 提出RPAD和RRAD指标，将AI输出与多个专家意见进行对比，而非单一参考标准。通过将性能归一化到专家间不一致性来提供更稳定和现实的诊断质量测量。使用自动化方法确定自由形式临床诊断的同一性。

Result: 在360个医疗对话上评估多个大语言模型，发现像DeepSeek-V3这样的顶级模型达到了与专家共识相当或更高的一致性。专家判断存在显著变异性，甚至高于AI与人类之间的差异。自动化诊断同一性确定方法达到了98%的准确率。

Conclusion: 专家判断的显著变性性强调了绝对指标的局限性，支持在医学AI中采用相对指标的必要性。新提出的RPAD/RRAD指标提供了更稳定和现实的AI诊断性能评估方法。

Abstract: The integration of artificial intelligence (AI) into medical diagnostic
workflows requires robust and consistent evaluation methods to ensure
reliability, clinical relevance, and the inherent variability in expert
judgments. Traditional metrics like precision and recall often fail to account
for the inherent variability in expert judgments, leading to inconsistent
assessments of AI performance. Inter-rater agreement statistics like Cohen's
Kappa are more reliable but they lack interpretability. We introduce Relative
Precision and Recall of Algorithmic Diagnostics (RPAD and RRAD) - a new
evaluation metrics that compare AI outputs against multiple expert opinions
rather than a single reference. By normalizing performance against inter-expert
disagreement, these metrics provide a more stable and realistic measure of the
quality of predicted diagnosis. In addition to the comprehensive analysis of
diagnostic quality measures, our study contains a very important side result.
Our evaluation methodology allows us to avoid selecting diagnoses from a
limited list when evaluating a given case. Instead, both the models being
tested and the examiners verifying them arrive at a free-form diagnosis. In
this automated methodology for establishing the identity of free-form clinical
diagnoses, a remarkable 98% accuracy becomes attainable. We evaluate our
approach using 360 medical dialogues, comparing multiple large language models
(LLMs) against a panel of physicians. Large-scale study shows that
top-performing models, such as DeepSeek-V3, achieve consistency on par with or
exceeding expert consensus. Moreover, we demonstrate that expert judgments
exhibit significant variability - often greater than that between AI and
humans. This finding underscores the limitations of any absolute metrics and
supports the need to adopt relative metrics in medical AI.

</details>


### [127] [Neuro-Symbolic Agents with Modal Logic for Autonomous Diagnostics](https://arxiv.org/abs/2509.11943)
*Antonin Sulc,Thorsten Hellert*

Main category: cs.AI

TL;DR: 提出了一种基于模态逻辑的神经符号多智能体架构，用于在复杂环境中进行可靠诊断和决策


<details>
  <summary>Details</summary>
Motivation: 当前AI研究主要关注模型和数据的规模化，但忽视了智能体在环境中推理结构、保真度和逻辑一致性的规模化需求。环境不是被动的测试场，而是需要智能体具备自适应、复杂和自主决策能力的挑战性条件。

Method: 采用神经符号多智能体架构，将个体智能体的信念状态形式化为Kripke模型，使用模态逻辑进行可能性和必然性推理。利用不可变的领域特定知识作为逻辑约束，指导语言模型的假设生成，防止得出物理或逻辑上不可行的结论。

Result: 在高保真模拟粒子加速器环境中，系统成功诊断了复杂的级联故障，结合了语言模型的强大语义直觉与模态逻辑的严格可验证性。

Conclusion: 该研究展示了构建更鲁棒、可靠和可验证自主智能体的可行路径，通过形式化逻辑约束与神经模型的结合，提升了智能体在复杂环境中的推理能力。

Abstract: The development of intelligent agents, particularly those powered by language
models (LMs), has shown the critical role in various environments that require
intelligent and autonomous decision. Environments are not passive testing
grounds and they represent the data required for agents to learn and exhibit
very challenging conditions that require adaptive, complex and autonomous
capacity to make decisions. While the paradigm of scaling models and datasets
has led to remarkable emergent capabilities, we argue that scaling the
structure, fidelity, and logical consistency of agent reasoning within these
environments is a crucial, yet underexplored, dimension of AI research. This
paper introduces a neuro-symbolic multi-agent architecture where the belief
states of individual agents are formally represented as Kripke models. This
foundational choice enables them to reason about known concepts of
\emph{possibility} and \emph{necessity} using the formal language of modal
logic. In this work, we use of immutable, domain-specific knowledge to make
infere information, which is encoded as logical constraints essential for
proper diagnosis. In the proposed model, we show constraints that actively
guide the hypothesis generation of LMs, effectively preventing them from
reaching physically or logically untenable conclusions. In a high-fidelity
simulated particle accelerator environment, our system successfully diagnoses
complex, cascading failures by combining the powerful semantic intuition of LMs
with the rigorous, verifiable validation of modal logic and a factual world
model and showcasing a viable path toward more robust, reliable, and verifiable
autonomous agents.

</details>


### [128] [Agentic Temporal Graph of Reasoning with Multimodal Language Models: A Potential AI Aid to Healthcare](https://arxiv.org/abs/2509.11944)
*Susanta Mitra*

Main category: cs.AI

TL;DR: 这篇论文提出了一种新的时序图基于多模态医疗推理框架，通过有向图模型化推理过程，支持回溯、精炼和动态调整推理内容，以提高医疗诊断的准确性。


<details>
  <summary>Details</summary>
Motivation: 医疗领域的多模态数据需要有效推理，但现有多模态推理模型在医疗诊断中的应用有限且准确性不足。需要一种能够处理多模态医疗数据、支持动态变化和提高诊断准确性的推理方法。

Method: 提出了一种时序图基于的推理过程，通过有向图模型化推理逻辑。该方法支持回溯调整、精炼推理内容、创建/删除推理条目。同时考虑不同时间点的多模态数据，并使用多代理时序推理框架进行任务分配和交叉验证。

Result: 基础实验和分析结果证明了该方法的新颖性和实际用途。该框架能够有效处理多模态医疗数据，跟踪分析患者健康状况和疾病进展，并通过交叉验证机制提高推理输出的准确性。

Conclusion: 该研究提出的时序图基于多模态医疗推理框架为医疗诊断提供了一种有效的解决方案，能够处理复杂的多模态数据，支持动态推理过程，并通过多代理协作提高诊断准确性，具有良好的应用前景。

Abstract: Healthcare and medicine are multimodal disciplines that deal with multimodal
data for reasoning and diagnosing multiple diseases. Although some multimodal
reasoning models have emerged for reasoning complex tasks in scientific
domains, their applications in the healthcare domain remain limited and fall
short in correct reasoning for diagnosis. To address the challenges of
multimodal medical reasoning for correct diagnosis and assist the healthcare
professionals, a novel temporal graph-based reasoning process modelled through
a directed graph has been proposed in the current work. It helps in
accommodating dynamic changes in reasons through backtracking, refining the
reasoning content, and creating new or deleting existing reasons to reach the
best recommendation or answer. Again, consideration of multimodal data at
different time points can enable tracking and analysis of patient health and
disease progression. Moreover, the proposed multi-agent temporal reasoning
framework provides task distributions and a cross-validation mechanism to
further enhance the accuracy of reasoning outputs. A few basic experiments and
analysis results justify the novelty and practical utility of the proposed
preliminary approach.

</details>


### [129] [MusicSwarm: Biologically Inspired Intelligence for Music Composition](https://arxiv.org/abs/2509.11973)
*Markus J. Buehler*

Main category: cs.AI

TL;DR: MusicSwarm：一种去中心化、无需权重更新的群体智能方法，通过局部协调实现连贯的长篇音乐创作，在质量、多样性和结构创新方面优于集中式系统


<details>
  <summary>Details</summary>
Motivation: 探索如何通过去中心化的群体智能实现长篇音乐创作，避免传统方法需要大量计算和参数更新的问题，提供计算和数据高效的创造性结构生成方案

Method: 使用相同的冻结基础模型组成去中心化群体，通过信息素式的对等信号协调，包含和声、节奏和结构线索的感知与沉积，短期记忆适应和共识达成机制

Result: 群体方法在符号、音频和图论分析中均表现出更优的质量，提供更大的多样性和结构变化，在创造力指标上领先，形成稳定的互补角色配置和小世界网络架构

Conclusion: MusicSwarm通过将专业化从参数更新转移到交互规则、共享记忆和动态共识，为长篇创造性结构提供了一条高效路径，可推广到协作写作、设计和科学发现等领域

Abstract: We show that coherent, long-form musical composition can emerge from a
decentralized swarm of identical, frozen foundation models that coordinate via
stigmergic, peer-to-peer signals, without any weight updates. We compare a
centralized multi-agent system with a global critic to a fully decentralized
swarm in which bar-wise agents sense and deposit harmonic, rhythmic, and
structural cues, adapt short-term memory, and reach consensus. Across symbolic,
audio, and graph-theoretic analyses, the swarm yields superior quality while
delivering greater diversity and structural variety and leads across creativity
metrics. The dynamics contract toward a stable configuration of complementary
roles, and self-similarity networks reveal a small-world architecture with
efficient long-range connectivity and specialized bridging motifs, clarifying
how local novelties consolidate into global musical form. By shifting
specialization from parameter updates to interaction rules, shared memory, and
dynamic consensus, MusicSwarm provides a compute- and data-efficient route to
long-horizon creative structure that is immediately transferable beyond music
to collaborative writing, design, and scientific discovery.

</details>


### [130] [Human-AI Use Patterns for Decision-Making in Disaster Scenarios: A Systematic Review](https://arxiv.org/abs/2509.12034)
*Emmanuel Adjei Domfeh,Christopher L. Dancy*

Main category: cs.AI

TL;DR: 本文系统综述了灾害管理中的人机协作模式，识别出四大类别：人机决策支持系统、任务资源协调、信任透明度、模拟训练，分析了AI如何提升灾害响应效率但存在可扩展性等限制。


<details>
  <summary>Details</summary>
Motivation: 在高风险灾害场景中，及时明智的决策至关重要，但常受到不确定性、动态环境和有限资源的挑战，需要研究人机协作如何支持灾害管理各阶段的决策。

Method: 基于51篇同行评审研究的系统综述，识别和分析人机协作模式，包括四大类别及其子模式如认知增强智能、多智能体协调等。

Result: 识别出四大人机协作模式类别，发现AI系统能增强态势感知、提高响应效率、支持复杂决策，但也存在可扩展性、可解释性和系统互操作性等关键限制。

Conclusion: 需要开发自适应、可信赖和情境感知的人机系统来提升灾害韧性，确保公平的恢复结果，并指出了未来研究的关键挑战和方向。

Abstract: In high-stakes disaster scenarios, timely and informed decision-making is
critical yet often challenged by uncertainty, dynamic environments, and limited
resources. This paper presents a systematic review of Human-AI collaboration
patterns that support decision-making across all disaster management phases.
Drawing from 51 peer-reviewed studies, we identify four major categories:
Human-AI Decision Support Systems, Task and Resource Coordination, Trust and
Transparency, and Simulation and Training. Within these, we analyze
sub-patterns such as cognitive-augmented intelligence, multi-agent
coordination, explainable AI, and virtual training environments. Our review
highlights how AI systems may enhance situational awareness, improves response
efficiency, and support complex decision-making, while also surfacing critical
limitations in scalability, interpretability, and system interoperability. We
conclude by outlining key challenges and future research directions,
emphasizing the need for adaptive, trustworthy, and context-aware Human-AI
systems to improve disaster resilience and equitable recovery outcomes.

</details>


### [131] [When Safe Unimodal Inputs Collide: Optimizing Reasoning Chains for Cross-Modal Safety in Multimodal Large Language Models](https://arxiv.org/abs/2509.12060)
*Wei Cai,Shujuan Liu,Jian Zhao,Ziyan Shi,Yusheng Zhao,Yuchen Yuan,Tianle Zhang,Chi Zhang,Xuelong Li*

Main category: cs.AI

TL;DR: MLLMs存在隐式推理风险，无害的单模态输入组合成危险的多模态数据会产生有害输出。研究提出SSUI数据集和SRPO训练框架来解决这一问题，在安全基准测试中达到最先进水平。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在长链推理中难以保持安全对齐，导致无害的单模态输入组合成危险的多模态内容产生有害输出。

Method: 提出SSUI数据集（包含可解释推理路径）和SRPO训练框架，通过优化推理路径使MLLM的内部推理过程与人类安全价值观对齐。

Result: SRPO训练模型在关键安全基准测试（包括新提出的RSBench）上达到最先进水平，显著优于开源和顶级商业MLLMs。

Conclusion: 该方法有效解决了MLLMs的隐式推理风险问题，通过推理路径优化实现了更好的安全对齐。

Abstract: Multimodal Large Language Models (MLLMs) are susceptible to the implicit
reasoning risk, wherein innocuous unimodal inputs synergistically assemble into
risky multimodal data that produce harmful outputs. We attribute this
vulnerability to the difficulty of MLLMs maintaining safety alignment through
long-chain reasoning. To address this issue, we introduce
Safe-Semantics-but-Unsafe-Interpretation (SSUI), the first dataset featuring
interpretable reasoning paths tailored for such a cross-modal challenge. A
novel training framework, Safety-aware Reasoning Path Optimization (SRPO), is
also designed based on the SSUI dataset to align the MLLM's internal reasoning
process with human safety values. Experimental results show that our
SRPO-trained models achieve state-of-the-art results on key safety benchmarks,
including the proposed Reasoning Path Benchmark (RSBench), significantly
outperforming both open-source and top-tier commercial MLLMs.

</details>


### [132] [Bridging Engineering and AI Planning through Model-Based Knowledge Transformation for the Validation of Automated Production System Variants](https://arxiv.org/abs/2509.12091)
*Hamied Nabizada,Lasse Beers,Alain Chahine,Felix Gehlhoff,Oliver Niggemann,Alexander Fay*

Main category: cs.AI

TL;DR: 这篇论文提出了一种模型驱动方法，能够在SysML工程模型中指定和自动生成符号规划件，以支持系统变体的验证和效率评估。


<details>
  <summary>Details</summary>
Motivation: 解决MBSE模型缺乏符号规划语义（如前提条件、效果、约束）的问题，这限制了系统变体能否完成特定任务的评估能力。

Method: 开发专用的SysML配置文件，引入可重用的模板用于核心规划构建，通过算法处理生成PDDL格式的有效域文件和问题文件。

Result: 通过航空组装案例研究验证了方法的可用性，能够在现有工程模型中添加规划语义，生成一致的规划件，支持AI规划进行系统变体验证。

Conclusion: 该方法实现了工程模型与规划件的本地集成和一致性维护，充分利用了现有模型信息，为系统变体验证提供了有效支持。

Abstract: Engineering models created in Model-Based Systems Engineering (MBSE)
environments contain detailed information about system structure and behavior.
However, they typically lack symbolic planning semantics such as preconditions,
effects, and constraints related to resource availability and timing. This
limits their ability to evaluate whether a given system variant can fulfill
specific tasks and how efficiently it performs compared to alternatives.
  To address this gap, this paper presents a model-driven method that enables
the specification and automated generation of symbolic planning artifacts
within SysML-based engineering models. A dedicated SysML profile introduces
reusable stereotypes for core planning constructs. These are integrated into
existing model structures and processed by an algorithm that generates a valid
domain file and a corresponding problem file in Planning Domain Definition
Language (PDDL). In contrast to previous approaches that rely on manual
transformations or external capability models, the method supports native
integration and maintains consistency between engineering and planning
artifacts.
  The applicability of the method is demonstrated through a case study from
aircraft assembly. The example illustrates how existing engineering models are
enriched with planning semantics and how the proposed workflow is applied to
generate consistent planning artifacts from these models. The generated
planning artifacts enable the validation of system variants through AI
planning.

</details>


### [133] [JustEva: A Toolkit to Evaluate LLM Fairness in Legal Knowledge Inference](https://arxiv.org/abs/2509.12104)
*Zongyue Xue,Siyuan Zheng,Shaochun Wang,Yiran Hu,Shenran Wang,Yuxin Yao,Haitao Li,Qingyao Ai,Yiqun Liu,Yun Liu,Weixing Shen*

Main category: cs.AI

TL;DR: JustEva是一个开源评估工具包，用于测量大型语言模型在法律任务中的公平性，包含65个法外因素标签系统、三个核心公平性指标、统计推断方法和可视化功能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在法律实践中的集成引发了关于司法公平性的担忧，特别是由于其"黑盒"特性，需要系统评估工具来确保公平性。

Method: 开发了JustEva工具包，包含结构化标签系统、三个公平性指标（不一致性、偏见和不平衡不准确性）、统计推断方法和可视化功能，支持生成结构化输出和统计分析两种实验类型。

Result: 实证应用显示当前LLMs存在显著的公平性缺陷，缺乏公平可信的法律工具。

Conclusion: JustEva为评估和改进法律领域算法公平性提供了便利工具和方法论基础。

Abstract: The integration of Large Language Models (LLMs) into legal practice raises
pressing concerns about judicial fairness, particularly due to the nature of
their "black-box" processes. This study introduces JustEva, a comprehensive,
open-source evaluation toolkit designed to measure LLM fairness in legal tasks.
JustEva features several advantages: (1) a structured label system covering 65
extra-legal factors; (2) three core fairness metrics - inconsistency, bias, and
imbalanced inaccuracy; (3) robust statistical inference methods; and (4)
informative visualizations. The toolkit supports two types of experiments,
enabling a complete evaluation workflow: (1) generating structured outputs from
LLMs using a provided dataset, and (2) conducting statistical analysis and
inference on LLMs' outputs through regression and other statistical methods.
Empirical application of JustEva reveals significant fairness deficiencies in
current LLMs, highlighting the lack of fair and trustworthy LLM legal tools.
JustEva offers a convenient tool and methodological foundation for evaluating
and improving algorithmic fairness in the legal domain.

</details>


### [134] [Co-Alignment: Rethinking Alignment as Bidirectional Human-AI Cognitive Adaptation](https://arxiv.org/abs/2509.12179)
*Yubo Li,Weiyi Song*

Main category: cs.AI

TL;DR: 提出双向认知对齐(BiCA)框架，实现人类与AI的相互适应，在协作导航任务中取得显著性能提升和安全改善


<details>
  <summary>Details</summary>
Motivation: 当前RLHF对齐方法只是单向的AI适应人类偏好，将人类认知视为固定不变，需要转向双向共同对齐范式

Method: 使用可学习协议、表征映射和KL预算约束来实现受控的共同进化，包括双向适应机制

Result: 在协作导航中达到85.5%成功率(比基线70.3%提升)，互适应能力提升230%，协议收敛性提升332%，涌现协议比手工协议优84%，安全性提升23%

Conclusion: 最优协作存在于人类和AI能力的交集而非并集，验证了从单向对齐向共同对齐范式转变的必要性

Abstract: Current AI alignment through RLHF follows a single directional paradigm that
AI conforms to human preferences while treating human cognition as fixed. We
propose a shift to co-alignment through Bidirectional Cognitive Alignment
(BiCA), where humans and AI mutually adapt. BiCA uses learnable protocols,
representation mapping, and KL-budget constraints for controlled co-evolution.
In collaborative navigation, BiCA achieved 85.5% success versus 70.3% baseline,
with 230% better mutual adaptation and 332% better protocol convergence.
Emergent protocols outperformed handcrafted ones by 84%, while bidirectional
adaptation unexpectedly improved safety (+23% out-of-distribution robustness).
The 46% synergy improvement demonstrates optimal collaboration exists at the
intersection, not union, of human and AI capabilities, validating the shift
from single-directional to co-alignment paradigms.

</details>


### [135] [Advancing Medical Artificial Intelligence Using a Century of Cases](https://arxiv.org/abs/2509.12194)
*Thomas A. Buckley,Riccardo Conci,Peter G. Brodeur,Jason Gusdorf,Sourik Beltrán,Bita Behrouzi,Byron Crowe,Jacob Dockterman,Muzzammil Muhammad,Sarah Ohnigian,Andrew Sanchez,James A. Diao,Aashna P. Shah,Daniel Restrepo,Eric S. Rosenberg,Andrew S. Lea,Marinka Zitnik,Scott H. Podolsky,Zahir Kanjee,Raja-Elie E. Abdulnour,Jacob M. Koshy,Adam Rodman,Arjun K. Manrai*

Main category: cs.AI

TL;DR: LLMs在复杂文本鉴别诊断方面超越医生表现，能够有效模拟专家医学演示，但在图像解释和文献检索方面仍有不足。


<details>
  <summary>Details</summary>
Motivation: 评估AI在医学诊断推理和演示方面的综合能力，而不仅仅是最终诊断准确性，填补现有AI评估的空白。

Method: 使用7102个CPC案例和1021个图像挑战创建CPC-Bench基准，评估领先LLMs，并开发Dr. CaBot AI讨论系统来生成书面和幻灯片演示。

Result: o3模型在60%案例中排名第一诊断，84%进入前十；图像任务准确率67%；在盲测中74%的医生无法区分AI和人类生成的鉴别诊断。

Conclusion: LLMs在文本诊断方面表现优异，能够有效模拟专家演示，但图像和文献检索仍需改进。CPC-Bench和CaBot为医学AI进展提供了透明追踪工具。

Abstract: BACKGROUND: For over a century, the New England Journal of Medicine
Clinicopathological Conferences (CPCs) have tested the reasoning of expert
physicians and, recently, artificial intelligence (AI). However, prior AI
evaluations have focused on final diagnoses without addressing the multifaceted
reasoning and presentation skills required of expert discussants.
  METHODS: Using 7102 CPCs (1923-2025) and 1021 Image Challenges (2006-2025),
we conducted extensive physician annotation and automated processing to create
CPC-Bench, a physician-validated benchmark spanning 10 text-based and
multimodal tasks, against which we evaluated leading large language models
(LLMs). Then, we developed "Dr. CaBot," an AI discussant designed to produce
written and slide-based video presentations using only the case presentation,
modeling the role of the human expert in these cases.
  RESULTS: When challenged with 377 contemporary CPCs, o3 (OpenAI) ranked the
final diagnosis first in 60% of cases and within the top ten in 84% of cases,
outperforming a 20-physician baseline; next-test selection accuracy reached
98%. Event-level physician annotations quantified AI diagnostic accuracy per
unit of information. Performance was lower on literature search and image
tasks; o3 and Gemini 2.5 Pro (Google) achieved 67% accuracy on image
challenges. In blinded comparisons of CaBot vs. human expert-generated text,
physicians misclassified the source of the differential in 46 of 62 (74%) of
trials, and scored CaBot more favorably across quality dimensions. To promote
research, we are releasing CaBot and CPC-Bench.
  CONCLUSIONS: LLMs exceed physician performance on complex text-based
differential diagnosis and convincingly emulate expert medical presentations,
but image interpretation and literature retrieval remain weaker. CPC-Bench and
CaBot may enable transparent and continued tracking of progress in medical AI.

</details>
