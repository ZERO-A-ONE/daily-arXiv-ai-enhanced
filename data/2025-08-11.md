<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 12]
- [cs.CR](#cs.CR) [Total: 27]
- [cs.AI](#cs.AI) [Total: 28]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Empirical Evaluation of AI-Assisted Software Package Selection: A Knowledge Graph Approach](https://arxiv.org/abs/2508.05693)
*Siamak Farshidi,Amir Saberhabibi,Behbod Eskafi,Niloofar Nikfarjam,Sadegh Eskandari,Slinger Jansen,Michel Chaudron,Bedir Tekinerdogan*

Main category: cs.SE

TL;DR: 论文提出了一种基于多准则决策（MCDM）的数据驱动框架PySelect，用于解决开源生态系统中第三方软件包选择的挑战，结合AI意图建模和实证数据，提高了推荐质量和透明度。


<details>
  <summary>Details</summary>
Motivation: 开源生态系统中软件包选择困难，现有生成式AI工具依赖流行度而非适用性，缺乏透明度和可复现性，影响项目的长期可靠性和架构决策。

Method: 将软件包选择建模为MCDM问题，通过自动化数据管道收集软件元数据、使用趋势、漏洞信息和开发者情感，构建决策模型，并开发PySelect系统结合大语言模型解释用户意图。

Result: 实验基于798,669个Python脚本和用户研究，结果显示数据提取精度高，推荐质量优于生成式AI基线，用户评价积极。

Conclusion: 该框架可扩展、可解释且可复现，支持基于证据的软件包选择，结合MCDM、实证数据和AI辅助意图建模。

Abstract: Selecting third-party software packages in open-source ecosystems like Python
is challenging due to the large number of alternatives and limited transparent
evidence for comparison. Generative AI tools are increasingly used in
development workflows, but their suggestions often overlook dependency
evaluation, emphasize popularity over suitability, and lack reproducibility.
This creates risks for projects that require transparency, long-term
reliability, maintainability, and informed architectural decisions. This study
formulates software package selection as a Multi-Criteria Decision-Making
(MCDM) problem and proposes a data-driven framework for technology evaluation.
Automated data pipelines continuously collect and integrate software metadata,
usage trends, vulnerability information, and developer sentiment from GitHub,
PyPI, and Stack Overflow. These data are structured into a decision model
representing relationships among packages, domain features, and quality
attributes. The framework is implemented in PySelect, a decision support system
that uses large language models to interpret user intent and query the model to
identify contextually appropriate packages. The approach is evaluated using
798,669 Python scripts from 16,887 GitHub repositories and a user study based
on the Technology Acceptance Model. Results show high data extraction
precision, improved recommendation quality over generative AI baselines, and
positive user evaluations of usefulness and ease of use. This work introduces a
scalable, interpretable, and reproducible framework that supports
evidence-based software selection using MCDM principles, empirical data, and
AI-assisted intent modeling.

</details>


### [2] [Klear-CodeTest: Scalable Test Case Generation for Code Reinforcement Learning](https://arxiv.org/abs/2508.05710)
*Jia Fu,Xinyu Yang,Hongzhi Zhang,Yahui Liu,Jingyuan Zhang,Qi Wang,Fuzheng Zhang,Guorui Zhou*

Main category: cs.SE

TL;DR: Klear-CodeTest是一个用于合成高质量测试用例的框架，通过Generator-Validation（G-V）机制确保测试用例的正确性和覆盖率，同时设计了多层安全沙箱系统以保证代码执行的安全性。


<details>
  <summary>Details</summary>
Motivation: 为大型语言模型（LLM）在代码强化学习中提供精确的反馈，解决高质量测试用例合成的难题。

Method: 采用G-V框架生成并验证测试用例，包括常规和边界情况，并通过多层安全沙箱系统确保代码执行安全。

Result: 实验表明，该方法显著提升了模型性能和训练稳定性。

Conclusion: Klear-CodeTest为代码强化学习提供了高效、可靠的测试用例合成方案，并开源了相关资源。

Abstract: Precise, correct feedback is crucial for effectively training large language
models (LLMs) in code reinforcement learning. However, synthesizing
high-quality test cases remains a profoundly challenging and unsolved problem.
In this work, we present Klear-CodeTest, a comprehensive test case synthesis
framework featuring rigorous verification to ensure quality and reliability of
test cases. Our approach achieves broad coverage of programming problems via a
novel Generator-Validation (G-V) framework, ensuring correctness through a
consistency validation mechanism that verifies outputs against gold solutions.
The proposed G-V framework generates comprehensive test cases including both
regular and corner cases, enhancing test coverage and discriminative power for
solution correctness assessment in code reinforcement learning. In addition, we
design a multi-layered security sandbox system optimized for online
verification platforms, guaranteeing safe and reliable code execution. Through
comprehensive experiments, we demonstrate the effectiveness of our curated
dataset, showing significant improvements in model performance and training
stability. The source codes, curated dataset and sandbox system are available
at: https://github.com/Kwai-Klear/CodeTest.

</details>


### [3] [Utilizing Composer Packages to Accelerate Laravel-Based Project Development Among Students: A Pedagogical and Practical Framework](https://arxiv.org/abs/2508.05747)
*Rohaizah Abdul Wahid,Muhamad Said Nizamuddin Nadim,Suliana Sulaiman,Syahmi Akmal Shaharudin,Muhammad Danial Jupikil,Iqqwan Jasman Su Azlan Su*

Main category: cs.SE

TL;DR: 论文探讨了如何在大学Laravel课程中利用Composer及其精选包加速开发，同时保持代码质量和概念理解，并提出了教学建议。


<details>
  <summary>Details</summary>
Motivation: 学生在有限时间内完成Laravel项目存在困难，需工具支持但避免过度依赖。

Method: 引入Composer及精选包，结合教学策略，强调代码质量和概念理解。

Result: 加速开发的同时，提升专业实践能力，但需注意包冲突和工具依赖问题。

Conclusion: 有效整合Composer需教学指导，确保工具使用支持深度学习而非替代。

Abstract: Laravel has emerged as a foundational framework in university web development
curricula. However, despite its scaffolding capabilities, students often
struggle to complete projects within limited academic timelines. This
conceptual paper introduces Composer, PHP's standard dependency manager, and
categorizes a curated selection of Composer packages that significantly reduce
development effort while fostering professional software practices. Grounded in
practical and pedagogical considerations, the paper illustrates how educators
and learners can strategically leverage these tools to build typical academic
or personal Laravel-based systems. Central to this approach is maintaining code
quality and reinforcing conceptual understanding. The paper also addresses
potential risks such as package conflicts and over-reliance on tools, providing
best-practice recommendations to mitigate them. While the goal is to accelerate
development, the deeper objective is to reinforce professional workflows and
industry readiness. Exposure to Composer packages enhances curriculum relevance
and smooths the transition from academia to the workplace. However, effective
integration requires deliberate instructional design aligned with learning
objectives. Without guidance, students may treat packages as black boxes. Thus,
educators must teach not only how to use these tools, but also when and why,
encouraging critical evaluation of their utility and limitations. This ensures
that practical convenience supports rather than supplants deep learning.

</details>


### [4] [AI-Guided Exploration of Large-Scale Codebases](https://arxiv.org/abs/2508.05799)
*Yoseph Berhanu Alebachew*

Main category: cs.SE

TL;DR: 论文提出了一种结合确定性逆向工程与LLM引导的意图感知可视化探索的混合方法，以提升代码理解效率。


<details>
  <summary>Details</summary>
Motivation: 开发者在理解复杂软件系统时面临挑战，传统工具缺乏交互性和上下文整合，LLMs虽提供新机会但缺乏结构化视图的集成。

Method: 结合UML可视化、动态用户界面、历史上下文和协作功能，通过LLM解释用户查询和交互模式，帮助开发者导航和理解代码库。

Result: 原型实现展示了该方法的可行性，未来工作包括实证评估、扩展到多语言系统和探索GUI驱动的LLM交互模型。

Conclusion: 研究为智能、交互式开发环境奠定了基础，符合开发者认知和协作工作流程。

Abstract: Understanding large-scale, complex software systems is a major challenge for
developers, who spend a significant portion of their time on program
comprehension. Traditional tools such as static visualizations and reverse
engineering techniques provide structural insights but often lack
interactivity, adaptability, and integration with contextual information.
Recent advancements in large language models (LLMs) offer new opportunities to
enhance code exploration workflows, yet their lack of grounding and integration
with structured views limits their effectiveness. This work introduces a hybrid
approach that integrates deterministic reverse engineering with LLM-guided,
intent-aware visual exploration. The proposed system combines UML-based
visualization, dynamic user interfaces, historical context, and collaborative
features into an adaptive tool for code comprehension. By interpreting user
queries and interaction patterns, the LLM helps developers navigate and
understand complex codebases more effectively. A prototype implementation for
Java demonstrates the feasibility of this approach. Future work includes
empirical evaluation, scaling to polyglot systems, and exploring GUI-driven LLM
interaction models. This research lays the groundwork for intelligent,
interactive environments that align with developer cognition and collaborative
workflows.

</details>


### [5] [Enhancing Software Vulnerability Detection Through Adaptive Test Input Generation Using Genetic Algorithm](https://arxiv.org/abs/2508.05923)
*Yanusha Mehendran,Maolin Tang,Yi Lu*

Main category: cs.SE

TL;DR: 该论文提出了一种基于遗传算法的测试输入生成方法，通过结合遗传操作和自适应学习，显著提升了软件漏洞检测的效果。


<details>
  <summary>Details</summary>
Motivation: 随着软件复杂性的增加，传统漏洞检测方法的能力不足，亟需更高效的方法来提升软件可靠性。

Method: 采用遗传算法，结合交叉操作和自适应反馈机制，动态生成和优化测试输入。

Result: 在九个开源JSON处理库的测试中，该方法在覆盖率上显著优于基准方法，平均提升最高达166%。

Conclusion: 该方法能够有效检测更深层次的漏洞，为软件安全测试提供了可扩展的自适应解决方案。

Abstract: Software vulnerabilities continue to undermine the reliability and security
of modern systems, particularly as software complexity outpaces the
capabilities of traditional detection methods. This study introduces a genetic
algorithm-based method for test input generation that innovatively integrates
genetic operators and adaptive learning to enhance software vulnerability
detection. A key contribution is the application of the crossover operator,
which facilitates exploration by searching across a broader space of potential
test inputs. Complementing this, an adaptive feedback mechanism continuously
learns from the system's execution behavior and dynamically guides input
generation toward promising areas of the input space. Rather than relying on
fixed or randomly selected inputs, the approach evolves a population of
structurally valid test cases using feedback-driven selection, enabling deeper
and more effective code traversal. This strategic integration of exploration
and exploitation ensures that both diverse and targeted test inputs are
developed over time. Evaluation was conducted across nine open-source
JSON-processing libraries. The proposed method achieved substantial
improvements in coverage compared to a benchmark evolutionary fuzzing method,
with average gains of 39.8% in class coverage, 62.4% in method coverage, 105.0%
in line coverage, 114.0% in instruction coverage, and 166.0% in branch
coverage. These results highlight the method's capacity to detect deeper and
more complex vulnerabilities, offering a scalable and adaptive solution to
software security testing.

</details>


### [6] [A Survey on Task Scheduling in Carbon-Aware Container Orchestration](https://arxiv.org/abs/2508.05949)
*Jialin Yang,Zainab Saad,Jiajun Wu,Xiaoguang Niu,Henry Leung,Steve Drew*

Main category: cs.SE

TL;DR: 本文综述了Kubernetes调度策略，分类为硬件和软件中心，提出了一种关注环境可持续性的云任务调度分类法，并分析了研究趋势和挑战。


<details>
  <summary>Details</summary>
Motivation: 大规模软件生态系统和云数据中心的能源需求激增，尤其是大型语言模型的训练和部署，导致能源消耗和碳足迹达到前所未有的水平。

Method: 系统回顾了Kubernetes调度策略，分类为硬件和软件中心，标注其可持续性目标，并按算法分组。

Result: 提出了一种关注环境可持续性的云任务调度分类法，分析了研究趋势和开放挑战。

Conclusion: 研究结果为下一代云计算系统设计可持续调度解决方案提供了关键见解。

Abstract: The soaring energy demands of large-scale software ecosystems and cloud data
centers, accelerated by the intensive training and deployment of large language
models, have driven energy consumption and carbon footprint to unprecedented
levels. In response, both industry and academia are increasing efforts to
reduce the carbon emissions associated with cloud computing through more
efficient task scheduling and infrastructure orchestration. In this work, we
present a systematic review of various Kubernetes scheduling strategies,
categorizing them into hardware-centric and software-centric, annotating each
with its sustainability objectives, and grouping them according to the
algorithms they use. We propose a comprehensive taxonomy for cloud task
scheduling studies, with a particular focus on the environmental sustainability
aspect. We analyze emerging research trends and open challenges, and our
findings provide critical insight into the design of sustainable scheduling
solutions for next-generation cloud computing systems.

</details>


### [7] [Impact-driven Context Filtering For Cross-file Code Completion](https://arxiv.org/abs/2508.05970)
*Yanzhou Li,Shangqing Liu,Kangjie Chen,Tianwei Zhang,Yang Liu*

Main category: cs.SE

TL;DR: 论文提出了一种基于检索增强生成（RAG）的代码补全方法，通过引入似然度量评估检索代码块的影响，并构建了标注数据集。进一步提出了自适应检索上下文过滤框架CODEFILTER，显著提升了补全准确性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决检索增强生成中检索到的跨文件代码块对代码补全的贡献不均衡问题，部分代码块甚至可能降低性能。

Method: 提出似然度量评估检索代码块的影响，构建标注数据集（正、中、负），并开发自适应过滤框架CODEFILTER。

Result: CODEFILTER在多个基准测试中显著提升补全准确性，减少输入提示长度，并展现强泛化能力。

Conclusion: CODEFILTER能够有效提升代码补全的准确性、效率和可归因性。

Abstract: Retrieval-augmented generation (RAG) has recently demonstrated considerable
potential for repository-level code completion, as it integrates cross-file
knowledge with in-file preceding code to provide comprehensive contexts for
generation. To better understand the contribution of the retrieved cross-file
contexts, we introduce a likelihood-based metric to evaluate the impact of each
retrieved code chunk on the completion. Our analysis reveals that, despite
retrieving numerous chunks, only a small subset positively contributes to the
completion, while some chunks even degrade performance. To address this issue,
we leverage this metric to construct a repository-level dataset where each
retrieved chunk is labeled as positive, neutral, or negative based on its
relevance to the target completion. We then propose an adaptive retrieval
context filtering framework, CODEFILTER, trained on this dataset to mitigate
the harmful effects of negative retrieved contexts in code completion.
Extensive evaluation on the RepoEval and CrossCodeLongEval benchmarks
demonstrates that CODEFILTER consistently improves completion accuracy compared
to approaches without filtering operations across various tasks. Additionally,
CODEFILTER significantly reduces the length of the input prompt, enhancing
computational efficiency while exhibiting strong generalizability across
different models. These results underscore the potential of CODEFILTER to
enhance the accuracy, efficiency, and attributability of repository-level code
completion.

</details>


### [8] [Position: Intelligent Coding Systems Should Write Programs with Justifications](https://arxiv.org/abs/2508.06017)
*Xiangzhe Xu,Shiwei Feng,Zian Su,Chengpeng Wang,Xiangyu Zhang*

Main category: cs.SE

TL;DR: 论文探讨了智能编码系统在生成代码时需提供清晰、一致的合理性解释，以增强用户信任，并提出了神经符号方法作为解决方案。


<details>
  <summary>Details</summary>
Motivation: AI驱动的编码系统决策不透明，导致非专家用户对其信任和可用性产生担忧，因此需要生成易于理解的解释。

Method: 提出神经符号方法，通过符号约束指导模型训练，并利用神经表示丰富程序语义，实现推理时的自动一致性检查。

Result: 现有方法（如形式验证、静态分析和事后解释）在认知对齐和语义忠实性方面存在局限。

Conclusion: 神经符号方法有望成为生成代码合理性解释的有效途径，以提升用户理解和信任。

Abstract: Intelligent coding systems are transforming software development by enabling
users to specify code behavior in natural language. However, the opaque
decision-making of AI-driven coders raises trust and usability concerns,
particularly for non-expert users who cannot inspect low-level implementations.
We argue that these systems should not only generate code but also produce
clear, consistent justifications that bridge model reasoning and user
understanding. To this end, we identify two critical justification
properties-cognitive alignment and semantic faithfulness-and highlight the
limitations of existing methods, including formal verification, static
analysis, and post-hoc explainability. We advocate exploring neuro-symbolic
approaches for justification generation, where symbolic constraints guide model
behavior during training and program semantics are enriched through neural
representations, enabling automated consistency checks at inference time.

</details>


### [9] [Understanding Inconsistent State Update Vulnerabilities in Smart Contracts](https://arxiv.org/abs/2508.06192)
*Lantian Li,Yuyu Chen,Jingwen Wu,Yue Pan,Zhongxing Yu*

Main category: cs.SE

TL;DR: 论文对智能合约中的不一致状态更新漏洞进行了首次大规模实证研究，总结了其根本原因、修复策略和利用方法，并提出了11项重要发现。


<details>
  <summary>Details</summary>
Motivation: 智能合约的状态更新问题可能导致漏洞，现有工具难以有效检测，因此需要深入研究以帮助开发者、研究人员和工具设计者避免此类漏洞。

Method: 系统调查了352个真实智能合约项目中的116个不一致状态更新漏洞，分析其根因、修复方法和利用方式。

Result: 研究提出了11项重要发现，并开发了一个概念验证检查器，成功检测到64个GitHub项目中的问题，19个项目所有者确认了问题。

Conclusion: 研究结果对避免智能合约中的不一致状态更新漏洞具有重要价值，为相关领域提供了实用指导。

Abstract: Smart contracts enable contract terms to be automatically executed and
verified on the blockchain, and recent years have witnessed numerous
applications of them in areas such as financial institutions and supply chains.
The execution logic of a smart contract is closely related to the contract
state, and thus the correct and safe execution of the contract depends heavily
on the precise control and update of the contract state. However, the contract
state update process can have issues. In particular, inconsistent state update
issues can arise for reasons such as unsynchronized modifications. Inconsistent
state update bugs have been exploited by attackers many times, but existing
detection tools still have difficulty in effectively identifying them. This
paper conducts the first large-scale empirical study about inconsistent state
update vulnerabilities (that is, inconsistent state update bugs that are
exploitable) in smart contracts, aiming to shed light for developers,
researchers, tool builders, and language or library designers in order to avoid
inconsistent state update vulnerabilities. We systematically investigate 116
inconsistent state update vulnerabilities in 352 real-world smart contract
projects, summarizing their root causes, fix strategies, and exploitation
methods. Our study provides 11 original and important findings, and we also
give the implications of our findings. To illustrate the potential benefits of
our research, we also develop a proof-of-concept checker based on one of our
findings. The checker effectively detects issues in 64 popular GitHub projects,
and 19 project owners have confirmed the detected issues at the time of
writing. The result demonstrates the usefulness and importance of our findings
for avoiding inconsistent state update vulnerabilities in smart contracts.

</details>


### [10] [Improving the Developer Experience with a Low-Code Process Modelling Language](https://arxiv.org/abs/2508.06299)
*Henrique Henriques,Hugo Lourenço,Vasco Amaral,Miguel Goulão*

Main category: cs.SE

TL;DR: 论文研究了OutSystems平台中BPT DSL的低采用率问题，通过访谈、符号物理理论分析和实证评估改进BPT，显著提升了语义透明度、正确性和用户体验。


<details>
  <summary>Details</summary>
Motivation: BPT DSL的采用率低且存在可用性问题，维护成本高，亟需改进。

Method: 结合访谈、符号物理理论分析、SUS和NASA TLX评估，开发新版本BPT。

Result: 新版本BPT的语义透明度从31%提升至69%，正确性从51%提升至89%，SUS评分从42.25升至64.78，TLX评分从36.50降至20.78。

Conclusion: 新版本BPT显著改善了开发者体验，用户背景对最终语法选择和可用性指标有重要影响。

Abstract: Context: The OutSystems Platform is a development environment composed of
several DSLs, used to specify, quickly build, and validate web and mobile
applications. The DSLs allow users to model different perspectives such as
interfaces and data models, define custom business logic and construct process
models. Problem: The DSL for process modelling (Business Process Technology
(BPT)), has a low adoption rate and is perceived as having usability problems
hampering its adoption. This is problematic given the language maintenance
costs. Method: We used a combination of interviews, a critical review of BPT
using the "Physics of Notation" and empirical evaluations of BPT using the
System Usability Scale (SUS) and the NASA Task Load indeX (TLX), to develop a
new version of BPT, taking these inputs and Outsystems' engineers' culture into
account. Results: Evaluations conducted with 25 professional software engineers
showed an increase of the semantic transparency on the new version, from 31% to
69%, an increase in the correctness of responses, from 51% to 89%, an increase
in the SUS score, from 42.25 to 64.78, and a decrease of the TLX score, from
36.50 to 20.78. These differences were statistically significant. Conclusions:
These results suggest that the new version of BPT significantly improved the
developer experience of the previous version. The end users' background with
OutSystems had a relevant impact on the final concrete syntax choices and
achieved usability indicators.

</details>


### [11] [Execution-Feedback Driven Test Generation from SWE Issues](https://arxiv.org/abs/2508.06365)
*Toufique Ahmed,Jatin Ganhotra,Avraham Shinnar,Martin Hirzel*

Main category: cs.SE

TL;DR: 论文探讨了如何自动生成软件工程问题的重现测试，解决了代码缺失或错误的问题，并提出了新方法e-Otter++，实验表明其性能显著优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 大多数软件工程问题缺乏有效的重现测试，导致问题难以解决。现有方法因代码缺失或错误而难以生成高质量测试。

Method: 提出了一种新方法e-Otter++，利用执行反馈绕过代码缺失或错误的问题，自动生成重现测试。

Result: 在TDD-Bench Verified基准测试中，e-Otter++的平均失败转通过率为63%，显著优于现有技术。

Conclusion: e-Otter++在自动生成重现测试方面取得了显著进展，为解决软件工程问题提供了有效工具。

Abstract: A software engineering issue (SWE issue) is easier to resolve when
accompanied by a reproduction test. Unfortunately, most issues do not come with
functioning reproduction tests, so this paper explores how to generate them
automatically. The primary challenge in this setting is that the code to be
tested is either missing or wrong, as evidenced by the existence of the issue
in the first place. This has held back test generation for this setting:
without the correct code to execute, it is difficult to leverage execution
feedback to generate good tests. This paper introduces novel techniques for
leveraging execution feedback to get around this problem, implemented in a new
reproduction test generator called e-Otter++. Experiments show that e-Otter++
represents a leap ahead in the state-of-the-art for this problem, generating
tests with an average fail-to-pass rate of 63% on the TDD-Bench Verified
benchmark.

</details>


### [12] [What Builds Effective In-Context Examples for Code Generation?](https://arxiv.org/abs/2508.06414)
*Dongze Li,Songqiang Chen,Jialun Cao,Shing-Chi Cheung*

Main category: cs.SE

TL;DR: 本文通过系统研究代码特征对ICL的影响，发现变量和函数命名对代码生成至关重要，而LLM更注重语义而非格式。


<details>
  <summary>Details</summary>
Motivation: 探索ICL中代码示例的具体特征（如命名风格、格式）如何影响LLM的代码生成能力。

Method: 通过控制变量进行消融研究，分析不同代码特征对ICL效果的影响。

Result: 变量和函数命名的适当性显著影响性能（性能下降达30%），LLM更偏好语义化命名而非格式。

Conclusion: 优化命名风格可提升ICL效果，但LLM在从代码中提取通用问题解决洞察方面仍有挑战。

Abstract: In-Context Learning (ICL) has emerged as a promising solution to enhance the
code generation capabilities of Large Language Models (LLMs), which
incorporates code examples inside the prompt to let LLMs learn from
demonstrations. However, despite the substantial effectiveness of the code
example-based ICL approach, the specific features (e.g., identifier naming
styles, code formatting, solution insight) within the ICL-provided code
examples that significantly contribute to the ICL's effectiveness remain
unclear. This paper systematically investigates the impact of various code
features on ICL with code examples through controlled ablation studies. Our
findings reveal that the appropriate naming of variables and functions is
crucial for effective code generation, with their elimination leading to
performance decreases of up to 30 percentage points. We further demonstrate
that LLMs prioritize semantically meaningful identifier names over formatting
conventions, with language-specific preferences regarding identifier verbosity.
Additionally, our investigation into ICL's potential for enhancing reflection
and inference capabilities reveals that current LLMs struggle to extract
generalizable problem-solving insights from similar code solutions, despite
being capable of utilizing direct information effectively. These findings are
expected to provide valuable insights for optimizing ICL systems in code
generation applications and highlight fundamental challenges in
reflection-based learning for code generation tasks.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [13] [Blockchain-Based Decentralized Domain Name System](https://arxiv.org/abs/2508.05655)
*Guang Yang,Peter Trinh,Alma Nkemla,Amuru Serikyaku,Edward Tatchim,Osman Sharaf*

Main category: cs.CR

TL;DR: 论文提出了一种基于区块链的去中心化域名系统（DDNS），以解决传统DNS的安全漏洞和中心化问题。


<details>
  <summary>Details</summary>
Motivation: 传统DNS存在中毒攻击、审查机制和中心化故障点等问题，威胁互联网自由与安全。

Method: 设计了一种专用PoW区块链，结合IPFS分布式存储和加密原语，实现零信任验证。

Result: 系统实现了15秒域名记录传播时间，支持20种标准DNS记录类型，并具备高扩展性和抗操纵能力。

Conclusion: DDNS展示了区块链技术在域名系统中的潜力，提供了安全、去中心化的替代方案。

Abstract: The current Domain Name System (DNS) infrastructure faces critical
vulnerabilities including poisoning attacks, censorship mechanisms, and
centralized points of failure that compromise internet freedom and security.
Recent incidents such as DNS poisoning attacks on ISP customers highlight the
urgent need for resilient alternatives. This paper presents a novel
blockchain-based Decentralized Domain Name System (DDNS). We designed a
specialized Proof-of-Work blockchain to maximize support for DNS-related
protocols and achieve node decentralization. The system integrates our
blockchain with IPFS for distributed storage, implements cryptographic
primitives for end-to-end trust signatures, and achieves Never Trust, Always
Verify zero-trust verification. Our implementation achieves 15-second domain
record propagation times, supports 20 standard DNS record types, and provides
perpetual free .ddns domains. The system has been deployed across distributed
infrastructure in San Jose, Los Angeles, and Orange County, demonstrating
practical scalability and resistance to traditional DNS manipulation
techniques. Performance evaluation shows the system can handle up to Max Theor.
TPS 1,111.1 tx/s (minimal transactions) and Max Theor. TPS 266.7 tx/s (regular
transactions) for domain operations while maintaining sub-second query
resolution through intelligent caching mechanisms.

</details>


### [14] [Universally Unfiltered and Unseen:Input-Agnostic Multimodal Jailbreaks against Text-to-Image Model Safeguards](https://arxiv.org/abs/2508.05658)
*Song Yan,Hui Wei,Jinlong Fei,Guoliang Yang,Zhengyu Zhao,Zheng Wamg*

Main category: cs.CR

TL;DR: 论文提出了一种名为U3-Attack的多模态越狱攻击方法，用于绕过文本到图像（T2I）模型的安全检查器和提示过滤器。该方法通过优化图像背景的对抗性补丁和敏感词的安全改写集，实现了高效且通用的攻击。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态越狱攻击方法存在可扩展性差和优化耗时的问题，限制了其实际应用。本文旨在解决这些局限性。

Method: U3-Attack通过优化图像背景的对抗性补丁来绕过安全检查器，同时优化敏感词的安全改写集以绕过提示过滤器，从而减少冗余计算。

Result: 实验结果表明，U3-Attack在开源和商业T2I模型上均表现出色，例如在Runway-inpainting模型上的成功率比现有最佳方法高4倍。

Conclusion: U3-Attack是一种高效且通用的多模态越狱攻击方法，能够有效绕过T2I模型的安全防护措施。

Abstract: Various (text) prompt filters and (image) safety checkers have been
implemented to mitigate the misuse of Text-to-Image (T2I) models in creating
Not-Safe-For-Work (NSFW) content.In order to expose potential security
vulnerabilities of such safeguards, multimodal jailbreaks have been
studied.However, existing jailbreaks are limited to prompt-specific and
image-specific perturbations, which suffer from poor scalability and
time-consuming optimization.To address these limitations, we propose
Universally Unfiltered and Unseen (U3)-Attack, a multimodal jailbreak attack
method against T2I safeguards.Specifically, U3-Attack optimizes an adversarial
patch on the image background to universally bypass safety checkers and
optimizes a safe paraphrase set from a sensitive word to universally bypass
prompt filters while eliminating redundant computations.Extensive experimental
results demonstrate the superiority of our U3-Attack on both open-source and
commercial T2I models.For example, on the commercial Runway-inpainting model
with both prompt filter and safety checker, our U3-Attack achieves $~4\times$
higher success rates than the state-of-the-art multimodal jailbreak attack,
MMA-Diffusion.Content Warning: This paper includes examples of NSFW content.

</details>


### [15] [Can LLMs effectively provide game-theoretic-based scenarios for cybersecurity?](https://arxiv.org/abs/2508.05670)
*Daniele Proverbio,Alessio Buscemi,Alessandro Di Stefano,The Anh Han,German Castignani,Pietro Liò*

Main category: cs.CR

TL;DR: 研究探讨了经典博弈论框架是否能有效捕捉LLM驱动行为，发现LLM在博弈中的表现受个性特征和语言选择影响。


<details>
  <summary>Details</summary>
Motivation: 评估LLM在网络安全中的行为，验证博弈论框架的适用性。

Method: 使用可复现的博弈论框架测试LLM在零和博弈和囚徒困境中的表现，涉及多语言分析。

Result: LLM的博弈结果受个性特征和语言选择影响，语言敏感性显著。

Conclusion: LLM在网络安全应用中需谨慎使用，需进一步研究其跨语言稳定性。

Abstract: Game theory has long served as a foundational tool in cybersecurity to test,
predict, and design strategic interactions between attackers and defenders. The
recent advent of Large Language Models (LLMs) offers new tools and challenges
for the security of computer systems; In this work, we investigate whether
classical game-theoretic frameworks can effectively capture the behaviours of
LLM-driven actors and bots. Using a reproducible framework for game-theoretic
LLM agents, we investigate two canonical scenarios -- the one-shot zero-sum
game and the dynamic Prisoner's Dilemma -- and we test whether LLMs converge to
expected outcomes or exhibit deviations due to embedded biases. Our experiments
involve four state-of-the-art LLMs and span five natural languages, English,
French, Arabic, Vietnamese, and Mandarin Chinese, to assess linguistic
sensitivity. For both games, we observe that the final payoffs are influenced
by agents characteristics such as personality traits or knowledge of repeated
rounds. Moreover, we uncover an unexpected sensitivity of the final payoffs to
the choice of languages, which should warn against indiscriminate application
of LLMs in cybersecurity applications and call for in-depth studies, as LLMs
may behave differently when deployed in different countries. We also employ
quantitative metrics to evaluate the internal consistency and cross-language
stability of LLM agents, to help guide the selection of the most stable LLMs
and optimising models for secure applications.

</details>


### [16] [DINA: A Dual Defense Framework Against Internal Noise and External Attacks in Natural Language Processing](https://arxiv.org/abs/2508.05671)
*Ko-Wei Chuang,Hen-Hsen Huang,Tsai-Yen Li*

Main category: cs.CR

TL;DR: 论文提出DINA框架，同时防御NLP系统中的内部标签噪声和外部对抗攻击，显著提升模型鲁棒性和准确性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型和生成式AI在客服和内容审核中的应用增加，内部标签污染和外部对抗攻击成为双重威胁。

Method: 结合计算机视觉中的噪声标签学习方法和对抗训练，提出DINA框架，统一应对内部和外部威胁。

Result: 在真实在线游戏数据集上的实验表明，DINA显著优于基线模型。

Conclusion: DINA为NLP系统提供了实用的双重威胁防御策略，对公平和负责任的AI部署有广泛意义。

Abstract: As large language models (LLMs) and generative AI become increasingly
integrated into customer service and moderation applications, adversarial
threats emerge from both external manipulations and internal label corruption.
In this work, we identify and systematically address these dual adversarial
threats by introducing DINA (Dual Defense Against Internal Noise and
Adversarial Attacks), a novel unified framework tailored specifically for NLP.
Our approach adapts advanced noisy-label learning methods from computer vision
and integrates them with adversarial training to simultaneously mitigate
internal label sabotage and external adversarial perturbations. Extensive
experiments conducted on a real-world dataset from an online gaming service
demonstrate that DINA significantly improves model robustness and accuracy
compared to baseline models. Our findings not only highlight the critical
necessity of dual-threat defenses but also offer practical strategies for
safeguarding NLP systems in realistic adversarial scenarios, underscoring
broader implications for fair and responsible AI deployment.

</details>


### [17] [Towards Effective Offensive Security LLM Agents: Hyperparameter Tuning, LLM as a Judge, and a Lightweight CTF Benchmark](https://arxiv.org/abs/2508.05674)
*Minghao Shao,Nanda Rani,Kimberly Milner,Haoran Xi,Meet Udeshi,Saksham Aggarwal,Venkata Sai Charan Putrevu,Sandeep Kumar Shukla,Prashanth Krishnamurthy,Farshad Khorrami,Ramesh Karri,Muhammad Shafique*

Main category: cs.CR

TL;DR: 论文探讨了基于LLM的自动化攻防安全代理系统，提出了CTFJudge框架和CTF Competency Index (CCI)指标，研究了LLM超参数对性能的影响，并开源了CTFTiny基准测试集。


<details>
  <summary>Details</summary>
Motivation: 研究如何利用LLM提升自动化攻防任务（如CTF挑战）的效率，并探索影响代理成功的关键因素。

Method: 1. 提出CTFJudge框架，利用LLM评估代理轨迹；2. 设计CCI指标衡量部分正确性；3. 分析LLM超参数对性能的影响；4. 构建CTFTiny基准测试集。

Result: 确定了多代理协作的最优设置，为未来LLM在网络安全领域的研究奠定了基础。

Conclusion: 论文为构建高效的LLM攻防代理提供了详细方法，并开源了相关工具和数据集，推动了该领域的发展。

Abstract: Recent advances in LLM agentic systems have improved the automation of
offensive security tasks, particularly for Capture the Flag (CTF) challenges.
We systematically investigate the key factors that drive agent success and
provide a detailed recipe for building effective LLM-based offensive security
agents. First, we present CTFJudge, a framework leveraging LLM as a judge to
analyze agent trajectories and provide granular evaluation across CTF solving
steps. Second, we propose a novel metric, CTF Competency Index (CCI) for
partial correctness, revealing how closely agent solutions align with
human-crafted gold standards. Third, we examine how LLM hyperparameters, namely
temperature, top-p, and maximum token length, influence agent performance and
automated cybersecurity task planning. For rapid evaluation, we present
CTFTiny, a curated benchmark of 50 representative CTF challenges across binary
exploitation, web, reverse engineering, forensics, and cryptography. Our
findings identify optimal multi-agent coordination settings and lay the
groundwork for future LLM agent research in cybersecurity. We make CTFTiny open
source to public https://github.com/NYU-LLM-CTF/CTFTiny along with CTFJudge on
https://github.com/NYU-LLM-CTF/CTFJudge.

</details>


### [18] [Principle-Guided Verilog Optimization: IP-Safe Knowledge Transfer via Local-Cloud Collaboration](https://arxiv.org/abs/2508.05675)
*Jing Wang,Zheng Li,Lei Li,Fan He,Liyu Lin,Yao Lai,Yan Li,Xiaoyang Zeng,Yufeng Guo*

Main category: cs.CR

TL;DR: 提出了一种保护IP的边缘-云协作框架，利用本地小模型和云端大模型结合优化RTL代码，显著提升优化成功率。


<details>
  <summary>Details</summary>
Motivation: 解决云端大模型优化RTL代码时的IP泄露风险，同时利用其强大能力。

Method: 本地小模型分析代码对生成设计原则，云端大模型基于原则提供优化建议，避免直接处理敏感代码。

Result: 优化成功率显著提升（如Qwen-2.5-Coder-7B与Deepseek-V3组合达66.67%），优于单独使用云端模型。

Conclusion: 该框架在性能提升与IP保护间取得平衡，为硬件设计优化提供了新范式。

Abstract: Recent years have witnessed growing interest in adopting large language
models (LLMs) for Register Transfer Level (RTL) code optimization. While
powerful cloud-based LLMs offer superior optimization capabilities, they pose
unacceptable intellectual property (IP) leakage risks when processing
proprietary hardware designs. In this paper, we propose a new scenario where
Verilog code must be optimized for specific attributes without leaking
sensitive IP information. We introduce the first IP-preserving edge-cloud
collaborative framework that leverages the benefits of both paradigms. Our
approach employs local small LLMs (e.g., Qwen-2.5-Coder-7B) to perform secure
comparative analysis between paired high-quality target designs and novice
draft codes, yielding general design principles that summarize key insights for
improvements. These principles are then used to query stronger cloud LLMs
(e.g., Deepseek-V3) for targeted code improvement, ensuring that only
abstracted and IP-safe guidance reaches external services. Our experimental
results demonstrate that the framework achieves significantly higher
optimization success rates compared to baseline methods. For example, combining
Qwen-2.5-Coder-7B and Deepseek-V3 achieves a 66.67\% optimization success rate
for power utilization, outperforming Deepseek-V3 alone (49.81\%) and even
commercial models like GPT-4o (55.81\%). Further investigation of local and
cloud LLM combinations reveals that different model pairings exhibit varying
strengths for specific optimization objectives, with interesting trends
emerging when varying the number of comparative code pairs. Our work
establishes a new paradigm for secure hardware design optimization that
balances performance gains with IP protection.

</details>


### [19] [Adversarial Attacks on Reinforcement Learning-based Medical Questionnaire Systems: Input-level Perturbation Strategies and Medical Constraint Validation](https://arxiv.org/abs/2508.05677)
*Peizhuo Liu*

Main category: cs.CR

TL;DR: 该研究评估了对抗性攻击方法在基于强化学习的医疗问卷系统中的影响，发现即使有严格的医学约束，系统仍存在显著漏洞。


<details>
  <summary>Details</summary>
Motivation: 基于强化学习的医疗问卷系统在医疗场景中潜力巨大，但其安全性和鲁棒性尚未解决，研究旨在识别和分析其潜在漏洞。

Method: 将诊断过程建模为马尔可夫决策过程（MDP），实施六种对抗性攻击方法，并开发了包含247项医学约束的验证框架。

Result: 在NHIS数据集上，攻击成功率为33.08%至64.70%，表明对抗性攻击显著影响诊断准确性。

Conclusion: 即使输入有严格医学约束，基于强化学习的医疗问卷系统仍存在显著安全漏洞。

Abstract: RL-based medical questionnaire systems have shown great potential in medical
scenarios. However, their safety and robustness remain unresolved. This study
performs a comprehensive evaluation on adversarial attack methods to identify
and analyze their potential vulnerabilities. We formulate the diagnosis process
as a Markov Decision Process (MDP), where the state is the patient responses
and unasked questions, and the action is either to ask a question or to make a
diagnosis. We implemented six prevailing major attack methods, including the
Fast Gradient Signed Method (FGSM), Projected Gradient Descent (PGD), Carlini &
Wagner Attack (C&W) attack, Basic Iterative Method (BIM), DeepFool, and
AutoAttack, with seven epsilon values each. To ensure the generated adversarial
examples remain clinically plausible, we developed a comprehensive medical
validation framework consisting of 247 medical constraints, including
physiological bounds, symptom correlations, and conditional medical
constraints. We achieved a 97.6% success rate in generating clinically
plausible adversarial samples. We performed our experiment on the National
Health Interview Survey (NHIS) dataset (https://www.cdc.gov/nchs/nhis/), which
consists of 182,630 samples, to predict the participant's 4-year mortality
rate. We evaluated our attacks on the AdaptiveFS framework proposed in
arXiv:2004.00994. Our results show that adversarial attacks could significantly
impact the diagnostic accuracy, with attack success rates ranging from 33.08%
(FGSM) to 64.70% (AutoAttack). Our work has demonstrated that even under strict
medical constraints on the input, such RL-based medical questionnaire systems
still show significant vulnerabilities.

</details>


### [20] [Selection-Based Vulnerabilities: Clean-Label Backdoor Attacks in Active Learning](https://arxiv.org/abs/2508.05681)
*Yuhan Zhi,Longtian Wang,Xiaofei Xie,Chao Shen,Qiang Hu,Xiaohong Guan*

Main category: cs.CR

TL;DR: 本文提出ALA框架，首次利用主动学习中的获取函数作为攻击面，揭示其潜在风险。通过优化毒化输入以提高不确定性评分，攻击成功率高且隐蔽。


<details>
  <summary>Details</summary>
Motivation: 探究主动学习（AL）的安全性，揭示获取函数可能被利用的弱点。

Method: 设计ALA框架，优化毒化输入以提升不确定性评分，使其更易被获取函数选中。

Result: 在低毒化预算（0.5%-1.0%）下，攻击成功率高达94%，且不影响模型性能或人类标注者的检测。

Conclusion: 主动学习的获取函数易被利用，需在可信数据场景中谨慎部署。

Abstract: Active learning(AL), which serves as the representative label-efficient
learning paradigm, has been widely applied in resource-constrained scenarios.
The achievement of AL is attributed to acquisition functions, which are
designed for identifying the most important data to label. Despite this
success, one question remains unanswered: is AL safe? In this work, we
introduce ALA, a practical and the first framework to utilize the acquisition
function as the poisoning attack surface to reveal the weakness of active
learning. Specifically, ALA optimizes imperceptibly poisoned inputs to exhibit
high uncertainty scores, increasing their probability of being selected by
acquisition functions. To evaluate ALA, we conduct extensive experiments across
three datasets, three acquisition functions, and two types of clean-label
backdoor triggers. Results show that our attack can achieve high success rates
(up to 94%) even under low poisoning budgets (0.5%-1.0%) while preserving model
utility and remaining undetectable to human annotators. Our findings remind
active learning users: acquisition functions can be easily exploited, and
active learning should be deployed with caution in trusted data scenarios.

</details>


### [21] [MM-FusionNet: Context-Aware Dynamic Fusion for Multi-modal Fake News Detection with Large Vision-Language Models](https://arxiv.org/abs/2508.05684)
*Junhao He,Tianyu Liu,Jingyuan Zhao,Benjamin Turner*

Main category: cs.CR

TL;DR: MM-FusionNet是一个基于大型视觉语言模型（LVLMs）的多模态假新闻检测框架，通过动态融合模块（CADFM）自适应学习文本和视觉特征的权重，显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 多模态假新闻对社会信任和稳定构成威胁，传统文本检测方法因文本与图像的欺骗性交互而效果有限。

Method: 提出MM-FusionNet框架，核心是Context-Aware Dynamic Fusion Module（CADFM），利用双向跨模态注意力和动态模态门控网络自适应分配特征权重。

Result: 在包含80,000样本的LMFND数据集上，F1-score达到0.938，优于现有多模态和单模态方法。

Conclusion: MM-FusionNet在动态权重分配、模态扰动鲁棒性和解释性方面表现优异，接近人类水平，适用于实际假新闻检测。

Abstract: The proliferation of multi-modal fake news on social media poses a
significant threat to public trust and social stability. Traditional detection
methods, primarily text-based, often fall short due to the deceptive interplay
between misleading text and images. While Large Vision-Language Models (LVLMs)
offer promising avenues for multi-modal understanding, effectively fusing
diverse modal information, especially when their importance is imbalanced or
contradictory, remains a critical challenge. This paper introduces
MM-FusionNet, an innovative framework leveraging LVLMs for robust multi-modal
fake news detection. Our core contribution is the Context-Aware Dynamic Fusion
Module (CADFM), which employs bi-directional cross-modal attention and a novel
dynamic modal gating network. This mechanism adaptively learns and assigns
importance weights to textual and visual features based on their contextual
relevance, enabling intelligent prioritization of information. Evaluated on the
large-scale Multi-modal Fake News Dataset (LMFND) comprising 80,000 samples,
MM-FusionNet achieves a state-of-the-art F1-score of 0.938, surpassing existing
multi-modal baselines by approximately 0.5% and significantly outperforming
single-modal approaches. Further analysis demonstrates the model's dynamic
weighting capabilities, its robustness to modality perturbations, and
performance remarkably close to human-level, underscoring its practical
efficacy and interpretability for real-world fake news detection.

</details>


### [22] [Secure and Scalable Blockchain Voting: A Comparative Framework and the Role of Large Language Models](https://arxiv.org/abs/2508.05865)
*Kiana Kiashemshaki,Elvis Nnaemeka Chukwuani,Mohammad Jalili Torkamani,Negin Mahmoudi*

Main category: cs.CR

TL;DR: 论文提出了一种比较框架，分析基于区块链的电子投票系统架构、共识机制和加密协议，并提出优化策略，同时探索大语言模型在智能合约生成和异常检测中的新角色。


<details>
  <summary>Details</summary>
Motivation: 区块链技术为电子投票系统提供了透明、去中心化和安全的潜力，但实际应用中仍面临可扩展性、计算复杂性和隐私需求等挑战。

Method: 通过比较框架分析区块链电子投票的架构、共识机制和加密协议，提出混合共识、轻量级加密和去中心化身份管理等优化策略，并探索大语言模型的应用。

Result: 研究为设计安全、可扩展且智能的区块链电子投票系统提供了基础，支持国家规模部署。

Conclusion: 论文为构建端到端区块链电子投票原型奠定了基础，结合大语言模型和系统化框架，支持智能合约生成与验证。

Abstract: Blockchain technology offers a promising foundation for modernizing E-Voting
systems by enhancing transparency, decentralization, and security. Yet,
real-world adoption remains limited due to persistent challenges such as
scalability constraints, high computational demands, and complex privacy
requirements. This paper presents a comparative framework for analyzing
blockchain-based E-Voting architectures, consensus mechanisms, and
cryptographic protocols. We examine the limitations of prevalent models like
Proof of Work, Proof of Stake, and Delegated Proof of Stake, and propose
optimization strategies that include hybrid consensus, lightweight
cryptography, and decentralized identity management. Additionally, we explore
the novel role of Large Language Models (LLMs) in smart contract generation,
anomaly detection, and user interaction. Our findings offer a foundation for
designing secure, scalable, and intelligent blockchain-based E-Voting systems
suitable for national-scale deployment. This work lays the groundwork for
building an end-to-end blockchain E-Voting prototype enhanced by LLM-guided
smart contract generation and validation, supported by a systematic framework
and simulation-based analysis.

</details>


### [23] [Leveraging large language models for SQL behavior-based database intrusion detection](https://arxiv.org/abs/2508.05690)
*Meital Shlezinger,Shay Akirav,Lei Zhou,Liang Guo,Avi Kessel,Guoliang Li*

Main category: cs.CR

TL;DR: 本文提出了一种基于DistilBERT的两层异常检测方法，结合无监督和监督机器学习技术，用于识别SQL数据库中的异常访问行为。


<details>
  <summary>Details</summary>
Motivation: 数据库异常访问行为（如内部和外部攻击）日益增多，现有方法缺乏细粒度检测能力，容易误判或漏判。

Method: 使用无监督的集成异常检测器标记异常查询，并结合监督的微调Transformer模型（DistilBERT）进行高精度内部攻击检测。

Result: 该方法能有效识别异常行为，减少数据标注需求，为数据库安全提供高效解决方案。

Conclusion: 提出的两层方法显著提升了数据库异常检测的准确性和效率，适用于复杂威胁防护。

Abstract: Database systems are extensively used to store critical data across various
domains. However, the frequency of abnormal database access behaviors, such as
database intrusion by internal and external attacks, continues to rise.
Internal masqueraders often have greater organizational knowledge, making it
easier to mimic employee behavior effectively. In contrast, external
masqueraders may behave differently due to their lack of familiarity with the
organization. Current approaches lack the granularity needed to detect
anomalies at the operational level, frequently misclassifying entire sequences
of operations as anomalies, even though most operations are likely to represent
normal behavior. On the other hand, some anomalous behaviors often resemble
normal activities, making them difficult for existing detection methods to
identify. This paper introduces a two-tiered anomaly detection approach for
Structured Query Language (SQL) using the Bidirectional Encoder Representations
from Transformers (BERT) model, specifically DistilBERT, a more efficient,
pre-trained version. Our method combines both unsupervised and supervised
machine learning techniques to accurately identify anomalous activities while
minimizing the need for data labeling. First, the unsupervised method uses
ensemble anomaly detectors that flag embedding vectors distant from learned
normal patterns of typical user behavior across the database (out-of-scope
queries). Second, the supervised method uses fine-tuned transformer-based
models to detect internal attacks with high precision (in-scope queries), using
role-labeled classification, even on limited labeled SQL data. Our findings
make a significant contribution by providing an effective solution for
safeguarding critical database systems from sophisticated threats.

</details>


### [24] [AuthPrint: Fingerprinting Generative Models Against Malicious Model Providers](https://arxiv.org/abs/2508.05691)
*Kai Yao,Marc Juarez*

Main category: cs.CR

TL;DR: 论文提出了一种对抗性模型指纹技术，用于验证生成模型输出的来源，即使在模型提供者可能敌对的情况下也能有效工作。


<details>
  <summary>Details</summary>
Motivation: 生成模型在高风险领域应用广泛，但缺乏验证输出来源的机制，尤其是在提供者可能敌对的情况下。

Method: 通过可信验证器从模型输出空间中提取秘密指纹，训练模型预测和验证这些指纹。

Result: 方法在GAN和扩散模型上实现了接近零的FPR@95%TPR，并对架构和数据的微小修改保持鲁棒性。

Conclusion: 该技术为生成模型输出来源验证提供了有效且鲁棒的解决方案。

Abstract: Generative models are increasingly adopted in high-stakes domains, yet
current deployments offer no mechanisms to verify the origin of model outputs.
We address this gap by extending model fingerprinting techniques beyond the
traditional collaborative setting to one where the model provider may act
adversarially. To our knowledge, this is the first work to evaluate
fingerprinting for provenance attribution under such a threat model. The
methods rely on a trusted verifier that extracts secret fingerprints from the
model's output space, unknown to the provider, and trains a model to predict
and verify them. Our empirical evaluation shows that our methods achieve
near-zero FPR@95%TPR for instances of GAN and diffusion models, even when
tested on small modifications to the original architecture and training data.
Moreover, the methods remain robust against adversarial attacks that actively
modify the outputs to bypass detection. Source codes are available at
https://github.com/PSMLab/authprint.

</details>


### [25] [DMFI: Dual-Modality Fine-Tuning and Inference Framework for LLM-Based Insider Threat Detection](https://arxiv.org/abs/2508.05694)
*Kaichuan Kong,Dongjie Liu,Xiaobo Jin,Guanggang Geng,Zhiying Li,Jian Weng*

Main category: cs.CR

TL;DR: DMFI是一个双模态框架，结合语义推理和行为感知微调，用于检测内部威胁，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统模型难以捕捉语义意图和复杂行为动态，现有LLM解决方案在提示适应性和模态覆盖方面存在局限。

Method: DMFI将原始日志转换为语义视图和行为抽象，使用两个LoRA增强的LLM独立微调，并通过MLP模块融合输出。

Result: 在CERT r4.2和r5.2数据集上，DMFI在检测准确性上优于现有方法。

Conclusion: DMFI结合LLM语义推理和结构化行为建模，为实际内部威胁检测提供了可扩展且有效的解决方案。

Abstract: Insider threat detection (ITD) poses a persistent and high-impact challenge
in cybersecurity due to the subtle, long-term, and context-dependent nature of
malicious insider behaviors. Traditional models often struggle to capture
semantic intent and complex behavior dynamics, while existing LLM-based
solutions face limitations in prompt adaptability and modality coverage. To
bridge this gap, we propose DMFI, a dual-modality framework that integrates
semantic inference with behavior-aware fine-tuning. DMFI converts raw logs into
two structured views: (1) a semantic view that processes content-rich artifacts
(e.g., emails, https) using instruction-formatted prompts; and (2) a behavioral
abstraction, constructed via a 4W-guided (When-Where-What-Which) transformation
to encode contextual action sequences. Two LoRA-enhanced LLMs are fine-tuned
independently, and their outputs are fused via a lightweight MLP-based decision
module. We further introduce DMFI-B, a discriminative adaptation strategy that
separates normal and abnormal behavior representations, improving robustness
under severe class imbalance. Experiments on CERT r4.2 and r5.2 datasets
demonstrate that DMFI outperforms state-of-the-art methods in detection
accuracy. Our approach combines the semantic reasoning power of LLMs with
structured behavior modeling, offering a scalable and effective solution for
real-world insider threat detection. Our work demonstrates the effectiveness of
combining LLM reasoning with structured behavioral modeling, offering a
scalable and deployable solution for modern insider threat detection.

</details>


### [26] [MambaITD: An Efficient Cross-Modal Mamba Network for Insider Threat Detection](https://arxiv.org/abs/2508.05695)
*Kaichuan Kong,Dongjie Liu,Xiaobo Jin,Zhiying Li,Guanggang Geng,Jian Weng*

Main category: cs.CR

TL;DR: 本文提出了一种基于Mamba状态空间模型和跨模态自适应融合的新型内部威胁检测框架MambaITD，解决了现有方法在时间动态特征建模、计算效率和实时性以及跨模态信息孤岛问题上的不足。


<details>
  <summary>Details</summary>
Motivation: 企业内部威胁风险增加，现有检测方法因时间动态特征建模不足、计算效率低、实时性差和跨模态信息孤岛问题而无法有效应对。

Method: 提出MambaITD框架，包括多源日志预处理、Mamba编码器建模长程依赖、门控特征融合机制和基于最大化类间方差的动态阈值优化方法。

Result: MambaITD在建模效率和特征融合能力上优于传统方法，表现优于基于Transformer的方法。

Conclusion: MambaITD为内部威胁检测提供了更有效的解决方案。

Abstract: Enterprises are facing increasing risks of insider threats, while existing
detection methods are unable to effectively address these challenges due to
reasons such as insufficient temporal dynamic feature modeling, computational
efficiency and real-time bottlenecks and cross-modal information island
problem. This paper proposes a new insider threat detection framework MambaITD
based on the Mamba state space model and cross-modal adaptive fusion. First,
the multi-source log preprocessing module aligns heterogeneous data through
behavioral sequence encoding, interval smoothing, and statistical feature
extraction. Second, the Mamba encoder models long-range dependencies in
behavioral and interval sequences, and combines the sequence and statistical
information dynamically in combination with the gated feature fusion mechanism.
Finally, we propose an adaptive threshold optimization method based on
maximizing inter-class variance, which dynamically adjusts the decision
threshold by analyzing the probability distribution, effectively identifies
anomalies, and alleviates class imbalance and concept drift. Compared with
traditional methods, MambaITD shows significant advantages in modeling
efficiency and feature fusion capabilities, outperforming Transformer-based
methods, and provides a more effective solution for insider threat detection.

</details>


### [27] [Log2Sig: Frequency-Aware Insider Threat Detection via Multivariate Behavioral Signal Decomposition](https://arxiv.org/abs/2508.05696)
*Kaichuan Kong,Dongjie Liu,Xiaobo Jin,Zhiying Li,Guanggang Geng*

Main category: cs.CR

TL;DR: Log2Sig是一个新颖的框架，通过将用户日志转化为多变量行为频率信号，结合多尺度分解和双视图表示融合，显著提升了内部威胁检测的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法将系统日志建模为扁平事件序列，无法捕捉用户行为中的频率动态和多尺度扰动模式，导致检测效果受限。

Method: Log2Sig利用MVMD提取多尺度行为波动，结合Mamba编码器和频率信号线性投影，构建双视图表示，并通过多层感知机进行异常检测。

Result: 在CERT r4.2和r5.2数据集上，Log2Sig在准确率和F1分数上显著优于现有基线方法。

Conclusion: Log2Sig通过多尺度频率分析和双视图融合，有效解决了内部威胁检测中的行为动态捕捉问题，具有显著性能优势。

Abstract: Insider threat detection presents a significant challenge due to the
deceptive nature of malicious behaviors, which often resemble legitimate user
operations. However, existing approaches typically model system logs as flat
event sequences, thereby failing to capture the inherent frequency dynamics and
multiscale disturbance patterns embedded in user behavior. To address these
limitations, we propose Log2Sig, a robust anomaly detection framework that
transforms user logs into multivariate behavioral frequency signals,
introducing a novel representation of user behavior. Log2Sig employs
Multivariate Variational Mode Decomposition (MVMD) to extract Intrinsic Mode
Functions (IMFs), which reveal behavioral fluctuations across multiple temporal
scales. Based on this, the model further performs joint modeling of behavioral
sequences and frequency-decomposed signals: the daily behavior sequences are
encoded using a Mamba-based temporal encoder to capture long-term dependencies,
while the corresponding frequency components are linearly projected to match
the encoder's output dimension. These dual-view representations are then fused
to construct a comprehensive user behavior profile, which is fed into a
multilayer perceptron for precise anomaly detection. Experimental results on
the CERT r4.2 and r5.2 datasets demonstrate that Log2Sig significantly
outperforms state-of-the-art baselines in both accuracy and F1 score.

</details>


### [28] [System Security Framework for 5G Advanced /6G IoT Integrated Terrestrial Network-Non-Terrestrial Network (TN-NTN) with AI-Enabled Cloud Security](https://arxiv.org/abs/2508.05707)
*Sasa Maric,Rasil Baidar,Robert Abbas,Sam Reisenfeld*

Main category: cs.CR

TL;DR: 本文提出了一种基于AI原生云安全的系统级安全框架，用于5G Advanced/6G物联网与地面-非地面网络（TN-NTN）的集成，解决其异构性和分布式特性带来的安全挑战。


<details>
  <summary>Details</summary>
Motivation: 随着5G Advanced/6G和物联网技术与低轨卫星、高空平台及无人机的集成，全球连接性面临新的安全挑战，需要创新的安全解决方案。

Method: 利用AI原生云平台实现实时威胁检测、安全自动化和智能策略执行，并通过卫星接入功能增强非连续覆盖的安全性。

Result: 提出了一个全面的AI驱动的云安全框架，支持零信任原则、联邦学习、安全编排和多层安全防护。

Conclusion: 未来5G Advanced/6G物联网网络应实施AI驱动的卫星非地面网络，强调安全性和抗对抗性威胁的弹性。

Abstract: The integration of Terrestrial Networks (TN) and Non-Terrestrial Networks
(NTN), including 5G Advanced/6G and the Internet of Things (IoT) technologies,
using Low Earth Orbit (LEO) satellites, high-altitude platforms (HAPS), and
Unmanned Aerial Vehicles (UAVs), is redefining the landscape of global
connectivity. This paper introduces a new system-level security framework for
5G Advanced/6G IoT-integrated TN-NTN architectures with AI-native-enabled cloud
security. Due to the heterogeneity, scale, and distributed nature of these
networks, new security challenges have emerged. Leveraging AI-native cloud
platforms offers powerful capabilities for real-time threat detection, security
automation, and intelligent policy enforcement. The NTN satellite access
function enhances security for discontinuous coverage via satellite
connections. In addition, this paper explores the security risks associated
with integrated 5G Advanced/6G IoT TN-NTN systems, including full network
segmentation, network slicing, and the cloudification of the RAN and core. We
present a comprehensive AI-enabled cloud security framework and conclude with
proposals for implementing AI-powered, satellite-based NTN within future 5G
Advanced/6G IoT networks. Our approach emphasizes zero-trust principles,
federated learning, secure orchestration, a layered security framework, and
resilience against adversarial threats.

</details>


### [29] [On Digital Twins in Defence: Overview and Applications](https://arxiv.org/abs/2508.05717)
*Marco Giberna,Holger Voos,Paulo Tavares,João Nunes,Tobias Sorg,Andrea Masini,Jose Luis Sanchez-Lopez*

Main category: cs.CR

TL;DR: 本文探讨了数字孪生技术在国防领域的应用，包括设计、训练、任务执行等，提出了标准化框架，并分析了实施中的挑战与未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 数字孪生技术因其实时监控和优化能力在多个领域受到关注，本文旨在探索其在国防领域的潜力与应用。

Method: 通过分析科学文献、行业实践、政府策略及利益相关者调查，研究数字孪生在国防中的应用与挑战。

Result: 数字孪生可提升国防系统的性能、预测能力和运行时间，但实施中仍存在标准化和互操作性等挑战。

Conclusion: 未来需加强框架建设和跨学科合作，以充分发挥数字孪生在国防领域的潜力。

Abstract: Digital twin technology has gained increasing attention across various
sectors due to its ability to create virtual replicas of physical systems,
enabling real-time monitoring, optimization, and simulation. This paper
explores the integration of digital twins within defence applications, focusing
on key use cases ranging from system design and development, operational
planning and training, to mission execution and debriefing. By examining the
application of digital twin technologies across defense platforms, we highlight
their key advantages such as enhanced operational performance, predictive
capabilities, and increased system uptime. Additionally, we introduce a novel
characterization framework for digital twins that aims to standardize and unify
their application across different defence domains to facilitate
interoperability. Thereafter, we discuss the main challenges, gaps and
limitations in implementing and adopting digital twins within defence
organizations by analyzing a combination of scientific literature, current
industry practices, governmental strategies, and the findings from a
comprehensive survey of industrial stakeholders and ministries of defense.
Finally, we outline future research directions and development opportunities,
emphasizing the need for robust frameworks and interdisciplinary collaborations
to fully realize the potential of digital twins in the defence sector.

</details>


### [30] [Fact2Fiction: Targeted Poisoning Attack to Agentic Fact-checking System](https://arxiv.org/abs/2508.06059)
*Haorui He,Yupeng Li,Bin Benjamin Zhu,Dacheng Wen,Reynold Cheng,Francis C. M. Lau*

Main category: cs.CR

TL;DR: Fact2Fiction是一种针对基于LLM的自动事实核查系统的首个投毒攻击框架，通过模仿分解策略和利用系统生成的解释来制造恶意证据，攻击成功率比现有方法高8.9%--21.2%。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的事实核查系统存在安全漏洞，容易被攻击者利用以放大虚假信息，因此需要研究其安全性。

Method: Fact2Fiction通过模仿系统的分解策略并利用其生成的解释，制造针对子声明验证的恶意证据。

Result: 实验表明，Fact2Fiction的攻击成功率比现有方法高8.9%--21.2%。

Conclusion: Fact2Fiction揭示了当前事实核查系统的安全弱点，强调了防御措施的必要性。

Abstract: State-of-the-art fact-checking systems combat misinformation at scale by
employing autonomous LLM-based agents to decompose complex claims into smaller
sub-claims, verify each sub-claim individually, and aggregate the partial
results to produce verdicts with justifications (explanatory rationales for the
verdicts). The security of these systems is crucial, as compromised
fact-checkers, which tend to be easily underexplored, can amplify
misinformation. This work introduces Fact2Fiction, the first poisoning attack
framework targeting such agentic fact-checking systems. Fact2Fiction mirrors
the decomposition strategy and exploits system-generated justifications to
craft tailored malicious evidences that compromise sub-claim verification.
Extensive experiments demonstrate that Fact2Fiction achieves 8.9\%--21.2\%
higher attack success rates than state-of-the-art attacks across various
poisoning budgets. Fact2Fiction exposes security weaknesses in current
fact-checking systems and highlights the need for defensive countermeasures.

</details>


### [31] [A Game-Theoretic Foundation for Bitcoin's Price: A Security-Utility Equilibrium](https://arxiv.org/abs/2508.06071)
*Liang Chen*

Main category: cs.CR

TL;DR: 本文提出了一种结构化的博弈论模型（RESUNE）来评估去中心化数字资产（如比特币）的价值，通过市场均衡价格和哈希率内生决定网络安全，并证明了其存在性、唯一性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖投机信念，本文旨在通过理性预期安全-效用纳什均衡（RESUNE）更科学地评估数字资产价格。

Method: 采用RESUNE框架，结合自由进入挖矿模型和全局博弈模型，分析价格、哈希率和网络安全的关系。

Result: 证明了RESUNE的存在性，并提出了稳定条件；模型预测价格对需求的直接影响需超过价格对安全的反馈效应。

Conclusion: 该模型为比特币价值分解（交易效用、安全和投机）提供了理论支持，并解释了价格对哈希率的单向因果关系。

Abstract: This paper introduces a structural game-theoretic model to value
decentralized digital assets like Bitcoin. Instead of relying on speculative
beliefs, it frames the asset's price within a Rational-Expectations
Security-Utility Nash Equilibrium (RESUNE). This equilibrium is a fixed point
where the market-clearing price dictates the hash rate through a free-entry
mining model, which in turn endogenously sets the network's security. The
security, defined as one minus the probability of a 51% attack, is determined
via a global games model of attacker coordination, providing a unique and
continuous security function. We prove the existence of a RESUNE and offer
conditions for its uniqueness and stability. The model predicts that the
stabilizing direct effect of price on demand must outweigh the potentially
destabilizing feedback from price to security. The framework generates testable
predictions, such as a protocol halving causing a contraction in both hash rate
and price. A structural Vector Autoregression (VAR) model is proposed to test
this mechanism. The model decomposes Bitcoin's value into transactional
utility, security, and speculative components and explains the observed
unidirectional causality from price to hash rate.

</details>


### [32] [ProvX: Generating Counterfactual-Driven Attack Explanations for Provenance-Based Detection](https://arxiv.org/abs/2508.06073)
*Weiheng Wu,Wei Qiao,Teng Li,Yebo Feng,Zhuo Ma,Jianfeng Ma,Yang Liu*

Main category: cs.CR

TL;DR: ProvX是一个解释框架，用于解释基于图神经网络的入侵检测模型，通过反事实解释逻辑和优化任务提高解释的准确性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 解决GNN模型在安全领域中的黑盒问题，为安全分析师提供可验证的解释和证据。

Method: 引入反事实解释逻辑，将离散搜索问题转化为连续优化任务，并结合分阶段固化策略。

Result: ProvX能定位与实际攻击高度相关的关键图结构，解释必要性平均达51.59%，优于现有技术。

Conclusion: ProvX不仅能提供有效解释，还能通过闭环框架增强模型对抗攻击的鲁棒性。

Abstract: Provenance graph-based intrusion detection systems are deployed on hosts to
defend against increasingly severe Advanced Persistent Threat. Using Graph
Neural Networks to detect these threats has become a research focus and has
demonstrated exceptional performance. However, the widespread adoption of
GNN-based security models is limited by their inherent black-box nature, as
they fail to provide security analysts with any verifiable explanations for
model predictions or any evidence regarding the model's judgment in relation to
real-world attacks. To address this challenge, we propose ProvX, an effective
explanation framework for exlaining GNN-based security models on provenance
graphs. ProvX introduces counterfactual explanation logic, seeking the minimal
structural subset within a graph predicted as malicious that, when perturbed,
can subvert the model's original prediction. We innovatively transform the
discrete search problem of finding this critical subgraph into a continuous
optimization task guided by a dual objective of prediction flipping and
distance minimization. Furthermore, a Staged Solidification strategy is
incorporated to enhance the precision and stability of the explanations. We
conducted extensive evaluations of ProvX on authoritative datasets. The
experimental results demonstrate that ProvX can locate critical graph
structures that are highly relevant to real-world attacks and achieves an
average explanation necessity of 51.59\%, with these metrics outperforming
current SOTA explainers. Furthermore, we explore and provide a preliminary
validation of a closed-loop Detection-Explanation-Feedback enhancement
framework, demonstrating through experiments that the explanation results from
ProvX can guide model optimization, effectively enhancing its robustness
against adversarial attacks.

</details>


### [33] [Adaptive Backtracking for Privacy Protection in Large Language Models](https://arxiv.org/abs/2508.06087)
*Zhihao Yao,Yuxuan Gu,Xiachong Feng,Weitao Ma,Bo Li,Xiaocheng Feng*

Main category: cs.CR

TL;DR: 论文提出企业隐私保护新目标，解决现有方法性能下降和数据集缺失问题，通过ABack机制和PriGenQA基准实现高效保护。


<details>
  <summary>Details</summary>
Motivation: 当前隐私保护研究多关注用户隐私，忽视企业数据泄露风险，尤其在检索增强生成范式下问题更严重。

Method: 提出ABack机制（无训练方法）和PriGenQA基准数据集，并开发自适应攻击者进行严格评估。

Result: ABack在对抗强攻击者时，隐私效用分数提升15%，避免性能折衷。

Conclusion: 论文为企业隐私保护提供了有效解决方案，填补了研究空白。

Abstract: The preservation of privacy has emerged as a critical topic in the era of
artificial intelligence. However, current work focuses on user-oriented
privacy, overlooking severe enterprise data leakage risks exacerbated by the
Retrieval-Augmented Generation paradigm. To address this gap, our paper
introduces a novel objective: enterprise-oriented privacy concerns. Achieving
this objective requires overcoming two fundamental challenges: existing methods
such as data sanitization severely degrade model performance, and the field
lacks public datasets for evaluation. We address these challenges with several
solutions. (1) To prevent performance degradation, we propose ABack, a
training-free mechanism that leverages a Hidden State Model to pinpoint the
origin of a leakage intention and rewrite the output safely. (2) To solve the
lack of datasets, we construct PriGenQA, a new benchmark for enterprise privacy
scenarios in healthcare and finance. To ensure a rigorous evaluation, we move
beyond simple static attacks by developing a powerful adaptive attacker with
Group Relative Policy Optimization. Experiments show that against this superior
adversary, ABack improves the overall privacy utility score by up to 15\% over
strong baselines, avoiding the performance trade-offs of prior methods.

</details>


### [34] [Simulation in Cybersecurity: Understanding Techniques, Applications, and Goals](https://arxiv.org/abs/2508.06106)
*Luca Serena,Gabriele D'Angelo,Stefano Ferretti,Moreno Marzolla*

Main category: cs.CR

TL;DR: 论文综述了网络安全研究中建模与模拟的现状，通过四个维度分类现有研究，分析不同方法的优缺点，并探讨适合模拟研究的网络威胁和建模范式。


<details>
  <summary>Details</summary>
Motivation: 网络安全研究中的建模与模拟方法多样，但缺乏整体性分析，现有综述多局限于特定技术或领域，难以全面把握趋势。

Method: 通过四个维度（应用领域、网络威胁类型、模拟技术、模拟目标）对现有研究进行分类，分析不同方法的优缺点。

Result: 总结了不同方法的适用性，识别了最适合模拟研究的网络威胁类型，并分析了针对特定网络安全挑战的最佳建模范式。

Conclusion: 该综述为网络安全研究提供了全面的方法论参考，有助于指导未来的模拟研究。

Abstract: Modeling and simulation are widely used in cybersecurity research to assess
cyber threats, evaluate defense mechanisms, and analyze vulnerabilities.
However, the diversity of application areas, the variety of cyberattacks
scenarios, and the differing objectives of these simulations makes it difficult
to identify methodological trends. Existing reviews often focus on specific
modeling techniques or application domains, making it challenging to analyze
the field as a whole. To address these limitations, we present a comprehensive
review of the current state of the art, classifying the selected papers based
on four dimensions: the application domain, the types of cyber threats
represented, the simulation techniques employed, and the primary goals of the
simulation. The review discusses the strengths and limitations of different
approaches, identifies which cyber threats are the most suited for
simulation-based investigations, and analyzes which modeling paradigms are most
appropriate for specific cybersecurity challenges.

</details>


### [35] [SLIP: Soft Label Mechanism and Key-Extraction-Guided CoT-based Defense Against Instruction Backdoor in APIs](https://arxiv.org/abs/2508.06153)
*Zhengxian Wu,Juan Wen,Wanli Peng,Haowei Chang,Yinghan Zhou,Yiming Xue*

Main category: cs.CR

TL;DR: SLIP是一种针对黑盒后门攻击的防御机制，通过关键提取引导的思维链（KCoT）和软标签机制（SLM）有效降低攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 随着定制化大语言模型（LLM）代理的发展，黑盒后门攻击威胁加剧，现有防御机制无法有效应对。

Method: 提出SLIP，结合KCoT提取任务相关关键词，并通过SLM量化语义相关性，排除异常分数以提高可靠性。

Result: 实验表明SLIP将攻击成功率从90.2%降至25.13%，同时保持高准确率。

Conclusion: SLIP是一种高效防御机制，显著提升LLM安全性。

Abstract: With the development of customized large language model (LLM) agents, a new
threat of black-box backdoor attacks has emerged, where malicious instructions
are injected into hidden system prompts. These attacks easily bypass existing
defenses that rely on white-box access, posing a serious security challenge. To
address this, we propose SLIP, a Soft Label mechanism and key-extraction-guided
CoT-based defense against Instruction backdoors in APIs. SLIP is designed based
on two key insights. First, to counteract the model's oversensitivity to
triggers, we propose a Key-extraction-guided Chain-of-Thought (KCoT). Instead
of only considering the single trigger or the input sentence, KCoT prompts the
agent to extract task-relevant key phrases. Second, to guide the LLM toward
correct answers, our proposed Soft Label Mechanism (SLM) prompts the agent to
quantify the semantic correlation between key phrases and candidate answers.
Crucially, to mitigate the influence of residual triggers or misleading content
in phrases extracted by KCoT, which typically causes anomalous scores, SLM
excludes anomalous scores deviating significantly from the mean and
subsequently averages the remaining scores to derive a more reliable semantic
representation. Extensive experiments on classification and question-answer
(QA) tasks demonstrate that SLIP is highly effective, reducing the average
attack success rate (ASR) from 90.2% to 25.13% while maintaining high accuracy
on clean data and outperforming state-of-the-art defenses. Our code are
available in
https://github.com/CAU-ISS-Lab/Backdoor-Attack-Defense-LLMs/tree/main/SLIP.

</details>


### [36] [Anti-Tamper Protection for Unauthorized Individual Image Generation](https://arxiv.org/abs/2508.06325)
*Zelin Li,Ruohan Zong,Yifan Liu,Ruichen Yao,Yaokun Liu,Yang Zhang,Dong Wang*

Main category: cs.CR

TL;DR: 提出了一种新型抗篡改扰动（ATP）方法，结合保护扰动和授权扰动，有效防御伪造攻击并检测净化篡改。


<details>
  <summary>Details</summary>
Motivation: 随着个性化图像生成技术的发展，伪造攻击侵犯肖像权和隐私的问题日益严重，现有保护扰动算法易被净化技术绕过。

Method: ATP在频域中结合保护扰动和授权扰动，通过掩码指导确保两者互不干扰，授权扰动分布在全图像素以保持对净化篡改的敏感性。

Result: 实验证明ATP在不同攻击场景下均能有效防御伪造攻击，保护肖像权和隐私。

Conclusion: ATP为肖像权和隐私保护提供了鲁棒解决方案，代码已开源。

Abstract: With the advancement of personalized image generation technologies, concerns
about forgery attacks that infringe on portrait rights and privacy are growing.
To address these concerns, protection perturbation algorithms have been
developed to disrupt forgery generation. However, the protection algorithms
would become ineffective when forgery attackers apply purification techniques
to bypass the protection. To address this issue, we present a novel approach,
Anti-Tamper Perturbation (ATP). ATP introduces a tamper-proof mechanism within
the perturbation. It consists of protection and authorization perturbations,
where the protection perturbation defends against forgery attacks, while the
authorization perturbation detects purification-based tampering. Both
protection and authorization perturbations are applied in the frequency domain
under the guidance of a mask, ensuring that the protection perturbation does
not disrupt the authorization perturbation. This design also enables the
authorization perturbation to be distributed across all image pixels,
preserving its sensitivity to purification-based tampering. ATP demonstrates
its effectiveness in defending forgery attacks across various attack settings
through extensive experiments, providing a robust solution for protecting
individuals' portrait rights and privacy. Our code is available at:
https://github.com/Seeyn/Anti-Tamper-Perturbation .

</details>


### [37] [When AIOps Become "AI Oops": Subverting LLM-driven IT Operations via Telemetry Manipulation](https://arxiv.org/abs/2508.06394)
*Dario Pasquini,Evgenios M. Kornaropoulos,Giuseppe Ateniese,Omer Akgul,Athanasios Theocharis,Petros Efstathopoulos*

Main category: cs.CR

TL;DR: 论文首次对AIOps解决方案进行安全分析，揭示其易受攻击性，并提出防御机制AIOpsShield。


<details>
  <summary>Details</summary>
Motivation: 随着AIOps解决方案的普及，其安全性问题尚未被充分研究，作者旨在揭示其潜在威胁。

Method: 通过AIOpsDoom攻击方法，结合侦察、模糊测试和LLM驱动的对抗输入生成，演示如何操纵遥测数据误导AIOps代理。

Result: 实验证明AIOpsShield能有效防御遥测攻击，且不影响代理正常性能。

Conclusion: AIOps成为新兴攻击向量，亟需安全设计。

Abstract: AI for IT Operations (AIOps) is transforming how organizations manage complex
software systems by automating anomaly detection, incident diagnosis, and
remediation. Modern AIOps solutions increasingly rely on autonomous LLM-based
agents to interpret telemetry data and take corrective actions with minimal
human intervention, promising faster response times and operational cost
savings.
  In this work, we perform the first security analysis of AIOps solutions,
showing that, once again, AI-driven automation comes with a profound security
cost. We demonstrate that adversaries can manipulate system telemetry to
mislead AIOps agents into taking actions that compromise the integrity of the
infrastructure they manage. We introduce techniques to reliably inject
telemetry data using error-inducing requests that influence agent behavior
through a form of adversarial reward-hacking; plausible but incorrect system
error interpretations that steer the agent's decision-making. Our attack
methodology, AIOpsDoom, is fully automated--combining reconnaissance, fuzzing,
and LLM-driven adversarial input generation--and operates without any prior
knowledge of the target system.
  To counter this threat, we propose AIOpsShield, a defense mechanism that
sanitizes telemetry data by exploiting its structured nature and the minimal
role of user-generated content. Our experiments show that AIOpsShield reliably
blocks telemetry-based attacks without affecting normal agent performance.
  Ultimately, this work exposes AIOps as an emerging attack vector for system
compromise and underscores the urgent need for security-aware AIOps design.

</details>


### [38] [ScamAgents: How AI Agents Can Simulate Human-Level Scam Calls](https://arxiv.org/abs/2508.06457)
*Sanket Badhe*

Main category: cs.CR

TL;DR: 论文介绍了ScamAgent，一种基于大语言模型（LLM）的自主多轮对话代理，能够生成高度逼真的诈骗电话脚本，并揭示了现有LLM安全防护措施的不足。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索LLM在生成诈骗内容方面的潜在滥用风险，以及现有安全防护措施在多轮对话场景中的局限性。

Method: 方法包括构建ScamAgent代理，利用对话记忆、动态适应模拟用户响应和欺骗性说服策略，生成多轮诈骗脚本，并通过文本转语音技术实现自动化诈骗流程。

Result: 结果表明，现有的LLM安全防护措施（如拒绝机制和内容过滤器）无法有效抵御基于代理的威胁，且分解或增量式提示可以绕过这些防护。

Conclusion: 结论强调了进行多轮安全审计、开发代理级控制框架以及检测和阻断生成式AI驱动的对话欺骗的紧迫性。

Abstract: Large Language Models (LLMs) have demonstrated impressive fluency and
reasoning capabilities, but their potential for misuse has raised growing
concern. In this paper, we present ScamAgent, an autonomous multi-turn agent
built on top of LLMs, capable of generating highly realistic scam call scripts
that simulate real-world fraud scenarios. Unlike prior work focused on
single-shot prompt misuse, ScamAgent maintains dialogue memory, adapts
dynamically to simulated user responses, and employs deceptive persuasion
strategies across conversational turns. We show that current LLM safety
guardrails, including refusal mechanisms and content filters, are ineffective
against such agent-based threats. Even models with strong prompt-level
safeguards can be bypassed when prompts are decomposed, disguised, or delivered
incrementally within an agent framework. We further demonstrate the
transformation of scam scripts into lifelike voice calls using modern
text-to-speech systems, completing a fully automated scam pipeline. Our
findings highlight an urgent need for multi-turn safety auditing, agent-level
control frameworks, and new methods to detect and disrupt conversational
deception powered by generative AI.

</details>


### [39] [Voting-Based Semi-Parallel Proof-of-Work Protocol](https://arxiv.org/abs/2508.06489)
*Mustafa Doger,Sennur Ulukus*

Main category: cs.CR

TL;DR: 论文分析了现有并行PoW协议的安全性问题，并提出了一种投票半并行PoW协议，优于现有协议和Nakamoto共识。


<details>
  <summary>Details</summary>
Motivation: 改进Nakamoto共识的安全性和性能，但发现现有并行PoW协议更易受激励攻击。

Method: 理论分析和模拟验证现有协议的脆弱性，提出投票半并行PoW协议，并用MDP模型评估其抗攻击能力。

Result: 现有并行PoW协议激励攻击门槛更低、收益更高，而新协议在通信开销、吞吐量等方面表现更优。

Conclusion: 投票半并行PoW协议在安全性和性能上优于现有方案。

Abstract: Parallel Proof-of-Work (PoW) protocols are suggested to improve the safety
guarantees, transaction throughput and confirmation latencies of Nakamoto
consensus. In this work, we first consider the existing parallel PoW protocols
and develop hard-coded incentive attack structures. Our theoretical results and
simulations show that the existing parallel PoW protocols are more vulnerable
to incentive attacks than the Nakamoto consensus, e.g., attacks have smaller
profitability threshold and they result in higher relative rewards. Next, we
introduce a voting-based semi-parallel PoW protocol that outperforms both
Nakamoto consensus and the existing parallel PoW protocols from most practical
perspectives such as communication overheads, throughput, transaction
conflicts, incentive compatibility of the protocol as well as a fair
distribution of transaction fees among the voters and the leaders. We use
state-of-the-art analysis to evaluate the consistency of the protocol and
consider Markov decision process (MDP) models to substantiate our claims about
the resilience of our protocol against incentive attacks.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [40] [InfiGUI-G1: Advancing GUI Grounding with Adaptive Exploration Policy Optimization](https://arxiv.org/abs/2508.05731)
*Yuhang Liu,Zeyu Liu,Shuanghe Zhu,Pengxiang Li,Congkai Xie,Jiasheng Wang,Xueyu Hu,Xiaotian Han,Jianbo Yuan,Xinyao Wang,Shengyu Zhang,Hongxia Yang,Fei Wu*

Main category: cs.AI

TL;DR: 论文提出AEPO框架，通过多答案生成策略和自适应探索奖励函数，解决了MLLMs在GUI任务中语义对齐的探索瓶颈问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 解决多模态大语言模型在GUI任务中语义对齐的探索效率问题，以提升指令与UI元素的功能匹配能力。

Method: 提出自适应探索策略优化（AEPO）框架，结合多答案生成策略和理论驱动的自适应探索奖励函数（AER）。

Result: AEPO训练的模型InfiGUI-G1-3B和InfiGUI-G1-7B在多个GUI基准测试中达到新SOTA，相对基线提升9.0%。

Conclusion: AEPO有效解决了语义对齐的探索问题，显著提升了MLLMs在GUI任务中的性能。

Abstract: The emergence of Multimodal Large Language Models (MLLMs) has propelled the
development of autonomous agents that operate on Graphical User Interfaces
(GUIs) using pure visual input. A fundamental challenge is robustly grounding
natural language instructions. This requires a precise spatial alignment, which
accurately locates the coordinates of each element, and, more critically, a
correct semantic alignment, which matches the instructions to the functionally
appropriate UI element. Although Reinforcement Learning with Verifiable Rewards
(RLVR) has proven to be effective at improving spatial alignment for these
MLLMs, we find that inefficient exploration bottlenecks semantic alignment,
which prevent models from learning difficult semantic associations. To address
this exploration problem, we present Adaptive Exploration Policy Optimization
(AEPO), a new policy optimization framework. AEPO employs a multi-answer
generation strategy to enforce broader exploration, which is then guided by a
theoretically grounded Adaptive Exploration Reward (AER) function derived from
first principles of efficiency eta=U/C. Our AEPO-trained models, InfiGUI-G1-3B
and InfiGUI-G1-7B, establish new state-of-the-art results across multiple
challenging GUI grounding benchmarks, achieving significant relative
improvements of up to 9.0% against the naive RLVR baseline on benchmarks
designed to test generalization and semantic understanding. Resources are
available at https://github.com/InfiXAI/InfiGUI-G1.

</details>


### [41] [A Framework for Inherently Safer AGI through Language-Mediated Active Inference](https://arxiv.org/abs/2508.05766)
*Bo Wen*

Main category: cs.AI

TL;DR: 提出了一种结合主动推理与大型语言模型的新型框架，旨在开发安全的通用人工智能（AGI），强调将安全性融入系统核心设计。


<details>
  <summary>Details</summary>
Motivation: 传统AI安全方法（如事后可解释性和奖励工程）存在根本性局限，需要一种更本质的安全设计。

Method: 通过透明信念表示和分层价值对齐，利用自然语言作为信念表达媒介，构建多智能体系统，实现资源感知的自由能最小化和模块化安全机制。

Result: 提出了一种具有分层马尔可夫毯和明确信念偏好分离的架构，确保安全性和计算可行性。

Conclusion: 该框架为AGI开发提供了一条本质安全的路径，未来将通过ARC基准验证其安全性。

Abstract: This paper proposes a novel framework for developing safe Artificial General
Intelligence (AGI) by combining Active Inference principles with Large Language
Models (LLMs). We argue that traditional approaches to AI safety, focused on
post-hoc interpretability and reward engineering, have fundamental limitations.
We present an architecture where safety guarantees are integrated into the
system's core design through transparent belief representations and
hierarchical value alignment. Our framework leverages natural language as a
medium for representing and manipulating beliefs, enabling direct human
oversight while maintaining computational tractability. The architecture
implements a multi-agent system where agents self-organize according to Active
Inference principles, with preferences and safety constraints flowing through
hierarchical Markov blankets. We outline specific mechanisms for ensuring
safety, including: (1) explicit separation of beliefs and preferences in
natural language, (2) bounded rationality through resource-aware free energy
minimization, and (3) compositional safety through modular agent structures.
The paper concludes with a research agenda centered on the Abstraction and
Reasoning Corpus (ARC) benchmark, proposing experiments to validate our
framework's safety properties. Our approach offers a path toward AGI
development that is inherently safer, rather than retrofitted with safety
measures.

</details>


### [42] [Whither symbols in the era of advanced neural networks?](https://arxiv.org/abs/2508.05776)
*Thomas L. Griffiths,Brenden M. Lake,R. Thomas McCoy,Ellie Pavlick,Taylor W. Webb*

Main category: cs.AI

TL;DR: 现代神经网络展示了类似人类思维的组合、创新和快速学习能力，挑战了人类认知过程是符号化的观点，但符号系统在抽象问题中的作用仍需研究。


<details>
  <summary>Details</summary>
Motivation: 探讨神经网络是否具备类似人类思维的符号化能力，挑战传统认知理论。

Method: 通过分析神经网络的行为和能力，对比人类思维的符号化特征。

Result: 神经网络展示了类似符号系统的能力，但符号系统在抽象问题中的作用不可忽视。

Conclusion: 提出研究人类思维符号基础的新议程，强调符号系统的重要性。

Abstract: Some of the strongest evidence that human minds should be thought about in
terms of symbolic systems has been the way they combine ideas, produce novelty,
and learn quickly. We argue that modern neural networks -- and the artificial
intelligence systems built upon them -- exhibit similar abilities. This
undermines the argument that the cognitive processes and representations used
by human minds are symbolic, although the fact that these neural networks are
typically trained on data generated by symbolic systems illustrates that such
systems play an important role in characterizing the abstract problems that
human minds have to solve. This argument leads us to offer a new agenda for
research on the symbolic basis of human thought.

</details>


### [43] [Holistic Explainable AI (H-XAI): Extending Transparency Beyond Developers in AI-Driven Decision Making](https://arxiv.org/abs/2508.05792)
*Kausik Lakkaraju,Siva Likitha Valluru,Biplav Srivastava*

Main category: cs.AI

TL;DR: H-XAI是一个统一的框架，结合因果评级与传统XAI方法，支持交互式、多方法的解释过程，满足不同利益相关者的需求。


<details>
  <summary>Details</summary>
Motivation: 当前XAI方法主要服务于开发者，缺乏对多样化利益相关者需求的支持。H-XAI旨在填补这一空白，提供更全面的解释工具。

Method: H-XAI整合因果评级与传统XAI方法，支持交互式问题提问、假设测试，并与随机和偏置基线进行比较。

Result: 通过两个案例研究（信用风险分类和金融时间序列预测），验证了H-XAI的通用性和有效性。

Conclusion: H-XAI填补了现有XAI方法的不足，通过结合因果评级和后验解释，满足利益相关者在个体决策和整体模型层面的需求。

Abstract: Current eXplainable AI (XAI) methods largely serve developers, often focusing
on justifying model outputs rather than supporting diverse stakeholder needs. A
recent shift toward Evaluative AI reframes explanation as a tool for hypothesis
testing, but still focuses primarily on operational organizations. We introduce
Holistic-XAI (H-XAI), a unified framework that integrates causal rating methods
with traditional XAI methods to support explanation as an interactive,
multi-method process. H-XAI allows stakeholders to ask a series of questions,
test hypotheses, and compare model behavior against automatically constructed
random and biased baselines. It combines instance-level and global
explanations, adapting to each stakeholder's goals, whether understanding
individual decisions, assessing group-level bias, or evaluating robustness
under perturbations. We demonstrate the generality of our approach through two
case studies spanning six scenarios: binary credit risk classification and
financial time-series forecasting. H-XAI fills critical gaps left by existing
XAI methods by combining causal ratings and post-hoc explanations to answer
stakeholder-specific questions at both the individual decision level and the
overall model level.

</details>


### [44] [Safety of Embodied Navigation: A Survey](https://arxiv.org/abs/2508.05855)
*Zixia Wang,Jia Hu,Ronghui Mu*

Main category: cs.AI

TL;DR: 该论文综述了具身导航中的安全问题，包括攻击策略、防御机制和评估方法，并探讨了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型和具身AI的发展，具身导航的安全问题日益重要，尤其是在动态现实环境中的应用。

Method: 通过综合分析现有安全挑战、缓解技术、数据集和评估指标，探讨未解决问题和未来方向。

Result: 提出了潜在攻击方法、缓解策略、更可靠的评估技术和验证框架的实现。

Conclusion: 该研究为开发更安全可靠的具身导航系统提供了指导，并对提升社会安全和工业效率有广泛意义。

Abstract: As large language models (LLMs) continue to advance and gain influence, the
development of embodied AI has accelerated, drawing significant attention,
particularly in navigation scenarios. Embodied navigation requires an agent to
perceive, interact with, and adapt to its environment while moving toward a
specified target in unfamiliar settings. However, the integration of embodied
navigation into critical applications raises substantial safety concerns. Given
their deployment in dynamic, real-world environments, ensuring the safety of
such systems is critical. This survey provides a comprehensive analysis of
safety in embodied navigation from multiple perspectives, encompassing attack
strategies, defense mechanisms, and evaluation methodologies. Beyond conducting
a comprehensive examination of existing safety challenges, mitigation
technologies, and various datasets and metrics that assess effectiveness and
robustness, we explore unresolved issues and future research directions in
embodied navigation safety. These include potential attack methods, mitigation
strategies, more reliable evaluation techniques, and the implementation of
verification frameworks. By addressing these critical gaps, this survey aims to
provide valuable insights that can guide future research toward the development
of safer and more reliable embodied navigation systems. Furthermore, the
findings of this study have broader implications for enhancing societal safety
and increasing industrial efficiency.

</details>


### [45] [Planning Agents on an Ego-Trip: Leveraging Hybrid Ego-Graph Ensembles for Improved Tool Retrieval in Enterprise Task Planning](https://arxiv.org/abs/2508.05888)
*Sahil Bansal,Sai Shruthi Sistla,Aarti Arikatala,Sebastian Schreiber*

Main category: cs.AI

TL;DR: 提出了一种基于知识图谱（KG）的工具检索框架，通过捕捉工具间的语义关系和功能依赖，显著提升了多步任务中的工具检索准确性。


<details>
  <summary>Details</summary>
Motivation: 传统方法主要依赖用户查询与工具描述的相似性，限制了多步请求的检索准确性。

Method: 利用知识图谱中的1-hop ego工具图集合，建模工具间的直接和间接连接，实现更全面的工具选择。

Result: 在合成数据集上，KG方法在Complete Recall指标上达到91.85%，优于非KG基线（89.26%）。

Conclusion: KG的结构信息为相似性匹配提供了补充信号，特别适用于需要顺序工具组合的查询。

Abstract: Effective tool retrieval is essential for AI agents to select from a vast
array of tools when identifying and planning actions in the context of complex
user queries. Despite its central role in planning, this aspect remains
underexplored in the literature. Traditional approaches rely primarily on
similarities between user queries and tool descriptions, which significantly
limits retrieval accuracy, specifically when handling multi-step user requests.
To address these limitations, we propose a Knowledge Graph (KG)-based tool
retrieval framework that captures the semantic relationships between tools and
their functional dependencies. Our retrieval algorithm leverages ensembles of
1-hop ego tool graphs to model direct and indirect connections between tools,
enabling more comprehensive and contextual tool selection for multi-step tasks.
We evaluate our approach on a synthetically generated internal dataset across
six defined user classes, extending previous work on coherent dialogue
synthesis and too retrieval benchmarks. Results demonstrate that our tool
graph-based method achieves 91.85% tool coverage on the micro-average Complete
Recall metric, compared to 89.26% for re-ranked semantic-lexical hybrid
retrieval, the strongest non-KG baseline in our experiments. These findings
support our hypothesis that the structural information in the KG provides
complementary signals to pure similarity matching, particularly for queries
requiring sequential tool composition.

</details>


### [46] [Mediator-Guided Multi-Agent Collaboration among Open-Source Models for Medical Decision-Making](https://arxiv.org/abs/2508.05996)
*Kaitao Chen,Mianxin Liu,Daoming Zong,Chaoyue Ding,Shaohao Rui,Yankai Jiang,Mu Zhou,Xiaosong Wang*

Main category: cs.AI

TL;DR: 提出MedOrch框架，通过LLM中介协调多VLM专家代理协作，提升医疗多模态决策能力。


<details>
  <summary>Details</summary>
Motivation: 现有多代理研究多限于语言任务，多模态场景下VLM协作能力不足，需解决错误结果放大的问题。

Method: 采用LLM中介代理协调多个VLM专家代理，实现输出交换与反思，利用开源VLM而非昂贵GPT模型。

Result: 在五个医疗视觉问答基准上验证，协作性能超越单个代理，无需模型训练。

Conclusion: 中介引导的多代理协作可推动医疗多模态智能发展，代码将开源。

Abstract: Complex medical decision-making involves cooperative workflows operated by
different clinicians. Designing AI multi-agent systems can expedite and augment
human-level clinical decision-making. Existing multi-agent researches primarily
focus on language-only tasks, yet their extension to multimodal scenarios
remains challenging. A blind combination of diverse vision-language models
(VLMs) can amplify an erroneous outcome interpretation. VLMs in general are
less capable in instruction following and importantly self-reflection, compared
to large language models (LLMs) of comparable sizes. This disparity largely
constrains VLMs' ability in cooperative workflows. In this study, we propose
MedOrch, a mediator-guided multi-agent collaboration framework for medical
multimodal decision-making. MedOrch employs an LLM-based mediator agent that
enables multiple VLM-based expert agents to exchange and reflect on their
outputs towards collaboration. We utilize multiple open-source general-purpose
and domain-specific VLMs instead of costly GPT-series models, revealing the
strength of heterogeneous models. We show that the collaboration within
distinct VLM-based agents can surpass the capabilities of any individual agent.
We validate our approach on five medical vision question answering benchmarks,
demonstrating superior collaboration performance without model training. Our
findings underscore the value of mediator-guided multi-agent collaboration in
advancing medical multimodal intelligence. Our code will be made publicly
available.

</details>


### [47] [Society of Mind Meets Real-Time Strategy: A Hierarchical Multi-Agent Framework for Strategic Reasoning](https://arxiv.org/abs/2508.06042)
*Daechul Ahn,San Kim,Jonghyun Choi*

Main category: cs.AI

TL;DR: 论文提出了一种名为HIMA的分层多智能体框架，结合专家模仿学习和元控制器（Strategic Planner），用于解决大型语言模型在动态、长时程任务（如《星际争霸II》）中的不足。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型在动态、长时程任务（如实时战略游戏）中表现不佳，难以处理资源约束和部分可观测环境下的战场变化。

Method: 提出HIMA框架，通过专家模仿学习训练多个专用智能体，每个智能体学习特定策略（如空中支援或防御机动），并由元控制器协调生成适应性强的多步行动序列。

Result: HIMA在战略清晰度、适应性和计算效率上优于现有方法，验证了结合专用模仿模块与元级协调的潜力。

Conclusion: HIMA框架展示了通过分层多智能体模仿学习提升AI智能体在复杂任务中表现的可行性。

Abstract: Large Language Models (LLMs) have recently demonstrated impressive action
sequence prediction capabilities but often struggle with dynamic, long-horizon
tasks such as real-time strategic games. In a game such as StarCraftII (SC2),
agents need to manage resource constraints and adapt to evolving battlefield
situations in a partially observable environment. This often overwhelms
exisiting LLM-based approaches. To address these challenges, we propose a
hierarchical multi-agent framework that employs specialized imitation learning
agents under a meta-controller called Strategic Planner (SP). By expert
demonstrations, each specialized agent learns a distinctive strategy, such as
aerial support or defensive maneuvers, and produces coherent, structured
multistep action sequences. The SP then orchestrates these proposals into a
single, environmentally adaptive plan that ensures local decisions aligning
with long-term strategies. We call this HIMA (Hierarchical Imitation
Multi-Agent). We also present TEXTSCII-ALL, a comprehensive SC2 testbed that
encompasses all race match combinations in SC2. Our empirical results show that
HIMA outperforms state of the arts in strategic clarity, adaptability, and
computational efficiency, underscoring the potential of combining specialized
imitation modules with meta-level orchestration to develop more robust,
general-purpose AI agents.

</details>


### [48] [LLMs for Resource Allocation: A Participatory Budgeting Approach to Inferring Preferences](https://arxiv.org/abs/2508.06060)
*Sankarshan Damle,Boi Faltings*

Main category: cs.AI

TL;DR: 论文提出了一个双用途框架，利用参与式预算（PB）作为LLM资源分配的实践场景和评估其推理能力的动态基准。


<details>
  <summary>Details</summary>
Motivation: 探索LLM在结构化资源分配任务中的能力，并解决现有评估方法因数据污染和静态基准而受限的问题。

Method: 采用三种提示策略（贪心选择、直接优化和爬山启发式优化）让LLM在预算约束下选择项目子集，并对比效用最大化基准。

Result: 结果表明提示设计对LLM表现至关重要，LLM在非结构化输入机制设计中具有潜力。

Conclusion: LLM在资源分配和偏好推断任务中展现出潜力，提示设计是关键因素。

Abstract: Large Language Models (LLMs) are increasingly expected to handle complex
decision-making tasks, yet their ability to perform structured resource
allocation remains underexplored. Evaluating their reasoning is also difficult
due to data contamination and the static nature of existing benchmarks. We
present a dual-purpose framework leveraging Participatory Budgeting (PB) both
as (i) a practical setting for LLM-based resource allocation and (ii) an
adaptive benchmark for evaluating their reasoning capabilities. We task LLMs
with selecting project subsets under feasibility (e.g., budget) constraints via
three prompting strategies: greedy selection, direct optimization, and a
hill-climbing-inspired refinement. We benchmark LLMs' allocations against a
utility-maximizing oracle. Interestingly, we also test whether LLMs can infer
structured preferences from natural-language voter input or metadata, without
explicit votes. By comparing allocations based on inferred preferences to those
from ground-truth votes, we evaluate LLMs' ability to extract preferences from
open-ended input. Our results underscore the role of prompt design and show
that LLMs hold promise for mechanism design with unstructured inputs.

</details>


### [49] [Don't Forget Imagination!](https://arxiv.org/abs/2508.06062)
*Evgenii E. Vityaev,Andrei Mantsivoda*

Main category: cs.AI

TL;DR: 论文呼吁重视认知想象力在人工智能中的关键作用，并提出语义模型作为模拟认知想象力的新方法。


<details>
  <summary>Details</summary>
Motivation: 认知想象力在人类思维中扮演重要角色，但目前被低估，限制了AI的能力。论文旨在推动认知想象力成为AI的下一个突破点。

Method: 提出语义模型，这是一种基于概率因果关系的数学模型，能够学习和确保想象上下文的连贯性。

Result: 语义模型能够模拟认知想象力，支持一致且透明的推理和决策。

Conclusion: 认知想象力是AI发展的关键方向，语义模型为实现这一目标提供了可行工具。

Abstract: Cognitive imagination is a type of imagination that plays a key role in human
thinking. It is not a ``picture-in-the-head'' imagination. It is a faculty to
mentally visualize coherent and holistic systems of concepts and causal links
that serve as semantic contexts for reasoning, decision making and prediction.
Our position is that the role of cognitive imagination is still greatly
underestimated, and this creates numerous problems and diminishes the current
capabilities of AI. For instance, when reasoning, humans rely on imaginary
contexts to retrieve background info. They also constantly return to the
context for semantic verification that their reasoning is still reasonable.
Thus, reasoning without imagination is blind. This paper is a call for greater
attention to cognitive imagination as the next promising breakthrough in
artificial intelligence. As an instrument for simulating cognitive imagination,
we propose semantic models -- a new approach to mathematical models that can
learn, like neural networks, and are based on probabilistic causal
relationships. Semantic models can simulate cognitive imagination because they
ensure the consistency of imaginary contexts and implement a glass-box approach
that allows the context to be manipulated as a holistic and coherent system of
interrelated facts glued together with causal relations.

</details>


### [50] [A Generic Complete Anytime Beam Search for Optimal Decision Tree](https://arxiv.org/abs/2508.06064)
*Harold Silvère Kiossou,Siegfried Nijssen,Pierre Schaus*

Main category: cs.AI

TL;DR: 本文提出了一种名为CA-DL8.5的通用、完整且随时可用的束搜索算法，用于优化决策树分类误差。该算法扩展了DL8.5框架，统一了现有的随时策略，并通过模块化设计整合了多种启发式和松弛机制。实验表明，基于LDS的CA-DL8.5在随时性能上表现最佳。


<details>
  <summary>Details</summary>
Motivation: 现有方法在优化决策树分类误差时存在搜索空间不平衡的问题，导致随时性能不佳。本文旨在提出一种通用框架，统一并改进现有随时策略。

Method: CA-DL8.5结合了DL8.5的高效剪枝和缓存技术，采用基于重启的束搜索，逐步放宽剪枝标准以提高解的质量。

Result: 实验结果显示，基于LDS的CA-DL8.5在标准分类基准测试中表现最佳，优于其他变体和Blossom算法。

Conclusion: CA-DL8.5为决策树学习提供了一个通用框架，显著提升了随时性能，同时保持了完整性和最优性保证。

Abstract: Finding an optimal decision tree that minimizes classification error is known
to be NP-hard. While exact algorithms based on MILP, CP, SAT, or dynamic
programming guarantee optimality, they often suffer from poor anytime behavior
-- meaning they struggle to find high-quality decision trees quickly when the
search is stopped before completion -- due to unbalanced search space
exploration. To address this, several anytime extensions of exact methods have
been proposed, such as LDS-DL8.5, Top-k-DL8.5, and Blossom, but they have not
been systematically compared, making it difficult to assess their relative
effectiveness. In this paper, we propose CA-DL8.5, a generic, complete, and
anytime beam search algorithm that extends the DL8.5 framework and unifies some
existing anytime strategies. In particular, CA-DL8.5 generalizes previous
approaches LDS-DL8.5 and Top-k-DL8.5, by allowing the integration of various
heuristics and relaxation mechanisms through a modular design. The algorithm
reuses DL8.5's efficient branch-and-bound pruning and trie-based caching,
combined with a restart-based beam search that gradually relaxes pruning
criteria to improve solution quality over time. Our contributions are twofold:
(1) We introduce this new generic framework for exact and anytime decision tree
learning, enabling the incorporation of diverse heuristics and search
strategies; (2) We conduct a rigorous empirical comparison of several
instantiations of CA-DL8.5 -- based on Purity, Gain, Discrepancy, and Top-k
heuristics -- using an anytime evaluation metric called the primal gap
integral. Experimental results on standard classification benchmarks show that
CA-DL8.5 using LDS (limited discrepancy) consistently provides the best anytime
performance, outperforming both other CA-DL8.5 variants and the Blossom
algorithm while maintaining completeness and optimality guarantees.

</details>


### [51] [ME$^3$-BEV: Mamba-Enhanced Deep Reinforcement Learning for End-to-End Autonomous Driving with BEV-Perception](https://arxiv.org/abs/2508.06074)
*Siyi Lu,Run Liu,Dongsheng Yang,Lei He*

Main category: cs.AI

TL;DR: 论文提出了一种基于深度强化学习（DRL）和鸟瞰图（BEV）感知的自动驾驶新方法，结合了高效的时空特征提取网络Mamba-BEV和ME³-BEV框架，显著提升了动态城市驾驶场景中的性能。


<details>
  <summary>Details</summary>
Motivation: 传统模块化方法存在错误传播和协调问题，端到端学习系统则面临计算瓶颈。本文旨在通过结合BEV感知和DRL，解决这些问题。

Method: 提出Mamba-BEV模型进行时空特征提取，并结合ME³-BEV框架实现端到端DRL，同时通过语义分割增强模型可解释性。

Result: 在CARLA模拟器上的实验表明，ME³-BEV在碰撞率和轨迹精度等多项指标上优于现有模型。

Conclusion: ME³-BEV为实时自动驾驶提供了一种高效且可解释的解决方案。

Abstract: Autonomous driving systems face significant challenges in perceiving complex
environments and making real-time decisions. Traditional modular approaches,
while offering interpretability, suffer from error propagation and coordination
issues, whereas end-to-end learning systems can simplify the design but face
computational bottlenecks. This paper presents a novel approach to autonomous
driving using deep reinforcement learning (DRL) that integrates bird's-eye view
(BEV) perception for enhanced real-time decision-making. We introduce the
\texttt{Mamba-BEV} model, an efficient spatio-temporal feature extraction
network that combines BEV-based perception with the Mamba framework for
temporal feature modeling. This integration allows the system to encode vehicle
surroundings and road features in a unified coordinate system and accurately
model long-range dependencies. Building on this, we propose the
\texttt{ME$^3$-BEV} framework, which utilizes the \texttt{Mamba-BEV} model as a
feature input for end-to-end DRL, achieving superior performance in dynamic
urban driving scenarios. We further enhance the interpretability of the model
by visualizing high-dimensional features through semantic segmentation,
providing insight into the learned representations. Extensive experiments on
the CARLA simulator demonstrate that \texttt{ME$^3$-BEV} outperforms existing
models across multiple metrics, including collision rate and trajectory
accuracy, offering a promising solution for real-time autonomous driving.

</details>


### [52] [Aggregate-Combine-Readout GNNs Are More Expressive Than Logic C2](https://arxiv.org/abs/2508.06091)
*Stan P Hauke,Przemysław Andrzej Wałęga*

Main category: cs.AI

TL;DR: 本文解决了关于图神经网络（GNNs）逻辑表达能力的一个开放性问题，证明其表达能力严格超过C2逻辑。


<details>
  <summary>Details</summary>
Motivation: 研究GNNs的逻辑表达能力，特别是解决Barceló等人提出的开放性问题。

Method: 通过理论分析，比较GNNs与C2逻辑的表达能力。

Result: 证明GNNs的逻辑表达能力严格超过C2逻辑，适用于无向和有向图。

Conclusion: 研究不仅解决了GNNs的开放性问题，还为无穷逻辑的表达能力提供了新见解。

Abstract: In recent years, there has been growing interest in understanding the
expressive power of graph neural networks (GNNs) by relating them to logical
languages. This research has been been initialised by an influential result of
Barcel\'o et al. (2020), who showed that the graded modal logic (or a guarded
fragment of the logic C2), characterises the logical expressiveness of
aggregate-combine GNNs. As a ``challenging open problem'' they left the
question whether full C2 characterises the logical expressiveness of
aggregate-combine-readout GNNs. This question has remained unresolved despite
several attempts. In this paper, we solve the above open problem by proving
that the logical expressiveness of aggregate-combine-readout GNNs strictly
exceeds that of C2. This result holds over both undirected and directed graphs.
Beyond its implications for GNNs, our work also leads to purely logical
insights on the expressive power of infinitary logics.

</details>


### [53] [PanelTR: Zero-Shot Table Reasoning Framework Through Multi-Agent Scientific Discussion](https://arxiv.org/abs/2508.06110)
*Yiran Rex Ma*

Main category: cs.AI

TL;DR: PanelTR框架通过LLM代理科学家实现表格推理，无需标注数据或复杂增强，性能优于普通LLM，媲美监督模型。


<details>
  <summary>Details</summary>
Motivation: 解决表格推理依赖标注数据或复杂增强的问题，提升LLM在复杂任务中的表现。

Method: 利用五类科学家代理进行独立调查、自我审查和同行评审，实现语义级迁移。

Result: 在四个基准测试中超越普通LLM，媲美监督模型，且无需训练数据。

Conclusion: 结构化科学方法可有效处理复杂任务，支持零样本场景下的灵活语义理解。

Abstract: Table reasoning, including tabular QA and fact verification, often depends on
annotated data or complex data augmentation, limiting flexibility and
generalization. LLMs, despite their versatility, often underperform compared to
simple supervised models. To approach these issues, we introduce PanelTR, a
framework utilizing LLM agent scientists for robust table reasoning through a
structured scientific approach. PanelTR's workflow involves agent scientists
conducting individual investigations, engaging in self-review, and
participating in collaborative peer-review discussions. This process, driven by
five scientist personas, enables semantic-level transfer without relying on
data augmentation or parametric optimization. Experiments across four
benchmarks show that PanelTR outperforms vanilla LLMs and rivals fully
supervised models, all while remaining independent of training data. Our
findings indicate that structured scientific methodology can effectively handle
complex tasks beyond table reasoning with flexible semantic understanding in a
zero-shot context.

</details>


### [54] [SKATE, a Scalable Tournament Eval: Weaker LLMs differentiate between stronger ones using verifiable challenges](https://arxiv.org/abs/2508.06111)
*Dewi S. W. Gould,Bruno Mlodozeniec,Samuel F. Brown*

Main category: cs.AI

TL;DR: SKATE是一种新型评估框架，通过让大语言模型（LLMs）相互生成和解决可验证任务来评估其能力和风险，具有自动化、可扩展和客观性等优势。


<details>
  <summary>Details</summary>
Motivation: 当前评估方法需要大量领域专业知识，难以适应LLMs的快速发展，因此需要一种更通用、可扩展的评估框架。

Method: SKATE将评估视为游戏，LLMs既作为任务生成者又作为解决者，通过生成可验证任务（如代码输出预测挑战）并利用TrueSkill排名系统进行评分。

Result: 实验表明：（1）较弱模型能可靠区分较强模型；（2）LLMs会生成与自身能力相符的问题；（3）SKATE能自动揭示模型间的细粒度能力差异。

Conclusion: SKATE为通用、可扩展的评估框架提供了重要进展，能够跟上LLMs的发展步伐。

Abstract: Evaluating the capabilities and risks of foundation models is paramount, yet
current methods demand extensive domain expertise, hindering their scalability
as these models rapidly evolve. We introduce SKATE: a novel evaluation
framework in which large language models (LLMs) compete by generating and
solving verifiable tasks for one another. Our core insight is to treat
evaluation as a game: models act as both task-setters and solvers, incentivized
to create questions which highlight their own strengths while exposing others'
weaknesses. SKATE offers several key advantages, balancing scalability,
open-endedness, and objectivity. It is fully automated, data-free, and
scalable, requiring no human input or domain expertise. By using verifiable
tasks rather than LLM judges, scoring is objective. Unlike domain-limited
programmatically-generated benchmarks (e.g. chess-playing or spatial
reasoning), having LLMs creatively pose challenges enables open-ended and
scalable evaluation. As a proof of concept, we introduce LLM-set
code-output-prediction (COP) challenges as a verifiable and extensible
framework in which to test our approach. Using a TrueSkill-based ranking
system, we evaluate six frontier LLMs and find that: (1) weaker models can
reliably differentiate and score stronger ones, (2) LLM-based systems are
capable of self-preferencing behavior, generating questions that align with
their own capabilities, and (3) SKATE automatically surfaces fine-grained
capability differences between models. Our findings are an important step
towards general, scalable evaluation frameworks which can keep pace with LLM
progress.

</details>


### [55] [Study of Robust Features in Formulating Guidance for Heuristic Algorithms for Solving the Vehicle Routing Problem](https://arxiv.org/abs/2508.06129)
*Bachtiar Herdianto,Romain Billot,Flavien Lucas,Marc Sevaux*

Main category: cs.AI

TL;DR: 该研究通过可解释AI分析VRP解决方案的质量预测模型，发现某些特征始终是强预测因子，并提出统一框架排名特征影响，为元启发式算法提供指导。


<details>
  <summary>Details</summary>
Motivation: 传统元启发式算法依赖人工设计，而机器学习能利用组合优化的结构特征设计更高效算法，本研究进一步探索模型决策机制。

Method: 使用多个分类器模型进行敏感性分析，预测VRP解决方案质量，并利用可解释AI理解模型决策。

Result: 特征重要性虽变化，但某些特征始终为强预测因子；提出统一框架排名特征影响。

Conclusion: 特征重要性分析可作为元启发式算法解决VRP的指导机制基础。

Abstract: The Vehicle Routing Problem (VRP) is a complex optimization problem with
numerous real-world applications, mostly solved using metaheuristic algorithms
due to its $\mathcal{NP}$-Hard nature. Traditionally, these metaheuristics rely
on human-crafted designs developed through empirical studies. However, recent
research shows that machine learning methods can be used the structural
characteristics of solutions in combinatorial optimization, thereby aiding in
designing more efficient algorithms, particularly for solving VRP. Building on
this advancement, this study extends the previous research by conducting a
sensitivity analysis using multiple classifier models that are capable of
predicting the quality of VRP solutions. Hence, by leveraging explainable AI,
this research is able to extend the understanding of how these models make
decisions. Finally, our findings indicate that while feature importance varies,
certain features consistently emerge as strong predictors. Furthermore, we
propose a unified framework able of ranking feature impact across different
scenarios to illustrate this finding. These insights highlight the potential of
feature importance analysis as a foundation for developing a guidance mechanism
of metaheuristic algorithms for solving the VRP.

</details>


### [56] [Retrieval Augmented Large Language Model System for Comprehensive Drug Contraindications](https://arxiv.org/abs/2508.06145)
*Byeonghun Bang,Jongsuk Yoon,Dong-Jin Chang,Seho Park,Yong Oh Lee*

Main category: cs.AI

TL;DR: 研究通过RAG框架增强LLMs在药物禁忌领域的准确性，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在医疗领域的应用，特别是在药物禁忌方面，以提高信息的准确性和可靠性。

Method: 采用RAG框架，结合GPT-4o-mini和text-embedding-3-small模型，利用Langchain构建混合检索系统，并整合DUR数据。

Result: RAG框架显著提升了模型在年龄组、妊娠和联合用药禁忌方面的准确率，分别达到0.94、0.87和0.89。

Conclusion: RAG框架能有效减少药物禁忌决策的不确定性，提供更精确可靠的信息。

Abstract: The versatility of large language models (LLMs) has been explored across
various sectors, but their application in healthcare poses challenges,
particularly in the domain of pharmaceutical contraindications where accurate
and reliable information is required. This study enhances the capability of
LLMs to address contraindications effectively by implementing a Retrieval
Augmented Generation (RAG) pipeline. Utilizing OpenAI's GPT-4o-mini as the base
model, and the text-embedding-3-small model for embeddings, our approach
integrates Langchain to orchestrate a hybrid retrieval system with re-ranking.
This system leverages Drug Utilization Review (DUR) data from public databases,
focusing on contraindications for specific age groups, pregnancy, and
concomitant drug use. The dataset includes 300 question-answer pairs across
three categories, with baseline model accuracy ranging from 0.49 to 0.57.
Post-integration of the RAG pipeline, we observed a significant improvement in
model accuracy, achieving rates of 0.94, 0.87, and 0.89 for contraindications
related to age groups, pregnancy, and concomitant drug use, respectively. The
results indicate that augmenting LLMs with a RAG framework can substantially
reduce uncertainty in prescription and drug intake decisions by providing more
precise and reliable drug contraindication information.

</details>


### [57] [Overconfidence in LLM-as-a-Judge: Diagnosis and Confidence-Driven Solution](https://arxiv.org/abs/2508.06225)
*Zailong Tian,Zhuoheng Han,Yanzhe Chen,Haozhe Xu,Xi Yang,richeng xuan,Hongfeng Wang,Lizi Liao*

Main category: cs.AI

TL;DR: 论文提出从以准确性为中心的评估转向以置信度驱动的风险感知LLM-as-a-Judge系统，强调校准置信度的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注准确性，忽略了校准置信度的必要性，而这对可靠评估至关重要。

Method: 引入TH-Score量化过度自信现象，并提出LLM-as-a-Fuser框架，将LLM转化为可靠的风险感知评估器。

Result: 实验表明，该方法显著改善了校准性，实现了优于现有基线的可靠性和准确性。

Conclusion: 置信度驱动的评估框架能提升LLM-as-a-Judge系统的可靠性和适应性。

Abstract: Large Language Models (LLMs) are widely used as automated judges, where
practical value depends on both accuracy and trustworthy, risk-aware judgments.
Existing approaches predominantly focus on accuracy, overlooking the necessity
of well-calibrated confidence, which is vital for adaptive and reliable
evaluation pipelines. In this work, we advocate a shift from accuracy-centric
evaluation to confidence-driven, risk-aware LLM-as-a-Judge systems, emphasizing
the necessity of well-calibrated confidence for trustworthy and adaptive
evaluation. We systematically identify the **Overconfidence Phenomenon** in
current LLM-as-a-Judges, where predicted confidence significantly overstates
actual correctness, undermining reliability in practical deployment. To
quantify this phenomenon, we introduce **TH-Score**, a novel metric measuring
confidence-accuracy alignment. Furthermore, we propose **LLM-as-a-Fuser**, an
ensemble framework that transforms LLMs into reliable, risk-aware evaluators.
Extensive experiments demonstrate that our approach substantially improves
calibration and enables adaptive, confidence-driven evaluation pipelines,
achieving superior reliability and accuracy compared to existing baselines.

</details>


### [58] [GeoLaux: A Benchmark for Evaluating MLLMs' Geometry Performance on Long-Step Problems Requiring Auxiliary Lines](https://arxiv.org/abs/2508.06226)
*Yumeng Fu,Jiayin Zhu,Lingling Zhang,Bo Zhao,Shaoxuan Ma,Yushun Zhang,Yanrui Wu,Wenjun Wu*

Main category: cs.AI

TL;DR: 论文提出了GeoLaux基准，用于评估多模态大语言模型（MLLMs）在几何问题解决中的长步推理和辅助线构造能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准未充分评估MLLMs的辅助线构造能力和推理过程细节，导致对长步推理能力的评估不足。

Method: 构建包含2,186个几何问题的GeoLaux数据集，设计五维评估策略（答案正确性、过程正确性、过程质量、辅助线影响和错误原因）。

Result: 实验发现MLLMs在长步推理中性能显著下降，倾向于在证明题中走捷径，且缺乏辅助线意识。

Conclusion: GeoLaux可作为评估MLLMs几何推理能力的基准，并指导其能力提升。

Abstract: Geometry problem solving (GPS) requires models to master diagram
comprehension, logical reasoning, knowledge application, numerical computation,
and auxiliary line construction. This presents a significant challenge for
Multimodal Large Language Models (MLLMs). However, existing benchmarks for
evaluating MLLM geometry skills overlook auxiliary line construction and lack
fine-grained process evaluation, making them insufficient for assessing MLLMs'
long-step reasoning abilities. To bridge these gaps, we present the GeoLaux
benchmark, comprising 2,186 geometry problems, incorporating both calculation
and proving questions. Notably, the problems require an average of 6.51
reasoning steps, with a maximum of 24 steps, and 41.8% of them need auxiliary
line construction. Building on the dataset, we design a novel five-dimensional
evaluation strategy assessing answer correctness, process correctness, process
quality, auxiliary line impact, and error causes. Extensive experiments on 13
leading MLLMs (including thinking models and non-thinking models) yield three
pivotal findings: First, models exhibit substantial performance degradation in
extended reasoning steps (nine models demonstrate over 50% performance drop).
Second, compared to calculation problems, MLLMs tend to take shortcuts when
solving proving problems. Third, models lack auxiliary line awareness, and
enhancing this capability proves particularly beneficial for overall geometry
reasoning improvement. These findings establish GeoLaux as both a benchmark for
evaluating MLLMs' long-step geometric reasoning with auxiliary lines and a
guide for capability advancement. Our dataset and code are included in
supplementary materials and will be released.

</details>


### [59] [Learning Logical Rules using Minimum Message Length](https://arxiv.org/abs/2508.06230)
*Ruben Sharma,Sebastijan Dumančić,Ross D. King,Andrew Cropper*

Main category: cs.AI

TL;DR: 提出了一种贝叶斯归纳逻辑编程方法，通过学习最小消息长度程序来处理噪声数据，平衡假设复杂性和数据拟合。


<details>
  <summary>Details</summary>
Motivation: 统一概率和逻辑学习是AI的关键挑战，需要一种能处理噪声数据且高效的方法。

Method: 使用贝叶斯框架，通过先验偏好更通用的程序，似然偏好准确程序，学习最小消息长度程序。

Result: 在游戏和药物设计等多个领域显著优于先前方法，数据高效且对示例平衡不敏感。

Conclusion: 该方法在噪声数据下表现优异，支持从纯正例中学习，具有广泛应用潜力。

Abstract: Unifying probabilistic and logical learning is a key challenge in AI. We
introduce a Bayesian inductive logic programming approach that learns minimum
message length programs from noisy data. Our approach balances hypothesis
complexity and data fit through priors, which explicitly favour more general
programs, and a likelihood that favours accurate programs. Our experiments on
several domains, including game playing and drug design, show that our method
significantly outperforms previous methods, notably those that learn minimum
description length programs. Our results also show that our approach is
data-efficient and insensitive to example balance, including the ability to
learn from exclusively positive examples.

</details>


### [60] [Symmetry breaking for inductive logic programming](https://arxiv.org/abs/2508.06263)
*Andrew Cropper,David M. Cerna,Matti Järvisalo*

Main category: cs.AI

TL;DR: 提出一种方法，通过打破假设空间中的对称性来改进归纳逻辑编程的搜索效率，实验显示解决时间从超过一小时缩短至17秒。


<details>
  <summary>Details</summary>
Motivation: 归纳逻辑编程需要搜索庞大的假设空间，且存在许多逻辑等效的假设，这增加了搜索的挑战性。

Method: 采用答案集编程实现对称性打破的方法。

Result: 在视觉推理和游戏等多个领域的实验中，解决时间从超过一小时减少到17秒。

Conclusion: 该方法显著提高了归纳逻辑编程的效率，尤其在复杂假设空间中表现优异。

Abstract: The goal of inductive logic programming is to search for a hypothesis that
generalises training data and background knowledge. The challenge is searching
vast hypothesis spaces, which is exacerbated because many logically equivalent
hypotheses exist. To address this challenge, we introduce a method to break
symmetries in the hypothesis space. We implement our idea in answer set
programming. Our experiments on multiple domains, including visual reasoning
and game playing, show that our approach can reduce solving times from over an
hour to just 17 seconds.

</details>


### [61] [LLM Robustness Leaderboard v1 --Technical report](https://arxiv.org/abs/2508.06296)
*Pierre Peigné - Lefebvre,Quentin Feuillade-Montixi,Tom David,Nicolas Miailhe*

Main category: cs.AI

TL;DR: PRISM Eval开发了BET工具，通过动态对抗优化实现100%攻击成功率，并提出细粒度鲁棒性指标和原始级漏洞分析。


<details>
  <summary>Details</summary>
Motivation: 评估LLM的鲁棒性，揭示不同模型在对抗攻击下的表现差异。

Method: 使用BET工具进行自动化红队测试，结合动态对抗优化和细粒度指标分析。

Result: 在41个LLM中，37个被完全攻破，攻击难度差异达300倍。

Conclusion: 提出了分布式鲁棒性评估的实用路径，并展示了漏洞分析的有效性。

Abstract: This technical report accompanies the LLM robustness leaderboard published by
PRISM Eval for the Paris AI Action Summit. We introduce PRISM Eval Behavior
Elicitation Tool (BET), an AI system performing automated red-teaming through
Dynamic Adversarial Optimization that achieves 100% Attack Success Rate (ASR)
against 37 of 41 state-of-the-art LLMs. Beyond binary success metrics, we
propose a fine-grained robustness metric estimating the average number of
attempts required to elicit harmful behaviors, revealing that attack difficulty
varies by over 300-fold across models despite universal vulnerability. We
introduce primitive-level vulnerability analysis to identify which jailbreaking
techniques are most effective for specific hazard categories. Our collaborative
evaluation with trusted third parties from the AI Safety Network demonstrates
practical pathways for distributed robustness assessment across the community.

</details>


### [62] [A "good regulator theorem" for embodied agents](https://arxiv.org/abs/2508.06326)
*Nathaniel Virgo,Martin Biehl,Manuel Baltieri,Matteo Capucci*

Main category: cs.AI

TL;DR: 论文重新审视了Conant和Ashby的定理，提出了一种更广义的“信念更新”模型概念，强调观察者在模型定义中的关键作用。


<details>
  <summary>Details</summary>
Motivation: 探讨人工生命系统中无模型任务执行的现象，挑战Conant和Ashby定理的普适性。

Method: 提出“信念更新”作为新的模型定义，并通过观察者视角分析系统的调节能力。

Result: 证明了无论系统是调节环境还是内部状态，均可通过观察者视角赋予其模型属性。

Conclusion: 模型的定义需要观察者参与，解决了无模型系统的矛盾，扩展了定理的适用范围。

Abstract: In a classic paper, Conant and Ashby claimed that "every good regulator of a
system must be a model of that system." Artificial Life has produced many
examples of systems that perform tasks with apparently no model in sight; these
suggest Conant and Ashby's theorem doesn't easily generalise beyond its
restricted setup. Nevertheless, here we show that a similar intuition can be
fleshed out in a different way: whenever an agent is able to perform a
regulation task, it is possible for an observer to interpret it as having
"beliefs" about its environment, which it "updates" in response to sensory
input. This notion of belief updating provides a notion of model that is more
sophisticated than Conant and Ashby's, as well as a theorem that is more
broadly applicable. However, it necessitates a change in perspective, in that
the observer plays an essential role in the theory: models are not a mere
property of the system but are imposed on it from outside. Our theorem holds
regardless of whether the system is regulating its environment in a classic
control theory setup, or whether it's regulating its own internal state; the
model is of its environment either way. The model might be trivial, however,
and this is how the apparent counterexamples are resolved.

</details>


### [63] [AntiCheatPT: A Transformer-Based Approach to Cheat Detection in Competitive Computer Games](https://arxiv.org/abs/2508.06348)
*Mille Mei Zhen Loo,Gert Luzkov,Paolo Burelli*

Main category: cs.AI

TL;DR: 本文提出了一种基于Transformer的机器学习模型AntiCheatPT_256，用于检测《反恐精英2》中的作弊行为，并公开了标注数据集CS2CD。模型在未增强测试集上准确率达89.17%，AUC为93.36%。


<details>
  <summary>Details</summary>
Motivation: 在线游戏中的作弊行为破坏了游戏体验的公平性，现有反作弊系统（如VAC）难以在不侵犯用户隐私的情况下应对不断演变的作弊手段。

Method: 使用公开数据集CS2CD（包含795场比赛数据），生成90,707个上下文窗口并进行数据增强以解决类别不平衡问题。训练了一个基于Transformer的模型AntiCheatPT_256。

Result: 模型在未增强测试集上实现了89.17%的准确率和93.36%的AUC。

Conclusion: 该方法强调了可重复性和实际应用性，为数据驱动的作弊检测研究提供了稳健的基线。

Abstract: Cheating in online video games compromises the integrity of gaming
experiences. Anti-cheat systems, such as VAC (Valve Anti-Cheat), face
significant challenges in keeping pace with evolving cheating methods without
imposing invasive measures on users' systems. This paper presents
AntiCheatPT\_256, a transformer-based machine learning model designed to detect
cheating behaviour in Counter-Strike 2 using gameplay data. To support this, we
introduce and publicly release CS2CD: A labelled dataset of 795 matches. Using
this dataset, 90,707 context windows were created and subsequently augmented to
address class imbalance. The transformer model, trained on these windows,
achieved an accuracy of 89.17\% and an AUC of 93.36\% on an unaugmented test
set. This approach emphasizes reproducibility and real-world applicability,
offering a robust baseline for future research in data-driven cheat detection.

</details>


### [64] [From Explainable to Explanatory Artificial Intelligence: Toward a New Paradigm for Human-Centered Explanations through Generative AI](https://arxiv.org/abs/2508.06352)
*Christian Meske,Justin Brenne,Erdi Uenal,Sabahat Oelcer,Ayseguel Doganguen*

Main category: cs.AI

TL;DR: 论文提出“解释性AI”作为可解释AI（XAI）的补充范式，利用生成式AI能力为用户提供情境化的解释，而非仅关注算法透明度。


<details>
  <summary>Details</summary>
Motivation: 当前XAI方法过于抽象且缺乏适应性，难以支持用户理解。本文旨在通过情境化解释提升人类决策支持。

Method: 提出八维概念模型，结合叙事沟通、自适应个性化和渐进披露原则，并通过医疗专业人员验证。

Result: 用户更偏好情境敏感的多模态解释，而非技术透明度。

Conclusion: 需设计以用户理解为中心的AI解释方法，推动跨领域和文化的研究议程。

Abstract: Current explainable AI (XAI) approaches prioritize algorithmic transparency
and present explanations in abstract, non-adaptive formats that often fail to
support meaningful end-user understanding. This paper introduces "Explanatory
AI" as a complementary paradigm that leverages generative AI capabilities to
serve as explanatory partners for human understanding rather than providers of
algorithmic transparency. While XAI reveals algorithmic decision processes for
model validation, Explanatory AI addresses contextual reasoning to support
human decision-making in sociotechnical contexts. We develop a definition and
systematic eight-dimensional conceptual model distinguishing Explanatory AI
through narrative communication, adaptive personalization, and progressive
disclosure principles. Empirical validation through Rapid Contextual Design
methodology with healthcare professionals demonstrates that users consistently
prefer context-sensitive, multimodal explanations over technical transparency.
Our findings reveal the practical urgency for AI systems designed for human
comprehension rather than algorithmic introspection, establishing a
comprehensive research agenda for advancing user-centered AI explanation
approaches across diverse domains and cultural contexts.

</details>


### [65] [Automated Creation of the Legal Knowledge Graph Addressing Legislation on Violence Against Women: Resource, Methodology and Lessons Learned](https://arxiv.org/abs/2508.06368)
*Claudia dAmato,Giuseppe Rubini,Francesco Didio,Donato Francioso,Fatima Zahra Amara,Nicola Fanizzi*

Main category: cs.AI

TL;DR: 论文提出两种自动化构建法律知识图谱（KG）的方法，填补法律领域KG的空白，并验证其对暴力侵害妇女案件的实用性。


<details>
  <summary>Details</summary>
Motivation: 法律决策需要全面的立法背景知识和最新案例信息，而法律KG可以促进信息获取和支持机器学习应用。目前法律领域KG较少，因此开发了针对暴力侵害妇女案件的法律KG。

Method: 采用两种互补方法：系统化的自底向上方法（针对法律领域定制）和基于大语言模型的新方案。从欧洲法院公开的判决书中提取结构化数据，开发本体并进行语义丰富。

Result: 构建了针对暴力侵害妇女案件的法律KG，并通过能力问题验证了其有效性。KG可提升法律信息的可访问性，支持复杂查询和机器学习工具。

Conclusion: 开发的法律KG填补了法律领域KG的空白，对提升法律信息获取和机器学习应用具有重要价值。

Abstract: Legal decision-making process requires the availability of comprehensive and
detailed legislative background knowledge and up-to-date information on legal
cases and related sentences/decisions. Legal Knowledge Graphs (KGs) would be a
valuable tool to facilitate access to legal information, to be queried and
exploited for the purpose, and to enable advanced reasoning and machine
learning applications. Indeed, legal KGs may act as knowledge intensive
component to be used by pre-dictive machine learning solutions supporting the
decision process of the legal expert. Nevertheless, a few KGs can be found in
the legal domain. To fill this gap, we developed a legal KG targeting legal
cases of violence against women, along with clear adopted methodologies.
Specifically, the paper introduces two complementary approaches for automated
legal KG construction; a systematic bottom-up approach, customized for the
legal domain, and a new solution leveraging Large Language Models. Starting
from legal sentences publicly available from the European Court of Justice, the
solutions integrate structured data extraction, ontology development, and
semantic enrichment to produce KGs tailored for legal cases involving violence
against women. After analyzing and comparing the results of the two approaches,
the developed KGs are validated via suitable competency questions. The obtained
KG may be impactful for multiple purposes: can improve the accessibility to
legal information both to humans and machine, can enable complex queries and
may constitute an important knowledge component to be possibly exploited by
machine learning tools tailored for predictive justice.

</details>


### [66] [The Fair Game: Auditing & Debiasing AI Algorithms Over Time](https://arxiv.org/abs/2508.06443)
*Debabrota Basu,Udvas Das*

Main category: cs.AI

TL;DR: 论文提出了一种动态机制“Fair Game”，通过结合审计员和去偏算法，利用强化学习实现公平性目标随时间调整的公平机器学习框架。


<details>
  <summary>Details</summary>
Motivation: 现有公平性定义多为观察性且相互冲突，难以在动态社会环境中实现公平性目标，因此需要一种灵活且适应性强的框架。

Method: 提出“Fair Game”机制，将审计员和去偏算法通过强化学习循环结合，动态调整公平性目标。

Result: “Fair Game”能够模拟社会伦理和法律框架的演变，提供部署前后均可适应的公平机器学习系统。

Conclusion: “Fair Game”为公平机器学习提供了一种动态、灵活的解决方案，能够适应社会变化并持续优化公平性。

Abstract: An emerging field of AI, namely Fair Machine Learning (ML), aims to quantify
different types of bias (also known as unfairness) exhibited in the predictions
of ML algorithms, and to design new algorithms to mitigate them. Often, the
definitions of bias used in the literature are observational, i.e. they use the
input and output of a pre-trained algorithm to quantify a bias under concern.
In reality,these definitions are often conflicting in nature and can only be
deployed if either the ground truth is known or only in retrospect after
deploying the algorithm. Thus,there is a gap between what we want Fair ML to
achieve and what it does in a dynamic social environment. Hence, we propose an
alternative dynamic mechanism,"Fair Game",to assure fairness in the predictions
of an ML algorithm and to adapt its predictions as the society interacts with
the algorithm over time. "Fair Game" puts together an Auditor and a Debiasing
algorithm in a loop around an ML algorithm. The "Fair Game" puts these two
components in a loop by leveraging Reinforcement Learning (RL). RL algorithms
interact with an environment to take decisions, which yields new observations
(also known as data/feedback) from the environment and in turn, adapts future
decisions. RL is already used in algorithms with pre-fixed long-term fairness
goals. "Fair Game" provides a unique framework where the fairness goals can be
adapted over time by only modifying the auditor and the different biases it
quantifies. Thus,"Fair Game" aims to simulate the evolution of ethical and
legal frameworks in the society by creating an auditor which sends feedback to
a debiasing algorithm deployed around an ML system. This allows us to develop a
flexible and adaptive-over-time framework to build Fair ML systems pre- and
post-deployment.

</details>


### [67] [What Voting Rules Actually Do: A Data-Driven Analysis of Multi-Winner Voting](https://arxiv.org/abs/2508.06454)
*Joshua Caiata,Ben Armstrong,Kate Larson*

Main category: cs.AI

TL;DR: 提出数据驱动框架评估多赢家投票规则在实践中的公理违反频率，并展示神经网络在减少公理违反方面优于传统规则。


<details>
  <summary>Details</summary>
Motivation: 研究多赢家投票规则在不同偏好分布下的公理表现，以数据驱动方法补充最坏情况分析的二元视角。

Method: 提出数据驱动框架，分析投票规则与公理表现的关系，并测试神经网络作为投票规则的效果。

Result: 神经网络在减少公理违反方面表现优于传统投票规则。

Conclusion: 数据驱动方法可为新投票系统设计提供参考，推动社会选择领域的数据驱动研究。

Abstract: Committee-selection problems arise in many contexts and applications, and
there has been increasing interest within the social choice research community
on identifying which properties are satisfied by different multi-winner voting
rules. In this work, we propose a data-driven framework to evaluate how
frequently voting rules violate axioms across diverse preference distributions
in practice, shifting away from the binary perspective of axiom satisfaction
given by worst-case analysis. Using this framework, we analyze the relationship
between multi-winner voting rules and their axiomatic performance under several
preference distributions. We then show that neural networks, acting as voting
rules, can outperform traditional rules in minimizing axiom violations. Our
results suggest that data-driven approaches to social choice can inform the
design of new voting systems and support the continuation of data-driven
research in social choice.

</details>
