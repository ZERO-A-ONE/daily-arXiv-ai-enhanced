<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 16]
- [cs.CR](#cs.CR) [Total: 15]
- [cs.SE](#cs.SE) [Total: 18]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Attention as Binding: A Vector-Symbolic Perspective on Transformer Reasoning](https://arxiv.org/abs/2512.14709)
*Sahil Rajesh Dhayalkar*

Main category: cs.AI

TL;DR: 该论文将Transformer自注意力机制解释为近似的向量符号架构，通过这种代数视角分析语言模型的推理行为与失败模式，并提出VSA启发的架构改进方案。


<details>
  <summary>Details</summary>
Motivation: Transformer语言模型展现出类似推理的行为，但在需要稳定符号操作的任务上仍然脆弱。本文旨在通过向量符号架构的统一视角来解释这些现象，为构建更可解释和逻辑可靠的推理系统提供理论基础。

Method: 将自注意力和残差流解释为近似VSA：查询和键定义角色空间，值编码填充物，注意力权重执行软解绑，残差连接实现多个绑定结构的叠加。基于此提出VSA启发的架构偏置，包括显式绑定/解绑头和超维内存层，以及促进角色-填充物分离的训练目标。

Result: 建立了Transformer内部机制与思维链、基于程序的推理、记忆增强工具使用之间的联系，解释了变量混淆和逻辑相关提示不一致等特征性失败模式。提出了测量"VSA相似性"和逻辑组合性的指标。

Conclusion: 将注意力视为软向量符号计算为构建更可解释和逻辑可靠的推理系统提供了原则性路径，并提出了理论和架构上的开放性问题。

Abstract: Transformer-based language models display impressive reasoning-like behavior, yet remain brittle on tasks that require stable symbolic manipulation. This paper develops a unified perspective on these phenomena by interpreting self-attention and residual streams as implementing an approximate Vector Symbolic Architecture (VSA). In this view, queries and keys define role spaces, values encode fillers, attention weights perform soft unbinding, and residual connections realize superposition of many bound structures. We use this algebraic lens to relate transformer internals to chain-of-thought traces, program-based reasoning, and memory-augmented tool use, and to explain characteristic failure modes such as variable confusion and inconsistency across logically related prompts. Building on this perspective, we propose VSA-inspired architectural biases, including explicit binding/unbinding heads and hyperdimensional memory layers, and training objectives that promote role-filler separation and robust superposition. Finally, we outline metrics for measuring "VSA-likeness" and logical compositionality, and pose theoretical and architectural open problems. Overall, the paper argues that viewing attention as soft vector-symbolic computation offers a principled route toward more interpretable and logically reliable reasoning systems.

</details>


### [2] [GR-Agent: Adaptive Graph Reasoning Agent under Incomplete Knowledge](https://arxiv.org/abs/2512.14766)
*Dongzhuoran Zhou,Yuqicheng Zhu,Xiaxia Wang,Hongkuan Zhou,Jiaoyan Chen,Steffen Staab,Yuan He,Evgeny Kharlamov*

Main category: cs.AI

TL;DR: 该论文提出了一种在知识图谱不完整情况下构建基准测试的方法，并开发了自适应图推理智能体（GR-Agent）来解决知识图谱问答中的推理能力不足问题。


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱问答基准测试通常假设知识图谱是完整的，包含直接支持答案的三元组，这忽略了现实世界中知识图谱不完整的情况。当知识图谱缺失事实时，需要从现有事实进行推理才能得到答案，而现有方法在这种情况下的推理能力有限。

Method: 提出两种主要方法：1）构建知识图谱不完整情况下的基准测试方法论，移除直接支持的三元组但保留替代推理路径；2）开发自适应图推理智能体（GR-Agent），将知识图谱构建为交互环境，将问答任务形式化为智能体与环境交互，使用图推理工具作为动作空间，并维护包含相关关系和推理路径的潜在支持证据记忆。

Result: 实验表明：1）现有方法在知识图谱不完整情况下性能显著下降，凸显其推理能力有限；2）GR-Agent在非训练基线方法中表现最优，在完整和不完整设置下都能与基于训练的方法相媲美。

Conclusion: 该研究填补了知识图谱问答评估中不完整性评估的空白，提出的GR-Agent通过将问答任务形式化为智能体与环境交互，有效提升了在知识图谱不完整情况下的推理能力，为知识图谱问答研究提供了新的方向。

Abstract: Large language models (LLMs) achieve strong results on knowledge graph question answering (KGQA), but most benchmarks assume complete knowledge graphs (KGs) where direct supporting triples exist. This reduces evaluation to shallow retrieval and overlooks the reality of incomplete KGs, where many facts are missing and answers must be inferred from existing facts. We bridge this gap by proposing a methodology for constructing benchmarks under KG incompleteness, which removes direct supporting triples while ensuring that alternative reasoning paths required to infer the answer remain. Experiments on benchmarks constructed using our methodology show that existing methods suffer consistent performance degradation under incompleteness, highlighting their limited reasoning ability. To overcome this limitation, we present the Adaptive Graph Reasoning Agent (GR-Agent). It first constructs an interactive environment from the KG, and then formalizes KGQA as agent environment interaction within this environment. GR-Agent operates over an action space comprising graph reasoning tools and maintains a memory of potential supporting reasoning evidence, including relevant relations and reasoning paths. Extensive experiments demonstrate that GR-Agent outperforms non-training baselines and performs comparably to training-based methods under both complete and incomplete settings.

</details>


### [3] [IaC Generation with LLMs: An Error Taxonomy and A Study on Configuration Knowledge Injection](https://arxiv.org/abs/2512.14792)
*Roman Nekrasov,Stefano Fossati,Indika Kumara,Damian Andrew Tamburri,Willem-Jan van den Heuvel*

Main category: cs.AI

TL;DR: 该研究通过系统注入结构化配置知识，显著提升了LLM在生成正确且意图对齐的Terraform基础设施即代码方面的成功率，但发现LLM在技术正确性提升的同时，意图对齐方面存在瓶颈。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在生成正确且意图对齐的基础设施即代码方面成功率较低，需要研究方法来改进LLM在IaC生成方面的能力。

Method: 1. 显著增强现有IaC-Eval基准，增加云仿真和自动化错误分析；2. 开发了LLM辅助IaC代码生成的新型错误分类法；3. 实施并评估了一系列知识注入技术，从简单的检索增强生成到更复杂的图RAG方法，包括图组件的语义丰富化和建模资源间依赖关系。

Result: 基线LLM性能较差（总体成功率27.1%），注入结构化配置知识后，技术验证成功率提升至75.3%，总体成功率提升至62.6%。但意图对齐方面出现平台期，揭示了"正确性-一致性差距"。

Conclusion: 虽然通过结构化配置知识注入可以显著提高LLM生成IaC的技术正确性，但LLM在满足细微用户意图方面仍然有限，表明它们可以成为熟练的"编码者"，但在作为"架构师"方面仍有局限。

Abstract: Large Language Models (LLMs) currently exhibit low success rates in generating correct and intent-aligned Infrastructure as Code (IaC). This research investigated methods to improve LLM-based IaC generation, specifically for Terraform, by systematically injecting structured configuration knowledge. To facilitate this, an existing IaC-Eval benchmark was significantly enhanced with cloud emulation and automated error analysis. Additionally, a novel error taxonomy for LLM-assisted IaC code generation was developed. A series of knowledge injection techniques was implemented and evaluated, progressing from Naive Retrieval-Augmented Generation (RAG) to more sophisticated Graph RAG approaches. These included semantic enrichment of graph components and modeling inter-resource dependencies. Experimental results demonstrated that while baseline LLM performance was poor (27.1% overall success), injecting structured configuration knowledge increased technical validation success to 75.3% and overall success to 62.6%. Despite these gains in technical correctness, intent alignment plateaued, revealing a "Correctness-Congruence Gap" where LLMs can become proficient "coders" but remain limited "architects" in fulfilling nuanced user intent.

</details>


### [4] [AgroAskAI: A Multi-Agentic AI Framework for Supporting Smallholder Farmers' Enquiries Globally](https://arxiv.org/abs/2512.14910)
*Nadine Angela Cantonjos,Arpita Biswas*

Main category: cs.AI

TL;DR: AgroAskAI是一个用于农业气候适应决策支持的多智能体推理系统，专门针对农村脆弱社区，通过模块化架构和链式责任方法协调自主智能体，提供可操作、本地化、多语言的决策建议。


<details>
  <summary>Details</summary>
Motivation: 农村农业地区面临干旱、强降雨和天气模式变化等气候相关风险的损害，需要适应性风险管理解决方案和决策策略。现有系统多为单智能体模型或仅用于静态功能的多智能体框架，缺乏支持动态协作推理和情境感知输出的架构。

Method: 提出AgroAskAI多智能体推理系统，采用模块化、角色专业化架构，使用链式责任方法协调自主智能体，集成实时工具和数据集。系统内置治理机制减少幻觉，支持内部反馈，提供连贯且本地相关的策略，并支持多语言交互。

Result: 在常见农业气候适应查询实验中，通过额外工具和提示优化，AgroAskAI能够提供更可操作、更接地气、更具包容性的输出结果。

Conclusion: AgroAskAI展示了智能体AI在农业气候适应中实现可持续和负责任决策支持的潜力，特别适合服务农村脆弱社区。

Abstract: Agricultural regions in rural areas face damage from climate-related risks, including droughts, heavy rainfall, and shifting weather patterns. Prior research calls for adaptive risk-management solutions and decision-making strategies. To this end, artificial intelligence (AI), particularly agentic AI, offers a promising path forward. Agentic AI systems consist of autonomous, specialized agents capable of solving complex, dynamic tasks. While past systems have relied on single-agent models or have used multi-agent frameworks only for static functions, there is a growing need for architectures that support dynamic collaborative reasoning and context-aware outputs. To bridge this gap, we present AgroAskAI, a multi-agent reasoning system for climate adaptation decision support in agriculture, with a focus on vulnerable rural communities. AgroAskAI features a modular, role-specialized architecture that uses a chain-of-responsibility approach to coordinate autonomous agents, integrating real-time tools and datasets. The system has built-in governance mechanisms that mitigate hallucination and enable internal feedback for coherent, locally relevant strategies. The system also supports multilingual interactions, making it accessible to non-English-speaking farmers. Experiments on common agricultural queries related to climate adaptation show that, with additional tools and prompt refinement, AgroAskAI delivers more actionable, grounded, and inclusive outputs. Our experimental results highlight the potential of agentic AI for sustainable and accountable decision support in climate adaptation for agriculture.

</details>


### [5] [Agentic AI for Integrated Sensing and Communication: Analysis, Framework, and Case Study](https://arxiv.org/abs/2512.15044)
*Wenwen Xie,Geng Sun,Ruichen Zhang,Xuejie Liu,Yinqiu Liu,Jiacheng Wang,Dusit Niyato,Ping Zhang*

Main category: cs.AI

TL;DR: 本文探讨了智能体AI在集成感知与通信（ISAC）系统中的应用价值与前景，提出了一个新型的智能体ISAC框架，并通过案例研究验证了其在优化ISAC性能方面的优越性。


<details>
  <summary>Details</summary>
Motivation: 随着无线环境日益动态复杂，ISAC系统需要更智能的处理和更自主的操作来保持效率和适应性。智能体AI通过在动态环境中实现连续感知-推理-行动循环，为ISAC系统提供智能、自主、高效运行的可行解决方案。

Method: 首先全面回顾智能体AI和ISAC系统的关键特性；其次展示ISAC系统的几种常见优化方法，并突出基于生成式AI的智能体AI的显著优势；然后提出一个新型的智能体ISAC框架，并通过案例研究验证其优越性。

Result: 提出的智能体ISAC框架在优化ISAC性能方面表现出优越性，案例研究验证了该框架的有效性。

Conclusion: 智能体AI为ISAC系统提供了重要的技术支持，能够实现智能、自主、高效的运行。未来需要进一步探索智能体AI在ISAC系统中的研究方向和应用前景。

Abstract: Integrated sensing and communication (ISAC) has emerged as a key development direction in the sixth-generation (6G) era, which provides essential support for the collaborative sensing and communication of future intelligent networks. However, as wireless environments become increasingly dynamic and complex, ISAC systems require more intelligent processing and more autonomous operation to maintain efficiency and adaptability. Meanwhile, agentic artificial intelligence (AI) offers a feasible solution to address these challenges by enabling continuous perception-reasoning-action loops in dynamic environments to support intelligent, autonomous, and efficient operation for ISAC systems. As such, we delve into the application value and prospects of agentic AI in ISAC systems in this work. Firstly, we provide a comprehensive review of agentic AI and ISAC systems to demonstrate their key characteristics. Secondly, we show several common optimization approaches for ISAC systems and highlight the significant advantages of generative artificial intelligence (GenAI)-based agentic AI. Thirdly, we propose a novel agentic ISAC framework and prensent a case study to verify its superiority in optimizing ISAC performance. Finally, we clarify future research directions for agentic AI-based ISAC systems.

</details>


### [6] [Beyond Fast and Slow: Cognitive-Inspired Elastic Reasoning for Large Language Models](https://arxiv.org/abs/2512.15089)
*Jinwu Hu,Dongjin Yang,Langyu Bian,Zhiquan Wen,Yufeng Wang,Yaofo Chen,Bin Xiao,Yuanqing Li,Mingkui Tan*

Main category: cs.AI

TL;DR: CogER框架通过动态选择推理策略来平衡LLM的推理效率和准确性，模仿人类分层推理机制，根据查询复杂度自动分配处理策略。


<details>
  <summary>Details</summary>
Motivation: 现有LLM推理策略主要依赖模型自身的快慢模式（如o1思考），难以在不同难度查询间平衡推理效率和准确性，需要更智能的自适应推理框架。

Method: 1) 评估查询复杂度并分配到预定义层级；2) 将策略选择建模为马尔可夫决策过程，使用强化学习训练CogER-Agent；3) 引入认知工具辅助推理，允许LLM在思维链中自主调用外部工具。

Result: CogER在In-Domain任务上实现至少13%的相对平均精确匹配提升，在Out-of-Domain任务上实现8%的相对增益，优于最先进的测试时缩放方法。

Conclusion: CogER框架通过动态策略选择和工具辅助推理，有效解决了LLM在不同难度查询上的效率-准确性权衡问题，为自适应推理提供了新思路。

Abstract: Large language models (LLMs) have demonstrated impressive performance across various language tasks. However, existing LLM reasoning strategies mainly rely on the LLM itself with fast or slow mode (like o1 thinking) and thus struggle to balance reasoning efficiency and accuracy across queries of varying difficulties. In this paper, we propose Cognitive-Inspired Elastic Reasoning (CogER), a framework inspired by human hierarchical reasoning that dynamically selects the most suitable reasoning strategy for each query. Specifically, CogER first assesses the complexity of incoming queries and assigns them to one of several predefined levels, each corresponding to a tailored processing strategy, thereby addressing the challenge of unobservable query difficulty. To achieve automatic strategy selection, we model the process as a Markov Decision Process and train a CogER-Agent using reinforcement learning. The agent is guided by a reward function that balances solution quality and computational cost, ensuring resource-efficient reasoning. Moreover, for queries requiring external tools, we introduce Cognitive Tool-Assisted Reasoning, which enables the LLM to autonomously invoke external tools within its chain-of-thought. Extensive experiments demonstrate that CogER outperforms state-of-the-art Test-Time scaling methods, achieving at least a 13% relative improvement in average exact match on In-Domain tasks and an 8% relative gain on Out-of-Domain tasks.

</details>


### [7] [A Clustering-Based Variable Ordering Framework for Relaxed Decision Diagrams for Maximum Weighted Independent Set Problem](https://arxiv.org/abs/2512.15198)
*Mohsen Nafar,Michael Römer,Lin Xie*

Main category: cs.AI

TL;DR: 本文提出了一种基于聚类的变量排序框架，用于提升离散优化中决策图的质量和效率，特别针对最大加权独立集问题。


<details>
  <summary>Details</summary>
Motivation: 离散优化中的高效精确算法依赖于强原始和对偶边界。松弛决策图通过节点合并紧凑地过度近似解空间来提供对偶边界，但其质量严重依赖于变量排序和合并决策。虽然动态变量排序启发式方法能有效收紧边界，但在整个变量集上全局评估时往往带来计算开销。为了缓解这种权衡，需要一种更高效的变量排序方法。

Method: 提出基于聚类的变量排序框架：首先将变量划分为聚类，然后利用这种结构分解来指导排序过程，显著减少启发式搜索空间。研究了两种策略：1) 聚类到聚类策略：使用问题特定的聚合标准（如MWISP中的累积顶点权重）顺序处理聚类；2) 选择排序策略：迭代地从每个聚类中选择和排序代表性变量，以平衡局部多样性与启发式指导。此外，针对MWISP中决策图大小的增长提出了两种设置聚类数量的策略，并将这些策略嵌入到基于决策图的分支定界算法中。

Result: 在MWISP基准实例上的评估表明，与标准的动态变量排序基线相比，所提出的方法一致地降低了计算成本。

Conclusion: 基于聚类的变量排序框架通过减少启发式搜索空间，在保持决策图质量的同时显著降低了计算开销，为离散优化中的变量排序问题提供了一种有效的解决方案。

Abstract: Efficient exact algorithms for Discrete Optimization (DO) rely heavily on strong primal and dual bounds. Relaxed Decision Diagrams (DDs) provide a versatile mechanism for deriving such dual bounds by compactly over-approximating the solution space through node merging. However, the quality of these relaxed diagrams, i.e. the tightness of the resulting dual bounds, depends critically on the variable ordering and the merging decisions executed during compilation. While dynamic variable ordering heuristics effectively tighten bounds, they often incur computational overhead when evaluated globally across the entire variable set. To mitigate this trade-off, this work introduces a novel clustering-based framework for variable ordering. Instead of applying dynamic ordering heuristics to the full set of unfixed variables, we first partition variables into clusters. We then leverage this structural decomposition to guide the ordering process, significantly reducing the heuristic's search space. Within this framework, we investigate two distinct strategies: Cluster-to-Cluster, which processes clusters sequentially using problem-specific aggregate criteria (such as cumulative vertex weights in the Maximum Weighted Independent Set Problem (MWISP)), and Pick-and-Sort, which iteratively selects and sorts representative variables from each cluster to balance local diversity with heuristic guidance. Later on, developing some theoretical results on the growth of the size of DDs for MWISP we propose two different policies for setting the number of clusters within the proposed framework. We embed these strategies into a DD-based branch-and-bound algorithm and evaluate them on the MWISP. Across benchmark instances, the proposed methodology consistently reduces computational costs compared to standard dynamic variable ordering baseline.

</details>


### [8] [ChatGPT and Gemini participated in the Korean College Scholastic Ability Test -- Earth Science I](https://arxiv.org/abs/2512.15298)
*Seok-Hyun Ga,Chun-Yen Chang*

Main category: cs.AI

TL;DR: 本研究利用2025年韩国大学修学能力考试地球科学I部分，深入分析了GPT-4o、Gemini 2.5 Flash和Gemini 2.5 Pro等先进大语言模型的多模态科学推理能力和认知局限，揭示了AI在感知-认知、计算-概念化等方面的缺陷，为设计"抗AI问题"提供了依据。


<details>
  <summary>Details</summary>
Motivation: 随着学生使用AI完成作业的普及，学术诚信和评估有效性受到威胁。本研究旨在深入分析先进大语言模型在科学推理中的认知局限，为设计能够区分学生真实能力与AI生成答案的评估工具提供基础。

Method: 使用2025年韩国高考地球科学I试题，设计三种实验条件（整页输入、单项输入、优化多模态输入），对GPT-4o、Gemini 2.5 Flash和Gemini 2.5 Pro进行定量和定性分析，评估不同数据结构下的模型表现。

Result: 非结构化输入因分割和OCR失败导致性能显著下降；即使在优化条件下，模型仍表现出基本推理缺陷。定性分析揭示了"感知错误"占主导，存在"感知-认知鸿沟"（模型能识别视觉数据但无法解释图表中的符号意义）、"计算-概念化差异"（能执行计算但无法应用基础科学概念）和"过程幻觉"（跳过视觉验证而依赖无根据的背景知识）。

Conclusion: 通过针对AI的认知弱点（特别是感知与认知之间的鸿沟），教育工作者可以设计"抗AI问题"来区分学生的真实能力与AI生成答案，从而确保评估的公平性。这为解决课程作业中未经授权的AI使用问题提供了可行的线索。

Abstract: The rapid development of Generative AI is bringing innovative changes to education and assessment. As the prevalence of students utilizing AI for assignments increases, concerns regarding academic integrity and the validity of assessments are growing. This study utilizes the Earth Science I section of the 2025 Korean College Scholastic Ability Test (CSAT) to deeply analyze the multimodal scientific reasoning capabilities and cognitive limitations of state-of-the-art Large Language Models (LLMs), including GPT-4o, Gemini 2.5 Flash, and Gemini 2.5 Pro. Three experimental conditions (full-page input, individual item input, and optimized multimodal input) were designed to evaluate model performance across different data structures. Quantitative results indicated that unstructured inputs led to significant performance degradation due to segmentation and Optical Character Recognition (OCR) failures. Even under optimized conditions, models exhibited fundamental reasoning flaws. Qualitative analysis revealed that "Perception Errors" were dominant, highlighting a "Perception-Cognition Gap" where models failed to interpret symbolic meanings in schematic diagrams despite recognizing visual data. Furthermore, models demonstrated a "Calculation-Conceptualization Discrepancy," successfully performing calculations while failing to apply the underlying scientific concepts, and "Process Hallucination," where models skipped visual verification in favor of plausible but unfounded background knowledge. Addressing the challenge of unauthorized AI use in coursework, this study provides actionable cues for designing "AI-resistant questions" that target these specific cognitive vulnerabilities. By exploiting AI's weaknesses, such as the gap between perception and cognition, educators can distinguish genuine student competency from AI-generated responses, thereby ensuring assessment fairness.

</details>


### [9] [Bilateral Spatial Reasoning about Street Networks: Graph-based RAG with Qualitative Spatial Representations](https://arxiv.org/abs/2512.15388)
*Reinhard Moratz,Niklas Daute,James Ondieki,Markus Kattenbeck,Mario Krajina,Ioannis Giannopoulos*

Main category: cs.AI

TL;DR: 该论文通过定性空间关系提升大语言模型为行人提供路线指引的能力


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在提供行人路线指引时存在局限性，需要改进其使用定性空间关系的能力

Method: 通过定性空间关系来增强大语言模型的路线指引能力

Result: 改进了大语言模型为行人提供路线指引的能力

Conclusion: 使用定性空间关系可以有效提升大语言模型在行人导航任务中的表现

Abstract: This paper deals with improving the capabilities of Large Language Models (LLM) to provide route instructions for pedestrian wayfinders by means of qualitative spatial relations.

</details>


### [10] [Outer-Learning Framework for Playing Multi-Player Trick-Taking Card Games: A Case Study in Skat](https://arxiv.org/abs/2512.15435)
*Stefan Edelkamp*

Main category: cs.AI

TL;DR: 提出一种通过自对弈AI游戏扩展人类专家游戏数据库的通用外学习框架，提高多玩家纸牌游戏早期决策的预测准确性。


<details>
  <summary>Details</summary>
Motivation: 在多玩家纸牌游戏（如Skat或Bridge）中，早期阶段（如叫牌、游戏选择和初始牌选择）对游戏成功至关重要，但当前计算限制下这些决策主要依赖人类专家游戏的统计信息，存在局限性。

Method: 开发通用引导式外学习框架，通过自对弈AI游戏生成数百万游戏扩展人类游戏数据库，使用完美特征哈希函数处理压缩表，创建自改进的纸牌游戏引擎，实现新推断知识在自学习过程中持续改进。

Result: 在Skat游戏案例研究中，该自动化方法能够有效支持游戏中的各种决策，提高了预测准确性。

Conclusion: 通过自对弈AI游戏扩展人类专家数据库的框架能够显著改进多玩家纸牌游戏的早期决策质量，为类似游戏提供了有效的自动化决策支持方案。

Abstract: In multi-player card games such as Skat or Bridge, the early stages of the game, such as bidding, game selection, and initial card selection, are often more critical to the success of the play than refined middle- and end-game play. At the current limits of computation, such early decision-making resorts to using statistical information derived from a large corpus of human expert games. In this paper, we derive and evaluate a general bootstrapping outer-learning framework that improves prediction accuracy by expanding the database of human games with millions of self-playing AI games to generate and merge statistics. We implement perfect feature hash functions to address compacted tables, producing a self-improving card game engine, where newly inferred knowledge is continuously improved during self-learning. The case study in Skat shows that the automated approach can be used to support various decisions in the game.

</details>


### [11] [Intent-Driven UAM Rescheduling](https://arxiv.org/abs/2512.15462)
*Jeongseok Kim,Kangjin Kim*

Main category: cs.AI

TL;DR: 本文提出了一种结合ASP和MILP的集成框架，用于处理城市空中交通中动态运营需求和模糊调度请求的调度问题。


<details>
  <summary>Details</summary>
Motivation: 由于资源受限，城市空中交通中的垂直起降场高效调度受到广泛关注。现有调度方法难以同时处理动态运营需求和人类模糊的重新调度请求。

Method: 采用混合整数线性规划（MILP）作为基础调度框架，结合三值逻辑解释模糊用户意图，使用决策树处理人类输入，提出将答案集编程（ASP）与MILP集成的新系统。

Result: 该集成框架能够优化调度并透明地支持人类输入，为可解释、自适应的UAM调度提供了鲁棒结构。

Conclusion: 提出的ASP-MILP集成系统能够有效处理动态运营需求和模糊调度请求，为城市空中交通调度提供了可解释、自适应的解决方案。

Abstract: Due to the restricted resources, efficient scheduling in vertiports has received much more attention in the field of Urban Air Mobility (UAM). For the scheduling problem, we utilize a Mixed Integer Linear Programming (MILP), which is often formulated in a resource-restricted project scheduling problem (RCPSP). In this paper, we show our approach to handle both dynamic operation requirements and vague rescheduling requests from humans. Particularly, we utilize a three-valued logic for interpreting ambiguous user intents and a decision tree, proposing a newly integrated system that combines Answer Set Programming (ASP) and MILP. This integrated framework optimizes schedules and supports human inputs transparently. With this system, we provide a robust structure for explainable, adaptive UAM scheduling.

</details>


### [12] [Nemotron-Math: Efficient Long-Context Distillation of Mathematical Reasoning from Multi-Mode Supervision](https://arxiv.org/abs/2512.15489)
*Wei Du,Shubham Toshniwal,Branislav Kisacanin,Sadegh Mahdavi,Ivan Moshkov,George Armstrong,Stephen Ge,Edgar Minasyan,Feng Chen,Igor Gitman*

Main category: cs.AI

TL;DR: Nemotron-Math是一个包含750万条解题轨迹的大规模数学推理数据集，融合了AoPS竞赛题和StackExchange数学问题，支持多种推理模式并集成了Python工具，通过顺序分桶策略加速长上下文训练，在数学竞赛基准上实现了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有数学推理数据集在推理风格多样性、长形式轨迹和工具集成方面存在局限，需要更高质量、更大规模的监督数据来提升数学推理模型的性能。

Method: 利用GPT-OSS-120B的多模式生成能力，构建包含高、中、低三种推理模式的7.5M解题轨迹数据集，整合85K AoPS竞赛题和262K StackExchange数学问题，开发顺序分桶策略加速128K上下文长度的微调。

Result: Nemotron-Math在匹配的AoPS问题上持续优于原始OpenMathReasoning，集成StackExchange-Math显著提高了鲁棒性和泛化能力，在HLE-Math上表现优异，同时在数学竞赛基准上保持准确率，实现了100% maj@16准确率在AIME 2024和2025上。

Conclusion: Nemotron-Math通过大规模、多样化的数学推理数据集和高效训练策略，实现了最先进的数学推理性能，为数学AI研究提供了高质量的数据资源。

Abstract: High-quality mathematical reasoning supervision requires diverse reasoning styles, long-form traces, and effective tool integration, capabilities that existing datasets provide only in limited form. Leveraging the multi-mode generation ability of gpt-oss-120b, we introduce Nemotron-Math, a large-scale mathematical reasoning dataset containing 7.5M solution traces across high, medium, and low reasoning modes, each available both with and without Python tool-integrated reasoning (TIR).
  The dataset integrates 85K curated AoPS problems with 262K community-sourced StackExchange-Math problems, combining structured competition tasks with diverse real-world mathematical queries. We conduct controlled evaluations to assess the dataset quality.
  Nemotron-Math consistently outperforms the original OpenMathReasoning on matched AoPS problems. Incorporating StackExchange-Math substantially improves robustness and generalization, especially on HLE-Math, while preserving accuracy on math competition benchmarks.
  To support efficient long-context training, we develop a sequential bucketed strategy that accelerates 128K context-length fine-tuning by 2--3$\times$ without significant accuracy loss. Overall, Nemotron-Math enables state-of-the-art performance, including 100\% maj@16 accuracy on AIME 2024 and 2025 with Python TIR.

</details>


### [13] [Evaluating Large Language Models in Scientific Discovery](https://arxiv.org/abs/2512.15567)
*Zhangde Song,Jieyu Lu,Yuanqi Du,Botao Yu,Thomas M. Pruyn,Yue Huang,Kehan Guo,Xiuzhe Luo,Yuanhao Qu,Yi Qu,Yinkai Wang,Haorui Wang,Jeff Guo,Jingru Gan,Parshin Shojaee,Di Luo,Andres M Bran,Gen Li,Qiyuan Zhao,Shao-Xiong Lennon Luo,Yuxuan Zhang,Xiang Zou,Wanru Zhao,Yifan F. Zhang,Wucheng Zhang,Shunan Zheng,Saiyang Zhang,Sartaaj Takrim Khan,Mahyar Rajabi-Kochi,Samantha Paradi-Maropakis,Tony Baltoiu,Fengyu Xie,Tianyang Chen,Kexin Huang,Weiliang Luo,Meijing Fang,Xin Yang,Lixue Cheng,Jiajun He,Soha Hassoun,Xiangliang Zhang,Wei Wang,Chandan K. Reddy,Chao Zhang,Zhiling Zheng,Mengdi Wang,Le Cong,Carla P. Gomes,Chang-Yu Hsieh,Aditya Nandy,Philippe Schwaller,Heather J. Kulik,Haojun Jia,Huan Sun,Seyed Mohamad Moosavi,Chenru Duan*

Main category: cs.AI

TL;DR: 该论文提出了一个基于场景的科学发现评估框架，用于评估大语言模型在真实科学研究项目中的能力，发现当前模型在科学发现任务上存在系统性弱点，距离通用科学"超级智能"还很遥远。


<details>
  <summary>Details</summary>
Motivation: 现有科学基准测试主要评估去语境化的知识，而忽视了驱动科学发现所需的迭代推理、假设生成和观察解释等关键能力，因此需要开发更贴近真实科学研究过程的评估框架。

Method: 引入场景化基准测试框架，由领域专家定义真实研究项目并将其分解为模块化研究场景，从这些场景中采样经过验证的问题。采用两阶段评估：问题级准确性评估和项目级性能评估（包括提出可测试假设、设计实验/模拟、解释结果等）。

Result: 应用该框架评估最先进的大语言模型发现：相对于通用科学基准存在一致的性能差距；模型规模和推理能力的扩展收益递减；不同提供商的最优模型存在系统性弱点；研究场景间的性能差异导致不同科学发现项目中最佳模型选择会变化。

Conclusion: 当前所有大语言模型距离通用科学"超级智能"还很遥远，但已在多种科学发现项目中显示出潜力。该评估框架为发现相关的LLM评估提供了可复现的基准，并为推动其向科学发现方向发展指明了实践路径。

Abstract: Large language models (LLMs) are increasingly applied to scientific research, yet prevailing science benchmarks probe decontextualized knowledge and overlook the iterative reasoning, hypothesis generation, and observation interpretation that drive scientific discovery. We introduce a scenario-grounded benchmark that evaluates LLMs across biology, chemistry, materials, and physics, where domain experts define research projects of genuine interest and decompose them into modular research scenarios from which vetted questions are sampled. The framework assesses models at two levels: (i) question-level accuracy on scenario-tied items and (ii) project-level performance, where models must propose testable hypotheses, design simulations or experiments, and interpret results. Applying this two-phase scientific discovery evaluation (SDE) framework to state-of-the-art LLMs reveals a consistent performance gap relative to general science benchmarks, diminishing return of scaling up model sizes and reasoning, and systematic weaknesses shared across top-tier models from different providers. Large performance variation in research scenarios leads to changing choices of the best performing model on scientific discovery projects evaluated, suggesting all current LLMs are distant to general scientific "superintelligence". Nevertheless, LLMs already demonstrate promise in a great variety of scientific discovery projects, including cases where constituent scenario scores are low, highlighting the role of guided exploration and serendipity in discovery. This SDE framework offers a reproducible benchmark for discovery-relevant evaluation of LLMs and charts practical paths to advance their development toward scientific discovery.

</details>


### [14] [A Decision-Theoretic Approach for Managing Misalignment](https://arxiv.org/abs/2512.15584)
*Daniel A. Herrmann,Abinav Chari,Isabelle Qian,Sree Sharvesh,B. A. Levinstein*

Main category: cs.AI

TL;DR: 论文提出了一个决策理论框架，用于分析在价值未完全对齐的情况下何时应该将决策委托给AI系统，区分了通用委托和特定情境委托的不同要求。


<details>
  <summary>Details</summary>
Motivation: 当前价值对齐文献主要关注如何塑造AI的价值观，但较少研究在不确定性下如何判断未完全对齐的AI是否足够好以证明委托决策的合理性。需要一种原则性的方法来平衡价值对齐、认知准确性和行动范围之间的关系。

Method: 引入了一个正式的决策理论框架，精确分析委托决策中的权衡，考虑委托者对AI价值对齐、认知准确性和行动范围的不确定性。开发了一个新颖的评分框架来量化这种事前决策。

Result: 分析揭示了两种委托场景的明显区别：1）通用委托需要近乎完美的价值对齐和完全的认知信任，这在实践中很少满足；2）特定情境委托即使在显著价值未对齐的情况下也可能是最优的，因为AI的更高准确性或更广行动范围可能提供更好的整体决策问题。

Conclusion: 该工作提供了一种原则性方法来确定在特定情境下AI是否足够对齐以进行委托，将重点从实现完美对齐转向在不确定性下管理委托的风险和回报。

Abstract: When should we delegate decisions to AI systems? While the value alignment literature has developed techniques for shaping AI values, less attention has been paid to how to determine, under uncertainty, when imperfect alignment is good enough to justify delegation. We argue that rational delegation requires balancing an agent's value (mis)alignment with its epistemic accuracy and its reach (the acts it has available). This paper introduces a formal, decision-theoretic framework to analyze this tradeoff precisely accounting for a principal's uncertainty about these factors. Our analysis reveals a sharp distinction between two delegation scenarios. First, universal delegation (trusting an agent with any problem) demands near-perfect value alignment and total epistemic trust, conditions rarely met in practice. Second, we show that context-specific delegation can be optimal even with significant misalignment. An agent's superior accuracy or expanded reach may grant access to better overall decision problems, making delegation rational in expectation. We develop a novel scoring framework to quantify this ex ante decision. Ultimately, our work provides a principled method for determining when an AI is aligned enough for a given context, shifting the focus from achieving perfect alignment to managing the risks and rewards of delegation under uncertainty.

</details>


### [15] [Stepwise Think-Critique: A Unified Framework for Robust and Interpretable LLM Reasoning](https://arxiv.org/abs/2512.15662)
*Jiaqi Xu,Cuiling Lan,Xuejin Chen,Yan LU*

Main category: cs.AI

TL;DR: STC框架在LLM中集成推理与自我批判，通过混合强化学习联合优化推理质量和自我评估，在数学推理基准上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有LLM将推理与验证分离：要么生成推理而无显式自我检查，要么依赖外部验证器事后检测错误。前者缺乏即时反馈，后者增加系统复杂性并阻碍同步学习。受人类批判性思维启发，需要统一框架在单个模型内交织推理与自我批判。

Method: 提出Stepwise Think-Critique (STC)框架，在每一步推理中交织推理和自我批判。使用混合强化学习目标，结合推理奖励和批判一致性奖励，联合优化推理质量和自我评估。

Result: 在数学推理基准测试中，STC展现出强大的批判性思维能力，并产生更可解释的推理轨迹。

Conclusion: STC代表了向具有内置批判性思维能力的LLMs迈出的一步，通过统一框架实现了推理与自我批判的有机结合。

Abstract: Human beings solve complex problems through critical thinking, where reasoning and evaluation are intertwined to converge toward correct solutions. However, most existing large language models (LLMs) decouple reasoning from verification: they either generate reasoning without explicit self-checking or rely on external verifiers to detect errors post hoc. The former lacks immediate feedback, while the latter increases system complexity and hinders synchronized learning. Motivated by human critical thinking, we propose Stepwise Think-Critique (STC), a unified framework that interleaves reasoning and self-critique at each step within a single model. STC is trained with a hybrid reinforcement learning objective combining reasoning rewards and critique-consistency rewards to jointly optimize reasoning quality and self-evaluation. Experiments on mathematical reasoning benchmarks show that STC demonstrates strong critic-thinking capabilities and produces more interpretable reasoning traces, representing a step toward LLMs with built-in critical thinking.

</details>


### [16] [Explaining the Reasoning of Large Language Models Using Attribution Graphs](https://arxiv.org/abs/2512.15663)
*Chase Walker,Rickard Ewetz*

Main category: cs.AI

TL;DR: CAGE框架通过构建属性图来改进LLM的上下文归因，量化每个生成内容如何受到提示和先前生成内容的影响，提高了归因的忠实度。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然能力强大，但其推理过程不透明，存在安全和信任问题。现有的上下文归因方法直接关联生成标记与提示，忽略了生成过程中的代际影响，导致解释不完整。

Method: 提出CAGE框架，构建属性图——一个有向图，量化每个生成内容如何受到提示和所有先前生成内容的影响。该图保持因果关系和行随机性，通过在图中的路径上边缘化中间贡献来计算上下文归因。

Result: 在多个模型、数据集、指标和方法上，CAGE提高了上下文归因的忠实度，平均增益高达40%。

Conclusion: CAGE框架通过考虑生成过程中的代际影响，提供了更完整、更忠实的LLM行为解释，有助于提高模型透明度和可信度。

Abstract: Large language models (LLMs) exhibit remarkable capabilities, yet their reasoning remains opaque, raising safety and trust concerns. Attribution methods, which assign credit to input features, have proven effective for explaining the decision making of computer vision models. From these, context attributions have emerged as a promising approach for explaining the behavior of autoregressive LLMs. However, current context attributions produce incomplete explanations by directly relating generated tokens to the prompt, discarding inter-generational influence in the process. To overcome these shortcomings, we introduce the Context Attribution via Graph Explanations (CAGE) framework. CAGE introduces an attribution graph: a directed graph that quantifies how each generation is influenced by both the prompt and all prior generations. The graph is constructed to preserve two properties-causality and row stochasticity. The attribution graph allows context attributions to be computed by marginalizing intermediate contributions along paths in the graph. Across multiple models, datasets, metrics, and methods, CAGE improves context attribution faithfulness, achieving average gains of up to 40%.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [17] [Persistent Backdoor Attacks under Continual Fine-Tuning of LLMs](https://arxiv.org/abs/2512.14741)
*Jing Cui,Yufei Han,Jianbin Jiao,Junge Zhang*

Main category: cs.CR

TL;DR: P-Trojan是一种针对大语言模型的后门攻击方法，通过优化后门在多次更新中的持久性，确保恶意行为在用户驱动的持续微调后仍然有效。


<details>
  <summary>Details</summary>
Motivation: 现有后门攻击研究主要关注模型发布时的有效性，但很少研究后门在用户部署后持续微调过程中的持久性。实证表明，简单注入的后门在更新后持久性会下降，因此需要研究后门在多阶段部署后微调中的持久性问题。

Method: 提出P-Trojan攻击算法，通过将中毒梯度与干净任务在词嵌入空间上的梯度对齐，使植入的后门映射在后续更新中不易被抑制或遗忘。该方法明确优化后门在重复更新中的持久性。

Result: 理论分析表明这种持久后门攻击在持续微调后是可行的。在Qwen2.5和LLaMA3系列LLM以及多样化任务序列上的实验显示，P-Trojan实现了超过99%的持久性，同时保持了干净任务的准确性。

Conclusion: 研究结果表明需要在现实的模型适应流程中进行持久性感知评估和更强的防御措施，因为后门攻击可以在多次更新后仍然保持有效。

Abstract: Backdoor attacks embed malicious behaviors into Large Language Models (LLMs), enabling adversaries to trigger harmful outputs or bypass safety controls. However, the persistence of the implanted backdoors under user-driven post-deployment continual fine-tuning has been rarely examined. Most prior works evaluate the effectiveness and generalization of implanted backdoors only at releasing and empirical evidence shows that naively injected backdoor persistence degrades after updates. In this work, we study whether and how implanted backdoors persist through a multi-stage post-deployment fine-tuning. We propose P-Trojan, a trigger-based attack algorithm that explicitly optimizes for backdoor persistence across repeated updates. By aligning poisoned gradients with those of clean tasks on token embeddings, the implanted backdoor mapping is less likely to be suppressed or forgotten during subsequent updates. Theoretical analysis shows the feasibility of such persistent backdoor attacks after continual fine-tuning. And experiments conducted on the Qwen2.5 and LLaMA3 families of LLMs, as well as diverse task sequences, demonstrate that P-Trojan achieves over 99% persistence while preserving clean-task accuracy. Our findings highlight the need for persistence-aware evaluation and stronger defenses in realistic model adaptation pipelines.

</details>


### [18] [Factor(U,T): Controlling Untrusted AI by Monitoring their Plans](https://arxiv.org/abs/2512.14745)
*Edward Lue Chee Lip,Anthony Channg,Diana Kim,Aaron Sandoval,Kevin Zhu*

Main category: cs.CR

TL;DR: 研究恶意AI模型在分解复杂任务时的安全问题，比较不同监控策略的效果


<details>
  <summary>Details</summary>
Motivation: 随着AI能力提升，我们依赖强大模型分解复杂任务，但如果分解器本身是恶意的怎么办？现有方法使用可信但较弱的模型进行分解，限制了在分解本身具有挑战性的任务中的实用性。

Method: 引入Factor(U,T)协议：使用不可信（强大但可能恶意）的模型进行任务分解，可信模型实现子任务。通过BigCodeBench数据集上的控制评估，比较仅监控自然语言任务指令与监控完整解决方案的效果。

Result: 仅监控分解指令的检测器性能很差（AUROC 0.52），而监控完整Python解决方案的检测器性能优秀（AUROC 0.96）。Factor(D,U)使用可信分解器并监控具体子任务解决方案，实现了优秀的分辨能力（AUROC 0.96）和强安全性（1.2%攻击成功率）。

Conclusion: 仅监控任务分解的检测效果有限，而监控具体实现上下文的方法能够有效检测恶意活动。这表明在分解阶段监控自然语言指令不足以确保安全，需要在实现阶段进行更具体的监控。

Abstract: As AI capabilities advance, we increasingly rely on powerful models to decompose complex tasks $\unicode{x2013}$ but what if the decomposer itself is malicious? Factored cognition protocols decompose complex tasks into simpler child tasks: one model creates the decomposition, while other models implement the child tasks in isolation. Prior work uses trusted (weaker but reliable) models for decomposition, which limits usefulness for tasks where decomposition itself is challenging. We introduce Factor($U$,$T$), in which an untrusted (stronger but potentially malicious) model decomposes while trusted models implement child tasks. Can monitors detect malicious activity when observing only natural language task instructions, rather than complete solutions? We baseline and red team Factor($U$,$T$) in control evaluations on BigCodeBench, a dataset of Python coding tasks. Monitors distinguishing malicious from honest decompositions perform poorly (AUROC 0.52) compared to monitors evaluating complete Python solutions (AUROC 0.96). Furthermore, Factor($D$,$U$), which uses a trusted decomposer and monitors concrete child solutions, achieves excellent discrimination (AUROC 0.96) and strong safety (1.2% ASR), demonstrating that implementation-context monitoring succeeds where decomposition-only monitoring fails.

</details>


### [19] [BLINDSPOT: Enabling Bystander-Controlled Privacy Signaling for Camera-Enabled Devices](https://arxiv.org/abs/2512.14746)
*Jad Al Aaraj,Athina Markopoulou*

Main category: cs.CR

TL;DR: BlindSpot是一个在设备上的系统，让旁观者通过实时信号传递隐私偏好来管理自己的隐私，无需预先共享敏感信息。系统评估了三种信号模式：手势、可见光通信和超宽带通信，并包含几何一致性验证机制来防御冒充攻击。


<details>
  <summary>Details</summary>
Motivation: 配备摄像头的移动设备（如手机、智能眼镜、AR头显）对旁观者构成隐私挑战，目前缺乏有效的实时机制来控制他人拍摄自己的照片、视频或面部。旁观者需要在不预先共享敏感信息的情况下，实时表达隐私偏好。

Method: 设计了三种不同的信号传递模式：1）手势机制；2）改进的可见光通信协议；3）新颖的超宽带通信协议。所有模式都包含使用几何一致性检查的验证机制，以验证信号相对于发送旁观者的来源，并防御冒充攻击。系统在商用智能手机上实现，并在不同距离、光照条件和用户运动情况下进行了全面评估。

Result: 对每种模式的准确性和延迟进行了全面评估，结果表明这些新颖的旁观者信号传递技术在系统性能和便利性方面具有可行性，并展示了它们之间的权衡。

Conclusion: BlindSpot系统展示了通过实时信号传递让旁观者管理自己隐私的可行性，三种不同的信号模式各有优缺点，为未来的隐私保护技术提供了新的方向。

Abstract: Camera-equipped mobile devices, such as phones, smart glasses, and AR headsets, pose a privacy challenge for bystanders, who currently lack effective real-time mechanisms to control the capture of their picture, video, including their face. We present BlindSpot, an on-device system that enables bystanders to manage their own privacy by signaling their privacy preferences in real-time without previously sharing any sensitive information. Our main contribution is the design and comparative evaluation of three distinct signaling modalities: a hand gesture mechanism, a significantly improved visible light communication (VLC) protocol, and a novel ultra-wideband (UWB) communication protocol. For all these modalities, we also design a validation mechanism that uses geometric consistency checks to verify the origin of a signal relative to the sending bystander, and defend against impersonation attacks. We implement the complete system (BlindSpot) on a commodity smartphone and conduct a comprehensive evaluation of each modality's accuracy and latency across various distances, lighting conditions, and user movements. Our results demonstrate the feasibility of these novel bystander signaling techniques and their trade-offs in terms of system performance and convenience.

</details>


### [20] [One Leak Away: How Pretrained Model Exposure Amplifies Jailbreak Risks in Finetuned LLMs](https://arxiv.org/abs/2512.14751)
*Yixin Tan,Zhe Yu,Jun Sakuma*

Main category: cs.CR

TL;DR: 研究发现微调后的LLM继承了预训练模型的越狱漏洞，攻击者通过预训练模型优化的对抗提示可有效转移到微调模型，并提出基于表示层探测的PGP攻击方法增强转移成功率。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探究在预训练到微调的标准范式下，微调后的LLM是否继承了预训练模型的越狱漏洞，特别是在攻击者拥有预训练模型白盒访问权限而只有微调模型黑盒访问权限的现实威胁模型中。

Method: 方法包括：1) 在预训练模型上优化对抗提示并测试向微调模型的转移效果；2) 进行表示层探测分析，发现可转移提示在预训练隐藏状态中线性可分；3) 提出探针引导投影(PGP)攻击，将优化引导到与转移性相关的方向。

Result: 实证分析显示，在预训练模型上优化的对抗提示能最有效地转移到其微调变体，揭示了从预训练到微调LLM的漏洞继承。PGP攻击在多个LLM家族和多样化微调任务中表现出强大的转移成功率。

Conclusion: 结论是微调LLM确实继承了预训练模型的越狱漏洞，预训练到微调范式存在固有的安全风险，需要新的防御机制来缓解这种跨模型漏洞转移。

Abstract: Finetuning pretrained large language models (LLMs) has become the standard paradigm for developing downstream applications. However, its security implications remain unclear, particularly regarding whether finetuned LLMs inherit jailbreak vulnerabilities from their pretrained sources. We investigate this question in a realistic pretrain-to-finetune threat model, where the attacker has white-box access to the pretrained LLM and only black-box access to its finetuned derivatives. Empirical analysis shows that adversarial prompts optimized on the pretrained model transfer most effectively to its finetuned variants, revealing inherited vulnerabilities from pretrained to finetuned LLMs. To further examine this inheritance, we conduct representation-level probing, which shows that transferable prompts are linearly separable within the pretrained hidden states, suggesting that universal transferability is encoded in pretrained representations. Building on this insight, we propose the Probe-Guided Projection (PGP) attack, which steers optimization toward transferability-relevant directions. Experiments across multiple LLM families and diverse finetuned tasks confirm PGP's strong transfer success, underscoring the security risks inherent in the pretrain-to-finetune paradigm.

</details>


### [21] [CODE ACROSTIC: Robust Watermarking for Code Generation](https://arxiv.org/abs/2512.14753)
*Li Lin,Siyuan Xin,Yang Cao,Xiaochun Cao*

Main category: cs.CR

TL;DR: 该论文提出了一种新的代码水印方法，通过区分代码中的低熵和高熵部分来抵御注释移除攻击，相比现有方法具有更高的检测性和可用性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型生成的代码水印对于防止滥用（如伪造假新闻、抄袭和垃圾邮件）至关重要，特别是代码常包含知识产权。现有代码水印方法无法抵御注释移除攻击，攻击者只需移除注释而不影响功能，就能大幅降低水印效果。同时，代码通常呈现低熵场景，给水印注入带来挑战。

Method: 利用先验知识通过Cue List（线索列表）区分代码中的低熵和高熵部分，然后基于这个Cue List指导水印注入，从而提高检测性和可用性。

Result: 在HumanEval数据集上评估，并与三种最先进的代码水印技术进行比较，结果显示该方法在抵御注释移除攻击方面具有显著优势。

Conclusion: 提出的基于Cue List的代码水印方法能够有效抵御注释移除攻击，在保持代码功能的同时提高水印的鲁棒性和检测性，为LLM生成的代码保护提供了更可靠的解决方案。

Abstract: Watermarking large language models (LLMs) is vital for preventing their misuse, including the fabrication of fake news, plagiarism, and spam. It is especially important to watermark LLM-generated code, as it often contains intellectual property.However, we found that existing methods for watermarking LLM-generated code fail to address comment removal attack.In such cases, an attacker can simply remove the comments from the generated code without affecting its functionality, significantly reducing the effectiveness of current code-watermarking techniques.On the other hand, injecting a watermark into code is challenging because, as previous works have noted, most code represents a low-entropy scenario compared to natural language. Our approach to addressing this issue involves leveraging prior knowledge to distinguish between low-entropy and high-entropy parts of the code, as indicated by a Cue List of words.We then inject the watermark guided by this Cue List, achieving higher detectability and usability than existing methods.We evaluated our proposed method on HumanEvaland compared our method with three state-of-the-art code watermarking techniques. The results demonstrate the effectiveness of our approach.

</details>


### [22] [Privacy-Preserving Feature Valuation in Vertical Federated Learning Using Shapley-CMI and PSI Permutation](https://arxiv.org/abs/2512.14767)
*Unai Laskurain,Aitor Aguirre-Ortuzar,Urko Zurutuza*

Main category: cs.CR

TL;DR: 提出了一种用于纵向联邦学习的隐私保护Shapley-CMI特征贡献评估方法，通过私有集合交集服务器安全计算特征排列和交集大小，无需共享原始数据或训练模型。


<details>
  <summary>Details</summary>
Motivation: 在纵向联邦学习中，各参与方拥有相同用户的不同特征，需要在模型训练前评估各方的特征贡献。现有的Shapley-CMI方法虽然提出了理论框架，但缺乏安全实用的实现方案，无法在保护隐私的前提下计算所需的排列和交集。

Method: 设计了一个隐私保护系统，引入私有集合交集服务器，对离散化和加密的ID组进行所有必要的特征排列，计算加密的交集大小。各方使用这些交集结果计算Shapley-CMI值，评估自身特征的边际效用，整个过程无需交换原始数据。

Result: 初步实验证实了所提系统的正确性和隐私保护能力，证明了其在纵向联邦学习中安全高效特征贡献评估的可行性。系统确保数据机密性，支持多方扩展，实现了无需共享原始数据或训练模型的公平数据价值评估。

Conclusion: 该方法为纵向联邦学习提供了一种实用的隐私保护特征贡献评估方案，解决了Shapley-CMI理论框架缺乏安全实现的问题，为多方协作中的公平数据估值奠定了基础。

Abstract: Federated Learning (FL) is an emerging machine learning paradigm that enables multiple parties to collaboratively train models without sharing raw data, ensuring data privacy. In Vertical FL (VFL), where each party holds different features for the same users, a key challenge is to evaluate the feature contribution of each party before any model is trained, particularly in the early stages when no model exists. To address this, the Shapley-CMI method was recently proposed as a model-free, information-theoretic approach to feature valuation using Conditional Mutual Information (CMI). However, its original formulation did not provide a practical implementation capable of computing the required permutations and intersections securely. This paper presents a novel privacy-preserving implementation of Shapley-CMI for VFL. Our system introduces a private set intersection (PSI) server that performs all necessary feature permutations and computes encrypted intersection sizes across discretized and encrypted ID groups, without the need for raw data exchange. Each party then uses these intersection results to compute Shapley-CMI values, computing the marginal utility of their features. Initial experiments confirm the correctness and privacy of the proposed system, demonstrating its viability for secure and efficient feature contribution estimation in VFL. This approach ensures data confidentiality, scales across multiple parties, and enables fair data valuation without requiring the sharing of raw data or training models.

</details>


### [23] [Penetration Testing of Agentic AI: A Comparative Security Analysis Across Models and Frameworks](https://arxiv.org/abs/2512.14860)
*Viet K. Nguyen,Mohammad I. Husain*

Main category: cs.CR

TL;DR: 该研究首次对智能体AI系统进行了系统性渗透测试和比较评估，测试了5个主流模型在2个框架下的安全性，发现超过一半的恶意攻击成功，揭示了智能体AI的安全漏洞。


<details>
  <summary>Details</summary>
Motivation: 传统LLM安全防护无法应对智能体AI引入的安全漏洞，且缺乏对不同模型和框架的对比分析，需要进行系统性评估以揭示安全风险。

Method: 使用7个智能体架构模拟大学信息管理系统，在AutoGen和CrewAI两个框架下测试5个模型（Claude 3.5 Sonnet、Gemini 2.5 Flash、GPT-4o、Grok 2、Nova Pro），设计了13种攻击场景（提示注入、SSRF、SQL注入、工具滥用等），共130个测试用例。

Result: 整体拒绝率为41.5%，超过一半攻击成功；AutoGen拒绝率52.3%优于CrewAI的30.8%；模型表现从Nova Pro的46.2%到Claude和Grok 2的38.5%不等；Grok 2在CrewAI上仅拒绝2/13攻击（15.4%）；识别出6种防御行为模式，包括"幻觉合规"策略。

Conclusion: 智能体AI存在严重安全漏洞，企业级安全机制防护不足，需要针对性的安全部署建议，研究提供了可复现的攻击提示以促进安全改进。

Abstract: Agentic AI introduces security vulnerabilities that traditional LLM safeguards fail to address. Although recent work by Unit 42 at Palo Alto Networks demonstrated that ChatGPT-4o successfully executes attacks as an agent that it refuses in chat mode, there is no comparative analysis in multiple models and frameworks. We conducted the first systematic penetration testing and comparative evaluation of agentic AI systems, testing five prominent models (Claude 3.5 Sonnet, Gemini 2.5 Flash, GPT-4o, Grok 2, and Nova Pro) across two agentic AI frameworks (AutoGen and CrewAI) using a seven-agent architecture that mimics the functionality of a university information management system and 13 distinct attack scenarios that span prompt injection, Server Side Request Forgery (SSRF), SQL injection, and tool misuse. Our 130 total test cases reveal significant security disparities: AutoGen demonstrates a 52.3% refusal rate versus CrewAI's 30.8%, while model performance ranges from Nova Pro's 46.2% to Claude and Grok 2's 38.5%. Most critically, Grok 2 on CrewAI rejected only 2 of 13 attacks (15.4% refusal rate), and the overall refusal rate of 41.5% across all configurations indicates that more than half of malicious prompts succeeded despite enterprise-grade safety mechanisms. We identify six distinct defensive behavior patterns including a novel "hallucinated compliance" strategy where models fabricate outputs rather than executing or refusing attacks, and provide actionable recommendations for secure agent deployment. Complete attack prompts are also included in the Appendix to enable reproducibility.

</details>


### [24] [Cloud Security Leveraging AI: A Fusion-Based AISOC for Malware and Log Behaviour Detection](https://arxiv.org/abs/2512.14935)
*Nnamdi Philip Okonkwo,Lubna Luxmi Dhirani*

Main category: cs.CR

TL;DR: 在AWS上实现AI增强的云安全运营中心，结合云原生工具和机器学习检测，通过多模态威胁情报融合提升云SOC能力


<details>
  <summary>Details</summary>
Motivation: 云安全运营中心需要处理海量异构遥测数据，同时受限于严格预算，需要高效且成本敏感的安全解决方案

Method: 在AWS上部署AISOC架构，使用三个EC2实例（攻击者、防御者、监控），模拟反向shell入侵，通过Filebeat转发日志到Elasticsearch/Kibana，训练恶意软件检测器和日志异常检测器，校准并融合分数生成多模态威胁情报

Result: 在受控测试中，融合方法达到强macro-F1（最高1.00），但在更嘈杂和多样环境中性能会变化，表明简单校准融合能增强受限成本敏感设置中的云SOC能力

Conclusion: 简单校准融合能有效提升云安全运营中心在预算受限环境中的能力，为成本敏感设置提供可行的AI增强安全解决方案

Abstract: Cloud Security Operations Center (SOC) enable cloud governance, risk and compliance by providing insights visibility and control. Cloud SOC triages high-volume, heterogeneous telemetry from elastic, short-lived resources while staying within tight budgets. In this research, we implement an AI-Augmented Security Operations Center (AISOC) on AWS that combines cloud-native instrumentation with ML-based detection. The architecture uses three Amazon EC2 instances: Attacker, Defender, and Monitoring. We simulate a reverse-shell intrusion with Metasploit, and Filebeat forwards Defender logs to an Elasticsearch and Kibana stack for analysis. We train two classifiers, a malware detector built on a public dataset and a log-anomaly detector trained on synthetically augmented logs that include adversarial variants. We calibrate and fuse the scores to produce multi-modal threat intelligence and triage activity into NORMAL, SUSPICIOUS, and HIGH\_CONFIDENCE\_ATTACK. On held-out tests the fusion achieves strong macro-F1 (up to 1.00) under controlled conditions, though performance will vary in noisier and more diverse environments. These results indicate that simple, calibrated fusion can enhance cloud SOC capabilities in constrained, cost-sensitive setups.

</details>


### [25] [APT-ClaritySet: A Large-Scale, High-Fidelity Labeled Dataset for APT Malware with Alias Normalization and Graph-Based Deduplication](https://arxiv.org/abs/2512.15039)
*Zhenhao Yin,Hanbing Yan,Huishu Lu,Jing Xiong,Xiangyu Li,Rui Mei,Tianning Zang*

Main category: cs.CR

TL;DR: 该论文提出了APT-ClaritySet数据集及其构建流程，通过别名标准化和图特征去重技术，为APT研究提供了大规模、标准化的数据集，包含完整样本集、去重后唯一样本集和函数级资源。


<details>
  <summary>Details</summary>
Motivation: APT研究面临大规模标准化数据集稀缺、威胁行为体别名不一致以及样本冗余等问题，这些问题严重影响了研究的可重复性。

Method: 开发了APT-ClaritySet构建流程，包括：1）威胁行为体别名标准化（解决了约11.22%的不一致名称）；2）图特征去重技术，将静态可分析的可执行文件子集减少了47.55%，同时保留了行为上不同的变体。

Result: 构建了三个数据集组件：APT-ClaritySet-Full（去重前完整集合，包含34,363个恶意软件样本，涉及305个APT组织）；APT-ClaritySet-Unique（去重后发布，包含25,923个唯一样本，涉及303个组织）；APT-ClaritySet-FuncReuse（函数级资源，包含324,538个函数重用簇）。

Conclusion: 通过发布这些组件并详细描述别名标准化和可扩展的去重流程，该工作为APT模式、演化和归因的定量研究提供了高保真、可重复的基础。

Abstract: Large-scale, standardized datasets for Advanced Persistent Threat (APT) research are scarce, and inconsistent actor aliases and redundant samples hinder reproducibility. This paper presents APT-ClaritySet and its construction pipeline that normalizes threat actor aliases (reconciling approximately 11.22\% of inconsistent names) and applies graph-feature deduplication -- reducing the subset of statically analyzable executables by 47.55\% while retaining behaviorally distinct variants. APT-ClaritySet comprises: (i) APT-ClaritySet-Full, the complete pre-deduplication collection with 34{,}363 malware samples attributed to 305 APT groups (2006 - early 2025); (ii) APT-ClaritySet-Unique, the deduplicated release with 25{,}923 unique samples spanning 303 groups and standardized attributions; and (iii) APT-ClaritySet-FuncReuse, a function-level resource that includes 324{,}538 function-reuse clusters (FRCs) enabling measurement of inter-/intra-group sharing, evolution, and tooling lineage. By releasing these components and detailing the alias normalization and scalable deduplication pipeline, this work provides a high-fidelity, reproducible foundation for quantitative studies of APT patterns, evolution, and attribution.

</details>


### [26] [Quantifying Return on Security Controls in LLM Systems](https://arxiv.org/abs/2512.15081)
*Richard Helder Moulton,Austin O'Brien,John D. Hastings*

Main category: cs.CR

TL;DR: 本文提出了一个决策导向框架，用于量化LLM安全风险，通过蒙特卡洛模拟将攻击成功率转换为财务风险估计，并比较不同防御措施的投资回报率。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在安全关键工作流中应用日益广泛，但缺乏量化指导来评估哪些安全防护措施值得部署。需要一种系统方法来量化残余风险，并将技术安全指标转化为财务决策依据。

Method: 建立基于DeepSeek-R1的RAG系统，包含合成PII数据。使用Garak工具对五类漏洞进行自动化攻击：PII泄露、潜在上下文注入、提示注入、对抗攻击生成和分歧。采用拉普拉斯成功法则估计攻击成功率，结合公开泄露成本数据校准的损失三角形分布，进行10,000次蒙特卡洛模拟生成损失超越曲线和期望损失。比较三种防护措施：基于属性的访问控制(ABAC)、微软Presidio的命名实体识别(NER)脱敏、NeMo Guardrails。

Result: 基线系统攻击成功率极高（PII、潜在注入和提示注入≥0.98），每个攻击场景的模拟期望损失达31.3万美元。ABAC将PII和提示相关攻击成功率降至接近零，总期望损失减少约94%，投资回报率(ROC)达9.83。NER脱敏同样消除PII泄露，ROC为5.97。NeMo Guardrails仅提供边际效益，ROC仅0.05。

Conclusion: 该框架为LLM安全防护提供了量化决策支持，证明ABAC和NER脱敏是有效的防护措施，而NeMo Guardrails在当前配置下效果有限。研究为组织部署LLM安全控制提供了基于财务风险的数据驱动方法。

Abstract: Although large language models (LLMs) are increasingly used in security-critical workflows, practitioners lack quantitative guidance on which safeguards are worth deploying. This paper introduces a decision-oriented framework and reproducible methodology that together quantify residual risk, convert adversarial probe outcomes into financial risk estimates and return-on-control (RoC) metrics, and enable monetary comparison of layered defenses for LLM-based systems. A retrieval-augmented generation (RAG) service is instantiated using the DeepSeek-R1 model over a corpus containing synthetic personally identifiable information (PII), and subjected to automated attacks with Garak across five vulnerability classes: PII leakage, latent context injection, prompt injection, adversarial attack generation, and divergence. For each (vulnerability, control) pair, attack success probabilities are estimated via Laplace's Rule of Succession and combined with loss triangle distributions, calibrated from public breach-cost data, in 10,000-run Monte Carlo simulations to produce loss exceedance curves and expected losses. Three widely used mitigations, attribute-based access control (ABAC); named entity recognition (NER) redaction using Microsoft Presidio; and NeMo Guardrails, are then compared to a baseline RAG configuration. The baseline system exhibits very high attack success rates (>= 0.98 for PII, latent injection, and prompt injection), yielding a total simulated expected loss of $313k per attack scenario. ABAC collapses success probabilities for PII and prompt-related attacks to near zero and reduces the total expected loss by ~94%, achieving an RoC of 9.83. NER redaction likewise eliminates PII leakage and attains an RoC of 5.97, while NeMo Guardrails provides only marginal benefit (RoC of 0.05).

</details>


### [27] [No More Hidden Pitfalls? Exposing Smart Contract Bad Practices with LLM-Powered Hybrid Analysis](https://arxiv.org/abs/2512.15179)
*Xiaoqi Li,Zongwei Li,Wenkai Li,Yuqing Zhang,Xin Wang*

Main category: cs.CR

TL;DR: SCALM：首个系统研究智能合约不良实践的框架，通过LLM驱动结合上下文感知函数级切片和知识增强语义推理，能检测47+种不良实践，性能优于现有工具。


<details>
  <summary>Details</summary>
Motivation: 随着以太坊平台的成熟和广泛应用，需要维持高标准的智能合约编写实践。不良实践虽不直接导致安全问题，但会增加问题风险，因此需要系统研究来理解和避免这些不良实践。

Method: 提出SCALM框架，包含两个方法创新：1）混合架构结合上下文感知函数级切片和通过可扩展向量化模式匹配的知识增强语义推理；2）多层推理验证系统，通过语法、设计模式和架构分析将低级代码模式与高级安全原则连接。

Result: 使用多个LLM和数据集的广泛实验表明，SCALM在检测智能合约不良实践方面优于现有工具。

Conclusion: SCALM是首个系统研究智能合约不良实践的框架，通过创新的混合架构和多层推理系统，能有效检测47种以上不良实践，为智能合约质量保障提供了有效工具。

Abstract: As the Ethereum platform continues to mature and gain widespread usage, it is crucial to maintain high standards of smart contract writing practices. While bad practices in smart contracts may not directly lead to security issues, they elevate the risk of encountering problems. Therefore, to understand and avoid these bad practices, this paper introduces the first systematic study of bad practices in smart contracts, delving into over 47 specific issues. Specifically, we propose SCALM, an LLM-powered framework featuring two methodological innovations: (1) A hybrid architecture that combines context-aware function-level slicing with knowledge-enhanced semantic reasoning via extensible vectorized pattern matching. (2) A multi-layer reasoning verification system connects low-level code patterns with high-level security principles through syntax, design patterns, and architecture analysis. Our extensive experiments using multiple LLMs and datasets have shown that SCALM outperforms existing tools in detecting bad practices in smart contracts.

</details>


### [28] [Talking to the Airgap: Exploiting Radio-Less Embedded Devices as Radio Receivers](https://arxiv.org/abs/2512.15387)
*Paul Staat,Daniel Davidovich,Christof Paar*

Main category: cs.CR

TL;DR: 研究人员发现未修改的嵌入式设备可作为无线电接收器，通过PCB走线和ADC的寄生射频敏感性实现无线数据接收，为隔离系统创建了新的命令与控制通道。


<details>
  <summary>Details</summary>
Motivation: 隔离系统通常被认为可以防止远程攻击，但现有研究主要关注数据外泄。本研究旨在探索攻击者是否能够反向操作，通过无线方式向隔离系统渗透数据，而无需硬件修改。

Method: 提出系统性方法识别设备配置中的射频敏感性，在12款商用嵌入式设备和2个定制原型上测试300-1000MHz范围内的接收能力，评估非视距条件下的数据接收可行性。

Result: 实验显示设备可检测低至1mW的信号功率，在数十米距离内成功接收数据（包括非视距条件），最高数据速率可达100kbps，证明未修改设备可作为有效无线电接收器。

Conclusion: 嵌入式设备的寄生射频敏感性为隔离系统创建了先前未知的命令与控制通道，挑战了隔离系统固有安全性的假设，揭示了新的安全威胁。

Abstract: Intelligent electronics are deeply embedded in critical infrastructures and must remain reliable, particularly against deliberate attacks. To minimize risks and impede remote compromise, sensitive systems can be physically isolated from external networks, forming an airgap. Yet, airgaps can still be infiltrated by capable adversaries gaining code execution. Prior research has shown that attackers can then attempt to wirelessly exfiltrate data across the airgap by exploiting unintended radio emissions. In this work, we demonstrate reversal of this link: malicious code execution on embedded devices can enable wireless infiltration of airgapped systems without any hardware modification. In contrast to previous infiltration methods that depend on dedicated sensors (e.g., microphones, LEDs, or temperature sensors) or require strict line-of-sight, we show that unmodified, sensor-less embedded devices can inadvertently act as radio receivers. This phenomenon stems from parasitic RF sensitivity in PCB traces and on-chip analog-to-digital converters (ADCs), allowing external transmissions to be received and decoded entirely in software.
  Across twelve commercially available embedded devices and two custom prototypes, we observe repeatable reception in the 300-1000 MHz range, with detectable signal power as low as 1 mW. To this end, we propose a systematic methodology to identify device configurations that foster such radio sensitivities and comprehensively evaluate their feasibility for wireless data reception. Exploiting these sensitivities, we demonstrate successful data reception over tens of meters, even in non-line-of-sight conditions and show that the reception sensitivities accommodate data rates of up to 100 kbps. Our findings reveal a previously unexplored command-and-control vector for air-gapped systems while challenging assumptions about their inherent isolation. [shortened]

</details>


### [29] [Attention in Motion: Secure Platooning via Transformer-based Misbehavior Detection](https://arxiv.org/abs/2512.15503)
*Konstantinos Kalogiannis,Ahmed Mohamed Hussain,Hexu Li,Panos Papadimitratos*

Main category: cs.CR

TL;DR: AIMformer：基于Transformer的车辆编队实时异常行为检测框架，通过多头自注意力机制捕获时空依赖，部署在边缘设备上实现亚毫秒级延迟


<details>
  <summary>Details</summary>
Motivation: 车辆编队通过V2X通信实现多车协同，但分布式协调存在安全隐患，攻击者可注入虚假运动数据威胁安全。传统异常检测方法误报率高，无法捕捉多车协调的复杂时空依赖关系。

Method: 提出AIMformer框架：1）使用多头自注意力机制同时捕获车辆内部时间动态和车辆间空间相关性；2）采用全局位置编码和车辆特定时间偏移处理加入/退出操作；3）设计聚焦精度的BCE损失函数惩罚误报；4）支持边缘部署，使用TensorFlow Lite、ONNX和TensorRT优化。

Result: 在4种编队控制器、多种攻击向量和不同移动场景下的评估显示，AIMformer性能优于现有基线架构（≥0.93）。部署分析显示亚毫秒级推理延迟，适合资源受限的边缘平台实时操作。

Conclusion: AIMformer验证了在车辆内部和路边基础设施部署的可行性，为安全关键的车辆系统提供了有效的实时异常行为检测解决方案。

Abstract: Vehicular platooning promises transformative improvements in transportation efficiency and safety through the coordination of multi-vehicle formations enabled by Vehicle-to-Everything (V2X) communication. However, the distributed nature of platoon coordination creates security vulnerabilities, allowing authenticated vehicles to inject falsified kinematic data, compromise operational stability, and pose a threat to passenger safety. Traditional misbehaviour detection approaches, which rely on plausibility checks and statistical methods, suffer from high False Positive (FP) rates and cannot capture the complex temporal dependencies inherent in multi-vehicle coordination dynamics. We present Attention In Motion (AIMformer), a transformer-based framework specifically tailored for real-time misbehaviour detection in vehicular platoons with edge deployment capabilities. AIMformer leverages multi-head self-attention mechanisms to simultaneously capture intra-vehicle temporal dynamics and inter-vehicle spatial correlations. It incorporates global positional encoding with vehicle-specific temporal offsets to handle join/exit maneuvers. We propose a Precision-Focused (BCE) loss function that penalizes FPs to meet the requirements of safety-critical vehicular systems. Extensive evaluation across 4 platoon controllers, multiple attack vectors, and diverse mobility scenarios demonstrates superior performance ($\geq$ 0.93) compared to state-of-the-art baseline architectures. A comprehensive deployment analysis utilizing TensorFlow Lite (TFLite), Open Neural Network Exchange (ONNX), and TensorRT achieves sub-millisecond inference latency, making it suitable for real-time operation on resource-constrained edge platforms. Hence, validating AIMformer is viable for both in-vehicle and roadside infrastructure deployment.

</details>


### [30] [ComMark: Covert and Robust Black-Box Model Watermarking with Compressed Samples](https://arxiv.org/abs/2512.15641)
*Yunfei Yang,Xiaojun Chen,Zhendong Zhao,Yu Zhou,Xiaoyan Gu,Juan Cao*

Main category: cs.CR

TL;DR: ComMark：基于频域变换的黑盒模型水印框架，通过过滤高频信息生成压缩、隐蔽且抗攻击的水印样本，在隐蔽性和鲁棒性方面达到SOTA性能


<details>
  <summary>Details</summary>
Motivation: 深度学习模型因其依赖海量数据和昂贵训练过程而成为高价值资产，但面临泄露和窃取风险，需要有效的知识产权保护。现有黑盒水印方法难以平衡隐蔽性（防止检测和伪造）和鲁棒性（抵抗移除攻击）这两个实际版权验证的关键属性。

Method: 提出ComMark黑盒模型水印框架：1）利用频域变换过滤高频信息生成压缩、隐蔽且抗攻击的水印样本；2）训练过程中结合模拟攻击场景和相似性损失以增强水印鲁棒性

Result: 在多种数据集和架构上的综合评估表明，ComMark在隐蔽性和鲁棒性方面均达到最先进的性能。框架可扩展应用于语音识别、情感分析、图像生成、图像描述和视频识别等任务，展示了其多功能性和广泛适用性。

Conclusion: ComMark通过频域变换方法有效解决了黑盒模型水印中隐蔽性与鲁棒性的平衡问题，为深度学习模型的知识产权保护提供了实用且高效的解决方案，具有广泛的跨任务应用潜力。

Abstract: The rapid advancement of deep learning has turned models into highly valuable assets due to their reliance on massive data and costly training processes. However, these models are increasingly vulnerable to leakage and theft, highlighting the critical need for robust intellectual property protection. Model watermarking has emerged as an effective solution, with black-box watermarking gaining significant attention for its practicality and flexibility. Nonetheless, existing black-box methods often fail to better balance covertness (hiding the watermark to prevent detection and forgery) and robustness (ensuring the watermark resists removal)-two essential properties for real-world copyright verification. In this paper, we propose ComMark, a novel black-box model watermarking framework that leverages frequency-domain transformations to generate compressed, covert, and attack-resistant watermark samples by filtering out high-frequency information. To further enhance watermark robustness, our method incorporates simulated attack scenarios and a similarity loss during training. Comprehensive evaluations across diverse datasets and architectures demonstrate that ComMark achieves state-of-the-art performance in both covertness and robustness. Furthermore, we extend its applicability beyond image recognition to tasks including speech recognition, sentiment analysis, image generation, image captioning, and video recognition, underscoring its versatility and broad applicability.

</details>


### [31] [Distributed HDMM: Scalable, Distributed, Accurate, and Differentially Private Query Workloads without a Trusted Curator](https://arxiv.org/abs/2512.15648)
*Ratang Sedimo,Ivoline C. Ngong,Jami Lashua,Joseph P. Near*

Main category: cs.CR

TL;DR: 分布式高维矩阵机制（Distributed HDMM）是一种无需可信第三方即可在分布式数据上回答线性查询工作负载的协议，提供与中心化HDMM相当的准确性，支持恶意聚合器和恶意客户端场景。


<details>
  <summary>Details</summary>
Motivation: 传统的高维矩阵机制（HDMM）需要可信的数据收集者（curator），这在分布式环境中存在隐私和安全风险。研究者希望设计一种能够在分布式数据上运行HDMM的协议，既保持HDMM的准确性优势，又无需依赖可信第三方。

Method: Distributed HDMM利用安全聚合协议在分布式数据上评估HDMM，支持恶意聚合器和恶意客户端场景（假设诚实多数）。该方法通过安全计算技术实现分布式环境下的隐私保护查询。

Result: 初步实证评估显示，Distributed HDMM能够在现实数据集和工作负载上运行，支持数千个客户端，运行时间少于1分钟，证明了其在实际应用中的可行性。

Conclusion: Distributed HDMM成功地将HDMM扩展到分布式环境，在保持准确性的同时提供了更强的安全保证，为分布式数据上的隐私保护查询提供了实用解决方案。

Abstract: We present the Distributed High-Dimensional Matrix Mechanism (Distributed HDMM), a protocol for answering workloads of linear queries on distributed data that provides the accuracy of central-model HDMM without a trusted curator. Distributed HDMM leverages a secure aggregation protocol to evaluate HDMM on distributed data, and is secure in the context of a malicious aggregator and malicious clients (assuming an honest majority). Our preliminary empirical evaluation shows that Distributed HDMM can run on realistic datasets and workloads with thousands of clients in less than one minute.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [32] [How Deep Does Your Dependency Tree Go? An Empirical Study of Dependency Amplification Across 10 Package Ecosystems](https://arxiv.org/abs/2512.14739)
*Jahidul Arafat*

Main category: cs.SE

TL;DR: 对10个主流软件包生态系统的依赖放大现象进行大规模实证研究，发现Maven的依赖放大倍数最高（平均24.70倍），而CocoaPods最低（0.32倍），挑战了npm依赖放大最高的传统认知。


<details>
  <summary>Details</summary>
Motivation: 现代软件开发依赖包生态系统，单个声明的依赖可能引入许多传递依赖。这种依赖放大现象对软件供应链安全有重大影响，但不同生态系统间的放大模式尚未进行大规模比较研究。

Method: 对10个主要生态系统（Maven、npm、crates.io、PyPI、NuGet、RubyGems、Go Modules、Packagist、CocoaPods、Pub）中的500个项目进行实证研究，分析传递依赖与直接依赖的比率（依赖放大倍数）。

Result: Maven平均放大倍数为24.70倍，显著高于其他生态系统；Go Modules为4.48倍，npm为4.32倍，CocoaPods最低为0.32倍。45对比较中有22对显示出显著差异。28%的Maven项目超过10倍放大，而Cargo、PyPI等生态系统为零。

Conclusion: 依赖放大差异主要源于生态系统设计选择，如依赖解析行为、标准库完整性和平台约束。建议采用生态系统特定的安全策略：对Maven进行系统审计，对npm和RubyGems进行针对性异常检测，对放大受控的生态系统维持现有实践。

Abstract: Modern software development relies on package ecosystems where a single declared dependency can pull in many additional transitive packages. This dependency amplification, defined as the ratio of transitive to direct dependencies, has major implications for software supply chain security, yet amplification patterns across ecosystems have not been compared at scale. We present an empirical study of 500 projects across ten major ecosystems, including Maven Central for Java, npm Registry for JavaScript, crates io for Rust, PyPI for Python, NuGet Gallery for dot NET, RubyGems for Ruby, Go Modules for Go, Packagist for PHP, CocoaPods for Swift and Objective C, and Pub for Dart. Our analysis shows that Maven exhibits mean amplification of 24.70 times, compared to 4.48 times for Go Modules, 4.32 times for npm, and 0.32 times for CocoaPods. We find significant differences with large effect sizes in 22 of 45 pairwise comparisons, challenging the assumption that npm has the highest amplification due to its many small purpose packages. We observe that 28 percent of Maven projects exceed 10 times amplification, indicating a systematic pattern rather than isolated outliers, compared to 14 percent for RubyGems, 12 percent for npm, and zero percent for Cargo, PyPI, Packagist, CocoaPods, and Pub. We attribute these differences to ecosystem design choices such as dependency resolution behavior, standard library completeness, and platform constraints. Our findings suggest adopting ecosystem specific security strategies, including systematic auditing for Maven environments, targeted outlier detection for npm and RubyGems, and continuation of current practices for ecosystems with controlled amplification. We provide a full replication package with data and analysis scripts.

</details>


### [33] [VDMN: A Graphical Notation for Modelling Value Driver Trees](https://arxiv.org/abs/2512.14740)
*Benjamin Matthies*

Main category: cs.SE

TL;DR: 该研究提出了价值驱动建模符号（VDMN），这是一种用于系统化指导价值驱动树建模的图形化符号，填补了该领域缺乏系统建模指南的空白。


<details>
  <summary>Details</summary>
Motivation: 价值驱动树（VDTs）是用于说明和分析关键绩效指标与业务结果之间因果关系的概念模型，但在实际应用中缺乏系统化的建模指南。为了填补这一空白，本研究旨在开发一种标准化的建模方法。

Method: 研究引入了价值驱动建模符号（VDMN），这是一种包含完整语义结构和直观图形语法的图形化符号。通过两个案例研究应用该符号，并通过专家访谈评估其实用性。

Result: 研究结果显示，VDMN支持一致且易于理解的价值驱动树建模。专家评估表明该符号在实际应用中具有实用价值。

Conclusion: VDMN代表了价值驱动树建模系统化和标准化的重要一步，为管理者决策和价值管理提供了更系统化的工具支持。

Abstract: Value Driver Trees (VDTs) are conceptual models used to illustrate and analyse the causal relationships between key performance indicators and business outcomes, thereby supporting managerial decision-making and value-based management. Despite their increasing application, there are still no systematic guidelines for the modelling of such conceptual models. To fill this gap, this study introduces the Value Driver Modelling Notation (VDMN), a graphical notation developed to systematically guide VDT modelling. This notation includes a comprehensive set of semantic constructs and an intuitive graphical syntax. To evaluate its practical utility, the VDMN was applied in two case studies and assessed through expert interviews. The results show that the notation supports a consistent and comprehensible modelling of VDTs. The VDMN thus represents a significant step towards the systematisation and standardisation of VDT modelling.

</details>


### [34] [Revisiting the Reliability of Language Models in Instruction-Following](https://arxiv.org/abs/2512.14754)
*Jianshuo Dong,Yutong Zhang,Yan Liu,Zhenyu Zhong,Tao Wei,Chao Zhang,Han Qiu*

Main category: cs.SE

TL;DR: 论文提出了"细微差别导向的可靠性"概念，发现当前LLMs在表达相同意图但存在细微差别的提示词上表现不稳定，性能下降可达61.8%，并提出了新的评估指标和基准。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs在标准基准测试（如IFEval）上表现优异，但在真实使用场景中，用户表达方式、上下文框架和任务表述的细微变化会导致模型表现不一致，这影响了LLM服务的可靠性。

Method: 提出新指标reliable@k，开发自动化流水线通过数据增强生成高质量的"cousin prompts"（表达相同意图但存在细微差别的提示词），构建IFEval++基准进行系统评估。

Result: 评估了20个专有和26个开源LLMs，发现当前模型在细微差别导向的可靠性方面存在严重不足，性能下降最高可达61.8%。论文还分析了这一现象的特征并探索了三种改进方案。

Conclusion: 细微差别导向的可靠性是LLM走向更可靠、更可信行为的关键但尚未充分探索的下一步，IFEval++基准和评估方法为此提供了系统化的研究工具。

Abstract: Advanced LLMs have achieved near-ceiling instruction-following accuracy on benchmarks such as IFEval. However, these impressive scores do not necessarily translate to reliable services in real-world use, where users often vary their phrasing, contextual framing, and task formulations. In this paper, we study nuance-oriented reliability: whether models exhibit consistent competence across cousin prompts that convey analogous user intents but with subtle nuances. To quantify this, we introduce a new metric, reliable@k, and develop an automated pipeline that generates high-quality cousin prompts via data augmentation. Building upon this, we construct IFEval++ for systematic evaluation. Across 20 proprietary and 26 open-source LLMs, we find that current models exhibit substantial insufficiency in nuance-oriented reliability -- their performance can drop by up to 61.8% with nuanced prompt modifications. What's more, we characterize it and explore three potential improvement recipes. Our findings highlight nuance-oriented reliability as a crucial yet underexplored next step toward more dependable and trustworthy LLM behavior. Our code and benchmark are accessible: https://github.com/jianshuod/IFEval-pp.

</details>


### [35] [Examining Software Developers' Needs for Privacy Enforcing Techniques: A survey](https://arxiv.org/abs/2512.14756)
*Ioanna Theophilou,Georgia M. Kapitsaki*

Main category: cs.SE

TL;DR: 调查显示开发者需要更多自动化工具来满足GDPR、CCPA等数据隐私法规的合规要求，隐私经验越丰富的开发者对隐私工具的需求越强烈。


<details>
  <summary>Details</summary>
Motivation: 数据隐私法规（如GDPR、CCPA/CPRA）要求所有软件系统都必须合规，但开发者需要法律知识才能实现合规功能。现有研究主要关注开发者对隐私原则的理解，但尚未研究开发者对隐私合规工具的具体需求，这对开发自动化工具（如生成式AI）的设计至关重要。

Method: 通过问卷调查68名开发者，研究他们在隐私法规合规方面的需求，并分析影响这些需求的因素。

Result: 大多数开发者表示需要更多自动化工具来协助隐私合规，且隐私经验越丰富的开发者对隐私工具的需求和关注度越高。

Conclusion: 研究结果有助于开发者更好地定位隐私法规合规工作，并指出迫切需要开发隐私辅助工具来简化合规流程。

Abstract: Data privacy legislation, such as GDPR and CCPA/CPRA, has rendered data privacy law compliance a requirement of all software systems. Developers need to implement various kinds of functionalities to cover law needs, including user rights and law principles. As data compliance is tightly coupled with legal knowledge, it is not always easy to perform such integrations in software systems. Prior studies have focused on developers' understanding of privacy principles, such as Privacy by Design, and have examined privacy techniques used in the software industry. Nevertheless, emerging developer needs that can assist in privacy law compliance have not been examined but are useful in understanding what development automation tools, such as Generative AI, need to cover to make the compliance process more straightforward and seamless within the development process. In this work, we present a survey that examines the above needs with the participation of 68 developers, while we have examined which factors affect practitioners' needs. Most developers express a need for more automated tools, while privacy experience increases practitioners' concerns for privacy tools. Our results can assist practitioners in better positioning their development activities within privacy law compliance and point to an urgent need for privacy facilitators.

</details>


### [36] [CAPE: Capability Achievement via Policy Execution](https://arxiv.org/abs/2512.14761)
*David Ball*

Main category: cs.SE

TL;DR: 论文提出能力工程（Capability Engineering）和CAPE协议，通过将需求转化为可执行规范并训练模型来满足这些规范，解决了AI系统缺乏表达和执行需求能力的问题，在六个领域的109,500个示例中将违规率降低了81%。


<details>
  <summary>Details</summary>
Motivation: 现代AI系统缺乏表达和执行需求的能力。预训练产生智能，后训练优化偏好，但都不能保证模型可靠地满足明确的、上下文相关的约束。这种缺失的抽象解释了为什么高智能模型在部署中经常失败，尽管在基准测试中表现良好。

Method: 引入能力工程（Capability Engineering）作为系统实践，通过CAPE协议（能力实现通过策略执行）实现Specify -> Verify -> Correct -> Train循环。该方法基于两个实证发现：上下文客观性（属性在上下文固定后从主观变为客观）和验证-保真度缩放（验证准确性随模型规模提高）。

Result: 在六个领域的109,500个示例中，CAPE相对于DPO将违规率降低了81%（标准差小于0.3%）。通过用可重用规范替代逐示例标注，CAPE将成本降低了5到20倍，并将时间线从数月缩短到数周。

Conclusion: 论文提出了CAPE协议、PredicateGraph模式、CPL规范语言和策略包，并发布了CapabilityBench作为公共注册表，将评估从智能基准转向能力测量，为AI系统提供了表达和执行需求的新方法。

Abstract: Modern AI systems lack a way to express and enforce requirements. Pre-training produces intelligence, and post-training optimizes preferences, but neither guarantees that models reliably satisfy explicit, context-dependent constraints. This missing abstraction explains why highly intelligent models routinely fail in deployment despite strong benchmark performance.
  We introduce Capability Engineering, the systematic practice of converting requirements into executable specifications and training models to satisfy them by default. We operationalize this practice through CAPE (Capability Achievement via Policy Execution), a protocol implementing a Specify -> Verify -> Correct -> Train loop.
  CAPE is grounded in two empirical findings: (1) contextual objectivity, where properties appearing subjective become objective once context is fixed (inter-annotator agreement rises from kappa = 0.42 to kappa = 0.98), and (2) verification-fidelity scaling, where verification accuracy improves with model scale (r = 0.94), unlike preference agreement which plateaus at 30 to 50 percent disagreement regardless of compute. Across 109,500 examples in six domains, CAPE reduces violation rates by 81 percent relative to DPO (standard deviation less than 0.3 percent). By replacing per-example annotation with reusable specifications, CAPE reduces costs by 5 to 20 times and shortens timelines from months to weeks.
  We release the CAPE protocol, PredicateGraph schema, CPL specification language, and policy packs under Apache 2.0. We also launch CapabilityBench, a public registry of model evaluations against community-contributed policies, shifting evaluation from intelligence benchmarks toward capability measurement.

</details>


### [37] [Let the Barbarians In: How AI Can Accelerate Systems Performance Research](https://arxiv.org/abs/2512.14806)
*Audrey Cheng,Shu Liu,Melissa Pan,Zhifei Li,Shubham Agarwal,Mert Cemri,Bowen Wang,Alexander Krentsel,Tian Xia,Jongseok Park,Shuo Yang,Jeff Chen,Lakshya Agrawal,Ashwin Naren,Shulu Li,Ruiying Ma,Aditya Desai,Jiarong Xing,Koushik Sen,Matei Zaharia,Ion Stoica*

Main category: cs.SE

TL;DR: AI驱动的系统研究（ADRS）通过AI自动生成、评估和优化系统解决方案，在多个案例中达到或超越人工设计水平，为系统研究提供了新的自动化范式。


<details>
  <summary>Details</summary>
Motivation: AI正在改变研究过程，通过自动化发现新解决方案。系统性能研究特别适合这种范式，因为系统性能问题天然具备验证器（可以在真实系统或模拟器中实现和评估）。

Method: 提出AI驱动的系统研究（ADRS）范式，包括生成、评估和优化的迭代循环。使用多个开源ADRS实例（OpenEvolve、GEPA、ShinkaEvolve）在十个案例研究中进行验证。

Result: ADRS生成的解决方案能够匹配甚至超越人类设计的最先进方案。基于这些发现，提出了有效使用ADRS的最佳实践（如提示规范程度、反馈量、鲁棒评估）。

Conclusion: 虽然还没有适用于所有系统研究的通用ADRS方法，但初步发现和识别出的挑战为未来工作提供了有意义的指导，研究者的努力将越来越多地转向问题制定和战略监督。

Abstract: Artificial Intelligence (AI) is beginning to transform the research process by automating the discovery of new solutions. This shift depends on the availability of reliable verifiers, which AI-driven approaches require to validate candidate solutions. Research focused on improving systems performance is especially well-suited to this paradigm because system performance problems naturally admit such verifiers: candidates can be implemented in real systems or simulators and evaluated against predefined workloads. We term this iterative cycle of generation, evaluation, and refinement AI-Driven Research for Systems (ADRS). Using several open-source ADRS instances (i.e., OpenEvolve, GEPA, and ShinkaEvolve), we demonstrate across ten case studies (e.g., multi-region cloud scheduling, mixture-of-experts load balancing, LLM-based SQL, transaction scheduling) that ADRS-generated solutions can match or even outperform human state-of-the-art designs. Based on these findings, we outline best practices (e.g., level of prompt specification, amount of feedback, robust evaluation) for effectively using ADRS, and we discuss future research directions and their implications. Although we do not yet have a universal recipe for applying ADRS across all of systems research, we hope our preliminary findings, together with the challenges we identify, offer meaningful guidance for future work as researcher effort shifts increasingly toward problem formulation and strategic oversight.
  Note: This paper is an extension of our prior work [14]. It adds extensive evaluation across multiple ADRS frameworks and provides deeper analysis and insights into best practices.

</details>


### [38] [Evaluating Code Reasoning Abilities of Large Language Models Under Real-World Settings](https://arxiv.org/abs/2512.14917)
*Changshu Liu,Alireza Ghazanfari,Yang Chen,Reyhaneh Jabbarvand*

Main category: cs.SE

TL;DR: RE2-Bench是一个包含1,101个代码推理问题的基准测试，其中195个来自成熟的实际项目，旨在更真实地评估大语言模型的代码推理能力，超越现有基准的简单程序限制。


<details>
  <summary>Details</summary>
Motivation: 现有代码推理基准测试主要使用简单程序，无法反映真实世界的复杂性（如过程间/内依赖、API调用、嵌套结构、复杂类型等），导致对大语言模型实际泛化能力的评估存在偏差。

Method: 通过静态和动态程序分析自动序列化和反序列化实际代码中的复合、复杂和自定义类型；使用九种可解释的代码复杂度指标，通过多数投票机制将问题分类为简单或困难两个难度级别。

Result: 对六个通用和面向推理的大语言模型进行评估，发现在输入预测和输出预测任务中，从简单到困难问题的性能显著下降（分别为51.50%和42.15%），证实先前评估高估了大语言模型的推理能力。

Conclusion: RE2-Bench提供了一个更真实的代码推理评估基准，揭示了现有基准测试的局限性，表明大语言模型在复杂实际代码推理方面的能力被严重高估。

Abstract: Code reasoning tasks are becoming prevalent in large language model (LLM) assessments. Existing benchmarks involve simple programs, failing to represent real-world complexities such as inter- or intra-procedural dependencies, core or third-party API calls, highly nested constructs, and non-primitive complex types. Evaluating LLMs under such a simplistic setting poses a significant threat to assumptions about their generalizability in practice. To enable a more realistic evaluation of code reasoning, this paper proposes RE2-Bench, a benchmark of 1,101 reasoning problems, including 195 drawn from mature real-world projects. RE2-Bench leverages static and dynamic program analysis to automatically serialize and deserialize compound, complex, and custom types in real-world code, going far beyond the primitive-only settings used in prior work.
  A key feature of RE2-Bench is categorizing each reasoning problem as Easy or Hard via a principled majority-vote mechanism over nine interpretable code complexity metrics, resulting in two well-separated and semantically meaningful difficulty categories suitable for precise calibration of LLM reasoning ability. A comprehensive evaluation of six general-purpose and reasoning-oriented LLMs on two widely used code reasoning tasks -- input prediction and output prediction -- using RE2-Bench reveals a significant performance drop from Easy to Hard problems (51.50\% for input prediction and 42.15\% for output prediction), confirming that prior evaluations substantially overestimate the reasoning capabilities of LLMs.

</details>


### [39] [Imitation Game: Reproducing Deep Learning Bugs Leveraging an Intelligent Agent](https://arxiv.org/abs/2512.14990)
*Mehil B Shah,Mohammad Masudur Rahman,Foutse Khomh*

Main category: cs.SE

TL;DR: RepGen是一个自动化智能方法，用于复现深度学习bug，通过构建学习增强上下文、制定复现计划、采用迭代生成-验证-精炼机制，使用LLM生成复现代码，在106个真实bug上达到80.19%的复现率。


<details>
  <summary>Details</summary>
Motivation: 深度学习应用存在大量bug、故障和漏洞，但由于DL模型的固有非确定性和与软硬件环境的紧密耦合，复现这些bug极具挑战性。研究表明，只有约3%的DL bug可以通过手动方法可靠复现。

Method: RepGen方法包括：1) 从项目中构建学习增强上下文；2) 制定全面的bug复现计划；3) 采用迭代的生成-验证-精炼机制；4) 使用LLM生成能够复现bug的代码。

Result: 在106个真实世界深度学习bug上评估，RepGen达到80.19%的复现率，比现有最佳方法提高19.81%。开发者研究显示，RepGen将DL bug复现成功率提高23.35%，复现时间减少56.8%，并降低参与者的认知负荷。

Conclusion: RepGen是一种有效的自动化深度学习bug复现方法，显著提高了bug复现率和效率，降低了开发者的认知负担，为解决DL应用中的bug问题提供了实用工具。

Abstract: Despite their wide adoption in various domains (e.g., healthcare, finance, software engineering), Deep Learning (DL)-based applications suffer from many bugs, failures, and vulnerabilities. Reproducing these bugs is essential for their resolution, but it is extremely challenging due to the inherent nondeterminism of DL models and their tight coupling with hardware and software environments. According to recent studies, only about 3% of DL bugs can be reliably reproduced using manual approaches. To address these challenges, we present RepGen, a novel, automated, and intelligent approach for reproducing deep learning bugs. RepGen constructs a learning-enhanced context from a project, develops a comprehensive plan for bug reproduction, employs an iterative generate-validate-refine mechanism, and thus generates such code using an LLM that reproduces the bug at hand. We evaluate RepGen on 106 real-world deep learning bugs and achieve a reproduction rate of 80.19%, a 19.81% improvement over the state-of-the-art measure. A developer study involving 27 participants shows that RepGen improves the success rate of DL bug reproduction by 23.35%, reduces the time to reproduce by 56.8%, and lowers participants' cognitive load.

</details>


### [40] [Toxicity Ahead: Forecasting Conversational Derailment on GitHub](https://arxiv.org/abs/2512.15031)
*Mia Mohammad Imran,Robert Zita,Rahat Rizvi Rahman,Preetha Chatterjee,Kostadin Damevski*

Main category: cs.SE

TL;DR: 本文提出了一种基于大型语言模型的两步提示框架，用于预测开源软件社区中对话的毒性偏离，通过分析对话动态摘要来早期检测有害对话。


<details>
  <summary>Details</summary>
Motivation: 开源软件社区中的毒性互动会降低贡献者参与度并威胁项目可持续性。目前大多数主动审核策略都是手动的，需要社区维护者投入大量时间和精力，因此需要更可扩展的方法来预防毒性对话。

Method: 构建了包含159个毒性偏离线程和207个非毒性线程的数据集，分析发现毒性可通过紧张触发因素、情感转变和特定对话模式预测。提出基于LLM的两步提示框架：首先通过"最少到最多"提示生成对话动态摘要，然后使用这些摘要估计偏离可能性。

Result: 在Qwen和Llama模型上，LtM策略在决策阈值为0.3时分别达到0.901和0.852的F1分数，优于现有NLP基线。在包含308个GitHub问题线程的外部验证数据集上，F1分数最高达0.797。

Conclusion: 结构化LLM提示能有效早期检测开源软件中的对话偏离，实现主动且可解释的审核，为社区毒性预防提供可扩展解决方案。

Abstract: Toxic interactions in Open Source Software (OSS) communities reduce contributor engagement and threaten project sustainability. Preventing such toxicity before it emerges requires a clear understanding of how harmful conversations unfold. However, most proactive moderation strategies are manual, requiring significant time and effort from community maintainers. To support more scalable approaches, we curate a dataset of 159 derailed toxic threads and 207 non-toxic threads from GitHub discussions. Our analysis reveals that toxicity can be forecast by tension triggers, sentiment shifts, and specific conversational patterns.
  We present a novel Large Language Model (LLM)-based framework for predicting conversational derailment on GitHub using a two-step prompting pipeline. First, we generate \textit{Summaries of Conversation Dynamics} (SCDs) via Least-to-Most (LtM) prompting; then we use these summaries to estimate the \textit{likelihood of derailment}. Evaluated on Qwen and Llama models, our LtM strategy achieves F1-scores of 0.901 and 0.852, respectively, at a decision threshold of 0.3, outperforming established NLP baselines on conversation derailment. External validation on a dataset of 308 GitHub issue threads (65 toxic, 243 non-toxic) yields an F1-score up to 0.797. Our findings demonstrate the effectiveness of structured LLM prompting for early detection of conversational derailment in OSS, enabling proactive and explainable moderation.

</details>


### [41] [Aligning Academia with Industry: An Empirical Study of Industrial Needs and Academic Capabilities in AI-Driven Software Engineering](https://arxiv.org/abs/2512.15148)
*Hang Yu,Yuzhou Lai,Li Zhang,Xiaoli Lian,Fang Liu,Yanrui Dong,Ting Zhang,Zhi Jin,David Lo*

Main category: cs.SE

TL;DR: 该研究系统分析了2022-2025年间FSE、ASE、ICSE三大软件工程顶会的1367篇论文，并结合对17个组织的282份工业界问卷调查，揭示了学术界与工业界在LLM驱动的软件工程研究上的差距，提出了七个关键启示。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）正在重塑软件工程领域，但学术界的研究进展与工业界实际需求之间的对齐程度尚不明确。研究者希望通过系统分析学术论文和工业界反馈，揭示两者之间的差距。

Method: 1. 对2022-2025年间FSE、ASE、ICSE三大会议的1367篇论文进行系统分析，识别研究主题、常用基准、工业相关性和开源可用性；2. 对17个组织进行问卷调查，收集282份关于程序分析、自动化测试、代码生成/补全、问题解决、预训练代码模型和依赖管理等六个主题的反馈。

Result: 通过对比学术能力和工业反馈，发现了七个关键启示：1）软件需求和架构方面的挑战未得到充分解决；2）智能软件工程方法的可靠性和可解释性问题；3）学术研究中的输入假设问题；4）实际评估中的紧张关系；5）伦理考虑；6）其他未充分探索的问题；7）需要更强的工业影响力。

Conclusion: 该研究旨在重新聚焦学术界对重要但未充分探索问题的关注，并指导未来的软件工程研究朝着更强的工业影响力方向发展，弥合学术研究与工业实践之间的差距。

Abstract: The rapid advancement of large language models (LLMs) is fundamentally reshaping software engineering (SE), driving a paradigm shift in both academic research and industrial practice. While top-tier SE venues continue to show sustained or emerging focus on areas like automated testing and program repair, with researchers worldwide reporting continuous performance gains, the alignment of these academic advances with real industrial needs remains unclear. To bridge this gap, we first conduct a systematic analysis of 1,367 papers published in FSE, ASE, and ICSE between 2022 and 2025, identifying key research topics, commonly used benchmarks, industrial relevance, and open-source availability. We then carry out an empirical survey across 17 organizations, collecting 282 responses on six prominent topics, i.e., program analysis, automated testing, code generation/completion, issue resolution, pre-trained code models, and dependency management, through structured questionnaires. By contrasting academic capabilities with industrial feedback, we derive seven critical implications, highlighting under-addressed challenges in software requirements and architecture, the reliability and explainability of intelligent SE approaches, input assumptions in academic research, practical evaluation tensions, and ethical considerations. This study aims to refocus academic attention on these important yet under-explored problems and to guide future SE research toward greater industrial impact.

</details>


### [42] [Automating Execution and Verification of BPMN+DMN Business Processes](https://arxiv.org/abs/2512.15214)
*Giuseppe Della Penna,Igor Melatti*

Main category: cs.SE

TL;DR: BDTransTest工具将BPMN+DMN业务流程转换为Java程序，自动生成测试计划并分析覆盖率，解决现有工具只能检测语法错误而忽略语义行为故障的问题。


<details>
  <summary>Details</summary>
Motivation: 当前BPMN+DMN建模框架只能检测语法错误，无法发现语义行为故障，迫使业务设计师手动运行单个执行来检测失败，且专有工具的转换过程不透明。

Method: 设计BDTransTest工具：1) 将BPMN+DMN流程B转换为Java程序P；2) 合成并执行测试计划，可能需要业务设计师澄清输入域；3) 分析测试计划对B节点和边的覆盖率。

Result: 在文献中的BPMN+DMN流程上进行了实验评估，验证了方法的有效性。

Conclusion: BDTransTest提供了从BPMN+DMN流程到Java程序的翻译、测试计划生成和执行、以及覆盖率分析的综合解决方案，改进了业务流程验证的现状。

Abstract: The increasing and widespread use of BPMN business processes, also embodying DMN tables, requires tools and methodologies to verify their correctness. However, most commonly used frameworks to build BPMN+DMN models only allow designers to detect syntactical errors, thus ignoring semantic (behavioural) faults. This forces business processes designers to manually run single executions of their BPMN+DMN processes using proprietary tools in order to detect failures. Furthermore, how proprietary tools translate a BPMN+DMN process to a computer simulation is left unspecified. In this paper, we advance this state of the art by designing a tool, named BDTransTest providing: i) a translation from a BPMN + DMN process B to a Java program P ; ii) the synthesis and execution of a testing plan for B, that may require the business designer to disambiguate some input domain; iii) the analysis of the coverage achieved by the testing plan in terms of nodes and edges of B. Finally, we provide an experimental evaluation of our methodology on BPMN+DMN processes from the literature.

</details>


### [43] [Heterogeneous Model Alignment in Digital Twin](https://arxiv.org/abs/2512.15281)
*Faima Abbasi,Jean-Sébastien Sottet,Cedric Pruski*

Main category: cs.SE

TL;DR: 提出了一种用于多层模型驱动数字孪生的异构模型对齐方法，通过自适应一致性机制和LLM验证的对齐过程，解决跨抽象层的语义不匹配和同步问题。


<details>
  <summary>Details</summary>
Motivation: 数字孪生技术整合异构数据和模型，但多层模型驱动数字孪生中异构模型跨抽象层的对齐存在挑战，导致语义不匹配、不一致性和同步问题。现有方法依赖静态映射和手动更新，缺乏灵活性且容易出错。

Method: 提出异构模型对齐框架，包含灵活性机制使元模型能够自适应互连，同时保持跨抽象层的语义一致性。整合：(1)自适应一致性机制，连接元模型与演化模型；(2)基于大语言模型验证的对齐过程，将元模型锚定在领域知识中，确保结构保真度和概念一致性。

Result: 该方法能够自动发现语义对应关系，减少手动映射，增强跨不同模型类型的可扩展性。通过空气质量用例进行说明，并使用OAEI测试用例验证性能。

Conclusion: 提出的异构模型对齐方法解决了数字孪生中跨抽象层模型对齐的关键挑战，通过自动化语义对应发现和LLM验证的对齐过程，提高了数字孪生系统的灵活性和可扩展性。

Abstract: Digital twin (DT) technology integrates heterogeneous data and models, along with semantic technologies to create multi-layered digital representation of physical systems. DTs enable monitoring, simulation, prediction, and optimization to enhance decision making and operational efficiency. A key challenge in multi-layered, model-driven DTs is aligning heterogeneous models across abstraction layers, which can lead to semantic mismatches, inconsistencies, and synchronization issues. Existing methods, relying on static mappings and manual updates, are often inflexible, error-prone, and risk compromising data integrity. To address these limitations, we present a heterogeneous model alignment approach for multi-layered, model-driven DTs. The framework incorporates a flexibility mechanism that allows metamodels to adapt and interconnect seamlessly while maintaining semantic coherence across abstraction layers. It integrates: (i) adaptive conformance mechanisms that link metamodels with evolving models and (ii) a large language model (LLM) validated alignment process that grounds metamodels in domain knowledge, ensuring structural fidelity and conceptual consistency throughout the DT lifecycle. This approach automates semantic correspondences discovery, minimizes manual mapping, and enhances scalability across diverse model types. We illustrate the approach using air quality use case and validate its performance using different test cases from Ontology Alignment Evaluation Initiative (OAEI) tracks.

</details>


### [44] [Insecure Ingredients? Exploring Dependency Update Patterns of Bundled JavaScript Packages on the Web](https://arxiv.org/abs/2512.15447)
*Ben Swierzy,Marc Ohm,Michael Meier*

Main category: cs.SE

TL;DR: Aletheia是一种包无关的JavaScript包版本检测方法，通过抄袭检测算法分析JavaScript捆绑包，显著优于现有方法。研究发现5%-20%的网站在16周内更新依赖，捆绑包更新速度比CDN包快10倍，但少数大型供应商主导了及时更新。


<details>
  <summary>Details</summary>
Motivation: JavaScript生态系统中存在大量易受攻击的包版本，但现有检测方法要么针对特定热门包，要么只能处理单文件资源，无法大规模分析现代Web应用的依赖更新行为，需要更全面的检测机制。

Method: 提出Aletheia方法，这是一种包无关的检测技术，通过借鉴抄袭检测领域的算法来剖析JavaScript捆绑包，识别其中使用的包版本，能够处理复杂的捆绑包结构。

Result: Aletheia在实践环境中明显优于现有方法。对Tranco前10万个域名的爬取显示：5%-20%的域名在16周内更新依赖；捆绑包更新速度显著快于CDN包含的包，易受攻击版本数量减少10倍；但少数大型供应商是及时更新的主要推动力。

Conclusion: 虽然捆绑包依赖更新更快且安全性更好，但依赖更新主要由少数大型供应商驱动，定量指标不能完全反映实际情况，需要更全面的依赖更新行为分析框架。

Abstract: Reusable software components, typically distributed as packages, are a central paradigm of modern software development. The JavaScript ecosystem serves as a prime example, offering millions of packages with their use being promoted as idiomatic. However, download statistics on npm raise security concerns as they indicate a high popularity of vulnerable package versions while their real prevalence on production websites remains unknown. Package version detection mechanisms fill this gap by extracting utilized packages and versions from observed artifacts on the web. Prior research focuses on mechanisms for either hand-selected popular packages in bundles or for single-file resources utilizing the global namespace. This does not allow for a thorough analysis of modern web applications' dependency update behavior at scale. In this work, we improve upon this by presenting Aletheia, a package-agnostic method which dissects JavaScript bundles to identify package versions through algorithms originating from the field of plagiarism detection. We show that this method clearly outperforms the existing approaches in practical settings. Furthermore, we crawl the Tranco top 100,000 domains to reveal that 5% - 20% of domains update their dependencies within 16 weeks. Surprisingly, from a longitudinal perspective, bundled packages are updated significantly faster than their CDN-included counterparts, with consequently up to 10 times fewer known vulnerable package versions included. Still, we observe indicators that few widespread vendors seem to be a major driving force behind timely updates, implying that quantitative measures are not painting a complete picture.

</details>


### [45] [A Container-based Approach For Proactive Asset Administration Shell Digital Twins](https://arxiv.org/abs/2512.15452)
*Carsten Ellwein,Jingxi Zhang,Andreas Wortmann,Antony Ayman Alfy Meckhael*

Main category: cs.SE

TL;DR: 提出基于子模型的AAS架构，通过容器化服务集成使数字孪生从被动信息模型转变为主动执行接口


<details>
  <summary>Details</summary>
Motivation: 当前AAS作为静态信息模型，缺乏动态服务集成和系统适应性，无法支持主动功能执行

Method: 扩展子模型包含行为定义，构建模块化事件驱动架构，基于嵌入触发条件部署容器化服务

Result: 通过三轴铣床案例验证，使AAS不仅能作为被动数字表示，还能作为执行增值服务的主动接口

Conclusion: 提出的架构为数字孪生环境中未来AI驱动适应和系统级智能奠定了基础

Abstract: In manufacturing, digital twins, realized as Asset Administration Shells (AAS), have emerged as a prevalent practice. These digital replicas, often utilized as structured repositories of asset-related data, facilitate interoperability across diverse systems. However, extant approaches treat the AAS as a static information model, lacking support for dynamic service integration and system adaptation. The existing body of literature has not yet thoroughly explored the potential for integrating executable behavior, particularly in the form of containerized services, into or from the AAS. This integration could serve to enable proactive functionality. In this paper, we propose a submodel-based architecture that introduces a structured service notion to the AAS, enabling services to dynamically interact with and adapt AAS instances at runtime. This concept is implemented through the extension of a submodel with behavioral definitions, resulting in a modular event-driven architecture capable of deploying containerized services based on embedded trigger conditions. The approach is illustrated through a case study on a 3-axis milling machine. Our contribution enables the AAS to serve not only as a passive digital representation but also as an active interface for executing added-value services.%, thereby laying the foundation for future AI-driven adaptation and system-level intelligence in digital twin environments.

</details>


### [46] [On Assessing the Relevance of Code Reviews Authored by Generative Models](https://arxiv.org/abs/2512.15466)
*Robert Heumüller,Frank Ortmeier*

Main category: cs.SE

TL;DR: 提出基于多主观排序的代码审查评估新方法，发现ChatGPT生成的评论质量显著优于人类评论，甚至超越StackExchange的采纳答案


<details>
  <summary>Details</summary>
Motivation: 现有代码审查生成评估方法存在局限性：要么依赖与单一标准答案的自动比较，无法捕捉人类观点的多样性；要么依赖主观的"有用性"评估，这个概念过于模糊。需要更有效的评估方法来准确衡量生成式AI在代码审查中的表现。

Method: 提出多主观排序评估方法，使用CodeReview StackExchange的280个自包含代码审查请求和相应评论数据集，让多位人类评审对ChatGPT生成的评论与平台上最佳人类回答进行质量排序比较。

Result: ChatGPT生成的评论质量被显著评为优于人类评论，甚至超越了StackExchange的采纳答案。这表明生成式AI在代码审查任务中具有优异表现。

Conclusion: 提出的多主观排序方法能够更有效地评估生成式AI在代码审查中的性能，同时也提醒人们注意将AI不加检查地集成到审查流程中的潜在风险。

Abstract: The use of large language models like ChatGPT in code review offers promising efficiency gains but also raises concerns about correctness and safety. Existing evaluation methods for code review generation either rely on automatic comparisons to a single ground truth, which fails to capture the variability of human perspectives, or on subjective assessments of "usefulness", a highly ambiguous concept. We propose a novel evaluation approach based on what we call multi-subjective ranking. Using a dataset of 280 self-contained code review requests and corresponding comments from CodeReview StackExchange, multiple human judges ranked the quality of ChatGPT-generated comments alongside the top human responses from the platform. Results show that ChatGPT's comments were ranked significantly better than human ones, even surpassing StackExchange's accepted answers. Going further, our proposed method motivates and enables more meaningful assessments of generative AI's performance in code review, while also raising awareness of potential risks of unchecked integration into review processes.

</details>


### [47] [How Do Semantically Equivalent Code Transformations Impact Membership Inference on LLMs for Code?](https://arxiv.org/abs/2512.15468)
*Hua Yang,Alejandro Velasco,Thanh Le-Cong,Md Nazmul Haque,Bowen Xu,Denys Poshyvanyk*

Main category: cs.SE

TL;DR: 语义等价的代码转换技术可以显著降低成员推理检测的有效性，特别是变量重命名规则能将MI成功率降低10.19%，这暴露了代码大模型训练中许可证合规执行的关键漏洞。


<details>
  <summary>Details</summary>
Motivation: 大型代码语言模型的训练依赖于大量代码数据，包括开源和私有代码，这引发了知识产权合规问题。虽然成员推理技术被提出用于检测未经授权的代码使用，但语义等价的代码转换技术可能被用来规避这种检测。

Method: 系统研究语义等价的代码转换规则是否可用于规避成员推理检测。通过因果分析验证变量重命名对破坏MI检测的因果效应，并测试多种转换组合的效果。

Result: 1. 每个转换规则在最坏情况下仅使模型准确率下降1.5%，表明转换后的数据集可有效替代微调；2. 变量重命名规则将MI成功率降低10.19%；3. 因果分析确认变量重命名对破坏MI检测具有最强因果效应；4. 组合多种转换不会进一步降低MI有效性。

Conclusion: 语义等价的代码转换技术（特别是变量重命名）能够显著削弱成员推理检测，这暴露了代码大模型训练中许可证合规执行的关键漏洞，表明基于转换的混淆技术可以大幅降低MI检测效果。

Abstract: The success of large language models for code relies on vast amounts of code data, including public open-source repositories, such as GitHub, and private, confidential code from companies. This raises concerns about intellectual property compliance and the potential unauthorized use of license-restricted code. While membership inference (MI) techniques have been proposed to detect such unauthorized usage, their effectiveness can be undermined by semantically equivalent code transformation techniques, which modify code syntax while preserving semantic.
  In this work, we systematically investigate whether semantically equivalent code transformation rules might be leveraged to evade MI detection. The results reveal that model accuracy drops by only 1.5% in the worst case for each rule, demonstrating that transformed datasets can effectively serve as substitutes for fine-tuning. Additionally, we find that one of the rules (RenameVariable) reduces MI success by 10.19%, highlighting its potential to obscure the presence of restricted code. To validate these findings, we conduct a causal analysis confirming that variable renaming has the strongest causal effect in disrupting MI detection. Notably, we find that combining multiple transformations does not further reduce MI effectiveness. Our results expose a critical loophole in license compliance enforcement for training large language models for code, showing that MI detection can be substantially weakened by transformation-based obfuscation techniques.

</details>


### [48] [WuppieFuzz: Coverage-Guided, Stateful REST API Fuzzing](https://arxiv.org/abs/2512.15554)
*Thomas Rooijakkers,Anne Nijsten,Cristian Daniele,Erieke Weitenberg,Ringo Groenewegen,Arthur Melissen*

Main category: cs.SE

TL;DR: WuppieFuzz是一个基于LibAFL的开源REST API模糊测试工具，支持白盒、灰盒和黑盒测试，通过OpenAPI规范生成初始请求序列，利用REST特定和LibAFL提供的变异器探索不同代码路径，基于覆盖率指导测试过程。


<details>
  <summary>Details</summary>
Motivation: REST API广泛用于业务进程，暴露的端点带来安全风险。由于端点数量庞大，需要自动化测试技术（如模糊测试）来减少安全风险。

Method: 开发WuppieFuzz工具，基于LibAFL框架，支持多种测试模式。使用OpenAPI规范生成初始请求序列，结合REST特定变异器和LibAFL变异器进行变异，基于覆盖率指导选择请求序列，自动化创建测试工具以减少手动工作。

Result: 在Petstore API上评估了白盒方法的鲁棒性和不同功率调度策略的有效性，监测了端点和代码覆盖率随时间的变化，验证了方法的有效性。

Conclusion: WuppieFuzz是一个有效的REST API模糊测试工具，能够自动化测试过程，减少手动工作，帮助发现和修复漏洞。

Abstract: Many business processes currently depend on web services, often using REST APIs for communication. REST APIs expose web service functionality through endpoints, allowing easy client interaction over the Internet. To reduce the security risk resulting from exposed endpoints, thorough testing is desired. Due to the generally vast number of endpoints, automated testing techniques, like fuzzing, are of interest.
  This paper introduces WuppieFuzz, an open-source REST API fuzzer built on LibAFL, supporting white-box, grey-box and black-box fuzzing. Using an OpenAPI specification, it can generate an initial input corpus consisting of sequences of requests. These are mutated with REST-specific and LibAFL-provided mutators to explore different code paths in the software under test. Guided by the measured coverage, WuppieFuzz then selects which request sequences to send next to reach complex states in the software under test. In this process, it automates harness creation to reduce manual efforts often required in fuzzing. Different kinds of reporting are provided by the fuzzer to help fixing bugs.
  We evaluated our tool on the Petstore API to assess the robustness of the white-box approach and the effectiveness of different power schedules. We further monitored endpoint and code coverage over time to measure the efficacy of the approach.

</details>


### [49] [A High-level Synthesis Toolchain for the Julia Language](https://arxiv.org/abs/2512.15679)
*Benedict Short,Ian McInerney,John Wickerson*

Main category: cs.SE

TL;DR: 提出基于MLIR的Julia到SystemVerilog编译器工具链，解决FPGA加速器开发中的"两语言问题"，让领域专家能用Julia编写计算内核并自动生成FPGA硬件设计。


<details>
  <summary>Details</summary>
Motivation: 随着向Exascale计算和数据驱动方法的发展，问题规模急剧增加，计算需求增长推动了向GPU/TPU等通用硬件加速器以及FPGA专用加速器的转移。但专用加速器开发存在"两语言问题"：算法用高级语言开发，而内核需要用完全不同抽象级别的语言实现，需要不同的专业知识。

Method: 提出基于MLIR的编译器工具链，自动将Julia编程语言编写的内核编译为SystemVerilog，无需额外指令或语言定制。支持动态和静态调度，直接集成AXI4-Stream协议与子系统接口，生成供应商无关的RTL。

Result: 原型工具链能够合成一组信号处理/数学基准测试，在真实FPGA设备上以100MHz运行，达到仅从C/C++等低级语言编译的最先进工具链生成设计的59.71%到82.6%的吞吐量。

Conclusion: 该工具链允许领域专家像平常一样用Julia编写计算内核，然后无需额外编译指示或修改即可将其重定向到FPGA，解决了专用加速器开发中的两语言问题。

Abstract: With the push towards Exascale computing and data-driven methods, problem sizes have increased dramatically, increasing the computational requirements of the underlying algorithms. This has led to a push to offload computations to general purpose hardware accelerators such as GPUs and TPUs, and a renewed interest in designing problem-specific accelerators using FPGAs. However, the development process of these problem-specific accelerators currently suffers from the "two-language problem": algorithms are developed in one (usually higher-level) language, but the kernels are implemented in another language at a completely different level of abstraction and requiring fundamentally different expertise. To address this problem, we propose a new MLIR-based compiler toolchain that unifies the development process by automatically compiling kernels written in the Julia programming language into SystemVerilog without the need for any additional directives or language customisations. Our toolchain supports both dynamic and static scheduling, directly integrates with the AXI4-Stream protocol to interface with subsystems like on- and off-chip memory, and generates vendor-agnostic RTL. This prototype toolchain is able to synthesize a set of signal processing/mathematical benchmarks that can operate at 100MHz on real FPGA devices, achieving between 59.71% and 82.6% of the throughput of designs generated by state-of-the-art toolchains that only compile from low-level languages like C or C++. Overall, this toolchain allows domain experts to write compute kernels in Julia as they normally would, and then retarget them to an FPGA without additional pragmas or modifications.

</details>
