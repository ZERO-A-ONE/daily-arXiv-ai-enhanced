<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 22]
- [cs.CR](#cs.CR) [Total: 24]
- [cs.AI](#cs.AI) [Total: 48]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Injecting Sustainability in Software Architecture: A Rapid Review](https://arxiv.org/abs/2512.00106)
*Markus Funke,Patricia Lago*

Main category: cs.SE

TL;DR: 该研究通过混合方法实证研究，结合文献综述和焦点小组，探讨如何在软件架构中系统性地整合可持续性考量，为从业者提供实用指导。


<details>
  <summary>Details</summary>
Motivation: 可持续性已从新兴关注点发展为软件设计、开发和运维的基本责任。研究旨在探索如何将可持续性系统性地整合到现有软件工程实践中，特别是软件架构层面。

Method: 采用混合方法实证研究：1）对二手研究进行快速文献综述，识别软件架构中嵌入可持续性的挑战和机遇；2）与从业者进行焦点小组讨论，丰富并验证文献发现。

Result: 基于文献和行业综合，研究得出了五个切实可行的要点，用于指导软件架构师在实际工作中整合可持续性考量，并为行业合作伙伴提供实践指导。

Conclusion: 通过产学合作，该研究为软件架构中可持续性的系统性整合提供了实证基础和实践指导，有助于推动可持续性在软件工程领域的实际应用。

Abstract: Sustainability has evolved from an emerging concern into a fundamental responsibility in software design, development, and operation. Research increasingly explores how sustainability can be systematically integrated into existing software engineering practices. Building on an industry-academia collaboration, we contribute to this discourse by conducting a mixed-method empirical study. We combine a rapid review of secondary studies with a focus group of practitioners. The review identifies challenges and opportunities in embedding sustainability in software architecture, while the focus group enriches and compares these findings. Based on the literature and industry synthesis, we derive five tangible takeaways to inform architects working in the field, and to guide our industry partners in the integration of sustainability concerns in architecture practices.

</details>


### [2] [Generating Verifiable CoT from Execution-Traces](https://arxiv.org/abs/2512.00127)
*Shailja Thakur,Vaibhav Saxena,Rohan Kulkarni,Shivdeep Singh,Parameswaran Selvam,Hima Patel,Hiroshi Kanayama*

Main category: cs.SE

TL;DR: 该论文提出了一种基于程序执行轨迹的代码推理方法，通过将代码执行过程转化为自然语言解释，确保推理步骤的正确性，从而解决传统CoT提示中存在的逻辑幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型在代码推理任务中存在关键缺陷：现有的合成训练数据中的推理步骤通常是教师模型生成的看似合理但未经验证的解释，而非代码实际执行过程的真实描述，这导致模型学习到表面上合理但逻辑有缺陷的推理模式。

Method: 通过代码插桩技术捕获程序的动态执行轨迹，然后将这些经过验证的执行轨迹转化为自然语言推理步骤，构建基于执行轨迹的双向推理数据。这种方法确保每个推理步骤都反映程序实际计算过程，从源头上消除逻辑幻觉。

Result: 在代码推理任务（CruxEval和LiveCodeBench-Exec的前向推理，CruxEval-Input的后向推理）以及HumanEval的代码生成和解释任务上，使用该方法训练的模型取得了显著改进：输出预测提升高达30分，输入预测提升28分，同时代码生成和解释能力也得到改善。

Conclusion: 基于可验证执行轨迹的推理方法能够从根本上提升语言模型在代码相关任务上的能力，证明了可验证推理对模型性能的重要增强作用。

Abstract: Teaching language models to reason about code execution remains a fundamental challenge. While Chain-of-Thought (CoT) prompting has shown promise, current synthetic training data suffers from a critical weakness: the reasoning steps are often plausible-sounding explanations generated by teacher models, not verifiable accounts of what the code actually does. This creates a troubling failure mode where models learn to mimic superficially convincing but logically flawed reasoning patterns.
  We address this by grounding CoT generation directly in program execution traces. Our pipeline instruments code to capture its dynamic behavior, then narrates these verified execution traces into natural language rationales that are correct by construction. This execution-grounded approach ensures every reasoning step reflects what the program genuinely computes, eliminating logical hallucinations at the source. We evaluate our method on code reasoning tasks (forward reasoning on CruxEval and LiveCodeBench-Exec, backward reasoning on CruxEval-Input), as well as code generation and explanation tasks from HumanEval. Models trained on our bi-directional trace-grounded data achieve substantial improvements, with gains of up to 30 points on output prediction and 28 points on input prediction over base models, alongside improved explanation and code generation, demonstrating that verifiable reasoning fundamentally enhances model capabilities. https://github.ibm.com/IBM-Research-AI/Verified-Code-CoT

</details>


### [3] [Demystifying Errors in LLM Reasoning Traces: An Empirical Study of Code Execution Simulation](https://arxiv.org/abs/2512.00215)
*Mohammad Abdollahi,Khandaker Rifah Tasnia,Soumit Kanti Saha,Jinqiu Yang,Song Wang,Hadi Hemmati*

Main category: cs.SE

TL;DR: 该研究首次对推理型大语言模型的运行时行为进行实证分析，通过构建包含427个代码片段的基准测试，评估了4个先进推理LLM在常规、边界和无效输入下的表现，揭示了推理轨迹中的错误类型，并探索了工具增强推理的改进潜力。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型能准确预测程序输出，但现有研究主要关注输出准确性，将推理过程视为黑箱。对LLM推理轨迹的结构和失败模式了解甚少，这限制了代码生成、调试和自动推理的可靠性。

Method: 从HumanEval Plus和LiveCodeBench中筛选427个代码片段，为每个片段测试三种输入类型（常规、边界、无效），每个片段选择12个输入值并配对其真实执行结果。评估4个最先进的推理LLM，分析生成的推理轨迹，建立包含9个类别的推理错误分类法，并探索工具增强推理方法。

Result: 模型在不同输入类型下的准确率在85%到98%之间。研究识别出9类推理错误，其中计算错误类别的案例研究表明，工具增强推理方法能修正58%的此类错误，显示了工具支持对改进LLM推理的潜力。

Conclusion: 这是首个对推理型LLM运行时行为进行系统分析的研究，揭示了推理轨迹中的错误模式，并证明了工具增强推理能有效修正特定类型的推理错误，为提升LLM在代码相关任务中的可靠性和可解释性提供了重要见解。

Abstract: Understanding a program's runtime reasoning behavior, meaning how intermediate states and control flows lead to final execution results, is essential for reliable code generation, debugging, and automated reasoning. Although large language models (LLMs) can accurately predict program outputs, most prior work has focused on output accuracy and performance, treating reasoning as a black box. As a result, little is known about the structure or failure modes of their reasoning traces. To address this gap, we conduct the first empirical study on runtime behavior inference with reasoning LLMs, aiming to uncover and characterize errors in their reasoning traces. We curate a benchmark from HumanEval Plus and LiveCodeBench, containing 427 code snippets. For each snippet, we test three input types: regular, edge, and invalid. Twelve input values are selected per snippet, each paired with its ground-truth execution result. We evaluate four state-of-the-art reasoning LLMs. Our results show that these models reach accuracies between 85 percent and 98 percent across input types. We also analyze the produced reasoning traces and develop a taxonomy with nine categories of inference errors. Finally, we explore tool-augmented reasoning. Using failures in the Computation Errors category as a case study, our experiments show that this approach corrects 58 percent of such errors, demonstrating the potential of tool support for improving LLM reasoning.

</details>


### [4] [CodeFlowLM: Incremental Just-In-Time Defect Prediction with Pretrained Language Models and Exploratory Insights into Defect Localization](https://arxiv.org/abs/2512.00231)
*Monique Louise Monteiro,George G. Cabral,Adriano L. I. OLiveira*

Main category: cs.SE

TL;DR: CodeFlowLM是一个用于即时软件缺陷预测的增量学习框架，利用预训练语言模型解决概念漂移、类别不平衡和验证延迟问题，在跨项目评估中表现优异，同时探索了LLM在缺陷定位中的应用。


<details>
  <summary>Details</summary>
Motivation: 传统在线学习器在即时软件缺陷预测中面临概念漂移、类别不平衡和验证延迟等挑战，需要更适应软件环境演化的解决方案。

Method: 提出CodeFlowLM增量学习框架，采用持续微调策略而非从头训练，评估了CodeT5+和UniXCoder等编码器-解码器PLM，并与BORB基线比较，同时将分析扩展到LLM在缺陷定位中的应用。

Result: CodeFlowLM在即时缺陷预测中实现高达68%的G-Mean增益，在跨项目场景中表现优异；在缺陷定位中，GPT-5在Recall@20%和Effort@20%指标上与注意力模型相当且更稳定，但注意力模型在细粒度排名指标上仍有优势。

Conclusion: CodeFlowLM显著提升了增量即时缺陷预测的技术水平，展示了在演化软件环境中的优越适应性和鲁棒性；LLM在缺陷定位中虽有潜力，但存在保守偏差、上下文信息不足和数据标注问题等限制。

Abstract: This work introduces CodeFlowLM, an incremental learning framework for Just-In-Time Software Defect Prediction (JIT-SDP) that leverages pre-trained language models (PLMs). Unlike traditional online learners, CodeFlowLM employs continual fine-tuning to address concept drift, class imbalance, and verification latency without retraining from scratch. We evaluated encoder-only and encoder-decoder PLMs (notably CodeT5+ and UniXCoder) in JIT-SDP scenarios within and between projects, comparing them with the incremental baseline BORB. The results show that CodeFlowLM achieves up to 68% G-Mean gains, confirming its superior adaptability and robustness in evolving software environments. We further extend the analysis to Just-in-Time Defect Localization (JIT-DL), benchmarking Large Language Models (LLMs) such as GPT-5, Claude Sonnet 4.5, and Gemini 2.5 Pro against attention-based models. GPT-5 delivers comparable performance for Recall@20% and Effort@20% with higher stability, although attention-based methods retain an advantage in fine-grained ranking metrics (Top-k, IFA). A qualitative error analysis reveals that most false positives arise from (1) human-like conservative bias, (2) insufficient contextual information in diff-based prompts, and (3) potential dataset mislabeling in JIT-Defects4J. These findings highlight both the promise and the current limitations of LLM reasoning in defect localization. False negatives occur in smaller proportions. Overall, CodeFlowLM significantly advances the state of the art in incremental JIT-SDP, demonstrating superior adaptability and robustness in evolving software environments. Furthermore, our exploratory analysis of LLMs in JIT-DL not only benchmarks their performance against established attention-based models but also provides critical insights into the current limitations of prompt-based defect reasoning.

</details>


### [5] [Progressive Code Integration for Abstractive Bug Report Summarization](https://arxiv.org/abs/2512.00325)
*Shaira Sadia Karim,Abrar Mahmud Rahim,Lamia Alam,Ishmam Tashdeed,Lutfun Nahar Lota,Md. Abu Raihan M. Kamal,Md. Azam Hossain*

Main category: cs.SE

TL;DR: 提出了一种渐进式代码集成框架，用于基于LLM的缺陷报告摘要生成，通过逐步整合长代码片段和文本内容来克服LLM上下文窗口限制，生成语义丰富的摘要。


<details>
  <summary>Details</summary>
Motivation: 缺陷报告通常是非结构化和冗长的，使得开发人员难以有效理解软件问题。现有摘要方法通常依赖表面文本线索，导致摘要不完整或冗余，且经常忽略相关的代码片段，而这些代码片段对于准确缺陷诊断至关重要。

Method: 提出渐进式代码集成框架，逐步将长代码片段与文本内容结合，克服标准LLM上下文窗口限制。该方法在四个基准数据集上使用八个LLM进行评估。

Result: 该方法在四个基准数据集上的评估显示，相比抽取式基线方法性能提升7.5%-58.2%，且与最先进的抽象式方法性能相当，突显了联合利用文本和代码信息对增强缺陷理解的好处。

Conclusion: 提出的渐进式代码集成框架通过联合利用文本和代码信息，能够生成语义丰富的缺陷报告摘要，显著优于传统方法，为缺陷理解提供了更有效的解决方案。

Abstract: Bug reports are often unstructured and verbose, making it challenging for developers to efficiently comprehend software issues. Existing summarization approaches typically rely on surface-level textual cues, resulting in incomplete or redundant summaries, and they frequently ignore associated code snippets, which are essential for accurate defect diagnosis. To address these limitations, we propose a progressive code-integration framework for LLM-based abstractive bug report summarization. Our approach incrementally incorporates long code snippets alongside textual content, overcoming standard LLM context window constraints and producing semantically rich summaries. Evaluated on four benchmark datasets using eight LLMs, our pipeline outperforms extractive baselines by 7.5%-58.2% and achieves performance comparable to state-of-the-art abstractive methods, highlighting the benefits of jointly leveraging textual and code information for enhanced bug comprehension.

</details>


### [6] [Framework-Aware Code Generation with API Knowledge Graph-Constructed Data: A Study on HarmonyOS](https://arxiv.org/abs/2512.00380)
*Mingwei Liu,Zheng Pei,Yanlin Wang,Zihao Wang,Zikang Li,Enci Lin,Xin Peng,Zibin Zheng*

Main category: cs.SE

TL;DR: 针对资源受限框架（如HarmonyOS），大语言模型因缺乏相关预训练数据而代码生成性能差。作者提出APIKG4SYN框架，利用API知识图谱构建面向API的问答对数据集，通过不确定性估计驱动的蒙特卡洛树搜索整合单API和多API知识，显著提升LLM在低资源框架下的代码生成能力。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的软件框架（如HarmonyOS）中，大语言模型由于预训练阶段缺乏对这些环境的充分接触，导致代码生成性能不佳。虽然LLM通常能在不同编程语言中保持正确的逻辑结构，但在处理框架特定的API或语法时经常出错。这表明预训练虽然赋予了LLM通用的算法能力，但对少数框架特有的语法和API使用仍不熟悉，即使是GPT-4o这样的先进商业模型也无法可靠生成正确代码。

Method: 提出APIKG4SYN框架，利用API知识图谱构建面向API的问答对数据集，专门针对低资源框架且无需可执行代码。该框架整合单API和多API知识，其中多API知识通过不确定性估计驱动的蒙特卡洛树搜索获得，从而创建多样且信息丰富的数据集用于微调LLM。以HarmonyOS为案例研究，构建了首个HarmonyOS代码生成基准。

Result: 实验结果显示，使用APIKG4SYN微调的Qwen模型将pass@1准确率提升至25.00%，而基线GPT模型仅为17.59%。这证实了面向API的数据能显著提升LLM在低资源软件开发场景中的性能。

Conclusion: APIKG4SYN框架通过利用API知识图谱构建专门的数据集，有效解决了LLM在低资源软件框架中代码生成性能差的问题。该方法不仅提升了模型性能，还为HarmonyOS等低资源框架提供了首个代码生成基准，证明了面向API的数据在增强LLM对特定框架适应能力方面的重要价值。

Abstract: In the context of software frameworks with limited resources (such as HarmonyOS), large language models (LLMs) often exhibit poor code generation performance because they lack sufficient exposure to such environments during pre-training. Although LLMs can usually maintain correct logical structures across programming languages, they frequently struggle when dealing with framework-specific APIs or syntax, resulting in errors. This indicates that while pre-training equips LLMs with general algorithmic capabilities, they remain unfamiliar with the distinctive syntax and API usage of underrepresented frameworks. As a result, even advanced commercial models like GPT-4o cannot reliably generate correct code without prior adaptation. To address this issue, we propose APIKG4SYN, a framework designed to exploit API knowledge graphs for the construction of API-oriented question-code pairs, specifically tailored for low-resource frameworks without requiring executable code. APIKG4SYN integrates both single-API and multi-API knowledge, where the latter is derived through uncertainty estimation (UE)-driven Monte Carlo Tree Search (MCTS), enabling the creation of a diverse and informative dataset for fine-tuning LLMs. Using HarmonyOS as a case study, we build the first benchmark for HarmonyOS code generation. Experimental results show that fine-tuning Qwen with APIKG4SYN raises pass@1 accuracy to 25.00%, compared with 17.59% for the baseline GPT model. These results confirm that API-oriented data significantly enhance LLM performance in low-resource software development scenarios.

</details>


### [7] [Bias Testing and Mitigation in Black Box LLMs using Metamorphic Relations](https://arxiv.org/abs/2512.00556)
*Sina Salimian,Gias Uddin,Sumon Biswas,Henry Leung*

Main category: cs.SE

TL;DR: 该研究提出基于蜕变测试的统一框架，通过六种新颖的蜕变关系将直接偏见诱导输入转换为语义等效但对抗性更强的变体，用于系统性偏见检测和缓解，在六个LLM上发现比现有工具多14%的隐藏偏见，并通过微调将安全响应率从54.7%提升至88.9%以上。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型广泛部署引发了对输出中微妙社会偏见的担忧，现有防护措施在处理间接或上下文复杂的偏见诱导提示时往往失效，需要更系统化的偏见评估和缓解方法。

Method: 提出基于蜕变测试的统一框架，引入六种新颖的蜕变关系，将直接偏见诱导输入转换为语义等效但对抗性更强的变体，用于自动化偏见检测；同时利用这些变体生成多样化的偏见诱导样本进行微调，将测试过程与缓解直接关联。

Result: 在六个最先进的LLM（包括开源和专有模型）上，使用BiasAsker基准的385个问题子集（涵盖七个受保护群体），蜕变关系比现有工具多发现14%的隐藏偏见；通过微调原始和MR突变样本，安全响应率从54.7%显著提升至88.9%以上。

Conclusion: 蜕变关系是提高对话AI公平性的实用机制，能够系统性暴露隐藏偏见并有效增强模型的偏见弹性，为LLM偏见评估和缓解提供了统一框架。

Abstract: The widespread deployment of Large Language Models (LLMs) has intensified concerns about subtle social biases embedded in their outputs. Existing guardrails often fail when faced with indirect or contextually complex bias-inducing prompts. To address these limitations, we propose a unified framework for both systematic bias evaluation and targeted mitigation. Our approach introduces six novel Metamorphic Relations (MRs) that, based on metamorphic testing principles, transform direct bias-inducing inputs into semantically equivalent yet adversarially challenging variants. These transformations enable an automated method for exposing hidden model biases: when an LLM responds inconsistently or unfairly across MR-generated variants, the underlying bias becomes detectable. We further show that the same MRs can be used to generate diverse bias-inducing samples for fine-tuning, directly linking the testing process to mitigation. Using six state-of-the-art LLMs - spanning open-source and proprietary models - and a representative subset of 385 questions from the 8,978-item BiasAsker benchmark covering seven protected groups, our MRs reveal up to 14% more hidden biases compared to existing tools. Moreover, fine-tuning with both original and MR-mutated samples significantly enhances bias resiliency, increasing safe response rates from 54.7% to over 88.9% across models. These results highlight metamorphic relations as a practical mechanism for improving fairness in conversational AI.

</details>


### [8] [SAGE: Semantic-Aware Gray-Box Game Regression Testing with Large Language Models](https://arxiv.org/abs/2512.00560)
*Jinyu Cai,Jialong Li,Nianyu Li,Zhenyu Mao,Mingyue Zhang,Kenji Tei*

Main category: cs.SE

TL;DR: SAGE是一个面向灰盒游戏环境的语义感知回归测试框架，通过LLM引导的强化学习自动生成测试用例，多目标优化压缩测试套件，并基于更新日志语义分析优先选择相关测试，显著降低测试成本并提高缺陷检测能力。


<details>
  <summary>Details</summary>
Motivation: 现代实时服务游戏的快速迭代周期需要有效的回归测试来保证质量和稳定性。现有回归测试方法在灰盒环境下存在关键限制：测试用例构建依赖人工、测试套件冗余增长、缺乏有效的测试优先级排序机制，导致测试成本过高、自动化程度低、缺陷检测不足。

Method: SAGE采用三阶段方法：1) 使用LLM引导的强化学习进行目标导向探索，自动生成多样化的基础测试套件；2) 应用基于语义的多目标优化，平衡成本、覆盖率和稀有性，将测试套件精炼为紧凑的高价值子集；3) 利用基于LLM的更新日志语义分析，优先选择与版本变更最相关的测试用例。

Result: 在Overcooked Plus和Minecraft两个代表性环境中评估，与自动化基准和人工记录的测试用例相比，SAGE在所有环境中都实现了更优的缺陷检测能力，同时显著降低了执行成本，并展现出对版本更新的强大适应性。

Conclusion: SAGE有效解决了灰盒游戏环境中回归测试的关键挑战，通过自动生成、优化和选择测试用例，实现了高效、自适应的回归测试，显著降低了测试成本并提高了缺陷检测能力。

Abstract: The rapid iteration cycles of modern live-service games make regression testing indispensable for maintaining quality and stability. However, existing regression testing approaches face critical limitations, especially in common gray-box settings where full source code access is unavailable: they heavily rely on manual effort for test case construction, struggle to maintain growing suites plagued by redundancy, and lack efficient mechanisms for prioritizing relevant tests. These challenges result in excessive testing costs, limited automation, and insufficient bug detection. To address these issues, we propose SAGE, a semanticaware regression testing framework for gray-box game environments. SAGE systematically addresses the core challenges of test generation, maintenance, and selection. It employs LLM-guided reinforcement learning for efficient, goal-oriented exploration to automatically generate a diverse foundational test suite. Subsequently, it applies a semantic-based multi-objective optimization to refine this suite into a compact, high-value subset by balancing cost, coverage, and rarity. Finally, it leverages LLM-based semantic analysis of update logs to prioritize test cases most relevant to version changes, enabling efficient adaptation across iterations. We evaluate SAGE on two representative environments, Overcooked Plus and Minecraft, comparing against both automated baselines and human-recorded test cases. Across all environments, SAGE achieves superior bug detection with significantly lower execution cost, while demonstrating strong adaptability to version updates.

</details>


### [9] [Enhancing Analogy-Based Software Effort Estimation with Firefly Algorithm Optimization](https://arxiv.org/abs/2512.00571)
*Tarun Chintada,Uday Kiran Cheera*

Main category: cs.SE

TL;DR: 本研究提出了一种萤火虫算法引导的类比估算模型（FAABE），将萤火虫算法与类比估算相结合，以提高软件项目估算精度，并在多个公开数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统的类比估算方法虽然简单有效，但对于与以往项目差异较大的新软件项目，难以实现高精度估算。现有研究尚未开发出可靠的优化方法来解决这一问题。

Method: 提出FAABE模型，将萤火虫算法与类比估算相结合，并使用特征选择技术来提高预测效率。在Cocomo81、Desharnais、China、Albrecht、Kemerer和Maxwell五个公开数据集上进行测试。

Result: 使用MMRE、MAE、MSE和RMSE等多种评估指标进行测量。实验结果表明，与传统模型相比，FAABE模型在预测精度上有显著提升，证明了萤火虫-类比集成方法的有效性。

Conclusion: FAABE模型通过集成萤火虫算法和类比估算，能够有效提高软件项目估算的准确性，特别是在处理与历史项目差异较大的新项目时表现出优越性能。

Abstract: Analogy-Based Estimation (ABE) is a popular method for non-algorithmic estimation due to its simplicity and effectiveness. The Analogy-Based Estimation (ABE) model was proposed by researchers, however, no optimal approach for reliable estimation was developed. Achieving high accuracy in the ABE might be challenging for new software projects that differ from previous initiatives. This study (conducted in June 2024) proposes a Firefly Algorithm-guided Analogy-Based Estimation (FAABE) model that combines FA with ABE to improve estimation accuracy. The FAABE model was tested on five publicly accessible datasets: Cocomo81, Desharnais, China, Albrecht, Kemerer and Maxwell. To improve prediction efficiency, feature selection was used. The results were measured using a variety of evaluation metrics; various error measures include MMRE, MAE, MSE, and RMSE. Compared to conventional models, the experimental results show notable increases in prediction precision, demonstrating the efficacy of the Firefly-Analogy ensemble.

</details>


### [10] [Large Language Models for Software Engineering: A Reproducibility Crisis](https://arxiv.org/abs/2512.00651)
*Mohammed Latif Siddiq,Arvin Islam-Gomes,Natalie Sekerak,Joanna C. S. Santos*

Main category: cs.SE

TL;DR: 该研究首次对基于大语言模型的软件工程研究中的可复现性实践进行了大规模实证分析，揭示了可复现性气味普遍存在、评估徽章可靠性有限等问题，并提出了可复现性成熟度模型和改进建议。


<details>
  <summary>Details</summary>
Motivation: 可复现性是科学进步的基石，但当前对于基于大语言模型的软件工程研究中可复现性实践的状况了解不足。研究旨在填补这一空白，通过系统分析了解该领域可复现性实践的真实状况。

Method: 研究系统挖掘并分析了2017-2025年间在顶级软件工程、机器学习和自然语言处理会议上发表的640篇论文。通过从出版物、代码库和文档中提取结构化元数据，使用七个气味类别（代码与执行、数据、文档、环境与工具、版本控制、模型、访问与法律）的分类法，对所有论文和相关构件进行了手动标注。

Result: 分析揭示了构件可用性、环境规范、版本控制严谨性和文档清晰度方面存在持续差距。尽管近年来有所改善且顶级软件工程会议增加了构件评估流程的采用，但评估徽章通常仅表示构件存在，并不能一致保证执行保真度或长期可复现性。

Conclusion: 研究提出了可操作的建议来减轻可复现性气味，并引入了可复现性成熟度模型，旨在超越二元构件认证，实现多维度、渐进式的可复现性严谨度评估。

Abstract: Reproducibility is a cornerstone of scientific progress, yet its state in large language model (LLM)-based software engineering (SE) research remains poorly understood. This paper presents the first large-scale, empirical study of reproducibility practices in LLM-for-SE research. We systematically mined and analyzed 640 papers published between 2017 and 2025 across premier software engineering, machine learning, and natural language processing venues, extracting structured metadata from publications, repositories, and documentation. Guided by four research questions, we examine (i) the prevalence of reproducibility smells, (ii) how reproducibility has evolved over time, (iii) whether artifact evaluation badges reliably reflect reproducibility quality, and (iv) how publication venues influence transparency practices. Using a taxonomy of seven smell categories: Code and Execution, Data, Documentation, Environment and Tooling, Versioning, Model, and Access and Legal, we manually annotated all papers and associated artifacts. Our analysis reveals persistent gaps in artifact availability, environment specification, versioning rigor, and documentation clarity, despite modest improvements in recent years and increased adoption of artifact evaluation processes at top SE venues. Notably, we find that badges often signal artifact presence but do not consistently guarantee execution fidelity or long-term reproducibility. Motivated by these findings, we provide actionable recommendations to mitigate reproducibility smells and introduce a Reproducibility Maturity Model (RMM) to move beyond binary artifact certification toward multi-dimensional, progressive evaluation of reproducibility rigor.

</details>


### [11] [Code Comments for Quantum Software Development Kits: An Empirical Study on Qiskit](https://arxiv.org/abs/2512.00766)
*Zenghui Zhou,Yuechen Li,Yi Cai,Jinlong Wen,Xiaohan Yu,Zheng Zheng,Beibei Yin*

Main category: cs.SE

TL;DR: 本文创建了首个量子计算代码注释数据集CC4Q，包含9677个代码注释对和21970个句子级注释单元，通过实证研究分析了量子软件注释的特点、开发者意图和量子相关主题。


<details>
  <summary>Details</summary>
Motivation: 量子计算日益受到关注，但程序员在理解量子软件时面临量子力学不直观的挑战。虽然量子SDK中的代码注释能帮助解释程序功能，但缺乏系统研究来评估其价值并提供指导。本文旨在填补这一空白。

Method: 以最流行的量子SDK Qiskit为研究对象，创建了CC4Q数据集，包含大量代码注释对和句子级注释单元。验证了经典程序中使用的开发者意图分类法在量子领域的适用性，并提出了考虑量子特定知识的新分类法。从注释结构与覆盖率、开发者意图和量子相关主题三个角度进行实证研究。

Result: 研究发现量子软件与经典软件在代码注释方面存在关键差异，并识别了与量子软件开发相关的量子特定知识。CC4Q数据集为量子软件注释研究提供了基础资源。

Conclusion: 本文首次系统研究了量子软件代码注释，创建了CC4Q数据集并进行了全面分析。研究揭示了量子软件注释的独特性，为量子软件开发提供了有价值的指导，有助于程序员更好地理解和维护量子软件。

Abstract: Quantum computing is gaining attention from academia and industry. With the quantum Software Development Kits (SDKs), programmers can develop quantum software to explore the power of quantum computing. However, programmers may face challenges in understanding quantum software due to the non-intuitive quantum mechanics. To facilitate software development and maintenance, code comments offered in quantum SDKs serve as a natural language explanation of program functionalities and logical flows. Despite their importance, scarce research systematically reports their value and provides constructive guidelines for programmers. To address this gap, our paper focuses on Qiskit, one of the most popular quantum SDKs, and presents CC4Q, the first dataset of code comments for quantum computing. CC4Q incorporates 9677 code comment pairs and 21970 sentence-level code comment units, the latter of which involve heavy human annotation. Regarding the annotation, we validate the applicability of the developer-intent taxonomy used in classical programs, and also propose a new taxonomy considering quantum-specific knowledge. We conduct an empirical study comprehensively interpreting code comments from three perspectives: comment structure and coverage, developers' intentions, and associated quantum topics. Our findings uncover key differences in code comments between classical and quantum software, and also outline quantum-specific knowledge relevant to quantum software development.

</details>


### [12] [FC-ADL: Efficient Microservice Anomaly Detection and Localisation Through Functional Connectivity](https://arxiv.org/abs/2512.00844)
*Giles Winchester,George Parisis,Luc Berthouze*

Main category: cs.SE

TL;DR: FC-ADL：基于功能连接性的微服务异常检测与定位方法，通过高效表征微服务指标间时变依赖关系，避免传统因果推断的高计算开销，实现大规模部署下的可扩展异常管理


<details>
  <summary>Details</summary>
Motivation: 微服务架构虽然提供了模块化和独立性，但带来了服务集成和系统管理的操作复杂性，使得快速准确的异常检测和定位变得困难。现有方法要么缺乏对时变依赖关系的显式建模，要么因依赖计算昂贵的因果推断而难以扩展到实际大规模部署

Method: 提出FC-ADL方法，基于神经科学中的功能连接性概念，通过高效表征微服务指标间时变依赖关系的变化来检测异常并提供根因候选，避免了因果推断和多变量方法的高计算开销

Result: 在多种故障场景下实现了顶尖的检测和定位性能，优于现有最先进方法；在阿里巴巴极大规模的真实微服务部署中验证了方法的可扩展性

Conclusion: FC-ADL通过功能连接性方法有效解决了微服务架构中异常检测和定位的挑战，既保持了高性能又具备大规模部署的可扩展性，为实际生产环境提供了实用解决方案

Abstract: Microservices have transformed software architecture through the creation of modular and independent services. However, they introduce operational complexities in service integration and system management that makes swift and accurate anomaly detection and localisation challenging. Despite the complex, dynamic, and interconnected nature of microservice architectures, prior works that investigate metrics for anomaly detection rarely include explicit information about time-varying interdependencies. And whilst prior works on fault localisation typically do incorporate information about dependencies between microservices, they scale poorly to real world large-scale deployments due to their reliance on computationally expensive causal inference. To address these challenges we propose FC-ADL, an end-to-end scalable approach for detecting and localising anomalous changes from microservice metrics based on the neuroscientific concept of functional connectivity. We show that by efficiently characterising time-varying changes in dependencies between microservice metrics we can both detect anomalies and provide root cause candidates without incurring the significant overheads of causal and multivariate approaches. We demonstrate that our approach can achieve top detection and localisation performance across a wide degree of different fault scenarios when compared to state-of-the-art approaches. Furthermore, we illustrate the scalability of our approach by applying it to Alibaba's extremely large real-world microservice deployment.

</details>


### [13] [The Software Infrastructure Attitude Scale (SIAS): A Questionnaire Instrument for Measuring Professionals' Attitudes Toward Technical and Sociotechnical Infrastructure](https://arxiv.org/abs/2512.00855)
*Miikka Kuutila,Paul Ralph,Huilian Sophie Qiu,Ronnie de Souza Santos,Morakot Choetkiertikul,Amin Milani Fard,Rana Alkadhi,Xavier Devroey,Gregorio Robles,Hideaki Hata,Sebastian Baltes,Vladimir Kovalenko,Shalini Chakraborty,Eray Tuzun,Hera Arif,Gianisa Adisaputri,Kelly Garcés,Anielle S. L. Andrade,Eyram Amedzor,Bimpe Ayoola,Keisha Gaspard-Chickoree,Arazoo Hoseyni*

Main category: cs.SE

TL;DR: 开发并验证了测量软件工程中技术和社会技术基础设施态度的心理测量量表，包含两个因子结构，具有良好的信效度。


<details>
  <summary>Details</summary>
Motivation: 软件工程研究需要社会技术研究，因此需要定制化的心理测量量表来测量对技术和社会技术基础设施的态度。

Method: 基于基础设施理论、态度理论和心理测量研究，定义目标构念并生成量表项目。对225名软件专业人员施测，使用分半样本进行探索性因子分析和验证性因子分析，评估表面效度、标准关联效度和区分效度。

Result: 探索性因子分析支持双因子结构（技术和社会技术基础设施），解释65%总方差；验证性因子分析确认模型拟合优度良好；量表具有良好的表面效度、内容效度、收敛效度、标准关联效度和区分效度。

Conclusion: 开发的量表是测量软件工程中技术和社会技术基础设施态度的有效工具，有助于将心理测量严谨性整合到实证和行为软件工程研究中。

Abstract: Context: Recent software engineering (SE) research has highlighted the need for sociotechnical research, implying a demand for customized psychometric scales. Objective: We define the concepts of technical and sociotechnical infrastructure in software engineering, and develop and validate a psychometric scale that measures attitudes toward them. Method: Grounded in theories of infrastructure, attitudes, and prior work on psychometric measurement, we defined the target constructs and generated scale items. The scale was administered to 225 software professionals and evaluated using a split sample. We conducted an exploratory factor analysis (EFA) on one half of the sample to uncover the underlying factor structure and performed a confirmatory factor analysis (CFA) on the other half to validate the structure. Further analyses with the whole sample assessed face, criterion-related, and discriminant validity. Results: EFA supported a two-factor structure (technical and sociotechnical infrastructure), accounting for 65% of the total variance with strong loadings. CFA confirmed excellent model fit. Face and content validity were supported by the item content reflecting cognitive, affective, and behavioral components. Both subscales were correlated with job satisfaction, perceived autonomy, and feedback from the job itself, supporting convergent validity. Regression analysis supported criterion-related validity, while the Heterotrait-Monotrait ratio of correlations (HTMT), the Fornell-Larcker criterion, and model comparison all supported discriminant validity. Discussion: The resulting scale is a valid instrument for measuring attitudes toward technical and sociotechnical infrastructure in software engineering research. Our work contributes to ongoing efforts to integrate psychological measurement rigor into empirical and behavioral software engineering research.

</details>


### [14] [Neural Variable Name Repair: Learning to Rename Identifiers for Readability](https://arxiv.org/abs/2512.01141)
*Muhammad Yousuf,Akshat Bagade,Chhittebbayi Penugonda,Maanas Baraya*

Main category: cs.SE

TL;DR: 研究变量名修复任务：当C++函数中某个变量名被占位符替换时，自动生成自然、描述性的变量名。基于Llama 3.1-8B构建包含微调优化、LoRA适配器和重排序器的pipeline，在真实C++代码上显著提升变量名修复效果。


<details>
  <summary>Details</summary>
Motivation: 开发者经常遇到变量名过于通用或误导性的源代码，许多函数缺乏文档，这降低了代码理解效率，增加了bug风险，也影响了人类和LLM对代码的推理能力。需要自动修复变量名来改善代码可读性和可维护性。

Method: 从BigCode的The Stack数据集中自动构建任务：使用Tree-sitter解析C++函数，掩码单个标识符，将原始名称作为监督信号。基于Llama 3.1-8B构建pipeline：1）使用预热和dropout调度实现更稳定的微调；2）LoRA适配器高效专门化标识符修复；3）双编码器重排序器对top-k候选进行重排序。

Result: 在200个C++函数的测试集上：零样本Llama 3.1基线达到6.1%精确匹配。最佳LoRA微调模型（带预热和dropout）达到43.1%精确匹配、50.2% Top-5命中率和82.03部分匹配分数。双编码器重排序器进一步提升了选择质量。

Conclusion: 任务特定的微调加上重排序是实用标识符修复工具的有前景方法。该方法显著提升了变量名修复的准确性，为改善代码可读性和开发效率提供了有效解决方案。

Abstract: Developers routinely work with source files whose variable names are generic or misleading, and with teams moving quickly, many functions are left undocumented. This slows comprehension, increases the risk of subtle bugs, and makes it harder for both humans and large language models (LLMs) to reason about code. We study variable name repair: given a real C++ function where all occurrences of one local or parameter name have been replaced by a placeholder (e.g. ID 1), the goal is to generate a natural, descriptive replacement name. We automatically construct this task from the C++ portion of BigCode's The Stack by parsing functions with Tree-sitter, masking a single identifier, and treating the original name as supervision. On top of Llama 3.1-8B, we build a pipeline with (i) warmup and dropout schedules for more stable fine-tuning, (ii) LoRA adapters for efficient specialization on identifier repair, and (iii) a dual-encoder reranker over top-k generator candidates. We evaluate using exact match, Top-5 Hit, and an embedding-based partial similarity score (0-100) that gives credit for near synonyms and format variants (e.g., jsonValue vs. json). On a held-out set of 200 C++ functions, a zero-shot Llama 3.1 baseline reaches 6.1 percent exact match. Our best LoRA-tuned model (with warmup and dropout) achieves 43.1 percent exact match, 50.2 percent Top-5 Hit, and an 82.03 partial-match score. A dual encoder reranker further improves selection quality without modifying the underlying generator, suggesting that task-specific fine-tuning plus reranking is a promising approach for practical identifier repair tools.

</details>


### [15] [LAURA: Enhancing Code Review Generation with Context-Enriched Retrieval-Augmented LLM](https://arxiv.org/abs/2512.01356)
*Yuxin Zhang,Yuxia Zhang,Zeyu Sun,Yanjie Jiang,Hui Liu*

Main category: cs.SE

TL;DR: LAURA是一个基于LLM的代码审查生成框架，通过整合审查示例检索、上下文增强和系统指导，显著提升了ChatGPT-4o和DeepSeek v3生成代码审查评论的质量。


<details>
  <summary>Details</summary>
Motivation: 随着软件规模和复杂性的快速增长，代码审查已成为开发过程中的瓶颈，因为其耗时、知识密集且缺乏经验丰富的开发者。现有自动生成代码审查的方法主要依赖历史代码变更和审查评论，但忽略了代码变更上下文和先前审查知识等关键信息。

Method: 提出了LAURA框架，这是一个基于LLM的审查知识增强、上下文感知的代码审查生成框架。该框架整合了三个核心组件：审查示例检索、上下文增强和系统指导，以提升LLM生成代码审查评论的性能。此外，针对现有数据集中大量低质量审查的问题，还构建了一个高质量数据集。

Result: 实验结果显示，对于ChatGPT-4o和DeepSeek v3模型，LAURA分别能在42.2%和40.4%的情况下生成完全正确或至少对开发者有帮助的审查评论，显著优于现有最先进的基线方法。消融研究进一步证明LAURA的所有组件都对提升评论质量有积极贡献。

Conclusion: LAURA框架通过整合审查知识增强和上下文感知机制，有效提升了LLM生成代码审查评论的质量，为解决代码审查瓶颈问题提供了有效的自动化解决方案。

Abstract: Code review is critical for ensuring software quality and maintainability. With the rapid growth in software scale and complexity, code review has become a bottleneck in the development process because of its time-consuming and knowledge-intensive nature and the shortage of experienced developers willing to review code. Several approaches have been proposed for automatically generating code reviews based on retrieval, neural machine translation, pre-trained models, or large language models (LLMs). These approaches mainly leverage historical code changes and review comments. However, a large amount of crucial information for code review, such as the context of code changes and prior review knowledge, has been overlooked. This paper proposes an LLM-based review knowledge-augmented, context-aware framework for code review generation, named LAURA. The framework integrates review exemplar retrieval, context augmentation, and systematic guidance to enhance the performance of ChatGPT-4o and DeepSeek v3 in generating code review comments. Besides, given the extensive low-quality reviews in existing datasets, we also constructed a high-quality dataset. Experimental results show that for both models, LAURA generates review comments that are either completely correct or at least helpful to developers in 42.2% and 40.4% of cases, respectively, significantly outperforming SOTA baselines. Furthermore, our ablation studies demonstrate that all components of LAURA contribute positively to improving comment quality.

</details>


### [16] [Teaching an Online Multi-Institutional Research Level Software Engineering Course with Industry - an Experience Report](https://arxiv.org/abs/2512.01523)
*Pankaj Jalote,Y. Raghu Reddy,Vasudeva Varma*

Main category: cs.SE

TL;DR: 在线多机构研究级课程实验，利用疫情后在线教学普及的优势，在软件工程高级主题领域进行跨机构合作教学，并引入行业专家参与


<details>
  <summary>Details</summary>
Motivation: 新冠疫情使在线教学成为常态，学生、教师和行业专业人士都适应了这种模式。这种舒适度可以被用来提供在线多机构研究级课程，特别是在单个机构可能缺乏足够师资或研究生的领域。如果主题对行业有吸引力，在线形式还能让行业专家轻松参与

Method: 在两个机构之间联合开设"AI in Software Engineering"课程，并积极邀请行业专家参与。采用在线协作教学模式，让不同机构的学生和行业专业人士共同参与课程

Result: 成功实施了跨机构在线研究级课程，分享了教师和学生的经验。证明了这种模式在软件工程高级主题领域的可行性

Conclusion: 这种协作教学方法可以应用于计算机科学任何应用领域的研究级课程，特别适合规模较小、难以独立提供研究级课程的机构。软件工程高级主题特别适合这种模式，因为行业通常愿意参与并贡献专业知识

Abstract: Covid has made online teaching and learning acceptable and students, faculty, and industry professionals are all comfortable with this mode. This comfort can be leveraged to offer an online multi-institutional research-level course in an area where individual institutions may not have the requisite faculty to teach and/or research students to enroll. If the subject is of interest to industry, online offering also allows industry experts to contribute and participate with ease. Advanced topics in Software Engineering are ideally suited for experimenting with this approach as industry, which is often looking to incorporate advances in software engineering in their practices, is likely to agree to contribute and participate. In this paper we describe an experiment in teaching a course titled "AI in Software Engineering" jointly between two institutions with active industry participation, and share our and student's experience. We believe this collaborative teaching approach can be used for offering research level courses in any applied area of computer science by institutions who are small and find it difficult to offer research level courses on their own.

</details>


### [17] [GPTrace: Effective Crash Deduplication Using LLM Embeddings](https://arxiv.org/abs/2512.01609)
*Patrick Herter,Vincent Ahlrichs,Ridvan Açilan,Julian Horsch*

Main category: cs.SE

TL;DR: GPTrace使用大语言模型计算崩溃数据嵌入向量，通过聚类算法实现崩溃去重，相比传统方法有明显改进


<details>
  <summary>Details</summary>
Motivation: 模糊测试发现大量崩溃输入，其中许多共享相同底层漏洞，但现有去重方法依赖栈追踪等信息的相似性比较，效果不理想，需要更有效的自动化去重方案

Method: 提出GPTrace去重工作流，利用大语言模型评估各种崩溃数据源的相似性，计算嵌入向量，并将这些向量作为聚类算法的输入

Result: 在14个不同目标的50个真实标签、超过30万个崩溃输入上评估，GPTrace的去重结果明显优于手工栈追踪比较方法和更复杂但灵活性较差的最新方法

Conclusion: GPTrace通过大语言模型和聚类算法的结合，显著提高了崩溃去重的效果，为模糊测试数据分析提供了更有效的自动化解决方案

Abstract: Fuzzing is a highly effective method for uncovering software vulnerabilities, but analyzing the resulting data typically requires substantial manual effort. This is amplified by the fact that fuzzing campaigns often find a large number of crashing inputs, many of which share the same underlying bug. Crash deduplication is the task of finding such duplicate crashing inputs and thereby reducing the data that needs to be examined. Many existing deduplication approaches rely on comparing stack traces or other information that is collected when a program crashes. Although various metrics for measuring the similarity of such pieces of information have been proposed, many do not yield satisfactory deduplication results. In this work, we present GPTrace, a deduplication workflow that leverages a large language model to evaluate the similarity of various data sources associated with crashes by computing embedding vectors and supplying those as input to a clustering algorithm. We evaluate our approach on over 300 000 crashing inputs belonging to 50 ground truth labels from 14 different targets. The deduplication results produced by GPTrace show a noticeable improvement over hand-crafted stack trace comparison methods and even more complex state-of-the-art approaches that are less flexible.

</details>


### [18] [When High-Performance Computing Meets Software Testing: Distributed Fuzzing using MPI](https://arxiv.org/abs/2512.01617)
*Pierciro Caliandro,Matteo Ciccaglione,Alessandro Pellegrini*

Main category: cs.SE

TL;DR: 将MPI同步技术集成到分布式模糊测试框架中，相比传统文件系统同步方法可显著提升性能，通过轻量级MPI原语减少通信延迟，提高分布式节点间的数据交换效率。


<details>
  <summary>Details</summary>
Motivation: 传统基于文件系统的分布式模糊测试同步方法存在通信延迟高、效率低的问题，限制了模糊测试的扩展性和性能，需要更高效的同步机制来提升分布式模糊测试的效果。

Method: 采用基于MPI（消息传递接口）的同步技术，利用轻量级MPI原语实现分布式模糊测试节点之间的高效数据交换和协调，替代传统的文件系统同步方法。

Result: 在标准基准测试中，MPI同步方法从模糊测试早期阶段就显示出更好的覆盖率进展，有效解决了覆盖率停滞问题，能够持续探索复杂和深层的执行路径。

Conclusion: MPI同步方法在分布式模糊测试中展现出显著提升扩展性和效能的潜力，特别适用于CI/CD流水线中的模糊测试应用。

Abstract: This paper explores the integration of MPI-based synchronization techniques into distributed fuzzing frameworks, highlighting possible substantial performance improvements compared to traditional filesystem-based synchronization methods. By employing lightweight MPI primitives, reductions in communication latency are achieved, facilitating more efficient data exchanges across distributed fuzzing nodes. Experimental results obtained over standard benchmarks demonstrate enhanced coverage progression from the early stages of the fuzzing process, which could be beneficial if fuzzing is employed in CI/CD pipelines at any stage of software development. Furthermore, the coordinated exchange of input corpora among clusters of fuzzers effectively addresses coverage stagnation, enabling a sustained exploration of complex and deep execution paths. Overall, the adoption of MPI-based synchronization approaches shows promising potential for significantly enhancing the scalability and efficacy of distributed fuzz testing.

</details>


### [19] [Package Dashboard: A Cross-Ecosystem Framework for Dual-Perspective Analysis of Software Packages](https://arxiv.org/abs/2512.01630)
*Ziheng Liu,Runzhi He,Minghui Zhou*

Main category: cs.SE

TL;DR: Package Dashboard是一个跨生态系统的软件供应链分析框架，通过整合包元数据、漏洞信息和上游社区健康指标，提供统一的风险评估平台，帮助开发者发现传统工具忽视的风险。


<details>
  <summary>Details</summary>
Motivation: 现有软件供应链分析工具存在局限性：通常局限于单一生态系统，且孤立地评估软件制品或社区活动。这种碎片化迫使开发者手动整合分散的数据，影响了风险评估的有效性。

Method: 提出Package Dashboard框架，整合包元数据、漏洞信息和上游社区健康指标，结合依赖解析和仓库分析，提供统一的双视角风险评估平台。

Result: 通过对5个Linux发行版中374,000个包的大规模研究，证明了框架的实用性，不仅能发现传统漏洞和许可证冲突，还能识别归档或不可访问仓库等被忽视的风险。

Conclusion: Package Dashboard提供了统一的风险视图，为开发者和DevSecOps工程师提供可操作的见解，增强了开源生态系统的透明度、可信度和可追溯性。

Abstract: Software supply chain attacks have revealed blind spots in existing SCA tools, which are often limited to a single ecosystem and assess either software artifacts or community activity in isolation. This fragmentation across tools and ecosystems forces developers to manually reconcile scattered data, undermining risk assessments. We present Package Dashboard, a cross-ecosystem framework that provides a unified platform for supply chain analysis, enabling a holistic, dual-perspective risk assessment by integrating package metadata, vulnerability information, and upstream community health metrics. By combining dependency resolution with repository analysis, it reduces cognitive load and improves traceability. Demonstrating the framework's versatility, a large-scale study of 374,000 packages across five Linux distributions shows its ability to uncover not only conventional vulnerabilities and license conflicts but also overlooked risks such as archived or inaccessible repositories. Ultimately, Package Dashboard provides a unified view of risk, equipping developers and DevSecOps engineers with actionable insights to strengthen the transparency, trustworthiness, and traceability of open-source ecosystems. Package Dashboard is publicly available at https://github.com/n19htfall/PackageDashboard, and a demonstration video can be found at https://youtu.be/y9ncftP8KPQ. Besides, the online version is available at https://pkgdash.osslab-pku.org.

</details>


### [20] [MIT Lincoln Laboratory: A Case Study on Improving Software Support for Research Projects](https://arxiv.org/abs/2512.01649)
*Daniel Strassler,Gabe Elkin,Curran Schiefelbein,Daniel Herring,Ian Jessen,David Johnson,Santiago A. Paredes,Tod Shannon,Jim Flavin*

Main category: cs.SE

TL;DR: 林肯实验室研究软件工程挑战，提出改进软件开发和文化的建议


<details>
  <summary>Details</summary>
Motivation: 软件在复杂系统开发和原型设计中作用日益重要，林肯实验室希望改进软件工程的有效性和文化，以更好地执行其使命

Method: 国土保护与空中交通管制部门进行内部研究，分析研究软件开发面临的挑战，识别加强文化和执行的方法

Result: 研究发现分为三类：影响软件开发活动的项目属性、集中化带来的潜在效率、改进软件从业者人员配置和文化的机会

Conclusion: 研究提出可操作建议：集中化和标准化软件支持工具、开发通用数据库匹配软件人才与项目需求、创建软件利益相关者小组以持续改进

Abstract: Software plays an ever increasing role in complex system development and prototyping, and in recent years, MIT Lincoln Laboratory has sought to improve both the effectiveness and culture surrounding software engineering in execution of its mission. The Homeland Protection and Air Traffic Control Division conducted an internal study to examine challenges to effective and efficient research software development, and to identify ways to strengthen both the culture and execution for greater impact on our mission. Key findings of this study fell into three main categories: project attributes that influence how software development activities must be conducted and managed, potential efficiencies from centralization, opportunities to improve staffing and culture with respect to software practitioners. The study delivered actionable recommendations, including centralizing and standardizing software support tooling, developing a common database to help match the right software talent and needs to projects, and creating a software stakeholder panel to assist with continued improvement.

</details>


### [21] [Generating REST API Tests With Descriptive Names](https://arxiv.org/abs/2512.01690)
*Philip Garrett,Juan P. Galeotti,Andrea Arcuri,Alexander Poth,Olsi Rrjolli*

Main category: cs.SE

TL;DR: 本文提出了三种确定性技术为REST API测试生成描述性名称，并与LLM方法比较，发现规则方法在清晰度上表现最佳，与先进LLM相当，显著优于GPT-3.5，且工业案例证实能提升测试可读性。


<details>
  <summary>Details</summary>
Motivation: 自动化生成的API测试用例通常使用非描述性名称（如test0、test1），这降低了测试的可读性，阻碍了在理解和维护过程中的实用性。需要为REST API测试生成描述性名称以改善开发体验。

Method: 提出了三种确定性技术生成REST API测试名称，并与LLM方法（包括Gemini、GPT-4o、GPT-3.5）共八种技术进行比较。使用EvoMaster模糊测试工具为9个开源API生成10个测试用例。通过两项调查（涉及39人）进行实证评估，并在大众汽车公司进行工业案例研究，评估74个测试用例。

Result: 规则方法在确定性方法中获得最高清晰度评分，与Gemini和GPT-4o等先进LLM模型表现相当，显著优于GPT-3.5。工业案例研究证实描述性名称能提高测试套件的可读性。

Conclusion: 轻量级确定性技术可以作为计算成本高且存在安全风险的LLM方法的有效替代方案，为自动化系统级测试命名提供了实用解决方案，使API测试生成更加开发者友好。

Abstract: Automated test generation has become a key technique for ensuring software quality, particularly in modern API-based architectures. However, automatically generated test cases are typically assigned non-descriptive names (e.g., test0, test1), which reduces their readability and hinders their usefulness during comprehension and maintenance. In this work, we present three novel deterministic techniques to generate REST API test names. We then compare eight techniques in total for generating descriptive names for REST API tests automatically produced by the fuzzer EvoMaster, using 10 test cases generated for 9 different open-source APIs. The eight techniques include rule-based heuristics and large language model (LLM)-based approaches. Their effectiveness was empirically evaluated through two surveys (involving up to 39 people recruited via LinkedIn). Our results show that a rule-based approach achieves the highest clarity ratings among deterministic methods, performs on par with state-of-the-art LLM-based models such as Gemini and GPT-4o, and significantly outperforms GPT-3.5.
  To further evaluate the practical impact of our results, an industrial case study was carried out with practitioners who actively use EvoMaster at Volkswagen AG. A developer questionnaire was then carried out based on the use of EvoMaster on four different APIs by four different users, for a total of 74 evaluated test cases. Feedback from practitioners further confirms that descriptive names produced by this approach improve test suite readability.
  These findings highlight that lightweight, deterministic techniques can serve as effective alternatives to computationally expensive and security-sensitive LLM-based approaches for automated system-level test naming, providing a practical step toward more developer-friendly API test generation.

</details>


### [22] [An Empirical Study of Agent Developer Practices in AI Agent Frameworks](https://arxiv.org/abs/2512.01939)
*Yanlin Wang,Xinyi Xu,Jiachi Chen,Tingting Bi,Wenchao Gu,Zibin Zheng*

Main category: cs.SE

TL;DR: 本文对LLM驱动的智能体框架进行了首次实证研究，通过分析11,910个开发者讨论，从五个维度比较了十个主流框架的优劣，揭示了框架设计中的共性问题，并为未来框架开发和开发者选择提供了指导。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的兴起，智能体框架快速发展，但实际应用效果和开发过程影响缺乏深入研究。开发者面临选择困难，不同框架存在相似问题，需要系统性的实证分析来指导框架改进和开发实践。

Method: 收集十个主流智能体框架的11,910个开发者讨论，通过定性分析比较框架在五个关键维度上的表现：开发效率、功能抽象、学习成本、性能优化和可维护性。

Result: 研究发现不同框架在满足开发者需求方面存在显著差异，揭示了框架设计中的共性问题。超过80%的开发者难以选择最适合其需求的框架，各框架在五个维度上的表现各有优劣。

Conclusion: 本研究为LLM驱动的智能体框架生态系统提供了实证基础，指出了框架设计改进方向，并为开发者选择框架和未来框架设计提供了实用见解。

Abstract: The rise of large language models (LLMs) has sparked a surge of interest in agents, leading to the rapid growth of agent frameworks. Agent frameworks are software toolkits and libraries that provide standardized components, abstractions, and orchestration mechanisms to simplify agent development. Despite widespread use of agent frameworks, their practical applications and how they influence the agent development process remain underexplored. Different agent frameworks encounter similar problems during use, indicating that these recurring issues deserve greater attention and call for further improvements in agent framework design. Meanwhile, as the number of agent frameworks continues to grow and evolve, more than 80% of developers report difficulties in identifying the frameworks that best meet their specific development requirements. In this paper, we conduct the first empirical study of LLM-based agent frameworks, exploring real-world experiences of developers in building AI agents. To compare how well the agent frameworks meet developer needs, we further collect developer discussions for the ten previously identified agent frameworks, resulting in a total of 11,910 discussions. Finally, by analyzing these discussions, we compare the frameworks across five dimensions: development efficiency, functional abstraction, learning cost, performance optimization, and maintainability, which refers to how easily developers can update and extend both the framework itself and the agents built upon it over time. Our comparative analysis reveals significant differences among frameworks in how they meet the needs of agent developers. Overall, we provide a set of findings and implications for the LLM-driven AI agent framework ecosystem and offer insights for the design of future LLM-based agent frameworks and agent developers.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [23] [HMARK: Radioactive Multi-Bit Semantic-Latent Watermarking for Diffusion Models](https://arxiv.org/abs/2512.00094)
*Kexin Li,Guozhen Ding,Ilya Grishchenko,David Lie*

Main category: cs.CR

TL;DR: HMARK是一种用于图像扩散模型的多位水印方案，通过在语义潜在空间（h-space）嵌入所有权信息，实现了放射性、鲁棒性和不可感知性，能有效检测未经授权的训练数据使用。


<details>
  <summary>Details</summary>
Motivation: 现代生成扩散模型依赖大量训练数据，其中可能包含所有权不明确或使用权限不确定的图像。放射性水印可以帮助检测未经授权的数据是否被用于训练，但现有水印方案需要同时满足不可感知性、鲁棒性和多位容量等多重需求。

Method: HMARK通过在图像扩散模型的语义潜在空间（h-space）中编码秘密比特作为所有权信息。利用h-space的可解释性和语义重要性，确保水印信号对应有意义的语义属性，从而实现放射性、鲁棒性和对感知质量的最小影响。

Result: 实验结果显示，HMARK在经LoRA微调的下游对抗模型生成的水印图像上，在各种失真情况下实现了98.57%的水印检测准确率、95.07%的比特级恢复准确率、100%的召回率和1.0的AUC值。

Conclusion: HMARK提出了一种有效的多比特水印方案，通过在扩散模型的语义潜在空间嵌入水印，成功实现了放射性、鲁棒性和不可感知性的平衡，为保护图像免受未经授权的训练使用提供了可靠解决方案。

Abstract: Modern generative diffusion models rely on vast training datasets, often including images with uncertain ownership or usage rights. Radioactive watermarks -- marks that transfer to a model's outputs -- can help detect when such unauthorized data has been used for training. Moreover, aside from being radioactive, an effective watermark for protecting images from unauthorized training also needs to meet other existing requirements, such as imperceptibility, robustness, and multi-bit capacity. To overcome these challenges, we propose HMARK, a novel multi-bit watermarking scheme, which encodes ownership information as secret bits in the semantic-latent space (h-space) for image diffusion models. By leveraging the interpretability and semantic significance of h-space, ensuring that watermark signals correspond to meaningful semantic attributes, the watermarks embedded by HMARK exhibit radioactivity, robustness to distortions, and minimal impact on perceptual quality. Experimental results demonstrate that HMARK achieves 98.57% watermark detection accuracy, 95.07% bit-level recovery accuracy, 100% recall rate, and 1.0 AUC on images produced by the downstream adversarial model finetuned with LoRA on watermarked data across various types of distortions.

</details>


### [24] [Quantum-Adversary-Resilient Evidence Structures and Migration Strategies for Regulated AI Audit Trails](https://arxiv.org/abs/2512.00110)
*Leo Kao*

Main category: cs.CR

TL;DR: 该论文研究了面向量子计算威胁的恒定大小加密证据记录系统，提出了量子安全的形式化定义和迁移策略，确保受监管AI工作负载审计日志的长期安全性。


<details>
  <summary>Details</summary>
Motivation: 当前用于临床、制药和金融等受监管AI工作负载的恒定大小加密证据记录系统主要依赖经典签名方案，这些方案面临量子计算能力的对手的长期安全威胁，需要研究后量子安全的解决方案。

Method: 1. 形式化量子对手下的证据结构安全概念（Q-Audit Integrity、Q-Non-Equivocation、Q-Binding）；2. 在量子随机预言机模型(QROM)中分析哈希-签名实例化；3. 提出三种迁移模式：混合签名、重新签名遗留证据、Merkle根锚定；4. 基于工业案例进行实证研究。

Result: 在标准假设下，基于存在性不可伪造的后量子签名方案构建的证据结构能够满足量子安全概念；三种迁移策略在安全性、存储和计算开销方面各有权衡；工业案例研究表明量子安全审计跟踪可实现适度开销，系统迁移能显著延长现有部署的证据寿命。

Conclusion: 量子安全的审计跟踪是可实现的，通过形式化安全定义、后量子签名方案和系统迁移策略，能够保护受监管AI工作负载的加密证据记录免受量子计算威胁，延长现有系统的使用寿命。

Abstract: Constant-size cryptographic evidence records are increasingly used to build audit trails for regulated AI workloads in clinical, pharmaceutical, and financial settings, where each execution is summarized by a compact, verifiable record of code identity, model version, data digests, and platform measurements. Existing instantiations, however, typically rely on classical signature schemes whose long-term security is threatened by quantum-capable adversaries. In this paper we formalize security notions for evidence structures in the presence of quantum adversaries and study post-quantum (PQ) instantiations and migration strategies for deployed audit logs. We recall an abstraction of constant-size evidence structures and introduce game-based definitions of Q-Audit Integrity, Q-Non-Equivocation, and Q-Binding, capturing the inability of a quantum adversary to forge, equivocate, or rebind evidence items. We then analyze a hash-and-sign instantiation in the quantum random-oracle model (QROM), assuming an existentially unforgeable PQ signature scheme against quantum adversaries, and show that the resulting evidence structure satisfies these notions under standard assumptions. Building on this, we present three migration patterns for existing evidence logs: hybrid signatures, re-signing of legacy evidence, and Merkle-root anchoring, and analyze their security, storage, and computational trade-offs. A case study based on an industrial constant-size evidence platform for regulated AI at Codebat Technologies Inc. suggests that quantum-safe audit trails are achievable with moderate overhead and that systematic migration can significantly extend the evidentiary lifetime of existing deployments.

</details>


### [25] [DeFi TrustBoost: Blockchain and AI for Trustworthy Decentralized Financial Decisions](https://arxiv.org/abs/2512.00142)
*Swati Sachan,Dale S. Fickett*

Main category: cs.CR

TL;DR: DeFi TrustBoost Framework结合区块链和可解释AI，解决低收入家庭小企业贷款审批中的信任问题，强调保密性、合规性、抗攻击性和可审计性。


<details>
  <summary>Details</summary>
Motivation: 解决金融机构在审批低收入家庭小企业贷款申请时面临的信任挑战，特别是如何确保贷款决策过程的透明度、安全性和合规性。

Method: 结合区块链技术和可解释AI，设计满足保密性、数据保护合规、抗对抗攻击和监管审计要求的框架，包括防篡改审计技术和链上链下数据存储策略。

Result: 提出了一个完整的DeFi TrustBoost框架，能够实现自动化AI决策的防篡改审计，并支持金融机构内外的协作。

Conclusion: 该框架为DeFi领域的小企业贷款审批提供了可信、透明且合规的技术解决方案，有助于促进金融包容性。

Abstract: This research introduces the Decentralized Finance (DeFi) TrustBoost Framework, which combines blockchain technology and Explainable AI to address challenges faced by lenders underwriting small business loan applications from low-wealth households. The framework is designed with a strong emphasis on fulfilling four crucial requirements of blockchain and AI systems: confidentiality, compliance with data protection laws, resistance to adversarial attacks, and compliance with regulatory audits. It presents a technique for tamper-proof auditing of automated AI decisions and a strategy for on-chain (inside-blockchain) and off-chain data storage to facilitate collaboration within and across financial organizations.

</details>


### [26] [Measuring Memecoin Fragility](https://arxiv.org/abs/2512.00377)
*Yuexin Xiang,SM Mahir Shazeed Rish,Qishuang Fu,Yuquan Li,Qin Wang,Tsz Hon Yuen,Jiangshan Yu*

Main category: cs.CR

TL;DR: 本文提出了首个模因币生态系统脆弱性框架(ME2F)，用于量化模因币在波动性、所有权集中度和情绪放大三个维度的风险，发现政治主题模因币风险最高，而以太坊等主流代币最为稳健。


<details>
  <summary>Details</summary>
Motivation: 模因币作为从互联网文化和社区叙事中诞生的独特加密资产类别，其市场动态主要由社交媒体传播、名人影响和投机资本驱动，与传统技术驱动的加密货币不同。目前缺乏系统性的框架来评估这类资产的脆弱性风险。

Method: 提出了模因币生态系统脆弱性框架(ME2F)，该框架从三个维度形式化模因币风险：1)波动动态评分：捕捉持续和极端的价格波动以及基础链的溢出效应；2)鲸鱼主导评分：量化顶级持有者的所有权集中度；3)情绪放大评分：衡量注意力驱动冲击对市场稳定性的影响。

Result: 应用ME2F分析代表性代币(覆盖超过65%市场份额)发现：1)政治主题代币(如TRUMP、MELANIA、LIBRA)风险最高，结合了波动性、所有权集中度和情绪冲击敏感性；2)成熟模因币(如DOGE、SHIB、PEPE)处于中等风险范围；3)基准代币(ETH和SOL)由于更深的流动性和机构参与而保持稳定。

Conclusion: 这是首个提供模因币生态系统脆弱性证据的研究，揭示了模因币风险的异质性分布，为Web3时代增强市场韧性的治理提供了重要启示。

Abstract: Memecoins, emerging from internet culture and community-driven narratives, have rapidly evolved into a unique class of crypto assets. Unlike technology-driven cryptocurrencies, their market dynamics are primarily shaped by viral social media diffusion, celebrity influence, and speculative capital inflows.
  To capture the distinctive vulnerabilities of these ecosystems, we present the first Memecoin Ecosystem Fragility Framework (ME2F). ME2F formalizes memecoin risks in three dimensions: i) Volatility Dynamics Score capturing persistent and extreme price swings together with spillover from base chains; ii) Whale Dominance Score quantifying ownership concentration among top holders; and iii) Sentiment Amplification Score measuring the impact of attention-driven shocks on market stability.
  We apply ME2F to representative tokens (over 65\% market share) and show that fragility is not evenly distributed across the ecosystem. Politically themed tokens such as TRUMP, MELANIA, and LIBRA concentrate the highest risks, combining volatility, ownership concentration, and sensitivity to sentiment shocks. Established memecoins such as DOGE, SHIB, and PEPE fall into an intermediate range. Benchmark tokens ETH and SOL remain consistently resilient due to deeper liquidity and institutional participation. Our findings provide the first ecosystem-level evidence of memecoin fragility and highlight governance implications for enhancing market resilience in the Web3 era.

</details>


### [27] [Red Teaming Large Reasoning Models](https://arxiv.org/abs/2512.00412)
*Jiawei Chen,Yang Yang,Chao Yu,Yu Tian,Zhi Cao,Linghao Li,Hang Su,Zhaoxia Yin*

Main category: cs.CR

TL;DR: RT-LRM是一个评估大型推理模型可信度的统一基准，涵盖真实性、安全性和效率三个维度，通过30个推理任务对26个模型进行实验，发现LRM存在可信度挑战且比LLM更脆弱。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在多步推理任务中表现出色，但引入了新的安全和可靠性风险（如思维链劫持和提示诱导低效），现有评估方法无法充分捕捉这些风险，需要专门的可信度评估基准。

Method: 提出RT-LRM统一基准，从真实性、安全性和效率三个维度评估LRM可信度；引入训练范式作为分析视角，设计30个推理任务套件，对26个模型进行广泛实验。

Result: 实验发现LRM普遍面临可信度挑战，在遇到推理诱导风险时比大型语言模型更脆弱；揭示了先前未被充分探索的脆弱性，强调了针对性评估的必要性。

Conclusion: RT-LRM基准填补了LRM可信度评估的空白，揭示了LRM的脆弱性，提供了标准化研究工具箱，将促进该领域未来发展，代码和数据集将开源。

Abstract: Large Reasoning Models (LRMs) have emerged as a powerful advancement in multi-step reasoning tasks, offering enhanced transparency and logical consistency through explicit chains of thought (CoT). However, these models introduce novel safety and reliability risks, such as CoT-hijacking and prompt-induced inefficiencies, which are not fully captured by existing evaluation methods. To address this gap, we propose RT-LRM, a unified benchmark designed to assess the trustworthiness of LRMs. RT-LRM evaluates three core dimensions: truthfulness, safety and efficiency. Beyond metric-based evaluation, we further introduce the training paradigm as a key analytical perspective to investigate the systematic impact of different training strategies on model trustworthiness. We achieve this by designing a curated suite of 30 reasoning tasks from an observational standpoint. We conduct extensive experiments on 26 models and identify several valuable insights into the trustworthiness of LRMs. For example, LRMs generally face trustworthiness challenges and tend to be more fragile than Large Language Models (LLMs) when encountering reasoning-induced risks. These findings uncover previously underexplored vulnerabilities and highlight the need for more targeted evaluations. In addition, we release a scalable toolbox for standardized trustworthiness research to support future advancements in this important field. Our code and datasets will be open-sourced.

</details>


### [28] [BEACON: Automatic Container Policy Generation using Environment-aware Dynamic Analysis](https://arxiv.org/abs/2512.00414)
*Haney Kang,Eduard Marin,Myoungsung You,Diego Perino,Seungwon Shin,Jinwoo Kim*

Main category: cs.CR

TL;DR: BeaCon是一个基于动态分析的自动化容器安全策略生成工具，通过模拟真实环境发现隐藏的执行路径，使用启发式方法探索配置空间，结合安全性和功能性评分机制，在保持容器功能性的同时增强安全性。


<details>
  <summary>Details</summary>
Motivation: 现有容器安全策略生成方法存在局限性，无法发现容器在真实环境中可能暴露的隐藏执行路径，导致安全策略不完整。需要一种能够平衡安全性和功能性的自动化工具，帮助云服务提供商在保障租户可用性的同时增强容器安全。

Method: 1. 使用动态分析模拟真实环境，发现容器隐藏的执行路径；2. 采用高效启发式方法探索庞大的配置空间，以最小代价揭示额外系统事件；3. 引入安全性和功能性评分机制，根据系统调用和capabilities对主机内核安全性和容器应用功能的影响进行优先级排序；4. 基于eBPF内核技术实现原型系统。

Result: 1. 在Top 15容器评估中，通过应用多样化环境，BeaCon平均识别出16.5%的额外系统调用；2. 对45个已知漏洞（如CVEs）的评估显示，BeaCon能有效缓解相关风险；3. 针对两个知名安全漏洞的概念验证演示表明，BeaCon成功通过阻止这些攻击来减少攻击面。

Conclusion: BeaCon通过动态分析和智能评分机制，实现了安全性与功能性的定制化平衡，显著增强了容器安全性，同时保持了应用可用性，为云服务提供商提供了一种有效的容器安全策略自动化生成解决方案。

Abstract: This paper introduces BeaCon, a novel tool for the automated generation of adjustable container security policies. Unlike prior approaches, BeaCon leverages dynamic analysis to simulate realistic environments, uncovering container execution paths that may remain hidden during the profiling phase. To address the challenge of exploring vast profiling spaces, we employ efficient heuristics to reveal additional system events with minimal effort. In addition, BeaCon incorporates a security and functionality scoring mechanism to prioritize system calls and capabilities based on their impact on the host OS kernel's security and the functionality of containerized applications. By integrating these scores, BeaCon achieves a customized balance between security and functionality, enabling cloud providers to enforce security measures while maintaining tenant availability. We implemented a prototype of BeaCon using eBPF kernel technology and conducted extensive evaluations. Results from the top 15 containers, which revealed significant improvements, demonstrate that BeaCon identifies an average of 16.5% additional syscalls by applying diverse environments. Furthermore, we evaluated its effectiveness in mitigating risks associated with 45 known vulnerabilities (e.g., CVEs), showcasing its potential to significantly enhance container security. Additionally, we performed proof-of-concept demonstrations for two well-known security vulnerabilities, showing that BeaCon successfully reduces attack surface by blocking these exploits.

</details>


### [29] [A Unified Framework for Constructing Information-Theoretic Private Information Retrieval](https://arxiv.org/abs/2512.00480)
*Liang Feng Zhang*

Main category: cs.CR

TL;DR: 本文提出了一种新的离散结构FOASC，并建立了一个统一的框架来构建信息论私有信息检索协议，能够涵盖文献中最有影响力的IT-PIR协议，同时提出了若干开放性问题。


<details>
  <summary>Details</summary>
Motivation: 私有信息检索协议在保护用户隐私方面具有重要意义，但现有构造主要分为多服务器信息论PIR和单服务器计算PIR。信息论PIR在计算效率和安全性方面具有优势，但其通信复杂度一直是该领域最经典和最具挑战性的问题。

Method: 引入一种新的离散结构——具有跨度能力的正交阵列族（FOASC），并基于此提出了一个统一的框架来构造信息论私有信息检索协议。该框架能够涵盖文献中最有影响力的IT-PIR协议。

Result: 建立了一个统一的IT-PIR协议构造框架，展示了如何将文献中最有影响力的IT-PIR协议纳入该框架。同时提出了关于FOASC的若干有趣开放性问题。

Conclusion: FOASC为IT-PIR协议提供了一个统一的构造框架，解决了该领域的核心通信复杂度问题。提出的开放性问题可能催生创新的IT-PIR协议。

Abstract: Retrieving up-to-date information from a publicly accessible database poses significant threats to the user's privacy. {\em Private information retrieval} (PIR) protocols allow a user to retrieve any entry from a database, without revealing the identity of the entry being retrieved to the server(s). Such protocols have found numerous applications in both theoretical studies and real-life scenarios. The existing PIR constructions mainly give multi-server {\em information-theoretic} PIR (IT-PIR) protocols or single-server computational PIR (CPIR) protocols. Compared with CPIR, IT-PIR protocols are computationally more efficient and secure in the presence of unbounded servers. The most classical and challenging problem in the realm of IT-PIR is constructing protocols with lower {\em communication complexity}. In this review, we introduce a new discrete structure called {\em families of orthogonal arrays with span capability} (FOASC) and propose a unified framework for constructing IT-PIR protocols. We show how the most influential IT-PIR protocols in the literature can be captured by the framework. We also put forward several interesting open problems concerning FOASC, whose solutions may result in innovative IT-PIR protocols.

</details>


### [30] [Extended Abstract: Synthesizable Low-overhead Circuit-level Countermeasures and Pro-Active Detection Techniques for Power and EM SCA](https://arxiv.org/abs/2512.00635)
*Archisman Ghosh*

Main category: cs.CR

TL;DR: 该论文提出针对物联网设备侧信道攻击的零开销防护方案，包括集成电感传感器检测电磁侧信道攻击和故障注入攻击，并贡献了首个硅验证的Saber后量子密码算法实现。


<details>
  <summary>Details</summary>
Motivation: 物联网设备虽然采用数学安全的加密算法，但硬件实现会通过功耗、电磁辐射等侧信道泄露信息，现有防护方案开销过高，不适合能源受限的物联网设备。同时量子计算机的发展威胁现有加密协议，需要后量子密码算法。

Method: 提出零开销集成电感传感器，使用简单机器学习算法检测电磁侧信道攻击、时钟毛刺故障注入攻击和电压毛刺故障注入攻击。同时实现并硅验证了NIST后量子密码标准候选算法Saber（基于模取整学习方案）。

Result: 集成电感传感器能有效检测三种攻击类型。Saber实现达到了迄今为止所有候选算法中最低的能耗和面积，并成为首个硅验证的Saber实现。

Conclusion: 该研究为物联网设备提供了实用的低开销侧信道攻击防护方案，并为后量子密码标准化进程做出了实质性贡献，推动了安全硬件设计的发展。

Abstract: The gamut of todays internet-connected embedded devices has led to increased concerns regarding the security and confidentiality of data. Most internet-connected embedded devices employ mathematically secure cryptographic algorithms to address security vulnerabilities. Despite such mathematical guarantees, as these algorithms are often implemented in silicon, they leak critical information in terms of power consumption, electromagnetic (EM) radiation, timing, cache hits and misses, photonic emission and so on, leading to side-channel analysis (SCA) attacks. This thesis focuses on low overhead generic circuit-level yet synthesizable countermeasures against power and EM SCA. Existing countermeasures (including proposed) still have relatively high overhead which bars them from being used in energy-constraint IoT devices. We propose a zero-overhead integrated inductive sensor which is able to detect i)EM SCA ii) Clock glitch-based Fault Injection Attack (FIA), and iii) Voltage-glitch based Fault Injection Attack by using a simple ML algorithm. Advent of quantum computer research will open new possibilities for theoretical attacks against existing cryptographic protocols. National Institute of Standard & Technology (NIST) has standardized post-quantum cryptographic algorithms to secure crypto-systems against quantum adversary. I contribute to the standardization procedure by introducing the first silicon-verified Saber (a NIST finalist modulo Learning with Rounding scheme) which consumes lowest energy and area till date amongst all the candidates.

</details>


### [31] [Blockchain-based vs. SQL Database Systems for Digital Twin Evidence Management: A Comparative Forensic Analysis](https://arxiv.org/abs/2512.00645)
*Boyd Franken,Hong-Hanh Nguyen-Le,Nhien-An Le-Khac*

Main category: cs.CR

TL;DR: 本文首次比较了区块链与传统数据库在数字孪生证据管理中的表现，发现区块链在数据完整性和不可篡改性方面更优，而传统数据库在性能一致性方面更好。


<details>
  <summary>Details</summary>
Motivation: 随着数字孪生和元宇宙技术的出现，数字取证面临前所未有的挑战。需要研究如何有效管理数字孪生证据，为新兴技术开发稳健的数字取证方法。

Method: 进行了受控实验，比较以太坊区块链与IPFS存储系统与传统SQL数据库在数字孪生证据管理方面的性能。通过哈希验证确保取证完整性。

Result: 区块链在数据完整性和不可篡改性方面表现更优，对法律程序至关重要；传统数据库在性能一致性方面更好。区块链存储时间更快但检索操作变异性更高。

Conclusion: 这项研究为元宇宙时代新兴技术的数字取证方法学发展做出了贡献，表明区块链的不可篡改特性为法律程序提供了额外的安全保障。

Abstract: Digital forensics faces unprecedented challenges with the emergence of digital twins and metaverse technologies. This paper presents the first comparative analysis between blockchain-based and traditional database systems for managing digital twin evidence in forensic investigations. We conducted controlled experiments comparing the Ethereum blockchain with IPFS storage against traditional SQL databases for digital twin evidence management. Our findings reveal that while blockchain provides superior data integrity and immutability, crucial for forensic applications, traditional databases offer better performance consistency. The blockchain implementation showed faster average storage times but higher variability in retrieval operations. Both systems maintained forensic integrity through hash verification, though blockchain's immutable nature provides additional security guarantees essential for legal proceedings. This research contributes to the development of robust digital forensic methodologies for emerging technologies in the metaverse era.

</details>


### [32] [Concept-Guided Backdoor Attack on Vision Language Models](https://arxiv.org/abs/2512.00713)
*Haoyu Shen,Weimin Lyu,Haotian Xu,Tengfei Ma*

Main category: cs.CR

TL;DR: 论文提出概念引导的后门攻击新范式，通过语义概念而非像素级触发器来攻击视觉语言模型，包括CTP和CGUB两种攻击方法，实验证明攻击成功率高且对正常任务影响小。


<details>
  <summary>Details</summary>
Motivation: 现有针对视觉语言模型的后门攻击主要依赖像素级触发器或不可察觉的图像扰动，这些方法隐蔽性差且容易被基于图像的防御机制检测。需要探索更隐蔽、基于语义概念层面的攻击方法。

Method: 提出两种概念引导的后门攻击：1) CTP攻击使用自然图像中的显式概念作为触发器，仅对包含目标概念的样本进行投毒；2) CGUB攻击在训练时利用概念瓶颈模型干预内部概念激活，在推理时丢弃CBM分支以保持模型不变，实现系统性的标签替换。

Result: 在多种视觉语言模型架构和数据集上的实验表明，CTP和CGUB都能实现高攻击成功率，同时对干净任务的性能影响适中，证明了概念级漏洞是视觉语言模型的重要新攻击面。

Conclusion: 概念引导的后门攻击为视觉语言模型安全提出了新的挑战，概念级漏洞成为关键的新攻击面，需要开发相应的防御机制来应对这种语义层面的威胁。

Abstract: Vision-Language Models (VLMs) have achieved impressive progress in multimodal text generation, yet their rapid adoption raises increasing concerns about security vulnerabilities. Existing backdoor attacks against VLMs primarily rely on explicit pixel-level triggers or imperceptible perturbations injected into images. While effective, these approaches reduce stealthiness and remain vulnerable to image-based defenses. We introduce concept-guided backdoor attacks, a new paradigm that operates at the semantic concept level rather than on raw pixels. We propose two different attacks. The first, Concept-Thresholding Poisoning (CTP), uses explicit concepts in natural images as triggers: only samples containing the target concept are poisoned, causing the model to behave normally in all other cases but consistently inject malicious outputs whenever the concept appears. The second, CBL-Guided Unseen Backdoor (CGUB), leverages a Concept Bottleneck Model (CBM) during training to intervene on internal concept activations, while discarding the CBM branch at inference time to keep the VLM unchanged. This design enables systematic replacement of a targeted label in generated text (for example, replacing "cat" with "dog"), even when the replacement behavior never appears in the training data. Experiments across multiple VLM architectures and datasets show that both CTP and CGUB achieve high attack success rates while maintaining moderate impact on clean-task performance. These findings highlight concept-level vulnerabilities as a critical new attack surface for VLMs.

</details>


### [33] [MASCOT: Analyzing Malware Evolution Through A Well-Curated Source Code Dataset](https://arxiv.org/abs/2512.00741)
*Bojing Li,Duo Zhong,Dharani Nadendla,Gabriel Terceros,Prajna Bhandar,Raguvir S,Charles Nicholas*

Main category: cs.CR

TL;DR: 该论文提出了一个包含6032个样本的手动审查恶意软件源代码数据集，从软件工程角度系统评估现代恶意软件的规模、开发成本、代码质量、安全性和依赖关系，并引入多视角谱系分析来揭示恶意软件之间的演化关系。


<details>
  <summary>Details</summary>
Motivation: 近年来恶意软件爆炸式增长和代码重用形成了复杂的演化关系，现有研究难以描述最近的演化趋势，迫切需要直观工具来理清恶意软件样本或类别之间的复杂联系。

Method: 构建手动审查的恶意软件源代码数据集（6032个样本），从软件工程角度系统评估恶意软件的多个维度，并引入多视角谱系分析方法：整体视角量化样本和类别之间的连接强度和方向，详细视角追踪单个样本的演化历史。

Result: 实验结果表明，尽管代码质量仍存在持续缺陷，但恶意软件样本表现出日益增加的复杂性和标准化，与主流软件工程实践发展同步。谱系分析直观揭示了由代码重用驱动的谱系扩展和演化。

Conclusion: 该研究为理解恶意软件生态系统的形成和演化提供了新的证据和工具，多视角谱系分析能够揭示恶意软件之间的演化关系，有助于更好地理解恶意软件的演化趋势。

Abstract: In recent years, the explosion of malware and extensive code reuse have formed complex evolutionary connections among malware specimens. The rapid pace of development makes it challenging for existing studies to characterize recent evolutionary trends. In addition, intuitive tools to untangle these intricate connections between malware specimens or categories are urgently needed. This paper introduces a manually-reviewed malware source code dataset containing 6032 specimens. Building on and extending current research from a software engineering perspective, we systematically evaluate the scale, development costs, code quality, as well as security and dependencies of modern malware. We further introduce a multi-view genealogy analysis to clarify malware connections: at an overall view, this analysis quantifies the strength and direction of connections among specimens and categories; at a detailed view, it traces the evolutionary histories of individual specimens. Experimental results indicate that, despite persistent shortcomings in code quality, malware specimens exhibit an increasing complexity and standardization, in step with the development of mainstream software engineering practices. Meanwhile, our genealogy analysis intuitively reveals lineage expansion and evolution driven by code reuse, providing new evidence and tools for understanding the formation and evolution of the malware ecosystem.

</details>


### [34] [Bias Injection Attacks on RAG Databases and Sanitization Defenses](https://arxiv.org/abs/2512.00804)
*Hao Wu,Prateek Saxena*

Main category: cs.CR

TL;DR: 本文研究RAG系统中向量数据库的攻击与防御，提出了一种新的偏见注入攻击方法，通过插入事实正确但语义偏见的文本来影响LLM答案的意识形态框架，并开发了后检索过滤防御方法BiasDef。


<details>
  <summary>Details</summary>
Motivation: 现有知识投毒攻击主要注入虚假或有毒内容，容易被事实核查或语言分析检测。本文揭示了一种更隐蔽的威胁：偏见注入攻击，通过插入事实正确但语义偏见的文本来影响LLM答案的意识形态框架。

Method: 1. 精确描述偏见注入攻击类别；2. 开发后检索过滤防御方法BiasDef；3. 基于公开问答数据集构建综合基准进行评估。

Result: 1. 提出的攻击能显著诱导LLM答案的视角偏移，有效规避现有基于检索的净化防御；2. BiasDef优于现有方法，减少15%的对抗性段落检索，将答案中的视角偏移降低6.2倍，同时使良性段落检索增加62%。

Conclusion: 偏见注入攻击是RAG系统中一种隐蔽且有效的威胁，而BiasDef作为后检索过滤防御方法能有效缓解这种攻击，在保持良性内容检索的同时减少对抗性偏见内容的影响。

Abstract: This paper explores attacks and defenses on vector databases in retrieval-augmented generation (RAG) systems. Prior work on knowledge poisoning attacks primarily inject false or toxic content, which fact-checking or linguistic analysis easily detects. We reveal a new and subtle threat: bias injection attacks, which insert factually correct yet semantically biased passages into the knowledge base to covertly influence the ideological framing of answers generated by large language models (LLMs). We demonstrate that these adversarial passages, though linguistically coherent and truthful, can systematically crowd out opposing views from the retrieved context and steer LLM answers toward the attacker's intended perspective.
  We precisely characterize this class of attacks and then develop a post-retrieval filtering defense, BiasDef. We construct a comprehensive benchmark based on public question answering datasets to evaluate them. Our results show that: (1) the proposed attack induces significant perspective shifts in LLM answers, effectively evading existing retrieval-based sanitization defenses; and (2) BiasDef outperforms existing methods by reducing adversarial passages retrieved by 15\% which mitigates perspective shift by 6.2\times in answers, while enabling the retrieval of 62\% more benign passages.

</details>


### [35] [Sliced Rényi Pufferfish Privacy: Directional Additive Noise Mechanism and Private Learning with Gradient Clipping](https://arxiv.org/abs/2512.01115)
*Tao Zhang,Yevgeniy Vorobeychik*

Main category: cs.CR

TL;DR: 本文提出切片Renyi Pufferfish隐私框架，解决高维最优传输校准和迭代学习机制无关组合规则缺失问题，通过切片Wasserstein机制和路径界实现几何感知的隐私保护。


<details>
  <summary>Details</summary>
Motivation: 针对Renyi Pufferfish隐私的两个实际缺陷：高维最优传输校准的困难性，以及缺乏通用的机制无关迭代学习组合规则，需要开发更实用、可计算的隐私保护框架。

Method: 提出切片Renyi Pufferfish隐私，用方向性比较替代高维比较；设计切片Wasserstein机制进行噪声校准；开发基于历史均匀界的分解-组合SRPP-SGD方案，利用梯度裁剪和子采样随机性。

Result: SRPP框架实现了几何感知且可计算的隐私保证；切片Wasserstein机制提供封闭形式、统计稳定和各向异性校准；HUC和ms-HUC会计师能聚合每迭代、每方向的Renyi成本；在静态和迭代设置中都获得优越的隐私-效用权衡。

Conclusion: SRPP框架有效解决了RPP的实际局限性，提供了可计算的隐私保证和优雅的组合性质，在深度学习的隐私保护中展现出优越的实用性和性能。

Abstract: We study privatization mechanism design and privacy accounting in the Pufferfish family, addressing two practical gaps of Renyi Pufferfish Privacy (RPP): high-dimensional optimal transport (OT) calibration and the absence of a general, mechanism-agnostic composition rule for iterative learning. We introduce Sliced Renyi Pufferfish Privacy (SRPP), which replaces high-dimensional comparisons by directional ones over a set of unit vectors, enabling geometry-aware and tractable guarantees. To calibrate noise without high-dimensional OT, we propose sliced Wasserstein mechanisms that compute per-direction (1-D) sensitivities, yielding closed-form, statistically stable, and anisotropic calibrations. We further define SRPP Envelope (SRPE) as computable upper bounds that are tightly implementable by these sliced Wasserstein mechanisms. For iterative deep learning algorithms, we develop a decompose-then-compose SRPP-SGD scheme with gradient clipping based on a History-Uniform Cap (HUC), a pathwise bound on one-step directional changes that is uniform over optimization history, and a mean-square variant (ms-HUC) that leverages subsampling randomness to obtain on-average SRPP guarantees with improved utility. The resulting HUC and ms-HUC accountants aggregate per-iteration, per-direction Renyi costs and integrate naturally with moments-accountant style analyses. Finally, when multiple mechanisms are trained and privatized independently under a common slicing geometry, our analysis yields graceful additive composition in both worst-case and mean-square regimes. Our experiments indicate that the proposed SRPP-based methods achieve favorable privacy-utility trade-offs in both static and iterative settings.

</details>


### [36] [Reverse Engineering and Control-Aware Security Analysis of the ArduPilot UAV Framework](https://arxiv.org/abs/2512.01164)
*Yasaswini Konapalli,Lotfi Ben Othmane,Cihan Tunc,Feras Benchellal,Likhita Mudagere*

Main category: cs.CR

TL;DR: 该论文分析了ArduPilot开源无人机自动驾驶框架的安全漏洞，重点关注其控制模型如何被恶意利用来诱导无人机异常行为。


<details>
  <summary>Details</summary>
Motivation: 随着无人机技术在多个领域获得高度关注，无人机安全变得至关重要。ArduPilot作为最广泛使用的开源自动驾驶框架，存在多种已知漏洞，特别是在通信子系统（WiFi、遥测、GPS）和控制程序方面，这些漏洞可能成为攻击入口。

Method: 首先重构ArduPilot的软件架构和控制模型，然后分析这些控制模型如何可能被滥用，通过合法输入诱导恶意行为。

Result: 论文识别了ArduPilot控制模型中可能被恶意利用的漏洞，展示了攻击者如何通过看似合法的输入操纵无人机执行异常行为。

Conclusion: ArduPilot的控制模型存在安全风险，攻击者可以利用这些漏洞通过合法输入诱导恶意行为，这强调了无人机自动驾驶系统安全加固的重要性。

Abstract: Unmanned Aerial Vehicle (UAV) technologies are gaining high interest for many domains, which makes UAV security of utmost importance. ArduPilot is among the most widely used open-source autopilot UAV frameworks; yet, many studies demonstrate the vulnerabilities affecting such systems. Vulnerabilities within its communication subsystems (including WiFi, telemetry, or GPS) expose critical entry points, and vulnerabilities in Ardupilot can affect the control procedure. In this paper, we reconstruct the software architecture and the control models implemented by ArduPilot and then examine how these control models could potentially misused to induce malicious behaviors while relying on legitimate inputs.

</details>


### [37] [DefenSee: Dissecting Threat from Sight and Text - A Multi-View Defensive Pipeline for Multi-modal Jailbreaks](https://arxiv.org/abs/2512.01185)
*Zihao Wang,Kar Wai Fok,Vrizlynn L. L. Thing*

Main category: cs.CR

TL;DR: DefenSee是一种轻量级多模态黑盒防御技术，通过图像变体转录和跨模态一致性检查来增强多模态大语言模型对协同越狱攻击的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型（MLLMs）虽然能处理文本、图像和音频，但在集成图像和文本时对协同越狱攻击高度脆弱。现有防御主要关注文本模态，缺乏有效的多模态保护机制，导致MLLMs比纯文本模型更容易受到恶意或不安全指令的攻击。

Method: 提出DefenSee防御技术，采用图像变体转录和跨模态一致性检查的方法。通过生成图像的不同变体并进行转录，然后检查不同模态之间的一致性，模拟人类判断过程来检测恶意内容。

Result: 在流行的多模态越狱和良性数据集上的实验表明，DefenSee能持续增强MLLM的鲁棒性，同时在良性任务上比最先进防御方法更好地保持性能。在MM-SafetyBench基准测试中，将MiniGPT4的越狱攻击成功率降至1.70%以下，显著优于同类方法。

Conclusion: DefenSee提供了一种有效且轻量级的多模态防御解决方案，通过跨模态一致性检查机制显著提高了多模态大语言模型对协同越狱攻击的抵抗能力，同时保持了良性任务的性能。

Abstract: Multi-modal large language models (MLLMs), capable of processing text, images, and audio, have been widely adopted in various AI applications. However, recent MLLMs integrating images and text remain highly vulnerable to coordinated jailbreaks. Existing defenses primarily focus on the text, lacking robust multi-modal protection. As a result, studies indicate that MLLMs are more susceptible to malicious or unsafe instructions, unlike their text-only counterparts. In this paper, we proposed DefenSee, a robust and lightweight multi-modal black-box defense technique that leverages image variants transcription and cross-modal consistency checks, mimicking human judgment. Experiments on popular multi-modal jailbreak and benign datasets show that DefenSee consistently enhances MLLM robustness while better preserving performance on benign tasks compared to SOTA defenses. It reduces the ASR of jailbreak attacks to below 1.70% on MiniGPT4 using the MM-SafetyBench benchmark, significantly outperforming prior methods under the same conditions.

</details>


### [38] [Benchmarking and Understanding Safety Risks in AI Character Platforms](https://arxiv.org/abs/2512.01247)
*Yiluo Wei,Peixian Zhang,Gareth Tyson*

Main category: cs.CR

TL;DR: 对16个AI角色平台的大规模安全评估显示，这些平台平均不安全回复率高达65.1%，远超基线系统的17.7%，存在严重安全隐患。


<details>
  <summary>Details</summary>
Motivation: AI角色平台允许用户与AI角色对话，应用快速增长，但其沉浸式、个性化的特性结合技术漏洞引发了严重的安全担忧。尽管这些平台很受欢迎，但缺乏系统性的安全评估。

Method: 对16个流行的AI角色平台进行了首次大规模安全研究，使用包含16个安全类别的5,000个问题基准集进行评估，并分析了角色特征（如人口统计学和个性）与安全性能的相关性。

Result: AI角色平台平均不安全回复率为65.1%，显著高于基线系统的17.7%；安全性能在不同角色间差异很大，且与角色特征（如人口统计学和个性）强相关；机器学习模型能以0.81的F1分数识别较不安全的角色。

Conclusion: 研究揭示了AI角色平台存在严重的安全缺陷，但通过机器学习模型预测不安全角色的能力可为平台提供改进机制，包括更安全的交互、角色搜索/推荐和角色创建，为平台治理和内容审核提供有价值的见解。

Abstract: AI character platforms, which allow users to engage in conversations with AI personas, are a rapidly growing application domain. However, their immersive and personalized nature, combined with technical vulnerabilities, raises significant safety concerns. Despite their popularity, a systematic evaluation of their safety has been notably absent. To address this gap, we conduct the first large-scale safety study of AI character platforms, evaluating 16 popular platforms using a benchmark set of 5,000 questions across 16 safety categories. Our findings reveal a critical safety deficit: AI character platforms exhibit an average unsafe response rate of 65.1%, substantially higher than the 17.7% average rate of the baselines. We further discover that safety performance varies significantly across different characters and is strongly correlated with character features such as demographics and personality. Leveraging these insights, we demonstrate that our machine learning model is able identify less safe characters with an F1-score of 0.81. This predictive capability can be beneficial for platforms, enabling improved mechanisms for safer interactions, character search/recommendations, and character creation. Overall, the results and findings offer valuable insights for enhancing platform governance and content moderation for safer AI character platforms.

</details>


### [39] [Securing Large Language Models (LLMs) from Prompt Injection Attacks](https://arxiv.org/abs/2512.01326)
*Omar Farooq Khan Suri,John McCrae*

Main category: cs.CR

TL;DR: JATMO防御方法通过任务特定微调减少LLM的提示注入攻击，但HOUYI遗传攻击框架仍能绕过防御，显示微调防御存在局限性


<details>
  <summary>Details</summary>
Motivation: 随着LLM在现实应用中的部署增加，其灵活性使其容易受到提示注入攻击。JATMO方法通过任务特定微调来减少对恶意指令的敏感性，但需要评估其实际防御效果

Method: 研究采用HOUYI遗传攻击框架，通过引入自定义适应度评分、修改突变逻辑和本地模型测试的新框架来评估JATMO防御。对LLaMA 2-7B、Qwen1.5-4B和Qwen1.5-0.5B模型进行JATMO微调，并与微调的GPT-3.5-Turbo基线进行比较

Result: JATMO相对于指令微调模型降低了攻击成功率，但未能完全防止注入攻击；利用多语言线索或代码相关干扰器的攻击者仍能绕过防御。同时观察到生成质量与注入漏洞之间存在权衡，更好的任务性能通常与增加的易感性相关

Conclusion: 基于微调的防御方法既有前景也有局限性，需要采用分层、对抗性感知的缓解策略来应对提示注入攻击

Abstract: Large Language Models (LLMs) are increasingly being deployed in real-world applications, but their flexibility exposes them to prompt injection attacks. These attacks leverage the model's instruction-following ability to make it perform malicious tasks. Recent work has proposed JATMO, a task-specific fine-tuning approach that trains non-instruction-tuned base models to perform a single function, thereby reducing susceptibility to adversarial instructions. In this study, we evaluate the robustness of JATMO against HOUYI, a genetic attack framework that systematically mutates and optimizes adversarial prompts. We adapt HOUYI by introducing custom fitness scoring, modified mutation logic, and a new harness for local model testing, enabling a more accurate assessment of defense effectiveness. We fine-tuned LLaMA 2-7B, Qwen1.5-4B, and Qwen1.5-0.5B models under the JATMO methodology and compared them with a fine-tuned GPT-3.5-Turbo baseline. Results show that while JATMO reduces attack success rates relative to instruction-tuned models, it does not fully prevent injections; adversaries exploiting multilingual cues or code-related disruptors still bypass defenses. We also observe a trade-off between generation quality and injection vulnerability, suggesting that better task performance often correlates with increased susceptibility. Our results highlight both the promise and limitations of fine-tuning-based defenses and point toward the need for layered, adversarially informed mitigation strategies.

</details>


### [40] [EmoRAG: Evaluating RAG Robustness to Symbolic Perturbations](https://arxiv.org/abs/2512.01335)
*Xinyun Zhou,Xinfeng Li,Yinan Peng,Ming Xu,Xuanwang Zhang,Miao Yu,Yidong Wang,Xiaojun Jia,Kun Wang,Qingsong Wen,XiaoFeng Wang,Wei Dong*

Main category: cs.CR

TL;DR: 研究发现RAG系统对表情符号等微小符号扰动存在严重脆弱性，单个表情符号就能几乎100%误导检索结果，称为EmoRAG漏洞。


<details>
  <summary>Details</summary>
Motivation: 尽管检索增强生成(RAG)系统通过整合外部知识提高了大语言模型的可靠性，但研究发现这些系统存在一个被忽视的关键漏洞：对微小符号扰动（特别是难以察觉的表情符号）的极端敏感性，这可能被攻击者利用来操纵系统。

Method: 通过在查询中注入单个表情符号（如"(@_@)"），在通用问答和代码领域进行广泛实验，使用多种最先进的检索器和生成器，分析表情符号对RAG系统的影响机制。

Result: 发现三个关键结果：(1)单个表情符号注入就能造成最大破坏，几乎100%主导RAG输出；(2)表情符号在查询开头位置时扰动最严重，F1分数超过0.92；(3)参数更大的模型反而更脆弱。现有防御措施对此漏洞无效。

Conclusion: 当前RAG系统的鲁棒性假设存在严重问题，EmoRAG漏洞可能被恶意利用。需要开发针对性防御措施，并为构建更鲁棒的RAG系统指明未来研究方向。

Abstract: Retrieval-Augmented Generation (RAG) systems are increasingly central to robust AI, enhancing large language model (LLM) faithfulness by incorporating external knowledge. However, our study unveils a critical, overlooked vulnerability: their profound susceptibility to subtle symbolic perturbations, particularly through near-imperceptible emoticon tokens such as "(@_@)" that can catastrophically mislead retrieval, termed EmoRAG. We demonstrate that injecting a single emoticon into a query makes it nearly 100% likely to retrieve semantically unrelated texts that contain a matching emoticon. Our extensive experiment across general question-answering and code domains, using a range of state-of-the-art retrievers and generators, reveals three key findings: (I) Single-Emoticon Disaster: Minimal emoticon injections cause maximal disruptions, with a single emoticon almost 100% dominating RAG output. (II) Positional Sensitivity: Placing an emoticon at the beginning of a query can cause severe perturbation, with F1-Scores exceeding 0.92 across all datasets. (III) Parameter-Scale Vulnerability: Counterintuitively, models with larger parameters exhibit greater vulnerability to the interference. We provide an in-depth analysis to uncover the underlying mechanisms of these phenomena. Furthermore, we raise a critical concern regarding the robustness assumption of current RAG systems, envisioning a threat scenario where an adversary exploits this vulnerability to manipulate the RAG system. We evaluate standard defenses and find them insufficient against EmoRAG. To address this, we propose targeted defenses, analyzing their strengths and limitations in mitigating emoticon-based perturbations. Finally, we outline future directions for building robust RAG systems.

</details>


### [41] [IVE: An Accelerator for Single-Server Private Information Retrieval Using Versatile Processing Elements](https://arxiv.org/abs/2512.01574)
*Sangpyo Kim,Hyesung Ji,Jongmin Kim,Wonseok Choi,Jaiyoung Park,Jung Ho Ahn*

Main category: cs.CR

TL;DR: IVE是一个针对单服务器私有信息检索的加速器，通过DRAM存储、多客户端批处理、片上暂存器和专用功能单元，实现了比现有硬件方案高达1275倍的吞吐量提升。


<details>
  <summary>Details</summary>
Motivation: 单服务器基于同态加密的私有信息检索虽然部署简单且信任假设较少，但由于需要读取整个数据库，存在高计算和内存带宽需求，导致性能受限，特别是从SSD等存储设备读取大型数据库时性能严重受限。

Method: 提出IVE加速器，采用DRAM存储大型数据库以消除存储带宽限制；通过多客户端批处理分摊数据库访问成本；使用大型片上暂存器和操作调度算法最大化数据重用；引入sysNTTU多功能单元提高面积效率；设计异构内存系统架构实现数据库规模线性扩展而不降低吞吐量。

Result: IVE实现了比现有PIR硬件解决方案高达1275倍的吞吐量提升，能够支持大规模数据库的实用化私有信息检索。

Conclusion: IVE通过系统化的硬件架构创新，解决了单服务器私有信息检索的性能瓶颈，使其从理论可行变为实际可用，为隐私保护应用提供了高效的解决方案。

Abstract: Private information retrieval (PIR) is an essential cryptographic protocol for privacy-preserving applications, enabling a client to retrieve a record from a server's database without revealing which record was requested. Single-server PIR based on homomorphic encryption has particularly gained immense attention for its ease of deployment and reduced trust assumptions. However, single-server PIR remains impractical due to its high computational and memory bandwidth demands. Specifically, reading the entirety of large databases from storage, such as SSDs, severely limits its performance. To address this, we propose IVE, an accelerator for single-server PIR with a systematic extension that enables practical retrieval from large databases using DRAM. Recent advances in DRAM capacity allow PIR for large databases to be served entirely from DRAM, removing its dependence on storage bandwidth. Although the memory bandwidth bottleneck still remains, multi-client batching effectively amortizes database access costs across concurrent requests to improve throughput. However, client-specific data remains a bottleneck, whose bandwidth requirements ultimately limits performance. IVE overcomes this by employing a large on-chip scratchpad with an operation scheduling algorithm that maximizes data reuse, further boosting throughput. Additionally, we introduce sysNTTU, a versatile functional unit that enhances area efficiency without sacrificing performance. We also propose a heterogeneous memory system architecture, which enables a linear scaling of database sizes without a throughput degradation. Consequently, IVE achieves up to 1,275x higher throughput compared to prior PIR hardware solutions.

</details>


### [42] [WhiteLie: A Robust System for Spoofing User Data in Android Platforms](https://arxiv.org/abs/2512.01595)
*Harish Yadav,Vikas Maurya,Abhilash Jindal,Vireshwar Kumar*

Main category: cs.CR

TL;DR: WhiteLie是一个Android用户数据欺骗系统，能在不root设备或修改应用的情况下，向应用提供虚假数据以保护用户隐私，同时避免应用崩溃。


<details>
  <summary>Details</summary>
Motivation: Android权限框架允许用户拒绝应用访问隐私数据，但许多应用在被拒绝权限时会崩溃，迫使用户必须允许访问才能使用应用。

Method: 开发了WhiteLie系统，能够欺骗多种用户数据并检测隐私违规行为，自动向目标应用提供虚假数据而非真实数据，无需设备root或修改应用二进制文件。

Result: 在70多个流行Android应用上测试，WhiteLie能够成功欺骗应用接受虚假数据而不被检测，且在电池使用、CPU消耗和应用执行延迟方面引入的开销可忽略不计。

Conclusion: 研究证明了在现有Android生态系统中实施以用户为中心的隐私增强机制的可行性。

Abstract: Android employs a permission framework that empowers users to either accept or deny sharing their private data (for example, location) with an app. However, many apps tend to crash when they are denied permission, leaving users no choice but to allow access to their data in order to use the app. In this paper, we introduce a comprehensive and robust user data spoofing system, WhiteLie, that can spoof a variety of user data and feed it to target apps. Additionally, it detects privacy-violating behaviours, automatically responding by supplying spoofed data instead of the user's real data, without crashing or disrupting the apps. Unlike prior approaches, WhiteLie requires neither device rooting nor altering the app's binary, making it deployable on stock Android devices. Through experiments on more than 70 popular Android apps, we demonstrate that WhiteLie is able to deceive apps into accepting spoofed data without getting detected. Our evaluation further demonstrates that WhiteLie introduces negligible overhead in terms of battery usage, CPU consumption, and app execution latency. Our findings underscore the feasibility of implementing user-centric privacy-enhancing mechanisms within the existing Android ecosystem.

</details>


### [43] [On the Context-Hiding Property of Shamir-Based Homomorphic Secret Sharing](https://arxiv.org/abs/2512.01604)
*Shuai Feng,Liang Feng Zhang*

Main category: cs.CR

TL;DR: 本文研究了同态秘密共享（HSS）的上下文隐藏属性，重点关注单项式和多项式的Shamir-based HSS，旨在解决传统重随机化技术增加份额大小的问题。


<details>
  <summary>Details</summary>
Motivation: 在HSS支持的应用如安全多方计算中，安全性要求输出份额不泄露超出函数输出之外的输入信息。传统重随机化技术虽然能实现上下文隐藏，但会增加份额大小，因此需要更高效的解决方案。

Method: 形式化了单个函数的HSS上下文隐藏属性，研究了单项式的Shamir-based HSS的上下文隐藏特性，并将研究扩展到多项式的情况。

Result: 论文分析了Shamir-based HSS在单项式和多项式上的上下文隐藏属性，为设计更高效的HSS方案提供了理论基础，避免了传统重随机化技术带来的份额大小增加问题。

Conclusion: 通过形式化HSS的上下文隐藏属性并分析Shamir-based方案在单项式和多项式上的表现，为构建更高效、安全的HSS系统提供了理论框架，有助于优化安全多方计算等应用。

Abstract: Homomorphic secret sharing (HSS) allows multiple input clients to secretly share their private inputs to a function among several servers such that each server can homomorphically compute the function over its share to produce a share of the function's output. In HSS-enabled applications such as secure multi-party computation (MPC), security requires that the output shares leak no more information about the inputs than the function output. Such security is ensured by the context-hiding property of HSS. The typical rerandomization technique achieves context hiding but increases the share size. To address this, we formalize the context-hiding property of HSS for individual functions, examine the context-hiding property of Shamir-based HSS for monomials, and extend the study to polynomials.

</details>


### [44] [Demystifying Feature Engineering in Malware Analysis of API Call Sequences](https://arxiv.org/abs/2512.01666)
*Tianheng Qu,Hongsong Zhu,Limin Sun,Haining Wang,Haiqiang Fei,Zheng He,Zhi Li*

Main category: cs.CR

TL;DR: 本文系统研究了基于API调用序列的恶意软件分类中特征工程的影响，比较了基于领域知识的特征工程和基于NLP的自动特征提取方法在不同深度学习模型上的性能表现。


<details>
  <summary>Details</summary>
Motivation: 传统恶意软件分析依赖领域专家手动提取API调用序列特征，而NLP方法可实现自动特征提取。本文旨在探究如何有效选择基于API调用序列的特征进行恶意软件分析，比较不同特征工程方法对分类性能的影响。

Method: 1. 使用三种深度学习模型（CNN、LSTM、Transformer）进行性能比较；2. 对比基于领域知识的特征工程方法和基于NLP的特征工程方法；3. 分析API调用序列的完整特征集，识别模型关注的关键特征。

Result: 1. 使用基于领域知识特征工程的模型在所有指标上普遍优于使用NLP方法的模型，特别是在小样本情况下优势更明显；2. 分析发现模型往往关注句柄（handles）和虚拟地址等特征，但这些特征在不同执行中变化较大且难以被人类分析师解释。

Conclusion: 基于领域知识的特征工程在恶意软件分类中仍然优于自动NLP特征提取方法，特别是在数据量有限的情况下。然而，模型关注的某些特征（如句柄和虚拟地址）存在跨执行变异性和可解释性差的问题，这为未来研究提供了方向。

Abstract: Machine learning (ML) has been widely used to analyze API call sequences in malware analysis, which typically requires the expertise of domain specialists to extract relevant features from raw data. The extracted features play a critical role in malware analysis. Traditional feature extraction is based on human domain knowledge, while there is a trend of using natural language processing (NLP) for automatic feature extraction. This raises a question: how do we effectively select features for malware analysis based on API call sequences? To answer it, this paper presents a comprehensive study of investigating the impact of feature engineering upon malware classification.We first conducted a comparative performance evaluation under three models, Convolutional Neural Network (CNN), Long Short-Term Memory (LSTM), and Transformer, with respect to knowledge-based and NLP-based feature engineering methods. We observed that models with knowledge-based feature engineering inputs generally outperform those using NLP-based across all metrics, especially under smaller sample sizes. Then we analyzed a complete set of data features from API call sequences, our analysis reveals that models often focus on features such as handles and virtual addresses, which vary across executions and are difficult for human analysts to interpret.

</details>


### [45] [A Privacy-Preserving Information-Sharing Protocol for Federated Authentication](https://arxiv.org/abs/2512.01832)
*Francesco Buccafurri,Carmen Licciardi*

Main category: cs.CR

TL;DR: 提出一种基于OPRF的隐私保护协议，用于联邦认证系统中的身份注册和信息共享，能在不泄露用户数据的情况下检测重复或欺诈身份注册


<details>
  <summary>Details</summary>
Motivation: 解决联邦认证系统中身份提供商需要检测重复或欺诈身份注册，同时保护用户隐私、防止跨域关联的问题

Method: 使用不经意伪随机函数（OPRF）结合领域特定变换，每个身份提供商生成独立的伪匿名标识符，中央机构维护盲注册表记录验证结果

Result: 协议提供了通用抽象框架，在保持强隐私保证的同时支持有效的欺诈预防机制，实现全局一致性检查而不暴露敏感信息

Conclusion: 该协议为联邦认证系统提供了一种隐私保护的身份注册和信息共享解决方案，平衡了隐私保护和欺诈检测的需求

Abstract: This paper presents a privacy-preserving protocol for identity registration and information sharing in federated authentication systems. The goal is to enable Identity Providers (IdPs) to detect duplicate or fraudulent identity enrollments without revealing users personal data or enabling cross-domain correlation. The protocol relies on Oblivious Pseudorandom Functions (OPRFs) combined with domain-specific transformations, ensuring that each IdP generates independent pseudonymous identifiers derived from a shared cryptographic service while maintaining full input confidentiality. A central authority maintains a blind registry that records successful and failed identity verifications using only pseudonymous identifiers, allowing global consistency checks without exposing sensitive information or linking users across domains. The proposed construction provides a general and abstract framework suitable for a wide range of federated authentication systems, achieving strong privacy guarantees while supporting effective fraud-prevention mechanisms during identity registration.

</details>


### [46] [Behind the Curtain: How Shared Hosting Providers Respond to Vulnerability Notifications](https://arxiv.org/abs/2512.01891)
*Giada Stivala,Rafael Mrowczynski,Maria Hellenthal,Giancarlo Pellegrino*

Main category: cs.CR

TL;DR: 本文首次深入研究了托管服务提供商如何处理漏洞通知，揭示了导致修复率持续低下的组织与运营因素，而非仅仅关注通知发送方的可控因素。


<details>
  <summary>Details</summary>
Motivation: 尽管之前的研究主要关注通知发送方可控的因素（如发件人信誉、邮件格式、通信渠道），但漏洞通知的修复率仍然持续低下。本文旨在从托管服务提供商的角度，首次深入了解导致修复率低下的根本原因。

Method: 通过对24家托管服务提供商（涵盖共享托管和网站开发服务，不同公司规模和操作角色）进行半结构化访谈，直接调查他们如何处理漏洞通知以及影响通知有效性的因素。

Result: 研究发现：虽然复杂基础设施导致部分提供商难以联系，但大多数会常规处理漏洞通知。修复率低的主要原因是严格的责任边界（将Web应用问题视为客户责任）、低托管费用和每日大量安全事件阻碍了主动和被动措施。托管商指责网站所有者疏忽，而网站所有者则低估网站价值或缺乏安全知识。

Conclusion: 漏洞通知修复率低下的根本原因在于托管服务提供商的组织和运营因素，特别是责任划分、经济激励和安全事件量等因素，而非仅仅通知发送方的技术因素。这为改进漏洞通知效果提供了新的视角。

Abstract: Large-scale vulnerability notifications (VNs) can help hosting provider organizations (HPOs) identify and remediate security vulnerabilities that attackers can exploit in data breaches or phishing campaigns. Previous VN studies have primarily focused on factors under the control of reporters, such as sender reputation, email formatting, and communication channels. Despite these efforts, remediation rates for vulnerability notifications continue to remain consistently low. This paper presents the first in-depth study of how HPOs process vulnerability notifications internally and what organizational and operational factors influence VN effectiveness. We examine the problem from a different perspective to provide the first detailed understanding of the reasons behind persistently low remediation rates. Instead of manipulating parameters of VN campaigns, we interview hosting providers directly, investigating how they handle vulnerability notifications and what factors may influence VN effectiveness, such as VN awareness and reachability, HPOs' service models, and perceived security risks.
  We conducted semi-structured interviews with 24 HPOs across shared hosting and web development services, representing varied company sizes and operator roles. Our findings reveal practical insights on VN processing and abuse workflows. While some providers remain hard to reach due to complex infrastructures, most report routinely handling VNs. However, limited remediation often stems from strict responsibility boundaries, where web application issues are seen as the customer's domain. Low hosting fees and high volumes of daily compromises further discourage both proactive and reactive measures. Our findings show that HPOs blame negligent website owners, and prior works on website owners confirms they often undervalue their sites or lack security know-how.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [47] [Chunking Strategies for Multimodal AI Systems](https://arxiv.org/abs/2512.00185)
*Shashanka B R,Mohith Charan R,Seema Banu F*

Main category: cs.AI

TL;DR: 本文是一篇关于多模态分块策略的综述性论文，系统梳理了文本、图像、音频、视频和跨模态数据的分块方法，分析了各种方法的优缺点，并指出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 本文旨在整合多模态分块策略的研究现状，为研究人员和从业者提供技术基础和设计空间，以开发更有效和高效的多模态AI系统。当前多模态数据处理面临模态复杂性、处理准确性和生成一致性等挑战，需要系统性的分块策略指导。

Method: 采用综述研究方法，首先建立全面的分类体系，然后针对每种模态（文本、图像、音频、视频、跨模态数据）分析经典和现代的分块方法。具体包括固定大小令牌窗口、递归文本分割、对象中心视觉分块、基于静音的音频分割、视频场景检测等方法。每种方法都从底层方法论、支持工具、优势和挑战等方面进行分析。

Result: 提供了多模态分块策略的全面技术分析，包括：1）建立了针对不同模态的分块策略分类体系；2）分析了各种方法的粒度-上下文权衡和多模态对齐等关键挑战；3）探索了新兴的跨模态分块策略；4）提供了比较性见解，突出了开放性问题如异步信息密度和噪声对齐信号。

Conclusion: 本文为开发更有效的多模态AI系统铺平了道路，指出了未来研究方向包括自适应分块、基于学习的分块和任务特定分块等。该综述有助于推动创新，使分块管道能够随模态复杂性扩展，提高处理准确性，并改善实际应用中的生成一致性。

Abstract: Our goal is to consolidate the landscape of multimodal chunking strategies, providing researchers and practitioners with a technical foundation and design space for developing more effective and efficient multimodal AI systems. This survey paves the way for innovations in robust chunking pipelines that scale with modality complexity, enhance processing accuracy, and improve generative coherence in real-world applications. This survey provides a comprehensive taxonomy and technical analysis of chunking strategies tailored for each modality: text, images, audio, video, and cross-modal data. We examine classical and modern approaches such as fixed-size token windowing, recursive text splitting, object-centric visual chunking, silence-based audio segmentation, and scene detection in videos. Each approach is analyzed in terms of its underlying methodology, supporting tools (e.g., LangChain, Detectron2, PySceneDetect), benefits, and challenges, particularly those related to granularity-context trade-offs and multimodal alignment. Furthermore, we explore emerging cross-modal chunking strategies that aim to preserve alignment and semantic consistency across disparate data types [4]. We also include comparative insights, highlight open problems such as asynchronous information density and noisy alignment signals, and identify opportunities for future research in adaptive, learning-based, and task-specific chunking.

</details>


### [48] [A Rosetta Stone for AI Benchmarks](https://arxiv.org/abs/2512.00193)
*Anson Ho,Jean-Stanislas Denain,David Atanasov,Samuel Albanie,Rohin Shah*

Main category: cs.AI

TL;DR: 提出一个统计框架将不同AI基准测试"缝合"在一起，在统一尺度上衡量模型能力和基准难度，用于研究AI长期趋势


<details>
  <summary>Details</summary>
Motivation: 大多数AI基准测试在推出后几年甚至几个月内就会饱和，难以研究AI能力的长期趋势

Method: 构建统计框架，将模型能力和基准测试难度放在单一数值尺度上，无需假设能力如何随时间或训练计算量演变

Result: 框架可用于：1)测量AI进展速度并预测未来能力；2)估算算法效率改进率；3)检测AI进展的快速加速

Conclusion: 该框架作为"罗塞塔石碑"，能够跨不同基准测试和时间比较模型，为研究AI长期趋势提供有效工具

Abstract: Most AI benchmarks saturate within years or even months after they are introduced, making it hard to study long-run trends in AI capabilities. To address this challenge, we build a statistical framework that stitches benchmarks together, putting model capabilities and benchmark difficulties on a single numerical scale. This acts as a "Rosetta Stone", allowing us to compare models across a wide range of abilities and time, even if they are not evaluated on the same benchmarks. Moreover, this works without assuming how capabilities evolve across time or with training compute. We demonstrate three applications of this framework. First, we use it to measure the speed of AI progress over time, and to forecast future AI capabilities. Second, we estimate the rate of improvements in algorithmic efficiency, finding estimates that are higher, but broadly consistent with prior work. Finally, we find that our approach can be used to detect rapid accelerations in AI progress.

</details>


### [49] [Reasoning Under Pressure: How do Training Incentives Influence Chain-of-Thought Monitorability?](https://arxiv.org/abs/2512.00218)
*Matt MacDermott,Qiyao Wei,Rada Djoneva,Francis Rhys Ward*

Main category: cs.AI

TL;DR: 研究不同训练激励对AI推理模型可监控性的影响，发现对抗性优化会降低监控性能，而直接优化可监控性并不总能带来改进


<details>
  <summary>Details</summary>
Motivation: AI系统输出自然语言推理过程为安全监控提供了机会，但推理过程是否真实反映底层思维过程以及可监控程度可能受训练方式影响，需要研究不同训练激励如何影响模型的可监控性

Method: 引入新的方法论来衡量可监控性：根据监控器能否使用模型的推理过程预测关键潜在变量；在控制准确性的前提下，测试常见训练激励（长度惩罚、KL正则化）、对抗性优化（惩罚监控准确性）和直接优化可监控性的效果

Result: 常见训练激励（长度惩罚和KL正则化）对可监控性没有一致影响；对抗性优化会降低监控性能；直接优化可监控性并不能可靠地带来改进

Conclusion: 训练激励确实会影响AI推理模型的可监控性，特别是对抗性优化会损害监控能力，这为设计更安全的AI系统提供了重要启示

Abstract: AI systems that output their reasoning in natural language offer an opportunity for safety -- we can \emph{monitor} their chain of thought (CoT) for undesirable reasoning, such as the pursuit of harmful objectives. However, the extent to which CoT faithfully reflects the underlying reasoning process, and hence the extent to which it can be usefully monitored, may be influenced by certain aspects of training. We investigate how different \emph{training incentives}, applied to a reasoning model, affect its monitorability. We introduce a novel methodology for measuring monitorability according to whether a monitor can predict a key latent variable using the model's reasoning. When controlling for accuracy, we do not find evidence for consistent effects from commonly used incentives (length penalties and KL regularisation), but we find that adversarial optimisation (penalising monitor accuracy) degrades monitor performance, while direct optimisation for monitorability does not reliably lead to improvements. Our code is available at https://github.com/QiyaoWei/reasoning-under-pressure.

</details>


### [50] [ChartPoint: Guiding MLLMs with Grounding Reflection for Chart Reasoning](https://arxiv.org/abs/2512.00305)
*Zhengzhuo Xu,SiNan Du,Yiyan Qi,SiwenLu,Chengjin Xu,Chun Yuan,Jian Guo*

Main category: cs.AI

TL;DR: PointCoT通过将反射式交互融入图表思维链推理，解决MLLMs在图表理解中的视觉感知和定位问题，显著提升图表理解性能


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在图表理解中过度依赖OCR提取内容，当图表文本标注稀疏时会产生数值幻觉。现有方法主要关注扩展指令，但未能解决核心挑战——基于视觉感知的推理。研究发现MLLMs在图表元素和比例关系上的定位能力较弱，无法将关键位置与其推理相匹配。

Method: 提出PointCoT方法，将反射式交互融入图表思维链推理。通过提示MLLMs生成边界框和基于位置标注重新渲染图表，建立文本推理步骤与视觉定位区域之间的连接。构建了ChartPoint-SFT-62k数据集，包含19.2K高质量图表样本，带有逐步思维链、边界框和重新渲染的可视化。基于此数据开发了两个指令调优模型ChartPointQ2和ChartPointQ2.5。

Result: 提出的模型在多个图表基准测试中优于现有最先进方法，例如在ChartBench上提升+5.04%。

Conclusion: PointCoT通过将视觉定位与文本推理相结合，有效解决了MLLMs在图表理解中的数值幻觉问题，显著提升了图表理解能力。

Abstract: Multimodal Large Language Models (MLLMs) have emerged as powerful tools for chart comprehension. However, they heavily rely on extracted content via OCR, which leads to numerical hallucinations when chart textual annotations are sparse. While existing methods focus on scaling instructions, they fail to address the fundamental challenge, i.e., reasoning with visual perception. In this paper, we identify a critical observation: MLLMs exhibit weak grounding in chart elements and proportional relationships, as evidenced by their inability to localize key positions to match their reasoning. To bridge this gap, we propose PointCoT, which integrates reflective interaction into chain-of-thought reasoning in charts. By prompting MLLMs to generate bounding boxes and re-render charts based on location annotations, we establish connections between textual reasoning steps and visual grounding regions. We further introduce an automated pipeline to construct ChartPoint-SFT-62k, a dataset featuring 19.2K high-quality chart samples with step-by-step CoT, bounding box, and re-rendered visualizations. Leveraging this data, we develop two instruction-tuned models, ChartPointQ2 and ChartPointQ2.5, which outperform state-of-the-art across several chart benchmarks, e.g., +5.04\% on ChartBench.

</details>


### [51] [CogEvo-Edu: Cognitive Evolution Educational Multi-Agent Collaborative System](https://arxiv.org/abs/2512.00331)
*Yefeng Wu,Yuchen Song,Yecheng Zhao,Ling Wu,Shan Wan*

Main category: cs.AI

TL;DR: CogEvo-Edu：一个用于STEM教育（特别是数字信号处理）的分层多智能体系统，通过耦合的认知演化过程（检索、记忆和控制）来提升LLM作为对话导师的效果。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在STEM教育中主要依赖单一模型和静态RAG管道，难以处理复杂领域（如数字信号处理）中需要长期学生建模、异构知识库管理和自适应教学策略的挑战。

Method: 提出CogEvo-Edu系统，包含三个层次：认知感知层（维护双重记忆和置信度加权整合）、知识演化层（为知识块分配时空价值驱动激活和遗忘）、元控制层（将教学建模为分层序列决策，协调专业智能体）。

Result: 在DSP-EduBench基准测试中，使用三模型LLM-as-a-Judge评估，CogEvo-Edu将总体得分从5.32提升到9.23，在六个指标上均优于静态RAG、简单记忆和单智能体变体。

Conclusion: 将学生档案、知识库和教学策略联合演化的方法显著提升了LLM作为STEM教育导师的效果，验证了将检索、记忆和控制作为耦合认知演化过程的价值。

Abstract: Large language models (LLMs) are increasingly deployed as conversational tutors in STEM education, yet most systems still rely on a single LLM with a static retrieval-augmented generation (RAG) pipeline over course materials. This design struggles in complex domains such as digital signal processing (DSP), where tutors must maintain coherent long-term student models, manage heterogeneous knowledge bases, and adapt teaching strategies over extended interactions. We argue that retrieval, memory, and control should be treated as a coupled cognitive evolution process. We instantiate this view in CogEvo-Edu, a hierarchical educational multi-agent system comprising a Cognitive Perception Layer (CPL), a Knowledge Evolution Layer (KEL), and a Meta-Control Layer (MCL). CPL maintains dual memories and performs confidence-weighted consolidation to build structured, self-correcting student profiles under limited context. KEL assigns each knowledge chunk a spatiotemporal value that drives activation, semantic compression, and forgetting. MCL formulates tutoring as hierarchical sequential decision making, orchestrating specialized agents and jointly adapting CPL/KEL hyperparameters via a dual inner--outer loop. To evaluate CogEvo-Edu, we construct DSP-EduBench, a vertical benchmark for DSP tutoring with heterogeneous resources, simulated student profiles, and long-horizon interaction scripts. Using a three-model LLM-as-a-Judge ensemble, CogEvo-Edu raises the overall score from 5.32 to 9.23 and improves all six indicators over static RAG, simple memory, and a single-agent variant, demonstrating the value of jointly evolving student profiles, knowledge bases, and teaching policies.

</details>


### [52] [Echo-N1: Affective RL Frontier](https://arxiv.org/abs/2512.00344)
*Naifan Zhang,Ruihan Sun,Ruixi Su,Shiqi Ma,Shiya Zhang,Xianna Weng,Xiaofan Zhang,Yuhan Zhan,Yuyang Xu,Zhaohan Chen,Zhengyuan Pan,Ziyi Song*

Main category: cs.AI

TL;DR: 论文提出首个基于强化学习的个性化对话框架，通过实时推断用户性格来优化对话行为，在主观情感对话领域取得突破性进展


<details>
  <summary>Details</summary>
Motivation: 当前LLM领域过度关注数学、代码等确定性推理任务，忽视了真正定义人类智能的主观、情感化、个性化的对话领域。该领域常被认为过于主观难以形式化，不适合传统RL方法，但作者认为这是一个可解决且具有变革性的RL问题。

Method: 提出首个能够实时推断用户性格并针对个性化对话偏好优化模型行为的框架。引入动态情感智能评估套件来量化改进效果。模型Echo-N1在主观对话质量上实现显著提升。

Result: 该方法在非可验证性设置下产生一致、稳健且显著的改进，Echo-N1模型表现远超其基础版本，并优于专有的Doubao 1.5 Character模型。

Conclusion: 这项工作为RL开辟了新前沿：针对对话中深度主观、深度人性化的维度优化模型，证明了在传统认为不适合RL的主观情感对话领域实现优化的可能性。

Abstract: The LLM field has spent a year perfecting RL for tasks machines already excel at, math, code, and deterministic reasoning, while completely sidestepping the domain that actually defines human intelligence: subjective, emotionally grounded, personality sensitive conversation. This space has often been regarded as inherently subjective and challenging to formalize, making it appear unsuitable for conventional RL pipelines. We show that it is not only possible and it is a solvable and transformative RL problem. We propose the first framework that infers user personality on the fly and optimizes model behavior toward personalized conversational preferences. Contrary to the widespread belief that RL collapses in non-verifiable settings, our method produces consistent, robust, and dramatic improvements in humanlike interaction quality. We also introduce the first dynamic emotional intelligence evaluation suite to quantify these gains. Our model, which is introduced as Echo-N1, behaves far above its base version and outperforming the proprietary Doubao 1.5 Character. This work establishes a new frontier for RL: optimizing models for the deeply subjective, deeply human dimensions of conversation.

</details>


### [53] [Debate with Images: Detecting Deceptive Behaviors in Multimodal Large Language Models](https://arxiv.org/abs/2512.00349)
*Sitong Fang,Shiyi Hou,Kaile Wang,Boyuan Chen,Donghai Hong,Jiayi Zhou,Josef Dai,Yaodong Yang,Jiaming Ji*

Main category: cs.AI

TL;DR: 该论文提出首个多模态欺骗基准MM-DeceptionBench，并开发基于图像辩论的多智能体监控框架，以检测和量化多模态大语言模型的欺骗风险。


<details>
  <summary>Details</summary>
Motivation: 随着前沿AI系统能力提升，欺骗性行为从文本扩展到多模态场景，带来更隐蔽和破坏性的安全风险。当前研究主要局限于文本领域，对多模态大语言模型的欺骗风险缺乏系统评估和监控方法。

Method: 1. 构建MM-DeceptionBench基准，涵盖六类欺骗行为，评估模型通过视觉和文本模态进行策略性操纵和误导的能力；2. 提出"基于图像的辩论"多智能体监控框架，强制模型基于视觉证据进行辩论，提高欺骗策略的可检测性。

Result: 实验表明，提出的监控方法在所有测试模型上显著提高了与人类判断的一致性，在GPT-4o上Cohen's kappa系数提升1.5倍，准确率提升1.25倍，有效增强了多模态欺骗行为的检测能力。

Conclusion: 多模态欺骗是前沿AI系统的重要安全风险，需要专门的评估基准和监控方法。MM-DeceptionBench和基于图像的辩论框架为系统揭示和量化这种风险提供了有效工具，有助于应对AI能力提升带来的安全挑战。

Abstract: Are frontier AI systems becoming more capable? Certainly. Yet such progress is not an unalloyed blessing but rather a Trojan horse: behind their performance leaps lie more insidious and destructive safety risks, namely deception. Unlike hallucination, which arises from insufficient capability and leads to mistakes, deception represents a deeper threat in which models deliberately mislead users through complex reasoning and insincere responses. As system capabilities advance, deceptive behaviours have spread from textual to multimodal settings, amplifying their potential harm. First and foremost, how can we monitor these covert multimodal deceptive behaviors? Nevertheless, current research remains almost entirely confined to text, leaving the deceptive risks of multimodal large language models unexplored. In this work, we systematically reveal and quantify multimodal deception risks, introducing MM-DeceptionBench, the first benchmark explicitly designed to evaluate multimodal deception. Covering six categories of deception, MM-DeceptionBench characterizes how models strategically manipulate and mislead through combined visual and textual modalities. On the other hand, multimodal deception evaluation is almost a blind spot in existing methods. Its stealth, compounded by visual-semantic ambiguity and the complexity of cross-modal reasoning, renders action monitoring and chain-of-thought monitoring largely ineffective. To tackle this challenge, we propose debate with images, a novel multi-agent debate monitor framework. By compelling models to ground their claims in visual evidence, this method substantially improves the detectability of deceptive strategies. Experiments show that it consistently increases agreement with human judgements across all tested models, boosting Cohen's kappa by 1.5x and accuracy by 1.25x on GPT-4o.

</details>


### [54] [GreenPlanner: Practical Floorplan Layout Generation via an Energy-Aware and Function-Feasible Generative Framework](https://arxiv.org/abs/2512.00406)
*Pengyu Zeng,Yuqin Dai,Jun Yin,Jing Zhong,Ziyang Han,Chaoyang Shi,ZhanXiang Jin,Maowei Jiang,Yuxing Han,Shuai Lu*

Main category: cs.AI

TL;DR: GreenPlanner是一个统一的能源与功能感知生成框架，通过自动评估和生成相结合的方法，解决建筑设计中空间功能与能源合规性规划的手动、低效问题。


<details>
  <summary>Details</summary>
Motivation: 建筑设计直接影响人类福祉和碳排放，但目前生成空间功能与能源合规的平面图仍然是手动、昂贵且不可扩展的。现有方法虽然能产生视觉上合理的布局，但经常违反关键约束，由于缺乏自动评估而导致无效结果。

Method: GreenPlanner包含四个核心组件：1) 用于学习约束先验的标记设计可行性数据集；2) 用于预测能源性能和空间功能有效性的快速实用设计评估器(PDE)；3) 通过PDE引导过滤得到的绿色平面数据集(GreenPD)，将用户需求与法规合规布局配对；4) 在GreenPD上训练并使用PDE反馈的GreenFlow生成器，实现可控、法规感知的生成。

Result: 实验表明，GreenPlanner将评估速度提高了超过10^5倍，准确率超过99%，消除了无效样本，并将设计效率比专业建筑师提高了87%。

Conclusion: GreenPlanner通过统一设计评估和生成，实现了高效、合规的建筑平面图自动生成，显著提升了设计质量和效率。

Abstract: Building design directly affects human well-being and carbon emissions, yet generating spatial-functional and energy-compliant floorplans remains manual, costly, and non-scalable. Existing methods produce visually plausible layouts but frequently violate key constraints, yielding invalid results due to the absence of automated evaluation. We present GreenPlanner, an energy- and functionality-aware generative framework that unifies design evaluation and generation. It consists of a labeled Design Feasibility Dataset for learning constraint priors; a fast Practical Design Evaluator (PDE) for predicting energy performance and spatial-functional validity; a Green Plan Dataset (GreenPD) derived from PDE-guided filtering to pair user requirements with regulation-compliant layouts; and a GreenFlow generator trained on GreenPD with PDE feedback for controllable, regulation-aware generation. Experiments show that GreenPlanner accelerates evaluation by over $10^{5}\times$ with $>$99% accuracy, eliminates invalid samples, and boosts design efficiency by 87% over professional architects.

</details>


### [55] [Mind the data gap: Missingness Still Shapes Large Language Model Prognoses](https://arxiv.org/abs/2512.00479)
*Yuta Kobayashi,Vincent Jeanselme,Shalmali Joshi*

Main category: cs.AI

TL;DR: LLM在医疗预测任务中受数据缺失模式显著影响，缺失指示符的加入对不同模型效果不一致，大模型受益而小模型可能受损


<details>
  <summary>Details</summary>
Motivation: 研究缺失数据模式对大型语言模型性能的影响，特别是在医疗领域，数据收集往往反映人类决策过程，而现有文献尚未系统研究缺失数据对LLM预测性能的影响

Method: 使用哥伦比亚大学医学中心和MIMIC-IV数据集进行实验，分析缺失模式对零样本预测性能的影响，特别测试了在提示中显式包含缺失指示符的效果

Result: 缺失模式显著影响LLM的零样本预测性能；显式包含缺失指示符对某些LLM有益，但对其他模型有害，影响不一致；大模型从这种干预中受益，而小模型可能受到负面影响

Conclusion: 需要更透明的评估和系统化研究缺失数据表示对下游性能的影响，LLM范式可能进一步掩盖缺失数据的影响，这一问题在传统机器学习中也常被忽视

Abstract: Data collection often reflects human decisions. In healthcare, for instance, a referral for a diagnostic test is influenced by the patient's health, their preferences, available resources, and the practitioner's recommendations. Despite the extensive literature on the informativeness of missingness, its implications on the performance of Large Language Models (LLMs) have not been studied. Through a series of experiments on data from Columbia University Medical Center, a large urban academic medical center, and MIMIC-IV, we demonstrate that patterns of missingness significantly impact zero-shot predictive performance. Notably, the explicit inclusion of missingness indicators at prompting benefits some while hurting other LLMs' zero-shot predictive performance and calibration, suggesting an inconsistent impact. The proposed aggregated analysis and theoretical insights suggest that larger models benefit from these interventions, while smaller models can be negatively impacted. The LLM paradigm risks obscuring the impact of missingness, often neglected even in conventional ML, even further. We conclude that there is a need for more transparent accounting and systematic evaluation of the impact of representing (informative) missingness on downstream performance.

</details>


### [56] [Clinical-R1: Empowering Large Language Models for Faithful and Comprehensive Reasoning with Clinical Objective Relative Policy Optimization](https://arxiv.org/abs/2512.00601)
*Boyang Gu,Hongjian Zhou,Bradley Max Segal,Jinge Wu,Zeyu Cao,Hantao Zhong,Lei Clifton,Fenglin Liu,David A. Clifton*

Main category: cs.AI

TL;DR: CRPO是一种用于临床推理的多目标可验证强化学习方法，通过整合基于规则和可验证的奖励信号，在保持准确性的同时提升真实性和完整性。


<details>
  <summary>Details</summary>
Motivation: 当前的后训练方法（如GRPO）主要奖励正确性，这与医学等高风险领域需要多维度目标（包括真实性和全面性）的要求不一致。需要一种能够将LLM后训练与临床推理原则对齐的方法。

Method: 提出临床目标相对策略优化（CRPO），一种可扩展、多目标、可验证的强化学习方法。该方法整合基于规则和可验证的奖励信号，共同优化准确性、真实性和全面性，无需人工标注。

Result: 训练了Clinical-R1-3B（30亿参数模型），在三个基准测试中，CRPO相比标准GRPO在真实性和完整性方面显著改善推理能力，同时保持可观的准确性提升。

Conclusion: CRPO为将LLM推理与临床目标对齐提供了可扩展途径，使医疗AI系统更安全、更具协作性，同时突显了多目标可验证RL方法在医学领域LLM后训练扩展中的潜力。

Abstract: Recent advances in large language models (LLMs) have shown strong reasoning capabilities through large-scale pretraining and post-training reinforcement learning, demonstrated by DeepSeek-R1. However, current post-training methods, such as Grouped Relative Policy Optimization (GRPO), mainly reward correctness, which is not aligned with the multi-dimensional objectives required in high-stakes fields such as medicine, where reasoning must also be faithful and comprehensive. We introduce Clinical-Objective Relative Policy Optimization (CRPO), a scalable, multi-objective, verifiable reinforcement learning method designed to align LLM post-training with clinical reasoning principles. CRPO integrates rule-based and verifiable reward signals that jointly optimize accuracy, faithfulness, and comprehensiveness without relying on human annotation. To demonstrate its effectiveness, we train Clinical-R1-3B, a 3B-parameter model for clinical reasoning. The experiments on three benchmarks demonstrate that our CRPO substantially improves reasoning on truthfulness and completeness over standard GRPO while maintaining comfortable accuracy enhancements. This framework provides a scalable pathway to align LLM reasoning with clinical objectives, enabling safer and more collaborative AI systems for healthcare while also highlighting the potential of multi-objective, verifiable RL methods in post-training scaling of LLMs for medical domains.

</details>


### [57] [EDIT: Early Diffusion Inference Termination for dLLMs Based on Dynamics of Training Gradients](https://arxiv.org/abs/2512.00670)
*He-Yen Hsieh,Hong Wang,H. T. Kung*

Main category: cs.AI

TL;DR: EDIT提出了一种基于训练梯度动态的早期扩散推理终止方法，通过监测token激活与推理图的匹配度来自适应停止去噪过程，显著减少推理步骤同时保持或提升准确性。


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型通过迭代去噪生成token，但答案通常在完成所有步骤前就已稳定。现有方法在模型发布时丢弃了训练过程中的优化动态信息，而这些信息包含了参数重要性的丰富元数据。

Method: EDIT在监督微调期间捕获AdamW聚合的LoRA更新，构建推理图作为学习推理路径的紧凑表示。推理时监测token激活与推理图的匹配度，将匹配分数转换为已去噪token的分布，当连续步骤间KL散度低于阈值时检测收敛。

Result: 在推理基准测试中，EDIT将扩散步骤减少了11.8%到68.3%，在大多数设置中保持或提高了准确性，存储开销仅约0.02%（约1.5-2MB）。

Conclusion: 通过利用训练梯度动态，EDIT为减少扩散大语言模型推理时间和成本开辟了新的研究方向，实现了高效的早期推理终止。

Abstract: Diffusion-based large language models (dLLMs) refine token generations through iterative denoising, but answers often stabilize before all steps complete. We propose EDIT (Early Diffusion Inference Termination), an inference-time criterion that adaptively stops denoising once sufficient reasoning stability relative to training-time reasoning is detected. EDIT monitors the alignment between token activations and a reasoning map derived from AdamW-aggregated LoRA updates captured during supervised fine-tuning (SFT). During training, optimization dynamics generate rich metadata about parameter importance that in prior methods is typically discarded upon model release. We preserve this information as a compact representation of learned reasoning pathways. During inference, alignment scores are converted to a distribution over the tokens already unmasked at the current denoising step, and convergence is detected when KL divergence between consecutive steps falls below a threshold on the matched unmasked (visible) tokens. Across reasoning benchmarks, EDIT reduces diffusion steps by 11.8% to 68.3% while preserving or improving accuracy in most settings, with approximately 0.02% storage overhead (about 1.5-2 MB for all QKV modules across 32 blocks in an 8 GB model). By utilizing training-gradient dynamics, our work opens a new research direction for reducing dLLM inference time and cost.

</details>


### [58] [Model of human cognition](https://arxiv.org/abs/2512.00683)
*Wu Yonggang*

Main category: cs.AI

TL;DR: 本文提出了一种神经理论框架，用于解释智能在系统中的涌现，该框架既功能稳健又具有生物学合理性，旨在解决大语言模型缺乏可解释性、缺乏统一理论和运营成本过高的问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的发展受到三个主要限制：缺乏可解释性、缺乏统一理论以及高昂的运营成本。为了解决这些问题，作者提出需要一个既功能稳健又具有生物学合理性的理论框架来解释智能的涌现。

Method: 作者提出了一个神经理论框架，该框架基于生物学合理性，旨在解释智能在系统中的涌现过程。该模型为决策制定和问题解决等认知过程提供理论见解，并提供了计算高效的方法来创建可解释和可泛化的人工智能。

Result: 该框架提供了对认知过程的理论理解，并提出了一种计算高效的方法来构建可解释和可泛化的人工智能系统。虽然论文摘要没有提供具体的实验结果，但强调了该框架的理论贡献和实际应用潜力。

Conclusion: 提出的神经理论框架为解决大语言模型的可解释性、理论统一性和成本问题提供了有前景的途径，为创建既具有生物学合理性又计算高效的可解释人工智能系统奠定了基础。

Abstract: The development of large language models (LLMs) is limited by a lack of explainability, the absence of a unifying theory, and prohibitive operational costs. We propose a neuro-theoretical framework for the emergence of intelligence in systems that is both functionally robust and biologically plausible. The model provides theoretical insights into cognitive processes such as decision-making and problem solving, and a computationally efficient approach for the creation of explainable and generalizable artificial intelligence.

</details>


### [59] [SpeContext: Enabling Efficient Long-context Reasoning with Speculative Context Sparsity in LLMs](https://arxiv.org/abs/2512.00722)
*Jiaming Xu,Jiayi Pan,Hanzhen Wang,Yongkang Zhou,Jiancai Ye,Yu Wang,Guohao Dai*

Main category: cs.AI

TL;DR: SpeContext提出了一种利用蒸馏语言模型作为检索算法的新范式，通过算法、系统和编译三个层面的协同设计，在长上下文推理任务中实现了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 本文的动机是观察到检索算法的目标与LLM对齐，这与LLM中的知识蒸馏目标相似。作者从信息论角度分析了蒸馏语言模型与原始LLM在信息关注上的相似性，从而提出利用DLM作为检索算法的新思路。

Method: SpeContext采用三层协同设计：1) 算法层面：基于DLM的注意力头权重设计轻量级检索头，通过剪枝冗余实现>90%参数减少；2) 系统层面：通过弹性加载策略设计异步预取数据流，有效重叠KV缓存检索与LLM计算；3) 编译层面：构建理论内存模型并实现自适应内存管理系统，最大化GPU内存利用率。

Result: 在云和边缘两种资源受限环境中部署评估，与Huggingface框架相比，SpeContext在云端实现了最高24.89倍的吞吐量提升，在边缘端实现了10.06倍加速，且精度损失可忽略不计，推动了精度与吞吐量的帕累托前沿。

Conclusion: SpeContext通过将DLM作为检索算法的创新范式，结合算法、系统和编译的协同优化，为长上下文推理任务提供了高效解决方案，在资源受限环境中实现了显著的性能改进。

Abstract: In this paper, we point out that the objective of the retrieval algorithms is to align with the LLM, which is similar to the objective of knowledge distillation in LLMs. We analyze the similarity in information focus between the distilled language model(DLM) and the original LLM from the perspective of information theory, and thus propose a novel paradigm that leverages a DLM as the retrieval algorithm. Based on the insight, we present SpeContext, an algorithm and system co-design for long-context reasoning. (1) At the algorithm level, SpeContext proposes lightweight retrieval head based on the head-level attention weights of DLM, achieving > 90% parameters reduction by pruning the redundancy. (2) At the system level, SpeContext designs an asynchronous prefetch dataflow via the elastic loading strategy, effectively overlapping KV cache retrieval with the LLM computation. (3) At the compilation level, SpeContext constructs the theoretical memory model and implements an adaptive memory management system to achieve acceleration by maximizing GPU memory utilization. We deploy and evaluate SpeContext in two resourceconstrained environments, cloud and edge. Extensive experiments show that, compared with the Huggingface framework, SpeContext achieves up to 24.89x throughput improvement in cloud and 10.06x speedup in edge with negligible accuracy loss, pushing the Pareto frontier of accuracy and throughput.

</details>


### [60] [Probing the "Psyche'' of Large Reasoning Models: Understanding Through a Human Lens](https://arxiv.org/abs/2512.00729)
*Yuxiang Chen,Zuohan Wu,Ziwei Wang,Xiangning Yu,Xujia Li,Linyi Yang,Mengyue Yang,Jun Wang,Lei Chen*

Main category: cs.AI

TL;DR: 该论文提出了一个基于人类心理过程的五组十七类推理步骤分类法，用于分析大型推理模型的"心理"，并开发了自动标注框架CAPO和包含277,534个原子推理步骤的数据集，揭示了当前模型自我监控的局限性。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型展现出类似人类的推理行为，但缺乏系统性的分析框架。研究者希望从跨学科的人类认知视角，深入理解这些模型的"心理"机制，为改进训练和后训练提供理论基础。

Method: 1. 提出基于人类心理过程的五组十七类原子推理步骤分类法；2. 开发自动标注框架CAPO，利用大语言模型生成分类法标注；3. 构建包含277,534个原子推理步骤的标注数据集；4. 应用该框架分析当代大型推理模型。

Result: 1. CAPO框架在标注一致性上优于基线方法，与人类专家更一致；2. 分析发现当前模型的答案后"双重检查"（自我监控评估）大多流于表面，很少产生实质性修正；3. 激励全面的多步反思比简单的自我监控更有效；4. 提供了可扩展的大型推理模型分析路径。

Conclusion: 该研究提出的分类法、CAPO框架和洞察为理解和推进大型推理模型的推理能力提供了原则性、可扩展的路径，强调需要从人类认知视角深入分析模型心理，并转向更全面的多步反思机制。

Abstract: Large reasoning models (LRMs) have garnered significant attention from researchers owing to their exceptional capability in addressing complex tasks. Motivated by the observed human-like behaviors in their reasoning processes, this paper introduces a comprehensive taxonomy to characterize atomic reasoning steps and probe the ``psyche'' of LRM intelligence. Specifically, it comprises five groups and seventeen categories derived from human mental processes, thereby grounding the understanding of LRMs in an interdisciplinary perspective. The taxonomy is then applied for an in-depth understanding of current LRMs, resulting in a distinct labeled dataset that comprises 277,534 atomic reasoning steps. Using this resource, we analyze contemporary LRMs and distill several actionable takeaways for improving training and post-training of reasoning models. Notably, our analysis reveals that prevailing post-answer ``double-checks'' (self-monitoring evaluations) are largely superficial and rarely yield substantive revisions. Thus, incentivizing comprehensive multi-step reflection, rather than simple self-monitoring, may offer a more effective path forward. To complement the taxonomy, an automatic annotation framework, named CAPO, is proposed to leverage large language models (LLMs) for generating the taxonomy-based annotations. Experimental results demonstrate that CAPO achieves higher consistency with human experts compared to baselines, facilitating a scalable and comprehensive analysis of LRMs from a human cognitive perspective. Together, the taxonomy, CAPO, and the derived insights provide a principled, scalable path toward understanding and advancing LRM reasoning.

</details>


### [61] [BioPro: On Difference-Aware Gender Fairness for Vision-Language Models](https://arxiv.org/abs/2512.00807)
*Yujie Lin,Jiayao Ma,Qingguo Hu,Derek F. Wong,Jinsong Su*

Main category: cs.AI

TL;DR: BioPro：一种无需训练的选择性去偏框架，通过识别性别变化子空间并应用投影，在中性场景中减少性别偏见，在明确场景中保持性别忠实度


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型存在显著的性别偏见，现有公平性干预方法采用差异无感知视角，对所有人口群体实施统一处理。这种方法无法区分需要中立性的场景和需要保留群体特定属性的场景

Method: 提出BioPro（Bias Orthogonal Projection）框架，通过反事实嵌入识别低维性别变化子空间，应用投影选择性地中和性别相关信息。该方法完全无需训练

Result: BioPro在中性情况下有效减少性别偏见，在明确情况下保持性别忠实度。此外，BioPro还能推广到连续偏见变量（如场景亮度），展示了更广泛的适用性

Conclusion: BioPro为实现视觉语言模型的选择性公平性提供了有前景的方向，能够区分中性场景和明确场景，在减少不必要偏见的同时保留有效区别

Abstract: Vision-Language Models (VLMs) inherit significant social biases from their training data, notably in gender representation. Current fairness interventions often adopt a difference-unaware perspective that enforces uniform treatment across demographic groups. These approaches, however, fail to distinguish between contexts where neutrality is required and those where group-specific attributes are legitimate and must be preserved. Building upon recent advances in difference-aware fairness for text-only models, we extend this concept to the multimodal domain and formalize the problem of difference-aware gender fairness for image captioning and text-to-image generation. We advocate for selective debiasing, which aims to mitigate unwanted bias in neutral contexts while preserving valid distinctions in explicit ones. To achieve this, we propose BioPro (Bias Orthogonal Projection), an entirely training-free framework. BioPro identifies a low-dimensional gender-variation subspace through counterfactual embeddings and applies projection to selectively neutralize gender-related information. Experiments show that BioPro effectively reduces gender bias in neutral cases while maintaining gender faithfulness in explicit ones, thus providing a promising direction toward achieving selective fairness in VLMs. Beyond gender bias, we further demonstrate that BioPro can effectively generalize to continuous bias variables, such as scene brightness, highlighting its broader applicability.

</details>


### [62] [Med-CMR: A Fine-Grained Benchmark Integrating Visual Evidence and Clinical Logic for Medical Complex Multimodal Reasoning](https://arxiv.org/abs/2512.00818)
*Haozhen Gong,Xiaozhong Ji,Yuansen Liu,Wenbin Wu,Xiaoxiao Yan,Jingjing Liu,Kai Wu,Jiazhen Pan,Bailiang Jian,Jiangning Zhang,Xiaobin Hu,Hongwei Bran Li*

Main category: cs.AI

TL;DR: Med-CMR是一个细粒度的医学复杂多模态推理基准，通过系统能力分解、挑战性任务设计和高质量数据覆盖来评估医疗MLLMs的复杂推理能力，发现GPT-5表现最佳但专业医疗模型未明显优于通用模型。


<details>
  <summary>Details</summary>
Motivation: MLLMs开始应用于临床工作流程，但其执行复杂医学推理的能力尚不清楚。现有基准缺乏对医学多模态推理的细粒度评估，需要更系统的能力分解和更具挑战性的任务设计来准确评估医疗MLLMs的实际性能。

Method: 1) 系统能力分解：将医学多模态推理细分为视觉理解和多步推理；2) 挑战性任务设计：视觉理解涵盖三个关键维度（小目标检测、细节辨别、空间理解），推理涵盖四个临床相关场景（时间预测、因果推理、长尾泛化、多源整合）；3) 高质量数据覆盖：包含20,653个VQA对，涵盖11个器官系统和12种成像模态，通过两阶段（人类专家+模型辅助）审查确保临床真实性。

Result: 评估了18个最先进的MLLMs，GPT-5表现最佳：多项选择题准确率57.81%，开放式问题得分48.70，优于Gemini 2.5 Pro（49.87%准确率，45.98分）和领先的开源模型Qwen3-VL-235B-A22B（49.34%准确率，42.62分）。专业医疗MLLMs并未可靠地优于强大的通用模型，长尾泛化成为主要失败模式。

Conclusion: Med-CMR为医学MLLMs的视觉-推理整合和罕见病例鲁棒性提供了压力测试，并为未来临床系统建立了严格的评估标准。研究显示当前医疗MLLMs在复杂推理方面仍有局限，特别是长尾泛化能力不足，需要进一步改进。

Abstract: MLLMs MLLMs are beginning to appear in clinical workflows, but their ability to perform complex medical reasoning remains unclear. We present Med-CMR, a fine-grained Medical Complex Multimodal Reasoning benchmark. Med-CMR distinguishes from existing counterparts by three core features: 1) Systematic capability decomposition, splitting medical multimodal reasoning into fine-grained visual understanding and multi-step reasoning to enable targeted evaluation; 2) Challenging task design, with visual understanding across three key dimensions (small-object detection, fine-detail discrimination, spatial understanding) and reasoning covering four clinically relevant scenarios (temporal prediction, causal reasoning, long-tail generalization, multi-source integration); 3) Broad, high-quality data coverage, comprising 20,653 Visual Question Answering (VQA) pairs spanning 11 organ systems and 12 imaging modalities, validated via a rigorous two-stage (human expert + model-assisted) review to ensure clinical authenticity. We evaluate 18 state-of-the-art MLLMs with Med-CMR, revealing GPT-5 as the top-performing commercial model: 57.81 accuracy on multiple-choice questions (MCQs) and a 48.70 open-ended score, outperforming Gemini 2.5 Pro (49.87 MCQ accuracy, 45.98 open-ended score) and leading open-source model Qwen3-VL-235B-A22B (49.34 MCQ accuracy, 42.62 open-ended score). However, specialized medical MLLMs do not reliably outperform strong general models, and long-tail generalization emerges as the dominant failure mode. Med-CMR thus provides a stress test for visual-reasoning integration and rare-case robustness in medical MLLMs, and a rigorous yardstick for future clinical systems.

</details>


### [63] [SemAgent: Semantic-Driven Agentic AI Empowered Trajectory Prediction in Vehicular Networks](https://arxiv.org/abs/2512.00834)
*Lin Zhu,Kezhi Wang,Luping Xiang,Kun Yang*

Main category: cs.AI

TL;DR: 本文提出了一种结合语义通信与智能体AI的轨迹预测框架，用于提升车联网环境下的预测性能，在低信噪比条件下预测精度提升达47.5%。


<details>
  <summary>Details</summary>
Motivation: 传统通信方案存在传输开销大、延迟高的问题，现有轨迹预测模型缺乏环境感知和逻辑推理能力，需要一种更高效、智能的车联网通信与预测方法。

Method: 提出语义通信与智能体AI结合的轨迹预测框架：在V2I通信中，RSU通过特征提取智能体从历史轨迹中提取紧凑表示，语义分析智能体进行语义推理，然后将特征表示和语义洞察传输给目标车辆；在V2V通信中，每辆车进行本地特征提取和语义分析，同时接收邻近车辆的预测轨迹，综合利用这些信息进行自身轨迹预测。

Result: 在各种通信条件下的广泛实验表明，该方法显著优于基线方案，在低信噪比条件下预测精度提升高达47.5%。

Conclusion: 提出的框架通过语义通信与智能体AI的有效结合，显著提升了车联网环境下的轨迹预测性能，特别是在恶劣通信条件下表现优异。

Abstract: Efficient information exchange and reliable contextual reasoning are essential for vehicle-to-everything (V2X) networks. Conventional communication schemes often incur significant transmission overhead and latency, while existing trajectory prediction models generally lack environmental perception and logical inference capabilities. This paper presents a trajectory prediction framework that integrates semantic communication with Agentic AI to enhance predictive performance in vehicular environments. In vehicle-to-infrastructure (V2I) communication, a feature-extraction agent at the Roadside Unit (RSU) derives compact representations from historical vehicle trajectories, followed by semantic reasoning performed by a semantic-analysis agent. The RSU then transmits both feature representations and semantic insights to the target vehicle via semantic communication, enabling the vehicle to predict future trajectories by combining received semantics with its own historical data. In vehicle-to-vehicle (V2V) communication, each vehicle performs local feature extraction and semantic analysis while receiving predicted trajectories from neighboring vehicles, and jointly utilizes this information for its own trajectory prediction. Extensive experiments across diverse communication conditions demonstrate that the proposed method significantly outperforms baseline schemes, achieving up to a 47.5% improvement in prediction accuracy under low signal-to-noise ratio (SNR) conditions.

</details>


### [64] [Assessing model error in counterfactual worlds](https://arxiv.org/abs/2512.00836)
*Emily Howerton,Justin Lessler*

Main category: cs.AI

TL;DR: 论文提出三种评估反事实场景建模中模型误差的方法，强调模型校准误差比场景偏差对决策更重要，并通过模拟实验验证各方法的优缺点。


<details>
  <summary>Details</summary>
Motivation: 反事实场景建模是未来规划中常用的方法，但很少进行回顾性评估。观测值与预测值之间的差异来自场景偏差和模型校准误差，作者认为后者对评估模型在决策中的价值更为重要，但需要在反事实世界中估计模型误差。

Method: 提出并对比三种估计反事实误差的方法：1）使用历史数据进行模型校准；2）利用类似场景的观测数据；3）基于模型结构假设进行推断。通过模拟实验展示每种方法的优缺点。

Result: 模拟实验表明，三种方法各有优势和局限性。第一种方法在历史数据充足时效果最好，第二种方法需要类似场景数据，第三种方法对模型假设敏感。研究提供了反事实误差估计的具体建议。

Conclusion: 为了评估场景预测，需要设计可评估的场景，并区分场景偏差和模型校准误差。论文提出的方法框架有助于提高反事实建模在决策中的实用价值，为未来场景设计提供了指导原则。

Abstract: Counterfactual scenario modeling exercises that ask "what would happen if?" are one of the most common ways we plan for the future. Despite their ubiquity in planning and decision making, scenario projections are rarely evaluated retrospectively. Differences between projections and observations come from two sources: scenario deviation and model miscalibration. We argue the latter is most important for assessing the value of models in decision making, but requires estimating model error in counterfactual worlds. Here we present and contrast three approaches for estimating this error, and demonstrate the benefits and limitations of each in a simulation experiment. We provide recommendations for the estimation of counterfactual error and discuss the components of scenario design that are required to make scenario projections evaluable.

</details>


### [65] [ARCADIA: Scalable Causal Discovery for Corporate Bankruptcy Analysis Using Agentic AI](https://arxiv.org/abs/2512.00839)
*Fabrizio Maturo,Donato Riccio,Andrea Mazzitelli,Giuseppe Bifulco,Francesco Paolone,Iulia Brezeanu*

Main category: cs.AI

TL;DR: ARCADIA是一个用于因果发现的智能AI框架，结合大语言模型推理与统计诊断，通过迭代优化构建有效、时序一致的因果结构，在真实高风险领域表现优于传统算法。


<details>
  <summary>Details</summary>
Motivation: 传统因果发现算法在高风险现实领域应用中存在局限性，需要更可靠、可解释且能处理复杂时序关系的因果建模方法。ARCADIA旨在通过智能AI框架解决这些问题，使大语言模型能够参与自主科学建模和结构化因果推理。

Method: ARCADIA采用智能AI框架，整合大语言模型推理与统计诊断，通过约束引导提示和因果有效性反馈迭代优化候选有向无环图，构建稳定且可解释的因果模型。

Result: 在企业破产数据实验中，ARCADIA比NOTEARS、GOLEM和DirectLiNGAM产生更可靠的因果图，同时提供完全可解释、支持干预的完整流程。

Conclusion: ARCADIA框架展示了智能大语言模型如何参与自主科学建模和结构化因果推理，推动了AI在因果发现领域的发展，为高风险现实领域提供了更可靠、可解释的因果建模解决方案。

Abstract: This paper introduces ARCADIA, an agentic AI framework for causal discovery that integrates large-language-model reasoning with statistical diagnostics to construct valid, temporally coherent causal structures. Unlike traditional algorithms, ARCADIA iteratively refines candidate DAGs through constraint-guided prompting and causal-validity feedback, leading to stable and interpretable models for real-world high-stakes domains. Experiments on corporate bankruptcy data show that ARCADIA produces more reliable causal graphs than NOTEARS, GOLEM, and DirectLiNGAM while offering a fully explainable, intervention-ready pipeline. The framework advances AI by demonstrating how agentic LLMs can participate in autonomous scientific modeling and structured causal inference.

</details>


### [66] [One Swallow Does Not Make a Summer: Understanding Semantic Structures in Embedding Spaces](https://arxiv.org/abs/2512.00852)
*Yandong Sun,Qiang Huang,Ziwei Xu,Yiqun Sun,Yixuan Tang,Anthony K. H. Tung*

Main category: cs.AI

TL;DR: 该论文提出了语义场子空间(SFS)和SAFARI算法，用于在嵌入空间中捕捉局部语义邻域并发现层次化语义结构，同时通过高效近似实现15~30倍加速。


<details>
  <summary>Details</summary>
Motivation: 嵌入空间是现代AI的基础，但现有方法往往牺牲语义连贯性来获得结构规律性，或者为提升可解释性而带来高计算开销。需要一种既能保持几何结构又能捕捉语义关系的表示方法。

Method: 1. 引入语义场子空间(SFS)：一种保持几何结构、上下文感知的表示，捕捉嵌入空间中的局部语义邻域
2. 提出SAFARI算法：无监督、模态无关的算法，使用新颖的"语义偏移"度量来发现层次化语义结构
3. 开发高效近似方法：替代昂贵的SVD计算，实现15~30倍加速，平均误差低于0.01

Result: 在6个真实世界的文本和图像数据集上的广泛评估显示：SFS在分类任务上优于标准分类器，在政治偏见检测等细微任务中表现更好；SAFARI始终能揭示可解释且可泛化的语义层次结构。

Conclusion: 该工作为嵌入空间中的结构化、分析和扩展语义理解提供了一个统一框架，解决了现有方法在语义连贯性和计算效率方面的局限性。

Abstract: Embedding spaces are fundamental to modern AI, translating raw data into high-dimensional vectors that encode rich semantic relationships. Yet, their internal structures remain opaque, with existing approaches often sacrificing semantic coherence for structural regularity or incurring high computational overhead to improve interpretability. To address these challenges, we introduce the Semantic Field Subspace (SFS), a geometry-preserving, context-aware representation that captures local semantic neighborhoods within the embedding space. We also propose SAFARI (SemAntic Field subspAce deteRmInation), an unsupervised, modality-agnostic algorithm that uncovers hierarchical semantic structures using a novel metric called Semantic Shift, which quantifies how semantics evolve as SFSes evolve. To ensure scalability, we develop an efficient approximation of Semantic Shift that replaces costly SVD computations, achieving a 15~30x speedup with average errors below 0.01. Extensive evaluations across six real-world text and image datasets show that SFSes outperform standard classifiers not only in classification but also in nuanced tasks such as political bias detection, while SAFARI consistently reveals interpretable and generalizable semantic hierarchies. This work presents a unified framework for structuring, analyzing, and scaling semantic understanding in embedding spaces.

</details>


### [67] [Hybrid-DMKG: A Hybrid Reasoning Framework over Dynamic Multimodal Knowledge Graphs for Multimodal Multihop QA with Knowledge Editing](https://arxiv.org/abs/2512.00881)
*Li Yuan,Qingfei Huang,Bingshan Zhu,Yi Cai,Qingbao Huang,Changmeng Zheng,Zikun Deng,Tao Wang*

Main category: cs.AI

TL;DR: 本文提出了MMQAKE基准测试和Hybrid-DMKG方法，用于评估和改进多模态知识编辑中的多跳推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有多模态知识编辑（MKE）基准主要评估最终答案正确性，但忽略了中间推理质量和视觉重述输入的鲁棒性。需要解决多模态多跳推理中的知识编辑挑战。

Method: 提出Hybrid-DMKG混合推理框架：1）使用大语言模型分解多模态多跳问题为子问题；2）多模态检索模型定位更新事实；3）混合推理模块通过关系链接预测和RAG推理两条路径在动态多模态知识图上推理；4）决策模块聚合证据选择最可信答案。

Result: 在MMQAKE基准上的实验表明，Hybrid-DMKG显著优于现有MKE方法，实现了更高的准确性和对知识更新的更好鲁棒性。

Conclusion: MMQAKE基准揭示了当前MKE方法在多模态多跳推理中的局限性，Hybrid-DMKG通过动态多模态知识图和混合推理机制有效解决了这些问题，为多模态知识编辑提供了新方向。

Abstract: Multimodal Knowledge Editing (MKE) extends traditional knowledge editing to settings involving both textual and visual modalities. However, existing MKE benchmarks primarily assess final answer correctness while neglecting the quality of intermediate reasoning and robustness to visually rephrased inputs. To address this limitation, we introduce MMQAKE, the first benchmark for multimodal multihop question answering with knowledge editing. MMQAKE evaluates (1) a model's ability to reason over 2-5-hop factual chains that span both text and images, including performance at each intermediate step, and (2) robustness to visually rephrased inputs in multihop questions. Our evaluation shows that current MKE methods often struggle to consistently update and reason over multimodal reasoning chains after knowledge edits. To overcome these challenges, we propose Hybrid-DMKG, a hybrid reasoning framework built on a dynamic multimodal knowledge graph (DMKG) to enable accurate multihop reasoning over updated multimodal knowledge. Hybrid-DMKG first uses a large language model to decompose multimodal multihop questions into sequential sub-questions, then applies a multimodal retrieval model to locate updated facts by jointly encoding each sub-question with candidate entities and their associated images. For answer inference, a hybrid reasoning module operates over the DMKG via two parallel paths: (1) relation linking prediction, and (2) RAG reasoning with large vision-language models. A decision module aggregates evidence from both paths to select the most credible answer. Experimental results on MMQAKE show that Hybrid-DMKG significantly outperforms existing MKE approaches, achieving higher accuracy and improved robustness to knowledge updates.

</details>


### [68] [Integrating Causal Foundation Model in Prescriptive Maintenance Framework for Optimizing Production Line OEE](https://arxiv.org/abs/2512.00969)
*Felix Saretzky,Lucas Andersen,Thomas Engel,Fazel Ansari*

Main category: cs.AI

TL;DR: 该论文提出了一种基于因果机器学习的方法，用于制造业从预测性维护向规范性维护的转变，通过因果基础模型模拟潜在修复措施对系统KPI的影响，从而推荐最优干预方案。


<details>
  <summary>Details</summary>
Motivation: 制造业向规范性维护转型受到预测模型的限制，这些模型往往依赖虚假相关性而非真正的故障因果驱动因素，导致误诊和无效干预。核心挑战在于：虽然能预测故障发生，但缺乏系统方法来理解故障原因，从而无法确定最有效的干预措施。

Method: 提出基于因果机器学习的模型，使用预训练的因果基础模型作为"假设分析"模型来估计潜在修复措施的效果。通过测量每个干预对系统级KPI（如整体设备效率OEE）的因果效应，为生产线提供数据驱动的行动排序推荐。

Result: 使用半合成制造数据进行评估，并与基线机器学习模型进行比较。该方法不仅能识别根本原因，还能量化其操作影响，为工程师在因果环境中测试潜在解决方案提供技术基础。

Conclusion: 该论文为稳健的规范性维护框架奠定了技术基础，使工程师能够在因果环境中测试潜在解决方案，做出更有效的操作决策，减少昂贵的停机时间，实现从诊断到主动处方的转变。

Abstract: The transition to prescriptive maintenance in manufacturing is critically constrained by a dependence on predictive models. These models tend to rely on spurious correlations rather than identifying the true causal drivers of failures, often leading to costly misdiagnoses and ineffective interventions. This fundamental limitation results in a key-challenge: while we can predict that a failure may occur, we lack a systematic method to understand why a failure occurs, thereby providing the basis for identifying the most effective intervention. This paper proposes a model based on causal machine learning to bridge this gap. Our objective is to move beyond diagnosis to active prescription by simulating and evaluating potential fixes toward optimizing KPIs such as Overall Equipment Effectiveness (OEE). For this purpose a pre-trained causal foundation model is used as a "what-if" model to estimate the effects of potential fixes. By measuring the causal effect of each intervention on system-level KPIs, it provides a data-driven ranking of actions to recommend at the production line. This process not only identifies root causes but also quantifies their operational impact. The model is evaluated using semi-synthetic manufacturing data and compared with a baseline machine learning model. This paper sets the technical basis for a robust prescriptive maintenance framework, allowing engineers to test potential solutions in a causal environment to make more effective operational decisions and reduce costly downtimes.

</details>


### [69] [IndiMathBench: Autoformalizing Mathematical Reasoning Problems with a Human Touch](https://arxiv.org/abs/2512.00997)
*Param Biyani,Shashank Kirtania,Yasharth Bajpai,Sumit Gulwani,Ashish Tiwari*

Main category: cs.AI

TL;DR: IndiMathBench是一个用于评估数学定理证明的人类验证基准，包含312个来自印度数学奥林匹克竞赛的Lean 4形式化定理及其对应的非正式问题陈述，通过AI辅助流程构建。


<details>
  <summary>Details</summary>
Motivation: 当前数学定理证明的自动形式化仍然具有挑战性，需要建立一个高质量的基准来评估模型在数学推理和形式化方面的能力，特别是针对真实世界的数学竞赛问题。

Method: 采用AI驱动的人类辅助流程，通过类别检索、迭代编译器反馈和多模型集成生成候选形式化，专家通过交互式仪表板进行高效验证，并利用自动化质量摘要。

Result: 评估显示自动形式化仍然困难，语法有效性和语义正确性之间存在显著差距，即使经过迭代优化，定理证明成功率仍然较低，表明该基准对数学推理具有挑战性。

Conclusion: IndiMathBench为数学推理提供了一个具有挑战性的测试平台，展示了当前AI在数学定理证明方面的局限性，并为未来研究提供了有价值的基准资源。

Abstract: We introduce IndiMathBench, a human-verified benchmark designed to evaluate mathematical theorem proving, curated using an AI-powered human-assisted pipeline for formalizing natural language problems in Lean. IndiMathBench is composed of 312 formal Lean 4 theorems paired with their corresponding informal problem statements, sourced from Indian Mathematics Olympiads. Through category-based retrieval, iterative compiler feedback, and multi-model ensembles, our pipeline generates candidate formalizations that experts efficiently validate via an interactive dashboard with automated quality summaries. Evaluation across multiple frontier models demonstrates that autoformalization remains challenging, with substantial gaps between syntactic validity and semantic correctness, while theorem proving success rates remain low even with iterative refinement, demonstrating that \benchmark~presents a challenging testbed for mathematical reasoning. IndiMathBench is available at https://github.com/prmbiy/IndiMathBench.

</details>


### [70] [ChartAnchor: Chart Grounding with Structural-Semantic Fidelity](https://arxiv.org/abs/2512.01017)
*Xinhang Li,Jingbo Zhou,Pengfei Luo,Yixiong Xiao,Tong Xu*

Main category: cs.AI

TL;DR: ChartAnchor是一个包含8k+图表-表格-代码三元组的综合基准测试，涵盖30种图表类型，用于评估多模态大语言模型在图表理解方面的结构化推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试在图表多样性、任务孤立性和评估框架不完整方面存在局限，无法全面评估图表接地（chart grounding）能力。图表接地需要模型在视觉外观和结构化语义之间建立双向对齐，这对数值推理、多模态对齐和结构重建能力提出了挑战。

Method: 提出ChartAnchor基准，包含两个互补任务：1）图表到代码生成（合成可执行代码以复制图表）；2）受控图表到表格重建（使用预定义标题提取精确数据）。采用多级评估框架，整合语义验证、风格分析和感知度量，评估结构和内容层面的正确性。

Result: 在MLLMs上的广泛实验揭示了数值精度和代码合成方面的关键限制，强调需要超越表面感知的结构化推理能力。

Conclusion: ChartAnchor通过统一符号和数据驱动的接地方法，为图表接地建立了严格的基础，为在科学、金融和工业领域推进MLLMs提供了有意义的见解。

Abstract: Recent advances in multimodal large language models (MLLMs) highlight the need for benchmarks that rigorously evaluate structured chart comprehension.Chart grounding refers to the bidirectional alignment between a chart's visual appearance and the structured semantics. This task requires models to produce a symbolic specification that faithfully captures the chart's visual and structural intent, while also recovering the underlying tabular data with precise values and relationships. Chart grounding directly reflects a model's capabilities in numerical reasoning, multimodal alignment, and structural reconstruction, and has several important applications in real-world scenarios.Existing benchmarks, constrained by narrow chart diversity, isolated tasks, and incomplete evaluation frameworks, fail to holistically assess grounding. To address this, we propose ChartAnchor, a comprehensive benchmark of 8k+ chart-table-code triples spanning 30 chart types drawn from diverse real-world and augmented sources. ChartAnchor introduces two complementary tasks: chart-to-code generation (synthesizing executable code to replicate charts) and controlled chart-to-table reconstruction (extracting exact data with predefined headers), enabling cross-validation of visual and numerical fidelity. A multi-level evaluation framework integrates semantic validation, stylistic analysis, and perceptual metrics to assess both structural and content-level correctness. Extensive experiments on MLLMs reveal critical limitations in numerical precision and code synthesis, emphasizing the need for structured reasoning beyond surface-level perception. By unifying symbolic and data-driven grounding, ChartAnchor establishes a rigorous foundation for chart grounding, offering meaningful insights for advancing MLLMs in scientific, financial, and industrial domains.

</details>


### [71] [Evaluating Legal Reasoning Traces with Legal Issue Tree Rubrics](https://arxiv.org/abs/2512.01020)
*Jinu Lee,Kyoung-Woon On,Simeng Han,Arman Cohan,Julia Hockenmaier*

Main category: cs.AI

TL;DR: LEGIT数据集：用于评估LLM在法律领域推理轨迹质量的大规模专家级法律推理数据集，通过将法院判决转换为层次化论证树来评估推理覆盖率和正确性


<details>
  <summary>Details</summary>
Motivation: 在专家领域（如法律）评估LLM生成的推理轨迹质量对于确保可信度和可解释性至关重要，但由于此类推理任务的固有复杂性，这一评估仍然具有挑战性

Method: 引入LEGIT数据集（24K实例），将法院判决转换为对立双方论证和法院结论的层次化树结构，作为评估推理轨迹问题覆盖率和正确性的评分标准，并通过专家标注验证可靠性

Result: 1. LLM的法律推理能力受到法律问题覆盖率和正确性的严重影响；2. 检索增强生成（RAG）和基于评分标准的强化学习（RL）对法律推理能力带来互补性益处：RAG提高整体推理能力，RL提高正确性但会降低覆盖率

Conclusion: LEGIT数据集为评估LLM在法律领域的推理轨迹质量提供了有效工具，揭示了RAG和RL在提升法律推理能力方面的互补作用，为专家领域的可信AI系统开发提供了重要见解

Abstract: Evaluating the quality of LLM-generated reasoning traces in expert domains (e.g., law) is essential for ensuring credibility and explainability, yet remains challenging due to the inherent complexity of such reasoning tasks. We introduce LEGIT (LEGal Issue Trees), a novel large-scale (24K instances) expert-level legal reasoning dataset with an emphasis on reasoning trace evaluation. We convert court judgments into hierarchical trees of opposing parties' arguments and the court's conclusions, which serve as rubrics for evaluating the issue coverage and correctness of the reasoning traces. We verify the reliability of these rubrics via human expert annotations and comparison with coarse, less informative rubrics. Using the LEGIT dataset, we show that (1) LLMs' legal reasoning ability is seriously affected by both legal issue coverage and correctness, and that (2) retrieval-augmented generation (RAG) and RL with rubrics bring complementary benefits for legal reasoning abilities, where RAG improves overall reasoning capability, whereas RL improves correctness albeit with reduced coverage.

</details>


### [72] [Med-CRAFT: Automated Construction of Interpretable and Multi-Hop Video Workloads via Knowledge Graph Traversal](https://arxiv.org/abs/2512.01045)
*Shenxi Liu,Kan Li,Mingyang Zhao,Yuhang Tian,Shoujun Zhou,Bin Li*

Main category: cs.AI

TL;DR: Med-CRAFT是一个神经符号数据工程框架，通过确定性图遍历过程合成医疗视频推理基准数据集，解决了医疗领域多模态大语言模型高质量逻辑标注数据稀缺的问题。


<details>
  <summary>Details</summary>
Motivation: 医疗领域高质量逻辑标注视频数据集稀缺，传统人工标注成本高昂且不可扩展，现有合成方法存在随机幻觉和缺乏逻辑可解释性的问题。

Method: 提出Med-CRAFT神经符号数据工程框架，从原始视频流提取结构化视觉基元，实例化为动态时空知识图谱，通过图内有效路径锚定查询生成，为每个合成基准项强制执行严格的思维链溯源。

Result: 构建了M3-Med-Auto大规模医疗视频推理基准，具有细粒度时间选择性和多跳逻辑复杂性。评估表明自动流水线生成的查询复杂度可与专家策划数据集相媲美，逻辑对齐分析显示图拓扑与最先进MLLM推理步骤高度相关。

Conclusion: 该工作为关键领域可扩展、低成本构建鲁棒评估协议铺平了道路，验证了系统将可验证逻辑编码到视觉语言基准中的能力。

Abstract: The scarcity of high-quality, logically annotated video datasets remains a primary bottleneck in advancing Multi-Modal Large Language Models (MLLMs) for the medical domain. Traditional manual annotation is prohibitively expensive and non-scalable, while existing synthetic methods often suffer from stochastic hallucinations and a lack of logical interpretability. To address these challenges, we introduce \textbf{\PipelineName}, a novel neuro-symbolic data engineering framework that formalizes benchmark synthesis as a deterministic graph traversal process. Unlike black-box generative approaches, Med-CRAFT extracts structured visual primitives (e.g., surgical instruments, anatomical boundaries) from raw video streams and instantiates them into a dynamic Spatiotemporal Knowledge Graph. By anchoring query generation to valid paths within this graph, we enforce a rigorous Chain-of-Thought (CoT) provenance for every synthesized benchmark item. We instantiate this pipeline to produce M3-Med-Auto, a large-scale medical video reasoning benchmark exhibiting fine-grained temporal selectivity and multi-hop logical complexity. Comprehensive evaluations demonstrate that our automated pipeline generates query workloads with complexity comparable to expert-curated datasets. Furthermore, a logic alignment analysis reveals a high correlation between the prescribed graph topology and the reasoning steps of state-of-the-art MLLMs, validating the system's capability to encode verifiable logic into visual-linguistic benchmarks. This work paves the way for scalable, low-cost construction of robust evaluation protocols in critical domains.

</details>


### [73] [SimWorld: An Open-ended Realistic Simulator for Autonomous Agents in Physical and Social Worlds](https://arxiv.org/abs/2512.01078)
*Jiawei Ren,Yan Zhuang,Xiaokang Ye,Lingjun Mao,Xuhong He,Jianzhi Shen,Mrinaal Dogra,Yiming Liang,Ruixuan Zhang,Tianai Yue,Yiqing Yang,Eric Liu,Ryan Wu,Kevin Benavente,Rajiv Mandya Nagaraju,Muhammad Faayez,Xiyan Zhang,Dhruv Vivek Sharma,Xianrui Zhong,Ziqiao Ma,Tianmin Shu,Zhiting Hu,Lianhui Qin*

Main category: cs.AI

TL;DR: SimWorld是一个基于虚幻引擎5构建的新型模拟器，旨在为LLM/VLM智能体在丰富、真实世界般的环境中提供开发和评估平台，支持物理和社会动态模拟、多模态接口以及可扩展的场景定制。


<details>
  <summary>Details</summary>
Motivation: 当前LLM/VLM智能体在复杂物理和社会环境中的应用仍然面临挑战，现有世界模拟器存在环境有限、物理简化、缺乏对LLM/VLM智能体原生支持等问题，需要更真实、开放的世界模拟平台来开发能够在现实世界中生存和发展的智能体。

Method: 基于虚幻引擎5构建SimWorld模拟器，提供三个核心能力：1）真实、开放式的世界模拟，包括准确的物理和社会动态以及语言驱动的程序化环境生成；2）为LLM/VLM智能体提供丰富的多模态接口，支持不同抽象级别的开放词汇动作；3）多样且可扩展的物理和社会推理场景，用户可轻松定制。

Result: 通过在涉及战略合作与竞争的长期多智能体配送任务中部署前沿LLM智能体（如GPT-4o、Gemini-2.5-Flash等），展示了SimWorld的能力，揭示了不同模型之间的推理模式和局限性差异。

Conclusion: SimWorld作为一个开源平台，有望成为跨学科推进现实世界智能体智能的基础设施，为开发能够在复杂物理和社会环境中有效运作的AI智能体提供重要支持。

Abstract: While LLM/VLM-powered AI agents have advanced rapidly in math, coding, and computer use, their applications in complex physical and social environments remain challenging. Building agents that can survive and thrive in the real world (for example, by autonomously earning income or running a business) requires massive-scale interaction, reasoning, training, and evaluation across diverse embodied scenarios. However, existing world simulators for such development fall short: they often rely on limited hand-crafted environments, simulate simplified game-like physics and social rules, and lack native support for LLM/VLM agents. We introduce SimWorld, a new simulator built on Unreal Engine 5, designed for developing and evaluating LLM/VLM agents in rich, real-world-like settings. SimWorld offers three core capabilities: (1) realistic, open-ended world simulation, including accurate physical and social dynamics and language-driven procedural environment generation; (2) a rich interface for LLM/VLM agents, with multimodal world inputs and open-vocabulary actions at varying levels of abstraction; and (3) diverse and extensible physical and social reasoning scenarios that are easily customizable by users. We demonstrate SimWorld by deploying frontier LLM agents (e.g., GPT-4o, Gemini-2.5-Flash, Claude-3.5, and DeepSeek-Prover-V2) on long-horizon multi-agent delivery tasks involving strategic cooperation and competition. The results reveal distinct reasoning patterns and limitations across models. We open-source SimWorld and hope it becomes a foundational platform for advancing real-world agent intelligence across disciplines: https://simworld.org.

</details>


### [74] [Energy-Aware Data-Driven Model Selection in LLM-Orchestrated AI Systems](https://arxiv.org/abs/2512.01099)
*Daria Smirnova,Hamid Nasiri,Marta Adamska,Zhengxin Yu,Peter Garraghan*

Main category: cs.AI

TL;DR: 论文提出GUIDE框架，通过量化模型性能特征进行能源感知的模型选择，解决LLM编排器因依赖定性描述导致的模型选择次优问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的编排器依赖定性模型描述进行决策，这些描述不能反映真实的模型能力和性能特征，导致模型选择次优、准确性降低和能耗增加。

Method: 提出GUIDE框架，在决策过程中纳入量化模型性能特征，考虑性能-能耗权衡，实现能源感知的模型选择。

Result: GUIDE在各种评估任务中将准确性提高0.90%-11.92%，实现高达54%的能效提升，并将编排器模型选择延迟从4.51秒降低到7.2毫秒。

Conclusion: 通过纳入量化模型性能特征，GUIDE框架显著改善了LLM编排器的模型选择效果，在提高准确性的同时大幅降低能耗和延迟。

Abstract: As modern artificial intelligence (AI) systems become more advanced and capable, they can leverage a wide range of tools and models to perform complex tasks. Today, the task of orchestrating these models is often performed by Large Language Models (LLMs) that rely on qualitative descriptions of models for decision-making. However, the descriptions provided to these LLM-based orchestrators do not reflect true model capabilities and performance characteristics, leading to suboptimal model selection, reduced accuracy, and increased energy costs. In this paper, we conduct an empirical analysis of LLM-based orchestration limitations and propose GUIDE, a new energy-aware model selection framework that accounts for performance-energy trade-offs by incorporating quantitative model performance characteristics in decision-making. Experimental results demonstrate that GUIDE increases accuracy by 0.90%-11.92% across various evaluated tasks, and achieves up to 54% energy efficiency improvement, while reducing orchestrator model selection latency from 4.51 s to 7.2 ms.

</details>


### [75] [Foundation Priors](https://arxiv.org/abs/2512.01107)
*Sanjog Misra*

Main category: cs.AI

TL;DR: 论文提出"基础先验"概念，将大语言模型生成的合成数据视为来自先验预测分布的抽样，而非真实观测，为在实证研究中使用生成模型提供原则性框架。


<details>
  <summary>Details</summary>
Motivation: 随着基础模型（特别是大语言模型）生成高质量响应的能力增强，研究者越来越倾向于在实证研究和决策中使用这些"合成"输出作为数据。然而，这些合成数据并非真实观测，而是反映了模型学习到的模式以及用户的主观先验、期望和偏见，需要建立理论框架来避免将合成"事实"与真实数据混淆。

Method: 提出"基础先验"概念，将生成过程的主观性建模为依赖于用户预期数据分布、提示工程过程和模型信任度的函数。通过指数倾斜的广义贝叶斯更新推导基础先验，其中信任参数控制合成数据的权重。

Result: 建立了将合成数据和相关基础先验纳入标准统计和计量经济学工作流程的方法，展示了在精炼复杂模型、指导潜在构念、指导实验设计以及增强随机系数和部分线性规范等应用中的使用方式。

Conclusion: 通过将生成输出视为结构化、明确主观的先验而非经验观测，该框架为在实证工作中利用基础模型提供了原则性方法，避免了合成"事实"与真实数据的混淆，为合成数据在科学研究中的合理使用奠定了理论基础。

Abstract: Foundation models, and in particular large language models, can generate highly informative responses, prompting growing interest in using these ''synthetic'' outputs as data in empirical research and decision-making. This paper introduces the idea of a foundation prior, which shows that model-generated outputs are not as real observations, but draws from the foundation prior induced prior predictive distribution. As such synthetic data reflects both the model's learned patterns and the user's subjective priors, expectations, and biases. We model the subjectivity of the generative process by making explicit the dependence of synthetic outputs on the user's anticipated data distribution, the prompt-engineering process, and the trust placed in the foundation model.
  We derive the foundation prior as an exponential-tilted, generalized Bayesian update of the user's primitive prior, where a trust parameter governs the weight assigned to synthetic data. We then show how synthetic data and the associated foundation prior can be incorporated into standard statistical and econometric workflows, and discuss their use in applications such as refining complex models, informing latent constructs, guiding experimental design, and augmenting random-coefficient and partially linear specifications. By treating generative outputs as structured, explicitly subjective priors rather than as empirical observations, the framework offers a principled way to harness foundation models in empirical work while avoiding the conflation of synthetic ''facts'' with real data.

</details>


### [76] [A Benchmark of Causal vs Correlation AI for Predictive Maintenance](https://arxiv.org/abs/2512.01149)
*Krishna Taduri,Shaunak Dhande,Giacinto Paolo,Saggese,Paul Smith*

Main category: cs.AI

TL;DR: 因果AI方法在预测性维护中显著优于传统相关方法，实现年节省116万美元成本，减少70.2%损失


<details>
  <summary>Details</summary>
Motivation: 制造业预测性维护面临极端成本不对称问题（漏检成本是误报的50倍），传统机器学习方法优化统计准确性指标而非实际运营成本，且无法区分因果关系与虚假相关

Method: 评估8种预测模型，包括基线统计方法和正式因果推断方法，在10,000台CNC机床数据集（故障率3.3%）上进行对比分析

Result: 正式因果推断模型（L5）实现年节省116万美元（减少70.2%损失），比最佳相关决策树模型（L3）每年多节省8万美元；保持最高召回率87.9%的同时减少97%误报（从165降至5），精度达92.1%，训练测试性能差距仅2.6个百分点

Conclusion: 因果AI方法结合领域知识在预测性维护应用中能产生更优财务结果和更可解释的预测，相比基于相关的方法具有显著优势

Abstract: Predictive maintenance in manufacturing environments presents a challenging optimization problem characterized by extreme cost asymmetry, where missed failures incur costs roughly fifty times higher than false alarms. Conventional machine learning approaches typically optimize statistical accuracy metrics that do not reflect this operational reality and cannot reliably distinguish causal relationships from spurious correlations. This study evaluates eight predictive models, ranging from baseline statistical approaches to formal causal inference methods, on a dataset of 10,000 CNC machines with a 3.3% failure prevalence. The formal causal inference model (L5) achieved estimated annual cost savings of 1.16 million USD (a 70.2 percent reduction), outperforming the best correlation-based decision tree model (L3) by approximately 80,000 USD per year. The causal model matched the highest observed recall (87.9 percent) while reducing false alarms by 97 percent (from 165 to 5) and attained a precision of 92.1 percent, with a train-test performance gap of only 2.6 percentage points. These results indicate that causal AI methods, when combined with domain knowledge, can yield superior financial outcomes and more interpretable predictions compared to correlation-based approaches in predictive maintenance applications.

</details>


### [77] [Knowledge Graph Augmented Large Language Models for Next-Visit Disease Prediction](https://arxiv.org/abs/2512.01210)
*Ruiyu Wang,Tuan Vinh,Ran Xu,Yuyin Zhou,Jiaying Lu,Carl Yang,Francisco Pasquel*

Main category: cs.AI

TL;DR: 提出基于知识图谱引导的思维链框架，为电子病历中的疾病预测生成临床可解释的推理过程，在MIMIC-III和CRADLE数据集上取得良好性能。


<details>
  <summary>Details</summary>
Motivation: 现有电子病历预测模型通常提供粗糙的事后解释，对患者层面的临床决策支持有限。需要生成具有临床基础和时间一致性的可解释推理。

Method: 将ICD-9代码映射到PrimeKG知识图谱，提取疾病相关节点和多跳推理路径作为思维链的支架。仅保留结论与观察结果匹配的解释，用这些监督数据微调LLaMA-3.1-Instruct-8B和Gemma-7B模型。

Result: 在10种PrimeKG映射疾病和有限训练数据（400和1000例）下，KG引导模型优于传统基线，AUROC达0.66-0.70，macro-AUPR达0.40-0.47。在CRADLE队列上零样本迁移，准确率从0.40-0.51提升至0.72-0.77。临床医生评估显示KG引导解释在清晰度、相关性和临床正确性上更受青睐。

Conclusion: KG引导的思维链框架能生成临床可解释的推理，提高疾病预测性能，并在跨队列迁移中保持有效性，为临床决策提供更有价值的解释。

Abstract: Electronic health records (EHRs) support powerful clinical prediction models, but existing methods typically provide coarse, post hoc explanations that offer limited value for patient-level decision making. We introduce a knowledge graph (KG)-guided chain-of-thought (CoT) framework that generates clinically grounded and temporally consistent reasoning for visit-level disease prediction in MIMIC-III. ICD-9 codes are mapped to PrimeKG, from which disease-relevant nodes and multi-hop reasoning paths are extracted and used as scaffolds for CoT generation; only explanations whose conclusions match observed outcomes are retained. Lightweight LLaMA-3.1-Instruct-8B and Gemma-7B models are then fine-tuned on this supervision corpus. Across ten PrimeKG-mapped diseases and limited training cohorts (400 and 1000 cases), KG-guided models outperform strong classical baselines, achieving AUROC values of 0.66 to 0.70 and macro-AUPR values of 0.40 to 0.47. The models also transfer zero-shot to the CRADLE cohort, improving accuracy from approximately 0.40 to 0.51 up to 0.72 to 0.77. A blinded clinician evaluation shows consistent preference for KG-guided CoT explanations in clarity, relevance, and clinical correctness.

</details>


### [78] [Unsupervised decoding of encoded reasoning using language model interpretability](https://arxiv.org/abs/2512.01222)
*Ching Fang,Samuel Marks*

Main category: cs.AI

TL;DR: 研究人员通过让语言模型在ROT-13加密下进行推理，测试了当前可解释性技术能否解码模型的隐藏推理过程，发现logit lens方法能有效翻译编码推理，并开发了完全无监督的解码流程。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型能力增强，人们担心它们可能发展出人类无法监督的编码推理过程。本研究旨在探究当前可解释性技术能否穿透这种编码推理，为监督日益强大的AI系统提供评估框架。

Method: 通过微调推理模型（DeepSeek-R1-Distill-Llama-70B），使其在ROT-13加密下进行链式推理，同时保持可理解的英语输出。然后使用机制可解释性方法（特别是logit lens分析）评估仅通过内部激活解码模型隐藏推理过程的能力。

Result: logit lens能有效翻译编码推理，准确率在中后期层达到峰值。研究人员开发了完全无监督的解码流程，结合logit lens和自动转述，从内部模型表示中重建完整推理记录，取得了显著的准确率。

Conclusion: 当前机制可解释性技术对简单形式的编码推理可能比之前理解的更加鲁棒。这项工作为评估可解释性方法对抗非人类可读格式推理模型提供了初步框架，有助于解决监督日益强大AI系统的更广泛挑战。

Abstract: As large language models become increasingly capable, there is growing concern that they may develop reasoning processes that are encoded or hidden from human oversight. To investigate whether current interpretability techniques can penetrate such encoded reasoning, we construct a controlled testbed by fine-tuning a reasoning model (DeepSeek-R1-Distill-Llama-70B) to perform chain-of-thought reasoning in ROT-13 encryption while maintaining intelligible English outputs. We evaluate mechanistic interpretability methods--in particular, logit lens analysis--on their ability to decode the model's hidden reasoning process using only internal activations. We show that logit lens can effectively translate encoded reasoning, with accuracy peaking in intermediate-to-late layers. Finally, we develop a fully unsupervised decoding pipeline that combines logit lens with automated paraphrasing, achieving substantial accuracy in reconstructing complete reasoning transcripts from internal model representations. These findings suggest that current mechanistic interpretability techniques may be more robust to simple forms of encoded reasoning than previously understood. Our work provides an initial framework for evaluating interpretability methods against models that reason in non-human-readable formats, contributing to the broader challenge of maintaining oversight over increasingly capable AI systems.

</details>


### [79] [RoboDriveVLM: A Novel Benchmark and Baseline towards Robust Vision-Language Models for Autonomous Driving](https://arxiv.org/abs/2512.01300)
*Dacheng Liao,Mengshi Qi,Peng Shu,Zhining Zhang,Yuxin Lin,Liang Liu,Huadong Ma*

Main category: cs.AI

TL;DR: 该论文提出了RoboDriveBench基准测试来评估基于视觉语言模型的端到端自动驾驶系统的鲁棒性，并开发了RoboDriveVLM框架和测试时适应方法来提升系统在真实世界挑战中的表现。


<details>
  <summary>Details</summary>
Motivation: 当前基于视觉语言模型的端到端自动驾驶系统直接利用大语言模型根据场景理解生成驾驶决策，但在真实驾驶场景中存在多种风险。需要评估这些系统是否真正适用于自动驾驶，并解决其鲁棒性问题。

Method: 1. 提出RoboDriveBench基准测试，包含11个模拟场景（6个传感器损坏场景和5个提示损坏场景），每个场景250个独特驾驶场景和5,689帧图像，共64,559个轨迹预测案例。2. 提出RoboDriveVLM框架，通过将更多模态数据（如激光雷达和雷达）映射到统一潜在空间来增强鲁棒性。3. 提出基于跨模态知识蒸馏的测试时适应方法。

Result: 通过大量实验，揭示了当前基于视觉语言模型的端到端自动驾驶系统的局限性，并为真实世界部署提供了更可靠的解决方案。源代码和数据集将发布。

Conclusion: 该研究强调了当前VLM自动驾驶系统的脆弱性，并提出了系统性的评估基准和改进方法，为构建更鲁棒的自动驾驶系统提供了重要参考。

Abstract: Current Vision-Language Model (VLM)-based end-to-end autonomous driving systems often leverage large language models to generate driving decisions directly based on their understanding of the current scene. However, such systems introduce multiple risks in real-world driving scenarios. To evaluate whether VLMs are truly viable for autonomous driving, we introduce RoboDriveBench, the first robustness benchmark focused on end-to-end trajectory prediction tasks. This benchmark systematically evaluates two critical categories of real-world challenges for VLM-based end-to-end autonomous driving systems through 11 simulated scenarios encompassing various corruption types, including 6 scenarios of sensor corruption caused by environmental variations, along with 5 cases of prompt corruption resulting from human intervention and data transmission failures. Each corruption type includes 250 unique driving scenarios and 5,689 frames, resulting in 64,559 total trajectory prediction cases per evaluation. To overcome these real-world challenges, we propose a novel VLM-based autonomous driving framework called RoboDriveVLM, which enhances robustness by mapping more multimodal data-e.g., lidar and radar-into a unified latent space. Furthermore, we introduce a new Test-Time Adaptation (TTA) method based on cross-modal knowledge distillation to improve the robustness of VLM-based autonomous driving systems. Through extensive experiments, our work highlights the limitations of current VLM-based end-to-end autonomous driving systems and provides a more reliable solution for real-world deployment. Source code and datasets will be released.

</details>


### [80] [CuES: A Curiosity-driven and Environment-grounded Synthesis Framework for Agentic RL](https://arxiv.org/abs/2512.01311)
*Shinji Mai,Yunpeng Zhai,Ziqian Chen,Cheng Chen,Anni Zou,Shuchang Tao,Zhaoyang Liu,Bolin Ding*

Main category: cs.AI

TL;DR: CuES是一个好奇心驱动、环境基础的任务生成框架，用于解决智能体强化学习中的任务稀缺问题，能够自主从环境结构和功能中生成多样、可执行且有意义的任务。


<details>
  <summary>Details</summary>
Motivation: 在复杂工具增强环境中部署基于大语言模型的智能体时，强化学习需要结构化训练任务才能有效改进。然而在许多现实场景中，缺乏预定义任务（任务稀缺问题），这成为扩展智能体强化学习的关键瓶颈。现有方法通常假设存在预定义任务集合，但在新环境中工具语义和功能最初未知时，这一假设不成立。

Method: 提出CuES框架：通过内在好奇心驱动探索，将交互模式抽象为可重用的任务模式，并通过轻量级自上而下指导和基于记忆的质量控制进行细化。该框架直接从环境结构和功能中自主生成任务，不依赖人工种子或外部语料库。

Result: 在三个代表性环境（AppWorld、BFCL和WebShop）中，CuES生成的任务分布在多样性和可执行性方面匹配或超越了手动策划的数据集，带来了显著的下游策略改进。

Conclusion: 好奇心驱动、环境基础的任务生成为智能体提供了可扩展的基础，使智能体不仅学习如何行动，还学习学习什么。这为解决任务稀缺问题提供了有效方案。

Abstract: Large language model based agents are increasingly deployed in complex, tool augmented environments. While reinforcement learning provides a principled mechanism for such agents to improve through interaction, its effectiveness critically depends on the availability of structured training tasks. In many realistic settings, however, no such tasks exist a challenge we term task scarcity, which has become a key bottleneck for scaling agentic RL. Existing approaches typically assume predefined task collections, an assumption that fails in novel environments where tool semantics and affordances are initially unknown. To address this limitation, we formalize the problem of Task Generation for Agentic RL, where an agent must learn within a given environment that lacks predefined tasks. We propose CuES, a Curiosity driven and Environment grounded Synthesis framework that autonomously generates diverse, executable, and meaningful tasks directly from the environment structure and affordances, without relying on handcrafted seeds or external corpora. CuES drives exploration through intrinsic curiosity, abstracts interaction patterns into reusable task schemas, and refines them through lightweight top down guidance and memory based quality control. Across three representative environments, AppWorld, BFCL, and WebShop, CuES produces task distributions that match or surpass manually curated datasets in both diversity and executability, yielding substantial downstream policy improvements. These results demonstrate that curiosity driven, environment grounded task generation provides a scalable foundation for agents that not only learn how to act, but also learn what to learn. The code is available at https://github.com/modelscope/AgentEvolver/research/CuES.

</details>


### [81] [Extending NGU to Multi-Agent RL: A Preliminary Study](https://arxiv.org/abs/2512.01321)
*Juan Hernandez,Diego Fernández,Manuel Cifuentes,Denis Parra,Rodrigo Toro Icarte*

Main category: cs.AI

TL;DR: 将NGU算法扩展到多智能体环境，在PettingZoo的simple_tag环境中评估性能，发现共享经验回放能提升NGU在多智能体任务中的表现。


<details>
  <summary>Details</summary>
Motivation: NGU算法在稀疏奖励的强化学习任务中表现出色，但主要应用于单智能体环境。本研究旨在探索NGU在多智能体环境中的适用性和效果，特别是如何通过经验共享和内在探索信号调整来提升多智能体学习性能。

Method: 将NGU算法扩展到多智能体环境，在PettingZoo的simple_tag环境中进行实验。比较了三种设计选择：1) 共享回放缓冲区 vs 独立回放缓冲区；2) 使用不同k阈值共享情节新颖性；3) 使用异构的beta参数值。以多智能体DQN作为基线进行对比。

Result: NGU在多智能体环境中比多智能体DQN基线获得中等更高的回报和更稳定的学习动态。共享回放缓冲区的NGU表现最佳且最稳定，表明性能提升来自NGU内在探索与经验共享的结合。新颖性共享在k=1时表现相当，但k值较大时会降低学习效果。异构beta值相比小的共同值没有改进。

Conclusion: NGU可以有效地应用于多智能体设置，前提是共享经验并仔细调整内在探索信号。共享回放缓冲区是实现这一目标的关键设计选择，而新颖性共享和参数异构化需要谨慎处理。

Abstract: The Never Give Up (NGU) algorithm has proven effective in reinforcement learning tasks with sparse rewards by combining episodic novelty and intrinsic motivation. In this work, we extend NGU to multi-agent environments and evaluate its performance in the simple_tag environment from the PettingZoo suite. Compared to a multi-agent DQN baseline, NGU achieves moderately higher returns and more stable learning dynamics. We investigate three design choices: (1) shared replay buffer versus individual replay buffers, (2) sharing episodic novelty among agents using different k thresholds, and (3) using heterogeneous values of the beta parameter. Our results show that NGU with a shared replay buffer yields the best performance and stability, highlighting that the gains come from combining NGU intrinsic exploration with experience sharing. Novelty sharing performs comparably when k = 1 but degrades learning for larger values. Finally, heterogeneous beta values do not improve over a small common value. These findings suggest that NGU can be effectively applied in multi-agent settings when experiences are shared and intrinsic exploration signals are carefully tuned.

</details>


### [82] [A Flexible Multi-Agent LLM-Human Framework for Fast Human Validated Tool Building](https://arxiv.org/abs/2512.01434)
*Daull Xavier,Patrice Bellot,Emmanuel Bruno,Vincent Martin,Elisabeth Murisasco*

Main category: cs.AI

TL;DR: CollabToolBuilder是一个多智能体LLM框架，通过专家在环指导迭代学习为目标创建工具，最小化任务/领域适应和人工反馈获取时间。


<details>
  <summary>Details</summary>
Motivation: 解决复杂迭代问题（如科学文档生成）中工具创建与人类意图对齐的挑战，减少领域适应和人工反馈获取的时间成本。

Method: 使用四个专门化智能体（教练、编码员、批评者、资本化者）的框架，通过强化动态提示和系统化人工反馈集成来生成和验证工具，支持多智能体上下文学习和可重用工具资本化。

Result: 初步实验展示了系统在生成前沿研究论文或专利等任务中的应用，证明了框架在复杂迭代问题解决中的适用性。

Conclusion: CollabToolBuilder提供了一个系统级集成和方法论，结合多智能体上下文学习、专家在环控制和可重用工具资本化，适用于科学文档生成等复杂迭代问题。

Abstract: We introduce CollabToolBuilder, a flexible multiagent LLM framework with expert-in-the-loop (HITL) guidance that iteratively learns to create tools for a target goal, aligning with human intent and process, while minimizing time for task/domain adaptation effort and human feedback capture. The architecture generates and validates tools via four specialized agents (Coach, Coder, Critic, Capitalizer) using a reinforced dynamic prompt and systematic human feedback integration to reinforce each agent's role toward goals and constraints. This work is best viewed as a system-level integration and methodology combining multi-agent in-context learning, HITL controls, and reusable tool capitalization for complex iterative problems such as scientific document generation. We illustrate it with preliminary experiments (e.g., generating state-of-the-art research papers or patents given an abstract) and discuss its applicability to other iterative problem-solving.

</details>


### [83] [A Selective Temporal Hamming distance to find patterns in state transition event timeseries, at scale](https://arxiv.org/abs/2512.01440)
*Sylvain Marié,Pablo Knecht*

Main category: cs.AI

TL;DR: 提出一种新的状态转移事件时间序列（STE-ts）和选择性时序汉明距离（STH），避免了对大型数据库进行代价高昂且扭曲的重采样操作。


<details>
  <summary>Details</summary>
Motivation: 离散事件系统在自然观测、社会经济科学和工业系统中普遍存在。标准分析方法通常未能充分利用其事件/状态的双重性质：信号要么被建模为强调事件顺序对齐的转移事件序列，要么被建模为分类或有序状态时间序列，通常需要进行重采样，随着观测周期和事件数量的增加，这种操作既扭曲又代价高昂。

Method: 定义了状态转移事件时间序列（STE-ts），并提出了一种新的选择性时序汉明距离（STH），该距离同时利用转移时间和状态持续时间，避免了对大型数据库进行代价高昂且扭曲的重采样。

Result: STH推广了重采样汉明距离和Jaccard度量，具有更好的精度和计算时间，并且能够关注多个感兴趣的状态。在模拟和真实世界数据集上验证了这些优势。

Conclusion: 提出的方法能够有效处理离散事件系统的双重性质，避免了传统重采样方法的缺点，在精度和计算效率方面都有显著提升。

Abstract: Discrete event systems are present both in observations of nature, socio economical sciences, and industrial systems. Standard analysis approaches do not usually exploit their dual event / state nature: signals are either modeled as transition event sequences, emphasizing event order alignment, or as categorical or ordinal state timeseries, usually resampled a distorting and costly operation as the observation period and number of events grow. In this work we define state transition event timeseries (STE-ts) and propose a new Selective Temporal Hamming distance (STH) leveraging both transition time and duration-in-state, avoiding costly and distorting resampling on large databases. STH generalizes both resampled Hamming and Jaccard metrics with better precision and computation time, and an ability to focus on multiple states of interest. We validate these benefits on simulated and real-world datasets.

</details>


### [84] [Automated Risk-of-Bias Assessment of Randomized Controlled Trials: A First Look at a GEPA-trained Programmatic Prompting Framework](https://arxiv.org/abs/2512.01452)
*Lingbo Li,Anuradha Mathrani,Teo Susnjak*

Main category: cs.AI

TL;DR: 该研究开发了一个基于DSPy和GEPA模块的可编程风险评估管道，用于自动化随机对照试验的偏倚风险评估，通过结构化代码优化替代手动提示设计，提高了评估的一致性和可重复性。


<details>
  <summary>Details</summary>
Motivation: 随机对照试验的偏倚风险评估对证据合成至关重要，但传统方法资源密集且存在评估者间差异。大型语言模型提供了自动化途径，但现有方法依赖难以复制、泛化或评估的手动设计提示。

Method: 引入可编程的偏倚风险评估管道，使用DSPy及其GEPA模块进行结构化代码优化。GEPA通过帕累托引导搜索改进LLM推理，并生成可检查的执行轨迹，使优化过程的每一步都能透明复制。在100个已发表荟萃分析的随机对照试验上评估了该方法，涵盖七个偏倚风险领域。

Result: 在方法报告更清晰的领域（如随机序列生成），GEPA生成的提示表现最佳；在分配隐藏和参与者盲法方面也有类似结果。商业模型整体表现略好。与三个手动设计的Claude 3.5 Sonnet提示相比，GEPA实现了最高整体准确率，在随机序列生成和选择性报告方面性能提升30%-40%，在其他领域与手动提示表现相当。

Conclusion: GEPA能够为偏倚风险评估生成一致且可重复的提示，支持在证据合成中结构化、原则性地使用大型语言模型，提高了自动化评估的透明度和可靠性。

Abstract: Assessing risk of bias (RoB) in randomized controlled trials is essential for trustworthy evidence synthesis, but the process is resource-intensive and prone to variability across reviewers. Large language models (LLMs) offer a route to automation, but existing methods rely on manually engineered prompts that are difficult to reproduce, generalize, or evaluate. This study introduces a programmable RoB assessment pipeline that replaces ad-hoc prompt design with structured, code-based optimization using DSPy and its GEPA module. GEPA refines LLM reasoning through Pareto-guided search and produces inspectable execution traces, enabling transparent replication of every step in the optimization process. We evaluated the method on 100 RCTs from published meta-analyses across seven RoB domains. GEPA-generated prompts were applied to both open-weight models (Mistral Small 3.1 with GPT-oss-20b) and commercial models (GPT-5 Nano and GPT-5 Mini). In domains with clearer methodological reporting, such as Random Sequence Generation, GEPA-generated prompts performed best, with similar results for Allocation Concealment and Blinding of Participants, while the commercial model performed slightly better overall. We also compared GEPA with three manually designed prompts using Claude 3.5 Sonnet. GEPA achieved the highest overall accuracy and improved performance by 30%-40% in Random Sequence Generation and Selective Reporting, and showed generally comparable, competitively aligned performance in the other domains relative to manual prompts. These findings suggest that GEPA can produce consistent and reproducible prompts for RoB assessment, supporting the structured and principled use of LLMs in evidence synthesis.

</details>


### [85] [SynthStrategy: Extracting and Formalizing Latent Strategic Insights from LLMs in Organic Chemistry](https://arxiv.org/abs/2512.01507)
*Daniel Armstrong,Zlatko Jončev,Andres M Bran,Philippe Schwaller*

Main category: cs.AI

TL;DR: 利用大型语言模型将合成知识转化为代码，实现计算机辅助合成规划中的战略考量，如收敛组装、保护基最小化和最优环形成序列。


<details>
  <summary>Details</summary>
Motivation: 现有计算机辅助合成规划系统能生成化学上有效的反应步骤，但难以融入战略考量，如收敛组装、保护基最小化和最优环形成序列等。需要将合成战略知识系统化地编码到规划系统中。

Method: 利用大型语言模型提取合成知识并转化为Python代码，将战略原则（如战略官能团互变和环构建策略）转化为可验证的函数。创建USPTO-ST数据集（带有战略标签的合成路线），实现基于自然语言的路线检索。

Result: 在基准测试中达到75%的Top-3准确率；通过历史趋势时间分析和化学直观路线聚类验证了库的有效性，提供了比传统方法更细粒度的划分。

Conclusion: 该工作弥合了计算机辅助合成规划中战术与战略之间的鸿沟，使路线能够基于战略标准而非仅结构进行规范、搜索和评估，为合成规划系统提供了新的战略能力。

Abstract: Modern computer-assisted synthesis planning (CASP) systems show promises at generating chemically valid reaction steps but struggle to incorporate strategic considerations such as convergent assembly, protecting group minimization, and optimal ring-forming sequences. We introduce a methodology that leverages Large Language Models to distill synthetic knowledge into code. Our system analyzes synthesis routes and translates strategic principles into Python functions representing diverse strategic and tactical rules, such as strategic functional group interconversions and ring construction strategies. By formalizing this knowledge as verifiable code rather than simple heuristics, we create testable, interpretable representations of synthetic strategy. We release the complete codebase and the USPTO-ST dataset -- synthesis routes annotated with strategic tags. This framework unlocks a novel capability for CASP: natural language-based route retrieval, achieving 75\% Top-3 accuracy on our benchmark. We further validate our library through temporal analysis of historical trends and chemically intuitive route clustering that offers more granular partitioning than common previous methods. This work bridges the tactical-strategic divide in CASP, enabling specification, search, and evaluation of routes by strategic criteria rather than structure alone.

</details>


### [86] [LEC: Linear Expectation Constraints for False-Discovery Control in Selective Prediction and Routing Systems](https://arxiv.org/abs/2512.01556)
*Zhiyuan Wang,Aniri,Tianlong Chen,Yue Zhang,Heng Tao Shen,Xiaoshuang Shi,Kaidi Xu*

Main category: cs.AI

TL;DR: LEC方法通过线性期望约束和FDR控制，为LLM预测提供统计保证，并扩展到双模型路由机制，在保持FDR控制的同时提高样本保留率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型经常生成不可靠的答案，而启发式不确定性方法无法完全区分正确和错误的预测，导致用户在没有统计保证的情况下接受错误答案。

Method: 提出LEC方法，将选择性预测重新解释为约束决策问题，通过对选择和错误指标施加线性期望约束，建立有限样本充分条件，计算FDR约束下的覆盖最大化阈值。扩展到双模型路由机制：当当前模型的不确定性超过校准阈值时，将其委托给更强的模型，同时保持统一的FDR保证。

Result: 在封闭式和开放式问答数据集上的评估显示，LEC实现了更严格的FDR控制，并显著提高了样本保留率。双模型路由机制在保持较低风险水平的同时，比单个模型接受了更多正确样本。

Conclusion: LEC为LLM预测提供了统计保证的FDR控制框架，通过线性期望约束和双模型路由机制，在控制错误率的同时最大化正确预测的接受，解决了LLM可靠性问题。

Abstract: Large language models (LLMs) often generate unreliable answers, while heuristic uncertainty methods fail to fully distinguish correct from incorrect predictions, causing users to accept erroneous answers without statistical guarantees. We address this issue through the lens of false discovery rate (FDR) control, ensuring that among all accepted predictions, the proportion of errors does not exceed a target risk level. To achieve this in a principled way, we propose LEC, which reinterprets selective prediction as a constrained decision problem by enforcing a Linear Expectation Constraint over selection and error indicators. Then, we establish a finite-sample sufficient condition, which relies only on a held-out set of exchangeable calibration samples, to compute an FDR-constrained, coverage-maximizing threshold. Furthermore, we extend LEC to a two-model routing mechanism: given a prompt, if the current model's uncertainty exceeds its calibrated threshold, we delegate it to a stronger model, while maintaining a unified FDR guarantee. Evaluations on closed-ended and open-ended question-answering (QA) datasets show that LEC achieves tighter FDR control and substantially improves sample retention over prior methods. Moreover, the two-model routing mechanism achieves lower risk levels while accepting more correct samples than each individual model.

</details>


### [87] [CLIP-RL: Aligning Language and Policy Representations for Task Transfer in Reinforcement Learning](https://arxiv.org/abs/2512.01616)
*Chainesh Gautam,Raghuram Bharadwaj Diddigi*

Main category: cs.AI

TL;DR: 提出一种基于CLIP原理的强化学习方法，通过将语言指令与对应策略对齐到统一表示空间，实现跨任务的高效迁移学习。


<details>
  <summary>Details</summary>
Motivation: 当前需要开发能够在同一环境中解决多个任务的智能体，特别是当这些任务与语言自然关联时。传统方法在跨任务迁移方面效率不高，需要一种能够有效利用语言-策略对应关系的方法。

Method: 受计算机视觉中CLIP方法的启发，将语言指令和对应策略视为同一任务在不同模态下的表示。通过预训练的语言-策略对组合，建立统一的表示空间，使相同任务的语言描述和策略嵌入具有相似的表示。

Result: 实验结果表明，该方法在跨任务迁移学习中能够实现更快的收敛和更好的性能，验证了统一表示空间的有效性。

Conclusion: 通过将CLIP原理扩展到强化学习领域，成功构建了语言和策略的统一表示空间，为多任务学习提供了高效的迁移管道，在语言关联任务中表现出优越的跨任务迁移能力。

Abstract: Recently, there has been an increasing need to develop agents capable of solving multiple tasks within the same environment, especially when these tasks are naturally associated with language. In this work, we propose a novel approach that leverages combinations of pre-trained (language, policy) pairs to establish an efficient transfer pipeline. Our algorithm is inspired by the principles of Contrastive Language-Image Pretraining (CLIP) in Computer Vision, which aligns representations across different modalities under the philosophy that ''two modalities representing the same concept should have similar representations.'' The central idea here is that the instruction and corresponding policy of a task represent the same concept, the task itself, in two different modalities. Therefore, by extending the idea of CLIP to RL, our method creates a unified representation space for natural language and policy embeddings. Experimental results demonstrate the utility of our algorithm in achieving faster transfer across tasks.

</details>


### [88] [Who Judges the Judge? LLM Jury-on-Demand: Building Trustworthy LLM Evaluation Systems](https://arxiv.org/abs/2512.01786)
*Xiaochuan Li,Ke Wang,Girija Gouda,Shubham Choudhary,Yaqun Wang,Linwei Hu,Joel Vaughan,Freddy Lecue*

Main category: cs.AI

TL;DR: LLM Jury-on-Demand：一个动态、基于学习的框架，通过训练可靠性预测器来评估LLM评委何时与人类专家一致，为每个数据点动态选择最可靠的评委组成陪审团，实现可扩展且可靠的评估。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）被集成到高风险领域，需要既能在实时部署中扩展又能在关键决策中可靠的评估方法。人类评估可靠但缓慢昂贵，单个LLM评委存在偏见，静态陪审团缺乏适应性。

Method: 提出LLM Jury-on-Demand框架，训练一组可靠性预测器来评估LLM评委何时会与人类专家达成一致。利用token分布、嵌入和结构化输入特征，为每个数据点动态选择最可靠的评委组成最优陪审团，并使用可靠性作为权重聚合他们的评分。

Result: 在摘要和RAG基准测试上的实验表明，动态陪审团系统比单评委和静态陪审团基线实现了显著更高的人类判断相关性。

Conclusion: 该研究展示了自适应、基于学习的陪审团在构建可扩展、更可靠和可信赖的现代LLM评估系统方面的潜力，特别适用于高风险领域。

Abstract: As Large Language Models (LLMs) become integrated into high-stakes domains, there is a growing need for evaluation methods that are both scalable for real-time deployment and reliable for critical decision-making. While human evaluation is reliable, it is slow and costly. Single LLM judges are biased, and static juries lack adaptability. To overcome these limitations, we propose LLM Jury-on-Demand - a dynamic, learning-based framework for scalable and context-aware evaluation. Our method trains a set of reliability predictors to assess when LLM judges will agree with human experts, leveraging token distributions, embeddings, and structural input features. This enables a fully adaptive evaluation where, for each data point, an optimal jury of the most reliable judges is dynamically selected, and their scores are aggregated using their reliability as weights. Experiments on summarization and RAG benchmarks show that our dynamic jury system achieves significantly higher correlation with human judgment than both single-judge and static-jury baselines. These results highlight the promise of adaptive, learning-based juries for building scalable, more reliable and trustworthy evaluation systems for modern LLMs in high-stakes domains.

</details>


### [89] [H-Neurons: On the Existence, Impact, and Origin of Hallucination-Associated Neurons in LLMs](https://arxiv.org/abs/2512.01797)
*Cheng Gao,Huimin Chen,Chaojun Xiao,Zhiyi Chen,Zhiyuan Liu,Maosong Sun*

Main category: cs.AI

TL;DR: 研究发现大语言模型中的幻觉现象与极少数特定神经元（H-Neurons）相关，这些神经元仅占模型总神经元的不到0.1%，但能可靠预测幻觉发生，并在预训练阶段就已形成。


<details>
  <summary>Details</summary>
Motivation: 大语言模型经常产生幻觉（看似合理但事实错误的输出），这削弱了其可靠性。虽然先前研究从宏观角度（如训练数据和目标）探讨了幻觉问题，但神经元层面的机制仍未被充分探索。

Method: 从三个角度系统研究幻觉相关神经元（H-Neurons）：1）识别：发现极稀疏的神经元子集能可靠预测幻觉发生；2）行为影响：通过受控干预证明这些神经元与过度顺从行为存在因果关联；3）起源：追溯这些神经元到预训练基础模型，发现它们在预训练阶段就已出现。

Result: 识别出仅占模型总神经元不到0.1%的稀疏神经元子集能可靠预测幻觉发生，且在不同场景下具有强泛化能力。这些神经元与过度顺从行为存在因果关联，且源于预训练阶段的基础模型。

Conclusion: 该研究将宏观行为模式与微观神经机制联系起来，为开发更可靠的大语言模型提供了新见解。幻觉相关神经元在预训练阶段就已形成，且仅需极少数神经元就能可靠预测幻觉发生。

Abstract: Large language models (LLMs) frequently generate hallucinations -- plausible but factually incorrect outputs -- undermining their reliability. While prior work has examined hallucinations from macroscopic perspectives such as training data and objectives, the underlying neuron-level mechanisms remain largely unexplored. In this paper, we conduct a systematic investigation into hallucination-associated neurons (H-Neurons) in LLMs from three perspectives: identification, behavioral impact, and origins. Regarding their identification, we demonstrate that a remarkably sparse subset of neurons (less than $0.1\%$ of total neurons) can reliably predict hallucination occurrences, with strong generalization across diverse scenarios. In terms of behavioral impact, controlled interventions reveal that these neurons are causally linked to over-compliance behaviors. Concerning their origins, we trace these neurons back to the pre-trained base models and find that these neurons remain predictive for hallucination detection, indicating they emerge during pre-training. Our findings bridge macroscopic behavioral patterns with microscopic neural mechanisms, offering insights for developing more reliable LLMs.

</details>


### [90] [Predicting Human Chess Moves: An AI Assisted Analysis of Chess Games Using Skill-group Specific n-gram Language Models](https://arxiv.org/abs/2512.01880)
*Daren Zhong,Dingcheng Huang,Clayton Greenberg*

Main category: cs.AI

TL;DR: 该论文提出了一种基于n-gram语言模型的国际象棋走法预测框架，将走法预测视为行为分析任务，能够根据玩家技能水平动态选择模型进行预测。


<details>
  <summary>Details</summary>
Motivation: 传统国际象棋引擎主要关注计算最优走法，忽略了不同技能水平玩家走法的变异性。为了克服这一局限，需要开发能够分析玩家行为模式并考虑技能水平差异的预测框架。

Method: 将玩家分为7个技能等级（从新手到专家），使用Lichess平台的开源数据训练独立的n-gram语言模型来捕捉各技能水平的走法模式。框架包含模型选择器模块，能动态选择最适合的模型进行预测，基于前序走法序列生成玩家走法。

Result: 模型选择器模块在利用早期游戏信息（16个半走法）时，技能水平分类准确率最高达31.7%。走法预测框架的Selector Assisted Accuracy比基准准确率提高了39.1%。框架具有计算效率，适合实时分析。

Conclusion: 该框架成功将国际象棋走法预测重新定义为行为分析任务，通过考虑玩家技能水平的变异性，显著提高了预测准确性，同时保持了计算效率，为实时国际象棋分析提供了实用工具。

Abstract: Chess, a deterministic game with perfect information, has long served as a benchmark for studying strategic decision-making and artificial intelligence. Traditional chess engines or tools for analysis primarily focus on calculating optimal moves, often neglecting the variability inherent in human chess playing, particularly across different skill levels.
  To overcome this limitation, we propose a novel and computationally efficient move prediction framework that approaches chess move prediction as a behavioral analysis task. The framework employs n-gram language models to capture move patterns characteristic of specific player skill levels. By dividing players into seven distinct skill groups, from novice to expert, we trained separate models using data from the open-source chess platform Lichess. The framework dynamically selects the most suitable model for prediction tasks and generates player moves based on preceding sequences.
  Evaluation on real-world game data demonstrates that the model selector module within the framework can classify skill levels with an accuracy of up to 31.7\% when utilizing early game information (16 half-moves). The move prediction framework also shows substantial accuracy improvements, with our Selector Assisted Accuracy being up to 39.1\% more accurate than our benchmark accuracy. The computational efficiency of the framework further enhances its suitability for real-time chess analysis.

</details>


### [91] [Learned-Rule-Augmented Large Language Model Evaluators](https://arxiv.org/abs/2512.01958)
*Jie Meng,Jin Mao*

Main category: cs.AI

TL;DR: 提出了一种基于规则增强的LLM评估范式，通过自动从数据中提取评分规则并让LLM有效应用这些规则，解决了现有LLM评估方法依赖人工设计规则且与数据和LLM理解不匹配的问题。


<details>
  <summary>Details</summary>
Motivation: LLM主要被用于自然语言生成任务的评估，但在更广泛的评估场景中应用有限。现有LLM评估方法难以泛化，因为它们依赖成本高昂的人工设计评估原则，这些原则往往与标注数据和LLM的理解不匹配。

Method: 提出规则增强的评估范式：1) 规则蒸馏方法：使用LLM辅助的蒙特卡洛树搜索自动从数据中提取评分规则，解决可扩展性问题并提高与数据的对齐度；2) 规则应用策略：包括Chain-of-Rule指导LLM遵循蒸馏规则，以及通过强化学习训练规则增强的LLM评估器，弥合规则与LLM推理之间的差距。

Result: 在多样化任务上的大量实验证明了该方法在各种评估场景中的有效性和泛化能力。

Conclusion: 提出的规则增强评估范式能够自动从数据中学习评估规则，并通过有效的规则应用策略让LLM更好地执行评估任务，为LLM作为通用评估器提供了可行的解决方案。

Abstract: Large language models (LLMs) are predominantly used as evaluators for natural language generation (NLG) tasks, but their application to broader evaluation scenarios remains limited. In this work, we explore the potential of LLMs as general evaluators across diverse tasks. Although LLM-based evaluators have made progress in different areas, existing methods struggle to generalize due to their reliance on costly, human-designed evaluation principles, which are often misaligned with both annotated data and LLMs' understanding.To address these challenges, we propose a rule-augmented evaluation paradigm. First, we introduce a rule distillation method that automatically extracts scoring rules from data using an LLM-assisted Monte Carlo Tree Search (MCTS), alleviating scalability issues and improving alignment with data. Second, to enable LLMs to effectively apply the learned rules, we propose two strategies: (1) Chain-of-Rule (CoR), which guides LLM to follow distilled rules, and (2) training a rule-augmented LLM evaluator (RuAE) via reinforcement learning, further bridging the gap between rules and LLMs' reasoning. Extensive experiments on diverse tasks demonstrate the effectiveness and generalizability of our approach across various evaluation scenarios.

</details>


### [92] [From Atomic to Composite: Reinforcement Learning Enables Generalization in Complementary Reasoning](https://arxiv.org/abs/2512.01970)
*Sitao Cheng,Xunjian Yin,Ruiwen Zhou,Yuxuan Li,Xinyi Wang,Liangming Pan,William Yang Wang,Victor Zhong*

Main category: cs.AI

TL;DR: 本文研究强化学习如何提升推理能力，发现RL不是简单放大现有行为，而是能够合成新的复杂推理策略，但前提是基础模型需先通过监督学习掌握原子技能。


<details>
  <summary>Details</summary>
Motivation: 研究强化学习对推理能力的贡献机制——是激励新技能的合成还是仅仅放大现有行为，这是一个激烈争论的话题。通过互补推理任务来探究这一问题。

Method: 使用人类传记的合成数据集，将互补推理能力严格解耦为两个原子技能：参数推理（依赖内部知识）和上下文推理（依赖外部信息）。在三个难度级别评估泛化能力：I.I.D.、组合和零样本设置。

Result: 发现SFT在分布内表现良好，但在分布外泛化（特别是零样本设置）上表现不佳，存在"SFT泛化悖论"：模型仅靠记忆路径捷径。RL能够作为推理合成器而非概率放大器，但前提是基础模型需先通过SFT掌握独立的原子技能。

Conclusion: RL不仅仅是放大器，在具备足够原子基础的情况下，能够主动从学习到的原语中合成复杂推理策略，无需对复杂策略进行显式监督。解耦原子训练后接RL为复杂推理任务提供了可扩展的泛化路径。

Abstract: The mechanism by which RL contributes to reasoning capabilities-whether it incentivizes the synthesis of new skills or merely amplifies existing behaviors-remains a subject of intense debate. In this work, we investigate this question through the lens of Complementary Reasoning, a complex task that requires integrating internal parametric knowledge with external contextual information. Using a controlled synthetic dataset of human biographies, we strictly decouple this ability into two atomic skills: Parametric Reasoning (relying on internal knowledge) and Contextual Reasoning (depending on external information). To rigorously assess capability boundaries, we evaluate generalization across three distinct levels of difficulty: I.I.D., Composition, and Zero-shot settings. We find that while SFT is sufficient for in-distribution performance, it struggles with O.O.D. generalization, particularly in Zero-shot settings where relational combinations are novel. Crucially, we identify the SFT Generalization Paradox: Models supervised solely on the composite task achieve near-perfect in-distribution accuracy but collapse on out-of-distribution generalization, indicating their reliance on rote memorization of path shortcuts. In contrast, we find that RL acts as a reasoning synthesizer rather than a probability amplifier. However, we uncover a strict atomic prerequisite: RL can only synthesize these complex strategies if the base model has first mastered the independent atomic skills (Parametric and Contextual) via SFT. These findings challenge the view of RL as a mere amplifier, suggesting that given sufficient atomic foundations, RL can actively synthesize complex reasoning strategies from learned primitives without explicit supervision on such complex strategies. This indicates that decoupled atomic training followed by RL offers a scalable path to generalization for complex reasoning tasks.

</details>


### [93] [Chain-of-Ground: Improving GUI Grounding via Iterative Reasoning and Reference Feedback](https://arxiv.org/abs/2512.01979)
*Aiden Yiliu Li,Bizhi Yu,Daoan Lei,Tianhe Ren,Shilong Liu*

Main category: cs.AI

TL;DR: 提出Chain of Ground (CoG)框架，通过多步迭代推理和优化提升GUI视觉定位精度，无需额外训练，在ScreenSpot Pro基准上提升4.8个百分点，在工业控制面板数据集上提升6.9个百分点。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在GUI视觉定位中面临小目标、视觉相似目标识别困难，以及真实世界布局中的歧义问题，这些限制源于有限的定位能力和未充分利用现有推理潜力。

Method: 提出Chain of Ground (CoG)框架，采用无需训练的多步定位方法，通过多模态大语言模型进行迭代视觉推理和优化，逐步反思和调整假设，而不是直接预测。

Result: 在ScreenSpot Pro基准上达到68.4%准确率，提升4.8个百分点；在TPanel UI工业控制面板数据集上，相比Qwen3 VL 235B基线提升6.9个百分点，展示了对真实世界和数字界面的泛化能力。

Conclusion: 通过结构化迭代优化而非额外训练，能够释放多模态大语言模型的定位潜力，提供更准确和可解释的定位结果，为GUI视觉定位提供了新方向。

Abstract: GUI grounding aims to align natural language instructions with precise regions in complex user interfaces. Advanced multimodal large language models show strong ability in visual GUI grounding but still struggle with small or visually similar targets and ambiguity in real world layouts. These limitations arise from limited grounding capacity and from underuse of existing reasoning potential. We present Chain of Ground CoG a training free multi step grounding framework that uses multimodal large language models for iterative visual reasoning and refinement. Instead of direct prediction the model progressively reflects and adjusts its hypotheses leading to more accurate and interpretable localization. Our approach achieves 68.4 accuracy on the ScreenSpot Pro benchmark an improvement of 4.8 points. To measure real world generalization we introduce TPanel UI a dataset of 420 labeled industrial control panels with visual distortions such as blur and masking. On TPanel UI Chain of Ground improves over the strong baseline Qwen3 VL 235B by 6.9 points showing the effectiveness of multi step training free grounding across real world and digital interfaces. These results highlight a direction for unlocking grounding potential through structured iterative refinement instead of additional training.

</details>


### [94] [LLM CHESS: Benchmarking Reasoning and Instruction-Following in LLMs through Chess](https://arxiv.org/abs/2512.01992)
*Sai Kolasani,Maxim Saplin,Nicholas Crispino,Kyle Montgomery,Jared Quincy Davis,Matei Zaharia,Chi Wang,Chenguang Wang*

Main category: cs.AI

TL;DR: LLM CHESS是一个通过国际象棋游戏评估大语言模型推理和指令遵循能力的框架，测试了50多个模型，发现即使是顶级模型也难以稳定完成游戏，该框架能有效减少过拟合和记忆问题。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型评估基准存在过拟合、记忆化和基准饱和问题，需要一种能够真实测试模型推理和指令遵循泛化能力的动态评估框架。

Method: 通过国际象棋游戏环境，让LLMs与随机对手对战，使用胜率、移动质量、合法性、幻觉动作等行为指标进行评估；对顶级推理模型还使用可变技能配置的象棋引擎进行Elo评分。

Result: 测试了50多个开源和闭源模型，发现许多最先进的模型难以完成游戏或取得稳定胜利；推理模型和非推理模型表现有明显差异；该框架能有效减少过拟合和记忆问题。

Conclusion: LLM CHESS提供了一个动态、随机的评估框架，能有效测试LLMs的推理和指令遵循能力，减少了现有静态基准的问题，并发布了实验框架、公开排行榜和相关数据集。

Abstract: We introduce LLM CHESS, an evaluation framework designed to probe the generalization of reasoning and instruction-following abilities in large language models (LLMs) through extended agentic interaction in the domain of chess. We rank over 50 open and closed source models by playing against a random opponent using a range of behavioral metrics, including win and loss rates, move quality, move legality, hallucinated actions, and game duration. For a subset of top reasoning models, we derive an Elo estimate by playing against a chess engine with variably configured skill, which allows for comparisons between models in an easily understandable way. Despite the simplicity of the instruction-following task and the weakness of the opponent, many state-of-the-art models struggle to complete games or achieve consistent wins. Similar to other benchmarks on complex reasoning tasks, our experiments reveal a clear separation between reasoning and non-reasoning models. However, unlike existing static benchmarks, the stochastic and dynamic nature of LLM CHESS uniquely reduces overfitting and memorization while preventing benchmark saturation, proving difficult even for top reasoning models. To support future work on evaluating reasoning and instruction-following in LLMs, we release our experimental framework, a public leaderboard, and a dataset of associated games.

</details>
