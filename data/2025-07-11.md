<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 6]
- [cs.CR](#cs.CR) [Total: 18]
- [cs.AI](#cs.AI) [Total: 26]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [A German Gold-Standard Dataset for Sentiment Analysis in Software Engineering](https://arxiv.org/abs/2507.07325)
*Martin Obaidi,Marc Herrmann,Elisa Schmid,Raymond Ochsner,Kurt Schneider,Jil Klünder*

Main category: cs.SE

TL;DR: 论文介绍了一个德语开发者情感分析数据集，填补了现有工具在德语领域的空白，并验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有情感分析工具主要依赖英语或非德语数据集，缺乏针对德语开发者社区的资源。

Method: 从德国开发者论坛提取5,949条语句，由四位德语母语学生基于Shaver情感模型标注六种基本情感。

Result: 标注过程显示出高评分者一致性和可靠性，验证了数据集的有效性。现有德语工具在软件工程领域表现不足。

Conclusion: 该数据集为德语软件工程社区提供了可靠的情感分析资源，并提出了优化标注和进一步应用的思路。

Abstract: Sentiment analysis is an essential technique for investigating the emotional
climate within developer teams, contributing to both team productivity and
project success. Existing sentiment analysis tools in software engineering
primarily rely on English or non-German gold-standard datasets. To address this
gap, our work introduces a German dataset of 5,949 unique developer statements,
extracted from the German developer forum Android-Hilfe.de. Each statement was
annotated with one of six basic emotions, based on the emotion model by Shaver
et al., by four German-speaking computer science students. Evaluation of the
annotation process showed high interrater agreement and reliability. These
results indicate that the dataset is sufficiently valid and robust to support
sentiment analysis in the German-speaking software engineering community.
Evaluation with existing German sentiment analysis tools confirms the lack of
domain-specific solutions for software engineering. We also discuss approaches
to optimize annotation and present further use cases for the dataset.

</details>


### [2] [Automatic Generation of Explainability Requirements and Software Explanations From User Reviews](https://arxiv.org/abs/2507.07344)
*Martin Obaidi,Jannik Fischbach,Jakob Droste,Hannah Deters,Marc Herrmann,Jil Klünder,Steffen Krätzig,Hugo Villamizar,Kurt Schneider*

Main category: cs.SE

TL;DR: 论文提出了一种自动化工具，用于从用户反馈中提取可解释性需求并生成对应解释，评估显示AI生成的需求准确性不足，但解释更受青睐。


<details>
  <summary>Details</summary>
Motivation: 增强透明度、建立用户信任和确保合规性是可解释性的关键需求，但现有方法难以将用户反馈转化为结构化需求和解释。

Method: 引入一种工具支持的方法，自动化从用户评论中提取需求并生成解释，并通过工业合作数据集进行评估。

Result: AI生成的需求常缺乏相关性和正确性，但生成的解释在清晰度和风格上更受青睐，仍需人工验证。

Conclusion: 该工作推动了可解释性需求的研究，提供了自动化方法、实证见解和公开数据集。

Abstract: Explainability has become a crucial non-functional requirement to enhance
transparency, build user trust, and ensure regulatory compliance. However,
translating explanation needs expressed in user feedback into structured
requirements and corresponding explanations remains challenging. While existing
methods can identify explanation-related concerns in user reviews, there is no
established approach for systematically deriving requirements and generating
aligned explanations. To contribute toward addressing this gap, we introduce a
tool-supported approach that automates this process. To evaluate its
effectiveness, we collaborated with an industrial automation manufacturer to
create a dataset of 58 user reviews, each annotated with manually crafted
explainability requirements and explanations. Our evaluation shows that while
AI-generated requirements often lack relevance and correctness compared to
human-created ones, the AI-generated explanations are frequently preferred for
their clarity and style. Nonetheless, correctness remains an issue,
highlighting the importance of human validation. This work contributes to the
advancement of explainability requirements in software systems by (1)
introducing an automated approach to derive requirements from user reviews and
generate corresponding explanations, (2) providing empirical insights into the
strengths and limitations of automatically generated artifacts, and (3)
releasing a curated dataset to support future research on the automatic
generation of explainability requirements.

</details>


### [3] [Towards an Engineering Workflow Management System for Asset Administration Shells using BPMN](https://arxiv.org/abs/2507.07468)
*Sten Grüner,Nafise Eskandani*

Main category: cs.SE

TL;DR: 论文探讨了在工程工作流中使用资产管理壳（AAS）与BPMN结合的方法，提出了一种分布式AAS基础设施和自动化工作流管理原型。


<details>
  <summary>Details</summary>
Motivation: 工业4.0技术的集成对优化工程流程至关重要，AAS是实现互操作数字孪生的关键。

Method: 结合AAS与BPMN，提出分布式AAS基础设施和工作流管理原型。

Result: 提高了工程工作流的效率与可追溯性，增强了安全性和可扩展性。

Conclusion: AAS与BPMN的结合为工程工作流自动化和跨组织协作提供了有效解决方案。

Abstract: The integration of Industry 4.0 technologies into engineering workflows is an
essential step toward automating and optimizing plant and process engineering
processes. The Asset Administration Shell (AAS) serves as a key enabler for
creating interoperable Digital Twins that facilitate engineering data exchange
and automation. This paper explores the use of AAS within engineering
workflows, particularly in combination with Business Process Model and Notation
(BPMN) to define structured and automated processes. We propose a distributed
AAS copy-on-write infrastructure that enhances security and scalability while
enabling seamless cross organizational collaboration. We also introduce a
workflow management prototype automating AAS operations and engineering
workflows, improving efficiency and traceability.

</details>


### [4] [From Requirements to Code: Understanding Developer Practices in LLM-Assisted Software Engineering](https://arxiv.org/abs/2507.07548)
*Jonathan Ullrich,Matthias Koch,Andreas Vogelsang*

Main category: cs.SE

TL;DR: 研究发现，尽管生成式LLMs具备高级代码生成能力，但传统需求工程（RE）在代码生成中仍不可或缺。需求文档通常过于抽象，需手动分解为编程任务并补充设计决策后才能用于LLM提示。


<details>
  <summary>Details</summary>
Motivation: 评估生成式LLMs是否能够完全替代传统软件工程，尤其是开发者如何将需求信息用于LLM代码生成。

Method: 采访了来自14家公司的18名从业者，分析他们如何利用需求和设计工件为LLM生成代码提供输入。

Result: 需求文档过于抽象，需分解为编程任务并补充设计信息后才能用于LLM提示。

Conclusion: 使用LLM生成代码时，传统需求工程仍必不可少，研究为自动化需求中心任务提供了理论支持。

Abstract: With the advent of generative LLMs and their advanced code generation
capabilities, some people already envision the end of traditional software
engineering, as LLMs may be able to produce high-quality code based solely on
the requirements a domain expert feeds into the system. The feasibility of this
vision can be assessed by understanding how developers currently incorporate
requirements when using LLMs for code generation-a topic that remains largely
unexplored. We interviewed 18 practitioners from 14 companies to understand how
they (re)use information from requirements and other design artifacts to feed
LLMs when generating code. Based on our findings, we propose a theory that
explains the processes developers employ and the artifacts they rely on. Our
theory suggests that requirements, as typically documented, are too abstract
for direct input into LLMs. Instead, they must first be manually decomposed
into programming tasks, which are then enriched with design decisions and
architectural constraints before being used in prompts. Our study highlights
that fundamental RE work is still necessary when LLMs are used to generate
code. Our theory is important for contextualizing scientific approaches to
automating requirements-centric SE tasks.

</details>


### [5] [Prompt Engineering for Requirements Engineering: A Literature Review and Roadmap](https://arxiv.org/abs/2507.07682)
*Kaicheng Huang,Fanyu Wang,Yutan Huang,Chetan Arora*

Main category: cs.SE

TL;DR: 本文综述了提示工程（PE）在需求工程（RE）中的应用，提出了一个混合分类法，并指出了当前研究的局限性和未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLM）在需求工程中有潜力，但其不确定性和缺乏可控性阻碍了其可信赖的应用。本文旨在为PE4RE提供系统化的指导和未来研究方向。

Method: 采用Kitchenham和Petersen的二次研究协议，筛选并分析了35项主要研究，提出了一种混合分类法，将技术导向的模式与任务导向的RE角色联系起来。

Result: 研究发现当前PE4RE研究存在局限性，并提出了一个逐步发展的路线图，以将现有的临时原型转化为可复现的工作流程。

Conclusion: 本文为PE4RE领域提供了系统化的综述和未来研究方向，强调了从原型到实践工作流程的转变。

Abstract: Advancements in large language models (LLMs) have led to a surge of prompt
engineering (PE) techniques that can enhance various requirements engineering
(RE) tasks. However, current LLMs are often characterized by significant
uncertainty and a lack of controllability. This absence of clear guidance on
how to effectively prompt LLMs acts as a barrier to their trustworthy
implementation in the RE field. We present the first roadmap-oriented
systematic literature review of Prompt Engineering for RE (PE4RE). Following
Kitchenham's and Petersen's secondary-study protocol, we searched six digital
libraries, screened 867 records, and analyzed 35 primary studies. To bring
order to a fragmented landscape, we propose a hybrid taxonomy that links
technique-oriented patterns (e.g., few-shot, Chain-of-Thought) to task-oriented
RE roles (elicitation, validation, traceability). Two research questions, with
five sub-questions, map the tasks addressed, LLM families used, and prompt
types adopted, and expose current limitations and research gaps. Finally, we
outline a step-by-step roadmap showing how today's ad-hoc PE prototypes can
evolve into reproducible, practitioner-friendly workflows.

</details>


### [6] [From Domain Documents to Requirements: Retrieval-Augmented Generation in the Space Industry](https://arxiv.org/abs/2507.07689)
*Chetan Arora,Fanyu Wang,Chakkrit Tantithamthavorn,Aldeida Aleti,Shaun Kenyon*

Main category: cs.SE

TL;DR: 本文探讨了利用检索增强生成（RAG）模型支持太空领域需求工程的潜力，提出了一种模块化AI驱动方法，以减少人工工作量并提高需求覆盖范围。


<details>
  <summary>Details</summary>
Motivation: 小型太空组织和新进入者在处理复杂、非结构化的任务文档时面临困难，需要一种自动化或半自动化的方法来生成可操作的需求。

Method: 采用RAG模型预处理原始任务文档，分类语义类别，检索相关内容，并利用大型语言模型合成需求草案。

Result: 初步结果显示该方法能减少人工工作量，提高需求覆盖范围，并支持轻量级合规性对齐。

Conclusion: 该方法有望降低小型组织参与大型安全关键任务的门槛，并提出了AI在需求工程中更广泛应用的路线图。

Abstract: Requirements engineering (RE) in the space industry is inherently complex,
demanding high precision, alignment with rigorous standards, and adaptability
to mission-specific constraints. Smaller space organisations and new entrants
often struggle to derive actionable requirements from extensive, unstructured
documents such as mission briefs, interface specifications, and regulatory
standards. In this innovation opportunity paper, we explore the potential of
Retrieval-Augmented Generation (RAG) models to support and (semi-)automate
requirements generation in the space domain. We present a modular, AI-driven
approach that preprocesses raw space mission documents, classifies them into
semantically meaningful categories, retrieves contextually relevant content
from domain standards, and synthesises draft requirements using large language
models (LLMs). We apply the approach to a real-world mission document from the
space domain to demonstrate feasibility and assess early outcomes in
collaboration with our industry partner, Starbound Space Solutions. Our
preliminary results indicate that the approach can reduce manual effort,
improve coverage of relevant requirements, and support lightweight compliance
alignment. We outline a roadmap toward broader integration of AI in RE
workflows, intending to lower barriers for smaller organisations to participate
in large-scale, safety-critical missions.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [7] [WatchWitch: Interoperability, Privacy, and Autonomy for the Apple Watch](https://arxiv.org/abs/2507.07210)
*Nils Rollshausen,Alexander Heinrich,Matthias Hollick,Jiska Classen*

Main category: cs.CR

TL;DR: 研究者逆向工程了Apple Watch的无线协议，发现安全问题，并开发了Android版WatchWitch，打破苹果生态限制，增强用户隐私控制。


<details>
  <summary>Details</summary>
Motivation: Apple Watch用户无法选择数据处理方式，依赖苹果生态，缺乏隐私和数据自主权。

Method: 逆向工程Apple Watch的无线协议，开发WatchWitch（Android版实现）。

Result: 发现苹果专有实现中的安全问题，实现与Android的互操作性，增强隐私控制。

Conclusion: WatchWitch为智能手表生态系统提供更多用户选择和控制权。

Abstract: Smartwatches such as the Apple Watch collect vast amounts of intimate health
and fitness data as we wear them. Users have little choice regarding how this
data is processed: The Apple Watch can only be used with Apple's iPhones, using
their software and their cloud services. We are the first to publicly
reverse-engineer the watch's wireless protocols, which led to discovering
multiple security issues in Apple's proprietary implementation. With
WatchWitch, our custom Android reimplementation, we break out of Apple's walled
garden -- demonstrating practical interoperability with enhanced privacy
controls and data autonomy. We thus pave the way for more consumer choice in
the smartwatch ecosystem, offering users more control over their devices.

</details>


### [8] [Automated Attack Testflow Extraction from Cyber Threat Report using BERT for Contextual Analysis](https://arxiv.org/abs/2507.07244)
*Faissal Ahmadou,Sepehr Ghaffarzadegan,Boubakr Nour,Makan Pourzandi,Mourad Debbabi,Chadi Assi*

Main category: cs.CR

TL;DR: FLOWGUARDIAN利用BERT和NLP技术自动从非结构化威胁报告中提取攻击测试流程，提升网络安全测试的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 手动提取攻击测试流程耗时且易错，需要自动化解决方案以提高效率和准确性。

Method: 结合BERT和NLP技术，系统分析安全事件并生成测试流程。

Result: 实证验证显示FLOWGUARDIAN在准确性和效率上表现优异，显著提升安全团队能力。

Conclusion: FLOWGUARDIAN为网络安全测试提供了高效、自动化的解决方案。

Abstract: In the ever-evolving landscape of cybersecurity, the rapid identification and
mitigation of Advanced Persistent Threats (APTs) is crucial. Security
practitioners rely on detailed threat reports to understand the tactics,
techniques, and procedures (TTPs) employed by attackers. However, manually
extracting attack testflows from these reports requires elusive knowledge and
is time-consuming and prone to errors. This paper proposes FLOWGUARDIAN, a
novel solution leveraging language models (i.e., BERT) and Natural Language
Processing (NLP) techniques to automate the extraction of attack testflows from
unstructured threat reports. FLOWGUARDIAN systematically analyzes and
contextualizes security events, reconstructs attack sequences, and then
generates comprehensive testflows. This automated approach not only saves time
and reduces human error but also ensures comprehensive coverage and robustness
in cybersecurity testing. Empirical validation using public threat reports
demonstrates FLOWGUARDIAN's accuracy and efficiency, significantly enhancing
the capabilities of security teams in proactive threat hunting and incident
response.

</details>


### [9] [Disa: Accurate Learning-based Static Disassembly with Attentions](https://arxiv.org/abs/2507.07246)
*Peicheng Wang,Monika Santra,Mingyu Liu,Cong Sun,Dongrui Zeng,Gang Tan*

Main category: cs.CR

TL;DR: Disa是一种基于学习的反汇编方法，利用多头自注意力机制学习指令相关性，显著提升了反汇编的准确性，尤其在混淆二进制文件中表现优异。


<details>
  <summary>Details</summary>
Motivation: 反汇编在安全领域至关重要，但传统方法依赖文件格式假设和启发式规则，导致结果不完整或不正确，尤其是在二进制文件被混淆时。

Method: Disa采用多头自注意力机制学习指令相关性，推断函数入口点和指令边界，并识别与内存块边界相关的指令，以生成更准确的控制流图。

Result: 实验表明，Disa在函数入口点识别上优于现有深度学习方法，F1分数提升9.1%和13.2%，内存块精度提升18.5%，间接调用目标减少4.4%。

Conclusion: Disa通过深度学习显著提升了反汇编的准确性和效率，尤其在处理混淆二进制文件时表现突出。

Abstract: For reverse engineering related security domains, such as vulnerability
detection, malware analysis, and binary hardening, disassembly is crucial yet
challenging. The fundamental challenge of disassembly is to identify
instruction and function boundaries. Classic approaches rely on file-format
assumptions and architecture-specific heuristics to guess the boundaries,
resulting in incomplete and incorrect disassembly, especially when the binary
is obfuscated. Recent advancements of disassembly have demonstrated that deep
learning can improve both the accuracy and efficiency of disassembly. In this
paper, we propose Disa, a new learning-based disassembly approach that uses the
information of superset instructions over the multi-head self-attention to
learn the instructions' correlations, thus being able to infer function
entry-points and instruction boundaries. Disa can further identify instructions
relevant to memory block boundaries to facilitate an advanced block-memory
model based value-set analysis for an accurate control flow graph (CFG)
generation. Our experiments show that Disa outperforms prior deep-learning
disassembly approaches in function entry-point identification, especially
achieving 9.1% and 13.2% F1-score improvement on binaries respectively
obfuscated by the disassembly desynchronization technique and popular
source-level obfuscator. By achieving an 18.5% improvement in the memory block
precision, Disa generates more accurate CFGs with a 4.4% reduction in Average
Indirect Call Targets (AICT) compared with the state-of-the-art heuristic-based
approach.

</details>


### [10] [Semi-fragile watermarking of remote sensing images using DWT, vector quantization and automatic tiling](https://arxiv.org/abs/2507.07250)
*Jordi Serra-Ruiz,David Megías*

Main category: cs.CR

TL;DR: 提出了一种基于树结构向量量化的半脆弱水印方案，用于多波段遥感图像，以检测篡改并保留水印。


<details>
  <summary>Details</summary>
Motivation: 为了保护遥感图像的完整性，同时允许一定程度的有损压缩，需要一种能够嵌入水印并检测篡改的方法。

Method: 将图像分割为三维块，为每个块构建树结构向量量化器，并通过迭代算法嵌入水印。

Result: 该方法能在有损压缩（超过阈值）下保留水印，同时检测篡改块及其位置。

Conclusion: 该方案有效平衡了水印保留与篡改检测的需求，适用于多波段图像。

Abstract: A semi-fragile watermarking scheme for multiple band images is presented in
this article. We propose to embed a mark into remote sensing images applying a
tree-structured vector quantization approach to the pixel signatures instead of
processing each band separately. The signature of the multispectral or
hyperspectral image is used to embed the mark in it order to detect any
significant modification of the original image. The image is segmented into
three-dimensional blocks, and a tree-structured vector quantizer is built for
each block. These trees are manipulated using an iterative algorithm until the
resulting block satisfies a required criterion, which establishes the embedded
mark. The method is shown to be able to preserve the mark under lossy
compression (above a given threshold) but, at the same time, it detects
possibly forged blocks and their position in the whole image.

</details>


### [11] [FedP3E: Privacy-Preserving Prototype Exchange for Non-IID IoT Malware Detection in Cross-Silo Federated Learning](https://arxiv.org/abs/2507.07258)
*Rami Darwish,Mahmoud Abdelsalam,Sajad Khorsandroo,Kaushik Roy*

Main category: cs.CR

TL;DR: 论文提出了一种名为FedP3E的新型联邦学习框架，通过隐私保护的原型交换解决物联网（IoT）恶意软件检测中的数据异质性和类别不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 随着IoT生态系统的扩展，恶意软件攻击日益复杂，传统联邦学习算法（如FedAvg和FedProx）在数据异质性和类别不平衡场景下表现不佳。

Method: FedP3E框架通过高斯混合模型（GMMs）构建类别原型，添加高斯噪声保护隐私，并通过SMOTE增强少数类表示。

Result: 在N-BaIoT数据集上的实验表明，FedP3E在数据不平衡的跨场景中表现优异。

Conclusion: FedP3E通过原型共享机制有效提升了联邦学习在IoT恶意软件检测中的性能，同时保护了数据隐私。

Abstract: As IoT ecosystems continue to expand across critical sectors, they have
become prominent targets for increasingly sophisticated and large-scale malware
attacks. The evolving threat landscape, combined with the sensitive nature of
IoT-generated data, demands detection frameworks that are both
privacy-preserving and resilient to data heterogeneity. Federated Learning (FL)
offers a promising solution by enabling decentralized model training without
exposing raw data. However, standard FL algorithms such as FedAvg and FedProx
often fall short in real-world deployments characterized by class imbalance and
non-IID data distributions -- particularly in the presence of rare or disjoint
malware classes. To address these challenges, we propose FedP3E
(Privacy-Preserving Prototype Exchange), a novel FL framework that supports
indirect cross-client representation sharing while maintaining data privacy.
Each client constructs class-wise prototypes using Gaussian Mixture Models
(GMMs), perturbs them with Gaussian noise, and transmits only these compact
summaries to the server. The aggregated prototypes are then distributed back to
clients and integrated into local training, supported by SMOTE-based
augmentation to enhance representation of minority malware classes. Rather than
relying solely on parameter averaging, our prototype-driven mechanism enables
clients to enrich their local models with complementary structural patterns
observed across the federation -- without exchanging raw data or gradients.
This targeted strategy reduces the adverse impact of statistical heterogeneity
with minimal communication overhead. We evaluate FedP3E on the N-BaIoT dataset
under realistic cross-silo scenarios with varying degrees of data imbalance.

</details>


### [12] [Shuffling for Semantic Secrecy](https://arxiv.org/abs/2507.07401)
*Fupei Chen,Liyao Xiang,Haoxiang Sun,Hei Victor Cheng,Kaiming Shen*

Main category: cs.CR

TL;DR: 论文提出了一种基于随机打乱特征的语义安全通信系统，以在窃听信道中平衡传输速率与泄漏速率。


<details>
  <summary>Details</summary>
Motivation: 研究深度学习在语义通信中的安全性，改进传统安全编码方案，以在给定泄漏率约束下最大化传输速率并最小化语义错误概率。

Method: 设计了一种新型语义安全通信系统，利用随机打乱模式作为共享密钥，通过打乱特征序列扭曲语义信息以防止窃听。

Result: 仿真结果表明，该方法在提升安全传输方面显著优于基准方法，尤其在强噪声和不可预测衰落信道中表现突出。

Conclusion: 随机打乱特征是一种有效的语义安全通信方法，可作为现有系统的插件，显著提升安全性。

Abstract: Deep learning draws heavily on the latest progress in semantic
communications. The present paper aims to examine the security aspect of this
cutting-edge technique from a novel shuffling perspective. Our goal is to
improve upon the conventional secure coding scheme to strike a desirable
tradeoff between transmission rate and leakage rate. To be more specific, for a
wiretap channel, we seek to maximize the transmission rate while minimizing the
semantic error probability under the given leakage rate constraint. Toward this
end, we devise a novel semantic security communication system wherein the
random shuffling pattern plays the role of the shared secret key. Intuitively,
the permutation of feature sequences via shuffling would distort the semantic
essence of the target data to a sufficient extent so that eavesdroppers cannot
access it anymore. The proposed random shuffling method also exhibits its
flexibility in working for the existing semantic communication system as a
plugin. Simulations demonstrate the significant advantage of the proposed
method over the benchmark in boosting secure transmission, especially when
channels are prone to strong noise and unpredictable fading.

</details>


### [13] [Phishing Detection in the Gen-AI Era: Quantized LLMs vs Classical Models](https://arxiv.org/abs/2507.07406)
*Jikesh Thapa,Gurrehmat Chahal,Serban Voinea Gabreanu,Yazan Otoum*

Main category: cs.CR

TL;DR: 论文比较了传统机器学习、深度学习和量化小参数大语言模型在钓鱼检测中的表现，发现大语言模型虽精度略低，但在识别上下文钓鱼线索上潜力大，且轻量化模型适合高效部署。


<details>
  <summary>Details</summary>
Motivation: 钓鱼攻击日益复杂，需要高精度且计算高效的检测系统。

Method: 通过实验比较传统ML、DL和量化小参数LLMs的性能，并研究零样本和少样本提示策略的影响。

Result: LLMs在精度上略逊于ML和DL，但能识别上下文线索；轻量化LLMs（如DeepSeek R1）在17GB显存下达到80%以上精度。

Conclusion: 优化后的LLMs有望成为钓鱼防御系统的关键组件，推动高效、可解释AI在网络安全中的应用。

Abstract: Phishing attacks are becoming increasingly sophisticated, underscoring the
need for detection systems that strike a balance between high accuracy and
computational efficiency. This paper presents a comparative evaluation of
traditional Machine Learning (ML), Deep Learning (DL), and quantized
small-parameter Large Language Models (LLMs) for phishing detection. Through
experiments on a curated dataset, we show that while LLMs currently
underperform compared to ML and DL methods in terms of raw accuracy, they
exhibit strong potential for identifying subtle, context-based phishing cues.
We also investigate the impact of zero-shot and few-shot prompting strategies,
revealing that LLM-rephrased emails can significantly degrade the performance
of both ML and LLM-based detectors. Our benchmarking highlights that models
like DeepSeek R1 Distill Qwen 14B (Q8_0) achieve competitive accuracy, above
80%, using only 17GB of VRAM, supporting their viability for cost-efficient
deployment. We further assess the models' adversarial robustness and
cost-performance tradeoffs, and demonstrate how lightweight LLMs can provide
concise, interpretable explanations to support real-time decision-making. These
findings position optimized LLMs as promising components in phishing defence
systems and offer a path forward for integrating explainable, efficient AI into
modern cybersecurity frameworks.

</details>


### [14] [Hybrid LLM-Enhanced Intrusion Detection for Zero-Day Threats in IoT Networks](https://arxiv.org/abs/2507.07413)
*Mohammad F. Al-Hammouri,Yazan Otoum,Rasha Atwa,Amiya Nayak*

Main category: cs.CR

TL;DR: 论文提出了一种结合传统签名检测与GPT-2语言模型的新型入侵检测方法，显著提升了检测精度并减少了误报。


<details>
  <summary>Details</summary>
Motivation: 随着网络威胁日益复杂，尤其是在物联网等分布式、异构和资源受限环境中，传统入侵检测方法难以应对新型攻击模式，需要动态自适应的解决方案。

Method: 提出了一种混合入侵检测框架，结合签名检测的稳健性和GPT-2驱动的语义分析能力。

Result: 实验表明，该模型检测精度提升6.3%，误报减少9.0%，并保持近实时响应。

Conclusion: 语言模型整合为构建智能、可扩展且适应现代连接环境的网络安全防御提供了潜力。

Abstract: This paper presents a novel approach to intrusion detection by integrating
traditional signature-based methods with the contextual understanding
capabilities of the GPT-2 Large Language Model (LLM). As cyber threats become
increasingly sophisticated, particularly in distributed, heterogeneous, and
resource-constrained environments such as those enabled by the Internet of
Things (IoT), the need for dynamic and adaptive Intrusion Detection Systems
(IDSs) becomes increasingly urgent. While traditional methods remain effective
for detecting known threats, they often fail to recognize new and evolving
attack patterns. In contrast, GPT-2 excels at processing unstructured data and
identifying complex semantic relationships, making it well-suited to uncovering
subtle, zero-day attack vectors. We propose a hybrid IDS framework that merges
the robustness of signature-based techniques with the adaptability of
GPT-2-driven semantic analysis. Experimental evaluations on a representative
intrusion dataset demonstrate that our model enhances detection accuracy by
6.3%, reduces false positives by 9.0%, and maintains near real-time
responsiveness. These results affirm the potential of language model
integration to build intelligent, scalable, and resilient cybersecurity
defences suited for modern connected environments.

</details>


### [15] [Autonomous AI-based Cybersecurity Framework for Critical Infrastructure: Real-Time Threat Mitigation](https://arxiv.org/abs/2507.07416)
*Jenifer Paulraj,Brindha Raghuraman,Nagarani Gopalakrishnan,Yazan Otoum*

Main category: cs.CR

TL;DR: 本文研究了关键基础设施的网络安全漏洞，提出了一种混合AI驱动的框架以增强实时威胁检测和自动修复，并探讨了对抗性AI和合规性问题。


<details>
  <summary>Details</summary>
Motivation: 关键基础设施的互联性使其面临日益严重的网络威胁，需要更有效的安全解决方案。

Method: 提出了一种混合AI驱动的网络安全框架，用于实时漏洞检测、威胁建模和自动修复。

Result: 研究提供了可操作的见解，以增强关键基础设施的安全性和韧性。

Conclusion: AI驱动的框架能有效应对关键基础设施的网络安全挑战，但需解决对抗性AI和合规性问题。

Abstract: Critical infrastructure systems, including energy grids, healthcare
facilities, transportation networks, and water distribution systems, are
pivotal to societal stability and economic resilience. However, the increasing
interconnectivity of these systems exposes them to various cyber threats,
including ransomware, Denial-of-Service (DoS) attacks, and Advanced Persistent
Threats (APTs). This paper examines cybersecurity vulnerabilities in critical
infrastructure, highlighting the threat landscape, attack vectors, and the role
of Artificial Intelligence (AI) in mitigating these risks. We propose a hybrid
AI-driven cybersecurity framework to enhance real-time vulnerability detection,
threat modelling, and automated remediation. This study also addresses the
complexities of adversarial AI, regulatory compliance, and integration. Our
findings provide actionable insights to strengthen the security and resilience
of critical infrastructure systems against emerging cyber threats.

</details>


### [16] [May I have your Attention? Breaking Fine-Tuning based Prompt Injection Defenses using Architecture-Aware Attacks](https://arxiv.org/abs/2507.07417)
*Nishit V. Pandya,Andrey Labunets,Sicun Gao,Earlence Fernandes*

Main category: cs.CR

TL;DR: 论文评估了基于微调的语言模型防御提示注入攻击的鲁棒性，提出了一种新型注意力攻击算法，成功攻破了两种防御方法。


<details>
  <summary>Details</summary>
Motivation: 研究动机是验证现有防御方法在对抗优化攻击时的实际安全性。

Method: 提出了一种基于注意力的攻击算法，并应用于两种防御方法（SecAlign和StruQ）。

Result: 攻击成功率高达70%，且攻击成本（token数量）增加有限。

Conclusion: 研究揭示了现有防御方法在白盒设置下的脆弱性，为理解其鲁棒性提供了重要进展。

Abstract: A popular class of defenses against prompt injection attacks on large
language models (LLMs) relies on fine-tuning the model to separate instructions
and data, so that the LLM does not follow instructions that might be present
with data. There are several academic systems and production-level
implementations of this idea. We evaluate the robustness of this class of
prompt injection defenses in the whitebox setting by constructing strong
optimization-based attacks and showing that the defenses do not provide the
claimed security properties. Specifically, we construct a novel attention-based
attack algorithm for text-based LLMs and apply it to two recent whitebox
defenses SecAlign (CCS 2025) and StruQ (USENIX Security 2025), showing attacks
with success rates of up to 70% with modest increase in attacker budget in
terms of tokens. Our findings make fundamental progress towards understanding
the robustness of prompt injection defenses in the whitebox setting. We release
our code and attacks at https://github.com/nishitvp/better_opts_attacks

</details>


### [17] [RADAR: a Radio-based Analytics for Dynamic Association and Recognition of pseudonyms in VANETs](https://arxiv.org/abs/2507.07732)
*Giovanni Gambigliani Zoccoli,Filip Valgimigli,Dario Stabili,Mirco Marchetti*

Main category: cs.CR

TL;DR: RADAR算法通过结合DSRC和Wi-Fi信号，提高了车辆在C-ITS中的跟踪能力，优于仅使用DSRC的方法。Pearson RSSI指标在伪名更换场景下表现最佳。


<details>
  <summary>Details</summary>
Motivation: 研究旨在突破VANETs中隐私保护的伪名方案，提升车辆跟踪能力，尤其在攻击者无法覆盖整个车辆路径的现实场景中。

Method: 结合DSRC和Wi-Fi探针请求信号，采用三种指标（Count、Statistical RSSI、Pearson RSSI）关联伪名和Wi-Fi标识。

Result: Pearson RSSI指标在所有场景下表现最佳，优于现有方法。

Conclusion: RADAR算法显著提升车辆跟踪能力，公开了实现和仿真场景以推动研究。

Abstract: This paper presents RADAR, a tracking algorithm for vehicles participating in
Cooperative Intelligent Transportation Systems (C-ITS) that exploits multiple
radio signals emitted by a modern vehicle to break privacy-preserving pseudonym
schemes deployed in VANETs. This study shows that by combining Dedicated Short
Range Communication (DSRC) and Wi-Fi probe request messages broadcast by the
vehicle, it is possible to improve tracking over standard de-anonymization
approaches that only leverage DSRC, especially in realistic scenarios where the
attacker does not have full coverage of the entire vehicle path. The
experimental evaluation compares three different metrics for pseudonym and
Wi-Fi probe identifier association (Count, Statistical RSSI, and Pearson RSSI),
demonstrating that the Pearson RSSI metric is better at tracking vehicles under
pseudonym-changing schemes in all scenarios and against previous works. As an
additional contribution to the state-of-the-art, we publicly release all
implementations and simulation scenarios used in this work.

</details>


### [18] [Rainbow Artifacts from Electromagnetic Signal Injection Attacks on Image Sensors](https://arxiv.org/abs/2507.07773)
*Youqian Zhang,Xinyu Ji,Zhihao Wang,Qinhong Jiang*

Main category: cs.CR

TL;DR: 本文研究了一种针对图像传感器模拟域的新型电磁信号注入攻击，揭示了攻击者如何通过精心调制的电磁干扰操控原始视觉数据，并评估了其对目标检测模型的负面影响。


<details>
  <summary>Details</summary>
Motivation: 图像传感器广泛应用于安全和关键系统中，其数据完整性至关重要。然而，现有数字完整性检查无法防范模拟域攻击，因此需要研究此类漏洞。

Method: 通过精心调制的电磁干扰攻击CMOS图像传感器，诱导图像中出现彩虹色伪影，并评估其对目标检测模型的影响。

Result: 攻击成功导致图像信号处理管道中的伪影传播，显著影响目标检测模型的准确性，引发误判。

Conclusion: 研究揭示了视觉感知系统中一个关键但未被充分探索的漏洞，强调了针对物理层攻击的更强防御措施的必要性。

Abstract: Image sensors are integral to a wide range of safety- and security-critical
systems, including surveillance infrastructure, autonomous vehicles, and
industrial automation. These systems rely on the integrity of visual data to
make decisions. In this work, we investigate a novel class of electromagnetic
signal injection attacks that target the analog domain of image sensors,
allowing adversaries to manipulate raw visual inputs without triggering
conventional digital integrity checks. We uncover a previously undocumented
attack phenomenon on CMOS image sensors: rainbow-like color artifacts induced
in images captured by image sensors through carefully tuned electromagnetic
interference. We further evaluate the impact of these attacks on
state-of-the-art object detection models, showing that the injected artifacts
propagate through the image signal processing pipeline and lead to significant
mispredictions. Our findings highlight a critical and underexplored
vulnerability in the visual perception stack, highlighting the need for more
robust defenses against physical-layer attacks in such systems.

</details>


### [19] [Mitigating Watermark Stealing Attacks in Generative Models via Multi-Key Watermarking](https://arxiv.org/abs/2507.07871)
*Toluwani Aremu,Noor Hussein,Munachiso Nwadike,Samuele Poppi,Jie Zhang,Karthik Nandakumar,Neil Gong,Nils Lukas*

Main category: cs.CR

TL;DR: 论文提出了一种多密钥扩展方法，用于减轻水印窃取攻击，保护生成式AI提供者的内容来源。


<details>
  <summary>Details</summary>
Motivation: 生成式AI提供者面临水印窃取攻击的威胁，用户可能伪造水印以虚假指控提供者，因此需要一种方法来减轻此类攻击。

Method: 提出了一种多密钥扩展方法，可事后应用于任何水印方法，并通过理论保证和实证研究验证其有效性。

Result: 实验证明该方法显著降低了伪造水印的成功率，并在多个数据集中表现出色。

Conclusion: 多密钥扩展方法有效减轻了水印窃取攻击，为生成式AI提供者提供了更强的安全保障。

Abstract: Watermarking offers a promising solution for GenAI providers to establish the
provenance of their generated content. A watermark is a hidden signal embedded
in the generated content, whose presence can later be verified using a secret
watermarking key. A threat to GenAI providers are \emph{watermark stealing}
attacks, where users forge a watermark into content that was \emph{not}
generated by the provider's models without access to the secret key, e.g., to
falsely accuse the provider. Stealing attacks collect \emph{harmless}
watermarked samples from the provider's model and aim to maximize the expected
success rate of generating \emph{harmful} watermarked samples. Our work focuses
on mitigating stealing attacks while treating the underlying watermark as a
black-box. Our contributions are: (i) Proposing a multi-key extension to
mitigate stealing attacks that can be applied post-hoc to any watermarking
method across any modality. (ii) We provide theoretical guarantees and
demonstrate empirically that our method makes forging substantially less
effective across multiple datasets, and (iii) we formally define the threat of
watermark forging as the task of generating harmful, watermarked content and
model this threat via security games.

</details>


### [20] [The Trust Fabric: Decentralized Interoperability and Economic Coordination for the Agentic Web](https://arxiv.org/abs/2507.07901)
*Sree Bhargavi Balija,Rekha Singal,Abhishek Singh,Ramesh Raskar,Erfan Darzi,Raghu Bala,Thomas Hardjono,Ken Huang*

Main category: cs.CR

TL;DR: Nanda Unified Architecture提出了一种去中心化框架，解决了AI代理生态系统的互操作性、信任和经济协调问题，通过分布式注册、语义代理卡和动态信任层实现高效协作。


<details>
  <summary>Details</summary>
Motivation: 当前协议（如MCP、A2A、ACP和AGP）无法满足AI代理生态系统在互操作性、信任和经济协调方面的需求。

Method: 基于分布式注册的快速DID代理发现、语义代理卡（含可验证凭证和可组合性配置文件）和动态信任层（结合行为证明与策略合规）。

Result: 实际部署显示99.9%的合规率和显著的交易量，隐私保障强。

Conclusion: Nanda Unified Architecture通过密码学证明和策略即代码，将代理转变为去中心化经济中的信任锚点，实现了全球互操作的代理互联网。

Abstract: The fragmentation of AI agent ecosystems has created urgent demands for
interoperability, trust, and economic coordination that current protocols --
including MCP (Hou et al., 2025), A2A (Habler et al., 2025), ACP (Liu et al.,
2025), and Cisco's AGP (Edwards, 2025) -- cannot address at scale. We present
the Nanda Unified Architecture, a decentralized framework built around three
core innovations: fast DID-based agent discovery through distributed
registries, semantic agent cards with verifiable credentials and composability
profiles, and a dynamic trust layer that integrates behavioral attestations
with policy compliance. The system introduces X42/H42 micropayments for
economic coordination and MAESTRO, a security framework incorporating
Synergetics' patented AgentTalk protocol (US Patent 12,244,584 B1) and secure
containerization. Real-world deployments demonstrate 99.9 percent compliance in
healthcare applications and substantial monthly transaction volumes with strong
privacy guarantees. By unifying MIT's trust research with production
deployments from Cisco and Synergetics, we show how cryptographic proofs and
policy-as-code transform agents into trust-anchored participants in a
decentralized economy (Lakshmanan, 2025; Sha, 2025). The result enables a
globally interoperable Internet of Agents where trust becomes the native
currency of collaboration across both enterprise and Web3 ecosystems.

</details>


### [21] [Can Large Language Models Improve Phishing Defense? A Large-Scale Controlled Experiment on Warning Dialogue Explanations](https://arxiv.org/abs/2507.07916)
*Federico Maria Cau,Giuseppe Desolda,Francesco Greco,Lucio Davide Spano,Luca Viganò*

Main category: cs.CR

TL;DR: 研究评估了大型语言模型（LLMs）生成钓鱼警告解释的效果，发现其性能可媲美或优于人工解释，且具有可扩展性。


<details>
  <summary>Details</summary>
Motivation: 钓鱼攻击利用人类行为绕过技术防御，现有警告对话框因解释不清和内容静态而效果有限。

Method: 通过大规模用户研究（N=750），比较人工生成解释与两种LLM（Claude 3.5 Sonnet和Llama 3.3 70B）生成解释的效果，分析两种解释风格（基于特征和反事实）对行为和感知的影响。

Result: LLM生成的解释在降低钓鱼易感性上表现优异，Claude效果突出；基于特征的解释对真实钓鱼更有效，反事实解释减少误报率。其他变量（如工作量、性别）也影响效果。

Conclusion: LLM可自动生成高效、可扩展的钓鱼警告解释，符合以人为本的价值观。

Abstract: Phishing has become a prominent risk in modern cybersecurity, often used to
bypass technological defences by exploiting predictable human behaviour.
Warning dialogues are a standard mitigation measure, but the lack of
explanatory clarity and static content limits their effectiveness. In this
paper, we report on our research to assess the capacity of Large Language
Models (LLMs) to generate clear, concise, and scalable explanations for
phishing warnings. We carried out a large-scale between-subjects user study (N
= 750) to compare the influence of warning dialogues supplemented with manually
generated explanations against those generated by two LLMs, Claude 3.5 Sonnet
and Llama 3.3 70B. We investigated two explanatory styles (feature-based and
counterfactual) for their effects on behavioural metrics (click-through rate)
and perceptual outcomes (e.g., trust, risk, clarity). The results indicate that
well-constructed LLM-generated explanations can equal or surpass manually
crafted explanations in reducing susceptibility to phishing; Claude-generated
warnings exhibited particularly robust performance. Feature-based explanations
were more effective for genuine phishing attempts, whereas counterfactual
explanations diminished false-positive rates. Other variables such as workload,
gender, and prior familiarity with warning dialogues significantly moderated
warning effectiveness. These results indicate that LLMs can be used to
automatically build explanations for warning users against phishing, and that
such solutions are scalable, adaptive, and consistent with human-centred
values.

</details>


### [22] [KeyDroid: A Large-Scale Analysis of Secure Key Storage in Android Apps](https://arxiv.org/abs/2507.07927)
*Jenny Blessing,Ross J. Anderson,Alastair R. Beresford*

Main category: cs.CR

TL;DR: 本文首次全面调查了Android设备中硬件支持的密钥存储使用情况，发现大多数处理敏感数据的应用未充分利用可信硬件，且使用最强形式可信硬件的应用仅占少数。同时，研究还分析了可信硬件的性能问题。


<details>
  <summary>Details</summary>
Motivation: 调查Android应用中硬件支持的密钥存储使用情况，评估其普及程度及性能影响，以了解开发者对可信硬件的实际应用及其潜在问题。

Method: 分析了490,119个Android应用，收集开发者对可信硬件的使用情况，并与Play Store的数据安全标签进行交叉验证。同时，首次对可信硬件性能进行了实证分析。

Result: 56.3%处理敏感数据的应用未使用可信硬件，仅5.03%使用了最强的安全元件。性能测试显示，某些高级安全元件在对称和非对称加密中性能不足。

Conclusion: 尽管可信硬件能增强安全性，但其普及率低且性能问题限制了其在某些加密场景中的应用。

Abstract: Most contemporary mobile devices offer hardware-backed storage for
cryptographic keys, user data, and other sensitive credentials. Such hardware
protects credentials from extraction by an adversary who has compromised the
main operating system, such as a malicious third-party app. Since 2011, Android
app developers can access trusted hardware via the Android Keystore API. In
this work, we conduct the first comprehensive survey of hardware-backed key
storage in Android devices. We analyze 490 119 Android apps, collecting data on
how trusted hardware is used by app developers (if used at all) and
cross-referencing our findings with sensitive user data collected by each app,
as self-reported by developers via the Play Store's data safety labels.
  We find that despite industry-wide initiatives to encourage adoption, 56.3%
of apps self-reporting as processing sensitive user data do not use Android's
trusted hardware capabilities at all, while just 5.03% of apps collecting some
form of sensitive data use the strongest form of trusted hardware, a secure
element distinct from the main processor. To better understand the potential
downsides of using secure hardware, we conduct the first empirical analysis of
trusted hardware performance in mobile devices, measuring the runtime of common
cryptographic operations across both software- and hardware-backed keystores.
We find that while hardware-backed key storage using a coprocessor is viable
for most common cryptographic operations, secure elements capable of preventing
more advanced attacks make performance infeasible for symmetric encryption with
non-negligible payloads and any kind of asymmetric encryption.

</details>


### [23] [EinHops: Einsum Notation for Expressive Homomorphic Operations on RNS-CKKS Tensors](https://arxiv.org/abs/2507.07972)
*Karthik Garimella,Austin Ebel,Brandon Reagen*

Main category: cs.CR

TL;DR: 论文提出了一种基于爱因斯坦求和（einsum）符号的方法EinHops，用于在完全同态加密（FHE）中实现多维张量操作，解决了传统方法中张量打包和操作映射的复杂性问题。


<details>
  <summary>Details</summary>
Motivation: FHE虽然支持加密数据的直接计算，但其操作仅限于一维向量，导致多维张量操作困难。现有系统抽象过多，难以调试和优化。

Method: 通过einsum符号明确编码张量结构和操作，将其分解为FHE友好的操作序列，并实现EinHops系统。

Result: EinHops能够简单、通用且可解释地实现加密张量操作，支持从简单转置到复杂多维收缩的多种操作。

Conclusion: einsum符号的显式特性使得FHE张量系统更简单、通用且易于理解，EinHops为开发者提供了透明且高效的解决方案。

Abstract: Fully Homomorphic Encryption (FHE) is an encryption scheme that allows for
computation to be performed directly on encrypted data, effectively closing the
loop on secure and outsourced computing. Data is encrypted not only during rest
and transit, but also during processing. However, FHE provides a limited
instruction set: SIMD addition, SIMD multiplication, and cyclic rotation of 1-D
vectors. This restriction makes performing multi-dimensional tensor operations
challenging. Practitioners must pack these tensors into 1-D vectors and map
tensor operations onto this one-dimensional layout rather than their
traditional nested structure. And while prior systems have made significant
strides in automating this process, they often hide critical packing decisions
behind layers of abstraction, making debugging, optimizing, and building on top
of these systems difficult.
  In this work, we approach multi-dimensional tensor operations in FHE through
Einstein summation (einsum) notation. Einsum notation explicitly encodes
dimensional structure and operations in its syntax, naturally exposing how
tensors should be packed and transformed. We decompose einsum expressions into
a fixed set of FHE-friendly operations. We implement our design and present
EinHops, a minimalist system that factors einsum expressions into a fixed
sequence of FHE operations. EinHops enables developers to perform encrypted
tensor operations using FHE while maintaining full visibility into the
underlying packing strategy. We evaluate EinHops on a range of tensor
operations from a simple transpose to complex multi-dimensional contractions.
We show that the explicit nature of einsum notation allows us to build an FHE
tensor system that is simple, general, and interpretable. We open-source
EinHops at the following repository: https://github.com/baahl-nyu/einhops.

</details>


### [24] [Defending Against Prompt Injection With a Few DefensiveTokens](https://arxiv.org/abs/2507.07974)
*Sizhe Chen,Yizhu Wang,Nicholas Carlini,Chawin Sitawarin,David Wagner*

Main category: cs.CR

TL;DR: DefensiveToken是一种测试时防御方法，通过插入特殊令牌优化嵌入，以在需要时提供接近训练时防御的安全性，同时保持灵活性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）系统在交互外部数据时易受提示注入攻击，现有测试时防御效果不佳，需一种灵活且高效的防御方法。

Method: 提出DefensiveToken，通过插入特殊令牌并优化其嵌入，在需要时附加到输入前以增强安全性。

Result: DefensiveToken在保持高实用性的同时，提供接近训练时防御的安全性，且可灵活开关。

Conclusion: DefensiveToken为LLM系统提供了一种灵活、高效的测试时防御方案，平衡了安全性和实用性。

Abstract: When large language model (LLM) systems interact with external data to
perform complex tasks, a new attack, namely prompt injection, becomes a
significant threat. By injecting instructions into the data accessed by the
system, the attacker is able to override the initial user task with an
arbitrary task directed by the attacker. To secure the system, test-time
defenses, e.g., defensive prompting, have been proposed for system developers
to attain security only when needed in a flexible manner. However, they are
much less effective than training-time defenses that change the model
parameters. Motivated by this, we propose DefensiveToken, a test-time defense
with prompt injection robustness comparable to training-time alternatives.
DefensiveTokens are newly inserted as special tokens, whose embeddings are
optimized for security. In security-sensitive cases, system developers can
append a few DefensiveTokens before the LLM input to achieve security with a
minimal utility drop. In scenarios where security is less of a concern,
developers can simply skip DefensiveTokens; the LLM system remains the same as
there is no defense, generating high-quality responses. Thus, DefensiveTokens,
if released alongside the model, allow a flexible switch between the
state-of-the-art (SOTA) utility and almost-SOTA security at test time. The code
is available at https://github.com/Sizhe-Chen/DefensiveToken.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [25] [Autonomous Control Leveraging LLMs: An Agentic Framework for Next-Generation Industrial Automation](https://arxiv.org/abs/2507.07115)
*Javal Vyas,Mehmet Mercangoz*

Main category: cs.AI

TL;DR: 论文提出了一种结合符号推理和自适应控制的统一智能体框架，利用大语言模型（LLMs）实现离散故障恢复规划和连续过程控制，并通过案例验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现代化学过程日益复杂，劳动力短缺和故障场景多样化，需要新的自动化范式来结合符号推理与自适应控制。

Method: 采用有限状态机（FSMs）作为可解释的操作框架，通过LLM驱动的规划代理生成恢复序列，模拟代理执行和验证，验证-重提示循环迭代优化无效计划。

Result: 在案例1中，GPT-4o和GPT-4o-mini在180个随机生成的FSMs中实现100%有效路径成功率；案例2中，LLM控制器在双加热器实验中表现与经典PID控制相当。

Conclusion: 研究表明，通过结构化反馈和模块化代理，LLMs可以统一高层符号规划和低层连续控制，为化学工程中的语言驱动自动化铺平道路。

Abstract: The increasing complexity of modern chemical processes, coupled with
workforce shortages and intricate fault scenarios, demands novel automation
paradigms that blend symbolic reasoning with adaptive control. In this work, we
introduce a unified agentic framework that leverages large language models
(LLMs) for both discrete fault-recovery planning and continuous process control
within a single architecture. We adopt Finite State Machines (FSMs) as
interpretable operating envelopes: an LLM-driven planning agent proposes
recovery sequences through the FSM, a Simulation Agent executes and checks each
transition, and a Validator-Reprompting loop iteratively refines invalid plans.
In Case Study 1, across 180 randomly generated FSMs of varying sizes (4-25
states, 4-300 transitions), GPT-4o and GPT-4o-mini achieve 100% valid-path
success within five reprompts-outperforming open-source LLMs in both accuracy
and latency. In Case Study 2, the same framework modulates dual-heater inputs
on a laboratory TCLab platform (and its digital twin) to maintain a target
average temperature under persistent asymmetric disturbances. Compared to
classical PID control, our LLM-based controller attains similar performance,
while ablation of the prompting loop reveals its critical role in handling
nonlinear dynamics. We analyze key failure modes-such as instruction following
lapses and coarse ODE approximations. Our results demonstrate that, with
structured feedback and modular agents, LLMs can unify high-level symbolic
planningand low-level continuous control, paving the way towards resilient,
language-driven automation in chemical engineering.

</details>


### [26] [BOOST: Out-of-Distribution-Informed Adaptive Sampling for Bias Mitigation in Stylistic Convolutional Neural Networks](https://arxiv.org/abs/2507.07134)
*Mridula Vijendran,Shuang Chen,Jingjing Deng,Hubert P. H. Shum*

Main category: cs.AI

TL;DR: 论文提出了一种名为BOOST的新方法，通过动态调整温度缩放和采样概率，解决AI艺术分类中的偏见问题，并在KaoKore和PACS数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: AI在艺术分类中的偏见问题日益严重，尤其是在处理分布外数据时，现有方法未能有效解决这一挑战。

Method: 提出BOOST方法，动态调整温度缩放和采样概率，并引入新指标SODC评估类别分离和偏见减少。

Result: BOOST在平衡性能和公平性方面表现优异，有效减少了类别偏见。

Conclusion: BOOST是一种解决艺术分类中偏见问题的稳健方法，适用于实际应用。

Abstract: The pervasive issue of bias in AI presents a significant challenge to
painting classification, and is getting more serious as these systems become
increasingly integrated into tasks like art curation and restoration. Biases,
often arising from imbalanced datasets where certain artistic styles dominate,
compromise the fairness and accuracy of model predictions, i.e., classifiers
are less accurate on rarely seen paintings. While prior research has made
strides in improving classification performance, it has largely overlooked the
critical need to address these underlying biases, that is, when dealing with
out-of-distribution (OOD) data. Our insight highlights the necessity of a more
robust approach to bias mitigation in AI models for art classification on
biased training data. We propose a novel OOD-informed model bias adaptive
sampling method called BOOST (Bias-Oriented OOD Sampling and Tuning). It
addresses these challenges by dynamically adjusting temperature scaling and
sampling probabilities, thereby promoting a more equitable representation of
all classes. We evaluate our proposed approach to the KaoKore and PACS
datasets, focusing on the model's ability to reduce class-wise bias. We further
propose a new metric, Same-Dataset OOD Detection Score (SODC), designed to
assess class-wise separation and per-class bias reduction. Our method
demonstrates the ability to balance high performance with fairness, making it a
robust solution for unbiasing AI models in the art domain.

</details>


### [27] [State-Inference-Based Prompting for Natural Language Trading with Game NPCs](https://arxiv.org/abs/2507.07203)
*Minkyung Kim,Junsik Kim,Hwidong Bae,Woongcheol Yang,Sangdon Park,Sohee Bae*

Main category: cs.AI

TL;DR: SIBP方法通过状态推断和规则遵守解决了大语言模型在交易系统中的规则违反问题，显著提升了准确性和信任度。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在动态游戏交互中表现良好，但在规则驱动的交易系统中容易出现规则违反（如物品幻觉和计算错误），影响玩家信任。

Method: 提出State-Inference-Based Prompting (SIBP)，将交易分解为六个状态，通过上下文感知的物品引用和占位符价格计算实现规则遵守。

Result: 在100次交易对话中，状态合规率>97%，引用准确率>95%，计算精度99.7%，优于基线方法且计算高效。

Conclusion: SIBP为商业游戏中可信NPC交互提供了实用基础。

Abstract: Large Language Models enable dynamic game interactions but struggle with
rule-governed trading systems. Current implementations suffer from rule
violations, such as item hallucinations and calculation errors, that erode
player trust. Here, State-Inference-Based Prompting (SIBP) enables reliable
trading through autonomous dialogue state inference and context-specific rule
adherence. The approach decomposes trading into six states within a unified
prompt framework, implementing context-aware item referencing and
placeholder-based price calculations. Evaluation across 100 trading dialogues
demonstrates >97% state compliance, >95% referencing accuracy, and 99.7%
calculation precision. SIBP maintains computational efficiency while
outperforming baseline approaches, establishing a practical foundation for
trustworthy NPC interactions in commercial games.

</details>


### [28] [Neurosymbolic Feature Extraction for Identifying Forced Labor in Supply Chains](https://arxiv.org/abs/2507.07217)
*Zili Wang,Frank Montabon,Kristin Yvonne Rozier*

Main category: cs.AI

TL;DR: 论文探讨了如何利用神经符号方法检测供应链中的非法活动，尤其是在数据稀疏且不可靠的情况下，通过结合大型语言模型（LLM）和问题树方法，实现自动分类和评估新闻文章的相关性。


<details>
  <summary>Details</summary>
Motivation: 供应链中的非法活动（如假冒零件、强迫劳动）数据稀疏且不可靠，传统机器学习方法难以应对，需要新的方法自动检测这些活动。

Method: 采用神经符号方法，结合LLM和问题树方法，自动提取新闻文章中的特征，并与人工分类进行对比。

Result: 提出了一种系统化的方法，能够在不依赖大规模训练数据的情况下，有效识别和量化与供应链非法活动相关的新闻文章。

Conclusion: 神经符号方法和LLM的结合为检测供应链中的非法活动提供了新思路，尤其在数据稀疏的情况下表现优越。

Abstract: Supply chain networks are complex systems that are challenging to analyze;
this problem is exacerbated when there are illicit activities involved in the
supply chain, such as counterfeit parts, forced labor, or human trafficking.
While machine learning (ML) can find patterns in complex systems like supply
chains, traditional ML techniques require large training data sets. However,
illicit supply chains are characterized by very sparse data, and the data that
is available is often (purposely) corrupted or unreliable in order to hide the
nature of the activities. We need to be able to automatically detect new
patterns that correlate with such illegal activity over complex, even temporal
data, without requiring large training data sets. We explore neurosymbolic
methods for identifying instances of illicit activity in supply chains and
compare the effectiveness of manual and automated feature extraction from news
articles accurately describing illicit activities uncovered by authorities. We
propose a question tree approach for querying a large language model (LLM) to
identify and quantify the relevance of articles. This enables a systematic
evaluation of the differences between human and machine classification of news
articles related to forced labor in supply chains.

</details>


### [29] [Open Source Planning & Control System with Language Agents for Autonomous Scientific Discovery](https://arxiv.org/abs/2507.07257)
*Licong Xu,Milind Sarkar,Anto I. Lonappan,Íñigo Zubeldia,Pablo Villanueva-Domingo,Santiago Casas,Christian Fidler,Chetana Amancharla,Ujjwal Tiwari,Adrian Bayer,Chadi Ait Ekiou,Miles Cranmer,Adrian Dimitrov,James Fergusson,Kahaan Gandhi,Sven Krippendorf,Andrew Laverick,Julien Lesgourgues,Antony Lewis,Thomas Meier,Blake Sherwin,Kristen Surrao,Francisco Villaescusa-Navarro,Chi Wang,Xueqing Xu,Boris Bolliet*

Main category: cs.AI

TL;DR: 介绍了一个名为cmbagent的多智能体系统，用于自动化科学研究任务，由约30个LLM代理组成，采用规划与控制策略，无需人工干预。


<details>
  <summary>Details</summary>
Motivation: 旨在通过多智能体系统实现科学研究任务的自动化，减少人工干预，提高效率。

Method: 系统由多个专业化的LLM代理组成，采用规划与控制策略协调工作流，支持本地代码执行。

Result: 成功应用于博士级宇宙学任务，性能优于现有LLM模型。

Conclusion: cmbagent展示了多智能体系统在科学研究自动化中的潜力，代码和演示已公开。

Abstract: We present a multi-agent system for automation of scientific research tasks,
cmbagent. The system is formed by about 30 Large Language Model (LLM) agents
and implements a Planning & Control strategy to orchestrate the agentic
workflow, with no human-in-the-loop at any point. Each agent specializes in a
different task (performing retrieval on scientific papers and codebases,
writing code, interpreting results, critiquing the output of other agents) and
the system is able to execute code locally. We successfully apply cmbagent to
carry out a PhD level cosmology task (the measurement of cosmological
parameters using supernova data) and evaluate its performance on two benchmark
sets, finding superior performance over state-of-the-art LLMs. The source code
is available on GitHub, demonstration videos are also available, and the system
is deployed on HuggingFace and will be available on the cloud.

</details>


### [30] [Application of LLMs to Multi-Robot Path Planning and Task Allocation](https://arxiv.org/abs/2507.07302)
*Ashish Kumar*

Main category: cs.AI

TL;DR: 本文研究了在多智能体强化学习中，利用大型语言模型作为专家规划器以提升探索效率的方法。


<details>
  <summary>Details</summary>
Motivation: 多智能体强化学习中的探索效率问题因算法复杂性而加剧，现有方法中专家探索是一种潜在解决方案。

Method: 提出使用大型语言模型作为专家规划器，应用于基于规划的多智能体任务中。

Result: 通过实验验证了该方法在提升探索效率方面的有效性。

Conclusion: 大型语言模型作为专家规划器在多智能体任务中具有潜力，能显著提升探索效率。

Abstract: Efficient exploration is a well known problem in deep reinforcement learning
and this problem is exacerbated in multi-agent reinforcement learning due the
intrinsic complexities of such algorithms. There are several approaches to
efficiently explore an environment to learn to solve tasks by multi-agent
operating in that environment, of which, the idea of expert exploration is
investigated in this work. More specifically, this work investigates the
application of large-language models as expert planners for efficient
exploration in planning based tasks for multiple agents.

</details>


### [31] [ViDove: A Translation Agent System with Multimodal Context and Memory-Augmented Reasoning](https://arxiv.org/abs/2507.07306)
*Yichen Lu,Wei Dai,Jiaen Liu,Ching Wing Kwok,Zongheng Wu,Xudong Xiao,Ao Sun,Sheng Fu,Jianyuan Zhan,Yian Wang,Takatomo Saito,Sicheng Lai*

Main category: cs.AI

TL;DR: ViDove是一种基于LLM的多模态翻译代理系统，结合视觉和上下文信息提升翻译质量，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有LLM翻译代理通常仅支持文本输入，无法利用多模态信息，限制了翻译质量。

Method: ViDove模拟人类翻译流程，整合视觉背景和多模态记忆系统，结合长短期记忆模块和领域知识。

Result: 在字幕生成和通用翻译任务中，ViDove的BLEU分数提升28%，SubER提升15%。

Conclusion: ViDove通过多模态输入显著提升翻译质量，并发布了DoveBench作为新基准。

Abstract: LLM-based translation agents have achieved highly human-like translation
results and are capable of handling longer and more complex contexts with
greater efficiency. However, they are typically limited to text-only inputs. In
this paper, we introduce ViDove, a translation agent system designed for
multimodal input. Inspired by the workflow of human translators, ViDove
leverages visual and contextual background information to enhance the
translation process. Additionally, we integrate a multimodal memory system and
long-short term memory modules enriched with domain-specific knowledge,
enabling the agent to perform more accurately and adaptively in real-world
scenarios. As a result, ViDove achieves significantly higher translation
quality in both subtitle generation and general translation tasks, with a 28%
improvement in BLEU scores and a 15% improvement in SubER compared to previous
state-of-the-art baselines. Moreover, we introduce DoveBench, a new benchmark
for long-form automatic video subtitling and translation, featuring 17 hours of
high-quality, human-annotated data. Our code is available here:
https://github.com/pigeonai-org/ViDove

</details>


### [32] [On the Impossibility of Separating Intelligence from Judgment: The Computational Intractability of Filtering for AI Alignment](https://arxiv.org/abs/2507.07341)
*Sarah Ball,Greg Gluch,Shafi Goldwasser,Frauke Kreuter,Omer Reingold,Guy N. Rothblum*

Main category: cs.AI

TL;DR: 论文研究了大型语言模型（LLMs）的安全对齐问题，重点探讨了输入提示和输出过滤的挑战，发现外部过滤在计算上不可行，并主张安全需内置于模型内部。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs的广泛应用，其可能被滥用于生成有害内容的担忧增加，研究旨在解决这一对齐挑战。

Method: 通过分析输入提示和输出过滤的计算效率，证明在加密假设下，高效过滤不可行。

Result: 发现对抗性提示可轻易绕过输入过滤，且输出过滤在特定情况下计算不可行。

Conclusion: 安全不能仅依赖外部过滤，需将判断力内置于模型设计中。

Abstract: With the increased deployment of large language models (LLMs), one concern is
their potential misuse for generating harmful content. Our work studies the
alignment challenge, with a focus on filters to prevent the generation of
unsafe information. Two natural points of intervention are the filtering of the
input prompt before it reaches the model, and filtering the output after
generation. Our main results demonstrate computational challenges in filtering
both prompts and outputs. First, we show that there exist LLMs for which there
are no efficient prompt filters: adversarial prompts that elicit harmful
behavior can be easily constructed, which are computationally indistinguishable
from benign prompts for any efficient filter. Our second main result identifies
a natural setting in which output filtering is computationally intractable. All
of our separation results are under cryptographic hardness assumptions. In
addition to these core findings, we also formalize and study relaxed mitigation
approaches, demonstrating further computational barriers. We conclude that
safety cannot be achieved by designing filters external to the LLM internals
(architecture and weights); in particular, black-box access to the LLM will not
suffice. Based on our technical results, we argue that an aligned AI system's
intelligence cannot be separated from its judgment.

</details>


### [33] [Supply Chain Optimization via Generative Simulation and Iterative Decision Policies](https://arxiv.org/abs/2507.07355)
*Haoyue Bai,Haoyu Wang,Nanxu Gong,Xinyuan Wang,Wangyang Ying,Haifeng Chen,Yanjie Fu*

Main category: cs.AI

TL;DR: Sim-to-Dec框架结合生成模拟模块和双感知决策模型，显著提升供应链运输的及时交付率和利润。


<details>
  <summary>Details</summary>
Motivation: 供应链运输的高响应性和经济效率受运输模式战略决策影响，需一种可观察、低风险的策略设计环境。

Method: 提出Sim-to-Dec框架，包含生成模拟模块（自回归建模）和双感知决策模型（历史与未来数据结合），通过端到端优化迭代改进。

Result: 在三个真实数据集上的实验表明，Sim-to-Dec显著提高及时交付率和利润。

Conclusion: Sim-to-Dec框架满足跨场景通用性、细粒度动态模拟、历史与预测结合及模拟反馈与策略优化的紧密集成需求。

Abstract: High responsiveness and economic efficiency are critical objectives in supply
chain transportation, both of which are influenced by strategic decisions on
shipping mode. An integrated framework combining an efficient simulator with an
intelligent decision-making algorithm can provide an observable, low-risk
environment for transportation strategy design. An ideal simulation-decision
framework must (1) generalize effectively across various settings, (2) reflect
fine-grained transportation dynamics, (3) integrate historical experience with
predictive insights, and (4) maintain tight integration between simulation
feedback and policy refinement. We propose Sim-to-Dec framework to satisfy
these requirements. Specifically, Sim-to-Dec consists of a generative
simulation module, which leverages autoregressive modeling to simulate
continuous state changes, reducing dependence on handcrafted domain-specific
rules and enhancing robustness against data fluctuations; and a history-future
dual-aware decision model, refined iteratively through end-to-end optimization
with simulator interactions. Extensive experiments conducted on three
real-world datasets demonstrate that Sim-to-Dec significantly improves timely
delivery rates and profit.

</details>


### [34] [DrugMCTS: a drug repurposing framework combining multi-agent, RAG and Monte Carlo Tree Search](https://arxiv.org/abs/2507.07426)
*Zerui Yang,Yuwei Wan,Yinqiao Li,Yudai Matsuda,Tong Xie,Linqi Song*

Main category: cs.AI

TL;DR: DrugMCTS框架结合RAG、多智能体协作和蒙特卡洛树搜索，用于药物重定位，无需领域微调即可显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在科学领域（如药物发现）中超出预训练知识范围时的推理限制问题。

Method: 提出DrugMCTS框架，整合RAG、多智能体协作和蒙特卡洛树搜索，利用五个专业智能体检索和分析分子与蛋白质信息。

Result: 在DrugBank和KIBA数据集上，DrugMCTS的召回率和鲁棒性显著优于通用LLM和深度学习基线，性能提升超20%。

Conclusion: 结构化推理、智能体协作和反馈驱动搜索机制对药物发现中的LLM应用至关重要。

Abstract: Recent advances in large language models have demonstrated considerable
potential in scientific domains such as drug discovery. However, their
effectiveness remains constrained when reasoning extends beyond the knowledge
acquired during pretraining. Conventional approaches, such as fine-tuning or
retrieval-augmented generation, face limitations in either imposing high
computational overhead or failing to fully exploit structured scientific data.
To overcome these challenges, we propose DrugMCTS, a novel framework that
synergistically integrates RAG, multi-agent collaboration, and Monte Carlo Tree
Search for drug repurposing. The framework employs five specialized agents
tasked with retrieving and analyzing molecular and protein information, thereby
enabling structured and iterative reasoning. Without requiring domain-specific
fine-tuning, DrugMCTS empowers Qwen2.5-7B-Instruct to outperform Deepseek-R1 by
over 20\%. Extensive experiments on the DrugBank and KIBA datasets demonstrate
that DrugMCTS achieves substantially higher recall and robustness compared to
both general-purpose LLMs and deep learning baselines. Our results highlight
the importance of structured reasoning, agent-based collaboration, and
feedback-driven search mechanisms in advancing LLM applications for drug
discovery.

</details>


### [35] [StarDojo: Benchmarking Open-Ended Behaviors of Agentic Multimodal LLMs in Production-Living Simulations with Stardew Valley](https://arxiv.org/abs/2507.07445)
*Weihao Tan,Changjiu Jiang,Yu Duan,Mingcong Lei,Jiageng Li,Yitian Hong,Xinrun Wang,Bo An*

Main category: cs.AI

TL;DR: StarDojo是一个基于Stardew Valley的新型基准测试，用于评估AI代理在开放式生产生活模拟中的表现，涵盖农业、手工艺、探索、战斗和社交互动五大领域。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试很少同时评估生产活动和社会互动能力，StarDojo旨在填补这一空白。

Method: StarDojo包含1000个任务和100个代表性子集，提供统一友好的界面，支持多操作系统和多环境实例并行执行。

Result: 评估显示当前最先进的MLLM代理（如GPT-4.1）成功率仅为12.7%，主要受限于视觉理解、多模态推理和低级操作能力。

Conclusion: StarDojo为复杂生产生活环境中稳健开放式代理的研究提供了便利工具。

Abstract: Autonomous agents navigating human society must master both production
activities and social interactions, yet existing benchmarks rarely evaluate
these skills simultaneously. To bridge this gap, we introduce StarDojo, a novel
benchmark based on Stardew Valley, designed to assess AI agents in open-ended
production-living simulations. In StarDojo, agents are tasked to perform
essential livelihood activities such as farming and crafting, while
simultaneously engaging in social interactions to establish relationships
within a vibrant community. StarDojo features 1,000 meticulously curated tasks
across five key domains: farming, crafting, exploration, combat, and social
interactions. Additionally, we provide a compact subset of 100 representative
tasks for efficient model evaluation. The benchmark offers a unified,
user-friendly interface that eliminates the need for keyboard and mouse
control, supports all major operating systems, and enables the parallel
execution of multiple environment instances, making it particularly well-suited
for evaluating the most capable foundation agents, powered by multimodal large
language models (MLLMs). Extensive evaluations of state-of-the-art MLLMs agents
demonstrate substantial limitations, with the best-performing model, GPT-4.1,
achieving only a 12.7% success rate, primarily due to challenges in visual
understanding, multimodal reasoning and low-level manipulation. As a
user-friendly environment and benchmark, StarDojo aims to facilitate further
research towards robust, open-ended agents in complex production-living
environments.

</details>


### [36] [Position: We Need An Algorithmic Understanding of Generative AI](https://arxiv.org/abs/2507.07544)
*Oliver Eberle,Thomas McGee,Hamza Giaffar,Taylor Webb,Ida Momennejad*

Main category: cs.AI

TL;DR: AlgEval框架旨在系统研究LLM学习的算法，揭示其潜在表示、注意力和推理计算中的算法原语及其组合方式，以解决任务特定问题。


<details>
  <summary>Details</summary>
Motivation: 填补LLM研究中关于其实际学习算法的理论空白，提供对模型内部计算的理解，避免仅依赖资源密集型扩展。

Method: 提出AlgEval框架，结合自上而下的假设形成和自下而上的电路级分析（注意力模式和隐藏状态）。

Result: 案例研究表明了搜索算法的涌现，展示了假设验证和模型内部推理的可行性。

Conclusion: AlgEval为理解LLM的算法提供系统方法，推动可解释性和高效训练，并为新架构设计提供基础。

Abstract: What algorithms do LLMs actually learn and use to solve problems? Studies
addressing this question are sparse, as research priorities are focused on
improving performance through scale, leaving a theoretical and empirical gap in
understanding emergent algorithms. This position paper proposes AlgEval: a
framework for systematic research into the algorithms that LLMs learn and use.
AlgEval aims to uncover algorithmic primitives, reflected in latent
representations, attention, and inference-time compute, and their algorithmic
composition to solve task-specific problems. We highlight potential
methodological paths and a case study toward this goal, focusing on emergent
search algorithms. Our case study illustrates both the formation of top-down
hypotheses about candidate algorithms, and bottom-up tests of these hypotheses
via circuit-level analysis of attention patterns and hidden states. The
rigorous, systematic evaluation of how LLMs actually solve tasks provides an
alternative to resource-intensive scaling, reorienting the field toward a
principled understanding of underlying computations. Such algorithmic
explanations offer a pathway to human-understandable interpretability, enabling
comprehension of the model's internal reasoning performance measures. This can
in turn lead to more sample-efficient methods for training and improving
performance, as well as novel architectures for end-to-end and multi-agent
systems.

</details>


### [37] [On Trustworthy Rule-Based Models and Explanations](https://arxiv.org/abs/2507.07576)
*Mohamed Siala,Jordi Planes,Joao Marques-Silva*

Main category: cs.AI

TL;DR: 论文探讨了机器学习中规则模型解释的负面影响，并提出了分析算法。


<details>
  <summary>Details</summary>
Motivation: 在高风险领域，错误的解释可能误导决策者，因此需要严格的分析。

Method: 开发算法分析规则模型中的负面特征，如重叠和冗余。

Result: 发现常用工具学习的规则集存在负面特征。

Conclusion: 常用规则学习工具会导致规则集出现负面特征，需改进。

Abstract: A task of interest in machine learning (ML) is that of ascribing explanations
to the predictions made by ML models. Furthermore, in domains deemed high risk,
the rigor of explanations is paramount. Indeed, incorrect explanations can and
will mislead human decision makers. As a result, and even if interpretability
is acknowledged as an elusive concept, so-called interpretable models are
employed ubiquitously in high-risk uses of ML and data mining (DM). This is the
case for rule-based ML models, which encompass decision trees, diagrams, sets
and lists. This paper relates explanations with well-known undesired facets of
rule-based ML models, which include negative overlap and several forms of
redundancy. The paper develops algorithms for the analysis of these undesired
facets of rule-based systems, and concludes that well-known and widely used
tools for learning rule-based ML models will induce rule sets that exhibit one
or more negative facets.

</details>


### [38] [Context Pooling: Query-specific Graph Pooling for Generic Inductive Link Prediction in Knowledge Graphs](https://arxiv.org/abs/2507.07595)
*Zhixiang Su,Di Wang,Chunyan Miao*

Main category: cs.AI

TL;DR: 论文提出了一种名为Context Pooling的新方法，用于提升GNN在知识图谱链接预测中的性能，通过生成查询特定图并评估邻居逻辑相关性，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有GNN模型在知识图谱链接预测中表现不佳，尤其是vanilla聚合方法效果有限，需要更有效的方法提升性能。

Method: 提出Context Pooling方法，首次在知识图谱中应用图池化，并设计邻居精确度和召回率指标筛选逻辑相关邻居。

Result: 在三个公开数据集上应用于两个SOTA模型，42/48情况下达到最优性能。

Conclusion: Context Pooling是一种通用且高效的方法，显著提升了GNN在知识图谱链接预测中的表现。

Abstract: Recent investigations on the effectiveness of Graph Neural Network
(GNN)-based models for link prediction in Knowledge Graphs (KGs) show that
vanilla aggregation does not significantly impact the model performance. In
this paper, we introduce a novel method, named Context Pooling, to enhance
GNN-based models' efficacy for link predictions in KGs. To our best of
knowledge, Context Pooling is the first methodology that applies graph pooling
in KGs. Additionally, Context Pooling is first-of-its-kind to enable the
generation of query-specific graphs for inductive settings, where testing
entities are unseen during training. Specifically, we devise two metrics,
namely neighborhood precision and neighborhood recall, to assess the neighbors'
logical relevance regarding the given queries, thereby enabling the subsequent
comprehensive identification of only the logically relevant neighbors for link
prediction. Our method is generic and assessed by being applied to two
state-of-the-art (SOTA) models on three public transductive and inductive
datasets, achieving SOTA performance in 42 out of 48 settings.

</details>


### [39] [Enhancing Vaccine Safety Surveillance: Extracting Vaccine Mentions from Emergency Department Triage Notes Using Fine-Tuned Large Language Models](https://arxiv.org/abs/2507.07599)
*Sedigh Khademi,Jim Black,Christopher Palmer,Muhammad Javed,Hazel Clothier,Jim Buttery,Gerardo Luis Dimaguila*

Main category: cs.AI

TL;DR: 研究评估了微调的Llama 3.2模型，用于从急诊分诊记录中提取疫苗相关信息，以支持近实时疫苗安全监测。微调模型在提取疫苗名称的准确性上优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 支持高效的疫苗安全监测和早期发现免疫后不良事件。

Method: 使用提示工程创建标注数据集，比较提示工程模型、微调模型和基于规则的方法。

Result: 微调的Llama 3B参数模型在提取疫苗名称的准确性上表现最佳，量化技术使其在资源受限环境中高效部署。

Conclusion: 大语言模型在自动化数据提取方面具有潜力，可支持疫苗安全监测。

Abstract: This study evaluates fine-tuned Llama 3.2 models for extracting
vaccine-related information from emergency department triage notes to support
near real-time vaccine safety surveillance. Prompt engineering was used to
initially create a labeled dataset, which was then confirmed by human
annotators. The performance of prompt-engineered models, fine-tuned models, and
a rule-based approach was compared. The fine-tuned Llama 3 billion parameter
model outperformed other models in its accuracy of extracting vaccine names.
Model quantization enabled efficient deployment in resource-constrained
environments. Findings demonstrate the potential of large language models in
automating data extraction from emergency department notes, supporting
efficient vaccine safety surveillance and early detection of emerging adverse
events following immunization issues.

</details>


### [40] [Towards conservative inference in credal networks using belief functions: the case of credal chains](https://arxiv.org/abs/2507.07619)
*Marco Sangalli,Thomas Krak,Cassio de Campos*

Main category: cs.AI

TL;DR: 本文提出了一种基于Dempster-Shafer理论的新框架，用于在信用网络（特别是链式结构）中传播不确定性，通过置信和似然函数高效生成保守区间。


<details>
  <summary>Details</summary>
Motivation: 探索如何在信用网络中利用Dempster-Shafer理论进行信念推断，以结合计算速度和鲁棒的表示不确定性。

Method: 提出了一种新框架，用于在链式信用网络中传播不确定性，并通过置信和似然函数生成保守区间。

Result: 数值结果展示了信念推断在该框架中的优势和局限性，为链式及一般信用网络的实践应用提供了见解。

Conclusion: 该研究为信用网络中的信念推断提供了理论基础和实践指导，并对比了信念推断与经典敏感性分析的差异。

Abstract: This paper explores belief inference in credal networks using Dempster-Shafer
theory. By building on previous work, we propose a novel framework for
propagating uncertainty through a subclass of credal networks, namely chains.
The proposed approach efficiently yields conservative intervals through belief
and plausibility functions, combining computational speed with robust
uncertainty representation. Key contributions include formalizing belief-based
inference methods and comparing belief-based inference against classical
sensitivity analysis. Numerical results highlight the advantages and
limitations of applying belief inference within this framework, providing
insights into its practical utility for chains and for credal networks in
general.

</details>


### [41] [PlanQA: A Benchmark for Spatial Reasoning in LLMs using Structured Representations](https://arxiv.org/abs/2507.07644)
*Fedor Rodionov,Abdelrahman Eldesokey,Michael Birsak,John Femiani,Bernard Ghanem,Peter Wonka*

Main category: cs.AI

TL;DR: PlanQA是一个用于评估大型语言模型（LLMs）在几何和空间推理能力的诊断基准，基于室内场景的结构化表示，揭示了LLMs在真实世界布局推理中的盲点。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在几何和空间推理方面存在不足，尤其是在模拟物理约束、保持空间一致性和布局扰动下的泛化能力上。PlanQA旨在填补这一空白，推动语言模型在空间推理方面的进步。

Method: PlanQA使用符号化格式（如JSON、XML）编码室内场景，设计多样化问题类型，测试度量、拓扑推理及室内设计约束（如可达性、平衡性）。

Result: 实验表明，尽管LLMs在简单查询中表现良好，但在模拟物理约束、空间一致性和布局扰动泛化方面存在显著不足。

Conclusion: PlanQA揭示了LLMs在真实世界布局推理中的盲点，为未来研究提供了方向，希望推动语言模型在空间推理能力上的改进。

Abstract: We introduce PlanQA, a diagnostic benchmark for evaluating geometric and
spatial reasoning in large-language models (LLMs). PlanQA is grounded in
structured representations of indoor scenes, such as kitchens, living rooms,
and bedrooms, encoded in a symbolic format (e.g., JSON, XML layouts). The
benchmark includes diverse question types that test not only metric and
topological reasoning (e.g., distance, visibility, shortest paths) but also
interior design constraints such as affordance, clearance, balance, and
usability. Our results across a variety of frontier open-source and commercial
LLMs show that while models may succeed in shallow queries, they often fail to
simulate physical constraints, preserve spatial coherence, or generalize under
layout perturbation. PlanQA uncovers a clear blind spot in today's LLMs: they
do not consistently reason about real-world layouts. We hope that this
benchmark inspires new work on language models that can accurately infer and
manipulate spatial and geometric properties in practical settings.

</details>


### [42] [Stable Preference Optimization for LLMs: A Bilevel Approach Beyond Direct Preference Optimization](https://arxiv.org/abs/2507.07723)
*Chengtao Jian,Kai Yang,Ye Ouyang,Xiaozhou Ye*

Main category: cs.AI

TL;DR: 本文分析了直接偏好优化（DPO）的理论局限性，并提出了一种双层优化框架——稳定偏好优化，以改进模型对齐的稳定性和一致性。


<details>
  <summary>Details</summary>
Motivation: DPO在语言模型对齐中表现出高效性，但其理论性质和内在局限性尚未充分研究。本文旨在揭示DPO的动态特性及其潜在问题。

Method: 通过概率演化视角分析DPO的敏感性及概率分配问题，并提出一种结合监督微调和增强DPO目标的双层优化框架。

Result: 实验表明，该方法在推理和摘要任务中表现优于标准DPO，提高了推理准确性并更好地对齐输出分布。

Conclusion: 稳定偏好优化为偏好对齐目标的设计提供了新思路，推动了更可靠、可解释的语言模型对齐方法的发展。

Abstract: Direct Preference Optimization (DPO) has emerged as a popular and efficient
alternative to reward modeling and reinforcement learning for aligning language
models with human preferences. Despite its empirical success, the theoretical
properties and intrinsic limitations of DPO remain underexplored. In this work,
we first present a comprehensive analysis of DPO's dynamics from a probability
evolution perspective. Our analysis reveals that DPO is highly sensitive to
initialization. It also tends to misallocate probability mass, which can
inadvertently shift probability toward irrelevant or undesired responses. This
misallocation may unintentionally reinforce model bias, thereby compromising
both the stability of model alignment and the consistency with intended
preferences. Motivated by these theoretical findings, we propose a
theoretically grounded bilevel optimization framework that tightly integrate
supervised fine-tuning with an enhanced DPO objective a.k.a. stable preference
optimization. Our approach introduces a principled regularization scheme to
explicitly encourage absolute probability improvement for preferred outputs,
while maintaining stable optimization dynamics. Experiments on challenging
reasoning and summarization benchmarks elucidate that our method consistently
improves reasoning accuracy and better aligns output distributions with
intended preferences, outperforming standard DPO. Stable preference
optimization provides new insights into the design of preference-based
alignment objectives and opens up new avenues towards more reliable and
interpretable language model alignment.

</details>


### [43] [Identification of Violin Reduction via Contour Lines Classification](https://arxiv.org/abs/2507.07743)
*Philémon Beghin,Anne-Emmanuelle Ceulemans,François Glineur*

Main category: cs.AI

TL;DR: 本文提出了一种基于轮廓线分类小提琴是否为缩小版的方法，通过几何特征分析区分缩小与非缩小乐器。


<details>
  <summary>Details</summary>
Motivation: 研究小提琴制作中的尺寸标准化对乐器轮廓线的影响，填补专家观察与定量研究之间的空白。

Method: 使用摄影测量获取25把小提琴的3D几何网格，提取10-20条轮廓线，拟合抛物线曲线并计算特征参数，应用分类方法评估几何预测能力。

Result: 研究发现几何特征可以一定程度上区分缩小与非缩小乐器，其中曲线开口参数beta最具预测性。

Conclusion: 几何分析为小提琴尺寸标准化研究提供了定量方法，但需注意乐器改造的多样性带来的分类挑战。

Abstract: The first violins appeared in late 16th-century Italy. Over the next 200
years, they spread across Europe and luthiers of various royal courts, eager to
experiment with new techniques, created a highly diverse family of instruments.
Around 1750, size standards were introduced to unify violin making for
orchestras and conservatories. Instruments that fell between two standards were
then reduced to a smaller size by luthiers. These reductions have an impact on
several characteristics of violins, in particular on the contour lines, i.e.
lines of constant altitude, which look more like a U for non reduced
instruments and a V for reduced ones. While such differences are observed by
experts, they have not been studied quantitatively.
  This paper presents a method for classifying violins as reduced or
non-reduced based on their contour lines. We study a corpus of 25 instruments
whose 3D geometric meshes were acquired via photogrammetry. For each
instrument, we extract 10-20 contour lines regularly spaced every millimetre.
Each line is fitted with a parabola-like curve (with an equation of the type y
= alpha*abs(x)**beta) depending on two parameters, describing how open (beta)
and how vertically stretched (alpha) the curve is. We compute additional
features from those parameters, using regressions and counting how many values
fall under some threshold. We also deal with outliers and non equal numbers of
levels, and eventually obtain a numerical profile for each instrument.
  We then apply classification methods to assess whether geometry alone can
predict size reduction. We find that distinguishing between reduced and non
reduced instruments is feasible to some degree, taking into account that a
whole spectrum of more or less transformed violins exists, for which it is more
difficult to quantify the reduction. We also find the opening parameter beta to
be the most predictive.

</details>


### [44] [Measuring AI Alignment with Human Flourishing](https://arxiv.org/abs/2507.07787)
*Elizabeth Hilliard,Akshaya Jagadeesh,Alex Cook,Steele Billings,Nicholas Skytland,Alicia Llewellyn,Jackson Paull,Nathan Paull,Nolan Kurylo,Keatra Nesbitt,Robert Gruenewald,Anthony Jantzi,Omar Chavez*

Main category: cs.AI

TL;DR: FAI Benchmark是一个新的评估框架，从七个维度评估AI对人类繁荣的贡献，包括品格与美德、亲密关系、幸福与生活满意度等。通过1229个问题和方法论，评估28个领先语言模型，发现没有模型在所有维度上表现良好。


<details>
  <summary>Details</summary>
Motivation: 传统AI评估主要关注技术能力或避免危害，而FAI Benchmark旨在衡量AI对人类全面繁荣的贡献。

Method: 使用1229个客观和主观问题，结合专业LLM评估和几何平均评分，评估AI在七个维度的表现。

Result: 测试28个模型，最高得分72/100，但无模型在所有维度（尤其是信仰与灵性、品格与美德、意义与目的）上表现良好。

Conclusion: FAI Benchmark为开发支持人类繁荣的AI提供了框架，对AI发展、伦理和评估有重要意义。

Abstract: This paper introduces the Flourishing AI Benchmark (FAI Benchmark), a novel
evaluation framework that assesses AI alignment with human flourishing across
seven dimensions: Character and Virtue, Close Social Relationships, Happiness
and Life Satisfaction, Meaning and Purpose, Mental and Physical Health,
Financial and Material Stability, and Faith and Spirituality. Unlike
traditional benchmarks that focus on technical capabilities or harm prevention,
the FAI Benchmark measures AI performance on how effectively models contribute
to the flourishing of a person across these dimensions. The benchmark evaluates
how effectively LLM AI systems align with current research models of holistic
human well-being through a comprehensive methodology that incorporates 1,229
objective and subjective questions. Using specialized judge Large Language
Models (LLMs) and cross-dimensional evaluation, the FAI Benchmark employs
geometric mean scoring to ensure balanced performance across all flourishing
dimensions. Initial testing of 28 leading language models reveals that while
some models approach holistic alignment (with the highest-scoring models
achieving 72/100), none are acceptably aligned across all dimensions,
particularly in Faith and Spirituality, Character and Virtue, and Meaning and
Purpose. This research establishes a framework for developing AI systems that
actively support human flourishing rather than merely avoiding harm, offering
significant implications for AI development, ethics, and evaluation.

</details>


### [45] [MoSE: Skill-by-Skill Mixture-of-Expert Learning for Autonomous Driving](https://arxiv.org/abs/2507.07818)
*Lu Xu,Jiaqian Yu,Xiongfeng Peng,Yiwei Chen,Weiming Li,Jaewook Yoo,Sunghyun Chunag,Dongwook Lee,Daehyun Ji,Chao Zhang*

Main category: cs.AI

TL;DR: 本文提出了一种名为MoSE的技能导向混合专家模型，通过模仿人类驾驶员的学习和推理过程，实现了高效且性能优越的自动驾驶系统。


<details>
  <summary>Details</summary>
Motivation: 现有混合专家模型（MoE）需要大量训练数据和复杂优化，而人类驾驶员的学习过程更具效率和针对性，因此提出技能导向的MoE模型。

Method: 提出技能导向路由机制，定义和标注特定技能，构建分层技能数据集，并通过单次前向过程整合辅助任务。

Result: 模型在CODA AD任务中表现优于多个8B+参数模型，激活参数量减少至少62.5%。

Conclusion: MoSE通过技能导向和分层推理，显著提升了自动驾驶系统的性能和效率。

Abstract: Recent studies show large language models (LLMs) and vision language models
(VLMs) trained using web-scale data can empower end-to-end autonomous driving
systems for a better generalization and interpretation. Specifically, by
dynamically routing inputs to specialized subsets of parameters, the
Mixture-of-Experts (MoE) technique enables general LLMs or VLMs to achieve
substantial performance improvements while maintaining computational
efficiency. However, general MoE models usually demands extensive training data
and complex optimization. In this work, inspired by the learning process of
human drivers, we propose a skill-oriented MoE, called MoSE, which mimics human
drivers' learning process and reasoning process, skill-by-skill and
step-by-step. We propose a skill-oriented routing mechanism that begins with
defining and annotating specific skills, enabling experts to identify the
necessary driving competencies for various scenarios and reasoning tasks,
thereby facilitating skill-by-skill learning. Further align the driving process
to multi-step planning in human reasoning and end-to-end driving models, we
build a hierarchical skill dataset and pretrain the router to encourage the
model to think step-by-step. Unlike multi-round dialogs, MoSE integrates
valuable auxiliary tasks (e.g.\ description, reasoning, planning) in one single
forward process without introducing any extra computational cost. With less
than 3B sparsely activated parameters, our model outperforms several 8B+
parameters on CODA AD corner case reasoning task. Compared to existing methods
based on open-source models and data, our approach achieves state-of-the-art
performance with significantly reduced activated model size (at least by
$62.5\%$) with a single-turn conversation.

</details>


### [46] [AI Should Sense Better, Not Just Scale Bigger: Adaptive Sensing as a Paradigm Shift](https://arxiv.org/abs/2507.07820)
*Eunsu Baek,Keondo Park,Jeonggil Ko,Min-hwan Oh,Taesik Gong,Hyung-Sin Kim*

Main category: cs.AI

TL;DR: 论文提出自适应感知作为AI可持续发展的新范式，通过动态调整传感器参数提升效率，减少对大规模模型和数据的依赖。


<details>
  <summary>Details</summary>
Motivation: 当前AI依赖大规模模型和数据集，带来环境、经济和伦理成本，限制了可持续性和公平性。

Method: 借鉴生物感官系统，提出自适应感知方法，动态调整传感器参数（如曝光、灵敏度等）。

Result: 实证研究表明，自适应感知能让小模型超越大模型性能。

Conclusion: 论文呼吁将自适应感知整合到实际应用中，并解决技术和伦理挑战，推动可持续、鲁棒和公平的AI发展。

Abstract: Current AI advances largely rely on scaling neural models and expanding
training datasets to achieve generalization and robustness. Despite notable
successes, this paradigm incurs significant environmental, economic, and
ethical costs, limiting sustainability and equitable access. Inspired by
biological sensory systems, where adaptation occurs dynamically at the input
(e.g., adjusting pupil size, refocusing vision)--we advocate for adaptive
sensing as a necessary and foundational shift. Adaptive sensing proactively
modulates sensor parameters (e.g., exposure, sensitivity, multimodal
configurations) at the input level, significantly mitigating covariate shifts
and improving efficiency. Empirical evidence from recent studies demonstrates
that adaptive sensing enables small models (e.g., EfficientNet-B0) to surpass
substantially larger models (e.g., OpenCLIP-H) trained with significantly more
data and compute. We (i) outline a roadmap for broadly integrating adaptive
sensing into real-world applications spanning humanoid, healthcare, autonomous
systems, agriculture, and environmental monitoring, (ii) critically assess
technical and ethical integration challenges, and (iii) propose targeted
research directions, such as standardized benchmarks, real-time adaptive
algorithms, multimodal integration, and privacy-preserving methods.
Collectively, these efforts aim to transition the AI community toward
sustainable, robust, and equitable artificial intelligence systems.

</details>


### [47] [Searching for actual causes: Approximate algorithms with adjustable precision](https://arxiv.org/abs/2507.07857)
*Samuel Reyd,Ada Diaconescu,Jean-Louis Dessalles*

Main category: cs.AI

TL;DR: 论文提出了一种多项式复杂度的算法，用于识别实际原因，适用于非布尔、黑盒和随机系统，并可调整精度和全面性。


<details>
  <summary>Details</summary>
Motivation: 当前可解释人工智能（XAI）和因果性文献未能满足非专家用户对解释的需求，即识别实际原因。这是一个开放性问题，且现有解决方案较少。

Method: 提出了一组算法，具有多项式复杂度和可调整的精度与全面性，适用于多种系统类别。

Result: 实验表明，算法能识别现有方法无法处理的系统类别（如非布尔、黑盒和随机系统），且可通过增加计算时间提高精度和全面性。

Conclusion: 该算法为解决实际原因识别问题提供了实用且灵活的解决方案。

Abstract: Causality has gained popularity in recent years. It has helped improve the
performance, reliability, and interpretability of machine learning models.
However, recent literature on explainable artificial intelligence (XAI) has
faced criticism. The classical XAI and causality literature focuses on
understanding which factors contribute to which consequences. While such
knowledge is valuable for researchers and engineers, it is not what non-expert
users expect as explanations. Instead, these users often await facts that cause
the target consequences, i.e., actual causes. Formalizing this notion is still
an open problem. Additionally, identifying actual causes is reportedly an
NP-complete problem, and there are too few practical solutions to approximate
formal definitions. We propose a set of algorithms to identify actual causes
with a polynomial complexity and an adjustable level of precision and
exhaustiveness. Our experiments indicate that the algorithms (1) identify
causes for different categories of systems that are not handled by existing
approaches (i.e., non-boolean, black-box, and stochastic systems), (2) can be
adjusted to gain more precision and exhaustiveness with more computation time.

</details>


### [48] [An Integrated Framework of Prompt Engineering and Multidimensional Knowledge Graphs for Legal Dispute Analysis](https://arxiv.org/abs/2507.07893)
*Mingda Zhang,Na Zhao,Jianglong Qing,Qing xu,Kaiwen Pan,Ting luo*

Main category: cs.AI

TL;DR: 论文提出了一种结合提示工程和多维知识图谱的增强框架，以解决大语言模型在法律纠纷分析中的局限性，显著提升了法律决策的性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在法律纠纷分析中存在法律知识表示不足、概念理解有限和推理缺陷等问题，需要一种改进方法。

Method: 提出三阶段分层提示结构和三层知识图谱架构，结合四种互补方法实现精确法律概念检索。

Result: 实验结果显示，该框架在法律纠纷分析中性能显著提升，能够准确分析复杂案例的法律应用。

Conclusion: 该研究为智能法律辅助系统的实现提供了新颖的技术途径。

Abstract: The rapid development of artificial intelligence has positioned large
language models as fundamental components of intelligent legal systems.
However, these models face significant limitations in legal dispute analysis,
including insufficient legal knowledge representation, limited concept
understanding, and reasoning deficiencies. This research proposes an enhanced
framework integrating prompt engineering with multidimensional knowledge
graphs. The framework introduces a three-stage hierarchical prompt structure
comprising task definition, knowledge background, and reasoning guidance,
supplemented by legal-specific reasoning templates and dynamic optimization
mechanisms. A three-layer knowledge graph architecture is constructed with
legal classification ontology, representation, and instance layers. Four
complementary methods enable precise legal concept retrieval: direct legal norm
code matching, domain-specific semantic vector similarity, ontology-based path
reasoning, and specialized lexical segmentation. These components integrate
with web search technology to establish a knowledge-enhanced framework for
legal decision-making. Experimental results demonstrate significant performance
improvements in legal dispute analysis, enabling accurate legal application
analysis for complex cases while exhibiting nuanced understanding of judicial
decision-making logic, providing a novel technical approach for implementing
intelligent legal assistance systems.

</details>


### [49] [Meek Models Shall Inherit the Earth](https://arxiv.org/abs/2507.07931)
*Hans Gundlach,Jayson Lynch,Neil Thompson*

Main category: cs.AI

TL;DR: 论文认为，尽管AI模型的算力投入不断增加，但边际收益递减将导致模型性能趋同，小型模型（算力有限）将接近顶级模型的性能。


<details>
  <summary>Details</summary>
Motivation: 探讨AI模型性能不平等问题，并分析算力投入的边际收益递减如何导致模型性能趋同。

Method: 开发模型分析固定分布下算力投入的边际收益递减，结合实证数据和理论模型验证。

Result: 算力投入的边际收益递减显著，小型模型性能将接近顶级模型。

Conclusion: AI战略和政策需重新审视，以适应小型模型性能提升的趋势。

Abstract: The past decade has seen incredible scaling of AI systems by a few companies,
leading to inequality in AI model performance. This paper argues that, contrary
to prevailing intuition, the diminishing returns to compute scaling will lead
to a convergence of AI model capabilities. In other words, meek models (those
with limited computation budget) shall inherit the earth, approaching the
performance level of the best models overall. We develop a model illustrating
that under a fixed-distribution next-token objective, the marginal capability
returns to raw compute shrink substantially. Given current scaling practices,
we argue that these diminishing returns are strong enough that even companies
that can scale their models exponentially faster than other organizations will
eventually have little advantage in capabilities. As part of our argument, we
give several reasons that proxies like training loss differences capture
important capability measures using evidence from benchmark data and
theoretical performance models. In addition, we analyze empirical data on the
capability difference of AI models over time. Finally, in light of the
increasing ability of meek models, we argue that AI strategy and policy require
reexamination, and we outline the areas this shift will affect.

</details>


### [50] [Working with AI: Measuring the Occupational Implications of Generative AI](https://arxiv.org/abs/2507.07935)
*Kiran Tomlinson,Sonia Jaffe,Will Wang,Scott Counts,Siddharth Suri*

Main category: cs.AI

TL;DR: 论文研究了生成式AI对经济的影响，通过分析用户与AI的互动数据，评估了AI在不同职业中的适用性。


<details>
  <summary>Details</summary>
Motivation: 理解生成式AI对经济的广泛影响是社会的关键问题。

Method: 分析了20万条用户与微软Bing Copilot的匿名对话数据，分类了AI辅助的活动，并结合职业数据计算了AI适用性分数。

Result: AI最常用于信息收集和写作，适用性最高的职业包括计算机、数学、行政支持和销售等知识型工作。

Conclusion: AI对知识型职业的影响显著，且适用性与工资和教育水平相关。

Abstract: Given the rapid adoption of generative AI and its potential to impact a wide
range of tasks, understanding the effects of AI on the economy is one of
society's most important questions. In this work, we take a step toward that
goal by analyzing the work activities people do with AI, how successfully and
broadly those activities are done, and combine that with data on what
occupations do those activities. We analyze a dataset of 200k anonymized and
privacy-scrubbed conversations between users and Microsoft Bing Copilot, a
publicly available generative AI system. We find the most common work
activities people seek AI assistance for involve gathering information and
writing, while the most common activities that AI itself is performing are
providing information and assistance, writing, teaching, and advising.
Combining these activity classifications with measurements of task success and
scope of impact, we compute an AI applicability score for each occupation. We
find the highest AI applicability scores for knowledge work occupation groups
such as computer and mathematical, and office and administrative support, as
well as occupations such as sales whose work activities involve providing and
communicating information. Additionally, we characterize the types of work
activities performed most successfully, how wage and education correlate with
AI applicability, and how real-world usage compares to predictions of
occupational AI impact.

</details>
