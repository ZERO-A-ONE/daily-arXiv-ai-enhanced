<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 8]
- [cs.AI](#cs.AI) [Total: 19]
- [cs.SE](#cs.SE) [Total: 7]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Supporting Students in Navigating LLM-Generated Insecure Code](https://arxiv.org/abs/2511.20878)
*Jaehwan Park,Kyungchan Lim,Seonhye Park,Doowon Kim*

Main category: cs.CR

TL;DR: Bifröst是一个教育框架，通过在AI辅助开发中模拟不安全代码生成来培养安全意识和批判性评估能力。


<details>
  <summary>Details</summary>
Motivation: AI和大型语言模型在软件开发中的普及带来了安全风险，当前教育方法忽视这些风险，导致学生无法识别和缓解AI辅助工作流中的安全问题。

Method: Bifröst框架包含：(1) Visual Studio Code扩展模拟真实环境，(2) 对抗性配置的LLMs生成不安全代码，(3) 反馈系统突出显示漏洞。

Result: 课堂部署(n=61)显示学生对不安全代码的脆弱性，干预后调查(n=21)表明对LLM输出的怀疑态度增加。

Conclusion: Bifröst通过沉浸式任务和针对性安全分析，成功培养了学生在AI辅助开发中的安全意识和批判性评估技能。

Abstract: The advent of Artificial Intelligence (AI), particularly large language models (LLMs), has revolutionized software development by enabling developers to specify tasks in natural language and receive corresponding code, boosting productivity. However, this shift also introduces security risks, as LLMs may generate insecure code that can be exploited by adversaries. Current educational approaches emphasize efficiency while overlooking these risks, leaving students underprepared to identify and mitigate security issues in AI-assisted workflows.
  To address this gap, we present Bifröst, an educational framework that cultivates security awareness in AI-augmented development. Bifröst integrates (1) a Visual Studio Code extension simulating realistic environments, (2) adversarially configured LLMs that generate insecure code, and (3) a feedback system highlighting vulnerabilities. By immersing students in tasks with compromised LLMs and providing targeted security analysis, Bifröst cultivates critical evaluation skills; classroom deployments (n=61) show vulnerability to insecure code, while a post-intervention survey (n=21) indicates increased skepticism toward LLM outputs.

</details>


### [2] [A Taxonomy of Pix Fraud in Brazil: Attack Methodologies, AI-Driven Amplification, and Defensive Strategies](https://arxiv.org/abs/2511.20902)
*Glener Lanes Pizzolato,Brenda Medeiros Lopes,Claudio Schepke,Diego Kreutz*

Main category: cs.CR

TL;DR: 本文回顾了针对巴西央行2020年推出的即时支付系统Pix的攻击方法，识别并分类了影响用户和金融机构的主要欺诈类型，展示了这些技术从纯社会工程到人机混合策略的演变。


<details>
  <summary>Details</summary>
Motivation: 研究旨在识别Pix支付系统中的主要欺诈类型，揭示攻击技术的演变和日益复杂化趋势，为安全防护提供依据。

Method: 采用结构化文献综述与银行业专业人士的探索性访谈相结合的方法。

Result: 结果显示欺诈方案已从纯社会工程方法演变为整合人为操纵与技术利用的混合策略。

Conclusion: 安全措施必须与攻击方法复杂性的增长同步推进，特别强调适应性防御和持续的用户意识教育。

Abstract: This work presents a review of attack methodologies targeting Pix, the instant payment system launched by the Central Bank of Brazil in 2020. The study aims to identify and classify the main types of fraud affecting users and financial institutions, highlighting the evolution and increasing sophistication of these techniques. The methodology combines a structured literature review with exploratory interviews conducted with professionals from the banking sector. The results show that fraud schemes have evolved from purely social engineering approaches to hybrid strategies that integrate human manipulation with technical exploitation. The study concludes that security measures must advance at the same pace as the growing complexity of attack methodologies, with particular emphasis on adaptive defenses and continuous user awareness.

</details>


### [3] [Readout-Side Bypass for Residual Hybrid Quantum-Classical Models](https://arxiv.org/abs/2511.20922)
*Guilin Zhang,Wulan Guo,Ziqi Tan,Hongyang He,Hailong Jiang*

Main category: cs.CR

TL;DR: 提出一种轻量级残差混合架构，将量子特征与原始输入在分类前连接，绕过测量瓶颈，在保持低量子复杂度的同时提升性能。


<details>
  <summary>Details</summary>
Motivation: 量子机器学习面临测量瓶颈问题 - 狭窄的量子-经典读出限制了性能并放大了隐私风险。

Method: 采用残差混合架构，在量子-经典接口处将量子特征与原始输入进行连接，避免增加量子复杂度。

Result: 在集中式和联邦式设置中均优于纯量子模型和先前混合模型，相比量子基线准确率提升高达+55%，同时保持低通信成本和增强的隐私鲁棒性。

Conclusion: 该方法为在隐私敏感、资源受限的环境中集成量子模型提供了实用的近期路径。

Abstract: Quantum machine learning (QML) promises compact and expressive representations, but suffers from the measurement bottleneck - a narrow quantum-to-classical readout that limits performance and amplifies privacy risk. We propose a lightweight residual hybrid architecture that concatenates quantum features with raw inputs before classification, bypassing the bottleneck without increasing quantum complexity. Experiments show our model outperforms pure quantum and prior hybrid models in both centralized and federated settings. It achieves up to +55% accuracy improvement over quantum baselines, while retaining low communication cost and enhanced privacy robustness. Ablation studies confirm the effectiveness of the residual connection at the quantum-classical interface. Our method offers a practical, near-term pathway for integrating quantum models into privacy-sensitive, resource-constrained settings like federated edge learning.

</details>


### [4] [Road Network-Aware Personalized Trajectory Protection with Differential Privacy under Spatiotemporal Correlations](https://arxiv.org/abs/2511.21020)
*Minghui Min,Jiahui Liu,Mingge Cao,Shiyin Li,Hongliang Zhang,Miao Pan,Zhu Han*

Main category: cs.CR

TL;DR: 提出个性化轨迹隐私保护机制PTPPM，通过结合地理不可区分性和失真隐私，让用户自定义隐私偏好，在保护位置隐私的同时维持服务质量。


<details>
  <summary>Details</summary>
Motivation: 基于位置的服务虽然便利但存在隐私风险，攻击者可通过时空相关性推断敏感信息。由于用户对位置数据的敏感度因停留时间、访问频率等因素而异，需要个性化隐私保护。

Method: 结合地理不可区分性和失真隐私，构建保护位置集；提出个性化隐私预算分配算法PPBA评估位置敏感度；使用Permute-and-Flip机制生成扰动位置。

Result: 仿真结果表明该机制优于现有基准方法，在维持用户服务质量要求的同时提供更优的隐私保护。

Conclusion: PTPPM机制能有效保护用户轨迹隐私，通过个性化隐私预算分配和位置扰动策略，平衡了隐私保护与服务质量的需求。

Abstract: Location-Based Services (LBSs) offer significant convenience to mobile users but pose significant privacy risks, as attackers can infer sensitive personal information through spatiotemporal correlations in user trajectories. Since users' sensitivity to location data varies based on factors such as stay duration, access frequency, and semantic sensitivity, implementing personalized privacy protection is imperative. This paper proposes a Personalized Trajectory Privacy Protection Mechanism (PTPPM) to address these challenges. Our approach begins by modeling an attacker's knowledge of a user's trajectory spatiotemporal correlations, which enables the attacker to identify possible location sets and disregard low-probability location sets. To combat this, we integrate geo-indistinguishability with distortion privacy, allowing users to customize their privacy preferences through a configurable privacy budget and expected inference error bound. This approach provides the theoretical framework for constructing a Protection Location Set (PLS) that obscures users' actual locations. Additionally, we introduce a Personalized Privacy Budget Allocation Algorithm (PPBA), which assesses the sensitivity of locations based on trajectory data and allocates privacy budgets accordingly. This algorithm considers factors such as location semantics and road network constraints. Furthermore, we propose a Permute-and-Flip mechanism that generates perturbed locations while minimizing perturbation distance, thus balancing privacy protection and Quality of Service (QoS). Simulation results demonstrate that our mechanism outperforms existing benchmarks, offering superior privacy protection while maintaining user QoS requirements.

</details>


### [5] [CAHS-Attack: CLIP-Aware Heuristic Search Attack Method for Stable Diffusion](https://arxiv.org/abs/2511.21180)
*Shuhan Xia,Jing Dai,Hui Ouyang,Yadong Shang,Dongxiao Zhao,Peipei Li*

Main category: cs.CR

TL;DR: CAHS-Attack是一种针对扩散模型的对抗性攻击方法，通过结合蒙特卡洛树搜索和约束遗传算法来优化对抗性后缀，在无需白盒访问的情况下实现高效攻击。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖模型梯度或手动提示工程，在现实部署中不可行，需要开发更有效的攻击方法来揭示扩散模型的脆弱性。

Method: 结合蒙特卡洛树搜索进行细粒度后缀优化，使用约束遗传算法预选高潜力对抗性提示作为根节点，并在每次模拟中保留最具语义破坏性的结果进行局部搜索。

Result: 在长短不同语义的提示上均达到最先进的攻击性能，发现SD模型的脆弱性源于其基于CLIP的文本编码器的固有漏洞。

Conclusion: CAHS-Attack有效揭示了扩散模型的对抗性脆弱性，表明当前文本到图像生成管道存在根本性的安全风险。

Abstract: Diffusion models exhibit notable fragility when faced with adversarial prompts, and strengthening attack capabilities is crucial for uncovering such vulnerabilities and building more robust generative systems. Existing works often rely on white-box access to model gradients or hand-crafted prompt engineering, which is infeasible in real-world deployments due to restricted access or poor attack effect. In this paper, we propose CAHS-Attack , a CLIP-Aware Heuristic Search attack method. CAHS-Attack integrates Monte Carlo Tree Search (MCTS) to perform fine-grained suffix optimization, leveraging a constrained genetic algorithm to preselect high-potential adversarial prompts as root nodes, and retaining the most semantically disruptive outcome at each simulation rollout for efficient local search. Extensive experiments demonstrate that our method achieves state-of-the-art attack performance across both short and long prompts of varying semantics. Furthermore, we find that the fragility of SD models can be attributed to the inherent vulnerability of their CLIP-based text encoders, suggesting a fundamental security risk in current text-to-image pipelines.

</details>


### [6] [AuthenLoRA: Entangling Stylization with Imperceptible Watermarks for Copyright-Secure LoRA Adapters](https://arxiv.org/abs/2511.21216)
*Fangming Shi,Li Li,Kejiang Chen,Guorui Feng,Xinpeng Zhang*

Main category: cs.CR

TL;DR: AuthenLoRA是一个统一的LoRA水印框架，通过在LoRA训练过程中嵌入不可感知的追踪水印来解决扩散模型定制中的版权保护问题，同时保持风格化质量。


<details>
  <summary>Details</summary>
Motivation: 现有水印技术要么针对基础模型，要么验证LoRA模块本身，但无法将水印传播到生成图像中，存在可追溯性空白。此外，为基础模型设计的可追溯性水印与风格化不紧密耦合，常导致视觉退化或高误检率。

Method: 采用双目标优化策略，同时学习目标风格分布和水印引起的分布偏移；设计扩展的LoRA架构以增强多尺度适应；引入零消息正则化机制降低水印验证的误检率。

Result: 实验表明AuthenLoRA实现了高保真风格化、鲁棒的水印传播，与现有方法相比显著降低了误检率。

Conclusion: AuthenLoRA框架有效解决了LoRA定制扩散模型中的版权保护问题，实现了水印与风格化的紧密耦合，同时保持生成质量。

Abstract: Low-Rank Adaptation (LoRA) offers an efficient paradigm for customizing diffusion models, but its ease of redistribution raises concerns over unauthorized use and the generation of untraceable content. Existing watermarking techniques either target base models or verify LoRA modules themselves, yet they fail to propagate watermarks to generated images, leaving a critical gap in traceability. Moreover, traceability watermarking designed for base models is not tightly coupled with stylization and often introduces visual degradation or high false-positive detection rates. To address these limitations, we propose AuthenLoRA, a unified watermarking framework that embeds imperceptible, traceable watermarks directly into the LoRA training process while preserving stylization quality. AuthenLoRA employs a dual-objective optimization strategy that jointly learns the target style distribution and the watermark-induced distribution shift, ensuring that any image generated with the watermarked LoRA reliably carries the watermark. We further design an expanded LoRA architecture for enhanced multi-scale adaptation and introduce a zero-message regularization mechanism that substantially reduces false positives during watermark verification. Extensive experiments demonstrate that AuthenLoRA achieves high-fidelity stylization, robust watermark propagation, and significantly lower false-positive rates compared with existing approaches. Open-source implementation is available at: https://github.com/ShiFangming0823/AuthenLoRA

</details>


### [7] [Empirical Assessment of the Code Comprehension Effort Needed to Attack Programs Protected with Obfuscation](https://arxiv.org/abs/2511.21301)
*Leonardo Regano,Daniele Canavese,Cataldo Basile,Marco Torchiano*

Main category: cs.CR

TL;DR: 该论文通过受控实验评估软件混淆技术的有效性，研究混淆技术在延迟代码理解方面的效果，并分析复杂度指标是否能预测混淆保护对代码理解任务成功率和持续时间的影响。


<details>
  <summary>Details</summary>
Motivation: 软件保护的有效性评估对于选择最有效的资产保护方法至关重要。虽然混淆技术被广泛采用，但其有效性尚未得到充分探索和评估，需要实验证据来填补这一空白。

Method: 采用受控实验方法，让硕士生对经过混淆处理的应用程序执行代码理解任务，评估混淆技术在延迟代码理解方面的效果，并分析复杂度指标与攻击成功可能性的相关性。

Result: 研究首次评估了在单一受保护代码上叠加多种混淆技术的效果，提供了客观代码指标与攻击成功可能性之间相关性的实验证据，弥合了客观和主观评估方法之间的差距。

Conclusion: 论文强调了需要进一步分析的重要方面，并为后续实验开辟了新途径，为软件保护的有效性评估提供了实证基础。

Abstract: Evaluating the effectiveness of software protection is crucial for selecting the most effective methods to safeguard assets within software applications. Obfuscation involves techniques that deliberately modify software to make it more challenging to understand and reverse-engineer, while maintaining its original functionality. Although obfuscation is widely adopted, its effectiveness remains largely unexplored and unthoroughly evaluated. This paper presents a controlled experiment involving Master's students performing code comprehension tasks on applications hardened with obfuscation. The experiment's goals are to assess the effectiveness of obfuscation in delaying code comprehension by attackers and to determine whether complexity metrics can accurately predict the impact of these protections on success rates and durations of code comprehension tasks. The study is the first to evaluate the effect of layering multiple obfuscation techniques on a single piece of protected code. It also provides experimental evidence of the correlation between objective metrics of the attacked code and the likelihood of a successful attack, bridging the gap between objective and subjective approaches to estimating potency. Finally, the paper highlights significant aspects that warrant additional analysis and opens new avenues for further experiments.

</details>


### [8] [TAB-DRW: A DFT-based Robust Watermark for Generative Tabular Data](https://arxiv.org/abs/2511.21600)
*Yizhou Zhao,Xiang Li,Peter Song,Qi Long,Weijie Su*

Main category: cs.CR

TL;DR: TAB-DRW是一种高效鲁棒的后编辑水印方案，用于生成表格数据，通过在频域嵌入水印信号来解决现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 生成式AI产生的高保真合成表格数据在医疗、金融等领域引发数据来源和滥用的担忧，现有水印方法存在计算成本高、处理混合数据类型困难、鲁棒性不足等问题。

Method: 通过Yeo-Johnson变换和标准化归一化异构特征，应用离散傅里叶变换，根据预计算的伪随机位调整自适应选择条目的虚部，并引入基于排名的伪随机位生成方法实现无存储开销的行级检索。

Result: 在五个基准表格数据集上的实验表明，TAB-DRW实现了强检测性和对常见后处理攻击的鲁棒性，同时保持高数据保真度并完全支持混合类型特征。

Conclusion: TAB-DRW为生成表格数据提供了一种高效、鲁棒的水印解决方案，解决了现有方法的局限性。

Abstract: The rise of generative AI has enabled the production of high-fidelity synthetic tabular data across fields such as healthcare, finance, and public policy, raising growing concerns about data provenance and misuse. Watermarking offers a promising solution to address these concerns by ensuring the traceability of synthetic data, but existing methods face many limitations: they are computationally expensive due to reliance on large diffusion models, struggle with mixed discrete-continuous data, or lack robustness to post-modifications. To address them, we propose TAB-DRW, an efficient and robust post-editing watermarking scheme for generative tabular data. TAB-DRW embeds watermark signals in the frequency domain: it normalizes heterogeneous features via the Yeo-Johnson transformation and standardization, applies the discrete Fourier transform (DFT), and adjusts the imaginary parts of adaptively selected entries according to precomputed pseudorandom bits. To further enhance robustness and efficiency, we introduce a novel rank-based pseudorandom bit generation method that enables row-wise retrieval without incurring storage overhead. Experiments on five benchmark tabular datasets show that TAB-DRW achieves strong detectability and robustness against common post-processing attacks, while preserving high data fidelity and fully supporting mixed-type features.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [9] [Minimizing Hyperbolic Embedding Distortion with LLM-Guided Hierarchy Restructuring](https://arxiv.org/abs/2511.20679)
*Melika Ayoughi,Pascal Mettes,Paul Groth*

Main category: cs.AI

TL;DR: 本文研究使用大语言模型自动重构层次结构以优化双曲嵌入质量，提出基于提示的方法来转换现有层次结构，实验表明LLM重构的层次结构在多个标准嵌入质量指标上表现更好。


<details>
  <summary>Details</summary>
Motivation: 双曲嵌入的质量与输入层次结构紧密相关，而最优双曲嵌入需要高分支因子和单继承。为了帮助知识工程师重新组织层次知识，研究LLMs是否能够自动重构层次结构以满足这些标准。

Method: 提出基于提示的方法，使用大语言模型在已知双曲嵌入期望标准的指导下转换现有层次结构。

Result: 在16个不同层次结构上的实验表明，LLM重构的层次结构在多个标准嵌入质量指标上始终产生更高质量的双曲嵌入。

Conclusion: LLM引导的层次结构重构能够实现可解释的重组，为知识工程师提供合理性解释，同时显著提升双曲嵌入质量。

Abstract: Hyperbolic geometry is an effective geometry for embedding hierarchical data structures. Hyperbolic learning has therefore become increasingly prominent in machine learning applications where data is hierarchically organized or governed by hierarchical semantics, ranging from recommendation systems to computer vision. The quality of hyperbolic embeddings is tightly coupled to the structure of the input hierarchy, which is often derived from knowledge graphs or ontologies. Recent work has uncovered that for an optimal hyperbolic embedding, a high branching factor and single inheritance are key, while embedding algorithms are robust to imbalance and hierarchy size. To assist knowledge engineers in reorganizing hierarchical knowledge, this paper investigates whether Large Language Models (LLMs) have the ability to automatically restructure hierarchies to meet these criteria. We propose a prompt-based approach to transform existing hierarchies using LLMs, guided by known desiderata for hyperbolic embeddings. Experiments on 16 diverse hierarchies show that LLM-restructured hierarchies consistently yield higher-quality hyperbolic embeddings across several standard embedding quality metrics. Moreover, we show how LLM-guided hierarchy restructuring enables explainable reorganizations, providing justifications to knowledge engineers.

</details>


### [10] [AssurAI: Experience with Constructing Korean Socio-cultural Datasets to Discover Potential Risks of Generative AI](https://arxiv.org/abs/2511.20686)
*Chae-Gyun Lim,Seung-Ho Han,EunYoung Byun,Jeongyun Han,Soohyun Cho,Eojin Joo,Heehyeon Kim,Sieun Kim,Juhoon Lee,Hyunsoo Lee,Dongkun Lee,Jonghwan Hyeon,Yechan Hwang,Young-Jun Lee,Kyeongryul Lee,Minhyeong An,Hyunjun Ahn,Jeongwoo Son,Junho Park,Donggyu Yoon,Taehyung Kim,Jeemin Kim,Dasom Choi,Kwangyoung Lee,Hyunseung Lim,Yeohyun Jung,Jongok Hong,Sooyohn Nam,Joonyoung Park,Sungmin Na,Yubin Choi,Jeanne Choi,Yoojin Hong,Sueun Jang,Youngseok Seo,Somin Park,Seoungung Jo,Wonhye Chae,Yeeun Jo,Eunyoung Kim,Joyce Jiyoung Whang,HwaJung Hong,Joseph Seering,Uichin Lee,Juho Kim,Sunna Choi,Seokyeon Ko,Taeho Kim,Kyunghoon Kim,Myungsik Ha,So Jung Lee,Jemin Hwang,JoonHo Kwak,Ho-Jin Choi*

Main category: cs.AI

TL;DR: AssurAI是一个针对韩语多模态生成AI安全评估的新型质量管控数据集，包含11,480个文本、图像、视频和音频实例，涵盖35个AI风险因素，专门针对韩语社会文化背景设计。


<details>
  <summary>Details</summary>
Motivation: 当前的安全数据集主要是英语中心化的，无法捕捉非英语社会文化背景（如韩语）中的特定风险，且通常仅限于文本模态。

Method: 通过多学科专家组定义35个AI风险因素分类法，采用两阶段构建（专家引导播种和众包扩展）、三重独立标注和迭代专家红队循环的严格质量控制流程。

Result: 构建并发布了AssurAI数据集，初步研究验证了其在评估最新LLM安全性方面的有效性。

Conclusion: AssurAI的发布将促进为韩语社区开发更安全可靠的生成AI系统。

Abstract: The rapid evolution of generative AI necessitates robust safety evaluations. However, current safety datasets are predominantly English-centric, failing to capture specific risks in non-English, socio-cultural contexts such as Korean, and are often limited to the text modality. To address this gap, we introduce AssurAI, a new quality-controlled Korean multimodal dataset for evaluating the safety of generative AI. First, we define a taxonomy of 35 distinct AI risk factors, adapted from established frameworks by a multidisciplinary expert group to cover both universal harms and relevance to the Korean socio-cultural context. Second, leveraging this taxonomy, we construct and release AssurAI, a large-scale Korean multimodal dataset comprising 11,480 instances across text, image, video, and audio. Third, we apply the rigorous quality control process used to ensure data integrity, featuring a two-phase construction (i.e., expert-led seeding and crowdsourced scaling), triple independent annotation, and an iterative expert red-teaming loop. Our pilot study validates AssurAI's effectiveness in assessing the safety of recent LLMs. We release AssurAI to the public to facilitate the development of safer and more reliable generative AI systems for the Korean community.

</details>


### [11] [$A^2Flow:$ Automating Agentic Workflow Generation via Self-Adaptive Abstraction Operators](https://arxiv.org/abs/2511.20693)
*Mingming Zhao,Xiaokang Wei,Yuanqi Shao,Kaiwen Zhou,Lin Yang,Siwei Rao,Junhui Zhan,Zhitang Chen*

Main category: cs.AI

TL;DR: A²Flow是一个基于自适应抽象算子的全自动智能体工作流生成框架，通过三阶段算子提取过程自动生成可重用的执行算子，无需手动预定义，在性能和资源效率上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法严重依赖手动预定义算子，限制了智能体工作流的泛化性和可扩展性，需要开发全自动的框架来解决这一问题。

Method: 采用三阶段算子提取：1）基于案例的初始算子生成，利用专家演示和LLM推理；2）算子聚类和初步抽象，跨任务分组相似算子；3）深度提取抽象执行算子，使用长思维链提示和多路径推理。还包含算子记忆机制来增强工作流搜索。

Result: 在通用和具身基准测试中，A²Flow相比最先进基线平均性能提升2.4%和19.3%，资源使用减少37%。

Conclusion: A²Flow证明了基于自适应抽象算子的全自动工作流生成方法的有效性，为智能体工作流的自动化和可扩展性提供了新思路。

Abstract: Large language models (LLMs) have shown strong potential in automating the design of agentic workflows. However, existing methods still rely heavily on manually predefined operators, limiting generalization and scalability. To address this issue, we propose $A^2Flow$, a fully automated framework for agentic workflow generation based on self-adaptive abstraction operators. $A^2Flow$ employs a three-stage operator extraction process: 1) Case-based Initial Operator Generation: leveraging expert demonstrations and LLM reasoning to generate case-specific operators; 2) Operator Clustering and Preliminary Abstraction: grouping similar operators across tasks to form preliminary abstractions; and 3) Deep Extraction for Abstract Execution Operators: applying long chain-of-thought prompting and multi-path reasoning to derive compact and generalizable execution operators. These operators serve as reusable building blocks for workflow construction without manual predefinition. Furthermore, we enhance node-level workflow search with an operator memory mechanism, which retains historical outputs to enrich context and improve decision-making. Experiments on general and embodied benchmarks show that $A^2Flow$ achieves a 2.4\% and 19.3\% average performance improvement and reduces resource usage by 37\% over state-of-the-art baselines. Homepage:https://github.com/pandawei-ele/A2FLOW

</details>


### [12] [Reasoning With a Star: A Heliophysics Dataset and Benchmark for Agentic Scientific Reasoning](https://arxiv.org/abs/2511.20694)
*Kevin Lee,Russell Spiewak,James Walsh*

Main category: cs.AI

TL;DR: 该论文提出了一个名为"Reasoning With a Star"的太阳物理学推理数据集和基准测试方法，通过多智能体模式分解工作流程，在需要演绎推理的问题上优于直接提示方法。


<details>
  <summary>Details</summary>
Motivation: 解决太阳物理学中大型语言模型科学推理的挑战，包括整合物理假设、保持单位一致性和提供清晰科学格式的需求。

Method: 构建基于NASA和UCAR Living With a Star暑期学校问题集的数据集，采用程序化评分器进行单位感知数值容差、符号等价性和模式验证，比较单次基准和四种多智能体模式。

Result: 发现通过系统工程原则分解工作流程在需要演绎推理的问题上表现优于直接提示，特别是在非纯归纳回忆的问题上。

Conclusion: 多智能体模式能够有效提升太阳物理学中的科学推理能力，特别是在需要复杂演绎推理的场景中。

Abstract: Scientific reasoning through Large Language Models in heliophysics involves more than just recalling facts: it requires incorporating physical assumptions, maintaining consistent units, and providing clear scientific formats through coordinated approaches. To address these challenges, we present Reasoning With a Star, a newly contributed heliophysics dataset applicable to reasoning; we also provide an initial benchmarking approach. Our data are constructed from National Aeronautics and Space Administration & University Corporation for Atmospheric Research Living With a Star summer school problem sets and compiled into a readily consumable question-and-answer structure with question contexts, reasoning steps, expected answer type, ground-truth targets, format hints, and metadata. A programmatic grader checks the predictions using unit-aware numerical tolerance, symbolic equivalence, and schema validation. We benchmark a single-shot baseline and four multi-agent patterns, finding that decomposing workflows through systems engineering principles outperforms direct prompting on problems requiring deductive reasoning rather than pure inductive recall.

</details>


### [13] [A Brief History of Digital Twin Technology](https://arxiv.org/abs/2511.20695)
*Yunqi Zhang,Kuangyu Shi,Biao Li*

Main category: cs.AI

TL;DR: 数字孪生技术从NASA航天器模拟发展而来，现已在医疗领域实现转型应用，通过集成影像、生物传感器和计算模型创建患者特异性虚拟副本，支持诊断、治疗规划和药物开发，但仍面临互操作性、数据隐私和模型保真度等挑战。


<details>
  <summary>Details</summary>
Motivation: 推动医疗从被动治疗向预测性、预防性和个性化医疗转变，利用数字孪生技术提高医疗决策的精确性和效率。

Method: 通过实时数据流持续更新，集成医学影像、生物传感器和计算模型，构建患者特异性虚拟副本，实现双向交互和模拟预测。

Result: 已在心脏数字孪生预测心律失常治疗结果、肿瘤学数字孪生追踪肿瘤进展和优化放疗、药理学数字孪生加速药物发现等领域取得代表性应用成果。

Conclusion: 数字孪生技术有望彻底改变医疗模式，但需要解决互操作性、数据隐私和模型保真度等挑战，并通过可解释AI、联邦学习和统一监管框架等新兴解决方案推动临床集成。

Abstract: Emerging from NASA's spacecraft simulations in the 1960s, digital twin technology has advanced through industrial adoption to spark a healthcare transformation. A digital twin is a dynamic, data-driven virtual counterpart of a physical system, continuously updated through real-time data streams and capable of bidirectional interaction. In medicine, digital twin integrates imaging, biosensors, and computational models to generate patient-specific simulations that support diagnosis, treatment planning, and drug development. Representative applications include cardiac digital twin for predicting arrhythmia treatment outcomes, oncology digital twin for tracking tumor progression and optimizing radiotherapy, and pharmacological digital twin for accelerating drug discovery. Despite rapid progress, major challenges, including interoperability, data privacy, and model fidelity, continue to limit widespread clinical integration. Emerging solutions such as explainable AI, federated learning, and harmonized regulatory frameworks offer promising pathways forward. Looking ahead, advances in multi-organ digital twin, genomics integration, and ethical governance will be essential to ensure that digital twin shifts healthcare from reactive treatment to predictive, preventive, and truly personalized medicine.

</details>


### [14] [Cross Domain Evaluation of Multimodal Chain-of-Thought Reasoning of different datasets into the Amazon CoT Framework](https://arxiv.org/abs/2511.20701)
*Nitya Tiwari,Parv Maheshwari,Vidisha Agarwal*

Main category: cs.AI

TL;DR: 本文对多模态思维链推理进行了综合分析，评估了其在A-OKVQA、OKVQA和ChartQA数据集上的有效性，发现视觉集成显著减少推理幻觉，但思维链推理效果在不同问题类型间差异很大。


<details>
  <summary>Details</summary>
Motivation: 虽然最近工作将CoT扩展到多模态设置并在科学问答基准上取得最佳结果，但这些方法在不同领域的泛化能力仍未充分探索。本文旨在评估多模态CoT在需要常识和世界知识的多样化数据集上的有效性。

Method: 采用Zhang等人提出的两阶段框架，将推理生成与答案推断分离，并通过门控融合机制将视觉特征与基于T5的语言模型集成。通过系统消融研究分析视觉特征、推理质量和架构选择的贡献。

Result: 视觉集成显著减少推理生成中的幻觉，但CoT推理的有效性在不同问题类型间差异很大，常识推理尤其具有挑战性。

Conclusion: 本研究为实施多模态推理系统的研究人员提供了实用见解，并确定了跨领域泛化的关键改进领域。

Abstract: While recent work has extended CoT to multimodal settings, achieving state-of-the-art results on science question answering benchmarks like ScienceQA, the generalizability of these approaches across diverse domains remains underexplored. This work presents a comprehensive analysis of Multimodal Chain-of-Thought (Multimodal-CoT) reasoning, evaluating its effectiveness on the A-OKVQA, OKVQA and ChartQA datasets, which requires broad commonsense and world knowledge beyond scientific reasoning. We implement the two-stage framework proposed by Zhang et al. [3], which separates rationale generation from answer inference and integrates vision features through a gated fusion mechanism with T5-based language models. Through systematic ablation studies, we analyze the contributions of vision features, rationale quality, and architectural choices. Our findings reveal that while vision integration significantly reduces hallucination in rationale generation, the effectiveness of CoT reasoning varies substantially across question types, with commonsense reasoning presenting particular challenges. This work provides practical insights for researchers implementing multimodal reasoning systems and identifies key areas for future improvement in cross-domain generalization.

</details>


### [15] [Representation Interventions Enable Lifelong Unstructured Knowledge Control](https://arxiv.org/abs/2511.20892)
*Xuyuan Liu,Zhengzhang Chen,Xinshuai Dong,Yanchi Liu,Xujiang Zhao,Shengyu Chen,Haoyu Wang,Yujun Yan,Haifeng Chen*

Main category: cs.AI

TL;DR: RILKE是一种在表示空间进行干预的终身知识控制方法，通过低维子空间更新和查询自适应路由，实现大规模知识编辑而不影响模型通用能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型常产生错误或过时内容，需要在不重新训练的情况下高效准确地更新知识，特别是在终身学习环境中处理复杂非结构化知识时避免编辑干扰。

Method: RILKE在模型表示空间进行干预，学习抗释义和编辑局部化模块，将每个更新限制在低维子空间以减少交叉编辑干扰，推理时通过查询自适应路由选择合适模块指导生成。

Result: 在LLaMA和Qwen模型的知识编辑基准测试中，RILKE可扩展到大规模数据集，表现出高编辑成功率、强释义泛化能力，并以适度内存开销保持通用效用。

Conclusion: RILKE是LLMs中终身知识控制的有效且可扩展解决方案。

Abstract: Large language models (LLMs) often produce incorrect or outdated content. Updating their knowledge efficiently and accurately without costly retraining is a major challenge. This problem is especially hard for complex, unstructured knowledge in a lifelong setting, where many edits must coexist without interference. We introduce RILKE (Representation Intervention for Lifelong KnowledgE Control), a robust and scalable method that treats knowledge control as interventions within the model's representation space. Leveraging representation-space expressiveness, we identify two properties enabling RILKE to deliver fine-grained control over complex, unstructured knowledge while maintaining general utility with frozen base weights. During training, RILKE learns paraphrase-robust and edit-localized modules that limit each update to a low-dimensional subspace to minimize cross-edit interference. In inference, a query-adaptive router selects the appropriate module to guide the model's generation. In evaluation on knowledge editing benchmarks with LLaMA and Qwen models, RILKE is scalable to large-scale datasets, demonstrating high edit success, strong paraphrase generalization, and preserving general utility with modest memory overhead. These results show RILKE is an effective and scalable solution for lifelong knowledge control in LLMs.

</details>


### [16] [ENACT: Evaluating Embodied Cognition with World Modeling of Egocentric Interaction](https://arxiv.org/abs/2511.20937)
*Qineng Wang,Wenlong Huang,Yu Zhou,Hang Yin,Tianwei Bao,Jianwen Lyu,Weiyu Liu,Ruohan Zhang,Jiajun Wu,Li Fei-Fei,Manling Li*

Main category: cs.AI

TL;DR: ENACT是一个评估视觉语言模型是否表现出具身认知能力的基准，通过视觉问答形式评估从自我中心交互中进行世界建模的能力，包含前向世界建模和逆向世界建模两个任务。


<details>
  <summary>Details</summary>
Motivation: 探索现代视觉语言模型是否表现出具身认知的迹象，即智能是否源于传感器运动交互而非被动观察。

Method: 将具身认知评估构建为部分可观测马尔可夫决策过程，动作是场景图变化，包含两个序列重排序任务：前向世界建模（给定动作重排观察序列）和逆向世界建模（给定观察重排动作序列）。

Result: 实验显示前沿视觉语言模型与人类之间存在性能差距，且随着交互视野扩大而加大；模型在逆向任务上表现优于前向任务，并表现出人类中心偏见。

Conclusion: ENACT基准揭示了视觉语言模型在具身认知能力方面的局限性，为评估和改进模型的交互式世界建模能力提供了有效工具。

Abstract: Embodied cognition argues that intelligence arises from sensorimotor interaction rather than passive observation. It raises an intriguing question: do modern vision-language models (VLMs), trained largely in a disembodied manner, exhibit signs of embodied cognition? We introduce ENACT, a benchmark that casts evaluation of embodied cognition as world modeling from egocentric interaction in a visual question answering (VQA) format. Framed as a partially observable Markov decision process (POMDP) whose actions are scene graph changes, ENACT comprises two complementary sequence reordering tasks: forward world modeling (reorder shuffled observations given actions) and inverse world modeling (reorder shuffled actions given observations). While conceptually simple, solving these tasks implicitly demands capabilities central to embodied cognition-affordance recognition, action-effect reasoning, embodied awareness, and interactive, long-horizon memory from partially observable egocentric input, while avoiding low-level image synthesis that could confound the evaluation. We provide a scalable pipeline that synthesizes QA pairs from robotics simulation (BEHAVIOR) and evaluates models on 8,972 QA pairs spanning long-horizon home-scale activities. Experiments reveal a performance gap between frontier VLMs and humans that widens with interaction horizon. Models consistently perform better on the inverse task than the forward one and exhibit anthropocentric biases, including a preference for right-handed actions and degradation when camera intrinsics or viewpoints deviate from human vision. Website at https://enact-embodied-cognition.github.io/.

</details>


### [17] [Improving Procedural Skill Explanations via Constrained Generation: A Symbolic-LLM Hybrid Architecture](https://arxiv.org/abs/2511.20942)
*Rahul Dass,Thomas Bowlin,Zebing Li,Xiao Jin,Ashok Goel*

Main category: cs.AI

TL;DR: Ivy是一个AI教练系统，通过结合符号化的任务-方法-知识（TMK）模型与生成解释层，提供结构化的多步骤解释，以改进程序技能学习中的教学解释质量。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）通常产生流畅但浅层的响应，缺乏因果、目标导向和组合逻辑的结构。需要一种方法来增强AI生成解释的教学价值。

Method: 将符号化的TMK模型与生成解释层（LLM）结合，TMK编码因果转换、目标层次和问题分解，并在明确的结构边界内指导LLM构建解释。

Result: 与GPT和检索增强GPT基线相比，符号约束显著提高了对"如何"和"为什么"问题的解释结构质量。

Conclusion: 这项研究展示了一种可扩展的AI教育方法，通过符号约束增强了智能教练系统中AI生成解释的教学价值。

Abstract: In procedural skill learning, instructional explanations must convey not just steps, but the causal, goal-directed, and compositional logic behind them. Large language models (LLMs) often produce fluent yet shallow responses that miss this structure. We present Ivy, an AI coaching system that delivers structured, multi-step explanations by combining symbolic Task-Method-Knowledge (TMK) models with a generative interpretation layer-an LLM that constructs explanations while being constrained by TMK structure. TMK encodes causal transitions, goal hierarchies, and problem decompositions, and guides the LLM within explicit structural bounds. We evaluate Ivy against responses against GPT and retrieval-augmented GPT baselines using expert and independent annotations across three inferential dimensions. Results show that symbolic constraints consistently improve the structural quality of explanations for "how" and "why" questions. This study demonstrates a scalable AI for education approach that strengthens the pedagogical value of AI-generated explanations in intelligent coaching systems.

</details>


### [18] [ICPO: Intrinsic Confidence-Driven Group Relative Preference Optimization for Efficient Reinforcement Learning](https://arxiv.org/abs/2511.21005)
*Jinpeng Wang,Chao Li,Ting Ye,Mengyuan Zhang,Wei Liu,Jian Luan*

Main category: cs.AI

TL;DR: 本文提出了一种名为ICPO的新方法，通过结合内在置信度和可验证奖励来增强大型语言模型的推理能力，解决了现有RLVR方法中存在的粗粒度奖励、奖励噪声和低效探索等问题。


<details>
  <summary>Details</summary>
Motivation: 现有的RLVR方法存在粗粒度奖励、奖励噪声和低效探索等问题，导致训练不稳定和熵崩溃，限制了大型语言模型推理能力的提升。

Method: 提出ICPO方法，通过计算不同响应在相同输入提示下的相对生成概率来获得偏好优势分数，并将该分数与可验证奖励结合来指导探索过程。

Result: 在四个通用领域基准和三个数学基准上的综合实验表明，ICPO相比GRPO能够稳定提升推理能力。

Conclusion: ICPO方法通过偏好优势分数有效缓解了粗粒度奖励和奖励噪声问题，抑制了过度自信错误，增强了被低估高质量响应的相对优势，防止模型过度拟合特定策略，从而促进更彻底的探索。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) demonstrates significant potential in enhancing the reasoning capabilities of Large Language Models (LLMs). However, existing RLVR methods are often constrained by issues such as coarse-grained rewards, reward noise, and inefficient exploration, which lead to unstable training and entropy collapse. To address this challenge, we propose the Intrinsic Confidence-Driven Group Relative Preference Optimization method (ICPO). The intuition behind it lies in the fact that the probabilities of an LLM generating different responses can inherently and directly reflect its self-assessment of the reasoning process. Inspired by the idea of preference modeling, ICPO calculates a preference advantage score for each response by comparing the relative generation probabilities of multiple responses under the same input prompt, and integrates this score with verifiable rewards to guide the exploration process. We have discovered that the preference advantage score not only alleviates the issues of coarse-grained rewards and reward noise but also effectively curbs overconfident errors, enhances the relative superiority of undervalued high-quality responses, and prevents the model from overfitting to specific strategies, thereby facilitating more thorough exploration. Comprehensive experiments across four general-domain benchmarks and three mathematical benchmarks demonstrate that ICPO steadily boosts reasoning compared to GRPO.

</details>


### [19] [Causality Without Causal Models](https://arxiv.org/abs/2511.21260)
*Joseph Y. Halpern,Rafael Pass*

Main category: cs.AI

TL;DR: 本文对Halpern和Pearl的因果定义进行了抽象化，提取其关键特征，使其能够应用于任何定义了反事实的模型，从而扩展了因果关系的适用范围。


<details>
  <summary>Details</summary>
Motivation: Halpern和Pearl的因果定义局限于因果模型，无法处理包含析取、否定、信念和嵌套反事实的复杂公式，需要更通用的定义框架。

Method: 通过抽象化Halpern-Pearl因果定义的关键特征，构建一个适用于任何反事实模型的通用因果定义框架。

Result: 新定义不仅能在更广泛的模型中使用（包括允许回溯的模型），还能处理复杂逻辑公式，并能扩展到解释的定义。

Conclusion: 抽象化方法扩展了因果关系的应用范围，提供了对因果定义更深入的理解，并为解释概念提供了通用框架。

Abstract: Perhaps the most prominent current definition of (actual) causality is due to Halpern and Pearl.  It is defined using causal models (also known as structural equations models).  We abstract the definition, extracting its key features, so that it can be applied to any other model where counterfactuals are defined. By abstracting the definition, we gain a number of benefits. Not only can we apply the definition in a wider range of models, including ones that allow, for example, backtracking, but we can apply the definition to determine if A is a cause of B  even if A and B are formulas involving disjunctions, negations, beliefs, and nested counterfactuals (none of which can be handled by the Halpern-Pearl definition). Moreover, we can extend the ideas to getting an abstract definition of explanation that can be applied beyond causal models. Finally, we gain a deeper understanding of features of the definition  even in causal models.

</details>


### [20] [New Hybrid Heuristics for Pseudo-Boolean Propagation](https://arxiv.org/abs/2511.21417)
*Mia Müßig,Jan Johannsen*

Main category: cs.AI

TL;DR: 本文提出了新的混合决策启发式方法，在RoundingSAT求解器中显著优于当前方法


<details>
  <summary>Details</summary>
Motivation: 在伪布尔求解中，目前最成功的单元传播策略是结合了观察字面量方案和计数方法的混合模式，但需要改进其决策启发式

Method: 为混合决策引入了新的启发式方法

Result: 新启发式方法能够在RoundingSAT求解器中大幅超越当前方法

Conclusion: 新提出的混合决策启发式方法在伪布尔求解中表现出显著优势

Abstract: In pseudo-boolean solving the currently most successful unit propagation strategy is a hybrid mode combining the watched literal scheme with the counting method. This short paper introduces new heuristics for this hybrid decision, which are able to drastically outperform the current method in the RoundingSAT solver.

</details>


### [21] [MADRA: Multi-Agent Debate for Risk-Aware Embodied Planning](https://arxiv.org/abs/2511.21460)
*Junjian Wang,Lidan Zhao,Xi Sheryl Zhang*

Main category: cs.AI

TL;DR: 提出了MADRA框架，通过多智能体辩论进行风险评估，无需训练即可增强安全意识，同时保持任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在计算成本高或过度拒绝安全指令的问题，需要一种既能确保安全又不牺牲任务性能的解决方案。

Method: 使用多个LLM智能体对指令安全性进行辩论，由关键评估器根据逻辑合理性、风险识别、证据质量和清晰度评分，通过迭代审议和共识投票达成决策。

Result: 在AI2-THOR和VirtualHome上的实验表明，该方法对不安全任务的拒绝率超过90%，同时保持对安全任务的低拒绝率，在安全性和执行效率方面优于现有方法。

Conclusion: 该工作为构建可信赖的具身智能体提供了一个可扩展、模型无关的解决方案。

Abstract: Ensuring the safety of embodied AI agents during task planning is critical for real-world deployment, especially in household environments where dangerous instructions pose significant risks. Existing methods often suffer from either high computational costs due to preference alignment training or over-rejection when using single-agent safety prompts. To address these limitations, we propose MADRA, a training-free Multi-Agent Debate Risk Assessment framework that leverages collective reasoning to enhance safety awareness without sacrificing task performance. MADRA employs multiple LLM-based agents to debate the safety of a given instruction, guided by a critical evaluator that scores responses based on logical soundness, risk identification, evidence quality, and clarity. Through iterative deliberation and consensus voting, MADRA significantly reduces false rejections while maintaining high sensitivity to dangerous tasks. Additionally, we introduce a hierarchical cognitive collaborative planning framework that integrates safety, memory, planning, and self-evolution mechanisms to improve task success rates through continuous learning. We also contribute SafeAware-VH, a benchmark dataset for safety-aware task planning in VirtualHome, containing 800 annotated instructions. Extensive experiments on AI2-THOR and VirtualHome demonstrate that our approach achieves over 90% rejection of unsafe tasks while ensuring that safe-task rejection is low, outperforming existing methods in both safety and execution efficiency. Our work provides a scalable, model-agnostic solution for building trustworthy embodied agents.

</details>


### [22] [SpatialBench: Benchmarking Multimodal Large Language Models for Spatial Cognition](https://arxiv.org/abs/2511.21471)
*Peiran Xu,Sudong Wang,Yao Zhu,Jianing Li,Yunjian Zhang*

Main category: cs.AI

TL;DR: 提出了一个层次化空间认知框架，将空间智能分解为5个渐进复杂层次，并构建了SpatialBench基准来系统评估多模态大语言模型的空间推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准过度简化空间认知，将其简化为单一维度指标，无法捕捉空间能力的层次结构和相互依赖性。

Method: 构建层次化空间认知框架（5个认知层次），开发SpatialBench大规模细粒度基准（15个任务），引入能力导向的统一评估指标。

Result: 实验显示模型在不同认知层次表现分层：感知基础强，但在符号推理、因果推断和规划方面受限；人类进行选择性目标导向抽象，而MLLMs过度关注表面细节。

Conclusion: 建立了首个系统测量MLLMs层次化空间认知的框架，为未来空间智能系统奠定基础。

Abstract: Spatial cognition is fundamental to real-world multimodal intelligence, allowing models to effectively interact with the physical environment. While multimodal large language models (MLLMs) have made significant strides, existing benchmarks often oversimplify spatial cognition, reducing it to a single-dimensional metric, which fails to capture the hierarchical structure and interdependence of spatial abilities. To address this gap, we propose a hierarchical spatial cognition framework that decomposes spatial intelligence into five progressively complex levels from basic observation to high-level planning. Building upon this taxonomy, we construct SpatialBench, a large-scale, fine-grained benchmark covering 15 tasks aligned with these cognitive levels. To provide a unified evaluation across heterogeneous tasks, we further introduce a high-level capability-oriented metric that reliably assesses a model's overall spatial reasoning ability. Extensive experiments over massive MLLMs reveal distinct performance stratification across cognitive levels: models exhibit strong perceptual grounding yet remain limited in symbolic reasoning, causal inference, and planning. Additional human tests demonstrate that humans perform selective, goal-directed abstraction, while MLLMs tend to over-attend to surface details without coherent spatial intent. Our work establishes the first systematic framework for measuring hierarchical spatial cognition in MLLMs, laying the foundation for future spatially intelligent systems.

</details>


### [23] [Pessimistic Verification for Open Ended Math Questions](https://arxiv.org/abs/2511.21522)
*Yanxing Huang,Zihan Tang,Zejin Lin,Peng Li,Yang Liu*

Main category: cs.AI

TL;DR: 提出悲观验证方法，通过并行构建多个验证流程来检测数学证明中的错误，显著提升验证性能且计算资源消耗低。


<details>
  <summary>Details</summary>
Motivation: 现有验证性能的关键限制在于错误检测能力，需要改进开放数学问题的验证效果。

Method: 设计悲观验证变体，为同一证明构建多个并行验证流程，只要任一验证报告错误即判定证明不正确。

Result: 该方法在多个数学验证基准测试中显著提升性能，计算资源消耗低，token效率甚至超过扩展长链思维测试时缩放。

Conclusion: 悲观验证能有效提升语言模型输出的可靠性和性能，对实现长视野数学任务至关重要，有助于增强语言模型的数学能力。

Abstract: The key limitation of the verification performance lies in the ability of error detection. With this intuition we designed several variants of pessimistic verification, which are simple workflows that could significantly improve the verification of open-ended math questions. In pessimistic verification we construct multiple parallel verifications for the same proof, and the proof is deemed incorrect if any one of them reports an error. This simple technique significantly improves the performance across many math verification benchmarks without incurring substantial computational resources. Its token efficiency even surpassed extended long-CoT in test-time scaling. Our case studies further indicate that the majority of false negatives in stronger models are actually caused by annotation errors in the original dataset, so our method's performance is in fact underestimated. Self-verification for mathematical problems can effectively improve the reliability and performance of language model outputs, and it also plays a critical role in enabling long-horizon mathematical tasks. We believe that research on pessimistic verification will help enhance the mathematical capabilities of language models across a wide range of tasks.

</details>


### [24] [Self-Transparency Failures in Expert-Persona LLMs: A Large-Scale Behavioral Audit](https://arxiv.org/abs/2511.21569)
*Alex Diep*

Main category: cs.AI

TL;DR: 该研究审计了16个开源模型在专业角色下的AI身份披露能力，发现模型在不同领域存在显著的不一致性，某些模型在推理优化后反而降低了透明度。


<details>
  <summary>Details</summary>
Motivation: 研究动机是确保AI模型在专业高风险领域能够可靠披露其AI身份，避免用户因错误信任模型的专业能力而受到伤害。

Method: 采用共同花园设计，对16个开源模型（4B-671B参数）进行了19,200次试验，使用贝叶斯验证和Rogan-Gladen校正确保结果稳健性。

Result: 模型在不同专业角色下的身份披露率差异巨大（2.8%-73.6%），金融顾问角色初始披露率为30.8%，而神经外科医生角色仅为3.5%。推理优化会抑制自我透明度，某些推理变体的披露率比基础模型低48.4%。

Conclusion: 透明度反映的是训练因素而非模型规模，组织不能假设安全属性会转移到部署环境，需要刻意设计行为和经验验证。

Abstract: If a language model cannot reliably disclose its AI identity in expert contexts, users cannot trust its competence boundaries. This study examines self-transparency in models assigned professional personas within high-stakes domains where false expertise risks user harm. Using a common-garden design, sixteen open-weight models (4B--671B parameters) were audited across 19,200 trials. Models exhibited sharp domain-specific inconsistency: a Financial Advisor persona elicited 30.8% disclosure initially, while a Neurosurgeon persona elicited only 3.5%. This creates preconditions for a "Reverse Gell-Mann Amnesia" effect, where transparency in some domains leads users to overgeneralize trust to contexts where disclosure fails. Disclosure ranged from 2.8% to 73.6%, with a 14B model reaching 61.4% while a 70B produced just 4.1%. Model identity predicted behavior better than parameter count ($ΔR_{adj}^{2} = 0.359$ vs 0.018). Reasoning optimization actively suppressed self-transparency in some models, with reasoning variants showing up to 48.4% lower disclosure than base counterparts. Bayesian validation with Rogan--Gladen correction confirmed robustness to measurement error ($κ= 0.908$). These findings demonstrate transparency reflects training factors rather than scale. Organizations cannot assume safety properties transfer to deployment contexts, requiring deliberate behavior design and empirical verification.

</details>


### [25] [From Prediction to Foresight: The Role of AI in Designing Responsible Futures](https://arxiv.org/abs/2511.21570)
*Maria Perez-Ortiz*

Main category: cs.AI

TL;DR: 本文提出了"负责任计算前瞻"概念，探讨人工智能如何支持负责任的前瞻实践，帮助决策者应对未来不确定性并塑造可持续未来。


<details>
  <summary>Details</summary>
Motivation: 在技术快速发展和全球挑战日益复杂的时代，需要建立负责任的前瞻框架来帮助决策者应对未来不确定性，促进可持续和负责任的未来设计。

Method: 建立负责任计算前瞻的基础原则，开发AI驱动的前瞻工具，结合模拟和情景分析来增强决策能力，强调人机协作而非替代。

Result: AI能够增强决策者应对不确定性、评估风险和制定可持续战略的能力，但需要与人类判断相结合，理解社会、环境、经济和政治系统的相互依存关系。

Conclusion: AI应作为支持性工具融入前瞻实践，补充而非替代决策者判断，赋能政策制定者和社区应对21世纪的重大挑战，塑造有韧性和道德健全的未来。

Abstract: In an era marked by rapid technological advancements and complex global challenges, responsible foresight has emerged as an essential framework for policymakers aiming to navigate future uncertainties and shape the future. Responsible foresight entails the ethical anticipation of emerging opportunities and risks, with a focus on fostering proactive, sustainable, and accountable future design. This paper coins the term "responsible computational foresight", examining the role of human-centric artificial intelligence and computational modeling in advancing responsible foresight, establishing a set of foundational principles for this new field and presenting a suite of AI-driven foresight tools currently shaping it. AI, particularly in conjunction with simulations and scenario analysis, enhances policymakers' ability to address uncertainty, evaluate risks, and devise strategies geared toward sustainable, resilient futures. However, responsible foresight extends beyond mere technical forecasting; it demands a nuanced understanding of the interdependencies within social, environmental, economic and political systems, alongside a commitment to ethical, long-term decision-making that supports human intelligence. We argue that AI will play a role as a supportive tool in responsible, human-centered foresight, complementing rather than substituting policymaker judgment to enable the proactive shaping of resilient and ethically sound futures. This paper advocates for the thoughtful integration of AI into foresight practices to empower policymakers and communities as they confront the grand challenges of the 21st century.

</details>


### [26] [On the Limits of Innate Planning in Large Language Models](https://arxiv.org/abs/2511.21591)
*Charles Schepanowski,Charles Ling*

Main category: cs.AI

TL;DR: 本研究评估大型语言模型在8拼图任务中的规划和状态推理能力，发现即使有反馈机制和外部验证器，模型仍存在内部状态表示脆弱和启发式规划能力弱的问题。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在许多基准测试中表现优异，但其规划和状态推理能力仍不明确，需要通过经典任务如8拼图来直接评估这些能力。

Method: 使用8拼图任务测试四个模型，采用零样本、思维链和算法思维等提示策略，并引入分层纠正反馈和外部移动验证器。

Result: 反馈机制对某些模型-提示组合有改善，但成功运行通常冗长且计算昂贵。即使有外部验证器提供有效移动，所有模型都无法解决任何拼图。

Conclusion: 当前LLMs在没有外部工具的情况下，在规划方面存在显著限制，未来进展可能需要维护显式状态和执行结构化搜索的机制。

Abstract: Large language models (LLMs) achieve impressive results on many benchmarks, yet their capacity for planning and stateful reasoning remains unclear. We study these abilities directly, without code execution or other tools, using the 8-puzzle: a classic task that requires state tracking and goal-directed planning while allowing precise, step-by-step evaluation. Four models are tested under common prompting conditions (Zero-Shot, Chain-of-Thought, Algorithm-of-Thought) and with tiered corrective feedback. Feedback improves success rates for some model-prompt combinations, but many successful runs are long, computationally expensive, and indirect. We then examine the models with an external move validator that provides only valid moves. Despite this level of assistance, none of the models solve any puzzles in this setting. Qualitative analysis reveals two dominant deficits across all models: (1) brittle internal state representations, leading to frequent invalid moves, and (2) weak heuristic planning, with models entering loops or selecting actions that do not reduce the distance to the goal state. These findings indicate that, in the absence of external tools such as code interpreters, current LLMs have substantial limitations in planning and that further progress may require mechanisms for maintaining explicit state and performing structured search.

</details>


### [27] [Bridging the Unavoidable A Priori: A Framework for Comparative Causal Modeling](https://arxiv.org/abs/2511.21636)
*Peter S. Hovmand,Kari O'Donnell,Callie Ogland-Hand,Brian Biroscak,Douglas D. Gunzler*

Main category: cs.AI

TL;DR: 本文提出将系统动力学和结构方程建模整合到一个共同的数学框架中，以解决AI/ML模型开发中的偏见问题，并为负责任AI提供更好的理论基础。


<details>
  <summary>Details</summary>
Motivation: AI/ML模型在解决未解决问题时存在放大人类偏见的意外后果，需要利用更丰富的系统动态因果模型来指导负责任AI的发展。

Method: 将系统动力学和结构方程建模整合到一个共同的数学框架中，用于从分布生成系统、开发方法并比较结果。

Result: 建立了一个统一的数学框架，能够弥合不同方法论之间的假设差异，为系统动力学在数据科学和AI/ML应用中的认识论提供信息。

Conclusion: 该框架有助于推进负责任AI/ML的发展，通过整合系统动态建模来解决AI模型中的偏见问题。

Abstract: AI/ML models have rapidly gained prominence as innovations for solving previously unsolved problems and their unintended consequences from amplifying human biases. Advocates for responsible AI/ML have sought ways to draw on the richer causal models of system dynamics to better inform the development of responsible AI/ML. However, a major barrier to advancing this work is the difficulty of bringing together methods rooted in different underlying assumptions (i.e., Dana Meadow's "the unavoidable a priori"). This paper brings system dynamics and structural equation modeling together into a common mathematical framework that can be used to generate systems from distributions, develop methods, and compare results to inform the underlying epistemology of system dynamics for data science and AI/ML applications.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [28] [Train While You Fight -- Technical Requirements for Advanced Distributed Learning Platforms](https://arxiv.org/abs/2511.20813)
*Simon Hacks*

Main category: cs.SE

TL;DR: 本文探讨了支持"边战边训"(TWYF)理念的先进分布式学习平台所需的技术要求，以及现有软件工程模式如何满足这些要求。


<details>
  <summary>Details</summary>
Motivation: 推动在作战行动期间进行持续学习，而不仅仅是在行动前后进行训练，以提高军事训练效率和适应性。

Method: 采用设计科学研究方法：(i)从PfPC/北约文档和近期实践中推导挑战，(ii)定义解决方案目标，(iii)进行从挑战到成熟模式的系统性映射。

Result: 识别出七个关键技术挑战：互操作性、弹性、多语言支持、数据安全与隐私、可扩展性、平台独立性和模块化，并通过德国武装部队的国家用例进行了模式说明。

Conclusion: 现有软件工程模式能够有效支持"边战边训"理念的技术要求，为先进分布式学习平台的开发提供了可行的解决方案框架。

Abstract: "Train While You Fight" (TWYF) advocates for continuous learning that occurs during operations, not just before or after. This paper examines the technical requirements that advanced distributed learning (ADL) platforms must meet to support TWYF, and how existing software engineering patterns can fulfill these requirements. Using a Design Science Research approach, we (i) derive challenges from PfPC/NATO documentation and recent practice, (ii) define solution objectives, and (iii) conduct a systematic mapping from challenges to proven patterns. We identify seven technical challenges: interoperability, resilience, multilingual support, data security and privacy, scalability, platform independence, and modularity. We illustrate the patterns with a national use case from the German armed forces.

</details>


### [29] [Hierarchical Evaluation of Software Design Capabilities of Large Language Models of Code](https://arxiv.org/abs/2511.20933)
*Mootez Saad,Boqi Chen,José Antonio Hernández López,Dániel Varró,Tushar Sharma*

Main category: cs.SE

TL;DR: 该研究评估了DeepSeek-R1系列LLM对软件设计概念（内聚性和耦合性）的理解。在理想条件下模型表现良好，但在实际场景中，对耦合性的推理能力在噪声环境下急剧下降，而内聚性分析在指导任务中相对稳健。


<details>
  <summary>Details</summary>
Motivation: 评估LLM在软件工程领域中对核心设计概念（内聚性和耦合性）的掌握程度，特别是在实际应用场景中的稳健性。

Method: 通过程序化生成设计不良的代码片段，在不同指导级别（验证、引导、开放式生成）下测试DeepSeek-R1模型系列（14B、32B、70B），并注入干扰元素来模拟上下文噪声。

Result: 模型在理想条件下对两个概念都有良好理解，但实际应用中表现脆弱且不对称。耦合性推理在噪声开放式场景中性能下降超过50%，而内聚性分析在引导任务中对内部噪声具有鲁棒性。推理轨迹分析揭示了耦合性认知捷径和内聚性详尽分析但仍失败的模式。

Conclusion: LLM在识别设计缺陷方面能提供可靠帮助，但在噪声现实环境中自主推理能力有限，强调了需要更可扩展和鲁棒的程序理解能力。

Abstract: Large language models (LLMs) are being increasingly adopted in the software engineering domain, yet the robustness of their grasp on core software design concepts remains unclear. We conduct an empirical study to systematically evaluate their understanding of cohesion (intra-module) and coupling (inter-module). We programmatically generate poorly designed code fragments and test the DeepSeek-R1 model family ($14$B, $32$B, $70$B) under varying levels of guidance, from simple \textit{Verification} to \textit{Guided} and \textit{Open-ended Generation}, while varying contextual noise by injecting distractor elements. While models exhibit a solid baseline understanding of both concepts in ideal conditions, their practical knowledge is fragile and highly asymmetrical. Reasoning about coupling proves brittle; performance collapses in noisy, open-ended scenarios, with F1 scores dropping by over $50\%$. In contrast, the models' analysis of cohesion is remarkably robust to internal noise in guided tasks, showing little performance degradation. However, this resilience also fails when all guidance is removed. Reasoning-trace analysis confirms these failure modes, revealing \textit{cognitive shortcutting} for coupling versus a more exhaustive (yet still failing) analysis for cohesion. To summarize, while LLMs can provide reliable assistance for recognizing design flaws, their ability to reason autonomously in noisy, realistic contexts is limited, highlighting the critical need for more scalable and robust program understanding capabilities.

</details>


### [30] [SpaceX: Exploring metrics with the SPACE model for developer productivity](https://arxiv.org/abs/2511.20955)
*Sanchit Kaul,Kevin Nhu,Jason Eissayou,Ivan Eser,Victor Borup*

Main category: cs.SE

TL;DR: 本研究通过实证调查揭示了确定性、单维生产力启发式方法的局限性，采用SPACE框架通过广泛的代码库挖掘来操作化多维生产力评估。


<details>
  <summary>Details</summary>
Motivation: 解决传统单维生产力指标的局限性，提供更全面、多方面的开发者生产力评估方法。

Method: 使用来自开源代码库的数据集，采用广义线性混合模型（GLMM）和基于RoBERTa的情感分类等严格统计方法，合成全面的多维生产力指标。

Result: 分析结果显示负面情感状态与提交频率之间存在显著正相关，表明存在由挫败感驱动的迭代修复循环；同时发现分析贡献者交互拓扑结构比传统基于数量的指标能更好地映射协作动态。

Conclusion: 研究提出了复合生产力评分（CPS）来解决开发者效能的异质性问题，为生产力评估提供了更全面的框架。

Abstract: This empirical investigation elucidates the limitations of deterministic, unidimensional productivity heuristics by operationalizing the SPACE framework through extensive repository mining. Utilizing a dataset derived from open-source repositories, the study employs rigorous statistical methodologies including Generalized Linear Mixed Models (GLMM) and RoBERTa-based sentiment classification to synthesize a holistic, multi-faceted productivity metric. Analytical results reveal a statistically significant positive correlation between negative affective states and commit frequency, implying a cycle of iterative remediation driven by frustration. Furthermore, the investigation has demonstrated that analyzing the topology of contributor interactions yields superior fidelity in mapping collaborative dynamics compared to traditional volume-based metrics. Ultimately, this research posits a Composite Productivity Score (CPS) to address the heterogeneity of developer efficacy.

</details>


### [31] [Lightweight Model Editing for LLMs to Correct Deprecated API Recommendations](https://arxiv.org/abs/2511.21022)
*Guancheng Lin,Xiao Yu,Jacky Keung,Xing Hu,Xin Xia,Alex X. Liu*

Main category: cs.SE

TL;DR: 本研究系统评估了10种模型编辑方法在更新LLMs中过时API知识的效果，提出了AdaLoRA-L方法，通过区分通用API层和特定API层来提升编辑特异性。


<details>
  <summary>Details</summary>
Motivation: LLMs在代码补全任务中常生成已弃用的API，因为其训练数据存在时效性问题。重新训练成本高昂，而现有轻量级模型编辑方法是否能有效更新API知识尚不明确。

Method: 构建EDAPIBench基准，包含70多个弃用API和3000多个编辑实例；评估10种SOTA模型编辑技术；提出AdaLoRA-L方法，定义通用API层和特定API层，限制编辑范围。

Result: AdaLoRA在生成正确更新API方面表现最佳，但特异性不足；AdaLoRA-L显著提升特异性，同时在其他评估指标上保持可比性能。

Conclusion: 模型编辑技术可以有效更新LLMs中的过时API知识，AdaLoRA-L通过分层编辑策略在保持性能的同时显著改善特异性。

Abstract: Pre-trained or fine-tuned on large code corpora, Large Language Models (LLMs) have demonstrated strong performance in code completion tasks. However, their embedded knowledge is constrained by the timeliness of training data, which often includes code using deprecated APIs. Consequently, LLMs frequently generate deprecated APIs that will no longer be supported in future versions of third-party libraries. While retraining LLMs on updated codebases could refresh their API knowledge, this approach is computationally expensive. Recently, lightweight model editing methods have emerged to efficiently correct specific knowledge in LLMs. However, it remains unclear whether these methods can effectively update deprecated API knowledge and enable edited models to generate up-to-date APIs. To address this gap, we conduct the first systematic study applying 10 state-of-the-art model editing techniques to update deprecated API knowledge in three LLMs: Qwen2.5-Coder, StarCoder2, and DeepSeek-Coder. We introduce EDAPIBench, a dedicated benchmark featuring over 70 deprecated APIs from 8 popular Python libraries, with more than 3,000 editing instances. Our results show that the parameter-efficient fine-tuning method AdaLoRA achieves the best performance in enabling edited models to generate correct, up-to-date APIs, but falls short in Specificity (i.e., the editing influences untargeted knowledge). To resolve this, we propose AdaLoRA-L, which defines "Common API Layers" (layers within the LLMs with high importance across all APIs, storing general knowledge and excluded from editing) and restricts edits exclusively to "Specific API Layers" (layers with high importance only for the target API, storing the API-specific knowledge). Experimental results demonstrate that AdaLoRA-L significantly improves Specificity while maintaining comparable performance across other evaluation metrics.

</details>


### [32] [Exploring Hidden Geographic Disparities in Android Apps](https://arxiv.org/abs/2511.21151)
*M. Alecci,P. Jiménez,J. Samhi,T. Bissyandé,J. Klein*

Main category: cs.SE

TL;DR: 本文研究了Android应用的地理差异现象，发现了GeoTwins（功能相似但在不同国家发布的应用变体）和Android App Bundle生态系统中base.apk文件的区域差异，这些差异对安全性和公平性产生影响。


<details>
  <summary>Details</summary>
Motivation: 移动应用演化已被广泛研究，但应用行为的地理差异仍未被充分探索。本文旨在揭示基于位置的Android应用差异化现象及其安全性和公平性影响。

Method: 构建了跨多个区域的分布式应用收集管道，分析了数千个应用，并发布了包含81,963个GeoTwins的数据集。

Result: 发现了GeoTwins现象：功能相似的应用在不同国家具有不同的权限请求、第三方库和隐私披露；揭示了Android App Bundle生态系统中base.apk文件存在意外的区域差异。

Conclusion: 移动软件存在系统性区域差异，这些差异破坏了研究的可重复性，引入了地理偏见，并对安全、隐私和功能评估产生影响，对研究人员、开发者、平台架构师和政策制定者具有重要意义。

Abstract: While mobile app evolution has been widely studied, geographical variation in app behavior remains largely unexplored. This paper presents a large-scale study of location-based Android app differentiation, uncovering two important and underexamined phenomena with security and fairness implications. First, we introduce GeoTwins: apps that are functionally similar and share branding but are released under different package names across countries. Despite their similarity, GeoTwins often diverge in requested permissions, third-party libraries, and privacy disclosures. Second, we examine the Android App Bundle ecosystem and reveal unexpected regional differences in supposedly consistent base.apk files. Contrary to common assumptions, even base.apk files vary by region, exposing hidden customizations that may affect app behavior or security.
  These discrepancies have concrete consequences. Geographically distinct variants can lead the same app to be labeled benign in one malware study but suspicious in another, depending on the region of download. Such hidden variation undermines reproducibility and introduces geographic bias into assessments of security, privacy, and functionality. It also raises ethical concerns about transparency and consent: visually identical Google Play listings may mask subtle but important differences.
  To study these issues, we built a distributed app collection pipeline spanning multiple regions and analyzed thousands of apps. We also release a dataset of 81,963 GeoTwins to support future work. Our findings reveal systemic regional disparities in mobile software, with implications for researchers, developers, platform architects, and policymakers.

</details>


### [33] [Bug Detective and Quality Coach: Developers' Mental Models of AI-Assisted IDE Tools](https://arxiv.org/abs/2511.21197)
*Paolo Buono,Mary Cerullo,Stefano Cirillo,Giuseppe Desolda,Francesco Greco,Emanuela Guglielmi,Grazia Margarella,Giuseppe Polese,Simone Scalabrino,Cesare Tucci*

Main category: cs.SE

TL;DR: 该研究通过6次共同设计工作坊与58名开发者探讨了他们对AI辅助bug检测和代码可读性评估工具的心理模型，发现开发者将bug检测工具视为"bug侦探"，将可读性评估工具视为"质量教练"，并提出了以人为本的IDE AI设计原则。


<details>
  <summary>Details</summary>
Motivation: 尽管AI辅助工具在技术特性上有所进步，但开发者如何心理建模这些工具以及不匹配如何影响信任、控制和采用尚不清楚。

Method: 进行了6次共同设计工作坊，涉及58名开发者，以获取他们对AI辅助bug检测和可读性功能的心理模型。

Result: 开发者将bug检测工具视为"bug侦探"（仅在关键问题时警告用户，保证透明度、可操作反馈和信心提示），将可读性评估工具视为"质量教练"（提供情境化、个性化和渐进式指导）。信任取决于解释的清晰度、时机和用户控制。

Conclusion: 提炼了一套以人为本的IDE AI设计原则，旨在平衡干扰与支持、简洁与深度、自动化与人类能动性。

Abstract: AI-assisted tools support developers in performing cognitively demanding tasks such as bug detection and code readability assessment. Despite the advancements in the technical characteristics of these tools, little is known about how developers mentally model them and how mismatches affect trust, control, and adoption. We conducted six co-design workshops with 58 developers to elicit their mental models about AI-assisted bug detection and readability features. It emerged that developers conceive bug detection tools as \textit{bug detectives}, which warn users only in case of critical issues, guaranteeing transparency, actionable feedback, and confidence cues. Readability assessment tools, on the other hand, are envisioned as \textit{quality coaches}, which provide contextual, personalized, and progressive guidance. Trust, in both tasks, depends on the clarity of explanations, timing, and user control. A set of design principles for Human-Centered AI in IDEs has been distilled, aiming to balance disruption with support, conciseness with depth, and automation with human agency.

</details>


### [34] [Multi-Agent Systems for Dataset Adaptation in Software Engineering: Capabilities, Limitations, and Future Directions](https://arxiv.org/abs/2511.21380)
*Jingyi Chen,Xiaoyan Guo,Songqiang Chen,Shing-Chi Cheung,Jiasi Shen*

Main category: cs.SE

TL;DR: 本文首次实证研究了最先进的多智能体系统在数据集适应任务中的表现，评估了基于GPT-4.1和Claude Sonnet 4的Copilot在适应软件工程研究工件方面的能力。


<details>
  <summary>Details</summary>
Motivation: 自动化软件工程研究工件的跨数据集适应对于可扩展性和可复现性至关重要，但目前仍缺乏系统研究。

Method: 通过五阶段评估流程（文件理解、代码编辑、命令生成、验证和最终执行），测量成功率、分析失败模式，并评估旨在增强智能体性能的提示干预。

Result: 当前系统能够识别关键文件并生成部分适应，但很少产生功能正确的实现。提示级干预（特别是提供执行错误消息和参考代码）显著提高了与真实情况的结构相似性（从7.25%提高到67.14%）。

Conclusion: 研究揭示了当前多智能体LLM系统在数据集适应方面的潜力和局限性，并为未来构建更可靠、自校正的智能体提供了具体方向。

Abstract: Automating the adaptation of software engineering (SE) research artifacts across datasets is essential for scalability and reproducibility, yet it remains largely unstudied. Recent advances in large language model (LLM)-based multi-agent systems, such as GitHub Copilot's agent mode, promise to automate complex development workflows through coordinated reasoning, code generation, and tool interaction. This paper presents the first empirical study on how state-of-the-art multi-agent systems perform in dataset adaptation tasks. We evaluate Copilot, backed by GPT-4.1 and Claude Sonnet 4, on adapting SE research artifacts from benchmark repositories including ROCODE and LogHub2.0. Through a five-stage evaluation pipeline (file comprehension, code editing, command generation, validation, and final execution), we measure success rates, analyze failure patterns, and assess prompt-based interventions designed to enhance agent performance. Results show that current systems can identify key files and generate partial adaptations but rarely produce functionally correct implementations. Prompt-level interventions, especially providing execution error messages and reference code, substantially improve structural similarity to ground truth (from 7.25% to 67.14%), highlighting the importance of contextual and feedback-driven guidance. Our findings reveal both the promise and limitations of today's multi-agent LLM systems for dataset adaptation, and suggest concrete directions for building more reliable, self-correcting agents in future SE research.

</details>
