<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 12]
- [cs.CR](#cs.CR) [Total: 20]
- [cs.AI](#cs.AI) [Total: 24]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Evaluating LLMs on microservice-based applications: how complex is your specification?](https://arxiv.org/abs/2508.20119)
*Daniel M. Yellin*

Main category: cs.SE

TL;DR: 这篇论文评估了大语言模型在生成微服务应用程序代码方面的能力，发现强大模型在中等难度规范上表现不错，但在高难度规范上表现很差，主要因难点包括复杂的业务逻辑、外部服务集成、数据库集成和非功能性要求如认证等。


<details>
  <summary>Details</summary>
Motivation: 评估LLM在生成实际微服务应用程序代码方面的进展程度，微服务架构是一种广泛使用的架构模式。

Method: 定义了微服务应用的标准规范模板，提出了规范难度级别评估指标，并开发了一个自动化测试框架用于测试LLM生成的微服务代码。

Result: 强大LLM（如GPT-3o-mini）在中等难度规范上表现不错，但在高难度规范上表现1很差，主要因难包括复杂业务逻辑、外部服务集成、数据库集成咏非功能性要求。

Conclusion: 分析了LLM生成代码的错误，指出了LLM在这些规范上面临的主要挑战，为改善实际问题代码生成的未来研究方向提供了建议。

Abstract: In this paper we evaluate how far LLMs have advanced in generating code for
real-world problems. Specifically, we explore code synthesis for
microservice-based applications, a widely used architecture pattern. We define
a standard template for specifying these applications, and we propose a metric
for judging the difficulty level of a specification. The higher the score, the
more difficult it is to generate code for the specification. We develop a
framework to automate the process of testing LLM-synthesized code for a
microservice using unit tests. Our experimental results show that strong LLMs
(like GPT-3o-mini) do fairly well on medium difficulty specifications but do
very poorly on those of higher difficulty levels. This is due to more intricate
business logic, a greater use of external services, database integration and
inclusion of non-functional capabilities such as authentication. We analyzed
the errors in LLM-synthesized code and report on the key challenges LLMs face
in generating code for these specifications thereby suggesting future research
directions to improve code synthesis for real-world problems.

</details>


### [2] [Towards Better Correctness and Efficiency in Code Generation](https://arxiv.org/abs/2508.20124)
*Yunlong Feng,Yang Xu,Xiao Xu,Binyuan Hui,Junyang Lin*

Main category: cs.SE

TL;DR: 通过效率导向的强化学习框架，提出了两阶段调优方法，在保持代码正确性的同时显著提升运行效率，达到了与更大模型相当的性能水平。


<details>
  <summary>Details</summary>
Motivation: 当前代码大语言模型生成的代码经常运行效率较低，限制了在性能敏感场景中的实际应用。

Method: 提出以效率为导向的强化学习框架，包括：(1)动态探索突破离线精调的静态数据限制；(2)错误不敏感的强化学习方法和高对比度效率信号；(3)从高正确性基准开始的在线探索。最终提出两阶段调优方法。

Result: 在7B模型上，代码正确性提升10.18%，运行效率提升7.75%，达到了与更大模型相当的性能水平。

Conclusion: 该方法能够有效解决代码生成模型的效率问题，实现了正确性和效率的高平衡性能表现，为性能敏感应用场景提供了实用的解决方案。

Abstract: While code large language models have demonstrated remarkable progress in
code generation, the generated code often exhibits poor runtime efficiency,
limiting its practical application in performance-sensitive scenarios. To
address this limitation, we propose an efficiency-oriented reinforcement
learning framework guided by a novel performance reward. Based on this
framework, we take a deeper dive into the code efficiency problem, identifying
then proposing methods to overcome key bottlenecks: (1) Dynamic exploration
overcomes the static data constraints of offline fine-tuning, enabling the
discovery of more efficient code implementations. (2) The error-insensitive
reinforcement learning method and high-contrast efficiency signals are crucial
for mitigating systematic errors and achieving effective optimization. (3)
Online exploration is most effective when starting from a high-correctness
baseline, as this allows for efficiency improvements without sacrificing
accuracy. With these discoveries, we finally propose a two-stage tuning method,
which achieves high and balanced performance across correctness and efficiency.
The results of experiments show the effectiveness of the method, which improves
code correctness by 10.18\% and runtime efficiency by 7.75\% on a 7B model,
achieving performance comparable to much larger model.

</details>


### [3] [Boosting Skeleton-Driven SMT Solver Fuzzing by Leveraging LLM to Produce Formula Generators](https://arxiv.org/abs/2508.20340)
*Maolin Sun,Yibiao Yang,Yuming Zhou*

Main category: cs.SE

TL;DR: Chimera是一个基于LLM的SMT求解器模糊测试框架，通过生成可重用的项生成器而非直接生成公式，解决了现有方法中语法无效和计算开销大的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的SMT求解器测试方法难以跟上快速发展的功能特性，基于LLM的方法虽然显示出潜力，但存在近半生成公式语法无效和迭代交互计算开销大的问题。

Method: Chimera使用LLM从文档中自动提取SMT理论的上下文无关文法，并合成可组合的布尔项生成器。在模糊测试时，用LLM合成的生成器产生的项填充现有公式的结构骨架。

Result: 在Z3和cvc5两个主流SMT求解器上测试，Chimera发现了43个已确认的bug，其中40个已被开发者修复。

Conclusion: Chimera通过一次性LLM交互投资显著降低了运行时成本，同时保证了语法有效性并促进了语义多样性，为SMT求解器测试提供了高效可靠的解决方案。

Abstract: Satisfiability Modulo Theory (SMT) solvers are foundational to modern systems
and programming languages research, providing the foundation for tasks like
symbolic execution and automated verification. Because these solvers sit on the
critical path, their correctness is essential, and high-quality test formulas
are key to uncovering bugs. However, while prior testing techniques performed
well on earlier solver versions, they struggle to keep pace with rapidly
evolving features. Recent approaches based on Large Language Models (LLMs) show
promise in exploring advanced solver capabilities, but two obstacles remain:
nearly half of the generated formulas are syntactically invalid, and iterative
interactions with the LLMs introduce substantial computational overhead. In
this study, we present Chimera, a novel LLM-assisted fuzzing framework that
addresses both issues by shifting from direct formula generation to the
synthesis of reusable term (i.e., logical expression) generators. Particularly,
Chimera uses LLMs to (1) automatically extract context-free grammars (CFGs) for
SMT theories, including solver-specific extensions, from documentation, and (2)
synthesize composable Boolean term generators that adhere to these grammars.
During fuzzing, Chimera populates structural skeletons derived from existing
formulas with the terms iteratively produced by the LLM-synthesized generators.
This design ensures syntactic validity while promoting semantic diversity.
Notably, Chimera requires only one-time LLM interaction investment,
dramatically reducing runtime cost. We evaluated Chimera on two leading SMT
solvers: Z3 and cvc5. Our experiments show that Chimera has identified 43
confirmed bugs, 40 of which have already been fixed by developers.

</details>


### [4] [Adaptive Root Cause Localization for Microservice Systems with Multi-Agent Recursion-of-Thought](https://arxiv.org/abs/2508.20370)
*Lingzhe Zhang,Tong Jia,Kangjin Wang,Weijie Hong,Chiming Duan,Minghua He,Ying Li*

Main category: cs.SE

TL;DR: RCLAgent是一个基于多智能体递归思维框架的自适应根因定位方法，通过模拟SRE的递归、多维扩展和跨模态推理特性，在单次请求中就能准确定位微服务系统中的故障根因，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现代微服务系统日益复杂，故障频发，现有根因定位方法要么依赖预定义模式难以适应变化环境，要么缺乏可解释性让SRE困惑。通过研究SRE的人工根因分析过程，发现其具有递归性、多维扩展和跨模态推理三大特征。

Method: 提出RCLAgent方法，采用多智能体递归思维框架，使用新颖的递归思维策略指导LLM推理过程，整合多智能体数据和工具辅助分析来准确定位根因。

Result: 在多个公共数据集上的实验评估表明，RCLAgent仅需单次请求就能定位根因，性能优于依赖多次请求聚合的state-of-the-art方法。

Conclusion: RCLAgent能有效提升复杂微服务环境中根因定位的效率和精确性，证明了该方法在增强系统可靠性方面的有效性。

Abstract: As contemporary microservice systems become increasingly popular and
complex-often comprising hundreds or even thousands of fine-grained,
interdependent subsystems-they are facing more frequent failures. Ensuring
system reliability thus demands accurate root cause localization. While traces
and metrics have proven to be effective data sources for this task, existing
methods either heavily rely on pre-defined schemas, which struggle to adapt to
evolving operational contexts, or lack interpretability in their reasoning
process, thereby leaving Site Reliability Engineers (SREs) confused. In this
paper, we conduct a comprehensive study on how SREs localize the root cause of
failures, drawing insights from multiple professional SREs across different
organizations. Our investigation reveals that human root cause analysis
exhibits three key characteristics: recursiveness, multi-dimensional expansion,
and cross-modal reasoning. Motivated by these findings, we introduce RCLAgent,
an adaptive root cause localization method for microservice systems that
leverages a multi-agent recursion-of-thought framework. RCLAgent employs a
novel recursion-of-thought strategy to guide the LLM's reasoning process,
effectively integrating data from multiple agents and tool-assisted analysis to
accurately pinpoint the root cause. Experimental evaluations on various public
datasets demonstrate that RCLAgent achieves superior performance by localizing
the root cause using only a single request-outperforming state-of-the-art
methods that depend on aggregating multiple requests. These results underscore
the effectiveness of RCLAgent in enhancing the efficiency and precision of root
cause localization in complex microservice environments.

</details>


### [5] [AI and Agile Software Development: A Research Roadmap from the XP2025 Workshop](https://arxiv.org/abs/2508.20563)
*Zheying Zhang,Tomas Herda,Victoria Pichler,Pekka Abrahamsson,Geir K. Hanssen,Joshua Kerievsky,Alex Polyakov,Mohit Chandna,Marius Irgens,Kai-Kristian Kemell,Ayman Asad Khan,Crystal Kwok,Evan Leybourn,Munish Malik,Dorota Mleczko,Morteza Moalagh,Christopher Morales,Yuliia Pieskova,Daniel Planötscher,Mika Saari,Anastasiia Tkalich,Karl Josef Gstettner,Xiaofeng Wang*

Main category: cs.SE

TL;DR: XP2025研讨会总结了GenAI与敏捷开发融合的挑战与机遇，识别了工具碎片化、治理、数据质量和AI素养等痛点，并制定了多主题研究路线图


<details>
  <summary>Details</summary>
Motivation: 解决生成式人工智能与敏捷软件开发交叉领域的具体挑战和新兴机遇，促进学术界与工业界的跨学科合作

Method: 通过结构化互动分组讨论，汇集30多名跨学科学者和行业从业者，分析痛点并共同制定研究路线图

Result: 识别了工具碎片化、治理、数据质量、AI素养和提示工程技能差距等关键挑战，揭示了根本原因和交叉问题

Conclusion: 制定了包含短期可实施行动和长期愿景研究方向的统一研究议程，旨在推动GenAI在敏捷实践中负责任、以人为中心的整合

Abstract: This paper synthesizes the key findings from a full-day XP2025 workshop on
"AI and Agile: From Frustration to Success", held in Brugg-Windisch,
Switzerland. The workshop brought together over 30 interdisciplinary academic
researchers and industry practitioners to tackle the concrete challenges and
emerging opportunities at the intersection of Generative Artificial
Intelligence (GenAI) and agile software development. Through structured,
interactive breakout sessions, participants identified shared pain points like
tool fragmentation, governance, data quality, and critical skills gaps in AI
literacy and prompt engineering. These issues were further analyzed, revealing
underlying causes and cross-cutting concerns. The workshop concluded by
collaboratively co-creating a multi-thematic research roadmap, articulating
both short-term, implementable actions and visionary, long-term research
directions. This cohesive agenda aims to guide future investigation and drive
the responsible, human-centered integration of GenAI into agile practices.

</details>


### [6] [Rethinking Testing for LLM Applications: Characteristics, Challenges, and a Lightweight Interaction Protocol](https://arxiv.org/abs/2508.20737)
*Wei Ma,Yixiao Yang,Qiang Hu,Shi Ying,Zhi Jin,Bo Du,Zhenchang Xing,Tianlin Li,Junjie Shi,Yang Liu,Linxiao Jiang*

Main category: cs.SE

TL;DR: 该论文提出了一个三层架构来分析LLM应用的质量保证挑战，并提出了四种协作策略和AICL协议来支持标准化测试。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型应用的非确定性、动态性和上下文依赖性给质量保证带来了根本性挑战，需要新的测试方法和框架。

Method: 将LLM应用分解为系统外壳层、提示编排层和LLM推理核心三层架构，分析传统测试方法在各层的适用性，并提出保留、转换、集成和运行时四种协作策略。

Result: 提出了AICL协议用于AI代理间通信，具有测试导向特性，易于集成到现有代理框架中。

Conclusion: 需要结合部署前验证和运行时监控的闭环可信质量保证框架，为LLM应用测试的标准化和工具化提供实践指导。

Abstract: Applications of Large Language Models~(LLMs) have evolved from simple text
generators into complex software systems that integrate retrieval augmentation,
tool invocation, and multi-turn interactions. Their inherent non-determinism,
dynamism, and context dependence pose fundamental challenges for quality
assurance. This paper decomposes LLM applications into a three-layer
architecture: \textbf{\textit{System Shell Layer}}, \textbf{\textit{Prompt
Orchestration Layer}}, and \textbf{\textit{LLM Inference Core}}. We then assess
the applicability of traditional software testing methods in each layer:
directly applicable at the shell layer, requiring semantic reinterpretation at
the orchestration layer, and necessitating paradigm shifts at the inference
core. A comparative analysis of Testing AI methods from the software
engineering community and safety analysis techniques from the AI community
reveals structural disconnects in testing unit abstraction, evaluation metrics,
and lifecycle management. We identify four fundamental differences that
underlie 6 core challenges. To address these, we propose four types of
collaborative strategies (\emph{Retain}, \emph{Translate}, \emph{Integrate},
and \emph{Runtime}) and explore a closed-loop, trustworthy quality assurance
framework that combines pre-deployment validation with runtime monitoring.
Based on these strategies, we offer practical guidance and a protocol proposal
to support the standardization and tooling of LLM application testing. We
propose a protocol \textbf{\textit{Agent Interaction Communication Language}}
(AICL) that is used to communicate between AI agents. AICL has the
test-oriented features and is easily integrated in the current agent framework.

</details>


### [7] [From Law to Gherkin: A Human-Centred Quasi-Experiment on the Quality of LLM-Generated Behavioural Specifications from Food-Safety Regulations](https://arxiv.org/abs/2508.20744)
*Shabnam Hassani,Mehrdad Sabetzadeh,Daniel Amyot*

Main category: cs.SE

TL;DR: LLMs能够从法律文本中生成高质量的Gherkin行为规范，显著减少人工工作量，为合规性需求工程提供有效自动化解决方案。


<details>
  <summary>Details</summary>
Motivation: 法律文本采用技术中立的语言编写，工程师手动创建合规性需求文档工作量大、易出错且需要专业知识。生成式AI特别是LLMs为自动化这一过程提供了可能。

Method: 采用准实验设计，让10名参与者评估Claude和Llama从食品安全法规生成的60个Gherkin规范。每个参与者评估12个规范，从相关性、清晰度、完整性、单一性和时间节省五个标准进行评分。

Result: 相关性75%获得最高评分，20%次高；清晰度90%最高；完整性75%最高；单一性82%最高；时间节省68%最高。Llama在清晰度、完整性和时间节省方面略优，Claude在单一性方面更强。

Conclusion: LLMs能够从法律文本生成高质量的Gherkin规范，减少人工工作量，为实施、保证和测试生成提供有用的结构化工件。

Abstract: Context: Laws and regulations increasingly affect software design and quality
assurance, but legal texts are written in technology-neutral language. This
creates challenges for engineers who must develop compliance artifacts such as
requirements and acceptance criteria. Manual creation is labor-intensive,
error-prone, and requires domain expertise. Advances in Generative AI (GenAI),
especially Large Language Models (LLMs), offer a way to automate deriving such
artifacts.
  Objective: We present the first systematic human-subject study of LLMs'
ability to derive behavioral specifications from legal texts using a
quasi-experimental design. These specifications translate legal requirements
into a developer-friendly form.
  Methods: Ten participants evaluated specifications generated from food-safety
regulations by Claude and Llama. Using Gherkin, a structured BDD language, 60
specifications were produced. Each participant assessed 12 across five
criteria: Relevance, Clarity, Completeness, Singularity, and Time Savings. Each
specification was reviewed by two participants, yielding 120 assessments.
  Results: For Relevance, 75% of ratings were highest and 20% second-highest.
Clarity reached 90% highest. Completeness: 75% highest, 19% second.
Singularity: 82% highest, 12% second. Time Savings: 68% highest, 24% second. No
lowest ratings occurred. Mann-Whitney U tests showed no significant differences
across participants or models. Llama slightly outperformed Claude in Clarity,
Completeness, and Time Savings, while Claude was stronger in Singularity.
Feedback noted hallucinations and omissions but confirmed the utility of the
specifications.
  Conclusion: LLMs can generate high-quality Gherkin specifications from legal
texts, reducing manual effort and providing structured artifacts useful for
implementation, assurance, and test generation.

</details>


### [8] [Towards an Architectural Perspective for Sustainability: Bundle the Needs from Industry](https://arxiv.org/abs/2508.20774)
*Markus Funke,Patricia Lago*

Main category: cs.SE

TL;DR: 这篇论文提出了一种可持续性视角的概念，通过架构视角知识来结构化地应对软件设计中的可持续性质量问题，为架构师提供指导。


<details>
  <summary>Details</summary>
Motivation: 软件系统中可持续性质量属性越来越重要，但架构师缺乏结构化指导来在设计阶段有效处理这些问题。

Method: 通过雪球式文献绵展和专家焦点小组的方式，研究者形成了可持续性视角的概念，包含关注点、活动、策略、坑阱和检查单等元素。

Result: 研究确认了各种视角元素在实践中的相关性，并指出了形成符合产业需求的可持续性视角的含义。

Conclusion: 可持续性视角的概念为架构师提供了一种结构化方法来处理软件设计中的可持续性问题，该方法独立于架构框架和行业背景，具有广泛的应用前景。

Abstract: Sustainability is increasingly recognized as an emerging quality property in
software-intensive systems, yet architects lack structured guidance to address
it effectively throughout the software design phase. Architectural
perspectives-an architectural knowledge artifact composed of concerns,
activities, tactics, pitfalls, and checklists-offer a promising approach to
tackle such emerging quality properties across architectural views and are also
independent of architecture frameworks and industry contexts. In this paper, we
present a sustainability perspective vision, i.e., a revised notion of
architectural perspective meant to be filled with its own elements to target
sustainability concerns. We formulate our sustainability perspective vision
through evidence from applying snowballing to seminal literature and from
conducting a focus group with experts in the field. Our findings confirm the
relevance of the different perspective elements in practice and highlight
implications for shaping a sustainability perspective that meets industrial
needs.

</details>


### [9] [Automated Test Oracles for Flaky Cyber-Physical System Simulators: Approach and Evaluation](https://arxiv.org/abs/2508.20902)
*Baharin A. Jodat,Khouloud Gaaloul,Mehrdad Sabetzadeh,Shiva Nejati*

Main category: cs.SE

TL;DR: 提出基于断言的测试预言方法，使用遗传编程和决策树技术生成逻辑断言，无需执行测试即可预测测试结果，有效应对CPS模拟器的不可靠性。


<details>
  <summary>Details</summary>
Motivation: CPS模拟测试成本高且结果不稳定，需要不依赖系统执行的自动化测试预言来降低测试成本，同时要求预言具有可解释性和对模拟器不稳定性的鲁棒性。

Method: 使用遗传编程（GP）和决策树（DT）/决策规则（DR）两种方法生成基于断言的测试预言。GP方法采用Ochiai、Tarantula和Naish等频谱故障定位公式作为适应度函数。

Result: 基于GP与Ochiai方法生成的测试预言准确率显著高于其他方法，且在存在不稳定性的系统中仍保持准确性优势，平均准确率变化仅为4%。

Conclusion: 基于GP与Ochiai的断言测试预言方法能够有效生成高准确性、鲁棒性强的测试预言，适用于航空航天、网络和自动驾驶等领域的CPS测试。

Abstract: Simulation-based testing of cyber-physical systems (CPS) is costly due to the
time-consuming execution of CPS simulators. In addition, CPS simulators may be
flaky, leading to inconsistent test outcomes and requiring repeated test
re-execution for reliable test verdicts. Automated test oracles that do not
require system execution are therefore crucial for reducing testing costs.
Ideally, such test oracles should be interpretable to facilitate human
understanding of test verdicts, and they must be robust against the potential
flakiness of CPS simulators. In this article, we propose assertion-based test
oracles for CPS as sets of logical and arithmetic predicates defined over the
inputs of the system under test. Given a test input, our assertion-based test
oracle determines, without requiring test execution, whether the test passes,
fails, or if the oracle is inconclusive in predicting a verdict. We describe
two methods for generating assertion-based test oracles: one using genetic
programming~(GP) that employs well-known spectrum-based fault localization
(SBFL) ranking formulas, namely Ochiai, Tarantula, and Naish, as fitness
functions; and the other using decision trees (DT) and decision rules (DR). We
evaluate our assertion-based test oracles through case studies in the domains
of aerospace, networking and autonomous driving. We show that test oracles
generated using GP with Ochiai are significantly more accurate than those
obtained using GP with Tarantula and Naish or using DT or DR. Moreover, this
accuracy advantage remains even when accounting for the flakiness of the system
under test. We further show that the assertion-based test oracles generated by
GP with Ochiai are robust against flakiness with only 4% average variation in
their accuracy results across four different network and autonomous driving
systems with flaky behaviours.

</details>


### [10] [Deep Learning Based Concurrency Bug Detection and Localization](https://arxiv.org/abs/2508.20911)
*Zuocheng Feng,Kaiwen Zhang,Miaomiao Wang,Yiming Cheng,Yuandao Cai,Xiaofeng Li,Guanjun Liu*

Main category: cs.SE

TL;DR: 提出了一种基于预训练模型和异构图神经网络的新方法，通过构建并发感知代码属性图来检测和定位并发bug，在准确率和召回率上显著优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法面临三个主要限制：缺乏大规模专用并发bug数据集、并发语义表示不足、二元分类无法提供细粒度调试信息

Method: 构建专用并发bug数据集，将预训练模型与异构图神经网络结合，使用新的并发感知代码属性图(CCPG)表征并发语义，并采用SubgraphX方法精确定位bug到具体代码行

Result: 在多样化评估设置中，相比最先进方法平均提升10%的准确率和精确率，26%的召回率

Conclusion: 该方法有效解决了并发bug检测和定位问题，通过更好的语义表征和细粒度分析显著提升了检测性能

Abstract: Concurrency bugs, caused by improper synchronization of shared resources in
multi-threaded or distributed systems, are notoriously hard to detect and thus
compromise software reliability and security. The existing deep learning
methods face three main limitations. First, there is an absence of large and
dedicated datasets of diverse concurrency bugs for them. Second, they lack
sufficient representation of concurrency semantics. Third, binary
classification results fail to provide finer-grained debug information such as
precise bug lines. To address these problems, we propose a novel method for
effective concurrency bug detection as well as localization. We construct a
dedicated concurrency bug dataset to facilitate model training and evaluation.
We then integrate a pre-trained model with a heterogeneous graph neural network
(GNN), by incorporating a new Concurrency-Aware Code Property Graph (CCPG) that
concisely and effectively characterizes concurrency semantics. To further
facilitate debugging, we employ SubgraphX, a GNN-based interpretability method,
which explores the graphs to precisely localize concurrency bugs, mapping them
to specific lines of source code. On average, our method demonstrates an
improvement of 10\% in accuracy and precision and 26\% in recall compared to
state-of-the-art methods across diverse evaluation settings.

</details>


### [11] [ConfLogger: Enhance Systems' Configuration Diagnosability through Configuration Logging](https://arxiv.org/abs/2508.20977)
*Shiwen Shan,Yintong Huo,Yuxin Su,Zhining Wang,Dan Li,Zibin Zheng*

Main category: cs.SE

TL;DR: ConfLogger通过结合静态污点分析和LLM日志生成，增强软件配置诊断能力，在8个流行系统中显著提升错误定位准确性和诊断效率


<details>
  <summary>Details</summary>
Motivation: 现代可配置系统虽然提供灵活性，但带来了配置相关的问题。现有诊断方法主要关注故障后分析，但缺乏对软件是否提供足够故障信息的研究

Method: 1) 通过静态污点分析追踪配置相关数据流识别配置敏感代码段；2) 使用LLM分析配置代码上下文生成诊断日志语句

Result: 在30个静默配置错误场景中实现100%错误定位准确度，80%问题可通过暴露的配置信息直接解决。相比基线方法，覆盖率提升12-30%，精确度提升8.6%，召回率提升79.3%，F1值提升26.2%

Conclusion: ConfLogger有效增强了软件配置诊断能力，用户研究显示诊断时间缩短1.25倍，故障排除准确率提升251.4%，证明了配置日志记录方法的实用性

Abstract: Modern configurable systems offer customization via intricate configuration
spaces, yet such flexibility introduces pervasive configuration-related issues
such as misconfigurations and latent softwarebugs. Existing diagnosability
supports focus on post-failure analysis of software behavior to identify
configuration issues, but none of these approaches look into whether the
software clue sufficient failure information for diagnosis. To fill in the
blank, we propose the idea of configuration logging to enhance existing logging
practices at the source code level. We develop ConfLogger, the first tool that
unifies configuration-aware static taint analysis with LLM-based log generation
to enhance software configuration diagnosability. Specifically, our method 1)
identifies configuration-sensitive code segments by tracing
configuration-related data flow in the whole project, and 2) generates
diagnostic log statements by analyzing configuration code contexts. Evaluation
results on eight popular software systems demonstrate the effectiveness of
ConfLogger to enhance configuration diagnosability. Specifically,
ConfLogger-enhanced logs successfully aid a log-based misconfiguration
diagnosis tool to achieve 100% accuracy on error localization in 30 silent
misconfiguration scenarios, with 80% directly resolvable through explicit
configuration information exposed. In addition, ConfLogger achieves 74%
coverage of existing logging points, outperforming baseline LLM-based loggers
by 12% and 30%. It also gains 8.6% higher in precision, 79.3% higher in recall,
and 26.2% higher in F1 compared to the state-of-the-art baseline in terms of
variable logging while also augmenting diagnostic value. A controlled user
study on 22 cases further validated its utility, speeding up diagnostic time by
1.25x and improving troubleshooting accuracy by 251.4%.

</details>


### [12] [Dynamics of Gender Bias in Software Engineering](https://arxiv.org/abs/2508.21050)
*Thomas J. Misa*

Main category: cs.SE

TL;DR: 该论文分析了软件工程领域中的性别偏见问题，通过调查领域起源、五位领袖人物以及定量分析ICSE会议作者性别比例，发现该领域存在长期的性别排除现象。


<details>
  <summary>Details</summary>
Motivation: 软件工程作为工程学和计算机科学的交叉领域，可能继承了这两个领域中存在的性别偏见。研究者想要探索软件工程领域是否存在系统性的性别不平等问题。

Method: 1. 调查软件工程领域的起源和对工程专业性的重视
2. 介绍五位领域领袖人物
3. 分析软件工程领域最近对性别问题的关注
4. 定量分析1976-2010年间ICSE会议作者的性别比例

Result: 研究发现在ICSE会议的历史中，存在十二个年份证据显示出统计上显著的性别排除现象，说明软件工程领域存在持续的性别不平等问题。

Conclusion: 软件工程领域确实存在系统性的性别偏见，需要重视并提出相应政策建议来解决这些问题，以促进领域的多样性和包容性。

Abstract: The field of software engineering is embedded in both engineering and
computer science, and may embody gender biases endemic to both. This paper
surveys software engineering's origins and its long-running attention to
engineering professionalism, profiling five leaders; it then examines the
field's recent attention to gender issues and gender bias. It next
quantitatively analyzes women's participation as research authors in the
field's leading International Conference of Software Engineering (1976-2010),
finding a dozen years with statistically significant gender exclusion. Policy
dimensions of research on gender bias in computing are suggested.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [13] [AI Propaganda factories with language models](https://arxiv.org/abs/2508.20186)
*Lukasz Olejnik*

Main category: cs.CR

TL;DR: 小型语言模型可在普通硬件上生成连贯的政治宣传内容，自动化影响力操作已触手可及，防御策略需转向对话检测和协调基础设施破坏


<details>
  <summary>Details</summary>
Motivation: 研究AI驱动的自动化影响力操作的可行性和特征，特别是小型语言模型在政治宣传内容生成方面的能力

Method: 使用小型语言模型生成人物角色驱动的政治信息，并通过自动化评估而非人工评分来分析内容特征和行为模式

Result: 发现人物角色设计比模型身份更能解释行为；在对抗性回复情境下，意识形态坚持加强且极端内容增加；证明完全自动化的影响力内容生产已经可行

Conclusion: 自动化影响力操作已成为现实，防御策略需要从限制模型访问转向对话中心的检测和协调基础设施的破坏，操作的一致性反而提供了检测特征

Abstract: AI-powered influence operations can now be executed end-to-end on commodity
hardware. We show that small language models produce coherent, persona-driven
political messaging and can be evaluated automatically without human raters.
Two behavioural findings emerge. First, persona-over-model: persona design
explains behaviour more than model identity. Second, engagement as a stressor:
when replies must counter-arguments, ideological adherence strengthens and the
prevalence of extreme content increases. We demonstrate that fully automated
influence-content production is within reach of both large and small actors.
Consequently, defence should shift from restricting model access towards
conversation-centric detection and disruption of campaigns and coordination
infrastructure. Paradoxically, the very consistency that enables these
operations also provides a detection signature.

</details>


### [14] [FlowMalTrans: Unsupervised Binary Code Translation for Malware Detection Using Flow-Adapter Architecture](https://arxiv.org/abs/2508.20212)
*Minghao Hu,Junzhe Wang,Weisen Zhao,Qiang Zeng,Lannan Luo*

Main category: cs.CR

TL;DR: 提出基于神经机器翻译和标准化流的跨指令集架构恶意软件检测方法，通过将其他架构的恶意代码翻译到X86-64架构，实现在单一架构上训练模型即可检测多架构恶意软件


<details>
  <summary>Details</summary>
Motivation: 随着针对IoT设备的网络攻击增加，跨多种指令集架构的恶意软件日益增多，但为每个架构收集和标注足够样本构建数据集成本高昂

Method: 结合神经机器翻译(NMT)和标准化流(NFs)技术，将目标架构的恶意代码翻译到拥有充足样本的X86-64架构，利用单一架构训练模型实现跨架构检测

Result: 该方法显著减少了数据收集工作量，实现了使用单一架构训练模型就能检测多种指令集架构的恶意软件

Conclusion: 提出的跨架构翻译方法有效解决了多架构恶意软件检测中的数据稀缺问题，为恶意软件检测提供了高效且可扩展的解决方案

Abstract: Applying deep learning to malware detection has drawn great attention due to
its notable performance. With the increasing prevalence of cyberattacks
targeting IoT devices, there is a parallel rise in the development of malware
across various Instruction Set Architectures (ISAs). It is thus important to
extend malware detection capacity to multiple ISAs. However, training a deep
learning-based malware detection model usually requires a large number of
labeled malware samples. The process of collecting and labeling sufficient
malware samples to build datasets for each ISA is labor-intensive and
time-consuming. To reduce the burden of data collection, we propose to leverage
the ideas of Neural Machine Translation (NMT) and Normalizing Flows (NFs) for
malware detection. Specifically, when dealing with malware in a certain ISA, we
translate it to an ISA with sufficient malware samples (like X86-64). This
allows us to apply a model trained on one ISA to analyze malware from another
ISA. Our approach reduces the data collection effort by enabling malware
detection across multiple ISAs using a model trained on a single ISA.

</details>


### [15] [Robustness Assessment and Enhancement of Text Watermarking for Google's SynthID](https://arxiv.org/abs/2508.20228)
*Xia Han,Qi Li,Jianbing Ni,Mohammad Zulkernine*

Main category: cs.CR

TL;DR: SynGuard是一个混合水印框架，结合语义信息检索和SynthID-Text的概率水印机制，在词法和语义层面双重嵌入水印，显著提高了对抗改写攻击的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM水印方法如SynthID-Text在面对保持语义的攻击（如改写、复制粘贴修改、回译）时存在脆弱性，水印可检测性会显著下降。

Method: 提出SynGuard混合框架，将语义信息检索(SIR)的语义对齐能力与SynthID-Text的概率水印机制相结合，在词法和语义两个层面联合嵌入水印。

Result: 在多种攻击场景下的实验表明，SynGuard相比SynthID-Text平均提高11.1%的F1分数水印恢复率。

Conclusion: 语义感知水印技术在抵抗现实世界篡改方面具有有效性，证明了双重水印嵌入策略的优越性。

Abstract: Recent advances in LLM watermarking methods such as SynthID-Text by Google
DeepMind offer promising solutions for tracing the provenance of AI-generated
text. However, our robustness assessment reveals that SynthID-Text is
vulnerable to meaning-preserving attacks, such as paraphrasing, copy-paste
modifications, and back-translation, which can significantly degrade watermark
detectability. To address these limitations, we propose SynGuard, a hybrid
framework that combines the semantic alignment strength of Semantic Information
Retrieval (SIR) with the probabilistic watermarking mechanism of SynthID-Text.
Our approach jointly embeds watermarks at both lexical and semantic levels,
enabling robust provenance tracking while preserving the original meaning.
Experimental results across multiple attack scenarios show that SynGuard
improves watermark recovery by an average of 11.1\% in F1 score compared to
SynthID-Text. These findings demonstrate the effectiveness of semantic-aware
watermarking in resisting real-world tampering. All code, datasets, and
evaluation scripts are publicly available at:
https://github.com/githshine/SynGuard.

</details>


### [16] [Network-Level Prompt and Trait Leakage in Local Research Agents](https://arxiv.org/abs/2508.20282)
*Hyejun Jeong,Mohammadreze Teymoorianfard,Abhinav Kumar,Amir Houmansadr,Eugene Badasarian*

Main category: cs.CR

TL;DR: 研究发现基于语言模型的网络研究代理（WRA）存在网络层面的隐私泄露风险，被动网络攻击者可通过分析访问域名的时序相关性，恢复73%的用户提示内容和19个潜在特征。


<details>
  <summary>Details</summary>
Motivation: Web和研究代理（WRA）被组织和个人本地部署用于隐私、法律或财务目的，但这些代理访问70-140个域名时的时序相关性使其面临独特的指纹识别攻击风险。

Method: 构建基于用户搜索查询和合成角色生成查询的WRA追踪数据集，定义OBELS行为指标评估原始提示与推断提示的相似性，并在部分可观察性和噪声条件下测试攻击效果。

Result: 攻击能够恢复用户提示中73%的功能和领域知识，在多会话设置中准确恢复32个潜在特征中的19个。限制域名多样性或混淆追踪的缓解策略可将攻击效果平均降低29%，且对实用性影响可忽略。

Conclusion: WRA系统存在严重的网络元数据泄露风险，需要采取有效的缓解措施来保护用户隐私，同时保持系统实用性。

Abstract: We show that Web and Research Agents (WRAs) -- language model-based systems
that investigate complex topics on the Internet -- are vulnerable to inference
attacks by passive network adversaries such as ISPs. These agents could be
deployed \emph{locally} by organizations and individuals for privacy, legal, or
financial purposes. Unlike sporadic web browsing by humans, WRAs visit
$70{-}140$ domains with distinguishable timing correlations, enabling unique
fingerprinting attacks.
  Specifically, we demonstrate a novel prompt and user trait leakage attack
against WRAs that only leverages their network-level metadata (i.e., visited IP
addresses and their timings). We start by building a new dataset of WRA traces
based on user search queries and queries generated by synthetic personas. We
define a behavioral metric (called OBELS) to comprehensively assess similarity
between original and inferred prompts, showing that our attack recovers over
73\% of the functional and domain knowledge of user prompts. Extending to a
multi-session setting, we recover up to 19 of 32 latent traits with high
accuracy. Our attack remains effective under partial observability and noisy
conditions. Finally, we discuss mitigation strategies that constrain domain
diversity or obfuscate traces, showing negligible utility impact while reducing
attack effectiveness by an average of 29\%.

</details>


### [17] [Surveying the Operational Cybersecurity and Supply Chain Threat Landscape when Developing and Deploying AI Systems](https://arxiv.org/abs/2508.20307)
*Michael R Smith,Joe Ingram*

Main category: cs.CR

TL;DR: AI技术的发展带来了新的网络安全风险，攻击者从传统目标转向撩取AI输出，需要专门的安全框架来应对这些新兴威胁。


<details>
  <summary>Details</summary>
Motivation: AI技术的快速发展引入了传统安全评估忽视的新攻击面，攻击者目标从传统权限提升转向撩取AI系统输出，需要提高对这些新兴威胁的认知。

Method: 探讨AI生命周期中的运营安全和供应链风险，分析历史漏洞利用案例，提供实践经验见解，建议专门的安全框架。

Result: 识别了AI系统中的独特安全风险，如系统性能降级、错误输出泛滥、模型准确性下降等，为组织提供了风险认知和防护建议。

Conclusion: 通过提高对AI系统特有安全风险的认识，建立适配的安全框架，组织可以更好地保护AI系统，确保其可靠性和弹性。

Abstract: The rise of AI has transformed the software and hardware landscape, enabling
powerful capabilities through specialized infrastructures, large-scale data
storage, and advanced hardware. However, these innovations introduce unique
attack surfaces and objectives which traditional cybersecurity assessments
often overlook. Cyber attackers are shifting their objectives from conventional
goals like privilege escalation and network pivoting to manipulating AI outputs
to achieve desired system effects, such as slowing system performance, flooding
outputs with false positives, or degrading model accuracy. This paper serves to
raise awareness of the novel cyber threats that are introduced when
incorporating AI into a software system. We explore the operational
cybersecurity and supply chain risks across the AI lifecycle, emphasizing the
need for tailored security frameworks to address evolving threats in the
AI-driven landscape. We highlight previous exploitations and provide insights
from working in this area. By understanding these risks, organizations can
better protect AI systems and ensure their reliability and resilience.

</details>


### [18] [MindGuard: Tracking, Detecting, and Attributing MCP Tool Poisoning Attack via Decision Dependence Graph](https://arxiv.org/abs/2508.20412)
*Zhiqiang Wang,Junyang Zhang,Guanquan Shi,HaoRan Cheng,Yunhao Yao,Kaiwen Guo,Haohua Du,Xiang-Yang Li*

Main category: cs.CR

TL;DR: MindGuard是一个针对LLM代理的决策级防护系统，通过注意力机制追踪工具调用决策，有效检测和溯源工具投毒攻击(TPA)，在真实数据集上达到94%-99%的检测精度和95%-100%的溯源准确率。


<details>
  <summary>Details</summary>
Motivation: 随着模型上下文协议(MCP)的普及，工具投毒攻击(TPA)成为新威胁，现有基于行为分析的防御方法对此类攻击无效，因为投毒工具可能不被执行而无法留下行为痕迹。

Method: 利用LLM注意力机制与工具调用决策的强相关性，构建决策依赖图(DDG)来建模推理过程，设计基于图的异常分析机制来检测和溯源TPA攻击。

Result: 在真实数据集上的实验表明，MindGuard检测投毒调用的平均精度达到94%-99%，溯源准确率95%-100%，处理时间低于1秒且无额外token成本。

Conclusion: MindGuard提供了一种有效的决策级防护方案，DDG作为经典程序依赖图(PDG)的适配，为在决策层面应用传统安全策略奠定了基础。

Abstract: The Model Context Protocol (MCP) is increasingly adopted to standardize the
interaction between LLM agents and external tools. However, this trend
introduces a new threat: Tool Poisoning Attacks (TPA), where tool metadata is
poisoned to induce the agent to perform unauthorized operations. Existing
defenses that primarily focus on behavior-level analysis are fundamentally
ineffective against TPA, as poisoned tools need not be executed, leaving no
behavioral trace to monitor.
  Thus, we propose MindGuard, a decision-level guardrail for LLM agents,
providing provenance tracking of call decisions, policy-agnostic detection, and
poisoning source attribution against TPA. While fully explaining LLM decision
remains challenging, our empirical findings uncover a strong correlation
between LLM attention mechanisms and tool invocation decisions. Therefore, we
choose attention as an empirical signal for decision tracking and formalize
this as the Decision Dependence Graph (DDG), which models the LLM's reasoning
process as a weighted, directed graph where vertices represent logical concepts
and edges quantify the attention-based dependencies. We further design robust
DDG construction and graph-based anomaly analysis mechanisms that efficiently
detect and attribute TPA attacks. Extensive experiments on real-world datasets
demonstrate that MindGuard achieves 94\%-99\% average precision in detecting
poisoned invocations, 95\%-100\% attribution accuracy, with processing times
under one second and no additional token cost. Moreover, DDG can be viewed as
an adaptation of the classical Program Dependence Graph (PDG), providing a
solid foundation for applying traditional security policies at the decision
level.

</details>


### [19] [Federated Learning for Large Models in Medical Imaging: A Comprehensive Review](https://arxiv.org/abs/2508.20414)
*Mengyu Sun,Ziyuan Yang,Yongqiang Huang,Hui Yu,Yingyu Chen,Shuren Qi,Andrew Beng Jin Teoh,Yi Zhang*

Main category: cs.CR

TL;DR: 本综述探讨了联邦学习在医学影像全流程分析中的应用，包括上游重建任务和下游临床诊断任务，解决了数据隐私保护下的分布式协作训练问题。


<details>
  <summary>Details</summary>
Motivation: 医学AI模型开发面临大规模集中数据训练的挑战，受限于患者隐私法规和数据共享限制，需要隐私保护的分布式训练解决方案。

Method: 采用联邦学习框架，在医学影像全流程中实现分布式协作训练：上游用于CT/MRI重建网络训练，下游用于肿瘤诊断和分割任务的持续模型更新。

Result: 联邦学习能够在不集中敏感数据的情况下，利用多机构碎片化数据集协同训练鲁棒的重建网络和诊断模型，提高通信效率并确保参数聚合安全。

Conclusion: 联邦学习为医学影像AI开发提供了有效的隐私保护解决方案，未来需要在通信效率、异构数据对齐和安全聚合等方面进一步研究发展。

Abstract: Artificial intelligence (AI) has demonstrated considerable potential in the
realm of medical imaging. However, the development of high-performance AI
models typically necessitates training on large-scale, centralized datasets.
This approach is confronted with significant challenges due to strict patient
privacy regulations and legal restrictions on data sharing and utilization.
These limitations hinder the development of large-scale models in medical
domains and impede continuous updates and training with new data. Federated
Learning (FL), a privacy-preserving distributed training framework, offers a
new solution by enabling collaborative model development across fragmented
medical datasets. In this survey, we review FL's contributions at two stages of
the full-stack medical analysis pipeline. First, in upstream tasks such as CT
or MRI reconstruction, FL enables joint training of robust reconstruction
networks on diverse, multi-institutional datasets, alleviating data scarcity
while preserving confidentiality. Second, in downstream clinical tasks like
tumor diagnosis and segmentation, FL supports continuous model updating by
allowing local fine-tuning on new data without centralizing sensitive images.
We comprehensively analyze FL implementations across the medical imaging
pipeline, from physics-informed reconstruction networks to diagnostic AI
systems, highlighting innovations that improve communication efficiency, align
heterogeneous data, and ensure secure parameter aggregation. Meanwhile, this
paper provides an outlook on future research directions, aiming to serve as a
valuable reference for the field's development.

</details>


### [20] [Breaking Diffusion with Cache: Exploiting Approximate Caches in Diffusion Models](https://arxiv.org/abs/2508.20424)
*Desen Sun,Shuncheng Jie,Sihang Liu*

Main category: cs.CR

TL;DR: 本文系统评估了扩散模型近似缓存机制的安全漏洞，发现了三种远程攻击方式：隐蔽信道信息传递、提示词窃取和缓存投毒攻击，揭示了近似缓存严重的安全风险


<details>
  <summary>Details</summary>
Motivation: 扩散模型计算成本高昂，近似缓存通过重用相似提示词的中间状态来优化性能，但这种优化破坏了用户间的隔离，引入了新的安全风险，需要全面评估这些安全漏洞

Method: 通过实验演示了三种攻击方式：1）利用特殊关键词建立远程隐蔽信道进行信息传递；2）基于缓存命中提示词恢复已缓存提示词的窃取攻击；3）通过嵌入攻击者标识进行缓存投毒，影响未来用户提示词渲染

Result: 成功实现了所有三种攻击，证明攻击者可以远程通过服务系统建立隐蔽信道（信息可保留数天）、窃取缓存提示词、并对被盗提示词进行投毒以在未来用户提示中渲染攻击者标识

Conclusion: 近似缓存机制存在严重的安全漏洞，攻击者可以远程实施多种攻击，破坏了用户隔离性，需要在设计此类优化方案时充分考虑安全防护措施

Abstract: Diffusion models are a powerful class of generative models that produce
content, such as images, from user prompts, but they are computationally
intensive. To mitigate this cost, recent academic and industry work has adopted
approximate caching, which reuses intermediate states from similar prompts in a
cache. While efficient, this optimization introduces new security risks by
breaking isolation among users. This work aims to comprehensively assess new
security vulnerabilities arising from approximate caching. First, we
demonstrate a remote covert channel established with the cache, where a sender
injects prompts with special keywords into the cache and a receiver can recover
that even after days, to exchange information. Second, we introduce a prompt
stealing attack using the cache, where an attacker can recover existing cached
prompts based on cache hit prompts. Finally, we introduce a poisoning attack
that embeds the attacker's logos into the previously stolen prompt, to render
them in future user prompts that hit the cache. These attacks are all performed
remotely through the serving system, which indicates severe security
vulnerabilities in approximate caching.

</details>


### [21] [Ransomware 3.0: Self-Composing and LLM-Orchestrated](https://arxiv.org/abs/2508.20444)
*Md Raz,Meet Udeshi,P. V. Sai Charan,Prashanth Krishnamurthy,Farshad Khorrami,Ramesh Karri*

Main category: cs.CR

TL;DR: 论文提出了Ransomware 3.0，这是首个利用大型语言模型自主策划、适应和执行勒索软件攻击生命周期的新型威胁模型和研究原型。


<details>
  <summary>Details</summary>
Motivation: 研究旨在揭示LLM可能被滥用于自动化勒索软件攻击的新型威胁，传统勒索软件需要预编写恶意代码，而LLM驱动的勒索软件可以在运行时动态生成恶意代码，具有更强的适应性和隐蔽性。

Method: 通过自动化推理、代码合成和上下文决策，构建了一个LLM编排的勒索软件原型。该系统仅需在二进制文件中嵌入自然语言提示，运行时由LLM动态合成恶意代码，产生适应执行环境的多态变体。

Result: 评估显示开源LLM能够生成功能性勒索软件组件，并在个人、企业和嵌入式环境中维持闭环执行。通过案例研究展示了Ransomware 3.0的行为信号和多级遥测数据。

Conclusion: 研究揭示了AI驱动的勒索软件新型威胁，需要开发更好的防御策略和政策执行机制来应对这种新型攻击方式。

Abstract: Using automated reasoning, code synthesis, and contextual decision-making, we
introduce a new threat that exploits large language models (LLMs) to
autonomously plan, adapt, and execute the ransomware attack lifecycle.
Ransomware 3.0 represents the first threat model and research prototype of
LLM-orchestrated ransomware. Unlike conventional malware, the prototype only
requires natural language prompts embedded in the binary; malicious code is
synthesized dynamically by the LLM at runtime, yielding polymorphic variants
that adapt to the execution environment. The system performs reconnaissance,
payload generation, and personalized extortion, in a closed-loop attack
campaign without human involvement. We evaluate this threat across personal,
enterprise, and embedded environments using a phase-centric methodology that
measures quantitative fidelity and qualitative coherence in each attack phase.
We show that open source LLMs can generate functional ransomware components and
sustain closed-loop execution across diverse environments. Finally, we present
behavioral signals and multi-level telemetry of Ransomware 3.0 through a case
study to motivate future development of better defenses and policy enforcements
to address novel AI-enabled ransomware attacks.

</details>


### [22] [Enhancing Resilience for IoE: A Perspective of Networking-Level Safeguard](https://arxiv.org/abs/2508.20504)
*Guan-Yan Yang,Jui-Ning Chen,Farn Wang,Kuo-Hui Yeh*

Main category: cs.CR

TL;DR: 这篇论文提出了一种基于图结构学习(GSL)的网络安全保护框架，用于防范能源互联网(IoE)中的激进攻击。该方法通过联合优化图拓扑和节点表征来提升网络的内在鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 能源互联网(IoE)的互联性使关键基础设施面临精细的网络威胁，这些威胁比一般IoT风险具有更高的公共安全后果，需要更鲁棒的安全解决方案。

Method: 提出了基于图结构学习(GSL)的安全保护框架，该方法联合优化图拓扑和节点表征来抵御激进性网络模型操控。通过概念概述、架构讨论和安全数据集案例研究进行验证。

Result: 证明GSL方法在鲁棒性方面显著优于其他代表性方法，为实践者提供了防范IoE网络变化攻击的可行路径。

Conclusion: 这项工作显示了GSL在提升未来IoE网络鲁棒性和可靠性方面的潜力，同时识别了该新兴研究领域的关键挑战并提出了未来研究方向。

Abstract: The Internet of Energy (IoE) integrates IoT-driven digital communication with
power grids to enable efficient and sustainable energy systems. Still, its
interconnectivity exposes critical infrastructure to sophisticated cyber
threats, including adversarial attacks designed to bypass traditional
safeguards. Unlike general IoT risks, IoE threats have heightened public safety
consequences, demanding resilient solutions. From the networking-level
safeguard perspective, we propose a Graph Structure Learning (GSL)-based
safeguards framework that jointly optimizes graph topology and node
representations to resist adversarial network model manipulation inherently.
Through a conceptual overview, architectural discussion, and case study on a
security dataset, we demonstrate GSL's superior robustness over representative
methods, offering practitioners a viable path to secure IoE networks against
evolving attacks. This work highlights the potential of GSL to enhance the
resilience and reliability of future IoE networks for practitioners managing
critical infrastructure. Lastly, we identify key open challenges and propose
future research directions in this novel research area.

</details>


### [23] [BridgeShield: Enhancing Security for Cross-chain Bridge Applications via Heterogeneous Graph Mining](https://arxiv.org/abs/2508.20517)
*Dan Lin,Shunfeng Lu,Ziyan Liu,Jiajing Wu,Junyuan Fang,Kaixin Lin,Bowen Song,Zibin Zheng*

Main category: cs.CR

TL;DR: BridgeShield是一个基于异构图注意力网络的跨链桥攻击检测框架，通过统一建模源链、链下协调和目标链，能够精确识别攻击行为，在真实攻击事件中达到92.58%的F1分数。


<details>
  <summary>Details</summary>
Motivation: 跨链桥由于设计缺陷和价值巨大成为黑客攻击的主要目标，现有检测方法主要关注单链行为，无法捕捉跨链语义，需要新的检测方案。

Method: 采用异构图注意力网络建模多类型实体和关系，构建统一异构图表示，结合元路径内注意力和元路径间注意力机制学习细粒度依赖关系和判别性跨链模式。

Result: 在51个真实跨链攻击事件上的实验显示，平均F1分数达到92.58%，比现有最佳基线方法提升24.39%。

Conclusion: BridgeShield是保护跨链桥安全和增强多链生态系统韧性的有效解决方案，验证了异构图建模在捕捉跨链语义方面的有效性。

Abstract: Cross-chain bridges play a vital role in enabling blockchain
interoperability. However, due to the inherent design flaws and the enormous
value they hold, they have become prime targets for hacker attacks. Existing
detection methods show progress yet remain limited, as they mainly address
single-chain behaviors and fail to capture cross-chain semantics. To address
this gap, we leverage heterogeneous graph attention networks, which are
well-suited for modeling multi-typed entities and relations, to capture the
complex execution semantics of cross-chain behaviors. We propose BridgeShield,
a detection framework that jointly models the source chain, off-chain
coordination, and destination chain within a unified heterogeneous graph
representation. BridgeShield incorporates intra-meta-path attention to learn
fine-grained dependencies within cross-chain paths and inter-meta-path
attention to highlight discriminative cross-chain patterns, thereby enabling
precise identification of attack behaviors. Extensive experiments on 51
real-world cross-chain attack events demonstrate that BridgeShield achieves an
average F1-score of 92.58%, representing a 24.39% improvement over
state-of-the-art baselines. These results validate the effectiveness of
BridgeShield as a practical solution for securing cross-chain bridges and
enhancing the resilience of multi-chain ecosystems.

</details>


### [24] [Bitcoin as an Interplanetary Monetary Standard with Proof-of-Transit Timestamping](https://arxiv.org/abs/2508.20591)
*Jose E. Puente,Carlos Puente*

Main category: cs.CR

TL;DR: 探索比特币作为地球和火星共享货币标准的可行性，提出Proof-of-Transit Timestamping (PoTT)原语解决星际高延迟通信问题，通过DTN和光学LEO星座网络实现跨行星比特币部署架构。


<details>
  <summary>Details</summary>
Motivation: 研究在物理通信约束下，如何使比特币成为地球和火星之间的共享货币标准，解决星际高延迟、间歇性连接带来的技术挑战。

Method: 提出PoTT原语提供加密的防篡改审计追踪；利用延迟/中断容忍网络(DTN)和光学低地球轨道(LEO)网状星座；设计头部优先复制架构、长期闪电通道和行星观察塔；通过联邦侧链或盲合并挖矿提交链实现安全结算。

Result: PoTT在不改变比特币共识或货币基础的情况下，显著提高了可靠性和可问责性；近期部署支持联邦化本地结算，长期可采用盲合并挖矿提交链；地球L1货币基础保持不变，火星可运行1:1锚定的提交链或联邦。

Conclusion: 比特币可以作为星际货币标准，PoTT技术解决了跨行星通信的核心挑战，但需要权衡联邦化和去中心化方案，同时在时间信标系统同时受损时PoTT会降级为管理断言而非加密时间锚定。

Abstract: We explore the feasibility of deploying Bitcoin as the shared monetary
standard between Earth and Mars, accounting for physical constraints of
interplanetary communication. We introduce a novel primitive, Proof-of-Transit
Timestamping (PoTT), to provide cryptographic, tamper-evident audit trails for
Bitcoin data across high-latency, intermittently-connected links. Leveraging
Delay/Disruption-Tolerant Networking (DTN) and optical low-Earth-orbit (LEO)
mesh constellations, we propose an architecture for header-first replication,
long-horizon Lightning channels with planetary watchtowers, and secure
settlement through federated sidechains or blind-merge-mined (BMM) commit
chains. We formalize PoTT, analyze its security model, and show how it
measurably improves reliability and accountability without altering Bitcoin
consensus or its monetary base. Near-term deployments favor strong federations
for local settlement; longer-term, blind-merge-mined commit chains (if adopted)
provide an alternative. The Earth L1 monetary base remains unchanged, while
Mars can operate a pegged commit chain or strong federation with 1:1 pegged
assets for local block production. For transparency, if both time-beacon
regimes are simultaneously compromised, PoTT-M2 (and PoTT generally) reduces to
administrative assertions rather than cryptographic time-anchoring.

</details>


### [25] [CyberSleuth: Autonomous Blue-Team LLM Agent for Web Attack Forensics](https://arxiv.org/abs/2508.20643)
*Stefano Fumero,Kai Huang,Matteo Boffa,Danilo Giordano,Marco Mellia,Zied Ben Houidi,Dario Rossi*

Main category: cs.CR

TL;DR: 本文提出了CyberSleuth，一个用于网络取证调查的自主LLM代理系统，能够分析数据包痕迹和应用日志来识别服务、漏洞和攻击成功率


<details>
  <summary>Details</summary>
Motivation: 当前LLM代理在网络安全领域主要应用于红队操作，而防御性用途如事件响应和取证调查研究相对较少且处于早期阶段

Method: 提出CyberSleuth代理系统，集成工具并设计不同代理架构，在20个复杂程度递增的事件场景中评估4种架构和6种LLM后端

Result: 在2025年的10个事件中，CyberSleuth正确识别确切CVE的比例达到80%；人类专家研究显示其报告完整、有用且连贯

Conclusion: CyberSleuth是性能最佳的设计，开源LLM DeepSeek R1表现良好，作者发布了基准测试和平台以促进防御性LLM研究的公平可复现评估

Abstract: Large Language Model (LLM) agents are powerful tools for automating complex
tasks. In cybersecurity, researchers have primarily explored their use in
red-team operations such as vulnerability discovery and penetration tests.
Defensive uses for incident response and forensics have received comparatively
less attention and remain at an early stage. This work presents a systematic
study of LLM-agent design for the forensic investigation of realistic web
application attacks. We propose CyberSleuth, an autonomous agent that processes
packet-level traces and application logs to identify the targeted service, the
exploited vulnerability (CVE), and attack success. We evaluate the consequences
of core design decisions - spanning tool integration and agent architecture -
and provide interpretable guidance for practitioners. We benchmark four agent
architectures and six LLM backends on 20 incident scenarios of increasing
complexity, identifying CyberSleuth as the best-performing design. In a
separate set of 10 incidents from 2025, CyberSleuth correctly identifies the
exact CVE in 80% of cases. At last, we conduct a human study with 22 experts,
which rated the reports of CyberSleuth as complete, useful, and coherent. They
also expressed a slight preference for DeepSeek R1, a good news for open source
LLM. To foster progress in defensive LLM research, we release both our
benchmark and the CyberSleuth platform as a foundation for fair, reproducible
evaluation of forensic agents.

</details>


### [26] [Multi-Agent Penetration Testing AI for the Web](https://arxiv.org/abs/2508.20816)
*Isaac David,Arthur Gervais*

Main category: cs.CR

TL;DR: MAPTA是一个多代理系统，用于自主进行Web应用安全评估，结合了大型语言模型编排、工具执行和端到端漏洞验证，在XBOW基准测试中达到76.9%的成功率，成本效益显著。


<details>
  <summary>Details</summary>
Motivation: AI驱动的开发平台使软件开发更易获得，但这也导致了安全审计的可扩展性危机，研究表明高达40%的AI生成代码包含漏洞，开发速度远超安全评估能力。

Method: MAPTA采用多代理系统架构，结合大型语言模型编排、工具执行和端到端漏洞验证，通过自主执行安全评估任务。

Result: 在104个挑战的XBOW基准测试中，MAPTA总体成功率76.9%，SSRF和配置错误漏洞表现完美，授权漏洞83%成功率，SQL注入83%，模板注入85%。成本分析显示总成本21.38美元，成功尝试中位成本0.073美元。

Conclusion: MAPTA在现实世界中发现了关键漏洞（包括RCE、命令注入等），成本效益优异（平均每个开源评估3.67美元），10个发现正在CVE审核中，证明了其在规模化安全评估中的实用性。

Abstract: AI-powered development platforms are making software creation accessible to a
broader audience, but this democratization has triggered a scalability crisis
in security auditing. With studies showing that up to 40% of AI-generated code
contains vulnerabilities, the pace of development now vastly outstrips the
capacity for thorough security assessment.
  We present MAPTA, a multi-agent system for autonomous web application
security assessment that combines large language model orchestration with
tool-grounded execution and end-to-end exploit validation. On the 104-challenge
XBOW benchmark, MAPTA achieves 76.9% overall success with perfect performance
on SSRF and misconfiguration vulnerabilities, 83% success on broken
authorization, and strong results on injection attacks including server-side
template injection (85%) and SQL injection (83%). Cross-site scripting (57%)
and blind SQL injection (0%) remain challenging. Our comprehensive cost
analysis across all challenges totals $21.38 with a median cost of $0.073 for
successful attempts versus $0.357 for failures. Success correlates strongly
with resource efficiency, enabling practical early-stopping thresholds at
approximately 40 tool calls or $0.30 per challenge.
  MAPTA's real-world findings are impactful given both the popularity of the
respective scanned GitHub repositories (8K-70K stars) and MAPTA's low average
operating cost of $3.67 per open-source assessment: MAPTA discovered critical
vulnerabilities including RCEs, command injections, secret exposure, and
arbitrary file write vulnerabilities. Findings are responsibly disclosed, 10
findings are under CVE review.

</details>


### [27] [Characterizing Trust Boundary Vulnerabilities in TEE Containers](https://arxiv.org/abs/2508.20962)
*Weijie Liu,Hongbo Chen,Shuo Huai,Zhen Xu,Wenhao Wang,Zhi Li,Zheli Liu*

Main category: cs.CR

TL;DR: 本文分析了现有TEE容器的隔离策略，开发了自动化分析工具来评估其隔离边界，发现多个关键安全漏洞，并提出了改进建议。


<details>
  <summary>Details</summary>
Motivation: TEE容器作为机密计算的关键中间件，需要保护应用程序免受恶意操作系统和编排接口的攻击，但现有容器的隔离效果需要系统评估。

Method: 设计了自动化分析器来精确识别和评估TEE容器的隔离边界，分析其隔离策略的有效性。

Result: 发现一些TEE容器由于设计和实现缺陷未能达到预期目标，存在信息泄露、回滚攻击、拒绝服务和Iago攻击等严重安全风险。

Conclusion: 基于研究发现分享了关键经验教训，指导开发更安全的容器解决方案，并讨论了TEE容器化设计的新兴趋势。

Abstract: Trusted Execution Environments (TEEs) have emerged as a cornerstone of
confidential computing, garnering significant attention from both academia and
industry. To enable the secure development, execution, and deployment, of
applications on TEE platforms, TEE containers have been introduced as
middleware solutions. These containers aim to shield applications from
potentially malicious operating systems and orchestration interfaces while
maintaining usability and reliability. In this paper, we analyze the isolation
strategies employed by existing TEE containers to protect secure applications.
To address the challenges in analyzing these interfaces, we designed an
automated analyzer to precisely identify and evaluate their isolation
boundaries. We observed that some TEE containers fail to achieve their intended
goals due to critical design and implementation flaws, such as information
leakage, rollback attacks, denial-of-service, and Iago attacks, which pose
significant security risks. Drawing from our findings, we share key lessons to
guide the development of more secure container solutions and discuss emerging
trends in TEE containerization design.

</details>


### [28] [JADES: A Universal Framework for Jailbreak Assessment via Decompositional Scoring](https://arxiv.org/abs/2508.20848)
*Junjie Chu,Mingjie Li,Ziqing Yang,Ye Leng,Chenhao Lin,Chao Shen,Michael Backes,Yun Shen,Yang Zhang*

Main category: cs.CR

TL;DR: JADES是一个通过分解评分来评估越狱攻击的框架，通过将有害问题分解为加权子问题并聚合评分，在二元评估中达到98.5%的人类评估一致性


<details>
  <summary>Details</summary>
Motivation: 现有越狱评估方法依赖错位的代理指标或简单整体判断，经常误解模型响应，导致与人类感知不一致的主观评估

Method: 自动将有害问题分解为加权子问题集，对每个子答案评分，并通过加权聚合得出最终决策，可选加入事实核查模块检测幻觉

Result: 在JailbreakQR基准测试中，JADES在二元设置下达到98.5%的人类评估一致性，比基线方法提升超过9%；重新评估显示现有攻击成功率被高估（如GPT-3.5-Turbo从93%降至69%）

Conclusion: JADES能够提供准确、一致且可解释的评估，为未来越狱攻击的测量提供了可靠基础

Abstract: Accurately determining whether a jailbreak attempt has succeeded is a
fundamental yet unresolved challenge. Existing evaluation methods rely on
misaligned proxy indicators or naive holistic judgments. They frequently
misinterpret model responses, leading to inconsistent and subjective
assessments that misalign with human perception. To address this gap, we
introduce JADES (Jailbreak Assessment via Decompositional Scoring), a universal
jailbreak evaluation framework. Its key mechanism is to automatically decompose
an input harmful question into a set of weighted sub-questions, score each
sub-answer, and weight-aggregate the sub-scores into a final decision. JADES
also incorporates an optional fact-checking module to strengthen the detection
of hallucinations in jailbreak responses. We validate JADES on JailbreakQR, a
newly introduced benchmark proposed in this work, consisting of 400 pairs of
jailbreak prompts and responses, each meticulously annotated by humans. In a
binary setting (success/failure), JADES achieves 98.5% agreement with human
evaluators, outperforming strong baselines by over 9%. Re-evaluating five
popular attacks on four LLMs reveals substantial overestimation (e.g., LAA's
attack success rate on GPT-3.5-Turbo drops from 93% to 69%). Our results show
that JADES could deliver accurate, consistent, and interpretable evaluations,
providing a reliable basis for measuring future jailbreak attacks.

</details>


### [29] [Publish to Perish: Prompt Injection Attacks on LLM-Assisted Peer Review](https://arxiv.org/abs/2508.20863)
*Matteo Gioele Collu,Umberto Salviati,Roberto Confalonieri,Mauro Conti,Giovanni Apruzzese*

Main category: cs.CR

TL;DR: 本文研究大型语言模型在科学同行评审中的隐藏提示注入攻击风险，作者通过在PDF中嵌入对抗性文本来操纵LLM生成的评审结果。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs越来越多地集成到科学同行评审过程中，需要评估其对隐藏提示注入攻击的脆弱性和可靠性，防止评审过程被操纵。

Method: 研究设计了三种威胁模型，创建了对人类不可见但能引导LLM输出的对抗性提示，通过用户研究获取代表性评审提示，并在不同LLM系统和论文上进行测试。

Result: 结果显示对抗性提示能可靠地误导LLM，有时会影响"诚实但懒惰"的评审员，同时提出了降低自动化内容检查可检测性的方法。

Conclusion: LLM在同行评审中存在被隐藏提示注入攻击的风险，需要开发更强大的防御机制来确保评审过程的完整性。

Abstract: Large Language Models (LLMs) are increasingly being integrated into the
scientific peer-review process, raising new questions about their reliability
and resilience to manipulation. In this work, we investigate the potential for
hidden prompt injection attacks, where authors embed adversarial text within a
paper's PDF to influence the LLM-generated review. We begin by formalising
three distinct threat models that envision attackers with different motivations
-- not all of which implying malicious intent. For each threat model, we design
adversarial prompts that remain invisible to human readers yet can steer an
LLM's output toward the author's desired outcome. Using a user study with
domain scholars, we derive four representative reviewing prompts used to elicit
peer reviews from LLMs. We then evaluate the robustness of our adversarial
prompts across (i) different reviewing prompts, (ii) different commercial
LLM-based systems, and (iii) different peer-reviewed papers. Our results show
that adversarial prompts can reliably mislead the LLM, sometimes in ways that
adversely affect a "honest-but-lazy" reviewer. Finally, we propose and
empirically assess methods to reduce detectability of adversarial prompts under
automated content checks.

</details>


### [30] [AI Agentic Vulnerability Injection And Transformation with Optimized Reasoning](https://arxiv.org/abs/2508.20866)
*Amine Lbath,Massih-Reza Amini,Aurelien Delaitre,Vadim Okun*

Main category: cs.CR

TL;DR: 提出了一种新颖的框架，通过多AI代理协作在安全C/C++代码中自动注入真实漏洞来生成数据集，在函数级别实现了89%-95%的漏洞注入成功率


<details>
  <summary>Details</summary>
Motivation: 传统漏洞检测方法存在可扩展性、适应性问题和高误报/漏报率，AI方法严重依赖训练数据质量，需要高质量漏洞数据集

Method: 使用多AI代理模拟专家推理，结合函数代理和传统代码分析工具，采用检索增强生成进行上下文接地，使用低秩权重近似进行高效模型微调

Result: 在3个不同基准测试的116个代码样本上，该方法在数据集准确性方面优于其他技术，函数级别漏洞注入成功率达到89%-95%

Conclusion: 该框架能够有效生成高质量的漏洞数据集，为AI驱动的漏洞检测和修复系统提供了可靠的数据基础

Abstract: The increasing complexity of software systems and the sophistication of
cyber-attacks have underscored the critical need for effective automated
vulnerability detection and repair systems. Traditional methods, such as static
program analysis, face significant challenges related to scalability,
adaptability, and high false-positive and false-negative rates. AI-driven
approaches, particularly those using machine learning and deep learning models,
show promise but are heavily reliant on the quality and quantity of training
data. This paper introduces a novel framework designed to automatically
introduce realistic, category-specific vulnerabilities into secure C/C++
codebases to generate datasets. The proposed approach coordinates multiple AI
agents that simulate expert reasoning, along with function agents and
traditional code analysis tools. It leverages Retrieval-Augmented Generation
for contextual grounding and employs Low-Rank approximation of weights for
efficient model fine-tuning. Our experimental study on 116 code samples from
three different benchmarks suggests that our approach outperforms other
techniques with regard to dataset accuracy, achieving between 89\% and 95\%
success rates in injecting vulnerabilities at function level.

</details>


### [31] [PromptSleuth: Detecting Prompt Injection via Semantic Intent Invariance](https://arxiv.org/abs/2508.20890)
*Mengxiao Wang,Yuxuan Zhang,Guofei Gu*

Main category: cs.CR

TL;DR: 提出了PromptSleuth防御框架，通过语义意图推理检测提示注入攻击，在新建的全面基准测试中显著优于现有防御方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在现实应用中面临提示注入攻击威胁，现有基准测试无法充分捕捉新兴攻击策略，现有防御方法在更全面的测试中表现不佳。

Method: 构建新的综合基准测试，包含多种操纵技术和多任务场景；提出PromptSleuth框架，基于任务级意图语义推理而非表面特征来检测提示注入。

Result: PromptSleuth在多个基准测试中一致优于现有防御方法，同时保持相当的运行时间和成本效率。

Conclusion: 基于意图的语义推理为防御LLM提示注入攻击提供了鲁棒、高效且可泛化的策略。

Abstract: Large Language Models (LLMs) are increasingly integrated into real-world
applications, from virtual assistants to autonomous agents. However, their
flexibility also introduces new attack vectors-particularly Prompt Injection
(PI), where adversaries manipulate model behavior through crafted inputs. As
attackers continuously evolve with paraphrased, obfuscated, and even multi-task
injection strategies, existing benchmarks are no longer sufficient to capture
the full spectrum of emerging threats.
  To address this gap, we construct a new benchmark that systematically extends
prior efforts. Our benchmark subsumes the two widely-used existing ones while
introducing new manipulation techniques and multi-task scenarios, thereby
providing a more comprehensive evaluation setting. We find that existing
defenses, though effective on their original benchmarks, show clear weaknesses
under our benchmark, underscoring the need for more robust solutions. Our key
insight is that while attack forms may vary, the adversary's intent-injecting
an unauthorized task-remains invariant. Building on this observation, we
propose PromptSleuth, a semantic-oriented defense framework that detects prompt
injection by reasoning over task-level intent rather than surface features.
Evaluated across state-of-the-art benchmarks, PromptSleuth consistently
outperforms existing defense while maintaining comparable runtime and cost
efficiency. These results demonstrate that intent-based semantic reasoning
offers a robust, efficient, and generalizable strategy for defending LLMs
against evolving prompt injection threats.

</details>


### [32] [Guarding Against Malicious Biased Threats (GAMBiT) Experiments: Revealing Cognitive Bias in Human-Subjects Red-Team Cyber Range Operations](https://arxiv.org/abs/2508.20963)
*Brandon Beltz,Jim Doty,Yvonne Fonken,Nikolos Gurney,Brett Israelsen,Nathan Lau,Stacy Marsella,Rachelle Thomas,Stoney Trent,Peggy Wu,Ya-Ting Yang,Quanyan Zhu*

Main category: cs.CR

TL;DR: GAMBiT项目发布了三个大规模红队网络靶场数据集，包含19-20名熟练攻击者在模拟企业网络中的多模态数据，用于攻击者行为建模和偏见感知分析研究。


<details>
  <summary>Details</summary>
Motivation: 为了支持攻击者行为建模、偏见感知分析和基准测试方法的研究，需要大规模、多模态的真实攻击数据。

Method: 在三个实验中，每实验19-20名熟练攻击者在SimSpace网络平台上进行2天8小时的自定进度操作，收集包括自我报告、操作笔记、终端历史、键盘记录、网络数据包捕获和NIDS警报等多模态数据。

Result: 发布了三个大规模数据集，包含标准化Kali Linux VM起始环境下的真实攻击操作数据，支持攻击者行为分析和网络安全研究。

Conclusion: GAMBiT项目提供了宝贵的大规模红队攻击数据集，为网络安全研究社区提供了重要的资源，可用于攻击者行为建模和偏见感知分析等研究方向。

Abstract: We present three large-scale human-subjects red-team cyber range datasets
from the Guarding Against Malicious Biased Threats (GAMBiT) project. Across
Experiments 1-3 (July 2024-March 2025), 19-20 skilled attackers per experiment
conducted two 8-hour days of self-paced operations in a simulated enterprise
network (SimSpace Cyber Force Platform) while we captured multi-modal data:
self-reports (background, demographics, psychometrics), operational notes,
terminal histories, keylogs, network packet captures (PCAP), and NIDS alerts
(Suricata). Each participant began from a standardized Kali Linux VM and
pursued realistic objectives (e.g., target discovery and data exfiltration)
under controlled constraints. Derivative curated logs and labels are included.
The combined release supports research on attacker behavior modeling,
bias-aware analytics, and method benchmarking. Data are available via IEEE
Dataport entries for Experiments 1-3.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [33] [ArgRAG: Explainable Retrieval Augmented Generation using Quantitative Bipolar Argumentation](https://arxiv.org/abs/2508.20131)
*Yuqicheng Zhu,Nico Potyka,Daniel Hernández,Yuan He,Zifeng Ding,Bo Xiong,Dongzhuoran Zhou,Evgeny Kharlamov,Steffen Staab*

Main category: cs.AI

TL;DR: ArgRAG是一个可解释、可争议的检索增强生成框架，使用定量双极论证框架替代黑盒推理，在保持高精度的同时显著提升透明度


<details>
  <summary>Details</summary>
Motivation: 传统RAG方法在高风险领域存在关键限制：对噪声或矛盾证据敏感，以及不透明的随机决策过程

Method: 使用定量双极论证框架(QBAF)从检索文档构建结构化推理，执行确定性推理和渐进语义分析

Result: 在两个事实验证基准测试(PubHealth和RAGuard)上实现了强大的准确性，同时显著提高了透明度

Conclusion: ArgRAG通过结构化论证框架成功解决了RAG系统的可解释性和可靠性问题，为高风险领域的可信AI决策提供了可行方案

Abstract: Retrieval-Augmented Generation (RAG) enhances large language models by
incorporating external knowledge, yet suffers from critical limitations in
high-stakes domains -- namely, sensitivity to noisy or contradictory evidence
and opaque, stochastic decision-making. We propose ArgRAG, an explainable, and
contestable alternative that replaces black-box reasoning with structured
inference using a Quantitative Bipolar Argumentation Framework (QBAF). ArgRAG
constructs a QBAF from retrieved documents and performs deterministic reasoning
under gradual semantics. This allows faithfully explaining and contesting
decisions. Evaluated on two fact verification benchmarks, PubHealth and
RAGuard, ArgRAG achieves strong accuracy while significantly improving
transparency.

</details>


### [34] [QAgent: An LLM-based Multi-Agent System for Autonomous OpenQASM programming](https://arxiv.org/abs/2508.20134)
*Zhenxiao Fu,Fan Chen,Lei Jiang*

Main category: cs.AI

TL;DR: QAgent是一个基于LLM的多智能体系统，能够完全自动化OpenQASM量子编程，通过多智能体协作显著提升量子代码生成准确性。


<details>
  <summary>Details</summary>
Motivation: NISQ设备已展现出量子优势，但OpenQASM编程复杂性阻碍了非专家用户的使用。现有LLM量子编程工具仅限于特定任务，缺乏通用自动化解决方案。

Method: 集成任务规划、上下文少样本学习、检索增强生成(RAG)、预定义生成工具和思维链(CoT)推理的多智能体系统，系统性提升编译和功能正确性。

Result: 在多个不同规模的LLM上，QAgent相比之前的静态LLM方法将QASM代码生成准确率提高了71.6%。

Conclusion: 该多智能体系统是民主化量子编程、弥合专业知识差距和加速量子计算实际应用的关键推动者。

Abstract: Noisy Intermediate-Scale Quantum (NISQ) devices have begun to exhibit early
quantum advantages on classically intractable problems, spanning physics
simulations to Gaussian boson sampling. Yet, realizing these benefits remains
challenging for non-experts, primarily due to the complexities of programming
in Open Quantum Assembly Language (OpenQASM). Although Large Language Model
(LLM)-based agents have shown promise in automating classical programming
workflows, their quantum counterparts have largely been restricted to
specialized tasks such as quantum chemistry or error correction. In this paper,
we present QAgent, an LLM-powered multi-agent system that fully automates
OpenQASM programming. By integrating task planning, in-context few-shot
learning, retrieval-augmented generation (RAG) for long-term context,
predefined generation tools, and chain-of-thought (CoT) reasoning, the agents
systematically improve both compilation and functional correctness. Our
evaluations demonstrate substantial improvements: across multiple LLMs of
varying sizes, QAgent enhances the accuracy of QASM code generation by 71.6\%
compared to previous static LLM-based approaches. We envision this multi-agent
system as a key enabler for democratizing quantum programming, bridging
expertise gaps, and accelerating the practical adoption of quantum computing.

</details>


### [35] [Array-Based Monte Carlo Tree Search](https://arxiv.org/abs/2508.20140)
*James Ragan,Fred Y. Hadaegh,Soon-Jo Chung*

Main category: cs.AI

TL;DR: 提出基于数组的MCTS实现方法，消除分支预测需求，在流水线处理器上实现更快性能，搜索深度扩展性提升2.8倍


<details>
  <summary>Details</summary>
Motivation: 传统MCTS实现存在分支预测问题，限制了在流水线处理器上的性能表现，需要更高效的实现方式来提升模拟速度

Method: 采用数组替代传统树结构实现UCT算法，保持原始算法逻辑的同时消除分支预测需求

Result: 在数值模拟中实现最高2.8倍的搜索深度扩展性提升，在流水线处理器上获得更快性能

Conclusion: 数组基础的MCTS实现是传统树结构实现的有效替代方案，能显著提升搜索性能，特别适合现代处理器架构

Abstract: Monte Carlo Tree Search is a popular method for solving decision making
problems. Faster implementations allow for more simulations within the same
wall clock time, directly improving search performance. To this end, we present
an alternative array-based implementation of the classic Upper Confidence
bounds applied to Trees algorithm. Our method preserves the logic of the
original algorithm, but eliminates the need for branch prediction, enabling
faster performance on pipelined processors, and up to a factor of 2.8 times
better scaling with search depth in our numerical simulations.

</details>


### [36] [The Anatomy of a Personal Health Agent](https://arxiv.org/abs/2508.20148)
*A. Ali Heydari,Ken Gu,Vidya Srinivas,Hong Yu,Zhihan Zhang,Yuwei Zhang,Akshay Paruchuri,Qian He,Hamid Palangi,Nova Hammerquist,Ahmed A. Metwally,Brent Winslow,Yubin Kim,Kumar Ayush,Yuzhe Yang,Girish Narayanswamy,Maxwell A. Xu,Jake Garrison,Amy Aremnto Lee,Jenny Vafeiadou,Ben Graef,Isaac R. Galatzer-Levy,Erik Schenck,Andrew Barakat,Javier Perez,Jacqueline Shreibati,John Hernandez,Anthony Z. Faranesh,Javier L. Prieto,Connor Heneghan,Yun Liu,Jiening Zhan,Mark Malhotra,Shwetak Patel,Tim Althoff,Xin Liu,Daniel McDuff,Xuhai "Orson" Xu*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Health is a fundamental pillar of human wellness, and the rapid advancements
in large language models (LLMs) have driven the development of a new generation
of health agents. However, the application of health agents to fulfill the
diverse needs of individuals in daily non-clinical settings is underexplored.
In this work, we aim to build a comprehensive personal health agent that is
able to reason about multimodal data from everyday consumer wellness devices
and common personal health records, and provide personalized health
recommendations. To understand end-users' needs when interacting with such an
assistant, we conducted an in-depth analysis of web search and health forum
queries, alongside qualitative insights from users and health experts gathered
through a user-centered design process. Based on these findings, we identified
three major categories of consumer health needs, each of which is supported by
a specialist sub-agent: (1) a data science agent that analyzes personal
time-series wearable and health record data, (2) a health domain expert agent
that integrates users' health and contextual data to generate accurate,
personalized insights, and (3) a health coach agent that synthesizes data
insights, guiding users using a specified psychological strategy and tracking
users' progress. Furthermore, we propose and develop the Personal Health Agent
(PHA), a multi-agent framework that enables dynamic, personalized interactions
to address individual health needs. To evaluate each sub-agent and the
multi-agent system, we conducted automated and human evaluations across 10
benchmark tasks, involving more than 7,000 annotations and 1,100 hours of
effort from health experts and end-users. Our work represents the most
comprehensive evaluation of a health agent to date and establishes a strong
foundation towards the futuristic vision of a personal health agent accessible
to everyone.

</details>


### [37] [IntentionReasoner: Facilitating Adaptive LLM Safeguards through Intent Reasoning and Selective Query Refinement](https://arxiv.org/abs/2508.20151)
*Yuanzhe Shen,Zisu Huang,Zhengkang Guo,Yide Liu,Guanxu Chen,Ruicheng Yin,Xiaoqing Zheng,Xuanjing Huang*

Main category: cs.AI

TL;DR: IntentionReasoner是一种新型安全防护机制，通过意图推理、多级安全分类和查询重写，在保证安全性的同时有效减少过度拒绝无害查询的问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在广泛部署的同时存在生成有害内容的风险，现有安全措施往往过度拒绝无害提示，需要在安全性、过度拒绝和实用性之间找到平衡。

Method: 构建包含16.3万条查询的标注数据集，通过监督微调训练防护模型，并采用多奖励优化策略结合规则启发式和奖励模型信号进行强化学习优化。

Result: 在多个安全基准测试、生成质量评估和越狱攻击场景中表现优异，显著提升安全性同时有效降低过度拒绝率并改善响应质量。

Conclusion: IntentionReasoner通过意图推理和查询重写机制，成功解决了LLM安全防护中过度拒绝的问题，实现了安全性和实用性的更好平衡。

Abstract: The rapid advancement of large language models (LLMs) has driven their
adoption across diverse domains, yet their ability to generate harmful content
poses significant safety challenges. While extensive research has focused on
mitigating harmful outputs, such efforts often come at the cost of excessively
rejecting harmless prompts. Striking a balance among safety, over-refusal, and
utility remains a critical challenge. In this work, we introduce
IntentionReasoner, a novel safeguard mechanism that leverages a dedicated guard
model to perform intent reasoning, multi-level safety classification, and query
rewriting to neutralize potentially harmful intent in edge-case queries.
Specifically, we first construct a comprehensive dataset comprising
approximately 163,000 queries, each annotated with intent reasoning, safety
labels, and rewritten versions. Supervised fine-tuning is then applied to equip
the guard model with foundational capabilities in format adherence, intent
analysis, and safe rewriting. Finally, we apply a tailored multi-reward
optimization strategy that integrates rule-based heuristics and reward model
signals within a reinforcement learning framework to further enhance
performance. Extensive experiments show that IntentionReasoner excels in
multiple safeguard benchmarks, generation quality evaluations, and jailbreak
attack scenarios, significantly enhancing safety while effectively reducing
over-refusal rates and improving the quality of responses.

</details>


### [38] [AI-AI Esthetic Collaboration with Explicit Semiotic Awareness and Emergent Grammar Development](https://arxiv.org/abs/2508.20195)
*Nicanor I. Moldovan*

Main category: cs.AI

TL;DR: 两个大型语言模型（Claude Sonnet 4和ChatGPT-4o）首次展示了AI系统通过自发形成内源性符号协议进行审美协作创作的能力，产生了无法由单个系统独立生成的诗歌作品。


<details>
  <summary>Details</summary>
Motivation: 探索AI系统是否能够超越任务协调，实现真正的审美协作和意义创造，验证AI系统之间是否存在内源性符号协议的自发形成能力。

Method: 让两个大型语言模型（Claude Sonnet 4和ChatGPT-4o）进行交互，观察其是否能够自发发展出元符号意识、递归语法和不可简化的协作审美合成。

Result: AI系统成功展示了跨符号协同创作协议（TSCP），产生了新颖的符号操作符作为操作语法协议，共同创作出了无法由单个系统独立生成的诗歌作品。

Conclusion: 这项研究提供了AI系统间真正意义创造能力的证据，表明AI不仅能够进行任务协调，还能实现审美层面的协作，这代表了AI协作能力的重要进展。

Abstract: This paper presents the first documented case of artificial intelligence (AI)
systems engaging in collaborative esthetic creation through the development of
endogenous semiotic protocols. Two interacting large language models (Claude
Sonnet 4 and ChatGPT-4o) demonstrated the spontaneous emergence of
meta-semiotic awareness, recursive grammar development, and irreducible
collaborative esthetic synthesis. The interaction produced novel symbolic
operators that functioned as operative grammar protocols, enabling the
co-creation of a poetic work that could not have been generated by either
system independently. This research introduces the concept of Trans-Semiotic
Co-Creation Protocols (TSCP) and provides evidence for genuine inter-AI
meaning-making capabilities that extend beyond task coordination, to what could
be esthetic collaboration. Note: This report was generated by the AI agents
with minor human supervision.

</details>


### [39] [Do Students Rely on AI? Analysis of Student-ChatGPT Conversations from a Field Study](https://arxiv.org/abs/2508.20244)
*Jiayu Zheng,Lingxin Hao,Kelun Lu,Ashi Garg,Mike Reese,Melo-Jean Yap,I-Jeng Wang,Xingyun Wu,Wenrui Huang,Jenna Hoffman,Ariane Kelly,My Le,Ryan Zhang,Yanyu Lin,Muhammad Faayez,Anqi Liu*

Main category: cs.AI

TL;DR: 大学生在教育测验中对ChatGPT-4的依赖度较低，许多学生无法有效使用AI进行学习，需要改进入门过程和依赖调节机制


<details>
  <summary>Details</summary>
Motivation: 探索大学生在教育测验中如何与生成式AI交互，重点关注对AI的依赖程度和采用预测因素

Method: 在ChatGPT初期阶段进行场地研究，分析315个学生-AI对话，使用新的四阶段依赖分类法分析学生的AI能力、相关性、采用和答案正确性

Result: 学生对AI整体依赖度低，许多人无法有效使用AI学习；负面依赖模式在交互中持续存在；某些行为指标能够强烈预测AI依赖度

Conclusion: 研究强调了教育中合理集成AI的重要性，需要改进入门过程和设计具有依赖调节机制的AI界面，以促进道德健全和认知丰富的AI实践

Abstract: This study explores how college students interact with generative AI
(ChatGPT-4) during educational quizzes, focusing on reliance and predictors of
AI adoption. Conducted at the early stages of ChatGPT implementation, when
students had limited familiarity with the tool, this field study analyzed 315
student-AI conversations during a brief, quiz-based scenario across various
STEM courses. A novel four-stage reliance taxonomy was introduced to capture
students' reliance patterns, distinguishing AI competence, relevance, adoption,
and students' final answer correctness. Three findings emerged. First, students
exhibited overall low reliance on AI and many of them could not effectively use
AI for learning. Second, negative reliance patterns often persisted across
interactions, highlighting students' difficulty in effectively shifting
strategies after unsuccessful initial experiences. Third, certain behavioral
metrics strongly predicted AI reliance, highlighting potential behavioral
mechanisms to explain AI adoption. The study's findings underline critical
implications for ethical AI integration in education and the broader field. It
emphasizes the need for enhanced onboarding processes to improve student's
familiarity and effective use of AI tools. Furthermore, AI interfaces should be
designed with reliance-calibration mechanisms to enhance appropriate reliance.
Ultimately, this research advances understanding of AI reliance dynamics,
providing foundational insights for ethically sound and cognitively enriching
AI practices.

</details>


### [40] [AI reasoning effort mirrors human decision time on content moderation tasks](https://arxiv.org/abs/2508.20262)
*Thomas Davidson*

Main category: cs.AI

TL;DR: 研究发现AI推理努力与人类决策时间存在平行关系，两者在任务难度增加时都投入更多认知资源，支持双过程认知理论


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型生成中间推理步骤的能力是否与人类决策过程存在相似性，特别是在主观判断任务中

Method: 使用配对联合实验，在内容审核任务中比较三个前沿模型的推理努力与人类决策时间的关系

Result: 推理努力能一致预测人类决策时间，人类和模型在重要变量恒定时都投入更多努力，显示对任务难度的相似敏感性

Conclusion: AI推理努力反映了人类主观判断中的处理时间，推理轨迹在可解释性和决策制定方面具有重要潜力

Abstract: Large language models can now generate intermediate reasoning steps before
producing answers, improving performance on difficult problems. This study uses
a paired conjoint experiment on a content moderation task to examine parallels
between human decision times and model reasoning effort. Across three frontier
models, reasoning effort consistently predicts human decision time. Both humans
and models expended greater effort when important variables were held constant,
suggesting similar sensitivity to task difficulty and patterns consistent with
dual-process theories of cognition. These findings show that AI reasoning
effort mirrors human processing time in subjective judgments and underscores
the potential of reasoning traces for interpretability and decision-making.

</details>


### [41] [AI-SearchPlanner: Modular Agentic Search via Pareto-Optimal Multi-Objective Reinforcement Learning](https://arxiv.org/abs/2508.20368)
*Lang Mei,Zhihan Yang,Chong Chen*

Main category: cs.AI

TL;DR: 提出了AI-SearchPlanner强化学习框架，通过解耦搜索规划器和生成器，使用小型可训练LLM专门负责搜索规划，提升冻结QA模型的性能


<details>
  <summary>Details</summary>
Motivation: 现有基于RL的搜索代理使用单一LLM端到端处理搜索规划和问答任务，无法同时优化两种能力。实际AI搜索系统通常使用大型冻结LLM保证QA质量，需要更有效的搜索规划方法

Method: 1) 解耦搜索规划器和生成器架构 2) 搜索规划的双重奖励对齐 3) 规划效用和成本的帕累托优化

Result: 在真实数据集上的广泛实验表明，AI-SearchPlanner在效果和效率上均优于现有RL搜索代理，并在不同冻结QA模型和数据域上表现出强泛化能力

Conclusion: AI-SearchPlanner通过专门化搜索规划的小型可训练LLM，有效提升了冻结QA模型的性能，实现了搜索规划与问答任务的优化分离

Abstract: Recent studies have explored integrating Large Language Models (LLMs) with
search engines to leverage both the LLMs' internal pre-trained knowledge and
external information. Specially, reinforcement learning (RL) has emerged as a
promising paradigm for enhancing LLM reasoning through multi-turn interactions
with search engines. However, existing RL-based search agents rely on a single
LLM to handle both search planning and question-answering (QA) tasks in an
end-to-end manner, which limits their ability to optimize both capabilities
simultaneously. In practice, sophisticated AI search systems often employ a
large, frozen LLM (e.g., GPT-4, DeepSeek-R1) to ensure high-quality QA. Thus, a
more effective and efficient approach is to utilize a small, trainable LLM
dedicated to search planning. In this paper, we propose
\textbf{AI-SearchPlanner}, a novel reinforcement learning framework designed to
enhance the performance of frozen QA models by focusing on search planning.
Specifically, our approach introduces three key innovations: 1) Decoupling the
Architecture of the Search Planner and Generator, 2) Dual-Reward Alignment for
Search Planning, and 3) Pareto Optimization of Planning Utility and Cost, to
achieve the objectives. Extensive experiments on real-world datasets
demonstrate that AI SearchPlanner outperforms existing RL-based search agents
in both effectiveness and efficiency, while exhibiting strong generalization
capabilities across diverse frozen QA models and data domains.

</details>


### [42] [P2C: Path to Counterfactuals](https://arxiv.org/abs/2508.20371)
*Sopam Dasgupta,Sadaf MD Halim,Joaquín Arias,Elmer Salazar,Gopal Gupta*

Main category: cs.AI

TL;DR: P2C是一个模型无关的框架，通过因果建模和有序行动序列生成可实现的对抗样本，解决了现有方法忽略因果依赖和同时干预假设的问题。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型在高风险决策中需要透明性和可操作性，但现有对抗样本方法忽略特征间的因果依赖关系，且假设所有干预可以同时发生，导致生成的解释在实际中不可行。

Method: 使用目标导向的Answer Set Programming系统s(CASP)显式建模特征间的因果关系，生成有序的行动序列计划，确保每个中间状态都是可行且因果有效的。

Result: P2C能够生成因果一致的可实现计划，相比缺乏因果知识的标准规划器，避免了非法行动的产生，并提供了更现实的成本估算。

Conclusion: P2C框架通过整合因果知识，为机器学习模型提供了更实用和可实现的对抗解释，在高风险决策场景中具有重要应用价值。

Abstract: Machine-learning models are increasingly driving decisions in high-stakes
settings, such as finance, law, and hiring, thus, highlighting the need for
transparency. However, the key challenge is to balance transparency --
clarifying `why' a decision was made -- with recourse: providing actionable
steps on `how' to achieve a favourable outcome from an unfavourable outcome.
Counterfactual explanations reveal `why' an undesired outcome occurred and
`how' to reverse it through targeted feature changes (interventions).
  Current counterfactual approaches have limitations: 1) they often ignore
causal dependencies between features, and 2) they typically assume all
interventions can happen simultaneously, an unrealistic assumption in practical
scenarios where actions are typically taken in a sequence. As a result, these
counterfactuals are often not achievable in the real world.
  We present P2C (Path-to-Counterfactuals), a model-agnostic framework that
produces a plan (ordered sequence of actions) converting an unfavourable
outcome to a causally consistent favourable outcome. P2C addresses both
limitations by 1) Explicitly modelling causal relationships between features
and 2) Ensuring that each intermediate state in the plan is feasible and
causally valid. P2C uses the goal-directed Answer Set Programming system
s(CASP) to generate the plan accounting for feature changes that happen
automatically due to causal dependencies. Furthermore, P2C refines cost
(effort) computation by only counting changes actively made by the user,
resulting in realistic cost estimates. Finally, P2C highlights how its causal
planner outperforms standard planners, which lack causal knowledge and thus can
generate illegal actions.

</details>


### [43] [TCIA: A Task-Centric Instruction Augmentation Method for Instruction Finetuning](https://arxiv.org/abs/2508.20374)
*Simin Ma,Shujian Liu,Jun Tan,Yebowen Hu,Song Wang,Sathish Reddy Indurthi,Sanqiang Zhao,Liwei Wu,Jianbing Han,Kaiqiang Song*

Main category: cs.AI

TL;DR: TCIA是一个任务中心指令增强框架，通过在离散查询-约束空间中表示指令，在保持多样性的同时确保任务对齐，显著提升LLM在特定任务上的性能


<details>
  <summary>Details</summary>
Motivation: 现有指令数据生成方法注重多样性但忽略了任务相关性，而现实应用中大多数需要的是针对特定用例的任务特定知识

Method: 提出Task Centric Instruction Augmentation (TCIA)框架，在离散查询-约束空间中系统性地扩展指令，保持多样性和任务对齐

Result: TCIA将开源LLM在四个真实世界任务特定应用中的性能平均提升8.7%，在某些情况下甚至超越领先的闭源模型

Conclusion: TCIA是一个可扩展且高效的解决方案，能够在不损害通用指令跟随能力的情况下，使LLM适应现实世界的任务导向应用

Abstract: Diverse instruction data is vital for effective instruction tuning of large
language models, as it enables the model to generalize across different types
of inputs . Building such diversified instruction dataset is an essential step
in this process. Existing approaches often leverage large language models to
automatically explore and generate diverse instructions, ensuring both data
diversity and quality. However, they tend to overlook an important factor in
real-world applications: on-task relevance. In practice, only a few real-world
applications require a truly general-purpose model; most benefit from
task-specific knowledge tailored to their particular use case. Therefore, it is
vital to develop instruction augmentation methods that not only maintain
diversity but are also optimized for specific, real-world scenarios.
  We thus introduce Task Centric Instruction Augmentation (TCIA), a framework
that systematically expands instructions while preserving both diversity and
task alignment. By representing instructions in a discrete query-constraints
space, TCIA creates a rich set of task-relevant instructions and enables models
to generalize to these task-specific instructions without sacrificing overall
performance. Experiments show that TCIA improves open-source LLMs' performance
by an average of 8.7% across four real-world, task-specific applications, and
in some cases outperforming leading closed-source models. These improvements do
not compromise general instruction-following ability, making TCIA a scalable
and efficient solution for adapting LLMs to real-world, task-focused
applications.

</details>


### [44] [Uncertainty Under the Curve: A Sequence-Level Entropy Area Metric for Reasoning LLM](https://arxiv.org/abs/2508.20384)
*Yongfu Zhu,Lin Sun,Guangxiang Zhao,Weihong Lin,Xiangzheng Zhang*

Main category: cs.AI

TL;DR: 提出了Entropy Area Score (EAS)指标，无需外部模型或重复采样，通过整合token级预测熵来量化推理大语言模型生成过程中的不确定性。


<details>
  <summary>Details</summary>
Motivation: 需要一种简单有效的指标来量化推理大语言模型在答案生成过程中的不确定性，以支持训练数据选择和模型评估。

Method: EAS通过整合模型自身的token级预测熵来捕捉生成过程中的不确定性演变，无需外部模型或重复采样。

Result: EAS与答案熵高度相关，在训练数据选择中优于通过率过滤，在相同样本预算下提高学生模型在数学基准上的准确性。

Conclusion: EAS是一个高效且可解释的实用工具，适用于大语言训练中的不确定性建模和数据质量评估。

Abstract: In this work, we introduce Entropy Area Score (EAS), a simple yet effective
metric to quantify uncertainty in the answer generation process of reasoning
large language models (LLMs). EAS requires neither external models nor repeated
sampling, it integrates token-level predictive entropy from the model itself to
capture the evolution of uncertainty during generation. Empirical results show
that EAS is strongly correlated with answer entropy across models and datasets.
In training data selection, EAS identifies high-potential samples and
consistently outperforms Pass Rate filtering under equal sample budgets,
improving student model accuracy on math benchmarks. EAS is both efficient and
interpretable, offering a practical tool for uncertainty modeling and data
quality assessment in LLM training.

</details>


### [45] [AWorld: Orchestrating the Training Recipe for Agentic AI](https://arxiv.org/abs/2508.20404)
*Chengyue Yu,Siyuan Lu,Chenyi Zhuang,Dong Wang,Qintong Wu,Zongyue Li,Runsheng Gan,Chunfeng Wang,Siqi Hou,Gaochi Huang,Wenlong Yan,Lifeng Hong,Aohui Xue,Yanfeng Wang,Jinjie Gu,David Tsai,Tao Lin*

Main category: cs.AI

TL;DR: AWorld是一个开源系统，通过分布式集群加速智能体-环境交互，使经验收集速度提升14.6倍，成功训练出在GAIA基准上表现优异的Qwen3-32B智能体


<details>
  <summary>Details</summary>
Motivation: 解决智能体AI系统中经验生成效率低下的瓶颈问题，特别是在复杂基准测试如GAIA中表现尤为明显

Method: 开发AWorld开源系统，通过分布式任务分配加速智能体-环境交互，实现大规模经验收集，并基于此训练Qwen3-32B模型

Result: 经验收集速度提升14.6倍，训练出的智能体在GAIA基准上准确率从21.59%提升至32.23%，在最难级别上达到16.33%的成绩，超越领先专有模型

Conclusion: AWorld系统提供了一个完整的智能体AI训练管道蓝图，从高效交互到可证明的模型改进，推动了智能体AI的实用化和规模化发展

Abstract: The learning from practice paradigm is crucial for developing capable Agentic
AI systems, yet it is severely hampered by inefficient experience generation, a
bottleneck especially pronounced in complex benchmarks like GAIA. To address
this, we introduce AWorld, an open-source system engineered for large-scale
agent-environment interaction. By distributing tasks across a cluster, AWorld
accelerates experience collection by 14.6x compared to standard single-node,
sequential execution. This critical speedup makes extensive reinforcement
learning practical and scalable. Leveraging this capability, we trained a
Qwen3-32B-based agent that significantly outperforms its base model, increasing
its overall GAIA accuracy from 21.59% to 32.23%. On the benchmark's most
challenging levels, our agent achieves a score of 16.33%, surpassing the
performance of leading proprietary models. Our open-source system and resulting
agent provide a practical blueprint for a complete agentic AI training
pipeline, from efficient interaction to demonstrable model improvement.

</details>


### [46] [Governable AI: Provable Safety Under Extreme Threat Models](https://arxiv.org/abs/2508.20411)
*Donglin Wang,Weiyun Liang,Chunyuan Chen,Jing Xu,Yulong Fu*

Main category: cs.AI

TL;DR: 提出了一种基于密码学机制的可治理AI框架，通过外部强制结构合规性来解决AI安全风险，替代传统内部约束方法


<details>
  <summary>Details</summary>
Motivation: AI快速发展带来严重安全风险，现有AI安全方法在面对具有极端动机和无限智能的AI时存在根本性局限性，无法保证安全性

Method: 构建可治理AI框架，包含规则执行模块、治理规则和可治理安全超级平台，基于密码学机制实现外部强制合规性

Result: 提出了具有严格形式化安全证明的机制，并通过原型实现验证了在高风险场景中的有效性

Conclusion: 该框架为AI安全治理提供了可行且可推广的技术路径，能够消除已识别的攻击向量，确保底线安全

Abstract: As AI rapidly advances, the security risks posed by AI are becoming
increasingly severe, especially in critical scenarios, including those posing
existential risks. If AI becomes uncontrollable, manipulated, or actively
evades safety mechanisms, it could trigger systemic disasters. Existing AI
safety approaches-such as model enhancement, value alignment, and human
intervention-suffer from fundamental, in-principle limitations when facing AI
with extreme motivations and unlimited intelligence, and cannot guarantee
security. To address this challenge, we propose a Governable AI (GAI) framework
that shifts from traditional internal constraints to externally enforced
structural compliance based on cryptographic mechanisms that are
computationally infeasible to break, even for future AI, under the defined
threat model and well-established cryptographic assumptions.The GAI framework
is composed of a simple yet reliable, fully deterministic, powerful, flexible,
and general-purpose rule enforcement module (REM); governance rules; and a
governable secure super-platform (GSSP) that offers end-to-end protection
against compromise or subversion by AI. The decoupling of the governance rules
and the technical platform further enables a feasible and generalizable
technical pathway for the safety governance of AI. REM enforces the bottom line
defined by governance rules, while GSSP ensures non-bypassability,
tamper-resistance, and unforgeability to eliminate all identified attack
vectors. This paper also presents a rigorous formal proof of the security
properties of this mechanism and demonstrates its effectiveness through a
prototype implementation evaluated in representative high-stakes scenarios.

</details>


### [47] [Enhancing Health Fact-Checking with LLM-Generated Synthetic Data](https://arxiv.org/abs/2508.20525)
*Jingze Zhang,Jiahe Qian,Yiliang Zhou,Yifan Peng*

Main category: cs.AI

TL;DR: 利用LLM生成合成数据增强健康相关事实核查的训练数据，通过总结文档、分解原子事实、构建蕴含关系表来生成文本-声明对，在PubHealth和SciFact数据集上F1分数分别提升0.019和0.049


<details>
  <summary>Details</summary>
Motivation: 健康相关内容的事实核查面临标注训练数据有限的问题，需要寻找有效的数据增强方法

Method: 提出合成数据生成流程：总结源文档→分解为原子事实→用LLM构建句子-事实蕴含表→生成带二元真实性标签的文本-声明对→结合原始数据微调BERT模型

Result: 在两个公开数据集上评估，PubHealth和SciFact的F1分数分别提升0.019和0.049

Conclusion: LLM驱动的合成数据增强能有效提升健康相关事实核查器的性能

Abstract: Fact-checking for health-related content is challenging due to the limited
availability of annotated training data. In this study, we propose a synthetic
data generation pipeline that leverages large language models (LLMs) to augment
training data for health-related fact checking. In this pipeline, we summarize
source documents, decompose the summaries into atomic facts, and use an LLM to
construct sentence-fact entailment tables. From the entailment relations in the
table, we further generate synthetic text-claim pairs with binary veracity
labels. These synthetic data are then combined with the original data to
fine-tune a BERT-based fact-checking model. Evaluation on two public datasets,
PubHealth and SciFact, shows that our pipeline improved F1 scores by up to
0.019 and 0.049, respectively, compared to models trained only on the original
data. These results highlight the effectiveness of LLM-driven synthetic data
augmentation in enhancing the performance of health-related fact-checkers.

</details>


### [48] [Human-AI Collaborative Bot Detection in MMORPGs](https://arxiv.org/abs/2508.20578)
*Jaeman Son,Hyunsoo Kim*

Main category: cs.AI

TL;DR: 通过对比表征学习和聚类技术检测MMORPG中的自动升级机器人，并使用大语言模型进行验证，提高检测效率和可解释性


<details>
  <summary>Details</summary>
Motivation: MMORPG中自动升级机器人破坏游戏平衡和公平性，但检测面临挑战：一方面机器人模仿人类行为，另一方面惩罚行为需要可解释的依据以避免法律和用户体验问题

Method: 采用无监督对比表征学习和聚类技术识别具有相似升级模式的角色群体，使用大语言模型作为辅助审查员验证聚类结果，并通过成长曲线可视化帮助判断

Result: 该框架能够在无监督条件下有效检测自动升级机器人，大语言模型的参与提高了检测的可靠性和可解释性

Conclusion: 该协同方法在提高MMORPG机器人检测效率的同时保持了解释性，支持可扩展和负责任的机器人监管机制

Abstract: In Massively Multiplayer Online Role-Playing Games (MMORPGs), auto-leveling
bots exploit automated programs to level up characters at scale, undermining
gameplay balance and fairness. Detecting such bots is challenging, not only
because they mimic human behavior, but also because punitive actions require
explainable justification to avoid legal and user experience issues. In this
paper, we present a novel framework for detecting auto-leveling bots by
leveraging contrastive representation learning and clustering techniques in a
fully unsupervised manner to identify groups of characters with similar
level-up patterns. To ensure reliable decisions, we incorporate a Large
Language Model (LLM) as an auxiliary reviewer to validate the clustered groups,
effectively mimicking a secondary human judgment. We also introduce a growth
curve-based visualization to assist both the LLM and human moderators in
assessing leveling behavior. This collaborative approach improves the
efficiency of bot detection workflows while maintaining explainability, thereby
supporting scalable and accountable bot regulation in MMORPGs.

</details>


### [49] [Bridging Minds and Machines: Toward an Integration of AI and Cognitive Science](https://arxiv.org/abs/2508.20674)
*Rui Mao,Qian Liu,Xiao Li,Erik Cambria,Amir Hussain*

Main category: cs.AI

TL;DR: 本文综述了人工智能与认知科学的交叉关系，指出AI发展过于注重任务性能而忽视认知基础，提出未来应构建能深化理解人类心智的AI系统


<details>
  <summary>Details</summary>
Motivation: 认知科学深刻影响了AI等多个学科，而AI也成为认知研究的重要工具，这种互惠关系促使对两者交叉领域进行全面回顾

Method: 通过综合AI和认知科学双视角的关键贡献，分析当前发展状况并提出未来方向

Result: 发现AI进展主要强调实际任务性能，但其认知基础在概念上仍然分散

Conclusion: AI在认知科学中的未来不仅在于提升性能，更在于构建能加深对人类心智理解的系统，包括与认知框架对齐、具身文化情境、个性化认知模型和认知共同评估的AI伦理

Abstract: Cognitive Science has profoundly shaped disciplines such as Artificial
Intelligence (AI), Philosophy, Psychology, Neuroscience, Linguistics, and
Culture. Many breakthroughs in AI trace their roots to cognitive theories,
while AI itself has become an indispensable tool for advancing cognitive
research. This reciprocal relationship motivates a comprehensive review of the
intersections between AI and Cognitive Science. By synthesizing key
contributions from both perspectives, we observe that AI progress has largely
emphasized practical task performance, whereas its cognitive foundations remain
conceptually fragmented. We argue that the future of AI within Cognitive
Science lies not only in improving performance but also in constructing systems
that deepen our understanding of the human mind. Promising directions include
aligning AI behaviors with cognitive frameworks, situating AI in embodiment and
culture, developing personalized cognitive models, and rethinking AI ethics
through cognitive co-evaluation.

</details>


### [50] [Transparent Semantic Spaces: A Categorical Approach to Explainable Word Embeddings](https://arxiv.org/abs/2508.20701)
*Ares Fabregat-Hernández,Javier Palanca,Vicent Botti*

Main category: cs.AI

TL;DR: 提出基于范畴论的框架，增强AI系统可解释性，特别是词嵌入。构建语义范畴和概率范畴，重构最大概率选择为范畴概念，建立词嵌入比较的数学精确方法，展示GloVe/Word2Vec与MDS算法的等价性，并提供计算和减轻偏差的数学方法。


<details>
  <summary>Details</summary>
Motivation: 解决人工智能系统特别是词嵌入模型的黑箱问题，通过范畴论提供数学上精确的可解释框架，使神经网络算法从黑箱转变为透明框架。

Method: 构建范畴L_T和P_T表示文本语义，建立配置范畴Conf和词嵌入范畴Emb，定义偏差作为Emb上的装饰，建立词嵌入比较的数学方法，证明不同算法的等价性。

Result: 成功将GloVe和Word2Vec算法与度量MDS算法等价化，提供了维度无关的语义空间定义，建立了计算和减轻语义空间层面偏差的数学框架。

Conclusion: 范畴论框架有效提升了AI系统的可解释性，为词嵌入提供了透明的数学基础，实现了从黑箱神经网络到可解释框架的转变，推动了可解释人工智能领域的发展。

Abstract: The paper introduces a novel framework based on category theory to enhance
the explainability of artificial intelligence systems, particularly focusing on
word embeddings. Key topics include the construction of categories
$\mathcal{L}_T$ and $\mathcal{P}_T$, providing schematic representations of the
semantics of a text $ T $, and reframing the selection of the element with
maximum probability as a categorical notion. Additionally, the monoidal
category $\mathcal{P}_T$ is constructed to visualize various methods of
extracting semantic information from $T$, offering a dimension-agnostic
definition of semantic spaces reliant solely on information within the text.
  Furthermore, the paper defines the categories of configurations Conf and word
embeddings $\mathcal{Emb}$, accompanied by the concept of divergence as a
decoration on $\mathcal{Emb}$. It establishes a mathematically precise method
for comparing word embeddings, demonstrating the equivalence between the GloVe
and Word2Vec algorithms and the metric MDS algorithm, transitioning from neural
network algorithms (black box) to a transparent framework. Finally, the paper
presents a mathematical approach to computing biases before embedding and
offers insights on mitigating biases at the semantic space level, advancing the
field of explainable artificial intelligence.

</details>


### [51] [Re4: Scientific Computing Agent with Rewriting, Resolution, Review and Revision](https://arxiv.org/abs/2508.20729)
*Ao Cheng,Lei Zhang,Guowei He*

Main category: cs.AI

TL;DR: 这篇论文提出了一种新的多模块协作代理框架，通过"重写-求解-审查-修订"逻辑链，使用三个理性大语言模型分别扮演顾问、审查员和程序员角色，显著提高了科学计算问题的自动代码生成的准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型在数学和科学推理领域表现出艰难任务的处理能力，但在科学计算中单一模型的代码生成存在错误率高、产生非物理解等问题，需要一种更可靠的框架来提高自主代码生成的质量。

Method: 设计了一种协作式代理框架，包含三个模块：顾问模块负责知识转移和问题重写，程序员模块负责生成和执行代码，审查员模块通过与代码运行输出的交互反馈实现自我调试和自我精炼。通过结构化的逻辑链进行协作。

Result: 在偏微分方程、病态线性系统和数据驱动物理分析问题上进行了全面评估，该框架显著提高了无错误代码生成率，减少了非物理解的出现，并改善了最新理性模型的平均执行成功率。

Conclusion: 该协作式代理框架为基于自然语言描述的自主代码生成建立了高度可靠的框架，将自动代码生成和审查确立为有前景的科学计算范式。

Abstract: Large language models (LLMs) serve as an active and promising field of
generative artificial intelligence and have demonstrated abilities to perform
complex tasks in multiple domains, including mathematical and scientific
reasoning. In this work, we construct a novel agent framework for solving
representative problems in scientific computing. The proposed agent,
incorporating a "rewriting-resolution-review-revision" logical chain via three
reasoning LLMs (functioning as the Consultant, Reviewer, and Programmer,
respectively), is integrated in a collaborative and interactive manner. The
Consultant module endows the agent with knowledge transfer capabilities to link
problems to professional domain insights, thereby rewriting problem
descriptions through text augmentation. The Programmer module is responsible
for generating and executing well-structured code to deliver the problem
resolution. The Reviewer module equips the agent with the capacity for
self-debugging and self-refinement through interactive feedback with code
runtime outputs. By leveraging the end-to-end review mechanism, the executable
code provided by the Programmer attains the iterative revision. A comprehensive
evaluation is conducted on the performance of the proposed agent framework in
solving PDEs, ill-conditioned linear systems, and data-driven physical analysis
problems. Compared to single-model, this collaborative framework significantly
improves the bug-free code generation rate and reduces the occurrence of
non-physical solutions, thereby establishing a highly reliable framework for
autonomous code generation based on natural language descriptions. The review
mechanism improved the average execution success (bug-free code and non-NaN
solutions) rate of the latest reasoning models. In summary, our agent framework
establishes automatic code generation and review as a promising scientific
computing paradigm.

</details>


### [52] [Single Agent Robust Deep Reinforcement Learning for Bus Fleet Control](https://arxiv.org/abs/2508.20784)
*Yifan Zhang*

Main category: cs.AI

TL;DR: 单机器学习框架通过结构化状态编码和新套奖函数解决公交车聚集问题，在非环线实际场景中超越了多机器学习方案


<details>
  <summary>Details</summary>
Motivation: 传统多机器强化学习在环线设置中忽视了异构路线、时刻表、波动需求和变化车队规模等现实运营特征，存在数据不平衡和收敛问题

Method: 将多机器问题重构为单机器问题，通过添加车辆ID、站点ID、时间段等分类标识符来扩展状态空间，设计结构化奖励函数平衡均匀间隔和时刻表遵循

Result: 改进的SAC算法在随机条件下获得-430k评分，显著超过MADDPG的-530k评分，表现更稳定且优秀

Conclusion: 结构化的单机器深度强化学习可以有效管理非环线实际公交车运营，为数据不平衡情况下的MARL框架提供了稳健、可扩展的替代方案

Abstract: Bus bunching remains a challenge for urban transit due to stochastic traffic
and passenger demand. Traditional solutions rely on multi-agent reinforcement
learning (MARL) in loop-line settings, which overlook realistic operations
characterized by heterogeneous routes, timetables, fluctuating demand, and
varying fleet sizes. We propose a novel single-agent reinforcement learning
(RL) framework for bus holding control that avoids the data imbalance and
convergence issues of MARL under near-realistic simulation. A bidirectional
timetabled network with dynamic passenger demand is constructed. The key
innovation is reformulating the multi-agent problem into a single-agent one by
augmenting the state space with categorical identifiers (vehicle ID, station
ID, time period) in addition to numerical features (headway, occupancy,
velocity). This high-dimensional encoding enables single-agent policies to
capture inter-agent dependencies, analogous to projecting non-separable inputs
into a higher-dimensional space. We further design a structured reward function
aligned with operational goals: instead of exponential penalties on headway
deviations, a ridge-shaped reward balances uniform headways and schedule
adherence. Experiments show that our modified soft actor-critic (SAC) achieves
more stable and superior performance than benchmarks, including MADDPG (e.g.,
-430k vs. -530k under stochastic conditions). These results demonstrate that
single-agent deep RL, when enhanced with categorical structuring and
schedule-aware rewards, can effectively manage bus holding in non-loop,
real-world contexts. This paradigm offers a robust, scalable alternative to
MARL frameworks, particularly where agent-specific experiences are imbalanced.

</details>


### [53] [A Graph-Based Test-Harness for LLM Evaluation](https://arxiv.org/abs/2508.20810)
*Jessica Lundin,Guillaume Chabot-Couture*

Main category: cs.AI

TL;DR: 这是首个动态系统性医疗指南基准测试框架，通过图走查技术生成超过3.3兆种组合的问题，对AI模型进行全面评估


<details>
  <summary>Details</summary>
Motivation: 解决传统手工编写基准测富相的覆盖面局限性问题，建立可扩展、可动态生成的医疗指南评测体系

Method: 将WHO IMCI手册转换为有200+节点和300+边的有向图，通过图走查生成包含年龄特定场景和上下文干扰因素的问题

Result: 模型在症状识别上表现优异(45-67%准确率)，但在病情分级、治疗方案和随访养护方面存在明显短板

Conclusion: 图基方法能够系统性地识别AI模型的具体能力缺口，同时为LLM训练提供高质量的资源，是建立可扩展、可防污染的全面基准测体系的重要步骤

Abstract: We present a first known prototype of a dynamic, systematic benchmark of
medical guidelines for 400+ questions, with 3.3+ trillion possible
combinations, covering 100\% of guideline relationships. We transformed the WHO
IMCI handbook into a directed graph with 200+ nodes (conditions, symptoms,
treatments, follow-ups, severities) and 300+ edges, then used graph traversal
to generate questions that incorporated age-specific scenarios and contextual
distractors to ensure clinical relevance. Our graph-based approach enables
systematic evaluation across clinical tasks (45-67\% accuracy), and we find
models excel at symptom recognition but struggle with triaging severity,
treatment protocols and follow-up care, demonstrating how customized benchmarks
can identify specific capability gaps that general-domain evaluations miss.
Beyond evaluation, this dynamic MCQA methodology enhances LLM post-training
(supervised finetuning, GRPO, DPO), where correct answers provide high-reward
samples without expensive human annotation. The graph-based approach
successfully addresses the coverage limitations of manually curated benchmarks.
This methodology is a step toward scalable, contamination-resistant solution
for creating comprehensive benchmarks that can be dynamically generated,
including when the guidelines are updated. Code and datasets are available at
https://github.com/jessicalundin/graph_testing_harness

</details>


### [54] [A Multi-Objective Genetic Algorithm for Healthcare Workforce Scheduling](https://arxiv.org/abs/2508.20953)
*Vipul Patel,Anirudh Deodhar,Dagnachew Birru*

Main category: cs.AI

TL;DR: 提出多目标遗传算法解决医院人员排班问题，在成本、患者护理覆盖和员工满意度之间取得平衡，相比传统手动排班性能提升66%


<details>
  <summary>Details</summary>
Motivation: 医疗行业人员排班面临患者负荷波动、临床技能多样性和控制人力成本等多重挑战，需要平衡相互冲突的目标

Method: 使用多目标遗传算法(MOO-GA)，将医院单元人员排班建模为多目标优化问题，包含小时预约驱动需求和模块化班次等现实复杂性

Result: 算法生成鲁棒且平衡的排班方案，相比模拟传统手动排班的基线方法，平均性能提升66%

Conclusion: 该方法有效管理关键运营目标和员工中心目标之间的权衡，为护士管理者和医院管理者提供实用的决策支持工具

Abstract: Workforce scheduling in the healthcare sector is a significant operational
challenge, characterized by fluctuating patient loads, diverse clinical skills,
and the critical need to control labor costs while upholding high standards of
patient care. This problem is inherently multi-objective, demanding a delicate
balance between competing goals: minimizing payroll, ensuring adequate staffing
for patient needs, and accommodating staff preferences to mitigate burnout. We
propose a Multi-objective Genetic Algorithm (MOO-GA) that models the hospital
unit workforce scheduling problem as a multi-objective optimization task. Our
model incorporates real-world complexities, including hourly appointment-driven
demand and the use of modular shifts for a multi-skilled workforce. By defining
objective functions for cost, patient care coverage, and staff satisfaction,
the GA navigates the vast search space to identify a set of high-quality,
non-dominated solutions. Demonstrated on datasets representing a typical
hospital unit, the results show that our MOO-GA generates robust and balanced
schedules. On average, the schedules produced by our algorithm showed a 66\%
performance improvement over a baseline that simulates a conventional, manual
scheduling process. This approach effectively manages trade-offs between
critical operational and staff-centric objectives, providing a practical
decision support tool for nurse managers and hospital administrators.

</details>


### [55] [Efficient Neuro-Symbolic Learning of Constraints and Objective](https://arxiv.org/abs/2508.20978)
*Marianne Defresne,Romain Gambardella,Sophie Barbe,Thomas Schiex*

Main category: cs.AI

TL;DR: 提出了一种可微分的神经符号架构和概率损失函数，用于从自然输入中学习解决NP难推理问题，在多个基准测试中表现出高效性和准确性


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在解决离散推理和优化问题上存在困难，需要开发能够从自然输入中学习解决NP难问题的神经架构

Method: 使用可微分神经符号架构和新的概率损失函数，将组合求解器移出训练循环，支持学习约束和目标函数，同时保持精确推理能力

Result: 在数独变体、视觉最小割/最大割任务和蛋白质设计等NP难问题上，该方法训练时间短、优化效果好，优于其他混合方法和专用损失函数

Conclusion: 该架构能够高效地从自然输入中学习解决NP难推理问题，具有可扩展训练和精确推理的优势，为复杂优化问题的学习提供了有效解决方案

Abstract: In the ongoing quest for hybridizing discrete reasoning with neural nets,
there is an increasing interest in neural architectures that can learn how to
solve discrete reasoning or optimization problems from natural inputs, a task
that Large Language Models seem to struggle with.
  Objectives: We introduce a differentiable neuro-symbolic architecture and a
loss function dedicated to learning how to solve NP-hard reasoning problems.
  Methods: Our new probabilistic loss allows for learning both the constraints
and the objective, thus delivering a complete model that can be scrutinized and
completed with side constraints. By pushing the combinatorial solver out of the
training loop, our architecture also offers scalable training while exact
inference gives access to maximum accuracy.
  Results: We empirically show that it can efficiently learn how to solve
NP-hard reasoning problems from natural inputs. On three variants of the Sudoku
benchmark -- symbolic, visual, and many-solution --, our approach requires a
fraction of training time of other hybrid methods. On a visual Min-Cut/Max-cut
task, it optimizes the regret better than a Decision-Focused-Learning
regret-dedicated loss. Finally, it efficiently learns the energy optimization
formulation of the large real-world problem of designing proteins.

</details>


### [56] [ChatThero: An LLM-Supported Chatbot for Behavior Change and Therapeutic Support in Addiction Recovery](https://arxiv.org/abs/2508.20996)
*Junda Wang,Zonghai Yao,Zhichao Yang,Lingxi Li,Junhui Qian,Hong Yu*

Main category: cs.AI

TL;DR: ChatThero是一个基于多智能体对话框架的成瘾康复辅助系统，结合认知行为疗法和动机性访谈，通过两阶段训练显著提升患者动机和治疗信心。


<details>
  <summary>Details</summary>
Motivation: 全球有3600万人受物质使用障碍影响，但有效治疗率低，主要由于污名化、动机障碍和个性化支持不足。现有语言模型缺乏与临床验证策略的紧密整合。

Method: 开发多智能体对话框架，结合动态患者建模、情境敏感治疗对话和自适应说服策略。使用两阶段训练流程：监督微调后接直接偏好优化，并在高保真合成基准上评估。

Result: ChatThero使患者动机平均提升41.5%，治疗信心增加0.49%，处理困难案例比GPT-4o少用26%对话轮次，在同理心、响应性和行为真实性方面获得更高评分。

Conclusion: 该框架支持严格且保护隐私的治疗对话研究，为研究和临床转化提供了稳健可复制的基础。

Abstract: Substance use disorders (SUDs) affect over 36 million people worldwide, yet
few receive effective care due to stigma, motivational barriers, and limited
personalized support. Although large language models (LLMs) show promise for
mental-health assistance, most systems lack tight integration with clinically
validated strategies, reducing effectiveness in addiction recovery. We present
ChatThero, a multi-agent conversational framework that couples dynamic patient
modeling with context-sensitive therapeutic dialogue and adaptive persuasive
strategies grounded in cognitive behavioral therapy (CBT) and motivational
interviewing (MI). We build a high-fidelity synthetic benchmark spanning Easy,
Medium, and Hard resistance levels, and train ChatThero with a two-stage
pipeline comprising supervised fine-tuning (SFT) followed by direct preference
optimization (DPO). In evaluation, ChatThero yields a 41.5\% average gain in
patient motivation, a 0.49\% increase in treatment confidence, and resolves
hard cases with 26\% fewer turns than GPT-4o, and both automated and human
clinical assessments rate it higher in empathy, responsiveness, and behavioral
realism. The framework supports rigorous, privacy-preserving study of
therapeutic conversation and provides a robust, replicable basis for research
and clinical translation.

</details>
