{"id": "2508.15776", "categories": ["cs.CR", "cs.CE"], "pdf": "https://arxiv.org/pdf/2508.15776", "abs": "https://arxiv.org/abs/2508.15776", "authors": ["Saeid Ghasemshirazi", "Ghazaleh Shirvani", "Marziye Ranjbar Tavakoli", "Bahar Ghaedi", "Mohammad Amin Langarizadeh"], "title": "Implementing Zero Trust Architecture to Enhance Security and Resilience in the Pharmaceutical Supply Chain", "comment": null, "summary": "The pharmaceutical supply chain faces escalating cybersecurity challenges\nthreatening patient safety and operational continuity. This paper examines the\ntransformative potential of zero trust architecture for enhancing security and\nresilience within this critical ecosystem. We explore the challenges posed by\ndata breaches, counterfeiting, and disruptions and introduce the principles of\ncontinuous verification, least-privilege access, and data-centric security\ninherent in zero trust. Real-world case studies illustrate successful\nimplementations. Benefits include heightened security, data protection, and\nadaptable resilience. As recognized by researchers and industrialists, a\nreliable drug tracing system is crucial for ensuring drug safety throughout the\npharmaceutical production process. One of the most pivotal domains within the\npharmaceutical industry and its associated supply chains where zero trust can\nbe effectively implemented is in the management of narcotics, high-health-risk\ndrugs, and abusable substances. By embracing zero trust, the pharmaceutical\nindustry fortifies its supply chain against constantly changing cyber threats,\nensuring the trustworthiness of critical medical operations."}
{"id": "2508.15778", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.15778", "abs": "https://arxiv.org/abs/2508.15778", "authors": ["Yifan Liao", "Yuxin Cao", "Yedi Zhang", "Wentao He", "Yan Xiao", "Xianglong Du", "Zhiyong Huang", "Jin Song Dong"], "title": "Towards Stealthy and Effective Backdoor Attacks on Lane Detection: A Naturalistic Data Poisoning Approach", "comment": "12 pages,7 figures", "summary": "Deep learning-based lane detection (LD) plays a critical role in autonomous\ndriving and advanced driver assistance systems. However, its vulnerability to\nbackdoor attacks presents a significant security concern. Existing backdoor\nattack methods on LD often exhibit limited practical utility due to the\nartificial and conspicuous nature of their triggers. To address this limitation\nand investigate the impact of more ecologically valid backdoor attacks on LD\nmodels, we examine the common data poisoning attack and introduce DBALD, a\nnovel diffusion-based data poisoning framework for generating naturalistic\nbackdoor triggers. DBALD comprises two key components: optimal trigger position\nfinding and stealthy trigger generation. Given the insight that attack\nperformance varies depending on the trigger position, we propose a\nheatmap-based method to identify the optimal trigger location, with gradient\nanalysis to generate attack-specific heatmaps. A region-based editing diffusion\nprocess is then applied to synthesize visually plausible triggers within the\nmost susceptible regions identified previously. Furthermore, to ensure scene\nintegrity and stealthy attacks, we introduce two loss strategies: one for\npreserving lane structure and another for maintaining the consistency of the\ndriving scene. Consequently, compared to existing attack methods, DBALD\nachieves both a high attack success rate and superior stealthiness. Extensive\nexperiments on 4 mainstream LD models show that DBALD exceeds state-of-the-art\nmethods, with an average success rate improvement of +10.87% and significantly\nenhanced stealthiness. The experimental results highlight significant practical\nchallenges in ensuring model robustness against real-world backdoor threats in\nLD."}
{"id": "2508.15808", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15808", "abs": "https://arxiv.org/abs/2508.15808", "authors": ["Benjamin Murphy", "Twm Stone"], "title": "Uplifted Attackers, Human Defenders: The Cyber Offense-Defense Balance for Trailing-Edge Organizations", "comment": null, "summary": "Advances in AI are widely understood to have implications for cybersecurity.\nArticles have emphasized the effect of AI on the cyber offense-defense balance,\nand commentators can be found arguing either that cyber will privilege\nattackers or defenders. For defenders, arguments are often made that AI will\nenable solutions like formal verification of all software--and for some\nwell-equipped companies, this may be true. This conversation, however, does not\nmatch the reality for most companies. \"Trailing-edge organizations,\" as we term\nthem, rely heavily on legacy software, poorly staff security roles, and\nstruggle to implement best practices like rapid deployment of security patches.\nThese decisions may be the result of corporate inertia, but may also be the\nresult of a seemingly-rational calculation that attackers may not bother\ntargeting a firm due to lack of economic incentives, and as a result,\nunderinvestment in defense will not be punished.\n  This approach to security may have been sufficient prior to the development\nof AI systems, but it is unlikely to remain viable in the near future. We argue\nthat continuing improvements in AI's capabilities poses additional risks on two\nfronts: First, increased usage of AI will alter the economics of the marginal\ncyberattack and expose these trailing-edge organizations to more attackers,\nmore frequently. Second, AI's advances will enable attackers to develop\nexploits and launch attacks earlier than they can today--meaning that it is\ninsufficient for these companies to attain parity with today's leading\ndefenders, but must instead aim for faster remediation timelines and more\nresilient software. The situation today portends a dramatically increased\nnumber of attacks in the near future. Moving forward, we offer a range of\nsolutions for both organizations and governments to improve the defensive\nposture of firms which lag behind their peers today."}
{"id": "2508.15839", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15839", "abs": "https://arxiv.org/abs/2508.15839", "authors": ["Yuksel Aydin"], "title": "CIA+TA Risk Assessment for AI Reasoning Vulnerabilities", "comment": null, "summary": "As AI systems increasingly influence critical decisions, they face threats\nthat exploit reasoning mechanisms rather than technical infrastructure. We\npresent a framework for cognitive cybersecurity, a systematic protection of AI\nreasoning processes from adversarial manipulation. Our contributions are\nthreefold. First, we establish cognitive cybersecurity as a discipline\ncomplementing traditional cybersecurity and AI safety, addressing\nvulnerabilities where legitimate inputs corrupt reasoning while evading\nconventional controls. Second, we introduce the CIA+TA, extending traditional\nConfidentiality, Integrity, and Availability triad with Trust (epistemic\nvalidation) and Autonomy (human agency preservation), requirements unique to\nsystems generating knowledge claims and mediating decisions. Third, we present\na quantitative risk assessment methodology with empirically-derived\ncoefficients, enabling organizations to measure cognitive security risks. We\nmap our framework to OWASP LLM Top 10 and MITRE ATLAS, facilitating operational\nintegration. Validation through previously published studies (151 human\nparticipants; 12,180 AI trials) reveals strong architecture dependence:\nidentical defenses produce effects ranging from 96% reduction to 135%\namplification of vulnerabilities. This necessitates pre-deployment Cognitive\nPenetration Testing as a governance requirement for trustworthy AI deployment."}
{"id": "2508.15943", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15943", "abs": "https://arxiv.org/abs/2508.15943", "authors": ["Riccardo Andreoni", "Andrei Buliga", "Alessandro Daniele", "Chiara Ghidini", "Marco Montali", "Massimiliano Ronzani"], "title": "T-ILR: a Neurosymbolic Integration for LTLf", "comment": "Accepted for presentation at NeSy 2025. 10 pages", "summary": "State-of-the-art approaches for integrating symbolic knowledge with deep\nlearning architectures have demonstrated promising results in static domains.\nHowever, methods to handle temporal logic specifications remain underexplored.\nThe only existing approach relies on an explicit representation of a\nfinite-state automaton corresponding to the temporal specification. Instead, we\naim at proposing a neurosymbolic framework designed to incorporate temporal\nlogic specifications, expressed in Linear Temporal Logic over finite traces\n(LTLf), directly into deep learning architectures for sequence-based tasks. We\nextend the Iterative Local Refinement (ILR) neurosymbolic algorithm, leveraging\nthe recent introduction of fuzzy LTLf interpretations. We name this proposed\nmethod Temporal Iterative Local Refinement (T-ILR). We assess T-ILR on an\nexisting benchmark for temporal neurosymbolic architectures, consisting of the\nclassification of image sequences in the presence of temporal knowledge. The\nresults demonstrate improved accuracy and computational efficiency compared to\nthe state-of-the-art method."}
{"id": "2508.15941", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.15941", "abs": "https://arxiv.org/abs/2508.15941", "authors": ["Imen Trabelsi", "Brahim Mahmoudi", "Jean Baptiste Minani", "Naouel Moha", "Yann-Gaël Guéhéneuc"], "title": "A Systematic Literature Review of Machine Learning Approaches for Migrating Monolithic Systems to Microservices", "comment": null, "summary": "Scalability and maintainability challenges in monolithic systems have led to\nthe adoption of microservices, which divide systems into smaller, independent\nservices. However, migrating existing monolithic systems to microservices is a\ncomplex and resource-intensive task, which can benefit from machine learning\n(ML) to automate some of its phases. Choosing the right ML approach for\nmigration remains challenging for practitioners. Previous works studied\nseparately the objectives, artifacts, techniques, tools, and benefits and\nchallenges of migrating monolithic systems to microservices. No work has yet\ninvestigated systematically existing ML approaches for this migration to\nunderstand the \\revised{automated migration phases}, inputs used, ML techniques\napplied, evaluation processes followed, and challenges encountered. We present\na systematic literature review (SLR) that aggregates, synthesises, and\ndiscusses the approaches and results of 81 primary studies (PSs) published\nbetween 2015 and 2024. We followed the Preferred Reporting Items for Systematic\nReview and Meta-Analysis (PRISMA) statement to report our findings and answer\nour research questions (RQs). We extract and analyse data from these PSs to\nanswer our RQs. We synthesise the findings in the form of a classification that\nshows the usage of ML techniques in migrating monolithic systems to\nmicroservices. The findings reveal that some phases of the migration process,\nsuch as monitoring and service identification, are well-studied, while others,\nlike packaging microservices, remain unexplored. Additionally, the findings\nhighlight key challenges, including limited data availability, scalability and\ncomplexity constraints, insufficient tool support, and the absence of\nstandardized benchmarking, emphasizing the need for more holistic solutions."}
{"id": "2508.15840", "categories": ["cs.CR", "cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.15840", "abs": "https://arxiv.org/abs/2508.15840", "authors": ["Robert Dilworth"], "title": "Unveiling Unicode's Unseen Underpinnings in Undermining Authorship Attribution", "comment": null, "summary": "When using a public communication channel -- whether formal or informal, such\nas commenting or posting on social media -- end users have no expectation of\nprivacy: they compose a message and broadcast it for the world to see. Even if\nan end user takes utmost precautions to anonymize their online presence --\nusing an alias or pseudonym; masking their IP address; spoofing their\ngeolocation; concealing their operating system and user agent; deploying\nencryption; registering with a disposable phone number or email; disabling\nnon-essential settings; revoking permissions; and blocking cookies and\nfingerprinting -- one obvious element still lingers: the message itself.\nAssuming they avoid lapses in judgment or accidental self-exposure, there\nshould be little evidence to validate their actual identity, right? Wrong. The\ncontent of their message -- necessarily open for public consumption -- exposes\nan attack vector: stylometric analysis, or author profiling. In this paper, we\ndissect the technique of stylometry, discuss an antithetical counter-strategy\nin adversarial stylometry, and devise enhancements through Unicode\nsteganography."}
{"id": "2508.16033", "categories": ["cs.AI", "eess.SP"], "pdf": "https://arxiv.org/pdf/2508.16033", "abs": "https://arxiv.org/abs/2508.16033", "authors": ["Jong-Hwan Jang", "Junho Song", "Yong-Yeon Jo"], "title": "CoFE: A Framework Generating Counterfactual ECG for Explainable Cardiac AI-Diagnostics", "comment": "Demo paper, 5 pages", "summary": "Recognizing the need for explainable AI (XAI) approaches to enable the\nsuccessful integration of AI-based ECG prediction models (AI-ECG) into clinical\npractice, we introduce a framework generating \\textbf{Co}unter\\textbf{F}actual\n\\textbf{E}CGs (i,e., named CoFE) to illustrate how specific features, such as\namplitudes and intervals, influence the model's predictive decisions. To\ndemonstrate the applicability of the CoFE, we present two case studies: atrial\nfibrillation classification and potassium level regression models. The CoFE\nreveals feature changes in ECG signals that align with the established clinical\nknowledge. By clarifying both \\textbf{where valid features appear} in the ECG\nand \\textbf{how they influence the model's predictions}, we anticipate that our\nframework will enhance the interpretability of AI-ECG models and support more\neffective clinical decision-making. Our demonstration video is available at:\nhttps://www.youtube.com/watch?v=YoW0bNBPglQ."}
{"id": "2508.16025", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.16025", "abs": "https://arxiv.org/abs/2508.16025", "authors": ["Saba Naqvi", "Mohammad Baqar"], "title": "Breaking Barriers in Software Testing: The Power of AI-Driven Automation", "comment": "10 Pages", "summary": "Software testing remains critical for ensuring reliability, yet traditional\napproaches are slow, costly, and prone to gaps in coverage. This paper presents\nan AI-driven framework that automates test case generation and validation using\nnatural language processing (NLP), reinforcement learning (RL), and predictive\nmodels, embedded within a policy-driven trust and fairness model. The approach\ntranslates natural language requirements into executable tests, continuously\noptimizes them through learning, and validates outcomes with real-time analysis\nwhile mitigating bias. Case studies demonstrate measurable gains in defect\ndetection, reduced testing effort, and faster release cycles, showing that\nAI-enhanced testing improves both efficiency and reliability. By addressing\nintegration and scalability challenges, the framework illustrates how AI can\nshift testing from a reactive, manual process to a proactive, adaptive system\nthat strengthens software quality in increasingly complex environments."}
{"id": "2508.15848", "categories": ["cs.CR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.15848", "abs": "https://arxiv.org/abs/2508.15848", "authors": ["Yinghan Zhou", "Juan Wen", "Wanli Peng", "Zhengxian Wu", "Ziwei Zhang", "Yiming Xue"], "title": "Self-Disguise Attack: Induce the LLM to disguise itself for AIGT detection evasion", "comment": null, "summary": "AI-generated text (AIGT) detection evasion aims to reduce the detection\nprobability of AIGT, helping to identify weaknesses in detectors and enhance\ntheir effectiveness and reliability in practical applications. Although\nexisting evasion methods perform well, they suffer from high computational\ncosts and text quality degradation. To address these challenges, we propose\nSelf-Disguise Attack (SDA), a novel approach that enables Large Language Models\n(LLM) to actively disguise its output, reducing the likelihood of detection by\nclassifiers. The SDA comprises two main components: the adversarial feature\nextractor and the retrieval-based context examples optimizer. The former\ngenerates disguise features that enable LLMs to understand how to produce more\nhuman-like text. The latter retrieves the most relevant examples from an\nexternal knowledge base as in-context examples, further enhancing the\nself-disguise ability of LLMs and mitigating the impact of the disguise process\non the diversity of the generated text. The SDA directly employs prompts\ncontaining disguise features and optimized context examples to guide the LLM in\ngenerating detection-resistant text, thereby reducing resource consumption.\nExperimental results demonstrate that the SDA effectively reduces the average\ndetection accuracy of various AIGT detectors across texts generated by three\ndifferent LLMs, while maintaining the quality of AIGT."}
{"id": "2508.16051", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.16051", "abs": "https://arxiv.org/abs/2508.16051", "authors": ["Yiheng Hu", "Xiaoyang Wang", "Qing Liu", "Xiwei Xu", "Qian Fu", "Wenjie Zhang", "Liming Zhu"], "title": "MMAPG: A Training-Free Framework for Multimodal Multi-hop Question Answering via Adaptive Planning Graphs", "comment": null, "summary": "Multimodal Multi-hop question answering requires integrating information from\ndiverse sources, such as images and texts, to derive answers. Existing methods\ntypically rely on sequential retrieval and reasoning, where each step builds on\nthe previous output. However, this single-path paradigm makes them vulnerable\nto errors due to misleading intermediate steps. Moreover, developing multimodal\nmodels can be computationally expensive, often requiring extensive training. To\naddress these limitations, we propose a training-free framework guided by an\nAdaptive Planning Graph, which consists of planning, retrieval and reasoning\nmodules. The planning module analyzes the current state of the Adaptive\nPlanning Graph, determines the next action and where to expand the graph, which\nenables dynamic and flexible exploration of reasoning paths. To handle\nretrieval of text to unspecified target modalities, we devise modality-specific\nstrategies that dynamically adapt to distinct data types. Our approach\npreserves the characteristics of multimodal information without costly\ntask-specific training, enabling seamless integration with up-to-date models.\nFinally, the experiments on MultimodalQA and WebQA show that our approach\nmatches or outperforms existing models that rely on training."}
{"id": "2508.16053", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.16053", "abs": "https://arxiv.org/abs/2508.16053", "authors": ["Shadikur Rahman", "Umme Ayman Koana", "Hasibul Karim Shanto", "Mahmuda Akter", "Chitra Roy", "Aras M. Ismael"], "title": "Measuring the effectiveness of code review comments in GitHub repositories: A machine learning approach", "comment": null, "summary": "This paper illustrates an empirical study of the working efficiency of\nmachine learning techniques in classifying code review text by semantic\nmeaning. The code review comments from the source control repository in GitHub\nwere extracted for development activity from the existing year for three\nopen-source projects. Apart from that, programmers need to be aware of their\ncode and point out their errors. In that case, it is a must to classify the\nsentiment polarity of the code review comments to avoid an error. We manually\nlabelled 13557 code review comments generated by three open source projects in\nGitHub during the existing year. In order to recognize the sentiment polarity\n(or sentiment orientation) of code reviews, we use seven machine learning\nalgorithms and compare those results to find the better ones. Among those\nLinear Support Vector Classifier(SVC) classifier technique achieves higher\naccuracy than others. This study will help programmers to make any solution\nbased on code reviews by avoiding misconceptions."}
{"id": "2508.15850", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.15850", "abs": "https://arxiv.org/abs/2508.15850", "authors": ["Ziyu Wang", "Elahe Khatibi", "Farshad Firouzi", "Sanaz Rahimi Mousavi", "Krishnendu Chakrabarty", "Amir M. Rahmani"], "title": "Linkage Attacks Expose Identity Risks in Public ECG Data Sharing", "comment": null, "summary": "The increasing availability of publicly shared electrocardiogram (ECG) data\nraises critical privacy concerns, as its biometric properties make individuals\nvulnerable to linkage attacks. Unlike prior studies that assume idealized\nadversarial capabilities, we evaluate ECG privacy risks under realistic\nconditions where attackers operate with partial knowledge. Using data from 109\nparticipants across diverse real-world datasets, our approach achieves 85%\naccuracy in re-identifying individuals in public datasets while maintaining a\n14.2% overall misclassification rate at an optimal confidence threshold, with\n15.6% of unknown individuals misclassified as known and 12.8% of known\nindividuals misclassified as unknown. These results highlight the inadequacy of\nsimple anonymization techniques in preventing re-identification, demonstrating\nthat even limited adversarial knowledge enables effective identity linkage. Our\nfindings underscore the urgent need for privacy-preserving strategies, such as\ndifferential privacy, access control, and encrypted computation, to mitigate\nre-identification risks while ensuring the utility of shared biosignal data in\nhealthcare applications."}
{"id": "2508.16054", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.16054", "abs": "https://arxiv.org/abs/2508.16054", "authors": ["Sonish Sivarajkumar", "Hang Zhang", "Yuelyu Ji", "Maneesh Bilalpur", "Xizhi Wu", "Chenyu Li", "Min Gu Kwak", "Shyam Visweswaran", "Yanshan Wang"], "title": "Generative Foundation Model for Structured and Unstructured Electronic Health Records", "comment": null, "summary": "Electronic health records (EHRs) are rich clinical data sources but complex\nrepositories of patient data, spanning structured elements (demographics,\nvitals, lab results, codes), unstructured clinical notes and other modalities\nof data. Harnessing this heterogeneity is critical for improving patient\noutcomes. Recent advances in large language models (LLMs) have enabled\nfoundation models that can learn from multiple data modalities and support\nclinical tasks. However, most current approaches simply serialize numeric EHR\ndata into text, which risks losing temporal and quantitative detail. We\nintroduce Generative Deep Patient (GDP), a multimodal foundation model that\nnatively encodes structured EHR time-series via a CNN-Transformer encoder and\nfuses it with unstructured EHRs through cross-modal attention into a\nLLaMA-based decoder. GDP is trained in two stages: (1) generative pretraining,\nwhere it learns to produce clinical narratives from raw patient timelines while\nalso performing masked feature prediction (MFP) and next time-step prediction\n(NTP) to capture temporal dynamics; and (2) multi-task fine-tuning for\nclinically meaningful predictions (e.g., heart failure, type 2 diabetes, 30-day\nreadmission). In clinical prediction, GDP demonstrated superior performance on\nMIMIC-IV: heart failure AUROC = 0.923, type 2 diabetes AUROC = 0.817, and\n30-day readmission AUROC = 0.627. For narrative generation, GDP achieved\nROUGE-L = 0.135 and BERTScore-F1 = 0.545. In a blinded human evaluation,\nGDP-Instruct scored highest on faithfulness, fluency, and overall clinical\nutility, suggesting reduced hospital documentation workload without sacrificing\naccuracy. Our results demonstrate that a single multimodal foundation model can\nboth predict clinically actionable events and generate high-quality clinical\nnarratives. Furthermore, GDP's flexible architecture can be extended to\nadditional modalities."}
{"id": "2508.16071", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.16071", "abs": "https://arxiv.org/abs/2508.16071", "authors": ["Mahinthan Chandramohan", "Jovan Jancic", "Yuntong Zhang", "Padmanabhan Krishnan"], "title": "From Benchmark Data To Applicable Program Repair: An Experience Report", "comment": null, "summary": "This paper describes our approach to automated program repair. We combine\nvarious techniques from the literature to achieve this. Our experiments show\nthat our approach performs better than other techniques on standard benchmarks.\nHowever, on closer inspection, none of these techniques work on realistic\ndefects that we see in industry.\n  We find that augmenting code with formal specifications enables LLMs to\ngenerate higher-quality unit tests, especially for complex production code with\nimproved coverage of edge cases and exception handling. However, specifications\nadd little value for well-understood errors (e.g., null pointer, index out of\nbounds), but are beneficial for logic and string manipulation errors. Despite\nencouraging benchmark results, real-world adoption is limited since passing\ntests do not guarantee correct patches. Current challenges include insufficient\nexpressiveness of the JML specification language, necessitating advanced\nverification tools and richer predicates. Our ongoing work is exploring\ncontract automata, programming by example, and testcase repair, with a focus on\nintegrating human feedback and measuring productivity gains - highlighting the\ngap between academic benchmarks and practical industry needs"}
{"id": "2508.15865", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15865", "abs": "https://arxiv.org/abs/2508.15865", "authors": ["Julia Boone", "Fatemeh Afghah"], "title": "Securing Swarms: Cross-Domain Adaptation for ROS2-based CPS Anomaly Detection", "comment": "Accepted for publication in MILCOM 2025. 6 pages, 2 figures", "summary": "Cyber-physical systems (CPS) are being increasingly utilized for critical\napplications. CPS combines sensing and computing elements, often having\nmulti-layer designs with networking, computational, and physical interfaces,\nwhich provide them with enhanced capabilities for a variety of application\nscenarios. However, the combination of physical and computational elements also\nmakes CPS more vulnerable to attacks compared to network-only systems, and the\nresulting impacts of CPS attacks can be substantial. Intelligent intrusion\ndetection systems (IDS) are an effective mechanism by which CPS can be secured,\nbut the majority of current solutions often train and validate on network\ntraffic-only datasets, ignoring the distinct attacks that may occur on other\nsystem layers. In order to address this, we develop an adaptable CPS anomaly\ndetection model that can detect attacks within CPS without the need for\npreviously labeled data. To achieve this, we utilize domain adaptation\ntechniques that allow us to transfer known attack knowledge from a network\ntraffic-only environment to a CPS environment. We validate our approach using a\nstate-of-the-art CPS intrusion dataset that combines network, operating system\n(OS), and Robot Operating System (ROS) data. Through this dataset, we are able\nto demonstrate the effectiveness of our model across network traffic-only and\nCPS environments with distinct attack types and its ability to outperform other\nanomaly detection methods."}
{"id": "2508.16057", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2508.16057", "abs": "https://arxiv.org/abs/2508.16057", "authors": ["Sijie Yang", "Binyu Lei", "Filip Biljecki"], "title": "Urban Comfort Assessment in the Era of Digital Planning: A Multidimensional, Data-driven, and AI-assisted Framework", "comment": "Presented at 19th International Conference on Computational Urban\n  Planning and Urban Management (CUPUM 2025)", "summary": "Ensuring liveability and comfort is one of the fundamental objectives of\nurban planning. Numerous studies have employed computational methods to assess\nand quantify factors related to urban comfort such as greenery coverage,\nthermal comfort, and walkability. However, a clear definition of urban comfort\nand its comprehensive evaluation framework remain elusive. Our research\nexplores the theoretical interpretations and methodologies for assessing urban\ncomfort within digital planning, emphasising three key dimensions:\nmultidimensional analysis, data support, and AI assistance."}
{"id": "2508.16104", "categories": ["cs.SE", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.16104", "abs": "https://arxiv.org/abs/2508.16104", "authors": ["Arturo Miguel Russell Bernal", "Maureen Petterson", "Pedro Antonio Alarcon Granadeno", "Michael Murphy", "James Mason", "Jane Cleland-Huang"], "title": "Validating Terrain Models in Digital Twins for Trustworthy sUAS Operations", "comment": "Submitted to EDTconf 2025", "summary": "With the increasing deployment of small Unmanned Aircraft Systems (sUAS) in\nunfamiliar and complex environments, Environmental Digital Twins (EDT) that\ncomprise weather, airspace, and terrain data are critical for safe flight\nplanning and for maintaining appropriate altitudes during search and\nsurveillance operations. With the expansion of sUAS capabilities through edge\nand cloud computing, accurate EDT are also vital for advanced sUAS\ncapabilities, like geolocation. However, real-world sUAS deployment introduces\nsignificant sources of uncertainty, necessitating a robust validation process\nfor EDT components. This paper focuses on the validation of terrain models, one\nof the key components of an EDT, for real-world sUAS tasks. These models are\nconstructed by fusing U.S. Geological Survey (USGS) datasets and satellite\nimagery, incorporating high-resolution environmental data to support mission\ntasks. Validating both the terrain models and their operational use by sUAS\nunder real-world conditions presents significant challenges, including limited\ndata granularity, terrain discontinuities, GPS and sensor inaccuracies, visual\ndetection uncertainties, as well as onboard resources and timing constraints.\nWe propose a 3-Dimensions validation process grounded in software engineering\nprinciples, following a workflow across granularity of tests, simulation to\nreal world, and the analysis of simple to edge conditions. We demonstrate our\napproach using a multi-sUAS platform equipped with a Terrain-Aware Digital\nShadow."}
{"id": "2508.15917", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.15917", "abs": "https://arxiv.org/abs/2508.15917", "authors": ["Xiaoli Zhuo", "Xuehu Yan", "Lintao Liu", "Wei Yan"], "title": "Evolving k-Threshold Visual Cryptography Schemes", "comment": null, "summary": "In evolving access structures, the number of participants is countably\ninfinite with no predetermined upper bound. While such structures have been\nrealized in secret sharing, research in secret image sharing has primarily\nfocused on visual cryptography schemes (VCS). However, there exists no\nconstruction for $(k,\\infty)$ VCS that applies to arbitrary $k$ values without\npixel expansion currently, and the contrast requires enhancement. In this\npaper, we first present a formal mathematical definition of $(k,\\infty)$ VCS.\nThen, propose a $(k,\\infty)$ VCS based on random grids that works for arbitrary\n$k$. In addition, to further improve contrast, we develop optimized\n$(k,\\infty)$ VCS for $k=2$ and $3$, along with contrast enhancement strategies\nfor $k\\geq 4$. Theoretical analysis and experimental results demonstrate the\nsuperiority of our proposed schemes."}
{"id": "2508.16059", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.16059", "abs": "https://arxiv.org/abs/2508.16059", "authors": ["Zhuomin Chen", "Dan Li", "Jiahui Zhou", "Shunyu Wu", "Haozheng Ye", "Jian Lou", "See-Kiong Ng"], "title": "Integrating Time Series into LLMs via Multi-layer Steerable Embedding Fusion for Enhanced Forecasting", "comment": "To be published in CIKM 2025", "summary": "Time series (TS) data are ubiquitous across various application areas,\nrendering time series forecasting (TSF) a fundamental task. With the astounding\nadvances in large language models (LLMs), a variety of methods have been\ndeveloped to adapt LLMs for time series forecasting. Despite unlocking the\npotential of LLMs in comprehending TS data, existing methods are inherently\nconstrained by their shallow integration of TS information, wherein LLMs\ntypically access TS representations at shallow layers, primarily at the input\nlayer. This causes the influence of TS representations to progressively fade in\ndeeper layers and eventually leads to ineffective adaptation between textual\nembeddings and TS representations. In this paper, we propose the Multi-layer\nSteerable Embedding Fusion (MSEF), a novel framework that enables LLMs to\ndirectly access time series patterns at all depths, thereby mitigating the\nprogressive loss of TS information in deeper layers. Specifically, MSEF\nleverages off-the-shelf time series foundation models to extract semantically\nrich embeddings, which are fused with intermediate text representations across\nLLM layers via layer-specific steering vectors. These steering vectors are\ndesigned to continuously optimize the alignment between time series and textual\nmodalities and facilitate a layer-specific adaptation mechanism that ensures\nefficient few-shot learning capabilities. Experimental results on seven\nbenchmarks demonstrate significant performance improvements by MSEF compared\nwith baselines, with an average reduction of 31.8% in terms of MSE. The code is\navailable at https://github.com/One1sAll/MSEF."}
{"id": "2508.16131", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.16131", "abs": "https://arxiv.org/abs/2508.16131", "authors": ["Zoe Kotti", "Konstantina Dritsa", "Diomidis Spinellis", "Panos Louridas"], "title": "The Fools are Certain; the Wise are Doubtful: Exploring LLM Confidence in Code Completion", "comment": "30 pages, 10 figures", "summary": "Code completion entails the task of providing missing tokens given a\nsurrounding context. It can boost developer productivity while providing a\npowerful code discovery tool. Following the Large Language Model (LLM) wave,\ncode completion has been approached with diverse LLMs fine-tuned on code (code\nLLMs). The performance of code LLMs can be assessed with downstream and\nintrinsic metrics. Downstream metrics are usually employed to evaluate the\npractical utility of a model, but can be unreliable and require complex\ncalculations and domain-specific knowledge. In contrast, intrinsic metrics such\nas perplexity, entropy, and mutual information, which measure model confidence\nor uncertainty, are simple, versatile, and universal across LLMs and tasks, and\ncan serve as proxies for functional correctness and hallucination risk in\nLLM-generated code. Motivated by this, we evaluate the confidence of LLMs when\ngenerating code by measuring code perplexity across programming languages,\nmodels, and datasets using various LLMs, and a sample of 1008 files from 657\nGitHub projects. We find that strongly-typed languages exhibit lower perplexity\nthan dynamically typed languages. Scripting languages also demonstrate higher\nperplexity. Perl appears universally high in perplexity, whereas Java appears\nlow. Code perplexity depends on the employed LLM, but not on the code dataset.\nAlthough code comments often increase perplexity, the language ranking based on\nperplexity is barely affected by their presence. LLM researchers, developers,\nand users can employ our findings to assess the benefits and suitability of\nLLM-based code completion in specific software projects based on how language,\nmodel choice, and code characteristics impact model confidence."}
{"id": "2508.15934", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.15934", "abs": "https://arxiv.org/abs/2508.15934", "authors": ["Onur Alp Kirci", "M. Emre Gursoy"], "title": "Strategic Sample Selection for Improved Clean-Label Backdoor Attacks in Text Classification", "comment": null, "summary": "Backdoor attacks pose a significant threat to the integrity of text\nclassification models used in natural language processing. While several\ndirty-label attacks that achieve high attack success rates (ASR) have been\nproposed, clean-label attacks are inherently more difficult. In this paper, we\npropose three sample selection strategies to improve attack effectiveness in\nclean-label scenarios: Minimum, Above50, and Below50. Our strategies identify\nthose samples which the model predicts incorrectly or with low confidence, and\nby injecting backdoor triggers into such samples, we aim to induce a stronger\nassociation between the trigger patterns and the attacker-desired target label.\nWe apply our methods to clean-label variants of four canonical backdoor attacks\n(InsertSent, WordInj, StyleBkd, SynBkd) and evaluate them on three datasets\n(IMDB, SST2, HateSpeech) and four model types (LSTM, BERT, DistilBERT,\nRoBERTa). Results show that the proposed strategies, particularly the Minimum\nstrategy, significantly improve the ASR over random sample selection with\nlittle or no degradation in the model's clean accuracy. Furthermore,\nclean-label attacks enhanced by our strategies outperform BITE, a state of the\nart clean-label attack method, in many configurations."}
{"id": "2508.16072", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.16072", "abs": "https://arxiv.org/abs/2508.16072", "authors": ["Zizhen Li", "Chuanhao Li", "Yibin Wang", "Qi Chen", "Diping Song", "Yukang Feng", "Jianwen Sun", "Jiaxin Ai", "Fanrui Zhang", "Mingzhu Sun", "Kaipeng Zhang"], "title": "InMind: Evaluating LLMs in Capturing and Applying Individual Human Reasoning Styles", "comment": "EMNLP 2025 MainConference", "summary": "LLMs have shown strong performance on human-centric reasoning tasks. While\nprevious evaluations have explored whether LLMs can infer intentions or detect\ndeception, they often overlook the individualized reasoning styles that\ninfluence how people interpret and act in social contexts. Social deduction\ngames (SDGs) provide a natural testbed for evaluating individualized reasoning\nstyles, where different players may adopt diverse but contextually valid\nreasoning strategies under identical conditions. To address this, we introduce\nInMind, a cognitively grounded evaluation framework designed to assess whether\nLLMs can capture and apply personalized reasoning styles in SDGs. InMind\nenhances structured gameplay data with round-level strategy traces and\npost-game reflections, collected under both Observer and Participant modes. It\nsupports four cognitively motivated tasks that jointly evaluate both static\nalignment and dynamic adaptation. As a case study, we apply InMind to the game\nAvalon, evaluating 11 state-of-the-art LLMs. General-purpose LLMs, even GPT-4o\nfrequently rely on lexical cues, struggling to anchor reflections in temporal\ngameplay or adapt to evolving strategies. In contrast, reasoning-enhanced LLMs\nlike DeepSeek-R1 exhibit early signs of style-sensitive reasoning. These\nfindings reveal key limitations in current LLMs' capacity for individualized,\nadaptive reasoning, and position InMind as a step toward cognitively aligned\nhuman-AI interaction."}
{"id": "2508.16165", "categories": ["cs.SE", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.16165", "abs": "https://arxiv.org/abs/2508.16165", "authors": ["Sebastian Lubos", "Alexander Felfernig", "Gerhard Leitner", "Julian Schwazer"], "title": "Towards Recommending Usability Improvements with Multimodal Large Language Models", "comment": null, "summary": "Usability describes a set of essential quality attributes of user interfaces\n(UI) that influence human-computer interaction. Common evaluation methods, such\nas usability testing and inspection, are effective but resource-intensive and\nrequire expert involvement. This makes them less accessible for smaller\norganizations. Recent advances in multimodal LLMs offer promising opportunities\nto automate usability evaluation processes partly by analyzing textual, visual,\nand structural aspects of software interfaces. To investigate this possibility,\nwe formulate usability evaluation as a recommendation task, where multimodal\nLLMs rank usability issues by severity. We conducted an initial\nproof-of-concept study to compare LLM-generated usability improvement\nrecommendations with usability expert assessments. Our findings indicate the\npotential of LLMs to enable faster and more cost-effective usability\nevaluation, which makes it a practical alternative in contexts with limited\nexpert resources."}
{"id": "2508.15987", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.15987", "abs": "https://arxiv.org/abs/2508.15987", "authors": ["Andreas D. Kellas", "Neophytos Christou", "Wenxin Jiang", "Penghui Li", "Laurent Simon", "Yaniv David", "Vasileios P. Kemerlis", "James C. Davis", "Junfeng Yang"], "title": "PickleBall: Secure Deserialization of Pickle-based Machine Learning Models", "comment": "To be published in the proceedings of 2025 ACM CCS", "summary": "Machine learning model repositories such as the Hugging Face Model Hub\nfacilitate model exchanges. However, bad actors can deliver malware through\ncompromised models. Existing defenses such as safer model formats, restrictive\n(but inflexible) loading policies, and model scanners have shortcomings: 44.9%\nof popular models on Hugging Face still use the insecure pickle format, 15% of\nthese cannot be loaded by restrictive loading policies, and model scanners have\nboth false positives and false negatives. Pickle remains the de facto standard\nfor model exchange, and the ML community lacks a tool that offers transparent\nsafe loading.\n  We present PickleBall to help machine learning engineers load pickle-based\nmodels safely. PickleBall statically analyzes the source code of a given\nmachine learning library and computes a custom policy that specifies a safe\nload-time behavior for benign models. PickleBall then dynamically enforces the\npolicy during load time as a drop-in replacement for the pickle module.\nPickleBall generates policies that correctly load 79.8% of benign pickle-based\nmodels in our dataset, while rejecting all (100%) malicious examples in our\ndataset. In comparison, evaluated model scanners fail to identify known\nmalicious models, and the state-of-art loader loads 22% fewer benign models\nthan PickleBall. PickleBall removes the threat of arbitrary function invocation\nfrom malicious pickle-based models, raising the bar for attackers to depend on\ncode reuse techniques."}
{"id": "2508.16112", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.16112", "abs": "https://arxiv.org/abs/2508.16112", "authors": ["Heewoong Noh", "Namkyeong Lee", "Gyoung S. Na", "Kibum Kim", "Chanyoung Park"], "title": "IR-Agent: Expert-Inspired LLM Agents for Structure Elucidation from Infrared Spectra", "comment": null, "summary": "Spectral analysis provides crucial clues for the elucidation of unknown\nmaterials. Among various techniques, infrared spectroscopy (IR) plays an\nimportant role in laboratory settings due to its high accessibility and low\ncost. However, existing approaches often fail to reflect expert analytical\nprocesses and lack flexibility in incorporating diverse types of chemical\nknowledge, which is essential in real-world analytical scenarios. In this\npaper, we propose IR-Agent, a novel multi-agent framework for molecular\nstructure elucidation from IR spectra. The framework is designed to emulate\nexpert-driven IR analysis procedures and is inherently extensible. Each agent\nspecializes in a specific aspect of IR interpretation, and their complementary\nroles enable integrated reasoning, thereby improving the overall accuracy of\nstructure elucidation. Through extensive experiments, we demonstrate that\nIR-Agent not only improves baseline performance on experimental IR spectra but\nalso shows strong adaptability to various forms of chemical information."}
{"id": "2508.16181", "categories": ["cs.SE", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.16181", "abs": "https://arxiv.org/abs/2508.16181", "authors": ["Zirui Li", "Stephan Husung", "Haoze Wang"], "title": "LLM-Assisted Semantic Alignment and Integration in Collaborative Model-Based Systems Engineering Using SysML v2", "comment": "Accepted by IEEE ISSE 2025, DOI pending", "summary": "Cross-organizational collaboration in Model-Based Systems Engineering (MBSE)\nfaces many challenges in achieving semantic alignment across independently\ndeveloped system models. SysML v2 introduces enhanced structural modularity and\nformal semantics, offering a stronger foundation for interoperable modeling.\nMeanwhile, GPT-based Large Language Models (LLMs) provide new capabilities for\nassisting model understanding and integration. This paper proposes a\nstructured, prompt-driven approach for LLM-assisted semantic alignment of SysML\nv2 models. The core contribution lies in the iterative development of an\nalignment approach and interaction prompts, incorporating model extraction,\nsemantic matching, and verification. The approach leverages SysML v2 constructs\nsuch as alias, import, and metadata extensions to support traceable, soft\nalignment integration. It is demonstrated with a GPT-based LLM through an\nexample of a measurement system. Benefits and limitations are discussed."}
{"id": "2508.16078", "categories": ["cs.CR", "cs.NI"], "pdf": "https://arxiv.org/pdf/2508.16078", "abs": "https://arxiv.org/abs/2508.16078", "authors": ["Nadeem Ahmed", "Lei Zhang", "Aryya Gangopadhyay"], "title": "A Survey of Post-Quantum Cryptography Support in Cryptographic Libraries", "comment": "To be published in IEEE International Conference on Quantum Computing\n  and Engineering (QCE) 2025", "summary": "The rapid advancement of quantum computing poses a significant threat to\nmodern cryptographic systems, necessitating the transition to Post-Quantum\nCryptography (PQC). This study evaluates the support for PQC algorithms within\nnine widely used open-source cryptographic libraries -- OpenSSL, wolfSSL,\nBoringSSL, LibreSSL, Bouncy Castle, libsodium, Crypto++, Botan, and MbedTLS --\nfocusing on their implementation of the NIST-selected PQC finalists:\nCRYSTALS-Kyber, CRYSTALS-Dilithium, FALCON, and SPHINCS+. Our analysis, based\non the latest available documentation, release notes, and industry reports as\nof early 2025, reveals a varied state of readiness across these libraries.\nWhile some libraries have integrated PQC support or have clear implementation\nroadmaps, others lag behind, creating potential security risks as quantum\nthreats become more imminent. We discuss key challenges, including performance\ntrade-offs, implementation security, and adoption hurdles in real-world\ncryptographic applications. Our findings highlight the urgent need for\ncontinued research, standardization efforts, and coordinated adoption\nstrategies to ensure a secure transition to the quantum-resistant cryptographic\nlandscape."}
{"id": "2508.16117", "categories": ["cs.AI", "cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.16117", "abs": "https://arxiv.org/abs/2508.16117", "authors": ["Saransh Kumar Gupta", "Rizwan Gulzar Mir", "Lipika Dey", "Partha Pratim Das", "Anirban Sen", "Ramesh Jain"], "title": "Extending FKG.in: Towards a Food Claim Traceability Network", "comment": "10 pages, 3 figures, 1 table, 45 references, ACM International\n  Conference on Multimedia 2025 - Multi-modal Food Computing Workshop", "summary": "The global food landscape is rife with scientific, cultural, and commercial\nclaims about what foods are, what they do, what they should not do, or should\nnot do. These range from rigorously studied health benefits (probiotics improve\ngut health) and misrepresentations (soaked almonds make one smarter) to vague\npromises (superfoods boost immunity) and culturally rooted beliefs (cold foods\ncause coughs). Despite their widespread influence, the infrastructure for\ntracing, verifying, and contextualizing these claims remains fragmented and\nunderdeveloped. In this paper, we propose a Food Claim-Traceability Network\n(FCN) as an extension of FKG.in, a knowledge graph of Indian food that we have\nbeen incrementally building. We also present the ontology design and the\nsemi-automated knowledge curation workflow that we used to develop a proof of\nconcept of FKG.in-FCN using Reddit data and Large Language Models. FCN\nintegrates curated data inputs, structured schemas, and provenance-aware\npipelines for food-related claim extraction and validation. While directly\nlinked to the Indian food knowledge graph as an application, our methodology\nremains application-agnostic and adaptable to other geographic, culinary, or\nregulatory settings. By modeling food claims and their traceability in a\nstructured, verifiable, and explainable way, we aim to contribute to more\ntransparent and accountable food knowledge ecosystems, supporting researchers,\npolicymakers, and most importantly, everyday consumers in navigating a world\nsaturated with dietary assertions."}
{"id": "2508.16273", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.16273", "abs": "https://arxiv.org/abs/2508.16273", "authors": ["Maria Teresa Rossi", "Martina De Sanctis", "Ludovico Iovino", "Manuel Wimmer"], "title": "A Systematic Mapping Study on Smart Cities Modeling Approaches", "comment": null, "summary": "The Smart City concept was introduced to define an idealized city\ncharacterized by automation and connection. It then evolved rapidly by\nincluding further aspects, such as economy, environment. Since then, many\npublications have explored various aspects of Smart Cities across different\napplication domains and research communities, acknowledging the\ninterdisciplinary nature of this subject. In particular, our interest focuses\non how smart cities are designed and modeled, as a whole or as regards with\ntheir subsystems, when dealing with the accomplishment of the research goals in\nthis complex and heterogeneous domain. To this aim, we performed a systematic\nmapping study on smart cities modeling approaches identifying the relevant\ncontributions (i) to get an overview of existing research approaches, (ii) to\nidentify whether there are any publication trends, and (iii) to identify\npossible future research directions. We followed the guidelines for conducting\nsystematic mapping studies by Petersen et al. to analyze smart cities modeling\npublications. Our analysis revealed the following main findings: (i) smart\ngovernance is the most investigated and modeled smart city dimension; (ii) the\nmost used modeling approaches are business, architectural, and ontological\nmodeling approaches, spanning multiple application fields; (iii) the great\nmajority of existing technologies for modeling smart cities are not yet proven\nin operational environments; (iv) diverse research communities publish their\nresults in a multitude of different venues which further motivates the\npresented literature study. Researchers can use our results for better\nunderstanding the state-of-the-art in modeling smart cities, and as a\nfoundation for further analysis of specific approaches about smart cities\nmodeling. Lastly, we also discuss the impact of our analysis for the\nModel-Driven Engineering community."}
{"id": "2508.16133", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.16133", "abs": "https://arxiv.org/abs/2508.16133", "authors": ["Shilin Xiao", "Wenjun Zhu", "Yan Jiang", "Kai Wang", "Peiwang Wang", "Chen Yan", "Xiaoyu Ji", "Wenyuan Xu"], "title": "SoK: Understanding the Fundamentals and Implications of Sensor Out-of-band Vulnerabilities", "comment": "Accepted by NDSS 2026", "summary": "Sensors are fundamental to cyber-physical systems (CPS), enabling perception\nand control by transducing physical stimuli into digital measurements. However,\ndespite growing research on physical attacks on sensors, our understanding of\nsensor hardware vulnerabilities remains fragmented due to the ad-hoc nature of\nthis field. Moreover, the infinite attack signal space further complicates\nthreat abstraction and defense. To address this gap, we propose a\nsystematization framework, termed sensor out-of-band (OOB) vulnerabilities,\nthat for the first time provides a comprehensive abstraction for sensor attack\nsurfaces based on underlying physical principles. We adopt a bottom-up\nsystematization methodology that analyzes OOB vulnerabilities across three\nlevels. At the component level, we identify the physical principles and\nlimitations that contribute to OOB vulnerabilities. At the sensor level, we\ncategorize known attacks and evaluate their practicality. At the system level,\nwe analyze how CPS features such as sensor fusion, closed-loop control, and\nintelligent perception impact the exposure and mitigation of OOB threats. Our\nfindings offer a foundational understanding of sensor hardware security and\nprovide guidance and future directions for sensor designers, security\nresearchers, and system developers aiming to build more secure sensors and CPS."}
{"id": "2508.16129", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.16129", "abs": "https://arxiv.org/abs/2508.16129", "authors": ["Ruiqi Wu", "Yuang Yao", "Tengfei Ma", "Chenran Zhang", "Na Su", "Tao Zhou", "Geng Chen", "Wen Fan", "Yi Zhou"], "title": "Bridging the Gap in Ophthalmic AI: MM-Retinal-Reason Dataset and OphthaReason Model toward Dynamic Multimodal Reasoning", "comment": null, "summary": "Multimodal large language models (MLLMs) have recently demonstrated\nremarkable reasoning abilities with reinforcement learning paradigm. Although\nseveral multimodal reasoning models have been explored in the medical domain,\nmost of them focus exclusively on basic reasoning, which refers to shallow\ninference based on visual feature matching. However, real-world clinical\ndiagnosis extends beyond basic reasoning, demanding reasoning processes that\nintegrate heterogeneous clinical information (such as chief complaints and\nmedical history) with multimodal medical imaging data. To bridge this gap, we\nintroduce MM-Retinal-Reason, the first ophthalmic multimodal dataset with the\nfull spectrum of perception and reasoning. It encompasses both basic reasoning\ntasks and complex reasoning tasks, aiming to enhance visual-centric fundamental\nreasoning capabilities and emulate realistic clinical thinking patterns.\nBuilding upon MM-Retinal-Reason, we propose OphthaReason, the first\nophthalmology-specific multimodal reasoning model with step-by-step reasoning\ntraces. To enable flexible adaptation to both basic and complex reasoning\ntasks, we specifically design a novel method called Uncertainty-Aware Dynamic\nThinking (UADT), which estimates sample-level uncertainty via entropy and\ndynamically modulates the model's exploration depth using a shaped advantage\nmechanism. Comprehensive experiments demonstrate that our model achieves\nstate-of-the-art performance on both basic and complex reasoning tasks,\noutperforming general-purpose MLLMs, medical MLLMs, RL-based medical MLLMs, and\nophthalmic MLLMs by at least 24.92\\%, 15.00\\%, 21.20\\%, and 17.66\\%. Project\nPage: \\href{https://github.com/lxirich/OphthaReason}{link}."}
{"id": "2508.16307", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.16307", "abs": "https://arxiv.org/abs/2508.16307", "authors": ["Jinsheng Ba", "Yuancheng Jiang", "Manuel Rigger"], "title": "Metamorphic Coverage", "comment": null, "summary": "Metamorphic testing is a widely used methodology that examines an expected\nrelation between pairs of executions to automatically find bugs, such as\ncorrectness bugs. We found that code coverage cannot accurately measure the\nextent to which code is validated and mutation testing is computationally\nexpensive for evaluating metamorphic testing methods. In this work, we propose\nMetamorphic Coverage (MC), a coverage metric that examines the distinct code\nexecuted by pairs of test inputs within metamorphic testing. Our intuition is\nthat, typically, a bug can be observed if the corresponding code is executed\nwhen executing either test input but not the other one, so covering more\ndifferential code covered by pairs of test inputs might be more likely to\nexpose bugs. While most metamorphic testing methods have been based on this\ngeneral intuition, our work defines and systematically evaluates MC on five\nwidely used metamorphic testing methods for testing database engines,\ncompilers, and constraint solvers. The code measured by MC overlaps with the\nbug-fix locations of 50 of 64 bugs found by metamorphic testing methods, and MC\nhas a stronger positive correlation with bug numbers than line coverage. MC is\n4x more sensitive than line coverage in distinguishing testing methods'\neffectiveness, and the average value of MC is 6x smaller than line coverage\nwhile still capturing the part of the program that is being tested. MC required\n359x less time than mutation testing. Based on a case study for an automated\ndatabase system testing approach, we demonstrate that when used for feedback\nguidance, MC significantly outperforms code coverage, by finding 41\\% more\nbugs. Consequently, this work might have broad applications for assessing\nmetamorphic testing methods and improving test-case generation."}
{"id": "2508.16150", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.16150", "abs": "https://arxiv.org/abs/2508.16150", "authors": ["Aristeidis Sidiropoulos", "Christos Chrysanthos Nikolaidis", "Theodoros Tsiolakis", "Nikolaos Pavlidis", "Vasilis Perifanis", "Pavlos S. Efraimidis"], "title": "Evaluating the Defense Potential of Machine Unlearning against Membership Inference Attacks", "comment": null, "summary": "Membership Inference Attacks (MIAs) pose a significant privacy risk, as they\nenable adversaries to determine whether a specific data point was included in\nthe training dataset of a model. While Machine Unlearning is primarily designed\nas a privacy mechanism to efficiently remove private data from a machine\nlearning model without the need for full retraining, its impact on the\nsusceptibility of models to MIA remains an open question. In this study, we\nsystematically assess the vulnerability of models to MIA after applying\nstate-of-art Machine Unlearning algorithms. Our analysis spans four diverse\ndatasets (two from the image domain and two in tabular format), exploring how\ndifferent unlearning approaches influence the exposure of models to membership\ninference. The findings highlight that while Machine Unlearning is not\ninherently a countermeasure against MIA, the unlearning algorithm and data\ncharacteristics can significantly affect a model's vulnerability. This work\nprovides essential insights into the interplay between Machine Unlearning and\nMIAs, offering guidance for the design of privacy-preserving machine learning\nsystems."}
{"id": "2508.16172", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.16172", "abs": "https://arxiv.org/abs/2508.16172", "authors": ["Kai Hu", "Parfait Atchade-Adelomou", "Carlo Adornetto", "Adrian Mora-Carrero", "Luis Alonso-Pastor", "Ariel Noyman", "Yubo Liu", "Kent Larson"], "title": "Graph RAG as Human Choice Model: Building a Data-Driven Mobility Agent with Preference Chain", "comment": null, "summary": "Understanding human behavior in urban environments is a crucial field within\ncity sciences. However, collecting accurate behavioral data, particularly in\nnewly developed areas, poses significant challenges. Recent advances in\ngenerative agents, powered by Large Language Models (LLMs), have shown promise\nin simulating human behaviors without relying on extensive datasets.\nNevertheless, these methods often struggle with generating consistent,\ncontext-sensitive, and realistic behavioral outputs. To address these\nlimitations, this paper introduces the Preference Chain, a novel method that\nintegrates Graph Retrieval-Augmented Generation (RAG) with LLMs to enhance\ncontext-aware simulation of human behavior in transportation systems.\nExperiments conducted on the Replica dataset demonstrate that the Preference\nChain outperforms standard LLM in aligning with real-world transportation mode\nchoices. The development of the Mobility Agent highlights potential\napplications of proposed method in urban mobility modeling for emerging cities,\npersonalized travel behavior analysis, and dynamic traffic forecasting. Despite\nlimitations such as slow inference and the risk of hallucination, the method\noffers a promising framework for simulating complex human behavior in\ndata-scarce environments, where traditional data-driven models struggle due to\nlimited data availability."}
{"id": "2508.16318", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.16318", "abs": "https://arxiv.org/abs/2508.16318", "authors": ["Juan C. Alonso", "Alberto Martin-Lopez", "Sergio Segura", "Gabriele Bavota", "Antonio Ruiz-Cortés"], "title": "SATORI: Static Test Oracle Generation for REST APIs", "comment": "Accepted for publication at 40th IEEE/ACM International Conference on\n  Automated Software Engineering, ASE 2025", "summary": "REST API test case generation tools are evolving rapidly, with growing\ncapabilities for the automated generation of complex tests. However, despite\ntheir strengths in test data generation, these tools are constrained by the\ntypes of test oracles they support, often limited to crashes, regressions, and\nnoncompliance with API specifications or design standards. This paper\nintroduces SATORI (Static API Test ORacle Inference), a black-box approach for\ngenerating test oracles for REST APIs by analyzing their OpenAPI Specification.\nSATORI uses large language models to infer the expected behavior of an API by\nanalyzing the properties of the response fields of its operations, such as\ntheir name and descriptions. To foster its adoption, we extended the\nPostmanAssertify tool to automatically convert the test oracles reported by\nSATORI into executable assertions. Evaluation results on 17 operations from 12\nindustrial APIs show that SATORI can automatically generate up to hundreds of\nvalid test oracles per operation. SATORI achieved an F1-score of 74.3%,\noutperforming the state-of-the-art dynamic approach AGORA+ (69.3%)-which\nrequires executing the API-when generating comparable oracle types. Moreover,\nour findings show that static and dynamic oracle inference methods are\ncomplementary: together, SATORI and AGORA+ found 90% of the oracles in our\nannotated ground-truth dataset. Notably, SATORI uncovered 18 bugs in popular\nAPIs (Amadeus Hotel, Deutschebahn, FDIC, GitLab, Marvel, OMDb and Vimeo)\nleading to documentation updates by the API maintainers."}
{"id": "2508.16189", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.16189", "abs": "https://arxiv.org/abs/2508.16189", "authors": ["Aparna Singh", "Geetanjali Rathee", "Chaker Abdelaziz Kerrache", "Mohamed Chahine Ghanem"], "title": "A Relay-Chain-Powered Ciphertext-Policy Attribute-Based Encryption in Intelligent Transportation Systems", "comment": null, "summary": "The very high growth of Intelligent Transportation Systems (ITS) has\ngenerated an urgent requirement for secure, effective, and context-aware data\nsharing mechanisms, especially over heterogeneous and geographically dispersed\nsettings. This work suggests a new architecture that combines a relay\nchain-driven encryption system with a modified Ciphertext-Policy\nAttribute-Based Encryption (CP-ABE) scheme to tackle the double impediment of\ndynamic access and low-latency communication. The model proposes a\ncontext-aware smart contract on a worldwide relay chain that checks against\ndata properties, including event type, time, and geographical region, to\nspecify the suitable level of encryption policy. From such relay-directed\njudgment, On-Board Units (OBUs) encrypt data end-to-end by utilising CP-ABE and\nstore ciphertext inside localised regional blockchains, preventing dependence\non symmetric encryption or off-chain storage. High-sensitivity events are\nsecured with firm, multi-attribute access rules, whereas common updates use\nlight policies to help reduce processing burdens. The crypto system also adds\ntraceability and low-latency revocation, with global enforcement managed\nthrough the relay chain. This distributed, scalable model provides a proper\nbalance between responsiveness in real time and security and is extremely apt\nfor next-gen vehicular networks that function across multi-jurisdictional\ndomains."}
{"id": "2508.16204", "categories": ["cs.AI", "cs.NE"], "pdf": "https://arxiv.org/pdf/2508.16204", "abs": "https://arxiv.org/abs/2508.16204", "authors": ["João Abrantes", "Robert Tjarko Lange", "Yujin Tang"], "title": "Competition and Attraction Improve Model Fusion", "comment": "Accepted at GECCO 2025 as a full paper", "summary": "Model merging is a powerful technique for integrating the specialized\nknowledge of multiple machine learning models into a single model. However,\nexisting methods require manually partitioning model parameters into fixed\ngroups for merging, which restricts the exploration of potential combinations\nand limits performance. To overcome these limitations, we propose Model Merging\nof Natural Niches (M2N2), an evolutionary algorithm with three key features:\n(1) dynamic adjustment of merging boundaries to progressively explore a broader\nrange of parameter combinations; (2) a diversity preservation mechanism\ninspired by the competition for resources in nature, to maintain a population\nof diverse, high-performing models that are particularly well-suited for\nmerging; and (3) a heuristicbased attraction metric to identify the most\npromising pairs of models for fusion. Our experimental results demonstrate, for\nthe first time, that model merging can be used to evolve models entirely from\nscratch. Specifically, we apply M2N2 to evolve MNIST classifiers from scratch\nand achieve performance comparable to CMA-ES, while being computationally more\nefficient. Furthermore, M2N2 scales to merge specialized language and image\ngeneration models, achieving state-of-the-art performance. Notably, it\npreserves crucial model capabilities beyond those explicitly optimized by the\nfitness function, highlighting its robustness and versatility. Our code is\navailable at https://github.com/SakanaAI/natural_niches"}
{"id": "2508.16341", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.16341", "abs": "https://arxiv.org/abs/2508.16341", "authors": ["Sebastian Copei", "Oliver Hohlfeld", "Jens Kosiol"], "title": "The (C)omprehensive (A)rchitecture (P)attern (I)ntegration method: Navigating the sea of technology", "comment": null, "summary": "The technological landscape changes daily, making it nearly impossible for a\nsingle person to be aware of all trends or available tools that may or may not\nbe suitable for their software project. This makes tool selection and\narchitectural design decisions a complex problem, especially for large-scale\nsoftware systems. To tackle this issue, we introduce CAPI, the Comprehensive\nArchitecture Pattern Integration method that uses a diagnostic decision tree to\nsuggest architectural patterns depending on user needs. By suggesting patterns\ninstead of tools, the overall complexity for further decisions is lower as\nthere are fewer architectural patterns than tools due to the abstract nature of\npatterns. Moreover, since tools implement patterns, each non-proposed pattern\nreduces the number of tools to choose from, reducing complexity. We iteratively\ndeveloped CAPI, evaluating its understandability and usability in small studies\nwith academic participants. When satisfied with the outcome, we performed a\nuser-study with industry representatives to investigate the state-of-the-art in\ntechnology selection and the effectiveness of our proposed method. We find that\ntechnology selection is largely performed via trial and error, that CAPI is\nuniformly perceived as helpful, and that CAPI is able to reproduce the\nproductive architectural environments of our participants."}
{"id": "2508.16202", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.16202", "abs": "https://arxiv.org/abs/2508.16202", "authors": ["Shu-Jie Cao", "Dongning Guo"], "title": "How to Beat Nakamoto in the Race", "comment": "Accepted for presentation at the 2025 ACM Conference on Computer and\n  Communications Security (CCS)", "summary": "This paper studies proof-of-work Nakamoto consensus under bounded network\ndelays, settling two long-standing questions in blockchain security: How can an\nadversary most effectively attack block safety under a given block confirmation\nlatency? And what is the resulting probability of safety violation? A Markov\ndecision process (MDP) framework is introduced to precise characterize the\nsystem state (including the tree and timings of all blocks mined), the\nadversary's potential actions, and the state transitions due to the adversarial\naction and the random block arrival processes. An optimal attack, called\nbait-and-switch, is proposed and proved to maximize the adversary's chance of\nviolating block safety by \"beating Nakamoto in the race\". The exact probability\nof this violation is calculated for any confirmation depth using Markov chain\nanalysis, offering fresh insights into the interplay of network delay,\nconfirmation rules, and blockchain security."}
{"id": "2508.16277", "categories": ["cs.AI", "cs.HC", "68T01, 68T05, 68T42, 91A80", "I.2; K.4"], "pdf": "https://arxiv.org/pdf/2508.16277", "abs": "https://arxiv.org/abs/2508.16277", "authors": ["Alexandru Tugui"], "title": "The next question after Turing's question: Introducing the Grow-AI test", "comment": "9th International Conference on Inventive Systems and Control ICISC\n  2025", "summary": "This study aims to extend the framework for assessing artificial\nintelligence, called GROW-AI (Growth and Realization of Autonomous Wisdom),\ndesigned to answer the question \"Can machines grow up?\" -- a natural successor\nto the Turing Test. The methodology applied is based on a system of six primary\ncriteria (C1-C6), each assessed through a specific \"game\", divided into four\narenas that explore both the human dimension and its transposition into AI. All\ndecisions and actions of the entity are recorded in a standardized AI Journal,\nthe primary source for calculating composite scores. The assessment uses the\nprior expert method to establish initial weights, and the global score -- Grow\nUp Index -- is calculated as the arithmetic mean of the six scores, with\ninterpretation on maturity thresholds. The results show that the methodology\nallows for a coherent and comparable assessment of the level of \"growth\" of AI\nentities, regardless of their type (robots, software agents, LLMs). The\nmulti-game structure highlights strengths and vulnerable areas, and the use of\na unified journal guarantees traceability and replicability in the evaluation.\nThe originality of the work lies in the conceptual transposition of the process\nof \"growing\" from the human world to that of artificial intelligence, in an\nintegrated testing format that combines perspectives from psychology, robotics,\ncomputer science, and ethics. Through this approach, GROW-AI not only measures\nperformance but also captures the evolutionary path of an AI entity towards\nmaturity."}
{"id": "2508.16402", "categories": ["cs.SE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.16402", "abs": "https://arxiv.org/abs/2508.16402", "authors": ["Zihan Wang", "Jiaze Chen", "Zhicheng Liu", "Markus Mak", "Yidi Du", "Geonsik Moon", "Luoqi Xu", "Aaron Tua", "Kunshuo Peng", "Jiayi Lu", "Mingfei Xia", "Boqian Zou", "Chenyang Ran", "Guang Tian", "Shoutai Zhu", "Yeheng Duan", "Zhenghui Kang", "Zhenxing Lin", "Shangshu Li", "Qiang Luo", "Qingshen Long", "Zhiyong Chen", "Yihan Xiao", "Yurong Wu", "Daoguang Zan", "Yuyi Fu", "Mingxuan Wang", "Ming Ding"], "title": "AetherCode: Evaluating LLMs' Ability to Win In Premier Programming Competitions", "comment": "15 pages", "summary": "Competitive programming has emerged as a critical benchmark for evaluating\nthe reasoning and coding capabilities of Large Language Models (LLMs). Despite\nimpressive progress on existing benchmarks, we argue that current evaluations\noverstate model proficiency, masking a substantial gap between LLMs and elite\nhuman programmers. This gap arises from two key limitations: insufficient\ndifficulty and scope of benchmark problems, and evaluation bias from\nlow-quality test cases. To address these shortcomings, we present AetherCode, a\nnew benchmark that draws problems from premier programming competitions such as\nIOI and ICPC, offering broader coverage and higher difficulty. AetherCode\nfurther incorporates comprehensive, expert-validated test suites built through\na hybrid of automated generation and human curation, ensuring rigorous and\nreliable assessment. By combining challenging problem design with robust\nevaluation, AetherCode provides a more faithful measure of LLM capabilities and\nsets a new standard for future research in code reasoning."}
{"id": "2508.16347", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.16347", "abs": "https://arxiv.org/abs/2508.16347", "authors": ["Yu Yan", "Sheng Sun", "Zhe Wang", "Yijun Lin", "Zenghao Duan", "zhifei zheng", "Min Liu", "Zhiyi yin", "Jianping Zhang"], "title": "Confusion is the Final Barrier: Rethinking Jailbreak Evaluation and Investigating the Real Misuse Threat of LLMs", "comment": null, "summary": "With the development of Large Language Models (LLMs), numerous efforts have\nrevealed their vulnerabilities to jailbreak attacks. Although these studies\nhave driven the progress in LLMs' safety alignment, it remains unclear whether\nLLMs have internalized authentic knowledge to deal with real-world crimes, or\nare merely forced to simulate toxic language patterns. This ambiguity raises\nconcerns that jailbreak success is often attributable to a hallucination loop\nbetween jailbroken LLM and judger LLM. By decoupling the use of jailbreak\ntechniques, we construct knowledge-intensive Q\\&A to investigate the misuse\nthreats of LLMs in terms of dangerous knowledge possession, harmful task\nplanning utility, and harmfulness judgment robustness. Experiments reveal a\nmismatch between jailbreak success rates and harmful knowledge possession in\nLLMs, and existing LLM-as-a-judge frameworks tend to anchor harmfulness\njudgments on toxic language patterns. Our study reveals a gap between existing\nLLM safety assessments and real-world threat potential."}
{"id": "2508.16279", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.16279", "abs": "https://arxiv.org/abs/2508.16279", "authors": ["Dawei Gao", "Zitao Li", "Yuexiang Xie", "Weirui Kuang", "Liuyi Yao", "Bingchen Qian", "Zhijian Ma", "Yue Cui", "Haohao Luo", "Shen Li", "Lu Yi", "Yi Yu", "Shiqi He", "Zhiling Luo", "Wenmeng Zhou", "Zhicheng Zhang", "Xuguang He", "Ziqian Chen", "Weikai Liao", "Farruh Isakulovich Kushnazarov", "Yaliang Li", "Bolin Ding", "Jingren Zhou"], "title": "AgentScope 1.0: A Developer-Centric Framework for Building Agentic Applications", "comment": null, "summary": "Driven by rapid advancements of Large Language Models (LLMs), agents are\nempowered to combine intrinsic knowledge with dynamic tool use, greatly\nenhancing their capacity to address real-world tasks. In line with such an\nevolution, AgentScope introduces major improvements in a new version (1.0),\ntowards comprehensively supporting flexible and efficient tool-based\nagent-environment interactions for building agentic applications. Specifically,\nwe abstract foundational components essential for agentic applications and\nprovide unified interfaces and extensible modules, enabling developers to\neasily leverage the latest progress, such as new models and MCPs. Furthermore,\nwe ground agent behaviors in the ReAct paradigm and offer advanced agent-level\ninfrastructure based on a systematic asynchronous design, which enriches both\nhuman-agent and agent-agent interaction patterns while improving execution\nefficiency. Building on this foundation, we integrate several built-in agents\ntailored to specific practical scenarios. AgentScope also includes robust\nengineering support for developer-friendly experiences. We provide a scalable\nevaluation module with a visual studio interface, making the development of\nlong-trajectory agentic applications more manageable and easier to trace. In\naddition, AgentScope offers a runtime sandbox to ensure safe agent execution\nand facilitates rapid deployment in production environments. With these\nenhancements, AgentScope provides a practical foundation for building scalable,\nadaptive, and effective agentic applications."}
{"id": "2508.16419", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.16419", "abs": "https://arxiv.org/abs/2508.16419", "authors": ["Akshay Mhatre", "Noujoud Nader", "Patrick Diehl", "Deepti Gupta"], "title": "LLM-GUARD: Large Language Model-Based Detection and Repair of Bugs and Security Vulnerabilities in C++ and Python", "comment": null, "summary": "Large Language Models (LLMs) such as ChatGPT-4, Claude 3, and LLaMA 4 are\nincreasingly embedded in software/application development, supporting tasks\nfrom code generation to debugging. Yet, their real-world effectiveness in\ndetecting diverse software bugs, particularly complex, security-relevant\nvulnerabilities, remains underexplored. This study presents a systematic,\nempirical evaluation of these three leading LLMs using a benchmark of\nfoundational programming errors, classic security flaws, and advanced,\nproduction-grade bugs in C++ and Python. The dataset integrates real code from\nSEED Labs, OpenSSL (via the Suresoft GLaDOS database), and PyBugHive, validated\nthrough local compilation and testing pipelines. A novel multi-stage,\ncontext-aware prompting protocol simulates realistic debugging scenarios, while\na graded rubric measures detection accuracy, reasoning depth, and remediation\nquality. Our results show that all models excel at identifying syntactic and\nsemantic issues in well-scoped code, making them promising for educational use\nand as first-pass reviewers in automated code auditing. Performance diminishes\nin scenarios involving complex security vulnerabilities and large-scale\nproduction code, with ChatGPT-4 and Claude 3 generally providing more nuanced\ncontextual analyses than LLaMA 4. This highlights both the promise and the\npresent constraints of LLMs in serving as reliable code analysis tools."}
{"id": "2508.16405", "categories": ["cs.CR", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2508.16405", "abs": "https://arxiv.org/abs/2508.16405", "authors": ["Min Wang", "Chuanpeng Jiang", "Zhaohao Wang", "Zhengyi Hou", "Zhongkui Zhang", "Yuanfu Zhao", "Hongxi Liu", "Weisheng Zhao"], "title": "Temperature-Resilient Reconfigurable PUF with Dual-Pulse Modulation based on SOT-MRAM Chip", "comment": null, "summary": "In the Internet of Things (IoT) era, hardware-based security solutions have\nbecome an emerging choice for enhancing end-terminal information security. As\none of the hardware technologies, physical unclonable functions (PUFs) utilize\nthe inherent variations in the manufacturing process to generate cryptographic\nkeys. Reconfigurable PUFs (rPUFs), characterized by updating cryptographic\nkeys, offer enhanced security ability for protecting massive amounts of data in\ndynamic operational scenarios. The core challenge lies in achieving real-time\nreconfiguration independent of environmental conditions, particularly operating\ntemperature, which has rarely been investigated and addressed. In this study,\nwe propose a dual-pulse reconfiguration strategy based on SOT-MRAM carriers,\nwhich effectively widens the operating window and exhibits excellent PUF\nmetrics. Experimental results demonstrate that our design achieves real-time\nreconfiguration across industrial-grade operating temperature ranges, without\nthe need for dynamic feedback of real-time temperature. The proposed SOT-MRAM\nrPUF design lays a solid foundation for next-generation IoT protection\narchitectures."}
{"id": "2508.16292", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.16292", "abs": "https://arxiv.org/abs/2508.16292", "authors": ["Wen-Han Hsieh", "Elvis Hsieh", "Dantong Niu", "Trevor Darrell", "Roei Herzig", "David M. Chan"], "title": "Do What? Teaching Vision-Language-Action Models to Reject the Impossible", "comment": "9 pages, 2 figures, 1 table", "summary": "Recently, Vision-Language-Action (VLA) models have demonstrated strong\nperformance on a range of robotic tasks. These models rely on multimodal\ninputs, with language instructions playing a crucial role -- not only in\npredicting actions, but also in robustly interpreting user intent, even when\nthe requests are impossible to fulfill. In this work, we investigate how VLAs\ncan recognize, interpret, and respond to false-premise instructions: natural\nlanguage commands that reference objects or conditions absent from the\nenvironment. We propose Instruct-Verify-and-Act (IVA), a unified framework that\n(i) detects when an instruction cannot be executed due to a false premise, (ii)\nengages in language-based clarification or correction, and (iii) grounds\nplausible alternatives in perception and action. Towards this end, we construct\na large-scale instruction tuning setup with structured language prompts and\ntrain a VLA model capable of handling both accurate and erroneous requests. Our\napproach leverages a contextually augmented, semi-synthetic dataset containing\npaired positive and false-premise instructions, enabling robust detection and\nnatural language correction. Our experiments show that IVA improves false\npremise detection accuracy by 97.56% over baselines, while increasing\nsuccessful responses in false-premise scenarios by 50.78%."}
{"id": "2508.16445", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.16445", "abs": "https://arxiv.org/abs/2508.16445", "authors": ["Sonia Nicoletti", "Paolo Ciancarini"], "title": "Using LLMs and Essence to Support Software Practice Adoption", "comment": null, "summary": "Recent advancements in natural language processing (NLP) have enabled the\ndevelopment of automated tools that support various domains, including software\nengineering. However, while NLP and artificial intelligence (AI) research has\nextensively focused on tasks such as code generation, less attention has been\ngiven to automating support for the adoption of best practices, the evolution\nof ways of working, and the monitoring of process health. This study addresses\nthis gap by exploring the integration of Essence, a standard and thinking\nframework for managing software engineering practices, with large language\nmodels (LLMs). To this end, a specialised chatbot was developed to assist\nstudents and professionals in understanding and applying Essence. The chatbot\nemploys a retrieval-augmented generation (RAG) system to retrieve relevant\ncontextual information from a curated knowledge base. Four different LLMs were\nused to create multiple chatbot configurations, each evaluated both as a base\nmodel and augmented with the RAG system. The system performance was evaluated\nthrough both the relevance of retrieved context and the quality of generated\nresponses. Comparative analysis against the general-purpose LLMs demonstrated\nthat the proposed system consistently outperforms its baseline counterpart in\ndomain-specific tasks. By facilitating access to structured software\nengineering knowledge, this work contributes to bridging the gap between\ntheoretical frameworks and practical application, potentially improving process\nmanagement and the adoption of software development practices. While further\nvalidation through user studies is required, these findings highlight the\npotential of LLM-based automation to enhance learning and decision-making in\nsoftware engineering."}
{"id": "2508.16406", "categories": ["cs.CR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.16406", "abs": "https://arxiv.org/abs/2508.16406", "authors": ["Guangyu Yang", "Jinghong Chen", "Jingbiao Mei", "Weizhe Lin", "Bill Byrne"], "title": "Retrieval-Augmented Defense: Adaptive and Controllable Jailbreak Prevention for Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) remain vulnerable to jailbreak attacks, which\nattempt to elicit harmful responses from LLMs. The evolving nature and\ndiversity of these attacks pose many challenges for defense systems, including\n(1) adaptation to counter emerging attack strategies without costly retraining,\nand (2) control of the trade-off between safety and utility. To address these\nchallenges, we propose Retrieval-Augmented Defense (RAD), a novel framework for\njailbreak detection that incorporates a database of known attack examples into\nRetrieval-Augmented Generation, which is used to infer the underlying,\nmalicious user query and jailbreak strategy used to attack the system. RAD\nenables training-free updates for newly discovered jailbreak strategies and\nprovides a mechanism to balance safety and utility. Experiments on StrongREJECT\nshow that RAD substantially reduces the effectiveness of strong jailbreak\nattacks such as PAP and PAIR while maintaining low rejection rates for benign\nqueries. We propose a novel evaluation scheme and show that RAD achieves a\nrobust safety-utility trade-off across a range of operating points in a\ncontrollable manner."}
{"id": "2508.16352", "categories": ["cs.AI", "eess.SP"], "pdf": "https://arxiv.org/pdf/2508.16352", "abs": "https://arxiv.org/abs/2508.16352", "authors": ["Nasir Khan", "Asmaa Abdallah", "Abdulkadir Celik", "Ahmed M. Eltawil", "Sinem Coleri"], "title": "Causal Beam Selection for Reliable Initial Access in AI-driven Beam Management", "comment": null, "summary": "Efficient and reliable beam alignment is a critical requirement for mmWave\nmultiple-input multiple-output (MIMO) systems, especially in 6G and beyond,\nwhere communication must be fast, adaptive, and resilient to real-world\nuncertainties. Existing deep learning (DL)-based beam alignment methods often\nneglect the underlying causal relationships between inputs and outputs, leading\nto limited interpretability, poor generalization, and unnecessary beam sweeping\noverhead. In this work, we propose a causally-aware DL framework that\nintegrates causal discovery into beam management pipeline. Particularly, we\npropose a novel two-stage causal beam selection algorithm to identify a minimal\nset of relevant inputs for beam prediction. First, causal discovery learns a\nBayesian graph capturing dependencies between received power inputs and the\noptimal beam. Then, this graph guides causal feature selection for the DL-based\nclassifier. Simulation results reveal that the proposed causal beam selection\nmatches the performance of conventional methods while drastically reducing\ninput selection time by 94.4% and beam sweeping overhead by 59.4% by focusing\nonly on causally relevant features."}
{"id": "2508.16499", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.16499", "abs": "https://arxiv.org/abs/2508.16499", "authors": ["Kazuki Kusama", "Honglin Shu", "Masanari Kondo", "Yasutaka Kamei"], "title": "How Small is Enough? Empirical Evidence of Quantized Small Language Models for Automated Program Repair", "comment": null, "summary": "Background: Large language models (LLMs) have greatly improved the accuracy\nof automated program repair (APR) methods. However, LLMs are constrained by\nhigh computational resource requirements. Aims: We focus on small language\nmodels (SLMs), which perform well even with limited computational resources\ncompared to LLMs. We aim to evaluate whether SLMs can achieve competitive\nperformance in APR tasks. Method: We conducted experiments on the QuixBugs\nbenchmark to compare the bug-fixing accuracy of SLMs and LLMs. We also analyzed\nthe impact of int8 quantization on APR performance. Results: The latest SLMs\ncan fix bugs as accurately as--or even more accurately than--LLMs. Also, int8\nquantization had minimal effect on APR accuracy while significantly reducing\nmemory requirements. Conclusions: SLMs present a viable alternative to LLMs for\nAPR, offering competitive accuracy with lower computational costs, and\nquantization can further enhance their efficiency without compromising\neffectiveness."}
{"id": "2508.16383", "categories": ["cs.AI", "cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2508.16383", "abs": "https://arxiv.org/abs/2508.16383", "authors": ["Xinyu Yang", "Chenlong Deng", "Zhicheng Dou"], "title": "GLARE: Agentic Reasoning for Legal Judgment Prediction", "comment": null, "summary": "Legal judgment prediction (LJP) has become increasingly important in the\nlegal field. In this paper, we identify that existing large language models\n(LLMs) have significant problems of insufficient reasoning due to a lack of\nlegal knowledge. Therefore, we introduce GLARE, an agentic legal reasoning\nframework that dynamically acquires key legal knowledge by invoking different\nmodules, thereby improving the breadth and depth of reasoning. Experiments\nconducted on the real-world dataset verify the effectiveness of our method.\nFurthermore, the reasoning chain generated during the analysis process can\nincrease interpretability and provide the possibility for practical\napplications."}
{"id": "2508.16517", "categories": ["cs.SE", "cs.PL"], "pdf": "https://arxiv.org/pdf/2508.16517", "abs": "https://arxiv.org/abs/2508.16517", "authors": ["Bingkun Yao", "Ning Wang", "Xiangfeng Liu", "Yuxin Du", "Yuchen Hu", "Hong Gao", "Zhe Jiang", "Nan Guan"], "title": "ARSP: Automated Repair of Verilog Designs via Semantic Partitioning", "comment": null, "summary": "Debugging functional Verilog bugs consumes a significant portion of front-end\ndesign time. While Large Language Models (LLMs) have demonstrated great\npotential in mitigating this effort, existing LLM-based automated debugging\nmethods underperform on industrial-scale modules. A major reason for this is\nbug signal dilution in long contexts, where a few bug-relevant tokens are\noverwhelmed by hundreds of unrelated lines, diffusing the model's attention. To\naddress this issue, we introduce ARSP, a two-stage system that mitigates\ndilution via semantics-guided fragmentation. A Partition LLM splits a module\ninto semantically tight fragments; a Repair LLM patches each fragment; edits\nare merged without altering unrelated logic. A synthetic data framework\ngenerates fragment-level training pairs spanning bug types, design styles, and\nscales to supervise both models. Experiments show that ARSP achieves 77.92%\npass@1 and 83.88% pass@5, outperforming mainstream commercial LLMs including\nClaude-3.7 and SOTA automated Verilog debugging tools Strider and MEIC. Also,\nsemantic partitioning improves pass@1 by 11.6% and pass@5 by 10.2% over\nwhole-module debugging, validating the effectiveness of fragment-level scope\nreduction in LLM-based Verilog debugging."}
{"id": "2508.16463", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.16463", "abs": "https://arxiv.org/abs/2508.16463", "authors": ["Aniello Panariello", "Emanuele Frascaroli", "Pietro Buzzega", "Lorenzo Bonicelli", "Angelo Porrello", "Simone Calderara"], "title": "Modular Embedding Recomposition for Incremental Learning", "comment": "Accepted to the 36th British Machine Vision Conference (BMVC 2025),\n  Sheffield, UK", "summary": "The advent of pre-trained Vision-Language Models (VLMs) has significantly\ntransformed Continual Learning (CL), mainly due to their zero-shot\nclassification abilities. Such proficiency makes VLMs well-suited for\nreal-world applications, enabling robust performance on novel unseen classes\nwithout requiring adaptation. However, fine-tuning remains essential when\ndownstream tasks deviate significantly from the pre-training domain. Prior CL\napproaches primarily focus on preserving the zero-shot capabilities of VLMs\nduring incremental fine-tuning on a downstream task. We take a step further by\ndevising an approach that transforms preservation into enhancement of the\nzero-shot capabilities of VLMs. Our approach, named MoDular Embedding\nRecomposition (MoDER), introduces a modular framework that trains multiple\ntextual experts, each specialized in a single seen class, and stores them in a\nfoundational hub. At inference time, for each unseen class, we query the hub\nand compose the retrieved experts to synthesize a refined prototype that\nimproves classification. We show the effectiveness of our method across two\npopular zero-shot incremental protocols, Class-IL and MTIL, comprising a total\nof 14 datasets. The codebase is available at\nhttps://github.com/aimagelab/mammoth."}
{"id": "2508.16524", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.16524", "abs": "https://arxiv.org/abs/2508.16524", "authors": ["Xuan Zhang", "Zhijian Zhou", "Weidi Xu", "Yanting Miao", "Chao Qu", "Yuan Qi"], "title": "Constraints-Guided Diffusion Reasoner for Neuro-Symbolic Learning", "comment": null, "summary": "Enabling neural networks to learn complex logical constraints and fulfill\nsymbolic reasoning is a critical challenge. Bridging this gap often requires\nguiding the neural network's output distribution to move closer to the symbolic\nconstraints. While diffusion models have shown remarkable generative capability\nacross various domains, we employ the powerful architecture to perform\nneuro-symbolic learning and solve logical puzzles. Our diffusion-based pipeline\nadopts a two-stage training strategy: the first stage focuses on cultivating\nbasic reasoning abilities, while the second emphasizes systematic learning of\nlogical constraints. To impose hard constraints on neural outputs in the second\nstage, we formulate the diffusion reasoner as a Markov decision process and\ninnovatively fine-tune it with an improved proximal policy optimization\nalgorithm. We utilize a rule-based reward signal derived from the logical\nconsistency of neural outputs and adopt a flexible strategy to optimize the\ndiffusion reasoner's policy. We evaluate our methodology on some classical\nsymbolic reasoning benchmarks, including Sudoku, Maze, pathfinding and\npreference learning. Experimental results demonstrate that our approach\nachieves outstanding accuracy and logical consistency among neural networks."}
{"id": "2508.16571", "categories": ["cs.AI", "cs.IR", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.16571", "abs": "https://arxiv.org/abs/2508.16571", "authors": ["Alisa Vinogradova", "Vlad Vinogradov", "Dmitrii Radkevich", "Ilya Yasny", "Dmitry Kobyzev", "Ivan Izmailov", "Katsiaryna Yanchanka", "Andrey Doronichev"], "title": "LLM-Based Agents for Competitive Landscape Mapping in Drug Asset Due Diligence", "comment": null, "summary": "In this paper, we describe and benchmark a competitor-discovery component\nused within an agentic AI system for fast drug asset due diligence. A\ncompetitor-discovery AI agent, given an indication, retrieves all drugs\ncomprising the competitive landscape of that indication and extracts canonical\nattributes for these drugs. The competitor definition is investor-specific, and\ndata is paywalled/licensed, fragmented across registries, ontology-mismatched\nby indication, alias-heavy for drug names, multimodal, and rapidly changing.\nAlthough considered the best tool for this problem, the current LLM-based AI\nsystems aren't capable of reliably retrieving all competing drug names, and\nthere is no accepted public benchmark for this task. To address the lack of\nevaluation, we use LLM-based agents to transform five years of multi-modal,\nunstructured diligence memos from a private biotech VC fund into a structured\nevaluation corpus mapping indications to competitor drugs with normalized\nattributes. We also introduce a competitor validating LLM-as-a-judge agent that\nfilters out false positives from the list of predicted competitors to maximize\nprecision and suppress hallucinations. On this benchmark, our\ncompetitor-discovery agent achieves 83% recall, exceeding OpenAI Deep Research\n(65%) and Perplexity Labs (60%). The system is deployed in production with\nenterprise users; in a case study with a biotech VC investment fund, analyst\nturnaround time dropped from 2.5 days to $\\sim$3 hours ($\\sim$20x) for the\ncompetitive analysis."}
{"id": "2508.15808", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15808", "abs": "https://arxiv.org/abs/2508.15808", "authors": ["Benjamin Murphy", "Twm Stone"], "title": "Uplifted Attackers, Human Defenders: The Cyber Offense-Defense Balance for Trailing-Edge Organizations", "comment": null, "summary": "Advances in AI are widely understood to have implications for cybersecurity.\nArticles have emphasized the effect of AI on the cyber offense-defense balance,\nand commentators can be found arguing either that cyber will privilege\nattackers or defenders. For defenders, arguments are often made that AI will\nenable solutions like formal verification of all software--and for some\nwell-equipped companies, this may be true. This conversation, however, does not\nmatch the reality for most companies. \"Trailing-edge organizations,\" as we term\nthem, rely heavily on legacy software, poorly staff security roles, and\nstruggle to implement best practices like rapid deployment of security patches.\nThese decisions may be the result of corporate inertia, but may also be the\nresult of a seemingly-rational calculation that attackers may not bother\ntargeting a firm due to lack of economic incentives, and as a result,\nunderinvestment in defense will not be punished.\n  This approach to security may have been sufficient prior to the development\nof AI systems, but it is unlikely to remain viable in the near future. We argue\nthat continuing improvements in AI's capabilities poses additional risks on two\nfronts: First, increased usage of AI will alter the economics of the marginal\ncyberattack and expose these trailing-edge organizations to more attackers,\nmore frequently. Second, AI's advances will enable attackers to develop\nexploits and launch attacks earlier than they can today--meaning that it is\ninsufficient for these companies to attain parity with today's leading\ndefenders, but must instead aim for faster remediation timelines and more\nresilient software. The situation today portends a dramatically increased\nnumber of attacks in the near future. Moving forward, we offer a range of\nsolutions for both organizations and governments to improve the defensive\nposture of firms which lag behind their peers today."}
{"id": "2508.15839", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15839", "abs": "https://arxiv.org/abs/2508.15839", "authors": ["Yuksel Aydin"], "title": "CIA+TA Risk Assessment for AI Reasoning Vulnerabilities", "comment": null, "summary": "As AI systems increasingly influence critical decisions, they face threats\nthat exploit reasoning mechanisms rather than technical infrastructure. We\npresent a framework for cognitive cybersecurity, a systematic protection of AI\nreasoning processes from adversarial manipulation. Our contributions are\nthreefold. First, we establish cognitive cybersecurity as a discipline\ncomplementing traditional cybersecurity and AI safety, addressing\nvulnerabilities where legitimate inputs corrupt reasoning while evading\nconventional controls. Second, we introduce the CIA+TA, extending traditional\nConfidentiality, Integrity, and Availability triad with Trust (epistemic\nvalidation) and Autonomy (human agency preservation), requirements unique to\nsystems generating knowledge claims and mediating decisions. Third, we present\na quantitative risk assessment methodology with empirically-derived\ncoefficients, enabling organizations to measure cognitive security risks. We\nmap our framework to OWASP LLM Top 10 and MITRE ATLAS, facilitating operational\nintegration. Validation through previously published studies (151 human\nparticipants; 12,180 AI trials) reveals strong architecture dependence:\nidentical defenses produce effects ranging from 96% reduction to 135%\namplification of vulnerabilities. This necessitates pre-deployment Cognitive\nPenetration Testing as a governance requirement for trustworthy AI deployment."}
{"id": "2508.15865", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15865", "abs": "https://arxiv.org/abs/2508.15865", "authors": ["Julia Boone", "Fatemeh Afghah"], "title": "Securing Swarms: Cross-Domain Adaptation for ROS2-based CPS Anomaly Detection", "comment": "Accepted for publication in MILCOM 2025. 6 pages, 2 figures", "summary": "Cyber-physical systems (CPS) are being increasingly utilized for critical\napplications. CPS combines sensing and computing elements, often having\nmulti-layer designs with networking, computational, and physical interfaces,\nwhich provide them with enhanced capabilities for a variety of application\nscenarios. However, the combination of physical and computational elements also\nmakes CPS more vulnerable to attacks compared to network-only systems, and the\nresulting impacts of CPS attacks can be substantial. Intelligent intrusion\ndetection systems (IDS) are an effective mechanism by which CPS can be secured,\nbut the majority of current solutions often train and validate on network\ntraffic-only datasets, ignoring the distinct attacks that may occur on other\nsystem layers. In order to address this, we develop an adaptable CPS anomaly\ndetection model that can detect attacks within CPS without the need for\npreviously labeled data. To achieve this, we utilize domain adaptation\ntechniques that allow us to transfer known attack knowledge from a network\ntraffic-only environment to a CPS environment. We validate our approach using a\nstate-of-the-art CPS intrusion dataset that combines network, operating system\n(OS), and Robot Operating System (ROS) data. Through this dataset, we are able\nto demonstrate the effectiveness of our model across network traffic-only and\nCPS environments with distinct attack types and its ability to outperform other\nanomaly detection methods."}
{"id": "2508.15934", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.15934", "abs": "https://arxiv.org/abs/2508.15934", "authors": ["Onur Alp Kirci", "M. Emre Gursoy"], "title": "Strategic Sample Selection for Improved Clean-Label Backdoor Attacks in Text Classification", "comment": null, "summary": "Backdoor attacks pose a significant threat to the integrity of text\nclassification models used in natural language processing. While several\ndirty-label attacks that achieve high attack success rates (ASR) have been\nproposed, clean-label attacks are inherently more difficult. In this paper, we\npropose three sample selection strategies to improve attack effectiveness in\nclean-label scenarios: Minimum, Above50, and Below50. Our strategies identify\nthose samples which the model predicts incorrectly or with low confidence, and\nby injecting backdoor triggers into such samples, we aim to induce a stronger\nassociation between the trigger patterns and the attacker-desired target label.\nWe apply our methods to clean-label variants of four canonical backdoor attacks\n(InsertSent, WordInj, StyleBkd, SynBkd) and evaluate them on three datasets\n(IMDB, SST2, HateSpeech) and four model types (LSTM, BERT, DistilBERT,\nRoBERTa). Results show that the proposed strategies, particularly the Minimum\nstrategy, significantly improve the ASR over random sample selection with\nlittle or no degradation in the model's clean accuracy. Furthermore,\nclean-label attacks enhanced by our strategies outperform BITE, a state of the\nart clean-label attack method, in many configurations."}
{"id": "2508.16025", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.16025", "abs": "https://arxiv.org/abs/2508.16025", "authors": ["Saba Naqvi", "Mohammad Baqar"], "title": "Breaking Barriers in Software Testing: The Power of AI-Driven Automation", "comment": "10 Pages", "summary": "Software testing remains critical for ensuring reliability, yet traditional\napproaches are slow, costly, and prone to gaps in coverage. This paper presents\nan AI-driven framework that automates test case generation and validation using\nnatural language processing (NLP), reinforcement learning (RL), and predictive\nmodels, embedded within a policy-driven trust and fairness model. The approach\ntranslates natural language requirements into executable tests, continuously\noptimizes them through learning, and validates outcomes with real-time analysis\nwhile mitigating bias. Case studies demonstrate measurable gains in defect\ndetection, reduced testing effort, and faster release cycles, showing that\nAI-enhanced testing improves both efficiency and reliability. By addressing\nintegration and scalability challenges, the framework illustrates how AI can\nshift testing from a reactive, manual process to a proactive, adaptive system\nthat strengthens software quality in increasingly complex environments."}
{"id": "2508.16071", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.16071", "abs": "https://arxiv.org/abs/2508.16071", "authors": ["Mahinthan Chandramohan", "Jovan Jancic", "Yuntong Zhang", "Padmanabhan Krishnan"], "title": "From Benchmark Data To Applicable Program Repair: An Experience Report", "comment": null, "summary": "This paper describes our approach to automated program repair. We combine\nvarious techniques from the literature to achieve this. Our experiments show\nthat our approach performs better than other techniques on standard benchmarks.\nHowever, on closer inspection, none of these techniques work on realistic\ndefects that we see in industry.\n  We find that augmenting code with formal specifications enables LLMs to\ngenerate higher-quality unit tests, especially for complex production code with\nimproved coverage of edge cases and exception handling. However, specifications\nadd little value for well-understood errors (e.g., null pointer, index out of\nbounds), but are beneficial for logic and string manipulation errors. Despite\nencouraging benchmark results, real-world adoption is limited since passing\ntests do not guarantee correct patches. Current challenges include insufficient\nexpressiveness of the JML specification language, necessitating advanced\nverification tools and richer predicates. Our ongoing work is exploring\ncontract automata, programming by example, and testcase repair, with a focus on\nintegrating human feedback and measuring productivity gains - highlighting the\ngap between academic benchmarks and practical industry needs"}
{"id": "2508.16131", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.16131", "abs": "https://arxiv.org/abs/2508.16131", "authors": ["Zoe Kotti", "Konstantina Dritsa", "Diomidis Spinellis", "Panos Louridas"], "title": "The Fools are Certain; the Wise are Doubtful: Exploring LLM Confidence in Code Completion", "comment": "30 pages, 10 figures", "summary": "Code completion entails the task of providing missing tokens given a\nsurrounding context. It can boost developer productivity while providing a\npowerful code discovery tool. Following the Large Language Model (LLM) wave,\ncode completion has been approached with diverse LLMs fine-tuned on code (code\nLLMs). The performance of code LLMs can be assessed with downstream and\nintrinsic metrics. Downstream metrics are usually employed to evaluate the\npractical utility of a model, but can be unreliable and require complex\ncalculations and domain-specific knowledge. In contrast, intrinsic metrics such\nas perplexity, entropy, and mutual information, which measure model confidence\nor uncertainty, are simple, versatile, and universal across LLMs and tasks, and\ncan serve as proxies for functional correctness and hallucination risk in\nLLM-generated code. Motivated by this, we evaluate the confidence of LLMs when\ngenerating code by measuring code perplexity across programming languages,\nmodels, and datasets using various LLMs, and a sample of 1008 files from 657\nGitHub projects. We find that strongly-typed languages exhibit lower perplexity\nthan dynamically typed languages. Scripting languages also demonstrate higher\nperplexity. Perl appears universally high in perplexity, whereas Java appears\nlow. Code perplexity depends on the employed LLM, but not on the code dataset.\nAlthough code comments often increase perplexity, the language ranking based on\nperplexity is barely affected by their presence. LLM researchers, developers,\nand users can employ our findings to assess the benefits and suitability of\nLLM-based code completion in specific software projects based on how language,\nmodel choice, and code characteristics impact model confidence."}
{"id": "2508.16165", "categories": ["cs.SE", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.16165", "abs": "https://arxiv.org/abs/2508.16165", "authors": ["Sebastian Lubos", "Alexander Felfernig", "Gerhard Leitner", "Julian Schwazer"], "title": "Towards Recommending Usability Improvements with Multimodal Large Language Models", "comment": null, "summary": "Usability describes a set of essential quality attributes of user interfaces\n(UI) that influence human-computer interaction. Common evaluation methods, such\nas usability testing and inspection, are effective but resource-intensive and\nrequire expert involvement. This makes them less accessible for smaller\norganizations. Recent advances in multimodal LLMs offer promising opportunities\nto automate usability evaluation processes partly by analyzing textual, visual,\nand structural aspects of software interfaces. To investigate this possibility,\nwe formulate usability evaluation as a recommendation task, where multimodal\nLLMs rank usability issues by severity. We conducted an initial\nproof-of-concept study to compare LLM-generated usability improvement\nrecommendations with usability expert assessments. Our findings indicate the\npotential of LLMs to enable faster and more cost-effective usability\nevaluation, which makes it a practical alternative in contexts with limited\nexpert resources."}
{"id": "2508.16181", "categories": ["cs.SE", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.16181", "abs": "https://arxiv.org/abs/2508.16181", "authors": ["Zirui Li", "Stephan Husung", "Haoze Wang"], "title": "LLM-Assisted Semantic Alignment and Integration in Collaborative Model-Based Systems Engineering Using SysML v2", "comment": "Accepted by IEEE ISSE 2025, DOI pending", "summary": "Cross-organizational collaboration in Model-Based Systems Engineering (MBSE)\nfaces many challenges in achieving semantic alignment across independently\ndeveloped system models. SysML v2 introduces enhanced structural modularity and\nformal semantics, offering a stronger foundation for interoperable modeling.\nMeanwhile, GPT-based Large Language Models (LLMs) provide new capabilities for\nassisting model understanding and integration. This paper proposes a\nstructured, prompt-driven approach for LLM-assisted semantic alignment of SysML\nv2 models. The core contribution lies in the iterative development of an\nalignment approach and interaction prompts, incorporating model extraction,\nsemantic matching, and verification. The approach leverages SysML v2 constructs\nsuch as alias, import, and metadata extensions to support traceable, soft\nalignment integration. It is demonstrated with a GPT-based LLM through an\nexample of a measurement system. Benefits and limitations are discussed."}
{"id": "2508.16189", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.16189", "abs": "https://arxiv.org/abs/2508.16189", "authors": ["Aparna Singh", "Geetanjali Rathee", "Chaker Abdelaziz Kerrache", "Mohamed Chahine Ghanem"], "title": "A Relay-Chain-Powered Ciphertext-Policy Attribute-Based Encryption in Intelligent Transportation Systems", "comment": null, "summary": "The very high growth of Intelligent Transportation Systems (ITS) has\ngenerated an urgent requirement for secure, effective, and context-aware data\nsharing mechanisms, especially over heterogeneous and geographically dispersed\nsettings. This work suggests a new architecture that combines a relay\nchain-driven encryption system with a modified Ciphertext-Policy\nAttribute-Based Encryption (CP-ABE) scheme to tackle the double impediment of\ndynamic access and low-latency communication. The model proposes a\ncontext-aware smart contract on a worldwide relay chain that checks against\ndata properties, including event type, time, and geographical region, to\nspecify the suitable level of encryption policy. From such relay-directed\njudgment, On-Board Units (OBUs) encrypt data end-to-end by utilising CP-ABE and\nstore ciphertext inside localised regional blockchains, preventing dependence\non symmetric encryption or off-chain storage. High-sensitivity events are\nsecured with firm, multi-attribute access rules, whereas common updates use\nlight policies to help reduce processing burdens. The crypto system also adds\ntraceability and low-latency revocation, with global enforcement managed\nthrough the relay chain. This distributed, scalable model provides a proper\nbalance between responsiveness in real time and security and is extremely apt\nfor next-gen vehicular networks that function across multi-jurisdictional\ndomains."}
{"id": "2508.16347", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.16347", "abs": "https://arxiv.org/abs/2508.16347", "authors": ["Yu Yan", "Sheng Sun", "Zhe Wang", "Yijun Lin", "Zenghao Duan", "zhifei zheng", "Min Liu", "Zhiyi yin", "Jianping Zhang"], "title": "Confusion is the Final Barrier: Rethinking Jailbreak Evaluation and Investigating the Real Misuse Threat of LLMs", "comment": null, "summary": "With the development of Large Language Models (LLMs), numerous efforts have\nrevealed their vulnerabilities to jailbreak attacks. Although these studies\nhave driven the progress in LLMs' safety alignment, it remains unclear whether\nLLMs have internalized authentic knowledge to deal with real-world crimes, or\nare merely forced to simulate toxic language patterns. This ambiguity raises\nconcerns that jailbreak success is often attributable to a hallucination loop\nbetween jailbroken LLM and judger LLM. By decoupling the use of jailbreak\ntechniques, we construct knowledge-intensive Q\\&A to investigate the misuse\nthreats of LLMs in terms of dangerous knowledge possession, harmful task\nplanning utility, and harmfulness judgment robustness. Experiments reveal a\nmismatch between jailbreak success rates and harmful knowledge possession in\nLLMs, and existing LLM-as-a-judge frameworks tend to anchor harmfulness\njudgments on toxic language patterns. Our study reveals a gap between existing\nLLM safety assessments and real-world threat potential."}
