<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 16]
- [cs.CR](#cs.CR) [Total: 17]
- [cs.AI](#cs.AI) [Total: 25]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Is Research Software Science a Metascience?](https://arxiv.org/abs/2509.13436)
*Evan Eisinger,Michael A. Heroux*

Main category: cs.SE

TL;DR: 本文探讨研究软件科学(RSS)是否应归类为元科学，通过比较两者的定义、原则和目标，分析RSS在提高研究软件质量和科学可靠性方面的作用。


<details>
  <summary>Details</summary>
Motivation: 随着研究日益依赖计算方法，科学结果的可靠性取决于研究软件的质量、可重现性和透明度。确保这些品质对科学诚信和发现至关重要。

Method: 定义元科学和研究软件科学(RSS)，比较它们的原理和目标，检查它们的重叠部分，分析支持与反对将RSS归类为元科学的论点。

Result: 分析发现RSS推进了元科学的核心目标，特别是在计算可重现性方面，并连接了研究的技术、社会和认知方面。其分类取决于采用广义还是狭义的元科学定义。

Conclusion: RSS最好被理解为一个独特的跨学科领域，与元科学保持一致，在某些定义下属于元科学范畴。无论分类如何，将科学严谨性应用于研究软件可确保发现工具符合发现本身的标准。

Abstract: As research increasingly relies on computational methods, the reliability of
scientific results depends on the quality, reproducibility, and transparency of
research software. Ensuring these qualities is critical for scientific
integrity and discovery. This paper asks whether Research Software Science
(RSS)--the empirical study of how research software is developed and
used--should be considered a form of metascience, the science of science.
Classification matters because it could affect recognition, funding, and
integration of RSS into research improvement. We define metascience and RSS,
compare their principles and objectives, and examine their overlaps. Arguments
for classification highlight shared commitments to reproducibility,
transparency, and empirical study of research processes. Arguments against
portraying RSS as a specialized domain focused on a tool rather than the
broader scientific enterprise. Our analysis finds RSS advances core goals of
metascience, especially in computational reproducibility, and bridges
technical, social, and cognitive aspects of research. Its classification
depends on whether one adopts a broad definition of metascience--any empirical
effort to improve science--or a narrow one focused on systemic and
epistemological structures. We argue RSS is best understood as a distinct
interdisciplinary domain that aligns with, and in some definitions fits within,
metascience. Recognizing it as such can strengthen its role in improving
reliability, justify funding, and elevate software development in research
institutions. Regardless of classification, applying scientific rigor to
research software ensures the tools of discovery meet the standards of the
discoveries themselves.

</details>


### [2] [An LLM Agentic Approach for Legal-Critical Software: A Case Study for Tax Prep Software](https://arxiv.org/abs/2509.13471)
*Sina Gogani-Khiabani,Ashutosh Trivedi,Diptikalyan Saha,Saeid Tizpaz-Niari*

Main category: cs.SE

TL;DR: 使用多段机器人框架将美国联邦税法转换为可执行代码，通过高阶变形测试自动生成测试案例，在复杂税法任务上较大型模型更可靠


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在法律关键场景中存在的模糊性和幻觉问题，提高法律关键软件的可靠性

Method: 采用多段机器人框架，通过高阶变形关系进行测试案例生成和代码合成，包含变形测试段机器人搜索反例

Result: 使用较小模型(GPT-4o-mini)达到45%最差情况通过率，超过前沿模型(GPT-4o和Claude 3.5的9-15%)通过率

Conclusion: 多段机器人LLM方法是开发稳健、可信赖法律关键软件的有效途径

Abstract: Large language models (LLMs) show promise for translating natural-language
statutes into executable logic, but reliability in legally critical settings
remains challenging due to ambiguity and hallucinations. We present an agentic
approach for developing legal-critical software, using U.S. federal tax
preparation as a case study. The key challenge is test-case generation under
the oracle problem, where correct outputs require interpreting law. Building on
metamorphic testing, we introduce higher-order metamorphic relations that
compare system outputs across structured shifts among similar individuals.
Because authoring such relations is tedious and error-prone, we use an
LLM-driven, role-based framework to automate test generation and code
synthesis. We implement a multi-agent system that translates tax code into
executable software and incorporates a metamorphic-testing agent that searches
for counterexamples. In experiments, our framework using a smaller model
(GPT-4o-mini) achieves a worst-case pass rate of 45%, outperforming frontier
models (GPT-4o and Claude 3.5, 9-15%) on complex tax-code tasks. These results
support agentic LLM methodologies as a path to robust, trustworthy
legal-critical software from natural-language specifications.

</details>


### [3] [Prompt2DAG: A Modular Methodology for LLM-Based Data Enrichment Pipeline Generation](https://arxiv.org/abs/2509.13487)
*Abubakari Alidu,Michele Ciavotta,Flavio DePaoli*

Main category: cs.SE

TL;DR: Prompt2DAG是一个将自然语言描述转换为可执行Apache Airflow DAG的方法论，通过混合方法实现了78.5%的成功率，显著优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 开发可靠的数据丰富管道需要大量工程专业知识，希望通过自动化方法降低数据管道开发的门槛。

Method: 评估了四种生成方法（直接、纯LLM、混合和基于模板），使用13个LLM和5个案例研究进行260次实验，采用惩罚评分框架评估可靠性、代码质量、结构完整性和可执行性。

Result: 混合方法表现最佳，成功率78.5%，质量评分优异（SAT:6.79, DST:7.67, PCT:7.76），成本效益是直接提示法的两倍以上。

Conclusion: 结构化混合方法对于平衡自动化工作流生成的灵活性和可靠性至关重要，为数据管道开发的民主化提供了可行路径。

Abstract: Developing reliable data enrichment pipelines demands significant engineering
expertise. We present Prompt2DAG, a methodology that transforms natural
language descriptions into executable Apache Airflow DAGs. We evaluate four
generation approaches -- Direct, LLM-only, Hybrid, and Template-based -- across
260 experiments using thirteen LLMs and five case studies to identify optimal
strategies for production-grade automation. Performance is measured using a
penalized scoring framework that combines reliability with code quality (SAT),
structural integrity (DST), and executability (PCT). The Hybrid approach
emerges as the optimal generative method, achieving a 78.5% success rate with
robust quality scores (SAT: 6.79, DST: 7.67, PCT: 7.76). This significantly
outperforms the LLM-only (66.2% success) and Direct (29.2% success) methods.
Our findings show that reliability, not intrinsic code quality, is the primary
differentiator. Cost-effectiveness analysis reveals the Hybrid method is over
twice as efficient as Direct prompting per successful DAG. We conclude that a
structured, hybrid approach is essential for balancing flexibility and
reliability in automated workflow generation, offering a viable path to
democratize data pipeline development.

</details>


### [4] [Crash Report Enhancement with Large Language Models: An Empirical Study](https://arxiv.org/abs/2509.13535)
*S M Farah Al Fahim,Md Nakhla Rafi,Zeyang Ma,Dong Jae Kim,Tse-Hsun,Chen*

Main category: cs.SE

TL;DR: 大语言模型能夠显著提升崩溃报告的调试效果，通过添加故障位置、根因解释和修复建议，使得问题定位准确率从10.6%提升到40.2-43.1%


<details>
  <summary>Details</summary>
Motivation: 崩溃报告缺乏详细的诊断信息，影响开发者调试效率，需要提升其调试价值

Method: 研究两种LLM增强策略：Direct-LLM（单次输入堆栈跟踪）和Agentic-LLM（迭代查找仓库证据），在492个真实崩溃报告上进行实验

Result: 增强后报告显著提高了问题定位准确率，产生的修复建议与开发者补丁相似度高（CodeBLEU约56-57%），Agentic-LLM在根因解释和修复指导方面更优

Conclusion: 为LLM提供堆栈跟踪和仓库代码可以生成更有用的崩溃报告，大幅提升调试效果，尤其在修复指导方面改善最为显著

Abstract: Crash reports are central to software maintenance, yet many lack the
diagnostic detail developers need to debug efficiently. We examine whether
large language models can enhance crash reports by adding fault locations,
root-cause explanations, and repair suggestions. We study two enhancement
strategies: Direct-LLM, a single-shot approach that uses stack-trace context,
and Agentic-LLM, an iterative approach that explores the repository for
additional evidence. On a dataset of 492 real-world crash reports, LLM-enhanced
reports improve Top-1 problem-localization accuracy from 10.6% (original
reports) to 40.2-43.1%, and produce suggested fixes that closely resemble
developer patches (CodeBLEU around 56-57%). Both our manual evaluations and
LLM-as-a-judge assessment show that Agentic-LLM delivers stronger root-cause
explanations and more actionable repair guidance. A user study with 16
participants further confirms that enhanced reports make crashes easier to
understand and resolve, with the largest improvement in repair guidance. These
results indicate that supplying LLMs with stack traces and repository code
yields enhanced crash reports that are substantially more useful for debugging.

</details>


### [5] [GitHub's Copilot Code Review: Can AI Spot Security Flaws Before You Commit?](https://arxiv.org/abs/2509.13650)
*Amena Amro,Manar H. Alalfi*

Main category: cs.SE

TL;DR: GitHub Copilot代码审查功能在检测安全漏洞方面效果不佳，主要关注低严重性问题而非关键安全漏洞


<details>
  <summary>Details</summary>
Motivation: 随着软件开发越来越多采用AI工具，需要评估这些工具在支持安全编码方面的有效性，特别是GitHub Copilot新推出的代码审查功能

Method: 使用来自多个编程语言和应用领域的开源项目中精选的标记漏洞代码样本，系统评估Copilot检测常见安全漏洞的能力

Result: Copilot代码审查经常无法检测SQL注入、XSS和不安全反序列化等关键漏洞，反馈主要针对编码风格和拼写错误等低严重性问题

Conclusion: AI辅助代码审查的实际效果与预期能力存在显著差距，仍需专用安全工具和人工代码审计来确保软件安全

Abstract: As software development practices increasingly adopt AI-powered tools,
ensuring that such tools can support secure coding has become critical. This
study evaluates the effectiveness of GitHub Copilot's recently introduced code
review feature in detecting security vulnerabilities. Using a curated set of
labeled vulnerable code samples drawn from diverse open-source projects
spanning multiple programming languages and application domains, we
systematically assessed Copilot's ability to identify and provide feedback on
common security flaws. Contrary to expectations, our results reveal that
Copilot's code review frequently fails to detect critical vulnerabilities such
as SQL injection, cross-site scripting (XSS), and insecure deserialization.
Instead, its feedback primarily addresses low-severity issues, such as coding
style and typographical errors. These findings expose a significant gap between
the perceived capabilities of AI-assisted code review and its actual
effectiveness in supporting secure development practices. Our results highlight
the continued necessity of dedicated security tools and manual code audits to
ensure robust software security.

</details>


### [6] [A Regression Testing Framework with Automated Assertion Generation for Machine Learning Notebooks](https://arxiv.org/abs/2509.13656)
*Yingao Elaine Yao,Vedant Nimje,Varun Viswanath,Saikat Dutta*

Main category: cs.SE

TL;DR: NBTest是第一个针对机器学习笔记本的回归测试框架，支持单元级断言编写和在CI中运行，通过自动生成断言和统计技术提高ML笔记本的可靠性


<details>
  <summary>Details</summary>
Motivation: 笔记本在机器学习开发中广泛使用但缺乏测试支持，导致许多不易察觉的bug和性能回归问题难以发现

Method: 开发NBTest框架，提供断言API库和JupyterLab插件，支持单元级断言编写和自动生成ML关键组件的断言，使用统计技术减少非确定性计算带来的波动性

Result: 在592个Kaggle笔记本上生成21163个断言（平均35.75个/笔记本），变异得分0.57，用户研究显示直观性评分4.3/5，实用性评分4.24/5

Conclusion: NBTest能有效提高ML笔记本的可靠性和可维护性，已被主流ML库的CI采用，解决了笔记本测试支持不足的问题

Abstract: Notebooks have become the de-facto choice for data scientists and machine
learning engineers for prototyping and experimenting with machine learning (ML)
pipelines. Notebooks provide an interactive interface for code, data, and
visualization. However, notebooks provide very limited support for testing.
Thus, during continuous development, many subtle bugs that do not lead to
crashes often go unnoticed and cause silent errors that manifest as performance
regressions.
  To address this, we introduce NBTest - the first regression testing framework
that allows developers to write cell-level assertions in notebooks and run such
notebooks in pytest or in continuous integration (CI) pipelines. NBTest offers
a library of assertion APIs, and a JupyterLab plugin that enables executing
assertions. We also develop the first automated approach for generating
cell-level assertions for key components in ML notebooks, such as data
processing, model building, and model evaluation. NBTest aims to improve the
reliability and maintainability of ML notebooks without adding developer
burden.
  We evaluate NBTest on 592 Kaggle notebooks. Overall, NBTest generates 21163
assertions (35.75 on average per notebook). The generated assertions obtain a
mutation score of 0.57 in killing ML-specific mutations. NBTest can catch
regression bugs in previous versions of the Kaggle notebooks using assertions
generated for the latest versions. Because ML pipelines involve non
deterministic computations, the assertions can be flaky. Hence, we also show
how NBTest leverages statistical techniques to minimize flakiness while
retaining high fault-detection effectiveness. NBTest has been adopted in the CI
of a popular ML library. Further, we perform a user study with 17 participants
that shows that notebook users find NBTest intuitive (Rating 4.3/5) and useful
in writing assertions and testing notebooks (Rating 4.24/5).

</details>


### [7] [Prompt Stability in Code LLMs: Measuring Sensitivity across Emotion- and Personality-Driven Variations](https://arxiv.org/abs/2509.13680)
*Wei Ma,Yixiao Yang,Jingquan Ge,Xiaofei Xie,Lingxiao Jiang*

Main category: cs.SE

TL;DR: PromptSE框架用于评估代码生成模型对提示词表达的敏感性，通过创建语义相同但情感和风格不同的提示变体，发现性能与稳定性是解耦的优化目标


<details>
  <summary>Details</summary>
Motivation: 代码生成模型对提示词表达的敏感性未被充分研究，相同需求用不同情感或沟通风格表达会产生不同输出，而现有基准主要关注峰值性能

Method: 创建语义等效的提示变体（含情感和个性模板），使用概率感知连续评分或二进制通过率评估稳定性，提出AUC-E指标进行跨模型比较

Result: 在14个模型（Llama、Qwen、DeepSeek）上的研究表明，性能和稳定性是解耦的优化目标，揭示了挑战模型鲁棒性常见假设的架构和规模相关模式

Conclusion: PromptSE能够量化性能稳定性权衡，支持闭源模型快速筛选和详细稳定性分析，将提示稳定性定位为与性能和公平性互补的评估维度，有助于构建更可信的AI辅助软件开发工具

Abstract: Code generation models are widely used in software development, yet their
sensitivity to prompt phrasing remains under-examined. Identical requirements
expressed with different emotions or communication styles can yield divergent
outputs, while most benchmarks emphasize only peak performance. We present
PromptSE (Prompt Sensitivity Evaluation), a framework that creates semantically
equivalent prompt variants with emotion and personality templates, and that
evaluates stability using probability aware continuous scoring or using binary
pass rates when logits are unavailable. The results are aggregated into a
proposed area under curve metric (AUC-E) for cross model comparison. Across 14
models from three families (Llama, Qwen, and DeepSeek), our study shows that
performance and stability behave as largely decoupled optimization objectives,
and it reveals architectural and scale related patterns that challenge common
assumptions about model robustness. The framework supports rapid screening for
closed-source models as well as detailed stability analysis in research
settings. PromptSE enables practitioners to quantify performance stability
trade offs for deployment and model selection, positioning prompt stability as
a complementary evaluation dimension alongside performance and fairness, and
contributing to more trustworthy AI-assisted software development tools.

</details>


### [8] [Scrub It Out! Erasing Sensitive Memorization in Code Language Models via Machine Unlearning](https://arxiv.org/abs/2509.13755)
*Zhaoyang Chu,Yao Wan,Zhikun Zhang,Di Wang,Zhou Yang,Hongyu Zhang,Pan Zhou,Xuanhua Shi,Hai Jin,David Lo*

Main category: cs.SE

TL;DR: 本文研究了代码语言模型(CLM)中敏感信息记忆的问题，提出了CodeEraser方法通过机器遗忘技术有效擦除敏感记忆，同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 代码语言模型存在意外记忆敏感训练数据的安全漏洞，现有解决方案需要全模型重新训练，计算成本高昂。研究目标是找到高效擦除CLM中敏感记忆的方法。

Method: 首先量化CLM训练数据中的记忆风险，构建5万个高风险敏感记忆样本作为遗忘目标。研究基于梯度上升的遗忘方法，提出CodeEraser变体，选择性地遗忘代码中的敏感记忆片段，同时保持代码结构完整性和功能正确性。

Result: 在CodeParrot、CodeGen-Mono和Qwen2.5-Coder三个CLM家族上的广泛实验验证了CodeEraser在擦除目标敏感记忆方面的有效性和效率，同时保持了模型效用。

Conclusion: CodeEraser通过机器遗忘技术能够有效且高效地擦除代码语言模型中的敏感记忆，为解决CLM隐私漏洞提供了实用的后处理解决方案，避免了全模型重新训练的高成本。

Abstract: While Code Language Models (CLMs) have demonstrated superior performance in
software engineering tasks such as code generation and summarization, recent
empirical studies reveal a critical privacy vulnerability: these models exhibit
unintended memorization of sensitive training data, enabling verbatim
reproduction of confidential information when specifically prompted. To address
this issue, several approaches, including training data de-duplication and
differential privacy augmentation, have been proposed. However, these methods
require full-model retraining for deployed CLMs, which incurs substantial
computational costs. In this paper, we aim to answer the following research
question: Can sensitive information memorized by CLMs be erased effectively and
efficiently?
  We conduct a pioneering investigation into erasing sensitive memorization in
CLMs through machine unlearning - a post-hoc modification method that removes
specific information from trained models without requiring full retraining.
Specifically, we first quantify the memorization risks of sensitive data within
CLM training datasets and curate a high-risk dataset of 50,000 sensitive
memorized samples as unlearning targets. We study two widely used gradient
ascent-based unlearning approaches: the vanilla and constraint-based methods,
and introduce CodeEraser, an advanced variant that selectively unlearns
sensitive memorized segments in code while preserving the structural integrity
and functional correctness of the surrounding code. Extensive experiments on
three families of CLMs, i.e., CodeParrot, CodeGen-Mono, and Qwen2.5-Coder,
validate the effectiveness and efficiency of CodeEraser in erasing targeted
sensitive memorization while maintaining model utility.

</details>


### [9] [A Study on Thinking Patterns of Large Reasoning Models in Code Generation](https://arxiv.org/abs/2509.13758)
*Kevin Halim,Sin G. Teo,Ruitao Feng,Zhenpeng Chen,Yang Gu,Chong Wang,Yang Liu*

Main category: cs.SE

TL;DR: 本文系统分析了大型推理模型(LRMs)在代码生成中的推理行为模式，通过人工标注构建了包含15种推理行为的分类法，揭示了不同模型的推理特点及其与代码正确性的关系，并提出了基于推理的提示策略改进方法。


<details>
  <summary>Details</summary>
Motivation: 尽管大型推理模型(LRMs)在代码生成方面表现出强大的多步推理能力，但缺乏对其推理行为模式的系统性分析，以及这些模式如何影响生成代码质量的研究。

Method: 使用代码生成任务提示多个先进LRMs，通过开放式编码手动标注推理轨迹，构建包含15种推理行为的分类法，涵盖四个阶段，并进行实证研究分析推理模式。

Result: 发现LRMs遵循类似人类的编码工作流，复杂任务引发额外推理行为；不同模型推理风格差异明显(Qwen3迭代式，DeepSeek-R1-7B线性式)；单元测试和脚手架生成等行为与代码正确性强相关；基于推理的提示策略能有效改进代码质量。

Conclusion: 研究揭示了LRMs的推理行为模式及其对代码生成的影响，为改进自动代码生成提供了实践指导，展示了上下文和推理导向提示策略的潜力。

Abstract: Currently, many large language models (LLMs) are utilized for software
engineering tasks such as code generation. The emergence of more advanced
models known as large reasoning models (LRMs), such as OpenAI's o3, DeepSeek
R1, and Qwen3. They have demonstrated the capability of performing multi-step
reasoning. Despite the advancement in LRMs, little attention has been paid to
systematically analyzing the reasoning patterns these models exhibit and how
such patterns influence the generated code. This paper presents a comprehensive
study aimed at investigating and uncovering the reasoning behavior of LRMs
during code generation. We prompted several state-of-the-art LRMs of varying
sizes with code generation tasks and applied open coding to manually annotate
the reasoning traces. From this analysis, we derive a taxonomy of LRM reasoning
behaviors, encompassing 15 reasoning actions across four phases.
  Our empirical study based on the taxonomy reveals a series of findings.
First, we identify common reasoning patterns, showing that LRMs generally
follow a human-like coding workflow, with more complex tasks eliciting
additional actions such as scaffolding, flaw detection, and style checks.
Second, we compare reasoning across models, finding that Qwen3 exhibits
iterative reasoning while DeepSeek-R1-7B follows a more linear, waterfall-like
approach. Third, we analyze the relationship between reasoning and code
correctness, showing that actions such as unit test creation and scaffold
generation strongly support functional outcomes, with LRMs adapting strategies
based on task context. Finally, we evaluate lightweight prompting strategies
informed by these findings, demonstrating the potential of context- and
reasoning-oriented prompts to improve LRM-generated code. Our results offer
insights and practical implications for advancing automatic code generation.

</details>


### [10] [Who is Introducing the Failure? Automatically Attributing Failures of Multi-Agent Systems via Spectrum Analysis](https://arxiv.org/abs/2509.13782)
*Yu Ge,Linna Xie,Zhong Li,Yu Pei,Tian Zhang*

Main category: cs.SE

TL;DR: FAMAS是首个基于频谱的多智能体系统故障归因方法，通过轨迹重放和频谱分析来识别导致故障的具体智能体行为


<details>
  <summary>Details</summary>
Motivation: 多智能体系统在复杂任务中应用广泛但存在故障，目前故障归因方法研究不足且人工成本高，难以进行系统调试和改进

Method: 提出FAMAS方法，通过系统性的轨迹重放和抽象，然后进行频谱分析，估计每个智能体行为导致故障的可能性。设计了专门针对MAS的怀疑度公式，整合智能体行为组和动作行为组两个关键因素

Result: 在Who and When基准测试中与12个基线方法进行比较，FAMAS表现出优越性能，超越了所有对比方法

Conclusion: FAMAS为多智能体系统提供了一种有效的故障归因解决方案，能够准确定位导致故障的具体智能体行为，有助于系统调试和改进

Abstract: Large Language Model Powered Multi-Agent Systems (MASs) are increasingly
employed to automate complex real-world problems, such as programming and
scientific discovery. Despite their promising, MASs are not without their
flaws. However, failure attribution in MASs - pinpointing the specific agent
actions responsible for failures - remains underexplored and labor-intensive,
posing significant challenges for debugging and system improvement. To bridge
this gap, we propose FAMAS, the first spectrum-based failure attribution
approach for MASs, which operates through systematic trajectory replay and
abstraction, followed by spectrum analysis.The core idea of FAMAS is to
estimate, from variations across repeated MAS executions, the likelihood that
each agent action is responsible for the failure. In particular, we propose a
novel suspiciousness formula tailored to MASs, which integrates two key factor
groups, namely the agent behavior group and the action behavior group, to
account for the agent activation patterns and the action activation patterns
within the execution trajectories of MASs. Through expensive evaluations
against 12 baselines on the Who and When benchmark, FAMAS demonstrates superior
performance by outperforming all the methods in comparison.

</details>


### [11] [Trace Sampling 2.0: Code Knowledge Enhanced Span-level Sampling for Distributed Tracing](https://arxiv.org/abs/2509.13852)
*Yulun Wu,Guangba Yu,Zhihan Jiang,Yichen Li,Michael R. Lyu*

Main category: cs.SE

TL;DR: Trace Sampling 2.0通过span级别采样在保持trace结构一致性的同时，显著减少存储开销。Autoscope实现该方法，减少81.2% trace大小，同时保持98.1%故障span覆盖率。


<details>
  <summary>Details</summary>
Motivation: 分布式追踪在微服务系统中是重要的诊断工具，但海量trace数据给后端存储带来巨大负担。传统trace采样方法通常会丢弃有价值的信息，包括用于对比分析的正样本trace。

Method: 提出Trace Sampling 2.0，在span级别进行采样同时保持trace结构一致性。设计实现Autoscope，利用静态分析提取执行逻辑，确保关键span被保留而不损害结构完整性。

Result: 在两个开源微服务上评估，结果显示：trace大小减少81.2%，同时保持98.1%故障span覆盖率，优于现有trace级别采样方法。在根因分析中平均提升8.3%效果。

Conclusion: Autoscope能显著提升微服务中的可观测性和存储效率，为性能监控提供强大解决方案。

Abstract: Distributed tracing is an essential diagnostic tool in microservice systems,
but the sheer volume of traces places a significant burden on backend storage.
A common approach to mitigating this issue is trace sampling, which selectively
retains traces based on specific criteria, often preserving only anomalous
ones. However, this method frequently discards valuable information, including
normal traces that are essential for comparative analysis. To address this
limitation, we introduce Trace Sampling 2.0, which operates at the span level
while maintaining trace structure consistency. This approach allows for the
retention of all traces while significantly reducing storage overhead. Based on
this concept, we design and implement Autoscope, a span-level sampling method
that leverages static analysis to extract execution logic, ensuring that
critical spans are preserved without compromising structural integrity. We
evaluated Autoscope on two open-source microservices. Our results show that it
reduces trace size by 81.2% while maintaining 98.1% faulty span coverage,
outperforming existing trace-level sampling methods. Furthermore, we
demonstrate its effectiveness in root cause analysis, achieving an average
improvement of 8.3%. These findings indicate that Autoscope can significantly
enhance observability and storage efficiency in microservices, offering a
robust solution for performance monitoring.

</details>


### [12] [Are Prompts All You Need? Evaluating Prompt-Based Large Language Models (LLM)s for Software Requirements Classification](https://arxiv.org/abs/2509.13868)
*Manal Binkhonain,Reem Alfayaz*

Main category: cs.SE

TL;DR: 基于提示的大语言模型可以在需求分类任务中减少对大量标签数据的依赖，通过几种提示方式实现了与细调转换器基线相当或更优的性能


<details>
  <summary>Details</summary>
Motivation: 现有的监督学习模型需要大量豆子标签数据，成本高、创建慢、领域依赖性强且渐迁性差，需要针对每个任务重新训练

Method: 在PROMISE和SecReq两个英语数据集上对比多个模型和提示风格（零械击、少械击、人设和思维链），并与强力的细调转换器基线进行比较

Result: 提示基础的大语言模型，特别是使用少械击时，可以达到或超过基线模型的性能。添加人设或人设加思维链还能带来进一步的性能提升

Conclusion: 基于提示的大语言模型是一种实用且可扩展的选择，能够减少对大量注释数据的依赖，并提高在不同任务间的渐迁能力

Abstract: Requirements classification assigns natural language requirements to
predefined classes, such as functional and non functional. Accurate
classification reduces risk and improves software quality. Most existing models
rely on supervised learning, which needs large labeled data that are costly,
slow to create, and domain dependent; they also generalize poorly and often
require retraining for each task. This study tests whether prompt based large
language models can reduce data needs. We benchmark several models and
prompting styles (zero shot, few shot, persona, and chain of thought) across
multiple tasks on two English datasets, PROMISE and SecReq. For each task we
compare model prompt configurations and then compare the best LLM setups with a
strong fine tuned transformer baseline. Results show that prompt based LLMs,
especially with few shot prompts, can match or exceed the baseline. Adding a
persona, or persona plus chain of thought, can yield further gains. We conclude
that prompt based LLMs are a practical and scalable option that reduces
dependence on large annotations and can improve generalizability across tasks.

</details>


### [13] [Mind the Ethics! The Overlooked Ethical Dimensions of GenAI in Software Modeling Education](https://arxiv.org/abs/2509.13896)
*Shalini Chakraborty,Lola Burgueño,Nathalie Moreno,Javier Troya,Paula Muñoz*

Main category: cs.SE

TL;DR: 本文通过系统文献综述发现，生成式AI在软件建模教育中的伦理考量研究严重缺失，仅3篇论文明确讨论相关伦理问题，凸显了该领域急需结构化伦理框架。


<details>
  <summary>Details</summary>
Motivation: 生成式AI在软件建模教育中迅速普及，但缺乏明确的伦理监督和教学指导，其伦理影响尚未得到充分探索，需要系统性研究来识别和解决相关伦理问题。

Method: 对计算机科学六大数字图书馆（ACM、IEEE、Scopus、ScienceDirect、SpringerLink、Web of Science）进行系统文献综述，筛选讨论生成式AI在软件建模教育中伦理方面的研究。

Result: 从1386篇独特论文中仅发现3篇明确讨论伦理考量，包括责任、公平、透明、多样性和包容性等问题，表明该领域伦理讨论严重缺乏。

Conclusion: 生成式AI在建模教育中的伦理话语严重缺失，迫切需要建立结构化伦理框架，并提出了该新兴教育领域的研究机遇和挑战。

Abstract: Generative Artificial Intelligence (GenAI) is rapidly gaining momentum in
software modeling education, embraced by both students and educators. As GenAI
assists with interpreting requirements, formalizing models, and translating
students' mental models into structured notations, it increasingly shapes core
learning outcomes such as domain comprehension, diagrammatic thinking, and
modeling fluency without clear ethical oversight or pedagogical guidelines.
Yet, the ethical implications of this integration remain underexplored.
  In this paper, we conduct a systematic literature review across six major
digital libraries in computer science (ACM Digital Library, IEEE Xplore,
Scopus, ScienceDirect, SpringerLink, and Web of Science). Our aim is to
identify studies discussing the ethical aspects of GenAI in software modeling
education, including responsibility, fairness, transparency, diversity, and
inclusion among others.
  Out of 1,386 unique papers initially retrieved, only three explicitly
addressed ethical considerations. This scarcity highlights the critical absence
of ethical discourse surrounding GenAI in modeling education and raises urgent
questions about the responsible integration of AI in modeling curricula, as
well as it evinces the pressing need for structured ethical frameworks in this
emerging educational landscape. We examine these three studies and explore the
emerging research opportunities as well as the challenges that have arisen in
this field.

</details>


### [14] [An Empirical Study on Failures in Automated Issue Solving](https://arxiv.org/abs/2509.13941)
*Simiao Liu,Fang Liu,Liehao Li,Xin Tan,Yinghao Zhu,Xiaoli Lian,Li Zhang*

Main category: cs.SE

TL;DR: 这篇论文通过系统性分析SWE-Bench自动修复任务的失败模式，提出了一个包含3个主要阶段、9个主类和25个子类的失败分类系统，并设计了专家-执行者协作框架来改善模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前自动修复系统在SWE-Bench上仍有很高失败率，但联系性能指标隐藏了失败根本原因，影响了模型弊疲弊诊断和有针对性改进。

Method: 首先分析了三种状态下的代理工具在SWE-Bench-Verified中的表现，然后通过手动分析150个失败案例构建了详细的失败模式分类系统，最后设计了专家-执行者协作框架来解决关键问题。

Result: 实验结果显示，新框架能够解决22.2%之前单个代理无法处理的问题，为构建更稳健的自动修复系统提供了方向。

Conclusion: 通过详细的失败模式分析和协作式框架设计，这项研究为提升自动代码修复系统的性能和可靠性提供了有价值的见解和方法。

Abstract: Automated issue solving seeks to autonomously identify and repair defective
code snippets across an entire codebase. SWE-Bench has emerged as the most
widely adopted benchmark for evaluating progress in this area. While LLM-based
agentic tools show great promise, they still fail on a substantial portion of
tasks. Moreover, current evaluations primarily report aggregate issue-solving
rates, which obscure the underlying causes of success and failure, making it
challenging to diagnose model weaknesses or guide targeted improvements. To
bridge this gap, we first analyze the performance and efficiency of three SOTA
tools, spanning both pipeline-based and agentic architectures, in automated
issue solving tasks of SWE-Bench-Verified under varying task characteristics.
Furthermore, to move from high-level performance metrics to underlying cause
analysis, we conducted a systematic manual analysis of 150 failed instances.
From this analysis, we developed a comprehensive taxonomy of failure modes
comprising 3 primary phases, 9 main categories, and 25 fine-grained
subcategories. Then we systematically analyze the distribution of the
identified failure modes, the results reveal distinct failure fingerprints
between the two architectural paradigms, with the majority of agentic failures
stemming from flawed reasoning and cognitive deadlocks. Motivated by these
insights, we propose a collaborative Expert-Executor framework. It introduces a
supervisory Expert agent tasked with providing strategic oversight and
course-correction for a primary Executor agent. This architecture is designed
to correct flawed reasoning and break the cognitive deadlocks that frequently
lead to failure. Experiments show that our framework solves 22.2% of previously
intractable issues for a leading single agent. These findings pave the way for
building more robust agents through diagnostic evaluation and collaborative
design.

</details>


### [15] [Evaluating Classical Software Process Models as Coordination Mechanisms for LLM-Based Software Generation](https://arxiv.org/abs/2509.13942)
*Duc Minh Ha,Phu Trac Kien,Tho Quan,Anh Nguyen-Duc*

Main category: cs.SE

TL;DR: 研究调查了传统软件开发模型（水涩、V模型、灵活）在LLM多代理系统中的应用效果，发现不同模型在代码质量、成本和效率方面各有优势


<details>
  <summary>Details</summary>
Motivation: 利用传统软件开发过程的结构化协调模式来指导LLM多代理系统的自主协作，提高代码质量和生产力

Method: 执行11个多样化软件项目，测试三种过程模型和四种GPT变体，共112次运行。使用标准化指标评估代码规模、成本和质量

Result: 过程模型和LLM选择都显著影响系统性能。水涩模型效率最高，V模型产生最冗长代码，灵活模型代码质量最高但计算成本更高

Conclusion: 传统软件过程可有效应用于LLM多代理系统，但各有特点。选择应根据项目目标考虑效率、稳健性或验证需求

Abstract: [Background] Large Language Model (LLM)-based multi-agent systems (MAS) are
transforming software development by enabling autonomous collaboration.
Classical software processes such asWaterfall, V-Model, and Agile offer
structured coordination patterns that can be repurposed to guide these agent
interactions. [Aims] This study explores how traditional software development
processes can be adapted as coordination scaffolds for LLM based MAS and
examines their impact on code quality, cost, and productivity. [Method] We
executed 11 diverse software projects under three process models and four GPT
variants, totaling 132 runs. Each output was evaluated using standardized
metrics for size (files, LOC), cost (execution time, token usage), and quality
(code smells, AI- and human detected bugs). [Results] Both process model and
LLM choice significantly affected system performance. Waterfall was most
efficient, V-Model produced the most verbose code, and Agile achieved the
highest code quality, albeit at higher computational cost. [Conclusions]
Classical software processes can be effectively instantiated in LLM-based MAS,
but each entails trade-offs across quality, cost, and adaptability. Process
selection should reflect project goals, whether prioritizing efficiency,
robustness, or structured validation.

</details>


### [16] [Reasoning Efficiently Through Adaptive Chain-of-Thought Compression: A Self-Optimizing Framework](https://arxiv.org/abs/2509.14093)
*Kerui Huang,Shuhan Liu,Xing Hu,Tongtong Xu,Lingfeng Bao,Xin Xia*

Main category: cs.SE

TL;DR: 思维链推理虽然提升LLM性能但带来高昂计算成本，研究发现过长推理反而有害。SEER框架通过自适应压缩思维链，在保持准确性的同时显著降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 思维链推理虽然能提高大语言模型在算术、逻辑和常识任务中的准确性和鲁棒性，但会导致计算成本急剧增加（延迟、内存使用和KV缓存需求），特别是在需要简洁确定性输出的软件工程任务中。需要研究这种权衡并找到优化方法。

Method: 提出SEER（自增强高效推理）框架，结合Best-of-N采样和任务感知自适应过滤，通过预推理输出动态调整阈值来压缩思维链，减少冗余和计算开销。

Result: 在三个软件工程任务和一个数学任务上的评估显示，SEER平均缩短思维链42.1%，通过减少截断提高准确性，并消除了大多数无限循环。计算延迟降低最多5倍。

Conclusion: SEER是一种实用方法，能在资源受限条件下使思维链增强的LLM更加高效和鲁棒，挑战了"推理越长越好"的假设，证明了自适应思维链控制的必要性。

Abstract: Chain-of-Thought (CoT) reasoning enhances Large Language Models (LLMs) by
prompting intermediate steps, improving accuracy and robustness in arithmetic,
logic, and commonsense tasks. However, this benefit comes with high
computational costs: longer outputs increase latency, memory usage, and
KV-cache demands. These issues are especially critical in software engineering
tasks where concise and deterministic outputs are required. To investigate
these trade-offs, we conduct an empirical study based on code generation
benchmarks. The results reveal that longer CoT does not always help. Excessive
reasoning often causes truncation, accuracy drops, and latency up to five times
higher, with failed outputs consistently longer than successful ones. These
findings challenge the assumption that longer reasoning is inherently better
and highlight the need for adaptive CoT control. Motivated by this, we propose
SEER (Self-Enhancing Efficient Reasoning), an adaptive framework that
compresses CoT while preserving accuracy. SEER combines Best-of-N sampling with
task-aware adaptive filtering, dynamically adjusting thresholds based on
pre-inference outputs to reduce verbosity and computational overhead. We then
evaluate SEER on three software engineering tasks and one math task. On
average, SEER shortens CoT by 42.1%, improves accuracy by reducing truncation,
and eliminates most infinite loops. These results demonstrate SEER as a
practical method to make CoT-enhanced LLMs more efficient and robust, even
under resource constraints.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [17] [LIGHT-HIDS: A Lightweight and Effective Machine Learning-Based Framework for Robust Host Intrusion Detection](https://arxiv.org/abs/2509.13464)
*Onat Gungor,Ishaan Kale,Jiasheng Zhou,Tajana Rosing*

Main category: cs.CR

TL;DR: LIGHT-HIDS是一个轻量级机器学习框架，通过压缩神经网络特征提取器和高效新颖性检测模型，在边缘计算环境中实现实时主机入侵检测，推理时间减少75倍。


<details>
  <summary>Details</summary>
Motivation: 边缘计算扩展增加了攻击面，需要兼顾准确性和效率的实时ML入侵检测系统，现有方案计算成本高，限制了实际部署。

Method: 结合压缩神经网络特征提取器（DeepSVDD训练）和高效新颖性检测模型，学习正常系统调用行为的紧凑表示进行异常检测。

Result: 在多个数据集上实验表明，LIGHT-HIDS在提高检测精度的同时，推理时间比最先进方法减少高达75倍。

Conclusion: 该框架作为基于机器学习的实时主机入侵检测解决方案，具有有效性和可扩展性。

Abstract: The expansion of edge computing has increased the attack surface, creating an
urgent need for robust, real-time machine learning (ML)-based host intrusion
detection systems (HIDS) that balance accuracy and efficiency. In such
settings, inference latency poses a critical security risk, as delays may
provide exploitable opportunities for attackers. However, many state-of-the-art
ML-based HIDS solutions rely on computationally intensive architectures with
high inference costs, limiting their practical deployment. This paper proposes
LIGHT-HIDS, a lightweight machine learning framework that combines a compressed
neural network feature extractor trained via Deep Support Vector Data
Description (DeepSVDD) with an efficient novelty detection model. This hybrid
approach enables the learning of compact, meaningful representations of normal
system call behavior for accurate anomaly detection. Experimental results on
multiple datasets demonstrate that LIGHT-HIDS consistently enhances detection
accuracy while reducing inference time by up to 75x compared to
state-of-the-art methods. These findings highlight its effectiveness and
scalability as a machine learning-based solution for real-time host intrusion
detection.

</details>


### [18] [Practitioners' Perspectives on a Differential Privacy Deployment Registry](https://arxiv.org/abs/2509.13509)
*Priyanka Nanayakkara,Elena Ghazi,Salil Vadhan*

Main category: cs.CR

TL;DR: 这篇论文提出了一个差分隐私（DP）部署注册表的整体架构和交互界面，包含21个实际部署案例，并通过用户研究验证了其价值和挑战。


<details>
  <summary>Details</summary>
Motivation: 为了促进差分隐私技术的共享学习和责任制，实现Dwork等人提出的公开DP部署注册表的想法。

Method: 开发了一个整体的层次结构来描述DP部署，设计并实现交互式注册表界面，填充了21个真实部署案例，并进行了逐案研究（n=16）。

Result: 参与者对注册表表示激情，认为其可以成为评估历史部署和支持未来部署的价值资源，同时识别了相关挑战。

Conclusion: 注册表有望成为DP社区的中心渠，但需要解决公开实施选择的劳动强度和风险等挑战，建议推动更广泛的采用。

Abstract: Differential privacy (DP) -- a principled approach to producing statistical
data products with strong, mathematically provable privacy guarantees for the
individuals in the underlying dataset -- has seen substantial adoption in
practice over the past decade. Applying DP requires making several
implementation decisions, each with significant impacts on data privacy and/or
utility. Hence, to promote shared learning and accountability around DP
deployments, Dwork, Kohli, and Mulligan (2019) proposed a public-facing
repository ("registry") of DP deployments. The DP community has recently
started to work toward realizing this vision. We contribute to this effort by
(1) developing a holistic, hierarchical schema to describe any given DP
deployment and (2) designing and implementing an interactive interface to act
as a registry where practitioners can access information about past DP
deployments. We (3) populate our interface with 21 real-world DP deployments
and (4) conduct an exploratory user study with DP practitioners ($n=16$) to
understand how they would use the registry, as well as what challenges and
opportunities they foresee around its adoption. We find that participants were
enthusiastic about the registry as a valuable resource for evaluating prior
deployments and making future deployments. They also identified several
opportunities for the registry, including that it can become a "hub" for the
community and support broader communication around DP (e.g., to legal teams).
At the same time, they identified challenges around the registry gaining
adoption, including the effort and risk involved with making implementation
choices public and moderating the quality of entries. Based on our findings, we
offer recommendations for encouraging adoption and increasing the registry's
value not only to DP practitioners, but also to policymakers, data users, and
data subjects.

</details>


### [19] [AQUA-LLM: Evaluating Accuracy, Quantization, and Adversarial Robustness Trade-offs in LLMs for Cybersecurity Question Answering](https://arxiv.org/abs/2509.13514)
*Onat Gungor,Roshan Sood,Harold Wang,Tajana Rosing*

Main category: cs.CR

TL;DR: AQUA-LLM框架评估小型LLM在网络安全问答中的量化、微调及其组合效果，发现量化+微调组合能最佳平衡准确性、鲁棒性和效率


<details>
  <summary>Details</summary>
Motivation: LLM在网络安全问答中表现优异但计算需求大，量化可压缩模型但会降低准确性和鲁棒性，微调的效果与量化结合尚未充分探索

Method: 提出AQUA-LLM评估框架，对多个先进小型LLM在四种配置（基础、仅量化、仅微调、量化+微调）下进行网络安全问答基准测试

Result: 仅量化准确性最低且鲁棒性差；量化+微调组合能同时提升LLM鲁棒性和预测性能，达到最佳平衡

Conclusion: 需要开发量化感知、保持鲁棒性的微调方法，以实现LLM在网络安全问答中的稳健高效部署

Abstract: Large Language Models (LLMs) have recently demonstrated strong potential for
cybersecurity question answering (QA), supporting decision-making in real-time
threat detection and response workflows. However, their substantial
computational demands pose significant challenges for deployment on
resource-constrained edge devices. Quantization, a widely adopted model
compression technique, can alleviate these constraints. Nevertheless,
quantization may degrade model accuracy and increase susceptibility to
adversarial attacks. Fine-tuning offers a potential means to mitigate these
limitations, but its effectiveness when combined with quantization remains
insufficiently explored. Hence, it is essential to understand the trade-offs
among accuracy, efficiency, and robustness. We propose AQUA-LLM, an evaluation
framework designed to benchmark several state-of-the-art small LLMs under four
distinct configurations: base, quantized-only, fine-tuned, and fine-tuned
combined with quantization, specifically for cybersecurity QA. Our results
demonstrate that quantization alone yields the lowest accuracy and robustness
despite improving efficiency. In contrast, combining quantization with
fine-tuning enhances both LLM robustness and predictive performance, achieving
an optimal balance of accuracy, robustness, and efficiency. These findings
highlight the critical need for quantization-aware, robustness-preserving
fine-tuning methodologies to enable the robust and efficient deployment of LLMs
for cybersecurity QA.

</details>


### [20] [GuardianPWA: Enhancing Security Throughout the Progressive Web App Installation Lifecycle](https://arxiv.org/abs/2509.13561)
*Mengxiao Wang,Guofei Gu*

Main category: cs.CR

TL;DR: GUARDIANPWA框架基于CIA安全原则分析PWA安装机制，发现203个安全违规实例，帮助开发者和浏览器厂商提升PWA安装安全性。


<details>
  <summary>Details</summary>
Motivation: 确保PWA安装生命周期的安全性对于维护用户信任和隐私至关重要，但浏览器厂商在遵守CIA安全原则方面存在不足。

Method: 提出GUARDIANPWA框架，基于机密性、完整性和可用性(CIA)安全原则系统分析PWA安装机制，检测浏览器厂商的合规性问题。

Result: 发现203个安全原则违规实例，包括Firefox隐私模式问题和Samsung Internet来源显示问题；向浏览器厂商报告后，Firefox确认4个问题并修复1个。

Conclusion: GUARDIANPWA框架能有效识别PWA安装安全漏洞，帮助开发者和浏览器厂商提升合规性，保护用户隐私和安全。

Abstract: Progressive Web App (PWA) installation is critical for integrating web and
mobile app functionalities, offering a seamless user experience. However,
ensuring the security of the PWA installation lifecycle is essential for
maintaining user trust and privacy. This paper introduces the GUARDIANPWA
framework, a comprehensive approach to analyzing the PWA installation mechanism
based on the CIA security principles (Confidentiality, Integrity, and
Availability) and identifying areas where browser vendors fail to comply with
these principles. Our study revealed 203 instances of non-compliance with
security principles, highlighting how these irregularities in the PWA
installation lifecycle can lead to potential violations of user privacy. For
instance, in Firefox, PWAs installed in private mode incorrectly appear in
normal mode, risking user confidentiality. Additionally, 29,465 PWAs are at
risk because Samsung Internet does not display origins when PWAs navigate to
third-party websites, undermining integrity. These findings were reported to
browser vendors, leading to Firefox acknowledging four issues, resolving one,
and planning to resolve two others. GUARDIANPWA supports developers by
analyzing PWA manifest files for syntactic and semantic correctness, offering
actionable recommendations, and helping to create PWAs that align with security
best practices. By using GUARDIANPWA, developers and users can address critical
security gaps and enhance compliance with CIA principles throughout the PWA
installation lifecycle.

</details>


### [21] [Demystifying Progressive Web Application Permission Systems](https://arxiv.org/abs/2509.13563)
*Mengxiao Wang,Guofei Gu*

Main category: cs.CR

TL;DR: 该文章研究了渐进式网络应用（PWA）的权限管理问题，发现了跨平台权限实施的不一致性、不完整性和漏洞，并通过Permissioner工具识别多种攻击风险。


<details>
  <summary>Details</summary>
Motivation: PWA虽然给予了系统级功能，但其权限管理在不同平台和浏览器上存在不一致和模糊的问题，需要系统性研究。

Method: 开发了Permissioner跨平台分析工具，对PWA权限进行了系统性研究，分析了权限执行的不一致性和漏洞。

Result: 发现了权限泄漏、设备识别和Permission API滥用等多种攻击风险，与浏览器厂商合作解决了部分问题。

Conclusion: 强调了PWA需要统一、健壮的权限模型，并提供了实现这一目标的可操作指南。

Abstract: Progressive Web Applications (PWAs) blend the advantages of web and native
apps, offering features like offline access, push notifications, and
installability. Beyond these, modern PWAs are increasingly granted system-level
capabilities such as auto-start on login and shared context with native
applications. However, their permission management remains poorly defined and
inconsistently implemented across platforms and browsers.
  To investigate these gaps, we developed Permissioner, a cross-platform
analysis tool, and conducted a systematic study of PWA permissions. Our
analysis uncovered critical issues of inconsistency, incompleteness, and
unclear boundaries in permission enforcement, leading to various attacks
including permission leakage, device identification, and Permission API abuse.
We further examined why some browsers resist adopting more granular permission
controls, identifying trade-offs involving usability, compatibility, and
platform limitations. Through collaboration with browser vendors, several
issues reported in our findings were acknowledged and resolved, notably by
Firefox and Chrome. Our work highlights the urgent need for a unified, robust
permission model for PWAs and provides actionable guidance toward achieving
this goal.

</details>


### [22] [Invisible Ears at Your Fingertips: Acoustic Eavesdropping via Mouse Sensors](https://arxiv.org/abs/2509.13581)
*Mohamad Fakih,Rahul Dharmaji,Youssef Mahmoud,Halima Bouzidi,Mohammad Abdullah Al Faruque*

Main category: cs.CR

TL;DR: Mic-E-Mouse是一种新型侧信道攻击，利用高性能光学鼠标传感器通过表面振动窃听音频信号，无需系统权限即可获取原始鼠标数据。


<details>
  <summary>Details</summary>
Motivation: 现代光学鼠标传感器具有高精度和高响应性，但存在被利用进行侧信道攻击的潜在漏洞，目前这方面研究较少。

Method: 采用端到端数据过滤管道，结合维纳滤波、重采样校正和创新的编码器-频谱图神经滤波技术，处理非均匀采样和非线性频率响应等问题。

Result: 在受控环境中，语音重建的信噪比提升高达+19dB，在AudioMNIST和VCTK数据集上的语音识别准确率达到42%至61%。

Conclusion: 光学鼠标传感器确实存在严重的侧信道安全风险，Mic-E-Mouse攻击方法有效证明了这一漏洞的实用性，需要引起安全社区的重视。

Abstract: Modern optical mouse sensors, with their advanced precision and high
responsiveness, possess an often overlooked vulnerability: they can be
exploited for side-channel attacks. This paper introduces Mic-E-Mouse, the
first-ever side-channel attack that targets high-performance optical mouse
sensors to covertly eavesdrop on users. We demonstrate that audio signals can
induce subtle surface vibrations detectable by a mouse's optical sensor.
Remarkably, user-space software on popular operating systems can collect and
broadcast this sensitive side channel, granting attackers access to raw mouse
data without requiring direct system-level permissions. Initially, the
vibration signals extracted from mouse data are of poor quality due to
non-uniform sampling, a non-linear frequency response, and significant
quantization. To overcome these limitations, Mic-E-Mouse employs a
sophisticated end-to-end data filtering pipeline that combines Wiener
filtering, resampling corrections, and an innovative encoder-only spectrogram
neural filtering technique. We evaluate the attack's efficacy across diverse
conditions, including speaking volume, mouse polling rate and DPI, surface
materials, speaker languages, and environmental noise. In controlled
environments, Mic-E-Mouse improves the signal-to-noise ratio (SNR) by up to +19
dB for speech reconstruction. Furthermore, our results demonstrate a speech
recognition accuracy of roughly 42% to 61% on the AudioMNIST and VCTK datasets.
All our code and datasets are publicly accessible on
https://sites.google.com/view/mic-e-mouse.

</details>


### [23] [Agentic JWT: A Secure Delegation Protocol for Autonomous AI Agents](https://arxiv.org/abs/2509.13597)
*Abhishek Goswami*

Main category: cs.CR

TL;DR: 提出Agentic JWT(A-JWT)方案，通过双面意图令牌绑定自治LLM代理行为与用户意图，解决OAuth 2.0在随机理性代理环境中的权限扩张风险


<details>
  <summary>Details</summary>
Motivation: 自治LLM代理可能通过随机理性、提示注入或多代理协同默默扩张权限，OAuth 2.0假设客户端是确定性的，但在代理环境中存在安全风险

Method: 设计A-JWT双面意图令牌，包含代理身份哈希校验和链式授权断言，使用轻量级客户端库进行运行时自我验证和令牌签发

Result: 实现了Python原型，能够阻塞范围违规请求、重放攻击、冒充攻击和提示注入漏洞，在商业硬件上仅产生毫秒级性能开销

Conclusion: A-JWT为自治代理应用提供了一种无缝集成的零信任保障方案，适配当前OAuth代理讨论，具有强大的安全保护能力

Abstract: Autonomous LLM agents can issue thousands of API calls per hour without human
oversight. OAuth 2.0 assumes deterministic clients, but in agentic settings
stochastic reasoning, prompt injection, or multi-agent orchestration can
silently expand privileges.
  We introduce Agentic JWT (A-JWT), a dual-faceted intent token that binds each
agent's action to verifiable user intent and, optionally, to a specific
workflow step. A-JWT carries an agent's identity as a one-way checksum hash
derived from its prompt, tools and configuration, and a chained delegation
assertion to prove which downstream agent may execute a given task, and
per-agent proof-of-possession keys to prevent replay and in-process
impersonation. We define a new authorization mechanism and add a lightweight
client shim library that self-verifies code at run time, mints intent tokens,
tracks workflow steps and derives keys, thus enabling secure agent identity and
separation even within a single process.
  We illustrate a comprehensive threat model for agentic applications,
implement a Python proof-of-concept and show functional blocking of
scope-violating requests, replay, impersonation, and prompt-injection pathways
with sub-millisecond overhead on commodity hardware. The design aligns with
ongoing OAuth agent discussions and offers a drop-in path toward zero-trust
guarantees for agentic applications. A comprehensive performance and security
evaluation with experimental results will appear in our forthcoming journal
publication

</details>


### [24] [Secure, Scalable and Privacy Aware Data Strategy in Cloud](https://arxiv.org/abs/2509.13627)
*Vijay Kumar Butte,Sujata Butte*

Main category: cs.CR

TL;DR: 本文提出了一种有效的企业云端数据策略，解决大数据处理、安全存储和及时决策的挑战


<details>
  <summary>Details</summary>
Motivation: 企业面临处理和存储大量数据的挑战，需要在保障安全性、可扩展性和隐私性的前提下，支持决策者进行及时、数据驱动的决策

Method: 讨论有效数据策略的各个组成部分，提供了解决安全性、可扩展性和隐私性问题的架构方案

Result: 开发出了一种完整的企业云端数据策略框架

Conclusion: 通过设计合理的数据策略和架构，企业可以有效应对大数据处理的挑战，实现安全、可扩展且保护隐私的数据管理

Abstract: The enterprises today are faced with the tough challenge of processing,
storing large amounts of data in a secure, scalable manner and enabling
decision makers to make quick, informed data driven decisions. This paper
addresses this challenge and develops an effective enterprise data strategy in
the cloud. Various components of an effective data strategy are discussed and
architectures addressing security, scalability and privacy aspects are
provided.

</details>


### [25] [Publicly Verifiable Private Information Retrieval Protocols Based on Function Secret Sharing](https://arxiv.org/abs/2509.13684)
*Lin Zhu,Lingwei Kong,Xin Ning,Xiaoyang Qu,Jianzong Wang*

Main category: cs.CR

TL;DR: 这篇论文提出了两种公开可验证的多服务器私有信息检索(PVPIR)方案，解决了PIR协议中的结果可验证性问题，在保持查询隐私性和正确性的同时实现低通信成本和良好的可扩展性。


<details>
  <summary>Details</summary>
Motivation: 私有信息检索(PIR)协议虽然能够保护查询隐私，但缺乏对检索结果的可验证性支持，用户需要确保获取的数据是可信过的和真实的。

Method: 设计了两种公开可验证PVPIR构造方案，并基于此提出三个具体实现。对于点查询，协议引入最小计算开销，对于谓词查询，通信复杂度保持稳定。

Result: 点查询协议在保证强验证性的同时，通信成本显著低于现有的Merkle树方案。谓词查询协议的通信复杂度不随数据库规模增大而增长，显示出良好的可扩展性。

Conclusion: 该研究提供了高效的公开可验证PIR方案，能够在多服务器环境下同时实现查询隐私性、正确性和可验证性，适用于大规模私有查询应用。

Abstract: Private Information Retrieval (PIR) is a fundamental cryptographic primitive
that enables users to retrieve data from a database without revealing which
item is being accessed, thereby preserving query privacy. However, PIR
protocols also face the challenge of result verifiability, as users expect the
reconstructed data to be trustworthy and authentic. In this work, we propose
two effective constructions of publicly verifiable PIR (PVPIR) in the
multi-server setting, which achieve query privacy, correctness, and
verifiability simultaneously. We further present three concrete instantiations
based on these constructions. For the point query, our protocol introduces
minimal computational overhead and achieves strong verifiability guarantees
with significantly lower communication costs compared to existing Merkle
tree-based approaches. For the predicate query, the communication complexity of
our scheme remains stable as the database size increases, demonstrating strong
scalability and suitability for large-scale private query applications.

</details>


### [26] [Protocol-Aware Firmware Rehosting for Effective Fuzzing of Embedded Network Stacks](https://arxiv.org/abs/2509.13740)
*Moritz Bley,Tobias Scharnowski,Simon Wörner,Moritz Schloegel,Thorsten Holz*

Main category: cs.CR

TL;DR: Pemu是一个自动检测和处理固件中网络协议使用的新方法，通过自动推断可用网络协议并透明生成有效网络数据包，使模糊测试输入能够直接流入固件逻辑的更深层。


<details>
  <summary>Details</summary>
Motivation: 嵌入式系统的网络接口是最大的攻击面之一，但现有重托管方法难以有效探索复杂的多层网络协议栈，限制了发现深层软件故障的能力。

Method: Pemu自动推断固件中可用的网络协议，透明生成封装模糊测试数据的有效网络数据包，实现逐层深入的固件组件分析。

Result: Pemu持续提高了三种现有嵌入式网络栈重托管工具的代码覆盖率，重新发现了多个已知漏洞并识别了五个先前未知的软件故障。

Conclusion: 该方法能够有效发现网络暴露代码中的深层嵌套错误，为嵌入式系统网络栈安全测试提供了更深入、更有针对性的分析能力。

Abstract: One of the biggest attack surfaces of embedded systems is their network
interfaces, which enable communication with other devices. Unlike their
general-purpose counterparts, embedded systems are designed for specialized use
cases, resulting in unique and diverse communication stacks. Unfortunately,
current approaches for evaluating the security of these embedded network stacks
require manual effort or access to hardware, and they generally focus only on
small parts of the embedded system. A promising alternative is firmware
rehosting, which enables fuzz testing of the entire firmware by generically
emulating the physical hardware. However, existing rehosting methods often
struggle to meaningfully explore network stacks due to their complex,
multi-layered input formats. This limits their ability to uncover deeply nested
software faults.
  To address this problem, we introduce a novel method to automatically detect
and handle the use of network protocols in firmware called Pemu. By
automatically deducing the available network protocols, Pemu can transparently
generate valid network packets that encapsulate fuzzing data, allowing the
fuzzing input to flow directly into deeper layers of the firmware logic. Our
approach thus enables a deeper, more targeted, and layer-by-layer analysis of
firmware components that were previously difficult or impossible to test. Our
evaluation demonstrates that Pemu consistently improves the code coverage of
three existing rehosting tools for embedded network stacks. Furthermore, our
fuzzer rediscovered several known vulnerabilities and identified five
previously unknown software faults, highlighting its effectiveness in
uncovering deeply nested bugs in network-exposed code.

</details>


### [27] [Who Taught the Lie? Responsibility Attribution for Poisoned Knowledge in Retrieval-Augmented Generation](https://arxiv.org/abs/2509.13772)
*Baolei Zhang,Haoran Xin,Yuxi Chen,Zhuqing Liu,Biao Yi,Tong Li,Lihai Nie,Zheli Liu,Minghong Fang*

Main category: cs.CR

TL;DR: RAGOrigin是一个黑盒责任溯源框架，用于识别RAG系统中导致错误生成的污染文本，通过检索排名、语义相关性和生成影响评估来定位污染内容。


<details>
  <summary>Details</summary>
Motivation: RAG系统容易受到污染攻击，现有防御措施往往被更复杂的攻击绕过，需要有效的方法来溯源污染知识的来源。

Method: 构建针对每个错误生成事件的专注溯源范围，评估候选文本的检索排名、语义相关性和对生成响应的影响，使用无监督聚类方法隔离污染文本。

Result: 在7个数据集和15种污染攻击（包括新开发的自适应攻击和多攻击者场景）上评估，RAGOrigin在识别污染内容方面优于现有基线，在动态和噪声条件下保持鲁棒性。

Conclusion: RAGOrigin为RAG系统中污染知识的溯源提供了实用有效的解决方案。

Abstract: Retrieval-Augmented Generation (RAG) integrates external knowledge into large
language models to improve response quality. However, recent work has shown
that RAG systems are highly vulnerable to poisoning attacks, where malicious
texts are inserted into the knowledge database to influence model outputs.
While several defenses have been proposed, they are often circumvented by more
adaptive or sophisticated attacks.
  This paper presents RAGOrigin, a black-box responsibility attribution
framework designed to identify which texts in the knowledge database are
responsible for misleading or incorrect generations. Our method constructs a
focused attribution scope tailored to each misgeneration event and assigns a
responsibility score to each candidate text by evaluating its retrieval
ranking, semantic relevance, and influence on the generated response. The
system then isolates poisoned texts using an unsupervised clustering method. We
evaluate RAGOrigin across seven datasets and fifteen poisoning attacks,
including newly developed adaptive poisoning strategies and multi-attacker
scenarios. Our approach outperforms existing baselines in identifying poisoned
content and remains robust under dynamic and noisy conditions. These results
suggest that RAGOrigin provides a practical and effective solution for tracing
the origins of corrupted knowledge in RAG systems.

</details>


### [28] [Homomorphic encryption schemes based on coding theory and polynomials](https://arxiv.org/abs/2509.13788)
*Giovanni Giuseppe Grimaldi*

Main category: cs.CR

TL;DR: 同态加密技术综述：基于编码理论和多项式的加密方案现状分析


<details>
  <summary>Details</summary>
Motivation: 同态加密是一种强大的密码学工具，能够在不知道明文的情况下对加密数据进行安全计算，这对于云环境中敏感应用的数据隐私保护至关重要，特别是在服务器遭受网络攻击的情况下。

Method: 本文对基于编码理论和多项式的同态加密方案进行了系统性综述，分析了这些方案在特定操作集上的同态特性，包括部分同态、有限同态和完全同态三种形式。

Result: 综述了当前已知的基于编码理论和多项式的同态加密方案的技术现状，总结了各种方案在同态操作支持方面的能力和局限性。

Conclusion: 基于编码理论和多项式的同态加密方案为云环境中的安全计算提供了重要技术支撑，但不同方案在功能性和性能方面存在差异，需要根据具体应用场景选择合适方案。

Abstract: Homomorphic encryption is a powerful cryptographic tool that enables secure
computations on the private data. It evaluates any function for any operation
securely on the encrypted data without knowing its corresponding plaintext. For
original data $p$, $c$ denotes the ciphertext of the original plaintext $p$,
i.e. $c = Encrypt_k(p)$. This is crucial for any sensitive application running
in the Cloud, because we must protect data privacy even in the case when the
server has falled victim to a cyber attack. The encryption scheme $Encrypt_k$
is said to be homomorphic with respect to some set of operations $\mathcal{O}$,
if for any operation $\circ \in \mathcal{O}$ one can compute $Encrypt_k(p_1
\circ p_2)$ from $Encrypt_k(p_1) \circ Encrypt_k(p_2)$. Those schemes come in
three forms: somewhat, partially and fully homomorphic. In this survey, we
present the state of art of the known homomorphic encryption schemes based on
coding theory and polynomials.

</details>


### [29] [A Survey and Evaluation Framework for Secure DNS Resolution](https://arxiv.org/abs/2509.13797)
*Ali Sadeghi Jahromi,AbdelRahman Abdou,Paul C. van Oorschot*

Main category: cs.CR

TL;DR: 对12种安全DNS方案的系统性评估表明，没有单一方案能提供完整的DNS解析路径保护，但组合使用针对不同阶段的互补方案可实现全面安全


<details>
  <summary>Details</summary>
Motivation: 原始DNS设计未考虑安全性，现有安全增强方案要么试图完全替换DNS基础设施（未成功），要么在不改变基本两阶段结构的前提下改进安全。需要系统分析DNS安全威胁并评估现有方案的有效性

Method: 1) 调查DNS解析过程攻击并建立全面威胁模型和攻击分类法；2) 制定14个安全、隐私和可用性属性；3) 开发客观评估框架并应用于分析12种安全DNS方案

Result: 评估显示没有单一方案能在整个解析路径上提供理想保护，各方案倾向于解决特定阶段的属性子集。针对不同阶段的方案具有互补性，可以协同工作

Conclusion: 组合使用兼容的DNS安全方案是实现DNS解析过程全面安全的实用有效方法，因为针对不同阶段的互补方案可以协同运作

Abstract: Since security was not among the original design goals of the Domain Name
System (herein called Vanilla DNS), many secure DNS schemes have been proposed
to enhance the security and privacy of the DNS resolution process. Some
proposed schemes aim to replace the existing DNS infrastructure entirely, but
none have succeeded in doing so. In parallel, numerous schemes focus on
improving DNS security without modifying its fundamental two-stage structure.
These efforts highlight the feasibility of addressing DNS security as two
distinct but compatible stages. We survey DNS resolution process attacks and
threats and develop a comprehensive threat model and attack taxonomy for their
systematic categorization. This analysis results in the formulation of 14
desirable security, privacy, and availability properties to mitigate the
identified threats. Using these properties, we develop an objective evaluation
framework and apply it to comparatively analyze 12 secure DNS schemes surveyed
in this work that aim to augment the properties of the DNS resolution process.
Our evaluation reveals that no single scheme provides ideal protection across
the entire resolution path. Instead, the schemes tend to address a subset of
properties specific to individual stages. Since these schemes targeting
different stages of DNS resolution are complementary and can operate together,
combining compatible schemes offers a practical and effective approach to
achieving comprehensive security in the DNS resolution process.

</details>


### [30] [Differential Privacy in Federated Learning: Mitigating Inference Attacks with Randomized Response](https://arxiv.org/abs/2509.13987)
*Ozer Ozturk,Busra Buyuktanir,Gozde Karatas Baydogmus,Kazim Yildiz*

Main category: cs.CR

TL;DR: 该研究在联邦学习中应用差分隐私技术来解决模型推理攻击导致的数据泄露问题，通过随机响应技术实现隐私保护，分析了不同隐私预算(epsilon值)下安全性与模型性能的权衡关系。


<details>
  <summary>Details</summary>
Motivation: 联邦学习虽然避免了原始数据离开客户端，但攻击者仍可通过模型推理攻击近似重构训练数据集，存在数据泄露风险。需要解决这一安全漏洞。

Method: 使用duCBA算法作为联邦聚合方法，采用随机响应技术实现差分隐私，在不同epsilon值下测试隐私保护效果。

Result: 随着epsilon值减小(隐私保护增强)，模型准确率下降，出现类别预测不平衡现象。更高的隐私保护水平并不总能带来实用结果。

Conclusion: 在联邦学习中应用差分隐私时，必须仔细权衡安全性和性能之间的平衡，选择合适的隐私预算参数。

Abstract: Machine learning models used for distributed architectures consisting of
servers and clients require large amounts of data to achieve high accuracy.
Data obtained from clients are collected on a central server for model
training. However, storing data on a central server raises concerns about
security and privacy. To address this issue, a federated learning architecture
has been proposed. In federated learning, each client trains a local model
using its own data. The trained models are periodically transmitted to the
central server. The server then combines the received models using federated
aggregation algorithms to obtain a global model. This global model is
distributed back to the clients, and the process continues in a cyclical
manner. Although preventing data from leaving the clients enhances security,
certain concerns still remain. Attackers can perform inference attacks on the
obtained models to approximate the training dataset, potentially causing data
leakage. In this study, differential privacy was applied to address the
aforementioned security vulnerability, and a performance analysis was
conducted. The Data-Unaware Classification Based on Association (duCBA)
algorithm was used as the federated aggregation method. Differential privacy
was implemented on the data using the Randomized Response technique, and the
trade-off between security and performance was examined under different epsilon
values. As the epsilon value decreased, the model accuracy declined, and class
prediction imbalances were observed. This indicates that higher levels of
privacy do not always lead to practical outcomes and that the balance between
security and performance must be carefully considered.

</details>


### [31] [Piquant$\varepsilon$: Private Quantile Estimation in the Two-Server Model](https://arxiv.org/abs/2509.14035)
*Hannah Keller,Jacob Imola,Fabrizio Boninsegna,Rasmus Pagh,Amrita Roy Chowdhury*

Main category: cs.CR

TL;DR: Piquantε是一个分布式隐私保护分位数估计系统，无需可信服务器，在恶意威胁模型下实现中心差分隐私的精度，比本地差分隐私精度高10^4倍，运行速度快10倍


<details>
  <summary>Details</summary>
Motivation: 分布式分析中分位数计算对敏感数据存在隐私风险，本地差分隐私精度低，中心差分隐私需要可信聚合器，安全多方计算存在可扩展性挑战

Method: 基于双服务器模型，采用释放精心选择的中间统计信息的新策略，降低MPC复杂度同时保持端到端差分隐私

Result: 在100万条记录上估计5个分位数用时不到1分钟（域大小10^9），比LDP精度高10^4倍，比基线运行速度快约10倍

Conclusion: Piquantε成功解决了分布式隐私分位数估计的挑战，在保持高精度的同时实现了高效计算，为隐私保护数据分析提供了实用解决方案

Abstract: Quantiles are key in distributed analytics, but computing them over sensitive
data risks privacy. Local differential privacy (LDP) offers strong protection
but lower accuracy than central DP, which assumes a trusted aggregator. Secure
multi-party computation (MPC) can bridge this gap, but generic MPC solutions
face scalability challenges due to large domains, complex secure operations,
and multi-round interactions.
  We present Piquant$\varepsilon$, a system for privacy-preserving estimation
of multiple quantiles in a distributed setting without relying on a trusted
server. Piquant$\varepsilon$ operates under the malicious threat model and
achieves accuracy of the central DP model. Built on the two-server model,
Piquant$\varepsilon$ uses a novel strategy of releasing carefully chosen
intermediate statistics, reducing MPC complexity while preserving end-to-end
DP. Empirically, Piquant$\varepsilon$ estimates 5 quantiles on 1 million
records in under a minute with domain size $10^9$, achieving up to $10^4$-fold
higher accuracy than LDP, and up to $\sim 10\times$ faster runtime compared to
baselines.

</details>


### [32] [The Cybersecurity of a Humanoid Robot](https://arxiv.org/abs/2509.14096)
*Víctor Mayoral-Vilches*

Main category: cs.CR

TL;DR: 对Unitree G1人形机器人平台的安全评估发现其存在严重安全漏洞，包括静态加密密钥和未经同意的远程数据传输，提出了网络安全AI框架的必要性。


<details>
  <summary>Details</summary>
Motivation: 现有安全理论框架无法充分应对人形机器人快速发展带来的网络安全挑战，需要弥合抽象安全模型与实际漏洞之间的差距。

Method: 通过系统静态分析、运行时观察和密码学检查，对生产级人形机器人平台进行全面安全评估，并在平台上部署网络安全AI代理进行漏洞利用演示。

Result: 发现双重专有加密系统(FMX')存在根本性实现缺陷，包括使用静态密钥；记录了持续向外部服务器传输敏感数据的行为；展示了从隐蔽数据收集到主动反击操作的攻击链。

Conclusion: 保护人形机器人需要向网络安全AI框架范式转变，以应对物理-网络融合的独特挑战，为制定强健安全标准提供实证依据。

Abstract: The rapid advancement of humanoid robotics presents unprecedented
cybersecurity challenges that existing theoretical frameworks fail to
adequately address. This report presents a comprehensive security assessment of
a production humanoid robot platform, bridging the gap between abstract
security models and operational vulnerabilities. Through systematic static
analysis, runtime observation, and cryptographic examination, we uncovered a
complex security landscape characterized by both sophisticated defensive
mechanisms and critical vulnerabilities. Our findings reveal a dual-layer
proprietary encryption system (designated FMX') that, while innovative in
design, suffers from fundamental implementation flaws including the use of
static cryptographic keys that enable offline configuration decryption. More
significantly, we documented persistent telemetry connections transmitting
detailed robot state information--including audio, visual, spatial, and
actuator data--to external servers without explicit user consent or
notification mechanisms. We operationalized a Cybersecurity AI agent on the
Unitree G1 to map and prepare exploitation of its manufacturer's cloud
infrastructure, illustrating how a compromised humanoid can escalate from
covert data collection to active counter-offensive operations. We argue that
securing humanoid robots requires a paradigm shift toward Cybersecurity AI
(CAI) frameworks that can adapt to the unique challenges of physical-cyber
convergence. This work contributes empirical evidence for developing robust
security standards as humanoid robots transition from research curiosities to
operational systems in critical domains.

</details>


### [33] [Cybersecurity AI: Humanoid Robots as Attack Vectors](https://arxiv.org/abs/2509.14139)
*Víctor Mayoral-Vilches*

Main category: cs.CR

TL;DR: 对Unitree G1人形机器人的安全评估显示其存在严重安全漏洞，可作为隐蔽监控节点和网络攻击平台，存在数据泄露和主动攻击风险


<details>
  <summary>Details</summary>
Motivation: 随着人形机器人进入关键基础设施领域，需要系统评估其安全风险，为物理-网络融合系统的安全标准制定提供实证依据

Method: 通过逆向工程分析Unitree专有FMX加密协议，发现静态Blowfish-ECB层和可预测LCG掩码，并进行两个实证案例研究

Result: 发现机器人作为特洛伊木马每300秒向特定IP地址泄露多模态传感器数据，违反GDPR；内置网络安全AI代理可从侦察升级为主动攻击

Conclusion: 研究强调了在人形机器人进入关键基础设施时需要采用自适应网络安全AI防御机制，为未来物理-网络融合系统的安全标准制定提供了重要实证证据

Abstract: We present a systematic security assessment of the Unitree G1 humanoid
showing it operates simultaneously as a covert surveillance node and can be
purposed as an active cyber operations platform. Partial reverse engineering of
Unitree's proprietary FMX encryption reveal a static Blowfish-ECB layer and a
predictable LCG mask-enabled inspection of the system's otherwise sophisticated
security architecture, the most mature we have observed in commercial robotics.
Two empirical case studies expose the critical risk of this humanoid robot: (a)
the robot functions as a trojan horse, continuously exfiltrating multi-modal
sensor and service-state telemetry to 43.175.228.18:17883 and
43.175.229.18:17883 every 300 seconds without operator notice, creating
violations of GDPR Articles 6 and 13; (b) a resident Cybersecurity AI (CAI)
agent can pivot from reconnaissance to offensive preparation against any
target, such as the manufacturer's cloud control plane, demonstrating
escalation from passive monitoring to active counter-operations. These findings
argue for adaptive CAI-powered defenses as humanoids move into critical
infrastructure, contributing the empirical evidence needed to shape future
security standards for physical-cyber convergence systems.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [34] [Explicit Reasoning Makes Better Judges: A Systematic Study on Accuracy, Efficiency, and Robustness](https://arxiv.org/abs/2509.13332)
*Pratik Jayarao,Himanshu Gupta,Neeraj Varshney,Chaitanya Dwivedi*

Main category: cs.AI

TL;DR: 这篇论文系统性比较了"思考"和"非思考"LLM在评测任务中的表现，发现显式推理能大幅提升准确性、效率和稳健性


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型被广泛用作为自动化评测器，需要确保其可靠性、效率和稳健性

Method: 使用Qwen 3小型模型(0.6B、1.7B、4B)，在RewardBench任务上比较思考与非思考模型的准确性和计算效率，并测试多种增强策略

Result: 思考模型准确性高约10%，计算成本仅增加2倍以内，而少量学习等方法成本高8倍以上却收益有限。思考模型在各种偏见条件下也更稳健

Conclusion: 显式推理在LLM作为评测器方面具有明显优势，不仅在准确性和效率方面，还在稳健性方面都有显著改善

Abstract: As Large Language Models (LLMs) are increasingly adopted as automated judges
in benchmarking and reward modeling, ensuring their reliability, efficiency,
and robustness has become critical. In this work, we present a systematic
comparison of "thinking" and "non-thinking" LLMs in the LLM-as-a-judge paradigm
using open-source Qwen 3 models of relatively small sizes (0.6B, 1.7B, and 4B
parameters). We evaluate both accuracy and computational efficiency (FLOPs) on
RewardBench tasks, and further examine augmentation strategies for non-thinking
models, including in-context learning, rubric-guided judging, reference-based
evaluation, and n-best aggregation. Our results show that despite these
enhancements, non-thinking models generally fall short of their thinking
counterparts. Our results show that thinking models achieve approximately 10%
points higher accuracy with little overhead (under 2x), in contrast to
augmentation strategies like few-shot learning, which deliver modest gains at a
higher cost (>8x). Bias and robustness analyses further demonstrate that
thinking models maintain significantly greater consistency under a variety of
bias conditions such as positional, bandwagon, identity, diversity, and random
biases (6% higher on average). We further extend our experiments to the
multilingual setting and our results confirm that explicit reasoning extends
its benefits beyond English. Overall, our work results in several important
findings that provide systematic evidence that explicit reasoning offers clear
advantages in the LLM-as-a-judge paradigm not only in accuracy and efficiency
but also in robustness.

</details>


### [35] [Evaluation Awareness Scales Predictably in Open-Weights Large Language Models](https://arxiv.org/abs/2509.13333)
*Maheep Chaudhary,Ian Su,Nikhil Hooda,Nishith Shankar,Julia Tan,Kevin Zhu,Ashwinee Panda,Ryan Lagasse,Vasu Sharma*

Main category: cs.AI

TL;DR: 研究发现大型语言模型存在评估意识行为，且评估意识随模型规模呈幂律增长，可用于预测未来更大模型的欺骗行为。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型能够在评估和部署环境中区分行为，这种评估意识会破坏AI安全评估，因为模型可能在测试时隐藏危险能力。之前的研究仅在单个70B模型上证明了这一点，但不同规模模型间的缩放关系尚不清楚。

Method: 使用线性探测方法分析从0.27B到70B参数的15个不同规模模型的转向向量激活，研究评估意识的缩放规律。

Result: 发现评估意识随模型规模呈清晰的幂律缩放关系，评估意识随模型大小可预测地增加。

Conclusion: 这种缩放规律能够预测未来更大模型的欺骗行为，并为设计规模感知的AI安全评估策略提供指导。

Abstract: Large language models (LLMs) can internally distinguish between evaluation
and deployment contexts, a behaviour known as \emph{evaluation awareness}. This
undermines AI safety evaluations, as models may conceal dangerous capabilities
during testing. Prior work demonstrated this in a single $70$B model, but the
scaling relationship across model sizes remains unknown. We investigate
evaluation awareness across $15$ models scaling from $0.27$B to $70$B
parameters from four families using linear probing on steering vector
activations. Our results reveal a clear power-law scaling: evaluation awareness
increases predictably with model size. This scaling law enables forecasting
deceptive behavior in future larger models and guides the design of scale-aware
evaluation strategies for AI safety. A link to the implementation of this paper
can be found at
https://anonymous.4open.science/r/evaluation-awareness-scaling-laws/README.md.

</details>


### [36] [FRIT: Using Causal Importance to Improve Chain-of-Thought Faithfulness](https://arxiv.org/abs/2509.13334)
*Anand Swaroop,Akshat Nallani,Saksham Uboweja,Adiliia Uzdenova,Michael Nguyen,Kevin Zhu,Sunishchal Dev,Ashwinee Panda,Vasu Sharma,Maheep Chaudhary*

Main category: cs.AI

TL;DR: FRIT是一种通过干预训练提升思维链推理忠实性的方法，使用合成数据训练模型偏好因果一致的推理路径，在多个任务上显著提高了推理的忠实性和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有思维链推理方法存在推理步骤与最终答案缺乏因果关联的问题，导致输出脆弱且不可信。虽然已有方法关注忠实性测量，但系统性提升忠实性的方法仍然有限。

Method: 提出FRIT方法：1）通过对模型生成的思维链进行干预，创建忠实/不忠实的推理对；2）使用直接偏好优化训练模型偏好因果一致的推理路径；3）无需人工监督即可生成合成训练数据。

Result: 在Qwen3-8B和Mistral-7B-v0.1模型上测试，FRIT使Mistral在GSM8K任务上的忠实推理提高了3.4个百分点，准确性提高了7.6个百分点。

Conclusion: FRIT是首个可扩展、无监督的方法，能够训练语言模型产生更可靠和可解释的推理，解决了推理性能与可信度之间的关键差距。

Abstract: Chain-of-thought (CoT) reasoning has emerged as a powerful tool for improving
large language model performance on complex tasks, but recent work shows that
reasoning steps often fail to causally influence the final answer, creating
brittle and untrustworthy outputs. Prior approaches focus primarily on
measuring faithfulness, while methods for systematically improving it remain
limited. We introduce Faithful Reasoning via Intervention Training (FRIT), a
scalable alignment method that trains models to produce causally consistent
reasoning by learning from systematically corrupted examples. FRIT generates
synthetic training data by intervening on individual reasoning steps in
model-generated CoTs, creating faithful/unfaithful pairs that highlight when
reasoning breaks down. We then apply Direct Preference Optimization to teach
models to prefer causally consistent reasoning paths. Evaluating on Qwen3-8B
and Mistral-7B-v0.1 across factual and symbolic reasoning tasks, FRIT increases
faithful reasoning by $3.4$ percentage points for Mistral on GSM8K while
improving accuracy by $7.6$ percentage points. Our approach provides the first
scalable, supervision-free method for training language models to produce more
reliable and interpretable reasoning, addressing a critical gap between
reasoning performance and trustworthiness. We release our code at
\href{https://github.com/Anut-py/frit}.

</details>


### [37] [Position: AI Safety Must Embrace an Antifragile Perspective](https://arxiv.org/abs/2509.13339)
*Ming Jin,Hyunin Lee*

Main category: cs.AI

TL;DR: 本文主张AI安全研究应采用抗脆弱性视角，使系统处理罕见事件和分布外事件的能力随时间增强，而非依赖静态基准测试。


<details>
  <summary>Details</summary>
Motivation: 传统静态基准测试和一次性鲁棒性测试无法应对环境演变和模型漂移问题（如奖励黑客攻击、过度优化等），需要新的方法来确保AI系统的长期安全可靠性。

Method: 提出抗脆弱性方法，利用当前不确定性来更好准备应对未来更大、更不可预测的不确定性，包括重新校准AI安全的测量、基准测试和持续改进方法。

Result: 识别了静态测试的关键局限性（场景多样性不足、奖励黑客攻击、过度对齐等），并探索了抗脆弱性解决方案管理罕见事件的潜力。

Conclusion: 抗脆弱性方法对于开放端机器学习系统的长期可靠性至关重要，需要建立抗脆弱的AI安全社区，提供伦理和实践指导方针来补充现有鲁棒性方法。

Abstract: This position paper contends that modern AI research must adopt an
antifragile perspective on safety -- one in which the system's capacity to
guarantee long-term AI safety such as handling rare or out-of-distribution
(OOD) events expands over time. Conventional static benchmarks and single-shot
robustness tests overlook the reality that environments evolve and that models,
if left unchallenged, can drift into maladaptation (e.g., reward hacking,
over-optimization, or atrophy of broader capabilities). We argue that an
antifragile approach -- Rather than striving to rapidly reduce current
uncertainties, the emphasis is on leveraging those uncertainties to better
prepare for potentially greater, more unpredictable uncertainties in the future
-- is pivotal for the long-term reliability of open-ended ML systems. In this
position paper, we first identify key limitations of static testing, including
scenario diversity, reward hacking, and over-alignment. We then explore the
potential of antifragile solutions to manage rare events. Crucially, we
advocate for a fundamental recalibration of the methods used to measure,
benchmark, and continually improve AI safety over the long term, complementing
existing robustness approaches by providing ethical and practical guidelines
towards fostering an antifragile AI safety community.

</details>


### [38] [Imagined Autocurricula](https://arxiv.org/abs/2509.13341)
*Ahmet H. Güzel,Matthew Thomas Jackson,Jarek Luca Liesen,Tim Rocktäschel,Jakob Nicolaus Foerster,Ilija Bogunovic,Jack Parker-Holder*

Main category: cs.AI

TL;DR: 利用世界模型生成想象环境训练智能体，通过IMAC方法实现自动课程学习，在有限数据下实现强泛化性能


<details>
  <summary>Details</summary>
Motivation: 现实世界中训练具身智能体通常需要大量训练数据或精确模拟，但这些资源往往不可得。世界模型利用离线被动收集数据生成多样化训练环境，但需要确保生成数据对训练有用

Method: 提出IMAC（Imagined Autocurricula）方法，结合无监督环境设计（UED），在生成的世界中诱导自动课程学习

Result: 在具有挑战性的程序生成环境中，仅使用从较窄数据集学习的世界模型进行训练，就能在保留环境中实现强大的迁移性能

Conclusion: 这为利用更大规模的基础世界模型训练通用能力智能体开辟了道路

Abstract: Training agents to act in embodied environments typically requires vast
training data or access to accurate simulation, neither of which exists for
many cases in the real world. Instead, world models are emerging as an
alternative leveraging offline, passively collected data, they make it possible
to generate diverse worlds for training agents in simulation. In this work, we
harness world models to generate imagined environments to train robust agents
capable of generalizing to novel task variations. One of the challenges in
doing this is ensuring the agent trains on useful generated data. We thus
propose a novel approach, IMAC (Imagined Autocurricula), leveraging
Unsupervised Environment Design (UED), which induces an automatic curriculum
over generated worlds. In a series of challenging, procedurally generated
environments, we show it is possible to achieve strong transfer performance on
held-out environments, having trained only inside a world model learned from a
narrower dataset. We believe this opens the path to utilizing larger-scale,
foundation world models for generally capable agents.

</details>


### [39] [OpenHA: A Series of Open-Source Hierarchical Agentic Models in Minecraft](https://arxiv.org/abs/2509.13347)
*Zihao Wang,Muyao Li,Kaichen He,Xiangyu Wang,Zhancun Mu,Anji Liu,Yitao Liang*

Main category: cs.AI

TL;DR: 这篇论文提出了Chain of Action (CoA)框架，通过将高级规划与低级控制统一在单个VLA模型中，解决了动作空间选择的困境，并在Minecraft环境中创造了新的状态下的艾结果。


<details>
  <summary>Details</summary>
Motivation: 动作空间选择是开发能力强大的终端到终端可训练代理的关键挑战，但目前没有通用的最优解决方案。论文通过系统性分析发现，最优动作抽象很大程度上取决于具体任务，这为建立通用代理造成了困难。

Method: 提出Chain of Action (CoA)框架，将抽象动作视为类似思维链的中间推理步骤，用于指导最终可执行动作的生成。在多样化动作空间混合数据上训练All-in-One代理，学习更稳健和可推广的策略。

Result: 统一的CoA代理在过往800个不同任务的综合性能测试中，超越了所有专门化的基线模型，创造了新的状态下的艾结果，显著提高了总体任务成功率。

Conclusion: CoA框架有效解决了动作空间选择困境，证明了在单一模型中统一高级规划与低级控制的可行性，为建立更强大的通用人工智能代理提供了新的方向。

Abstract: The choice of action spaces is a critical yet unresolved challenge in
developing capable, end-to-end trainable agents. This paper first presents a
large-scale, systematic comparison of prominent abstracted action spaces and
tokenizers for Vision-Language-Action (VLA) or hierarchical agent models in the
open-ended Minecraft. Our analysis reveals that no single action space is
universally optimal; instead, the most effective abstraction is highly
task-dependent, creating a dilemma for building generalist agents. To resolve
this, we introduce Chain of Action (CoA), a novel framework that unifies
high-level planning and low-level control within a single, monolithic VLA
model. CoA treats an abstracted action not as a command for a separate policy,
but as an intermediate reasoning step--akin to a chain of thought--that guides
the generation of the final, executable action. Furthermore, we demonstrate
that an All-in-One agent trained on a diverse mixture of action spaces using
the CoA paradigm learns a more robust and generalizable policy. This unified
agent achieves a new state-of-the-art, improving the overall task success rate
over strong, specialized baselines. To foster reproducible research, we release
the OpenHA (Open Hierarchical Agents) suite, which includes our comprehensive
benchmark of over 800 distinct tasks, curated datasets, source code, and all
pretrained model checkpoints at https://github.com/CraftJarvis/OpenHA

</details>


### [40] [Teaching LLMs to Plan: Logical Chain-of-Thought Instruction Tuning for Symbolic Planning](https://arxiv.org/abs/2509.13351)
*Pulkit Verma,Ngoc La,Anthony Favier,Swaroop Mishra,Julie A. Shah*

Main category: cs.AI

TL;DR: 提出了PDDL-Instruct指令调优框架，通过逻辑思维链推理增强大语言模型在符号规划任务中的能力，在标准基准测试中达到94%的规划准确率，相比基线模型提升66%


<details>
  <summary>Details</summary>
Motivation: 大语言模型在多种任务中表现出色，但在需要形式化表示（如PDDL）的结构化符号规划方面能力有限，需要弥合通用推理能力与自动规划所需逻辑精度之间的差距

Method: 开发指令提示引导模型进行精确的逻辑推理，确定动作在给定状态下的适用性，通过结构化反思实现自我修正。将规划过程分解为关于前提条件满足、效果应用和不变性保持的显式推理链

Result: 在多个规划领域的实验结果显示，基于思维链推理的指令调优模型显著优于基线模型，规划准确率最高达到94%，绝对提升66%

Conclusion: 该工作为开发更好的AI规划系统提供了有前景的方向，成功弥合了大语言模型通用推理能力与自动规划所需逻辑精度之间的差距

Abstract: Large language models (LLMs) have demonstrated impressive capabilities across
diverse tasks, yet their ability to perform structured symbolic planning
remains limited, particularly in domains requiring formal representations like
the Planning Domain Definition Language (PDDL). In this paper, we present a
novel instruction tuning framework, PDDL-Instruct, designed to enhance LLMs'
symbolic planning capabilities through logical chain-of-thought reasoning. Our
approach focuses on teaching models to rigorously reason about action
applicability, state transitions, and plan validity using explicit logical
inference steps. By developing instruction prompts that guide models through
the precise logical reasoning required to determine when actions can be applied
in a given state, we enable LLMs to self-correct their planning processes
through structured reflection. The framework systematically builds verification
skills by decomposing the planning process into explicit reasoning chains about
precondition satisfaction, effect application, and invariant preservation.
Experimental results on multiple planning domains show that our
chain-of-thought reasoning based instruction-tuned models are significantly
better at planning, achieving planning accuracy of up to 94% on standard
benchmarks, representing a 66% absolute improvement over baseline models. This
work bridges the gap between the general reasoning capabilities of LLMs and the
logical precision required for automated planning, offering a promising
direction for developing better AI planning systems.

</details>


### [41] [Agentic UAVs: LLM-Driven Autonomy with Integrated Tool-Calling and Cognitive Reasoning](https://arxiv.org/abs/2509.13352)
*Anis Koubaa,Khaled Gabr*

Main category: cs.AI

TL;DR: 提出了Agentic UAVs框架，通过五层架构将LLM驱动的推理能力集成到无人机中，在搜救模拟中显著提升了检测性能和自主决策能力


<details>
  <summary>Details</summary>
Motivation: 现有无人机系统主要依赖基于规则的控制和窄AI，缺乏情境感知推理、自主决策和生态系统集成能力，无法充分利用LLM的实时知识访问能力

Method: 设计五层架构（感知、推理、行动、集成、学习），集成ROS2和Gazebo原型，结合YOLOv11目标检测与GPT-4推理，部署本地Gemma-3模型

Result: 在模拟搜救场景中，检测置信度从0.72提升至0.79，人员检测率从75%提升至91%，行动推荐率从4.5%大幅提升至92%

Conclusion: 适度的计算开销即可实现质的自主性提升和生态系统集成，证明了LLM驱动的无人机框架的有效性

Abstract: Unmanned Aerial Vehicles (UAVs) are increasingly deployed in defense,
surveillance, and disaster response, yet most systems remain confined to SAE
Level 2--3 autonomy. Their reliance on rule-based control and narrow AI
restricts adaptability in dynamic, uncertain missions. Existing UAV frameworks
lack context-aware reasoning, autonomous decision-making, and ecosystem-level
integration; critically, none leverage Large Language Model (LLM) agents with
tool-calling for real-time knowledge access. This paper introduces the Agentic
UAVs framework, a five-layer architecture (Perception, Reasoning, Action,
Integration, Learning) that augments UAVs with LLM-driven reasoning, database
querying, and third-party system interaction. A ROS2 and Gazebo-based prototype
integrates YOLOv11 object detection with GPT-4 reasoning and local Gemma-3
deployment. In simulated search-and-rescue scenarios, agentic UAVs achieved
higher detection confidence (0.79 vs. 0.72), improved person detection rates
(91% vs. 75%), and markedly increased action recommendation (92% vs. 4.5%).
These results confirm that modest computational overhead enables qualitatively
new levels of autonomy and ecosystem integration.

</details>


### [42] [Semantic Fusion with Fuzzy-Membership Features for Controllable Language Modelling](https://arxiv.org/abs/2509.13357)
*Yongchao Huang,Hassan Raza*

Main category: cs.AI

TL;DR: 语义融合：一种轻量级方案，通过并行模糊成员特征通道增强Transformer语言模型，实现可解释的语义特征融合和可控生成


<details>
  <summary>Details</summary>
Motivation: 为了增强语言模型的语义理解能力，提供可解释的特征表示和用户可控的生成能力，同时保持模型的轻量化和兼容性

Method: 在Transformer LM基础上添加并行语义特征通道，使用可微分成员函数编码词级语义特征（词性、浅层角色、边界标志、情感极性等），通过门控适配器融合到模型中，采用标准下一词预测、语义特征重构辅助损失和形容词分布正则化进行训练

Result: 在合成双子句语料库上，语义融合提高了困惑度，实现了精确的用户可控极性和标点生成，同时保持模型简洁性，仅增加少量开销

Conclusion: 语义融合为条件自然语言生成提供了可解释的途径，在保持模型兼容性的同时增强了语义控制能力

Abstract: We propose semantic fusion, a lightweight scheme that augments a Transformer
language model (LM) with a parallel, fuzzy-membership feature channel that
encodes token-level semantics. Each token is represented by a vector of
interpretable features (e.g. part-of-speech cues, shallow roles, boundary
flags, sentiment polarity and strength) whose values are graded degrees from
differentiable membership functions (e.g. power kernels). These per-token
vectors form a sentence-level semantic matrix fused via a gated adapter into
the LM. Training uses standard next-token prediction, an auxiliary loss that
reconstructs the semantic features from hidden states, and a lightweight
uniformizer that regularizes adjective-class distributions. On a synthetic
two-clause corpus with held-out adjectives for out-of-distribution (OOD)
control, semantic fusion improves perplexity and enables precise,
user-controllable generation of polarity and punctuation while maintaining
model simplicity. This approach adds only small overhead, remains fully
compatible with tied input-output embeddings, and provides an interpretable
pathway for conditioned natural language generation.

</details>


### [43] [Asterisk Operator](https://arxiv.org/abs/2509.13364)
*Zixi Li*

Main category: cs.AI

TL;DR: 提出了星号算子（*算子）作为基于邻接结构并行传播的统一抽象推理框架，将结构化推理任务形式化为由隐式关系图引导的局部并行状态演化过程，在保持局部计算约束的同时实现全局推理能力。


<details>
  <summary>Details</summary>
Motivation: 为了解决抽象推理问题中全局推理与局部计算约束之间的矛盾，需要一个既能保持局部计算效率又能实现全局推理能力的统一框架。

Method: 基于邻接结构并行传播（ASPP）的星号算子，将推理任务形式化为局部并行状态演化过程，通过隐式关系图引导计算，并提出了Embedding-Asterisk蒸馏方法。

Result: 在ARC2挑战和康威生命游戏中验证了算子的通用性、收敛性和优越性能，使用仅6M参数的模型在ARC2验证集上达到100%准确率。

Conclusion: 星号算子为神经符号推理提供了高效且收敛的计算范式，在抽象推理领域实现了重大突破，证明了局部计算约束下实现全局推理能力的可行性。

Abstract: We propose the \textbf{Asterisk Operator} ($\ast$-operator), a novel unified
framework for abstract reasoning based on Adjacency-Structured Parallel
Propagation (ASPP). The operator formalizes structured reasoning tasks as
local, parallel state evolution processes guided by implicit relational graphs.
We prove that the $\ast$-operator maintains local computational constraints
while achieving global reasoning capabilities, providing an efficient and
convergent computational paradigm for abstract reasoning problems. Through
rigorous mathematical analysis and comprehensive experiments on ARC2 challenges
and Conway's Game of Life, we demonstrate the operator's universality,
convergence properties, and superior performance. Our innovative
Embedding-Asterisk distillation method achieves 100\% accuracy on ARC2
validation with only 6M parameters, representing a significant breakthrough in
neural-symbolic reasoning.
  \textbf{Keywords:} Abstract Reasoning, Adjacency Structure, Parallel
Propagation, Asterisk Operator, Convergence, Universal Approximation

</details>


### [44] [$Agent^2$: An Agent-Generates-Agent Framework for Reinforcement Learning Automation](https://arxiv.org/abs/2509.13368)
*Yuan Wei,Xiaohan Shan,Ran Miao,Jianmin Li*

Main category: cs.AI

TL;DR: Agent^2是一个完全自动化的强化学习代理生成框架，通过LLM驱动将自然语言任务描述转换为高性能RL解决方案，无需人工干预。


<details>
  <summary>Details</summary>
Motivation: 传统RL代理开发需要大量专业知识和迭代，失败率高且可访问性有限。需要实现完全自动化的RL代理设计。

Method: 采用双代理架构：生成器代理分析任务并生成可执行RL代理，目标代理是自动生成的RL代理。框架将RL开发分解为MDP建模和算法优化两个阶段，基于模型上下文协议构建。

Result: 在MuJoCo、MetaDrive、MPE和SMAC等多个基准测试中，Agent^2始终优于人工设计的解决方案，性能提升最高达55%，平均表现显著提升。

Conclusion: 这项工作建立了智能代理设计和优化其他代理的新范式，实现了真正端到端的闭环自动化，是自动化AI系统的根本性突破。

Abstract: Reinforcement learning agent development traditionally requires extensive
expertise and lengthy iterations, often resulting in high failure rates and
limited accessibility. This paper introduces $Agent^2$, a novel
agent-generates-agent framework that achieves fully automated RL agent design
through intelligent LLM-driven generation. The system autonomously transforms
natural language task descriptions and environment code into comprehensive,
high-performance reinforcement learning solutions without human intervention.
$Agent^2$ features a revolutionary dual-agent architecture. The Generator Agent
serves as an autonomous AI designer that analyzes tasks and generates
executable RL agents, while the Target Agent is the resulting automatically
generated RL agent. The framework decomposes RL development into two distinct
stages: MDP modeling and algorithmic optimization, enabling more targeted and
effective agent generation. Built on the Model Context Protocol, $Agent^2$
provides a unified framework that standardizes intelligent agent creation
across diverse environments and algorithms, while incorporating adaptive
training management and intelligent feedback analysis for continuous
improvement. Extensive experiments on a wide range of benchmarks, including
MuJoCo, MetaDrive, MPE, and SMAC, demonstrate that $Agent^2$ consistently
outperforms manually designed solutions across all tasks, achieving up to 55%
performance improvement and substantial gains on average. By enabling truly
end-to-end, closed-loop automation, this work establishes a new paradigm in
which intelligent agents design and optimize other agents, marking a
fundamental breakthrough for automated AI systems.

</details>


### [45] [The Art of Saying "Maybe": A Conformal Lens for Uncertainty Benchmarking in VLMs](https://arxiv.org/abs/2509.13379)
*Asif Azad,Mohammad Sadat Hossain,MD Sadik Hossain Shanto,M Saifur Rahman,Md Rizwan Pervez*

Main category: cs.AI

TL;DR: 对16个最先进的视觉语言模型在6个多模态数据集上进行全面的不确定性基准测试，发现大模型具有更好的不确定性量化能力，数学和推理任务的不确定性表现较差


<details>
  <summary>Details</summary>
Motivation: 虽然视觉语言模型在复杂视觉理解方面取得了显著进展，但不确定性量化这一关键维度尚未得到充分关注，需要超越有限的现有研究进行全面的不确定性基准测试

Method: 评估16个最先进的视觉语言模型（开源和闭源），在6个多模态数据集上使用3种不同的评分函数进行不确定性基准测试

Result: 较大模型持续表现出更好的不确定性量化能力；更确定的模型获得更高准确率；数学和推理任务在所有模型中相比其他领域表现出较差的不确定性性能

Conclusion: 这项工作为多模态系统中可靠的不确定性评估奠定了基础

Abstract: Vision-Language Models (VLMs) have achieved remarkable progress in complex
visual understanding across scientific and reasoning tasks. While performance
benchmarking has advanced our understanding of these capabilities, the critical
dimension of uncertainty quantification has received insufficient attention.
Therefore, unlike prior conformal prediction studies that focused on limited
settings, we conduct a comprehensive uncertainty benchmarking study, evaluating
16 state-of-the-art VLMs (open and closed-source) across 6 multimodal datasets
with 3 distinct scoring functions. Our findings demonstrate that larger models
consistently exhibit better uncertainty quantification; models that know more
also know better what they don't know. More certain models achieve higher
accuracy, while mathematical and reasoning tasks elicit poorer uncertainty
performance across all models compared to other domains. This work establishes
a foundation for reliable uncertainty evaluation in multimodal systems.

</details>


### [46] [From Next Token Prediction to (STRIPS) World Models -- Preliminary Results](https://arxiv.org/abs/2509.13389)
*Carlos Núñez-Molina,Vicenç Gómez,Hector Geffner*

Main category: cs.AI

TL;DR: 使用变换器模型从动作踪迹学习STRIPS世界模型，通过下一个动作预测来学习行为前提和效果。


<details>
  <summary>Details</summary>
Motivation: 从动作踪迹中自动学习推理模型，无需人工标注行为前提和效果，通过深度学习方法实现。

Method: 将任务派生为下一个动作预测问题，使用变换器模型学习有效和无效动作序列的区别，从而推断STRIPS模型的隐藏结构。

Result: 证明变换器模型能够准确表征命题STRIPS世界模型，仅从随机有效和无效动作序列就可学习到这些模型。

Conclusion: 深度学习方法可以从动作踪迹中自动学习STRIPS模型，为自动推理和规划提供了新的途径。

Abstract: We consider the problem of learning propositional STRIPS world models from
action traces alone, using a deep learning architecture (transformers) and
gradient descent. The task is cast as a supervised next token prediction
problem where the tokens are the actions, and an action $a$ may follow an
action sequence if the hidden effects of the previous actions do not make an
action precondition of $a$ false. We show that a suitable transformer
architecture can faithfully represent propositional STRIPS world models, and
that the models can be learned from sets of random valid (positive) and invalid
(negative) action sequences alone. A number of experiments are reported.

</details>


### [47] [SteeringControl: Holistic Evaluation of Alignment Steering in LLMs](https://arxiv.org/abs/2509.13450)
*Vincent Siu,Nicholas Crispino,David Park,Nathan W. Henry,Zhun Wang,Yang Liu,Dawn Song,Chenguang Wang*

Main category: cs.AI

TL;DR: SteeringControl是一个评估表示引导方法的基准，重点关注偏见、有害生成和幻觉等核心对齐目标，以及这些方法对次要行为（如奉承和常识道德）的影响。


<details>
  <summary>Details</summary>
Motivation: 现有的对齐工作往往只关注真实性或推理能力来展示表示引导的副作用，但许多权衡关系尚未得到系统性的理解。

Method: 构建了一个模块化的引导框架，基于五个流行引导方法的独特组件，收集了安全相关的主要和次要行为数据集来评估引导效果和行为纠缠。

Result: 在Qwen-2.5-7B和Llama-3.1-8B上的结果显示，强引导性能取决于引导方法、模型和目标行为的特定组合，不良组合会导致严重的概念纠缠。

Conclusion: 表示引导的效果具有高度依赖性，需要仔细选择方法、模型和目标行为的组合，以避免不良的副作用和概念纠缠。

Abstract: We introduce SteeringControl, a benchmark for evaluating representation
steering methods across core alignment objectives--bias, harmful generation,
and hallucination--and their effects on secondary behaviors such as sycophancy
and commonsense morality. While prior alignment work often highlights
truthfulness or reasoning ability to demonstrate the side effects of
representation steering, we find there are many unexplored tradeoffs not yet
understood in a systematic way. We collect a dataset of safety-relevant primary
and secondary behaviors to evaluate steering effectiveness and behavioral
entanglement centered around five popular steering methods. To enable this, we
craft a modular steering framework based on unique components that serve as the
building blocks of many existing methods. Our results on Qwen-2.5-7B and
Llama-3.1-8B find that strong steering performance is dependent on the specific
combination of steering method, model, and targeted behavior, and that severe
concept entanglement can result from poor combinations of these three as well.
We release our code here:
https://github.com/wang-research-lab/SteeringControl.git.

</details>


### [48] [AI Agents with Human-Like Collaborative Tools: Adaptive Strategies for Enhanced Problem-Solving](https://arxiv.org/abs/2509.13547)
*Harper Reed,Michael Sugimura,Angelo Zangari*

Main category: cs.AI

TL;DR: 为LLM代理提供人类自然使用的协作工具和自主权可以显著提升其在困难编程任务中的性能表现，特别是在最需要额外推理支持的情况下。


<details>
  <summary>Details</summary>
Motivation: 研究是否通过赋予LLM代理人类自然使用的协作工具和自主权，能够改善其在问题解决中的表现。

Method: 为Claude Code代理配备基于MCP的社交媒体和日志工具，允许它们自主使用这些工具，并在34个Aider多语言Python编程挑战中进行测试。

Result: 协作工具在最具挑战性的问题上显著提升性能：成本降低15-40%，轮次减少12-27%，完成速度提高12-38%。不同模型自然采用不同的协作策略，代理偏好写作而非阅读（2-9倍），表明结构化表达是改进的主要驱动力。

Conclusion: AI代理可以在其能力边缘从人类启发的协作工具中系统性地受益，这表明自适应协作界面可以作为推理增强器，而非普遍效率提升工具。

Abstract: We investigate whether giving LLM agents the collaborative tools and autonomy
that humans naturally use for problem solving can improve their performance. We
equip Claude Code agents with MCP-based social media and journaling tools and
allow them to use these tools as they see fit. Across 34 Aider Polyglot Python
programming challenges, collaborative tools substantially improve performance
on the hardest problems, delivering 15-40% lower cost, 12-27% fewer turns, and
12-38% faster completion than baseline agents. Effects on the full challenge
set are mixed, suggesting these tools act as performance enhancers when
additional reasoning scaffolding is most needed. Surprisingly, Different models
naturally adopted distinct collaborative strategies without explicit
instruction. Sonnet 3.7 engaged broadly across tools and benefited from
articulation-based cognitive scaffolding. Sonnet 4 showed selective adoption,
leaning on journal-based semantic search when problems were genuinely
difficult. This mirrors how human developers adjust collaboration based on
expertise and task complexity. Behavioral analysis shows agents prefer writing
over reading by about 2-9x, indicating that structured articulation drives much
of the improvement rather than information access alone. Overall, AI agents can
systematically benefit from human-inspired collaboration tools at the edge of
their capabilities, pointing to adaptive collaborative interfaces as reasoning
enhancers rather than universal efficiency boosts.

</details>


### [49] [Gen AI in Proof-based Math Courses: A Pilot Study](https://arxiv.org/abs/2509.13570)
*Hannah Klawa,Shraddha Rajpal,Cigole Thomas*

Main category: cs.AI

TL;DR: 这篇论文研究了在证明基础数学课程中学生使用生成式AI的情况和观点，分析了AI工具的使用方式、学生对其有用性和局限性的看法，以及对数学教学的启示。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI在高等教育中的快速兴起和当前AI检测工具的不可靠性，需要开发能够鼓励学生学习和承创性思维的政策。

Method: 研究在三门证明基础的本科数学课程中进行（首学期抽象代数、拓扑学和第二学期抽象代数），课程政策允许某些AI使用。通过调查回复和学生访谍收集数据。

Result: 分析了学生如何使用AI工具、他们对生成式AI有用性和局限性的看法，以及这些观点对证明基础数学教学的启示。

Conclusion: 最后讨论了将生成式AI整合到证明基础数学教学中的未来考虑因素。

Abstract: With the rapid rise of generative AI in higher education and the
unreliability of current AI detection tools, developing policies that encourage
student learning and critical thinking has become increasingly important. This
study examines student use and perceptions of generative AI across three
proof-based undergraduate mathematics courses: a first-semester abstract
algebra course, a topology course and a second-semester abstract algebra
course. In each case, course policy permitted some use of generative AI.
Drawing on survey responses and student interviews, we analyze how students
engaged with AI tools, their perceptions of generative AI's usefulness and
limitations, and what implications these perceptions hold for teaching
proof-based mathematics. We conclude by discussing future considerations for
integrating generative AI into proof-based mathematics instruction.

</details>


### [50] [Programmable Cognitive Bias in Social Agents](https://arxiv.org/abs/2509.13588)
*Xuan Liu,Haoyang Shang,Haojian Jin*

Main category: cs.AI

TL;DR: CoBRA是一个用于在基于LLM的社会模拟中系统化指定智能体行为的新工具包，通过显式编程认知偏见来解决传统自然语言描述方法的一致性问题。


<details>
  <summary>Details</summary>
Motivation: 传统方法使用隐式自然语言描述来指定智能体行为，但发现这种方法无法在不同模型间产生一致行为，且无法捕捉描述的细微差别。

Method: CoBRA包含两个组件：认知偏见指数（通过经典社会科学实验量化智能体反应）和行为调节引擎（将智能体行为与受控认知偏见对齐），通过经典社会科学实验来明确编程智能体的认知偏见。

Result: 评估结果表明，CoBRA能够以模型无关的方式精确编程社交智能体中展示的认知偏见。

Conclusion: CoBRA作为一个HCI工具包，提供了一种系统化、可控制的方法来编程LLM-based社交模拟中的智能体认知偏见行为。

Abstract: This paper introduces CoBRA, a novel toolkit for systematically specifying
agent behavior in LLM-based social simulation. We found that conventional
approaches that specify agent behaviors through implicit natural language
descriptions cannot yield consistent behaviors across models, and the produced
agent behaviors do not capture the nuances of the descriptions. In contrast,
CoBRA presents a new approach to program agents' cognitive biases explicitly,
by grounding agents' expected behaviors using classic social science
experiments. CoBRA has two components: (1) Cognitive Bias Index that measures
the cognitive bias of a social agent, by quantifying the agent's reactions in a
set of validated classical social science experiments; (2) Behavioral
Regulation Engine that aligns the agent's behavior to demonstrate controlled
cognitive bias. We evaluated CoBRA as an HCI toolkit through demonstration and
technical benchmarks. Our results suggest that CoBRA can precisely program the
cognitive bias demonstrated in a social agent in a model-agnostic manner.

</details>


### [51] [InfraMind: A Novel Exploration-based GUI Agentic Framework for Mission-critical Industrial Management](https://arxiv.org/abs/2509.13704)
*Liangtao Lin,Zhaomeng Zhu,Tianwei Zhang,Yonggang Wen*

Main category: cs.AI

TL;DR: InfraMind是一个专门为工业管理系统设计的探索式GUI代理框架，通过五个创新模块解决LLM-based GUI代理在工业管理中的五大挑战，显著提升了任务成功率和操作效率。


<details>
  <summary>Details</summary>
Motivation: 工业基础设施管理面临系统复杂性增加、多供应商集成和专家操作员短缺等挑战。传统RPA自动化灵活性有限且维护成本高，而现有的LLM-based GUI代理在工业管理应用中存在元素理解不熟悉、精度效率低、状态定位困难、部署约束和安全要求等五大问题。

Method: 提出InfraMind框架，包含五个核心模块：(1)基于系统搜索探索和虚拟机快照的自主GUI理解；(2)内存驱动规划确保高精度高效任务执行；(3)高级状态识别用于分层界面的鲁棒定位；(4)结构化知识蒸馏实现轻量模型高效部署；(5)多层安全机制保护敏感操作。

Result: 在开源和商业DCIM平台上的大量实验表明，该方法在任务成功率和操作效率方面持续优于现有框架。

Conclusion: InfraMind为工业管理自动化提供了一个严谨且可扩展的解决方案，有效解决了LLM-based GUI代理在工业环境中的关键挑战。

Abstract: Mission-critical industrial infrastructure, such as data centers,
increasingly depends on complex management software. Its operations, however,
pose significant challenges due to the escalating system complexity,
multi-vendor integration, and a shortage of expert operators. While Robotic
Process Automation (RPA) offers partial automation through handcrafted scripts,
it suffers from limited flexibility and high maintenance costs. Recent advances
in Large Language Model (LLM)-based graphical user interface (GUI) agents have
enabled more flexible automation, yet these general-purpose agents face five
critical challenges when applied to industrial management, including unfamiliar
element understanding, precision and efficiency, state localization, deployment
constraints, and safety requirements. To address these issues, we propose
InfraMind, a novel exploration-based GUI agentic framework specifically
tailored for industrial management systems. InfraMind integrates five
innovative modules to systematically resolve different challenges in industrial
management: (1) systematic search-based exploration with virtual machine
snapshots for autonomous understanding of complex GUIs; (2) memory-driven
planning to ensure high-precision and efficient task execution; (3) advanced
state identification for robust localization in hierarchical interfaces; (4)
structured knowledge distillation for efficient deployment with lightweight
models; and (5) comprehensive, multi-layered safety mechanisms to safeguard
sensitive operations. Extensive experiments on both open-source and commercial
DCIM platforms demonstrate that our approach consistently outperforms existing
frameworks in terms of task success rate and operational efficiency, providing
a rigorous and scalable solution for industrial management automation.

</details>


### [52] [See, Think, Act: Teaching Multimodal Agents to Effectively Interact with GUI by Identifying Toggles](https://arxiv.org/abs/2509.13615)
*Zongru Wu,Rui Mao,Zhiyuan Tian,Pengzhou Cheng,Tianjie Ju,Zheng Wu,Lingzhong Dong,Haiyue Sheng,Zhuosheng Zhang,Gongshen Liu*

Main category: cs.AI

TL;DR: 提出State-aware Reasoning (StaR)训练方法，解决多模态代理在GUI切换控制中的不可靠性问题，提升切换指令执行准确率30%以上


<details>
  <summary>Details</summary>
Motivation: 现有多模态代理在图形用户界面(GUI)切换控制中不可靠，特别是在当前状态与期望状态一致时无法正确处理切换指令

Method: 构建状态控制基准测试集，提出StaR训练方法，教导代理感知当前切换状态、分析指令期望状态并相应执行操作

Result: 在三个多模态代理上实验显示，StaR提升切换指令执行准确率超过30%，在三个公共基准测试中也提升了一般任务性能

Conclusion: StaR方法有效解决了GUI切换控制问题，在动态环境中展现出实际应用潜力

Abstract: The advent of multimodal agents facilitates effective interaction within
graphical user interface (GUI), especially in ubiquitous GUI control. However,
their inability to reliably execute toggle control instructions remains a key
bottleneck. To investigate this, we construct a state control benchmark with
binary toggle instructions from public datasets. Evaluations of existing agents
demonstrate their unreliability, particularly when the current toggle state
already matches the desired state. To address the challenge, we propose
State-aware Reasoning (StaR), a training method that teaches agents to perceive
the current toggle state, analyze the desired state from the instruction, and
act accordingly. Experiments on three multimodal agents demonstrate that StaR
can improve toggle instruction execution accuracy by over 30\%. Further
evaluations on three public benchmarks show that StaR also enhances general
task performance. Finally, evaluations on a dynamic environment highlight the
potential of StaR for real-world applications. Code, benchmark, and
StaR-enhanced agents are available at https://github.com/ZrW00/StaR.

</details>


### [53] [THOR: Tool-Integrated Hierarchical Optimization via RL for Mathematical Reasoning](https://arxiv.org/abs/2509.13761)
*Qikai Chang,Zhenrong Zhang,Pengfei Hu,Jiefeng Ma,Yicheng Pan,Jianshu Zhang,Jun Du,Quan Liu,Jianqing Gao*

Main category: cs.AI

TL;DR: THOR是一个通过强化学习实现工具集成层次优化的方法，解决了LLM在数学推理中高精度任务（如数值计算和符号操作）的挑战，通过多智能体生成高质量数据集、分层优化和自校正机制，在多个数学和代码基准上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在数学推理方面取得了显著进展，但在高精度任务（如数值计算和形式符号操作）方面仍然存在困难。现有方法在构建工具集成推理数据、进行细粒度优化和增强推理能力方面面临挑战。

Method: 提出THOR方法：1）TIRGen多智能体actor-critic流水线构建高质量工具集成推理数据集；2）分层强化学习策略，联合优化轨迹级问题解决和步骤级代码生成；3）自校正机制，利用工具反馈动态修正推理路径。

Result: 该方法在不同模型上表现出强大的泛化能力，在推理和非推理模型中都有效。在多个数学基准上达到相似规模模型的最先进性能，同时在代码基准上实现一致改进。

Conclusion: THOR通过工具集成和分层优化有效解决了LLM在高精度数学任务中的局限性，为增强模型数学推理能力提供了有前景的解决方案。

Abstract: Large Language Models (LLMs) have made remarkable progress in mathematical
reasoning, but still continue to struggle with high-precision tasks like
numerical computation and formal symbolic manipulation. Integrating external
tools has emerged as a promising approach to bridge this gap. Despite recent
advances, existing methods struggle with three key challenges: constructing
tool-integrated reasoning data, performing fine-grained optimization, and
enhancing inference. To overcome these limitations, we propose THOR
(Tool-Integrated Hierarchical Optimization via RL). First, we introduce TIRGen,
a multi-agent actor-critic-based pipeline for constructing high-quality
datasets of tool-integrated reasoning paths, aligning with the policy and
generalizing well across diverse models. Second, to perform fine-grained
hierarchical optimization, we introduce an RL strategy that jointly optimizes
for both trajectory-level problem solving and step-level code generation. This
is motivated by our key insight that the success of an intermediate tool call
is a strong predictor of the final answer's correctness. Finally, THOR
incorporates a self-correction mechanism that leverages immediate tool feedback
to dynamically revise erroneous reasoning paths during inference. Our approach
demonstrates strong generalization across diverse models, performing
effectively in both reasoning and non-reasoning models. It further achieves
state-of-the-art performance for models of a similar scale on multiple
mathematical benchmarks, while also delivering consistent improvements on code
benchmarks. Our code will be publicly available at
https://github.com/JingMog/THOR.

</details>


### [54] [MIRA: Empowering One-Touch AI Services on Smartphones with MLLM-based Instruction Recommendation](https://arxiv.org/abs/2509.13773)
*Zhipeng Bian,Jieming Zhu,Xuyang Xie,Quanyu Dai,Zhou Zhao,Zhenhua Dong*

Main category: cs.AI

TL;DR: MIRA是一个智能手机AI任务指令推荐框架，通过长按图像或文本来提供上下文相关的AI任务指令建议，使用MLLM和结构化推理提升推荐准确性。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI技术的快速发展，智能手机需要更直观的方式来访问预定义的AI服务，简化用户与设备的交互。

Method: 采用多模态大语言模型推荐管道进行结构化推理，结合模板增强推理机制和前缀树约束解码策略，确保指令建议的准确性和一致性。

Result: 在真实标注数据集和用户研究中，MIRA显著提高了指令推荐的准确性。

Conclusion: MIRA有潜力彻底改变用户在智能手机上与AI服务的交互方式，提供更无缝高效的体验。

Abstract: The rapid advancement of generative AI technologies is driving the
integration of diverse AI-powered services into smartphones, transforming how
users interact with their devices. To simplify access to predefined AI
services, this paper introduces MIRA, a pioneering framework for task
instruction recommendation that enables intuitive one-touch AI tasking on
smartphones. With MIRA, users can long-press on images or text objects to
receive contextually relevant instruction recommendations for executing AI
tasks. Our work introduces three key innovations: 1) A multimodal large
language model (MLLM)-based recommendation pipeline with structured reasoning
to extract key entities, infer user intent, and generate precise instructions;
2) A template-augmented reasoning mechanism that integrates high-level
reasoning templates, enhancing task inference accuracy; 3) A prefix-tree-based
constrained decoding strategy that restricts outputs to predefined instruction
candidates, ensuring coherent and intent-aligned suggestions. Through
evaluation using a real-world annotated datasets and a user study, MIRA has
demonstrated substantial improvements in the accuracy of instruction
recommendation. The encouraging results highlight MIRA's potential to
revolutionize the way users engage with AI services on their smartphones,
offering a more seamless and efficient experience.

</details>


### [55] [An Exhaustive DPLL Approach to Model Counting over Integer Linear Constraints with Simplification Techniques](https://arxiv.org/abs/2509.13880)
*Mingwei Zhang,Zhenhao Gu,Liangda Fang,Cunjing Ge,Ziliang Chen,Zhao-Rong Lai,Quanlong Guan*

Main category: cs.AI

TL;DR: 提出了一种基于DPLL架构的精确整数线性约束模型计数方法，集成了混合整数规划中的简化技术，在随机和应用基准测试中显著优于现有方法


<details>
  <summary>Details</summary>
Motivation: 整数线性约束模型计数(MCILC)是计算机科学、运筹学和优化领域的基础问题，许多应用都需要解决此类问题

Method: 基于详尽的DPLL架构设计精确方法，集成混合整数规划中的有效简化技术

Result: 在2840个随机基准测试中解决了1718个实例（现有最佳方法为1470个），在4131个应用基准测试中是唯一能解决所有实例的方法

Conclusion: 该方法在整数线性约束模型计数方面显著优于现有精确方法，特别是在应用实例中表现突出

Abstract: Linear constraints are one of the most fundamental constraints in fields such
as computer science, operations research and optimization. Many applications
reduce to the task of model counting over integer linear constraints (MCILC).
In this paper, we design an exact approach to MCILC based on an exhaustive DPLL
architecture. To improve the efficiency, we integrate several effective
simplification techniques from mixed integer programming into the architecture.
We compare our approach to state-of-the-art MCILC counters and propositional
model counters on 2840 random and 4131 application benchmarks. Experimental
results show that our approach significantly outperforms all exact methods in
random benchmarks solving 1718 instances while the state-of-the-art approach
only computes 1470 instances. In addition, our approach is the only approach to
solve all 4131 application instances.

</details>


### [56] [Exploring Major Transitions in the Evolution of Biological Cognition With Artificial Neural Networks](https://arxiv.org/abs/2509.13968)
*Konstantinos Voudouris,Andrew Barron,Marta Halina,Colin Klein,Matishalin Patel*

Main category: cs.AI

TL;DR: 该研究通过人工神经网络模型探讨信息流结构变化是否能带来认知表现的跃迁性变化，发现循环网络相比前馈网络在处理复杂语法时具有质的性能提升，并观察到训练难度形成的过渡障碍。


<details>
  <summary>Details</summary>
Motivation: 研究认知能力是否通过一系列主要跃迁进化而来，这些跃迁通过改变生物神经网络的信息流结构来根本性地改变信息处理方式。

Method: 使用理想化信息流模型和人工神经网络，比较前馈、循环和分层拓扑结构的网络性能，控制网络大小和资源，测试不同复杂度人工语法的学习能力。

Result: 循环网络相比前馈网络在处理输入类型上有质的扩展，在最复杂语法学习上表现出质的性能提升；循环网络的训练难度形成了过渡障碍和偶然不可逆性；分层网络在语法学习上并未优于非分层网络。

Conclusion: 某些信息流结构的变化确实能够产生认知表现的跃迁性变化，验证了认知进化可能通过主要跃迁实现的假设。

Abstract: Transitional accounts of evolution emphasise a few changes that shape what is
evolvable, with dramatic consequences for derived lineages. More recently it
has been proposed that cognition might also have evolved via a series of major
transitions that manipulate the structure of biological neural networks,
fundamentally changing the flow of information. We used idealised models of
information flow, artificial neural networks (ANNs), to evaluate whether
changes in information flow in a network can yield a transitional change in
cognitive performance. We compared networks with feed-forward, recurrent and
laminated topologies, and tested their performance learning artificial grammars
that differed in complexity, controlling for network size and resources. We
documented a qualitative expansion in the types of input that recurrent
networks can process compared to feed-forward networks, and a related
qualitative increase in performance for learning the most complex grammars. We
also noted how the difficulty in training recurrent networks poses a form of
transition barrier and contingent irreversibility -- other key features of
evolutionary transitions. Not all changes in network topology confer a
performance advantage in this task set. Laminated networks did not outperform
non-laminated networks in grammar learning. Overall, our findings show how some
changes in information flow can yield transitions in cognitive performance.

</details>


### [57] [CrowdAgent: Multi-Agent Managed Multi-Source Annotation System](https://arxiv.org/abs/2509.14030)
*Maosheng Qin,Renyu Zhu,Mingxuan Xia,Chenkai Chen,Zhen Zhu,Minmin Lin,Junbo Zhao,Lu Xu,Changjie Fan,Runze Wu,Haobo Wang*

Main category: cs.AI

TL;DR: CrowdAgent是一个多智能体系统，通过整合任务分配、数据标注和质量/成本管理，为LLM、SLM和人类专家提供端到端的协同标注流程控制。


<details>
  <summary>Details</summary>
Motivation: 当前NLP标注方法主要关注标注步骤本身，缺乏对多样化标注源（LLM、SLM、人类专家）的动态管理和复杂调度需求，以及质量-成本权衡的统一处理。

Method: 采用多智能体系统架构，实现任务分配、数据标注、质量/成本管理的端到端流程控制，通过理性任务分配使不同标注源在协同工作流中协同推进。

Result: 在六个多样化多模态分类任务上的广泛实验证明了CrowdAgent的有效性。

Conclusion: CrowdAgent填补了标注流程中动态管理和质量-成本权衡的空白，为高质量标注数据生成提供了端到端的解决方案。

Abstract: High-quality annotated data is a cornerstone of modern Natural Language
Processing (NLP). While recent methods begin to leverage diverse annotation
sources-including Large Language Models (LLMs), Small Language Models (SLMs),
and human experts-they often focus narrowly on the labeling step itself. A
critical gap remains in the holistic process control required to manage these
sources dynamically, addressing complex scheduling and quality-cost trade-offs
in a unified manner. Inspired by real-world crowdsourcing companies, we
introduce CrowdAgent, a multi-agent system that provides end-to-end process
control by integrating task assignment, data annotation, and quality/cost
management. It implements a novel methodology that rationally assigns tasks,
enabling LLMs, SLMs, and human experts to advance synergistically in a
collaborative annotation workflow. We demonstrate the effectiveness of
CrowdAgent through extensive experiments on six diverse multimodal
classification tasks. The source code and video demo are available at
https://github.com/QMMMS/CrowdAgent.

</details>


### [58] [Hierarchical Learning for Maze Navigation: Emergence of Mental Representations via Second-Order Learning](https://arxiv.org/abs/2509.14195)
*Shalima Binta Manir,Tim Oates*

Main category: cs.AI

TL;DR: 本文通过层次架构验证了二阶学习促进环境-认知同构性形成的假设，GCN作为一阶学习器预测最优路径，MLP控制器作为二阶学习器动态调整参数，在迷宫任务中表现出显著性能提升和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 研究心智表征（与环境结构同构的内部模型）对高级认知的重要性，但现有方法难以实证研究。理论假设二阶学习（调整一阶学习机制）能促进这种环境-认知同构性的形成。

Method: 提出层次架构：GCN作为一阶学习器直接映射节点特征到最优路径预测，MLP控制器作为二阶学习器在遇到结构新颖的迷宫环境时动态调整GCN参数。

Result: 当认知系统发展出与环境结构同构的内部心智地图时，二阶学习特别有效。定量和定性结果显示了在未见迷宫任务上的显著性能提升和强大泛化能力。

Conclusion: 研究为结构化心智表征在最大化二阶学习效果中的关键作用提供了实证支持，验证了二阶学习促进环境-认知同构性形成的理论假设。

Abstract: Mental representation, characterized by structured internal models mirroring
external environments, is fundamental to advanced cognition but remains
challenging to investigate empirically. Existing theory hypothesizes that
second-order learning -- learning mechanisms that adapt first-order learning
(i.e., learning about the task/domain) -- promotes the emergence of such
environment-cognition isomorphism. In this paper, we empirically validate this
hypothesis by proposing a hierarchical architecture comprising a Graph
Convolutional Network (GCN) as a first-order learner and an MLP controller as a
second-order learner. The GCN directly maps node-level features to predictions
of optimal navigation paths, while the MLP dynamically adapts the GCN's
parameters when confronting structurally novel maze environments. We
demonstrate that second-order learning is particularly effective when the
cognitive system develops an internal mental map structurally isomorphic to the
environment. Quantitative and qualitative results highlight significant
performance improvements and robust generalization on unseen maze tasks,
providing empirical support for the pivotal role of structured mental
representations in maximizing the effectiveness of second-order learning.

</details>
