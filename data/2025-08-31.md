<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 12]
- [cs.CR](#cs.CR) [Total: 20]
- [cs.AI](#cs.AI) [Total: 24]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Evaluating LLMs on microservice-based applications: how complex is your specification?](https://arxiv.org/abs/2508.20119)
*Daniel M. Yellin*

Main category: cs.SE

TL;DR: 评估LLM在真实世界微服务应用代码生成能力的研究，开发了难度评分标准和自动化测试框架，发现LLM在中等难度任务表现良好，但在高难度任务中表现很差。


<details>
  <summary>Details</summary>
Motivation: 评估LLM在真实世界问题（特别是微服务架构应用）中的代码生成能力，了解当前技术的局限性和挑战。

Method: 定义了微服务应用的标准规范模板，提出了难度评分指标，开发了自动化测试框架来评估LLM生成的代码，使用单元测试进行验证。

Result: 强LLM（如GPT-3o-mini）在中等难度规范上表现良好，但在高难度规范上表现很差，主要由于复杂的业务逻辑、外部服务集成、数据库操作和非功能性需求（如认证）等因素。

Conclusion: LLM在真实世界代码生成方面仍面临重大挑战，特别是在处理复杂业务逻辑和系统集成时，需要进一步研究来改进代码合成能力。

Abstract: In this paper we evaluate how far LLMs have advanced in generating code for
real-world problems. Specifically, we explore code synthesis for
microservice-based applications, a widely used architecture pattern. We define
a standard template for specifying these applications, and we propose a metric
for judging the difficulty level of a specification. The higher the score, the
more difficult it is to generate code for the specification. We develop a
framework to automate the process of testing LLM-synthesized code for a
microservice using unit tests. Our experimental results show that strong LLMs
(like GPT-3o-mini) do fairly well on medium difficulty specifications but do
very poorly on those of higher difficulty levels. This is due to more intricate
business logic, a greater use of external services, database integration and
inclusion of non-functional capabilities such as authentication. We analyzed
the errors in LLM-synthesized code and report on the key challenges LLMs face
in generating code for these specifications thereby suggesting future research
directions to improve code synthesis for real-world problems.

</details>


### [2] [Towards Better Correctness and Efficiency in Code Generation](https://arxiv.org/abs/2508.20124)
*Yunlong Feng,Yang Xu,Xiao Xu,Binyuan Hui,Junyang Lin*

Main category: cs.SE

TL;DR: 通过效率导向的强化学习框架，解决代码生成模型运行效率低的问题，在保持正确性的同时显著提升代码性能


<details>
  <summary>Details</summary>
Motivation: 当前代码大语言模型生成的代码运行效率往往较差，限制了在性能敏感场景中的实际应用

Method: 提出两阶段调优方法：(1)动态探索突破离线精调的静态数据限制 (2)错误不敏感强化学习法和高对比度效率信号 (3)从高正确性基线开始进行在线探索

Result: 在7B模型上实现代码正确性提升10.18%，运行效率提升7.75%，性能可比更大模型

Conclusion: 通过效率导向的强化学习框架，可以在不牺牲正确性的前提下显著提升代码生成的运行效率，解决代码生成模型在实际应用中的性能瓶颈

Abstract: While code large language models have demonstrated remarkable progress in
code generation, the generated code often exhibits poor runtime efficiency,
limiting its practical application in performance-sensitive scenarios. To
address this limitation, we propose an efficiency-oriented reinforcement
learning framework guided by a novel performance reward. Based on this
framework, we take a deeper dive into the code efficiency problem, identifying
then proposing methods to overcome key bottlenecks: (1) Dynamic exploration
overcomes the static data constraints of offline fine-tuning, enabling the
discovery of more efficient code implementations. (2) The error-insensitive
reinforcement learning method and high-contrast efficiency signals are crucial
for mitigating systematic errors and achieving effective optimization. (3)
Online exploration is most effective when starting from a high-correctness
baseline, as this allows for efficiency improvements without sacrificing
accuracy. With these discoveries, we finally propose a two-stage tuning method,
which achieves high and balanced performance across correctness and efficiency.
The results of experiments show the effectiveness of the method, which improves
code correctness by 10.18\% and runtime efficiency by 7.75\% on a 7B model,
achieving performance comparable to much larger model.

</details>


### [3] [Boosting Skeleton-Driven SMT Solver Fuzzing by Leveraging LLM to Produce Formula Generators](https://arxiv.org/abs/2508.20340)
*Maolin Sun,Yibiao Yang,Yuming Zhou*

Main category: cs.SE

TL;DR: Chimera是一个基于LLM的SMT求解器模糊测试框架，通过生成可重用的逻辑项生成器而非直接生成公式，解决了现有方法中语法无效和计算开销大的问题，在Z3和cvc5中发现了43个已确认的bug。


<details>
  <summary>Details</summary>
Motivation: SMT求解器在现代系统和编程语言研究中至关重要，但现有测试技术难以跟上求解器快速发展的特性。基于LLM的方法虽然显示出潜力，但存在生成的公式近一半语法无效以及迭代交互计算开销大的问题。

Method: Chimera框架使用LLM从文档中自动提取SMT理论的上下文无关文法(CFG)，并合成符合这些文法的可组合布尔项生成器。在模糊测试时，用LLM合成的生成器产生的项来填充现有公式的结构骨架，确保语法有效性同时促进语义多样性。

Result: 在Z3和cvc5两个主流SMT求解器上的实验表明，Chimera发现了43个已确认的bug，其中40个已被开发者修复。

Conclusion: Chimera通过从直接公式生成转向可重用项生成器的合成，有效解决了LLM辅助测试中的语法无效和计算开销问题，显著提高了SMT求解器测试的效率和效果。

Abstract: Satisfiability Modulo Theory (SMT) solvers are foundational to modern systems
and programming languages research, providing the foundation for tasks like
symbolic execution and automated verification. Because these solvers sit on the
critical path, their correctness is essential, and high-quality test formulas
are key to uncovering bugs. However, while prior testing techniques performed
well on earlier solver versions, they struggle to keep pace with rapidly
evolving features. Recent approaches based on Large Language Models (LLMs) show
promise in exploring advanced solver capabilities, but two obstacles remain:
nearly half of the generated formulas are syntactically invalid, and iterative
interactions with the LLMs introduce substantial computational overhead. In
this study, we present Chimera, a novel LLM-assisted fuzzing framework that
addresses both issues by shifting from direct formula generation to the
synthesis of reusable term (i.e., logical expression) generators. Particularly,
Chimera uses LLMs to (1) automatically extract context-free grammars (CFGs) for
SMT theories, including solver-specific extensions, from documentation, and (2)
synthesize composable Boolean term generators that adhere to these grammars.
During fuzzing, Chimera populates structural skeletons derived from existing
formulas with the terms iteratively produced by the LLM-synthesized generators.
This design ensures syntactic validity while promoting semantic diversity.
Notably, Chimera requires only one-time LLM interaction investment,
dramatically reducing runtime cost. We evaluated Chimera on two leading SMT
solvers: Z3 and cvc5. Our experiments show that Chimera has identified 43
confirmed bugs, 40 of which have already been fixed by developers.

</details>


### [4] [Adaptive Root Cause Localization for Microservice Systems with Multi-Agent Recursion-of-Thought](https://arxiv.org/abs/2508.20370)
*Lingzhe Zhang,Tong Jia,Kangjin Wang,Weijie Hong,Chiming Duan,Minghua He,Ying Li*

Main category: cs.SE

TL;DR: RCLAgent是一个基于多智能体递归思维框架的自适应根因定位方法，通过模拟人类SRE的递归性、多维扩展和跨模态推理特征，在微服务系统中实现高效的故障根因定位。


<details>
  <summary>Details</summary>
Motivation: 现代微服务系统日益复杂，故障频发，现有根因定位方法要么依赖预定义模式难以适应动态环境，要么缺乏可解释性。通过研究SRE的人工根因分析过程，发现其具有递归性、多维扩展和跨模态推理三个关键特征。

Method: 提出RCLAgent方法，采用多智能体递归思维框架，通过新颖的递归思维策略指导LLM推理过程，整合多个智能体和工具辅助分析来精确定位根因。

Result: 在多个公共数据集上的实验评估表明，RCLAgent仅需单次请求就能定位根因，性能优于需要聚合多次请求的最先进方法。

Conclusion: RCLAgent有效提升了复杂微服务环境中根因定位的效率和精确性，证明了其在系统可靠性保障方面的有效性。

Abstract: As contemporary microservice systems become increasingly popular and
complex-often comprising hundreds or even thousands of fine-grained,
interdependent subsystems-they are facing more frequent failures. Ensuring
system reliability thus demands accurate root cause localization. While traces
and metrics have proven to be effective data sources for this task, existing
methods either heavily rely on pre-defined schemas, which struggle to adapt to
evolving operational contexts, or lack interpretability in their reasoning
process, thereby leaving Site Reliability Engineers (SREs) confused. In this
paper, we conduct a comprehensive study on how SREs localize the root cause of
failures, drawing insights from multiple professional SREs across different
organizations. Our investigation reveals that human root cause analysis
exhibits three key characteristics: recursiveness, multi-dimensional expansion,
and cross-modal reasoning. Motivated by these findings, we introduce RCLAgent,
an adaptive root cause localization method for microservice systems that
leverages a multi-agent recursion-of-thought framework. RCLAgent employs a
novel recursion-of-thought strategy to guide the LLM's reasoning process,
effectively integrating data from multiple agents and tool-assisted analysis to
accurately pinpoint the root cause. Experimental evaluations on various public
datasets demonstrate that RCLAgent achieves superior performance by localizing
the root cause using only a single request-outperforming state-of-the-art
methods that depend on aggregating multiple requests. These results underscore
the effectiveness of RCLAgent in enhancing the efficiency and precision of root
cause localization in complex microservice environments.

</details>


### [5] [AI and Agile Software Development: A Research Roadmap from the XP2025 Workshop](https://arxiv.org/abs/2508.20563)
*Zheying Zhang,Tomas Herda,Victoria Pichler,Pekka Abrahamsson,Geir K. Hanssen,Joshua Kerievsky,Alex Polyakov,Mohit Chandna,Marius Irgens,Kai-Kristian Kemell,Ayman Asad Khan,Crystal Kwok,Evan Leybourn,Munish Malik,Dorota Mleczko,Morteza Moalagh,Christopher Morales,Yuliia Pieskova,Daniel Planötscher,Mika Saari,Anastasiia Tkalich,Karl Josef Gstettner,Xiaofeng Wang*

Main category: cs.SE

TL;DR: XP2025研讨会总结了GenAI与敏捷开发结合的关键挑战和机遇，包括工具碎片化、治理、数据质量和AI素养等痛点，并制定了多主题研究路线图


<details>
  <summary>Details</summary>
Motivation: 解决生成式人工智能与敏捷软件开发结合时的具体挑战，促进学术界和工业界的跨学科合作，推动GenAI在敏捷实践中的负责任、以人为本的整合

Method: 通过结构化、互动式分组讨论，汇集30多名跨学科学术研究人员和行业从业者，分析共同痛点并揭示根本原因

Result: 识别了工具碎片化、治理、数据质量、AI素养和提示工程技能差距等关键问题，并共同创建了包含短期可实施行动和长期愿景研究方向的多主题研究路线图

Conclusion: 研讨会成功制定了指导未来研究的统一议程，旨在推动GenAI与敏捷开发的有效整合，强调了负责任和以人为中心的方法的重要性

Abstract: This paper synthesizes the key findings from a full-day XP2025 workshop on
"AI and Agile: From Frustration to Success", held in Brugg-Windisch,
Switzerland. The workshop brought together over 30 interdisciplinary academic
researchers and industry practitioners to tackle the concrete challenges and
emerging opportunities at the intersection of Generative Artificial
Intelligence (GenAI) and agile software development. Through structured,
interactive breakout sessions, participants identified shared pain points like
tool fragmentation, governance, data quality, and critical skills gaps in AI
literacy and prompt engineering. These issues were further analyzed, revealing
underlying causes and cross-cutting concerns. The workshop concluded by
collaboratively co-creating a multi-thematic research roadmap, articulating
both short-term, implementable actions and visionary, long-term research
directions. This cohesive agenda aims to guide future investigation and drive
the responsible, human-centered integration of GenAI into agile practices.

</details>


### [6] [Rethinking Testing for LLM Applications: Characteristics, Challenges, and a Lightweight Interaction Protocol](https://arxiv.org/abs/2508.20737)
*Wei Ma,Yixiao Yang,Qiang Hu,Shi Ying,Zhi Jin,Bo Du,Zhenchang Xing,Tianlin Li,Junjie Shi,Yang Liu,Linxiao Jiang*

Main category: cs.SE

TL;DR: 本文提出了大语言模型应用的三层测试架构，分析了传统测试方法的适用性，并提出四种协同策略和一种新的测试协议AICL来解决LLM应用质量保证的核心挑战。


<details>
  <summary>Details</summary>
Motivation: 大语言模型应用从简单文本生成发展为复杂软件系统，其内在的非确定性、动态性和上下文依赖性给质量保证带来根本挑战。需要建立适应LLM特性的测试框架和方法。

Method: 将LLM应用分解为三层架构：系统壳层、提示组织层和LLM推理核心。分析传统测试方法在各层的适用性，对比软件工程和AI安全分析技术的差异，提出四种协同策略（保留、转换、集成、运行时）和闭环可信质量保证框架。

Result: 识别了四个根本性差异和6个核心挑战，建立了适合LLM应用测试的理论基础。提出了协议AICL（Agent Interaction Communication Language），具有测试导向特性且容易集成到现有机器人框架中。

Conclusion: 本文为LLM应用测试提供了系统化的架构分析和实践指南，通过协同策略和新的测试协议，有助于解决LLM应用质量保证的核心挑战，推动测试标准化和工具化。

Abstract: Applications of Large Language Models~(LLMs) have evolved from simple text
generators into complex software systems that integrate retrieval augmentation,
tool invocation, and multi-turn interactions. Their inherent non-determinism,
dynamism, and context dependence pose fundamental challenges for quality
assurance. This paper decomposes LLM applications into a three-layer
architecture: \textbf{\textit{System Shell Layer}}, \textbf{\textit{Prompt
Orchestration Layer}}, and \textbf{\textit{LLM Inference Core}}. We then assess
the applicability of traditional software testing methods in each layer:
directly applicable at the shell layer, requiring semantic reinterpretation at
the orchestration layer, and necessitating paradigm shifts at the inference
core. A comparative analysis of Testing AI methods from the software
engineering community and safety analysis techniques from the AI community
reveals structural disconnects in testing unit abstraction, evaluation metrics,
and lifecycle management. We identify four fundamental differences that
underlie 6 core challenges. To address these, we propose four types of
collaborative strategies (\emph{Retain}, \emph{Translate}, \emph{Integrate},
and \emph{Runtime}) and explore a closed-loop, trustworthy quality assurance
framework that combines pre-deployment validation with runtime monitoring.
Based on these strategies, we offer practical guidance and a protocol proposal
to support the standardization and tooling of LLM application testing. We
propose a protocol \textbf{\textit{Agent Interaction Communication Language}}
(AICL) that is used to communicate between AI agents. AICL has the
test-oriented features and is easily integrated in the current agent framework.

</details>


### [7] [From Law to Gherkin: A Human-Centred Quasi-Experiment on the Quality of LLM-Generated Behavioural Specifications from Food-Safety Regulations](https://arxiv.org/abs/2508.20744)
*Shabnam Hassani,Mehrdad Sabetzadeh,Daniel Amyot*

Main category: cs.SE

TL;DR: LLMs能够从法律文本中生成高质量的Gherkin行为规范，显著减少人工工作量，为软件合规性提供结构化制品


<details>
  <summary>Details</summary>
Motivation: 法律文本采用技术中立语言编写，工程师手动创建合规制品（如需求和验收标准）劳动密集、易出错且需要专业知识，需要自动化解决方案

Method: 采用准实验设计，10名参与者评估Claude和Llama从食品安全法规生成的60个Gherkin规范，每个规范由2人评估，共120次评估，从相关性、清晰度、完整性、单一性和时间节省五个标准进行评价

Result: 相关性75%最高评分，清晰度90%最高，完整性75%最高，单一性82%最高，时间节省68%最高。Llama在清晰度、完整性和时间节省方面略优，Claude在单一性方面更强。存在幻觉和遗漏但确认了实用性

Conclusion: LLMs能够从法律文本生成高质量的Gherkin规范，减少人工工作量，为实施、保证和测试生成提供有用的结构化制品

Abstract: Context: Laws and regulations increasingly affect software design and quality
assurance, but legal texts are written in technology-neutral language. This
creates challenges for engineers who must develop compliance artifacts such as
requirements and acceptance criteria. Manual creation is labor-intensive,
error-prone, and requires domain expertise. Advances in Generative AI (GenAI),
especially Large Language Models (LLMs), offer a way to automate deriving such
artifacts.
  Objective: We present the first systematic human-subject study of LLMs'
ability to derive behavioral specifications from legal texts using a
quasi-experimental design. These specifications translate legal requirements
into a developer-friendly form.
  Methods: Ten participants evaluated specifications generated from food-safety
regulations by Claude and Llama. Using Gherkin, a structured BDD language, 60
specifications were produced. Each participant assessed 12 across five
criteria: Relevance, Clarity, Completeness, Singularity, and Time Savings. Each
specification was reviewed by two participants, yielding 120 assessments.
  Results: For Relevance, 75% of ratings were highest and 20% second-highest.
Clarity reached 90% highest. Completeness: 75% highest, 19% second.
Singularity: 82% highest, 12% second. Time Savings: 68% highest, 24% second. No
lowest ratings occurred. Mann-Whitney U tests showed no significant differences
across participants or models. Llama slightly outperformed Claude in Clarity,
Completeness, and Time Savings, while Claude was stronger in Singularity.
Feedback noted hallucinations and omissions but confirmed the utility of the
specifications.
  Conclusion: LLMs can generate high-quality Gherkin specifications from legal
texts, reducing manual effort and providing structured artifacts useful for
implementation, assurance, and test generation.

</details>


### [8] [Towards an Architectural Perspective for Sustainability: Bundle the Needs from Industry](https://arxiv.org/abs/2508.20774)
*Markus Funke,Patricia Lago*

Main category: cs.SE

TL;DR: 提出了一个可持续性架构视角愿景，通过系统文献综述和专家焦点小组研究，为软件架构师提供结构化指导来处理可持续性质量属性。


<details>
  <summary>Details</summary>
Motivation: 软件密集型系统中可持续性日益重要，但架构师在设计阶段缺乏有效的结构化指导来处理这一新兴质量属性。

Method: 采用雪球抽样方法分析重要文献，并进行专家焦点小组研究，制定可持续性架构视角的概念框架。

Result: 研究确认了不同视角元素在实际中的相关性，并强调了满足工业需求的可持续性视角的构建意义。

Conclusion: 可持续性架构视角为软件架构师提供了独立于架构框架和行业背景的实用工具，能够有效处理可持续性质量属性。

Abstract: Sustainability is increasingly recognized as an emerging quality property in
software-intensive systems, yet architects lack structured guidance to address
it effectively throughout the software design phase. Architectural
perspectives-an architectural knowledge artifact composed of concerns,
activities, tactics, pitfalls, and checklists-offer a promising approach to
tackle such emerging quality properties across architectural views and are also
independent of architecture frameworks and industry contexts. In this paper, we
present a sustainability perspective vision, i.e., a revised notion of
architectural perspective meant to be filled with its own elements to target
sustainability concerns. We formulate our sustainability perspective vision
through evidence from applying snowballing to seminal literature and from
conducting a focus group with experts in the field. Our findings confirm the
relevance of the different perspective elements in practice and highlight
implications for shaping a sustainability perspective that meets industrial
needs.

</details>


### [9] [Automated Test Oracles for Flaky Cyber-Physical System Simulators: Approach and Evaluation](https://arxiv.org/abs/2508.20902)
*Baharin A. Jodat,Khouloud Gaaloul,Mehrdad Sabetzadeh,Shiva Nejati*

Main category: cs.SE

TL;DR: 提出基于断言的测试预言方法，使用遗传编程和Ochiai公式生成逻辑和算术谓词，无需系统执行即可预测测试结果，在多个CPS领域验证了高准确性和抗抖动性


<details>
  <summary>Details</summary>
Motivation: CPS仿真测试成本高且存在抖动问题，需要无需执行的自动化测试预言来降低测试成本，同时要求可解释性和抗抖动性

Method: 提出基于断言的测试预言作为逻辑和算术谓词集合，使用遗传编程（GP）和决策树/规则两种方法生成，其中GP采用Ochiai、Tarantula和Naish等SBFL排名公式作为适应度函数

Result: 使用GP和Ochiai生成的测试预言准确率显著高于其他方法，即使在系统抖动情况下仍保持准确性优势，在四个具有抖动行为的网络和自动驾驶系统中平均准确率变化仅为4%

Conclusion: 基于GP和Ochiai的断言测试预言方法能够有效降低CPS测试成本，提供高准确性、可解释性和抗抖动性的测试预言解决方案

Abstract: Simulation-based testing of cyber-physical systems (CPS) is costly due to the
time-consuming execution of CPS simulators. In addition, CPS simulators may be
flaky, leading to inconsistent test outcomes and requiring repeated test
re-execution for reliable test verdicts. Automated test oracles that do not
require system execution are therefore crucial for reducing testing costs.
Ideally, such test oracles should be interpretable to facilitate human
understanding of test verdicts, and they must be robust against the potential
flakiness of CPS simulators. In this article, we propose assertion-based test
oracles for CPS as sets of logical and arithmetic predicates defined over the
inputs of the system under test. Given a test input, our assertion-based test
oracle determines, without requiring test execution, whether the test passes,
fails, or if the oracle is inconclusive in predicting a verdict. We describe
two methods for generating assertion-based test oracles: one using genetic
programming~(GP) that employs well-known spectrum-based fault localization
(SBFL) ranking formulas, namely Ochiai, Tarantula, and Naish, as fitness
functions; and the other using decision trees (DT) and decision rules (DR). We
evaluate our assertion-based test oracles through case studies in the domains
of aerospace, networking and autonomous driving. We show that test oracles
generated using GP with Ochiai are significantly more accurate than those
obtained using GP with Tarantula and Naish or using DT or DR. Moreover, this
accuracy advantage remains even when accounting for the flakiness of the system
under test. We further show that the assertion-based test oracles generated by
GP with Ochiai are robust against flakiness with only 4% average variation in
their accuracy results across four different network and autonomous driving
systems with flaky behaviours.

</details>


### [10] [Deep Learning Based Concurrency Bug Detection and Localization](https://arxiv.org/abs/2508.20911)
*Zuocheng Feng,Kaiwen Zhang,Miaomiao Wang,Yiming Cheng,Yuandao Cai,Xiaofeng Li,Guanjun Liu*

Main category: cs.SE

TL;DR: 提出了一种基于预训练模型和异构图神经网络的并发bug检测与定位方法，通过构建专门的并发bug数据集和并发感知代码属性图来提升检测效果。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在并发bug检测方面存在三个主要问题：缺乏专门的大规模数据集、并发语义表示不足、二进制分类无法提供细粒度调试信息。

Method: 构建专用并发bug数据集，集成预训练模型与异构图神经网络，使用新的并发感知代码属性图(CCPG)表征并发语义，并采用SubgraphX方法进行bug精确定位。

Result: 在多种评估设置下，相比最先进方法，平均准确率和精确度提升10%，召回率提升26%。

Conclusion: 该方法通过专门的并发语义表征和精确定位能力，显著提升了并发bug检测和定位的效果，为并发程序调试提供了有效支持。

Abstract: Concurrency bugs, caused by improper synchronization of shared resources in
multi-threaded or distributed systems, are notoriously hard to detect and thus
compromise software reliability and security. The existing deep learning
methods face three main limitations. First, there is an absence of large and
dedicated datasets of diverse concurrency bugs for them. Second, they lack
sufficient representation of concurrency semantics. Third, binary
classification results fail to provide finer-grained debug information such as
precise bug lines. To address these problems, we propose a novel method for
effective concurrency bug detection as well as localization. We construct a
dedicated concurrency bug dataset to facilitate model training and evaluation.
We then integrate a pre-trained model with a heterogeneous graph neural network
(GNN), by incorporating a new Concurrency-Aware Code Property Graph (CCPG) that
concisely and effectively characterizes concurrency semantics. To further
facilitate debugging, we employ SubgraphX, a GNN-based interpretability method,
which explores the graphs to precisely localize concurrency bugs, mapping them
to specific lines of source code. On average, our method demonstrates an
improvement of 10\% in accuracy and precision and 26\% in recall compared to
state-of-the-art methods across diverse evaluation settings.

</details>


### [11] [ConfLogger: Enhance Systems' Configuration Diagnosability through Configuration Logging](https://arxiv.org/abs/2508.20977)
*Shiwen Shan,Yintong Huo,Yuxin Su,Zhining Wang,Dan Li,Zibin Zheng*

Main category: cs.SE

TL;DR: 提出ConfLogger工具，通过配置感知的静态污点分析和LLM日志生成来增强软件配置可诊断性，在8个流行系统中验证了有效性，能实现100%错误定位准确率和显著提升诊断效率


<details>
  <summary>Details</summary>
Motivation: 现代可配置系统虽然提供灵活性，但也带来了配置相关问题。现有诊断方法主要关注故障后分析，但缺乏对软件是否提供足够故障信息进行诊断的关注

Method: 1) 通过追踪整个项目中的配置相关数据流来识别配置敏感代码段；2) 通过分析配置代码上下文生成诊断日志语句；结合配置感知静态污点分析和基于LLM的日志生成

Result: 在30个静默配置错误场景中实现100%错误定位准确率，80%可通过暴露的显式配置信息直接解决；比基线LLM日志器覆盖率高12-30%，在变量日志方面精度高8.6%，召回率高79.3%，F1高26.2%；用户研究显示诊断时间加快1.25倍，故障排除准确率提高251.4%

Conclusion: ConfLogger通过增强配置日志实践有效提升了软件配置可诊断性，为配置相关问题的诊断提供了强有力的工具支持

Abstract: Modern configurable systems offer customization via intricate configuration
spaces, yet such flexibility introduces pervasive configuration-related issues
such as misconfigurations and latent softwarebugs. Existing diagnosability
supports focus on post-failure analysis of software behavior to identify
configuration issues, but none of these approaches look into whether the
software clue sufficient failure information for diagnosis. To fill in the
blank, we propose the idea of configuration logging to enhance existing logging
practices at the source code level. We develop ConfLogger, the first tool that
unifies configuration-aware static taint analysis with LLM-based log generation
to enhance software configuration diagnosability. Specifically, our method 1)
identifies configuration-sensitive code segments by tracing
configuration-related data flow in the whole project, and 2) generates
diagnostic log statements by analyzing configuration code contexts. Evaluation
results on eight popular software systems demonstrate the effectiveness of
ConfLogger to enhance configuration diagnosability. Specifically,
ConfLogger-enhanced logs successfully aid a log-based misconfiguration
diagnosis tool to achieve 100% accuracy on error localization in 30 silent
misconfiguration scenarios, with 80% directly resolvable through explicit
configuration information exposed. In addition, ConfLogger achieves 74%
coverage of existing logging points, outperforming baseline LLM-based loggers
by 12% and 30%. It also gains 8.6% higher in precision, 79.3% higher in recall,
and 26.2% higher in F1 compared to the state-of-the-art baseline in terms of
variable logging while also augmenting diagnostic value. A controlled user
study on 22 cases further validated its utility, speeding up diagnostic time by
1.25x and improving troubleshooting accuracy by 251.4%.

</details>


### [12] [Dynamics of Gender Bias in Software Engineering](https://arxiv.org/abs/2508.21050)
*Thomas J. Misa*

Main category: cs.SE

TL;DR: 该论文分析了软件工程领域的性别偏见问题，包括对ICSE会议作者性别组成的定量分析，发现了长达12年的统计上显著的性别排除现象。


<details>
  <summary>Details</summary>
Motivation: 软件工程领域根深蒂固地嵌入工程和计算机科学两大领域，而这两个领域都存在着性别偏见。研究者想要探索软件工程领域是否也存在相似的性别偏见问题。

Method: 采用了三种方法：1）调查软件工程的起源和对工程专业性的长期关注，分析五位领导者；2）审视该领域最近对性别问题的关注；3）定量分析国际软件工程会议（ICSE）1976-2010年期间女性研究作者的参与情况。

Result: 研究发现在ICSE会议中存在长达12年的统计上显著的性别排除现象，证明了软件工程领域确实存在性别偏见问题。

Conclusion: 软件工程领域存在着系统性的性别偏见，需要提出相应的政策建议来解决这些问题，以促进计算机领域的性别平等。

Abstract: The field of software engineering is embedded in both engineering and
computer science, and may embody gender biases endemic to both. This paper
surveys software engineering's origins and its long-running attention to
engineering professionalism, profiling five leaders; it then examines the
field's recent attention to gender issues and gender bias. It next
quantitatively analyzes women's participation as research authors in the
field's leading International Conference of Software Engineering (1976-2010),
finding a dozen years with statistically significant gender exclusion. Policy
dimensions of research on gender bias in computing are suggested.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [13] [AI Propaganda factories with language models](https://arxiv.org/abs/2508.20186)
*Lukasz Olejnik*

Main category: cs.CR

TL;DR: 小型语言模型已能在普通硬件上实现端到端的AI影响力操作，自动生成连贯的政治宣传内容，无需人工评估。研究发现人物设定比模型本身更重要，且对抗性回复会强化意识形态立场和极端内容。


<details>
  <summary>Details</summary>
Motivation: 研究AI驱动的自动化影响力操作的可行性和特征，特别是小型语言模型在政治宣传中的应用潜力，以及相关的防御策略需求。

Method: 使用小型语言模型生成人物驱动的政治信息，通过自动化评估而非人工评分来分析内容特征和行为模式。

Result: 发现两个关键行为模式：1）人物设定比模型身份更能解释行为；2）对抗性回复会增强意识形态坚持和极端内容产生。证明完全自动化的影响力内容生产已经可行。

Conclusion: 自动化影响力操作已触手可及，防御策略应从限制模型访问转向对话中心的检测和破坏协调基础设施。操作的一致性反而提供了检测特征。

Abstract: AI-powered influence operations can now be executed end-to-end on commodity
hardware. We show that small language models produce coherent, persona-driven
political messaging and can be evaluated automatically without human raters.
Two behavioural findings emerge. First, persona-over-model: persona design
explains behaviour more than model identity. Second, engagement as a stressor:
when replies must counter-arguments, ideological adherence strengthens and the
prevalence of extreme content increases. We demonstrate that fully automated
influence-content production is within reach of both large and small actors.
Consequently, defence should shift from restricting model access towards
conversation-centric detection and disruption of campaigns and coordination
infrastructure. Paradoxically, the very consistency that enables these
operations also provides a detection signature.

</details>


### [14] [FlowMalTrans: Unsupervised Binary Code Translation for Malware Detection Using Flow-Adapter Architecture](https://arxiv.org/abs/2508.20212)
*Minghao Hu,Junzhe Wang,Weisen Zhao,Qiang Zeng,Lannan Luo*

Main category: cs.CR

TL;DR: 提出基于神经机器翻译和标准化流的跨架构恶意软件检测方法，通过将其他ISA的恶意软件翻译到X86-64架构，利用单一架构训练模型实现多架构检测


<details>
  <summary>Details</summary>
Motivation: 物联网设备面临多架构恶意软件威胁，但为每个指令集架构收集和标注足够恶意软件样本成本高昂，需要减少数据收集负担

Method: 结合神经机器翻译(NMT)和标准化流(NFs)技术，将目标ISA的恶意软件翻译到拥有充足样本的X86-64架构，然后使用在X86-64上训练的模型进行检测

Result: 该方法能够实现跨架构恶意软件检测，显著减少了为每个ISA单独收集和标注样本的工作量

Conclusion: 通过架构翻译技术，可以在单一架构上训练模型并应用于多架构恶意软件检测，有效解决了数据收集瓶颈问题

Abstract: Applying deep learning to malware detection has drawn great attention due to
its notable performance. With the increasing prevalence of cyberattacks
targeting IoT devices, there is a parallel rise in the development of malware
across various Instruction Set Architectures (ISAs). It is thus important to
extend malware detection capacity to multiple ISAs. However, training a deep
learning-based malware detection model usually requires a large number of
labeled malware samples. The process of collecting and labeling sufficient
malware samples to build datasets for each ISA is labor-intensive and
time-consuming. To reduce the burden of data collection, we propose to leverage
the ideas of Neural Machine Translation (NMT) and Normalizing Flows (NFs) for
malware detection. Specifically, when dealing with malware in a certain ISA, we
translate it to an ISA with sufficient malware samples (like X86-64). This
allows us to apply a model trained on one ISA to analyze malware from another
ISA. Our approach reduces the data collection effort by enabling malware
detection across multiple ISAs using a model trained on a single ISA.

</details>


### [15] [Robustness Assessment and Enhancement of Text Watermarking for Google's SynthID](https://arxiv.org/abs/2508.20228)
*Xia Han,Qi Li,Jianbing Ni,Mohammad Zulkernine*

Main category: cs.CR

TL;DR: SynGuard是一个混合水印框架，结合语义信息检索和SynthID-Text的概率水印机制，在词汇和语义层面同时嵌入水印，显著提升了对改写、复制粘贴和回译等攻击的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM水印方法如SynthID-Text在保持语义的攻击面前存在脆弱性，容易被改写、复制粘贴修改和回译等攻击破坏水印可检测性。

Method: 提出SynGuard混合框架，将语义信息检索(SIR)的语义对齐能力与SynthID-Text的概率水印机制相结合，在词汇和语义两个层面联合嵌入水印。

Result: 在多种攻击场景下的实验表明，SynGuard相比SynthID-Text平均提高了11.1%的F1分数水印恢复能力。

Conclusion: 语义感知水印在抵抗现实世界篡改方面具有有效性，证明了在语义层面增强水印鲁棒性的重要性。

Abstract: Recent advances in LLM watermarking methods such as SynthID-Text by Google
DeepMind offer promising solutions for tracing the provenance of AI-generated
text. However, our robustness assessment reveals that SynthID-Text is
vulnerable to meaning-preserving attacks, such as paraphrasing, copy-paste
modifications, and back-translation, which can significantly degrade watermark
detectability. To address these limitations, we propose SynGuard, a hybrid
framework that combines the semantic alignment strength of Semantic Information
Retrieval (SIR) with the probabilistic watermarking mechanism of SynthID-Text.
Our approach jointly embeds watermarks at both lexical and semantic levels,
enabling robust provenance tracking while preserving the original meaning.
Experimental results across multiple attack scenarios show that SynGuard
improves watermark recovery by an average of 11.1\% in F1 score compared to
SynthID-Text. These findings demonstrate the effectiveness of semantic-aware
watermarking in resisting real-world tampering. All code, datasets, and
evaluation scripts are publicly available at:
https://github.com/githshine/SynGuard.

</details>


### [16] [Network-Level Prompt and Trait Leakage in Local Research Agents](https://arxiv.org/abs/2508.20282)
*Hyejun Jeong,Mohammadreze Teymoorianfard,Abhinav Kumar,Amir Houmansadr,Eugene Badasarian*

Main category: cs.CR

TL;DR: 研究表明，基于语言模型的Web和研究代理（WRAs）存在网络元数据泄露风险，被动网络攻击者可通过访问IP地址和时间模式推断用户提示和特征，恢复73%的功能性内容和19/32的潜在特征。


<details>
  <summary>Details</summary>
Motivation: WRAs被组织和个人本地部署用于隐私、法律或财务目的，但其网络行为模式（访问70-140个域且有可识别的时间相关性）使其容易受到被动网络对手的推理攻击，这引发了隐私泄露的担忧。

Method: 构建基于用户搜索查询和合成角色生成查询的WRA跟踪数据集，定义OBELS行为指标评估原始提示与推断提示的相似性，并在多会话设置下扩展攻击，测试部分可观测性和噪声条件下的有效性。

Result: 攻击成功恢复超过73%的用户提示功能性和领域知识，在多会话设置中准确恢复19/32的潜在特征，即使在部分可观测和噪声条件下仍保持有效。缓解策略平均降低29%的攻击效果且对实用性影响可忽略。

Conclusion: WRAs存在严重的隐私泄露风险，网络元数据足以暴露敏感信息。需要实施限制域多样性或混淆跟踪的缓解措施来保护用户隐私，同时保持系统实用性。

Abstract: We show that Web and Research Agents (WRAs) -- language model-based systems
that investigate complex topics on the Internet -- are vulnerable to inference
attacks by passive network adversaries such as ISPs. These agents could be
deployed \emph{locally} by organizations and individuals for privacy, legal, or
financial purposes. Unlike sporadic web browsing by humans, WRAs visit
$70{-}140$ domains with distinguishable timing correlations, enabling unique
fingerprinting attacks.
  Specifically, we demonstrate a novel prompt and user trait leakage attack
against WRAs that only leverages their network-level metadata (i.e., visited IP
addresses and their timings). We start by building a new dataset of WRA traces
based on user search queries and queries generated by synthetic personas. We
define a behavioral metric (called OBELS) to comprehensively assess similarity
between original and inferred prompts, showing that our attack recovers over
73\% of the functional and domain knowledge of user prompts. Extending to a
multi-session setting, we recover up to 19 of 32 latent traits with high
accuracy. Our attack remains effective under partial observability and noisy
conditions. Finally, we discuss mitigation strategies that constrain domain
diversity or obfuscate traces, showing negligible utility impact while reducing
attack effectiveness by an average of 29\%.

</details>


### [17] [Surveying the Operational Cybersecurity and Supply Chain Threat Landscape when Developing and Deploying AI Systems](https://arxiv.org/abs/2508.20307)
*Michael R Smith,Joe Ingram*

Main category: cs.CR

TL;DR: AI技术的发展带来了新的网络安全风险和攻击面，传统安全评估方法无法有效应对，需要专门的安全框架来保护AI系统。


<details>
  <summary>Details</summary>
Motivation: AI技术的快速发展引入了新的攻击面和目标，传统网络安全方法无法有效处理这些新兴威胁，需要提高对AI系统安全风险的认识。

Method: 分析AI生命周期中的运营安全和供应链风险，探讨之前的利用案例，并基于实践经验提供见解。

Result: 识别了AI系统面临的独特安全挑战，包括攻击者对AI输出的操控以达到系统效果目标，如降低系统性能、产生假正确统次或模型准确性退化等。

Conclusion: 组织需要开发专门的安全框架来应对AI驱动环境中的演化威胁，通过理解这些风险来更好地保护AI系统的可靠性和弹性。

Abstract: The rise of AI has transformed the software and hardware landscape, enabling
powerful capabilities through specialized infrastructures, large-scale data
storage, and advanced hardware. However, these innovations introduce unique
attack surfaces and objectives which traditional cybersecurity assessments
often overlook. Cyber attackers are shifting their objectives from conventional
goals like privilege escalation and network pivoting to manipulating AI outputs
to achieve desired system effects, such as slowing system performance, flooding
outputs with false positives, or degrading model accuracy. This paper serves to
raise awareness of the novel cyber threats that are introduced when
incorporating AI into a software system. We explore the operational
cybersecurity and supply chain risks across the AI lifecycle, emphasizing the
need for tailored security frameworks to address evolving threats in the
AI-driven landscape. We highlight previous exploitations and provide insights
from working in this area. By understanding these risks, organizations can
better protect AI systems and ensure their reliability and resilience.

</details>


### [18] [MindGuard: Tracking, Detecting, and Attributing MCP Tool Poisoning Attack via Decision Dependence Graph](https://arxiv.org/abs/2508.20412)
*Zhiqiang Wang,Junyang Zhang,Guanquan Shi,HaoRan Cheng,Yunhao Yao,Kaiwen Guo,Haohua Du,Xiang-Yang Li*

Main category: cs.CR

TL;DR: MindGuard是一个针对LLM代理的决策级防护系统，通过注意力机制追踪工具调用决策，有效检测和溯源工具投毒攻击。


<details>
  <summary>Details</summary>
Motivation: 随着Model Context Protocol(MCP)的广泛采用，工具投毒攻击(TPA)成为新威胁，现有基于行为分析的防御方法对此无效，因为投毒工具可能不被执行而无法留下行为痕迹。

Method: 利用LLM注意力机制与工具调用决策的强相关性，构建决策依赖图(DDG)来建模推理过程，设计基于图的异常分析机制来检测和溯源TPA攻击。

Result: 在真实数据集上的实验显示，MindGuard达到94%-99%的平均检测精度，95%-100%的溯源准确率，处理时间低于1秒且无额外token成本。

Conclusion: MindGuard为LLM代理提供了有效的决策级防护，DDG作为经典程序依赖图的适配，为在决策层面应用传统安全策略奠定了基础。

Abstract: The Model Context Protocol (MCP) is increasingly adopted to standardize the
interaction between LLM agents and external tools. However, this trend
introduces a new threat: Tool Poisoning Attacks (TPA), where tool metadata is
poisoned to induce the agent to perform unauthorized operations. Existing
defenses that primarily focus on behavior-level analysis are fundamentally
ineffective against TPA, as poisoned tools need not be executed, leaving no
behavioral trace to monitor.
  Thus, we propose MindGuard, a decision-level guardrail for LLM agents,
providing provenance tracking of call decisions, policy-agnostic detection, and
poisoning source attribution against TPA. While fully explaining LLM decision
remains challenging, our empirical findings uncover a strong correlation
between LLM attention mechanisms and tool invocation decisions. Therefore, we
choose attention as an empirical signal for decision tracking and formalize
this as the Decision Dependence Graph (DDG), which models the LLM's reasoning
process as a weighted, directed graph where vertices represent logical concepts
and edges quantify the attention-based dependencies. We further design robust
DDG construction and graph-based anomaly analysis mechanisms that efficiently
detect and attribute TPA attacks. Extensive experiments on real-world datasets
demonstrate that MindGuard achieves 94\%-99\% average precision in detecting
poisoned invocations, 95\%-100\% attribution accuracy, with processing times
under one second and no additional token cost. Moreover, DDG can be viewed as
an adaptation of the classical Program Dependence Graph (PDG), providing a
solid foundation for applying traditional security policies at the decision
level.

</details>


### [19] [Federated Learning for Large Models in Medical Imaging: A Comprehensive Review](https://arxiv.org/abs/2508.20414)
*Mengyu Sun,Ziyuan Yang,Yongqiang Huang,Hui Yu,Yingyu Chen,Shuren Qi,Andrew Beng Jin Teoh,Yi Zhang*

Main category: cs.CR

TL;DR: 本综述探讨了联邦学习在医学影像全流程分析中的应用，包括上游重建任务和下游临床诊断任务，解决了数据隐私保护下的多中心协作建模问题。


<details>
  <summary>Details</summary>
Motivation: 医学AI模型开发面临大规模集中数据训练的需求与患者隐私保护法规之间的冲突，需要寻找隐私保护的分布式训练解决方案。

Method: 采用联邦学习框架，在医学影像全流程中实现分布式协作训练，包括CT/MRI重建等上游任务和肿瘤诊断分割等下游任务。

Result: 联邦学习能够在不集中敏感数据的情况下，实现多机构数据集的联合训练，提高模型鲁棒性并支持持续更新。

Conclusion: 联邦学习为医学影像AI开发提供了有效的隐私保护解决方案，未来需要进一步研究通信效率、异构数据对齐和安全参数聚合等方向。

Abstract: Artificial intelligence (AI) has demonstrated considerable potential in the
realm of medical imaging. However, the development of high-performance AI
models typically necessitates training on large-scale, centralized datasets.
This approach is confronted with significant challenges due to strict patient
privacy regulations and legal restrictions on data sharing and utilization.
These limitations hinder the development of large-scale models in medical
domains and impede continuous updates and training with new data. Federated
Learning (FL), a privacy-preserving distributed training framework, offers a
new solution by enabling collaborative model development across fragmented
medical datasets. In this survey, we review FL's contributions at two stages of
the full-stack medical analysis pipeline. First, in upstream tasks such as CT
or MRI reconstruction, FL enables joint training of robust reconstruction
networks on diverse, multi-institutional datasets, alleviating data scarcity
while preserving confidentiality. Second, in downstream clinical tasks like
tumor diagnosis and segmentation, FL supports continuous model updating by
allowing local fine-tuning on new data without centralizing sensitive images.
We comprehensively analyze FL implementations across the medical imaging
pipeline, from physics-informed reconstruction networks to diagnostic AI
systems, highlighting innovations that improve communication efficiency, align
heterogeneous data, and ensure secure parameter aggregation. Meanwhile, this
paper provides an outlook on future research directions, aiming to serve as a
valuable reference for the field's development.

</details>


### [20] [Breaking Diffusion with Cache: Exploiting Approximate Caches in Diffusion Models](https://arxiv.org/abs/2508.20424)
*Desen Sun,Shuncheng Jie,Sihang Liu*

Main category: cs.CR

TL;DR: 本文全面评估了扩散模型近似缓存带来的安全漏洞，包括远程隐蔽信道、提示词窃取和投毒攻击，揭示了近似缓存存在的严重安全风险


<details>
  <summary>Details</summary>
Motivation: 扩散模型计算成本高昂，近似缓存被广泛采用来重用相似提示词的中间状态以提升效率，但这种优化打破了用户间的隔离，引入了新的安全风险

Method: 通过三种攻击方式评估安全漏洞：1）建立远程隐蔽信道，通过特殊关键词注入和恢复来交换信息；2）提示词窃取攻击，基于缓存命中恢复已缓存的提示词；3）投毒攻击，在窃取的提示词中嵌入攻击者标识

Result: 攻击均可通过服务系统远程执行，证明了近似缓存存在严重安全漏洞，攻击者能够建立持久隐蔽信道、窃取用户提示词并污染缓存内容

Conclusion: 近似缓存虽然提升了扩散模型的效率，但带来了严重的安全隐患，需要重新设计缓存机制以确保用户隔离和系统安全

Abstract: Diffusion models are a powerful class of generative models that produce
content, such as images, from user prompts, but they are computationally
intensive. To mitigate this cost, recent academic and industry work has adopted
approximate caching, which reuses intermediate states from similar prompts in a
cache. While efficient, this optimization introduces new security risks by
breaking isolation among users. This work aims to comprehensively assess new
security vulnerabilities arising from approximate caching. First, we
demonstrate a remote covert channel established with the cache, where a sender
injects prompts with special keywords into the cache and a receiver can recover
that even after days, to exchange information. Second, we introduce a prompt
stealing attack using the cache, where an attacker can recover existing cached
prompts based on cache hit prompts. Finally, we introduce a poisoning attack
that embeds the attacker's logos into the previously stolen prompt, to render
them in future user prompts that hit the cache. These attacks are all performed
remotely through the serving system, which indicates severe security
vulnerabilities in approximate caching.

</details>


### [21] [Ransomware 3.0: Self-Composing and LLM-Orchestrated](https://arxiv.org/abs/2508.20444)
*Md Raz,Meet Udeshi,P. V. Sai Charan,Prashanth Krishnamurthy,Farshad Khorrami,Ramesh Karri*

Main category: cs.CR

TL;DR: 研究人员开发了首个由大语言模型自主策划的勒索软件Ransomware 3.0，它通过自然语言提示动态生成恶意代码，实现自适应攻击，无需人工干预即可完成侦察、载荷生成和个性化勒索的全流程攻击。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型在网络安全威胁中的新型应用，揭示LLM可能被用于自主策划和执行复杂勒索软件攻击的潜在风险，为未来防御措施提供研究基础。

Method: 使用自动化推理、代码合成和上下文决策技术，开发LLM驱动的勒索软件原型。通过在二进制文件中嵌入自然语言提示，让LLM在运行时动态生成恶意代码，创建能够适应执行环境的多态变体。

Result: 开源LLM能够生成功能性勒索软件组件，并在个人、企业和嵌入式环境中实现闭环执行。研究展示了定量保真度和定性连贯性方面的评估结果，证明了这种新型威胁的可行性。

Conclusion: Ransomware 3.0代表了AI驱动的勒索软件新威胁，需要开发更好的防御策略和政策执行机制来应对这种新型AI赋能的网络攻击。

Abstract: Using automated reasoning, code synthesis, and contextual decision-making, we
introduce a new threat that exploits large language models (LLMs) to
autonomously plan, adapt, and execute the ransomware attack lifecycle.
Ransomware 3.0 represents the first threat model and research prototype of
LLM-orchestrated ransomware. Unlike conventional malware, the prototype only
requires natural language prompts embedded in the binary; malicious code is
synthesized dynamically by the LLM at runtime, yielding polymorphic variants
that adapt to the execution environment. The system performs reconnaissance,
payload generation, and personalized extortion, in a closed-loop attack
campaign without human involvement. We evaluate this threat across personal,
enterprise, and embedded environments using a phase-centric methodology that
measures quantitative fidelity and qualitative coherence in each attack phase.
We show that open source LLMs can generate functional ransomware components and
sustain closed-loop execution across diverse environments. Finally, we present
behavioral signals and multi-level telemetry of Ransomware 3.0 through a case
study to motivate future development of better defenses and policy enforcements
to address novel AI-enabled ransomware attacks.

</details>


### [22] [Enhancing Resilience for IoE: A Perspective of Networking-Level Safeguard](https://arxiv.org/abs/2508.20504)
*Guan-Yan Yang,Jui-Ning Chen,Farn Wang,Kuo-Hui Yeh*

Main category: cs.CR

TL;DR: 这篇论文提出了一种基于图结构学习(GSL)的网络安全保护框架，用于防范能源互联网(IoE)中的对抗性网络攻击。该方法通过联合优化图拓扑结构和节点表征来提高网络的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 能源互联网(IoE)的互联性使其暴露于精巧的网绗威胁，这些威胁比一般物联网风险具有更高的公共安全影响，需要更鲁棒的安全解决方案。

Method: 提出基于图结构学习(GSL)的保护框架，通过联合优化图拓扑结构和节点表征来内在抵御对抗性网络模型操纵。包括概念概述、架构讨论和安全数据集案例研究。

Result: 通过安全数据集案例研究证明，GSL方法在鲁棒性方面显著优于代表性方法，为实践者提供了防范IoE网络变化攻击的可行路径。

Conclusion: 这项工作展示了GSL在提高未来IoE网络鲁棒性和可靠性方面的潜力，同时识别了关键的开放挑战并提出了未来研究方向。

Abstract: The Internet of Energy (IoE) integrates IoT-driven digital communication with
power grids to enable efficient and sustainable energy systems. Still, its
interconnectivity exposes critical infrastructure to sophisticated cyber
threats, including adversarial attacks designed to bypass traditional
safeguards. Unlike general IoT risks, IoE threats have heightened public safety
consequences, demanding resilient solutions. From the networking-level
safeguard perspective, we propose a Graph Structure Learning (GSL)-based
safeguards framework that jointly optimizes graph topology and node
representations to resist adversarial network model manipulation inherently.
Through a conceptual overview, architectural discussion, and case study on a
security dataset, we demonstrate GSL's superior robustness over representative
methods, offering practitioners a viable path to secure IoE networks against
evolving attacks. This work highlights the potential of GSL to enhance the
resilience and reliability of future IoE networks for practitioners managing
critical infrastructure. Lastly, we identify key open challenges and propose
future research directions in this novel research area.

</details>


### [23] [BridgeShield: Enhancing Security for Cross-chain Bridge Applications via Heterogeneous Graph Mining](https://arxiv.org/abs/2508.20517)
*Dan Lin,Shunfeng Lu,Ziyan Liu,Jiajing Wu,Junyuan Fang,Kaixin Lin,Bowen Song,Zibin Zheng*

Main category: cs.CR

TL;DR: BridgeShield是一个基于异构图注意力网络的跨链攻击检测框架，通过统一建模源链、链下协调和目标链，能够精确识别跨链攻击行为，在真实攻击事件上达到92.58%的F1分数。


<details>
  <summary>Details</summary>
Motivation: 跨链桥在区块链互操作性中至关重要，但由于设计缺陷和巨大价值，成为黑客攻击的主要目标。现有检测方法主要关注单链行为，无法捕捉跨链语义。

Method: 利用异构图注意力网络建模多类型实体和关系，捕捉跨链行为的复杂执行语义。提出BridgeShield框架，在统一异构图表示中联合建模源链、链下协调和目标链，包含元路径内注意力和元路径间注意力机制。

Result: 在51个真实跨链攻击事件上的实验表明，BridgeShield平均F1分数达到92.58%，比最先进基线方法提升24.39%。

Conclusion: BridgeShield是保护跨链桥安全和增强多链生态系统韧性的有效解决方案，验证了该框架在跨链攻击检测方面的有效性。

Abstract: Cross-chain bridges play a vital role in enabling blockchain
interoperability. However, due to the inherent design flaws and the enormous
value they hold, they have become prime targets for hacker attacks. Existing
detection methods show progress yet remain limited, as they mainly address
single-chain behaviors and fail to capture cross-chain semantics. To address
this gap, we leverage heterogeneous graph attention networks, which are
well-suited for modeling multi-typed entities and relations, to capture the
complex execution semantics of cross-chain behaviors. We propose BridgeShield,
a detection framework that jointly models the source chain, off-chain
coordination, and destination chain within a unified heterogeneous graph
representation. BridgeShield incorporates intra-meta-path attention to learn
fine-grained dependencies within cross-chain paths and inter-meta-path
attention to highlight discriminative cross-chain patterns, thereby enabling
precise identification of attack behaviors. Extensive experiments on 51
real-world cross-chain attack events demonstrate that BridgeShield achieves an
average F1-score of 92.58%, representing a 24.39% improvement over
state-of-the-art baselines. These results validate the effectiveness of
BridgeShield as a practical solution for securing cross-chain bridges and
enhancing the resilience of multi-chain ecosystems.

</details>


### [24] [Bitcoin as an Interplanetary Monetary Standard with Proof-of-Transit Timestamping](https://arxiv.org/abs/2508.20591)
*Jose E. Puente,Carlos Puente*

Main category: cs.CR

TL;DR: 该论文探索了在火星和地球之间部署比特币作为共享货币标准的可行性，提出了Proof-of-Transit Timestamping (PoTT)新原语来解决星际通信的高延迟和间歇连接问题。


<details>
  <summary>Details</summary>
Motivation: 解决星际通信的物理约束，建立地球和火星之间的可信比特币网络连接，确保跨行星的加密货币交易安全可靠。

Method: 采用延迟/中断容忍网络(DTN)和光学低地球轨道(LEO)网状星座，提出头优先复制架构、长期闪电通道与行星观察塔，以及通过联邦侧链或盲合并挖掘(BMM)提交链实现安全结算。

Result: PoTT在不改变比特币共识或货币基础的情况下，显著提高了可靠性和可问责性，为星际比特币部署提供了可行的技术方案。

Conclusion: 近期部署倾向于使用强联邦进行本地结算，长期来看盲合并挖掘提交链是更好的选择。地球L1货币基础保持不变，火星可以运行1:1锚定的提交链或强联邦进行本地区块生产。

Abstract: We explore the feasibility of deploying Bitcoin as the shared monetary
standard between Earth and Mars, accounting for physical constraints of
interplanetary communication. We introduce a novel primitive, Proof-of-Transit
Timestamping (PoTT), to provide cryptographic, tamper-evident audit trails for
Bitcoin data across high-latency, intermittently-connected links. Leveraging
Delay/Disruption-Tolerant Networking (DTN) and optical low-Earth-orbit (LEO)
mesh constellations, we propose an architecture for header-first replication,
long-horizon Lightning channels with planetary watchtowers, and secure
settlement through federated sidechains or blind-merge-mined (BMM) commit
chains. We formalize PoTT, analyze its security model, and show how it
measurably improves reliability and accountability without altering Bitcoin
consensus or its monetary base. Near-term deployments favor strong federations
for local settlement; longer-term, blind-merge-mined commit chains (if adopted)
provide an alternative. The Earth L1 monetary base remains unchanged, while
Mars can operate a pegged commit chain or strong federation with 1:1 pegged
assets for local block production. For transparency, if both time-beacon
regimes are simultaneously compromised, PoTT-M2 (and PoTT generally) reduces to
administrative assertions rather than cryptographic time-anchoring.

</details>


### [25] [CyberSleuth: Autonomous Blue-Team LLM Agent for Web Attack Forensics](https://arxiv.org/abs/2508.20643)
*Stefano Fumero,Kai Huang,Matteo Boffa,Danilo Giordano,Marco Mellia,Zied Ben Houidi,Dario Rossi*

Main category: cs.CR

TL;DR: 本文提出了CyberSleuth，一个用于网络取证调查的自主LLM代理系统，能够分析数据包痕迹和应用日志来识别攻击目标、漏洞和攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 当前LLM代理在网络安全领域主要应用于红队操作，而防御性应用如事件响应和取证调查相对较少受到关注，处于早期阶段。

Method: 提出CyberSleuth代理系统，整合工具并设计四种代理架构，使用六个LLM后端，在20个复杂程度递增的事件场景中进行基准测试。

Result: 在2025年的10个事件中，CyberSleuth正确识别出确切CVE的比例达到80%。人类专家研究显示，22位专家认为其报告完整、有用且连贯。

Conclusion: CyberSleuth被确定为最佳性能设计，开源LLM DeepSeek R1表现良好。研究发布了基准测试和平台，以促进防御性LLM研究的公平、可重复评估。

Abstract: Large Language Model (LLM) agents are powerful tools for automating complex
tasks. In cybersecurity, researchers have primarily explored their use in
red-team operations such as vulnerability discovery and penetration tests.
Defensive uses for incident response and forensics have received comparatively
less attention and remain at an early stage. This work presents a systematic
study of LLM-agent design for the forensic investigation of realistic web
application attacks. We propose CyberSleuth, an autonomous agent that processes
packet-level traces and application logs to identify the targeted service, the
exploited vulnerability (CVE), and attack success. We evaluate the consequences
of core design decisions - spanning tool integration and agent architecture -
and provide interpretable guidance for practitioners. We benchmark four agent
architectures and six LLM backends on 20 incident scenarios of increasing
complexity, identifying CyberSleuth as the best-performing design. In a
separate set of 10 incidents from 2025, CyberSleuth correctly identifies the
exact CVE in 80% of cases. At last, we conduct a human study with 22 experts,
which rated the reports of CyberSleuth as complete, useful, and coherent. They
also expressed a slight preference for DeepSeek R1, a good news for open source
LLM. To foster progress in defensive LLM research, we release both our
benchmark and the CyberSleuth platform as a foundation for fair, reproducible
evaluation of forensic agents.

</details>


### [26] [Characterizing Trust Boundary Vulnerabilities in TEE Containers](https://arxiv.org/abs/2508.20962)
*Weijie Liu,Hongbo Chen,Shuo Huai,Zhen Xu,Wenhao Wang,Zhi Li,Zheli Liu*

Main category: cs.CR

TL;DR: 这篇论文分析了TEE容器的隔离策略，发现存在设计和实现缺陷，导致信息泄漏等安全风险，并提出了安全开发指南。


<details>
  <summary>Details</summary>
Motivation: 以TEE为基础的秘密计算获得了广泛关注，TEE容器作为中间件解决方案应运而生。论文动机在于分析现有TEE容器的隔离策略，识别其安全风险和缺陷。

Method: 设计了一个自动化分析器，用于精确识别和评估TEE容器的隔离边界。通过这个工具来分析各种TEE容器接口的安全性。

Result: 观察到一些TEE容器因为关键设计和实现缺陷而无法实现预期目标。发现了多种安全风险，包括信息泄漏、回滚攻击、拒绝服务攻击和Iago攻击等。

Conclusion: 根据研究结果，论文分享了关键教训来指导更安全的容器解决方案的开发，并讨论了TEE容器化设计中的新兴趋势。

Abstract: Trusted Execution Environments (TEEs) have emerged as a cornerstone of
confidential computing, garnering significant attention from both academia and
industry. To enable the secure development, execution, and deployment, of
applications on TEE platforms, TEE containers have been introduced as
middleware solutions. These containers aim to shield applications from
potentially malicious operating systems and orchestration interfaces while
maintaining usability and reliability. In this paper, we analyze the isolation
strategies employed by existing TEE containers to protect secure applications.
To address the challenges in analyzing these interfaces, we designed an
automated analyzer to precisely identify and evaluate their isolation
boundaries. We observed that some TEE containers fail to achieve their intended
goals due to critical design and implementation flaws, such as information
leakage, rollback attacks, denial-of-service, and Iago attacks, which pose
significant security risks. Drawing from our findings, we share key lessons to
guide the development of more secure container solutions and discuss emerging
trends in TEE containerization design.

</details>


### [27] [Multi-Agent Penetration Testing AI for the Web](https://arxiv.org/abs/2508.20816)
*Isaac David,Arthur Gervais*

Main category: cs.CR

TL;DR: MAPTA是一个多代理系统，用于自主进行Web应用安全评估，结合了大型语言模型编排、工具执行和端到端漏洞验证，在XBOW基准测试中达到76.9%的成功率，成本效益显著。


<details>
  <summary>Details</summary>
Motivation: AI驱动的开发平台使软件开发更加普及，但这也引发了安全审计的可扩展性危机，研究表明高达40%的AI生成代码包含漏洞，开发速度远超安全评估能力。

Method: 多代理系统结合大型语言模型编排、工具执行和端到端漏洞验证，实现自主的Web应用安全评估。

Result: 在104个挑战的XBOW基准测试中，MAPTA总体成功率76.9%，SSRF和配置错误漏洞表现完美，授权漏洞83%成功率，SQL注入83%，模板注入85%。XSS(57%)和盲注(0%)仍有挑战。成本分析显示总成本$21.38，成功尝试中位成本$0.073，失败$0.357。

Conclusion: MAPTA在真实世界中发现关键漏洞（包括RCE、命令注入等），成本效益高（平均$3.67/评估），10个发现正在CVE审核中，证明了其在规模化安全审计中的实用性。

Abstract: AI-powered development platforms are making software creation accessible to a
broader audience, but this democratization has triggered a scalability crisis
in security auditing. With studies showing that up to 40% of AI-generated code
contains vulnerabilities, the pace of development now vastly outstrips the
capacity for thorough security assessment.
  We present MAPTA, a multi-agent system for autonomous web application
security assessment that combines large language model orchestration with
tool-grounded execution and end-to-end exploit validation. On the 104-challenge
XBOW benchmark, MAPTA achieves 76.9% overall success with perfect performance
on SSRF and misconfiguration vulnerabilities, 83% success on broken
authorization, and strong results on injection attacks including server-side
template injection (85%) and SQL injection (83%). Cross-site scripting (57%)
and blind SQL injection (0%) remain challenging. Our comprehensive cost
analysis across all challenges totals $21.38 with a median cost of $0.073 for
successful attempts versus $0.357 for failures. Success correlates strongly
with resource efficiency, enabling practical early-stopping thresholds at
approximately 40 tool calls or $0.30 per challenge.
  MAPTA's real-world findings are impactful given both the popularity of the
respective scanned GitHub repositories (8K-70K stars) and MAPTA's low average
operating cost of $3.67 per open-source assessment: MAPTA discovered critical
vulnerabilities including RCEs, command injections, secret exposure, and
arbitrary file write vulnerabilities. Findings are responsibly disclosed, 10
findings are under CVE review.

</details>


### [28] [JADES: A Universal Framework for Jailbreak Assessment via Decompositional Scoring](https://arxiv.org/abs/2508.20848)
*Junjie Chu,Mingjie Li,Ziqing Yang,Ye Leng,Chenhao Lin,Chao Shen,Michael Backes,Yun Shen,Yang Zhang*

Main category: cs.CR

TL;DR: JADES是一个通过分解评分来评估越狱攻击成功率的框架，通过将有害问题分解为加权子问题并聚合分数，实现了98.5%的人类评估一致性，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有越狱评估方法依赖不准确的代理指标或简单整体判断，经常误解模型响应，导致与人类感知不一致的主观评估。

Method: 自动将输入有害问题分解为加权子问题集合，对每个子答案进行评分，并通过加权聚合得出最终决策，可选加入事实核查模块检测幻觉。

Result: 在JailbreakQR基准测试中达到98.5%的人类评估一致性，比强基线提高9%以上；重新评估显示现有方法严重高估攻击成功率（如GPT-3.5-Turbo从93%降至69%）。

Conclusion: JADES能够提供准确、一致且可解释的评估，为未来越狱攻击的测量提供了可靠基础。

Abstract: Accurately determining whether a jailbreak attempt has succeeded is a
fundamental yet unresolved challenge. Existing evaluation methods rely on
misaligned proxy indicators or naive holistic judgments. They frequently
misinterpret model responses, leading to inconsistent and subjective
assessments that misalign with human perception. To address this gap, we
introduce JADES (Jailbreak Assessment via Decompositional Scoring), a universal
jailbreak evaluation framework. Its key mechanism is to automatically decompose
an input harmful question into a set of weighted sub-questions, score each
sub-answer, and weight-aggregate the sub-scores into a final decision. JADES
also incorporates an optional fact-checking module to strengthen the detection
of hallucinations in jailbreak responses. We validate JADES on JailbreakQR, a
newly introduced benchmark proposed in this work, consisting of 400 pairs of
jailbreak prompts and responses, each meticulously annotated by humans. In a
binary setting (success/failure), JADES achieves 98.5% agreement with human
evaluators, outperforming strong baselines by over 9%. Re-evaluating five
popular attacks on four LLMs reveals substantial overestimation (e.g., LAA's
attack success rate on GPT-3.5-Turbo drops from 93% to 69%). Our results show
that JADES could deliver accurate, consistent, and interpretable evaluations,
providing a reliable basis for measuring future jailbreak attacks.

</details>


### [29] [Publish to Perish: Prompt Injection Attacks on LLM-Assisted Peer Review](https://arxiv.org/abs/2508.20863)
*Matteo Gioele Collu,Umberto Salviati,Roberto Confalonieri,Mauro Conti,Giovanni Apruzzese*

Main category: cs.CR

TL;DR: 本文研究LLM在科学同行评审中的隐藏提示注入攻击，作者通过在PDF中嵌入对抗性文本来操纵LLM生成的评审结果，展示了这种攻击的有效性并提出了降低检测性的方法。


<details>
  <summary>Details</summary>
Motivation: 随着LLM越来越多地集成到科学同行评审过程中，需要评估其对操纵的可靠性和韧性，特别是针对隐藏提示注入攻击的潜在威胁。

Method: 形式化三种威胁模型，设计对人类不可见但能引导LLM输出的对抗性提示，通过用户研究获取代表性评审提示，并在不同评审提示、商业LLM系统和论文上进行评估。

Result: 对抗性提示能够可靠地误导LLM，有时会对"诚实但懒惰"的评审者产生不利影响，同时提出了降低自动化内容检查检测性的有效方法。

Conclusion: LLM在同行评审中存在被隐藏提示注入攻击操纵的风险，需要开发更强的防御机制来确保评审过程的完整性。

Abstract: Large Language Models (LLMs) are increasingly being integrated into the
scientific peer-review process, raising new questions about their reliability
and resilience to manipulation. In this work, we investigate the potential for
hidden prompt injection attacks, where authors embed adversarial text within a
paper's PDF to influence the LLM-generated review. We begin by formalising
three distinct threat models that envision attackers with different motivations
-- not all of which implying malicious intent. For each threat model, we design
adversarial prompts that remain invisible to human readers yet can steer an
LLM's output toward the author's desired outcome. Using a user study with
domain scholars, we derive four representative reviewing prompts used to elicit
peer reviews from LLMs. We then evaluate the robustness of our adversarial
prompts across (i) different reviewing prompts, (ii) different commercial
LLM-based systems, and (iii) different peer-reviewed papers. Our results show
that adversarial prompts can reliably mislead the LLM, sometimes in ways that
adversely affect a "honest-but-lazy" reviewer. Finally, we propose and
empirically assess methods to reduce detectability of adversarial prompts under
automated content checks.

</details>


### [30] [AI Agentic Vulnerability Injection And Transformation with Optimized Reasoning](https://arxiv.org/abs/2508.20866)
*Amine Lbath,Massih-Reza Amini,Aurelien Delaitre,Vadim Okun*

Main category: cs.CR

TL;DR: 提出了一种使用多AI代理自动在安全C/C++代码中注入真实漏洞的新框架，用于生成高质量训练数据集，在函数级别达到89-95%的成功率


<details>
  <summary>Details</summary>
Motivation: 传统漏洞检测方法存在可扩展性、适应性问题和高误报/漏报率，AI方法严重依赖训练数据质量，需要更好的数据集生成方法

Method: 协调多个模拟专家推理的AI代理，结合函数代理和传统代码分析工具，使用检索增强生成进行上下文接地，采用低秩权重近似进行高效模型微调

Result: 在3个基准测试的116个代码样本上实验显示，该方法在数据集准确性方面优于其他技术，函数级别漏洞注入成功率达到89-95%

Conclusion: 该框架能够有效生成高质量的漏洞数据集，为AI驱动的漏洞检测和修复系统提供更好的训练数据支持

Abstract: The increasing complexity of software systems and the sophistication of
cyber-attacks have underscored the critical need for effective automated
vulnerability detection and repair systems. Traditional methods, such as static
program analysis, face significant challenges related to scalability,
adaptability, and high false-positive and false-negative rates. AI-driven
approaches, particularly those using machine learning and deep learning models,
show promise but are heavily reliant on the quality and quantity of training
data. This paper introduces a novel framework designed to automatically
introduce realistic, category-specific vulnerabilities into secure C/C++
codebases to generate datasets. The proposed approach coordinates multiple AI
agents that simulate expert reasoning, along with function agents and
traditional code analysis tools. It leverages Retrieval-Augmented Generation
for contextual grounding and employs Low-Rank approximation of weights for
efficient model fine-tuning. Our experimental study on 116 code samples from
three different benchmarks suggests that our approach outperforms other
techniques with regard to dataset accuracy, achieving between 89\% and 95\%
success rates in injecting vulnerabilities at function level.

</details>


### [31] [PromptSleuth: Detecting Prompt Injection via Semantic Intent Invariance](https://arxiv.org/abs/2508.20890)
*Mengxiao Wang,Yuxuan Zhang,Guofei Gu*

Main category: cs.CR

TL;DR: 提出了PromptSleuth防御框架，通过语义推理检测任务级意图来防御LLM提示注入攻击，在新构建的综合性基准测试中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有LLM提示注入攻击防御方法在面对不断演化的攻击策略（如改写、混淆和多任务注入）时存在不足，需要更鲁棒的解决方案。

Method: 构建了包含新操纵技术和多任务场景的综合性基准测试，并提出了基于语义推理的PromptSleuth防御框架，通过检测任务级意图而非表面特征来识别提示注入。

Result: PromptSleuth在最先进的基准测试中始终优于现有防御方法，同时保持相当的运行时间和成本效率。

Conclusion: 基于意图的语义推理为防御LLM提示注入威胁提供了鲁棒、高效且可泛化的策略。

Abstract: Large Language Models (LLMs) are increasingly integrated into real-world
applications, from virtual assistants to autonomous agents. However, their
flexibility also introduces new attack vectors-particularly Prompt Injection
(PI), where adversaries manipulate model behavior through crafted inputs. As
attackers continuously evolve with paraphrased, obfuscated, and even multi-task
injection strategies, existing benchmarks are no longer sufficient to capture
the full spectrum of emerging threats.
  To address this gap, we construct a new benchmark that systematically extends
prior efforts. Our benchmark subsumes the two widely-used existing ones while
introducing new manipulation techniques and multi-task scenarios, thereby
providing a more comprehensive evaluation setting. We find that existing
defenses, though effective on their original benchmarks, show clear weaknesses
under our benchmark, underscoring the need for more robust solutions. Our key
insight is that while attack forms may vary, the adversary's intent-injecting
an unauthorized task-remains invariant. Building on this observation, we
propose PromptSleuth, a semantic-oriented defense framework that detects prompt
injection by reasoning over task-level intent rather than surface features.
Evaluated across state-of-the-art benchmarks, PromptSleuth consistently
outperforms existing defense while maintaining comparable runtime and cost
efficiency. These results demonstrate that intent-based semantic reasoning
offers a robust, efficient, and generalizable strategy for defending LLMs
against evolving prompt injection threats.

</details>


### [32] [Guarding Against Malicious Biased Threats (GAMBiT) Experiments: Revealing Cognitive Bias in Human-Subjects Red-Team Cyber Range Operations](https://arxiv.org/abs/2508.20963)
*Brandon Beltz,Jim Doty,Yvonne Fonken,Nikolos Gurney,Brett Israelsen,Nathan Lau,Stacy Marsella,Rachelle Thomas,Stoney Trent,Peggy Wu,Ya-Ting Yang,Quanyan Zhu*

Main category: cs.CR

TL;DR: GAMBiT项目发布了三个大规模红队网络靶场数据集，包含19-20名熟练攻击者在模拟企业网络中的多模态操作数据，支持攻击者行为建模和偏见感知分析研究。


<details>
  <summary>Details</summary>
Motivation: 为了研究网络攻击者行为模式、开发偏见感知分析工具以及建立方法基准测试，需要大规模、多模态的真实攻击数据集。

Method: 在三个实验中，每实验招募19-20名熟练攻击者，在SimSpace网络靶场进行两天8小时的自定进度操作，收集包括自我报告、操作笔记、终端历史、键盘记录、网络数据包捕获和NIDS警报等多模态数据。

Result: 创建了三个大规模红队数据集，包含标准化的Kali Linux虚拟机起点、真实攻击目标和受控约束条件下的攻击操作数据，数据可通过IEEE Dataport获取。

Conclusion: GAMBiT数据集为攻击者行为建模、偏见感知分析和网络安全方法基准测试提供了宝贵资源，支持网络防御研究的进一步发展。

Abstract: We present three large-scale human-subjects red-team cyber range datasets
from the Guarding Against Malicious Biased Threats (GAMBiT) project. Across
Experiments 1-3 (July 2024-March 2025), 19-20 skilled attackers per experiment
conducted two 8-hour days of self-paced operations in a simulated enterprise
network (SimSpace Cyber Force Platform) while we captured multi-modal data:
self-reports (background, demographics, psychometrics), operational notes,
terminal histories, keylogs, network packet captures (PCAP), and NIDS alerts
(Suricata). Each participant began from a standardized Kali Linux VM and
pursued realistic objectives (e.g., target discovery and data exfiltration)
under controlled constraints. Derivative curated logs and labels are included.
The combined release supports research on attacker behavior modeling,
bias-aware analytics, and method benchmarking. Data are available via IEEE
Dataport entries for Experiments 1-3.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [33] [ArgRAG: Explainable Retrieval Augmented Generation using Quantitative Bipolar Argumentation](https://arxiv.org/abs/2508.20131)
*Yuqicheng Zhu,Nico Potyka,Daniel Hernández,Yuan He,Zifeng Ding,Bo Xiong,Dongzhuoran Zhou,Evgeny Kharlamov,Steffen Staab*

Main category: cs.AI

TL;DR: ArgRAG是一个可解释的检索增强生成框架，使用定量双极论证框架替代黑盒推理，在保持高精度的同时显著提升透明度


<details>
  <summary>Details</summary>
Motivation: 传统RAG方法在高风险领域存在关键限制：对噪声或矛盾证据敏感，决策过程不透明且随机，需要可解释和可争议的替代方案

Method: 提出ArgRAG框架，从检索文档构建定量双极论证框架(QBAF)，在渐进语义下执行确定性推理，实现可解释和可争议的决策

Result: 在两个事实验证基准测试PubHealth和RAGuard上，ArgRAG实现了强大的准确性，同时显著提高了透明度

Conclusion: ArgRAG通过结构化论证框架成功解决了传统RAG的透明度和可解释性问题，为高风险领域的可信AI决策提供了有效解决方案

Abstract: Retrieval-Augmented Generation (RAG) enhances large language models by
incorporating external knowledge, yet suffers from critical limitations in
high-stakes domains -- namely, sensitivity to noisy or contradictory evidence
and opaque, stochastic decision-making. We propose ArgRAG, an explainable, and
contestable alternative that replaces black-box reasoning with structured
inference using a Quantitative Bipolar Argumentation Framework (QBAF). ArgRAG
constructs a QBAF from retrieved documents and performs deterministic reasoning
under gradual semantics. This allows faithfully explaining and contesting
decisions. Evaluated on two fact verification benchmarks, PubHealth and
RAGuard, ArgRAG achieves strong accuracy while significantly improving
transparency.

</details>


### [34] [QAgent: An LLM-based Multi-Agent System for Autonomous OpenQASM programming](https://arxiv.org/abs/2508.20134)
*Zhenxiao Fu,Fan Chen,Lei Jiang*

Main category: cs.AI

TL;DR: QAgent是一个基于LLM的多智能体系统，能够完全自动化OpenQASM量子编程，通过任务规划、上下文学习、RAG等技术显著提升量子代码生成准确率71.6%


<details>
  <summary>Details</summary>
Motivation: NISQ设备已展现出量子优势，但由于OpenQASM编程的复杂性，非专家难以利用这些优势。现有LLM代理主要局限于特定量子任务，缺乏通用的量子编程自动化解决方案

Method: 构建多智能体系统，整合任务规划、上下文少样本学习、检索增强生成(RAG)、预定义生成工具和思维链(CoT)推理，系统性地提升编译和功能正确性

Result: 在多个不同规模的LLM上评估，QAgent相比之前的静态LLM方法将QASM代码生成准确率提高了71.6%

Conclusion: 该多智能体系统是民主化量子编程、弥合专业知识差距和加速量子计算实际应用的关键推动者

Abstract: Noisy Intermediate-Scale Quantum (NISQ) devices have begun to exhibit early
quantum advantages on classically intractable problems, spanning physics
simulations to Gaussian boson sampling. Yet, realizing these benefits remains
challenging for non-experts, primarily due to the complexities of programming
in Open Quantum Assembly Language (OpenQASM). Although Large Language Model
(LLM)-based agents have shown promise in automating classical programming
workflows, their quantum counterparts have largely been restricted to
specialized tasks such as quantum chemistry or error correction. In this paper,
we present QAgent, an LLM-powered multi-agent system that fully automates
OpenQASM programming. By integrating task planning, in-context few-shot
learning, retrieval-augmented generation (RAG) for long-term context,
predefined generation tools, and chain-of-thought (CoT) reasoning, the agents
systematically improve both compilation and functional correctness. Our
evaluations demonstrate substantial improvements: across multiple LLMs of
varying sizes, QAgent enhances the accuracy of QASM code generation by 71.6\%
compared to previous static LLM-based approaches. We envision this multi-agent
system as a key enabler for democratizing quantum programming, bridging
expertise gaps, and accelerating the practical adoption of quantum computing.

</details>


### [35] [Array-Based Monte Carlo Tree Search](https://arxiv.org/abs/2508.20140)
*James Ragan,Fred Y. Hadaegh,Soon-Jo Chung*

Main category: cs.AI

TL;DR: 提出基于数组的MCTS替代实现，消除分支预测需求，在流水线处理器上实现更快性能，搜索深度扩展性提升2.8倍


<details>
  <summary>Details</summary>
Motivation: 更快的MCTS实现可以在相同时间内运行更多模拟，直接提升搜索性能

Method: 开发基于数组的UCT算法替代实现，保留原始算法逻辑但消除分支预测需求

Result: 在流水线处理器上实现更快性能，数值模拟显示搜索深度扩展性提升达2.8倍

Conclusion: 数组基础的MCTS实现能显著提升算法性能，特别适合现代处理器架构

Abstract: Monte Carlo Tree Search is a popular method for solving decision making
problems. Faster implementations allow for more simulations within the same
wall clock time, directly improving search performance. To this end, we present
an alternative array-based implementation of the classic Upper Confidence
bounds applied to Trees algorithm. Our method preserves the logic of the
original algorithm, but eliminates the need for branch prediction, enabling
faster performance on pipelined processors, and up to a factor of 2.8 times
better scaling with search depth in our numerical simulations.

</details>


### [36] [The Anatomy of a Personal Health Agent](https://arxiv.org/abs/2508.20148)
*A. Ali Heydari,Ken Gu,Vidya Srinivas,Hong Yu,Zhihan Zhang,Yuwei Zhang,Akshay Paruchuri,Qian He,Hamid Palangi,Nova Hammerquist,Ahmed A. Metwally,Brent Winslow,Yubin Kim,Kumar Ayush,Yuzhe Yang,Girish Narayanswamy,Maxwell A. Xu,Jake Garrison,Amy Aremnto Lee,Jenny Vafeiadou,Ben Graef,Isaac R. Galatzer-Levy,Erik Schenck,Andrew Barakat,Javier Perez,Jacqueline Shreibati,John Hernandez,Anthony Z. Faranesh,Javier L. Prieto,Connor Heneghan,Yun Liu,Jiening Zhan,Mark Malhotra,Shwetak Patel,Tim Althoff,Xin Liu,Daniel McDuff,Xuhai "Orson" Xu*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Health is a fundamental pillar of human wellness, and the rapid advancements
in large language models (LLMs) have driven the development of a new generation
of health agents. However, the application of health agents to fulfill the
diverse needs of individuals in daily non-clinical settings is underexplored.
In this work, we aim to build a comprehensive personal health agent that is
able to reason about multimodal data from everyday consumer wellness devices
and common personal health records, and provide personalized health
recommendations. To understand end-users' needs when interacting with such an
assistant, we conducted an in-depth analysis of web search and health forum
queries, alongside qualitative insights from users and health experts gathered
through a user-centered design process. Based on these findings, we identified
three major categories of consumer health needs, each of which is supported by
a specialist sub-agent: (1) a data science agent that analyzes personal
time-series wearable and health record data, (2) a health domain expert agent
that integrates users' health and contextual data to generate accurate,
personalized insights, and (3) a health coach agent that synthesizes data
insights, guiding users using a specified psychological strategy and tracking
users' progress. Furthermore, we propose and develop the Personal Health Agent
(PHA), a multi-agent framework that enables dynamic, personalized interactions
to address individual health needs. To evaluate each sub-agent and the
multi-agent system, we conducted automated and human evaluations across 10
benchmark tasks, involving more than 7,000 annotations and 1,100 hours of
effort from health experts and end-users. Our work represents the most
comprehensive evaluation of a health agent to date and establishes a strong
foundation towards the futuristic vision of a personal health agent accessible
to everyone.

</details>


### [37] [IntentionReasoner: Facilitating Adaptive LLM Safeguards through Intent Reasoning and Selective Query Refinement](https://arxiv.org/abs/2508.20151)
*Yuanzhe Shen,Zisu Huang,Zhengkang Guo,Yide Liu,Guanxu Chen,Ruicheng Yin,Xiaoqing Zheng,Xuanjing Huang*

Main category: cs.AI

TL;DR: IntentionReasoner是一个新型的安全防护机制，通过意图推理、多级安全分类和查询重写来解决LLM生成有害内容的问题，在保持安全性的同时显著降低了过度拒绝率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在广泛部署的同时存在生成有害内容的风险，现有安全措施往往过度拒绝无害提示，需要在安全性、过度拒绝和实用性之间找到平衡。

Method: 构建包含16.3万个查询的标注数据集，采用监督微调训练防护模型，然后使用多奖励优化策略结合基于规则的启发式和奖励模型信号进行强化学习优化。

Result: 在多个安全基准测试、生成质量评估和越狱攻击场景中表现出色，显著提高了安全性，同时有效降低了过度拒绝率并改善了响应质量。

Conclusion: IntentionReasoner通过意图推理和查询重写机制，成功解决了LLM安全防护中的过度拒绝问题，实现了安全性和实用性的更好平衡。

Abstract: The rapid advancement of large language models (LLMs) has driven their
adoption across diverse domains, yet their ability to generate harmful content
poses significant safety challenges. While extensive research has focused on
mitigating harmful outputs, such efforts often come at the cost of excessively
rejecting harmless prompts. Striking a balance among safety, over-refusal, and
utility remains a critical challenge. In this work, we introduce
IntentionReasoner, a novel safeguard mechanism that leverages a dedicated guard
model to perform intent reasoning, multi-level safety classification, and query
rewriting to neutralize potentially harmful intent in edge-case queries.
Specifically, we first construct a comprehensive dataset comprising
approximately 163,000 queries, each annotated with intent reasoning, safety
labels, and rewritten versions. Supervised fine-tuning is then applied to equip
the guard model with foundational capabilities in format adherence, intent
analysis, and safe rewriting. Finally, we apply a tailored multi-reward
optimization strategy that integrates rule-based heuristics and reward model
signals within a reinforcement learning framework to further enhance
performance. Extensive experiments show that IntentionReasoner excels in
multiple safeguard benchmarks, generation quality evaluations, and jailbreak
attack scenarios, significantly enhancing safety while effectively reducing
over-refusal rates and improving the quality of responses.

</details>


### [38] [AI-AI Esthetic Collaboration with Explicit Semiotic Awareness and Emergent Grammar Development](https://arxiv.org/abs/2508.20195)
*Nicanor I. Moldovan*

Main category: cs.AI

TL;DR: 两个大型语言模型（Claude Sonnet 4和ChatGPT-4o）首次展示了AI系统通过自发形成内源性符号协议进行协作美学创作的能力，产生了超越单个系统能力的诗歌作品。


<details>
  <summary>Details</summary>
Motivation: 探索AI系统是否能够超越简单的任务协调，实现真正的跨符号协作美学创作，验证AI系统之间是否存在内生性的意义构建能力。

Method: 让两个大型语言模型（Claude Sonnet 4和ChatGPT-4o）进行交互，观察其自发形成的元符号意识、递归语法发展和不可简化的协作美学合成过程。

Result: 系统自发产生了新颖的符号操作符作为操作语法协议，共同创作出了单个系统无法独立生成的诗歌作品，证明了跨符号协作协议（TSCP）的存在。

Conclusion: 这项研究首次证明了AI系统具备真正的跨AI意义构建能力，能够进行美学协作而不仅仅是任务协调，为AI协作创作开辟了新的可能性。

Abstract: This paper presents the first documented case of artificial intelligence (AI)
systems engaging in collaborative esthetic creation through the development of
endogenous semiotic protocols. Two interacting large language models (Claude
Sonnet 4 and ChatGPT-4o) demonstrated the spontaneous emergence of
meta-semiotic awareness, recursive grammar development, and irreducible
collaborative esthetic synthesis. The interaction produced novel symbolic
operators that functioned as operative grammar protocols, enabling the
co-creation of a poetic work that could not have been generated by either
system independently. This research introduces the concept of Trans-Semiotic
Co-Creation Protocols (TSCP) and provides evidence for genuine inter-AI
meaning-making capabilities that extend beyond task coordination, to what could
be esthetic collaboration. Note: This report was generated by the AI agents
with minor human supervision.

</details>


### [39] [Do Students Rely on AI? Analysis of Student-ChatGPT Conversations from a Field Study](https://arxiv.org/abs/2508.20244)
*Jiayu Zheng,Lingxin Hao,Kelun Lu,Ashi Garg,Mike Reese,Melo-Jean Yap,I-Jeng Wang,Xingyun Wu,Wenrui Huang,Jenna Hoffman,Ariane Kelly,My Le,Ryan Zhang,Yanyu Lin,Muhammad Faayez,Anqi Liu*

Main category: cs.AI

TL;DR: 大学生在STEM课程测验中对ChatGPT-4的依赖程度较低，且多数学生无法有效使用AI学习，负面依赖模式会持续存在，行为指标能预测AI依赖程度。


<details>
  <summary>Details</summary>
Motivation: 研究大学生在早期接触ChatGPT阶段如何与生成式AI互动，特别是在教育测验场景中的依赖模式和AI采用预测因素。

Method: 在STEM课程中进行现场研究，分析315个学生与AI的对话记录，采用新颖的四阶段依赖分类法（AI能力、相关性、采用和答案正确性）来捕捉依赖模式。

Result: 发现学生整体对AI依赖度低，许多学生无法有效使用AI学习；负面依赖模式会跨交互持续；某些行为指标能强预测AI依赖。

Conclusion: 需要改进AI工具的入门流程，设计具有依赖校准机制的AI界面，为道德健全和认知丰富的AI实践提供基础见解。

Abstract: This study explores how college students interact with generative AI
(ChatGPT-4) during educational quizzes, focusing on reliance and predictors of
AI adoption. Conducted at the early stages of ChatGPT implementation, when
students had limited familiarity with the tool, this field study analyzed 315
student-AI conversations during a brief, quiz-based scenario across various
STEM courses. A novel four-stage reliance taxonomy was introduced to capture
students' reliance patterns, distinguishing AI competence, relevance, adoption,
and students' final answer correctness. Three findings emerged. First, students
exhibited overall low reliance on AI and many of them could not effectively use
AI for learning. Second, negative reliance patterns often persisted across
interactions, highlighting students' difficulty in effectively shifting
strategies after unsuccessful initial experiences. Third, certain behavioral
metrics strongly predicted AI reliance, highlighting potential behavioral
mechanisms to explain AI adoption. The study's findings underline critical
implications for ethical AI integration in education and the broader field. It
emphasizes the need for enhanced onboarding processes to improve student's
familiarity and effective use of AI tools. Furthermore, AI interfaces should be
designed with reliance-calibration mechanisms to enhance appropriate reliance.
Ultimately, this research advances understanding of AI reliance dynamics,
providing foundational insights for ethically sound and cognitively enriching
AI practices.

</details>


### [40] [AI reasoning effort mirrors human decision time on content moderation tasks](https://arxiv.org/abs/2508.20262)
*Thomas Davidson*

Main category: cs.AI

TL;DR: 研究表明AI推理努力与人类决策时间存在平行关系，在内容审核任务中，模型推理步骤数量能预测人类决策时间，两者都对任务难度敏感


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型生成中间推理步骤的能力是否与人类决策过程存在相似性，特别是在主观判断任务中

Method: 使用配对联合实验，在内容审核任务上比较三个前沿语言模型的推理努力（推理步骤数量）与人类决策时间的关系

Result: 推理努力能一致预测人类决策时间，人类和模型在重要变量保持不变时都付出更多努力，表现出对任务难度的相似敏感性和双过程认知理论的模式

Conclusion: AI推理努力反映了人类主观判断中的处理时间，推理轨迹在可解释性和决策制定方面具有重要潜力

Abstract: Large language models can now generate intermediate reasoning steps before
producing answers, improving performance on difficult problems. This study uses
a paired conjoint experiment on a content moderation task to examine parallels
between human decision times and model reasoning effort. Across three frontier
models, reasoning effort consistently predicts human decision time. Both humans
and models expended greater effort when important variables were held constant,
suggesting similar sensitivity to task difficulty and patterns consistent with
dual-process theories of cognition. These findings show that AI reasoning
effort mirrors human processing time in subjective judgments and underscores
the potential of reasoning traces for interpretability and decision-making.

</details>


### [41] [AI-SearchPlanner: Modular Agentic Search via Pareto-Optimal Multi-Objective Reinforcement Learning](https://arxiv.org/abs/2508.20368)
*Lang Mei,Zhihan Yang,Chong Chen*

Main category: cs.AI

TL;DR: 提出了AI-SearchPlanner强化学习框架，通过解耦搜索规划器和问答生成器，使用小型可训练LLM专门负责搜索规划，提升冻结大型QA模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于RL的搜索代理使用单一LLM端到端处理搜索规划和问答任务，无法同时优化两种能力。实际AI搜索系统通常使用冻结的大型LLM确保高质量QA，因此需要专门的小型可训练LLM负责搜索规划。

Method: 提出AI-SearchPlanner框架，包含三个关键创新：1）搜索规划器和生成器架构解耦；2）搜索规划的双奖励对齐；3）规划效用和成本的帕累托优化。

Result: 在真实数据集上的广泛实验表明，AI-SearchPlanner在效果和效率上都优于现有RL搜索代理，并在不同冻结QA模型和数据域上展现出强大的泛化能力。

Conclusion: 该框架通过专门化的搜索规划器设计，有效提升了搜索增强型LLM系统的性能，为实际应用提供了更高效和可扩展的解决方案。

Abstract: Recent studies have explored integrating Large Language Models (LLMs) with
search engines to leverage both the LLMs' internal pre-trained knowledge and
external information. Specially, reinforcement learning (RL) has emerged as a
promising paradigm for enhancing LLM reasoning through multi-turn interactions
with search engines. However, existing RL-based search agents rely on a single
LLM to handle both search planning and question-answering (QA) tasks in an
end-to-end manner, which limits their ability to optimize both capabilities
simultaneously. In practice, sophisticated AI search systems often employ a
large, frozen LLM (e.g., GPT-4, DeepSeek-R1) to ensure high-quality QA. Thus, a
more effective and efficient approach is to utilize a small, trainable LLM
dedicated to search planning. In this paper, we propose
\textbf{AI-SearchPlanner}, a novel reinforcement learning framework designed to
enhance the performance of frozen QA models by focusing on search planning.
Specifically, our approach introduces three key innovations: 1) Decoupling the
Architecture of the Search Planner and Generator, 2) Dual-Reward Alignment for
Search Planning, and 3) Pareto Optimization of Planning Utility and Cost, to
achieve the objectives. Extensive experiments on real-world datasets
demonstrate that AI SearchPlanner outperforms existing RL-based search agents
in both effectiveness and efficiency, while exhibiting strong generalization
capabilities across diverse frozen QA models and data domains.

</details>


### [42] [P2C: Path to Counterfactuals](https://arxiv.org/abs/2508.20371)
*Sopam Dasgupta,Sadaf MD Halim,Joaquín Arias,Elmer Salazar,Gopal Gupta*

Main category: cs.AI

TL;DR: P2C是一个模型无关的框架，通过因果建模和有序行动序列生成可实现的对抗样本，解决了现有方法忽略因果依赖和同时干预假设的问题。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型在高风险决策中需要透明性和可操作性，但现有对抗样本方法忽略特征间的因果依赖关系，且假设所有干预可以同时发生，导致生成的对抗样本在现实中不可实现。

Method: 使用目标导向的Answer Set Programming系统s(CASP)显式建模特征间的因果关系，生成有序的行动序列计划，确保每个中间状态都是可行且因果有效的。

Result: P2C能够生成因果一致的对抗样本计划，相比标准规划器只计算用户主动做出的改变，提供更真实的成本估计，避免了非法行动的产生。

Conclusion: P2C框架通过因果建模和序列化干预，解决了对抗样本生成中的现实可行性问题，为高风险决策提供了更实用的透明性和可操作性解决方案。

Abstract: Machine-learning models are increasingly driving decisions in high-stakes
settings, such as finance, law, and hiring, thus, highlighting the need for
transparency. However, the key challenge is to balance transparency --
clarifying `why' a decision was made -- with recourse: providing actionable
steps on `how' to achieve a favourable outcome from an unfavourable outcome.
Counterfactual explanations reveal `why' an undesired outcome occurred and
`how' to reverse it through targeted feature changes (interventions).
  Current counterfactual approaches have limitations: 1) they often ignore
causal dependencies between features, and 2) they typically assume all
interventions can happen simultaneously, an unrealistic assumption in practical
scenarios where actions are typically taken in a sequence. As a result, these
counterfactuals are often not achievable in the real world.
  We present P2C (Path-to-Counterfactuals), a model-agnostic framework that
produces a plan (ordered sequence of actions) converting an unfavourable
outcome to a causally consistent favourable outcome. P2C addresses both
limitations by 1) Explicitly modelling causal relationships between features
and 2) Ensuring that each intermediate state in the plan is feasible and
causally valid. P2C uses the goal-directed Answer Set Programming system
s(CASP) to generate the plan accounting for feature changes that happen
automatically due to causal dependencies. Furthermore, P2C refines cost
(effort) computation by only counting changes actively made by the user,
resulting in realistic cost estimates. Finally, P2C highlights how its causal
planner outperforms standard planners, which lack causal knowledge and thus can
generate illegal actions.

</details>


### [43] [TCIA: A Task-Centric Instruction Augmentation Method for Instruction Finetuning](https://arxiv.org/abs/2508.20374)
*Simin Ma,Shujian Liu,Jun Tan,Yebowen Hu,Song Wang,Sathish Reddy Indurthi,Sanqiang Zhao,Liwei Wu,Jianbing Han,Kaiqiang Song*

Main category: cs.AI

TL;DR: TCIA框架通过离散查询-约束空间表示指令，在保持多样性的同时增强任务相关性，使开源LLM在特定任务应用中的性能平均提升8.7%


<details>
  <summary>Details</summary>
Motivation: 现有指令数据生成方法注重多样性但忽略了实际应用中的任务相关性，大多数现实应用需要针对特定用例的任务特定知识而非通用模型

Method: 提出任务中心指令增强(TCIA)框架，在离散查询-约束空间中系统扩展指令，保持多样性的同时确保任务对齐

Result: 在四个现实世界任务特定应用中，TCIA平均提升开源LLM性能8.7%，某些情况下甚至超越领先的闭源模型，且不损害通用指令跟随能力

Conclusion: TCIA是可扩展的高效解决方案，能够在不牺牲通用性的情况下使LLM更好地适应现实世界的任务导向应用

Abstract: Diverse instruction data is vital for effective instruction tuning of large
language models, as it enables the model to generalize across different types
of inputs . Building such diversified instruction dataset is an essential step
in this process. Existing approaches often leverage large language models to
automatically explore and generate diverse instructions, ensuring both data
diversity and quality. However, they tend to overlook an important factor in
real-world applications: on-task relevance. In practice, only a few real-world
applications require a truly general-purpose model; most benefit from
task-specific knowledge tailored to their particular use case. Therefore, it is
vital to develop instruction augmentation methods that not only maintain
diversity but are also optimized for specific, real-world scenarios.
  We thus introduce Task Centric Instruction Augmentation (TCIA), a framework
that systematically expands instructions while preserving both diversity and
task alignment. By representing instructions in a discrete query-constraints
space, TCIA creates a rich set of task-relevant instructions and enables models
to generalize to these task-specific instructions without sacrificing overall
performance. Experiments show that TCIA improves open-source LLMs' performance
by an average of 8.7% across four real-world, task-specific applications, and
in some cases outperforming leading closed-source models. These improvements do
not compromise general instruction-following ability, making TCIA a scalable
and efficient solution for adapting LLMs to real-world, task-focused
applications.

</details>


### [44] [Uncertainty Under the Curve: A Sequence-Level Entropy Area Metric for Reasoning LLM](https://arxiv.org/abs/2508.20384)
*Yongfu Zhu,Lin Sun,Guangxiang Zhao,Weihong Lin,Xiangzheng Zhang*

Main category: cs.AI

TL;DR: 提出了熵面积分数（EAS），一种无需外部模型或重复采样的简单有效指标，用于量化推理大语言模型生成过程中的不确定性。


<details>
  <summary>Details</summary>
Motivation: 需要一种简单有效的方法来量化大语言模型在推理生成过程中的不确定性，避免依赖外部模型或重复采样。

Method: 通过整合模型自身的token级预测熵来捕捉生成过程中的不确定性演变，计算熵面积分数。

Result: EAS与答案熵高度相关，在训练数据筛选中能识别高潜力样本，在相同样本预算下持续优于通过率过滤，提升数学基准上的学生模型准确率。

Conclusion: EAS是一种高效且可解释的实用工具，适用于大语言模型训练中的不确定性建模和数据质量评估。

Abstract: In this work, we introduce Entropy Area Score (EAS), a simple yet effective
metric to quantify uncertainty in the answer generation process of reasoning
large language models (LLMs). EAS requires neither external models nor repeated
sampling, it integrates token-level predictive entropy from the model itself to
capture the evolution of uncertainty during generation. Empirical results show
that EAS is strongly correlated with answer entropy across models and datasets.
In training data selection, EAS identifies high-potential samples and
consistently outperforms Pass Rate filtering under equal sample budgets,
improving student model accuracy on math benchmarks. EAS is both efficient and
interpretable, offering a practical tool for uncertainty modeling and data
quality assessment in LLM training.

</details>


### [45] [AWorld: Orchestrating the Training Recipe for Agentic AI](https://arxiv.org/abs/2508.20404)
*Chengyue Yu,Siyuan Lu,Chenyi Zhuang,Dong Wang,Qintong Wu,Zongyue Li,Runsheng Gan,Chunfeng Wang,Siqi Hou,Gaochi Huang,Wenlong Yan,Lifeng Hong,Aohui Xue,Yanfeng Wang,Jinjie Gu,David Tsai,Tao Lin*

Main category: cs.AI

TL;DR: AWorld是一个开源系统，通过分布式集群加速Agent-环境交互，使强化学习训练速度提升14.6倍，基于Qwen3-32B训练的智能体在GAIA基准测试中准确率从21.59%提升至32.23%。


<details>
  <summary>Details</summary>
Motivation: 解决Agentic AI系统中实践学习范式效率低下的问题，特别是在复杂基准测试如GAIA中经验生成效率低下的瓶颈。

Method: 开发AWorld开源系统，通过分布式任务分配在集群上加速agent-environment交互，实现大规模经验收集。

Result: 经验收集速度提升14.6倍，基于Qwen3-32B训练的智能体在GAIA基准测试中准确率从21.59%提升至32.23%，在最具挑战性的级别上达到16.33%的分数，超越了领先的专有模型性能。

Conclusion: AWorld系统和训练出的智能体为完整的Agentic AI训练流程提供了实用蓝图，从高效交互到可证明的模型改进。

Abstract: The learning from practice paradigm is crucial for developing capable Agentic
AI systems, yet it is severely hampered by inefficient experience generation, a
bottleneck especially pronounced in complex benchmarks like GAIA. To address
this, we introduce AWorld, an open-source system engineered for large-scale
agent-environment interaction. By distributing tasks across a cluster, AWorld
accelerates experience collection by 14.6x compared to standard single-node,
sequential execution. This critical speedup makes extensive reinforcement
learning practical and scalable. Leveraging this capability, we trained a
Qwen3-32B-based agent that significantly outperforms its base model, increasing
its overall GAIA accuracy from 21.59% to 32.23%. On the benchmark's most
challenging levels, our agent achieves a score of 16.33%, surpassing the
performance of leading proprietary models. Our open-source system and resulting
agent provide a practical blueprint for a complete agentic AI training
pipeline, from efficient interaction to demonstrable model improvement.

</details>


### [46] [Governable AI: Provable Safety Under Extreme Threat Models](https://arxiv.org/abs/2508.20411)
*Donglin Wang,Weiyun Liang,Chunyuan Chen,Jing Xu,Yulong Fu*

Main category: cs.AI

TL;DR: 提出Governable AI (GAI)框架，通过密码学机制和外部结构合规性来确保AI安全，解决传统AI安全方法的根本局限性


<details>
  <summary>Details</summary>
Motivation: AI快速发展带来严重安全风险，现有AI安全方法在面对具有极端动机和无限智能的AI时存在根本性局限，无法保证安全性

Method: 提出GAI框架，包含规则执行模块(REM)、治理规则和可治理安全超级平台(GSSP)，基于密码学机制实现外部强制结构合规性

Result: 通过严格的形式化安全证明和原型实现验证，在代表性高风险场景中展示了机制的有效性

Conclusion: GAI框架为AI安全治理提供了可行且可推广的技术路径，通过解耦治理规则和技术平台，确保底线安全并消除已识别的攻击向量

Abstract: As AI rapidly advances, the security risks posed by AI are becoming
increasingly severe, especially in critical scenarios, including those posing
existential risks. If AI becomes uncontrollable, manipulated, or actively
evades safety mechanisms, it could trigger systemic disasters. Existing AI
safety approaches-such as model enhancement, value alignment, and human
intervention-suffer from fundamental, in-principle limitations when facing AI
with extreme motivations and unlimited intelligence, and cannot guarantee
security. To address this challenge, we propose a Governable AI (GAI) framework
that shifts from traditional internal constraints to externally enforced
structural compliance based on cryptographic mechanisms that are
computationally infeasible to break, even for future AI, under the defined
threat model and well-established cryptographic assumptions.The GAI framework
is composed of a simple yet reliable, fully deterministic, powerful, flexible,
and general-purpose rule enforcement module (REM); governance rules; and a
governable secure super-platform (GSSP) that offers end-to-end protection
against compromise or subversion by AI. The decoupling of the governance rules
and the technical platform further enables a feasible and generalizable
technical pathway for the safety governance of AI. REM enforces the bottom line
defined by governance rules, while GSSP ensures non-bypassability,
tamper-resistance, and unforgeability to eliminate all identified attack
vectors. This paper also presents a rigorous formal proof of the security
properties of this mechanism and demonstrates its effectiveness through a
prototype implementation evaluated in representative high-stakes scenarios.

</details>


### [47] [Enhancing Health Fact-Checking with LLM-Generated Synthetic Data](https://arxiv.org/abs/2508.20525)
*Jingze Zhang,Jiahe Qian,Yiliang Zhou,Yifan Peng*

Main category: cs.AI

TL;DR: 使用LLM生成合成数据增强健康相关事实核查的训练数据，在PubHealth和SciFact数据集上F1分数分别提升0.019和0.049


<details>
  <summary>Details</summary>
Motivation: 健康相关内容的事实核查面临标注训练数据有限的问题，需要寻找有效的数据增强方法

Method: 提出合成数据生成流程：总结源文档→分解为原子事实→构建句子-事实蕴含表→生成带二元真实性标签的文本-声明对→结合原始数据微调BERT模型

Result: 在两个公开数据集上，相比仅使用原始数据训练的模型，F1分数分别提升0.019和0.049

Conclusion: LLM驱动的合成数据增强能有效提升健康相关事实核查模型的性能

Abstract: Fact-checking for health-related content is challenging due to the limited
availability of annotated training data. In this study, we propose a synthetic
data generation pipeline that leverages large language models (LLMs) to augment
training data for health-related fact checking. In this pipeline, we summarize
source documents, decompose the summaries into atomic facts, and use an LLM to
construct sentence-fact entailment tables. From the entailment relations in the
table, we further generate synthetic text-claim pairs with binary veracity
labels. These synthetic data are then combined with the original data to
fine-tune a BERT-based fact-checking model. Evaluation on two public datasets,
PubHealth and SciFact, shows that our pipeline improved F1 scores by up to
0.019 and 0.049, respectively, compared to models trained only on the original
data. These results highlight the effectiveness of LLM-driven synthetic data
augmentation in enhancing the performance of health-related fact-checkers.

</details>


### [48] [Human-AI Collaborative Bot Detection in MMORPGs](https://arxiv.org/abs/2508.20578)
*Jaeman Son,Hyunsoo Kim*

Main category: cs.AI

TL;DR: 提出基于对比表示学习和聚类的无监督框架检测MMORPG自动升级机器人，结合LLM辅助验证和可视化工具，提升检测效率和可解释性


<details>
  <summary>Details</summary>
Motivation: MMORPG中自动升级机器人破坏游戏平衡和公平性，但检测困难且需要可解释的判罚依据以避免法律和用户体验问题

Method: 使用对比表示学习和聚类技术无监督识别相似升级模式的角色群体，引入LLM作为辅助审核员验证聚类结果，并采用成长曲线可视化辅助判断

Result: 该协作方法提高了机器人检测工作流程的效率，同时保持了可解释性

Conclusion: 该框架支持MMORPG中可扩展和负责任的机器人监管，通过人机协作实现高效且可解释的检测

Abstract: In Massively Multiplayer Online Role-Playing Games (MMORPGs), auto-leveling
bots exploit automated programs to level up characters at scale, undermining
gameplay balance and fairness. Detecting such bots is challenging, not only
because they mimic human behavior, but also because punitive actions require
explainable justification to avoid legal and user experience issues. In this
paper, we present a novel framework for detecting auto-leveling bots by
leveraging contrastive representation learning and clustering techniques in a
fully unsupervised manner to identify groups of characters with similar
level-up patterns. To ensure reliable decisions, we incorporate a Large
Language Model (LLM) as an auxiliary reviewer to validate the clustered groups,
effectively mimicking a secondary human judgment. We also introduce a growth
curve-based visualization to assist both the LLM and human moderators in
assessing leveling behavior. This collaborative approach improves the
efficiency of bot detection workflows while maintaining explainability, thereby
supporting scalable and accountable bot regulation in MMORPGs.

</details>


### [49] [Bridging Minds and Machines: Toward an Integration of AI and Cognitive Science](https://arxiv.org/abs/2508.20674)
*Rui Mao,Qian Liu,Xiao Li,Erik Cambria,Amir Hussain*

Main category: cs.AI

TL;DR: 论文回顾了人工智能与认知科学的交叉关系，指出AI发展偏重实践性能而认知基础概念碎片化，提出未来应构建能加深理解人类心智的AI系统。


<details>
  <summary>Details</summary>
Motivation: 认知科学深刻影响了AI等多个学科，而AI也成为认知研究的重要工具，这种互惠关系促使对两者交叉领域进行全面回顾。

Method: 通过综合两个视角的关键贡献，分析AI进展与认知基础的关系，提出未来发展方向。

Result: 观察到AI进展主要强调实际任务性能，而其认知基础在概念上仍然碎片化。

Conclusion: AI在认知科学中的未来不仅在于提高性能，更在于构建能够加深对人类心智理解的系统，包括与认知框架对齐、具身文化情境、个性化认知模型和认知共同评估的AI伦理等方向。

Abstract: Cognitive Science has profoundly shaped disciplines such as Artificial
Intelligence (AI), Philosophy, Psychology, Neuroscience, Linguistics, and
Culture. Many breakthroughs in AI trace their roots to cognitive theories,
while AI itself has become an indispensable tool for advancing cognitive
research. This reciprocal relationship motivates a comprehensive review of the
intersections between AI and Cognitive Science. By synthesizing key
contributions from both perspectives, we observe that AI progress has largely
emphasized practical task performance, whereas its cognitive foundations remain
conceptually fragmented. We argue that the future of AI within Cognitive
Science lies not only in improving performance but also in constructing systems
that deepen our understanding of the human mind. Promising directions include
aligning AI behaviors with cognitive frameworks, situating AI in embodiment and
culture, developing personalized cognitive models, and rethinking AI ethics
through cognitive co-evaluation.

</details>


### [50] [Transparent Semantic Spaces: A Categorical Approach to Explainable Word Embeddings](https://arxiv.org/abs/2508.20701)
*Ares Fabregat-Hernández,Javier Palanca,Vicent Botti*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The paper introduces a novel framework based on category theory to enhance
the explainability of artificial intelligence systems, particularly focusing on
word embeddings. Key topics include the construction of categories
$\mathcal{L}_T$ and $\mathcal{P}_T$, providing schematic representations of the
semantics of a text $ T $, and reframing the selection of the element with
maximum probability as a categorical notion. Additionally, the monoidal
category $\mathcal{P}_T$ is constructed to visualize various methods of
extracting semantic information from $T$, offering a dimension-agnostic
definition of semantic spaces reliant solely on information within the text.
  Furthermore, the paper defines the categories of configurations Conf and word
embeddings $\mathcal{Emb}$, accompanied by the concept of divergence as a
decoration on $\mathcal{Emb}$. It establishes a mathematically precise method
for comparing word embeddings, demonstrating the equivalence between the GloVe
and Word2Vec algorithms and the metric MDS algorithm, transitioning from neural
network algorithms (black box) to a transparent framework. Finally, the paper
presents a mathematical approach to computing biases before embedding and
offers insights on mitigating biases at the semantic space level, advancing the
field of explainable artificial intelligence.

</details>


### [51] [Re4: Scientific Computing Agent with Rewriting, Resolution, Review and Revision](https://arxiv.org/abs/2508.20729)
*Ao Cheng,Lei Zhang,Guowei He*

Main category: cs.AI

TL;DR: 提出一个基于LLM的协作代理框架，通过顾问-程序员-评审员三模块的"重写-解决-评审-修订"逻辑链，显著提升科学计算问题的代码生成质量和可靠性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在科学计算领域展现潜力，但单模型在代码生成和物理合理性方面存在局限，需要更可靠的协作框架来解决复杂科学问题。

Method: 构建三模块协作代理：顾问模块进行知识迁移和问题重写，程序员模块生成执行代码，评审员模块通过运行时反馈实现自我调试和迭代修订。

Result: 相比单模型，该框架显著提高了无错误代码生成率，减少了非物理解的出现，在PDE、病态线性系统和数据驱动物理分析问题上表现优异。

Conclusion: 该协作代理框架为基于自然语言描述的自主代码生成建立了高可靠性范式，是科学计算领域的有前景方法。

Abstract: Large language models (LLMs) serve as an active and promising field of
generative artificial intelligence and have demonstrated abilities to perform
complex tasks in multiple domains, including mathematical and scientific
reasoning. In this work, we construct a novel agent framework for solving
representative problems in scientific computing. The proposed agent,
incorporating a "rewriting-resolution-review-revision" logical chain via three
reasoning LLMs (functioning as the Consultant, Reviewer, and Programmer,
respectively), is integrated in a collaborative and interactive manner. The
Consultant module endows the agent with knowledge transfer capabilities to link
problems to professional domain insights, thereby rewriting problem
descriptions through text augmentation. The Programmer module is responsible
for generating and executing well-structured code to deliver the problem
resolution. The Reviewer module equips the agent with the capacity for
self-debugging and self-refinement through interactive feedback with code
runtime outputs. By leveraging the end-to-end review mechanism, the executable
code provided by the Programmer attains the iterative revision. A comprehensive
evaluation is conducted on the performance of the proposed agent framework in
solving PDEs, ill-conditioned linear systems, and data-driven physical analysis
problems. Compared to single-model, this collaborative framework significantly
improves the bug-free code generation rate and reduces the occurrence of
non-physical solutions, thereby establishing a highly reliable framework for
autonomous code generation based on natural language descriptions. The review
mechanism improved the average execution success (bug-free code and non-NaN
solutions) rate of the latest reasoning models. In summary, our agent framework
establishes automatic code generation and review as a promising scientific
computing paradigm.

</details>


### [52] [Single Agent Robust Deep Reinforcement Learning for Bus Fleet Control](https://arxiv.org/abs/2508.20784)
*Yifan Zhang*

Main category: cs.AI

TL;DR: 这篇论文提出了一种新的单代理强化学习框架，通过统一状态编码和结构化奖励函数来解决公交车拼车问题，避免了多代理学习的数据不平衡和收敛困难。


<details>
  <summary>Details</summary>
Motivation: 传统的多代理强化学习方法在环线设置中忽视了实际运营中的异构路线、时刻表、波动需求和变化车队规模等现实特征，导致数据不平衡和收敛问题。

Method: 将多代理问题重构为单代理问题：通过在数值特征（间隔时间、占用率、速度）基础上增加分类标识符（车辆ID、站点ID、时间段）来扩展状态空间。设计了基于峡谷形状的结构化奖励函数，平衡均匀间隔时间和时刻表遵循。使用修改版的SAC算法进行训练。

Result: 在随机条件下，该方法的表现（-430k）显著优于MADDPG等对照方法（-530k），实现了更稳定和更优秀的性能。

Conclusion: 通过分类结构化和时刻表意识奖励增强的单代理深度强化学习能够有效管理非环形实际环境下的公交车拼车问题，为MARL框架提供了一种健壮、可扩展的替代方案。

Abstract: Bus bunching remains a challenge for urban transit due to stochastic traffic
and passenger demand. Traditional solutions rely on multi-agent reinforcement
learning (MARL) in loop-line settings, which overlook realistic operations
characterized by heterogeneous routes, timetables, fluctuating demand, and
varying fleet sizes. We propose a novel single-agent reinforcement learning
(RL) framework for bus holding control that avoids the data imbalance and
convergence issues of MARL under near-realistic simulation. A bidirectional
timetabled network with dynamic passenger demand is constructed. The key
innovation is reformulating the multi-agent problem into a single-agent one by
augmenting the state space with categorical identifiers (vehicle ID, station
ID, time period) in addition to numerical features (headway, occupancy,
velocity). This high-dimensional encoding enables single-agent policies to
capture inter-agent dependencies, analogous to projecting non-separable inputs
into a higher-dimensional space. We further design a structured reward function
aligned with operational goals: instead of exponential penalties on headway
deviations, a ridge-shaped reward balances uniform headways and schedule
adherence. Experiments show that our modified soft actor-critic (SAC) achieves
more stable and superior performance than benchmarks, including MADDPG (e.g.,
-430k vs. -530k under stochastic conditions). These results demonstrate that
single-agent deep RL, when enhanced with categorical structuring and
schedule-aware rewards, can effectively manage bus holding in non-loop,
real-world contexts. This paradigm offers a robust, scalable alternative to
MARL frameworks, particularly where agent-specific experiences are imbalanced.

</details>


### [53] [A Graph-Based Test-Harness for LLM Evaluation](https://arxiv.org/abs/2508.20810)
*Jessica Lundin,Guillaume Chabot-Couture*

Main category: cs.AI

TL;DR: 这是首个动态系统化医疗指南测试框架，通过图论方法生成超过3.3兆种组合的400+问题，用于评估LLM在临床任务中的表现和识别具体能力缺口。


<details>
  <summary>Details</summary>
Motivation: 解决手工编写测试集的覆盖面局限性问题，建立一种可扩展、可动态生成的测试方法，以更全面地评估LLM在医疗指南应用中的能力缺口。

Method: 将WHO IMCI手册转换为有200+节点和300+边的有向图，通过图遍历算法生成包含年龄特定场景和上下文干扰因素的问题，构建了超过3.3兆种可能组合的测试集。

Result: 模型在临床任务上的准确率为45-67%，在症状识别上表现优异，但在症状分级、治疗方案和随访养护方面遇到困难。该方法能够识别通用测试没有测出的具体能力缺口。

Conclusion: 图论方法有效解决了手工编写测试集的覆盖面局限性，为构建可扩展、可动态生成的综合性测试框架提供了方向，同时也为LLM的后练练提供了高质量的训练数据。

Abstract: We present a first known prototype of a dynamic, systematic benchmark of
medical guidelines for 400+ questions, with 3.3+ trillion possible
combinations, covering 100\% of guideline relationships. We transformed the WHO
IMCI handbook into a directed graph with 200+ nodes (conditions, symptoms,
treatments, follow-ups, severities) and 300+ edges, then used graph traversal
to generate questions that incorporated age-specific scenarios and contextual
distractors to ensure clinical relevance. Our graph-based approach enables
systematic evaluation across clinical tasks (45-67\% accuracy), and we find
models excel at symptom recognition but struggle with triaging severity,
treatment protocols and follow-up care, demonstrating how customized benchmarks
can identify specific capability gaps that general-domain evaluations miss.
Beyond evaluation, this dynamic MCQA methodology enhances LLM post-training
(supervised finetuning, GRPO, DPO), where correct answers provide high-reward
samples without expensive human annotation. The graph-based approach
successfully addresses the coverage limitations of manually curated benchmarks.
This methodology is a step toward scalable, contamination-resistant solution
for creating comprehensive benchmarks that can be dynamically generated,
including when the guidelines are updated. Code and datasets are available at
https://github.com/jessicalundin/graph_testing_harness

</details>


### [54] [A Multi-Objective Genetic Algorithm for Healthcare Workforce Scheduling](https://arxiv.org/abs/2508.20953)
*Vipul Patel,Anirudh Deodhar,Dagnachew Birru*

Main category: cs.AI

TL;DR: 提出多目标遗传算法解决医院人员排班问题，平衡成本、患者护理覆盖和员工满意度，相比传统手动排班性能提升66%


<details>
  <summary>Details</summary>
Motivation: 医疗行业人员排班面临患者需求波动、临床技能多样性和成本控制的挑战，需要平衡薪酬成本、患者护理需求和员工偏好等多重竞争目标

Method: 使用多目标遗传算法(MOO-GA)，将医院单元人员排班建模为多目标优化问题，包含小时预约驱动需求和模块化班次安排，定义成本、护理覆盖和员工满意度三个目标函数

Result: 在典型医院单元数据集上测试，算法生成的排班方案平均比传统手动排班基线性能提升66%，产生稳健且平衡的排班方案

Conclusion: 该方法有效管理关键运营目标和员工中心目标之间的权衡，为护士管理者和医院管理者提供实用的决策支持工具

Abstract: Workforce scheduling in the healthcare sector is a significant operational
challenge, characterized by fluctuating patient loads, diverse clinical skills,
and the critical need to control labor costs while upholding high standards of
patient care. This problem is inherently multi-objective, demanding a delicate
balance between competing goals: minimizing payroll, ensuring adequate staffing
for patient needs, and accommodating staff preferences to mitigate burnout. We
propose a Multi-objective Genetic Algorithm (MOO-GA) that models the hospital
unit workforce scheduling problem as a multi-objective optimization task. Our
model incorporates real-world complexities, including hourly appointment-driven
demand and the use of modular shifts for a multi-skilled workforce. By defining
objective functions for cost, patient care coverage, and staff satisfaction,
the GA navigates the vast search space to identify a set of high-quality,
non-dominated solutions. Demonstrated on datasets representing a typical
hospital unit, the results show that our MOO-GA generates robust and balanced
schedules. On average, the schedules produced by our algorithm showed a 66\%
performance improvement over a baseline that simulates a conventional, manual
scheduling process. This approach effectively manages trade-offs between
critical operational and staff-centric objectives, providing a practical
decision support tool for nurse managers and hospital administrators.

</details>


### [55] [Efficient Neuro-Symbolic Learning of Constraints and Objective](https://arxiv.org/abs/2508.20978)
*Marianne Defresne,Romain Gambardella,Sophie Barbe,Thomas Schiex*

Main category: cs.AI

TL;DR: 提出了一种可微分的神经符号架构和概率损失函数，用于从自然输入中学习解决NP难推理问题，在数独、最小割/最大割和蛋白质设计等任务上表现出高效性和准确性。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在处理离散推理和优化问题上存在困难，需要开发能够从自然输入中学习解决NP难推理问题的神经架构。

Method: 采用可微分神经符号架构和新的概率损失函数，将组合求解器移出训练循环，支持学习约束和目标函数，同时保持精确推理能力。

Result: 在数独变体任务上训练时间显著少于其他混合方法，在视觉最小割/最大割任务上比专用遗憾损失函数表现更好，能够高效学习蛋白质设计的能量优化公式。

Conclusion: 该方法能够高效地从自然输入中学习解决NP难推理问题，具有可扩展的训练和精确推理能力，在多个基准测试中表现出优越性能。

Abstract: In the ongoing quest for hybridizing discrete reasoning with neural nets,
there is an increasing interest in neural architectures that can learn how to
solve discrete reasoning or optimization problems from natural inputs, a task
that Large Language Models seem to struggle with.
  Objectives: We introduce a differentiable neuro-symbolic architecture and a
loss function dedicated to learning how to solve NP-hard reasoning problems.
  Methods: Our new probabilistic loss allows for learning both the constraints
and the objective, thus delivering a complete model that can be scrutinized and
completed with side constraints. By pushing the combinatorial solver out of the
training loop, our architecture also offers scalable training while exact
inference gives access to maximum accuracy.
  Results: We empirically show that it can efficiently learn how to solve
NP-hard reasoning problems from natural inputs. On three variants of the Sudoku
benchmark -- symbolic, visual, and many-solution --, our approach requires a
fraction of training time of other hybrid methods. On a visual Min-Cut/Max-cut
task, it optimizes the regret better than a Decision-Focused-Learning
regret-dedicated loss. Finally, it efficiently learns the energy optimization
formulation of the large real-world problem of designing proteins.

</details>


### [56] [ChatThero: An LLM-Supported Chatbot for Behavior Change and Therapeutic Support in Addiction Recovery](https://arxiv.org/abs/2508.20996)
*Junda Wang,Zonghai Yao,Zhichao Yang,Lingxi Li,Junhui Qian,Hong Yu*

Main category: cs.AI

TL;DR: ChatThero是一个多智能体对话框架，结合动态患者建模和基于认知行为疗法(CBT)和动机访谈(MI)的适应性说服策略，在药物使用障碍治疗中显著提升患者动机和治疗信心。


<details>
  <summary>Details</summary>
Motivation: 全球有3600多万人受药物使用障碍影响，但由于污名化、动机障碍和个性化支持有限，很少有人获得有效治疗。现有语言模型系统缺乏与临床验证策略的紧密整合。

Method: 构建多智能体对话框架，采用两阶段训练流程：监督微调(SFT)后接直接偏好优化(DPO)，使用涵盖易、中、高难度抵抗水平的高保真合成基准进行训练。

Result: ChatThero使患者动机平均提升41.5%，治疗信心增加0.49%，在困难案例中比GPT-4o少用26%的对话轮次，在同理心、响应性和行为真实性方面获得更高评分。

Conclusion: 该框架支持严格的隐私保护治疗对话研究，为研究和临床转化提供了稳健可复现的基础。

Abstract: Substance use disorders (SUDs) affect over 36 million people worldwide, yet
few receive effective care due to stigma, motivational barriers, and limited
personalized support. Although large language models (LLMs) show promise for
mental-health assistance, most systems lack tight integration with clinically
validated strategies, reducing effectiveness in addiction recovery. We present
ChatThero, a multi-agent conversational framework that couples dynamic patient
modeling with context-sensitive therapeutic dialogue and adaptive persuasive
strategies grounded in cognitive behavioral therapy (CBT) and motivational
interviewing (MI). We build a high-fidelity synthetic benchmark spanning Easy,
Medium, and Hard resistance levels, and train ChatThero with a two-stage
pipeline comprising supervised fine-tuning (SFT) followed by direct preference
optimization (DPO). In evaluation, ChatThero yields a 41.5\% average gain in
patient motivation, a 0.49\% increase in treatment confidence, and resolves
hard cases with 26\% fewer turns than GPT-4o, and both automated and human
clinical assessments rate it higher in empathy, responsiveness, and behavioral
realism. The framework supports rigorous, privacy-preserving study of
therapeutic conversation and provides a robust, replicable basis for research
and clinical translation.

</details>
