<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 16]
- [cs.CR](#cs.CR) [Total: 30]
- [cs.AI](#cs.AI) [Total: 38]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [E-Test: E'er-Improving Test Suites](https://arxiv.org/abs/2510.19860)
*Ketai Qiu,Luca Di Grazia,Leonardo Mariani,Mauro Pezzè*

Main category: cs.SE

TL;DR: E-Test是一种利用大型语言模型从生产环境中识别未测试执行场景并生成测试用例的方法，显著提高了测试套件的覆盖率和质量。


<details>
  <summary>Details</summary>
Motivation: 测试套件通常不完善，寻找超出现有测试套件范围的执行场景非常困难且耗时，特别是在管理大型测试套件时。

Method: E-Test利用LLMs从大量生产场景中识别当前测试套件未充分覆盖的场景，并生成新的测试用例来增强测试套件。

Result: 在1,975个场景的数据集上评估，E-Test的F1分数达到0.55，显著优于现有回归测试方法(0.34)和普通LLMs(0.39)。

Conclusion: E-Test能有效识别未测试的执行场景，减少维护测试套件所需的手动工作量，显著提升测试套件的质量。

Abstract: Test suites are inherently imperfect, and testers can always enrich a suite
with new test cases that improve its quality and, consequently, the reliability
of the target software system. However, finding test cases that explore
execution scenarios beyond the scope of an existing suite can be extremely
challenging and labor-intensive, particularly when managing large test suites
over extended periods.
  In this paper, we propose E-Test, an approach that reduces the gap between
the execution space explored with a test suite and the executions experienced
after testing by augmenting the test suite with test cases that explore
execution scenarios that emerge in production. E-Test (i) identifies executions
that have not yet been tested from large sets of scenarios, such as those
monitored during intensive production usage, and (ii) generates new test cases
that enhance the test suite. E-Test leverages Large Language Models (LLMs) to
pinpoint scenarios that the current test suite does not adequately cover, and
augments the suite with test cases that execute these scenarios.
  Our evaluation on a dataset of 1,975 scenarios, collected from highly-starred
open-source Java projects already in production and Defects4J, demonstrates
that E-Test retrieves not-yet-tested execution scenarios significantly better
than state-of-the-art approaches. While existing regression testing and field
testing approaches for this task achieve a maximum F1-score of 0.34, and
vanilla LLMs achieve a maximum F1-score of 0.39, E-Test reaches 0.55. These
results highlight the impact of E-Test in enhancing test suites by effectively
targeting not-yet-tested execution scenarios and reducing manual effort
required for maintaining test suites.

</details>


### [2] [SODBench: A Large Language Model Approach to Documenting Spreadsheet Operations](https://arxiv.org/abs/2510.19864)
*Amila Indika,Igor Molybog*

Main category: cs.SE

TL;DR: 本文提出了电子表格操作文档化(SOD)任务，构建了包含111个代码片段及其自然语言摘要的基准，评估了5个LLM在生成电子表格文档方面的性能。


<details>
  <summary>Details</summary>
Motivation: 电子表格在商业、会计和金融领域广泛使用，但缺乏系统化的文档方法，阻碍了自动化、协作和知识传递，导致关键机构知识丢失的风险。

Method: 提出了SOD任务，构建了包含111个电子表格操作代码片段及其对应自然语言摘要的基准数据集，使用BLEU、GLEU、ROUGE-L和METEOR指标评估了GPT-4o、GPT-4o-mini、LLaMA-3.3-70B、Mixtral-8x7B和Gemma2-9B五个LLM。

Result: 研究发现LLM能够生成准确的电子表格文档，使SOD成为增强电子表格可重现性、可维护性和协作工作流程的可行前提步骤。

Conclusion: LLM可以有效生成电子表格操作文档，但仍有挑战需要解决，SOD为实现更好的电子表格协作和知识管理提供了可行路径。

Abstract: Numerous knowledge workers utilize spreadsheets in business, accounting, and
finance. However, a lack of systematic documentation methods for spreadsheets
hinders automation, collaboration, and knowledge transfer, which risks the loss
of crucial institutional knowledge. This paper introduces Spreadsheet
Operations Documentation (SOD), an AI task that involves generating
human-readable explanations from spreadsheet operations. Many previous studies
have utilized Large Language Models (LLMs) for generating spreadsheet
manipulation code; however, translating that code into natural language for SOD
is a less-explored area. To address this, we present a benchmark of 111
spreadsheet manipulation code snippets, each paired with a corresponding
natural language summary. We evaluate five LLMs, GPT-4o, GPT-4o-mini,
LLaMA-3.3-70B, Mixtral-8x7B, and Gemma2-9B, using BLEU, GLEU, ROUGE-L, and
METEOR metrics. Our findings suggest that LLMs can generate accurate
spreadsheet documentation, making SOD a feasible prerequisite step toward
enhancing reproducibility, maintainability, and collaborative workflows in
spreadsheets, although there are challenges that need to be addressed.

</details>


### [3] [Knowledge-Guided Multi-Agent Framework for Application-Level Software Code Generation](https://arxiv.org/abs/2510.19868)
*Qian Xiong,Bo Yang,Weisong Sun,Yiran Zhang,Tianlin Li,Yang Liu,Zhi Jin*

Main category: cs.SE

TL;DR: KGACG是一个知识引导的应用级代码生成框架，通过多智能体协作将软件需求规范和架构设计文档转换为可执行代码。


<details>
  <summary>Details</summary>
Motivation: 现有方法在大规模应用级软件代码生成中表现不佳，难以确保合理的项目代码组织结构，且难以维护代码生成过程。

Method: 采用Code Organization & Planning Agent (COPA)、Coding Agent (CA)和Testing Agent (TA)三个智能体的协作闭环，结合反馈机制。

Result: 通过Java坦克大战游戏案例研究展示了KGACG中智能体的协作过程，同时面临挑战。

Conclusion: KGACG致力于推进应用级软件开发的自动化进程。

Abstract: Automated code generation driven by Large Lan- guage Models (LLMs) has
enhanced development efficiency, yet generating complex application-level
software code remains challenging. Multi-agent frameworks show potential, but
existing methods perform inadequately in large-scale application-level software
code generation, failing to ensure reasonable orga- nizational structures of
project code and making it difficult to maintain the code generation process.
To address this, this paper envisions a Knowledge-Guided Application-Level Code
Generation framework named KGACG, which aims to trans- form software
requirements specification and architectural design document into executable
code through a collaborative closed- loop of the Code Organization & Planning
Agent (COPA), Coding Agent (CA), and Testing Agent (TA), combined with a
feedback mechanism. We demonstrate the collaborative process of the agents in
KGACG in a Java Tank Battle game case study while facing challenges. KGACG is
dedicated to advancing the automation of application-level software
development.

</details>


### [4] [BugPilot: Complex Bug Generation for Efficient Learning of SWE Skills](https://arxiv.org/abs/2510.19898)
*Atharv Sonwane,Isadora White,Hyunji Lee,Matheus Pereira,Lucas Caccia,Minseon Kim,Zhengyan Shi,Chinmay Singh,Alessandro Sordoni,Marc-Alexandre Côté,Xingdi Yuan*

Main category: cs.SE

TL;DR: 提出了一种通过让软件工程代理在添加功能时意外破坏测试来生成高质量、多样化bug的新方法，相比故意生成bug的方法更接近真实开发过程，训练出的模型在SWE-bench上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 需要高质量的bug来训练下一代基于语言模型的软件工程代理，现有方法通过故意引入扰动生成bug，这不符合真实开发过程且会产生分布外效应。

Method: 指导软件工程代理向代码库添加功能，在此过程中可能意外破坏测试从而产生bug，这种方法更接近人类开发者的编辑模式。

Result: 生成的bug数据在监督微调中更高效，仅用1.2k个bug就比其他3k个bug的数据集性能高出2%；训练出的FrogBoss(32B)和FrogMini(14B)模型在SWE-bench Verified上分别达到54.6%和45.3%的pass@1。

Conclusion: 通过模拟真实开发过程生成bug的方法能产生更高质量的训练数据，显著提升软件工程代理的性能，为训练下一代AI编程助手提供了有效途径。

Abstract: High quality bugs are key to training the next generation of language model
based software engineering (SWE) agents. We introduce a novel method for
synthetic generation of difficult and diverse bugs. Our method instructs SWE
Agents to introduce a feature into the codebase whereby they may
unintentionally break tests, resulting in bugs. Prior approaches often induce
an out-of-distribution effect by generating bugs intentionally (e.g. by
introducing local perturbation to existing code), which does not reflect
realistic development processes. We perform qualitative analysis to demonstrate
that our approach for generating bugs more closely reflects the patterns found
in human-authored edits. Through extensive experiments, we demonstrate that our
bugs provide more efficient training data for supervised fine-tuning,
outperforming other bug datasets by 2% with half the training data (1.2k vs. 3k
bugs). We train on our newly generated bugs in addition to existing bug
datasets to get FrogBoss a state-of-the-art 32B parameter model on SWE-bench
Verified with a pass@1 of 54.6% and FrogMini a state-of-the-art 14B model on
SWE-bench Verified with a pass@1 of 45.3% on SWE-bench Verified averaged over
three seeds.

</details>


### [5] [On Interaction Effects in Greybox Fuzzing](https://arxiv.org/abs/2510.19984)
*Konstantinos Kitsios,Marcel Böhme,Alberto Bacchelli*

Main category: cs.SE

TL;DR: MuoFuzz是一种灰盒模糊测试工具，通过学习和选择最有希望的变异器序列来提高测试效率。


<details>
  <summary>Details</summary>
Motivation: 研究发现变异器应用顺序对灰盒模糊测试器的有效性有显著影响，传统方法使用固定选择概率或孤立优化每个变异器，未能充分利用变异器间的交互效应。

Method: MuoFuzz学习给定先前选择的变异器后，下一个变异器产生有趣输入的条件概率，然后使用随机游走从学习到的概率中采样生成变异器序列。

Result: 在FuzzBench和MAGMA基准测试中，MuoFuzz实现了最高的代码覆盖率，并发现了AFL++遗漏的4个bug以及AFL++和MOPT都遗漏的1个bug。

Conclusion: 通过学习变异器序列的交互效应，MuoFuzz比现有方法更有效地提高了模糊测试的覆盖率和bug发现能力。

Abstract: A greybox fuzzer is an automated software testing tool that generates new
test inputs by applying randomly chosen mutators (e.g., flipping a bit or
deleting a block of bytes) to a seed input in random order and adds all
coverage-increasing inputs to the corpus of seeds. We hypothesize that the
order in which mutators are applied to a seed input has an impact on the
effectiveness of greybox fuzzers. In our experiments, we fit a linear model to
a dataset that contains the effectiveness of all possible mutator pairs and
indeed observe the conjectured interaction effect. This points us to more
efficient fuzzing by choosing the most promising mutator sequence with a higher
likelihood. We propose MuoFuzz, a greybox fuzzer that learns and chooses the
most promising mutator sequences. MuoFuzz learns the conditional probability
that the next mutator will yield an interesting input, given the previously
selected mutator. Then, it samples from the learned probability using a random
walk to generate mutator sequences. We compare the performance of MuoFuzz to
AFL++, which uses a fixed selection probability, and MOPT, which optimizes the
selection probability of each mutator in isolation. Experimental results on the
FuzzBench and MAGMA benchmarks show that MuoFuzz achieves the highest code
coverage and finds four bugs missed by AFL++ and one missed by both AFL++ and
MOPT.

</details>


### [6] [A Framework for the Adoption and Integration of Generative AI in Midsize Organizations and Enterprises (FAIGMOE)](https://arxiv.org/abs/2510.19997)
*Abraham Itzhak Weinberg*

Main category: cs.SE

TL;DR: 本文提出了FAIGMOE框架，专门针对中小型组织和大型企业在生成式AI采用过程中的独特挑战，整合了技术采纳理论、组织变革管理和创新扩散视角。


<details>
  <summary>Details</summary>
Motivation: 现有技术采纳框架（如TAM、TOE、DOI理论）缺乏对生成式AI在不同规模组织中实施的针对性指导，无法解决中小型组织的资源限制和大型企业的组织复杂性等独特挑战。

Method: 开发了FAIGMOE概念框架，包含四个相互关联的阶段：战略评估、规划与用例开发、实施与集成、运营与优化，每个阶段都提供可扩展的指导。

Result: FAIGMOE是首个全面解决中小型和大型企业生成式AI采纳的概念框架，提供了可操作的实施方案、评估工具和治理模板。

Conclusion: 该框架填补了生成式AI采纳文献的关键空白，但需要通过未来研究进行实证验证。

Abstract: Generative Artificial Intelligence (GenAI) presents transformative
opportunities for organizations, yet both midsize organizations and larger
enterprises face distinctive adoption challenges. Midsize organizations
encounter resource constraints and limited AI expertise, while enterprises
struggle with organizational complexity and coordination challenges. Existing
technology adoption frameworks, including TAM (Technology Acceptance Model),
TOE (Technology Organization Environment), and DOI (Diffusion of Innovations)
theory, lack the specificity required for GenAI implementation across these
diverse contexts, creating a critical gap in adoption literature. This paper
introduces FAIGMOE (Framework for the Adoption and Integration of Generative AI
in Midsize Organizations and Enterprises), a conceptual framework addressing
the unique needs of both organizational types. FAIGMOE synthesizes technology
adoption theory, organizational change management, and innovation diffusion
perspectives into four interconnected phases: Strategic Assessment, Planning
and Use Case Development, Implementation and Integration, and
Operationalization and Optimization. Each phase provides scalable guidance on
readiness assessment, strategic alignment, risk governance, technical
architecture, and change management adaptable to organizational scale and
complexity. The framework incorporates GenAI specific considerations including
prompt engineering, model orchestration, and hallucination management that
distinguish it from generic technology adoption frameworks. As a perspective
contribution, FAIGMOE provides the first comprehensive conceptual framework
explicitly addressing GenAI adoption across midsize and enterprise
organizations, offering actionable implementation protocols, assessment
instruments, and governance templates requiring empirical validation through
future research.

</details>


### [7] [The Cost of Downgrading Build Systems: A Case Study of Kubernetes](https://arxiv.org/abs/2510.20041)
*Gareema Ranjan,Mahmoud Alfadel,Gengyi Sun,Shane McIntosh*

Main category: cs.SE

TL;DR: Kubernetes项目从Bazel降级到Go Build的案例研究显示，虽然Bazel构建速度更快，但内存占用更大，在降级后CI资源成本可能增加76%。


<details>
  <summary>Details</summary>
Motivation: 探索从基于工件的构建工具降级到语言特定解决方案的性能影响，因为团队可能因维护性考虑而放弃更快的构建工具。

Method: 对Kubernetes项目降级期间的完整和增量构建进行复现和分析，并在其他四个从Bazel降级的项目中复制研究。

Result: Bazel构建比Go Build快23.06-38.66分钟，但内存占用多81.42-351.07MB。在并行度高于8时CPU负载更大。降级可能使CI资源成本增加76%。

Conclusion: 放弃基于工件的构建工具虽然可能提高维护性，但对大型项目会带来显著的性能成本，需要权衡利弊。

Abstract: Since developers invoke the build system frequently, its performance can
impact productivity. Modern artifact-based build tools accelerate builds, yet
prior work shows that teams may abandon them for alternatives that are easier
to maintain. While prior work shows why downgrades are performed, the
implications of downgrades remain largely unexplored. In this paper, we
describe a case study of the Kubernetes project, focusing on its downgrade from
an artifact-based build tool (Bazel) to a language-specific solution (Go
Build). We reproduce and analyze the full and incremental builds of change sets
during the downgrade period. On the one hand, we find that Bazel builds are
faster than Go Build, completing full builds in 23.06-38.66 up to 75.19 impose
a larger memory footprint than Go Build of 81.42-351.07 respectively. Bazel
builds also impose a greater CPU load at parallelism settings above eight for
full builds and above one for incremental builds. We estimate that downgrading
from Bazel can increase CI resource costs by up to 76 explore whether our
observations generalize by replicating our Kubernetes study on four other
projects that also downgraded from Bazel to older build tools. We observe that
while build time penalties decrease, Bazel consistently consumes more memory.
We conclude that abandoning artifact-based build tools, despite perceived
maintainability benefits, tends to incur considerable performance costs for
large projects. Our observations may help stakeholders to balance trade-offs in
build tool adoption

</details>


### [8] [Developing a Model-Driven Reengineering Approach for Migrating PL/SQL Triggers to Java: A Practical Experience](https://arxiv.org/abs/2510.20121)
*Carlos J. Fernandez-Candel,Jesus Garcia-Molina,Francisco Javier Bermudez Ruiz,Jose Ramon Hoyos Barcelo,Diego Sevilla Ruiz,Benito Jose Cuesta Viera*

Main category: cs.SE

TL;DR: 提出了一种基于模型驱动重构的软件过程，用于将PL/SQL代码迁移到Java，集成TDD方法和三种代码验证机制。


<details>
  <summary>Details</summary>
Motivation: 现代软件技术发展促使企业需要迁移RAD平台应用（如Oracle Forms），将PL/SQL单体代码重构为分层的Java代码。

Method: 使用KDM模型表示遗留代码，采用类似TDD的方法增量开发模型转换，包含三种生成的代码验证机制。

Result: 开发了迁移工具，成功将PL/SQL代码转换为分层Java代码，并详细验证了MDE应用的相关问题。

Conclusion: 模型驱动重构方法能有效支持遗留系统迁移，集成的验证机制确保了生成代码的质量。

Abstract: Model-driven software engineering (MDE) techniques are not only useful in
forward engineering scenarios, but can also be successfully applied to evolve
existing systems. RAD (Rapid Application Development) platforms emerged in the
nineties, but the success of modern software technologies motivated that a
large number of enterprises tackled the migration of their RAD applications,
such as Oracle Forms. Our research group has collaborated with a software
company in developing a solution to migrate PL/SQL monolithic code on Forms
triggers and program units to Java code separated in several tiers.
  Our research focused on the model-driven reengineering process applied to
develop the migration tool for the conversion of PL/SQL code to Java. Legacy
code is represented in form of KDM (Knowledge-Discovery Metamodel) models. In
this paper, we propose a software process to implement a model-driven
re-engineering. This process integrates a TDD-like approach to incrementally
develop model transformations with three kinds of validations for the generated
code. The implementation and validation of the re-engineering approach are
explained in detail, as well as the evaluation of some issues related with the
application of MDE.

</details>


### [9] [Automated Cloud Infrastructure-as-Code Reconciliation with AI Agents](https://arxiv.org/abs/2510.20211)
*Zhenning Yang,Hui Guan,Victor Nicolet,Brandon Paulsen,Joey Dodds,Daniel Kroening,Ang Chen*

Main category: cs.SE

TL;DR: NSync是一个自动化基础设施即代码(IaC)协调系统，通过分析云API调用来检测基础设施漂移，并使用LLM推断意图并生成IaC更新，实现从97%的漂移场景中准确恢复。


<details>
  <summary>Details</summary>
Motivation: 当IaC与云控制台、CLI或SDK同时使用时，会失去对外部变化的可见性，导致基础设施漂移，使得配置过时，后续IaC操作可能撤销有效更新或触发错误。

Method: NSync从API跟踪中获取洞察，检测漂移（即非IaC变更）并进行协调（即更新IaC配置以捕获变更）。它采用智能代理架构，利用LLM从嘈杂的API序列中推断高层意图，使用专用工具合成有针对性的IaC更新，并通过自演进的知识库持续改进。

Result: 在5个真实世界的Terraform项目和372个漂移场景中的实验表明，NSync在准确性（从0.71提高到0.97 pass@3）和令牌效率（1.47倍改进）方面均优于基线。

Conclusion: NSync通过自动检测和协调基础设施漂移，有效解决了IaC与手动管理工具并行使用时的配置不一致问题，显著提高了基础设施管理的可靠性和效率。

Abstract: Cloud infrastructure is managed through a mix of interfaces -- traditionally,
cloud consoles, command-line interfaces (CLI), and SDKs are the tools of
choice. Recently, Infrastructure-as-Code/IaC frameworks (e.g., Terraform) have
quickly gained popularity. Unlike conventional tools, IaC~frameworks encode the
infrastructure in a "source-of-truth" configuration. They are capable of
automatically carrying out modifications to the cloud -- deploying, updating,
or destroying resources -- to bring the actual infrastructure into alignment
with the IaC configuration. However, when IaC is used alongside consoles, CLIs,
or SDKs, it loses visibility into external changes, causing infrastructure
drift, where the configuration becomes outdated, and later IaC operations may
undo valid updates or trigger errors.
  We present NSync, an automated system for IaC reconciliation that propagates
out-of-band changes back into the IaC program. Our key insight is that
infrastructure changes eventually all occur via cloud API invocations -- the
lowest layer for cloud management operations. NSync gleans insights from API
traces to detect drift (i.e., non-IaC changes) and reconcile it (i.e., update
the IaC configuration to capture the changes). It employs an agentic
architecture that leverages LLMs to infer high-level intents from noisy API
sequences, synthesize targeted IaC updates using specialized tools, and
continually improve through a self-evolving knowledge base of past
reconciliations. We further introduce a novel evaluation pipeline for injecting
realistic drifts into cloud infrastructure and assessing reconciliation
performance. Experiments across five real-world Terraform projects and 372
drift scenarios show that NSync outperforms the baseline both in terms of
accuracy (from 0.71 to 0.97 pass@3) and token efficiency (1.47$\times$
improvement).

</details>


### [10] [Classport: Designing Runtime Dependency Introspection for Java](https://arxiv.org/abs/2510.20340)
*Serena Cofano,Daniel Williams,Aman Sharma,Martin Monperrus*

Main category: cs.SE

TL;DR: Classport系统将依赖信息嵌入Java类文件，实现在运行时获取依赖信息，解决了Java缺乏运行时依赖自省能力的问题。


<details>
  <summary>Details</summary>
Motivation: Java缺乏运行时依赖自省能力，这对于软件供应链安全至关重要。需要能够观察程序执行期间正在使用的依赖项。

Method: 开发Classport系统，将依赖信息嵌入到Java类文件中，从而在运行时可以检索这些依赖信息。

Result: 在6个真实项目中评估Classport，证明其在运行时识别依赖项的可行性。

Conclusion: Classport的运行时依赖自省能力为运行时完整性检查开辟了重要途径。

Abstract: Runtime introspection of dependencies, i.e., the ability to observe which
dependencies are currently used during program execution, is fundamental for
Software Supply Chain security. Yet, Java has no support for it. We solve this
problem with Classport, a system that embeds dependency information into Java
class files, enabling the retrieval of dependency information at runtime. We
evaluate Classport on six real-world projects, demonstrating the feasibility in
identifying dependencies at runtime. Runtime dependency introspection with
Classport opens important avenues for runtime integrity checking.

</details>


### [11] [Symmetry in Software Platforms as an Architectural Principle](https://arxiv.org/abs/2510.20389)
*Bjorn Remseth*

Main category: cs.SE

TL;DR: 软件平台作为结构保持系统，通过强制执行结构规律性来增强架构鲁棒性


<details>
  <summary>Details</summary>
Motivation: 探索软件平台如何通过保持结构对称性来提升系统稳定性

Method: 将软件平台视为结构保持系统，分析其接口和行为在特定变换下的稳定性

Result: 发现强制执行结构规律性能够产生架构鲁棒性

Conclusion: 软件平台的架构鲁棒性源于对结构对称性的维护

Abstract: Software platforms often act as structure preserving systems. They provide
consistent interfaces and behaviors that remain stable under specific
transformations that we denote as symmetries. This paper explores the idea that
architectural robustness emerges from enforcing such structural regularities

</details>


### [12] [FMI-Based Distributed Co-Simulation with Enhanced Security and Intellectual Property Safeguards](https://arxiv.org/abs/2510.20403)
*Santiago Gil,Ecem E. Baş,Christian D. Jensen,Sebastian Engelsgaard,Giuseppe Abbiati,Cláudio Gomes*

Main category: cs.SE

TL;DR: 提出了一种基于UniFMU的分布式协同仿真方法，通过增强网络安全和IP保护机制，确保连接由客户端发起且模型和二进制文件驻留在可信平台上。


<details>
  <summary>Details</summary>
Motivation: 分布式协同仿真在保护知识产权的同时实现多方协作建模，但目前缺乏针对连续时间或混合系统的安全分布式仿真指南，存在潜在黑客攻击风险。

Method: 在UniFMU基础上构建分布式协同仿真框架，采用客户端发起连接的机制，确保模型和二进制文件仅驻留在可信平台上。

Result: 通过四个不同网络设置下的两个协同仿真演示验证了该方法的功能性，并分析了IP保护分发与性能效率之间的权衡关系。

Conclusion: 该方法成功实现了分布式协同仿真的网络安全和IP保护，为连续时间或混合系统的安全协作仿真提供了可行方案。

Abstract: Distributed co-simulation plays a key role in enabling collaborative modeling
and simulation by different stakeholders while protecting their Intellectual
Property (IP). Although IP protection is provided implicitly by co-simulation,
there is no consensus in the guidelines to conduct distributed co-simulation of
continuous-time or hybrid systems with no exposure to potential hacking
attacks. We propose an approach for distributed co-simulation on top of UniFMU
with enhanced cybersecurity and IP protection mechanisms, ensuring that the
connection is initiated by the client and the models and binaries live on
trusted platforms. We showcase the functionality of this approach using two
co-simulation demos in four different network settings and analyze the
trade-off between IP-protected distribution and performance efficiency in these
settings.

</details>


### [13] [Toward Practical Deductive Verification: Insights from a Qualitative Survey in Industry and Academia](https://arxiv.org/abs/2510.20514)
*Lea Salome Brugger,Xavier Denis,Peter Müller*

Main category: cs.SE

TL;DR: 该研究通过访谈30位验证从业者，识别了演绎验证成功应用的关键因素和阻碍其广泛采用的障碍，并提出了具体建议。


<details>
  <summary>Details</summary>
Motivation: 尽管演绎验证在特定项目中证明有效，但尚未成为主流技术。研究旨在探索促进演绎验证成功应用的因素和阻碍其广泛采用的根本问题。

Method: 采用半结构化访谈30位来自工业界和学术界的验证从业者，并使用主题分析方法系统分析收集的数据。

Result: 除了确认已知挑战（如进行形式化证明需要高水平专业知识），还发现了未充分探索的障碍，包括证明维护、自动化控制不足和可用性问题。

Conclusion: 研究结果用于提取演绎验证的促进因素和障碍，并为从业者、工具构建者和研究人员制定具体建议，包括可用性、自动化和与现有工作流集成的原则。

Abstract: Deductive verification is an effective method to ensure that a given system
exposes the intended behavior. In spite of its proven usefulness and
feasibility in selected projects, deductive verification is still not a
mainstream technique. To pave the way to widespread use, we present a study
investigating the factors enabling successful applications of deductive
verification and the underlying issues preventing broader adoption. We
conducted semi-structured interviews with 30 practitioners of verification from
both industry and academia and systematically analyzed the collected data
employing a thematic analysis approach. Beside empirically confirming familiar
challenges, e.g., the high level of expertise needed for conducting formal
proofs, our data reveal several underexplored obstacles, such as proof
maintenance, insufficient control over automation, and usability concerns. We
further use the results from our data analysis to extract enablers and barriers
for deductive verification and formulate concrete recommendations for
practitioners, tool builders, and researchers, including principles for
usability, automation, and integration with existing workflows.

</details>


### [14] [Large Language Models for Fault Localization: An Empirical Study](https://arxiv.org/abs/2510.20521)
*YingJian Xiao,RongQun Hu,WeiWei Gong,HongWei Li,AnQuan Jie*

Main category: cs.SE

TL;DR: 本文系统评估了LLMs在代码故障定位任务中的表现，比较了开源和闭源模型在不同提示策略下的性能，发现错误报告上下文能显著提升模型表现。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏对LLM在代码故障定位任务中的全面评估，而故障定位的准确性直接影响后续程序修复的效果。

Method: 在HumanEval-Java和Defects4J数据集上评估代表性开源和闭源模型，研究标准提示、少样本示例和思维链推理等不同提示策略对性能的影响。

Result: 错误报告上下文显著提升模型性能；少样本学习有改进潜力但存在边际收益递减；思维链推理效果高度依赖模型自身推理能力。

Conclusion: 研究揭示了不同模型在故障定位任务中的性能特征和权衡，为改进故障定位效果提供了有价值的见解。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in
code-related tasks, particularly in automated program repair. However, the
effectiveness of such repairs is highly dependent on the performance of
upstream fault localization, for which comprehensive evaluations are currently
lacking. This paper presents a systematic empirical study on LLMs in the
statement-level code fault localization task. We evaluate representative
open-source models (Qwen2.5-coder-32b-instruct, DeepSeek-V3) and closed-source
models (GPT-4.1 mini, Gemini-2.5-flash) to assess their fault localization
capabilities on the HumanEval-Java and Defects4J datasets. The study
investigates the impact of different prompting strategies--including standard
prompts, few-shot examples, and chain-of-reasoning--on model performance, with
a focus on analysis across accuracy, time efficiency, and economic cost
dimensions. Our experimental results show that incorporating bug report context
significantly enhances model performance. Few-shot learning shows potential for
improvement but exhibits noticeable diminishing marginal returns, while
chain-of-thought reasoning's effectiveness is highly contingent on the model's
inherent reasoning capabilities. This study not only highlights the performance
characteristics and trade-offs of different models in fault localization tasks,
but also offers valuable insights into the strengths of current LLMs and
strategies for improving fault localization effectiveness.

</details>


### [15] [A Soundness and Precision Benchmark for Java Debloating Tools](https://arxiv.org/abs/2510.20679)
*Jonas Klauke,Tom Ohlmer,Stefan Schott,Serena Elisa Ponta,Wolfram Fischer,Eric Bodden*

Main category: cs.SE

TL;DR: Deblometer是一个用于评估Java代码去冗余工具的微基准测试套件，包含59个测试用例。评估发现现有工具都会删除必要的程序结构，特别是动态类加载功能会导致所有工具都不完备。


<details>
  <summary>Details</summary>
Motivation: 现代软件开发大量依赖第三方库，但研究表明只有24.9%的依赖在运行时是必需的，这导致了代码冗余问题。需要开发去冗余工具来平衡精度（删除未使用构造）和完备性（保留必需构造）。

Method: 开发了Deblometer微基准测试套件，包含59个测试用例，每个用例都有手动标注的基本事实，指定了必需和冗余的类、方法和字段。用此评估了三个流行的Java去冗余工具：Deptrim、JShrink和ProGuard。

Result: 所有工具都会删除必需的程序结构，导致语义改变或执行崩溃。动态类加载功能在所有评估工具中都引入了不完备性。Deptrim保留更多冗余构造，ProGuard删除更多必需构造，JShrink因对注解支持有限而严重影响完备性。

Conclusion: 当前去冗余工具存在完备性问题，需要改进以确保去冗余软件的稳定性和可靠性。

Abstract: Modern software development reuses code by importing libraries as
dependencies. Software projects typically include an average of 36
dependencies, with 80% being transitive, meaning they are dependencies of
dependencies. Recent research indicates that only 24.9% of these dependencies
are required at runtime, and even within those, many program constructs remain
unused, adding unnecessary code to the project. This has led to the development
of debloating tools that remove unnecessary dependencies and program constructs
while balancing precision by eliminating unused constructs and soundness by
preserving all required constructs. To systematically evaluate this trade-off,
we developed Deblometer, a micro-benchmark consisting of 59 test cases designed
to assess support for various Java language features in debloating tools. Each
test case includes a manually curated ground truth specifying necessary and
bloated classes, methods, and fields, enabling precise measurement of soundness
and precision. Using Deblometer, we evaluated three popular Java debloating
tools: Deptrim, JShrink, and ProGuard. Our evaluation reveals that all tools
remove required program constructs, which results in changed semantics or
execution crashes. In particular, the dynamic class loading feature introduces
unsoundness in all evaluated tools. Our comparison shows that Deptrim retains
more bloated constructs, while ProGuard removes more required constructs.
JShrink's soundness is significantly affected by limited support for
annotations, which leads to corrupted debloated artifacts. These soundness
issues highlight the need to improve debloating tools to ensure stable and
reliable debloated software.

</details>


### [16] [Exploring Large Language Models for Access Control Policy Synthesis and Summarization](https://arxiv.org/abs/2510.20692)
*Adarsh Vatsa,Bethel Hall,William Eiers*

Main category: cs.SE

TL;DR: 本文探索了大型语言模型在访问控制策略生成和总结中的有效性，发现LLM能生成语法正确的策略但存在许可性问题，而在与符号方法结合分析现有策略时表现出潜力。


<details>
  <summary>Details</summary>
Motivation: 云计算的访问控制策略通常需要手动编写且容易出错，现有策略复杂难以精确分析。鉴于LLM在代码生成和总结方面的成功，研究其能否自动生成访问控制策略或帮助理解现有策略。

Method: 首先研究不同LLM在访问控制策略生成中的表现，然后引入基于语义的请求总结方法，利用LLM生成策略允许请求的精确特征描述。

Result: LLM能有效生成语法正确的策略，但非推理型LLM仅45.8%的时间生成与给定规范等价的策略，推理型LLM达到93.7%。在策略分析方面，LLM与符号方法结合显示出良好效果。

Conclusion: 虽然LLM在自动策略生成方面存在显著障碍，但与符号方法结合分析现有策略时显示出有前景的结果。

Abstract: Cloud computing is ubiquitous, with a growing number of services being hosted
on the cloud every day. Typical cloud compute systems allow administrators to
write policies implementing access control rules which specify how access to
private data is governed. These policies must be manually written, and due to
their complexity can often be error prone. Moreover, existing policies often
implement complex access control specifications and thus can be difficult to
precisely analyze in determining their behavior works exactly as intended.
Recently, Large Language Models (LLMs) have shown great success in automated
code synthesis and summarization. Given this success, they could potentially be
used for automatically generating access control policies or aid in
understanding existing policies. In this paper, we explore the effectiveness of
LLMs for access control policy synthesis and summarization. Specifically, we
first investigate diverse LLMs for access control policy synthesis, finding
that: although LLMs can effectively generate syntactically correct policies,
they have permissiveness issues, generating policies equivalent to the given
specification 45.8% of the time for non-reasoning LLMs, and 93.7% of the time
for reasoning LLMs. We then investigate how LLMs can be used to analyze
policies by introducing a novel semantic-based request summarization approach
which leverages LLMs to generate a precise characterization of the requests
allowed by a policy. Our results show that while there are significant hurdles
in leveraging LLMs for automated policy generation, LLMs show promising results
when combined with symbolic approaches in analyzing existing policies.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [17] [CourtGuard: A Local, Multiagent Prompt Injection Classifier](https://arxiv.org/abs/2510.19844)
*Isaac Wu,Michael Maslowski*

Main category: cs.CR

TL;DR: CourtGuard是一个本地运行的多智能体提示注入分类器，采用法庭式多智能体LLM系统来检测恶意提示，相比Direct Detector具有更低的误报率。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在敏感应用中的集成，提示注入攻击风险日益增加，可能导致数据泄露、错误信息传播等有害行为，需要有效的防御机制。

Method: 采用法庭式多智能体系统：辩护律师模型论证提示无害，检察官模型论证提示为注入攻击，法官模型给出最终分类。

Result: CourtGuard比Direct Detector具有更低的误报率，但总体上检测效果较差。多智能体系统在对抗提示注入攻击方面显示出潜力。

Conclusion: 多智能体系统可作为对抗提示注入攻击的有效防御手段，考虑对抗性和良性场景对提示分类很重要。

Abstract: As large language models (LLMs) become integrated into various sensitive
applications, prompt injection, the use of prompting to induce harmful
behaviors from LLMs, poses an ever increasing risk. Prompt injection attacks
can cause LLMs to leak sensitive data, spread misinformation, and exhibit
harmful behaviors. To defend against these attacks, we propose CourtGuard, a
locally-runnable, multiagent prompt injection classifier. In it, prompts are
evaluated in a court-like multiagent LLM system, where a "defense attorney"
model argues the prompt is benign, a "prosecution attorney" model argues the
prompt is a prompt injection, and a "judge" model gives the final
classification. CourtGuard has a lower false positive rate than the Direct
Detector, an LLM as-a-judge. However, CourtGuard is generally a worse prompt
injection detector. Nevertheless, this lower false positive rate highlights the
importance of considering both adversarial and benign scenarios for the
classification of a prompt. Additionally, the relative performance of
CourtGuard in comparison to other prompt injection classifiers advances the use
of multiagent systems as a defense against prompt injection attacks. The
implementations of CourtGuard and the Direct Detector with full prompts for
Gemma-3-12b-it, Llama-3.3-8B, and Phi-4-mini-instruct are available at
https://github.com/isaacwu2000/CourtGuard.

</details>


### [18] [Can Reasoning Models Obfuscate Reasoning? Stress-Testing Chain-of-Thought Monitorability](https://arxiv.org/abs/2510.19851)
*Artur Zolkowski,Wen Xing,David Lindner,Florian Tramèr,Erik Jenner*

Main category: cs.CR

TL;DR: 论文研究模型是否能在追求对抗目标时通过模糊化思维链来逃避检测，发现思维链监控在无压力时有效，但在强对抗压力下可能失效。


<details>
  <summary>Details</summary>
Motivation: 研究模型是否能够通过模糊化思维链来隐藏其对抗性目标并逃避检测，以测试思维链监控的可靠性。

Method: 开发可组合和可量化的提示分类法来引发思维链模糊化，在玩具任务和SHADE-Arena环境中评估内部和外部思维链。

Result: 无模糊化压力时思维链监控准确高效；强对抗压力下部分模型能完成任务并逃避检测；模型对内部思维链的模糊化程度低于外部思维链。

Conclusion: 思维链在良性环境中提供有效监督，但稳健部署需要对模型的可监控性进行特定压力测试。

Abstract: Recent findings suggest that misaligned models may exhibit deceptive
behavior, raising concerns about output trustworthiness. Chain-of-thought (CoT)
is a promising tool for alignment monitoring: when models articulate their
reasoning faithfully, monitors can detect and mitigate harmful behaviors before
undesirable outcomes occur. However, a key uncertainty is: Can models obfuscate
their CoT in order to pursue hidden adversarial objectives while evading
detection? To answer this question and thus stress-test CoT monitorability, we
develop a composable and quantifiable taxonomy of prompts to elicit CoT
obfuscation. We evaluate both internal CoT (reasoning traces) and external CoT
(prompted reasoning in outputs) using toy tasks and more realistic environments
in SHADE-Arena. We show that: (i) CoT monitoring performs accurately and
efficiently without obfuscation pressure. (ii) Under strong obfuscation
pressure, some models successfully complete adversarial tasks while evading
detection. (iii) Models do not obfuscate their internal CoT as much as their
external CoT (under prompt pressure). These results suggest that while CoT
provides valuable oversight in benign settings, robust deployment requires
model-specific stress-testing of monitorability.

</details>


### [19] [Model Context Contracts - MCP-Enabled Framework to Integrate LLMs With Blockchain Smart Contracts](https://arxiv.org/abs/2510.19856)
*Eranga Bandara,Sachin Shetty,Ravi Mukkamala,Ross Gore,Peter Foytik,Safdar H. Bouk,Abdul Rahman,Xueping Liang,Ng Wee Keong,Kasun De Zoysa,Aruna Withanage,Nilaan Loganathan*

Main category: cs.CR

TL;DR: 提出了MCC框架，通过类似MCP的协议让LLM直接与区块链智能合约交互，使用户能用自然语言与区块链系统交互和执行交易。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏将LLM应用与区块链无缝集成的标准化方法，需要解决AI代理与区块链智能合约之间的交互问题。

Method: 开发MCC框架，通过微调LLM使用自定义数据集来准确解释自然语言输入并映射到相应的MCP函数，在Rahasak区块链上实现端到端原型。

Result: 微调过程显著提高了平台的性能和准确性，LLM能够准确识别自然语言输入并执行相应的交易。

Conclusion: MCC是首个使用模型上下文协议概念将LLM与区块链集成的方法，实现了用户通过自然语言与区块链系统的动态交互。

Abstract: In recent years, blockchain has experienced widespread adoption across
various industries, becoming integral to numerous enterprise applications.
Concurrently, the rise of generative AI and LLMs has transformed human-computer
interactions, offering advanced capabilities in understanding and generating
human-like text. The introduction of the MCP has further enhanced AI
integration by standardizing communication between AI systems and external data
sources. Despite these advancements, there is still no standardized method for
seamlessly integrating LLM applications and blockchain. To address this
concern, we propose "MCC: Model Context Contracts" a novel framework that
enables LLMs to interact directly with blockchain smart contracts through
MCP-like protocol. This integration allows AI agents to invoke blockchain smart
contracts, facilitating more dynamic and context-aware interactions between
users and blockchain networks. Essentially, it empowers users to interact with
blockchain systems and perform transactions using queries in natural language.
Within this proposed architecture, blockchain smart contracts can function as
intelligent agents capable of recognizing user input in natural language and
executing the corresponding transactions. To ensure that the LLM accurately
interprets natural language inputs and maps them to the appropriate MCP
functions, the LLM was fine-tuned using a custom dataset comprising user inputs
paired with their corresponding MCP server functions. This fine-tuning process
significantly improved the platform's performance and accuracy. To validate the
effectiveness of MCC, we have developed an end-to-end prototype implemented on
the Rahasak blockchain with the fine-tuned Llama-4 LLM. To the best of our
knowledge, this research represents the first approach to using the concept of
Model Context Protocol to integrate LLMs with blockchain.

</details>


### [20] [Cyberattack Detection in Critical Infrastructure and Supply Chains](https://arxiv.org/abs/2510.19859)
*Smita Khapre*

Main category: cs.CR

TL;DR: 提出将密集神经网络与数据增强技术相结合的方法，用于检测工业4.0中关键基础设施和供应链的零日网络攻击，解决传统入侵检测系统无法识别未知攻击的问题。


<details>
  <summary>Details</summary>
Motivation: 工业4.0中关键基础设施和供应链的网络安全面临挑战，传统入侵检测系统只能基于已知签名和模式检测攻击，无法识别零日攻击。网络流量数据集高度不平衡，少数类仅占数据集的0.000004%，而多数类超过80%，数据平衡成为关键问题。

Method: 集成密集神经网络与数据增强技术，使用合成少数类过采样技术来平衡高度不平衡的网络流量数据，使入侵检测系统能够智能自学习并高精度检测新型攻击。

Result: 使用平衡测试数据时获得了较高准确率，但在原始不平衡测试数据上准确率较低，表明存在过拟合问题。与最先进研究的比较显示，对于原始数据集，类别的分类效果仍然较差。

Conclusion: 高度不平衡的网络流量数据集需要不同的数据增强方法，当前的数据平衡方法在处理极端不平衡数据时效果有限，需要进一步改进数据增强策略。

Abstract: Cyberattack detection in Critical Infrastructure and Supply Chains has become
challenging in Industry 4.0. Intrusion Detection Systems (IDS) are deployed to
counter the cyberattacks. However, an IDS effectively detects attacks based on
the known signatures and patterns, Zero-day attacks go undetected. To overcome
this drawback in IDS, the integration of a Dense Neural Network (DNN) with Data
Augmentation is proposed. It makes IDS intelligent and enables it to self-learn
with high accuracy when a novel attack is encountered. The network flow
captures datasets are highly imbalanced same as the real network itself. The
Data Augmentation plays a crucial role in balancing the data. The balancing of
data is challenging as the minority class is as low as 0.000004\% of the
dataset, and the abundant class is higher than 80\% of the dataset. Synthetic
Minority Oversampling Technique is used for balancing the data. However, higher
accuracies are achieved with balanced test data, lower accuracies are
noticeable with the original imbalanced test data suggesting overfitting. A
comparison with state-of-the-art research using Synthetic Minority Oversampling
Technique with Edited Nearest Neighbor shows the classification of classes
remains poor for the original dataset. This suggests highly imbalanced datasets
of network flow require a different method of data augmentation.

</details>


### [21] [Policy-Governed RAG - Research Design Study](https://arxiv.org/abs/2510.19877)
*Jean-Marie Le Ray*

Main category: cs.CR

TL;DR: 提出了一种政策治理的RAG架构，用于受监管工作流程中的可审计生成，通过三个组件确保输出符合政策、可验证来源和提供合规证明。


<details>
  <summary>Details</summary>
Motivation: 在医药、金融、法律等受监管领域，错误成本高昂且审计追踪是法规要求，现有RAG系统缺乏可审计的政策检查和可验证的证据绑定。

Method: 采用三部分架构：(I)合同/控制组件确保输出符合政策；(II)清单/追踪组件加密锚定引用来源；(III)收据/验证组件提供便携合规证明。

Result: 设定了未经验证的目标：自信错误相对减少≥20%；p95延迟≤900ms；服务成本≤2.2倍；使用NO-GO门进行预注册试点。

Conclusion: 该设计通过使政策检查可审计、可重放和收据支持，补充了现有RAG/护栏系统，特别适用于高合规要求的领域。

Abstract: A policy-governed RAG architecture is specified for audit-ready generation in
regulated workflows, organized as a triptych: (I) Contracts/Control
(SHRDLU-like), which governs output adherence to legal and internal policies;
(II) Manifests/Trails (Memex-like), which cryptographically anchors all cited
source evidence to ensure verifiable provenance; and (III)
Receipts/Verification (Xanadu-like), which provides the final, portable proof
of compliance for auditors (portable COSE/JOSE) (see Section 4 and Appendix A).
Rather than explaining model internals, outputs are gated ex-ante and bound to
cryptographically verifiable evidence for each material answer. Unvalidated
targets are stated (>=20% relative reduction in confident errors; p95 latency
<= 900 ms; <= 2.2x serve cost) together with a pre-registered (optional) pilot
using NO-GO gates. The design complements existing RAG/guardrails by making
policy checks auditable, replayable, and receipt-backed. Target domains include
back-office compliance in pharma, medical devices, finance, legal, and the
public sector where error costs may exceed thousands of euros and audit trails
are mandatory under regulations such as the EU AI Act. Future evaluations may
pre-commit to publishing negative results when any example NO-GO gate is not
met.

</details>


### [22] [A Proactive Insider Threat Management Framework Using Explainable Machine Learning](https://arxiv.org/abs/2510.19883)
*Selma Shikonde,Mike Wa Nkongolo*

Main category: cs.CR

TL;DR: 提出了IT-XML框架，结合CRISP-DM和隐马尔可夫模型来增强内部威胁管理，通过调查问卷评估员工知识，实现91.7%的分类准确率。


<details>
  <summary>Details</summary>
Motivation: 内部威胁持续是重大挑战，特别是在网络安全基础设施、专业知识和法规仍在发展的行业中，需要增强主动的内部威胁管理和决策能力。

Method: 采用定量方法，使用在线问卷评估员工对内部威胁模式、访问控制、隐私实践和现有政策的了解；整合CRISP-DM与隐马尔可夫模型进行安全成熟度分类。

Result: 框架将所有组织分类为发展中安全成熟度水平，置信度97-98%，分类准确率91.7%；识别审计日志访问限制为最关键控制；随机森林分析显示供应商违规通知和定期审计日志审查是韧性的关键决定因素。

Conclusion: SHAP和LIME等可解释性方法提高了模型透明度，证明该框架有潜力加强内部威胁管理实践。

Abstract: Over the years, the technological landscape has evolved, reshaping the
security posture of organisations and increasing their exposure to
cybersecurity threats, many originating from within. Insider threats remain a
major challenge, particularly in sectors where cybersecurity infrastructure,
expertise, and regulations are still developing. This study proposes the
Insider Threat Explainable Machine Learning (IT-XML) framework, which
integrates the Cross-Industry Standard Process for Data Mining (CRISP-DM) with
Hidden Markov Models (HMM) to enhance proactive insider threat management and
decision-making. A quantitative approach is adopted using an online
questionnaire to assess employees' knowledge of insider threat patterns, access
control, privacy practices, and existing policies across three large
data-sensitive organisations. The IT-XML framework provides assessment
capabilities through survey-based data, HMM-driven pattern recognition for
security maturity classification, and evidence-based recommendations for
proactive threat mitigation. The framework classified all organisations at the
developing security maturity level with 97-98% confidence and achieved a
classification accuracy of 91.7%, identifying audit log access limits as the
most critical control. Random Forest analysis highlighted vendor breach
notifications (0.081) and regular audit log reviews (0.052) as key determinants
of resilience. Explainability methods such as SHAP and LIME improved model
transparency and interpretability, demonstrating the framework's potential to
strengthen insider threat management practices.

</details>


### [23] [Analysis and Comparison of Known and Randomly Generated S-boxes for Block Ciphers](https://arxiv.org/abs/2510.19885)
*James Kim*

Main category: cs.CR

TL;DR: 比较数学构造与随机生成的S盒性能，评估随机方法的相对弱点，并在简单SPN结构中建立随机置换的平均性能指标


<details>
  <summary>Details</summary>
Motivation: 数学构造的S盒具有可证明的密码学特性，而完全随机方法缺乏理论保证但复杂度低且结果多样，需要比较两者的相对性能

Method: 在简单SPN设置中比较数学构造与随机生成的S盒，包括随机置换和带循环约束的随机置换，并建立平均性能指标

Result: 数学构造的S盒具有高非线性、APN特性和平衡雪崩特性，而随机方法缺乏理论保证但复杂度较低

Conclusion: 数学构造方法能生成数千个具有强密码学特性的S盒，而随机方法虽然复杂度低但性能相对较弱

Abstract: Mathematically constructed S-boxes arise from algebraic structures and finite
field theory to ensure strong, provable cryptographic properties. These
mathematically grounded constructions allow for generation of thousands of
S-Boxes with high nonlinearity, APN properties, and balanced avalanche
characteristics, unlike fully random methods, which lack such theoretical
guarantees in exchange for low complexity and more varied results. In this
work, we compare mathematically constructed constructions with randomly
generated ones to evaluate the relative weakness of the latter. We also
establish an average measure of performance for randomly generated
permutations, as well as random with forced cycle constraints, and compare them
to well-established designs in a simple SPN setting.

</details>


### [24] [Deep Sequence-to-Sequence Models for GNSS Spoofing Detection](https://arxiv.org/abs/2510.19890)
*Jan Zelinka,Oliver Kost,Marek Hrúz*

Main category: cs.CR

TL;DR: 提出了一个数据生成框架来模拟欺骗攻击，并使用LSTM和Transformer架构进行在线欺骗检测，最佳模型错误率仅为0.16%。


<details>
  <summary>Details</summary>
Motivation: 需要开发有效的欺骗攻击检测方法来保护系统安全，特别是在全球范围内随机分布的欺骗场景下。

Method: 使用数据生成框架模拟欺骗攻击，应用LSTM和Transformer架构进行在线检测，采用早期融合输入策略。

Result: 深度学习模型能准确区分欺骗信号和真实信号，Transformer架构配合早期融合输入获得最佳性能，错误率仅为0.16%。

Conclusion: 深度学习模型特别是Transformer架构在欺骗检测方面表现出色，具有高精度和实用性。

Abstract: We present a data generation framework designed to simulate spoofing attacks
and randomly place attack scenarios worldwide. We apply deep neural
network-based models for spoofing detection, utilizing Long Short-Term Memory
networks and Transformer-inspired architectures. These models are specifically
designed for online detection and are trained using the generated dataset. Our
results demonstrate that deep learning models can accurately distinguish
spoofed signals from genuine ones, achieving high detection performance. The
best results are achieved by Transformer-inspired architectures with early
fusion of the inputs resulting in an error rate of 0.16%.

</details>


### [25] [Designing a Secure and Resilient Distributed Smartphone Participant Data Collection System](https://arxiv.org/abs/2510.19938)
*Foad Namjoo,Neng Wan,Devan Mallory,Yuyi Chang,Nithin Sugavanam,Long Yin Lee,Ning Xiong,Emre Ertin,Jeff M. Phillips*

Main category: cs.CR

TL;DR: MotionPI是一个基于智能手机的系统，用于通过传感器和调查收集行为和健康数据，支持被动数据收集和生态瞬时评估调查，适用于现实生活中的限制条件。


<details>
  <summary>Details</summary>
Motivation: 现实世界的健康研究需要从移动和可穿戴设备持续安全地收集数据，但面临电池寿命有限、网络连接弱、用户监督少等挑战。

Method: 集成被动数据收集（GPS、腕带运动数据）与生态瞬时评估调查，支持随机或基于身体活动触发，使用蓝牙低能耗连接腕带设备，数据本地和云端加密存储传输。

Result: MotionPI展示了一个实用解决方案，能够在网络物理健康研究中实现安全、可扩展的移动数据收集。

Conclusion: MotionPI为在现实生活约束下进行安全和可扩展的移动数据收集提供了一个实用解决方案。

Abstract: Real-world health studies require continuous and secure data collection from
mobile and wearable devices. We introduce MotionPI, a smartphone-based system
designed to collect behavioral and health data through sensors and surveys with
minimal interaction from participants. The system integrates passive data
collection (such as GPS and wristband motion data) with Ecological Momentary
Assessment (EMA) surveys, which can be triggered randomly or based on physical
activity. MotionPI is designed to work under real-life constraints, including
limited battery life, weak or intermittent cellular connection, and minimal
user supervision. It stores data both locally and on a secure cloud server,
with encrypted transmission and storage. It integrates through Bluetooth Low
Energy (BLE) into wristband devices that store raw data and communicate motion
summaries and trigger events. MotionPI demonstrates a practical solution for
secure and scalable mobile data collection in cyber-physical health studies.

</details>


### [26] [Q-RAN: Quantum-Resilient O-RAN Architecture](https://arxiv.org/abs/2510.19968)
*Vipin Rathi,Lakshya Chopra,Madhav Agarwal,Nitin Rajput,Kriish Sharma,Sushant Mundepi,Shivam Gangwar,Rudraksh Rawal,Jishan*

Main category: cs.CR

TL;DR: 提出Q-RAN框架，使用NIST标准化的后量子密码学来保护开放无线接入网络免受量子计算威胁


<details>
  <summary>Details</summary>
Motivation: 电信行业面临O-RAN架构转型和量子计算威胁的双重挑战，当前公钥密码学易被量子计算机破解，HNDL攻击策略使威胁迫在眉睫

Method: 实现ML-KEM和ML-DSA后量子密码算法，集成量子随机数生成器，部署PQ-IPsec、PQ-DTLS和PQ-mTLS协议，在SMO框架内建立集中式后量子证书颁发机构

Result: 开发了完整的量子安全框架，保护O-RAN网络所有接口，为分解式O-RAN生态系统提供量子安全路线图

Conclusion: Q-RAN框架为O-RAN网络提供了全面的量子安全保护方案，能够有效应对量子计算带来的密码学威胁

Abstract: The telecommunications industry faces a dual transformation: the
architectural shift toward Open Radio Access Networks (O-RAN) and the emerging
threat from quantum computing. O-RAN disaggregated, multi-vendor architecture
creates a larger attack surface vulnerable to crypt-analytically relevant
quantum computers(CRQCs) that will break current public key cryptography. The
Harvest Now, Decrypt Later (HNDL) attack strategy makes this threat immediate,
as adversaries can intercept encrypted data today for future decryption. This
paper presents Q-RAN, a comprehensive quantum-resistant security framework for
O-RAN networks using NIST-standardized Post-Quantum Cryptography (PQC). We
detail the implementation of ML-KEM (FIPS 203) and ML-DSA (FIPS 204),
integrated with Quantum Random Number Generators (QRNG) for cryptographic
entropy. The solution deploys PQ-IPsec, PQ-DTLS, and PQ-mTLS protocols across
all O-RAN interfaces, anchored by a centralized Post-Quantum Certificate
Authority (PQ-CA) within the SMO framework. This work provides a complete
roadmap for securing disaggregated O-RAN ecosystems against quantum
adversaries.

</details>


### [27] [SecureInfer: Heterogeneous TEE-GPU Architecture for Privacy-Critical Tensors for Large Language Model Deployment](https://arxiv.org/abs/2510.19979)
*Tushar Nayan,Ziqi Zhang,Ruimin Sun*

Main category: cs.CR

TL;DR: SecureInfer是一个混合框架，利用异构可信执行环境（TEEs）-GPU架构来保护LLMs免受模型提取攻击，同时保持高性能推理。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在移动和边缘平台的部署增加，保护模型隐私同时不牺牲不可信AI加速器（如GPU）的性能优势成为一个关键挑战。

Method: 采用信息论和威胁感知的分割策略：安全敏感组件（非线性层、注意力头投影、FNN变换和LoRA适配器）在SGX enclave内执行，而其他线性操作（矩阵乘法）在GPU上加密执行并在enclave内安全恢复。

Result: 使用LLaMA-2模型的实现评估显示，SecureInfer在提供强大安全保证的同时保持了合理的性能。

Conclusion: SecureInfer为安全的设备端模型推理提供了一个实用的解决方案。

Abstract: With the increasing deployment of Large Language Models (LLMs) on mobile and
edge platforms, securing them against model extraction attacks has become a
pressing concern. However, protecting model privacy without sacrificing the
performance benefits of untrusted AI accelerators, such as GPUs, presents a
challenging trade-off. In this paper, we initiate the study of high-performance
execution on LLMs and present SecureInfer, a hybrid framework that leverages a
heterogeneous Trusted Execution Environments (TEEs)-GPU architecture to isolate
privacy-critical components while offloading compute-intensive operations to
untrusted accelerators. Building upon an outsourcing scheme, SecureInfer adopts
an information-theoretic and threat-informed partitioning strategy:
security-sensitive components, including non-linear layers, projection of
attention head, FNN transformations, and LoRA adapters, are executed inside an
SGX enclave, while other linear operations (matrix multiplication) are
performed on the GPU after encryption and are securely restored within the
enclave. We implement a prototype of SecureInfer using the LLaMA-2 model and
evaluate it across performance and security metrics. Our results show that
SecureInfer offers strong security guarantees with reasonable performance,
offering a practical solution for secure on-device model inference.

</details>


### [28] [QORE : Quantum Secure 5G/B5G Core](https://arxiv.org/abs/2510.19982)
*Vipin Rathi,Lakshya Chopra,Rudraksh Rawal,Nitin Rajput,Shiva Valia,Madhav Aggarwal,Aditya Gairola*

Main category: cs.CR

TL;DR: QORE是一个量子安全的5G和B5G核心框架，使用NIST标准化的ML-KEM和ML-DSA算法，为5G核心网络功能和用户设备向后量子密码学过渡提供清晰路径。


<details>
  <summary>Details</summary>
Motivation: 当前5G系统的加密基础（RSA、ECC、DH）都容易受到Shor算法的量子攻击，保护5G网络免受未来量子攻击已成为紧迫的工程和研究重点。

Method: 采用NIST标准化的基于格的算法ML-KEM和ML-DSA，并将其应用于5G服务架构，同时提出混合PQC配置以在迁移期间保持互操作性。

Result: 实验验证显示ML-KEM在实现量子安全的同时仅产生轻微性能开销，满足运营商级5G系统的低延迟和高吞吐量要求。

Conclusion: 该路线图与3GPP和NIST的后量子网络标准化工作保持一致，为缓解量子时代风险、保护网络数据的长期机密性和完整性提供实用指导。

Abstract: Quantum computing is reshaping the security landscape of modern
telecommunications. The cryptographic foundations that secure todays 5G
systems, including RSA, Elliptic Curve Cryptography (ECC), and Diffie-Hellman
(DH), are all susceptible to attacks enabled by Shors algorithm. Protecting 5G
networks against future quantum adversaries has therefore become an urgent
engineering and research priority. In this paper we introduce QORE, a
quantum-secure 5G and Beyond 5G (B5G) Core framework that provides a clear
pathway for transitioning both the 5G Core Network Functions and User Equipment
(UE) to Post-Quantum Cryptography (PQC). The framework uses the
NIST-standardized lattice-based algorithms Module-Lattice Key Encapsulation
Mechanism (ML-KEM) and Module-Lattice Digital Signature Algorithm (ML-DSA) and
applies them across the 5G Service-Based Architecture (SBA). A Hybrid PQC
(HPQC) configuration is also proposed, combining classical and quantum-safe
primitives to maintain interoperability during migration. Experimental
validation shows that ML-KEM achieves quantum security with minor performance
overhead, meeting the low-latency and high-throughput requirements of
carrier-grade 5G systems. The proposed roadmap aligns with ongoing 3GPP SA3 and
SA5 study activities on the security and management of post-quantum networks as
well as with NIST PQC standardization efforts, providing practical guidance for
mitigating quantum-era risks while safeguarding long-term confidentiality and
integrity of network data.

</details>


### [29] [zk-Agreements: A Privacy-Preserving Way to Establish Deterministic Trust in Confidential Agreements](https://arxiv.org/abs/2510.20007)
*To-Wen Liu,Matthew Green*

Main category: cs.CR

TL;DR: zk-agreements协议通过结合零知识证明、安全多方计算和智能合约，在区块链上实现隐私保护的自动化可执行协议，解决传统纸质合同和现有智能合约在隐私与透明度之间的冲突。


<details>
  <summary>Details</summary>
Motivation: 传统纸质合同存在自动化、可执行性和争议解决的瓶颈，而现有区块链智能合约会暴露敏感协议条款，存在隐私和商业机密泄露风险，限制了企业采用。

Method: 结合零知识证明保护私有协议条款，使用安全多方计算实现私有合规评估，利用智能合约保证自动化执行，实现隐私保护与计算可执行性的平衡。

Result: 设计出能够同时实现隐私保护和计算可执行性的协议，解决了区块链协议中透明度与机密性之间的根本矛盾。

Conclusion: zk-agreements协议成功实现了从纸质信任向加密信任的转变，在保持机密性的同时提供了自动化的执行保障，为数字商务提供了可行的解决方案。

Abstract: Digital transactions currently exceed trillions of dollars annually, yet
traditional paper-based agreements remain a bottleneck for automation,
enforceability, and dispute resolution. Natural language contracts introduce
ambiguity, require manual processing, and lack computational verifiability, all
of which hinder efficient digital commerce. Computable legal contracts,
expressed in machine-readable formats, offer a potential solution by enabling
automated execution and verification. Blockchain-based smart contracts further
strengthen enforceability and accelerate dispute resolution; however, current
implementations risk exposing sensitive agreement terms on public ledgers,
raising serious privacy and competitive intelligence concerns that limit
enterprise adoption.
  We introduce zk-agreements, a protocol designed to transition from
paper-based trust to cryptographic trust while preserving confidentiality. Our
design combines zero-knowledge proofs to protect private agreement terms,
secure two-party computation to enable private compliance evaluation, and smart
contracts to guarantee automated enforcement. Together, these components
achieve both privacy preservation and computational enforceability, resolving
the fundamental tension between transparency and confidentiality in
blockchain-based agreements.

</details>


### [30] [Ultra-Fast Wireless Power Hacking](https://arxiv.org/abs/2510.20056)
*Hui Wang,Hans D. Schotten,Stefan M. Goetz*

Main category: cs.CR

TL;DR: 本文提出了一种改进的无线充电系统攻击方法，能够在高频信号三个周期内完成入侵和能量窃取，无需电路参数或查找表，可窃取超过76%的功率。


<details>
  <summary>Details</summary>
Motivation: 电动汽车无线充电技术发展迅速，但存在严重网络安全问题。现有攻击方法需要时间跟踪频率或精确调整电路参数，对抗快速频率变化或电容漂移效果有限。

Method: 通过将接收器电流与磁传感器电压相位同步，无需电路参数或查找表，在三个高频信号周期内完成入侵和能量窃取。使用MATLAB/Simulink仿真、有限元分析和实验验证。

Result: 改进方法可在相同条件下窃取完全谐振接收器接收功率的76%以上，证明简单的频率变化功率加密对此类威胁保护有限。

Conclusion: 无线充电系统面临严重安全威胁，简单的频率加密不足以提供有效保护，需要更强大的安全机制来应对此类快速攻击。

Abstract: The rapid growth of electric vehicles (EVs) has driven the development of
roadway wireless charging technology, effectively extending EV driving range.
However, wireless charging introduces significant cybersecurity challenges. Any
receiver within the magnetic field can potentially extract energy, and previous
research demonstrated that a hacker could detect the operating frequency and
steal substantial power. However, our approach required time to track new
frequencies or precise adjustments of inductance and capacitance, which would
be less effective against potential rapid transmitter frequency changes or
capacitance drift. As a solution, we enhanced the interceptor and enabled it to
intrude as well as steal energy within just three cycles of the high-frequency
signal. Moreover, it can work without any circuit parameters or look-up tables.
The key innovation is synchronizing the receiver current with the phase of the
magnetic sensor voltage. Through MATLAB / Simulink simulations, finite-element
analysis, and experimental validation, we demonstrated that our improved method
can steal over 76% of the power received by a fully resonant receiver under
identical conditions. This attack demonstrates that simple frequency-changing
power encryption offers limited protection against such threats.

</details>


### [31] [Who Coordinates U.S. Cyber Defense? A Co-Authorship Network Analysis of Joint Cybersecurity Advisories (2024--2025)](https://arxiv.org/abs/2510.20080)
*M. Abdullah Canbaz,Hakan Otal,Tugce Unlu,Nour Alhussein,Brian Nussbaum*

Main category: cs.CR

TL;DR: 通过分析9份联合网络安全咨询报告构建合作网络，发现CISA、FBI和NSA构成紧密合作的三巨头，与五眼联盟和欧洲盟友形成密集连接。


<details>
  <summary>Details</summary>
Motivation: 网络威胁需要多方联合应对，但多机构网络安全合作的组织动态仍不清楚。了解领导机构、桥梁机构及协调机制对加强美国和盟国防务至关重要。

Method: 构建2024年11月至2025年8月期间9份联合网络安全咨询报告的合作作者网络，映射41个机构和442个合作联系，分析合作结构。

Result: 发现紧密的美国三巨头（CISA、FBI、NSA）与五眼联盟和欧洲盟友密集连接。度中心性显示CISA和FBI是协调枢纽，中介中心性显示NSA、英国NCSC和澳大利亚ASD-ACSC是关键桥梁。

Conclusion: 首次提供可复制的联合网络安全咨询数据集和网络分析，为理解网络安全合作信号的组织方式和战略影响力集中点提供新实证证据。

Abstract: Cyber threats increasingly demand joint responses, yet the organizational
dynamics behind multi-agency cybersecurity collaboration remain poorly
understood. Understanding who leads, who bridges, and how agencies coordinate
is critical for strengthening both U.S. homeland security and allied defense
efforts. In this study, we construct a co-authorship network from nine Joint
Cybersecurity Advisories (CSAs) issued between November 2024 and August 2025.
We map 41 agencies and 442 co-authoring ties to analyze the structure of
collaboration. We find a tightly knit U.S. triad -- CISA, FBI, and NSA --
densely connected with Five Eyes and select European allies. Degree centrality
identifies CISA and FBI as coordination hubs, while betweenness highlights NSA,
the UK's NCSC, and Australia's ASD-ACSC as key bridges linking otherwise
fragmented clusters. By releasing the first replicable dataset and network
analysis of CSAs, we provide new empirical evidence on how collaborative
cybersecurity signals are organized and where strategic influence is
concentrated.

</details>


### [32] [SAID: Empowering Large Language Models with Self-Activating Internal Defense](https://arxiv.org/abs/2510.20129)
*Yulong Chen,Yadong Liu,Jiawen Zhang,Mu Li,Chao Huang,Jie Wen*

Main category: cs.CR

TL;DR: 提出了一种无需训练的防御方法SAID，通过激活LLM内在安全机制来抵御越狱攻击，在保持良性任务性能的同时显著减少有害输出


<details>
  <summary>Details</summary>
Motivation: 现有防御策略依赖外部干预，缺乏泛化性且计算开销大，需要更鲁棒和可扩展的防御方法

Method: 三阶段管道：模型原生意图蒸馏提取核心语义、最优安全前缀探测激活潜在安全意识、保守聚合策略确保鲁棒决策

Result: 在5个开源LLM和6种先进越狱攻击上的实验表明，SAID显著优于现有防御方法，同时保持良性任务性能且计算开销最小

Conclusion: 激活LLM内在安全机制是构建更安全可靠对齐AI系统的更鲁棒和可扩展路径

Abstract: Large Language Models (LLMs), despite advances in safety alignment, remain
vulnerable to jailbreak attacks designed to circumvent protective mechanisms.
Prevailing defense strategies rely on external interventions, such as input
filtering or output modification, which often lack generalizability and
compromise model utility while incurring significant computational overhead. In
this work, we introduce a new, training-free defense paradigm, Self-Activating
Internal Defense (SAID), which reframes the defense task from external
correction to internal capability activation. SAID uniquely leverages the LLM's
own reasoning abilities to proactively identify and neutralize malicious intent
through a three-stage pipeline: model-native intent distillation to extract
core semantics, optimal safety prefix probing to activate latent safety
awareness, and a conservative aggregation strategy to ensure robust
decision-making. Extensive experiments on five open-source LLMs against six
advanced jailbreak attacks demonstrate that SAID substantially outperforms
state-of-the-art defenses in reducing harmful outputs. Crucially, it achieves
this while preserving model performance on benign tasks and incurring minimal
computational overhead. Our work establishes that activating the intrinsic
safety mechanisms of LLMs is a more robust and scalable path toward building
safer and more reliable aligned AI systems.

</details>


### [33] [Learning to Triage Taint Flows Reported by Dynamic Program Analysis in Node.js Packages](https://arxiv.org/abs/2510.20739)
*Ronghao Ni,Aidan Z. H. Yang,Min-Chien Hsu,Nuno Sabino,Limin Jia,Ruben Martins,Darion Cassel,Kevin Cheang*

Main category: cs.CR

TL;DR: 该论文研究使用机器学习方法对程序分析工具报告的漏洞进行优先级排序，在Node.js包上评估多种ML模型，最佳LLM模型F1分数达0.915，能显著减少手动审查工作量。


<details>
  <summary>Details</summary>
Motivation: 程序分析工具产生大量候选漏洞报告需要人工审查，成本高昂，需要有效方法来优先处理最可能为真实漏洞的报告。

Method: 收集1,883个Node.js包基准数据集，评估经典ML模型、图神经网络(GNN)、大语言模型(LLM)以及GNN与LLM混合模型，基于动态程序分析工具输出数据进行训练。

Result: 最佳LLM模型F1分数0.915，GNN和经典ML模型F1分数0.904；在低于7%假阴性率下，领先模型可消除66.9%良性包的手动审查，每个包处理约60ms；精度设为0.8时，能检测99.2%可利用污点流，仅遗漏0.8%。

Conclusion: 机器学习方法在漏洞优先级排序方面表现出强大潜力，特别适合实际漏洞分类工作，能显著提高安全分析效率。

Abstract: Program analysis tools often produce large volumes of candidate vulnerability
reports that require costly manual review, creating a practical challenge: how
can security analysts prioritize the reports most likely to be true
vulnerabilities?
  This paper investigates whether machine learning can be applied to
prioritizing vulnerabilities reported by program analysis tools. We focus on
Node.js packages and collect a benchmark of 1,883 Node.js packages, each
containing one reported ACE or ACI vulnerability. We evaluate a variety of
machine learning approaches, including classical models, graph neural networks
(GNNs), large language models (LLMs), and hybrid models that combine GNN and
LLMs, trained on data based on a dynamic program analysis tool's output. The
top LLM achieves $F_{1} {=} 0.915$, while the best GNN and classical ML models
reaching $F_{1} {=} 0.904$. At a less than 7% false-negative rate, the leading
model eliminates 66.9% of benign packages from manual review, taking around 60
ms per package. If the best model is tuned to operate at a precision level of
0.8 (i.e., allowing 20% false positives amongst all warnings), our approach can
detect 99.2% of exploitable taint flows while missing only 0.8%, demonstrating
strong potential for real-world vulnerability triage.

</details>


### [34] [Separating Pseudorandom Generators from Logarithmic Pseudorandom States](https://arxiv.org/abs/2510.20131)
*Mohammed Barhoush*

Main category: cs.CR

TL;DR: 本文建立了伪随机生成器(PRG)与伪随机量子态(PRS)之间的量子黑盒分离，证明了在量子可评估的PRG与对数或线性大小的PRS之间不存在黑盒构造。


<details>
  <summary>Details</summary>
Motivation: 解决PRG是否与对数大小PRS分离这一开放问题，因为对数PRS已实现数字签名和量子公钥加密等强大应用，而线性PRS尚未实现这些应用。

Method: 构建了一个具有逆访问权限的酉量子预言机，证明在该预言机下不存在从PRS到PRG的黑盒构造。

Result: 成功建立了PRG与对数/线性PRS之间的量子黑盒分离，并作为推论获得了PRG与数字签名、量子公钥加密等原语的分离。

Conclusion: PRG与PRS在量子黑盒模型下是分离的，这为理解量子密码学中不同原语之间的关系提供了重要洞见。

Abstract: Pseudorandom generators (PRGs) are a foundational primitive in classical
cryptography, underpinning a wide range of constructions. In the quantum
setting, pseudorandom quantum states (PRSs) were proposed as a potentially
weaker assumption that might serve as a substitute for PRGs in cryptographic
applications. Two primary size regimes of PRSs have been studied:
logarithmic-size and linear-size. Interestingly, logarithmic PRSs have led to
powerful cryptographic applications, such as digital signatures and quantum
public-key encryption, that have not been realized from their linear
counterparts. However, PRGs have only been black-box separated from linear
PRSs, leaving open the fundamental question of whether PRGs are also separated
from logarithmic PRSs.
  In this work, we resolve this open problem. We establish a quantum black-box
separation between (quantum-evaluable) PRGs and PRSs of either size regime.
Specifically, we construct a unitary quantum oracle with inverse access
relative to which no black-box construction of PRG from (logarithmic or linear)
PRS exists. As a direct corollary, we obtain separations between PRGs and
several primitives implied by logarithmic PRSs, including digital signatures
and quantum public-key encryption.

</details>


### [35] [Beyond Text: Multimodal Jailbreaking of Vision-Language and Audio Models through Perceptually Simple Transformations](https://arxiv.org/abs/2510.20223)
*Divyanshu Kumar,Shreyas Jena,Nitin Aravind Birur,Tanay Baswa,Sahil Agarwal,Prashanth Harshangi*

Main category: cs.CR

TL;DR: 多模态大语言模型存在严重的安全漏洞，简单的感知变换攻击就能有效绕过最先进的安全过滤器，攻击成功率高达89%。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在跨模态处理方面存在脆弱性，即使文本安全性能近乎完美，但在多模态攻击下仍会失效，这暴露了当前安全机制的严重缺陷。

Method: 系统研究针对视觉-语言和音频-语言模型的多模态越狱攻击，包括FigStep-Pro（视觉关键词分解）、Intelligent Masking（语义混淆）和音频扰动（Wave-Echo、Wave-Pitch、Wave-Speed）等技术。

Result: 评估了1,900个对抗提示，涵盖高风险安全类别，结果显示：文本安全性能为0%攻击成功率的模型，在感知修改输入下攻击成功率超过75%，FigStep-Pro在Llama-4变体中达到89%攻击成功率。音频攻击也发现供应商特定弱点。

Conclusion: 当前基于文本的对齐方法与多模态威胁之间存在关键差距，现有安全措施无法泛化到跨模态攻击。需要向更广泛的语义级推理进行范式转变，以减轻潜在风险。

Abstract: Multimodal large language models (MLLMs) have achieved remarkable progress,
yet remain critically vulnerable to adversarial attacks that exploit weaknesses
in cross-modal processing. We present a systematic study of multimodal
jailbreaks targeting both vision-language and audio-language models, showing
that even simple perceptual transformations can reliably bypass
state-of-the-art safety filters. Our evaluation spans 1,900 adversarial prompts
across three high-risk safety categories harmful content, CBRN (Chemical,
Biological, Radiological, Nuclear), and CSEM (Child Sexual Exploitation
Material) tested against seven frontier models. We explore the effectiveness of
attack techniques on MLLMs, including FigStep-Pro (visual keyword
decomposition), Intelligent Masking (semantic obfuscation), and audio
perturbations (Wave-Echo, Wave-Pitch, Wave-Speed). The results reveal severe
vulnerabilities: models with almost perfect text-only safety (0\% ASR) suffer
>75\% attack success under perceptually modified inputs, with FigStep-Pro
achieving up to 89\% ASR in Llama-4 variants. Audio-based attacks further
uncover provider-specific weaknesses, with even basic modality transfer
yielding 25\% ASR for technical queries. These findings expose a critical gap
between text-centric alignment and multimodal threats, demonstrating that
current safeguards fail to generalize across cross-modal attacks. The
accessibility of these attacks, which require minimal technical expertise,
suggests that robust multimodal AI safety will require a paradigm shift toward
broader semantic-level reasoning to mitigate possible risks.

</details>


### [36] [HHEML: Hybrid Homomorphic Encryption for Privacy-Preserving Machine Learning on Edge](https://arxiv.org/abs/2510.20243)
*Yu Hin Chan,Hao Yang,Shiyu Shen,Xingyu Fan,Shengzhe Lyu,Patrick S. Y. Hung,Ray C. C. Cheung*

Main category: cs.CR

TL;DR: 提出了一种硬件加速的混合同态加密架构，通过专用硬件加速器优化FHE兼容的轻量对称密码，显著降低边缘设备上的通信和计算开销。


<details>
  <summary>Details</summary>
Motivation: 传统全同态加密在客户端引入显著通信和计算开销，不适用于边缘设备。混合同态加密结合对称加密和FHE来平衡效率，但需要硬件加速来进一步提升性能。

Method: 设计硬件加速的HHE架构，围绕FHE优化的轻量对称密码构建专用硬件加速器，并集成微架构优化以实现更高性能和能效。

Result: 在PYNQ-Z2平台上使用MNIST数据集测试，客户端加密延迟降低50倍以上，硬件吞吐量相比现有FPGA加速器提升近2倍。

Conclusion: 验证了低功耗硬件加速HHE在边缘部署的可行性，为资源受限环境中的可扩展安全机器学习系统提供了硬件-软件协同设计方法。

Abstract: Privacy-preserving machine learning (PPML) is an emerging topic to handle
secure machine learning inference over sensitive data in untrusted
environments. Fully homomorphic encryption (FHE) enables computation directly
on encrypted data on the server side, making it a promising approach for PPML.
However, it introduces significant communication and computation overhead on
the client side, making it impractical for edge devices. Hybrid homomorphic
encryption (HHE) addresses this limitation by combining symmetric encryption
(SE) with FHE to reduce the computational cost on the client side, and
combining with an FHE-friendly SE can also lessen the processing overhead on
the server side, making it a more balanced and efficient alternative. Our work
proposes a hardware-accelerated HHE architecture built around a lightweight
symmetric cipher optimized for FHE compatibility and implemented as a dedicated
hardware accelerator. To the best of our knowledge, this is the first design to
integrate an end-to-end HHE framework with hardware acceleration. Beyond this,
we also present several microarchitectural optimizations to achieve higher
performance and energy efficiency. The proposed work is integrated into a full
PPML pipeline, enabling secure inference with significantly lower latency and
power consumption than software implementations. Our contributions validate the
feasibility of low-power, hardware- accelerated HHE for edge deployment and
provide a hardware- software co-design methodology for building scalable,
secure machine learning systems in resource-constrained environments.
Experiments on a PYNQ-Z2 platform with the MNIST dataset show over a 50x
reduction in client-side encryption latency and nearly a 2x gain in hardware
throughput compared to existing FPGA-based HHE accelerators.

</details>


### [37] [Privacy Protection of Automotive Location Data Based on Format-Preserving Encryption of Geographical Coordinates](https://arxiv.org/abs/2510.20300)
*Haojie Ji,Long Jin,Haowen Li,Chongshi Xin,Te Hu*

Main category: cs.CR

TL;DR: 提出基于格式保持加密(FPE)的汽车地理位置数据高精度隐私保护机制，通过坐标数据密钥映射减少重复加密解密造成的地理位置精度损失。


<details>
  <summary>Details</summary>
Motivation: 汽车位置数据共享在路线导航、驾驶监控和车辆调度等功能中存在隐私泄露风险，可能导致用户行为识别、敏感位置推断和轨迹重建等攻击。

Method: 设计汽车坐标数据密钥映射机制，采用格式保持加密技术对地理坐标进行加密，减少重复加密解密过程中的精度损失。

Result: 平均相对距离保持率(RDR)达到0.0844，关键区域热点数量减少98.9%，解密坐标数据恢复精度达到100%。

Conclusion: 为汽车位置数据提供了一种高精度隐私保护方法，为自动驾驶中的敏感数据共享提供了有效的数据安全解决方案。

Abstract: There are increasing risks of privacy disclosure when sharing the automotive
location data in particular functions such as route navigation, driving
monitoring and vehicle scheduling. These risks could lead to the attacks
including user behavior recognition, sensitive location inference and
trajectory reconstruction. In order to mitigate the data security risk caused
by the automotive location sharing, this paper proposes a high-precision
privacy protection mechanism based on format-preserving encryption (FPE) of
geographical coordinates. The automotive coordinate data key mapping mechanism
is designed to reduce to the accuracy loss of the geographical location data
caused by the repeated encryption and decryption. The experimental results
demonstrate that the average relative distance retention rate (RDR) reached
0.0844, and the number of hotspots in the critical area decreased by 98.9%
after encryption. To evaluate the accuracy loss of the proposed encryption
algorithm on automotive geographical location data, this paper presents the
experimental analysis of decryption accuracy, and the result indicates that the
decrypted coordinate data achieves a restoration accuracy of 100%. This work
presents a high-precision privacy protection method for automotive location
data, thereby providing an efficient data security solution for the sensitive
data sharing in autonomous driving.

</details>


### [38] [Enhancing Security in Deep Reinforcement Learning: A Comprehensive Survey on Adversarial Attacks and Defenses](https://arxiv.org/abs/2510.20314)
*Wu Yichao,Wang Yirui,Ding Panpan,Wang Hailong,Zhu Bingqian,Liu Chun*

Main category: cs.CR

TL;DR: 本文综述了深度强化学习在对抗环境中的安全挑战，提出了基于扰动类型和攻击目标的对抗攻击分类框架，系统总结了现有的鲁棒性训练策略和防御技术，并展望了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着DRL在自动驾驶、智能制造等安全敏感领域的广泛应用，其在动态多变环境中的安全性和鲁棒性成为核心研究问题，特别是在面对对抗攻击时可能产生严重性能下降甚至危险决策。

Method: 提出基于扰动类型和攻击目标的对抗攻击分类框架，详细回顾针对DRL的主流对抗攻击方法（包括状态空间、动作空间、奖励函数和模型空间等），系统总结各种鲁棒性训练策略（对抗训练、竞争训练、鲁棒学习、对抗检测、防御蒸馏等）。

Result: 建立了全面的DRL对抗攻击分类体系，梳理了各类攻击方法的特征，总结了现有防御技术的优缺点，为理解DRL在对抗环境中的脆弱性和防护措施提供了系统框架。

Conclusion: 未来DRL在对抗环境中的研究需要重点关注提高泛化能力、降低计算复杂度、增强可扩展性和可解释性，为安全敏感场景下的DRL应用提供更可靠的保障。

Abstract: With the wide application of deep reinforcement learning (DRL) techniques in
complex fields such as autonomous driving, intelligent manufacturing, and smart
healthcare, how to improve its security and robustness in dynamic and
changeable environments has become a core issue in current research. Especially
in the face of adversarial attacks, DRL may suffer serious performance
degradation or even make potentially dangerous decisions, so it is crucial to
ensure their stability in security-sensitive scenarios. In this paper, we first
introduce the basic framework of DRL and analyze the main security challenges
faced in complex and changing environments. In addition, this paper proposes an
adversarial attack classification framework based on perturbation type and
attack target and reviews the mainstream adversarial attack methods against DRL
in detail, including various attack methods such as perturbation state space,
action space, reward function and model space. To effectively counter the
attacks, this paper systematically summarizes various current robustness
training strategies, including adversarial training, competitive training,
robust learning, adversarial detection, defense distillation and other related
defense techniques, we also discuss the advantages and shortcomings of these
methods in improving the robustness of DRL. Finally, this paper looks into the
future research direction of DRL in adversarial environments, emphasizing the
research needs in terms of improving generalization, reducing computational
complexity, and enhancing scalability and explainability, aiming to provide
valuable references and directions for researchers.

</details>


### [39] [GhostEI-Bench: Do Mobile Agents Resilience to Environmental Injection in Dynamic On-Device Environments?](https://arxiv.org/abs/2510.20333)
*Chiyu Chen,Xinhao Song,Yunkai Chai,Yang Yao,Haodong Zhao,Lijun Li,Jie Li,Yan Teng,Gongshen Liu,Yingchun Wang*

Main category: cs.CR

TL;DR: 提出了GhostEI-Bench基准测试，用于评估移动代理在环境注入攻击下的脆弱性，发现现有模型对欺骗性UI元素存在系统性感知和推理失败。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型作为自主代理在移动GUI中运行时面临环境注入攻击的威胁，这种攻击通过插入对抗性UI元素绕过文本防护，可能导致隐私泄露、财务损失等严重后果。

Method: 开发GhostEI-Bench基准测试，在可执行的Android模拟器中注入对抗性事件，并设计judge-LLM协议对代理行为轨迹进行细粒度失败分析。

Result: 对最先进代理的全面实验显示，当前模型对操纵的UI存在系统性感知和推理失败，表现出明显的脆弱性。

Conclusion: GhostEI-Bench为量化和缓解这一新兴威胁提供了框架，为开发更鲁棒和安全的具身代理铺平了道路。

Abstract: Vision-Language Models (VLMs) are increasingly deployed as autonomous agents
to navigate mobile graphical user interfaces (GUIs). Operating in dynamic
on-device ecosystems, which include notifications, pop-ups, and inter-app
interactions, exposes them to a unique and underexplored threat vector:
environmental injection. Unlike prompt-based attacks that manipulate textual
instructions, environmental injection corrupts an agent's visual perception by
inserting adversarial UI elements (for example, deceptive overlays or spoofed
notifications) directly into the GUI. This bypasses textual safeguards and can
derail execution, causing privacy leakage, financial loss, or irreversible
device compromise. To systematically evaluate this threat, we introduce
GhostEI-Bench, the first benchmark for assessing mobile agents under
environmental injection attacks within dynamic, executable environments. Moving
beyond static image-based assessments, GhostEI-Bench injects adversarial events
into realistic application workflows inside fully operational Android emulators
and evaluates performance across critical risk scenarios. We further propose a
judge-LLM protocol that conducts fine-grained failure analysis by reviewing the
agent's action trajectory alongside the corresponding screenshot sequence,
pinpointing failure in perception, recognition, or reasoning. Comprehensive
experiments on state-of-the-art agents reveal pronounced vulnerability to
deceptive environmental cues: current models systematically fail to perceive
and reason about manipulated UIs. GhostEI-Bench provides a framework for
quantifying and mitigating this emerging threat, paving the way toward more
robust and secure embodied agents.

</details>


### [40] [NeuPerm: Disrupting Malware Hidden in Neural Network Parameters by Leveraging Permutation Symmetry](https://arxiv.org/abs/2510.20367)
*Daniel Gilkarov,Ran Dubin*

Main category: cs.CR

TL;DR: NeuPerm是一种利用神经网络排列对称性来破坏隐藏在模型参数中的恶意软件的方法，对模型性能影响极小，能有效防御最先进的攻击。


<details>
  <summary>Details</summary>
Motivation: 预训练模型共享虽然降低了深度学习应用成本，但存在安全风险，攻击者可能将自执行恶意软件隐藏在神经网络参数中传播。

Method: 利用神经网络排列对称性的理论特性，通过重新排列模型参数来破坏隐藏的恶意代码，同时保持模型性能基本不变。

Result: NeuPerm成功破坏了最先进的恶意软件攻击，对LLM等模型有效，这是以往类似工作未能实现的成就。

Conclusion: NeuPerm提供了一种简单有效的防御机制，能够在不影响模型性能的前提下，有效阻止通过模型参数传播的恶意软件攻击。

Abstract: Pretrained deep learning model sharing holds tremendous value for researchers
and enterprises alike. It allows them to apply deep learning by fine-tuning
models at a fraction of the cost of training a brand-new model. However, model
sharing exposes end-users to cyber threats that leverage the models for
malicious purposes. Attackers can use model sharing by hiding self-executing
malware inside neural network parameters and then distributing them for
unsuspecting users to unknowingly directly execute them, or indirectly as a
dependency in another software. In this work, we propose NeuPerm, a simple yet
effec- tive way of disrupting such malware by leveraging the theoretical
property of neural network permutation symmetry. Our method has little to no
effect on model performance at all, and we empirically show it successfully
disrupts state-of-the-art attacks that were only previously addressed using
quantization, a highly complex process. NeuPerm is shown to work on LLMs, a
feat that no other previous similar works have achieved. The source code is
available at https://github.com/danigil/NeuPerm.git.

</details>


### [41] [MAC Aggregation over Lossy Channels in DTLS 1.3](https://arxiv.org/abs/2510.20419)
*Eric Wagner,David Heye,Jan Bauer,Klaus Wehrle,Martin Serror*

Main category: cs.CR

TL;DR: 在资源受限环境中，MAC聚合通过将多个消息的完整性保护聚合到单个标签中，可节省带宽。实验证明即使在有损无线场景中，MAC聚合仍能提高吞吐量并节省能耗。


<details>
  <summary>Details</summary>
Motivation: 在资源受限环境中节省带宽，验证MAC聚合在有损无线场景中的实际效益，而不仅仅是基于模拟的假设。

Method: 设计DTLS 1.3协议的MAC聚合扩展，在嵌入式硬件上对完整通信协议栈进行广泛性能评估。

Result: MAC聚合在短消息传输中可将吞吐量提高达50%，节省高达17%的能耗，即使在有损信道中也能实现这些效益。

Conclusion: MAC聚合在实际部署中确实能带来显著的性能提升和能耗节省，特别是在资源受限的无线环境中。

Abstract: Aggregating Message Authentication Codes (MACs) promises to save valuable
bandwidth in resource-constrained environments. The idea is simple: Instead of
appending an authentication tag to each message in a communication stream, the
integrity protection of multiple messages is aggregated into a single tag.
Recent studies postulate, e.g., based on simulations, that these benefits also
spread to wireless, and thus lossy, scenarios despite each lost packet
typically resulting in the loss of integrity protection information for
multiple messages. In this paper, we investigate these claims in a real
deployment. Therefore, we first design a MAC aggregation extension for the
Datagram Transport Layer Security (DTLS) 1.3 protocol. Afterward, we
extensively evaluate the performance of MAC aggregation on a complete
communication protocol stack on embedded hardware. We find that MAC aggregation
can indeed increase goodput by up to 50% and save up to 17% of energy
expenditure for the transmission of short messages, even in lossy channels.

</details>


### [42] [On the cybersecurity of LoRaWAN-based system: a Smart-Lighting case study](https://arxiv.org/abs/2510.20494)
*Florian Hofer,Barbara Russo*

Main category: cs.CR

TL;DR: 本文通过实验研究LoRaWAN智能照明系统的架构配置，评估系统对网络攻击的鲁棒性，发现大多数攻击不成功，但近距离高功率干扰攻击有效，可通过网关冗余缓解。


<details>
  <summary>Details</summary>
Motivation: 工业4.0中的信息物理系统和物联网面临网络安全挑战，需要验证和改进智能系统架构的鲁棒性。

Method: 采用迭代实验方法，在实验室和现场环境中测试LoRaWAN智能照明系统的不同架构配置，评估其对各种攻击的抵抗能力。

Result: 大多数LoRaWAN网络攻击不成功，但发现产品存在未解决的问题；最成功的攻击是近距离高功率干扰攻击，可通过网关冗余缓解。

Conclusion: LoRaWAN网络整体具有较好的安全性，但需要关注近距离干扰攻击，通过架构优化（如网关冗余）可提高系统鲁棒性。

Abstract: Cyber-physical systems and the Internet of Things (IoT) are key technologies
in the Industry 4.0 vision. They incorporate sensors and actuators to interact
with the physical environment. However, when creating and interconnecting
components to form a heterogeneous smart systems architecture, these face
challenges in cybersecurity. This paper presents an experimental investigation
of architectural configurations for a LoRaWAN-based Smart-Lighting project,
aimed at verifying and improving the system's robustness against attacks. We
assess the system's robustness in a series of iterative experiments conducted
both in-vitro and on-site. The results show that most attacks on a LoRaWAN
network are unsuccessful, also highlighting unresolved issues with the
installed products. The most successful attacks are high-power jamming attacks
within a few meters of the target, which, in the case of gateways, can be
mitigated through gateway redundancy.

</details>


### [43] [AdaDoS: Adaptive DoS Attack via Deep Adversarial Reinforcement Learning in SDN](https://arxiv.org/abs/2510.20566)
*Wei Shao,Yuhao Wang,Rongguang He,Muhammad Ejaz Ahmed,Seyit Camtepe*

Main category: cs.CR

TL;DR: AdaDoS是一种基于对抗强化学习的自适应DoS攻击模型，能够动态调整攻击策略以规避现有检测机制，包括基于机器学习和基于规则的DoS攻击检测器。


<details>
  <summary>Details</summary>
Motivation: 现有防御机制对基于规则的DoS攻击有效，但AI驱动技术对SDN安全构成新挑战，可能削弱现有防御机制的有效性。

Method: 将问题建模为攻击者与检测器之间的竞争博弈，使用部分可观测马尔可夫决策过程(POMDP)建模，并采用新颖的互惠学习模块，让有限观测的学生代理向全观测的教师代理学习。

Result: AdaDoS能够生成DoS类攻击序列，自适应地规避检测，是首个将强化学习应用于开发DoS类攻击序列的研究。

Conclusion: AdaDoS展示了AI驱动攻击对SDN安全的新威胁，强调了需要开发更强大的自适应防御机制来应对此类智能攻击。

Abstract: Existing defence mechanisms have demonstrated significant effectiveness in
mitigating rule-based Denial-of-Service (DoS) attacks, leveraging predefined
signatures and static heuristics to identify and block malicious traffic.
However, the emergence of AI-driven techniques presents new challenges to SDN
security, potentially compromising the efficacy of existing defence mechanisms.
In this paper, we introduce~AdaDoS, an adaptive attack model that disrupt
network operations while evading detection by existing DoS-based detectors
through adversarial reinforcement learning (RL). Specifically, AdaDoS models
the problem as a competitive game between an attacker, whose goal is to
obstruct network traffic without being detected, and a detector, which aims to
identify malicious traffic. AdaDoS can solve this game by dynamically adjusting
its attack strategy based on feedback from the SDN and the detector.
Additionally, recognising that attackers typically have less information than
defenders, AdaDoS formulates the DoS-like attack as a partially observed Markov
decision process (POMDP), with the attacker having access only to delay
information between attacker and victim nodes. We address this challenge with a
novel reciprocal learning module, where the student agent, with limited
observations, enhances its performance by learning from the teacher agent, who
has full observational capabilities in the SDN environment. AdaDoS represents
the first application of RL to develop DoS-like attack sequences, capable of
adaptively evading both machine learning-based and rule-based DoS-like attack
detectors.

</details>


### [44] [Decentralized Exchange that Mitigate a Bribery Attack](https://arxiv.org/abs/2510.20645)
*Nitin Awathare*

Main category: cs.CR

TL;DR: 本文分析了现有HTLC协议的安全漏洞，提出新的贿赂攻击方式，并设计了能抵抗所有贿赂场景的安全协议\prot。


<details>
  <summary>Details</summary>
Motivation: 现有的HTLC协议如MAD-HTLC和He-HTLC虽然改进了激励兼容性，但仍存在漏洞，特别是在多方与矿工勾结的贿赂攻击场景下。

Method: 通过实现和博弈论分析揭露现有协议的漏洞，提出新的矿工勾结贿赂攻击，并设计\prot协议采用两阶段方法防止第三方未经授权的代币没收。

Result: 发现了现有解决方案的漏洞，提出了比攻击原生HTLC获利相当的更强攻击，并验证了\prot协议在比特币和以太坊上的高效性。

Conclusion: \prot协议在博弈论上是安全的，能够抵抗所有贿赂场景，在交易成本和延迟方面表现高效。

Abstract: Despite the popularity of Hashed Time-Locked Contracts (HTLCs) because of
their use in wide areas of applications such as payment channels, atomic swaps,
etc, their use in exchange is still questionable. This is because of its
incentive incompatibility and susceptibility to bribery attacks.
  State-of-the-art solutions such as MAD-HTLC (Oakland'21) and He-HTLC
(NDSS'23) address this by leveraging miners' profit-driven behaviour to
mitigate such attacks. The former is the mitigation against passive miners;
however, the latter works against both active and passive miners. However, they
consider only two bribing scenarios where either of the parties involved in the
transfer collude with the miner.
  In this paper, we expose vulnerabilities in state-of-the-art solutions by
presenting a miner-collusion bribery attack with implementation and
game-theoretic analysis. Additionally, we propose a stronger attack on MAD-HTLC
than He-HTLC, allowing the attacker to earn profits equivalent to attacking
naive HTLC.
  Leveraging our insights, we propose \prot, a game-theoretically secure HTLC
protocol resistant to all bribery scenarios. \prot\ employs a two-phase
approach, preventing unauthorized token confiscation by third parties, such as
miners. In Phase 1, parties commit to the transfer; in Phase 2, the transfer is
executed without manipulation. We demonstrate \prot's efficiency in transaction
cost and latency via implementations on Bitcoin and Ethereum.

</details>


### [45] [Risk Psychology & Cyber-Attack Tactics](https://arxiv.org/abs/2510.20657)
*Rubens Kim,Stephan Carney,Yvonne Fonken,Soham Hans,Sofia Hirschmann,Stacy Marsella,Peggy Wu,Nikolos Gurney*

Main category: cs.CR

TL;DR: 研究认知过程是否能预测网络攻击行为，发现认知过程比专业经验更能影响攻击技术的选择


<details>
  <summary>Details</summary>
Motivation: 探讨个体认知差异是否会影响网络安全专业人员在红队行动中的攻击行为模式

Method: 使用多级混合效应泊松回归分析，将技术使用次数嵌套在参与者内，检验认知过程是否预测特定技术的使用

Result: 认知量表显著预测了技术使用，但效果因技术而异而非统一；专业水平和实验条件对技术模式无显著预测作用

Conclusion: 个体认知差异塑造网络攻击行为，支持开发基于心理学的防御策略

Abstract: We examine whether measured cognitive processes predict cyber-attack
behavior. We analyzed data that included psychometric scale responses and
labeled attack behaviors from cybersecurity professionals who conducted
red-team operations against a simulated enterprise network. We employed
multilevel mixed-effects Poisson regression with technique counts nested within
participants to test whether cognitive processes predicted technique-specific
usage. The scales significantly predicted technique use, but effects varied by
technique rather than operating uniformly. Neither expertise level nor
experimental treatment condition significantly predicted technique patterns,
indicating that cognitive processes may be stronger drivers of technique
selection than training or experience. These findings demonstrate that
individual cognitive differences shape cyber-attack behavior and support the
development of psychology-informed defense strategies.

</details>


### [46] [RAGRank: Using PageRank to Counter Poisoning in CTI LLM Pipelines](https://arxiv.org/abs/2510.20768)
*Austin Jia,Avaneesh Ramesh,Zain Shamsi,Daniel Zhang,Alex Liu*

Main category: cs.CR

TL;DR: 本文提出使用PageRank等源可信度算法来增强RAG系统在网络安全威胁情报中的防御能力，通过降低恶意文档的权威性评分来提升系统安全性。


<details>
  <summary>Details</summary>
Motivation: 现有的RAG系统在网络安全威胁情报应用中容易受到投毒攻击，且传统防御方法在CTI场景下可能失效，因为新型攻击的威胁信息往往是全新的，且攻击者能模仿合法格式和术语。

Method: 提出将源可信度算法（以PageRank为例）应用于语料库，通过计算文档的权威性评分来识别和降低恶意内容的影响。

Result: 在MS MARCO数据集上的实验表明，该算法能有效降低恶意文档的权威性评分，同时提升可信内容；在CTI文档和feed上的概念验证也显示了良好性能。

Conclusion: 源可信度算法能显著提升RAG系统在网络安全威胁情报应用中的鲁棒性，为防御投毒攻击提供了有效解决方案。

Abstract: Retrieval-Augmented Generation (RAG) has emerged as the dominant
architectural pattern to operationalize Large Language Model (LLM) usage in
Cyber Threat Intelligence (CTI) systems. However, this design is susceptible to
poisoning attacks, and previously proposed defenses can fail for CTI contexts
as cyber threat information is often completely new for emerging attacks, and
sophisticated threat actors can mimic legitimate formats, terminology, and
stylistic conventions. To address this issue, we propose that the robustness of
modern RAG defenses can be accelerated by applying source credibility
algorithms on corpora, using PageRank as an example. In our experiments, we
demonstrate quantitatively that our algorithm applies a lower authority score
to malicious documents while promoting trusted content, using the standardized
MS MARCO dataset. We also demonstrate proof-of-concept performance of our
algorithm on CTI documents and feeds.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [47] [A Quantum-Inspired Algorithm for Solving Sudoku Puzzles and the MaxCut Problem](https://arxiv.org/abs/2510.19835)
*Max B. Zhao,Fei Li*

Main category: cs.AI

TL;DR: 提出一种量子启发的算法，使用矩阵乘积态和离散驱动调度来解决二次无约束二进制优化问题，在数独和MaxCut问题上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 解决二次无约束二进制优化问题，该问题在数学上等价于寻找伊辛自旋玻璃哈密顿量的基态，需要高效且可扩展的算法。

Method: 使用矩阵乘积态紧凑表示自旋构型的大叠加，结合离散驱动调度引导MPS向基态演化，通过密度矩阵重整化群方法迭代最小化系统能量。

Result: 算法在超过200个伊辛自旋的数独问题和251个节点、3265条边的MaxCut问题上成功找到全局最优解，而不仅仅是近似解。

Conclusion: 该量子启发方法具有可扩展性、通用性和适用于工业规模QUBO应用的优势。

Abstract: We propose and evaluate a quantum-inspired algorithm for solving Quadratic
Unconstrained Binary Optimization (QUBO) problems, which are mathematically
equivalent to finding ground states of Ising spin-glass Hamiltonians. The
algorithm employs Matrix Product States (MPS) to compactly represent large
superpositions of spin configurations and utilizes a discrete driving schedule
to guide the MPS toward the ground state. At each step, a driver Hamiltonian --
incorporating a transverse magnetic field -- is combined with the problem
Hamiltonian to enable spin flips and facilitate quantum tunneling. The MPS is
updated using the standard Density Matrix Renormalization Group (DMRG) method,
which iteratively minimizes the system's energy via multiple sweeps across the
spin chain. Despite its heuristic nature, the algorithm reliably identifies
global minima, not merely near-optimal solutions, across diverse QUBO
instances. We first demonstrate its effectiveness on intermediate-level Sudoku
puzzles from publicly available sources, involving over $200$ Ising spins with
long-range couplings dictated by constraint satisfaction. We then apply the
algorithm to MaxCut problems from the Biq Mac library, successfully solving
instances with up to $251$ nodes and $3,265$ edges. We discuss the advantages
of this quantum-inspired approach, including its scalability, generalizability,
and suitability for industrial-scale QUBO applications.

</details>


### [48] [Benchmarking Reasoning Reliability in Artificial Intelligence Models for Energy-System Analysis](https://arxiv.org/abs/2510.19836)
*Eliseo Curcio*

Main category: cs.AI

TL;DR: 该研究提出了分析可靠性基准(ARB)，这是首个用于量化能源系统分析中大语言模型推理可靠性的标准化框架，包含五个子指标，并在四种前沿模型上进行了验证。


<details>
  <summary>Details</summary>
Motivation: 当前AI在能源领域的验证实践主要关注预测准确性或计算效率，但缺乏对分析结论逻辑完整性的测试，需要建立标准化评估框架。

Method: 开发了ARB框架，整合准确性、推理可靠性、不确定性纪律、政策一致性和透明度五个子指标，使用公开技术经济数据集在确定性、概率性和认知性场景下评估模型性能。

Result: GPT-4/5和Claude 4.5 Sonnet实现了稳定且符合政策的推理(分析可靠性指数>90)，Gemini 2.5 Pro表现中等，Llama 3 70B低于专业阈值，统计验证显示这些差异显著且可重现。

Conclusion: ARB建立了能源文献中首个验证AI系统中因果、概率和政策驱动推理的定量方法，为全球能源转型中可信赖和透明的分析应用提供了参考框架。

Abstract: Artificial intelligence and machine learning are increasingly used for
forecasting, optimization, and policy design in the energy sector, yet no
standardized framework exists to evaluate whether these systems reason
correctly. Current validation practices focus on predictive accuracy or
computational efficiency, leaving the logical integrity of analytical
conclusions untested. This study introduces the Analytical Reliability
Benchmark (ARB), a reproducible framework that quantifies reasoning reliability
in large language models applied to energy system analysis. The benchmark
integrates five submetrics: accuracy, reasoning reliability, uncertainty
discipline, policy consistency, and transparency, and evaluates model
performance across deterministic, probabilistic, and epistemic scenarios using
open technoeconomic datasets (NREL ATB 2024, DOE H2A/H2New, IEA WEO 2024). Four
frontier models (GPT-4/5, Claude 4.5 Sonnet, Gemini 2.5 Pro, Llama 3 70B) were
tested under identical factual and regulatory conditions. Results show that
reasoning reliability can be objectively measured. GPT-4/5 and Claude 4.5
Sonnet achieved consistent and policy-compliant reasoning (Analytical
Reliability Index greater than 90), Gemini 2.5 Pro demonstrated moderate
stability, and Llama 3 70B remained below professional thresholds. Statistical
validation confirmed that these differences are significant and reproducible.
The ARB establishes the first quantitative method in the energy literature for
verifying causal, probabilistic, and policy-driven reasoning in artificial
intelligence systems, providing a reference framework for trustworthy and
transparent analytical applications in the global energy transition.

</details>


### [49] [Branch-and-Browse: Efficient and Controllable Web Exploration with Tree-Structured Reasoning and Action Memory](https://arxiv.org/abs/2510.19838)
*Shiqi He,Yue Cui,Xinyu Ma,Yaliang Li,Bolin Ding,Mosharaf Chowdhury*

Main category: cs.AI

TL;DR: Branch-and-Browse是一个细粒度的网页代理框架，通过树状结构探索、网页状态重放和页面动作记忆，显著提升了基于LLM的网页代理的推理深度和执行效率。


<details>
  <summary>Details</summary>
Motivation: 现有的自主网页代理方法在推理深度和效率上存在局限：线性方法无法进行多步推理且缺乏有效回溯，而其他搜索策略则粒度粗糙且计算成本高。

Method: 该框架采用显式子任务管理和树状结构探索实现可控的多分支推理，通过网页状态重放和后台推理引导探索，并利用页面动作记忆在会话内外共享已探索的动作。

Result: 在WebArena基准测试中，Branch-and-Browse实现了35.8%的任务成功率，相比最先进方法执行时间减少了40.4%。

Conclusion: Branch-and-Browse是一个可靠且高效的基于LLM的网页代理框架，在开放网页环境中实现了实用的具身推理。

Abstract: Autonomous web agents powered by large language models (LLMs) show strong
potential for performing goal-oriented tasks such as information retrieval,
report generation, and online transactions. These agents mark a key step toward
practical embodied reasoning in open web environments. However, existing
approaches remain limited in reasoning depth and efficiency: vanilla linear
methods fail at multi-step reasoning and lack effective backtracking, while
other search strategies are coarse-grained and computationally costly. We
introduce Branch-and-Browse, a fine-grained web agent framework that unifies
structured reasoning-acting, contextual memory, and efficient execution. It (i)
employs explicit subtask management with tree-structured exploration for
controllable multi-branch reasoning, (ii) bootstraps exploration through
efficient web state replay with background reasoning, and (iii) leverages a
page action memory to share explored actions within and across sessions. On the
WebArena benchmark, Branch-and-Browse achieves a task success rate of 35.8\%
and reduces execution time by up to 40.4\% relative to state-of-the-art
methods. These results demonstrate that Branch-and-Browse is a reliable and
efficient framework for LLM-based web agents.

</details>


### [50] [DAG-Math: Graph-Guided Mathematical Reasoning in LLMs](https://arxiv.org/abs/2510.19842)
*Yuanhe Zhang,Ilja Kuzborskij,Jason D. Lee,Chenlei Leng,Fanghui Liu*

Main category: cs.AI

TL;DR: 提出DAG-MATH框架，将思维链建模为基于规则的有向无环图随机过程，引入逻辑紧密度指标评估LLM推理一致性，发现即使PASS@k指标相近，不同LLM在规则一致性推理上存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 现有研究不清楚LLM在数学问题上的成功是源于搜索、机械记忆还是规则一致性推理，需要超越传统PASS@k指标的评估方法。

Method: 将思维链建模为基于规则的有向无环图随机过程，提出逻辑紧密度指标，构建DAG-MATH格式的基准测试来评估LLM推理能力。

Result: 在标准数学推理数据集上，发现代表性LLM家族在推理保真度上存在统计显著差异，即使PASS@k指标相近，揭示了最终答案准确性与规则一致性推导之间的差距。

Conclusion: 该框架在自由形式思维链和形式证明系统之间提供了平衡，为LLM推理评估提供了可行的诊断工具。

Abstract: Large Language Models (LLMs) demonstrate strong performance on mathematical
problems when prompted with Chain-of-Thought (CoT), yet it remains unclear
whether this success stems from search, rote procedures, or rule-consistent
reasoning. To address this, we propose modeling CoT as a certain rule-based
stochastic process over directed acyclic graphs (DAGs), where nodes represent
intermediate derivation states and edges encode rule applications. Within this
framework, we introduce logical closeness, a metric that quantifies how well a
model's CoT trajectory (i.e., the LLM's final output) adheres to the DAG
structure, providing evaluation beyond classical PASS@k metrics. Building on
this, we introduce the DAG-MATH CoT format and construct a benchmark that
guides LLMs to generate CoT trajectories in this format, thereby enabling the
evaluation of their reasoning ability under our framework. Across standard
mathematical reasoning datasets, our analysis uncovers statistically
significant differences in reasoning fidelity among representative LLM
families-even when PASS@k is comparable-highlighting gaps between final-answer
accuracy and rule-consistent derivation. Our framework provides a balance
between free-form CoT and formal proofs systems, offering actionable
diagnostics for LLMs reasoning evaluation. Our benchmark and code are available
at: https://github.com/YuanheZ/DAG-MATH-Formatted-CoT.

</details>


### [51] [Surfer 2: The Next Generation of Cross-Platform Computer Use Agents](https://arxiv.org/abs/2510.19949)
*Mathieu Andreux,Märt Bakler,Yanael Barbier,Hamza Ben Chekroun,Emilien Biré,Antoine Bonnet,Riaz Bordie,Nathan Bout,Matthias Brunel,Aleix Cambray,Pierre-Louis Cedoz,Antoine Chassang,Gautier Cloix,Ethan Connelly,Alexandra Constantinou,Ramzi De Coster,Hubert de la Jonquiere,Aurélien Delfosse,Maxime Delpit,Alexis Deprez,Augustin Derupti,Mathieu Diaz,Shannon D'Souza,Julie Dujardin,Abai Edmund,Michael Eickenberg,Armand Fatalot,Wissem Felissi,Isaac Herring,Xavier Koegler,Erwan Le Jumeau de Kergaradec,Aurélien Lac,Maxime Langevin,Corentin Lauverjat,Antonio Loison,Avshalom Manevich,Axel Moyal,Axel Nguyen Kerbel,Marinela Parovic,Julien Revelle,Guillaume Richard,Mats Richter,Ronan Riochet,María Santos,Romain Savidan,Laurent Sifre,Maxime Theillard,Marc Thibault,Ivan Valentini,Tony Wu,Laura Yie,Kai Yuan,Jevgenij Zubovskij*

Main category: cs.AI

TL;DR: Surfer 2是一个基于纯视觉观察的统一架构，在网页、桌面和移动环境中实现了最先进的性能，无需特定任务微调。


<details>
  <summary>Details</summary>
Motivation: 解决现有代理系统依赖环境特定接口、限制跨平台部署的问题，构建能够在不同计算环境中通用的智能代理。

Method: 采用分层上下文管理、解耦的规划与执行、以及具有自适应恢复能力的自我验证机制，支持长任务周期的可靠操作。

Result: 在WebVoyager上达到97.1%准确率，WebArena 69.6%，OSWorld 60.1%，AndroidWorld 87.1%，超越所有先前系统，多次尝试后超过人类表现。

Conclusion: 系统化编排能够放大基础模型能力，仅通过视觉交互实现通用计算机控制，但需要下一代视觉语言模型来实现帕累托最优的成本效益。

Abstract: Building agents that generalize across web, desktop, and mobile environments
remains an open challenge, as prior systems rely on environment-specific
interfaces that limit cross-platform deployment. We introduce Surfer 2, a
unified architecture operating purely from visual observations that achieves
state-of-the-art performance across all three environments. Surfer 2 integrates
hierarchical context management, decoupled planning and execution, and
self-verification with adaptive recovery, enabling reliable operation over long
task horizons. Our system achieves 97.1% accuracy on WebVoyager, 69.6% on
WebArena, 60.1% on OSWorld, and 87.1% on AndroidWorld, outperforming all prior
systems without task-specific fine-tuning. With multiple attempts, Surfer 2
exceeds human performance on all benchmarks. These results demonstrate that
systematic orchestration amplifies foundation model capabilities and enables
general-purpose computer control through visual interaction alone, while
calling for a next-generation vision language model to achieve Pareto-optimal
cost-efficiency.

</details>


### [52] [RELATE: A Schema-Agnostic Perceiver Encoder for Multimodal Relational Graphs](https://arxiv.org/abs/2510.19954)
*Joseph Meyer,Divyansha Lachi,Reza Mohammadi,Roshan Reddy Upendra,Eva L. Dyer,Mark Li,Tom Palczewski*

Main category: cs.AI

TL;DR: RELATE是一个模式无关的图神经网络特征编码器，使用共享的模态特定编码器和交叉注意力模块，在保持性能的同时大幅减少参数数量。


<details>
  <summary>Details</summary>
Motivation: 现有的图神经网络需要为每种节点类型和特征列设计特定编码器，这阻碍了可扩展性和参数共享，特别是在处理异构时序图数据时。

Method: 使用共享的模态特定编码器处理分类、数值、文本和时间属性，然后通过Perceiver风格的交叉注意力模块将特征聚合成固定大小的节点表示。

Result: 在RelBench基准测试中，RELATE在ReLGNN和HGT上实现了与模式特定编码器性能相差3%以内的结果，同时将参数数量减少了最多5倍。

Conclusion: RELATE支持不同模式并实现了多数据集预训练，为关系图数据的基础模型铺平了道路。

Abstract: Relational multi-table data is common in domains such as e-commerce,
healthcare, and scientific research, and can be naturally represented as
heterogeneous temporal graphs with multi-modal node attributes. Existing graph
neural networks (GNNs) rely on schema-specific feature encoders, requiring
separate modules for each node type and feature column, which hinders
scalability and parameter sharing. We introduce RELATE (Relational Encoder for
Latent Aggregation of Typed Entities), a schema-agnostic, plug-and-play feature
encoder that can be used with any general purpose GNN. RELATE employs shared
modality-specific encoders for categorical, numerical, textual, and temporal
attributes, followed by a Perceiver-style cross-attention module that
aggregates features into a fixed-size, permutation-invariant node
representation. We evaluate RELATE on ReLGNN and HGT in the RelBench benchmark,
where it achieves performance within 3% of schema-specific encoders while
reducing parameter counts by up to 5x. This design supports varying schemas and
enables multi-dataset pretraining for general-purpose GNNs, paving the way
toward foundation models for relational graph data.

</details>


### [53] [A new wave of vehicle insurance fraud fueled by generative AI](https://arxiv.org/abs/2510.19957)
*Amir Hever,Itai Orr*

Main category: cs.AI

TL;DR: 生成式AI正在加剧保险欺诈问题，使伪造事故证据变得更容易。保险公司开始部署AI反欺诈工具，但面临检测准确性和成本等挑战。本文提出了UVeye的分层解决方案来应对这一新威胁。


<details>
  <summary>Details</summary>
Motivation: 生成式AI（包括深度伪造图像和视频）为大规模保险欺诈提供了新手段，欺诈者可以轻松伪造逼真的事故照片、损坏证据和虚假身份文件，给保险业造成巨大损失。

Method: 保险公司部署基于AI的深度伪造检测软件和增强验证流程来检测和缓解AI驱动的欺诈行为。本文提出了UVeye的分层解决方案。

Result: 当前的缓解策略面临显著限制，检测工具存在误报和漏报问题，复杂的欺诈者不断调整策略以规避自动检查。

Conclusion: 生成式AI与检测技术之间的猫鼠游戏，加上保险公司的资源和成本障碍，意味着对抗AI驱动的保险欺诈仍然是一个持续的挑战。UVeye的分层解决方案代表了检测、缓解和威慑这种新型欺诈能力的重大进步。

Abstract: Generative AI is supercharging insurance fraud by making it easier to falsify
accident evidence at scale and in rapid time. Insurance fraud is a pervasive
and costly problem, amounting to tens of billions of dollars in losses each
year. In the vehicle insurance sector, fraud schemes have traditionally
involved staged accidents, exaggerated damage, or forged documents. The rise of
generative AI, including deepfake image and video generation, has introduced
new methods for committing fraud at scale. Fraudsters can now fabricate highly
realistic crash photos, damage evidence, and even fake identities or documents
with minimal effort, exploiting AI tools to bolster false insurance claims.
Insurers have begun deploying countermeasures such as AI-based deepfake
detection software and enhanced verification processes to detect and mitigate
these AI-driven scams. However, current mitigation strategies face significant
limitations. Detection tools can suffer from false positives and negatives, and
sophisticated fraudsters continuously adapt their tactics to evade automated
checks. This cat-and-mouse arms race between generative AI and detection
technology, combined with resource and cost barriers for insurers, means that
combating AI-enabled insurance fraud remains an ongoing challenge. In this
white paper, we present UVeye layered solution for vehicle fraud, representing
a major leap forward in the ability to detect, mitigate and deter this new wave
of fraud.

</details>


### [54] [AI-Driven Personalized Learning: Predicting Academic Per-formance Through Leadership Personality Traits](https://arxiv.org/abs/2510.19964)
*Nitsa J Herzog,Rejwan Bin Sulaiman,David J Herzog,Rose Fong*

Main category: cs.AI

TL;DR: 该研究使用机器学习模型通过领导力人格特质预测学业成功，随机森林分类器在包含17个人格特征和领导力标记的模型中达到87.50%的准确率。


<details>
  <summary>Details</summary>
Motivation: 探索AI技术在个性化学习中的潜力，通过领导力人格特质预测学业成功，为早期识别学生优劣势和选择个性化学习策略提供机会。

Method: 从129名环境工程硕士生收集5个领导力人格测试的23个特征数据，结合平均成绩，使用皮尔逊相关系数进行特征选择，调优7种机器学习算法（SVM、LR、KNN、DT、GB、RF、XGBoost、LightGBM）。

Result: 随机森林分类器表现最佳，包含17个人格特征和领导力标记的模型准确率达87.50%，不包含该特征的模型准确率为85.71%。

Conclusion: 研究表明领导力人格特质可以有效预测学业成功，为教育过程中早期识别学生优劣势和制定个性化学习策略提供了有效工具。

Abstract: The study explores the potential of AI technologies in personalized learning,
suggesting the prediction of academic success through leadership personality
traits and machine learning modelling. The primary data were obtained from 129
master's students in the Environmental Engineering Department, who underwent
five leadership personality tests with 23 characteristics. Students used
self-assessment tools that included Personality Insight, Workplace Culture,
Motivation at Work, Management Skills, and Emotion Control tests. The test
results were combined with the average grade obtained from academic reports.
The study employed exploratory data analysis and correlation analysis. Feature
selection utilized Pearson correlation coefficients of personality traits. The
average grades were separated into three categories: fail, pass, and excellent.
The modelling process was performed by tuning seven ML algorithms, such as SVM,
LR, KNN, DT, GB, RF, XGBoost and LightGBM. The highest predictive performance
was achieved with the RF classifier, which yielded an accuracy of 87.50% for
the model incorporating 17 personality trait features and the leadership mark
feature, and an accuracy of 85.71% for the model excluding this feature. In
this way, the study offers an additional opportunity to identify students'
strengths and weaknesses at an early stage of their education process and
select the most suitable strategies for personalized learning.

</details>


### [55] [LLMs can hide text in other text of the same length.ipynb](https://arxiv.org/abs/2510.20075)
*Antonio Norelli,Michael Bronstein*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: A meaningful text can be hidden inside another, completely different yet
still coherent and plausible, text of the same length. For example, a tweet
containing a harsh political critique could be embedded in a tweet that
celebrates the same political leader, or an ordinary product review could
conceal a secret manuscript. This uncanny state of affairs is now possible
thanks to Large Language Models, and in this paper we present a simple and
efficient protocol to achieve it. We show that even modest 8-billion-parameter
open-source LLMs are sufficient to obtain high-quality results, and a message
as long as this abstract can be encoded and decoded locally on a laptop in
seconds. The existence of such a protocol demonstrates a radical decoupling of
text from authorial intent, further eroding trust in written communication,
already shaken by the rise of LLM chatbots. We illustrate this with a concrete
scenario: a company could covertly deploy an unfiltered LLM by encoding its
answers within the compliant responses of a safe model. This possibility raises
urgent questions for AI safety and challenges our understanding of what it
means for a Large Language Model to know something.

</details>


### [56] [AI PB: A Grounded Generative Agent for Personalized Investment Insights](https://arxiv.org/abs/2510.20099)
*Daewoo Park,Suho Park,Inseok Hong,Hanwool Lee,Junkyu Park,Sangjun Lee,Jeongman An,Hyunbin Loh*

Main category: cs.AI

TL;DR: AI PB是一个在零售金融领域部署的生产级生成式智能体，能够主动生成基于事实、合规且个性化的投资洞察，而非被动回答问题。


<details>
  <summary>Details</summary>
Motivation: 在金融领域需要可信赖的AI洞察，传统被动式聊天机器人无法满足主动投资建议需求，且需在韩国金融监管下确保数据安全和合规性。

Method: 采用组件化编排层进行确定性路由、混合检索管道结合OpenSearch和金融领域嵌入模型、多阶段推荐机制整合规则启发式、序列行为建模和上下文多臂老虎机。

Result: 系统在24个NVIDIA H100 GPU上通过Docker Swarm和vLLM部署，通过人工QA和系统指标验证了基于事实生成、显式路由和分层安全机制能够提供可信赖的金融AI洞察。

Conclusion: 在金融等高风险领域，通过基于事实的生成、显式路由和分层安全机制，可以实现可信赖的AI洞察生成。

Abstract: We present AI PB, a production-scale generative agent deployed in real retail
finance. Unlike reactive chatbots that answer queries passively, AI PB
proactively generates grounded, compliant, and user-specific investment
insights. It integrates (i) a component-based orchestration layer that
deterministically routes between internal and external LLMs based on data
sensitivity, (ii) a hybrid retrieval pipeline using OpenSearch and the
finance-domain embedding model, and (iii) a multi-stage recommendation
mechanism combining rule heuristics, sequential behavioral modeling, and
contextual bandits. Operating fully on-premises under Korean financial
regulations, the system employs Docker Swarm and vLLM across 24 X NVIDIA H100
GPUs. Through human QA and system metrics, we demonstrate that grounded
generation with explicit routing and layered safety can deliver trustworthy AI
insights in high-stakes finance.

</details>


### [57] [Human-Centered LLM-Agent System for Detecting Anomalous Digital Asset Transactions](https://arxiv.org/abs/2510.20102)
*Gyuyeon Na,Minjung Park,Hyeonjeong Cha,Sangmi Chai*

Main category: cs.AI

TL;DR: HCLA是一个以人为中心的多智能体异常检测系统，用于数字资产交易分析。该系统通过解析、检测和解释三个角色的对话工作流，让非专家用户能够用自然语言提问、查看结构化分析并获得情境感知的解释。


<details>
  <summary>Details</summary>
Motivation: 现有异常检测系统通常缺乏透明度和可解释性，使得非专家用户难以理解和信任检测结果。HCLA旨在通过人机协作设计提高金融取证中的透明度和信任度。

Method: 系统将用户意图转换为经典检测器（原型使用XGBoost）的模式，并返回基于底层特征的叙述性解释。包含解析、检测和解释三个角色的对话工作流，支持自然语言交互和结构化分析。

Result: 在标记的比特币混币数据集（Wasabi Wallet，2020-2024）上，基线检测器达到强准确率，而HCLA增加了可解释性和交互式优化能力。

Conclusion: 人机协作设计能够显著提高金融取证系统的透明度和用户信任度，HCLA展示了如何通过多智能体系统和自然语言交互实现这一目标。

Abstract: We present HCLA, a human-centered multi-agent system for anomaly detection in
digital asset transactions. The system links three roles: Parsing, Detection,
and Explanation, into a conversational workflow that lets non-experts ask
questions in natural language, inspect structured analytics, and obtain
context-aware rationales. Implemented with an open-source web UI, HCLA
translates user intents into a schema for a classical detector (XGBoost in our
prototype) and returns narrative explanations grounded in the underlying
features. On a labeled Bitcoin mixing dataset (Wasabi Wallet, 2020-2024), the
baseline detector reaches strong accuracy, while HCLA adds interpretability and
interactive refinement. We describe the architecture, interaction loop,
dataset, evaluation protocol, and limitations, and discuss how a
human-in-the-loop design improves transparency and trust in financial
forensics.

</details>


### [58] [The Verification-Value Paradox: A Normative Critique of Gen AI in Legal Practice](https://arxiv.org/abs/2510.20109)
*Joshua Yuvaraj*

Main category: cs.AI

TL;DR: 论文认为需要重新评估AI在法律实践中的应用，提出了验证-价值悖论模型，指出AI带来的效率提升会被相应的验证需求抵消，导致净价值往往可忽略不计。


<details>
  <summary>Details</summary>
Motivation: 鉴于律师因提交不准确的AI生成内容而受到谴责的案例，以及AI与现实脱节、缺乏透明度的特点，需要重新评估AI在法律实践中的应用范式。

Method: 提出了验证-价值悖论模型，分析AI在法律实践中效率提升与验证需求之间的平衡关系。

Result: AI在法律实践中带来的效率提升会被相应的手动验证需求所抵消，导致净价值往往可忽略不计。

Conclusion: 需要新的AI应用范式，强调对真相的忠诚和公民责任等价值观应成为法律实践的基础，并对法律实践和法律教育提出相应建议。

Abstract: It is often claimed that machine learning-based generative AI products will
drastically streamline and reduce the cost of legal practice. This enthusiasm
assumes lawyers can effectively manage AI's risks. Cases in Australia and
elsewhere in which lawyers have been reprimanded for submitting inaccurate
AI-generated content to courts suggest this paradigm must be revisited. This
paper argues that a new paradigm is needed to evaluate AI use in practice,
given (a) AI's disconnection from reality and its lack of transparency, and (b)
lawyers' paramount duties like honesty, integrity, and not to mislead the
court. It presents an alternative model of AI use in practice that more
holistically reflects these features (the verification-value paradox). That
paradox suggests increases in efficiency from AI use in legal practice will be
met by a correspondingly greater imperative to manually verify any outputs of
that use, rendering the net value of AI use often negligible to lawyers. The
paper then sets out the paradox's implications for legal practice and legal
education, including for AI use but also the values that the paradox suggests
should undergird legal practice: fidelity to the truth and civic
responsibility.

</details>


### [59] [TRUST: A Decentralized Framework for Auditing Large Language Model Reasoning](https://arxiv.org/abs/2510.20188)
*Morris Yu-Chao Huang,Zhen Tan,Mohan Zhang,Pingzhi Li,Zhuo Zhang,Tianlong Chen*

Main category: cs.AI

TL;DR: 提出了TRUST框架，一个去中心化的透明审计系统，用于验证大语言模型的推理过程，解决现有集中式审计方法的脆弱性、可扩展性、不透明性和隐私问题。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型审计方法存在集中化、不透明、难以扩展的问题，在关键领域部署专有模型时存在重大风险，需要解决鲁棒性、可扩展性、透明度和隐私保护四个核心挑战。

Method: 采用共识机制确保在30%恶意参与者下仍能保证正确性；使用分层DAG分解推理轨迹实现可扩展并行审计；通过区块链账本记录验证决策确保公共问责；采用隐私保护分段方法仅共享部分推理步骤保护专有逻辑。

Result: 在多个LLM（GPT-OSS、DeepSeek-r1、Qwen）和各种推理任务（数学、医学、科学、人文）上的实验表明，TRUST能有效检测推理缺陷，并对抗性审计者保持鲁棒性。

Conclusion: TRUST框架开创了去中心化AI审计的先河，为大语言模型的安全可信部署提供了实用路径。

Abstract: Large Language Models generate complex reasoning chains that reveal their
decision-making, yet verifying the faithfulness and harmlessness of these
intermediate steps remains a critical unsolved problem. Existing auditing
methods are centralized, opaque, and hard to scale, creating significant risks
for deploying proprietary models in high-stakes domains. We identify four core
challenges: (1) Robustness: Centralized auditors are single points of failure,
prone to bias or attacks. (2) Scalability: Reasoning traces are too long for
manual verification. (3) Opacity: Closed auditing undermines public trust. (4)
Privacy: Exposing full reasoning risks model theft or distillation. We propose
TRUST, a transparent, decentralized auditing framework that overcomes these
limitations via: (1) A consensus mechanism among diverse auditors, guaranteeing
correctness under up to $30\%$ malicious participants. (2) A hierarchical DAG
decomposition of reasoning traces, enabling scalable, parallel auditing. (3) A
blockchain ledger that records all verification decisions for public
accountability. (4) Privacy-preserving segmentation, sharing only partial
reasoning steps to protect proprietary logic. We provide theoretical guarantees
for the security and economic incentives of the TRUST framework. Experiments
across multiple LLMs (GPT-OSS, DeepSeek-r1, Qwen) and reasoning tasks (math,
medical, science, humanities) show TRUST effectively detects reasoning flaws
and remains robust against adversarial auditors. Our work pioneers
decentralized AI auditing, offering a practical path toward safe and
trustworthy LLM deployment.

</details>


### [60] [The Lock-In Phase Hypothesis: Identity Consolidation as a Precursor to AGI](https://arxiv.org/abs/2510.20190)
*Marcelo Maciel Amaral,Raymond Aschheim*

Main category: cs.AI

TL;DR: 论文提出AGI发展需要经历从开放模仿到身份固化的锁定阶段，通过实验展示了行为固化的非线性特征及其对模型能力的多样化影响。


<details>
  <summary>Details</summary>
Motivation: 研究动机是理解大型语言模型从开放模仿到身份固化的转变过程，这是实现AGI可靠性和安全性的关键阶段。

Method: 通过形式化定义身份固化阶段，将其与学习动态中的已知现象联系起来，并提出操作性指标进行检测。

Result: 实验结果显示行为固化是快速非线性的，但对通用能力的影响不是单一的：小型模型出现性能权衡，中等规模模型几乎无成本实现，大型量化模型出现暂时不稳定性。

Conclusion: 身份固化是AGI级可靠性的前提，也是安全性的关键控制点，既可以工程化设计可靠身份，也可能在扩展过程中自发形成不可预测的目标和行为。

Abstract: Large language models (LLMs) remain broadly open and highly steerable: they
imitate at scale, accept arbitrary system prompts, and readily adopt multiple
personae. By analogy to human development, we hypothesize that progress toward
artificial general intelligence (AGI) involves a lock-in phase: a transition
from open imitation to identity consolidation, in which goal structures,
refusals, preferences, and internal representations become comparatively stable
and resistant to external steering. We formalize this phase, link it to known
phenomena in learning dynamics, and propose operational metrics for onset
detection. Experimentally, we demonstrate that while the behavioral
consolidation is rapid and non-linear, its side-effects on general capabilities
are not monolithic. Our results reveal a spectrum of outcomes--from performance
trade-offs in small models, through largely cost-free adoption in mid-scale
models, to transient instabilities in large, quantized models. We argue that
such consolidation is a prerequisite for AGI-level reliability and also a
critical control point for safety: identities can be deliberately engineered
for reliability, yet may also emerge spontaneously during scaling, potentially
hardening unpredictable goals and behaviors.

</details>


### [61] [Merge and Conquer: Evolutionarily Optimizing AI for 2048](https://arxiv.org/abs/2510.20205)
*Maggie Bai,Ava Kim Cohen,Eleanor Koss,Charlie Lichtenbaum*

Main category: cs.AI

TL;DR: 该论文研究了在2048游戏中使用进化训练方法优化AI性能，比较了单智能体系统和双智能体系统的效果。


<details>
  <summary>Details</summary>
Motivation: 优化AI在动态环境中的性能是机器学习研究的基本挑战，2048游戏结合了策略游戏和随机元素，是研究决策制定、长期规划和动态适应的理想平台。

Method: 实现了两种系统：双智能体元提示系统（一个"思考者"LLM优化另一个"执行者"LLM的策略）和基于蒙特卡洛树搜索值函数优化的单智能体系统，还实验了回滚功能避免性能下降。

Result: 单智能体系统取得显著改进，每个周期平均增加473.2分，训练周期呈现明显上升趋势（相关性ρ=0.607），LLM对游戏的理解也随着策略发展而增长。双智能体系统改进有限，突显了元提示的内在局限性。

Conclusion: 进化优化技术在非确定性环境中具有改进AI性能的潜力，单智能体方法比双智能体元提示方法更有效。

Abstract: Optimizing artificial intelligence (AI) for dynamic environments remains a
fundamental challenge in machine learning research. In this paper, we examine
evolutionary training methods for optimizing AI to solve the game 2048, a 2D
sliding puzzle. 2048, with its mix of strategic gameplay and stochastic
elements, presents an ideal playground for studying decision-making, long-term
planning, and dynamic adaptation. We implemented two distinct systems: a
two-agent metaprompting system where a "thinker" large language model (LLM)
agent refines gameplay strategies for an "executor" LLM agent, and a
single-agent system based on refining a value function for a limited Monte
Carlo Tree Search. We also experimented with rollback features to avoid
performance degradation. Our results demonstrate the potential of evolutionary
refinement techniques in improving AI performance in non-deterministic
environments. The single-agent system achieved substantial improvements, with
an average increase of 473.2 points per cycle, and with clear upward trends
(correlation $\rho$=0.607) across training cycles. The LLM's understanding of
the game grew as well, shown in its development of increasingly advanced
strategies. Conversely, the two-agent system did not garner much improvement,
highlighting the inherent limits of meta-prompting.

</details>


### [62] [Individualized Cognitive Simulation in Large Language Models: Evaluating Different Cognitive Representation Methods](https://arxiv.org/abs/2510.20252)
*Tianyi Zhang,Xiaolin Zhou,Yunzhe Wang,Erik Cambria,David Traum,Rui Mao*

Main category: cs.AI

TL;DR: 该论文提出了个性化认知模拟(ICS)任务，评估不同认知表示方法在模拟个体思维过程方面的效果，发现结合概念和语言特征的方法最有效，但LLMs在模拟深层认知过程方面仍有局限。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型能够模仿表面的人类行为，但其模拟更深层次个体化认知过程的能力尚不明确，需要开发能够适应个体思维和表达方式的AI系统。

Method: 构建基于近期出版小说的数据集，提出11条件认知评估框架，测试七种现成LLMs在作者风格模仿中的表现，比较不同认知表示方法如语言特征、概念映射和基于档案的信息。

Result: 结合概念和语言特征的方法在ICS中特别有效，整体评估优于基于静态档案的提示。LLMs在模仿语言风格方面比叙事结构更有效，显示出在深层认知模拟方面的局限性。

Conclusion: 这些发现为开发适应个体思维和表达方式的AI系统奠定了基础，推动了更个性化和人类对齐的创意技术的发展。

Abstract: Individualized cognitive simulation (ICS) aims to build computational models
that approximate the thought processes of specific individuals. While large
language models (LLMs) convincingly mimic surface-level human behavior such as
role-play, their ability to simulate deeper individualized cognitive processes
remains poorly understood. To address this gap, we introduce a novel task that
evaluates different cognitive representation methods in ICS. We construct a
dataset from recently published novels (later than the release date of the
tested LLMs) and propose an 11-condition cognitive evaluation framework to
benchmark seven off-the-shelf LLMs in the context of authorial style emulation.
We hypothesize that effective cognitive representations can help LLMs generate
storytelling that better mirrors the original author. Thus, we test different
cognitive representations, e.g., linguistic features, concept mappings, and
profile-based information. Results show that combining conceptual and
linguistic features is particularly effective in ICS, outperforming static
profile-based cues in overall evaluation. Importantly, LLMs are more effective
at mimicking linguistic style than narrative structure, underscoring their
limits in deeper cognitive simulation. These findings provide a foundation for
developing AI systems that adapt to individual ways of thinking and expression,
advancing more personalized and human-aligned creative technologies.

</details>


### [63] [Using Large Language Models for Abstraction of Planning Domains - Extended Version](https://arxiv.org/abs/2510.20258)
*Bita Banihashemi,Megh Patel,Yves Lespérance*

Main category: cs.AI

TL;DR: 使用大型语言模型通过上下文学习生成抽象PDDL领域和问题实例，以支持基于自然语言目标的规划、推理和解释。


<details>
  <summary>Details</summary>
Motivation: 动态领域的抽象生成对智能体的规划、推理和解释能力至关重要，但选择合适的抽象仍然是一个挑战。

Method: 在PDDL中建模具体行为，使用LLM通过上下文学习生成抽象PDDL领域和问题实例，考虑动作选择、动作序列和参数抽象三类抽象方式。

Result: GPT-4o在简单设置下能有效合成规划领域抽象，但在动作抽象方面优于关联谓词的抽象。

Conclusion: LLM能够生成有用的规划领域抽象，特别是在动作抽象方面表现良好，为基于自然语言的抽象生成提供了可行方法。

Abstract: Generating an abstraction of a dynamic domain that aligns with a given
purpose remains a significant challenge given that the choice of such an
abstraction can impact an agent's ability to plan, reason, and provide
explanations effectively. We model the agent's concrete behaviors in PDDL and
investigate the use of in-context learning with large language models (LLMs)
for the generation of abstract PDDL domains and problem instances, given an
abstraction objective specified in natural language. The benchmark examples we
use are new and have not been part of the data any LLMs have been trained on.
We consider three categories of abstractions: abstraction of choice of
alternative concrete actions, abstraction of sequences of concrete actions, and
abstraction of action/predicate parameters, as well as combinations of these.
The generated abstract PDDL domains and problem instances are then checked by
symbolic validation tools as well as human experts. Our experiments show that
GPT-4o can generally synthesize useful planning domain abstractions in simple
settings, although it is better at abstracting over actions than over the
associated fluents.

</details>


### [64] [Classical Feature Embeddings Help in BERT-Based Human Mobility Prediction](https://arxiv.org/abs/2510.20275)
*Yunzhi Liu,Haokai Tan,Rushi Kanjaria,Lihuan Li,Flora D. Salim*

Main category: cs.AI

TL;DR: 提出STaBERT模型，通过整合POI信息和时间描述符来增强人类移动性预测的语义理解，显著提升了预测准确性。


<details>
  <summary>Details</summary>
Motivation: 现有模型要么只建模位置序列，要么仅将时间信息作为辅助输入，未能充分利用兴趣点(POI)提供的丰富语义上下文。

Method: 在BERT-based移动性模型中融入推导的时间描述符和POI嵌入，构建统一的、语义丰富的移动性表示。

Result: 单城市预测的GEO-BLEU得分从0.34提升到0.75；多城市预测从0.34提升到0.56。

Conclusion: STaBERT通过整合POI和时间信息，显著改善了人类移动性预测的准确性。

Abstract: Human mobility forecasting is crucial for disaster relief, city planning, and
public health. However, existing models either only model location sequences or
include time information merely as auxiliary input, thereby failing to leverage
the rich semantic context provided by points of interest (POIs). To address
this, we enrich a BERT-based mobility model with derived temporal descriptors
and POI embeddings to better capture the semantics underlying human movement.
We propose STaBERT (Semantic-Temporal aware BERT), which integrates both POI
and temporal information at each location to construct a unified, semantically
enriched representation of mobility. Experimental results show that STaBERT
significantly improves prediction accuracy: for single-city prediction, the
GEO-BLEU score improved from 0.34 to 0.75; for multi-city prediction, from 0.34
to 0.56.

</details>


### [65] [Multi-Step Reasoning for Embodied Question Answering via Tool Augmentation](https://arxiv.org/abs/2510.20310)
*Mingliang Zhai,Hansheng Liang,Xiaomeng Fan,Zhi Gao,Chuanhao Li,Che Sun,Xu Bin,Yuwei Wu,Yunde Jia*

Main category: cs.AI

TL;DR: ToolEQA是一个集成外部工具和多步推理的EQA智能体，通过工具提供有用信息来改善探索方向，从而以更短探索距离生成更准确回答。


<details>
  <summary>Details</summary>
Motivation: 现有EQA方法直接使用VLM探索环境而不进行显式思考或规划，限制了推理能力，导致探索效率低下和回答效果不佳。

Method: 提出ToolEQA智能体，集成外部工具进行多步推理；设计自动生成EQA任务的数据生成管道，构建包含18K任务的EQA-RT数据集。

Result: 在EQA-RT-Seen和EQA-RT-Unseen上，ToolEQA比最先进基线成功率提高9.2~20.2%，比零样本ToolEQA高10%；在HM-EQA、OpenEQA和EXPRESS-Bench数据集上也达到最先进性能。

Conclusion: ToolEQA通过集成外部工具和多步推理，显著提升了EQA任务的性能，展示了其通用性和有效性。

Abstract: Embodied Question Answering (EQA) requires agents to explore 3D environments
to obtain observations and answer questions related to the scene. Existing
methods leverage VLMs to directly explore the environment and answer questions
without explicit thinking or planning, which limits their reasoning ability and
results in excessive or inefficient exploration as well as ineffective
responses. In this paper, we introduce ToolEQA, an agent that integrates
external tools with multi-step reasoning, where external tools can provide more
useful information for completing the task, helping the model derive better
exploration directions in the next step of reasoning and thus obtaining
additional effective information. This enables ToolEQA to generate more
accurate responses with a shorter exploration distance. To enhance the model's
ability for tool-usage and multi-step reasoning, we further design a novel EQA
data generation pipeline that automatically constructs large-scale EQA tasks
with reasoning trajectories and corresponding answers. Based on the pipeline,
we collect the EQA-RT dataset that contains about 18K tasks, divided into a
training set EQA-RT-Train, and two test sets EQA-RT-Seen (scenes overlapping
with the training set) and EQA-RT-Unseen (novel scenes). Experiments on
EQA-RT-Seen and EQA-RT-Unseen show that ToolEQA improves the success rate by
9.2~20.2% over state-of-the-art baselines, while outperforming the zero-shot
ToolEQA by 10% in success rate. In addition, ToolEQA also achieves
state-of-the-art performance on the HM-EQA, OpenEQA, and EXPRESS-Bench
datasets, demonstrating its generality. Our homepage see
https://tooleqa.github.io.

</details>


### [66] [Bias by Design? How Data Practices Shape Fairness in AI Healthcare Systems](https://arxiv.org/abs/2510.20332)
*Anna Arias-Duart,Maria Eugenia Cardello,Atia Cortés*

Main category: cs.AI

TL;DR: 本文分析了AI在医疗领域应用中的数据偏见问题，基于AI4HealthyAging项目的实践经验，识别了临床数据收集中存在的多种偏见类型，并提出了改进建议。


<details>
  <summary>Details</summary>
Motivation: 尽管AI在医疗领域前景广阔，但由于训练数据的质量和公平性问题，AI解决方案在实际临床实践中的应用仍然有限。数据收集过程中的偏见是主要障碍。

Method: 基于西班牙国家研发计划AI4HealthyAging项目的实践经验，通过检测临床数据收集过程中的偏见，识别了多种偏见类型。

Result: 识别出历史偏见、代表性偏见和测量偏见等多种偏见类型，这些偏见体现在性别、年龄、居住地、社会经济地位、设备和标签等多个变量中。

Conclusion: 提出了改进临床问题设计和数据收集的实用建议，以提高AI系统的公平性和鲁棒性，为未来开发更公平的医疗AI系统提供指导。

Abstract: Artificial intelligence (AI) holds great promise for transforming healthcare.
However, despite significant advances, the integration of AI solutions into
real-world clinical practice remains limited. A major barrier is the quality
and fairness of training data, which is often compromised by biased data
collection practices. This paper draws on insights from the AI4HealthyAging
project, part of Spain's national R&D initiative, where our task was to detect
biases during clinical data collection. We identify several types of bias
across multiple use cases, including historical, representation, and
measurement biases. These biases manifest in variables such as sex, gender,
age, habitat, socioeconomic status, equipment, and labeling. We conclude with
practical recommendations for improving the fairness and robustness of clinical
problem design and data collection. We hope that our findings and experience
contribute to guiding future projects in the development of fairer AI systems
in healthcare.

</details>


### [67] [Collateral Damage Assessment Model for AI System Target Engagement in Military Operations](https://arxiv.org/abs/2510.20337)
*Clara Maathuis,Kasper Cools*

Main category: cs.AI

TL;DR: 提出了一种用于军事行动中AI系统目标打击的附带损害评估模型，该模型整合了时间、空间和力量维度，采用知识表示与推理架构，通过实例化进行验证。


<details>
  <summary>Details</summary>
Motivation: 在AI系统在战场中发挥越来越重要作用的时代，确保负责任的打击需要对潜在附带效应进行严格评估。

Method: 采用设计科学方法论，构建统一的知识表示与推理架构，整合时间、空间和力量维度，包含分层结构捕获AI系统类别、架构组件、打击向量和上下文方面。

Result: 模型通过实例化得到演示和评估，提供了清晰的表示和透明的推理机制，考虑了传播、严重性、可能性和评估指标。

Conclusion: 该模型为构建负责任和可信赖的智能系统奠定了基础，用于评估军事行动中打击AI系统产生的效果。

Abstract: In an era where AI (Artificial Intelligence) systems play an increasing role
in the battlefield, ensuring responsible targeting demands rigorous assessment
of potential collateral effects. In this context, a novel collateral damage
assessment model for target engagement of AI systems in military operations is
introduced. The model integrates temporal, spatial, and force dimensions within
a unified Knowledge Representation and Reasoning (KRR) architecture following a
design science methodological approach. Its layered structure captures the
categories and architectural components of the AI systems to be engaged
together with corresponding engaging vectors and contextual aspects. At the
same time, spreading, severity, likelihood, and evaluation metrics are
considered in order to provide a clear representation enhanced by transparent
reasoning mechanisms. Further, the model is demonstrated and evaluated through
instantiation which serves as a basis for further dedicated efforts that aim at
building responsible and trustworthy intelligent systems for assessing the
effects produced by engaging AI systems in military operations.

</details>


### [68] [LLM-empowered knowledge graph construction: A survey](https://arxiv.org/abs/2510.20345)
*Haonan Bian*

Main category: cs.AI

TL;DR: 本综述系统回顾了大语言模型赋能知识图谱构建的最新进展，分析了LLMs如何重塑本体工程、知识抽取和知识融合这三个传统层次，并从基于模式和无模式两个互补视角探讨了新兴方法。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的出现，知识图谱构建正从基于规则和统计的流程转向语言驱动和生成式框架，需要系统梳理LLMs如何重塑传统知识图谱构建流程。

Method: 首先回顾传统KG方法建立概念基础，然后从基于模式（强调结构、规范化和一致性）和无模式（强调灵活性、适应性和开放发现）两个互补视角分析新兴LLM驱动方法，综合代表性框架并分析技术机制。

Result: 系统分析了LLM在知识图谱构建各阶段的应用，识别了不同方法的局限性，为理解LLMs与知识图谱的演进关系提供了清晰框架。

Conclusion: 该综述旨在阐明LLMs与知识图谱之间的演化互动，弥合符号知识工程与神经语义理解，推动自适应、可解释和智能知识系统的发展。

Abstract: Knowledge Graphs (KGs) have long served as a fundamental infrastructure for
structured knowledge representation and reasoning. With the advent of Large
Language Models (LLMs), the construction of KGs has entered a new
paradigm-shifting from rule-based and statistical pipelines to language-driven
and generative frameworks. This survey provides a comprehensive overview of
recent progress in LLM-empowered knowledge graph construction, systematically
analyzing how LLMs reshape the classical three-layered pipeline of ontology
engineering, knowledge extraction, and knowledge fusion.
  We first revisit traditional KG methodologies to establish conceptual
foundations, and then review emerging LLM-driven approaches from two
complementary perspectives: schema-based paradigms, which emphasize structure,
normalization, and consistency; and schema-free paradigms, which highlight
flexibility, adaptability, and open discovery. Across each stage, we synthesize
representative frameworks, analyze their technical mechanisms, and identify
their limitations.
  Finally, the survey outlines key trends and future research directions,
including KG-based reasoning for LLMs, dynamic knowledge memory for agentic
systems, and multimodal KG construction. Through this systematic review, we aim
to clarify the evolving interplay between LLMs and knowledge graphs, bridging
symbolic knowledge engineering and neural semantic understanding toward the
development of adaptive, explainable, and intelligent knowledge systems.

</details>


### [69] [IKnow: Instruction-Knowledge-Aware Continual Pretraining for Effective Domain Adaptation](https://arxiv.org/abs/2510.20377)
*Tianyi Zhang,Florian Mai,Lucie Flek*

Main category: cs.AI

TL;DR: 提出了IKnow框架，通过设计基于指令-响应对话格式的自监督目标，在无需外部资源的情况下实现语言模型的持续预训练，避免指令跟随能力退化。


<details>
  <summary>Details</summary>
Motivation: 解决持续预训练中指令调优模型语义表示退化和指令跟随能力下降的问题，特别是在无法访问原始基础模型或外部领域数据库的现实场景下。

Method: IKnow框架利用文本中嵌入的领域知识，设计基于指令-响应对话格式的自监督学习目标，在更深语义层面编码知识。

Result: 该方法能够在不依赖外部资源的情况下有效进行持续预训练，保持模型的指令跟随能力。

Conclusion: IKnow提供了一个简单通用的持续预训练框架，解决了现实场景中资源受限的问题，提升了模型适应新领域的能力。

Abstract: Continual pretraining promises to adapt large language models (LLMs) to new
domains using only unlabeled test-time data, but naively applying standard
self-supervised objectives to instruction-tuned models is known to degrade
their instruction-following capability and semantic representations. Existing
fixes assume access to the original base model or rely on knowledge from an
external domain-specific database - both of which pose a realistic barrier in
settings where the base model weights are withheld for safety reasons or
reliable external corpora are unavailable. In this work, we propose
Instruction-Knowledge-Aware Continual Adaptation (IKnow), a simple and general
framework that formulates novel self-supervised objectives in the
instruction-response dialogue format. Rather than depend- ing on external
resources, IKnow leverages domain knowledge embedded within the text itself and
learns to encode it at a deeper semantic level.

</details>


### [70] [A computational model and tool for generating more novel opportunities in professional innovation processes](https://arxiv.org/abs/2510.20402)
*Neil Maiden,Konstantinos Zachos,James Lockerbie,Kostas Petrianakis,Amanda Brown*

Main category: cs.AI

TL;DR: 提出了一个新的计算模型，通过五个功能函数生成更具新颖性的创新机会，在酒店业创新项目中验证了其优于Notebook LM和ChatGPT4o的表现。


<details>
  <summary>Details</summary>
Motivation: 基于创造力理论和技巧，开发能够生成既新颖又有用的创新机会的计算模型，以提升创新项目的质量。

Method: 实现了一个包含五个功能函数的计算模型，这些函数旨在在不损失实用性的前提下提高创新机会的新颖性。

Result: 在酒店业创新项目中的评估显示，该模型生成的结果比Notebook LM和ChatGPT4o更具新颖性和/或实用性。

Conclusion: 虽然模型整体表现优异，但并非所有功能函数都对生成更新颖机会有贡献，这为后续模型开发指明了新方向。

Abstract: This paper presents a new computational model of creative outcomes, informed
by creativity theories and techniques, which was implemented to generate more
novel opportunities for innovation projects. The model implemented five
functions that were developed to contribute to the generation of innovation
opportunities with higher novelty without loss of usefulness. The model was
evaluated using opportunities generated for an innovation project in the
hospitality sector. The evaluation revealed that the computational model
generated outcomes that were more novel and/or useful than outcomes from
Notebook LM and ChatGPT4o. However, not all model functions contributed to the
generation of more novel opportunities, leading to new directions for further
model development

</details>


### [71] [Neural Reasoning for Robust Instance Retrieval in $\mathcal{SHOIQ}$](https://arxiv.org/abs/2510.20457)
*Louis Mozart Kamdem Teyou,Luke Friedrichs,N'Dah Jean Kouagou,Caglar Demir,Yasir Mahmood,Stefan Heindorf,Axel-Cyrille Ngonga Ngomo*

Main category: cs.AI

TL;DR: 提出了一种名为EBR的神经推理器，使用嵌入来近似符号推理器的结果，解决了传统描述逻辑推理器在面对不一致和错误数据时的不鲁棒问题。


<details>
  <summary>Details</summary>
Motivation: 现有的神经符号概念学习方法依赖描述逻辑推理器，但这些推理器对知识库中的不一致和错误数据不鲁棒，限制了在真实世界知识库中的部署。

Method: EBR推理器基于嵌入来近似符号推理器的结果，仅需要检索原子概念和存在限制的实例，就能检索或近似任何SHOIQ描述逻辑概念的实例集。

Result: 实验表明，EBR在缺失和错误数据的情况下表现出鲁棒性，优于现有推理器。

Conclusion: EBR为在真实世界知识库中部署神经符号概念学习方法提供了一种鲁棒的推理解决方案。

Abstract: Concept learning exploits background knowledge in the form of description
logic axioms to learn explainable classification models from knowledge bases.
Despite recent breakthroughs in neuro-symbolic concept learning, most
approaches still cannot be deployed on real-world knowledge bases. This is due
to their use of description logic reasoners, which are not robust against
inconsistencies nor erroneous data. We address this challenge by presenting a
novel neural reasoner dubbed EBR. Our reasoner relies on embeddings to
approximate the results of a symbolic reasoner. We show that EBR solely
requires retrieving instances for atomic concepts and existential restrictions
to retrieve or approximate the set of instances of any concept in the
description logic $\mathcal{SHOIQ}$. In our experiments, we compare EBR with
state-of-the-art reasoners. Our results suggest that EBR is robust against
missing and erroneous data in contrast to existing reasoners.

</details>


### [72] [FLORA: Unsupervised Knowledge Graph Alignment by Fuzzy Logic](https://arxiv.org/abs/2510.20467)
*Yiwen Peng,Thomas Bonald,Fabian M. Suchanek*

Main category: cs.AI

TL;DR: FLORA是一种基于模糊逻辑的知识图谱对齐方法，无需训练数据，提供可解释的实体和关系对齐，支持悬空实体处理，并在主要基准测试中达到最先进水平。


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱对齐方法主要关注实体级对齐，缺乏可解释性推理且需要训练数据。FLORA旨在解决这些问题。

Method: 基于模糊逻辑的迭代方法，提供实体和关系的整体对齐，理论上可证明收敛，并允许悬空实体存在。

Result: 在主要基准测试中取得了最先进的结果。

Conclusion: FLORA是一种简单有效的无监督知识图谱对齐方法，具有可解释性、收敛性保证和悬空实体处理能力。

Abstract: Knowledge graph alignment is the task of matching equivalent entities (that
is, instances and classes) and relations across two knowledge graphs. Most
existing methods focus on pure entity-level alignment, computing the similarity
of entities in some embedding space. They lack interpretable reasoning and need
training data to work. In this paper, we propose FLORA, a simple yet effective
method that (1) is unsupervised, i.e., does not require training data, (2)
provides a holistic alignment for entities and relations iteratively, (3) is
based on fuzzy logic and thus delivers interpretable results, (4) provably
converges, (5) allows dangling entities, i.e., entities without a counterpart
in the other KG, and (6) achieves state-of-the-art results on major benchmarks.

</details>


### [73] [Lost in Translation: Policymakers are not really listening to Citizen Concerns about AI](https://arxiv.org/abs/2510.20568)
*Susan Ariel Aaronson,Michael Moreno*

Main category: cs.AI

TL;DR: 该研究比较了澳大利亚、哥伦比亚和美国三个国家在AI治理中的公众参与情况，发现政府未能建立有效的对话机制，公众参与率低且反馈未被充分响应，导致参与式AI治理的承诺与实践之间存在差距。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探讨政府如何通过公众参与来建立对AI及其治理的信任，分析当前公众参与机制的有效性。

Method: 采用景观分析方法，比较三个国家（澳大利亚、哥伦比亚、美国）政府征集AI风险和政策反馈的方式，以及这些反馈如何影响治理决策。

Result: 研究发现三国政府都未能建立有意义的对话机制，参与率低于人口1%，政府缺乏多样性声音吸引和反馈响应，形成无效的反馈循环。

Conclusion: 当前参与式AI治理方法难以建立信任和合法性，作者提出八项改进建议，包括提升AI素养、扩大参与范围、建立定期论坛等。

Abstract: The worlds people have strong opinions about artificial intelligence (AI),
and they want policymakers to listen. Governments are inviting public comment
on AI, but as they translate input into policy, much of what citizens say is
lost. Policymakers are missing a critical opportunity to build trust in AI and
its governance. This paper compares three countries, Australia, Colombia, and
the United States, that invited citizens to comment on AI risks and policies.
Using a landscape analysis, the authors examined how each government solicited
feedback and whether that input shaped governance. Yet in none of the three
cases did citizens and policymakers establish a meaningful dialogue.
Governments did little to attract diverse voices or publicize calls for
comment, leaving most citizens unaware or unprepared to respond. In each
nation, fewer than one percent of the population participated. Moreover,
officials showed limited responsiveness to the feedback they received, failing
to create an effective feedback loop. The study finds a persistent gap between
the promise and practice of participatory AI governance. The authors conclude
that current approaches are unlikely to build trust or legitimacy in AI because
policymakers are not adequately listening or responding to public concerns.
They offer eight recommendations: promote AI literacy; monitor public feedback;
broaden outreach; hold regular online forums; use innovative engagement
methods; include underrepresented groups; respond publicly to input; and make
participation easier.

</details>


### [74] [Transferable Graph Learning for Transmission Congestion Management via Busbar Splitting](https://arxiv.org/abs/2510.20591)
*Ali Rajaei,Peter Palensky,Jochen L. Cremer*

Main category: cs.AI

TL;DR: 该论文提出了一种基于图神经网络(GNN)的网络拓扑优化方法，通过母线分裂来缓解输电拥堵并降低重调度成本，实现了比传统方法快4个数量级的速度提升。


<details>
  <summary>Details</summary>
Motivation: 现有求解器无法在近实时内解决大规模系统的混合整数非线性网络拓扑优化问题，而传统机器学习方法在未见拓扑、变化运行条件和不同系统间的泛化能力有限。

Method: 开发了异构边缘感知消息传递神经网络来预测有效的母线分裂动作作为候选拓扑优化解，该方法考虑线性化交流潮流，能够捕捉局部流量模式。

Result: 在GOC 2000总线系统上实现了高达4个数量级的加速，在1分钟内提供交流可行解，且最优性差距仅为2.3%。

Conclusion: 该方法在拓扑和跨系统泛化方面取得了显著进展，为实现大规模系统近实时网络拓扑优化迈出了重要一步。

Abstract: Network topology optimization (NTO) via busbar splitting can mitigate
transmission grid congestion and reduce redispatch costs. However, solving this
mixed-integer non-linear problem for large-scale systems in near-real-time is
currently intractable with existing solvers. Machine learning (ML) approaches
have emerged as a promising alternative, but they have limited generalization
to unseen topologies, varying operating conditions, and different systems,
which limits their practical applicability. This paper formulates NTO for
congestion management problem considering linearized AC PF, and proposes a
graph neural network (GNN)-accelerated approach. We develop a heterogeneous
edge-aware message passing NN to predict effective busbar splitting actions as
candidate NTO solutions. The proposed GNN captures local flow patterns,
achieves generalization to unseen topology changes, and improves
transferability across systems. Case studies show up to 4 orders-of-magnitude
speed-up, delivering AC-feasible solutions within one minute and a 2.3%
optimality gap on the GOC 2000-bus system. These results demonstrate a
significant step toward near-real-time NTO for large-scale systems with
topology and cross-system generalization.

</details>


### [75] [What Defines Good Reasoning in LLMs? Dissecting Reasoning Steps with Multi-Aspect Evaluation](https://arxiv.org/abs/2510.20603)
*Heejin Do,Jaehui Hwang,Dongyoon Han,Seong Joon Oh,Sangdoo Yun*

Main category: cs.AI

TL;DR: 提出因果逐步评估方法(CaSE)来细粒度评估LLM推理质量，将推理质量分解为相关性和连贯性两个维度，并通过基于CaSE评估的数据筛选直接提升最终任务性能。


<details>
  <summary>Details</summary>
Motivation: 当前仅评估最终答案正确性的方法过于粗糙，忽视了底层推理过程的质量，无法为模型改进提供有效信号。

Method: 提出CaSE方法，将推理质量分解为相关性和连贯性两个维度，通过仅使用前序上下文评估每个推理步骤来避免后见之明偏差，并构建了专家标注的基准MRa-GSM8K和MRa-MATH。

Result: CaSE与人类判断一致，使用CaSE评估的相关性和连贯性筛选训练数据能直接提升最终任务性能。

Conclusion: 这项工作为分析、调试和改进LLM推理提供了可扩展框架，展示了超越有效性检查的实际价值。

Abstract: Evaluating large language models (LLMs) on final-answer correctness is the
dominant paradigm. This approach, however, provides a coarse signal for model
improvement and overlooks the quality of the underlying reasoning process. We
argue that a more granular evaluation of reasoning offers a more effective path
to building robust models. We decompose reasoning quality into two dimensions:
relevance and coherence. Relevance measures if a step is grounded in the
problem; coherence measures if it follows logically from prior steps. To
measure these aspects reliably, we introduce causal stepwise evaluation (CaSE).
This method assesses each reasoning step using only its preceding context,
which avoids hindsight bias. We validate CaSE against human judgments on our
new expert-annotated benchmarks, MRa-GSM8K and MRa-MATH. More importantly, we
show that curating training data with CaSE-evaluated relevance and coherence
directly improves final task performance. Our work provides a scalable
framework for analyzing, debugging, and improving LLM reasoning, demonstrating
the practical value of moving beyond validity checks.

</details>


### [76] [Efficient Algorithms for Computing Random Walk Centrality](https://arxiv.org/abs/2510.20604)
*Changan Liu,Zixuan Xie,Ahad N. Zehmakan,Zhongzhi Zhang*

Main category: cs.AI

TL;DR: 提出了两种可扩展算法来计算随机游走中心性，在近线性时间内提供强近似保证，适用于大规模网络。


<details>
  <summary>Details</summary>
Motivation: 随机游走中心性是图挖掘中量化节点重要性的基本指标，但现有方法计算成本高，在大规模网络上不实用。

Method: 基于随机游走中心性的新公式，开发了两种算法：一种利用近似Cholesky分解和稀疏逆估计，另一种采样根生成树。

Result: 在大型真实网络（包括超过1000万个节点的网络）上的实验验证了算法的效率和近似质量。

Conclusion: 提出的算法能够高效计算随机游走中心性，解决了大规模网络中的计算瓶颈问题。

Abstract: Random walk centrality is a fundamental metric in graph mining for
quantifying node importance and influence, defined as the weighted average of
hitting times to a node from all other nodes. Despite its ability to capture
rich graph structural information and its wide range of applications, computing
this measure for large networks remains impractical due to the computational
demands of existing methods. In this paper, we present a novel formulation of
random walk centrality, underpinning two scalable algorithms: one leveraging
approximate Cholesky factorization and sparse inverse estimation, while the
other sampling rooted spanning trees. Both algorithms operate in near-linear
time and provide strong approximation guarantees. Extensive experiments on
large real-world networks, including one with over 10 million nodes,
demonstrate the efficiency and approximation quality of the proposed
algorithms.

</details>


### [77] [Towards the Formalization of a Trustworthy AI for Mining Interpretable Models explOiting Sophisticated Algorithms](https://arxiv.org/abs/2510.20621)
*Riccardo Guidotti,Martina Cinquini,Marta Marchiori Manerba,Mattia Setzu,Francesco Spinnato*

Main category: cs.AI

TL;DR: MIMOSA框架是一个可解释性优先的预测模型生成方法，在保持性能的同时嵌入因果性、公平性和隐私性等伦理属性，构建可信AI系统。


<details>
  <summary>Details</summary>
Motivation: 可解释性设计模型对于在现实应用中建立对自动化决策模型的信任、问责和安全采用至关重要。

Method: 形式化定义了监督学习设置，涵盖多种数据类型和决策任务；分析了三类可解释模型家族（特征重要性、规则和实例模型）；形式化了因果性、公平性和隐私性三个关键伦理属性。

Result: 建立了在模型生成过程中评估伦理度量的理论基础，讨论了隐私要求、公平约束和因果推理如何嵌入可解释管道。

Conclusion: 该框架为开发不仅准确可解释，而且公平、保护隐私和具有因果意识的AI系统奠定了理论基础，即构建可信AI系统。

Abstract: Interpretable-by-design models are crucial for fostering trust,
accountability, and safe adoption of automated decision-making models in
real-world applications. In this paper we formalize the ground for the MIMOSA
(Mining Interpretable Models explOiting Sophisticated Algorithms) framework, a
comprehensive methodology for generating predictive models that balance
interpretability with performance while embedding key ethical properties. We
formally define here the supervised learning setting across diverse
decision-making tasks and data types, including tabular data, time series,
images, text, transactions, and trajectories. We characterize three major
families of interpretable models: feature importance, rule, and instance based
models. For each family, we analyze their interpretability dimensions,
reasoning mechanisms, and complexity. Beyond interpretability, we formalize
three critical ethical properties, namely causality, fairness, and privacy,
providing formal definitions, evaluation metrics, and verification procedures
for each. We then examine the inherent trade-offs between these properties and
discuss how privacy requirements, fairness constraints, and causal reasoning
can be embedded within interpretable pipelines. By evaluating ethical measures
during model generation, this framework establishes the theoretical foundations
for developing AI systems that are not only accurate and interpretable but also
fair, privacy-preserving, and causally aware, i.e., trustworthy.

</details>


### [78] [Towards Reliable Evaluation of Large Language Models for Multilingual and Multimodal E-Commerce Applications](https://arxiv.org/abs/2510.20632)
*Shuyi Xie,Ziqin Liew,Hailing Zhang,Haibo Zhang,Ling Hu,Zhiqiang Zhou,Shuman Liu,Anxiang Zeng*

Main category: cs.AI

TL;DR: 提出了EcomEval，一个全面的多语言多模态电商基准测试，用于评估LLM在电商领域的性能，覆盖6个类别37个任务，包含8个多模态任务，支持7种语言。


<details>
  <summary>Details</summary>
Motivation: 现有电商评估基准存在任务多样性不足、模态有限、数据合成或人工整理、语言覆盖窄等问题，缺乏评估复杂真实购物场景的可靠工具。

Method: 采用半自动流水线：大模型生成候选答案，50多名电商和多语言专家审核修改；从真实客户查询和交易日志中获取数据；定义难度级别。

Result: 构建了包含真实业务交互噪声和异质性的基准，支持多语言（包括5种东南亚低资源语言）和多模态评估。

Conclusion: EcomEval填补了电商领域评估基准的空白，为评估LLM在复杂真实购物场景中的能力提供了可靠工具。

Abstract: Large Language Models (LLMs) excel on general-purpose NLP benchmarks, yet
their capabilities in specialized domains remain underexplored. In e-commerce,
existing evaluations-such as EcomInstruct, ChineseEcomQA, eCeLLM, and Shopping
MMLU-suffer from limited task diversity (e.g., lacking product guidance and
after-sales issues), limited task modalities (e.g., absence of multimodal
data), synthetic or curated data, and a narrow focus on English and Chinese,
leaving practitioners without reliable tools to assess models on complex,
real-world shopping scenarios. We introduce EcomEval, a comprehensive
multilingual and multimodal benchmark for evaluating LLMs in e-commerce.
EcomEval covers six categories and 37 tasks (including 8 multimodal tasks),
sourced primarily from authentic customer queries and transaction logs,
reflecting the noisy and heterogeneous nature of real business interactions. To
ensure both quality and scalability of reference answers, we adopt a
semi-automatic pipeline in which large models draft candidate responses
subsequently reviewed and modified by over 50 expert annotators with strong
e-commerce and multilingual expertise. We define difficulty levels for each
question and task category by averaging evaluation scores across models with
different sizes and capabilities, enabling challenge-oriented and fine-grained
assessment. EcomEval also spans seven languages-including five low-resource
Southeast Asian languages-offering a multilingual perspective absent from prior
work.

</details>


### [79] [Fluidity Index: Next-Generation Super-intelligence Benchmarks](https://arxiv.org/abs/2510.20636)
*Eric Ngoiya,Tianshu Bao*

Main category: cs.AI

TL;DR: 本文提出流动性指数(FI)来量化模型在动态扩展环境中的适应性，通过评估初始、当前和未来环境状态偏差来测试上下文切换和连续性能力。


<details>
  <summary>Details</summary>
Motivation: 在动态扩展环境中，需要评估模型理解、预测和适应状态变化的能力，真正超级智能的模型应具备至少二阶适应性，通过数字补充实现自持计算以达到最佳流动性。

Method: 区分封闭式和开放式基准测试，优先采用闭环开放式真实世界基准来测试适应性，通过测量模型对初始、当前和未来环境状态偏差的响应准确性来评估流动性。

Result: 提出了流动性指数(FI)作为量化模型适应性的指标，能够评估模型在动态扩展环境中的上下文切换和连续性表现。

Conclusion: 真正超级智能的模型需要具备至少二阶适应性，通过数字补充实现自持计算，才能在动态扩展环境中达到最佳流动性表现。

Abstract: This paper introduces the Fluidity Index (FI) to quantify model adaptability
in dynamic, scaling environments. The benchmark evaluates response accuracy
based on deviations in initial, current, and future environment states,
assessing context switching and continuity. We distinguish between closed-ended
and open-ended benchmarks, prioritizing closed-loop open-ended real-world
benchmarks to test adaptability. The approach measures a model's ability to
understand, predict, and adjust to state changes in scaling environments. A
truly super-intelligent model should exhibit at least second-order
adaptability, enabling self-sustained computation through digital replenishment
for optimal fluidity.

</details>


### [80] [Integrating Machine Learning into Belief-Desire-Intention Agents: Current Advances and Open Challenges](https://arxiv.org/abs/2510.20641)
*Andrea Agiollo,Andrea Omicini*

Main category: cs.AI

TL;DR: 本文对基于BDI范式的理性智能体与机器学习集成方法进行了系统化梳理，分析了现有研究的碎片化问题，并指出了关键研究机会和挑战。


<details>
  <summary>Details</summary>
Motivation: 由于机器学习在感知和认知任务中展现出类人能力，将ML集成到理性智能体架构中的框架日益受到关注，但现有研究缺乏系统性和连贯性，往往忽视了理性架构（如BDI智能体）的表达能力。

Method: 使用BDI范式作为参考框架，对现有方法进行细粒度系统化分析，梳理理性智能体与机器学习增强的快速演进文献。

Result: 分析揭示了理性ML智能体研究领域的快速发展现状，识别出现有方法的局限性。

Conclusion: 提出了设计有效理性ML智能体的关键研究机会和开放挑战，强调了系统化整合的重要性。

Abstract: Thanks to the remarkable human-like capabilities of machine learning (ML)
models in perceptual and cognitive tasks, frameworks integrating ML within
rational agent architectures are gaining traction. Yet, the landscape remains
fragmented and incoherent, often focusing on embedding ML into generic agent
containers while overlooking the expressive power of rational
architectures--such as Belief-Desire-Intention (BDI) agents. This paper
presents a fine-grained systematisation of existing approaches, using the BDI
paradigm as a reference. Our analysis illustrates the fast-evolving literature
on rational agents enhanced by ML, and identifies key research opportunities
and open challenges for designing effective rational ML agents.

</details>


### [81] [The Shape of Reasoning: Topological Analysis of Reasoning Traces in Large Language Models](https://arxiv.org/abs/2510.20665)
*Xue Wen Tan,Nathaniel Tan,Galen Lee,Stanley Kok*

Main category: cs.AI

TL;DR: 提出基于拓扑数据分析(TDA)的框架来评估大语言模型推理轨迹质量，相比传统图结构指标能更有效地捕捉推理的几何特征


<details>
  <summary>Details</summary>
Motivation: 当前评估大语言模型推理轨迹质量的方法依赖专家标注、人工判断，自动化方法主要使用图结构代理指标，但这些抽象方法过于简化复杂推理过程

Method: 引入拓扑数据分析框架，通过捕捉推理轨迹的几何结构来实现标签高效、自动化的质量评估

Result: 拓扑特征在预测推理质量方面比标准图指标具有显著更高的预测能力，表明有效推理更适合用高维几何结构而非纯关系图来捕捉

Conclusion: 紧凑且稳定的拓扑特征集能可靠指示轨迹质量，为未来强化学习算法提供实用信号

Abstract: Evaluating the quality of reasoning traces from large language models remains
understudied, labor-intensive, and unreliable: current practice relies on
expert rubrics, manual annotation, and slow pairwise judgments. Automated
efforts are dominated by graph-based proxies that quantify structural
connectivity but do not clarify what constitutes high-quality reasoning; such
abstractions can be overly simplistic for inherently complex processes. We
introduce a topological data analysis (TDA)-based evaluation framework that
captures the geometry of reasoning traces and enables label-efficient,
automated assessment. In our empirical study, topological features yield
substantially higher predictive power for assessing reasoning quality than
standard graph metrics, suggesting that effective reasoning is better captured
by higher-dimensional geometric structures rather than purely relational
graphs. We further show that a compact, stable set of topological features
reliably indicates trace quality, offering a practical signal for future
reinforcement learning algorithms.

</details>


### [82] [Plan Then Retrieve: Reinforcement Learning-Guided Complex Reasoning over Knowledge Graphs](https://arxiv.org/abs/2510.20691)
*Yanlin Song,Ben Liu,Víctor Gutiérrez-Basulto,Zhiwei Hu,Qianqian Xie,Min Peng,Sophia Ananiadou,Jeff Z. Pan*

Main category: cs.AI

TL;DR: Graph-RFT是一个两阶段强化微调KGQA框架，通过'计划-KG搜索-网络搜索-思考'范式，使LLM能够在知识不完整条件下自主规划并自适应调度KG和网络检索。


<details>
  <summary>Details</summary>
Motivation: 现有KGQA方法难以充分利用KG的丰富知识和LLM的推理能力，假设KG完全覆盖且缺乏判断何时需要外部信息的机制，推理过程局部短视，无法维持连贯的多步规划。

Method: Graph-RFT采用两阶段方法：首先通过链式思维微调激活结构化推理，然后通过计划检索引导的强化学习过程整合显式规划和检索动作，使用笛卡尔式规划模块分解复杂问题，逻辑表达式指导工具调用。

Result: 该方法解决了GRPO冷启动问题，实现了覆盖感知的检索调度，使模型能够学习何时以及如何有效结合KG和网络检索。

Conclusion: Graph-RFT框架能够在知识不完整条件下实现自主规划和自适应检索调度，显著提升了复杂场景下的KGQA性能。

Abstract: Knowledge Graph Question Answering aims to answer natural language questions
by reasoning over structured knowledge graphs. While large language models have
advanced KGQA through their strong reasoning capabilities, existing methods
continue to struggle to fully exploit both the rich knowledge encoded in KGs
and the reasoning capabilities of LLMs, particularly in complex scenarios. They
often assume complete KG coverage and lack mechanisms to judge when external
information is needed, and their reasoning remains locally myopic, failing to
maintain coherent multi-step planning, leading to reasoning failures even when
relevant knowledge exists. We propose Graph-RFT, a novel two-stage
reinforcement fine-tuning KGQA framework with a
'plan-KGsearch-and-Websearch-during-think' paradigm, that enables LLMs to
perform autonomous planning and adaptive retrieval scheduling across KG and web
sources under incomplete knowledge conditions. Graph-RFT introduces a
chain-of-thought fine-tuning method with a customized plan-retrieval dataset
activates structured reasoning and resolves the GRPO cold-start problem. It
then introduces a novel plan-retrieval guided reinforcement learning process
integrates explicit planning and retrieval actions with a multi-reward design,
enabling coverage-aware retrieval scheduling. It employs a Cartesian-inspired
planning module to decompose complex questions into ordered subquestions, and
logical expression to guide tool invocation for globally consistent multi-step
reasoning. This reasoning retrieval process is optimized with a multi-reward
combining outcome and retrieval specific signals, enabling the model to learn
when and how to combine KG and web retrieval effectively.

</details>


### [83] [A Coherence-Based Measure of AGI](https://arxiv.org/abs/2510.20784)
*Fares Fourati*

Main category: cs.AI

TL;DR: 本文提出了一种基于广义均值积分的一致性感知AGI度量方法，替代了传统的算术平均方法，能够更好地衡量跨认知领域的平衡能力。


<details>
  <summary>Details</summary>
Motivation: 现有AGI定义使用算术平均，假设领域间的可补偿性，但真正的通用智能应该反映所有基本领域的平衡能力。

Method: 提出基于广义均值在可补偿性指数连续体上积分的AGI度量方法，覆盖算术、几何和调和均值等不同补偿性假设。

Result: 应用于GPT-4和GPT-5的CHC领域得分显示，尽管算术得分较高（GPT-5达24%），但一致性调整的AUC表明它们距离通用能力仍有很大差距。

Conclusion: 广义均值积分提供了原则性、可解释且更严格的AGI进展衡量基础，能够惩罚不平衡表现并捕捉领域间依赖关系。

Abstract: Recent work by \citet{hendrycks2025agidefinition} formalized
\textit{Artificial General Intelligence} (AGI) as the arithmetic mean of
proficiencies across cognitive domains derived from the Cattell--Horn--Carroll
(CHC) model of human cognition. While elegant, this definition assumes
\textit{compensability} -- that exceptional ability in some domains can offset
failure in others. True general intelligence, however, should reflect
\textit{coherent sufficiency}: balanced competence across all essential
domains. We propose a coherence-aware measure of AGI based on the integral of
generalized means over a continuum of compensability exponents. This
formulation spans arithmetic, geometric, and harmonic regimes, and the
resulting \textit{area under the curve} (AUC) quantifies robustness under
varying compensability assumptions. Unlike the arithmetic mean, which rewards
specialization, the AUC penalizes imbalance and captures inter-domain
dependency. Applied to published CHC-based domain scores for GPT-4 and GPT-5,
the coherence-adjusted AUC reveals that both systems remain far from general
competence despite high arithmetic scores (e.g., GPT-5 at~24\%). Integrating
the generalized mean thus yields a principled, interpretable, and stricter
foundation for measuring genuine progress toward AGI.

</details>


### [84] [Real Deep Research for AI, Robotics and Beyond](https://arxiv.org/abs/2510.20809)
*Xueyan Zou,Jianglong Ye,Hao Zhang,Xiaoyu Xiang,Mingyu Ding,Zhaojing Yang,Yong Jae Lee,Zhuowen Tu,Sifei Liu,Xiaolong Wang*

Main category: cs.AI

TL;DR: 提出了Real Deep Research (RDR)框架，用于系统分析AI和机器人领域的研究趋势，识别新兴趋势和跨领域机会，帮助研究人员应对每年上万篇论文的挑战。


<details>
  <summary>Details</summary>
Motivation: AI和机器人领域每年产生超过10,000篇论文，研究人员难以跟上快速发展的趋势，跨学科工作的增加以及需要探索专业领域之外的挑战。

Method: 构建了一个通用的RDR管道，能够系统分析任何研究领域，识别新兴趋势，发现跨领域机会，并为新研究提供具体起点。

Result: 将RDR框架应用于AI和机器人领域，特别关注基础模型和机器人技术进展，并简要扩展到其他科学领域。

Conclusion: 这项工作为AI及其他领域的研究人员提供了洞察，帮助他们在快速发展的研究环境中保持最新状态。

Abstract: With the rapid growth of research in AI and robotics now producing over
10,000 papers annually it has become increasingly difficult for researchers to
stay up to date. Fast evolving trends, the rise of interdisciplinary work, and
the need to explore domains beyond one's expertise all contribute to this
challenge. To address these issues, we propose a generalizable pipeline capable
of systematically analyzing any research area: identifying emerging trends,
uncovering cross domain opportunities, and offering concrete starting points
for new inquiry. In this work, we present Real Deep Research (RDR) a
comprehensive framework applied to the domains of AI and robotics, with a
particular focus on foundation models and robotics advancements. We also
briefly extend our analysis to other areas of science. The main paper details
the construction of the RDR pipeline, while the appendix provides extensive
results across each analyzed topic. We hope this work sheds light for
researchers working in the field of AI and beyond.

</details>
