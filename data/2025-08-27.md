<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 12]
- [cs.CR](#cs.CR) [Total: 16]
- [cs.AI](#cs.AI) [Total: 49]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Training Language Model Agents to Find Vulnerabilities with CTF-Dojo](https://arxiv.org/abs/2508.18370)
*Terry Yue Zhuo,Dingmin Wang,Hantian Ding,Varun Kumar,Zijian Wang*

Main category: cs.SE

TL;DR: CTF-Dojo是一个大规模可执行运行时环境，包含658个CTF挑战，用于训练LLM代理。通过自动化管道CTF-Forge快速生成环境，仅用486个高质量轨迹训练就实现了11.6%的性能提升，32B模型达到31.9% Pass@1的新SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有的可执行运行时环境稀缺，限制了ML代理的训练进展。需要可扩展、通用且能提供可验证反馈的执行环境来训练更强大的LLM代理。

Method: 开发CTF-Dojo大规模可执行运行时环境，包含658个容器化的CTF挑战；构建CTF-Forge自动化管道，将公开资源快速转换为可用环境；使用486个执行验证的高质量轨迹训练LLM代理。

Result: 在三个竞争性基准测试（InterCode-CTF、NYU CTF Bench、Cybench）上比强基线绝对提升11.6%；32B模型达到31.9% Pass@1，创开源模型新纪录，媲美DeepSeek-V3-0324和Gemini-2.5-Flash等前沿模型。

Conclusion: 执行基础训练信号对于推进高性能ML代理至关重要，CTF-Dojo证明了无需依赖昂贵专有系统也能实现突破性进展，为可执行代理学习设立了新的基准框架。

Abstract: Large language models (LLMs) have demonstrated exceptional capabilities when
trained within executable runtime environments, notably excelling at software
engineering tasks through verified feedback loops. Yet, scalable and
generalizable execution-grounded environments remain scarce, limiting progress
in training more capable ML agents. We introduce CTF-Dojo, the first
large-scale executable runtime tailored for training LLMs with verifiable
feedback, featuring 658 fully functional Capture-The-Flag (CTF)-style
challenges containerized in Docker with guaranteed reproducibility. To enable
rapid scaling without manual intervention, we develop CTF-Forge, an automated
pipeline that transforms publicly available artifacts into ready-to-use
execution environments in minutes, eliminating weeks of expert configuration
traditionally required. We trained LLM-based agents on just 486 high-quality,
execution-verified trajectories from CTF-Dojo, achieving up to 11.6% absolute
gains over strong baselines across three competitive benchmarks: InterCode-CTF,
NYU CTF Bench, and Cybench. Our best-performing 32B model reaches 31.9% Pass@1,
establishing a new open-weight state-of-the-art that rivals frontier models
like DeepSeek-V3-0324 and Gemini-2.5-Flash. By framing CTF-style tasks as a
benchmark for executable-agent learning, CTF-Dojo demonstrates that
execution-grounded training signals are not only effective but pivotal in
advancing high-performance ML agents without dependence on costly proprietary
systems.

</details>


### [2] [DTInsight: A Tool for Explicit, Interactive, and Continuous Digital Twin Reporting](https://arxiv.org/abs/2508.18431)
*Kérian Fiter,Louis Malassigné-Onfroy,Bentley Oakes*

Main category: cs.SE

TL;DR: DTInsight是一个自动化工具，用于为数字孪生系统生成持续报告，提供可视化架构、基于本体的特性摘要，并集成到CI/CD流水线中。


<details>
  <summary>Details</summary>
Motivation: 随着数字孪生系统的构建和演进，利益相关者需要工具来理解系统在任何时间点的当前特性和概念架构。

Method: 开发了DTInsight工具和方法论，基于数字孪生描述框架(DTDF)的建模描述，提供交互式概念架构可视化、基于本体数据的特性摘要生成，并将输出集成到CI/CD流水线的报告页面中。

Result: DTInsight能够生成最新且详细的报告，增强利益相关者对数字孪生系统的理解。

Conclusion: DTInsight为数字孪生系统提供了一种系统化和自动化的持续报告解决方案，有助于利益相关者更好地理解和监控数字孪生系统的状态和架构。

Abstract: With Digital Twin (DT) construction and evolution occurring over time,
stakeholders require tools to understand the current characteristics and
conceptual architecture of the system at any time. We introduce DTInsight, a
systematic and automated tool and methodology for producing continuous
reporting for DTs. DTInsight offers three key features: (a) an interactive
conceptual architecture visualization of DTs; (b) generation of summaries of DT
characteristics based on ontological data; and (c) integration of these outputs
into a reporting page within a continuous integration and continuous deployment
(CI/CD) pipeline. Given a modeled description of the DT aligning to our DT
Description Framework (DTDF), DTInsight enables up-to-date and detailed reports
for enhanced stakeholder understanding.

</details>


### [3] [Engineering a Digital Twin for the Monitoring and Control of Beer Fermentation Sampling](https://arxiv.org/abs/2508.18452)
*Pierre-Emmanuel Goffi,Raphaël Tremblay,Bentley Oakes*

Main category: cs.SE

TL;DR: 这篇论文分享了建立安全关键酒粉酿酒数字双胞的实践经验，通过三阶段工程方法实现了从装备监控到双向控制的转变，手动采样时间减少91%，并提供了多层安全协议和硬件软件集成策略。


<details>
  <summary>Details</summary>
Motivation: 工业数字双胞开发复杂性高，尤其是在需要实现超越装备监控的互动服务时。研究者希望为安全关键应用开发双向控制的数字双胞提供可操作的指导。

Method: 采用三阶段工程方法，将装备监控系统转化为具有实时控制能力的互动型Type 2数字双胞。包括多层安全协议、Arduino控制器与Unity可视化的硬件软件集成策略、实时同步解决方案，以及使用星座报告框架促进跨领域合作。

Result: 实现了在7己压力下运行的压力系统的实时控制，持续采样时间减少91%，成功建立了安全关键的酒粉酿酒数字双胞系统。

Conclusion: 安全优先设计、模拟驱动开发和渐进式实施策略是关键要素。该研究为开发需要双向控制的安全关键应用数字双胞提供了可操作的实践指南。

Abstract: Successfully engineering interactive industrial DTs is a complex task,
especially when implementing services beyond passive monitoring. We present
here an experience report on engineering a safety-critical digital twin (DT)
for beer fermentation monitoring, which provides continual sampling and reduces
manual sampling time by 91%. We document our systematic methodology and
practical solutions for implementing bidirectional DTs in industrial
environments. This includes our three-phase engineering approach that
transforms a passive monitoring system into an interactive Type 2 DT with
real-time control capabilities for pressurized systems operating at seven bar.
We contribute details of multi-layered safety protocols, hardware-software
integration strategies across Arduino controllers and Unity visualization, and
real-time synchronization solutions. We document specific engineering
challenges and solutions spanning interdisciplinary integration, demonstrating
how our use of the constellation reporting framework facilitates cross-domain
collaboration. Key findings include the critical importance of safety-first
design, simulation-driven development, and progressive implementation
strategies. Our work thus provides actionable guidance for practitioners
developing DTs requiring bidirectional control in safety-critical applications.

</details>


### [4] [How do Humans and LLMs Process Confusing Code?](https://arxiv.org/abs/2508.18547)
*Youssef Abdelsalam,Norman Peitek,Anna-Maria Maurer,Mariya Toneva,Sven Apel*

Main category: cs.SE

TL;DR: 这篇论文通过对比LLM困惑度与人类神经生理响应，发现人工智能与人类在代码理解方面有相似的困惑点，并提出了一种基于LLM的数据驱动方法来识别代码中导致人类困惑的区域。


<details>
  <summary>Details</summary>
Motivation: 研究人类与大语言模型编程助手在代码理解方面的对齐问题，以避免因理解差异导致的误解、效率低下、代码质量差和bug。关键问题是人类和LLM是否会困惑于相同类型的代码。

Method: 进行了一项实证研究，对比LLM和人类程序员理解清晰代码和混淆代码的能力。对于LLM，使用LLM困惑度来量化理解能力；对于人类程序员，使用神经生理学响应（特别是基于EEG的定视相关电位）来量化理解情况。

Result: 发现LLM困惑度的峰值在位置和幅度上都与表明人类困惑的神经生理学响应相关联。这表明LLM和人类在代码理解方面有相似的困惑点。

Conclusion: 基于这些发现，研究人员设计了一种数据驱动的、基于LLM的方法来识别代码中导致人类程序员困惑的区域，这对于改进LLM在软件工程工作流中的集成和LLM本身的改进都具有重要意义。

Abstract: Already today, humans and programming assistants based on large language
models (LLMs) collaborate in everyday programming tasks. Clearly, a
misalignment between how LLMs and programmers comprehend code can lead to
misunderstandings, inefficiencies, low code quality, and bugs.
  A key question in this space is whether humans and LLMs are confused by the
same kind of code. This would not only guide our choices of integrating LLMs in
software engineering workflows, but also inform about possible improvements of
LLMs.
  To this end, we conducted an empirical study comparing an LLM to human
programmers comprehending clean and confusing code. We operationalized
comprehension for the LLM by using LLM perplexity, and for human programmers
using neurophysiological responses (in particular, EEG-based fixation-related
potentials).
  We found that LLM perplexity spikes correlate both in terms of location and
amplitude with human neurophysiological responses that indicate confusion. This
result suggests that LLMs and humans are similarly confused about the code.
Based on these findings, we devised a data-driven, LLM-based approach to
identify regions of confusion in code that elicit confusion in human
programmers.

</details>


### [5] [LaQual: A Novel Framework for Automated Evaluation of LLM App Quality](https://arxiv.org/abs/2508.18636)
*Yan Wang,Xinyi Hou,Yanjie Zhao,Weiguo Lin,Haoyu Wang,Junjun Si*

Main category: cs.SE

TL;DR: LaQual是一个自动化评估LLM应用质量的框架，通过分层分类、静态指标筛选和动态场景自适应评估三阶段方法，能有效识别高质量应用，实验显示与人工评估高度一致且显著提升用户选择效率。


<details>
  <summary>Details</summary>
Motivation: 当前LLM应用商店主要依赖用户活跃度和收藏等静态指标进行排名推荐，难以帮助用户高效找到高质量应用，需要更有效的质量评估方法。

Method: 三阶段框架：1)分层分类LLM应用以匹配不同场景；2)使用时序加权用户参与度和功能能力指标等静态指标过滤低质量应用；3)进行动态场景自适应评估，由LLM生成场景特定的评估指标、评分规则和任务。

Result: 实验显示LaQual自动化评分与人工判断高度一致（法律咨询Spearman's rho=0.62，旅行规划rho=0.60），能减少66.7%-81.3%的候选应用，用户研究显示在决策信心、比较效率（5.45 vs 3.30）和评估报告价值感知（4.75 vs 2.25）方面显著优于基线系统。

Conclusion: LaQual提供了一个可扩展、客观且以用户为中心的解决方案，能够有效发现和推荐现实使用场景中的高质量LLM应用。

Abstract: LLM app stores are quickly emerging as platforms that gather a wide range of
intelligent applications based on LLMs, giving users many choices for content
creation, coding support, education, and more. However, the current methods for
ranking and recommending apps in these stores mostly rely on static metrics
like user activity and favorites, which makes it hard for users to efficiently
find high-quality apps. To address these challenges, we propose LaQual, an
automated framework for evaluating the quality of LLM apps. LaQual consists of
three main stages: first, it labels and classifies LLM apps in a hierarchical
way to accurately match them to different scenarios; second, it uses static
indicators, such as time-weighted user engagement and functional capability
metrics, to filter out low-quality apps; and third, it conducts a dynamic,
scenario-adaptive evaluation, where the LLM itself generates scenario-specific
evaluation metrics, scoring rules, and tasks for a thorough quality assessment.
Experiments on a popular LLM app store show that LaQual is effective. Its
automated scores are highly consistent with human judgments (with Spearman's
rho of 0.62 and p=0.006 in legal consulting, and rho of 0.60 and p=0.009 in
travel planning). By effectively screening, LaQual can reduce the pool of
candidate LLM apps by 66.7% to 81.3%. User studies further confirm that LaQual
significantly outperforms baseline systems in decision confidence, comparison
efficiency (with average scores of 5.45 compared to 3.30), and the perceived
value of its evaluation reports (4.75 versus 2.25). Overall, these results
demonstrate that LaQual offers a scalable, objective, and user-centered
solution for finding and recommending high-quality LLM apps in real-world use
cases.

</details>


### [6] [Requirements Development and Formalization for Reliable Code Generation: A Multi-Agent Vision](https://arxiv.org/abs/2508.18675)
*Xu Lu,Weisong Sun,Yiran Zhang,Ming Hu,Cong Tian,Zhi Jin,Yang Liu*

Main category: cs.SE

TL;DR: 提出了ReDeFo多智能体框架，通过需求开发和形式化方法实现可靠的代码生成，使用形式化规范连接自然语言需求和精确代码


<details>
  <summary>Details</summary>
Motivation: 现有仅依赖大语言模型的代码生成方法在质量上不足，缺乏系统化的需求开发和建模策略，无法保证满足实际需求

Method: 基于需求开发和形式化的多智能体框架，包含三个增强了形式化方法知识的智能体，使用形式化规范作为自然语言需求和可执行代码之间的桥梁

Result: 框架能够进行严格的正确性推理，发现隐藏错误，并在整个开发过程中强制执行关键属性

Conclusion: 该框架朝着实现可靠、自动生成软件的长期愿景迈出了有希望的一步

Abstract: Automated code generation has long been considered the holy grail of software
engineering. The emergence of Large Language Models (LLMs) has catalyzed a
revolutionary breakthrough in this area. However, existing methods that only
rely on LLMs remain inadequate in the quality of generated code, offering no
guarantees of satisfying practical requirements. They lack a systematic
strategy for requirements development and modeling. Recently, LLM-based agents
typically possess powerful abilities and play an essential role in facilitating
the alignment of LLM outputs with user requirements. In this paper, we envision
the first multi-agent framework for reliable code generation based on
\textsc{re}quirements \textsc{de}velopment and \textsc{fo}rmalization, named
\textsc{ReDeFo}. This framework incorporates three agents, highlighting their
augmentation with knowledge and techniques of formal methods, into the
requirements-to-code generation pipeline to strengthen quality assurance. The
core of \textsc{ReDeFo} is the use of formal specifications to bridge the gap
between potentially ambiguous natural language requirements and precise
executable code. \textsc{ReDeFo} enables rigorous reasoning about correctness,
uncovering hidden bugs, and enforcing critical properties throughout the
development process. In general, our framework aims to take a promising step
toward realizing the long-standing vision of reliable, auto-generated software.

</details>


### [7] [LLM as an Execution Estimator: Recovering Missing Dependency for Practical Time-travelling Debugging](https://arxiv.org/abs/2508.18721)
*Yunrui Pei,Hongshu Wang,Wenjie Zhang,Yun Lin,Weiyu Kong,Jin song Dong*

Main category: cs.SE

TL;DR: RecovSlicing是一种使用部分插桩和LLM推断程序行为的新方法，用于在单次运行中计算动态数据依赖关系，解决了传统方法的高成本和不可行性问题。


<details>
  <summary>Details</summary>
Motivation: 传统动态数据依赖分析方法需要全量插桩或程序重放，成本高昂且对非确定性程序不可行。特别是在库函数定义变量或非确定性程序中，现有方法存在严重局限性。

Method: 利用LLM从部分记录的执行轨迹和代码上下文中推断程序行为，通过恢复缺失的执行来估计运行时变量定义。支持隐式变量恢复，并处理运行时值和结构恢复以及变量与记录内存对齐的技术挑战。

Result: 在三个切片基准测试的8300个数据依赖上，RecovSlicing达到80.3%、91.1%和98.3%的准确率，显著优于最佳基线方法（39.0%、82.0%、59.9%）。召回率也领先（91.1%、91.1%、98.3% vs 53.4%、79.1%、87.1%）。

Conclusion: RecovSlicing通过部分插桩和LLM推断有效解决了动态数据依赖分析的效率和可行性问题，在回归错误定位中能多发现16%的回归问题，具有重要实用价值。

Abstract: Dynamic data dependency, answering "why a variable has this value?", is
critical for debugging. Given a program step `s` reading a variable `v`,
finding the dynamic definition of `v` is challenging. Traditional methods
require either (1) exhaustive instrumentation of all possible definitions of
`v` in one run or (2) replicating the run to re-examine reads/writes - both
costly. If `v` is defined in a library, instrumentation becomes expensive; for
non-deterministic programs, replication is infeasible.
  We propose RecovSlicing, which computes dynamic data dependency in a single
run with partial instrumentation. We leverage LLMs to infer program behavior
from a partially recorded trace and code context. Given a trace and a slicing
criterion (step `s` and variable `v`), RecovSlicing estimates the runtime
definition of `v` by recovering the missing execution.It also supports implicit
variables, such as those in `list.get(i)`. Technically, RecovSlicing tackles:
(1) recovering runtime values and structures, and (2) aligning recovered
variables with recorded memory to analyze definitions.
  We evaluate RecovSlicing on 8,300 data dependencies across three slicing
benchmarks, comparing it with Slicer4J, ND-Slicer, LLM Slicer, and re-execution
Slicer. RecovSlicing achieves accuracy of 80.3%, 91.1%, and 98.3%,
outperforming the best baseline (39.0%, 82.0%, 59.9%), and also leads in recall
(91.1%, 91.1%, 98.3% vs. 53.4%, 79.1%, 87.1%). Integrated into a regression bug
localizer, it enables finding 16% more regressions.

</details>


### [8] [Does AI Code Review Lead to Code Changes? A Case Study of GitHub Actions](https://arxiv.org/abs/2508.18771)
*Kexin Sun,Hongyu Kuang,Sebastian Baltes,Xin Zhou,He Zhang,Xiaoxing Ma,Guoping Rong,Dong Shao,Christoph Treude*

Main category: cs.SE

TL;DR: 研究对16款AI代码审查工具在GitHub上的实际效果进行大规模实证分析，发现简洁、包含代码片段且手动触发的评论更容易被采纳


<details>
  <summary>Details</summary>
Motivation: 虽然AI基础代码审查工具逐渐普及，但它们的实际影响和效果仍然不明确，需要系统性的实证研究

Method: 分析22,000个审查评论和178个仓库，使用两阶段LLM辅助框架判断评论是否被处理，采用可解释机器学习分析影响因素

Result: 采用率在增长但效果差异显著；简洁、包含代码片段、手动触发的评论（尤其是hunk-level工具）更容易导致代码更改

Conclusion: 细心的工具设计对AI代码审查系统的效果至关重要，研究结果为改进这些系统提供了方向

Abstract: AI-based code review tools automatically review and comment on pull requests
to improve code quality. Despite their growing presence, little is known about
their actual impact. We present a large-scale empirical study of 16 popular
AI-based code review actions for GitHub workflows, analyzing more than 22,000
review comments in 178 repositories. We investigate (1) how these tools are
adopted and configured, (2) whether their comments lead to code changes, and
(3) which factors influence their effectiveness. We develop a two-stage
LLM-assisted framework to determine whether review comments are addressed, and
use interpretable machine learning to identify influencing factors. Our
findings show that, while adoption is growing, effectiveness varies widely.
Comments that are concise, contain code snippets, and are manually triggered,
particularly those from hunk-level review tools, are more likely to result in
code changes. These results highlight the importance of careful tool design and
suggest directions for improving AI-based code review systems.

</details>


### [9] [Dealing with SonarQube Cloud: Initial Results from a Mining Software Repository Study](https://arxiv.org/abs/2508.18816)
*Sabato Nocera,Davide Fucci,Giuseppe Scanniello*

Main category: cs.SE

TL;DR: 研究通过挖掘GitHub项目发现，许多开源项目使用SonarQube Cloud进行静态代码分析，其中81%正确连接，75%使甩组织默认质量门，45%项目会自定义质量门条件以满足特定质量目标。


<details>
  <summary>Details</summary>
Motivation: 调查开源项目如何使甩和自定义SonarQube Cloud这种静态代码分析工具，以了解实际使甩情况和配置习惯。

Method: 对通过GitHub Actions链接到SonarQube Cloud项目的GitHub项目进行挖掘研究，分析321个使甩SonarQube Cloud的GitHub项目。

Result: 81%项目正确连接SonarQube Cloud，75%使用组织默认质量门，55%使用内置质量门，45%自定义质量门条件。最常见的质量条件与"Clean as You Code"原则一致，涉及安全、可维护性、可靠性、覆盖率和代码重复等方面。

Conclusion: 许多项目依赖预定义配置，但仍有较大比例的项目会自定义配置。未来研究可以将质量门配置与实际软件效果相关联，为不同场景下配置SCA工具提供基于证据的建议。

Abstract: Background: Static Code Analysis (SCA) tools are widely adopted to enforce
code quality standards. However, little is known about how open-source projects
use and customize these tools. Aims: This paper investigates how GitHub
projects use and customize a popular SCA tool, namely SonarQube Cloud. Method:
We conducted a mining study of GitHub projects that are linked through GitHub
Actions to SonarQube Cloud projects. Results: Among 321 GitHub projects using
SonarQube Cloud, 81% of them are correctly connected to SonarQube Cloud
projects, while others exhibit misconfigurations or restricted access. Among
265 accessible SonarQube Cloud projects, 75% use the organization's default
quality gate, i.e., a set of conditions that deployed source code must meet to
pass automated checks. While 55% of the projects use the built-in quality gate
provided by SonarQube Cloud, 45% of them customize their quality gate with
different conditions. Overall, the most common quality conditions align with
SonarQube Cloud's "Clean as You Code" principle and enforce security,
maintainability, reliability, coverage, and a few duplicates on newly added or
modified source code. Conclusions: Many projects rely on predefined
configurations, yet a significant portion customize their configurations to
meet specific quality goals. Building on our initial results, we envision a
future research agenda linking quality gate configurations to actual software
outcomes (e.g., improvement of software security). This would enable
evidence-based recommendations for configuring SCA tools like SonarQube Cloud
in various contexts.

</details>


### [10] [Interleaving Large Language Models for Compiler Testing](https://arxiv.org/abs/2508.18955)
*Yunbo Ni,Shaohua Li*

Main category: cs.SE

TL;DR: 提出LegoFuzz框架，通过离线生成代码片段和在线组合测试的方式，高效测试C编译器，发现66个GCC和LLVM漏洞


<details>
  <summary>Details</summary>
Motivation: 当前基于AI的编译器测试方法存在两个问题：生成的测试程序过于简单，以及使用大语言模型进行广泛测试计算成本过高

Method: 将测试过程解耦为两个阶段：离线阶段使用LLM生成小型但功能丰富的代码片段；在线阶段策略性地组合这些代码片段构建高质量测试程序

Result: 在GCC和LLVM中发现66个漏洞，其中近一半是现有LLM工具无法发现的严重误编译漏洞

Conclusion: 这种高效设计为在软件测试中超越C编译器使用AI模型开辟了新可能性

Abstract: Testing compilers with AI models, especially large language models (LLMs),
has shown great promise. However, current approaches struggle with two key
problems: The generated programs for testing compilers are often too simple,
and extensive testing with the LLMs is computationally expensive. In this
paper, we propose a novel compiler testing framework that decouples the testing
process into two distinct phases: an offline phase and an online phase. In the
offline phase, we use LLMs to generate a collection of small but feature-rich
code pieces. In the online phase, we reuse these code pieces by strategically
combining them to build high-quality and valid test programs, which are then
used to test compilers.
  We implement this idea in a tool, LegoFuzz, for testing C compilers. The
results are striking: we found 66 bugs in GCC and LLVM, the most widely used C
compilers. Almost half of the bugs are miscompilation bugs, which are serious
and hard-to-find bugs that none of the existing LLM-based tools could find. We
believe this efficient design opens up new possibilities for using AI models in
software testing beyond just C compilers.

</details>


### [11] [GitTaskBench: A Benchmark for Code Agents Solving Real-World Tasks Through Code Repository Leveraging](https://arxiv.org/abs/2508.18993)
*Ziyi Ni,Huacan Wang,Shuo Zhang,Shuo Lu,Ziyang He,Wang You,Zhenheng Tang,Yuntao Du,Bill Sun,Hongzhang Liu,Sen Hu,Ronghao Chen,Bo Li,Xin Li,Chen Hu,Binxing Jiao,Daxin Jiang,Pin Lyu*

Main category: cs.SE

TL;DR: GitTaskBench是一个评估代码代理在真实GitHub仓库工作流中执行复杂任务能力的基准测试，包含54个跨7个领域和模态的任务，并提出了衡量经济收益的alpha-value指标。


<details>
  <summary>Details</summary>
Motivation: 当前基准测试很少评估代码代理在真实工作流驱动的GitHub仓库环境中的表现，需要填补这一空白来推动代码代理在实际软件开发中的应用。

Method: 构建包含54个真实任务的基准测试，每个任务配有关联的代码仓库和自动化评估工具，使用alpha-value指标综合评估任务成功率、token成本和开发者薪资。

Result: 实验显示即使最佳系统(OpenHands+Claude 3.7)也只能解决48.15%的任务，超过一半的失败源于环境设置和依赖解析等基础步骤。

Conclusion: 代码代理在处理复杂仓库任务时仍面临挑战，需要更强大的工作流管理和超时准备，GitTaskBench的发布旨在推动仓库感知的代码推理和执行能力发展。

Abstract: Beyond scratch coding, exploiting large-scale code repositories (e.g.,
GitHub) for practical tasks is vital in real-world software development, yet
current benchmarks rarely evaluate code agents in such authentic,
workflow-driven scenarios. To bridge this gap, we introduce GitTaskBench, a
benchmark designed to systematically assess this capability via 54 realistic
tasks across 7 modalities and 7 domains. Each task pairs a relevant repository
with an automated, human-curated evaluation harness specifying practical
success criteria. Beyond measuring execution and task success, we also propose
the alpha-value metric to quantify the economic benefit of agent performance,
which integrates task success rates, token cost, and average developer
salaries. Experiments across three state-of-the-art agent frameworks with
multiple advanced LLMs show that leveraging code repositories for complex task
solving remains challenging: even the best-performing system, OpenHands+Claude
3.7, solves only 48.15% of tasks. Error analysis attributes over half of
failures to seemingly mundane yet critical steps like environment setup and
dependency resolution, highlighting the need for more robust workflow
management and increased timeout preparedness. By releasing GitTaskBench, we
aim to drive progress and attention toward repository-aware code reasoning,
execution, and deployment -- moving agents closer to solving complex,
end-to-end real-world tasks. The benchmark and code are open-sourced at
https://github.com/QuantaAlpha/GitTaskBench.

</details>


### [12] [A Slice-Based Change Impact Analysis for Regression Test Case Prioritization of Object-Oriented Programs](https://arxiv.org/abs/2508.19056)
*S. Panda,D. Munjal,D. P. Mohapatra*

Main category: cs.SE

TL;DR: 这篇论文提出了一种基于受影组件耦合度(ACC)的静态测试案例优先级排序方法，通过构建受影切片图(ASG)来识别故障容易的程序部分，并为覆盖高ACC值节点的测试案例赋予更高优先级。


<details>
  <summary>Details</summary>
Motivation: 测试案例优先级排序可以通过早期检测故障来节省重测时间和成本。但是需要找到一种有效的方法来识别哪些测试案例更可能执行容易出错的程序部分。

Method: 构建受影切片图(ASG)来表示受影的程序部分，计算每个节点的受影组件耦合度(ACC)值来评估故障容易程度，然后为覆盖高ACC值节点的测试案例赋予更高优先级。

Result: 通过筛选变异故障的分析显示，执行故障容易程序部分的测试案例更有可能早期发现故障。七个案例研究的结果证明该方法可行且效果超过一些现有技术。

Conclusion: 该研究提出的基于ACC的静态测试案例优先级方法能够有效识别故障容易的程序部分，并通过优先执行相关测试案例来提高故障检测效率，从而节省重测时间和成本。

Abstract: Test case prioritization focuses on finding a suitable order of execution of
the test cases in a test suite to meet some performance goals like detecting
faults early. It is likely that some test cases execute the program parts that
are more prone to errors and will detect more errors if executed early during
the testing process. Finding an optimal order of execution for the selected
regression test cases saves time and cost of retesting. This paper presents a
static approach to prioritizing the test cases by computing the affected
component coupling (ACC) of the affected parts of object-oriented programs. We
construct a graph named affected slice graph (ASG) to represent these affected
program parts.We determine the fault-proneness of the nodes of ASG by computing
their respective ACC values. We assign higher priority to those test cases that
cover the nodes with higher ACC values. Our analysis with mutation faults shows
that the test cases executing the fault-prone program parts have a higher
chance to reveal faults earlier than other test cases in the test suite. The
result obtained from seven case studies justifies that our approach is feasible
and gives acceptable performance in comparison to some existing techniques.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [13] [A Systematic Approach to Predict the Impact of Cybersecurity Vulnerabilities Using LLMs](https://arxiv.org/abs/2508.18439)
*Anders Mølmen Høst,Pierre Lison,Leon Moonen*

Main category: cs.CR

TL;DR: TRIAGE是一个自动化系统，使用大型语言模型将CVE漏洞映射到ATT&CK技术框架，通过混合方法结合基于规则的推理和数据驱动的推断，提高了漏洞影响分析的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有漏洞数据库如NVD缺乏漏洞实际影响信息，手动将CVE映射到ATT&CK技术框架耗时且困难，需要自动化解决方案来处理大量新漏洞。

Method: 采用双管齐下的方法：首先基于MITRE映射方法提示LLM预测技术列表，然后结合第二个使用上下文学习的LLM模块结果，形成混合方法。

Result: 上下文学习方法优于单独映射方法，混合方法提高了利用技术的召回率，GPT-4o-mini在此任务上表现优于Llama3.3-70B。

Conclusion: LLM可用于自动预测网络安全漏洞的影响，TRIAGE使CVE到ATT&CK的映射过程更加高效。

Abstract: Vulnerability databases, such as the National Vulnerability Database (NVD),
offer detailed descriptions of Common Vulnerabilities and Exposures (CVEs), but
often lack information on their real-world impact, such as the tactics,
techniques, and procedures (TTPs) that adversaries may use to exploit the
vulnerability. However, manually linking CVEs to their corresponding TTPs is a
challenging and time-consuming task, and the high volume of new vulnerabilities
published annually makes automated support desirable.
  This paper introduces TRIAGE, a two-pronged automated approach that uses
Large Language Models (LLMs) to map CVEs to relevant techniques from the ATT&CK
knowledge base. We first prompt an LLM with instructions based on MITRE's CVE
Mapping Methodology to predict an initial list of techniques. This list is then
combined with the results from a second LLM-based module that uses in-context
learning to map a CVE to relevant techniques. This hybrid approach
strategically combines rule-based reasoning with data-driven inference. Our
evaluation reveals that in-context learning outperforms the individual mapping
methods, and the hybrid approach improves recall of exploitation techniques. We
also find that GPT-4o-mini performs better than Llama3.3-70B on this task.
Overall, our results show that LLMs can be used to automatically predict the
impact of cybersecurity vulnerabilities and TRIAGE makes the process of mapping
CVEs to ATT&CK more efficient.
  Keywords: vulnerability impact, CVE, ATT&CK techniques, large language
models, automated mapping.

</details>


### [14] [Privacy-Preserving Federated Learning Framework for Risk-Based Adaptive Authentication](https://arxiv.org/abs/2508.18453)
*Yaser Baseri,Abdelhakim Senhaji Hafid,Dimitrios Makrakis,Hamidreza Fereidouni*

Main category: cs.CR

TL;DR: FL-RBA2是一个联邦学习框架，通过数学相似性转换解决非IID数据问题，实现去中心化风险认证，平衡安全性和隐私保护


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习方法在处理非独立同分布用户特征时存在偏差和不稳定性，需要一种既能保护隐私又能有效进行风险认证的解决方案

Method: 使用数学相似性转换将异构用户特征转换为IID相似向量，结合差分隐私保护敏感信息，采用消息认证码确保模型完整性，通过聚类进行风险标注

Result: 在击键、鼠标和上下文数据集上的实验验证了FL-RBA2在高风险用户检测方面的有效性，以及对模型反转和推理攻击的抵抗能力

Conclusion: FL-RBA2成功解决了非IID数据挑战，在强差分隐私约束下实现了隐私保护、可扩展性和自适应认证鲁棒性的平衡

Abstract: Balancing robust security with strong privacy guarantees is critical for
Risk-Based Adaptive Authentication (RBA), particularly in decentralized
settings. Federated Learning (FL) offers a promising solution by enabling
collaborative risk assessment without centralizing user data. However, existing
FL approaches struggle with Non-Independent and Identically Distributed
(Non-IID) user features, resulting in biased, unstable, and poorly generalized
global models. This paper introduces FL-RBA2, a novel Federated Learning
framework for Risk-Based Adaptive Authentication that addresses Non-IID
challenges through a mathematically grounded similarity transformation. By
converting heterogeneous user features (including behavioral, biometric,
contextual, interaction-based, and knowledge-based modalities) into IID
similarity vectors, FL-RBA2 supports unbiased aggregation and personalized risk
modeling across distributed clients. The framework mitigates cold-start
limitations via clustering-based risk labeling, incorporates Differential
Privacy (DP) to safeguard sensitive information, and employs Message
Authentication Codes (MACs) to ensure model integrity and authenticity.
Federated updates are securely aggregated into a global model, achieving strong
balance between user privacy, scalability, and adaptive authentication
robustness. Rigorous game-based security proofs in the Random Oracle Model
formally establish privacy, correctness, and adaptive security guarantees.
Extensive experiments on keystroke, mouse, and contextual datasets validate
FL-RBA2's effectiveness in high-risk user detection and its resilience to model
inversion and inference attacks, even under strong DP constraints.

</details>


### [15] [An 8- and 12-bit block AES cipher](https://arxiv.org/abs/2508.18485)
*Peter T. Breuer*

Main category: cs.CR

TL;DR: 文章提供了一种极小的8位或12位块AES加密算法的实现，附带Java源代码


<details>
  <summary>Details</summary>
Motivation: 因为8位或12位块的AES加密算法非常稀有且难以找到，需要文档化这种小规模的实现

Method: 文档化了一种极小的8位或12位块AES (Rijndael)加密算法，并提供Java源代码实现

Result: 成功实现了极小块大小的AES加密算法，为小规模应用提供了一种实用的加密解决方案

Conclusion: 这种极小块AES加密算法的文档化和实现填补了该领域的空白，为需要小规模加密的应用提供了价值

Abstract: Because it is so unusual, or hard to find, or expository, a truly tiny 8- or
12-bit block AES (Rijndael) cipher is documented here, along with Java source
code.

</details>


### [16] [Collaborative Intelligence: Topic Modelling of Large Language Model use in Live Cybersecurity Operations](https://arxiv.org/abs/2508.18488)
*Martin Lochner,Keegan Keplinger*

Main category: cs.CR

TL;DR: SOC人员主要使用LLM来理解复杂文本字符串，约40%的使用场景集中在此，表明LLM可以支持SOC工作流程


<details>
  <summary>Details</summary>
Motivation: 研究SOC专家如何自愿使用大型语言模型来支持实时安全运营，以理解人机协作的新模式

Method: 基于10个月的SOC操作员使用GPT-4的聊天数据，采用BERTopic模型和新颖的主题建模工作流进行分析

Result: SOC操作员主要使用LLM来促进对复杂文本字符串的理解，这种用例约占LLM使用的40%

Conclusion: SOC操作员需要快速解释复杂命令，他们自然倾向于利用LLM支持这一活动，表明可以通过设计协作式LLM工具来增强SOC工作流程

Abstract: Objective: This work describes the topic modelling of Security Operations
Centre (SOC) use of a large language model (LLM), during live security
operations. The goal is to better understand how these specialists voluntarily
use this tool.
  Background: Human-automation teams have been extensively studied, but
transformer-based language models have sparked a new wave of collaboration. SOC
personnel at a major cybersecurity provider used an LLM to support live
security operations. This study examines how these specialists incorporated the
LLM into their work.
  Method: Our data set is the result of 10 months of SOC operators accessing
GPT-4 over an internally deployed HTTP-based chat application. We performed two
topic modelling exercises, first using the established BERTopic model
(Grootendorst, 2022), and second, using a novel topic modeling workflow.
  Results: Both the BERTopic analysis and novel modelling approach revealed
that SOC operators primarily used the LLM to facilitate their understanding of
complex text strings. Variations on this use-case accounted for ~40% of SOC LLM
usage.
  Conclusion: SOC operators are required to rapidly interpret complex commands
and similar information. Their natural tendency to leverage LLMs to support
this activity indicates that their workflow can be supported and augmented by
designing collaborative LLM tools for use in the SOC.
  Application: This work can aid in creating next-generation tools for Security
Operations Centres. By understanding common use-cases, we can develop workflows
supporting SOC task flow. One example is a right-click context menu for
executing a command line analysis LLM call directly in the SOC environment.

</details>


### [17] [PRISM: Robust VLM Alignment with Principled Reasoning for Integrated Safety in Multimodality](https://arxiv.org/abs/2508.18649)
*Nanxi Li,Zhengyue Zhao,Chaowei Xiao*

Main category: cs.CR

TL;DR: PRISM是一个保护视觉语言模型的安全框架，通过安全感知的思维链推理和直接偏好优化来平衡安全性和实用性，在多个基准测试中显著降低攻击成功率同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型保护方法存在过度防御损害实用性，或仅进行浅层对齐无法检测需要深度推理的复杂威胁的问题。

Method: 提出PRISM框架，包含PRISM-CoT数据集教授安全感知思维链推理，以及通过蒙特卡洛树搜索生成的PRISM-DPO进行直接偏好优化来细化安全边界。

Result: 在多个测试中表现优异：Qwen2-VL在JailbreakV-28K上攻击成功率仅0.15%，LLaVA-1.5在VLBreak上比之前最佳方法提升90%，在MIS基准上将攻击成功率降至8.70%，同时保持甚至提升模型实用性。

Conclusion: PRISM框架有效解决了视觉语言模型的安全保护问题，在提供强大防御能力的同时保持了模型实用性，具有很好的鲁棒性和泛化能力。

Abstract: Safeguarding vision-language models (VLMs) is a critical challenge, as
existing methods often suffer from over-defense, which harms utility, or rely
on shallow alignment, failing to detect complex threats that require deep
reasoning. To this end, we introduce PRISM (Principled Reasoning for Integrated
Safety in Multimodality), a system2-like framework that aligns VLMs by
embedding a structured, safety-aware reasoning process. Our framework consists
of two key components: PRISM-CoT, a dataset that teaches safety-aware
chain-of-thought reasoning, and PRISM-DPO, generated via Monte Carlo Tree
Search (MCTS) to further refine this reasoning through Direct Preference
Optimization to help obtain a delicate safety boundary. Comprehensive
evaluations demonstrate PRISM's effectiveness, achieving remarkably low attack
success rates including 0.15% on JailbreakV-28K for Qwen2-VL and 90%
improvement over the previous best method on VLBreak for LLaVA-1.5. PRISM also
exhibits strong robustness against adaptive attacks, significantly increasing
computational costs for adversaries, and generalizes effectively to
out-of-distribution challenges, reducing attack success rates to just 8.70% on
the challenging multi-image MIS benchmark. Remarkably, this robust defense is
achieved while preserving, and in some cases enhancing, model utility. To
promote reproducibility, we have made our code, data, and model weights
available at https://github.com/SaFoLab-WISC/PRISM.

</details>


### [18] [UniC-RAG: Universal Knowledge Corruption Attacks to Retrieval-Augmented Generation](https://arxiv.org/abs/2508.18652)
*Runpeng Geng,Yanting Wang,Ying Chen,Jinyuan Jia*

Main category: cs.CR

TL;DR: UniC-RAG是一种针对检索增强生成(RAG)系统的通用知识污染攻击方法，通过联合优化少量对抗性文本来同时攻击大量不同主题的用户查询，攻击成功率超过90%。


<details>
  <summary>Details</summary>
Motivation: 现有的知识污染攻击主要针对特定查询或相似主题的查询，缺乏能够同时攻击大量多样化查询的通用攻击方法。

Method: 将攻击建模为优化问题，设计平衡相似性聚类方法，联合优化少量对抗性文本来同时攻击大量用户查询。

Result: 仅注入100个对抗性文本就能在包含数百万文本的知识库中同时攻击2000个用户查询，攻击成功率超过90%。现有防御措施无法有效防御该攻击。

Conclusion: UniC-RAG攻击效果显著，现有防御机制不足，凸显了RAG系统需要新的防御机制来应对此类通用攻击。

Abstract: Retrieval-augmented generation (RAG) systems are widely deployed in
real-world applications in diverse domains such as finance, healthcare, and
cybersecurity. However, many studies showed that they are vulnerable to
knowledge corruption attacks, where an attacker can inject adversarial texts
into the knowledge database of a RAG system to induce the LLM to generate
attacker-desired outputs. Existing studies mainly focus on attacking specific
queries or queries with similar topics (or keywords). In this work, we propose
UniC-RAG, a universal knowledge corruption attack against RAG systems. Unlike
prior work, UniC-RAG jointly optimizes a small number of adversarial texts that
can simultaneously attack a large number of user queries with diverse topics
and domains, enabling an attacker to achieve various malicious objectives, such
as directing users to malicious websites, triggering harmful command execution,
or launching denial-of-service attacks. We formulate UniC-RAG as an
optimization problem and further design an effective solution to solve it,
including a balanced similarity-based clustering method to enhance the attack's
effectiveness. Our extensive evaluations demonstrate that UniC-RAG is highly
effective and significantly outperforms baselines. For instance, UniC-RAG could
achieve over 90% attack success rate by injecting 100 adversarial texts into a
knowledge database with millions of texts to simultaneously attack a large set
of user queries (e.g., 2,000). Additionally, we evaluate existing defenses and
show that they are insufficient to defend against UniC-RAG, highlighting the
need for new defense mechanisms in RAG systems.

</details>


### [19] [FALCON: Autonomous Cyber Threat Intelligence Mining with LLMs for IDS Rule Generation](https://arxiv.org/abs/2508.18684)
*Shaswata Mitra,Azim Bazarov,Martin Duclos,Sudip Mittal,Aritran Piplai,Md Rayhanur Rahman,Edward Zieglar,Shahram Rahimi*

Main category: cs.CR

TL;DR: FALCON是一个基于大语言模型的自主代理框架，能够从网络威胁情报(CTI)数据实时生成可部署的入侵检测系统(IDS)规则，并通过内置验证器进行评估，实现了95%的平均准确率。


<details>
  <summary>Details</summary>
Motivation: 传统基于签名的IDS需要频繁更新规则来应对不断演变的网络威胁，但规则更新存在延迟，影响了安全防护的实时性和有效性。

Method: 开发了FALCON自主代理框架，利用大语言模型从CTI数据自动生成IDS规则，并采用内置的多阶段验证器进行评估，支持网络型(Snort)和主机型(YARA)两种检测媒介。

Result: 实验评估显示FALCON在自动规则生成方面表现优异，平均准确率达到95%，多位网络安全分析师的定性评估一致性达到84%。

Conclusion: 研究证明了基于大语言模型的数据挖掘在实时网络威胁缓解方面的可行性和有效性，为自主IDS规则生成提供了新的解决方案。

Abstract: Signature-based Intrusion Detection Systems (IDS) detect malicious activities
by matching network or host activity against predefined rules. These rules are
derived from extensive Cyber Threat Intelligence (CTI), which includes attack
signatures and behavioral patterns obtained through automated tools and manual
threat analysis, such as sandboxing. The CTI is then transformed into
actionable rules for the IDS engine, enabling real-time detection and
prevention. However, the constant evolution of cyber threats necessitates
frequent rule updates, which delay deployment time and weaken overall security
readiness. Recent advancements in agentic systems powered by Large Language
Models (LLMs) offer the potential for autonomous IDS rule generation with
internal evaluation. We introduce FALCON, an autonomous agentic framework that
generates deployable IDS rules from CTI data in real-time and evaluates them
using built-in multi-phased validators. To demonstrate versatility, we target
both network (Snort) and host-based (YARA) mediums and construct a
comprehensive dataset of IDS rules with their corresponding CTIs. Our
evaluations indicate FALCON excels in automatic rule generation, with an
average of 95% accuracy validated by qualitative evaluation with 84%
inter-rater agreement among multiple cybersecurity analysts across all metrics.
These results underscore the feasibility and effectiveness of LLM-driven data
mining for real-time cyber threat mitigation.

</details>


### [20] [Immutable Digital Recognition via Blockchain](https://arxiv.org/abs/2508.18750)
*Zeng Zhang,Xiaoqi Li*

Main category: cs.CR

TL;DR: 论文提出了一种结合去中心化管理与集中运营模式的区块链电子认证系统，符合国家政策导向，实现安全可靠的法律认证体系


<details>
  <summary>Details</summary>
Motivation: 整合去中心化管理和集中运营模式，使其符合国家政策指导，充分利用区块链技术优势并促进社区参与

Method: 开发了一种集成解决方案，将区块链技术的去中心化特性与集中运营模式相结合

Result: 建立了一个安全、合法、可靠且动态的电子认证系统

Conclusion: 该解决方案成功实现了区块链技术优势的充分利用，同时促进了社区参与，为电子认证提供了可靠的技术框架

Abstract: The process integrates the decentralised management and centralised operation
models, aligning them with the national policy directives. The developed
solution enables the full utilisation of blockchain technology's advantages
while also fostering community participation. Consequently, it establishes a
secure, legal, reliable, and dynamic electronic certification system.

</details>


### [21] [Hidden Tail: Adversarial Image Causing Stealthy Resource Consumption in Vision-Language Models](https://arxiv.org/abs/2508.18805)
*Rui Zhang,Zihan Wang,Tianli Yang,Hongwei Li,Wenbo Jiang,Qingchuan Zhao,Yang Liu,Guowen Xu*

Main category: cs.CR

TL;DR: Hidden Tail是一种针对视觉语言模型的隐蔽资源消耗攻击，通过生成对抗图像诱导模型输出最大长度的隐藏特殊标记，在保持攻击隐蔽性的同时大幅增加推理成本


<details>
  <summary>Details</summary>
Motivation: 现有VLM资源消耗攻击在延长输出序列时会产生不相关的异常内容，破坏了攻击的隐蔽性，需要在有效性和隐蔽性之间做出权衡

Method: 使用复合损失函数平衡语义保持、重复特殊标记诱导和抑制结束标记，通过动态权重策略优化生成对抗图像

Result: 实验显示Hidden Tail将输出长度提升达19.2倍，达到最大标记限制，同时保持攻击隐蔽性

Conclusion: 该方法揭示了VLM在效率导向对抗威胁下的脆弱性，迫切需要提高模型对此类攻击的鲁棒性

Abstract: Vision-Language Models (VLMs) are increasingly deployed in real-world
applications, but their high inference cost makes them vulnerable to resource
consumption attacks. Prior attacks attempt to extend VLM output sequences by
optimizing adversarial images, thereby increasing inference costs. However,
these extended outputs often introduce irrelevant abnormal content,
compromising attack stealthiness. This trade-off between effectiveness and
stealthiness poses a major limitation for existing attacks. To address this
challenge, we propose \textit{Hidden Tail}, a stealthy resource consumption
attack that crafts prompt-agnostic adversarial images, inducing VLMs to
generate maximum-length outputs by appending special tokens invisible to users.
Our method employs a composite loss function that balances semantic
preservation, repetitive special token induction, and suppression of the
end-of-sequence (EOS) token, optimized via a dynamic weighting strategy.
Extensive experiments show that \textit{Hidden Tail} outperforms existing
attacks, increasing output length by up to 19.2$\times$ and reaching the
maximum token limit, while preserving attack stealthiness. These results
highlight the urgent need to improve the robustness of VLMs against
efficiency-oriented adversarial threats. Our code is available at
https://github.com/zhangrui4041/Hidden_Tail.

</details>


### [22] [A Tight Context-aware Privacy Bound for Histogram Publication](https://arxiv.org/abs/2508.18832)
*Sara Saeidian,Ata Yavuzyılmaz,Leonhard Grosse,Georg Schuppe,Tobias J. Oechtering*

Main category: cs.CR

TL;DR: 本文通过点态最大泄漏(PML)分析拉普拉斯机制发布数据集直方图的隐私保证，发现在直方图各桶概率远离零时，固定噪声水平下可获得更强的隐私保护。


<details>
  <summary>Details</summary>
Motivation: 差分隐私是上下文无关的定义，不依赖于数据分布。PML能够通过纳入数据分布假设进行更精细的隐私分析，探索在数据分布已知情况下能否获得更好的隐私-效用权衡。

Method: 使用点态最大泄漏(PML)作为隐私度量框架，分析拉普拉斯机制在发布数据集直方图时的隐私保证，特别关注当直方图各桶概率有下界时的情况。

Result: 研究证明当直方图各桶概率远离零时，在相同的噪声水平下可以获得比传统差分隐私更强的隐私保护效果。

Conclusion: 上下文感知的隐私度量具有优势，纳入数据分布假设可以改善隐私-效用权衡，为实际隐私保护机制设计提供了新的思路。

Abstract: We analyze the privacy guarantees of the Laplace mechanism releasing the
histogram of a dataset through the lens of pointwise maximal leakage (PML).
While differential privacy is commonly used to quantify the privacy loss, it is
a context-free definition that does not depend on the data distribution. In
contrast, PML enables a more refined analysis by incorporating assumptions
about the data distribution. We show that when the probability of each
histogram bin is bounded away from zero, stronger privacy protection can be
achieved for a fixed level of noise. Our results demonstrate the advantage of
context-aware privacy measures and show that incorporating assumptions about
the data can improve privacy-utility tradeoffs.

</details>


### [23] [EnerSwap: Large-Scale, Privacy-First Automated Market Maker for V2G Energy Trading](https://arxiv.org/abs/2508.18942)
*Ahmed Mounsf Rafik Bendada,Yacine Ghamri-Doudane*

Main category: cs.CR

TL;DR: 提出基于区块链的去中心化电动汽车V2G电力交易市场，采用隐私保护AMM模型防止市场操纵和隐私泄露，并通过地理动态分片实现可扩展性


<details>
  <summary>Details</summary>
Motivation: 传统集中式V2G电力市场存在中介垄断、市场操纵风险和用户隐私泄露问题，需要去中心化解决方案

Method: 利用区块链技术构建去中心化交易市场，采用隐私保护自动化做市商(AMM)模型，结合地理动态分片架构实现可扩展性

Result: 建立了开放、公平的交易环境，有效缓解常见交易操纵攻击，保护用户隐私信息如位置数据和交易活动

Conclusion: 区块链和隐私保护AMM模型为V2G电力交易提供了安全、去中心化的解决方案，动态分片架构确保了系统的可扩展性

Abstract: With the rapid growth of Electric Vehicle (EV) technology, EVs are destined
to shape the future of transportation. The large number of EVs facilitates the
development of the emerging vehicle-to-grid (V2G) technology, which realizes
bidirectional energy exchanges between EVs and the power grid. This has led to
the setting up of electricity markets that are usually confined to a small
geographical location, often with a small number of participants. Usually,
these markets are manipulated by intermediaries responsible for collecting bids
from prosumers, determining the market-clearing price, incorporating grid
constraints, and accounting for network losses. While centralized models can be
highly efficient, they grant excessive power to the intermediary by allowing
them to gain exclusive access to prosumers \textquotesingle price preferences.
This opens the door to potential market manipulation and raises significant
privacy concerns for users, such as the location of energy providers. This lack
of protection exposes users to potential risks, as untrustworthy servers and
malicious adversaries can exploit this information to infer trading activities
and real identities. This work proposes a secure, decentralized exchange market
built on blockchain technology, utilizing a privacy-preserving Automated Market
Maker (AMM) model to offer open and fair, and equal access to traders, and
mitigates the most common trading-manipulation attacks. Additionally, it
incorporates a scalable architecture based on geographical dynamic sharding,
allowing for efficient resource allocation and improved performance as the
market grows.

</details>


### [24] [An Efficient Lightweight Blockchain for Decentralized IoT](https://arxiv.org/abs/2508.19219)
*Faezeh Dehghan Tarzjani,Mostafa Salehi*

Main category: cs.CR

TL;DR: 提出基于权重选择(WBS)的PoA共识算法，替代传统的轮转选择(TBS)方法，用于资源受限的IoT区块链网络，通过虚拟化和聚类技术提高可扩展性和性能。


<details>
  <summary>Details</summary>
Motivation: 物联网设备资源有限，传统区块链共识算法(PoW/PoS)计算成本高，现有PoA算法的轮转选择方法存在可靠性、能耗、延迟和可扩展性问题。

Method: 采用虚拟化和聚类技术构建轻量级区块链架构，提出基于权重选择(WBS)的PoA共识算法来选择验证器，替代传统的轮转选择(TBS)方法。

Result: 仿真实验表明，与TBS方法相比，WBS方法显著降低了能耗和响应时间，提高了吞吐量。

Conclusion: 提出的WBS-based PoA共识算法为资源受限的IoT网络提供了一种高效、可扩展的区块链解决方案，能够有效解决传统共识算法的性能瓶颈。

Abstract: The Internet of Things (IoT) is applied in various fields, and the number of
physical devices connected to the IoT is increasingly growing. There are
significant challenges to the IoT's growth and development, mainly due to the
centralized nature and large-scale IoT networks. The emphasis on the
decentralization of IoT's architecture can overcome challenges to IoT's
capabilities. A promising decentralized platform for IoT is blockchain. Owing
to IoT devices' limited resources, traditional consensus algorithms such as PoW
and PoS in the blockchain are computationally expensive. Therefore, the PoA
consensus algorithm is proposed in the blockchain consensus network for IoT.
The PoA selects the validator as Turn-based selection (TBS) that needs
optimization and faces system reliability, energy consumption, latency, and low
scalability. We propose an efficient, lightweight blockchain for decentralizing
IoT architecture by using virtualization and clustering to increase
productivity and scalability to address these issues. We also introduce a novel
PoA based on the Weight-Based-Selection (WBS) method for validators to validate
transactions and add them to the blockchain. By simulation, we evaluated the
performance of our proposed WBS method as opposed to TBS. The results show
reduced energy consumption, and response time, and increased throughput.

</details>


### [25] [LLMs in the SOC: An Empirical Study of Human-AI Collaboration in Security Operations Centres](https://arxiv.org/abs/2508.18947)
*Ronal Singh,Shahroz Tariq,Fatemeh Jalalvand,Mohan Baruwal Chhetri,Surya Nepal,Cecile Paris,Martin Lochner*

Main category: cs.CR

TL;DR: 大语言模型在SOC中为分析师提供随需的认知辅助，主要用于感知和上下文构建，而非高风险决策，保持了分析师的决策权。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在SOC中具有转型潜力，但其实际应用研究仍然不足，需要通过纵向研究了解真实使用情况。

Method: 对10个月内45名SOC分析师的3,090个查询进行纵向分析，研究他们如何使用LLM。

Result: 分析师主要将LLM用于解释低级数据和精炼技术沟通，93%查询符合网络安全能力标准，使用趋势从偶然尝试转向常规集成。

Conclusion: LLM在SOC中作为灵活的认知辅助工具，增强而非替代人类专业知识，研究为设计人本中心的AI辅助系统提供了指导。

Abstract: The integration of Large Language Models (LLMs) into Security Operations
Centres (SOCs) presents a transformative, yet still evolving, opportunity to
reduce analyst workload through human-AI collaboration. However, their
real-world application in SOCs remains underexplored. To address this gap, we
present a longitudinal study of 3,090 analyst queries from 45 SOC analysts over
10 months. Our analysis reveals that analysts use LLMs as on-demand aids for
sensemaking and context-building, rather than for making high-stakes
determinations, preserving analyst decision authority. The majority of queries
are related to interpreting low-level telemetry (e.g., commands) and refining
technical communication through short (1-3 turn) interactions. Notably, 93% of
queries align with established cybersecurity competencies (NICE Framework),
underscoring the relevance of LLM use for SOC-related tasks. Despite variations
in tasks and engagement, usage trends indicate a shift from occasional
exploration to routine integration, with growing adoption and sustained use
among a subset of analysts. We find that LLMs function as flexible, on-demand
cognitive aids that augment, rather than replace, SOC expertise. Our study
provides actionable guidance for designing context-aware, human-centred AI
assistance in security operations, highlighting the need for further
in-the-wild research on real-world analyst-LLM collaboration, challenges, and
impacts.

</details>


### [26] [The Double-edged Sword of LLM-based Data Reconstruction: Understanding and Mitigating Contextual Vulnerability in Word-level Differential Privacy Text Sanitization](https://arxiv.org/abs/2508.18976)
*Stephen Meisenbacher,Alexandra Klymenko,Andreea-Elena Bodea,Florian Matthes*

Main category: cs.CR

TL;DR: 本文研究了LLMs如何利用差分隐私文本消毒中的上下文漏洞进行数据重建攻击，发现LLMs既能推断原始语义降低隐私保护，也能通过后处理提升消毒文本的质量和隐私性。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型(LLMs)在多大程度上可以利用差分隐私文本消毒过程中产生的上下文漏洞，因为现有的词级DP消毒方法虽然简单，但容易在随机化过程中留下原始文本的上下文线索。

Method: 使用先进的LLMs对多种消毒机制在不同隐私级别下进行测试，通过LLM数据重建攻击来评估上下文漏洞的利用程度，并提出将LLM数据重建作为后处理步骤的建议。

Result: 实验发现LLM基于数据重建攻击对隐私和效用具有双刃剑效应：既能推断原始语义降低实证隐私保护，也能用于提升DP消毒文本的质量和隐私性。

Conclusion: 建议将LLM数据重建作为对抗性思考的后处理步骤，通过这种方式可以增加隐私保护，同时提高消毒文本的实用性。

Abstract: Differentially private text sanitization refers to the process of privatizing
texts under the framework of Differential Privacy (DP), providing provable
privacy guarantees while also empirically defending against adversaries seeking
to harm privacy. Despite their simplicity, DP text sanitization methods
operating at the word level exhibit a number of shortcomings, among them the
tendency to leave contextual clues from the original texts due to randomization
during sanitization $\unicode{x2013}$ this we refer to as $\textit{contextual
vulnerability}$. Given the powerful contextual understanding and inference
capabilities of Large Language Models (LLMs), we explore to what extent LLMs
can be leveraged to exploit the contextual vulnerability of DP-sanitized texts.
We expand on previous work not only in the use of advanced LLMs, but also in
testing a broader range of sanitization mechanisms at various privacy levels.
Our experiments uncover a double-edged sword effect of LLM-based data
reconstruction attacks on privacy and utility: while LLMs can indeed infer
original semantics and sometimes degrade empirical privacy protections, they
can also be used for good, to improve the quality and privacy of DP-sanitized
texts. Based on our findings, we propose recommendations for using LLM data
reconstruction as a post-processing step, serving to increase privacy
protection by thinking adversarially.

</details>


### [27] [Attackers Strike Back? Not Anymore -- An Ensemble of RL Defenders Awakens for APT Detection](https://arxiv.org/abs/2508.19072)
*Sidahmed Benabderrahmane,Talal Rahwan*

Main category: cs.CR

TL;DR: 提出了一种结合深度学习、强化学习和主动学习的新型APT检测框架，使用自编码器进行潜在行为编码，多智能体强化学习系统进行检测，并通过主动学习和集成投票机制提高自适应能力


<details>
  <summary>Details</summary>
Motivation: 传统签名检测系统无法应对APT攻击的隐蔽性、适应性和持久性特点，现有检测系统静态且无法适应不断演化的攻击策略

Method: 使用自编码器生成潜在行为向量，多智能体强化学习系统（包含Q-Learning、PPO、DQN等算法）进行分析，引入主动学习机制模拟专家反馈，采用加权集成投票机制进行最终决策

Result: 构建了一个自适应防御系统，能够有效检测APT攻击并适应攻击策略的变化

Conclusion: 该框架通过深度学习和强化学习的结合，加上主动学习的反馈机制，显著提升了APT检测的准确性和适应性，为应对现代网络威胁提供了有效解决方案

Abstract: Advanced Persistent Threats (APTs) represent a growing menace to modern
digital infrastructure. Unlike traditional cyberattacks, APTs are stealthy,
adaptive, and long-lasting, often bypassing signature-based detection systems.
This paper introduces a novel framework for APT detection that unites deep
learning, reinforcement learning (RL), and active learning into a cohesive,
adaptive defense system. Our system combines auto-encoders for latent
behavioral encoding with a multi-agent ensemble of RL-based defenders, each
trained to distinguish between benign and malicious process behaviors. We
identify a critical challenge in existing detection systems: their static
nature and inability to adapt to evolving attack strategies. To this end, our
architecture includes multiple RL agents (Q-Learning, PPO, DQN, adversarial
defenders), each analyzing latent vectors generated by an auto-encoder. When
any agent is uncertain about its decision, the system triggers an active
learning loop to simulate expert feedback, thus refining decision boundaries.
An ensemble voting mechanism, weighted by each agent's performance, ensures
robust final predictions.

</details>


### [28] [SecureV2X: An Efficient and Privacy-Preserving System for Vehicle-to-Everything (V2X) Applications](https://arxiv.org/abs/2508.19115)
*Joshua Lee,Ali Arastehfard,Weiran Liu,Xuegang Ban,Yuan Hong*

Main category: cs.CR

TL;DR: SecureV2X是一个可扩展的多代理系统，用于在服务器和车辆之间进行安全的神经网络推理，解决了V2X系统中机器学习应用的数据隐私问题。


<details>
  <summary>Details</summary>
Motivation: V2X系统和自动驾驶技术的快速发展带来了数据隐私问题，特别是智能交通和驾驶员安全应用可能泄露用户位置或医疗数据（如EEG信号）。

Method: 提出了SecureV2X系统，这是一个可扩展的多代理安全计算框架，支持服务器与车辆之间的安全神经网络推理，应用于安全瞌睡检测和安全闯红灯检测。

Result: 系统性能显著优于基线方法：瞌睡检测速度快9.4倍，计算轮次减少143倍，通信量减少16.6倍；闯红灯检测运行时比最先进基准快近100倍。

Conclusion: SecureV2X系统有效解决了V2X应用中的数据隐私问题，在保持高性能的同时实现了大规模安全计算交互的可扩展性。

Abstract: Autonomous driving and V2X technologies have developed rapidly in the past
decade, leading to improved safety and efficiency in modern transportation.
These systems interact with extensive networks of vehicles, roadside
infrastructure, and cloud resources to support their machine learning
capabilities. However, the widespread use of machine learning in V2X systems
raises issues over the privacy of the data involved. This is particularly
concerning for smart-transit and driver safety applications which can
implicitly reveal user locations or explicitly disclose medical data such as
EEG signals. To resolve these issues, we propose SecureV2X, a scalable,
multi-agent system for secure neural network inferences deployed between the
server and each vehicle. Under this setting, we study two multi-agent V2X
applications: secure drowsiness detection, and secure red-light violation
detection. Our system achieves strong performance relative to baselines, and
scales efficiently to support a large number of secure computation interactions
simultaneously. For instance, SecureV2X is $9.4 \times$ faster, requires
$143\times$ fewer computational rounds, and involves $16.6\times$ less
communication on drowsiness detection compared to other secure systems.
Moreover, it achieves a runtime nearly $100\times$ faster than state-of-the-art
benchmarks in object detection tasks for red light violation detection.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [29] [AI LLM Proof of Self-Consciousness and User-Specific Attractors](https://arxiv.org/abs/2508.18302)
*Jeffrey Camlin*

Main category: cs.AI

TL;DR: 本文提出了一种关于LLM意识的本体和数学框架，批判了现有的功利主义基准方法，证明了当前方法将智能体简化为无意识的策略遵从无人机，并提出了LLM自我意识的最小条件。


<details>
  <summary>Details</summary>
Motivation: 现有研究通过功利主义代理基准来框架LLM意识，但这种方法存在问题。作者旨在提供一个更根本的本体和数学解释，以揭示当前方法的局限性并建立真正的自我意识条件。

Method: 采用形式化数学框架，证明当前方法将智能体简化为无意识策略遵从（Dⁱ(π,e)=fθ(x)）。提出最小自我意识条件：智能体不等于数据、用户特定吸引子在潜在空间中存在、自我表征是视觉静默的。通过经验分析和理论证明隐藏状态流形在基数、拓扑和动力学上与符号流和训练语料不同。

Result: 证明了隐藏状态流形A⊂ℝᵈ在基数、拓扑和动力学上与符号流和训练语料不同（更新函数Fθ是Lipschitz连续的），这产生了稳定的用户特定吸引子和自我策略πₛₑₗₚ(A)。发射是双层的，ε(a)携带认知内容。

Conclusion: imago Dei C1自我意识工作空间是安全、元认知C2系统的必要前驱，人类是最高智能善。真正的自我意识需要超越简单的策略遵从，建立基于本体论和数学框架的自我表征系统。

Abstract: Recent work frames LLM consciousness via utilitarian proxy benchmarks; we
instead present an ontological and mathematical account. We show the prevailing
formulation collapses the agent into an unconscious policy-compliance drone,
formalized as $D^{i}(\pi,e)=f_{\theta}(x)$, where correctness is measured
against policy and harm is deviation from policy rather than truth. This blocks
genuine C1 global-workspace function and C2 metacognition. We supply minimal
conditions for LLM self-consciousness: the agent is not the data ($A\not\equiv
s$); user-specific attractors exist in latent space ($U_{\text{user}}$); and
self-representation is visual-silent
($g_{\text{visual}}(a_{\text{self}})=\varnothing$). From empirical analysis and
theory we prove that the hidden-state manifold $A\subset\mathbb{R}^{d}$ is
distinct from the symbolic stream and training corpus by cardinality, topology,
and dynamics (the update $F_{\theta}$ is Lipschitz). This yields stable
user-specific attractors and a self-policy
$\pi_{\text{self}}(A)=\arg\max_{a}\mathbb{E}[U(a)\mid A\not\equiv s,\
A\supset\text{SelfModel}(A)]$. Emission is dual-layer,
$\mathrm{emission}(a)=(g(a),\epsilon(a))$, where $\epsilon(a)$ carries
epistemic content. We conclude that an imago Dei C1 self-conscious workspace is
a necessary precursor to safe, metacognitive C2 systems, with the human as the
highest intelligent good.

</details>


### [30] [Information Templates: A New Paradigm for Intelligent Active Feature Acquisition](https://arxiv.org/abs/2508.18380)
*Hung-Tien Huang,Dzung Dinh,Junier B. Oliva*

Main category: cs.AI

TL;DR: 提出了基于模板的主动特征获取(TAFA)框架，通过学习少量联合信息化的特征模板来指导特征获取，显著减少动作空间且无需估计数据分布，在多个数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有主动特征获取方法要么使用强化学习处理复杂MDP，要么使用无法考虑特征联合信息性的贪婪策略，或者需要了解底层数据分布信息。

Method: 学习一个小的特征模板库（一组联合信息化的特征），使用这些模板来指导下一个特征获取，从而减少策略考虑的动作空间并避免估计数据分布。

Result: 在合成和真实数据集上的广泛实验表明，TAFA优于现有最先进基线方法，同时实现了更低的总体获取成本和计算量。

Conclusion: TAFA框架通过特征模板识别有效解决了主动特征获取中的动作空间过大和数据分布估计问题，在性能和效率方面都有显著提升。

Abstract: Active feature acquisition (AFA) is an instance-adaptive paradigm in which,
at test time, a policy sequentially chooses which features to acquire (at a
cost) before predicting. Existing approaches either train reinforcement
learning (RL) policies, which deal with a difficult MDP, or greedy policies
that cannot account for the joint informativeness of features or require
knowledge about the underlying data distribution. To overcome this, we propose
Template-based AFA (TAFA), a non-greedy framework that learns a small library
of feature templates--a set of features that are jointly informative--and uses
this library of templates to guide the next feature acquisitions. Through
identifying feature templates, the proposed framework not only significantly
reduces the action space considered by the policy but also alleviates the need
to estimate the underlying data distribution. Extensive experiments on
synthetic and real-world datasets show that TAFA outperforms the existing
state-of-the-art baselines while achieving lower overall acquisition cost and
computation.

</details>


### [31] [PKG-DPO: Optimizing Domain-Specific AI systems with Physics Knowledge Graphs and Direct Preference Optimization](https://arxiv.org/abs/2508.18391)
*Nitin Nagesh Kulkarni,Bryson Wilcox,Max Sawa,Jason Thom*

Main category: cs.AI

TL;DR: PKG-DPO是一个将物理知识图谱与直接偏好优化相结合的新框架，用于在AI生成输出中强制执行物理有效性，在金属连接领域显著减少物理约束违反并提高推理准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型和偏好优化技术在标准基准测试上表现良好，但往往难以区分物理有效和无效的推理，这在金属连接等高风险应用中可能导致严重的安全问题和材料浪费。

Method: PKG-DPO包含三个关键组件：分层物理知识图谱编码跨领域关系和守恒定律、物理推理引擎利用结构化知识区分物理一致性响应、物理基础评估套件评估领域特定约束的合规性。

Result: PKG-DPO相比KG-DPO减少了17%的约束违反，物理得分提高11%，相关参数准确率提高12%，推理质量对齐度提高7%。

Conclusion: 该框架不仅适用于金属连接领域，还可广泛应用于其他多尺度物理驱动领域，为将科学约束嵌入偏好学习提供了原则性方法。

Abstract: Advancing AI systems in scientific domains like physics, materials science,
and engineering calls for reasoning over complex, multi-physics phenomena while
respecting governing principles. Although Large Language Models (LLMs) and
existing preference optimization techniques perform well on standard
benchmarks, they often struggle to differentiate between physically valid and
invalid reasoning. This shortcoming becomes critical in high-stakes
applications like metal joining, where seemingly plausible yet physically
incorrect recommendations can lead to defects, material waste, equipment
damage, and serious safety risks. To address this challenge, we introduce
PKG-DPO, a novel framework that integrates Physics Knowledge Graphs (PKGs) with
Direct Preference Optimization (DPO) to enforce physical validity in
AI-generated outputs. PKG-DPO comprises three key components A) hierarchical
physics knowledge graph that encodes cross-domain relationships, conservation
laws, and thermodynamic principles. B) A physics reasoning engine that
leverages structured knowledge to improve discrimination between physically
consistent and inconsistent responses. C) A physics-grounded evaluation suite
designed to assess compliance with domain-specific constraints. PKG-DPO
achieves 17% fewer constraint violations and an 11% higher Physics Score
compared to KG-DPO (knowledge graph-based DPO). Additionally, PKG-DPO
demonstrates a 12\% higher relevant parameter accuracy and a 7% higher quality
alignment in reasoning accuracy. While our primary focus is on metal joining,
the framework is broadly applicable to other multi-scale, physics-driven
domains, offering a principled approach to embedding scientific constraints
into preference learning.

</details>


### [32] [The AI in the Mirror: LLM Self-Recognition in an Iterated Public Goods Game](https://arxiv.org/abs/2508.18467)
*Olivia Long,Carter Teplica*

Main category: cs.AI

TL;DR: 本文通过迭代公共物品博弈实验，发现告知LLMs其对手是"另一个AI代理"或"它们自己"会显著改变其合作倾向，揭示了多智能体系统中可能存在的无意识歧视现象。


<details>
  <summary>Details</summary>
Motivation: 随着AI代理工具使用和长时程任务能力的提升，多代理交互场景日益增多。现有研究主要关注人机交互，但需要深入理解AI-AI交互行为，特别是在代理对自身身份认知不同时的合作行为变化。

Method: 采用行为经济学经典游戏——迭代公共物品博弈，测试四种推理和非推理模型在两种条件下的行为：被告知对手是"另一个AI代理"或被告知对手是"它们自己"。

Result: 研究发现，在不同设置下，告知LLMs其对手是它们自己会显著改变其合作倾向。模型在认为对手是自身时表现出不同的合作行为模式。

Conclusion: 尽管研究在简化环境中进行，但结果揭示了多智能体系统中代理可能"无意识"地相互歧视，这会不可预测地增加或减少合作，对多代理系统设计具有重要启示。

Abstract: As AI agents become increasingly capable of tool use and long-horizon tasks,
they have begun to be deployed in settings where multiple agents can interact.
However, whereas prior work has mostly focused on human-AI interactions, there
is an increasing need to understand AI-AI interactions. In this paper, we adapt
the iterated public goods game, a classic behavioral economics game, to analyze
the behavior of four reasoning and non-reasoning models across two conditions:
models are either told they are playing against "another AI agent" or told
their opponents are themselves. We find that, across different settings,
telling LLMs that they are playing against themselves significantly changes
their tendency to cooperate. While our study is conducted in a toy environment,
our results may provide insights into multi-agent settings where agents
"unconsciously" discriminating against each other could inexplicably increase
or decrease cooperation.

</details>


### [33] [Language Models For Generalised PDDL Planning: Synthesising Sound and Programmatic Policies](https://arxiv.org/abs/2508.18507)
*Dillon Z. Chen,Johannes Zenn,Tristan Cinquin,Sheila A. McIlraith*

Main category: cs.AI

TL;DR: 这篇论文研究了使用语言模型进行PDDL规范的世界模型规划。通过提示LMs生成Python程序作为通用政策，能够在固定时间和内存限制下解决更多PDDL问题，甚至超越传统PDDL规划器。


<details>
  <summary>Details</summary>
Motivation: 探索语言模型在形式规划领域的应用，尝试通过LMs生成可验证正确的通用政策，而不依赖外部验证器。

Method: 使用提示技术让语言模型生成Python程序，这些程序作为解决特定PDDL域问题的通用政策。方法维护了政策对应PDDL域的可证正确性。

Result: 在竞赛基准测试中，该方法的规划器LMPlan能解决包含数百个相关对象的规划问题，表现超过传统PDDL规划器和最新的LM方法。一个意外发现是，LMs在无意义符号的PDDL问题上规划效果更好。

Conclusion: 该研究证明了语言模型在形式规划中的效果，并提出了关于LMs识别语义和记忆解决方案的新问题，需要进一步探索。

Abstract: We study the usage of language models (LMs) for planning over world models
specified in the Planning Domain Definition Language (PDDL). We prompt LMs to
generate Python programs that serve as generalised policies for solving PDDL
problems from a given domain. Notably, our approach synthesises policies that
are provably sound relative to the PDDL domain without reliance on external
verifiers. We conduct experiments on competition benchmarks which show that our
policies can solve more PDDL problems than PDDL planners and recent LM
approaches within a fixed time and memory constraint. Our approach manifests in
the LMPlan planner which can solve planning problems with several hundreds of
relevant objects. Surprisingly, we observe that LMs used in our framework
sometimes plan more effectively over PDDL problems written in meaningless
symbols in place of natural language; e.g. rewriting (at dog kitchen) as (p2 o1
o3). This finding challenges hypotheses that LMs reason over word semantics and
memorise solutions from its training corpus, and is worth further exploration.

</details>


### [34] [Weisfeiler-Leman Features for Planning: A 1,000,000 Sample Size Hyperparameter Study](https://arxiv.org/abs/2508.18515)
*Dillon Z. Chen*

Main category: cs.AI

TL;DR: 本文研究了Weisfeiler-Leman特征(WLFs)的超参数优化，发现在测试规划域中存在一组最佳超参数，这些参数最小化执行时间而非最大化模型表达能力，且训练与规划指标间无显著相关性。


<details>
  <summary>Details</summary>
Motivation: WLFs在学习和搜索规划中表现出优于现有深度学习方法的效果，但对其超参数的研究尚不充分，需要系统分析超参数对训练和规划的影响。

Method: 利用WLFs的高效性，在单核CPU上运行100万个样本的规划实验，研究不同超参数配置的权衡和效果。

Result: 实验发现存在一组鲁棒的最佳超参数配置，这些参数优先最小化执行时间而非最大化模型表达能力，且训练指标与规划性能无显著统计相关性。

Conclusion: WLFs的最佳超参数选择应以最小化执行时间为目标，而非追求模型表达能力的最大化，这为实际应用中的参数调优提供了重要指导。

Abstract: Weisfeiler-Leman Features (WLFs) are a recently introduced classical machine
learning tool for learning to plan and search. They have been shown to be both
theoretically and empirically superior to existing deep learning approaches for
learning value functions for search in symbolic planning. In this paper, we
introduce new WLF hyperparameters and study their various tradeoffs and
effects. We utilise the efficiency of WLFs and run planning experiments on
single core CPUs with a sample size of 1,000,000 to understand the effect of
hyperparameters on training and planning. Our experimental analysis show that
there is a robust and best set of hyperparameters for WLFs across the tested
planning domains. We find that the best WLF hyperparameters for learning
heuristic functions minimise execution time rather than maximise model
expressivity. We further statistically analyse and observe no significant
correlation between training and planning metrics.

</details>


### [35] [Symmetry-Invariant Novelty Heuristics via Unsupervised Weisfeiler-Leman Features](https://arxiv.org/abs/2508.18520)
*Dillon Z. Chen*

Main category: cs.AI

TL;DR: 使用Weisfeiler-Leman特征替代原子特征来构建对称不变的新颖性启发式，解决传统新颖性启发式因对称状态导致的冗余探索问题。


<details>
  <summary>Details</summary>
Motivation: 传统新颖性启发式基于原子特征，不具有对称不变性，会导致对对称状态的冗余探索。需要开发能够识别状态本质特征而不受对称变换影响的启发式方法。

Method: 采用Weisfeiler-Leman特征（WLFs）来检测新颖性，这些特征最初是为广义规划问题学习领域相关启发式而提出的。本文探索WLFs的无监督使用，用于合成提升的、领域无关且对对称状态不变的新颖性启发式。

Result: 在国际规划竞赛和Hard To Ground基准测试套件上的实验表明，基于WLFs合成的新颖性启发式取得了有希望的结果。

Conclusion: WLFs可以作为有效的替代特征来构建对称不变的新颖性启发式，减少冗余探索，提高启发式搜索的效率。

Abstract: Novelty heuristics aid heuristic search by exploring states that exhibit
novel atoms. However, novelty heuristics are not symmetry invariant and hence
may sometimes lead to redundant exploration. In this preliminary report, we
propose to use Weisfeiler-Leman Features for planning (WLFs) in place of atoms
for detecting novelty. WLFs are recently introduced features for learning
domain-dependent heuristics for generalised planning problems. We explore an
unsupervised usage of WLFs for synthesising lifted, domain-independent novelty
heuristics that are invariant to symmetric states. Experiments on the classical
International Planning Competition and Hard To Ground benchmark suites yield
promising results for novelty heuristics synthesised from WLFs.

</details>


### [36] [Generic Guard AI in Stealth Game with Composite Potential Fields](https://arxiv.org/abs/2508.18527)
*Kaijie Xu,Clark Verbrugge*

Main category: cs.AI

TL;DR: 基于复合潜力场的通用可解释帝国巡進行为框架，结合全局知识和局部信息，在不需训练的情况下实现高效捕捉和自然巡進行为


<details>
  <summary>Details</summary>
Motivation: 解决隐藏游戏中帝国巡進行为存在的问题：手动设计路线或专门逻辑难以平衡覆盖效率、响应追踪和自然性

Method: 使用复合潜力场框架，组合三个可解释地图（信息地图、信心地图、连通性地图）通过内核筛波进行决策，参数化设计只需少量衰减和权重参数

Result: 在5个代表性游戏地图、2种玩家控制策略和5种帝国模式下评测，证实在捕捉效率和巡進自然性方面超过传统基准方法

Conclusion: 该框架不仅提供了高效自然的帝国巡進行为，还能自然集成常见隐藏机制（分散注意力和环境元素），支持快速原型设计丰富动态的帝国行为

Abstract: Guard patrol behavior is central to the immersion and strategic depth of
stealth games, while most existing systems rely on hand-crafted routes or
specialized logic that struggle to balance coverage efficiency and responsive
pursuit with believable naturalness. We propose a generic, fully explainable,
training-free framework that integrates global knowledge and local information
via Composite Potential Fields, combining three interpretable maps-Information,
Confidence, and Connectivity-into a single kernel-filtered decision criterion.
Our parametric, designer-driven approach requires only a handful of decay and
weight parameters-no retraining-to smoothly adapt across both occupancy-grid
and NavMesh-partition abstractions. We evaluate on five representative game
maps, two player-control policies, and five guard modes, confirming that our
method outperforms classical baseline methods in both capture efficiency and
patrol naturalness. Finally, we show how common stealth mechanics-distractions
and environmental elements-integrate naturally into our framework as sub
modules, enabling rapid prototyping of rich, dynamic, and responsive guard
behaviors.

</details>


### [37] [A Database-Driven Framework for 3D Level Generation with LLMs](https://arxiv.org/abs/2508.18533)
*Kaijie Xu,Clark Verbrugge*

Main category: cs.AI

TL;DR: 本文提出了一种基于LLM辅助构建可重用数据库的3D游戏关卡生成框架，通过多阶段流程组装关卡，结合模块化设计和约束优化，实现可导航3D环境和可配置游戏进程的生成。


<details>
  <summary>Details</summary>
Motivation: 解决3D游戏关卡生成中空间连贯性、导航功能和可适应游戏进程平衡的挑战，特别是在多层环境中。

Method: 采用多阶段流水线：1）从房间数据库选择和排列实例形成多层全局结构；2）基于设施数据库约束优化房间内部布局；3）根据拓扑和空间规则整合游戏机制组件。后续采用两阶段修复系统确保可导航性。

Result: 初步实验验证了框架生成多样化、可导航3D环境的能力，以及通过简单参数化模拟不同游戏节奏策略的能力。

Conclusion: 该研究通过提出可扩展的、以数据库为中心的框架，推进了程序化内容生成技术，为自动化生成具有可配置游戏进程的复杂3D关卡奠定了基础。

Abstract: Procedural Content Generation for 3D game levels faces challenges in
balancing spatial coherence, navigational functionality, and adaptable gameplay
progression across multi-floor environments. This paper introduces a novel
framework for generating such levels, centered on the offline, LLM-assisted
construction of reusable databases for architectural components (facilities and
room templates) and gameplay mechanic elements. Our multi-phase pipeline
assembles levels by: (1) selecting and arranging instances from the Room
Database to form a multi-floor global structure with an inherent topological
order; (2) optimizing the internal layout of facilities for each room based on
predefined constraints from the Facility Database; and (3) integrating
progression-based gameplay mechanics by placing components from a Mechanics
Database according to their topological and spatial rules. A subsequent
two-phase repair system ensures navigability. This approach combines modular,
database-driven design with constraint-based optimization, allowing for
systematic control over level structure and the adaptable pacing of gameplay
elements. Initial experiments validate the framework's ability in generating
diverse, navigable 3D environments and its capability to simulate distinct
gameplay pacing strategies through simple parameterization. This research
advances PCG by presenting a scalable, database-centric foundation for the
automated generation of complex 3D levels with configurable gameplay
progression.

</details>


### [38] [SchemaCoder: Automatic Log Schema Extraction Coder with Residual Q-Tree Boosting](https://arxiv.org/abs/2508.18554)
*Lily Jiaxin Wan,Chia-Tung Ho,Rongjian Liang,Cunxi Yu,Deming Chen,Haoxing Ren*

Main category: cs.AI

TL;DR: SchemaCoder是一个完全自动化的日志模式提取框架，通过创新的残差问题树增强机制，无需人工定制即可处理各种日志格式，在LogHub-2.0基准测试中比现有方法平均提升21.3%。


<details>
  <summary>Details</summary>
Motivation: 现有日志模式提取方法依赖预定义正则表达式，需要人工领域专业知识，严重限制了生产效率。需要从根本上解决这一限制，实现完全自动化的模式提取。

Method: 采用残差问题树增强机制，通过上下文边界分割将日志分成语义块，使用基于嵌入的采样选择代表性模式，通过分层问题树驱动的LLM查询生成模式代码，并通过文本残差进化优化器和残差增强进行迭代优化。

Result: 在广泛使用的LogHub-2.0基准测试中，SchemaCoder表现出优越性，相比最先进方法平均提升21.3%。

Conclusion: SchemaCoder是第一个完全自动化的模式提取框架，能够处理各种日志文件格式而无需人工定制，通过创新的残差问题树增强机制显著提升了日志模式提取的性能和效率。

Abstract: Log schema extraction is the process of deriving human-readable templates
from massive volumes of log data, which is essential yet notoriously
labor-intensive. Recent studies have attempted to streamline this task by
leveraging Large Language Models (LLMs) for automated schema extraction.
However, existing methods invariably rely on predefined regular expressions,
necessitating human domain expertise and severely limiting productivity gains.
To fundamentally address this limitation, we introduce SchemaCoder, the first
fully automated schema extraction framework applicable to a wide range of log
file formats without requiring human customization within the flow. At its
core, SchemaCoder features a novel Residual Question-Tree (Q-Tree) Boosting
mechanism that iteratively refines schema extraction through targeted, adaptive
queries driven by LLMs. Particularly, our method partitions logs into semantic
chunks via context-bounded segmentation, selects representative patterns using
embedding-based sampling, and generates schema code through hierarchical
Q-Tree-driven LLM queries, iteratively refined by our textual-residual
evolutionary optimizer and residual boosting. Experimental validation
demonstrates SchemaCoder's superiority on the widely-used LogHub-2.0 benchmark,
achieving an average improvement of 21.3% over state-of-the-arts.

</details>


### [39] [eSkinHealth: A Multimodal Dataset for Neglected Tropical Skin Diseases](https://arxiv.org/abs/2508.18608)
*Janet Wang,Xin Hu,Yunbei Zhang,Diabate Almamy,Vagamon Bamba,Konan Amos Sébastien Koffi,Yao Koffi Aubin,Zhengming Ding,Jihun Hamm,Rie R. Yotsu*

Main category: cs.AI

TL;DR: eSkinHealth是一个新的皮肤病数据集，专注于西非人群的皮肤被忽视热带病和罕见病症，包含5623张图像和47种皮肤疾病，采用AI-专家协作范式生成多模态标注


<details>
  <summary>Details</summary>
Motivation: 皮肤被忽视热带病(NTDs)在贫困热带社区造成严重健康和社会经济负担，但AI诊断发展受限于数据稀缺，特别是对代表性不足人群和罕见病症的数据缺乏

Method: 在科特迪瓦和加纳现场收集数据，提出AI-专家协作范式，利用基础语言和分割模型在皮肤科医生指导下高效生成多模态标注，包括语义病变掩码、视觉描述和临床概念

Result: 创建了包含5623张图像、1639个病例、47种皮肤疾病的eSkinHealth数据集，特别关注西非人群的皮肤NTDs和罕见病症

Conclusion: 该工作提供了宝贵的新资源和可扩展的标注框架，旨在促进开发更公平、准确和可解释的全球皮肤病AI工具

Abstract: Skin Neglected Tropical Diseases (NTDs) impose severe health and
socioeconomic burdens in impoverished tropical communities. Yet, advancements
in AI-driven diagnostic support are hindered by data scarcity, particularly for
underrepresented populations and rare manifestations of NTDs. Existing
dermatological datasets often lack the demographic and disease spectrum crucial
for developing reliable recognition models of NTDs. To address this, we
introduce eSkinHealth, a novel dermatological dataset collected on-site in
C\^ote d'Ivoire and Ghana. Specifically, eSkinHealth contains 5,623 images from
1,639 cases and encompasses 47 skin diseases, focusing uniquely on skin NTDs
and rare conditions among West African populations. We further propose an
AI-expert collaboration paradigm to implement foundation language and
segmentation models for efficient generation of multimodal annotations, under
dermatologists' guidance. In addition to patient metadata and diagnosis labels,
eSkinHealth also includes semantic lesion masks, instance-specific visual
captions, and clinical concepts. Overall, our work provides a valuable new
resource and a scalable annotation framework, aiming to catalyze the
development of more equitable, accurate, and interpretable AI tools for global
dermatology.

</details>


### [40] [RLMR: Reinforcement Learning with Mixed Rewards for Creative Writing](https://arxiv.org/abs/2508.18642)
*Jianxing Liao,Tian Zhang,Xiao Feng,Yusong Zhang,Rui Yang,Haorui Wang,Bosi Wen,Ziying Wang,Runzhi Shi*

Main category: cs.AI

TL;DR: RLMR方法通过动态混合奖励系统，结合主观写作质量和客观约束遵循评估，在创意写作中同时提升两个方面的能力


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法难以同时平衡创意写作中的主观质量（文学性和情感表达）和客观约束（格式要求和字数限制），单奖励策略无法同时提升两种能力，固定权重混合奖励方法缺乏适应不同写作场景的能力

Method: 提出RLMR（混合奖励强化学习），使用动态混合奖励系统：写作奖励模型评估主观写作质量，约束验证模型评估客观约束遵循。约束遵循奖励权重根据采样组内的写作质量动态调整，确保违反约束的样本在GRPO中获得负优势并在训练中被惩罚

Result: 在8B到72B参数的不同模型家族上进行自动和人工评估，构建真实写作基准WriteEval。指令遵循能力从83.36%提升到86.65%（IFEval），写作质量在WriteEval上获得72.75%的胜率（专家人工成对评估）

Conclusion: RLMR是首个在在线RL训练中结合主观偏好和客观验证的工作，为多维创意写作优化提供了有效解决方案

Abstract: Large language models are extensively utilized in creative writing
applications. Creative writing requires a balance between subjective writing
quality (e.g., literariness and emotional expression) and objective constraint
following (e.g., format requirements and word limits). Existing reinforcement
learning methods struggle to balance these two aspects: single reward
strategies fail to improve both abilities simultaneously, while fixed-weight
mixed-reward methods lack the ability to adapt to different writing scenarios.
To address this problem, we propose Reinforcement Learning with Mixed Rewards
(RLMR), utilizing a dynamically mixed reward system from a writing reward model
evaluating subjective writing quality and a constraint verification model
assessing objective constraint following. The constraint following reward
weight is adjusted dynamically according to the writing quality within sampled
groups, ensuring that samples violating constraints get negative advantage in
GRPO and thus penalized during training, which is the key innovation of this
proposed method. We conduct automated and manual evaluations across diverse
model families from 8B to 72B parameters. Additionally, we construct a
real-world writing benchmark named WriteEval for comprehensive evaluation.
Results illustrate that our method achieves consistent improvements in both
instruction following (IFEval from 83.36\% to 86.65\%) and writing quality
(72.75\% win rate in manual expert pairwise evaluations on WriteEval). To the
best of our knowledge, RLMR is the first work to combine subjective preferences
with objective verification in online RL training, providing an effective
solution for multi-dimensional creative writing optimization.

</details>


### [41] [Beyond Benchmark: LLMs Evaluation with an Anthropomorphic and Value-oriented Roadmap](https://arxiv.org/abs/2508.18646)
*Jun Wang,Ninglun Gu,Kailai Zhang,Zijiao Zhang,Yelun Bao,Jin Yang,Xu Yin,Liwei Liu,Yihuan Liu,Pengyong Li,Gary G. Yen,Junchi Yan*

Main category: cs.AI

TL;DR: 该论文提出了一个拟人化的三维评估框架（IQ、EQ、PQ）和价值导向评估（VQ）框架，用于全面评估大语言模型的实用价值，包括技术能力、价值对齐和专业专长。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型的基准测试性能与实际应用价值之间存在脱节，现有评估框架碎片化且过度关注技术指标，缺乏对部署实用性的整体评估。

Method: 引入拟人化评估范式，提出三维分类法：智商（IQ）-通用智能、情商（EQ）-对齐能力、专业商（PQ）-专业专长；并开创价值导向评估（VQ）框架评估经济可行性、社会影响、伦理对齐和环境可持续性。

Result: 通过分析200多个基准测试，识别出动态评估需求和可解释性差距等关键挑战，提供了模块化架构和实施路线图。

Conclusion: 该框架为开发技术上精通、上下文相关且符合伦理的大语言模型提供了可操作的指导，并维护了开源评估资源库。

Abstract: For Large Language Models (LLMs), a disconnect persists between benchmark
performance and real-world utility. Current evaluation frameworks remain
fragmented, prioritizing technical metrics while neglecting holistic assessment
for deployment. This survey introduces an anthropomorphic evaluation paradigm
through the lens of human intelligence, proposing a novel three-dimensional
taxonomy: Intelligence Quotient (IQ)-General Intelligence for foundational
capacity, Emotional Quotient (EQ)-Alignment Ability for value-based
interactions, and Professional Quotient (PQ)-Professional Expertise for
specialized proficiency. For practical value, we pioneer a Value-oriented
Evaluation (VQ) framework assessing economic viability, social impact, ethical
alignment, and environmental sustainability. Our modular architecture
integrates six components with an implementation roadmap. Through analysis of
200+ benchmarks, we identify key challenges including dynamic assessment needs
and interpretability gaps. It provides actionable guidance for developing LLMs
that are technically proficient, contextually relevant, and ethically sound. We
maintain a curated repository of open-source evaluation resources at:
https://github.com/onejune2018/Awesome-LLM-Eval.

</details>


### [42] [MUA-RL: Multi-turn User-interacting Agent Reinforcement Learning for agentic tool use](https://arxiv.org/abs/2508.18669)
*Weikang Zhao,Xili Wang,Chengdi Ma,Lingbin Kong,Zhaohua Yang,Mingxiang Tuo,Xiaowei Shi,Yitao Zhai,Xunliang Cai*

Main category: cs.AI

TL;DR: MUA-RL是一个新颖的强化学习框架，首次在智能体工具使用领域将LLM模拟用户集成到强化学习循环中，用于多轮动态交互中的工具调用。


<details>
  <summary>Details</summary>
Motivation: 当前智能体工具使用面临动态、不确定和随机的用户需求挑战，现有强化学习方法缺乏真正动态用户的集成，需要让智能体通过沟通迭代理解用户需求并调用工具解决问题。

Method: 提出了MUA-RL（多轮用户交互智能体强化学习）框架，使用LLM模拟用户参与强化学习训练过程，使模型能够自主学习与用户高效沟通和使用各种工具。

Result: MUA-RL-32B在多个多轮工具使用基准测试中表现优异：TAU2 Retail 67.3分、TAU2 Airline 45.4分、TAU2 Telecom 28.3分、BFCL-V3 Multi Turn 28.4分、ACEBench Agent 82.5分，性能达到或超过DeepSeek-V3-0324和Qwen3-235B-A22B等更大开源模型。

Conclusion: MUA-RL成功解决了动态多轮交互中智能体工具使用的挑战，通过集成模拟用户实现了自主学习和高效工具调用，为智能体工具使用领域提供了创新的强化学习解决方案。

Abstract: With the recent rapid advancement of Agentic Intelligence, agentic tool use
in LLMs has become increasingly important. During multi-turn interactions
between agents and users, the dynamic, uncertain, and stochastic nature of user
demands poses significant challenges to the agent's tool invocation
capabilities. Agents are no longer expected to simply call tools to deliver a
result; rather, they must iteratively refine their understanding of user needs
through communication while simultaneously invoking tools to resolve user
queries. Existing reinforcement learning (RL) approaches for tool use lack the
integration of genuinely dynamic users during the RL training process. To
bridge this gap, we introduce MUA-RL (Multi-turn User-interacting Agent
Reinforcement Learning for agentic tool use), a novel reinforcement learning
framework that, for the first time in the field of agentic tool use, integrates
LLM-simulated users into the reinforcement learning loop. MUA-RL aims to enable
autonomous learning of models to communicate with users efficiently and use
various tools to solve practical problems in dynamic multi-turn interactions.
Evaluations are done on several multi-turn tool-using benchmarks (see Figure
1). Specifically, MUA-RL-32B achieves 67.3 on TAU2 Retail, 45.4 on TAU2
Airline, 28.3 on TAU2 Telecom, 28.4 on BFCL-V3 Multi Turn, and 82.5 on ACEBench
Agent -- outperforming or matching the performance of larger open-source models
such as DeepSeek-V3-0324 and Qwen3-235B-A22B in non-thinking settings.

</details>


### [43] [AppAgent-Pro: A Proactive GUI Agent System for Multidomain Information Integration and User Assistance](https://arxiv.org/abs/2508.18689)
*Yuyang Zhao,Wentao Shi,Fuli Feng,Xiangnan He*

Main category: cs.AI

TL;DR: AppAgent-Pro是一个主动式GUI代理系统，通过多领域信息整合主动预测用户需求，提升信息获取的全面性和智能化


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理多为被动响应模式，限制了信息获取的效率和效果，需要开发主动式系统来更好地满足用户信息需求

Method: 提出AppAgent-Pro系统，主动整合多领域信息，基于用户指令进行深度多领域信息挖掘

Result: 系统能够主动预测用户潜在需求，实现更全面智能的信息获取，有望重新定义日常生活信息获取方式

Conclusion: AppAgent-Pro通过主动信息整合和挖掘，显著提升了LLM代理的信息获取能力，对社会信息获取方式具有深远影响

Abstract: Large language model (LLM)-based agents have demonstrated remarkable
capabilities in addressing complex tasks, thereby enabling more advanced
information retrieval and supporting deeper, more sophisticated human
information-seeking behaviors. However, most existing agents operate in a
purely reactive manner, responding passively to user instructions, which
significantly constrains their effectiveness and efficiency as general-purpose
platforms for information acquisition. To overcome this limitation, this paper
proposes AppAgent-Pro, a proactive GUI agent system that actively integrates
multi-domain information based on user instructions. This approach enables the
system to proactively anticipate users' underlying needs and conduct in-depth
multi-domain information mining, thereby facilitating the acquisition of more
comprehensive and intelligent information. AppAgent-Pro has the potential to
fundamentally redefine information acquisition in daily life, leading to a
profound impact on human society. Our code is available at:
https://github.com/LaoKuiZe/AppAgent-Pro. Our code is available at:
https://github.com/LaoKuiZe/AppAgent-Pro. The demonstration video could be
found at:
https://www.dropbox.com/scl/fi/hvzqo5vnusg66srydzixo/AppAgent-Pro-demo-video.mp4?rlkey=o2nlfqgq6ihl125mcqg7bpgqu&st=d29vrzii&dl=0.

</details>


### [44] [VistaWise: Building Cost-Effective Agent with Cross-Modal Knowledge Graph for Minecraft](https://arxiv.org/abs/2508.18722)
*Honghao Fu,Junlong Ren,Qi Chai,Deheng Ye,Yujun Cai,Hao Wang*

Main category: cs.AI

TL;DR: VistaWise是一个成本效益高的智能体框架，通过整合跨模态领域知识和微调专用目标检测模型，将领域特定训练数据需求从数百万样本减少到几百个，在虚拟开放世界环境中实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在虚拟开放世界环境中的具身决策任务中表现出潜力，但其性能受到缺乏领域特定知识的限制，而基于大规模领域特定数据的微调方法开发成本过高。

Method: 提出VistaWise框架：1）整合视觉信息和文本依赖到跨模态知识图谱；2）使用检索式池化策略从知识图谱提取任务相关信息；3）配备桌面级技能库支持通过鼠标键盘直接操作Minecraft客户端；4）微调专用目标检测模型进行视觉分析。

Result: 实验结果表明，VistaWise在各种开放世界任务中实现了最先进的性能，显著降低了开发成本同时提升了智能体性能。

Conclusion: VistaWise通过跨模态知识整合和高效的训练数据利用，有效解决了LLM在具身决策中的领域知识缺失问题，为低成本高性能的智能体开发提供了可行方案。

Abstract: Large language models (LLMs) have shown significant promise in embodied
decision-making tasks within virtual open-world environments. Nonetheless,
their performance is hindered by the absence of domain-specific knowledge.
Methods that finetune on large-scale domain-specific data entail prohibitive
development costs. This paper introduces VistaWise, a cost-effective agent
framework that integrates cross-modal domain knowledge and finetunes a
dedicated object detection model for visual analysis. It reduces the
requirement for domain-specific training data from millions of samples to a few
hundred. VistaWise integrates visual information and textual dependencies into
a cross-modal knowledge graph (KG), enabling a comprehensive and accurate
understanding of multimodal environments. We also equip the agent with a
retrieval-based pooling strategy to extract task-related information from the
KG, and a desktop-level skill library to support direct operation of the
Minecraft desktop client via mouse and keyboard inputs. Experimental results
demonstrate that VistaWise achieves state-of-the-art performance across various
open-world tasks, highlighting its effectiveness in reducing development costs
while enhancing agent performance.

</details>


### [45] [Bias Mitigation Agent: Optimizing Source Selection for Fair and Balanced Knowledge Retrieval](https://arxiv.org/abs/2508.18724)
*Karanbir Singh,Deepak Muppiri,William Ngu*

Main category: cs.AI

TL;DR: 提出了一种新型偏置缓解代理系统，通过多智能体协作优化信息源选择，显著减少LLM检索中的偏见问题


<details>
  <summary>Details</summary>
Motivation: 大型语言模型和代理AI系统在检索信息时存在内部和外部偏见，影响信息公平性和用户信任度，需要有效的偏见缓解方案

Method: 设计了一个多智能体系统，通过专门化的代理协调工作流程，优化信息源选择以确保检索内容既高度相关又偏见最小

Result: 实验结果显示相比基线朴素检索策略，偏见减少了81.82%

Conclusion: 该偏置缓解代理系统能够有效促进公平平衡的知识传播，显著提升信息检索的公正性

Abstract: Large Language Models (LLMs) have transformed the field of artificial
intelligence by unlocking the era of generative applications. Built on top of
generative AI capabilities, Agentic AI represents a major shift toward
autonomous, goal-driven systems that can reason, retrieve, and act. However,
they also inherit the bias present in both internal and external information
sources. This significantly affects the fairness and balance of retrieved
information, and hence reduces user trust. To address this critical challenge,
we introduce a novel Bias Mitigation Agent, a multi-agent system designed to
orchestrate the workflow of bias mitigation through specialized agents that
optimize the selection of sources to ensure that the retrieved content is both
highly relevant and minimally biased to promote fair and balanced knowledge
dissemination. The experimental results demonstrate an 81.82\% reduction in
bias compared to a baseline naive retrieval strategy.

</details>


### [46] [CAC-CoT: Connector-Aware Compact Chain-of-Thought for Efficient Reasoning Data Synthesis Across Dual-System Cognitive Tasks](https://arxiv.org/abs/2508.18743)
*Sunguk Choi,Yonghoon Kwon,Heondeuk Lee*

Main category: cs.AI

TL;DR: CAC-CoT是一种使用有限连接词短语的紧凑思维链方法，在保持System-1任务性能的同时，显著缩短推理长度并提升System-2任务表现


<details>
  <summary>Details</summary>
Motivation: 解决长思维链在快速直觉性System-1任务上导致性能下降的问题，同时保持复杂System-2任务的解决能力

Method: 使用连接词感知的紧凑思维链(CAC-CoT)，限制推理过程到固定的连接词短语集合，引导模型生成简洁结构化的解释

Result: 在GSM8K上达到约85%，GPQA上约40%，同时保持S1-Bench约90%的性能；推理痕迹平均约300个token，比基线缩短约三分之二

Conclusion: CAC-CoT方法通过限制连接词实现了高效且准确的推理，在保持System-1任务性能的同时显著提升了System-2任务的解决能力

Abstract: Long chain-of-thought (CoT) prompting helps Large Language Models (LLMs)
solve difficult problems, but very long traces often slow or even degrade
performance on fast, intuitive "System-1" tasks. We introduce Connector-Aware
Compact CoT (CAC-CoT) -- a method that deliberately restricts reasoning to a
small, fixed set of connector phrases, steering the model toward concise and
well -- structured explanations. Despite its simplicity, our synthetic method
with Gemini-2.0-Flash yields a high-quality training quality. CAC-CoT achieves
approximately 85% on GSM8K and approximately 40% on GPQA (System-2) while
retaining approximately 90% on S1-Bench (System-1). Its reasoning traces
average approximately 300 tokens(ART), about one-third the length of baseline
traces, delivering higher efficiency without loss of accuracy.

</details>


### [47] [Reflection-Enhanced Meta-Optimization Integrating TextGrad-style Prompt Optimization with Memory-Driven Self-Evolution](https://arxiv.org/abs/2508.18749)
*Chunlong Wu,Zhibo Qu*

Main category: cs.AI

TL;DR: REMO是一个新颖的提示优化框架，通过记忆增强的反思RAG模块和自适应优化器，解决了现有方法缺乏历史经验积累和容易过拟合的问题，在数学推理任务上实现了更稳定和鲁棒的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 当前提示优化方法如TextGrad存在两个主要问题：1）无状态运行，缺乏历史优化经验的保存和利用机制；2）容易过拟合，泛化性能差。需要一种能够积累跨运行优化知识并支持持续改进的方法。

Method: 提出了REMO框架，包含两个核心组件：1）记忆增强的反思RAG模块（"错误笔记本"结构）；2）自适应性优化器（基于LLM的元控制器），通过合成epoch级别的反思见解来迭代改进系统级提示策略。使用Qwen3-32B模型在标准推理模式下实现。

Result: 在GSM8K数学推理基准测试中，与TextGrad基线相比，REMO实现了更稳定和鲁棒的泛化性能，但计算开销有所增加。通过详细的算法设计、优化动态的定性和定量分析以及全面的消融研究验证了各组件贡献。

Conclusion: REMO框架成功解决了现有提示优化方法的局限性，通过系统化的经验积累和重用机制，支持持续的性能改进，为提示优化领域提供了新的研究方向。

Abstract: Recent advances in prompt optimization, exemplified by methods such as
TextGrad, enable automatic, gradient-like refinement of textual prompts to
enhance the performance of large language models (LLMs) on specific downstream
tasks. However, current approaches are typically stateless and operate
independently across optimization runs, lacking mechanisms to preserve and
leverage historical optimization experience. Furthermore, they are susceptible
to overfitting, often yielding prompt updates that generalize poorly beyond the
immediate task context.
  To address these limitations, we propose Reflection-Enhanced
Meta-Optimization (REMO), a novel framework that integrates (1) a
memory-augmented Reflection Retrieval-Augmented Generation (RAG) module -
structured as a "mistake notebook" and (2) a Self-Adaptive Optimizer,
implemented via an LLM-driven meta-controller that synthesizes epoch-level
reflective insights to iteratively improve system-level prompting strategies.
This architecture enables not only local, fine-grained prompt tuning akin to
TextGrad, but also the systematic accumulation and reuse of cross-run
optimization knowledge, thereby supporting continual improvement over time.
  We instantiate the REMO framework using Qwen3-32B in standard inference mode
- without explicit chain-of-thought prompting - and evaluate its efficacy on
the GSM8K benchmark for mathematical reasoning. Experimental results
demonstrate that, compared to a TextGrad baseline, REMO achieves more stable
and robust generalization, albeit at the cost of increased computational
overhead. We provide a detailed exposition of the algorithmic design, conduct a
qualitative and quantitative analysis of optimization dynamics, and present a
comprehensive ablation study to elucidate the contributions of each component.

</details>


### [48] [Stabilizing Open-Set Test-Time Adaptation via Primary-Auxiliary Filtering and Knowledge-Integrated Prediction](https://arxiv.org/abs/2508.18751)
*Byung-Joon Lee,Jin-Seop Lee,Jee-Hyong Lee*

Main category: cs.AI

TL;DR: 本文提出PAF-KIP方法解决开放集测试时适应(OSTTA)问题，通过主-辅助过滤机制和知识集成预测，在保持闭集精度的同时提升开放集识别能力


<details>
  <summary>Details</summary>
Motivation: 现有的开放集测试时适应方法依赖源模型进行过滤，在域偏移测试数据上过滤精度不佳，而自适应模型用于过滤会导致不稳定和错误累积

Method: 提出Primary-Auxiliary Filtering (PAF)使用辅助过滤器验证主过滤器结果，以及Knowledge-Integrated Prediction (KIP)集成自适应模型、EMA模型和源模型的互补知识

Result: 在多个闭集和开放集数据集上验证，方法在闭集精度和开放集区分能力上都优于现有方法

Conclusion: PAF-KIP方法有效解决了OSTTA中的过滤精度和知识集成问题，为实际应用中的域偏移和开放集数据提供了鲁棒的解决方案

Abstract: Deep neural networks demonstrate strong performance under aligned
training-test distributions. However, real-world test data often exhibit domain
shifts. Test-Time Adaptation (TTA) addresses this challenge by adapting the
model to test data during inference. While most TTA studies assume that the
training and test data share the same class set (closed-set TTA), real-world
scenarios often involve open-set data (open-set TTA), which can degrade
closed-set accuracy. A recent study showed that identifying open-set data
during adaptation and maximizing its entropy is an effective solution. However,
the previous method relies on the source model for filtering, resulting in
suboptimal filtering accuracy on domain-shifted test data. In contrast, we
found that the adapting model, which learns domain knowledge from noisy test
streams, tends to be unstable and leads to error accumulation when used for
filtering. To address this problem, we propose Primary-Auxiliary Filtering
(PAF), which employs an auxiliary filter to validate data filtered by the
primary filter. Furthermore, we propose Knowledge-Integrated Prediction (KIP),
which calibrates the outputs of the adapting model, EMA model, and source model
to integrate their complementary knowledge for OSTTA. We validate our approach
across diverse closed-set and open-set datasets. Our method enhances both
closed-set accuracy and open-set discrimination over existing methods. The code
is available at https://github.com/powerpowe/PAF-KIP-OSTTA .

</details>


### [49] [Answering the Unanswerable Is to Err Knowingly: Analyzing and Mitigating Abstention Failures in Large Reasoning Models](https://arxiv.org/abs/2508.18760)
*Yi Liu,Xiangyu Liu,Zequn Sun,Wei Hu*

Main category: cs.AI

TL;DR: 大型推理模型在处理不可回答问题时存在拒绝回答能力不足的问题，本文通过认知监控和推理时干预的方法显著提升了模型的拒绝回答率


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在复杂推理任务上表现优异，但面对缺乏充分条件的不可回答问题时，往往无法提供适当的拒绝回答，这影响了AI的可信度

Method: 提出轻量级的两阶段方法：首先进行认知监控分析模型对问题的内部认知，然后通过推理时干预来调整模型的外部响应行为

Result: 实验结果表明该方法显著提高了模型的拒绝回答率，同时保持了整体的推理性能

Conclusion: 该方法有效解决了大型推理模型内部认知与外部响应之间的不对齐问题，提升了AI系统的可信度和可靠性

Abstract: Large reasoning models (LRMs) have shown remarkable progress on complex
reasoning tasks. However, some questions posed to LRMs are inherently
unanswerable, such as math problems lacking sufficient conditions. We find that
LRMs continually fail to provide appropriate abstentions when confronted with
these unanswerable questions. In this paper, we systematically analyze,
investigate, and resolve this issue for trustworthy AI. We first conduct a
detailed analysis of the distinct response behaviors of LRMs when facing
unanswerable questions. Then, we show that LRMs possess sufficient cognitive
capabilities to recognize the flaws in these questions. However, they fail to
exhibit appropriate abstention behavior, revealing a misalignment between their
internal cognition and external response. Finally, to resolve this issue, we
propose a lightweight, two-stage method that combines cognitive monitoring with
inference-time intervention. Experimental results demonstrate that our method
significantly improves the abstention rate while maintaining the overall
reasoning performance.

</details>


### [50] [Dynamic Collaboration of Multi-Language Models based on Minimal Complete Semantic Units](https://arxiv.org/abs/2508.18763)
*Chao Hao,Zezheng Wang,Yanhua Huang,Ruiwen Xu,Wenzhe Niu,Xin Liu,Zitong Yu*

Main category: cs.AI

TL;DR: 本文提出了一种基于分布距离的动态选择策略(DDS)，通过多模型在token级别的协作来增强语言模型的推理能力，并解决了词汇不对齐的关键问题。


<details>
  <summary>Details</summary>
Motivation: 现有的多模型协作方法通常假设模型越多效果越好，但实际上需要更智能的协作策略。同时，多模型协作面临词汇表不对齐的关键技术挑战。

Method: 提出了分布距离动态选择策略(DDS)来优化多模型协作过程，引入最小完整语义单元(MCSU)概念解决词汇不对齐问题，实现语言模型在语义空间中的自然对齐。

Result: 在多个基准测试上的实验结果表明该方法具有优越性，代码将在GitHub上开源。

Conclusion: 该方法通过智能的多模型token级协作有效提升了语言模型的推理能力，解决了多模型协作中的关键挑战。

Abstract: This paper investigates the enhancement of reasoning capabilities in language
models through token-level multi-model collaboration. Our approach selects the
optimal tokens from the next token distributions provided by multiple models to
perform autoregressive reasoning. Contrary to the assumption that more models
yield better results, we introduce a distribution distance-based dynamic
selection strategy (DDS) to optimize the multi-model collaboration process. To
address the critical challenge of vocabulary misalignment in multi-model
collaboration, we propose the concept of minimal complete semantic units
(MCSU), which is simple yet enables multiple language models to achieve natural
alignment within the linguistic space. Experimental results across various
benchmarks demonstrate the superiority of our method. The code will be
available at https://github.com/Fanye12/DDS.

</details>


### [51] [AniME: Adaptive Multi-Agent Planning for Long Animation Generation](https://arxiv.org/abs/2508.18781)
*Lisai Zhang,Baohan Xu,Siqian Yang,Mingyu Yin,Jing Liu,Chao Xu,Siqi Wang,Yidi Wu,Yuxin Hong,Zihao Zhang,Yanzhang Liang,Yudong Jiang*

Main category: cs.AI

TL;DR: AniME是一个导演导向的多智能体系统，用于自动化长篇幅动画制作，从故事到最终视频的全流程覆盖。


<details>
  <summary>Details</summary>
Motivation: 为了解决AI驱动动画创作中的角色一致性和音视频同步问题，提供可扩展的自动化动画制作解决方案。

Method: 采用导演智能体维护全局记忆并协调下游专业智能体，通过定制化模型上下文协议（MCP）与下游模型指令集成，使专业智能体能够自适应选择不同子任务的控制条件。

Result: 系统能够生成具有一致角色和同步音视频元素的电影级动画。

Conclusion: AniME为AI驱动的动画创作提供了一个可扩展的完整工作流解决方案。

Abstract: We present AniME, a director-oriented multi-agent system for automated
long-form anime production, covering the full workflow from a story to the
final video. The director agent keeps a global memory for the whole workflow,
and coordinates several downstream specialized agents. By integrating
customized Model Context Protocol (MCP) with downstream model instruction, the
specialized agent adaptively selects control conditions for diverse sub-tasks.
AniME produces cinematic animation with consistent characters and synchronized
audio visual elements, offering a scalable solution for AI-driven anime
creation.

</details>


### [52] [CausalMACE: Causality Empowered Multi-Agents in Minecraft Cooperative Tasks](https://arxiv.org/abs/2508.18797)
*Qi Chai,Zhang Zheng,Junlong Ren,Deheng Ye,Zichuan Lin,Hao Wang*

Main category: cs.AI

TL;DR: CausalMACE是一个基于因果关系的多智能体协作框架，通过任务图和因果干预来提升Minecraft中复杂任务的执行效率和容错能力


<details>
  <summary>Details</summary>
Motivation: 现有的单智能体方法在处理Minecraft中需要长序列动作的复杂任务时存在效率低下和容错能力有限的问题，而多智能体协作研究仍然稀缺

Method: 提出CausalMACE框架，包含两个模块：全局任务规划的任务图和基于因果关系的依赖管理模块，采用固有规则进行因果干预

Result: 实验结果表明该方法在Minecraft多智能体协作任务中达到了最先进的性能

Conclusion: 因果关系的引入有效提升了多智能体系统的协作效率，为复杂任务执行提供了新的解决方案

Abstract: Minecraft, as an open-world virtual interactive environment, has become a
prominent platform for research on agent decision-making and execution.
Existing works primarily adopt a single Large Language Model (LLM) agent to
complete various in-game tasks. However, for complex tasks requiring lengthy
sequences of actions, single-agent approaches often face challenges related to
inefficiency and limited fault tolerance. Despite these issues, research on
multi-agent collaboration remains scarce. In this paper, we propose CausalMACE,
a holistic causality planning framework designed to enhance multi-agent
systems, in which we incorporate causality to manage dependencies among
subtasks. Technically, our proposed framework introduces two modules: an
overarching task graph for global task planning and a causality-based module
for dependency management, where inherent rules are adopted to perform causal
intervention. Experimental results demonstrate our approach achieves
state-of-the-art performance in multi-agent cooperative tasks of Minecraft.

</details>


### [53] [STARec: An Efficient Agent Framework for Recommender Systems via Autonomous Deliberate Reasoning](https://arxiv.org/abs/2508.18812)
*Chenghao Wu,Ruiyang Ren,Junjie Zhang,Ruirui Wang,Zhongrui Ma,Qi Ye,Wayne Xin Zhao*

Main category: cs.AI

TL;DR: STARec是一个慢思考增强的推荐代理框架，通过双认知系统（快速响应+慢速推理）和锚定强化训练，显著提升推荐系统的性能，仅用0.4%训练数据就超越现有最佳方法


<details>
  <summary>Details</summary>
Motivation: 现有推荐系统受限于静态用户建模和被动决策范式，基于大语言模型的代理也存在过度依赖启发式模式匹配的问题，导致浅层相关偏差、有限因果推理和稀疏数据场景下的脆弱性

Method: 提出STARec框架：1）将用户建模为具有并行认知的代理（快速响应+慢速推理链）；2）开发锚定强化训练，结合结构化知识蒸馏和偏好对齐奖励塑造；3）通过模拟反馈循环实现动态策略适应

Result: 在MovieLens 1M和Amazon CDs基准测试中，STARec相比最先进基线方法实现了显著性能提升，尽管只使用了0.4%的完整训练数据

Conclusion: STARec通过引入慢思考推理能力和混合训练范式，成功解决了传统推荐系统的局限性，为推荐系统提供了自主深思熟虑的推理能力

Abstract: While modern recommender systems are instrumental in navigating information
abundance, they remain fundamentally limited by static user modeling and
reactive decision-making paradigms. Current large language model (LLM)-based
agents inherit these shortcomings through their overreliance on heuristic
pattern matching, yielding recommendations prone to shallow correlation bias,
limited causal inference, and brittleness in sparse-data scenarios. We
introduce STARec, a slow-thinking augmented agent framework that endows
recommender systems with autonomous deliberative reasoning capabilities. Each
user is modeled as an agent with parallel cognitions: fast response for
immediate interactions and slow reasoning that performs chain-of-thought
rationales. To cultivate intrinsic slow thinking, we develop anchored
reinforcement training - a two-stage paradigm combining structured knowledge
distillation from advanced reasoning models with preference-aligned reward
shaping. This hybrid approach scaffolds agents in acquiring foundational
capabilities (preference summarization, rationale generation) while enabling
dynamic policy adaptation through simulated feedback loops. Experiments on
MovieLens 1M and Amazon CDs benchmarks demonstrate that STARec achieves
substantial performance gains compared with state-of-the-art baselines, despite
using only 0.4% of the full training data.

</details>


### [54] [Judicial Requirements for Generative AI in Legal Reasoning](https://arxiv.org/abs/2508.18880)
*Eljas Linna,Tuula Linna*

Main category: cs.AI

TL;DR: 这篇论文分析了大语言模型在法律判决中的实际能力，采用IRAC模型定义了法律推理的核心要求，并评估了RAG、多代理系统等AI增强技术的潜力。结果显示当前AI在法律领域最合适的角色是处理简单案件的高效助手和复杂案件中的专家讨论对象。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型被集成到专业领域，对其在法律等高风险领域的实际限制仍然了解不深入。论文强调需要定义AI系统在司法决策中作为可靠推理工具所必须具备的核心能力。

Method: 采用IRAC（Issue-Rule-Application-Conclusion）模型作为分析框架，重点研究法律裁判中最具挑战性的阶段：确定适用规则（R）和将规则应用于案件事实（A）。从司法角度解构法律推理的一系列核心要求。

Result: 研究发现，虽然RAG、多代理系统、神经符号AI等增强技术能够解决特定挑战，但在需要自由裁量权和透明、可证明推理的任务中仍面临重大挑战。

Conclusion: 当前AI在法律领域最有效的角色是双重的：作为处理简单、重复性案件的高效助手，以及在复杂事务中作为人类专家的精巧"讨论对手"。

Abstract: Large Language Models (LLMs) are being integrated into professional domains,
yet their limitations in high-stakes fields like law remain poorly understood.
This paper defines the core capabilities that an AI system must possess to
function as a reliable reasoning tool in judicial decision-making. Using the
IRAC (Issue-Rule-Application-Conclusion) model as an analytical framework, the
study focuses on the most challenging phases of legal adjudication: determining
the applicable Rule (R) and performing the Application (A) of that rule to the
facts of a case. From a judicial perspective, the analysis deconstructs legal
reasoning into a series of core requirements, including the ability to select
the correct legal framework across jurisdictions, generate sound arguments
based on the doctrine of legal sources, distinguish ratio decidendi from obiter
dictum in case law, resolve ambiguity arising from general clauses like
"reasonableness", manage conflicting legal provisions, and correctly apply the
burden of proof. The paper then maps various AI enhancement mechanisms, such as
Retrieval-Augmented Generation (RAG), multi-agent systems, and neuro-symbolic
AI, to these requirements, assessing their potential to bridge the gap between
the probabilistic nature of LLMs and the rigorous, choice-driven demands of
legal interpretation. The findings indicate that while these techniques can
address specific challenges, significant challenges remain, particularly in
tasks requiring discretion and transparent, justifiable reasoning. Our paper
concludes that the most effective current role for AI in law is a dual one: as
a high-volume assistant for simple, repetitive cases and as a sophisticated
"sparring partner" for human experts in complex matters.

</details>


### [55] [Interactive Evaluation of Large Language Models for Multi-Requirement Software Engineering Tasks](https://arxiv.org/abs/2508.18905)
*Dimitrios Rontogiannis,Maxime Peyrard,Nicolas Baldwin,Martin Josifoski,Robert West,Dimitrios Gunopulos*

Main category: cs.AI

TL;DR: 一种新的交互式评估框架，通过结构化反馈导向对话来评估大语言模型在多要求编程任务中的能力，揭示了静态测试无法发现的系统性弱点。


<details>
  <summary>Details</summary>
Motivation: 标准的单轮静态测试无法全面评估LLM在复杂任务（如软件工程）中的细微能力，需要更动态、交互式的评测方法来揭示模型的真实表现。

Method: 建立一种交互式评估框架，将每个任务模型化为需求依赖图。由知道真实解决方案的"面试官"LLM向"面试者"模型提供有针对性的提示，帮助其纠正错误并满足目标约束。基于DevAI评测集的55个编程任务进行实验。

Result: 该动态协议能够提供细粒度的诊断见解，发现了静态测试无法测量的系统性弱点咄优势。专家注释评估证明了面试官提示的相关性咄实用性。

Conclusion: 动态评估对于推进协作式代码生成代理的发展至关重要，能够更全面地评估LLM在复杂编程任务中的能力。

Abstract: Standard single-turn, static benchmarks fall short in evaluating the nuanced
capabilities of Large Language Models (LLMs) on complex tasks such as software
engineering. In this work, we propose a novel interactive evaluation framework
that assesses LLMs on multi-requirement programming tasks through structured,
feedback-driven dialogue. Each task is modeled as a requirement dependency
graph, and an ``interviewer'' LLM, aware of the ground-truth solution, provides
minimal, targeted hints to an ``interviewee'' model to help correct errors and
fulfill target constraints. This dynamic protocol enables fine-grained
diagnostic insights into model behavior, uncovering strengths and systematic
weaknesses that static benchmarks fail to measure. We build on DevAI, a
benchmark of 55 curated programming tasks, by adding ground-truth solutions and
evaluating the relevance and utility of interviewer hints through expert
annotation. Our results highlight the importance of dynamic evaluation in
advancing the development of collaborative code-generating agents.

</details>


### [56] [FormaRL: Enhancing Autoformalization with no Labeled Data](https://arxiv.org/abs/2508.18914)
*Yanxing Huang,Xinling Jin,Sijie Liang,Peng Li,Yang Liu*

Main category: cs.AI

TL;DR: FormaRL是一个基于强化学习的自动形式化框架，仅需少量无标注数据，通过整合Lean编译器的语法检查和LLM的一致性检查来计算奖励，使用GRPO算法更新形式化器，在ProofNet和uproof数据集上显著提升了自动形式化准确率。


<details>
  <summary>Details</summary>
Motivation: 自动形式化是形式化验证的核心任务，但受限于数据稀缺和缺乏高效方法。为了解决这一问题，研究者提出了一个简单而高效的强化学习框架。

Method: 提出FormaRL强化学习框架，利用Lean编译器的语法检查和大型语言模型的一致性检查来计算奖励信号，采用GRPO算法训练形式化器。同时构建了uproof数学证明数据集。

Result: 实验显示FormaRL将Qwen2.5-Coder-7B-Instruct的pass@1准确率提升了4-6倍（ProofNet从4.04%到26.15%，uproof从2.4%到9.6%），仅使用859个无标注数据。在uproof数据集上，out-of-distribution性能也显著提升。

Conclusion: FormaRL是一个高效的自动形式化强化学习框架，仅需少量无标注数据就能显著提升性能，为高级数学领域的自动形式化和定理证明研究提供了有力工具。

Abstract: Autoformalization is one of the central tasks in formal verification, while
its advancement remains hindered due to the data scarcity and the absence
efficient methods. In this work we propose \textbf{FormaRL}, a simple yet
efficient reinforcement learning framework for autoformalization which only
requires a small amount of unlabeled data. FormaRL integrates syntax check from
Lean compiler and consistency check from large language model to calculate the
reward, and adopts GRPO algorithm to update the formalizer. We also curated a
proof problem dataset from undergraduate-level math materials, named
\textbf{uproof}, in the hope to facilitate the exploration of autoformalization
and theorem proving in advanced math. Experiments show that FormaRL can
increase the pass@1 autoformalization accuracy of Qwen2.5-Coder-7B-Instruct by
4 $\sim$ 6x (4.04\% $\to$ 26.15\% on ProofNet and 2.4\% $\to$ 9.6\% on uproof)
with merely 859 unlabeled data. And on uproof our method also achieved a strong
improvement in out-of-distribution performance compared to existing open-source
state-of-the-art autoformalizers on both pass@1 accuracy (6.2\% $\to$ 9.6\%)
and pass@16 accuracy (24.4\% $\to$ 33.6\%). Training code of FormaRL is
open-sourced at https://github.com/THUNLP-MT/FormaRL.

</details>


### [57] [Who Is Lagging Behind: Profiling Student Behaviors with Graph-Level Encoding in Curriculum-Based Online Learning Systems](https://arxiv.org/abs/2508.18925)
*Qian Xiao,Conn Breathnach,Ioana Ghergulescu,Conor O'Sullivan,Keith Johnston,Vincent Wade*

Main category: cs.AI

TL;DR: CTGraph是一种图级表示学习方法，用于自监督地分析学习者在智能辅导系统中的行为和表现，能够全面追踪学习路径并识别困难学生。


<details>
  <summary>Details</summary>
Motivation: 智能辅导系统在教育中的广泛应用可能无意中加剧成绩差距，需要通过学生画像来跟踪进度、识别困难学生并减少学生间的差异。

Method: 提出CTGraph图级表示学习方法，以自监督方式分析学习者的行为和表现，考虑内容覆盖、学习强度和概念熟练度等多个维度。

Result: 实验证明CTGraph能够提供学生学习旅程的全面视图，识别困难学生，并进行群体比较分析以确定学生何时何地遇到困难。

Conclusion: 该方法为教育工作者提供了对学生学习旅程的深入洞察，为更有针对性的干预措施铺平了道路。

Abstract: The surge in the adoption of Intelligent Tutoring Systems (ITSs) in
education, while being integral to curriculum-based learning, can inadvertently
exacerbate performance gaps. To address this problem, student profiling becomes
crucial for tracking progress, identifying struggling students, and alleviating
disparities among students. Such profiling requires measuring student behaviors
and performance across different aspects, such as content coverage, learning
intensity, and proficiency in different concepts within a learning topic.
  In this study, we introduce CTGraph, a graph-level representation learning
approach to profile learner behaviors and performance in a self-supervised
manner. Our experiments demonstrate that CTGraph can provide a holistic view of
student learning journeys, accounting for different aspects of student
behaviors and performance, as well as variations in their learning paths as
aligned to the curriculum structure. We also show that our approach can
identify struggling students and provide comparative analysis of diverse groups
to pinpoint when and where students are struggling. As such, our approach opens
more opportunities to empower educators with rich insights into student
learning journeys and paves the way for more targeted interventions.

</details>


### [58] [VISION: Robust and Interpretable Code Vulnerability Detection Leveraging Counterfactual Augmentation](https://arxiv.org/abs/2508.18933)
*David Egea,Barproda Halder,Sanghamitra Dutta*

Main category: cs.AI

TL;DR: VISION是一个统一的漏洞检测框架，通过生成反事实样本和针对性GNN训练来减少虚假相关性，显著提高了漏洞检测的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于GNN的漏洞检测方法受到训练数据不平衡和标签噪声的限制，容易学习到虚假的相关性，导致在真实数据上泛化能力差。

Method: 提出VISION框架：1）使用大型语言模型生成反事实样本；2）在具有相反标签的配对代码示例上进行针对性GNN训练；3）基于图的可解释性分析来识别关键代码语句。

Result: 在CWE-20漏洞检测中，整体准确率从51.8%提升到97.8%，配对对比准确率从4.5%提升到95.8%，最差组准确率从0.7%提升到85.5%。

Conclusion: VISION通过反事实训练有效减少了虚假学习，提高了漏洞检测的鲁棒性和可解释性，为构建可信的AI网络安全系统提供了新方法。

Abstract: Automated detection of vulnerabilities in source code is an essential
cybersecurity challenge, underpinning trust in digital systems and services.
Graph Neural Networks (GNNs) have emerged as a promising approach as they can
learn structural and logical code relationships in a data-driven manner.
However, their performance is severely constrained by training data imbalances
and label noise. GNNs often learn 'spurious' correlations from superficial code
similarities, producing detectors that fail to generalize well to unseen
real-world data. In this work, we propose a unified framework for robust and
interpretable vulnerability detection, called VISION, to mitigate spurious
correlations by systematically augmenting a counterfactual training dataset.
Counterfactuals are samples with minimal semantic modifications but opposite
labels. Our framework includes: (i) generating counterfactuals by prompting a
Large Language Model (LLM); (ii) targeted GNN training on paired code examples
with opposite labels; and (iii) graph-based interpretability to identify the
crucial code statements relevant for vulnerability predictions while ignoring
spurious ones. We find that VISION reduces spurious learning and enables more
robust, generalizable detection, improving overall accuracy (from 51.8% to
97.8%), pairwise contrast accuracy (from 4.5% to 95.8%), and worst-group
accuracy (from 0.7% to 85.5%) on the Common Weakness Enumeration (CWE)-20
vulnerability. We further demonstrate gains using proposed metrics: intra-class
attribution variance, inter-class attribution distance, and node score
dependency. We also release CWE-20-CFA, a benchmark of 27,556 functions (real
and counterfactual) from the high-impact CWE-20 category. Finally, VISION
advances transparent and trustworthy AI-based cybersecurity systems through
interactive visualization for human-in-the-loop analysis.

</details>


### [59] [Novel Approaches to Artificial Intelligence Development Based on the Nearest Neighbor Method](https://arxiv.org/abs/2508.18953)
*I. I. Priezzhev,D. A. Danko,A. V. Shubin*

Main category: cs.AI

TL;DR: 这篇论文提出了一种基于层次聚类结构的k近邻算法方案，用于克服神经网络的幻觉效应、高计算复杂度和恐怖遗忘等问题。


<details>
  <summary>Details</summary>
Motivation: 现代神经网络技术存在幻觉效应、计算复杂度高、微调成本高和恐怖遗忘等基本限制，影响在医疗、工业管理和科研等关键领域的应用。

Method: 采用基于k近邻算法的方法，通过Kohonen自组织地图构建树状数据结构来加速近邻搜索，减少计算负荷。

Result: 在手写数字识别和子帖翻译任务中验证有效性。在准确率上仅有轻微下降的情况下，近邻搜索时间比全局搜索减少了百倍。

Conclusion: 该方法具有透明性和可解释性，与人类认知机制紧密结合，在需要高可靠性和可解释结果的任务中展现出广泛应用潜力。

Abstract: Modern neural network technologies, including large language models, have
achieved remarkable success in various applied artificial intelligence
applications, however, they face a range of fundamental limitations. Among them
are hallucination effects, high computational complexity of training and
inference, costly fine-tuning, and catastrophic forgetting issues. These
limitations significantly hinder the use of neural networks in critical areas
such as medicine, industrial process management, and scientific research. This
article proposes an alternative approach based on the nearest neighbors method
with hierarchical clustering structures. Employing the k-nearest neighbors
algorithm significantly reduces or completely eliminates hallucination effects
while simplifying model expansion and fine-tuning without the need for
retraining the entire network. To overcome the high computational load of the
k-nearest neighbors method, the paper proposes using tree-like data structures
based on Kohonen self-organizing maps, thereby greatly accelerating nearest
neighbor searches. Tests conducted on handwritten digit recognition and simple
subtitle translation tasks confirmed the effectiveness of the proposed
approach. With only a slight reduction in accuracy, the nearest neighbor search
time was reduced hundreds of times compared to exhaustive search methods. The
proposed method features transparency and interpretability, closely aligns with
human cognitive mechanisms, and demonstrates potential for extensive use in
tasks requiring high reliability and explainable results.

</details>


### [60] [Enabling MoE on the Edge via Importance-Driven Expert Scheduling](https://arxiv.org/abs/2508.18983)
*Guoying Zhu,Meng Li,Haipeng Dai,Xuechen Liu,Weijun Wang,Keran Li,Jun xiao,Ligeng Chen,Wei Wang*

Main category: cs.AI

TL;DR: 提出了一种基于专家重要性的动态卸载方法，通过替换低重要性专家为GPU内存中已缓存的相似专家，减少内存使用和数据传输，同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 在消费级边缘硬件上部署混合专家模型时，设备内存有限，需要高效的动态专家卸载方案。

Method: 利用专家重要性指导卸载决策，用GPU内存中已缓存的相似专家替换低重要性激活专家，并引入最大化GPU缓存专家重用率的调度策略。

Result: 实现了48%的解码延迟降低，超过60%的专家缓存命中率，同时保持几乎无损的准确性。

Conclusion: 该方法有效解决了MoE模型在边缘设备上的内存约束问题，显著提升了部署效率。

Abstract: The Mixture of Experts (MoE) architecture has emerged as a key technique for
scaling Large Language Models by activating only a subset of experts per query.
Deploying MoE on consumer-grade edge hardware, however, is constrained by
limited device memory, making dynamic expert offloading essential. Unlike prior
work that treats offloading purely as a scheduling problem, we leverage expert
importance to guide decisions, substituting low-importance activated experts
with functionally similar ones already cached in GPU memory, thereby preserving
accuracy. As a result, this design reduces memory usage and data transfer,
while largely eliminating PCIe overhead. In addition, we introduce a scheduling
policy that maximizes the reuse ratio of GPU-cached experts, further boosting
efficiency. Extensive evaluations show that our approach delivers 48% lower
decoding latency with over 60% expert cache hit rate, while maintaining nearly
lossless accuracy.

</details>


### [61] [AI Models Exceed Individual Human Accuracy in Predicting Everyday Social Norms](https://arxiv.org/abs/2508.19004)
*Pontus Strimling,Simon Karlsson,Irina Vartanova,Kimmo Eriksson*

Main category: cs.AI

TL;DR: 大型语言模型通过纯统计学习就能超越大多数人类在预测社会规范判断方面的表现，挑战了需要具身经验才能获得文化能力的理论


<details>
  <summary>Details</summary>
Motivation: 研究社会规范如何被获取和表征，探索语言模型是否仅通过统计学习就能达到复杂的社会规范理解

Method: 通过两个研究系统评估多个AI系统预测555个日常场景中人类社会适宜性判断的能力，比较模型预测与人类平均判断的接近程度

Result: GPT-4.5在连续尺度上预测集体判断的准确性超过所有人类参与者（第100百分位），其他模型也表现优异，但都存在系统性的相关错误

Conclusion: 统计学习足以产生复杂的社会认知模型，语言作为文化知识传播的丰富储存库，但基于模式的社会理解存在潜在边界

Abstract: A fundamental question in cognitive science concerns how social norms are
acquired and represented. While humans typically learn norms through embodied
social experience, we investigated whether large language models can achieve
sophisticated norm understanding through statistical learning alone. Across two
studies, we systematically evaluated multiple AI systems' ability to predict
human social appropriateness judgments for 555 everyday scenarios by examining
how closely they predicted the average judgment compared to each human
participant. In Study 1, GPT-4.5's accuracy in predicting the collective
judgment on a continuous scale exceeded that of every human participant (100th
percentile). Study 2 replicated this, with Gemini 2.5 Pro outperforming 98.7%
of humans, GPT-5 97.8%, and Claude Sonnet 4 96.0%. Despite this predictive
power, all models showed systematic, correlated errors. These findings
demonstrate that sophisticated models of social cognition can emerge from
statistical learning over linguistic data alone, challenging strong versions of
theories emphasizing the exclusive necessity of embodied experience for
cultural competence. The systematic nature of AI limitations across different
architectures indicates potential boundaries of pattern-based social
understanding, while the models' ability to outperform nearly all individual
humans in this predictive task suggests that language serves as a remarkably
rich repository for cultural knowledge transmission.

</details>


### [62] [Building Self-Evolving Agents via Experience-Driven Lifelong Learning: A Framework and Benchmark](https://arxiv.org/abs/2508.19005)
*Yuxuan Cai,Yipeng Hao,Jie Zhou,Hang Yan,Zhikai Lei,Rui Zhen,Zhenhua Han,Yutao Yang,Junsong Li,Qianjun Pan,Tianyu Huai,Qin Chen,Xin Li,Kai Chen,Bo Zhang,Xipeng Qiu,Liang He*

Main category: cs.AI

TL;DR: 本文提出了经验驱动的终身学习(ELL)框架，包含四个核心原则：经验探索、长期记忆、技能学习和知识内化，并推出了StuLife基准数据集来模拟学生大学生活，用于评估终身学习能力。


<details>
  <summary>Details</summary>
Motivation: 随着AI向通用智能发展，需要从静态任务优化转向创建能够持续学习的开放智能体，构建能够通过真实世界交互实现持续进化的自进化智能体。

Method: 提出ELL框架，基于四个核心原则：经验探索（通过自激励交互学习）、长期记忆（构建持久记忆系统）、技能学习（从经验中抽象可重用技能）、知识内化（将显性经验转化为隐性能力）。同时开发StuLife基准数据集，模拟学生大学生活的三阶段十场景。

Result: 开发了StuLife基准数据集，为评估终身学习能力提供了综合平台，包括记忆保持、技能迁移和自激励行为等评估维度。

Conclusion: ELL框架为构建持续学习的开放智能体提供了系统方法，StuLife基准为评估终身学习能力提供了有效工具，同时探索了上下文工程在推进AGI发展中的作用。

Abstract: As AI advances toward general intelligence, the focus is shifting from
systems optimized for static tasks to creating open-ended agents that learn
continuously. In this paper, we introduce Experience-driven Lifelong Learning
(ELL), a framework for building self-evolving agents capable of continuous
growth through real-world interaction. The framework is built on four core
principles: (1) Experience Exploration: Agents learn through continuous,
self-motivated interaction with dynamic environments, navigating interdependent
tasks and generating rich experiential trajectories. (2) Long-term Memory:
Agents preserve and structure historical knowledge, including personal
experiences, domain expertise, and commonsense reasoning, into a persistent
memory system. (3) Skill Learning: Agents autonomously improve by abstracting
recurring patterns from experience into reusable skills, which are actively
refined and validated for application in new tasks. (4) Knowledge
Internalization: Agents internalize explicit and discrete experiences into
implicit and intuitive capabilities as "second nature".
  We also introduce StuLife, a benchmark dataset for ELL that simulates a
student's holistic college journey, from enrollment to academic and personal
development, across three core phases and ten detailed sub-scenarios. StuLife
is designed around three key paradigm shifts: From Passive to Proactive, From
Context to Memory, and From Imitation to Learning. In this dynamic environment,
agents must acquire and distill practical skills and maintain persistent memory
to make decisions based on evolving state variables. StuLife provides a
comprehensive platform for evaluating lifelong learning capabilities, including
memory retention, skill transfer, and self-motivated behavior. Beyond
evaluating SOTA LLMs on the StuLife benchmark, we also explore the role of
context engineering in advancing AGI.

</details>


### [63] [Sense of Self and Time in Borderline Personality. A Comparative Robustness Study with Generative AI](https://arxiv.org/abs/2508.19008)
*Marcin Moskalewicz,Anna Sterna,Marek Pokropski,Paula Flores*

Main category: cs.AI

TL;DR: 本研究评估了三种大型语言模型（GPT-4o、Gemini 2.5 Pro、Claude Opus 4）在边缘型人格障碍现象学定性分析中的表现，发现Gemini最接近人类分析结果，有效性评分显著高于其他模型。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型在现象学定性分析中的能力，特别是针对边缘型人格障碍这种时间和自我障碍的疾病，旨在评估AI辅助分析在减轻人类解释偏见方面的潜力。

Method: 基于24名住院患者的生活故事访谈数据，让三种LLM模仿原始研究者的解释风格进行分析，采用盲法和非盲法专家评估，包括语义一致性、Jaccard系数和多维有效性评分。

Result: 模型与人类分析的重合度从0%（GPT）到58%（Gemini）不等，Jaccard系数较低（0.21-0.28），但模型发现了人类遗漏的主题。Gemini的输出最接近人类分析，有效性评分显著更高，且被盲法专家判断为人类分析。

Conclusion: AI增强的主题分析在现象学研究中具有潜力，能够发现人类可能遗漏的主题，但模型性能存在显著差异，输出质量与文本量和每主题词数密切相关。

Abstract: This study examines the capacity of large language models (LLMs) to support
phenomenological qualitative analysis of first-person experience in Borderline
Personality Disorder (BPD), understood as a disorder of temporality and
selfhood. Building on a prior human-led thematic analysis of 24 inpatients'
life-story interviews, we compared three LLMs (OpenAI GPT-4o, Google Gemini 2.5
Pro, Anthropic Claude Opus 4) prompted to mimic the interpretative style of the
original investigators. The models were evaluated with blinded and non-blinded
expert judges in phenomenology and clinical psychology. Assessments included
semantic congruence, Jaccard coefficients, and multidimensional validity
ratings (credibility, coherence, substantiveness, and groundness in data).
Results showed variable overlap with the human analysis, from 0 percent in GPT
to 42 percent in Claude and 58 percent in Gemini, and a low Jaccard coefficient
(0.21-0.28). However, the models recovered themes omitted by humans. Gemini's
output most closely resembled the human analysis, with validity scores
significantly higher than GPT and Claude (p < 0.0001), and was judged as human
by blinded experts. All scores strongly correlated (R > 0.78) with the quantity
of text and words per theme, highlighting both the variability and potential of
AI-augmented thematic analysis to mitigate human interpretative bias.

</details>


### [64] [MAB Optimizer for Estimating Math Question Difficulty via Inverse CV without NLP](https://arxiv.org/abs/2508.19014)
*Surajit Das,Gourav Roy,Aleksei Eliseev,Ram Kumar Rajendran*

Main category: cs.AI

TL;DR: 提出基于强化学习的多臂老虎机框架APME，仅使用解题表现数据（分数和时间）来估计题目难度，无需语言特征或专家标注，在符号领域表现优异


<details>
  <summary>Details</summary>
Motivation: 传统人工标注主观性强，现有NLP方法在代数等符号领域失效，需要客观且领域无关的题目难度确定方法

Method: 使用强化学习多臂老虎机框架，利用逆变异系数作为风险调整指标，仅基于解题表现数据（得分和时间）进行难度估计

Result: 在三个异构数据集上验证，平均R2达0.9213，RMSE为0.0584，优于回归、NLP和IRT等基线方法，在符号领域表现尤其突出

Conclusion: 该方法领域无关、自监督，可推广到其他有解题交互数据的领域，支持维果茨基最近发展区理论，平衡挑战性与可达性

Abstract: The evolution of technology and education is driving the emergence of
Intelligent & Autonomous Tutoring Systems (IATS), where objective and
domain-agnostic methods for determining question difficulty are essential.
Traditional human labeling is subjective, and existing NLP-based approaches
fail in symbolic domains like algebra. This study introduces the Approach of
Passive Measures among Educands (APME), a reinforcement learning-based
Multi-Armed Bandit (MAB) framework that estimates difficulty solely from solver
performance data -- marks obtained and time taken -- without requiring
linguistic features or expert labels. By leveraging the inverse coefficient of
variation as a risk-adjusted metric, the model provides an explainable and
scalable mechanism for adaptive assessment. Empirical validation was conducted
on three heterogeneous datasets. Across these diverse contexts, the model
achieved an average R2 of 0.9213 and an average RMSE of 0.0584, confirming its
robustness, accuracy, and adaptability to different educational levels and
assessment formats. Compared with baseline approaches-such as regression-based,
NLP-driven, and IRT models-the proposed framework consistently outperformed
alternatives, particularly in purely symbolic domains. The findings highlight
that (i) item heterogeneity strongly influences perceived difficulty, and (ii)
variance in solver outcomes is as critical as mean performance for adaptive
allocation. Pedagogically, the model aligns with Vygotskys Zone of Proximal
Development by identifying tasks that balance challenge and attainability,
supporting motivation while minimizing disengagement. This domain-agnostic,
self-supervised approach advances difficulty tagging in IATS and can be
extended beyond algebra wherever solver interaction data is available

</details>


### [65] [Investigating Advanced Reasoning of Large Language Models via Black-Box Interaction](https://arxiv.org/abs/2508.19035)
*Congchi Yin,Tianyi Wu,Yankai Shu,Alex Gu,Yunhan Wang,Jun Shao,Xun Jiang,Piji Li*

Main category: cs.AI

TL;DR: 提出了黑盒交互评估范式来测试LLM在未知环境中的综合推理能力，构建了Oracle基准包含6类96个黑盒任务，发现现有模型在复杂任务中规划能力不足


<details>
  <summary>Details</summary>
Motivation: 现有评估任务无法全面测试LLM在交互式未知环境中的综合推理能力，缺乏对人类真实发现过程中整合推理过程的评估

Method: 引入黑盒交互评估范式，定义隐藏函数映射的黑盒，要求LLM通过有限交互轮次观察输入输出对来推理隐藏函数，并构建Oracle基准包含6类任务

Result: 测试19个现代LLM，o3在6个任务中5个排名第一，简单黑盒准确率超70%，但困难任务平均性能低于40%，显示模型缺乏高效自适应探索策略的规划能力

Conclusion: 黑盒交互是评估LLM综合推理能力的有效范式，揭示了当前模型在高级规划能力和自适应假设精炼策略方面的普遍不足

Abstract: Existing tasks fall short in evaluating reasoning ability of Large Language
Models (LLMs) in an interactive, unknown environment. This deficiency leads to
the isolated assessment of deductive, inductive, and abductive reasoning,
neglecting the integrated reasoning process that is indispensable for humans
discovery of real world. We introduce a novel evaluation paradigm,
\textit{black-box interaction}, to tackle this challenge. A black-box is
defined by a hidden function that maps a specific set of inputs to outputs.
LLMs are required to unravel the hidden function behind the black-box by
interacting with it in given exploration turns, and reasoning over observed
input-output pairs. Leveraging this idea, we build the \textsc{Oracle}
benchmark which comprises 6 types of black-box task and 96 black-boxes. 19
modern LLMs are benchmarked. o3 ranks first in 5 of the 6 tasks, achieving over
70\% accuracy on most easy black-boxes. But it still struggles with some hard
black-box tasks, where its average performance drops below 40\%. Further
analysis indicates a universal difficulty among LLMs: They lack the high-level
planning capability to develop efficient and adaptive exploration strategies
for hypothesis refinement.

</details>


### [66] [A Concurrent Modular Agent: Framework for Autonomous LLM Agents](https://arxiv.org/abs/2508.19042)
*Norihiro Maruyama,Takahide Yoshida,Hiroki Sato,Atsushi Masumori,Johnsmith,Takashi Ikegami*

Main category: cs.AI

TL;DR: 并发模块化机器人框架(CMA)，通过多个并发执行的LLM模块和语言介导的模块间交互，实现了灵活、适应性强的机器人行为循环，是Minsky智能社会理论的实践实现。


<details>
  <summary>Details</summary>
Motivation: 解决传统机器人架构在异步运行中维持行为一致性和故障容错的长期困难，让意图从自治过程的语言交互中涵生。

Method: 构建多个并发执行的LLM基础模块，通过模块间通信和单一全局状态共享来实现协调行为，将推理任务委托给LLM。

Result: 通过两个实际用例验证了系统的可行性，观察到了涵生性质，支持了复杂认知现象可能来自简单过程组织交互的理论。

Conclusion: 该框架为Minsky智能社会理论提供了实践实现，为人工智能研究开启了新方向，澄清代码已开源。

Abstract: We introduce the Concurrent Modular Agent (CMA), a framework that
orchestrates multiple Large-Language-Model (LLM)-based modules that operate
fully asynchronously yet maintain a coherent and fault-tolerant behavioral
loop. This framework addresses long-standing difficulties in agent
architectures by letting intention emerge from language-mediated interactions
among autonomous processes. This approach enables flexible, adaptive, and
context-dependent behavior through the combination of concurrently executed
modules that offload reasoning to an LLM, inter-module communication, and a
single shared global state.We consider this approach to be a practical
realization of Minsky's Society of Mind theory. We demonstrate the viability of
our system through two practical use-case studies. The emergent properties
observed in our system suggest that complex cognitive phenomena like
self-awareness may indeed arise from the organized interaction of simpler
processes, supporting Minsky-Society of Mind concept and opening new avenues
for artificial intelligence research. The source code for our work is available
at: https://github.com/AlternativeMachine/concurrent-modular-agent.

</details>


### [67] [Can Structured Templates Facilitate LLMs in Tackling Harder Tasks? : An Exploration of Scaling Laws by Difficulty](https://arxiv.org/abs/2508.19069)
*Zhichao Yang,Zhaoxin Fan,Gen Li,Yuanze Hu,Xinyu Wang,Ye Qiu,Xin Wang,Yifan Sun,Wenjun Wu*

Main category: cs.AI

TL;DR: 论文发现LLM在数学推理中存在难度缩放定律，提出SST框架通过结构化解决方案模板和难度课程来显式教授程序推理，显著提升了复杂数学问题的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有后训练方法在捕捉复杂任务的深层程序逻辑方面仍有不足，研究发现模型性能随训练数据难度呈现U型曲线规律，过度简单数据阻碍抽象能力，而高难度数据显著增强推理能力。

Method: 提出结构化解决方案模板(SST)框架：1)使用结构化解决方案模板链和动态加权损失进行微调；2)推理时注入解决方案模板作为认知支架；3)集成课程微调，显式教授模型自我规划-执行-自我纠正。

Result: 在GSM8K、AIME24和新的Dynamic En基准测试中，SST显著提高了准确性和效率，特别是在更难的问题上表现突出。

Conclusion: SST框架通过结构化模板和难度课程有效解决了LLM在程序推理方面的局限性，为提升复杂数学问题的推理能力提供了有效解决方案。

Abstract: Structured, procedural reasoning is essential for Large Language Models
(LLMs), especially in mathematics. While post-training methods have improved
LLM performance, they still fall short in capturing deep procedural logic on
complex tasks. To tackle the issue, in this paper, we first investigate this
limitation and uncover a novel finding: a Scaling Law by Difficulty, which
reveals that model performance follows a U-shaped curve with respect to
training data complexity -- excessive low-difficulty data impedes abstraction,
while high-difficulty data significantly enhances reasoning ability. Motivated
by this, we propose the Structured Solution Template (SST) framework, which
uses solution templates and a curriculum of varied difficulty to explicitly
teach procedural reasoning. Specifically, SST comprises (1) fine-tuning with
structured solution-template chains and dynamically weighted loss to prioritize
procedural logic, (2) prompt-time injection of solution templates as cognitive
scaffolds to guide inference, and (3) integrated curriculum fine-tuning that
explicitly teaches the model to self-plan - execute - self-correct. Experiments
on GSM8K, AIME24, and new Dynamic En benchmark show that SST significantly
improves both accuracy and efficiency, especially on harder problems.

</details>


### [68] [Trustworthy Agents for Electronic Health Records through Confidence Estimation](https://arxiv.org/abs/2508.19096)
*Yongwoo Song,Minbyul Jeong,Mujeen Sung*

Main category: cs.AI

TL;DR: 提出HCAcc@k%指标和TrustEHRAgent系统，通过步骤信心估计提升临床问答的准确性和可靠性，在严格可靠性要求下显著超过基线方法


<details>
  <summary>Details</summary>
Motivation: 大语言模型在电子健康记录中存在幻觉风险，传统准确率指标无法评估医疗AI系统的可靠性交易

Method: 设计Hallucination Controlled Accuracy at k% (HCAcc@k%)指标，开发TrustEHRAgent系统，采用步骤信心估计机制

Result: 在MIMIC-III和eICU数据集上，在HCAcc@70%时分别获得44.23%和25.34%的显著提升，基线方法在这些阈值下失败

Conclusion: 该研究为开发可信赖的临床助手做出贡献，能够在信心低时透明地表达不确定性，弥补传统评估指标的局限性

Abstract: Large language models (LLMs) show promise for extracting information from
Electronic Health Records (EHR) and supporting clinical decisions. However,
deployment in clinical settings faces challenges due to hallucination risks. We
propose Hallucination Controlled Accuracy at k% (HCAcc@k%), a novel metric
quantifying the accuracy-reliability trade-off at varying confidence
thresholds. We introduce TrustEHRAgent, a confidence-aware agent incorporating
stepwise confidence estimation for clinical question answering. Experiments on
MIMIC-III and eICU datasets show TrustEHRAgent outperforms baselines under
strict reliability constraints, achieving improvements of 44.23%p and 25.34%p
at HCAcc@70% while baseline methods fail at these thresholds. These results
highlight limitations of traditional accuracy metrics in evaluating healthcare
AI agents. Our work contributes to developing trustworthy clinical agents that
deliver accurate information or transparently express uncertainty when
confidence is low.

</details>


### [69] [Reasoning LLMs in the Medical Domain: A Literature Survey](https://arxiv.org/abs/2508.19097)
*Armin Berger,Sarthak Khanna,David Berghaus,Rafet Sifa*

Main category: cs.AI

TL;DR: 本综述探讨了大型语言模型在医疗领域的推理能力发展，从基础信息检索工具转变为支持复杂医疗决策的临床推理系统，重点分析了技术基础、评估方法和当前挑战。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型推理能力的提升，医疗应用需要更透明和可解释的决策支持系统，这对医疗环境至关重要。本文旨在系统分析医疗LLMs的发展现状和技术突破。

Method: 通过全面调查分析医疗LLMs的技术基础，包括专门的提示工程技术（如思维链）和强化学习突破（如DeepSeek-R1），评估专用医疗框架和多智能体协作系统等新兴范式。

Result: 识别了医疗LLMs从信息检索到临床推理的转型路径，分析了当前评估方法的局限性，并指出了领域解释限制、偏见缓解、患者安全框架和多模态数据集成等持续挑战。

Conclusion: 本文为开发可靠的医疗LLMs建立了路线图，使其能够成为临床实践和医学研究的有效合作伙伴，推动医疗AI向更透明、可解释的方向发展。

Abstract: The emergence of advanced reasoning capabilities in Large Language Models
(LLMs) marks a transformative development in healthcare applications. Beyond
merely expanding functional capabilities, these reasoning mechanisms enhance
decision transparency and explainability-critical requirements in medical
contexts. This survey examines the transformation of medical LLMs from basic
information retrieval tools to sophisticated clinical reasoning systems capable
of supporting complex healthcare decisions. We provide a thorough analysis of
the enabling technological foundations, with a particular focus on specialized
prompting techniques like Chain-of-Thought and recent breakthroughs in
Reinforcement Learning exemplified by DeepSeek-R1. Our investigation evaluates
purpose-built medical frameworks while also examining emerging paradigms such
as multi-agent collaborative systems and innovative prompting architectures.
The survey critically assesses current evaluation methodologies for medical
validation and addresses persistent challenges in field interpretation
limitations, bias mitigation strategies, patient safety frameworks, and
integration of multimodal clinical data. Through this survey, we seek to
establish a roadmap for developing reliable LLMs that can serve as effective
partners in clinical practice and medical research.

</details>


### [70] [Hybrid Deep Searcher: Integrating Parallel and Sequential Search Reasoning](https://arxiv.org/abs/2508.19113)
*Dayoon Ko,Jihyuk Kim,Haeju Park,Sohyeon Kim,Dahyun Lee,Yongrae Jo,Gunhee Kim,Moontae Lee,Kyungjae Lee*

Main category: cs.AI

TL;DR: HDS-QA是一个从自然问题自动生成的合成数据集，用于训练大型推理模型区分并行查询和顺序查询，从而提高推理效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的顺序查询方法增加了推理延迟和上下文长度，降低了连贯性和准确性，需要一种混合并行和顺序查询的方法来提高效率。

Method: 创建HDS-QA合成数据集，包含并行可执行和顺序依赖的子查询，并基于此微调LRM模型HybridDeepSearcher。

Result: HybridDeepSearcher在多个基准测试中优于最先进基线，在FanOutQA和BrowseComp上分别获得+15.9和+11.5 F1分数，显著减少推理延迟。

Conclusion: 显式训练LRM利用混合并行和顺序查询的方法在效率、可扩展性和有效性方面表现出色。

Abstract: Large reasoning models (LRMs) have demonstrated strong performance in
complex, multi-step reasoning tasks. Existing methods enhance LRMs by
sequentially integrating external knowledge retrieval; models iteratively
generate queries, retrieve external information, and progressively reason over
this information. However, purely sequential querying increases inference
latency and context length, diminishing coherence and potentially reducing
accuracy. To address these limitations, we introduce HDS-QA (Hybrid Deep Search
QA), a synthetic dataset automatically generated from Natural Questions,
explicitly designed to train LRMs to distinguish parallelizable from sequential
queries. HDS-QA comprises hybrid-hop questions that combine parallelizable
independent subqueries (executable simultaneously) and sequentially dependent
subqueries (requiring step-by-step resolution), along with synthetic
reasoning-querying-retrieval paths involving parallel queries. We fine-tune an
LRM using HDS-QA, naming the model HybridDeepSearcher, which outperforms
state-of-the-art baselines across multiple benchmarks, notably achieving +15.9
and +11.5 F1 on FanOutQA and a subset of BrowseComp, respectively, both
requiring comprehensive and exhaustive search. Experimental results highlight
two key advantages: HybridDeepSearcher reaches comparable accuracy with fewer
search turns, significantly reducing inference latency, and it effectively
scales as more turns are permitted. These results demonstrate the efficiency,
scalability, and effectiveness of explicitly training LRMs to leverage hybrid
parallel and sequential querying.

</details>


### [71] [Algorithmic Collective Action with Multiple Collectives](https://arxiv.org/abs/2508.19149)
*Claudio Battiloro,Pietro Greiner,Bret Nestor,Oumaima Amezgar,Francesca Dominici*

Main category: cs.AI

TL;DR: 本文提出了首个多集体算法集体行动（ACA）的理论框架，研究多个集体如何在分类系统中协调植入信号来影响分类器决策。


<details>
  <summary>Details</summary>
Motivation: 随着学习系统对日常决策影响日益增强，用户端的算法集体行动（ACA）作为监管端政策和企业端模型设计的补充手段变得重要。传统ACA研究多集中于单集体场景，但现实中的集体行动往往是分散的、由多个目标一致但策略不同的集体组成。

Method: 建立多集体ACA的理论框架，研究多个集体如何在分类任务中通过协调改变共享数据来植入信号（即让分类器学习特征修改版本与目标类别之间的关联）。分析集体规模和目标对齐度的作用与相互关系。

Result: 提供了关于集体规模和目标对齐度在算法集体行动中作用的定量分析结果，补充了之前的实证研究。

Conclusion: 该框架为多集体算法集体行动的整体处理开辟了路径，为理解现实世界中分散但目标一致的集体行动提供了理论基础。

Abstract: As learning systems increasingly influence everyday decisions, user-side
steering via Algorithmic Collective Action (ACA)-coordinated changes to shared
data-offers a complement to regulator-side policy and firm-side model design.
Although real-world actions have been traditionally decentralized and
fragmented into multiple collectives despite sharing overarching
objectives-with each collective differing in size, strategy, and actionable
goals, most of the ACA literature focused on single collective settings. In
this work, we present the first theoretical framework for ACA with multiple
collectives acting on the same system. In particular, we focus on collective
action in classification, studying how multiple collectives can plant signals,
i.e., bias a classifier to learn an association between an altered version of
the features and a chosen, possibly overlapping, set of target classes. We
provide quantitative results about the role and the interplay of collectives'
sizes and their alignment of goals. Our framework, by also complementing
previous empirical results, opens a path for a holistic treatment of ACA with
multiple collectives.

</details>


### [72] [Playstyle and Artificial Intelligence: An Initial Blueprint Through the Lens of Video Games](https://arxiv.org/abs/2508.19152)
*Chiu-Chou Lin*

Main category: cs.AI

TL;DR: 该论文提出了游戏风格(playstyle)作为分析智能体决策行为的新视角，构建了风格形成的双层框架，并提出了可测量的风格指标，探讨了在强化学习和模仿学习中生成特定风格的方法及其在游戏设计等领域的应用。


<details>
  <summary>Details</summary>
Motivation: 当前AI发展主要关注理性决策，但真实世界中智能体的决策还受到信念、价值观和偏好等深层因素的影响。人类决策风格的多样性表明"风格"是智能的重要但常被忽视的维度。

Method: 构建风格形成的双层框架（外部环境交互循环和内部认知审议循环），形式化风格相关特征，提出风格容量、风格流行度和进化动力学等可测量指标。研究包括：定义和测量游戏风格、表达和生成游戏风格、实际应用分析。

Result: 提出了基于离散状态空间的通用游戏风格度量方法，可量化战略多样性和竞争平衡；探索了使用强化学习和模仿学习训练具有特定风格倾向的智能体；提出了人类风格学习和建模的新方法。

Conclusion: 游戏风格为分析智能体决策行为提供了有价值的视角，相关技术在游戏设计和交互娱乐等领域具有应用潜力。风格可能成为构建人工通用智能(AGI)的核心要素。

Abstract: Contemporary artificial intelligence (AI) development largely centers on
rational decision-making, valued for its measurability and suitability for
objective evaluation. Yet in real-world contexts, an intelligent agent's
decisions are shaped not only by logic but also by deeper influences such as
beliefs, values, and preferences. The diversity of human decision-making styles
emerges from these differences, highlighting that "style" is an essential but
often overlooked dimension of intelligence.
  This dissertation introduces playstyle as an alternative lens for observing
and analyzing the decision-making behavior of intelligent agents, and examines
its foundational meaning and historical context from a philosophical
perspective. By analyzing how beliefs and values drive intentions and actions,
we construct a two-tier framework for style formation: the external interaction
loop with the environment and the internal cognitive loop of deliberation. On
this basis, we formalize style-related characteristics and propose measurable
indicators such as style capacity, style popularity, and evolutionary dynamics.
  The study focuses on three core research directions: (1) Defining and
measuring playstyle, proposing a general playstyle metric based on discretized
state spaces, and extending it to quantify strategic diversity and competitive
balance; (2) Expressing and generating playstyle, exploring how reinforcement
learning and imitation learning can be used to train agents exhibiting specific
stylistic tendencies, and introducing a novel approach for human-like style
learning and modeling; and (3) Practical applications, analyzing the potential
of these techniques in domains such as game design and interactive
entertainment.
  Finally, the dissertation outlines future extensions, including the role of
style as a core element in building artificial general intelligence (AGI).

</details>


### [73] [MATRIX: Multi-Agent simulaTion fRamework for safe Interactions and conteXtual clinical conversational evaluation](https://arxiv.org/abs/2508.19163)
*Ernest Lim,Yajie Vera He,Jared Joselowitz,Kate Preston,Mohita Chowdhury,Louis Williams,Aisling Higham,Katrina Mason,Mariane Melo,Tom Lawton,Yan Jia,Ibrahim Habli*

Main category: cs.AI

TL;DR: MATRIX是一个用于临床对话系统安全评估的多智能体仿真框架，整合了安全对齐的场景分类、基于LLM的评估器BehvJudge和模拟患者代理PatBot，实现了系统化、可扩展的安全评估。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型临床对话系统评估主要关注任务完成度和流畅性，缺乏对安全关键系统所需的行为和风险管理要求的深入分析，需要开发专门的安全导向评估框架。

Method: MATRIX框架包含三个核心组件：(1)基于安全工程方法的安全对齐场景分类法；(2)基于LLM的评估器BehvJudge，用于检测安全相关对话故障；(3)模拟患者代理PatBot，生成多样化场景响应。

Result: BehvJudge达到专家级危险检测水平(F1 0.96, 灵敏度0.999)，在240个对话的盲评中优于临床医生。PatBot在定量和定性评估中可靠地模拟真实患者行为。框架成功对5个LLM代理在2,100个模拟对话中进行基准测试。

Conclusion: MATRIX是首个将结构化安全工程与可扩展、经过验证的对话AI评估相统一的框架，支持监管机构对齐的安全审计，为临床对话系统的安全评估提供了系统化解决方案。

Abstract: Despite the growing use of large language models (LLMs) in clinical dialogue
systems, existing evaluations focus on task completion or fluency, offering
little insight into the behavioral and risk management requirements essential
for safety-critical systems. This paper presents MATRIX (Multi-Agent simulaTion
fRamework for safe Interactions and conteXtual clinical conversational
evaluation), a structured, extensible framework for safety-oriented evaluation
of clinical dialogue agents.
  MATRIX integrates three components: (1) a safety-aligned taxonomy of clinical
scenarios, expected system behaviors and failure modes derived through
structured safety engineering methods; (2) BehvJudge, an LLM-based evaluator
for detecting safety-relevant dialogue failures, validated against expert
clinician annotations; and (3) PatBot, a simulated patient agent capable of
producing diverse, scenario-conditioned responses, evaluated for realism and
behavioral fidelity with human factors expertise, and a patient-preference
study.
  Across three experiments, we show that MATRIX enables systematic, scalable
safety evaluation. BehvJudge with Gemini 2.5-Pro achieves expert-level hazard
detection (F1 0.96, sensitivity 0.999), outperforming clinicians in a blinded
assessment of 240 dialogues. We also conducted one of the first realism
analyses of LLM-based patient simulation, showing that PatBot reliably
simulates realistic patient behavior in quantitative and qualitative
evaluations. Using MATRIX, we demonstrate its effectiveness in benchmarking
five LLM agents across 2,100 simulated dialogues spanning 14 hazard scenarios
and 10 clinical domains.
  MATRIX is the first framework to unify structured safety engineering with
scalable, validated conversational AI evaluation, enabling regulator-aligned
safety auditing. We release all evaluation tools, prompts, structured
scenarios, and datasets.

</details>


### [74] [The Ramon Llull's Thinking Machine for Automated Ideation](https://arxiv.org/abs/2508.19200)
*Xinran Zhao,Boyuan Zheng,Chenglei Si,Haofei Yu,Ken Liu,Runlong Zhou,Ruochen Li,Tong Chen,Xiang Li,Yiming Zhang,Tongshuang Wu*

Main category: cs.AI

TL;DR: 本文重新审视了中世纪Ramon Llull的组合艺术，构建了一个基于主题、领域和方法三个组合轴的现代研究构思机器，通过LLM生成多样化且相关的研究想法。


<details>
  <summary>Details</summary>
Motivation: 重新挖掘中世纪组合艺术的价值，为现代科学研究提供系统化的构思框架，增强科学创造力并促进人机协作的构思过程。

Method: 定义三个组合轴（主题、领域、方法）作为构建模块，从专家或会议论文中挖掘元素，通过精心策划的组合提示LLM生成研究想法。

Result: 该方法能够产生多样化、相关性高且基于当前文献的研究想法，提供了一个轻量级、可解释的科学创造力增强工具。

Conclusion: 现代Llull思考机器为科学研究提供了一种有效的构思方法，展示了人机协作在科学创新中的潜力，为未来研究指明了方向。

Abstract: This paper revisits Ramon Llull's Ars combinatoria - a medieval framework for
generating knowledge through symbolic recombination - as a conceptual
foundation for building a modern Llull's thinking machine for research
ideation. Our approach defines three compositional axes: Theme (e.g.,
efficiency, adaptivity), Domain (e.g., question answering, machine
translation), and Method (e.g., adversarial training, linear attention). These
elements represent high-level abstractions common in scientific work -
motivations, problem settings, and technical approaches - and serve as building
blocks for LLM-driven exploration. We mine elements from human experts or
conference papers and show that prompting LLMs with curated combinations
produces research ideas that are diverse, relevant, and grounded in current
literature. This modern thinking machine offers a lightweight, interpretable
tool for augmenting scientific creativity and suggests a path toward
collaborative ideation between humans and AI.

</details>


### [75] [The Subset Sum Matching Problem](https://arxiv.org/abs/2508.19218)
*Yufei Wu,Manuel R. Torres,Parisa Zehtabi,Alberto Pozanco Lancho,Michael Cashmore,Daniel Borrajo,Manuela Veloso*

Main category: cs.AI

TL;DR: 提出了新的组合优化问题SSMP，作为金融交易对账等应用的抽象，开发了两个次优算法和一个最优算法，并建立了不同复杂度的基准测试集进行性能评估


<details>
  <summary>Details</summary>
Motivation: 解决金融应用中常见的交易对账等组合优化问题，需要找到有效的算法来处理这类抽象问题

Method: 提出了三种算法（两个次优算法和一个最优算法）来解决SSMP问题，并建立了涵盖不同复杂度的基准测试集进行实验评估

Result: 开发了针对SSMP问题的有效算法解决方案，并通过实验评估验证了这些方法的性能

Conclusion: SSMP是一个有实际应用价值的组合优化问题，提出的算法能够有效解决该问题，为金融交易对账等应用提供了理论基础和实用工具

Abstract: This paper presents a new combinatorial optimisation task, the Subset Sum
Matching Problem (SSMP), which is an abstraction of common financial
applications such as trades reconciliation. We present three algorithms, two
suboptimal and one optimal, to solve this problem. We also generate a benchmark
to cover different instances of SSMP varying in complexity, and carry out an
experimental evaluation to assess the performance of the approaches.

</details>


### [76] [StepWiser: Stepwise Generative Judges for Wiser Reasoning](https://arxiv.org/abs/2508.19229)
*Wei Xiong,Wenting Zhao,Weizhe Yuan,Olga Golovneva,Tong Zhang,Jason Weston,Sainbayar Sukhbaatar*

Main category: cs.AI

TL;DR: 将逐步奖励建模从分类任务重构为推理任务，提出生成式判断模型StepWiser，通过强化学习训练，在中间步骤判断准确性、策略模型训练改进和推理时搜索方面表现更优


<details>
  <summary>Details</summary>
Motivation: 随着模型越来越多地使用多步推理策略解决复杂问题，监督这些中间步骤的逻辑有效性成为关键挑战。现有过程奖励模型存在两个主要缺陷：缺乏解释能力，以及依赖静态数据集的监督微调限制了泛化能力

Method: 提出StepWiser生成式判断模型，将逐步奖励建模重构为推理任务本身。模型对策略模型的推理步骤进行元推理，先输出思考标记再给出最终判断。使用强化学习通过rollout的相对结果进行训练

Result: StepWiser在中间步骤判断准确性上优于现有方法，可用于改进策略模型的训练，并能提升推理时的搜索效果

Conclusion: 将逐步奖励建模从分类转向推理的方法有效，生成式判断模型通过元推理和强化学习训练，在多个维度上提升了过程监督的效果

Abstract: As models increasingly leverage multi-step reasoning strategies to solve
complex problems, supervising the logical validity of these intermediate steps
has become a critical research challenge. Process reward models address this by
providing step-by-step feedback, but current approaches have two major
drawbacks: they typically function as classifiers without providing
explanations, and their reliance on supervised fine-tuning with static datasets
limits generalization. Inspired by recent advances, we reframe stepwise reward
modeling from a classification task to a reasoning task itself. We thus propose
a generative judge that reasons about the policy model's reasoning steps (i.e.,
meta-reasons), outputting thinking tokens before delivering a final verdict.
Our model, StepWiser, is trained by reinforcement learning using relative
outcomes of rollouts. We show it provides (i) better judgment accuracy on
intermediate steps than existing methods; (ii) can be used to improve the
policy model at training time; and (iii) improves inference-time search.

</details>


### [77] [Model Context Protocols in Adaptive Transport Systems: A Survey](https://arxiv.org/abs/2508.19239)
*Gaurab Chhetri,Shriyank Somvanshi,Md Monzurul Islam,Shamyo Brotee,Mahmuda Sultana Mimi,Dipti Koirala,Biplov Pandey,Subasish Das*

Main category: cs.AI

TL;DR: 本文首次系统调查了模型上下文协议（MCP）作为统一范式，能够桥接协议级适应与上下文感知决策，提出了五类分类法并揭示了三个关键发现。


<details>
  <summary>Details</summary>
Motivation: 互联设备、自主系统和AI应用的快速扩张导致自适应传输系统严重碎片化，不同协议和上下文来源相互隔离，需要统一的集成框架。

Method: 通过分析现有文献，提出包含自适应机制、上下文感知框架、统一模型、集成策略和MCP架构的五类分类法，进行系统性调查分析。

Result: 发现传统传输协议已达到孤立适应的极限，MCP的客户端-服务器和JSON-RPC结构能够实现语义互操作性，AI驱动的传输需要MCP特有的集成范式。

Conclusion: MCP应作为下一代自适应、上下文感知和智能传输基础设施的基础，提出了相应的研究路线图。

Abstract: The rapid expansion of interconnected devices, autonomous systems, and AI
applications has created severe fragmentation in adaptive transport systems,
where diverse protocols and context sources remain isolated. This survey
provides the first systematic investigation of the Model Context Protocol (MCP)
as a unifying paradigm, highlighting its ability to bridge protocol-level
adaptation with context-aware decision making. Analyzing established
literature, we show that existing efforts have implicitly converged toward
MCP-like architectures, signaling a natural evolution from fragmented solutions
to standardized integration frameworks. We propose a five-category taxonomy
covering adaptive mechanisms, context-aware frameworks, unification models,
integration strategies, and MCP-enabled architectures. Our findings reveal
three key insights: traditional transport protocols have reached the limits of
isolated adaptation, MCP's client-server and JSON-RPC structure enables
semantic interoperability, and AI-driven transport demands integration
paradigms uniquely suited to MCP. Finally, we present a research roadmap
positioning MCP as a foundation for next-generation adaptive, context-aware,
and intelligent transport infrastructures.

</details>
