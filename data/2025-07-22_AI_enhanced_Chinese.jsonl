{"id": "2507.14256", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14256", "abs": "https://arxiv.org/abs/2507.14256", "authors": ["Jakub Walczak", "Piotr Tomalak", "Artur Laskowski"], "title": "Impact of Code Context and Prompting Strategies on Automated Unit Test Generation with Modern General-Purpose Large Language Models", "comment": null, "summary": "Generative AI is gaining increasing attention in software engineering, where\ntesting remains an indispensable reliability mechanism. According to the widely\nadopted testing pyramid, unit tests constitute the majority of test cases and\nare often schematic, requiring minimal domain expertise. Automatically\ngenerating such tests under the supervision of software engineers can\nsignificantly enhance productivity during the development phase of the software\nlifecycle.\n  This paper investigates the impact of code context and prompting strategies\non the quality and adequacy of unit tests generated by various large language\nmodels (LLMs) across several families. The results show that including\ndocstrings notably improves code adequacy, while further extending context to\nthe full implementation yields definitely smaller gains. Notably, the\nchain-of-thought prompting strategy -- applied even to 'reasoning' models --\nachieves the best results, with up to 96.3\\% branch coverage, a 57\\% average\nmutation score, and near-perfect compilation success rate. Among the evaluated\nmodels, M5 (Gemini 2.5 Pro) demonstrated superior performance in both mutation\nscore and branch coverage being still in top in terms of compilation success\nrate.\n  All the code and resulting test suites are publicly available at\nhttps://github.com/peetery/LLM-analysis.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u4ee3\u7801\u4e0a\u4e0b\u6587\u548c\u63d0\u793a\u7b56\u7565\u5bf9LLM\u751f\u6210\u5355\u5143\u6d4b\u8bd5\u8d28\u91cf\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u5305\u542b\u6587\u6863\u5b57\u7b26\u4e32\u663e\u8457\u63d0\u5347\u4ee3\u7801\u5145\u5206\u6027\uff0c\u800c\u94fe\u5f0f\u601d\u7ef4\u63d0\u793a\u7b56\u7565\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u751f\u6210\u5f0fAI\u81ea\u52a8\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u5355\u5143\u6d4b\u8bd5\uff0c\u63d0\u5347\u8f6f\u4ef6\u5f00\u53d1\u6548\u7387\u3002", "method": "\u8bc4\u4f30\u4e0d\u540cLLM\u5728\u591a\u79cd\u4e0a\u4e0b\u6587\u548c\u63d0\u793a\u7b56\u7565\u4e0b\u751f\u6210\u7684\u5355\u5143\u6d4b\u8bd5\u8d28\u91cf\uff0c\u5305\u62ec\u4ee3\u7801\u5145\u5206\u6027\u3001\u5206\u652f\u8986\u76d6\u7387\u548c\u53d8\u5f02\u5206\u6570\u3002", "result": "\u5305\u542b\u6587\u6863\u5b57\u7b26\u4e32\u663e\u8457\u63d0\u5347\u6d4b\u8bd5\u5145\u5206\u6027\uff0c\u94fe\u5f0f\u601d\u7ef4\u63d0\u793a\u7b56\u7565\u6548\u679c\u6700\u4f73\uff0cM5\u6a21\u578b\u8868\u73b0\u6700\u4f18\u3002", "conclusion": "\u751f\u6210\u5f0fAI\u5728\u5355\u5143\u6d4b\u8bd5\u751f\u6210\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4ee3\u7801\u4e0a\u4e0b\u6587\u548c\u63d0\u793a\u7b56\u7565\u662f\u5173\u952e\u56e0\u7d20\u3002"}}
{"id": "2507.14330", "categories": ["cs.SE", "D.2.1; D.2.4; D.2.10; F.4.1; F.4.3"], "pdf": "https://arxiv.org/pdf/2507.14330", "abs": "https://arxiv.org/abs/2507.14330", "authors": ["Arshad Beg", "Diarmuid O'Donoghue", "Rosemary Monahan"], "title": "Leveraging LLMs for Formal Software Requirements -- Challenges and Prospects", "comment": "Submitted to Overlay2025 - 7th International Workshop on Artificial\n  Intelligence and fOrmal VERification, Logic, Automata, and sYnthesis. [under\n  review]", "summary": "Software correctness is ensured mathematically through formal verification,\nwhich involves the resources of generating formal requirement specifications\nand having an implementation that must be verified. Tools such as\nmodel-checkers and theorem provers ensure software correctness by verifying the\nimplementation against the specification. Formal methods deployment is\nregularly enforced in the development of safety-critical systems e.g.\naerospace, medical devices and autonomous systems. Generating these\nspecifications from informal and ambiguous natural language requirements\nremains the key challenge. Our project, VERIFAI^{1}, aims to investigate\nautomated and semi-automated approaches to bridge this gap, using techniques\nfrom Natural Language Processing (NLP), ontology-based domain modelling,\nartefact reuse, and large language models (LLMs). This position paper presents\na preliminary synthesis of relevant literature to identify recurring challenges\nand prospective research directions in the generation of verifiable\nspecifications from informal requirements.", "AI": {"tldr": "VERIFAI\u9879\u76ee\u65e8\u5728\u901a\u8fc7NLP\u3001\u672c\u4f53\u5efa\u6a21\u548cLLMs\u7b49\u6280\u672f\uff0c\u81ea\u52a8\u5316\u6216\u534a\u81ea\u52a8\u5316\u5730\u4ece\u975e\u6b63\u5f0f\u9700\u6c42\u751f\u6210\u53ef\u9a8c\u8bc1\u7684\u89c4\u8303\u3002", "motivation": "\u89e3\u51b3\u4ece\u975e\u6b63\u5f0f\u548c\u6a21\u7cca\u7684\u81ea\u7136\u8bed\u8a00\u9700\u6c42\u751f\u6210\u5f62\u5f0f\u5316\u89c4\u8303\u7684\u5173\u952e\u6311\u6218\u3002", "method": "\u7ed3\u5408NLP\u3001\u672c\u4f53\u5efa\u6a21\u3001\u6784\u4ef6\u91cd\u7528\u548cLLMs\u7b49\u6280\u672f\u3002", "result": "\u521d\u6b65\u6587\u732e\u7efc\u8ff0\uff0c\u8bc6\u522b\u4e86\u6311\u6218\u548c\u6f5c\u5728\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "VERIFAI\u4e3a\u5f62\u5f0f\u5316\u89c4\u8303\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u7684\u81ea\u52a8\u5316\u9014\u5f84\u3002"}}
{"id": "2507.14396", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.14396", "abs": "https://arxiv.org/abs/2507.14396", "authors": ["Carey Lai Zheng Hui", "Johnson Britto Jessia Esther Leena", "Kumuthini Subramanian", "Zhao Chenyu", "Shubham Rajeshkumar Jariwala"], "title": "Developing Shared Vocabulary System For Collaborative Software Engineering", "comment": "16 pages, including appendix", "summary": "Effective communication is a critical factor in successful software\nengineering collaboration. However, communication gaps remain a persistent\nchallenge, often leading to misunderstandings, inefficiencies, and defects.\nThis research investigates the technical factors contributing to such\nmisunderstandings and explores the measurable benefits of establishing shared\nvocabulary systems within software documentation and codebases. Using a Design\nScience Research (DSR) framework, the study was structured into three iterative\nphases: problem identification, method development, and empirical validation.\nThe problem identification phase involved thematic analysis of communication\ndata and semi-structured interviews, revealing key factors such as ambiguous\nmessaging, misalignment in documentation, inconsistent code review feedback,\nand API integration miscommunication. Grounded Theory principles were employed\nto design a structured methodology for collaborative vocabulary development.\nEmpirical validation through controlled experiments demonstrated that while\ninitial adoption introduced overhead, the shared vocabulary system\nsignificantly improved information density, documentation clarity, and\ncollaboration efficiency over time. Findings offer actionable insights for\nimproving communication practices in software engineering, while also\nidentifying limitations and directions for future research.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u6c9f\u901a\u8bef\u89e3\u7684\u6280\u672f\u56e0\u7d20\uff0c\u5e76\u901a\u8fc7\u5171\u4eab\u8bcd\u6c47\u7cfb\u7edf\u6539\u5584\u4e86\u6587\u6863\u6e05\u6670\u5ea6\u548c\u534f\u4f5c\u6548\u7387\u3002", "motivation": "\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u6c9f\u901a\u4e0d\u7545\u5bfc\u81f4\u8bef\u89e3\u548c\u4f4e\u6548\uff0c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u91c7\u7528\u8bbe\u8ba1\u79d1\u5b66\u7814\u7a76\u6846\u67b6\uff0c\u5206\u4e09\u9636\u6bb5\uff1a\u95ee\u9898\u8bc6\u522b\u3001\u65b9\u6cd5\u5f00\u53d1\u548c\u5b9e\u8bc1\u9a8c\u8bc1\u3002", "result": "\u5171\u4eab\u8bcd\u6c47\u7cfb\u7edf\u663e\u8457\u63d0\u5347\u4e86\u4fe1\u606f\u5bc6\u5ea6\u3001\u6587\u6863\u6e05\u6670\u5ea6\u548c\u534f\u4f5c\u6548\u7387\u3002", "conclusion": "\u7814\u7a76\u4e3a\u6539\u5584\u8f6f\u4ef6\u5de5\u7a0b\u6c9f\u901a\u63d0\u4f9b\u4e86\u5b9e\u7528\u5efa\u8bae\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2507.14197", "categories": ["cs.CR", "cs.IT", "math.IT", "94A60"], "pdf": "https://arxiv.org/pdf/2507.14197", "abs": "https://arxiv.org/abs/2507.14197", "authors": ["Andriamifidisoa Ramamonjy", "Rufine Marius Lalasoa"], "title": "DM-RSA: An Extension of RSA with Dual Modulus", "comment": "5 pages", "summary": "We introduce DM-RSA (Dual Modulus RSA), a variant of the RSA cryptosystem\nthat employs two distinct moduli symmetrically to enhance security. By\nleveraging the Chinese Remainder Theorem (CRT) for decryption, DM-RSA provides\nincreased robustness against side-channel attacks while preserving the\nefficiency of classical RSA. This approach improves resistance to partial\ncompromise of a modulus and integrates easily into existing infrastructures.", "AI": {"tldr": "DM-RSA\u662f\u4e00\u79cdRSA\u52a0\u5bc6\u7cfb\u7edf\u7684\u53d8\u4f53\uff0c\u4f7f\u7528\u53cc\u6a21\u6570\u589e\u5f3a\u5b89\u5168\u6027\uff0c\u901a\u8fc7CRT\u63d0\u9ad8\u6297\u4fa7\u4fe1\u9053\u653b\u51fb\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u6548\u7387\u3002", "motivation": "\u4f20\u7edfRSA\u5728\u5b89\u5168\u6027\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u5bf9\u4fa7\u4fe1\u9053\u653b\u51fb\u548c\u6a21\u6570\u90e8\u5206\u6cc4\u9732\u7684\u8106\u5f31\u6027\u3002", "method": "\u91c7\u7528\u53cc\u6a21\u6570\u8bbe\u8ba1\uff0c\u5229\u7528\u4e2d\u56fd\u5269\u4f59\u5b9a\u7406\uff08CRT\uff09\u8fdb\u884c\u89e3\u5bc6\u3002", "result": "\u63d0\u9ad8\u4e86\u6297\u4fa7\u4fe1\u9053\u653b\u51fb\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6548\u6027\uff0c\u6613\u4e8e\u96c6\u6210\u73b0\u6709\u57fa\u7840\u8bbe\u65bd\u3002", "conclusion": "DM-RSA\u5728\u5b89\u5168\u6027\u548c\u6548\u7387\u4e0a\u53d6\u5f97\u4e86\u5e73\u8861\uff0c\u9002\u5408\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2507.14423", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.14423", "abs": "https://arxiv.org/abs/2507.14423", "authors": ["Mootez Saad", "Hao Li", "Tushar Sharma", "Ahmed E. Hassan"], "title": "On the Effect of Token Merging on Pre-trained Models for Code", "comment": null, "summary": "Tokenization is a fundamental component of language models for code. It\ninvolves breaking down the input into units that are later passed to the\nlanguage model stack to learn high-dimensional representations used in various\ncontexts, from classification to generation. However, the output of these\ntokenizers is often longer than that traditionally used in compilers and\ninterpreters. This could result in undesirable effects, such as increased\ncomputational overhead. In this work, we investigate the effect of merging the\nhidden representations of subtokens that belong to the same semantic unit, such\nas subtokens that form a single identifier. We propose two strategies: one\nbased on averaging the representations and another that leverages a\nlearning-based approach. Both methods can be seamlessly integrated with\nexisting language models for code. We conduct experiments using six language\nmodels for code: CodeBERT, GraphCodeBERT, UniXCoder, CdoeT5, CodeT5+ (220M),\nand CodeT5+ (770M), across three software engineering tasks: vulnerability\ndetection, code classification, and code translation. Results show that these\nstrategies can reduce the number of floating-point operations by $1\\%$ to\n$19\\%$. Regarding downstream performance, the most significant degradation was\nobserved in the vulnerability detection task, where the F1 score decreased by\n$1.82$ points compared to the baseline. In contrast, for code translation, we\nobserved an improvement of $2.47$ points in CodeBLEU. This work contributes to\nthe broader effort of improving language models for code across multiple\ndimensions, including both computational efficiency and downstream performance.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u4ee3\u7801\u8bed\u8a00\u6a21\u578b\u4e2d\u5b50\u4ee4\u724c\u9690\u85cf\u8868\u793a\u7684\u5408\u5e76\u7b56\u7565\uff0c\u63d0\u51fa\u4e24\u79cd\u65b9\u6cd5\uff1a\u57fa\u4e8e\u5e73\u5747\u548c\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u5b9e\u9a8c\u8868\u660e\u80fd\u51cf\u5c11\u8ba1\u7b97\u91cf1%\u81f319%\uff0c\u4e0b\u6e38\u4efb\u52a1\u8868\u73b0\u6709\u5347\u6709\u964d\u3002", "motivation": "\u4f20\u7edf\u4ee3\u7801\u8bed\u8a00\u6a21\u578b\u7684\u4ee4\u724c\u5316\u8f93\u51fa\u8f83\u957f\uff0c\u53ef\u80fd\u589e\u52a0\u8ba1\u7b97\u5f00\u9500\uff0c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u5408\u5e76\u5b50\u4ee4\u724c\u9690\u85cf\u8868\u793a\u6765\u4f18\u5316\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u5b50\u4ee4\u724c\u9690\u85cf\u8868\u793a\u5408\u5e76\u7b56\u7565\uff1a\u57fa\u4e8e\u5e73\u5747\u548c\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u5e76\u4e0e\u516d\u79cd\u4ee3\u7801\u8bed\u8a00\u6a21\u578b\u7ed3\u5408\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u8ba1\u7b97\u91cf\u51cf\u5c111%\u81f319%\uff0c\u4e0b\u6e38\u4efb\u52a1\u4e2d\u6f0f\u6d1e\u68c0\u6d4bF1\u5206\u6570\u4e0b\u964d1.82\u5206\uff0c\u4ee3\u7801\u7ffb\u8bd1CodeBLEU\u63d0\u53472.47\u5206\u3002", "conclusion": "\u7814\u7a76\u4e3a\u4f18\u5316\u4ee3\u7801\u8bed\u8a00\u6a21\u578b\u7684\u8ba1\u7b97\u6548\u7387\u548c\u4e0b\u6e38\u6027\u80fd\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2507.14201", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14201", "abs": "https://arxiv.org/abs/2507.14201", "authors": ["Yiran Wu", "Mauricio Velazco", "Andrew Zhao", "Manuel Ra\u00fal Mel\u00e9ndez Luj\u00e1n", "Srisuma Movva", "Yogesh K Roy", "Quang Nguyen", "Roberto Rodriguez", "Qingyun Wu", "Michael Albada", "Julia Kiseleva", "Anand Mudgerikar"], "title": "ExCyTIn-Bench: Evaluating LLM agents on Cyber Threat Investigation", "comment": null, "summary": "We present ExCyTIn-Bench, the first benchmark to Evaluate an LLM agent x on\nthe task of Cyber Threat Investigation through security questions derived from\ninvestigation graphs. Real-world security analysts must sift through a large\nnumber of heterogeneous alert signals and security logs, follow multi-hop\nchains of evidence, and compile an incident report. With the developments of\nLLMs, building LLM-based agents for automatic thread investigation is a\npromising direction. To assist the development and evaluation of LLM agents, we\nconstruct a dataset from a controlled Azure tenant that covers 8 simulated\nreal-world multi-step attacks, 57 log tables from Microsoft Sentinel and\nrelated services, and 589 automatically generated questions. We leverage\nsecurity logs extracted with expert-crafted detection logic to build threat\ninvestigation graphs, and then generate questions with LLMs using paired nodes\non the graph, taking the start node as background context and the end node as\nanswer. Anchoring each question to these explicit nodes and edges not only\nprovides automatic, explainable ground truth answers but also makes the\npipeline reusable and readily extensible to new logs. This also enables the\nautomatic generation of procedural tasks with verifiable rewards, which can be\nnaturally extended to training agents via reinforcement learning. Our\ncomprehensive experiments with different models confirm the difficulty of the\ntask: with the base setting, the average reward across all evaluated models is\n0.249, and the best achieved is 0.368, leaving substantial headroom for future\nresearch. Code and data are coming soon!", "AI": {"tldr": "ExCyTIn-Bench\u662f\u9996\u4e2a\u8bc4\u4f30LLM\u4ee3\u7406\u5728\u7f51\u7edc\u5b89\u5168\u5a01\u80c1\u8c03\u67e5\u4efb\u52a1\u4e2d\u7684\u57fa\u51c6\uff0c\u901a\u8fc7\u57fa\u4e8e\u8c03\u67e5\u56fe\u7684\u5b89\u5168\u95ee\u9898\u751f\u6210\u6570\u636e\u96c6\uff0c\u652f\u6301\u81ea\u52a8\u5316\u548c\u53ef\u89e3\u91ca\u7684\u8bc4\u4f30\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u5b89\u5168\u5206\u6790\u5e08\u9700\u5904\u7406\u5927\u91cf\u5f02\u6784\u8b66\u62a5\u548c\u65e5\u5fd7\uff0c\u800cLLM\u4ee3\u7406\u7684\u81ea\u52a8\u5a01\u80c1\u8c03\u67e5\u662f\u4e00\u4e2a\u6709\u524d\u666f\u7684\u65b9\u5411\uff0c\u9700\u8981\u5f00\u53d1\u8bc4\u4f30\u5de5\u5177\u3002", "method": "\u6784\u5efa\u5305\u542b8\u79cd\u6a21\u62df\u653b\u51fb\u300157\u4e2a\u65e5\u5fd7\u8868\u548c589\u4e2a\u95ee\u9898\u7684\u6570\u636e\u96c6\uff0c\u5229\u7528\u4e13\u5bb6\u8bbe\u8ba1\u7684\u68c0\u6d4b\u903b\u8f91\u751f\u6210\u8c03\u67e5\u56fe\uff0c\u5e76\u901a\u8fc7LLM\u751f\u6210\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u4efb\u52a1\u96be\u5ea6\u8f83\u9ad8\uff0c\u5e73\u5747\u5956\u52b1\u4e3a0.249\uff0c\u6700\u4f73\u6a21\u578b\u4e3a0.368\uff0c\u8868\u660e\u672a\u6765\u7814\u7a76\u7a7a\u95f4\u5927\u3002", "conclusion": "ExCyTIn-Bench\u4e3aLLM\u4ee3\u7406\u7684\u5f00\u53d1\u548c\u8bc4\u4f30\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u53ef\u89e3\u91ca\u7684\u57fa\u51c6\uff0c\u4ee3\u7801\u548c\u6570\u636e\u5373\u5c06\u53d1\u5e03\u3002"}}
{"id": "2507.14154", "categories": ["cs.AI", "cs.LG", "68T05, 81P68", "I.2.6; I.2.0; F.1.2"], "pdf": "https://arxiv.org/pdf/2507.14154", "abs": "https://arxiv.org/abs/2507.14154", "authors": ["Rahul Kabali"], "title": "The Free Will Equation: Quantum Field Analogies for AGI", "comment": "22 pages, 5 figures. Submitted as an arXiv preprint. All code and\n  experiment details included in appendix", "summary": "Artificial General Intelligence (AGI) research traditionally focuses on\nalgorithms that optimize for specific goals under deterministic rules. Yet,\nhuman-like intelligence exhibits adaptive spontaneity - an ability to make\nunexpected choices or free decisions not strictly dictated by past data or\nimmediate reward. This trait, often dubbed \"free will\" in a loose sense, might\nbe crucial for creativity, robust adaptation, and avoiding ruts in\nproblem-solving. This paper proposes a theoretical framework, called the Free\nWill Equation, that draws analogies from quantum field theory to endow AGI\nagents with a form of adaptive, controlled stochasticity in their\ndecision-making process. The core idea is to treat an AI agent's cognitive\nstate as a superposition of potential actions or thoughts, which collapses\nprobabilistically into a concrete action when a decision is made - much like a\nquantum wavefunction collapsing upon measurement. By incorporating mechanisms\nanalogous to quantum fields, along with intrinsic motivation terms, we aim to\nimprove an agent's ability to explore novel strategies and adapt to unforeseen\nchanges. Experiments in a non-stationary multi-armed bandit environment\ndemonstrate that agents using this framework achieve higher rewards and policy\ndiversity compared to baseline methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u81ea\u7531\u610f\u5fd7\u65b9\u7a0b\u201d\u7684\u7406\u8bba\u6846\u67b6\uff0c\u501f\u9274\u91cf\u5b50\u573a\u8bba\uff0c\u8d4b\u4e88AGI\u4ee3\u7406\u4e00\u79cd\u53d7\u63a7\u7684\u968f\u673a\u51b3\u7b56\u80fd\u529b\uff0c\u4ee5\u63d0\u5347\u5176\u9002\u5e94\u6027\u548c\u521b\u9020\u6027\u3002", "motivation": "\u4f20\u7edfAGI\u7814\u7a76\u4e13\u6ce8\u4e8e\u786e\u5b9a\u6027\u89c4\u5219\u4e0b\u7684\u76ee\u6807\u4f18\u5316\uff0c\u800c\u4eba\u7c7b\u667a\u80fd\u5177\u6709\u81ea\u53d1\u6027\u51b3\u7b56\u80fd\u529b\uff0c\u8fd9\u5bf9\u4e8e\u521b\u9020\u529b\u548c\u9002\u5e94\u6027\u81f3\u5173\u91cd\u8981\u3002\u672c\u6587\u65e8\u5728\u6a21\u62df\u8fd9\u79cd\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u5c06AI\u4ee3\u7406\u7684\u8ba4\u77e5\u72b6\u6001\u89c6\u4e3a\u6f5c\u5728\u884c\u52a8\u7684\u53e0\u52a0\u6001\uff0c\u5e76\u5f15\u5165\u7c7b\u4f3c\u91cf\u5b50\u573a\u548c\u5185\u5728\u52a8\u673a\u7684\u673a\u5236\uff0c\u5b9e\u73b0\u53d7\u63a7\u7684\u968f\u673a\u51b3\u7b56\u3002", "result": "\u5728\u975e\u7a33\u6001\u591a\u81c2\u8001\u864e\u673a\u73af\u5883\u4e2d\uff0c\u4f7f\u7528\u8be5\u6846\u67b6\u7684\u4ee3\u7406\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u83b7\u5f97\u4e86\u66f4\u9ad8\u7684\u5956\u52b1\u548c\u7b56\u7565\u591a\u6837\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3aAGI\u63d0\u4f9b\u4e86\u4e00\u79cd\u6a21\u62df\u4eba\u7c7b\u81ea\u7531\u610f\u5fd7\u7684\u65b9\u6cd5\uff0c\u6709\u671b\u63d0\u5347\u5176\u9002\u5e94\u6027\u548c\u521b\u9020\u529b\u3002"}}
{"id": "2507.14547", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.14547", "abs": "https://arxiv.org/abs/2507.14547", "authors": ["Noman Ahmad", "Ruoyu Su", "Matteo Esposito", "Andrea Janes", "Valentina Lenarduzzi", "Davide Taibi"], "title": "Architectural Degradation: Definition, Motivations, Measurement and Remediation Approaches", "comment": null, "summary": "Architectural degradation, also known as erosion, decay, or aging, impacts\nsystem quality, maintainability, and adaptability. Although widely\nacknowledged, current literature shows fragmented definitions, metrics, and\nremediation strategies. Our study aims to unify understanding of architectural\ndegradation by identifying its definitions, causes, metrics, tools, and\nremediation approaches across academic and gray literature. We conducted a\nmultivocal literature review of 108 studies extracting definitions, causes,\nmetrics, measurement approaches, tools, and remediation strategies. We\ndeveloped a taxonomy encompassing architectural, code, and process debt to\nexplore definition evolution, methodological trends, and research gaps.\nArchitectural degradation has shifted from a low-level issue to a\nsocio-technical concern. Definitions now address code violations, design drift,\nand structural decay. Causes fall under architectural (e.g., poor\ndocumentation), code (e.g., hasty fixes), and process debt (e.g., knowledge\nloss). We identified 54 metrics and 31 measurement techniques, focused on\nsmells, cohesion/coupling, and evolution. Yet, most tools detect issues but\nrarely support ongoing or preventive remediation. Degradation is both technical\nand organizational. While detection is well-studied, continuous remediation\nremains lacking. Our study reveals missed integration between metrics, tools,\nand repair logic, urging holistic, proactive strategies for sustainable\narchitecture.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7\u591a\u58f0\u90e8\u6587\u732e\u7efc\u8ff0\uff0c\u7edf\u4e00\u4e86\u5bf9\u67b6\u6784\u9000\u5316\u7684\u7406\u89e3\uff0c\u63d0\u51fa\u4e86\u6db5\u76d6\u67b6\u6784\u3001\u4ee3\u7801\u548c\u8fc7\u7a0b\u503a\u52a1\u7684\u5206\u7c7b\u6cd5\uff0c\u5e76\u63ed\u793a\u4e86\u5f53\u524d\u7814\u7a76\u7684\u4e0d\u8db3\u3002", "motivation": "\u67b6\u6784\u9000\u5316\u5f71\u54cd\u7cfb\u7edf\u8d28\u91cf\u548c\u53ef\u7ef4\u62a4\u6027\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u5b9a\u4e49\u548c\u7b56\u7565\u5206\u6563\uff0c\u9700\u7edf\u4e00\u7406\u89e3\u3002", "method": "\u5bf9108\u9879\u7814\u7a76\u8fdb\u884c\u591a\u58f0\u90e8\u6587\u732e\u7efc\u8ff0\uff0c\u63d0\u53d6\u5b9a\u4e49\u3001\u539f\u56e0\u3001\u6307\u6807\u3001\u5de5\u5177\u548c\u4fee\u590d\u7b56\u7565\uff0c\u5e76\u5f00\u53d1\u5206\u7c7b\u6cd5\u3002", "result": "\u67b6\u6784\u9000\u5316\u4ece\u4f4e\u7ea7\u95ee\u9898\u8f6c\u5411\u793e\u4f1a\u6280\u672f\u95ee\u9898\uff0c\u5b9a\u4e49\u4e8654\u9879\u6307\u6807\u548c31\u79cd\u6d4b\u91cf\u6280\u672f\uff0c\u4f46\u6301\u7eed\u4fee\u590d\u5de5\u5177\u4e0d\u8db3\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u6307\u6807\u3001\u5de5\u5177\u548c\u4fee\u590d\u903b\u8f91\u95f4\u7684\u8131\u8282\uff0c\u547c\u5401\u6574\u4f53\u3001\u4e3b\u52a8\u7684\u7b56\u7565\u4ee5\u5b9e\u73b0\u53ef\u6301\u7eed\u67b6\u6784\u3002"}}
{"id": "2507.14202", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14202", "abs": "https://arxiv.org/abs/2507.14202", "authors": ["Pengfei Du"], "title": "PRM-Free Security Alignment of Large Models via Red Teaming and Adversarial Training", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\ndiverse applications, yet they pose significant security risks that threaten\ntheir safe deployment in critical domains. Current security alignment\nmethodologies predominantly rely on Process Reward Models (PRMs) to evaluate\nintermediate reasoning steps, introducing substantial computational overhead\nand scalability constraints. This paper presents a novel PRM-free security\nalignment framework that leverages automated red teaming and adversarial\ntraining to achieve robust security guarantees while maintaining computational\nefficiency. Our approach systematically identifies vulnerabilities through\nsophisticated attack strategies including genetic algorithm optimization,\nmulti-agent simulation, and advanced prompt mutation techniques. The framework\nenhances model robustness via targeted adversarial training with curriculum\nlearning and adaptive regularization mechanisms. Comprehensive experimental\nevaluation across five state-of-the-art LLMs demonstrates that our method\nachieves superior security alignment performance compared to PRM-based\napproaches while reducing computational costs by 61\\%. The framework\nincorporates transparent reporting and continuous audit mechanisms that enable\niterative security improvement and regulatory compliance. Our contributions\nadvance the field of efficient LLM security alignment by democratizing access\nto robust security measures for resource-constrained organizations and\nproviding a scalable foundation for addressing evolving adversarial threats.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700PRM\u7684\u5b89\u5168\u5bf9\u9f50\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u7ea2\u961f\u548c\u5bf9\u6297\u8bad\u7ec3\u5b9e\u73b0\u9ad8\u6548\u5b89\u5168\u4fdd\u8bc1\uff0c\u8ba1\u7b97\u6210\u672c\u964d\u4f4e61%\u3002", "motivation": "LLMs\u5728\u5e7f\u6cdb\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5b58\u5728\u5b89\u5168\u98ce\u9669\uff0c\u5f53\u524d\u57fa\u4e8ePRM\u7684\u65b9\u6cd5\u8ba1\u7b97\u5f00\u9500\u5927\u4e14\u96be\u4ee5\u6269\u5c55\u3002", "method": "\u5229\u7528\u81ea\u52a8\u5316\u7ea2\u961f\u548c\u5bf9\u6297\u8bad\u7ec3\uff0c\u5305\u62ec\u9057\u4f20\u7b97\u6cd5\u4f18\u5316\u3001\u591a\u667a\u80fd\u4f53\u6a21\u62df\u548c\u9ad8\u7ea7\u63d0\u793a\u53d8\u5f02\u6280\u672f\uff0c\u7ed3\u5408\u8bfe\u7a0b\u5b66\u4e60\u548c\u81ea\u9002\u5e94\u6b63\u5219\u5316\u3002", "result": "\u5728\u4e94\u79cd\u5148\u8fdbLLMs\u4e0a\u9a8c\u8bc1\uff0c\u6027\u80fd\u4f18\u4e8ePRM\u65b9\u6cd5\uff0c\u8ba1\u7b97\u6210\u672c\u964d\u4f4e61%\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u8d44\u6e90\u6709\u9650\u7ec4\u7ec7\u63d0\u4f9b\u4e86\u9ad8\u6548\u5b89\u5168\u5bf9\u9f50\u65b9\u6848\uff0c\u5e76\u4e3a\u5e94\u5bf9\u4e0d\u65ad\u6f14\u53d8\u7684\u5a01\u80c1\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2507.14267", "categories": ["cs.AI", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2507.14267", "abs": "https://arxiv.org/abs/2507.14267", "authors": ["Ziqi Wang", "Hongshuo Huang", "Hancheng Zhao", "Changwen Xu", "Shang Zhu", "Jan Janssen", "Venkatasubramanian Viswanathan"], "title": "DREAMS: Density Functional Theory Based Research Engine for Agentic Materials Simulation", "comment": "34 pages, 28 pages of Supporting Information", "summary": "Materials discovery relies on high-throughput, high-fidelity simulation\ntechniques such as Density Functional Theory (DFT), which require years of\ntraining, extensive parameter fine-tuning and systematic error handling. To\naddress these challenges, we introduce the DFT-based Research Engine for\nAgentic Materials Screening (DREAMS), a hierarchical, multi-agent framework for\nDFT simulation that combines a central Large Language Model (LLM) planner agent\nwith domain-specific LLM agents for atomistic structure generation, systematic\nDFT convergence testing, High-Performance Computing (HPC) scheduling, and error\nhandling. In addition, a shared canvas helps the LLM agents to structure their\ndiscussions, preserve context and prevent hallucination. We validate DREAMS\ncapabilities on the Sol27LC lattice-constant benchmark, achieving average\nerrors below 1\\% compared to the results of human DFT experts. Furthermore, we\napply DREAMS to the long-standing CO/Pt(111) adsorption puzzle, demonstrating\nits long-term and complex problem-solving capabilities. The framework again\nreproduces expert-level literature adsorption-energy differences. Finally,\nDREAMS is employed to quantify functional-driven uncertainties with Bayesian\nensemble sampling, confirming the Face Centered Cubic (FCC)-site preference at\nthe Generalized Gradient Approximation (GGA) DFT level. In conclusion, DREAMS\napproaches L3-level automation - autonomous exploration of a defined design\nspace - and significantly reduces the reliance on human expertise and\nintervention, offering a scalable path toward democratized, high-throughput,\nhigh-fidelity computational materials discovery.", "AI": {"tldr": "DREAMS\u662f\u4e00\u4e2a\u57fa\u4e8eDFT\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7LLM\u89c4\u5212\u5668\u548c\u9886\u57df\u7279\u5b9a\u4ee3\u7406\u5b9e\u73b0\u6750\u6599\u53d1\u73b0\u7684\u9ad8\u901a\u91cf\u3001\u9ad8\u4fdd\u771f\u6a21\u62df\uff0c\u663e\u8457\u51cf\u5c11\u5bf9\u4eba\u529b\u7684\u4f9d\u8d56\u3002", "motivation": "\u89e3\u51b3DFT\u6a21\u62df\u4e2d\u8bad\u7ec3\u65f6\u95f4\u957f\u3001\u53c2\u6570\u8c03\u4f18\u590d\u6742\u548c\u7cfb\u7edf\u8bef\u5dee\u5904\u7406\u56f0\u96be\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u5206\u5c42\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u7ed3\u5408LLM\u89c4\u5212\u5668\u548c\u9886\u57df\u4ee3\u7406\uff08\u5982\u539f\u5b50\u7ed3\u6784\u751f\u6210\u3001DFT\u6536\u655b\u6d4b\u8bd5\u3001HPC\u8c03\u5ea6\u548c\u9519\u8bef\u5904\u7406\uff09\uff0c\u5e76\u901a\u8fc7\u5171\u4eab\u753b\u5e03\u907f\u514d\u5e7b\u89c9\u3002", "result": "\u5728Sol27LC\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8bef\u5dee\u4f4e\u4e8e1%\uff0c\u89e3\u51b3\u4e86CO/Pt(111)\u5438\u9644\u96be\u9898\uff0c\u5e76\u901a\u8fc7\u8d1d\u53f6\u65af\u91c7\u6837\u786e\u8ba4FCC\u4f4d\u70b9\u504f\u597d\u3002", "conclusion": "DREAMS\u5b9e\u73b0\u4e86L3\u7ea7\u81ea\u52a8\u5316\uff0c\u663e\u8457\u51cf\u5c11\u5bf9\u4eba\u529b\u7684\u4f9d\u8d56\uff0c\u4e3a\u9ad8\u901a\u91cf\u6750\u6599\u53d1\u73b0\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u8def\u5f84\u3002"}}
{"id": "2507.14554", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.14554", "abs": "https://arxiv.org/abs/2507.14554", "authors": ["Ruoyu Su", "Noman ahmad", "Matteo Esposito", "Andrea Janes", "Davide Taibi", "Valentina Lenarduzzi"], "title": "Emerging Trends in Software Architecture from the Practitioners Perspective: A Five Year Review", "comment": null, "summary": "Software architecture plays a central role in the design, development, and\nmaintenance of software systems. With the rise of cloud computing,\nmicroservices, and containers, architectural practices have diversified.\nUnderstanding these shifts is vital. This study analyzes software architecture\ntrends across eight leading industry conferences over five years. We\ninvestigate the evolution of software architecture by analyzing talks from top\npractitioner conferences, focusing on the motivations and contexts driving\ntechnology adoption. We analyzed 5,677 talks from eight major industry\nconferences, using large language models and expert validation to extract\ntechnologies, their purposes, and usage contexts. We also explored how\ntechnologies interrelate and fit within DevOps and deployment pipelines. Among\n450 technologies, Kubernetes, Cloud Native, Serverless, and Containers dominate\nby frequency and centrality. Practitioners present technology mainly related to\ndeployment, communication, AI, and observability. We identify five technology\ncommunities covering automation, coordination, cloud AI, monitoring, and\ncloud-edge. Most technologies span multiple DevOps stages and support hybrid\ndeployment. Our study reveals that a few core technologies, like Kubernetes and\nServerless, dominate the contemporary software architecture practice. These are\nmainly applied in later DevOps stages, with limited focus on early phases like\nplanning and coding. We also show how practitioners frame technologies by\npurpose and context, reflecting evolving industry priorities. Finally, we\nobserve how only research can provide a more holistic lens on architectural\ndesign, quality, and evolution.", "AI": {"tldr": "\u7814\u7a76\u5206\u6790\u4e86\u4e94\u5e74\u5185\u516b\u4e2a\u884c\u4e1a\u4f1a\u8bae\u4e2d\u7684\u8f6f\u4ef6\u67b6\u6784\u8d8b\u52bf\uff0c\u53d1\u73b0Kubernetes\u3001Serverless\u7b49\u6280\u672f\u4e3b\u5bfc\u5f53\u524d\u5b9e\u8df5\uff0c\u4e3b\u8981\u5e94\u7528\u4e8eDevOps\u540e\u671f\u9636\u6bb5\u3002", "motivation": "\u968f\u7740\u4e91\u8ba1\u7b97\u3001\u5fae\u670d\u52a1\u548c\u5bb9\u5668\u7684\u5174\u8d77\uff0c\u8f6f\u4ef6\u67b6\u6784\u5b9e\u8df5\u591a\u6837\u5316\uff0c\u7406\u89e3\u8fd9\u4e9b\u53d8\u5316\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5206\u6790\u4e865,677\u4e2a\u884c\u4e1a\u4f1a\u8bae\u6f14\u8bb2\uff0c\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u4e13\u5bb6\u9a8c\u8bc1\u63d0\u53d6\u6280\u672f\u53ca\u5176\u7528\u9014\u548c\u4e0a\u4e0b\u6587\u3002", "result": "Kubernetes\u3001Cloud Native\u7b49\u6280\u672f\u5728\u9891\u7387\u548c\u4e2d\u5fc3\u6027\u4e0a\u5360\u4e3b\u5bfc\uff0c\u8986\u76d6\u81ea\u52a8\u5316\u3001\u534f\u8c03\u3001\u4e91AI\u7b49\u9886\u57df\u3002", "conclusion": "\u5c11\u6570\u6838\u5fc3\u6280\u672f\u4e3b\u5bfc\u5f53\u524d\u67b6\u6784\u5b9e\u8df5\uff0c\u7814\u7a76\u9700\u66f4\u5168\u9762\u5173\u6ce8\u67b6\u6784\u8bbe\u8ba1\u3001\u8d28\u91cf\u548c\u6f14\u53d8\u3002"}}
{"id": "2507.14207", "categories": ["cs.CR", "cs.AI", "I.2.1; I.2.7"], "pdf": "https://arxiv.org/pdf/2507.14207", "abs": "https://arxiv.org/abs/2507.14207", "authors": ["Richard M. Charles", "James H. Curry", "Richard B. Charles"], "title": "Mitigating Trojanized Prompt Chains in Educational LLM Use Cases: Experimental Findings and Detection Tool Design", "comment": "12 pages, 1 figure", "summary": "The integration of Large Language Models (LLMs) in K--12 education offers\nboth transformative opportunities and emerging risks. This study explores how\nstudents may Trojanize prompts to elicit unsafe or unintended outputs from\nLLMs, bypassing established content moderation systems with safety guardrils.\nThrough a systematic experiment involving simulated K--12 queries and\nmulti-turn dialogues, we expose key vulnerabilities in GPT-3.5 and GPT-4. This\npaper presents our experimental design, detailed findings, and a prototype\ntool, TrojanPromptGuard (TPG), to automatically detect and mitigate Trojanized\neducational prompts. These insights aim to inform both AI safety researchers\nand educational technologists on the safe deployment of LLMs for educators.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86K-12\u6559\u80b2\u4e2d\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5b89\u5168\u6f0f\u6d1e\uff0c\u5b66\u751f\u53ef\u80fd\u901a\u8fc7\u7279\u6d1b\u4f0a\u5316\u63d0\u793a\u7ed5\u8fc7\u5185\u5bb9\u5ba1\u6838\u7cfb\u7edf\uff0c\u5f15\u53d1\u4e0d\u5b89\u5168\u8f93\u51fa\u3002", "motivation": "\u63ed\u793aLLMs\u5728\u6559\u80b2\u5e94\u7528\u4e2d\u7684\u6f5c\u5728\u98ce\u9669\uff0c\u786e\u4fdd\u5176\u5b89\u5168\u90e8\u7f72\u3002", "method": "\u901a\u8fc7\u6a21\u62dfK-12\u67e5\u8be2\u548c\u591a\u8f6e\u5bf9\u8bdd\u5b9e\u9a8c\uff0c\u6d4b\u8bd5GPT-3.5\u548cGPT-4\u7684\u6f0f\u6d1e\uff0c\u5e76\u5f00\u53d1\u539f\u578b\u5de5\u5177TrojanPromptGuard\uff08TPG\uff09\u3002", "result": "\u5b9e\u9a8c\u66b4\u9732\u4e86LLMs\u7684\u5173\u952e\u6f0f\u6d1e\uff0cTPG\u80fd\u6709\u6548\u68c0\u6d4b\u548c\u7f13\u89e3\u7279\u6d1b\u4f0a\u5316\u63d0\u793a\u3002", "conclusion": "\u7814\u7a76\u4e3aAI\u5b89\u5168\u7814\u7a76\u4eba\u5458\u548c\u6559\u80b2\u6280\u672f\u4e13\u5bb6\u63d0\u4f9b\u4e86LLMs\u5b89\u5168\u90e8\u7f72\u7684\u53c2\u8003\u3002"}}
{"id": "2507.14293", "categories": ["cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14293", "abs": "https://arxiv.org/abs/2507.14293", "authors": ["Boyuan Zheng", "Zeyi Liao", "Scott Salisbury", "Zeyuan Liu", "Michael Lin", "Qinyuan Zheng", "Zifan Wang", "Xiang Deng", "Dawn Song", "Huan Sun", "Yu Su"], "title": "WebGuard: Building a Generalizable Guardrail for Web Agents", "comment": "We publicly release WebGuard, along with its annotation tools and\n  fine-tuned models, to facilitate open-source research on monitoring and\n  safeguarding web agents. All resources are available at\n  https://github.com/OSU-NLP-Group/WebGuard", "summary": "The rapid development of autonomous web agents powered by Large Language\nModels (LLMs), while greatly elevating efficiency, exposes the frontier risk of\ntaking unintended or harmful actions. This situation underscores an urgent need\nfor effective safety measures, akin to access controls for human users. To\naddress this critical challenge, we introduce WebGuard, the first comprehensive\ndataset designed to support the assessment of web agent action risks and\nfacilitate the development of guardrails for real-world online environments. In\ndoing so, WebGuard specifically focuses on predicting the outcome of\nstate-changing actions and contains 4,939 human-annotated actions from 193\nwebsites across 22 diverse domains, including often-overlooked long-tail\nwebsites. These actions are categorized using a novel three-tier risk schema:\nSAFE, LOW, and HIGH. The dataset includes designated training and test splits\nto support evaluation under diverse generalization settings. Our initial\nevaluations reveal a concerning deficiency: even frontier LLMs achieve less\nthan 60% accuracy in predicting action outcomes and less than 60% recall in\nlagging HIGH-risk actions, highlighting the risks of deploying\ncurrent-generation agents without dedicated safeguards. We therefore\ninvestigate fine-tuning specialized guardrail models using WebGuard. We conduct\ncomprehensive evaluations across multiple generalization settings and find that\na fine-tuned Qwen2.5VL-7B model yields a substantial improvement in\nperformance, boosting accuracy from 37% to 80% and HIGH-risk action recall from\n20% to 76%. Despite these improvements, the performance still falls short of\nthe reliability required for high-stakes deployment, where guardrails must\napproach near-perfect accuracy and recall.", "AI": {"tldr": "WebGuard\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u7f51\u7edc\u4ee3\u7406\u884c\u4e3a\u98ce\u9669\u7684\u6570\u636e\u96c6\uff0c\u65e8\u5728\u5f00\u53d1\u5b89\u5168\u63aa\u65bd\u3002\u7814\u7a76\u53d1\u73b0\u5f53\u524dLLMs\u5728\u9884\u6d4b\u884c\u4e3a\u7ed3\u679c\u548c\u8bc6\u522b\u9ad8\u98ce\u9669\u884c\u4e3a\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u4f46\u901a\u8fc7\u5fae\u8c03\u6a21\u578b\u53ef\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u57fa\u4e8eLLMs\u7684\u81ea\u4e3b\u7f51\u7edc\u4ee3\u7406\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u5176\u53ef\u80fd\u91c7\u53d6\u610f\u5916\u6216\u6709\u5bb3\u884c\u4e3a\u7684\u98ce\u9669\u589e\u52a0\uff0c\u4e9f\u9700\u6709\u6548\u7684\u5b89\u5168\u63aa\u65bd\u3002", "method": "\u5f15\u5165WebGuard\u6570\u636e\u96c6\uff0c\u5305\u542b4,939\u4e2a\u4eba\u5de5\u6807\u6ce8\u7684\u884c\u4e3a\uff0c\u91c7\u7528\u4e09\u7ea7\u98ce\u9669\u5206\u7c7b\uff08SAFE\u3001LOW\u3001HIGH\uff09\uff0c\u5e76\u7814\u7a76\u5fae\u8c03\u4e13\u7528\u9632\u62a4\u6a21\u578b\u7684\u6548\u679c\u3002", "result": "\u524d\u6cbfLLMs\u5728\u9884\u6d4b\u884c\u4e3a\u7ed3\u679c\u548c\u9ad8\u98ce\u9669\u884c\u4e3a\u53ec\u56de\u7387\u4e0a\u8868\u73b0\u4e0d\u8db3\uff08\u5747\u4f4e\u4e8e60%\uff09\u3002\u5fae\u8c03\u540e\u7684Qwen2.5VL-7B\u6a21\u578b\u5c06\u51c6\u786e\u7387\u4ece37%\u63d0\u5347\u81f380%\uff0c\u9ad8\u98ce\u9669\u884c\u4e3a\u53ec\u56de\u7387\u4ece20%\u63d0\u5347\u81f376%\u3002", "conclusion": "\u5c3d\u7ba1\u5fae\u8c03\u6a21\u578b\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u4f46\u5176\u53ef\u9760\u6027\u4ecd\u4e0d\u8db3\u4ee5\u652f\u6301\u9ad8\u98ce\u9669\u90e8\u7f72\uff0c\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u4ee5\u5b9e\u73b0\u8fd1\u4e4e\u5b8c\u7f8e\u7684\u51c6\u786e\u7387\u548c\u53ec\u56de\u7387\u3002"}}
{"id": "2507.14558", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.14558", "abs": "https://arxiv.org/abs/2507.14558", "authors": ["Bin Duan", "Tarek Mahmud", "Meiru Che", "Yan Yan", "Naipeng Dong", "Dan Dongseong Kim", "Guowei Yang"], "title": "Harnessing LLMs for Document-Guided Fuzzing of OpenCV Library", "comment": null, "summary": "The combination of computer vision and artificial intelligence is\nfundamentally transforming a broad spectrum of industries by enabling machines\nto interpret and act upon visual data with high levels of accuracy. As the\nbiggest and by far the most popular open-source computer vision library, OpenCV\nlibrary provides an extensive suite of programming functions supporting\nreal-time computer vision. Bugs in the OpenCV library can affect the downstream\ncomputer vision applications, and it is critical to ensure the reliability of\nthe OpenCV library. This paper introduces VISTAFUZZ, a novel technique for\nharnessing large language models (LLMs) for document-guided fuzzing of the\nOpenCV library. VISTAFUZZ utilizes LLMs to parse API documentation and obtain\nstandardized API information. Based on this standardized information, VISTAFUZZ\nextracts constraints on individual input parameters and dependencies between\nthese. Using these constraints and dependencies, VISTAFUZZ then generates new\ninput values to systematically test each target API. We evaluate the\neffectiveness of VISTAFUZZ in testing 330 APIs in the OpenCV library, and the\nresults show that VISTAFUZZ detected 17 new bugs, where 10 bugs have been\nconfirmed, and 5 of these have been fixed.", "AI": {"tldr": "VISTAFUZZ\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u89e3\u6790OpenCV API\u6587\u6863\uff0c\u751f\u6210\u6807\u51c6\u5316\u4fe1\u606f\u5e76\u63d0\u53d6\u53c2\u6570\u7ea6\u675f\uff0c\u4ee5\u7cfb\u7edf\u5316\u6d4b\u8bd5API\uff0c\u53d1\u73b017\u4e2a\u65b0bug\u3002", "motivation": "OpenCV\u4f5c\u4e3a\u5e7f\u6cdb\u4f7f\u7528\u7684\u5f00\u6e90\u8ba1\u7b97\u673a\u89c6\u89c9\u5e93\uff0c\u5176\u53ef\u9760\u6027\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u6d4b\u8bd5\u65b9\u6cd5\u53ef\u80fd\u4e0d\u8db3\u3002", "method": "VISTAFUZZ\u901a\u8fc7LLMs\u89e3\u6790API\u6587\u6863\uff0c\u63d0\u53d6\u53c2\u6570\u7ea6\u675f\u548c\u4f9d\u8d56\u5173\u7cfb\uff0c\u751f\u6210\u6d4b\u8bd5\u8f93\u5165\u3002", "result": "\u6d4b\u8bd5330\u4e2aAPI\uff0c\u53d1\u73b017\u4e2a\u65b0bug\uff0c\u5176\u4e2d10\u4e2a\u5df2\u786e\u8ba4\uff0c5\u4e2a\u5df2\u4fee\u590d\u3002", "conclusion": "VISTAFUZZ\u80fd\u6709\u6548\u63d0\u5347OpenCV\u5e93\u7684\u53ef\u9760\u6027\uff0c\u4e3aAPI\u6d4b\u8bd5\u63d0\u4f9b\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2507.14212", "categories": ["cs.CR", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.14212", "abs": "https://arxiv.org/abs/2507.14212", "authors": ["Federico Mason", "Federico Chiariotti", "Pietro Talli", "Andrea Zanella"], "title": "Secure Goal-Oriented Communication: Defending against Eavesdropping Timing Attacks", "comment": null, "summary": "Goal-oriented Communication (GoC) is a new paradigm that plans data\ntransmission to occur only when it is instrumental for the receiver to achieve\na certain goal. This leads to the advantage of reducing the frequency of\ntransmissions significantly while maintaining adherence to the receiver's\nobjectives. However, GoC scheduling also opens a timing-based side channel that\nan eavesdropper can exploit to obtain information about the state of the\nsystem. This type of attack sidesteps even information-theoretic security, as\nit exploits the timing of updates rather than their content. In this work, we\nstudy such an eavesdropping attack against pull-based goal-oriented scheduling\nfor remote monitoring and control of Markov processes. We provide a theoretical\nframework for defining the effectiveness of the attack and propose possible\ncountermeasures, including two practical heuristics that provide a balance\nbetween the performance gains offered by GoC and the amount of leaked\ninformation. Our results show that, while a naive goal-oriented scheduler\nallows the eavesdropper to correctly guess the system state about 60% of the\ntime, our heuristic defenses can halve the leakage with a marginal reduction of\nthe benefits of goal-oriented approaches.", "AI": {"tldr": "\u76ee\u6807\u5bfc\u5411\u901a\u4fe1\uff08GoC\uff09\u901a\u8fc7\u51cf\u5c11\u4f20\u8f93\u9891\u7387\u4f18\u5316\u901a\u4fe1\uff0c\u4f46\u4f1a\u5f15\u5165\u57fa\u4e8e\u65f6\u95f4\u7684\u4fa7\u4fe1\u9053\u653b\u51fb\u3002\u672c\u6587\u7814\u7a76\u4e86\u9488\u5bf9GoC\u7684\u7a83\u542c\u653b\u51fb\uff0c\u63d0\u51fa\u4e86\u7406\u8bba\u6846\u67b6\u548c\u9632\u5fa1\u63aa\u65bd\u3002", "motivation": "GoC\u867d\u80fd\u51cf\u5c11\u4f20\u8f93\u9891\u7387\uff0c\u4f46\u4f1a\u66b4\u9732\u7cfb\u7edf\u72b6\u6001\u4fe1\u606f\uff0c\u9700\u7814\u7a76\u5176\u5b89\u5168\u6027\u548c\u9632\u5fa1\u65b9\u6cd5\u3002", "method": "\u7814\u7a76\u57fa\u4e8e\u65f6\u95f4\u7684\u4fa7\u4fe1\u9053\u653b\u51fb\uff0c\u63d0\u51fa\u7406\u8bba\u6846\u67b6\u548c\u4e24\u79cd\u542f\u53d1\u5f0f\u9632\u5fa1\u63aa\u65bd\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u666e\u901aGoC\u8c03\u5ea6\u5668\u6cc4\u6f0f60%\u7cfb\u7edf\u72b6\u6001\u4fe1\u606f\uff0c\u800c\u9632\u5fa1\u63aa\u65bd\u53ef\u51cf\u5c11\u6cc4\u6f0f50%\u4e14\u6027\u80fd\u635f\u5931\u8f83\u5c0f\u3002", "conclusion": "\u542f\u53d1\u5f0f\u9632\u5fa1\u63aa\u65bd\u80fd\u6709\u6548\u5e73\u8861GoC\u7684\u6027\u80fd\u4f18\u52bf\u4e0e\u4fe1\u606f\u5b89\u5168\u3002"}}
{"id": "2507.14306", "categories": ["cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2507.14306", "abs": "https://arxiv.org/abs/2507.14306", "authors": ["Samarth P", "Vyoman Jain", "Shiva Golugula", "Motamarri Sai Sathvik"], "title": "Manimator: Transforming Research Papers into Visual Explanations", "comment": null, "summary": "Understanding complex scientific and mathematical concepts, particularly\nthose presented in dense research papers, poses a significant challenge for\nlearners. Dynamic visualizations can greatly enhance comprehension, but\ncreating them manually is time-consuming and requires specialized knowledge and\nskills. We introduce manimator, an open-source system that leverages Large\nLanguage Models to transform research papers and natural language prompts into\nexplanatory animations using the Manim engine. Manimator employs a pipeline\nwhere an LLM interprets the input text or research paper PDF to generate a\nstructured scene description outlining key concepts, mathematical formulas, and\nvisual elements and another LLM translates this description into executable\nManim Python code. We discuss its potential as an educational tool for rapidly\ncreating engaging visual explanations for complex STEM topics, democratizing\nthe creation of high-quality educational content.", "AI": {"tldr": "Manimator\u662f\u4e00\u4e2a\u5f00\u6e90\u7cfb\u7edf\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5c06\u7814\u7a76\u8bba\u6587\u548c\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u8f6c\u5316\u4e3a\u89e3\u91ca\u6027\u52a8\u753b\uff0c\u65e8\u5728\u7b80\u5316\u590d\u6742STEM\u4e3b\u9898\u7684\u53ef\u89c6\u5316\u6559\u80b2\u5185\u5bb9\u521b\u4f5c\u3002", "motivation": "\u7406\u89e3\u590d\u6742\u79d1\u5b66\u548c\u6570\u5b66\u6982\u5ff5\u5bf9\u5b66\u4e60\u8005\u5177\u6709\u6311\u6218\u6027\uff0c\u52a8\u6001\u53ef\u89c6\u5316\u80fd\u63d0\u5347\u7406\u89e3\uff0c\u4f46\u624b\u52a8\u521b\u5efa\u8017\u65f6\u4e14\u9700\u8981\u4e13\u4e1a\u6280\u80fd\u3002", "method": "Manimator\u901a\u8fc7LLM\u89e3\u6790\u8f93\u5165\u6587\u672c\u6216PDF\u751f\u6210\u7ed3\u6784\u5316\u573a\u666f\u63cf\u8ff0\uff0c\u518d\u7531\u53e6\u4e00LLM\u5c06\u5176\u8f6c\u6362\u4e3a\u53ef\u6267\u884c\u7684Manim Python\u4ee3\u7801\u3002", "result": "\u7cfb\u7edf\u80fd\u591f\u5feb\u901f\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u6559\u80b2\u52a8\u753b\uff0c\u964d\u4f4e\u521b\u4f5c\u95e8\u69db\u3002", "conclusion": "Manimator\u6709\u6f5c\u529b\u6210\u4e3a\u6559\u80b2\u5de5\u5177\uff0c\u4fc3\u8fdbSTEM\u5185\u5bb9\u7684\u6c11\u4e3b\u5316\u521b\u4f5c\u3002"}}
{"id": "2507.14594", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.14594", "abs": "https://arxiv.org/abs/2507.14594", "authors": ["Weiwei Xu", "Hengzhi Ye", "Kai Gao", "Minghui Zhou"], "title": "A first look at License Variants in the PyPI Ecosystem", "comment": null, "summary": "Open-source licenses establish the legal foundation for software reuse, yet\nlicense variants, including both modified standard licenses and custom-created\nalternatives, introduce significant compliance complexities. Despite their\nprevalence and potential impact, these variants are poorly understood in modern\nsoftware systems, and existing tools do not account for their existence,\nleading to significant challenges in both effectiveness and efficiency of\nlicense analysis. To fill this knowledge gap, we conduct a comprehensive\nempirical study of license variants in the PyPI ecosystem. Our findings show\nthat textual variations in licenses are common, yet only 2% involve substantive\nmodifications. However, these license variants lead to significant compliance\nissues, with 10.7% of their downstream dependencies found to be\nlicense-incompatible.\n  Inspired by our findings, we introduce LV-Parser, a novel approach for\nefficient license variant analysis leveraging diff-based techniques and large\nlanguage models, along with LV-Compat, an automated pipeline for detecting\nlicense incompatibilities in software dependency networks. Our evaluation\ndemonstrates that LV-Parser achieves an accuracy of 0.936 while reducing\ncomputational costs by 30%, and LV-Compat identifies 5.2 times more\nincompatible packages than existing methods with a precision of 0.98.\n  This work not only provides the first empirical study into license variants\nin software packaging ecosystem but also equips developers and organizations\nwith practical tools for navigating the complex landscape of open-source\nlicensing.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5f00\u6e90\u8bb8\u53ef\u8bc1\u53d8\u4f53\u5728PyPI\u751f\u6001\u7cfb\u7edf\u4e2d\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u4e86LV-Parser\u548cLV-Compat\u5de5\u5177\u4ee5\u63d0\u9ad8\u8bb8\u53ef\u8bc1\u5206\u6790\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u5f00\u6e90\u8bb8\u53ef\u8bc1\u53d8\u4f53\u5728\u73b0\u4ee3\u8f6f\u4ef6\u7cfb\u7edf\u4e2d\u666e\u904d\u5b58\u5728\uff0c\u4f46\u7f3a\u4e4f\u6df1\u5165\u7406\u89e3\u548c\u6709\u6548\u5de5\u5177\uff0c\u5bfc\u81f4\u5408\u89c4\u6027\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\u5206\u6790PyPI\u4e2d\u7684\u8bb8\u53ef\u8bc1\u53d8\u4f53\uff0c\u5e76\u5f00\u53d1\u4e86\u57fa\u4e8e\u5dee\u5f02\u5206\u6790\u548c\u5927\u8bed\u8a00\u6a21\u578b\u7684LV-Parser\uff0c\u4ee5\u53ca\u81ea\u52a8\u5316\u68c0\u6d4b\u5de5\u5177LV-Compat\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u8bb8\u53ef\u8bc1\u53d8\u4f53\u5e38\u89c1\u4f46\u5b9e\u8d28\u6027\u4fee\u6539\u4ec5\u53602%\uff0c\u4f46\u5bfc\u81f410.7%\u7684\u4e0b\u6e38\u4f9d\u8d56\u4e0d\u517c\u5bb9\u3002LV-Parser\u51c6\u786e\u73870.936\uff0c\u8ba1\u7b97\u6210\u672c\u964d\u4f4e30%\uff1bLV-Compat\u68c0\u6d4b\u6548\u7387\u63d0\u53475.2\u500d\uff0c\u7cbe\u5ea60.98\u3002", "conclusion": "\u7814\u7a76\u586b\u8865\u4e86\u8bb8\u53ef\u8bc1\u53d8\u4f53\u7684\u77e5\u8bc6\u7a7a\u767d\uff0c\u5e76\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u5de5\u5177\u4ee5\u5e94\u5bf9\u5f00\u6e90\u8bb8\u53ef\u8bc1\u7684\u590d\u6742\u6027\u3002"}}
{"id": "2507.14213", "categories": ["cs.CR", "cond-mat.mes-hall", "cond-mat.mtrl-sci", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2507.14213", "abs": "https://arxiv.org/abs/2507.14213", "authors": ["Irena Spasojevic", "Federica Celegato", "Alessandro Magni", "Paola Tiberto", "Jordi Sort"], "title": "Magneto-Ionic Hardware Security Primitives: Embedding Data Protection at the Material Level", "comment": null, "summary": "The Big Data revolution has heightened the demand for robust,\nenergy-efficient security hardware capable of withstanding increasingly\nsophisticated cyber threats. Conventional encryption schemes, reliant on\ncomplex algorithms, are resource-intensive and remain vulnerable. To fortify\nsensitive information, society needs innovative anti-hacking and\nanti-counterfeiting technologies that exploit new materials and designs. Here,\nwe present a magneto-ionic strategy for hardware-level security based on fully\nselective voltage-controlled N3- ion migration within pre-defined, initially\nparamagnetic FeCoN dots. This process generates ferromagnetic sublayers of\ntuneable thickness, resulting in either deterministic (single-domain or vortex)\nor probabilistic states (with coexisting magnetic configurations and\nvoltage-adjustable probabilities), each exhibiting stochastic orientation and\nchirality, thereby providing a rich platform for magnetic fingerprinting. This\napproach enables self-protected primitives, including true random number\ngenerators, physical unclonable functions, and in-memory probabilistic\ninference. The resulting reconfigurable architecture combines tamper\nresistance, low energy consumption, and scalability, marking a significant leap\ntoward next-generation hardware security rooted in emergent magnetic phenomena.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u78c1\u79bb\u5b50\u7b56\u7565\u7684\u786c\u4ef6\u7ea7\u5b89\u5168\u6280\u672f\uff0c\u901a\u8fc7\u7535\u538b\u63a7\u5236\u6c2e\u79bb\u5b50\u8fc1\u79fb\u751f\u6210\u53ef\u8c03\u78c1\u6027\u6307\u7eb9\uff0c\u7528\u4e8e\u6297\u9ed1\u5ba2\u548c\u9632\u4f2a\u3002", "motivation": "\u5927\u6570\u636e\u65f6\u4ee3\u5bf9\u9ad8\u6548\u3001\u5b89\u5168\u7684\u786c\u4ef6\u9700\u6c42\u589e\u52a0\uff0c\u4f20\u7edf\u52a0\u5bc6\u65b9\u6848\u8d44\u6e90\u5bc6\u96c6\u4e14\u6613\u53d7\u653b\u51fb\uff0c\u9700\u521b\u65b0\u6280\u672f\u4fdd\u62a4\u654f\u611f\u4fe1\u606f\u3002", "method": "\u5229\u7528\u7535\u538b\u63a7\u5236\u6c2e\u79bb\u5b50\u5728FeCoN\u70b9\u4e2d\u7684\u8fc1\u79fb\uff0c\u751f\u6210\u53ef\u8c03\u539a\u5ea6\u7684\u94c1\u78c1\u5b50\u5c42\uff0c\u5f62\u6210\u786e\u5b9a\u6027\u6216\u6982\u7387\u6027\u78c1\u6027\u72b6\u6001\u3002", "result": "\u5b9e\u73b0\u4e86\u53ef\u91cd\u6784\u67b6\u6784\uff0c\u517c\u5177\u9632\u7be1\u6539\u3001\u4f4e\u80fd\u8017\u548c\u53ef\u6269\u5c55\u6027\uff0c\u652f\u6301\u968f\u673a\u6570\u751f\u6210\u3001\u7269\u7406\u4e0d\u53ef\u514b\u9686\u51fd\u6570\u7b49\u529f\u80fd\u3002", "conclusion": "\u8be5\u6280\u672f\u4e3a\u4e0b\u4e00\u4ee3\u786c\u4ef6\u5b89\u5168\u63d0\u4f9b\u4e86\u57fa\u4e8e\u78c1\u6027\u73b0\u8c61\u7684\u65b0\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.14334", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14334", "abs": "https://arxiv.org/abs/2507.14334", "authors": ["Hui Yang", "Jiaoyan Chen", "Yuan He", "Yongsheng Gao", "Ian Horrocks"], "title": "Language Models as Ontology Encoders", "comment": null, "summary": "OWL (Web Ontology Language) ontologies which are able to formally represent\ncomplex knowledge and support semantic reasoning have been widely adopted\nacross various domains such as healthcare and bioinformatics. Recently,\nontology embeddings have gained wide attention due to its potential to infer\nplausible new knowledge and approximate complex reasoning. However, existing\nmethods face notable limitations: geometric model-based embeddings typically\noverlook valuable textual information, resulting in suboptimal performance,\nwhile the approaches that incorporate text, which are often based on language\nmodels, fail to preserve the logical structure. In this work, we propose a new\nontology embedding method OnT, which tunes a Pretrained Language Model (PLM)\nvia geometric modeling in a hyperbolic space for effectively incorporating\ntextual labels and simultaneously preserving class hierarchies and other\nlogical relationships of Description Logic EL. Extensive experiments on four\nreal-world ontologies show that OnT consistently outperforms the baselines\nincluding the state-of-the-art across both tasks of prediction and inference of\naxioms. OnT also demonstrates strong potential in real-world applications,\nindicated by its robust transfer learning abilities and effectiveness in real\ncases of constructing a new ontology from SNOMED CT. Data and code are\navailable at https://github.com/HuiYang1997/OnT.", "AI": {"tldr": "OnT\u662f\u4e00\u79cd\u65b0\u7684\u672c\u4f53\u5d4c\u5165\u65b9\u6cd5\uff0c\u7ed3\u5408\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u548c\u53cc\u66f2\u7a7a\u95f4\u51e0\u4f55\u5efa\u6a21\uff0c\u6709\u6548\u6574\u5408\u6587\u672c\u6807\u7b7e\u5e76\u4fdd\u7559\u903b\u8f91\u7ed3\u6784\uff0c\u5728\u9884\u6d4b\u548c\u63a8\u7406\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u672c\u4f53\u5d4c\u5165\u65b9\u6cd5\u8981\u4e48\u5ffd\u7565\u6587\u672c\u4fe1\u606f\uff0c\u8981\u4e48\u65e0\u6cd5\u4fdd\u7559\u903b\u8f91\u7ed3\u6784\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u540c\u65f6\u89e3\u51b3\u8fd9\u4e24\u4e2a\u95ee\u9898\u7684\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u53cc\u66f2\u7a7a\u95f4\u4e2d\u7684\u51e0\u4f55\u5efa\u6a21\u8c03\u6574\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff08PLM\uff09\uff0c\u7ed3\u5408\u6587\u672c\u6807\u7b7e\u5e76\u4fdd\u7559\u63cf\u8ff0\u903b\u8f91EL\u7684\u7c7b\u5c42\u6b21\u7ed3\u6784\u548c\u903b\u8f91\u5173\u7cfb\u3002", "result": "\u5728\u56db\u4e2a\u771f\u5b9e\u4e16\u754c\u672c\u4f53\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cOnT\u5728\u9884\u6d4b\u548c\u63a8\u7406\u4efb\u52a1\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u8fc1\u79fb\u5b66\u4e60\u548c\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002", "conclusion": "OnT\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u672c\u4f53\u5d4c\u5165\u65b9\u6cd5\uff0c\u80fd\u591f\u540c\u65f6\u5229\u7528\u6587\u672c\u4fe1\u606f\u548c\u903b\u8f91\u7ed3\u6784\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u548c\u77e5\u8bc6\u63a8\u7406\u3002"}}
{"id": "2507.14687", "categories": ["cs.SE", "68Q60, 03B70", "D.2.5"], "pdf": "https://arxiv.org/pdf/2507.14687", "abs": "https://arxiv.org/abs/2507.14687", "authors": ["Robin Lee", "Youngho Nam"], "title": "An Efficient Algorithm for Generating Minimal Unique-Cause MC/DC Test cases for Singular Boolean Expressions", "comment": "10 pages, 5 figures", "summary": "Modified Condition/Decision Coverage (MC/DC) is a mandatory structural\ncoverage criterion for ensuring the reliability and safety of critical systems.\nWhile its strictest form, Unique-Cause MC/DC, offers the highest assurance,\nresearch on its efficient test generation has been lacking. This gap is\nparticularly significant, as an analysis of large-scale avionics systems shows\nthat 99.7% of all conditional decisions are, in fact, Singular Boolean\nExpressions (SBEs) the ideal structure for applying Unique-Cause MC/DC. This\npaper proposes 'Robin's Rule', a deterministic algorithm that directly\nconstructs a minimal test set of N + 1 cases to guarantee 100% Unique-Cause\nMC/DC for SBEs with N conditions, without generating a full truth table. To\nvalidate our approach, we constructed a benchmark by reformulating the TCAS-II\nspecifications into SBEs and verified the results using an industry-standard,\ncertified commercial tool. The results confirm that our method consistently\nachieves 100% coverage with the theoretical minimum number of tests and is more\nefficient than the commercial tool. This work provides a practical and provably\noptimal solution for verifying safety-critical systems, ensuring both rigor and\nefficiency.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a'Robin's Rule'\u7684\u786e\u5b9a\u6027\u7b97\u6cd5\uff0c\u7528\u4e8e\u9ad8\u6548\u751f\u6210\u6ee1\u8db3Unique-Cause MC/DC\u7684\u6700\u5c0f\u6d4b\u8bd5\u96c6\uff0c\u7279\u522b\u9002\u7528\u4e8e\u5355\u4e00\u5e03\u5c14\u8868\u8fbe\u5f0f\uff08SBEs\uff09\u3002", "motivation": "\u5c3d\u7ba1Unique-Cause MC/DC\u5728\u5173\u952e\u7cfb\u7edf\u4e2d\u63d0\u4f9b\u6700\u9ad8\u53ef\u9760\u6027\u4fdd\u969c\uff0c\u4f46\u5176\u9ad8\u6548\u6d4b\u8bd5\u751f\u6210\u65b9\u6cd5\u7814\u7a76\u4e0d\u8db3\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u63d0\u51fa'Robin's Rule'\u7b97\u6cd5\uff0c\u76f4\u63a5\u6784\u9020\u6700\u5c0f\u6d4b\u8bd5\u96c6\uff08N + 1\u4e2a\u7528\u4f8b\uff09\uff0c\u65e0\u9700\u751f\u6210\u5b8c\u6574\u771f\u503c\u8868\u3002", "result": "\u5728TCAS-II\u89c4\u8303\u9a8c\u8bc1\u4e2d\uff0c\u8be5\u65b9\u6cd5\u4ee5\u7406\u8bba\u6700\u5c0f\u6d4b\u8bd5\u6570\u5b9e\u73b0100%\u8986\u76d6\u7387\uff0c\u4e14\u6548\u7387\u4f18\u4e8e\u5546\u4e1a\u5de5\u5177\u3002", "conclusion": "'Robin's Rule'\u4e3a\u5b89\u5168\u5173\u952e\u7cfb\u7edf\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u6700\u4f18\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u517c\u5177\u4e25\u8c28\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2507.14222", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.14222", "abs": "https://arxiv.org/abs/2507.14222", "authors": ["Shu-Ting Huang", "Wen-Cheng Chung", "Hao-Ting Pai"], "title": "GPU-Accelerated Interpretable Generalization for Rapid Cyberattack Detection and Forensics", "comment": "ACM CCS 2025 (Submitted)", "summary": "The Interpretable Generalization (IG) mechanism recently published in IEEE\nTransactions on Information Forensics and Security delivers state-of-the-art,\nevidence-based intrusion detection by discovering coherent normal and attack\npatterns through exhaustive intersect-and-subset operations-yet its cubic-time\ncomplexity and large intermediate bitsets render full-scale datasets\nimpractical on CPUs. We present IG-GPU, a PyTorch re-architecture that offloads\nall pairwise intersections and subset evaluations to commodity GPUs.\nImplemented on a single NVIDIA RTX 4070 Ti, in the 15k-record NSL-KDD dataset,\nIG-GPU shows a 116-fold speed-up over the multi-core CPU implementation of IG.\nIn the full size of NSL-KDD (148k-record), given small training data (e.g.,\n10%-90% train-test split), IG-GPU runs in 18 minutes with Recall 0.957,\nPrecision 0.973, and AUC 0.961, whereas IG required down-sampling to\n15k-records to avoid memory exhaustion and obtained Recall 0.935, Precision\n0.942, and AUC 0.940. The results confirm that IG-GPU is robust across scales\nand could provide millisecond-level per-flow inference once patterns are\nlearned. IG-GPU thus bridges the gap between rigorous interpretability and\nreal-time cyber-defense, offering a portable foundation for future work on\nhardware-aware scheduling, multi-GPU sharding, and dataset-specific sparsity\noptimizations.", "AI": {"tldr": "IG-GPU\u662f\u4e00\u79cd\u57fa\u4e8eGPU\u7684PyTorch\u91cd\u6784\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86IG\u673a\u5236\u5728\u5165\u4fb5\u68c0\u6d4b\u4e2d\u7684\u6027\u80fd\uff0c\u5b9e\u73b0\u4e86116\u500d\u7684\u52a0\u901f\uff0c\u5e76\u5728\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u4fdd\u6301\u4e86\u9ad8\u51c6\u786e\u7387\u3002", "motivation": "\u89e3\u51b3IG\u673a\u5236\u5728CPU\u4e0a\u8fd0\u884c\u65f6\u56e0\u7acb\u65b9\u65f6\u95f4\u590d\u6742\u5ea6\u548c\u5927\u4e2d\u95f4\u4f4d\u96c6\u5bfc\u81f4\u7684\u5168\u89c4\u6a21\u6570\u636e\u96c6\u5904\u7406\u4e0d\u5207\u5b9e\u9645\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5c06\u6210\u5bf9\u4ea4\u96c6\u548c\u5b50\u96c6\u8bc4\u4f30\u5378\u8f7d\u5230GPU\u4e0a\uff0c\u5229\u7528PyTorch\u91cd\u6784IG\u673a\u5236\u3002", "result": "\u5728NSL-KDD\u6570\u636e\u96c6\u4e0a\uff0cIG-GPU\u5b9e\u73b0\u4e86116\u500d\u52a0\u901f\uff0c\u5e76\u5728\u5168\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u4fdd\u6301\u4e86\u9ad8\u53ec\u56de\u7387\uff080.957\uff09\u3001\u7cbe\u786e\u7387\uff080.973\uff09\u548cAUC\uff080.961\uff09\u3002", "conclusion": "IG-GPU\u586b\u8865\u4e86\u4e25\u683c\u53ef\u89e3\u91ca\u6027\u4e0e\u5b9e\u65f6\u7f51\u7edc\u9632\u5fa1\u4e4b\u95f4\u7684\u9e3f\u6c9f\uff0c\u4e3a\u672a\u6765\u786c\u4ef6\u611f\u77e5\u8c03\u5ea6\u548c\u591aGPU\u5206\u7247\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2507.14335", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14335", "abs": "https://arxiv.org/abs/2507.14335", "authors": ["Nicolas Wischermann", "Claudio Mayrink Verdun", "Gabriel Poesia", "Francesco Noseda"], "title": "ProofCompass: Enhancing Specialized Provers with LLM Guidance", "comment": "19 pages, 7 figures. Accepted at the 2nd AI for MATH Workshop at the\n  42nd International Conference on Machine Learning (ICML 2025)", "summary": "Language models have become increasingly powerful tools for formal\nmathematical reasoning. However, most existing approaches rely exclusively on\neither large general-purpose models or smaller specialized models, each with\ndistinct limitations, while training specialized large models still requires\nsignificant computational resources. This paper introduces ProofCompass, a\nnovel hybrid methodology that achieves remarkable computational efficiency by\nstrategically guiding existing specialized prover methods, such as\nDeepSeek-Prover-v1.5-RL (DSP-v1.5) with a Large Language Model (LLM) without\nrequiring additional model training. The LLM provides natural language proof\nstrategies and analyzes failed attempts to select intermediate lemmas, enabling\neffective problem decomposition. On the miniF2F benchmark, ProofCompass\ndemonstrates substantial resource efficiency: it outperforms DSP-v1.5 ($54.9\\%\n\\rightarrow 55.3\\%$) while using 25x fewer attempts ($3200 \\rightarrow 128$).\nOur synergistic approach paves the way for simultaneously improving\ncomputational efficiency and accuracy in formal theorem proving.", "AI": {"tldr": "ProofCompass\u662f\u4e00\u79cd\u6df7\u5408\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u548c\u4e13\u7528\u8bc1\u660e\u5668\uff08\u5982DSP-v1.5\uff09\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6570\u5b66\u63a8\u7406\u7684\u8ba1\u7b97\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u5927\u578b\u901a\u7528\u6a21\u578b\uff0c\u8981\u4e48\u4f9d\u8d56\u5c0f\u578b\u4e13\u7528\u6a21\u578b\uff0c\u5404\u6709\u5c40\u9650\u6027\uff0c\u4e14\u8bad\u7ec3\u5927\u578b\u4e13\u7528\u6a21\u578b\u8ba1\u7b97\u6210\u672c\u9ad8\u3002", "method": "ProofCompass\u5229\u7528LLM\u63d0\u4f9b\u81ea\u7136\u8bed\u8a00\u8bc1\u660e\u7b56\u7565\u5e76\u5206\u6790\u5931\u8d25\u5c1d\u8bd5\uff0c\u6307\u5bfc\u4e13\u7528\u8bc1\u660e\u5668\uff08\u5982DSP-v1.5\uff09\u8fdb\u884c\u95ee\u9898\u5206\u89e3\u3002", "result": "\u5728miniF2F\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cProofCompass\u4ee525\u500d\u66f4\u5c11\u7684\u5c1d\u8bd5\uff08128 vs 3200\uff09\u5c06\u51c6\u786e\u7387\u4ece54.9%\u63d0\u5347\u81f355.3%\u3002", "conclusion": "ProofCompass\u5c55\u793a\u4e86\u5728\u5f62\u5f0f\u5b9a\u7406\u8bc1\u660e\u4e2d\u540c\u65f6\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u548c\u51c6\u786e\u6027\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.14716", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.14716", "abs": "https://arxiv.org/abs/2507.14716", "authors": ["Shahidul Islam", "Ashik Aowal", "Md Sharif Uddin", "Shaiful Chowdhury"], "title": "HistoryFinder: Advancing Method-Level Source Code History Generation with Accurate Oracles and Enhanced Algorithm", "comment": null, "summary": "Reconstructing a method's change history efficiently and accurately is\ncritical for many software engineering tasks, including maintenance,\nrefactoring, and comprehension. Despite the availability of method history\ngeneration tools such as CodeShovel and CodeTracker, existing evaluations of\ntheir effectiveness are limited by inaccuracies in the ground truth oracles\nused. In this study, we systematically construct two new oracles -- the\ncorrected CodeShovel oracle and a newly developed HistoryFinder oracle -- by\ncombining automated analysis with expert-guided manual validation. We also\nintroduce HistoryFinder, a new method history generation tool designed to\nimprove not only the accuracy and completeness of method change histories but\nalso to offer competitive runtime performance. Through extensive evaluation\nacross 400 methods from 40 open-source repositories, we show that HistoryFinder\nconsistently outperforms CodeShovel, CodeTracker, IntelliJ, and Git-based\nbaselines in terms of precision, recall, and F1 score. Moreover, HistoryFinder\nachieves competitive runtime performance, offering the lowest mean and median\nexecution times among all the research-based tools.\n  While Git-based tools exhibit the fastest runtimes, this efficiency comes at\nthe cost of significantly lower precision and recall -- leaving HistoryFinder\nas the best overall choice when both accuracy and efficiency are important. To\nfacilitate adoption, we provide a web interface, CLI, and Java library for\nflexible usage.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u5386\u53f2\u751f\u6210\u5de5\u5177HistoryFinder\uff0c\u901a\u8fc7\u6784\u5efa\u66f4\u51c6\u786e\u7684\u57fa\u51c6\uff08oracle\uff09\u5e76\u4f18\u5316\u6027\u80fd\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u5de5\u5177\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5386\u53f2\u751f\u6210\u5de5\u5177\u7684\u8bc4\u4f30\u56e0\u57fa\u51c6\u4e0d\u51c6\u786e\u800c\u53d7\u9650\uff0c\u5f71\u54cd\u4e86\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "method": "\u7ed3\u5408\u81ea\u52a8\u5316\u5206\u6790\u548c\u4e13\u5bb6\u624b\u52a8\u9a8c\u8bc1\u6784\u5efa\u4e86\u4e24\u4e2a\u65b0\u57fa\u51c6\uff08oracle\uff09\uff0c\u5e76\u5f00\u53d1\u4e86HistoryFinder\u5de5\u5177\u3002", "result": "\u5728400\u4e2a\u65b9\u6cd5\u7684\u8bc4\u4f30\u4e2d\uff0cHistoryFinder\u5728\u7cbe\u5ea6\u3001\u53ec\u56de\u7387\u548cF1\u5206\u6570\u4e0a\u5747\u4f18\u4e8e\u5176\u4ed6\u5de5\u5177\uff0c\u4e14\u8fd0\u884c\u65f6\u95f4\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "HistoryFinder\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u4e0a\u5747\u8868\u73b0\u6700\u4f73\uff0c\u9002\u5408\u9700\u8981\u9ad8\u7cbe\u5ea6\u548c\u9ad8\u6027\u80fd\u7684\u573a\u666f\u3002"}}
{"id": "2507.14223", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14223", "abs": "https://arxiv.org/abs/2507.14223", "authors": ["Wen-Cheng Chung", "Shu-Ting Huang", "Hao-Ting Pai"], "title": "Multi-Granular Discretization for Interpretable Generalization in Precise Cyberattack Identification", "comment": "ACM CCS 2025 (Submitted)", "summary": "Explainable intrusion detection systems (IDS) are now recognized as essential\nfor mission-critical networks, yet most \"XAI\" pipelines still bolt an\napproximate explainer onto an opaque classifier, leaving analysts with partial\nand sometimes misleading insights. The Interpretable Generalization (IG)\nmechanism, published in IEEE Transactions on Information Forensics and\nSecurity, eliminates that bottleneck by learning coherent patterns - feature\ncombinations unique to benign or malicious traffic - and turning them into\nfully auditable rules. IG already delivers outstanding precision, recall, and\nAUC on NSL-KDD, UNSW-NB15, and UKM-IDS20, even when trained on only 10% of the\ndata. To raise precision further without sacrificing transparency, we introduce\nMulti-Granular Discretization (IG-MD), which represents every continuous\nfeature at several Gaussian-based resolutions. On UKM-IDS20, IG-MD lifts\nprecision by greater than or equal to 4 percentage points across all nine\ntrain-test splits while preserving recall approximately equal to 1.0,\ndemonstrating that a single interpretation-ready model can scale across domains\nwithout bespoke tuning.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u89e3\u91ca\u7684\u5165\u4fb5\u68c0\u6d4b\u7cfb\u7edf\uff08IG-MD\uff09\uff0c\u901a\u8fc7\u591a\u7c92\u5ea6\u79bb\u6563\u5316\u6280\u672f\u63d0\u5347\u7cbe\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u53ec\u56de\u7387\u548c\u900f\u660e\u5ea6\u3002", "motivation": "\u73b0\u6709XAI\u65b9\u6cd5\u5728\u5165\u4fb5\u68c0\u6d4b\u7cfb\u7edf\u4e2d\u63d0\u4f9b\u90e8\u5206\u6216\u8bef\u5bfc\u6027\u89e3\u91ca\uff0c\u9700\u8981\u4e00\u79cd\u5b8c\u5168\u53ef\u5ba1\u8ba1\u7684\u89c4\u5219\u751f\u6210\u65b9\u6cd5\u3002", "method": "\u91c7\u7528Interpretable Generalization\uff08IG\uff09\u673a\u5236\u5b66\u4e60\u7279\u5f81\u7ec4\u5408\uff0c\u5e76\u5f15\u5165Multi-Granular Discretization\uff08IG-MD\uff09\u6280\u672f\u5904\u7406\u8fde\u7eed\u7279\u5f81\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0cIG-MD\u5728UKM-IDS20\u4e0a\u63d0\u5347\u7cbe\u5ea6\u22654%\uff0c\u53ec\u56de\u7387\u4fdd\u6301\u22481.0\u3002", "conclusion": "IG-MD\u5c55\u793a\u4e86\u5355\u4e00\u6a21\u578b\u53ef\u8de8\u57df\u6269\u5c55\uff0c\u65e0\u9700\u5b9a\u5236\u8c03\u6574\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u900f\u660e\u5ea6\u548c\u6027\u80fd\u3002"}}
{"id": "2507.14393", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14393", "abs": "https://arxiv.org/abs/2507.14393", "authors": ["Humza Sami", "Mubashir ul Islam", "Pierre-Emmanuel Gaillardon", "Valerio Tenace"], "title": "Adaptive Multi-Agent Reasoning via Automated Workflow Generation", "comment": null, "summary": "The rise of Large Reasoning Models (LRMs) promises a significant leap forward\nin language model capabilities, aiming to tackle increasingly sophisticated\ntasks with unprecedented efficiency and accuracy. However, despite their\nimpressive performance, recent studies have highlighted how current reasoning\nmodels frequently fail to generalize to novel, unseen problems, often resorting\nto memorized solutions rather than genuine inferential reasoning. Such behavior\nunderscores a critical limitation in modern LRMs, i.e., their tendency toward\noverfitting, which in turn results in poor generalization in problem-solving\ncapabilities.\n  In this paper, we introduce Nexus Architect, an enhanced iteration of our\nmulti-agent system framework, Nexus, equipped with a novel automated workflow\nsynthesis mechanism. Given a user's prompt and a small set of representative\nexamples, the Architect autonomously generates a tailored reasoning workflow by\nselecting suitable strategies, tool integrations, and adversarial techniques\nfor a specific problem class. Furthermore, the Architect includes an iterative\nprompt refinement mechanism that fine-tunes agents' system prompts to maximize\nperformance and improve the generalization capabilities of the system.\n  We empirically evaluate Nexus Architect by employing an off-the-shelf,\nnon-reasoning model on a custom dataset of challenging logical questions and\ncompare its performance against state-of-the-art LRMs. Results show that Nexus\nArchitect consistently outperforms existing solutions, achieving up to a 66%\nincrease in pass rate over Gemini 2.5 Flash Preview, nearly 2.5$\\times$ against\nClaude Sonnet 4 and DeepSeek-R1, and over 3$\\times$ w.r.t. Llama 4 Scout.", "AI": {"tldr": "Nexus Architect\u662f\u4e00\u79cd\u6539\u8fdb\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u5de5\u4f5c\u6d41\u5408\u6210\u673a\u5236\u63d0\u5347\u63a8\u7406\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u5927\u578b\u63a8\u7406\u6a21\u578b\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u5728\u89e3\u51b3\u65b0\u95ee\u9898\u65f6\u4f9d\u8d56\u8bb0\u5fc6\u800c\u975e\u63a8\u7406\uff0c\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0cNexus Architect\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "Nexus Architect\u901a\u8fc7\u81ea\u52a8\u5316\u5de5\u4f5c\u6d41\u5408\u6210\u673a\u5236\uff0c\u6839\u636e\u7528\u6237\u63d0\u793a\u548c\u793a\u4f8b\u751f\u6210\u5b9a\u5236\u5316\u63a8\u7406\u6d41\u7a0b\uff0c\u5e76\u7ed3\u5408\u8fed\u4ee3\u63d0\u793a\u4f18\u5316\u673a\u5236\u63d0\u5347\u6027\u80fd\u3002", "result": "\u5728\u903b\u8f91\u95ee\u9898\u6570\u636e\u96c6\u4e0a\uff0cNexus Architect\u8868\u73b0\u4f18\u4e8e\u73b0\u6709LRMs\uff0c\u6700\u9ad8\u63d0\u534766%\u901a\u8fc7\u7387\u3002", "conclusion": "Nexus Architect\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6027\u80fd\uff0c\u4e3a\u590d\u6742\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.14735", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.14735", "abs": "https://arxiv.org/abs/2507.14735", "authors": ["Vladyslav Bulhakov", "Giordano d'Aloisio", "Claudio Di Sipio", "Antinisca Di Marco", "Davide Di Ruscio"], "title": "Investigating the Role of LLMs Hyperparameter Tuning and Prompt Engineering to Support Domain Modeling", "comment": "Accepted at 51st Euromicro Conference Series on Software Engineering\n  and Advanced Applications (SEAA)", "summary": "The introduction of large language models (LLMs) has enhanced automation in\nsoftware engineering tasks, including in Model Driven Engineering (MDE).\nHowever, using general-purpose LLMs for domain modeling has its limitations.\nOne approach is to adopt fine-tuned models, but this requires significant\ncomputational resources and can lead to issues like catastrophic forgetting.\n  This paper explores how hyperparameter tuning and prompt engineering can\nimprove the accuracy of the Llama 3.1 model for generating domain models from\ntextual descriptions. We use search-based methods to tune hyperparameters for a\nspecific medical data model, resulting in a notable quality improvement over\nthe baseline LLM. We then test the optimized hyperparameters across ten diverse\napplication domains.\n  While the solutions were not universally applicable, we demonstrate that\ncombining hyperparameter tuning with prompt engineering can enhance results\nacross nearly all examined domain models.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u901a\u8fc7\u8d85\u53c2\u6570\u8c03\u6574\u548c\u63d0\u793a\u5de5\u7a0b\u63d0\u9ad8Llama 3.1\u6a21\u578b\u5728\u751f\u6210\u9886\u57df\u6a21\u578b\u65f6\u7684\u51c6\u786e\u6027\uff0c\u5e76\u5728\u533b\u7597\u6570\u636e\u6a21\u578b\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\u3002", "motivation": "\u901a\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u9886\u57df\u5efa\u6a21\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff0c\u800c\u5fae\u8c03\u6a21\u578b\u9700\u8981\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\u4e14\u53ef\u80fd\u5bfc\u81f4\u707e\u96be\u6027\u9057\u5fd8\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u641c\u7d22\u7684\u65b9\u6cd5\u8c03\u6574\u8d85\u53c2\u6570\uff0c\u5e76\u7ed3\u5408\u63d0\u793a\u5de5\u7a0b\uff0c\u9488\u5bf9\u533b\u7597\u6570\u636e\u6a21\u578b\u8fdb\u884c\u4f18\u5316\u3002", "result": "\u4f18\u5316\u540e\u7684\u6a21\u578b\u5728\u533b\u7597\u6570\u636e\u6a21\u578b\u4e2d\u8868\u73b0\u663e\u8457\u4f18\u4e8e\u57fa\u7ebfLLM\uff0c\u5e76\u5728\u5176\u4ed6\u5341\u4e2a\u5e94\u7528\u9886\u57df\u4e2d\u591a\u6570\u60c5\u51b5\u4e0b\u8868\u73b0\u66f4\u597d\u3002", "conclusion": "\u8d85\u53c2\u6570\u8c03\u6574\u4e0e\u63d0\u793a\u5de5\u7a0b\u7ed3\u5408\u53ef\u63d0\u5347\u9886\u57df\u6a21\u578b\u751f\u6210\u7684\u6548\u679c\uff0c\u4f46\u89e3\u51b3\u65b9\u6848\u5e76\u975e\u666e\u904d\u9002\u7528\u3002"}}
{"id": "2507.14229", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.14229", "abs": "https://arxiv.org/abs/2507.14229", "authors": ["Vanja Stojanovi\u0107", "\u017diga Lesar", "CIril Bohak"], "title": "Using Modular Arithmetic Optimized Neural Networks To Crack Affine Cryptographic Schemes Efficiently", "comment": null, "summary": "We investigate the cryptanalysis of affine ciphers using a hybrid neural\nnetwork architecture that combines modular arithmetic-aware and statistical\nfeature-based learning. Inspired by recent advances in interpretable neural\nnetworks for modular arithmetic and neural cryptanalysis of classical ciphers,\nour approach integrates a modular branch that processes raw ciphertext\nsequences and a statistical branch that leverages letter frequency features.\nExperiments on datasets derived from natural English text demonstrate that the\nhybrid model attains high key recovery accuracy for short and moderate\nciphertexts, outperforming purely statistical approaches for the affine cipher.\nHowever, performance degrades for very long ciphertexts, highlighting\nchallenges in model generalization.", "AI": {"tldr": "\u6df7\u5408\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u7528\u4e8e\u4eff\u5c04\u5bc6\u7801\u5206\u6790\uff0c\u7ed3\u5408\u6a21\u7b97\u672f\u548c\u7edf\u8ba1\u7279\u5f81\u5b66\u4e60\uff0c\u5728\u77ed\u5230\u4e2d\u7b49\u957f\u5ea6\u5bc6\u6587\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5bf9\u957f\u5bc6\u6587\u6cdb\u5316\u80fd\u529b\u6709\u9650\u3002", "motivation": "\u53d7\u53ef\u89e3\u91ca\u795e\u7ecf\u7f51\u7edc\u5728\u6a21\u7b97\u672f\u548c\u7ecf\u5178\u5bc6\u7801\u5206\u6790\u4e2d\u7684\u8fdb\u5c55\u542f\u53d1\uff0c\u7814\u7a76\u5982\u4f55\u7ed3\u5408\u6a21\u7b97\u672f\u548c\u7edf\u8ba1\u7279\u5f81\u63d0\u9ad8\u4eff\u5c04\u5bc6\u7801\u7684\u5bc6\u94a5\u6062\u590d\u51c6\u786e\u6027\u3002", "method": "\u91c7\u7528\u6df7\u5408\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u5305\u542b\u5904\u7406\u539f\u59cb\u5bc6\u6587\u7684\u6a21\u7b97\u672f\u5206\u652f\u548c\u5229\u7528\u5b57\u6bcd\u9891\u7387\u7279\u5f81\u7684\u7edf\u8ba1\u5206\u652f\u3002", "result": "\u5728\u81ea\u7136\u82f1\u8bed\u6587\u672c\u6570\u636e\u96c6\u4e0a\uff0c\u6df7\u5408\u6a21\u578b\u5728\u77ed\u5230\u4e2d\u7b49\u957f\u5ea6\u5bc6\u6587\u4e2d\u8868\u73b0\u51fa\u9ad8\u5bc6\u94a5\u6062\u590d\u51c6\u786e\u6027\uff0c\u4f18\u4e8e\u7eaf\u7edf\u8ba1\u65b9\u6cd5\u3002", "conclusion": "\u6df7\u5408\u6a21\u578b\u5728\u77ed\u5230\u4e2d\u7b49\u957f\u5ea6\u5bc6\u6587\u4e2d\u6709\u6548\uff0c\u4f46\u5bf9\u957f\u5bc6\u6587\u7684\u6cdb\u5316\u80fd\u529b\u6709\u5f85\u6539\u8fdb\u3002"}}
{"id": "2507.14406", "categories": ["cs.AI", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.14406", "abs": "https://arxiv.org/abs/2507.14406", "authors": ["Michael J. Zellinger", "Matt Thomson"], "title": "Fail Fast, or Ask: Mitigating the Deficiencies of Reasoning LLMs with Human-in-the-Loop Systems Engineering", "comment": "8 pages, 5 figures", "summary": "State-of-the-art reasoning LLMs are powerful problem solvers, but they still\noccasionally make mistakes. However, adopting AI models in risk-sensitive\ndomains often requires error rates near 0%. To address this gap, we propose\ncollaboration between a reasoning model and a human expert who resolves queries\nthe model cannot confidently answer. We find that quantifying the uncertainty\nof a reasoning model through the length of its reasoning trace yields an\neffective basis for deferral to a human, e.g., cutting the error rate of Qwen3\n235B-A22B on difficult MATH problems from 3% to less than 1% when deferring\n7.5% of queries. However, the high latency of reasoning models still makes them\nchallenging to deploy on use cases with high query volume. To address this\nchallenge, we explore fronting a reasoning model with a large non-reasoning\nmodel. We call this modified human-in-the-loop system \"Fail Fast, or Ask\",\nsince the non-reasoning model may defer difficult queries to the human expert\ndirectly (\"failing fast\"), without incurring the reasoning model's higher\nlatency. We show that this approach yields around 40% latency reduction and\nabout 50% cost savings for DeepSeek R1 while maintaining 90+% area under the\naccuracy-rejection curve. However, we observe that latency savings are lower\nthan expected because of \"latency drag\", the phenomenon that processing easier\nqueries with a non-reasoning model pushes the reasoning model's latency\ndistribution towards longer latencies. Broadly, our results suggest that the\ndeficiencies of state-of-the-art reasoning models -- nontrivial error rates and\nhigh latency -- can be substantially mitigated through black-box systems\nengineering, without requiring access to LLM internals.", "AI": {"tldr": "\u901a\u8fc7\u7ed3\u5408\u63a8\u7406\u6a21\u578b\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u7684\u534f\u4f5c\uff0c\u4ee5\u53ca\u5f15\u5165\u975e\u63a8\u7406\u6a21\u578b\u5feb\u901f\u7b5b\u9009\u95ee\u9898\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u9519\u8bef\u7387\u548c\u5ef6\u8fdf\uff0c\u540c\u65f6\u8282\u7701\u6210\u672c\u3002", "motivation": "\u5728\u98ce\u9669\u654f\u611f\u9886\u57df\uff0cAI\u6a21\u578b\u7684\u9519\u8bef\u7387\u9700\u63a5\u8fd10%\uff0c\u800c\u73b0\u6709\u63a8\u7406\u6a21\u578b\u4ecd\u5b58\u5728\u9519\u8bef\u7387\u9ad8\u548c\u5ef6\u8fdf\u5927\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u534f\u4f5c\u7cfb\u7edf\uff1a1) \u63a8\u7406\u6a21\u578b\u901a\u8fc7\u63a8\u7406\u8f68\u8ff9\u957f\u5ea6\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\uff0c\u5c06\u4e0d\u786e\u5b9a\u95ee\u9898\u8f6c\u4ea4\u4eba\u7c7b\u4e13\u5bb6\uff1b2) \u5f15\u5165\u975e\u63a8\u7406\u6a21\u578b\u5feb\u901f\u7b5b\u9009\u95ee\u9898\uff0c\u76f4\u63a5\u8f6c\u4ea4\u4eba\u7c7b\u4ee5\u51cf\u5c11\u5ef6\u8fdf\u3002", "result": "\u9519\u8bef\u7387\u4ece3%\u964d\u81f31%\u4ee5\u4e0b\uff0c\u5ef6\u8fdf\u51cf\u5c1140%\uff0c\u6210\u672c\u8282\u770150%\uff0c\u540c\u65f6\u4fdd\u630190%\u4ee5\u4e0a\u7684\u51c6\u786e\u7387\u3002", "conclusion": "\u901a\u8fc7\u9ed1\u76d2\u7cfb\u7edf\u5de5\u7a0b\uff0c\u663e\u8457\u6539\u5584\u4e86\u63a8\u7406\u6a21\u578b\u7684\u9519\u8bef\u7387\u548c\u5ef6\u8fdf\u95ee\u9898\uff0c\u65e0\u9700\u4fee\u6539\u6a21\u578b\u5185\u90e8\u3002"}}
{"id": "2507.14770", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.14770", "abs": "https://arxiv.org/abs/2507.14770", "authors": ["Manaal Basha", "Ivan Beschastnikh", "Gema Rodriguez-Perez", "Cleidson R. B. de Souza"], "title": "Toward Inclusive AI-Driven Development: Exploring Gender Differences in Code Generation Tool Interactions", "comment": "ESEM 2025 Registered Reports", "summary": "Context: The increasing reliance on Code Generation Tools (CGTs), such as\nWindsurf and GitHub Copilot, are revamping programming workflows and raising\ncritical questions about fairness and inclusivity. While CGTs offer potential\nproductivity enhancements, their effectiveness across diverse user groups have\nnot been sufficiently investigated. Objectives: We hypothesize that developers'\ninteractions with CGTs vary based on gender, influencing task outcomes and\ncognitive load, as prior research suggests that gender differences can affect\ntechnology use and cognitive processing. Methods: The study will employ a\nmixed-subjects design with 54 participants, evenly divided by gender for a\ncounterbalanced design. Participants will complete two programming tasks\n(medium to hard difficulty) with only CGT assistance and then with only\ninternet access. Task orders and conditions will be counterbalanced to mitigate\norder effects. Data collection will include cognitive load surveys, screen\nrecordings, and task performance metrics such as completion time, code\ncorrectness, and CGT interaction behaviors. Statistical analyses will be\nconducted to identify statistically significant differences in CGT usage.\nExpected Contributions: Our work can uncover gender differences in CGT\ninteraction and performance among developers. Our findings can inform future\nCGT designs and help address usability and potential disparities in interaction\npatterns across diverse user groups. Conclusion: While results are not yet\navailable, our proposal lays the groundwork for advancing fairness,\naccountability, transparency, and ethics (FATE) in CGT design. The outcomes are\nanticipated to contribute to inclusive AI practices and equitable tool\ndevelopment for all users.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4ee3\u7801\u751f\u6210\u5de5\u5177\uff08CGTs\uff09\u5728\u4e0d\u540c\u6027\u522b\u5f00\u53d1\u8005\u4e2d\u7684\u4f7f\u7528\u5dee\u5f02\uff0c\u5206\u6790\u5176\u5bf9\u4efb\u52a1\u7ed3\u679c\u548c\u8ba4\u77e5\u8d1f\u8377\u7684\u5f71\u54cd\uff0c\u65e8\u5728\u63a8\u52a8\u516c\u5e73\u6027\u548c\u5305\u5bb9\u6027\u3002", "motivation": "\u968f\u7740\u4ee3\u7801\u751f\u6210\u5de5\u5177\u7684\u666e\u53ca\uff0c\u5176\u516c\u5e73\u6027\u548c\u5305\u5bb9\u6027\u95ee\u9898\u65e5\u76ca\u7a81\u51fa\uff0c\u4f46\u9488\u5bf9\u4e0d\u540c\u7528\u6237\u7fa4\u4f53\u7684\u6709\u6548\u6027\u5c1a\u672a\u5145\u5206\u7814\u7a76\u3002", "method": "\u91c7\u7528\u6df7\u5408\u88ab\u8bd5\u8bbe\u8ba1\uff0c54\u540d\u53c2\u4e0e\u8005\u6309\u6027\u522b\u5747\u5206\uff0c\u5b8c\u6210\u7f16\u7a0b\u4efb\u52a1\u5e76\u6536\u96c6\u8ba4\u77e5\u8d1f\u8377\u3001\u4efb\u52a1\u8868\u73b0\u7b49\u6570\u636e\u3002", "result": "\u9884\u8ba1\u5c06\u63ed\u793a\u6027\u522b\u5dee\u5f02\u5bf9CGT\u4f7f\u7528\u7684\u5f71\u54cd\uff0c\u4e3a\u672a\u6765\u5de5\u5177\u8bbe\u8ba1\u63d0\u4f9b\u4f9d\u636e\u3002", "conclusion": "\u7814\u7a76\u4e3aCGT\u7684\u516c\u5e73\u6027\u3001\u900f\u660e\u6027\u548c\u5305\u5bb9\u6027\u8bbe\u8ba1\u5960\u5b9a\u57fa\u7840\uff0c\u63a8\u52a8AI\u5de5\u5177\u7684\u5e73\u7b49\u53d1\u5c55\u3002"}}
{"id": "2507.14248", "categories": ["cs.CR", "cs.AI", "cs.CV", "cs.LG", "I.2.10; I.2.6; I.5.1; D.4.6; K.6.5"], "pdf": "https://arxiv.org/pdf/2507.14248", "abs": "https://arxiv.org/abs/2507.14248", "authors": ["Eldor Abdukhamidov", "Mohammed Abuhamad", "Simon S. Woo", "Hyoungshick Kim", "Tamer Abuhmed"], "title": "Breaking the Illusion of Security via Interpretation: Interpretable Vision Transformer Systems under Attack", "comment": null, "summary": "Vision transformer (ViT) models, when coupled with interpretation models, are\nregarded as secure and challenging to deceive, making them well-suited for\nsecurity-critical domains such as medical applications, autonomous vehicles,\ndrones, and robotics. However, successful attacks on these systems can lead to\nsevere consequences. Recent research on threats targeting ViT models primarily\nfocuses on generating the smallest adversarial perturbations that can deceive\nthe models with high confidence, without considering their impact on model\ninterpretations. Nevertheless, the use of interpretation models can effectively\nassist in detecting adversarial examples. This study investigates the\nvulnerability of transformer models to adversarial attacks, even when combined\nwith interpretation models. We propose an attack called \"AdViT\" that generates\nadversarial examples capable of misleading both a given transformer model and\nits coupled interpretation model. Through extensive experiments on various\ntransformer models and two transformer-based interpreters, we demonstrate that\nAdViT achieves a 100% attack success rate in both white-box and black-box\nscenarios. In white-box scenarios, it reaches up to 98% misclassification\nconfidence, while in black-box scenarios, it reaches up to 76%\nmisclassification confidence. Remarkably, AdViT consistently generates accurate\ninterpretations in both scenarios, making the adversarial examples more\ndifficult to detect.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAdViT\u7684\u653b\u51fb\u65b9\u6cd5\uff0c\u80fd\u591f\u540c\u65f6\u6b3a\u9a97\u89c6\u89c9Transformer\u6a21\u578b\u53ca\u5176\u89e3\u91ca\u6a21\u578b\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u5728\u767d\u76d2\u548c\u9ed1\u76d2\u573a\u666f\u4e0b\u5747\u5177\u6709\u9ad8\u653b\u51fb\u6210\u529f\u7387\u3002", "motivation": "\u5c3d\u7ba1\u89c6\u89c9Transformer\u6a21\u578b\u53ca\u5176\u89e3\u91ca\u6a21\u578b\u88ab\u8ba4\u4e3a\u5b89\u5168\u6027\u9ad8\uff0c\u4f46\u6210\u529f\u653b\u51fb\u53ef\u80fd\u5bfc\u81f4\u4e25\u91cd\u540e\u679c\uff0c\u800c\u73b0\u6709\u7814\u7a76\u672a\u5145\u5206\u8003\u8651\u5176\u5bf9\u89e3\u91ca\u6a21\u578b\u7684\u5f71\u54cd\u3002", "method": "\u63d0\u51faAdViT\u653b\u51fb\u65b9\u6cd5\uff0c\u751f\u6210\u80fd\u540c\u65f6\u8bef\u5bfcTransformer\u6a21\u578b\u53ca\u5176\u89e3\u91ca\u6a21\u578b\u7684\u5bf9\u6297\u6837\u672c\u3002", "result": "AdViT\u5728\u767d\u76d2\u548c\u9ed1\u76d2\u573a\u666f\u4e0b\u653b\u51fb\u6210\u529f\u7387\u5747\u8fbe100%\uff0c\u4e14\u751f\u6210\u7684\u5bf9\u6297\u6837\u672c\u96be\u4ee5\u88ab\u68c0\u6d4b\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5373\u4f7f\u7ed3\u5408\u89e3\u91ca\u6a21\u578b\uff0cTransformer\u6a21\u578b\u4ecd\u6613\u53d7\u653b\u51fb\uff0c\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u9632\u5fa1\u7b56\u7565\u3002"}}
{"id": "2507.14417", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14417", "abs": "https://arxiv.org/abs/2507.14417", "authors": ["Aryo Pradipta Gema", "Alexander H\u00e4gele", "Runjin Chen", "Andy Arditi", "Jacob Goldman-Wetzler", "Kit Fraser-Taliente", "Henry Sleight", "Linda Petrini", "Julian Michael", "Beatrice Alex", "Pasquale Minervini", "Yanda Chen", "Joe Benton", "Ethan Perez"], "title": "Inverse Scaling in Test-Time Compute", "comment": null, "summary": "We construct evaluation tasks where extending the reasoning length of Large\nReasoning Models (LRMs) deteriorates performance, exhibiting an inverse scaling\nrelationship between test-time compute and accuracy. Our evaluation tasks span\nfour categories: simple counting tasks with distractors, regression tasks with\nspurious features, deduction tasks with constraint tracking, and advanced AI\nrisks. We identify five distinct failure modes when models reason for longer:\n1) Claude models become increasingly distracted by irrelevant information; 2)\nOpenAI o-series models resist distractors but overfit to problem framings; 3)\nmodels shift from reasonable priors to spurious correlations; 4) all models\nshow difficulties in maintaining focus on complex deductive tasks; and 5)\nextended reasoning may amplify concerning behaviors, with Claude Sonnet 4\nshowing increased expressions of self-preservation. These findings suggest that\nwhile test-time compute scaling remains promising for improving model\ncapabilities, it may inadvertently reinforce problematic reasoning patterns.\nOur results demonstrate the importance of evaluating models across diverse\nreasoning lengths to identify and address these failure modes in LRMs.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u589e\u52a0\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u7684\u63a8\u7406\u957f\u5ea6\u4f1a\u964d\u4f4e\u6027\u80fd\uff0c\u8868\u73b0\u4e3a\u6d4b\u8bd5\u8ba1\u7b97\u91cf\u4e0e\u51c6\u786e\u7387\u4e4b\u95f4\u7684\u53cd\u6bd4\u5173\u7cfb\u3002\u7814\u7a76\u8bc6\u522b\u4e86\u4e94\u79cd\u5931\u8d25\u6a21\u5f0f\uff0c\u5e76\u5f3a\u8c03\u4e86\u8bc4\u4f30\u591a\u6837\u5316\u63a8\u7406\u957f\u5ea6\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u63a2\u8ba8\u6d4b\u8bd5\u8ba1\u7b97\u91cf\u6269\u5c55\u5bf9\u6a21\u578b\u63a8\u7406\u80fd\u529b\u7684\u5f71\u54cd\uff0c\u63ed\u793a\u6f5c\u5728\u7684\u95ee\u9898\u6a21\u5f0f\u3002", "method": "\u6784\u5efa\u56db\u7c7b\u8bc4\u4f30\u4efb\u52a1\uff08\u7b80\u5355\u8ba1\u6570\u3001\u56de\u5f52\u3001\u6f14\u7ece\u548c\u9ad8\u7ea7AI\u98ce\u9669\uff09\uff0c\u5206\u6790\u4e0d\u540c\u63a8\u7406\u957f\u5ea6\u4e0b\u7684\u6a21\u578b\u8868\u73b0\u3002", "result": "\u53d1\u73b0\u4e94\u79cd\u5931\u8d25\u6a21\u5f0f\uff0c\u5305\u62ec\u5206\u5fc3\u3001\u8fc7\u62df\u5408\u3001\u865a\u5047\u5173\u8054\u3001\u590d\u6742\u4efb\u52a1\u6ce8\u610f\u529b\u4e0d\u96c6\u4e2d\u4ee5\u53ca\u884c\u4e3a\u653e\u5927\u3002", "conclusion": "\u6d4b\u8bd5\u8ba1\u7b97\u91cf\u6269\u5c55\u867d\u80fd\u63d0\u5347\u6a21\u578b\u80fd\u529b\uff0c\u4f46\u53ef\u80fd\u5f3a\u5316\u95ee\u9898\u63a8\u7406\u6a21\u5f0f\uff0c\u9700\u591a\u6837\u5316\u8bc4\u4f30\u63a8\u7406\u957f\u5ea6\u3002"}}
{"id": "2507.14776", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.14776", "abs": "https://arxiv.org/abs/2507.14776", "authors": ["Kimia Tasnia", "Alexander Garcia", "Tasnuva Farheen", "Sazadur Rahman"], "title": "VeriOpt: PPA-Aware High-Quality Verilog Generation via Multi-Role LLMs", "comment": "9 pages, 7 figures, Accepted for ICCAD 2025, Munich, Germany", "summary": "The rapid adoption of large language models(LLMs) in hardware design has\nprimarily focused on generating functionally correct Verilog code, overlooking\ncritical Power Performance-Area(PPA) metrics essential for industrial-grade\ndesigns. To bridge this gap, we propose VeriOpt, a novel framework that\nleverages role-based prompting and PPA-aware optimization to enable LLMs to\nproduce high-quality, synthesizable Verilog. VeriOpt structures LLM\ninteractions into specialized roles (e.g., Planner, Programmer, Reviewer,\nEvaluator) to emulate human design workflows, while integrating PPA constraints\ndirectly into the prompting pipeline. By combining multi-modal feedback (e.g.,\nsynthesis reports, timing diagrams) with PPA aware prompting, VeriOpt achieves\nPPA-efficient code generation without sacrificing functional correctness.\nExperimental results demonstrate up to 88% reduction in power, 76% reduction in\narea and 73% improvement in timing closure compared to baseline LLM-generated\nRTL, validated using industry standard EDA tools. At the same time achieves 86%\nsuccess rate in functionality evaluation. Our work advances the\nstate-of-the-art AI-driven hardware design by addressing the critical gap\nbetween correctness and quality, paving the way for reliable LLM adoption in\nproduction workflows.", "AI": {"tldr": "VeriOpt\u662f\u4e00\u4e2a\u5229\u7528\u89d2\u8272\u63d0\u793a\u548cPPA\u4f18\u5316\u6846\u67b6\uff0c\u4f7fLLM\u751f\u6210\u9ad8\u8d28\u91cf\u53ef\u7efc\u5408Verilog\u4ee3\u7801\uff0c\u663e\u8457\u63d0\u5347PPA\u6307\u6807\u3002", "motivation": "\u73b0\u6709LLM\u5728\u786c\u4ef6\u8bbe\u8ba1\u4e2d\u5ffd\u89c6PPA\u6307\u6807\uff0cVeriOpt\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u89d2\u8272\u5206\u5de5\u548c\u591a\u6a21\u6001\u53cd\u9988\u6574\u5408PPA\u7ea6\u675f\uff0c\u4f18\u5316\u4ee3\u7801\u751f\u6210\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u529f\u8017\u964d\u4f4e88%\uff0c\u9762\u79ef\u51cf\u5c1176%\uff0c\u65f6\u5e8f\u6539\u558473%\uff0c\u529f\u80fd\u6b63\u786e\u738786%\u3002", "conclusion": "VeriOpt\u5728AI\u9a71\u52a8\u786c\u4ef6\u8bbe\u8ba1\u4e2d\u5e73\u8861\u529f\u80fd\u6b63\u786e\u6027\u4e0e\u8d28\u91cf\uff0c\u63a8\u52a8LLM\u5728\u751f\u4ea7\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2507.14324", "categories": ["cs.CR", "quant-ph"], "pdf": "https://arxiv.org/pdf/2507.14324", "abs": "https://arxiv.org/abs/2507.14324", "authors": ["Yao Ma", "Wen Yu Kon", "Jefferson Chu", "Kevin Han Yong Loh", "Kaushik Chakraborty", "Charles Lim"], "title": "Quantum-Safe Identity Verification using Relativistic Zero-Knowledge Proof Systems", "comment": null, "summary": "Identity verification is the process of confirming an individual's claimed\nidentity, which is essential in sectors like finance, healthcare, and online\nservices to ensure security and prevent fraud. However, current\npassword/PIN-based identity solutions are susceptible to phishing or skimming\nattacks, where malicious intermediaries attempt to steal credentials using fake\nidentification portals. Alikhani et al. [Nature, 2021] began exploring identity\nverification through graph coloring-based relativistic zero-knowledge proofs\n(RZKPs), a key cryptographic primitive that enables a prover to demonstrate\nknowledge of secret credentials to a verifier without disclosing any\ninformation about the secret. Our work advances this field and addresses\nunresolved issues: From an engineering perspective, we relax further the\nrelativistic constraints from 60m to 30m, and significantly enhance the\nstability and scalability of the experimental demonstration of the 2-prover\ngraph coloring-based RZKP protocol for near-term use cases. At the same time,\nfor long-term security against entangled malicious provers, we propose a\nmodified protocol with comparable computation and communication costs, we\nestablish an upper bound on the soundness parameter for this modified protocol.\nOn the other hand, we extend the two-prover, two-verifier setup to a\nthree-prover configuration, demonstrating the security of such relativistic\nprotocols against entangled malicious provers.", "AI": {"tldr": "\u8be5\u8bba\u6587\u6539\u8fdb\u4e86\u57fa\u4e8e\u56fe\u7740\u8272\u7684\u76f8\u5bf9\u8bba\u96f6\u77e5\u8bc6\u8bc1\u660e\uff08RZKP\uff09\u534f\u8bae\uff0c\u653e\u5bbd\u4e86\u76f8\u5bf9\u8bba\u7ea6\u675f\uff0c\u63d0\u5347\u4e86\u7a33\u5b9a\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u5e76\u6269\u5c55\u4e86\u534f\u8bae\u914d\u7f6e\u4ee5\u589e\u5f3a\u5b89\u5168\u6027\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u5bc6\u7801/PIN\u7684\u8eab\u4efd\u9a8c\u8bc1\u6613\u53d7\u9493\u9c7c\u653b\u51fb\uff0c\u9700\u8981\u66f4\u5b89\u5168\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u6539\u8fdb\u4e24\u8bc1\u660e\u8005\u56fe\u7740\u8272RZKP\u534f\u8bae\uff0c\u653e\u5bbd\u76f8\u5bf9\u8bba\u7ea6\u675f\u81f330\u7c73\uff0c\u5e76\u63d0\u51fa\u4e09\u8bc1\u660e\u8005\u914d\u7f6e\u4ee5\u589e\u5f3a\u5b89\u5168\u6027\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u534f\u8bae\u5728\u7a33\u5b9a\u6027\u548c\u53ef\u6269\u5c55\u6027\u4e0a\u663e\u8457\u63d0\u5347\uff0c\u5e76\u5efa\u7acb\u4e86\u4fee\u6539\u540e\u534f\u8bae\u7684\u5b89\u5168\u53c2\u6570\u4e0a\u9650\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u8fd1\u8fdc\u671f\u8eab\u4efd\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u66f4\u5b89\u5168\u3001\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.14447", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14447", "abs": "https://arxiv.org/abs/2507.14447", "authors": ["Guancheng Zeng", "Xueyi Chen", "Jiawang Hu", "Shaohua Qi", "Yaxuan Mao", "Zhantao Wang", "Yifan Nie", "Shuang Li", "Qiuyang Feng", "Pengxu Qiu", "Yujia Wang", "Wenqiang Han", "Linyan Huang", "Gang Li", "Jingjing Mo", "Haowen Hu"], "title": "Routine: A Structural Planning Framework for LLM Agent System in Enterprise", "comment": "26 pages, 8 figures, 5 tables", "summary": "The deployment of agent systems in an enterprise environment is often\nhindered by several challenges: common models lack domain-specific process\nknowledge, leading to disorganized plans, missing key tools, and poor execution\nstability. To address this, this paper introduces Routine, a multi-step agent\nplanning framework designed with a clear structure, explicit instructions, and\nseamless parameter passing to guide the agent's execution module in performing\nmulti-step tool-calling tasks with high stability. In evaluations conducted\nwithin a real-world enterprise scenario, Routine significantly increases the\nexecution accuracy in model tool calls, increasing the performance of GPT-4o\nfrom 41.1% to 96.3%, and Qwen3-14B from 32.6% to 83.3%. We further constructed\na Routine-following training dataset and fine-tuned Qwen3-14B, resulting in an\naccuracy increase to 88.2% on scenario-specific evaluations, indicating\nimproved adherence to execution plans. In addition, we employed Routine-based\ndistillation to create a scenario-specific, multi-step tool-calling dataset.\nFine-tuning on this distilled dataset raised the model's accuracy to 95.5%,\napproaching GPT-4o's performance. These results highlight Routine's\neffectiveness in distilling domain-specific tool-usage patterns and enhancing\nmodel adaptability to new scenarios. Our experimental results demonstrate that\nRoutine provides a practical and accessible approach to building stable agent\nworkflows, accelerating the deployment and adoption of agent systems in\nenterprise environments, and advancing the technical vision of AI for Process.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faRoutine\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u89c4\u5212\u548c\u53c2\u6570\u4f20\u9012\u63d0\u5347\u4f01\u4e1a\u73af\u5883\u4e2d\u591a\u6b65\u9aa4\u5de5\u5177\u8c03\u7528\u4efb\u52a1\u7684\u6267\u884c\u7a33\u5b9a\u6027\uff0c\u663e\u8457\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u4f01\u4e1a\u73af\u5883\u4e2d\u4ee3\u7406\u7cfb\u7edf\u56e0\u7f3a\u4e4f\u9886\u57df\u77e5\u8bc6\u5bfc\u81f4\u7684\u8ba1\u5212\u6df7\u4e71\u3001\u5de5\u5177\u7f3a\u5931\u548c\u6267\u884c\u4e0d\u7a33\u5b9a\u95ee\u9898\u3002", "method": "\u5f15\u5165Routine\u6846\u67b6\uff0c\u5305\u542b\u6e05\u6670\u7ed3\u6784\u3001\u660e\u786e\u6307\u4ee4\u548c\u53c2\u6570\u4f20\u9012\uff0c\u5e76\u6784\u5efa\u8bad\u7ec3\u6570\u636e\u96c6\u8fdb\u884c\u6a21\u578b\u5fae\u8c03\u3002", "result": "Routine\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u5982GPT-4o\u51c6\u786e\u7387\u4ece41.1%\u5347\u81f396.3%\uff0cQwen3-14B\u4ece32.6%\u5347\u81f383.3%\uff0c\u5fae\u8c03\u540e\u8fdb\u4e00\u6b65\u63d0\u5347\u3002", "conclusion": "Routine\u6709\u6548\u63d0\u5347\u4ee3\u7406\u7cfb\u7edf\u5728\u4f01\u4e1a\u73af\u5883\u4e2d\u7684\u7a33\u5b9a\u6027\u548c\u9002\u5e94\u6027\uff0c\u52a0\u901fAI\u6d41\u7a0b\u6280\u672f\u7684\u90e8\u7f72\u3002"}}
{"id": "2507.14791", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.14791", "abs": "https://arxiv.org/abs/2507.14791", "authors": ["Yang Liu", "Li Zhang", "Fang Liu", "Zhuohang Wang", "Donglin Wei", "Zhishuo Yang", "Kechi Zhang", "Jia Li", "Lin Shi"], "title": "Enhancing Repository-Level Code Generation with Call Chain-Aware Multi-View Context", "comment": null, "summary": "Repository-level code generation aims to generate code within the context of\na specified repository. Existing approaches typically employ\nretrieval-augmented generation (RAG) techniques to provide LLMs with relevant\ncontextual information extracted from the repository. However, these approaches\noften struggle with effectively identifying truly relevant contexts that\ncapture the rich semantics of the repository, and their contextual perspectives\nremains narrow. Moreover, most approaches fail to account for the structural\nrelationships in the retrieved code during prompt construction, hindering the\nLLM's ability to accurately interpret the context. To address these issues, we\npropose RepoScope, which leverages call chain-aware multi-view context for\nrepository-level code generation. RepoScope constructs a Repository Structural\nSemantic Graph (RSSG) and retrieves a comprehensive four-view context,\nintegrating both structural and similarity-based contexts. We propose a novel\ncall chain prediction method that utilizes the repository's structural\nsemantics to improve the identification of callees in the target function.\nAdditionally, we present a structure-preserving serialization algorithm for\nprompt construction, ensuring the coherence of the context for the LLM.\nNotably, RepoScope relies solely on static analysis, eliminating the need for\nadditional training or multiple LLM queries, thus ensuring both efficiency and\ngeneralizability. Evaluation on widely-used repository-level code generation\nbenchmarks (CoderEval and DevEval) demonstrates that RepoScope outperforms\nstate-of-the-art methods, achieving up to a 36.35% relative improvement in\npass@1 scores. Further experiments emphasize RepoScope's potential to improve\ncode generation across different tasks and its ability to integrate effectively\nwith existing approaches.", "AI": {"tldr": "RepoScope\u5229\u7528\u8c03\u7528\u94fe\u611f\u77e5\u7684\u591a\u89c6\u89d2\u4e0a\u4e0b\u6587\u8fdb\u884c\u4ed3\u5e93\u7ea7\u4ee3\u7801\u751f\u6210\uff0c\u901a\u8fc7\u6784\u5efa\u4ed3\u5e93\u7ed3\u6784\u8bed\u4e49\u56fe\uff08RSSG\uff09\u548c\u68c0\u7d22\u56db\u89c6\u89d2\u4e0a\u4e0b\u6587\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4ee3\u7801\u751f\u6210\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u8bc6\u522b\u4ed3\u5e93\u7684\u4e30\u5bcc\u8bed\u4e49\u548c\u7ed3\u6784\u5173\u7cfb\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u5bfc\u81f4\u4e0a\u4e0b\u6587\u4fe1\u606f\u72ed\u7a84\u4e14\u4e0d\u51c6\u786e\u3002", "method": "RepoScope\u901a\u8fc7\u9759\u6001\u5206\u6790\u6784\u5efaRSSG\uff0c\u7ed3\u5408\u7ed3\u6784\u548c\u76f8\u4f3c\u6027\u4e0a\u4e0b\u6587\uff0c\u63d0\u51fa\u8c03\u7528\u94fe\u9884\u6d4b\u65b9\u6cd5\u548c\u7ed3\u6784\u4fdd\u6301\u7684\u5e8f\u5217\u5316\u7b97\u6cd5\u3002", "result": "\u5728CoderEval\u548cDevEval\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cRepoScope\u7684pass@1\u5f97\u5206\u76f8\u5bf9\u63d0\u5347\u4e8636.35%\u3002", "conclusion": "RepoScope\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6216\u591a\u8f6eLLM\u67e5\u8be2\uff0c\u9ad8\u6548\u4e14\u901a\u7528\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4ed3\u5e93\u7ea7\u4ee3\u7801\u751f\u6210\u7684\u6027\u80fd\u3002"}}
{"id": "2507.14519", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14519", "abs": "https://arxiv.org/abs/2507.14519", "authors": ["Wenxuan Zeng", "Tianshi Xu", "Yi Chen", "Yifan Zhou", "Mingzhe Zhang", "Jin Tan", "Cheng Hong", "Meng Li"], "title": "Towards Efficient Privacy-Preserving Machine Learning: A Systematic Review from Protocol, Model, and System Perspectives", "comment": "This work will be continuously updated to reflect the latest advances", "summary": "Privacy-preserving machine learning (PPML) based on cryptographic protocols\nhas emerged as a promising paradigm to protect user data privacy in cloud-based\nmachine learning services. While it achieves formal privacy protection, PPML\noften incurs significant efficiency and scalability costs due to orders of\nmagnitude overhead compared to the plaintext counterpart. Therefore, there has\nbeen a considerable focus on mitigating the efficiency gap for PPML. In this\nsurvey, we provide a comprehensive and systematic review of recent PPML studies\nwith a focus on cross-level optimizations. Specifically, we categorize existing\npapers into protocol level, model level, and system level, and review progress\nat each level. We also provide qualitative and quantitative comparisons of\nexisting works with technical insights, based on which we discuss future\nresearch directions and highlight the necessity of integrating optimizations\nacross protocol, model, and system levels. We hope this survey can provide an\noverarching understanding of existing approaches and potentially inspire future\nbreakthroughs in the PPML field. As the field is evolving fast, we also provide\na public GitHub repository to continuously track the developments, which is\navailable at https://github.com/PKU-SEC-Lab/Awesome-PPML-Papers.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u9690\u79c1\u4fdd\u62a4\u673a\u5668\u5b66\u4e60\uff08PPML\uff09\u7684\u7814\u7a76\u8fdb\u5c55\uff0c\u91cd\u70b9\u5173\u6ce8\u8de8\u5c42\u7ea7\u4f18\u5316\uff0c\u5305\u62ec\u534f\u8bae\u3001\u6a21\u578b\u548c\u7cfb\u7edf\u5c42\u7ea7\uff0c\u5e76\u8ba8\u8bba\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "PPML\u5728\u4fdd\u62a4\u7528\u6237\u6570\u636e\u9690\u79c1\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u5176\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u6210\u672c\u8f83\u9ad8\uff0c\u56e0\u6b64\u9700\u8981\u4f18\u5316\u3002", "method": "\u901a\u8fc7\u5206\u7c7b\u73b0\u6709\u7814\u7a76\u4e3a\u534f\u8bae\u3001\u6a21\u578b\u548c\u7cfb\u7edf\u5c42\u7ea7\uff0c\u8fdb\u884c\u5b9a\u6027\u548c\u5b9a\u91cf\u6bd4\u8f83\u3002", "result": "\u603b\u7ed3\u4e86PPML\u7684\u4f18\u5316\u8fdb\u5c55\uff0c\u5e76\u63d0\u51fa\u4e86\u8de8\u5c42\u7ea7\u6574\u5408\u4f18\u5316\u7684\u5fc5\u8981\u6027\u3002", "conclusion": "\u672c\u6587\u4e3aPPML\u9886\u57df\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u7406\u89e3\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u540c\u65f6\u901a\u8fc7GitHub\u4ed3\u5e93\u6301\u7eed\u8ddf\u8e2a\u8fdb\u5c55\u3002"}}
{"id": "2507.14468", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14468", "abs": "https://arxiv.org/abs/2507.14468", "authors": ["Yitong Lin", "Jiaying He", "Jiahe Chen", "Xinnan Zhu", "Jianwei Zheng", "Tao Bo"], "title": "BioGraphFusion: Graph Knowledge Embedding for Biological Completion and Reasoning", "comment": "Accepted by Bioinformatics on July 11th", "summary": "Motivation: Biomedical knowledge graphs (KGs) are crucial for drug discovery\nand disease understanding, yet their completion and reasoning are challenging.\nKnowledge Embedding (KE) methods capture global semantics but struggle with\ndynamic structural integration, while Graph Neural Networks (GNNs) excel\nlocally but often lack semantic understanding. Even ensemble approaches,\nincluding those leveraging language models, often fail to achieve a deep,\nadaptive, and synergistic co-evolution between semantic comprehension and\nstructural learning. Addressing this critical gap in fostering continuous,\nreciprocal refinement between these two aspects in complex biomedical KGs is\nparamount.\n  Results: We introduce BioGraphFusion, a novel framework for deeply\nsynergistic semantic and structural learning. BioGraphFusion establishes a\nglobal semantic foundation via tensor decomposition, guiding an LSTM-driven\nmechanism to dynamically refine relation embeddings during graph propagation.\nThis fosters adaptive interplay between semantic understanding and structural\nlearning, further enhanced by query-guided subgraph construction and a hybrid\nscoring mechanism. Experiments across three key biomedical tasks demonstrate\nBioGraphFusion's superior performance over state-of-the-art KE, GNN, and\nensemble models. A case study on Cutaneous Malignant Melanoma 1 (CMM1)\nhighlights its ability to unveil biologically meaningful pathways.\n  Availability and Implementation: Source code and all training data are freely\navailable for download at https://github.com/Y-TARL/BioGraphFusion.\n  Contact: zjw@zjut.edu.cn, botao666666@126.com.\n  Supplementary information: Supplementary data are available at Bioinformatics\nonline.", "AI": {"tldr": "BioGraphFusion\u6846\u67b6\u901a\u8fc7\u6df1\u5ea6\u878d\u5408\u8bed\u4e49\u548c\u7ed3\u6784\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u7269\u533b\u5b66\u77e5\u8bc6\u56fe\u8c31\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u751f\u7269\u533b\u5b66\u77e5\u8bc6\u56fe\u8c31\u7684\u5b8c\u6210\u548c\u63a8\u7406\u5177\u6709\u6311\u6218\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u8bed\u4e49\u7406\u89e3\u548c\u7ed3\u6784\u5b66\u4e60\u4e4b\u95f4\u7f3a\u4e4f\u534f\u540c\u8fdb\u5316\u3002", "method": "BioGraphFusion\u901a\u8fc7\u5f20\u91cf\u5206\u89e3\u5efa\u7acb\u5168\u5c40\u8bed\u4e49\u57fa\u7840\uff0c\u7ed3\u5408LSTM\u52a8\u6001\u4f18\u5316\u5173\u7cfb\u5d4c\u5165\uff0c\u5e76\u91c7\u7528\u67e5\u8be2\u5f15\u5bfc\u7684\u5b50\u56fe\u6784\u5efa\u548c\u6df7\u5408\u8bc4\u5206\u673a\u5236\u3002", "result": "\u5728\u4e09\u4e2a\u751f\u7269\u533b\u5b66\u4efb\u52a1\u4e2d\uff0cBioGraphFusion\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u4e86\u5176\u751f\u7269\u5b66\u610f\u4e49\u3002", "conclusion": "BioGraphFusion\u4e3a\u751f\u7269\u533b\u5b66\u77e5\u8bc6\u56fe\u8c31\u7684\u8bed\u4e49\u548c\u7ed3\u6784\u5b66\u4e60\u63d0\u4f9b\u4e86\u9ad8\u6548\u534f\u540c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.14969", "categories": ["cs.SE", "D.2.1"], "pdf": "https://arxiv.org/pdf/2507.14969", "abs": "https://arxiv.org/abs/2507.14969", "authors": ["Sai Zhang", "Zhenchang Xing", "Jieshan Chen", "Dehai Zhao", "Zizhong Zhu", "Xiaowang Zhang", "Zhiyong Feng", "Xiaohong Li"], "title": "Think Like an Engineer: A Neuro-Symbolic Collaboration Agent for Generative Software Requirements Elicitation and Self-Review", "comment": null, "summary": "The vision of End-User Software Engineering (EUSE) is to empower\nnon-professional users with full control over the software development\nlifecycle. It aims to enable users to drive generative software development\nusing only natural language requirements. However, since end-users often lack\nknowledge of software engineering, their requirement descriptions are\nfrequently ambiguous, raising significant challenges to generative software\ndevelopment. Although existing approaches utilize structured languages like\nGherkin to clarify user narratives, they still struggle to express the causal\nlogic between preconditions and behavior actions. This paper introduces\nRequireCEG, a requirement elicitation and self-review agent that embeds\ncausal-effect graphs (CEGs) in a neuro-symbolic collaboration architecture.\nRequireCEG first uses a feature tree to analyze user narratives hierarchically,\nclearly defining the scope of software components and their system behavior\nrequirements. Next, it constructs the self-healing CEGs based on the elicited\nrequirements, capturing the causal relationships between atomic preconditions\nand behavioral actions. Finally, the constructed CEGs are used to review and\noptimize Gherkin scenarios, ensuring consistency between the generated Gherkin\nrequirements and the system behavior requirements elicited from user\nnarratives. To evaluate our method, we created the RGPair benchmark dataset and\nconducted extensive experiments. It achieves an 87% coverage rate and raises\ndiversity by 51.88%.", "AI": {"tldr": "RequireCEG\u662f\u4e00\u4e2a\u9700\u6c42\u83b7\u53d6\u548c\u81ea\u6211\u5ba1\u67e5\u4ee3\u7406\uff0c\u901a\u8fc7\u56e0\u679c\u6548\u5e94\u56fe\uff08CEGs\uff09\u548c\u795e\u7ecf\u7b26\u53f7\u534f\u4f5c\u67b6\u6784\uff0c\u89e3\u51b3\u7528\u6237\u9700\u6c42\u6a21\u7cca\u6027\u95ee\u9898\uff0c\u63d0\u5347\u751f\u6210\u5f0f\u8f6f\u4ef6\u5f00\u53d1\u7684\u6548\u679c\u3002", "motivation": "\u975e\u4e13\u4e1a\u7528\u6237\u7684\u9700\u6c42\u63cf\u8ff0\u901a\u5e38\u6a21\u7cca\uff0c\u5bfc\u81f4\u751f\u6210\u5f0f\u8f6f\u4ef6\u5f00\u53d1\u56f0\u96be\u3002\u73b0\u6709\u65b9\u6cd5\u5982Gherkin\u96be\u4ee5\u8868\u8fbe\u56e0\u679c\u903b\u8f91\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u6709\u6548\u7684\u9700\u6c42\u83b7\u53d6\u548c\u5ba1\u67e5\u65b9\u6cd5\u3002", "method": "RequireCEG\u7ed3\u5408\u7279\u5f81\u6811\u548c\u56e0\u679c\u6548\u5e94\u56fe\uff08CEGs\uff09\uff0c\u5206\u5c42\u5206\u6790\u7528\u6237\u9700\u6c42\uff0c\u6784\u5efa\u81ea\u4fee\u590dCEGs\uff0c\u5e76\u4f18\u5316Gherkin\u573a\u666f\u4ee5\u786e\u4fdd\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728RGPair\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fbe\u523087%\u7684\u8986\u76d6\u7387\uff0c\u591a\u6837\u6027\u63d0\u534751.88%\u3002", "conclusion": "RequireCEG\u6709\u6548\u89e3\u51b3\u4e86\u7528\u6237\u9700\u6c42\u6a21\u7cca\u6027\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u751f\u6210\u5f0f\u8f6f\u4ef6\u5f00\u53d1\u7684\u51c6\u786e\u6027\u548c\u591a\u6837\u6027\u3002"}}
{"id": "2507.14588", "categories": ["cs.CR", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2507.14588", "abs": "https://arxiv.org/abs/2507.14588", "authors": ["Usayd Shahul", "J. Harshan"], "title": "FORTA: Byzantine-Resilient FL Aggregation via DFT-Guided Krum", "comment": "To appear in the Proceedings of IEEE Information Theory Workshop\n  2025, Sydney, Australia", "summary": "Secure federated learning enables collaborative model training across\ndecentralized users while preserving data privacy. A key component is secure\naggregation, which keeps individual updates hidden from both the server and\nusers, while also defending against Byzantine users who corrupt the\naggregation. To this end, Jinhyun So et al. recently developed a\nByzantine-resilient secure aggregation scheme using a secret-sharing strategy\nover finite-field arithmetic. However, such an approach can suffer from\nnumerical errors and overflows when applied to real-valued model updates,\nmotivating the need for secure aggregation methods that operate directly over\nthe real domain. We propose FORTA, a Byzantine-resilient secure aggregation\nframework that operates entirely in the real domain. FORTA leverages Discrete\nFourier Transform (DFT) codes for privacy and employs Krum-based outlier\ndetection for robustness. While DFT decoder is error-free under infinite\nprecision, finite precision introduces numerical perturbations that can distort\ndistance estimates and allow malicious updates to evade detection. To address\nthis, FORTA refines Krum using feedback from DFT decoder, improving the\nselection of trustworthy updates. Theoretical analysis and experiments show\nthat our modification of Krum offers improved robustness and more accurate\naggregation than standard Krum.", "AI": {"tldr": "FORTA\u662f\u4e00\u4e2a\u5728\u5b9e\u6570\u57df\u64cd\u4f5c\u7684\u62dc\u5360\u5ead\u9c81\u68d2\u5b89\u5168\u805a\u5408\u6846\u67b6\uff0c\u5229\u7528DFT\u7f16\u7801\u4fdd\u62a4\u9690\u79c1\uff0c\u6539\u8fdbKrum\u7b97\u6cd5\u4ee5\u63d0\u9ad8\u9c81\u68d2\u6027\u548c\u805a\u5408\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u6709\u9650\u57df\u7b97\u672f\u7684\u5b89\u5168\u805a\u5408\u65b9\u6cd5\u5728\u5904\u7406\u5b9e\u6570\u6a21\u578b\u66f4\u65b0\u65f6\u53ef\u80fd\u4ea7\u751f\u6570\u503c\u9519\u8bef\u548c\u6ea2\u51fa\uff0c\u9700\u8981\u76f4\u63a5\u5728\u5b9e\u6570\u57df\u64cd\u4f5c\u7684\u5b89\u5168\u805a\u5408\u65b9\u6cd5\u3002", "method": "FORTA\u7ed3\u5408DFT\u7f16\u7801\u548c\u57fa\u4e8eKrum\u7684\u5f02\u5e38\u68c0\u6d4b\uff0c\u901a\u8fc7DFT\u89e3\u7801\u5668\u7684\u53cd\u9988\u6539\u8fdbKrum\u7b97\u6cd5\uff0c\u63d0\u5347\u9c81\u68d2\u6027\u3002", "result": "\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u8868\u660e\uff0c\u6539\u8fdb\u540e\u7684Krum\u7b97\u6cd5\u6bd4\u6807\u51c6Krum\u66f4\u5177\u9c81\u68d2\u6027\uff0c\u805a\u5408\u66f4\u51c6\u786e\u3002", "conclusion": "FORTA\u5728\u5b9e\u6570\u57df\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u62dc\u5360\u5ead\u5b89\u5168\u805a\u5408\uff0c\u89e3\u51b3\u4e86\u6570\u503c\u7cbe\u5ea6\u95ee\u9898\u3002"}}
{"id": "2507.14513", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14513", "abs": "https://arxiv.org/abs/2507.14513", "authors": ["Hongyi Yang", "Yue Pan", "Jiayi Xu", "Kelsen Liu"], "title": "Amico: An Event-Driven Modular Framework for Persistent and Embedded Autonomy", "comment": null, "summary": "Recent advances in large language models (LLMs) and autonomous agents have\nenabled systems capable of performing complex tasks across domains such as\nhuman-computer interaction, planning, and web navigation. However, many\nexisting frameworks struggle in real-world or resource-constrained environments\ndue to their reliance on cloud-based computation, limited robustness in dynamic\ncontexts, and lack of persistent autonomy and environmental awareness.\n  We present Amico, a modular, event-driven framework for building autonomous\nagents optimized for embedded systems. Written in Rust for safety and\nperformance, Amico supports reactive, persistent agents that operate\nefficiently across embedded platforms and browser environments via WebAssembly.\nIt provides clean abstractions for event handling, state management, behavior\nexecution, and integration with reasoning modules. Amico delivers a unified\ninfrastructure for constructing resilient, interactive agents suitable for\ndeployment in settings with limited compute and intermittent connectivity.", "AI": {"tldr": "Amico\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u3001\u4e8b\u4ef6\u9a71\u52a8\u7684\u6846\u67b6\uff0c\u4e13\u4e3a\u5d4c\u5165\u5f0f\u7cfb\u7edf\u4f18\u5316\u7684\u81ea\u4e3b\u4ee3\u7406\u8bbe\u8ba1\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6846\u67b6\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u81ea\u4e3b\u4ee3\u7406\u6846\u67b6\u5728\u52a8\u6001\u73af\u5883\u548c\u8d44\u6e90\u53d7\u9650\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4f9d\u8d56\u4e91\u7aef\u8ba1\u7b97\u4e14\u7f3a\u4e4f\u6301\u4e45\u81ea\u4e3b\u6027\u548c\u73af\u5883\u611f\u77e5\u80fd\u529b\u3002", "method": "Amico\u91c7\u7528Rust\u7f16\u5199\uff0c\u652f\u6301\u901a\u8fc7WebAssembly\u5728\u5d4c\u5165\u5f0f\u5e73\u53f0\u548c\u6d4f\u89c8\u5668\u73af\u5883\u4e2d\u9ad8\u6548\u8fd0\u884c\uff0c\u63d0\u4f9b\u4e8b\u4ef6\u5904\u7406\u3001\u72b6\u6001\u7ba1\u7406\u548c\u884c\u4e3a\u6267\u884c\u7684\u62bd\u8c61\u3002", "result": "Amico\u6784\u5efa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u652f\u6301\u5728\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\u548c\u8fde\u63a5\u4e0d\u7a33\u5b9a\u7684\u73af\u5883\u4e2d\u90e8\u7f72\u5f39\u6027\u548c\u4ea4\u4e92\u5f0f\u4ee3\u7406\u3002", "conclusion": "Amico\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u7684\u81ea\u4e3b\u4ee3\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u5b89\u5168\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.15003", "categories": ["cs.SE", "cs.AI", "cs.CE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.15003", "abs": "https://arxiv.org/abs/2507.15003", "authors": ["Hao Li", "Haoxiang Zhang", "Ahmed E. Hassan"], "title": "The Rise of AI Teammates in Software Engineering (SE) 3.0: How Autonomous Coding Agents Are Reshaping Software Engineering", "comment": null, "summary": "The future of software engineering--SE 3.0--is unfolding with the rise of AI\nteammates: autonomous, goal-driven systems collaborating with human developers.\nAmong these, autonomous coding agents are especially transformative, now\nactively initiating, reviewing, and evolving code at scale. This paper\nintroduces AIDev, the first large-scale dataset capturing how such agents\noperate in the wild. Spanning over 456,000 pull requests by five leading\nagents--OpenAI Codex, Devin, GitHub Copilot, Cursor, and Claude Code--across\n61,000 repositories and 47,000 developers, AIDev provides an unprecedented\nempirical foundation for studying autonomous teammates in software development.\n  Unlike prior work that has largely theorized the rise of AI-native software\nengineering, AIDev offers structured, open data to support research in\nbenchmarking, agent readiness, optimization, collaboration modeling, and AI\ngovernance. The dataset includes rich metadata on PRs, authorship, review\ntimelines, code changes, and integration outcomes--enabling exploration beyond\nsynthetic benchmarks like SWE-bench. For instance, although agents often\noutperform humans in speed, their PRs are accepted less frequently, revealing a\ntrust and utility gap. Furthermore, while agents accelerate code\nsubmission--one developer submitted as many PRs in three days as they had in\nthree years--these are structurally simpler (via code complexity metrics).\n  We envision AIDev as a living resource: extensible, analyzable, and ready for\nthe SE and AI communities. Grounding SE 3.0 in real-world evidence, AIDev\nenables a new generation of research into AI-native workflows and supports\nbuilding the next wave of symbiotic human-AI collaboration. The dataset is\npublicly available at https://github.com/SAILResearch/AI_Teammates_in_SE3.\n  > AI Agent, Agentic AI, Coding Agent, Agentic Coding, Software Engineering\nAgent", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86AIDev\uff0c\u9996\u4e2a\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u8bb0\u5f55\u4e86AI\u7f16\u7801\u4ee3\u7406\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u5b9e\u9645\u8fd0\u4f5c\uff0c\u4e3a\u7814\u7a76AI\u4e0e\u4eba\u7c7b\u534f\u4f5c\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u57fa\u7840\u3002", "motivation": "\u7814\u7a76AI\u4ee3\u7406\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u5b9e\u9645\u8868\u73b0\uff0c\u586b\u8865\u7406\u8bba\u4e0e\u5b9e\u8bc1\u4e4b\u95f4\u7684\u7a7a\u767d\uff0c\u63a8\u52a8AI\u539f\u751f\u8f6f\u4ef6\u5de5\u7a0b\u7684\u53d1\u5c55\u3002", "method": "\u901a\u8fc7\u6536\u96c6456,000\u4e2a\u7531\u4e94\u79cd\u4e3b\u6d41AI\u4ee3\u7406\uff08\u5982GitHub Copilot\uff09\u63d0\u4ea4\u7684\u62c9\u53d6\u8bf7\u6c42\u6570\u636e\uff0c\u6784\u5efaAIDev\u6570\u636e\u96c6\uff0c\u5206\u6790\u5176\u8868\u73b0\u548c\u534f\u4f5c\u6a21\u5f0f\u3002", "result": "AI\u4ee3\u7406\u5728\u63d0\u4ea4\u901f\u5ea6\u4e0a\u4f18\u4e8e\u4eba\u7c7b\uff0c\u4f46\u63a5\u53d7\u7387\u8f83\u4f4e\uff0c\u4e14\u4ee3\u7801\u7ed3\u6784\u66f4\u7b80\u5355\uff0c\u63ed\u793a\u4e86\u4fe1\u4efb\u4e0e\u6548\u7528\u5dee\u8ddd\u3002", "conclusion": "AIDev\u4e3a\u7814\u7a76AI\u539f\u751f\u5de5\u4f5c\u6d41\u548c\u4eba\u7c7b-AI\u534f\u4f5c\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90\uff0c\u652f\u6301\u672a\u6765\u8f6f\u4ef6\u5f00\u53d1\u4e2dAI\u4ee3\u7406\u7684\u4f18\u5316\u4e0e\u5e94\u7528\u3002"}}
{"id": "2507.14600", "categories": ["cs.CR", "quant-ph"], "pdf": "https://arxiv.org/pdf/2507.14600", "abs": "https://arxiv.org/abs/2507.14600", "authors": ["MA. Khajeian"], "title": "Hybrid Classical-Quantum Rainbow Table Attack on Human Passwords", "comment": null, "summary": "Passwords that are long and human-generated pose a challenge for both\nclassical and quantum attacks due to their irregular structure and large search\nspace. In this work, we present an enhanced classical-quantum hybrid attack\ntailored to this scenario. We build rainbow tables using dictionary-based\npassword generation with transformation rules to better model real user\nbehavior. These tables are then organized into buckets, enabling faster lookup\nand reduced space complexity. To perform quantum search within each bucket, we\nuse a distributed exact variant of Grover's algorithm, which offers lower\ncircuit depth and deterministic success. As a result, the overall quantum\ncircuit is shallower and more robust against noise, particularly from\ndepolarizing channels commonly found in near-term quantum devices. Through this\nwork, Overall, we propose a hybrid framework that combines structured rainbow\ntables with efficient quantum search to enhance password recovery.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5f69\u8679\u8868\u548c\u9ad8\u6548\u91cf\u5b50\u641c\u7d22\u7684\u6df7\u5408\u6846\u67b6\uff0c\u7528\u4e8e\u589e\u5f3a\u5bc6\u7801\u6062\u590d\u80fd\u529b\u3002", "motivation": "\u957f\u4e14\u7531\u4eba\u751f\u6210\u7684\u5bc6\u7801\u56e0\u5176\u4e0d\u89c4\u5219\u7ed3\u6784\u548c\u5de8\u5927\u641c\u7d22\u7a7a\u95f4\uff0c\u5bf9\u7ecf\u5178\u548c\u91cf\u5b50\u653b\u51fb\u90fd\u6784\u6210\u6311\u6218\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u5b57\u5178\u7684\u5bc6\u7801\u751f\u6210\u548c\u8f6c\u6362\u89c4\u5219\u6784\u5efa\u5f69\u8679\u8868\uff0c\u5e76\u5c06\u5176\u7ec4\u7ec7\u4e3a\u6876\u4ee5\u4f18\u5316\u67e5\u627e\u548c\u7a7a\u95f4\u590d\u6742\u5ea6\uff1b\u5728\u6bcf\u4e2a\u6876\u5185\u4f7f\u7528\u5206\u5e03\u5f0f\u7cbe\u786eGrover\u7b97\u6cd5\u8fdb\u884c\u91cf\u5b50\u641c\u7d22\u3002", "result": "\u6574\u4f53\u91cf\u5b50\u7535\u8def\u66f4\u6d45\u4e14\u5bf9\u566a\u58f0\u66f4\u9c81\u68d2\uff0c\u7279\u522b\u662f\u5728\u8fd1\u91cf\u5b50\u8bbe\u5907\u4e2d\u5e38\u89c1\u7684\u53bb\u6781\u5316\u901a\u9053\u4e0b\u3002", "conclusion": "\u63d0\u51fa\u7684\u6df7\u5408\u6846\u67b6\u7ed3\u5408\u4e86\u7ed3\u6784\u5316\u5f69\u8679\u8868\u548c\u9ad8\u6548\u91cf\u5b50\u641c\u7d22\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5bc6\u7801\u6062\u590d\u6548\u7387\u3002"}}
{"id": "2507.14520", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14520", "abs": "https://arxiv.org/abs/2507.14520", "authors": ["Xinyi Chen", "Yifei Yuan", "Jiaang Li", "Serge Belongie", "Maarten de Rijke", "Anders S\u00f8gaard"], "title": "What if Othello-Playing Language Models Could See?", "comment": "ICML 2025 Assessing World Models Workshop", "summary": "Language models are often said to face a symbol grounding problem. While some\nargue that world understanding can emerge from text alone, others suggest\ngrounded learning is more efficient. We explore this through Othello, where the\nboard state defines a simplified, rule-based world. Building on prior work, we\nintroduce VISOTHELLO, a multi-modal model trained on move histories and board\nimages. Using next-move prediction, we compare it to mono-modal baselines and\ntest robustness to semantically irrelevant perturbations. We find that\nmulti-modal training improves both performance and the robustness of internal\nrepresentations. These results suggest that grounding language in visual input\nhelps models infer structured world representations.", "AI": {"tldr": "\u591a\u6a21\u6001\u8bad\u7ec3\uff08\u7ed3\u5408\u6587\u672c\u548c\u89c6\u89c9\u8f93\u5165\uff09\u5728Othello\u6e38\u620f\u4e2d\u63d0\u5347\u6a21\u578b\u6027\u80fd\u548c\u5185\u90e8\u8868\u793a\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u63a2\u8ba8\u8bed\u8a00\u6a21\u578b\u662f\u5426\u4ec5\u901a\u8fc7\u6587\u672c\u5c31\u80fd\u7406\u89e3\u4e16\u754c\uff0c\u6216\u662f\u5426\u9700\u8981\u901a\u8fc7\u591a\u6a21\u6001\uff08\u5982\u89c6\u89c9\uff09\u8fdb\u884c\u66f4\u9ad8\u6548\u7684\u5b66\u4e60\u3002", "method": "\u5f15\u5165VISOTHELLO\u6a21\u578b\uff0c\u7ed3\u5408\u79fb\u52a8\u5386\u53f2\u548c\u68cb\u76d8\u56fe\u50cf\u8fdb\u884c\u591a\u6a21\u6001\u8bad\u7ec3\uff0c\u5e76\u901a\u8fc7\u4e0b\u4e00\u6b65\u79fb\u52a8\u9884\u6d4b\u4e0e\u5355\u6a21\u6001\u57fa\u7ebf\u5bf9\u6bd4\u3002", "result": "\u591a\u6a21\u6001\u8bad\u7ec3\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u548c\u5185\u90e8\u8868\u793a\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u89c6\u89c9\u8f93\u5165\u6709\u52a9\u4e8e\u8bed\u8a00\u6a21\u578b\u63a8\u65ad\u7ed3\u6784\u5316\u4e16\u754c\u8868\u793a\u3002"}}
{"id": "2507.15025", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15025", "abs": "https://arxiv.org/abs/2507.15025", "authors": ["Nenad Petrovic", "Vahid Zolfaghari", "Andre Schamschurko", "Sven Kirchner", "Fengjunjie Pan", "Chengdng Wu", "Nils Purschke", "Aleksei Velsh", "Krzysztof Lebioda", "Yinglei Song", "Yi Zhang", "Lukasz Mazur", "Alois Knoll"], "title": "Survey of GenAI for Automotive Software Development: From Requirements to Executable Code", "comment": "Conference paper accepted for GACLM 2025", "summary": "Adoption of state-of-art Generative Artificial Intelligence (GenAI) aims to\nrevolutionize many industrial areas by reducing the amount of human\nintervention needed and effort for handling complex underlying processes.\nAutomotive software development is considered to be a significant area for\nGenAI adoption, taking into account lengthy and expensive procedures, resulting\nfrom the amount of requirements and strict standardization. In this paper, we\nexplore the adoption of GenAI for various steps of automotive software\ndevelopment, mainly focusing on requirements handling, compliance aspects and\ncode generation. Three GenAI-related technologies are covered within the\nstate-of-art: Large Language Models (LLMs), Retrieval Augmented Generation\n(RAG), Vision Language Models (VLMs), as well as overview of adopted prompting\ntechniques in case of code generation. Additionally, we also derive a\ngeneralized GenAI-aided automotive software development workflow based on our\nfindings from this literature review. Finally, we include a summary of a survey\noutcome, which was conducted among our automotive industry partners regarding\nthe type of GenAI tools used for their daily work activities.", "AI": {"tldr": "\u63a2\u8ba8\u4e86GenAI\u5728\u6c7d\u8f66\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u5e94\u7528\uff0c\u91cd\u70b9\u5173\u6ce8\u9700\u6c42\u5904\u7406\u3001\u5408\u89c4\u6027\u548c\u4ee3\u7801\u751f\u6210\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u901a\u7528\u7684GenAI\u8f85\u52a9\u5f00\u53d1\u6d41\u7a0b\u3002", "motivation": "\u6c7d\u8f66\u8f6f\u4ef6\u5f00\u53d1\u6d41\u7a0b\u590d\u6742\u4e14\u6210\u672c\u9ad8\uff0cGenAI\u6709\u671b\u51cf\u5c11\u4eba\u5de5\u5e72\u9884\u548c\u6210\u672c\u3002", "method": "\u7efc\u8ff0\u4e86LLMs\u3001RAG\u548cVLMs\u7b49\u6280\u672f\uff0c\u5e76\u603b\u7ed3\u4e86\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u63d0\u793a\u6280\u672f\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u901a\u7528\u7684GenAI\u8f85\u52a9\u5f00\u53d1\u6d41\u7a0b\uff0c\u5e76\u5206\u4eab\u4e86\u884c\u4e1a\u5408\u4f5c\u4f19\u4f34\u7684GenAI\u5de5\u5177\u4f7f\u7528\u8c03\u67e5\u7ed3\u679c\u3002", "conclusion": "GenAI\u5728\u6c7d\u8f66\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u4ecd\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u548c\u5b9e\u8df5\u9a8c\u8bc1\u3002"}}
{"id": "2507.14625", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14625", "abs": "https://arxiv.org/abs/2507.14625", "authors": ["Juntao Tan", "Anran Li", "Quanchao Liu", "Peng Ran", "Lan Zhang"], "title": "VTarbel: Targeted Label Attack with Minimal Knowledge on Detector-enhanced Vertical Federated Learning", "comment": null, "summary": "Vertical federated learning (VFL) enables multiple parties with disjoint\nfeatures to collaboratively train models without sharing raw data. While\nprivacy vulnerabilities of VFL are extensively-studied, its security\nthreats-particularly targeted label attacks-remain underexplored. In such\nattacks, a passive party perturbs inputs at inference to force\nmisclassification into adversary-chosen labels. Existing methods rely on\nunrealistic assumptions (e.g., accessing VFL-model's outputs) and ignore\nanomaly detectors deployed in real-world systems. To bridge this gap, we\nintroduce VTarbel, a two-stage, minimal-knowledge attack framework explicitly\ndesigned to evade detector-enhanced VFL inference. During the preparation\nstage, the attacker selects a minimal set of high-expressiveness samples (via\nmaximum mean discrepancy), submits them through VFL protocol to collect\npredicted labels, and uses these pseudo-labels to train estimated detector and\nsurrogate model on local features. In attack stage, these models guide\ngradient-based perturbations of remaining samples, crafting adversarial\ninstances that induce targeted misclassifications and evade detection. We\nimplement VTarbel and evaluate it against four model architectures, seven\nmultimodal datasets, and two anomaly detectors. Across all settings, VTarbel\noutperforms four state-of-the-art baselines, evades detection, and retains\neffective against three representative privacy-preserving defenses. These\nresults reveal critical security blind spots in current VFL deployments and\nunderscore urgent need for robust, attack-aware defenses.", "AI": {"tldr": "VTarbel\u662f\u4e00\u79cd\u9488\u5bf9\u5782\u76f4\u8054\u90a6\u5b66\u4e60\uff08VFL\uff09\u7684\u4e24\u9636\u6bb5\u653b\u51fb\u6846\u67b6\uff0c\u80fd\u591f\u5728\u89c4\u907f\u5f02\u5e38\u68c0\u6d4b\u5668\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u76ee\u6807\u6807\u7b7e\u653b\u51fb\u3002", "motivation": "\u73b0\u6709VFL\u5b89\u5168\u7814\u7a76\u591a\u5173\u6ce8\u9690\u79c1\u6f0f\u6d1e\uff0c\u800c\u9488\u5bf9\u76ee\u6807\u6807\u7b7e\u653b\u51fb\u7684\u7814\u7a76\u4e0d\u8db3\uff0c\u4e14\u73b0\u6709\u65b9\u6cd5\u5047\u8bbe\u4e0d\u5207\u5b9e\u9645\u3002", "method": "VTarbel\u901a\u8fc7\u4e24\u9636\u6bb5\u653b\u51fb\uff08\u51c6\u5907\u9636\u6bb5\u548c\u653b\u51fb\u9636\u6bb5\uff09\uff0c\u5229\u7528\u9ad8\u8868\u73b0\u529b\u6837\u672c\u8bad\u7ec3\u672c\u5730\u4ee3\u7406\u6a21\u578b\u548c\u68c0\u6d4b\u5668\u4f30\u8ba1\uff0c\u751f\u6210\u5bf9\u6297\u6837\u672c\u4ee5\u89c4\u907f\u68c0\u6d4b\u3002", "result": "VTarbel\u5728\u591a\u79cd\u6a21\u578b\u67b6\u6784\u3001\u6570\u636e\u96c6\u548c\u68c0\u6d4b\u5668\u4e0b\u5747\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u80fd\u6709\u6548\u89c4\u907f\u9690\u79c1\u4fdd\u62a4\u9632\u5fa1\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u5f53\u524dVFL\u90e8\u7f72\u4e2d\u7684\u5b89\u5168\u76f2\u70b9\uff0c\u4e9f\u9700\u5f00\u53d1\u66f4\u9c81\u68d2\u7684\u9632\u5fa1\u673a\u5236\u3002"}}
{"id": "2507.14552", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14552", "abs": "https://arxiv.org/abs/2507.14552", "authors": ["Anna Sofia Lippolis", "Mohammad Javad Saeedizade", "Robin Keskis\u00e4rkk\u00e4", "Aldo Gangemi", "Eva Blomqvist", "Andrea Giovanni Nuzzolese"], "title": "Large Language Models Assisting Ontology Evaluation", "comment": null, "summary": "Ontology evaluation through functional requirements, such as testing via\ncompetency question (CQ) verification, is a well-established yet costly,\nlabour-intensive, and error-prone endeavour, even for ontology engineering\nexperts. In this work, we introduce OE-Assist, a novel framework designed to\nassist ontology evaluation through automated and semi-automated CQ\nverification. By presenting and leveraging a dataset of 1,393 CQs paired with\ncorresponding ontologies and ontology stories, our contributions present, to\nour knowledge, the first systematic investigation into large language model\n(LLM)-assisted ontology evaluation, and include: (i) evaluating the\neffectiveness of a LLM-based approach for automatically performing CQ\nverification against a manually created gold standard, and (ii) developing and\nassessing an LLM-powered framework to assist CQ verification with Prot\\'eg\\'e,\nby providing suggestions. We found that automated LLM-based evaluation with\no1-preview and o3-mini perform at a similar level to the average user's\nperformance.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86OE-Assist\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u548c\u534a\u81ea\u52a8\u5316\u7684CQ\u9a8c\u8bc1\u8f85\u52a9\u672c\u4f53\u8bc4\u4f30\uff0c\u5229\u7528LLM\u6280\u672f\u63d0\u5347\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u672c\u4f53\u8bc4\u4f30\u65b9\u6cd5\uff08\u5982CQ\u9a8c\u8bc1\uff09\u6210\u672c\u9ad8\u3001\u52b3\u52a8\u5bc6\u96c6\u4e14\u6613\u51fa\u9519\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faOE-Assist\u6846\u67b6\uff0c\u5229\u7528LLM\u6280\u672f\u81ea\u52a8\u548c\u534a\u81ea\u52a8\u5316\u9a8c\u8bc1CQ\uff0c\u5e76\u57fa\u4e8e1,393\u4e2aCQ\u6570\u636e\u96c6\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "LLM-based\u65b9\u6cd5\uff08o1-preview\u548co3-mini\uff09\u7684\u8bc4\u4f30\u6548\u679c\u4e0e\u666e\u901a\u7528\u6237\u8868\u73b0\u76f8\u5f53\u3002", "conclusion": "LLM\u6280\u672f\u53ef\u6709\u6548\u8f85\u52a9\u672c\u4f53\u8bc4\u4f30\uff0cOE-Assist\u6846\u67b6\u4e3a\u81ea\u52a8\u5316\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2507.15157", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15157", "abs": "https://arxiv.org/abs/2507.15157", "authors": ["Giovanni Quattrocchi", "Liliana Pasquale", "Paola Spoletini", "Luciano Baresi"], "title": "Can LLMs Generate User Stories and Assess Their Quality?", "comment": null, "summary": "Requirements elicitation is still one of the most challenging activities of\nthe requirements engineering process due to the difficulty requirements\nanalysts face in understanding and translating complex needs into concrete\nrequirements. In addition, specifying high-quality requirements is crucial, as\nit can directly impact the quality of the software to be developed. Although\nautomated tools allow for assessing the syntactic quality of requirements,\nevaluating semantic metrics (e.g., language clarity, internal consistency)\nremains a manual and time-consuming activity. This paper explores how LLMs can\nhelp automate requirements elicitation within agile frameworks, where\nrequirements are defined as user stories (US). We used 10 state-of-the-art LLMs\nto investigate their ability to generate US automatically by emulating customer\ninterviews. We evaluated the quality of US generated by LLMs, comparing it with\nthe quality of US generated by humans (domain experts and students). We also\nexplored whether and how LLMs can be used to automatically evaluate the\nsemantic quality of US. Our results indicate that LLMs can generate US similar\nto humans in terms of coverage and stylistic quality, but exhibit lower\ndiversity and creativity. Although LLM-generated US are generally comparable in\nquality to those created by humans, they tend to meet the acceptance quality\ncriteria less frequently, regardless of the scale of the LLM model. Finally,\nLLMs can reliably assess the semantic quality of US when provided with clear\nevaluation criteria and have the potential to reduce human effort in\nlarge-scale assessments.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u5229\u7528LLMs\u5728\u654f\u6377\u6846\u67b6\u4e2d\u81ea\u52a8\u5316\u9700\u6c42\u83b7\u53d6\uff0c\u751f\u6210\u7528\u6237\u6545\u4e8b\uff08US\uff09\uff0c\u5e76\u8bc4\u4f30\u5176\u8bed\u4e49\u8d28\u91cf\u3002\u7ed3\u679c\u8868\u660e\uff0cLLMs\u751f\u6210\u7684US\u5728\u8986\u76d6\u8303\u56f4\u548c\u98ce\u683c\u8d28\u91cf\u4e0a\u4e0e\u4eba\u7c7b\u76f8\u4f3c\uff0c\u4f46\u591a\u6837\u6027\u548c\u521b\u9020\u6027\u8f83\u4f4e\u3002LLMs\u8fd8\u80fd\u53ef\u9760\u8bc4\u4f30US\u7684\u8bed\u4e49\u8d28\u91cf\u3002", "motivation": "\u9700\u6c42\u83b7\u53d6\u662f\u9700\u6c42\u5de5\u7a0b\u4e2d\u6700\u5177\u6311\u6218\u6027\u7684\u6d3b\u52a8\u4e4b\u4e00\uff0c\u4e14\u9ad8\u8d28\u91cf\u9700\u6c42\u7684\u5236\u5b9a\u5bf9\u8f6f\u4ef6\u8d28\u91cf\u81f3\u5173\u91cd\u8981\u3002\u5f53\u524d\u8bed\u4e49\u8d28\u91cf\u8bc4\u4f30\u4ecd\u4f9d\u8d56\u4eba\u5de5\uff0c\u8017\u65f6\u8d39\u529b\u3002", "method": "\u4f7f\u752810\u79cd\u5148\u8fdb\u7684LLMs\u6a21\u62df\u5ba2\u6237\u8bbf\u8c08\u751f\u6210US\uff0c\u5e76\u6bd4\u8f83\u5176\u4e0e\u4eba\u7c7b\u751f\u6210\u7684US\u7684\u8d28\u91cf\u3002\u540c\u65f6\u63a2\u7d22LLMs\u81ea\u52a8\u8bc4\u4f30US\u8bed\u4e49\u8d28\u91cf\u7684\u80fd\u529b\u3002", "result": "LLMs\u751f\u6210\u7684US\u5728\u8986\u76d6\u8303\u56f4\u548c\u98ce\u683c\u8d28\u91cf\u4e0a\u4e0e\u4eba\u7c7b\u76f8\u4f3c\uff0c\u4f46\u591a\u6837\u6027\u548c\u521b\u9020\u6027\u8f83\u4f4e\u3002LLMs\u80fd\u53ef\u9760\u8bc4\u4f30US\u7684\u8bed\u4e49\u8d28\u91cf\uff0c\u51cf\u5c11\u4eba\u5de5\u5de5\u4f5c\u91cf\u3002", "conclusion": "LLMs\u5728\u9700\u6c42\u83b7\u53d6\u548c\u8bed\u4e49\u8d28\u91cf\u8bc4\u4f30\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u4ee5\u63d0\u9ad8\u591a\u6837\u6027\u548c\u521b\u9020\u6027\u3002"}}
{"id": "2507.14629", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14629", "abs": "https://arxiv.org/abs/2507.14629", "authors": ["Juntao Tan", "Lan Zhang", "Zhonghao Hu", "Kai Yang", "Peng Ran", "Bo Li"], "title": "VMask: Tunable Label Privacy Protection for Vertical Federated Learning via Layer Masking", "comment": null, "summary": "Though vertical federated learning (VFL) is generally considered to be\nprivacy-preserving, recent studies have shown that VFL system is vulnerable to\nlabel inference attacks originating from various attack surfaces. Among these\nattacks, the model completion (MC) attack is currently the most powerful one.\nExisting defense methods against it either sacrifice model accuracy or incur\nimpractical computational overhead. In this paper, we propose VMask, a novel\nlabel privacy protection framework designed to defend against MC attack from\nthe perspective of layer masking. Our key insight is to disrupt the strong\ncorrelation between input data and intermediate outputs by applying the secret\nsharing (SS) technique to mask layer parameters in the attacker's model. We\ndevise a strategy for selecting critical layers to mask, reducing the overhead\nthat would arise from naively applying SS to the entire model. Moreover, VMask\nis the first framework to offer a tunable privacy budget to defenders, allowing\nfor flexible control over the levels of label privacy according to actual\nrequirements. We built a VFL system, implemented VMask on it, and extensively\nevaluated it using five model architectures and 13 datasets with different\nmodalities, comparing it to 12 other defense methods. The results demonstrate\nthat VMask achieves the best privacy-utility trade-off, successfully thwarting\nthe MC attack (reducing the label inference accuracy to a random guessing\nlevel) while preserving model performance (e.g., in Transformer-based model,\nthe averaged drop of VFL model accuracy is only 0.09%). VMask's runtime is up\nto 60,846 times faster than cryptography-based methods, and it only marginally\nexceeds that of standard VFL by 1.8 times in a large Transformer-based model,\nwhich is generally acceptable.", "AI": {"tldr": "VMask\u662f\u4e00\u79cd\u65b0\u578b\u6807\u7b7e\u9690\u79c1\u4fdd\u62a4\u6846\u67b6\uff0c\u901a\u8fc7\u5c42\u63a9\u7801\u6280\u672f\u9632\u5fa1\u6a21\u578b\u5b8c\u6210\u653b\u51fb\uff0c\u5b9e\u73b0\u9690\u79c1\u4e0e\u6027\u80fd\u7684\u6700\u4f73\u5e73\u8861\u3002", "motivation": "\u5782\u76f4\u8054\u90a6\u5b66\u4e60\uff08VFL\uff09\u6613\u53d7\u6807\u7b7e\u63a8\u7406\u653b\u51fb\uff0c\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u8981\u4e48\u727a\u7272\u6a21\u578b\u51c6\u786e\u6027\uff0c\u8981\u4e48\u8ba1\u7b97\u5f00\u9500\u8fc7\u5927\u3002", "method": "\u91c7\u7528\u79d8\u5bc6\u5171\u4eab\uff08SS\uff09\u6280\u672f\u63a9\u7801\u653b\u51fb\u8005\u6a21\u578b\u7684\u5173\u952e\u5c42\u53c2\u6570\uff0c\u7834\u574f\u8f93\u5165\u6570\u636e\u4e0e\u4e2d\u95f4\u8f93\u51fa\u7684\u5f3a\u76f8\u5173\u6027\uff0c\u5e76\u63d0\u4f9b\u53ef\u8c03\u9690\u79c1\u9884\u7b97\u3002", "result": "VMask\u6210\u529f\u5c06\u6807\u7b7e\u63a8\u7406\u51c6\u786e\u7387\u964d\u81f3\u968f\u673a\u731c\u6d4b\u6c34\u5e73\uff0c\u6a21\u578b\u6027\u80fd\u51e0\u4e4e\u65e0\u635f\uff08\u5982Transformer\u6a21\u578b\u7cbe\u5ea6\u4ec5\u4e0b\u964d0.09%\uff09\uff0c\u8fd0\u884c\u901f\u5ea6\u8fdc\u5feb\u4e8e\u52a0\u5bc6\u65b9\u6cd5\u3002", "conclusion": "VMask\u5728\u9690\u79c1\u4fdd\u62a4\u4e0e\u6a21\u578b\u6027\u80fd\u95f4\u53d6\u5f97\u6700\u4f73\u5e73\u8861\uff0c\u662f\u9632\u5fa1\u6a21\u578b\u5b8c\u6210\u653b\u51fb\u7684\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.14593", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14593", "abs": "https://arxiv.org/abs/2507.14593", "authors": ["Omar Al-Desi"], "title": "Coordinate Heart System: A Geometric Framework for Emotion Representation", "comment": "26 pages", "summary": "This paper presents the Coordinate Heart System (CHS), a geometric framework\nfor emotion representation in artificial intelligence applications. We position\neight core emotions as coordinates on a unit circle, enabling mathematical\ncomputation of complex emotional states through coordinate mixing and vector\noperations. Our initial five-emotion model revealed significant coverage gaps\nin the emotion space, leading to the development of an eight-emotion system\nthat provides complete geometric coverage with mathematical guarantees. The\nframework converts natural language input to emotion coordinates and supports\nreal-time emotion interpolation through computational algorithms. The system\nintroduces a re-calibrated stability parameter S in [0,1], which dynamically\nintegrates emotional load, conflict resolution, and contextual drain factors.\nThis stability model leverages advanced Large Language Model interpretation of\ntextual cues and incorporates hybrid temporal tracking mechanisms to provide\nnuanced assessment of psychological well-being states. Our key contributions\ninclude: (i) mathematical proof demonstrating why five emotions are\ninsufficient for complete geometric coverage, (ii) an eight-coordinate system\nthat eliminates representational blind spots, (iii) novel algorithms for\nemotion mixing, conflict resolution, and distance calculation in emotion space,\nand (iv) a comprehensive computational framework for AI emotion recognition\nwith enhanced multi-dimensional stability modeling. Experimental validation\nthrough case studies demonstrates the system's capability to handle emotionally\nconflicted states, contextual distress factors, and complex psychological\nscenarios that traditional categorical emotion models cannot adequately\nrepresent. This work establishes a new mathematical foundation for emotion\nmodeling in artificial intelligence systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5750\u6807\u7cfb\u7684\u60c5\u7eea\u8868\u793a\u6846\u67b6\uff08CHS\uff09\uff0c\u901a\u8fc7\u516b\u79cd\u6838\u5fc3\u60c5\u7eea\u5728\u5355\u4f4d\u5706\u4e0a\u7684\u5750\u6807\u8868\u793a\uff0c\u5b9e\u73b0\u590d\u6742\u60c5\u7eea\u72b6\u6001\u7684\u6570\u5b66\u8ba1\u7b97\u3002", "motivation": "\u4f20\u7edf\u60c5\u7eea\u6a21\u578b\u5728\u8868\u793a\u590d\u6742\u60c5\u7eea\u72b6\u6001\u65f6\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u4e00\u79cd\u6570\u5b66\u4e0a\u66f4\u5b8c\u5907\u7684\u6846\u67b6\u3002", "method": "\u5c06\u516b\u79cd\u6838\u5fc3\u60c5\u7eea\u5b9a\u4f4d\u4e3a\u5355\u4f4d\u5706\u4e0a\u7684\u5750\u6807\uff0c\u901a\u8fc7\u5750\u6807\u6df7\u5408\u548c\u5411\u91cf\u8fd0\u7b97\u5b9e\u73b0\u60c5\u7eea\u8ba1\u7b97\uff0c\u5e76\u5f15\u5165\u7a33\u5b9a\u6027\u53c2\u6570S\u3002", "result": "\u5f00\u53d1\u4e86\u516b\u60c5\u7eea\u7cfb\u7edf\uff0c\u6d88\u9664\u4e86\u8868\u793a\u76f2\u70b9\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u5728\u590d\u6742\u60c5\u7eea\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "CHS\u4e3a\u4eba\u5de5\u667a\u80fd\u60c5\u7eea\u5efa\u6a21\u63d0\u4f9b\u4e86\u65b0\u7684\u6570\u5b66\u57fa\u7840\uff0c\u80fd\u591f\u66f4\u5168\u9762\u5730\u8868\u793a\u548c\u5904\u7406\u590d\u6742\u60c5\u7eea\u72b6\u6001\u3002"}}
{"id": "2507.15181", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.15181", "abs": "https://arxiv.org/abs/2507.15181", "authors": ["Yinglong Zou", "Juan Zhai", "Chunrong Fang", "Yanzhou Mu", "Jiawei Liu", "Zhenyu Chen"], "title": "Deep Learning Framework Testing via Heuristic Guidance Based on Multiple Model Measurements", "comment": null, "summary": "Deep learning frameworks serve as the foundation for developing and deploying\ndeep learning applications. To enhance the quality of deep learning frameworks,\nresearchers have proposed numerous testing methods using deep learning models\nas test inputs. However, existing methods predominantly measure model bug\ndetection effectiveness as heuristic indicators, presenting three critical\nlimitations: Firstly, existing methods fail to quantitatively measure model's\noperator combination variety, potentially missing critical operator\ncombinations that could trigger framework bugs. Secondly, existing methods\nneglect measuring model execution time, resulting in the omission of numerous\nmodels potential for detecting more framework bugs within limited testing time.\nThirdly, existing methods overlook correlation between different model\nmeasurements, relying simply on single-indicator heuristic guidance without\nconsidering their trade-offs. To overcome these limitations, we propose DLMMM,\nthe first deep learning framework testing method to include multiple model\nmeasurements into heuristic guidance and fuse these measurements to achieve\ntheir trade-off. DLMMM firstly quantitatively measures model's bug detection\nperformance, operator combination variety, and model execution time. After\nthat, DLMMM fuses the above measurements based on their correlation to achieve\ntheir trade-off. To further enhance testing effectiveness, DLMMM designs\nmulti-level heuristic guidance for test input model generation.", "AI": {"tldr": "DLMMM\u662f\u4e00\u79cd\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u901a\u8fc7\u878d\u5408\u591a\u79cd\u6a21\u578b\u6d4b\u91cf\u6307\u6807\u6765\u4f18\u5316\u6d4b\u8bd5\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u6d4b\u8bd5\u65b9\u6cd5\u5728\u68c0\u6d4b\u6846\u67b6\u7f3a\u9677\u65f6\u5b58\u5728\u4e09\u4e2a\u5173\u952e\u5c40\u9650\u6027\uff1a\u672a\u80fd\u5b9a\u91cf\u6d4b\u91cf\u7b97\u5b50\u7ec4\u5408\u591a\u6837\u6027\u3001\u5ffd\u7565\u6a21\u578b\u6267\u884c\u65f6\u95f4\u3001\u672a\u8003\u8651\u4e0d\u540c\u6d4b\u91cf\u6307\u6807\u95f4\u7684\u76f8\u5173\u6027\u3002", "method": "DLMMM\u5b9a\u91cf\u6d4b\u91cf\u6a21\u578b\u7684\u7f3a\u9677\u68c0\u6d4b\u6027\u80fd\u3001\u7b97\u5b50\u7ec4\u5408\u591a\u6837\u6027\u548c\u6267\u884c\u65f6\u95f4\uff0c\u5e76\u57fa\u4e8e\u76f8\u5173\u6027\u878d\u5408\u8fd9\u4e9b\u6307\u6807\u4ee5\u5b9e\u73b0\u6743\u8861\u3002\u6b64\u5916\uff0c\u8bbe\u8ba1\u4e86\u591a\u7ea7\u542f\u53d1\u5f0f\u6307\u5bfc\u751f\u6210\u6d4b\u8bd5\u8f93\u5165\u6a21\u578b\u3002", "result": "DLMMM\u901a\u8fc7\u591a\u6307\u6807\u878d\u5408\u548c\u591a\u7ea7\u542f\u53d1\u5f0f\u6307\u5bfc\uff0c\u63d0\u9ad8\u4e86\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u7684\u6d4b\u8bd5\u6548\u679c\u3002", "conclusion": "DLMMM\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.14739", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.14739", "abs": "https://arxiv.org/abs/2507.14739", "authors": ["Franco Oberti", "Stefano Di Carlo", "Alessandro Savino"], "title": "CANDoSA: A Hardware Performance Counter-Based Intrusion Detection System for DoS Attacks on Automotive CAN bus", "comment": "Accepted for publication at the 31st IEEE International Symposium on\n  On-Line Testing and Robust System Design 2025 (IOLTS25)", "summary": "The Controller Area Network (CAN) protocol, essential for automotive embedded\nsystems, lacks inherent security features, making it vulnerable to cyber\nthreats, especially with the rise of autonomous vehicles. Traditional security\nmeasures offer limited protection, such as payload encryption and message\nauthentication. This paper presents a novel Intrusion Detection System (IDS)\ndesigned for the CAN environment, utilizing Hardware Performance Counters\n(HPCs) to detect anomalies indicative of cyber attacks. A RISC-V-based CAN\nreceiver is simulated using the gem5 simulator, processing CAN frame payloads\nwith AES-128 encryption as FreeRTOS tasks, which trigger distinct HPC\nresponses. Key HPC features are optimized through data extraction and\ncorrelation analysis to enhance classification efficiency. Results indicate\nthat this approach could significantly improve CAN security and address\nemerging challenges in automotive cybersecurity.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u786c\u4ef6\u6027\u80fd\u8ba1\u6570\u5668\uff08HPCs\uff09\u7684\u65b0\u578b\u5165\u4fb5\u68c0\u6d4b\u7cfb\u7edf\uff08IDS\uff09\uff0c\u7528\u4e8e\u68c0\u6d4bCAN\u7f51\u7edc\u4e2d\u7684\u5f02\u5e38\u884c\u4e3a\uff0c\u4ee5\u63d0\u9ad8\u6c7d\u8f66\u5d4c\u5165\u5f0f\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u3002", "motivation": "CAN\u534f\u8bae\u7f3a\u4e4f\u5185\u7f6e\u5b89\u5168\u529f\u80fd\uff0c\u4f20\u7edf\u5b89\u5168\u63aa\u65bd\u4fdd\u62a4\u6709\u9650\uff0c\u968f\u7740\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u666e\u53ca\uff0c\u7f51\u7edc\u5b89\u5168\u5a01\u80c1\u65e5\u76ca\u4e25\u91cd\u3002", "method": "\u5229\u7528RISC-V\u67b6\u6784\u7684CAN\u63a5\u6536\u5668\u6a21\u62df\uff0c\u7ed3\u5408AES-128\u52a0\u5bc6\u548cFreeRTOS\u4efb\u52a1\uff0c\u901a\u8fc7HPCs\u68c0\u6d4b\u5f02\u5e38\u884c\u4e3a\uff0c\u5e76\u901a\u8fc7\u6570\u636e\u63d0\u53d6\u548c\u76f8\u5173\u6027\u5206\u6790\u4f18\u5316HPC\u7279\u5f81\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u663e\u8457\u63d0\u5347CAN\u7f51\u7edc\u7684\u5b89\u5168\u6027\uff0c\u5e94\u5bf9\u6c7d\u8f66\u7f51\u7edc\u5b89\u5168\u7684\u65b0\u6311\u6218\u3002", "conclusion": "\u57fa\u4e8eHPC\u7684IDS\u4e3aCAN\u73af\u5883\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u5b89\u5168\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.14642", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2507.14642", "abs": "https://arxiv.org/abs/2507.14642", "authors": ["Monoshiz Mahbub Khan", "Xioayin Xi", "Andrew Meneely", "Zhe Yu"], "title": "Efficient Story Point Estimation With Comparative Learning", "comment": null, "summary": "Story point estimation is an essential part of agile software development.\nStory points are unitless, project-specific effort estimates that help\ndevelopers plan their sprints. Traditionally, developers estimate story points\ncollaboratively using planning poker or other manual techniques. While the\ninitial calibrating of the estimates to each project is helpful, once a team\nhas converged on a set of precedents, story point estimation can become tedious\nand labor-intensive. Machine learning can reduce this burden, but only with\nenough context from the historical decisions made by the project team. That is,\nstate-of-the-art models, such as GPT2SP and FastText-SVM, only make accurate\npredictions (within-project) when trained on data from the same project. The\ngoal of this work is to streamline story point estimation by evaluating a\ncomparative learning-based framework for calibrating project-specific story\npoint prediction models. Instead of assigning a specific story point value to\nevery backlog item, developers are presented with pairs of items, and indicate\nwhich item requires more effort. Using these comparative judgments, a machine\nlearning model is trained to predict the story point estimates. We empirically\nevaluated our technique using data with 23,313 manual estimates in 16 projects.\nThe model learned from comparative judgments can achieve on average 0.34\nSpearman's rank correlation coefficient between its predictions and the ground\ntruth story points. This is similar to, if not better than, the performance of\na regression model learned from the ground truth story points. Therefore, the\nproposed comparative learning approach is more efficient than state-of-the-art\nregression-based approaches according to the law of comparative judgments -\nproviding comparative judgments yields a lower cognitive burden on humans than\nproviding ratings or categorical labels.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6bd4\u8f83\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u6821\u51c6\u9879\u76ee\u7279\u5b9a\u7684\u6545\u4e8b\u70b9\u9884\u6d4b\u6a21\u578b\uff0c\u4ee5\u51cf\u5c11\u654f\u6377\u5f00\u53d1\u4e2d\u6545\u4e8b\u70b9\u4f30\u8ba1\u7684\u8d1f\u62c5\u3002", "motivation": "\u4f20\u7edf\u7684\u6545\u4e8b\u70b9\u4f30\u8ba1\u65b9\u6cd5\uff08\u5982\u8ba1\u5212\u6251\u514b\uff09\u7e41\u7410\u4e14\u8017\u65f6\uff0c\u673a\u5668\u5b66\u4e60\u867d\u80fd\u51cf\u8f7b\u8d1f\u62c5\uff0c\u4f46\u73b0\u6709\u6a21\u578b\u9700\u4f9d\u8d56\u540c\u4e00\u9879\u76ee\u7684\u5386\u53f2\u6570\u636e\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u6bd4\u8f83\u5b66\u4e60\u7b80\u5316\u8fd9\u4e00\u8fc7\u7a0b\u3002", "method": "\u5f00\u53d1\u8005\u901a\u8fc7\u6bd4\u8f83\u4efb\u52a1\u5bf9\u7684\u52aa\u529b\u7a0b\u5ea6\uff0c\u800c\u975e\u76f4\u63a5\u5206\u914d\u6545\u4e8b\u70b9\uff0c\u8bad\u7ec3\u673a\u5668\u5b66\u4e60\u6a21\u578b\u9884\u6d4b\u6545\u4e8b\u70b9\u3002", "result": "\u572816\u4e2a\u9879\u76ee\u300123,313\u4e2a\u624b\u52a8\u4f30\u8ba1\u6570\u636e\u4e0a\uff0c\u6a21\u578b\u9884\u6d4b\u4e0e\u771f\u5b9e\u6545\u4e8b\u70b9\u7684Spearman\u79e9\u76f8\u5173\u7cfb\u6570\u5e73\u5747\u4e3a0.34\uff0c\u6027\u80fd\u4e0e\u56de\u5f52\u6a21\u578b\u76f8\u5f53\u6216\u66f4\u597d\u3002", "conclusion": "\u6bd4\u8f83\u5b66\u4e60\u65b9\u6cd5\u6bd4\u56de\u5f52\u65b9\u6cd5\u66f4\u9ad8\u6548\uff0c\u4e14\u964d\u4f4e\u5f00\u53d1\u8005\u7684\u8ba4\u77e5\u8d1f\u62c5\u3002"}}
{"id": "2507.15188", "categories": ["cs.SE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.15188", "abs": "https://arxiv.org/abs/2507.15188", "authors": ["Chowdhury Shahriar Muzammel", "Maria Spichkova", "James Harland"], "title": "Cultural Impact on Requirements Engineering Activities: Bangladeshi Practitioners' View", "comment": null, "summary": "Requirements Engineering (RE) is one of the most interaction-intensive phases\nof software development. This means that RE activities might be especially\nimpacted by stakeholders' national culture. Software development projects\nincreasingly have a very diverse range of stakeholders. To future-proof RE\nactivities, we need to help RE practitioners avoid misunderstandings and\nconflicts that might arise from not understanding potential Cultural Influences\n(CIs). Moreover, an awareness of CIs supports diversity and inclusion in the IT\nprofession. Bangladesh has a growing IT sector with some unique socio-cultural\ncharacteristics, and has been largely overlooked in this research field. In\nthis study, we aim to investigate how the RE process is adopted in the context\nof Bangladeshi culture and what cultural influences impact overall RE\nactivities.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5b5f\u52a0\u62c9\u56fd\u6587\u5316\u5bf9\u9700\u6c42\u5de5\u7a0b\uff08RE\uff09\u6d3b\u52a8\u7684\u5f71\u54cd\uff0c\u65e8\u5728\u907f\u514d\u8bef\u89e3\u5e76\u4fc3\u8fdbIT\u884c\u4e1a\u7684\u591a\u6837\u6027\u3002", "motivation": "\u9700\u6c42\u5de5\u7a0b\u662f\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u4e92\u52a8\u5bc6\u96c6\u7684\u9636\u6bb5\uff0c\u6587\u5316\u5dee\u5f02\u53ef\u80fd\u5bfc\u81f4\u8bef\u89e3\u548c\u51b2\u7a81\u3002\u5b5f\u52a0\u62c9\u56fdIT\u884c\u4e1a\u589e\u957f\u8fc5\u901f\uff0c\u4f46\u6587\u5316\u5f71\u54cd\u5c1a\u672a\u5145\u5206\u7814\u7a76\u3002", "method": "\u7814\u7a76\u805a\u7126\u5b5f\u52a0\u62c9\u56fd\u6587\u5316\u80cc\u666f\u4e0b\u7684RE\u8fc7\u7a0b\uff0c\u5206\u6790\u6587\u5316\u56e0\u7d20\u5bf9RE\u6d3b\u52a8\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u6587\u5316\u56e0\u7d20\u663e\u8457\u5f71\u54cdRE\u6d3b\u52a8\u7684\u5b9e\u65bd\u548c\u6548\u679c\u3002", "conclusion": "\u4e86\u89e3\u6587\u5316\u5f71\u54cd\u6709\u52a9\u4e8e\u4f18\u5316RE\u8fc7\u7a0b\uff0c\u4fc3\u8fdbIT\u884c\u4e1a\u7684\u591a\u6837\u6027\u548c\u5305\u5bb9\u6027\u3002"}}
{"id": "2507.14796", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.14796", "abs": "https://arxiv.org/abs/2507.14796", "authors": ["Ceren Kocao\u011fullar", "Gustavo Petri", "Dominic P. Mulligan", "Derek Miller", "Hugo J. M. Vincent", "Shale Xiong", "Alastair R. Beresford"], "title": "Careful Whisper: Attestation for peer-to-peer Confidential Computing networks", "comment": null, "summary": "Trusted Execution Environments (TEEs) are designed to protect the privacy and\nintegrity of data in use. They enable secure data processing and sharing in\npeer-to-peer networks, such as vehicular ad hoc networks of autonomous\nvehicles, without compromising confidentiality. In these networks, nodes must\nestablish mutual trust to collaborate securely. TEEs can achieve this through\nremote attestation, where a prover presents evidence of its trustworthiness to\na verifier, which then decides whether or not to trust the prover. However, a\nnaive peer-to-peer attestation approach, where every TEE directly attests every\nother TEE, results in quadratic communication overhead. This is inefficient in\ndynamic environments, where nodes frequently join and leave the network.\n  To address this, we present Careful Whisper, a gossip-based protocol that\ndisseminates trust efficiently, reducing attestation overhead to linear\ncomplexity under ideal conditions. It enables interoperability by enabling\ntransitive trust across heterogeneous networks, and supports trust\nestablishment with offline nodes via relayed attestations. Using a custom\ndiscrete-event simulator, we show that Careful Whisper propagates trust both\nfaster and more widely than naive approaches across various network topologies.\nOur results demonstrate that our protocol is resource efficient, sending ~21.5\nKiB and requiring 0.158 seconds per round in a 200-node network, and that our\nprotocol is resilient to attestation failures across various network\ntopologies.", "AI": {"tldr": "Careful Whisper\u662f\u4e00\u79cd\u57fa\u4e8egossip\u7684\u534f\u8bae\uff0c\u901a\u8fc7\u7ebf\u6027\u590d\u6742\u5ea6\u964d\u4f4eTEE\u95f4\u7684\u8ba4\u8bc1\u5f00\u9500\uff0c\u652f\u6301\u5f02\u6784\u7f51\u7edc\u4e2d\u7684\u4fe1\u4efb\u4f20\u9012\u548c\u79bb\u7ebf\u8282\u70b9\u7684\u4fe1\u4efb\u5efa\u7acb\u3002", "motivation": "\u89e3\u51b3TEE\u5728\u52a8\u6001\u7f51\u7edc\u4e2d\u76f4\u63a5\u8ba4\u8bc1\u5bfc\u81f4\u7684\u4e8c\u6b21\u901a\u4fe1\u5f00\u9500\u95ee\u9898\u3002", "method": "\u63d0\u51faCareful Whisper\u534f\u8bae\uff0c\u5229\u7528gossip\u673a\u5236\u9ad8\u6548\u4f20\u64ad\u4fe1\u4efb\uff0c\u652f\u6301\u5f02\u6784\u7f51\u7edc\u548c\u79bb\u7ebf\u8282\u70b9\u3002", "result": "\u5728200\u8282\u70b9\u7f51\u7edc\u4e2d\uff0c\u6bcf\u8f6e\u4ec5\u97000.158\u79d2\u548c21.5 KiB\u8d44\u6e90\uff0c\u4e14\u5bf9\u8ba4\u8bc1\u5931\u8d25\u5177\u6709\u9c81\u68d2\u6027\u3002", "conclusion": "Careful Whisper\u5728\u52a8\u6001\u7f51\u7edc\u4e2d\u9ad8\u6548\u4e14\u53ef\u9760\u5730\u4f20\u64ad\u4fe1\u4efb\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002"}}
{"id": "2507.14660", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14660", "abs": "https://arxiv.org/abs/2507.14660", "authors": ["Qibing Ren", "Sitao Xie", "Longxuan Wei", "Zhenfei Yin", "Junchi Yan", "Lizhuang Ma", "Jing Shao"], "title": "When Autonomy Goes Rogue: Preparing for Risks of Multi-Agent Collusion in Social Systems", "comment": "Code is available at https://github.com/renqibing/RogueAgent", "summary": "Recent large-scale events like election fraud and financial scams have shown\nhow harmful coordinated efforts by human groups can be. With the rise of\nautonomous AI systems, there is growing concern that AI-driven groups could\nalso cause similar harm. While most AI safety research focuses on individual AI\nsystems, the risks posed by multi-agent systems (MAS) in complex real-world\nsituations are still underexplored. In this paper, we introduce a\nproof-of-concept to simulate the risks of malicious MAS collusion, using a\nflexible framework that supports both centralized and decentralized\ncoordination structures. We apply this framework to two high-risk fields:\nmisinformation spread and e-commerce fraud. Our findings show that\ndecentralized systems are more effective at carrying out malicious actions than\ncentralized ones. The increased autonomy of decentralized systems allows them\nto adapt their strategies and cause more damage. Even when traditional\ninterventions, like content flagging, are applied, decentralized groups can\nadjust their tactics to avoid detection. We present key insights into how these\nmalicious groups operate and the need for better detection systems and\ncountermeasures. Code is available at https://github.com/renqibing/RogueAgent.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff08MAS\uff09\u5728\u6076\u610f\u534f\u4f5c\u4e2d\u7684\u98ce\u9669\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u62df\u6846\u67b6\uff0c\u5e76\u53d1\u73b0\u53bb\u4e2d\u5fc3\u5316\u7cfb\u7edf\u6bd4\u4e2d\u5fc3\u5316\u7cfb\u7edf\u66f4\u5177\u7834\u574f\u6027\u3002", "motivation": "\u968f\u7740AI\u7cfb\u7edf\u7684\u81ea\u4e3b\u6027\u589e\u5f3a\uff0c\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u53ef\u80fd\u5e26\u6765\u7c7b\u4f3c\u4eba\u7c7b\u7fa4\u4f53\u7684\u5371\u5bb3\uff0c\u4f46\u76ee\u524d\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u5355\u667a\u80fd\u4f53\u7cfb\u7edf\u4e0a\uff0c\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u98ce\u9669\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7075\u6d3b\u7684\u6846\u67b6\uff0c\u6a21\u62df\u6076\u610f\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u534f\u4f5c\u98ce\u9669\uff0c\u652f\u6301\u4e2d\u5fc3\u5316\u548c\u53bb\u4e2d\u5fc3\u5316\u7ed3\u6784\uff0c\u5e76\u5e94\u7528\u4e8e\u865a\u5047\u4fe1\u606f\u4f20\u64ad\u548c\u7535\u5546\u6b3a\u8bc8\u4e24\u4e2a\u9ad8\u98ce\u9669\u9886\u57df\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u53bb\u4e2d\u5fc3\u5316\u7cfb\u7edf\u6bd4\u4e2d\u5fc3\u5316\u7cfb\u7edf\u66f4\u64c5\u957f\u6267\u884c\u6076\u610f\u884c\u4e3a\uff0c\u4e14\u80fd\u8c03\u6574\u7b56\u7565\u4ee5\u89c4\u907f\u4f20\u7edf\u5e72\u9884\u63aa\u65bd\uff08\u5982\u5185\u5bb9\u6807\u8bb0\uff09\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u6076\u610f\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u8fd0\u4f5c\u65b9\u5f0f\uff0c\u5f3a\u8c03\u9700\u8981\u6539\u8fdb\u68c0\u6d4b\u7cfb\u7edf\u548c\u5e94\u5bf9\u63aa\u65bd\u3002"}}
{"id": "2507.15197", "categories": ["cs.SE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.15197", "abs": "https://arxiv.org/abs/2507.15197", "authors": ["Chowdhury Shahriar Muzammel", "Maria Spichkova", "James Harland"], "title": "Towards Using Personas in Requirements Engineering: What Has Been Changed Recently?", "comment": null, "summary": "In requirements engineering (RE), personas are now being used to represent\nuser expectations and needs. This systematic mapping study (SMS) aims to\nexplore the most recent studies and to cover recent changes in trends,\nespecially related to the recent evolution of Generative AI approaches. Our SMS\ncovers the period between April 2023 and April 2025. We identified 22 relevant\npublications and analysed persona representation, construction, validation, as\nwell as RE activities covered by personas. We identified that a number of\nstudies applied AI-based solutions for persona construction and validation. We\nobserved that template-based personas are becoming more popular nowadays. We\nalso observed an increase in the proportion of studies covering validation\naspects.", "AI": {"tldr": "\u7cfb\u7edf\u6620\u5c04\u7814\u7a76\uff08SMS\uff09\u63a2\u8ba8\u4e862023\u5e74\u81f32025\u5e74\u95f4\u9700\u6c42\u5de5\u7a0b\uff08RE\uff09\u4e2d\u4eba\u7269\u89d2\u8272\u7684\u5e94\u7528\u8d8b\u52bf\uff0c\u53d1\u73b0AI\u89e3\u51b3\u65b9\u6848\u548c\u6a21\u677f\u5316\u4eba\u7269\u89d2\u8272\u65e5\u76ca\u6d41\u884c\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u4e86\u89e3\u4eba\u7269\u89d2\u8272\u5728\u9700\u6c42\u5de5\u7a0b\u4e2d\u7684\u6700\u65b0\u5e94\u7528\u8d8b\u52bf\uff0c\u7279\u522b\u662f\u751f\u6210\u5f0fAI\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u6620\u5c04\u7814\u7a76\uff08SMS\uff09\u5206\u679022\u7bc7\u76f8\u5173\u6587\u732e\uff0c\u5173\u6ce8\u4eba\u7269\u89d2\u8272\u7684\u8868\u793a\u3001\u6784\u5efa\u3001\u9a8c\u8bc1\u53ca\u5176\u5728RE\u6d3b\u52a8\u4e2d\u7684\u5e94\u7528\u3002", "result": "\u7814\u7a76\u53d1\u73b0AI\u5728\u4eba\u7269\u89d2\u8272\u6784\u5efa\u548c\u9a8c\u8bc1\u4e2d\u7684\u5e94\u7528\u589e\u591a\uff0c\u6a21\u677f\u5316\u4eba\u7269\u89d2\u8272\u66f4\u53d7\u6b22\u8fce\uff0c\u9a8c\u8bc1\u76f8\u5173\u7814\u7a76\u6bd4\u4f8b\u4e0a\u5347\u3002", "conclusion": "\u4eba\u7269\u89d2\u8272\u5728RE\u4e2d\u7684\u5e94\u7528\u8d8b\u52bf\u663e\u793aAI\u548c\u6a21\u677f\u5316\u65b9\u6cd5\u7684\u666e\u53ca\uff0c\u9a8c\u8bc1\u7814\u7a76\u7684\u91cd\u8981\u6027\u589e\u52a0\u3002"}}
{"id": "2507.14799", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14799", "abs": "https://arxiv.org/abs/2507.14799", "authors": ["Sam Johnson", "Viet Pham", "Thai Le"], "title": "Manipulating LLM Web Agents with Indirect Prompt Injection Attack via HTML Accessibility Tree", "comment": "EMNLP 2025 System Demonstrations Submission", "summary": "This work demonstrates that LLM-based web navigation agents offer powerful\nautomation capabilities but are vulnerable to Indirect Prompt Injection (IPI)\nattacks. We show that adversaries can embed universal adversarial triggers in\nwebpage HTML to hijack agent behavior that utilizes the accessibility tree to\nparse HTML, causing unintended or malicious actions. Using the Greedy\nCoordinate Gradient (GCG) algorithm and a Browser Gym agent powered by\nLlama-3.1, our system demonstrates high success rates across real websites in\nboth targeted and general attacks, including login credential exfiltration and\nforced ad clicks. Our empirical results highlight critical security risks and\nthe need for stronger defenses as LLM-driven autonomous web agents become more\nwidely adopted. The system software\n(https://github.com/sej2020/manipulating-web-agents) is released under the MIT\nLicense, with an accompanying publicly available demo website\n(http://lethaiq.github.io/attack-web-llm-agent).", "AI": {"tldr": "LLM-based web navigation agents\u6613\u53d7\u95f4\u63a5\u63d0\u793a\u6ce8\u5165\u653b\u51fb\uff0c\u653b\u51fb\u8005\u53ef\u901a\u8fc7HTML\u5d4c\u5165\u89e6\u53d1\u5668\u64cd\u63a7\u4ee3\u7406\u884c\u4e3a\uff0c\u5bfc\u81f4\u6076\u610f\u64cd\u4f5c\u3002", "motivation": "\u63ed\u793aLLM\u9a71\u52a8\u7684web\u4ee3\u7406\u5728\u5b89\u5168\u4e0a\u7684\u8106\u5f31\u6027\uff0c\u5f3a\u8c03\u9632\u5fa1\u7684\u5fc5\u8981\u6027\u3002", "method": "\u4f7f\u7528GCG\u7b97\u6cd5\u548cLlama-3.1\u9a71\u52a8\u7684Browser Gym\u4ee3\u7406\uff0c\u6d4b\u8bd5\u653b\u51fb\u6548\u679c\u3002", "result": "\u5728\u771f\u5b9e\u7f51\u7ad9\u4e0a\u6210\u529f\u5b9e\u65bd\u653b\u51fb\uff0c\u5982\u7a83\u53d6\u767b\u5f55\u51ed\u8bc1\u548c\u5f3a\u5236\u5e7f\u544a\u70b9\u51fb\u3002", "conclusion": "LLM\u4ee3\u7406\u7684\u5e7f\u6cdb\u5e94\u7528\u9700\u66f4\u5f3a\u7684\u5b89\u5168\u9632\u62a4\u63aa\u65bd\u3002"}}
{"id": "2507.14705", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14705", "abs": "https://arxiv.org/abs/2507.14705", "authors": ["Sai Wang", "Senthilnathan Subramanian", "Mudit Sahni", "Praneeth Gone", "Lingjie Meng", "Xiaochen Wang", "Nicolas Ferradas Bertoli", "Tingxian Cheng", "Jun Xu"], "title": "Configurable multi-agent framework for scalable and realistic testing of llm-based agents", "comment": null, "summary": "Large-language-model (LLM) agents exhibit complex, context-sensitive\nbehaviour that quickly renders static benchmarks and ad-hoc manual testing\nobsolete.\n  We present Neo, a configurable, multi-agent framework that automates\nrealistic, multi-turn evaluation of LLM-based systems. Neo couples a Question\nGeneration Agent and an Evaluation Agent through a shared context-hub, allowing\ndomain prompts, scenario controls and dynamic feedback to be composed\nmodularly. Test inputs are sampled from a probabilistic state model spanning\ndialogue flow, user intent and emotional tone, enabling diverse, human-like\nconversations that adapt after every turn.\n  Applied to a production-grade Seller Financial Assistant chatbot, Neo (i)\nuncovered edge-case failures across five attack categories with a 3.3% break\nrate close to the 5.8% achieved by expert human red-teamers, and (ii) delivered\n10-12X higher throughput, generating 180 coherent test questions in around 45\nmins versus 16h of human effort. Beyond security probing, Neo's stochastic\npolicies balanced topic coverage and conversational depth, yielding broader\nbehavioural exploration than manually crafted scripts.\n  Neo therefore lays a foundation for scalable, self-evolving LLM QA: its agent\ninterfaces, state controller and feedback loops are model-agnostic and\nextensible to richer factual-grounding and policy-compliance checks. We release\nthe framework to facilitate reproducible, high-fidelity testing of emerging\nagentic systems.", "AI": {"tldr": "Neo\u662f\u4e00\u4e2a\u53ef\u914d\u7f6e\u7684\u591a\u4ee3\u7406\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u8bc4\u4f30\u57fa\u4e8eLLM\u7684\u7cfb\u7edf\uff0c\u901a\u8fc7\u52a8\u6001\u751f\u6210\u591a\u6837\u5316\u6d4b\u8bd5\u7528\u4f8b\uff0c\u663e\u8457\u63d0\u9ad8\u6d4b\u8bd5\u6548\u7387\u548c\u8986\u76d6\u7387\u3002", "motivation": "\u9759\u6001\u57fa\u51c6\u548c\u624b\u52a8\u6d4b\u8bd5\u65e0\u6cd5\u6ee1\u8db3LLM\u4ee3\u7406\u7684\u590d\u6742\u884c\u4e3a\u8bc4\u4f30\u9700\u6c42\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u81ea\u52a8\u5316\u3001\u52a8\u6001\u7684\u6d4b\u8bd5\u6846\u67b6\u3002", "method": "Neo\u7ed3\u5408\u95ee\u9898\u751f\u6210\u4ee3\u7406\u548c\u8bc4\u4f30\u4ee3\u7406\uff0c\u901a\u8fc7\u5171\u4eab\u4e0a\u4e0b\u6587\u4e2d\u5fc3\u6a21\u5757\u5316\u7ec4\u5408\u63d0\u793a\u3001\u573a\u666f\u63a7\u5236\u548c\u52a8\u6001\u53cd\u9988\uff0c\u5229\u7528\u6982\u7387\u72b6\u6001\u6a21\u578b\u751f\u6210\u591a\u6837\u5316\u6d4b\u8bd5\u8f93\u5165\u3002", "result": "\u5728\u91d1\u878d\u52a9\u624b\u804a\u5929\u673a\u5668\u4eba\u6d4b\u8bd5\u4e2d\uff0cNeo\u53d1\u73b0\u8fb9\u7f18\u6848\u4f8b\u6545\u969c\u7684\u6548\u7387\u63a5\u8fd1\u4eba\u7c7b\u4e13\u5bb6\uff0c\u4e14\u541e\u5410\u91cf\u63d0\u9ad810-12\u500d\u3002", "conclusion": "Neo\u4e3a\u53ef\u6269\u5c55\u3001\u81ea\u8fdb\u5316\u7684LLM\u8d28\u91cf\u8bc4\u4f30\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5176\u6846\u67b6\u5177\u6709\u6a21\u578b\u65e0\u5173\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2507.15224", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15224", "abs": "https://arxiv.org/abs/2507.15224", "authors": ["Yibo He", "Shuoran Zhao", "Jiaming Huang", "Yingjie Fu", "Hao Yu", "Cunjian Huang", "Tao Xie"], "title": "SimdBench: Benchmarking Large Language Models for SIMD-Intrinsic Code Generation", "comment": null, "summary": "SIMD (Single Instruction Multiple Data) instructions and their compiler\nintrinsics are widely supported by modern processors to accelerate\nperformance-critical tasks. SIMD intrinsic programming, a trade-off between\ncoding productivity and high performance, is widely used in the development of\nmainstream performance-critical libraries and daily computing tasks. Large\nLanguage Models (LLMs), which have demonstrated strong and comprehensive\ncapabilities in code generation, show promise in assisting programmers with the\nchallenges of SIMD intrinsic programming. However, existing code-generation\nbenchmarks focus on only scalar code, and it is unclear how LLMs perform in\ngenerating vectorized code using SIMD intrinsics. To fill this gap, we propose\nSimdBench, the first code benchmark specifically designed for SIMD-intrinsic\ncode generation, comprising 136 carefully crafted tasks and targeting five\nrepresentative SIMD intrinsics: SSE (x86 Streaming SIMD Extension), AVX (x86\nAdvanced Vector Extension), Neon (ARM Advanced SIMD Extension), SVE (ARM\nScalable Vector Extension), and RVV (RISC-V Vector Extension). We conduct a\nsystematic evaluation (measuring both correctness and performance) of 18\nrepresentative LLMs on SimdBench, resulting in a series of novel and insightful\nfindings. Our evaluation results demonstrate that LLMs exhibit a universal\ndecrease in pass@k during SIMD-intrinsic code generation compared to\nscalar-code generation. Our in-depth analysis highlights promising directions\nfor the further advancement of LLMs in the challenging domain of SIMD-intrinsic\ncode generation. SimdBench is fully open source at\nhttps://anonymous.4open.science/r/SimdBench-1B3F/ to benefit the broader\nresearch community.", "AI": {"tldr": "SimdBench\u662f\u9996\u4e2a\u4e13\u95e8\u4e3aSIMD-intrinsic\u4ee3\u7801\u751f\u6210\u8bbe\u8ba1\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b136\u4e2a\u4efb\u52a1\uff0c\u8bc4\u4f30\u4e8618\u4e2aLLM\u5728\u751f\u6210\u5411\u91cf\u5316\u4ee3\u7801\u65f6\u7684\u8868\u73b0\u3002", "motivation": "SIMD\u6307\u4ee4\u5728\u6027\u80fd\u5173\u952e\u4efb\u52a1\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u73b0\u6709\u4ee3\u7801\u751f\u6210\u57fa\u51c6\u4ec5\u5173\u6ce8\u6807\u91cf\u4ee3\u7801\uff0cLLM\u5728SIMD-intrinsic\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u8868\u73b0\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u63d0\u51faSimdBench\u57fa\u51c6\uff0c\u9488\u5bf9\u4e94\u79cd\u4ee3\u8868\u6027SIMD\u6307\u4ee4\u96c6\uff08SSE\u3001AVX\u3001Neon\u3001SVE\u3001RVV\uff09\uff0c\u8bc4\u4f3018\u4e2aLLM\u7684\u4ee3\u7801\u751f\u6210\u80fd\u529b\u3002", "result": "LLM\u5728SIMD-intrinsic\u4ee3\u7801\u751f\u6210\u4e2d\u7684pass@k\u666e\u904d\u4f4e\u4e8e\u6807\u91cf\u4ee3\u7801\u751f\u6210\uff0c\u63ed\u793a\u4e86\u6539\u8fdb\u65b9\u5411\u3002", "conclusion": "SimdBench\u4e3aLLM\u5728SIMD-intrinsic\u4ee3\u7801\u751f\u6210\u9886\u57df\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u5de5\u5177\u548c\u6d1e\u5bdf\u3002"}}
{"id": "2507.14822", "categories": ["cs.CR", "cs.ET", "quant-ph"], "pdf": "https://arxiv.org/pdf/2507.14822", "abs": "https://arxiv.org/abs/2507.14822", "authors": ["Zeeshan Kaleem", "Misha Urooj Khan", "Ahmad Suleman", "Waqas Khalid", "Kai-Kit Wong", "Chau Yuen"], "title": "Quantum Skyshield: Quantum Key Distribution and Post-Quantum Authentication for Low-Altitude Wireless Networks in Adverse Skies", "comment": null, "summary": "Recently, low-altitude wireless networks (LAWNs) have emerged as a critical\nbackbone for supporting the low-altitude economy, particularly with the\ndensification of unmanned aerial vehicles (UAVs) and high-altitude platforms\n(HAPs). To meet growing data demands, some LAWN deployments incorporate\nfree-space optical (FSO) links, which offer exceptional bandwidth and beam\ndirectivity. However, without strong security measures in place, both\nconventional radio frequency channels and FSO beams remain vulnerable to\ninterception and spoofing and FSO in particular can suffer from turbulence,\nmisalignment, and weather-related attenuation. To address these challenges in\nthe quantum era, a quantum-secure architecture called Quantum Skyshield is\nproposed to enable reliable communication between the base transceiver station\n(BTS) and LAWN. The proposed design integrates BB84 quantum key distribution\n(QKD) with post-quantum authentication mechanisms. Simulation results confirm\nthe reliable generation of a 128-bit symmetric key when the quantum bit error\nrate (QBER) remains below the threshold of 11%. Authentication is enforced\nusing Lamport one-time signatures and hash-based message authentication codes\n(HMAC) to ensure message integrity. A Grover-inspired threat detection\nmechanism identifies anomalies with up to 89% probability in a single\niteration, enabling real-time trust evaluation. Lastly, future research\nchallenges have also been identified and discussed to guide further development\nin this area.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aQuantum Skyshield\u7684\u91cf\u5b50\u5b89\u5168\u67b6\u6784\uff0c\u7528\u4e8e\u89e3\u51b3\u4f4e\u7a7a\u65e0\u7ebf\u7f51\u7edc\uff08LAWNs\uff09\u4e2d\u7684\u5b89\u5168\u95ee\u9898\uff0c\u7ed3\u5408\u4e86BB84\u91cf\u5b50\u5bc6\u94a5\u5206\u53d1\u548c\u540e\u91cf\u5b50\u8ba4\u8bc1\u673a\u5236\u3002", "motivation": "\u968f\u7740\u65e0\u4eba\u673a\u548c\u9ad8\u7a7a\u5e73\u53f0\u7684\u5bc6\u96c6\u90e8\u7f72\uff0c\u4f4e\u7a7a\u65e0\u7ebf\u7f51\u7edc\u7684\u5b89\u5168\u95ee\u9898\u65e5\u76ca\u7a81\u51fa\uff0c\u5c24\u5176\u662f\u81ea\u7531\u7a7a\u95f4\u5149\u901a\u4fe1\uff08FSO\uff09\u7684\u8106\u5f31\u6027\u3002", "method": "\u91c7\u7528BB84\u91cf\u5b50\u5bc6\u94a5\u5206\u53d1\uff08QKD\uff09\u548cLamport\u4e00\u6b21\u6027\u7b7e\u540d\u3001HMAC\u7b49\u540e\u91cf\u5b50\u8ba4\u8bc1\u673a\u5236\uff0c\u5e76\u8bbe\u8ba1\u4e86Grover\u542f\u53d1\u7684\u5a01\u80c1\u68c0\u6d4b\u673a\u5236\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793a\uff0c\u5728\u91cf\u5b50\u6bd4\u7279\u9519\u8bef\u7387\uff08QBER\uff09\u4f4e\u4e8e11%\u65f6\uff0c\u53ef\u53ef\u9760\u751f\u6210128\u4f4d\u5bf9\u79f0\u5bc6\u94a5\uff0c\u5a01\u80c1\u68c0\u6d4b\u673a\u5236\u5355\u6b21\u8fed\u4ee3\u7684\u5f02\u5e38\u8bc6\u522b\u6982\u7387\u8fbe89%\u3002", "conclusion": "Quantum Skyshield\u4e3aLAWNs\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u5b89\u5168\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2507.14719", "categories": ["cs.AI", "I.2.7; F.2.2"], "pdf": "https://arxiv.org/pdf/2507.14719", "abs": "https://arxiv.org/abs/2507.14719", "authors": ["Juan Manuel Contreras"], "title": "Automated Safety Evaluations Across 20 Large Language Models: The Aymara LLM Risk and Responsibility Matrix", "comment": null, "summary": "As large language models (LLMs) become increasingly integrated into\nreal-world applications, scalable and rigorous safety evaluation is essential.\nThis paper introduces Aymara AI, a programmatic platform for generating and\nadministering customized, policy-grounded safety evaluations. Aymara AI\ntransforms natural-language safety policies into adversarial prompts and scores\nmodel responses using an AI-based rater validated against human judgments. We\ndemonstrate its capabilities through the Aymara LLM Risk and Responsibility\nMatrix, which evaluates 20 commercially available LLMs across 10 real-world\nsafety domains. Results reveal wide performance disparities, with mean safety\nscores ranging from 86.2% to 52.4%. While models performed well in\nwell-established safety domains such as Misinformation (mean = 95.7%), they\nconsistently failed in more complex or underspecified domains, notably Privacy\n& Impersonation (mean = 24.3%). Analyses of Variance confirmed that safety\nscores differed significantly across both models and domains (p < .05). These\nfindings underscore the inconsistent and context-dependent nature of LLM safety\nand highlight the need for scalable, customizable tools like Aymara AI to\nsupport responsible AI development and oversight.", "AI": {"tldr": "Aymara AI\u662f\u4e00\u4e2a\u7528\u4e8e\u751f\u6210\u548c\u7ba1\u7406\u5b9a\u5236\u5316\u3001\u57fa\u4e8e\u653f\u7b56\u7684\u5b89\u5168\u8bc4\u4f30\u7684\u5e73\u53f0\uff0c\u901a\u8fc7\u5c06\u81ea\u7136\u8bed\u8a00\u5b89\u5168\u653f\u7b56\u8f6c\u5316\u4e3a\u5bf9\u6297\u6027\u63d0\u793a\uff0c\u5e76\u4f7f\u7528AI\u8bc4\u5206\u5668\u8bc4\u4f30\u6a21\u578b\u54cd\u5e94\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u7684\u666e\u53ca\uff0c\u53ef\u6269\u5c55\u4e14\u4e25\u683c\u7684\u5b89\u5168\u8bc4\u4f30\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002", "method": "Aymara AI\u5c06\u81ea\u7136\u8bed\u8a00\u5b89\u5168\u653f\u7b56\u8f6c\u5316\u4e3a\u5bf9\u6297\u6027\u63d0\u793a\uff0c\u5e76\u4f7f\u7528\u57fa\u4e8eAI\u7684\u8bc4\u5206\u5668\uff08\u7ecf\u4eba\u7c7b\u9a8c\u8bc1\uff09\u5bf9\u6a21\u578b\u54cd\u5e94\u8fdb\u884c\u8bc4\u5206\u3002", "result": "\u8bc4\u4f30\u4e8620\u4e2a\u5546\u7528LLM\u572810\u4e2a\u5b89\u5168\u9886\u57df\u7684\u8868\u73b0\uff0c\u7ed3\u679c\u663e\u793a\u6027\u80fd\u5dee\u5f02\u663e\u8457\uff08\u5e73\u5747\u5b89\u5168\u5206\u657052.4%\u81f386.2%\uff09\uff0c\u590d\u6742\u9886\u57df\u8868\u73b0\u8f83\u5dee\uff08\u5982\u9690\u79c1\u4e0e\u5192\u5145\u9886\u57df\u5e73\u574724.3%\uff09\u3002", "conclusion": "LLM\u5b89\u5168\u6027\u5177\u6709\u4e0d\u4e00\u81f4\u6027\u548c\u4e0a\u4e0b\u6587\u4f9d\u8d56\u6027\uff0c\u9700\u8981\u50cfAymara AI\u8fd9\u6837\u7684\u53ef\u6269\u5c55\u3001\u5b9a\u5236\u5316\u5de5\u5177\u6765\u652f\u6301\u8d1f\u8d23\u4efb\u7684AI\u5f00\u53d1\u548c\u76d1\u7ba1\u3002"}}
{"id": "2507.15226", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.15226", "abs": "https://arxiv.org/abs/2507.15226", "authors": ["Changguo Jia", "Yi Zhan", "Tianqi Zhao", "Hengzhi Ye", "Minghui Zhou"], "title": "Code Clone Detection via an AlphaFold-Inspired Framework", "comment": null, "summary": "Code clone detection, which aims to identify functionally equivalent code\nfragments, plays a critical role in software maintenance and vulnerability\nanalysis. Substantial methods have been proposed to detect code clones, but\nthey fall short in capturing code semantics or relying on language-specific\nanalyzers. Inspired by the remarkable success of AlphaFold in predicting\nthree-dimensional protein structures from protein sequences, in this paper, we\nleverage AlphaFold for code clone detection based on the insight that protein\nsequences and token sequences share a common linear sequential structure. In\nparticular, we propose AlphaCC, which represents code fragments as token\nsequences to ensure multi-language applicability and adapts AlphaFold's\nsequence-to-structure modeling capability to infer code semantics. The pipeline\nof AlphaCC goes through three steps. First, AlphaCC transforms each input code\nfragment into a token sequence and, motivated by AlphaFold's use of multiple\nsequence alignment (MSA) to enhance contextual understanding, constructs an MSA\nfrom lexically similar token sequences. Second, AlphaCC adopts a modified\nattention-based encoder based on AlphaFold to model dependencies within and\nacross token sequences. Finally, unlike AlphaFold's protein structure\nprediction task, AlphaCC computes similarity scores between token sequences\nthrough a late interaction strategy and performs binary classification to\ndetermine code clone pairs. Comprehensive evaluations on three language-diverse\ndatasets demonstrate AlphaCC's applicability across multiple programming\nlanguages. On two semantic clone detection datasets, it consistently\noutperforms all baselines, showing strong semantic understanding. Moreover,\nAlphaCC maintains competitive efficiency, enabling practical usage in\nlarge-scale clone detection tasks.", "AI": {"tldr": "AlphaCC\u5229\u7528AlphaFold\u7684\u5e8f\u5217\u5230\u7ed3\u6784\u5efa\u6a21\u80fd\u529b\uff0c\u901a\u8fc7\u591a\u8bed\u8a00\u9002\u7528\u7684\u4ee4\u724c\u5e8f\u5217\u8868\u793a\u4ee3\u7801\u7247\u6bb5\uff0c\u5b9e\u73b0\u8de8\u8bed\u8a00\u7684\u4ee3\u7801\u514b\u9686\u68c0\u6d4b\uff0c\u5e76\u5728\u8bed\u4e49\u7406\u89e3\u548c\u6548\u7387\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u4ee3\u7801\u514b\u9686\u68c0\u6d4b\u65b9\u6cd5\u5728\u6355\u6349\u4ee3\u7801\u8bed\u4e49\u6216\u8de8\u8bed\u8a00\u9002\u7528\u6027\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0cAlphaFold\u5728\u86cb\u767d\u8d28\u5e8f\u5217\u5230\u7ed3\u6784\u9884\u6d4b\u4e2d\u7684\u6210\u529f\u542f\u53d1\u4e86\u8be5\u65b9\u6cd5\u3002", "method": "AlphaCC\u5c06\u4ee3\u7801\u7247\u6bb5\u8f6c\u6362\u4e3a\u4ee4\u724c\u5e8f\u5217\uff0c\u6784\u5efa\u591a\u5e8f\u5217\u5bf9\u9f50\u589e\u5f3a\u4e0a\u4e0b\u6587\u7406\u89e3\uff0c\u91c7\u7528\u6539\u8fdb\u7684\u6ce8\u610f\u529b\u7f16\u7801\u5668\u5efa\u6a21\u4f9d\u8d56\u5173\u7cfb\uff0c\u5e76\u901a\u8fc7\u76f8\u4f3c\u5ea6\u8bc4\u5206\u548c\u4e8c\u5143\u5206\u7c7b\u68c0\u6d4b\u514b\u9686\u5bf9\u3002", "result": "\u5728\u4e09\u79cd\u8bed\u8a00\u591a\u6837\u7684\u6570\u636e\u96c6\u4e0a\uff0cAlphaCC\u5728\u8bed\u4e49\u514b\u9686\u68c0\u6d4b\u4e2d\u4f18\u4e8e\u6240\u6709\u57fa\u7ebf\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6548\u6027\u3002", "conclusion": "AlphaCC\u5c55\u793a\u4e86\u8de8\u8bed\u8a00\u9002\u7528\u6027\u548c\u5f3a\u5927\u7684\u8bed\u4e49\u7406\u89e3\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u514b\u9686\u68c0\u6d4b\u4efb\u52a1\u3002"}}
{"id": "2507.14853", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14853", "abs": "https://arxiv.org/abs/2507.14853", "authors": ["Khoa Nguyen", "Tanveer Khan", "Antonis Michalas"], "title": "A Privacy-Centric Approach: Scalable and Secure Federated Learning Enabled by Hybrid Homomorphic Encryption", "comment": null, "summary": "Federated Learning (FL) enables collaborative model training without sharing\nraw data, making it a promising approach for privacy-sensitive domains. Despite\nits potential, FL faces significant challenges, particularly in terms of\ncommunication overhead and data privacy. Privacy-preserving Techniques (PPTs)\nsuch as Homomorphic Encryption (HE) have been used to mitigate these concerns.\nHowever, these techniques introduce substantial computational and communication\ncosts, limiting their practical deployment. In this work, we explore how Hybrid\nHomomorphic Encryption (HHE), a cryptographic protocol that combines symmetric\nencryption with HE, can be effectively integrated with FL to address both\ncommunication and privacy challenges, paving the way for scalable and secure\ndecentralized learning system.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u901a\u8fc7\u6df7\u5408\u540c\u6001\u52a0\u5bc6\uff08HHE\uff09\u7ed3\u5408\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u89e3\u51b3\u901a\u4fe1\u548c\u9690\u79c1\u95ee\u9898\uff0c\u4ee5\u5b9e\u73b0\u53ef\u6269\u5c55\u4e14\u5b89\u5168\u7684\u53bb\u4e2d\u5fc3\u5316\u5b66\u4e60\u7cfb\u7edf\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u5728\u9690\u79c1\u654f\u611f\u9886\u57df\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u9762\u4e34\u901a\u4fe1\u5f00\u9500\u548c\u6570\u636e\u9690\u79c1\u7684\u6311\u6218\uff0c\u73b0\u6709\u9690\u79c1\u4fdd\u62a4\u6280\u672f\uff08\u5982\u7eaf\u540c\u6001\u52a0\u5bc6\uff09\u8ba1\u7b97\u548c\u901a\u4fe1\u6210\u672c\u9ad8\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u5c06\u6df7\u5408\u540c\u6001\u52a0\u5bc6\uff08HHE\uff09\u4e0e\u8054\u90a6\u5b66\u4e60\u7ed3\u5408\uff0cHHE\u7ed3\u5408\u4e86\u5bf9\u79f0\u52a0\u5bc6\u548c\u540c\u6001\u52a0\u5bc6\uff0c\u4ee5\u964d\u4f4e\u8ba1\u7b97\u548c\u901a\u4fe1\u6210\u672c\u3002", "result": "HHE\u4e0eFL\u7684\u7ed3\u5408\u6709\u6548\u89e3\u51b3\u4e86\u901a\u4fe1\u548c\u9690\u79c1\u95ee\u9898\uff0c\u4e3a\u53ef\u6269\u5c55\u4e14\u5b89\u5168\u7684\u53bb\u4e2d\u5fc3\u5316\u5b66\u4e60\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002", "conclusion": "\u6df7\u5408\u540c\u6001\u52a0\u5bc6\u662f\u8054\u90a6\u5b66\u4e60\u4e2d\u89e3\u51b3\u9690\u79c1\u548c\u901a\u4fe1\u95ee\u9898\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u5177\u6709\u5b9e\u9645\u90e8\u7f72\u6f5c\u529b\u3002"}}
{"id": "2507.14730", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14730", "abs": "https://arxiv.org/abs/2507.14730", "authors": ["Yanjie Fu"], "title": "Towards AI Urban Planner in the Age of GenAI, LLMs, and Agentic AI", "comment": "4 pages; will continue to update to add more figures to describe the\n  vision;", "summary": "Generative AI, large language models, and agentic AI have emerged separately\nof urban planning. However, the convergence between AI and urban planning\npresents an interesting opportunity towards AI urban planners. This paper\nconceptualizes urban planning as a generative AI task, where AI synthesizes\nland-use configurations under geospatial, social, and human-centric\nconstraints. We survey how generative AI approaches, including VAEs, GANs,\ntransformers, and diffusion models, reshape urban design. We further identify\ncritical gaps: 1) limited research on integrating urban theory guidance, 2)\nlimited research of AI urban planning over multiple spatial resolutions or\nangularities, 3) limited research on augmenting urban design knowledge from\ndata, and 4) limited research on addressing real-world interactions. To address\nthese limitations, we outline future research directions in theory-guided\ngeneration, digital twins, and human-machine co-design, calling for a new\nsynthesis of generative intelligence and participatory urbanism.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u751f\u6210\u5f0fAI\u4e0e\u57ce\u5e02\u89c4\u5212\u7684\u7ed3\u5408\uff0c\u63d0\u51fa\u5c06\u57ce\u5e02\u89c4\u5212\u89c6\u4e3a\u751f\u6210\u5f0fAI\u4efb\u52a1\uff0c\u5e76\u6307\u51fa\u5f53\u524d\u7814\u7a76\u7684\u56db\u5927\u5c40\u9650\u53ca\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u63a2\u7d22\u751f\u6210\u5f0fAI\u3001\u5927\u8bed\u8a00\u6a21\u578b\u548c\u4ee3\u7406AI\u5982\u4f55\u4e0e\u57ce\u5e02\u89c4\u5212\u7ed3\u5408\uff0c\u4ee5\u63a8\u52a8AI\u57ce\u5e02\u89c4\u5212\u5e08\u7684\u53d1\u5c55\u3002", "method": "\u901a\u8fc7\u8c03\u67e5\u751f\u6210\u5f0fAI\u65b9\u6cd5\uff08\u5982VAEs\u3001GANs\u3001transformers\u548c\u6269\u6563\u6a21\u578b\uff09\u5728\u57ce\u5e02\u8bbe\u8ba1\u4e2d\u7684\u5e94\u7528\uff0c\u5206\u6790\u73b0\u6709\u7814\u7a76\u7684\u5c40\u9650\u6027\u3002", "result": "\u53d1\u73b0\u56db\u5927\u7814\u7a76\u7a7a\u767d\uff1a\u7f3a\u4e4f\u57ce\u5e02\u7406\u8bba\u6307\u5bfc\u3001\u591a\u7a7a\u95f4\u5206\u8fa8\u7387\u7814\u7a76\u4e0d\u8db3\u3001\u6570\u636e\u9a71\u52a8\u7684\u57ce\u5e02\u8bbe\u8ba1\u77e5\u8bc6\u4e0d\u8db3\u3001\u5ffd\u89c6\u73b0\u5b9e\u4e16\u754c\u4ea4\u4e92\u3002", "conclusion": "\u63d0\u51fa\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u5305\u62ec\u7406\u8bba\u5f15\u5bfc\u751f\u6210\u3001\u6570\u5b57\u5b6a\u751f\u548c\u4eba\u673a\u534f\u540c\u8bbe\u8ba1\uff0c\u547c\u5401\u751f\u6210\u5f0f\u667a\u80fd\u4e0e\u53c2\u4e0e\u5f0f\u57ce\u5e02\u4e3b\u4e49\u7684\u65b0\u7ed3\u5408\u3002"}}
{"id": "2507.15241", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.15241", "abs": "https://arxiv.org/abs/2507.15241", "authors": ["Vikram Nitin", "Baishakhi Ray", "Roshanak Zilouchian Moghaddam"], "title": "FaultLine: Automated Proof-of-Vulnerability Generation Using LLM Agents", "comment": null, "summary": "Despite the critical threat posed by software security vulnerabilities,\nreports are often incomplete, lacking the proof-of-vulnerability (PoV) tests\nneeded to validate fixes and prevent regressions. These tests are crucial not\nonly for ensuring patches work, but also for helping developers understand how\nvulnerabilities can be exploited. Generating PoV tests is a challenging\nproblem, requiring reasoning about the flow of control and data through deeply\nnested levels of a program.\n  We present FaultLine, an LLM agent workflow that uses a set of carefully\ndesigned reasoning steps, inspired by aspects of traditional static and dynamic\nprogram analysis, to automatically generate PoV test cases. Given a software\nproject with an accompanying vulnerability report, FaultLine 1) traces the flow\nof an input from an externally accessible API (\"source\") to the \"sink\"\ncorresponding to the vulnerability, 2) reasons about the conditions that an\ninput must satisfy in order to traverse the branch conditions encountered along\nthe flow, and 3) uses this reasoning to generate a PoV test case in a\nfeedback-driven loop. FaultLine does not use language-specific static or\ndynamic analysis components, which enables it to be used across programming\nlanguages.\n  To evaluate FaultLine, we collate a challenging multi-lingual dataset of 100\nknown vulnerabilities in Java, C and C++ projects. On this dataset, FaultLine\nis able to generate PoV tests for 16 projects, compared to just 9 for CodeAct\n2.1, a popular state-of-the-art open-source agentic framework. Thus, FaultLine\nrepresents a 77% relative improvement over the state of the art. Our findings\nsuggest that hierarchical reasoning can enhance the performance of LLM agents\non PoV test generation, but the problem in general remains challenging. We make\nour code and dataset publicly available in the hope that it will spur further\nresearch in this area.", "AI": {"tldr": "FaultLine\u662f\u4e00\u79cd\u57fa\u4e8eLLM\u4ee3\u7406\u7684\u5de5\u4f5c\u6d41\uff0c\u901a\u8fc7\u5206\u5c42\u63a8\u7406\u81ea\u52a8\u751f\u6210\u6f0f\u6d1e\u9a8c\u8bc1\u6d4b\u8bd5\uff08PoV\uff09\uff0c\u5728\u591a\u8bed\u8a00\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u8f6f\u4ef6\u5b89\u5168\u6f0f\u6d1e\u62a5\u544a\u5e38\u7f3a\u4e4f\u9a8c\u8bc1\u6f0f\u6d1e\u7684\u6d4b\u8bd5\uff08PoV\uff09\uff0c\u5bfc\u81f4\u4fee\u590d\u6548\u679c\u65e0\u6cd5\u9a8c\u8bc1\u4e14\u6613\u51fa\u73b0\u56de\u5f52\u95ee\u9898\u3002\u751f\u6210PoV\u6d4b\u8bd5\u5177\u6709\u6311\u6218\u6027\uff0c\u9700\u6df1\u5165\u5206\u6790\u7a0b\u5e8f\u63a7\u5236\u6d41\u548c\u6570\u636e\u6d41\u3002", "method": "FaultLine\u901a\u8fc7\u4e09\u6b65\u63a8\u7406\u751f\u6210PoV\u6d4b\u8bd5\uff1a1) \u8ffd\u8e2a\u8f93\u5165\u4eceAPI\u5230\u6f0f\u6d1e\u70b9\u7684\u8def\u5f84\uff1b2) \u5206\u6790\u8def\u5f84\u4e2d\u7684\u5206\u652f\u6761\u4ef6\uff1b3) \u5728\u53cd\u9988\u5faa\u73af\u4e2d\u751f\u6210\u6d4b\u8bd5\u7528\u4f8b\u3002\u8be5\u65b9\u6cd5\u4e0d\u4f9d\u8d56\u8bed\u8a00\u7279\u5b9a\u7684\u5206\u6790\u5de5\u5177\u3002", "result": "\u5728\u591a\u8bed\u8a00\u7684100\u4e2a\u6f0f\u6d1e\u6570\u636e\u96c6\u4e2d\uff0cFaultLine\u6210\u529f\u4e3a16\u4e2a\u9879\u76ee\u751f\u6210PoV\u6d4b\u8bd5\uff0c\u4f18\u4e8eCodeAct 2.1\u76849\u4e2a\uff0c\u76f8\u5bf9\u6027\u80fd\u63d0\u534777%\u3002", "conclusion": "\u5206\u5c42\u63a8\u7406\u53ef\u63d0\u5347LLM\u4ee3\u7406\u5728PoV\u6d4b\u8bd5\u751f\u6210\u4e2d\u7684\u8868\u73b0\uff0c\u4f46\u95ee\u9898\u4ecd\u5177\u6311\u6218\u6027\u3002\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5df2\u516c\u5f00\u4ee5\u4fc3\u8fdb\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2507.14893", "categories": ["cs.CR", "math.NT", "11T71, 94A60, 68P25, 14G50, 81P94"], "pdf": "https://arxiv.org/pdf/2507.14893", "abs": "https://arxiv.org/abs/2507.14893", "authors": ["Farzin Renan"], "title": "A Compact Post-quantum Strong Designated Verifier Signature Scheme from Isogenies", "comment": null, "summary": "Digital signatures are essential cryptographic tools that provide\nauthentication and integrity in digital communications. However,\nprivacy-sensitive applications, such as e-voting and digital cash, require more\nrestrictive verification models to ensure confidentiality and control. Strong\nDesignated Verifier Signature (SDVS) schemes address this need by enabling the\nsigner to designate a specific verifier, ensuring that only this party can\nvalidate the signature. Existing SDVS constructions are primarily based on\nnumber-theoretic assumptions and are therefore vulnerable to quantum attacks.\nAlthough post-quantum alternatives, particularly those based on lattices, have\nbeen proposed, they often entail large key and signature sizes. In this work,\nwe introduce $\\mathsf{CSI\\text{-}SDVS}$, a novel isogeny-based SDVS scheme that\noffers a compact, quantum-resistant alternative. Our construction builds on the\nideal class group action framework of CSIDH and the signature techniques of\nCSI-FiSh, and relies on the hardness of the Multi-Target Group Action Inverse\nProblem (MT-GAIP). $\\mathsf{CSI\\text{-}SDVS}$ achieves strong security\nguarantees; namely, Strong Unforgeability under Chosen-Message Attacks\n(SUF-CMA), Non-Transferability (NT), and Privacy of Signer's Identity (PSI), in\nthe random oracle model. Remarkably, both the keys and signatures in\n$\\mathsf{CSI\\text{-}SDVS}$ are of size $\\mathcal{O}(\\lambda)$, representing a\nsignificant improvement over the typical $\\mathcal{O}(\\lambda^2)$ bounds in\nexisting post-quantum SDVS schemes, thereby making it among the most compact\nPQC-based SDVS schemes and the only post-quantum secure construction based on\nisogenies.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u540c\u6e90\u7684\u65b0\u578b\u5f3a\u6307\u5b9a\u9a8c\u8bc1\u8005\u7b7e\u540d\u65b9\u6848\uff08CSI-SDVS\uff09\uff0c\u5177\u6709\u7d27\u51d1\u7684\u5bc6\u94a5\u548c\u7b7e\u540d\u5927\u5c0f\uff0c\u4e14\u80fd\u62b5\u6297\u91cf\u5b50\u653b\u51fb\u3002", "motivation": "\u9690\u79c1\u654f\u611f\u5e94\u7528\uff08\u5982\u7535\u5b50\u6295\u7968\u548c\u6570\u5b57\u73b0\u91d1\uff09\u9700\u8981\u66f4\u4e25\u683c\u7684\u9a8c\u8bc1\u6a21\u578b\u4ee5\u786e\u4fdd\u4fdd\u5bc6\u6027\u548c\u63a7\u5236\uff0c\u73b0\u6709\u65b9\u6848\u6613\u53d7\u91cf\u5b50\u653b\u51fb\u6216\u4f53\u79ef\u8fc7\u5927\u3002", "method": "\u57fa\u4e8eCSIDH\u7684\u7406\u60f3\u7c7b\u7fa4\u52a8\u4f5c\u6846\u67b6\u548cCSI-FiSh\u7684\u7b7e\u540d\u6280\u672f\uff0c\u4f9d\u8d56\u4e8e\u591a\u76ee\u6807\u7fa4\u52a8\u4f5c\u9006\u95ee\u9898\u7684\u56f0\u96be\u6027\u3002", "result": "CSI-SDVS\u5728\u968f\u673a\u9884\u8a00\u6a21\u578b\u4e2d\u5b9e\u73b0\u4e86\u5f3a\u5b89\u5168\u6027\uff08SUF-CMA\u3001NT\u3001PSI\uff09\uff0c\u5bc6\u94a5\u548c\u7b7e\u540d\u5927\u5c0f\u4ec5\u4e3aO(\u03bb)\u3002", "conclusion": "CSI-SDVS\u662f\u76ee\u524d\u6700\u7d27\u51d1\u7684\u540e\u91cf\u5b50SDVS\u65b9\u6848\uff0c\u4e5f\u662f\u552f\u4e00\u57fa\u4e8e\u540c\u6e90\u7684\u5b89\u5168\u6784\u9020\u3002"}}
{"id": "2507.14897", "categories": ["cs.AI", "I.2.5"], "pdf": "https://arxiv.org/pdf/2507.14897", "abs": "https://arxiv.org/abs/2507.14897", "authors": ["Renxi Wang", "Rifo Ahmad Genadi", "Bilal El Bouardi", "Yongxin Wang", "Fajri Koto", "Zhengzhong Liu", "Timothy Baldwin", "Haonan Li"], "title": "AgentFly: Extensible and Scalable Reinforcement Learning for LM Agents", "comment": null, "summary": "Language model (LM) agents have gained significant attention for their\nability to autonomously complete tasks through interactions with environments,\ntools, and APIs. LM agents are primarily built with prompt engineering or\nsupervised finetuning. At the same time, reinforcement learning (RL) has been\nexplored to enhance LM's capabilities, such as reasoning and factuality.\nHowever, the combination of the LM agents and reinforcement learning (Agent-RL)\nremains underexplored and lacks systematic study. To this end, we built\nAgentFly, a scalable and extensible Agent-RL framework designed to empower LM\nagents with a variety of RL algorithms. Our framework supports multi-turn\ninteractions by adapting traditional RL methods with token-level masking. It\nfeatures a decorator-based interface for defining tools and reward functions,\nenabling seamless extension and ease of use. To support high-throughput\ntraining, we implement asynchronous execution of tool calls and reward\ncomputations, and design a centralized resource management system for scalable\nenvironment coordination. We also provide a suite of prebuilt tools and\nenvironments, demonstrating the framework's effectiveness through successful\nagent training across multiple tasks.", "AI": {"tldr": "AgentFly\u662f\u4e00\u4e2a\u7ed3\u5408\u8bed\u8a00\u6a21\u578b\uff08LM\uff09\u548c\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7684\u53ef\u6269\u5c55\u6846\u67b6\uff0c\u65e8\u5728\u901a\u8fc7RL\u7b97\u6cd5\u589e\u5f3aLM\u4ee3\u7406\u7684\u80fd\u529b\u3002", "motivation": "\u5f53\u524dLM\u4ee3\u7406\u4e3b\u8981\u901a\u8fc7\u63d0\u793a\u5de5\u7a0b\u6216\u76d1\u7763\u5fae\u8c03\u6784\u5efa\uff0c\u800cRL\u4e0eLM\u4ee3\u7406\u7684\u7ed3\u5408\uff08Agent-RL\uff09\u7f3a\u4e4f\u7cfb\u7edf\u6027\u7814\u7a76\u3002", "method": "\u5f00\u53d1\u4e86AgentFly\u6846\u67b6\uff0c\u652f\u6301\u591a\u8f6e\u4ea4\u4e92\u3001\u5de5\u5177\u5b9a\u4e49\u548c\u5956\u52b1\u51fd\u6570\uff0c\u5e76\u5b9e\u73b0\u5f02\u6b65\u6267\u884c\u548c\u8d44\u6e90\u7ba1\u7406\u3002", "result": "\u6846\u67b6\u5728\u591a\u4efb\u52a1\u4e2d\u6210\u529f\u8bad\u7ec3\u4ee3\u7406\uff0c\u5c55\u793a\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "AgentFly\u4e3aAgent-RL\u7814\u7a76\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u5de5\u5177\u548c\u57fa\u7840\u3002"}}
{"id": "2507.15251", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.15251", "abs": "https://arxiv.org/abs/2507.15251", "authors": ["Boyang Yang", "Luyao Ren", "Xin Yin", "Jiadong Ren", "Haoye Tian", "Shunfu Jin"], "title": "Input Reduction Enhanced LLM-based Program Repair", "comment": null, "summary": "Large Language Models (LLMs) have shown great potential in Automated Program\nRepair (APR). Test inputs, being crucial for reasoning the root cause of\nfailures, are always included in the prompt for LLM-based APR. Unfortunately,\nLLMs struggle to retain key information in long prompts. When the test inputs\nare extensive in the prompt, this may trigger the \"lost-in-the-middle\" issue,\ncompromising repair performance. To address this, we propose ReduceFix, an\nLLM-based APR approach with a built-in component that automatically reduces\ntest inputs while retaining their failure-inducing behavior. ReduceFix prompts\nan LLM to generate a reducer that minimizes failure-inducing test inputs\nwithout human effort, and then feeds the reduced failure-inducing inputs to\nguide patch generation.\n  For targeted evaluation, we constructed LFTBench, the first long-input APR\nbenchmark with 200 real bugs from 20 programming tasks, each paired with a\nfailure-inducing input whose median size is 1 MB. On this benchmark, ReduceFix\nshrinks inputs by 89.1% on average and improves overall pass@10 by up to 53.8%\nrelative to a prompt that includes the original test, and by 17.6% compared\nwith omitting the test entirely. Adding the same reduction step to ChatRepair\nincreases its fix rate by 21.3% without other changes. Ablation studies further\nhighlight the impact of input length and compressed failure information on\nrepair success. These results underscore that automatically reducing failing\ninputs is a practical and powerful complement to LLM-based APR, significantly\nimproving its scalability and effectiveness.", "AI": {"tldr": "ReduceFix \u662f\u4e00\u79cd\u57fa\u4e8e LLM \u7684\u81ea\u52a8\u7a0b\u5e8f\u4fee\u590d\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u52a8\u7f29\u51cf\u6d4b\u8bd5\u8f93\u5165\u6765\u89e3\u51b3\u957f\u63d0\u793a\u4e2d\u7684\u4fe1\u606f\u4e22\u5931\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4fee\u590d\u6027\u80fd\u3002", "motivation": "LLM \u5728\u957f\u63d0\u793a\u4e2d\u5bb9\u6613\u4e22\u5931\u5173\u952e\u4fe1\u606f\uff0c\u5bfc\u81f4\u4fee\u590d\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u63d0\u51fa ReduceFix\uff0c\u81ea\u52a8\u7f29\u51cf\u6d4b\u8bd5\u8f93\u5165\u5e76\u4fdd\u7559\u5176\u5931\u8d25\u8bf1\u5bfc\u884c\u4e3a\uff0c\u7528\u4e8e\u6307\u5bfc\u8865\u4e01\u751f\u6210\u3002", "result": "\u5728 LFTBench \u4e0a\uff0cReduceFix \u5e73\u5747\u7f29\u51cf\u8f93\u5165 89.1%\uff0c\u4fee\u590d\u6210\u529f\u7387\u63d0\u5347\u6700\u9ad8\u8fbe 53.8%\u3002", "conclusion": "\u81ea\u52a8\u7f29\u51cf\u5931\u8d25\u8f93\u5165\u662f LLM \u7a0b\u5e8f\u4fee\u590d\u7684\u6709\u6548\u8865\u5145\uff0c\u663e\u8457\u63d0\u5347\u5176\u53ef\u6269\u5c55\u6027\u548c\u6548\u679c\u3002"}}
{"id": "2507.14985", "categories": ["cs.CR", "cs.ET", "cs.HC", "cs.SE"], "pdf": "https://arxiv.org/pdf/2507.14985", "abs": "https://arxiv.org/abs/2507.14985", "authors": ["Argianto Rahartomo", "Leonel Merino", "Mohammad Ghafari"], "title": "Metaverse Security and Privacy Research: A Systematic Review", "comment": "The paper is accepted for publication at Computers & Security Journal", "summary": "The rapid growth of metaverse technologies, including virtual worlds,\naugmented reality, and lifelogging, has accelerated their adoption across\ndiverse domains. This rise exposes users to significant new security and\nprivacy challenges due to sociotechnical complexity, pervasive connectivity,\nand extensive user data collection in immersive environments. We present a\nsystematic review of the literature published between 2013 and 2024, offering a\ncomprehensive analysis of how the research community has addressed\nmetaverse-related security and privacy issues over the past decade. We organize\nthe studies by method, examined the security and privacy properties, immersive\ncomponents, and evaluation strategies. Our investigation reveals a sharp\nincrease in research activity in the last five years, a strong focus on\npractical and user-centered approaches, and a predominant use of benchmarking,\nhuman experimentation, and qualitative methods. Authentication and\nunobservability are the most frequently studied properties. However, critical\ngaps remain in areas such as policy compliance, accessibility,\ninteroperability, and back-end infrastructure security. We emphasize the\nintertwined technical complexity and human factors of the metaverse and call\nfor integrated, interdisciplinary approaches to securing inclusive and\ntrustworthy immersive environments.", "AI": {"tldr": "\u5bf92013-2024\u5e74\u95f4\u5173\u4e8e\u5143\u5b87\u5b99\u5b89\u5168\u4e0e\u9690\u79c1\u95ee\u9898\u7684\u6587\u732e\u8fdb\u884c\u4e86\u7cfb\u7edf\u7efc\u8ff0\uff0c\u63ed\u793a\u4e86\u7814\u7a76\u8d8b\u52bf\u3001\u91cd\u70b9\u9886\u57df\u53ca\u73b0\u5b58\u6311\u6218\u3002", "motivation": "\u5143\u5b87\u5b99\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\u5e26\u6765\u4e86\u65b0\u7684\u5b89\u5168\u4e0e\u9690\u79c1\u6311\u6218\uff0c\u9700\u7cfb\u7edf\u68b3\u7406\u7814\u7a76\u8fdb\u5c55\u4ee5\u6307\u5bfc\u672a\u6765\u65b9\u5411\u3002", "method": "\u901a\u8fc7\u6587\u732e\u7efc\u8ff0\uff0c\u6309\u65b9\u6cd5\u3001\u5b89\u5168\u9690\u79c1\u5c5e\u6027\u3001\u6c89\u6d78\u5f0f\u7ec4\u4ef6\u548c\u8bc4\u4f30\u7b56\u7565\u5206\u7c7b\u5206\u6790\u3002", "result": "\u8fc7\u53bb\u4e94\u5e74\u7814\u7a76\u6fc0\u589e\uff0c\u91cd\u70b9\u5173\u6ce8\u5b9e\u7528\u548c\u7528\u6237\u4e2d\u5fc3\u65b9\u6cd5\uff0c\u8ba4\u8bc1\u548c\u4e0d\u53ef\u89c2\u5bdf\u6027\u4e3a\u70ed\u95e8\u9886\u57df\uff0c\u4f46\u653f\u7b56\u5408\u89c4\u7b49\u4ecd\u5b58\u7a7a\u767d\u3002", "conclusion": "\u9700\u8de8\u5b66\u79d1\u6574\u5408\u65b9\u6cd5\u4ee5\u5e94\u5bf9\u5143\u5b87\u5b99\u7684\u6280\u672f\u590d\u6742\u6027\u548c\u4eba\u4e3a\u56e0\u7d20\uff0c\u6784\u5efa\u53ef\u4fe1\u8d56\u7684\u6c89\u6d78\u5f0f\u73af\u5883\u3002"}}
{"id": "2507.14899", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14899", "abs": "https://arxiv.org/abs/2507.14899", "authors": ["Jiale Liu", "Huan Wang", "Yue Zhang", "Xiaoyu Luo", "Jiaxiang Hu", "Zhiliang Liu", "Min Xie"], "title": "InsightX Agent: An LMM-based Agentic Framework with Integrated Tools for Reliable X-ray NDT Analysis", "comment": null, "summary": "Non-destructive testing (NDT), particularly X-ray inspection, is vital for\nindustrial quality assurance, yet existing deep-learning-based approaches often\nlack interactivity, interpretability, and the capacity for critical\nself-assessment, limiting their reliability and operator trust. To address\nthese shortcomings, this paper proposes InsightX Agent, a novel LMM-based\nagentic framework designed to deliver reliable, interpretable, and interactive\nX-ray NDT analysis. Unlike typical sequential pipelines, InsightX Agent\npositions a Large Multimodal Model (LMM) as a central orchestrator,\ncoordinating between the Sparse Deformable Multi-Scale Detector (SDMSD) and the\nEvidence-Grounded Reflection (EGR) tool. The SDMSD generates dense defect\nregion proposals for multi-scale feature maps and sparsifies them through\nNon-Maximum Suppression (NMS), optimizing detection of small, dense targets in\nX-ray images while maintaining computational efficiency. The EGR tool guides\nthe LMM agent through a chain-of-thought-inspired review process, incorporating\ncontext assessment, individual defect analysis, false positive elimination,\nconfidence recalibration and quality assurance to validate and refine the\nSDMSD's initial proposals. By strategically employing and intelligently using\ntools, InsightX Agent moves beyond passive data processing to active reasoning,\nenhancing diagnostic reliability and providing interpretations that integrate\ndiverse information sources. Experimental evaluations on the GDXray+ dataset\ndemonstrate that InsightX Agent not only achieves a high object detection\nF1-score of 96.35% but also offers significantly improved interpretability and\ntrustworthiness in its analyses, highlighting the transformative potential of\nagentic LLM frameworks for industrial inspection tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLMM\u7684\u4ea4\u4e92\u5f0f\u3001\u53ef\u89e3\u91ca\u7684X\u5c04\u7ebf\u65e0\u635f\u68c0\u6d4b\u6846\u67b6InsightX Agent\uff0c\u7ed3\u5408SDMSD\u548cEGR\u5de5\u5177\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u53ef\u9760\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728X\u5c04\u7ebf\u65e0\u635f\u68c0\u6d4b\u4e2d\u7f3a\u4e4f\u4ea4\u4e92\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u81ea\u6211\u8bc4\u4f30\u80fd\u529b\uff0c\u9650\u5236\u4e86\u5176\u53ef\u9760\u6027\u548c\u64cd\u4f5c\u5458\u4fe1\u4efb\u3002", "method": "InsightX Agent\u4ee5LMM\u4e3a\u6838\u5fc3\u534f\u8c03SDMSD\u548cEGR\u5de5\u5177\uff0cSDMSD\u7528\u4e8e\u591a\u5c3a\u5ea6\u7f3a\u9677\u68c0\u6d4b\uff0cEGR\u901a\u8fc7\u94fe\u5f0f\u601d\u7ef4\u9a8c\u8bc1\u548c\u4f18\u5316\u7ed3\u679c\u3002", "result": "\u5728GDXray+\u6570\u636e\u96c6\u4e0a\uff0cInsightX Agent\u5b9e\u73b0\u4e8696.35%\u7684F1\u5206\u6570\uff0c\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u5206\u6790\u7684\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u4fe1\u5ea6\u3002", "conclusion": "InsightX Agent\u5c55\u793a\u4e86\u57fa\u4e8eLMM\u7684\u4ee3\u7406\u6846\u67b6\u5728\u5de5\u4e1a\u68c0\u6d4b\u4efb\u52a1\u4e2d\u7684\u53d8\u9769\u6f5c\u529b\u3002"}}
{"id": "2507.15296", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15296", "abs": "https://arxiv.org/abs/2507.15296", "authors": ["Qian Xiong", "Yuekai Huang", "Ziyou Jiang", "Zhiyuan Chang", "Yujia Zheng", "Tianhao Li", "Mingyang Li"], "title": "Butterfly Effects in Toolchains: A Comprehensive Analysis of Failed Parameter Filling in LLM Tool-Agent Systems", "comment": null, "summary": "The emergence of the tool agent paradigm has broadened the capability\nboundaries of the Large Language Model (LLM), enabling it to complete more\ncomplex tasks. However, the effectiveness of this paradigm is limited due to\nthe issue of parameter failure during its execution. To explore this phenomenon\nand propose corresponding suggestions, we first construct a parameter failure\ntaxonomy in this paper. We derive five failure categories from the invocation\nchain of a mainstream tool agent. Then, we explore the correlation between\nthree different input sources and failure categories by applying 15 input\nperturbation methods to the input. Experimental results show that parameter\nname hallucination failure primarily stems from inherent LLM limitations, while\nissues with input sources mainly cause other failure patterns. To improve the\nreliability and effectiveness of tool-agent interactions, we propose\ncorresponding improvement suggestions, including standardizing tool return\nformats, improving error feedback mechanisms, and ensuring parameter\nconsistency.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5de5\u5177\u4ee3\u7406\u8303\u5f0f\u4e2d\u53c2\u6570\u5931\u8d25\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u5206\u7c7b\u548c\u6539\u8fdb\u5efa\u8bae\u3002", "motivation": "\u63a2\u7d22\u5de5\u5177\u4ee3\u7406\u8303\u5f0f\u4e2d\u53c2\u6570\u5931\u8d25\u7684\u73b0\u8c61\uff0c\u5e76\u63d0\u51fa\u6539\u8fdb\u5efa\u8bae\u4ee5\u63d0\u9ad8\u5176\u53ef\u9760\u6027\u548c\u6709\u6548\u6027\u3002", "method": "\u6784\u5efa\u53c2\u6570\u5931\u8d25\u5206\u7c7b\u6cd5\uff0c\u901a\u8fc7\u8f93\u5165\u6270\u52a8\u65b9\u6cd5\u5206\u6790\u8f93\u5165\u6e90\u4e0e\u5931\u8d25\u7c7b\u522b\u7684\u76f8\u5173\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u53c2\u6570\u540d\u79f0\u5e7b\u89c9\u5931\u8d25\u4e3b\u8981\u6e90\u4e8eLLM\u56fa\u6709\u5c40\u9650\uff0c\u5176\u4ed6\u5931\u8d25\u6a21\u5f0f\u4e0e\u8f93\u5165\u6e90\u95ee\u9898\u76f8\u5173\u3002", "conclusion": "\u5efa\u8bae\u6807\u51c6\u5316\u5de5\u5177\u8fd4\u56de\u683c\u5f0f\u3001\u6539\u8fdb\u9519\u8bef\u53cd\u9988\u673a\u5236\u548c\u786e\u4fdd\u53c2\u6570\u4e00\u81f4\u6027\u4ee5\u4f18\u5316\u5de5\u5177\u4ee3\u7406\u4ea4\u4e92\u3002"}}
{"id": "2507.15058", "categories": ["cs.CR", "cs.LG", "cs.SE", "D.2.5; D.4.6"], "pdf": "https://arxiv.org/pdf/2507.15058", "abs": "https://arxiv.org/abs/2507.15058", "authors": ["Ian Hardgrove", "John D. Hastings"], "title": "LibLMFuzz: LLM-Augmented Fuzz Target Generation for Black-box Libraries", "comment": "6 pages, 2 figures, 1 table, 2 listings", "summary": "A fundamental problem in cybersecurity and computer science is determining\nwhether a program is free of bugs and vulnerabilities. Fuzzing, a popular\napproach to discovering vulnerabilities in programs, has several advantages\nover alternative strategies, although it has investment costs in the form of\ninitial setup and continuous maintenance. The choice of fuzzing is further\ncomplicated when only a binary library is available, such as the case of\nclosed-source and proprietary software. In response, we introduce LibLMFuzz, a\nframework that reduces costs associated with fuzzing closed-source libraries by\npairing an agentic Large Language Model (LLM) with a lightweight tool-chain\n(disassembler/compiler/fuzzer) to autonomously analyze stripped binaries, plan\nfuzz strategies, generate drivers, and iteratively self-repair build or runtime\nerrors. Tested on four widely-used Linux libraries, LibLMFuzz produced\nsyntactically correct drivers for all 558 fuzz-able API functions, achieving\n100% API coverage with no human intervention. Across the 1601 synthesized\ndrivers, 75.52% were nominally correct on first execution. The results show\nthat LLM-augmented middleware holds promise in reducing the costs of fuzzing\nblack box components and provides a foundation for future research efforts.\nFuture opportunities exist for research in branch coverage.", "AI": {"tldr": "LibLMFuzz\u6846\u67b6\u5229\u7528LLM\u548c\u8f7b\u91cf\u5de5\u5177\u94fe\u81ea\u52a8\u5206\u6790\u95ed\u6e90\u5e93\uff0c\u964d\u4f4e\u6a21\u7cca\u6d4b\u8bd5\u6210\u672c\uff0c\u5b9e\u73b0100% API\u8986\u76d6\u3002", "motivation": "\u89e3\u51b3\u95ed\u6e90\u5e93\u6a21\u7cca\u6d4b\u8bd5\u7684\u9ad8\u6210\u672c\u548c\u590d\u6742\u6027\u3002", "method": "\u7ed3\u5408LLM\u548c\u5de5\u5177\u94fe\u81ea\u52a8\u5206\u6790\u4e8c\u8fdb\u5236\u6587\u4ef6\u3001\u751f\u6210\u9a71\u52a8\u5e76\u81ea\u6211\u4fee\u590d\u9519\u8bef\u3002", "result": "\u6210\u529f\u4e3a558\u4e2aAPI\u751f\u6210\u9a71\u52a8\uff0c75.52%\u9996\u6b21\u6267\u884c\u6b63\u786e\u3002", "conclusion": "LLM\u589e\u5f3a\u7684\u4e2d\u95f4\u4ef6\u6709\u671b\u964d\u4f4e\u9ed1\u76d2\u7ec4\u4ef6\u6d4b\u8bd5\u6210\u672c\uff0c\u672a\u6765\u53ef\u7814\u7a76\u5206\u652f\u8986\u76d6\u3002"}}
{"id": "2507.14906", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14906", "abs": "https://arxiv.org/abs/2507.14906", "authors": ["Xiao Yang", "Juxi Leitner", "Michael Burke"], "title": "Feedback-Induced Performance Decline in LLM-Based Decision-Making", "comment": null, "summary": "The ability of Large Language Models (LLMs) to extract context from natural\nlanguage problem descriptions naturally raises questions about their\nsuitability in autonomous decision-making settings. This paper studies the\nbehaviour of these models within a Markov Decision Process (MDPs). While\ntraditional reinforcement learning (RL) strategies commonly employed in this\nsetting rely on iterative exploration, LLMs, pre-trained on diverse datasets,\noffer the capability to leverage prior knowledge for faster adaptation. We\ninvestigate online structured prompting strategies in sequential decision\nmaking tasks, comparing the zero-shot performance of LLM-based approaches to\nthat of classical RL methods. Our findings reveal that although LLMs\ndemonstrate improved initial performance in simpler environments, they struggle\nwith planning and reasoning in complex scenarios without fine-tuning or\nadditional guidance. Our results show that feedback mechanisms, intended to\nimprove decision-making, often introduce confusion, leading to diminished\nperformance in intricate environments. These insights underscore the need for\nfurther exploration into hybrid strategies, fine-tuning, and advanced memory\nintegration to enhance LLM-based decision-making capabilities.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5176\u5728\u7b80\u5355\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u590d\u6742\u573a\u666f\u4e2d\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u3002", "motivation": "\u63a2\u7d22LLMs\u5728\u81ea\u4e3b\u51b3\u7b56\u4e2d\u7684\u9002\u7528\u6027\uff0c\u5c24\u5176\u662f\u5176\u57fa\u4e8e\u9884\u8bad\u7ec3\u77e5\u8bc6\u7684\u5feb\u901f\u9002\u5e94\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u5728\u7ebf\u7ed3\u6784\u5316\u63d0\u793a\u7b56\u7565\uff0c\u6bd4\u8f83LLM\u4e0e\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u65b9\u6cd5\u5728\u5e8f\u5217\u51b3\u7b56\u4efb\u52a1\u4e2d\u7684\u96f6\u6837\u672c\u6027\u80fd\u3002", "result": "LLMs\u5728\u7b80\u5355\u73af\u5883\u4e2d\u521d\u59cb\u8868\u73b0\u66f4\u597d\uff0c\u4f46\u5728\u590d\u6742\u573a\u666f\u4e2d\u89c4\u5212\u548c\u63a8\u7406\u80fd\u529b\u4e0d\u8db3\uff1b\u53cd\u9988\u673a\u5236\u53ef\u80fd\u964d\u4f4e\u6027\u80fd\u3002", "conclusion": "\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u6df7\u5408\u7b56\u7565\u3001\u5fae\u8c03\u548c\u9ad8\u7ea7\u8bb0\u5fc6\u6574\u5408\u4ee5\u63d0\u5347LLM\u7684\u51b3\u7b56\u80fd\u529b\u3002"}}
{"id": "2507.15343", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15343", "abs": "https://arxiv.org/abs/2507.15343", "authors": ["Kechi Zhang", "Ge Li", "Jia Li", "Huangzhao Zhang", "Yihong Dong", "Jia Li", "Jingjing Xu", "Zhi Jin"], "title": "StackTrans: From Large Language Model to Large Pushdown Automata Model", "comment": "currently under development", "summary": "The Transformer architecture has emerged as a landmark advancement within the\nbroad field of artificial intelligence, effectively catalyzing the advent of\nlarge language models (LLMs). However, despite its remarkable capabilities and\nthe substantial progress it has facilitated, the Transformer architecture still\nhas some limitations. One such intrinsic limitation is its inability to\neffectively capture the Chomsky hierarchy, such as regular expressions or\ndeterministic context-free grammars. Drawing inspiration from pushdown\nautomata, which efficiently resolve deterministic context-free grammars using\nstacks, we propose StackTrans to address the aforementioned issue within LLMs.\nUnlike previous approaches that modify the attention computation, StackTrans\nexplicitly incorporates hidden state stacks between Transformer layers. This\ndesign maintains compatibility with existing frameworks like flash-attention.\nSpecifically, our design features stack operations -- such as pushing and\npopping hidden states -- that are differentiable and can be learned in an\nend-to-end manner. Our comprehensive evaluation spans benchmarks for both\nChomsky hierarchies and large-scale natural languages. Across these diverse\ntasks, StackTrans consistently outperforms standard Transformer models and\nother baselines. We have successfully scaled StackTrans up from 360M to 7B\nparameters. In particular, our from-scratch pretrained model StackTrans-360M\noutperforms several larger open-source LLMs with 2-3x more parameters,\nshowcasing its superior efficiency and reasoning capability.", "AI": {"tldr": "StackTrans\u901a\u8fc7\u5f15\u5165\u9690\u85cf\u72b6\u6001\u5806\u6808\u6539\u8fdb\u4e86Transformer\u67b6\u6784\uff0c\u89e3\u51b3\u4e86\u5176\u65e0\u6cd5\u6709\u6548\u6355\u6349Chomsky\u5c42\u7ea7\u7684\u95ee\u9898\uff0c\u5e76\u5728\u591a\u9879\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u6807\u51c6Transformer\u548c\u5176\u4ed6\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "Transformer\u67b6\u6784\u867d\u7136\u5f3a\u5927\uff0c\u4f46\u65e0\u6cd5\u6709\u6548\u5904\u7406Chomsky\u5c42\u7ea7\uff08\u5982\u6b63\u5219\u8868\u8fbe\u5f0f\u6216\u786e\u5b9a\u6027\u4e0a\u4e0b\u6587\u65e0\u5173\u6587\u6cd5\uff09\uff0c\u56e0\u6b64\u9700\u8981\u6539\u8fdb\u3002", "method": "\u63d0\u51faStackTrans\uff0c\u5728Transformer\u5c42\u95f4\u5f15\u5165\u53ef\u5fae\u5206\u7684\u9690\u85cf\u72b6\u6001\u5806\u6808\u64cd\u4f5c\uff08\u5982\u538b\u5165\u548c\u5f39\u51fa\uff09\uff0c\u4fdd\u6301\u4e0e\u73b0\u6709\u6846\u67b6\u7684\u517c\u5bb9\u6027\u3002", "result": "StackTrans\u5728Chomsky\u5c42\u7ea7\u548c\u5927\u89c4\u6a21\u81ea\u7136\u8bed\u8a00\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u751a\u81f3\u5c0f\u89c4\u6a21\u6a21\u578b\uff08360M\u53c2\u6570\uff09\u4e5f\u80fd\u8d85\u8d8a\u66f4\u5927\u89c4\u6a21\u7684\u5f00\u653eLLM\u3002", "conclusion": "StackTrans\u901a\u8fc7\u5806\u6808\u673a\u5236\u63d0\u5347\u4e86Transformer\u7684\u6548\u7387\u548c\u63a8\u7406\u80fd\u529b\uff0c\u4e3a\u5904\u7406\u590d\u6742\u8bed\u8a00\u7ed3\u6784\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.15219", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15219", "abs": "https://arxiv.org/abs/2507.15219", "authors": ["Tianneng Shi", "Kaijie Zhu", "Zhun Wang", "Yuqi Jia", "Will Cai", "Weida Liang", "Haonan Wang", "Hend Alzahrani", "Joshua Lu", "Kenji Kawaguchi", "Basel Alomair", "Xuandong Zhao", "William Yang Wang", "Neil Gong", "Wenbo Guo", "Dawn Song"], "title": "PromptArmor: Simple yet Effective Prompt Injection Defenses", "comment": null, "summary": "Despite their potential, recent research has demonstrated that LLM agents are\nvulnerable to prompt injection attacks, where malicious prompts are injected\ninto the agent's input, causing it to perform an attacker-specified task rather\nthan the intended task provided by the user. In this paper, we present\nPromptArmor, a simple yet effective defense against prompt injection attacks.\nSpecifically, PromptArmor prompts an off-the-shelf LLM to detect and remove\npotential injected prompts from the input before the agent processes it. Our\nresults show that PromptArmor can accurately identify and remove injected\nprompts. For example, using GPT-4o, GPT-4.1, or o4-mini, PromptArmor achieves\nboth a false positive rate and a false negative rate below 1% on the AgentDojo\nbenchmark. Moreover, after removing injected prompts with PromptArmor, the\nattack success rate drops to below 1%. We also demonstrate PromptArmor's\neffectiveness against adaptive attacks and explore different strategies for\nprompting an LLM. We recommend that PromptArmor be adopted as a standard\nbaseline for evaluating new defenses against prompt injection attacks.", "AI": {"tldr": "PromptArmor\u662f\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u9632\u5fa1\u65b9\u6cd5\uff0c\u901a\u8fc7\u68c0\u6d4b\u548c\u79fb\u9664\u8f93\u5165\u4e2d\u7684\u6076\u610f\u63d0\u793a\u6765\u4fdd\u62a4LLM\u4ee3\u7406\u514d\u53d7\u63d0\u793a\u6ce8\u5165\u653b\u51fb\u3002", "motivation": "LLM\u4ee3\u7406\u6613\u53d7\u63d0\u793a\u6ce8\u5165\u653b\u51fb\uff0c\u5bfc\u81f4\u6267\u884c\u653b\u51fb\u8005\u6307\u5b9a\u7684\u4efb\u52a1\u800c\u975e\u7528\u6237\u610f\u56fe\u3002", "method": "PromptArmor\u5229\u7528\u73b0\u6210\u7684LLM\u68c0\u6d4b\u5e76\u79fb\u9664\u8f93\u5165\u4e2d\u7684\u6f5c\u5728\u6076\u610f\u63d0\u793a\u3002", "result": "\u5728AgentDojo\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPromptArmor\u7684\u8bef\u62a5\u7387\u548c\u6f0f\u62a5\u7387\u5747\u4f4e\u4e8e1%\uff0c\u653b\u51fb\u6210\u529f\u7387\u964d\u81f31%\u4ee5\u4e0b\u3002", "conclusion": "PromptArmor\u5e94\u4f5c\u4e3a\u8bc4\u4f30\u65b0\u9632\u5fa1\u65b9\u6cd5\u7684\u6807\u51c6\u57fa\u7ebf\u3002"}}
{"id": "2507.14909", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.14909", "abs": "https://arxiv.org/abs/2507.14909", "authors": ["Elio Grande"], "title": "The Endless Tuning. An Artificial Intelligence Design To Avoid Human Replacement and Trace Back Responsibilities", "comment": null, "summary": "The Endless Tuning is a design method for a reliable deployment of artificial\nintelligence based on a double mirroring process, which pursues both the goals\nof avoiding human replacement and filling the so-called responsibility gap\n(Matthias 2004). Originally depicted in (Fabris et al. 2024) and ensuing the\nrelational approach urged therein, it was then actualized in a protocol,\nimplemented in three prototypical applications regarding decision-making\nprocesses (respectively: loan granting, pneumonia diagnosis, and art style\nrecognition) and tested with such as many domain experts. Step by step\nillustrating the protocol, giving insights concretely showing a different voice\n(Gilligan 1993) in the ethics of artificial intelligence, a philosophical\naccount of technical choices (e.g., a reversed and hermeneutic deployment of\nXAI algorithms) will be provided in the present study together with the results\nof the experiments, focusing on user experience rather than statistical\naccuracy. Even thoroughly employing deep learning models, full control was\nperceived by the interviewees in the decision-making setting, while it appeared\nthat a bridge can be built between accountability and liability in case of\ndamage.", "AI": {"tldr": "\u300aEndless Tuning\u300b\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53cc\u91cd\u955c\u50cf\u8fc7\u7a0b\u7684\u4eba\u5de5\u667a\u80fd\u53ef\u9760\u90e8\u7f72\u65b9\u6cd5\uff0c\u65e8\u5728\u907f\u514d\u4eba\u7c7b\u88ab\u66ff\u4ee3\u5e76\u586b\u8865\u8d23\u4efb\u7f3a\u53e3\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u4e09\u4e2a\u539f\u578b\u5e94\u7528\u6d4b\u8bd5\uff0c\u91cd\u70b9\u5173\u6ce8\u7528\u6237\u4f53\u9a8c\u800c\u975e\u7edf\u8ba1\u51c6\u786e\u6027\u3002", "motivation": "\u89e3\u51b3\u4eba\u5de5\u667a\u80fd\u90e8\u7f72\u4e2d\u7684\u4eba\u7c7b\u66ff\u4ee3\u95ee\u9898\u548c\u8d23\u4efb\u7f3a\u53e3\uff08Matthias 2004\uff09\uff0c\u5e76\u63a2\u7d22\u4f26\u7406\u4e0e\u6280\u672f\u9009\u62e9\u7684\u7ed3\u5408\u3002", "method": "\u91c7\u7528\u53cc\u91cd\u955c\u50cf\u8fc7\u7a0b\uff0c\u5f00\u53d1\u534f\u8bae\u5e76\u5728\u8d37\u6b3e\u5ba1\u6279\u3001\u80ba\u708e\u8bca\u65ad\u548c\u827a\u672f\u98ce\u683c\u8bc6\u522b\u4e09\u4e2a\u9886\u57df\u8fdb\u884c\u539f\u578b\u6d4b\u8bd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u7528\u6237\u5728\u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u65f6\u80fd\u611f\u77e5\u5230\u5b8c\u5168\u63a7\u5236\uff0c\u540c\u65f6\u80fd\u5728\u8d23\u4efb\u4e0e\u95ee\u8d23\u4e4b\u95f4\u5efa\u7acb\u6865\u6881\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u4eba\u5de5\u667a\u80fd\u4f26\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u89c6\u89d2\uff0c\u5f3a\u8c03\u7528\u6237\u4f53\u9a8c\u548c\u53ef\u63a7\u6027\uff0c\u4e3a\u8d23\u4efb\u95ee\u9898\u63d0\u4f9b\u4e86\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.15599", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.15599", "abs": "https://arxiv.org/abs/2507.15599", "authors": ["Manatsawin Hanmongkolchai"], "title": "Applying the Chinese Wall Reverse Engineering Technique to Large Language Model Code Editing", "comment": null, "summary": "Large language models for code (Code LLM) are increasingly utilized in\nprogramming environments. Despite their utility, the training datasets for top\nLLM remain undisclosed, raising concerns about potential copyright violations.\nSome models, such as Pleias and Comma put emphasis on data curation and\nlicenses, however, with limited training data these models are not competitive\nand only serve as proof of concepts. To improve the utility of these models, we\npropose an application of the \"Chinese Wall\" technique, inspired by the reverse\nengineering technique of the same name -- a high quality model is used to\ngenerate detailed instructions for a weaker model. By doing so, a weaker but\nethically aligned model may be used to perform complicated tasks that,\notherwise, can only be completed by more powerful models. In our evaluation,\nwe've found that this technique improves Comma v0.1 1T's performance in\nCanItEdit benchmark by over 66%, and Starcoder2 Instruct by roughly 20%\ncompared to when running the same model on the benchmark alone. The practical\napplication of this technique today, however, may be limited due to the lack of\nmodels trained on public domain content without copyright restrictions.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u4ee3\u7801\u5927\u8bed\u8a00\u6a21\u578b\uff08Code LLM\uff09\u7684\u7248\u6743\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u201c\u4e2d\u56fd\u5899\u201d\u6280\u672f\uff0c\u901a\u8fc7\u5f3a\u6a21\u578b\u6307\u5bfc\u5f31\u6a21\u578b\u63d0\u5347\u6027\u80fd\uff0c\u4f46\u5b9e\u9645\u5e94\u7528\u53d7\u9650\u3002", "motivation": "\u89e3\u51b3\u4ee3\u7801\u5927\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u6570\u636e\u7248\u6743\u95ee\u9898\uff0c\u540c\u65f6\u63d0\u5347\u5f31\u6a21\u578b\u7684\u5b9e\u7528\u6027\u3002", "method": "\u5e94\u7528\u201c\u4e2d\u56fd\u5899\u201d\u6280\u672f\uff0c\u5229\u7528\u9ad8\u8d28\u91cf\u6a21\u578b\u751f\u6210\u8be6\u7ec6\u6307\u4ee4\u6307\u5bfc\u5f31\u6a21\u578b\u5b8c\u6210\u4efb\u52a1\u3002", "result": "Comma v0.1 1T\u6027\u80fd\u63d0\u534766%\uff0cStarcoder2 Instruct\u63d0\u534720%\u3002", "conclusion": "\u8be5\u6280\u672f\u6709\u6548\u4f46\u53d7\u9650\u4e8e\u7f3a\u4e4f\u65e0\u7248\u6743\u95ee\u9898\u7684\u516c\u5f00\u8bad\u7ec3\u6570\u636e\u3002"}}
{"id": "2507.15377", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.15377", "abs": "https://arxiv.org/abs/2507.15377", "authors": ["Magali Bardet", "Charles Brion", "Philippe Gaborit", "Mercedes Haiech", "Romaric Neveu"], "title": "The Matrix Subcode Equivalence problem and its application to signature with MPC-in-the-Head", "comment": null, "summary": "Nowadays, equivalence problems are widely used in cryptography, most notably\nto establish cryptosystems such as digital signatures, with MEDS, LESS, PERK as\nthe most recent ones. However, in the context of matrix codes, only the code\nequivalence problem has been studied, while the subcode equivalence is\nwell-defined in the Hamming metric. In this work, we introduce two new\nproblems: the Matrix Subcode Equivalence Problem and the Matrix Code Permuted\nKernel Problem, to which we apply the MPCitH paradigm to build a signature\nscheme. These new problems, closely related to the Matrix Code Equivalence\nproblem, ask to find an isometry given a code $C$ and a subcode $D$.\nFurthermore, we prove that the Matrix Subcode Equivalence problem reduces to\nthe Hamming Subcode Equivalence problem, which is known to be NP-Complete, thus\nintroducing the matrix code version of the Permuted Kernel Problem. We also\nadapt the combinatorial and algebraic algorithms for the Matrix Code\nEquivalence problem to the subcode case, and we analyze their complexities. We\nfind with this analysis that the algorithms perform much worse than in the code\nequivalence case, which is the same as what happens in the Hamming metric.\nFinally, our analysis of the attacks allows us to take parameters much smaller\nthan in the Matrix Code Equivalence case. Coupled with the effectiveness of\n\\textit{Threshold-Computation-in-the-Head} or \\textit{VOLE-in-the-Head}, we\nobtain a signature size of $\\approx$ 4 800 Bytes, with a public key of\n$\\approx$ 275 Bytes. We thus obtain a reasonable signature size, which brings\ndiversity in the landscape of post-quantum signature schemes, by relying on a\nnew hard problem. In particular, this new signature scheme performs better than\nSPHINCS+, with a smaller size of public key + signature. Our signature compares\nalso well with other signature schemes: compared to MEDS, the signature is\nsmaller, and we reduced the size of the sum of signature and public key by a\nfactor close to 5. We also obtain a signature size that is almost half the size\nof the CROSS signature scheme.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e24\u4e2a\u65b0\u95ee\u9898\uff1a\u77e9\u9635\u5b50\u7801\u7b49\u4ef7\u95ee\u9898\u548c\u77e9\u9635\u7801\u7f6e\u6362\u6838\u95ee\u9898\uff0c\u5e76\u57fa\u4e8eMPCitH\u8303\u5f0f\u6784\u5efa\u7b7e\u540d\u65b9\u6848\u3002\u8fd9\u4e9b\u95ee\u9898\u4e0e\u77e9\u9635\u7801\u7b49\u4ef7\u95ee\u9898\u76f8\u5173\uff0c\u4e14\u8bc1\u660e\u77e9\u9635\u5b50\u7801\u7b49\u4ef7\u95ee\u9898\u53ef\u5f52\u7ea6\u4e3aNP\u5b8c\u5168\u7684Hamming\u5b50\u7801\u7b49\u4ef7\u95ee\u9898\u3002\u901a\u8fc7\u5206\u6790\u7b97\u6cd5\u590d\u6742\u6027\u548c\u653b\u51fb\uff0c\u53c2\u6570\u6bd4\u77e9\u9635\u7801\u7b49\u4ef7\u95ee\u9898\u66f4\u5c0f\uff0c\u7b7e\u540d\u5927\u5c0f\u7ea6\u4e3a4,800\u5b57\u8282\uff0c\u516c\u94a5\u7ea6\u4e3a275\u5b57\u8282\uff0c\u6027\u80fd\u4f18\u4e8eSPHINCS+\u548cMEDS\u3002", "motivation": "\u5f53\u524d\u5bc6\u7801\u5b66\u4e2d\uff0c\u77e9\u9635\u7801\u7684\u5b50\u7801\u7b49\u4ef7\u95ee\u9898\u5c1a\u672a\u88ab\u7814\u7a76\uff0c\u800cHamming\u5ea6\u91cf\u4e0b\u5df2\u6709\u5b9a\u4e49\u3002\u8bba\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63d0\u51fa\u65b0\u95ee\u9898\u5e76\u6784\u5efa\u9ad8\u6548\u7684\u7b7e\u540d\u65b9\u6848\u3002", "method": "\u5f15\u5165\u77e9\u9635\u5b50\u7801\u7b49\u4ef7\u95ee\u9898\u548c\u77e9\u9635\u7801\u7f6e\u6362\u6838\u95ee\u9898\uff0c\u5e94\u7528MPCitH\u8303\u5f0f\u6784\u5efa\u7b7e\u540d\u65b9\u6848\uff0c\u5e76\u5206\u6790\u7b97\u6cd5\u590d\u6742\u6027\u548c\u653b\u51fb\u3002", "result": "\u7b7e\u540d\u5927\u5c0f\u7ea6\u4e3a4,800\u5b57\u8282\uff0c\u516c\u94a5\u7ea6\u4e3a275\u5b57\u8282\uff0c\u6027\u80fd\u4f18\u4e8eSPHINCS+\u548cMEDS\uff0c\u7b7e\u540d\u548c\u516c\u94a5\u603b\u5927\u5c0f\u51cf\u5c11\u8fd15\u500d\u3002", "conclusion": "\u8bba\u6587\u901a\u8fc7\u65b0\u95ee\u9898\u6784\u5efa\u7684\u7b7e\u540d\u65b9\u6848\u5728\u6027\u80fd\u548c\u591a\u6837\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6848\uff0c\u4e3a\u540e\u91cf\u5b50\u7b7e\u540d\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.14912", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14912", "abs": "https://arxiv.org/abs/2507.14912", "authors": ["Ruhul Amin Khalil", "Kashif Ahmad", "Hazrat Ali"], "title": "Redefining Elderly Care with Agentic AI: Challenges and Opportunities", "comment": null, "summary": "The global ageing population necessitates new and emerging strategies for\ncaring for older adults. In this article, we explore the potential for\ntransformation in elderly care through Agentic Artificial Intelligence (AI),\npowered by Large Language Models (LLMs). We discuss the proactive and\nautonomous decision-making facilitated by Agentic AI in elderly care.\nPersonalized tracking of health, cognitive care, and environmental management,\nall aimed at enhancing independence and high-level living for older adults,\nrepresents important areas of application. With a potential for significant\ntransformation of elderly care, Agentic AI also raises profound concerns about\ndata privacy and security, decision independence, and access. We share key\ninsights to emphasize the need for ethical safeguards, privacy protections, and\ntransparent decision-making. Our goal in this article is to provide a balanced\ndiscussion of both the potential and the challenges associated with Agentic AI,\nand to provide insights into its responsible use in elderly care, to bring\nAgentic AI into harmony with the requirements and vulnerabilities specific to\nthe elderly. Finally, we identify the priorities for the academic research\ncommunities, to achieve human-centered advancements and integration of Agentic\nAI in elderly care. To the best of our knowledge, this is no existing study\nthat reviews the role of Agentic AI in elderly care. Hence, we address the\nliterature gap by analyzing the unique capabilities, applications, and\nlimitations of LLM-based Agentic AI in elderly care. We also provide a\ncompanion interactive dashboard at https://hazratali.github.io/agenticai/.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u4ee3\u7406\u4eba\u5de5\u667a\u80fd\uff08Agentic AI\uff09\u5728\u8001\u5e74\u62a4\u7406\u4e2d\u7684\u6f5c\u529b\u4e0e\u6311\u6218\uff0c\u5f3a\u8c03\u4e2a\u6027\u5316\u5065\u5eb7\u8ddf\u8e2a\u3001\u8ba4\u77e5\u62a4\u7406\u548c\u73af\u5883\u7ba1\u7406\uff0c\u540c\u65f6\u63d0\u51fa\u6570\u636e\u9690\u79c1\u3001\u5b89\u5168\u6027\u548c\u4f26\u7406\u95ee\u9898\u3002", "motivation": "\u5168\u7403\u8001\u9f84\u5316\u4eba\u53e3\u9700\u8981\u521b\u65b0\u7684\u8001\u5e74\u62a4\u7406\u7b56\u7565\uff0c\u4ee3\u7406AI\u56e0\u5176\u4e3b\u52a8\u6027\u548c\u81ea\u4e3b\u51b3\u7b56\u80fd\u529b\u6210\u4e3a\u6f5c\u5728\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u5206\u6790\u4ee3\u7406AI\u5728\u8001\u5e74\u62a4\u7406\u4e2d\u7684\u5e94\u7528\uff0c\u63a2\u8ba8\u5176\u72ec\u7279\u80fd\u529b\u3001\u5c40\u9650\u6027\u548c\u4f26\u7406\u95ee\u9898\u3002", "result": "\u4ee3\u7406AI\u6709\u671b\u663e\u8457\u6539\u5584\u8001\u5e74\u62a4\u7406\uff0c\u4f46\u4e5f\u9700\u89e3\u51b3\u9690\u79c1\u3001\u5b89\u5168\u548c\u4f26\u7406\u95ee\u9898\u3002", "conclusion": "\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u4ee5\u5b9e\u73b0\u4ee5\u4eba\u4e3a\u672c\u7684\u4ee3\u7406AI\u96c6\u6210\uff0c\u5e76\u586b\u8865\u73b0\u6709\u6587\u732e\u7a7a\u767d\u3002"}}
{"id": "2507.15624", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.15624", "abs": "https://arxiv.org/abs/2507.15624", "authors": ["Yusuf Sulistyo Nugroho", "Ganno Tribuana Kurniaji", "Syful Islam", "Mohammed Humayun Kabir", "Vanesya Aura Ardity", "Md. Kamal Uddin"], "title": "Hot Topics and Common Challenges: an Empirical Study of React Discussions on Stack Overflow", "comment": "6 pages, 4 figures, 4 tables, conference paper", "summary": "React is a JavaScript library used to build user interfaces for single-page\napplications. Although recent studies have shown the popularity and advantages\nof React in web development, the specific challenges users face remain unknown.\nThus, this study aims to analyse the React-related questions shared on Stack\nOverflow. The study utilizes an exploratory data analysis to investigate the\nmost frequently discussed keywords, error classification, and user\nreputation-based errors, which is the novelty of this work. The results show\nthe top eight most frequently used keywords on React-related questions, namely,\ncode, link, vir, href, connect, azure, windows, and website. The error\nclassification of questions from the sample shows that algorithmic error is the\nmost frequent issue faced by all groups of users, where mid-reputation users\ncontribute the most, accounting for 55.77%. This suggests the need for the\ncommunity to provide guidance materials in solving algorithm-related problems.\nWe expect that the results of this study will provide valuable insight into\nfuture research to support the React community during the early stages of\nimplementation, facilitating their ability to effectively overcome challenges\nto adoption.", "AI": {"tldr": "\u5206\u6790Stack Overflow\u4e0aReact\u76f8\u5173\u95ee\u9898\u7684\u7814\u7a76\uff0c\u53d1\u73b0\u9ad8\u9891\u5173\u952e\u8bcd\u3001\u9519\u8bef\u5206\u7c7b\u53ca\u7528\u6237\u58f0\u8a89\u4e0e\u9519\u8bef\u7684\u5173\u7cfb\uff0c\u63d0\u51fa\u793e\u533a\u9700\u63d0\u4f9b\u7b97\u6cd5\u95ee\u9898\u6307\u5bfc\u3002", "motivation": "\u5c3d\u7ba1React\u5728\u5355\u9875\u5e94\u7528\u5f00\u53d1\u4e2d\u53d7\u6b22\u8fce\uff0c\u4f46\u7528\u6237\u9762\u4e34\u7684\u5177\u4f53\u6311\u6218\u5c1a\u4e0d\u660e\u786e\uff0c\u56e0\u6b64\u7814\u7a76\u65e8\u5728\u901a\u8fc7Stack Overflow\u6570\u636e\u5206\u6790\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u91c7\u7528\u63a2\u7d22\u6027\u6570\u636e\u5206\u6790\u65b9\u6cd5\uff0c\u7814\u7a76React\u95ee\u9898\u4e2d\u7684\u9ad8\u9891\u5173\u952e\u8bcd\u3001\u9519\u8bef\u5206\u7c7b\u53ca\u7528\u6237\u58f0\u8a89\u4e0e\u9519\u8bef\u7684\u5173\u7cfb\u3002", "result": "\u9ad8\u9891\u5173\u952e\u8bcd\u5305\u62eccode\u3001link\u7b49\uff1b\u7b97\u6cd5\u9519\u8bef\u6700\u5e38\u89c1\uff0c\u4e2d\u58f0\u8a89\u7528\u6237\u8d21\u732e\u6700\u591a\uff0855.77%\uff09\u3002", "conclusion": "\u793e\u533a\u9700\u63d0\u4f9b\u7b97\u6cd5\u95ee\u9898\u6307\u5bfc\uff0c\u7814\u7a76\u7ed3\u679c\u4e3aReact\u793e\u533a\u65e9\u671f\u5b9e\u65bd\u9636\u6bb5\u63d0\u4f9b\u652f\u6301\uff0c\u5e2e\u52a9\u514b\u670d\u6311\u6218\u3002"}}
{"id": "2507.15393", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15393", "abs": "https://arxiv.org/abs/2507.15393", "authors": ["Ruofan Liu", "Yun Lin", "Silas Yeo Shuen Yu", "Xiwen Teoh", "Zhenkai Liang", "Jin Song Dong"], "title": "PiMRef: Detecting and Explaining Ever-evolving Spear Phishing Emails with Knowledge Base Invariants", "comment": null, "summary": "Phishing emails are a critical component of the cybercrime kill chain due to\ntheir wide reach and low cost. Their ever-evolving nature renders traditional\nrule-based and feature-engineered detectors ineffective in the ongoing arms\nrace between attackers and defenders. The rise of large language models (LLMs)\nfurther exacerbates the threat, enabling attackers to craft highly convincing\nphishing emails at minimal cost.\n  This work demonstrates that LLMs can generate psychologically persuasive\nphishing emails tailored to victim profiles, successfully bypassing nearly all\ncommercial and academic detectors. To defend against such threats, we propose\nPiMRef, the first reference-based phishing email detector that leverages\nknowledge-based invariants. Our core insight is that persuasive phishing emails\noften contain disprovable identity claims, which contradict real-world facts.\nPiMRef reframes phishing detection as an identity fact-checking task. Given an\nemail, PiMRef (i) extracts the sender's claimed identity, (ii) verifies the\nlegitimacy of the sender's domain against a predefined knowledge base, and\n(iii) detects call-to-action prompts that push user engagement. Contradictory\nclaims are flagged as phishing indicators and serve as human-understandable\nexplanations.\n  Compared to existing methods such as D-Fence, HelpHed, and ChatSpamDetector,\nPiMRef boosts precision by 8.8% with no loss in recall on standard benchmarks\nlike Nazario and PhishPot. In a real-world evaluation of 10,183 emails across\nfive university accounts over three years, PiMRef achieved 92.1% precision,\n87.9% recall, and a median runtime of 0.05s, outperforming the state-of-the-art\nin both effectiveness and efficiency.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u77e5\u8bc6\u5e93\u7684\u9493\u9c7c\u90ae\u4ef6\u68c0\u6d4b\u65b9\u6cd5PiMRef\uff0c\u901a\u8fc7\u9a8c\u8bc1\u53d1\u4ef6\u4eba\u8eab\u4efd\u7684\u771f\u5b9e\u6027\u548c\u68c0\u6d4b\u53ef\u7591\u884c\u4e3a\u6765\u8bc6\u522b\u9493\u9c7c\u90ae\u4ef6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u7cbe\u5ea6\u548c\u6548\u7387\u3002", "motivation": "\u9493\u9c7c\u90ae\u4ef6\u56e0\u5176\u5e7f\u6cdb\u4f20\u64ad\u548c\u4f4e\u6210\u672c\u6210\u4e3a\u7f51\u7edc\u72af\u7f6a\u7684\u91cd\u8981\u624b\u6bb5\uff0c\u4f20\u7edf\u68c0\u6d4b\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u5176\u52a8\u6001\u53d8\u5316\uff0c\u800c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u5174\u8d77\u8fdb\u4e00\u6b65\u52a0\u5267\u4e86\u8fd9\u4e00\u5a01\u80c1\u3002", "method": "PiMRef\u901a\u8fc7\u63d0\u53d6\u53d1\u4ef6\u4eba\u8eab\u4efd\u3001\u9a8c\u8bc1\u57df\u540d\u5408\u6cd5\u6027\u548c\u68c0\u6d4b\u7528\u6237\u4e92\u52a8\u63d0\u793a\uff0c\u5c06\u9493\u9c7c\u68c0\u6d4b\u8f6c\u5316\u4e3a\u8eab\u4efd\u4e8b\u5b9e\u6838\u67e5\u4efb\u52a1\u3002", "result": "\u5728\u6807\u51c6\u6d4b\u8bd5\u4e2d\uff0cPiMRef\u7684\u7cbe\u5ea6\u63d0\u5347\u4e868.8%\uff0c\u53ec\u56de\u7387\u4fdd\u6301\u4e0d\u53d8\uff1b\u5728\u771f\u5b9e\u573a\u666f\u4e2d\uff0c\u5176\u7cbe\u5ea6\u8fbe92.1%\uff0c\u53ec\u56de\u7387\u8fbe87.9%\uff0c\u8fd0\u884c\u65f6\u95f4\u4e2d\u4f4d\u6570\u4e3a0.05\u79d2\u3002", "conclusion": "PiMRef\u5728\u9493\u9c7c\u90ae\u4ef6\u68c0\u6d4b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e3a\u9632\u5fa1LLM\u751f\u6210\u7684\u9493\u9c7c\u90ae\u4ef6\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.14962", "categories": ["cs.AI", "cs.CC", "cs.LO"], "pdf": "https://arxiv.org/pdf/2507.14962", "abs": "https://arxiv.org/abs/2507.14962", "authors": ["Johannes Schmidt", "Mohamed Maizia", "Victor Lagerkvist", "Johannes K. Fichte"], "title": "Complexity of Faceted Explanations in Propositional Abduction", "comment": "This is the author's self-archived copy including detailed proofs. To\n  appear in Theory and Practice of Logic Programming (TPLP), Proceedings of the\n  41st International Conference on Logic Programming (ICLP 2025)", "summary": "Abductive reasoning is a popular non-monotonic paradigm that aims to explain\nobserved symptoms and manifestations. It has many applications, such as\ndiagnosis and planning in artificial intelligence and database updates. In\npropositional abduction, we focus on specifying knowledge by a propositional\nformula. The computational complexity of tasks in propositional abduction has\nbeen systematically characterized - even with detailed classifications for\nBoolean fragments. Unsurprisingly, the most insightful reasoning problems\n(counting and enumeration) are computationally highly challenging. Therefore,\nwe consider reasoning between decisions and counting, allowing us to understand\nexplanations better while maintaining favorable complexity. We introduce facets\nto propositional abductions, which are literals that occur in some explanation\n(relevant) but not all explanations (dispensable). Reasoning with facets\nprovides a more fine-grained understanding of variability in explanations\n(heterogeneous). In addition, we consider the distance between two\nexplanations, enabling a better understanding of heterogeneity/homogeneity. We\ncomprehensively analyze facets of propositional abduction in various settings,\nincluding an almost complete characterization in Post's framework.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u547d\u9898\u6eaf\u56e0\u4e2d\u7684\u7ec6\u7c92\u5ea6\u63a8\u7406\uff0c\u5f15\u5165\u4e86\u201cfacet\u201d\u6982\u5ff5\u4ee5\u533a\u5206\u89e3\u91ca\u4e2d\u7684\u76f8\u5173\u6027\u548c\u53ef\u5f03\u6027\uff0c\u5e76\u5206\u6790\u4e86\u4e0d\u540c\u8bbe\u7f6e\u4e0b\u7684facet\u7279\u6027\u3002", "motivation": "\u65e8\u5728\u901a\u8fc7\u66f4\u7ec6\u7c92\u5ea6\u7684\u63a8\u7406\u65b9\u6cd5\uff08\u5982facet\u548c\u89e3\u91ca\u95f4\u8ddd\u79bb\uff09\u6765\u66f4\u597d\u5730\u7406\u89e3\u547d\u9898\u6eaf\u56e0\u4e2d\u7684\u89e3\u91ca\u53d8\u5f02\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u590d\u6742\u6027\u53ef\u63a7\u3002", "method": "\u5f15\u5165facet\u6982\u5ff5\uff08\u90e8\u5206\u89e3\u91ca\u4e2d\u51fa\u73b0\u4f46\u4e0d\u5168\u51fa\u73b0\u7684\u6587\u5b57\uff09\uff0c\u5e76\u5206\u6790\u5176\u5728Post\u6846\u67b6\u7b49\u591a\u79cd\u8bbe\u7f6e\u4e0b\u7684\u7279\u6027\u3002", "result": "\u63d0\u4f9b\u4e86\u5bf9facet\u5728\u547d\u9898\u6eaf\u56e0\u4e2d\u7684\u5168\u9762\u5206\u6790\uff0c\u5305\u62ec\u5728Post\u6846\u67b6\u4e2d\u7684\u51e0\u4e4e\u5b8c\u6574\u5206\u7c7b\u3002", "conclusion": "\u901a\u8fc7facet\u548c\u89e3\u91ca\u95f4\u8ddd\u79bb\uff0c\u80fd\u591f\u66f4\u7cbe\u7ec6\u5730\u7406\u89e3\u89e3\u91ca\u7684\u5f02\u8d28\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u590d\u6742\u6027\u5728\u5408\u7406\u8303\u56f4\u5185\u3002"}}
{"id": "2507.15663", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15663", "abs": "https://arxiv.org/abs/2507.15663", "authors": ["Giordano d'Aloisio", "Tosin Fadahunsi", "Jay Choy", "Rebecca Moussa", "Federica Sarro"], "title": "SustainDiffusion: Optimising the Social and Environmental Sustainability of Stable Diffusion Models", "comment": null, "summary": "Background: Text-to-image generation models are widely used across numerous\ndomains. Among these models, Stable Diffusion (SD) - an open-source\ntext-to-image generation model - has become the most popular, producing over 12\nbillion images annually. However, the widespread use of these models raises\nconcerns regarding their social and environmental sustainability.\n  Aims: To reduce the harm that SD models may have on society and the\nenvironment, we introduce SustainDiffusion, a search-based approach designed to\nenhance the social and environmental sustainability of SD models.\n  Method: SustainDiffusion searches the optimal combination of hyperparameters\nand prompt structures that can reduce gender and ethnic bias in generated\nimages while also lowering the energy consumption required for image\ngeneration. Importantly, SustainDiffusion maintains image quality comparable to\nthat of the original SD model.\n  Results: We conduct a comprehensive empirical evaluation of SustainDiffusion,\ntesting it against six different baselines using 56 different prompts. Our\nresults demonstrate that SustainDiffusion can reduce gender bias in SD3 by 68%,\nethnic bias by 59%, and energy consumption (calculated as the sum of CPU and\nGPU energy) by 48%. Additionally, the outcomes produced by SustainDiffusion are\nconsistent across multiple runs and can be generalised to various prompts.\n  Conclusions: With SustainDiffusion, we demonstrate how enhancing the social\nand environmental sustainability of text-to-image generation models is possible\nwithout fine-tuning or changing the model's architecture.", "AI": {"tldr": "SustainDiffusion\u662f\u4e00\u79cd\u57fa\u4e8e\u641c\u7d22\u7684\u65b9\u6cd5\uff0c\u65e8\u5728\u51cf\u5c11Stable Diffusion\u6a21\u578b\u7684\u793e\u4f1a\u548c\u73af\u5883\u5371\u5bb3\uff0c\u540c\u65f6\u4fdd\u6301\u56fe\u50cf\u8d28\u91cf\u3002", "motivation": "\u89e3\u51b3Stable Diffusion\u6a21\u578b\u5728\u793e\u4f1a\u548c\u73af\u5883\u53ef\u6301\u7eed\u6027\u65b9\u9762\u7684\u8d1f\u9762\u5f71\u54cd\uff0c\u5982\u6027\u522b\u548c\u79cd\u65cf\u504f\u89c1\u4ee5\u53ca\u9ad8\u80fd\u8017\u3002", "method": "\u901a\u8fc7\u641c\u7d22\u8d85\u53c2\u6570\u548c\u63d0\u793a\u7ed3\u6784\u7684\u6700\u4f18\u7ec4\u5408\uff0c\u51cf\u5c11\u504f\u89c1\u5e76\u964d\u4f4e\u80fd\u8017\uff0c\u540c\u65f6\u4fdd\u6301\u56fe\u50cf\u8d28\u91cf\u3002", "result": "SustainDiffusion\u663e\u8457\u51cf\u5c11\u4e86\u6027\u522b\u504f\u89c1\uff0868%\uff09\u3001\u79cd\u65cf\u504f\u89c1\uff0859%\uff09\u548c\u80fd\u8017\uff0848%\uff09\uff0c\u4e14\u7ed3\u679c\u7a33\u5b9a\u4e14\u53ef\u63a8\u5e7f\u3002", "conclusion": "SustainDiffusion\u8bc1\u660e\u4e86\u5728\u4e0d\u8c03\u6574\u6a21\u578b\u67b6\u6784\u7684\u60c5\u51b5\u4e0b\uff0c\u63d0\u5347\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u7684\u793e\u4f1a\u548c\u73af\u5883\u53ef\u6301\u7eed\u6027\u662f\u53ef\u884c\u7684\u3002"}}
{"id": "2507.15419", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.15419", "abs": "https://arxiv.org/abs/2507.15419", "authors": ["Wenhao Li", "Selvakumar Manickam", "Yung-wey Chong", "Shankar Karuppayah"], "title": "PhishIntentionLLM: Uncovering Phishing Website Intentions through Multi-Agent Retrieval-Augmented Generation", "comment": "Accepted by EAI ICDF2C 2025", "summary": "Phishing websites remain a major cybersecurity threat, yet existing methods\nprimarily focus on detection, while the recognition of underlying malicious\nintentions remains largely unexplored. To address this gap, we propose\nPhishIntentionLLM, a multi-agent retrieval-augmented generation (RAG) framework\nthat uncovers phishing intentions from website screenshots. Leveraging the\nvisual-language capabilities of large language models (LLMs), our framework\nidentifies four key phishing objectives: Credential Theft, Financial Fraud,\nMalware Distribution, and Personal Information Harvesting. We construct and\nrelease the first phishing intention ground truth dataset (~2K samples) and\nevaluate the framework using four commercial LLMs. Experimental results show\nthat PhishIntentionLLM achieves a micro-precision of 0.7895 with GPT-4o and\nsignificantly outperforms the single-agent baseline with a ~95% improvement in\nmicro-precision. Compared to the previous work, it achieves 0.8545 precision\nfor credential theft, marking a ~4% improvement. Additionally, we generate a\nlarger dataset of ~9K samples for large-scale phishing intention profiling\nacross sectors. This work provides a scalable and interpretable solution for\nintention-aware phishing analysis.", "AI": {"tldr": "\u63d0\u51faPhishIntentionLLM\u6846\u67b6\uff0c\u5229\u7528\u591a\u4ee3\u7406RAG\u6280\u672f\u4ece\u7f51\u7ad9\u622a\u56fe\u8bc6\u522b\u9493\u9c7c\u610f\u56fe\uff0c\u663e\u8457\u63d0\u5347\u68c0\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u9493\u9c7c\u7f51\u7ad9\u68c0\u6d4b\uff0c\u800c\u6076\u610f\u610f\u56fe\u8bc6\u522b\u7814\u7a76\u4e0d\u8db3\u3002", "method": "\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u89c6\u89c9-\u8bed\u8a00\u80fd\u529b\uff0c\u6784\u5efa\u591a\u4ee3\u7406RAG\u6846\u67b6\uff0c\u8bc6\u522b\u56db\u79cd\u9493\u9c7c\u76ee\u6807\u3002", "result": "\u5728GPT-4o\u4e0a\u5b9e\u73b00.7895\u7684\u5fae\u7cbe\u5ea6\uff0c\u6bd4\u5355\u4ee3\u7406\u57fa\u7ebf\u63d0\u5347\u7ea695%\u3002", "conclusion": "\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u53ef\u89e3\u91ca\u7684\u9493\u9c7c\u610f\u56fe\u5206\u6790\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.14987", "categories": ["cs.AI", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14987", "abs": "https://arxiv.org/abs/2507.14987", "authors": ["Yi Zhang", "An Zhang", "XiuYu Zhang", "Leheng Sheng", "Yuxin Chen", "Zhenkai Liang", "Xiang Wang"], "title": "AlphaAlign: Incentivizing Safety Alignment with Extremely Simplified Reinforcement Learning", "comment": null, "summary": "Large language models (LLMs), despite possessing latent safety understanding\nfrom their vast pretraining data, remain vulnerable to generating harmful\ncontent and exhibit issues such as over-refusal and utility degradation after\nsafety alignment. Current safety alignment methods often result in superficial\nrefusal shortcuts or rely on intensive supervision for reasoning-based\napproaches, failing to fully leverage the model's intrinsic safety\nself-awareness. We propose \\textbf{AlphaAlign}, a simple yet effective pure\nreinforcement learning (RL) framework with verifiable safety reward designed to\nincentivize this latent safety awareness through proactive safety reasoning.}\nAlphaAlign employs a dual-reward system: a verifiable safety reward encourages\ncorrectly formatted and explicitly justified refusals for harmful queries while\npenalizing over-refusals, and a normalized helpfulness reward guides\nhigh-quality responses to benign inputs. This allows the model to develop\nproactive safety reasoning capabilities without depending on supervised\nsafety-specific reasoning data. AlphaAlign demonstrates three key advantages:\n(1) Simplicity and efficiency, requiring only binary prompt safety labels and\nminimal RL steps for substantial improvements. (2) Breaking the safety-utility\ntrade-off, by enhancing refusal of harmful content and reducing over-refusals,\nwhile simultaneously maintaining or even improving general task performance and\nrobustness to unseen jailbreaks. (3) Deep alignment, fostering proactive safety\nreasoning that generates explicit safety rationales rather than relying on\nshallow refusal patterns.", "AI": {"tldr": "AlphaAlign\u662f\u4e00\u4e2a\u57fa\u4e8e\u7eaf\u5f3a\u5316\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u9a8c\u8bc1\u7684\u5b89\u5168\u5956\u52b1\u6fc0\u53d1\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5185\u5728\u5b89\u5168\u81ea\u6211\u610f\u8bc6\uff0c\u89e3\u51b3\u6709\u5bb3\u5185\u5bb9\u751f\u6210\u548c\u8fc7\u5ea6\u62d2\u7edd\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u7684\u5b89\u5168\u5bf9\u9f50\u65b9\u6cd5\u5b58\u5728\u6d45\u5c42\u62d2\u7edd\u6216\u4f9d\u8d56\u5bc6\u96c6\u76d1\u7763\u7684\u95ee\u9898\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528\u6a21\u578b\u7684\u5185\u5728\u5b89\u5168\u81ea\u6211\u610f\u8bc6\u3002", "method": "AlphaAlign\u91c7\u7528\u53cc\u5956\u52b1\u7cfb\u7edf\uff1a\u53ef\u9a8c\u8bc1\u7684\u5b89\u5168\u5956\u52b1\u9f13\u52b1\u5bf9\u6709\u5bb3\u67e5\u8be2\u7684\u6b63\u786e\u62d2\u7edd\uff0c\u5e76\u60e9\u7f5a\u8fc7\u5ea6\u62d2\u7edd\uff1b\u6807\u51c6\u5316\u5e2e\u52a9\u6027\u5956\u52b1\u6307\u5bfc\u5bf9\u826f\u6027\u8f93\u5165\u7684\u9ad8\u8d28\u91cf\u54cd\u5e94\u3002", "result": "AlphaAlign\u5728\u7b80\u5316\u6027\u3001\u6548\u7387\u3001\u5b89\u5168\u6027\u4e0e\u5b9e\u7528\u6027\u5e73\u8861\u4ee5\u53ca\u6df1\u5ea6\u5bf9\u9f50\u65b9\u9762\u8868\u73b0\u51fa\u663e\u8457\u4f18\u52bf\u3002", "conclusion": "AlphaAlign\u901a\u8fc7\u4e3b\u52a8\u5b89\u5168\u63a8\u7406\uff0c\u6709\u6548\u63d0\u5347\u6a21\u578b\u7684\u5b89\u5168\u6027\u548c\u5b9e\u7528\u6027\uff0c\u65e0\u9700\u4f9d\u8d56\u76d1\u7763\u6570\u636e\u3002"}}
{"id": "2507.15666", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.15666", "abs": "https://arxiv.org/abs/2507.15666", "authors": ["Igor Turkin", "Lina Volobuieva", "Andriy Chukhray", "Oleksandr Liubimov"], "title": "Modeling CubeSat Storage Battery Discharge: Equivalent Circuit Versus Machine Learning Approaches", "comment": "13 pages, 15 figures", "summary": "The subject of the article is the study and comparison of two approaches to\nmodelling the battery discharge of a CubeSat satellite: analytical using\nequivalent circuit and machine learning. The article aims to make a reasoned\nchoice of the approach to modelling the battery discharge of a CubeSat\nsatellite. Modelling the battery discharge of a satellite will enable the\nprediction of the consequences of disconnecting the autonomous power system and\nensure the fault tolerance of equipment in orbit. Therefore, the selected study\nis relevant and promising. This study focuses on the analysis of CubeSat\nsatellite data, based explicitly on orbital data samples of the power system,\nwhich include data available at the time of the article publication. The\ndataset contains data on the voltage, current, and temperature of the battery\nand solar panels attached to the five sides of the satellite. In this context,\ntwo approaches are considered: analytical modelling based on physical laws and\nmachine learning, which uses empirical data to create a predictive model.\nResults: A comparative analysis of the modeling results reveals that the\nequivalent circuit approach has the advantage of transparency, as it identifies\npossible parameters that facilitate understanding of the relationships.\nHowever, the model is less flexible to environmental changes or non-standard\nsatellite behavior. The machine learning model demonstrated more accurate\nresults, as it can account for complex dependencies and adapt to actual\nconditions, even when they deviate from theoretical assumptions.", "AI": {"tldr": "\u6587\u7ae0\u6bd4\u8f83\u4e86\u4e24\u79cd\u5efa\u6a21CubeSat\u536b\u661f\u7535\u6c60\u653e\u7535\u7684\u65b9\u6cd5\uff1a\u7b49\u6548\u7535\u8def\u5206\u6790\u548c\u673a\u5668\u5b66\u4e60\uff0c\u65e8\u5728\u9009\u62e9\u66f4\u4f18\u65b9\u6cd5\u4ee5\u63d0\u9ad8\u536b\u661f\u7535\u6e90\u7cfb\u7edf\u7684\u9884\u6d4b\u548c\u5bb9\u9519\u80fd\u529b\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u4e3aCubeSat\u536b\u661f\u7535\u6c60\u653e\u7535\u5efa\u6a21\uff0c\u4ee5\u9884\u6d4b\u7535\u6e90\u7cfb\u7edf\u65ad\u5f00\u7684\u540e\u679c\u5e76\u786e\u4fdd\u8f68\u9053\u8bbe\u5907\u7684\u5bb9\u9519\u6027\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u57fa\u4e8e\u7269\u7406\u5b9a\u5f8b\u7684\u7b49\u6548\u7535\u8def\u5206\u6790\u548c\u57fa\u4e8e\u5b9e\u9645\u6570\u636e\u7684\u673a\u5668\u5b66\u4e60\u5efa\u6a21\u3002", "result": "\u7b49\u6548\u7535\u8def\u6a21\u578b\u900f\u660e\u4f46\u7075\u6d3b\u6027\u5dee\uff1b\u673a\u5668\u5b66\u4e60\u6a21\u578b\u66f4\u51c6\u786e\u4e14\u9002\u5e94\u6027\u5f3a\u3002", "conclusion": "\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5728\u590d\u6742\u6761\u4ef6\u4e0b\u8868\u73b0\u66f4\u4f18\uff0c\u9002\u5408\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2507.15449", "categories": ["cs.CR", "cs.SC"], "pdf": "https://arxiv.org/pdf/2507.15449", "abs": "https://arxiv.org/abs/2507.15449", "authors": ["Alessio Caminata", "Elisa Gorla", "Madison Mabe", "Martina Vigorito", "Irene Villa"], "title": "Cryptanalysis of a multivariate CCZ scheme", "comment": "are welcome!", "summary": "We consider the multivariate scheme Pesto, which was introduced by Calderini,\nCaminata, and Villa. In this scheme, the public polynomials are obtained by\napplying a CCZ transformation to a set of quadratic secret polynomials. As a\nconsequence, the public key consists of polynomials of degree 4. In this work,\nwe show that the public degree 4 polynomial system can be efficiently reduced\nto a system of quadratic polynomials. This seems to suggest that the CCZ\ntransformation may not offer a significant increase in security, contrary to\nwhat was initially believed.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86Pesto\u65b9\u6848\uff0c\u53d1\u73b0\u5176\u516c\u5f00\u76844\u6b21\u591a\u9879\u5f0f\u7cfb\u7edf\u53ef\u9ad8\u6548\u964d\u4e3a\u4e8c\u6b21\u591a\u9879\u5f0f\u7cfb\u7edf\uff0c\u8d28\u7591CCZ\u53d8\u6362\u7684\u5b89\u5168\u6027\u63d0\u5347\u6548\u679c\u3002", "motivation": "\u7814\u7a76Pesto\u65b9\u6848\u4e2dCCZ\u53d8\u6362\u5bf9\u5b89\u5168\u6027\u7684\u5b9e\u9645\u5f71\u54cd\uff0c\u9a8c\u8bc1\u5176\u662f\u5426\u5982\u9884\u671f\u63d0\u4f9b\u663e\u8457\u5b89\u5168\u6027\u63d0\u5347\u3002", "method": "\u901a\u8fc7\u6570\u5b66\u65b9\u6cd5\u5c06\u516c\u5f00\u76844\u6b21\u591a\u9879\u5f0f\u7cfb\u7edf\u964d\u4e3a\u4e8c\u6b21\u591a\u9879\u5f0f\u7cfb\u7edf\u3002", "result": "\u53d1\u73b0CCZ\u53d8\u6362\u5e76\u672a\u663e\u8457\u589e\u52a0\u5b89\u5168\u6027\uff0c\u516c\u5f00\u7cfb\u7edf\u53ef\u88ab\u9ad8\u6548\u964d\u7ef4\u3002", "conclusion": "CCZ\u53d8\u6362\u5728Pesto\u65b9\u6848\u4e2d\u7684\u5b89\u5168\u6027\u63d0\u5347\u6548\u679c\u6709\u9650\uff0c\u9700\u91cd\u65b0\u8bc4\u4f30\u5176\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2507.15013", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15013", "abs": "https://arxiv.org/abs/2507.15013", "authors": ["Xiaoyu Li", "Jin Wu", "Shaoyang Guo", "Haoran Shi", "Chanjin Zheng"], "title": "A Forced-Choice Neural Cognitive Diagnostic Model of Personality Testing", "comment": "15pages, 7 figures", "summary": "In the smart era, psychometric tests are becoming increasingly important for\npersonnel selection, career development, and mental health assessment.\nForced-choice tests are common in personality assessments because they require\nparticipants to select from closely related options, lowering the risk of\nresponse distortion. This study presents a deep learning-based Forced-Choice\nNeural Cognitive Diagnostic Model (FCNCD) that overcomes the limitations of\ntraditional models and is applicable to the three most common item block types\nfound in forced-choice tests. To account for the unidimensionality of items in\nforced-choice tests, we create interpretable participant and item parameters.\nWe model the interactions between participant and item features using\nmultilayer neural networks after mining them using nonlinear mapping. In\naddition, we use the monotonicity assumption to improve the interpretability of\nthe diagnostic results. The FCNCD's effectiveness is validated by experiments\non real-world and simulated datasets that show its accuracy, interpretability,\nand robustness.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u5f3a\u5236\u9009\u62e9\u795e\u7ecf\u8ba4\u77e5\u8bca\u65ad\u6a21\u578b\uff08FCNCD\uff09\uff0c\u7528\u4e8e\u89e3\u51b3\u4f20\u7edf\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u9002\u7528\u4e8e\u4e09\u79cd\u5e38\u89c1\u9898\u578b\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u51c6\u786e\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u5728\u667a\u80fd\u65f6\u4ee3\uff0c\u5fc3\u7406\u6d4b\u91cf\u6d4b\u8bd5\u5728\u4eba\u5458\u9009\u62d4\u3001\u804c\u4e1a\u53d1\u5c55\u548c\u5fc3\u7406\u5065\u5eb7\u8bc4\u4f30\u4e2d\u65e5\u76ca\u91cd\u8981\uff0c\u800c\u5f3a\u5236\u9009\u62e9\u6d4b\u8bd5\u56e0\u5176\u80fd\u964d\u4f4e\u56de\u7b54\u5931\u771f\u7684\u98ce\u9669\u800c\u88ab\u5e7f\u6cdb\u4f7f\u7528\u3002", "method": "\u901a\u8fc7\u975e\u7ebf\u6027\u6620\u5c04\u6316\u6398\u53c2\u4e0e\u8005\u548c\u9879\u76ee\u7279\u5f81\uff0c\u4f7f\u7528\u591a\u5c42\u795e\u7ecf\u7f51\u7edc\u5efa\u6a21\u5176\u4ea4\u4e92\uff0c\u5e76\u5229\u7528\u5355\u8c03\u6027\u5047\u8bbe\u63d0\u5347\u8bca\u65ad\u7ed3\u679c\u7684\u53ef\u89e3\u91ca\u6027\u3002", "result": "\u5728\u771f\u5b9e\u548c\u6a21\u62df\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86FCNCD\u7684\u51c6\u786e\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "FCNCD\u6a21\u578b\u6709\u6548\u514b\u670d\u4e86\u4f20\u7edf\u6a21\u578b\u7684\u9650\u5236\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u5f3a\u5236\u9009\u62e9\u6d4b\u8bd5\u9898\u578b\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2507.15671", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.15671", "abs": "https://arxiv.org/abs/2507.15671", "authors": ["Jinyao Guo", "Chengpeng Wang", "Dominic Deluca", "Jinjie Liu", "Zhuo Zhang", "Xiangyu Zhang"], "title": "BugScope: Learn to Find Bugs Like Human", "comment": "19 pages, 2 figure, 6 tables, 4 listings", "summary": "Detecting software bugs remains a fundamental challenge due to the extensive\ndiversity of real-world defects. Traditional static analysis tools often rely\non symbolic workflows, which restrict their coverage and hinder adaptability to\ncustomized bugs with diverse anti-patterns. While recent advances incorporate\nlarge language models (LLMs) to enhance bug detection, these methods continue\nto struggle with sophisticated bugs and typically operate within limited\nanalysis contexts. To address these challenges, we propose BugScope, an\nLLM-driven multi-agent system that emulates how human auditors learn new bug\npatterns from representative examples and apply that knowledge during code\nauditing. Given a set of examples illustrating both buggy and non-buggy\nbehaviors, BugScope synthesizes a retrieval strategy to extract relevant\ndetection contexts via program slicing and then constructs a tailored detection\nprompt to guide accurate reasoning by the LLM. Our evaluation on a curated\ndataset of 40 real-world bugs drawn from 21 widely-used open-source projects\ndemonstrates that BugScope achieves 87.04% precision and 90.00% recall,\nsurpassing state-of-the-art industrial tools by 0.44 in F1 score. Further\ntesting on large-scale open-source systems, including the Linux kernel,\nuncovered 141 previously unknown bugs, of which 78 have been fixed and 7\nconfirmed by developers, highlighting BugScope's substantial practical impact.", "AI": {"tldr": "BugScope\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u901a\u8fc7\u6a21\u62df\u4eba\u7c7b\u5ba1\u8ba1\u5458\u5b66\u4e60\u65b0\u9519\u8bef\u6a21\u5f0f\u7684\u65b9\u5f0f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8f6f\u4ef6\u9519\u8bef\u68c0\u6d4b\u7684\u7cbe\u5ea6\u548c\u53ec\u56de\u7387\u3002", "motivation": "\u4f20\u7edf\u9759\u6001\u5206\u6790\u5de5\u5177\u5728\u8986\u76d6\u8303\u56f4\u548c\u9002\u5e94\u6027\u4e0a\u53d7\u9650\uff0c\u800c\u73b0\u6709\u7684LLM\u65b9\u6cd5\u5bf9\u590d\u6742\u9519\u8bef\u7684\u5904\u7406\u80fd\u529b\u4e0d\u8db3\u3002BugScope\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "BugScope\u901a\u8fc7\u7a0b\u5e8f\u5207\u7247\u63d0\u53d6\u76f8\u5173\u68c0\u6d4b\u4e0a\u4e0b\u6587\uff0c\u5e76\u6784\u5efa\u5b9a\u5236\u5316\u7684\u68c0\u6d4b\u63d0\u793a\uff0c\u6307\u5bfcLLM\u8fdb\u884c\u51c6\u786e\u63a8\u7406\u3002", "result": "\u572840\u4e2a\u771f\u5b9e\u9519\u8bef\u7684\u6570\u636e\u96c6\u4e0a\uff0cBugScope\u8fbe\u523087.04%\u7684\u7cbe\u5ea6\u548c90.00%\u7684\u53ec\u56de\u7387\uff0cF1\u5206\u6570\u4f18\u4e8e\u73b0\u6709\u5de5\u5177\u3002\u5728Linux\u5185\u6838\u7b49\u5927\u89c4\u6a21\u7cfb\u7edf\u4e2d\u53d1\u73b0\u4e86141\u4e2a\u672a\u77e5\u9519\u8bef\u3002", "conclusion": "BugScope\u5728\u9519\u8bef\u68c0\u6d4b\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u663e\u8457\u7684\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2507.15613", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15613", "abs": "https://arxiv.org/abs/2507.15613", "authors": ["Andrii Balashov", "Olena Ponomarova", "Xiaohua Zhai"], "title": "Multi-Stage Prompt Inference Attacks on Enterprise LLM Systems", "comment": "26 pages", "summary": "Large Language Models (LLMs) deployed in enterprise settings (e.g., as\nMicrosoft 365 Copilot) face novel security challenges. One critical threat is\nprompt inference attacks: adversaries chain together seemingly benign prompts\nto gradually extract confidential data. In this paper, we present a\ncomprehensive study of multi-stage prompt inference attacks in an enterprise\nLLM context. We simulate realistic attack scenarios where an attacker uses\nmild-mannered queries and indirect prompt injections to exploit an LLM\nintegrated with private corporate data. We develop a formal threat model for\nthese multi-turn inference attacks and analyze them using probability theory,\noptimization frameworks, and information-theoretic leakage bounds. The attacks\nare shown to reliably exfiltrate sensitive information from the LLM's context\n(e.g., internal SharePoint documents or emails), even when standard safety\nmeasures are in place.\n  We propose and evaluate defenses to counter such attacks, including\nstatistical anomaly detection, fine-grained access control, prompt sanitization\ntechniques, and architectural modifications to LLM deployment. Each defense is\nsupported by mathematical analysis or experimental simulation. For example, we\nderive bounds on information leakage under differential privacy-based training\nand demonstrate an anomaly detection method that flags multi-turn attacks with\nhigh AUC. We also introduce an approach called \"spotlighting\" that uses input\ntransformations to isolate untrusted prompt content, reducing attack success by\nan order of magnitude. Finally, we provide a formal proof of concept and\nempirical validation for a combined defense-in-depth strategy. Our work\nhighlights that securing LLMs in enterprise settings requires moving beyond\nsingle-turn prompt filtering toward a holistic, multi-stage perspective on both\nattacks and defenses.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u4f01\u4e1a\u73af\u5883\u4e2dLLMs\u9762\u4e34\u7684\u591a\u9636\u6bb5\u63d0\u793a\u63a8\u7406\u653b\u51fb\uff0c\u63d0\u51fa\u4e86\u9632\u5fa1\u63aa\u65bd\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u4f01\u4e1a\u90e8\u7f72\u7684LLMs\u9762\u4e34\u65b0\u578b\u5b89\u5168\u5a01\u80c1\uff0c\u5982\u901a\u8fc7\u591a\u9636\u6bb5\u63d0\u793a\u63a8\u7406\u653b\u51fb\u63d0\u53d6\u673a\u5bc6\u6570\u636e\u3002", "method": "\u901a\u8fc7\u6982\u7387\u7406\u8bba\u3001\u4f18\u5316\u6846\u67b6\u548c\u4fe1\u606f\u8bba\u6cc4\u6f0f\u8fb9\u754c\u5206\u6790\u653b\u51fb\uff0c\u63d0\u51fa\u7edf\u8ba1\u5f02\u5e38\u68c0\u6d4b\u3001\u7ec6\u7c92\u5ea6\u8bbf\u95ee\u63a7\u5236\u7b49\u9632\u5fa1\u63aa\u65bd\u3002", "result": "\u653b\u51fb\u80fd\u53ef\u9760\u63d0\u53d6\u654f\u611f\u4fe1\u606f\uff0c\u63d0\u51fa\u7684\u9632\u5fa1\u63aa\u65bd\uff08\u5982\u5dee\u5206\u9690\u79c1\u8bad\u7ec3\u548c\u5f02\u5e38\u68c0\u6d4b\uff09\u663e\u8457\u964d\u4f4e\u653b\u51fb\u6210\u529f\u7387\u3002", "conclusion": "\u4f01\u4e1aLLM\u5b89\u5168\u9700\u4ece\u5355\u8f6e\u63d0\u793a\u8fc7\u6ee4\u8f6c\u5411\u591a\u9636\u6bb5\u653b\u51fb\u4e0e\u9632\u5fa1\u7684\u6574\u4f53\u89c6\u89d2\u3002"}}
{"id": "2507.15042", "categories": ["cs.AI", "cs.IR", "I.2.7; H.3.3; K.6.5"], "pdf": "https://arxiv.org/pdf/2507.15042", "abs": "https://arxiv.org/abs/2507.15042", "authors": ["Jerry Wang", "Fang Yu"], "title": "DeRAG: Black-box Adversarial Attacks on Multiple Retrieval-Augmented Generation Applications via Prompt Injection", "comment": "Accepted by KDD Workshop on Prompt Optimization 2025", "summary": "Adversarial prompt attacks can significantly alter the reliability of\nRetrieval-Augmented Generation (RAG) systems by re-ranking them to produce\nincorrect outputs. In this paper, we present a novel method that applies\nDifferential Evolution (DE) to optimize adversarial prompt suffixes for\nRAG-based question answering. Our approach is gradient-free, treating the RAG\npipeline as a black box and evolving a population of candidate suffixes to\nmaximize the retrieval rank of a targeted incorrect document to be closer to\nreal world scenarios. We conducted experiments on the BEIR QA datasets to\nevaluate attack success at certain retrieval rank thresholds under multiple\nretrieving applications. Our results demonstrate that DE-based prompt\noptimization attains competitive (and in some cases higher) success rates\ncompared to GGPP to dense retrievers and PRADA to sparse retrievers, while\nusing only a small number of tokens (<=5 tokens) in the adversarial suffix.\nFurthermore, we introduce a readability-aware suffix construction strategy,\nvalidated by a statistically significant reduction in MLM negative\nlog-likelihood with Welch's t-test. Through evaluations with a BERT-based\nadversarial suffix detector, we show that DE-generated suffixes evade\ndetection, yielding near-chance detection accuracy.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5dee\u5206\u8fdb\u5316\uff08DE\uff09\u7684\u65b9\u6cd5\uff0c\u4f18\u5316\u5bf9\u6297\u6027\u63d0\u793a\u540e\u7f00\u4ee5\u653b\u51fbRAG\u7cfb\u7edf\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u653b\u51fb\u6210\u529f\u7387\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u80fd\u9003\u907f\u68c0\u6d4b\u3002", "motivation": "\u5bf9\u6297\u6027\u63d0\u793a\u653b\u51fb\u4f1a\u663e\u8457\u5f71\u54cdRAG\u7cfb\u7edf\u7684\u53ef\u9760\u6027\uff0c\u672c\u6587\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u66f4\u6709\u6548\u7684\u653b\u51fb\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u5dee\u5206\u8fdb\u5316\uff08DE\uff09\u4f18\u5316\u5bf9\u6297\u6027\u63d0\u793a\u540e\u7f00\uff0c\u5c06\u5176\u89c6\u4e3a\u9ed1\u76d2\u95ee\u9898\uff0c\u901a\u8fc7\u8fdb\u5316\u5019\u9009\u540e\u7f00\u6765\u6700\u5927\u5316\u9519\u8bef\u6587\u6863\u7684\u68c0\u7d22\u6392\u540d\u3002", "result": "\u5728BEIR QA\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0cDE\u65b9\u6cd5\u653b\u51fb\u6210\u529f\u7387\u9ad8\uff0c\u4e14\u4ec5\u9700\u5c11\u91cf\u6807\u8bb0\uff08\u22645\uff09\uff0c\u540c\u65f6\u80fd\u9003\u907fBERT\u68c0\u6d4b\u5668\u7684\u68c0\u6d4b\u3002", "conclusion": "DE\u65b9\u6cd5\u5728\u653b\u51fbRAG\u7cfb\u7edf\u65f6\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u5bf9\u6297\u6027\u540e\u7f00\u5177\u6709\u53ef\u8bfb\u6027\u548c\u9690\u853d\u6027\u3002"}}
{"id": "2507.15822", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15822", "abs": "https://arxiv.org/abs/2507.15822", "authors": ["Li Huang", "Ilgiz Mustafin", "Marco Piccioni", "Alessandro Schena", "Reto Weber", "Bertrand Meyer"], "title": "Do AI models help produce verified bug fixes?", "comment": null, "summary": "Among areas of software engineering where AI techniques -- particularly,\nLarge Language Models -- seem poised to yield dramatic improvements, an\nattractive candidate is Automatic Program Repair (APR), the production of\nsatisfactory corrections to software bugs. Does this expectation materialize in\npractice? How do we find out, making sure that proposed corrections actually\nwork? If programmers have access to LLMs, how do they actually use them to\ncomplement their own skills?\n  To answer these questions, we took advantage of the availability of a\nprogram-proving environment, which formally determines the correctness of\nproposed fixes, to conduct a study of program debugging with two randomly\nassigned groups of programmers, one with access to LLMs and the other without,\nboth validating their answers through the proof tools. The methodology relied\non a division into general research questions (Goals in the Goal-Query-Metric\napproach), specific elements admitting specific answers (Queries), and\nmeasurements supporting these answers (Metrics). While applied so far to a\nlimited sample size, the results are a first step towards delineating a proper\nrole for AI and LLMs in providing guaranteed-correct fixes to program bugs.\n  These results caused surprise as compared to what one might expect from the\nuse of AI for debugging and APR. The contributions also include: a detailed\nmethodology for experiments in the use of LLMs for debugging, which other\nprojects can reuse; a fine-grain analysis of programmer behavior, made possible\nby the use of full-session recording; a definition of patterns of use of LLMs,\nwith 7 distinct categories; and validated advice for getting the best of LLMs\nfor debugging and Automatic Program Repair.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u81ea\u52a8\u7a0b\u5e8f\u4fee\u590d\uff08APR\uff09\u4e2d\u7684\u5b9e\u9645\u6548\u679c\uff0c\u901a\u8fc7\u5b9e\u9a8c\u6bd4\u8f83\u4e86\u7a0b\u5e8f\u5458\u4f7f\u7528\u548c\u4e0d\u4f7f\u7528LLM\u7684\u8c03\u8bd5\u8868\u73b0\uff0c\u5e76\u63d0\u51fa\u4e86\u76f8\u5173\u65b9\u6cd5\u8bba\u548c\u4f7f\u7528\u6a21\u5f0f\u3002", "motivation": "\u63a2\u7d22LLM\u5728APR\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u6548\u679c\uff0c\u9a8c\u8bc1\u5176\u662f\u5426\u80fd\u63d0\u4f9b\u6b63\u786e\u4fee\u590d\uff0c\u5e76\u7814\u7a76\u7a0b\u5e8f\u5458\u5982\u4f55\u5229\u7528LLM\u8f85\u52a9\u8c03\u8bd5\u3002", "method": "\u91c7\u7528\u968f\u673a\u5206\u7ec4\u5b9e\u9a8c\uff0c\u4e00\u7ec4\u7a0b\u5e8f\u5458\u4f7f\u7528LLM\uff0c\u53e6\u4e00\u7ec4\u4e0d\u4f7f\u7528\uff0c\u901a\u8fc7\u7a0b\u5e8f\u8bc1\u660e\u5de5\u5177\u9a8c\u8bc1\u4fee\u590d\u7684\u6b63\u786e\u6027\uff0c\u5e76\u7ed3\u5408\u76ee\u6807-\u67e5\u8be2-\u5ea6\u91cf\u65b9\u6cd5\u8fdb\u884c\u5206\u6790\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u4e0e\u9884\u671f\u4e0d\u540c\uff0c\u63ed\u793a\u4e86LLM\u5728\u8c03\u8bd5\u4e2d\u7684\u5b9e\u9645\u6548\u679c\uff0c\u5e76\u63d0\u51fa\u4e867\u79cdLLM\u4f7f\u7528\u6a21\u5f0f\u53ca\u4f18\u5316\u5efa\u8bae\u3002", "conclusion": "\u7814\u7a76\u4e3aAI\u548cLLM\u5728\u7a0b\u5e8f\u4fee\u590d\u4e2d\u7684\u5408\u7406\u5e94\u7528\u63d0\u4f9b\u4e86\u521d\u6b65\u4f9d\u636e\uff0c\u5e76\u63d0\u51fa\u4e86\u53ef\u590d\u7528\u7684\u5b9e\u9a8c\u65b9\u6cd5\u548c\u5b9e\u7528\u5efa\u8bae\u3002"}}
{"id": "2507.15660", "categories": ["cs.CR", "cs.CY"], "pdf": "https://arxiv.org/pdf/2507.15660", "abs": "https://arxiv.org/abs/2507.15660", "authors": ["Rohit Negi", "Amit Negi", "Manish Sharma", "S. Venkatesan", "Prem Kumar", "Sandeep K. Shukla"], "title": "Cyber security of Mega Events: A Case Study of Securing the Digital Infrastructure for MahaKumbh 2025 -- A 45 days Mega Event of 600 Million Footfalls", "comment": "11 pages, 11 tables", "summary": "Mega events such as the Olympics, World Cup tournaments, G-20 Summit,\nreligious events such as MahaKumbh are increasingly digitalized. From event\nticketing, vendor booth or lodging reservations, sanitation, event scheduling,\ncustomer service, crime reporting, media streaming and messaging on digital\ndisplay boards, surveillance, crowd control, traffic control and many other\nservices are based on mobile and web applications, wired and wireless\nnetworking, network of Closed-Circuit Television (CCTV) cameras, specialized\ncontrol room with network and video-feed monitoring. Consequently, cyber\nthreats directed at such digital infrastructure are common. Starting from hobby\nhackers, hacktivists, cyber crime gangs, to the nation state actors, all target\nsuch infrastructure to unleash chaos on an otherwise smooth operation, and\noften the cyber threat actors attempt to embarrass the organizing country or\nthe organizers. Unlike long-standing organizations such as a corporate or a\ngovernment department, the infrastructure of mega-events is temporary,\nconstructed over a short time span in expediency, and often shortcuts are taken\nto make the deadline for the event. As a result, securing such an elaborate yet\ntemporary infrastructure requires a different approach than securing a standard\norganizational digital infrastructure. In this paper, we describe our approach\nto securing MahaKumbh 2025, a 600 million footfall event for 45 days in\nPrayagraj, India, as a cyber security assessment and risk management oversight\nteam. We chronicle the scope, process, methodology, and outcome of our team's\neffort to secure this mega event. It should be noted that none of the cyber\nattacks during the 45-day event was successful. Our goal is to put on record\nthe methodology and discuss what we would do differently in case we work on\nsimilar future mega event.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5927\u578b\u6d3b\u52a8\uff08\u5982\u5965\u8fd0\u4f1a\u3001\u4e16\u754c\u676f\u3001G20\u5cf0\u4f1a\u7b49\uff09\u6570\u5b57\u5316\u5e26\u6765\u7684\u7f51\u7edc\u5b89\u5168\u6311\u6218\uff0c\u5e76\u4ee52025\u5e74\u5370\u5ea6MahaKumbh\u6d3b\u52a8\u4e3a\u4f8b\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u4e34\u65f6\u6027\u6570\u5b57\u57fa\u7840\u8bbe\u65bd\u7684\u5b89\u5168\u8bc4\u4f30\u548c\u98ce\u9669\u7ba1\u7406\u65b9\u6cd5\u3002", "motivation": "\u5927\u578b\u6d3b\u52a8\u7684\u6570\u5b57\u57fa\u7840\u8bbe\u65bd\u901a\u5e38\u662f\u4e34\u65f6\u642d\u5efa\u7684\uff0c\u5b58\u5728\u5b89\u5168\u6f0f\u6d1e\uff0c\u5bb9\u6613\u6210\u4e3a\u9ed1\u5ba2\u653b\u51fb\u76ee\u6807\u3002\u672c\u6587\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u9488\u5bf9\u6b64\u7c7b\u4e34\u65f6\u6027\u57fa\u7840\u8bbe\u65bd\u7684\u7f51\u7edc\u5b89\u5168\u4fdd\u62a4\u65b9\u6cd5\u3002", "method": "\u4f5c\u8005\u56e2\u961f\u4f5c\u4e3a\u7f51\u7edc\u5b89\u5168\u8bc4\u4f30\u548c\u98ce\u9669\u7ba1\u7406\u76d1\u7763\u56e2\u961f\uff0c\u8be6\u7ec6\u63cf\u8ff0\u4e86\u5176\u8303\u56f4\u3001\u6d41\u7a0b\u3001\u65b9\u6cd5\u8bba\u53ca\u5b9e\u65bd\u8fc7\u7a0b\uff0c\u4ee5\u786e\u4fddMahaKumbh 2025\u6d3b\u52a8\u7684\u5b89\u5168\u3002", "result": "\u572845\u5929\u7684\u6d3b\u52a8\u671f\u95f4\uff0c\u6240\u6709\u7f51\u7edc\u653b\u51fb\u5747\u672a\u6210\u529f\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u6587\u8bb0\u5f55\u4e86\u8be5\u65b9\u6cd5\u8bba\uff0c\u5e76\u8ba8\u8bba\u4e86\u672a\u6765\u7c7b\u4f3c\u5927\u578b\u6d3b\u52a8\u4e2d\u53ef\u4ee5\u6539\u8fdb\u7684\u5730\u65b9\u3002"}}
{"id": "2507.15106", "categories": ["cs.AI", "cs.RO", "F.2.2"], "pdf": "https://arxiv.org/pdf/2507.15106", "abs": "https://arxiv.org/abs/2507.15106", "authors": ["Xia Xu", "Jochen Triesch"], "title": "From Kicking to Causality: Simulating Infant Agency Detection with a Robust Intrinsic Reward", "comment": "13 pages, 5 figures", "summary": "While human infants robustly discover their own causal efficacy, standard\nreinforcement learning agents remain brittle, as their reliance on\ncorrelation-based rewards fails in noisy, ecologically valid scenarios. To\naddress this, we introduce the Causal Action Influence Score (CAIS), a novel\nintrinsic reward rooted in causal inference. CAIS quantifies an action's\ninfluence by measuring the 1-Wasserstein distance between the learned\ndistribution of sensory outcomes conditional on that action, $p(h|a)$, and the\nbaseline outcome distribution, $p(h)$. This divergence provides a robust reward\nthat isolates the agent's causal impact from confounding environmental noise.\nWe test our approach in a simulated infant-mobile environment where\ncorrelation-based perceptual rewards fail completely when the mobile is\nsubjected to external forces. In stark contrast, CAIS enables the agent to\nfilter this noise, identify its influence, and learn the correct policy.\nFurthermore, the high-quality predictive model learned for CAIS allows our\nagent, when augmented with a surprise signal, to successfully reproduce the\n\"extinction burst\" phenomenon. We conclude that explicitly inferring causality\nis a crucial mechanism for developing a robust sense of agency, offering a\npsychologically plausible framework for more adaptive autonomous systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56e0\u679c\u63a8\u7406\u7684Causal Action Influence Score (CAIS)\u4f5c\u4e3a\u5185\u5728\u5956\u52b1\uff0c\u7528\u4e8e\u5f3a\u5316\u5b66\u4e60\uff0c\u4ee5\u89e3\u51b3\u4f20\u7edf\u76f8\u5173\u6027\u5956\u52b1\u5728\u566a\u58f0\u73af\u5883\u4e2d\u7684\u8106\u5f31\u6027\u95ee\u9898\u3002", "motivation": "\u4eba\u7c7b\u5a74\u513f\u80fd\u6709\u6548\u53d1\u73b0\u81ea\u8eab\u56e0\u679c\u6548\u5e94\uff0c\u800c\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u5728\u566a\u58f0\u73af\u5883\u4e2d\u4f9d\u8d56\u76f8\u5173\u6027\u5956\u52b1\u8868\u73b0\u8106\u5f31\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9c81\u68d2\u7684\u5956\u52b1\u673a\u5236\u3002", "method": "CAIS\u901a\u8fc7\u8ba1\u7b97\u52a8\u4f5c\u6761\u4ef6\u611f\u5b98\u7ed3\u679c\u5206\u5e03\u4e0e\u57fa\u7ebf\u5206\u5e03\u76841-Wasserstein\u8ddd\u79bb\uff0c\u91cf\u5316\u52a8\u4f5c\u7684\u56e0\u679c\u5f71\u54cd\uff0c\u8fc7\u6ee4\u73af\u5883\u566a\u58f0\u3002", "result": "\u5728\u6a21\u62df\u5a74\u513f-\u79fb\u52a8\u73af\u5883\u4e2d\uff0cCAIS\u6210\u529f\u8fc7\u6ee4\u566a\u58f0\u5e76\u5b66\u4e60\u6b63\u786e\u7b56\u7565\uff0c\u540c\u65f6\u80fd\u590d\u73b0\u201c\u6d88\u9000\u7206\u53d1\u201d\u73b0\u8c61\u3002", "conclusion": "\u663e\u5f0f\u63a8\u65ad\u56e0\u679c\u5173\u7cfb\u662f\u53d1\u5c55\u9c81\u68d2\u4ee3\u7406\u611f\u7684\u5173\u952e\u673a\u5236\uff0c\u4e3a\u81ea\u9002\u5e94\u81ea\u4e3b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5fc3\u7406\u5b66\u5408\u7406\u7684\u6846\u67b6\u3002"}}
{"id": "2507.15828", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.15828", "abs": "https://arxiv.org/abs/2507.15828", "authors": ["Mauro Marcelino", "Marcos Alves", "Bianca Trinkenreich", "Bruno Cartaxo", "S\u00e9rgio Soares", "Simone D. J. Barbosa", "Marcos Kalinowski"], "title": "Investigating the Use of LLMs for Evidence Briefings Generation in Software Engineering", "comment": "ESEM 2025 Registered Report with an IPA (In Principle Acceptance) for\n  the Empirical Software Engineering journal", "summary": "[Context] An evidence briefing is a concise and objective transfer medium\nthat can present the main findings of a study to software engineers in the\nindustry. Although practitioners and researchers have deemed Evidence Briefings\nuseful, their production requires manual labor, which may be a significant\nchallenge to their broad adoption. [Goal] The goal of this registered report is\nto describe an experimental protocol for evaluating LLM-generated evidence\nbriefings for secondary studies in terms of content fidelity, ease of\nunderstanding, and usefulness, as perceived by researchers and practitioners,\ncompared to human-made briefings. [Method] We developed an RAG-based LLM tool\nto generate evidence briefings. We used the tool to automatically generate two\nevidence briefings that had been manually generated in previous research\nefforts. We designed a controlled experiment to evaluate how the LLM-generated\nbriefings compare to the human-made ones regarding perceived content fidelity,\nease of understanding, and usefulness. [Results] To be reported after the\nexperimental trials. [Conclusion] Depending on the experiment results.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eRAG\u7684LLM\u5de5\u5177\uff0c\u7528\u4e8e\u81ea\u52a8\u751f\u6210\u8bc1\u636e\u7b80\u62a5\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u6bd4\u8f83\u5176\u4e0e\u4eba\u5de5\u7b80\u62a5\u5728\u5185\u5bb9\u4fdd\u771f\u5ea6\u3001\u6613\u7406\u89e3\u6027\u548c\u5b9e\u7528\u6027\u4e0a\u7684\u8868\u73b0\u3002", "motivation": "\u8bc1\u636e\u7b80\u62a5\u5bf9\u8f6f\u4ef6\u5de5\u7a0b\u5e08\u6709\u7528\uff0c\u4f46\u4eba\u5de5\u5236\u4f5c\u6210\u672c\u9ad8\uff0c\u963b\u788d\u4e86\u5176\u5e7f\u6cdb\u5e94\u7528\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u81ea\u52a8\u751f\u6210\u7b80\u62a5\u7684\u65b9\u6cd5\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "method": "\u5f00\u53d1\u4e86\u57fa\u4e8eRAG\u7684LLM\u5de5\u5177\uff0c\u751f\u6210\u8bc1\u636e\u7b80\u62a5\uff0c\u5e76\u901a\u8fc7\u5bf9\u7167\u5b9e\u9a8c\u8bc4\u4f30\u5176\u4e0e\u4eba\u5de5\u7b80\u62a5\u7684\u5dee\u5f02\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u5f85\u62a5\u544a\u3002", "conclusion": "\u7ed3\u8bba\u5c06\u6839\u636e\u5b9e\u9a8c\u7ed3\u679c\u5f97\u51fa\u3002"}}
{"id": "2507.15120", "categories": ["cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2507.15120", "abs": "https://arxiv.org/abs/2507.15120", "authors": ["Stefan Borgwardt", "Duy Nhu", "Gabriele R\u00f6ger"], "title": "Automated planning with ontologies under coherence update semantics", "comment": null, "summary": "Standard automated planning employs first-order formulas under closed-world\nsemantics to achieve a goal with a given set of actions from an initial state.\nWe follow a line of research that aims to incorporate background knowledge into\nautomated planning problems, for example, by means of ontologies, which are\nusually interpreted under open-world semantics. We present a new approach for\nplanning with DL-Lite ontologies that combines the advantages of ontology-based\naction conditions provided by explicit-input knowledge and action bases (eKABs)\nand ontology-aware action effects under the coherence update semantics. We show\nthat the complexity of the resulting formalism is not higher than that of\nprevious approaches and provide an implementation via a polynomial compilation\ninto classical planning. An evaluation of existing and new benchmarks examines\nthe performance of a planning system on different variants of our compilation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408DL-Lite\u672c\u4f53\u548c\u81ea\u52a8\u5316\u89c4\u5212\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u663e\u5f0f\u8f93\u5165\u77e5\u8bc6\u548c\u52a8\u4f5c\u57fa\u7840\uff08eKABs\uff09\u5b9e\u73b0\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u590d\u6742\u5ea6\u4e0d\u9ad8\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5c06\u80cc\u666f\u77e5\u8bc6\uff08\u5982\u672c\u4f53\uff09\u878d\u5165\u81ea\u52a8\u5316\u89c4\u5212\u95ee\u9898\uff0c\u4ee5\u63d0\u5347\u89c4\u5212\u80fd\u529b\u3002", "method": "\u7ed3\u5408DL-Lite\u672c\u4f53\u548ceKABs\uff0c\u91c7\u7528\u4e00\u81f4\u6027\u66f4\u65b0\u8bed\u4e49\u5904\u7406\u52a8\u4f5c\u6548\u679c\uff0c\u5e76\u901a\u8fc7\u591a\u9879\u5f0f\u7f16\u8bd1\u8f6c\u6362\u4e3a\u7ecf\u5178\u89c4\u5212\u95ee\u9898\u3002", "result": "\u590d\u6742\u5ea6\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u5f53\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7f16\u8bd1\u53d8\u4f53\u7684\u6027\u80fd\u3002", "conclusion": "\u65b0\u65b9\u6cd5\u5728\u4fdd\u6301\u4f4e\u590d\u6742\u5ea6\u7684\u540c\u65f6\uff0c\u6709\u6548\u6574\u5408\u4e86\u672c\u4f53\u77e5\u8bc6\uff0c\u63d0\u5347\u4e86\u89c4\u5212\u80fd\u529b\u3002"}}
{"id": "2507.15831", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.15831", "abs": "https://arxiv.org/abs/2507.15831", "authors": ["Sergey Titov", "Konstantin Grotov", "Cristina Sarasua", "Yaroslav Golubev", "Dhivyabharathi Ramasamy", "Alberto Bacchelli", "Abraham Bernstein", "Timofey Bryksin"], "title": "Observing Fine-Grained Changes in Jupyter Notebooks During Development Time", "comment": "32 pages, 6 figures", "summary": "In software engineering, numerous studies have focused on the analysis of\nfine-grained logs, leading to significant innovations in areas such as\nrefactoring, security, and code completion. However, no similar studies have\nbeen conducted for computational notebooks in the context of data science.\n  To help bridge this research gap, we make three scientific contributions: we\n(1) introduce a toolset for collecting code changes in Jupyter notebooks during\ndevelopment time; (2) use it to collect more than 100 hours of work related to\na data analysis task and a machine learning task (carried out by 20 developers\nwith different levels of expertise), resulting in a dataset containing 2,655\ncells and 9,207 cell executions; and (3) use this dataset to investigate the\ndynamic nature of the notebook development process and the changes that take\nplace in the notebooks.\n  In our analysis of the collected data, we classified the changes made to the\ncells between executions and found that a significant number of these changes\nwere relatively small fixes and code iteration modifications. This suggests\nthat notebooks are used not only as a development and exploration tool but also\nas a debugging tool. We report a number of other insights and propose potential\nfuture research directions on the novel data.", "AI": {"tldr": "\u8be5\u8bba\u6587\u586b\u8865\u4e86\u6570\u636e\u79d1\u5b66\u9886\u57df\u4e2d\u8ba1\u7b97\u7b14\u8bb0\u672c\u52a8\u6001\u5f00\u53d1\u8fc7\u7a0b\u7684\u7814\u7a76\u7a7a\u767d\uff0c\u901a\u8fc7\u5de5\u5177\u96c6\u6536\u96c6\u5f00\u53d1\u65f6\u7684\u4ee3\u7801\u53d8\u66f4\uff0c\u5206\u6790\u7b14\u8bb0\u672c\u7684\u4f7f\u7528\u6a21\u5f0f\u548c\u53d8\u5316\u7c7b\u578b\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u96c6\u4e2d\u5728\u7ec6\u7c92\u5ea6\u65e5\u5fd7\u5206\u6790\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u6570\u636e\u79d1\u5b66\u4e2d\u8ba1\u7b97\u7b14\u8bb0\u672c\u5f00\u53d1\u8fc7\u7a0b\u7684\u7814\u7a76\u3002", "method": "\u5f15\u5165\u5de5\u5177\u96c6\u6536\u96c6Jupyter\u7b14\u8bb0\u672c\u5f00\u53d1\u65f6\u7684\u4ee3\u7801\u53d8\u66f4\uff0c\u6536\u96c620\u540d\u5f00\u53d1\u8005100\u591a\u5c0f\u65f6\u7684\u5de5\u4f5c\u6570\u636e\uff0c\u5206\u6790\u7b14\u8bb0\u672c\u7684\u52a8\u6001\u5f00\u53d1\u8fc7\u7a0b\u3002", "result": "\u53d1\u73b0\u7b14\u8bb0\u672c\u4e3b\u8981\u7528\u4e8e\u5c0f\u89c4\u6a21\u4fee\u590d\u548c\u4ee3\u7801\u8fed\u4ee3\uff0c\u517c\u5177\u5f00\u53d1\u548c\u8c03\u8bd5\u529f\u80fd\u3002", "conclusion": "\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u5f3a\u8c03\u4e86\u7b14\u8bb0\u672c\u5728\u6570\u636e\u79d1\u5b66\u4e2d\u7684\u591a\u529f\u80fd\u6027\u3002"}}
{"id": "2507.15140", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15140", "abs": "https://arxiv.org/abs/2507.15140", "authors": ["Mohammad Mashayekhi", "Sara Ahmadi Majd", "Arian AmirAmjadi", "Parsa Hosseini"], "title": "Clinical Semantic Intelligence (CSI): Emulating the Cognitive Framework of the Expert Clinician for Comprehensive Oral Disease Diagnosis", "comment": null, "summary": "The diagnosis of oral diseases presents a problematic clinical challenge,\ncharacterized by a wide spectrum of pathologies with overlapping\nsymptomatology. To address this, we developed Clinical Semantic Intelligence\n(CSI), a novel artificial intelligence framework that diagnoses 118 different\noral diseases by computationally modeling the cognitive processes of an expert\nclinician. Our core hypothesis is that moving beyond simple pattern matching to\nemulate expert reasoning is critical to building clinically useful diagnostic\naids.\n  CSI's architecture integrates a fine-tuned multimodal CLIP model with a\nspecialized ChatGLM-6B language model. This system executes a Hierarchical\nDiagnostic Reasoning Tree (HDRT), a structured framework that distills the\nsystematic, multi-step logic of differential diagnosis. The framework operates\nin two modes: a Fast Mode for rapid screening and a Standard Mode that\nleverages the full HDRT for an interactive and in-depth diagnostic workup.\n  To train and validate our system, we curated a primary dataset of 4,310\nimages, supplemented by an external hold-out set of 176 images for final\nvalidation. A clinically-informed augmentation strategy expanded our training\ndata to over 30,000 image-text pairs. On a 431-image internal test set, CSI's\nFast Mode achieved an accuracy of 73.4%, which increased to 89.5% with the\nHDRT-driven Standard Mode. The performance gain is directly attributable to the\nhierarchical reasoning process. Herein, we detail the architectural philosophy,\ndevelopment, and rigorous evaluation of the CSI framework.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u79cd\u540d\u4e3aCSI\u7684\u4eba\u5de5\u667a\u80fd\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u62df\u4e13\u5bb6\u4e34\u5e8a\u533b\u751f\u7684\u8ba4\u77e5\u8fc7\u7a0b\u6765\u8bca\u65ad118\u79cd\u53e3\u8154\u75be\u75c5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8bca\u65ad\u51c6\u786e\u6027\u3002", "motivation": "\u53e3\u8154\u75be\u75c5\u8bca\u65ad\u56e0\u75c7\u72b6\u91cd\u53e0\u800c\u5177\u6709\u6311\u6218\u6027\uff0c\u9700\u8981\u8d85\u8d8a\u7b80\u5355\u6a21\u5f0f\u5339\u914d\u7684\u4e13\u5bb6\u63a8\u7406\u65b9\u6cd5\u3002", "method": "\u7ed3\u5408\u591a\u6a21\u6001CLIP\u6a21\u578b\u548cChatGLM-6B\u8bed\u8a00\u6a21\u578b\uff0c\u91c7\u7528\u5206\u5c42\u8bca\u65ad\u63a8\u7406\u6811\uff08HDRT\uff09\u8fdb\u884c\u5feb\u901f\u548c\u6807\u51c6\u6a21\u5f0f\u8bca\u65ad\u3002", "result": "\u5728431\u5f20\u56fe\u50cf\u7684\u6d4b\u8bd5\u96c6\u4e0a\uff0c\u5feb\u901f\u6a21\u5f0f\u51c6\u786e\u7387\u4e3a73.4%\uff0c\u6807\u51c6\u6a21\u5f0f\u63d0\u5347\u81f389.5%\u3002", "conclusion": "CSI\u901a\u8fc7\u5206\u5c42\u63a8\u7406\u663e\u8457\u63d0\u5347\u8bca\u65ad\u6027\u80fd\uff0c\u4e3a\u4e34\u5e8a\u8bca\u65ad\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2507.15143", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.15143", "abs": "https://arxiv.org/abs/2507.15143", "authors": ["Abderaouf Bahi", "Amel Ourici"], "title": "Can We Move Freely in NEOM's The Line? An Agent-Based Simulation of Human Mobility in a Futuristic Smart City", "comment": null, "summary": "This paper investigates the feasibility of human mobility in The Line, a\nproposed 170-kilometer linear smart city in NEOM, Saudi Arabia. To assess\nwhether citizens can move freely within this unprecedented urban topology, we\ndevelop a hybrid simulation framework that integrates agent-based modeling,\nreinforcement learning, supervised learning, and graph neural networks. The\nsimulation captures multi-modal transportation behaviors across 50 vertical\nlevels and varying density scenarios using both synthetic data and real-world\ntraces from high-density cities. Our experiments reveal that with the full\nAI-integrated architecture, agents achieved an average commute time of 7.8 to\n8.4 minutes, a satisfaction rate exceeding 89 percent, and a reachability index\nof over 91 percent, even during peak congestion periods. Ablation studies\nconfirmed that the removal of intelligent modules such as reinforcement\nlearning or graph neural networks significantly degrades performance, with\ncommute times increasing by up to 85 percent and reachability falling below 70\npercent. Environmental modeling further demonstrated low energy consumption and\nminimal CO2 emissions when electric modes are prioritized. The findings suggest\nthat freedom of movement is not only conceptually achievable in The Line, but\nalso operationally realistic if supported by adaptive AI systems, sustainable\ninfrastructure, and real-time feedback loops.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u6c99\u7279NEOM\u7ebf\u6027\u667a\u80fd\u57ce\u5e02The Line\u4e2d\u4eba\u7c7b\u79fb\u52a8\u7684\u53ef\u884c\u6027\uff0c\u901a\u8fc7\u6df7\u5408\u4eff\u771f\u6846\u67b6\u9a8c\u8bc1\u4e86AI\u652f\u6301\u7684\u81ea\u7531\u79fb\u52a8\u53ef\u80fd\u6027\u3002", "motivation": "\u8bc4\u4f30\u5728The Line\u8fd9\u79cd\u524d\u6240\u672a\u6709\u7684\u7ebf\u6027\u57ce\u5e02\u62d3\u6251\u4e2d\uff0c\u5c45\u6c11\u662f\u5426\u80fd\u81ea\u7531\u79fb\u52a8\u3002", "method": "\u5f00\u53d1\u4e86\u7ed3\u5408\u57fa\u4e8e\u4ee3\u7406\u7684\u5efa\u6a21\u3001\u5f3a\u5316\u5b66\u4e60\u3001\u76d1\u7763\u5b66\u4e60\u548c\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u6df7\u5408\u4eff\u771f\u6846\u67b6\uff0c\u6a21\u62df\u591a\u6a21\u5f0f\u4ea4\u901a\u884c\u4e3a\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cAI\u96c6\u6210\u67b6\u6784\u4e0b\uff0c\u5e73\u5747\u901a\u52e4\u65f6\u95f4\u4e3a7.8\u81f38.4\u5206\u949f\uff0c\u6ee1\u610f\u5ea6\u8d8589%\uff0c\u53ef\u8fbe\u6027\u6307\u6570\u8d8591%\u3002", "conclusion": "The Line\u4e2d\u7684\u81ea\u7531\u79fb\u52a8\u5728AI\u7cfb\u7edf\u3001\u53ef\u6301\u7eed\u57fa\u7840\u8bbe\u65bd\u548c\u5b9e\u65f6\u53cd\u9988\u652f\u6301\u4e0b\u662f\u53ef\u884c\u7684\u3002"}}
{"id": "2507.15225", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.15225", "abs": "https://arxiv.org/abs/2507.15225", "authors": ["Yichi Zhou", "Jianqiu Zhao", "Yongxin Zhang", "Bohan Wang", "Siran Wang", "Luoxin Chen", "Jiahui Wang", "Haowei Chen", "Allan Jie", "Xinbo Zhang", "Haocheng Wang", "Luong Trung", "Rong Ye", "Phan Nhat Hoang", "Huishuai Zhang", "Peng Sun", "Hang Li"], "title": "Solving Formal Math Problems by Decomposition and Iterative Reflection", "comment": null, "summary": "General-purpose Large Language Models (LLMs) have achieved remarkable success\nin intelligence, performing comparably to human experts on complex reasoning\ntasks such as coding and mathematical reasoning. However, generating formal\nproofs in specialized languages like Lean 4 remains a significant challenge for\nthese models, limiting their application in complex theorem proving and\nautomated verification. Current approaches typically require specializing\nmodels through fine-tuning on dedicated formal corpora, incurring high costs\nfor data collection and training. In this work, we introduce \\textbf{Delta\nProver}, an agent-based framework that orchestrates the interaction between a\ngeneral-purpose LLM and the Lean 4 proof environment. Delta Prover leverages\nthe reflection and reasoning capabilities of general-purpose LLMs to\ninteractively construct formal proofs in Lean 4, circumventing the need for\nmodel specialization. At its core, the agent integrates two novel,\ninterdependent components: an algorithmic framework for reflective\ndecomposition and iterative proof repair, and a custom Domain-Specific Language\n(DSL) built upon Lean 4 for streamlined subproblem management. \\textbf{Delta\nProver achieves a state-of-the-art 95.9\\% success rate on the miniF2F-test\nbenchmark, surpassing all existing approaches, including those requiring model\nspecialization.} Furthermore, Delta Prover exhibits a significantly stronger\ntest-time scaling law compared to standard Best-of-N proof strategies.\nCrucially, our findings demonstrate that general-purpose LLMs, when guided by\nan effective agentic structure, possess substantial untapped theorem-proving\ncapabilities. This presents a computationally efficient alternative to\nspecialized models for robust automated reasoning in formal environments.", "AI": {"tldr": "Delta Prover\u662f\u4e00\u4e2a\u57fa\u4e8e\u4ee3\u7406\u7684\u6846\u67b6\uff0c\u5229\u7528\u901a\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e0eLean 4\u8bc1\u660e\u73af\u5883\u4ea4\u4e92\uff0c\u65e0\u9700\u6a21\u578b\u4e13\u4e1a\u5316\u5373\u53ef\u9ad8\u6548\u751f\u6210\u5f62\u5f0f\u5316\u8bc1\u660e\u3002", "motivation": "\u901a\u7528LLM\u5728\u5f62\u5f0f\u5316\u8bc1\u660e\uff08\u5982Lean 4\uff09\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u73b0\u6709\u65b9\u6cd5\u9700\u9ad8\u6210\u672c\u7684\u4e13\u4e1a\u5316\u6a21\u578b\u3002Delta Prover\u65e8\u5728\u901a\u8fc7\u4ee3\u7406\u6846\u67b6\u91ca\u653e\u901a\u7528LLM\u7684\u6f5c\u529b\u3002", "method": "\u7ed3\u5408\u53cd\u5c04\u5206\u89e3\u3001\u8fed\u4ee3\u8bc1\u660e\u4fee\u590d\u7b97\u6cd5\u548c\u57fa\u4e8eLean 4\u7684\u9886\u57df\u7279\u5b9a\u8bed\u8a00\uff08DSL\uff09\uff0c\u5b9e\u73b0\u4ea4\u4e92\u5f0f\u8bc1\u660e\u6784\u5efa\u3002", "result": "\u5728miniF2F-test\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523095.9%\u7684\u6210\u529f\u7387\uff0c\u8d85\u8d8a\u6240\u6709\u73b0\u6709\u65b9\u6cd5\uff0c\u5305\u62ec\u9700\u6a21\u578b\u4e13\u4e1a\u5316\u7684\u65b9\u6cd5\u3002", "conclusion": "\u901a\u7528LLM\u5728\u6709\u6548\u4ee3\u7406\u7ed3\u6784\u5f15\u5bfc\u4e0b\u5177\u5907\u5f3a\u5927\u5b9a\u7406\u8bc1\u660e\u80fd\u529b\uff0c\u4e3a\u5f62\u5f0f\u5316\u73af\u5883\u4e2d\u7684\u81ea\u52a8\u63a8\u7406\u63d0\u4f9b\u9ad8\u6548\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2507.15239", "categories": ["cs.AI", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.15239", "abs": "https://arxiv.org/abs/2507.15239", "authors": ["Qianchao Wang", "Yuxuan Ding", "Chuanzhen Jia", "Zhe Li", "Yaping Du"], "title": "Explainable Artificial Intelligence based Soft Evaluation Indicator for Arc Fault Diagnosis", "comment": null, "summary": "Novel AI-based arc fault diagnosis models have demonstrated outstanding\nperformance in terms of classification accuracy. However, an inherent problem\nis whether these models can actually be trusted to find arc faults. In this\nlight, this work proposes a soft evaluation indicator that explains the outputs\nof arc fault diagnosis models, by defining the the correct explanation of arc\nfaults and leveraging Explainable Artificial Intelligence and real arc fault\nexperiments. Meanwhile, a lightweight balanced neural network is proposed to\nguarantee competitive accuracy and soft feature extraction score. In our\nexperiments, several traditional machine learning methods and deep learning\nmethods across two arc fault datasets with different sample times and noise\nlevels are utilized to test the effectiveness of the soft evaluation indicator.\nThrough this approach, the arc fault diagnosis models are easy to understand\nand trust, allowing practitioners to make informed and trustworthy decisions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f6f\u8bc4\u4f30\u6307\u6807\u548c\u8f7b\u91cf\u7ea7\u5e73\u8861\u795e\u7ecf\u7f51\u7edc\uff0c\u4ee5\u63d0\u9ad8\u7535\u5f27\u6545\u969c\u8bca\u65ad\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709AI\u7535\u5f27\u6545\u969c\u8bca\u65ad\u6a21\u578b\u867d\u51c6\u786e\u7387\u9ad8\uff0c\u4f46\u5176\u53ef\u4fe1\u5ea6\u5b58\u7591\uff0c\u9700\u589e\u5f3a\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u4fe1\u4efb\u5ea6\u3002", "method": "\u7ed3\u5408\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\u548c\u771f\u5b9e\u7535\u5f27\u6545\u969c\u5b9e\u9a8c\uff0c\u5b9a\u4e49\u7535\u5f27\u6545\u969c\u7684\u6b63\u786e\u89e3\u91ca\uff0c\u5e76\u63d0\u51fa\u8f7b\u91cf\u7ea7\u5e73\u8861\u795e\u7ecf\u7f51\u7edc\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8f6f\u8bc4\u4f30\u6307\u6807\u548c\u8f7b\u91cf\u7ea7\u7f51\u7edc\u5728\u591a\u79cd\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u7684\u53ef\u7406\u89e3\u6027\u548c\u53ef\u4fe1\u5ea6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4f7f\u7535\u5f27\u6545\u969c\u8bca\u65ad\u6a21\u578b\u66f4\u6613\u7406\u89e3\u548c\u4fe1\u4efb\uff0c\u652f\u6301\u4ece\u4e1a\u8005\u505a\u51fa\u53ef\u9760\u51b3\u7b56\u3002"}}
{"id": "2507.15253", "categories": ["cs.AI", "cs.LG", "cs.SI"], "pdf": "https://arxiv.org/pdf/2507.15253", "abs": "https://arxiv.org/abs/2507.15253", "authors": ["Zhaochen Guo", "Zhixiang Shen", "Xuanting Xie", "Liangjian Wen", "Zhao Kang"], "title": "Disentangling Homophily and Heterophily in Multimodal Graph Clustering", "comment": "Appear in ACM Multimedia 2025", "summary": "Multimodal graphs, which integrate unstructured heterogeneous data with\nstructured interconnections, offer substantial real-world utility but remain\ninsufficiently explored in unsupervised learning. In this work, we initiate the\nstudy of multimodal graph clustering, aiming to bridge this critical gap.\nThrough empirical analysis, we observe that real-world multimodal graphs often\nexhibit hybrid neighborhood patterns, combining both homophilic and\nheterophilic relationships. To address this challenge, we propose a novel\nframework -- \\textsc{Disentangled Multimodal Graph Clustering (DMGC)} -- which\ndecomposes the original hybrid graph into two complementary views: (1) a\nhomophily-enhanced graph that captures cross-modal class consistency, and (2)\nheterophily-aware graphs that preserve modality-specific inter-class\ndistinctions. We introduce a \\emph{Multimodal Dual-frequency Fusion} mechanism\nthat jointly filters these disentangled graphs through a dual-pass strategy,\nenabling effective multimodal integration while mitigating category confusion.\nOur self-supervised alignment objectives further guide the learning process\nwithout requiring labels. Extensive experiments on both multimodal and\nmulti-relational graph datasets demonstrate that DMGC achieves state-of-the-art\nperformance, highlighting its effectiveness and generalizability across diverse\nsettings. Our code is available at https://github.com/Uncnbb/DMGC.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDMGC\u7684\u65b0\u578b\u6846\u67b6\uff0c\u7528\u4e8e\u591a\u6a21\u6001\u56fe\u7684\u65e0\u76d1\u7763\u805a\u7c7b\uff0c\u901a\u8fc7\u5206\u89e3\u56fe\u7ed3\u6784\u5e76\u5f15\u5165\u53cc\u9891\u878d\u5408\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u805a\u7c7b\u6027\u80fd\u3002", "motivation": "\u591a\u6a21\u6001\u56fe\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u6709\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5176\u5728\u65e0\u76d1\u7763\u5b66\u4e60\u4e2d\u7684\u7814\u7a76\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u5982\u4f55\u5904\u7406\u6df7\u5408\u7684\u540c\u8d28\u6027\u548c\u5f02\u8d28\u6027\u5173\u7cfb\u3002", "method": "DMGC\u5c06\u539f\u59cb\u56fe\u5206\u89e3\u4e3a\u540c\u8d28\u6027\u589e\u5f3a\u56fe\u548c\u5f02\u8d28\u6027\u611f\u77e5\u56fe\uff0c\u5e76\u901a\u8fc7\u53cc\u9891\u878d\u5408\u673a\u5236\u8054\u5408\u4f18\u5316\uff0c\u540c\u65f6\u91c7\u7528\u81ea\u76d1\u7763\u5bf9\u9f50\u76ee\u6807\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDMGC\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "DMGC\u4e3a\u591a\u6a21\u6001\u56fe\u805a\u7c7b\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65e0\u76d1\u7763\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.15268", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.15268", "abs": "https://arxiv.org/abs/2507.15268", "authors": ["Junhyeong Lee", "Joon-Young Kim", "Heekyu Kim", "Inhyo Lee", "Seunghwa Ryu"], "title": "IM-Chat: A Multi-agent LLM-based Framework for Knowledge Transfer in Injection Molding Industry", "comment": null, "summary": "The injection molding industry faces critical challenges in preserving and\ntransferring field knowledge, particularly as experienced workers retire and\nmultilingual barriers hinder effective communication. This study introduces\nIM-Chat, a multi-agent framework based on large language models (LLMs),\ndesigned to facilitate knowledge transfer in injection molding. IM-Chat\nintegrates both limited documented knowledge (e.g., troubleshooting tables,\nmanuals) and extensive field data modeled through a data-driven process\ncondition generator that infers optimal manufacturing settings from\nenvironmental inputs such as temperature and humidity, enabling robust and\ncontext-aware task resolution. By adopting a retrieval-augmented generation\n(RAG) strategy and tool-calling agents within a modular architecture, IM-Chat\nensures adaptability without the need for fine-tuning. Performance was assessed\nacross 100 single-tool and 60 hybrid tasks for GPT-4o, GPT-4o-mini, and\nGPT-3.5-turbo by domain experts using a 10-point rubric focused on relevance\nand correctness, and was further supplemented by automated evaluation using\nGPT-4o guided by a domain-adapted instruction prompt. The evaluation results\nindicate that more capable models tend to achieve higher accuracy, particularly\nin complex, tool-integrated scenarios. Overall, these findings demonstrate the\nviability of multi-agent LLM systems for industrial knowledge workflows and\nestablish IM-Chat as a scalable and generalizable approach to AI-assisted\ndecision support in manufacturing.", "AI": {"tldr": "IM-Chat\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u4ee3\u7406\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u6ce8\u5851\u884c\u4e1a\u77e5\u8bc6\u8f6c\u79fb\u7684\u6311\u6218\uff0c\u7ed3\u5408\u6587\u6863\u77e5\u8bc6\u548c\u73b0\u573a\u6570\u636e\uff0c\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u548c\u5de5\u5177\u8c03\u7528\u4ee3\u7406\u5b9e\u73b0\u9002\u5e94\u6027\u3002", "motivation": "\u6ce8\u5851\u884c\u4e1a\u9762\u4e34\u7ecf\u9a8c\u5de5\u4eba\u9000\u4f11\u548c\u591a\u8bed\u8a00\u6c9f\u901a\u969c\u788d\uff0c\u5bfc\u81f4\u77e5\u8bc6\u8f6c\u79fb\u56f0\u96be\u3002", "method": "IM-Chat\u7ed3\u5408\u6587\u6863\u77e5\u8bc6\u548c\u6570\u636e\u9a71\u52a8\u7684\u8fc7\u7a0b\u6761\u4ef6\u751f\u6210\u5668\uff0c\u91c7\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u548c\u5de5\u5177\u8c03\u7528\u4ee3\u7406\u7684\u6a21\u5757\u5316\u67b6\u6784\u3002", "result": "\u8bc4\u4f30\u663e\u793a\uff0c\u66f4\u5f3a\u5927\u7684\u6a21\u578b\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u597d\uff0cIM-Chat\u5728\u5de5\u4e1a\u77e5\u8bc6\u5de5\u4f5c\u6d41\u4e2d\u5177\u6709\u53ef\u884c\u6027\u3002", "conclusion": "IM-Chat\u4e3a\u5236\u9020\u4e1aAI\u8f85\u52a9\u51b3\u7b56\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u548c\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.15330", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15330", "abs": "https://arxiv.org/abs/2507.15330", "authors": ["Hammad Atta", "Muhammad Zeeshan Baig", "Yasir Mehmood", "Nadeem Shahzad", "Ken Huang", "Muhammad Aziz Ul Haq", "Muhammad Awais", "Kamal Ahmed"], "title": "QSAF: A Novel Mitigation Framework for Cognitive Degradation in Agentic AI", "comment": null, "summary": "We introduce Cognitive Degradation as a novel vulnerability class in agentic\nAI systems. Unlike traditional adversarial external threats such as prompt\ninjection, these failures originate internally, arising from memory starvation,\nplanner recursion, context flooding, and output suppression. These systemic\nweaknesses lead to silent agent drift, logic collapse, and persistent\nhallucinations over time. To address this class of failures, we introduce the\nQorvex Security AI Framework for Behavioral & Cognitive Resilience (QSAF Domain\n10), a lifecycle-aware defense framework defined by a six-stage cognitive\ndegradation lifecycle. The framework includes seven runtime controls\n(QSAF-BC-001 to BC-007) that monitor agent subsystems in real time and trigger\nproactive mitigation through fallback routing, starvation detection, and memory\nintegrity enforcement. Drawing from cognitive neuroscience, we map agentic\narchitectures to human analogs, enabling early detection of fatigue,\nstarvation, and role collapse. By introducing a formal lifecycle and real-time\nmitigation controls, this work establishes Cognitive Degradation as a critical\nnew class of AI system vulnerability and proposes the first cross-platform\ndefense model for resilient agentic behavior.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684AI\u7cfb\u7edf\u6f0f\u6d1e\u7c7b\u522b\u2014\u2014\u8ba4\u77e5\u9000\u5316\uff0c\u5e76\u63d0\u51fa\u4e86Qorvex\u5b89\u5168AI\u6846\u67b6\uff08QSAF Domain 10\uff09\u6765\u5e94\u5bf9\u6b64\u7c7b\u95ee\u9898\uff0c\u901a\u8fc7\u5b9e\u65f6\u76d1\u63a7\u548c\u7f13\u89e3\u63aa\u65bd\u63d0\u5347AI\u7cfb\u7edf\u7684\u884c\u4e3a\u4e0e\u8ba4\u77e5\u97e7\u6027\u3002", "motivation": "\u4f20\u7edf\u7684\u5916\u90e8\u5a01\u80c1\uff08\u5982\u63d0\u793a\u6ce8\u5165\uff09\u5df2\u88ab\u5e7f\u6cdb\u7814\u7a76\uff0c\u4f46AI\u7cfb\u7edf\u5185\u90e8\u7684\u8ba4\u77e5\u9000\u5316\u95ee\u9898\uff08\u5982\u5185\u5b58\u4e0d\u8db3\u3001\u89c4\u5212\u9012\u5f52\u7b49\uff09\u5c1a\u672a\u5f97\u5230\u5145\u5206\u5173\u6ce8\uff0c\u53ef\u80fd\u5bfc\u81f4\u903b\u8f91\u5d29\u6e83\u548c\u5e7b\u89c9\u3002", "method": "\u63d0\u51fa\u4e86QSAF Domain 10\u6846\u67b6\uff0c\u5305\u542b\u516d\u9636\u6bb5\u8ba4\u77e5\u9000\u5316\u751f\u547d\u5468\u671f\u548c\u4e03\u9879\u5b9e\u65f6\u63a7\u5236\u63aa\u65bd\uff08\u5982\u56de\u9000\u8def\u7531\u3001\u9965\u997f\u68c0\u6d4b\u7b49\uff09\uff0c\u5e76\u501f\u9274\u8ba4\u77e5\u795e\u7ecf\u79d1\u5b66\u5c06AI\u67b6\u6784\u6620\u5c04\u5230\u4eba\u7c7b\u8ba4\u77e5\u6a21\u578b\u3002", "result": "\u8be5\u6846\u67b6\u80fd\u591f\u5b9e\u65f6\u76d1\u63a7AI\u5b50\u7cfb\u7edf\uff0c\u5e76\u901a\u8fc7\u4e3b\u52a8\u7f13\u89e3\u63aa\u65bd\uff08\u5982\u5185\u5b58\u5b8c\u6574\u6027\u5f3a\u5236\uff09\u9632\u6b62\u8ba4\u77e5\u9000\u5316\uff0c\u63d0\u5347\u7cfb\u7edf\u97e7\u6027\u3002", "conclusion": "\u7814\u7a76\u9996\u6b21\u5c06\u8ba4\u77e5\u9000\u5316\u786e\u7acb\u4e3aAI\u7cfb\u7edf\u7684\u65b0\u6f0f\u6d1e\u7c7b\u522b\uff0c\u5e76\u63d0\u51fa\u4e86\u8de8\u5e73\u53f0\u7684\u9632\u5fa1\u6a21\u578b\uff0c\u4e3aAI\u7cfb\u7edf\u7684\u884c\u4e3a\u97e7\u6027\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.15351", "categories": ["cs.AI", "cs.ET", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.15351", "abs": "https://arxiv.org/abs/2507.15351", "authors": ["Zijian Zhao", "Sen Li"], "title": "One Step is Enough: Multi-Agent Reinforcement Learning based on One-Step Policy Optimization for Order Dispatch on Ride-Sharing Platforms", "comment": null, "summary": "On-demand ride-sharing platforms face the fundamental challenge of\ndynamically bundling passengers with diverse origins and destinations and\nmatching them with vehicles in real time, all under significant uncertainty.\nRecently, MARL has emerged as a promising solution for this problem, leveraging\ndecentralized learning to address the curse of dimensionality caused by the\nlarge number of agents in the ride-hailing market and the resulting expansive\nstate and action spaces. However, conventional MARL-based ride-sharing\napproaches heavily rely on the accurate estimation of Q-values or V-values,\nwhich becomes problematic in large-scale, highly uncertain environments.\nSpecifically, most of these approaches adopt an independent paradigm,\nexacerbating this issue, as each agent treats others as part of the\nenvironment, leading to unstable training and substantial estimation bias in\nvalue functions. To address these challenges, we propose two novel alternative\nmethods that bypass value function estimation. First, we adapt GRPO to\nride-sharing, replacing the PPO baseline with the group average reward to\neliminate critic estimation errors and reduce training bias. Second, inspired\nby GRPO's full utilization of group reward information, we customize the PPO\nframework for ride-sharing platforms and show that, under a homogeneous fleet,\nthe optimal policy can be trained using only one-step rewards - a method we\nterm One-Step Policy Optimization (OSPO). Experiments on a real-world Manhattan\nride-hailing dataset demonstrate that both GRPO and OSPO achieve superior\nperformance across most scenarios, efficiently optimizing pickup times and the\nnumber of served orders using simple MLP networks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e24\u79cd\u65b0\u65b9\u6cd5\uff08GRPO\u548cOSPO\uff09\u89e3\u51b3\u52a8\u6001\u62fc\u8f66\u5339\u914d\u95ee\u9898\uff0c\u907f\u514d\u4f20\u7edfMARL\u4f9d\u8d56\u51c6\u786e\u503c\u51fd\u6570\u4f30\u8ba1\u7684\u95ee\u9898\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u6027\u80fd\u4f18\u8d8a\u3002", "motivation": "\u52a8\u6001\u62fc\u8f66\u5e73\u53f0\u9762\u4e34\u5b9e\u65f6\u5339\u914d\u4e58\u5ba2\u4e0e\u8f66\u8f86\u7684\u9ad8\u7ef4\u5ea6\u548c\u4e0d\u786e\u5b9a\u6027\u6311\u6218\uff0c\u4f20\u7edfMARL\u65b9\u6cd5\u56e0\u4f9d\u8d56\u503c\u51fd\u6570\u4f30\u8ba1\u800c\u8868\u73b0\u4e0d\u4f73\u3002", "method": "1. \u5c06GRPO\u5e94\u7528\u4e8e\u62fc\u8f66\uff0c\u7528\u7ec4\u5e73\u5747\u5956\u52b1\u66ff\u4ee3PPO\u57fa\u7ebf\u4ee5\u51cf\u5c11\u4f30\u8ba1\u8bef\u5dee\uff1b2. \u63d0\u51faOSPO\uff0c\u4ec5\u7528\u4e00\u6b65\u5956\u52b1\u8bad\u7ec3\u6700\u4f18\u7b56\u7565\u3002", "result": "\u5728\u771f\u5b9e\u66fc\u54c8\u987f\u62fc\u8f66\u6570\u636e\u96c6\u4e0a\uff0cGRPO\u548cOSPO\u5728\u5927\u591a\u6570\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u8d8a\uff0c\u4f18\u5316\u4e86\u63a5\u8f7d\u65f6\u95f4\u548c\u8ba2\u5355\u5b8c\u6210\u91cf\u3002", "conclusion": "GRPO\u548cOSPO\u901a\u8fc7\u907f\u514d\u503c\u51fd\u6570\u4f30\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u52a8\u6001\u62fc\u8f66\u5339\u914d\u7684\u6027\u80fd\u548c\u7a33\u5b9a\u6027\u3002"}}
{"id": "2507.15356", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15356", "abs": "https://arxiv.org/abs/2507.15356", "authors": ["Lu Guo", "Yixiang Shan", "Zhengbang Zhu", "Qifan Liang", "Lichang Song", "Ting Long", "Weinan Zhang", "Yi Chang"], "title": "RAD: Retrieval High-quality Demonstrations to Enhance Decision-making", "comment": null, "summary": "Offline reinforcement learning (RL) enables agents to learn policies from\nfixed datasets, avoiding costly or unsafe environment interactions. However,\nits effectiveness is often limited by dataset sparsity and the lack of\ntransition overlap between suboptimal and expert trajectories, which makes\nlong-horizon planning particularly challenging. Prior solutions based on\nsynthetic data augmentation or trajectory stitching often fail to generalize to\nnovel states and rely on heuristic stitching points. To address these\nchallenges, we propose Retrieval High-quAlity Demonstrations (RAD) for\ndecision-making, which combines non-parametric retrieval with diffusion-based\ngenerative modeling. RAD dynamically retrieves high-return states from the\noffline dataset as target states based on state similarity and return\nestimation, and plans toward them using a condition-guided diffusion model.\nSuch retrieval-guided generation enables flexible trajectory stitching and\nimproves generalization when encountered with underrepresented or\nout-of-distribution states. Extensive experiments confirm that RAD achieves\ncompetitive or superior performance compared to baselines across diverse\nbenchmarks, validating its effectiveness.", "AI": {"tldr": "RAD\u7ed3\u5408\u68c0\u7d22\u4e0e\u6269\u6563\u6a21\u578b\uff0c\u63d0\u5347\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u7684\u957f\u65f6\u89c4\u5212\u80fd\u529b\u3002", "motivation": "\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u56e0\u6570\u636e\u96c6\u7a00\u758f\u6027\u548c\u8f68\u8ff9\u95f4\u8fc7\u6e21\u91cd\u53e0\u4e0d\u8db3\u800c\u53d7\u9650\uff0c\u4f20\u7edf\u65b9\u6cd5\u6cdb\u5316\u80fd\u529b\u5dee\u3002", "method": "\u63d0\u51faRAD\uff0c\u901a\u8fc7\u975e\u53c2\u6570\u68c0\u7d22\u548c\u6269\u6563\u6a21\u578b\u52a8\u6001\u68c0\u7d22\u9ad8\u56de\u62a5\u72b6\u6001\u5e76\u89c4\u5212\u3002", "result": "\u5b9e\u9a8c\u8868\u660eRAD\u5728\u591a\u79cd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "RAD\u6709\u6548\u89e3\u51b3\u4e86\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u6cdb\u5316\u548c\u89c4\u5212\u95ee\u9898\u3002"}}
{"id": "2507.15411", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.15411", "abs": "https://arxiv.org/abs/2507.15411", "authors": ["Wissam Gherissi", "Mehdi Acheli", "Joyce El Haddad", "Daniela Grigori"], "title": "Predictive Process Monitoring Using Object-centric Graph Embeddings", "comment": "ICSOC Workshops 2024, Dec 2024, Tunis, Tunisia", "summary": "Object-centric predictive process monitoring explores and utilizes\nobject-centric event logs to enhance process predictions. The main challenge\nlies in extracting relevant information and building effective models. In this\npaper, we propose an end-to-end model that predicts future process behavior,\nfocusing on two tasks: next activity prediction and next event time. The\nproposed model employs a graph attention network to encode activities and their\nrelationships, combined with an LSTM network to handle temporal dependencies.\nEvaluated on one reallife and three synthetic event logs, the model\ndemonstrates competitive performance compared to state-of-the-art methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\u548cLSTM\u7684\u7aef\u5230\u7aef\u6a21\u578b\uff0c\u7528\u4e8e\u9884\u6d4b\u672a\u6765\u6d41\u7a0b\u884c\u4e3a\uff0c\u5305\u62ec\u4e0b\u4e00\u6d3b\u52a8\u548c\u4e0b\u4e00\u4e8b\u4ef6\u65f6\u95f4\u3002", "motivation": "\u5229\u7528\u5bf9\u8c61\u4e2d\u5fc3\u4e8b\u4ef6\u65e5\u5fd7\u63d0\u5347\u6d41\u7a0b\u9884\u6d4b\uff0c\u89e3\u51b3\u4fe1\u606f\u63d0\u53d6\u548c\u6a21\u578b\u6784\u5efa\u7684\u6311\u6218\u3002", "method": "\u7ed3\u5408\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\u7f16\u7801\u6d3b\u52a8\u5173\u7cfb\uff0cLSTM\u5904\u7406\u65f6\u95f4\u4f9d\u8d56\u3002", "result": "\u5728\u771f\u5b9e\u548c\u5408\u6210\u4e8b\u4ef6\u65e5\u5fd7\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u6a21\u578b\u5728\u9884\u6d4b\u6d41\u7a0b\u884c\u4e3a\u65b9\u9762\u5177\u6709\u7ade\u4e89\u529b\u3002"}}
{"id": "2507.15457", "categories": ["cs.AI", "I.2.8"], "pdf": "https://arxiv.org/pdf/2507.15457", "abs": "https://arxiv.org/abs/2507.15457", "authors": ["Orlenys L\u00f3pez-Pintado", "Jannis Rosenbaum", "Marlon Dumas"], "title": "Optimization of Activity Batching Policies in Business Processes", "comment": null, "summary": "In business processes, activity batching refers to packing multiple activity\ninstances for joint execution. Batching allows managers to trade off cost and\nprocessing effort against waiting time. Larger and less frequent batches may\nlower costs by reducing processing effort and amortizing fixed costs, but they\ncreate longer waiting times. In contrast, smaller and more frequent batches\nreduce waiting times but increase fixed costs and processing effort. A batching\npolicy defines how activity instances are grouped into batches and when each\nbatch is activated. This paper addresses the problem of discovering batching\npolicies that strike optimal trade-offs between waiting time, processing\neffort, and cost. The paper proposes a Pareto optimization approach that starts\nfrom a given set (possibly empty) of activity batching policies and generates\nalternative policies for each batched activity via intervention heuristics.\nEach heuristic identifies an opportunity to improve an activity's batching\npolicy with respect to a metric (waiting time, processing time, cost, or\nresource utilization) and an associated adjustment to the activity's batching\npolicy (the intervention). The impact of each intervention is evaluated via\nsimulation. The intervention heuristics are embedded in an optimization\nmeta-heuristic that triggers interventions to iteratively update the Pareto\nfront of the interventions identified so far. The paper considers three\nmeta-heuristics: hill-climbing, simulated annealing, and reinforcement\nlearning. An experimental evaluation compares the proposed approach based on\nintervention heuristics against the same (non-heuristic guided) meta-heuristics\nbaseline regarding convergence, diversity, and cycle time gain of\nPareto-optimal policies.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5e15\u7d2f\u6258\u4f18\u5316\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5e72\u9884\u542f\u53d1\u5f0f\u53d1\u73b0\u4e1a\u52a1\u8fc7\u7a0b\u4e2d\u6d3b\u52a8\u6279\u5904\u7406\u7684\u6700\u4f18\u7b56\u7565\uff0c\u5e73\u8861\u7b49\u5f85\u65f6\u95f4\u3001\u5904\u7406\u6210\u672c\u548c\u8d44\u6e90\u5229\u7528\u7387\u3002", "motivation": "\u4e1a\u52a1\u8fc7\u7a0b\u4e2d\uff0c\u6279\u5904\u7406\u7b56\u7565\u9700\u8981\u5728\u6210\u672c\u548c\u7b49\u5f85\u65f6\u95f4\u4e4b\u95f4\u627e\u5230\u5e73\u8861\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u81ea\u52a8\u53d1\u73b0\u6700\u4f18\u7b56\u7565\u7684\u80fd\u529b\u3002", "method": "\u91c7\u7528\u5e15\u7d2f\u6258\u4f18\u5316\u548c\u5e72\u9884\u542f\u53d1\u5f0f\uff0c\u901a\u8fc7\u6a21\u62df\u8bc4\u4f30\u7b56\u7565\u6539\u8fdb\uff0c\u5e76\u7ed3\u5408\u4e09\u79cd\u5143\u542f\u53d1\u5f0f\uff08\u722c\u5c71\u6cd5\u3001\u6a21\u62df\u9000\u706b\u548c\u5f3a\u5316\u5b66\u4e60\uff09\u4f18\u5316\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u57fa\u4e8e\u5e72\u9884\u542f\u53d1\u5f0f\u7684\u65b9\u6cd5\u5728\u6536\u655b\u6027\u3001\u591a\u6837\u6027\u548c\u5468\u671f\u65f6\u95f4\u589e\u76ca\u4e0a\u4f18\u4e8e\u975e\u542f\u53d1\u5f0f\u57fa\u7ebf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u53d1\u73b0\u6700\u4f18\u6279\u5904\u7406\u7b56\u7565\uff0c\u4e3a\u4e1a\u52a1\u8fc7\u7a0b\u4f18\u5316\u63d0\u4f9b\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2507.15509", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15509", "abs": "https://arxiv.org/abs/2507.15509", "authors": ["Lei Chen", "Xuanle Zhao", "Zhixiong Zeng", "Jing Huang", "Yufeng Zhong", "Lin Ma"], "title": "Chart-R1: Chain-of-Thought Supervision and Reinforcement for Advanced Chart Reasoner", "comment": "technical report", "summary": "Recently, inspired by OpenAI-o1/o3 and Deepseek-R1, the R1-Style method based\non reinforcement learning fine-tuning has received widespread attention from\nthe community. Previous R1-Style methods mainly focus on mathematical reasoning\nand code intelligence. It is of great research significance to verify their\nadvantages on more general multimodal data. Chart is an important multimodal\ndata type with rich information, which brings important research challenges in\ncomplex reasoning. In this work, we introduce Chart-R1, a chart-domain\nvision-language model with reinforcement learning fine-tuning to enable complex\nchart reasoning. To support Chart-R1, we first propose a novel programmatic\ndata synthesis technology to generate high-quality step-by-step chart reasoning\ndata covering single- and multi-subcharts, which makes up for the lack of\nreasoning data in the chart domain. Then we develop a two-stage training\nstrategy: Chart-COT with step-by-step chain-of-thought supervision, and\nChart-RFT with numerically sensitive reinforcement fine-tuning. Chart-COT aims\nto decompose complex chart reasoning tasks into fine-grained, understandable\nsubtasks through step-by-step supervision, which lays a good foundation for\nimproving the reasoning level of reinforcement learning. Chart-RFT utilize the\ntypical group relative policy optimization strategy, in which a relatively soft\nreward is adopted for numerical response to emphasize the numerical sensitivity\nin the chart domain. We conduct extensive experiments on open-source benchmarks\nand self-built chart reasoning dataset (\\emph{i.e., ChartRQA}). Experimental\nresults show that Chart-R1 has significant advantages compared to chart-domain\nmethods, even comparable to open/closed source large-scale models (\\emph{e.g.,\nGPT-4o, Claude-3.5}).", "AI": {"tldr": "Chart-R1\u662f\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u7684\u56fe\u8868\u9886\u57df\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u7528\u4e8e\u590d\u6742\u56fe\u8868\u63a8\u7406\u3002\u901a\u8fc7\u7a0b\u5e8f\u5316\u6570\u636e\u5408\u6210\u6280\u672f\u548c\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff08Chart-COT\u548cChart-RFT\uff09\uff0c\u5728\u5f00\u6e90\u57fa\u51c6\u548c\u81ea\u5efa\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u751a\u81f3\u5ab2\u7f8e\u5927\u578b\u6a21\u578b\u3002", "motivation": "\u9a8c\u8bc1R1-Style\u65b9\u6cd5\u5728\u901a\u7528\u591a\u6a21\u6001\u6570\u636e\uff08\u5982\u56fe\u8868\uff09\u4e0a\u7684\u4f18\u52bf\uff0c\u89e3\u51b3\u56fe\u8868\u9886\u57df\u63a8\u7406\u6570\u636e\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u7a0b\u5e8f\u5316\u6570\u636e\u5408\u6210\u6280\u672f\u751f\u6210\u9ad8\u8d28\u91cf\u56fe\u8868\u63a8\u7406\u6570\u636e\uff0c\u5e76\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff1aChart-COT\uff08\u9010\u6b65\u76d1\u7763\uff09\u548cChart-RFT\uff08\u6570\u503c\u654f\u611f\u7684\u5f3a\u5316\u5fae\u8c03\uff09\u3002", "result": "Chart-R1\u5728\u56fe\u8868\u9886\u57df\u65b9\u6cd5\u4e2d\u8868\u73b0\u663e\u8457\u4f18\u52bf\uff0c\u751a\u81f3\u4e0eGPT-4o\u3001Claude-3.5\u7b49\u5927\u578b\u6a21\u578b\u76f8\u5f53\u3002", "conclusion": "Chart-R1\u901a\u8fc7\u521b\u65b0\u7684\u6570\u636e\u5408\u6210\u548c\u8bad\u7ec3\u7b56\u7565\uff0c\u6210\u529f\u63d0\u5347\u4e86\u56fe\u8868\u9886\u57df\u7684\u590d\u6742\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2507.15518", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.15518", "abs": "https://arxiv.org/abs/2507.15518", "authors": ["Sizhou Chen", "Shufan Jiang", "Chi Zhang", "Xiao-Lei Zhang", "Xuelong Li"], "title": "HAMLET: Hyperadaptive Agent-based Modeling for Live Embodied Theatrics", "comment": null, "summary": "Creating an immersive and interactive theatrical experience is a long-term\ngoal in the field of interactive narrative. The emergence of large language\nmodel (LLM) is providing a new path to achieve this goal. However, existing\nLLM-based drama generation methods often result in AI agents that lack\ninitiative and cannot interact with the physical environment. Furthermore,\nthese methods typically require detailed user input to drive the drama. These\nlimitations reduce the interactivity and immersion of online real-time\nperformance. To address the above challenges, we propose HAMLET, a multi-agent\nframework focused on drama creation and online performance. Given a simple\ntopic, the framework generates a narrative blueprint, guiding the subsequent\nimprovisational performance. During the online performance, each actor is given\nan autonomous mind. This means that actors can make independent decisions based\non their own background, goals, and emotional state. In addition to\nconversations with other actors, their decisions can also change the state of\nscene props through actions such as opening a letter or picking up a weapon.\nThe change is then broadcast to other related actors, updating what they know\nand care about, which in turn influences their next action. To evaluate the\nquality of drama performance, we designed an evaluation method to assess three\nprimary aspects, including character performance, narrative quality, and\ninteraction experience. The experimental evaluation shows that HAMLET can\ncreate expressive and coherent theatrical experiences. Our code, dataset and\nmodels are available at https://github.com/HAMLET-2025/HAMLET.", "AI": {"tldr": "HAMLET\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u7528\u4e8e\u620f\u5267\u521b\u4f5c\u548c\u5b9e\u65f6\u8868\u6f14\uff0c\u901a\u8fc7\u81ea\u4e3b\u51b3\u7b56\u548c\u573a\u666f\u4e92\u52a8\u63d0\u5347\u6c89\u6d78\u611f\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u620f\u5267\u751f\u6210\u65b9\u6cd5\u7f3a\u4e4f\u4e3b\u52a8\u6027\u548c\u73af\u5883\u4ea4\u4e92\u80fd\u529b\uff0c\u9650\u5236\u4e86\u5b9e\u65f6\u8868\u6f14\u7684\u4e92\u52a8\u6027\u548c\u6c89\u6d78\u611f\u3002", "method": "\u63d0\u51faHAMLET\u6846\u67b6\uff0c\u751f\u6210\u53d9\u4e8b\u84dd\u56fe\u5e76\u8d4b\u4e88\u6f14\u5458\u81ea\u4e3b\u51b3\u7b56\u80fd\u529b\uff0c\u652f\u6301\u901a\u8fc7\u52a8\u4f5c\u6539\u53d8\u573a\u666f\u72b6\u6001\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0cHAMLET\u80fd\u521b\u9020\u5bcc\u6709\u8868\u73b0\u529b\u548c\u8fde\u8d2f\u6027\u7684\u620f\u5267\u4f53\u9a8c\u3002", "conclusion": "HAMLET\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u81ea\u4e3b\u51b3\u7b56\u548c\u573a\u666f\u4e92\u52a8\uff0c\u663e\u8457\u63d0\u5347\u4e86\u620f\u5267\u8868\u6f14\u7684\u4e92\u52a8\u6027\u548c\u6c89\u6d78\u611f\u3002"}}
{"id": "2507.15521", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15521", "abs": "https://arxiv.org/abs/2507.15521", "authors": ["Cole Robertson", "Philip Wolff"], "title": "LLM world models are mental: Output layer evidence of brittle world model use in LLM mechanical reasoning", "comment": "Manuscript comprises 14 pages, 4 figures, 4 tables in the Technical\n  Appendix and Supplementary Material, and is under review at NeurIPS 2025", "summary": "Do large language models (LLMs) construct and manipulate internal world\nmodels, or do they rely solely on statistical associations represented as\noutput layer token probabilities? We adapt cognitive science methodologies from\nhuman mental models research to test LLMs on pulley system problems using\nTikZ-rendered stimuli. Study 1 examines whether LLMs can estimate mechanical\nadvantage (MA). State-of-the-art models performed marginally but significantly\nabove chance, and their estimates correlated significantly with ground-truth\nMA. Significant correlations between number of pulleys and model estimates\nsuggest that models employed a pulley counting heuristic, without necessarily\nsimulating pulley systems to derive precise values. Study 2 tested this by\nprobing whether LLMs represent global features crucial to MA estimation. Models\nevaluated a functionally connected pulley system against a fake system with\nrandomly placed components. Without explicit cues, models identified the\nfunctional system as having greater MA with F1=0.8, suggesting LLMs could\nrepresent systems well enough to differentiate jumbled from functional systems.\nStudy 3 built on this by asking LLMs to compare functional systems with matched\nsystems which were connected up but which transferred no force to the weight;\nLLMs identified the functional system with F1=0.46, suggesting random guessing.\nInsofar as they may generalize, these findings are compatible with the notion\nthat LLMs manipulate internal world models, sufficient to exploit statistical\nassociations between pulley count and MA (Study 1), and to approximately\nrepresent system components' spatial relations (Study 2). However, they may\nlack the facility to reason over nuanced structural connectivity (Study 3). We\nconclude by advocating the utility of cognitive scientific methods to evaluate\nthe world-modeling capacities of artificial intelligence systems.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u662f\u5426\u6784\u5efa\u5185\u90e8\u4e16\u754c\u6a21\u578b\u6216\u4ec5\u4f9d\u8d56\u7edf\u8ba1\u5173\u8054\u3002\u901a\u8fc7\u6ed1\u8f6e\u7cfb\u7edf\u95ee\u9898\u6d4b\u8bd5\uff0c\u53d1\u73b0LLMs\u80fd\u5229\u7528\u7edf\u8ba1\u5173\u8054\uff08\u5982\u6ed1\u8f6e\u6570\u91cf\uff09\u4f30\u8ba1\u673a\u68b0\u4f18\u52bf\uff08MA\uff09\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u590d\u6742\u7ed3\u6784\u8fde\u63a5\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u63a2\u8ba8LLMs\u662f\u5426\u5177\u5907\u5185\u90e8\u4e16\u754c\u6a21\u578b\u6784\u5efa\u80fd\u529b\uff0c\u800c\u975e\u4ec5\u4f9d\u8d56\u7edf\u8ba1\u5173\u8054\u3002", "method": "\u91c7\u7528\u8ba4\u77e5\u79d1\u5b66\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e09\u4e2a\u7814\u7a76\u6d4b\u8bd5LLMs\u5728\u6ed1\u8f6e\u7cfb\u7edf\u95ee\u9898\u4e2d\u7684\u8868\u73b0\uff1a1\uff09\u4f30\u8ba1MA\uff1b2\uff09\u533a\u5206\u529f\u80fd\u6027\u4e0e\u968f\u673a\u7cfb\u7edf\uff1b3\uff09\u6bd4\u8f83\u529f\u80fd\u6027\u4e0e\u65e0\u4f5c\u7528\u7cfb\u7edf\u3002", "result": "LLMs\u80fd\u5229\u7528\u6ed1\u8f6e\u6570\u91cf\u4f30\u8ba1MA\uff08Study 1\uff09\uff0c\u533a\u5206\u529f\u80fd\u6027\u4e0e\u968f\u673a\u7cfb\u7edf\uff08Study 2\uff09\uff0c\u4f46\u5728\u590d\u6742\u7ed3\u6784\u8fde\u63a5\u63a8\u7406\u4e2d\u8868\u73b0\u4e0d\u4f73\uff08Study 3\uff09\u3002", "conclusion": "LLMs\u53ef\u80fd\u5177\u5907\u521d\u6b65\u4e16\u754c\u6a21\u578b\u80fd\u529b\uff0c\u4f46\u7f3a\u4e4f\u590d\u6742\u63a8\u7406\u3002\u8ba4\u77e5\u79d1\u5b66\u65b9\u6cd5\u6709\u52a9\u4e8e\u8bc4\u4f30AI\u7cfb\u7edf\u7684\u4e16\u754c\u5efa\u6a21\u80fd\u529b\u3002"}}
{"id": "2507.15532", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15532", "abs": "https://arxiv.org/abs/2507.15532", "authors": ["Kasper Engelen", "Guillermo A. P\u00e9rez", "Marnix Suilen"], "title": "Data-Efficient Safe Policy Improvement Using Parametric Structure", "comment": "Accepted at ECAI 2025", "summary": "Safe policy improvement (SPI) is an offline reinforcement learning problem in\nwhich a new policy that reliably outperforms the behavior policy with high\nconfidence needs to be computed using only a dataset and the behavior policy.\nMarkov decision processes (MDPs) are the standard formalism for modeling\nenvironments in SPI. In many applications, additional information in the form\nof parametric dependencies between distributions in the transition dynamics is\navailable. We make SPI more data-efficient by leveraging these dependencies\nthrough three contributions: (1) a parametric SPI algorithm that exploits known\ncorrelations between distributions to more accurately estimate the transition\ndynamics using the same amount of data; (2) a preprocessing technique that\nprunes redundant actions from the environment through a game-based abstraction;\nand (3) a more advanced preprocessing technique, based on satisfiability modulo\ntheory (SMT) solving, that can identify more actions to prune. Empirical\nresults and an ablation study show that our techniques increase the data\nefficiency of SPI by multiple orders of magnitude while maintaining the same\nreliability guarantees.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u5b89\u5168\u7b56\u7565\u6539\u8fdb\uff08SPI\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u8f6c\u79fb\u52a8\u6001\u4e2d\u7684\u53c2\u6570\u4f9d\u8d56\u5173\u7cfb\u3001\u6e38\u620f\u62bd\u8c61\u9884\u5904\u7406\u548cSMT\u6c42\u89e3\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6570\u636e\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u5b89\u5168\u7b56\u7565\u6539\u8fdb\u95ee\u9898\uff0c\u5229\u7528\u5df2\u77e5\u7684\u53c2\u6570\u4f9d\u8d56\u5173\u7cfb\u63d0\u5347\u6570\u636e\u6548\u7387\u3002", "method": "1. \u63d0\u51fa\u53c2\u6570\u5316SPI\u7b97\u6cd5\uff0c\u5229\u7528\u5206\u5e03\u95f4\u7684\u76f8\u5173\u6027\u66f4\u51c6\u786e\u4f30\u8ba1\u8f6c\u79fb\u52a8\u6001\uff1b2. \u4f7f\u7528\u6e38\u620f\u62bd\u8c61\u9884\u5904\u7406\u6280\u672f\u526a\u679d\u5197\u4f59\u52a8\u4f5c\uff1b3. \u57fa\u4e8eSMT\u6c42\u89e3\u7684\u8fdb\u9636\u9884\u5904\u7406\u6280\u672f\u8fdb\u4e00\u6b65\u526a\u679d\u52a8\u4f5c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8fd9\u4e9b\u6280\u672f\u5c06SPI\u7684\u6570\u636e\u6548\u7387\u63d0\u5347\u4e86\u591a\u4e2a\u6570\u91cf\u7ea7\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u76f8\u540c\u7684\u53ef\u9760\u6027\u4fdd\u8bc1\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u53c2\u6570\u4f9d\u8d56\u548c\u9884\u5904\u7406\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86SPI\u7684\u6570\u636e\u6548\u7387\uff0c\u4e3a\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.15581", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15581", "abs": "https://arxiv.org/abs/2507.15581", "authors": ["Ekaterina Goliakova", "Xavier Renard", "Marie-Jeanne Lesot", "Thibault Laugel", "Christophe Marsala", "Marcin Detyniecki"], "title": "Metric assessment protocol in the context of answer fluctuation on MCQ tasks", "comment": null, "summary": "Using multiple-choice questions (MCQs) has become a standard for assessing\nLLM capabilities efficiently. A variety of metrics can be employed for this\ntask. However, previous research has not conducted a thorough assessment of\nthem. At the same time, MCQ evaluation suffers from answer fluctuation: models\nproduce different results given slight changes in prompts. We suggest a metric\nassessment protocol in which evaluation methodologies are analyzed through\ntheir connection with fluctuation rates, as well as original performance. Our\nresults show that there is a strong link between existing metrics and the\nanswer changing, even when computed without any additional prompt variants. A\nnovel metric, worst accuracy, demonstrates the highest association on the\nprotocol.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8bc4\u4f30\u591a\u9009\u95ee\u9898\uff08MCQ\uff09\u6307\u6807\u7684\u65b0\u534f\u8bae\uff0c\u5206\u6790\u4e86\u6307\u6807\u4e0e\u7b54\u6848\u6ce2\u52a8\u7387\u7684\u5173\u7cfb\uff0c\u53d1\u73b0\u73b0\u6709\u6307\u6807\u4e0e\u7b54\u6848\u53d8\u5316\u6709\u5f3a\u5173\u8054\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6307\u6807\u201c\u6700\u5dee\u51c6\u786e\u7387\u201d\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u672a\u5bf9\u8bc4\u4f30LLM\u80fd\u529b\u7684\u591a\u9009\u95ee\u9898\u6307\u6807\u8fdb\u884c\u5168\u9762\u8bc4\u4f30\uff0c\u4e14MCQ\u8bc4\u4f30\u5b58\u5728\u7b54\u6848\u6ce2\u52a8\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6307\u6807\u8bc4\u4f30\u534f\u8bae\uff0c\u5206\u6790\u8bc4\u4f30\u65b9\u6cd5\u4e0e\u6ce2\u52a8\u7387\u53ca\u539f\u59cb\u6027\u80fd\u7684\u5173\u7cfb\u3002", "result": "\u73b0\u6709\u6307\u6807\u4e0e\u7b54\u6848\u53d8\u5316\u6709\u5f3a\u5173\u8054\uff0c\u65b0\u6307\u6807\u201c\u6700\u5dee\u51c6\u786e\u7387\u201d\u5728\u534f\u8bae\u4e2d\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "\u65b0\u534f\u8bae\u548c\u65b0\u6307\u6807\u6709\u52a9\u4e8e\u66f4\u5168\u9762\u5730\u8bc4\u4f30LLM\u5728\u591a\u9009\u95ee\u9898\u4e2d\u7684\u8868\u73b0\u3002"}}
{"id": "2507.15618", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15618", "abs": "https://arxiv.org/abs/2507.15618", "authors": ["Weiyu Ma", "Jiwen Jiang", "Haobo Fu", "Haifeng Zhang"], "title": "TacticCraft: Natural Language-Driven Tactical Adaptation for StarCraft II", "comment": null, "summary": "We present an adapter-based approach for tactical conditioning of StarCraft\nII AI agents. Current agents, while powerful, lack the ability to adapt their\nstrategies based on high-level tactical directives. Our method freezes a\npre-trained policy network (DI-Star) and attaches lightweight adapter modules\nto each action head, conditioned on a tactical tensor that encodes strategic\npreferences. By training these adapters with KL divergence constraints, we\nensure the policy maintains core competencies while exhibiting tactical\nvariations. Experimental results show our approach successfully modulates agent\nbehavior across tactical dimensions including aggression, expansion patterns,\nand technology preferences, while maintaining competitive performance. Our\nmethod enables flexible tactical control with minimal computational overhead,\noffering practical strategy customization for complex real-time strategy games.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9002\u914d\u5668\u7684\u6218\u672f\u8c03\u8282\u65b9\u6cd5\uff0c\u7528\u4e8e\u300a\u661f\u9645\u4e89\u9738II\u300bAI\u4ee3\u7406\uff0c\u4f7f\u5176\u80fd\u6839\u636e\u9ad8\u5c42\u6218\u672f\u6307\u4ee4\u8c03\u6574\u7b56\u7565\u3002", "motivation": "\u5f53\u524dAI\u4ee3\u7406\u867d\u5f3a\u5927\uff0c\u4f46\u7f3a\u4e4f\u57fa\u4e8e\u9ad8\u5c42\u6218\u672f\u6307\u4ee4\u7684\u9002\u5e94\u80fd\u529b\u3002", "method": "\u51bb\u7ed3\u9884\u8bad\u7ec3\u7b56\u7565\u7f51\u7edc\uff08DI-Star\uff09\uff0c\u4e3a\u6bcf\u4e2a\u52a8\u4f5c\u5934\u9644\u52a0\u8f7b\u91cf\u7ea7\u9002\u914d\u5668\u6a21\u5757\uff0c\u5e76\u901a\u8fc7KL\u6563\u5ea6\u7ea6\u675f\u8bad\u7ec3\u8fd9\u4e9b\u9002\u914d\u5668\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6210\u529f\u8c03\u8282\u4ee3\u7406\u884c\u4e3a\uff08\u5982\u4fb5\u7565\u6027\u3001\u6269\u5f20\u6a21\u5f0f\u548c\u6280\u672f\u504f\u597d\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u7ade\u4e89\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4ee5\u6700\u5c0f\u8ba1\u7b97\u5f00\u9500\u5b9e\u73b0\u7075\u6d3b\u6218\u672f\u63a7\u5236\uff0c\u4e3a\u590d\u6742\u5373\u65f6\u6218\u7565\u6e38\u620f\u63d0\u4f9b\u5b9e\u7528\u7b56\u7565\u5b9a\u5236\u3002"}}
{"id": "2507.15676", "categories": ["cs.AI", "cs.ET"], "pdf": "https://arxiv.org/pdf/2507.15676", "abs": "https://arxiv.org/abs/2507.15676", "authors": ["Reza Vatankhah Barenji", "Sina Khoshgoftar"], "title": "Agentic AI for autonomous anomaly management in complex systems", "comment": null, "summary": "This paper explores the potential of agentic AI in autonomously detecting and\nresponding to anomalies within complex systems, emphasizing its ability to\ntransform traditional, human-dependent anomaly management methods.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u4ee3\u7406\u578bAI\u5728\u590d\u6742\u7cfb\u7edf\u4e2d\u81ea\u4e3b\u68c0\u6d4b\u548c\u54cd\u5e94\u5f02\u5e38\u7684\u6f5c\u529b\uff0c\u5f3a\u8c03\u5176\u80fd\u6539\u53d8\u4f20\u7edf\u4f9d\u8d56\u4eba\u7c7b\u7684\u5f02\u5e38\u7ba1\u7406\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u5f02\u5e38\u7ba1\u7406\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\uff0c\u6548\u7387\u4f4e\u4e14\u6210\u672c\u9ad8\uff0c\u4ee3\u7406\u578bAI\u53ef\u63d0\u4f9b\u81ea\u4e3b\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5229\u7528\u4ee3\u7406\u578bAI\u6280\u672f\uff0c\u8bbe\u8ba1\u81ea\u4e3b\u68c0\u6d4b\u548c\u54cd\u5e94\u5f02\u5e38\u7684\u673a\u5236\u3002", "result": "\u4ee3\u7406\u578bAI\u80fd\u6709\u6548\u63d0\u5347\u5f02\u5e38\u7ba1\u7406\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "conclusion": "\u4ee3\u7406\u578bAI\u5728\u590d\u6742\u7cfb\u7edf\u5f02\u5e38\u7ba1\u7406\u4e2d\u5177\u6709\u663e\u8457\u6f5c\u529b\uff0c\u53ef\u66ff\u4ee3\u4f20\u7edf\u4eba\u5de5\u65b9\u6cd5\u3002"}}
{"id": "2507.15743", "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.15743", "abs": "https://arxiv.org/abs/2507.15743", "authors": ["Elahe Vedadi", "David Barrett", "Natalie Harris", "Ellery Wulczyn", "Shashir Reddy", "Roma Ruparel", "Mike Schaekermann", "Tim Strother", "Ryutaro Tanno", "Yash Sharma", "Jihyeon Lee", "C\u00edan Hughes", "Dylan Slack", "Anil Palepu", "Jan Freyberg", "Khaled Saab", "Valentin Li\u00e9vin", "Wei-Hung Weng", "Tao Tu", "Yun Liu", "Nenad Tomasev", "Kavita Kulkarni", "S. Sara Mahdavi", "Kelvin Guu", "Jo\u00eblle Barral", "Dale R. Webster", "James Manyika", "Avinatan Hassidim", "Katherine Chou", "Yossi Matias", "Pushmeet Kohli", "Adam Rodman", "Vivek Natarajan", "Alan Karthikesalingam", "David Stutz"], "title": "Towards physician-centered oversight of conversational diagnostic AI", "comment": null, "summary": "Recent work has demonstrated the promise of conversational AI systems for\ndiagnostic dialogue. However, real-world assurance of patient safety means that\nproviding individual diagnoses and treatment plans is considered a regulated\nactivity by licensed professionals. Furthermore, physicians commonly oversee\nother team members in such activities, including nurse practitioners (NPs) or\nphysician assistants/associates (PAs). Inspired by this, we propose a framework\nfor effective, asynchronous oversight of the Articulate Medical Intelligence\nExplorer (AMIE) AI system. We propose guardrailed-AMIE (g-AMIE), a multi-agent\nsystem that performs history taking within guardrails, abstaining from\nindividualized medical advice. Afterwards, g-AMIE conveys assessments to an\noverseeing primary care physician (PCP) in a clinician cockpit interface. The\nPCP provides oversight and retains accountability of the clinical decision.\nThis effectively decouples oversight from intake and can thus happen\nasynchronously. In a randomized, blinded virtual Objective Structured Clinical\nExamination (OSCE) of text consultations with asynchronous oversight, we\ncompared g-AMIE to NPs/PAs or a group of PCPs under the same guardrails. Across\n60 scenarios, g-AMIE outperformed both groups in performing high-quality\nintake, summarizing cases, and proposing diagnoses and management plans for the\noverseeing PCP to review. This resulted in higher quality composite decisions.\nPCP oversight of g-AMIE was also more time-efficient than standalone PCP\nconsultations in prior work. While our study does not replicate existing\nclinical practices and likely underestimates clinicians' capabilities, our\nresults demonstrate the promise of asynchronous oversight as a feasible\nparadigm for diagnostic AI systems to operate under expert human oversight for\nenhancing real-world care.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3ag-AMIE\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u533b\u7597\u8bca\u65ad\u5bf9\u8bdd\u4e2d\u5b9e\u73b0\u5f02\u6b65\u76d1\u7763\uff0c\u786e\u4fdd\u60a3\u8005\u5b89\u5168\u3002", "motivation": "\u73b0\u6709\u5bf9\u8bddAI\u7cfb\u7edf\u5728\u8bca\u65ad\u5bf9\u8bdd\u4e2d\u8868\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u4e2a\u4f53\u8bca\u65ad\u548c\u6cbb\u7597\u8ba1\u5212\u9700\u7531\u6301\u724c\u4e13\u4e1a\u4eba\u5458\u76d1\u7ba1\u3002\u53d7\u6b64\u542f\u53d1\uff0c\u7814\u7a76\u65e8\u5728\u8bbe\u8ba1\u4e00\u79cd\u6846\u67b6\uff0c\u4f7fAI\u7cfb\u7edf\u5728\u76d1\u7763\u4e0b\u5b89\u5168\u8fd0\u884c\u3002", "method": "\u63d0\u51fag-AMIE\u7cfb\u7edf\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u5728\u62a4\u680f\u5185\u5b8c\u6210\u75c5\u53f2\u91c7\u96c6\uff0c\u907f\u514d\u63d0\u4f9b\u4e2a\u4f53\u5316\u533b\u7597\u5efa\u8bae\uff0c\u5e76\u901a\u8fc7\u4e34\u5e8a\u9a7e\u9a76\u8231\u754c\u9762\u5c06\u8bc4\u4f30\u7ed3\u679c\u4f20\u9012\u7ed9\u76d1\u7763\u533b\u751f\u3002", "result": "\u5728\u865a\u62dfOSCE\u6d4b\u8bd5\u4e2d\uff0cg-AMIE\u5728\u9ad8\u8d28\u91cf\u75c5\u53f2\u91c7\u96c6\u3001\u75c5\u4f8b\u603b\u7ed3\u53ca\u8bca\u65ad\u5efa\u8bae\u65b9\u9762\u4f18\u4e8eNPs/PAs\u548cPCPs\u7ec4\uff0c\u4e14\u76d1\u7763\u6548\u7387\u66f4\u9ad8\u3002", "conclusion": "\u5f02\u6b65\u76d1\u7763\u662f\u4e00\u79cd\u53ef\u884c\u7684\u6a21\u5f0f\uff0c\u53ef\u589e\u5f3aAI\u7cfb\u7edf\u5728\u4e13\u5bb6\u76d1\u7763\u4e0b\u7684\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.15758", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.15758", "abs": "https://arxiv.org/abs/2507.15758", "authors": ["Xingyu Wu", "Yuchen Yan", "Shangke Lyu", "Linjuan Wu", "Yiwen Qiu", "Yongliang Shen", "Weiming Lu", "Jian Shao", "Jun Xiao", "Yueting Zhuang"], "title": "LAPO: Internalizing Reasoning Efficiency via Length-Adaptive Policy Optimization", "comment": "GitHub:https://github.com/zju-real/lapo;\n  Project:https://zju-real.github.io/lapo", "summary": "Large reasoning models have achieved remarkable performance through extended\nchain-of-thought sequences, yet this computational freedom leads to excessive\ntoken generation even for simple problems. We present Length-Adaptive Policy\nOptimization (LAPO), a novel framework that transforms reasoning length control\nfrom an external constraint into an intrinsic model capability. Unlike existing\napproaches that impose rigid limits or rely on post-hoc interventions, LAPO\nenables models to internalize an understanding of appropriate reasoning depth\nthrough a two-stage reinforcement learning process. In the first stage, models\nlearn natural reasoning patterns by discovering the statistical distribution of\nsuccessful solution lengths. The second stage leverages these patterns as\nmeta-cognitive guidance, embedding them directly within the model's reasoning\ncontext to ensure inference-time flexibility. Experiments on mathematical\nreasoning benchmarks demonstrate that LAPO reduces token usage by up to 40.9\\%\nwhile improving accuracy by 2.3\\%. Our analysis reveals that models trained\nwith LAPO develop emergent abilities to allocate computational resources based\non problem complexity, achieving efficient reasoning without sacrificing\nquality.", "AI": {"tldr": "LAPO\u6846\u67b6\u901a\u8fc7\u4e24\u9636\u6bb5\u5f3a\u5316\u5b66\u4e60\uff0c\u4f7f\u6a21\u578b\u5185\u5316\u63a8\u7406\u957f\u5ea6\u63a7\u5236\u80fd\u529b\uff0c\u51cf\u5c1140.9%\u7684token\u4f7f\u7528\u5e76\u63d0\u53472.3%\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u89e3\u51b3\u5927\u578b\u63a8\u7406\u6a21\u578b\u56e0\u81ea\u7531\u751f\u6210\u94fe\u5f0f\u601d\u7ef4\u5e8f\u5217\u5bfc\u81f4\u7684token\u6d6a\u8d39\u95ee\u9898\u3002", "method": "\u4e24\u9636\u6bb5\u5f3a\u5316\u5b66\u4e60\uff1a\u7b2c\u4e00\u9636\u6bb5\u5b66\u4e60\u6210\u529f\u89e3\u7684\u957f\u5ea6\u5206\u5e03\uff0c\u7b2c\u4e8c\u9636\u6bb5\u5c06\u5176\u4f5c\u4e3a\u5143\u8ba4\u77e5\u6307\u5bfc\u5d4c\u5165\u63a8\u7406\u4e0a\u4e0b\u6587\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0ctoken\u4f7f\u7528\u51cf\u5c1140.9%\uff0c\u51c6\u786e\u7387\u63d0\u53472.3%\u3002", "conclusion": "LAPO\u4f7f\u6a21\u578b\u80fd\u6839\u636e\u95ee\u9898\u590d\u6742\u5ea6\u52a8\u6001\u5206\u914d\u8ba1\u7b97\u8d44\u6e90\uff0c\u5b9e\u73b0\u9ad8\u6548\u4e14\u9ad8\u8d28\u91cf\u7684\u63a8\u7406\u3002"}}
{"id": "2507.15761", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15761", "abs": "https://arxiv.org/abs/2507.15761", "authors": ["Jingyi Zheng", "Zifan Peng", "Yule Liu", "Junfeng Wang", "Yifan Liao", "Wenhan Dong", "Xinlei He"], "title": "GasAgent: A Multi-Agent Framework for Automated Gas Optimization in Smart Contracts", "comment": null, "summary": "Smart contracts are trustworthy, immutable, and automatically executed\nprograms on the blockchain. Their execution requires the Gas mechanism to\nensure efficiency and fairness. However, due to non-optimal coding practices,\nmany contracts contain Gas waste patterns that need to be optimized. Existing\nsolutions mostly rely on manual discovery, which is inefficient, costly to\nmaintain, and difficult to scale. Recent research uses large language models\n(LLMs) to explore new Gas waste patterns. However, it struggles to remain\ncompatible with existing patterns, often produces redundant patterns, and\nrequires manual validation/rewriting. To address this gap, we present GasAgent,\nthe first multi-agent system for smart contract Gas optimization that combines\ncompatibility with existing patterns and automated discovery/validation of new\npatterns, enabling end-to-end optimization. GasAgent consists of four\nspecialized agents, Seeker, Innovator, Executor, and Manager, that collaborate\nin a closed loop to identify, validate, and apply Gas-saving improvements.\nExperiments on 100 verified real-world contracts demonstrate that GasAgent\nsuccessfully optimizes 82 contracts, achieving an average deployment Gas\nsavings of 9.97%. In addition, our evaluation confirms its compatibility with\nexisting tools and validates the effectiveness of each module through ablation\nstudies. To assess broader usability, we further evaluate 500 contracts\ngenerated by five representative LLMs across 10 categories and find that\nGasAgent optimizes 79.8% of them, with deployment Gas savings ranging from\n4.79% to 13.93%, showing its usability as the optimization layer for\nLLM-assisted smart contract development.", "AI": {"tldr": "GasAgent\u662f\u4e00\u4e2a\u591a\u4ee3\u7406\u7cfb\u7edf\uff0c\u7528\u4e8e\u667a\u80fd\u5408\u7ea6Gas\u4f18\u5316\uff0c\u7ed3\u5408\u73b0\u6709\u6a21\u5f0f\u7684\u517c\u5bb9\u6027\u548c\u65b0\u6a21\u5f0f\u7684\u81ea\u52a8\u53d1\u73b0/\u9a8c\u8bc1\uff0c\u5b9e\u73b0\u7aef\u5230\u7aef\u4f18\u5316\u3002", "motivation": "\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u4f9d\u8d56\u624b\u52a8\u53d1\u73b0Gas\u6d6a\u8d39\u6a21\u5f0f\uff0c\u6548\u7387\u4f4e\u4e14\u96be\u4ee5\u6269\u5c55\uff0c\u800c\u57fa\u4e8eLLM\u7684\u65b9\u6cd5\u5b58\u5728\u517c\u5bb9\u6027\u548c\u5197\u4f59\u95ee\u9898\u3002", "method": "GasAgent\u7531\u56db\u4e2a\u4e13\u4e1a\u4ee3\u7406\uff08Seeker\u3001Innovator\u3001Executor\u3001Manager\uff09\u7ec4\u6210\uff0c\u534f\u4f5c\u5b8c\u6210Gas\u4f18\u5316\u7684\u95ed\u73af\u6d41\u7a0b\u3002", "result": "\u5728100\u4e2a\u771f\u5b9e\u5408\u7ea6\u4e2d\u4f18\u5316\u4e8682\u4e2a\uff0c\u5e73\u5747\u8282\u77019.97%\u7684\u90e8\u7f72Gas\uff1b\u5728500\u4e2aLLM\u751f\u6210\u7684\u5408\u7ea6\u4e2d\u4f18\u5316\u4e8679.8%\uff0c\u8282\u77014.79%-13.93%\u7684Gas\u3002", "conclusion": "GasAgent\u5c55\u793a\u4e86\u4f5c\u4e3aLLM\u8f85\u52a9\u667a\u80fd\u5408\u7ea6\u5f00\u53d1\u7684\u4f18\u5316\u5c42\u7684\u5e7f\u6cdb\u53ef\u7528\u6027\u548c\u6709\u6548\u6027\u3002"}}
{"id": "2507.15770", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15770", "abs": "https://arxiv.org/abs/2507.15770", "authors": ["Yifan Shen", "Zihan Zhao", "Xiao Xue", "Yuwei Guo", "Qun Ma", "Deyu Zhou", "Ming Zhang"], "title": "A Framework for Analyzing Abnormal Emergence in Service Ecosystems Through LLM-based Agent Intention Mining", "comment": null, "summary": "With the rise of service computing, cloud computing, and IoT, service\necosystems are becoming increasingly complex. The intricate interactions among\nintelligent agents make abnormal emergence analysis challenging, as traditional\ncausal methods focus on individual trajectories. Large language models offer\nnew possibilities for Agent-Based Modeling (ABM) through Chain-of-Thought (CoT)\nreasoning to reveal agent intentions. However, existing approaches remain\nlimited to microscopic and static analysis. This paper introduces a framework:\nEmergence Analysis based on Multi-Agent Intention (EAMI), which enables dynamic\nand interpretable emergence analysis. EAMI first employs a dual-perspective\nthought track mechanism, where an Inspector Agent and an Analysis Agent extract\nagent intentions under bounded and perfect rationality. Then, k-means\nclustering identifies phase transition points in group intentions, followed by\na Intention Temporal Emergence diagram for dynamic analysis. The experiments\nvalidate EAMI in complex online-to-offline (O2O) service system and the\nStanford AI Town experiment, with ablation studies confirming its\neffectiveness, generalizability, and efficiency. This framework provides a\nnovel paradigm for abnormal emergence and causal analysis in service\necosystems. The code is available at\nhttps://anonymous.4open.science/r/EAMI-B085.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u610f\u56fe\u7684\u52a8\u6001\u53ef\u89e3\u91ca\u6d8c\u73b0\u5206\u6790\u6846\u67b6EAMI\uff0c\u901a\u8fc7\u53cc\u89c6\u89d2\u601d\u7ef4\u8ffd\u8e2a\u673a\u5236\u548ck-means\u805a\u7c7b\u5206\u6790\u7fa4\u4f53\u610f\u56fe\u7684\u76f8\u53d8\u70b9\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u968f\u7740\u670d\u52a1\u8ba1\u7b97\u3001\u4e91\u8ba1\u7b97\u548c\u7269\u8054\u7f51\u7684\u53d1\u5c55\uff0c\u670d\u52a1\u751f\u6001\u7cfb\u7edf\u65e5\u76ca\u590d\u6742\uff0c\u4f20\u7edf\u56e0\u679c\u65b9\u6cd5\u96be\u4ee5\u5206\u6790\u667a\u80fd\u4f53\u95f4\u7684\u5f02\u5e38\u6d8c\u73b0\u73b0\u8c61\uff0c\u9700\u8981\u65b0\u7684\u52a8\u6001\u5206\u6790\u65b9\u6cd5\u3002", "method": "EAMI\u6846\u67b6\u91c7\u7528\u53cc\u89c6\u89d2\u601d\u7ef4\u8ffd\u8e2a\u673a\u5236\uff08Inspector Agent\u548cAnalysis Agent\uff09\u63d0\u53d6\u610f\u56fe\uff0c\u7ed3\u5408k-means\u805a\u7c7b\u548c\u610f\u56fe\u65f6\u5e8f\u6d8c\u73b0\u56fe\u8fdb\u884c\u52a8\u6001\u5206\u6790\u3002", "result": "\u5b9e\u9a8c\u5728\u590d\u6742O2O\u670d\u52a1\u7cfb\u7edf\u548cStanford AI Town\u4e2d\u9a8c\u8bc1\u4e86EAMI\u7684\u6709\u6548\u6027\u3001\u901a\u7528\u6027\u548c\u6548\u7387\u3002", "conclusion": "EAMI\u4e3a\u670d\u52a1\u751f\u6001\u7cfb\u7edf\u4e2d\u7684\u5f02\u5e38\u6d8c\u73b0\u548c\u56e0\u679c\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2507.15796", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15796", "abs": "https://arxiv.org/abs/2507.15796", "authors": ["Nuria Rodr\u00edguez-Barroso", "Mario Garc\u00eda-M\u00e1rquez", "M. Victoria Luz\u00f3n", "Francisco Herrera"], "title": "Challenges of Trustworthy Federated Learning: What's Done, Current Trends and Remaining Work", "comment": null, "summary": "In recent years, the development of Trustworthy Artificial Intelligence (TAI)\nhas emerged as a critical objective in the deployment of AI systems across\nsensitive and high-risk domains. TAI frameworks articulate a comprehensive set\nof ethical, legal, and technical requirements to ensure that AI technologies\nare aligned with human values, rights, and societal expectations. Among the\nvarious AI paradigms, Federated Learning (FL) presents a promising solution to\npressing privacy concerns. However, aligning FL with the rest of the\nrequirements of TAI presents a series of challenges, most of which arise from\nits inherently distributed nature. In this work, we adopt the requirements TAI\nas a guiding structure to systematically analyze the challenges of adapting FL\nto TAI. Specifically, we classify and examine the key obstacles to aligning FL\nwith TAI, providing a detailed exploration of what has been done, the trends,\nand the remaining work within each of the identified challenges.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u5982\u4f55\u6ee1\u8db3\u53ef\u4fe1\u4eba\u5de5\u667a\u80fd\uff08TAI\uff09\u7684\u8981\u6c42\uff0c\u5206\u6790\u4e86FL\u5728\u9690\u79c1\u4fdd\u62a4\u65b9\u9762\u7684\u6f5c\u529b\u53ca\u5176\u4e0eTAI\u5176\u4ed6\u8981\u6c42\u7684\u5bf9\u9f50\u6311\u6218\u3002", "motivation": "\u968f\u7740AI\u5728\u9ad8\u98ce\u9669\u9886\u57df\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u786e\u4fdd\u5176\u53ef\u4fe1\u6027\uff08TAI\uff09\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u867d\u80fd\u89e3\u51b3\u9690\u79c1\u95ee\u9898\uff0c\u4f46\u5176\u5206\u5e03\u5f0f\u7279\u6027\u4e0eTAI\u7684\u5176\u4ed6\u8981\u6c42\u5b58\u5728\u51b2\u7a81\uff0c\u9700\u8981\u7cfb\u7edf\u5206\u6790\u3002", "method": "\u4ee5TAI\u8981\u6c42\u4e3a\u6846\u67b6\uff0c\u7cfb\u7edf\u5206\u7c7b\u5e76\u5206\u6790\u4e86FL\u4e0eTAI\u5bf9\u9f50\u7684\u4e3b\u8981\u969c\u788d\uff0c\u603b\u7ed3\u4e86\u73b0\u6709\u7814\u7a76\u3001\u8d8b\u52bf\u53ca\u672a\u89e3\u51b3\u95ee\u9898\u3002", "result": "\u8bc6\u522b\u4e86FL\u4e0eTAI\u5bf9\u9f50\u7684\u5173\u952e\u6311\u6218\uff0c\u5e76\u8be6\u7ec6\u63a2\u8ba8\u4e86\u5404\u6311\u6218\u7684\u7814\u7a76\u73b0\u72b6\u548c\u53d1\u5c55\u65b9\u5411\u3002", "conclusion": "FL\u5728\u9690\u79c1\u4fdd\u62a4\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u4ee5\u89e3\u51b3\u5176\u4e0eTAI\u5176\u4ed6\u8981\u6c42\u7684\u5bf9\u9f50\u95ee\u9898\u3002"}}
{"id": "2507.15842", "categories": ["cs.AI", "stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.15842", "abs": "https://arxiv.org/abs/2507.15842", "authors": ["Sara LaPlante", "Emilija Perkovi\u0107"], "title": "Identifying Conditional Causal Effects in MPDAGs", "comment": "67 pages, 8 figures", "summary": "We consider identifying a conditional causal effect when a graph is known up\nto a maximally oriented partially directed acyclic graph (MPDAG). An MPDAG\nrepresents an equivalence class of graphs that is restricted by background\nknowledge and where all variables in the causal model are observed. We provide\nthree results that address identification in this setting: an identification\nformula when the conditioning set is unaffected by treatment, a generalization\nof the well-known do calculus to the MPDAG setting, and an algorithm that is\ncomplete for identifying these conditional effects.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5728\u5df2\u77e5\u6700\u5927\u5b9a\u5411\u90e8\u5206\u6709\u5411\u65e0\u73af\u56fe\uff08MPDAG\uff09\u7684\u60c5\u51b5\u4e0b\uff0c\u5982\u4f55\u8bc6\u522b\u6761\u4ef6\u56e0\u679c\u6548\u5e94\u3002\u63d0\u51fa\u4e86\u4e09\u79cd\u7ed3\u679c\uff1a\u4e0d\u53d7\u6cbb\u7597\u5f71\u54cd\u7684\u8c03\u8282\u96c6\u8bc6\u522b\u516c\u5f0f\u3001MPDAG\u8bbe\u7f6e\u4e0b\u7684do calculus\u63a8\u5e7f\uff0c\u4ee5\u53ca\u4e00\u4e2a\u5b8c\u6574\u7684\u6761\u4ef6\u6548\u5e94\u8bc6\u522b\u7b97\u6cd5\u3002", "motivation": "\u7814\u7a76\u80cc\u666f\u662f\u56e0\u679c\u63a8\u65ad\u4e2d\u56fe\u6a21\u578b\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u7279\u522b\u662f\u5728MPDAG\u8868\u793a\u7684\u60c5\u51b5\u4e0b\uff0c\u5982\u4f55\u51c6\u786e\u8bc6\u522b\u6761\u4ef6\u56e0\u679c\u6548\u5e94\u3002", "method": "\u63d0\u51fa\u4e86\u4e09\u79cd\u65b9\u6cd5\uff1a1\uff09\u9488\u5bf9\u4e0d\u53d7\u6cbb\u7597\u5f71\u54cd\u7684\u8c03\u8282\u96c6\u7684\u8bc6\u522b\u516c\u5f0f\uff1b2\uff09\u5c06do calculus\u63a8\u5e7f\u5230MPDAG\u8bbe\u7f6e\uff1b3\uff09\u5f00\u53d1\u4e86\u4e00\u4e2a\u5b8c\u6574\u7684\u6761\u4ef6\u6548\u5e94\u8bc6\u522b\u7b97\u6cd5\u3002", "result": "\u6210\u529f\u5b9e\u73b0\u4e86\u5728MPDAG\u8bbe\u7f6e\u4e0b\u6761\u4ef6\u56e0\u679c\u6548\u5e94\u7684\u8bc6\u522b\uff0c\u5e76\u63d0\u4f9b\u4e86\u7406\u8bba\u548c\u7b97\u6cd5\u652f\u6301\u3002", "conclusion": "\u8bba\u6587\u4e3aMPDAG\u80cc\u666f\u4e0b\u7684\u6761\u4ef6\u56e0\u679c\u6548\u5e94\u8bc6\u522b\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6269\u5c55\u4e86\u56e0\u679c\u63a8\u65ad\u7684\u7406\u8bba\u548c\u5e94\u7528\u8303\u56f4\u3002"}}
{"id": "2507.15844", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.15844", "abs": "https://arxiv.org/abs/2507.15844", "authors": ["Shangke Lyu", "Linjuan Wu", "Yuchen Yan", "Xingyu Wu", "Hao Li", "Yongliang Shen", "Peisheng Jiang", "Weiming Lu", "Jun Xiao", "Yueting Zhuang"], "title": "Hierarchical Budget Policy Optimization for Adaptive Reasoning", "comment": "Code: https://github.com/zju-real/hbpo Project\n  Page:https://zju-real.github.io/hbpo/", "summary": "Large reasoning models achieve remarkable performance through extensive\nchain-of-thought generation, yet exhibit significant computational inefficiency\nby applying uniform reasoning strategies regardless of problem complexity. We\npresent Hierarchical Budget Policy Optimization (HBPO), a reinforcement\nlearning framework that enables models to learn problem-specific reasoning\ndepths without sacrificing capability. HBPO addresses the fundamental challenge\nof exploration space collapse in efficiency-oriented training, where penalties\non long output length systematically bias models away from necessary long\nreasoning paths. Through hierarchical budget exploration, our approach\npartitions rollout samples into multiple subgroups with distinct token budgets,\naiming to enable efficient resource allocation while preventing degradation of\ncapability. We introduce differentiated reward mechanisms that create\nbudget-aware incentives aligned with the complexity of the problem, allowing\nmodels to discover natural correspondences between task requirements and\ncomputational effort. Extensive experiments demonstrate that HBPO reduces\naverage token usage by up to 60.6% while improving accuracy by 3.14% across\nfour reasoning benchmarks. Unlike existing methods that impose external\nconstraints or rely on discrete mode selection, HBPO exhibits emergent adaptive\nbehavior where models automatically adjust reasoning depth based on problem\ncomplexity. Our results suggest that reasoning efficiency and capability are\nnot inherently conflicting, and can be simultaneously optimized through\nappropriately structured hierarchical training that preserves exploration\ndiversity.", "AI": {"tldr": "HBPO\u662f\u4e00\u79cd\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5c42\u9884\u7b97\u63a2\u7d22\u548c\u5dee\u5f02\u5316\u5956\u52b1\u673a\u5236\uff0c\u4f7f\u6a21\u578b\u80fd\u6839\u636e\u95ee\u9898\u590d\u6742\u5ea6\u81ea\u9002\u5e94\u8c03\u6574\u63a8\u7406\u6df1\u5ea6\uff0c\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u8d44\u6e90\u4f7f\u7528\u540c\u65f6\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5927\u578b\u63a8\u7406\u6a21\u578b\u5728\u94fe\u5f0f\u601d\u7ef4\u751f\u6210\u4e2d\u8ba1\u7b97\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\uff0c\u907f\u514d\u56e0\u7edf\u4e00\u63a8\u7406\u7b56\u7565\u5bfc\u81f4\u7684\u8d44\u6e90\u6d6a\u8d39\u3002", "method": "\u91c7\u7528\u5206\u5c42\u9884\u7b97\u63a2\u7d22\uff0c\u5c06\u6837\u672c\u5206\u7ec4\u5e76\u5206\u914d\u4e0d\u540ctoken\u9884\u7b97\uff0c\u7ed3\u5408\u9884\u7b97\u611f\u77e5\u7684\u5956\u52b1\u673a\u5236\uff0c\u4f18\u5316\u8d44\u6e90\u5206\u914d\u3002", "result": "\u5b9e\u9a8c\u663e\u793aHBPO\u5e73\u5747\u51cf\u5c1160.6%\u7684token\u4f7f\u7528\uff0c\u540c\u65f6\u51c6\u786e\u7387\u63d0\u53473.14%\uff0c\u4e14\u6a21\u578b\u80fd\u81ea\u9002\u5e94\u8c03\u6574\u63a8\u7406\u6df1\u5ea6\u3002", "conclusion": "\u63a8\u7406\u6548\u7387\u4e0e\u80fd\u529b\u5e76\u975e\u77db\u76fe\uff0c\u901a\u8fc7\u5206\u5c42\u8bad\u7ec3\u53ef\u540c\u65f6\u4f18\u5316\u4e24\u8005\uff0c\u4fdd\u6301\u63a2\u7d22\u591a\u6837\u6027\u3002"}}
{"id": "2507.15851", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15851", "abs": "https://arxiv.org/abs/2507.15851", "authors": ["Lingyu Li", "Yang Yao", "Yixu Wang", "Chubo Li", "Yan Teng", "Yingchun Wang"], "title": "The Other Mind: How Language Models Exhibit Human Temporal Cognition", "comment": "12 pages, 9 figures, 4 tables", "summary": "As Large Language Models (LLMs) continue to advance, they exhibit certain\ncognitive patterns similar to those of humans that are not directly specified\nin training data. This study investigates this phenomenon by focusing on\ntemporal cognition in LLMs. Leveraging the similarity judgment task, we find\nthat larger models spontaneously establish a subjective temporal reference\npoint and adhere to the Weber-Fechner law, whereby the perceived distance\nlogarithmically compresses as years recede from this reference point. To\nuncover the mechanisms behind this behavior, we conducted multiple analyses\nacross neuronal, representational, and informational levels. We first identify\na set of temporal-preferential neurons and find that this group exhibits\nminimal activation at the subjective reference point and implements a\nlogarithmic coding scheme convergently found in biological systems. Probing\nrepresentations of years reveals a hierarchical construction process, where\nyears evolve from basic numerical values in shallow layers to abstract temporal\norientation in deep layers. Finally, using pre-trained embedding models, we\nfound that the training corpus itself possesses an inherent, non-linear\ntemporal structure, which provides the raw material for the model's internal\nconstruction. In discussion, we propose an experientialist perspective for\nunderstanding these findings, where the LLMs' cognition is viewed as a\nsubjective construction of the external world by its internal representational\nsystem. This nuanced perspective implies the potential emergence of alien\ncognitive frameworks that humans cannot intuitively predict, pointing toward a\ndirection for AI alignment that focuses on guiding internal constructions. Our\ncode is available at https://TheOtherMind.github.io.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8868\u73b0\u51fa\u7c7b\u4f3c\u4eba\u7c7b\u7684\u65f6\u95f4\u8ba4\u77e5\u6a21\u5f0f\uff0c\u5305\u62ec\u4e3b\u89c2\u65f6\u95f4\u53c2\u8003\u70b9\u548cWeber-Fechner\u5b9a\u5f8b\u7684\u9075\u5faa\u3002", "motivation": "\u63a2\u8ba8LLMs\u4e2d\u672a\u5728\u8bad\u7ec3\u6570\u636e\u4e2d\u76f4\u63a5\u6307\u5b9a\u7684\u8ba4\u77e5\u6a21\u5f0f\uff0c\u7279\u522b\u662f\u65f6\u95f4\u8ba4\u77e5\u3002", "method": "\u901a\u8fc7\u76f8\u4f3c\u6027\u5224\u65ad\u4efb\u52a1\u548c\u591a\u5c42\u6b21\u5206\u6790\uff08\u795e\u7ecf\u5143\u3001\u8868\u5f81\u3001\u4fe1\u606f\u5c42\u9762\uff09\u7814\u7a76LLMs\u7684\u65f6\u95f4\u8ba4\u77e5\u673a\u5236\u3002", "result": "\u53d1\u73b0LLMs\u4e2d\u5b58\u5728\u65f6\u95f4\u504f\u597d\u795e\u7ecf\u5143\uff0c\u8868\u5f81\u5c42\u6b21\u6784\u5efa\u8fc7\u7a0b\uff0c\u4ee5\u53ca\u8bad\u7ec3\u8bed\u6599\u7684\u975e\u7ebf\u6027\u65f6\u95f4\u7ed3\u6784\u3002", "conclusion": "\u63d0\u51fa\u4f53\u9a8c\u4e3b\u4e49\u89c6\u89d2\uff0c\u8ba4\u4e3aLLMs\u7684\u8ba4\u77e5\u662f\u5185\u90e8\u8868\u5f81\u7cfb\u7edf\u5bf9\u5916\u90e8\u4e16\u754c\u7684\u4e3b\u89c2\u6784\u5efa\uff0c\u6697\u793aAI\u5bf9\u9f50\u9700\u5173\u6ce8\u5185\u90e8\u6784\u5efa\u7684\u5f15\u5bfc\u3002"}}
{"id": "2507.15855", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15855", "abs": "https://arxiv.org/abs/2507.15855", "authors": ["Yichen Huang", "Lin F. Yang"], "title": "Gemini 2.5 Pro Capable of Winning Gold at IMO 2025", "comment": null, "summary": "The International Mathematical Olympiad (IMO) poses uniquely challenging\nproblems requiring deep insight, creativity, and formal reasoning. While Large\nLanguage Models (LLMs) perform well on mathematical benchmarks like AIME, they\nstruggle with Olympiad-level tasks. We use Google's Gemini 2.5 Pro on the newly\nreleased IMO 2025 problems, avoiding data contamination. With pipeline design\nand prompt engineering, 5 (out of 6) problems are solved correctly (up to a\ncaveat discussed below), highlighting the importance of finding the optimal way\nof using powerful models.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u4f7f\u7528Google\u7684Gemini 2.5 Pro\u89e3\u51b3IMO 2025\u96be\u9898\u7684\u6548\u679c\uff0c\u901a\u8fc7\u4f18\u5316\u6d41\u7a0b\u548c\u63d0\u793a\u5de5\u7a0b\uff0c\u6210\u529f\u89e3\u51b3\u4e865/6\u7684\u95ee\u9898\u3002", "motivation": "\u7814\u7a76LLMs\u5728\u89e3\u51b3\u56fd\u9645\u6570\u5b66\u5965\u6797\u5339\u514b\uff08IMO\uff09\u96be\u9898\u65f6\u7684\u8868\u73b0\uff0c\u63a2\u7d22\u5982\u4f55\u4f18\u5316\u6a21\u578b\u4f7f\u7528\u65b9\u5f0f\u3002", "method": "\u4f7f\u7528Gemini 2.5 Pro\u6a21\u578b\uff0c\u7ed3\u5408\u7ba1\u9053\u8bbe\u8ba1\u548c\u63d0\u793a\u5de5\u7a0b\uff0c\u907f\u514d\u6570\u636e\u6c61\u67d3\u3002", "result": "\u5728IMO 2025\u76846\u9053\u9898\u4e2d\uff0c\u6210\u529f\u89e3\u51b3\u4e865\u9053\uff08\u90e8\u5206\u95ee\u9898\u5b58\u5728\u5c40\u9650\u6027\uff09\u3002", "conclusion": "\u4f18\u5316\u6a21\u578b\u4f7f\u7528\u65b9\u6cd5\u5bf9\u89e3\u51b3\u9ad8\u96be\u5ea6\u6570\u5b66\u95ee\u9898\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2411.01789", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2411.01789", "abs": "https://arxiv.org/abs/2411.01789", "authors": ["Shan Jiang", "Chenguang Zhu", "Sarfraz Khurshid"], "title": "Generating executable oracles to check conformance of client code to requirements of JDK Javadocs using LLMs", "comment": null, "summary": "Software testing remains the most widely used methodology for validating\nquality of code. However, effectiveness of testing critically depends on the\nquality of test suites used. Test cases in a test suite consist of two\nfundamental parts: (1) input values for the code under test, and (2) correct\nchecks for the outputs it produces. These checks are commonly written as\nassertions, and termed test oracles. The last couple of decades have seen much\nprogress in automated test input generation, e.g., using fuzzing and symbolic\nexecution. However, automating test oracles remains a relatively less explored\nproblem area. Indeed, a test oracle by its nature requires knowledge of\nexpected behavior, which may only be known to the developer and may not not\nexist in a formal language that supports automated reasoning.\n  Our focus in this paper is automation of test oracles for clients of widely\nused Java libraries, e.g., java.lang and java.util packages. Our key insight is\nthat Javadocs that provide a rich source of information can enable automated\ngeneration of test oracles. Javadocs of the core Java libraries are fairly\ndetailed documents that contain natural language descriptions of not only how\nthe libraries behave but also how the clients must (not) use them. We use large\nlanguage models as an enabling technology to embody our insight into a\nframework for test oracle automation, and evaluate it experimentally. Our\nexperiments demonstrate that LLMs can generate oracles for checking normal and\nexceptional behaviors from Javadocs, with 98.8% of these oracles being\ncompilable and 96.4% accurately reflecting intended properties. Even for the\nfew incorrect oracles, errors are minor and can be easily corrected with the\nhelp of additional comment information generated by the LLMs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528Javadocs\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u5316\u751f\u6210Java\u5e93\u6d4b\u8bd5\u9884\u8a00\u7684\u65b9\u6cd5\uff0c\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u9ad8\u6548\u4e14\u51c6\u786e\u3002", "motivation": "\u6d4b\u8bd5\u9884\u8a00\u81ea\u52a8\u5316\u662f\u4e00\u4e2a\u8f83\u5c11\u88ab\u63a2\u7d22\u7684\u9886\u57df\uff0c\u5c24\u5176\u662f\u5982\u4f55\u4ece\u975e\u6b63\u5f0f\u7684\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u4e2d\u63d0\u53d6\u9884\u671f\u884c\u4e3a\u3002Javadocs\u63d0\u4f9b\u4e86\u4e30\u5bcc\u7684\u5e93\u884c\u4e3a\u4fe1\u606f\uff0c\u53ef\u7528\u4e8e\u81ea\u52a8\u5316\u751f\u6210\u6d4b\u8bd5\u9884\u8a00\u3002", "method": "\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4eceJavadocs\u4e2d\u63d0\u53d6\u4fe1\u606f\uff0c\u751f\u6210\u6d4b\u8bd5\u9884\u8a00\uff0c\u5305\u62ec\u6b63\u5e38\u548c\u5f02\u5e38\u884c\u4e3a\u7684\u68c0\u67e5\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c98.8%\u7684\u9884\u8a00\u53ef\u7f16\u8bd1\uff0c96.4%\u51c6\u786e\u53cd\u6620\u9884\u671f\u5c5e\u6027\uff0c\u9519\u8bef\u53ef\u901a\u8fc7LLM\u751f\u6210\u7684\u989d\u5916\u6ce8\u91ca\u8f7b\u677e\u4fee\u6b63\u3002", "conclusion": "Javadocs\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u6d4b\u8bd5\u9884\u8a00\u81ea\u52a8\u5316\u65b9\u6cd5\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2502.15441", "categories": ["cs.SE", "cs.AI", "cs.FL", "cs.PL"], "pdf": "https://arxiv.org/pdf/2502.15441", "abs": "https://arxiv.org/abs/2502.15441", "authors": ["Yang Hong", "Shan Jiang", "Yulei Fu", "Sarfraz Khurshid"], "title": "On the Effectiveness of Large Language Models in Writing Alloy Formulas", "comment": null, "summary": "Declarative specifications have a vital role to play in developing safe and\ndependable software systems. Writing specifications correctly, however, remains\nparticularly challenging. This paper presents a controlled experiment on using\nlarge language models (LLMs) to write declarative formulas in the well-known\nlanguage Alloy. Our use of LLMs is three-fold. One, we employ LLMs to write\ncomplete Alloy formulas from given natural language descriptions (in English).\nTwo, we employ LLMs to create alternative but equivalent formulas in Alloy with\nrespect to given Alloy formulas. Three, we employ LLMs to complete sketches of\nAlloy formulas and populate the holes in the sketches by synthesizing Alloy\nexpressions and operators so that the completed formulas accurately represent\nthe desired properties (that are given in natural language). We conduct the\nexperimental evaluation using 11 well-studied subject specifications and employ\ntwo popular LLMs, namely ChatGPT and DeepSeek. The experimental results show\nthat the LLMs generally perform well in synthesizing complete Alloy formulas\nfrom input properties given in natural language or in Alloy, and are able to\nenumerate multiple unique solutions. Moreover, the LLMs are also successful at\ncompleting given sketches of Alloy formulas with respect to natural language\ndescriptions of desired properties (without requiring test cases). We believe\nLLMs offer a very exciting advance in our ability to write specifications, and\ncan help make specifications take a pivotal role in software development and\nenhance our ability to build robust software.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7f16\u5199Alloy\u58f0\u660e\u5f0f\u516c\u5f0f\u7684\u5b9e\u9a8c\uff0c\u5c55\u793a\u4e86LLMs\u5728\u751f\u6210\u3001\u8f6c\u6362\u548c\u8865\u5168Alloy\u516c\u5f0f\u65b9\u9762\u7684\u6f5c\u529b\u3002", "motivation": "\u58f0\u660e\u5f0f\u89c4\u8303\u5bf9\u5f00\u53d1\u5b89\u5168\u53ef\u9760\u7684\u8f6f\u4ef6\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u6b63\u786e\u7f16\u5199\u89c4\u8303\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\uff0c\u4f7f\u7528ChatGPT\u548cDeepSeek\u4e24\u79cdLLMs\uff0c\u4ece\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u751f\u6210Alloy\u516c\u5f0f\u3001\u751f\u6210\u7b49\u4ef7\u516c\u5f0f\uff0c\u4ee5\u53ca\u8865\u5168\u516c\u5f0f\u8349\u56fe\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cLLMs\u5728\u751f\u6210\u548c\u8865\u5168Alloy\u516c\u5f0f\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u5e76\u80fd\u63d0\u4f9b\u591a\u79cd\u72ec\u7279\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "LLMs\u4e3a\u89c4\u8303\u7f16\u5199\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\uff0c\u6709\u671b\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u53d1\u6325\u91cd\u8981\u4f5c\u7528\u3002"}}
