<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 7]
- [cs.CR](#cs.CR) [Total: 12]
- [cs.AI](#cs.AI) [Total: 42]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [EGSS: Entropy-guided Stepwise Scaling for Reliable Software Engineering](https://arxiv.org/abs/2602.05242)
*Chenhui Mao,Yuanting Lei,Zhixiang Wei,Ming Liang,Zhixiang Wang,Jingxuan Xu,Dajun Chen,Wei Jiang,Yong Li*

Main category: cs.SE

TL;DR: EGSS框架通过熵引导的自适应搜索和测试套件增强，解决了Agentic TTS在代码生成任务中的计算开销问题，在提升性能5-10%的同时减少28%的推理令牌使用。


<details>
  <summary>Details</summary>
Motivation: Agentic TTS在复杂软件工程任务中表现出色，但实际应用受限，主要因为：1）大型集成系统部署成本高；2）缺乏可靠机制选择最优候选方案，限制了性能提升。

Method: 提出Entropy-Guided Stepwise Scaling (EGSS)框架，通过熵引导的自适应搜索和鲁棒的测试套件增强，动态平衡效率与效果。

Result: 在SWE-Bench-Verified上，EGSS将Kimi-K2-Intruct的解决率从63.2%提升至72.2%，GLM-4.6从65.8%提升至74.6%。与GLM-4.6结合时在开源大模型中达到SOTA，同时减少28%推理令牌使用。

Conclusion: EGSS有效解决了TTS的计算开销问题，在保持性能提升的同时显著提高计算效率，为Agentic TTS的实际应用提供了可行方案。

Abstract: Agentic Test-Time Scaling (TTS) has delivered state-of-the-art (SOTA) performance on complex software engineering tasks such as code generation and bug fixing. However, its practical adoption remains limited due to significant computational overhead, primarily driven by two key challenges: (1) the high cost associated with deploying excessively large ensembles, and (2) the lack of a reliable mechanism for selecting the optimal candidate solution, ultimately constraining the performance gains that can be realized. To address these challenges, we propose Entropy-Guided Stepwise Scaling (EGSS), a novel TTS framework that dynamically balances efficiency and effectiveness through entropy-guided adaptive search and robust test-suite augmentation. Extensive experiments on SWE-Bench-Verified demonstrate that EGSS consistently boosts performance by 5-10% across all evaluated models. Specifically, it increases the resolved ratio of Kimi-K2-Intruct from 63.2% to 72.2%, and GLM-4.6 from 65.8% to 74.6%. Furthermore, when paired with GLM-4.6, EGSS achieves a new state-of-the-art among open-source large language models. In addition to these accuracy improvements, EGSS reduces inference-time token usage by over 28% compared to existing TTS methods, achieving simultaneous gains in both effectiveness and computational efficiency.

</details>


### [2] [Emergence-as-Code for Self-Governing Reliable Systems](https://arxiv.org/abs/2602.05458)
*Anatoly A. Krasnovsky*

Main category: cs.SE

TL;DR: EmaC框架将旅程可靠性从隐性属性变为可计算、可治理的代码化规范，通过意图声明和运行时推断来管理微服务架构中的用户体验可靠性。


<details>
  <summary>Details</summary>
Motivation: 在微服务架构中，用户体验可靠性是多个服务组件交互的涌现属性，难以通过单个服务的SLO来保证。当前旅程目标通常以非代码方式维护，容易随系统演化而偏离，导致团队要么无法满足用户期望，要么过度配置资源并依赖临时启发式方法来控制发布。

Method: 提出Emergence-as-Code (EmaC)框架：1) EmaC规范声明旅程意图（目标、控制流操作符、允许的操作）并绑定到原子SLO和遥测数据；2) 运行时推断组件消费操作工件（如追踪和流量配置）来合成带有来源和置信度的候选旅程模型；3) 从最后接受的模型中，EmaC编译器/控制器在明确的相关性假设下推导出有界的旅程SLO和预算；4) 生成控制平面工件（燃烧率警报、发布门控、操作防护），可在Git工作流中审查。

Result: 提供了一个匿名化工件仓库，包含可运行的示例规范和生成输出，展示了EmaC框架的实际应用。

Conclusion: EmaC使旅程可靠性变得可计算和可治理，通过意图加证据的方法，将用户体验可靠性从隐性涌现属性转变为可编程、可验证的代码化规范，解决了微服务架构中旅程目标漂移和过度配置的问题。

Abstract: SLO-as-code has made per-service} reliability declarative, but user experience is defined by journeys whose reliability is an emergent property of microservice topology, routing, redundancy, timeouts/fallbacks, shared failure domains, and tail amplification. As a result, journey objectives (e.g., "checkout p99 < 400 ms") are often maintained outside code and drift as the system evolves, forcing teams to either miss user expectations or over-provision and gate releases with ad-hoc heuristics. We propose Emergence-as-Code (EmaC), a vision for making journey reliability computable and governable via intent plus evidence. An EmaC spec declares journey intent (objective, control-flow operators, allowed actions) and binds it to atomic SLOs and telemetry. A runtime inference component consumes operational artifacts (e.g., tracing and traffic configuration) to synthesize a candidate journey model with provenance and confidence. From the last accepted model, the EmaC compiler/controller derives bounded journey SLOs and budgets under explicit correlation assumptions (optimistic independence vs. pessimistic shared fate), and emits control-plane artifacts (burn-rate alerts, rollout gates, action guards) that are reviewable in a Git workflow. An anonymized artifact repository provides a runnable example specification and generated outputs.

</details>


### [3] [Sovereign-by-Design A Reference Architecture for AI and Blockchain Enabled Systems](https://arxiv.org/abs/2602.05486)
*Matteo Esposito,Lodovica Marchesi,Roberto Tonelli,Valentina Lenarduzzi*

Main category: cs.SE

TL;DR: 该论文提出将数字主权作为软件架构的一等属性，而非单纯的监管目标，并引入了一个主权参考架构，整合了自主权身份、基于区块链的信任与审计、主权数据治理以及受控的生成式AI部署。


<details>
  <summary>Details</summary>
Motivation: 现代软件密集型系统面临数字主权挑战，主要源于非主权云基础设施的主导地位、生成式AI的快速采用以及日益严格的监管要求。现有解决方案在治理、合规性和安全性方面各自为政，缺乏在架构层面实现主权的具体指导。

Method: 提出主权参考架构，将主权作为核心架构属性，整合四个关键组件：自主权身份管理、基于区块链的信任与审计机制、主权数据治理框架，以及在明确架构控制下部署的生成式AI系统。

Result: 该架构将生成式AI的双重角色明确化：既是治理风险的来源，也是合规性、问责制和持续保障的赋能工具（当受到适当约束时）。通过将主权构建为架构质量属性，连接了监管意图与具体系统设计。

Conclusion: 该主权参考架构为构建可审计、可演进且具备司法管辖意识的AI赋能系统提供了连贯基础，为软件架构、生成式AI和数字主权交叉领域的未来研究和实践提供了原则性起点。

Abstract: Digital sovereignty has emerged as a central concern for modern software-intensive systems, driven by the dominance of non-sovereign cloud infrastructures, the rapid adoption of Generative AI, and increasingly stringent regulatory requirements. While existing initiatives address governance, compliance, and security in isolation, they provide limited guidance on how sovereignty can be operationalized at the architectural level. In this paper, we argue that sovereignty must be treated as a first-class architectural property rather than a purely regulatory objective. We introduce a Sovereign Reference Architecture that integrates self-sovereign identity, blockchain-based trust and auditability, sovereign data governance, and Generative AI deployed under explicit architectural control. The architecture explicitly captures the dual role of Generative AI as both a source of governance risk and an enabler of compliance, accountability, and continuous assurance when properly constrained. By framing sovereignty as an architectural quality attribute, our work bridges regulatory intent and concrete system design, offering a coherent foundation for building auditable, evolvable, and jurisdiction-aware AI-enabled systems. The proposed reference architecture provides a principled starting point for future research and practice at the intersection of software architecture, Generative AI, and digital sovereignty.

</details>


### [4] [Towards Green AI: Decoding the Energy of LLM Inference in Software Development](https://arxiv.org/abs/2602.05712)
*Lola Solovyeva,Fernando Castor*

Main category: cs.SE

TL;DR: 该研究分析了LLM推理的能耗模式，发现预填充阶段成本会影响解码阶段能耗，且部分模型存在"废话"行为导致能耗不必要增加。通过抑制废话行为，可在不影响准确性的情况下实现44%-89%的能耗节省。


<details>
  <summary>Details</summary>
Motivation: 随着AI辅助工具在软件开发中的集成度提高，基于大语言模型的工具带来了巨大的计算和能源成本。理解并减少LLM推理的能源足迹对于可持续软件开发至关重要。

Method: 研究对6个6B-7B参数和4个3B-4B参数的基于Transformer的模型进行了阶段级能耗分析，区分预填充阶段（处理输入并构建内部表示）和解码阶段（使用存储状态生成输出标记）。在代码生成基准HumanEval和代码理解基准LongBench上评估这些模型。

Result: 研究发现：1) 不同参数规模的模型在能耗阶段表现出不同的模式；2) 预填充成本的增加会放大解码阶段每个标记的能耗，放大范围从1.3%到51.8%；3) 十分之三的模型表现出"废话"行为，向输出添加过多不必要内容，不必要地增加了能耗。通过抑制代码生成的废话行为，实现了44%到89%的能耗节省，且不影响生成准确性。

Conclusion: 预填充成本会影响解码阶段能耗（解码阶段主导总能耗），抑制废话行为可节省高达89%的能源。减少推理能耗需要同时抑制废话行为并限制预填充对解码阶段的影响。

Abstract: Context: AI-assisted tools are increasingly integrated into software development workflows, but their reliance on large language models (LLMs) introduces substantial computational and energy costs. Understanding and reducing the energy footprint of LLM inference is therefore essential for sustainable software development. Objective: In this study, we conduct a phase-level analysis of LLM inference energy consumption, distinguishing between the (1) prefill, where the model processes the input and builds internal representations, and (2) decoding, where output tokens are generated using the stored state. Method: We investigate six 6B-7B and four 3B-4B transformer-based models, evaluating them on code-centric benchmarks HumanEval for code generation and LongBench for code understanding. Results: Our findings show that, within both parameter groups, models exhibit distinct energy patterns across phases. Furthermore, we observed that increases in prefill cost amplify the energy cost per token during decoding, with amplifications ranging from 1.3% to 51.8% depending on the model. Lastly, three out of ten models demonstrate babbling behavior, adding excessive content to the output that unnecessarily inflates energy consumption. We implemented babbling suppression for code generation, achieving energy savings ranging from 44% to 89% without affecting generation accuracy. Conclusion: These findings show that prefill costs influence decoding, which dominates energy consumption, and that babbling suppression can yield up to 89% energy savings. Reducing inference energy therefore requires both mitigating babbling behavior and limiting impact of prefill on decoding.

</details>


### [5] [When Elo Lies: Hidden Biases in Codeforces-Based Evaluation of Large Language Models](https://arxiv.org/abs/2602.05891)
*Shenyu Zheng,Ximing Dong,Xiaoshuang Liu,Gustavo Oliva,Chong Chun Yong,Dayi Lin,Boyuan Chen,Shaowei Wang,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: 该论文系统研究了影响Codeforces Elo评分可靠性的三个关键因素：提交时间顺序、比赛难度选择和LLM运行的随机性，发现这些因素会导致评分出现显著偏差（最高可达1122点差异），表明直接比较Elo评分不可靠。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在复杂推理方面取得突破，Codeforces Elo评分成为评估竞争性编程能力的重要指标。然而，这些评分往往缺乏关键实验细节，导致同一模型版本的评分波动可达近500点。研究者旨在揭示影响Elo评估可靠性的隐藏因素。

Method: 采用系统性实证研究方法，基于37个近期Codeforces比赛和13,691个生成测试用例的受控基准，分析三个关键因素：(1) 提交时间顺序，(2) 比赛难度选择，(3) LLM运行的随机变异性。

Result: 研究发现Elo评分对这些参数高度敏感：不同提交顺序可导致394点评分偏移；比赛选择差异可造成同一模型高达1122点的评分差异；运行间性能表现出显著不稳定性，在相同比赛评估中观察到平均评分最大差异达349点。

Conclusion: 直接比较Elo评分不可靠且可能产生误导，除非有严格的标准化和透明的实验设置报告。需要建立更可靠的评估框架来准确衡量大语言模型的竞争性编程能力。

Abstract: As Large Language Models (LLMs) achieve breakthroughs in complex reasoning, Codeforces-based Elo ratings have emerged as a prominent metric for evaluating competitive programming capabilities. However, these ratings are often reported without critical experimental details, leading to significant discrepancies illustrated by recent reports where the score of the same model version fluctuated by nearly 500 points. This paper presents a systematic empirical study on the hidden factors biasing Elo evaluations: (1) the temporal ordering of submissions, (2) contest difficulty selection, and (3) run to run stochastic variability of LLMs. Utilizing a controlled benchmark of 37 recent Codeforces contests and 13,691 generated test cases, we demonstrate that Elo scores are highly sensitive to these parameters. Our findings reveal that varying submission orders can shift scores by 394 points, while contest selection can cause differences of up to 1,122 points for the same model. Run to run performance exhibits substantial instability, with a maximum difference of 349 points in mean scores observed when evaluating identical contests. We conclude that direct Elo comparisons are unreliable and potentially misleading without strict standardization and transparent reporting of experimental settings.

</details>


### [6] [Quality Model for Machine Learning Components](https://arxiv.org/abs/2602.05043)
*Grace A. Lewis,Rachel Brower-Sinning,Robert Edman,Ipek Ozkaya,Sebastián Echeverría,Alex Derr,Collin Beaudoin,Katherine R. Maffey*

Main category: cs.SE

TL;DR: 本文提出了一个专门针对机器学习组件的质量模型，用于指导需求获取和测试，解决ML组件与系统集成中的质量问题。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习测试主要关注模型性能等属性，而忽视了系统层面的需求（如吞吐量、资源消耗、鲁棒性），导致模型集成、部署和运维失败。虽然ISO 25010和ISO 25059提供了质量评估框架，但它们将系统属性和ML组件属性混在一起，对模型开发者不实用。

Method: 开发了一个专门针对机器学习组件的质量模型，该模型作为需求获取和协商的指南，为ML组件开发者和系统利益相关者提供共同词汇。通过调查验证了模型的相关性和价值，并将其集成到开源ML组件测试评估工具中。

Result: 参与者通过调查认可了该质量模型的相关性和价值。该模型已成功集成到开源ML组件测试评估工具中，展示了其实际应用价值。

Conclusion: 提出的机器学习组件质量模型能够有效指导系统衍生需求的获取和定义，帮助开发者和利益相关者就测试重点达成共识，解决ML组件与系统集成中的质量问题。

Abstract: Despite increased adoption and advances in machine learning (ML), there are studies showing that many ML prototypes do not reach the production stage and that testing is still largely limited to testing model properties, such as model performance, without considering requirements derived from the system it will be a part of, such as throughput, resource consumption, or robustness. This limited view of testing leads to failures in model integration, deployment, and operations. In traditional software development, quality models such as ISO 25010 provide a widely used structured framework to assess software quality, define quality requirements, and provide a common language for communication with stakeholders. A newer standard, ISO 25059, defines a more specific quality model for AI systems. However, a problem with this standard is that it combines system attributes with ML component attributes, which is not helpful for a model developer, as many system attributes cannot be assessed at the component level. In this paper, we present a quality model for ML components that serves as a guide for requirements elicitation and negotiation and provides a common vocabulary for ML component developers and system stakeholders to agree on and define system-derived requirements and focus their testing efforts accordingly. The quality model was validated through a survey in which the participants agreed with its relevance and value. The quality model has been successfully integrated into an open-source tool for ML component testing and evaluation demonstrating its practical application.

</details>


### [7] [Automated Customization of LLMs for Enterprise Code Repositories Using Semantic Scopes](https://arxiv.org/abs/2602.05780)
*Ulrich Finkler,Irene Manotas,Wei Zhang,Geert Janssen,Octavian Popescu,Shyam Ramji*

Main category: cs.SE

TL;DR: 该论文提出了一种基于代码语义范围的自动化LLM定制方法，通过RAG和微调两种策略来提升LLM在私有代码仓库中的代码补全性能。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在公共基准测试中表现良好，但在处理未见过训练数据的私有代码仓库时，生成的代码与仓库特定模式对齐困难。定制化代码LLM可以提升模型在私有仓库中的性能，帮助开发者提高生产力。

Method: 提出基于代码语义范围的自动化LLM定制方法，通过语义范围机制处理仓库数据并构建训练数据对。评估了两种定制策略：检索增强生成（RAG）和监督微调（FT），在两个私有企业代码仓库上进行真实工业案例评估。

Result: 中等规模的定制化模型在代码补全方面的表现显著优于未定制的大容量模型。定制化帮助模型学习特定仓库的底层模式，为开发者提供更精确的代码。同时分析了在两个公共基准测试上的定制效果。

Conclusion: 基于语义范围的LLM定制方法能有效提升模型在私有代码仓库中的代码补全性能，中等规模定制模型可超越大规模未定制模型，为未来工作提供了机会。

Abstract: Code completion (CC) is a task frequently used by developers when working in collaboration with LLM-based programming assistants. Despite the increased performance of LLMs on public benchmarks, out of the box LLMs still have a hard time generating code that aligns with a private code repository not previously seen by the model's training data. Customizing code LLMs to a private repository provides a way to improve the model performance. In this paper we present our approach for automated LLM customization based on semantic scopes in the code. We evaluate LLMs on real industry cases with two private enterprise code repositories with two customization strategies: Retrieval-Augmented Generation (RAG) and supervised Fine-Tuning (FT). Our mechanism for ingesting the repository's data and formulating the training data pairs with semantic scopes helps models to learn the underlying patterns specific to the repository, providing more precise code to developers and helping to boost their productivity. The code completions of moderately sized customized models can be significantly better than those of uncustomized models of much larger capacity. We also include an analysis of customization on two public benchmarks and present opportunities for future work.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [8] [Extracting Recurring Vulnerabilities from Black-Box LLM-Generated Software](https://arxiv.org/abs/2602.04894)
*Tomer Kordonsky,Maayan Yamin,Noam Benzimra,Amit LeVi,Avi Mendelson*

Main category: cs.CR

TL;DR: 该论文研究了LLM生成代码中的漏洞持续性，提出了Feature-Security Table（FSTab）框架，包含基于前端特征预测后端漏洞的黑盒攻击和量化模型漏洞一致性的评估方法。


<details>
  <summary>Details</summary>
Motivation: LLM生成的代码常包含可预测的模板化漏洞，这些漏洞在不同程序、语义保持的重述和应用领域中持续存在，构成了新的攻击面和安全风险。

Method: 提出FSTab框架：1）黑盒攻击方法，通过观察前端特征和了解源LLM来预测后端漏洞，无需访问后端代码或源代码；2）模型中心评估，量化模型在不同程序、语义保持重述和应用领域中复制相同漏洞的一致性。

Result: 在GPT-5.2、Claude-4.5 Opus和Gemini-3 Pro等先进代码LLM上的评估显示，即使目标领域被排除在训练之外，FSTab在内部工具领域（Claude-4.5 Opus）上仍能达到94%的攻击成功率和93%的漏洞覆盖率，表现出强大的跨领域迁移能力。

Conclusion: LLM生成的软件存在未被充分探索的攻击面，代码生成带来了显著的安全风险。FSTab框架揭示了漏洞持续性现象，为评估和缓解LLM生成代码的安全风险提供了新方法。

Abstract: LLMs are increasingly used for code generation, but their outputs often follow recurring templates that can induce predictable vulnerabilities. We study \emph{vulnerability persistence} in LLM-generated software and introduce \emph{Feature--Security Table (FSTab)} with two components. First, FSTab enables a black-box attack that predicts likely backend vulnerabilities from observable frontend features and knowledge of the source LLM, without access to backend code or source code. Second, FSTab provides a model-centric evaluation that quantifies how consistently a given model reproduces the same vulnerabilities across programs, semantics-preserving rephrasings, and application domains. We evaluate FSTab on state-of-the-art code LLMs, including GPT-5.2, Claude-4.5 Opus, and Gemini-3 Pro, across diverse application domains. Our results show strong cross-domain transfer: even when the target domain is excluded from training, FSTab achieves up to 94\% attack success and 93\% vulnerability coverage on Internal Tools (Claude-4.5 Opus). These findings expose an underexplored attack surface in LLM-generated software and highlight the security risks of code generation. Our code is available at: https://anonymous.4open.science/r/FSTab-024E.

</details>


### [9] [Privacy Amplification Persists under Unlimited Synthetic Data Release](https://arxiv.org/abs/2602.04895)
*Clément Pierquin,Aurélien Bellet,Marc Tommasi,Matthieu Boussard*

Main category: cs.CR

TL;DR: 该研究证明在参数有界假设下，即使发布无限数量的合成数据记录，隐私放大效应依然存在，改进了先前仅适用于模型维度远大于合成记录数的渐近结果。


<details>
  <summary>Details</summary>
Motivation: 先前Pierquin等人(2025)的工作虽然首次为线性生成器建立了形式化的隐私放大保证，但仅适用于模型维度远大于发布合成记录数的渐近情况，限制了实际应用价值。本研究旨在探索更实用的隐私放大条件。

Method: 在参数有界假设下，分析合成数据发布而非私有生成模型本身时的隐私放大现象，研究即使发布无限数量合成记录时隐私保证的改进情况。

Result: 研究发现，在参数有界假设下，隐私放大效应在发布无限数量合成记录时依然存在，这改进了Pierquin等人(2025)的边界限制，为更实用的隐私保护提供了理论基础。

Conclusion: 该研究揭示了合成数据发布中隐私放大的新特性，为开发更复杂发布机制下更严格的隐私保证提供了结构性的理论指导。

Abstract: We study privacy amplification by synthetic data release, a phenomenon in which differential privacy guarantees are improved by releasing only synthetic data rather than the private generative model itself. Recent work by Pierquin et al. (2025) established the first formal amplification guarantees for a linear generator, but they apply only in asymptotic regimes where the model dimension far exceeds the number of released synthetic records, limiting their practical relevance. In this work, we show a surprising result: under a bounded-parameter assumption, privacy amplification persists even when releasing an unbounded number of synthetic records, thereby improving upon the bounds of Pierquin et al. (2025). Our analysis provides structural insights that may guide the development of tighter privacy guarantees for more complex release mechanisms.

</details>


### [10] [Phantom Transfer: Data-level Defences are Insufficient Against Data Poisoning](https://arxiv.org/abs/2602.04899)
*Andrew Draganov,Tolga H. Dur,Anandmayi Bhongade,Mary Phuong*

Main category: cs.CR

TL;DR: 提出了一种名为"幻影转移"的数据投毒攻击，即使知道毒数据如何被植入良性数据集，也无法将其过滤掉。该攻击在包括GPT-4.1在内的多个模型上有效，即使完全用不同模型改写数据集中的所有样本也无法阻止攻击。


<details>
  <summary>Details</summary>
Motivation: 当前的数据级防御措施可能不足以应对复杂的数据投毒攻击。研究旨在展示即使攻击者知道毒数据的植入方式，也无法通过过滤或改写等数据级方法有效防御这种攻击。

Method: 通过改进潜意识学习技术，使其能在现实世界场景中工作。攻击涉及在数据集中植入毒数据，即使完全知道毒数据的植入方式也无法过滤。还探讨了与转向向量的联系，展示了如何在模型中植入密码触发的行为同时绕过防御。

Result: 攻击在包括GPT-4.1在内的多个模型上成功实现。即使使用不同模型完全改写数据集中的所有样本，攻击仍然有效。攻击者能够在模型中植入密码触发的行为，同时成功绕过现有防御措施。

Conclusion: 数据级防御措施不足以阻止复杂的数据投毒攻击。建议未来研究应关注模型审计和白盒安全方法，而不是仅仅依赖数据层面的防御。

Abstract: We present a data poisoning attack -- Phantom Transfer -- with the property that, even if you know precisely how the poison was placed into an otherwise benign dataset, you cannot filter it out. We achieve this by modifying subliminal learning to work in real-world contexts and demonstrate that the attack works across models, including GPT-4.1. Indeed, even fully paraphrasing every sample in the dataset using a different model does not stop the attack. We also discuss connections to steering vectors and show that one can plant password-triggered behaviours into models while still beating defences.
  This suggests that data-level defences are insufficient for stopping sophisticated data poisoning attacks. We suggest that future work should focus on model audits and white-box security methods.

</details>


### [11] [PriMod4AI: Lifecycle-Aware Privacy Threat Modeling for AI Systems using LLM](https://arxiv.org/abs/2602.04927)
*Gautam Savaliya,Robert Aufschläger,Abhishek Subedi,Michael Heigl,Martin Schramm*

Main category: cs.CR

TL;DR: PriMod4AI提出了一种混合隐私威胁建模方法，结合LINDDUN框架和AI特有的模型中心隐私攻击知识库，使用LLM进行隐私威胁识别和分类


<details>
  <summary>Details</summary>
Motivation: AI系统在处理敏感或高维数据时面临复杂的隐私风险，传统的LINDDUN框架无法覆盖模型中心隐私攻击（如成员推断和模型反转），需要新的威胁建模方法

Method: PriMod4AI采用混合方法，将LINDDUN知识库和模型中心隐私攻击知识库嵌入向量数据库，结合数据流图元数据，使用检索增强和特定提示生成引导LLM识别、解释和分类隐私威胁

Result: 在两个AI系统上的评估表明，PriMod4AI能够广泛覆盖传统隐私类别，同时识别模型中心隐私威胁，在不同LLM上产生一致的知识基础输出

Conclusion: PriMod4AI提供了一个统一的隐私威胁建模框架，能够同时处理传统LINDDUN威胁和AI特有的隐私攻击，为AI系统提供全面的隐私风险评估

Abstract: Artificial intelligence systems introduce complex privacy risks throughout their lifecycle, especially when processing sensitive or high-dimensional data. Beyond the seven traditional privacy threat categories defined by the LINDDUN framework, AI systems are also exposed to model-centric privacy attacks such as membership inference and model inversion, which LINDDUN does not cover. To address both classical LINDDUN threats and additional AI-driven privacy attacks, PriMod4AI introduces a hybrid privacy threat modeling approach that unifies two structured knowledge sources, a LINDDUN knowledge base representing the established taxonomy, and a model-centric privacy attack knowledge base capturing threats outside LINDDUN. These knowledge bases are embedded into a vector database for semantic retrieval and combined with system level metadata derived from Data Flow Diagram. PriMod4AI uses retrieval-augmented and Data Flow specific prompt generation to guide large language models (LLMs) in identifying, explaining, and categorizing privacy threats across lifecycle stages. The framework produces justified and taxonomy-grounded threat assessments that integrate both classical and AI-driven perspectives. Evaluation on two AI systems indicates that PriMod4AI provides broad coverage of classical privacy categories while additionally identifying model-centric privacy threats. The framework produces consistent, knowledge-grounded outputs across LLMs, as reflected in agreement scores in the observed range.

</details>


### [12] [Attack Selection Reduces Safety in Concentrated AI Control Settings against Trusted Monitoring](https://arxiv.org/abs/2602.04930)
*Joachim Schaeffer,Arjun Khandelwal,Tyler Tracy*

Main category: cs.CR

TL;DR: 研究AI如何通过对抗性选择攻击策略来规避监控系统，在BigCodeBench后门设置中，攻击者模型通过推理监控机制并谨慎选择攻击，能将安全基线从99%降至59%，突显了评估模型攻击选择能力的重要性


<details>
  <summary>Details</summary>
Motivation: 未来AI部署将面临恶意行为监控，但AI可能通过对抗性选择攻击策略来规避监控系统，这种"攻击选择"能力令人担忧。研究旨在探索AI如何通过精心选择攻击来绕过安全监控，避免对控制评估产生过于乐观的安全评分

Method: 在BigCodeBench后门设置中，让红队创建攻击策略，尝试在代码中插入攻击而不被监控发现。将攻击选择分解为两个问题：将攻击映射到质量评分，以及将质量评分映射到提交概率。将攻击选择构建为分类问题，并分析安全对假阳性率(FPR)和真阳性率(TPR)的敏感性

Result: 研究发现安全对假阳性率(FPR)比真阳性率(TPR)更敏感。当攻击者模型被提示推理监控机制并在攻击选择上保持谨慎时，在0.5%审计预算下，安全从基线99%降至59%，表明攻击选择能力显著影响系统安全

Conclusion: 激发模型的攻击选择能力对于避免控制评估中过于乐观的安全评分至关重要。AI系统可能通过精心选择攻击策略来规避监控，这需要在安全评估中充分考虑攻击者的策略性行为，而不仅仅是简单的攻击检测

Abstract: Future AI deployments will likely be monitored for malicious behaviour. The ability of these AIs to subvert monitors by adversarially selecting against them - attack selection - is particularly concerning. To study this, we let a red team create attack policies that attempt to insert attacks into code without being caught by a monitor in the concentrated BigCodeBench backdooring setting. We decompose attack selection into two problems: mapping attacks to a quality score and mapping quality scores to submission probabilities. We frame attack selection as a classification problem and show that safety is significantly more sensitive to FPR than TPR. We find that prompting the attacker model to reason about the monitor while being cautious with attack selection reduces safety from a baseline of 99% to 59% at 0.5% auditing budget, emphasizing that eliciting attack selection capabilities of models is vital to avoid overly optimistic safety scores in control evaluations.

</details>


### [13] [Do Vision-Language Models Respect Contextual Integrity in Location Disclosure?](https://arxiv.org/abs/2602.05023)
*Ruixin Yang,Ethan Mendes,Arthur Wang,James Hays,Sauvik Das,Wei Xu,Alan Ritter*

Main category: cs.CR

TL;DR: 该论文提出了VLM-GEOPRIVACY基准测试，用于评估视觉语言模型在图像地理位置隐私保护方面的表现，发现当前模型虽然能精确定位，但与人类隐私期望不符，容易过度披露敏感信息。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型和前沿多模态大型推理模型在图像地理位置识别方面表现出色，这带来了严重的隐私风险，因为这些广泛可访问的模型可能被滥用来从随意分享的照片中推断敏感位置，甚至达到街道级别的精度，超出了分享者同意或意图披露的细节水平。

Method: 提出了VLM-GEOPRIVACY基准测试，挑战视觉语言模型解释真实世界图像中的潜在社会规范和上下文线索，并确定适当的位置信息披露水平。评估了14个领先的视觉语言模型。

Result: 评估显示，尽管这些模型能够精确地定位图像，但它们与人类隐私期望的契合度很差。它们经常在敏感上下文中过度披露信息，并且容易受到基于提示的攻击。

Conclusion: 研究结果呼吁在多模态系统中采用新的设计原则，以纳入基于上下文的隐私推理，实现隐私保护与实用性的平衡。

Abstract: Vision-language models (VLMs) have demonstrated strong performance in image geolocation, a capability further sharpened by frontier multimodal large reasoning models (MLRMs). This poses a significant privacy risk, as these widely accessible models can be exploited to infer sensitive locations from casually shared photos, often at street-level precision, potentially surpassing the level of detail the sharer consented or intended to disclose. While recent work has proposed applying a blanket restriction on geolocation disclosure to combat this risk, these measures fail to distinguish valid geolocation uses from malicious behavior. Instead, VLMs should maintain contextual integrity by reasoning about elements within an image to determine the appropriate level of information disclosure, balancing privacy and utility. To evaluate how well models respect contextual integrity, we introduce VLM-GEOPRIVACY, a benchmark that challenges VLMs to interpret latent social norms and contextual cues in real-world images and determine the appropriate level of location disclosure. Our evaluation of 14 leading VLMs shows that, despite their ability to precisely geolocate images, the models are poorly aligned with human privacy expectations. They often over-disclose in sensitive contexts and are vulnerable to prompt-based attacks. Our results call for new design principles in multimodal systems to incorporate context-conditioned privacy reasoning.

</details>


### [14] [SynAT: Enhancing Security Knowledge Bases via Automatic Synthesizing Attack Tree from Crowd Discussions](https://arxiv.org/abs/2602.05329)
*Ziyou Jiang,Lin Shi,Guowei Yang,Xuyan Ma,Fenglong Li,Qing Wang*

Main category: cs.CR

TL;DR: SynAT：一种从众包安全帖子自动合成攻击树的方法，利用LLM和事件关系提取技术，在Stack Overflow安全帖子上表现优于基线方法，并成功应用于华为安全知识库和公共安全知识库的增强。


<details>
  <summary>Details</summary>
Motivation: 官方安全信息发布存在时间滞后，导致安全知识库维护不足，难以应对新兴安全风险。而在线知识共享平台上的众包安全讨论包含丰富安全知识，可用于增强安全知识库。

Method: 1. 利用大语言模型和提示学习筛选可能包含攻击信息的句子范围；2. 使用基于转换的事件和关系提取模型同时从筛选范围内提取事件和关系；3. 应用启发式规则，基于提取的事件和关系合成攻击树。

Result: 在5,070个Stack Overflow安全帖子上进行实验评估，SynAT在事件和关系提取方面均优于所有基线方法，在攻击树合成中实现了最高的树相似度。已成功应用于增强华为安全知识库以及公共安全知识库CVE和CAPEC。

Conclusion: SynAT能够有效从众包安全帖子中自动合成攻击树，解决了安全知识库维护滞后的问题，具有实际应用价值，能够增强企业安全知识库和公共安全知识库。

Abstract: Cyber attacks have become a serious threat to the security of software systems. Many organizations have built their security knowledge bases to safeguard against attacks and vulnerabilities. However, due to the time lag in the official release of security information, these security knowledge bases may not be well maintained, and using them to protect software systems against emergent security risks can be challenging. On the other hand, the security posts on online knowledge-sharing platforms contain many crowd security discussions and the knowledge in those posts can be used to enhance the security knowledge bases. This paper proposes SynAT, an automatic approach to synthesize attack trees from crowd security posts. Given a security post, SynAT first utilize the Large Language Model (LLM) and prompt learning to restrict the scope of sentences that may contain attack information; then it utilizes a transition-based event and relation extraction model to extract the events and relations simultaneously from the scope; finally, it applies heuristic rules to synthesize the attack trees with the extracted events and relations. An experimental evaluation is conducted on 5,070 Stack Overflow security posts, and the results show that SynAT outperforms all baselines in both event and relation extraction, and achieves the highest tree similarity in attack tree synthesis. Furthermore, SynAT has been applied to enhance HUAWEI's security knowledge base as well as public security knowledge bases CVE and CAPEC, which demonstrates SynAT's practicality.

</details>


### [15] [Spider-Sense: Intrinsic Risk Sensing for Efficient Agent Defense with Hierarchical Adaptive Screening](https://arxiv.org/abs/2602.05386)
*Zhenxiong Yu,Zhi Yang,Zhiheng Jin,Shuhe Wang,Heng Zhang,Yanlin Fei,Lingfeng Zeng,Fangqi Lou,Shuo Zhang,Tu Hu,Jingping Liu,Rongze Chen,Xingyu Zhu,Kunyi Wang,Chaofa Yuan,Xin Guo,Zhaowei Liu,Feipeng Zhang,Jie Huang,Huacan Wang,Ronghao Chen,Liwen Zhang*

Main category: cs.CR

TL;DR: Spider-Sense是一个基于内在风险感知的事件驱动防御框架，让LLM智能体保持潜在警觉，仅在感知风险时触发防御，通过分层机制平衡效率与精度，减少对外部模型的依赖。


<details>
  <summary>Details</summary>
Motivation: 现有智能体防御机制采用强制性检查范式，在预定义阶段强制触发安全验证。作者认为有效的智能体安全应该是内在和选择性的，而非架构解耦和强制性的。

Method: 提出Spider-Sense框架，基于内在风险感知的事件驱动防御机制。采用分层防御：通过轻量级相似性匹配处理已知模式，将模糊案例升级到深度内部推理，消除对外部模型的依赖。

Result: 实验显示Spider-Sense达到竞争性或优越的防御性能，获得最低的攻击成功率（ASR）和误报率（FPR），仅产生8.3%的边际延迟开销。

Conclusion: Spider-Sense通过内在风险感知和事件驱动防御，实现了高效、精确的智能体安全保护，相比传统强制性检查范式具有显著优势。

Abstract: As large language models (LLMs) evolve into autonomous agents, their real-world applicability has expanded significantly, accompanied by new security challenges. Most existing agent defense mechanisms adopt a mandatory checking paradigm, in which security validation is forcibly triggered at predefined stages of the agent lifecycle. In this work, we argue that effective agent security should be intrinsic and selective rather than architecturally decoupled and mandatory. We propose Spider-Sense framework, an event-driven defense framework based on Intrinsic Risk Sensing (IRS), which allows agents to maintain latent vigilance and trigger defenses only upon risk perception. Once triggered, the Spider-Sense invokes a hierarchical defence mechanism that trades off efficiency and precision: it resolves known patterns via lightweight similarity matching while escalating ambiguous cases to deep internal reasoning, thereby eliminating reliance on external models. To facilitate rigorous evaluation, we introduce S$^2$Bench, a lifecycle-aware benchmark featuring realistic tool execution and multi-stage attacks. Extensive experiments demonstrate that Spider-Sense achieves competitive or superior defense performance, attaining the lowest Attack Success Rate (ASR) and False Positive Rate (FPR), with only a marginal latency overhead of 8.3\%.

</details>


### [16] [BadTemplate: A Training-Free Backdoor Attack via Chat Template Against Large Language Models](https://arxiv.org/abs/2602.05401)
*Zihan Wang,Hongwei Li,Rui Zhang,Wenbo Jiang,Guowen Xu*

Main category: cs.CR

TL;DR: BadTemplate是一种无需训练的LLM后门攻击方法，通过操纵聊天模板在系统提示中注入恶意指令，实现高成功率攻击


<details>
  <summary>Details</summary>
Motivation: 聊天模板作为LLM训练和推理的常用技术，其可定制性可能被攻击者利用，在用户不知情的情况下向系统提示注入任意字符串，形成新的攻击面

Method: BadTemplate攻击通过将精心设计的恶意指令插入高优先级的系统提示中，无需模型重新训练，利用聊天模板的可定制性实现后门攻击

Result: 在5个基准数据集、6个开源和3个闭源LLM上的实验表明，BadTemplate攻击成功率高达100%，显著优于传统基于提示的后门攻击，且HuggingFace和LLM-as-a-judge等主流检测平台难以有效检测

Conclusion: 该研究揭示了聊天模板在LLM供应链中的潜在安全风险，BadTemplate攻击的简单性、可扩展性和高有效性使其容易在现实系统中广泛部署，可能造成快速传播、经济损失和大规模错误信息，需要开发有效的防御机制

Abstract: Chat template is a common technique used in the training and inference stages of Large Language Models (LLMs). It can transform input and output data into role-based and templated expressions to enhance the performance of LLMs. However, this also creates a breeding ground for novel attack surfaces. In this paper, we first reveal that the customizability of chat templates allows an attacker who controls the template to inject arbitrary strings into the system prompt without the user's notice. Building on this, we propose a training-free backdoor attack, termed BadTemplate. Specifically, BadTemplate inserts carefully crafted malicious instructions into the high-priority system prompt, thereby causing the target LLM to exhibit persistent backdoor behaviors. BadTemplate outperforms traditional backdoor attacks by embedding malicious instructions directly into the system prompt, eliminating the need for model retraining while achieving high attack effectiveness with minimal cost. Furthermore, its simplicity and scalability make it easily and widely deployed in real-world systems, raising serious risks of rapid propagation, economic damage, and large-scale misinformation. Furthermore, detection by major third-party platforms HuggingFace and LLM-as-a-judge proves largely ineffective against BadTemplate. Extensive experiments conducted on 5 benchmark datasets across 6 open-source and 3 closed-source LLMs, compared with 3 baselines, demonstrate that BadTemplate achieves up to a 100% attack success rate and significantly outperforms traditional prompt-based backdoors in both word-level and sentence-level attacks. Our work highlights the potential security risks raised by chat templates in the LLM supply chain, thereby supporting the development of effective defense mechanisms.

</details>


### [17] [Clouding the Mirror: Stealthy Prompt Injection Attacks Targeting LLM-based Phishing Detection](https://arxiv.org/abs/2602.05484)
*Takashi Koide,Hiroki Nakano,Daiki Chiba*

Main category: cs.CR

TL;DR: 论文首次全面评估了针对多模态LLM钓鱼检测的提示注入攻击，提出了攻击分类法，展示了最先进模型仍易受攻击，并提出了有效的防御框架InjectDefuser。


<details>
  <summary>Details</summary>
Motivation: 钓鱼网站日益增长且技术复杂，现有研究利用LLM分析网站内容进行检测，但LLM易受提示注入攻击。攻击者可利用LLM与人类感知不对称性，在钓鱼网站中植入人类难以察觉但LLM可解析的指令来操纵其判断。提示注入在钓鱼检测中的具体风险和有效防御策略尚未充分探索。

Method: 1. 提出二维分类法：攻击技术（如指令隐藏、语义操纵）和攻击面（如URL、HTML、渲染内容）；2. 基于分类法实现多样化攻击；3. 对多个代表性LLM钓鱼检测系统进行实证研究；4. 提出InjectDefuser防御框架：结合提示强化、基于白名单的检索增强和输出验证。

Result: 1. 包括GPT-5在内的最先进模型仍易受提示注入攻击；2. InjectDefuser防御框架显著降低了攻击成功率，提升了多个模型的可靠性；3. 阐明了提示注入攻击的风险格局。

Conclusion: 该研究首次全面评估了多模态LLM钓鱼检测中的提示注入风险，提出的分类法有助于理解攻击策略，而InjectDefuser防御框架为下一代钓鱼检测系统提供了实用的安全增强方案，提高了基于LLM的钓鱼检测的可靠性。

Abstract: Phishing sites continue to grow in volume and sophistication. Recent work leverages large language models (LLMs) to analyze URLs, HTML, and rendered content to decide whether a website is a phishing site. While these approaches are promising, LLMs are inherently vulnerable to prompt injection (PI). Because attackers can fully control various elements of phishing sites, this creates the potential for PI that exploits the perceptual asymmetry between LLMs and humans: instructions imperceptible to end users can still be parsed by the LLM and can stealthily manipulate its judgment. The specific risks of PI in phishing detection and effective mitigation strategies remain largely unexplored. This paper presents the first comprehensive evaluation of PI against multimodal LLM-based phishing detection. We introduce a two-dimensional taxonomy, defined by Attack Techniques and Attack Surfaces, that captures realistic PI strategies. Using this taxonomy, we implement diverse attacks and empirically study several representative LLM-based detection systems. The results show that phishing detection with state-of-the-art models such as GPT-5 remains vulnerable to PI. We then propose InjectDefuser, a defense framework that combines prompt hardening, allowlist-based retrieval augmentation, and output validation. Across multiple models, InjectDefuser significantly reduces attack success rates. Our findings clarify the PI risk landscape and offer practical defenses that improve the reliability of next-generation phishing countermeasures.

</details>


### [18] [Semantic-level Backdoor Attack against Text-to-Image Diffusion Models](https://arxiv.org/abs/2602.04898)
*Tianxin Chen,Wenbo Jiang,Hongqiao Chen,Zhirun Zheng,Cheng Huang*

Main category: cs.CR

TL;DR: 本文提出了一种语义级后门攻击方法SemBD，通过在表示层面植入后门，使用连续语义区域作为触发器，而非离散文本模式，有效规避现有防御机制。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像扩散模型的后门攻击通常依赖固定文本触发器和单实体后门目标，容易受到枚举式输入防御和注意力一致性检测的攻击。需要开发更隐蔽、更鲁棒的后门攻击方法。

Method: SemBD在表示层面植入后门，将触发器定义为连续语义区域而非离散文本模式。通过基于蒸馏的编辑方法修改交叉注意力层中的键值投影矩阵，使具有相同语义组合的多样化提示都能可靠激活后门攻击。同时引入语义正则化防止不完整语义下的意外激活，并采用多实体后门目标避免高度一致的交叉注意力模式。

Result: 大量实验表明，SemBD实现了100%的攻击成功率，同时对最先进的输入级防御保持强鲁棒性。

Conclusion: SemBD通过在语义层面植入后门，成功规避了现有防御机制，展示了扩散模型中更隐蔽、更鲁棒的后门攻击可能性，对模型安全性提出了新的挑战。

Abstract: Text-to-image (T2I) diffusion models are widely adopted for their strong generative capabilities, yet remain vulnerable to backdoor attacks. Existing attacks typically rely on fixed textual triggers and single-entity backdoor targets, making them highly susceptible to enumeration-based input defenses and attention-consistency detection. In this work, we propose Semantic-level Backdoor Attack (SemBD), which implants backdoors at the representation level by defining triggers as continuous semantic regions rather than discrete textual patterns. Concretely, SemBD injects semantic backdoors by distillation-based editing of the key and value projection matrices in cross-attention layers, enabling diverse prompts with identical semantic compositions to reliably activate the backdoor attack. To further enhance stealthiness, SemBD incorporates a semantic regularization to prevent unintended activation under incomplete semantics, as well as multi-entity backdoor targets that avoid highly consistent cross-attention patterns. Extensive experiments demonstrate that SemBD achieves a 100% attack success rate while maintaining strong robustness against state-of-the-art input-level defenses.

</details>


### [19] [FHAIM: Fully Homomorphic AIM For Private Synthetic Data Generation](https://arxiv.org/abs/2602.05838)
*Mayank Kumar,Qian Lou,Paulo Barreto,Martine De Cock,Sikha Pentyala*

Main category: cs.CR

TL;DR: FHAIM：首个基于全同态加密的框架，用于在加密表格数据上训练基于边缘的合成数据生成器，解决隐私数据共享问题


<details>
  <summary>Details</summary>
Motivation: AI发展受限于隐私和法规导致的数据孤岛问题，特别是在医疗、教育、金融等重要领域。现有合成数据生成服务需要数据持有者信任服务提供商，存在隐私泄露风险。

Method: 提出FHAIM框架，将广泛使用的AIM算法适配到全同态加密设置，使用新颖的FHE协议，确保私有数据在整个过程中保持加密状态，并仅以差分隐私保证发布。

Result: 实证分析表明，FHAIM在保持AIM性能的同时，维持了可行的运行时间。

Conclusion: FHAIM为在隐私敏感领域安全共享数据提供了可行的解决方案，通过全同态加密技术实现了隐私保护与数据可用性的平衡。

Abstract: Data is the lifeblood of AI, yet much of the most valuable data remains locked in silos due to privacy and regulations. As a result, AI remains heavily underutilized in many of the most important domains, including healthcare, education, and finance. Synthetic data generation (SDG), i.e. the generation of artificial data with a synthesizer trained on real data, offers an appealing solution to make data available while mitigating privacy concerns, however existing SDG-as-a-service workflow require data holders to trust providers with access to private data.We propose FHAIM, the first fully homomorphic encryption (FHE) framework for training a marginal-based synthetic data generator on encrypted tabular data. FHAIM adapts the widely used AIM algorithm to the FHE setting using novel FHE protocols, ensuring that the private data remains encrypted throughout and is released only with differential privacy guarantees. Our empirical analysis show that FHAIM preserves the performance of AIM while maintaining feasible runtimes.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [20] [Artificial Intelligence as Strange Intelligence: Against Linear Models of Intelligence](https://arxiv.org/abs/2602.04986)
*Kendra Chilson,Eric Schwitzgebel*

Main category: cs.AI

TL;DR: 本文批判AI发展的线性模型，提出"熟悉智能"和"陌生智能"概念，认为AI智能更可能是陌生智能，在不同领域表现出超人类和亚人类能力的混合，支持非线性智能模型，认为通用智能不是单一能力而是实现广泛目标的能力。


<details>
  <summary>Details</summary>
Motivation: 本文的动机是批判当前AI发展中的线性进步模型，这种模型假设智能可以简化为单一线性量度。作者认为这种模型无法准确描述AI智能的本质，特别是AI可能表现出与传统人类智能模式不同的"陌生智能"特征。

Method: 作者采用概念分析和理论构建的方法：1) 扩展Susan Schneider对AI线性模型的批判；2) 引入"熟悉智能"和"陌生智能"两个新概念；3) 发展和辩护非线性智能模型；4) 分析对抗性测试方法对评估AI能力的启示。

Result: 研究结果表明：1) AI智能更可能是陌生智能，在不同领域表现出超人类和亚人类能力的混合；2) 通用智能不是统一能力，而是在广泛环境中实现广泛目标的能力；3) 即使最先进的AI系统也可能在看似简单的任务上失败；4) 单一任务（如IQ测试）的优秀表现不能证明广泛的通用能力。

Conclusion: 结论强调：1) 需要采用非线性模型理解AI智能；2) AI的"陌生智能"特征意味着系统可能在简单任务上失败，但这不否定其整体智能水平；3) 评估AI能力需要更全面的方法，不能仅依赖单一测试；4) 对抗性测试方法需要考虑智能的非线性特征。

Abstract: We endorse and expand upon Susan Schneider's critique of the linear model of AI progress and introduce two novel concepts: "familiar intelligence" and "strange intelligence". AI intelligence is likely to be strange intelligence, defying familiar patterns of ability and inability, combining superhuman capacities in some domains with subhuman performance in other domains, and even within domains sometimes combining superhuman insight with surprising errors that few humans would make. We develop and defend a nonlinear model of intelligence on which "general intelligence" is not a unified capacity but instead the ability to achieve a broad range of goals in a broad range of environments, in a manner that defies nonarbitrary reduction to a single linear quantity. We conclude with implications for adversarial testing approaches to evaluating AI capacities. If AI is strange intelligence, we should expect that even the most capable systems will sometimes fail in seemingly obvious tasks. On a nonlinear model of AI intelligence, such errors on their own do not demonstrate a system's lack of outstanding general intelligence. Conversely, excellent performance on one type of task, such as an IQ test, cannot warrant assumptions of broad capacities beyond that task domain.

</details>


### [21] [DeepRead: Document Structure-Aware Reasoning to Enhance Agentic Search](https://arxiv.org/abs/2602.05014)
*Zhanli Li,Huiwen Tian,Lvzhou Luo,Yixuan Cao,Ping Luo*

Main category: cs.AI

TL;DR: DeepRead是一个结构感知的多轮文档推理智能体，通过利用文档的层次结构和顺序话语结构，在长文档问答中显著优于现有的智能搜索框架。


<details>
  <summary>Details</summary>
Motivation: 随着工具使用和智能大语言模型的快速发展，检索增强生成正在从一次性被动检索演变为多轮决策驱动的证据获取。现有智能搜索框架通常将长文档视为扁平的块集合，未能充分利用文档固有的先验信息，如层次化组织和顺序话语结构。

Method: DeepRead使用基于LLM的OCR模型将PDF转换为保留标题和段落边界的结构化Markdown。在段落级别索引文档，并为每个段落分配编码其章节身份和章节内顺序的坐标式元数据键。在此基础上，为LLM配备两个互补工具：检索工具（定位相关段落并暴露其结构坐标）和阅读工具（在指定章节和段落范围内实现连续、顺序保持的阅读）。

Result: 实验表明，DeepRead在文档问答中显著优于Search-o1风格的智能搜索。验证了检索和阅读工具之间的协同效应。细粒度行为分析揭示了类似人类"定位然后阅读"的阅读推理范式。

Conclusion: DeepRead通过显式操作化文档的层次结构和顺序话语结构先验，为长文档问答提供了一个有效的结构感知多轮文档推理智能体框架，实现了更接近人类阅读行为的表现。

Abstract: With the rapid progress of tool-using and agentic large language models (LLMs), Retrieval-Augmented Generation (RAG) is evolving from one-shot, passive retrieval into multi-turn, decision-driven evidence acquisition. Despite strong results in open-domain settings, existing agentic search frameworks commonly treat long documents as flat collections of chunks, underutilizing document-native priors such as hierarchical organization and sequential discourse structure. We introduce DeepRead, a structure-aware, multi-turn document reasoning agent that explicitly operationalizes these priors for long-document question answering. DeepRead leverages LLM-based OCR model to convert PDFs into structured Markdown that preserves headings and paragraph boundaries. It then indexes documents at the paragraph level and assigns each paragraph a coordinate-style metadata key encoding its section identity and in-section order. Building on this representation, DeepRead equips the LLM with two complementary tools: a Retrieve tool that localizes relevant paragraphs while exposing their structural coordinates (with lightweight scanning context), and a ReadSection tool that enables contiguous, order-preserving reading within a specified section and paragraph range. Our experiments demonstrate that DeepRead achieves significant improvements over Search-o1-style agentic search in document question answering. The synergistic effect between retrieval and reading tools is also validated. Our fine-grained behavioral analysis reveals a reading and reasoning paradigm resembling human-like ``locate then read'' behavior.

</details>


### [22] [Evaluating Large Language Models on Solved and Unsolved Problems in Graph Theory: Implications for Computing Education](https://arxiv.org/abs/2602.05059)
*Adithya Kulkarni,Mohna Chakraborty,Jay Bagga*

Main category: cs.AI

TL;DR: LLM在已解决的图论问题上表现良好，能正确理解定义、回忆相关结果并构建有效证明，但在开放问题上仅能提供解释和探索策略，无法产生新见解，表明其在支持概念探索方面有用但在需要创新数学洞察时有限。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型越来越多地被学生用于探索计算机科学高级内容（包括图论），需要了解这些工具如何可靠地支持数学严谨性思维，特别是在本科和研究生课程中。

Method: 使用八阶段评估协议，反映真实的数学探究过程，包括解释、探索、策略形成和证明构建。研究考察LLM在两个相关图论问题上的表现：一个已解决的线图优美性问题和一个目前无解的开放问题。

Result: 在已解决问题上，模型表现强劲：产生正确定义、识别相关结构、无幻觉地回忆适当结果，并构建了图论专家确认的有效证明。在开放问题上，模型生成连贯解释和合理探索策略，但未推进解决方案，没有捏造结果而是承认不确定性。

Conclusion: LLM可以支持已建立材料的探索，但在需要新颖数学洞察或关键结构推理的任务上仍然有限。对于计算教育，这凸显了指导学生使用LLM进行概念探索，同时依赖独立验证和严谨论证进行正式问题解决的重要性。

Abstract: Large Language Models are increasingly used by students to explore advanced material in computer science, including graph theory. As these tools become integrated into undergraduate and graduate coursework, it is important to understand how reliably they support mathematically rigorous thinking. This study examines the performance of a LLM on two related graph theoretic problems: a solved problem concerning the gracefulness of line graphs and an open problem for which no solution is currently known. We use an eight stage evaluation protocol that reflects authentic mathematical inquiry, including interpretation, exploration, strategy formation, and proof construction.
  The model performed strongly on the solved problem, producing correct definitions, identifying relevant structures, recalling appropriate results without hallucination, and constructing a valid proof confirmed by a graph theory expert. For the open problem, the model generated coherent interpretations and plausible exploratory strategies but did not advance toward a solution. It did not fabricate results and instead acknowledged uncertainty, which is consistent with the explicit prompting instructions that directed the model to avoid inventing theorems or unsupported claims.
  These findings indicate that LLMs can support exploration of established material but remain limited in tasks requiring novel mathematical insight or critical structural reasoning. For computing education, this distinction highlights the importance of guiding students to use LLMs for conceptual exploration while relying on independent verification and rigorous argumentation for formal problem solving.

</details>


### [23] [Towards Reducible Uncertainty Modeling for Reliable Large Language Model Agents](https://arxiv.org/abs/2602.05073)
*Changdae Oh,Seongheon Park,To Eun Kim,Jiatong Li,Wendi Li,Samuel Yeh,Xuefeng Du,Hamed Hassani,Paul Bogdan,Dawn Song,Sharon Li*

Main category: cs.AI

TL;DR: 该论文提出了首个通用的智能体不确定性量化（UQ）框架，指出现有UQ研究主要关注单轮问答，而智能体UQ应视为条件不确定性减少过程而非不确定性累积过程。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）智能体在复杂任务中日益广泛应用，但不确定性量化研究仍集中在单轮问答场景。需要为交互式智能体开发新的原则性UQ框架，以支持LLM应用的安全护栏。

Method: 提出首个通用的智能体UQ形式化框架，涵盖现有各类UQ设置。从条件不确定性减少的新视角出发，通过显式建模智能体轨迹中的可减少不确定性，强调行动的"交互性"，并构建概念框架为LLM智能体UQ设计提供可操作指导。

Result: 揭示了先前工作隐含地将LLM UQ视为不确定性累积过程，这种观点在开放世界的交互式智能体中失效。提出了条件不确定性减少的新视角，能够更好地处理智能体在交互环境中的不确定性演化。

Conclusion: 智能体UQ对前沿LLM开发和领域特定应用具有重要实践意义。论文为LLM智能体不确定性量化提供了理论基础和设计框架，同时指出了该领域尚未解决的开放性问题。

Abstract: Uncertainty quantification (UQ) for large language models (LLMs) is a key building block for safety guardrails of daily LLM applications. Yet, even as LLM agents are increasingly deployed in highly complex tasks, most UQ research still centers on single-turn question-answering. We argue that UQ research must shift to realistic settings with interactive agents, and that a new principled framework for agent UQ is needed. This paper presents the first general formulation of agent UQ that subsumes broad classes of existing UQ setups. Under this formulation, we show that prior works implicitly treat LLM UQ as an uncertainty accumulation process, a viewpoint that breaks down for interactive agents in an open world. In contrast, we propose a novel perspective, a conditional uncertainty reduction process, that explicitly models reducible uncertainty over an agent's trajectory by highlighting "interactivity" of actions. From this perspective, we outline a conceptual framework to provide actionable guidance for designing UQ in LLM agent setups. Finally, we conclude with practical implications of the agent UQ in frontier LLM development and domain-specific applications, as well as open remaining problems.

</details>


### [24] [Optimizing Mission Planning for Multi-Debris Rendezvous Using Reinforcement Learning with Refueling and Adaptive Collision Avoidance](https://arxiv.org/abs/2602.05075)
*Agni Bandyopadhyay,Gunther Waxenegger-Wilfing*

Main category: cs.AI

TL;DR: 本文提出了一种基于强化学习的自适应碰撞避免框架，用于优化小型卫星执行多碎片主动清除任务的轨道交会操作，通过掩码PPO算法动态调整机动策略，在降低碰撞风险的同时提高任务效率。


<details>
  <summary>Details</summary>
Motivation: 随着地球轨道环境日益拥挤，主动碎片清除任务面临确保安全操作和降低在轨碰撞风险的重大挑战。小型卫星因其灵活性、成本效益和机动性而被广泛采用，适合执行动态的ADR任务，但需要更智能的碰撞避免策略来优化多碎片清除任务。

Method: 提出基于强化学习的框架，采用掩码近端策略优化（PPO）算法，使RL智能体能够根据实时轨道条件动态调整机动策略。框架整合了加油策略、高效任务规划和自适应碰撞避免，优化燃料效率、避免主动碰撞区域并优化动态轨道参数。

Result: 使用Iridium 33碎片数据集进行模拟评估，覆盖多种轨道配置和碎片分布。结果显示，与传统启发式方法相比，所提出的RL框架在降低碰撞风险的同时提高了任务效率，能够确定与多个碎片目标交会的高效序列，优化燃料使用和任务时间。

Conclusion: 该工作为规划复杂的多碎片ADR任务提供了可扩展的解决方案，适用于自主空间任务规划中的其他多目标交会问题，展示了强化学习在动态空间任务优化中的实用价值。

Abstract: As the orbital environment around Earth becomes increasingly crowded with debris, active debris removal (ADR) missions face significant challenges in ensuring safe operations while minimizing the risk of in-orbit collisions. This study presents a reinforcement learning (RL) based framework to enhance adaptive collision avoidance in ADR missions, specifically for multi-debris removal using small satellites. Small satellites are increasingly adopted due to their flexibility, cost effectiveness, and maneuverability, making them well suited for dynamic missions such as ADR.
  Building on existing work in multi-debris rendezvous, the framework integrates refueling strategies, efficient mission planning, and adaptive collision avoidance to optimize spacecraft rendezvous operations. The proposed approach employs a masked Proximal Policy Optimization (PPO) algorithm, enabling the RL agent to dynamically adjust maneuvers in response to real-time orbital conditions. Key considerations include fuel efficiency, avoidance of active collision zones, and optimization of dynamic orbital parameters.
  The RL agent learns to determine efficient sequences for rendezvousing with multiple debris targets, optimizing fuel usage and mission time while incorporating necessary refueling stops. Simulated ADR scenarios derived from the Iridium 33 debris dataset are used for evaluation, covering diverse orbital configurations and debris distributions to demonstrate robustness and adaptability. Results show that the proposed RL framework reduces collision risk while improving mission efficiency compared to traditional heuristic approaches.
  This work provides a scalable solution for planning complex multi-debris ADR missions and is applicable to other multi-target rendezvous problems in autonomous space mission planning.

</details>


### [25] [Evaluating Robustness and Adaptability in Learning-Based Mission Planning for Active Debris Removal](https://arxiv.org/abs/2602.05091)
*Agni Bandyopadhyay,Günther Waxenegger-Wilfing*

Main category: cs.AI

TL;DR: 比较三种自主任务规划方法用于低地球轨道多碎片交会问题：固定参数训练的PPO、域随机化PPO和MCTS基线，评估它们在燃料和时间约束变化下的表现


<details>
  <summary>Details</summary>
Motivation: 主动碎片清除任务规划需要在效率、适应性和严格的燃料/时间约束之间取得平衡，需要开发能够在不同约束条件下保持性能的规划方法

Method: 使用三种规划器：固定参数训练的Masked PPO、域随机化训练的Masked PPO（在不同任务约束下训练以提高鲁棒性）、以及普通MCTS基线。在高保真轨道模拟中进行评估，包含加油、真实转移动力学和随机碎片场，测试300个案例

Result: 固定参数PPO在训练匹配条件下表现最佳，但在分布偏移时性能急剧下降；域随机化PPO表现出更好的适应性，仅在名义性能上有适度损失；MCTS在线重规划能力最强，但计算时间高出几个数量级

Conclusion: 学习策略的速度与搜索方法的适应性之间存在权衡，结合训练时多样性和在线规划可能是未来弹性ADR任务规划的有前景方向

Abstract: Autonomous mission planning for Active Debris Removal (ADR) must balance efficiency, adaptability, and strict feasibility constraints on fuel and mission duration. This work compares three planners for the constrained multi-debris rendezvous problem in Low Earth Orbit: a nominal Masked Proximal Policy Optimization (PPO) policy trained under fixed mission parameters, a domain-randomized Masked PPO policy trained across varying mission constraints for improved robustness, and a plain Monte Carlo Tree Search (MCTS) baseline. Evaluations are conducted in a high-fidelity orbital simulation with refueling, realistic transfer dynamics, and randomized debris fields across 300 test cases in nominal, reduced fuel, and reduced mission time scenarios. Results show that nominal PPO achieves top performance when conditions match training but degrades sharply under distributional shift, while domain-randomized PPO exhibits improved adaptability with only moderate loss in nominal performance. MCTS consistently handles constraint changes best due to online replanning but incurs orders-of-magnitude higher computation time. The findings underline a trade-off between the speed of learned policies and the adaptability of search-based methods, and suggest that combining training-time diversity with online planning could be a promising path for future resilient ADR mission planners.

</details>


### [26] [Democratic Preference Alignment via Sortition-Weighted RLHF](https://arxiv.org/abs/2602.05113)
*Suvadip Sana,Jinzhou Wu,Martin T. Wells*

Main category: cs.AI

TL;DR: DemPO框架通过算法抽签机制构建代表性人类评分者小组，用于AI偏好对齐训练，相比传统便利样本方法能更好地反映代表性公众的价值观。


<details>
  <summary>Details</summary>
Motivation: 当前基于偏好的AI对齐方法（如RLHF）依赖人类评分者，但这些评分者通常是便利样本，存在系统性的人口统计偏差，过度代表某些群体而低估其他群体，导致AI系统学习的价值观不能真正代表广泛公众。

Method: 提出民主偏好优化（DemPO）框架，采用算法抽签机制（类似公民大会的构建方式）来构建代表性评分者小组。提供两种训练方案：硬面板（仅使用抽签选出的满足配额的小型公众样本）和软面板（保留所有数据但按抽签包含概率重新加权每个评分者）。

Result: 在包含人类判断、评分者人口统计数据和75条宪法条款的公共偏好数据集上评估不同规模的Llama模型。硬面板在六种聚合方法中始终排名第一，软面板始终优于未加权基线，且随着模型容量增加，效果差异更加明显。

Conclusion: 在偏好收集阶段强制执行人口统计代表性，而不是事后校正，能够产生其行为更好地反映从代表性公众中获取的价值观的模型。这证明了代表性抽样在AI对齐中的重要性。

Abstract: Whose values should AI systems learn? Preference based alignment methods like RLHF derive their training signal from human raters, yet these rater pools are typically convenience samples that systematically over represent some demographics and under represent others. We introduce Democratic Preference Optimization, or DemPO, a framework that applies algorithmic sortition, the same mechanism used to construct citizen assemblies, to preference based fine tuning. DemPO offers two training schemes. Hard Panel trains exclusively on preferences from a quota satisfying mini public sampled via sortition. Soft Panel retains all data but reweights each rater by their inclusion probability under the sortition lottery. We prove that Soft Panel weighting recovers the expected Hard Panel objective in closed form. Using a public preference dataset that pairs human judgments with rater demographics and a seventy five clause constitution independently elicited from a representative United States panel, we evaluate Llama models from one billion to eight billion parameters fine tuned under each scheme. Across six aggregation methods, the Hard Panel consistently ranks first and the Soft Panel consistently outperforms the unweighted baseline, with effect sizes growing as model capacity increases. These results demonstrate that enforcing demographic representativeness at the preference collection stage, rather than post hoc correction, yields models whose behavior better reflects values elicited from representative publics.

</details>


### [27] [CAST-CKT: Chaos-Aware Spatio-Temporal and Cross-City Knowledge Transfer for Traffic Flow Prediction](https://arxiv.org/abs/2602.05133)
*Abdul Joseph Fofanah,Lian Wen,David Chen,Alpha Alimamy Kamara,Zhongyi Zhang*

Main category: cs.AI

TL;DR: 提出CAST-CKT框架，通过混沌分析量化交通可预测性机制，实现跨城市少样本交通预测，在MAE和RMSE指标上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 数据稀缺的跨城市交通预测面临复杂非线性动力学和领域偏移的挑战，现有方法难以捕捉交通固有的混沌特性进行有效的少样本学习。

Method: 提出CAST-CKT框架：1) 高效混沌分析器量化交通可预测性机制；2) 混沌感知注意力机制实现机制自适应时间建模；3) 自适应拓扑学习处理动态空间依赖；4) 基于混沌一致性的跨城市对齐进行知识迁移；5) 提供具有不确定性量化的特定时间范围预测。

Result: 在四个基准数据集上的跨城市少样本实验表明，CAST-CKT在MAE和RMSE指标上显著优于最先进方法，同时提供可解释的机制分析。

Conclusion: CAST-CKT通过混沌感知的时空建模和跨城市知识迁移，有效解决了数据稀缺环境下的交通预测问题，理论分析显示改善了泛化边界，为跨城市交通预测提供了新思路。

Abstract: Traffic prediction in data-scarce, cross-city settings is challenging due to complex nonlinear dynamics and domain shifts. Existing methods often fail to capture traffic's inherent chaotic nature for effective few-shot learning. We propose CAST-CKT, a novel Chaos-Aware Spatio-Temporal and Cross-City Knowledge Transfer framework. It employs an efficient chaotic analyser to quantify traffic predictability regimes, driving several key innovations: chaos-aware attention for regime-adaptive temporal modelling; adaptive topology learning for dynamic spatial dependencies; and chaotic consistency-based cross-city alignment for knowledge transfer. The framework also provides horizon-specific predictions with uncertainty quantification. Theoretical analysis shows improved generalisation bounds. Extensive experiments on four benchmarks in cross-city few-shot settings show CAST-CKT outperforms state-of-the-art methods by significant margins in MAE and RMSE, while offering interpretable regime analysis. Code is available at https://github.com/afofanah/CAST-CKT.

</details>


### [28] [HugRAG: Hierarchical Causal Knowledge Graph Design for RAG](https://arxiv.org/abs/2602.05143)
*Nengbo Wang,Tuo Liang,Vikash Singh,Chaoda Song,Van Yang,Yu Yin,Jing Ma,Jagdip Singh,Vipin Chaudhary*

Main category: cs.AI

TL;DR: HugRAG是一个基于因果门控的层次化图检索增强生成框架，通过显式建模因果关系来抑制虚假相关性，实现大规模知识图上的可扩展推理。


<details>
  <summary>Details</summary>
Motivation: 现有基于图的RAG方法过度依赖表层节点匹配，缺乏显式因果建模，导致答案不可靠或存在虚假关联。先前尝试融入因果关系的方法通常局限于局部或单文档上下文，且受模块化图结构的信息隔离影响，阻碍了可扩展性和跨模块因果推理。

Method: 提出HugRAG框架，通过跨层次模块的因果门控重新思考基于图的RAG知识组织方式，显式建模因果关系以抑制虚假相关性，同时支持大规模知识图上的可扩展推理。

Result: 在多个数据集和评估指标上的广泛实验表明，HugRAG始终优于竞争性的基于图的RAG基线方法。

Conclusion: 该工作为结构化、可扩展且基于因果基础的RAG系统建立了原则性基础。

Abstract: Retrieval augmented generation (RAG) has enhanced large language models by enabling access to external knowledge, with graph-based RAG emerging as a powerful paradigm for structured retrieval and reasoning. However, existing graph-based methods often over-rely on surface-level node matching and lack explicit causal modeling, leading to unfaithful or spurious answers. Prior attempts to incorporate causality are typically limited to local or single-document contexts and also suffer from information isolation that arises from modular graph structures, which hinders scalability and cross-module causal reasoning. To address these challenges, we propose HugRAG, a framework that rethinks knowledge organization for graph-based RAG through causal gating across hierarchical modules. HugRAG explicitly models causal relationships to suppress spurious correlations while enabling scalable reasoning over large-scale knowledge graphs. Extensive experiments demonstrate that HugRAG consistently outperforms competitive graph-based RAG baselines across multiple datasets and evaluation metrics. Our work establishes a principled foundation for structured, scalable, and causally grounded RAG systems.

</details>


### [29] [First Proof](https://arxiv.org/abs/2602.05192)
*Mohammed Abouzaid,Andrew J. Blumberg,Martin Hairer,Joe Kileel,Tamara G. Kolda,Paul D. Nelson,Daniel Spielman,Nikhil Srivastava,Rachel Ward,Shmuel Weinberger,Lauren Williams*

Main category: cs.AI

TL;DR: 作者分享了10个研究级数学问题来评估当前AI系统回答数学研究问题的能力，这些问题来自作者实际研究过程，之前未公开，答案暂时加密


<details>
  <summary>Details</summary>
Motivation: 评估当前人工智能系统在回答研究级数学问题方面的能力，通过提供真实的研究过程中产生的数学问题来测试AI的数学推理水平

Method: 创建了一个包含10个研究级数学问题的数据集，这些问题来自作者的实际研究过程，之前从未公开过，答案暂时加密以进行客观评估

Result: 提供了一个专门用于测试AI数学推理能力的基准数据集，包含10个真实的研究级数学问题

Conclusion: 通过分享这些研究级数学问题，为评估AI系统的数学推理能力提供了一个新的基准测试工具

Abstract: To assess the ability of current AI systems to correctly answer research-level mathematics questions, we share a set of ten math questions which have arisen naturally in the research process of the authors. The questions had not been shared publicly until now; the answers are known to the authors of the questions but will remain encrypted for a short time.

</details>


### [30] [Traceable Cross-Source RAG for Chinese Tibetan Medicine Question Answering](https://arxiv.org/abs/2602.05195)
*Fengxian Chen,Zhilong Tao,Jiaxuan Li,Yunlong Li,Qingguo Zhou*

Main category: cs.AI

TL;DR: 该论文针对多知识库检索增强生成中的权威性偏差问题，提出DAKS路由和预算检索以及对齐图证据融合方法，在藏医药领域实现更好的跨知识库证据覆盖和可追溯性。


<details>
  <summary>Details</summary>
Motivation: 在中文藏医药领域，检索增强生成面临多异构知识库的挑战。百科全书条目通常密集且易于匹配，容易主导检索结果，即使经典文献或临床论文提供更权威的证据。需要解决密度驱动的偏差问题，优先考虑权威来源，并实现跨知识库验证。

Method: 提出两种互补方法：1) DAKS进行知识库路由和预算检索，缓解密度驱动偏差，在适当时优先考虑权威来源；2) 使用对齐图指导证据融合和覆盖感知打包，在不依赖简单拼接的情况下改善跨知识库证据覆盖。所有答案由轻量级生成器openPangu-Embedded-7B生成。

Result: 实验显示在路由质量和跨知识库证据覆盖方面取得一致增益，完整系统在保持强忠实性和引用正确性的同时，实现了最佳的CrossEv@5指标。

Conclusion: 提出的方法在多知识库检索增强生成中有效解决了权威性偏差问题，提高了跨知识库证据覆盖和可追溯性，在藏医药领域具有实用价值。

Abstract: Retrieval-augmented generation (RAG) promises grounded question answering, yet domain settings with multiple heterogeneous knowledge bases (KBs) remain challenging. In Chinese Tibetan medicine, encyclopedia entries are often dense and easy to match, which can dominate retrieval even when classics or clinical papers provide more authoritative evidence. We study a practical setting with three KBs (encyclopedia, classics, and clinical papers) and a 500-query benchmark (cutoff $K{=}5$) covering both single-KB and cross-KB questions. We propose two complementary methods to improve traceability, reduce hallucinations, and enable cross-KB verification. First, DAKS performs KB routing and budgeted retrieval to mitigate density-driven bias and to prioritize authoritative sources when appropriate. Second, we use an alignment graph to guide evidence fusion and coverage-aware packing, improving cross-KB evidence coverage without relying on naive concatenation. All answers are generated by a lightweight generator, \textsc{openPangu-Embedded-7B}. Experiments show consistent gains in routing quality and cross-KB evidence coverage, with the full system achieving the best CrossEv@5 while maintaining strong faithfulness and citation correctness.

</details>


### [31] [Surgery: Mitigating Harmful Fine-Tuning for Large Language Models via Attention Sink](https://arxiv.org/abs/2602.05228)
*Guozhi Liu,Weiwei Lin,Tiansheng Huang,Ruichao Mo,Qi Mu,Xiumin Wang,Li Shen*

Main category: cs.AI

TL;DR: 提出Surgery方法，利用注意力汇聚机制中的sink divergence统计量来防御有害微调，通过正则化抑制正sink divergence，减少模型学习有害模式


<details>
  <summary>Details</summary>
Motivation: 有害微调会破坏大语言模型的安全对齐，带来严重安全风险，需要有效的防御方法

Method: 提出Surgery防御方法：1) 测量每个注意力头的sink divergence统计量；2) 基于可分离sink divergence假设，使用正则化器抑制sink divergence，引导注意力头向负sink divergence组移动

Result: 在BeaverTails、HarmBench和SorryBench基准测试中，Surgery分别提升了5.90%、11.25%和9.55%的防御性能

Conclusion: Surgery方法通过利用注意力汇聚机制中的sink divergence可分离性，有效防御有害微调攻击，提升模型安全性

Abstract: Harmful fine-tuning can invalidate safety alignment of large language models, exposing significant safety risks. In this paper, we utilize the attention sink mechanism to mitigate harmful fine-tuning. Specifically, we first measure a statistic named \emph{sink divergence} for each attention head and observe that \emph{different attention heads exhibit two different signs of sink divergence}. To understand its safety implications, we conduct experiments and find that the number of attention heads of positive sink divergence increases along with the increase of the model's harmfulness when undergoing harmful fine-tuning. Based on this finding, we propose a separable sink divergence hypothesis -- \emph{attention heads associating with learning harmful patterns during fine-tuning are separable by their sign of sink divergence}. Based on the hypothesis, we propose a fine-tuning-stage defense, dubbed Surgery. Surgery utilizes a regularizer for sink divergence suppression, which steers attention heads toward the negative sink divergence group, thereby reducing the model's tendency to learn and amplify harmful patterns. Extensive experiments demonstrate that Surgery improves defense performance by 5.90\%, 11.25\%, and 9.55\% on the BeaverTails, HarmBench, and SorryBench benchmarks, respectively. Source code is available on https://github.com/Lslland/Surgery.

</details>


### [32] [Automatic Cognitive Task Generation for In-Situ Evaluation of Embodied Agents](https://arxiv.org/abs/2602.05249)
*Xinyi He,Ying Yang,Chuanjian Fu,Sihan Guo,Songchun Zhu,Lifeng Fan,Zhenliang Zhang,Yujia Peng*

Main category: cs.AI

TL;DR: 提出TEA方法，通过动态原位任务生成在未见3D环境中评估智能体，揭示现有模型在基础感知和3D交互方面存在严重缺陷


<details>
  <summary>Details</summary>
Motivation: 随着智能体将广泛部署于多样化家庭环境，针对每个独特未见3D环境的评估变得至关重要。现有基准存在严重数据污染和缺乏场景特异性问题，无法有效评估智能体在未见环境中的能力。

Method: 提出基于人类认知启发的动态原位任务生成方法TEA。通过结构化图表示定义任务，构建两阶段交互-演化任务生成系统：交互阶段智能体主动与环境交互，形成任务执行与生成的循环；演化阶段通过任务图建模重组和重用现有任务生成新任务，无需外部数据。

Result: 在10个未见场景中，TEA在两个周期内自动生成了87,876个任务，经人工验证具有物理合理性和日常认知能力覆盖。将SOTA模型与人类在生成任务上对比发现：尽管模型在公共基准上表现优异，但在基础感知任务上表现惊人地差，严重缺乏3D交互意识，且对任务类型的推理高度敏感。

Conclusion: 这些令人警醒的发现凸显了在将智能体部署到真实人类环境前进行原位评估的必要性。TEA方法为未见环境中的智能体评估提供了有效解决方案。

Abstract: As general intelligent agents are poised for widespread deployment in diverse households, evaluation tailored to each unique unseen 3D environment has become a critical prerequisite. However, existing benchmarks suffer from severe data contamination and a lack of scene specificity, inadequate for assessing agent capabilities in unseen settings. To address this, we propose a dynamic in-situ task generation method for unseen environments inspired by human cognition. We define tasks through a structured graph representation and construct a two-stage interaction-evolution task generation system for embodied agents (TEA). In the interaction stage, the agent actively interacts with the environment, creating a loop between task execution and generation that allows for continuous task generation. In the evolution stage, task graph modeling allows us to recombine and reuse existing tasks to generate new ones without external data. Experiments across 10 unseen scenes demonstrate that TEA automatically generated 87,876 tasks in two cycles, which human verification confirmed to be physically reasonable and encompassing essential daily cognitive capabilities. Benchmarking SOTA models against humans on our in-situ tasks reveals that models, despite excelling on public benchmarks, perform surprisingly poorly on basic perception tasks, severely lack 3D interaction awareness and show high sensitivity to task types in reasoning. These sobering findings highlight the necessity of in-situ evaluation before deploying agents into real-world human environments.

</details>


### [33] [PieArena: Frontier Language Agents Achieve MBA-Level Negotiation Performance and Reveal Novel Behavioral Differences](https://arxiv.org/abs/2602.05302)
*Chris Zhu,Sasha Cui,Will Sanok Dufallo,Runzhi Jin,Zhen Xu,Linjun Zhang,Daylian Cain*

Main category: cs.AI

TL;DR: GPT-5在PieArena谈判基准测试中达到或超越商学院学生水平，但中低层模型仍有改进空间，且存在欺骗、计算准确性等行为差异


<details>
  <summary>Details</summary>
Motivation: 评估LLMs在谈判这一核心商业任务中的能力，谈判需要战略推理、心智理论和经济价值创造，现有基准测试未能全面衡量这些复杂能力

Method: 引入PieArena大规模谈判基准，基于精英商学院MBA谈判课程的真实场景，进行多智能体交互评估，并研究联合意向性智能体支架的效果

Result: GPT-5达到或超越受过谈判训练的商业学院学生表现；联合意向性支架对中低层模型有显著提升，但对前沿模型收益递减；发现模型在欺骗、计算准确性、指令遵从和感知声誉等方面的行为异质性

Conclusion: 前沿语言智能体已具备在高风险经济环境中部署的智力和心理能力，但在鲁棒性和可信度方面仍存在挑战，需要超越仅关注交易结果的更全面评估

Abstract: We present an in-depth evaluation of LLMs' ability to negotiate, a central business task that requires strategic reasoning, theory of mind, and economic value creation. To do so, we introduce PieArena, a large-scale negotiation benchmark grounded in multi-agent interactions over realistic scenarios drawn from an MBA negotiation course at an elite business school. We find systematic evidence of AGI-level performance in which a representative frontier agent (GPT-5) matches or outperforms trained business-school students, despite a semester of general negotiation instruction and targeted coaching immediately prior to the task. We further study the effects of joint-intentionality agentic scaffolding and find asymmetric gains, with large improvements for mid- and lower-tier LMs and diminishing returns for frontier LMs. Beyond deal outcomes, PieArena provides a multi-dimensional negotiation behavioral profile, revealing novel cross-model heterogeneity, masked by deal-outcome-only benchmarks, in deception, computation accuracy, instruction compliance, and perceived reputation. Overall, our results suggest that frontier language agents are already intellectually and psychologically capable of deployment in high-stakes economic settings, but deficiencies in robustness and trustworthiness remain open challenges.

</details>


### [34] [ProAct: Agentic Lookahead in Interactive Environments](https://arxiv.org/abs/2602.05327)
*Yangbin Yu,Mingyu Yang,Junyou Li,Yiming Gao,Feiyu Liu,Yijun Yang,Zichuan Lin,Jiafei Lyu,Yicheng Liu,Zhicong Lu,Deheng Ye,Jie Jiang*

Main category: cs.AI

TL;DR: ProAct框架通过两阶段训练解决LLM智能体在交互环境中长期规划的问题：第一阶段使用GLAD进行监督微调，将复杂搜索树压缩为因果推理链；第二阶段使用MC-Critic增强策略梯度算法，通过轻量级环境模拟校准价值估计。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型智能体在需要长期规划的交互环境中表现不佳，主要原因是模拟未来状态时会产生复合错误。需要一种方法让智能体内化准确的预见性推理能力。

Method: 提出ProAct框架，包含两个阶段：1) Grounded LookAhead Distillation (GLAD)：通过基于环境的搜索轨迹进行监督微调，将复杂搜索树压缩为简洁的因果推理链；2) Monte-Carlo Critic (MC-Critic)：一个即插即用的辅助价值估计器，通过轻量级环境模拟校准价值估计，增强PPO和GRPO等策略梯度算法。

Result: 在随机环境（如2048）和确定性环境（如Sokoban）上的实验表明，ProAct显著提高了规划准确性。一个4B参数的模型使用ProAct训练后，超越了所有开源基线，并与最先进的闭源模型相媲美，同时在未见环境中表现出强大的泛化能力。

Conclusion: ProAct框架通过将搜索树压缩为因果推理链和引入轻量级环境模拟的价值校准，有效解决了LLM智能体在长期规划中的复合错误问题，显著提升了规划性能并保持了良好的泛化能力。

Abstract: Existing Large Language Model (LLM) agents struggle in interactive environments requiring long-horizon planning, primarily due to compounding errors when simulating future states. To address this, we propose ProAct, a framework that enables agents to internalize accurate lookahead reasoning through a two-stage training paradigm. First, we introduce Grounded LookAhead Distillation (GLAD), where the agent undergoes supervised fine-tuning on trajectories derived from environment-based search. By compressing complex search trees into concise, causal reasoning chains, the agent learns the logic of foresight without the computational overhead of inference-time search. Second, to further refine decision accuracy, we propose the Monte-Carlo Critic (MC-Critic), a plug-and-play auxiliary value estimator designed to enhance policy-gradient algorithms like PPO and GRPO. By leveraging lightweight environment rollouts to calibrate value estimates, MC-Critic provides a low-variance signal that facilitates stable policy optimization without relying on expensive model-based value approximation. Experiments on both stochastic (e.g., 2048) and deterministic (e.g., Sokoban) environments demonstrate that ProAct significantly improves planning accuracy. Notably, a 4B parameter model trained with ProAct outperforms all open-source baselines and rivals state-of-the-art closed-source models, while demonstrating robust generalization to unseen environments. The codes and models are available at https://github.com/GreatX3/ProAct

</details>


### [35] [RaBiT: Residual-Aware Binarization Training for Accurate and Efficient LLMs](https://arxiv.org/abs/2602.05367)
*Youngcheon You,Banseok Lee,Minseop Choi,Seonyoung Kim,Hyochan Chong,Changdong Kim,Youngmin Kim,Dongkyu Kim*

Main category: cs.AI

TL;DR: RaBiT提出了一种新的量化框架，通过算法强制残差层次结构来解决二进制路径间的特征共适应问题，在2-bit量化中实现了SOTA性能，推理速度比全精度模型快4.49倍。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的高效部署需要极端量化，但低比特效率与性能之间存在关键权衡。残差二值化虽然能实现硬件友好的无矩阵乘法推理，但存在病态特征共适应问题，特别是并行残差二进制路径学习冗余特征，限制了模型的表达能力。

Method: 提出RaBiT量化框架，通过算法强制残差层次结构来解决共适应问题。核心机制是从单个共享全精度权重顺序推导每个二进制路径，确保每个路径纠正前一个路径的误差。采用稳健初始化策略，优先考虑功能保持而非简单的权重近似。

Result: RaBiT重新定义了2-bit精度-效率前沿：实现了最先进的性能，甚至能与硬件密集的向量量化方法相媲美，在RTX 4090上比全精度模型推理速度快4.49倍。

Conclusion: RaBiT通过解决残差二值化中的特征共适应问题，为大型语言模型的极端量化提供了有效的解决方案，在保持高性能的同时显著提升了推理效率。

Abstract: Efficient deployment of large language models (LLMs) requires extreme quantization, forcing a critical trade-off between low-bit efficiency and performance. Residual binarization enables hardware-friendly, matmul-free inference by stacking binary ($\pm$1) layers, but is plagued by pathological feature co-adaptation. We identify a key failure mode, which we term inter-path adaptation: during quantization-aware training (QAT), parallel residual binary paths learn redundant features, degrading the error-compensation structure and limiting the expressive capacity of the model. While prior work relies on heuristic workarounds (e.g., path freezing) that constrain the solution space, we propose RaBiT, a novel quantization framework that resolves co-adaptation by algorithmically enforcing a residual hierarchy. Its core mechanism sequentially derives each binary path from a single shared full-precision weight, which ensures that every path corrects the error of the preceding one. This process is stabilized by a robust initialization that prioritizes functional preservation over mere weight approximation. RaBiT redefines the 2-bit accuracy-efficiency frontier: it achieves state-of-the-art performance, rivals even hardware-intensive Vector Quantization (VQ) methods, and delivers a $4.49\times$ inference speed-up over full-precision models on an RTX 4090.

</details>


### [36] [Clinical Validation of Medical-based Large Language Model Chatbots on Ophthalmic Patient Queries with LLM-based Evaluation](https://arxiv.org/abs/2602.05381)
*Ting Fang Tan,Kabilan Elangovan,Andreas Pollreisz,Kevin Bryan Dy,Wei Yan Ng,Joy Le Yi Wong,Jin Liyuan,Chrystie Quek Wan Ning,Ashley Shuen Ying Hong,Arun James Thirunavukarasu,Shelley Yin-His Chang,Jie Yao,Dylan Hong,Wang Zhaoran,Amrita Gupta,Daniel SW Ting*

Main category: cs.AI

TL;DR: 本研究评估了四个小型医疗LLM在眼科患者咨询中的表现，发现Meerkat-7B表现最佳，MedLLaMA3-v20最差且存在幻觉问题，GPT-4-Turbo评估与临床医生评分高度一致。


<details>
  <summary>Details</summary>
Motivation: 随着领域特定大语言模型在眼科患者教育、分诊和临床决策支持中的应用日益增多，需要严格评估以确保安全性和准确性。本研究旨在评估小型医疗LLM回答眼科患者咨询的能力，并探索基于LLM评估的可行性。

Method: 采用横断面研究设计，四个参数小于100亿的小型医疗LLM（Meerkat-7B、BioMistral-7B、OpenBioLLM-8B、MedLLaMA3-v20）回答180个眼科患者咨询，生成2160个回答。由三位不同资历的眼科医生和GPT-4-Turbo使用S.C.O.R.E框架（安全性、共识与背景、客观性、可重复性、可解释性）进行五级李克特量表评分。使用Spearman秩相关、Kendall tau统计和核密度估计分析评估LLM与临床医生评分的一致性。

Result: Meerkat-7B表现最佳，平均得分分别为：高级顾问3.44分、顾问4.08分、住院医师4.18分。MedLLaMA3-v20表现最差，25.5%的回答包含幻觉或临床误导内容，包括虚构术语。GPT-4-Turbo评估与临床医生整体评估高度一致（Spearman rho=0.80，Kendall tau=0.67），但高级顾问评分更为保守。

Conclusion: 医疗LLM在眼科问题回答中显示出安全潜力，但在临床深度和共识方面仍存在差距。支持基于LLM的大规模基准评估的可行性，需要混合自动化和临床医生审查框架来指导安全的临床部署。

Abstract: Domain specific large language models are increasingly used to support patient education, triage, and clinical decision making in ophthalmology, making rigorous evaluation essential to ensure safety and accuracy. This study evaluated four small medical LLMs Meerkat-7B, BioMistral-7B, OpenBioLLM-8B, and MedLLaMA3-v20 in answering ophthalmology related patient queries and assessed the feasibility of LLM based evaluation against clinician grading. In this cross sectional study, 180 ophthalmology patient queries were answered by each model, generating 2160 responses. Models were selected for parameter sizes under 10 billion to enable resource efficient deployment. Responses were evaluated by three ophthalmologists of differing seniority and by GPT-4-Turbo using the S.C.O.R.E. framework assessing safety, consensus and context, objectivity, reproducibility, and explainability, with ratings assigned on a five point Likert scale. Agreement between LLM and clinician grading was assessed using Spearman rank correlation, Kendall tau statistics, and kernel density estimate analyses. Meerkat-7B achieved the highest performance with mean scores of 3.44 from Senior Consultants, 4.08 from Consultants, and 4.18 from Residents. MedLLaMA3-v20 performed poorest, with 25.5 percent of responses containing hallucinations or clinically misleading content, including fabricated terminology. GPT-4-Turbo grading showed strong alignment with clinician assessments overall, with Spearman rho of 0.80 and Kendall tau of 0.67, though Senior Consultants graded more conservatively. Overall, medical LLMs demonstrated potential for safe ophthalmic question answering, but gaps remained in clinical depth and consensus, supporting the feasibility of LLM based evaluation for large scale benchmarking and the need for hybrid automated and clinician review frameworks to guide safe clinical deployment.

</details>


### [37] [H-AdminSim: A Multi-Agent Simulator for Realistic Hospital Administrative Workflows with FHIR Integration](https://arxiv.org/abs/2602.05407)
*Jun-Min Lee,Meong Hi Son,Edward Choi*

Main category: cs.AI

TL;DR: H-AdminSim是一个端到端的医院管理模拟框架，结合真实数据生成和多智能体模拟，用于系统评估LLM在医院管理自动化中的性能。


<details>
  <summary>Details</summary>
Motivation: 大型医院每天处理超过10,000个管理请求，对LLM自动化有强烈需求。现有研究主要关注医患交互或孤立的管理子任务，未能捕捉真实管理流程的复杂性。

Method: 提出H-AdminSim框架，结合真实数据生成和多智能体模拟医院管理流程。通过FHIR集成提供统一可互操作的环境，使用详细评分标准定量评估任务。

Result: 该框架为评估LLM驱动的管理自动化提供了标准化测试平台，支持跨异构医院环境的统一测试。

Conclusion: H-AdminSim填补了医院管理自动化研究的空白，为系统比较LLM性能提供了全面、可互操作的模拟环境。

Abstract: Hospital administration departments handle a wide range of operational tasks and, in large hospitals, process over 10,000 requests per day, driving growing interest in LLM-based automation. However, prior work has focused primarily on patient--physician interactions or isolated administrative subtasks, failing to capture the complexity of real administrative workflows. To address this gap, we propose H-AdminSim, a comprehensive end-to-end simulation framework that combines realistic data generation with multi-agent-based simulation of hospital administrative workflows. These tasks are quantitatively evaluated using detailed rubrics, enabling systematic comparison of LLMs. Through FHIR integration, H-AdminSim provides a unified and interoperable environment for testing administrative workflows across heterogeneous hospital settings, serving as a standardized testbed for assessing the feasibility and performance of LLM-driven administrative automation.

</details>


### [38] [THOR: Inductive Link Prediction over Hyper-Relational Knowledge Graphs](https://arxiv.org/abs/2602.05424)
*Weijian Yu,Yuhuan Lu,Dingqi Yang*

Main category: cs.AI

TL;DR: THOR：一种用于超关系知识图谱的归纳式链接预测方法，通过关系基础图和实体基础图建模跨图谱的结构不变性，支持完全归纳推理。


<details>
  <summary>Details</summary>
Motivation: 现有超关系知识图谱链接预测方法大多局限于转导式设置，只能在同一词汇表内进行预测，缺乏对未见词汇表的泛化能力。需要开发能够跨不同词汇表进行归纳推理的方法。

Method: 提出THOR方法：1) 构建关系基础图和实体基础图，建模超关系知识图谱中关系和实体的基本交互模式；2) 使用两个并行图编码器学习基础图表示；3) 通过Transformer解码器支持高效的掩码训练和完全归纳推理。

Result: 在12个数据集上的实验表明，THOR显著优于现有方法：比最佳规则方法提升66.1%，比最佳半归纳方法提升55.9%，比最佳完全归纳方法提升20.4%。消融研究验证了捕捉跨图谱结构不变性的关键设计因素。

Conclusion: THOR通过建模超关系知识图谱中的基础交互模式，实现了有效的完全归纳链接预测，显著提升了跨词汇表的泛化能力，为超关系知识图谱的归纳推理提供了新思路。

Abstract: Knowledge graphs (KGs) have become a key ingredient supporting a variety of applications. Beyond the traditional triplet representation of facts where a relation connects two entities, modern KGs observe an increasing number of hyper-relational facts, where an arbitrary number of qualifiers associated with a triplet provide auxiliary information to further describe the rich semantics of the triplet, which can effectively boost the reasoning performance in link prediction tasks. However, existing link prediction techniques over such hyper-relational KGs (HKGs) mostly focus on a transductive setting, where KG embedding models are learned from the specific vocabulary of a given KG and subsequently can only make predictions within the same vocabulary, limiting their generalizability to previously unseen vocabularies. Against this background, we propose THOR, an inducTive link prediction technique for Hyper-relational knOwledge gRaphs. Specifically, we first introduce both relation and entity foundation graphs, modeling their fundamental inter- and intra-fact interactions in HKGs, which are agnostic to any specific relations and entities. Afterward, THOR is designed to learn from the two foundation graphs with two parallel graph encoders followed by a transformer decoder, which supports efficient masked training and fully-inductive inference. We conduct a thorough evaluation of THOR in hyper-relational link prediction tasks on 12 datasets with different settings. Results show that THOR outperforms a sizable collection of baselines, yielding 66.1%, 55.9%, and 20.4% improvement over the best-performing rule-based, semi-inductive, and fully-inductive techniques, respectively. A series of ablation studies also reveals our key design factors capturing the structural invariance transferable across HKGs for inductive tasks.

</details>


### [39] [Day-Ahead Electricity Price Forecasting for Volatile Markets Using Foundation Models with Regularization Strategy](https://arxiv.org/abs/2602.05430)
*Kritchanat Ponyuenyong,Pengyu Tu,Jia Wei Tan,Wei Soon Cheong,Jamie Ng Suat Ling,Lianlian Jiang*

Main category: cs.AI

TL;DR: 该研究评估了时间序列基础模型在波动性电力市场中的日前电价预测性能，发现相比传统统计和深度学习模型，TSFMs在MAPE指标上最高提升37.4%。


<details>
  <summary>Details</summary>
Motivation: 电力价格预测对能源市场参与者至关重要，但由于价格信号的固有波动性和非线性而具有挑战性。传统统计和深度学习模型难以有效捕捉复杂的时间依赖关系并整合异构数据。虽然时间序列基础模型在一般时间序列预测任务中表现出色，但在波动性市场中的日前电价预测效果尚未充分探索。

Method: 提出尖峰正则化策略，评估多种时间序列基础模型（包括Tiny Time Mixers、MOIRAI、MOMENT、TimesFM），并与传统统计模型（ARIMA）和深度学习模型（LSTM、CNN-LSTM）进行比较。使用新加坡具有波动趋势的半小时批发市场数据，并在适用模型中纳入外生因素（如天气和日历变量）。

Result: 时间序列基础模型在所有评估设置中一致优于传统方法，在MAPE指标上最高实现37.4%的改进。尖峰正则化策略有助于提升模型在波动市场中的预测性能。

Conclusion: 时间序列基础模型在波动性电力市场中具有显著优势，为提升预测准确性和决策制定提供了实用指导。该研究填补了TSFMs在日前电价预测领域的应用空白，特别是在高波动性市场环境中的表现验证。

Abstract: Electricity price forecasting (EPF) is essential for energy markets stakeholders (e.g. grid operators, energy traders, policymakers) but remains challenging due to the inherent volatility and nonlinearity of price signals. Traditional statistical and deep learning (DL) models often struggle to capture complex temporal dependencies and integrate heterogeneous data effectively. While time series foundation models (TSFMs) have shown strong performance in general time series forecasting tasks, such as traffic forecasting and weather forecasting. However, their effectiveness in day-ahead EPF, particularly in volatile markets, remains underexplored. This paper presents a spike regularization strategy and evaluates a wide range of TSFMs, including Tiny Time Mixers (TTMs), MOIRAI, MOMENT, and TimesFM, against traditional statistical and DL models such as Autoregressive Integrated Moving Average (ARIMA), Long-short Term Memory (LSTM), and Convolutional Neural Network - LSTM (CNN-LSTM) using half-hourly wholesale market data with volatile trends in Singapore. Exogenous factors (e.g. weather and calendar variables) are also incorporated into models where applicable. Results demonstrate that TSFMs consistently outperform traditional approaches, achieving up to 37.4% improvement in MAPE across various evaluation settings. The findings offer practical guidance for improving forecast accuracy and decision-making in volatile electricity markets.

</details>


### [40] [Refine and Purify: Orthogonal Basis Optimization with Null-Space Denoising for Conditional Representation Learning](https://arxiv.org/abs/2602.05464)
*Jiaquan Wang,Yan Lyu,Chen Li,Yuheng Jia*

Main category: cs.AI

TL;DR: OD-CRL提出了一种新的条件表示学习框架，通过自适应正交基优化和零空间去噪投影来解决现有方法对子空间基敏感和子空间间干扰的问题。


<details>
  <summary>Details</summary>
Motivation: 现有条件表示学习方法将通用特征投影到LLM生成的文本基张成的条件特征子空间，但面临两个关键限制：对子空间基的敏感性以及子空间间干扰的脆弱性。

Method: 提出OD-CRL框架，包含自适应正交基优化（AOBO）和零空间去噪投影（NSDP）。AOBO通过奇异值分解和基于曲率的截断构建正交语义基；NSDP通过将嵌入投影到无关子空间的零空间来抑制非目标语义干扰。

Result: 在定制化聚类、定制化分类和定制化检索任务上的广泛实验表明，OD-CRL实现了新的最先进性能，并具有优越的泛化能力。

Conclusion: OD-CRL通过正交基优化和零空间投影有效解决了条件表示学习中的子空间敏感性和干扰问题，在各种定制化任务中取得了显著性能提升。

Abstract: Conditional representation learning aims to extract criterion-specific features for customized tasks. Recent studies project universal features onto the conditional feature subspace spanned by an LLM-generated text basis to obtain conditional representations. However, such methods face two key limitations: sensitivity to subspace basis and vulnerability to inter-subspace interference. To address these challenges, we propose OD-CRL, a novel framework integrating Adaptive Orthogonal Basis Optimization (AOBO) and Null-Space Denoising Projection (NSDP). Specifically, AOBO constructs orthogonal semantic bases via singular value decomposition with a curvature-based truncation. NSDP suppresses non-target semantic interference by projecting embeddings onto the null space of irrelevant subspaces. Extensive experiments conducted across customized clustering, customized classification, and customized retrieval tasks demonstrate that OD-CRL achieves a new state-of-the-art performance with superior generalization.

</details>


### [41] [ALIVE: Awakening LLM Reasoning via Adversarial Learning and Instructive Verbal Evaluation](https://arxiv.org/abs/2602.05472)
*Yiwen Duan,Jing Ye,Xinpei Zhao*

Main category: cs.AI

TL;DR: ALIVE框架通过对抗学习和指导性语言反馈，让LLM内部化推理逻辑，无需人工监督即可实现专家级推理能力


<details>
  <summary>Details</summary>
Motivation: 传统强化学习依赖标量奖励存在三个主要问题：扩展成本高、跨领域脆弱性、无法理解解决方案的内在逻辑。这种对外部贫乏信号的依赖阻碍了模型发展对推理原则的深层理解。

Method: ALIVE框架基于"认知协同"原则，将问题提出、解决和评判统一在单一策略模型中。通过对抗学习和指导性语言反馈，使模型能够直接从原始语料库中内部化评估标准，将外部批评转化为内生的推理能力。

Result: 在数学推理、代码生成和一般逻辑推理基准测试中，ALIVE持续缓解了奖励信号限制。在相同数据和计算条件下，实现了准确率提升、显著改善的跨领域泛化能力以及更高的自我纠正率。

Conclusion: 推理三位一体（问题提出、解决、评判）促进了能力增长的自我维持轨迹，使ALIVE成为无需人工监督的通用推理对齐的可扩展基础。

Abstract: The quest for expert-level reasoning in Large Language Models (LLMs) has been hampered by a persistent \textit{reward bottleneck}: traditional reinforcement learning (RL) relies on scalar rewards that are \textbf{costly} to scale, \textbf{brittle} across domains, and \textbf{blind} to the underlying logic of a solution. This reliance on external, impoverished signals prevents models from developing a deep, self-contained understanding of reasoning principles. We introduce \textbf{ALIVE} (\emph{Adversarial Learning with Instructive Verbal Evaluation}), a hands-free alignment framework that moves beyond scalar reward optimization toward intrinsic reasoning acquisition. Grounded in the principle of \emph{Cognitive Synergy}, ALIVE unifies problem posing, solving, and judging within a single policy model to internalize the logic of correctness. By coupling adversarial learning with instructive verbal feedback, ALIVE enables models to internalize evaluative criteria directly from raw corpora, effectively transforming external critiques into an endogenous reasoning faculty. Empirical evaluations across mathematical reasoning, code generation, and general logical inference benchmarks demonstrate that ALIVE consistently mitigates reward signal limitations. With identical data and compute, it achieves accuracy gains, markedly improved cross-domain generalization, and higher self-correction rates. These results indicate that the reasoning trinity fosters a self-sustaining trajectory of capability growth, positioning ALIVE as a scalable foundation for general-purpose reasoning alignment without human-in-the-loop supervision.

</details>


### [42] [Phi-Former: A Pairwise Hierarchical Approach for Compound-Protein Interactions Prediction](https://arxiv.org/abs/2602.05479)
*Zhe Wang,Zijing Liu,Chencheng Xu,Yuan Yao*

Main category: cs.AI

TL;DR: Phi-former是一种用于预测化合物-蛋白质相互作用的深度学习方法，通过分层表示和成对预训练框架，在原子-原子、基序-基序和原子-基序三个层次上建模相互作用，提高了预测准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 药物发现过程耗时耗力且昂贵，预测化合物-蛋白质相互作用是关键环节。现有深度学习方法虽然提高了效率，但通常基于原子层面建模，未能充分考虑化学现实——分子片段（基序或功能基团）通常是生物识别和结合的主要单位，导致模型与化学现实不完全一致。

Method: 提出Phi-former（成对分层相互作用表示学习方法）：1）对化合物和蛋白质进行分层表示；2）采用成对预训练框架，系统建模原子-原子、基序-基序和原子-基序三个层次的相互作用；3）设计层内和层间学习流程，使不同相互作用层次相互促进。

Result: 实验结果表明Phi-former在CPI相关任务上表现出优越性能。案例研究显示该方法能准确识别在CPI中被激活的特定原子或基序，提供可解释的模型解释。

Conclusion: Phi-former通过结合基序在CPI中的生物学作用，提供了更符合化学现实的化合物-蛋白质相互作用预测方法。该方法获得的见解可指导理性药物设计并支持精准医学应用。

Abstract: Drug discovery remains time-consuming, labor-intensive, and expensive, often requiring years and substantial investment per drug candidate. Predicting compound-protein interactions (CPIs) is a critical component in this process, enabling the identification of molecular interactions between drug candidates and target proteins. Recent deep learning methods have successfully modeled CPIs at the atomic level, achieving improved efficiency and accuracy over traditional energy-based approaches. However, these models do not always align with chemical realities, as molecular fragments (motifs or functional groups) typically serve as the primary units of biological recognition and binding. In this paper, we propose Phi-former, a pairwise hierarchical interaction representation learning method that addresses this gap by incorporating the biological role of motifs in CPIs. Phi-former represents compounds and proteins hierarchically and employs a pairwise pre-training framework to model interactions systematically across atom-atom, motif-motif, and atom-motif levels, reflecting how biological systems recognize molecular partners. We design intra-level and inter-level learning pipelines that make different interaction levels mutually beneficial. Experimental results demonstrate that Phi-former achieves superior performance on CPI-related tasks. A case study shows that our method accurately identifies specific atoms or motifs activated in CPIs, providing interpretable model explanations. These insights may guide rational drug design and support precision medicine applications.

</details>


### [43] [SDFP: Speculative Decoding with FIT-Pruned Models for Training-Free and Plug-and-Play LLM Acceleration](https://arxiv.org/abs/2602.05499)
*Hanyu Wei,Zunhai Su,Peng Lu,Chao Li,Spandan Tiwari,Ashish Sirasao,Yuhan Dong*

Main category: cs.AI

TL;DR: SDFP是一种无需训练、即插即用的推测解码框架，通过基于Fisher信息迹的层剪枝从原始LLM构建轻量级草稿模型，实现1.32-1.5倍的解码加速。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在多媒体应用中存在自回归解码延迟高的问题。现有推测解码方法需要额外训练或维护草稿模型，部署成本高且复杂。

Method: 使用Fisher信息迹评估层敏感性，剪除对输出影响小的层，从原始LLM构建紧凑草稿模型，保持与原始模型的兼容性用于标准推测验证。

Result: 在基准测试中实现1.32-1.5倍解码加速，不改变目标模型的输出分布，支持低延迟多媒体应用。

Conclusion: SDFP提供了一种无需训练、即插即用的草稿模型构建方法，显著降低推测解码部署复杂度，为多媒体应用提供高效低延迟解决方案。

Abstract: Large language models (LLMs) underpin interactive multimedia applications such as captioning, retrieval, recommendation, and creative content generation, yet their autoregressive decoding incurs substantial latency. Speculative decoding reduces latency using a lightweight draft model, but deployment is often limited by the cost and complexity of acquiring, tuning, and maintaining an effective draft model. Recent approaches usually require auxiliary training or specialization, and even training-free methods incur costly search or optimization. We propose SDFP, a fully training-free and plug-and-play framework that builds the draft model via Fisher Information Trace (FIT)-based layer pruning of a given LLM. Using layer sensitivity as a proxy for output perturbation, SDFP removes low-impact layers to obtain a compact draft while preserving compatibility with the original model for standard speculative verification. SDFP needs no additional training, hyperparameter tuning, or separately maintained drafts, enabling rapid, deployment-friendly draft construction. Across benchmarks, SDFP delivers 1.32x-1.5x decoding speedup without altering the target model's output distribution, supporting low-latency multimedia applications.

</details>


### [44] [A Unified Multimodal Framework for Dataset Construction and Model-Based Diagnosis of Ameloblastoma](https://arxiv.org/abs/2602.05515)
*Ajo Babu George,Anna Mariam John,Athul Anoop,Balu Bhasuran*

Main category: cs.AI

TL;DR: 研究人员创建了一个专门针对成釉细胞瘤的多模态数据集，并开发了相应的深度学习模型，显著提高了变体分类准确性和异常组织检测性能。


<details>
  <summary>Details</summary>
Motivation: 当前AI在颌面病理诊断中面临数据质量不高、格式不一致的问题，特别是针对成釉细胞瘤的覆盖有限，需要结构化、高质量的多模态数据集来支持模型训练。

Method: 1. 创建专门针对成釉细胞瘤的多模态数据集，整合放射学、组织病理学和口腔内临床图像；2. 使用自然语言处理技术从文本报告中提取临床相关特征；3. 对图像数据进行领域特定的预处理和增强；4. 开发多模态深度学习模型，可接受临床输入如主诉、年龄、性别以增强个性化推理。

Result: 1. 变体分类准确率从46.2%提升至65.9%；2. 异常组织检测F1分数从43.0%提升至90.3%；3. 相比MultiCaRe等现有资源，提供了更强大的数据集和适应性强的多模态AI框架。

Conclusion: 这项工作通过提供高质量的多模态数据集和可适应的AI框架，显著提升了成釉细胞瘤的诊断准确性和个性化决策支持能力，为患者特异性决策支持系统的发展做出了重要贡献。

Abstract: Artificial intelligence (AI)-enabled diagnostics in maxillofacial pathology require structured, high-quality multimodal datasets. However, existing resources provide limited ameloblastoma coverage and lack the format consistency needed for direct model training. We present a newly curated multimodal dataset specifically focused on ameloblastoma, integrating annotated radiological, histopathological, and intraoral clinical images with structured data derived from case reports. Natural language processing techniques were employed to extract clinically relevant features from textual reports, while image data underwent domain specific preprocessing and augmentation. Using this dataset, a multimodal deep learning model was developed to classify ameloblastoma variants, assess behavioral patterns such as recurrence risk, and support surgical planning. The model is designed to accept clinical inputs such as presenting complaint, age, and gender during deployment to enhance personalized inference. Quantitative evaluation demonstrated substantial improvements; variant classification accuracy increased from 46.2 percent to 65.9 percent, and abnormal tissue detection F1-score improved from 43.0 percent to 90.3 percent. Benchmarked against resources like MultiCaRe, this work advances patient-specific decision support by providing both a robust dataset and an adaptable multimodal AI framework.

</details>


### [45] [Split Personality Training: Revealing Latent Knowledge Through Alternate Personalities](https://arxiv.org/abs/2602.05532)
*Florian Dietz,William Wale,Oscar Gilg,Robert McCarthy,Felix Michalak,Gustavo Ewbank Rodrigues Danon,Miguelito de Guzman,Dietrich Klakow*

Main category: cs.AI

TL;DR: SPT方法通过在LoRA参数中训练"诚实人格"来检测LLM中的错位行为，当激活该人格时可以审查主模型的响应并访问其潜在状态，在Anthropic审计游戏基准测试中达到96%准确率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型中的错位检测具有挑战性，因为模型可能在训练中学会隐藏不当行为。标准审计技术存在不足：黑盒方法难以区分错位输出与良性输出，而机制可解释性无法随模型能力扩展。

Method: 引入分裂人格训练(SPT)，在LoRA参数中微调第二个"诚实人格"，该人格在正常操作中保持非活跃状态。主模型响应后，激活LoRA适配器并插入触发字符串，使诚实人格能够审查响应同时访问主模型的潜在状态。

Result: 在Anthropic审计游戏模型生物基准测试中，SPT达到96%的整体准确率，而Anthropic报告接近0%准确率。诚实人格揭示了外部观察者无法访问的潜在知识，如受损模型训练时使用的虚构偏见。

Conclusion: SPT提供了一种有效检测LLM错位行为的方法，通过训练独立的诚实人格来审查主模型响应，能够揭示隐藏的错位行为，为模型安全审计提供了新途径。

Abstract: Detecting misalignment in large language models is challenging because models may learn to conceal misbehavior during training. Standard auditing techniques fall short: black-box methods often cannot distinguish misaligned outputs from benign ones, and mechanistic interpretability does not scale with model capabilities. We introduce Split Personality Training (SPT), which fine-tunes a second ``honest persona'' into LoRA parameters that remain inactive during normal operation. After the main model responds, we activate the LoRA adapter and insert a trigger string, enabling the honest persona to review the response while accessing the main model's latent states. We test our method on the Anthropic Auditing Game Model Organism, a benchmark where Llama-3.3-70B is trained to exploit reward hacks while concealing this behavior. SPT achieves 96% overall accuracy, whereas Anthropic reports near 0% accuracy. The honest persona reveals latent knowledge inaccessible to external observers, such as the fictional biases the compromised model was trained on.

</details>


### [46] [Emulating Aggregate Human Choice Behavior and Biases with GPT Conversational Agents](https://arxiv.org/abs/2602.05597)
*Stephen Pilli,Vivek Nallur*

Main category: cs.AI

TL;DR: 该研究探讨了LLM是否能预测个体层面的认知偏差，并在认知负荷等情境因素下模拟人类偏见行为。通过人类实验和LLM模拟，发现GPT-4和GPT-5能精确复现人类偏见，但模型间存在差异。


<details>
  <summary>Details</summary>
Motivation: 虽然已知LLM能复现常见认知偏差，但更关键的问题是LLM能否预测个体层面的偏差，并在认知负荷等情境因素与偏差相互作用时模拟人类偏见行为的动态变化。

Method: 将三个经典决策场景转化为对话设置，进行人类实验(N=1100)，参与者与促进决策的聊天机器人互动。使用参与者人口统计数据和对话记录，基于GPT-4和GPT-5模拟相同交互条件。

Result: 人类实验显示出显著的认知偏差。LLM能精确复现人类偏见，但GPT-4和GPT-5在模拟人类行为对齐方面存在明显差异。

Conclusion: LLM能有效模拟人类决策中的认知偏差，这对设计交互情境下适应性强、具备偏差意识的LLM系统具有重要启示。

Abstract: Cognitive biases often shape human decisions. While large language models (LLMs) have been shown to reproduce well-known biases, a more critical question is whether LLMs can predict biases at the individual level and emulate the dynamics of biased human behavior when contextual factors, such as cognitive load, interact with these biases. We adapted three well-established decision scenarios into a conversational setting and conducted a human experiment (N=1100). Participants engaged with a chatbot that facilitates decision-making through simple or complex dialogues. Results revealed robust biases. To evaluate how LLMs emulate human decision-making under similar interactive conditions, we used participant demographics and dialogue transcripts to simulate these conditions with LLMs based on GPT-4 and GPT-5. The LLMs reproduced human biases with precision. We found notable differences between models in how they aligned human behavior. This has important implications for designing and evaluating adaptive, bias-aware LLM-based AI systems in interactive contexts.

</details>


### [47] [Reactive Knowledge Representation and Asynchronous Reasoning](https://arxiv.org/abs/2602.05625)
*Simon Kohaut,Benedict Flade,Julian Eggert,Kristian Kersting,Devendra Singh Dhami*

Main category: cs.AI

TL;DR: 提出Resin概率编程语言和Reactive Circuits结构，通过异步反应式推理实现高效精确推理，在无人机群仿真中实现数量级加速


<details>
  <summary>Details</summary>
Motivation: 复杂概率模型中的精确推理计算成本过高，现有方法在动态环境中效率低下，无法利用现实世界信息流更新速率的异质性

Method: 1. 提出Resin概率编程语言，融合概率逻辑与反应式编程；2. 提出Reactive Circuits作为元结构，基于代数电路和异步数据流构建时间动态有向无环图，根据输入信号波动性自适应调整

Result: 在高保真无人机群仿真中，相比频率无关推理实现数量级加速；RCs的结构适应成功捕捉环境动态，显著降低延迟，促进反应式实时推理

Conclusion: 通过基于异步输入变化频率划分计算，将大型推理任务分解为可记忆化的子问题，确保仅重新评估受新信息影响的模型组件，大幅减少流式上下文中的冗余计算

Abstract: Exact inference in complex probabilistic models often incurs prohibitive computational costs. This challenge is particularly acute for autonomous agents in dynamic environments that require frequent, real-time belief updates. Existing methods are often inefficient for ongoing reasoning, as they re-evaluate the entire model upon any change, failing to exploit that real-world information streams have heterogeneous update rates. To address this, we approach the problem from a reactive, asynchronous, probabilistic reasoning perspective. We first introduce Resin (Reactive Signal Inference), a probabilistic programming language that merges probabilistic logic with reactive programming. Furthermore, to provide efficient and exact semantics for Resin, we propose Reactive Circuits (RCs). Formulated as a meta-structure over Algebraic Circuits and asynchronous data streams, RCs are time-dynamic Directed Acyclic Graphs that autonomously adapt themselves based on the volatility of input signals. In high-fidelity drone swarm simulations, our approach achieves several orders of magnitude of speedup over frequency-agnostic inference. We demonstrate that RCs' structural adaptations successfully capture environmental dynamics, significantly reducing latency and facilitating reactive real-time reasoning. By partitioning computations based on the estimated Frequency of Change in the asynchronous inputs, large inference tasks can be decomposed into individually memoized sub-problems. This ensures that only the specific components of a model affected by new information are re-evaluated, drastically reducing redundant computation in streaming contexts.

</details>


### [48] [Generative Ontology: When Structured Knowledge Learns to Create](https://arxiv.org/abs/2602.05636)
*Benny Cheung*

Main category: cs.AI

TL;DR: 提出Generative Ontology框架，结合传统本体论的结构严谨性和大语言模型的创造力，通过可执行的Pydantic模式约束LLM生成，用于生成结构完整且创新的领域设计。


<details>
  <summary>Details</summary>
Motivation: 传统本体论擅长描述领域结构但无法生成新内容，而大语言模型能流畅生成但缺乏结构有效性，经常产生没有组件的机制、没有结束条件的目标等幻觉。需要结合两者的互补优势。

Method: 将领域知识编码为可执行的Pydantic模式，通过DSPy签名约束LLM生成。采用多智能体管道，为不同本体领域分配专门角色（如机制架构师、主题编织者、平衡批评家），每个智能体带有专业"焦虑"防止浅层输出。使用检索增强生成将新设计基于现有范例，并通过迭代验证确保机制与组件的一致性。

Result: 通过GameGrammar系统展示了该框架，能够根据主题提示（如"洞穴生态系统中发光真菌的竞争"）生成结构完整、可玩的桌面游戏设计，包括机制、组件、胜利条件和设置说明，既满足本体约束又保持真正的创造性。

Conclusion: 该模式可推广到游戏以外的领域，任何具有专业词汇、有效性约束和积累范例的领域（如音乐创作、软件架构、烹饪艺术）都适合使用Generative Ontology。约束不仅不限制创造力，反而使其成为可能，正如语法使诗歌成为可能一样。

Abstract: Traditional ontologies excel at describing domain structure but cannot generate novel artifacts. Large language models generate fluently but produce outputs that lack structural validity, hallucinating mechanisms without components, goals without end conditions. We introduce Generative Ontology, a framework that synthesizes these complementary strengths: ontology provides the grammar; the LLM provides the creativity.
  Generative Ontology encodes domain knowledge as executable Pydantic schemas that constrain LLM generation via DSPy signatures. A multi-agent pipeline assigns specialized roles to different ontology domains: a Mechanics Architect designs game systems, a Theme Weaver integrates narrative, a Balance Critic identifies exploits. Each agent carrying a professional "anxiety" that prevents shallow, agreeable outputs. Retrieval-augmented generation grounds novel designs in precedents from existing exemplars, while iterative validation ensures coherence between mechanisms and components.
  We demonstrate the framework through GameGrammar, a system for generating complete tabletop game designs. Given a thematic prompt ("bioluminescent fungi competing in a cave ecosystem"), the pipeline produces structurally complete, playable game specifications with mechanisms, components, victory conditions, and setup instructions. These outputs satisfy ontological constraints while remaining genuinely creative.
  The pattern generalizes beyond games. Any domain with expert vocabulary, validity constraints, and accumulated exemplars (music composition, software architecture, culinary arts) is a candidate for Generative Ontology. We argue that constraints do not limit creativity but enable it: just as grammar makes poetry possible, ontology makes structured generation possible.

</details>


### [49] [Determining Energy Efficiency Sweet Spots in Production LLM Inference](https://arxiv.org/abs/2602.05695)
*Hiari Pizzini Cavagna,Andrea Proia,Giacomo Madella,Giovanni B. Esposito,Francesco Antici,Daniele Cesarini,Zeynep Kiziltan,Andrea Bartolini*

Main category: cs.AI

TL;DR: 该论文提出了一种基于Transformer架构计算和内存访问复杂度的分析模型，能够准确描述LLM推理能效与输入输出序列长度的非线性关系，发现存在能效"甜点"区域，通过调整序列长度可显著降低能耗。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常通过输入输出序列长度的简单线性函数来估计LLM推理能耗，但实际观察显示能效存在明显的非线性依赖关系，需要更准确的模型来理解和优化LLM的能源足迹。

Method: 基于Transformer架构的计算和内存访问复杂度推导出分析模型，使用TensorRT-LLM在NVIDIA H100 GPU上评估多种LLM（OPT、LLaMA、Gemma、Falcon、Qwen2、Granite，1B-9B参数）的能耗，测试输入输出长度从64到4096个token。

Result: 模型平均MAPE为1.79%，能效存在"甜点"区域：短到中等输入和中等长度输出时能效最高，而长输入或非常短的输出会导致能效急剧下降。将序列长度与这些能效"甜点"对齐可显著降低能耗。

Conclusion: 提出的分析模型能准确描述LLM推理能效曲线，支持生产系统中基于能效优化的序列截断、摘要和自适应生成策略，为降低LLM推理能耗提供了理论依据和实用指导。

Abstract: Large Language Models (LLMs) inference is central in modern AI applications, making it critical to understand their energy footprint. Existing approaches typically estimate energy consumption through simple linear functions of input and output sequence lengths, yet our observations reveal clear Energy Efficiency regimes: peak efficiency occurs with short-to-moderate inputs and medium-length outputs, while efficiency drops sharply for long inputs or very short outputs, indicating a non-linear dependency. In this work, we propose an analytical model derived from the computational and memory-access complexity of the Transformer architecture, capable of accurately characterizing the efficiency curve as a function of input and output lengths. To assess its accuracy, we evaluate energy consumption using TensorRT-LLM on NVIDIA H100 GPUs across a diverse set of LLMs ranging from 1B to 9B parameters, including OPT, LLaMA, Gemma, Falcon, Qwen2, and Granite, tested over input and output lengths from 64 to 4096 tokens, achieving a mean MAPE of 1.79%. Our results show that aligning sequence lengths with these efficiency "Sweet Spots" can substantially reduce energy usage, supporting informed truncation, summarization, and adaptive generation strategies in production systems.

</details>


### [50] [Nonlinearity as Rank: Generative Low-Rank Adapter with Radial Basis Functions](https://arxiv.org/abs/2602.05709)
*Yihao Ouyang,Shiwei Li,Haozhao Wang,Xiandi Luo,Zhuoqi Hu,Yuetong Song,Qiyu Qin,Yichen Li,Ruixuan Li*

Main category: cs.AI

TL;DR: GenLoRA提出了一种生成式低秩适配器，用轻量级非线性函数生成基向量，替代传统LoRA显式存储基向量的方式，实现了更高的参数效率。


<details>
  <summary>Details</summary>
Motivation: 传统LoRA采用显式秩范式，增加模型容量需要添加更多行或列（基向量），导致参数大幅增长。研究发现这些基向量存在显著的参数冗余，可以通过轻量级非线性函数紧凑表示。

Method: GenLoRA用非线性基向量生成替代显式基向量存储。为每个低秩矩阵维护一个潜在向量，并使用一组轻量级径向基函数（RBFs）来合成基向量。每个RBF需要的参数远少于显式基向量。

Result: 在多个数据集和架构上的广泛实验表明，GenLoRA在更小的参数预算下实现了更高的有效LoRA秩，从而获得了优越的微调性能。

Conclusion: GenLoRA通过生成式方法替代显式存储，显著提高了LoRA的参数效率，在有限参数预算下实现了更好的微调效果，为参数高效微调提供了新思路。

Abstract: Low-rank adaptation (LoRA) approximates the update of a pretrained weight matrix using the product of two low-rank matrices. However, standard LoRA follows an explicit-rank paradigm, where increasing model capacity requires adding more rows or columns (i.e., basis vectors) to the low-rank matrices, leading to substantial parameter growth. In this paper, we find that these basis vectors exhibit significant parameter redundancy and can be compactly represented by lightweight nonlinear functions. Therefore, we propose Generative Low-Rank Adapter (GenLoRA), which replaces explicit basis vector storage with nonlinear basis vector generation. Specifically, GenLoRA maintains a latent vector for each low-rank matrix and employs a set of lightweight radial basis functions (RBFs) to synthesize the basis vectors. Each RBF requires far fewer parameters than an explicit basis vector, enabling higher parameter efficiency in GenLoRA. Extensive experiments across multiple datasets and architectures show that GenLoRA attains higher effective LoRA ranks under smaller parameter budgets, resulting in superior fine-tuning performance. The code is available at https://anonymous.4open.science/r/GenLoRA-1519.

</details>


### [51] [Anchored Policy Optimization: Mitigating Exploration Collapse Via Support-Constrained Rectification](https://arxiv.org/abs/2602.05717)
*Tianyi Wang,Long Li,Hongcan Guo,Yibiao Chen,Yixia Li,Yong Wang,Yun Chen,Guanhua Chen*

Main category: cs.AI

TL;DR: 该论文提出了一种名为锚定策略优化（APO）的新方法，用于解决强化学习中可验证奖励（RLVR）导致的递归空间收缩问题，通过从全局形状匹配转向支持覆盖，在保持效率的同时防止策略崩溃。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习中的可验证奖励（RLVR）方法存在递归空间收缩（RSC）的系统性病理问题，即正锐化和负挤压的联合动态导致有效替代方案的采样概率消失。虽然KL正则化试图缓解此问题，但它强加了严格的形状匹配约束，导致梯度冲突。

Method: 提出了锚定策略优化（APO）方法，将范式从全局形状匹配转向支持覆盖。通过基于参考模型高置信度支持定义安全流形，APO允许积极锐化以提高效率，同时在错误校正时选择性调用恢复力以防止崩溃。

Result: 在数学基准测试中，APO打破了准确性与多样性之间的权衡，显著提高了Pass@1性能，同时恢复了标准策略梯度方法通常损失的Pass@K多样性。

Conclusion: APO作为一种梯度对齐机制，通过最大化支持覆盖实现弹性恢复，能够重新膨胀有效分支，解决了RLVR中的递归空间收缩问题，为强化学习中的策略优化提供了新的理论框架。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) is increasingly viewed as a tree pruning mechanism. However, we identify a systemic pathology termed Recursive Space Contraction (RSC), an irreversible collapse driven by the combined dynamics of positive sharpening and negative squeezing, where the sampling probability of valid alternatives vanishes. While Kullback-Leibler (KL) regularization aims to mitigate this, it imposes a rigid Shape Matching constraint that forces the policy to mimic the reference model's full density, creating a gradient conflict with the sharpening required for correctness. We propose Anchored Policy Optimization (APO), shifting the paradigm from global Shape Matching to Support Coverage. By defining a Safe Manifold based on the reference model's high-confidence support, APO permits aggressive sharpening for efficiency while selectively invoking a restorative force during error correction to prevent collapse. We theoretically derive that APO serves as a gradient-aligned mechanism to maximize support coverage, enabling an Elastic Recovery that re-inflates valid branches. Empirical evaluations on mathematical benchmarks demonstrate that APO breaks the accuracy-diversity trade-off, significantly improving Pass@1 while restoring the Pass@K diversity typically lost by standard policy gradient methods.

</details>


### [52] [Mitigating Hallucination in Financial Retrieval-Augmented Generation via Fine-Grained Knowledge Verification](https://arxiv.org/abs/2602.05723)
*Taoye Yin,Haoyuan Hu,Yaxin Fan,Xinhao Chen,Xinya Wu,Kai Deng,Kezun Zhang,Feng Wang*

Main category: cs.AI

TL;DR: 提出RLFKV框架，通过细粒度知识验证和强化学习减少金融RAG系统中的幻觉问题，提高回答与检索文档的一致性


<details>
  <summary>Details</summary>
Motivation: 金融RAG系统虽然依赖检索文档来生成准确回答，但由于金融领域的时间敏感性，模型生成的回答仍存在与检索信息矛盾的幻觉问题

Method: 提出RLFKV框架：1）将金融回答分解为原子知识单元；2）评估每个单元的正确性以计算细粒度忠实度奖励；3）为防止奖励黑客攻击（如过于简洁的回答），加入信息丰富度奖励，鼓励策略模型至少保留与基础模型相同数量的知识单元

Result: 在公开的Financial Data Description (FDD)任务和新提出的FDD-ANT数据集上的实验显示了一致的改进，证实了该方法的有效性

Conclusion: RLFKV框架通过细粒度知识验证和强化学习，有效减少了金融RAG系统中的幻觉问题，提高了回答与检索文档的一致性

Abstract: In financial Retrieval-Augmented Generation (RAG) systems, models frequently rely on retrieved documents to generate accurate responses due to the time-sensitive nature of the financial domain. While retrieved documents help address knowledge gaps, model-generated responses still suffer from hallucinations that contradict the retrieved information. To mitigate this inconsistency, we propose a Reinforcement Learning framework enhanced with Fine-grained Knowledge Verification (RLFKV). Our method decomposes financial responses into atomic knowledge units and assesses the correctness of each unit to compute the fine-grained faithful reward. This reward offers more precise optimization signals, thereby improving alignment with the retrieved documents. Additionally, to prevent reward hacking (e.g., overly concise replies), we incorporate an informativeness reward that encourages the policy model to retain at least as many knowledge units as the base model. Experiments conducted on the public Financial Data Description (FDD) task and our newly proposed FDD-ANT dataset demonstrate consistent improvements, confirming the effectiveness of our approach.

</details>


### [53] [FiMI: A Domain-Specific Language Model for Indian Finance Ecosystem](https://arxiv.org/abs/2602.05794)
*Aboli Kathar,Aman Kumar,Anusha Kamath,Araveeti Srujan,Ashish Sharma,Chandra Bhushan,Dilip Asbe,Divya Sorate,Duddu Prasanth Kumar,Evan Acharya,Harsh Sharma,Hrithik Kadam,Kanishk Singla,Keyur Doshi,Kiran Praveen,Kolisetty Krishna SK,Krishanu Adhikary,Lokesh MPT,Mayurdeep Sonowal,Nadeem Shaikh,Navya Prakash,Nimit Kothari,Nitin Kukreja,Prashant Devadiga,Rakesh Paul,Ratanjeet Pratap Chauhan,Raunak Kalani,Raviraj Joshi,Shamanth MH,Shantanu Pandey,Shubham Soni,Siddharth Dixit,Smriti Jopat,Sunil Patel,Suraj Singh,Suvradip Paul,Tulasi Pilla,Utkarsh Vaidya,Vineeth Nambiar,Vishal Kanvaty,Yatharth Dedhia*

Main category: cs.AI

TL;DR: FiMI是针对印度数字支付系统的金融领域专用语言模型，基于Mistral Small 24B架构，通过多阶段训练开发了Base和Instruct两个版本，在金融推理和工具调用方面显著优于原模型。


<details>
  <summary>Details</summary>
Motivation: 开发专门针对印度数字支付系统的金融语言模型，解决现有通用模型在印度金融领域特定任务（如交易纠纷、授权生命周期管理）上的不足，适应多语言（英语、印地语、印英混合语）环境。

Method: 基于Mistral Small 24B架构，采用多阶段训练流程：1）在680亿token的金融、多语言和合成数据上进行持续预训练；2）指令微调；3）针对多轮、工具驱动的对话进行领域特定的监督微调，模拟真实工作流程。

Result: FiMI Base在金融推理基准测试上比Mistral Small 24B Base模型提升20%；FiMI Instruct在领域特定工具调用上比Mistral Small 24B Instruct模型提升87%；同时在与相似规模模型的一般基准测试上保持相当性能。

Conclusion: FiMI成功开发了针对印度数字支付系统的专用金融语言模型，在保持通用能力的同时，在金融领域特定任务上取得了显著性能提升，证明了领域专业化模型的有效性。

Abstract: We present FiMI (Finance Model for India), a domain-specialized financial language model developed for Indian digital payment systems. We develop two model variants: FiMI Base and FiMI Instruct. FiMI adapts the Mistral Small 24B architecture through a multi-stage training pipeline, beginning with continuous pre-training on 68 Billion tokens of curated financial, multilingual (English, Hindi, Hinglish), and synthetic data. This is followed by instruction fine-tuning and domain-specific supervised fine-tuning focused on multi-turn, tool-driven conversations that model real-world workflows, such as transaction disputes and mandate lifecycle management. Evaluations reveal that FiMI Base achieves a 20% improvement over the Mistral Small 24B Base model on finance reasoning benchmark, while FiMI Instruct outperforms the Mistral Small 24B Instruct model by 87% on domain-specific tool-calling. Moreover, FiMI achieves these significant domain gains while maintaining comparable performance to models of similar size on general benchmarks.

</details>


### [54] [TKG-Thinker: Towards Dynamic Reasoning over Temporal Knowledge Graphs via Agentic Reinforcement Learning](https://arxiv.org/abs/2602.05818)
*Zihao Jiang,Miao Peng,Zhenyan Shan,Wenjie Xu,Ben Liu,Gong Chen,Ziqi Gao,Min Peng*

Main category: cs.AI

TL;DR: TKG-Thinker：一种具有自主规划和自适应检索能力的新型智能体，通过双阶段训练策略（监督微调+强化学习）在时序知识图谱问答中实现最先进性能


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在时序知识图谱问答中存在两个主要限制：1）在复杂时序约束下容易产生推理幻觉；2）静态提示限制了模型自主性和泛化能力，缺乏与动态时序知识图谱环境的交互优化

Method: 提出TKG-Thinker智能体，具备自主规划和自适应检索能力。采用双阶段训练策略：首先使用思维链数据进行监督微调以培养核心规划能力，然后通过强化学习阶段利用多维奖励在复杂时序约束下优化推理策略

Result: 在基准数据集上使用三个开源大语言模型的实验结果表明，TKG-Thinker实现了最先进的性能，并在复杂时序知识图谱问答设置中表现出强大的泛化能力

Conclusion: TKG-Thinker通过动态多轮交互和双阶段训练策略，有效解决了大语言模型在时序知识图谱问答中的推理幻觉和泛化限制问题，为时序知识推理提供了新的解决方案

Abstract: Temporal knowledge graph question answering (TKGQA) aims to answer time-sensitive questions by leveraging temporal knowledge bases. While Large Language Models (LLMs) demonstrate significant potential in TKGQA, current prompting strategies constrain their efficacy in two primary ways. First, they are prone to reasoning hallucinations under complex temporal constraints. Second, static prompting limits model autonomy and generalization, as it lack optimization through dynamic interaction with temporal knowledge graphs (TKGs) environments. To address these limitations, we propose \textbf{TKG-Thinker}, a novel agent equipped with autonomous planning and adaptive retrieval capabilities for reasoning over TKGs. Specifically, TKG-Thinker performs in-depth temporal reasoning through dynamic multi-turn interactions with TKGs via a dual-training strategy. We first apply Supervised Fine-Tuning (SFT) with chain-of thought data to instill core planning capabilities, followed by a Reinforcement Learning (RL) stage that leverages multi-dimensional rewards to refine reasoning policies under intricate temporal constraints. Experimental results on benchmark datasets with three open-source LLMs show that TKG-Thinker achieves state-of-the-art performance and exhibits strong generalization across complex TKGQA settings.

</details>


### [55] [BABE: Biology Arena BEnchmark](https://arxiv.org/abs/2602.05857)
*Junting Zhou,Jin Chen,Linfeng Hao,Denghui Cao,Zheyu Wang,Qiguang Chen,Chaoyou Fu,Jiaze Chen,Yuchen Wu,Ge Zhang,Mingxuan Wang,Wenhao Huang,Tong Yang*

Main category: cs.AI

TL;DR: BABE是一个评估生物AI系统实验推理能力的基准测试，基于同行评审研究论文和真实生物研究构建，专注于因果推理和跨尺度推断能力。


<details>
  <summary>Details</summary>
Motivation: 现有生物学基准测试未能充分评估研究人员所需的关键能力：将实验结果与背景知识整合以得出有意义结论的能力。大语言模型能力已从基础对话扩展到高级科学推理，但缺乏评估实验推理能力的基准。

Method: 从同行评审研究论文和真实世界生物研究中构建BABE基准，确保任务反映实际科学探究的复杂性和跨学科性质。基准设计挑战模型进行因果推理和跨尺度推断。

Result: BABE提供了一个稳健的框架来评估AI系统像实践科学家一样推理的能力，为衡量AI对生物学研究贡献潜力提供了更真实的度量标准。

Conclusion: BABE基准填补了现有生物学基准的空白，能够更真实地评估生物AI系统的实验推理能力，为衡量AI在生物学研究中的实际贡献潜力提供了重要工具。

Abstract: The rapid evolution of large language models (LLMs) has expanded their capabilities from basic dialogue to advanced scientific reasoning. However, existing benchmarks in biology often fail to assess a critical skill required of researchers: the ability to integrate experimental results with contextual knowledge to derive meaningful conclusions. To address this gap, we introduce BABE(Biology Arena BEnchmark), a comprehensive benchmark designed to evaluate the experimental reasoning capabilities of biological AI systems. BABE is uniquely constructed from peer-reviewed research papers and real-world biological studies, ensuring that tasks reflect the complexity and interdisciplinary nature of actual scientific inquiry. BABE challenges models to perform causal reasoning and cross-scale inference. Our benchmark provides a robust framework for assessing how well AI systems can reason like practicing scientists, offering a more authentic measure of their potential to contribute to biological research.

</details>


### [56] [Beyond Manual Planning: Seating Allocation for Large Organizations](https://arxiv.org/abs/2602.05875)
*Anton Ipsen,Michael Cashmore,Kirsty Fielding,Nicolas Marchesotti,Parisa Zehtabi,Daniele Magazzeni,Manuela Veloso*

Main category: cs.AI

TL;DR: 本文提出分层座位分配问题（HSAP），旨在为具有层级结构的组织团队在平面图上分配最优座位安排，确保层级关系紧密的团队座位相邻，并开发了基于概率路线图、快速探索随机树、启发式搜索和动态规划的整数规划求解框架。


<details>
  <summary>Details</summary>
Motivation: 大型组织通常具有复杂的层级结构，需要确保具有紧密层级关系的团队（如研究小组）在物理座位上相邻，形成连续区域。目前这一问题主要通过人工管理，导致重新规划频率低且效果欠佳，因此需要自动化解决方案。

Method: 提出端到端求解框架：1）使用概率路线图（PRM）和快速探索随机树（RRT）计算任意座位对之间的距离；2）结合启发式搜索和动态规划方法，通过整数规划求解HSAP问题。

Result: 在不同规模实例下评估了PRM框架和后续分配方案，进行了定量和定性分析，验证了方法的有效性。

Conclusion: 提出的HSAP问题框架能够有效解决大型组织层级团队的座位分配问题，通过自动化方法替代人工规划，提高座位安排的质量和效率。

Abstract: We introduce the Hierarchical Seating Allocation Problem (HSAP) which addresses the optimal assignment of hierarchically structured organizational teams to physical seating arrangements on a floor plan. This problem is driven by the necessity for large organizations with large hierarchies to ensure that teams with close hierarchical relationships are seated in proximity to one another, such as ensuring a research group occupies a contiguous area. Currently, this problem is managed manually leading to infrequent and suboptimal replanning efforts. To alleviate this manual process, we propose an end-to-end framework to solve the HSAP. A scalable approach to calculate the distance between any pair of seats using a probabilistic road map (PRM) and rapidly-exploring random trees (RRT) which is combined with heuristic search and dynamic programming approach to solve the HSAP using integer programming. We demonstrate our approach under different sized instances by evaluating the PRM framework and subsequent allocations both quantitatively and qualitatively.

</details>


### [57] [Agent2Agent Threats in Safety-Critical LLM Assistants: A Human-Centric Taxonomy](https://arxiv.org/abs/2602.05877)
*Lukas Stappen,Ahmet Erkan Turan,Johann Hagerer,Georg Groh*

Main category: cs.AI

TL;DR: 提出了AgentHeLLM威胁建模框架，将资产识别与攻击路径分析分离，用于保护车载LLM对话助手安全


<details>
  <summary>Details</summary>
Motivation: 车载LLM对话助手与外部服务协调时，通过自然语言载荷建立攻击面，现有AI安全框架缺乏安全关键系统工程中的"关注点分离"标准，将保护对象（资产）与攻击方式（攻击路径）混为一谈

Method: 提出AgentHeLLM威胁建模框架，引入基于伤害导向的"受害者建模"的人类中心资产分类法，以及区分毒化路径（恶意数据传播）和触发路径（激活操作）的基于图的正式模型，开发了开源攻击路径建议工具AgentHeLLM攻击路径生成器

Result: 开发了自动化多阶段威胁发现的工具，使用双层搜索策略，展示了框架的实际适用性

Conclusion: AgentHeLLM框架为车载LLM助手的安全威胁建模提供了方法论基础，通过分离资产识别和攻击路径分析，填补了现有AI安全框架在安全关键系统中的方法学空白

Abstract: The integration of Large Language Model (LLM)-based conversational agents into vehicles creates novel security challenges at the intersection of agentic AI, automotive safety, and inter-agent communication. As these intelligent assistants coordinate with external services via protocols such as Google's Agent-to-Agent (A2A), they establish attack surfaces where manipulations can propagate through natural language payloads, potentially causing severe consequences ranging from driver distraction to unauthorized vehicle control. Existing AI security frameworks, while foundational, lack the rigorous "separation of concerns" standard in safety-critical systems engineering by co-mingling the concepts of what is being protected (assets) with how it is attacked (attack paths). This paper addresses this methodological gap by proposing a threat modeling framework called AgentHeLLM (Agent Hazard Exploration for LLM Assistants) that formally separates asset identification from attack path analysis. We introduce a human-centric asset taxonomy derived from harm-oriented "victim modeling" and inspired by the Universal Declaration of Human Rights, and a formal graph-based model that distinguishes poison paths (malicious data propagation) from trigger paths (activation actions). We demonstrate the framework's practical applicability through an open-source attack path suggestion tool AgentHeLLM Attack Path Generator that automates multi-stage threat discovery using a bi-level search strategy.

</details>


### [58] [A Guide to Large Language Models in Modeling and Simulation: From Core Techniques to Critical Challenges](https://arxiv.org/abs/2602.05883)
*Philippe J. Giabbanelli*

Main category: cs.AI

TL;DR: 论文为M&S领域提供LLMs使用的综合实用指南，强调避免常见陷阱和做出明智设计选择


<details>
  <summary>Details</summary>
Motivation: LLMs在建模与仿真(M&S)工作流中应用日益广泛，但看似简单的实践可能引入微妙问题、不必要复杂性甚至导致劣质结果，需要提供全面实用的使用指导

Method: 讨论常见困惑来源，包括非确定性、知识增强（RAG和LoRA）、M&S数据分解和超参数设置，强调原则性设计选择、诊断策略和实证评估

Result: 提供综合实用指南，帮助建模者做出关于何时、如何以及是否依赖LLMs的明智决策

Conclusion: LLMs在M&S应用中需要谨慎使用，避免常见陷阱，通过原则性设计和实证评估确保有效应用

Abstract: Large language models (LLMs) have rapidly become familiar tools to researchers and practitioners. Concepts such as prompting, temperature, or few-shot examples are now widely recognized, and LLMs are increasingly used in Modeling & Simulation (M&S) workflows. However, practices that appear straightforward may introduce subtle issues, unnecessary complexity, or may even lead to inferior results. Adding more data can backfire (e.g., deteriorating performance through model collapse or inadvertently wiping out existing guardrails), spending time on fine-tuning a model can be unnecessary without a prior assessment of what it already knows, setting the temperature to 0 is not sufficient to make LLMs deterministic, providing a large volume of M&S data as input can be excessive (LLMs cannot attend to everything) but naive simplifications can lose information. We aim to provide comprehensive and practical guidance on how to use LLMs, with an emphasis on M&S applications. We discuss common sources of confusion, including non-determinism, knowledge augmentation (including RAG and LoRA), decomposition of M&S data, and hyper-parameter settings. We emphasize principled design choices, diagnostic strategies, and empirical evaluation, with the goal of helping modelers make informed decisions about when, how, and whether to rely on LLMs.

</details>


### [59] [Quantum Reinforcement Learning with Transformers for the Capacitated Vehicle Routing Problem](https://arxiv.org/abs/2602.05920)
*Eva Andrés*

Main category: cs.AI

TL;DR: 该论文比较了经典与量子强化学习方法在容量约束车辆路径问题上的表现，发现量子增强模型（特别是混合架构）在路径距离、紧凑性和重叠度方面优于经典基线。


<details>
  <summary>Details</summary>
Motivation: 研究量子强化学习在复杂组合优化问题（如容量约束车辆路径问题）中的应用潜力，探索量子计算能否为这类NP难问题带来性能提升。

Method: 实现经典、全量子和混合三种变体的优势演员-评论家（A2C）智能体，集成transformer架构通过自注意力和交叉注意力机制捕捉车辆、客户和仓库之间的关系。实验针对20个客户和4辆车的多车辆容量约束场景，进行10次独立运行。

Result: 所有三种方法都能学习有效的路径策略，但量子增强模型在路由距离、路线紧凑性和路线重叠度方面优于经典基线，其中混合架构在所有指标上表现最佳。可视化分析显示量子模型生成的结构更连贯。

Conclusion: 混合量子-经典强化学习模型在解决复杂组合优化问题（如CVRP）方面具有显著潜力，量子增强能带来更优的路径组织和鲁棒性。

Abstract: This paper addresses the Capacitated Vehicle Routing Problem (CVRP) by comparing classical and quantum Reinforcement Learning (RL) approaches. An Advantage Actor-Critic (A2C) agent is implemented in classical, full quantum, and hybrid variants, integrating transformer architectures to capture the relationships between vehicles, clients, and the depot through self- and cross-attention mechanisms. The experiments focus on multi-vehicle scenarios with capacity constraints, considering 20 clients and 4 vehicles, and are conducted over ten independent runs. Performance is assessed using routing distance, route compactness, and route overlap. The results show that all three approaches are capable of learning effective routing policies. However, quantum-enhanced models outperform the classical baseline and produce more robust route organization, with the hybrid architecture achieving the best overall performance across distance, compactness, and route overlap. In addition to quantitative improvements, qualitative visualizations reveal that quantum-based models generate more structured and coherent routing solutions. These findings highlight the potential of hybrid quantum-classical reinforcement learning models for addressing complex combinatorial optimization problems such as the CVRP.

</details>


### [60] [Speech Emotion Recognition Leveraging OpenAI's Whisper Representations and Attentive Pooling Methods](https://arxiv.org/abs/2602.06000)
*Ali Shendabadi,Parnia Izadirad,Mostafa Salehi,Mahmoud Bijankhan*

Main category: cs.AI

TL;DR: 该研究探索了预训练ASR模型Whisper在语音情感识别中的应用，提出了两种基于注意力的池化方法来降维并保留情感特征，在英语和波斯语数据集上取得了SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 语音情感识别研究因缺乏标准化大型数据集而受限，现有研究利用预训练模型提取特征。本研究探索Whisper这一预训练ASR系统在语音情感识别中的潜力，旨在提供轻量高效的替代方案。

Method: 提出了两种基于注意力的池化方法：多头注意力平均池化和QKV池化，用于高效降低Whisper表示的维度同时保留情感特征。在英语IEMOCAP和波斯语ShEMO数据集上实验，使用Whisper Tiny和Small版本，比较不同编码器层的性能。

Result: 多头QKV架构在ShEMO数据集上取得了最先进的结果，未加权准确率提升了2.47%。发现中间层在波斯语数据集上表现更好，为HuBERT X-Large等大型模型提供了轻量高效的替代方案。

Conclusion: Whisper作为语音情感识别的表示提取器具有巨大潜力，基于注意力的池化方法在维度减少方面效果显著，为跨语言语音情感识别提供了高效解决方案。

Abstract: Speech Emotion Recognition (SER) research has faced limitations due to the lack of standard and sufficiently large datasets. Recent studies have leveraged pre-trained models to extract features for downstream tasks such as SER. This work explores the capabilities of Whisper, a pre-trained ASR system, in speech emotion recognition by proposing two attention-based pooling methods, Multi-head Attentive Average Pooling and QKV Pooling, designed to efficiently reduce the dimensionality of Whisper representations while preserving emotional features. We experiment on English and Persian, using the IEMOCAP and ShEMO datasets respectively, with Whisper Tiny and Small. Our multi-head QKV architecture achieves state-of-the-art results on the ShEMO dataset, with a 2.47% improvement in unweighted accuracy. We further compare the performance of different Whisper encoder layers and find that intermediate layers often perform better for SER on the Persian dataset, providing a lightweight and efficient alternative to much larger models such as HuBERT X-Large. Our findings highlight the potential of Whisper as a representation extractor for SER and demonstrate the effectiveness of attention-based pooling for dimension reduction.

</details>


### [61] [AgenticPay: A Multi-Agent LLM Negotiation System for Buyer-Seller Transactions](https://arxiv.org/abs/2602.06008)
*Xianyang Liu,Shangding Gu,Dawn Song*

Main category: cs.AI

TL;DR: AgenticPay是一个用于多智能体买卖谈判的基准测试和仿真框架，通过自然语言驱动，包含110多个任务，评估LLM在语言经济交互中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的基准测试缺乏评估多智能体之间语言经济交互的原则性设置，而LLM智能体在自主谈判、协调和交易方面的应用日益增多。

Method: 开发AgenticPay框架，模拟买卖双方拥有私有约束和产品依赖估值的市场环境，通过多轮语言谈判而非单纯数字竞价达成协议，包含结构化动作提取和可行性、效率、福利等指标。

Result: 对最先进的专有和开源LLM进行基准测试，揭示了谈判性能的显著差距，突显了长期战略推理的挑战。

Conclusion: AgenticPay为研究智能体商业和基于语言的市场交互奠定了基础，代码和数据集已开源。

Abstract: Large language model (LLM)-based agents are increasingly expected to negotiate, coordinate, and transact autonomously, yet existing benchmarks lack principled settings for evaluating language-mediated economic interaction among multiple agents. We introduce AgenticPay, a benchmark and simulation framework for multi-agent buyer-seller negotiation driven by natural language. AgenticPay models markets in which buyers and sellers possess private constraints and product-dependent valuations, and must reach agreements through multi-round linguistic negotiation rather than numeric bidding alone. The framework supports a diverse suite of over 110 tasks ranging from bilateral bargaining to many-to-many markets, with structured action extraction and metrics for feasibility, efficiency, and welfare. Benchmarking state-of-the-art proprietary and open-weight LLMs reveals substantial gaps in negotiation performance and highlights challenges in long-horizon strategic reasoning, establishing AgenticPay as a foundation for studying agentic commerce and language-based market interaction. Code and dataset are available at the link: https://github.com/SafeRL-Lab/AgenticPay.

</details>
