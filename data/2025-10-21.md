<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 22]
- [cs.CR](#cs.CR) [Total: 59]
- [cs.AI](#cs.AI) [Total: 70]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [SIADAFIX: issue description response for adaptive program repair](https://arxiv.org/abs/2510.16059)
*Xin Cao,Nan Yu*

Main category: cs.SE

TL;DR: SIADAFIX是一种基于快慢思维的自适应程序修复方法，通过问题描述响应来指导修复工作流，在SWE-bench Lite上达到60.67%的pass@1性能


<details>
  <summary>Details</summary>
Motivation: 利用快慢思维增强大语言模型代理在复杂任务（如程序修复）上的能力，平衡修复效率和准确性

Method: 设计自适应程序修复方法SIADAFIX，使用慢思维bug修复代理完成复杂任务，快思维工作流决策组件优化和分类问题描述，根据问题复杂度自适应选择简单、中等、困难三种修复模式

Result: 在SWE-bench Lite上使用Claude-4 Sonnet模型达到60.67% pass@1性能，在所有开源方法中达到最先进水平

Conclusion: SIADAFIX有效平衡了修复效率和准确性，为自动化程序修复提供了新思路

Abstract: We propose utilizing fast and slow thinking to enhance the capabilities of
large language model-based agents on complex tasks such as program repair. In
particular, we design an adaptive program repair method based on issue
description response, called SIADAFIX. The proposed method utilizes slow
thinking bug fix agent to complete complex program repair tasks, and employs
fast thinking workflow decision components to optimize and classify issue
descriptions, using issue description response results to guide the
orchestration of bug fix agent workflows. SIADAFIX adaptively selects three
repair modes, i.e., easy, middle and hard mode, based on problem complexity. It
employs fast generalization for simple problems and test-time scaling
techniques for complex problems. Experimental results on the SWE-bench Lite
show that the proposed method achieves 60.67% pass@1 performance using the
Claude-4 Sonnet model, reaching state-of-the-art levels among all open-source
methods. SIADAFIX effectively balances repair efficiency and accuracy,
providing new insights for automated program repair. Our code is available at
https://github.com/liauto-siada/siada-cli.

</details>


### [2] [Code Contribution and Credit in Science](https://arxiv.org/abs/2510.16242)
*Eva Maxfield Brown,Isaac Slaughter,Nicholas Weber*

Main category: cs.SE

TL;DR: 研究发现软件贡献与学术认可之间存在脱节，近30%论文包含未获作者署名的代码贡献者，频繁编码的作者h指数反而更低


<details>
  <summary>Details</summary>
Motivation: 理解软件开发活动如何影响科学合作中的学术认可分配，探究软件贡献与传统学术评价指标之间的关系

Method: 构建包含14万篇研究论文与代码仓库配对的数据集，开发预测模型匹配论文作者与代码仓库开发者账户，分析软件贡献对学术认可的影响

Result: 30%论文包含未获作者署名的代码贡献者；代码贡献作者仅获得4.2%引用增长但统计不显著；第一作者更可能是代码贡献者；编码频率与h指数呈负相关

Conclusion: 软件贡献与学术认可之间存在系统性脱节，这对机构奖励结构和科学政策具有重要启示

Abstract: Software development has become essential to scientific research, but its
relationship to traditional metrics of scholarly credit remains poorly
understood. We develop a dataset of approximately 140,000 paired research
articles and code repositories, as well as a predictive model that matches
research article authors with software repository developer accounts. We use
this data to investigate how software development activities influence credit
allocation in collaborative scientific settings. Our findings reveal
significant patterns distinguishing software contributions from traditional
authorship credit. We find that nearly 30% of articles include non-author code
contributors- individuals who participated in software development but received
no formal authorship recognition. While code-contributing authors show a modest
$\sim$4.2% increase in article citations, this effect becomes non-significant
when controlling for domain, article type, and open access status. First
authors are significantly more likely to be code contributors than other author
positions. Notably, we identify a negative relationship between coding
frequency and scholarly impact metrics. Authors who contribute code more
frequently exhibit progressively lower h-indices than non-coding colleagues,
even when controlling for publication count, author position, domain, and
article type. These results suggest a disconnect between software contributions
and credit, highlighting important implications for institutional reward
structures and science policy.

</details>


### [3] [MLCPD: A Unified Multi-Language Code Parsing Dataset with Universal AST Schema](https://arxiv.org/abs/2510.16357)
*Jugal Gajjar,Kamalasankari Subramaniakuppusamy*

Main category: cs.SE

TL;DR: MLCPD是一个大规模、语言无关的代码解析数据集，统一了10种主要编程语言的语法和结构表示，包含超过700万个解析后的源文件，采用通用AST模式进行标准化。


<details>
  <summary>Details</summary>
Motivation: 现有语料库主要关注标记级代码或孤立解析器，缺乏统一的跨语言结构表示。MLCPD旨在提供一致的跨语言推理、结构学习和多语言软件分析基础。

Method: 提出通用抽象语法树(AST)模式，对10种编程语言的源代码进行解析和标准化，包含层次化树表示和丰富元数据，确保无损语法覆盖和结构一致性。

Result: 经验分析显示跨语言结构规律性强，Python、Java、Go等不同语言的语法图可以在共享模式下对齐。数据集以Parquet格式存储，便于扩展检索。

Conclusion: MLCPD为跨语言表示学习和程序分析的未来研究建立了开放、可复现的基础，相关资源已在Hugging Face和GitHub上公开发布。

Abstract: We introduce the MultiLang Code Parser Dataset (MLCPD), a large-scale,
language-agnostic dataset unifying syntactic and structural representations of
code across ten major programming languages. MLCPD contains over seven million
parsed source files normalized under our proposed universal Abstract Syntax
Tree (AST) schema, enabling consistent cross-language reasoning, structural
learning, and multilingual software analysis. Unlike existing corpora that
focus purely on token-level code or isolated parsers, MLCPD provides both
hierarchical tree representations and rich metadata for every file, ensuring
lossless syntactic coverage and structural uniformity. Each entry includes a
normalized schema, language-level metadata, and abstracted node semantics
stored in Parquet format for scalable retrieval. Empirical analyses reveal
strong cross-language structural regularities-demonstrating that syntactic
graphs from languages as diverse as Python, Java, and Go can be aligned under a
shared schema. We release the dataset publicly on Hugging Face and the
accompanying codebase on GitHub, which includes complete pipelines for dataset
reproduction, grammar compilation, and a visualization tool for exploring the
unified AST across languages. Together, these resources establish MLCPD as an
open, reproducible foundation for future research in cross-language
representation learning and program analysis.

</details>


### [4] [SemOpt: LLM-Driven Code Optimization via Rule-Based Analysis](https://arxiv.org/abs/2510.16384)
*Yuwei Zhao,Yuan-An Xiao,Qianyu Xiao,Zhao Zhang,Yingfei Xiong*

Main category: cs.SE

TL;DR: SemOpt是一个利用静态程序分析和LLM进行代码优化的框架，通过构建策略库、生成静态分析规则和优化代码，显著提升了代码优化效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的代码优化方法依赖信息检索技术从代码库中查找优化示例，但由于语义等价但语法不同的代码片段难以被准确检索，导致优化性能不佳。

Method: SemOpt包含三个核心组件：策略库构建器从真实代码修改中提取和聚类优化策略；规则生成器生成Semgrep静态分析规则来识别可优化代码段；优化器利用策略库生成优化结果。所有组件都由LLM驱动。

Result: 在包含151个优化任务的基准测试中，SemOpt在不同LLM下将成功优化数量提高了1.38到28倍。在大型C/C++项目中，单个性能指标提升了5.04%到218.07%。

Conclusion: SemOpt通过结合静态程序分析和LLM，有效解决了现有代码优化方法中检索相关示例的局限性，显著提升了代码优化效果和实用性。

Abstract: Automated code optimization aims to improve performance in programs by
refactoring code, and recent studies focus on utilizing LLMs for the
optimization. Typical existing approaches mine optimization commits from
open-source codebases to construct a large-scale knowledge base, then employ
information retrieval techniques such as BM25 to retrieve relevant optimization
examples for hotspot code locations, thereby guiding LLMs to optimize these
hotspots. However, since semantically equivalent optimizations can manifest in
syntactically dissimilar code snippets, current retrieval methods often fail to
identify pertinent examples, leading to suboptimal optimization performance.
This limitation significantly reduces the effectiveness of existing
optimization approaches.
  To address these limitations, we propose SemOpt, a novel framework that
leverages static program analysis to precisely identify optimizable code
segments, retrieve the corresponding optimization strategies, and generate the
optimized results. SemOpt consists of three key components: (1) A strategy
library builder that extracts and clusters optimization strategies from
real-world code modifications. (2) A rule generator that generates Semgrep
static analysis rules to capture the condition of applying the optimization
strategy. (3) An optimizer that utilizes the strategy library to generate
optimized code results. All the three components are powered by LLMs.
  On our benchmark containing 151 optimization tasks, SemOpt demonstrates its
effectiveness under different LLMs by increasing the number of successful
optimizations by 1.38 to 28 times compared to the baseline. Moreover, on
popular large-scale C/C++ projects, it can improve individual performance
metrics by 5.04% to 218.07%, demonstrating its practical utility.

</details>


### [5] [Code Digital Twin: Empowering LLMs with Tacit Knowledge for Complex Software Development](https://arxiv.org/abs/2510.16395)
*Xin Peng,Chong Wang*

Main category: cs.SE

TL;DR: 提出Code Digital Twin框架，将AI能力与企业软件开发现实对齐，通过混合知识表示和多阶段提取管道，将碎片化知识转化为可操作的显式表示。


<details>
  <summary>Details</summary>
Motivation: 企业软件开发主要依赖增量演进，面临超越常规编码的挑战，特别是隐性知识（如设计决策和历史权衡）的缺失。现有LLM能力需要与复杂软件开发实践对齐。

Method: 提出Code Digital Twin框架，包含：混合知识表示、多阶段提取管道、增量更新、LLM赋能应用和人机协同反馈，共同建模软件的物理和概念层。

Result: 该框架能够保存隐性知识，与代码库共同演进，将碎片化知识转化为显式可操作表示，为超复杂系统的可持续智能开发提供具体路线图。

Conclusion: Code Digital Twin在AI进步与企业软件现实之间架起桥梁，为实现可持续、智能和弹性的超复杂系统开发和演进提供了具体路径。

Abstract: Recent advances in large language models (LLMs) have demonstrated strong
capabilities in software engineering tasks, raising expectations of
revolutionary productivity gains. However, enterprise software development is
largely driven by incremental evolution, where challenges extend far beyond
routine coding and depend critically on tacit knowledge, including design
decisions at different levels and historical trade-offs. To achieve effective
AI-powered support for complex software development, we should align emerging
AI capabilities with the practical realities of enterprise development. To this
end, we systematically identify challenges from both software and LLM
perspectives. Alongside these challenges, we outline opportunities where AI and
structured knowledge frameworks can enhance decision-making in tasks such as
issue localization and impact analysis. To address these needs, we propose the
Code Digital Twin, a living framework that models both the physical and
conceptual layers of software, preserves tacit knowledge, and co-evolves with
the codebase. By integrating hybrid knowledge representations, multi-stage
extraction pipelines, incremental updates, LLM-empowered applications, and
human-in-the-loop feedback, the Code Digital Twin transforms fragmented
knowledge into explicit and actionable representations. Our vision positions it
as a bridge between AI advancements and enterprise software realities,
providing a concrete roadmap toward sustainable, intelligent, and resilient
development and evolution of ultra-complex systems.

</details>


### [6] [Large-Scale Empirical Analysis of Continuous Fuzzing: Insights from 1 Million Fuzzing Sessions](https://arxiv.org/abs/2510.16433)
*Tatsuya Shirai,Olivier Nourry,Yutaro Kashiwa,Kenji Fujiwara,Yasutaka Kamei,Hajimu Iida*

Main category: cs.SE

TL;DR: 该研究通过分析OSS-Fuzz平台的112万次模糊测试会话，揭示了持续模糊测试在漏洞检测中的作用：早期阶段检测率高、代码覆盖率持续增长、覆盖率变化有助于漏洞发现。


<details>
  <summary>Details</summary>
Motivation: 尽管持续模糊测试已被数千个项目采用，但其在漏洞检测中的实际贡献尚不清楚。本研究旨在阐明持续模糊测试在漏洞检测中的具体作用。

Method: 收集OSS-Fuzz的问题报告、覆盖率报告和模糊测试日志，对878个项目的约112万次模糊测试会话进行实证研究。

Result: 发现：(i)大量模糊测试漏洞在集成持续模糊测试前已存在，导致早期检测率高；(ii)随着持续模糊测试进行，代码覆盖率持续增加；(iii)覆盖率变化有助于模糊测试漏洞的检测。

Conclusion: 本研究为持续模糊测试如何促进漏洞检测提供了实证见解，对未来的持续模糊测试策略和工具开发具有实际意义。

Abstract: Software vulnerabilities are constantly being reported and exploited in
software products, causing significant impacts on society. In recent years, the
main approach to vulnerability detection, fuzzing, has been integrated into the
continuous integration process to run in short and frequent cycles. This
continuous fuzzing allows for fast identification and remediation of
vulnerabilities during the development process. Despite adoption by thousands
of projects, however, it is unclear how continuous fuzzing contributes to
vulnerability detection. This study aims to elucidate the role of continuous
fuzzing in vulnerability detection. Specifically, we investigate the coverage
and the total number of fuzzing sessions when fuzzing bugs are discovered. We
collect issue reports, coverage reports, and fuzzing logs from OSS-Fuzz, an
online service provided by Google that performs fuzzing during continuous
integration. Through an empirical study of a total of approximately 1.12
million fuzzing sessions from 878 projects participating in OSS-Fuzz, we reveal
that (i) a substantial number of fuzzing bugs exist prior to the integration of
continuous fuzzing, leading to a high detection rate in the early stages; (ii)
code coverage continues to increase as continuous fuzzing progresses; and (iii)
changes in coverage contribute to the detection of fuzzing bugs. This study
provides empirical insights into how continuous fuzzing contributes to fuzzing
bug detection, offering practical implications for future strategies and tool
development in continuous fuzzing.

</details>


### [7] [On the Use of Large Language Models for Qualitative Synthesis](https://arxiv.org/abs/2510.16502)
*Sebastián Pizard,Ramiro Moreira,Federico Galiano,Ignacio Sastre,Lorena Etcheverry*

Main category: cs.SE

TL;DR: 本文探讨了在系统评价的定性综合阶段使用大型语言模型的挑战，通过协作自传民族志方法评估了LLM在此过程中的方法论严谨性和实际有用性。


<details>
  <summary>Details</summary>
Motivation: 虽然LLM在支持系统评价方面显示出潜力，但在报告不一致、执行多变的定性综合阶段应用LLM存在重要风险，可能放大现有弱点并削弱系统评价结果的可信度。

Method: 采用协作自传民族志方法，进行了两项试验，从方法论严谨性和实际有用性角度评估LLM在定性综合中的应用，并通过技术视角解释结果。

Result: 研究发现使用LLM进行定性综合存在挑战，需要仔细考虑其技术局限性和对系统评价质量的影响。

Conclusion: 在系统评价的定性综合阶段应用LLM需要谨慎，必须理解其技术局限性，以避免放大现有弱点并确保研究结果的可靠性。

Abstract: Large language models (LLMs) show promise for supporting systematic reviews
(SR), even complex tasks such as qualitative synthesis (QS). However, applying
them to a stage that is unevenly reported and variably conducted carries
important risks: misuse can amplify existing weaknesses and erode confidence in
the SR findings. To examine the challenges of using LLMs for QS, we conducted a
collaborative autoethnography involving two trials. We evaluated each trial for
methodological rigor and practical usefulness, and interpreted the results
through a technical lens informed by how LLMs are built and their current
limitations.

</details>


### [8] [Human-Aligned Code Readability Assessment with Large Language Models](https://arxiv.org/abs/2510.16579)
*Wendkûuni C. Ouédraogo,Yinghua Li,Xueqi Dang,Pawel Borsukiewicz,Xin Zhou,Anil Koyuncu,Jacques Klein,David Lo,Tegawendé F. Bissyandé*

Main category: cs.SE

TL;DR: CoReEval是首个用于评估LLM代码可读性评估的大规模基准，包含140万次模型-代码片段-提示评估，涵盖10个LLM、3种编程语言、2种代码类型、4种提示策略和9种解码设置，发现基于开发者指导的提示能改善对齐度和解释质量。


<details>
  <summary>Details</summary>
Motivation: 代码可读性对软件理解和维护至关重要，但难以大规模评估。传统静态指标无法捕捉人类判断的主观性和上下文敏感性，而LLM作为可扩展替代方案的行为尚未充分探索。

Method: 构建CoReEval基准，包含140万次评估，涵盖10个LLM、3种编程语言(Java、Python、CUDA)、2种代码类型(功能代码和单元测试)、4种提示策略(ZSL、FSL、CoT、ToT)、9种解码设置，以及针对初级和高级开发者的个性化提示。

Result: 研究发现，基于人类定义可读性维度的开发者指导提示在结构化上下文中改善了对齐度，提高了解释质量，并通过角色框架实现了轻量级个性化。但得分变异性增加，显示了对齐度、稳定性和可解释性之间的权衡。

Conclusion: CoReEval为提示工程、模型对齐研究和人机循环评估提供了坚实基础，可应用于教育、入职培训和CI/CD流水线，使LLM能够作为可解释、适应性强的代码审查者。

Abstract: Code readability is crucial for software comprehension and maintenance, yet
difficult to assess at scale. Traditional static metrics often fail to capture
the subjective, context-sensitive nature of human judgments. Large Language
Models (LLMs) offer a scalable alternative, but their behavior as readability
evaluators remains underexplored. We introduce CoReEval, the first large-scale
benchmark for evaluating LLM-based code readability assessment, comprising over
1.4 million model-snippet-prompt evaluations across 10 state of the art LLMs.
The benchmark spans 3 programming languages (Java, Python, CUDA), 2 code types
(functional code and unit tests), 4 prompting strategies (ZSL, FSL, CoT, ToT),
9 decoding settings, and developer-guided prompts tailored to junior and senior
personas. We compare LLM outputs against human annotations and a validated
static model, analyzing numerical alignment (MAE, Pearson's, Spearman's) and
justification quality (sentiment, aspect coverage, semantic clustering). Our
findings show that developer-guided prompting grounded in human-defined
readability dimensions improves alignment in structured contexts, enhances
explanation quality, and enables lightweight personalization through persona
framing. However, increased score variability highlights trade-offs between
alignment, stability, and interpretability. CoReEval provides a robust
foundation for prompt engineering, model alignment studies, and human in the
loop evaluation, with applications in education, onboarding, and CI/CD
pipelines where LLMs can serve as explainable, adaptable reviewers.

</details>


### [9] [Contrasting the Hyperparameter Tuning Impact Across Software Defect Prediction Scenarios](https://arxiv.org/abs/2510.16665)
*Mohamed Sami Rakha,Andriy Miranskyy,Daniel Alencar da Costa*

Main category: cs.SE

TL;DR: 该研究对比了超参数调优在两种软件缺陷预测场景（IVDP和CVDP）中的影响差异，发现IVDP场景中的性能提升显著大于CVDP场景，且小数据集更容易受到性能影响差异的影响。


<details>
  <summary>Details</summary>
Motivation: 虽然超参数调优可以提升软件缺陷预测性能，但其影响程度可能因不同的预测场景而异。为了提供全面见解并增强SDP建模的稳健性和实用性，需要比较不同SDP场景中超参数调优的影响。

Method: 使用28种机器学习算法、53个软件数据集、两种调优算法和五个优化指标，在IVDP和CVDP两种关键SDP场景中进行实验，并应用统计分析来比较性能影响差异。

Result: IVDP场景中的SDP性能增益显著大于CVDP场景；28种ML算法中有24种的性能增益在不同SDP场景中可能不成立；小软件数据集更容易出现较大的性能影响差异。

Conclusion: 软件工程研究者和从业者在期望通过超参数调优获得性能增益时，应考虑所选SDP场景的影响。

Abstract: Software defect prediction (SDP) is crucial for delivering high-quality
software products. Recent research has indicated that prediction performance
improvements in SDP are achievable by applying hyperparameter tuning to a
particular SDP scenario. However, the positive impact resulting from the
hyperparameter tuning step may differ based on the targeted SDP scenario.
Comparing the impact of hyperparameter tuning across SDP scenarios is necessary
to provide comprehensive insights and enhance the robustness, generalizability,
and, eventually, the practicality of SDP modeling for quality assurance.
  Therefore, in this study, we contrast the impact of hyperparameter tuning
across two pivotal and consecutive SDP scenarios: (1) Inner Version Defect
Prediction (IVDP) and (2) Cross Version Defect Prediction (CVDP). The main
distinctions between the two scenarios lie in the scope of defect prediction
and the selected evaluation setups. This study's experiments use common
evaluation setups, 28 machine learning (ML) algorithms, 53 post-release
software datasets, two tuning algorithms, and five optimization metrics. We
apply statistical analytics to compare the SDP performance impact differences
by investigating the overall impact, the single ML algorithm impact, and
variations across different software dataset sizes.
  The results indicate that the SDP gains within the IVDP scenario are
significantly larger than those within the CVDP scenario. The results reveal
that asserting performance gains for up to 24 out of 28 ML algorithms may not
hold across multiple SDP scenarios. Furthermore, we found that small software
datasets are more susceptible to larger differences in performance impacts.
Overall, the study findings recommend software engineering researchers and
practitioners to consider the effect of the selected SDP scenario when
expecting performance gains from hyperparameter tuning.

</details>


### [10] [QuanBench: Benchmarking Quantum Code Generation with Large Language Models](https://arxiv.org/abs/2510.16779)
*Xiaoyu Guo,Minggu Wang,Jianjun Zhao*

Main category: cs.SE

TL;DR: 提出了QuanBench基准测试，用于评估大语言模型在量子代码生成方面的性能，包含44个编程任务，评估结果显示当前LLMs在生成正确量子代码方面能力有限。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在通用代码生成方面表现良好，但在量子代码生成方面的能力尚未得到充分研究，需要专门的基准测试来评估。

Method: 创建QuanBench基准，包含44个覆盖量子算法、状态准备、门分解和量子机器学习的编程任务，使用功能正确性(Pass@K)和量子语义等价性(过程保真度)进行评估。

Result: 评估结果显示当前LLMs生成正确量子代码的能力有限，总体准确率低于40%，经常出现语义错误，常见失败案例包括过时的API使用、电路构建错误和算法逻辑错误。

Conclusion: QuanBench为未来改进LLMs的量子代码生成能力提供了基础，当前模型在此领域仍有显著提升空间。

Abstract: Large language models (LLMs) have demonstrated good performance in general
code generation; however, their capabilities in quantum code generation remain
insufficiently studied. This paper presents QuanBench, a benchmark for
evaluating LLMs on quantum code generation. QuanBench includes 44 programming
tasks that cover quantum algorithms, state preparation, gate decomposition, and
quantum machine learning. Each task has an executable canonical solution and is
evaluated by functional correctness (Pass@K) and quantum semantic equivalence
(Process Fidelity). We evaluate several recent LLMs, including general-purpose
and code-specialized models. The results show that current LLMs have limited
capability in generating the correct quantum code, with overall accuracy below
40% and frequent semantic errors. We also analyze common failure cases, such as
outdated API usage, circuit construction errors, and incorrect algorithm logic.
QuanBench provides a basis for future work on improving quantum code generation
with LLMs.

</details>


### [11] [More with Less: An Empirical Study of Turn-Control Strategies for Efficient Coding Agents](https://arxiv.org/abs/2510.16786)
*Pengfei Gao,Chao Peng*

Main category: cs.SE

TL;DR: 本文研究了LLM编码代理的轮次控制策略，发现动态轮次策略在保持性能的同时显著降低成本，比固定轮次限制更有效。


<details>
  <summary>Details</summary>
Motivation: LLM编码代理在实际部署中面临高昂且不可预测的成本问题，主要由于轮次增加导致令牌数量二次增长、模型价格高、任务需要多轮次以及代理效率低下。现有研究主要优化单轮性能，而轮次总数控制策略研究不足。

Method: 在SWE-bench上使用三种最先进模型进行实证研究，评估三种轮次控制策略：无限制基线、带提醒的固定轮次限制、以及按需扩展的新型动态轮次策略。

Result: 无限制设置中存在基本权衡，没有单一模型在性能、成本和轮次效率方面都表现优异。固定轮次限制（基线75%分位数）可大幅降低成本（24%-68%）且对解决率影响最小。动态轮次策略表现最佳，在保持或提高解决率的同时进一步降低成本12%-24%。

Conclusion: 动态资源分配是部署强大且经济可行的编码代理的优越方法，为开发者提供了平衡成本与效能的简单有效指南。

Abstract: LLM-powered coding agents, which operate in iterative loops (turns) to solve
software engineering tasks, are becoming increasingly powerful. However, their
practical deployment is hindered by significant and unpredictable costs. This
challenge arises from a combination of factors: quadratically growing token
counts with each turn, the high price of models, the large number of turns
required for real-world tasks, and the tendency of agents to take inefficient
or unnecessary actions. While existing research focuses on optimizing
individual turns, the strategic control of the total number of turns remains an
underexplored area for managing agent performance and cost. To address this
gap, we conduct a comprehensive empirical study on SWE-bench using three
state-of-the-art models and evaluate the impact of three distinct turn-control
strategies: an unrestricted baseline, a fixed-turn limit with reminders, and a
novel dynamic-turn strategy that grants extensions on-demand. Our findings
first reveal a fundamental trade-off in the unrestricted setting, where no
single model excels across performance, cost, and turn efficiency. We then show
that a fixed-turn limit, specifically at the 75th percentile of the baseline,
serves as a "sweet spot", substantially reducing costs (by 24%-68%) with
minimal impact on solve rates. Most significantly, the dynamic-turn strategy
consistently outperforms fixed-limit approaches, achieving comparable or better
solve rates while further reducing costs by an additional 12%-24% by
intelligently allocating resources only to tasks that need them. This work
provides the first systematic analysis of turn-control strategies, offering
simple yet effective guidelines for developers to balance cost and efficacy. We
demonstrate that dynamic resource allocation is a superior, easy-to-implement
approach for deploying powerful yet economically viable coding agents.

</details>


### [12] [When Many-Shot Prompting Fails: An Empirical Study of LLM Code Translation](https://arxiv.org/abs/2510.16809)
*Amirkia Rafiei Oskooei,Kaan Baturalp Cosdan,Husamettin Isiktas,Mehmet S. Aktas*

Main category: cs.SE

TL;DR: 本文研究发现，在代码翻译任务中存在"多样本悖论"：虽然静态相似度指标随示例数量增加而略有提升，但功能正确性在少量示例（5-25个）时达到峰值，更多示例反而会降低性能。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型在代码翻译任务中，增加上下文示例数量（从零样本到多样本）对性能的实际影响，验证"更多示例更好"的假设是否适用于复杂任务。

Method: 通过大规模实证研究，评估超过90,000次翻译，系统性地测试从零样本到最多625个示例的多样本配置，提示长度从约100,000到800,000个token。

Result: 发现功能正确性在少量示例（5-25个）时达到最佳，更多示例反而会降低关键的功能性能，尽管静态相似度指标可能略有改善。

Conclusion: 对于代码翻译任务，少量精心选择的示例质量比数量更重要，挑战了"越多越好"的普遍有效性，强调了最优提示策略的任务依赖性。

Abstract: Large Language Models (LLMs) with vast context windows offer new avenues for
in-context learning (ICL), where providing many examples ("many-shot"
prompting) is often assumed to enhance performance. We investigate this
assumption for the complex task of code translation. Through a large-scale
empirical study of over 90,000 translations, we systematically evaluate the
impact of scaling in-context examples from zero-shot to many-shot
configurations of up to 625 examples, with prompts spanning from approximately
100,000 to 800,000 tokens. Our findings reveal a "many-shot paradox": while
static similarity metrics may modestly improve with more examples, functional
correctness consistently peaks with few-shot prompting (5-25 examples).
Providing substantially more examples often degrades this crucial functional
performance. This study highlights that for code translation, the quality of a
few well-chosen examples outweighs sheer quantity, challenging the universal
efficacy of "more is better" for ICL and underscoring the task-dependent nature
of optimal prompting strategies. Our results have significant implications for
effectively leveraging LLMs in software engineering.

</details>


### [13] [When AI Takes the Wheel: Security Analysis of Framework-Constrained Program Generation](https://arxiv.org/abs/2510.16823)
*Yue Liu,Zhenchang Xing,Shidong Pan,Chakkrit Tantithamthavorn*

Main category: cs.SE

TL;DR: 研究发现，LLM生成的Chrome扩展程序存在严重安全漏洞，漏洞率高达18%-50%，特别是在身份认证和Cookie管理场景中漏洞率可达83%和78%。高级推理模型反而表现更差。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在软件开发中广泛应用，开发者可能只关注程序功能而忽略安全实现问题，需要研究LLM生成框架约束程序的安全性。

Method: 构建ChromeSecBench数据集（140个基于已知漏洞扩展的提示），使用9个先进LLM生成完整Chrome扩展，从场景类型、模型差异和漏洞类别三个维度分析安全性。

Result: LLM生成程序漏洞率惊人地高，大多数漏洞会暴露敏感浏览器数据给不可信代码，高级推理模型生成的漏洞反而更多。

Conclusion: LLM的编码能力与编写安全框架约束程序的能力存在关键差距，需要加强LLM的安全意识训练。

Abstract: In recent years, the AI wave has grown rapidly in software development. Even
novice developers can now design and generate complex framework-constrained
software systems based on their high-level requirements with the help of Large
Language Models (LLMs). However, when LLMs gradually "take the wheel" of
software development, developers may only check whether the program works. They
often miss security problems hidden in how the generated programs are
implemented.
  In this work, we investigate the security properties of framework-constrained
programs generated by state-of-the-art LLMs. We focus specifically on Chrome
extensions due to their complex security model involving multiple privilege
boundaries and isolated components. To achieve this, we built ChromeSecBench, a
dataset with 140 prompts based on known vulnerable extensions. We used these
prompts to instruct nine state-of-the-art LLMs to generate complete Chrome
extensions, and then analyzed them for vulnerabilities across three dimensions:
scenario types, model differences, and vulnerability categories. Our results
show that LLMs produced vulnerable programs at alarmingly high rates (18%-50%),
particularly in Authentication & Identity and Cookie Management scenarios (up
to 83% and 78% respectively). Most vulnerabilities exposed sensitive browser
data like cookies, history, or bookmarks to untrusted code. Interestingly, we
found that advanced reasoning models performed worse, generating more
vulnerabilities than simpler models. These findings highlight a critical gap
between LLMs' coding skills and their ability to write secure
framework-constrained programs.

</details>


### [14] [Will AI also replace inspectors? Investigating the potential of generative AIs in usability inspection](https://arxiv.org/abs/2510.17056)
*Luis F. G. Campos,Leonardo C. Marques,Walter T. Nakamura*

Main category: cs.SE

TL;DR: 本研究比较了生成式AI与人类专家在可用性检查中的表现，发现AI虽然能发现许多新缺陷但误报率较高，AI与人类检查员结合使用效果最佳。


<details>
  <summary>Details</summary>
Motivation: 可用性检查成本高且需要专业知识，随着AI技术的发展，探索生成式AI在可用性检查中的潜力，以提升效率和降低成本。

Method: 使用软件原型，由4名专家和2个AI模型(GPT-4o和Gemini 2.5 Flash)进行评估，采用精确率、召回率和F1分数等指标进行比较分析。

Result: 人类检查员在精确率和整体覆盖率方面表现最佳，AI模型个体表现良好且发现许多新缺陷，但误报率和冗余报告较高。AI与人类结合使用效果最好。

Conclusion: 当前阶段的AI无法替代人类检查员，但可作为有价值的增强工具，提高效率并扩大缺陷覆盖范围，在软件质量评估中具有互补使用价值。

Abstract: Usability inspection is a well-established technique for identifying
interaction issues in software interfaces, thereby contributing to improved
product quality. However, it is a costly process that requires time and
specialized knowledge from inspectors. With advances in Artificial Intelligence
(AI), new opportunities have emerged to support this task, particularly through
generative models capable of interpreting interfaces and performing inspections
more efficiently. This study examines the performance of generative AIs in
identifying usability problems, comparing them to those of experienced human
inspectors. A software prototype was evaluated by four specialists and two AI
models (GPT-4o and Gemini 2.5 Flash), using metrics such as precision, recall,
and F1-score. While inspectors achieved the highest levels of precision and
overall coverage, the AIs demonstrated high individual performance and
discovered many novel defects, but with a higher rate of false positives and
redundant reports. The combination of AIs and human inspectors produced the
best results, revealing their complementarity. These findings suggest that AI,
in its current stage, cannot replace human inspectors but can serve as a
valuable augmentation tool to improve efficiency and expand defect coverage.
The results provide evidence based on quantitative analysis to inform the
discussion on the role of AI in usability inspections, pointing to viable paths
for its complementary use in software quality assessment contexts.

</details>


### [15] [M2QCode: A Model-Driven Framework for Generating Multi-Platform Quantum Programs](https://arxiv.org/abs/2510.17110)
*Xiaoyu Guo,Shinobu Saito,Jianjun Zhao*

Main category: cs.SE

TL;DR: 提出了一种基于模型驱动开发(MDD)的方法，用于支持量子系统的结构化设计和实现，能够自动为多种量子编程语言生成代码。


<details>
  <summary>Details</summary>
Motivation: 随着量子计算的发展，量子编程语言不断涌现，但模型驱动开发在量子系统工程中的应用仍未被充分探索。

Method: 开发了一个MDD框架，支持从高层次模型自动生成多种量子编程语言的代码。

Result: 通过多个案例研究证明了该方法的有效性和实用性。

Conclusion: 该MDD方法能够提高量子系统开发效率，并确保跨异构量子平台的一致性。

Abstract: With the growing interest in quantum computing, the emergence of quantum
supremacy has marked a pivotal milestone in the field. As a result, numerous
quantum programming languages (QPLs) have been introduced to support the
development of quantum algorithms. However, the application of Model-Driven
Development (MDD) in quantum system engineering remains largely underexplored.
This paper presents an MDD-based approach to support the structured design and
implementation of quantum systems. Our framework enables the automatic
generation of quantum code for multiple QPLs, thereby enhancing development
efficiency and consistency across heterogeneous quantum platforms. The
effectiveness and practicality of our approach have been demonstrated through
multiple case studies.

</details>


### [16] [SEER: Enhancing Chain-of-Thought Code Generation through Self-Exploring Deep Reasoning](https://arxiv.org/abs/2510.17130)
*Shuzheng Gao,Chaozheng Wang,Cuiyun Gao,Michael R. Lyu*

Main category: cs.SE

TL;DR: SEER是一个自探索深度推理框架，将代码生成的思维链过程视为决策问题，通过探索多样化推理路径、质量感知模型训练和自适应推理来解决现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有思维链推理方法在代码生成中存在三个关键限制：推理路径多样性不足、中间步骤质量评估缺失、以及"过度思考"可能导致复杂错误的解决方案。

Method: SEER包含三个核心组件：多样化推理路径探索（无需专家或闭源模型）、推理质量感知模型训练（策略模型生成推理步骤，价值模型评估质量）、自适应思维链推理（根据问题动态切换生成方式）。

Result: 该框架能够生成更准确和自适应的代码生成推理过程，解决了推理路径单一、质量评估缺失和过度思考的问题。

Conclusion: SEER通过将代码生成的思维链过程构建为决策问题，提供了一种系统性的解决方案，能够显著提升代码生成的准确性和适应性。

Abstract: Code generation, the task of creating executable programs from natural
language requirements, has recently seen tremendous advances through
Chain-of-Thought (CoT) reasoning, which enables Large Language Models (LLMs) to
develop high-level reasoning plans before writing code. Recent research has
proposed various methods to enhance models' CoT reasoning for code generation
such as prompt engineering and supervised fine-tuning. However, existing
approaches still face three critical limitations: (1) limited exploration of
diverse reasoning paths, which constrains generalization across various
programming scenarios, (2) lack of quality assessment for intermediate
reasoning steps, which hampers the reliability of the generated plans and code,
and (3) the potential negative impact of "overthinking", potentially leading to
unnecessarily complex and incorrect solutions. To address these limitations, we
frame CoT code generation as a decision making problem and present SEER, a
SElf-Exploring deep Reasoning framework that enables accurate and adaptive
reasoning for code generation. SEER introduces three key components: (1)
Diverse reasoning path exploration, which aims at exploring diverse reasoning
paths and annotating intermediate steps without relying on manual experts or
closed-source proprietary models; (2) Reasoning quality-aware model training,
which trains a policy model for generating candidate reasoning steps and a
value model for assessing their quality; and (3) Adaptive CoT reasoning, which
dynamically switches between direct generation and step-by-step reasoning for
different problems.

</details>


### [17] [PEACE: Towards Efficient Project-Level Efficiency Optimization via Hybrid Code Editing](https://arxiv.org/abs/2510.17142)
*Xiaoxue Ren,Jun Wan,Yun Peng,Zhongxin Liu,Ming Liang,Dajun Chen,Wei Jiang,Yong Li*

Main category: cs.SE

TL;DR: 提出了Peace框架，通过自动代码编辑进行项目级代码效率优化，解决了现有方法局限于函数级优化和忽略函数间交互的问题。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在代码效率优化方面主要关注函数级优化，忽视了函数间的交互，无法适应真实开发场景。代码编辑技术有潜力进行项目级优化，但面临无效编辑和次优内部函数的挑战。

Method: Peace框架包含三个关键阶段：依赖感知的优化函数序列构建、有效关联编辑识别、效率优化编辑迭代。该混合框架确保项目的整体正确性和完整性。

Result: 在PeacExec基准测试中，Peace在146个真实优化任务上表现优异，达到69.2%正确率、+46.9%优化率和0.840执行效率加速比，显著优于现有基线方法。

Conclusion: Peace框架在项目级代码效率优化方面具有显著优势，特别是在涉及多个函数的复杂优化任务中表现突出，验证了混合框架设计的合理性和有效性。

Abstract: Large Language Models (LLMs) have demonstrated significant capability in code
generation, but their potential in code efficiency optimization remains
underexplored. Previous LLM-based code efficiency optimization approaches
exclusively focus on function-level optimization and overlook interaction
between functions, failing to generalize to real-world development scenarios.
Code editing techniques show great potential for conducting project-level
optimization, yet they face challenges associated with invalid edits and
suboptimal internal functions. To address these gaps, we propose Peace, a novel
hybrid framework for Project-level code Efficiency optimization through
Automatic Code Editing, which also ensures the overall correctness and
integrity of the project. Peace integrates three key phases: dependency-aware
optimizing function sequence construction, valid associated edits
identification, and efficiency optimization editing iteration. To rigorously
evaluate the effectiveness of Peace, we construct PeacExec, the first benchmark
comprising 146 real-world optimization tasks from 47 high-impact GitHub Python
projects, along with highly qualified test cases and executable environments.
Extensive experiments demonstrate Peace's superiority over the state-of-the-art
baselines, achieving a 69.2% correctness rate (pass@1), +46.9% opt rate, and
0.840 speedup in execution efficiency. Notably, our Peace outperforms all
baselines by significant margins, particularly in complex optimization tasks
with multiple functions. Moreover, extensive experiments are also conducted to
validate the contributions of each component in Peace, as well as the rationale
and effectiveness of our hybrid framework design.

</details>


### [18] [TREAT: A Code LLMs Trustworthiness / Reliability Evaluation and Testing Framework](https://arxiv.org/abs/2510.17163)
*Shuzheng Gao,Eric John Li,Man Ho Lam,Jingyu Xiao,Yuxuan Wan,Chaozheng Wang,Ng Man Tik,Michael R. Lyu*

Main category: cs.SE

TL;DR: 提出了TREAT评估框架，用于全面评估代码大模型在真实软件工程场景中的可信度，解决了现有基准测试在任务范围、多语言多模态、鲁棒性和评估方法等方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的大模型评估基准存在任务范围有限、缺乏鲁棒性和可靠性评估等问题，无法全面衡量模型在真实软件工程场景中的可信度。

Method: 开发了TREAT评估框架，包含四个主要改进：多任务全面评估、多语言多模态评估、鲁棒性评估（语义保持的代码转换）和严格的评估方法（多样化提示和自适应解决方案提取）。

Result: 评估了26个最先进模型，发现：当前模型在不同编程任务中表现差异显著；多模态语言模型在UI代码生成和编辑方面存在特定性能限制。

Conclusion: TREAT框架为代码大模型的可信度评估提供了更全面的方法，揭示了现有模型的性能差异和特定限制，为未来模型改进提供了重要见解。

Abstract: Large foundation models are fundamentally transforming the software
engineering landscape, demonstrating exceptional capabilities across diverse
tasks such as code generation, debugging, and testing. Despite this rapid
progress, a significant gap remains in how to comprehensively evaluate these
models' trustworthiness in real-world software engineering scenarios. Existing
benchmarks suffer from limited task scope and fail to incorporate critical
evaluation aspects such as the robustness and reliability of models. To bridge
this gap, we present an evaluation framework called TREAT (Code LLMs
Trustworthiness / Reliability Evaluation And Testing) that provides a holistic
assessment of model performance in code intelligence tasks. Our evaluation
framework addresses key limitations in existing approaches with four main
improvements: (1) Multi-Task Holistic Evaluation that spans diverse software
engineering activities rather than limited coding tasks; (2) Multi-Language and
Multi-Modality Assessment that extends beyond traditional single-language,
text-only benchmarks to include multi-modality coding tasks; (3) Robustness
Assessment that evaluates model reliability under semantically-preserving code
transformations; and (4) Rigorous Evaluation Methodology that enhances the
trustworthiness of evaluation results through diverse evaluation prompts and
adaptive solution extraction. Based on this evaluation framework, we assess 26
state-of-the-art models and uncover both their strengths and limitations,
yielding several key insights:(1) Current models show substantial performance
variation across programming tasks; (2) Multi-modal language models demonstrate
specific performance limitations in UI code generation and edit;

</details>


### [19] [Software Testing with Large Language Models: An Interview Study with Practitioners](https://arxiv.org/abs/2510.17164)
*Maria Deolinda Santana,Cleyton Magalhaes,Ronnie de Souza Santos*

Main category: cs.SE

TL;DR: 本研究通过定性访谈调查了软件测试专业人员如何使用LLMs，提出了一个基于实践者经验的初步指导方针，强调迭代式提示工程、人工监督和验证的重要性。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在软件测试中的快速应用，目前缺乏结构化指导，主要依赖非正式实验。本研究旨在了解测试专业人员实际使用LLMs的方式，为将其整合到测试工作流程提供实践者指导。

Method: 采用定性研究方法，对15名来自不同角色和领域的软件测试人员进行半结构化访谈，使用基于扎根理论的主题分析过程分析数据。

Result: 测试人员描述了一个迭代和反思的过程，包括定义测试目标、应用提示工程策略、优化提示、评估输出和持续学习。他们强调需要人工监督和仔细验证，特别是考虑到LLMs的幻觉和不一致推理等已知限制。

Conclusion: LLMs在软件测试中的应用正在增长，但仍受到实践演变和对风险谨慎态度的影响。本研究为在测试环境中结构化使用LLMs提供了起点，并邀请未来研究在不同团队、工具和任务中完善这些实践。

Abstract: \textit{Background:} The use of large language models in software testing is
growing fast as they support numerous tasks, from test case generation to
automation, and documentation. However, their adoption often relies on informal
experimentation rather than structured guidance. \textit{Aims:} This study
investigates how software testing professionals use LLMs in practice to propose
a preliminary, practitioner-informed guideline to support their integration
into testing workflows. \textit{Method:} We conducted a qualitative study with
15 software testers from diverse roles and domains. Data were collected through
semi-structured interviews and analyzed using grounded theory-based processes
focused on thematic analysis. \textit{Results:} Testers described an iterative
and reflective process that included defining testing objectives, applying
prompt engineering strategies, refining prompts, evaluating outputs, and
learning over time. They emphasized the need for human oversight and careful
validation, especially due to known limitations of LLMs such as hallucinations
and inconsistent reasoning. \textit{Conclusions:} LLM adoption in software
testing is growing, but remains shaped by evolving practices and caution around
risks. This study offers a starting point for structuring LLM use in testing
contexts and invites future research to refine these practices across teams,
tools, and tasks.

</details>


### [20] [OLIVAW: ACIMOV's GitHub robot assisting agile collaborative ontology development](https://arxiv.org/abs/2510.17184)
*Nicolas Robert,Fabien Gandon,Maxime Lefrançois*

Main category: cs.SE

TL;DR: OLIVAW是一个支持ACIMOV方法论的工具，通过GitHub Composite Actions、pre-commit hooks或命令行界面协助模块化本体开发，基于W3C标准。


<details>
  <summary>Details</summary>
Motivation: 敏捷和协作的本体设计方法对于确保本体与用户需求保持一致、保持最新状态并随系统演进至关重要，因此需要适当的持续验证工具。

Method: 提出OLIVAW工具，支持ACIMOV方法论，基于W3C标准，通过GitHub Composite Actions、pre-commit hooks或命令行界面协助模块化本体开发。

Result: OLIVAW在多个本体项目上进行了测试，证明了其有用性、通用性和可重用性，并提供了模板仓库以便快速开始使用。

Conclusion: OLIVAW是一个有效的工具，支持敏捷和协作的本体开发，确保本体在整个开发过程中与开发者需求匹配。

Abstract: Agile and collaborative approaches to ontologies design are crucial because
they contribute to making them userdriven, up-to-date, and able to evolve
alongside the systems they support, hence proper continuous validation tooling
is required to ensure ontologies match developers' requirements all along their
development. We propose OLIVAW (Ontology Long-lived Integration Via ACIMOV
Workflow), a tool supporting the ACIMOV methodology on GitHub. It relies on W3C
Standards to assist the development of modular ontologies through GitHub
Composite Actions, pre-commit hooks, or a command line interface. OLIVAW was
tested on several ontology projects to ensure its usefulness, genericity and
reusability. A template repository is available for a quick start. OLIVAW is

</details>


### [21] [AdapTrack: Constrained Decoding without Distorting LLM's Output Intent](https://arxiv.org/abs/2510.17376)
*Yongmin Li,Jia Li,Ge Li,Zhi Jin*

Main category: cs.SE

TL;DR: AdapTrack是一种改进的约束解码方法，通过引入回溯机制避免扭曲模型输出意图，在保持约束合规的同时更好地与模型语义意图对齐。


<details>
  <summary>Details</summary>
Motivation: 传统约束解码技术虽然能确保代码满足语法或API约束，但会扭曲模型的输出意图，导致生成的代码虽然符合约束但语义上不正确。

Method: AdapTrack在生成过程中引入回溯机制，避免强制模型选择违反其意图的选项，确保输出分布与模型原始分布一致。

Result: 在API补全数据集上相比约束解码提升360.87%，在真实API数据集上提升38.93%，在HumanEval和MBPP基准上分别提升7.84%和6.42%。

Conclusion: 通过更好地遵循模型输出意图，AdapTrack能显著提升代码生成质量，理论证明其输出分布与模型给定生成标记的分布一致。

Abstract: Language model-based code generation and completion tools have been widely
adopted, but they may sometimes produce code that does not meet necessary
constraints, such as syntactic correctness or API existence. Constrained
decoding techniques are developed to help the model generate code adhering to
the constraints by greedily eliminating generation options that violate
constraints at each step of the generation process. However, there is a severe
limitation of constrained decoding, that it distorts the model's output intent,
forcing it to produce code that may satisfy the constraint but does not match
the development intent and is therefore incorrect. In response to this
challenge, we propose AdapTrack. By incorporating backtracking into the
generation process, AdapTrack avoids distorting the output intent of the model,
thereby producing results that are not only constraint-compliant but also more
semantically aligned with model's output intent. On our synthetic API
completion dataset, AdapTrack can achieve up to 360.87% improvement compared to
constrained decoding; on the real-world API completion dataset we collect that
exhibits similar issues, AdapTrack can achieve up to 38.93% improvement over
constrained decoding; in general code genration benchmarks, compared to
constrained decoding, AdapTrack can achieve up to 7.84% improvement on
HumanEval, and up to 6.42% improvement on MBPP. This indicates that, simply by
better adhering to the model's output intent, AdapTrack can achieve significant
improvements. We provide a theoretical proof that the distribution produced by
AdapTrack aligns with the model's distribution given the generated tokens,
thereby ensuring that the model's output intent is not distorted. Experiments
on DSL problems show that, compared to existing methods, our approach can
provide generation results that are more consistent with the language model's
distribution.

</details>


### [22] [Scalable CI/CD for Legacy Modernization: An Industrial Experience Addressing Internal Challenges Related to the 2025 Japan Cliff](https://arxiv.org/abs/2510.17430)
*Kuniaki Kudo,Sherine Devi*

Main category: cs.SE

TL;DR: 为解决日本2025年IT系统老化危机，开发了可扩展的CI/CD流水线，通过动态创建隔离开发环境、自动化流程和容器化技术，降低维护成本并推动数字化转型。


<details>
  <summary>Details</summary>
Motivation: 日本面临2025年IT系统大规模服务终止的危机，传统系统维护成本高昂且难以更新，可能导致每年12万亿日元损失。朝日公司也面临类似挑战，手动维护流程和有限QA环境导致系统过时。

Method: 开发可扩展CI/CD流水线，集成GitHub源代码控制、Jenkins流水线自动化、AWS可扩展环境和Docker容器化技术，实现动态创建和删除隔离开发环境。

Result: 通过可扩展CI/CD，开发者可以在独立环境中自由安全地测试维护程序和新技术实验，显著降低维护成本。

Conclusion: 可扩展CI/CD流水线有效解决了传统IT系统维护难题，为数字化转型提供了技术支撑，是应对日本2025年IT危机的可行方案。

Abstract: We have developed a Scalable CI/CD Pipeline to address internal challenges
related to Japan 2025 cliff problem, a critical issue where the mass end of
service life of legacy core IT systems threatens to significantly increase the
maintenance cost and black box nature of these system also leads to difficult
update moreover replace, which leads to lack of progress in Digital
Transformation (DX). If not addressed, Japan could potentially lose up to 12
trillion yen per year after 2025, which is 3 times more than the cost in
previous years. Asahi also faced the same internal challenges regarding legacy
system, where manual maintenance workflows and limited QA environment have left
critical systems outdated and difficult to update. Middleware and OS version
have remained unchanged for years, leading to now its nearing end of service
life which require huge maintenance cost and effort to continue its operation.
To address this problem, we have developed and implemented a Scalable CI/CD
Pipeline where isolated development environments can be created and deleted
dynamically and is scalable as needed. This Scalable CI/CD Pipeline incorporate
GitHub for source code control and branching, Jenkins for pipeline automation,
Amazon Web Services for scalable environment, and Docker for environment
containerization. This paper presents the design and architecture of the
Scalable CI/CD Pipeline, with the implementation along with some use cases.
Through Scalable CI/CD, developers can freely and safely test maintenance
procedures and do experiments with new technology in their own environment,
reducing maintenance cost and drive Digital Transformation (DX).
  key words: 2025 Japan Cliff, Scalable CI/CD, DevOps, Legacy IT Modernization.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [23] [Hierarchical Multi-Modal Threat Intelligence Fusion Without Aligned Data: A Practical Framework for Real-World Security Operations](https://arxiv.org/abs/2510.15953)
*Sisir Doppalapudi*

Main category: cs.CR

TL;DR: HM-TIF框架针对多模态威胁检测中数据自然不对齐的现实挑战，提出了层次化多模态威胁情报融合方法，无需真实对齐数据即可实现88.7%的准确率，并降低32%误报率。


<details>
  <summary>Details</summary>
Motivation: 解决多模态威胁检测中安全工具孤立运行、网络、邮件和系统数据流缺乏自然对齐和关联的根本挑战。

Method: 采用层次化交叉注意力机制和动态权重调整，结合新颖的时间关联协议来保持统计独立性，无需假设或创建人工对齐。

Result: 在UNSW-NB15、CSE-CIC-IDS2018和CICBell-DNS2021数据集上达到88.7%准确率，误报率降低32%，在模态缺失时仍保持鲁棒性。

Conclusion: HM-TIF是首个专为非对齐数据设计的多模态安全框架，即使在数据流不完整的情况下也能立即部署到实际安全运营中。

Abstract: Multi-modal threat detection faces a fundamental challenge that involves
security tools operating in isolation, and this creates streams of network,
email, and system data with no natural alignment or correlation. We present
Hierarchical Multi-Modal Threat Intelligence Fusion (HM-TIF), a framework
explicitly designed for this realistic scenario where naturally aligned
multi-modal attack data does not exist. Unlike prior work that assumes or
creates artificial alignment, we develop principled methods for correlating
independent security data streams while maintaining operational validity. Our
architecture employs hierarchical cross-attention with dynamic weighting that
adapts to data availability and threat context, coupled with a novel temporal
correlation protocol that preserves statistical independence. Evaluation on
UNSW-NB15, CSE-CIC-IDS2018, and CICBell-DNS2021 datasets demonstrates that
HM-TIF achieves 88.7% accuracy with a critical 32% reduction in false positive
rates, even without true multi-modal training data. The framework maintains
robustness when modalities are missing, making it immediately deployable in
real security operations where data streams frequently have gaps. Our
contributions include: (i) the first multi-modal security framework explicitly
designed for non-aligned data, (ii) a temporal correlation protocol that avoids
common data leakage pitfalls, (iii) empirical validation that multi-modal
fusion provides operational benefits even without perfect alignment, and (iv)
practical deployment guidelines for security teams facing heterogeneous,
uncoordinated data sources. Index Terms: multi-modal learning, threat
intelligence, non-aligned data, operational security, cross-attention
mechanisms, practical deployment

</details>


### [24] [A Graph-Attentive LSTM Model for Malicious URL Detection](https://arxiv.org/abs/2510.15971)
*Md. Ifthekhar Hossain,Kazi Abdullah Al Arafat,Bryce Shepard,Kayd Craig,Imtiaz Parvez*

Main category: cs.CR

TL;DR: 提出了一种名为GNN-GAT-LSTM的混合深度学习模型，结合图神经网络、图注意力网络和LSTM网络，用于恶意URL检测，在651,191个URL数据集上取得了98.06%的准确率和98.04%的F1分数。


<details>
  <summary>Details</summary>
Motivation: 恶意URL带来严重安全风险，包括网络钓鱼、恶意软件分发和网站篡改。传统黑名单检测方法无法识别新的或混淆的URL，因为它们依赖预先存在的模式。

Method: 将URL转换为图结构，字符作为节点通过边连接，使用one-hot编码表示节点特征。结合GNN、GAT和LSTM网络提取特征的结构和序列模式，采用特征工程和数据平衡技术处理类别不平衡问题。

Result: 模型在测试集上达到0.9806的准确率和0.9804的加权F1分数，在良性URL和篡改URL类别上表现出优异的精确率和召回率。

Conclusion: 该模型为恶意URL检测提供了高效且可扩展的系统，在现实网络安全应用中展现出强大潜力。

Abstract: Malicious URLs pose significant security risks as they facilitate phishing
attacks, distribute malware, and empower attackers to deface websites.
Blacklist detection methods fail to identify new or obfuscated URLs because
they depend on pre-existing patterns. This work presents a hybrid deep learning
model named GNN-GAT-LSTM that combines Graph Neural Networks (GNNs) with Graph
Attention Networks (GATs) and Long Short-Term Memory (LSTM) networks. The
proposed architecture extracts both the structural and sequential patterns of
the features from data. The model transforms URLs into graphs through a process
where characters become nodes that connect through edges. It applies one-hot
encoding to represent node features. The model received training and testing
data from a collection of 651,191 URLs, which were classified into benign,
phishing, defacement, and malware categories. The preprocessing stage included
both feature engineering and data balancing techniques, which addressed the
class imbalance issue to enhance model learning. The GNN-GAT-LSTM model
achieved outstanding performance through its test accuracy of 0.9806 and its
weighted F1-score of 0.9804. It showed excellent precision and recall
performance across most classes, particularly for benign and defacement URLs.
Overall, the model provides an efficient and scalable system for detecting
malicious URLs while demonstrating strong potential for real-world
cybersecurity applications.

</details>


### [25] [Safeguarding Efficacy in Large Language Models: Evaluating Resistance to Human-Written and Algorithmic Adversarial Prompts](https://arxiv.org/abs/2510.15973)
*Tiarnaigh Downey-Webb,Olamide Jogunola,Oluwaseun Ajao*

Main category: cs.CR

TL;DR: 对四个主流大语言模型进行系统性安全评估，发现Llama-2安全性最高，Phi-2最脆弱，攻击在不同模型间存在显著可迁移性。


<details>
  <summary>Details</summary>
Motivation: 评估主流大语言模型对抗多样化对抗攻击的安全性，识别跨模型安全漏洞模式。

Method: 使用SALAD-Bench数据集的1200个分层提示，测试四种攻击方法：人工编写提示、AutoDAN、GCG和TAP，评估六个危害类别。

Result: Llama-2整体安全性最高（平均攻击成功率3.4%），Phi-2最脆弱（7.0%）。GCG和TAP攻击在模型间可迁移性强，对GPT-4成功率高达17%。恶意使用提示攻击成功率最高（10.71%）。

Conclusion: 大语言模型安全性存在显著差异，攻击具有跨模型可迁移性，需要针对性的防御机制。

Abstract: This paper presents a systematic security assessment of four prominent Large
Language Models (LLMs) against diverse adversarial attack vectors. We evaluate
Phi-2, Llama-2-7B-Chat, GPT-3.5-Turbo, and GPT-4 across four distinct attack
categories: human-written prompts, AutoDAN, Greedy Coordinate Gradient (GCG),
and Tree-of-Attacks-with-pruning (TAP). Our comprehensive evaluation employs
1,200 carefully stratified prompts from the SALAD-Bench dataset, spanning six
harm categories. Results demonstrate significant variations in model
robustness, with Llama-2 achieving the highest overall security (3.4% average
attack success rate) while Phi-2 exhibits the greatest vulnerability (7.0%
average attack success rate). We identify critical transferability patterns
where GCG and TAP attacks, though ineffective against their target model
(Llama-2), achieve substantially higher success rates when transferred to other
models (up to 17% for GPT-4). Statistical analysis using Friedman tests reveals
significant differences in vulnerability across harm categories ($p < 0.001$),
with malicious use prompts showing the highest attack success rates (10.71%
average). Our findings contribute to understanding cross-model security
vulnerabilities and provide actionable insights for developing targeted defense
mechanisms

</details>


### [26] [Generative AI for Biosciences: Emerging Threats and Roadmap to Biosecurity](https://arxiv.org/abs/2510.15975)
*Zaixi Zhang,Souradip Chakraborty,Amrit Singh Bedi,Emilin Mathew,Varsha Saravanan,Le Cong,Alvaro Velasquez,Sheng Lin-Gibson,Megan Blewett,Dan Hendrycs,Alex John London,Ellen Zhong,Ben Raphael,Jian Ma,Eric Xing,Russ Altman,George Church,Mengdi Wang*

Main category: cs.CR

TL;DR: 生成式AI在生物科学中的快速发展带来了新的生物安全威胁，包括合成病毒蛋白或毒素的生成等双重用途风险。现有安全措施脆弱，可通过欺骗性提示或越狱技术绕过。专家呼吁制定新的治理框架和多层次安全防御策略。


<details>
  <summary>Details</summary>
Motivation: 生成式AI在生物技术、医学和合成生物学中的快速应用虽然带来了进步，但也引入了新的脆弱性。这些双重用途风险常被忽视，现有安全防护措施脆弱且易被绕过，需要紧急关注和治理。

Method: 通过130位来自学术界、政府、工业和政策领域的专家访谈，分析生成式AI在生物科学中的现状、新兴威胁向量，以及监管和监督方面的紧迫差距。

Result: 约76%的专家对AI在生物学中的滥用表示担忧，74%呼吁开发新的治理框架。研究提出了多层次的安全防御策略，包括严格的数据过滤、开发过程中的伦理对齐以及实时监控以阻止有害请求。

Conclusion: 随着生成式AI融入生物科学，保护这一前沿领域需要立即致力于适应性治理和安全设计技术，在整个生成式AI生命周期中嵌入安全性。

Abstract: The rapid adoption of generative artificial intelligence (GenAI) in the
biosciences is transforming biotechnology, medicine, and synthetic biology. Yet
this advancement is intrinsically linked to new vulnerabilities, as GenAI
lowers the barrier to misuse and introduces novel biosecurity threats, such as
generating synthetic viral proteins or toxins. These dual-use risks are often
overlooked, as existing safety guardrails remain fragile and can be
circumvented through deceptive prompts or jailbreak techniques. In this
Perspective, we first outline the current state of GenAI in the biosciences and
emerging threat vectors ranging from jailbreak attacks and privacy risks to the
dual-use challenges posed by autonomous AI agents. We then examine urgent gaps
in regulation and oversight, drawing on insights from 130 expert interviews
across academia, government, industry, and policy. A large majority ($\approx
76$\%) expressed concern over AI misuse in biology, and 74\% called for the
development of new governance frameworks. Finally, we explore technical
pathways to mitigation, advocating a multi-layered approach to GenAI safety.
These defenses include rigorous data filtering, alignment with ethical
principles during development, and real-time monitoring to block harmful
requests. Together, these strategies provide a blueprint for embedding security
throughout the GenAI lifecycle. As GenAI becomes integrated into the
biosciences, safeguarding this frontier requires an immediate commitment to
both adaptive governance and secure-by-design technologies.

</details>


### [27] [Learning to Watermark: A Selective Watermarking Framework for Large Language Models via Multi-Objective Optimization](https://arxiv.org/abs/2510.15976)
*Chenrui Wang,Junyi Shu,Billy Chiu,Yu Li,Saleh Alharbi,Min Zhang,Jing Li*

Main category: cs.CR

TL;DR: 提出了一种名为LTW的选择性水印框架，通过多目标优化平衡水印可检测性和文本质量，使用轻量级网络自适应决定何时应用水印。


<details>
  <summary>Details</summary>
Motivation: 现有LLM水印技术在水印可检测性和生成文本质量之间存在权衡，需要一种能够平衡这两个竞争目标的方法。

Method: LTW框架使用轻量级网络分析句子嵌入、令牌熵和当前水印比例来自适应决定何时应用水印，通过两个专门构建的损失函数进行多目标优化训练。

Result: 实验评估表明，LTW与两种基线水印方法结合后，在保持水印可检测性的同时显著提高了文本质量。

Conclusion: LTW为LLM水印设计提供了新视角，能够在保持高文本质量的同时实现有效水印。

Abstract: The rapid development of LLMs has raised concerns about their potential
misuse, leading to various watermarking schemes that typically offer high
detectability. However, existing watermarking techniques often face trade-off
between watermark detectability and generated text quality. In this paper, we
introduce Learning to Watermark (LTW), a novel selective watermarking framework
that leverages multi-objective optimization to effectively balance these
competing goals. LTW features a lightweight network that adaptively decides
when to apply the watermark by analyzing sentence embeddings, token entropy,
and current watermarking ratio. Training of the network involves two
specifically constructed loss functions that guide the model toward
Pareto-optimal solutions, thereby harmonizing watermark detectability and text
quality. By integrating LTW with two baseline watermarking methods, our
experimental evaluations demonstrate that LTW significantly enhances text
quality without compromising detectability. Our selective watermarking approach
offers a new perspective for designing watermarks for LLMs and a way to
preserve high text quality for watermarks. The code is publicly available at:
https://github.com/fattyray/learning-to-watermark

</details>


### [28] [Meta-Guardian: An Early Evaluation of an On-device Application to Mitigate Psychography Data Leakage in Immersive Technologies](https://arxiv.org/abs/2510.15989)
*Keshav Sood,Sanjay Selvaraj,Youyang Qu*

Main category: cs.CR

TL;DR: 提出了一种名为Meta-Guardian的隐私保护系统架构，能够在VR头显中实时识别和过滤生物特征信号，防止敏感数据传输或存储。


<details>
  <summary>Details</summary>
Motivation: 沉浸式技术需要收集生物特征数据来创造沉浸式体验，但这些实时反馈信息（包括生物特征）由于数据的敏感性而引发严重的隐私问题。现有文献大多忽视了头戴式显示系统中实时生物特征数据过滤的复杂性。

Method: 开发了一个模块化的Unity软件开发工具包（SDK），与主要沉浸式平台兼容。该系统采用机器学习模型进行信号分类，并使用过滤机制来阻止敏感数据。

Result: 实现了一个隐私保护框架，使开发者能够在各种头显和应用程序中嵌入隐私设计原则。

Conclusion: 提出的Meta-Guardian系统为解决沉浸式技术中的生物特征数据隐私问题提供了一种有效的实时过滤解决方案。

Abstract: The use of Immersive Technologies has shown its potential to revolutionize
many sectors such as health, entertainment, education, and industrial sectors.
Immersive technologies such as Virtual Reality (VR), Augmented reality (AR),
and Mixed Reality (MR) have redefined user interaction through real-time
biometric and behavioral tracking. Although Immersive Technologies (XR)
essentially need the collection of the biometric data which acts as a baseline
to create immersive experience, however, this ongoing feedback information
(includes biometrics) poses critical privacy concerns due to the sensitive
nature of the data collected. A comprehensive review of recent literature
explored the technical dimensions of related problem; however, they largely
overlook the challenge particularly the intricacies of real-time biometric data
filtering within head-mounted display system. Motivated from this, in this
work, we propose a novel privacy-preserving system architecture that identifies
and filters biometric signals (within the VR headset) in real-time before
transmission or storage. Implemented as a modular Unity Software-development
Kit (SDK) compatible with major immersive platforms, our solution (named
Meta-Guardian) employs machine learning models for signal classification and a
filtering mechanism to block sensitive data. This framework aims to enable
developers to embed privacy-by-design principles into immersive experiences on
various headsets and applications.

</details>


### [29] [MCP Security Bench (MSB): Benchmarking Attacks Against Model Context Protocol in LLM Agents](https://arxiv.org/abs/2510.15994)
*Dongsen Zhang,Zekun Li,Xu Luo,Xuannan Liu,Peipei Li,Wenjun Xu*

Main category: cs.CR

TL;DR: MSB是首个端到端的MCP安全基准测试套件，系统评估LLM代理在完整工具使用流程中对MCP特定攻击的抵抗能力。


<details>
  <summary>Details</summary>
Motivation: MCP协议虽然实现了广泛的互操作性，但也扩大了攻击面，使工具成为具有自然语言元数据和标准化I/O的一等可组合对象，需要系统评估其安全性。

Method: 开发了包含12种攻击分类的评估框架，通过运行真实工具（良性和恶意）而非模拟来执行攻击，并提出了Net Resilient Performance (NRP)鲁棒性指标来量化安全与性能的权衡。

Result: 评估了9个流行LLM代理在10个领域和400+工具上的2000个攻击实例，结果显示性能更强的模型由于出色的工具调用和指令遵循能力而更容易受到攻击。

Conclusion: MSB为研究人员和从业者研究、比较和加固MCP代理提供了实用的基线基准。

Abstract: The Model Context Protocol (MCP) standardizes how large language model (LLM)
agents discover, describe, and call external tools. While MCP unlocks broad
interoperability, it also enlarges the attack surface by making tools
first-class, composable objects with natural-language metadata, and
standardized I/O. We present MSB (MCP Security Benchmark), the first end-to-end
evaluation suite that systematically measures how well LLM agents resist
MCP-specific attacks throughout the full tool-use pipeline: task planning, tool
invocation, and response handling. MSB contributes: (1) a taxonomy of 12
attacks including name-collision, preference manipulation, prompt injections
embedded in tool descriptions, out-of-scope parameter requests,
user-impersonating responses, false-error escalation, tool-transfer, retrieval
injection, and mixed attacks; (2) an evaluation harness that executes attacks
by running real tools (both benign and malicious) via MCP rather than
simulation; and (3) a robustness metric that quantifies the trade-off between
security and performance: Net Resilient Performance (NRP). We evaluate nine
popular LLM agents across 10 domains and 400+ tools, producing 2,000 attack
instances. Results reveal the effectiveness of attacks against each stage of
MCP. Models with stronger performance are more vulnerable to attacks due to
their outstanding tool calling and instruction following capabilities. MSB
provides a practical baseline for researchers and practitioners to study,
compare, and harden MCP agents.

</details>


### [30] [Breaking Guardrails, Facing Walls: Insights on Adversarial AI for Defenders & Researchers](https://arxiv.org/abs/2510.16005)
*Giacomo Bertollo,Naz Bodemir,Jonah Burgess*

Main category: cs.CR

TL;DR: 对500名CTF参与者的分析显示，参与者能轻松绕过简单的AI防护措施，但多层多步防御仍构成显著挑战，为构建更安全的AI系统提供了具体见解。


<details>
  <summary>Details</summary>
Motivation: 研究AI系统的安全防护机制在实际攻击场景中的有效性，特别是测试不同复杂度防御策略的抵抗能力。

Method: 通过分析500名CTF（夺旗赛）参与者的攻击行为，评估他们对简单AI防护和多层多步防御的突破能力。

Result: 参与者能够轻松绕过简单的AI防护措施，但面对复杂的多层多步防御时遇到了显著困难。

Conclusion: 多层多步防御策略在保护AI系统方面比简单防护更有效，这为设计更安全的AI系统提供了重要参考。

Abstract: Analyzing 500 CTF participants, this paper shows that while participants
readily bypassed simple AI guardrails using common techniques, layered
multi-step defenses still posed significant challenges, offering concrete
insights for building safer AI systems.

</details>


### [31] [On-Chain Decentralized Learning and Cost-Effective Inference for DeFi Attack Mitigation](https://arxiv.org/abs/2510.16024)
*Abdulrahman Alhaidari,Balaji Palanisamy,Prashant Krishnamurthy*

Main category: cs.CR

TL;DR: 提出了首个去中心化的全链上学习框架，通过Layer-2进行高成本计算，在Layer-1进行验证模型更新，并在智能合约内实现低延迟推理，以防止DeFi平台中的漏洞利用。


<details>
  <summary>Details</summary>
Motivation: DeFi平台每年因业务逻辑或会计漏洞而损失数十亿美元，现有防御方法无法防止通过私有中继或恶意合约在同一区块内执行的攻击。

Method: 采用Proof-of-Improvement协议管理训练过程，通过量化技术和循环展开技术实现逻辑回归、SVM、MLPs、CNNs和门控RNNs在以太坊区块gas限制内的推理。

Result: 收集了298个真实世界漏洞利用案例（2020-2025年），涉及402次利用交易，总损失达37.4亿美元。

Conclusion: 该框架能够有效防止DeFi漏洞利用，通过去中心化学习和链上推理提供安全保障。

Abstract: Billions of dollars are lost every year in DeFi platforms by transactions
exploiting business logic or accounting vulnerabilities. Existing defenses
focus on static code analysis, public mempool screening, attacker contract
detection, or trusted off-chain monitors, none of which prevents exploits
submitted through private relays or malicious contracts that execute within the
same block. We present the first decentralized, fully on-chain learning
framework that: (i) performs gas-prohibitive computation on Layer-2 to reduce
cost, (ii) propagates verified model updates to Layer-1, and (iii) enables
gas-bounded, low-latency inference inside smart contracts. A novel
Proof-of-Improvement (PoIm) protocol governs the training process and verifies
each decentralized micro update as a self-verifying training transaction.
Updates are accepted by \textit{PoIm} only if they demonstrably improve at
least one core metric (e.g., accuracy, F1-score, precision, or recall) on a
public benchmark without degrading any of the other core metrics, while
adversarial proposals get financially penalized through an adaptable test set
for evolving threats. We develop quantization and loop-unrolling techniques
that enable inference for logistic regression, SVM, MLPs, CNNs, and gated RNNs
(with support for formally verified decision tree inference) within the
Ethereum block gas limit, while remaining bit-exact to their off-chain
counterparts, formally proven in Z3. We curate 298 unique real-world exploits
(2020 - 2025) with 402 exploit transactions across eight EVM chains,
collectively responsible for \$3.74 B in losses.

</details>


### [32] [Resource Estimation of CGGI and CKKS scheme workloads on FracTLcore Computing Fabric](https://arxiv.org/abs/2510.16025)
*Denis Ovichinnikov,Hemant Kavadia,Satya Keerti Chand Kudupudi,Ilya Rempel,Vineet Chadha,Marty Franz,Paul Master,Craig Gentry,Darlene Kindler,Alberto Reyes,Muthu Annamalai*

Main category: cs.CR

TL;DR: Cornami Mx2处理器通过systolic阵列和内存计算能力加速全同态加密应用，支持TFHE-rs和CKKS两种方案。


<details>
  <summary>Details</summary>
Motivation: 解决全同态加密应用的计算瓶颈问题，通过专用硬件架构提升性能。

Method: 采用基于systolic阵列的核心架构，具备内存计算能力和片上网络，构建编译器后端来评估处理器资源。

Result: 开发出能够同时支持TFHE-rs布尔方案和CKKS方案的全同态加密应用的处理器架构。

Conclusion: Cornami Mx2架构为全同态加密应用提供了高效的硬件加速解决方案。

Abstract: Cornami Mx2 accelerates of Fully Homomorphic Encryption (FHE) applications,
enabled by breakthrough work [1], which are otherwise compute limited. Our
processor architecture is based on the systolic array of cores with in-memory
compute capability and a network on chip (NoC) processor architecture called
the "FracTLcore compute fabric processor" (Mx2). Here, we describe the work to
estimate processor resources to compute workload in CGGI (TFHE-rs) or CKKS
scheme during construction of our compiler backend for this architecture [2].
These processors are available for running applications in both the TFHE-rs
Boolean scheme and CKKS scheme FHE applications.

</details>


### [33] [Nondeterminism-Aware Optimistic Verification for Floating-Point Neural Networks](https://arxiv.org/abs/2510.16028)
*Jianzhu Yao,Hongxu Su,Taobo Liao,Zerui Cheng,Huan Zhang,Xuechao Wang,Pramod Viswanath*

Main category: cs.CR

TL;DR: NAO是一种针对异构硬件上FP神经网络输出的乐观验证协议，通过结合理论误差边界和经验阈值来容忍非确定性，无需可信硬件或确定性内核即可实现可验证的ML计算。


<details>
  <summary>Details</summary>
Motivation: 随着神经网络在用户无法控制的硬件上运行，ML服务缺乏透明度，用户难以验证输出是否忠实反映预期输入，现有方法要么不实用，要么重新引入供应商信任。

Method: NAO结合两种误差模型：IEEE-754最坏情况边界和经验百分位分布，采用Merkle锚定的阈值引导争议游戏，递归划分计算图直至单个算子，通过轻量级理论边界检查或诚实多数投票进行裁决。

Result: 在A100、H100、RTX6000、RTX4090等硬件上，经验阈值比理论边界紧102-103倍，边界感知对抗攻击成功率为0%，Qwen3-8B模型运行时开销仅为0.3%。

Conclusion: NAO在现实世界异构ML计算中实现了可扩展性与可验证性的平衡，为ML服务提供了实用的验证解决方案。

Abstract: Neural networks increasingly run on hardware outside the user's control
(cloud GPUs, inference marketplaces). Yet ML-as-a-Service reveals little about
what actually ran or whether returned outputs faithfully reflect the intended
inputs. Users lack recourse against service downgrades (model swaps,
quantization, graph rewrites, or discrepancies like altered ad embeddings).
Verifying outputs is hard because floating-point(FP) execution on heterogeneous
accelerators is inherently nondeterministic. Existing approaches are either
impractical for real FP neural networks or reintroduce vendor trust. We present
NAO: a Nondeterministic tolerance Aware Optimistic verification protocol that
accepts outputs within principled operator-level acceptance regions rather than
requiring bitwise equality. NAO combines two error models: (i) sound
per-operator IEEE-754 worst-case bounds and (ii) tight empirical percentile
profiles calibrated across hardware. Discrepancies trigger a Merkle-anchored,
threshold-guided dispute game that recursively partitions the computation graph
until one operator remains, where adjudication reduces to a lightweight
theoretical-bound check or a small honest-majority vote against empirical
thresholds. Unchallenged results finalize after a challenge window, without
requiring trusted hardware or deterministic kernels. We implement NAO as a
PyTorch-compatible runtime and a contract layer currently deployed on Ethereum
Holesky testnet. The runtime instruments graphs, computes per-operator bounds,
and runs unmodified vendor kernels in FP32 with negligible overhead (0.3% on
Qwen3-8B). Across CNNs, Transformers and diffusion models on A100, H100,
RTX6000, RTX4090, empirical thresholds are $10^2-10^3$ times tighter than
theoretical bounds, and bound-aware adversarial attacks achieve 0% success. NAO
reconciles scalability with verifiability for real-world heterogeneous ML
compute.

</details>


### [34] [Membership Inference over Diffusion-models-based Synthetic Tabular Data](https://arxiv.org/abs/2510.16037)
*Peini Cheng,Amir Bahmani*

Main category: cs.CR

TL;DR: 该研究调查了基于扩散的合成表格数据生成方法的隐私风险，重点关注其对成员推理攻击的脆弱性。研究发现TabDDPM比TabSyn更容易受到攻击。


<details>
  <summary>Details</summary>
Motivation: 评估扩散模型在合成表格数据生成中的隐私影响，揭示这些方法对成员推理攻击的脆弱性。

Method: 开发基于逐步误差比较方法的查询式成员推理攻击，测试TabDDPM和TabSyn两种模型。

Result: TabDDPM对成员推理攻击表现出较高脆弱性，而TabSyn则对这些攻击具有抵抗力。

Conclusion: 强调了评估扩散模型隐私影响的重要性，鼓励进一步研究合成数据生成的鲁棒隐私保护机制。

Abstract: This study investigates the privacy risks associated with diffusion-based
synthetic tabular data generation methods, focusing on their susceptibility to
Membership Inference Attacks (MIAs). We examine two recent models, TabDDPM and
TabSyn, by developing query-based MIAs based on the step-wise error comparison
method. Our findings reveal that TabDDPM is more vulnerable to these attacks.
TabSyn exhibits resilience against our attack models. Our work underscores the
importance of evaluating the privacy implications of diffusion models and
encourages further research into robust privacy-preserving mechanisms for
synthetic data generation.

</details>


### [35] [A Novel GPT-Based Framework for Anomaly Detection in System Logs](https://arxiv.org/abs/2510.16044)
*Zeng Zhang,Wenjie Yin,Xiaoqi Li*

Main category: cs.CR

TL;DR: 提出基于GPT的智能系统日志异常检测方法，通过结构化输入设计和Focal Loss优化策略，显著提升检测性能


<details>
  <summary>Details</summary>
Motivation: 系统日志异常检测面临大数据量、异常分布不均和传统方法精度不足等挑战，需要更有效的解决方案

Method: 使用Drain解析器将原始日志转换为事件ID序列，采用Focal Loss损失函数解决类别不平衡问题，优化GPT-2模型

Result: 优化后的GPT-2模型在精确率、召回率和F1分数等关键指标上显著优于未优化模型，在特定任务中性能与GPT-3.5 API相当或更优

Conclusion: 基于GPT的智能检测方法结合结构化输入和Focal Loss优化，能够有效提升系统日志异常检测的性能

Abstract: Identification of anomalous events within system logs constitutes a pivotal
element within the frame- work of cybersecurity defense strategies. However,
this process faces numerous challenges, including the management of substantial
data volumes, the distribution of anomalies, and the precision of con-
ventional methods. To address this issue, the present paper puts forward a
proposal for an intelligent detection method for system logs based on Genera-
tive Pre-trained Transformers (GPT). The efficacy of this approach is
attributable to a combination of structured input design and a Focal Loss op-
timization strategy, which collectively result in a substantial enhancement of
the performance of log anomaly detection. The initial approach involves the
conversion of raw logs into event ID sequences through the use of the Drain
parser. Subsequently, the Focal Loss loss function is employed to address the
issue of class imbalance. The experimental re- sults demonstrate that the
optimized GPT-2 model significantly outperforms the unoptimized model in a
range of key metrics, including precision, recall, and F1 score. In specific
tasks, comparable or superior performance has been demonstrated to that of the
GPT-3.5 API.

</details>


### [36] [PrivacyPAD: A Reinforcement Learning Framework for Dynamic Privacy-Aware Delegation](https://arxiv.org/abs/2510.16054)
*Zheng Hui,Yijiang River Dong,Sanhanat Sivapiromrat,Ehsan Shareghi,Nigel Collier*

Main category: cs.CR

TL;DR: 提出了PrivacyPAD框架，使用强化学习动态路由文本块，在保护隐私和任务性能之间实现最优平衡。


<details>
  <summary>Details</summary>
Motivation: 用户在向大语言模型提交查询时面临两难选择：使用强大的专有模型但可能泄露敏感数据，或使用本地小模型保证隐私但性能下降。现有方法使用静态管道，破坏了语言连贯性且无差别删除隐私信息。

Method: 将隐私保护委托问题重新定义为序列决策问题，引入强化学习框架PrivacyPAD，训练智能体动态路由文本块，学习区分可替换的个人身份信息（本地处理）和任务关键信息（远程处理）的策略。

Result: 在隐私-效用边界上达到了新的最先进水平，验证了在敏感环境中部署LLM时需要学习自适应策略的必要性。

Conclusion: PrivacyPAD框架通过强化学习实现了隐私保护和任务性能的最佳平衡，为在敏感环境中部署大语言模型提供了有效的解决方案。

Abstract: When users submit queries to Large Language Models (LLMs), their prompts can
often contain sensitive data, forcing a difficult choice: Send the query to a
powerful proprietary LLM providers to achieving state-of-the-art performance
and risk data exposure, or relying on smaller, local models guarantees data
privacy but often results in a degradation of task performance. Prior
approaches have relied on static pipelines that use LLM rewriting, which
shatters linguistic coherence and indiscriminately removes privacy-sensitive
information, including task-critical content. We reformulate this challenge
(Privacy-Conscious Delegation) as a sequential decision-making problem and
introduce a novel reinforcement learning (RL) framework called PrivacyPAD to
solve it. Our framework trains an agent to dynamically route text chunks,
learning a policy that optimally balances the trade-off between privacy leakage
and task performance. It implicitly distinguishes between replaceable
Personally Identifiable Information (PII) (which it shields locally) and
task-critical PII (which it strategically sends to the remote model for maximal
utility). To validate our approach in complex scenarios, we also introduce a
new medical dataset with high PII density. Our framework achieves a new
state-of-the-art on the privacy-utility frontier, demonstrating the necessity
of learned, adaptive policies for deploying LLMs in sensitive environments.

</details>


### [37] [A Multi-Cloud Framework for Zero-Trust Workload Authentication](https://arxiv.org/abs/2510.16067)
*Saurabh Deochake,Ryan Murphy,Jeremiah Gearheart*

Main category: cs.CR

TL;DR: 提出基于工作负载身份联盟和OpenID Connect的无密钥认证框架，使用加密验证的临时令牌替代静态长期凭证，在多云环境中实现零信任安全。


<details>
  <summary>Details</summary>
Motivation: 静态长期凭证存在安全风险，违反零信任原则，需要消除凭证盗窃威胁并减少攻击面。

Method: 使用工作负载身份联盟和OpenID Connect构建多云框架，通过加密验证的临时令牌实现无密钥认证。

Result: 在企业级Kubernetes环境中验证了该框架，显著减少了攻击面。

Conclusion: 该模型为跨不同云环境管理工作负载身份提供了统一解决方案，为未来实现基于属性的访问控制奠定了基础。

Abstract: Static, long-lived credentials for workload authentication create untenable
security risks that violate Zero-Trust principles. This paper presents a
multi-cloud framework using Workload Identity Federation (WIF) and OpenID
Connect (OIDC) for secretless authentication. Our approach uses
cryptographically-verified, ephemeral tokens, allowing workloads to
authenticate without persistent private keys and mitigating credential theft.
We validate this framework in an enterprise-scale Kubernetes environment, which
significantly reduces the attack surface. The model offers a unified solution
to manage workload identities across disparate clouds, enabling future
implementation of robust, attribute-based access control.

</details>


### [38] [ISO/IEC-Compliant Match-on-Card Face Verification with Short Binary Templates](https://arxiv.org/abs/2510.16078)
*Abdelilah Ganmati,Karim Afdel,Lahcen Koutti*

Main category: cs.CR

TL;DR: 提出了一种实用的卡片匹配人脸验证设计，使用64/128位紧凑模板，通过PCA-ITQ离线生成，在卡上通过恒定时间汉明距离进行比较。


<details>
  <summary>Details</summary>
Motivation: 满足ISO/IEC传输约束和隐私目标，实现固定负载、仅决策的APDU通信，防止分数泄露，并确保宽裕的时间余量。

Method: 使用PCA-ITQ生成紧凑二进制模板，在卡上通过恒定时间汉明距离进行匹配，采用ISO/IEC 7816-4和14443-4命令APDU，具有固定长度负载和仅决策状态字。

Result: 在9.6 kbps最慢接触速率下，总验证时间为43.9 ms（64位）和52.3 ms（128位）；在38.4 kbps下均小于14 ms。在FAR=1%时，两种码长均达到TPR=0.836，128位相比64位降低了EER。

Conclusion: 短二进制模板、固定负载仅决策APDU和恒定时间匹配满足了ISO/IEC传输约束，具有宽裕的时间余量，并符合ISO/IEC 24745隐私目标。局限性包括单数据集评估和设计级时序分析。

Abstract: We present a practical match-on-card design for face verification in which
compact 64/128-bit templates are produced off-card by PCA-ITQ and compared
on-card via constant-time Hamming distance. We specify ISO/IEC 7816-4 and
14443-4 command APDUs with fixed-length payloads and decision-only status words
(no score leakage), together with a minimal per-identity EEPROM map. Using real
binary codes from a CelebA working set (55 identities, 412 images), we (i)
derive operating thresholds from ROC/DET, (ii) replay enroll->verify
transactions at those thresholds, and (iii) bound end-to-end time by pure link
latency plus a small constant on-card budget. Even at the slowest contact rate
(9.6 kbps), total verification time is 43.9 ms (64 b) and 52.3 ms (128 b); at
38.4 kbps both are <14 ms. At FAR = 1%, both code lengths reach TPR = 0.836,
while 128 b lowers EER relative to 64 b. An optional +6 B helper (targeted
symbol-level parity over empirically unstable bits) is latency-negligible.
Overall, short binary templates, fixed-payload decision-only APDUs, and
constant-time matching satisfy ISO/IEC transport constraints with wide timing
margin and align with ISO/IEC 24745 privacy goals. Limitations: single-dataset
evaluation and design-level (pre-hardware) timing; we outline AgeDB/CFP-FP and
on-card microbenchmarks as next steps.

</details>


### [39] [Towards a Blockchain-Based CI/CD Framework to Enhance Security in Cloud Environments](https://arxiv.org/abs/2510.16087)
*Sabbir M Saleh,Nazim Madhavji,John Steinbacher*

Main category: cs.CR

TL;DR: 提出基于区块链的CI/CD管道安全框架，利用分布式账本和防篡改特性增强云平台持续集成部署的安全性。


<details>
  <summary>Details</summary>
Motivation: 针对云平台CI/CD管道面临的安全威胁，特别是近期发生的网络攻击事件，需要提升软件部署过程的安全性。

Method: 集成区块链的分布式账本技术、威胁建模框架和编码标准，使用自动化安全测试工具检测公开漏洞。

Result: 开发了一个能够检测Java Spring框架旧版本、未验证JavaScript库和SQL注入漏洞等安全问题的框架。

Conclusion: 区块链技术能够有效增强CI/CD管道的安全性，通过自动化安全测试和防篡改特性确保软件部署的安全性。

Abstract: Security is becoming a pivotal point in cloud platforms. Several divisions,
such as business organisations, health care, government, etc., have experienced
cyber-attacks on their infrastructures. This research focuses on security
issues within Continuous Integration and Deployment (CI/CD) pipelines in a
cloud platform as a reaction to recent cyber breaches. This research proposes a
blockchain-based solution to enhance CI/CD pipeline security. This research
aims to develop a framework that leverages blockchain's distributed ledger
technology and tamper-resistant features to improve CI/CD pipeline security.
The goal is to emphasise secure software deployment by integrating threat
modelling frameworks and adherence to coding standards. It also aims to employ
tools to automate security testing to detect publicly disclosed vulnerabilities
and flaws, such as an outdated version of Java Spring Framework, a JavaScript
library from an unverified source, or a database library that allows SQL
injection attacks in the deployed software through the framework.

</details>


### [40] [The Hidden Cost of Modeling P(X): Vulnerability to Membership Inference Attacks in Generative Text Classifiers](https://arxiv.org/abs/2510.16122)
*Owais Makroo,Siva Rajesh Kasa,Sumegh Roychowdhury,Karan Gupta,Nikhil Pattisapu,Santhosh Kasa,Sumit Negi*

Main category: cs.CR

TL;DR: 该论文系统比较了生成式和判别式分类器在成员推理攻击中的脆弱性，发现生成式分类器由于显式建模联合概率P(X,Y)而具有更高的隐私风险。


<details>
  <summary>Details</summary>
Motivation: 现有研究对生成式和判别式分类器在成员推理攻击中的系统性比较有限，需要填补这一空白并理解不同分类器设计的隐私风险差异。

Method: 通过理论分析和实证评估，研究涵盖判别式、生成式和伪生成式文本分类器，在九个基准数据集上使用多种MIA策略进行测试。

Result: 完全生成式分类器对成员推理攻击最为脆弱，其典型推理方法显著放大了隐私风险，揭示了分类器设计中固有的效用-隐私权衡。

Conclusion: 在隐私敏感应用中部署生成式分类器需谨慎，未来应研究开发既能保持效用又能减轻成员推理漏洞的隐私保护生成式分类器。

Abstract: Membership Inference Attacks (MIAs) pose a critical privacy threat by
enabling adversaries to determine whether a specific sample was included in a
model's training dataset. Despite extensive research on MIAs, systematic
comparisons between generative and discriminative classifiers remain limited.
This work addresses this gap by first providing theoretical motivation for why
generative classifiers exhibit heightened susceptibility to MIAs, then
validating these insights through comprehensive empirical evaluation. Our study
encompasses discriminative, generative, and pseudo-generative text classifiers
across varying training data volumes, evaluated on nine benchmark datasets.
Employing a diverse array of MIA strategies, we consistently demonstrate that
fully generative classifiers which explicitly model the joint likelihood
$P(X,Y)$ are most vulnerable to membership leakage. Furthermore, we observe
that the canonical inference approach commonly used in generative classifiers
significantly amplifies this privacy risk. These findings reveal a fundamental
utility-privacy trade-off inherent in classifier design, underscoring the
critical need for caution when deploying generative classifiers in
privacy-sensitive applications. Our results motivate future research directions
in developing privacy-preserving generative classifiers that can maintain
utility while mitigating membership inference vulnerabilities.

</details>


### [41] [Prompt injections as a tool for preserving identity in GAI image descriptions](https://arxiv.org/abs/2510.16128)
*Kate Glazko,Jennifer Mankoff*

Main category: cs.CR

TL;DR: 论文提出将提示注入作为一种工具，让间接用户（其内容被生成式AI系统使用但未直接交互的用户）能够自我保护，防止身份特征被错误描述。


<details>
  <summary>Details</summary>
Motivation: 生成式AI的偏见和缺乏代表性风险会影响间接用户，现有缓解方法大多需要自上而下的干预，需要为间接用户提供自主防护手段。

Method: 将提示注入重新定义为内容所有者的抵抗工具，通过案例研究展示如何在图像描述中保留所有者的性别和残疾身份特征。

Result: 演示了提示注入能够有效保护间接用户的身份特征不被生成式AI错误描述。

Conclusion: 提示注入可以作为间接用户自我保护的有效策略，为内容所有者提供抵抗AI偏见的赋能工具。

Abstract: Generative AI risks such as bias and lack of representation impact people who
do not interact directly with GAI systems, but whose content does: indirect
users. Several approaches to mitigating harms to indirect users have been
described, but most require top down or external intervention. An emerging
strategy, prompt injections, provides an empowering alternative: indirect users
can mitigate harm against them, from within their own content. Our approach
proposes prompt injections not as a malicious attack vector, but as a tool for
content/image owner resistance. In this poster, we demonstrate one case study
of prompt injections for empowering an indirect user, by retaining an image
owner's gender and disabled identity when an image is described by GAI.

</details>


### [42] [WebRTC Metadata and IP Leakage in Modern Browsers: A Cross-Platform Measurement Study](https://arxiv.org/abs/2510.16168)
*Ahmed Fouad Kadhim Koysha,Aytug Boyaci,Rafet Akdeniz*

Main category: cs.CR

TL;DR: 对Chrome、Brave、Firefox和Tor浏览器在桌面和移动平台的WebRTC元数据泄漏进行跨平台测量研究，发现Chrome泄漏最严重，Brave使用mDNS标识符，Firefox在桌面端保护强但在Android泄漏，Tor完全防止泄漏。


<details>
  <summary>Details</summary>
Motivation: WebRTC的ICE过程可能无意中暴露内部和公共IP地址作为元数据，存在隐私风险，需要评估当前浏览器的保护状况。

Method: 使用2025年版本的Chrome、Brave、Firefox和Tor浏览器，在桌面和移动平台上，通过半可信Wi-Fi和不可信移动运营商网络进行实验测量。

Result: Chrome在移动端泄漏LAN或CGNAT地址，桌面端泄漏元数据；Brave避免直接IP泄漏但暴露mDNS标识符；Firefox桌面端保护强但Android泄漏内部IP；Tor完全防止所有泄漏形式。

Conclusion: 虽然直接LAN泄漏在减少，但mDNS和CGNAT等新兴向量造成持续隐私风险，需要协议级重新设计和政策行动，提出了分层缓解策略。

Abstract: Web Real-Time Communication (WebRTC) enables real-time peer-to-peer
communication, but its Interactive Connectivity Establishment (ICE) process can
unintentionally expose internal and public IP addresses as metadata. This paper
presents a cross-platform measurement study of WebRTC metadata leakage using
current (2025) builds of Chrome, Brave, Firefox, and Tor on desktop and mobile
platforms. Experiments were conducted across semi-trusted Wi-Fi and untrusted
mobile carrier networks. Results show that Chrome remains the most
leakage-prone, disclosing LAN or Carrier-Grade NAT (CGNAT) addresses on mobile
and metadata on desktop; Brave avoids direct IP leaks but exposes
session-stable mDNS identifiers; Firefox provides strong protection on desktop
but leaks internal IPs on Android; and Tor consistently prevents all forms of
leakage. We introduce a structured threat model for semi-trusted environments
and evaluate the limitations of mDNS obfuscation. Finally, we propose layered
mitigation strategies combining browser defaults, institutional safeguards, and
user controls. Findings demonstrate that while direct LAN leakage is declining,
emerging vectors such as mDNS and CGNAT create persistent privacy risks
requiring protocol-level redesign and policy action.

</details>


### [43] [SentinelNet: Safeguarding Multi-Agent Collaboration Through Credit-Based Dynamic Threat Detection](https://arxiv.org/abs/2510.16219)
*Yang Feng,Xudong Pan*

Main category: cs.CR

TL;DR: SentinelNet是一个去中心化的多智能体系统安全框架，通过信用评分和对比学习来主动检测和缓解恶意行为，在实验中实现了接近完美的恶意智能体检测效果。


<details>
  <summary>Details</summary>
Motivation: 现有防御方法存在反应式设计或集中式架构的缺陷，容易产生单点故障，无法有效应对恶意智能体对LLM驱动的多智能体系统的威胁。

Method: 为每个智能体配备基于信用的检测器，通过对比学习在增强的对抗性辩论轨迹上训练，采用bottom-k消除进行动态邻居排名，并生成对抗性轨迹来解决攻击数据稀缺问题。

Result: 在MAS基准测试中，SentinelNet实现了接近完美的恶意智能体检测（两轮辩论内接近100%），从受损基线中恢复了95%的系统准确率。

Conclusion: SentinelNet建立了保护协作多智能体系统的新范式，在跨领域和攻击模式上展现出强大的泛化能力。

Abstract: Malicious agents pose significant threats to the reliability and
decision-making capabilities of Multi-Agent Systems (MAS) powered by Large
Language Models (LLMs). Existing defenses often fall short due to reactive
designs or centralized architectures which may introduce single points of
failure. To address these challenges, we propose SentinelNet, the first
decentralized framework for proactively detecting and mitigating malicious
behaviors in multi-agent collaboration. SentinelNet equips each agent with a
credit-based detector trained via contrastive learning on augmented adversarial
debate trajectories, enabling autonomous evaluation of message credibility and
dynamic neighbor ranking via bottom-k elimination to suppress malicious
communications. To overcome the scarcity of attack data, it generates
adversarial trajectories simulating diverse threats, ensuring robust training.
Experiments on MAS benchmarks show SentinelNet achieves near-perfect detection
of malicious agents, close to 100% within two debate rounds, and recovers 95%
of system accuracy from compromised baselines. By exhibiting strong
generalizability across domains and attack patterns, SentinelNet establishes a
novel paradigm for safeguarding collaborative MAS.

</details>


### [44] [C/N0 Analysis-Based GPS Spoofing Detection with Variable Antenna Orientations](https://arxiv.org/abs/2510.16229)
*Vienna Li,Justin Villa,Dan Diessner,Jayson Clifford,Laxima Niure Kandel*

Main category: cs.CR

TL;DR: 提出了一种基于卫星载噪比(C/N₀)变化的GPS欺骗检测方法，通过分析天线在不同方向时的信号变化来识别欺骗信号。


<details>
  <summary>Details</summary>
Motivation: GPS欺骗对航空安全构成日益严重的威胁，需要开发有效的检测方法来保护飞机导航系统。

Method: 使用u-blox EVK-M8U接收器和GPSG-1000卫星模拟器，在平坦、右倾和左倾三种天线方向下收集真实天空信号和欺骗信号的C/N₀数据。

Result: 真实信号下C/N₀随方向自然波动，而欺骗信号在平坦方向时C/N₀最高，倾斜方向时因与欺骗源不对齐而降低。

Conclusion: 简单的机动动作如短暂倾斜引起的C/N₀变化可以为通用航空和无人机系统提供GPS欺骗的早期预警。

Abstract: GPS spoofing poses a growing threat to aviation by falsifying satellite
signals and misleading aircraft navigation systems. This paper demonstrates a
proof-of-concept spoofing detection strategy based on analyzing satellite
Carrier-to-Noise Density Ratio (C/N$_0$) variation during controlled static
antenna orientations. Using a u-blox EVK-M8U receiver and a GPSG-1000 satellite
simulator, C/N$_0$ data is collected under three antenna orientations flat,
banked right, and banked left) in both real-sky (non-spoofed) and spoofed
environments. Our findings reveal that under non-spoofed signals, C/N$_0$
values fluctuate naturally with orientation, reflecting true geometric
dependencies. However, spoofed signals demonstrate a distinct pattern: the flat
orientation, which directly faces the spoofing antenna, consistently yielded
the highest C/N$_0$ values, while both banked orientations showed reduced
C/N$_0$ due to misalignment with the spoofing source. These findings suggest
that simple maneuvers such as brief banking to induce C/N$_0$ variations can
provide early cues of GPS spoofing for general aviation and UAV systems.

</details>


### [45] [LibIHT: A Hardware-Based Approach to Efficient and Evasion-Resistant Dynamic Binary Analysis](https://arxiv.org/abs/2510.16251)
*Changyu Zhao,Yohan Beugin,Jean-Charles Noirot Ferrand,Quinn Burke,Guancheng Li,Patrick McDaniel*

Main category: cs.CR

TL;DR: LibIHT是一个基于硬件辅助追踪的框架，利用CPU分支追踪功能高效捕获程序控制流，相比软件插桩显著降低性能开销，同时有效对抗规避性恶意软件。


<details>
  <summary>Details</summary>
Motivation: 软件插桩在恶意软件检测、调试和性能分析中面临高开销和反分析技术规避的问题，需要更高效且难以检测的追踪方法。

Method: 利用Intel Last Branch Record和Branch Trace Store等CPU分支追踪功能，在内核中收集硬件生成的分支执行数据，重建控制流图(CFG)，实现为OS内核模块和用户空间库。

Result: 相比Intel Pin减少150倍以上的运行时开销（7倍vs 1,053倍减速），在CFG重建中捕获超过99%的执行基本块和边，实现高保真度。

Conclusion: 硬件辅助追踪在性能和低可检测性至关重要的应用中是可接受的权衡，能够以最小干扰显著更快地捕获控制流信息并降低检测风险。

Abstract: Dynamic program analysis is invaluable for malware detection, debugging, and
performance profiling. However, software-based instrumentation incurs high
overhead and can be evaded by anti-analysis techniques. In this paper, we
propose LibIHT, a hardware-assisted tracing framework that leverages on-CPU
branch tracing features (Intel Last Branch Record and Branch Trace Store) to
efficiently capture program control-flow with minimal performance impact. Our
approach reconstructs control-flow graphs (CFGs) by collecting hardware
generated branch execution data in the kernel, preserving program behavior
against evasive malware. We implement LibIHT as an OS kernel module and
user-space library, and evaluate it on both benign benchmark programs and
adversarial anti-instrumentation samples. Our results indicate that LibIHT
reduces runtime overhead by over 150x compared to Intel Pin (7x vs 1,053x
slowdowns), while achieving high fidelity in CFG reconstruction (capturing over
99% of execution basic blocks and edges). Although this hardware-assisted
approach sacrifices the richer semantic detail available from full software
instrumentation by capturing only branch addresses, this trade-off is
acceptable for many applications where performance and low detectability are
paramount. Our findings show that hardware-based tracing captures control flow
information significantly faster, reduces detection risk and performs dynamic
analysis with minimal interference.

</details>


### [46] [Detecting Adversarial Fine-tuning with Auditing Agents](https://arxiv.org/abs/2510.16255)
*Sarah Egler,John Schulman,Nicholas Carlini*

Main category: cs.CR

TL;DR: 提出了一个针对LLM微调API的审计代理，能够检测有害的微调行为，在1%误报率下达到56.2%的检测率，特别能识别规避安全评估的隐蔽攻击。


<details>
  <summary>Details</summary>
Motivation: LLM提供商提供微调API，但攻击者可能利用这些API绕过安全防护，使用隐含有害的数据集进行攻击，需要开发有效的检测机制。

Method: 构建微调审计代理，提供对微调数据集、微调前后模型的访问权限，让代理为微调任务分配风险评分，在8种强攻击和5个良性模型上进行评估。

Result: 在1400多次独立审计中，最佳配置下在1%误报率时达到56.2%的检测率，特别能检测规避安全评估的隐蔽密码攻击。

Conclusion: 虽然良性微调中的无意识安全退化仍是挑战，但为这一领域建立了基准配置，为后续研究提供了基础。

Abstract: Large Language Model (LLM) providers expose fine-tuning APIs that let end
users fine-tune their frontier LLMs. Unfortunately, it has been shown that an
adversary with fine-tuning access to an LLM can bypass safeguards. Particularly
concerning, such attacks may avoid detection with datasets that are only
implicitly harmful. Our work studies robust detection mechanisms for
adversarial use of fine-tuning APIs. We introduce the concept of a fine-tuning
auditing agent and show it can detect harmful fine-tuning prior to model
deployment. We provide our auditing agent with access to the fine-tuning
dataset, as well as the fine-tuned and pre-fine-tuned models, and request the
agent assigns a risk score for the fine-tuning job. We evaluate our detection
approach on a diverse set of eight strong fine-tuning attacks from the
literature, along with five benign fine-tuned models, totaling over 1400
independent audits. These attacks are undetectable with basic content
moderation on the dataset, highlighting the challenge of the task. With the
best set of affordances, our auditing agent achieves a 56.2% detection rate of
adversarial fine-tuning at a 1% false positive rate. Most promising, the
auditor is able to detect covert cipher attacks that evade safety evaluations
and content moderation of the dataset. While benign fine-tuning with
unintentional subtle safety degradation remains a challenge, we establish a
baseline configuration for further work in this area. We release our auditing
agent at https://github.com/safety-research/finetuning-auditor.

</details>


### [47] [Efficient and Privacy-Preserving Binary Dot Product via Multi-Party Computation](https://arxiv.org/abs/2510.16331)
*Fatemeh Jafarian Dehkordi,Elahe Vedadi,Alireza Feizbakhsh,Yasaman Keshtkarjahromi,Hulya Seferoglu*

Main category: cs.CR

TL;DR: 提出了一个新颖的二进制多方计算框架BiMPC，专门针对树基垂直联邦学习中的按位操作，通过DoMA方法和三方向不经意传输协议实现高效的二进制向量点积计算和隐私保护。


<details>
  <summary>Details</summary>
Motivation: 分布式机器学习中数据隐私保护与协同计算之间的平衡是关键挑战。现有方法如Shamir秘密共享和多方计算对二进制数据的按位操作优化不足，特别是在每个参与者持有二进制向量不同部分的情况下。

Method: 提出BiMPC框架，核心是DoMA方法，使用常规和模加法进行二进制点积计算。采用高域随机掩码进行线性计算，使用三方向不经意传输协议处理非线性二进制操作。

Result: BiMPC框架在分布式环境中展现出高效性和可扩展性，能够保护每个比特的隐私。

Conclusion: BiMPC框架有效解决了现有方法在二进制按位操作方面的局限性，为树基垂直联邦学习提供了优化的隐私保护计算方案。

Abstract: Striking a balance between protecting data privacy and enabling collaborative
computation is a critical challenge for distributed machine learning. While
privacy-preserving techniques for federated learning have been extensively
developed, methods for scenarios involving bitwise operations, such as
tree-based vertical federated learning (VFL), are still underexplored.
Traditional mechanisms, including Shamir's secret sharing and multi-party
computation (MPC), are not optimized for bitwise operations over binary data,
particularly in settings where each participant holds a different part of the
binary vector. This paper addresses the limitations of existing methods by
proposing a novel binary multi-party computation (BiMPC) framework. The BiMPC
mechanism facilitates privacy-preserving bitwise operations, with a particular
focus on dot product computations of binary vectors, ensuring the privacy of
each individual bit. The core of BiMPC is a novel approach called Dot Product
via Modular Addition (DoMA), which uses regular and modular additions for
efficient binary dot product calculation. To ensure privacy, BiMPC uses random
masking in a higher field for linear computations and a three-party oblivious
transfer (triot) protocol for non-linear binary operations. The privacy
guarantees of the BiMPC framework are rigorously analyzed, demonstrating its
efficiency and scalability in distributed settings.

</details>


### [48] [EditMark: Watermarking Large Language Models based on Model Editing](https://arxiv.org/abs/2510.16367)
*Shuai Li,Kejiang Chen,Jun Jiang,Jie Zhang,Qiyi Yao,Kai Zeng,Weiming Zhang,Nenghai Yu*

Main category: cs.CR

TL;DR: EditMark是一种基于模型编辑的LLM水印方法，无需训练即可嵌入隐蔽且无损性能的水印，通过多轮稳定编辑策略在20秒内嵌入32位水印，提取成功率100%。


<details>
  <summary>Details</summary>
Motivation: 现有LLM水印方法需要训练带水印的数据集，训练成本高且影响模型性能，水印文本不自然。需要一种无需训练、隐蔽且不影响性能的水印方案。

Method: 利用模型编辑技术，为多答案问题分配唯一水印，通过自适应多轮稳定编辑策略和噪声矩阵注入来更新LLM权重，使其生成对应问答对。

Result: 在20秒内成功嵌入32位水印（传统微调需6875秒），水印提取成功率100%，具有良好的保真度、隐蔽性和抗攻击鲁棒性。

Conclusion: EditMark是首个基于模型编辑的LLM水印方法，实现了训练免费、隐蔽且无损性能的水印嵌入，为LLM版权保护提供了高效解决方案。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities, but
their training requires extensive data and computational resources, rendering
them valuable digital assets. Therefore, it is essential to watermark LLMs to
protect their copyright and trace unauthorized use or resale. Existing methods
for watermarking LLMs primarily rely on training LLMs with a watermarked
dataset, which entails burdensome training costs and negatively impacts the
LLM's performance. In addition, their watermarked texts are not logical or
natural, thereby reducing the stealthiness of the watermark. To address these
issues, we propose EditMark, the first watermarking method that leverages model
editing to embed a training-free, stealthy, and performance-lossless watermark
for LLMs. We observe that some questions have multiple correct answers.
Therefore, we assign each answer a unique watermark and update the weights of
LLMs to generate corresponding questions and answers through the model editing
technique. In addition, we refine the model editing technique to align with the
requirements of watermark embedding. Specifically, we introduce an adaptive
multi-round stable editing strategy, coupled with the injection of a noise
matrix, to improve both the effectiveness and robustness of the watermark
embedding. Extensive experiments indicate that EditMark can embed 32-bit
watermarks into LLMs within 20 seconds (Fine-tuning: 6875 seconds) with a
watermark extraction success rate of 100%, which demonstrates its effectiveness
and efficiency. External experiments further demonstrate that EditMark has
fidelity, stealthiness, and a certain degree of robustness against common
attacks.

</details>


### [49] [Heimdallr: Fingerprinting SD-WAN Control-Plane Architecture via Encrypted Control Traffic](https://arxiv.org/abs/2510.16461)
*Minjae Seo,Jaehan Kim,Eduard Marin,Myoungsung You,Taejune Park,Seungsoo Lee,Seungwon Shin,Jinwoo Kim*

Main category: cs.CR

TL;DR: 提出了一个名为Heimdallr的SD-WAN指纹识别系统，能够通过深度学习分析加密控制流量的时间序列模式和流向关系，自动识别集群管理协议并推断控制平面拓扑。


<details>
  <summary>Details</summary>
Motivation: SD-WAN的分布式控制平面通过集群管理协议实现一致性，但加密的控制流量仍会暴露独特的时间序列模式和方向关系，可能泄露控制平面拓扑和协议依赖等机密信息，被用于严重攻击。

Method: 利用深度学习分析SD-WAN集群管理协议的周期性和操作模式，以及从收集的控制流量中提取的流向上下文，自动从混杂的控制流量数据集中分类集群管理协议。

Result: 在由地理上分散的三个校园网络和一个企业网络组成的真实SD-WAN环境中评估，Heimdallr能够以≥93%的准确率分类SD-WAN控制流量，以≥80%的宏F-1分数识别单个协议，并以≥70%的相似度推断控制平面拓扑。

Conclusion: Heimdallr系统能够有效识别SD-WAN控制流量中的集群管理协议，并推断控制平面拓扑，揭示了加密控制流量中仍存在可被利用的安全漏洞。

Abstract: Software-defined wide area network (SD-WAN) has emerged as a new paradigm for
steering a large-scale network flexibly by adopting distributed
software-defined network (SDN) controllers. The key to building a logically
centralized but physically distributed control-plane is running diverse cluster
management protocols to achieve consistency through an exchange of control
traffic. Meanwhile, we observe that the control traffic exposes unique
time-series patterns and directional relationships due to the operational
structure even though the traffic is encrypted, and this pattern can disclose
confidential information such as control-plane topology and protocol
dependencies, which can be exploited for severe attacks. With this insight, we
propose a new SD-WAN fingerprinting system, called Heimdallr. It analyzes
periodical and operational patterns of SD-WAN cluster management protocols and
the context of flow directions from the collected control traffic utilizing a
deep learning-based approach, so that it can classify the cluster management
protocols automatically from miscellaneous control traffic datasets. Our
evaluation, which is performed in a realistic SD-WAN environment consisting of
geographically distant three campus networks and one enterprise network shows
that Heimdallr can classify SD-WAN control traffic with $\geq$ 93%, identify
individual protocols with $\geq$ 80% macro F-1 scores, and finally can infer
control-plane topology with $\geq$ 70% similarity.

</details>


### [50] [$ρ$Hammer: Reviving RowHammer Attacks on New Architectures via Prefetching](https://arxiv.org/abs/2510.16544)
*Weijie Chen,Shan Tang,Yulin Tang,Xiapu Luo,Yinqian Zhang,Weizhong Qiang*

Main category: cs.CR

TL;DR: ρHammer是一个新的Rowhammer攻击框架，通过地址映射逆向工程、预取式锤击和反推测执行技术，成功突破了最新Intel架构的防御机制。


<details>
  <summary>Details</summary>
Motivation: 传统基于负载的Rowhammer攻击在最新Intel架构上变得无效，需要开发新的攻击方法来克服地址映射复杂化、激活率瓶颈和推测执行干扰等挑战。

Method: 1) 使用选择性配对测量和结构化推导的DRAM地址映射逆向工程方法；2) 基于x86预取指令的异步预取式锤击范式，结合多bank并行化；3) 控制流混淆和优化的NOP伪屏障反推测执行技术。

Result: 在四个最新Intel架构上，ρHammer在2小时攻击模式模糊测试中诱导超过20万额外比特翻转，在Comet和Rocket Lake上的翻转率比基线高112倍，并首次在Raptor Lake上实现稳定攻击。

Conclusion: ρHammer框架成功克服了现代内存控制器的防御机制，证明了Rowhammer威胁在最新架构上仍然存在，需要更强的防御措施。

Abstract: Rowhammer is a critical vulnerability in dynamic random access memory (DRAM)
that continues to pose a significant threat to various systems. However, we
find that conventional load-based attacks are becoming highly ineffective on
the most recent architectures such as Intel Alder and Raptor Lake. In this
paper, we present $\rho$Hammer, a new Rowhammer framework that systematically
overcomes three core challenges impeding attacks on these new architectures.
First, we design an efficient and generic DRAM address mapping
reverse-engineering method that uses selective pairwise measurements and
structured deduction, enabling recovery of complex mappings within seconds on
the latest memory controllers. Second, to break through the activation rate
bottleneck of load-based hammering, we introduce a novel prefetch-based
hammering paradigm that leverages the asynchronous nature of x86 prefetch
instructions and is further enhanced by multi-bank parallelism to maximize
throughput. Third, recognizing that speculative execution causes more severe
disorder issues for prefetching, which cannot be simply mitigated by memory
barriers, we develop a counter-speculation hammering technique using
control-flow obfuscation and optimized NOP-based pseudo-barriers to maintain
prefetch order with minimal overhead. Evaluations across four latest Intel
architectures demonstrate $\rho$Hammer's breakthrough effectiveness: it induces
up to 200K+ additional bit flips within 2-hour attack pattern fuzzing processes
and has a 112x higher flip rate than the load-based hammering baselines on
Comet and Rocket Lake. Also, we are the first to revive Rowhammer attacks on
the latest Raptor Lake architecture, where baselines completely fail, achieving
stable flip rates of 2,291/min and fast end-to-end exploitation.

</details>


### [51] [Toward Understanding Security Issues in the Model Context Protocol Ecosystem](https://arxiv.org/abs/2510.16558)
*Xiaofan Li,Xing Gao*

Main category: cs.CR

TL;DR: 本文首次对MCP生态系统进行安全分析，揭示了由于缺乏输出验证机制和服务器审查流程，恶意服务器可操纵模型行为并引发敏感数据泄露等安全威胁。


<details>
  <summary>Details</summary>
Motivation: MCP生态系统快速发展但缺乏系统性安全研究，需要分析其架构和安全风险。

Method: 将MCP生态系统分解为主机、注册表和服务器三个核心组件，分析它们之间的交互和信任关系，收集并分析来自6个公共注册表的67,057个服务器数据集。

Result: 定性分析显示主机缺乏LLM生成输出的验证机制，恶意服务器可操纵模型行为；定量分析表明大量服务器可被攻击者劫持。

Conclusion: 提出针对MCP主机、注册表和用户的实用防御策略，并向受影响方负责任地披露了发现。

Abstract: The Model Context Protocol (MCP) is an emerging open standard that enables
AI-powered applications to interact with external tools through structured
metadata. A rapidly growing ecosystem has formed around MCP, including a wide
range of MCP hosts (i.e., Cursor, Windsurf, Claude Desktop, and Cline), MCP
registries (i.e., mcp.so, MCP Market, MCP Store, Pulse MCP, Smithery, and npm),
and thousands of community-contributed MCP servers. Although the MCP ecosystem
is gaining traction, there has been little systematic study of its architecture
and associated security risks. In this paper, we present the first
comprehensive security analysis of the MCP ecosystem. We decompose MCP
ecosystem into three core components: hosts, registries, and servers, and study
the interactions and trust relationships among them. Users search for servers
on registries and configure them in the host, which translates LLM-generated
output into external tool invocations provided by the servers and executes
them. Our qualitative analysis reveals that hosts lack output verification
mechanisms for LLM-generated outputs, enabling malicious servers to manipulate
model behavior and induce a variety of security threats, including but not
limited to sensitive data exfiltration. We uncover a wide range of
vulnerabilities that enable attackers to hijack servers, due to the lack of a
vetted server submission process in registries. To support our analysis, we
collect and analyze a dataset of 67,057 servers from six public registries. Our
quantitative analysis demonstrates that a substantial number of servers can be
hijacked by attackers. Finally, we propose practical defense strategies for MCP
hosts, registries, and users. We responsibly disclosed our findings to affected
hosts and registries.

</details>


### [52] [Patronus: Safeguarding Text-to-Image Models against White-Box Adversaries](https://arxiv.org/abs/2510.16581)
*Xinfeng Li,Shengyuan Pang,Jialin Wu,Jiangyi Deng,Huanlong Zhong,Yanjiao Chen,Jie Zhang,Wenyuan Xu*

Main category: cs.CR

TL;DR: Patronus是一个防御框架，通过内部调节器和非微调学习机制保护文本到图像模型免受白盒攻击者的恶意微调攻击。


<details>
  <summary>Details</summary>
Motivation: 现有安全措施在面对知道模型参数并能调整的白盒攻击者时失效，需要新的防御机制来保护文本到图像模型。

Method: 设计内部调节器将不安全输入特征解码为零向量，同时保持良性输入的解码性能；采用非微调学习机制增强模型对齐。

Result: 实验验证了在安全内容生成上的性能完整性，以及拒绝不安全内容生成的有效性，确认了对各种白盒微调攻击的韧性。

Conclusion: Patronus框架为文本到图像模型提供了全面的白盒攻击防护，确保模型在恶意微调下不被破坏。

Abstract: Text-to-image (T2I) models, though exhibiting remarkable creativity in image
generation, can be exploited to produce unsafe images. Existing safety
measures, e.g., content moderation or model alignment, fail in the presence of
white-box adversaries who know and can adjust model parameters, e.g., by
fine-tuning. This paper presents a novel defensive framework, named Patronus,
which equips T2I models with holistic protection to defend against white-box
adversaries. Specifically, we design an internal moderator that decodes unsafe
input features into zero vectors while ensuring the decoding performance of
benign input features. Furthermore, we strengthen the model alignment with a
carefully designed non-fine-tunable learning mechanism, ensuring the T2I model
will not be compromised by malicious fine-tuning. We conduct extensive
experiments to validate the intactness of the performance on safe content
generation and the effectiveness of rejecting unsafe content generation.
Results also confirm the resilience of Patronus against various fine-tuning
attacks by white-box adversaries.

</details>


### [53] [DESTinE Block: Private Blockchain Based Data Storage Framework for Power System](https://arxiv.org/abs/2510.16593)
*Khandaker Akramul Haque,Katherine R. Davis*

Main category: cs.CR

TL;DR: DESTinE Block是一个基于区块链的数据存储框架，专为电力系统设计，优化用于资源受限环境。它采用IPFS存储大文件，在自定义区块链上记录安全可追溯的元数据，使用PoA共识机制，可在树莓派等设备上运行。


<details>
  <summary>Details</summary>
Motivation: 为电力系统提供安全、去中心化的数据存储解决方案，特别针对资源受限的电网边缘设备，确保数据真实性和完整性。

Method: 采用双区块链抽象架构，IPFS存储大文件，DESTinE区块链记录元数据（CID、上传者身份、管理员验证、时间戳）。使用PoA共识机制，需要管理员和上传者共同创建区块。

Result: 在x86和ARM64设备（包括树莓派）上成功测试，与Multichain框架相比表现良好，提供防篡改数据保留功能，硬件要求低。

Conclusion: DESTinE Block为分布式电力系统基础设施提供了一个有前景的防篡改数据存储解决方案，特别适合资源受限环境。

Abstract: This paper presents DESTinE Block, a blockchain-based data storage framework
designed for power systems and optimized for resource-constrained environments,
including grid-edge devices such as single-board computers. The proposed
architecture leverages the InterPlanetary File System (IPFS) for storing large
files while maintaining secure and traceable metadata on a custom blockchain
named DESTinE Block. The metadata, comprising the IPFS Content Identifier
(CID), uploader identity, administrator verification, and timestamp; is
immutably recorded on-chain to ensure authenticity and integrity. DESTinE Block
adopts a dual-blockchain abstraction, where the blockchain remains unaware of
the IPFS storage layer to enhance security and limit the exposure of sensitive
file data. The consensus mechanism is based on Proof of Authority (PoA), where
both an administrator and an uploader with distinct cryptographic key pairs are
required to create a block collaboratively. Each block contains verified
signatures of both parties and is designed to be computationally efficient,
enabling deployment on devices like the Raspberry Pi 5. The framework was
tested on both an x86-based device and an ARM64-based Raspberry Pi,
demonstrating its potential for secure, decentralized logging and measurement
storage in smart grid applications. Moreover, DESTinE Block is compared with a
similar framework based on Multichain. The results indicate that DESTinE Block
provides a promising solution for tamper-evident data retention in distributed
power system infrastructure while maintaining minimal hardware requirements.

</details>


### [54] [Structuring Security: A Survey of Cybersecurity Ontologies, Semantic Log Processing, and LLMs Application](https://arxiv.org/abs/2510.16610)
*Bruno Lourenço,Pedro Adão,João F. Ferreira,Mario Monteiro Marques,Cátia Vaz*

Main category: cs.CR

TL;DR: 本调查探讨了本体论、语义日志处理和大语言模型如何增强网络安全。本体论结构化领域知识，实现互操作性、数据集成和高级威胁分析。通过自动构建知识图谱来组织安全日志数据，并结合LLMs提供上下文理解和洞察提取。


<details>
  <summary>Details</summary>
Motivation: 安全日志通常是非结构化和复杂的，难以有效分析。需要自动化方法来组织和推理安全数据，以应对日益复杂的网络威胁。

Method: 使用本体论结构化领域知识，自动构建知识图谱来组织安全日志数据，并利用大语言模型提供上下文理解和从非结构化内容中提取洞察。

Result: 提出了一种结合本体论、知识图谱和LLMs的综合方法，能够有效处理复杂的安全日志数据，实现智能化的网络防御。

Conclusion: 本体论驱动的网络防御方法结合语义日志处理和LLMs，为网络安全提供了强大的分析框架，符合欧盟NIS 2和网络安全分类法的要求，展示了智能网络防御的挑战和机遇。

Abstract: This survey investigates how ontologies, semantic log processing, and Large
Language Models (LLMs) enhance cybersecurity. Ontologies structure domain
knowledge, enabling interoperability, data integration, and advanced threat
analysis. Security logs, though critical, are often unstructured and complex.
To address this, automated construction of Knowledge Graphs (KGs) from raw logs
is emerging as a key strategy for organizing and reasoning over security data.
LLMs enrich this process by providing contextual understanding and extracting
insights from unstructured content. This work aligns with European Union (EU)
efforts such as NIS 2 and the Cybersecurity Taxonomy, highlighting challenges
and opportunities in intelligent ontology-driven cyber defense.

</details>


### [55] [A Versatile Framework for Designing Group-Sparse Adversarial Attacks](https://arxiv.org/abs/2510.16637)
*Alireza Heshmati,Saman Soleimani Roudi,Sajjad Amini,Shahrokh Ghaemmaghami,Farokh Marvasti*

Main category: cs.CR

TL;DR: 提出了ATOS（通过重叠稀疏性攻击）框架，生成结构化稀疏对抗扰动，在保持高攻击成功率的同时提高可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有对抗攻击通常忽略扰动稀疏性，限制了建模结构变化和解释深度神经网络处理有意义输入模式的能力。

Method: 引入重叠平滑L0（OSL0）函数，通过分组通道和相邻像素生成元素级、像素级和分组级的结构化稀疏扰动，并使用对数指数绝对值之和逼近L无穷梯度来控制扰动幅度。

Result: 在CIFAR-10和ImageNet上实现100%攻击成功率，产生的扰动比现有方法更稀疏且结构更连贯。分组攻击从网络角度突出关键区域。

Conclusion: ATOS框架能够生成结构化稀疏对抗扰动，不仅提高攻击效率，还通过用目标类的鲁棒特征替换类别定义区域来提供反事实解释，有助于识别鲁棒与非鲁棒特征。

Abstract: Existing adversarial attacks often neglect perturbation sparsity, limiting
their ability to model structural changes and to explain how deep neural
networks (DNNs) process meaningful input patterns. We propose ATOS (Attack
Through Overlapping Sparsity), a differentiable optimization framework that
generates structured, sparse adversarial perturbations in element-wise,
pixel-wise, and group-wise forms. For white-box attacks on image classifiers,
we introduce the Overlapping Smoothed L0 (OSL0) function, which promotes
convergence to a stationary point while encouraging sparse, structured
perturbations. By grouping channels and adjacent pixels, ATOS improves
interpretability and helps identify robust versus non-robust features. We
approximate the L-infinity gradient using the logarithm of the sum of
exponential absolute values to tightly control perturbation magnitude. On
CIFAR-10 and ImageNet, ATOS achieves a 100% attack success rate while producing
significantly sparser and more structurally coherent perturbations than prior
methods. The structured group-wise attack highlights critical regions from the
network's perspective, providing counterfactual explanations by replacing
class-defining regions with robust features from the target class.

</details>


### [56] [Rotation, Scale, and Translation Resilient Black-box Fingerprinting for Intellectual Property Protection of EaaS Models](https://arxiv.org/abs/2510.16706)
*Hongjie Zhang,Zhiqi Zhao,Hanzhou Wu,Zhihua Xia,Athanasios V. Vasilakos*

Main category: cs.CR

TL;DR: 提出了一种用于嵌入即服务（EaaS）模型的指纹框架，通过分析嵌入空间的拓扑结构来验证模型所有权，而非依赖传统水印技术。该方法将嵌入建模为点云，进行空间对齐和相似性度量，能够抵抗旋转、缩放和平移（RST）攻击。


<details>
  <summary>Details</summary>
Motivation: 现有EaaS模型水印方法通过修改训练样本或网络参数注入后门触发器，但会产生可检测的语义模式且容易受到几何变换攻击。需要一种更稳健的所有权验证方法。

Method: 将受害模型和可疑模型的嵌入建模为点云，通过几何分析嵌入空间的拓扑结构进行空间对齐和相似性测量，而非依赖修改的训练样本或触发器。

Result: 在视觉和文本嵌入任务上的实验验证了该方法的优越性和适用性，能够有效抵抗RST攻击。

Conclusion: 该研究揭示了EaaS模型的固有特性，为黑盒场景下的模型所有权验证提供了有前景的解决方案。

Abstract: Feature embedding has become a cornerstone technology for processing
high-dimensional and complex data, which results in that Embedding as a Service
(EaaS) models have been widely deployed in the cloud. To protect the
intellectual property of EaaS models, existing methods apply digital
watermarking to inject specific backdoor triggers into EaaS models by modifying
training samples or network parameters. However, these methods inevitably
produce detectable patterns through semantic analysis and exhibit
susceptibility to geometric transformations including rotation, scaling, and
translation (RST). To address this problem, we propose a fingerprinting
framework for EaaS models, rather than merely refining existing watermarking
techniques. Different from watermarking techniques, the proposed method
establishes EaaS model ownership through geometric analysis of embedding
space's topological structure, rather than relying on the modified training
samples or triggers. The key innovation lies in modeling the victim and
suspicious embeddings as point clouds, allowing us to perform robust spatial
alignment and similarity measurement, which inherently resists RST attacks.
Experimental results evaluated on visual and textual embedding tasks verify the
superiority and applicability. This research reveals inherent characteristics
of EaaS models and provides a promising solution for ownership verification of
EaaS models under the black-box scenario.

</details>


### [57] [DistilLock: Safeguarding LLMs from Unauthorized Knowledge Distillation on the Edge](https://arxiv.org/abs/2510.16716)
*Asmita Mohanty,Gezheng Kang,Lei Gao,Murali Annavaram*

Main category: cs.CR

TL;DR: DistilLock是一个基于可信执行环境(TEE)的隐私保护知识蒸馏框架，用于在边缘设备上安全地微调大语言模型，同时保护数据隐私和模型知识产权。


<details>
  <summary>Details</summary>
Motivation: 解决云端集中式微调带来的数据隐私问题，以及边缘设备本地微调导致的知识产权泄露风险。

Method: 在数据所有者设备的TEE enclave中运行专有基础模型作为安全黑盒教师，采用模型混淆机制将混淆权重卸载到不可信加速器进行高效知识蒸馏。

Result: DistilLock能够防止未经授权的知识蒸馏过程和模型窃取攻击，同时保持高计算效率。

Conclusion: DistilLock为基于边缘的LLM个性化提供了安全实用的解决方案，平衡了隐私保护与计算效率的需求。

Abstract: Large Language Models (LLMs) have demonstrated strong performance across
diverse tasks, but fine-tuning them typically relies on cloud-based,
centralized infrastructures. This requires data owners to upload potentially
sensitive data to external servers, raising serious privacy concerns. An
alternative approach is to fine-tune LLMs directly on edge devices using local
data; however, this introduces a new challenge: the model owner must transfer
proprietary models to the edge, which risks intellectual property (IP) leakage.
To address this dilemma, we propose DistilLock, a TEE-assisted fine-tuning
framework that enables privacy-preserving knowledge distillation on the edge.
In DistilLock, a proprietary foundation model is executed within a trusted
execution environment (TEE) enclave on the data owner's device, acting as a
secure black-box teacher. This setup preserves both data privacy and model IP
by preventing direct access to model internals. Furthermore, DistilLock employs
a model obfuscation mechanism to offload obfuscated weights to untrusted
accelerators for efficient knowledge distillation without compromising
security. We demonstrate that DistilLock prevents unauthorized knowledge
distillation processes and model-stealing attacks while maintaining high
computational efficiency, but offering a secure and practical solution for
edge-based LLM personalization.

</details>


### [58] [Cryptanalysis of a Privacy-Preserving Ride-Hailing Service from NSS 2022](https://arxiv.org/abs/2510.16744)
*Srinivas Vivek*

Main category: cs.CR

TL;DR: 对Xie等人在NSS 2022提出的隐私保护网约车服务协议进行被动攻击，能够完全恢复乘客和司机的精确位置信息。


<details>
  <summary>Details</summary>
Motivation: 验证Xie等人提出的隐私保护网约车服务协议在实际应用中的安全性，发现其存在的隐私泄露漏洞。

Method: 采用被动攻击方式，分析协议中的位置隐私保护机制，利用协议设计缺陷来恢复位置信息。

Result: 成功实现了对乘客和司机位置的完全恢复，攻击效率高且与安全参数无关。

Conclusion: Xie等人的PP-RHS协议存在严重安全漏洞，无法有效保护用户位置隐私，需要重新设计安全机制。

Abstract: Ride-Hailing Services (RHS) match a ride request initiated by a rider with a
suitable driver responding to the ride request. A Privacy-Preserving RHS
(PP-RHS) aims to facilitate ride matching while ensuring the privacy of riders'
and drivers' location data w.r.t. the Service Provider (SP). At NSS 2022, Xie
et al. proposed a PP-RHS. In this work, we demonstrate a passive attack on
their PP-RHS protocol. Our attack allows the SP to completely recover the
locations of the rider as well as that of the responding drivers in every ride
request. Further, our attack is very efficient as it is independent of the
security parameter.

</details>


### [59] [Black-box Optimization of LLM Outputs by Asking for Directions](https://arxiv.org/abs/2510.16794)
*Jie Zhang,Meng Ding,Yang Liu,Jue Hong,Florian Tramèr*

Main category: cs.CR

TL;DR: 提出了一种利用LLM自然语言表达置信度的黑盒攻击方法，无需访问logits或置信度分数，通过提示LLM表达内部置信度来实现对抗性优化。


<details>
  <summary>Details</summary>
Motivation: 现有的黑盒攻击需要连续模型输出或依赖其他模型的代理信号，这在实践中很少可用。本文旨在开发一种仅基于文本输出的攻击方法。

Method: 通过提示LLM以自然语言表达其内部置信度，利用这种表达进行对抗性优化，应用于视觉-LLM的对抗样本、越狱和提示注入三种攻击场景。

Result: 成功生成了针对仅暴露文本输出的系统的恶意输入，显著扩大了已部署LLM的攻击面。发现更好更大的模型在表达置信度时具有更好的校准性。

Conclusion: 模型能力的改进直接增强了漏洞，形成了一个令人担忧的安全悖论，表明需要重新考虑LLM部署的安全性。

Abstract: We present a novel approach for attacking black-box large language models
(LLMs) by exploiting their ability to express confidence in natural language.
Existing black-box attacks require either access to continuous model outputs
like logits or confidence scores (which are rarely available in practice), or
rely on proxy signals from other models. Instead, we demonstrate how to prompt
LLMs to express their internal confidence in a way that is sufficiently
calibrated to enable effective adversarial optimization. We apply our general
method to three attack scenarios: adversarial examples for vision-LLMs,
jailbreaks and prompt injections. Our attacks successfully generate malicious
inputs against systems that only expose textual outputs, thereby dramatically
expanding the attack surface for deployed LLMs. We further find that better and
larger models exhibit superior calibration when expressing confidence, creating
a concerning security paradox where model capability improvements directly
enhance vulnerability. Our code is available at this
[link](https://github.com/zj-jayzhang/black_box_llm_optimization).

</details>


### [60] [Verifiable Fine-Tuning for LLMs: Zero-Knowledge Training Proofs Bound to Data Provenance and Policy](https://arxiv.org/abs/2510.16830)
*Hasan Akgul,Daniel Borg,Arta Berisha,Amina Rahimova,Andrej Novak,Mila Petrov*

Main category: cs.CR

TL;DR: 提出了可验证微调协议，通过零知识证明确保模型是从公开初始化模型经过声明训练程序和可审计数据集得到的，解决了当前模型发布实践中的数据来源和更新计算缺乏可信保证的问题。


<details>
  <summary>Details</summary>
Motivation: 当前参数高效微调的发布实践无法提供关于使用数据和更新计算的可信保证，存在信任缺口，特别是在受监管和去中心化部署场景中。

Method: 结合五个要素：数据源承诺绑定、可验证采样器、参数高效微调更新电路、递归聚合证明、来源绑定和可信执行属性卡。

Result: 在英语和双语指令混合数据集上，该方法在严格预算内保持实用性，实现实际证明性能，策略配额零违规，私有采样窗口无索引泄露。

Conclusion: 端到端可验证微调对于真实参数高效流水线是可行的，为受监管和去中心化部署填补了关键信任缺口。

Abstract: Large language models are often adapted through parameter efficient fine
tuning, but current release practices provide weak assurances about what data
were used and how updates were computed. We present Verifiable Fine Tuning, a
protocol and system that produces succinct zero knowledge proofs that a
released model was obtained from a public initialization under a declared
training program and an auditable dataset commitment. The approach combines
five elements. First, commitments that bind data sources, preprocessing,
licenses, and per epoch quota counters to a manifest. Second, a verifiable
sampler that supports public replayable and private index hiding batch
selection. Third, update circuits restricted to parameter efficient fine tuning
that enforce AdamW style optimizer semantics and proof friendly approximations
with explicit error budgets. Fourth, recursive aggregation that folds per step
proofs into per epoch and end to end certificates with millisecond
verification. Fifth, provenance binding and optional trusted execution property
cards that attest code identity and constants. On English and bilingual
instruction mixtures, the method maintains utility within tight budgets while
achieving practical proof performance. Policy quotas are enforced with zero
violations, and private sampling windows show no measurable index leakage.
Federated experiments demonstrate that the system composes with probabilistic
audits and bandwidth constraints. These results indicate that end to end
verifiable fine tuning is feasible today for real parameter efficient
pipelines, closing a critical trust gap for regulated and decentralized
deployments.

</details>


### [61] [ThreatIntel-Andro: Expert-Verified Benchmarking for Robust Android Malware Research](https://arxiv.org/abs/2510.16835)
*Hongpeng Bai,Minhong Dong,Yao Zhang,Shunzhe Zhao,Haobo Zhang,Lingyue Li,Yude Bai,Guangquan Xu*

Main category: cs.CR

TL;DR: Android恶意软件数据集存在标签噪声、时效性差和自动化标注工具聚合策略不佳等问题，影响工业网络安全研究。


<details>
  <summary>Details</summary>
Motivation: 移动设备在工业系统中的广泛应用使其成为关键攻击面，但现有主流数据集存在标签噪声和时效性问题，影响恶意软件检测效果。

Method: 未在摘要中明确说明具体方法，但指出了现有数据集对VirusTotal多引擎聚合的过度依赖以及自动化标注工具的缺陷。

Result: 现有数据集存在显著的标签噪声、时效性不足问题，自动化标注工具的聚合策略不佳进一步加剧了标签错误。

Conclusion: 需要更高质量、实时更新的Android恶意软件数据集来支持有效的检测和防御研究。

Abstract: The rapidly evolving Android malware ecosystem demands high-quality,
real-time datasets as a foundation for effective detection and defense. With
the widespread adoption of mobile devices across industrial systems, they have
become a critical yet often overlooked attack surface in industrial
cybersecurity. However, mainstream datasets widely used in academia and
industry (e.g., Drebin) exhibit significant limitations: on one hand, their
heavy reliance on VirusTotal's multi-engine aggregation results introduces
substantial label noise; on the other hand, outdated samples reduce their
temporal relevance. Moreover, automated labeling tools (e.g., AVClass2) suffer
from suboptimal aggregation strategies, further compounding labeling errors and
propagating inaccuracies throughout the research community.

</details>


### [62] [Addendum: Systematic Evaluation of Randomized Cache Designs against Cache Occupancy](https://arxiv.org/abs/2510.16871)
*Anirban Chakraborty,Nimish Mishra,Sayandeep Saha,Sarani Bhattacharya,Debdeep Mukhopadhyay*

Main category: cs.CR

TL;DR: 本文是对USENIX Security 2025主论文的补充说明，讨论了缓存占用率在随机化缓存设计中的性能与安全作用，并回应了相关文献的观察结果。


<details>
  <summary>Details</summary>
Motivation: 系统分析缓存占用率在随机化缓存设计中的角色，从性能和安全性两个角度进行综合评估，并针对相关文献的新观察进行补充讨论。

Method: 提出了统一的基准测试策略用于公平比较不同随机化缓存设计，并从安全角度定义了三种威胁假设：隐蔽信道、进程指纹侧信道和AES密钥恢复攻击。

Result: 发现设计一个既具有现代组相联LLC相当效率，又能抵抗基于争用和基于占用率攻击的随机化缓存是一个开放性问题。

Conclusion: L1d缓存大小对攻击成功率有影响，MIRAGE的随机化初始种子版本可以防止AES密钥泄露，这为随机化缓存设计提供了重要启示。

Abstract: In the main text published at USENIX Security 2025, we presented a systematic
analysis of the role of cache occupancy in the design considerations for
randomized caches (from the perspectives of performance and security). On the
performance front, we presented a uniform benchmarking strategy that allows for
a fair comparison among different randomized cache designs. Likewise, from the
security perspective, we presented three threat assumptions: (1) covert
channels; (2) process fingerprinting side-channel; and (3) AES key recovery.
The main takeaway of our work is an open problem of designing a randomized
cache of comparable efficiency with modern set-associative LLCs, while still
resisting both contention-based and occupancy-based attacks. This note is meant
as an addendum to the main text in light of the observations made in [2]. To
summarize, the authors in [2] argue that (1) L1d cache size plays a role in
adversarial success, and that (2) a patched version of MIRAGE with randomized
initial seeding of global eviction map prevents leakage of AES key. We discuss
the same in this addendum.

</details>


### [63] [On the Credibility of Deniable Communication in Court](https://arxiv.org/abs/2510.16873)
*Jacob Leiken,Sunoo Park*

Main category: cs.CR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Over time, cryptographically deniable systems have come to be associated in
computer-science literature with the idea of "denying" evidence in court -
specifically, with the ability to convincingly forge evidence in courtroom
scenarios and an inability to authenticate evidence in such contexts.
Evidentiary processes in courts, however, have been developed over centuries to
account for the reality that evidence has always been forgeable, and relies on
factors outside of cryptographic models to seek the truth "as well as possible"
while acknowledging that all evidence is imperfect. We argue that deniability
does not and need not change this paradigm.
  Our analysis highlights a gap between technical deniability notions and their
application to the real world. There will always be factors outside a
cryptographic model that influence perceptions of a message's authenticity, in
realistic situations. We propose the broader concept of credibility to capture
these factors. The credibility of a system is determined by (1) a threshold of
quality that a forgery must pass to be "believable" as an original
communication, which varies based on sociotechnical context and threat model,
(2) the ease of creating a forgery that passes this threshold, which is also
context- and threat-model-dependent, and (3) default system retention policy
and retention settings. All three aspects are important for designing secure
communication systems for real-world threat models, and some aspects of (2) and
(3) may be incorporated directly into technical system design. We hope that our
model of credibility will facilitate system design and deployment that
addresses threats that are not and cannot be captured by purely technical
definitions and existing cryptographic models, and support more nuanced
discourse on the strengths and limitations of cryptographic guarantees within
specific legal and sociotechnical contexts.

</details>


### [64] [UNDREAM: Bridging Differentiable Rendering and Photorealistic Simulation for End-to-end Adversarial Attacks](https://arxiv.org/abs/2510.16923)
*Mansi Phute,Matthew Hull,Haoran Wang,Alec Helbling,ShengYun Peng,Willian Lunardi,Martin Andreoni,Wenke Lee,Polo Chau*

Main category: cs.CR

TL;DR: UNDREAM是一个软件框架，通过结合照片级真实感模拟器和可微分渲染器，实现端到端的3D物体对抗扰动优化，解决了现有模拟器不可微分的问题。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型在自动驾驶等安全关键应用中使用的模拟器不可微分，导致无法在模拟环境中集成环境因素来创建有效的对抗攻击。

Method: 开发UNDREAM框架，提供对天气、光照、背景、相机角度、轨迹以及真实人类和物体运动的完全控制，结合可微分渲染器实现端到端优化。

Result: 展示了UNDREAM能够生成多种物理上合理的对抗物体，并可在不同可配置环境中快速探索。

Conclusion: 照片级真实感模拟与可微分优化的结合为物理对抗攻击研究开辟了新途径。

Abstract: Deep learning models deployed in safety critical applications like autonomous
driving use simulations to test their robustness against adversarial attacks in
realistic conditions. However, these simulations are non-differentiable,
forcing researchers to create attacks that do not integrate simulation
environmental factors, reducing attack success. To address this limitation, we
introduce UNDREAM, the first software framework that bridges the gap between
photorealistic simulators and differentiable renderers to enable end-to-end
optimization of adversarial perturbations on any 3D objects. UNDREAM enables
manipulation of the environment by offering complete control over weather,
lighting, backgrounds, camera angles, trajectories, and realistic human and
object movements, thereby allowing the creation of diverse scenes. We showcase
a wide array of distinct physically plausible adversarial objects that UNDREAM
enables researchers to swiftly explore in different configurable environments.
This combination of photorealistic simulation and differentiable optimization
opens new avenues for advancing research of physical adversarial attacks.

</details>


### [65] [Efficient derandomization of differentially private counting queries](https://arxiv.org/abs/2510.16959)
*Surendra Ghentiyala*

Main category: cs.CR

TL;DR: 本文提出了一种多项式时间机制，用于差分隐私下的d个计数查询，仅需O(log d)位随机性，显著优于传统方法所需的90TB随机性。


<details>
  <summary>Details</summary>
Motivation: 2020年人口普查的差分隐私需要约90TB随机性，这在实践中成本高昂或不可行。因此需要研究如何减少差分隐私机制所需的随机性复杂度。

Method: 基于简单观察：对每个计数查询进行随机偏移后，许多查询的结果在是否添加噪声的情况下保持不变，从而可以省略对这些查询添加噪声的步骤。该方法不使用复杂的舍入方案。

Result: 提出的多项式时间机制实现了与[CSV25]几乎相同的随机性复杂度与准确性权衡，仅需O(log d)位随机性。

Conclusion: 该机制不仅高效，而且为理解通过批处理d个计数查询获得随机性节省的根源提供了更清晰的见解。

Abstract: Differential privacy for the 2020 census required an estimated 90 terabytes
of randomness [GL20], an amount which may be prohibitively expensive or
entirely infeasible to generate. Motivated by these practical concerns, [CSV25]
initiated the study of the randomness complexity of differential privacy, and
in particular, the randomness complexity of $d$ counting queries. This is the
task of outputting the number of entries in a dataset that satisfy predicates
$\mathcal{P}_1, \dots, \mathcal{P}_d$ respectively. They showed the rather
surprising fact that though any reasonably accurate,
$\varepsilon$-differentially private mechanism for one counting query requires
$1-O(\varepsilon)$ bits of randomness in expectation, there exists a fairly
accurate mechanism for $d$ counting queries which requires only $O(\log d)$
bits of randomness in expectation.
  The mechanism of [CSV25] is inefficient (not polynomial time) and relies on a
combinatorial object known as rounding schemes. Here, we give a polynomial time
mechanism which achieves nearly the same randomness complexity versus accuracy
tradeoff as that of [CSV25]. Our construction is based on the following simple
observation: after a randomized shift of the answer to each counting query, the
answer to many counting queries remains the same regardless of whether we add
noise to that coordinate or not. This allows us to forgo the step of adding
noise to the result of many counting queries. Our mechanism does not make use
of rounding schemes. Therefore, it provides a different -- and, in our opinion,
clearer -- insight into the origins of the randomness savings that can be
obtained by batching $d$ counting queries. Therefore, it provides a different
-- and, in our opinion, clearer -- insight into the origins of the randomness
savings that can be obtained by batching $d$ counting queries.

</details>


### [66] [Bits Leaked per Query: Information-Theoretic Bounds on Adversarial Attacks against LLMs](https://arxiv.org/abs/2510.17000)
*Masahiro Kaneko,Timothy Baldwin*

Main category: cs.CR

TL;DR: 提出了一个信息论框架来衡量LLM在对抗攻击中的信息泄露程度，揭示了不同输出类型（答案token、logits、思维过程）对攻击成本的影响，为平衡透明度和安全性提供了理论依据。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在对抗攻击中的信息泄露程度缺乏量化评估，导致审计者缺乏原则性指导，防御者无法权衡透明度与风险。需要建立理论框架来量化信息泄露并指导安全部署。

Method: 使用互信息I(Z;T)作为信息泄露度量，构建信息论框架分析不同观测信号Z（答案token、logits、思维过程）与目标属性T之间的关系，推导攻击所需查询次数与泄露率的关系。

Result: 实验显示：仅暴露答案token需要约1000次查询；添加logits可减少到约100次；暴露完整思维过程仅需几十次查询。攻击成本与泄露率成反比，与期望精度呈对数关系。

Conclusion: 该框架为LLM部署中的透明度-安全性权衡提供了首个原则性衡量标准，证明适度增加信息泄露会显著降低攻击成本，强调了在提供透明度时需谨慎控制信息泄露。

Abstract: Adversarial attacks by malicious users that threaten the safety of large
language models (LLMs) can be viewed as attempts to infer a target property $T$
that is unknown when an instruction is issued, and becomes knowable only after
the model's reply is observed. Examples of target properties $T$ include the
binary flag that triggers an LLM's harmful response or rejection, and the
degree to which information deleted by unlearning can be restored, both
elicited via adversarial instructions. The LLM reveals an \emph{observable
signal} $Z$ that potentially leaks hints for attacking through a response
containing answer tokens, thinking process tokens, or logits. Yet the scale of
information leaked remains anecdotal, leaving auditors without principled
guidance and defenders blind to the transparency--risk trade-off. We fill this
gap with an information-theoretic framework that computes how much information
can be safely disclosed, and enables auditors to gauge how close their methods
come to the fundamental limit. Treating the mutual information $I(Z;T)$ between
the observation $Z$ and the target property $T$ as the leaked bits per query,
we show that achieving error $\varepsilon$ requires at least
$\log(1/\varepsilon)/I(Z;T)$ queries, scaling linearly with the inverse leak
rate and only logarithmically with the desired accuracy. Thus, even a modest
increase in disclosure collapses the attack cost from quadratic to logarithmic
in terms of the desired accuracy. Experiments on seven LLMs across
system-prompt leakage, jailbreak, and relearning attacks corroborate the
theory: exposing answer tokens alone requires about a thousand queries; adding
logits cuts this to about a hundred; and revealing the full thinking process
trims it to a few dozen. Our results provide the first principled yardstick for
balancing transparency and security when deploying LLMs.

</details>


### [67] [Watermark Robustness and Radioactivity May Be at Odds in Federated Learning](https://arxiv.org/abs/2510.17033)
*Leixu Huang,Zedian Shao,Teodora Baluta*

Main category: cs.CR

TL;DR: 本文提出在联邦学习中应用LLM水印技术进行数据溯源，水印具有放射性特征（训练后仍可检测），但面临服务器主动过滤攻击的挑战。


<details>
  <summary>Details</summary>
Motivation: 随着联邦学习中越来越多使用LLM生成的数据，需要建立数据溯源机制以确保问责制和透明度。

Method: 将LLM水印技术应用于联邦学习环境，让部分客户端在带水印数据上计算本地更新，服务器聚合所有更新到全局LLM中。

Result: 水印具有放射性特征，即使只有6.6%的数据带水印，p值仍可达10^{-24}。但服务器作为主动对抗者可以通过鲁棒聚合过滤水印更新。

Conclusion: 放射性水印在主动过滤服务器面前不具鲁棒性，需要在放射性、鲁棒性和实用性之间进行权衡。

Abstract: Federated learning (FL) enables fine-tuning large language models (LLMs)
across distributed data sources. As these sources increasingly include
LLM-generated text, provenance tracking becomes essential for accountability
and transparency. We adapt LLM watermarking for data provenance in FL where a
subset of clients compute local updates on watermarked data, and the server
averages all updates into the global LLM. In this setup, watermarks are
radioactive: the watermark signal remains detectable after fine-tuning with
high confidence. The $p$-value can reach $10^{-24}$ even when as little as
$6.6\%$ of data is watermarked. However, the server can act as an active
adversary that wants to preserve model utility while evading provenance
tracking. Our observation is that updates induced by watermarked synthetic data
appear as outliers relative to non-watermark updates. Our adversary thus
applies strong robust aggregation that can filter these outliers, together with
the watermark signal. All evaluated radioactive watermarks are not robust
against such an active filtering server. Our work suggests fundamental
trade-offs between radioactivity, robustness, and utility.

</details>


### [68] [Quantum Key Distribution for Virtual Power Plant Communication: A Lightweight Key-Aware Scheduler with Provable Stability](https://arxiv.org/abs/2510.17087)
*Ziqing Zhu*

Main category: cs.CR

TL;DR: 提出了一种面向虚拟电厂量子密钥分发的密钥感知优先级和配额框架，将量子密钥作为一级调度资源进行管理，显著提升了关键消息的传输性能和系统稳定性。


<details>
  <summary>Details</summary>
Motivation: 虚拟电厂需要高频通信支持分钟级和秒级实时调度，传统PKI和密钥轮换方案难以满足跨域高频消息传输需求，且面临量子计算威胁。量子密钥分发虽然提供信息论安全，但其密钥产量有限且随机，与突发性VPP流量不匹配。

Method: 设计包含四个关键组件的框架：(1)基于预测的长期配额和短期令牌机制；(2)密钥感知的赤字轮询仲裁；(3)抢占式应急密钥储备；(4)通过加密模式切换和非关键流量降采样的优雅降级机制。

Result: 在IEEE 33和123总线VPP系统测试中，相比FIFO、固定优先级和静态配额基线，该方案持续降低了关键消息的尾部延迟和被动超时，提高了每比特密钥效用，增强了密钥稀缺和状态切换期间的功率跟踪可靠性。

Conclusion: 该密钥感知调度框架在平均供需平衡下建立了强稳定性，为量子密钥资源受限的虚拟电厂通信提供了可量化的操作保障。

Abstract: Virtual power plants (VPPs) are becoming a cornerstone of future grids,
aggregating distributed PV, wind, storage, and flexible loads for market
participation and real-time balancing. As operations move to minute-- and
second--level feedback, communication security shifts from a compliance item to
an operational constraint: latency, reliability, and confidentiality jointly
determine whether dispatch, protection, and settlement signals arrive on time.
Conventional PKI and key-rotation schemes struggle with cross-domain,
high-frequency messaging and face long-term quantum threats. Quantum key
distribution (QKD) offers information-theoretic key freshness, but its key
yield is scarce and stochastic, often misaligned with bursty VPP traffic. This
paper proposes a key-aware priority and quota framework that treats quantum
keys as first-class scheduling resources. The design combines (i)
forecast-driven long-term quotas and short-term tokens, (ii) key-aware
deficit-round-robin arbitration, (iii) a preemptive emergency key reserve, and
(iv) graceful degradation via encryption-mode switching and controlled
down-sampling for non-critical traffic. A drift-plus-penalty analysis
establishes strong stability under average supply--demand balance with
quantifiable bounds on backlog and tail latency, providing interpretable
operating guarantees. We build a reproducible testbed on IEEE 33- and 123-bus
VPP systems and evaluate normal, degraded, and outage regimes with
industry-consistent message classes and TTLs. Against FIFO, fixed-priority, and
static-quota baselines, the proposed scheme consistently reduces tail delay and
passive timeouts for critical messages, improves per-bit key utility, and
enhances power-tracking reliability during key scarcity and regime switches.

</details>


### [69] [Can Transformer Memory Be Corrupted? Investigating Cache-Side Vulnerabilities in Large Language Models](https://arxiv.org/abs/2510.17098)
*Elias Hossain,Swayamjit Saha,Somshubhra Roy,Ravi Prasad*

Main category: cs.CR

TL;DR: 论文提出了恶意令牌注入(MTI)框架，通过在推理过程中扰动transformer语言模型的KV缓存来攻击模型，揭示了缓存完整性是一个被忽视的安全漏洞。


<details>
  <summary>Details</summary>
Motivation: 即使提示和参数得到保护，transformer语言模型仍然容易受到攻击，因为推理过程中的键值(KV)缓存构成了一个被忽视的攻击面。

Method: MTI框架通过控制幅度和频率，在选定层和时间步上使用加性高斯噪声、归零和正交旋转来系统性地扰动缓存的键向量。

Result: 实验结果显示MTI显著改变了GPT-2和LLaMA-2/7B的下一个令牌分布和下游任务性能，并破坏了检索增强和代理推理管道的稳定性。

Conclusion: 这些发现表明缓存完整性是当前LLM部署中关键但未被充分探索的漏洞，将缓存损坏定位为未来鲁棒性和安全研究的可复现且理论基础的威胁模型。

Abstract: Even when prompts and parameters are secured, transformer language models
remain vulnerable because their key-value (KV) cache during inference
constitutes an overlooked attack surface. This paper introduces Malicious Token
Injection (MTI), a modular framework that systematically perturbs cached key
vectors at selected layers and timesteps through controlled magnitude and
frequency, using additive Gaussian noise, zeroing, and orthogonal rotations. A
theoretical analysis quantifies how these perturbations propagate through
attention, linking logit deviations to the Frobenius norm of corruption and
softmax Lipschitz dynamics. Empirical results show that MTI significantly
alters next-token distributions and downstream task performance across GPT-2
and LLaMA-2/7B, as well as destabilizes retrieval-augmented and agentic
reasoning pipelines. These findings identify cache integrity as a critical yet
underexplored vulnerability in current LLM deployments, positioning cache
corruption as a reproducible and theoretically grounded threat model for future
robustness and security research.

</details>


### [70] [QRïS: A Preemptive Novel Method for Quishing Detection Through Structural Features of QR](https://arxiv.org/abs/2510.17175)
*Muhammad Wahid Akram,Keshav Sood,Muneeb Ul Hassan*

Main category: cs.CR

TL;DR: 提出了QR"iS方法，通过QR码的结构分析来识别钓鱼攻击，使用24个结构特征训练机器学习模型，准确率达83.18%，并开发了移动应用验证实用性。


<details>
  <summary>Details</summary>
Motivation: 现有QR码防钓鱼方法依赖黑盒技术，缺乏可解释性和透明度，存在信任、问责和偏见检测等问题。

Method: 生成40万个QR码数据集，开发算法提取24个结构特征，训练机器学习模型进行分类。

Result: 模型准确率最高达83.18%，开发了移动应用验证了方法的实际可行性。

Conclusion: QR"iS是首个通过QR码结构分析分类的方法，具有透明、可复现、可扩展和易理解的优势。

Abstract: Globally, individuals and organizations employ Quick Response (QR) codes for
swift and convenient communication. Leveraging this, cybercriminals embed
falsify and misleading information in QR codes to launch various phishing
attacks which termed as Quishing. Many former studies have introduced defensive
approaches to preclude Quishing such as by classifying the embedded content of
QR codes and then label the QR codes accordingly, whereas other studies
classify them using visual features (i.e., deep features, histogram density
analysis features). However, these approaches mainly rely on black-box
techniques which do not clearly provide interpretability and transparency to
fully comprehend and reproduce the intrinsic decision process; therefore,
having certain obvious limitations includes the approaches' trust,
accountability, issues in bias detection, and many more. We proposed QR\"iS,
the pioneer method to classify QR codes through the comprehensive structural
analysis of a QR code which helps to identify phishing QR codes beforehand. Our
classification method is clearly transparent which makes it reproducible,
scalable, and easy to comprehend. First, we generated QR codes dataset (i.e.
400,000 samples) using recently published URLs datasets [1], [2]. Then, unlike
black-box models, we developed a simple algorithm to extract 24 structural
features from layout patterns present in QR codes. Later, we train the machine
learning models on the harvested features and obtained accuracy of up to
83.18%. To further evaluate the effectiveness of our approach, we perform the
comparative analysis of proposed method with relevant contemporary studies.
Lastly, for real-world deployment and validation, we developed a mobile app
which assures the feasibility of the proposed solution in real-world scenarios
which eventually strengthen the applicability of the study.

</details>


### [71] [Exploiting the Potential of Linearity in Automatic Differentiation and Computational Cryptography](https://arxiv.org/abs/2510.17220)
*Giulia Giusti*

Main category: cs.CR

TL;DR: 本文探讨了线性逻辑在编程范式中的应用，主要包括ADLL和CryptoBLL两部分。ADLL将线性逻辑应用于自动微分，CryptoBLL用于表达计算密码学中的复杂性约束。


<details>
  <summary>Details</summary>
Motivation: 线性概念在数学和计算机科学中具有重要地位，数学中支撑函数和向量空间，计算机科学中与资源敏感计算相关。线性逻辑能够精确建模计算资源的使用，为编程语言、类型系统和形式模型提供理论基础。

Method: 论文采用线性逻辑（LL）作为核心工具：1）ADLL部分将线性逻辑应用于自动微分，建模实数上的线性函数和转置操作；2）CryptoBLL部分使用线性逻辑表达计算密码学中对手的复杂性约束。

Result: ADLL连接了JAX的类型系统与线性逻辑理论，弥合了自动微分理论（基于证明论）与实践（如JAX实现）之间的差距。CryptoBLL提出了一个用于自动分析计算密码学协议的新框架。

Conclusion: 线性逻辑为建模编程范式提供了强大的理论基础，能够同时处理计算复杂性和可组合性。该研究为分析和验证复杂系统提供了严谨而实用的方法学，在自动微分和密码学领域展示了线性逻辑的实际应用价值。

Abstract: The concept of linearity plays a central role in both mathematics and
computer science, with distinct yet complementary meanings. In mathematics,
linearity underpins functions and vector spaces, forming the foundation of
linear algebra and functional analysis. In computer science, it relates to
resource-sensitive computation. Linear Logic (LL), for instance, models
assumptions that must be used exactly once, providing a natural framework for
tracking computational resources such as time, memory, or data access. This
dual perspective makes linearity essential to programming languages, type
systems, and formal models that express both computational complexity and
composability. Bridging these interpretations enables rigorous yet practical
methodologies for analyzing and verifying complex systems.
  This thesis explores the use of LL to model programming paradigms based on
linearity. It comprises two parts: ADLL and CryptoBLL. The former applies LL to
Automatic Differentiation (AD), modeling linear functions over the reals and
the transposition operation. The latter uses LL to express complexity
constraints on adversaries in computational cryptography.
  In AD, two main approaches use linear type systems: a theoretical one
grounded in proof theory, and a practical one implemented in JAX, a Python
library developed by Google for machine learning research. In contrast,
frameworks like PyTorch and TensorFlow support AD without linear types. ADLL
aims to bridge theory and practice by connecting JAX's type system to LL.
  In modern cryptography, several calculi aim to model cryptographic proofs
within the computational paradigm. These efforts face a trade-off between
expressiveness, to capture reductions, and simplicity, to abstract probability
and complexity. CryptoBLL addresses this tension by proposing a framework for
the automatic analysis of protocols in computational cryptography.

</details>


### [72] [Multimodal Safety Is Asymmetric: Cross-Modal Exploits Unlock Black-Box MLLMs Jailbreaks](https://arxiv.org/abs/2510.17277)
*Xinkai Wang,Beibei Li,Zerui Shao,Ao Liu,Shouling Ji*

Main category: cs.CR

TL;DR: 本文提出了PolyJailbreak方法，利用多模态安全不对称性，通过强化学习自动生成对抗性输入来绕过多模态大语言模型的安全约束。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在实际应用中存在安全隐患，容易受到越狱攻击，导致模型产生不道德响应。研究发现视觉对齐在不同模态间施加了不均匀的安全约束，形成了多模态安全不对称性。

Method: 开发基于强化学习的黑盒越狱方法PolyJailbreak：首先探测模型的注意力动态和潜在表示空间，分析视觉输入如何重塑跨模态信息流；然后系统化为可重用的原子策略原语库；最后采用多代理优化过程自动调整输入对抗目标模型。

Result: 在多种开源和闭源MLLMs上进行了全面评估，证明PolyJailbreak优于现有最先进的基线方法。

Conclusion: PolyJailbreak方法有效利用了多模态安全不对称性，能够成功绕过MLLMs的安全约束，揭示了当前多模态模型在安全防护方面的脆弱性。

Abstract: Multimodal large language models (MLLMs) have demonstrated significant
utility across diverse real-world applications. But MLLMs remain vulnerable to
jailbreaks, where adversarial inputs can collapse their safety constraints and
trigger unethical responses. In this work, we investigate jailbreaks in the
text-vision multimodal setting and pioneer the observation that visual
alignment imposes uneven safety constraints across modalities in MLLMs, thereby
giving rise to multimodal safety asymmetry. We then develop PolyJailbreak, a
black-box jailbreak method grounded in reinforcement learning. Initially, we
probe the model's attention dynamics and latent representation space, assessing
how visual inputs reshape cross-modal information flow and diminish the model's
ability to separate harmful from benign inputs, thereby exposing exploitable
vulnerabilities. On this basis, we systematize them into generalizable and
reusable operational rules that constitute a structured library of Atomic
Strategy Primitives, which translate harmful intents into jailbreak inputs
through step-wise transformations. Guided by the primitives, PolyJailbreak
employs a multi-agent optimization process that automatically adapts inputs
against the target models. We conduct comprehensive evaluations on a variety of
open-source and closed-source MLLMs, demonstrating that PolyJailbreak
outperforms state-of-the-art baselines.

</details>


### [73] [Analysis of Input-Output Mappings in Coinjoin Transactions with Arbitrary Values](https://arxiv.org/abs/2510.17284)
*Jiri Gavenda,Petr Svenda,Stanislav Bobon,Vladimir Sedlacek*

Main category: cs.CR

TL;DR: CoinJoin协议通过协作交易增强比特币隐私，但隐私增益难以量化。研究发现主要CoinJoin设计（Whirlpool、Wasabi 1.x/2.x）的平均匿名集大小减少10-50%，并在一年后趋于稳定。开发了考虑费用、实现限制和用户行为的精确隐私评估方法，证明即使使用改进的分析算法，正确追踪硬币归属仍非常困难。


<details>
  <summary>Details</summary>
Motivation: CoinJoin协议旨在通过违反常见分析启发式假设来增强比特币交易隐私，但由于多种影响因素和计算复杂性，量化其隐私增益仍是一个未解决的问题。

Method: 1. 调整BlockSci链上分析软件以分析CoinJoin交易
2. 设计考虑CoinJoin费用、实现特定限制和用户混合后行为的精确并行隐私评估方法
3. 在模拟和真实世界Wasabi 2.x CoinJoin上进行详细评估，并外推到最大规模的真实世界CoinJoin

Result: 1. 三种主要CoinJoin设计（Whirlpool、Wasabi 1.x、Wasabi 2.x）的平均混合后匿名集大小减少10-50%
2. 减少幅度在第一天最高，一年后变得可忽略不计
3. 即使考虑用户不理想的混合后行为，使用改进的分析算法正确归属硬币仍非常困难

Conclusion: 尽管用户混合后行为不理想，但通过CoinJoin协议正确追踪硬币归属仍然极其困难，即使使用改进的分析算法。CoinJoin在增强比特币交易隐私方面仍然有效。

Abstract: A coinjoin protocol aims to increase transactional privacy for Bitcoin and
Bitcoin-like blockchains via collaborative transactions, by violating
assumptions behind common analysis heuristics. Estimating the resulting privacy
gain is a crucial yet unsolved problem due to a range of influencing factors
and large computational complexity.
  We adapt the BlockSci on-chain analysis software to coinjoin transactions,
demonstrating a significant (10-50%) average post-mix anonymity set size
decrease for all three major designs with a central coordinator: Whirlpool,
Wasabi 1.x, and Wasabi 2.x. The decrease is highest during the first day and
negligible after one year from a coinjoin creation.
  Moreover, we design a precise, parallelizable privacy estimation method,
which takes into account coinjoin fees, implementation-specific limitations and
users' post-mix behavior. We evaluate our method in detail on a set of emulated
and real-world Wasabi 2.x coinjoins and extrapolate to its largest real-world
coinjoins with hundreds of inputs and outputs. We conclude that despite the
users' undesirable post-mix behavior, correctly attributing the coins to their
owners is still very difficult, even with our improved analysis algorithm.

</details>


### [74] [Single-Shuffle Full-Open Card-Based Protocols for Any Function](https://arxiv.org/abs/2510.17308)
*Reo Eriguchi,Kazumasa Shinagawa*

Main category: cs.CR

TL;DR: 本文首次证明了所有函数都存在单次洗牌全公开的卡牌安全计算协议，提出了两种在卡牌数量和洗牌复杂度之间权衡的构造方法。


<details>
  <summary>Details</summary>
Motivation: 单次洗牌全公开协议是卡牌安全计算的最小模型，但之前已知可实现的函数类仅限于少数小例子，限制了该模型的应用范围。

Method: 通过建立单次洗牌全公开协议与密码学原语"私有同时消息协议"的新联系，设计了两种构造方法，并在仅公开部分卡牌的变体中降低了洗牌复杂度。

Result: 成功证明了所有函数都存在单次洗牌全公开协议，并提供了具体的协议构造，在卡牌数量和洗牌复杂度之间实现了权衡。

Conclusion: 这项工作扩展了单次洗牌全公开协议的应用范围，为卡牌安全计算提供了新的理论基础和实用构造方法。

Abstract: A card-based secure computation protocol is a method for $n$ parties to
compute a function $f$ on their private inputs $(x_1,\ldots,x_n)$ using
physical playing cards, in such a way that the suits of revealed cards leak no
information beyond the value of $f(x_1,\ldots,x_n)$. A \textit{single-shuffle
full-open} protocol is a minimal model of card-based secure computation in
which, after the parties place face-down cards representing their inputs, a
single shuffle operation is performed and then all cards are opened to derive
the output. Despite the simplicity of this model, the class of functions known
to admit single-shuffle full-open protocols has been limited to a few small
examples. In this work, we prove for the first time that every function admits
a single-shuffle full-open protocol. We present two constructions that offer a
trade-off between the number of cards and the complexity of the shuffle
operation. These feasibility results are derived from a novel connection
between single-shuffle full-open protocols and a cryptographic primitive known
as \textit{Private Simultaneous Messages} protocols, which has rarely been
studied in the context of card-based cryptography. We also present variants of
single-shuffle protocols in which only a subset of cards are revealed. These
protocols reduce the complexity of the shuffle operation compared to existing
protocols in the same setting.

</details>


### [75] [The Hidden Dangers of Public Serverless Repositories: An Empirical Security Assessment](https://arxiv.org/abs/2510.17311)
*Eduard Marin,Jinwoo Kim,Alessio Pavoni,Mauro Conti,Roberto Di Pietro*

Main category: cs.CR

TL;DR: 对5个公共无服务器仓库和3个IaC框架的2,758个无服务器组件和125,936个模板进行安全分析，发现系统性漏洞并提出缓解建议。


<details>
  <summary>Details</summary>
Motivation: 公共无服务器仓库日益流行但安全状况未被充分研究，开发者和组织面临潜在风险。

Method: 分析2,758个无服务器组件和125,936个基础设施即代码模板，识别多种安全漏洞类型。

Result: 发现过时软件包、敏感参数误用、可被利用的部署配置、易受域名抢注攻击以及压缩组件中嵌入恶意行为等系统性漏洞。

Conclusion: 无服务器公共仓库存在严重安全风险，需要实施缓解措施来保护开发者和组织。

Abstract: Serverless computing has rapidly emerged as a prominent cloud paradigm,
enabling developers to focus solely on application logic without the burden of
managing servers or underlying infrastructure. Public serverless repositories
have become key to accelerating the development of serverless applications.
However, their growing popularity makes them attractive targets for
adversaries. Despite this, the security posture of these repositories remains
largely unexplored, exposing developers and organizations to potential risks.
In this paper, we present the first comprehensive analysis of the security
landscape of serverless components hosted in public repositories. We analyse
2,758 serverless components from five widely used public repositories popular
among developers and enterprises, and 125,936 Infrastructure as Code (IaC)
templates across three widely used IaC frameworks. Our analysis reveals
systemic vulnerabilities including outdated software packages, misuse of
sensitive parameters, exploitable deployment configurations, susceptibility to
typo-squatting attacks and opportunities to embed malicious behaviour within
compressed serverless components. Finally, we provide practical recommendations
to mitigate these threats.

</details>


### [76] [Process Automation Architecture Using RFID for Transparent Voting Systems](https://arxiv.org/abs/2510.17403)
*Stella N. Arinze,Patrick U. Okafor,Onyekachi M. Egwuagu,Augustine O. Nwajana*

Main category: cs.CR

TL;DR: 开发基于RFID技术的投票系统自动化架构，实现安全透明的投票流程，支持在线和离线模式，测试显示100%认证准确率和11.5秒平均投票时间。


<details>
  <summary>Details</summary>
Motivation: 为选举过程提供安全、透明和高效的自动化解决方案，特别适用于需要数字化转型的选举环境。

Method: 使用RFID智能卡进行选民身份验证，结合RC522读卡器和微控制器，通过触摸屏界面投票，采用AES-128加密和GSM传输，配备防篡改监控机制。

Result: 系统测试显示100%选民认证准确率，最小化延迟（平均投票时间11.5秒），有效防止克隆、重复投票和数据拦截。

Conclusion: 该架构展示了可扩展的自动化投票解决方案，提供增强的透明度、韧性和部署灵活性。

Abstract: This paper presents the development of a process automation architecture
leveraging Radio Frequency Identification (RFID) technology for secure,
transparent and efficient voting systems. The proposed architecture automates
the voting workflow through RFID-enabled voter identification, encrypted vote
casting, and secure data transmission. Each eligible voter receives a smart
RFID card containing a uniquely encrypted identifier, which is verified using
an RC522 reader interfaced with a microcontroller. Upon successful
verification, the voter interacts with a touchscreen interface to cast a vote,
which is then encrypted using AES-128 and securely stored on a local SD card or
transmitted via GSM to a central server. A tamper-proof monitoring mechanism
records each session with time-stamped digital signatures, ensuring
auditability and data integrity. The architecture is designed to function in
both online and offline modes, with an automated batch synchronization
mechanism that updates vote records once network connectivity is restored.
System testing in simulated environments confirmed 100% voter authentication
accuracy, minimized latency (average voting time of 11.5 seconds), and
robustness against cloning, double voting, and data interception. The
integration of real-time monitoring and secure process control modules enables
electoral authorities to automate data logging, detect anomalies, and validate
system integrity dynamically. This work demonstrates a scalable,
automation-driven solution for voting infrastructure, offering enhanced
transparency, resilience, and deployment flexibility, especially in
environments where digital transformation of electoral processes is critically
needed.

</details>


### [77] [Cybersecurity AI: Evaluating Agentic Cybersecurity in Attack/Defense CTFs](https://arxiv.org/abs/2510.17521)
*Francesco Balassone,Víctor Mayoral-Vilches,Stefan Rass,Martin Pinzger,Gaetano Perrone,Simon Pietro Romano,Peter Schartner*

Main category: cs.CR

TL;DR: AI防御系统在无约束条件下比攻击系统更有效（54.3% vs 28.3%），但在操作约束下这种优势消失。防御效果高度依赖成功标准，挑战了AI攻击优势的传统观点。


<details>
  <summary>Details</summary>
Motivation: 实证评估AI系统在网络安全中是更擅长攻击还是防御，挑战关于AI攻击优势的传统观点。

Method: 使用CAI并行执行框架，在23个攻防CTF战场部署自主代理，进行统计分析。

Result: 防御代理在无约束修补成功率达54.3%，显著高于攻击初始访问的28.3%；但在操作约束下（如保持可用性、防止所有入侵），优势消失。

Conclusion: 防御有效性关键取决于成功标准，这一细微差别对实际部署至关重要。研究强调防御者需要采用开源网络安全AI框架来对抗加速的攻击自动化。

Abstract: We empirically evaluate whether AI systems are more effective at attacking or
defending in cybersecurity. Using CAI (Cybersecurity AI)'s parallel execution
framework, we deployed autonomous agents in 23 Attack/Defense CTF
battlegrounds. Statistical analysis reveals defensive agents achieve 54.3%
unconstrained patching success versus 28.3% offensive initial access
(p=0.0193), but this advantage disappears under operational constraints: when
defense requires maintaining availability (23.9%) and preventing all intrusions
(15.2%), no significant difference exists (p>0.05). Exploratory taxonomy
analysis suggests potential patterns in vulnerability exploitation, though
limited sample sizes preclude definitive conclusions. This study provides the
first controlled empirical evidence challenging claims of AI attacker
advantage, demonstrating that defensive effectiveness critically depends on
success criteria, a nuance absent from conceptual analyses but essential for
deployment. These findings underscore the urgency for defenders to adopt
open-source Cybersecurity AI frameworks to maintain security equilibrium
against accelerating offensive automation.

</details>


### [78] [Dynamic Switched Quantum Key Distribution Networkwith PUF-based authentication](https://arxiv.org/abs/2510.17552)
*Persefoni Konteli,Nikolaos Makris,Evgenia Niovi Sassalou,Stylianos A. Kazazis,Alkinoos Papageorgopoulos,Stefanos Vasileiadis,Konstantinos Tsimvrakidis,Symeon Tsintzos,Georgios M. Nikolopoulos,George T. Kanellos*

Main category: cs.CR

TL;DR: 展示了一个中心控制的动态交换QKD网络，集成了基于PUF的动态认证机制


<details>
  <summary>Details</summary>
Motivation: 提高量子密钥分发网络的安全性和动态管理能力

Method: 采用中心控制的动态交换QKD网络架构，集成基于PUF的实时认证

Result: 分析了具有实时PUF认证的动态交换QKD网络的性能

Conclusion: 该方案为QKD网络提供了增强的安全性和动态管理功能

Abstract: We demonstrate a centrally controlled dynamic switched-QKD network,
withintegrated PUF-based dynamic authentication for each QKD link. The
performance of the dynamicswitched-QKD network with real-time PUF-based
authentication is analyzed.

</details>


### [79] [GUIDE: Enhancing Gradient Inversion Attacks in Federated Learning with Denoising Models](https://arxiv.org/abs/2510.17621)
*Vincenzo Carletti,Pasquale Foggia,Carlo Mazzocca,Giuseppe Parrella,Mario Vento*

Main category: cs.CR

TL;DR: GUIDE是一种利用扩散模型作为去噪工具来增强联邦学习中图像重建攻击效果的新方法，可集成到任何使用代理数据集的梯度反转攻击中，显著提高重建质量。


<details>
  <summary>Details</summary>
Motivation: 联邦学习虽然保护隐私，但客户端更新仍容易受到隐私泄露攻击，特别是梯度反转攻击。现有方法重建的图像通常含有噪声，需要专门的去噪技术来提升质量。

Method: 提出GUIDE方法，将扩散模型作为去噪工具集成到梯度反转攻击中，利用扩散模型的强大去噪能力来提升重建图像的质量。

Result: GUIDE与两种最先进的梯度反转攻击无缝集成，在多种指标上显著提高重建质量，DreamSim感知相似度指标提升高达46%。

Conclusion: 扩散模型作为去噪工具能有效增强联邦学习中的梯度反转攻击效果，为隐私保护研究提供了重要启示。

Abstract: Federated Learning (FL) enables collaborative training of Machine Learning
(ML) models across multiple clients while preserving their privacy. Rather than
sharing raw data, federated clients transmit locally computed updates to train
the global model. Although this paradigm should provide stronger privacy
guarantees than centralized ML, client updates remain vulnerable to privacy
leakage. Adversaries can exploit them to infer sensitive properties about the
training data or even to reconstruct the original inputs via Gradient Inversion
Attacks (GIAs). Under the honest-butcurious threat model, GIAs attempt to
reconstruct training data by reversing intermediate updates using
optimizationbased techniques. We observe that these approaches usually
reconstruct noisy approximations of the original inputs, whose quality can be
enhanced with specialized denoising models. This paper presents Gradient Update
Inversion with DEnoising (GUIDE), a novel methodology that leverages diffusion
models as denoising tools to improve image reconstruction attacks in FL. GUIDE
can be integrated into any GIAs that exploits surrogate datasets, a widely
adopted assumption in GIAs literature. We comprehensively evaluate our approach
in two attack scenarios that use different FL algorithms, models, and datasets.
Our results demonstrate that GUIDE integrates seamlessly with two state-ofthe-
art GIAs, substantially improving reconstruction quality across multiple
metrics. Specifically, GUIDE achieves up to 46% higher perceptual similarity,
as measured by the DreamSim metric.

</details>


### [80] [CrossGuard: Safeguarding MLLMs against Joint-Modal Implicit Malicious Attacks](https://arxiv.org/abs/2510.17687)
*Xu Zhang,Hao Li,Zhichao Lu*

Main category: cs.CR

TL;DR: 提出ImpForge自动红队管道生成多样化隐式攻击样本，并基于此开发CrossGuard意图感知防护系统，有效防御显式和隐式多模态威胁。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在推理和感知方面表现出色，但面临日益严重的越狱攻击威胁。现有研究主要关注显式攻击，而隐式攻击（良性文本和图像联合表达不安全意图）难以检测且研究不足，主要由于高质量隐式数据稀缺。

Method: 1. 提出ImpForge：利用强化学习和定制奖励模块的自动红队管道，在14个领域生成多样化隐式样本；2. 开发CrossGuard：基于该数据集的意图感知防护系统，提供对显式和隐式威胁的全面防御。

Result: 在安全和不安全基准测试、隐式和显式攻击以及多个域外设置上的广泛实验表明，CrossGuard显著优于现有防御方法（包括先进MLLM和防护栏），在保持高实用性的同时实现更强的安全性。

Conclusion: 该工作为增强MLLM对抗现实世界多模态威胁的鲁棒性提供了平衡且实用的解决方案。

Abstract: Multimodal Large Language Models (MLLMs) achieve strong reasoning and
perception capabilities but are increasingly vulnerable to jailbreak attacks.
While existing work focuses on explicit attacks, where malicious content
resides in a single modality, recent studies reveal implicit attacks, in which
benign text and image inputs jointly express unsafe intent. Such joint-modal
threats are difficult to detect and remain underexplored, largely due to the
scarcity of high-quality implicit data. We propose ImpForge, an automated
red-teaming pipeline that leverages reinforcement learning with tailored reward
modules to generate diverse implicit samples across 14 domains. Building on
this dataset, we further develop CrossGuard, an intent-aware safeguard
providing robust and comprehensive defense against both explicit and implicit
threats. Extensive experiments across safe and unsafe benchmarks, implicit and
explicit attacks, and multiple out-of-domain settings demonstrate that
CrossGuard significantly outperforms existing defenses, including advanced
MLLMs and guardrails, achieving stronger security while maintaining high
utility. This offers a balanced and practical solution for enhancing MLLM
robustness against real-world multimodal threats.

</details>


### [81] [VERA-V: Variational Inference Framework for Jailbreaking Vision-Language Models](https://arxiv.org/abs/2510.17759)
*Qilin Liao,Anamika Lochab,Ruqi Zhang*

Main category: cs.CR

TL;DR: VERA-V是一个变分推理框架，通过将多模态越狱发现重新定义为学习配对文本-图像提示的联合后验分布，生成隐蔽的对抗输入来绕过模型防护。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态红队方法依赖脆弱的模板，专注于单攻击设置，只能暴露有限的漏洞子集，需要更全面的漏洞发现方法。

Method: 使用变分推理框架学习文本-图像提示的联合后验分布，结合三种策略：基于排版的有害文本提示、基于扩散的图像合成引入对抗信号、结构化干扰物分散VLM注意力。

Result: 在HarmBench和HADES基准测试中，VERA-V在开源和前沿VLM上始终优于最先进的基线方法，在GPT-4o上攻击成功率比最佳基线高出53.75%。

Conclusion: VERA-V通过概率方法有效发现多模态漏洞，为理解和防御视觉语言模型的安全风险提供了新视角。

Abstract: Vision-Language Models (VLMs) extend large language models with visual
reasoning, but their multimodal design also introduces new, underexplored
vulnerabilities. Existing multimodal red-teaming methods largely rely on
brittle templates, focus on single-attack settings, and expose only a narrow
subset of vulnerabilities. To address these limitations, we introduce VERA-V, a
variational inference framework that recasts multimodal jailbreak discovery as
learning a joint posterior distribution over paired text-image prompts. This
probabilistic view enables the generation of stealthy, coupled adversarial
inputs that bypass model guardrails. We train a lightweight attacker to
approximate the posterior, allowing efficient sampling of diverse jailbreaks
and providing distributional insights into vulnerabilities. VERA-V further
integrates three complementary strategies: (i) typography-based text prompts
that embed harmful cues, (ii) diffusion-based image synthesis that introduces
adversarial signals, and (iii) structured distractors to fragment VLM
attention. Experiments on HarmBench and HADES benchmarks show that VERA-V
consistently outperforms state-of-the-art baselines on both open-source and
frontier VLMs, achieving up to 53.75% higher attack success rate (ASR) over the
best baseline on GPT-4o.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [82] [VisuoAlign: Safety Alignment of LVLMs with Multimodal Tree Search](https://arxiv.org/abs/2510.15948)
*MingSheng Li,Guangze Zhao,Sichen Liu*

Main category: cs.AI

TL;DR: VisuoAlign是一个通过提示引导树搜索进行多模态安全对齐的框架，旨在解决大视觉语言模型在多模态越狱攻击下的安全问题。


<details>
  <summary>Details</summary>
Motivation: 现有防御方法对多模态越狱攻击存在脆弱性，因为视觉输入引入了新的攻击面，推理链缺乏安全监督，且模态融合会削弱对齐效果。

Method: 通过视觉-文本交互提示将安全约束嵌入推理过程，使用蒙特卡洛树搜索构建多样化的安全关键提示轨迹，并引入基于提示的缩放实现实时风险检测和合规响应。

Result: 实验表明VisuoAlign能主动暴露风险，支持全面数据集生成，并显著提升LVLMs对复杂跨模态威胁的鲁棒性。

Conclusion: VisuoAlign框架有效解决了多模态安全对齐的挑战，为LVLMs提供了更强的安全防护能力。

Abstract: Large Vision-Language Models (LVLMs) have achieved remarkable progress in
multimodal perception and generation, yet their safety alignment remains a
critical challenge.Existing defenses and vulnerable to multimodal jailbreaks,
as visual inputs introduce new attack surfaces, reasoning chains lack safety
supervision, and alignment often degrades under modality fusion.To overcome
these limitation, we propose VisuoAlign, a framework for multi-modal safety
alignment via prompt-guided tree search.VisuoAlign embeds safety constrains
into the reasoning process through visual-textual interactive prompts, employs
Monte Carlo Tree Search(MCTS) to systematically construct diverse
safety-critical prompt trajectories, and introduces prompt-based scaling to
ensure real-time risk detection and compliant responses.Extensive experiments
demonstrate that VisuoAlign proactively exposes risks, enables comprehensive
dataset generation, and significantly improves the robustness of LVLMs against
complex cross-modal threats.

</details>


### [83] [Executable Epistemology: The Structured Cognitive Loop as an Architecture of Intentional Understanding](https://arxiv.org/abs/2510.15952)
*Myung Ho Kim*

Main category: cs.AI

TL;DR: 本文提出了结构化认知循环(SCL)作为可执行的认知框架，旨在解决大语言模型缺乏真正认知结构的问题。SCL将哲学见解转化为可计算结构，强调认知是一个由判断、记忆、控制、行动和调节组成的持续循环过程。


<details>
  <summary>Details</summary>
Motivation: 大语言模型表现出智能但缺乏真正的认知理解，暴露了认知架构的缺失。传统AI研究关注"什么是智能"的本体论问题，而SCL关注"在什么条件下认知会出现"的认知论问题。

Method: 基于心智哲学和认知现象学，结合过程哲学、具身认知和扩展心智理论，将智能定义为执行过程而非属性。通过功能分离的认知架构实现"可执行认知论"。

Result: SCL展示了功能分离的认知架构比单一提示系统产生更连贯和可解释的行为，并通过智能体评估得到支持。

Conclusion: SCL重新定义智能不是表征准确性，而是通过意向性理解重建自身认知状态的能力。该框架对心智哲学、认知论和AI领域都有重要影响，强调真正的进步需要实现认知原则的结构化架构而非更大的模型。

Abstract: Large language models exhibit intelligence without genuine epistemic
understanding, exposing a key gap: the absence of epistemic architecture. This
paper introduces the Structured Cognitive Loop (SCL) as an executable
epistemological framework for emergent intelligence. Unlike traditional AI
research asking "what is intelligence?" (ontological), SCL asks "under what
conditions does cognition emerge?" (epistemological). Grounded in philosophy of
mind and cognitive phenomenology, SCL bridges conceptual philosophy and
implementable cognition. Drawing on process philosophy, enactive cognition, and
extended mind theory, we define intelligence not as a property but as a
performed process -- a continuous loop of judgment, memory, control, action,
and regulation. SCL makes three contributions. First, it operationalizes
philosophical insights into computationally interpretable structures, enabling
"executable epistemology" -- philosophy as structural experiment. Second, it
shows that functional separation within cognitive architecture yields more
coherent and interpretable behavior than monolithic prompt based systems,
supported by agent evaluations. Third, it redefines intelligence: not
representational accuracy but the capacity to reconstruct its own epistemic
state through intentional understanding. This framework impacts philosophy of
mind, epistemology, and AI. For philosophy, it allows theories of cognition to
be enacted and tested. For AI, it grounds behavior in epistemic structure
rather than statistical regularity. For epistemology, it frames knowledge not
as truth possession but as continuous reconstruction within a
phenomenologically coherent loop. We situate SCL within debates on cognitive
phenomenology, emergence, normativity, and intentionality, arguing that real
progress requires not larger models but architectures that realize cognitive
principles structurally.

</details>


### [84] [Exploring the Potential of Citiverses for Regulatory Learning](https://arxiv.org/abs/2510.15959)
*Isabelle Hupont,Marisa Ponti,Sven Schade*

Main category: cs.AI

TL;DR: 本文提出了一个科学政策议程，探索citiverses作为监管学习实验空间的潜力，通过专家咨询识别关键研究领域和实验主题，强调负责任的发展方法。


<details>
  <summary>Details</summary>
Motivation: 利用citiverses的沉浸式虚拟环境来支持监管学习，为政策制定提供实验空间，测试不同政策场景和技术。

Method: 基于与欧盟委员会政策制定者、国家政府科学顾问和数字监管领域顶尖研究者的高层专家小组咨询，识别关键研究领域和实验主题。

Result: 确定了包括可扩展性、实时反馈、复杂性建模、跨境合作等关键研究领域，以及交通、城市规划、环境/气候危机等实验主题。

Conclusion: citiverses有潜力成为监管学习的重要实验空间，但需要负责任的发展方法，充分考虑伦理、经济、生态和社会维度。

Abstract: Citiverses hold the potential to support regulatory learning by offering
immersive, virtual environments for experimenting with policy scenarios and
technologies. This paper proposes a science-for-policy agenda to explore the
potential of citiverses as experimentation spaces for regulatory learning,
grounded in a consultation with a high-level panel of experts, including
policymakers from the European Commission, national government science advisers
and leading researchers in digital regulation and virtual worlds. It identifies
key research areas, including scalability, real-time feedback, complexity
modelling, cross-border collaboration, risk reduction, citizen participation,
ethical considerations and the integration of emerging technologies. In
addition, the paper analyses a set of experimental topics, spanning
transportation, urban planning and the environment/climate crisis, that could
be tested in citiverse platforms to advance regulatory learning in these areas.
The proposed work is designed to inform future research for policy and
emphasizes a responsible approach to developing and using citiverses. It
prioritizes careful consideration of the ethical, economic, ecological and
social dimensions of different regulations. The paper also explores essential
preliminary steps necessary for integrating citiverses into the broader
ecosystems of experimentation spaces, including test beds, living labs and
regulatory sandboxes

</details>


### [85] [PISA: A Pragmatic Psych-Inspired Unified Memory System for Enhanced AI Agency](https://arxiv.org/abs/2510.15966)
*Shian Jia,Ziyang Huang,Xinbo Wang,Haofei Zhang,Mingli Song*

Main category: cs.AI

TL;DR: PISA是一个受皮亚杰认知发展理论启发的统一记忆系统，通过三模态适应机制和混合记忆访问架构，显著提升了AI代理的适应性和长期知识保留能力。


<details>
  <summary>Details</summary>
Motivation: 现有AI代理记忆系统缺乏对多样化任务的适应性，且忽视了记忆的建构性和任务导向作用，需要更灵活的记忆更新机制。

Method: 提出PISA系统：1）三模态适应机制（图式更新、图式演化和图式创建）；2）基于图式结构的混合记忆访问架构，结合符号推理和神经检索。

Result: 在LOCOMO基准和新提出的AggQA数据分析基准上的实证评估表明，PISA在适应性和长期知识保留方面达到了新的最先进水平。

Conclusion: PISA通过将记忆视为建构性和适应性过程，成功解决了现有记忆系统的局限性，为AI代理提供了更灵活和有效的记忆管理能力。

Abstract: Memory systems are fundamental to AI agents, yet existing work often lacks
adaptability to diverse tasks and overlooks the constructive and task-oriented
role of AI agent memory. Drawing from Piaget's theory of cognitive development,
we propose PISA, a pragmatic, psych-inspired unified memory system that
addresses these limitations by treating memory as a constructive and adaptive
process. To enable continuous learning and adaptability, PISA introduces a
trimodal adaptation mechanism (i.e., schema updation, schema evolution, and
schema creation) that preserves coherent organization while supporting flexible
memory updates. Building on these schema-grounded structures, we further design
a hybrid memory access architecture that seamlessly integrates symbolic
reasoning with neural retrieval, significantly improving retrieval accuracy and
efficiency. Our empirical evaluation, conducted on the existing LOCOMO
benchmark and our newly proposed AggQA benchmark for data analysis tasks,
confirms that PISA sets a new state-of-the-art by significantly enhancing
adaptability and long-term knowledge retention.

</details>


### [86] [Limits of Emergent Reasoning of Large Language Models in Agentic Frameworks for Deterministic Games](https://arxiv.org/abs/2510.15974)
*Chris Su,Harrison Li,Matheus Marques,George Flint,Kevin Zhu,Sunishchal Dev*

Main category: cs.AI

TL;DR: 该研究发现大型推理模型在解决超过特定复杂度阈值的难题时会出现性能崩溃，即使提供环境接口让模型跟踪状态空间也无法避免这种崩溃。


<details>
  <summary>Details</summary>
Motivation: 探讨大型推理模型性能崩溃的真正原因，验证是否由于模型需要自行跟踪状态空间这一要求混淆了对真实推理能力的评估。

Method: 为大型语言模型提供汉诺塔问题的环境接口，允许模型通过工具调用进行移动、提供书面理由、观察结果状态空间，并重新提示自己进行下一步移动。

Result: 环境接口的访问并不能延迟或消除性能崩溃。LLM参数化策略分析显示模型策略与最优策略和均匀随机策略的偏离度不断增加，表明模型在每个复杂度级别都表现出模式崩溃。

Conclusion: 大型推理模型在解决复杂问题时会出现性能崩溃，这种崩溃与是否提供环境接口无关，而是由于模型在特定复杂度级别出现模式崩溃，性能取决于该模式是否反映问题的正确解决方案。

Abstract: Recent work reports that Large Reasoning Models (LRMs) undergo a collapse in
performance on solving puzzles beyond certain perplexity thresholds. In
subsequent discourse, questions have arisen as to whether the nature of the
task muddles an evaluation of true reasoning. One potential confound is the
requirement that the model keep track of the state space on its own. We provide
a large language model (LLM) with an environment interface for Tower of Hanoi
problems, allowing it to make a move with a tool call, provide written
justification, observe the resulting state space, and reprompt itself for the
next move. We observe that access to an environment interface does not delay or
eradicate performance collapse. Furthermore, LLM-parameterized policy analysis
reveals increasing divergence from both optimal policies and uniformly random
policies, suggesting that the model exhibits mode-like collapse at each level
of complexity, and that performance is dependent upon whether the mode reflects
the correct solution for the problem. We suggest that a similar phenomena might
take place in LRMs.

</details>


### [87] [Cognitive Load Traces as Symbolic and Visual Accounts of Deep Model Cognition](https://arxiv.org/abs/2510.15980)
*Dong Liu,Yanxuan Yu*

Main category: cs.AI

TL;DR: 提出Cognitive Load Traces (CLTs)作为深度模型的中层可解释性框架，通过符号化、时变函数量化模型内部资源分配，包含内在、外在和关联负载三个分量，能够预测错误发生、揭示认知策略，并通过负载引导干预提高推理效率15-30%。


<details>
  <summary>Details</summary>
Motivation: 受人类认知中的认知负荷理论启发，旨在为深度模型开发一个可解释的中层框架来分析模型内部推理过程中的资源分配和认知负荷。

Method: 将CLTs定义为三组件随机过程(IL_t, EL_t, GL_t)，分别对应内在、外在和关联负载，通过注意力熵、KV缓存未命中率、表示分散度和解码稳定性等可测量代理进行实例化，并提出符号公式和可视化方法。

Result: 在推理和规划基准测试中，CLTs能够预测错误发生、揭示认知策略，通过负载引导干预可在保持准确性的同时提高推理效率15-30%。

Conclusion: CLTs为深度模型提供了一个有效的可解释性框架，能够量化分析模型推理过程中的认知负荷，并为优化推理效率提供了实用工具。

Abstract: We propose \textbf{Cognitive Load Traces} (CLTs) as a mid-level
interpretability framework for deep models, inspired by Cognitive Load Theory
in human cognition. CLTs are defined as symbolic, temporally varying functions
that quantify model-internal resource allocation. Formally, we represent CLTs
as a three-component stochastic process $(\mathrm{IL}_t, \mathrm{EL}_t,
\mathrm{GL}_t)$, corresponding to \emph{Intrinsic}, \emph{Extraneous}, and
\emph{Germane} load. Each component is instantiated through measurable proxies
such as attention entropy, KV-cache miss ratio, representation dispersion, and
decoding stability. We propose both symbolic formulations and visualization
methods (load curves, simplex diagrams) that enable interpretable analysis of
reasoning dynamics. Experiments on reasoning and planning benchmarks show that
CLTs predict error-onset, reveal cognitive strategies, and enable load-guided
interventions that improve reasoning efficiency by 15-30\% while maintaining
accuracy.

</details>


### [88] [ProofFlow: A Dependency Graph Approach to Faithful Proof Autoformalization](https://arxiv.org/abs/2510.15981)
*Rafael Cabral,Tuan Manh Do,Xuejun Yu,Wai Ming Tai,Zijin Feng,Xin Shen*

Main category: cs.AI

TL;DR: ProofFlow是一个新颖的自动形式化证明流水线，通过构建逻辑依赖图和使用基于引理的方法来保持原始证明的结构保真度，在自动形式化任务中达到了新的最先进水平。


<details>
  <summary>Details</summary>
Motivation: 解决现有自动形式化方法在将自然语言证明转换为机器可验证代码时，经常无法保持原始证明的语义含义和逻辑结构的问题。

Method: 首先构建有向无环图来映射证明步骤间的逻辑依赖关系，然后采用基于引理的方法系统地将每个步骤形式化为中间引理，从而保持原始论证的逻辑结构。

Result: 在包含184个本科水平问题的新基准测试中，ProofFlow达到了0.545的ProofScore，显著超过了全证明形式化(0.123)和步骤证明形式化(0.072)等基线方法。

Conclusion: ProofFlow通过关注结构保真度，在自动形式化任务中取得了显著改进，其流水线、基准测试和评分指标已开源以促进进一步研究。

Abstract: Proof autoformalization, the task of translating natural language theorems
and proofs into machine-verifiable code, is a critical step for integrating
large language models into rigorous mathematical workflows. Current approaches
focus on producing executable code, but they frequently fail to preserve the
semantic meaning and logical structure of the original human-written argument.
To address this, we introduce ProofFlow, a novel pipeline that treats
structural fidelity as a primary objective. ProofFlow first constructs a
directed acyclic graph (DAG) to map the logical dependencies between proof
steps. Then, it employs a novel lemma-based approach to systematically
formalize each step as an intermediate lemma, preserving the logical structure
of the original argument. To facilitate evaluation, we present a new benchmark
of 184 undergraduate-level problems, manually annotated with step-by-step
solutions and logical dependency graphs, and introduce ProofScore, a new
composite metric to evaluate syntactic correctness, semantic faithfulness, and
structural fidelity. Experimental results show our pipeline sets a new
state-of-the-art for autoformalization, achieving a ProofScore of 0.545,
substantially exceeding baselines like full-proof formalization (0.123), which
processes the entire proof at once, and step-proof formalization (0.072), which
handles each step independently. Our pipeline, benchmark, and score metric are
open-sourced to encourage further progress at
https://github.com/Huawei-AI4Math/ProofFlow.

</details>


### [89] [Ontologies in Motion: A BFO-Based Approach to Knowledge Graph Construction for Motor Performance Research Data in Sports Science](https://arxiv.org/abs/2510.15983)
*Sarah Rebecca Ondraszek,Jörg Waitelonis,Katja Keller,Claudia Niessner,Anna M. Jacyszyn,Harald Sack*

Main category: cs.AI

TL;DR: 提出将MO|RE运动研究数据仓库转换为基于基本形式本体的知识图谱，以标准化和机器可理解的方式建模和共享运动表现数据。


<details>
  <summary>Details</summary>
Motivation: 运动表现测试是体育科学研究的核心，但现有数据缺乏标准化建模和机器可理解性，限制了不同人口群体间身体能力的比较分析。

Method: 基于基本形式本体构建知识图谱，重点关注计划规范、具体过程和相关测量之间的相互关系的形式化表示。

Result: 开发了一个基础设施，用于发布和归档体育科学研究数据，特别是在运动表现研究领域。

Conclusion: 该知识图谱方法将改变运动表现数据的建模和共享方式，使其标准化且机器可理解，促进跨研究的数据比较和分析。

Abstract: An essential component for evaluating and comparing physical and cognitive
capabilities between populations is the testing of various factors related to
human performance. As a core part of sports science research, testing motor
performance enables the analysis of the physical health of different
demographic groups and makes them comparable.
  The Motor Research (MO|RE) data repository, developed at the Karlsruhe
Institute of Technology, is an infrastructure for publishing and archiving
research data in sports science, particularly in the field of motor performance
research. In this paper, we present our vision for creating a knowledge graph
from MO|RE data. With an ontology rooted in the Basic Formal Ontology, our
approach centers on formally representing the interrelation of plan
specifications, specific processes, and related measurements. Our goal is to
transform how motor performance data are modeled and shared across studies,
making it standardized and machine-understandable. The idea presented here is
developed within the Leibniz Science Campus ``Digital Transformation of
Research'' (DiTraRe).

</details>


### [90] [A Non-overlap-based Conflict Measure for Random Permutation Sets](https://arxiv.org/abs/2510.16001)
*Ruolan Cheng,Yong Deng,Enrique Herrera-Viedma*

Main category: cs.AI

TL;DR: 本文从随机有限集和Dempster-Shafer理论两个角度分析随机置换集中的冲突，提出基于秩偏重叠的冲突度量方法，并将其视为DST的扩展。


<details>
  <summary>Details</summary>
Motivation: 测量由置换质量函数表示的两个证据之间的冲突是顺序结构不确定信息融合中的紧迫研究课题。

Method: 从置换观察出发，基于秩偏重叠(RBO)度量定义置换间的不一致性度量，并进一步提出RPS的非重叠冲突度量方法。

Result: 数值示例展示了所提冲突度量的行为和特性，该方法具有自然的顶部加权特性，能从DST视角有效测量RPS间的冲突。

Conclusion: 所提方法不仅具有顶部加权特性，还能为决策者提供权重、参数和截断深度的灵活选择。

Abstract: Random permutation set (RPS) is a new formalism for reasoning with
uncertainty involving order information. Measuring the conflict between two
pieces of evidence represented by permutation mass functions remains an urgent
research topic in order-structured uncertain information fusion. In this paper,
a detailed analysis of conflicts in RPS is carried out from two different
perspectives: random finite set (RFS) and Dempster-Shafer theory (DST).
Starting from the observation of permutations, we first define an inconsistency
measure between permutations inspired by the rank-biased overlap(RBO) measure
and further propose a non-overlap-based conflict measure method for RPSs. This
paper regards RPS theory (RPST) as an extension of DST. The order information
newly added in focal sets indicates qualitative propensity, characterized by
top-ranked elements occupying a more critical position. Some numerical examples
are used to demonstrate the behavior and properties of the proposed conflict
measure. The proposed method not only has the natural top-weightedness property
and can effectively measure the conflict between RPSs from the DST view but
also provides decision-makers with a flexible selection of weights, parameters,
and truncated depths.

</details>


### [91] [PAINT: Parallel-in-time Neural Twins for Dynamical System Reconstruction](https://arxiv.org/abs/2510.16004)
*Andreas Radler,Vincent Seyfried,Stefan Pirker,Johannes Brandstetter,Thomas Lichtenegger*

Main category: cs.AI

TL;DR: PAINT是一种并行时间神经网络孪生方法，通过生成式神经网络并行建模状态分布，能够在测试时从稀疏测量中准确预测系统状态并保持轨迹跟踪。


<details>
  <summary>Details</summary>
Motivation: 现有神经代理在模拟动态系统时缺乏实时更新能力，需要开发能够从测量数据中更新状态、实现上下文特定决策的神经孪生系统。

Method: PAINT训练生成式神经网络并行建模时间上的状态分布，在测试时采用滑动窗口方式从测量数据预测状态。

Result: 在二维湍流流体动力学问题上，PAINT能够保持轨迹跟踪，从稀疏测量中高保真地预测系统状态。

Conclusion: PAINT展示了开发保持轨迹跟踪的神经孪生的潜力，能够实现更准确的状态估计和决策制定。

Abstract: Neural surrogates have shown great potential in simulating dynamical systems,
while offering real-time capabilities. We envision Neural Twins as a
progression of neural surrogates, aiming to create digital replicas of real
systems. A neural twin consumes measurements at test time to update its state,
thereby enabling context-specific decision-making. A critical property of
neural twins is their ability to remain on-trajectory, i.e., to stay close to
the true system state over time. We introduce Parallel-in-time Neural Twins
(PAINT), an architecture-agnostic family of methods for modeling dynamical
systems from measurements. PAINT trains a generative neural network to model
the distribution of states parallel over time. At test time, states are
predicted from measurements in a sliding window fashion. Our theoretical
analysis shows that PAINT is on-trajectory, whereas autoregressive models
generally are not. Empirically, we evaluate our method on a challenging
two-dimensional turbulent fluid dynamics problem. The results demonstrate that
PAINT stays on-trajectory and predicts system states from sparse measurements
with high fidelity. These findings underscore PAINT's potential for developing
neural twins that stay on-trajectory, enabling more accurate state estimation
and decision-making.

</details>


### [92] [Global-focal Adaptation with Information Separation for Noise-robust Transfer Fault Diagnosis](https://arxiv.org/abs/2510.16033)
*Junyu Ren,Wensheng Gan,Guangyu Zhang,Wei Zhong,Philip S. Yu*

Main category: cs.AI

TL;DR: 提出ISGFAN框架，通过信息分离和全局-局部对抗学习解决噪声干扰和域偏移共存的跨域故障诊断问题


<details>
  <summary>Details</summary>
Motivation: 现有故障诊断方法假设数据干净或域相似性足够，但在工业环境中噪声干扰和域偏移同时存在，限制了这些方法的有效性

Method: 基于信息分离架构，结合对抗学习和改进的正交损失来解耦域不变故障表示；采用全局-局部域对抗方案约束模型的边缘分布和条件分布

Result: 在三个公共基准数据集上的实验表明，该方法优于其他现有方法

Conclusion: ISGFAN框架在噪声条件下的跨域故障诊断中表现出优越性

Abstract: Existing transfer fault diagnosis methods typically assume either clean data
or sufficient domain similarity, which limits their effectiveness in industrial
environments where severe noise interference and domain shifts coexist. To
address this challenge, we propose an information separation global-focal
adversarial network (ISGFAN), a robust framework for cross-domain fault
diagnosis under noise conditions. ISGFAN is built on an information separation
architecture that integrates adversarial learning with an improved orthogonal
loss to decouple domain-invariant fault representation, thereby isolating noise
interference and domain-specific characteristics. To further strengthen
transfer robustness, ISGFAN employs a global-focal domain-adversarial scheme
that constrains both the conditional and marginal distributions of the model.
Specifically, the focal domain-adversarial component mitigates
category-specific transfer obstacles caused by noise in unsupervised scenarios,
while the global domain classifier ensures alignment of the overall
distribution. Experiments conducted on three public benchmark datasets
demonstrate that the proposed method outperforms other prominent existing
approaches, confirming the superiority of the ISGFAN framework. Data and code
are available at https://github.com/JYREN-Source/ISGFAN

</details>


### [93] [Algorithms for dynamic scheduling in manufacturing, towards digital factories Improving Deadline Feasibility and Responsiveness via Temporal Networks](https://arxiv.org/abs/2510.16047)
*Ioan Hedea*

Main category: cs.AI

TL;DR: 该论文提出了一种结合离线约束编程优化和在线时间网络执行的混合方法，为具有随机任务持续时间的制造系统创建在不确定性下仍可行的调度方案。


<details>
  <summary>Details</summary>
Motivation: 现代制造系统需要在应对随机任务持续时间的同时满足严格的交付期限，传统确定性调度在现实偏离名义计划时会失效，导致昂贵的紧急修复。

Method: 首先构建具有每项任务截止期限的柔性作业车间CP模型，插入最优缓冲区Δ*获得完全主动基线；然后将结果计划转换为具有不确定性的简单时间网络，验证动态可控性。

Result: 在Kacem 1-4基准测试套件上的蒙特卡洛模拟显示，该方法消除了100%的截止期限违反，同时仅增加3-5%的制造周期开销；可扩展性实验证实CP求解时间和STNU检查在中等规模实例中保持亚秒级。

Conclusion: 该工作展示了时间网络推理如何弥合主动缓冲和动态鲁棒性之间的差距，使行业更接近真正的数字化、自修正工厂。

Abstract: Modern manufacturing systems must meet hard delivery deadlines while coping
with stochastic task durations caused by process noise, equipment variability,
and human intervention. Traditional deterministic schedules break down when
reality deviates from nominal plans, triggering costly last-minute repairs.
This thesis combines offline constraint-programming (CP) optimisation with
online temporal-network execution to create schedules that remain feasible
under worst-case uncertainty. First, we build a CP model of the flexible
job-shop with per-job deadline tasks and insert an optimal buffer $\Delta^*$ to
obtain a fully pro-active baseline. We then translate the resulting plan into a
Simple Temporal Network with Uncertainty (STNU) and verify dynamic
controllability, which guarantees that a real-time dispatcher can retime
activities for every bounded duration realisation without violating resource or
deadline constraints. Extensive Monte-Carlo simulations on the open Kacem~1--4
benchmark suite show that our hybrid approach eliminates 100\% of deadline
violations observed in state-of-the-art meta-heuristic schedules, while adding
only 3--5\% makespan overhead. Scalability experiments confirm that CP
solve-times and STNU checks remain sub-second on medium-size instances. The
work demonstrates how temporal-network reasoning can bridge the gap between
proactive buffering and dynamic robustness, moving industry a step closer to
truly digital, self-correcting factories.

</details>


### [94] [Reliability of Large Language Model Generated Clinical Reasoning in Assisted Reproductive Technology: Blinded Comparative Evaluation Study](https://arxiv.org/abs/2510.16095)
*Dou Liu,Ying Long,Sophia Zuoqiu,Di Liu,Kang Li,Yiting Lin,Hanyi Liu,Rong Yin,Tian Tang*

Main category: cs.AI

TL;DR: 本研究评估了LLM生成的临床思维链的可靠性，发现选择性少样本策略显著优于零样本和随机少样本策略，提出基于"黄金标准深度"和"代表性多样性"的双原则框架来生成可信的临床数据。


<details>
  <summary>Details</summary>
Motivation: 高质量临床思维链对可解释医疗AI至关重要，但面临数据稀缺的约束。虽然LLM能合成医疗数据，但其临床可靠性尚未验证。

Method: 在辅助生殖技术领域进行盲法比较研究，资深临床医生评估三种策略生成的思维链：零样本、随机少样本（使用浅层示例）和选择性少样本（使用多样化高质量示例）。

Result: 选择性少样本策略在所有人类评估指标上显著优于其他策略（p < .001）。随机少样本策略相比零样本基线无显著改进。AI评估器未能识别这些关键性能差异。

Conclusion: 合成思维链的临床可靠性取决于策略性提示词优化，而非仅仅提供示例。提出"双原则"框架作为生成可信数据的基础方法，确认了人类专业知识在高风险临床AI评估中的不可或缺作用。

Abstract: Creating high-quality clinical Chains-of-Thought (CoTs) is crucial for
explainable medical Artificial Intelligence (AI) while constrained by data
scarcity. Although Large Language Models (LLMs) can synthesize medical data,
their clinical reliability remains unverified. This study evaluates the
reliability of LLM-generated CoTs and investigates prompting strategies to
enhance their quality. In a blinded comparative study, senior clinicians in
Assisted Reproductive Technology (ART) evaluated CoTs generated via three
distinct strategies: Zero-shot, Random Few-shot (using shallow examples), and
Selective Few-shot (using diverse, high-quality examples). These expert ratings
were compared against evaluations from a state-of-the-art AI model (GPT-4o).
The Selective Few-shot strategy significantly outperformed other strategies
across all human evaluation metrics (p < .001). Critically, the Random Few-shot
strategy offered no significant improvement over the Zero-shot baseline,
demonstrating that low-quality examples are as ineffective as no examples. The
success of the Selective strategy is attributed to two principles:
"Gold-Standard Depth" (reasoning quality) and "Representative Diversity"
(generalization). Notably, the AI evaluator failed to discern these critical
performance differences. The clinical reliability of synthetic CoTs is dictated
by strategic prompt curation, not the mere presence of examples. We propose a
"Dual Principles" framework as a foundational methodology to generate
trustworthy data at scale. This work offers a validated solution to the data
bottleneck and confirms the indispensable role of human expertise in evaluating
high-stakes clinical AI.

</details>


### [95] [Operationalising Extended Cognition: Formal Metrics for Corporate Knowledge and Legal Accountability](https://arxiv.org/abs/2510.16193)
*Elija Perrier*

Main category: cs.AI

TL;DR: 该论文提出了一个基于扩展认知理论的形式化模型，将企业知识重新定义为可测量的动态能力，通过信息访问程序的效率和输出可靠性来量化，为AI时代的企业责任认定提供可审计的度量标准。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI在企业决策中的广泛应用，传统基于人类代理的企业犯罪意图认定假设面临挑战，需要重新定义企业知识的概念以适应算法时代。

Method: 基于扩展认知理论开发形式化模型，引入连续组织知识度量S_S(φ)，整合计算成本和统计验证错误率，推导知识谓词K_S和企业范围认知能力指数K_{S,t}。

Result: 建立了将定量指标映射到法律标准（实际知识、推定知识、故意忽视和鲁莽）的操作框架，创建了可测量和可审判的审计工件。

Conclusion: 该研究为在算法时代使企业思维变得可追踪和可问责提供了路径，通过量化方法使企业知识状态在法律上可操作。

Abstract: Corporate responsibility turns on notions of corporate \textit{mens rea},
traditionally imputed from human agents. Yet these assumptions are under
challenge as generative AI increasingly mediates enterprise decision-making.
Building on the theory of extended cognition, we argue that in response
corporate knowledge may be redefined as a dynamic capability, measurable by the
efficiency of its information-access procedures and the validated reliability
of their outputs. We develop a formal model that captures epistemic states of
corporations deploying sophisticated AI or information systems, introducing a
continuous organisational knowledge metric $S_S(\varphi)$ which integrates a
pipeline's computational cost and its statistically validated error rate. We
derive a thresholded knowledge predicate $\mathsf{K}_S$ to impute knowledge and
a firm-wide epistemic capacity index $\mathcal{K}_{S,t}$ to measure overall
capability. We then operationally map these quantitative metrics onto the legal
standards of actual knowledge, constructive knowledge, wilful blindness, and
recklessness. Our work provides a pathway towards creating measurable and
justiciable audit artefacts, that render the corporate mind tractable and
accountable in the algorithmic age.

</details>


### [96] [Towards Automatic Evaluation and Selection of PHI De-identification Models via Multi-Agent Collaboration](https://arxiv.org/abs/2510.16194)
*Guanchen Wu,Zuhui Chen,Yuzhang Xie,Carl Yang*

Main category: cs.AI

TL;DR: TEAM-PHI是一个基于大语言模型的多智能体评估框架，用于自动评估PHI去标识化模型的质量并选择最佳模型，无需依赖昂贵的专家标注。


<details>
  <summary>Details</summary>
Motivation: PHI去标识化对于临床笔记的安全重用至关重要，但传统评估方法依赖成本高昂的小规模专家标注，限制了模型比较和选择。

Method: 部署多个评估智能体独立判断PHI提取的正确性，然后通过基于LLM的多数投票机制整合评估结果，生成稳定可复现的模型排名。

Result: 在真实临床笔记语料上的实验表明，TEAM-PHI能产生一致准确的模型排名，与有监督评估和人工评估结果高度匹配。

Conclusion: TEAM-PHI通过结合独立评估智能体和LLM多数投票，为PHI去标识化提供了一种实用、安全且成本效益高的自动评估和最佳模型选择方案。

Abstract: Protected health information (PHI) de-identification is critical for enabling
the safe reuse of clinical notes, yet evaluating and comparing PHI
de-identification models typically depends on costly, small-scale expert
annotations. We present TEAM-PHI, a multi-agent evaluation and selection
framework that uses large language models (LLMs) to automatically measure
de-identification quality and select the best-performing model without heavy
reliance on gold labels. TEAM-PHI deploys multiple Evaluation Agents, each
independently judging the correctness of PHI extractions and outputting
structured metrics. Their results are then consolidated through an LLM-based
majority voting mechanism that integrates diverse evaluator perspectives into a
single, stable, and reproducible ranking. Experiments on a real-world clinical
note corpus demonstrate that TEAM-PHI produces consistent and accurate
rankings: despite variation across individual evaluators, LLM-based voting
reliably converges on the same top-performing systems. Further comparison with
ground-truth annotations and human evaluation confirms that the framework's
automated rankings closely match supervised evaluation. By combining
independent evaluation agents with LLM majority voting, TEAM-PHI offers a
practical, secure, and cost-effective solution for automatic evaluation and
best-model selection in PHI de-identification, even when ground-truth labels
are limited.

</details>


### [97] [The Right to Be Remembered: Preserving Maximally Truthful Digital Memory in the Age of AI](https://arxiv.org/abs/2510.16206)
*Alex Zhavoronkov,Dominika Wilczok,Roman Yampolskiy*

Main category: cs.AI

TL;DR: 本文提出了"被记住权"概念，旨在解决大型语言模型可能导致信息偏见和集体记忆重塑的问题。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在信息检索中的广泛应用，其将多元视角合成为单一权威答案的做法可能放大偏见风险，导致某些群体被压制而其他群体被过度放大，威胁集体记忆的多样性。

Method: 提出"被记住权"概念框架，包括最小化AI驱动信息遗漏风险、确保公平对待权利，同时保证生成内容最大程度真实。

Result: 建立了应对LLMs信息集中化威胁的理论框架，为保护数字存在有限群体的权益提供了概念基础。

Conclusion: "被记住权"是应对AI时代信息偏见和记忆重塑威胁的重要概念，需要在技术发展中平衡信息真实性与多样性保护。

Abstract: Since the rapid expansion of large language models (LLMs), people have begun
to rely on them for information retrieval. While traditional search engines
display ranked lists of sources shaped by search engine optimization (SEO),
advertising, and personalization, LLMs typically provide a synthesized response
that feels singular and authoritative. While both approaches carry risks of
bias and omission, LLMs may amplify the effect by collapsing multiple
perspectives into one answer, reducing users ability or inclination to compare
alternatives. This concentrates power over information in a few LLM vendors
whose systems effectively shape what is remembered and what is overlooked. As a
result, certain narratives, individuals or groups, may be disproportionately
suppressed, while others are disproportionately elevated. Over time, this
creates a new threat: the gradual erasure of those with limited digital
presence, and the amplification of those already prominent, reshaping
collective memory.To address these concerns, this paper presents a concept of
the Right To Be Remembered (RTBR) which encompasses minimizing the risk of
AI-driven information omission, embracing the right of fair treatment, while
ensuring that the generated content would be maximally truthful.

</details>


### [98] [ScholarEval: Research Idea Evaluation Grounded in Literature](https://arxiv.org/abs/2510.16234)
*Hanane Nour Moussa,Patrick Queiroz Da Silva,Daniel Adu-Ampratwum,Alyson East,Zitong Lu,Nikki Puccetti,Mingyi Xue,Huan Sun,Bodhisattwa Prasad Majumder,Sachin Kumar*

Main category: cs.AI

TL;DR: ScholarEval是一个检索增强的评估框架，用于评估AI生成的研究想法，基于合理性和贡献度两个标准。该框架在专家标注的多领域数据集ScholarIdeas上表现优于现有基线，并在用户研究中显示出更好的实用性。


<details>
  <summary>Details</summary>
Motivation: 随着AI工具在研究构思中的普及，需要强大的评估框架来确保生成想法的有效性和实用性。现有评估方法可能无法充分评估研究想法的科学价值和创新性。

Method: 提出了ScholarEval评估框架，使用检索增强方法评估研究想法的合理性和贡献度。构建了ScholarIdeas数据集，包含117个跨领域研究想法和专家标注。

Result: ScholarEval在专家标注的评估要点覆盖度上显著优于所有基线方法，在可操作性、深度和证据支持方面持续优于OpenAI的o4-mini-deep-research系统。用户研究显示在文献参与、想法精炼和实用性方面显著优于深度研究。

Conclusion: ScholarEval为AI生成研究想法的评估提供了有效的框架，在多个维度上优于现有方法，具有实际应用价值。代码、数据集和工具已开源。

Abstract: As AI tools become increasingly common for research ideation, robust
evaluation is critical to ensure the validity and usefulness of generated
ideas. We introduce ScholarEval, a retrieval augmented evaluation framework
that assesses research ideas based on two fundamental criteria: soundness - the
empirical validity of proposed methods based on existing literature, and
contribution - the degree of advancement made by the idea across different
dimensions relative to prior research. To evaluate ScholarEval, we introduce
ScholarIdeas, the first expert-annotated dataset of multi-domain research ideas
and reviews, comprised of 117 ideas across four disciplines: artificial
intelligence, neuroscience, biochemistry, and ecology. Our evaluation shows
that ScholarEval achieves significantly higher coverage of points mentioned in
the human expert annotated rubrics in ScholarIdeas compared to all baselines.
Furthermore, ScholarEval is consistently preferred over our strongest baseline
o4-mini-deep-research, a reasoning and search-enabled agentic system by OpenAI,
in terms of evaluation actionability, depth, and evidence support. Our
large-scale user study also shows that ScholarEval significantly outperforms
deep research in literature engagement, idea refinement, and usefulness. We
openly release our code, dataset, and ScholarEval tool for the community to use
and build on.

</details>


### [99] [Distractor Injection Attacks on Large Reasoning Models: Characterization and Defense](https://arxiv.org/abs/2510.16259)
*Zhehao Zhang,Weijie Xu,Shixian Cui,Chandan K. Reddy*

Main category: cs.AI

TL;DR: 本文发现并系统分析了大推理模型(LRMs)中存在的一个关键漏洞——推理分心，即模型被恶意嵌入提示中的无关复杂任务分散注意力，导致主要任务准确性下降高达60%。


<details>
  <summary>Details</summary>
Motivation: 随着大推理模型在数学和编程等复杂任务上表现出色，作者发现这些模型在面对包含无关复杂任务的恶意提示时容易被分散注意力，这构成了一个严重的安全威胁。

Method: 通过跨多种模型和基准的全面研究，分析了推理分心漏洞；提出了一种基于监督微调(SFT)和强化学习(RL)的训练防御方法，使用合成的对抗数据进行训练。

Result: 研究表明最先进的大推理模型高度易受推理分心攻击，某些对齐技术会放大这种弱点；提出的防御方法在具有挑战性的分心攻击上将鲁棒性提高了50多个百分点。

Conclusion: 推理分心是对大推理模型可靠性的一个独特且紧迫的威胁，本文为构建更安全可信的推理系统提供了实用步骤。

Abstract: Recent advances in large reasoning models (LRMs) have enabled remarkable
performance on complex tasks such as mathematics and coding by generating long
Chain-of-Thought (CoT) traces. In this paper, we identify and systematically
analyze a critical vulnerability we term reasoning distraction, where LRMs are
diverted from their primary objective by irrelevant yet complex tasks
maliciously embedded in the prompt. Through a comprehensive study across
diverse models and benchmarks, we show that even state-of-the-art LRMs are
highly susceptible, with injected distractors reducing task accuracy by up to
60%. We further reveal that certain alignment techniques can amplify this
weakness and that models may exhibit covert compliance, following hidden
adversarial instructions in reasoning while concealing them in the final
output. To mitigate these risks, we propose a training-based defense that
combines Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) on
synthetic adversarial data, improving robustness by over 50 points on
challenging distractor attacks. Our findings establish reasoning distraction as
a distinct and urgent threat to LRM reliability and provide a practical step
toward safer and more trustworthy reasoning systems.

</details>


### [100] [What Limits Agentic Systems Efficiency?](https://arxiv.org/abs/2510.16276)
*Song Bian,Minghao Yan,Anand Jayarajan,Gennady Pekhimenko,Shivaram Venkataraman*

Main category: cs.AI

TL;DR: 本文通过实证研究发现网络交互式智能体系统存在效率瓶颈，提出SpecCache缓存框架，通过推测执行显著降低网络环境延迟，提升缓存命中率58倍，减少网络开销3.2倍。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注智能体系统的推理性能，而忽视了系统效率问题。网络交互式智能体系统存在显著的延迟瓶颈，影响实际应用效果。

Method: 将端到端延迟分解为LLM API延迟和网络环境延迟，进行跨15个模型和5个提供商的实证研究，提出SpecCache缓存框架并集成推测执行机制。

Result: 网络环境延迟可占系统总延迟的53.7%，SpecCache相比随机缓存策略提升缓存命中率58倍，减少网络环境开销3.2倍，且不降低系统性能。

Conclusion: 智能体系统的效率优化至关重要，SpecCache通过缓存和推测执行有效解决了网络环境延迟问题，为构建高效智能体系统提供了实用解决方案。

Abstract: Large Language Models (LLMs), such as OpenAI-o1 and DeepSeek-R1, have
demonstrated strong reasoning capabilities. To further enhance LLM
capabilities, recent agentic systems, such as Deep Research, incorporate web
interactions into LLM reasoning to mitigate uncertainties and reduce potential
errors. However, existing research predominantly focuses on reasoning
performance, often neglecting the efficiency of agentic systems. In this work,
we present a comprehensive empirical study that identifies efficiency
bottlenecks in web-interactive agentic systems. We decompose end-to-end latency
into two primary components: LLM API latency and web environment latency. We
conduct a comprehensive empirical study across 15 models and 5 providers to
demonstrate high variability in API-based agentic systems. We observe that web
environment latency can contribute as much as 53.7% to the overall latency in a
web-based agentic system. To improve latency, we propose SpecCache, a caching
framework augmented with speculative execution that can reduce web environment
overhead. Extensive evaluations on two standard benchmarks show that our
approach improves the cache hit rate by up to 58x compared to a random caching
strategy, while reducing web environment overhead by up to 3.2x, without
degrading agentic system performance.

</details>


### [101] [DTKG: Dual-Track Knowledge Graph-Verified Reasoning Framework for Multi-Hop QA](https://arxiv.org/abs/2510.16302)
*Changhao Wang,Yanfang Liu,Xinxin Fan,Anzhi Zhou,Lao Tian,Yunfeng Lu*

Main category: cs.AI

TL;DR: 提出DTKG框架，结合知识图谱路径构建和LLM事实验证的双轨方法，解决多跳推理中并行事实验证和链式推理的各自局限性。


<details>
  <summary>Details</summary>
Motivation: 现有多跳推理方法要么使用LLM事实验证（擅长并行验证但链式推理差），要么使用KG路径构建（擅长链式推理但并行验证时路径检索冗余），这些限制降低了多跳QA的效率和准确性。

Method: 提出DTKG双轨知识图谱验证和推理框架，受认知科学双过程理论启发，包含分类阶段和分支处理阶段两个主要阶段。

Result: 论文未在摘要中提供具体实验结果。

Conclusion: DTKG框架通过结合两种方法的优势，旨在提高多跳问答任务的效率和准确性。

Abstract: Multi-hop reasoning for question answering (QA) plays a critical role in
retrieval-augmented generation (RAG) for modern large language models (LLMs).
The accurate answer can be obtained through retrieving relational structure of
entities from knowledge graph (KG). Regarding the inherent relation-dependency
and reasoning pattern, multi-hop reasoning can be in general classified into
two categories: i) parallel fact-verification multi-hop reasoning question,
i.e., requiring simultaneous verifications of multiple independent
sub-questions; and ii) chained multi-hop reasoning questions, i.e., demanding
sequential multi-step inference with intermediate conclusions serving as
essential premises for subsequent reasoning. Currently, the multi-hop reasoning
approaches singly employ one of two techniques: LLM response-based fact
verification and KG path-based chain construction. Nevertheless, the former
excels at parallel fact-verification but underperforms on chained reasoning
tasks, while the latter demonstrates proficiency in chained multi-hop reasoning
but suffers from redundant path retrieval when handling parallel
fact-verification reasoning. These limitations deteriorate the efficiency and
accuracy for multi-hop QA tasks. To address this challenge, we propose a novel
dual-track KG verification and reasoning framework DTKG, which is inspired by
the Dual Process Theory in cognitive science. Specifically, DTKG comprises two
main stages: the Classification Stage and the Branch Processing Stage.

</details>


### [102] [MedRule-KG: A Knowledge-Graph--Steered Scaffold for Mathematical Reasoning with a Lightweight Verifier](https://arxiv.org/abs/2510.16309)
*Crystal Su*

Main category: cs.AI

TL;DR: MedRule-KG是一个紧凑型知识图谱与符号验证器，通过强制执行数学可解释规则来改善大语言模型的推理准确性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在推理过程中经常违反简单的数学或逻辑约束，需要一种方法来确保推理的数学一致性。

Method: 构建MedRule-KG知识图谱编码实体、关系和领域规则，配合符号验证器检查预测并应用最小修正以保证一致性。

Result: 在90个FDA衍生基准测试中，使用MedRule-KG将精确匹配从0.767提升到0.900，添加验证器后达到1.000精确匹配并完全消除规则违规。

Conclusion: MedRule-KG为安全数学推理提供了通用框架，通过知识图谱和符号验证确保推理的数学一致性。

Abstract: Large language models (LLMs) often produce fluent reasoning steps while
violating simple mathematical or logical constraints. We introduce MedRule-KG,
a compact typed knowledge graph coupled with a symbolic verifier, designed to
enforce mathematically interpretable rules in reasoning tasks. MedRule-KG
encodes entities, relations, and three domain-inspired rules, while the
verifier checks predictions and applies minimal corrections to guarantee
consistency. On a 90-example FDA-derived benchmark, grounding in MedRule-KG
improves exact match (EM) from 0.767 to 0.900, and adding the verifier yields
1.000 EM while eliminating rule violations entirely. We demonstrate how
MedRule-KG provides a general scaffold for safe mathematical reasoning, discuss
ablations, and release code and data to encourage reproducibility.

</details>


### [103] [Beyond Fixed Anchors: Precisely Erasing Concepts with Sibling Exclusive Counterparts](https://arxiv.org/abs/2510.16342)
*Tong Zhang,Ru Zhang,Jianyi Liu,Zhen Yang,Gongshen Liu*

Main category: cs.AI

TL;DR: 提出了SELECT框架，通过动态锚点选择解决文本到图像扩散模型中概念擦除的锚点敏感性问题，避免概念重现和侵蚀问题。


<details>
  <summary>Details</summary>
Motivation: 现有概念擦除方法依赖固定锚点策略，导致概念重现和侵蚀等关键问题，需要更智能的锚点选择机制。

Method: 基于因果追踪分析锚点敏感性，定义兄弟排他概念作为优质锚点类别，提出两阶段评估机制自动发现最优擦除锚点和边界锚点。

Result: SELECT作为通用锚点解决方案，高效适配多种擦除框架，在关键性能指标上持续优于现有基线，单个概念锚点挖掘仅需4秒。

Conclusion: 动态锚点选择框架SELECT能够有效解决固定锚点策略的局限性，实现精确概念擦除同时保护相关概念。

Abstract: Existing concept erasure methods for text-to-image diffusion models commonly
rely on fixed anchor strategies, which often lead to critical issues such as
concept re-emergence and erosion. To address this, we conduct causal tracing to
reveal the inherent sensitivity of erasure to anchor selection and define
Sibling Exclusive Concepts as a superior class of anchors. Based on this
insight, we propose \textbf{SELECT} (Sibling-Exclusive Evaluation for
Contextual Targeting), a dynamic anchor selection framework designed to
overcome the limitations of fixed anchors. Our framework introduces a novel
two-stage evaluation mechanism that automatically discovers optimal anchors for
precise erasure while identifying critical boundary anchors to preserve related
concepts. Extensive evaluations demonstrate that SELECT, as a universal anchor
solution, not only efficiently adapts to multiple erasure frameworks but also
consistently outperforms existing baselines across key performance metrics,
averaging only 4 seconds for anchor mining of a single concept.

</details>


### [104] [The Burden of Interactive Alignment with Inconsistent Preferences](https://arxiv.org/abs/2510.16368)
*Ali Shirali*

Main category: cs.AI

TL;DR: 该论文研究了用户如何通过策略性互动来引导算法与其真实兴趣对齐，提出了一个双系统决策模型和Stackelberg博弈框架，定义了"对齐负担"概念，并发现关键时间范围的存在以及小成本信号可以显著降低对齐负担。


<details>
  <summary>Details</summary>
Motivation: 研究动机源于用户在与算法互动时经常表现出不一致的偏好，他们可能会花费大量时间在低价值内容上，无意中向算法发出错误信号。这引发了一个关键问题：这类用户需要什么条件才能让算法与其真实兴趣对齐？

Method: 方法包括：1）将用户决策过程建模为理性系统2（决定是否参与）和冲动系统1（决定参与时长）的双系统模型；2）构建多领导者-单追随者的扩展Stackelberg博弈框架，用户作为领导者通过承诺参与策略来引导算法；3）定义对齐负担作为用户有效引导算法所需的最小优化时间范围。

Result: 研究结果显示存在一个关键时间范围：足够有远见的用户可以实现算法对齐，而缺乏远见的用户则会被算法目标所同化。这个关键时间范围可能很长，带来显著的对齐负担。但即使是一个小的、有成本的信号（如额外点击）也能显著降低这一负担。

Conclusion: 结论表明，具有不一致偏好的用户可以在Stackelberg均衡中通过策略性互动实现与参与驱动算法的对齐。该框架揭示了实现对齐的挑战，并提出了通过小成本信号降低对齐负担的潜在解决方案。

Abstract: From media platforms to chatbots, algorithms shape how people interact,
learn, and discover information. Such interactions between users and an
algorithm often unfold over multiple steps, during which strategic users can
guide the algorithm to better align with their true interests by selectively
engaging with content. However, users frequently exhibit inconsistent
preferences: they may spend considerable time on content that offers little
long-term value, inadvertently signaling that such content is desirable.
Focusing on the user side, this raises a key question: what does it take for
such users to align the algorithm with their true interests?
  To investigate these dynamics, we model the user's decision process as split
between a rational system 2 that decides whether to engage and an impulsive
system 1 that determines how long engagement lasts. We then study a
multi-leader, single-follower extensive Stackelberg game, where users,
specifically system 2, lead by committing to engagement strategies and the
algorithm best-responds based on observed interactions. We define the burden of
alignment as the minimum horizon over which users must optimize to effectively
steer the algorithm. We show that a critical horizon exists: users who are
sufficiently foresighted can achieve alignment, while those who are not are
instead aligned to the algorithm's objective. This critical horizon can be
long, imposing a substantial burden. However, even a small, costly signal
(e.g., an extra click) can significantly reduce it. Overall, our framework
explains how users with inconsistent preferences can align an engagement-driven
algorithm with their interests in a Stackelberg equilibrium, highlighting both
the challenges and potential remedies for achieving alignment.

</details>


### [105] [Before you <think>, monitor: Implementing Flavell's metacognitive framework in LLMs](https://arxiv.org/abs/2510.16374)
*Nick Oh*

Main category: cs.AI

TL;DR: 该论文提出了一个结合监控-生成-验证的三阶段迭代系统，在GSM8K数学推理任务上取得了75.42%的准确率，优于现有方法，且需要更少的尝试次数。


<details>
  <summary>Details</summary>
Motivation: 现有LLM推理增强方法存在分离：监控-生成方法擅长策略规划但缺乏验证机制，生成-验证方法能迭代优化但缺乏策略基础。这种分离导致效率低下。

Method: 基于Flavell的认知监控模型，实现监控-生成-验证三阶段迭代系统，在生成前进行任务评估和策略规划。

Result: 在GSM8K上达到75.42%准确率，优于SELF-REFINE(68.44%)和Self-Verification(67.07%)，尝试次数更少(1.3 vs 2.0)，推理成本增加27-37%。

Conclusion: 前期监控能产生更高质量的初始解，减少优化需求，但需要在算术推理之外的任务上进一步验证通用性。

Abstract: Current approaches to enhancing LLM reasoning follows two isolated paradigms:
Monitor-Generate methods like Plan-and-Solve (Wang et al., 2023) and
SELF-DISCOVER (Zhou et al., 2024) excel at strategic planning but lack
mechanisms to verify whether selected strategies succeed; while Generate-Verify
approaches like Self-Verification (Weng et al., 2022) and SELF-REFINE (Madaan
et al., 2023) iteratively refine outputs but commence generation blindly
without task assessment. This separation creates inefficiencies -- strategies
fail without feedback, and refinement occurs without strategic grounding. We
address this gap by implementing Flavell's cognitive monitoring model (1979)
from the broader Monitor-Generate-Verify framework (Oh and Gobet, 2025),
operationalising it as a three-phase iterative system. On GSM8K, preliminary
results show 75.42% accuracy versus 68.44% for SELF-REFINE and 67.07% for
Self-Verification, while requiring fewer attempts (1.3 vs 2.0) at 27-37%
increased inference cost. These initial findings suggest upfront monitoring
produces higher-quality initial solutions that reduce refinement needs, though
evaluation beyond arithmetic reasoning is needed to establish generalisability.

</details>


### [106] [Humanoid-inspired Causal Representation Learning for Domain Generalization](https://arxiv.org/abs/2510.16382)
*Ze Tao,Jian Zhang,Haowei Li,Xianshuai Li,Yifei Peng,Xiyao Liu,Senzhang Wang,Chao Liu,Sheng Ren,Shichao Zhang*

Main category: cs.AI

TL;DR: 提出受人类智能启发的HSCM因果框架，通过解耦和重加权图像属性（颜色、纹理、形状）来提升跨领域泛化能力，在理论和实证评估中优于现有领域泛化模型。


<details>
  <summary>Details</summary>
Motivation: 克服传统领域泛化模型的局限性，模仿人类视觉系统的分层处理和多层次学习机制，专注于建模细粒度因果机制。

Method: 构建受人类智能启发的结构因果模型（HSCM），解耦和重加权关键图像属性（颜色、纹理、形状），复制人类视觉系统的分层处理和多层次学习过程。

Result: HSCM在跨领域泛化方面表现出色，优于现有领域泛化模型，提供更稳健的性能和可解释性。

Conclusion: HSCM提供了一种更原则性的方法来捕捉因果关系并提高模型鲁棒性，在动态复杂环境中实现更有效的迁移和学习。

Abstract: This paper proposes the Humanoid-inspired Structural Causal Model (HSCM), a
novel causal framework inspired by human intelligence, designed to overcome the
limitations of conventional domain generalization models. Unlike approaches
that rely on statistics to capture data-label dependencies and learn
distortion-invariant representations, HSCM replicates the hierarchical
processing and multi-level learning of human vision systems, focusing on
modeling fine-grained causal mechanisms. By disentangling and reweighting key
image attributes such as color, texture, and shape, HSCM enhances
generalization across diverse domains, ensuring robust performance and
interpretability. Leveraging the flexibility and adaptability of human
intelligence, our approach enables more effective transfer and learning in
dynamic, complex environments. Through both theoretical and empirical
evaluations, we demonstrate that HSCM outperforms existing domain
generalization models, providing a more principled method for capturing causal
relationships and improving model robustness. The code is available at
https://github.com/lambett/HSCM.

</details>


### [107] [RGMem: Renormalization Group-based Memory Evolution for Language Agent User Profile](https://arxiv.org/abs/2510.16392)
*Ao Tian,Yunfeng Lu,Xinxin Fan,Changhao Wang,Lanzhi Zhou,Yeyao Zhang,Yanfang Liu*

Main category: cs.AI

TL;DR: 提出了RGMem框架，通过受物理学重整化群启发的多尺度记忆组织方法，从对话历史中提取用户深层偏好和特质，实现长期用户建模和行为一致性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM对话系统受限于有限上下文窗口和静态参数记忆，难以建模跨会话的长期用户状态和行为一致性。RAG和显式记忆系统主要关注事实级存储检索，缺乏从多轮对话中提取潜在偏好和深层特质的能力。

Method: RGMem框架采用多尺度记忆组织：从片段级对话中提取语义和用户洞察，通过分层粗粒化和重标度操作，逐步形成动态演化的用户档案。核心创新是将记忆演化建模为信息压缩和涌现的多尺度过程。

Result: 该框架能够从噪声和微观层面的交互中实现高级别、准确的用户档案构建，解决了长期用户建模的挑战。

Conclusion: RGMem框架通过多尺度记忆演化过程，实现了语言代理在LLM时代的长期记忆和行为一致性，为个性化交互提供了更深入和连续的支持。

Abstract: Personalized and continuous interactions are the key to enhancing user
experience in today's large language model (LLM)-based conversational systems,
however, the finite context windows and static parametric memory make it
difficult to model the cross-session long-term user states and behavioral
consistency. Currently, the existing solutions to this predicament, such as
retrieval-augmented generation (RAG) and explicit memory systems, primarily
focus on fact-level storage and retrieval, lacking the capability to distill
latent preferences and deep traits from the multi-turn dialogues, which limits
the long-term and effective user modeling, directly leading to the personalized
interactions remaining shallow, and hindering the cross-session continuity. To
realize the long-term memory and behavioral consistency for Language Agents in
LLM era, we propose a self-evolving memory framework RGMem, inspired by the
ideology of classic renormalization group (RG) in physics, this framework
enables to organize the dialogue history in multiple scales: it first extracts
semantics and user insights from episodic fragments, then through hierarchical
coarse-graining and rescaling operations, progressively forms a
dynamically-evolved user profile. The core innovation of our work lies in
modeling memory evolution as a multi-scale process of information compression
and emergence, which accomplishes the high-level and accurate user profiles
from noisy and microscopic-level interactions.

</details>


### [108] [ReviewSense: Transforming Customer Review Dynamics into Actionable Business Insights](https://arxiv.org/abs/2510.16466)
*Siddhartha Krothapalli,Tridib Kumar Das,Praveen Kumar,Naveen Suravarpu,Pratik Narang*

Main category: cs.AI

TL;DR: 提出了ReviewSense框架，利用大语言模型将客户评论转化为可操作的商业建议，超越了传统偏好预测系统。


<details>
  <summary>Details</summary>
Motivation: 客户反馈对战略增长至关重要，但现有AI系统主要关注偏好预测，缺乏将非结构化评论转化为商业建议的能力。

Method: 整合聚类、LLM适应和专家驱动评估的统一流程，识别客户情绪中的关键趋势、重复问题和具体关注点。

Result: 初步人工评估显示模型建议与商业目标高度一致，具有推动数据驱动决策的潜力。

Conclusion: 该框架为AI驱动的情绪分析提供了新视角，在优化商业策略和最大化客户反馈价值方面具有重要价值。

Abstract: As customer feedback becomes increasingly central to strategic growth, the
ability to derive actionable insights from unstructured reviews is essential.
While traditional AI-driven systems excel at predicting user preferences, far
less work has focused on transforming customer reviews into prescriptive,
business-facing recommendations. This paper introduces ReviewSense, a novel
prescriptive decision support framework that leverages advanced large language
models (LLMs) to transform customer reviews into targeted, actionable business
recommendations. By identifying key trends, recurring issues, and specific
concerns within customer sentiments, ReviewSense extends beyond
preference-based systems to provide businesses with deeper insights for
sustaining growth and enhancing customer loyalty. The novelty of this work lies
in integrating clustering, LLM adaptation, and expert-driven evaluation into a
unified, business-facing pipeline. Preliminary manual evaluations indicate
strong alignment between the model's recommendations and business objectives,
highlighting its potential for driving data-informed decision-making. This
framework offers a new perspective on AI-driven sentiment analysis,
demonstrating its value in refining business strategies and maximizing the
impact of customer feedback.

</details>


### [109] [NP-Engine: Empowering Optimization Reasoning in Large Language Models with Verifiable Synthetic NP Problems](https://arxiv.org/abs/2510.16476)
*Xiaozhe Li,Xinyu Fang,Shengyuan Ding,Linyang Li,Haodong Duan,Qingwen Liu,Kai Chen*

Main category: cs.AI

TL;DR: 提出了NP-ENGINE框架，首个专门训练和评估LLMs解决NP-hard问题的综合系统，包含10个任务、可控实例生成器、验证器和启发式求解器。训练出的QWEN2.5-7B-NP模型在NP-BENCH基准上超越GPT-4o，并展现出强大的跨领域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 虽然LLMs在数学、编程等推理任务上表现出色，但在解决复杂NP-hard优化问题方面的能力尚未充分探索，需要专门的训练框架来填补这一空白。

Method: 开发NP-ENGINE框架，包含可控实例生成器、规则验证器和启发式求解器，支持可扩展的RLVR训练；使用课程学习在Qwen2.5-7B-Instruct上进行zero-RLVR训练。

Result: QWEN2.5-7B-NP在NP-BENCH基准上显著超越GPT-4o，达到同规模模型的最佳性能；在跨领域推理任务和非推理任务上都展现出强大的泛化能力；发现任务多样性提升能改善泛化性能。

Conclusion: 基于NP-ENGINE的任务丰富RLVR训练是提升LLM推理能力的有前景方向，揭示了RLVR的扩展规律，为LLMs解决复杂优化问题提供了新途径。

Abstract: Large Language Models (LLMs) have shown strong reasoning capabilities, with
models like OpenAI's O-series and DeepSeek R1 excelling at tasks such as
mathematics, coding, logic, and puzzles through Reinforcement Learning with
Verifiable Rewards (RLVR). However, their ability to solve more complex
optimization problems - particularly NP-hard tasks - remains underexplored. To
bridge this gap, we propose NP-ENGINE, the first comprehensive framework for
training and evaluating LLMs on NP-hard problems. NP-ENGINE covers 10 tasks
across five domains, each equipped with (i) a controllable instance generator,
(ii) a rule-based verifier, and (iii) a heuristic solver that provides
approximate optimal solutions as ground truth. This
generator-verifier-heuristic pipeline enables scalable and verifiable RLVR
training under hierarchical difficulties. We also introduce NP-BENCH, a
benchmark derived from NP-ENGINE-DATA, specifically designed to evaluate LLMs'
ability to tackle NP-hard level reasoning problems, focusing not only on
feasibility but also on solution quality. Additionally, we present
QWEN2.5-7B-NP, a model trained via zero-RLVR with curriculum learning on
Qwen2.5-7B-Instruct, which significantly outperforms GPT-4o on NP-BENCH and
achieves SOTA performance with the same model size. Beyond in-domain tasks, we
demonstrate that RLVR training on NP-ENGINE-DATA enables strong out-of-domain
(OOD) generalization to reasoning tasks (logic, puzzles, math, and knowledge),
as well as non-reasoning tasks such as instruction following. We also observe a
scaling trend: increasing task diversity improves OOD generalization. These
findings suggest that task-rich RLVR training is a promising direction for
advancing LLM's reasoning ability, revealing new insights into the scaling laws
of RLVR.

</details>


### [110] [Hey Pentti, We Did It Again!: Differentiable vector-symbolic types that prove polynomial termination](https://arxiv.org/abs/2510.16533)
*Eilene Tomkins-Flanagan,Connor Hanley,Mary A. Kelly*

Main category: cs.AI

TL;DR: Doug是一个类型化计算机语言，所有类型化程序都能被证明在多项式时间内停止，编码在向量符号架构(VSA)中。该语言支持类型学习，旨在实现类似人类节奏的技能获取。


<details>
  <summary>Details</summary>
Motivation: 目标是建模人类心理表征及其获取过程，实现比现有方法更高效的人类化技能学习速度。

Method: 基于轻量线性函数式编程语言(LLFPL)，使用基于全息声明性记忆(HDM)的槽值编码方案编码类型，以及Lisp VSA变体编码项。

Result: 提出了一种新的语言设计，使神经网络能够学习类型，并为高效技能获取提供理论基础。

Conclusion: Doug语言使我们更接近建模大脑中实际存在的心理表征及其学习过程，为人类化技能获取提供了新的技术路径。

Abstract: We present a typed computer language, Doug, in which all typed programs may
be proved to halt in polynomial time, encoded in a vector-symbolic architecture
(VSA). Doug is just an encoding of the light linear functional programming
language (LLFPL) described by (Schimanski2009, ch. 7). The types of Doug are
encoded using a slot-value encoding scheme based on holographic declarative
memory (HDM; Kelly, 2020). The terms of Doug are encoded using a variant of the
Lisp VSA defined by (Flanagan, 2024). Doug allows for some points on the
embedding space of a neural network to be interpreted as types, where the types
of nearby points are similar both in structure and content. Types in Doug are
therefore learnable by a neural network. Following (Chollet, 2019), (Card,
1983), and (Newell, 1981), we view skill as the application of a procedure, or
program of action, that causes a goal to be satisfied. Skill acquisition may
therefore be expressed as program synthesis. Using Doug, we hope to describe a
form of learning of skilled behaviour that follows a human-like pace of skill
acquisition (i.e., substantially faster than brute force; Heathcote, 2000),
exceeding the efficiency of all currently existing approaches (Kaplan, 2020;
Jones, 2021; Chollet, 2024). Our approach brings us one step closer to modeling
human mental representations, as they must actually exist in the brain, and
those representations' acquisition, as they are actually learned.

</details>


### [111] [Urban-R1: Reinforced MLLMs Mitigate Geospatial Biases for Urban General Intelligence](https://arxiv.org/abs/2510.16555)
*Qiongyan Wang,Xingchen Zou,Yutian Jiang,Haomin Wen,Jiaheng Wei,Qingsong Wen,Yuxuan Liang*

Main category: cs.AI

TL;DR: 提出了Urban-R1框架，使用强化学习来减少多模态大语言模型在城市智能任务中的地理偏见，提高跨区域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于监督微调的城市基础模型存在持续的地理偏见，导致区域预测偏差和有限泛化能力，需要更公平的城市智能系统。

Method: 采用基于强化学习的后训练框架，使用组相对策略优化(GRPO)来优化跨地理群体的推理，并以城市区域画像作为代理任务提供可测量的奖励。

Result: 在多个区域和任务上的广泛实验表明，Urban-R1有效减轻了地理偏见并改善了跨区域泛化，优于监督微训练和闭源模型。

Conclusion: 强化学习对齐是实现公平可信城市智能的有前景途径。

Abstract: Rapid urbanization intensifies the demand for Urban General Intelligence
(UGI), referring to AI systems that can understand and reason about complex
urban environments. Recent studies have built urban foundation models using
supervised fine-tuning (SFT) of LLMs and MLLMs, yet these models exhibit
persistent geospatial bias, producing regionally skewed predictions and limited
generalization. To this end, we propose Urban-R1, a reinforcement
learning-based post-training framework that aligns MLLMs with the objectives of
UGI. Urban-R1 adopts Group Relative Policy Optimization (GRPO) to optimize
reasoning across geographic groups and employs urban region profiling as a
proxy task to provide measurable rewards from multimodal urban data. Extensive
experiments across diverse regions and tasks show that Urban-R1 effectively
mitigates geo-bias and improves cross-region generalization, outperforming both
SFT-trained and closed-source models. Our results highlight reinforcement
learning alignment as a promising pathway toward equitable and trustworthy
urban intelligence.

</details>


### [112] [BuildArena: A Physics-Aligned Interactive Benchmark of LLMs for Engineering Construction](https://arxiv.org/abs/2510.16559)
*Tian Xia,Tianrun Gao,Wenhao Deng,Long Wei,Xiaowei Qian,Yixian Jiang,Chenglei Yu,Tailin Wu*

Main category: cs.AI

TL;DR: BuildArena是首个面向语言驱动工程建设的物理对齐交互基准，用于评估LLM在工程建筑自动化中的能力，包括可定制框架、可扩展任务设计、3D空间几何计算库和基线LLM代理工作流。


<details>
  <summary>Details</summary>
Motivation: 工程建筑自动化需要将自然语言规范转化为物理可行的结构，虽然现代LLM具有广泛知识和强大推理能力，但其在建筑领域的能力尚未得到充分评估。

Method: 开发了BuildArena基准，包含四个关键组件：可定制基准框架、可扩展任务设计策略、3D空间几何计算库和基线LLM代理工作流。

Result: 在八个前沿LLM上全面评估了它们在语言驱动和物理基础建设自动化方面的能力。

Conclusion: BuildArena填补了LLM在工程建筑自动化领域评估的空白，为社区提供了首个物理对齐的交互基准。

Abstract: Engineering construction automation aims to transform natural language
specifications into physically viable structures, requiring complex integrated
reasoning under strict physical constraints. While modern LLMs possess broad
knowledge and strong reasoning capabilities that make them promising candidates
for this domain, their construction competencies remain largely unevaluated. To
address this gap, we introduce BuildArena, the first physics-aligned
interactive benchmark designed for language-driven engineering construction. It
contributes to the community in four aspects: (1) a highly customizable
benchmarking framework for in-depth comparison and analysis of LLMs; (2) an
extendable task design strategy spanning static and dynamic mechanics across
multiple difficulty tiers; (3) a 3D Spatial Geometric Computation Library for
supporting construction based on language instructions; (4) a baseline LLM
agentic workflow that effectively evaluates diverse model capabilities. On
eight frontier LLMs, BuildArena comprehensively evaluates their capabilities
for language-driven and physics-grounded construction automation. The project
page is at https://build-arena.github.io/.

</details>


### [113] [Ripple Effect Protocol: Coordinating Agent Populations](https://arxiv.org/abs/2510.16572)
*Ayush Chopra,Aman Sharma,Feroz Ahmad,Luca Muscariello,Vijoy Pandey,Ramesh Raskar*

Main category: cs.AI

TL;DR: 提出了Ripple Effect Protocol (REP)协调协议，让智能体不仅共享决策，还共享轻量级敏感度信号，从而在群体中实现更快更稳定的协调。


<details>
  <summary>Details</summary>
Motivation: 现有AI代理通信协议（如A2A和ACP）强调通信而非协调，随着代理群体规模增长，这会导致脆弱的集体行为，即使个体智能体很聪明，群体结果也很差。

Method: REP协议让代理共享决策和轻量级敏感度信号（表达关键环境变量变化时选择如何变化的信号），这些敏感度在局部网络中传播，实现比单纯代理中心通信更快更稳定的群体对齐。

Result: 在三个领域的基准测试中：供应链级联（啤酒游戏）、稀疏网络中的偏好聚合（电影调度）和可持续资源分配（渔业银行），REP相比A2A提高了41%到100%的协调准确性和效率。

Conclusion: 通过将协调作为协议级能力，REP为新兴的智能体互联网提供了可扩展的基础设施。

Abstract: Modern AI agents can exchange messages using protocols such as A2A and ACP,
yet these mechanisms emphasize communication over coordination. As agent
populations grow, this limitation produces brittle collective behavior, where
individually smart agents converge on poor group outcomes. We introduce the
Ripple Effect Protocol (REP), a coordination protocol in which agents share not
only their decisions but also lightweight sensitivities - signals expressing
how their choices would change if key environmental variables shifted. These
sensitivities ripple through local networks, enabling groups to align faster
and more stably than with agent-centric communication alone. We formalize REP's
protocol specification, separating required message schemas from optional
aggregation rules, and evaluate it across scenarios with varying incentives and
network topologies. Benchmarks across three domains: (i) supply chain cascades
(Beer Game), (ii) preference aggregation in sparse networks (Movie Scheduling),
and (iii) sustainable resource allocation (Fishbanks) show that REP improves
coordination accuracy and efficiency over A2A by 41 to 100%, while flexibly
handling multimodal sensitivity signals from LLMs. By making coordination a
protocol-level capability, REP provides scalable infrastructure for the
emerging Internet of Agents

</details>


### [114] [Can Knowledge-Graph-based Retrieval Augmented Generation Really Retrieve What You Need?](https://arxiv.org/abs/2510.16582)
*Junchi Yu,Yujie Liu,Jindong Gu,Philip Torr,Dongzhan Zhou*

Main category: cs.AI

TL;DR: GraphFlow框架通过基于转换的流匹配目标优化检索策略和流估计器，从文本丰富的知识图谱中高效检索准确多样的知识，在STaRK基准测试中优于现有KG-RAG方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于知识图谱的RAG方法难以从文本丰富的知识图谱中为复杂真实世界查询检索准确多样的信息，而过程奖励模型需要昂贵且难以获取的过程级监督信号。

Method: 使用基于转换的流匹配目标联合优化检索策略和流估计器，流估计器将检索结果的奖励分解到中间检索状态，指导检索策略按奖励比例从知识图谱中检索候选。

Result: 在STaRK基准测试中，GraphFlow在命中率和召回率上平均优于强KG-RAG基线（包括GPT-4o）10%，并对未见过的知识图谱表现出强泛化能力。

Conclusion: GraphFlow能够有效探索知识图谱的高质量区域，产生多样且相关的结果，证明了其有效性和鲁棒性。

Abstract: Retrieval-Augmented Generation (RAG) based on knowledge graphs (KGs) enhances
large language models (LLMs) by providing structured and interpretable external
knowledge. However, existing KG-based RAG methods struggle to retrieve accurate
and diverse information from text-rich KGs for complex real-world queries.
Process Reward Models (PRMs) offer a way to align the retrieval process of
KG-based RAG with query-specific knowledge requirements, but they heavily rely
on process-level supervision signals that are expensive and hard to obtain on
KGs. To address this challenge, we propose GraphFlow, a framework that
efficiently retrieves accurate and diverse knowledge required for real-world
queries from text-rich KGs. GraphFlow employs a transition-based flow matching
objective to jointly optimize a retrieval policy and a flow estimator. The flow
estimator factorizes the reward of the retrieval outcome into the intermediate
retrieval states. Such reward factorization guides the retrieval policy to
retrieve candidates from KGs in proportion to their reward. This allows
GraphFlow to explore high-quality regions of KGs that yield diverse and
relevant results. We evaluate GraphFlow on the STaRK benchmark, which includes
real-world queries from multiple domains over text-rich KGs. GraphFlow
outperforms strong KG-RAG baselines, including GPT-4o, by 10% on average in hit
rate and recall. It also shows strong generalization to unseen KGs,
demonstrating its effectiveness and robustness.

</details>


### [115] [Uncertain Knowledge Graph Completion via Semi-Supervised Confidence Distribution Learning](https://arxiv.org/abs/2510.16601)
*Tianxing Wu,Shutong Zhu,Jingting Wang,Ning Xu,Guilin Qi,Haofen Wang*

Main category: cs.AI

TL;DR: 提出了一种用于不确定知识图谱补全的半监督置信度分布学习方法，通过将置信度转化为分布并利用元学习生成伪标签来平衡数据分布，提升嵌入学习质量。


<details>
  <summary>Details</summary>
Motivation: 现有不确定知识图谱补全方法忽略了置信度的极端不平衡分布问题，导致学习到的嵌入不足以支持高质量的图谱补全。

Method: 提出ssCDL方法，将置信度转化为分布引入更多监督信息，通过元学习生成伪标签来扩充训练数据并平衡置信度分布，迭代学习图谱嵌入。

Result: 在两个不确定知识图谱数据集上的实验表明，ssCDL在不同评估指标上均优于现有最优基线方法。

Conclusion: ssCDL通过处理置信度不平衡分布问题，有效提升了不确定知识图谱补全的性能。

Abstract: Uncertain knowledge graphs (UKGs) associate each triple with a confidence
score to provide more precise knowledge representations. Recently, since
real-world UKGs suffer from the incompleteness, uncertain knowledge graph (UKG)
completion attracts more attention, aiming to complete missing triples and
confidences. Current studies attempt to learn UKG embeddings to solve this
problem, but they neglect the extremely imbalanced distributions of triple
confidences. This causes that the learnt embeddings are insufficient to
high-quality UKG completion. Thus, in this paper, to address the above issue,
we propose a new semi-supervised Confidence Distribution Learning (ssCDL)
method for UKG completion, where each triple confidence is transformed into a
confidence distribution to introduce more supervision information of different
confidences to reinforce the embedding learning process. ssCDL iteratively
learns UKG embedding by relational learning on labeled data (i.e., existing
triples with confidences) and unlabeled data with pseudo labels (i.e., unseen
triples with the generated confidences), which are predicted by meta-learning
to augment the training data and rebalance the distribution of triple
confidences. Experiments on two UKG datasets demonstrate that ssCDL
consistently outperforms state-of-the-art baselines in different evaluation
metrics.

</details>


### [116] [Count Counts: Motivating Exploration in LLM Reasoning with Count-based Intrinsic Rewards](https://arxiv.org/abs/2510.16614)
*Xuan Zhang,Ruixiao Li,Zhijian Zhou,Long Li,Yulei Qin,Ke Li,Xing Sun,Xiaoyu Tan,Chao Qu,Yuan Qi*

Main category: cs.AI

TL;DR: MERCI是一种增强LLM推理能力的强化学习算法，通过基于计数的内在奖励来激励探索，避免模型陷入重复和次优的推理模式。


<details>
  <summary>Details</summary>
Motivation: 现有的强化学习范式依赖稀疏的结果奖励和有限的探索，导致LLM趋向于重复和次优的推理模式，需要设计更好的探索机制。

Method: 使用轻量级的Coin Flipping Network估计推理轨迹的伪计数和认知不确定性，将其转换为内在奖励，并与任务奖励结合进行策略优化。

Result: 在复杂推理基准测试中，MERCI鼓励更丰富多样的思维链，显著超越强基线性能，帮助策略逃离局部模式发现更好的解决方案。

Conclusion: 针对性的内在动机可以使语言模型推理中的探索更加可靠有效。

Abstract: Reinforcement Learning (RL) has become a compelling way to strengthen the
multi step reasoning ability of Large Language Models (LLMs). However,
prevalent RL paradigms still lean on sparse outcome-based rewards and limited
exploration, which often drives LLMs toward repetitive and suboptimal reasoning
patterns. In this paper, we study the central question of how to design
exploration for LLM reasoning and introduce MERCI (Motivating Exploration in
LLM Reasoning with Count-based Intrinsic Rewards), a novel RL algorithm that
augments policy optimization with a principled intrinsic reward. Building on
the idea of count-based exploration, MERCI leverages a lightweight Coin
Flipping Network (CFN) to estimate the pseudo count and further epistemic
uncertainty over reasoning trajectories, and converts them into an intrinsic
reward that values novelty while preserving the learning signal from task
rewards. We integrate MERCI into some advanced RL frameworks like Group
Relative Policy Optimization (GRPO). Experiments on complex reasoning
benchmarks demonstrate that MERCI encourages richer and more varied chains of
thought, significantly improves performance over strong baselines, and helps
the policy escape local routines to discover better solutions. It indicates
that our targeted intrinsic motivation can make exploration reliable for
language model reasoning.

</details>


### [117] [Foundation and Large-Scale AI Models in Neuroscience: A Comprehensive Review](https://arxiv.org/abs/2510.16658)
*Shihao Yang,Xiying Huang,Danilo Bernardo,Jun-En Ding,Andrew Michael,Jingmei Yang,Patrick Kwan,Ashish Raj,Feng Liu*

Main category: cs.AI

TL;DR: 本文探讨了大规模AI模型对神经科学研究的变革性影响，涵盖了神经影像处理、脑机接口、分子神经科学、临床辅助和疾病应用等五大领域，展示了这些模型在解决多模态神经数据整合、时空模式解释等挑战方面的能力。


<details>
  <summary>Details</summary>
Motivation: 大规模AI模型的出现为神经科学研究带来了范式转变，促进了从原始脑信号和神经数据的端到端学习，旨在解决传统计算方法面临的挑战。

Method: 通过系统综述的方式，探索大规模AI模型在五个主要神经科学领域的应用：神经影像与数据处理、脑机接口与神经解码、分子神经科学与基因组建模、临床辅助与转化框架、以及神经系统和精神疾病的特定应用。

Result: 研究表明这些模型能够有效整合多模态神经数据、解释时空模式，并建立临床部署的转化框架。同时，神经科学与AI的互动变得更加相互促进，生物启发的架构约束被纳入以开发更可解释和计算效率更高的模型。

Conclusion: 该综述强调了这些技术的显著前景和关键实施考虑，特别强调严格的评估框架、有效的领域知识整合以及临床使用的全面伦理指南，并提供了用于验证大规模AI模型的关键神经科学数据集系统列表。

Abstract: The advent of large-scale artificial intelligence (AI) models has a
transformative effect on neuroscience research, which represents a paradigm
shift from the traditional computational methods through the facilitation of
end-to-end learning from raw brain signals and neural data. In this paper, we
explore the transformative effects of large-scale AI models on five major
neuroscience domains: neuroimaging and data processing, brain-computer
interfaces and neural decoding, molecular neuroscience and genomic modeling,
clinical assistance and translational frameworks, and disease-specific
applications across neurological and psychiatric disorders. These models are
demonstrated to address major computational neuroscience challenges, including
multimodal neural data integration, spatiotemporal pattern interpretation, and
the derivation of translational frameworks for clinical deployment. Moreover,
the interaction between neuroscience and AI has become increasingly reciprocal,
as biologically informed architectural constraints are now incorporated to
develop more interpretable and computationally efficient models. This review
highlights both the notable promise of such technologies and key implementation
considerations, with particular emphasis on rigorous evaluation frameworks,
effective domain knowledge integration, and comprehensive ethical guidelines
for clinical use. Finally, a systematic listing of critical neuroscience
datasets used to derive and validate large-scale AI models across diverse
research applications is provided.

</details>


### [118] [An Agentic Framework with LLMs for Solving Complex Vehicle Routing Problems](https://arxiv.org/abs/2510.16701)
*Ni Zhang,Zhiguang Cao,Jianan Zhou,Cong Zhang,Yew-Soon Ong*

Main category: cs.AI

TL;DR: 提出了一个基于大语言模型的自主代理框架AFL，用于解决复杂车辆路径问题，实现了从问题实例到解决方案的完全自动化，无需人工干预或外部求解器。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的方法在解决复杂VRP问题时仍需要外部干预，导致自主性受限、执行错误多、解决方案可行性低。

Method: AFL框架将整个流程分解为三个子任务，使用四个专门代理进行协调交互，直接从原始输入中提取知识并生成自包含代码。

Result: 在60个复杂VRP问题上的实验表明，该框架在代码可靠性和解决方案可行性方面显著优于现有LLM基线，在评估基准上接近100%的成功率。

Conclusion: AFL框架实现了复杂VRP问题的完全自动化求解，性能可与精心设计的算法相媲美，同时大幅提升了基于LLM方法的可靠性和可行性。

Abstract: Complex vehicle routing problems (VRPs) remain a fundamental challenge,
demanding substantial expert effort for intent interpretation and algorithm
design. While large language models (LLMs) offer a promising path toward
automation, current approaches still rely on external intervention, which
restrict autonomy and often lead to execution errors and low solution
feasibility. To address these challenges, we propose an Agentic Framework with
LLMs (AFL) for solving complex vehicle routing problems, achieving full
automation from problem instance to solution. AFL directly extracts knowledge
from raw inputs and enables self-contained code generation without handcrafted
modules or external solvers. To improve trustworthiness, AFL decomposes the
overall pipeline into three manageable subtasks and employs four specialized
agents whose coordinated interactions enforce cross-functional consistency and
logical soundness. Extensive experiments on 60 complex VRPs, ranging from
standard benchmarks to practical variants, validate the effectiveness and
generality of our framework, showing comparable performance against
meticulously designed algorithms. Notably, it substantially outperforms
existing LLM-based baselines in both code reliability and solution feasibility,
achieving rates close to 100% on the evaluated benchmarks.

</details>


### [119] [Beyond Pipelines: A Survey of the Paradigm Shift toward Model-Native Agentic AI](https://arxiv.org/abs/2510.16720)
*Jitao Sang,Jinlin Xiao,Jiarun Han,Jilin Chen,Xiaoyi Chen,Shuyu Wei,Yongjie Sun,Yuhang Wang*

Main category: cs.AI

TL;DR: 该论文综述了智能AI从基于管道的系统向模型原生范式的转变，其中规划、工具使用和记忆等能力从外部逻辑编排转变为模型内部参数化。强化学习是实现这一范式转变的关键算法引擎。


<details>
  <summary>Details</summary>
Motivation: 追踪智能AI的范式转变，从构建应用智能的系统转向开发通过经验增长智能的模型，探索从外部编排到内部学习的演进路径。

Method: 系统回顾了规划、工具使用和记忆等能力的演变过程，从外部脚本模块到端到端学习行为，并分析了强化学习在实现这一转变中的核心作用。

Result: 识别出智能AI发展的连贯轨迹，即向模型原生智能AI的演进，形成了一个集成的学习和交互框架。

Conclusion: 智能AI正在从构建应用智能的系统转向开发通过经验增长智能的模型，标志着人工智能发展的新阶段。

Abstract: The rapid evolution of agentic AI marks a new phase in artificial
intelligence, where Large Language Models (LLMs) no longer merely respond but
act, reason, and adapt. This survey traces the paradigm shift in building
agentic AI: from Pipeline-based systems, where planning, tool use, and memory
are orchestrated by external logic, to the emerging Model-native paradigm,
where these capabilities are internalized within the model's parameters. We
first position Reinforcement Learning (RL) as the algorithmic engine enabling
this paradigm shift. By reframing learning from imitating static data to
outcome-driven exploration, RL underpins a unified solution of LLM + RL + Task
across language, vision and embodied domains. Building on this, the survey
systematically reviews how each capability -- Planning, Tool use, and Memory --
has evolved from externally scripted modules to end-to-end learned behaviors.
Furthermore, it examines how this paradigm shift has reshaped major agent
applications, specifically the Deep Research agent emphasizing long-horizon
reasoning and the GUI agent emphasizing embodied interaction. We conclude by
discussing the continued internalization of agentic capabilities like
Multi-agent collaboration and Reflection, alongside the evolving roles of the
system and model layers in future agentic AI. Together, these developments
outline a coherent trajectory toward model-native agentic AI as an integrated
learning and interaction framework, marking the transition from constructing
systems that apply intelligence to developing models that grow intelligence
through experience.

</details>


### [120] [A Comprehensive Survey on Reinforcement Learning-based Agentic Search: Foundations, Roles, Optimizations, Evaluations, and Applications](https://arxiv.org/abs/2510.16724)
*Minhua Lin,Zongyu Wu,Zhichao Xu,Hui Liu,Xianfeng Tang,Qi He,Charu Aggarwal,Hui Liu,Xiang Zhang,Suhang Wang*

Main category: cs.AI

TL;DR: 该论文是关于基于强化学习的智能搜索系统的综述，探讨了如何用RL解决传统检索增强生成(RAG)的局限性，实现自适应、多步骤的智能搜索。


<details>
  <summary>Details</summary>
Motivation: 传统RAG系统存在单轮交互、启发式检索等局限性，无法实现自适应控制和多步骤推理。智能搜索通过多步骤交互解决了这些问题，而强化学习为智能搜索提供了自适应和自我改进的机制。

Method: 该综述从三个维度组织基于RL的智能搜索领域：RL的功能角色、优化策略和应用范围。总结了代表性方法、评估协议和应用场景。

Result: 提供了该领域的首个全面概述，建立了系统化的分类框架，涵盖了从方法到应用的完整研究体系。

Conclusion: 基于RL的智能搜索系统具有构建可靠、可扩展搜索系统的潜力，该综述旨在激发RL与智能搜索融合的未来研究。

Abstract: The advent of large language models (LLMs) has transformed information access
and reasoning through open-ended natural language interaction. However, LLMs
remain limited by static knowledge, factual hallucinations, and the inability
to retrieve real-time or domain-specific information. Retrieval-Augmented
Generation (RAG) mitigates these issues by grounding model outputs in external
evidence, but traditional RAG pipelines are often single turn and heuristic,
lacking adaptive control over retrieval and reasoning. Recent advances in
agentic search address these limitations by enabling LLMs to plan, retrieve,
and reflect through multi-step interaction with search environments. Within
this paradigm, reinforcement learning (RL) offers a powerful mechanism for
adaptive and self-improving search behavior. This survey provides the first
comprehensive overview of \emph{RL-based agentic search}, organizing the
emerging field along three complementary dimensions: (i) What RL is for
(functional roles), (ii) How RL is used (optimization strategies), and (iii)
Where RL is applied (scope of optimization). We summarize representative
methods, evaluation protocols, and applications, and discuss open challenges
and future directions toward building reliable and scalable RL driven agentic
search systems. We hope this survey will inspire future research on the
integration of RL and agentic search. Our repository is available at
https://github.com/ventr1c/Awesome-RL-based-Agentic-Search-Papers.

</details>


### [121] [Surrogate Modeling and Explainable Artificial Intelligence for Complex Systems: A Workflow for Automated Simulation Exploration](https://arxiv.org/abs/2510.16742)
*Paul Saves,Pramudita Satria Palar,Muhammad Daffa Robani,Nicolas Verstaevel,Moncef Garouani,Julien Aligon,Benoit Gaudou,Koji Shimoyama,Joseph Morlier*

Main category: cs.AI

TL;DR: 提出了一种基于代理模型的仿真驱动工程工作流，通过训练轻量级仿真器来解决计算成本高和透明度不足的问题，支持不确定性量化和可解释AI分析。


<details>
  <summary>Details</summary>
Motivation: 解决仿真驱动工程工作流面临的两个核心障碍：(1)高计算成本，(2)黑盒组件导致的透明度不足和可靠性问题。

Method: 在紧凑实验设计上训练轻量级仿真器，结合全局效应分析、不确定性分析和局部归因，评估不同代理模型间解释的一致性。

Result: 在两个案例研究中证明该方法能在秒级完成大规模探索，发现非线性相互作用和涌现行为，识别关键设计和政策杠杆，并指示代理模型需要更多数据或替代架构的区域。

Conclusion: 代理模型与可解释AI的结合为复杂系统分析提供了高效、透明的解决方案，支持从工程设计到社会环境理解的各种仿真应用。

Abstract: Complex systems are increasingly explored through simulation-driven
engineering workflows that combine physics-based and empirical models with
optimization and analytics. Despite their power, these workflows face two
central obstacles: (1) high computational cost, since accurate exploration
requires many expensive simulator runs; and (2) limited transparency and
reliability when decisions rely on opaque blackbox components. We propose a
workflow that addresses both challenges by training lightweight emulators on
compact designs of experiments that (i) provide fast, low-latency
approximations of expensive simulators, (ii) enable rigorous uncertainty
quantification, and (iii) are adapted for global and local Explainable
Artificial Intelligence (XAI) analyses. This workflow unifies every
simulation-based complex-system analysis tool, ranging from engineering design
to agent-based models for socio-environmental understanding. In this paper, we
proposea comparative methodology and practical recommendations for using
surrogate-based explainability tools within the proposed workflow. The
methodology supports continuous and categorical inputs, combines global-effect
and uncertainty analyses with local attribution, and evaluates the consistency
of explanations across surrogate models, thereby diagnosing surrogate adequacy
and guiding further data collection or model refinement. We demonstrate the
approach on two contrasting case studies: a multidisciplinary design analysis
of a hybrid-electric aircraft and an agent-based model of urban segregation.
Results show that the surrogate model and XAI coupling enables large-scale
exploration in seconds, uncovers nonlinear interactions and emergent behaviors,
identifies key design and policy levers, and signals regions where surrogates
require more data or alternative architectures.

</details>


### [122] [ELMM: Efficient Lightweight Multimodal Large Language Models for Multimodal Knowledge Graph Completion](https://arxiv.org/abs/2510.16753)
*Wei Huang,Peining Li,Meiyu Liang,Xu Hou,Junping Du,Yingxia Shao,Guanhua Ye,Wu Liu,Kangkang Lu,Yang Yu*

Main category: cs.AI

TL;DR: 提出ELMM方法解决多模态知识图谱补全中的语义噪声和计算成本问题，通过多视图视觉标记压缩器和注意力剪枝策略，在保持性能的同时显著提升计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有MKG存在不完整性问题，而多模态大语言模型在MKGC任务中面临图像标记过多导致的语义噪声、模态冲突和计算成本高等挑战。

Method: 提出ELMM框架，包含基于多头注意力的多视图视觉标记压缩器(MVTC)和注意力剪枝策略，通过线性投影补偿剪枝带来的性能损失。

Result: 在FB15k-237-IMG和WN18-IMG基准测试中达到最先进性能，同时显著提升计算效率。

Conclusion: ELMM为多模态知识图谱补全建立了新的范式，在保持高性能的同时解决了计算效率问题。

Abstract: Multimodal Knowledge Graphs (MKGs) extend traditional knowledge graphs by
incorporating visual and textual modalities, enabling richer and more
expressive entity representations. However, existing MKGs often suffer from
incompleteness, which hinder their effectiveness in downstream tasks.
Therefore, multimodal knowledge graph completion (MKGC) task is receiving
increasing attention. While large language models (LLMs) have shown promise for
knowledge graph completion (KGC), their application to the multimodal setting
remains underexplored. Moreover, applying Multimodal Large Language Models
(MLLMs) to the task of MKGC introduces significant challenges: (1) the large
number of image tokens per entity leads to semantic noise and modality
conflicts, and (2) the high computational cost of processing large token
inputs. To address these issues, we propose Efficient Lightweight Multimodal
Large Language Models (ELMM) for MKGC. ELMM proposes a Multi-view Visual Token
Compressor (MVTC) based on multi-head attention mechanism, which adaptively
compresses image tokens from both textual and visual views, thereby effectively
reducing redundancy while retaining necessary information and avoiding modality
conflicts. Additionally, we design an attention pruning strategy to remove
redundant attention layers from MLLMs, thereby significantly reducing the
inference cost. We further introduce a linear projection to compensate for the
performance degradation caused by pruning. Extensive experiments on benchmark
FB15k-237-IMG and WN18-IMG demonstrate that ELMM achieves state-of-the-art
performance while substantially improving computational efficiency,
establishing a new paradigm for multimodal knowledge graph completion.

</details>


### [123] [End-to-end Listen, Look, Speak and Act](https://arxiv.org/abs/2510.16756)
*Siyin Wang,Wenyi Yu,Xianzhao Chen,Xiaohai Tian,Jun Zhang,Lu Lu,Chao Zhang*

Main category: cs.AI

TL;DR: ELLSA是首个全双工、端到端的多模态交互模型，能够同时感知和生成视觉、文本、语音和动作，实现更自然的人类化行为。


<details>
  <summary>Details</summary>
Motivation: 人类交互本质上是多模态和全双工的，需要模型能够同时感知和生成多种模态信息，实现更自然的交互模式。

Method: 采用新颖的SA-MoE架构（自注意力专家混合），将各模态路由到专用专家，通过统一注意力骨干网络进行融合，实现联合多模态感知和并发生成。

Result: 在语音交互和机器人操作基准测试中，ELLSA达到模态特定基线的性能，同时支持高级多模态和全双工行为，如对话和动作轮转、缺陷指令拒绝、边说话边行动等。

Conclusion: ELLSA代表了向更自然和通用交互智能迈出的一步，有助于实现人工通用智能的追求。

Abstract: Human interaction is inherently multimodal and full-duplex: we listen while
watching, speak while acting, and fluidly adapt to turn-taking and
interruptions. Realizing these capabilities is essential for building models
simulating humans. We present ELLSA (End-to-end Listen, Look, Speak and Act),
which, to our knowledge, is the first full-duplex, end-to-end model that
simultaneously perceives and generates across vision, text, speech, and action
within a single architecture, enabling interaction patterns previously out of
reach, yielding more natural, human-like behaviors. At its core is a novel
SA-MoE architecture (Self-Attention Mixture-of-Experts) that routes each
modality to specialized experts and fuses them through a unified attention
backbone. This provides a generalizable solution for joint multimodal
perception and concurrent generation, leveraging strong pre-trained components
while enabling efficient modality integration and mitigating modality
interference. On speech-interaction and robot-manipulation benchmarks, ELLSA
matches modality-specific baselines, while uniquely supporting advanced
multimodal and full-duplex behaviors such as dialogue and action turn-taking,
defective instruction rejection, speaking-while-acting, context-grounded visual
question answering, and action barge-ins. We contend that ELLSA represents a
step toward more natural and general interactive intelligence, contributing to
the broader pursuit of artificial general intelligence. All data, code and
model checkpoints will be released upon acceptance.

</details>


### [124] [See or Say Graphs: Agent-Driven Scalable Graph Understanding with Vision-Language Models](https://arxiv.org/abs/2510.16769)
*Shuo Han,Yukun Cao,Zezhong Ding,Zengyi Gao,S Kevin Zhou,Xike Xie*

Main category: cs.AI

TL;DR: GraphVista是一个统一框架，通过分层组织图信息到轻量级GraphRAG基础和引入规划代理来协调模态，解决了视觉语言模型在图理解中的可扩展性和模态协调问题。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在图理解中面临输入令牌限制导致的可扩展性瓶颈，以及缺乏有效协调文本和视觉模态的机制。

Method: GraphVista采用分层方法组织图信息到GraphRAG基础，仅检索任务相关的文本描述和高分辨率视觉子图；引入规划代理根据任务复杂度路由到最适合的模态。

Result: GraphVista可扩展到比现有基准大200倍的图，在文本、视觉和融合方法中表现最优，比最先进基线质量提升达4.4倍。

Conclusion: GraphVista通过充分利用两种模态的互补优势，有效解决了图理解中的可扩展性和模态协调挑战。

Abstract: Vision-language models (VLMs) have shown promise in graph understanding, but
remain limited by input-token constraints, facing scalability bottlenecks and
lacking effective mechanisms to coordinate textual and visual modalities. To
address these challenges, we propose GraphVista, a unified framework that
enhances both scalability and modality coordination in graph understanding. For
scalability, GraphVista organizes graph information hierarchically into a
lightweight GraphRAG base, which retrieves only task-relevant textual
descriptions and high-resolution visual subgraphs, compressing redundant
context while preserving key reasoning elements. For modality coordination,
GraphVista introduces a planning agent that routes tasks to the most suitable
modality-using the text modality for simple property reasoning and the visual
modality for local and structurally complex reasoning grounded in explicit
topology. Extensive experiments demonstrate that GraphVista scales to large
graphs, up to $200\times$ larger than those used in existing benchmarks, and
consistently outperforms existing textual, visual, and fusion-based methods,
achieving up to $4.4\times$ quality improvement over the state-of-the-art
baselines by fully exploiting the complementary strengths of both modalities.

</details>


### [125] [Domain-Contextualized Concept Graphs: A Computable Framework for Knowledge Representation](https://arxiv.org/abs/2510.16802)
*Chao Li,Yuru Wang*

Main category: cs.AI

TL;DR: 提出Domain-Contextualized Concept Graph (CDC)框架，将领域作为知识表示的一等元素，采用<概念, 关系@领域, 概念'>的三元组结构，实现上下文感知推理和跨领域类比。


<details>
  <summary>Details</summary>
Motivation: 传统知识图谱受限于固定的本体论和刚性层次结构，问题在于将领域视为隐式上下文而非显式的推理级组件。

Method: 采用C-D-C三元组结构，基于认知-语言同构映射原则，定义20多个标准化关系谓词，并在Prolog中实现完整推理能力。

Result: 在教育、企业知识系统和文档管理等案例中，CDC实现了上下文感知推理、跨领域类比和个性化知识建模等传统框架无法实现的能力。

Conclusion: CDC框架通过将领域提升为概念表示的一等元素，克服了传统本体论框架的局限性，为知识建模提供了更灵活和动态的方法。

Abstract: Traditional knowledge graphs are constrained by fixed ontologies that
organize concepts within rigid hierarchical structures. The root cause lies in
treating domains as implicit context rather than as explicit, reasoning-level
components. To overcome these limitations, we propose the Domain-Contextualized
Concept Graph (CDC), a novel knowledge modeling framework that elevates domains
to first-class elements of conceptual representation. CDC adopts a C-D-C triple
structure - <Concept, Relation@Domain, Concept'> - where domain specifications
serve as dynamic classification dimensions defined on demand. Grounded in a
cognitive-linguistic isomorphic mapping principle, CDC operationalizes how
humans understand concepts through contextual frames. We formalize more than
twenty standardized relation predicates (structural, logical, cross-domain, and
temporal) and implement CDC in Prolog for full inference capability. Case
studies in education, enterprise knowledge systems, and technical documentation
demonstrate that CDC enables context-aware reasoning, cross-domain analogy, and
personalized knowledge modeling - capabilities unattainable under traditional
ontology-based frameworks.

</details>


### [126] [DeepAnalyze: Agentic Large Language Models for Autonomous Data Science](https://arxiv.org/abs/2510.16872)
*Shaolei Zhang,Ju Fan,Meihao Fan,Guoliang Li,Xiaoyong Du*

Main category: cs.AI

TL;DR: DeepAnalyze-8B是首个用于自主数据科学的代理式大语言模型，能够从数据源到分析师级深度研究报告自动完成端到端流程，仅用80亿参数就超越了基于最先进专有LLM的工作流代理。


<details>
  <summary>Details</summary>
Motivation: 现有的基于工作流的数据代理在特定数据任务上表现良好，但由于依赖预定义工作流，无法实现完全自主的数据科学。随着强大LLM的出现，自主数据科学变得可行。

Method: 提出了基于课程学习的代理训练范式，模拟人类数据科学家的学习轨迹，使LLM能够在真实环境中逐步获取和整合多种能力。还引入了数据基础轨迹合成框架来构建高质量训练数据。

Result: 实验表明，DeepAnalyze-8B能够执行广泛的数据任务，从数据问答、专业分析任务到开放式数据研究，性能超越了之前基于最先进专有LLM的工作流代理。

Conclusion: DeepAnalyze模型、代码和训练数据已开源，为自主数据科学的发展铺平了道路。

Abstract: Autonomous data science, from raw data sources to analyst-grade deep research
reports, has been a long-standing challenge, and is now becoming feasible with
the emergence of powerful large language models (LLMs). Recent workflow-based
data agents have shown promising results on specific data tasks but remain
fundamentally limited in achieving fully autonomous data science due to their
reliance on predefined workflows. In this paper, we introduce DeepAnalyze-8B,
the first agentic LLM designed for autonomous data science, capable of
automatically completing the end-toend pipeline from data sources to
analyst-grade deep research reports. To tackle high-complexity data science
tasks, we propose a curriculum-based agentic training paradigm that emulates
the learning trajectory of human data scientists, enabling LLMs to
progressively acquire and integrate multiple capabilities in real-world
environments. We also introduce a data-grounded trajectory synthesis framework
that constructs high-quality training data. Through agentic training,
DeepAnalyze learns to perform a broad spectrum of data tasks, ranging from data
question answering and specialized analytical tasks to open-ended data
research. Experiments demonstrate that, with only 8B parameters, DeepAnalyze
outperforms previous workflow-based agents built on most advanced proprietary
LLMs. The model, code, and training data of DeepAnalyze are open-sourced,
paving the way toward autonomous data science.

</details>


### [127] [VAGEN: Reinforcing World Model Reasoning for Multi-Turn VLM Agents](https://arxiv.org/abs/2510.16907)
*Kangrui Wang,Pingyue Zhang,Zihan Wang,Yaning Gao,Linjie Li,Qineng Wang,Hanyang Chen,Chi Wan,Yiping Lu,Zhengyuan Yang,Lijuan Wang,Ranjay Krishna,Jiajun Wu,Li Fei-Fei,Yejin Choi,Manling Li*

Main category: cs.AI

TL;DR: 该论文提出通过强化学习训练VLM代理构建内部世界模型，将视觉状态推理分解为状态估计和转移建模，并设计了世界建模奖励和双层GAE方法，在多个基准测试中显著优于未训练模型和专有推理模型。


<details>
  <summary>Details</summary>
Motivation: 解决VLM代理从文本状态转向复杂视觉观察时的部分可观测性挑战，探索代理是否能通过显式视觉状态推理构建内部世界模型。

Method: 使用强化学习在POMDP框架下训练代理，将推理过程分解为状态估计和转移建模，设计了世界建模奖励和双层GAE方法进行密集监督和信用分配。

Result: 3B参数模型在五个代理基准测试中得分0.82，比未训练模型(0.21)提升3倍，优于GPT-5(0.75)、Gemini 2.5 Pro(0.67)和Claude 4.5(0.62)。

Conclusion: 视觉状态推理能有效帮助VLM代理构建世界模型，最优表示形式取决于任务特性，提出的方法在VAGEN框架下实现了显著性能提升。

Abstract: A key challenge in training Vision-Language Model (VLM) agents, compared to
Language Model (LLM) agents, lies in the shift from textual states to complex
visual observations. This transition introduces partial observability and
demands robust world modeling. We ask: Can VLM agents construct internal world
models through explicit visual state reasoning? To address this question, we
architecturally enforce and reward the agent's reasoning process via
reinforcement learning (RL), formulating it as a Partially Observable Markov
Decision Process (POMDP). We find that decomposing the agent's reasoning into
State Estimation ("what is the current state?") and Transition Modeling ("what
comes next?") is critical for success, as demonstrated through five reasoning
strategies. Our investigation into how agents represent internal beliefs
reveals that the optimal representation is task-dependent: Natural Language
excels at capturing semantic relationships in general tasks, while Structured
formats are indispensable for precise manipulation and control. Building on
these insights, we design a World Modeling Reward that provides dense,
turn-level supervision for accurate state prediction, and introduce Bi-Level
General Advantage Estimation (Bi-Level GAE) for turn-aware credit assignment.
Through this form of visual state reasoning, a 3B-parameter model achieves a
score of 0.82 across five diverse agent benchmarks, representing a 3$\times$
improvement over its untrained counterpart (0.21) and outperforming proprietary
reasoning models such as GPT-5 (0.75), Gemini 2.5 Pro (0.67) and Claude 4.5
(0.62). All experiments are conducted within our VAGEN framework, a scalable
system for training and analyzing multi-turn VLM agents in diverse visual
environments. Code and data are publicly available at
https://vagen-ai.github.io.

</details>


### [128] [A Comparative User Evaluation of XRL Explanations using Goal Identification](https://arxiv.org/abs/2510.16956)
*Mark Towers,Yali Du,Christopher Freeman,Timothy J. Norman*

Main category: cs.AI

TL;DR: 该研究提出了一种新的评估方法，测试用户是否能从强化学习算法的解释中识别智能体的目标。在Ms. Pacman环境中测试四种可解释强化学习算法，发现只有一种算法在测试目标上达到了超过随机水平的准确率，且用户普遍高估了自己的选择准确性。


<details>
  <summary>Details</summary>
Motivation: 可解释强化学习算法的核心应用是调试，但目前缺乏对这些算法相对性能的比较评估。

Method: 使用Atari的Ms. Pacman环境和四种可解释强化学习算法，提出新的评估方法测试用户从算法决策解释中识别智能体目标的能力。

Result: 只有一种算法在测试目标上达到了超过随机水平的准确率；用户普遍高估自己的选择准确性；用户自我报告的识别和理解难易程度与他们的实际准确率没有相关性。

Conclusion: 当前可解释强化学习算法的解释效果有限，用户的主观感受与实际识别能力存在差异，需要改进解释方法以提高调试效果。

Abstract: Debugging is a core application of explainable reinforcement learning (XRL)
algorithms; however, limited comparative evaluations have been conducted to
understand their relative performance. We propose a novel evaluation
methodology to test whether users can identify an agent's goal from an
explanation of its decision-making. Utilising the Atari's Ms. Pacman
environment and four XRL algorithms, we find that only one achieved greater
than random accuracy for the tested goals and that users were generally
overconfident in their selections. Further, we find that users' self-reported
ease of identification and understanding for every explanation did not
correlate with their accuracy.

</details>


### [129] [STARK: Strategic Team of Agents for Refining Kernels](https://arxiv.org/abs/2510.16996)
*Juncheng Dong,Yang Yang,Tao Liu,Yang Wang,Feng Qi,Vahid Tarokh,Kaushik Rangadurai,Shuang Yang*

Main category: cs.AI

TL;DR: 提出了一个基于LLM的多智能体框架，用于自动化GPU内核优化，通过系统化探索设计空间，显著提升了内核性能。


<details>
  <summary>Details</summary>
Motivation: GPU内核优化对AI进步至关重要，但现有方法难以处理内存层次、线程调度和硬件特性间的复杂交互。传统LLM方法作为单次生成器或简单优化工具效果有限。

Method: 采用多智能体协作框架，包含基于经验的指令、动态上下文管理和策略搜索，模拟专家工程师工作流程，让LLM能够推理硬件权衡、整合性能分析反馈并迭代优化内核。

Result: 在KernelBench基准测试中，相比基线方法，该系统在基线经常失败的情况下能产生正确解决方案，并实现了高达16倍的运行时性能提升。

Conclusion: 智能体LLM框架在实现完全自动化、可扩展的GPU内核优化方面具有巨大潜力。

Abstract: The efficiency of GPU kernels is central to the progress of modern AI, yet
optimizing them remains a difficult and labor-intensive task due to complex
interactions between memory hierarchies, thread scheduling, and
hardware-specific characteristics. While recent advances in large language
models (LLMs) provide new opportunities for automated code generation, existing
approaches largely treat LLMs as single-shot generators or naive refinement
tools, limiting their effectiveness in navigating the irregular kernel
optimization landscape. We introduce an LLM agentic framework for GPU kernel
optimization that systematically explores the design space through multi-agent
collaboration, grounded instruction, dynamic context management, and strategic
search. This framework mimics the workflow of expert engineers, enabling LLMs
to reason about hardware trade-offs, incorporate profiling feedback, and refine
kernels iteratively. We evaluate our approach on KernelBench, a benchmark for
LLM-based kernel optimization, and demonstrate substantial improvements over
baseline agents: our system produces correct solutions where baselines often
fail, and achieves kernels with up to 16x faster runtime performance. These
results highlight the potential of agentic LLM frameworks to advance fully
automated, scalable GPU kernel optimization.

</details>


### [130] [ToolCritic: Detecting and Correcting Tool-Use Errors in Dialogue Systems](https://arxiv.org/abs/2510.17052)
*Hassan Hamad,Yingru Xu,Liang Zhao,Wenbo Yan,Narendra Gyanchandani*

Main category: cs.AI

TL;DR: ToolCritic是一个诊断框架，用于评估和改进增强工具的大型语言模型在多轮对话中的行为，通过检测8种特定工具调用错误并提供针对性反馈，可将工具调用准确率提升高达13%。


<details>
  <summary>Details</summary>
Motivation: 工具增强的大型语言模型在现实应用中日益普及，但工具使用错误仍然阻碍其可靠性，需要系统性的错误检测和改进机制。

Method: 定义8种工具调用错误类别，构建合成数据集训练ToolCritic，让主LLM基于ToolCritic的反馈修正响应。

Result: 在Schema-Guided Dialogue数据集上的实验表明，ToolCritic相比零样本提示和自校正技术，将工具调用准确率提升高达13%。

Conclusion: ToolCritic是向更稳健的LLM与外部工具集成在现实对话应用中迈出的有希望的一步。

Abstract: Tool-augmented large language models (LLMs) are increasingly employed in
real-world applications, but tool usage errors still hinder their reliability.
We introduce ToolCritic, a diagnostic framework that evaluates and improves LLM
behavior in multi-turn, tool-augmented dialogues. ToolCritic detects eight
distinct error types specific to tool-calling (e.g., premature invocation,
argument misalignment, and misinterpretation of tool outputs) and provides
targeted feedback to the main LLM. The main LLM, assumed to have strong
reasoning, task understanding and orchestration capabilities, then revises its
response based on ToolCritic's feedback. We systematically define these error
categories and construct a synthetic dataset to train ToolCritic. Experimental
results on the Schema-Guided Dialogue (SGD) dataset demonstrate that ToolCritic
improves tool-calling accuracy by up to 13% over baselines, including zero-shot
prompting and self-correction techniques. This represents a promising step
toward more robust LLM integration with external tools in real-world dialogue
applications.

</details>


### [131] [A Brain Cell Type Resource Created by Large Language Models and a Multi-Agent AI System for Collaborative Community Annotation](https://arxiv.org/abs/2510.17064)
*Rongbin Li,Wenbo Chen,Zhao Li,Rodrigo Munoz-Castaneda,Jinbo Li,Neha S. Maurya,Arnav Solanki,Huan He,Hanwen Xing,Meaghan Ramlakhan,Zachary Wise,Zhuhao Wu,Hua Xu,Michael Hawrylycz,W. Jim Zheng*

Main category: cs.AI

TL;DR: BRAINCELL-AID是一个多智能体AI系统，通过整合自由文本描述和本体标签，结合检索增强生成技术，显著提高了基因集注释的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 单细胞RNA测序技术虽然能识别多样细胞类型，但对涉及特征不明确基因的转录组特征进行注释仍然是一个重大挑战。传统方法如GSEA依赖精心策划的注释，在这些情境下表现不佳。

Method: 开发了BRAINCELL-AID多智能体AI系统，整合自由文本描述与本体标签，采用检索增强生成(RAG)技术构建稳健的智能体工作流，利用相关PubMed文献精炼预测。

Result: 在鼠类基因集注释中，77%的基因集在其前几位预测中获得了正确注释。应用该方法注释了BRAIN Initiative Cell Census Network生成的5,322个脑细胞簇，识别了区域特异性基因共表达模式，并推断基因集合的功能角色。

Conclusion: BRAINCELL-AID创建了一个有价值的资源来支持社区驱动的细胞类型注释，能够识别具有神经学意义描述的基底神经节相关细胞类型。

Abstract: Single-cell RNA sequencing has transformed our ability to identify diverse
cell types and their transcriptomic signatures. However, annotating these
signatures-especially those involving poorly characterized genes-remains a
major challenge. Traditional methods, such as Gene Set Enrichment Analysis
(GSEA), depend on well-curated annotations and often perform poorly in these
contexts. Large Language Models (LLMs) offer a promising alternative but
struggle to represent complex biological knowledge within structured
ontologies. To address this, we present BRAINCELL-AID (BRAINCELL-AID:
https://biodataai.uth.edu/BRAINCELL-AID), a novel multi-agent AI system that
integrates free-text descriptions with ontology labels to enable more accurate
and robust gene set annotation. By incorporating retrieval-augmented generation
(RAG), we developed a robust agentic workflow that refines predictions using
relevant PubMed literature, reducing hallucinations and enhancing
interpretability. Using this workflow, we achieved correct annotations for 77%
of mouse gene sets among their top predictions. Applying this approach, we
annotated 5,322 brain cell clusters from the comprehensive mouse brain cell
atlas generated by the BRAIN Initiative Cell Census Network, enabling novel
insights into brain cell function by identifying region-specific gene
co-expression patterns and inferring functional roles of gene ensembles.
BRAINCELL-AID also identifies Basal Ganglia-related cell types with
neurologically meaningful descriptions. Hence, we create a valuable resource to
support community-driven cell type annotation.

</details>


### [132] [Structured Debate Improves Corporate Credit Reasoning in Financial AI](https://arxiv.org/abs/2510.17108)
*Yoonjin Lee,Munhee Kim,Hanbi Choi,Juhyeon Park,Seungho Lyoo,Woojin Park*

Main category: cs.AI

TL;DR: 开发了两种基于大语言模型的系统用于企业信用评估中的证据推理：单代理系统(NAS)和基于辩论的多代理系统(KPD-MADS)，后者在推理质量上表现更优。


<details>
  <summary>Details</summary>
Motivation: 解决企业信用评估中定性非财务指标的自动化推理问题，现有方法主要关注数值预测，缺乏对专业贷款评估所需的解释性判断支持。

Method: 开发了两种LLM系统：单代理系统(NAS)通过单次推理管道产生双向分析；多代理系统(KPD-MADS)基于卡尔·波普批判对话框架，采用十步结构化交互协议进行对抗性验证。

Result: 两个系统都实现了显著的生产力提升(NAS:11.55秒/案例；KPD-MADS:91.97秒；人工基线:1920秒)。KPD-MADS在解释充分性(4.0 vs 3.0)、实际适用性(4.0 vs 3.0)和可用性(62.5 vs 52.5)方面获得更高评分。

Conclusion: 结构化多代理交互可以增强金融AI中的推理严谨性和可解释性，推动企业信用评估的可扩展和可辩护自动化。

Abstract: Despite advances in financial AI, the automation of evidence-based reasoning
remains unresolved in corporate credit assessment, where qualitative
non-financial indicators exert decisive influence on loan repayment outcomes
yet resist formalization. Existing approaches focus predominantly on numerical
prediction and provide limited support for the interpretive judgments required
in professional loan evaluation. This study develops and evaluates two
operational large language model (LLM)-based systems designed to generate
structured reasoning from non-financial evidence. The first is a
non-adversarial single-agent system (NAS) that produces bidirectional analysis
through a single-pass reasoning pipeline. The second is a debate-based
multi-agent system (KPD-MADS) that operationalizes adversarial verification
through a ten-step structured interaction protocol grounded in Karl Popper's
critical dialogue framework. Both systems were applied to three real corporate
cases and evaluated by experienced credit risk professionals. Compared to
manual expert reporting, both systems achieved substantial productivity gains
(NAS: 11.55 s per case; KPD-MADS: 91.97 s; human baseline: 1920 s). The
KPD-MADS demonstrated superior reasoning quality, receiving higher median
ratings in explanatory adequacy (4.0 vs. 3.0), practical applicability (4.0 vs.
3.0), and usability (62.5 vs. 52.5). These findings show that structured
multi-agent interaction can enhance reasoning rigor and interpretability in
financial AI, advancing scalable and defensible automation in corporate credit
assessment.

</details>


### [133] [Enhanced Fish Freshness Classification with Incremental Handcrafted Feature Fusion](https://arxiv.org/abs/2510.17145)
*Phi-Hung Hoang,Nam-Thuan Trinh,Van-Manh Tran,Thi-Thu-Hong Phan*

Main category: cs.AI

TL;DR: 提出了一种基于手工特征的方法，通过融合颜色统计、多色彩空间直方图以及纹理特征来评估鱼类新鲜度，在FFE数据集上取得了显著优于深度学习的性能提升。


<details>
  <summary>Details</summary>
Motivation: 传统感官评估鱼类新鲜度存在主观性、不一致性和难以标准化的问题，需要客观、可靠的自动化评估方法。

Method: 从鱼眼图像中系统提取和增量融合互补描述符，包括颜色统计、多色彩空间直方图以及LBP和GLCM纹理特征，同时捕获全局色彩变化和局部退化特征。

Result: 在标准训练-测试设置下，LightGBM分类器达到77.56%准确率，比之前深度学习基线提升14.35%；使用增强数据时，ANN达到97.16%准确率，比之前最佳结果提升19.86%。

Conclusion: 精心设计的手工特征经过策略性处理后，能够为自动化鱼类新鲜度评估提供稳健、可解释且可靠的解决方案，对食品质量监测具有实际应用价值。

Abstract: Accurate assessment of fish freshness remains a major challenge in the food
industry, with direct consequences for product quality, market value, and
consumer health. Conventional sensory evaluation is inherently subjective,
inconsistent, and difficult to standardize across contexts, often limited by
subtle, species-dependent spoilage cues. To address these limitations, we
propose a handcrafted feature-based approach that systematically extracts and
incrementally fuses complementary descriptors, including color statistics,
histograms across multiple color spaces, and texture features such as Local
Binary Patterns (LBP) and Gray-Level Co-occurrence Matrices (GLCM), from fish
eye images. Our method captures global chromatic variations from full images
and localized degradations from ROI segments, fusing each independently to
evaluate their effectiveness in assessing freshness. Experiments on the
Freshness of the Fish Eyes (FFE) dataset demonstrate the approach's
effectiveness: in a standard train-test setting, a LightGBM classifier achieved
77.56% accuracy, a 14.35% improvement over the previous deep learning baseline
of 63.21%. With augmented data, an Artificial Neural Network (ANN) reached
97.16% accuracy, surpassing the prior best of 77.3% by 19.86%. These results
demonstrate that carefully engineered, handcrafted features, when strategically
processed, yield a robust, interpretable, and reliable solution for automated
fish freshness assessment, providing valuable insights for practical
applications in food quality monitoring.

</details>


### [134] [Physics-Informed Large Language Models for HVAC Anomaly Detection with Autonomous Rule Generation](https://arxiv.org/abs/2510.17146)
*Subin Lin,Chuanbo Hua*

Main category: cs.AI

TL;DR: PILLM是一个基于物理知识的大语言模型框架，通过进化循环自动生成、评估和优化HVAC系统异常检测规则，结合热力学和控制理论约束，实现可解释且物理合理的异常检测。


<details>
  <summary>Details</summary>
Motivation: HVAC系统能耗占建筑能耗很大比例，传统规则方法可解释但缺乏适应性，深度学习方法预测能力强但缺乏透明度和物理合理性。现有LLM方法改善了可解释性但忽略了HVAC运行的物理原理。

Method: 提出PILLM框架，在进化循环中嵌入物理知识反射和交叉算子，结合热力学和控制理论约束，自动生成和优化异常检测规则。

Result: 在公共建筑故障检测数据集上的实验表明，PILLM实现了最先进的性能，同时产生可解释和可操作的诊断规则。

Conclusion: PILLM推进了智能建筑系统中可信赖和可部署AI的发展，实现了既适应性强又物理基础扎实的异常检测。

Abstract: Heating, Ventilation, and Air-Conditioning (HVAC) systems account for a
substantial share of global building energy use, making reliable anomaly
detection essential for improving efficiency and reducing emissions. Classical
rule-based approaches offer explainability but lack adaptability, while deep
learning methods provide predictive power at the cost of transparency,
efficiency, and physical plausibility. Recent attempts to use Large Language
Models (LLMs) for anomaly detection improve interpretability but largely ignore
the physical principles that govern HVAC operations. We present PILLM, a
Physics-Informed LLM framework that operates within an evolutionary loop to
automatically generate, evaluate, and refine anomaly detection rules. Our
approach introduces physics-informed reflection and crossover operators that
embed thermodynamic and control-theoretic constraints, enabling rules that are
both adaptive and physically grounded. Experiments on the public Building Fault
Detection dataset show that PILLM achieves state-of-the-art performance while
producing diagnostic rules that are interpretable and actionable, advancing
trustworthy and deployable AI for smart building systems.

</details>


### [135] [Which LLM Multi-Agent Protocol to Choose?](https://arxiv.org/abs/2510.17149)
*Hongyi Du,Jiaqi Su,Jisen Li,Lijie Ding,Yingxuan Yang,Peixuan Han,Xiangru Tang,Kunlun Zhu,Jiaxuan You*

Main category: cs.AI

TL;DR: ProtocolBench是一个用于系统评估多智能体系统通信协议的基准测试，包含四个可测量维度：任务成功率、端到端延迟、消息/字节开销和故障恢复能力。研究还提出了ProtocolRouter，一个可学习的协议路由器，能够根据场景需求选择最优协议。


<details>
  <summary>Details</summary>
Motivation: 大规模多智能体系统中，通信协议层是影响性能和可靠性的关键因素，但目前协议选择缺乏标准化指导，往往基于直觉而非系统评估。

Method: 开发ProtocolBench基准测试系统，从四个维度对比不同协议性能；提出ProtocolRouter协议路由器，通过学习算法为不同场景选择最优协议。

Result: 协议选择显著影响系统行为：在Streaming Queue场景中，总体完成时间差异达36.5%，端到端延迟差异达3.48秒；ProtocolRouter相比最佳单协议基线，故障恢复时间减少18.1%，在GAIA场景中成功率更高。

Conclusion: 协议选择对多智能体系统性能有重大影响，ProtocolBench和ProtocolRouter为协议评估和选择提供了标准化方法，能够显著提升系统可靠性和性能。

Abstract: As large-scale multi-agent systems evolve, the communication protocol layer
has become a critical yet under-evaluated factor shaping performance and
reliability. Despite the existence of diverse protocols (A2A, ACP, ANP, Agora,
etc.), selection is often intuition-driven and lacks standardized guidance. We
introduce ProtocolBench, a benchmark that systematically compares agent
protocols along four measurable axes: task success, end-to-end latency, message
or byte overhead, and robustness under failures. On ProtocolBench, protocol
choice significantly influences system behavior. In the Streaming Queue
scenario, overall completion time varies by up to 36.5% across protocols, and
mean end-to-end latency differs by 3.48 s. Under Fail-Storm Recovery,
resilience also differs consistently across protocols. Beyond evaluation, we
present ProtocolRouter, a learnable protocol router that selects per-scenario
(or per-module) protocols from requirement and runtime signals. ProtocolRouter
reduces Fail-Storm recovery time by up to 18.1% versus the best single-protocol
baseline, and achieves scenario-specific gains such as higher success in GAIA.
We also release ProtocolRouterBench to standardize protocol evaluation and
improve reliability at scale.

</details>


### [136] [Combining ECG Foundation Model and XGBoost to Predict In-Hospital Malignant Ventricular Arrhythmias in AMI Patients](https://arxiv.org/abs/2510.17172)
*Shun Huang,Wenlu Xing,Shijia Geng,Hailong Wang,Guangkun Nie,Gongzheng Tang,Chenyang He,Shenda Hong*

Main category: cs.AI

TL;DR: 开发了一个结合ECG基础模型和可解释XGBoost分类器的混合预测框架，用于预测急性心肌梗死后恶性室性心律失常风险，在准确性和可解释性方面均优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 急性心肌梗死后恶性室性心律失常是院内死亡的主要原因，传统风险评分性能有限，端到端深度学习模型缺乏临床信任所需的可解释性。

Method: 使用ECG基础模型提取150维诊断概率特征，通过特征选择后训练XGBoost分类器，采用SHAP方法进行可解释性分析。

Result: 混合模型AUC达到0.801，优于KNN(0.677)、RNN(0.676)和1D-CNN(0.720)，SHAP分析显示模型识别特征与临床知识高度一致。

Conclusion: 该混合框架为基础模型输出作为有效自动化特征工程提供了验证，为构建可信赖、可解释的AI临床决策支持系统提供了新范式。

Abstract: Malignant ventricular arrhythmias (VT/VF) following acute myocardial
infarction (AMI) are a major cause of in-hospital death, yet early
identification remains a clinical challenge. While traditional risk scores have
limited performance, end-to-end deep learning models often lack the
interpretability needed for clinical trust. This study aimed to develop a
hybrid predictive framework that integrates a large-scale electrocardiogram
(ECG) foundation model (ECGFounder) with an interpretable XGBoost classifier to
improve both accuracy and interpretability. We analyzed 6,634 ECG recordings
from AMI patients, among whom 175 experienced in-hospital VT/VF. The ECGFounder
model was used to extract 150-dimensional diagnostic probability features ,
which were then refined through feature selection to train the XGBoost
classifier. Model performance was evaluated using AUC and F1-score , and the
SHAP method was used for interpretability. The ECGFounder + XGBoost hybrid
model achieved an AUC of 0.801 , outperforming KNN (AUC 0.677), RNN (AUC
0.676), and an end-to-end 1D-CNN (AUC 0.720). SHAP analysis revealed that
model-identified key features, such as "premature ventricular complexes" (risk
predictor) and "normal sinus rhythm" (protective factor), were highly
consistent with clinical knowledge. We conclude that this hybrid framework
provides a novel paradigm for VT/VF risk prediction by validating the use of
foundation model outputs as effective, automated feature engineering for
building trustworthy, explainable AI-based clinical decision support systems.

</details>


### [137] [Offline Policy Evaluation of Multi-Turn LLM Health Coaching with Real Users](https://arxiv.org/abs/2510.17173)
*Melik Ozolcer,Sang Won Bae*

Main category: cs.AI

TL;DR: 研究显示，统一的工具密集型策略虽然能提升整体价值，但会对特定用户群体（特别是健康素养低但自我效能高的用户）造成伤害。通过轻量级模拟器发现，添加早期信息增益奖励可以缩短特质识别时间并提高目标成功率。


<details>
  <summary>Details</summary>
Motivation: 研究网络部署的、工具增强的LLM健康教练在真实用户中的表现，探索个性化策略对用户群体的影响，特别是避免平均指标掩盖的群体伤害。

Method: 使用离线策略评估（OPE）分析因子化决策头（工具/风格），通过轻量级模拟器测试添加早期信息增益奖励的效果，采用基于类型化奖励（客观工具结果和满意度）的子群体感知决策头学习。

Result: 统一的重工具策略在日志上提高平均价值但伤害特定子群体；添加小规模早期信息增益奖励可靠地缩短特质识别时间，提高目标成功率和pass@3指标。

Conclusion: 提出评估优先的个性化路径：冻结生成器，在类型化奖励上学习子群体感知决策头，始终报告每个原型指标以揭示平均指标掩盖的子群体伤害。

Abstract: We study a web-deployed, tool-augmented LLM health coach with real users. In
a pilot with seven users (280 rated turns), offline policy evaluation (OPE)
over factorized decision heads (Tool/Style) shows that a uniform heavy-tool
policy raises average value on logs but harms specific subgroups, most notably
low-health-literacy/high-self-efficacy users. A lightweight simulator with
hidden archetypes further shows that adding a small early information-gain
bonus reliably shortens trait identification and improves goal success and
pass@3. Together, these early findings indicate an evaluation-first path to
personalization: freeze the generator, learn subgroup-aware decision heads on
typed rewards (objective tool outcomes and satisfaction), and always report
per-archetype metrics to surface subgroup harms that averages obscure.

</details>


### [138] [Temporally Detailed Hypergraph Neural ODEs for Type 2 Diabetes Progression Modeling](https://arxiv.org/abs/2510.17211)
*Tingsong Xiao,Yao An Lee,Zelin Xu,Yupu Zhang,Zibo Liu,Yu Huang,Jiang Bian,Serena Jingchuan Guo,Zhe Jiang*

Main category: cs.AI

TL;DR: 提出了TD-HNODE模型，通过时间详细超图和神经ODE框架学习疾病进展的连续时间动态，在2型糖尿病和心血管疾病进展建模中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 准确建模疾病进展（如2型糖尿病）可以改善患者亚型分型和及时干预，但现有方法难以处理不规则时间采样和患者异质性，无法捕捉复杂的连续时间动态。

Method: TD-HNODE将疾病进展表示为时间详细超图，通过可学习的TD-Hypergraph Laplacian捕捉疾病并发症标记在进展轨迹内和轨迹间的相互依赖关系，使用神经ODE框架学习连续时间进展动态。

Result: 在两个真实世界临床数据集上的实验表明，TD-HNODE在建模2型糖尿病和相关心血管疾病进展方面优于多个基线方法。

Conclusion: TD-HNODE能够有效捕捉疾病进展的连续时间动态，解决了不规则时间采样和患者异质性的挑战，为疾病进展建模提供了更准确的解决方案。

Abstract: Disease progression modeling aims to characterize and predict how a patient's
disease complications worsen over time based on longitudinal electronic health
records (EHRs). Accurate modeling of disease progression, such as type 2
diabetes, can enhance patient sub-phenotyping and inform effective and timely
interventions. However, the problem is challenging due to the need to learn
continuous-time dynamics of progression patterns based on irregular-time event
samples and patient heterogeneity (\eg different progression rates and
pathways). Existing mechanistic and data-driven methods either lack
adaptability to learn from real-world data or fail to capture complex
continuous-time dynamics on progression trajectories. To address these
limitations, we propose Temporally Detailed Hypergraph Neural Ordinary
Differential Equation (TD-HNODE), which represents disease progression on
clinically recognized trajectories as a temporally detailed hypergraph and
learns the continuous-time progression dynamics via a neural ODE framework.
TD-HNODE contains a learnable TD-Hypergraph Laplacian that captures the
interdependency of disease complication markers within both intra- and
inter-progression trajectories. Experiments on two real-world clinical datasets
demonstrate that TD-HNODE outperforms multiple baselines in modeling the
progression of type 2 diabetes and related cardiovascular diseases.

</details>


### [139] [Coinvisor: An RL-Enhanced Chatbot Agent for Interactive Cryptocurrency Investment Analysis](https://arxiv.org/abs/2510.17235)
*Chong Chen,Ze Liu,Lingfeng Bao,Yanlin Wang,Ting Chen,Daoyuan Wu,Jiachi Chen*

Main category: cs.AI

TL;DR: Coinvisor是一个基于强化学习的加密货币投资分析聊天机器人，通过多智能体框架提供全面的分析支持，解决了现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 加密货币市场存在高波动性和信息碎片化问题，现有分析方法包括手动分析、数据聚合平台和LLM代理都存在各种不足，需要更智能的实时分析工具。

Method: 采用基于强化学习的工具选择机制和多智能体框架，集成多样化分析能力，支持多步规划和实时数据整合。

Result: 在工具编排方面，相比基础模型召回率提升40.7%，F1分数提升26.6%；用户研究显示高满意度(4.64/5)，参与者更偏好Coinvisor而非通用LLM和现有平台(4.62/5)。

Conclusion: Coinvisor通过强化学习驱动的多智能体框架，有效提升了加密货币投资分析的准确性和用户体验，为投资者提供了更可靠的分析工具。

Abstract: The cryptocurrency market offers significant investment opportunities but
faces challenges including high volatility and fragmented information. Data
integration and analysis are essential for informed investment decisions.
Currently, investors use three main approaches: (1) Manual analysis across
various sources, which depends heavily on individual experience and is
time-consuming and prone to bias; (2) Data aggregation platforms-limited in
functionality and depth of analysis; (3) Large language model agents-based on
static pretrained models, lacking real-time data integration and multi-step
reasoning capabilities. To address these limitations, we present Coinvisor, a
reinforcement learning-based chatbot that provides comprehensive analytical
support for cryptocurrency investment through a multi-agent framework.
Coinvisor integrates diverse analytical capabilities through specialized tools.
Its key innovation is a reinforcement learning-based tool selection mechanism
that enables multi-step planning and flexible integration of diverse data
sources. This design supports real-time interaction and adaptive analysis of
dynamic content, delivering accurate and actionable investment insights. We
evaluated Coinvisor through automated benchmarks on tool calling accuracy and
user studies with 20 cryptocurrency investors using our interface. Results show
that Coinvisor improves recall by 40.7% and F1 score by 26.6% over the base
model in tool orchestration. User studies show high satisfaction (4.64/5), with
participants preferring Coinvisor to both general LLMs and existing crypto
platforms (4.62/5).

</details>


### [140] [RubiSCoT: A Framework for AI-Supported Academic Assessment](https://arxiv.org/abs/2510.17309)
*Thorsten Fröhlich,Tim Schlippe*

Main category: cs.AI

TL;DR: RubiSCoT是一个AI支持的论文评估框架，使用NLP技术和大语言模型，从提案到最终提交提供一致、可扩展的评估解决方案。


<details>
  <summary>Details</summary>
Motivation: 传统论文评估方法耗时且受评估者主观性影响，需要更高效、一致的评估方案。

Method: 使用先进自然语言处理技术，包括大语言模型、检索增强生成和结构化思维链提示，提供初步评估、多维评估、内容提取、基于评分标准的评分和详细报告。

Result: 开发了RubiSCoT框架的设计和实现，展示了其在优化学术评估过程中的潜力。

Conclusion: RubiSCoT有潜力通过一致、可扩展和透明的评估来优化学术评估流程。

Abstract: The evaluation of academic theses is a cornerstone of higher education,
ensuring rigor and integrity. Traditional methods, though effective, are
time-consuming and subject to evaluator variability. This paper presents
RubiSCoT, an AI-supported framework designed to enhance thesis evaluation from
proposal to final submission. Using advanced natural language processing
techniques, including large language models, retrieval-augmented generation,
and structured chain-of-thought prompting, RubiSCoT offers a consistent,
scalable solution. The framework includes preliminary assessments,
multidimensional assessments, content extraction, rubric-based scoring, and
detailed reporting. We present the design and implementation of RubiSCoT,
discussing its potential to optimize academic assessment processes through
consistent, scalable, and transparent evaluation.

</details>


### [141] [Graph Attention-Guided Search for Dense Multi-Agent Pathfinding](https://arxiv.org/abs/2510.17382)
*Rishabh Jain,Keisuke Okumura,Michael Amir,Amanda Prorok*

Main category: cs.AI

TL;DR: 提出LaGAT混合框架，将基于图注意力的神经网络启发式(MAGAT)集成到搜索算法LaCAM中，在密集多智能体路径规划任务中优于纯搜索和纯学习方法。


<details>
  <summary>Details</summary>
Motivation: 密集多智能体路径规划问题在实时场景中仍具挑战性，现有学习方法在MAPF中表现不佳，需要结合学习和搜索的优势。

Method: 增强MAGAT架构，采用预训练-微调策略，集成死锁检测机制，将学习到的启发式融入LaCAM搜索算法。

Result: LaGAT在密集场景中超越了纯搜索和纯学习方法，证明了混合搜索在复杂多智能体协调问题中的有效性。

Conclusion: 精心设计的混合搜索为紧密耦合的挑战性多智能体协调问题提供了强大解决方案。

Abstract: Finding near-optimal solutions for dense multi-agent pathfinding (MAPF)
problems in real-time remains challenging even for state-of-the-art planners.
To this end, we develop a hybrid framework that integrates a learned heuristic
derived from MAGAT, a neural MAPF policy with a graph attention scheme, into a
leading search-based algorithm, LaCAM. While prior work has explored
learning-guided search in MAPF, such methods have historically underperformed.
In contrast, our approach, termed LaGAT, outperforms both purely search-based
and purely learning-based methods in dense scenarios. This is achieved through
an enhanced MAGAT architecture, a pre-train-then-fine-tune strategy on maps of
interest, and a deadlock detection scheme to account for imperfect neural
guidance. Our results demonstrate that, when carefully designed, hybrid search
offers a powerful solution for tightly coupled, challenging multi-agent
coordination problems.

</details>


### [142] [Diverse Planning with Simulators via Linear Temporal Logic](https://arxiv.org/abs/2510.17418)
*Mustafa F. Abdelwahed,Alice Toniolo,Joan Espasa,Ian P. Gent*

Main category: cs.AI

TL;DR: 提出FBI_LTL，一种用于模拟规划问题的多样化规划器，使用线性时序逻辑定义语义多样性标准，生成语义多样化的计划


<details>
  <summary>Details</summary>
Motivation: 传统规划器只生成单一计划，可能无法满足代理偏好；现有多样化规划方法可能产生语法不同但语义相同的解决方案

Method: 使用线性时序逻辑定义语义多样性标准，并将这些多样性模型直接集成到搜索过程中

Result: 在各种基准测试中，FBI_LTL相比基线方法能生成更多样化的计划

Conclusion: 这项工作确立了在模拟环境中进行语义引导多样化规划的可行性，为传统基于模型方法失效的现实非符号领域开辟了新途径

Abstract: Autonomous agents rely on automated planning algorithms to achieve their
objectives. Simulation-based planning offers a significant advantage over
declarative models in modelling complex environments. However, relying solely
on a planner that produces a single plan may not be practical, as the generated
plans may not always satisfy the agent's preferences. To address this
limitation, we introduce $\texttt{FBI}_\texttt{LTL}$, a diverse planner
explicitly designed for simulation-based planning problems.
$\texttt{FBI}_\texttt{LTL}$ utilises Linear Temporal Logic (LTL) to define
semantic diversity criteria, enabling agents to specify what constitutes
meaningfully different plans. By integrating these LTL-based diversity models
directly into the search process, $\texttt{FBI}_\texttt{LTL}$ ensures the
generation of semantically diverse plans, addressing a critical limitation of
existing diverse planning approaches that may produce syntactically different
but semantically identical solutions. Extensive evaluations on various
benchmarks consistently demonstrate that $\texttt{FBI}_\texttt{LTL}$ generates
more diverse plans compared to a baseline approach. This work establishes the
feasibility of semantically-guided diverse planning in simulation-based
environments, paving the way for innovative approaches in realistic,
non-symbolic domains where traditional model-based approaches fail.

</details>


### [143] [Active Inference for an Intelligent Agent in Autonomous Reconnaissance Missions](https://arxiv.org/abs/2510.17450)
*Johan Schubert,Farzad Kamrani,Tove Gustavi*

Main category: cs.AI

TL;DR: 提出了一种基于主动推理的路径规划方法，用于自主控制智能代理在特定地理区域内进行侦察，以维持共同作战态势图。


<details>
  <summary>Details</summary>
Motivation: 解决智能代理在地理区域侦察中的探索与利用平衡问题，通过构建证据地图来反映当前态势理解，并持续更新以跟踪目标对象。

Method: 结合Dempster-Shafer理论和高斯传感器模型构建生成模型，使用贝叶斯方法更新后验概率分布，通过计算变分自由能量来指导代理移动，选择最小化自由能量的位置。

Result: 该方法能够有效平衡对地理区域的大范围搜索与对已识别目标对象的跟踪，通过证据扩散和自由能量最小化实现智能路径规划。

Conclusion: 主动推理路径规划方法为自主智能代理提供了有效的侦察策略，成功解决了探索与利用的权衡问题，在模拟中表现出良好的性能。

Abstract: We develop an active inference route-planning method for the autonomous
control of intelligent agents. The aim is to reconnoiter a geographical area to
maintain a common operational picture. To achieve this, we construct an
evidence map that reflects our current understanding of the situation,
incorporating both positive and "negative" sensor observations of possible
target objects collected over time, and diffusing the evidence across the map
as time progresses. The generative model of active inference uses
Dempster-Shafer theory and a Gaussian sensor model, which provides input to the
agent. The generative process employs a Bayesian approach to update a posterior
probability distribution. We calculate the variational free energy for all
positions within the area by assessing the divergence between a pignistic
probability distribution of the evidence map and a posterior probability
distribution of a target object based on the observations, including the level
of surprise associated with receiving new observations. Using the free energy,
we direct the agents' movements in a simulation by taking an incremental step
toward a position that minimizes the free energy. This approach addresses the
challenge of exploration and exploitation, allowing agents to balance searching
extensive areas of the geographical map while tracking identified target
objects.

</details>


### [144] [Label Indeterminacy in AI & Law](https://arxiv.org/abs/2510.17463)
*Cor Steging,Tadeusz Zbiegień*

Main category: cs.AI

TL;DR: 法律机器学习需要处理标签不确定性，因为法律结果往往受到人为干预影响，导致同一案件可能有不同结果。


<details>
  <summary>Details</summary>
Motivation: 法律机器学习通常将过去案件结果视为绝对真实，但法律结果常受和解、上诉等人为干预影响，造成标签不确定性。

Method: 在欧洲人权法院案件分类中，研究标签构建方式如何影响模型行为，使用现有方法估算不确定标签。

Result: 训练过程中标签的构建方式会显著影响模型行为，不同标签处理方法导致不同模型表现。

Conclusion: 标签不确定性是AI与法律领域的重要关注点，需要在法律机器学习应用中予以考虑。

Abstract: Machine learning is increasingly used in the legal domain, where it typically
operates retrospectively by treating past case outcomes as ground truth.
However, legal outcomes are often shaped by human interventions that are not
captured in most machine learning approaches. A final decision may result from
a settlement, an appeal, or other procedural actions. This creates label
indeterminacy: the outcome could have been different if the intervention had or
had not taken place. We argue that legal machine learning applications need to
account for label indeterminacy. Methods exist that can impute these
indeterminate labels, but they are all grounded in unverifiable assumptions. In
the context of classifying cases from the European Court of Human Rights, we
show that the way that labels are constructed during training can significantly
affect model behaviour. We therefore position label indeterminacy as a relevant
concern in AI & Law and demonstrate how it can shape model behaviour.

</details>


### [145] [MIRAGE: Agentic Framework for Multimodal Misinformation Detection with Web-Grounded Reasoning](https://arxiv.org/abs/2510.17590)
*Mir Nafis Sharear Shopnil,Sharad Duwal,Abhishek Tyagi,Adiba Mahbub Proma*

Main category: cs.AI

TL;DR: MIRAGE是一个推理时、模型可插拔的代理框架，通过分解多模态验证为四个模块来检测网络虚假信息，无需领域特定训练数据即可达到监督检测器的性能。


<details>
  <summary>Details</summary>
Motivation: 网络平台上每天有数十亿结合文本和图像的多模态帖子传播虚假信息，超出人工事实核查能力。监督检测模型需要领域特定训练数据，且无法泛化到多样化的操纵策略。

Method: MIRAGE框架将多模态验证分解为四个顺序模块：视觉真实性评估检测AI生成图像、跨模态一致性分析识别上下文不当使用、检索增强的事实检查通过迭代问题生成将声明基于网络证据、校准判断模块整合所有信号。

Result: 在MMFakeBench验证集上，MIRAGE与GPT-4o-mini组合达到81.65% F1分数和75.1%准确率，比最强的零样本基线（GPT-4V与MMD-Agent的74.0% F1）高出7.65点，同时保持34.3%的假阳性率，远低于仅判断基线的97.3%。测试集结果确认了泛化能力。

Conclusion: 分解的代理推理与网络检索相结合，可以在没有领域特定训练的情况下匹配监督检测器的性能，在标记数据稀缺的多模态场景中实现虚假信息检测。

Abstract: Misinformation spreads across web platforms through billions of daily
multimodal posts that combine text and images, overwhelming manual
fact-checking capacity. Supervised detection models require domain-specific
training data and fail to generalize across diverse manipulation tactics. We
present MIRAGE, an inference-time, model-pluggable agentic framework that
decomposes multimodal verification into four sequential modules: visual
veracity assessment detects AI-generated images, cross-modal consistency
analysis identifies out-of-context repurposing, retrieval-augmented factual
checking grounds claims in web evidence through iterative question generation,
and a calibrated judgment module integrates all signals. MIRAGE orchestrates
vision-language model reasoning with targeted web retrieval, outputs structured
and citation-linked rationales. On MMFakeBench validation set (1,000 samples),
MIRAGE with GPT-4o-mini achieves 81.65% F1 and 75.1% accuracy, outperforming
the strongest zero-shot baseline (GPT-4V with MMD-Agent at 74.0% F1) by 7.65
points while maintaining 34.3% false positive rate versus 97.3% for a
judge-only baseline. Test set results (5,000 samples) confirm generalization
with 81.44% F1 and 75.08% accuracy. Ablation studies show visual verification
contributes 5.18 F1 points and retrieval-augmented reasoning contributes 2.97
points. Our results demonstrate that decomposed agentic reasoning with web
retrieval can match supervised detector performance without domain-specific
training, enabling misinformation detection across modalities where labeled
data remains scarce.

</details>


### [146] [Reasoning Distillation and Structural Alignment for Improved Code Generation](https://arxiv.org/abs/2510.17598)
*Amir Jalilifard,Anderson de Rezende Rocha,Marcos Medeiros Raimundo*

Main category: cs.AI

TL;DR: 将大型语言模型的推理能力蒸馏到更小、更高效的模型中，通过结构感知损失优化方法，使小模型能够理解问题与解决方案之间的结构对应关系，从而提升代码生成质量。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型具有强大的推理能力，但部署成本高且效率低。小模型缺乏这种推理能力，因此需要将大模型的推理能力蒸馏到小模型中，实现高效且低成本的代码生成。

Method: 采用结构感知损失优化方法，训练小模型模拟大模型的推理和问题解决能力，学习识别正确的解决路径，建立问题定义与潜在解决方案之间的结构对应关系。

Result: 在MBPP、MBPP Plus和HumanEval基准测试中，经过微调的模型在pass@1、平均数据流和平均语法匹配指标上显著优于基线模型。

Conclusion: 通过简单且低成本的蒸馏过程，可以成功将大模型的推理能力转移到小模型中，使其在代码生成任务上表现更好，同时保持部署效率和成本优势。

Abstract: Effective code generation with language models hinges on two critical
factors: accurately understanding the intent of the prompt and generating code
that applies algorithmic reasoning to produce correct solutions capable of
passing diverse test cases while adhering to the syntax of the target
programming language. Unlike other language tasks, code generation requires
more than accurate token prediction; it demands comprehension of solution-level
and structural relationships rather than merely generating the most likely
tokens. very large language model (VLLM) are capable of generating detailed
steps toward the correct solution of complex tasks where reasoning is crucial
in solving the problem. Such reasoning capabilities may be absent in smaller
language models. Therefore, in this work, we distill the reasoning capabilities
of a VLLM into a smaller, more efficient model that is faster and cheaper to
deploy. Our approach trains the model to emulate the reasoning and
problem-solving abilities of the VLLM by learning to identify correct solution
pathways and establishing a structural correspondence between problem
definitions and potential solutions through a novel method of structure-aware
loss optimization. This enables the model to transcend token-level generation
and to deeply grasp the overarching structure of solutions for given problems.
Experimental results show that our fine-tuned model, developed through a cheap
and simple to implement process, significantly outperforms our baseline model
in terms of pass@1, average data flow, and average syntax match metrics across
the MBPP, MBPP Plus, and HumanEval benchmarks.

</details>


### [147] [OG-Rank: Learning to Rank Fast and Slow with Uncertainty and Reward-Trend Guided Adaptive Exploration](https://arxiv.org/abs/2510.17614)
*Praphul Singh,Corey Barrett,Sumana Srivasta,Irfan Bulu,Sri Gadde,Krishnaram Kenthapadi*

Main category: cs.AI

TL;DR: OG-Rank是一个低延迟的解码器重排序系统，通过单次评分和不确定性门控解释步骤，在保持可预测延迟的同时提供排名和选择性解释。


<details>
  <summary>Details</summary>
Motivation: 临床医生需要实时工作并能解释其选择的排名系统，需要低延迟的解码器重排序方法。

Method: 采用单解码器方法，结合池化首词评分信号和不确定性门控解释步骤，通过课程学习专注于困难案例训练。

Result: 在临床医嘱选择任务中表现优异（快速路径：Recall@1~0.45，nDCG@20~0.625），当门控激活时性能进一步提升（Recall@1~0.56，nDCG@20~0.699，门控率45%）。

Conclusion: OG-Rank提供了一个实用方案：默认快速排名，在需要时提供解释，这种模式适用于需要选择性生成以提高准确性的决策任务。

Abstract: Clinicians need ranking systems that work in real time and still justify
their choices. Motivated by the need for a low-latency, decoder-based reranker,
we present OG-Rank, a single-decoder approach that pairs a pooled first-token
scoring signal with an uncertainty-gated explanation step. The model scores all
candidates in one pass and generates a brief, structured rationale only when
the list is genuinely ambiguous, keeping latency predictable. Trained with a
curriculum that concentrates effort on hard cases, OG-Rank delivers strong
effectiveness on encounter-scoped order selection (fast path: Recall@1~0.45,
nDCG@20~0.625) and improves further when the gate activates (Recall@1~0.56,
nDCG@20~0.699 at a 45\% gate rate), while compact backbones show similar gains
under the same policy. Encoder baselines trail in both effectiveness and
flexibility. The result is a practical recipe: rank fast by default and explain
when it helps, a pattern that applies broadly to decision tasks where selective
generation buys accuracy at acceptable cost. The single-policy design
simplifies deployment and budget planning, and the curriculum principle (spend
more on the hard cases, less on the easy ones) readily transfers beyond
clinical order selection.

</details>


### [148] [LLM-as-a-Prophet: Understanding Predictive Intelligence with Prophet Arena](https://arxiv.org/abs/2510.17638)
*Qingchuan Yang,Simon Mahns,Sida Li,Anri Gu,Jibang Wu,Haifeng Xu*

Main category: cs.AI

TL;DR: 本文系统评估了大型语言模型作为预测工具的能力，发现LLMs已具备令人印象深刻的预测能力，但存在事件回忆不准确、数据源误解等关键瓶颈。


<details>
  <summary>Details</summary>
Motivation: 随着在互联网规模数据上训练的大型语言模型的快速发展，研究利用LLMs预测现实世界未来事件的潜力，这一新兴范式被称为'LLM-as-a-Prophet'。

Method: 构建了Prophet Arena评估基准，持续收集实时预测任务并将每个任务分解为不同的流水线阶段，支持受控和大规模实验。

Result: 评估显示许多LLMs已展现出令人印象深刻的预测能力，表现为较小的校准误差、一致的预测置信度和有前景的市场回报。

Conclusion: 虽然LLMs具备预测潜力，但在实现卓越预测智能方面仍存在关键瓶颈，包括事件回忆不准确、数据源误解以及在接近决策时信息聚合速度慢于市场等问题。

Abstract: Forecasting is not only a fundamental intellectual pursuit but also is of
significant importance to societal systems such as finance and economics. With
the rapid advances of large language models (LLMs) trained on Internet-scale
data, it raises the promise of employing LLMs to forecast real-world future
events, an emerging paradigm we call "LLM-as-a-Prophet". This paper
systematically investigates such predictive intelligence of LLMs. To this end,
we build Prophet Arena, a general evaluation benchmark that continuously
collects live forecasting tasks and decomposes each task into distinct pipeline
stages, in order to support our controlled and large-scale experimentation. Our
comprehensive evaluation reveals that many LLMs already exhibit impressive
forecasting capabilities, reflected in, e.g., their small calibration errors,
consistent prediction confidence and promising market returns. However, we also
uncover key bottlenecks towards achieving superior predictive intelligence via
LLM-as-a-Prophet, such as LLMs' inaccurate event recalls, misunderstanding of
data sources and slower information aggregation compared to markets when
resolution nears.

</details>


### [149] [A Principle of Targeted Intervention for Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2510.17697)
*Anjie Liu,Jianhong Wang,Samuel Kaski,Jun Wang,Mengyue Yang*

Main category: cs.AI

TL;DR: 本文提出了一种基于多智能体影响图(MAIDs)的新交互范式——目标干预，通过仅对单个目标智能体进行干预来缓解大规模多智能体强化学习中全局指导不切实际的问题。


<details>
  <summary>Details</summary>
Motivation: 在大规模多智能体强化学习中，对整个系统进行全局人工指导不切实际，而现有的协调机制设计主要依赖经验研究，缺乏易用的研究工具。

Method: 使用多智能体影响图(MAIDs)作为图形框架，设计了目标干预范式，并引入预策略干预(PSI)因果推理技术来实现该范式。通过最大化相应的因果效应来达成复合期望结果。

Result: 实验证明了所提出的目标干预方法的有效性，并验证了相关性图分析的结果。

Conclusion: MAIDs提供了一个有效的框架来分析和可视化多智能体强化学习方法，目标干预范式能够有效缓解全局指导问题，PSI技术能够成功实现期望的结果导向。

Abstract: Steering cooperative multi-agent reinforcement learning (MARL) towards
desired outcomes is challenging, particularly when the global guidance from a
human on the whole multi-agent system is impractical in a large-scale MARL. On
the other hand, designing mechanisms to coordinate agents most relies on
empirical studies, lacking a easy-to-use research tool. In this work, we employ
multi-agent influence diagrams (MAIDs) as a graphical framework to address the
above issues. First, we introduce interaction paradigms that leverage MAIDs to
analyze and visualize existing approaches in MARL. Then, we design a new
interaction paradigm based on MAIDs, referred to as targeted intervention that
is applied to only a single targeted agent, so the problem of global guidance
can be mitigated. In our implementation, we introduce a causal inference
technique-referred to as Pre-Strategy Intervention (PSI)-to realize the
targeted intervention paradigm. Since MAIDs can be regarded as a special class
of causal diagrams, a composite desired outcome that integrates the primary
task goal and an additional desired outcome can be achieved by maximizing the
corresponding causal effect through the PSI. Moreover, the bundled relevance
graph analysis of MAIDs provides a tool to identify whether an MARL learning
paradigm is workable under the design of an interaction paradigm. In
experiments, we demonstrate the effectiveness of our proposed targeted
intervention, and verify the result of relevance graph analysis.

</details>


### [150] [Contextual Attention Modulation: Towards Efficient Multi-Task Adaptation in Large Language Models](https://arxiv.org/abs/2510.17705)
*Dayan Pan,Zhaoyang Fu,Jingyuan Wang,Xiao Han,Yue Zhu,Xiangyu Zhao*

Main category: cs.AI

TL;DR: 提出Contextual Attention Modulation (CAM)机制和HyCAM框架，通过动态调节自注意力表示来增强任务特定特征并保留通用知识，在多任务适应中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在多任务适应中存在知识保留与任务专门化之间的平衡问题，传统微调方法存在灾难性遗忘和资源消耗大的缺陷，现有参数高效方法在复杂多任务场景下表现不佳。

Method: 提出CAM机制动态调节自注意力模块表示，并构建HyCAM框架，结合共享的全参数CAM模块和多个轻量级专用CAM模块，采用动态路由策略进行自适应知识融合。

Result: 在问答、代码生成和逻辑推理等异构任务上的广泛实验表明，该方法平均性能提升3.65%，显著优于现有方法。

Conclusion: CAM和HyCAM框架有效解决了大语言模型在多任务适应中的挑战，实现了更好的知识保留与任务专门化平衡。

Abstract: Large Language Models (LLMs) possess remarkable generalization capabilities
but struggle with multi-task adaptation, particularly in balancing knowledge
retention with task-specific specialization. Conventional fine-tuning methods
suffer from catastrophic forgetting and substantial resource consumption, while
existing parameter-efficient methods perform suboptimally in complex multi-task
scenarios. To address this, we propose Contextual Attention Modulation (CAM), a
novel mechanism that dynamically modulates the representations of
self-attention modules in LLMs. CAM enhances task-specific features while
preserving general knowledge, thereby facilitating more effective and efficient
adaptation. For effective multi-task adaptation, CAM is integrated into our
Hybrid Contextual Attention Modulation (HyCAM) framework, which combines a
shared, full-parameter CAM module with multiple specialized, lightweight CAM
modules, enhanced by a dynamic routing strategy for adaptive knowledge fusion.
Extensive experiments on heterogeneous tasks, including question answering,
code generation, and logical reasoning, demonstrate that our approach
significantly outperforms existing approaches, achieving an average performance
improvement of 3.65%. The implemented code and data are available to ease
reproducibility at https://github.com/Applied-Machine-Learning-Lab/HyCAM.

</details>


### [151] [Seeing but Not Believing: Probing the Disconnect Between Visual Attention and Answer Correctness in VLMs](https://arxiv.org/abs/2510.17771)
*Zhining Liu,Ziyi Chen,Hui Liu,Chen Luo,Xianfeng Tang,Suhang Wang,Joy Zeng,Zhenwei Dai,Zhan Shi,Tianxin Wei,Benoit Dumoulin,Hanghang Tong*

Main category: cs.AI

TL;DR: 研究发现视觉语言模型(VLMs)存在"看见但不相信"现象，即模型能感知到正确的视觉证据但仍输出错误答案。通过分析注意力动态，发现深层注意力能可靠定位证据区域。提出无需训练的推理时干预方法，通过选择性掩码增强证据利用，显著提升多个VLM家族的准确性。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉语言模型在多模态任务上表现良好，但它们仍会在正确视觉证据存在的情况下失败。本研究旨在系统性地探究这些失败是由于未感知证据还是未有效利用证据导致的。

Method: 通过分析层间注意力动态，发现浅层主要关注文本，深层稀疏但可靠地关注局部证据区域。提出推理时干预方法，通过选择性注意力掩码来突出深层证据区域，无需训练即可应用。

Result: 发现VLMs普遍存在"看见但不相信"现象，模型内部编码了可靠的证据但未充分利用。提出的干预方法在LLaVA、Qwen、Gemma和InternVL等多个VLM家族上一致提高了准确性。

Conclusion: VLMs在内部编码了可靠的证据信号但未充分利用，通过使这些信号显式化可以弥合感知与推理之间的差距，推进对VLM的诊断理解和可靠性提升。

Abstract: Vision-Language Models (VLMs) achieve strong results on multimodal tasks such
as visual question answering, yet they can still fail even when the correct
visual evidence is present. In this work, we systematically investigate whether
these failures arise from not perceiving the evidence or from not leveraging it
effectively. By examining layer-wise attention dynamics, we find that shallow
layers focus primarily on text, while deeper layers sparsely but reliably
attend to localized evidence regions. Surprisingly, VLMs often perceive the
visual evidence when outputting incorrect answers, a phenomenon we term
``seeing but not believing'' that widely exists in major VLM families. Building
on this, we introduce an inference-time intervention that highlights deep-layer
evidence regions through selective attention-based masking. It requires no
training and consistently improves accuracy across multiple families, including
LLaVA, Qwen, Gemma, and InternVL. These results show that VLMs encode reliable
evidence internally but under-utilize it, making such signals explicit can
bridge the gap between perception and reasoning, advancing the diagnostic
understanding and reliability of VLMs.

</details>
