<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 10]
- [cs.CR](#cs.CR) [Total: 10]
- [cs.AI](#cs.AI) [Total: 11]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [The Impact of Large Language Models (LLMs) on Code Review Process](https://arxiv.org/abs/2508.11034)
*Antonio Collante,Samuel Abedu,SayedHassan Khatoonabadi,Ahmad Abdellatif,Ebube Alor,Emad Shihab*

Main category: cs.SE

TL;DR: 研究探讨了GPT在GitHub代码审查流程中的影响，发现其能显著减少解决时间，优化各阶段性能，并帮助开发者。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在软件开发中广泛应用，但其在代码审查流程中的阶段特异性影响尚未充分研究。

Method: 通过半自动化方法识别GPT辅助的PRs，并应用统计建模（如多元线性回归和Mann-Whitney U检验）分析数据。

Result: GPT辅助的PRs中位解决时间减少60%（9小时vs 23小时），审查时间减少33%，等待时间减少87%。

Conclusion: GPT在代码审查中的早期采用能显著提升效率，为团队协作提供实用建议。

Abstract: Large language models (LLMs) have recently gained prominence in the field of
software development, significantly boosting productivity and simplifying
teamwork. Although prior studies have examined task-specific applications, the
phase-specific effects of LLM assistance on the efficiency of code review
processes remain underexplored. This research investigates the effect of GPT on
GitHub pull request (PR) workflows, with a focus on reducing resolution time,
optimizing phase-specific performance, and assisting developers. We curated a
dataset of 25,473 PRs from 9,254 GitHub projects and identified GPT-assisted
PRs using a semi-automated heuristic approach that combines keyword-based
detection, regular expression filtering, and manual verification until
achieving 95% labeling accuracy. We then applied statistical modeling,
including multiple linear regression and Mann-Whitney U test, to evaluate
differences between GPT-assisted and non-assisted PRs, both at the overall
resolution level and across distinct review phases. Our research has revealed
that early adoption of GPT can substantially boost the effectiveness of the PR
process, leading to considerable time savings at various stages. Our findings
suggest that GPT-assisted PRs reduced median resolution time by more than 60%
(9 hours compared to 23 hours for non-assisted PRs). We discovered that
utilizing GPT can reduce the review time by 33% and the waiting time before
acceptance by 87%. Analyzing a sample dataset of 300 GPT-assisted PRs, we
discovered that developers predominantly use GPT for code optimization (60%),
bug fixing (26%), and documentation updates (12%). This research sheds light on
the impact of the GPT model on the code review process, offering actionable
insights for software teams seeking to enhance workflows and promote seamless
collaboration.

</details>


### [2] [Diffusion is a code repair operator and generator](https://arxiv.org/abs/2508.11110)
*Mukul Singh,Gust Verbruggen,Vu Le,Sumit Gulwani*

Main category: cs.SE

TL;DR: 论文研究了如何利用预训练的代码扩散模型进行“最后一英里修复”，通过添加噪声或生成训练数据来优化代码片段。


<details>
  <summary>Details</summary>
Motivation: 探索扩散模型在代码生成后期阶段的潜力，利用其修复能力解决代码片段的不完整或错误问题。

Method: 通过添加噪声并恢复扩散过程，或从扩散过程中采样生成训练数据，评估其在Python、Excel和PowerShell中的应用。

Result: 实验表明，扩散模型可以有效用于最后一英里修复，并生成大量训练数据。

Conclusion: 扩散模型在代码修复和训练数据生成方面具有显著潜力，适用于多种编程语言和工具。

Abstract: Code diffusion models generate code by iteratively removing noise from the
latent representation of a code snippet. During later steps of the diffusion
process, when the code snippet has almost converged, differences between
discrete representations of these snippets look like last-mile repairs applied
to broken or incomplete code. We evaluate the extent to which this resemblance
can be exploited to leverage pre-trained code diffusion models for the problem
of last-mile repair by considering two applications with significant potential.
First, we can leverage the diffusion model for last-mile repair by adding noise
to a broken code snippet and resuming the diffusion process. Second, we can
leverage the diffusion model to generate arbitrary amount of training data for
last-mile repair tasks (that are computationally more efficient) by sampling an
intermediate program (input) and the final program (output) from the diffusion
process. We perform experiments on 3 domains (Python, Excel and PowerShell) to
evaluate applications, as well as analyze properties.

</details>


### [3] [AI Agentic Programming: A Survey of Techniques, Challenges, and Opportunities](https://arxiv.org/abs/2508.11126)
*Huanting Wang,Jingzhi Gong,Huawei Zhang,Zheng Wang*

Main category: cs.SE

TL;DR: AI agentic programming是一种新兴范式，利用LLMs自主规划、执行和与外部工具交互，完成复杂软件开发任务。本文综述了其行为分类、系统架构、核心技术及挑战。


<details>
  <summary>Details</summary>
Motivation: 随着AI agentic编程的快速发展，需要明确其范围、巩固技术基础并识别研究挑战。

Method: 通过分类代理行为与系统架构，分析规划、记忆管理、工具集成等核心技术，并评估现有基准。

Result: 研究发现处理长上下文、持久记忆不足及安全性等挑战，同时提出改进可靠性和透明性的机会。

Conclusion: 本文为构建下一代智能可信AI编程代理提供了研究基础与未来方向。

Abstract: AI agentic programming is an emerging paradigm in which large language models
(LLMs) autonomously plan, execute, and interact with external tools like
compilers, debuggers, and version control systems to iteratively perform
complex software development tasks. Unlike conventional code generation tools,
agentic systems are capable of decomposing high-level goals, coordinating
multi-step processes, and adapting their behavior based on intermediate
feedback. These capabilities are transforming the software development
practice. As this emerging field evolves rapidly, there is a need to define its
scope, consolidate its technical foundations, and identify open research
challenges. This survey provides a comprehensive and timely review of AI
agentic programming. We introduce a taxonomy of agent behaviors and system
architectures, and examine core techniques including planning, memory and
context management, tool integration, and execution monitoring. We also analyze
existing benchmarks and evaluation methodologies used to assess coding agent
performance. Our study identifies several key challenges, including limitations
in handling long context, a lack of persistent memory across tasks, and
concerns around safety, alignment with user intent, and collaboration with
human developers. We discuss emerging opportunities to improve the reliability,
adaptability, and transparency of agentic systems. By synthesizing recent
advances and outlining future directions, this survey aims to provide a
foundation for research and development in building the next generation of
intelligent and trustworthy AI coding agents.

</details>


### [4] [From Feedback to Failure: Automated Android Performance Issue Reproduction](https://arxiv.org/abs/2508.11147)
*Zhengquan Li,Zhenhao Li,Zishuo Ding*

Main category: cs.SE

TL;DR: RevPerf是一个通过Google Play评论检测移动应用性能问题的工具，结合提示工程和多方面检测方法，成功率达到70%。


<details>
  <summary>Details</summary>
Motivation: 移动应用性能问题在开发环境中难以检测，影响用户体验。

Method: 利用Google Play评论获取信息，通过提示工程丰富评论细节，执行代理生成并执行命令复现问题，结合日志、GUI变化和资源使用检测问题。

Result: 在构建的数据集上，RevPerf成功复现性能问题的成功率为70%。

Conclusion: RevPerf能有效复现移动应用性能问题，为开发者提供实用工具。

Abstract: Mobile application performance is a vital factor for user experience. Yet,
performance issues are notoriously difficult to detect within development
environments, where their manifestations are often less conspicuous and
diagnosis proves more challenging. To address this limitation, we propose
RevPerf, an advanced performance issue reproduction tool that leverages app
reviews from Google Play to acquire pertinent information. RevPerf employs
relevant reviews and prompt engineering to enrich the original review with
performance issue details. An execution agent is then employed to generate and
execute commands to reproduce the issue. After executing all necessary steps,
the system incorporates multifaceted detection methods to identify performance
issues by monitoring Android logs, GUI changes, and system resource utilization
during the reproduction process. Experimental results demonstrate that our
proposed framework achieves a 70\% success rate in reproducing performance
issues on the dataset we constructed and manually validated.

</details>


### [5] [PTMPicker: Facilitating Efficient Pretrained Model Selection for Application Developers](https://arxiv.org/abs/2508.11179)
*Pei Liu,Terry Zhuo,Jiawei Deng,Zhenchang Xing,Qinghua Lu,Xiaoning Du,Hongyu Zhan*

Main category: cs.SE

TL;DR: PTMPicker是一种基于结构化模板的预训练模型选择工具，通过统一格式表示模型和用户需求，结合嵌入相似性和提示评估，显著提高了模型选择的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有基于关键词的预训练模型搜索方法难以全面捕捉用户意图，尤其是在考虑偏差缓解、硬件需求或许可证合规性等因素时。

Method: 定义结构化模板表示模型和用户需求，计算功能相关属性的嵌入相似性，并通过提示评估特殊约束（如许可证合规性）。

Result: 在Hugging Face的543,949个预训练模型上测试，PTMPicker在85%的请求中成功在前10名候选模型中定位到合适模型。

Conclusion: PTMPicker通过结构化表示和综合评估，显著提升了预训练模型选择的效率和准确性。

Abstract: The rapid emergence of pretrained models (PTMs) has attracted significant
attention from both Deep Learning (DL) researchers and downstream application
developers. However, selecting appropriate PTMs remains challenging because
existing methods typically rely on keyword-based searches in which the keywords
are often derived directly from function descriptions. This often fails to
fully capture user intent and makes it difficult to identify suitable models
when developers also consider factors such as bias mitigation, hardware
requirements, or license compliance. To address the limitations of
keyword-based model search, we propose PTMPicker to accurately identify
suitable PTMs. We first define a structured template composed of common and
essential attributes for PTMs and then PTMPicker represents both candidate
models and user-intended features (i.e., model search requests) in this unified
format. To determine whether candidate models satisfy user requirements, it
computes embedding similarities for function-related attributes and uses
well-crafted prompts to evaluate special constraints such as license compliance
and hardware requirements. We scraped a total of 543,949 pretrained models from
Hugging Face to prepare valid candidates for selection. PTMPicker then
represented them in the predefined structured format by extracting their
associated descriptions. Guided by the extracted metadata, we synthesized a
total of 15,207 model search requests with carefully designed prompts, as no
such search requests are readily available. Experiments on the curated PTM
dataset and the synthesized model search requests show that PTMPicker can help
users effectively identify models,with 85% of the sampled requests successfully
locating appropriate PTMs within the top-10 ranked candidates.

</details>


### [6] [ORFuzz: Fuzzing the "Other Side" of LLM Safety -- Testing Over-Refusal](https://arxiv.org/abs/2508.11222)
*Haonan Zhang,Dongxia Wang,Yi Liu,Kexin Chen,Jiashui Wang,Xinlei Ying,Long Liu,Wenhai Wang*

Main category: cs.SE

TL;DR: 论文提出ORFuzz框架，用于检测和分析大型语言模型（LLMs）的过度拒绝行为，通过进化测试生成多样化测试用例，并创建ORFuzzSet基准数据集。


<details>
  <summary>Details</summary>
Motivation: LLMs的过度拒绝行为（错误拒绝良性查询）影响其可靠性和可用性，现有测试方法存在不足。

Method: ORFuzz框架结合安全类别感知种子选择、自适应突变优化和OR-Judge评估模型，生成多样化测试用例。

Result: ORFuzz生成测试用例的成功率（6.98%）是现有方法的两倍，ORFuzzSet基准数据集在10种LLMs中平均过度拒绝率达63.56%。

Conclusion: ORFuzz和ORFuzzSet为开发更可靠的LLM系统提供了自动化测试框架和社区资源。

Abstract: Large Language Models (LLMs) increasingly exhibit over-refusal - erroneously
rejecting benign queries due to overly conservative safety measures - a
critical functional flaw that undermines their reliability and usability.
Current methods for testing this behavior are demonstrably inadequate,
suffering from flawed benchmarks and limited test generation capabilities, as
highlighted by our empirical user study. To the best of our knowledge, this
paper introduces the first evolutionary testing framework, ORFuzz, for the
systematic detection and analysis of LLM over-refusals. ORFuzz uniquely
integrates three core components: (1) safety category-aware seed selection for
comprehensive test coverage, (2) adaptive mutator optimization using reasoning
LLMs to generate effective test cases, and (3) OR-Judge, a human-aligned judge
model validated to accurately reflect user perception of toxicity and refusal.
Our extensive evaluations demonstrate that ORFuzz generates diverse, validated
over-refusal instances at a rate (6.98% average) more than double that of
leading baselines, effectively uncovering vulnerabilities. Furthermore,
ORFuzz's outputs form the basis of ORFuzzSet, a new benchmark of 1,855 highly
transferable test cases that achieves a superior 63.56% average over-refusal
rate across 10 diverse LLMs, significantly outperforming existing datasets.
ORFuzz and ORFuzzSet provide a robust automated testing framework and a
valuable community resource, paving the way for developing more reliable and
trustworthy LLM-based software systems.

</details>


### [7] [Hallucination in LLM-Based Code Generation: An Automotive Case Study](https://arxiv.org/abs/2508.11257)
*Marc Pavel,Nenad Petrovic,Lukasz Mazur,Vahid Zolfaghari,Fengjunjie Pan,Alois Knoll*

Main category: cs.SE

TL;DR: 论文研究了大型语言模型（LLM）在代码生成中的幻觉现象，特别是在汽车领域。通过案例研究评估了不同提示复杂度下多个代码LLM的表现，发现即使先进模型如GPT-4.1和GPT-4o也存在高频率错误。


<details>
  <summary>Details</summary>
Motivation: LLM在代码生成中表现出潜力，但幻觉问题限制了其实际应用，尤其是在安全关键领域如汽车软件系统。

Method: 通过案例研究，评估了多个代码LLM（如GPT-4.1、Codex和GPT-4o）在不同提示复杂度下的表现，从简单提示到包含额外上下文的复杂提示。

Result: 研究发现，即使是先进模型也存在语法错误、无效引用和API知识冲突等问题。仅GPT-4.1和GPT-4o在最复杂的提示下能生成正确代码。

Conclusion: 研究强调了需要有效的缓解技术，以确保LLM生成的代码在安全关键领域中的可靠使用。

Abstract: Large Language Models (LLMs) have shown significant potential in automating
code generation tasks offering new opportunities across software engineering
domains. However, their practical application remains limited due to
hallucinations - outputs that appear plausible but are factually incorrect,
unverifiable or nonsensical. This paper investigates hallucination phenomena in
the context of code generation with a specific focus on the automotive domain.
A case study is presented that evaluates multiple code LLMs for three different
prompting complexities ranging from a minimal one-liner prompt to a prompt with
Covesa Vehicle Signal Specifications (VSS) as additional context and finally to
a prompt with an additional code skeleton. The evaluation reveals a high
frequency of syntax violations, invalid reference errors and API knowledge
conflicts in state-of-the-art models GPT-4.1, Codex and GPT-4o. Among the
evaluated models, only GPT-4.1 and GPT-4o were able to produce a correct
solution when given the most context-rich prompt. Simpler prompting strategies
failed to yield a working result, even after multiple refinement iterations.
These findings highlight the need for effective mitigation techniques to ensure
the safe and reliable use of LLM generated code, especially in safety-critical
domains such as automotive software systems.

</details>


### [8] [Defects4Log: Benchmarking LLMs for Logging Code Defect Detection and Reasoning](https://arxiv.org/abs/2508.11305)
*Xin Wang,Zhenhao Li,Zishuo Ding*

Main category: cs.SE

TL;DR: 本文提出了一种全面的日志代码缺陷分类法，构建了真实世界的缺陷数据集，并评估了大型语言模型（LLMs）在检测日志代码缺陷中的表现。


<details>
  <summary>Details</summary>
Motivation: 日志代码缺陷可能导致日志信息误解，现有研究缺乏系统性分析，且LLMs在此领域的潜力尚未充分探索。

Method: 作者提出了一种分类法（7种缺陷模式，14种场景），构建了包含164个真实缺陷的数据集，并设计了一个自动化框架评估LLMs的检测能力。

Result: 实验表明，LLMs仅基于源代码检测缺陷表现不佳，但引入相关知识后检测准确率提升10.9%。

Conclusion: 研究为开发者提供了避免常见缺陷的指导，并为改进基于LLM的日志缺陷检测奠定了基础。

Abstract: Logging code is written by developers to capture system runtime behavior and
plays a vital role in debugging, performance analysis, and system monitoring.
However, defects in logging code can undermine the usefulness of logs and lead
to misinterpretations. Although prior work has identified several logging
defect patterns and provided valuable insights into logging practices, these
studies often focus on a narrow range of defect patterns derived from limited
sources (e.g., commit histories) and lack a systematic and comprehensive
analysis. Moreover, large language models (LLMs) have demonstrated promising
generalization and reasoning capabilities across a variety of code-related
tasks, yet their potential for detecting logging code defects remains largely
unexplored.
  In this paper, we derive a comprehensive taxonomy of logging code defects,
which encompasses seven logging code defect patterns with 14 detailed
scenarios. We further construct a benchmark dataset, \dataset, consisting of
164 developer-verified real-world logging defects. Then we propose an automated
framework that leverages various prompting strategies and contextual
information to evaluate LLMs' capability in detecting and reasoning logging
code defects. Experimental results reveal that LLMs generally struggle to
accurately detect and reason logging code defects based on the source code
only. However, incorporating proper knowledge (e.g., detailed scenarios of
defect patterns) can lead to 10.9\% improvement in detection accuracy. Overall,
our findings provide actionable guidance for practitioners to avoid common
defect patterns and establish a foundation for improving LLM-based reasoning in
logging code defect detection.

</details>


### [9] [TRACY: Benchmarking Execution Efficiency of LLM-Based Code Translation](https://arxiv.org/abs/2508.11468)
*Zhihao Gong,Zeyu Sun,Dong Huang,Qingyuan Liang,Jie M. Zhang,Dan Hao*

Main category: cs.SE

TL;DR: TRACY是首个评估LLM翻译代码执行效率的基准测试，揭示了当前LLM在生成高效代码方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在代码翻译中注重正确性，但忽略了执行效率，TRACY填补了这一空白。

Method: 通过LLM驱动的两阶段流程构建TRACY：首先生成压力测试，再筛选效率区分任务，最终包含1,011个任务。

Result: 评估26个LLM发现，即使顶级模型在效率上表现不佳，算法缺陷和资源处理不当是主要问题。

Conclusion: 未来LLM代码翻译需同时优化正确性和效率。

Abstract: Automatic code translation is a fundamental task in modern software
development. While the advent of Large Language Models (LLMs) has significantly
improved the correctness of code translation, the critical dimension of
execution efficiency remains overlooked. To address this gap, we introduce
TRACY, the first comprehensive benchmark designed to evaluate the execution
efficiency of LLM-translated code. TRACY is constructed through an LLM-driven
two-stage pipeline: an initial stage generates a suite of stress tests to
amplify performance differences, followed by an efficiency-oriented task
pruning stage that isolates the efficiency-distinguishing tasks. The resulting
benchmark comprises 1,011 code translation tasks across C++, Java, and Python,
each accompanied by an average of 22.1 verified reference translations and 10
computationally demanding tests. Our extensive evaluation of 26 representative
LLMs reveals that even top-tier LLMs struggle to consistently produce efficient
code translations. For instance, Claude-4-think, the leading model for
correctness, ranks eighth overall when time efficiency is taken into account,
surpassed by several smaller open-source models. We further pinpoint that
algorithmic flaws and improper resource handling are the most detrimental,
causing a median time slowdown of 5.6$\times$ and memory increase of
12.0$\times$, respectively. Our work underscores the necessity of jointly
optimizing for correctness and efficiency in future LLM-based code translation.

</details>


### [10] [Temporal Network Analysis of Microservice Architectural Degradation](https://arxiv.org/abs/2508.11571)
*Alexander Bakhtin*

Main category: cs.SE

TL;DR: 论文探讨了如何从微服务系统中获取时间网络并分析其挑战，指出现有数据规模限制了分析方法的应用。


<details>
  <summary>Details</summary>
Motivation: 研究微服务架构的时间网络分析方法，以更好地理解和优化微服务系统的动态行为。

Method: 利用网络科学中的时间网络分析方法，研究微服务系统的依赖关系随时间的变化。

Result: 获取的最完整时间网络包含7个时间实例和42个微服务，数据规模限制了分析方法的深度应用。

Conclusion: 尽管数据规模有限，时间网络分析仍为微服务系统研究提供了有价值的视角，未来需扩展数据规模以支持更深入分析。

Abstract: Microservice architecture can be modeled as a network of microservices making
calls to each other, commonly known as the service dependency graph. Network
Science can provide methods to study such networks. In particular, temporal
network analysis is a branch of Network Science that analyzes networks evolving
with time. In microservice systems, temporal networks can arise if we examine
the architecture of the system across releases or monitor a deployed system
using tracing.
  In this research summary paper, I discuss the challenges in obtaining
temporal networks from microservice systems and analyzing them with the
temporal network methods. In particular, the most complete temporal network
that we could obtain contains 7 time instances and 42 microservices, which
limits the potential analysis that could be applied.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [11] [MCP-Guard: A Defense Framework for Model Context Protocol Integrity in Large Language Model Applications](https://arxiv.org/abs/2508.10991)
*Wenpeng Xing,Zhonghao Qi,Yupeng Qin,Yilin Li,Caini Chang,Jiahui Yu,Changting Lin,Zhenzhen Xie,Meng Han*

Main category: cs.CR

TL;DR: 论文提出MCP-Guard，一种针对LLM与工具交互的安全防御架构，通过三阶段检测管道应对安全威胁，并引入MCP-AttackBench作为评估基准。


<details>
  <summary>Details</summary>
Motivation: LLM与外部工具集成（如通过MCP协议）会引入安全漏洞（如提示注入、数据泄露等），亟需解决方案。

Method: MCP-Guard采用三阶段检测管道：轻量级静态扫描、深度神经网络检测语义攻击、E5微调模型（准确率96.01%），最后通过轻量级LLM仲裁器综合决策。

Result: MCP-Guard在识别对抗性提示方面表现优异（准确率96.01%），并开发了包含70,000样本的MCP-AttackBench作为评估基准。

Conclusion: MCP-Guard为LLM-工具交互提供了一种高效、准确的安全防御方案，MCP-AttackBench为未来研究奠定了基础。

Abstract: The integration of Large Language Models (LLMs) with external tools via
protocols such as the Model Context Protocol (MCP) introduces critical security
vulnerabilities, including prompt injection, data exfiltration, and other
threats. To counter these challenges, we propose MCP-Guard, a robust, layered
defense architecture designed for LLM--tool interactions. MCP-Guard employs a
three-stage detection pipeline that balances efficiency with accuracy: it
progresses from lightweight static scanning for overt threats and a deep neural
detector for semantic attacks, to our fine-tuned E5-based model achieves
(96.01) accuracy in identifying adversarial prompts. Finally, a lightweight LLM
arbitrator synthesizes these signals to deliver the final decision while
minimizing false positives. To facilitate rigorous training and evaluation, we
also introduce MCP-AttackBench, a comprehensive benchmark of over 70,000
samples. Sourced from public datasets and augmented by GPT-4, MCP-AttackBench
simulates diverse, real-world attack vectors in the MCP format, providing a
foundation for future research into securing LLM-tool ecosystems.

</details>


### [12] [A Constant-Time Hardware Architecture for the CSIDH Key-Exchange Protocol](https://arxiv.org/abs/2508.11082)
*Sina Bagheri,Masoud Kaveh,Francisco Hernando-Gallego,Diego Martín,Nuria Serrano*

Main category: cs.CR

TL;DR: 本文首次全面研究了CSIDH算法的硬件实现，提供了FPGA和ASIC平台的性能基准，展示了其作为后量子密钥交换协议的潜力。


<details>
  <summary>Details</summary>
Motivation: CSIDH算法因其极小的密钥尺寸而备受关注，但计算密集的密钥生成和恒定时间实现的需求带来了性能挑战。

Method: 提出了一种统一的硬件架构，采用顶层有限状态机（FSM）和深度流水线算术逻辑单元（ALU），加速512位有限域运算。

Result: 在FPGA上实现200 MHz时钟频率，密钥生成延迟515 ms；ASIC上实现180 MHz频率，延迟591 ms。

Conclusion: 本研究为未来基于同源的后量子密码加速器提供了关键的性能基准。

Abstract: The commutative supersingular isogeny Diffie-Hellman (CSIDH) algorithm is a
promising post-quantum key exchange protocol, notable for its exceptionally
small key sizes, but hindered by computationally intensive key generation.
Furthermore, practical implementations must operate in constant time to
mitigate side-channel vulnerabilities, which presents an additional performance
challenge. This paper presents, to our knowledge, the first comprehensive
hardware study of CSIDH, establishing a performance baseline with a unified
architecture on both field-programmable gate array (FPGA) and
application-specific integrated circuit (ASIC) platforms. The architecture
features a top-level finite state machine (FSM) that orchestrates a deeply
pipelined arithmetic logic unit (ALU) to accelerate the underlying 512-bit
finite field operations. The ALU employs a parallelized schoolbook multiplier,
completing a 512$\times$512-bit multiplication in 22 clock cycles and enabling
a full Montgomery modular multiplication in 87 cycles. The constant-time
CSIDH-512 design requires $1.03\times10^{8}$ clock cycles per key generation.
When implemented on a Xilinx Zynq UltraScale+ FPGA, the architecture achieves a
200 MHz clock frequency, corresponding to a 515 ms latency. For ASIC
implementation in a 180nm process, the design requires $1.065\times10^{8}$
clock cycles and achieves a \textasciitilde 180 MHz frequency, resulting in a
key generation latency of 591 ms. By providing the first public hardware
performance metrics for CSIDH on both FPGA and ASIC platforms, this work
delivers a crucial benchmark for future isogeny-based post-quantum cryptography
(PQC) accelerators.

</details>


### [13] [HEIR: A Universal Compiler for Homomorphic Encryption](https://arxiv.org/abs/2508.11095)
*Asra Ali,Jaeho Choi,Bryant Gipson,Shruthi Gorantala,Jeremy Kun,Wouter Legiest,Lawrence Lim,Alexander Viand,Meron Zerihun Demissie,Hongren Zheng*

Main category: cs.CR

TL;DR: HEIR是一个统一的同态加密编译器框架，支持主流技术、软件库和硬件加速器，并推动研究。


<details>
  <summary>Details</summary>
Motivation: 解决同态加密优化技术难以结合和比较的问题，提供一个研究和基准测试平台。

Method: 基于MLIR编译器框架，引入HE特定抽象层，支持多种前端（如Python）。

Result: HEIR被验证能处理比现有文献更复杂多样的程序，并成为学术和工业界的实际标准。

Conclusion: HEIR为同态加密编译器提供了一个高效、可扩展的平台，推动领域发展。

Abstract: This work presents Homomorphic Encryption Intermediate Representation (HEIR),
a unified approach to building homomorphic encryption (HE) compilers. HEIR aims
to support all mainstream techniques in homomorphic encryption, integrate with
all major software libraries and hardware accelerators, and advance the field
by providing a platform for research and benchmarking. Built on the MLIR
compiler framework, HEIR introduces HE-specific abstraction layers at which
existing optimizations and new research ideas may be easily implemented.
Although many HE optimization techniques have been proposed, it remains
difficult to combine or compare them effectively. HEIR provides a means to
effectively explore the space of HE optimizations. HEIR addresses the entire HE
stack and includes support for various frontends, including Python. The
contribution of this work includes: (1) We introduce HEIR as a framework for
building HE compilers. (2) We validate HEIR's design by porting a large
fraction of the HE literature to HEIR, and we argue that HEIR can tackle more
complicated and diverse programs than prior literature. (3) We provide evidence
that HEIR is emerging as the de facto HE compiler for academic research and
industry development.

</details>


### [14] [Salty Seagull: A VSAT Honeynet to Follow the Bread Crumb of Attacks in Ship Networks](https://arxiv.org/abs/2508.11325)
*Georgios Michail Makrakis,Jeroen Pijpker,Remco Hassing,Rob Loves,Stephen McCombie*

Main category: cs.CR

TL;DR: 本文提出了一种名为Salty Seagull的蜜网系统，模拟船舶VSAT系统，用于收集针对海事行业的网络攻击数据。实验结果显示，系统成功吸引了攻击者，但仅有一名具备相关知识的攻击者深入访问。


<details>
  <summary>Details</summary>
Motivation: 近年来海事行业面临的网络威胁显著增加，传统系统和操作限制加剧了其脆弱性。需要创新方法来理解和应对这些威胁。

Method: 设计了Salty Seagull蜜网，模拟船舶VSAT系统，并故意集成已知漏洞以吸引攻击者。系统通过Web仪表板和CLI环境提供交互，并暴露于互联网30天以测试其效果。

Result: 实验期间，系统吸引了大量通用攻击尝试，但仅有一名具备系统知识的攻击者深入访问，未完全探索系统潜力。

Conclusion: 蜜网技术可用于收集海事行业网络攻击数据，但需进一步优化以提高攻击者参与度和数据收集效果。

Abstract: Cyber threats against the maritime industry have increased notably in recent
years, highlighting the need for innovative cybersecurity approaches. Ships, as
critical assets, possess highly specialized and interconnected network
infrastructures, where their legacy systems and operational constraints further
exacerbate their vulnerability to cyberattacks. To better understand this
evolving threat landscape, we propose the use of cyber-deception techniques and
in particular honeynets, as a means to gather valuable insights into ongoing
attack campaigns targeting the maritime sector.
  In this paper we present Salty Seagull, a honeynet conceived to simulate a
VSAT system for ships. This environment mimics the operations of a functional
VSAT system onboard and, at the same time, enables a user to interact with it
through a Web dashboard and a CLI environment. Furthermore, based on existing
vulnerabilities, we purposefully integrate them into our system to increase
attacker engagement. We exposed our honeynet for 30 days to the Internet to
assess its capability and measured the received interaction. Results show that
while numerous generic attacks have been attempted, only one curious attacker
with knowledge of the nature of the system and its vulnerabilities managed to
access it, without however exploring its full potential.

</details>


### [15] [RMSL: Weakly-Supervised Insider Threat Detection with Robust Multi-sphere Learning](https://arxiv.org/abs/2508.11472)
*Yang Wang,Yaxin Zhao,Xinyu Jiao,Sihan Xu,Xiangrui Cai,Ying Zhang,Xiaojie Yuan*

Main category: cs.CR

TL;DR: 论文提出了一种基于弱标签序列的行为级内部威胁检测框架RMSL，通过多超球体学习和自适应训练优化检测性能。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏细粒度的行为级标注，现有方法在检测用户行为序列中的异常时面临高误报率和漏报率。

Method: 提出RMSL框架，利用多超球体表示正常行为模式，结合多实例学习和自适应行为级自训练去偏，优化特征表示。

Result: 实验表明，RMSL显著提升了行为级内部威胁检测的性能。

Conclusion: RMSL通过弱序列标签和自适应训练，有效解决了行为级异常检测的挑战。

Abstract: Insider threat detection aims to identify malicious user behavior by
analyzing logs that record user interactions. Due to the lack of fine-grained
behavior-level annotations, detecting specific behavior-level anomalies within
user behavior sequences is challenging. Unsupervised methods face high false
positive rates and miss rates due to the inherent ambiguity between normal and
anomalous behaviors. In this work, we instead introduce weak labels of behavior
sequences, which have lower annotation costs, i.e., the training labels
(anomalous or normal) are at sequence-level instead of behavior-level, to
enhance the detection capability for behavior-level anomalies by learning
discriminative features. To achieve this, we propose a novel framework called
Robust Multi-sphere Learning (RMSL). RMSL uses multiple hyper-spheres to
represent the normal patterns of behaviors. Initially, a one-class classifier
is constructed as a good anomaly-supervision-free starting point. Building on
this, using multiple instance learning and adaptive behavior-level
self-training debiasing based on model prediction confidence, the framework
further refines hyper-spheres and feature representations using weak
sequence-level labels. This approach enhances the model's ability to
distinguish between normal and anomalous behaviors. Extensive experiments
demonstrate that RMSL significantly improves the performance of behavior-level
insider threat detection.

</details>


### [16] [KV-Auditor: Auditing Local Differential Privacy for Correlated Key-Value Estimation](https://arxiv.org/abs/2508.11495)
*Jingnan Xu,Leixia Wang,Xiaofeng Meng*

Main category: cs.CR

TL;DR: KV-Auditor是一个用于审计基于本地差分隐私（LDP）的键值估计机制的框架，填补了现有审计方法在连续数据和键值数据上的空白。


<details>
  <summary>Details</summary>
Motivation: 现有LDP审计方法主要针对离散数据的频率估计任务，缺乏对键值数据的支持，KV-Auditor旨在解决这一问题。

Method: KV-Auditor通过分析无界输出分布估计隐私下界，支持连续数据，并针对交互式和非交互式机制设计了不同审计策略。

Result: 实验验证了KV-Auditor的有效性，并为优化LDP键值估计器提供了见解。

Conclusion: KV-Auditor填补了LDP审计在键值数据上的空白，为实际应用提供了可靠的隐私保障。

Abstract: To protect privacy for data-collection-based services, local differential
privacy (LDP) is widely adopted due to its rigorous theoretical bound on
privacy loss. However, mistakes in complex theoretical analysis or subtle
implementation errors may undermine its practical guarantee. To address this,
auditing is crucial to confirm that LDP protocols truly protect user data.
However, existing auditing methods, though, mainly target machine learning and
federated learning tasks based on centralized differentially privacy (DP), with
limited attention to LDP. Moreover, the few studies on LDP auditing focus
solely on simple frequency estimation task for discrete data, leaving
correlated key-value data - which requires both discrete frequency estimation
for keys and continuous mean estimation for values - unexplored.
  To bridge this gap, we propose KV-Auditor, a framework for auditing LDP-based
key-value estimation mechanisms by estimating their empirical privacy lower
bounds. Rather than traditional LDP auditing methods that relies on binary
output predictions, KV-Auditor estimates this lower bound by analyzing
unbounded output distributions, supporting continuous data. Specifically, we
classify state-of-the-art LDP key-value mechanisms into interactive and
non-interactive types. For non-interactive mechanisms, we propose horizontal
KV-Auditor for small domains with sufficient samples and vertical KV-Auditor
for large domains with limited samples. For interactive mechanisms, we design a
segmentation strategy to capture incremental privacy leakage across iterations.
Finally, we perform extensive experiments to validate the effectiveness of our
approach, offering insights for optimizing LDP-based key-value estimators.

</details>


### [17] [Copyright Protection for Large Language Models: A Survey of Methods, Challenges, and Trends](https://arxiv.org/abs/2508.11548)
*Zhenhua Xu,Xubin Yue,Zhebo Wang,Qichen Liu,Xixiang Zhao,Jingxuan Zhang,Wenjun Zeng,Wengpeng Xing,Dezhang Kong,Changting Lin,Meng Han*

Main category: cs.CR

TL;DR: 本文综述了大语言模型（LLM）版权保护技术，重点探讨了模型指纹识别，并澄清了文本水印、模型水印和模型指纹的关系。


<details>
  <summary>Details</summary>
Motivation: 鉴于LLM的高开发成本、专有价值及潜在滥用风险，保护其版权至关重要。现有研究多关注文本水印，缺乏对模型保护的全面探讨。

Method: 通过系统分类和比较现有技术，澄清概念关系，统一术语，并首次提出指纹转移和移除技术。

Result: 总结了模型指纹的评价指标（如有效性、无害性等），并讨论了开放挑战与未来方向。

Conclusion: 本文为研究者提供了对LLM时代文本水印和模型指纹技术的全面理解，推动知识产权保护的进一步发展。

Abstract: Copyright protection for large language models is of critical importance,
given their substantial development costs, proprietary value, and potential for
misuse. Existing surveys have predominantly focused on techniques for tracing
LLM-generated content-namely, text watermarking-while a systematic exploration
of methods for protecting the models themselves (i.e., model watermarking and
model fingerprinting) remains absent. Moreover, the relationships and
distinctions among text watermarking, model watermarking, and model
fingerprinting have not been comprehensively clarified. This work presents a
comprehensive survey of the current state of LLM copyright protection
technologies, with a focus on model fingerprinting, covering the following
aspects: (1) clarifying the conceptual connection from text watermarking to
model watermarking and fingerprinting, and adopting a unified terminology that
incorporates model watermarking into the broader fingerprinting framework; (2)
providing an overview and comparison of diverse text watermarking techniques,
highlighting cases where such methods can function as model fingerprinting; (3)
systematically categorizing and comparing existing model fingerprinting
approaches for LLM copyright protection; (4) presenting, for the first time,
techniques for fingerprint transfer and fingerprint removal; (5) summarizing
evaluation metrics for model fingerprints, including effectiveness,
harmlessness, robustness, stealthiness, and reliability; and (6) discussing
open challenges and future research directions. This survey aims to offer
researchers a thorough understanding of both text watermarking and model
fingerprinting technologies in the era of LLMs, thereby fostering further
advances in protecting their intellectual property.

</details>


### [18] [Pushing the Limits of Frequency Analysis in Leakage Abuse Attacks](https://arxiv.org/abs/2508.11563)
*Nathaniel Moyer,Charalampos Papamanthou,Evgenios Kornaropoulos*

Main category: cs.CR

TL;DR: 本文提出了一种名为LAMA的通用攻击框架，针对支持加密范围查询的方案，通过频率分析利用访问模式泄漏重构明文数据。


<details>
  <summary>Details</summary>
Motivation: 探索在仅知道查询分布和访问模式泄漏的情况下，攻击者能重构多少信息，填补现有研究的空白。

Method: 提出LAMA框架，通过频率匹配技术分析高维加密数据，并证明其极限性能。

Result: LAMA首次成功重构了四维加密范围查询的明文数据，并确定了使频率分析困难的查询分布。

Conclusion: LAMA展示了频率分析的极限性能，并提出了可能的缓解机制。

Abstract: Searchable encryption (SE) is the most scalable cryptographic primitive for
searching on encrypted data. Typical SE constructions often allow
access-pattern leakage, revealing which encrypted records are retrieved in the
server's responses. All the known generic cryptanalyses assume either that the
queries are issued uniformly at random or that the attacker observes the
search-pattern leakage. It remains unclear what can be reconstructed when using
only the access-pattern leakage and knowledge of the query distribution. In
this work, we focus on the cryptanalytic technique of frequency analysis in the
context of leakage-abuse attacks on schemes that support encrypted range
queries. Frequency analysis matches the frequency of retrieval of an encrypted
record with a plaintext value based on its probability of retrieval that
follows from the knowledge of the query distribution. We generalize this
underexplored cryptanalytic technique and introduce a generic attack framework
called Leakage-Abuse via Matching (LAMA) that works even on high-dimensional
encrypted data. We identify a parameterization of LAMA that brings frequency
analysis to its limit -- that is, we prove that there is no additional
frequency matching that an attacker can perform to refine the result.
Furthermore, we show that our results hold for any class of convex queries, and
not just axis-aligned rectangles, which is the assumption in all other attacks
on range schemes. Using these results, we identify query distributions that
make frequency analysis challenging for the attacker and, thus, can act as a
mitigation mechanism. Finally, we implement and benchmark LAMA and reconstruct,
for the first time, plaintext data from encrypted range queries spanning up to
four dimensions.

</details>


### [19] [Activate Me!: Designing Efficient Activation Functions for Privacy-Preserving Machine Learning with Fully Homomorphic Encryption](https://arxiv.org/abs/2508.11575)
*Nges Brian Njungle,Michel A. Kinsy*

Main category: cs.CR

TL;DR: 论文探讨了在基于全同态加密（FHE）的机器学习中设计激活函数的方法，比较了Square函数和ReLU在LeNet-5和ResNet-20架构中的表现，揭示了速度与准确性的权衡。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习在医疗和国防等敏感领域的广泛应用，数据隐私和安全问题日益突出。FHE虽能保护数据隐私，但其仅支持线性操作，难以实现现代神经网络中的非线性激活函数。

Method: 研究设计了适用于FHE的激活函数，重点评估了Square函数和ReLU（包括多项式近似和新型方案切换技术），并在LeNet-5和ResNet-20架构中进行了实验。

Result: Square函数在浅层网络（LeNet-5）中表现优异（99.4%准确率，128秒/图），而ReLU更适合深层网络（ResNet-20）。多项式近似ReLU准确率为83.8%（1,145秒/图），方案切换技术提升至89.8%（1,697秒/图）。

Conclusion: FHE机器学习中激活函数的选择需要在速度和准确性之间权衡：快速函数可能降低准确性，而高准确性函数则需要更多计算资源。

Abstract: The growing adoption of machine learning in sensitive areas such as
healthcare and defense introduces significant privacy and security challenges.
These domains demand robust data protection, as models depend on large volumes
of sensitive information for both training and inference. Fully Homomorphic
Encryption (FHE) presents a compelling solution by enabling computations
directly on encrypted data, maintaining confidentiality across the entire
machine learning workflow. However, FHE inherently supports only linear
operations, making it difficult to implement non-linear activation functions,
essential components of modern neural networks. This work focuses on designing,
implementing, and evaluating activation functions tailored for FHE-based
machine learning. We investigate two commonly used functions: the Square
function and Rectified Linear Unit (ReLU), using LeNet-5 and ResNet-20
architectures with the CKKS scheme from the OpenFHE library. For ReLU, we
assess two methods: a conventional low-degree polynomial approximation and a
novel scheme-switching technique that securely evaluates ReLU under FHE
constraints. Our findings show that the Square function performs well in
shallow networks like LeNet-5, achieving 99.4% accuracy with 128 seconds per
image. In contrast, deeper models like ResNet-20 benefit more from ReLU. The
polynomial approximation yields 83.8% accuracy with 1,145 seconds per image,
while our scheme-switching method improves accuracy to 89.8%, albeit with a
longer inference time of 1,697 seconds. These results underscore a critical
trade-off in FHE-based ML: faster activation functions often reduce accuracy,
whereas those preserving accuracy demand greater computational resources.

</details>


### [20] [CryptoScope: Utilizing Large Language Models for Automated Cryptographic Logic Vulnerability Detection](https://arxiv.org/abs/2508.11599)
*Zhihao Li,Zimo Ji,Tao Zheng,Hao Ren,Xiao Lan*

Main category: cs.CR

TL;DR: CryptoScope是一个基于大语言模型（LLMs）的自动化加密漏洞检测框架，结合了Chain-of-Thought提示和检索增强生成（RAG），在多个基准测试中显著提升了性能，并发现了开源加密项目中的新漏洞。


<details>
  <summary>Details</summary>
Motivation: 加密算法的实现常存在难以检测的逻辑漏洞，需要一种自动化工具来提高漏洞检测的效率和准确性。

Method: CryptoScope结合Chain-of-Thought提示和检索增强生成（RAG），利用包含12,000条记录的加密知识库进行指导。

Result: 在LLM-CLVA基准测试中，CryptoScope显著提升了多个LLM模型的性能（如DeepSeek-V3提升11.62%），并发现了9个未公开的开源项目漏洞。

Conclusion: CryptoScope通过结合LLM和加密知识库，有效提升了加密漏洞检测的能力，具有实际应用价值。

Abstract: Cryptographic algorithms are fundamental to modern security, yet their
implementations frequently harbor subtle logic flaws that are hard to detect.
We introduce CryptoScope, a novel framework for automated cryptographic
vulnerability detection powered by Large Language Models (LLMs). CryptoScope
combines Chain-of-Thought (CoT) prompting with Retrieval-Augmented Generation
(RAG), guided by a curated cryptographic knowledge base containing over 12,000
entries. We evaluate CryptoScope on LLM-CLVA, a benchmark of 92 cases primarily
derived from real-world CVE vulnerabilities, complemented by cryptographic
challenges from major Capture The Flag (CTF) competitions and synthetic
examples across 11 programming languages. CryptoScope consistently improves
performance over strong LLM baselines, boosting DeepSeek-V3 by 11.62%,
GPT-4o-mini by 20.28%, and GLM-4-Flash by 28.69%. Additionally, it identifies 9
previously undisclosed flaws in widely used open-source cryptographic projects.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [21] [Grounding Rule-Based Argumentation Using Datalog](https://arxiv.org/abs/2508.10976)
*Martin Diller,Sarah Alice Gaggl,Philipp Hanisch,Giuseppina Monterosso,Fritz Rauschenbach*

Main category: cs.AI

TL;DR: ASPIC+框架缺乏对一阶规则推理的智能基础化方法，本文提出了一种基于Datalog的智能基础化方法，并通过实验验证了其可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有ASPIC+方法主要支持命题规则，缺乏对一阶规则的高效基础化方法，导致输入理论规模可能指数级增长。

Method: 将一阶ASPIC+实例转化为Datalog程序，利用Datalog引擎获取基础化规则和反例，并针对ASPIC+形式化提出简化方法。

Result: 提出的方法能有效控制基础化规模，同时保持推理正确性，实验验证了其可扩展性。

Conclusion: 本文方法填补了ASPIC+在一阶规则推理中的空白，为高效基础化提供了可行方案。

Abstract: ASPIC+ is one of the main general frameworks for rule-based argumentation for
AI. Although first-order rules are commonly used in ASPIC+ examples, most
existing approaches to reason over rule-based argumentation only support
propositional rules. To enable reasoning over first-order instances, a
preliminary grounding step is required. As groundings can lead to an
exponential increase in the size of the input theories, intelligent procedures
are needed. However, there is a lack of dedicated solutions for ASPIC+.
Therefore, we propose an intelligent grounding procedure that keeps the size of
the grounding manageable while preserving the correctness of the reasoning
process. To this end, we translate the first-order ASPIC+ instance into a
Datalog program and query a Datalog engine to obtain ground substitutions to
perform the grounding of rules and contraries. Additionally, we propose
simplifications specific to the ASPIC+ formalism to avoid grounding of rules
that have no influence on the reasoning process. Finally, we performed an
empirical evaluation of a prototypical implementation to show scalability.

</details>


### [22] [From Individual to Multi-Agent Algorithmic Recourse: Minimizing the Welfare Gap via Capacitated Bipartite Matching](https://arxiv.org/abs/2508.11070)
*Zahra Khotanlou,Kate Larson,Amir-Hossein Karimi*

Main category: cs.AI

TL;DR: 论文提出了一种多主体算法追索框架，解决多追索者和提供者交互的问题，通过分层优化实现社会福利最大化。


<details>
  <summary>Details</summary>
Motivation: 现实应用中，算法追索通常涉及多个交互主体，而现有研究多关注单主体场景，忽略了多主体竞争资源的复杂性。

Method: 将多对多交互建模为带权二分图匹配问题，提出三层优化框架：基础匹配、容量再分配和成本感知优化。

Result: 实验表明，该框架能在系统设置最小修改下实现接近最优的社会福利。

Conclusion: 该研究将算法追索从个体推荐扩展到系统设计，兼顾社会福利与个体可操作性。

Abstract: Decision makers are increasingly relying on machine learning in sensitive
situations. In such settings, algorithmic recourse aims to provide individuals
with actionable and minimally costly steps to reverse unfavorable AI-driven
decisions. While existing research predominantly focuses on single-individual
(i.e., seeker) and single-model (i.e., provider) scenarios, real-world
applications often involve multiple interacting stakeholders. Optimizing
outcomes for seekers under an individual welfare approach overlooks the
inherently multi-agent nature of real-world systems, where individuals interact
and compete for limited resources. To address this, we introduce a novel
framework for multi-agent algorithmic recourse that accounts for multiple
recourse seekers and recourse providers. We model this many-to-many interaction
as a capacitated weighted bipartite matching problem, where matches are guided
by both recourse cost and provider capacity. Edge weights, reflecting recourse
costs, are optimized for social welfare while quantifying the welfare gap
between individual welfare and this collectively feasible outcome. We propose a
three-layer optimization framework: (1) basic capacitated matching, (2) optimal
capacity redistribution to minimize the welfare gap, and (3) cost-aware
optimization balancing welfare maximization with capacity adjustment costs.
Experimental validation on synthetic and real-world datasets demonstrates that
our framework enables the many-to-many algorithmic recourse to achieve
near-optimal welfare with minimum modification in system settings. This work
extends algorithmic recourse from individual recommendations to system-level
design, providing a tractable path toward higher social welfare while
maintaining individual actionability.

</details>


### [23] [Learn to optimize for automatic proton PBS treatment planning for H&N cancers](https://arxiv.org/abs/2508.11085)
*Qingqing Wang,Liqiang Xiao,Chang Chang*

Main category: cs.AI

TL;DR: 提出了一种基于数据驱动的逆优化器和PPO框架的自动治疗计划方法，显著提高了H&N癌症质子PBS治疗计划的效率和质量。


<details>
  <summary>Details</summary>
Motivation: 传统质子PBS治疗计划中，目标参数调整和逆优化耗时且依赖经验，亟需自动化方法提升效率。

Method: 结合L2O逆优化器和PPO框架，利用Transformer处理长上下文，自动调整目标参数并生成高质量计划。

Result: 相比L-BFGSB，L2O逆优化器效率提升36.41%，效果提升22.97%；生成的计划在2.55小时内达到或优于人工计划。

Conclusion: 该方法显著提升了治疗计划的自动化水平和临床适用性，为复杂病例提供了高效解决方案。

Abstract: Proton PBS treatment planning for H&N cancers involves numerous conflicting
objectives, requiring significant effort from human planners to balance and
satisfy multiple clinical goals during planning. To achieve this,
experience-demanding objective parameter adjustment and computationally
expensive inverse optimization are performed iteratively. Extensive efforts
have been made to automatically adjust objective parameters, but the most
time-consuming component, i.e., inverse optimization, still relies heavily on
theory-driven approaches. We propose a data-driven inverse optimizer and
integrate it into a PPO-based automatic treatment planning framework to
automatically generate high-quality plans within a clinical acceptable planning
time. The inverse optimizer is a L2O method that predicts update steps by
learning from the task-specific data distribution. For the first time, we
integrate techniques designed for long-context processing, originally developed
for LLMs, into a Transformer-based L2O framework to address the scalability
issue of existing L2O methods. The PPO framework functions as an outer-loop
virtual planner, autonomously adjusting objective parameters through a policy
network, and the dose predictor is used to initialize objective parameters. The
inner-loop L2O inverse optimizer computes machine-deliverable MU values based
on objectives refined by the PPO policy network. 97 patients are collected in
this study, and compared with L-BFGSB, our L2O-based inverse optimizer improves
the effectiveness and efficiency by 22.97% and 36.41%, respectively. In
conjunction with the PPO-based learned virtual planner, plans generated by our
framework within an average of 2.55 hours show improved or comparable OAR
sparing with superior target coverage for patients with different prescription
dose levels, number of target volumes, beam angles, etc., compared with
human-generated plans.

</details>


### [24] [On Strong and Weak Admissibility in Non-Flat Assumption-Based Argumentation](https://arxiv.org/abs/2508.11182)
*Matti Berthold,Lydia Blümel,Anna Rapberger*

Main category: cs.AI

TL;DR: 本文扩展了假设基础论证（ABA）中可接受性概念的研究，重点研究了强和弱可接受性，并引入了相应的优选、完全和基础语义。


<details>
  <summary>Details</summary>
Motivation: 研究ABA中非标准可接受性概念（强和弱可接受性）及其在非平坦ABA框架中的应用，填补强可接受性研究的空白。

Method: 使用抽象双极集合基础论证框架（BSAFs）作为形式化工具，研究强和弱可接受性在非平坦ABA中的性质。

Result: 证明了强和弱可接受性在非平坦ABA中保持模块化性质，但也存在与标准可接受性类似的缺陷。

Conclusion: 强和弱可接受性在非平坦ABA中具有潜力，但需进一步研究以解决其缺陷。

Abstract: In this work, we broaden the investigation of admissibility notions in the
context of assumption-based argumentation (ABA). More specifically, we study
two prominent alternatives to the standard notion of admissibility from
abstract argumentation, namely strong and weak admissibility, and introduce the
respective preferred, complete and grounded semantics for general (sometimes
called non-flat) ABA. To do so, we use abstract bipolar set-based argumentation
frameworks (BSAFs) as formal playground since they concisely capture the
relations between assumptions and are expressive enough to represent general
non-flat ABA frameworks, as recently shown. While weak admissibility has been
recently investigated for a restricted fragment of ABA in which assumptions
cannot be derived (flat ABA), strong admissibility has not been investigated
for ABA so far. We introduce strong admissibility for ABA and investigate
desirable properties. We furthermore extend the recent investigations of weak
admissibility in the flat ABA fragment to the non-flat case. We show that the
central modularization property is maintained under classical, strong, and weak
admissibility. We also show that strong and weakly admissible semantics in
non-flat ABA share some of the shortcomings of standard admissible semantics
and discuss ways to address these.

</details>


### [25] [Beyond Solving Math Quiz: Evaluating the Ability of Large Reasoning Models to Ask for Information](https://arxiv.org/abs/2508.11252)
*Youcheng Huang,Bowen Qin,Chen Huang,Duanyu Feng,Xi Yang,Wenqiang Lei*

Main category: cs.AI

TL;DR: 论文提出新数据集评估大型推理模型（LRMs）在不完整问题上的表现，发现其无法主动请求信息，并揭示其过度思考和幻觉行为。


<details>
  <summary>Details</summary>
Motivation: 现有基准仅评估LRMs在明确问题上的表现，忽略了其作为智能代理应具备的主动请求信息的能力。

Method: 构建包含两类不完整问题的数据集，系统评估LRMs的表现。

Result: LRMs无法主动请求信息，存在过度思考和幻觉行为。监督微调在此能力学习上具有潜力与挑战。

Conclusion: 研究为开发真正智能的LRMs提供了新视角，强调其不仅需解决问题，还需具备主动交互能力。

Abstract: Large Reasoning Models (LRMs) have demonstrated remarkable problem-solving
abilities in mathematics, as evaluated by existing benchmarks exclusively on
well-defined problems. However, such evaluation setup constitutes a critical
gap, since a genuine intelligent agent should not only solve problems (as a
math quiz solver), but also be able~to ask for information when the problems
lack sufficient information, enabling proactivity in responding users'
requests. To bridge such gap, we proposes a new dataset consisting of two types
of incomplete problems with diverse contexts. Based on the dataset, our
systematical evaluation of LRMs reveals their inability in proactively asking
for information. In addition, we uncover the behaviors related to overthinking
and hallucination of LRMs, and highlight the potential and challenges of
supervised fine-tuning in learning such ability. We hope to provide new
insights in developing LRMs with genuine intelligence, rather than just solving
problems.

</details>


### [26] [SAGE: Scale-Aware Gradual Evolution for Continual Knowledge Graph Embedding](https://arxiv.org/abs/2508.11347)
*Yifei Li,Lingling Zhang,Hang Yan,Tianzhe Zhao,Zihan Ma,Muye Huang,Jun Liu*

Main category: cs.AI

TL;DR: SAGE提出了一种动态知识图谱嵌入框架，通过自适应维度扩展和动态蒸馏机制，显著提升了动态知识图谱的性能。


<details>
  <summary>Details</summary>
Motivation: 传统知识图谱嵌入方法主要针对静态图谱，而现实中的知识图谱是动态变化的，现有方法未能充分考虑更新的不同规模和系统评估。

Method: SAGE框架根据更新规模确定嵌入维度并扩展嵌入空间，采用动态蒸馏机制平衡新旧知识的保留与整合。

Result: 在七个基准测试中，SAGE显著优于现有基线方法，MRR、H@1和H@10分别提升1.38%、1.25%和1.6%。

Conclusion: SAGE证明了自适应嵌入维度在动态知识图谱嵌入中的重要性，并在每个快照上实现了最优性能。

Abstract: Traditional knowledge graph (KG) embedding methods aim to represent entities
and relations in a low-dimensional space, primarily focusing on static graphs.
However, real-world KGs are dynamically evolving with the constant addition of
entities, relations and facts. To address such dynamic nature of KGs, several
continual knowledge graph embedding (CKGE) methods have been developed to
efficiently update KG embeddings to accommodate new facts while maintaining
learned knowledge. As KGs grow at different rates and scales in real-world
scenarios, existing CKGE methods often fail to consider the varying scales of
updates and lack systematic evaluation throughout the entire update process. In
this paper, we propose SAGE, a scale-aware gradual evolution framework for
CKGE. Specifically, SAGE firstly determine the embedding dimensions based on
the update scales and expand the embedding space accordingly. The Dynamic
Distillation mechanism is further employed to balance the preservation of
learned knowledge and the incorporation of new facts. We conduct extensive
experiments on seven benchmarks, and the results show that SAGE consistently
outperforms existing baselines, with a notable improvement of 1.38% in MRR,
1.25% in H@1 and 1.6% in H@10. Furthermore, experiments comparing SAGE with
methods using fixed embedding dimensions show that SAGE achieves optimal
performance on every snapshot, demonstrating the importance of adaptive
embedding dimensions in CKGE. The codes of SAGE are publicly available at:
https://github.com/lyfxjtu/Dynamic-Embedding.

</details>


### [27] [CRAFT-GUI: Curriculum-Reinforced Agent For GUI Tasks](https://arxiv.org/abs/2508.11360)
*Songqin Nong,Jingxuan Xu,Sheng Zhou,Jianfeng Chen,Xiaoxuan Tang,Tao Jiang,Wenhao Xu*

Main category: cs.AI

TL;DR: 提出CRAFT-GUI框架，通过课程学习和细粒度奖励优化，提升GUI交互任务中强化学习的效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视GUI任务难度的差异，且奖励信号过于粗糙，导致学习效率低下。

Method: 基于GRPO的课程学习框架，结合规则和模型评估设计细粒度奖励函数。

Result: 在公开和内部基准测试中分别提升5.6%和10.3%，验证了方法的有效性。

Conclusion: 结合强化学习和课程学习能显著提升GUI交互任务的性能。

Abstract: As autonomous agents become adept at understanding and interacting with
graphical user interface (GUI) environments, a new era of automated task
execution is emerging. Recent studies have demonstrated that Reinforcement
Learning (RL) can effectively enhance agents' performance in dynamic
interactive GUI environments. However, these methods face two key limitations:
(1) they overlook the significant variation in difficulty across different GUI
tasks by treating the entire training data as a uniform set, which hampers the
agent's ability to adapt its learning process; and (2) most approaches collapse
task-specific nuances into a single, coarse reward, leaving the agent with a
uniform signal that yields inefficient policy updates. To address these
limitations, we propose CRAFT-GUI, a curriculum learning framework based on
Group Relative Policy Optimization (GRPO) that explicitly accounts for the
varying difficulty across trajectories. To enable more fine-grained policy
optimization, we design a reward function that combines simple rule-based
signals with model-judged evaluation, providing richer and more nuanced
feedback during training. Experimental results demonstrate that our method
achieves significant improvements over previous state-of-the-art approaches,
outperforming them by 5.6% on public benchmarks Android Control and 10.3% on
our internal online benchmarks, respectively. These findings empirically
validate the effectiveness of integrating reinforcement learning with
curriculum learning in GUI interaction tasks.

</details>


### [28] [AIM-Bench: Evaluating Decision-making Biases of Agentic LLM as Inventory Manager](https://arxiv.org/abs/2508.11416)
*Xuhua Zhao,Yuxuan Xie,Caihua Chen,Yuxiang Sun*

Main category: cs.AI

TL;DR: 论文提出AIM-Bench基准，评估LLM代理在不确定供应链管理中的决策行为，发现其存在类似人类的决策偏差，并探讨了缓解方法。


<details>
  <summary>Details</summary>
Motivation: 研究LLM代理在库存决策中的能力和潜在偏差，填补其在不确定环境下决策行为的研究空白。

Method: 通过AIM-Bench基准，设计多样化的库存补充实验，评估不同LLM的决策行为。

Result: 不同LLM表现出类似人类的决策偏差，并探索了缓解策略（认知反思和信息共享）。

Conclusion: 需谨慎考虑LLM在库存决策中的潜在偏差，为开发人本决策支持系统提供启示。

Abstract: Recent advances in mathematical reasoning and the long-term planning
capabilities of large language models (LLMs) have precipitated the development
of agents, which are being increasingly leveraged in business operations
processes. Decision models to optimize inventory levels are one of the core
elements of operations management. However, the capabilities of the LLM agent
in making inventory decisions in uncertain contexts, as well as the
decision-making biases (e.g. framing effect, etc.) of the agent, remain largely
unexplored. This prompts concerns regarding the capacity of LLM agents to
effectively address real-world problems, as well as the potential implications
of biases that may be present. To address this gap, we introduce AIM-Bench, a
novel benchmark designed to assess the decision-making behaviour of LLM agents
in uncertain supply chain management scenarios through a diverse series of
inventory replenishment experiments. Our results reveal that different LLMs
typically exhibit varying degrees of decision bias that are similar to those
observed in human beings. In addition, we explored strategies to mitigate the
pull-to-centre effect and the bullwhip effect, namely cognitive reflection and
implementation of information sharing. These findings underscore the need for
careful consideration of the potential biases in deploying LLMs in Inventory
decision-making scenarios. We hope that these insights will pave the way for
mitigating human decision bias and developing human-centred decision support
systems for supply chains.

</details>


### [29] [Inclusion Arena: An Open Platform for Evaluating Large Foundation Models with Real-World Apps](https://arxiv.org/abs/2508.11452)
*Kangyu Wang,Hongliang He,Lin Liu,Ruiqi Liang,Zhenzhong Lan,Jianguo Li*

Main category: cs.AI

TL;DR: Inclusion Arena是一个实时排行榜，通过从AI应用中收集的人类反馈对模型进行排名，解决了现有基准测试在真实场景中的不足。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试多依赖静态数据集或众包通用提示，难以反映模型在实际应用中的表现。

Method: 平台整合了成对模型比较，采用Bradley-Terry模型，并引入Placement Matches和Proximity Sampling两项创新技术。

Result: 实证分析表明，Inclusion Arena提供可靠且稳定的排名，数据传递性更高，并能显著减少恶意操纵风险。

Conclusion: Inclusion Arena通过连接基础模型和实际应用，加速开发更实用的LLMs和MLLMs。

Abstract: Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs)
have ushered in a new era of AI capabilities, demonstrating near-human-level
performance across diverse scenarios. While numerous benchmarks (e.g., MMLU)
and leaderboards (e.g., Chatbot Arena) have been proposed to help evolve the
development of LLMs and MLLMs, most rely on static datasets or crowdsourced
general-domain prompts, often falling short of reflecting performance in
real-world applications. To bridge this critical gap, we present Inclusion
Arena, a live leaderboard that ranks models based on human feedback collected
directly from AI-powered applications. Our platform integrates pairwise model
comparisons into natural user interactions, ensuring evaluations reflect
practical usage scenarios. For robust model ranking, we employ the
Bradley-Terry model augmented with two key innovations: (1) Placement Matches,
a cold-start mechanism to quickly estimate initial ratings for newly integrated
models, and (2) Proximity Sampling, an intelligent comparison strategy that
prioritizes battles between models of similar capabilities to maximize
information gain and enhance rating stability. Extensive empirical analyses and
simulations demonstrate that Inclusion Arena yields reliable and stable
rankings, exhibits higher data transitivity compared to general crowdsourced
datasets, and significantly mitigates the risk of malicious manipulation. By
fostering an open alliance between foundation models and real-world
applications, Inclusion Arena aims to accelerate the development of LLMs and
MLLMs truly optimized for practical, user-centric deployments. The platform is
publicly accessible at https://doraemon.alipay.com/model-ranking.

</details>


### [30] [Landmark-Assisted Monte Carlo Planning](https://arxiv.org/abs/2508.11493)
*David H. Chan,Mark Roberts,Dana S. Nau*

Main category: cs.AI

TL;DR: 论文提出了一种利用概率性地标（probabilistic landmarks）改进UCT算法在随机规划中性能的方法，并通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 地标在经典规划中已有显著贡献，但在随机领域应用较少。本文旨在探索地标在随机规划中的潜力。

Method: 将概率性地标形式化，并调整UCT算法以地标为子目标分解MDP，核心在于平衡贪婪地标达成与长期目标达成。

Result: 实验表明，合适的地标能显著提升UCT在在线概率规划中的性能，但最佳平衡点因问题而异。

Conclusion: 地标可为解决MDP的随时算法提供有效指导。

Abstract: Landmarks$\unicode{x2013}$conditions that must be satisfied at some point in
every solution plan$\unicode{x2013}$have contributed to major advancements in
classical planning, but they have seldom been used in stochastic domains. We
formalize probabilistic landmarks and adapt the UCT algorithm to leverage them
as subgoals to decompose MDPs; core to the adaptation is balancing between
greedy landmark achievement and final goal achievement. Our results in
benchmark domains show that well-chosen landmarks can significantly improve the
performance of UCT in online probabilistic planning, while the best balance of
greedy versus long-term goal achievement is problem-dependent. The results
suggest that landmarks can provide helpful guidance for anytime algorithms
solving MDPs.

</details>


### [31] [Inspire or Predict? Exploring New Paradigms in Assisting Classical Planners with Large Language Models](https://arxiv.org/abs/2508.11524)
*Wenkai Yu,Jianhang Tang,Yang Zhang,Shanjiang Tang,Kebing Jin,Hankz Hankui Zhuo*

Main category: cs.AI

TL;DR: 论文提出了一种结合LLM和问题分解的新型规划器，通过LLM4Inspire和LLM4Predict两种范式辅助分解大规模规划问题，实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决大规模规划问题中的状态空间爆炸问题，并探索如何结合LLM与领域知识以生成有效规划。

Method: 提出LLM辅助规划器，结合问题分解和两种LLM范式（LLM4Inspire和LLM4Predict），分别利用通用知识和领域知识指导分解。

Result: 实验结果表明，LLM能有效修剪搜索空间并找到可行解，其中结合领域知识的LLM4Predict表现优于仅依赖通用知识的LLM4Inspire。

Conclusion: 结合LLM与领域知识的问题分解方法在大规模规划问题中具有潜力，尤其是LLM4Predict范式表现突出。

Abstract: Addressing large-scale planning problems has become one of the central
challenges in the planning community, deriving from the state-space explosion
caused by growing objects and actions. Recently, researchers have explored the
effectiveness of leveraging Large Language Models (LLMs) to generate helpful
actions and states to prune the search space. However, prior works have largely
overlooked integrating LLMs with domain-specific knowledge to ensure valid
plans. In this paper, we propose a novel LLM-assisted planner integrated with
problem decomposition, which first decomposes large planning problems into
multiple simpler sub-tasks. Then we explore two novel paradigms to utilize
LLMs, i.e., LLM4Inspire and LLM4Predict, to assist problem decomposition, where
LLM4Inspire provides heuristic guidance according to general knowledge and
LLM4Predict employs domain-specific knowledge to infer intermediate conditions.
We empirically validate the effectiveness of our planner across multiple
domains, demonstrating the ability of search space partition when solving
large-scale planning problems. The experimental results show that LLMs
effectively locate feasible solutions when pruning the search space, where
infusing domain-specific knowledge into LLMs, i.e., LLM4Predict, holds
particular promise compared with LLM4Inspire, which offers general knowledge
within LLMs.

</details>
