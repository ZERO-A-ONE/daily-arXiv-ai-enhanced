<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 24]
- [cs.CR](#cs.CR) [Total: 15]
- [cs.MA](#cs.MA) [Total: 3]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Enhancing Clinical Decision Support and EHR Insights through LLMs and the Model Context Protocol: An Open-Source MCP-FHIR Framework](https://arxiv.org/abs/2506.13800)
*Abul Ehtesham,Aditi Singh,Saket Kumar*

Main category: cs.SE

TL;DR: 本文提出了一种基于开源代理的框架，通过整合大型语言模型（LLMs）和HL7 FHIR数据，动态提取和推理电子健康记录（EHRs），以提升临床决策支持、减轻文档负担并改善患者健康素养。


<details>
  <summary>Details</summary>
Motivation: 解决数字健康领域中临床决策支持不足、文档负担重和患者健康素养低的问题。

Method: 采用基于代理的框架，通过Model Context Protocol（MCP）整合LLMs与FHIR数据，支持实时总结、解释和个性化通信。

Result: 框架在SMART Health IT沙盒中使用合成EHR数据进行了评估，展示了可扩展、可解释和互操作的AI驱动EHR应用。

Conclusion: 该框架为个性化数字健康解决方案提供了坚实基础，支持多种FHIR格式和用户角色。

Abstract: Enhancing clinical decision support (CDS), reducing documentation burdens, and improving patient health literacy remain persistent challenges in digital health. This paper presents an open-source, agent-based framework that integrates Large Language Models (LLMs) with HL7 FHIR data via the Model Context Protocol (MCP) for dynamic extraction and reasoning over electronic health records (EHRs). Built on the established MCP-FHIR implementation, the framework enables declarative access to diverse FHIR resources through JSON-based configurations, supporting real-time summarization, interpretation, and personalized communication across multiple user personas, including clinicians, caregivers, and patients. To ensure privacy and reproducibility, the framework is evaluated using synthetic EHR data from the SMART Health IT sandbox (https://r4.smarthealthit.org/), which conforms to the FHIR R4 standard. Unlike traditional approaches that rely on hardcoded retrieval and static workflows, the proposed method delivers scalable, explainable, and interoperable AI-powered EHR applications. The agentic architecture further supports multiple FHIR formats, laying a robust foundation for advancing personalized digital health solutions.

</details>


### [2] [Instruction and Solution Probabilities as Heuristics for Inductive Programming](https://arxiv.org/abs/2506.13804)
*Edward McDaid,Sarah McDaid*

Main category: cs.SE

TL;DR: 论文提出了一种通过引入指令和解决方案概率来进一步缩小归纳编程搜索空间的方法，结合指令子集（IS）可将搜索空间缩小超过100个数量级。


<details>
  <summary>Details</summary>
Motivation: 为了更高效地缩小归纳编程（IP）的搜索空间，作者提出利用指令和解决方案概率作为额外的启发式方法。

Method: 通过计算指令在代码样本中的出现频率定义指令概率，并基于此计算解决方案概率。利用观察到的解决方案概率阈值在搜索过程中剪枝。

Result: 实验表明，该方法能进一步显著缩小搜索空间（高达数十个数量级），结合IS时效果更佳（超过100个数量级）。

Conclusion: 该方法有效且适用于未见过的代码，未来可进一步优化。

Abstract: Instruction subsets (ISs) are heuristics that can shrink the size of the inductive programming (IP) search space by tens of orders of magnitude. Here, we extend the IS approach by introducing instruction and solution probabilities as additional heuristics. Instruction probability reflects the expectation of an instruction occurring in a solution, based on the frequency of instruction occurrence in a large code sample. The solution probability for a partial or complete program is simply the product of all constituent instruction probabilities, including duplicates. We treat the minimum solution probabilities observed in code sample program units of different sizes as solution probability thresholds. These thresholds are used to prune the search space as partial solutions are constructed, thereby eliminating any branches containing unlikely combinations of instructions. The new approach has been evaluated using a large sample of human code. We tested two formulations of instruction probability: one based on instruction occurrence across the entire code sample and another that measured the distribution separately for each IS. Our results show that both variants produce substantial further reductions in the IP search space size of up to tens of orders of magnitude, depending on solution size. In combination with IS, reductions of over 100 orders of magnitude can be achieved. We also carried out cross-validation testing to show that the heuristics should work effectively with unseen code. The approach is described and the results and some ideas for future work are discussed.

</details>


### [3] [Signal-First Architectures: Rethinking Front-End Reactivity](https://arxiv.org/abs/2506.13815)
*Shrinivass Arunachalam Balasubramanian*

Main category: cs.SE

TL;DR: 本文提出了一种名为Signal-First Architecture的新范式，通过依赖追踪的信号作为反应性的原子单位，解决了现代前端框架中反应性管理的挑战。


<details>
  <summary>Details</summary>
Motivation: 现代前端框架面临反应性管理的挑战，如复杂可观察链导致的性能下降和不可预测的重新渲染。

Method: Signal-First Architecture通过显式信号声明、computed()派生值和effect()限定副作用，确保确定性行为。

Result: 通过对比Angular的RxJS、NgRx和Signal-First模型，实验证明Signal-First在性能、内存和审计方面具有优势。

Conclusion: Signal-First Architecture通过优化反应性图评估，提供了一种更高效和可预测的反应性管理方案。

Abstract: Modern front-end frameworks face escalating reactivity management challenges, including performance degradation from complex observable chains and unpredictable re-renders. This paper introduces Signal-First Architecture--a novel paradigm where granular, dependency-tracked signals are the atomic unit of reactivity. Unlike traditional RxJS or NgRx patterns, Signal-First enforces reactive flows from explicit signal declarations, with derived values via computed() and side effects scoped to effect(). This model ensures deterministic behavior by eliminating implicit subscriptions and optimizing reactive graph evaluation.
  We present a comparative analysis of three Angular reactivity models: RxJS service-based, NgRx global stores, and pure Signal-First implementations. Through controlled benchmarking, including Chrome DevTools performance tracing, memory heap snapshots, and Lighthouse audits, this study quantifies Signal-First advantages.

</details>


### [4] [Structured Program Synthesis using LLMs: Results and Insights from the IPARC Challenge](https://arxiv.org/abs/2506.13820)
*Shraddha Surana,Ashwin Srinivasan,Michael Bain*

Main category: cs.SE

TL;DR: IPARC Challenge通过合成图像任务评估程序合成能力，提出了一种基于LLM的结构化归纳编程方法，成功解决所有任务类别，揭示了LLM代码生成的关键机制。


<details>
  <summary>Details</summary>
Motivation: IPARC Challenge的600项任务难以自动化解决，研究旨在探索LLM在程序合成中的潜力及其与人类协作的价值。

Method: 采用结构化归纳编程方法，结合LLM生成代码，并通过人类细化优化结果。

Result: 成功解决所有IPARC任务类别，揭示了LLM代码生成的关键机制，如代码冻结、重用和激发人类创造力。

Conclusion: 研究为人类与LLM协作解决复杂程序合成问题提供了有价值的机制。

Abstract: The IPARC Challenge, inspired by ARC, provides controlled program synthesis tasks over synthetic images to evaluate automatic program construction, focusing on sequence, selection, and iteration. This set of 600 tasks has resisted automated solutions. This paper presents a structured inductive programming approach with LLMs that successfully solves tasks across all IPARC categories. The controlled nature of IPARC reveals insights into LLM-based code generation, including the importance of prior structuring, LLMs' ability to aid structuring (requiring human refinement), the need to freeze correct code, the efficiency of code reuse, and how LLM-generated code can spark human creativity. These findings suggest valuable mechanisms for human-LLM collaboration in tackling complex program synthesis.

</details>


### [5] [Role, cost, and complexity of software in the real-world: a case for formal methods](https://arxiv.org/abs/2506.13821)
*Giovanni Bernardi,Adrian Francalanza,Marco Peressotti,Mohammad Reza Mousavi*

Main category: cs.SE

TL;DR: 本章讨论了软件在现代社会中的作用以及低质量软件的巨大成本，并通过过去40年的重大软件故障案例支持研究形式化软件验证和程序分析的必要性。


<details>
  <summary>Details</summary>
Motivation: 揭示低质量软件带来的巨大成本，并论证形式化软件验证和程序分析的重要性。

Method: 回顾过去40年的重大软件故障案例及其成本。

Result: 通过工业成功案例支持形式化软件验证和程序分析的应用价值。

Conclusion: 形式化软件验证和程序分析的研究和应用是必要的，以降低软件故障的成本。

Abstract: In this chapter we outline the role that software has in modern society, along with the staggering costs of poor software quality. To lay this bare, we recall the costs of some of the major software failures that happened during the last~$40$ years. We argue that these costs justify researching, studying and applying formal software verification and in particular program analysis. This position is supported by successful industrial experiences.

</details>


### [6] [MLDebugging: Towards Benchmarking Code Debugging Across Multi-Library Scenarios](https://arxiv.org/abs/2506.13824)
*Jinyang Huang,Xiachong Feng,Qiguang Chen,Hanjie Zhao,Zihui Cheng,Jiesong Bai,Jingxuan Zhou,Min Li,Libo Qin*

Main category: cs.SE

TL;DR: 论文提出了MLDebugging，一个针对多库Python代码调试的基准测试，评估了主流LLMs在此场景下的表现。


<details>
  <summary>Details</summary>
Motivation: 当前研究主要关注简单无库或单库场景，忽略了现实应用中复杂的多库调试问题。

Method: 构建了包含126个Python库的MLDebugging基准，涵盖七类多库代码问题，并评估了开源和闭源LLMs的表现。

Result: 当前LLMs在多库调试场景中表现不佳。

Conclusion: MLDebugging揭示了LLMs在多库调试中的潜力，为未来研究提供了方向。

Abstract: Code debugging is a crucial task in software engineering, which attracts increasing attention. While remarkable success has been made in the era of large language models (LLMs), current research still focuses on the simple no-library or single-library setting, ignoring the complex multi-library scenario in real-world applications. To address this limitation, we make the first attempt to introduce MLDebugging (Multi-Library Debugging), a comprehensive benchmark designed to assess debugging challenges within multi-library Python code. Specifically, MLDebugging encompasses 126 distinct Python libraries, covering a wide range of multi-library code issues, categorized into seven distinct types. Furthermore, we conduct a thorough evaluation of MLDebugging using both mainstream open-source and closed-source LLMs and highlight that current LLMs still struggle to correctly perform code debugging across multi-library scenarios. We hope this work can uncover the potential of LLMs in multi-library debugging scenario and offer insights for future research.

</details>


### [7] [FrontendBench: A Benchmark for Evaluating LLMs on Front-End Development via Automatic Evaluation](https://arxiv.org/abs/2506.13832)
*Hongda Zhu,Yiwen Zhang,Bing Zhao,Jingzhe Ding,Siyao Liu,Tong Liu,Dandan Wang,Yanan Liu,Zhaojian Li*

Main category: cs.SE

TL;DR: FrontendBench是一个由人类和LLMs共同开发的基准测试，用于更全面、实际地评估前端代码生成能力，解决了现有基准测试的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试存在任务过于简单、测试用例不严谨和缺乏端到端验证等问题，无法准确评估模型性能。

Method: FrontendBench包含148个任务，覆盖从基础UI到复杂交互功能的五个级别，并引入自动评估框架，在沙盒环境中执行代码并通过测试脚本评估结果。

Result: 自动评估框架与专家评估的一致性达90.54%，且在不同LLMs上观察到显著的性能差异。

Conclusion: FrontendBench是一个可靠且可扩展的基准测试，为前端代码生成的未来研究提供了坚实基础。

Abstract: Large Language Models (LLMs) have made significant strides in front-end code generation. However, existing benchmarks exhibit several critical limitations: many tasks are overly simplistic, test cases often lack rigor, and end-to-end validation is absent. These issues hinder the accurate assessment of model performance. To address these challenges, we present FrontendBench, a benchmark co-developed by humans and LLMs. FrontendBench categorizes tasks based on code functionality and incorporates interactive test scenarios, enabling a more comprehensive and practical evaluation of front-end code generation capabilities. The benchmark comprises 148 meticulously crafted prompt-test case pairs spanning five levels of web components, from basic UI elements to complex interactive features. Each task reflects realistic front-end development challenges. Furthermore, we introduce an automatic evaluation framework that executes generated code within a sandbox environment and assesses outcomes using predefined test scripts. This framework achieves a 90.54% agreement rate with expert human evaluations, demonstrating high reliability. We benchmark several state-of-the-art LLMs on FrontendBench and observe substantial performance disparities in handling real-world front-end tasks. These results highlight FrontendBench as a reliable and scalable benchmark, supporting consistent multimodal evaluation and providing a robust foundation for future research in front-end code generation. Our data and code will be released soon.

</details>


### [8] [How Does LLM Reasoning Work for Code? A Survey and a Call to Action](https://arxiv.org/abs/2506.13932)
*Ira Ceka,Saurabh Pujar,Irene Manotas,Gail Kaiser,Baishakhi Ray,Shyam Ramji*

Main category: cs.SE

TL;DR: 本文综述了大型语言模型（LLMs）在代码推理任务中的应用，总结了现有技术、分类方法、性能评估及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在代码任务中的实际应用潜力，特别是在软件工程（SWE）领域，如GitHub问题解决。

Method: 通过调查和分类代码推理技术，分析性能基准，并探索代码核心属性对推理技术的影响。

Result: 提出了代码推理的总体策略、混合和代理方法分类，并展示了新基准的潜力。

Conclusion: 总结了当前研究的不足，并指出了未来可能的研究方向。

Abstract: The rise of large language models (LLMs) has led to dramatic improvements across a wide range of natural language tasks. These advancements have extended into the domain of code, facilitating complex tasks such as code generation, translation, summarization, and repair. However, their utility for real-world deployment in-the-wild has only recently been studied, particularly on software engineering (SWE) tasks such as GitHub issue resolution. In this study, we examine the code reasoning techniques that underlie the ability to perform such tasks, and examine the paradigms used to drive their performance. Our contributions in this paper are: (1) the first dedicated survey on code reasoning for code tasks, highlighting overarching strategies, hybrid and agentic approaches; (2) a taxonomy of various techniques used to drive code reasoning; (3) a comprehensive overview of performance on common benchmarks and a showcase of new, under-explored benchmarks with high potential in SWE; (4) an exploration on how core properties of code can be used to explain different reasoning techniques; and (5) gaps and potentially under-explored areas for future research.

</details>


### [9] [CRITICTOOL: Evaluating Self-Critique Capabilities of Large Language Models in Tool-Calling Error Scenarios](https://arxiv.org/abs/2506.13977)
*Shiting Huang,Zhen Fang,Zehui Chen,Siyu Yuan,Junjie Ye,Yu Zeng,Lin Chen,Qi Mao,Feng Zhao*

Main category: cs.SE

TL;DR: 论文提出CRITICTOOL，一个专为工具学习设计的评估基准，用于处理复杂任务中LLM工具使用的错误。


<details>
  <summary>Details</summary>
Motivation: 随着任务复杂性和长期性增加，LLM工具使用过程中可能触发意外错误，需研究如何有效识别、诊断和恢复这些错误。

Method: 通过分析多个工具评估基准中的错误类型，提出基于进化策略的数据集构建方法，创建CRITICTOOL。

Result: 实验验证了CRITICTOOL的泛化性和有效性，并深入分析了不同LLM的工具反思能力。

Conclusion: CRITICTOOL为LLM工具学习提供了新的评估视角，推动了该领域的发展。

Abstract: The ability of large language models (LLMs) to utilize external tools has enabled them to tackle an increasingly diverse range of tasks. However, as the tasks become more complex and long-horizon, the intricate tool utilization process may trigger various unexpected errors. Therefore, how to effectively handle such errors, including identifying, diagnosing, and recovering from them, has emerged as a key research direction for advancing tool learning. In this work, we first extensively analyze the types of errors encountered during the function-calling process on several competitive tool evaluation benchmarks. Based on it, we introduce CRITICTOOL, a comprehensive critique evaluation benchmark specialized for tool learning. Building upon a novel evolutionary strategy for dataset construction, CRITICTOOL holds diverse tool-use errors with varying complexities, which better reflects real-world scenarios. We conduct extensive experiments on CRITICTOOL, and validate the generalization and effectiveness of our constructed benchmark strategy. We also provide an in-depth analysis of the tool reflection ability on various LLMs, offering a new perspective on the field of tool learning in LLMs. The code is available at \href{https://github.com/Shellorley0513/CriticTool}{https://github.com/Shellorley0513/CriticTool}.

</details>


### [10] [Characterising Bugs in Jupyter Platform](https://arxiv.org/abs/2506.14055)
*Yutian Tang,Hongchen Cao,Yuxi Chen,David Lo*

Main category: cs.SE

TL;DR: 本文研究了Jupyter平台中的387个bug，将其分类为11种根本原因和11种症状，并提出了14个主要发现，为开发者提供了新工具开发方向。


<details>
  <summary>Details</summary>
Motivation: Jupyter作为流行的编程平台，其正确性、安全性和鲁棒性至关重要，但此前研究未关注其平台本身的bug。

Method: 分析了Jupyter平台中的387个bug，并进行了分类研究。

Result: 识别了11种根本原因和11种症状，总结了14个主要发现。

Conclusion: 研究为开发检测和修复Jupyter平台bug的工具提供了新方向。

Abstract: As a representative literate programming platform, Jupyter is widely adopted by developers, data analysts, and researchers for replication, data sharing, documentation, interactive data visualization, and more. Understanding the bugs in the Jupyter platform is essential for ensuring its correctness, security, and robustness. Previous studies focused on code reuse, restoration, and repair execution environment for Jupyter notebooks. However, the bugs in Jupyter notebooks' hosting platform Jupyter are not investigated. In this paper, we investigate 387 bugs in the Jupyter platform. These Jupyter bugs are classified into 11 root causes and 11 bug symptoms. We identify 14 major findings for developers. More importantly, our study opens new directions in building tools for detecting and fixing bugs in the Jupyter platform.

</details>


### [11] [A Quantum Annealing Approach for Solving Optimal Feature Selection and Next Release Problems](https://arxiv.org/abs/2506.14129)
*Shuchang Wang,Xiaopeng Qiu,Yingxing Xue,Yanfu Li,Wei Yang*

Main category: cs.SE

TL;DR: 论文提出基于量子退火（QA）的算法，解决搜索式软件工程（SBSE）中的多目标优化问题，针对不同规模问题提出两种方法，实验表明其在执行时间和解质量上优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统启发式方法和整数线性规划（ILP）在小规模SBSE问题中有效，但大规模问题的可扩展性未知，因此探索量子退火的潜力。

Method: 针对小规模问题，将多目标优化（MOO）转化为单目标优化（SOO）；针对大规模问题，采用基于最大能量影响（MEI）的分解策略，结合量子退火和局部搜索。

Result: 实验表明，QA方法在非支配解数量上虽不及$ε$-约束方法，但显著减少执行时间；与NSGA-II相比，提供更多非支配解且计算效率更高。

Conclusion: 量子退火为SBSE问题提供了可扩展且高效的解决方案，展示了其在复杂优化问题中的潜力。

Abstract: Search-based software engineering (SBSE) addresses critical optimization challenges in software engineering, including the next release problem (NRP) and feature selection problem (FSP). While traditional heuristic approaches and integer linear programming (ILP) methods have demonstrated efficacy for small to medium-scale problems, their scalability to large-scale instances remains unknown. Here, we introduce quantum annealing (QA) as a subroutine to tackling multi-objective SBSE problems, leveraging the computational potential of quantum systems. We propose two QA-based algorithms tailored to different problem scales. For small-scale problems, we reformulate multi-objective optimization (MOO) as single-objective optimization (SOO) using penalty-based mappings for quantum processing. For large-scale problems, we employ a decomposition strategy guided by maximum energy impact (MEI), integrating QA with a steepest descent method to enhance local search efficiency. Applied to NRP and FSP, our approaches are benchmarked against the heuristic NSGA-II and the ILP-based $ε$-constraint method. Experimental results reveal that while our methods produce fewer non-dominated solutions than $ε$-constraint, they achieve significant reductions in execution time. Moreover, compared to NSGA-II, our methods deliver more non-dominated solutions with superior computational efficiency. These findings underscore the potential of QA in advancing scalable and efficient solutions for SBSE challenges.

</details>


### [12] [Mobile Application Review Summarization using Chain of Density Prompting](https://arxiv.org/abs/2506.14192)
*Shristi Shrestha,Anas Mahmoud*

Main category: cs.SE

TL;DR: 利用大型语言模型（LLM）和Chain of Density（CoD）提示，生成移动应用评论的摘要，提高用户选择应用的效率。


<details>
  <summary>Details</summary>
Motivation: 移动应用商店的评论数量庞大，导致信息过载，用户难以做出明智选择。

Method: 使用OpenAI GPT-4和CoD提示，迭代提取关键实体并生成固定长度的摘要。

Result: 通过实验验证，CoD提示能有效提取评论主题并生成易读的摘要。

Conclusion: 该方法提升了移动应用用户体验，为总结重要用户反馈提供了有效机制。

Abstract: Mobile app users commonly rely on app store ratings and reviews to find apps that suit their needs. However, the sheer volume of reviews available on app stores can lead to information overload, thus impeding users' ability to make informed app selection decisions. To address this challenge, we leverage Large Language Models (LLMs) to summarize mobile app reviews. In particular, we use the Chain of Density (CoD) prompt to guide OpenAI GPT-4 to generate abstractive, semantically dense, and easily interpretable summaries of mobile app reviews. The CoD prompt is engineered to iteratively extract salient entities from the source text and fuse them into a fixed-length summary. We evaluate the performance of our approach using a large dataset of mobile app reviews. We further conduct an empirical evaluation with 48 study participants to assess the readability of the generated summaries. Our results demonstrate that adapting the CoD prompt to focus on app features improves its ability to extract key themes from user reviews and generate natural language summaries tailored for end-user consumption. The prompt also manages to maintain the readability of the generated summaries while increasing their semantic density. Our work in this paper aims to improve mobile app users' experience by providing an effective mechanism for summarizing important user feedback in the review stream.

</details>


### [13] [The Tech DEI Backlash -- The Changing Landscape of Diversity, Equity, and Inclusion in Software Engineering](https://arxiv.org/abs/2506.14232)
*Sonja M. Hyrynsalmi,Mary Sanchez-Gordon,Anna Szlavi,Letizia Jaccheri*

Main category: cs.SE

TL;DR: 论文研究了领先软件公司近年来的DEI策略变化，发现公司因反对声浪而调整策略，分为减少、增加或重命名DEI举措三类，并提出了DEI Universe Map可视化工具。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏关于DEI反对声浪的学术研究，因此探讨软件公司如何调整DEI策略具有重要意义。

Method: 通过灰色文献研究，分析了10家领先软件公司的DEI举措现状。

Result: 公司将DEI策略分为减少、增加或重命名三类，部分公司仍坚持原有策略。

Conclusion: 软件公司确实因反对声浪调整DEI策略，DEI Universe Map为行业趋势提供了可视化工具。

Abstract: Not long ago, Diversity, Equity, and Inclusion (DEI) initiatives were a top priority for leading software companies. However, in a short period, a wave of backlash has led many firms to re-assess their DEI strategies. Responding to this DEI backlash is crucial in academic research, especially because, currently, little scholarly research has been done on it. In this paper, therefore, we have set forth the following research question (RQ): "How have leading software companies changed their DEI strategies in recent years?" Given the novelty of the RQ and, consequently, the lack of scholarly research on it, we are conducting a grey literature study, examining the current state of DEI initiatives in 10 leading software companies. Based on our analysis, we have classified companies into categories based on their shift in commitment to DEI. We can identify that companies are indeed responding to the backlash by rethinking their strategy, either by reducing, increasing, or renaming their DEI initiatives. In contrast, some companies keep on with their DEI strategy, at least so far, despite the challenging political climate. To illustrate these changes, we introduce the DEI Universe Map, a visual representation of software industry trends in DEI commitment and actions.

</details>


### [14] [Designing a Custom Chaos Engineering Framework for Enhanced System Resilience at Softtech](https://arxiv.org/abs/2506.14281)
*Ethem Utku Aktas,Burak Tuzlutas,Burak Yesiltas*

Main category: cs.SE

TL;DR: 本文提出了一种为金融行业软件公司Softtech定制的混沌工程框架设计，旨在通过引入故障提升系统韧性，同时考虑行业法规。


<details>
  <summary>Details</summary>
Motivation: 金融行业对系统稳定性和合规性要求高，需定制化混沌工程框架以提升软件韧性。

Method: 基于混沌工程基础概念，设计迭代、可扩展的框架，结合Softtech的基础设施和业务需求。

Result: 提出了一个适合Softtech的混沌工程框架，包含关键活动和组件。

Conclusion: 定制化混沌工程框架能有效提升金融行业软件公司的系统韧性。

Abstract: Chaos Engineering is a discipline which enhances software resilience by introducing faults to observe and improve system behavior intentionally. This paper presents a design proposal for a customized Chaos Engineering framework tailored for Softtech, a leading software development company serving the financial sector. It outlines foundational concepts and activities for introducing Chaos Engineering within Softtech, while considering financial sector regulations. Building on these principles, the framework aims to be iterative and scalable, enabling development teams to progressively improve their practices. The study addresses two primary questions: how Softtech's unique infrastructure, business priorities, and organizational context shape the customization of its Chaos Engineering framework and what key activities and components are necessary for creating an effective framework tailored to Softtech's needs.

</details>


### [15] [Anticipating Bugs: Ticket-Level Bug Prediction and Temporal Proximity Effects](https://arxiv.org/abs/2506.14290)
*Daniele La Prova,Emanuele Gentili,Davide Falessi*

Main category: cs.SE

TL;DR: 本文提出并评估了Ticket-Level Prediction (TLP)方法，用于预测哪些工单在实现后会引入缺陷。通过分析工单生命周期的三个阶段（Open、In Progress、Closed），发现TLP准确性随工单进展而提高，且不同特征在不同阶段具有不同预测能力。


<details>
  <summary>Details</summary>
Motivation: 当前缺陷预测方法仅支持修复而非预防，本文旨在通过TLP将缺陷预测提前到工单级别，以优化测试资源分配。

Method: TLP利用72个特征（分为6类），采用滑动窗口方法和三种机器学习分类器，对两个Apache开源项目的约10,000个工单进行分析。

Result: TLP准确性随工单进展而提高，不同特征家族在不同阶段表现各异：早期开发者特征最有效，后期代码和JIT特征主导，温度特征全程补充。

Conclusion: TLP将缺陷预测提前至工单级别，为风险感知的工单分类和开发者分配提供了新机会，扩展了现有缺陷预测的研究范围。

Abstract: The primary goal of bug prediction is to optimize testing efforts by focusing on software fragments, i.e., classes, methods, commits (JIT), or lines of code, most likely to be buggy. However, these predicted fragments already contain bugs. Thus, the current bug prediction approaches support fixing rather than prevention. The aim of this paper is to introduce and evaluate Ticket-Level Prediction (TLP), an approach to identify tickets that will introduce bugs once implemented. We analyze TLP at three temporal points, each point represents a ticket lifecycle stage: Open, In Progress, or Closed. We conjecture that: (1) TLP accuracy increases as tickets progress towards the closed stage due to improved feature reliability over time, and (2) the predictive power of features changes across these temporal points. Our TLP approach leverages 72 features belonging to six different families: code, developer, external temperature, internal temperature, intrinsic, ticket to tickets, and JIT. Our TLP evaluation uses a sliding-window approach, balancing feature selection and three machine-learning bug prediction classifiers on about 10,000 tickets of two Apache open-source projects. Our results show that TLP accuracy increases with proximity, confirming the expected trade-off between early prediction and accuracy. Regarding the prediction power of feature families, no single feature family dominates across stages; developer-centric signals are most informative early, whereas code and JIT metrics prevail near closure, and temperature-based features provide complementary value throughout. Our findings complement and extend the literature on bug prediction at the class, method, or commit level by showing that defect prediction can be effectively moved upstream, offering opportunities for risk-aware ticket triaging and developer assignment before any code is written.

</details>


### [16] [Quality Assessment of Python Tests Generated by Large Language Models](https://arxiv.org/abs/2506.14297)
*Victor Alves,Carla Bezerra,Ivan Machado,Larissa Rocha,Tássio Virgínio,Publio Silva*

Main category: cs.SE

TL;DR: 研究评估了GPT-4o、Amazon Q和LLama 3.3三种大语言模型生成的Python测试代码质量，发现大多数测试套件存在错误或测试异味，断言错误最常见。提示上下文显著影响质量，GPT-4o表现最佳。


<details>
  <summary>Details</summary>
Motivation: 手动生成测试脚本耗时且易错，自动化解决方案具有重要价值。大语言模型在测试代码生成中展现出潜力，但其质量需进一步评估。

Method: 研究比较了三种LLM在两种提示上下文（Text2Code和Code2Code）下生成的测试代码质量，分析错误和测试异味，并关联设计模式问题。

Result: 大多数测试套件存在错误或测试异味，断言错误占64%，测试异味中缺乏测试用例内聚力最常见（41%）。GPT-4o错误率最低，Amazon Q最高。提示上下文显著影响结果。

Conclusion: LLM生成的测试代码质量有待改进，未来研究应探索优化生成场景和提示工程策略。

Abstract: The manual generation of test scripts is a time-intensive, costly, and error-prone process, indicating the value of automated solutions. Large Language Models (LLMs) have shown great promise in this domain, leveraging their extensive knowledge to produce test code more efficiently. This study investigates the quality of Python test code generated by three LLMs: GPT-4o, Amazon Q, and LLama 3.3. We evaluate the structural reliability of test suites generated under two distinct prompt contexts: Text2Code (T2C) and Code2Code (C2C). Our analysis includes the identification of errors and test smells, with a focus on correlating these issues to inadequate design patterns. Our findings reveal that most test suites generated by the LLMs contained at least one error or test smell. Assertion errors were the most common, comprising 64% of all identified errors, while the test smell Lack of Cohesion of Test Cases was the most frequently detected (41%). Prompt context significantly influenced test quality; textual prompts with detailed instructions often yielded tests with fewer errors but a higher incidence of test smells. Among the evaluated LLMs, GPT-4o produced the fewest errors in both contexts (10% in C2C and 6% in T2C), whereas Amazon Q had the highest error rates (19% in C2C and 28% in T2C). For test smells, Amazon Q had fewer detections in the C2C context (9%), while LLama 3.3 performed best in the T2C context (10%). Additionally, we observed a strong relationship between specific errors, such as assertion or indentation issues, and test case cohesion smells. These findings demonstrate opportunities for improving the quality of test generation by LLMs and highlight the need for future research to explore optimized generation scenarios and better prompt engineering strategies.

</details>


### [17] [Agile and Student-Centred Teaching of Agile/Scrum Concepts](https://arxiv.org/abs/2506.14369)
*Maria Spichkova*

Main category: cs.SE

TL;DR: 论文讨论了设计和教授一门以敏捷/Scrum开发和需求工程为核心的软件工程项目管理课程的经验，重点介绍了2020年后的教学改革、真实评估的重要性以及大规模学生群体的评估挑战。


<details>
  <summary>Details</summary>
Motivation: 适应疫情后教学环境的变化，改进课程设计以更注重学生中心和灵活性，同时解决大规模学生群体的评估难题。

Method: 通过调整课程结构，采用真实评估方法，并探索支持混合学习的有效课程设计和在线评估结构。

Result: 总结了教学经验，为教授敏捷/Scrum概念提供了实用见解，并提出了适合大规模学生群体的真实且可扩展的评估方案。

Conclusion: 课程改革和评估方法的调整有效支持了混合学习模式，同时为未来类似课程的设计提供了参考。

Abstract: In this paper, we discuss our experience in designing and teaching a course on Software Engineering Project Management, where the focus is on Agile/Scrum development and Requirement Engineering activities. The course has undergone fundamental changes since 2020 to make the teaching approach more student-centred and flexible. As many universities abandoned having face-to-face exams at the end of the semester, authentic assessments now play an even more important role than before. This makes assessment of students' work even more challenging, especially if we are dealing with large cohorts of students. The complexity is not only in dealing with diversity in the student cohorts when elaborating the assessment tasks, but also in being able to provide feedback and marks in a timely and fairly. We report our lessons learned, which might provide useful insights for teaching Agile/Scrum concepts to undergraduate and postgraduate students. We also analyse what course structure might be effective to support a blended learning approach, as well as what could be a reasonable structure of online assessments, to keep them both authentic and scalable for large cohorts of students.

</details>


### [18] [Defining the Game Producer: A Mapping of Key Characteristics and Differentiators of the Professional Behind Digital Game Production](https://arxiv.org/abs/2506.14409)
*Rafael C. Lopes,Danilo M. Ribeiro*

Main category: cs.SE

TL;DR: 该研究通过定性分析确定了数字游戏制作人的核心特征、技能和能力，为职业培训和招聘提供了基础。


<details>
  <summary>Details</summary>
Motivation: 随着数字游戏复杂度的提升，游戏制作人在协调创意、技术和商业方面的作用日益重要，研究旨在明确其核心特征和能力。

Method: 采用定性研究方法，对11位专业人士进行半结构化访谈，并通过扎根理论分析数据。

Result: 研究总结出游戏制作人必备的个人特征、实践技能和战略能力，其中沟通、适应性和项目管理是关键要素。

Conclusion: 研究结果为游戏制作人的职业培训、招聘策略及未来领导力研究提供了理论基础。

Abstract: Introduction: As digital games grow in complexity, the role of the Game Producer becomes increasingly relevant for aligning creative, technical, and business dimensions. Objective: This study aimed to identify and map the main characteristics, skills, and competencies that define the Digital Game Producer profile. Methodology: A qualitative investigation was conducted with 11 semi-structured interviews, analyzed through Grounded Theory to build categories grounded in professional practice. Results: The study produced a structured set of personal characteristics, practical skills, and strategic competencies considered essential for Game Producers. Communication, adaptability, and project management emerged as central elements across the sample. Conclusion: The resulting model offers a foundation for professional training, recruitment strategies, and future research on leadership roles in game development.

</details>


### [19] [Automatic Qiskit Code Refactoring Using Large Language Models](https://arxiv.org/abs/2506.14535)
*José Manuel Suárez,Luis Mariano Bibbó,Joaquin Bogado,Alejandro Fernandez*

Main category: cs.SE

TL;DR: 本文提出了一种利用大型语言模型（LLMs）重构Qiskit代码的新方法，通过提取官方文档中的迁移场景分类法，并结合源代码输入LLM，实现了高效的量子代码迁移。


<details>
  <summary>Details</summary>
Motivation: 随着量子软件框架的快速发展，开发者面临API频繁变更带来的兼容性问题，需要一种自动化工具辅助代码迁移。

Method: 从Qiskit官方文档中提取迁移场景分类法，结合源代码输入LLM，设计结构化输入以克服LLM的上下文长度限制，实现代码迁移建议。

Result: 实验表明，结合领域知识的LLM能有效自动化Qiskit代码迁移，并提供了从旧版本迁移到0.46版的分类法和提示集。

Conclusion: 该方法不仅为量子代码迁移提供了实用工具，还验证了LLM在量子计算领域的应用潜力。

Abstract: As quantum software frameworks evolve, developers face increasing challenges in maintaining compatibility with rapidly changing APIs. In this work, we present a novel methodology for refactoring Qiskit code using large language models (LLMs). We begin by extracting a taxonomy of migration scenarios from the different sources of official Qiskit documentation (such as release notes), capturing common patterns such as migration of functionality to different modules and deprecated usage. This taxonomy, along with the original Python source code, is provided as input to an LLM, which is then tasked with identifying instances of migration scenarios in the code and suggesting appropriate refactoring solutions. Our approach is designed to address the context length limitations of current LLMs by structuring the input and reasoning process in a targeted, efficient manner. The results demonstrate that LLMs, when guided by domain-specific migration knowledge, can effectively assist in automating Qiskit code migration. This work contributes both a set of proven prompts and taxonomy for Qiskit code migration from earlier versions to version 0.46 and a methodology to asses the capabilities of LLMs to assist in the migration of quantum code.

</details>


### [20] [Low-code to fight climate change: the Climaborough project](https://arxiv.org/abs/2506.14623)
*Aaron Conrardy,Armen Sulejmani,Cindy Guerlain,Daniele Pagani,David Hick,Matteo Satta,Jordi Cabot*

Main category: cs.SE

TL;DR: Climaborough项目通过低代码/无代码策略快速开发气候仪表盘，帮助欧洲城市实现2030年碳中和目标。


<details>
  <summary>Details</summary>
Motivation: 支持欧洲城市快速实现碳中和目标，通过实时数据监控和用户友好的仪表盘评估气候行动效果。

Method: 采用低代码策略加速仪表盘开发，嵌入无代码哲学，使非技术用户也能自定义配置。

Result: 开发了Climaborough城市平台，聚合历史与实时数据，展示气候行动进展。

Conclusion: 低代码/无代码策略有效支持了气候仪表盘的快速部署和用户自定义需求。

Abstract: The EU-funded Climaborough project supports European cities to achieve carbon neutrality by 2030. Eleven cities in nine countries will deploy in real conditions products and services fostering climate transition in their local environment. The Climaborough City Platform is being developed to monitor the cities' overall progress towards their climate goals by aggregating historic and real-time data and displaying the results in user-friendly dashboards that will be used by non-technical experts to evaluate the effectiveness of local experimental initiatives, identify those that yield significant impact, and assess the potential consequences of scaling them up to a broader level. In this paper, we explain how we have put in place a low-code/no-code strategy in Climaborough in response to the project's aim to quickly deploy climate dashboards. A low-code strategy is used to accelerate the development of the dashboards. The dashboards embed a no-code philosophy that enables all types of citizen profiles to configure and adapt the dashboard to their specific needs.

</details>


### [21] [ACM Survey Draft on Formalising Software Requirements with Large Language Models](https://arxiv.org/abs/2506.14627)
*Arshad Beg,Diarmuid O'Donoghue,Rosemary Monahan*

Main category: cs.SE

TL;DR: 本文档汇总了94篇论文，并包含软件需求追踪（第4节）、形式化方法及其工具（第5节）、统一编程理论（UTP）和机构理论（第6节）等内容。与类似标题的近期文献[7,8]相比，[7]是两页的会议海报，[8]是经过严格评审的九页论文。


<details>
  <summary>Details</summary>
Motivation: 总结多篇论文内容，并补充特定领域的深入分析，如软件需求追踪和形式化方法。

Method: 通过文献综述和分类整理，结合具体领域的理论工具（如UTP和机构理论）进行分析。

Result: 提供了94篇论文的综述，并详细探讨了软件需求追踪、形式化方法等主题。

Conclusion: 本文档为相关领域的研究提供了全面的参考，并突出了与近期文献[7,8]的区别。

Abstract: This draft is a working document, having a summary of nighty-four (94) papers with additional sections on Traceability of Software Requirements (Section 4), Formal Methods and Its Tools (Section 5), Unifying Theories of Programming (UTP) and Theory of Institutions (Section 6). Please refer to abstract of [7,8]. Key difference of this draft from our recently anticipated ones with similar titles, i.e. AACS 2025 [7] and SAIV 2025 [8] is:
  [7] is a two page submission to ADAPT Annual Conference, Ireland. Submitted on 18th of March, 2025, it went through the light-weight blind review and accepted for poster presentation. Conference was held on 15th of May, 2025.
  [8] is a nine page paper with additional nine pages of references and summary tables, submitted to Symposium on AI Verification (SAIV 2025) on 24th of April, 2025. It went through rigorous review process. The uploaded version on arXiv.org [8] is the improved one of the submission, after addressing the specific suggestions to improve the paper.

</details>


### [22] [Navigating the growing field of research on AI for software testing -- the taxonomy for AI-augmented software testing and an ontology-driven literature survey](https://arxiv.org/abs/2506.14640)
*Ina K. Schieferdecker*

Main category: cs.SE

TL;DR: 本文综述了AI在软件测试自动化中的增强作用，提出了新的分类法ai4st，并探讨了AI带来的新型测试形式。


<details>
  <summary>Details</summary>
Motivation: 随着AI在工程领域的突破，软件测试（包括手动和自动化测试）迎来了新的机遇，但测试自动化的设计、开发和维护仍面临巨大挑战。

Method: 通过回顾AI在软件测试自动化中的最新研究，从无自动化到全自动化，提出并应用新的分类法ai4st。

Result: 提出了ai4st分类法，用于分类近期研究并识别开放性问题。

Conclusion: AI为软件测试自动化提供了新的视角和可能性，但仍有许多开放性问题需要进一步研究。

Abstract: In industry, software testing is the primary method to verify and validate the functionality, performance, security, usability, and so on, of software-based systems. Test automation has gained increasing attention in industry over the last decade, following decades of intense research into test automation and model-based testing. However, designing, developing, maintaining and evolving test automation is a considerable effort. Meanwhile, AI's breakthroughs in many engineering fields are opening up new perspectives for software testing, for both manual and automated testing. This paper reviews recent research on AI augmentation in software test automation, from no automation to full automation. It also discusses new forms of testing made possible by AI. Based on this, the newly developed taxonomy, ai4st, is presented and used to classify recent research and identify open research questions.

</details>


### [23] [Issue Retrieval and Verification Enhanced Supplementary Code Comment Generation](https://arxiv.org/abs/2506.14649)
*Yanzhen Zou,Xianlin Zhao,Xinglu Pan,Bing Xie*

Main category: cs.SE

TL;DR: IsComment 是一种基于问题的 LLM 检索和验证方法，用于生成代码补充注释，减少幻觉并提高可靠性。


<details>
  <summary>Details</summary>
Motivation: 问题报告中包含丰富信息，可用于生成代码补充注释，但如何减少生成注释中的幻觉仍具挑战性。

Method: 通过代码-注释-问题分析识别五种补充信息类型，检索相关句子生成候选注释，并过滤无关或不可验证的注释。

Result: IsComment 显著提高了补充注释的覆盖率，ChatGPT 从 33.6% 提升至 72.2%，GPT-4o 从 35.8% 提升至 88.4%，DeepSeek-V3 从 35.0% 提升至 86.2%。

Conclusion: IsComment 能生成更丰富、有用的代码补充注释，通过 MESIA 指标验证其优于现有方法。

Abstract: Issue reports have been recognized to contain rich information for retrieval-augmented code comment generation. However, how to minimize hallucinations in the generated comments remains significant challenges. In this paper, we propose IsComment, an issue-based LLM retrieval and verification approach for generating method's design rationale, usage directives, and so on as supplementary code comments. We first identify five main types of code supplementary information that issue reports can provide through code-comment-issue analysis. Next, we retrieve issue sentences containing these types of supplementary information and generate candidate code comments. To reduce hallucinations, we filter out those candidate comments that are irrelevant to the code or unverifiable by the issue report, making the code comment generation results more reliable. Our experiments indicate that compared with LLMs, IsComment increases the coverage of manual supplementary comments from 33.6% to 72.2% for ChatGPT, from 35.8% to 88.4% for GPT-4o, and from 35.0% to 86.2% for DeepSeek-V3. Compared with existing work, IsComment can generate richer and more useful supplementary code comments for programming understanding, which is quantitatively evaluated through the MESIA metric on both methods with and without manual code comments.

</details>


### [24] [Unified Software Engineering agent as AI Software Engineer](https://arxiv.org/abs/2506.14683)
*Leonhard Applis,Yuntong Zhang,Shanchao Liang,Nan Jiang,Lin Tan,Abhik Roychoudhury*

Main category: cs.SE

TL;DR: 论文探讨了LLM代理是否能成为AI软件工程师，提出了统一软件工程代理USEagent，并开发了评估基准USEbench。USEagent在多项任务中表现优于现有通用代理，但仍存在改进空间。


<details>
  <summary>Details</summary>
Motivation: 研究LLM代理是否能胜任AI软件工程师的角色，解决软件工程中多任务协调的复杂性问题。

Method: 开发了统一软件工程代理USEagent，能够协调处理多种软件工程任务，并构建了评估基准USEbench。

Result: USEagent在1,271个任务中表现优于现有通用代理，但在某些编码任务上仍有不足。

Conclusion: USEagent是未来AI软件工程师的雏形，但仍需进一步改进以填补能力差距。

Abstract: The growth of Large Language Model (LLM) technology has raised expectations for automated coding. However, software engineering is more than coding and is concerned with activities including maintenance and evolution of a project. In this context, the concept of LLM agents has gained traction, which utilize LLMs as reasoning engines to invoke external tools autonomously. But is an LLM agent the same as an AI software engineer? In this paper, we seek to understand this question by developing a Unified Software Engineering agent or USEagent. Unlike existing work which builds specialized agents for specific software tasks such as testing, debugging, and repair, our goal is to build a unified agent which can orchestrate and handle multiple capabilities. This gives the agent the promise of handling complex scenarios in software development such as fixing an incomplete patch, adding new features, or taking over code written by others. We envision USEagent as the first draft of a future AI Software Engineer which can be a team member in future software development teams involving both AI and humans. To evaluate the efficacy of USEagent, we build a Unified Software Engineering bench (USEbench) comprising of myriad tasks such as coding, testing, and patching. USEbench is a judicious mixture of tasks from existing benchmarks such as SWE-bench, SWT-bench, and REPOCOD. In an evaluation on USEbench consisting of 1,271 repository-level software engineering tasks, USEagent shows improved efficacy compared to existing general agents such as OpenHands CodeActAgent. There exist gaps in the capabilities of USEagent for certain coding tasks, which provides hints on further developing the AI Software Engineer of the future.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [25] [A Dual-Layer Image Encryption Framework Using Chaotic AES with Dynamic S-Boxes and Steganographic QR Codes](https://arxiv.org/abs/2506.13895)
*Md Rishadul Bayesh,Dabbrata Das,Md Ahadullah*

Main category: cs.CR

TL;DR: 本文提出了一种结合增强AES-128算法、混沌理论和高级隐写技术的双层次安全图像加密与密钥分发框架，通过动态ShiftRows操作、可变S-box和双密钥分发实现高效保护。


<details>
  <summary>Details</summary>
Motivation: 解决敏感图像传输中的加密强度和密钥安全分发问题，适用于监控、医学影像等领域。

Method: 采用动态ShiftRows操作、基于混沌理论的S-box生成、反馈链式加密和双密钥隐写分发技术。

Result: 实验显示框架性能优于现有方法，熵值接近理想（7.997），抗噪声和抗数据丢失能力强。

Conclusion: 该框架为敏感图像传输提供了可扩展的安全解决方案，兼具加密强度和密钥分发安全性。

Abstract: This paper presents a robust image encryption and key distribution framework that integrates an enhanced AES-128 algorithm with chaos theory and advanced steganographic techniques for dual-layer security. The encryption engine features a dynamic ShiftRows operation controlled by a logistic map, variable S-boxes generated from a two-dimensional Henon map for substitution and key expansion, and feedback chaining with post-encryption XOR diffusion to improve confusion, diffusion, and key sensitivity. To address secure key delivery, the scheme introduces dual-key distribution via steganographically modified QR codes. A static key and an AES-encrypted dynamic session key are embedded with a covert hint message using least significant bit (LSB) steganography. This design ensures the dynamic key can only be decrypted after reconstructing the static key from the hidden message, offering multi-factor protection against interception. Experimental results demonstrate the framework outperforms existing chaos-based and hybrid AES methods, achieving near-ideal entropy (7.997), minimal pixel correlation, and strong differential resistance with NPCR (>99.6%) and UACI (50.1%). Encrypted images show uniform histograms and robustness against noise and data loss. The framework offers a scalable, secure solution for sensitive image transmission in applications such as surveillance, medical imaging, and digital forensics, bridging the gap between cryptographic strength and safe key distribution.

</details>


### [26] [SoK: Advances and Open Problems in Web Tracking](https://arxiv.org/abs/2506.14057)
*Yash Vekaria,Yohan Beugin,Shaoor Munir,Gunes Acar,Nataliia Bielova,Steven Englehardt,Umar Iqbal,Alexandros Kapravelos,Pierre Laperdrix,Nick Nikiforakis,Jason Polakis,Franziska Roesner,Zubair Shafiq,Sebastian Zimmeck*

Main category: cs.CR

TL;DR: 本文是一篇关于网络跟踪技术的系统化知识综述，旨在整合分散的研究，提供技术机制、对策和法规的全面概述，并指出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 网络跟踪技术日益复杂且分散，研究领域缺乏统一视角，亟需整合现有研究以识别趋势和填补空白。

Method: 采用系统化知识（SoK）方法，综合现有文献，分析技术机制、对策和法规。

Result: 提供了网络跟踪领域的全面综述，包括技术发展、应对措施和法规影响，并指出开放性问题。

Conclusion: 本文为研究者、从业者和政策制定者提供了统一参考，并明确了未来研究方向。

Abstract: Web tracking is a pervasive and opaque practice that enables personalized advertising, retargeting, and conversion tracking. Over time, it has evolved into a sophisticated and invasive ecosystem, employing increasingly complex techniques to monitor and profile users across the web. The research community has a long track record of analyzing new web tracking techniques, designing and evaluating the effectiveness of countermeasures, and assessing compliance with privacy regulations. Despite a substantial body of work on web tracking, the literature remains fragmented across distinctly scoped studies, making it difficult to identify overarching trends, connect new but related techniques, and identify research gaps in the field. Today, web tracking is undergoing a once-in-a-generation transformation, driven by fundamental shifts in the advertising industry, the adoption of anti-tracking countermeasures by browsers, and the growing enforcement of emerging privacy regulations. This Systematization of Knowledge (SoK) aims to consolidate and synthesize this wide-ranging research, offering a comprehensive overview of the technical mechanisms, countermeasures, and regulations that shape the modern and rapidly evolving web tracking landscape. This SoK also highlights open challenges and outlines directions for future research, aiming to serve as a unified reference and introductory material for researchers, practitioners, and policymakers alike.

</details>


### [27] [From Permissioned to Proof-of-Stake Consensus](https://arxiv.org/abs/2506.14124)
*Jovan Komatovic,Andrew Lewis-Pye,Joachim Neu,Tim Roughgarden,Ertem Nusret Tas*

Main category: cs.CR

TL;DR: 本文提出了一种通用编译器，可将任何许可共识协议转换为权益证明（PoS）的无许可共识协议，并保留原有协议的部分同步设置下的特性。


<details>
  <summary>Details</summary>
Motivation: 解决许可共识协议向无许可共识协议的转换问题，同时保持原有协议的特性（如一致性、活性等）并增加问责性。

Method: 设计一种通用编译器，将许可协议转换为PoS无许可协议，确保在部分同步和准无许可设置下保留原有协议的特性。

Result: 转换后的协议保留了原有协议的一致性、活性、乐观响应性等特性，并增加了问责性。

Conclusion: 该编译器为许可协议向无许可协议的转换提供了一种通用且高效的解决方案。

Abstract: This paper presents the first generic compiler that transforms any permissioned consensus protocol into a proof-of-stake permissionless consensus protocol. For each of the following properties, if the initial permissioned protocol satisfies that property in the partially synchronous setting, the consequent proof-of-stake protocol also satisfies that property in the partially synchronous and quasi-permissionless setting (with the same fault-tolerance): consistency; liveness; optimistic responsiveness; every composable log-specific property; and message complexity of a given order. Moreover, our transformation ensures that the output protocol satisfies accountability (identifying culprits in the event of a consistency violation), whether or not the original permissioned protocol satisfied it.

</details>


### [28] [Vulnerability Disclosure or Notification? Best Practices for Reaching Stakeholders at Scale](https://arxiv.org/abs/2506.14323)
*Ting-Han Chen,Jeroen van der Ham-de Vos*

Main category: cs.CR

TL;DR: 论文分析了漏洞披露与漏洞通知的区别，总结了近年策略变化及最佳实践。


<details>
  <summary>Details</summary>
Motivation: 研究漏洞披露与通知的差异及其对利益相关者的影响，以改进现有实践。

Method: 基于早期披露经验和前人工作，进行元综述，分析策略变化和结果。

Result: 总结了现有披露指南和通知操作的最佳实践。

Conclusion: 漏洞披露与通知需区分对待，各自面临不同挑战，需采用不同策略。

Abstract: Security researchers are interested in security vulnerabilities, but these security vulnerabilities create risks for stakeholders. Coordinated Vulnerability Disclosure has been an accepted best practice for many years in disclosing newly discovered vulnerabilities. This practice has mostly worked, but it can become challenging when there are many different parties involved.
  There has also been research into known vulnerabilities, using datasets or active scans to discover how many machines are still vulnerable. The ethical guidelines suggest that researchers also make an effort to notify the owners of these machines. We posit that this differs from vulnerability disclosure, but rather the practice of vulnerability notification. This practice has some similarities with vulnerability disclosure but should be distinguished from it, providing other challenges and requiring a different approach.
  Based on our earlier disclosure experience and on prior work documenting their disclosure and notification operations, we provide a meta-review on vulnerability disclosure and notification to observe the shifts in strategies in recent years. We assess how researchers initiated their messaging and examine the outcomes. We then compile the best practices for the existing disclosure guidelines and for notification operations.

</details>


### [29] [LLM-Powered Intent-Based Categorization of Phishing Emails](https://arxiv.org/abs/2506.14337)
*Even Eilertsen,Vasileios Mavroeidis,Gudmund Grov*

Main category: cs.CR

TL;DR: 论文探讨了利用大型语言模型（LLM）检测钓鱼邮件的潜力，提出了一种基于意图的分类方法，并展示了LLM在此领域的有效性。


<details>
  <summary>Details</summary>
Motivation: 钓鱼攻击对网络安全构成重大威胁，传统检测系统依赖用户不可见的元数据，而LLM能通过分析邮件文本意图提供更直接的检测手段。

Method: 研究使用LLM对钓鱼邮件进行二元分类，并引入意图类型分类法，将邮件分到不同类别以生成可操作的威胁信息。

Result: 实验结果表明，现有LLM能够有效检测和分类钓鱼邮件。

Conclusion: LLM在钓鱼邮件检测领域具有实际应用潜力。

Abstract: Phishing attacks remain a significant threat to modern cybersecurity, as they successfully deceive both humans and the defense mechanisms intended to protect them. Traditional detection systems primarily focus on email metadata that users cannot see in their inboxes. Additionally, these systems struggle with phishing emails, which experienced users can often identify empirically by the text alone. This paper investigates the practical potential of Large Language Models (LLMs) to detect these emails by focusing on their intent. In addition to the binary classification of phishing emails, the paper introduces an intent-type taxonomy, which is operationalized by the LLMs to classify emails into distinct categories and, therefore, generate actionable threat information. To facilitate our work, we have curated publicly available datasets into a custom dataset containing a mix of legitimate and phishing emails. Our results demonstrate that existing LLMs are capable of detecting and categorizing phishing emails, underscoring their potential in this domain.

</details>


### [30] [Quantum Enhanced Entropy Pool for Cryptographic Applications and Proofs](https://arxiv.org/abs/2506.14340)
*Buniechukwu Njoku,Sonai Biswas,Milad Ghadimi,Mohammad Shojafar,Gabriele Gradoni,Riccardo Bassoli,Frank H. P. Fitzek*

Main category: cs.CR

TL;DR: 论文研究了将量子随机性集成到基于Ed25519椭圆曲线的可验证随机函数（VRF）中，以增强密码安全性。通过用量子熵源替代传统伪随机数生成器，评估了对密钥安全和性能指标的影响，包括执行时间和资源使用。结果表明，量子随机数生成器（QRNG）虽能提升VRF的不可预测性和可验证性，但也带来了时间和计算开销的挑战。


<details>
  <summary>Details</summary>
Motivation: 探索量子随机性在VRF中的应用，以提升密码系统的安全性和性能。

Method: 模拟修改后的VRF设置，其中初始化密钥来自量子随机数生成器（QRNG），并评估其对执行时间、资源使用等性能指标的影响。

Result: QRNG增强了VRF的不可预测性和可验证性，但导致密钥生成时间（50至400+微秒）和验证时间（500至3500微秒）增加，CPU使用率（17%至30%）上升，相比传统Go-based VRF性能更不稳定。

Conclusion: 量子随机性在VRF中的应用存在安全性与效率的权衡，为未来更安全高效的协议设计提供了潜在路径。

Abstract: This paper investigates the integration of quantum randomness into Verifiable Random Functions (VRFs) using the Ed25519 elliptic curve to strengthen cryptographic security. By replacing traditional pseudorandom number generators with quantum entropy sources, we assess the impact on key security and performance metrics, including execution time, and resource usage. Our approach simulates a modified VRF setup where initialization keys are derived from a quantum random number generator source (QRNG). The results show that while QRNGs could enhance the unpredictability and verifiability of VRFs, their incorporation introduces challenges related to temporal and computational overhead. This study provides valuable insights into the trade-offs of leveraging quantum randomness in API-driven cryptographic systems and offers a potential path toward more secure and efficient protocol design. The QRNG-based system shows increased (key generation times from 50 to 400+ microseconds, verification times from 500 to 3500 microseconds) and higher CPU usage (17% to 30%) compared to the more consistent performance of a Go-based VRF (key generation times below 200 microseconds, verification times under 2000 microseconds, CPU usage below 10%), highlighting trade-offs in computational efficiency and resource demands.

</details>


### [31] [Excessive Reasoning Attack on Reasoning LLMs](https://arxiv.org/abs/2506.14374)
*Wai Man Si,Mingjie Li,Michael Backes,Yang Zhang*

Main category: cs.CR

TL;DR: 论文提出了一种新的威胁：对抗性输入可以利用大型语言模型的过度推理行为增加计算开销，同时保持模型性能。作者提出了一种包含三种损失组件的框架，实验结果显示推理长度显著增加且具有可迁移性。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在复杂任务中表现优异，但存在过度推理导致计算成本高的问题。对抗性输入可能进一步加剧这一问题。

Method: 提出了一种包含三种损失组件的框架：Priority Cross-Entropy Loss、Excessive Reasoning Loss和Delayed Termination Loss，用于优化对抗性攻击。

Result: 在GSM8K和ORCA数据集上，推理长度增加了3倍至9倍，同时保持了模型性能。对抗性输入还表现出对其他模型的迁移性。

Conclusion: 该研究揭示了对抗性输入对大型语言模型计算开销的潜在威胁，并提出了有效的攻击框架，为未来防御研究提供了方向。

Abstract: Recent reasoning large language models (LLMs), such as OpenAI o1 and DeepSeek-R1, exhibit strong performance on complex tasks through test-time inference scaling. However, prior studies have shown that these models often incur significant computational costs due to excessive reasoning, such as frequent switching between reasoning trajectories (e.g., underthinking) or redundant reasoning on simple questions (e.g., overthinking). In this work, we expose a novel threat: adversarial inputs can be crafted to exploit excessive reasoning behaviors and substantially increase computational overhead without compromising model utility. Therefore, we propose a novel loss framework consisting of three components: (1) Priority Cross-Entropy Loss, a modification of the standard cross-entropy objective that emphasizes key tokens by leveraging the autoregressive nature of LMs; (2) Excessive Reasoning Loss, which encourages the model to initiate additional reasoning paths during inference; and (3) Delayed Termination Loss, which is designed to extend the reasoning process and defer the generation of final outputs. We optimize and evaluate our attack for the GSM8K and ORCA datasets on DeepSeek-R1-Distill-LLaMA and DeepSeek-R1-Distill-Qwen. Empirical results demonstrate a 3x to 9x increase in reasoning length with comparable utility performance. Furthermore, our crafted adversarial inputs exhibit transferability, inducing computational overhead in o3-mini, o1-mini, DeepSeek-R1, and QWQ models.

</details>


### [32] [Consensus Power Inequality: A Comparative Study of Blockchain Networks](https://arxiv.org/abs/2506.14393)
*Kamil Tylinski,Abylay Satybaldy,Paolo Tasca*

Main category: cs.CR

TL;DR: 研究评估了五种区块链网络的共识权力不平等，发现Hedera和比特币分布较均衡，以太坊和Cardano中等，而Algorand集中度高。


<details>
  <summary>Details</summary>
Motivation: 探讨共识权力分布对区块链去中心化、安全性和公平性的影响。

Method: 使用Gini系数和Theil指数等经济指标，分析2022年1月至2024年7月的数据。

Result: Hedera和比特币分布均衡，以太坊转向PoS后更集中，Algorand权力集中度高。

Conclusion: 提出了评估共识权力不平等的方法框架，强调需针对性策略以实现更公平的分布。

Abstract: The distribution of consensus power is a cornerstone of decentralisation, influencing the security, resilience, and fairness of blockchain networks while ensuring equitable impact among participants. This study provides a rigorous evaluation of consensus power inequality across five prominent blockchain networks - Bitcoin, Ethereum, Cardano, Hedera, and Algorand - using data collected from January 2022 to July 2024. Leveraging established economic metrics, including the Gini coefficient and Theil index, the research quantitatively assesses how power is distributed among blockchain network participants. A robust dataset, capturing network-specific characteristics such as mining pools, staking patterns, and consensus nodes, forms the foundation of the analysis, enabling meaningful comparisons across diverse architectures. Through an in-depth comparative study, the paper identifies key disparities in consensus power distribution. Hedera and Bitcoin demonstrate more balanced power distribution, aligning closely with the principles of decentralisation. Ethereum and Cardano demonstrate moderate levels of inequality. However, contrary to expectations, Ethereum has become more concentrated following its transition to Proof-of-Stake. Meanwhile, Algorand shows a pronounced centralisation of power. Moreover, the findings highlight the structural and operational drivers of inequality, including economic barriers, governance models, and network effects, offering actionable insights for more equitable network design. This study establishes a methodological framework for evaluating blockchain consensus power inequality, emphasising the importance of targeted strategies to ensure fairer power distribution and enhancing the sustainability of decentralised systems. Future research will build on these findings by integrating additional metrics and examining the influence of emerging consensus mechanisms.

</details>


### [33] [MalGuard: Towards Real-Time, Accurate, and Actionable Detection of Malicious Packages in PyPI Ecosystem](https://arxiv.org/abs/2506.14466)
*Xingan Gao,Xiaobing Sun,Sicong Cao,Kaifeng Huang,Di Wu,Xiaolei Liu,Xingwei Lin,Yang Xiang*

Main category: cs.CR

TL;DR: 论文提出了一种名为MalGuard的新方法，结合图中心性分析和LIME算法，用于检测PyPI中的恶意包，显著提升了检测精度和召回率。


<details>
  <summary>Details</summary>
Motivation: 现有恶意包检测方法随着模型复杂度增加导致时间消耗上升，且传统机器学习模型缺乏解释性，需要大量人工分析特征。

Method: 利用图中心性分析自动提取敏感API，结合LLM优化特征集，并集成LIME算法提供解释性。

Result: MalGuard在实验中提升了0.5%-33.2%的精度和1.8%-22.1%的召回率，成功识别了113个未知恶意包。

Conclusion: MalGuard通过自动化和解释性优化，显著提升了恶意包检测的效率和效果。

Abstract: Malicious package detection has become a critical task in ensuring the security and stability of the PyPI. Existing detection approaches have focused on advancing model selection, evolving from traditional machine learning (ML) models to large language models (LLMs). However, as the complexity of the model increases, the time consumption also increases, which raises the question of whether a lightweight model achieves effective detection. Through empirical research, we demonstrate that collecting a sufficiently comprehensive feature set enables even traditional ML models to achieve outstanding performance. However, with the continuous emergence of new malicious packages, considerable human and material resources are required for feature analysis. Also, traditional ML model-based approaches lack of explainability to malicious packages.Therefore, we propose a novel approach MalGuard based on graph centrality analysis and the LIME (Local Interpretable Model-agnostic Explanations) algorithm to detect malicious packages.To overcome the above two challenges, we leverage graph centrality analysis to extract sensitive APIs automatically to replace manual analysis. To understand the sensitive APIs, we further refine the feature set using LLM and integrate the LIME algorithm with ML models to provide explanations for malicious packages. We evaluated MalGuard against six SOTA baselines with the same settings. Experimental results show that our proposed MalGuard, improves precision by 0.5%-33.2% and recall by 1.8%-22.1%. With MalGuard, we successfully identified 113 previously unknown malicious packages from a pool of 64,348 newly-uploaded packages over a five-week period, and 109 out of them have been removed by the PyPI official.

</details>


### [34] [ReDASH: Fast and efficient Scaling in Arithmetic Garbled Circuits for Secure Outsourced Inference](https://arxiv.org/abs/2506.14489)
*Felix Maurer,Jonas Sander,Thomas Eisenbarth*

Main category: cs.CR

TL;DR: ReDash通过改进Dash的算术混淆电路，引入新的混淆缩放机制，支持任意缩放因子，显著提升安全外包推理的效率和灵活性。


<details>
  <summary>Details</summary>
Motivation: 解决Dash仅支持2的幂次方缩放的限制，提供更灵活的量化方案和更高效的模型评估。

Method: 提出基于残数系统的广义基扩展的混淆缩放机制，并引入ScaleQuant⁺量化方法。

Result: ReDash比Dash快33倍，同时保持算术混淆的安全性。

Conclusion: ReDash通过性能提升和量化灵活性，扩展了混淆卷积神经网络推理的实用性。

Abstract: ReDash extends Dash's arithmetic garbled circuits to provide a more flexible and efficient framework for secure outsourced inference. By introducing a novel garbled scaling gadget based on a generalized base extension for the residue number system, ReDash removes Dash's limitation of scaling exclusively by powers of two. This enables arbitrary scaling factors drawn from the residue number system's modular base, allowing for tailored quantization schemes and more efficient model evaluation.
  Through the new $\text{ScaleQuant}^+$ quantization mechanism, ReDash supports optimized modular bases that can significantly reduce the overhead of arithmetic operations during convolutional neural network inference. ReDash achieves up to a 33-fold speedup in overall inference time compared to Dash Despite these enhancements, ReDash preserves the robust security guarantees of arithmetic garbling. By delivering both performance gains and quantization flexibility, ReDash expands the practicality of garbled convolutional neural network inference.

</details>


### [35] [Anonymous Authentication using Attribute-based Encryption](https://arxiv.org/abs/2506.14566)
*Nouha Oualha*

Main category: cs.CR

TL;DR: 本文提出了一种基于属性加密（ABE）的匿名认证机制，用户无需暴露身份即可完成认证，并通过OpenID Connect验证其可行性。


<details>
  <summary>Details</summary>
Motivation: 在数字时代，个人数据面临泄露风险，需要隐私保护的数据保护方法。

Method: 采用属性加密（ABE）技术，设计匿名认证机制，仅基于用户属性进行授权。

Result: 通过OpenID Connect实现，验证了该机制在现实系统中的可行性。

Conclusion: 提出的匿名认证机制为隐私保护提供了有效解决方案，适用于实际应用。

Abstract: In today's digital age, personal data is constantly at risk of compromise. Attribute-Based Encryption (ABE) has emerged as a promising approach to privacy-preserving data protection. This paper proposes an anonymous authentication mechanism based on ABE, which allows users to authenticate without revealing their identity. The mechanism adds a privacy-preserving layer by enabling authorization based solely on user attributes. The proposed approach is implemented using OpenID Connect, demonstrating its feasibility in real-world systems.

</details>


### [36] [SoK: Privacy-Enhancing Technologies in Artificial Intelligence](https://arxiv.org/abs/2506.14576)
*Nouha Oualha*

Main category: cs.CR

TL;DR: 本文探讨了AI背景下数据隐私的现状，回顾了隐私增强技术（PETs）在AI系统中的整合，并评估了其成就与挑战。


<details>
  <summary>Details</summary>
Motivation: 随着AI在各领域的广泛应用，保护个人和敏感数据的需求日益增长，隐私增强技术（PETs）应运而生。

Method: 通过分析当前数据隐私的现状，并研究PETs在AI系统中的整合方式。

Result: 总结了PETs在隐私保护方面的成就，同时指出了仍需解决的挑战。

Conclusion: PETs在AI系统中具有重要潜力，但需进一步解决技术挑战以实现更广泛的应用。

Abstract: As artificial intelligence (AI) continues to permeate various sectors, safeguarding personal and sensitive data has become increasingly crucial. To address these concerns, privacy-enhancing technologies (PETs) have emerged as a suite of digital tools that enable data collection and processing while preserving privacy. This paper explores the current landscape of data privacy in the context of AI, reviews the integration of PETs within AI systems, and assesses both their achievements and the challenges that remain.

</details>


### [37] [Busting the Paper Ballot: Voting Meets Adversarial Machine Learning](https://arxiv.org/abs/2506.14582)
*Kaleel Mahmood,Caleb Manicke,Ethan Rathbun,Aayushi Verma,Sohaib Ahmad,Nicholas Stamatakis,Laurent Michel,Benjamin Fuller*

Main category: cs.CR

TL;DR: 论文探讨了在美国选举计票器中使用机器学习分类器的安全风险，通过新数据集和多种模型测试，揭示了传统白盒攻击的局限性，并提出改进方法。


<details>
  <summary>Details</summary>
Motivation: 研究动机是揭示机器学习分类器在选举计票中的潜在安全漏洞，尤其是对抗性攻击对选举结果的影响。

Method: 方法包括引入四个新数据集、训练多种模型（如SVM、CNN、ViT）、分析梯度掩蔽问题，并提出改进的损失函数。

Result: 结果表明，传统白盒攻击因梯度掩蔽无效，但改进方法在物理世界中可实现5%攻击成功率，足以影响选举结果。

Conclusion: 结论是选举计票中的机器学习模型存在安全风险，需关注对抗性攻击的潜在影响及实际可行性。

Abstract: We show the security risk associated with using machine learning classifiers in United States election tabulators. The central classification task in election tabulation is deciding whether a mark does or does not appear on a bubble associated to an alternative in a contest on the ballot. Barretto et al. (E-Vote-ID 2021) reported that convolutional neural networks are a viable option in this field, as they outperform simple feature-based classifiers.
  Our contributions to election security can be divided into four parts. To demonstrate and analyze the hypothetical vulnerability of machine learning models on election tabulators, we first introduce four new ballot datasets. Second, we train and test a variety of different models on our new datasets. These models include support vector machines, convolutional neural networks (a basic CNN, VGG and ResNet), and vision transformers (Twins and CaiT). Third, using our new datasets and trained models, we demonstrate that traditional white box attacks are ineffective in the voting domain due to gradient masking. Our analyses further reveal that gradient masking is a product of numerical instability. We use a modified difference of logits ratio loss to overcome this issue (Croce and Hein, ICML 2020). Fourth, in the physical world, we conduct attacks with the adversarial examples generated using our new methods. In traditional adversarial machine learning, a high (50% or greater) attack success rate is ideal. However, for certain elections, even a 5% attack success rate can flip the outcome of a race. We show such an impact is possible in the physical domain. We thoroughly discuss attack realism, and the challenges and practicality associated with printing and scanning ballot adversarial examples.

</details>


### [38] [AIRTBench: Measuring Autonomous AI Red Teaming Capabilities in Language Models](https://arxiv.org/abs/2506.14682)
*Ads Dawson,Rob Mulla,Nick Landers,Shane Caldwell*

Main category: cs.CR

TL;DR: AIRTBench是一个AI红队基准测试，用于评估语言模型自主发现和利用AI/ML安全漏洞的能力。Claude-3.7-Sonnet表现最佳，解决了61%的挑战，而开源模型表现较差。


<details>
  <summary>Details</summary>
Motivation: 填补评估自主AI红队能力的空白，提供首个专门设计的全面基准测试。

Method: 基于70个现实黑盒CTF挑战，要求模型编写Python代码攻击AI系统。

Result: 前沿模型在提示注入攻击中表现优异（平均49%成功率），但在系统利用和模型反转中表现较差（低于26%）。

Conclusion: AIRTBench为评估和追踪自主AI红队能力提供了关键工具，显示前沿模型在效率上远超人类和开源模型。

Abstract: We introduce AIRTBench, an AI red teaming benchmark for evaluating language models' ability to autonomously discover and exploit Artificial Intelligence and Machine Learning (AI/ML) security vulnerabilities. The benchmark consists of 70 realistic black-box capture-the-flag (CTF) challenges from the Crucible challenge environment on the Dreadnode platform, requiring models to write python code to interact with and compromise AI systems. Claude-3.7-Sonnet emerged as the clear leader, solving 43 challenges (61% of the total suite, 46.9% overall success rate), with Gemini-2.5-Pro following at 39 challenges (56%, 34.3% overall), GPT-4.5-Preview at 34 challenges (49%, 36.9% overall), and DeepSeek R1 at 29 challenges (41%, 26.9% overall). Our evaluations show frontier models excel at prompt injection attacks (averaging 49% success rates) but struggle with system exploitation and model inversion challenges (below 26%, even for the best performers). Frontier models are far outpacing open-source alternatives, with the best truly open-source model (Llama-4-17B) solving 7 challenges (10%, 1.0% overall), though demonstrating specialized capabilities on certain hard challenges. Compared to human security researchers, large language models (LLMs) solve challenges with remarkable efficiency completing in minutes what typically takes humans hours or days-with efficiency advantages of over 5,000x on hard challenges. Our contribution fills a critical gap in the evaluation landscape, providing the first comprehensive benchmark specifically designed to measure and track progress in autonomous AI red teaming capabilities.

</details>


### [39] [AGENTSAFE: Benchmarking the Safety of Embodied Agents on Hazardous Instructions](https://arxiv.org/abs/2506.14697)
*Aishan Liu,Zonghao Ying,Le Wang,Junjie Mu,Jinyang Guo,Jiakai Wang,Yuqing Ma,Siyuan Liang,Mingchuan Zhang,Xianglong Liu,Dacheng Tao*

Main category: cs.CR

TL;DR: AGENTSAFE是一个用于评估视觉语言模型（VLM）代理在危险指令下安全性的综合基准，通过模拟环境和新型适配模块实现安全测试。


<details>
  <summary>Details</summary>
Motivation: 随着视觉语言模型在现实环境中的部署增加，其安全性问题日益突出，尤其是在处理危险指令时。

Method: 提出AGENTSAFE基准，包括模拟环境、适配模块和风险感知指令数据集，涵盖45种对抗场景和8,100条危险指令。

Result: AGENTSAFE能够系统性地测试代理在感知、规划和执行阶段的对抗条件下表现。

Conclusion: AGENTSAFE为评估VLM代理的安全性提供了首个全面基准，有助于提升其在现实环境中的安全部署。

Abstract: The rapid advancement of vision-language models (VLMs) and their integration into embodied agents have unlocked powerful capabilities for decision-making. However, as these systems are increasingly deployed in real-world environments, they face mounting safety concerns, particularly when responding to hazardous instructions. In this work, we propose AGENTSAFE, the first comprehensive benchmark for evaluating the safety of embodied VLM agents under hazardous instructions. AGENTSAFE simulates realistic agent-environment interactions within a simulation sandbox and incorporates a novel adapter module that bridges the gap between high-level VLM outputs and low-level embodied controls. Specifically, it maps recognized visual entities to manipulable objects and translates abstract planning into executable atomic actions in the environment. Building on this, we construct a risk-aware instruction dataset inspired by Asimovs Three Laws of Robotics, including base risky instructions and mutated jailbroken instructions. The benchmark includes 45 adversarial scenarios, 1,350 hazardous tasks, and 8,100 hazardous instructions, enabling systematic testing under adversarial conditions ranging from perception, planning, and action execution stages.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [40] [AgentFacts: Universal KYA Standard for Verified AI Agent Metadata & Deployment](https://arxiv.org/abs/2506.13794)
*Jared James Grogan*

Main category: cs.MA

TL;DR: AgentFacts是一种通用元数据标准，通过加密签名声明、多权威验证和动态权限管理，解决企业AI部署中的信任问题。


<details>
  <summary>Details</summary>
Motivation: 企业AI部署面临第三方代理能力验证和信任建立的挑战，缺乏标准化元数据或验证基础设施。

Method: 提出AgentFacts标准，支持加密签名能力声明、多权威验证和动态权限管理，实现系统化代理验证。

Result: AgentFacts将代理采购从定制集成项目转变为标准化管理，提供透明度和治理基础设施。

Conclusion: AgentFacts为企业AI规模化协调提供了必要的信任和治理支持。

Abstract: Enterprise AI deployment faces critical "Know Your Agent" (KYA) challenges where organizations must verify third-party agent capabilities and establish trust without standardized metadata or verification infrastructure. Current approaches rely on self-declared capabilities and custom integration processes that create trust gaps and coordination friction limiting confident enterprise adoption. This paper presents AgentFacts, a universal metadata standard that enables systematic agent verification through cryptographically-signed capability declarations, multi-authority validation, and dynamic permission management. The specification introduces domain-specialized verification where different trusted authorities validate specific metadata aspects based on their expertise, eliminating single points of trust failure while enabling graduated confidence assessment. AgentFacts transforms agent procurement from custom integration projects into standardized workforce management, providing the transparency and governance infrastructure necessary for enterprise AI coordination at scale.

</details>


### [41] [Investigating the Potential of Large Language Model-Based Router Multi-Agent Architectures for Foundation Design Automation: A Task Classification and Expert Selection Study](https://arxiv.org/abs/2506.13811)
*Sompote Youwai,David Phim,Vianne Gayl Murcia,Rianne Clair Onas*

Main category: cs.MA

TL;DR: 研究探讨了基于路由器的多智能体系统在基础设计计算中的自动化应用，通过智能任务分类和专家选择，性能显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 提高基础设计计算的自动化水平，同时满足工程安全性和专业文档标准。

Method: 评估了单智能体处理、多智能体设计-检查架构和基于路由器的专家选择三种方法，使用多种基线模型进行性能测试。

Result: 基于路由器的配置在浅基础和桩基础设计中分别达到95.00%和90.63%的性能，显著优于传统工作流程。

Conclusion: 基于路由器的多智能体系统是基础设计自动化的最优选择，但仍需人类监督以确保安全。

Abstract: This study investigates router-based multi-agent systems for automating foundation design calculations through intelligent task classification and expert selection. Three approaches were evaluated: single-agent processing, multi-agent designer-checker architecture, and router-based expert selection. Performance assessment utilized baseline models including DeepSeek R1, ChatGPT 4 Turbo, Grok 3, and Gemini 2.5 Pro across shallow foundation and pile design scenarios. The router-based configuration achieved performance scores of 95.00% for shallow foundations and 90.63% for pile design, representing improvements of 8.75 and 3.13 percentage points over standalone Grok 3 performance respectively. The system outperformed conventional agentic workflows by 10.0 to 43.75 percentage points. Grok 3 demonstrated superior standalone performance without external computational tools, indicating advances in direct LLM mathematical reasoning for engineering applications. The dual-tier classification framework successfully distinguished foundation types, enabling appropriate analytical approaches. Results establish router-based multi-agent systems as optimal for foundation design automation while maintaining professional documentation standards. Given safety-critical requirements in civil engineering, continued human oversight remains essential, positioning these systems as advanced computational assistance tools rather than autonomous design replacements in professional practice.

</details>


### [42] [Hierarchical Multi-Agent Reinforcement Learning-based Coordinated Spatial Reuse for Next Generation WLANs](https://arxiv.org/abs/2506.14187)
*Jiaming Yu,Le Liang,Hao Ye,Shi Jin*

Main category: cs.MA

TL;DR: 论文提出了一种分层多智能体强化学习（HMARL）方法，用于解决Wi-Fi网络中下行链路空间复用的挑战，显著提升了网络吞吐量和延迟性能。


<details>
  <summary>Details</summary>
Motivation: 高密度Wi-Fi部署中同频干扰严重，影响网络性能，需要协调多AP实现空间复用。

Method: 将协调空间复用（CSR）分为轮询和决策两阶段，采用HMARL算法，通过分层结构分别处理站点选择和功率控制。

Result: 仿真表明，该方法在吞吐量和延迟上优于基线方法，且与传统AP共存时表现稳健。奖励函数设计还提高了高干扰区域AP的传输公平性。

Conclusion: HMARL算法有效解决了高密度Wi-Fi网络的干扰问题，提升了性能和公平性。

Abstract: High-density Wi-Fi deployments often result in significant co-channel interference, which degrades overall network performance. To address this issue, coordination of multi access points (APs) has been considered to enable coordinated spatial reuse (CSR) in next generation wireless local area networks. This paper tackles the challenge of downlink spatial reuse in Wi-Fi networks, specifically in scenarios involving overlapping basic service sets, by employing hierarchical multi-agent reinforcement learning (HMARL). We decompose the CSR process into two phases, i.e., a polling phase and a decision phase, and introduce the HMARL algorithm to enable efficient CSR. To enhance training efficiency, the proposed HMARL algorithm employs a hierarchical structure, where station selection and power control are determined by a high- and low-level policy network, respectively. Simulation results demonstrate that this approach consistently outperforms baseline methods in terms of throughput and latency across various network topologies. Moreover, the algorithm exhibits robust performance when coexisting with legacy APs. Additional experiments in a representative topology further reveal that the carefully designed reward function not only maximizes the overall network throughput, but also improves fairness in transmission opportunities for APs in high-interference regions.

</details>
