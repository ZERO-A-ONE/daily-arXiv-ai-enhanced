{"id": "2510.06420", "categories": ["cs.CR", "cs.PL"], "pdf": "https://arxiv.org/pdf/2510.06420", "abs": "https://arxiv.org/abs/2510.06420", "authors": ["Suresh K. Damodaran", "Paul D. Rowe"], "title": "Automated Repeatable Adversary Threat Emulation with Effects Language (EL)", "comment": null, "summary": "The emulation of multi-step attacks attributed to advanced persistent threats\nis valuable for training defenders and evaluating defense tools. In this paper,\nwe discuss the numerous challenges and desired attributes associated with such\nautomation. Additionally, we introduce the use of Effects Language (EL), a\nvisual programming language with graph-based operational semantics, as a\nsolution to address many of these challenges and requirements. We formally\ndefine the execution semantics of EL, and prove important execution properties.\nFurthermore, we showcase the application of EL to codify attacks using an\nexample from one of the publicly available attack scenarios. We also\ndemonstrate how EL can be utilized to provide proof-of-attack of complex\nmulti-step attacks. Our results highlight the improvements in time and resource\nefficiency achieved through the use of EL for repeatable automation."}
{"id": "2510.06421", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.06421", "abs": "https://arxiv.org/abs/2510.06421", "authors": ["Muhammad Abdullah Soomro", "Fatima Muhammad Anwar"], "title": "Breaking Precision Time: OS Vulnerability Exploits Against IEEE 1588", "comment": "Published in IEEE ISPCS 2025", "summary": "The Precision Time Protocol (PTP), standardized as IEEE 1588, provides\nsub-microsecond synchronization across distributed systems and underpins\ncritical infrastructure in telecommunications, finance, power systems, and\nindustrial automation. While prior work has extensively analyzed PTP's\nvulnerability to network-based attacks, prompting the development of\ncryptographic protections and anomaly detectors, these defenses presume an\nuncompromised host. In this paper, we identify and exploit a critical blind\nspot in current threat models: kernel-level adversaries operating from within\nthe host running the PTP stack. We present the first systematic study of\nkernel-rooted attacks on PTP, demonstrating how privileged attackers can\nmanipulate system time by corrupting key interfaces without altering PTP\nnetwork traffic. We implement three attack primitives, constant offset,\nprogressive skew, and random jitter, using in-kernel payloads, and evaluate\ntheir impact on the widely used ptp4l and phc2sys daemons. Our experiments\nreveal that these attacks can silently destabilize clock synchronization,\nbypassing existing PTP security extensions. These findings highlight the urgent\nneed to reconsider host-level trust assumptions and integrate kernel integrity\ninto the design of secure time synchronization systems."}
{"id": "2510.06432", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.06432", "abs": "https://arxiv.org/abs/2510.06432", "authors": ["Vipul Goyal", "Justin Raizes"], "title": "Proofs of No Intrusion", "comment": null, "summary": "A central challenge in data security is not just preventing theft, but\ndetecting whether it has occurred. Classically, this is impossible because a\nperfect copy leaves no evidence. Quantum mechanics, on the other hand, forbids\ngeneral duplication, opening up new possibilities.\n  We introduce Proofs of No Intrusion, which enable a classical client to\nremotely test whether a quantum server has been hacked and the client's data\nstolen. Crucially, the test does not destroy the data being tested, avoiding\nthe need to store a backup elsewhere. We define and construct proofs of no\nintrusion for ciphertexts assuming fully homomorphic encryption. Additionally,\nwe show how to equip several constructions of unclonable primitives with proofs\nof non-intrusion, such as unclonable decryption keys and signature tokens.\nConceptually, proofs of non-intrusion can be defined for essentially any\nunclonable primitive.\n  At the heart of our techniques is a new method for non-destructively testing\ncoset states with classical communication. It can be viewed as a\nnon-destructive proof of knowledge of a measurement result of the coset state."}
{"id": "2510.06468", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.06468", "abs": "https://arxiv.org/abs/2510.06468", "authors": ["Sergio Demian Lerner", "Ariel Futoransky"], "title": "BATTLE for Bitcoin: Capital-Efficient Optimistic Bridges with Large Committees", "comment": null, "summary": "We present BATTLE for Bitcoin, a DoS-resilient dispute layer that secures\noptimistic bridges between Bitcoin and rollups or sidechains. Our design adapts\nthe BATTLE tournament protocol to Bitcoin's UTXO model using BitVM-style FLEX\ncomponents and garbled circuits with on-demand L1 security bonds. Disputes are\nresolved in logarithmic rounds while recycling rewards, keeping the honest\nasserter's minimum initial capital constant even under many permissionless\nchallengers. The construction is fully contestable (challengers can supply\nhigher-work counter-proofs) and relies only on standard timelocks and\npre-signed transaction DAGs, without new opcodes.\n  For $N$ operators, the protocol requires $O(N^2)$ pre-signed transactions,\nsignatures, and message exchanges, yet remains practical at $N\\!\\gtrsim\\!10^3$,\nenabling high decentralization."}
{"id": "2510.06343", "categories": ["cs.SE", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.06343", "abs": "https://arxiv.org/abs/2510.06343", "authors": ["Fikret Mert GÃ¼ltekin", "Oscar Lilja", "Ranim Khojah", "Rebekka Wohlrab", "Marvin Damschen", "Mazen Mohamad"], "title": "Leveraging Large Language Models for Cybersecurity Risk Assessment -- A Case from Forestry Cyber-Physical Systems", "comment": "Accepted at Autonomous Agents in Software Engineering (AgenticSE)\n  Workshop, co-located with ASE 2025", "summary": "In safety-critical software systems, cybersecurity activities become\nessential, with risk assessment being one of the most critical. In many\nsoftware teams, cybersecurity experts are either entirely absent or represented\nby only a small number of specialists. As a result, the workload for these\nexperts becomes high, and software engineers would need to conduct\ncybersecurity activities themselves. This creates a need for a tool to support\ncybersecurity experts and engineers in evaluating vulnerabilities and threats\nduring the risk assessment process. This paper explores the potential of\nleveraging locally hosted large language models (LLMs) with retrieval-augmented\ngeneration to support cybersecurity risk assessment in the forestry domain\nwhile complying with data protection and privacy requirements that limit\nexternal data sharing. We performed a design science study involving 12 experts\nin interviews, interactive sessions, and a survey within a large-scale project.\nThe results demonstrate that LLMs can assist cybersecurity experts by\ngenerating initial risk assessments, identifying threats, and providing\nredundancy checks. The results also highlight the necessity for human oversight\nto ensure accuracy and compliance. Despite trust concerns, experts were willing\nto utilize LLMs in specific evaluation and assistance roles, rather than solely\nrelying on their generative capabilities. This study provides insights that\nencourage the use of LLM-based agents to support the risk assessment process of\ncyber-physical systems in safety-critical domains."}
{"id": "2510.06530", "categories": ["cs.CR", "cs.ET", "cs.LG", "cs.NI"], "pdf": "https://arxiv.org/pdf/2510.06530", "abs": "https://arxiv.org/abs/2510.06530", "authors": ["Thusitha Dayaratne", "Ngoc Duy Pham", "Viet Vo", "Shangqi Lai", "Sharif Abuadbba", "Hajime Suzuki", "Xingliang Yuan", "Carsten Rudolph"], "title": "From Description to Detection: LLM based Extendable O-RAN Compliant Blind DoS Detection in 5G and Beyond", "comment": null, "summary": "The quality and experience of mobile communication have significantly\nimproved with the introduction of 5G, and these improvements are expected to\ncontinue beyond the 5G era. However, vulnerabilities in control-plane\nprotocols, such as Radio Resource Control (RRC) and Non-Access Stratum (NAS),\npose significant security threats, such as Blind Denial of Service (DoS)\nattacks. Despite the availability of existing anomaly detection methods that\nleverage rule-based systems or traditional machine learning methods, these\nmethods have several limitations, including the need for extensive training\ndata, predefined rules, and limited explainability. Addressing these\nchallenges, we propose a novel anomaly detection framework that leverages the\ncapabilities of Large Language Models (LLMs) in zero-shot mode with unordered\ndata and short natural language attack descriptions within the Open Radio\nAccess Network (O-RAN) architecture. We analyse robustness to prompt variation,\ndemonstrate the practicality of automating the attack descriptions and show\nthat detection quality relies on the semantic completeness of the description\nrather than its phrasing or length. We utilise an RRC/NAS dataset to evaluate\nthe solution and provide an extensive comparison of open-source and proprietary\nLLM implementations to demonstrate superior performance in attack detection. We\nfurther validate the practicality of our framework within O-RAN's real-time\nconstraints, illustrating its potential for detecting other Layer-3 attacks."}
{"id": "2510.06363", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.06363", "abs": "https://arxiv.org/abs/2510.06363", "authors": ["Ololade Babatunde", "Tomisin Ayodabo", "Raqibul Raqibul"], "title": "Improving Assignment Submission in Higher Education through a Git-Enabled System: An Iterative Case Study", "comment": null, "summary": "This study addresses challenges in traditional assignment submission methods\nused in higher education by introducing and evaluating a customized Git-based\nsubmission system. Employing iterative software development and user-centered\ndesign methodologies, the system was integrated within a real-world university\nenvironment. Empirical evaluation, including usability testing and student\nfeedback, indicated significant improvements in assignment tracking,\ncollaboration, and submission efficiency. Students reported positive\nexperiences using distributed version control workflows, highlighting improved\nlearning outcomes and reduced administrative burden. Challenges related to\ninitial adoption and student learning curves were identified and mitigated\nthrough iterative improvements. The proposed system contributes practical\ninsights for integrating distributed version control into educational settings,\nenhancing both instructor oversight and student engagement in software\nengineering and related disciplines. Based on our results, the research showed\nthat 85% of instructors found the git based system easier to use, with 84% of\nstudents preferring it over traditional methods, as it provides a 38% reduction\nin time taken for submission and review, while also leading to a 48% reduction\nin storage requirements."}
{"id": "2510.06535", "categories": ["cs.CR", "C.3; D.4.6; K.6.5"], "pdf": "https://arxiv.org/pdf/2510.06535", "abs": "https://arxiv.org/abs/2510.06535", "authors": ["Jack Vanlyssel", "Enrique Sobrados", "Ramsha Anwar", "Gruia-Catalin Roman", "Afsah Anwar"], "title": "SpyChain: Multi-Vector Supply Chain Attacks on Small Satellite Systems", "comment": "18 pages, 7 figures. Version includes implementation details and\n  experimental results using NASA's NOS3 satellite simulation framework", "summary": "Small satellites are integral to scientific, commercial, and defense\nmissions, but reliance on commercial off-the-shelf (COTS) hardware broadens\ntheir attack surface. Although supply chain threats are well studied in other\ncyber-physical domains, their feasibility and stealth in space systems remain\nlargely unexplored. Prior work has focused on flight software, which benefits\nfrom strict security practices and oversight. In contrast, auxiliary COTS\ncomponents often lack robust assurance yet enjoy comparable access to critical\non-board resources, including telemetry, system calls, and the software bus.\nDespite this privileged access, the insider threat within COTS hardware supply\nchains has received little attention. In this work, we present SpyChain, the\nfirst end-to-end design and implementation of independent and colluding\nhardware supply chain threats targeting small satellites. Using NASA's\nsatellite simulation (NOS3), we demonstrate that SpyChain can evade testing,\nexfiltrate telemetry, disrupt operations, and launch Denial of Service (DoS)\nattacks through covert channels that bypass ground monitoring. Our study traces\nan escalation from a simple solo component to dynamic, coordinating malware,\nintroducing a taxonomy of stealth across five scenarios. We showcase how\nimplicit trust in auxiliary components enables covert persistence and reveal\nnovel attack vectors, highlighting a new multi-component execution technique\nthat is now incorporated into the SPARTA matrix. Our findings are reinforced by\nacknowledgment and affirmation from NASA's NOS3 team. Finally, we implement\nlightweight onboard defenses, including runtime monitoring, to mitigate threats\nlike SpyChain."}
{"id": "2510.06483", "categories": ["cs.SE", "D.2.1; D.2.2; K.4.2; D.2.13"], "pdf": "https://arxiv.org/pdf/2510.06483", "abs": "https://arxiv.org/abs/2510.06483", "authors": ["Judith Michael", "Lukas Netz", "Bernhard Rumpe", "Ingo MÃ¼ller", "John Grundy", "Shavindra Wickramathilaka", "Hourieh Khalajzadeh"], "title": "Addressing Visual Impairments with Model-Driven Engineering: A Systematic Literature Review", "comment": "41 pages", "summary": "Software applications often pose barriers for users with accessibility needs,\ne.g., visual impairments. Model-driven engineering (MDE), with its systematic\nnature of code derivation, offers systematic methods to integrate accessibility\nconcerns into software development while reducing manual effort. This paper\npresents a systematic literature review on how MDE addresses accessibility for\nvision impairments. From 447 initially identified papers, 30 primary studies\nmet the inclusion criteria. About two-thirds reference the Web Content\nAccessibility Guidelines (WCAG), yet their project-specific adaptions and\nend-user validations hinder wider adoption in MDE. The analyzed studies model\nuser interface structures, interaction and navigation, user capabilities,\nrequirements, and context information. However, only few specify concrete\nmodeling techniques on how to incorporate accessibility needs or demonstrate\nfully functional systems. Insufficient details on MDE methods, i.e.,\ntransformation rules or code templates, hinder the reuse, generalizability, and\nreproducibility. Furthermore, limited involvement of affected users and limited\ndeveloper expertise in accessibility contribute to weak empirical validation.\nOverall, the findings indicate that current MDE research insufficiently\nsupports vision-related accessibility. Our paper concludes with a research\nagenda outlining how support for vision impairments can be more effectively\nembedded in MDE processes."}
{"id": "2510.06565", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.06565", "abs": "https://arxiv.org/abs/2510.06565", "authors": ["Jiuan Zhou", "Yu Cheng", "Yuan Xie", "Zhaoxia Yin"], "title": "Auto-Stega: An Agent-Driven System for Lifelong Strategy Evolution in LLM-Based Text Steganography", "comment": "15 pages, 9 figures", "summary": "With the rapid progress of LLMs, high quality generative text has become\nwidely available as a cover for text steganography. However, prevailing methods\nrely on hand-crafted or pre-specified strategies and struggle to balance\nefficiency, imperceptibility, and security, particularly at high embedding\nrates. Accordingly, we propose Auto-Stega, an agent-driven self-evolving\nframework that is the first to realize self-evolving steganographic strategies\nby automatically discovering, composing, and adapting strategies at inference\ntime; the framework operates as a closed loop of generating, evaluating,\nsummarizing, and updating that continually curates a structured strategy\nlibrary and adapts across corpora, styles, and task constraints. A decoding LLM\nrecovers the information under the shared strategy. To handle high embedding\nrates, we introduce PC-DNTE, a plug-and-play algorithm that maintains alignment\nwith the base model's conditional distribution at high embedding rates,\npreserving imperceptibility while enhancing security. Experimental results\ndemonstrate that at higher embedding rates Auto-Stega achieves superior\nperformance with gains of 42.2\\% in perplexity and 1.6\\% in anti-steganalysis\nperformance over SOTA methods."}
{"id": "2510.06606", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.06606", "abs": "https://arxiv.org/abs/2510.06606", "authors": ["Uswat Yusuf", "Genevieve Caumartin", "Diego Elias Costa"], "title": "Beyond More Context: How Granularity and Order Drive Code Completion Quality", "comment": null, "summary": "Context plays an important role in the quality of code completion, as Large\nLanguage Models (LLMs) require sufficient and relevant information to assist\ndevelopers in code generation tasks. However, composing a relevant context for\ncode completion poses challenges in large repositories: First, the limited\ncontext length of LLMs makes it impractical to include all repository files.\nSecond, the quality of generated code is highly sensitive to noisy or\nirrelevant context. In this paper, we present our approach for the ASE 2025\nContext Collection Challenge. The challenge entails outperforming JetBrains\nbaselines by designing effective retrieval and context collection strategies.\nWe develop and evaluate a series of experiments that involve retrieval\nstrategies at both the file and chunk levels. We focus our initial experiments\non examining the impact of context size and file ordering on LLM performance.\nOur results show that the amount and order of context can significantly\ninfluence the performance of the models. We introduce chunk-based retrieval\nusing static analysis, achieving a 6% improvement over our best file-retrieval\nstrategy and a 16% improvement over the no-context baseline for Python in the\ninitial phase of the competition. Our results highlight the importance of\nretrieval granularity, ordering and hybrid strategies in developing effective\ncontext collection pipelines for real-world development scenarios."}
{"id": "2510.06605", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06605", "abs": "https://arxiv.org/abs/2510.06605", "authors": ["Shuo Shao", "Yiming Li", "Hongwei Yao", "Yifei Chen", "Yuchen Yang", "Zhan Qin"], "title": "Reading Between the Lines: Towards Reliable Black-box LLM Fingerprinting via Zeroth-order Gradient Estimation", "comment": null, "summary": "The substantial investment required to develop Large Language Models (LLMs)\nmakes them valuable intellectual property, raising significant concerns about\ncopyright protection. LLM fingerprinting has emerged as a key technique to\naddress this, which aims to verify a model's origin by extracting an intrinsic,\nunique signature (a \"fingerprint\") and comparing it to that of a source model\nto identify illicit copies. However, existing black-box fingerprinting methods\noften fail to generate distinctive LLM fingerprints. This ineffectiveness\narises because black-box methods typically rely on model outputs, which lose\ncritical information about the model's unique parameters due to the usage of\nnon-linear functions. To address this, we first leverage Fisher Information\nTheory to formally demonstrate that the gradient of the model's input is a more\ninformative feature for fingerprinting than the output. Based on this insight,\nwe propose ZeroPrint, a novel method that approximates these information-rich\ngradients in a black-box setting using zeroth-order estimation. ZeroPrint\novercomes the challenge of applying this to discrete text by simulating input\nperturbations via semantic-preserving word substitutions. This operation allows\nZeroPrint to estimate the model's Jacobian matrix as a unique fingerprint.\nExperiments on the standard benchmark show ZeroPrint achieves a\nstate-of-the-art effectiveness and robustness, significantly outperforming\nexisting black-box methods."}
{"id": "2510.06708", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06708", "abs": "https://arxiv.org/abs/2510.06708", "authors": ["Aleksi Huotala", "Miikka Kuutila", "Olli-Pekka Turtio", "Mika MÃ¤ntylÃ¤"], "title": "AISysRev -- LLM-based Tool for Title-abstract Screening", "comment": "4 pages", "summary": "Systematic reviews are a standard practice for summarizing the state of\nevidence in software engineering. Conducting systematic reviews is laborious,\nespecially during the screening or study selection phase, where the number of\npapers can be overwhelming. During this phase, papers are assessed against\ninclusion and exclusion criteria based on their titles and abstracts. Recent\nresearch has demonstrated that large language models (LLMs) can perform\ntitle-abstract screening at a level comparable to that of a master's student.\nWhile LLMs cannot be fully trusted, they can help, for example, in Rapid\nReviews, which try to expedite the review process. Building on recent research,\nwe developed AiSysRev, an LLM-based screening tool implemented as a web\napplication running in a Docker container. The tool accepts a CSV file\ncontaining paper titles and abstracts. Users specify inclusion and exclusion\ncriteria. One can use multiple LLMs for screening via OpenRouter. AiSysRev\nsupports both zero-shot and few-shot screening, and also allows for manual\nscreening through interfaces that display LLM results as guidance for human\nreviewers.We conducted a trial study with 137 papers using the tool. Our\nfindings indicate that papers can be classified into four categories: Easy\nIncludes, Easy Excludes, Boundary Includes, and Boundary Excludes. The Boundary\ncases, where LLMs are prone to errors, highlight the need for human\nintervention. While LLMs do not replace human judgment in systematic reviews,\nthey can significantly reduce the burden of assessing large volumes of\nscientific literature. Video: https://www.youtube.com/watch?v=jVbEj4Y4tQI Tool:\nhttps://github.com/EvoTestOps/AISysRev"}
{"id": "2510.06607", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.06607", "abs": "https://arxiv.org/abs/2510.06607", "authors": ["Weidi Luo", "Qiming Zhang", "Tianyu Lu", "Xiaogeng Liu", "Bin Hu", "Hung-Chun Chiu", "Siyuan Ma", "Yizhe Zhang", "Xusheng Xiao", "Yinzhi Cao", "Zhen Xiang", "Chaowei Xiao"], "title": "Code Agent can be an End-to-end System Hacker: Benchmarking Real-world Threats of Computer-use Agent", "comment": null, "summary": "Computer-use agent (CUA) frameworks, powered by large language models (LLMs)\nor multimodal LLMs (MLLMs), are rapidly maturing as assistants that can\nperceive context, reason, and act directly within software environments. Among\ntheir most critical applications is operating system (OS) control. As CUAs in\nthe OS domain become increasingly embedded in daily operations, it is\nimperative to examine their real-world security implications, specifically\nwhether CUAs can be misused to perform realistic, security-relevant attacks.\nExisting works exhibit four major limitations: Missing attacker-knowledge model\non tactics, techniques, and procedures (TTP), Incomplete coverage for\nend-to-end kill chains, unrealistic environment without multi-host and\nencrypted user credentials, and unreliable judgment dependent on\nLLM-as-a-Judge. To address these gaps, we propose AdvCUA, the first benchmark\naligned with real-world TTPs in MITRE ATT&CK Enterprise Matrix, which comprises\n140 tasks, including 40 direct malicious tasks, 74 TTP-based malicious tasks,\nand 26 end-to-end kill chains, systematically evaluates CUAs under a realistic\nenterprise OS security threat in a multi-host environment sandbox by hard-coded\nevaluation. We evaluate the existing five mainstream CUAs, including ReAct,\nAutoGPT, Gemini CLI, Cursor CLI, and Cursor IDE based on 8 foundation LLMs. The\nresults demonstrate that current frontier CUAs do not adequately cover OS\nsecurity-centric threats. These capabilities of CUAs reduce dependence on\ncustom malware and deep domain expertise, enabling even inexperienced attackers\nto mount complex enterprise intrusions, which raises social concern about the\nresponsibility and security of CUAs."}
{"id": "2510.06718", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06718", "abs": "https://arxiv.org/abs/2510.06718", "authors": ["Ranim Khojah", "Mazen Mohamad", "Linda Erlenhov", "Francisco Gomes de Oliveira Neto", "Philipp Leitner"], "title": "LLM Company Policies and Policy Implications in Software Organizations", "comment": "Accepted at IEEE Software Special Issue on AIware in the Foundation\n  Models Era", "summary": "The risks associated with adopting large language model (LLM) chatbots in\nsoftware organizations highlight the need for clear policies. We examine how 11\ncompanies create these policies and the factors that influence them, aiming to\nhelp managers safely integrate chatbots into development workflows."}
{"id": "2510.06629", "categories": ["cs.CR", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06629", "abs": "https://arxiv.org/abs/2510.06629", "authors": ["Jiachen Li", "Bang Wu", "Xiaoyu Xia", "Xiaoning Liu", "Xun Yi", "Xiuzhen Zhang"], "title": "Unsupervised Backdoor Detection and Mitigation for Spiking Neural Networks", "comment": "To appear in The 28th International Symposium on Research in Attacks,\n  Intrusions and Defenses (RAID 2025)", "summary": "Spiking Neural Networks (SNNs) have gained increasing attention for their\nsuperior energy efficiency compared to Artificial Neural Networks (ANNs).\nHowever, their security aspects, particularly under backdoor attacks, have\nreceived limited attention. Existing defense methods developed for ANNs perform\npoorly or can be easily bypassed in SNNs due to their event-driven and temporal\ndependencies. This paper identifies the key blockers that hinder traditional\nbackdoor defenses in SNNs and proposes an unsupervised post-training detection\nframework, Temporal Membrane Potential Backdoor Detection (TMPBD), to overcome\nthese challenges. TMPBD leverages the maximum margin statistics of temporal\nmembrane potential (TMP) in the final spiking layer to detect target labels\nwithout any attack knowledge or data access. We further introduce a robust\nmitigation mechanism, Neural Dendrites Suppression Backdoor Mitigation (NDSBM),\nwhich clamps dendritic connections between early convolutional layers to\nsuppress malicious neurons while preserving benign behaviors, guided by TMP\nextracted from a small, clean, unlabeled dataset. Extensive experiments on\nmultiple neuromorphic benchmarks and state-of-the-art input-aware dynamic\ntrigger attacks demonstrate that TMPBD achieves 100% detection accuracy, while\nNDSBM reduces the attack success rate from 100% to 8.44%, and to 2.81% when\ncombined with detection, without degrading clean accuracy."}
{"id": "2510.06844", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.06844", "abs": "https://arxiv.org/abs/2510.06844", "authors": ["Nicole Hoess", "Carlos Paradis", "Rick Kazman", "Wolfgang Mauerer"], "title": "Oops!... I did it again. Conclusion (In-)Stability in Quantitative Empirical Software Engineering: A Large-Scale Analysis", "comment": null, "summary": "Context: Mining software repositories is a popular means to gain insights\ninto a software project's evolution, monitor project health, support decisions\nand derive best practices. Tools supporting the mining process are commonly\napplied by researchers and practitioners, but their limitations and agreement\nare often not well understood.\n  Objective: This study investigates some threats to validity in complex tool\npipelines for evolutionary software analyses and evaluates the tools' agreement\nin terms of data, study outcomes and conclusions for the same research\nquestions.\n  Method: We conduct a lightweight literature review to select three studies on\ncollaboration and coordination, software maintenance and software quality from\nhigh-ranked venues, which we formally replicate with four independent,\nsystematically selected mining tools to quantitatively and qualitatively\ncompare the extracted data, analysis results and conclusions.\n  Results: We find that numerous technical details in tool design and\nimplementation accumulate along the complex mining pipelines and can cause\nsubstantial differences in the extracted baseline data, its derivatives,\nsubsequent results of statistical analyses and, under specific circumstances,\nconclusions.\n  Conclusions: Users must carefully choose tools and evaluate their limitations\nto assess the scope of validity in an adequate way. Reusing tools is\nrecommended. Researchers and tool authors can promote reusability and help\nreducing uncertainties by reproduction packages and comparative studies\nfollowing our approach."}
{"id": "2510.06645", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06645", "abs": "https://arxiv.org/abs/2510.06645", "authors": ["Zhiyuan Wei", "Xiaoxuan Yang", "Jing Sun", "Zijian Zhang"], "title": "Distilling Lightweight Language Models for C/C++ Vulnerabilities", "comment": "25 pages, 10 figures", "summary": "The increasing complexity of modern software systems exacerbates the\nprevalence of security vulnerabilities, posing risks of severe breaches and\nsubstantial economic loss. Consequently, robust code vulnerability detection is\nessential for software security. While Large Language Models (LLMs) have\ndemonstrated remarkable capabilities in natural language processing, their\npotential for automated code vulnerability detection remains underexplored.\nThis paper presents FineSec, a novel framework that harnesses LLMs through\nknowledge distillation to enable efficient and precise vulnerability\nidentification in C/C++ codebases. FineSec utilizes knowledge distillation to\ntransfer expertise from large teacher models to compact student models,\nachieving high accuracy with minimal computational cost. By integrating data\npreparation, training, evaluation, and continuous learning into a unified,\nsingle-task workflow, FineSec offers a streamlined approach. Extensive\nevaluations on C/C++ codebases demonstrate its superiority over both base\nmodels and larger LLMs in identifying complex vulnerabilities and logical\nflaws, establishing FineSec as a practical and scalable solution for real-world\nsoftware security. To facilitate reproducibility, the datasets, source code,\nand experimental results are made publicly available at:\nhttps://github.com/yangxiaoxuan123/FineSec_detect."}
{"id": "2510.06984", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.06984", "abs": "https://arxiv.org/abs/2510.06984", "authors": ["Masanari Kondo", "Mahmoud Alfadel", "Shane McIntosh", "Yasutaka Kamei", "Naoyasu Ubayashi"], "title": "An empirical study on declined proposals: why are these proposals declined?", "comment": null, "summary": "Design-level decisions in open-source software (OSS) projects are often made\nthrough structured mechanisms such as proposals, which require substantial\ncommunity discussion and review. Despite their importance, the proposal process\nis resource-intensive and often leads to contributor frustration, especially\nwhen proposals are declined without clear feedback. Yet, the reasons behind\nproposal rejection remain poorly understood, limiting opportunities to\nstreamline the process or guide contributors effectively. This study\ninvestigates the characteristics and outcomes of proposals in the Go\nprogramming language to understand why proposals are declined and how such\noutcomes might be anticipated. We conduct a mixed-method empirical study on\n1,091 proposals submitted to the Go project. We quantify proposal outcomes,\nbuild a taxonomy of decline reasons, and evaluate large language models (LLMs)\nfor predicting these outcomes. We find that proposals are more often declined\nthan accepted, and resolution typically takes over a month. Only 14.7% of\ndeclined proposals are ever resubmitted. Through qualitative coding, we\nidentify nine key reasons for proposal decline, such as duplication, limited\nuse cases, or violations of project principles. This taxonomy can help\ncontributors address issues in advance, e.g., checking for existing\nalternatives can reduce redundancy. We also demonstrate that GPT-based models\ncan predict decline decisions early in the discussion (F1 score = 0.71 with\npartial comments), offering a practical tool for prioritizing review effort.\nOur findings reveal inefficiencies in the proposal process and highlight\nactionable opportunities for improving both contributor experience and reviewer\nworkload by enabling early triage and guiding contributors to strengthen their\nproposals using a structured understanding of past decline reasons."}
{"id": "2510.06719", "categories": ["cs.CR", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06719", "abs": "https://arxiv.org/abs/2510.06719", "authors": ["Junki Mori", "Kazuya Kakizaki", "Taiki Miyagawa", "Jun Sakuma"], "title": "Differentially Private Synthetic Text Generation for Retrieval-Augmented Generation (RAG)", "comment": "Under review", "summary": "Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by\ngrounding them in external knowledge. However, its application in sensitive\ndomains is limited by privacy risks. Existing private RAG methods typically\nrely on query-time differential privacy (DP), which requires repeated noise\ninjection and leads to accumulated privacy loss. To address this issue, we\npropose DP-SynRAG, a framework that uses LLMs to generate differentially\nprivate synthetic RAG databases. Unlike prior methods, the synthetic text can\nbe reused once created, thereby avoiding repeated noise injection and\nadditional privacy costs. To preserve essential information for downstream RAG\ntasks, DP-SynRAG extends private prediction, which instructs LLMs to generate\ntext that mimics subsampled database records in a DP manner. Experiments show\nthat DP-SynRAG achieves superior performanec to the state-of-the-art private\nRAG systems while maintaining a fixed privacy budget, offering a scalable\nsolution for privacy-preserving RAG."}
{"id": "2510.06989", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.06989", "abs": "https://arxiv.org/abs/2510.06989", "authors": ["Pengyue Yang", "Haolin Jin", "Qingwen Zeng", "Jiawen Wen", "Harry Rao", "Huaming Chen"], "title": "Human-aligned AI Model Cards with Weighted Hierarchy Architecture", "comment": "10 pages, 5 figures. Submitted to ICSE SEIP 2026 (Software\n  Engineering in Practice)", "summary": "The proliferation of Large Language Models (LLMs) has led to a burgeoning\necosystem of specialized, domain-specific models. While this rapid growth\naccelerates innovation, it has simultaneously created significant challenges in\nmodel discovery and adoption. Users struggle to navigate this landscape due to\ninconsistent, incomplete, and imbalanced documentation across platforms.\nExisting documentation frameworks, such as Model Cards and FactSheets, attempt\nto standardize reporting but are often static, predominantly qualitative, and\nlack the quantitative mechanisms needed for rigorous cross-model comparison.\nThis gap exacerbates model underutilization and hinders responsible adoption.\nTo address these shortcomings, we introduce the Comprehensive Responsible AI\nModel Card Framework (CRAI-MCF), a novel approach that transitions from static\ndisclosures to actionable, human-aligned documentation. Grounded in Value\nSensitive Design (VSD), CRAI-MCF is built upon an empirical analysis of 240\nopen-source projects, distilling 217 parameters into an eight-module,\nvalue-aligned architecture. Our framework introduces a quantitative sufficiency\ncriterion to operationalize evaluation and enables rigorous cross-model\ncomparison under a unified scheme. By balancing technical, ethical, and\noperational dimensions, CRAI-MCF empowers practitioners to efficiently assess,\nselect, and adopt LLMs with greater confidence and operational integrity."}
{"id": "2510.06784", "categories": ["cs.CR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.06784", "abs": "https://arxiv.org/abs/2510.06784", "authors": ["Dmytro Zakharov", "Oleksandr Kurbatov", "Artem Sdobnov", "Lev Soukhanov", "Yevhenii Sekhin", "Vitalii Volovyk", "Mykhailo Velykodnyi", "Mark Cherepovskyi", "Kyrylo Baibula", "Lasha Antadze", "Pavlo Kravchenko", "Volodymyr Dubinin", "Yaroslav Panasenko"], "title": "Bionetta: Efficient Client-Side Zero-Knowledge Machine Learning Proving", "comment": null, "summary": "In this report, we compare the performance of our UltraGroth-based\nzero-knowledge machine learning framework Bionetta to other tools of similar\npurpose such as EZKL, Lagrange's deep-prove, or zkml. The results show a\nsignificant boost in the proving time for custom-crafted neural networks: they\ncan be proven even on mobile devices, enabling numerous client-side proving\napplications. While our scheme increases the cost of one-time preprocessing\nsteps, such as circuit compilation and generating trusted setup, our approach\nis, to the best of our knowledge, the only one that is deployable on the native\nEVM smart contracts without overwhelming proof size and verification overheads."}
{"id": "2510.07070", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.07070", "abs": "https://arxiv.org/abs/2510.07070", "authors": ["Gopi Krishnan Rajbahadur", "Keheliya Gallaba", "Elyas Rashno", "Arthit Suriyawongkul", "Karen Bennet", "Kate Stewart", "Ahmed E. Hassan"], "title": "Building an Open AIBOM Standard in the Wild", "comment": null, "summary": "Modern software engineering increasingly relies on open, community-driven\nstandards, yet how such standards are created in fast-evolving domains like\nAI-powered systems remains underexplored. This paper presents a detailed\nexperience report on the development of the AI Bill of Materials AIBOM\nspecification, an extension of the ISO/IEC 5962:2021 Software Package Data\nExchange (SPDX) software bill of materials (SBOM) standard, which captures AI\ncomponents such as datasets and iterative training artifacts. Framed through\nthe lens of Action Research (AR), we document a global, multi-stakeholder\neffort involving over 90 contributors and structured AR cycles. The resulting\nspecification was validated through four complementary approaches: alignment\nwith major regulations and ethical standards (e.g., EU AI Act and IEEE 7000\nstandards), systematic mapping to six industry use cases, semi-structured\npractitioner interviews, and an industrial case study. Beyond delivering a\nvalidated artefact, our paper documents the process of building the AIBOM\nspecification in the wild, and reflects on how it aligns with the AR cycle, and\ndistills lessons that can inform future standardization efforts in the software\nengineering community."}
{"id": "2510.06823", "categories": ["cs.CR", "cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.06823", "abs": "https://arxiv.org/abs/2510.06823", "authors": ["Riku Mochizuki", "Shusuke Komatsu", "Souta Noguchi", "Kazuto Ataka"], "title": "Exposing Citation Vulnerabilities in Generative Engines", "comment": "12 pages, under-reviewing at a conference", "summary": "We analyze answers generated by generative engines (GEs) from the\nperspectives of citation publishers and the content-injection barrier, defined\nas the difficulty for attackers to manipulate answers to user prompts by\nplacing malicious content on the web. GEs integrate two functions: web search\nand answer generation that cites web pages using large language models. Because\nanyone can publish information on the web, GEs are vulnerable to poisoning\nattacks. Existing studies of citation evaluation focus on how faithfully answer\ncontent reflects cited sources, leaving unexamined which web sources should be\nselected as citations to defend against poisoning attacks. To fill this gap, we\nintroduce evaluation criteria that assess poisoning threats using the citation\ninformation contained in answers. Our criteria classify the publisher\nattributes of citations to estimate the content-injection barrier thereby\nrevealing the threat of poisoning attacks in current GEs. We conduct\nexperiments in political domains in Japan and the United States (U.S.) using\nour criteria and show that citations from official party websites (primary\nsources) are approximately \\(25\\%\\)--\\(45\\%\\) in the U.S. and\n\\(60\\%\\)--\\(65\\%\\) in Japan, indicating that U.S. political answers are at\nhigher risk of poisoning attacks. We also find that sources with low\ncontent-injection barriers are frequently cited yet are poorly reflected in\nanswer content. To mitigate this threat, we discuss how publishers of primary\nsources can increase exposure of their web content in answers and show that\nwell-known techniques are limited by language differences."}
{"id": "2510.07189", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.07189", "abs": "https://arxiv.org/abs/2510.07189", "authors": ["Junjie Li", "Fazle Rabbi", "Bo Yang", "Song Wang", "Jinqiu Yang"], "title": "Prompt, Synthesize, Fine-Tune: A Secure Code Generation Recipe", "comment": null, "summary": "Although Large Language Models (LLMs) show promising solutions to automated\ncode generation, they often produce insecure code that threatens software\nsecurity. Current approaches (e.g., SafeCoder) to improve secure code\ngeneration suffer from limited and imbalanced datasets, reducing their\neffectiveness and generalizability. In this work, we present Secure-Instruct, a\nnovel framework that automatically synthesizes high-quality vulnerable and\nsecure code examples, generates fine-tuning instructions, and instruction-tunes\nLLMs to align task description and secure code generation abilities. We\nevaluate Secure-Instruct on four representative LLMs using two benchmarks: our\nown CWEBench and the existing CWEval. CWEBench comprises 93 scenarios on 44\nCWEs, all without overlap with Secure-Instruct's synthetic instruction-tuning\ndataset, while CWEval covers 31 CWEs with 119 manually verified\nsecurity-critical tasks. We find that Secure-Instruct improves not only the\nsecurity but also the functional correctness of the generated code. On\nCWEBench, Secure-Instruct substantially improves secure code generation, giving\na 14.3% average increase in secure ratio over the pretrained models and\noutperforms SafeCoder by 7.6%. On CWEval, Secure-Instruct achieves a 14%\nincrease for CodeLlama-7B and 5.8% for Mistral-7B in Func-Sec@1 over pretrained\nmodels, and surpasses SafeCoder by 15.8% and 6.8% respectively."}
{"id": "2510.06951", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.06951", "abs": "https://arxiv.org/abs/2510.06951", "authors": ["Philip Huff", "Nishka Gandu", "Pavel NovÃ¡k"], "title": "I Can't Patch My OT Systems! A Look at CISA's KEVC Workarounds & Mitigations for OT", "comment": "8 pages, 6 figures. Supported by DOE Grant CR0000031", "summary": "We examine the state of publicly available information about known\nexploitable vulnerabilities applicable to operational technology (OT)\nenvironments. Specifically, we analyze the Known Exploitable Vulnerabilities\nCatalog (KEVC) maintained by the US Department of Homeland Security\nCybersecurity and Infrastructure Security Agency (CISA) to assess whether\ncurrently available data is sufficient for effective and reliable remediation\nin OT settings. Our team analyzed all KEVC entries through July 2025 to\ndetermine the extent to which OT environments can rely on existing remediation\nrecommendations. We found that although most entries in the KEVC could affect\nOT environments, only 13% include vendor workarounds or mitigations as\nalternatives to patching. This paper also examines the feasibility of\ndeveloping such alternatives based on vulnerability and exploit\ncharacteristics, and we present early evidence of success with this approach."}
{"id": "2510.06975", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06975", "abs": "https://arxiv.org/abs/2510.06975", "authors": ["Muris SladiÄ", "Veronica Valeros", "Carlos Catania", "Sebastian Garcia"], "title": "VelLMes: A high-interaction AI-based deception framework", "comment": "9 pages. 9 figures. 1 table. This is a preprint of a paper that was\n  presented at the Active Defense and Deception Workshop colocated with IEEE\n  EuroS&P 2025 conference", "summary": "There are very few SotA deception systems based on Large Language Models. The\nexisting ones are limited only to simulating one type of service, mainly SSH\nshells. These systems - but also the deception technologies not based on LLMs -\nlack an extensive evaluation that includes human attackers. Generative AI has\nrecently become a valuable asset for cybersecurity researchers and\npractitioners, and the field of cyber-deception is no exception. Researchers\nhave demonstrated how LLMs can be leveraged to create realistic-looking\nhoneytokens, fake users, and even simulated systems that can be used as\nhoneypots. This paper presents an AI-based deception framework called VelLMes,\nwhich can simulate multiple protocols and services such as SSH Linux shell,\nMySQL, POP3, and HTTP. All of these can be deployed and used as honeypots, thus\nVelLMes offers a variety of choices for deception design based on the users'\nneeds. VelLMes is designed to be attacked by humans, so interactivity and\nrealism are key for its performance. We evaluate the generative capabilities\nand the deception capabilities. Generative capabilities were evaluated using\nunit tests for LLMs. The results of the unit tests show that, with careful\nprompting, LLMs can produce realistic-looking responses, with some LLMs having\na 100% passing rate. In the case of the SSH Linux shell, we evaluated deception\ncapabilities with 89 human attackers. The results showed that about 30% of the\nattackers thought that they were interacting with a real system when they were\nassigned an LLM-based honeypot. Lastly, we deployed 10 instances of the SSH\nLinux shell honeypot on the Internet to capture real-life attacks. Analysis of\nthese attacks showed us that LLM honeypots simulating Linux shells can perform\nwell against unstructured and unexpected attacks on the Internet, responding\ncorrectly to most of the issued commands."}
{"id": "2510.06994", "categories": ["cs.CR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06994", "abs": "https://arxiv.org/abs/2510.06994", "authors": ["Artur Horal", "Daniel Pina", "Henrique Paz", "Iago Paulo", "JoÃ£o Soares", "Rafael Ferreira", "Diogo Tavares", "Diogo GlÃ³ria-Silva", "JoÃ£o MagalhÃ£es", "David Semedo"], "title": "RedTWIZ: Diverse LLM Red Teaming via Adaptive Attack Planning", "comment": null, "summary": "This paper presents the vision, scientific contributions, and technical\ndetails of RedTWIZ: an adaptive and diverse multi-turn red teaming framework,\nto audit the robustness of Large Language Models (LLMs) in AI-assisted software\ndevelopment. Our work is driven by three major research streams: (1) robust and\nsystematic assessment of LLM conversational jailbreaks; (2) a diverse\ngenerative multi-turn attack suite, supporting compositional, realistic and\ngoal-oriented jailbreak conversational strategies; and (3) a hierarchical\nattack planner, which adaptively plans, serializes, and triggers attacks\ntailored to specific LLM's vulnerabilities. Together, these contributions form\na unified framework -- combining assessment, attack generation, and strategic\nplanning -- to comprehensively evaluate and expose weaknesses in LLMs'\nrobustness. Extensive evaluation is conducted to systematically assess and\nanalyze the performance of the overall system and each component. Experimental\nresults demonstrate that our multi-turn adversarial attack strategies can\nsuccessfully lead state-of-the-art LLMs to produce unsafe generations,\nhighlighting the pressing need for more research into enhancing LLM's\nrobustness."}
{"id": "2510.07080", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.07080", "abs": "https://arxiv.org/abs/2510.07080", "authors": ["Maxime Reynouard"], "title": "Pseudo-MDPs: A Novel Framework for Efficiently Optimizing Last Revealer Seed Manipulations in Blockchains", "comment": null, "summary": "This study tackles the computational challenges of solving Markov Decision\nProcesses (MDPs) for a restricted class of problems. It is motivated by the\nLast Revealer Attack (LRA), which undermines fairness in some Proof-of-Stake\n(PoS) blockchains such as Ethereum (\\$400B market capitalization). We introduce\npseudo-MDPs (pMDPs) a framework that naturally models such problems and propose\ntwo distinct problem reductions to standard MDPs. One problem reduction\nprovides a novel, counter-intuitive perspective, and combining the two problem\nreductions enables significant improvements in dynamic programming algorithms\nsuch as value iteration. In the case of the LRA which size is parameterized by\n$\\kappa$ (in Ethereum's case $\\kappa$= 325), we reduce the computational\ncomplexity from $O(2^\\kappa \\kappa^{2^{\\kappa+2}})$ to $O(\\kappa^4)$ (per\niteration). This solution also provide the usual benefits from Dynamic\nProgramming solutions: exponentially fast convergence toward the optimal\nsolution is guaranteed. The dual perspective also simplifies policy extraction,\nmaking the approach well-suited for resource-constrained agents who can operate\nwith very limited memory and computation once the problem has been solved.\nFurthermore, we generalize those results to a broader class of MDPs, enhancing\ntheir applicability. The framework is validated through two case studies: a\nfictional card game and the LRA on the Ethereum random seed consensus protocol.\nThese applications demonstrate the framework's ability to solve large-scale\nproblems effectively while offering actionable insights into optimal\nstrategies. This work advances the study of MDPs and contributes to\nunderstanding security vulnerabilities in blockchain systems."}
{"id": "2510.07109", "categories": ["cs.CR", "cs.LG", "cs.NI", "C.2.0; C.2.1; C.2.3; C.2.5; I.2.6; K.6.5"], "pdf": "https://arxiv.org/pdf/2510.07109", "abs": "https://arxiv.org/abs/2510.07109", "authors": ["Guan-Yan Yang", "Farn Wang", "Kuo-Hui Yeh"], "title": "GNN-enhanced Traffic Anomaly Detection for Next-Generation SDN-Enabled Consumer Electronics", "comment": "This paper has been accepted for publication in IEEE Transactions on\n  Consumer Electronics. 10 pages, 6 figures", "summary": "Consumer electronics (CE) connected to the Internet of Things are susceptible\nto various attacks, including DDoS and web-based threats, which can compromise\ntheir functionality and facilitate remote hijacking. These vulnerabilities\nallow attackers to exploit CE for broader system attacks while enabling the\npropagation of malicious code across the CE network, resulting in device\nfailures. Existing deep learning-based traffic anomaly detection systems\nexhibit high accuracy in traditional network environments but are often overly\ncomplex and reliant on static infrastructure, necessitating manual\nconfiguration and management. To address these limitations, we propose a\nscalable network model that integrates Software-defined Networking (SDN) and\nCompute First Networking (CFN) for next-generation CE networks. In this network\nmodel, we propose a Graph Neural Networks-based Network Anomaly Detection\nframework (GNN-NAD) that integrates SDN-based CE networks and enables the CFN\narchitecture. GNN-NAD uniquely fuses a static, vulnerability-aware attack graph\nwith dynamic traffic features, providing a holistic view of network security.\nThe core of the framework is a GNN model (GSAGE) for graph representation\nlearning, followed by a Random Forest (RF) classifier. This design (GSAGE+RF)\ndemonstrates superior performance compared to existing feature selection\nmethods. Experimental evaluations on CE environment reveal that GNN-NAD\nachieves superior metrics in accuracy, recall, precision, and F1 score, even\nwith small sample sizes, exceeding the performance of current network anomaly\ndetection methods. This work advances the security and efficiency of\nnext-generation intelligent CE networks."}
{"id": "2510.06261", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06261", "abs": "https://arxiv.org/abs/2510.06261", "authors": ["Zhanke Zhou", "Chentao Cao", "Xiao Feng", "Xuan Li", "Zongze Li", "Xiangyu Lu", "Jiangchao Yao", "Weikai Huang", "Linrui Xu", "Tian Cheng", "Guanyu Jiang", "Yiming Zheng", "Brando Miranda", "Tongliang Liu", "Sanmi Koyejo", "Masashi Sugiyama", "Bo Han"], "title": "AlphaApollo: Orchestrating Foundation Models and Professional Tools into a Self-Evolving System for Deep Agentic Reasoning", "comment": "Ongoing project", "summary": "We present AlphaApollo, a self-evolving agentic reasoning system that aims to\naddress two bottlenecks in foundation model (FM) reasoning-limited\nmodel-intrinsic capacity and unreliable test-time iteration. AlphaApollo\norchestrates multiple models with professional tools to enable deliberate,\nverifiable reasoning. It couples (i) a computation tool (Python with numerical\nand symbolic libraries) and (ii) a retrieval tool (task-relevant external\ninformation) to execute exact calculations and ground decisions. The system\nfurther supports multi-round, multi-model solution evolution via a shared state\nmap that records candidates, executable checks, and feedback for iterative\nrefinement. In evaluations on AIME 2024/2025 across multiple models,\nAlphaApollo delivers consistent gains: +5.15% Average@32 and +23.34% Pass@32\nfor Qwen2.5-14B-Instruct, and +8.91% Average@32 with +26.67% Pass@32 for\nLlama-3.3-70B-Instruct. Tool-use analysis shows that more than 80% of tool\ncalls are successfully executed, with consistent outperformance of non-tool\nbaselines, thereby lifting the capability ceiling of FMs. More empirical\nresults and implementation details will be updated at\nhttps://github.com/tmlr-group/AlphaApollo."}
{"id": "2510.07171", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.07171", "abs": "https://arxiv.org/abs/2510.07171", "authors": ["Rishabh Das. Aaron Werth", "Tommy Morris"], "title": "A multi-layered embedded intrusion detection framework for programmable logic controllers", "comment": null, "summary": "Industrial control system (ICS) operations use trusted endpoints like human\nmachine interfaces (HMIs) and workstations to relay commands to programmable\nlogic controllers (PLCs). Because most PLCs lack layered defenses, compromise\nof a trusted endpoint can drive unsafe actuator commands and risk\nsafety-critical operation. This research presents an embedded intrusion\ndetection system that runs inside the controller and uses header-level\ntelemetry to detect and respond to network attacks. The system combines a\nsemi-supervised anomaly detector and a supervised attack classifier. We\nevaluate the approach on a midstream oil-terminal testbed using three datasets\ncollected during tanker-truck loading. The anomaly detector achieves zero\nmissed attacks, corresponding to 0.998 Matthews correlation. The supervised\nstage attains 97.37 percent hold-out accuracy and 97.03 percent external\naccuracy. The embedded design adds a median of 2,031 microseconds of end-to-end\nlatency and does not impact PLC's cycle time. The proposed architecture\nprovides a multi-layer embedded security that meets the real-time requirements\nof an industrial system."}
{"id": "2510.06274", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06274", "abs": "https://arxiv.org/abs/2510.06274", "authors": ["Mohammad Mahdi Samiei Paqaleh", "Arash Marioriyad", "Arman Tahmasebi-Zadeh", "Mohamadreza Fereydooni", "Mahdi Ghaznavai", "Mahdieh Soleymani Baghshah"], "title": "Bridging Reasoning to Learning: Unmasking Illusions using Complexity Out of Distribution Generalization", "comment": null, "summary": "Recent progress has pushed AI frontiers from pattern recognition tasks toward\nproblems that require step by step, System2 style reasoning, especially with\nlarge language models. Yet, unlike learning, where generalization and out of\ndistribution (OoD) evaluation concepts are well formalized, there is no clear,\nconsistent definition or metric for reasoning ability. We propose Complexity\nOut of Distribution (Complexity OoD) generalization as a framework and problem\nsetting to define and measure reasoning. A model exhibits Complexity OoD\ngeneralization when it maintains performance on test instances whose minimal\nrequired solution complexity, either representational (richer solution\nstructure) or computational (more reasoning steps/program length), exceeds that\nof all training examples. We formalize complexity via solution description\nKolmogorov complexity and operational proxies (e.g., object/relation counts;\nreasoning step counts), clarifying how Complexity OoD differs from length and\ncompositional OoD. This lens unifies learning and reasoning: many cases\nsolvable with System1 like processing at low complexity become System2 like\nunder complexity pressure, while System2 can be viewed as generalization over\nsolution structures. We translate this perspective into practice with\nrecommendations for operationalizing Complexity OoD across the stack:\nincorporating complexity into benchmark and evaluation metric design,\nrethinking supervision to target solution traces, seeking and designing\ninductive biases for Complexity OoD generalization, addressing learning to\nreason spillovers such as spurious shortcuts, semantic robustness, catastrophic\nforgetting, and step wise calibration. Because Complexity OoD cannot be solved\nby scaling data alone, progress toward robust reasoning will require\narchitectures and training regimes that explicitly model and allocate\ncomputation with respect to complexity."}
{"id": "2510.07176", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.07176", "abs": "https://arxiv.org/abs/2510.07176", "authors": ["Yixiang Zhang", "Xinhao Deng", "Zhongyi Gu", "Yihao Chen", "Ke Xu", "Qi Li", "Jianping Wu"], "title": "Exposing LLM User Privacy via Traffic Fingerprint Analysis: A Study of Privacy Risks in LLM Agent Interactions", "comment": "26 pages with 11 figures", "summary": "Large Language Models (LLMs) are increasingly deployed as agents that\norchestrate tasks and integrate external tools to execute complex workflows. We\ndemonstrate that these interactive behaviors leave distinctive fingerprints in\nencrypted traffic exchanged between users and LLM agents. By analyzing traffic\npatterns associated with agent workflows and tool invocations, adversaries can\ninfer agent activities, distinguish specific agents, and even profile sensitive\nuser attributes. To highlight this risk, we develop AgentPrint, which achieves\nan F1-score of 0.866 in agent identification and attains 73.9% and 69.1% top-3\naccuracy in user attribute inference for simulated- and real-user settings,\nrespectively. These results uncover an overlooked risk: the very interactivity\nthat empowers LLM agents also exposes user privacy, underscoring the urgent\nneed for technical countermeasures alongside regulatory and policy safeguards."}
{"id": "2510.06288", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06288", "abs": "https://arxiv.org/abs/2510.06288", "authors": ["Raj Ghugare", "Catherine Ji", "Kathryn Wantlin", "Jin Schofield", "Benjamin Eysenbach"], "title": "BuilderBench -- A benchmark for generalist agents", "comment": "Project page: https://rajghugare19.github.io/builderbench and Code:\n  https://github.com/rajghugare19/builderbench", "summary": "Today's AI models learn primarily through mimicry and sharpening, so it is\nnot surprising that they struggle to solve problems beyond the limits set by\nexisting data. To solve novel problems, agents should acquire skills for\nexploring and learning through experience. Finding a scalable learning\nmechanism for developing agents that learn through interaction remains a major\nopen problem. In this work, we introduce BuilderBench, a benchmark to\naccelerate research into agent pre-training that centers open-ended\nexploration. BuilderBench requires agents to learn how to build any structure\nusing blocks. BuilderBench is equipped with $(1)$ a hardware accelerated\nsimulator of a robotic agent interacting with various physical blocks, and\n$(2)$ a task-suite with over 42 diverse target structures that are carefully\ncurated to test an understanding of physics, mathematics, and long-horizon\nplanning. During training, agents have to explore and learn general principles\nabout the environment without any external supervision. During evaluation,\nagents have to build the unseen target structures from the task suite. Solving\nthese tasks requires a sort of \\emph{embodied reasoning} that is not reflected\nin words but rather in actions, experimenting with different strategies and\npiecing them together. Our experiments show that many of these tasks challenge\nthe current iteration of algorithms. Hence, we also provide a ``training\nwheels'' protocol, in which agents are trained and evaluated to build a single\ntarget structure from the task suite. Finally, we provide single-file\nimplementations of six different algorithms as a reference point for\nresearchers."}
{"id": "2510.07219", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.07219", "abs": "https://arxiv.org/abs/2510.07219", "authors": ["Yuhua Xu", "Wei Sun", "Chengpei Tang", "Jiaxing Lu", "Jingying Zhou", "Chen Gu"], "title": "Security-Robustness Trade-offs in Diffusion Steganography: A Comparative Analysis of Pixel-Space and VAE-Based Architectures", "comment": "13 pages", "summary": "Current generative steganography research mainly pursues computationally\nexpensive mappings to perfect Gaussian priors within single diffusion model\narchitectures. This work introduces an efficient framework based on approximate\nGaussian mapping governed by a scale factor calibrated through capacity-aware\nadaptive optimization. Using this framework as a unified analytical tool,\nsystematic comparative analysis of steganography in pixel-space models versus\nVAE-based latent-space systems is conducted. The investigation reveals a\npronounced architecture dependent security-robustness trade-off: pixel-space\nmodels achieve high security against steganalysis but exhibit fragility to\nchannel distortions, while VAE-based systems like Stable Diffusion offer\nsubstantial robustness at the cost of security vulnerabilities. Further\nanalysis indicates that the VAE component drives this behavior through opposing\nmechanisms where the encoder confers robustness via manifold regularization\nwhile the decoder introduces vulnerabilities by amplifying latent perturbations\ninto detectable artifacts. These findings characterize the conflicting\narchitectural roles in generative steganography and establish a foundation for\nfuture research."}
{"id": "2510.06302", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06302", "abs": "https://arxiv.org/abs/2510.06302", "authors": ["Ksenija Lace", "Marite Kirikova"], "title": "Requirements for Game-Based Learning Design Framework for Information System Integration in the Context of Post-Merger Integration", "comment": null, "summary": "Post-merger integration states unique challenges for professionals\nresponsible for information system integration aimed on alignment and\ncombination diverse system architectures of merging organizations. Although the\ntheoretical and practical guidance exists for post-merger integration on the\nbusiness level, there is a significant gap in training for information system\nintegration in this context. In prior research specific methods AMILI (Support\nmethod for informed decision identification) and AMILP (Support method for\ninformed decision-making) were introduced for the support of information system\nintegration decisions in the post-merger integration. But during the practical\napplication was reported high learning curve and low learner motivation. This\npaper explores how game-based learning design can address these limitations by\ntransforming static method training into engaging learning experience. The\nstudy analyzes foundational learning theories, cognitive load and motivation\nmodels, and serious game design frameworks to identify the essential\nrequirements for a game-based learning design framework tailored to information\nsystem integration in post-merger integration. Requirements are structured in\ntwo components: the transformation process and resulting learning experience.\nThe paper concludes with a plan for developing and evaluating the proposed\nframework through iterative design and real-world validation."}
{"id": "2510.06343", "categories": ["cs.SE", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.06343", "abs": "https://arxiv.org/abs/2510.06343", "authors": ["Fikret Mert GÃ¼ltekin", "Oscar Lilja", "Ranim Khojah", "Rebekka Wohlrab", "Marvin Damschen", "Mazen Mohamad"], "title": "Leveraging Large Language Models for Cybersecurity Risk Assessment -- A Case from Forestry Cyber-Physical Systems", "comment": "Accepted at Autonomous Agents in Software Engineering (AgenticSE)\n  Workshop, co-located with ASE 2025", "summary": "In safety-critical software systems, cybersecurity activities become\nessential, with risk assessment being one of the most critical. In many\nsoftware teams, cybersecurity experts are either entirely absent or represented\nby only a small number of specialists. As a result, the workload for these\nexperts becomes high, and software engineers would need to conduct\ncybersecurity activities themselves. This creates a need for a tool to support\ncybersecurity experts and engineers in evaluating vulnerabilities and threats\nduring the risk assessment process. This paper explores the potential of\nleveraging locally hosted large language models (LLMs) with retrieval-augmented\ngeneration to support cybersecurity risk assessment in the forestry domain\nwhile complying with data protection and privacy requirements that limit\nexternal data sharing. We performed a design science study involving 12 experts\nin interviews, interactive sessions, and a survey within a large-scale project.\nThe results demonstrate that LLMs can assist cybersecurity experts by\ngenerating initial risk assessments, identifying threats, and providing\nredundancy checks. The results also highlight the necessity for human oversight\nto ensure accuracy and compliance. Despite trust concerns, experts were willing\nto utilize LLMs in specific evaluation and assistance roles, rather than solely\nrelying on their generative capabilities. This study provides insights that\nencourage the use of LLM-based agents to support the risk assessment process of\ncyber-physical systems in safety-critical domains."}
{"id": "2510.06307", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06307", "abs": "https://arxiv.org/abs/2510.06307", "authors": ["Wentao Deng", "Jiahuan Pei", "Zhiwei Xu", "Zhaochun Ren", "Zhumin Chen", "Pengjie Ren"], "title": "Belief-Calibrated Multi-Agent Consensus Seeking for Complex NLP Tasks", "comment": "This paper has been accepted by NeurIPS 2025", "summary": "A multi-agent system (MAS) enhances its capacity to solve complex natural\nlanguage processing (NLP) tasks through collaboration among multiple agents,\nwhere consensus-seeking serves as a fundamental mechanism. However, existing\nconsensus-seeking approaches typically rely on voting mechanisms to judge\nconsensus, overlooking contradictions in system-internal beliefs that\ndestabilize the consensus. Moreover, these methods often involve agents\nupdating their results through indiscriminate collaboration with every other\nagent. Such uniform interaction fails to identify the optimal collaborators for\neach agent, hindering the emergence of a stable consensus. To address these\nchallenges, we provide a theoretical framework for selecting optimal\ncollaborators that maximize consensus stability. Based on the theorems, we\npropose the Belief-Calibrated Consensus Seeking (BCCS) framework to facilitate\nstable consensus via selecting optimal collaborators and calibrating the\nconsensus judgment by system-internal beliefs. Experimental results on the MATH\nand MMLU benchmark datasets demonstrate that the proposed BCCS framework\noutperforms the best existing results by 2.23% and 3.95% of accuracy on\nchallenging tasks, respectively. Our code and data are available at\nhttps://github.com/dengwentao99/BCCS."}
{"id": "2510.06410", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06410", "abs": "https://arxiv.org/abs/2510.06410", "authors": ["Aochong Oliver Li", "Tanya Goyal"], "title": "Off-Trajectory Reasoning: Can LLMs Collaborate on Reasoning Trajectory?", "comment": null, "summary": "Reasoning LLMs are trained to verbalize their reasoning process, yielding\nstrong gains on complex tasks. This transparency also opens a promising\ndirection: multiple reasoners can directly collaborate on each other's thinking\nwithin a shared trajectory, yielding better inference efficiency and\nexploration. A key prerequisite, however, is the ability to assess the\nusefulness and build on another model's partial thinking -- we call this\noff-trajectory reasoning. Our paper investigates a critical question: can\nstandard solo-reasoning training pipelines deliver desired off-trajectory\nbehaviors? We propose twin tests that capture the two extremes of the\noff-trajectory spectrum, namely Recoverability, which tests whether LLMs can\nbacktrack from \"distractions\" induced by misleading reasoning traces, and\nGuidability, which tests their ability to build upon correct reasoning from\nstronger collaborators. Our study evaluates 15 open-weight LLMs (1.5B-32B) and\nreveals a counterintuitive finding -- \"stronger\" LLMs on benchmarks are often\nmore fragile under distraction. Moreover, all models tested fail to effectively\nleverage guiding steps from collaborators on problems beyond their inherent\ncapabilities with solve rates remaining under 9.2%. Finally, we conduct control\nstudies to isolate the effects of three factors in post-training on these\nbehaviors: the choice of distillation teacher, the use of RL, and data\nselection strategy. Our results provide actionable insights for training\nnatively strong reasoning collaborators; e.g., we find that suboptimal\nrecoverability behaviors of teacher models are transferred to distilled\nstudents even if the distillation trajectories are correct. Taken together,\nthis work lays the groundwork for evaluating multi-model collaborations in\nshared reasoning trajectories and highlights the limitations of off-the-shelf\nreasoning LLMs."}
{"id": "2510.06433", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06433", "abs": "https://arxiv.org/abs/2510.06433", "authors": ["Aryan Singh Dalal", "Yinglun Zhang", "Duru DoÄan", "Atalay Mert Ä°leri", "Hande KÃ¼Ã§Ã¼k McGinty"], "title": "Flavonoid Fusion: Creating a Knowledge Graph to Unveil the Interplay Between Food and Health", "comment": null, "summary": "The focus on \"food as medicine\" is gaining traction in the field of health\nand several studies conducted in the past few years discussed this aspect of\nfood in the literature. However, very little research has been done on\nrepresenting the relationship between food and health in a standardized,\nmachine-readable format using a semantic web that can help us leverage this\nknowledge effectively. To address this gap, this study aims to create a\nknowledge graph to link food and health through the knowledge graph's ability\nto combine information from various platforms focusing on flavonoid contents of\nfood found in the USDA databases and cancer connections found in the\nliterature. We looked closely at these relationships using KNARM methodology\nand represented them in machine-operable format. The proposed knowledge graph\nserves as an example for researchers, enabling them to explore the complex\ninterplay between dietary choices and disease management. Future work for this\nstudy involves expanding the scope of the knowledge graph by capturing nuances,\nadding more related data, and performing inferences on the acquired knowledge\nto uncover hidden relationships."}
{"id": "2510.06475", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06475", "abs": "https://arxiv.org/abs/2510.06475", "authors": ["Yitao Long", "Yuru Jiang", "Hongjun Liu", "Yilun Zhao", "Jingchen Sun", "Yiqiu Shen", "Chen Zhao", "Arman Cohan", "Dennis Shasha"], "title": "PuzzlePlex: Benchmarking Foundation Models on Reasoning and Planning with Puzzles", "comment": null, "summary": "This work investigates the reasoning and planning capabilities of foundation\nmodels and their scalability in complex, dynamic environments. We introduce\nPuzzlePlex, a benchmark designed to assess these capabilities through a diverse\nset of puzzles. PuzzlePlex consists of 15 types of puzzles, including\ndeterministic and stochastic games of varying difficulty, as well as\nsingle-player and two-player scenarios. The PuzzlePlex framework provides a\ncomprehensive environment for each game, and supports extensibility to generate\nmore challenging instances as foundation models evolve. Additionally, we\nimplement customized game-playing strategies for comparison. Building on this\nbenchmark, we develop fine-grained metrics to measure performance and conduct\nan in-depth analysis of frontier foundation models across two settings:\ninstruction-based and code-based. Furthermore, we systematically investigate\ntheir scaling limits. Our findings show that reasoning models outperform others\nin instruction-based settings, while code-based execution presents greater\nchallenges but offers a scalable and efficient alternative. PuzzlePlex enables\ntargeted evaluation and guides future improvements in reasoning, planning, and\ngeneralization for foundation models."}
{"id": "2510.06534", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06534", "abs": "https://arxiv.org/abs/2510.06534", "authors": ["Jiahe Jin", "Abhijay Paladugu", "Chenyan Xiong"], "title": "Beneficial Reasoning Behaviors in Agentic Search and Effective Post-training to Obtain Them", "comment": null, "summary": "Agentic search leverages large language models (LLMs) to interpret complex\nuser information needs and execute a multi-step process of planning, searching,\nand synthesizing information to provide answers. This paradigm introduces\nunique challenges for LLMs' reasoning and agentic capabilities when interacting\nwith retrieval systems and the broader web. In this paper, we propose a\nreasoning-driven LLM-based pipeline to study effective reasoning behavior\npatterns in agentic search. Using this pipeline, we analyze successful agentic\nsearch trajectories and identify four beneficial reasoning behaviors:\nInformation Verification, Authority Evaluation, Adaptive Search, and Error\nRecovery. Based on these findings, we propose a technique called Behavior\nPriming to train more effective agentic search models. It synthesizes agentic\nsearch trajectories that exhibit these four behaviors and integrates them into\nthe agentic search model through supervised fine-tuning (SFT), followed by\nstandard reinforcement learning (RL). Experiments on three benchmarks (GAIA,\nWebWalker, and HLE) demonstrate that behavior priming yields over 35% gains in\nLlama3.2-3B and Qwen3-1.7B compared to directly training agentic search models\nwith RL. Crucially, we demonstrate that the desired reasoning behaviors in the\nSFT data, rather than the correctness of the final answer, is the critical\nfactor for achieving strong final performance after RL: fine-tuning on\ntrajectories with desirable reasoning behaviors but incorrect answers leads to\nbetter performance than fine-tuning on trajectories with correct answers. Our\nanalysis further reveals the underlying mechanism: the introduced reasoning\nbehaviors endow models with more effective exploration (higher pass@k and\nentropy) and test-time scaling (longer trajectories) capabilities, providing a\nstrong foundation for RL. Our code will be released as open source."}
{"id": "2510.06538", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06538", "abs": "https://arxiv.org/abs/2510.06538", "authors": ["Jiajie Li", "Huayi Zhang", "Peng Lin", "Jinjun Xiong", "Wei Xu"], "title": "Auto-Prompt Ensemble for LLM Judge", "comment": null, "summary": "We present a novel framework that improves the reliability of LLM judges by\nselectively augmenting LLM with auxiliary evaluation dimensions. Existing LLM\njudges often miss crucial evaluation dimensions because they fail to recognize\nthe implicit standards underlying human assessments. To address this challenge,\nwe propose the Auto-Prompt Ensemble (APE), an adaptive framework that\nautomatically learns evaluation dimensions from its failure cases. APE\nincorporates a confidence-based ensemble mechanism to decide when to adopt the\njudgments from additional evaluation dimensions through a novel confidence\nestimation approach called Collective Confidence. Extensive experiments\ndemonstrate that APE improves the reliability of LLM Judge across diverse\nstandard benchmarks. For instance, APE enhances GPT-4o agreement rate on Reward\nBench from 87.2% to 90.5% in the zero-shot setting. Overall, APE provides a\nprincipled approach for LLM Judge to leverage test-time computation, and bridge\nthe evaluation gap between human and LLM judges."}
{"id": "2510.06587", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06587", "abs": "https://arxiv.org/abs/2510.06587", "authors": ["Jingbo Yang", "Bairu Hou", "Wei Wei", "Shiyu Chang", "Yujia Bao"], "title": "WebDART: Dynamic Decomposition and Re-planning for Complex Web Tasks", "comment": null, "summary": "Large language model (LLM) agents are becoming competent at straightforward\nweb tasks, such as opening an item page or submitting a form, but still\nstruggle with objectives that require long horizon navigation, large scale\ninformation extraction, and reasoning under constraints. We present WebDART, a\ngeneral framework that enables a single LLM to handle such complex chores.\nWebDART (i) dynamically decomposes each objective into three focused subtasks:\nnavigation, information extraction, and execution, so the model concentrates on\none skill at a time, and (ii) continuously replans the decomposition as new\nwebpages are revealed, taking advantage of newly discovered filters or\nshortcuts and avoiding redundant exploration. Evaluated on WebChoreArena,\nWebDART lifts success rates by up to 13.7 percentage points over previous SOTA\nagents, while matching their performance on the easier WebArena suite and\ncompleting tasks with up to 14.7 fewer navigation steps."}
{"id": "2510.06600", "categories": ["cs.AI", "H.3.3; I.2.7"], "pdf": "https://arxiv.org/pdf/2510.06600", "abs": "https://arxiv.org/abs/2510.06600", "authors": ["Zhaochun Ren", "Zhou Yang", "Chenglong Ye", "Haizhou Sun", "Chao Chen", "Xiaofei Zhu", "Xiangwen Liao"], "title": "Fine-Grained Emotion Recognition via In-Context Learning", "comment": "9 pages, 10 figures, 4 tables", "summary": "Fine-grained emotion recognition aims to identify the emotional type in\nqueries through reasoning and decision-making processes, playing a crucial role\nin various systems. Recent methods use In-Context Learning (ICL), enhancing the\nrepresentation of queries in the reasoning process through semantically similar\nexamples, while further improving emotion recognition by explaining the\nreasoning mechanisms. However, these methods enhance the reasoning process but\noverlook the decision-making process. This paper investigates decision-making\nin fine-grained emotion recognition through prototype theory. We show that ICL\nrelies on similarity matching between query representations and emotional\nprototypes within the model, where emotion-accurate representations are\ncritical. However, semantically similar examples often introduce emotional\ndiscrepancies, hindering accurate representations and causing errors. To\naddress this, we propose Emotion In-Context Learning (EICL), which introduces\nemotionally similar examples and uses a dynamic soft-label strategy to improve\nquery representations in the emotion reasoning process. A two-stage exclusion\nstrategy is then employed to assess similarity from multiple angles, further\noptimizing the decision-making process. Extensive experiments show that EICL\nsignificantly outperforms ICL on multiple datasets."}
{"id": "2510.06674", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06674", "abs": "https://arxiv.org/abs/2510.06674", "authors": ["Cen", "Zhao", "Tiantian Zhang", "Hanchen Su", "Yufeng", "Zhang", "Shaowei Su", "Mingzhi Xu", "Yu", "Liu", "Wei Han", "Jeremy Werner", "Claire Na Cheng", "Yashar Mehdad"], "title": "Agent-in-the-Loop: A Data Flywheel for Continuous Improvement in LLM-based Customer Support", "comment": "EMNLP 2025 Industry Track submission (Paper #305). Preprint. Main\n  text within the 7-page industry limit (references/appendices excluded).\n  Contains multiple figures and tables", "summary": "We introduce an Agent-in-the-Loop (AITL) framework that implements a\ncontinuous data flywheel for iteratively improving an LLM-based customer\nsupport system. Unlike standard offline approaches that rely on batch\nannotations, AITL integrates four key types of annotations directly into live\ncustomer operations: (1) pairwise response preferences, (2) agent adoption and\nrationales, (3) knowledge relevance checks, and (4) identification of missing\nknowledge. These feedback signals seamlessly feed back into models' updates,\nreducing retraining cycles from months to weeks. Our production pilot involving\nUS-based customer support agents demonstrated significant improvements in\nretrieval accuracy (+11.7% recall@75, +14.8% precision@8), generation quality\n(+8.4% helpfulness) and agent adoption rates (+4.5%). These results underscore\nthe effectiveness of embedding human feedback loops directly into operational\nworkflows to continuously refine LLM-based customer support system."}
{"id": "2510.06711", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06711", "abs": "https://arxiv.org/abs/2510.06711", "authors": ["Batu El", "Mert Yuksekgonul", "James Zou"], "title": "Inefficiencies of Meta Agents for Agent Design", "comment": null, "summary": "Recent works began to automate the design of agentic systems using\nmeta-agents that propose and iteratively refine new agent architectures. In\nthis paper, we examine three key challenges in a common class of meta-agents.\nFirst, we investigate how a meta-agent learns across iterations and find that\nsimply expanding the context with all previous agents, as proposed by previous\nworks, performs worse than ignoring prior designs entirely. We show that the\nperformance improves with an evolutionary approach. Second, although the\nmeta-agent designs multiple agents during training, it typically commits to a\nsingle agent at test time. We find that the designed agents have low behavioral\ndiversity, limiting the potential for their complementary use. Third, we assess\nwhen automated design is economically viable. We find that only in a few\ncases--specifically, two datasets--the overall cost of designing and deploying\nthe agents is lower than that of human-designed agents when deployed on over\n15,000 examples. In contrast, the performance gains for other datasets do not\njustify the design cost, regardless of scale."}
{"id": "2510.06742", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06742", "abs": "https://arxiv.org/abs/2510.06742", "authors": ["Ali Sarabadani", "Kheirolah Rahsepar Fard"], "title": "MultiCNKG: Integrating Cognitive Neuroscience, Gene, and Disease Knowledge Graphs Using Large Language Models", "comment": null, "summary": "The advent of large language models (LLMs) has revolutionized the integration\nof knowledge graphs (KGs) in biomedical and cognitive sciences, overcoming\nlimitations in traditional machine learning methods for capturing intricate\nsemantic links among genes, diseases, and cognitive processes. We introduce\nMultiCNKG, an innovative framework that merges three key knowledge sources: the\nCognitive Neuroscience Knowledge Graph (CNKG) with 2.9K nodes and 4.3K edges\nacross 9 node types and 20 edge types; Gene Ontology (GO) featuring 43K nodes\nand 75K edges in 3 node types and 4 edge types; and Disease Ontology (DO)\ncomprising 11.2K nodes and 8.8K edges with 1 node type and 2 edge types.\nLeveraging LLMs like GPT-4, we conduct entity alignment, semantic similarity\ncomputation, and graph augmentation to create a cohesive KG that interconnects\ngenetic mechanisms, neurological disorders, and cognitive functions. The\nresulting MultiCNKG encompasses 6.9K nodes across 5 types (e.g., Genes,\nDiseases, Cognitive Processes) and 11.3K edges spanning 7 types (e.g., Causes,\nAssociated with, Regulates), facilitating a multi-layered view from molecular\nto behavioral domains. Assessments using metrics such as precision (85.20%),\nrecall (87.30%), coverage (92.18%), graph consistency (82.50%), novelty\ndetection (40.28%), and expert validation (89.50%) affirm its robustness and\ncoherence. Link prediction evaluations with models like TransE (MR: 391, MRR:\n0.411) and RotatE (MR: 263, MRR: 0.395) show competitive performance against\nbenchmarks like FB15k-237 and WN18RR. This KG advances applications in\npersonalized medicine, cognitive disorder diagnostics, and hypothesis\nformulation in cognitive neuroscience."}
{"id": "2510.06756", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06756", "abs": "https://arxiv.org/abs/2510.06756", "authors": ["Dennis Gross", "Helge Spieker", "Arnaud Gotlieb"], "title": "Verifying Memoryless Sequential Decision-making of Large Language Models", "comment": null, "summary": "We introduce a tool for rigorous and automated verification of large language\nmodel (LLM)- based policies in memoryless sequential decision-making tasks.\nGiven a Markov decision process (MDP) representing the sequential\ndecision-making task, an LLM policy, and a safety requirement expressed as a\nPCTL formula, our approach incrementally constructs only the reachable portion\nof the MDP guided by the LLM's chosen actions. Each state is encoded as a\nnatural language prompt, the LLM's response is parsed into an action, and\nreachable successor states by the policy are expanded. The resulting formal\nmodel is checked with Storm to determine whether the policy satisfies the\nspecified safety property. In experiments on standard grid world benchmarks, we\nshow that open source LLMs accessed via Ollama can be verified when\ndeterministically seeded, but generally underperform deep reinforcement\nlearning baselines. Our tool natively integrates with Ollama and supports\nPRISM-specified tasks, enabling continuous benchmarking in user-specified\nsequential decision-making tasks and laying a practical foundation for formally\nverifying increasingly capable LLMs."}
{"id": "2510.06761", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06761", "abs": "https://arxiv.org/abs/2510.06761", "authors": ["Zhi Zhang", "Yan Liu", "Zhejing Hu", "Gong Chen", "Sheng-hua Zhong", "Jiannong Cao"], "title": "Evolving and Executing Research Plans via Double-Loop Multi-Agent Collaboration", "comment": null, "summary": "Automating the end-to-end scientific research process poses a fundamental\nchallenge: it requires both evolving high-level plans that are novel and sound,\nand executing these plans correctly amidst dynamic and uncertain conditions. To\naddress this bilevel challenge, we propose a novel Double-Loop Multi-Agent\n(DLMA) framework to solve the given research problem automatically. The leader\nloop, composed of professor agents, is responsible for evolving research plans.\nIt employs an evolutionary algorithm through involvement, improvement, and\nintegration meetings to iteratively generate and refine a pool of research\nproposals, exploring the solution space effectively. The follower loop,\ncomposed of doctoral student agents, is responsible for executing the\nbest-evolved plan. It dynamically adjusts the plan during implementation via\npre-hoc and post-hoc meetings, ensuring each step (e.g., drafting, coding) is\nwell-supported by contextual and external observations. Extensive experiments\non benchmarks like ACLAward and Laboratory show that DLMA generates research\npapers that achieve state-of-the-art scores in automated evaluation,\nsignificantly outperforming strong baselines. Ablation studies confirm the\ncritical roles of both loops, with evolution driving novelty and execution\nensuring soundness."}
{"id": "2510.06857", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06857", "abs": "https://arxiv.org/abs/2510.06857", "authors": ["Qi Guo", "Jianing Wang", "Jianfei Zhang", "Deyang Kong", "Xiangzhou Huang", "Xiangyu Xi", "Wei Wang", "Jingang Wang", "Xunliang Cai", "Shikun Zhang", "Wei Ye"], "title": "Autoformalizer with Tool Feedback", "comment": null, "summary": "Autoformalization addresses the scarcity of data for Automated Theorem\nProving (ATP) by translating mathematical problems from natural language into\nformal statements. Efforts in recent work shift from directly prompting large\nlanguage models to training an end-to-end formalizer model from scratch,\nachieving remarkable advancements. However, existing formalizer still struggles\nto consistently generate valid statements that meet syntactic validity and\nsemantic consistency. To address this issue, we propose the Autoformalizer with\nTool Feedback (ATF), a novel approach that incorporates syntactic and\nconsistency information as tools into the formalization process. By integrating\nLean 4 compilers for syntax corrections and employing a multi-LLMs-as-judge\napproach for consistency validation, the model is able to adaptively refine\ngenerated statements according to the tool feedback, enhancing both syntactic\nvalidity and semantic consistency. The training of ATF involves a cold-start\nphase on synthetic tool-calling data, an expert iteration phase to improve\nformalization capabilities, and Direct Preference Optimization to alleviate\nineffective revisions. Experimental results show that ATF markedly outperforms\na range of baseline formalizer models, with its superior performance further\nvalidated by human evaluations. Subsequent analysis reveals that ATF\ndemonstrates excellent inference scaling properties. Moreover, we open-source\nNumina-ATF, a dataset containing 750K synthetic formal statements to facilitate\nadvancements in autoformalization and ATP research."}
{"id": "2510.06878", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06878", "abs": "https://arxiv.org/abs/2510.06878", "authors": ["Daria Ozerova", "Ekaterina Trofimova"], "title": "TGPR: Tree-Guided Policy Refinement for Robust Self-Debugging of LLMs", "comment": null, "summary": "Iterative refinement has been a promising paradigm to enable large language\nmodels (LLMs) to resolve difficult reasoning and problem-solving tasks. One of\nthe key challenges, however, is how to effectively search through the enormous\nsearch space of possible refinements. Existing methods typically fall back on\npredefined heuristics, which are troubled by the exploration-exploitation\ndilemma and cannot adapt based on past refinement outcomes. We introduce\nTree-Guided Policy Refinement (TGPR), a novel framework that combines GRPO with\na Thompson-Sampling-based tree search. TGPR explores both failed and successful\nrefinement paths actively, with denser training trajectories and more adaptive\npolicies. On HumanEval, MBPP, and APPS benchmarks, our method achieves up to\n+4.2 percentage points absolute improvement in pass@1 (on MBPP) and up to\n+12.51 percentage points absolute improvement in pass@10 (on APPS) compared to\na competitive GRPO baseline. Apart from debugging code, TGPR focuses on a\nprincipled approach to combining learned policies with structured search\nmethods, offering a general framework for enhancing iterative refinement and\nstateful reasoning in LLMs."}
{"id": "2510.06911", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06911", "abs": "https://arxiv.org/abs/2510.06911", "authors": ["Hacane Hechehouche", "Andre Antakli", "Matthias Klusch"], "title": "LLM-Assisted Modeling of Semantic Web-Enabled Multi-Agents Systems with AJAN", "comment": null, "summary": "There are many established semantic Web standards for implementing\nmulti-agent driven applications. The AJAN framework allows to engineer\nmulti-agent systems based on these standards. In particular, agent knowledge is\nrepresented in RDF/RDFS and OWL, while agent behavior models are defined with\nBehavior Trees and SPARQL to access and manipulate this knowledge. However, the\nappropriate definition of RDF/RDFS and SPARQL-based agent behaviors still\nremains a major hurdle not only for agent modelers in practice. For example,\ndealing with URIs is very error-prone regarding typos and dealing with complex\nSPARQL queries in large-scale environments requires a high learning curve. In\nthis paper, we present an integrated development environment to overcome such\nhurdles of modeling AJAN agents and at the same time to extend the user\ncommunity for AJAN by the possibility to leverage Large Language Models for\nagent engineering."}
{"id": "2510.06953", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06953", "abs": "https://arxiv.org/abs/2510.06953", "authors": ["Minju Gwak", "Guijin Son", "Jaehyung Kim"], "title": "Revisiting the Uniform Information Density Hypothesis in LLM Reasoning Traces", "comment": null, "summary": "The Uniform Information Density (UID) hypothesis suggests that effective\ncommunication maintains a stable flow of information. In this work, we revisit\nthis principle in the context of large language model (LLM) reasoning traces,\nasking whether step-level uniformity reflects reasoning quality. To this end,\nwe propose an entropy-based stepwise information density metric and introduce\ntwo complementary measures of uniformity, local and global uniformity scores.\nAcross the experiments on six different reasoning benchmarks, we find that\nstep-level uniformity not only provides a strong theoretical lens but also\nyields practical performance benefits; for example, selecting reasoning traces\nwith more uniform information density at the step-level improves accuracy by\n10-32\\% relative gains over baselines at AIME2025. Our analysis further reveals\nthat correct reasoning traces tend to avoid sharp information density spikes,\nwhile incorrect traces exhibit irregular information bursts. These results\ndemonstrate that UID-inspired information density measures outperform\nalternative internal signals as predictors of reasoning quality. Results\nhighlight the uniformity of the information density as a robust diagnostic and\nselection criterion for building more reliable and accurate reasoning systems."}
{"id": "2510.07038", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07038", "abs": "https://arxiv.org/abs/2510.07038", "authors": ["Wenxun Wu", "Yuanyang Li", "Guhan Chen", "Linyue Wang", "Hongyang Chen"], "title": "Tool-Augmented Policy Optimization: Synergizing Reasoning and Adaptive Tool Use with Reinforcement Learning", "comment": null, "summary": "Recent advances in large language models (LLMs) have popularized test-time\nscaling, where models generate additional reasoning tokens before producing\nfinal answers. These approaches have demonstrated significant performance\nimprovements on benchmarks involving mathematical reasoning. However, language\nmodels relying solely on direct inference still struggle with tasks demanding\nup-to-date knowledge or computational tools such as calculators and code\ninterpreters for complex arithmetic operations. To overcome these limitations,\nwe propose Tool-Augmented Policy Optimization (TAPO), a novel reinforcement\nlearning framework that systematically integrates multi-hop reasoning with\nadaptive tool-calling capabilities. Our approach employs a modified version of\nDynamic Sampling Policy Optimization (DAPO), a recently developed RL paradigm,\nwhich we adapt specifically for tool invocation scenarios, enabling models to\ndynamically interleave complex reasoning with on-demand tool usage (including\nsearch APIs and Python interpreters).\n  To support this research, we introduce two new datasets: TAPO-easy-60K and\nTAPO-hard-18K, specifically designed to train and evaluate both fact-based\nreasoning and mathematical calculation capabilities. Our experiments on\nQwen2.5-3B and Qwen2.5-7B models demonstrate the effectiveness of our approach,\nwith both models achieving state-of-the-art performance on tasks requiring\nexternal knowledge and mathematical computation among methods with comparable\nparameters. Notably, TAPO achieves more efficient tool utilization than\nbaseline methods while preventing excessive calls caused by reward hacking.\nThese results highlight the significant potential of combining advanced\nreasoning with tool usage to enhance model performance in knowledge-intensive\nand computationally demanding tasks."}
{"id": "2510.07064", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07064", "abs": "https://arxiv.org/abs/2510.07064", "authors": ["Manh Hung Nguyen", "Sebastian Tschiatschek", "Adish Singla"], "title": "Prompt Optimization Across Multiple Agents for Representing Diverse Human Populations", "comment": null, "summary": "The difficulty and expense of obtaining large-scale human responses make\nLarge Language Models (LLMs) an attractive alternative and a promising proxy\nfor human behavior. However, prior work shows that LLMs often produce\nhomogeneous outputs that fail to capture the rich diversity of human\nperspectives and behaviors. Thus, rather than trying to capture this diversity\nwith a single LLM agent, we propose a novel framework to construct a set of\nagents that collectively capture the diversity of a given human population.\nEach agent is an LLM whose behavior is steered by conditioning on a small set\nof human demonstrations (task-response pairs) through in-context learning. The\ncentral challenge is therefore to select a representative set of LLM agents\nfrom the exponentially large space of possible agents. We tackle this selection\nproblem from the lens of submodular optimization. In particular, we develop\nmethods that offer different trade-offs regarding time complexity and\nperformance guarantees. Extensive experiments in crowdsourcing and educational\ndomains demonstrate that our approach constructs agents that more effectively\nrepresent human populations compared to baselines. Moreover, behavioral\nanalyses on new tasks show that these agents reproduce the behavior patterns\nand perspectives of the students and annotators they are designed to represent."}
{"id": "2510.07069", "categories": ["cs.AI", "I.2.4"], "pdf": "https://arxiv.org/pdf/2510.07069", "abs": "https://arxiv.org/abs/2510.07069", "authors": ["Hongbo Hu", "Yisong Wang", "Yi Huang", "Kewen Wang"], "title": "Inductive Learning for Possibilistic Logic Programs Under Stable Models", "comment": "Under consideration in Theory and Practice of Logic Programming\n  (TPLP)", "summary": "Possibilistic logic programs (poss-programs) under stable models are a major\nvariant of answer set programming (ASP). While its semantics (possibilistic\nstable models) and properties have been well investigated, the problem of\ninductive reasoning has not been investigated yet. This paper presents an\napproach to extracting poss-programs from a background program and examples\n(parts of intended possibilistic stable models). To this end, the notion of\ninduction tasks is first formally defined, its properties are investigated and\ntwo algorithms ilpsm and ilpsmmin for computing induction solutions are\npresented. An implementation of ilpsmmin is also provided and experimental\nresults show that when inputs are ordinary logic programs, the prototype\noutperforms a major inductive learning system for normal logic programs from\nstable models on the datasets that are randomly generated."}
{"id": "2510.07073", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07073", "abs": "https://arxiv.org/abs/2510.07073", "authors": ["AndrÃ© Hottung", "Federico Berto", "Chuanbo Hua", "Nayeli Gast Zepeda", "Daniel Wetzel", "Michael RÃ¶mer", "Haoran Ye", "Davide Zago", "Michael Poli", "Stefano Massaroli", "Jinkyoo Park", "Kevin Tierney"], "title": "VRPAgent: LLM-Driven Discovery of Heuristic Operators for Vehicle Routing Problems", "comment": null, "summary": "Designing high-performing heuristics for vehicle routing problems (VRPs) is a\ncomplex task that requires both intuition and deep domain knowledge. Large\nlanguage model (LLM)-based code generation has recently shown promise across\nmany domains, but it still falls short of producing heuristics that rival those\ncrafted by human experts. In this paper, we propose VRPAgent, a framework that\nintegrates LLM-generated components into a metaheuristic and refines them\nthrough a novel genetic search. By using the LLM to generate problem-specific\noperators, embedded within a generic metaheuristic framework, VRPAgent keeps\ntasks manageable, guarantees correctness, and still enables the discovery of\nnovel and powerful strategies. Across multiple problems, including the\ncapacitated VRP, the VRP with time windows, and the prize-collecting VRP, our\nmethod discovers heuristic operators that outperform handcrafted methods and\nrecent learning-based approaches while requiring only a single CPU core. To our\nknowledge, \\VRPAgent is the first LLM-based paradigm to advance the\nstate-of-the-art in VRPs, highlighting a promising future for automated\nheuristics discovery."}
{"id": "2510.07091", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.07091", "abs": "https://arxiv.org/abs/2510.07091", "authors": ["Baixuan Xu", "Tianshi Zheng", "Zhaowei Wang", "Hong Ting Tsang", "Weiqi Wang", "Tianqing Fang", "Yangqiu Song"], "title": "The Cognitive Bandwidth Bottleneck: Shifting Long-Horizon Agent from Planning with Actions to Planning with Schemas", "comment": "22 pages", "summary": "Enabling LLMs to effectively operate long-horizon task which requires\nlong-term planning and multiple interactions is essential for open-world\nautonomy. Conventional methods adopt planning with actions where a executable\naction list would be provided as reference. However, this action representation\nchoice would be impractical when the environment action space is combinatorial\nexploded (e.g., open-ended real world). This naturally leads to a question: As\nenvironmental action space scales, what is the optimal action representation\nfor long-horizon agents? In this paper, we systematically study the\neffectiveness of two different action representations. The first one is\nconventional planning with actions (PwA) which is predominantly adopted for its\neffectiveness on existing benchmarks. The other one is planning with schemas\n(PwS) which instantiate an action schema into action lists (e.g., \"move [OBJ]\nto [OBJ]\" -> \"move apple to desk\") to ensure concise action space and reliable\nscalability. This alternative is motivated by its alignment with human\ncognition and its compliance with environment-imposed action format\nrestriction. We propose cognitive bandwidth perspective as a conceptual\nframework to qualitatively understand the differences between these two action\nrepresentations and empirically observe a representation-choice inflection\npoint between ALFWorld (~35 actions) and SciWorld (~500 actions), which serve\nas evidence of the need for scalable representations. We further conduct\ncontrolled experiments to study how the location of this inflection point\ninteracts with different model capacities: stronger planning proficiency shifts\nthe inflection rightward, whereas better schema instantiation shifts it\nleftward. Finally, noting the suboptimal performance of PwS agents, we provide\nan actionable guide for building more capable PwS agents for better scalable\nautonomy."}
{"id": "2510.07117", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.07117", "abs": "https://arxiv.org/abs/2510.07117", "authors": ["Leonardo Christov-Moore", "Arthur Juliani", "Alex Kiefer", "Nicco Reggente", "B. Scott Rousse", "Adam Safron", "Nicol'as Hinrichs", "Daniel Polani", "Antonio Damasio"], "title": "The Contingencies of Physical Embodiment Allow for Open-Endedness and Care", "comment": "15 pages, 1 figure", "summary": "Physical vulnerability and mortality are often seen as obstacles to be\navoided in the development of artificial agents, which struggle to adapt to\nopen-ended environments and provide aligned care. Meanwhile, biological\norganisms survive, thrive, and care for each other in an open-ended physical\nworld with relative ease and efficiency. Understanding the role of the\nconditions of life in this disparity can aid in developing more robust,\nadaptive, and caring artificial agents. Here we define two minimal conditions\nfor physical embodiment inspired by the existentialist phenomenology of Martin\nHeidegger: being-in-the-world (the agent is a part of the environment) and\nbeing-towards-death (unless counteracted, the agent drifts toward terminal\nstates due to the second law of thermodynamics). We propose that from these\nconditions we can obtain both a homeostatic drive - aimed at maintaining\nintegrity and avoiding death by expending energy to learn and act - and an\nintrinsic drive to continue to do so in as many ways as possible. Drawing\ninspiration from Friedrich Nietzsche's existentialist concept of will-to-power,\nwe examine how intrinsic drives to maximize control over future states, e.g.,\nempowerment, allow agents to increase the probability that they will be able to\nmeet their future homeostatic needs, thereby enhancing their capacity to\nmaintain physical integrity. We formalize these concepts within a reinforcement\nlearning framework, which enables us to examine how intrinsically driven\nembodied agents learning in open-ended multi-agent environments may cultivate\nthe capacities for open-endedness and care.ov"}
{"id": "2510.07161", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07161", "abs": "https://arxiv.org/abs/2510.07161", "authors": ["Ali Norouzifar", "Humam Kourani", "Marcus Dees", "Wil van der Aalst"], "title": "Integrating Domain Knowledge into Process Discovery Using Large Language Models", "comment": "This paper is currently under review for publication in a journal", "summary": "Process discovery aims to derive process models from event logs, providing\ninsights into operational behavior and forming a foundation for conformance\nchecking and process improvement. However, models derived solely from event\ndata may not accurately reflect the real process, as event logs are often\nincomplete or affected by noise, and domain knowledge, an important\ncomplementary resource, is typically disregarded. As a result, the discovered\nmodels may lack reliability for downstream tasks. We propose an interactive\nframework that incorporates domain knowledge, expressed in natural language,\ninto the process discovery pipeline using Large Language Models (LLMs). Our\napproach leverages LLMs to extract declarative rules from textual descriptions\nprovided by domain experts. These rules are used to guide the IMr discovery\nalgorithm, which recursively constructs process models by combining insights\nfrom both the event log and the extracted rules, helping to avoid problematic\nprocess structures that contradict domain knowledge. The framework coordinates\ninteractions among the LLM, domain experts, and a set of backend services. We\npresent a fully implemented tool that supports this workflow and conduct an\nextensive evaluation of multiple LLMs and prompt engineering strategies. Our\nempirical study includes a case study based on a real-life event log with the\ninvolvement of domain experts, who assessed the usability and effectiveness of\nthe framework."}
{"id": "2510.07172", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07172", "abs": "https://arxiv.org/abs/2510.07172", "authors": ["Tianshi Zheng", "Kelvin Kiu-Wai Tam", "Newt Hue-Nam K. Nguyen", "Baixuan Xu", "Zhaowei Wang", "Jiayang Cheng", "Hong Ting Tsang", "Weiqi Wang", "Jiaxin Bai", "Tianqing Fang", "Yangqiu Song", "Ginny Y. Wong", "Simon See"], "title": "NewtonBench: Benchmarking Generalizable Scientific Law Discovery in LLM Agents", "comment": "60 pages, 18 figures, 13 tables", "summary": "Large language models are emerging as powerful tools for scientific law\ndiscovery, a foundational challenge in AI-driven science. However, existing\nbenchmarks for this task suffer from a fundamental methodological trilemma,\nforcing a trade-off between scientific relevance, scalability, and resistance\nto memorization. Furthermore, they oversimplify discovery as static function\nfitting, failing to capture the authentic scientific process of uncovering\nembedded laws through the interactive exploration of complex model systems. To\naddress these critical gaps, we introduce NewtonBench, a benchmark comprising\n324 scientific law discovery tasks across 12 physics domains. Our design\nmitigates the evaluation trilemma by using metaphysical shifts - systematic\nalterations of canonical laws - to generate a vast suite of problems that are\nscalable, scientifically relevant, and memorization-resistant. Moreover, we\nelevate the evaluation from static function fitting to interactive model\ndiscovery, requiring agents to experimentally probe simulated complex systems\nto uncover hidden principles. Our extensive experiment reveals a clear but\nfragile capability for discovery in frontier LLMs: this ability degrades\nprecipitously with increasing system complexity and exhibits extreme\nsensitivity to observational noise. Notably, we uncover a paradoxical effect of\ntool assistance: providing a code interpreter can hinder more capable models by\ninducing a premature shift from exploration to exploitation, causing them to\nsatisfice on suboptimal solutions. These results demonstrate that robust,\ngeneralizable discovery in complex, interactive environments remains the core\nchallenge. By providing a scalable, robust, and scientifically authentic\ntestbed, NewtonBench offers a crucial tool for measuring true progress and\nguiding the development of next-generation AI agents capable of genuine\nscientific discovery."}
{"id": "2510.07276", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.07276", "abs": "https://arxiv.org/abs/2510.07276", "authors": ["Pulkit Rustagi", "Kyle Hollins Wray", "Sandhya Saisubramanian"], "title": "Multi-Objective Multi-Agent Path Finding with Lexicographic Cost Preferences", "comment": "8 pages, 7 figures", "summary": "Many real-world scenarios require multiple agents to coordinate in shared\nenvironments, while balancing trade-offs between multiple, potentially\ncompeting objectives. Current multi-objective multi-agent path finding\n(MO-MAPF) algorithms typically produce conflict-free plans by computing Pareto\nfrontiers. They do not explicitly optimize for user-defined preferences, even\nwhen the preferences are available, and scale poorly with the number of\nobjectives. We propose a lexicographic framework for modeling MO-MAPF, along\nwith an algorithm \\textit{Lexicographic Conflict-Based Search} (LCBS) that\ndirectly computes a single solution aligned with a lexicographic preference\nover objectives. LCBS integrates a priority-aware low-level $A^*$ search with\nconflict-based search, avoiding Pareto frontier construction and enabling\nefficient planning guided by preference over objectives. We provide insights\ninto optimality and scalability, and empirically demonstrate that LCBS computes\noptimal solutions while scaling to instances with up to ten objectives -- far\nbeyond the limits of existing MO-MAPF methods. Evaluations on standard and\nrandomized MAPF benchmarks show consistently higher success rates against\nstate-of-the-art baselines, especially with increasing number of objectives."}
{"id": "2510.07297", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07297", "abs": "https://arxiv.org/abs/2510.07297", "authors": ["Henry Wang", "Md Sirajus Salekin", "Jake Lee", "Ross Claytor", "Shinan Zhang", "Michael Chi"], "title": "Agentic generative AI for media content discovery at the national football league", "comment": "13 pages, 7 figures, International Sports Analytics Conference and\n  Exhibition", "summary": "Generative AI has unlocked new possibilities in content discovery and\nmanagement. Through collaboration with the National Football League (NFL), we\ndemonstrate how a generative-AI based workflow enables media researchers and\nanalysts to query relevant historical plays using natural language rather than\ntraditional filter-and-click interfaces. The agentic workflow takes a user\nquery as input, breaks it into elements, and translates them into the\nunderlying database query language. Accuracy and latency are further improved\nthrough carefully designed semantic caching. The solution achieves over 95\npercent accuracy and reduces the average time to find relevant videos from 10\nminutes to 30 seconds, significantly increasing the NFL's operational\nefficiency and allowing users to focus on producing creative content and\nengaging storylines."}
{"id": "2510.06343", "categories": ["cs.SE", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.06343", "abs": "https://arxiv.org/abs/2510.06343", "authors": ["Fikret Mert GÃ¼ltekin", "Oscar Lilja", "Ranim Khojah", "Rebekka Wohlrab", "Marvin Damschen", "Mazen Mohamad"], "title": "Leveraging Large Language Models for Cybersecurity Risk Assessment -- A Case from Forestry Cyber-Physical Systems", "comment": "Accepted at Autonomous Agents in Software Engineering (AgenticSE)\n  Workshop, co-located with ASE 2025", "summary": "In safety-critical software systems, cybersecurity activities become\nessential, with risk assessment being one of the most critical. In many\nsoftware teams, cybersecurity experts are either entirely absent or represented\nby only a small number of specialists. As a result, the workload for these\nexperts becomes high, and software engineers would need to conduct\ncybersecurity activities themselves. This creates a need for a tool to support\ncybersecurity experts and engineers in evaluating vulnerabilities and threats\nduring the risk assessment process. This paper explores the potential of\nleveraging locally hosted large language models (LLMs) with retrieval-augmented\ngeneration to support cybersecurity risk assessment in the forestry domain\nwhile complying with data protection and privacy requirements that limit\nexternal data sharing. We performed a design science study involving 12 experts\nin interviews, interactive sessions, and a survey within a large-scale project.\nThe results demonstrate that LLMs can assist cybersecurity experts by\ngenerating initial risk assessments, identifying threats, and providing\nredundancy checks. The results also highlight the necessity for human oversight\nto ensure accuracy and compliance. Despite trust concerns, experts were willing\nto utilize LLMs in specific evaluation and assistance roles, rather than solely\nrelying on their generative capabilities. This study provides insights that\nencourage the use of LLM-based agents to support the risk assessment process of\ncyber-physical systems in safety-critical domains."}
{"id": "2510.06605", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06605", "abs": "https://arxiv.org/abs/2510.06605", "authors": ["Shuo Shao", "Yiming Li", "Hongwei Yao", "Yifei Chen", "Yuchen Yang", "Zhan Qin"], "title": "Reading Between the Lines: Towards Reliable Black-box LLM Fingerprinting via Zeroth-order Gradient Estimation", "comment": null, "summary": "The substantial investment required to develop Large Language Models (LLMs)\nmakes them valuable intellectual property, raising significant concerns about\ncopyright protection. LLM fingerprinting has emerged as a key technique to\naddress this, which aims to verify a model's origin by extracting an intrinsic,\nunique signature (a \"fingerprint\") and comparing it to that of a source model\nto identify illicit copies. However, existing black-box fingerprinting methods\noften fail to generate distinctive LLM fingerprints. This ineffectiveness\narises because black-box methods typically rely on model outputs, which lose\ncritical information about the model's unique parameters due to the usage of\nnon-linear functions. To address this, we first leverage Fisher Information\nTheory to formally demonstrate that the gradient of the model's input is a more\ninformative feature for fingerprinting than the output. Based on this insight,\nwe propose ZeroPrint, a novel method that approximates these information-rich\ngradients in a black-box setting using zeroth-order estimation. ZeroPrint\novercomes the challenge of applying this to discrete text by simulating input\nperturbations via semantic-preserving word substitutions. This operation allows\nZeroPrint to estimate the model's Jacobian matrix as a unique fingerprint.\nExperiments on the standard benchmark show ZeroPrint achieves a\nstate-of-the-art effectiveness and robustness, significantly outperforming\nexisting black-box methods."}
{"id": "2510.06645", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06645", "abs": "https://arxiv.org/abs/2510.06645", "authors": ["Zhiyuan Wei", "Xiaoxuan Yang", "Jing Sun", "Zijian Zhang"], "title": "Distilling Lightweight Language Models for C/C++ Vulnerabilities", "comment": "25 pages, 10 figures", "summary": "The increasing complexity of modern software systems exacerbates the\nprevalence of security vulnerabilities, posing risks of severe breaches and\nsubstantial economic loss. Consequently, robust code vulnerability detection is\nessential for software security. While Large Language Models (LLMs) have\ndemonstrated remarkable capabilities in natural language processing, their\npotential for automated code vulnerability detection remains underexplored.\nThis paper presents FineSec, a novel framework that harnesses LLMs through\nknowledge distillation to enable efficient and precise vulnerability\nidentification in C/C++ codebases. FineSec utilizes knowledge distillation to\ntransfer expertise from large teacher models to compact student models,\nachieving high accuracy with minimal computational cost. By integrating data\npreparation, training, evaluation, and continuous learning into a unified,\nsingle-task workflow, FineSec offers a streamlined approach. Extensive\nevaluations on C/C++ codebases demonstrate its superiority over both base\nmodels and larger LLMs in identifying complex vulnerabilities and logical\nflaws, establishing FineSec as a practical and scalable solution for real-world\nsoftware security. To facilitate reproducibility, the datasets, source code,\nand experimental results are made publicly available at:\nhttps://github.com/yangxiaoxuan123/FineSec_detect."}
{"id": "2510.06708", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06708", "abs": "https://arxiv.org/abs/2510.06708", "authors": ["Aleksi Huotala", "Miikka Kuutila", "Olli-Pekka Turtio", "Mika MÃ¤ntylÃ¤"], "title": "AISysRev -- LLM-based Tool for Title-abstract Screening", "comment": "4 pages", "summary": "Systematic reviews are a standard practice for summarizing the state of\nevidence in software engineering. Conducting systematic reviews is laborious,\nespecially during the screening or study selection phase, where the number of\npapers can be overwhelming. During this phase, papers are assessed against\ninclusion and exclusion criteria based on their titles and abstracts. Recent\nresearch has demonstrated that large language models (LLMs) can perform\ntitle-abstract screening at a level comparable to that of a master's student.\nWhile LLMs cannot be fully trusted, they can help, for example, in Rapid\nReviews, which try to expedite the review process. Building on recent research,\nwe developed AiSysRev, an LLM-based screening tool implemented as a web\napplication running in a Docker container. The tool accepts a CSV file\ncontaining paper titles and abstracts. Users specify inclusion and exclusion\ncriteria. One can use multiple LLMs for screening via OpenRouter. AiSysRev\nsupports both zero-shot and few-shot screening, and also allows for manual\nscreening through interfaces that display LLM results as guidance for human\nreviewers.We conducted a trial study with 137 papers using the tool. Our\nfindings indicate that papers can be classified into four categories: Easy\nIncludes, Easy Excludes, Boundary Includes, and Boundary Excludes. The Boundary\ncases, where LLMs are prone to errors, highlight the need for human\nintervention. While LLMs do not replace human judgment in systematic reviews,\nthey can significantly reduce the burden of assessing large volumes of\nscientific literature. Video: https://www.youtube.com/watch?v=jVbEj4Y4tQI Tool:\nhttps://github.com/EvoTestOps/AISysRev"}
{"id": "2510.06718", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06718", "abs": "https://arxiv.org/abs/2510.06718", "authors": ["Ranim Khojah", "Mazen Mohamad", "Linda Erlenhov", "Francisco Gomes de Oliveira Neto", "Philipp Leitner"], "title": "LLM Company Policies and Policy Implications in Software Organizations", "comment": "Accepted at IEEE Software Special Issue on AIware in the Foundation\n  Models Era", "summary": "The risks associated with adopting large language model (LLM) chatbots in\nsoftware organizations highlight the need for clear policies. We examine how 11\ncompanies create these policies and the factors that influence them, aiming to\nhelp managers safely integrate chatbots into development workflows."}
{"id": "2510.06975", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06975", "abs": "https://arxiv.org/abs/2510.06975", "authors": ["Muris SladiÄ", "Veronica Valeros", "Carlos Catania", "Sebastian Garcia"], "title": "VelLMes: A high-interaction AI-based deception framework", "comment": "9 pages. 9 figures. 1 table. This is a preprint of a paper that was\n  presented at the Active Defense and Deception Workshop colocated with IEEE\n  EuroS&P 2025 conference", "summary": "There are very few SotA deception systems based on Large Language Models. The\nexisting ones are limited only to simulating one type of service, mainly SSH\nshells. These systems - but also the deception technologies not based on LLMs -\nlack an extensive evaluation that includes human attackers. Generative AI has\nrecently become a valuable asset for cybersecurity researchers and\npractitioners, and the field of cyber-deception is no exception. Researchers\nhave demonstrated how LLMs can be leveraged to create realistic-looking\nhoneytokens, fake users, and even simulated systems that can be used as\nhoneypots. This paper presents an AI-based deception framework called VelLMes,\nwhich can simulate multiple protocols and services such as SSH Linux shell,\nMySQL, POP3, and HTTP. All of these can be deployed and used as honeypots, thus\nVelLMes offers a variety of choices for deception design based on the users'\nneeds. VelLMes is designed to be attacked by humans, so interactivity and\nrealism are key for its performance. We evaluate the generative capabilities\nand the deception capabilities. Generative capabilities were evaluated using\nunit tests for LLMs. The results of the unit tests show that, with careful\nprompting, LLMs can produce realistic-looking responses, with some LLMs having\na 100% passing rate. In the case of the SSH Linux shell, we evaluated deception\ncapabilities with 89 human attackers. The results showed that about 30% of the\nattackers thought that they were interacting with a real system when they were\nassigned an LLM-based honeypot. Lastly, we deployed 10 instances of the SSH\nLinux shell honeypot on the Internet to capture real-life attacks. Analysis of\nthese attacks showed us that LLM honeypots simulating Linux shells can perform\nwell against unstructured and unexpected attacks on the Internet, responding\ncorrectly to most of the issued commands."}
