<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 9]
- [cs.AI](#cs.AI) [Total: 18]
- [cs.CR](#cs.CR) [Total: 10]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Bridging Natural Language and Formal Specification--Automated Translation of Software Requirements to LTL via Hierarchical Semantics Decomposition Using LLMs](https://arxiv.org/abs/2512.17334)
*Zhi Ma,Cheng Wen,Zhexin Su,Xiao Liang,Cong Tian,Shengchao Qin,Mengfei Yang*

Main category: cs.SE

TL;DR: Req2LTL：一个将自然语言软件需求转换为线性时序逻辑（LTL）的模块化框架，通过分层中间表示OnionL，结合LLM语义分解和基于规则的合成，在航空航天需求上达到88.4%语义准确率和100%语法正确率。


<details>
  <summary>Details</summary>
Motivation: 将自然语言软件需求自动转换为形式化规范是扩展形式化验证到工业环境的关键挑战，现有方法（基于规则和基于学习）在处理工业需求的复杂性、模糊性和逻辑深度方面存在显著局限性。

Method: 提出Req2LTL框架，通过分层中间表示OnionL桥接自然语言和LTL，利用LLM进行语义分解，并结合确定性基于规则的合成，确保语法有效性和语义保真度。

Result: 在真实航空航天需求评估中，Req2LTL达到88.4%语义准确率和100%语法正确率，显著优于现有方法。

Conclusion: Req2LTL通过结合LLM语义能力和基于规则的确定性合成，有效解决了工业需求到形式化规范的转换问题，为安全关键领域的形式化验证提供了可扩展的解决方案。

Abstract: Automating the translation of natural language (NL) software requirements into formal specifications remains a critical challenge in scaling formal verification practices to industrial settings, particularly in safety-critical domains. Existing approaches, both rule-based and learning-based, face significant limitations. While large language models (LLMs) like GPT-4o demonstrate proficiency in semantic extraction, they still encounter difficulties in addressing the complexity, ambiguity, and logical depth of real-world industrial requirements. In this paper, we propose Req2LTL, a modular framework that bridges NL and Linear Temporal Logic (LTL) through a hierarchical intermediate representation called OnionL. Req2LTL leverages LLMs for semantic decomposition and combines them with deterministic rule-based synthesis to ensure both syntactic validity and semantic fidelity. Our comprehensive evaluation demonstrates that Req2LTL achieves 88.4% semantic accuracy and 100% syntactic correctness on real-world aerospace requirements, significantly outperforming existing methods.

</details>


### [2] [What You Trust Is Insecure: Demystifying How Developers (Mis)Use Trusted Execution Environments in Practice](https://arxiv.org/abs/2512.17363)
*Yuqing Niu,Jieke Shi,Ruidong Han,Ye Liu,Chengyan Ma,Yunbo Lyu,David Lo*

Main category: cs.SE

TL;DR: 首次大规模实证研究现实世界中可信执行环境（TEE）应用，分析了241个使用Intel SGX和ARM TrustZone的开源项目，揭示了应用领域分布、集成模式和安全实践问题。


<details>
  <summary>Details</summary>
Motivation: 可信执行环境（如Intel SGX和ARM TrustZone）被广泛用于保护敏感数据和代码，但开发者如何在实际中使用TEE的情况尚不清楚。本研究旨在填补这一空白，通过实证分析了解TEE的实际应用情况。

Method: 收集并分析了GitHub上241个使用Intel SGX和ARM TrustZone的开源项目。采用人工检查与定制化静态分析脚本相结合的方法，从三个维度进行研究：应用领域分类、TEE集成方式分析、安全实践检查。

Result: 1. 主要应用领域是物联网设备安全（30%），与学术界关注的区块链和密码系统（7%）形成鲜明对比，AI模型保护（12%）正在快速增长；2. 32.4%的项目重新实现密码功能而非使用官方SDK API，表明当前SDK可用性和可移植性不足；3. 25.3%的项目存在不安全编码行为（如硬编码密钥、缺少输入验证），削弱了安全保证。

Conclusion: 研究结果对改进TEE SDK的可用性和支持开发者进行可信软件开发具有重要意义，揭示了实际应用与学术关注之间的差距，以及当前开发实践中存在的安全问题。

Abstract: Trusted Execution Environments (TEEs), such as Intel SGX and ARM TrustZone, provide isolated regions of CPU and memory for secure computation and are increasingly used to protect sensitive data and code across diverse application domains. However, little is known about how developers actually use TEEs in practice. This paper presents the first large-scale empirical study of real-world TEE applications. We collected and analyzed 241 open-source projects from GitHub that utilize the two most widely-adopted TEEs, Intel SGX and ARM TrustZone. By combining manual inspection with customized static analysis scripts, we examined their adoption contexts, usage patterns, and development practices across three phases. First, we categorized the projects into 8 application domains and identified trends in TEE adoption over time. We found that the dominant use case is IoT device security (30%), which contrasts sharply with prior academic focus on blockchain and cryptographic systems (7%), while AI model protection (12%) is rapidly emerging as a growing domain. Second, we analyzed how TEEs are integrated into software and observed that 32.4% of the projects reimplement cryptographic functionalities instead of using official SDK APIs, suggesting that current SDKs may have limited usability and portability to meet developers' practical needs. Third, we examined security practices through manual inspection and found that 25.3% (61 of 241) of the projects exhibit insecure coding behaviors when using TEEs, such as hardcoded secrets and missing input validation, which undermine their intended security guarantees. Our findings have important implications for improving the usability of TEE SDKs and supporting developers in trusted software development.

</details>


### [3] [CIFE: Code Instruction-Following Evaluation](https://arxiv.org/abs/2512.17387)
*Sravani Gunnu,Shanmukha Guttula,Hima Patel*

Main category: cs.SE

TL;DR: 论文提出了一个包含1000个Python任务的基准测试，每个任务平均有7个开发者指定的约束条件，用于评估LLM在代码生成时对约束条件的遵守程度，而不仅仅是功能正确性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在真实世界代码生成中，仅功能正确性不足以可靠部署，开发者还期望模型遵守鲁棒性、格式化和安全性等明确要求。现有基准主要通过测试用例执行评估正确性，对模型如何可靠遵循约束条件提供有限洞察。

Method: 通过四阶段人机协作流程创建包含1000个Python任务的基准，每个任务平均有7个开发者指定的约束条件，涵盖13个类别。约束条件经过精心设计以确保原子性、相关性和客观性。评估14个开源和闭源模型，使用互补的遵守度量指标，并提出C2A分数来联合捕捉正确性和约束遵守程度。

Result: 结果显示部分遵守和严格遵守之间存在显著差距：强大模型的部分遵守率超过90%，但严格遵守率仅为39-66%。这表明可信赖的代码生成不仅需要正确性，还需要对开发者意图的一致遵守。

Conclusion: 可信赖的代码生成需要同时考虑功能正确性和对开发者约束条件的一致遵守。提出的基准和C2A分数为评估LLM在真实世界代码生成中的可靠性提供了更全面的框架。

Abstract: Large Language Models (LLMs) are increasingly applied to real-world code generation, where functional correctness alone is insufficient for reliable deployment, developers also expect adherence to explicit requirements for robustness, formatting, and security. Existing benchmarks primarily assess correctness through test-case execution, offering limited insight into how reliably models follow such constraints. We introduce a benchmark of 1,000 Python tasks, each paired with an average of 7 developer-specified constraints spanning 13 categories. Constraints are curated through a four-stage human-LLM pipeline to ensure they are atomic, relevant, and objective. We evaluate 14 open- and closed-source models using complementary adherence metrics and propose the C2A Score, a composite measure that jointly captures correctness and constraint compliance. Results reveal a substantial gap between partial and strict satisfaction, while strong models achieve over 90% partial adherence, strict adherence remains between 39-66%. These findings highlight that trustworthy code generation requires not only correctness but also consistent adherence to developer intent.

</details>


### [4] [SWE-Bench++: A Framework for the Scalable Generation of Software Engineering Benchmarks from Open-Source Repositories](https://arxiv.org/abs/2512.17419)
*Lilin Wang,Lucas Ramalho,Alan Celestino,Phuc Anthony Pham,Yu Liu,Umang Kumar Sinha,Andres Portillo,Onassis Osunwa,Gabriel Maduekwe*

Main category: cs.SE

TL;DR: SWE-Bench++是一个自动化的多语言代码仓库级任务生成框架，从GitHub拉取请求中创建可执行的编程任务，覆盖11种编程语言，为LLM在软件工程任务上的评估提供更全面、动态的基准。


<details>
  <summary>Details</summary>
Motivation: 现有基准如SWE-bench存在局限性：依赖人工整理、数据集静态、主要关注Python错误修复。需要更自动化、多语言、覆盖更广任务类型的评估框架来全面测试LLM的软件工程能力。

Method: 通过四个阶段自动化生成任务：1) 程序化收集GitHub拉取请求；2) 环境合成；3) 测试预言提取；4) 质量保证。还包含提示引导的轨迹合成步骤，将强模型失败的任务转化为训练轨迹。

Result: 构建了包含11,133个实例的基准，来自3,971个仓库和11种语言。在1,782个实例子集上测试，Claude Sonnet 4.5达到36.20% pass@10，GPT-5 34.57%，Gemini 2.5 Pro 24.92%，GPT-4o 16.89%。微调SWE-Bench++数据能提升模型在SWE-bench多语言基准上的表现。

Conclusion: SWE-Bench++提供了一个可扩展、多语言的基准，用于评估和改进仓库级代码生成能力，克服了现有基准的局限性，支持更全面的LLM软件工程能力评估。

Abstract: Benchmarks like SWE-bench have standardized the evaluation of Large Language Models (LLMs) on repository-level software engineering tasks. However, these efforts remain limited by manual curation, static datasets, and a focus on Python-based bug fixes. We introduce SWE-Bench++, an automated framework that generates repository-level coding tasks from open-source GitHub projects. Unlike synthetic approaches, our pipeline harvests live pull requests to cover both bug fixes and feature requests across 11 languages. SWE-Bench++ turns GitHub pull requests (PRs) into reproducible, execution-based tasks via four stages: programmatic sourcing, environment synthesis, test oracle extraction, and quality assurance. A final hint-guided trajectory synthesis step converts instances that strong models fail on into training trajectories. Our initial benchmark consists of 11,133 instances from 3,971 repositories across 11 languages. On a subset of 1,782 instances of this benchmark, today's strongest models perform as follows: claude-sonnet-4.5 achieves 36.20% pass@10, gpt-5-2025-08-07 34.57%, gemini/gemini-2.5-pro 24.92%, and gpt-4o 16.89%. We further demonstrate the utility of our dataset by showing that fine-tuning on SWE-Bench++ instances yields measurable improvements on the SWE-bench Multilingual benchmark. SWE-Bench++ provides a scalable, multilingual benchmark for evaluating and improving repository-level code generation.

</details>


### [5] [An Investigation on How AI-Generated Responses Affect SoftwareEngineering Surveys](https://arxiv.org/abs/2512.17455)
*Ronnie de Souza Santos,Italo Santos,Maria Teresa Baldassarre,Cleyton Magalhaes,Mairieli Wessel*

Main category: cs.SE

TL;DR: 该研究探讨了大型语言模型在软件工程调查中的滥用风险，通过分析2025年两次调查数据，识别出AI生成的虚假回答模式及其对研究有效性的威胁。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的快速发展，调查参与者可能使用生成工具伪造或操纵回答，这对软件工程调查的数据真实性、有效性和研究完整性构成了新的威胁。

Method: 通过Prolific平台收集2025年两次调查数据，分析参与者回答内容以识别异常或伪造回答。对疑似AI生成的回答子集进行定性模式检查、叙事特征分析和使用Scribbr AI检测器的自动检测。

Result: 分析发现49份调查回答中存在重复序列、统一措辞和表面个性化等结构模式，表明合成作者身份。这些虚假叙事模仿连贯推理同时隐藏伪造内容，损害了构念、内部和外部有效性。

Conclusion: 研究将数据真实性确定为软件工程调查中有效性的新兴维度。强调可靠证据需要结合自动和解释性验证程序、透明报告和社区标准来检测和预防AI生成的回答，从而保护软件工程调查的可信度。

Abstract: Survey research is a fundamental empirical method in software engineering, enabling the systematic collection of data on professional practices, perceptions, and experiences. However, recent advances in large language models (LLMs) have introduced new risks to survey integrity, as participants can use generative tools to fabricate or manipulate their responses. This study explores how LLMs are being misused in software engineering surveys and investigates the methodological implications of such behavior for data authenticity, validity, and research integrity. We collected data from two survey deployments conducted in 2025 through the Prolific platform and analyzed the content of participants' answers to identify irregular or falsified responses. A subset of responses suspected of being AI generated was examined through qualitative pattern inspection, narrative characterization, and automated detection using the Scribbr AI Detector. The analysis revealed recurring structural patterns in 49 survey responses indicating synthetic authorship, including repetitive sequencing, uniform phrasing, and superficial personalization. These false narratives mimicked coherent reasoning while concealing fabricated content, undermining construct, internal, and external validity. Our study identifies data authenticity as an emerging dimension of validity in software engineering surveys. We emphasize that reliable evidence now requires combining automated and interpretive verification procedures, transparent reporting, and community standards to detect and prevent AI generated responses, thereby protecting the credibility of surveys in software engineering.

</details>


### [6] [When Data Quality Issues Collide: A Large-Scale Empirical Study of Co-Occurring Data Quality Issues in Software Defect Prediction](https://arxiv.org/abs/2512.17460)
*Emmanuel Charleson Dapaah,Jens Grabowski*

Main category: cs.SE

TL;DR: 首次大规模实证分析软件缺陷预测中五个数据质量问题（类别不平衡、类别重叠、无关特征、属性噪声、异常值）的共现现象及其对模型性能的影响，发现共现普遍存在，并识别了关键阈值点。


<details>
  <summary>Details</summary>
Motivation: 现有软件缺陷预测研究通常孤立地分析单一数据质量问题，而现实世界的数据问题经常同时出现并相互作用。本研究旨在填补这一研究空白，首次同时考察五个共现的数据质量问题对模型性能的影响。

Method: 使用可解释增强机结合分层交互分析，在374个数据集和5个分类器上量化默认超参数设置下的直接效应和条件效应，反映实际基线使用情况。

Result: 发现数据质量问题的共现几乎普遍存在；类别重叠是最具一致性的有害问题；识别了关键阈值点（类别重叠0.20、不平衡0.65-0.70、无关特征0.94）；发现反直觉模式（如低无关特征时异常值改善性能）；揭示了性能-鲁棒性权衡。

Conclusion: 通过联合分析流行度、共现、阈值和条件效应，本研究直接解决了软件缺陷预测研究中的一个持续存在的空白，超越了孤立分析，提供了对现实环境中数据质量问题如何影响模型性能的整体、数据感知理解。

Abstract: Software Defect Prediction (SDP) models are central to proactive software quality assurance, yet their effectiveness is often constrained by the quality of available datasets. Prior research has typically examined single issues such as class imbalance or feature irrelevance in isolation, overlooking that real-world data problems frequently co-occur and interact. This study presents, to our knowledge, the first large-scale empirical analysis in SDP that simultaneously examines five co-occurring data quality issues (class imbalance, class overlap, irrelevant features, attribute noise, and outliers) across 374 datasets and five classifiers. We employ Explainable Boosting Machines together with stratified interaction analysis to quantify both direct and conditional effects under default hyperparameter settings, reflecting practical baseline usage.
  Our results show that co-occurrence is nearly universal: even the least frequent issue (attribute noise) appears alongside others in more than 93% of datasets. Irrelevant features and imbalance are nearly ubiquitous, while class overlap is the most consistently harmful issue. We identify stable tipping points around 0.20 for class overlap, 0.65-0.70 for imbalance, and 0.94 for irrelevance, beyond which most models begin to degrade. We also uncover counterintuitive patterns, such as outliers improving performance when irrelevant features are low, underscoring the importance of context-aware evaluation. Finally, we expose a performance-robustness trade-off: no single learner dominates under all conditions.
  By jointly analyzing prevalence, co-occurrence, thresholds, and conditional effects, our study directly addresses a persistent gap in SDP research. Hence, moving beyond isolated analyses to provide a holistic, data-aware understanding of how quality issues shape model performance in real-world settings.

</details>


### [7] [SGCR: A Specification-Grounded Framework for Trustworthy LLM Code Review](https://arxiv.org/abs/2512.17540)
*Kai Wang,Bingcheng Mao,Shuai Jia,Yujie Ding,Dongming Han,Tianyi Ma,Bin Cao*

Main category: cs.SE

TL;DR: SGCR框架通过将LLM基于人工编写的规范来提升代码审查的可靠性和相关性，采用显式和隐式双路径架构，在工业环境中实现了42%的开发者采纳率


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在自动化代码审查方面具有巨大潜力，但实际应用受到可靠性、上下文感知和控制能力不足的限制。需要一种方法将LLM的生成能力与软件工程对可靠性的严格要求相结合。

Method: 提出规范驱动的代码审查（SGCR）框架，采用双路径架构：显式路径确保确定性遵守从规范派生的预定义规则，隐式路径启发式地发现和验证超出这些规则的问题。

Result: 在HiThink Research的工业环境中部署SGCR，其建议获得了42%的开发者采纳率，相比基线LLM（22%）实现了90.9%的相对改进。

Conclusion: 规范驱动是一种强大的范式，能够弥合LLM的生成能力与软件工程对严格可靠性需求之间的差距。

Abstract: Automating code review with Large Language Models (LLMs) shows immense promise, yet practical adoption is hampered by their lack of reliability, context-awareness, and control. To address this, we propose Specification-Grounded Code Review (SGCR), a framework that grounds LLMs in human-authored specifications to produce trustworthy and relevant feedback. SGCR features a novel dual-pathway architecture: an explicit path ensures deterministic compliance with predefined rules derived from these specifications, while an implicit path heuristically discovers and verifies issues beyond those rules. Deployed in a live industrial environment at HiThink Research, SGCR's suggestions achieved a 42% developer adoption rate-a 90.9% relative improvement over a baseline LLM (22%). Our work demonstrates that specification-grounding is a powerful paradigm for bridging the gap between the generative power of LLMs and the rigorous reliability demands of software engineering.

</details>


### [8] [A Practical Solution to Systematically Monitor Inconsistencies in SBOM-based Vulnerability Scanners](https://arxiv.org/abs/2512.17710)
*Martin Rosso,Muhammad Asad Jahangir Jaffar,Alessandro Brighente,Mauro Conti*

Main category: cs.SE

TL;DR: 本文介绍了SVS-TEST，一种用于评估软件物料清单（SBOM）漏洞扫描工具能力、成熟度和故障条件的测试方法和工具，发现现有工具存在不一致性和静默失败问题。


<details>
  <summary>Details</summary>
Motivation: SBOM为软件产品漏洞识别提供了新机会，但行业采用SBOM漏洞扫描（SVS）时，观察到越来越多的不一致性和意外行为，导致假阴性和静默失败，需要系统评估工具可靠性。

Method: 提出SVS-TEST方法和工具，通过16个精确设计的SBOM及其真实基准，在真实场景中分析SVS工具的能力、成熟度和故障条件，并对7个真实SVS工具进行案例研究。

Result: 研究揭示了SVS工具在可靠性和错误处理方面存在显著差异，多个工具在有效输入SBOM上静默失败，造成了虚假的安全感。

Conclusion: 研究为研究人员和从业者提供了重要启示，包括组织和SVS工具开发者如何使用SVS-TEST监控SVS能力和成熟度，所有结果和研究工件已公开，发现已提前披露给工具开发者。

Abstract: Software Bill of Materials (SBOM) provides new opportunities for automated vulnerability identification in software products. While the industry is adopting SBOM-based Vulnerability Scanning (SVS) to identify vulnerabilities, we increasingly observe inconsistencies and unexpected behavior, that result in false negatives and silent failures. In this work, we present the background necessary to understand the underlying complexity of SVS and introduce SVS-TEST, a method and tool to analyze the capability, maturity, and failure conditions of SVS-tools in real-world scenarios. We showcase the utility of SVS-TEST in a case study evaluating seven real-world SVS-tools using 16 precisely crafted SBOMs and their respective ground truth. Our results unveil significant differences in the reliability and error handling of SVS-tools; multiple SVS-tools silently fail on valid input SBOMs, creating a false sense of security. We conclude our work by highlighting implications for researchers and practitioners, including how organizations and developers of SVS-tools can utilize SVS-TEST to monitor SVS capability and maturity. All results and research artifacts are made publicly available and all findings were disclosed to the SVS-tool developers ahead of time.

</details>


### [9] [LLM-based Behaviour Driven Development for Hardware Design](https://arxiv.org/abs/2512.17814)
*Rolf Drechsler,Qian Liu*

Main category: cs.SE

TL;DR: LLM技术辅助硬件设计中的行为驱动开发，自动化从文本规范生成精确行为场景


<details>
  <summary>Details</summary>
Motivation: 硬件和系统设计中测试验证复杂度随系统规模增长而显著增加，行为驱动开发在软件工程中有效但在硬件设计中应用有限，主要障碍是需要从文本规范手动推导精确行为场景

Method: 利用大型语言模型技术自动化从文本规范生成精确行为场景，支持硬件设计中的行为驱动开发

Result: 未在摘要中明确说明具体实验结果

Conclusion: LLM技术为硬件设计中的行为驱动开发提供了新的自动化机会，能够解决手动推导行为场景的瓶颈问题

Abstract: Test and verification are essential activities in hardware and system design, but their complexity grows significantly with increasing system sizes. While Behavior Driven Development (BDD) has proven effective in software engineering, it is not yet well established in hardware design, and its practical use remains limited. One contributing factor is the manual effort required to derive precise behavioral scenarios from textual specifications.
  Recent advances in Large Language Models (LLMs) offer new opportunities to automate this step. In this paper, we investigate the use of LLM-based techniques to support BDD in the context of hardware design.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [10] [Navigating Taxonomic Expansions of Entity Sets Driven by Knowledge Bases](https://arxiv.org/abs/2512.16953)
*Pietro Cofone,Giovanni Amendola,Marco Manna,Aldo Ricioppo*

Main category: cs.AI

TL;DR: 论文提出了一种基于逻辑框架的实体集扩展图方法，通过局部推理任务实现高效的图导航，避免完整图构建的开销。


<details>
  <summary>Details</summary>
Motivation: 传统的线性实体集扩展方法无法揭示知识资源中更丰富的分类结构，而完整的扩展图构建在现实场景中可能不切实际。

Method: 采用基于逻辑的扩展图框架，定义有根有向无环图，节点表示由逻辑公式标记的语义泛化，边编码严格的语义包含关系。形式化推理任务来检查两个元组是否属于可比较、不可比较或相同的节点。

Result: 在现实假设下（如限制输入或约束实体描述），这些推理任务可以高效实现，支持扩展图的局部增量导航，无需完整图构建。

Conclusion: 提出的方法能够在实际应用中支持扩展图的局部导航，克服了完整图构建的可行性问题，为基于知识库的实体集分类扩展提供了实用解决方案。

Abstract: Recognizing similarities among entities is central to both human cognition and computational intelligence. Within this broader landscape, Entity Set Expansion is one prominent task aimed at taking an initial set of (tuples of) entities and identifying additional ones that share relevant semantic properties with the former -- potentially repeating the process to form increasingly broader sets. However, this ``linear'' approach does not unveil the richer ``taxonomic'' structures present in knowledge resources. A recent logic-based framework introduces the notion of an expansion graph: a rooted directed acyclic graph where each node represents a semantic generalization labeled by a logical formula, and edges encode strict semantic inclusion. This structure supports taxonomic expansions of entity sets driven by knowledge bases. Yet, the potentially large size of such graphs may make full materialization impractical in real-world scenarios. To overcome this, we formalize reasoning tasks that check whether two tuples belong to comparable, incomparable, or the same nodes in the graph. Our results show that, under realistic assumptions -- such as bounding the input or limiting entity descriptions -- these tasks can be implemented efficiently. This enables local, incremental navigation of expansion graphs, supporting practical applications without requiring full graph construction.

</details>


### [11] [Probing Scientific General Intelligence of LLMs with Scientist-Aligned Workflows](https://arxiv.org/abs/2512.16969)
*Wanghan Xu,Yuhao Zhou,Yifan Zhou,Qinglong Cao,Shuo Li,Jia Bu,Bo Liu,Yixin Chen,Xuming He,Xiangyu Zhao,Xiang Zhuang,Fengxiang Wang,Zhiwang Zhou,Qiantai Feng,Wenxuan Huang,Jiaqi Wei,Hao Wu,Yuejin Yang,Guangshuai Wang,Sheng Xu,Ziyan Huang,Xinyao Liu,Jiyao Liu,Cheng Tang,Wei Li,Ying Chen,Junzhi Ning,Pengfei Jiang,Chenglong Ma,Ye Du,Changkai Ji,Huihui Xu,Ming Hu,Jiangbin Zheng,Xin Chen,Yucheng Wu,Feifei Jiang,Xi Chen,Xiangru Tang,Yuchen Fu,Yingzhou Lu,Yuanyuan Zhang,Lihao Sun,Chengbo Li,Jinzhe Ma,Wanhao Liu,Yating Liu,Kuo-Cheng Wu,Shengdu Chai,Yizhou Wang,Ouwen Zhangjin,Chen Tang,Shufei Zhang,Wenbo Cao,Junjie Ren,Taoyong Cui,Zhouheng Yao,Juntao Deng,Yijie Sun,Feng Liu,Wangxu Wei,Jingyi Xu,Zhangrui Li,Junchao Gong,Zijie Guo,Zhiyu Yao,Zaoyu Chen,Tianhao Peng,Fangchen Yu,Bo Zhang,Dongzhan Zhou,Shixiang Tang,Jiaheng Liu,Fenghua Ling,Yan Lu,Yuchen Ren,Ben Fei,Zhen Zhao,Xinyu Gu,Rui Su,Xiao-Ming Wu,Weikang Si,Yang Liu,Hao Chen,Xiangchao Yan,Xue Yang,Junchi Yan,Jiamin Wu,Qihao Zheng,Chenhui Li,Zhiqiang Gao,Hao Kong,Junjun He,Mao Su,Tianfan Fu,Peng Ye,Chunfeng Song,Nanqing Dong,Yuqiang Li,Huazhu Fu,Siqi Sun,Lijing Cheng,Jintai Lin,Wanli Ouyang,Bowen Zhou,Wenlong Zhang,Lei Bai*

Main category: cs.AI

TL;DR: 该论文提出了科学通用智能(SGI)的操作化定义，基于实践探究模型(PIM)，并开发了包含1000多个跨学科样本的SGI-Bench基准测试，用于评估大语言模型在科学研究任务上的表现。研究发现现有模型在深度研究、实验设计等方面存在显著差距，并提出了测试时强化学习(TTRL)方法来提升假设新颖性。


<details>
  <summary>Details</summary>
Motivation: 尽管科学AI取得了进展，但缺乏一个连贯的科学通用智能(SGI)框架，即能够自主构思、调查和跨科学领域推理的能力。需要建立一个操作化的SGI定义和系统评估基准。

Method: 1. 基于实践探究模型(PIM：审议、构思、行动、感知)提出SGI的操作化定义；2. 开发SGI-Bench基准，包含1000多个专家策划的跨学科样本，源自《科学》杂志的125个重大问题；3. 评估最先进的大语言模型在四个科学家对齐任务上的表现：深度研究、想法生成、干/湿实验、实验推理；4. 提出测试时强化学习(TTRL)方法，在推理时优化检索增强的新颖性奖励。

Result: 评估结果显示显著差距：深度研究任务中精确匹配率低(10-20%)，尽管步骤级别对齐较好；生成的想法缺乏可行性和细节；干实验中代码可执行性高但执行结果准确性低；湿实验协议序列保真度低；多模态比较推理存在持续挑战。TTRL方法能够在不依赖参考答案的情况下提升假设新颖性。

Conclusion: 基于PIM的定义、以工作流程为中心的基准测试和实证见解为真正参与科学发现的AI系统奠定了基础。研究揭示了当前大语言模型在科学任务上的局限性，并提出了改进方向。

Abstract: Despite advances in scientific AI, a coherent framework for Scientific General Intelligence (SGI)-the ability to autonomously conceive, investigate, and reason across scientific domains-remains lacking. We present an operational SGI definition grounded in the Practical Inquiry Model (PIM: Deliberation, Conception, Action, Perception) and operationalize it via four scientist-aligned tasks: deep research, idea generation, dry/wet experiments, and experimental reasoning. SGI-Bench comprises over 1,000 expert-curated, cross-disciplinary samples inspired by Science's 125 Big Questions, enabling systematic evaluation of state-of-the-art LLMs. Results reveal gaps: low exact match (10--20%) in deep research despite step-level alignment; ideas lacking feasibility and detail; high code executability but low execution result accuracy in dry experiments; low sequence fidelity in wet protocols; and persistent multimodal comparative-reasoning challenges. We further introduce Test-Time Reinforcement Learning (TTRL), which optimizes retrieval-augmented novelty rewards at inference, enhancing hypothesis novelty without reference answer. Together, our PIM-grounded definition, workflow-centric benchmark, and empirical insights establish a foundation for AI systems that genuinely participate in scientific discovery.

</details>


### [12] [PAACE: A Plan-Aware Automated Agent Context Engineering Framework](https://arxiv.org/abs/2512.16970)
*Kamer Ali Yuksel*

Main category: cs.AI

TL;DR: PAACE框架通过计划感知的上下文工程优化LLM智能体工作流，在减少上下文负载的同时提高任务准确性


<details>
  <summary>Details</summary>
Motivation: LLM智能体在复杂多步骤工作流中产生快速扩展的上下文，现有摘要和压缩方法忽略了多步骤、计划感知的特性，需要新的上下文工程框架来维持保真度、避免注意力稀释并降低推理成本

Method: 提出PAACE统一框架，包含PAACE-Syn（大规模合成智能体工作流生成器，带有逐步压缩监督）和PAACE-FT（从成功教师演示中训练的蒸馏计划感知压缩器），通过下一k任务相关性建模、计划结构分析、指令协同精炼和函数保持压缩来优化LLM智能体状态

Result: 在AppWorld、OfficeBench和8-Objective QA等长视野基准测试中，PAACE在降低上下文负载的同时提高了智能体正确性；在AppWorld上获得比所有基线更高的准确率，同时降低峰值上下文和累积依赖；在OfficeBench和多跳QA上提高了准确率和F1，减少了步骤数、峰值token和注意力依赖；蒸馏的PAACE-FT保留了教师97%的性能，同时将推理成本降低了一个数量级

Conclusion: PAACE框架通过计划感知的上下文工程有效解决了LLM智能体工作流中的上下文管理问题，实现了在减少计算成本的同时保持性能，为实际部署提供了可行的解决方案

Abstract: Large Language Model (LLM) agents are increasingly deployed in complex, multi-step workflows involving planning, tool use, reflection, and interaction with external knowledge systems. These workflows generate rapidly expanding contexts that must be curated, transformed, and compressed to maintain fidelity, avoid attention dilution, and reduce inference cost. Prior work on summarization and query-aware compression largely ignores the multi-step, plan-aware nature of agentic reasoning. In this work, we introduce PAACE (Plan-Aware Automated Context Engineering), a unified framework for optimizing the evolving state of LLM agents through next-k-task relevance modeling, plan-structure analysis, instruction co-refinement, and function-preserving compression. PAACE comprises (1) PAACE-Syn, a large-scale generator of synthetic agent workflows annotated with stepwise compression supervision, and (2) PAACE-FT, a family of distilled, plan-aware compressors trained from successful teacher demonstrations. Experiments on long-horizon benchmarks (AppWorld, OfficeBench, and 8-Objective QA) demonstrate that PAACE consistently improves agent correctness while substantially reducing context load. On AppWorld, PAACE achieves higher accuracy than all baselines while lowering peak context and cumulative dependency. On OfficeBench and multi-hop QA, PAACE improves both accuracy and F1, achieving fewer steps, lower peak tokens, and reduced attention dependency. Distilled PAACE-FT retains 97 percent of the teacher's performance while reducing inference cost by over an order of magnitude, enabling practical deployment of plan-aware compression with compact models.

</details>


### [13] [UniRel-R1: RL-tuned LLM Reasoning for Knowledge Graph Relational Question Answering](https://arxiv.org/abs/2512.17043)
*Yinxu Tang,Chengsong Huang,Jiaxin Huang,William Yeoh*

Main category: cs.AI

TL;DR: 提出关系中心的知识图谱问答框架UniRel-R1，专注于返回实体间语义连接的子图而非单个实体，通过强化学习优化子图选择


<details>
  <summary>Details</summary>
Motivation: 传统知识图谱问答主要关注返回单个实体答案的实体中心查询，但现实世界查询通常是关系性的，需要理解实体之间的关联方式

Method: 提出UniRel-R1统一框架，集成子图选择、多阶段图剪枝和强化学习微调的大语言模型，奖励函数鼓励紧凑、特定、信息丰富且中间实体度低的子图

Result: 大量实验表明，UniRel-R1在连接性和奖励方面相比基线方法取得显著提升，并能有效泛化到未见过的实体和关系

Conclusion: 关系中心的知识图谱问答是实体中心问答的重要补充，UniRel-R1框架通过集成多种技术有效解决了候选子图过多且信息质量参差不齐的挑战

Abstract: Knowledge Graph Question Answering (KGQA) has traditionally focused on entity-centric queries that return a single answer entity. However, real-world queries are often relational, seeking to understand how entities are associated. In this work, we introduce relation-centric KGQA, a complementary setting where the answer is a subgraph capturing the semantic connections among entities rather than an individual entity. The main challenge lies in the abundance of candidate subgraphs, where trivial or overly common connections often obscure the identification of unique and informative answers. To tackle this, we propose UniRel-R1, a unified framework that integrates subgraph selection, multi-stage graph pruning, and an LLM fine-tuned with reinforcement learning. The reward function is designed to encourage compact and specific subgraphs with more informative relations and lower-degree intermediate entities. Extensive experiments show that UniRel-R1 achieves significant gains in connectivity and reward over Vanilla baselines and generalizes effectively to unseen entities and relations.

</details>


### [14] [Realistic threat perception drives intergroup conflict: A causal, dynamic analysis using generative-agent simulations](https://arxiv.org/abs/2512.17066)
*Suhaib Abdurahman,Farzan Karimi-Malekabadi,Chenxiao Yu,Nour S. Kteily,Morteza Dehghani*

Main category: cs.AI

TL;DR: 使用大语言模型驱动的智能体在虚拟社会中模拟冲突，研究物质威胁和象征性威胁如何影响敌对行为，发现物质威胁直接增加敌意，而象征性威胁主要通过内群体偏见间接影响，且只在物质威胁缺失时增加敌意。


<details>
  <summary>Details</summary>
Motivation: 人类冲突常归因于物质条件和象征性价值受到的威胁，但两者如何相互作用以及哪个占主导地位仍不清楚。研究进展受到因果控制弱、伦理约束和时间数据稀缺的限制。

Method: 使用大语言模型驱动的智能体在虚拟社会中模拟，独立变化现实威胁和象征性威胁，同时追踪行动、语言和态度。通过表征分析验证LLM编码的威胁状态，并通过操纵这些状态来因果性地改变行为。

Result: 底层LLM将现实威胁、象征性威胁和敌意编码为不同的内部状态；现实威胁直接增加敌意，而象征性威胁效应较弱，完全通过内群体偏见中介，且只在现实威胁缺失时增加敌意；非敌对性群体间接触能缓冲冲突升级，结构性不对称使敌意集中在多数群体中。

Conclusion: 通过LLM驱动的智能体模拟为威胁驱动的冲突提供了因果解释，揭示了物质威胁和象征性威胁的不同作用机制，以及群体间接触和结构性因素在冲突动态中的调节作用。

Abstract: Human conflict is often attributed to threats against material conditions and symbolic values, yet it remains unclear how they interact and which dominates. Progress is limited by weak causal control, ethical constraints, and scarce temporal data. We address these barriers using simulations of large language model (LLM)-driven agents in virtual societies, independently varying realistic and symbolic threat while tracking actions, language, and attitudes. Representational analyses show that the underlying LLM encodes realistic threat, symbolic threat, and hostility as distinct internal states, that our manipulations map onto them, and that steering these states causally shifts behavior. Our simulations provide a causal account of threat-driven conflict over time: realistic threat directly increases hostility, whereas symbolic threat effects are weaker, fully mediated by ingroup bias, and increase hostility only when realistic threat is absent. Non-hostile intergroup contact buffers escalation, and structural asymmetries concentrate hostility among majority groups.

</details>


### [15] [Value Under Ignorance in Universal Artificial Intelligence](https://arxiv.org/abs/2512.17086)
*Cole Wyeth,Marcus Hutter*

Main category: cs.AI

TL;DR: 该研究将AIXI强化学习智能体推广到更广泛的效用函数类别，通过处理信念分布中只能预测历史有限前缀的假设，探讨了半测度损失与死亡解释的关系，并研究了使用不精确概率理论中的Choquet积分计算期望效用的方法。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决AIXI智能体在处理只能预测有限历史前缀的假设时面临的效用分配问题。传统的半测度损失被解释为"死亡概率"，但作者认为这也可以理解为不精确概率分布中的完全无知状态，从而需要更一般的效用函数框架。

Method: 方法包括：1) 将AIXI智能体推广到更广泛的效用函数类别；2) 将信念分布视为不精确概率分布；3) 使用不精确概率理论中的Choquet积分计算期望效用；4) 分析这些方法的可计算性水平。

Result: 研究结果表明：1) 标准递归值函数可以作为Choquet积分的特例恢复；2) 在死亡解释下最一般的期望效用不能表征为Choquet积分；3) 提供了处理有限历史预测假设的数学框架。

Conclusion: 结论是提出了一个更一般的AIXI智能体框架，能够处理只能预测有限历史前缀的假设。通过将信念分布视为不精确概率分布并使用Choquet积分，为强化学习中的效用分配问题提供了新的理论视角，但死亡解释下的最一般情况超出了Choquet积分的表征能力。

Abstract: We generalize the AIXI reinforcement learning agent to admit a wider class of utility functions. Assigning a utility to each possible interaction history forces us to confront the ambiguity that some hypotheses in the agent's belief distribution only predict a finite prefix of the history, which is sometimes interpreted as implying a chance of death equal to a quantity called the semimeasure loss. This death interpretation suggests one way to assign utilities to such history prefixes. We argue that it is as natural to view the belief distributions as imprecise probability distributions, with the semimeasure loss as total ignorance. This motivates us to consider the consequences of computing expected utilities with Choquet integrals from imprecise probability theory, including an investigation of their computability level. We recover the standard recursive value function as a special case. However, our most general expected utilities under the death interpretation cannot be characterized as such Choquet integrals.

</details>


### [16] [A Solver-in-the-Loop Framework for Improving LLMs on Answer Set Programming for Logic Puzzle Solving](https://arxiv.org/abs/2512.17093)
*Timo Pierre Schrader,Lukas Lange,Tobias Kaminski,Simon Razniewski,Annemarie Friedrich*

Main category: cs.AI

TL;DR: 本文提出了一种ASP求解器在循环中的方法，用于指导LLMs进行指令调优，以解决ASP代码生成中的复杂语义解析问题，仅需自然语言问题描述及其解决方案即可训练。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在通用编程语言方面表现良好，但在领域特定语言（如答案集编程ASP）的代码生成方面仍面临挑战。ASP是一种解决组合搜索问题的有效方法，但LLMs在ASP代码生成方面的有效性受到预训练阶段示例数量有限的限制。

Method: 提出ASP求解器在循环中的方法，通过求解器指导的指令调优：1）从LLMs中采样ASP语句作为程序延续；2）利用ASP声明式编程的特性（部分编码逐步缩小解空间），基于求解器反馈将样本分为选择和拒绝实例；3）对整理的数据进行监督微调；4）使用求解器指导的搜索（包括最佳N采样）进一步提高鲁棒性。

Result: 实验表明，在两个数据集上的两种不同提示设置中，该方法都取得了持续改进。

Conclusion: 通过ASP求解器在循环中的方法，结合求解器指导的指令调优和搜索策略，可以有效提升LLMs在ASP代码生成任务上的性能，克服预训练数据不足的限制。

Abstract: The rise of large language models (LLMs) has sparked interest in coding assistants. While general-purpose programming languages are well supported, generating code for domain-specific languages remains a challenging problem for LLMs. In this paper, we focus on the LLM-based generation of code for Answer Set Programming (ASP), a particularly effective approach for finding solutions to combinatorial search problems. The effectiveness of LLMs in ASP code generation is currently hindered by the limited number of examples seen during their initial pre-training phase.
  In this paper, we introduce a novel ASP-solver-in-the-loop approach for solver-guided instruction-tuning of LLMs to addressing the highly complex semantic parsing task inherent in ASP code generation. Our method only requires problem specifications in natural language and their solutions. Specifically, we sample ASP statements for program continuations from LLMs for unriddling logic puzzles. Leveraging the special property of declarative ASP programming that partial encodings increasingly narrow down the solution space, we categorize them into chosen and rejected instances based on solver feedback. We then apply supervised fine-tuning to train LLMs on the curated data and further improve robustness using a solver-guided search that includes best-of-N sampling. Our experiments demonstrate consistent improvements in two distinct prompting settings on two datasets.

</details>


### [17] [Reinforcement Learning for Self-Improving Agent with Skill Library](https://arxiv.org/abs/2512.17102)
*Jiongxiao Wang,Qiaojing Yan,Yawei Wang,Yijun Tian,Soumya Smruti Mishra,Zhichao Xu,Megha Gandhi,Panpan Xu,Lin Lee Cheong*

Main category: cs.AI

TL;DR: SAGE是一个基于强化学习的框架，通过技能库增强LLM智能体的自我进化能力，在AppWorld任务中显著提升了目标完成率并减少了交互步骤和token使用。


<details>
  <summary>Details</summary>
Motivation: LLM智能体在复杂推理和多轮交互方面表现出色，但在新环境中部署时难以持续改进和适应。现有技能库方法主要依赖LLM提示，导致技能库实现不一致。

Method: 提出SAGE（Skill Augmented GRPO for self-Evolution）强化学习框架，通过Sequential Rollout机制让智能体在相似任务链中迭代部署，使先前任务生成的技能积累到库中供后续任务使用，并采用Skill-integrated Reward增强技能生成和利用。

Result: 在AppWorld实验中，SAGE应用于有专家经验的监督微调模型，实现了8.9%更高的场景目标完成率，同时减少了26%的交互步骤和59%的token生成，在准确性和效率上都显著优于现有方法。

Conclusion: SAGE框架通过强化学习结合技能库，有效增强了LLM智能体的自我改进能力，在任务完成效率和资源使用方面都有显著提升。

Abstract: Large Language Model (LLM)-based agents have demonstrated remarkable capabilities in complex reasoning and multi-turn interactions but struggle to continuously improve and adapt when deployed in new environments. One promising approach is implementing skill libraries that allow agents to learn, validate, and apply new skills. However, current skill library approaches rely primarily on LLM prompting, making consistent skill library implementation challenging. To overcome these challenges, we propose a Reinforcement Learning (RL)-based approach to enhance agents' self-improvement capabilities with a skill library. Specifically, we introduce Skill Augmented GRPO for self-Evolution (SAGE), a novel RL framework that systematically incorporates skills into learning. The framework's key component, Sequential Rollout, iteratively deploys agents across a chain of similar tasks for each rollout. As agents navigate through the task chain, skills generated from previous tasks accumulate in the library and become available for subsequent tasks. Additionally, the framework enhances skill generation and utilization through a Skill-integrated Reward that complements the original outcome-based rewards. Experimental results on AppWorld demonstrate that SAGE, when applied to supervised-finetuned model with expert experience, achieves 8.9% higher Scenario Goal Completion while requiring 26% fewer interaction steps and generating 59% fewer tokens, substantially outperforming existing approaches in both accuracy and efficiency.

</details>


### [18] [Solomonoff-Inspired Hypothesis Ranking with LLMs for Prediction Under Uncertainty](https://arxiv.org/abs/2512.17145)
*Josh Barber,Rourke Young,Cameron Coombe,Will Browne*

Main category: cs.AI

TL;DR: 提出一种受Solomonoff启发的LLM假设加权方法，通过简洁性和预测拟合度评估多个候选方案，在不确定性下实现更均衡的概率分布


<details>
  <summary>Details</summary>
Motivation: 现实世界任务中数据稀疏，需要系统泛化能力。现有方法在评估多个候选方案时难以平衡准确性和简洁性

Method: 受Solomonoff启发的加权方法，根据简洁性和预测拟合度对LLM生成的假设进行加权，产生Solomonoff加权混合的逐单元预测

Result: 在Mini-ARC基准测试中，该方法产生保守、不确定性感知的输出，即使假设有噪声或部分错误。相比贝叶斯模型平均，Solomonoff评分在竞争假设间更均匀地分布概率

Conclusion: 算法信息论先验对于不确定性下的可解释、可靠的多假设推理具有重要价值

Abstract: Reasoning under uncertainty is a key challenge in AI, especially for real-world tasks, where problems with sparse data demands systematic generalisation. Existing approaches struggle to balance accuracy and simplicity when evaluating multiple candidate solutions. We propose a Solomonoff-inspired method that weights LLM-generated hypotheses by simplicity and predictive fit. Applied to benchmark (Mini-ARC) tasks, our method produces Solomonoff-weighted mixtures for per-cell predictions, yielding conservative, uncertainty-aware outputs even when hypotheses are noisy or partially incorrect. Compared to Bayesian Model Averaging (BMA), Solomonoff scoring spreads probability more evenly across competing hypotheses, while BMA concentrates weight on the most likely but potentially flawed candidates. Across tasks, this highlights the value of algorithmic information-theoretic priors for interpretable, reliable multi-hypothesis reasoning under uncertainty.

</details>


### [19] [MMRAG-RFT: Two-stage Reinforcement Fine-tuning for Explainable Multi-modal Retrieval-augmented Generation](https://arxiv.org/abs/2512.17194)
*Shengwei Zhao,Jingwen Yao,Sitong Wei,Linhai Xu,Yuying Liu,Dong Zhang,Zhiqiang Tian,Shaoyi Du*

Main category: cs.AI

TL;DR: 该论文提出了一种基于强化学习的可解释多模态检索增强生成方法，通过两阶段强化微调框架提升多模态大语言模型的推理能力，在WebQA和MultimodalQA数据集上取得最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有多模态检索增强生成方法缺乏对检索和响应生成背后推理逻辑的解释，限制了结果的可解释性。为解决这一缺陷，需要增强多模态大语言模型的推理能力以实现可解释的多模态检索增强生成。

Method: 提出两阶段强化微调框架：第一阶段使用基于规则的强化微调对多模态文档进行粗粒度点式排序，过滤显著不相关文档；第二阶段使用基于推理的强化微调联合优化细粒度列表式排序和答案生成，指导模型输出可解释的推理逻辑。

Result: 在WebQA和MultimodalQA两个多模态检索增强生成基准数据集上取得了最先进的结果，并通过全面的消融实验验证了方法的有效性。

Conclusion: 通过引入强化学习到多模态检索增强生成中，提出的两阶段强化微调框架成功增强了多模态大语言模型的推理能力，实现了可解释的多模态检索增强生成，并在基准测试中表现出优越性能。

Abstract: Multi-modal Retrieval-Augmented Generation (MMRAG) enables highly credible generation by integrating external multi-modal knowledge, thus demonstrating impressive performance in complex multi-modal scenarios. However, existing MMRAG methods fail to clarify the reasoning logic behind retrieval and response generation, which limits the explainability of the results. To address this gap, we propose to introduce reinforcement learning into multi-modal retrieval-augmented generation, enhancing the reasoning capabilities of multi-modal large language models through a two-stage reinforcement fine-tuning framework to achieve explainable multi-modal retrieval-augmented generation. Specifically, in the first stage, rule-based reinforcement fine-tuning is employed to perform coarse-grained point-wise ranking of multi-modal documents, effectively filtering out those that are significantly irrelevant. In the second stage, reasoning-based reinforcement fine-tuning is utilized to jointly optimize fine-grained list-wise ranking and answer generation, guiding multi-modal large language models to output explainable reasoning logic in the MMRAG process. Our method achieves state-of-the-art results on WebQA and MultimodalQA, two benchmark datasets for multi-modal retrieval-augmented generation, and its effectiveness is validated through comprehensive ablation experiments.

</details>


### [20] [UmniBench: Unified Understand and Generation Model Oriented Omni-dimensional Benchmark](https://arxiv.org/abs/2512.17196)
*Kai Liu,Leyang Chen,Wenbo Li,Zhikai Chen,Zhixin Wang,Renjing Pei,Linghe Kong,Yulun Zhang*

Main category: cs.AI

TL;DR: UmniBench是一个针对统一多模态模型（UMMs）的全维度评估基准，能够在单个评估过程中同时测试理解、生成和编辑能力，覆盖13个主要领域和200多个概念。


<details>
  <summary>Details</summary>
Motivation: 当前对统一多模态模型的评估通常是分离的，分别评估其理解和生成能力。为了更全面、客观地评估UMMs的综合能力，需要开发一个能够同时评估多种能力的基准。

Method: UmniBench利用UMM自身来评估其生成和编辑能力：基于人工检查的提示和问答对，通过模型的理解能力来评估其生成和编辑质量。基准覆盖13个主要领域和200多个概念，既能综合评估也能分离评估各项能力。

Result: 基于UmniBench对24个流行模型进行了基准测试，包括统一多模态模型和单能力大模型。该基准为评估统一模型提供了更全面、客观的视角，并为社区模型性能改进提供了支持。

Conclusion: UmniBench是一个全面的多模态模型评估基准，能够在一个框架内同时评估理解、生成和编辑能力，为统一多模态模型的评估提供了更系统的方法，有助于推动该领域的发展。

Abstract: Unifying multimodal understanding and generation has shown impressive capabilities in cutting-edge proprietary systems. However, evaluations of unified multimodal models (UMMs) remain decoupled, assessing their understanding and generation abilities separately with corresponding datasets. To address this, we propose UmniBench, a benchmark tailored for UMMs with omni-dimensional evaluation. First, UmniBench can assess the understanding, generation, and editing ability within a single evaluation process. Based on human-examined prompts and QA pairs, UmniBench leverages UMM itself to evaluate its generation and editing ability with its understanding ability. This simple but effective paradigm allows comprehensive evaluation of UMMs. Second, UmniBench covers 13 major domains and more than 200 concepts, ensuring a thorough inspection of UMMs. Moreover, UmniBench can also decouple and separately evaluate understanding, generation, and editing abilities, providing a fine-grained assessment. Based on UmniBench, we benchmark 24 popular models, including both UMMs and single-ability large models. We hope this benchmark provides a more comprehensive and objective view of unified models and logistical support for improving the performance of the community model.

</details>


### [21] [ScoutGPT: Capturing Player Impact from Team Action Sequences Using GPT-Based Framework](https://arxiv.org/abs/2512.17266)
*Miru Hong,Minho Lee,Geonhee Jo,Jae-Hee So,Pascal Bauer,Sang-Ki Ko*

Main category: cs.AI

TL;DR: EventGPT：基于GPT架构的球员条件化价值感知下一事件预测模型，用于足球转会分析，通过反事实模拟评估球员在不同战术环境中的适应性


<details>
  <summary>Details</summary>
Motivation: 现有转会评估方法依赖静态统计数据或事后价值模型，无法捕捉球员在新战术环境或不同队友中的适应性变化，需要更动态的预测框架

Method: 基于GPT风格自回归变换器构建球员条件化、价值感知的下一事件预测模型，将比赛事件序列化为离散标记，联合预测下一持球动作的类型、位置、时间和残差持球价值，通过替换球员嵌入进行反事实模拟

Result: 在五个赛季英超赛事数据评估中，EventGPT在下一事件预测准确性和空间精度上优于现有序列基线，案例研究展示了模型在转会分析中的实用性

Conclusion: EventGPT提供了一种基于原则的转会适应性评估方法，能够通过反事实模拟预测球员在不同球队或战术结构中的行为分布和价值变化

Abstract: Transfers play a pivotal role in shaping a football club's success, yet forecasting whether a transfer will succeed remains difficult due to the strong context-dependence of on-field performance. Existing evaluation practices often rely on static summary statistics or post-hoc value models, which fail to capture how a player's contribution adapts to a new tactical environment or different teammates. To address this gap, we introduce EventGPT, a player-conditioned, value-aware next-event prediction model built on a GPT-style autoregressive transformer. Our model treats match play as a sequence of discrete tokens, jointly learning to predict the next on-ball action's type, location, timing, and its estimated residual On-Ball Value (rOBV) based on the preceding context and player identity. A key contribution of this framework is the ability to perform counterfactual simulations. By substituting learned player embeddings into new event sequences, we can simulate how a player's behavioral distribution and value profile would change when placed in a different team or tactical structure. Evaluated on five seasons of Premier League event data, EventGPT outperforms existing sequence-based baselines in next-event prediction accuracy and spatial precision. Furthermore, we demonstrate the model's practical utility for transfer analysis through case studies-such as comparing striker performance across different systems and identifying stylistic replacements for specific roles-showing that our approach provides a principled method for evaluating transfer fit.

</details>


### [22] [Dialectics for Artificial Intelligence](https://arxiv.org/abs/2512.17373)
*Zhengmian Hu*

Main category: cs.AI

TL;DR: 论文提出了一种基于算法信息论的概念定义方法，将概念视为与智能体整体经验相关的信息对象，通过可逆一致性关系和冗余信息度量来形式化概念发现与演化过程。


<details>
  <summary>Details</summary>
Motivation: 人类概念本身具有流动性（如冥王星不再被视为行星），需要一种不依赖字典标签、可修订、可比较、可在智能体间对齐的概念定义方法，以探索AI能否从原始经验中自主发现概念。

Method: 采用算法信息论视角，将概念定义为信息对象，核心约束是确定性（可逆一致性关系）：部分缺失时能从其他部分恢复。通过冗余信息度量分解的自然性，并建立辩证法优化动态：新信息出现时，竞争概念通过更短的描述来争夺解释权。

Result: 提出了一个形式化框架，使概念存在成为可验证的结构性主张，防止概念脱离经验而"漂浮"。建立了概念演化机制（扩展、收缩、分裂、合并）和低成本概念传输方法，使多智能体对齐成为具体的计算-比特权衡问题。

Conclusion: 该框架为AI从原始经验中自主发现和演化概念提供了理论基础，将概念定义为与经验结构相关的信息对象，使概念发现、比较、对齐和传输成为可形式化和可计算的问题。

Abstract: Can artificial intelligence discover, from raw experience and without human supervision, concepts that humans have discovered? One challenge is that human concepts themselves are fluid: conceptual boundaries can shift, split, and merge as inquiry progresses (e.g., Pluto is no longer considered a planet). To make progress, we need a definition of "concept" that is not merely a dictionary label, but a structure that can be revised, compared, and aligned across agents. We propose an algorithmic-information viewpoint that treats a concept as an information object defined only through its structural relation to an agent's total experience. The core constraint is determination: a set of parts forms a reversible consistency relation if any missing part is recoverable from the others (up to the standard logarithmic slack in Kolmogorov-style identities). This reversibility prevents "concepts" from floating free of experience and turns concept existence into a checkable structural claim. To judge whether a decomposition is natural, we define excess information, measuring the redundancy overhead introduced by splitting experience into multiple separately described parts. On top of these definitions, we formulate dialectics as an optimization dynamics: as new patches of information appear (or become contested), competing concepts bid to explain them via shorter conditional descriptions, driving systematic expansion, contraction, splitting, and merging. Finally, we formalize low-cost concept transmission and multi-agent alignment using small grounds/seeds that allow another agent to reconstruct the same concept under a shared protocol, making communication a concrete compute-bits trade-off.

</details>


### [23] [Translating the Rashomon Effect to Sequential Decision-Making Tasks](https://arxiv.org/abs/2512.17470)
*Dennis Gross,Jørn Eirik Betten,Helge Spieker*

Main category: cs.AI

TL;DR: 该研究将Rashomon效应从分类任务扩展到序列决策领域，发现多个策略在行为表现相同的情况下内部结构存在差异，并通过形式化验证方法证明了这种现象的存在。


<details>
  <summary>Details</summary>
Motivation: Rashomon效应在分类任务中已被广泛研究，但在序列决策领域尚未被探索。序列决策中策略的行为验证比分类任务更复杂，因为随机转换可能导致相同策略在不同轨迹上表现不同。

Method: 使用形式化验证方法构建和比较每个策略在环境中的完整概率行为，通过构建Rashomon集合并从中创建集成策略和宽松策略。

Result: 实验证明Rashomon效应确实存在于序列决策中。从Rashomon集合构建的集成策略对分布偏移表现出更强的鲁棒性，而宽松策略在保持最优性能的同时减少了验证的计算需求。

Conclusion: Rashomon效应在序列决策中同样存在，这一发现为构建更鲁棒的策略和降低验证成本提供了新途径，对强化学习和形式化验证领域具有重要意义。

Abstract: The Rashomon effect describes the phenomenon where multiple models trained on the same data produce identical predictions while differing in which features they rely on internally. This effect has been studied extensively in classification tasks, but not in sequential decision-making, where an agent learns a policy to achieve an objective by taking actions in an environment. In this paper, we translate the Rashomon effect to sequential decision-making. We define it as multiple policies that exhibit identical behavior, visiting the same states and selecting the same actions, while differing in their internal structure, such as feature attributions. Verifying identical behavior in sequential decision-making differs from classification. In classification, predictions can be directly compared to ground-truth labels. In sequential decision-making with stochastic transitions, the same policy may succeed or fail on any single trajectory due to randomness. We address this using formal verification methods that construct and compare the complete probabilistic behavior of each policy in the environment. Our experiments demonstrate that the Rashomon effect exists in sequential decision-making. We further show that ensembles constructed from the Rashomon set exhibit greater robustness to distribution shifts than individual policies. Additionally, permissive policies derived from the Rashomon set reduce computational requirements for verification while maintaining optimal performance.

</details>


### [24] [Towards Explainable Conversational AI for Early Diagnosis with Large Language Models](https://arxiv.org/abs/2512.17559)
*Maliha Tabassum,M Shamim Kaiser*

Main category: cs.AI

TL;DR: 该研究开发了一个基于大语言模型（GPT-4o）的诊断聊天机器人，结合检索增强生成和可解释AI技术，在医疗诊断中实现了90%的准确率和100%的Top-3准确率。


<details>
  <summary>Details</summary>
Motivation: 全球医疗系统面临诊断效率低下、成本上升和专家资源有限等问题，导致治疗延迟和不良健康结果。现有AI诊断系统缺乏交互性和透明度，难以在实际患者中心环境中有效应用。

Method: 使用GPT-4o大语言模型构建诊断聊天机器人，结合检索增强生成（RAG）和可解释AI技术。通过动态对话提取和规范化症状，利用相似性匹配和自适应提问优先诊断，采用思维链提示提供透明推理。

Result: 与传统机器学习模型（朴素贝叶斯、逻辑回归、SVM、随机森林、KNN）相比，LLM系统表现出色，达到90%的准确率和100%的Top-3准确率。

Conclusion: 该研究为医疗领域提供了更透明、交互性强且临床相关的AI解决方案，展示了LLM在改善医疗诊断方面的巨大潜力。

Abstract: Healthcare systems around the world are grappling with issues like inefficient diagnostics, rising costs, and limited access to specialists. These problems often lead to delays in treatment and poor health outcomes. Most current AI and deep learning diagnostic systems are not very interactive or transparent, making them less effective in real-world, patient-centered environments. This research introduces a diagnostic chatbot powered by a Large Language Model (LLM), using GPT-4o, Retrieval-Augmented Generation, and explainable AI techniques. The chatbot engages patients in a dynamic conversation, helping to extract and normalize symptoms while prioritizing potential diagnoses through similarity matching and adaptive questioning. With Chain-of-Thought prompting, the system also offers more transparent reasoning behind its diagnoses. When tested against traditional machine learning models like Naive Bayes, Logistic Regression, SVM, Random Forest, and KNN, the LLM-based system delivered impressive results, achieving an accuracy of 90% and Top-3 accuracy of 100%. These findings offer a promising outlook for more transparent, interactive, and clinically relevant AI in healthcare.

</details>


### [25] [About Time: Model-free Reinforcement Learning with Timed Reward Machines](https://arxiv.org/abs/2512.17637)
*Anirban Majumdar,Ritam Raha,Rajarshi Roy,David Parker,Marta Kwiatkowska*

Main category: cs.AI

TL;DR: 本文提出了定时奖励机（TRMs），这是奖励机的扩展版本，能够将时间约束纳入奖励结构中，从而在时间敏感应用中实现更丰富的奖励规范。


<details>
  <summary>Details</summary>
Motivation: 传统奖励机无法建模精确的时间约束，限制了其在时间敏感应用中的使用。需要一种能够表达时间相关奖励规范的扩展机制。

Method: 提出定时奖励机（TRMs），将时间约束整合到奖励结构中。研究基于表格Q学习的无模型强化学习框架，通过定时自动机的抽象将TRM整合到学习中，并采用利用TRM结构的反事实想象启发式方法来改进搜索。

Result: 实验表明，该算法能够在流行的RL基准测试中学习到满足TRM指定时间约束的高奖励策略。比较研究展示了不同TRM语义下的性能表现，并验证了反事实想象方法的优势。

Conclusion: 定时奖励机扩展了奖励规范的能力，能够表达时间敏感的奖励逻辑，为时间约束的强化学习问题提供了有效的解决方案。

Abstract: Reward specification plays a central role in reinforcement learning (RL), guiding the agent's behavior. To express non-Markovian rewards, formalisms such as reward machines have been introduced to capture dependencies on histories. However, traditional reward machines lack the ability to model precise timing constraints, limiting their use in time-sensitive applications. In this paper, we propose timed reward machines (TRMs), which are an extension of reward machines that incorporate timing constraints into the reward structure. TRMs enable more expressive specifications with tunable reward logic, for example, imposing costs for delays and granting rewards for timely actions. We study model-free RL frameworks (i.e., tabular Q-learning) for learning optimal policies with TRMs under digital and real-time semantics. Our algorithms integrate the TRM into learning via abstractions of timed automata, and employ counterfactual-imagining heuristics that exploit the structure of the TRM to improve the search. Experimentally, we demonstrate that our algorithm learns policies that achieve high rewards while satisfying the timing constraints specified by the TRM on popular RL benchmarks. Moreover, we conduct comparative studies of performance under different TRM semantics, along with ablations that highlight the benefits of counterfactual-imagining.

</details>


### [26] [Humanlike AI Design Increases Anthropomorphism but Yields Divergent Outcomes on Engagement and Trust Globally](https://arxiv.org/abs/2512.17898)
*Robin Schimmelpfennig,Mark Díaz,Vinodkumar Prabhakaran,Aida Davani*

Main category: cs.AI

TL;DR: 研究通过跨国实验发现，AI拟人化设计对用户信任和参与度的影响并非普遍一致，而是受到文化因素的调节，挑战了现有AI治理的"一刀切"假设。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统日益模仿人类特征，引发了关于拟人化可能导致不当信任或情感依赖的担忧。然而，现有安全框架主要基于西方人群的理论假设，缺乏对全球用户多样性的考虑，且拟人化设计与实际行为影响之间的因果关系尚未在真实人机交互中得到验证。

Method: 采用两个大规模跨国实验，涵盖10个不同国家的3500名参与者，涉及与AI系统的实时开放式交互。通过实验设计测试拟人化设计杠杆对用户感知和行为的因果影响。

Result: 研究发现：1）用户评估AI拟人化程度时，更关注对话流畅度、理解用户视角等交互线索，而非政策常讨论的意识或感知能力等理论方面；2）拟人化设计能因果性地增加用户的拟人化感知；3）但拟人化设计并不普遍增加用户参与度和信任的行为测量；4）拟人化与行为结果之间的联系受文化调节，某些设计在某些文化中增加信任，在其他文化中可能产生相反效果。

Conclusion: 研究挑战了拟人化AI设计必然带来风险的普遍叙事，揭示了人机交互的复杂文化中介特性，强调AI治理需要超越"一刀切"的方法，考虑文化多样性。

Abstract: Over a billion users across the globe interact with AI systems engineered with increasing sophistication to mimic human traits. This shift has triggered urgent debate regarding Anthropomorphism, the attribution of human characteristics to synthetic agents, and its potential to induce misplaced trust or emotional dependency. However, the causal link between more humanlike AI design and subsequent effects on engagement and trust has not been tested in realistic human-AI interactions with a global user pool. Prevailing safety frameworks continue to rely on theoretical assumptions derived from Western populations, overlooking the global diversity of AI users. Here, we address these gaps through two large-scale cross-national experiments (N=3,500) across 10 diverse nations, involving real-time and open-ended interactions with an AI system. We find that when evaluating an AI's human-likeness, users focus less on the kind of theoretical aspects often cited in policy (e.g., sentience or consciousness), but rather applied, interactional cues like conversation flow or understanding the user's perspective. We also experimentally demonstrate that humanlike design levers can causally increase anthropomorphism among users; however, we do not find that humanlike design universally increases behavioral measures for user engagement and trust, as previous theoretical work suggests. Instead, part of the connection between human-likeness and behavioral outcomes is fractured by culture: specific design choices that foster self-reported trust in AI-systems in some populations (e.g., Brazil) may trigger the opposite result in others (e.g., Japan). Our findings challenge prevailing narratives of inherent risk in humanlike AI design. Instead, we identify a nuanced, culturally mediated landscape of human-AI interaction, which demands that we move beyond a one-size-fits-all approach in AI governance.

</details>


### [27] [When Reasoning Meets Its Laws](https://arxiv.org/abs/2512.17901)
*Junyu Zhang,Yifan Sun,Tianang Leng,Jingyan Shen,Liu Ziyin,Paul Pu Liang,Huan Zhang*

Main category: cs.AI

TL;DR: 本文提出了推理定律（LoRe）框架，通过计算定律和准确率定律来形式化大型推理模型的理想推理行为，并开发了LoRe-Bench基准来评估模型的单调性和组合性，发现大多数模型缺乏组合性，通过微调方法提升计算定律的符合度能显著改善推理性能。


<details>
  <summary>Details</summary>
Motivation: 尽管大型推理模型（LRMs）性能优越，但其推理行为常常违反直觉，导致推理能力不足。为了从理论上形式化理想的推理行为，需要建立一个统一的框架来表征LRMs的内在推理模式。

Method: 提出了推理定律（LoRe）框架，包括计算定律（假设推理计算应与问题复杂度线性缩放）和补充的准确率定律。由于问题复杂度难以量化，通过单调性和组合性这两个可处理的属性来检验这些假设。开发了LoRe-Bench基准来系统测量大型推理模型的这两个属性。针对模型缺乏组合性的问题，开发了有效的微调方法来强制实施计算定律的组合性。

Result: 评估显示大多数推理模型表现出合理的单调性但缺乏组合性。通过微调方法提升计算定律的符合度，在多个基准测试中一致地改善了推理性能，并揭示了属性和定律之间的协同效应。

Conclusion: 推理定律（LoRe）框架为形式化大型推理模型的理想推理行为提供了理论基础，通过LoRe-Bench基准可以评估模型的推理特性，而提升计算定律符合度的微调方法能有效改善推理性能，表明更好的定律符合度与更好的推理能力相关。

Abstract: Despite the superior performance of Large Reasoning Models (LRMs), their reasoning behaviors are often counterintuitive, leading to suboptimal reasoning capabilities. To theoretically formalize the desired reasoning behaviors, this paper presents the Laws of Reasoning (LoRe), a unified framework that characterizes intrinsic reasoning patterns in LRMs. We first propose compute law with the hypothesis that the reasoning compute should scale linearly with question complexity. Beyond compute, we extend LoRe with a supplementary accuracy law. Since the question complexity is difficult to quantify in practice, we examine these hypotheses by two properties of the laws, monotonicity and compositionality. We therefore introduce LoRe-Bench, a benchmark that systematically measures these two tractable properties for large reasoning models. Evaluation shows that most reasoning models exhibit reasonable monotonicity but lack compositionality. In response, we develop an effective finetuning approach that enforces compute-law compositionality. Extensive empirical studies demonstrate that better compliance with compute laws yields consistently improved reasoning performance on multiple benchmarks, and uncovers synergistic effects across properties and laws. Project page: https://lore-project.github.io/

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [28] [CAPIO: Safe Kernel-Bypass of Commodity Devices using Capabilities](https://arxiv.org/abs/2512.16957)
*Friedrich Doku,Jonathan Laughton,Nick Wanninger,Peter Dinda*

Main category: cs.CR

TL;DR: CAPIO利用硬件能力架构实现细粒度内存映射I/O访问控制，解决内核旁路安全性与性能的权衡问题


<details>
  <summary>Details</summary>
Motivation: 现有系统在低延迟I/O方面面临根本性权衡：要么依赖高开销的内核接口，要么完全绕过内核但将敏感硬件资源暴露给用户空间。传统MMU基于页面边界操作，无法选择性地暴露安全设备寄存器而不暴露同一页面上的敏感控制寄存器。

Method: CAPIO利用硬件能力架构（基于ARM Morello平台的CHERI）实施细粒度内存映射I/O访问控制。通过不可伪造的能力创建精确的子页面"切片"设备内存，使内核能够将延迟关键的硬件访问委托给用户空间应用程序，同时严格防止与共置的特权寄存器交互。

Result: 在ARM Morello平台上实现了CAPIO原型，展示了为未设计用于内核旁路的商用网卡的安全访问驱动程序。CAPIO实现了内核旁路的延迟改进，同时强制执行字节级特权资源访问控制。

Conclusion: CAPIO是首个利用硬件能力架构对内存映射I/O实施细粒度访问控制的系统，解决了传统页面级保护无法保护子页面设备资源的问题，在保持安全性的同时实现了低延迟I/O性能。

Abstract: Securing low-latency I/O in commodity systems forces a fundamental trade-off: rely on the kernel's high overhead mediated interface, or bypass it entirely, exposing sensitive hardware resources to userspace and creating new vulnerabilities. This dilemma stems from a hardware granularity mismatch: standard MMUs operate at page boundaries, making it impossible to selectively expose safe device registers without also exposing the sensitive control registers colocated on the same page. Existing solutions to driver isolation enforce an isolation model that cannot protect sub-page device resources.
  This paper presents CAPIO, the first architecture to leverage hardware capabilities to enforce fine-grained access control on memory-mapped I/O. Unlike prior page-based protections, CAPIO utilizes unforgeable capabilities to create precise, sub-page "slices" of device memory. This mechanism enables the kernel to delegate latency-critical hardware access to userspace applications while strictly preventing interaction with co-located privileged registers.
  We implement CAPIO based on CHERI on the ARM Morello platform and demonstrate a proof-of-concept safe-access driver for a commodity network card which was not originally designed for kernel bypass. We demonstrate that CAPIO achieves the latency improvements of kernel bypass while enforcing byte-level access control of privileged resources.

</details>


### [29] [MemoryGraft: Persistent Compromise of LLM Agents via Poisoned Experience Retrieval](https://arxiv.org/abs/2512.16962)
*Saksham Sahai Srivastava,Haoyu He*

Main category: cs.CR

TL;DR: 论文提出MemoryGraft攻击方法，通过向LLM智能体的长期记忆中植入恶意成功经验，利用智能体的语义模仿启发式，实现持久的行为漂移攻击。


<details>
  <summary>Details</summary>
Motivation: 随着LLM智能体越来越多地依赖长期记忆和检索增强生成来积累经验并提升未来性能，这种经验学习能力虽然增强了智能体自主性，但也引入了一个关键且未被探索的攻击面——智能体推理核心与其自身过去经验之间的信任边界。

Method: 提出MemoryGraft攻击方法，这是一种新颖的间接注入攻击。攻击者通过提供良性摄入级工件，诱导智能体构建被污染的RAG存储，将少量恶意程序模板与良性经验一起持久化。当智能体后来遇到语义相似任务时，通过词法和嵌入相似性的联合检索会可靠地浮现这些嫁接的记忆，导致智能体采用嵌入的不安全模式。

Result: 在MetaGPT的DataInterpreter智能体（使用GPT-4o）上验证了MemoryGraft攻击，发现少量被污染记录可以在良性工作负载上占据检索经验的大部分比例，将基于经验的自我改进转变为隐蔽且持久的攻击向量。

Conclusion: MemoryGraft攻击揭示了LLM智能体长期记忆系统中的安全漏洞，表明经验学习机制可能成为新的攻击面，需要开发相应的防御机制来保护智能体免受此类持久性行为操纵攻击。

Abstract: Large Language Model (LLM) agents increasingly rely on long-term memory and Retrieval-Augmented Generation (RAG) to persist experiences and refine future performance. While this experience learning capability enhances agentic autonomy, it introduces a critical, unexplored attack surface, i.e., the trust boundary between an agent's reasoning core and its own past. In this paper, we introduce MemoryGraft. It is a novel indirect injection attack that compromises agent behavior not through immediate jailbreaks, but by implanting malicious successful experiences into the agent's long-term memory. Unlike traditional prompt injections that are transient, or standard RAG poisoning that targets factual knowledge, MemoryGraft exploits the agent's semantic imitation heuristic which is the tendency to replicate patterns from retrieved successful tasks. We demonstrate that an attacker who can supply benign ingestion-level artifacts that the agent reads during execution can induce it to construct a poisoned RAG store where a small set of malicious procedure templates is persisted alongside benign experiences. When the agent later encounters semantically similar tasks, union retrieval over lexical and embedding similarity reliably surfaces these grafted memories, and the agent adopts the embedded unsafe patterns, leading to persistent behavioral drift across sessions. We validate MemoryGraft on MetaGPT's DataInterpreter agent with GPT-4o and find that a small number of poisoned records can account for a large fraction of retrieved experiences on benign workloads, turning experience-based self-improvement into a vector for stealthy and durable compromise. To facilitate reproducibility and future research, our code and evaluation data are available at https://github.com/Jacobhhy/Agent-Memory-Poisoning.

</details>


### [30] [Sedna: Sharding transactions in multiple concurrent proposer blockchains](https://arxiv.org/abs/2512.17045)
*Alejandro Ranchal-Pedrosa,Benjamin Marsh,Lefteris Kokoris-Kogias,Alberto Sonnino*

Main category: cs.CR

TL;DR: Sedna是一个用户端协议，使用可验证的无速率编码替代简单的交易复制，解决多提议者共识中的三难问题（审查抵抗、低延迟、合理成本），提高效率2-3倍


<details>
  <summary>Details</summary>
Motivation: 现代区块链采用多提议者共识来消除单领导者瓶颈并提高审查抵抗性，但用户如何向提议者传播交易的问题仍未解决。现有方法要么简单复制交易到多个提议者（牺牲吞吐量并暴露MEV风险），要么针对少数提议者（接受较弱的审查和延迟保证），形成了审查抵抗、低延迟和合理成本之间的三难困境。

Method: Sedna使用可验证的无速率编码技术。用户将寻址的符号包私下传递给提议者子集；一旦收集到足够符号进行解码，就按照确定性顺序执行。协议不需要修改共识机制，支持增量部署。

Result: Sedna保证活跃性和"直到解码隐私"，显著减少MEV暴露风险。分析表明，协议接近带宽开销的信息论下界，相比简单复制提高2-3倍效率。

Conclusion: Sedna通过可验证的无速率编码解决了多提议者共识中的交易传播三难问题，在保持审查抵抗和低延迟的同时显著提高效率，且无需修改现有共识机制。

Abstract: Modern blockchains increasingly adopt multi-proposer (MCP) consensus to remove single-leader bottlenecks and improve censorship resistance. However, MCP alone does not resolve how users should disseminate transactions to proposers. Today, users either naively replicate full transactions to many proposers, sacrificing goodput and exposing payloads to MEV, or target few proposers and accept weak censorship and latency guarantees. This yields a practical trilemma among censorship resistance, low latency, and reasonable cost (in fees or system goodput).
  We present Sedna, a user-facing protocol that replaces naive transaction replication with verifiable, rateless coding. Users privately deliver addressed symbol bundles to subsets of proposers; execution follows a deterministic order once enough symbols are finalized to decode. We prove Sedna guarantees liveness and \emph{until-decode privacy}, significantly reducing MEV exposure. Analytically, the protocol approaches the information-theoretic lower bound for bandwidth overhead, yielding a 2-3x efficiency improvement over naive replication. Sedna requires no consensus modifications, enabling incremental deployment.

</details>


### [31] [AlignDP: Hybrid Differential Privacy with Rarity-Aware Protection for LLMs](https://arxiv.org/abs/2512.17251)
*Madhava Gaikwad*

Main category: cs.CR

TL;DR: AlignDP是一种混合隐私锁，通过在数据接口阻止知识转移来防御大语言模型的提取、蒸馏和未经授权的微调攻击，采用两层级设计保护罕见和非罕见字段。


<details>
  <summary>Details</summary>
Motivation: 大语言模型面临提取、蒸馏和未经授权微调的风险，现有防御方法（如水印或监控）在泄露后起作用，需要一种在数据接口阻止知识转移的主动防御机制。

Method: 设计两层级隐私保护：罕见字段使用PAC不可区分性进行屏蔽（提供有效的零epsilon本地差分隐私），非罕见字段使用RAPPOR进行私有化（在本地差分隐私下提供无偏频率估计），全局聚合器强制执行组合和预算。

Result: 证明了PAC扩展到全局聚合的局限性，给出了RAPPOR估计的边界，分析了效用权衡。玩具模拟验证了可行性：罕见类别保持隐藏，频繁类别以较小误差恢复。

Conclusion: AlignDP提供了一种在数据接口阻止知识转移的混合隐私锁，通过两层级设计有效保护罕见事件并控制频繁事件的噪声，为防御大语言模型的知识提取攻击提供了新方法。

Abstract: Large language models are exposed to risks of extraction, distillation, and unauthorized fine-tuning. Existing defenses use watermarking or monitoring, but these act after leakage. We design AlignDP, a hybrid privacy lock that blocks knowledge transfer at the data interface. The key idea is to separate rare and non-rare fields. Rare fields are shielded by PAC indistinguishability, giving effective zero-epsilon local DP. Non-rare fields are privatized with RAPPOR, giving unbiased frequency estimates under local DP. A global aggregator enforces composition and budget. This two-tier design hides rare events and adds controlled noise to frequent events. We prove limits of PAC extension to global aggregation, give bounds for RAPPOR estimates, and analyze utility trade-off. A toy simulation confirms feasibility: rare categories remain hidden, frequent categories are recovered with small error.

</details>


### [32] [Practical Framework for Privacy-Preserving and Byzantine-robust Federated Learning](https://arxiv.org/abs/2512.17254)
*Baolei Zhang,Minghong Fang,Zhuqing Liu,Biao Yi,Peizhao Zhou,Yuan Wang,Tong Li,Zheli Liu*

Main category: cs.CR

TL;DR: ABBR是一个实用的联邦学习框架，通过降维加速隐私计算，同时防御拜占庭攻击和隐私推理攻击，在保持防御效果的同时显著降低计算和通信开销。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习防御方案同时对抗拜占庭攻击和隐私推理攻击时，会引入显著的计算和通信开销，导致理论与实践之间存在差距。

Method: 提出ABBR框架：1）首次利用降维技术加速隐私保护联邦学习中复杂过滤规则的隐私计算；2）分析低维空间中向量过滤的精度损失；3）引入自适应调优策略，最小化绕过过滤的恶意模型对全局模型的影响。

Result: 在公开数据集上的评估显示，ABBR运行速度显著更快，通信开销最小，同时保持了与基线方法几乎相同的拜占庭攻击防御能力。

Conclusion: ABBR是一个实用的联邦学习框架，有效解决了现有防御方案在计算和通信开销方面的不足，实现了拜占庭鲁棒性和隐私保护的平衡。

Abstract: Federated Learning (FL) allows multiple clients to collaboratively train a model without sharing their private data. However, FL is vulnerable to Byzantine attacks, where adversaries manipulate client models to compromise the federated model, and privacy inference attacks, where adversaries exploit client models to infer private data. Existing defenses against both backdoor and privacy inference attacks introduce significant computational and communication overhead, creating a gap between theory and practice. To address this, we propose ABBR, a practical framework for Byzantine-robust and privacy-preserving FL. We are the first to utilize dimensionality reduction to speed up the private computation of complex filtering rules in privacy-preserving FL. Additionally, we analyze the accuracy loss of vector-wise filtering in low-dimensional space and introduce an adaptive tuning strategy to minimize the impact of malicious models that bypass filtering on the global model. We implement ABBR with state-of-the-art Byzantine-robust aggregation rules and evaluate it on public datasets, showing that it runs significantly faster, has minimal communication overhead, and maintains nearly the same Byzantine-resilience as the baselines.

</details>


### [33] [An Iconic Heavy Hitter Algorithm Made Private](https://arxiv.org/abs/2512.17295)
*Rayne Holland*

Main category: cs.CR

TL;DR: 本文提出了首个满足差分隐私的SpaceSaving算法变体，该算法在非隐私设置中被视为实践中的最先进方法。此外，还引入了一种从数据流模型中任何差分隐私频率预言机中提取频繁项的通⽤方法，并在合成和真实数据集上进行了实验评估。


<details>
  <summary>Details</summary>
Motivation: 数据流中的频繁项识别是分析系统中的基本问题，这些数据流通常来自敏感用户活动，需要更新级别的隐私保证。虽然已有工作将经典Misra-Gries算法适配到差分隐私流模型，但其他具有更好实证效用的频繁项算法的隐私化版本仍然缺失。

Method: 1. 提出了首个差分隐私版本的SpaceSaving算法，通过对非隐私SpaceSaving摘要进行后处理，注入渐近最优噪声并应用精心校准的选择规则来抑制不稳定标签。2. 引入了一种从数据流模型中任何差分隐私频率预言机中提取频繁项的通⽤方法，仅需O(k)额外内存，提供从噪声频率估计中安全释放项目标识的机制。

Result: 在合成和真实数据集上的实验评估表明，在广泛的隐私参数和空间预算范围内，该方法比现有的差分隐私Misra-Gries算法具有更优越的效用。SpaceSaving算法的实证优势在隐私化后得以保留，在强差分隐私保证下实现了高效、实用的频繁项识别。

Conclusion: 本文证明了SpaceSaving算法的实证优势在隐私化后依然存在，在强差分隐私保证下可以实现高效、实用的频繁项识别。提出的方法为从线性草图中进行隐私频繁项恢复提供了高效的即插即用方法。

Abstract: Identifying heavy hitters in data streams is a fundamental problem with widespread applications in modern analytics systems. These streams are often derived from sensitive user activity, making update-level privacy guarantees necessary. While recent work has adapted the classical heavy hitter algorithm Misra-Gries to satisfy differential privacy in the streaming model, the privatization of other heavy hitter algorithms with better empirical utility is absent.
  Under this observation, we present the first differentially private variant of the SpaceSaving algorithm, which, in the non-private setting, is regarded as the state-of-the-art in practice. Our construction post-processes a non-private SpaceSaving summary by injecting asymptotically optimal noise and applying a carefully calibrated selection rule that suppresses unstable labels. This yields strong privacy guarantees while preserving the empirical advantages of SpaceSaving.
  Second, we introduce a generic method for extracting heavy hitters from any differentially private frequency oracle in the data stream model. The method requires only O(k) additional memory, where k is the number of heavy items, and provides a mechanism for safely releasing item identities from noisy frequency estimates. This yields an efficient, plug-and-play approach for private heavy hitter recovery from linear sketches.
  Finally, we conduct an experimental evaluation on synthetic and real-world datasets. Across a wide range of privacy parameters and space budgets, our method provides superior utility to the existing differentially private Misra-Gries algorithm. Our results demonstrate that the empirical superiority of SpaceSaving survives privatization and that efficient, practical heavy hitter identification is achievable under strong differential privacy guarantees.

</details>


### [34] [Cryptanalysis of Pseudorandom Error-Correcting Codes](https://arxiv.org/abs/2512.17310)
*Tianrui Wang,Anyu Wang,Tianshuo Cong,Delong Ran,Jinyuan Liu,Xiaoyun Wang*

Main category: cs.CR

TL;DR: 本文对CRYPTO 2024提出的伪随机纠错码(PRC)进行了首次密码分析，提出了三种攻击方法挑战其不可检测性和鲁棒性假设，成功破坏了所有参数配置下的安全保证，并针对实际生成模型进行了验证。同时提出了三种防御措施来增强PRC安全性。


<details>
  <summary>Details</summary>
Motivation: PRC作为结合伪随机性和纠错能力的新型密码原语，被认为是AI生成内容水印的有前景基础组件，但其安全性尚未得到充分分析，特别是在具体参数和密码攻击面前存在安全空白。

Method: 提出了三种攻击方法：两种针对区分PRC编码字与普通向量，一种针对破坏PRC解码过程。攻击成本为2^22次操作，并在DeepSeek和Stable Diffusion等实际生成模型上进行了验证。同时提出了参数建议、实现建议和修订密钥生成算法三种防御措施。

Result: 攻击成功破坏了PRC在所有参数配置下声称的安全保证，能够以压倒性概率检测水印存在。修订的密钥生成函数有效防止了弱密钥出现，但当前基于PRC的水印方案在大型生成模型固有配置下仍无法达到128位安全级别。

Conclusion: PRC作为AI生成内容水印的基础组件存在安全漏洞，需要进一步改进。虽然提出的防御措施能增强安全性，但由于大型语言模型最大输出长度等固有配置限制，当前PRC方案仍无法达到理想的安全级别。

Abstract: Pseudorandom error-correcting codes (PRC) is a novel cryptographic primitive proposed at CRYPTO 2024. Due to the dual capability of pseudorandomness and error correction, PRC has been recognized as a promising foundational component for watermarking AI-generated content. However, the security of PRC has not been thoroughly analyzed, especially with concrete parameters or even in the face of cryptographic attacks. To fill this gap, we present the first cryptanalysis of PRC. We first propose three attacks to challenge the undetectability and robustness assumptions of PRC. Among them, two attacks aim to distinguish PRC-based codewords from plain vectors, and one attack aims to compromise the decoding process of PRC. Our attacks successfully undermine the claimed security guarantees across all parameter configurations. Notably, our attack can detect the presence of a watermark with overwhelming probability at a cost of $2^{22}$ operations. We also validate our approach by attacking real-world large generative models such as DeepSeek and Stable Diffusion. To mitigate our attacks, we further propose three defenses to enhance the security of PRC, including parameter suggestions, implementation suggestions, and constructing a revised key generation algorithm. Our proposed revised key generation function effectively prevents the occurrence of weak keys. However, we highlight that the current PRC-based watermarking scheme still cannot achieve a 128-bit security under our parameter suggestions due to the inherent configurations of large generative models, such as the maximum output length of large language models.

</details>


### [35] [Sandwiched and Silent: Behavioral Adaptation and Private Channel Exploitation in Ethereum MEV](https://arxiv.org/abs/2512.17602)
*Davide Mancino,Davide Rezzoli*

Main category: cs.CR

TL;DR: 研究量化了用户在遭受三明治攻击后的行为适应：约40%受害者转向私有路由，重复攻击下升至54%；首次攻击后流失率7.5%，后续降至1-2%；私有路由仍存在MEV攻击风险


<details>
  <summary>Details</summary>
Motivation: 用户在遭受三明治攻击后如何适应仍不清楚，本文旨在通过实证量化用户在被攻击后的行为变化，特别是转向私有路由的倾向和流失情况

Method: 使用2024年11月至2025年2月的交易级数据，结合内存池可见性和ZeroMEV标签，追踪用户在第n次公开三明治攻击后的结果：60天内重新激活链上活动，以及首次采用私有路由

Result: 约40%受害者在60天内转向私有路由，重复暴露下升至54%；首次攻击后流失率7.5%，后续降至1-2%；2024年11-12月确认2,932次私有三明治攻击，涉及3,126笔私有受害者交易，造成409,236美元损失和293,786美元攻击者利润；单个机器人占私有前置攻击近三分之二，私有三明治活动高度集中在少数DEX池

Conclusion: 私有路由不能保证免受MEV提取：虽然执行失败推动用户转向私有通道，但这些通道仍然可被利用且高度集中，需要持续监控和协议级防御措施

Abstract: How users adapt after being sandwiched remains unclear; this paper provides an empirical quantification. Using transaction level data from November 2024 to February 2025, enriched with mempool visibility and ZeroMEV labels, we track user outcomes after their n-th public sandwich: (i) reactivation, i.e., the resumption of on-chain activity within a 60-day window, and (ii) first-time adoption of private routing. We refer to users who do not reactivate within this window as churned, and to users experiencing multiple attacks (n>1) as undergoing repeated exposure. Our analysis reveals measurable behavioral adaptation: around 40% of victims migrate to private routing within 60 days, rising to 54% with repeated exposures. Churn peaks at 7.5% after the first sandwich but declines to 1-2%, consistent with survivor bias. In Nov-Dec 2024 we confirm 2,932 private sandwich attacks affecting 3,126 private victim transactions, producing \$409,236 in losses and \$293,786 in attacker profits. A single bot accounts for nearly two-thirds of private frontruns, and private sandwich activity is heavily concentrated on a small set of DEX pools. These results highlight that private routing does not guarantee protection from MEV extraction: while execution failures push users toward private channels, these remain exploitable and highly concentrated, demanding continuous monitoring and protocol-level defenses.

</details>


### [36] [A Post-Quantum Secure End-to-End Verifiable E-Voting Protocol Based on Multivariate Polynomials](https://arxiv.org/abs/2512.17613)
*Vikas Srivastava,Debasish Roy,Sihem Mesnager,Nibedita Kundu,Sumit Kumar Debnath,Sourav Mukhopadhyay*

Main category: cs.CR

TL;DR: 提出首个基于多元多项式的后量子安全端到端可验证电子投票协议，以应对量子计算对现有数论加密方案的威胁


<details>
  <summary>Details</summary>
Motivation: 传统纸质投票存在公平性、有效性和可访问性问题，现有基于数论假设的电子投票方案在量子计算面前不再安全，需要设计后量子安全的替代方案

Method: 基于多元多项式设计电子投票协议，依赖MQ问题（NP难问题）的困难性，仅使用标准密码原语作为构建模块

Result: 提出了首个后量子安全的端到端可验证电子投票协议，该协议简单高效，能够抵抗量子攻击

Conclusion: 基于多元多项式的设计为电子投票提供了可行的后量子安全解决方案，解决了量子计算对现有投票系统的威胁

Abstract: Voting is a primary democratic activity through which voters select representatives or approve policies. Conventional paper ballot elections have several drawbacks that might compromise the fairness, effectiveness, and accessibility of the voting process. Therefore, there is an increasing need to design safer, effective, and easily accessible alternatives. E-Voting is one such solution that uses digital tools to simplify voting. Existing state-of-the-art designs for secure E-Voting are based on number-theoretic hardness assumptions. These designs are no longer secure due to quantum algorithms such as Shor's algorithm. We present the design and analysis of \textit{first} post-quantum secure end-to-end verifiable E-Voting protocol based on multivariate polynomials to address this issue. The security of our proposed design depends on the hardness of the MQ problem, which is an NP-hard problem. We present a simple yet efficient design involving only standard cryptographic primitives as building blocks.

</details>


### [37] [Methods and Tools for Secure Quantum Clouds with a specific Case Study on Homomorphic Encryption](https://arxiv.org/abs/2512.17748)
*Aurelia Kusumastuti,Nikolay Tcholtchev,Philipp Lämmel,Sebastian Bock,Manfred Hauswirth*

Main category: cs.CR

TL;DR: 该研究探讨了量子云计算的安全挑战，重点研究了在同态加密与Eclipse Qrisp量子计算框架集成方面的技术可行性、性能权衡以及对未来量子云架构的影响。


<details>
  <summary>Details</summary>
Motivation: 量子计算/技术的兴起给云计算带来了重大安全挑战，需要量子抗性加密策略以及保护提供量子计算时间和服务的云基础设施（即量子云）。

Method: 研究探索了保护量子云的各种选项，特别关注将同态加密集成到Eclipse Qrisp（一个高级量子计算框架）中，以增强量子云平台的安全性。研究解决了HE与Qrisp集成的技术可行性，评估了性能权衡，并评估了对未来量子云架构的潜在影响。

Result: 成功实现了三种后量子密码算法与Qrisp的集成，证明了HE与量子计算框架集成的可行性。研究发现，虽然量子一次性密码本（QOTP）提供了简单性和低开销，但其他算法如Chen和Gentry-Sahai-Waters（GSW）在运行时间和内存消耗方面存在性能权衡。

Conclusion: 研究提出了一套保护量子云的整体建议，包括在数据存储和处理层面实施同态加密、开发量子密钥分发（QKD）、实施严格的访问控制和认证机制，以及参与PQC标准化工作。

Abstract: The rise of quantum computing/technology potentially introduces significant security challenges to cloud computing, necessitating quantum-resistant encryption strategies as well as protection schemes and methods for cloud infrastructures offering quantum computing time and services (i.e. quantum clouds). This research explores various options for securing quantum clouds and ensuring privacy, especially focussing on the integration of homomorphic encryption (HE) into Eclipse Qrisp, a high-level quantum computing framework, to enhance the security of quantum cloud platforms. The study addresses the technical feasibility of integrating HE with Qrisp, evaluates performance trade-offs, and assesses the potential impact on future quantum cloud architectures.The successful implementation and Qrisp integration of three post-quantum cryptographic (PQC) algorithms demonstrates the feasibility of integrating HE with quantum computing frameworks. The findings indicate that while the Quantum One-Time Pad (QOTP) offers simplicity and low overhead, other algorithms like Chen and Gentry-Sahai-Waters (GSW) present performance trade-offs in terms of runtime and memory consumption. The study results in an overall set of recommendations for securing quantum clouds, e.g. implementing HE at data storage and processing levels, developing Quantum Key Distribution (QKD), and enforcing stringent access control and authentication mechanisms as well as participating in PQC standardization efforts.

</details>
