<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 13]
- [cs.CR](#cs.CR) [Total: 20]
- [cs.AI](#cs.AI) [Total: 25]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Pattern-Based File and Data Access with Python Glob: A Comprehensive Guide for Computational Research](https://arxiv.org/abs/2509.08843)
*Sidney Shapiro*

Main category: cs.SE

TL;DR: 这篇论文介绍了Python glob模块在数据科学、商业分析和人工智能应用中的重要作用，通过具体示例展示如何使用通配符模式进行文件搜索、过滤和数据消化，以支持可扩展的工作流程和可复现研究。


<details>
  <summary>Details</summary>
Motivation: 文件模式访问是计算研究的基础但经常被忽视的方面，本文的动机是为研空人员和实践者提供一个简洁的参考指南，将基础概念与应用实践相结合，使glob模块成为Python研究工作流中文件模式匹配的默认引用。

Method: 通过具体的Python示例，结合pandas、scikit-learn、matplotlib等广泛使用的库，展示如何使用glob模块进行大规模数据消化、组织数据分析、AI数据集构建等应用场景，并在可复现研究和数据工程的更广泛背景下讨论其方法论意义。

Result: 论文展示了glob模块在各种应用场景中的实际效果，包括高效的文件遍历、与分析管道的集成，以及在数据科学、商业分析和AI领域的应用能力，证明了其作为方法论基础构建块的价值。

Conclusion: 本文强调了Python glob模块在现代计算研究中的重要性，它提供了一种简单但功能强大的文件模式匹配方式，能够支持可扩展的工作流程和可复现研究实践，是数据科学和AI应用开发中不可或缺的工具。

Abstract: Pattern-based file access is a fundamental but often under-documented aspect
of computational research. The Python glob module provides a simple yet
powerful way to search, filter, and ingest files using wildcard patterns,
enabling scalable workflows across disciplines. This paper introduces glob as a
versatile tool for data science, business analytics, and artificial
intelligence applications. We demonstrate use cases including large-scale data
ingestion, organizational data analysis, AI dataset construction, and
reproducible research practices. Through concrete Python examples with widely
used libraries such as pandas,scikit-learn, and matplotlib, we show how glob
facilitates efficient file traversal and integration with analytical pipelines.
By situating glob within the broader context of reproducible research and data
engineering, we highlight its role as a methodological building block. Our goal
is to provide researchers and practitioners with a concise reference that
bridges foundational concepts and applied practice, making glob a default
citation for file pattern matching in Python-based research workflows.

</details>


### [2] [A Systematic Mapping Study on Chatbots in Programming Education](https://arxiv.org/abs/2509.08857)
*Marcelino Garcia,Renato Garcia,Arthur Parizotto,Andre Mendes,Pedro Valle,Ricardo Vilela,Renato Balancieri,Williamson Silva*

Main category: cs.SE

TL;DR: 这是一份关于教育聊天机器在编程教育中应用的系统性地图研究，分析了54项研究的趋势和空白


<details>
  <summary>Details</summary>
Motivation: 教育聊天机器在编程教育中趋于普及，需要系统性地研究其开发和应用情况

Method: 采用系统性地图研究方法，从3,216篇文献中选出54项研究，基于5个研究子问题进行分析

Result: 发现Python教学聊天机器占主导地位，主要关注基础编程概念，采用多种教学方法和技术架构

Conclusion: 识别了文献中的趋势和空白，为开发新的编程教育工具提供了见解

Abstract: Educational chatbots have gained prominence as support tools for teaching
programming, particularly in introductory learning contexts. This paper
presents a Systematic Mapping Study (SMS) that investigated how such agents
have been developed and applied in programming education. From an initial set
of 3,216 publications, 54 studies were selected and analyzed based on five
research subquestions, addressing chatbot types, programming languages used,
educational content covered, interaction models, and application contexts. The
results reveal a predominance of chatbots designed for Python instruction,
focusing on fundamental programming concepts, and employing a wide variety of
pedagogical approaches and technological architectures. In addition to
identifying trends and gaps in the literature, this study provides insights to
inform the development of new educational tools for programming instruction.

</details>


### [3] [GeoJSON Agents:A Multi-Agent LLM Architecture for Geospatial Analysis-Function Calling vs Code Generation](https://arxiv.org/abs/2509.08863)
*Qianqian Luo,Liuchang Xu,Qingming Lin,Sensen Wu,Ruichen Mao,Chao Wang,Hailin Feng,Bo Huang,Zhenhong Du*

Main category: cs.SE

TL;DR: 提出了GeoJSON Agents多智能体架构，通过Function Calling和Code Generation两种方法将自然语言任务转换为GeoJSON操作命令，显著提升了GIS自动化性能


<details>
  <summary>Details</summary>
Motivation: LLMs在GIS领域缺乏专业知识，存在局限性，需要专门的方法来处理空间数据和地理信息任务

Method: 三组件架构：任务解析、智能体协作、结果集成。Planner智能体解析自然语言任务，Worker智能体通过函数调用或代码生成执行空间数据处理

Result: Function Calling方法准确率85.71%，Code Generation方法准确率97.14%，均显著优于通用模型(48.57%)

Conclusion: Code Generation方法更灵活，Function Calling方法更稳定，为改进GeoAI系统性能提供了新视角

Abstract: LLMs have made substantial progress in task automation and natural language
understanding.However,without expertise in GIS,they continue to encounter
limitations.To address these issues, we propose GeoJSON Agents-a multi-agent
LLM architecture.This framework transforms natural language tasks into
structured GeoJSON operation commands and processes spatial data using two
widely adopted LLM enhancement techniques:Function Calling and Code
Generation.The architecture consists of three components-task parsing,agent
collaboration,and result integration-aimed at enhancing both the performance
and scalability of GIS automation.The Planner agent interprets natural language
tasks into structured GeoJSON commands.Then,specialized Worker agents
collaborate according to assigned roles to perform spatial data processing and
analysis,either by invoking predefined function APIs or by dynamically
generating and executing Python-based spatial analysis code.Finally,the system
integrates the outputs from multiple execution rounds into
reusable,standards-compliant GeoJSON files.To systematically evaluate the
performance of the two approaches,we constructed a benchmark dataset of 70
tasks with varying complexity and conducted experiments using OpenAI's GPT-4o
as the core model.Results indicate that the Function Calling-based GeoJSON
Agent achieved an accuracy of 85.71%,while the Code Generation-based agent
reached 97.14%,both significantly outperforming the best-performing
general-purpose model (48.57%).Further analysis reveals that the Code
Generation provides greater flexibility,whereas the Function Calling approach
offers more stable execution.This study is the first to introduce an LLM
multi-agent framework for GeoJSON data and to compare the strengths and
limitations of two mainstream LLM enhancement methods,offering new perspectives
for improving GeoAI system performance.

</details>


### [4] [TraceRAG: A LLM-Based Framework for Explainable Android Malware Detection and Behavior Analysis](https://arxiv.org/abs/2509.08865)
*Guangyu Zhang,Xixuan Wang,Shiyu Sun,Peiyan Xiao,Kun Sun,Yanhai Xiong*

Main category: cs.SE

TL;DR: TraceRAG是一个基于检索增强生成(RAG)的框架，通过自然语言查询和Java代码分析实现可解释的Android恶意软件检测，准确率达到96%


<details>
  <summary>Details</summary>
Motivation: 传统分析方法难以恢复深度隐藏的恶意行为或提供人类可读的决策解释，需要更强大的分析框架来应对复杂的Android恶意软件规避策略

Method: 首先生成方法级代码片段的摘要并索引到向量数据库，然后通过行为聚焦问题检索语义相关的代码片段进行深度检查，最后基于多轮分析结果生成包含恶意行为及其代码实现的人类可读报告

Result: 实验结果显示该方法达到96%的恶意软件检测准确率和83.81%的行为识别准确率，基于更新的VirusTotal扫描和人工验证

Conclusion: 专家评估确认了TraceRAG生成报告的实际实用性，证明了该框架在可解释恶意软件分析方面的有效性

Abstract: Sophisticated evasion tactics in malicious Android applications, combined
with their intricate behavioral semantics, enable attackers to conceal
malicious logic within legitimate functions, underscoring the critical need for
robust and in-depth analysis frameworks. However, traditional analysis
techniques often fail to recover deeply hidden behaviors or provide
human-readable justifications for their decisions. Inspired by advances in
large language models (LLMs), we introduce TraceRAG, a retrieval-augmented
generation (RAG) framework that bridges natural language queries and Java code
to deliver explainable malware detection and analysis. First, TraceRAG
generates summaries of method-level code snippets, which are indexed in a
vector database. At query time, behavior-focused questions retrieve the most
semantically relevant snippets for deeper inspection. Finally, based on the
multi-turn analysis results, TraceRAG produces human-readable reports that
present the identified malicious behaviors and their corresponding code
implementations. Experimental results demonstrate that our method achieves 96\%
malware detection accuracy and 83.81\% behavior identification accuracy based
on updated VirusTotal (VT) scans and manual verification. Furthermore, expert
evaluation confirms the practical utility of the reports generated by TraceRAG.

</details>


### [5] [Benchmarking Energy Efficiency of Large Language Models Using vLLM](https://arxiv.org/abs/2509.08867)
*K. Pronk,Q. Zhao*

Main category: cs.SE

TL;DR: 这篇论文提出了LLM效能净化基准测试，使用vLLM后端模拟真实使用场景，分析模型大小、架构和并发请求对推理能耗的影响。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的普及对气候产生了较大影响，当前的能耗评测基准常异于真实生产环境，需要更好反映实际部署条件的能效评测方法。

Method: 使用vLLM高吞吐量生产环境后端，构建LLM效能准基准测试，模拟真实使用场景，重点考察模型大小、架构设计和并发请求量等因素对推理能耗的影响。

Result: 证明了可以构建更好反映实际部署条件的能效评测基准，为开发者提供了有价值的可持续性建议。

Conclusion: 该研究为开发更可持续的AI系统提供了实用的能效评测方法，有助于降低LLM部署的环境影响。

Abstract: The prevalence of Large Language Models (LLMs) is having an growing impact on
the climate due to the substantial energy required for their deployment and
use. To create awareness for developers who are implementing LLMs in their
products, there is a strong need to collect more information about the energy
efficiency of LLMs. While existing research has evaluated the energy efficiency
of various models, these benchmarks often fall short of representing realistic
production scenarios. In this paper, we introduce the LLM Efficiency Benchmark,
designed to simulate real-world usage conditions. Our benchmark utilizes vLLM,
a high-throughput, production-ready LLM serving backend that optimizes model
performance and efficiency. We examine how factors such as model size,
architecture, and concurrent request volume affect inference energy efficiency.
Our findings demonstrate that it is possible to create energy efficiency
benchmarks that better reflect practical deployment conditions, providing
valuable insights for developers aiming to build more sustainable AI systems.

</details>


### [6] [CLARA: A Developer's Companion for Code Comprehension and Analysis](https://arxiv.org/abs/2509.09072)
*Ahmed Adnan,Mushfiqur Rahman,Saad Sakib Noor,Kazi Sakib*

Main category: cs.SE

TL;DR: CLARA是一个基于先进推理模型的浏览器扩展工具，帮助开发者和研究人员进行代码理解、重构和质量检测，经评估显示其实用且准确


<details>
  <summary>Details</summary>
Motivation: 现有代码分析工具需要项目设置、缺乏上下文感知且手动工作量大，需要更便捷的代码理解辅助工具

Method: 开发浏览器扩展CLARA，使用先进推理模型，支持代码文件/片段理解、重构和质量属性检测，并通过数据集评估和10人用户研究进行验证

Result: CLARA在代码理解和分析任务中表现出有用性、准确性和实用性

Conclusion: CLARA是一个开源工具，为开发者和研究人员提供了有效的代码理解与分析解决方案

Abstract: Code comprehension and analysis of open-source project codebases is a task
frequently performed by developers and researchers. However, existing tools
that practitioners use for assistance with such tasks often require prior
project setup, lack context-awareness, and involve significant manual effort.
To address this, we present CLARA, a browser extension that utilizes a
state-of-the-art inference model to assist developers and researchers in: (i)
comprehending code files and code fragments, (ii) code refactoring, and (iii)
code quality attribute detection. We qualitatively evaluated CLARA's inference
model using existing datasets and methodology, and performed a comprehensive
user study with 10 developers and academic researchers to assess its usability
and usefulness. The results show that CLARA is useful, accurate, and practical
in code comprehension and analysis tasks. CLARA is an open-source tool
available at https://github.com/SaadNoor555/CLARA_tool_demo. A video showing
the full capabilities of CLARA can be found at
https://youtu.be/VDKVXvIH41Q?si=qBFsmS_Y4m_9x3YH.

</details>


### [7] [Probing Pre-trained Language Models on Code Changes: Insights from ReDef, a High-Confidence Just-in-Time Defect Prediction Dataset](https://arxiv.org/abs/2509.09192)
*Doha Nam,Taehyoun Kim,Duksan Ryu,Jongmoon Baik*

Main category: cs.SE

TL;DR: 这篇论文提出了ReDef数据集，通过revert commits和GPT辅助筛选构建了高信任度的软件缺陷预测数据集，并系统评估了预训练语言模型在代码修改理解方面的能力和局限性。


<details>
  <summary>Details</summary>
Motivation: 解决现有JIT-SDP数据集中标签噪声大、缺陷识别精度低的问题，以及评估预训练语言模型是否真正理解代码修改语义。

Method: 通过revert commits标记缺陷修改、历史检查验证清洁修改、GPT辅助多次投票审核过滤模糊实例，构建ReDef数据集。对CodeBERT、CodeT5+、UniXcoder进行细调，使用5种编码策略，并通过变换添加/删除块、反转diff构造等反事实干扰探测模型敏感性。

Result: 构建了3,164个缺陷修改和10,268个清洁修改的高信任度数据集。紧凑的diff格式编码在所有PLM中都超过整体函数格式。但反事实干扰测试显示性能减退很少，表明模型依赖表面线索而非真正语义理解。

Conclusion: 与快照基于任务不同，当前的预训练语言模型在理解代码修改方面仍有限，虽然表面上显得稳健，但实际上依赖表面特征而非深层语义理解。

Abstract: Just-in-Time software defect prediction (JIT-SDP) plays a critical role in
prioritizing risky code changes during code review and continuous integration.
However, existing datasets often suffer from noisy labels and low precision in
identifying bug-inducing commits. To address this, we present ReDef
(Revert-based Defect dataset), a high-confidence benchmark of function-level
modifications curated from 22 large-scale C/C++ projects. Defective cases are
anchored by revert commits, while clean cases are validated through post-hoc
history checks. Ambiguous instances are conservatively filtered out via a
GPT-assisted triage process involving multiple votes and audits. This pipeline
yields 3,164 defective and 10,268 clean modifications, offering substantially
more reliable labels than prior existing resources. Beyond dataset
construction, we provide the first systematic evaluation of how pre-trained
language models (PLMs) reason about code modifications -- specifically, which
input encodings most effectively expose change information, and whether models
genuinely capture edit semantics. We fine-tune CodeBERT, CodeT5+, and UniXcoder
under five encoding strategies, and further probe their sensitivity through
counterfactual perturbations that swap added/deleted blocks, invert diff
polarity, or inject spurious markers. Our results show that compact diff-style
encodings consistently outperform whole-function formats across all PLMs, with
statistical tests confirming large, model-independent effects. However, under
counterfactual tests, performance degrades little or not at all -- revealing
that what appears to be robustness in fact reflects reliance on superficial
cues rather than true semantic understanding. These findings indicate that,
unlike in snapshot-based tasks, current PLMs remain limited in their ability to
genuinely comprehend code modifications.

</details>


### [8] [On Integrating Large Language Models and Scenario-Based Programming for Improving Software Reliability](https://arxiv.org/abs/2509.09194)
*Ayelet Berzack,Guy Katz*

Main category: cs.SE

TL;DR: 提出了一种将大语言模型与传统软件工程方法（场景编程范式）相结合的结构化方法，以提高软件开发可靠性并减少错误


<details>
  <summary>Details</summary>
Motivation: 虽然LLMs能显著减少开发时间并生成组织良好的代码，但经常引入严重错误并以说服性自信呈现错误代码，需要更可靠的方式将LLMs整合到软件开发周期中

Method: 结合大语言模型与场景编程范式（SBP），让开发人员注入专业知识并检查验证模型输出，通过结构化方式整合传统软件工程技术

Result: 通过Connect4游戏案例研究，成功创建了能够击败现有强智能体的高能力智能体，并在某些情况下能够形式化验证智能体的正确性

Conclusion: 该方法展示了将LLMs与传统软件工程结合的有效性，提高了开发过程的可靠性和验证能力，同时提供了关于方法易用性的有益见解

Abstract: Large Language Models (LLMs) are fast becoming indispensable tools for
software developers, assisting or even partnering with them in crafting complex
programs. The advantages are evident -- LLMs can significantly reduce
development time, generate well-organized and comprehensible code, and
occasionally suggest innovative ideas that developers might not conceive on
their own. However, despite their strengths, LLMs will often introduce
significant errors and present incorrect code with persuasive confidence,
potentially misleading developers into accepting flawed solutions.
  In order to bring LLMs into the software development cycle in a more reliable
manner, we propose a methodology for combining them with ``traditional''
software engineering techniques in a structured way, with the goal of
streamlining the development process, reducing errors, and enabling users to
verify crucial program properties with increased confidence. Specifically, we
focus on the Scenario-Based Programming (SBP) paradigm -- an event-driven,
scenario-based approach for software engineering -- to allow human developers
to pour their expert knowledge into the LLM, as well as to inspect and verify
its outputs.
  To evaluate our methodology, we conducted a significant case study, and used
it to design and implement the Connect4 game. By combining LLMs and SBP we were
able to create a highly-capable agent, which could defeat various strong
existing agents. Further, in some cases, we were able to formally verify the
correctness of our agent. Finally, our experience reveals interesting insights
regarding the ease-of-use of our proposed approach. The full code of our
case-study will be made publicly available with the final version of this
paper.

</details>


### [9] [Altered Histories in Version Control System Repositories: Evidence from the Trenches](https://arxiv.org/abs/2509.09294)
*Solal Rapaport,Laurent Pautet,Samuel Tardieu,Stefano Zacchiroli*

Main category: cs.SE

TL;DR: 对1.11亿个Git仓库的大规模研究发现，122万个仓库存在历史重写行为，总计870万次历史修改，主要涉及许可证变更和密钥删除等不良实践。


<details>
  <summary>Details</summary>
Motivation: 研究Git历史重写行为对公共代码仓库的影响，包括破坏推送/拉取工作流、威胁仓库完整性和可重现性，以及供应链攻击风险。

Method: 分析Software Heritage存档的1.11亿个代码仓库，识别历史修改行为，并通过两个案例研究分析许可证变更和密钥删除的具体情况。

Result: 发现122万个仓库存在历史重写，总计870万次修改，主要修改文件内容和提交元数据，存在许可证追溯变更和密钥误提交后删除等问题。

Conclusion: Git历史重写行为在公共仓库中普遍存在且存在风险，为此开发了GitHistorian工具来自动检测和描述这些修改行为。

Abstract: Version Control Systems (VCS) like Git allow developers to locally rewrite
recorded history, e.g., to reorder and suppress commits or specific data in
them. These alterations have legitimate use cases, but become problematic when
performed on public branches that have downstream users: they break push/pull
workflows, challenge the integrity and reproducibility of repositories, and
create opportunities for supply chain attackers to sneak into them nefarious
changes. We conduct the first large-scale investigation of Git history
alterations in public code repositories. We analyze 111 M (millions)
repositories archived by Software Heritage, which preserves VCS histories even
across alterations. We find history alterations in 1.22 M repositories, for a
total of 8.7 M rewritten histories. We categorize changes by where they happen
(which repositories, which branches) and what is changed in them (files or
commit metadata). Conducting two targeted case studies we show that altered
histories recurrently change licenses retroactively, or are used to remove
''secrets'' (e.g., private keys) committed by mistake. As these behaviors
correspond to bad practices-in terms of project governance or security
management, respectively-that software recipients might want to avoid, we
introduce GitHistorian, an automated tool, that developers can use to spot and
describe history alterations in public Git repositories.

</details>


### [10] [Cross-Domain Evaluation of Transformer-Based Vulnerability Detection on Open & Industry Data](https://arxiv.org/abs/2509.09313)
*Moritz Mock,Thomas Forrer,Barbara Russo*

Main category: cs.SE

TL;DR: 这篇论文研究了如何将深度学习漏洞检测技术从学术界转移到产业界，开发了AI-DO系统并通过调查验证其实用性。


<details>
  <summary>Details</summary>
Motivation: 学术界深度学习漏洞检测方案在产业环境中应用遇到了可访问性、信任度、系统集成和工作流程等挑战。

Method: 首先评估CodeBERT在产业和开源软件中检测漏洞函数的性能，分析其跨域汇总能力，然后开发了集成到CI/CD流程的AI-DO推荐系统。

Result: 结果显示基于产业数据训练的模型在同域内检测准确，但在开源代码上性能下降；使用适当的下采样技术在开源数据上微调的深度学习模型能够改善漏洞检测。

Conclusion: 研究成功开发了不打断工作流程的CI/CD集成漏洞检测系统，并通过调查验证了其在产业环境中的实用性和接受度。

Abstract: Deep learning solutions for vulnerability detection proposed in academic
research are not always accessible to developers, and their applicability in
industrial settings is rarely addressed. Transferring such technologies from
academia to industry presents challenges related to trustworthiness, legacy
systems, limited digital literacy, and the gap between academic and industrial
expertise. For deep learning in particular, performance and integration into
existing workflows are additional concerns. In this work, we first evaluate the
performance of CodeBERT for detecting vulnerable functions in industrial and
open-source software. We analyse its cross-domain generalisation when
fine-tuned on open-source data and tested on industrial data, and vice versa,
also exploring strategies for handling class imbalance. Based on these results,
we develop AI-DO(Automating vulnerability detection Integration for Developers'
Operations), a Continuous Integration-Continuous Deployment (CI/CD)-integrated
recommender system that uses fine-tuned CodeBERT to detect and localise
vulnerabilities during code review without disrupting workflows. Finally, we
assess the tool's perceived usefulness through a survey with the company's IT
professionals. Our results show that models trained on industrial data detect
vulnerabilities accurately within the same domain but lose performance on
open-source code, while a deep learner fine-tuned on open data, with
appropriate undersampling techniques, improves the detection of
vulnerabilities.

</details>


### [11] [ORCA: Unveiling Obscure Containers In The Wild](https://arxiv.org/abs/2509.09322)
*Jacopo Bufalino,Agathe Blaise,Stefano Secci*

Main category: cs.SE

TL;DR: 这篇论文研究了容器安全分析工具在遇到隐藏/不完整容器时的限制，提出了一种具有弹性的分析方法ORCA，能够在400个常用容器中提高40%的文件覆盖率。


<details>
  <summary>Details</summary>
Motivation: 现代软件开发依赖开源库和第三方组件，容器化环境带来安全风险。当容器文件系统发生无意修改时，会影响SCA工具的可靠性。需要解决SCA工具在分析隐藏容器时的限制。

Method: 分析了600个流行容器，识别隐藏容器的存在问题。提出了一种具有弹性的容器分析方法，并开发了开源实现ORCA。

Result: 发现隐藏容器在知名注册表和可信图像中常见，许多工具无法分析这类容器。ORCA能有效检测隐藏容器内容，与Docker Scout和Syft相比文件覆盖率提高了中位数40%。

Conclusion: 容器隐藏问题是实际存在的安全挑战，ORCA提供了一种有效的解决方案，能够更全面地分析容器内容，提高SCA工具的可靠性。

Abstract: Modern software development increasingly depends on open-source libraries and
third-party components, which are often encapsulated into containerized
environments. While improving the development and deployment of applications,
this approach introduces security risks, particularly when outdated or
vulnerable components are inadvertently included in production environments.
Software Composition Analysis (SCA) is a critical process that helps identify
and manage packages and dependencies inside a container. However, unintentional
modifications to the container filesystem can lead to incomplete container
images, which compromise the reliability of SCA tools. In this paper, we
examine the limitations of both cloud-based and open-source SCA tools when
faced with such obscure images. An analysis of 600 popular containers revealed
that obscure containers exist in well-known registries and trusted images and
that many tools fail to analyze such containers. To mitigate these issues, we
propose an obscuration-resilient methodology for container analysis and
introduce ORCA (Obscuration-Resilient Container Analyzer), its open-source
implementation. We reported our findings to all vendors using their appropriate
channels. Our results demonstrate that ORCA effectively detects the content of
obscure containers and achieves a median 40% improvement in file coverage
compared to Docker Scout and Syft.

</details>


### [12] [LoCoBench: A Benchmark for Long-Context Large Language Models in Complex Software Engineering](https://arxiv.org/abs/2509.09614)
*Jielin Qiu,Zuxin Liu,Zhiwei Liu,Rithesh Murthy,Jianguo Zhang,Haolin Chen,Shiyu Wang,Ming Zhu,Liangwei Yang,Juntao Tan,Zhepeng Cen,Cheng Qian,Shelby Heinecke,Weiran Yao,Silvio Savarese,Caiming Xiong,Huan Wang*

Main category: cs.SE

TL;DR: LoCoBench是一个专门评估长上下文LLM在复杂软件开发场景中表现的基准测试，包含8000个评估场景、10种编程语言、10K到1M token的上下文长度，涵盖8个任务类别和17个评估指标。


<details>
  <summary>Details</summary>
Motivation: 随着支持百万token上下文窗口的语言模型出现，需要专门评估这些模型在理解完整代码库、跨文件推理和维护大型软件系统架构一致性等长上下文能力方面的基准测试。

Method: 通过5阶段流水线系统生成多样化高质量评估场景，涵盖10种编程语言，上下文长度从10K到1M token，包含8个任务类别，并使用包含17个指标的综合性评估框架。

Result: 对最先进的长上下文模型评估显示存在显著的性能差距，表明复杂软件开发中的长上下文理解仍然是一个重大未解决的挑战。

Conclusion: LoCoBench填补了长上下文能力评估的关键空白，揭示了当前模型在复杂软件开发场景中的局限性，为未来研究提供了重要的评估工具。

Abstract: The emergence of long-context language models with context windows extending
to millions of tokens has created new opportunities for sophisticated code
understanding and software development evaluation. We propose LoCoBench, a
comprehensive benchmark specifically designed to evaluate long-context LLMs in
realistic, complex software development scenarios. Unlike existing code
evaluation benchmarks that focus on single-function completion or short-context
tasks, LoCoBench addresses the critical evaluation gap for long-context
capabilities that require understanding entire codebases, reasoning across
multiple files, and maintaining architectural consistency across large-scale
software systems. Our benchmark provides 8,000 evaluation scenarios
systematically generated across 10 programming languages, with context lengths
spanning 10K to 1M tokens, a 100x variation that enables precise assessment of
long-context performance degradation in realistic software development
settings. LoCoBench introduces 8 task categories that capture essential
long-context capabilities: architectural understanding, cross-file refactoring,
multi-session development, bug investigation, feature implementation, code
comprehension, integration testing, and security analysis. Through a 5-phase
pipeline, we create diverse, high-quality scenarios that challenge LLMs to
reason about complex codebases at unprecedented scale. We introduce a
comprehensive evaluation framework with 17 metrics across 4 dimensions,
including 8 new evaluation metrics, combined in a LoCoBench Score (LCBS). Our
evaluation of state-of-the-art long-context models reveals substantial
performance gaps, demonstrating that long-context understanding in complex
software development represents a significant unsolved challenge that demands
more attention. LoCoBench is released at:
https://github.com/SalesforceAIResearch/LoCoBench.

</details>


### [13] [I Know Who Clones Your Code: Interpretable Smart Contract Similarity Detection](https://arxiv.org/abs/2509.09630)
*Zhenguang Liu,Lixun Ma,Zhongzheng Mu,Chengkun Wei,Xiaojun Xu,Yingying Jiao,Kui Ren*

Main category: cs.SE

TL;DR: SmartDetector是一种新颖的智能合约函数相似性检测方法，通过将AST分解为语句树并进行细粒度比较，在三个真实数据集上平均F1分数达到95.88%，比现有方法提升14.01%。


<details>
  <summary>Details</summary>
Motivation: 智能合约开发中广泛重用开源代码导致bug传播，现有AST方法难以处理复杂树结构，深度学习方法忽视代码语法和可解释性，性能不理想。

Method: 将智能合约函数的AST分解为语句树，使用分类器计算函数间语句树对的相似度得分，通过余弦扩散过程高效搜索最优超参数。

Result: 在三个大型真实数据集上的实验表明，SmartDetector平均F1分数达到95.88%，比当前最先进方法平均提升14.01%。

Conclusion: SmartDetector通过细粒度语句级比较有效解决了智能合约函数相似性检测问题，在性能和可解释性方面均优于现有方法。

Abstract: Widespread reuse of open-source code in smart contract development boosts
programming efficiency but significantly amplifies bug propagation across
contracts, while dedicated methods for detecting similar smart contract
functions remain very limited. Conventional abstract-syntax-tree (AST) based
methods for smart contract similarity detection face challenges in handling
intricate tree structures, which impedes detailed semantic comparison of code.
Recent deep-learning based approaches tend to overlook code syntax and
detection interpretability, resulting in suboptimal performance.
  To fill this research gap, we introduce SmartDetector, a novel approach for
computing similarity between smart contract functions, explainable at the
fine-grained statement level. Technically, SmartDetector decomposes the AST of
a smart contract function into a series of smaller statement trees, each
reflecting a structural element of the source code. Then, SmartDetector uses a
classifier to compute the similarity score of two functions by comparing each
pair of their statement trees. To address the infinite hyperparameter space of
the classifier, we mathematically derive a cosine-wise diffusion process to
efficiently search optimal hyperparameters. Extensive experiments conducted on
three large real-world datasets demonstrate that SmartDetector outperforms
current state-of-the-art methods by an average improvement of 14.01% in
F1-score, achieving an overall average F1-score of 95.88%.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [14] [Cross-Service Token: Finding Attacks in 5G Core Networks](https://arxiv.org/abs/2509.08992)
*Anqi Chen,Riccardo Preatoni,Alessandro Brighente,Mauro Conti,Cristina Nita-Rotaru*

Main category: cs.CR

TL;DR: FivGeeFuzz是一个基于语法的模糊测试框架，用于发现5G核心网络服务接口的安全漏洞，在free5GC中发现了8个未知漏洞


<details>
  <summary>Details</summary>
Motivation: 5G核心网络采用基于服务的架构，网络功能通过HTTP API通信，云基础设施容易受到内部攻击，需要研究安全漏洞

Method: 从3GPP API规范自动推导语法，生成畸形、意外或语义不一致的输入，结合自动化错误检测与手动验证和根本原因分析

Result: 在free5GC中发现8个未知漏洞，包括运行时崩溃、错误处理不当和未授权资源访问，其中7个已修复

Conclusion: FivGeeFuzz能有效发现5G核心网络服务接口的安全漏洞，证明了5G架构中访问控制机制的重要性

Abstract: 5G marks a major departure from previous cellular architectures, by
transitioning from a monolithic design of the core network to a Service-Based
Architecture (SBA) where services are modularized as Network Functions (NFs)
which communicate with each other via standard-defined HTTP-based APIs called
Service-Based Interfaces (SBIs). These NFs are deployed in private and public
cloud infrastructure, and an access control framework based on OAuth restricts
how they communicate with each other and obtain access to resources. Given the
increased vulnerabilities of clouds to insiders, it is important to study the
security of the 5G Core services for vulnerabilities that allow attackers to
use compromised NFs to obtain unauthorized access to resources.
  We present FivGeeFuzz, a grammar-based fuzzing framework designed to uncover
security flaws in 5G core SBIs. FivGeeFuzz automatically derives grammars from
3GPP API specifications to generate malformed, unexpected, or semantically
inconsistent inputs, and it integrates automated bug detection with manual
validation and root-cause analysis. We evaluate our approach on free5GC, the
only open-source 5G core implementing Release 17-compliant SBIs with an access
control mechanism. Using FivGeeFuzz, we discovered 8 previously unknown
vulnerabilities in free5GC, leading to runtime crashes, improper error
handling, and unauthorized access to resources, including a very severe attack
we call Cross-Service Token Attack. All bugs were confirmed by the free5GC
team, 7 have already been patched, and the remaining one has a patch under
development.

</details>


### [15] [When FinTech Meets Privacy: Securing Financial LLMs with Differential Private Fine-Tuning](https://arxiv.org/abs/2509.08995)
*Sichen Zhu,Hoyeung Leung,Xiaoyi Wang,Jia Wei,Honghui Xu*

Main category: cs.CR

TL;DR: DPFinLLM是一个专为金融应用设计的轻量级隐私保护大语言模型，结合差分隐私机制和精简架构，在保护用户数据隐私的同时保持高性能。


<details>
  <summary>Details</summary>
Motivation: 随着AI模型在金融边缘设备部署的增加，保护敏感金融数据的隐私成为重要挑战，需要开发既能保护隐私又高效的轻量级LLM解决方案。

Method: 提出DPFinLLM模型，结合鲁棒的差分隐私机制和受先进模型启发的精简架构，专门为设备端金融应用设计。

Result: 在多个金融情感数据集上的广泛实验验证了DPFinLLM的有效性，即使在严格隐私约束下也能达到与完全微调模型相当的性能。

Conclusion: DPFinLLM成功解决了金融领域隐私保护与性能平衡的问题，为设备端金融AI应用提供了安全高效的解决方案。

Abstract: The integration of Large Language Models (LLMs) into financial technology
(FinTech) has revolutionized the analysis and processing of complex financial
data, driving advancements in real-time decision-making and analytics. With the
growing trend of deploying AI models on edge devices for financial
applications, ensuring the privacy of sensitive financial data has become a
significant challenge. To address this, we propose DPFinLLM, a
privacy-enhanced, lightweight LLM specifically designed for on-device financial
applications. DPFinLLM combines a robust differential privacy mechanism with a
streamlined architecture inspired by state-of-the-art models, enabling secure
and efficient processing of financial data. This proposed DPFinLLM can not only
safeguard user data from privacy breaches but also ensure high performance
across diverse financial tasks. Extensive experiments on multiple financial
sentiment datasets validate the effectiveness of DPFinLLM, demonstrating its
ability to achieve performance comparable to fully fine-tuned models, even
under strict privacy constraints.

</details>


### [16] [Beyond Tag Collision: Cluster-based Memory Management for Tag-based Sanitizers](https://arxiv.org/abs/2509.09089)
*Mengfei Xie,Yan Lin,Hongtao Wu,Jianming Fu,Chenke Luo,Guojun Peng*

Main category: cs.CR

TL;DR: ClusterTag是一种基于集群的内存分配器，通过将内存对象分组到独立集群中并在集群间引入随机地址间隔，有效缓解标签冲突问题，提高内存安全检测的确定性。


<details>
  <summary>Details</summary>
Motivation: 现有的标签式内存消毒器由于标签编码空间有限，在时间和空间维度上难以给内存对象分配唯一标签，导致标签冲突和潜在的内存违规检测失败。

Method: 设计集群化内存分配器，将内存对象分组到独立集群中限制标签冲突范围；采用集群粒度堆随机化方案，在集群间引入随机地址间隔，打破标签空间熵限制。

Result: 在Juliet数据集上的安全评估显示，ClusterTag在500次重复测试中表现确定性结果，而现有标签分配策略都存在概率性假阴性；在三个标签冲突距离指标上均实现平衡改进。

Conclusion: ClusterTag能够有效缓解标签冲突问题，提供更可靠的内存安全保护，性能开销控制在1%以内，可与现有标签式消毒器无缝集成。

Abstract: Tag-based sanitizers attach a small "key" to each pointer and a matching
"lock" tag to its target memory object, enabling runtime verification of
pointer-object consistency and helping developers to detect potential memory
violations. However, the limited tag encoding space challenges existing studies
in assigning distinct tags to memory objects across temporal and spatial
dimensions, leading to potential tag collisions. In this paper, we present
ClusterTag, a novel cluster-based memory allocator aimed at simultaneously
mitigating tag collisions in both temporal and spatial dimensions. The core
design of ClusterTag effectively balances the significant mismatch between tag
encoding space and memory objects: it divides memory objects into multiple
independent clusters, thereby limiting tag collisions to finite chunks within
each cluster. To mitigate tag collisions across clusters, we design a
cluster-grained heap randomization scheme. This approach introduces random
address intervals between clusters and further breaks the entropy limitation of
the tag space. ClusterTag has been implemented as an independent memory
allocator that seamlessly integrates with tag-based sanitizers such as HWASan,
and maintains comparable performance overhead (within 1%) at various
randomization densities. Security evaluations on the Juliet dataset indicate
that ClusterTag exhibits deterministic results across 500 repeated tests (5,652
reported and 1,530 missed), while the existing three types of tag assignment
strategies all exhibit probabilistic false negatives due to tag collisions.
Quantitative analysis across three tag collision distance metrics-minimum,
average, and unpredictability-demonstrates that ClusterTag achieves balanced
improvements across all three, whereas prior tag assignment schemes (random,
staggered, fixed) show significant trade-offs in at least one metric.

</details>


### [17] [Towards Confidential and Efficient LLM Inference with Dual Privacy Protection](https://arxiv.org/abs/2509.09091)
*Honglan Yu,Yibin Wang,Feifei Dai,Dong Liu,Haihui Fan,Xiaoyan Gu*

Main category: cs.CR

TL;DR: CMIF是一个保护隐私的高效模型推理框架，通过在客户端TEE中部署嵌入层并在GPU服务器上运行后续层，结合优化的Report-Noisy-Max机制，在保护敏感数据隐私的同时减少TEE推理开销。


<details>
  <summary>Details</summary>
Motivation: 解决TEE中高推理延迟和DP方法影响LLM性能的问题，传统分区方法在LLM密集非线性层中产生显著通信开销，DP方法添加随机噪声会损害模型语义理解能力。

Method: 在客户端TEE中机密部署嵌入层，后续层部署在GPU服务器上；优化Report-Noisy-Max机制来保护敏感输入；在Llama系列模型上进行广泛实验验证。

Result: CMIF显著减少了TEE中的额外推理开销，同时有效保护了用户数据隐私，模型性能仅有轻微下降。

Conclusion: CMIF框架成功解决了TEE和DP方法在LLM隐私保护推理中的局限性，实现了机密性和效率的平衡，为大规模语言模型的隐私保护推理提供了有效解决方案。

Abstract: CPU-based trusted execution environments (TEEs) and differential privacy (DP)
have gained wide applications for private inference. Due to high inference
latency in TEEs, researchers use partition-based approaches that offload linear
model components to GPUs. However, dense nonlinear layers of large language
models (LLMs) result in significant communication overhead between TEEs and
GPUs. DP-based approaches apply random noise to protect data privacy, but this
compromises LLM performance and semantic understanding. To overcome the above
drawbacks, this paper proposes CMIF, a Confidential and efficient Model
Inference Framework. CMIF confidentially deploys the embedding layer in the
client-side TEE and subsequent layers on GPU servers. Meanwhile, it optimizes
the Report-Noisy-Max mechanism to protect sensitive inputs with a slight
decrease in model performance. Extensive experiments on Llama-series models
demonstrate that CMIF reduces additional inference overhead in TEEs while
preserving user data privacy.

</details>


### [18] [DP-FedLoRA: Privacy-Enhanced Federated Fine-Tuning for On-Device Large Language Models](https://arxiv.org/abs/2509.09097)
*Honghui Xu,Shiva Shrestha,Wei Chen,Zhiyuan Li,Zhipeng Cai*

Main category: cs.CR

TL;DR: DP-FedLoRA是一个隐私增强的联邦微调框架，将LoRA适配与差分隐私结合，在边缘设备上实现隐私保护的LLM微调。


<details>
  <summary>Details</summary>
Motivation: 随着设备端大语言模型系统的普及，联邦微调涉及处理敏感的用户特定数据，引发了联邦学习框架中的隐私担忧。

Method: 每个客户端使用高斯噪声对LoRA矩阵进行本地裁剪和扰动，满足(ε,δ)-差分隐私要求，并提供理论分析证明更新的无偏性和噪声引入的方差界限。

Result: 在主流基准测试中，DP-FedLoRA在提供强大隐私保证的同时表现出有竞争力的性能。

Conclusion: 该方法为在设备端环境中实现可扩展且隐私保护的LLM部署铺平了道路。

Abstract: As on-device large language model (LLM) systems become increasingly
prevalent, federated fine-tuning enables advanced language understanding and
generation directly on edge devices; however, it also involves processing
sensitive, user-specific data, raising significant privacy concerns within the
federated learning framework. To address these challenges, we propose
DP-FedLoRA, a privacy-enhanced federated fine-tuning framework that integrates
LoRA-based adaptation with differential privacy in a communication-efficient
setting. Each client locally clips and perturbs its LoRA matrices using
Gaussian noise to satisfy ($\epsilon$, $\delta$)-differential privacy. We
further provide a theoretical analysis demonstrating the unbiased nature of the
updates and deriving bounds on the variance introduced by noise, offering
practical guidance for privacy-budget calibration. Experimental results across
mainstream benchmarks show that DP-FedLoRA delivers competitive performance
while offering strong privacy guarantees, paving the way for scalable and
privacy-preserving LLM deployment in on-device environments.

</details>


### [19] [AgriSentinel: Privacy-Enhanced Embedded-LLM Crop Disease Alerting System](https://arxiv.org/abs/2509.09103)
*Chanti Raju Mylay,Bobin Deng,Zhipeng Cai,Honghui Xu*

Main category: cs.CR

TL;DR: AgriSentinel是一个基于嵌入式LLM的隐私增强型作物病害预警系统，通过差分隐私机制保护数据，在移动设备上实现高效病害分类，并提供可操作的病害管理建议。


<details>
  <summary>Details</summary>
Motivation: 现有作物病害预警系统忽视数据隐私、市场定价权和农民友好性等问题，使农民面临隐私泄露和经济剥削风险，需要开发更全面的解决方案。

Method: 采用差分隐私机制保护敏感作物图像数据，开发轻量级深度学习病害分类模型优化移动设备部署，并集成微调的本地大语言模型提供具体管理建议。

Result: 实验验证系统能有效保护数据隐私，保持高分类性能，并提供实用的病害管理策略，实现自动化病害预警和管理。

Conclusion: AgriSentinel为农民提供了强大、友好的作物病害预警管理解决方案，有助于改善农业决策和提高作物生产力。

Abstract: Crop diseases pose significant threats to global food security, agricultural
productivity, and sustainable farming practices, directly affecting farmers'
livelihoods and economic stability. To address the growing need for effective
crop disease management, AI-based disease alerting systems have emerged as
promising tools by providing early detection and actionable insights for timely
intervention. However, existing systems often overlook critical aspects such as
data privacy, market pricing power, and farmer-friendly usability, leaving
farmers vulnerable to privacy breaches and economic exploitation. To bridge
these gaps, we propose AgriSentinel, the first Privacy-Enhanced Embedded-LLM
Crop Disease Alerting System. AgriSentinel incorporates a differential privacy
mechanism to protect sensitive crop image data while maintaining classification
accuracy. Its lightweight deep learning-based crop disease classification model
is optimized for mobile devices, ensuring accessibility and usability for
farmers. Additionally, the system includes a fine-tuned, on-device large
language model (LLM) that leverages a curated knowledge pool to provide farmers
with specific, actionable suggestions for managing crop diseases, going beyond
simple alerting. Comprehensive experiments validate the effectiveness of
AgriSentinel, demonstrating its ability to safeguard data privacy, maintain
high classification performance, and deliver practical, actionable disease
management strategies. AgriSentinel offers a robust, farmer-friendly solution
for automating crop disease alerting and management, ultimately contributing to
improved agricultural decision-making and enhanced crop productivity.

</details>


### [20] [CryptGNN: Enabling Secure Inference for Graph Neural Networks](https://arxiv.org/abs/2509.09107)
*Pritam Sen,Yao Ma,Cristian Borcea*

Main category: cs.CR

TL;DR: CryptGNN是一个基于安全多方计算的安全图神经网络推理解决方案，保护客户端数据和模型隐私


<details>
  <summary>Details</summary>
Motivation: 解决云环境中第三方GNN模型推理时的隐私保护问题，防止云服务提供商和模型所有者获取敏感数据

Method: 使用分布式安全多方计算技术实现安全消息传递和特征变换层，支持任意数量的SMPC参与方，无需可信服务器

Result: 理论分析和实验证明CryptGNN具有安全性和高效性，即使P-1个云参与方合谋也能保证安全

Conclusion: CryptGNN为MLaaS场景下的GNN推理提供了有效的隐私保护解决方案

Abstract: We present CryptGNN, a secure and effective inference solution for
third-party graph neural network (GNN) models in the cloud, which are accessed
by clients as ML as a service (MLaaS). The main novelty of CryptGNN is its
secure message passing and feature transformation layers using distributed
secure multi-party computation (SMPC) techniques. CryptGNN protects the
client's input data and graph structure from the cloud provider and the
third-party model owner, and it protects the model parameters from the cloud
provider and the clients. CryptGNN works with any number of SMPC parties, does
not require a trusted server, and is provably secure even if P-1 out of P
parties in the cloud collude. Theoretical analysis and empirical experiments
demonstrate the security and efficiency of CryptGNN.

</details>


### [21] [Character-Level Perturbations Disrupt LLM Watermarks](https://arxiv.org/abs/2509.09112)
*Zhaoxi Zhang,Xiaomei Zhang,Yanjun Zhang,He Zhang,Shirui Pan,Bo Liu,Asif Qumer Gill,Leo Yu Zhang*

Main category: cs.CR

TL;DR: 这篇论文提出了一种基于字符级批逆的高效水印移除攻击方法，通过打断标记化过程同时影响多个标记，在限制性威胁模型下显示了更好的水印移除效果。


<details>
  <summary>Details</summary>
Motivation: 现有水印移除攻击方法通常不优，造成了需要大规模批逆或强大攻击者才能有效移除水印的误解。论文旨在突破这一偏见，提出更有效的移除方法。

Method: 首先形式化LLM水印系统模型，分析不同类型批逆的攻击范围。发现字符级批逆（如错别字、交换、删除、同形异体字）通过打断标记化过程可同时影响多个标记。提出基于遗传算法的导向移除攻击，利用参考检测器进行优化。还提出了适应性复合字符级攻击。

Result: 实验结果证明字符级批逆在最严格的威胁模型下显著更有效。在限制黑盒查询的实际威胁模型下，GA方法展现出强大的移除性能。适应性复合攻击能够有效突破潜在防御措施。

Conclusion: 现有LLM水印方案存在重大漏洞，字符级批逆攻出显著提高了水印移除效果。论文强调了开发新的稳健水印机制的紧迫性，因为任何固定防御都可能被适应性攻击突破。

Abstract: Large Language Model (LLM) watermarking embeds detectable signals into
generated text for copyright protection, misuse prevention, and content
detection. While prior studies evaluate robustness using watermark removal
attacks, these methods are often suboptimal, creating the misconception that
effective removal requires large perturbations or powerful adversaries.
  To bridge the gap, we first formalize the system model for LLM watermark, and
characterize two realistic threat models constrained on limited access to the
watermark detector. We then analyze how different types of perturbation vary in
their attack range, i.e., the number of tokens they can affect with a single
edit. We observe that character-level perturbations (e.g., typos, swaps,
deletions, homoglyphs) can influence multiple tokens simultaneously by
disrupting the tokenization process. We demonstrate that character-level
perturbations are significantly more effective for watermark removal under the
most restrictive threat model. We further propose guided removal attacks based
on the Genetic Algorithm (GA) that uses a reference detector for optimization.
Under a practical threat model with limited black-box queries to the watermark
detector, our method demonstrates strong removal performance. Experiments
confirm the superiority of character-level perturbations and the effectiveness
of the GA in removing watermarks under realistic constraints. Additionally, we
argue there is an adversarial dilemma when considering potential defenses: any
fixed defense can be bypassed by a suitable perturbation strategy. Motivated by
this principle, we propose an adaptive compound character-level attack.
Experimental results show that this approach can effectively defeat the
defenses. Our findings highlight significant vulnerabilities in existing LLM
watermark schemes and underline the urgency for the development of new robust
mechanisms.

</details>


### [22] [IoTFuzzSentry: A Protocol Guided Mutation Based Fuzzer for Automatic Vulnerability Testing in Commercial IoT Devices](https://arxiv.org/abs/2509.09158)
*Priyanka Rushikesh Chaudhary,Rajib Ranjan Maiti*

Main category: cs.CR

TL;DR: 通过变异基的协议模糊测试工具IoTFuzzSentry，发现并利用商用IoT设备中的安全漏洞，包括身份凭据泄漏、视频流抠发等多类威胁，并成功申请CVE漏洞编号。


<details>
  <summary>Details</summary>
Motivation: 因为IoT设备在运营阶段运行轻量服务器，但输送层或应用层安全机制的实现缺陷可能导致未授权访问和数据泄漏等风险。

Method: 开发变异基的模糊测试工具IoTFuzzSentry，向IoT通信注入精心构造的输送层和应用层数据包，并集成到Cotopaxi测试工具中。使用IP摄像头和智能插座等商用设备进行评估。

Result: 发现了4类漏洞：IoT身份凭据泄漏、潜入实时视频流、潜入实时图片、IoT命令注入。已申请2个CVE（CVE-2024-41623和CVE-2024-42531），1个待办理。对6款额外设备的分析显示存在相似漏洞。

Conclusion: IoTFuzzSentry能够自动化发现非传统安全威胁，帮助IoT厂商以可忽略的开销加强商用设备的安全性。

Abstract: Protocol fuzzing is a scalable and cost-effective technique for identifying
security vulnerabilities in deployed Internet of Things devices. During their
operational phase, IoT devices often run lightweight servers to handle user
interactions, such as video streaming or image capture in smart cameras.
Implementation flaws in transport or application-layer security mechanisms can
expose IoT devices to a range of threats, including unauthorized access and
data leakage. This paper addresses the challenge of uncovering such
vulnerabilities by leveraging protocol fuzzing techniques that inject crafted
transport and application-layer packets into IoT communications. We present a
mutation-based fuzzing tool, named IoTFuzzSentry, to identify specific
non-trivial vulnerabilities in commercial IoT devices. We further demonstrate
how these vulnerabilities can be exploited in real-world scenarios. We
integrated our fuzzing tool into a well-known testing tool Cotopaxi and
evaluated it with commercial-off-the-shelf IoT devices such as IP cameras and
Smart Plug. Our evaluation revealed vulnerabilities categorized into 4 types
(IoT Access Credential Leakage, Sneak IoT Live Video Stream, Creep IoT Live
Image, IoT Command Injection) and we show their exploits using three IoT
devices. We have responsibly disclosed all these vulnerabilities to the
respective vendors. So far, we have published two CVEs, CVE-2024-41623 and
CVE-2024-42531, and one is awaiting. To extend the applicability, we have
investigated the traffic of six additional IoT devices and our analysis shows
that these devices can have similar vulnerabilities, due to the presence of a
similar set of application protocols. We believe that IoTFuzzSentry has the
potential to discover unconventional security threats and allow IoT vendors to
strengthen the security of their commercialized IoT devices automatically with
negligible overhead.

</details>


### [23] [Enhancing Cyber Threat Hunting -- A Visual Approach with the Forensic Visualization Toolkit](https://arxiv.org/abs/2509.09185)
*Jihane Najar,Marinos Tsantekidis,Aris Sotiropoulos,Vassilis Prevelakis*

Main category: cs.CR

TL;DR: 本文介绍了Forensic Visualization Toolkit (FVT)，这是一个用于数字取证调查和网络安全态势感知的强大可视化工具，已在多个欧盟研究项目中应用和改进。


<details>
  <summary>Details</summary>
Motivation: 在动态网络威胁环境中，传统安全措施可能无法检测到高级威胁，需要主动的威胁狩猎方法来增强网络安全防御能力。

Method: 开发了Forensic Visualization Toolkit (FVT)，这是一个直观交互的工具，用于数字证据分析和高级可视化，支持网络安全态势感知和风险管理。

Result: 通过实际场景演示，FVT显著增强了网络安全专业人员的威胁识别、分析和响应能力，并已在多个欧盟项目中成功集成和应用。

Conclusion: FVT作为一个有效的可视化工具，能够提升网络安全威胁狩猎的效率和效果，为安全分析师提供强大的支持。

Abstract: In today's dynamic cyber threat landscape, organizations must take proactive
steps to bolster their cybersecurity defenses. Cyber threat hunting is a
proactive and iterative process aimed at identifying and mitigating advanced
threats that may go undetected by traditional security measures. Rather than
waiting for automated security systems to flag potential threats, threat
hunting involves actively searching for signs of malicious activity within an
organization's network. In this paper, we present the Forensic Visualization
Toolkit, a powerful tool designed for digital forensics investigations,
analysis of digital evidence, and advanced visualizations to enhance
cybersecurity situational awareness and risk management and empower security
analysts with an intuitive and interactive tool. Through practical, real-world
scenarios, we demonstrate how FVT significantly amplifies the capabilities of
cybersecurity professionals, enabling them to effectively identify, analyze,
and respond to threats. Furthermore, it is important to highlight that FVT has
been integrated into, utilized, and continually enhanced within various
EU-funded research projects over recent years.

</details>


### [24] [Shell or Nothing: Real-World Benchmarks and Memory-Activated Agents for Automated Penetration Testing](https://arxiv.org/abs/2509.09207)
*Wuyuao Mai,Geng Hong,Qi Liu,Jinsong Chen,Jiarun Dai,Xudong Pan,Yuan Zhang,Min Yang*

Main category: cs.CR

TL;DR: 提出了第一个真实世界的渗透测试基准TermiBench和新型多代理框架TermiAgent，解决了现有AI渗透测试在简化CTF环境中表现良好但在真实场景中效果差的问题。


<details>
  <summary>Details</summary>
Motivation: 传统渗透测试成本高、耗时长且依赖专家，现有AI驱动的渗透测试代理在简化的CTF环境中表现良好，但在真实复杂环境中效果不佳，需要更真实的评估基准和方法。

Method: 开发TermiBench基准测试（510个主机、25种服务、30个CVE），提出TermiAgent多代理框架，采用定位记忆激活机制缓解长上下文遗忘，通过结构化代码理解构建可靠的漏洞利用库。

Result: 现有系统在真实条件下几乎无法获得系统shell，TermiAgent在评估中优于最先进代理，表现出更强的渗透测试能力，减少执行时间和成本，甚至在笔记本电脑规模部署中也具有实用性。

Conclusion: 该工作提供了首个开源的真实世界自主渗透测试基准和新型代理框架，为AI驱动的渗透测试建立了里程碑。

Abstract: Penetration testing is critical for identifying and mitigating security
vulnerabilities, yet traditional approaches remain expensive, time-consuming,
and dependent on expert human labor. Recent work has explored AI-driven
pentesting agents, but their evaluation relies on oversimplified
capture-the-flag (CTF) settings that embed prior knowledge and reduce
complexity, leading to performance estimates far from real-world practice. We
close this gap by introducing the first real-world, agent-oriented pentesting
benchmark, TermiBench, which shifts the goal from 'flag finding' to achieving
full system control. The benchmark spans 510 hosts across 25 services and 30
CVEs, with realistic environments that require autonomous reconnaissance,
discrimination between benign and exploitable services, and robust exploit
execution. Using this benchmark, we find that existing systems can hardly
obtain system shells under realistic conditions.
  To address these challenges, we propose TermiAgent, a multi-agent penetration
testing framework. TermiAgent mitigates long-context forgetting with a Located
Memory Activation mechanism and builds a reliable exploit arsenal via
structured code understanding rather than naive retrieval. In evaluations, our
work outperforms state-of-the-art agents, exhibiting stronger penetration
testing capability, reducing execution time and financial cost, and
demonstrating practicality even on laptop-scale deployments. Our work delivers
both the first open-source benchmark for real-world autonomous pentesting and a
novel agent framework that establishes a milestone for AI-driven penetration
testing.

</details>


### [25] [A Cyber-Twin Based Honeypot for Gathering Threat Intelligence](https://arxiv.org/abs/2509.09222)
*Muhammad Azmi Umer,Zhan Xuna,Yan Lin Aung,Aditya P. Mathur,Jianying Zhou*

Main category: cs.CR

TL;DR: 基于水处理厂组织的近似系统构建的麻雀罐，用于吸引和分析网络攻击，为关键基础设施提供威胁情报


<details>
  <summary>Details</summary>
Motivation: 关键基础设施容易受到网络攻击，需要有效的防护技术来应对这些威胁

Method: 开发了一个基于水处理厂组织近似系统的麻雀罐，作为实际系统的实际副本，用于吸引潜在攻击者

Result: 该麻雀罐已经运行，并多次受到攻击（包括劫持病毒攻击），攻击行为被记录和分析

Conclusion: 通过麻雀罐获得的威胁情报可以分享给水处理厂管理者，以改善厂区保护系统

Abstract: Critical Infrastructure (CI) is prone to cyberattacks. Several techniques
have been developed to protect CI against such attacks. In this work, we
describe a honeypot based on a cyber twin for a water treatment plant. The
honeypot is intended to serve as a realistic replica of a water treatment plant
that attracts potential attackers. The attacks launched on the honeypot are
recorded and analyzed for threat intelligence. The intelligence so obtained is
shared with the management of water treatment plants, who in turn may use it to
improve plant protection systems. The honeypot used here is operational and has
been attacked on several occasions using, for example, a ransomware attack that
is described in detail.

</details>


### [26] [What You Code Is What We Prove: Translating BLE App Logic into Formal Models with LLMs for Vulnerability Detection](https://arxiv.org/abs/2509.09291)
*Biwei Yan,Yue Zhang,Minghui Xu,Runyu Pan,Jinku Li,Xiuzhen Cheng*

Main category: cs.CR

TL;DR: VerifiaBLE系统利用LLM将BLE应用代码转换为可验证的形式模型，实现对Android BLE应用的大规模安全验证，发现大多数应用存在严重的安全保护缺失。


<details>
  <summary>Details</summary>
Motivation: BLE应用层安全漏洞日益增多，开发者经常忽略加密、认证和新鲜性等关键保护措施，而传统形式验证需要大量人工建模工作，难以进行大规模分析。

Method: 将BLE安全分析重构为语义翻译问题，使用LLM作为翻译器将BLE特定代码转换为ProVerif可验证的过程模型，结合静态分析、提示引导的LLM翻译和符号验证来检查加密、随机性和认证三个核心安全特性。

Result: 在1,050个Android BLE应用的分析中，只有10.2%的应用实现了所有三种保护措施，而53.9%的应用完全忽略了这些保护。

Conclusion: 使用LLM作为结构化翻译器可以降低形式化方法的门槛，在安全关键领域实现可扩展的验证。

Abstract: The application layer of Bluetooth Low Energy (BLE) is a growing source of
security vulnerabilities, as developers often neglect to implement critical
protections such as encryption, authentication, and freshness. While formal
verification offers a principled way to check these properties, the manual
effort of constructing formal models makes it impractical for large-scale
analysis. This paper introduces a key insight: BLE application security
analysis can be reframed as a semantic translation problem, i.e., from
real-world code to formal models. We leverage large language models (LLMs) not
to directly detect vulnerabilities, but to serve as translators that convert
BLE-specific code into process models verifiable by tools like ProVerif. We
implement this idea in VerifiaBLE, a system that combines static analysis,
prompt-guided LLM translation, and symbolic verification to check three core
security features: encryption, randomness, and authentication. Applied to 1,050
Android BLE apps, VerifiaBLE uncovers systemic weaknesses: only 10.2\% of apps
implement all three protections, while 53.9\% omit them entirely. Our work
demonstrates that using LLMs as structured translators can lower the barrier to
formal methods, unlocking scalable verification across security-critical
domains.

</details>


### [27] [On the Security of SSH Client Signatures](https://arxiv.org/abs/2509.09331)
*Fabian Bäumer,Marcus Brinkmann,Maximilian Radoy,Jörg Schwenk,Juraj Somorovsky*

Main category: cs.CR

TL;DR: 该研究通过收集和分析SSH客户端公钥，发现ECDSA确定性nonce算法存在安全漏洞，可导致私钥泄露。PuTTY客户端因此漏洞被分配CVE-2024-31497。


<details>
  <summary>Details</summary>
Motivation: SSH客户端密钥的安全性研究相对缺乏，特别是客户端无法通过互联网扫描进行测量。研究旨在填补这一空白，系统分析SSH客户端密钥的安全状况。

Method: 1) 从GitHub和GitLab等开放开发平台收集SSH客户端公钥，进行纵向安全测试；2) 对24个流行SSH客户端的签名算法实现进行黑盒实验分析

Result: 收集了31,622,338个密钥，发现98个损坏的短密钥、139个弱随机性生成的密钥、149个具有公共或小因子的密钥。首次证明ECDSA确定性nonce在P-521曲线下仅需58个有效签名即可恢复PuTTY私钥

Conclusion: 虽然大多数SSH密钥没有暴露弱点，但ECDSA确定性nonce算法存在严重安全漏洞，需要引起重视。研究推动了PuTTY修复相关漏洞。

Abstract: Administrators and developers use SSH client keys and signatures for
authentication, for example, to access internet backbone servers or to commit
new code on platforms like GitHub. However, unlike servers, SSH clients cannot
be measured through internet scans. We close this gap in two steps. First, we
collect SSH client public keys. Such keys are regularly published by their
owners on open development platforms like GitHub and GitLab. We systematize
previous non-academic work by subjecting these keys to various security tests
in a longitudinal study. Second, in a series of black-box lab experiments, we
analyze the implementations of algorithms for SSH client signatures in 24
popular SSH clients for Linux, Windows, and macOS.
  We extracted 31,622,338 keys from three public sources in two scans. Compared
to previous work, we see a clear tendency to abandon RSA signatures in favor of
EdDSA signatures. Still, in January 2025, we found 98 broken short keys, 139
keys generated from weak randomness, and 149 keys with common or small
factors-the large majority of the retrieved keys exposed no weakness.
  Weak randomness can not only compromise a secret key through its public key,
but also through signatures. It is well-known that a bias in random nonces in
ECDSA can reveal the secret key through public signatures. For the first time,
we show that the use of deterministic nonces in ECDSA can also be dangerous:
The private signing key of a PuTTY client can be recovered from just 58 valid
signatures if ECDSA with NIST curve P-521 is used. PuTTY acknowledged our
finding in CVE-2024-31497, and they subsequently replaced the nonce generation
algorithm.

</details>


### [28] [[Extended] Ethics in Computer Security Research: A Data-Driven Assessment of the Past, the Present, and the Possible Future](https://arxiv.org/abs/2509.09351)
*Harshini Sri Ramulu,Helen Schmitt,Bogdan Rerich,Rachel Gonzalez Rodriguez,Tadayoshi Kohno,Yasemin Acar*

Main category: cs.CR

TL;DR: 该研究分析了2024年1154篇顶级安全论文的伦理报告现状，发现存在不一致性，主要关注机构审批和人类受试者保护，缺乏对利益权衡的讨论。通过访谈24位研究人员，发现虽然渴望伦理研究，但缺乏一致的价值观、伦理框架和决策标准。


<details>
  <summary>Details</summary>
Motivation: 计算机安全研究缺乏明确的伦理决策指导，研究人员在道德模糊情况下难以做出、记录和评估伦理决策，需要系统性的伦理分析框架。

Method: 1) 回顾分析2024年所有1154篇顶级安全论文的伦理报告情况；2) 对24位计算机安全和隐私研究人员进行半结构化访谈研究，包括作者、审稿人、伦理委员会成员和程序主席。

Result: 发现伦理报告水平不一致，过度关注机构审批和人类受试者保护，缺乏利益权衡讨论；研究人员渴望伦理研究但缺乏一致的价值观、伦理框架和决策标准。

Conclusion: 提出了改进计算机安全研究伦理状况的建议，包括建立更系统的伦理评估框架和标准化报告要求，以提升该领域的伦理研究水平。

Abstract: Ethical questions are discussed regularly in computer security. Still,
researchers in computer security lack clear guidance on how to make, document,
and assess ethical decisions in research when what is morally right or
acceptable is not clear-cut. In this work, we give an overview of the
discussion of ethical implications in current published work in computer
security by reviewing all 1154 top-tier security papers published in 2024,
finding inconsistent levels of ethics reporting with a strong focus of
reporting institutional or ethics board approval, human subjects protection,
and responsible disclosure, and a lack of discussion of balancing harms and
benefits. We further report on the results of a semi-structured interview study
with 24 computer security and privacy researchers (among whom were also:
reviewers, ethics committee members, and/or program chairs) and their ethical
decision-making both as authors and during peer review, finding a strong desire
for ethical research, but a lack of consistency in considered values, ethical
frameworks (if articulated), decision-making, and outcomes. We present an
overview of the current state of the discussion of ethics and current de-facto
standards in computer security research, and contribute suggestions to improve
the state of ethics in computer security research.

</details>


### [29] [ENSI: Efficient Non-Interactive Secure Inference for Large Language Models](https://arxiv.org/abs/2509.09424)
*Zhiyu He,Maojiang Wang,Xinwen Gao,Yuchuan Luo,Lin Liu,Shaojing Fu*

Main category: cs.CR

TL;DR: ENSI是一个创新的非交互式安全推理框架，通过密码协议与LLM架构的协同设计，显著提升大语言模型的安全推理效率


<details>
  <summary>Details</summary>
Motivation: 现有密码协议与大规模语言模型结合时存在计算复杂度过高、实用性受限的问题，需要解决加密矩阵乘法、softmax计算和bootstrapping操作等瓶颈

Method: 采用优化的编码策略整合CKKS方案与轻量级BitNet模型，使用sigmoid注意力机制替代softmax，并将Bootstrapping操作嵌入RMSNorm过程中

Result: 实验显示ENSI在矩阵乘法上实现约8倍加速，softmax推理速度提升2.6倍，bootstrapping比例降至仅1%

Conclusion: ENSI框架通过架构与密码协议的协同设计，有效解决了LLM安全推理中的关键性能瓶颈，为隐私保护机器学习提供了实用解决方案

Abstract: Secure inference enables privacy-preserving machine learning by leveraging
cryptographic protocols that support computations on sensitive user data
without exposing it. However, integrating cryptographic protocols with large
language models (LLMs) presents significant challenges, as the inherent
complexity of these protocols, together with LLMs' massive parameter scale and
sophisticated architectures, severely limits practical usability. In this work,
we propose ENSI, a novel non-interactive secure inference framework for LLMs,
based on the principle of co-designing the cryptographic protocols and LLM
architecture. ENSI employs an optimized encoding strategy that seamlessly
integrates CKKS scheme with a lightweight LLM variant, BitNet, significantly
reducing the computational complexity of encrypted matrix multiplications. In
response to the prohibitive computational demands of softmax under homomorphic
encryption (HE), we pioneer the integration of the sigmoid attention mechanism
with HE as a seamless, retraining-free alternative. Furthermore, by embedding
the Bootstrapping operation within the RMSNorm process, we efficiently refresh
ciphertexts while markedly decreasing the frequency of costly bootstrapping
invocations. Experimental evaluations demonstrate that ENSI achieves
approximately an 8x acceleration in matrix multiplications and a 2.6x speedup
in softmax inference on CPU compared to state-of-the-art method, with the
proportion of bootstrapping is reduced to just 1%.

</details>


### [30] [Prompt Pirates Need a Map: Stealing Seeds helps Stealing Prompts](https://arxiv.org/abs/2509.09488)
*Felix Mächtle,Ashwath Shetty,Jonas Sander,Nils Loose,Sören Pirk,Thomas Eisenbarth*

Main category: cs.CR

TL;DR: 本文研究了扩散模型中的提示词窃取攻击，发现基于数值优化的提示词恢复方法存在根本性局限性，并利用PyTorch在CPU上生成初始随机噪声时种子值范围受限的漏洞（CWE-339），开发了种子恢复工具SeedSnitch和基于遗传算法的提示词窃取方法PromptPirate，显著提升了窃取效果，同时提出了有效的防御措施。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在文本到图像生成方面取得重大进展，生成的图像具有很高的知识产权和经济价值。提示词窃取对安全和隐私构成严重威胁，需要研究相关的攻击方法和防御措施。

Method: 1) 识别并利用PyTorch在CPU生成初始随机噪声时种子值范围受限的漏洞（CWE-339）；2) 开发SeedSnitch工具对CivitAI平台上的图像进行大规模种子暴力破解；3) 提出基于遗传算法的PromptPirate方法进行提示词窃取优化

Result: 1) 约95%的图像种子值可在140分钟内被暴力破解；2) PromptPirate相比现有最佳方法（PromptStealer、P2HP、CLIP-Interrogator）在LPIPS相似度上提升8-11%；3) 提出了有效的防御措施使种子窃取和基于优化的提示词窃取失效

Conclusion: 扩散模型存在严重的提示词窃取安全漏洞，通过利用噪声生成漏洞可以高效恢复种子并窃取提示词。研究提出了有效的攻击方法和防御措施，已与开发者合作进行协调缓解工作。

Abstract: Diffusion models have significantly advanced text-to-image generation,
enabling the creation of highly realistic images conditioned on textual prompts
and seeds. Given the considerable intellectual and economic value embedded in
such prompts, prompt theft poses a critical security and privacy concern. In
this paper, we investigate prompt-stealing attacks targeting diffusion models.
We reveal that numerical optimization-based prompt recovery methods are
fundamentally limited as they do not account for the initial random noise used
during image generation. We identify and exploit a noise-generation
vulnerability (CWE-339), prevalent in major image-generation frameworks,
originating from PyTorch's restriction of seed values to a range of $2^{32}$
when generating the initial random noise on CPUs. Through a large-scale
empirical analysis conducted on images shared via the popular platform CivitAI,
we demonstrate that approximately 95% of these images' seed values can be
effectively brute-forced in 140 minutes per seed using our seed-recovery tool,
SeedSnitch. Leveraging the recovered seed, we propose PromptPirate, a genetic
algorithm-based optimization method explicitly designed for prompt stealing.
PromptPirate surpasses state-of-the-art methods, i.e., PromptStealer, P2HP, and
CLIP-Interrogator, achieving an 8-11% improvement in LPIPS similarity.
Furthermore, we introduce straightforward and effective countermeasures that
render seed stealing, and thus optimization-based prompt stealing, ineffective.
We have disclosed our findings responsibly and initiated coordinated mitigation
efforts with the developers to address this critical vulnerability.

</details>


### [31] [What Does Normal Even Mean? Evaluating Benign Traffic in Intrusion Detection Datasets](https://arxiv.org/abs/2509.09564)
*Meghan Wilkinson,Robert H Thomson*

Main category: cs.CR

TL;DR: 这篇论文通过无监督聚类技术分析了几个众多网络入侵检测数据集中的良性流量结构，发现其中存在有意义的子类别，这可能提高多类分类性能。


<details>
  <summary>Details</summary>
Motivation: 目前网络入侵检测数据集通常将所有非攻击流量标记为单一的大型良性类别，而忽略了其内部可能存在的有意义子结构。这种处理方式可能影响监督学习算法的性能。

Method: 使用HDBSCAN、Mean Shift等无监督聚类技术，对NSL-KDD、UNSW-NB15和CIC-IDS 2017等常用入侵检测数据集中的良性流量进行结构分析，识别其中的子类别。

Result: 研究发现良性流量内部确实存在有意义的子类别结构，这些子类别的识别和利用可能提高多类分类算法的整体性能。

Conclusion: 传统的单一良性类标签方式存在限制，通过无监督聚类技术识别良性流量内部的子结构，可以为网络入侵检测系统的性能优化提供新的视角和方法。

Abstract: Supervised machine learning techniques rely on labeled data to achieve high
task performance, but this requires the labels to capture some meaningful
differences in the underlying data structure. For training network intrusion
detection algorithms, most datasets contain a series of attack classes and a
single large benign class which captures all non-attack network traffic. A
review of intrusion detection papers and guides that explicitly state their
data preprocessing steps identified that the majority took the labeled
categories of the dataset at face value when training their algorithms. The
present paper evaluates the structure of benign traffic in several common
intrusion detection datasets (NSL-KDD, UNSW-NB15, and CIC-IDS 2017) and
determines whether there are meaningful sub-categories within this traffic
which may improve overall multi-classification performance using common machine
learning techniques. We present an overview of some unsupervised clustering
techniques (e.g., HDBSCAN, Mean Shift Clustering) and show how they
differentially cluster the benign traffic space.

</details>


### [32] [Bridging the Gap in Phishing Detection: A Comprehensive Phishing Dataset Collector](https://arxiv.org/abs/2509.09592)
*Aditya Kulkarni,Shahil Manishbhai Patel,Shivam Pradip Tirmare,Vivek Balachandran,Tamal Das*

Main category: cs.CR

TL;DR: 这篇论文提出了一种新的资源收集工具，用于采集鲁鱼网站的多种资源（URL、源代码、截图、CSS、JavaScript等），并分享了一个包含9,722个URL的样本数据集。


<details>
  <summary>Details</summary>
Motivation: 解决鲁鱼检测领域数据集缺乏多样性的问题，因为鲁鱼网站存活时间短且收集困难，而现有数据集库无法全面涵盖鲁鱼网页的所有相关元素。

Method: 开发了一种自动化工具，利用PhishTank作为鲁鱼URL来源，采集URL、网页源代码、截图、CSS、JavaScript、favicon、图片等多种资源，比PyWebCopy库能够获取更多类型的网页资源。

Result: 生成了一个包含4,056个合法URL和5,666个鲁鱼URL的综合数据集，并分析了鲁鱼特征与类别标签的相关性。

Conclusion: 该工具提供了全面的资源集合，能够帮助研究人员开发更有效的鲁鱼检测方法，解决了鲁鱼数据集多样性不足的挑战。

Abstract: To combat phishing attacks -- aimed at luring web users to divulge their
sensitive information -- various phishing detection approaches have been
proposed. As attackers focus on devising new tactics to bypass existing
detection solutions, researchers have adapted by integrating machine learning
and deep learning into phishing detection. Phishing dataset collection is vital
to developing effective phishing detection approaches, which highly depend on
the diversity of the gathered datasets. The lack of diversity in the dataset
results in a biased model. Since phishing websites are often short-lived,
collecting them is also a challenge. Consequently, very few phishing webpage
dataset repositories exist to date. No single repository comprehensively
consolidates all phishing elements corresponding to a phishing webpage, namely,
URL, webpage source code, screenshot, and related webpage resources. This paper
introduces a resource collection tool designed to gather various resources
associated with a URL, such as CSS, Javascript, favicons, webpage images, and
screenshots. Our tool leverages PhishTank as the primary source for obtaining
active phishing URLs. Our tool fetches several additional webpage resources
compared to PyWebCopy Python library, which provides webpage content for a
given URL. Additionally, we share a sample dataset generated using our tool
comprising 4,056 legitimate and 5,666 phishing URLs along with their associated
resources. We also remark on the top correlated phishing features with their
associated class label found in our dataset. Our tool offers a comprehensive
resource set that can aid researchers in developing effective phishing
detection approaches.

</details>


### [33] [CryptoGuard: An AI-Based Cryptojacking Detection Dashboard Prototype](https://arxiv.org/abs/2509.09638)
*Amitabh Chakravorty,Jess Kropczynski,Nelly Elsayed*

Main category: cs.CR

TL;DR: 提出了CryptoGuard AI安全仪表板的前端原型设计，通过用户中心设计方法帮助加密货币钱包用户监控登录和交易活动，检测可疑行为并采取行动。


<details>
  <summary>Details</summary>
Motivation: 随着加密货币的广泛采用，加密劫持已成为钱包用户的重要安全威胁，需要开发用户友好的安全工具来保护非技术用户。

Method: 采用用户中心设计流程，基于Figma构建高保真可点击原型，模拟关键用户交互，集成可视化警报和报告功能。

Result: 开发了一个直观的AI安全仪表板原型，专为非技术用户设计，展示了如何通过可用性启发式方法支持用户在真实威胁下快速决策。

Conclusion: 实用的安全工具不仅需要强大的后端功能，还需要用户中心的设计来有效传达风险并赋能用户采取有意义的行动。

Abstract: With the widespread adoption of cryptocurrencies, cryptojacking has become a
significant security threat to crypto wallet users. This paper presents a
front-end prototype of an AI-powered security dashboard, namely, CryptoGuard.
Developed through a user-centered design process, the prototype was constructed
as a high-fidelity, click-through model from Figma mockups to simulate key user
interactions. It is designed to assist users in monitoring their login and
transaction activity, identifying any suspicious behavior, and enabling them to
take action directly within the wallet interface. The dashboard is designed for
a general audience, prioritizing an intuitive user experience for non-technical
individuals. Although its AI functionality is conceptual, the prototype
demonstrates features like visual alerts and reporting. This work is positioned
explicitly as a design concept, bridging cryptojacking detection research with
human-centered interface design. This paper also demonstrates how usability
heuristics can directly inform a tool's ability to support rapid and confident
decision-making under real-world threats. This paper argues that practical
security tools require not only robust backend functionality but also a
user-centric design that communicates risk and empowers users to take
meaningful action.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [34] [An Interval Type-2 Version of Bayes Theorem Derived from Interval Probability Range Estimates Provided by Subject Matter Experts](https://arxiv.org/abs/2509.08834)
*John T. Rickard,William A. Dembski,James Rickards*

Main category: cs.AI

TL;DR: 本文提出了区间二型贝叶斯定理，通过保守方法处理输入区间的不一致性，并开发了将专家提供的区间编码为二型模糊隶属函数的新算法。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯推断假设精确输入值，但现实应用中专家通常提供区间范围估计，需要扩展贝叶斯定理处理区间不确定性。

Method: 开发了区间二型贝叶斯定理版本，使用保守方法避免输入不一致性；提出了将专家区间编码为二型模糊隶属函数的灵活算法。

Result: 成功扩展了贝叶斯定理到区间二型版本，能够处理专家提供的区间输入概率，避免了传统精确输入假设的局限性。

Conclusion: 该方法为贝叶斯推断提供了更现实的区间输入处理能力，扩展了先前主要针对词语隶属函数编码的研究工作。

Abstract: Bayesian inference is widely used in many different fields to test hypotheses
against observations. In most such applications, an assumption is made of
precise input values to produce a precise output value. However, this is
unrealistic for real-world applications. Often the best available information
from subject matter experts (SMEs) in a given field is interval range estimates
of the input probabilities involved in Bayes Theorem. This paper provides two
key contributions to extend Bayes Theorem to an interval type-2 (IT2) version.
First, we develop an IT2 version of Bayes Theorem that uses a novel and
conservative method to avoid potential inconsistencies in the input IT2 MFs
that otherwise might produce invalid output results. We then describe a novel
and flexible algorithm for encoding SME-provided intervals into IT2 fuzzy
membership functions (MFs), which we can use to specify the input probabilities
in Bayes Theorem. Our algorithm generalizes and extends previous work on this
problem that primarily addressed the encoding of intervals into word MFs for
Computing with Words applications.

</details>


### [35] [Automated Unity Game Template Generation from GDDs via NLP and Multi-Modal LLMs](https://arxiv.org/abs/2509.08847)
*Amna Hassan*

Main category: cs.AI

TL;DR: 通过精调的LLaMA-3模型和Unity集成包，将游戏设计文档自动转换为功能性Unity游戏原型，在编译成功率、设计遵循度等指标上显著提升


<details>
  <summary>Details</summary>
Motivation: 解决AI辅助游戏开发中从设计到实现的转换问题，流程化游戏开发过程

Method: 组合精调的LLaMA-3模型专门用于Unity代码生成，以及自定义Unity集成包，构建端到端系统解析GDD文档并生成Unity兼容C#代码

Result: 精调模型在编译成功率、GDD遵循度、最佳实践采用和代码模块化指标上较基线模型有4.8/5.0的平均分数，显著提升

Conclusion: 系统有效解决了AI辅助游戏开发的关键问题，使LLM成为流程化从游戏设计到实现过渡的价值工具

Abstract: This paper presents a novel framework for automated game template generation
by transforming Game Design Documents (GDDs) into functional Unity game
prototypes using Natural Language Processing (NLP) and multi-modal Large
Language Models (LLMs). We introduce an end-to-end system that parses GDDs,
extracts structured game specifications, and synthesizes Unity-compatible C#
code that implements the core mechanics, systems, and architecture defined in
the design documentation. Our approach combines a fine-tuned LLaMA-3 model
specialized for Unity code generation with a custom Unity integration package
that streamlines the implementation process. Evaluation results demonstrate
significant improvements over baseline models, with our fine-tuned model
achieving superior performance (4.8/5.0 average score) compared to
state-of-the-art LLMs across compilation success, GDD adherence, best practices
adoption, and code modularity metrics. The generated templates demonstrate high
adherence to GDD specifications across multiple game genres. Our system
effectively addresses critical gaps in AI-assisted game development,
positioning LLMs as valuable tools in streamlining the transition from game
design to implementation.

</details>


### [36] [Global Constraint LLM Agents for Text-to-Model Translation](https://arxiv.org/abs/2509.08970)
*Junyang Cai,Serdar Kadioglu,Bistra Dilkina*

Main category: cs.AI

TL;DR: 使用多个专门的LLM模型代理分解MiniZinc建模任务，通过不同继续代理处理不同类型的全局约束，最终集成完整模型，在初步实验中表现更好的性能。


<details>
  <summary>Details</summary>
Motivation: 自然语言描述向MiniZinc模型的转换需要逻辑推理和约束编程专业知识，这是一项具有挑战性的任务。

Method: 采用多代理模式，每个专门的LLM代理负责检测和生成特定类型的全局约束代码，最后由集成代理组装成完整的MiniZinc模型。

Result: 在初步实验中，该框架在多个LLM上都表现出更好的性能，超过了一次性提示和思维链提示等基准方法。

Conclusion: 该研究提出了一个有效的多代理框架来处理复杂的MiniZinc建模任务，并提出了未来工作的全面路线图，指明了潜在的改进方向。

Abstract: Natural language descriptions of optimization or satisfaction problems are
challenging to translate into correct MiniZinc models, as this process demands
both logical reasoning and constraint programming expertise. We introduce a
framework that addresses this challenge with an agentic approach: multiple
specialized large language model (LLM) agents decompose the modeling task by
global constraint type. Each agent is dedicated to detecting and generating
code for a specific class of global constraint, while a final assembler agent
integrates these constraint snippets into a complete MiniZinc model. By
dividing the problem into smaller, well-defined sub-tasks, each LLM handles a
simpler reasoning challenge, potentially reducing overall complexity. We
conduct initial experiments with several LLMs and show better performance
against baselines such as one-shot prompting and chain-of-thought prompting.
Finally, we outline a comprehensive roadmap for future work, highlighting
potential enhancements and directions for improvement.

</details>


### [37] [ForTIFAI: Fending Off Recursive Training Induced Failure for AI Models](https://arxiv.org/abs/2509.08972)
*Soheil Zibakhsh Shabgahi,Pedram Aghazadeh,Azalia Mirhosseini,Farinaz Koushanfar*

Main category: cs.AI

TL;DR: 提出了一种基于截断交叉熵（TCE）的损失函数来缓解生成式AI模型在合成数据上重复训练导致的模型崩溃问题


<details>
  <summary>Details</summary>
Motivation: 随着合成数据在训练中的比例不断增加，模型在自身生成的数据上重复训练会导致模型崩溃现象，现有缓解策略有限

Method: 识别模型对自生成数据过度自信是崩溃的关键驱动因素，提出置信度感知的损失函数TCE，在训练中对高置信度预测进行降权

Result: TCE显著延迟了递归训练中的模型崩溃，将模型崩溃前的保真度间隔延长了2.3倍以上，且方法具有跨模态通用性

Conclusion: 损失函数设计为在合成数据时代保持生成模型质量提供了简单而强大的工具

Abstract: The increasing reliance on generative AI models has accelerated the
generation rate of synthetic data, with some projections suggesting that most
available new data for training could be machine-generated by 2030. This shift
to a mainly synthetic content presents a critical challenge: repeated training
in synthetic data leads to a phenomenon known as model collapse, where model
performance degrades over generations of training, eventually rendering the
models ineffective. Although prior studies have explored the causes and
detection of model collapse, existing mitigation strategies remain limited.
  In this paper, we identify model overconfidence in their self-generated data
as a key driver of collapse. Building on this observation, we propose a
confidence-aware loss function that downweights high-confidence predictions
during training. We introduce a novel loss function we call Truncated Cross
Entropy (TCE). We demonstrate that TCE significantly delays model collapse in
recursive training.
  We provide a model-agnostic framework that links the loss function design to
model collapse mitigation and validate our approach both theoretically and
empirically, showing that it can extend the model's fidelity interval before
collapse by more than 2.3x. Finally, we show that our method generalizes across
modalities. These findings suggest that the design of loss functions provides a
simple yet powerful tool for preserving the quality of generative models in the
era of increasing synthetic data.

</details>


### [38] [Uncertainty Awareness and Trust in Explainable AI- On Trust Calibration using Local and Global Explanations](https://arxiv.org/abs/2509.08989)
*Carina Newen,Daniel Bodemer,Sonja Glantz,Emmanuel Müller,Magdalena Wischnewski,Lenka Schnaubert*

Main category: cs.AI

TL;DR: 该研究关注可解释AI中的不确定性解释和全局解释，通过测试一种能同时处理不确定性、鲁棒性和全局XAI概念的算法，来评估其信任校准能力和用户满意度。


<details>
  <summary>Details</summary>
Motivation: 虽然可解释AI(XAI)已有广泛研究，但不确定性解释和全局解释这两个领域相对较少被关注。研究者希望填补这一空白，探索能够提供更直观视觉理解的算法是否能提高用户满意度和人类可解释性。

Method: 选择了一种能够同时涵盖不确定性、鲁棒性和全局XAI概念的算法进行测试，重点评估该算法在信任校准方面的能力，并检查其复杂但直观的视觉表示是否能提升用户满意度。

Result: 研究发现该算法能够有效校准用户信任，尽管算法本身较为复杂，但其提供的直观视觉理解确实能够带来更高的用户满意度和人类可解释性。

Conclusion: 研究表明，即使算法本身复杂，通过提供直观的视觉解释方式，可以显著提高XAI系统的用户满意度和可解释性，特别是在不确定性解释和全局解释方面具有重要价值。

Abstract: Explainable AI has become a common term in the literature, scrutinized by
computer scientists and statisticians and highlighted by psychological or
philosophical researchers. One major effort many researchers tackle is
constructing general guidelines for XAI schemes, which we derived from our
study. While some areas of XAI are well studied, we focus on uncertainty
explanations and consider global explanations, which are often left out. We
chose an algorithm that covers various concepts simultaneously, such as
uncertainty, robustness, and global XAI, and tested its ability to calibrate
trust. We then checked whether an algorithm that aims to provide more of an
intuitive visual understanding, despite being complicated to understand, can
provide higher user satisfaction and human interpretability.

</details>


### [39] [Instructional Prompt Optimization for Few-Shot LLM-Based Recommendations on Cold-Start Users](https://arxiv.org/abs/2509.09066)
*Haowei Yang,Yushang Zhao,Sitao Min,Bo Su,Chao Yao,Wei Xu*

Main category: cs.AI

TL;DR: 本文提出了一种基于上下文条件提示的方法来优化大语言模型在冷启动推荐任务中的表现，通过最优示例注入和指令结构化显著提高了few-shot场景下的推荐精度。


<details>
  <summary>Details</summary>
Motivation: 解决推荐系统中冷启动用户因缺乏历史行为信息而导致的推荐效果下降问题，探索如何通过提示工程优化大语言模型在低数据环境下的推荐性能。

Method: 提出上下文条件提示公式P(u, Ds)→R̂，其中u是冷启动用户画像，Ds是精选支持集，R̂是预测的项目排序列表。使用基于transformer的自回归大语言模型（BioGPT、LLaMA-2、GPT-4），采用token级对齐和嵌入空间正则化技术。

Result: 实验证明最优示例注入和指令结构化能显著提高precision@k和NDCG分数，提示组合不仅影响语法结构，还直接控制注意力尺度和解码器推理行为。

Conclusion: 基于提示的适配方法可以作为解决基于大语言模型的推荐系统中冷启动问题的一种有效途径，展示了及时组合提示在功能上的重要性。

Abstract: The cold-start user issue further compromises the effectiveness of
recommender systems in limiting access to the historical behavioral
information. It is an effective pipeline to optimize instructional prompts on a
few-shot large language model (LLM) used in recommender tasks. We introduce a
context-conditioned prompt formulation method P(u,\ Ds)\ \rightarrow\
R\widehat, where u is a cold-start user profile, Ds is a curated support set,
and R\widehat is the predicted ranked list of items. Based on systematic
experimentation with transformer-based autoregressive LLMs (BioGPT, LLaMA-2,
GPT-4), we provide empirical evidence that optimal exemplar injection and
instruction structuring can significantly improve the precision@k and NDCG
scores of such models in low-data settings. The pipeline uses token-level
alignments and embedding space regularization with a greater semantic fidelity.
Our findings not only show that timely composition is not merely syntactic but
also functional as it is in direct control of attention scales and decoder
conduct through inference. This paper shows that prompt-based adaptation may be
considered one of the ways to address cold-start recommendation issues in
LLM-based pipelines.

</details>


### [40] [Understanding Economic Tradeoffs Between Human and AI Agents in Bargaining Games](https://arxiv.org/abs/2509.09071)
*Crystal Qian,Kehang Zhu,John Horton,Benjamin S. Manning,Vivian Tsai,James Wexler,Nithum Thain*

Main category: cs.AI

TL;DR: 人类、LLM和贝叶斯模型在动态协商任务中的表现对比：贝叶斯模型通过突出优化获得最高剩余，LLM采取保守策略，人类则体现更多战略性和公平性考虑


<details>
  <summary>Details</summary>
Motivation: 随着协调任务趋向由自主组件执行，需要评估不同类型代理在动态多代理环境中的协商过程和性能差异

Method: 在动态协商设置中进行直接比较，涵盖结果和行为动态，包括216名人类参与者、GPT-4o、Gemini 1.5 Pro和贝叶斯代理

Result: 贝叶斯代理通过突出优化获得最高剩余，但拒绝交易频繁；人类和LLM能达到类似总体剩余，但行为差异：LLM偏向保守透协策略，人类则采用更多战略性、冒险和公平导向行为

Conclusion: 代理评估中的性能平等指标可能隐藏了过程和对齐方面的根本差异，这对实际部署在真实协调任务中至关重要

Abstract: Coordination tasks traditionally performed by humans are increasingly being
delegated to autonomous agents. As this pattern progresses, it becomes critical
to evaluate not only these agents' performance but also the processes through
which they negotiate in dynamic, multi-agent environments. Furthermore,
different agents exhibit distinct advantages: traditional statistical agents,
such as Bayesian models, may excel under well-specified conditions, whereas
large language models (LLMs) can generalize across contexts. In this work, we
compare humans (N = 216), LLMs (GPT-4o, Gemini 1.5 Pro), and Bayesian agents in
a dynamic negotiation setting that enables direct, identical-condition
comparisons across populations, capturing both outcomes and behavioral
dynamics. Bayesian agents extract the highest surplus through aggressive
optimization, at the cost of frequent trade rejections. Humans and LLMs can
achieve similar overall surplus, but through distinct behaviors: LLMs favor
conservative, concessionary trades with few rejections, while humans employ
more strategic, risk-taking, and fairness-oriented behaviors. Thus, we find
that performance parity -- a common benchmark in agent evaluation -- can
conceal fundamental differences in process and alignment, which are critical
for practical deployment in real-world coordination tasks.

</details>


### [41] [Anti-Money Laundering Machine Learning Pipelines; A Technical Analysis on Identifying High-risk Bank Clients with Supervised Learning](https://arxiv.org/abs/2509.09127)
*Khashayar Namdar,Pin-Chien Wang,Tushar Raju,Steven Zheng,Fiona Li,Safwat Tahmin Khan*

Main category: cs.AI

TL;DR: 本文提出了一种用于反洗钱(AML)的机器学习管道开发方法，通过16步设计和统计分析构建稳健模型，在银行客户风险识别任务中取得了0.961的AUROC分数，获得竞赛第二名。


<details>
  <summary>Details</summary>
Motivation: 金融机构的反洗钱措施是优先事项，机器学习在此领域显示出巨大潜力。本文旨在开发一个系统化的ML管道来识别高风险银行客户。

Method: 采用16步设计和统计分析方法，构建SQLite数据库框架，开发基于SQL的特征工程算法，连接预训练模型并提供可解释AI模块进行特征重要性分析。

Result: 管道在包含195,789个客户ID的数据集上实现了0.961的平均AUROC（标准差0.005），在多伦多大学大数据与人工智能竞赛中获得第二名。

Conclusion: 提出的综合系统化方法能够有效开发稳健的AML机器学习管道，在客户风险识别方面表现出色，证明了该方法在金融反洗钱应用中的实用价值。

Abstract: Anti-money laundering (AML) actions and measurements are among the priorities
of financial institutions, for which machine learning (ML) has shown to have a
high potential. In this paper, we propose a comprehensive and systematic
approach for developing ML pipelines to identify high-risk bank clients in a
dataset curated for Task 1 of the University of Toronto 2023-2024 Institute for
Management and Innovation (IMI) Big Data and Artificial Intelligence
Competition. The dataset included 195,789 customer IDs, and we employed a
16-step design and statistical analysis to ensure the final pipeline was
robust. We also framed the data in a SQLite database, developed SQL-based
feature engineering algorithms, connected our pre-trained model to the
database, and made it inference-ready, and provided explainable artificial
intelligence (XAI) modules to derive feature importance. Our pipeline achieved
a mean area under the receiver operating characteristic curve (AUROC) of 0.961
with a standard deviation (SD) of 0.005. The proposed pipeline achieved second
place in the competition.

</details>


### [42] [Mind Meets Space: Rethinking Agentic Spatial Intelligence from a Neuroscience-inspired Perspective](https://arxiv.org/abs/2509.09154)
*Bui Duc Manh,Soumyaratna Debnath,Zetong Zhang,Shriram Damodaran,Arvind Kumar,Yueyi Zhang,Lu Mi,Erik Cambria,Lin Wang*

Main category: cs.AI

TL;DR: 这篇论文提出了一个基于神经科学原理的计算框架，目的是提升人工智能系统的空间推理能力，连接了生物学功能与计算模块。


<details>
  <summary>Details</summary>
Motivation: 当前的代理智能系统虽然能够自主执行任务和语言推理，但空间推理能力仍有限，主要限于符号和序列处理。人类空间智能基于多感知、空间记忆和认知地图，能够在非结构化环境中进行灵活的情境感知决策。缩小这个差距对于提升代理空间智能至关重要。

Method: 首先审视计算神经科学中的空间神经模型，然后提出一个基于神经科学原理的新鲜计算框架。该框架将核心生物功能映射到六个关键计算模块：受生物启发的多模态感知、多感官整合、自我中心-绝对坐标转换、人工认知地图、空间记忆和空间推理。

Result: 这些模块构成了一个充分的视角场景，支持虚拟和物理环境中的代理空间推理能力。论文进行了框架导向的方法分析，评估了最新方法与每个模块的相关性，识别了阻碍发展更多神经科学基础空间推理模块的关键缺口。

Conclusion: 论文还研究了新兴的标准测试集和数据集，探讨了从虚拟到体现系统（如机器人学）的潜在应用领域，并给出了可能的研究方向，强调了在动态或非结构化环境中通用空间推理的充满期待的路线图。这项工作有望为研究社区提供基于神经科学的视角和结构化路径。

Abstract: Recent advances in agentic AI have led to systems capable of autonomous task
execution and language-based reasoning, yet their spatial reasoning abilities
remain limited and underexplored, largely constrained to symbolic and
sequential processing. In contrast, human spatial intelligence, rooted in
integrated multisensory perception, spatial memory, and cognitive maps, enables
flexible, context-aware decision-making in unstructured environments.
Therefore, bridging this gap is critical for advancing Agentic Spatial
Intelligence toward better interaction with the physical 3D world. To this end,
we first start from scrutinizing the spatial neural models as studied in
computational neuroscience, and accordingly introduce a novel computational
framework grounded in neuroscience principles. This framework maps core
biological functions to six essential computation modules: bio-inspired
multimodal sensing, multi-sensory integration, egocentric-allocentric
conversion, an artificial cognitive map, spatial memory, and spatial reasoning.
Together, these modules form a perspective landscape for agentic spatial
reasoning capability across both virtual and physical environments. On top, we
conduct a framework-guided analysis of recent methods, evaluating their
relevance to each module and identifying critical gaps that hinder the
development of more neuroscience-grounded spatial reasoning modules. We further
examine emerging benchmarks and datasets and explore potential application
domains ranging from virtual to embodied systems, such as robotics. Finally, we
outline potential research directions, emphasizing the promising roadmap that
can generalize spatial reasoning across dynamic or unstructured environments.
We hope this work will benefit the research community with a
neuroscience-grounded perspective and a structured pathway. Our project page
can be found at Github.

</details>


### [43] [ProgD: Progressive Multi-scale Decoding with Dynamic Graphs for Joint Multi-agent Motion Forecasting](https://arxiv.org/abs/2509.09210)
*Xing Gao,Zherui Huang,Weiyao Lin,Xiao Sun*

Main category: cs.AI

TL;DR: 提出ProgD方法，通过动态异构图建模和多尺度解码策略，解决多智能体运动预测中交互关系动态演化的问题，在多个基准测试中达到最先进性能


<details>
  <summary>Details</summary>
Motivation: 现有多智能体联合预测方法忽视了交互关系的动态演化特性，无法准确捕捉未来场景中不断变化的社会交互

Method: 采用动态异构图进行渐进式场景建模，设计分解式架构处理时空依赖关系，结合多尺度解码逐步消除未来运动不确定性

Result: 在INTERACTION多智能体预测基准中排名第一，在Argoverse 2多世界预测基准上达到最先进性能

Conclusion: ProgD方法通过显式建模动态交互关系和多尺度解码策略，有效提升了多智能体运动预测的准确性和一致性

Abstract: Accurate motion prediction of surrounding agents is crucial for the safe
planning of autonomous vehicles. Recent advancements have extended prediction
techniques from individual agents to joint predictions of multiple interacting
agents, with various strategies to address complex interactions within future
motions of agents. However, these methods overlook the evolving nature of these
interactions. To address this limitation, we propose a novel progressive
multi-scale decoding strategy, termed ProgD, with the help of dynamic
heterogeneous graph-based scenario modeling. In particular, to explicitly and
comprehensively capture the evolving social interactions in future scenarios,
given their inherent uncertainty, we design a progressive modeling of scenarios
with dynamic heterogeneous graphs. With the unfolding of such dynamic
heterogeneous graphs, a factorized architecture is designed to process the
spatio-temporal dependencies within future scenarios and progressively
eliminate uncertainty in future motions of multiple agents. Furthermore, a
multi-scale decoding procedure is incorporated to improve on the future
scenario modeling and consistent prediction of agents' future motion. The
proposed ProgD achieves state-of-the-art performance on the INTERACTION
multi-agent prediction benchmark, ranking $1^{st}$, and the Argoverse 2
multi-world forecasting benchmark.

</details>


### [44] [Enabling Regulatory Multi-Agent Collaboration: Architecture, Challenges, and Solutions](https://arxiv.org/abs/2509.09215)
*Qinnan Hu,Yuntao Wang,Yuan Gao,Zhou Su,Linkang Du*

Main category: cs.AI

TL;DR: 基于区块链的多自治理架构，通过行为追踪、声誉评估和恶意行为预测三个模块实现自主代理系统的可信过程监管


<details>
  <summary>Details</summary>
Motivation: 大语言模型驱动的自主代理在重要领域带来机遇的同时，其不可预测性和异构能力也带来了重大的管理和责任挑战

Method: 提出一种区块链启用的层状监管架构，包含代理层、区块链数据层和监管应用层，设计了行为追踪仲裁、动态声誉评估和恶意行为预测三个核心模块

Result: 构建了一个系统性的可信、弹性和可扩展的监管机制基础，适用于大规模自主代理生态系统

Conclusion: 该框架为多自主代理系统提供了可靠的监管解决方案，并指明了区块链在自主代理监管领域的未来研究方向

Abstract: Large language models (LLMs)-empowered autonomous agents are transforming
both digital and physical environments by enabling adaptive, multi-agent
collaboration. While these agents offer significant opportunities across
domains such as finance, healthcare, and smart manufacturing, their
unpredictable behaviors and heterogeneous capabilities pose substantial
governance and accountability challenges. In this paper, we propose a
blockchain-enabled layered architecture for regulatory agent collaboration,
comprising an agent layer, a blockchain data layer, and a regulatory
application layer. Within this framework, we design three key modules: (i) an
agent behavior tracing and arbitration module for automated accountability,
(ii) a dynamic reputation evaluation module for trust assessment in
collaborative scenarios, and (iii) a malicious behavior forecasting module for
early detection of adversarial activities. Our approach establishes a
systematic foundation for trustworthy, resilient, and scalable regulatory
mechanisms in large-scale agent ecosystems. Finally, we discuss the future
research directions for blockchain-enabled regulatory frameworks in multi-agent
systems.

</details>


### [45] [Jupiter: Enhancing LLM Data Analysis Capabilities via Notebook and Inference-Time Value-Guided Search](https://arxiv.org/abs/2509.09245)
*Shuocheng Li,Yihao Liu,Silin Du,Wenxuan Zeng,Zhe Xu,Mengyu Zhou,Yeye He,Haoyu Dong,Shi Han,Dongmei Zhang*

Main category: cs.AI

TL;DR: 提出了NbQA数据集和Jupiter框架，通过从真实Jupyter笔记本提取高质量工具使用任务和解决方案，结合MCTS搜索增强多步推理能力，在数据科学任务上达到或超越GPT-4o性能


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在多步推理和工具使用方面仍存在困难，限制了在复杂数据分析任务中的有效性

Method: 1) 从真实Jupyter笔记本提取工具型数据分析任务和可执行多步解决方案构建NbQA数据集；2) 提出Jupiter框架，将数据分析建模为搜索问题，使用MCTS生成多样化解决方案轨迹进行价值模型学习；3) 推理时结合价值模型和节点访问计数高效生成可执行多步计划

Result: Qwen2.5-7B和14B-Instruct模型在NbQA上分别解决了77.82%和86.38%的InfiAgent-DABench任务，匹配或超越了GPT-4o和先进代理框架的性能

Conclusion: 该方法显著提升了多步推理和工具使用能力，在多样化多步推理任务上展现出更好的泛化性能和更强的工具使用推理能力

Abstract: Large language models (LLMs) have shown great promise in automating data
science workflows, but existing models still struggle with multi-step reasoning
and tool use, which limits their effectiveness on complex data analysis tasks.
To address this, we propose a scalable pipeline that extracts high-quality,
tool-based data analysis tasks and their executable multi-step solutions from
real-world Jupyter notebooks and associated data files. Using this pipeline, we
introduce NbQA, a large-scale dataset of standardized task-solution pairs that
reflect authentic tool-use patterns in practical data science scenarios. To
further enhance multi-step reasoning, we present Jupiter, a framework that
formulates data analysis as a search problem and applies Monte Carlo Tree
Search (MCTS) to generate diverse solution trajectories for value model
learning. During inference, Jupiter combines the value model and node visit
counts to efficiently collect executable multi-step plans with minimal search
steps. Experimental results show that Qwen2.5-7B and 14B-Instruct models on
NbQA solve 77.82% and 86.38% of tasks on InfiAgent-DABench,
respectively-matching or surpassing GPT-4o and advanced agent frameworks.
Further evaluations demonstrate improved generalization and stronger tool-use
reasoning across diverse multi-step reasoning tasks.

</details>


### [46] [Fusing Knowledge and Language: A Comparative Study of Knowledge Graph-Based Question Answering with LLMs](https://arxiv.org/abs/2509.09272)
*Vaibhav Chaudhary,Neha Soni,Narotam Singh,Amita Kapoor*

Main category: cs.AI

TL;DR: 这篇论文对三种知识图构建方法（spaCy、Stanford CoreNLP-OpenIE、GraphRAG）进行技术比较研究，分析它们在大语言模型问答系统中的效果、可行性和适配性。实验结果显示OpenIE产生最全面的三元组，GraphRAG表现出最优的推理能力。


<details>
  <summary>Details</summary>
Motivation: 传统的检索增强生成（RAG）方法在处理复杂、长文本的主题和整体理解时遇到限制，需要对文本和上下文进行更深入的分析。知识图作为结构化信息的强大工具，最近成为提升问答系统性能的新技术。

Method: 进行了一项综合性技术比较研究，对三种不同的知识图三元组构建方法（spaCy、Stanford CoreNLP-OpenIE和GraphRAG）进行评估，并将它们与大语言模型集成用于问题回答。所有方法都利用了开源技术。

Result: 实验结果表明，OpenIE方法能够提供最全面的三元组覆盖范围，而GraphRAG在三种方法中表现出最优称的推理能力。

Conclusion: 论文讨论了每种方法的优势和局限性，并为改进基于知识图的问题回答系统提供了未来发展方向的见解。

Abstract: Knowledge graphs, a powerful tool for structuring information through
relational triplets, have recently become the new front-runner in enhancing
question-answering systems. While traditional Retrieval Augmented Generation
(RAG) approaches are proficient in fact-based and local context-based
extraction from concise texts, they encounter limitations when addressing the
thematic and holistic understanding of complex, extensive texts, requiring a
deeper analysis of both text and context. This paper presents a comprehensive
technical comparative study of three different methodologies for constructing
knowledge graph triplets and integrating them with Large Language Models (LLMs)
for question answering: spaCy, Stanford CoreNLP-OpenIE, and GraphRAG, all
leveraging open source technologies. We evaluate the effectiveness,
feasibility, and adaptability of these methods by analyzing their capabilities,
state of development, and their impact on the performance of LLM-based question
answering. Experimental results indicate that while OpenIE provides the most
comprehensive coverage of triplets, GraphRAG demonstrates superior reasoning
abilities among the three. We conclude with a discussion on the strengths and
limitations of each method and provide insights into future directions for
improving knowledge graph-based question answering.

</details>


### [47] [Tree-OPO: Off-policy Monte Carlo Tree-Guided Advantage Optimization for Multistep Reasoning](https://arxiv.org/abs/2509.09284)
*Bingning Huang,Tu Nguyen,Matthieu Zimmer*

Main category: cs.AI

TL;DR: 将MCTS轨迹用于GRPO策略优化，提出基于树状结构的优势估计方法，解决优势饱和和奖励信号崩溃问题


<details>
  <summary>Details</summary>
Motivation: 探索如何将传统用于训练价值/奖励模型的MCTS轨迹重新用于改进基于偏好的强化学习中的策略优化

Method: 提出分阶段GRPO训练范式，使用部分揭示的MCTS rollout生成完成序列，引入树状结构设置进行优势估计

Result: 结构化优势估计可以稳定更新并更好反映组合推理质量，但仍面临优势饱和和奖励信号崩溃等挑战

Conclusion: 提出了启发式和统计解决方案来缓解这些问题，并讨论了在分阶段或树状奖励结构下学习的开放挑战

Abstract: Recent advances in reasoning with large language models (LLMs) have shown the
effectiveness of Monte Carlo Tree Search (MCTS) for generating high-quality
intermediate trajectories, particularly in math and symbolic domains. Inspired
by this, we explore how MCTS-derived trajectories, traditionally used for
training value or reward models, can be repurposed to improve policy
optimization in preference-based reinforcement learning (RL). Specifically, we
focus on Group Relative Policy Optimization (GRPO), a recent algorithm that
enables preference-consistent policy learning without value networks. We
propose a staged GRPO training paradigm where completions are derived from
partially revealed MCTS rollouts, introducing a novel tree-structured setting
for advantage estimation. This leads to a rich class of prefix-conditioned
reward signals, which we analyze theoretically and empirically. Our initial
results indicate that while structured advantage estimation can stabilize
updates and better reflect compositional reasoning quality, challenges such as
advantage saturation and reward signal collapse remain. We propose heuristic
and statistical solutions to mitigate these issues and discuss open challenges
for learning under staged or tree-like reward structures.

</details>


### [48] [LightAgent: Production-level Open-source Agentic AI Framework](https://arxiv.org/abs/2509.09292)
*Weige Cai,Tong Zhu,Jinyi Niu,Ruiqi Hu,Lingyao Li,Tenglong Wang,Xiaowu Dai,Weining Shen,Liwen Zhang*

Main category: cs.AI

TL;DR: LightAgent是一个轻量级但功能强大的多智能体框架，解决了现有框架在灵活性和简单性之间的权衡问题，集成了内存、工具和思维树等核心功能。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的快速发展，多智能体系统在各种应用场景中取得了显著进展，但在设计多功能、鲁棒和高效的智能体部署平台方面仍存在重大挑战。

Method: 提出了LightAgent框架，集成Memory (mem0)、Tools和Tree of Thought (ToT)等核心功能，同时保持极轻量级结构，作为完全开源解决方案与主流聊天平台无缝集成。

Result: 开发了一个轻量级但强大的智能体框架，使开发者能够轻松构建自学习智能体。

Conclusion: LightAgent有效解决了现有框架在灵活性和简单性之间的权衡问题，为多智能体系统部署提供了一个实用的解决方案。

Abstract: With the rapid advancement of large language models (LLMs), Multi-agent
Systems (MAS) have achieved significant progress in various application
scenarios. However, substantial challenges remain in designing versatile,
robust, and efficient platforms for agent deployment. To address these
limitations, we propose \textbf{LightAgent}, a lightweight yet powerful agentic
framework, effectively resolving the trade-off between flexibility and
simplicity found in existing frameworks. LightAgent integrates core
functionalities such as Memory (mem0), Tools, and Tree of Thought (ToT), while
maintaining an extremely lightweight structure. As a fully open-source
solution, it seamlessly integrates with mainstream chat platforms, enabling
developers to easily build self-learning agents. We have released LightAgent at
\href{https://github.com/wxai-space/LightAgent}{https://github.com/wxai-space/LightAgent}

</details>


### [49] [Explaining Tournament Solutions with Minimal Supports](https://arxiv.org/abs/2509.09312)
*Clément Contet,Umberto Grandi,Jérôme Mengin*

Main category: cs.AI

TL;DR: 该论文研究锦标赛中获胜者的可解释性认证，通过识别最小支撑子锦标赛来解释为什么某个候选人在不同锦标赛规则下必然获胜。


<details>
  <summary>Details</summary>
Motivation: 锦标赛广泛应用于表示候选者之间的成对优势关系，但缺乏对获胜原因的正式解释。研究旨在为各种锦标赛规则提供认证解释，这是可解释AI的核心概念。

Method: 识别最小支撑子锦标赛，即候选人在其中无论锦标赛其余部分如何完成都必然获胜的最小子锦标赛。针对多种常见锦标赛规则进行分析。

Result: 确定了各规则的最小支撑大小，提出了多项式时间算法计算除加权未覆盖集外的所有规则的最小支撑，证明加权未覆盖集问题是NP完全的。

Conclusion: 最小支撑子锦标赛能够提供紧凑、认证且直观的解释，为锦标赛获胜者的可解释性提供了有效的理论框架和计算方法。

Abstract: Tournaments are widely used models to represent pairwise dominance between
candidates, alternatives, or teams. We study the problem of providing certified
explanations for why a candidate appears among the winners under various
tournament rules. To this end, we identify minimal supports, minimal
sub-tournaments in which the candidate is guaranteed to win regardless of how
the rest of the tournament is completed (that is, the candidate is a necessary
winner of the sub-tournament). This notion corresponds to an abductive
explanation for the question,"Why does the winner win the tournament", a
central concept in formal explainable AI. We focus on common tournament
solutions: the top cycle, the uncovered set, the Copeland rule, the Borda rule,
the maximin rule, and the weighted uncovered set. For each rule we determine
the size of the smallest minimal supports, and we present polynomial-time
algorithms to compute them for all but the weighted uncovered set, for which
the problem is NP-complete. Finally, we show how minimal supports can serve to
produce compact, certified, and intuitive explanations.

</details>


### [50] [Measuring Implicit Spatial Coordination in Teams: Effects on Collective Intelligence and Performance](https://arxiv.org/abs/2509.09314)
*Thuy Ngoc Nguyen,Anita Williams Woolley,Cleotilde Gonzalez*

Main category: cs.AI

TL;DR: 研究隐式空间协调在限制明确沟通环境中对团队表现的影响，发现空间专门化积极推动性能，适度调整空间距离最优


<details>
  <summary>Details</summary>
Motivation: 体育团队等同步团队隐式协调研究较多，但物理空间中无法依靠视觉线索或明确沟通的团队（如消防、军队、紧急响应）缺乏研究，需要探索通过运动模式进行隐式协调的机制

Method: 在限制明确沟通的协作在线搜救任务中，对34个四人团队（136名参与者）进行角色分工，通过空间距离、分布模式和运动对齐等指标来分析三个维度：探索多样性、运动专门化和适应性空间距离

Result: 空间专门化正向预测团队表现，适应性空间距离呈倒U型关系（中等适应水平最优），这些指标的时间动态能够区分高低表现团队

Conclusion: 隐式空间协调在角色基础团队中关键，平衡的适应策略最优，研究结果对培训和AI辅助团队支持系统具有重要意义

Abstract: Coordinated teamwork is essential in fast-paced decision-making environments
that require dynamic adaptation, often without an opportunity for explicit
communication. Although implicit coordination has been extensively considered
in the existing literature, the majority of work has focused on co-located,
synchronous teamwork (such as sports teams) or, in distributed teams, primarily
on coordination of knowledge work. However, many teams (firefighters, military,
law enforcement, emergency response) must coordinate their movements in
physical space without the benefit of visual cues or extensive explicit
communication. This paper investigates how three dimensions of spatial
coordination, namely exploration diversity, movement specialization, and
adaptive spatial proximity, influence team performance in a collaborative
online search and rescue task where explicit communication is restricted and
team members rely on movement patterns to infer others' intentions and
coordinate actions. Our metrics capture the relational aspects of teamwork by
measuring spatial proximity, distribution patterns, and alignment of movements
within shared environments. We analyze data from 34 four-person teams (136
participants) assigned to specialized roles in a search and rescue task.
Results show that spatial specialization positively predicts performance, while
adaptive spatial proximity exhibits a marginal inverted U-shaped relationship,
suggesting moderate levels of adaptation are optimal. Furthermore, the temporal
dynamics of these metrics differentiate high- from low-performing teams over
time. These findings provide insights into implicit spatial coordination in
role-based teamwork and highlight the importance of balanced adaptive
strategies, with implications for training and AI-assisted team support
systems.

</details>


### [51] [Towards Adaptive ML Benchmarks: Web-Agent-Driven Construction, Domain Expansion, and Metric Optimization](https://arxiv.org/abs/2509.09321)
*Hangyi Jia,Yuxi Qian,Hanwen Tong,Xinhui Wu,Lin Chen,Feng Wei*

Main category: cs.AI

TL;DR: TAM Bench是一个用于评估基于LLM的自动机器学习代理的多样化、现实化基准测试，包含150个从Kaggle等平台自动收集的ML任务，具有浏览器自动化的任务获取系统、基于排行榜的难度建模和多维度评估框架


<details>
  <summary>Details</summary>
Motivation: 现有基准测试在任务覆盖、领域多样性、难度建模和评估严谨性方面存在局限，无法充分评估LLM代理在真实场景中的端到端ML工作流能力

Method: 1) 浏览器自动化和LLM驱动的任务获取系统自动收集和结构化ML挑战；2) 基于排行榜参与人数和分数离散度的难度建模机制；3) 包含性能、格式合规性、约束遵守和任务泛化的多维度评估框架

Result: 构建了包含150个AutoML任务的基准测试，提供Lite(18任务)、Medium和Full三个不同规模的子集，其中Lite版本在模态和难度级别上平衡覆盖，适合日常基准测试和比较研究

Conclusion: TAM Bench提供了一个多样化、现实化且结构化的基准测试，能够更全面地评估LLM代理在端到端机器学习工作流中的能力

Abstract: Recent advances in large language models (LLMs) have enabled the emergence of
general-purpose agents for automating end-to-end machine learning (ML)
workflows, including data analysis, feature engineering, model training, and
competition solving. However, existing benchmarks remain limited in task
coverage, domain diversity, difficulty modeling, and evaluation rigor, failing
to capture the full capabilities of such agents in realistic settings. We
present TAM Bench, a diverse, realistic, and structured benchmark for
evaluating LLM-based agents on end-to-end ML tasks. TAM Bench features three
key innovations: (1) A browser automation and LLM-based task acquisition system
that automatically collects and structures ML challenges from platforms such as
Kaggle, AIcrowd, and Biendata, spanning multiple task types and data modalities
(e.g., tabular, text, image, graph, audio); (2) A leaderboard-driven difficulty
modeling mechanism that estimates task complexity using participant counts and
score dispersion, enabling scalable and objective task calibration; (3) A
multi-dimensional evaluation framework incorporating performance, format
compliance, constraint adherence, and task generalization. Based on 150 curated
AutoML tasks, we construct three benchmark subsets of different sizes -- Lite,
Medium, and Full -- designed for varying evaluation scenarios. The Lite
version, with 18 tasks and balanced coverage across modalities and difficulty
levels, serves as a practical testbed for daily benchmarking and comparative
studies.

</details>


### [52] [Curriculum-Based Multi-Tier Semantic Exploration via Deep Reinforcement Learning](https://arxiv.org/abs/2509.09356)
*Abdel Hakim Drid,Vincenzo Suriani,Daniele Nardi,Abderrezzak Debilou*

Main category: cs.AI

TL;DR: 通过集成视觉-语言模型和层状奖励函数的新题DRL架构，实现了资源效率高的语义探索，代理能够策略性地查询VLM获取外部指导


<details>
  <summary>Details</summary>
Motivation: 传统RL方法在语义探索中平衡效率探索和语义理解遇到困难，需要更高级的认知能力来支持自主环境探索

Method: 集成VLM通用知识到层状奖励函数中，将VLM查询模型化为专门动作，结合课程学习策略在不同复杂度级别指导学习

Result: 实验结果显示代理在物体发现率上显著提升，能够有效导航到语义丰富区域，并掌握策略性查询外部环境信息的时机

Conclusion: 该研究提供了一种将通用知识语义推理嵌入自主代理的实用可扩展方法，为实现全面智能自主探索提供了新方法

Abstract: Navigating and understanding complex and unknown environments autonomously
demands more than just basic perception and movement from embodied agents.
Truly effective exploration requires agents to possess higher-level cognitive
abilities, the ability to reason about their surroundings, and make more
informed decisions regarding exploration strategies. However, traditional RL
approaches struggle to balance efficient exploration and semantic understanding
due to limited cognitive capabilities embedded in the small policies for the
agents, leading often to human drivers when dealing with semantic exploration.
In this paper, we address this challenge by presenting a novel Deep
Reinforcement Learning (DRL) architecture that is specifically designed for
resource efficient semantic exploration. A key methodological contribution is
the integration of a Vision-Language Model (VLM) common-sense through a layered
reward function. The VLM query is modeled as a dedicated action, allowing the
agent to strategically query the VLM only when deemed necessary for gaining
external guidance, thereby conserving resources. This mechanism is combined
with a curriculum learning strategy designed to guide learning at different
levels of complexity to ensure robust and stable learning. Our experimental
evaluation results convincingly demonstrate that our agent achieves
significantly enhanced object discovery rates and develops a learned capability
to effectively navigate towards semantically rich regions. Furthermore, it also
shows a strategic mastery of when to prompt for external environmental
information. By demonstrating a practical and scalable method for embedding
common-sense semantic reasoning with autonomous agents, this research provides
a novel approach to pursuing a fully intelligent and self-guided exploration in
robotics.

</details>


### [53] [TORSO: Template-Oriented Reasoning Towards General Tasks](https://arxiv.org/abs/2509.09448)
*Minhyuk Kim,Seungyoon Lee,Heuiseok Lim*

Main category: cs.AI

TL;DR: TORSO方法通过引导LLM利用内部推理能力生成响应，无需手动构建少样本示例，在多个基准测试中表现出色


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖少样本提示，限制了模型内在推理能力的利用，且构建任务特定的少样本提示成本高且存在不一致性问题

Method: 提出模板导向推理(TORSO)方法，引导模型利用内部推理能力生成适当响应，无需手动制作的少样本示例

Result: 实验结果表明TORSO在多样化LLM基准测试中实现了强大性能，并生成合理的推理过程

Conclusion: TORSO提供了一种有效的方法来激发LLM的内在推理能力，无需依赖外部少样本示例，在多个任务上表现优异

Abstract: The approaches that guide Large Language Models (LLMs) to emulate human
reasoning during response generation have emerged as an effective method for
enabling them to solve complex problems in a step-by-step manner, thereby
achieving superior performance. However, most existing approaches using
few-shot prompts to generate responses heavily depend on the provided examples,
limiting the utilization of the model's inherent reasoning capabilities.
Moreover, constructing task-specific few-shot prompts is often costly and may
lead to inconsistencies across different tasks. In this work, we introduce
Template-Oriented Reasoning (TORSO), which elicits the model to utilize
internal reasoning abilities to generate proper responses across various tasks
without the need for manually crafted few-shot examples. Our experimental
results demonstrate that TORSO achieves strong performance on diverse LLMs
benchmarks with reasonable rationales.

</details>


### [54] [Inteligencia Artificial jurídica y el desafío de la veracidad: análisis de alucinaciones, optimización de RAG y principios para una integración responsable](https://arxiv.org/abs/2509.09467)
*Alex Dantart*

Main category: cs.AI

TL;DR: 论文分析了法律领域LLM中的“幽灵现象”问题，研究了RAG策略的有效性和局限性，建议采用以验证性和可追溯性为优先的“咨询式AI”范式


<details>
  <summary>Details</summary>
Motivation: 解决法律领域大语言模型中出现假信息的挑战，探索有效的减少幽灵现象的策略

Method: 分析幽灵现象的原因和表现形式，评估RAG缓解策略的效果和局限性，提出整体优化方案

Result: 发现RAG策略存在局限性，需要更全面的优化方案，人类监督在法律应用中仍然不可或缺

Conclusion: 解决方向不是累计改进生成模型，而是采用“咨询式AI”范式，将AI作为增强专业判断的工具，而非替代人类专业能力

Abstract: This technical report analyzes the challenge of "hallucinations" (false
information) in LLMs applied to law. It examines their causes, manifestations,
and the effectiveness of the RAG mitigation strategy, highlighting its
limitations and proposing holistic optimizations. The paper explores the
ethical and regulatory implications, emphasizing human oversight as an
irreplaceable role. It concludes that the solution lies not in incrementally
improving generative models, but in adopting a "consultative" AI paradigm that
prioritizes veracity and traceability, acting as a tool to amplify, not
replace, professional judgment.
  --
  Este informe t\'ecnico analiza el desaf\'io de las "alucinaciones"
(informaci\'on falsa) en los LLMs aplicados al derecho. Se examinan sus causas,
manifestaciones y la efectividad de la estrategia de mitigaci\'on RAG,
exponiendo sus limitaciones y proponiendo optimizaciones hol\'isticas. Se
exploran las implicaciones \'eticas y regulatorias, enfatizando la
supervisi\'on humana como un rol insustituible. El documento concluye que la
soluci\'on no reside en mejorar incrementalmente los modelos generativos, sino
en adoptar un paradigma de IA "consultiva" que priorice la veracidad y la
trazabilidad, actuando como una herramienta para amplificar, y no sustituir, el
juicio profesional.

</details>


### [55] [SEDM: Scalable Self-Evolving Distributed Memory for Agents](https://arxiv.org/abs/2509.09498)
*Haoran Xu,Jiacong Hu,Ke Zhang,Lei Yu,Yuxin Tang,Xinyuan Song,Yiqun Duan,Lynn Ai,Bill Shi*

Main category: cs.AI

TL;DR: SEDM是一个自演化的分布式内存框架，通过可验证写入、动态内存调度和跨域知识扩散来解决多智能体系统中的内存管理问题，提高推理准确性并降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 长期多智能体系统产生大量轨迹和交互数据，现有内存管理方法存在噪声积累、内存膨胀和跨域泛化能力有限的问题，需要更高效可扩展的内存管理方案。

Method: SEDM框架包含三个核心组件：基于可重现回放的可验证写入准入机制、根据经验效用动态排序和整合条目的自调度内存控制器、以及抽象可重用见解支持跨异构任务迁移的跨域知识扩散。

Result: 在基准数据集上的评估显示，SEDM相比强基线方法提高了推理准确性，减少了token开销，并能将从事实验证中提炼的知识用于增强多跳推理能力。

Conclusion: SEDM为开放式多智能体协作提供了一个可扩展且可持续的内存机制，将内存从被动存储转变为主动自优化的组件。

Abstract: Long-term multi-agent systems inevitably generate vast amounts of
trajectories and historical interactions, which makes efficient memory
management essential for both performance and scalability. Existing methods
typically depend on vector retrieval and hierarchical storage, yet they are
prone to noise accumulation, uncontrolled memory expansion, and limited
generalization across domains. To address these challenges, we present SEDM,
Self-Evolving Distributed Memory, a verifiable and adaptive framework that
transforms memory from a passive repository into an active, self-optimizing
component. SEDM integrates verifiable write admission based on reproducible
replay, a self-scheduling memory controller that dynamically ranks and
consolidates entries according to empirical utility, and cross-domain knowledge
diffusion that abstracts reusable insights to support transfer across
heterogeneous tasks. Evaluations on benchmark datasets demonstrate that SEDM
improves reasoning accuracy while reducing token overhead compared with strong
memory baselines, and further enables knowledge distilled from fact
verification to enhance multi-hop reasoning. The results highlight SEDM as a
scalable and sustainable memory mechanism for open-ended multi-agent
collaboration. The code will be released in the later stage of this project.

</details>


### [56] [Compositional Concept Generalization with Variational Quantum Circuits](https://arxiv.org/abs/2509.09541)
*Hala Hawashin,Mina Abbaszadeh,Nicholas Joseph,Beth Pearson,Martha Lewis,Mehrnoosh sadrzadeh*

Main category: cs.AI

TL;DR: 量子模型在组合泛化任务中优于经典组合模型，特别是在使用多热编码的图像表示时表现良好，但在CLIP图像向量上表现不一。


<details>
  <summary>Details</summary>
Motivation: 人类具有组合泛化的认知能力，但当前AI工具如视觉语言模型缺乏这种能力。先前基于张量的组合语义方法效果不佳，作者推测量子模型更高的训练效率可能改善这类任务的性能。

Method: 将组合张量模型的表示解释在希尔伯特空间中，训练变分量子电路来学习这些表示。使用两种图像编码技术：多热编码(MHE)处理二值图像向量，以及角度/幅度编码处理来自CLIP模型的图像向量。

Result: 使用噪声MHE编码获得了良好的概念验证结果。在CLIP图像向量上的表现较为复杂，但仍然超越了经典的组合模型。

Conclusion: 量子模型在组合泛化任务中展现出潜力，特别是在特定编码方式下表现优于传统方法，为量子计算在AI组合泛化问题中的应用提供了有希望的方向。

Abstract: Compositional generalization is a key facet of human cognition, but lacking
in current AI tools such as vision-language models. Previous work examined
whether a compositional tensor-based sentence semantics can overcome the
challenge, but led to negative results. We conjecture that the increased
training efficiency of quantum models will improve performance in these tasks.
We interpret the representations of compositional tensor-based models in
Hilbert spaces and train Variational Quantum Circuits to learn these
representations on an image captioning task requiring compositional
generalization. We used two image encoding techniques: a multi-hot encoding
(MHE) on binary image vectors and an angle/amplitude encoding on image vectors
taken from the vision-language model CLIP. We achieve good proof-of-concept
results using noisy MHE encodings. Performance on CLIP image vectors was more
mixed, but still outperformed classical compositional models.

</details>


### [57] [Boosting Embodied AI Agents through Perception-Generation Disaggregation and Asynchronous Pipeline Execution](https://arxiv.org/abs/2509.09560)
*Shulai Zhang,Ao Xu,Quan Chen,Han Zhao,Weihao Cui,Ningxin Zheng,Haibin Lin,Xin Liu,Minyi Guo*

Main category: cs.AI

TL;DR: Auras是一个算法-系统协同设计的推理框架，通过解耦感知和生成模块并提供受控的流水线并行化，显著提升了具身AI系统的推理频率和吞吐量，同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 具身AI系统在动态环境中需要处理高频输入输出需求，传统顺序计算模式在保证准确性的同时难以达到实际应用所需的"思考"频率。

Method: Auras框架将感知和生成模块解耦，为它们提供受控的流水线并行化。通过建立公共上下文供感知和生成共享，解决了并行化增加时出现的数据陈旧性问题。

Result: 实验结果显示，Auras平均提升吞吐量2.54倍，同时达到原始准确性的102.7%，证明了其在克服顺序计算限制和提供高吞吐量方面的有效性。

Conclusion: Auras通过算法-系统协同设计成功解决了具身AI系统中感知和生成模块的高频推理需求，在保持准确性的同时显著提升了系统性能。

Abstract: Embodied AI systems operate in dynamic environments, requiring seamless
integration of perception and generation modules to process high-frequency
input and output demands. Traditional sequential computation patterns, while
effective in ensuring accuracy, face significant limitations in achieving the
necessary "thinking" frequency for real-world applications. In this work, we
present Auras, an algorithm-system co-designed inference framework to optimize
the inference frequency of embodied AI agents. Auras disaggregates the
perception and generation and provides controlled pipeline parallelism for them
to achieve high and stable throughput. Faced with the data staleness problem
that appears when the parallelism is increased, Auras establishes a public
context for perception and generation to share, thereby promising the accuracy
of embodied agents. Experimental results show that Auras improves throughput by
2.54x on average while achieving 102.7% of the original accuracy, demonstrating
its efficacy in overcoming the constraints of sequential computation and
providing high throughput.

</details>


### [58] [The Illusion of Diminishing Returns: Measuring Long Horizon Execution in LLMs](https://arxiv.org/abs/2509.09677)
*Akshit Sinha,Arvindh Arun,Shashwat Goel,Steffen Staab,Jonas Geiping*

Main category: cs.AI

TL;DR: 论文研究发现，大语言模型在长任务执行中存在执行错误而非推理能力不足的问题，模型规模扩大能显著提升多步任务执行能力，但存在自我条件效应限制性能提升。


<details>
  <summary>Details</summary>
Motivation: 探讨大语言模型规模扩展是否带来边际收益递减，研究模型在长任务执行中的失败原因，重点关注执行能力而非推理能力的限制。

Method: 通过显式提供知识和计划来隔离执行能力，测试不同规模模型在多步任务中的表现，分析错误模式和自我条件效应。

Result: 大模型能正确执行更多步骤，即使小模型单步准确率达100%；发现模型存在自我条件效应（上下文包含先前错误会增加后续错误概率），且该效应不随模型规模扩大而减弱；思维模型能执行更长的单步任务且无自我条件效应。

Conclusion: 执行能力是长任务成功的关键因素，模型规模扩展和序列测试时计算对长视野任务有巨大益处，思维模型在避免自我条件效应方面表现优异。

Abstract: Does continued scaling of large language models (LLMs) yield diminishing
returns? Real-world value often stems from the length of task an agent can
complete. We start this work by observing the simple but counterintuitive fact
that marginal gains in single-step accuracy can compound into exponential
improvements in the length of a task a model can successfully complete. Then,
we argue that failures of LLMs when simple tasks are made longer arise from
mistakes in execution, rather than an inability to reason. We propose isolating
execution capability, by explicitly providing the knowledge and plan needed to
solve a long-horizon task. We find that larger models can correctly execute
significantly more turns even when small models have 100\% single-turn
accuracy. We observe that the per-step accuracy of models degrades as the
number of steps increases. This is not just due to long-context limitations --
curiously, we observe a self-conditioning effect -- models become more likely
to make mistakes when the context contains their errors from prior turns.
Self-conditioning does not reduce by just scaling the model size. In contrast,
recent thinking models do not self-condition, and can also execute much longer
tasks in a single turn. We conclude by benchmarking frontier thinking models on
the length of task they can execute in a single turn. Overall, by focusing on
the ability to execute, we hope to reconcile debates on how LLMs can solve
complex reasoning problems yet fail at simple tasks when made longer, and
highlight the massive benefits of scaling model size and sequential test-time
compute for long-horizon tasks.

</details>
