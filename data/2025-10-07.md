<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 46]
- [cs.CR](#cs.CR) [Total: 40]
- [cs.AI](#cs.AI) [Total: 89]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Repairing Leaks in Resource Wrappers](https://arxiv.org/abs/2510.03461)
*Sanjay Malakar,Michael D. Ernst,Martin Kellogg,Manu Sridharan*

Main category: cs.SE

TL;DR: 本文提出了一种改进资源泄漏修复的方法，特别针对资源包装器的情况，通过集成资源管理规范推断、程序变换、字段包含分析和新的修复模式，显著提高了修复率。


<details>
  <summary>Details</summary>
Motivation: 现有资源泄漏修复方法只能修复硬编码库资源类型的泄漏，无法处理实际代码中广泛使用的资源包装器，这些包装器将资源存储在字段中且自身需要关闭。

Method: 1. 将资源管理规范推断集成到修复流程中；2. 通过程序变换使分析更容易；3. 使用字段包含分析推理资源生命周期；4. 引入新的修复模式处理非final字段中的资源。

Result: 在NJR基准测试套件中，现有方法修复了41%的资源泄漏警告，而本文实现的Arodnap系统修复了68%。

Conclusion: 通过集成规范推断、程序变换、字段分析和新的修复模式，本文方法显著提高了资源泄漏修复能力，特别是在处理资源包装器方面表现突出。

Abstract: A resource leak occurs when a program fails to release a finite resource like
a socket, file descriptor or database connection. While sound static analysis
tools can detect all leaks, automatically repairing them remains challenging.
Prior work took the output of a detection tool and attempted to repair only
leaks from a hard-coded list of library resource types. That approach limits
the scope of repairable leaks: real-world code uses resource wrappers that
store a resource in a field and must themselves be closed. This paper makes
four key contributions to improve resource leak repair in the presence of
wrappers. (1) It integrates inference of resource management specifications
into the repair pipeline, enabling extant fixing approaches to reason about
wrappers. (2) It transforms programs into variants that are easier to analyze,
making inference, detection, and fixing tools more effective; for instance, it
makes detection tools report problems closer to the root cause, often in a
client of a resource wrapper rather than within the wrapper class itself. (3) A
novel field containment analysis reasons about resource lifetimes, enabling
repair of more leaks involving resources stored in fields. (4) It introduces a
new repair pattern and more precise reasoning to better handle resources stored
in non-final fields. Prior work fixed 41% of resource leak warnings in the NJR
benchmark suite; our implementation Arodnap fixes 68%.

</details>


### [2] [ALMAS: an Autonomous LLM-based Multi-Agent Software Engineering Framework](https://arxiv.org/abs/2510.03463)
*Vali Tawosi,Keshav Ramani,Salwa Alamir,Xiaomo Liu*

Main category: cs.SE

TL;DR: ALMAS是一个基于LLM的自主多智能体软件工程框架，遵循软件开发生命周期，能够在敏捷开发团队中执行端到端任务。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体LLM系统主要关注代码实现，但软件开发是包含多个阶段的复杂环境，需要覆盖整个软件开发生命周期。

Method: 提出ALMAS框架，将智能体与敏捷角色对齐，采用模块化设计，能够与人类开发者和开发环境无缝集成。

Result: 通过已发表工作和用例展示，ALMAS能够无缝生成应用程序并添加新功能。

Conclusion: ALMAS为基于LLM的多智能体软件工程提供了一个可行的框架，能够有效支持敏捷开发团队的工作流程。

Abstract: Multi-agent Large Language Model (LLM) systems have been leading the way in
applied LLM research across a number of fields. One notable area is software
development, where researchers have advanced the automation of code
implementation, code testing, code maintenance, inter alia, using LLM agents.
However, software development is a multifaceted environment that extends beyond
just code. As such, a successful LLM system must factor in multiple stages of
the software development life-cycle (SDLC). In this paper, we propose a vision
for ALMAS, an Autonomous LLM-based Multi-Agent Software Engineering framework,
which follows the above SDLC philosophy such that it may work within an agile
software development team to perform several tasks end-to-end. ALMAS aligns its
agents with agile roles, and can be used in a modular fashion to seamlessly
integrate with human developers and their development environment. We showcase
the progress towards ALMAS through our published works and a use case
demonstrating the framework, where ALMAS is able to seamlessly generate an
application and add a new feature.

</details>


### [3] [Relative Code Comprehensibility Prediction](https://arxiv.org/abs/2510.03474)
*Nadeeshan De Silva,Martin Kellogg,Oscar Chaparro*

Main category: cs.SE

TL;DR: 该论文提出使用相对可理解性预测模型替代绝对可理解性预测模型，通过比较两个代码片段的可理解性来减轻人类数据中的噪声影响，实验证明相对模型性能显著优于绝对模型。


<details>
  <summary>Details</summary>
Motivation: 现有代码可理解性度量指标和机器学习模型预测准确度有限，主要原因是人类可理解性数据存在固有噪声，直接预测绝对可理解性容易受到噪声干扰。

Method: 提出训练模型预测两个代码片段的相对可理解性（哪个更容易理解），而不是单独预测每个片段的绝对可理解性。使用包含150个Java代码片段和12.5k个人类可理解性测量的数据集进行实验。

Result: 绝对可理解性模型相比基线最多提升33.4%，且经常表现不佳；而相对可理解性模型平均提升137.8%（片段级别）和74.7%（开发者级别），性能显著更好。

Conclusion: 相对可理解性模型能更有效地从数据中学习，支持其在实际软件工程任务中的适用性。

Abstract: Automatically predicting how difficult it is for humans to understand a code
snippet can assist developers in tasks like deciding when and where to
refactor. Despite many proposed code comprehensibility metrics, studies have
shown they often correlate poorly with actual measurements of human
comprehensibility. This has motivated the use of machine learning models to
predict human comprehensibility directly from code, but these models have also
shown limited accuracy.
  We argue that model inaccuracy stems from inherent noise in human
comprehensibility data, which confuses models trained to predict it directly.
To address this, we propose training models to predict the relative
comprehensibility of two code snippets - that is, predicting which snippet a
human would find easier to understand without predicting each snippet's
comprehensibility in isolation. This mitigates noise in predicting 'absolute'
comprehensibility measurements, but is still useful for downstream
software-engineering tasks like assessing whether refactoring improves or
hinders comprehensibility.
  We conducted a study to assess and compare the effectiveness of absolute and
relative code comprehensibility prediction via machine learning. We used a
dataset of 150 Java code snippets and 12.5k human comprehensibility
measurements from prior user studies, comparing the models' performance with
naive baselines (eg 'always predict the majority class'). Our findings indicate
that absolute comprehensibility models improve over the baselines by at most
33.4% and frequently underperform. In contrast, relative comprehensibility
models are substantially better, with average improvements of 137.8% and 74.7%
for snippet-wise and developer-wise prediction, respectively. These results
suggest that relative comprehensibility models learn more effectively from the
data, supporting their practical applicability for downstream SE tasks.

</details>


### [4] [LLM Agents for Automated Dependency Upgrades](https://arxiv.org/abs/2510.03480)
*Vali Tawosi,Salwa Alamir,Xiaomo Liu,Manuela Veloso*

Main category: cs.SE

TL;DR: 提出一个基于LLM代理的框架，结合迁移文档自动推荐和应用代码更新，确保与新版本库的兼容性。


<details>
  <summary>Details</summary>
Motivation: 随着代码库扩展，库依赖会过时需要更新以保持创新和安全，但更新可能引入破坏性变更，需要大量开发时间维护。

Method: 使用LLM代理框架（包括摘要代理、控制代理和代码代理），结合迁移文档自动定位库使用并实施推荐修复。

Result: 在工业用例中测试，使用更少的token完成升级，达到71.4%的精确度，相比现有方法更高效有效。

Conclusion: 该方法能高效自动更新库依赖，减少开发维护负担，提升代码库的现代化和安全性。

Abstract: As a codebase expands over time, its library dependencies can become outdated
and require updates to maintain innovation and security. However, updating a
library can introduce breaking changes in the code, necessitating significant
developer time for maintenance. To address this, we introduce a framework of
LLM agents to be used in combination with migration documentation to
automatically recommend and apply code updates and ensure compatibility with
new versions. Our solution can automatically localize updated library usages in
live Java codebases and implement recommended fixes in a user-friendly manner.
The system architecture consists of multiple key components: a Summary Agent,
Control Agent, and Code Agent. To validate our approach, we apply the framework
on an industrial use case by which we create three synthetic code repositories
with major Upgrade changes and benchmark our approach against state-of-the-art
methods. Results show that our approach not only performs upgrades using fewer
tokens across all cases but also achieves a precision of 71.4%, highlighting
its efficiency and effectiveness compared to state-of-the-art methods.

</details>


### [5] [AgentHub: A Research Agenda for Agent Sharing Infrastructure](https://arxiv.org/abs/2510.03495)
*Erik Pautsch,Tanmay Singla,Wenxin Jiang,Huiyun Peng,Behnaz Hassanshahi,Konstantin Läufer,George K. Thiruvathukal,James C. Davis*

Main category: cs.SE

TL;DR: 提出了AgentHub研究议程，旨在解决LLM智能体生态系统中发现、评估和治理基础设施碎片化的问题，推动构建可靠可扩展的智能体生态系统。


<details>
  <summary>Details</summary>
Motivation: 当前LLM智能体基础设施碎片化，缺乏像npm和Hugging Face那样的成熟生态系统，现有研究仅关注分发、命名或协议协商等狭窄领域，需要更全面的软件工程视角来改善开源分发和重用。

Method: 提出AgentHub研究议程，通过界定能力清晰度、生命周期透明度、互操作性、治理、安全性和工作流集成等关键挑战，为构建智能体生态系统制定社区范围内的路线图。

Result: 构建了一个研究框架，明确了智能体生态系统发展的关键维度，为未来智能体共享、信任和组合提供了理论基础。

Conclusion: AgentHub愿景是让智能体能够像今天的软件库一样无缝共享、信任和组合，推动智能体生态系统的成熟发展。

Abstract: LLM-based agents are rapidly proliferating, yet the infrastructure for
discovering, evaluating, and governing them remains fragmented compared to
mature ecosystems like software package registries (e.g., npm) and model hubs
(e.g., Hugging Face). Recent research and engineering works have begun to
consider the requisite infrastructure, but so far they focus narrowly -- on
distribution, naming, or protocol negotiation. However, considering broader
software engineering requirements would improve open-source distribution and
ease reuse. We therefore propose AgentHub, a research agenda for agent sharing.
By framing the key challenges of capability clarity, lifecycle transparency,
interoperability, governance, security, and workflow integration, AgentHub
charts a community-wide agenda for building reliable and scalable agent
ecosystems. Our vision is a future where agents can be shared, trusted, and
composed as seamlessly as today's software libraries.

</details>


### [6] [REFINE: Enhancing Program Repair Agents through Context-Aware Patch Refinement](https://arxiv.org/abs/2510.03588)
*Anvith Pabba,Simin Chen,Alex Mathai,Anindya Chakraborty,Baishakhi Ray*

Main category: cs.SE

TL;DR: 提出了Refine框架，通过系统化地将部分正确的草稿补丁转化为正确补丁，解决了LLM在自动程序修复中因代码上下文理解不足和过度依赖测试套件而产生错误修复的问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的自动程序修复技术由于对代码上下文理解有限和过度依赖不完整的测试套件，经常产生部分正确的草稿补丁，无法生成完全正确的修复。

Method: Refine框架通过三个关键步骤：消除模糊问题和代码上下文的歧义、通过测试时扩展多样化补丁候选、通过LLM驱动的代码审查过程聚合部分修复。

Result: 在SWE-Bench Lite基准测试中，Refine实现了工作流方法中的最先进结果，将AutoCodeRover性能提升14.67%，达到51.67%的分数。在SWE-Bench Verified上，解决率提高12.2%，在多个APR系统中平均改进14%。

Conclusion: 精炼是当前APR流程中缺失的关键组件，代理协作在缩小接近正确与完全正确补丁之间的差距方面具有巨大潜力。

Abstract: Large Language Models (LLMs) have recently shown strong potential in
automatic program repair (APR), especially in repository-level settings where
the goal is to generate patches based on natural language issue descriptions,
large codebases, and regression tests. However, despite their promise, current
LLM-based APR techniques often struggle to produce correct fixes due to limited
understanding of code context and over-reliance on incomplete test suites. As a
result, they frequently generate Draft Patches-partially correct patches that
either incompletely address the bug or overfit to the test cases. In this work,
we propose a novel patch refinement framework, Refine, that systematically
transforms Draft Patches into correct ones. Refine addresses three key
challenges: disambiguating vague issue and code context, diversifying patch
candidates through test-time scaling, and aggregating partial fixes via an
LLM-powered code review process. We implement Refine as a general refinement
module that can be integrated into both open-agent-based and workflow-based APR
systems. Our evaluation on the SWE-Bench Lite benchmark shows that Refine
achieves state-of-the-art results among workflow-based approaches and
approaches the best-known performance across all APR categories. Specifically,
Refine boosts AutoCodeRover's performance by 14.67%, achieving a score of
51.67% and surpassing all prior baselines. On SWE-Bench Verified, Refine
improves the resolution rate by 12.2%, and when integrated across multiple APR
systems, it yields an average improvement of 14%-demonstrating its broad
effectiveness and generalizability. These results highlight the effectiveness
of refinement as a missing component in current APR pipelines and the potential
of agentic collaboration in closing the gap between near-correct and correct
patches. We also open source our code.

</details>


### [7] [Generating High-Level Test Cases from Requirements using LLM: An Industry Study](https://arxiv.org/abs/2510.03641)
*Satoshi Masuda,Satoshi Kouzawa,Kyousuke Sezai,Hidetoshi Suhara,Yasuaki Hiruta,Kunihiro Kudou*

Main category: cs.SE

TL;DR: 提出了一种仅使用提示词而不创建RAG的从需求文档生成高层次测试用例的方法，通过LLM先生成测试设计技术，再为每个技术生成测试用例，在蓝牙和Mozilla数据集上验证了可行性。


<details>
  <summary>Details</summary>
Motivation: 当前从需求文档生成高层次测试用例需要手动完成，工业界对使用LLM自动生成有强烈需求。现有RAG方法需要为每个应用定制知识系统，工作量大，且没有通用的非RAG生成方法。

Method: 首先将需求文档输入LLM生成对应的测试设计技术，然后为每个生成的测试设计技术生成高层次测试用例，并基于语义相似度验证评估方法。

Result: 在蓝牙和Mozilla数据集上进行实验，分别实现了0.81和0.37的宏召回率测量，表明该方法在不使用RAG的情况下具有实际应用可行性。

Conclusion: 提出的方法能够在不创建RAG的情况下从需求文档自动生成高层次测试用例，具有跨更广泛需求文档的泛化能力，适合实际应用。

Abstract: Currently, generating high-level test cases described in natural language
from requirement documents is performed manually. In the industry, including
companies specializing in software testing, there is a significant demand for
the automatic generation of high-level test cases from requirement documents
using Large Language Models (LLMs). Efforts to utilize LLMs for requirement
analysis are underway. In some cases, retrieval-augmented generation (RAG) is
employed for generating high-level test cases using LLMs. However, in practical
applications, it is necessary to create a RAG tailored to the knowledge system
of each specific application, which is labor-intensive. Moreover, when applying
high-level test case generation as a prompt, there is no established method for
instructing the generation of high-level test cases at a level applicable to
other specifications without using RAG. It is required to establish a method
for the automatic generation of high-level test cases that can be generalized
across a wider range of requirement documents. In this paper, we propose a
method for generating high-level (GHL) test cases from requirement documents
using only prompts, without creating RAGs. In the proposed method, first, the
requirement document is input into the LLM to generate test design techniques
corresponding to the requirement document. Then, high-level test cases are
generated for each of the generated test design techniques. Furthermore, we
verify an evaluation method based on semantic similarity of the generated
high-level test cases. In the experiments, we confirmed the method using
datasets from Bluetooth and Mozilla, where requirement documents and high-level
test cases are available, achieving macro-recall measurement of 0.81 and 0.37,
respectively. We believe that the method is feasible for practical application
in generating high-level test cases without using RAG.

</details>


### [8] [Detecting and Preventing Latent Risk Accumulation in High-Performance Software Systems](https://arxiv.org/abs/2510.03712)
*Jahidul Arafat,Kh. M. Moniruzzaman,Shamim Hossain,Fariha Tasmin,Kamrujjaman,Ahsan Habib Tareq*

Main category: cs.SE

TL;DR: 提出了首个系统性检测、预防和优化分布式系统中潜在风险的框架，通过集成数学建模、智能扰动测试和风险感知性能优化，将可靠性工程从被动事故管理转变为主动风险感知优化。


<details>
  <summary>Details</summary>
Motivation: 现代分布式系统采用激进的优化策略，这些策略会创建潜在风险——当优化失败时，异常性能掩盖了灾难性的脆弱性。当前可靠性工程专注于被动事故响应，而非主动检测优化引发的漏洞。

Method: 框架集成三个系统：HYDRA使用六种优化感知扰动策略实现89.7%的风险发现率；RAVEN提供持续生产监控，在1,748个场景中达到92.9%精度和93.8%召回率；APEX实现风险感知优化，保持96.6%基线性能同时减少59.2%潜在风险。

Result: 评估显示强统计验证（Cohen d>2.0）和卓越可重复性（r>0.92）。24周生产部署显示平均恢复时间减少69.1%，事故严重性减少78.6%，预防81起事故，产生144万美元年均收益，投资回报期3.2个月。

Conclusion: 该方法将可靠性工程从被动事故管理转变为主动风险感知优化，通过系统性潜在风险检测和预防框架显著提升系统可靠性。

Abstract: Modern distributed systems employ aggressive optimization strategies that
create latent risks - hidden vulnerabilities where exceptional performance
masks catastrophic fragility when optimizations fail. Cache layers achieving
99% hit rates can obscure database bottlenecks until cache failures trigger
100x load amplification and cascading collapse. Current reliability engineering
focuses on reactive incident response rather than proactive detection of
optimization-induced vulnerabilities. This paper presents the first
comprehensive framework for systematic latent risk detection, prevention, and
optimization through integrated mathematical modeling, intelligent perturbation
testing, and risk-aware performance optimization. We introduce the Latent Risk
Index (LRI) that correlates strongly with incident severity (r=0.863, p<0.001),
enabling predictive risk assessment. Our framework integrates three systems:
HYDRA employing six optimization-aware perturbation strategies achieving 89.7%
risk discovery rates, RAVEN providing continuous production monitoring with
92.9% precision and 93.8% recall across 1,748 scenarios, and APEX enabling
risk-aware optimization maintaining 96.6% baseline performance while reducing
latent risks by 59.2%. Evaluation across three testbed environments
demonstrates strong statistical validation with large effect sizes (Cohen
d>2.0) and exceptional reproducibility (r>0.92). Production deployment over 24
weeks shows 69.1% mean time to recovery reduction, 78.6% incident severity
reduction, and 81 prevented incidents generating 1.44M USD average annual
benefits with 3.2-month ROI. Our approach transforms reliability engineering
from reactive incident management to proactive risk-aware optimization.

</details>


### [9] [APIDA-Chat: Structured Synthesis of API Search Dialogues to Bootstrap Conversational Agents](https://arxiv.org/abs/2510.03743)
*Zachary Eberhart,Collin McMillan*

Main category: cs.SE

TL;DR: APIDA-Chat是一个开源管道，将符号对话脚本转换为真实的API搜索对话，使用轻量级模型生成廉价训练数据，解决小众或专有库对话数据稀缺的问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型助手在解释流行API时表现良好，但在小众或专有库上表现不佳，因为用于微调的多轮对话数据稀缺。

Method: 采用两阶段方法：第一阶段使用传统对话规划器与教师LLM合成"黄金集"对话；第二阶段使用微调后的学生模型与相同规划器，无需外部服务即可快速合成新对话。

Result: 微调后的学生模型BLEU从0.38提升到0.50，BERTScore从0.88提升到0.91，且完全在单个消费级GPU上运行。

Conclusion: APIDA-Chat提供了一个模块化、开源的解决方案，可作为未来工作的保守基准，有效解决小众API对话数据生成问题。

Abstract: Large-language-model assistants are suitable for explaining popular APIs, yet
they falter on niche or proprietary libraries because the multi-turn dialogue
data needed for fine-tuning are scarce. We present APIDA-Chat, an open-source
pipeline that converts symbolic dialogue-act "scripts" into realistic,
domain-grounded API Search conversations using a lightweight model for
inexpensive training data generation. Phase I pairs a legacy dialogue planner
with a high-capability teacher LLM (o4-mini) to synthesize a "gold set" of
realized dialogues; then, a smaller Llama 3.2 3B student model is fine-tuned on
this corpus. Phase II drops the teacher and reuses the same planner with the
fine-tuned model, allowing rapid, low-cost synthesis of new dialogues without
exposing source code to external services. The fine-tuned student improves BLEU
from 0.38 to 0.50 and BERTScore from 0.88 to 0.91 versus the base model while
running entirely on a single consumer GPU. All components are modular and
publicly released to serve as a conservative baseline for future work.
APIDA-Chat is open-sourced at https://github.com/Zeberhart/apida-chat and a
video demo is available at https://youtu.be/YqmZBHyGbPs .

</details>


### [10] [Code4MeV2: a Research-oriented Code-completion Platform](https://arxiv.org/abs/2510.03755)
*Roham Koohestani,Parham Bateni,Aydin Ebrahimi,Behdad Etezadi,Kiarash Karimi,Maliheh Izadi*

Main category: cs.SE

TL;DR: Code4MeV2是一个开源代码补全插件，旨在解决AI代码补全工具用户交互数据被大公司垄断的问题，为学术界提供模块化、透明的数据收集框架。


<details>
  <summary>Details</summary>
Motivation: AI代码补全工具的用户交互数据被大公司垄断，阻碍了学术研究。研究人员需要开发专用平台来研究人机交互，这使得可重复研究和大规模数据分析变得不切实际。

Method: 采用客户端-服务器架构，提供内联代码补全和上下文感知聊天助手。核心贡献是模块化、透明的数据收集框架，让研究人员能够精细控制遥测和上下文收集。

Result: Code4MeV2在代码补全方面达到行业可比性能，平均延迟为200毫秒。通过专家评估和8名参与者的用户研究进行评估，获得研究人员和日常用户的积极反馈。

Conclusion: Code4MeV2成功解决了AI代码补全工具数据访问受限的问题，为学术界提供了实用的研究工具，邀请社区采用和贡献。

Abstract: The adoption of AI-powered code completion tools in software development has
increased substantially, yet the user interaction data produced by these
systems remain proprietary within large corporations. This creates a barrier
for the academic community, as researchers must often develop dedicated
platforms to conduct studies on human--AI interaction, making reproducible
research and large-scale data analysis impractical. In this work, we introduce
Code4MeV2, a research-oriented, open-source code completion plugin for
JetBrains IDEs, as a solution to this limitation. Code4MeV2 is designed using a
client--server architecture and features inline code completion and a
context-aware chat assistant. Its core contribution is a modular and
transparent data collection framework that gives researchers fine-grained
control over telemetry and context gathering. Code4MeV2 achieves
industry-comparable performance in terms of code completion, with an average
latency of 200~ms. We assess our tool through a combination of an expert
evaluation and a user study with eight participants. Feedback from both
researchers and daily users highlights its informativeness and usefulness. We
invite the community to adopt and contribute to this tool. More information
about the tool can be found at https://app.code4me.me.

</details>


### [11] [A First Look at the Lifecycle of DL-Specific Self-Admitted Technical Debt](https://arxiv.org/abs/2510.03802)
*Gilberto Recupito,Vincenzo De Martino,Dario Di Nucci,Fabio Palomba*

Main category: cs.SE

TL;DR: 本研究分析了深度学习系统中技术债务的生命周期，发现在项目早期和中期阶段最常引入技术债务，其中训练和硬件阶段的技术债务持续时间最长。


<details>
  <summary>Details</summary>
Motivation: 深度学习系统的快速采用带来了独特的软件质量挑战，特别是自认技术债务对系统可维护性的影响。然而，深度学习特定技术债务的生命周期尚未得到充分研究。

Method: 使用软件仓库挖掘技术，分析了40个机器学习项目中的185个深度学习特定技术债务实例，通过提交历史跟踪技术债务的引入和持续性。

Result: 深度学习特定技术债务主要在项目开发的早期和中期阶段引入，训练和硬件阶段的技术债务持续时间最长，开发者在功能实现和错误修复时更频繁引入技术债务。

Conclusion: 需要针对深度学习系统的技术债务管理策略，通过理解技术债务的时间特征和演化规律，开发者可以在关键阶段优先干预，提高系统的可维护性和质量。

Abstract: The rapid adoption of Deep Learning (DL)-enabled systems has revolutionized
software development, driving innovation across various domains. However, these
systems also introduce unique challenges, particularly in maintaining software
quality and performance. Among these challenges, Self-Admitted Technical Debt
(SATD) has emerged as a growing concern, significantly impacting the
maintainability and overall quality of ML and DL-enabled systems. Despite its
critical implications, the lifecycle of DL-specific SATD, how developers
introduce, acknowledge, and address it over time-remains underexplored. This
study presents a preliminary analysis of the persistence and lifecycle of
DL-specific SATD in DL-enabled systems. The purpose of this project is to
uncover the patterns of SATD introduction, recognition, and durability during
the development life cycle, providing information on how to manage these
issues. Using mining software repository techniques, we examined 40 ML
projects, focusing on 185 DL-specific SATD instances. The analysis tracked the
introduction and persistence of SATD instances through project commit histories
to assess their lifecycle and developer actions. The findings indicate that
DL-specific SATD is predominantly introduced during the early and middle stages
of project development. Training and Hardware phases showed the longest SATD
durations, highlighting critical areas where debt accumulates and persists.
Additionally, developers introduce DL-specific SATD more frequently during
feature implementation and bug fixes. This study emphasizes the need for
targeted DL-specific SATD management strategies in DL-enabled systems to
mitigate its impact. By understanding the temporal characteristics and
evolution of DL-specific SATD, developers can prioritize interventions at
critical stages to improve the maintainability and quality of the system.

</details>


### [12] [Smart Paste: Automatically Fixing Copy/Paste for Google Developers](https://arxiv.org/abs/2510.03843)
*Vincent Nguyen,Guilherme Herzog,José Cambronero,Marcus Revaj,Aditya Kini,Alexander Frömmgen,Maxim Tabachnyk*

Main category: cs.SE

TL;DR: 开发了Smart Paste IDE功能，通过深度学习预测粘贴代码后的编辑需求，在Google内部部署后获得45%接受率，占公司代码总量的1%以上


<details>
  <summary>Details</summary>
Motivation: 手动编辑粘贴代码是开发者长期痛点，Google内部观察到代码粘贴频率是手动输入的4倍，且经常需要后续编辑

Method: 迭代开发和扩展Smart Paste IDE功能，涵盖用户体验、系统集成和模型能力，使用深度学习预测粘贴后的编辑需求

Result: 部署后获得压倒性积极反馈，接受率达到45%，在Google企业规模下，这些接受的建议占公司所有代码的1%以上

Conclusion: Smart Paste成功展示了AI实践者在功能开发中的整体方法，可作为IDE功能开发的指南

Abstract: Manually editing pasted code is a long-standing developer pain point. In
internal software development at Google, we observe that code is pasted 4 times
more often than it is manually typed. These paste actions frequently require
follow-up edits, ranging from simple reformatting and renaming to more complex
style adjustments and cross-language translations. Prior work has shown deep
learning can be used to predict these edits. In this work, we show how to
iteratively develop and scale Smart Paste, an IDE feature for post-paste edit
suggestions, to Google's development environment. This experience can serve as
a guide for AI practitioners on a holistic approach to feature development,
covering user experience, system integration, and model capabilities. Since
deployment, Smart Paste has had overwhelmingly positive feedback with a 45%
acceptance rate. At Google's enterprise scale, these accepted suggestions
account substantially for over 1% of all code written company-wide.

</details>


### [13] [Designing Empirical Studies on LLM-Based Code Generation: Towards a Reference Framework](https://arxiv.org/abs/2510.03862)
*Nathalia Nascimento,Everton Guimaraes,Paulo Alencar*

Main category: cs.SE

TL;DR: 提出了一个用于设计和报告基于LLM的代码生成实证研究的理论框架，以解决当前评估缺乏标准化的问题。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在代码生成方面的实证评估缺乏标准化，不同研究在目标、任务和指标上差异很大，限制了可比性和可重复性。

Method: 基于作者先前经验和近期研究的比较分析，构建了一个理论框架，围绕问题来源、质量属性和指标等核心组件组织评估。

Result: 通过代表性案例映射展示了该框架的适用性，并确定了改进机会。

Conclusion: 该框架为标准化LLM在软件工程环境中的评估提供了基础，未来计划将其发展成更成熟和稳健的工具。

Abstract: The rise of large language models (LLMs) has introduced transformative
potential in automated code generation, addressing a wide range of software
engineering challenges. However, empirical evaluation of LLM-based code
generation lacks standardization, with studies varying widely in goals, tasks,
and metrics, which limits comparability and reproducibility. In this paper, we
propose a theoretical framework for designing and reporting empirical studies
on LLM-based code generation. The framework is grounded in both our prior
experience conducting such experiments and a comparative analysis of key
similarities and differences among recent studies. It organizes evaluation
around core components such as problem sources, quality attributes, and
metrics, supporting structured and systematic experimentation. We demonstrate
its applicability through representative case mappings and identify
opportunities for refinement. Looking forward, we plan to evolve the framework
into a more robust and mature tool for standardizing LLM evaluation across
software engineering contexts.

</details>


### [14] [Adversarial Agent Collaboration for C to Rust Translation](https://arxiv.org/abs/2510.03879)
*Tianyu Li,Ruishi Li,Bo Wang,Brandon Paulsen,Umang Mathur,Prateek Saxena*

Main category: cs.SE

TL;DR: ACToR是一个基于LLM代理的C到Rust翻译器，通过生成器和判别器代理的对抗协作，能够可靠地将大型C代码库（平均485行）转换为内存安全的Rust代码，测试通过率超过90%。


<details>
  <summary>Details</summary>
Motivation: 现有C到Rust的翻译方法（包括LLM辅助方法）无法处理大型代码库（>500行），因为它们依赖复杂的程序分析，经常失败。需要一种能够可靠翻译大型C程序的方法。

Method: 采用对抗性方法，让生成器代理合成和优化Rust翻译以通过测试套件，判别器代理则寻找新的失败测试，两者迭代协作。

Result: 成功翻译了63个真实世界的命令行工具（平均485行代码），测试通过率超过90%，无需人工干预。相比基线方法，翻译正确性提高了18.9%。

Conclusion: ACToR是第一个能够可靠翻译这种规模C程序的系统，通过简单的LLM代理对抗方法实现了高效的大型代码库翻译。

Abstract: Translating C to memory-safe languages, like Rust, prevents critical memory
safety vulnerabilities that are prevalent in legacy C software. Existing
approaches for C to safe Rust translation, including LLM-assisted ones, do not
generalize on larger (> 500 LoC) C codebases because they depend on complex
program analyses that frequently break. In this work, we present ACToR
(Adversarial C To Rust translator), a simple LLM agent-based approach. Inspired
by GANs, ACToR pits a generator agent against a discriminator agent, which
collaborate to iteratively generate a Rust translation. On each iteration, the
translator agent synthesizes and refines a Rust translation to pass an existing
suite of tests, and then the discriminator agent finds new failing tests. We
demonstrate that ACToR translates all of the 63 real-world command line
utilities considered in our benchmarks, which have an average size of 485 lines
of code, and it achieves over 90% test pass rate with zero human intervention.
To our knowledge, it is the first such system that reliably translates C
programs of this scale. Furthermore, ACToR improves translation correctness by
up to 18.9% compared to baseline, non-adversarial approaches.

</details>


### [15] [Rethinking Services in the Quantum Age: The SOQ Paradigm](https://arxiv.org/abs/2510.03890)
*Jose Garcia-Alonso,Enrique Moguel,Jaime Alvarado-Valiente,Javier Romero-Alvarez,Álvaro M. Aparicio-Morales,Juan M. Murillo,Francisco Javier Cavero,Adrián Romero-Flores,Alfonso E. Marquez-Chamorro,José Antonio Parejo,Antonio Ruiz-Cortés,Giuseppe Bisicchia,Alessandro Bocci,Antonio Brogi*

Main category: cs.SE

TL;DR: 提出服务导向量子(SOQ)新范式，将量子软件系统重新构想为自主、可组合、可互操作的量子服务实体，不同于传统将量子能力作为辅助组件的方法。


<details>
  <summary>Details</summary>
Motivation: 量子计算从理论走向实际应用，但在集成到现实软件系统时面临硬件脆弱性、平台异构性和缺乏稳健软件工程实践的约束。

Method: 定义SOQ基础原则，提出分层技术栈支持其实现，包括互操作性、混合性、定价模型、服务抽象和人力发展等关键挑战。

Result: SOQ范式能够实现量子计算到现实软件系统的可扩展、模块化和可互操作集成，无需依赖专用经典环境来管理量子处理。

Conclusion: SOQ方法对量子技术发展至关重要，为量子软件工程提供了新的系统化框架。

Abstract: Quantum computing is rapidly progressing from theoretical promise to
practical implementation, offering significant computational advantages for
tasks in optimization, simulation, cryptography, and machine learning. However,
its integration into real-world software systems remains constrained by
hardware fragility, platform heterogeneity, and the absence of robust software
engineering practices. This paper introduces Service-Oriented Quantum (SOQ), a
novel paradigm that reimagines quantum software systems through the lens of
classical service-oriented computing. Unlike prior approaches such as Quantum
Service-Oriented Computing (QSOC), which treat quantum capabilities as
auxiliary components within classical systems, SOQ positions quantum services
as autonomous, composable, and interoperable entities. We define the
foundational principles of SOQ, propose a layered technology stack to support
its realization, and identify the key research and engineering challenges that
must be addressed, including interoperability, hybridity, pricing models,
service abstractions, and workforce development. This approach is of vital
importance for the advancement of quantum technology because it enables the
scalable, modular, and interoperable integration of quantum computing into
real-world software systems independently and without relying on a dedicated
classical environment to manage quantum processing.

</details>


### [16] [A Brief History of the Waterfall Model: Past, Present, and Future](https://arxiv.org/abs/2510.03894)
*Antonios Saravanos*

Main category: cs.SE

TL;DR: 本文对瀑布模型进行了历史回顾和批判性分析，探讨了其起源、演变以及在当代软件开发中的持续影响。尽管瀑布模型因其僵化性和高失败率而受到批评，但其原则仍在特定领域和混合开发框架中发挥作用。


<details>
  <summary>Details</summary>
Motivation: 重新评估瀑布模型的历史地位和当代价值，探讨其如何从独立框架转变为现代混合方法论的组成部分，并分析其持续相关性的原因。

Method: 基于学术文献的综合分析，追溯瀑布模型的概念起源、Royce的形式化描述，以及数十年来行业采用和批评的演变过程。

Result: 瀑布模型虽然不再是主流开发方法，但其原则继续影响当代混合开发框架，在特定领域仍保持实用性，并作为情境感知开发策略的组成部分而保持相关性。

Conclusion: 瀑布模型通过适应性和在混合方法中的整合保持了持久相关性，理解其局限性和优势有助于从业者在多样化开发环境中做出更明智的方法选择和流程设计决策。

Abstract: The waterfall model, one of the earliest software development methodologies,
has played a foundational role in shaping contemporary software engineering
practices. This paper provides a historical and critical overview of the model,
tracing its conceptual origins in software engineering, its formalization by
Royce, and its evolution through decades of industry adoption and critique.
Although often criticized for its rigidity, shortcomings, and high failure
rates, the waterfall model persists in specific domains. Its principles
continue to influence contemporary hybrid development frameworks that combine
traditional and agile methods. Drawing on a range of scholarly sources, this
study synthesizes key developments in the perception and application of the
waterfall model. The analysis highlights how the model has shifted from a
standalone framework to a component within modern hybrid methodologies. By
revisiting its origins, assessing its present utility, and examining its role
in contemporary development practices, this paper argues that the waterfall
model remains relevant, not as a relic of the past but as part of context-aware
development strategies. The paper contends that the model's enduring relevance
lies in its adaptability. By recognizing both its limitations and its
strengths, and by understanding its integration within hybrid approaches,
practitioners can make more informed decisions about methodology selection and
process design in diverse development environments.

</details>


### [17] [Multi-Agent Code-Orchestrated Generation for Reliable Infrastructure-as-Code](https://arxiv.org/abs/2510.03902)
*Rana Nameer Hussain Khan,Dawood Wasif,Jin-Hee Cho,Ali Butt*

Main category: cs.SE

TL;DR: MACOG是一个基于多智能体LLM的IaC生成架构，通过分解任务到专门智能体，显著提升了Terraform配置的语法正确性、策略合规性和语义一致性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在生成基础设施即代码时存在语法错误、策略违规和不可扩展设计等问题，需要更可靠的生成方法。

Method: 提出多智能体架构，包含架构师、提供商协调器、工程师、审查员、安全证明者、成本容量规划师、DevOps和记忆策展人等专门智能体，通过共享黑板和有限状态协调器交互。

Result: 在IaC-Eval基准测试中表现优异，GPT-5从54.90提升到74.02，Gemini-2.5 Pro从43.56提升到60.13，并在BLEU、CodeBERTScore和LLM-judge指标上均有提升。

Conclusion: MACOG通过多智能体协作和约束解码、部署反馈等机制，显著提升了IaC生成的质量和可靠性。

Abstract: The increasing complexity of cloud-native infrastructure has made
Infrastructure-as-Code (IaC) essential for reproducible and scalable
deployments. While large language models (LLMs) have shown promise in
generating IaC snippets from natural language prompts, their monolithic,
single-pass generation approach often results in syntactic errors, policy
violations, and unscalable designs. In this paper, we propose MACOG
(Multi-Agent Code-Orchestrated Generation), a novel multi-agent LLM-based
architecture for IaC generation that decomposes the task into modular subtasks
handled by specialized agents: Architect, Provider Harmonizer, Engineer,
Reviewer, Security Prover, Cost and Capacity Planner, DevOps, and Memory
Curator. The agents interact via a shared-blackboard, finite-state orchestrator
layer, and collectively produce Terraform configurations that are not only
syntactically valid but also policy-compliant and semantically coherent. To
ensure infrastructure correctness and governance, we incorporate Terraform Plan
for execution validation and Open Policy Agent (OPA) for customizable policy
enforcement. We evaluate MACOG using the IaC-Eval benchmark, where MACOG is the
top enhancement across models, e.g., GPT-5 improves from 54.90 (RAG) to 74.02
and Gemini-2.5 Pro from 43.56 to 60.13, with concurrent gains on BLEU,
CodeBERTScore, and an LLM-judge metric. Ablations show constrained decoding and
deploy feedback are critical: removing them drops IaC-Eval to 64.89 and 56.93,
respectively.

</details>


### [18] [Refactoring with LLMs: Bridging Human Expertise and Machine Understanding](https://arxiv.org/abs/2510.03914)
*Yonnel Chen Kuang Piao,Jean Carlors Paul,Leuson Da Silva,Arghavan Moradi Dakhel,Mohammad Hamdaqa,Foutse Khomh*

Main category: cs.SE

TL;DR: 本研究探索基于人类最佳实践指南的指令策略能否增强大语言模型执行多样化代码重构任务的能力，结果表明基于Fowler指南的指令设计能让LLMs成功执行所有基准重构类型并在真实场景中保持程序语义。


<details>
  <summary>Details</summary>
Motivation: 代码重构是改善代码质量和可维护性的重要实践，但开发者常因时间、精力和缺乏即时功能回报而忽视重构。现有自动化重构工具支持的重构类型有限，需要探索更有效的自动化重构方法。

Method: 利用最先进LLMs的指令跟随和代码理解能力，基于Martin Fowler的重构指南设计多种指令策略，编码61种知名重构类型的动机、步骤和转换目标，并在基准示例和GitHub真实代码片段上进行评估。

Result: 基于Fowler指南的指令设计使LLMs能成功执行所有基准重构类型，并在真实场景中保持程序语义。规则型指令在特定场景表现更好，而允许模型关注重构整体目标而非固定转换类型能带来更大代码质量改进。

Conclusion: 基于人类最佳实践指南的指令策略能有效增强LLMs执行多样化重构任务的能力，为自动化代码重构提供了有前景的新途径。

Abstract: Code refactoring is a fundamental software engineering practice aimed at
improving code quality and maintainability. Despite its importance, developers
often neglect refactoring due to the significant time, effort, and resources it
requires, as well as the lack of immediate functional rewards. Although several
automated refactoring tools have been proposed, they remain limited in
supporting a broad spectrum of refactoring types. In this study, we explore
whether instruction strategies inspired by human best-practice guidelines can
enhance the ability of Large Language Models (LLMs) to perform diverse
refactoring tasks automatically. Leveraging the instruction-following and code
comprehension capabilities of state-of-the-art LLMs (e.g., GPT-mini and
DeepSeek-V3), we draw on Martin Fowler's refactoring guidelines to design
multiple instruction strategies that encode motivations, procedural steps, and
transformation objectives for 61 well-known refactoring types. We evaluate
these strategies on benchmark examples and real-world code snippets from GitHub
projects. Our results show that instruction designs grounded in Fowler's
guidelines enable LLMs to successfully perform all benchmark refactoring types
and preserve program semantics in real-world settings, an essential criterion
for effective refactoring. Moreover, while descriptive instructions are more
interpretable to humans, our results show that rule-based instructions often
lead to better performance in specific scenarios. Interestingly, allowing
models to focus on the overall goal of refactoring, rather than prescribing a
fixed transformation type, can yield even greater improvements in code quality.

</details>


### [19] [Why Does the Engineering Manager Still Exist in Agile Software Development?](https://arxiv.org/abs/2510.03920)
*Ravi Kalluri*

Main category: cs.SE

TL;DR: 尽管敏捷方法强调去中心化决策和团队自主性，但工程经理在敏捷软件组织中仍然存在，本文通过多维框架分析这一现象，并提出协调敏捷原则与管理必要性的概念模型。


<details>
  <summary>Details</summary>
Motivation: 探索敏捷组织中工程经理持续存在的原因，解决敏捷理论去中心化与管理实践需求之间的明显矛盾。

Method: 采用系统性文献综述方法，辅以案例研究，从历史背景、理论张力、组织现实、实证证据、管理角色演变等多个维度进行分析。

Result: 构建了一个多维分析框架，揭示了工程经理在敏捷环境中的持续价值和功能，提出了协调敏捷原则与管理必要性的概念模型。

Conclusion: 工程经理在敏捷组织中具有持续的必要性，需要重新定义其角色以平衡敏捷原则与组织管理需求，为实践者、研究者和工具设计者提供指导。

Abstract: Although Agile methodologies emphasize decentralized decision-making and team
autonomy, engineering managers continue to be employed in Agile software
organizations. This apparent paradox suggests that traditional managerial
functions persist despite the theoretical displacement of managerial hierarchy
in Agile. This paper explores the persistence of engineering managers through a
multidimensional framework encompassing historical context, theoretical
tensions, organizational realities, empirical evidence, evolving managerial
roles, and practical implications. A systematic literature review underpins our
multifaceted analysis, supplemented by illustrative case studies. We conclude
by proposing a conceptual model that reconciles Agile principles with
managerial necessity, offering guidance for practitioners, researchers, and
tool designers. Implications for leadership development, tool integration, and
future research are discussed.

</details>


### [20] [Bamboo: LLM-Driven Discovery of API-Permission Mappings in the Android Framework](https://arxiv.org/abs/2510.04078)
*Han Hu,Wei Minn,Yonghui Liu,Jiakun Liu,Ferdian Thung,Terry Yue Zhuo,Lwin Khin Shar,Debin Gao,David Lo*

Main category: cs.SE

TL;DR: 本文提出了一种基于大语言模型(LLMs)的新方法，用于系统性地分析Android API权限映射，解决了现有方法在适应性和代码覆盖率方面的不足。


<details>
  <summary>Details</summary>
Motivation: Android官方API文档存在不精确和不完整的问题，导致开发者难以准确识别必要权限，可能引发安全违规和应用故障。现有基于静态和动态代码分析的方法存在适应SDK更新困难、代码覆盖率有限等问题。

Method: 采用大语言模型(LLMs)，结合双角色提示策略和API驱动的代码生成方法，构建了API权限映射发现工具。

Result: 在Android版本6、7和10中分别识别出2,234、3,552和4,576个API权限映射，显著优于现有基线方法。

Conclusion: 基于LLM的方法在识别API权限映射方面表现出色，能够有效补充官方文档的不足，为Android应用开发提供更准确的权限指导。

Abstract: The permission mechanism in the Android Framework is integral to safeguarding
the privacy of users by managing users' and processes' access to sensitive
resources and operations. As such, developers need to be equipped with an
in-depth understanding of API permissions to build robust Android apps.
Unfortunately, the official API documentation by Android chronically suffers
from imprecision and incompleteness, causing developers to spend significant
effort to accurately discern necessary permissions. This potentially leads to
incorrect permission declarations in Android app development, potentially
resulting in security violations and app failures. Recent efforts in improving
permission specification primarily leverage static and dynamic code analyses to
uncover API-permission mappings within the Android framework. Yet, these
methodologies encounter substantial shortcomings, including poor adaptability
to Android SDK and Framework updates, restricted code coverage, and a
propensity to overlook essential API-permission mappings in intricate
codebases. This paper introduces a pioneering approach utilizing large language
models (LLMs) for a systematic examination of API-permission mappings. In
addition to employing LLMs, we integrate a dual-role prompting strategy and an
API-driven code generation approach into our mapping discovery pipeline,
resulting in the development of the corresponding tool, \tool{}. We formulate
three research questions to evaluate the efficacy of \tool{} against
state-of-the-art baselines, assess the completeness of official SDK
documentation, and analyze the evolution of permission-required APIs across
different SDK releases. Our experimental results reveal that \tool{} identifies
2,234, 3,552, and 4,576 API-permission mappings in Android versions 6, 7, and
10 respectively, substantially outprforming existing baselines.

</details>


### [21] [GA4GC: Greener Agent for Greener Code via Multi-Objective Configuration Optimization](https://arxiv.org/abs/2510.04135)
*Jingzhi Gong,Yixin Bian,Luis de la Cal,Giovanni Pinna,Anisha Uteem,David Williams,Mar Zamorano,Karine Even-Mendoza,W. B. Langdon,Hector Menendez,Federica Sarro*

Main category: cs.SE

TL;DR: GA4GC框架通过发现帕累托最优代理超参数和提示模板，系统优化编码代理运行时间与代码性能之间的权衡，在SWE-Perf基准测试中实现高达135倍的超体积改进。


<details>
  <summary>Details</summary>
Motivation: LLM驱动的编码代理在工业部署中面临可持续性和可扩展性挑战，单次运行消耗超过10万token，环境成本可能超过优化效益。

Method: 引入GA4GC框架，系统性地优化编码代理运行时间（更环保的代理）和代码性能（更环保的代码）之间的权衡，通过发现帕累托最优代理超参数和提示模板。

Result: 在SWE-Perf基准测试中实现高达135倍的超体积改进，代理运行时间减少37.7%，同时提高正确性。温度被确立为最关键的超参数。

Conclusion: 研究结果为工业部署中平衡代理可持续性与代码优化有效性提供了可行的策略。

Abstract: Coding agents powered by LLMs face critical sustainability and scalability
challenges in industrial deployment, with single runs consuming over 100k
tokens and incurring environmental costs that may exceed optimization benefits.
This paper introduces GA4GC, the first framework to systematically optimize
coding agent runtime (greener agent) and code performance (greener code)
trade-offs by discovering Pareto-optimal agent hyperparameters and prompt
templates. Evaluation on the SWE-Perf benchmark demonstrates up to 135x
hypervolume improvement, reducing agent runtime by 37.7% while improving
correctness. Our findings establish temperature as the most critical
hyperparameter, and provide actionable strategies to balance agent
sustainability with code optimization effectiveness in industrial deployment.

</details>


### [22] [Detecting Semantic Clones of Unseen Functionality](https://arxiv.org/abs/2510.04143)
*Konstantinos Kitsios,Francesco Sovrano,Earl T. Barr,Alberto Bacchelli*

Main category: cs.SE

TL;DR: 该论文研究发现现有语义代码克隆检测模型在处理未见过的功能时性能显著下降，提出使用对比学习方法来提升模型对未见功能克隆的检测能力。


<details>
  <summary>Details</summary>
Motivation: 现有神经模型在语义代码克隆检测任务中虽然表现优异，但主要基于训练数据进行推理，对于训练中未见过的功能克隆检测效果较差。开发者实际需要检测各种类型的克隆，包括未见功能的克隆。

Method: 重新评估了6个最先进模型（包括任务特定模型和生成式LLM）在未见功能克隆检测上的表现，并提出使用对比学习：对任务特定模型替换最终分类器为对比分类器，对LLM提出对比上下文学习。

Result: 任务特定模型在未见功能克隆检测上F1下降高达48%（平均31%），LLM表现与任务特定模型相当但泛化能力更好（F1下降仅5%，平均3%）。使用对比学习后，任务特定模型F1提升高达26%（平均9%），LLM提升高达5%（平均3%）。

Conclusion: 对比学习能有效提升代码克隆检测模型对未见功能的泛化能力，LLM在未见功能克隆检测上具有更好的泛化性能。

Abstract: Semantic code clone detection is the task of detecting whether two snippets
of code implement the same functionality (e.g., Sort Array). Recently, many
neural models achieved near-perfect performance on this task. These models seek
to make inferences based on their training data. Consequently, they better
detect clones similar to those they have seen during training and may struggle
to detect those they have not. Developers seeking clones are, of course,
interested in both types of clones. We confirm this claim through a literature
review, identifying three practical clone detection tasks in which the model's
goal is to detect clones of a functionality even if it was trained on clones of
different functionalities. In light of this finding, we re-evaluate six
state-of-the-art models, including both task-specific models and generative
LLMs, on the task of detecting clones of unseen functionality. Our experiments
reveal a drop in F1 of up to 48% (average 31%) for task-specific models. LLMs
perform on par with task-specific models without explicit training for clone
detection, but generalize better to unseen functionalities, where F1 drops up
to 5% (average 3%) instead. We propose and evaluate the use of contrastive
learning to improve the performance of existing models on clones of unseen
functionality. We draw inspiration from the computer vision and natural
language processing fields where contrastive learning excels at measuring
similarity between two objects, even if they come from classes unseen during
training. We replace the final classifier of the task-specific models with a
contrastive classifier, while for the generative LLMs we propose contrastive
in-context learning, guiding the LLMs to focus on the differences between
clones and non-clones. The F1 on clones of unseen functionality is improved by
up to 26% (average 9%) for task-specific models and up to 5% (average 3%) for
LLMs.

</details>


### [23] [Multi Language Models for On-the-Fly Syntax Highlighting](https://arxiv.org/abs/2510.04166)
*Marco Edoardo Palma,Pooja Rani,Harald C. Gall*

Main category: cs.SE

TL;DR: 提出了一种统一的语法高亮模型，可同时支持六种主流编程语言，通过新颖的归一化技术提升泛化能力，并利用小样本学习减少对大数据集的依赖。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习语法高亮模型存在单语言限制、需要大量训练数据和资源密集训练等问题，在多语言环境中部署复杂且成本高。

Method: 采用Deep Abstraction过程，将暴力语法解析器的行为编码为快速统计模型，引入归一化技术增强泛化，并应用小样本学习减少数据需求。

Result: 实现了六种编程语言的统一高亮模型，部署复杂度降低六倍，在未见语言上表现优异，小样本学习可替代大数据集。

Conclusion: 该创新方法实现了跨多种编程语言的高效、可扩展且经济高效的语法高亮解决方案。

Abstract: Syntax highlighting is a critical feature in modern software development
environments, enhancing code readability and developer productivity. However,
delivering accurate highlighting in real time remains challenging for online
and web-based development tools due to strict time and memory constraints on
backend services. These systems must serve highlights rapidly and frequently,
even when code is partially valid or invalid. This has led to on-the-fly syntax
highlighting, where visual annotations are generated just before content is
served, often at high request rates and under incomplete input conditions. To
meet these demands efficiently, state-of-the-art models use deep learning to
learn the behavior of brute-force syntax highlighting resolvers, tools that are
easy to implement but too slow for production. Through the Deep Abstraction
process, brute-force strategies are encoded into fast statistical models that
achieve both high accuracy and low-latency inference. Despite their success,
such models face key challenges: they support only one programming language per
model, require large datasets from slow brute-force generators, and involve
resource-intensive training. In multi-language environments, this means
maintaining multiple independent models, increasing system complexity and
operational cost. This work addresses these issues by introducing a unified
model capable of highlighting up to six mainstream programming languages,
reducing deployment complexity by a factor of six and improving performance on
unseen languages. A novel normalization technique significantly enhances model
generalization, while few-shot learning experiments show that a small number of
oracle samples can replace large datasets, minimizing dependence on brute-force
generators. Combined, these innovations enable efficient, scalable, and
cost-effective syntax highlighting across diverse programming languages.

</details>


### [24] [Selecting Cybersecurity Requirements: Effects of LLM Use and Professional Software Development Experience](https://arxiv.org/abs/2510.04274)
*Damjan Fujs,Damjan Vavpotič,Tomaž Hovelja,Marko Poženel*

Main category: cs.SE

TL;DR: 研究探讨了LLM访问权限和专业软件开发经验如何影响网络安全需求优先级排序，发现LLM使用无显著影响，但经验差异在成本、用户体验和风险评估方面产生显著差异。


<details>
  <summary>Details</summary>
Motivation: 了解LLM工具和不同专业经验水平如何影响开发者在网络安全需求优先级决策中的表现，特别是在评估安全解决方案时。

Method: 23名研究生参与研究，使用MoSCoW方法对安全需求进行优先级排序，分为有/无LLM支持两组，随后按多个标准评估解决方案。

Result: LLM访问对参与者评估网络安全解决方案无显著影响；但经验组间在开发成本估算、用户体验影响感知和风险评估方面存在显著差异，经验更丰富者倾向于给出更高的用户体验影响评分和更低的风险估计。

Conclusion: 专业软件开发经验而非LLM工具访问是影响网络安全需求优先级决策的关键因素，经验丰富的开发者更关注用户体验并做出更准确的风险评估。

Abstract: This study investigates how access to Large Language Models (LLMs) and
varying levels of professional software development experience affect the
prioritization of cybersecurity requirements for web applications. Twenty-three
postgraduate students participated in a research study to prioritize security
requirements (SRs) using the MoSCoW method and subsequently rated their
proposed solutions against multiple evaluation criteria. We divided
participants into two groups (one with and the other without access to LLM
support during the task). Results showed no significant differences related to
LLM use, suggesting that access to LLMs did not noticeably influence how
participants evaluated cybersecurity solutions. However, statistically
significant differences emerged between experience groups for certain criteria,
such as estimated cost to develop a feature, perceived impact on user
experience, and risk assessment related to non-implementation of the proposed
feature. Participants with more professional experience tended to provide
higher ratings for user experience impact and lower risk estimates.

</details>


### [25] [Challenge on Optimization of Context Collection for Code Completion](https://arxiv.org/abs/2510.04349)
*Dmitry Ustalov,Egor Bogomolov,Alexander Bezzubov,Yaroslav Golubev,Evgeniy Glukhov,Georgii Levtsov,Vladimir Kovalenko*

Main category: cs.SE

TL;DR: 该论文介绍了JetBrains与Mistral AI在ASE 2025会议上组织的代码完成上下文收集优化挑战赛，旨在开发从源代码仓库高效收集上下文以改进Python和Kotlin代码补全的机制。


<details>
  <summary>Details</summary>
Motivation: 随着AI在软件工程中工作流和方法的快速发展，需要系统评估它们利用整个项目信息的能力，特别是在大型代码库中。

Method: 构建了基于许可开源项目的Python和Kotlin真实代码大型数据集，参与者开发高效上下文收集机制，使用chrF指标评估多个最先进神经模型的补全质量。

Result: 公开阶段有19个团队提交Python赛道解决方案，8个团队提交Kotlin赛道解决方案；私有阶段有6个团队竞争，其中5个团队向研讨会提交了论文。

Conclusion: 该挑战赛成功推动了代码补全上下文收集方法的发展，为大型代码库中的AI辅助软件工程提供了系统评估框架。

Abstract: The rapid advancement of workflows and methods for software engineering using
AI emphasizes the need for a systematic evaluation and analysis of their
ability to leverage information from entire projects, particularly in large
code bases. In this challenge on optimization of context collection for code
completion, organized by JetBrains in collaboration with Mistral AI as part of
the ASE 2025 conference, participants developed efficient mechanisms for
collecting context from source code repositories to improve fill-in-the-middle
code completions for Python and Kotlin. We constructed a large dataset of
real-world code in these two programming languages using permissively licensed
open-source projects. The submissions were evaluated based on their ability to
maximize completion quality for multiple state-of-the-art neural models using
the chrF metric. During the public phase of the competition, nineteen teams
submitted solutions to the Python track and eight teams submitted solutions to
the Kotlin track. In the private phase, six teams competed, of which five
submitted papers to the workshop.

</details>


### [26] [MacroBench: A Novel Testbed for Web Automation Scripts via Large Language Models](https://arxiv.org/abs/2510.04363)
*Hyunjun Kim,Sejong Kim*

Main category: cs.SE

TL;DR: MacroBench是一个评估LLMs从自然语言目标生成可重用浏览器自动化程序能力的代码优先基准，包含7个自托管网站和681个任务，通过端到端验证协议测试模型性能。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs能否通过阅读HTML/DOM并生成Python Selenium代码来合成可重用的浏览器自动化程序，填补现有基准在web自动化宏合成评估方面的空白。

Method: 构建7个自托管网站（Airbnb、TikTok、Reddit等类似平台），设计681个任务覆盖不同交互复杂度和目标难度，采用端到端验证协议包括静态检查、沙箱执行、DOM断言和数据库快照验证。

Result: GPT-4o-Mini达到96.8%成功率，GPT-4.1为95.3%，Gemini-2.5-Pro为89.0%，DeepSeek-V3.1为83.4%。模型在简单任务上可靠（91.7%），但在复杂工作流上完全失败（0.0%），且都不符合生产级编码实践。

Conclusion: LLMs在web自动化宏合成方面表现出分层性能，能可靠处理简单任务但无法应对复杂工作流，需要进一步改进以实现生产级质量。

Abstract: We introduce MacroBench, a code-first benchmark that evaluates whether LLMs
can synthesize reusable browser automation programs from natural language goals
by reading HTML/DOM and emitting Python with Selenium. MacroBench instantiates
seven self-hosted sites: Airbnb-like, TikTok-like, Reddit-like, Instagram-like,
Facebook-like, Discord-like, and Threads-like, covering 681 tasks across
interaction complexity and targeting difficulty. Our end-to-end protocol
validates generated code via static checks, sandboxed execution, and outcome
verification including DOM assertions and database snapshots, and includes a
safety suite for scraping, spam/abuse, and credential/privacy prompts. Across
2636 model-task runs, we observe stratified success: GPT-4o-Mini achieves 96.8
percent, GPT-4.1 achieves 95.3 percent, Gemini-2.5-Pro achieves 89.0 percent,
and DeepSeek-V3.1 achieves 83.4 percent. Models handle simple tasks reliably at
91.7 percent but fail on complex workflows at 0.0 percent, and none meet
production-quality coding practices despite functional completion. We release
our complete benchmark pipeline, evaluation framework, and experimental results
to enable reproducible assessment of macro synthesis for web automation.

</details>


### [27] [Reconsidering Requirements Engineering: Human-AI Collaboration in AI-Native Software Development](https://arxiv.org/abs/2510.04380)
*Mateen Ahmed Abbasi,Petri Ihantola,Tommi Mikkonen,Niko Mäkitalo*

Main category: cs.SE

TL;DR: 本文探讨人工智能如何改进需求工程实践，通过自动化任务、支持需求优先级排序和促进协作，同时分析AI带来的机遇与挑战，强调伦理实践和产学合作的重要性。


<details>
  <summary>Details</summary>
Motivation: 需求工程是软件开发成功的基础，但面临模糊性、利益冲突和需求演化等持续挑战。AI有潜力优化RE流程，但同时也引发伦理、偏见和透明度等新问题。

Method: 探索AI如何增强传统RE实践，包括自动化劳动密集型任务、支持需求优先级排序、促进利益相关者与AI系统协作，并分析AI带来的机遇与挑战。

Result: AI能够显著提升需求工程的效率和准确性，但需要解决伦理问题和建立可信赖的AI解决方案。

Conclusion: 应关注AI在RE中的伦理实践，加强产学合作，开发既强大又可信赖的实用AI解决方案，以适应快速发展的软件开发环境。

Abstract: Requirement Engineering (RE) is the foundation of successful software
development. In RE, the goal is to ensure that implemented systems satisfy
stakeholder needs through rigorous requirements elicitation, validation, and
evaluation processes. Despite its critical role, RE continues to face
persistent challenges, such as ambiguity, conflicting stakeholder needs, and
the complexity of managing evolving requirements. A common view is that
Artificial Intelligence (AI) has the potential to streamline the RE process,
resulting in improved efficiency, accuracy, and management actions. However,
using AI also introduces new concerns, such as ethical issues, biases, and lack
of transparency. This paper explores how AI can enhance traditional RE
practices by automating labor-intensive tasks, supporting requirement
prioritization, and facilitating collaboration between stakeholders and AI
systems. The paper also describes the opportunities and challenges that AI
brings to RE. In particular, the vision calls for ethical practices in AI,
along with a much-enhanced collaboration between academia and industry
professionals. The focus should be on creating not only powerful but also
trustworthy and practical AI solutions ready to adapt to the fast-paced world
of software development.

</details>


### [28] [Smart Hiring Redefined: An Intelligent Recruitment Management Platform](https://arxiv.org/abs/2510.04437)
*Fangzhe Wu,Dongyang Lyu,Xiaoqi Li*

Main category: cs.SE

TL;DR: 智能招聘管理系统通过自动化和数据驱动方法，显著提升招聘效率和准确性，解决传统招聘模式效率低、成本高和信息不对称的问题。


<details>
  <summary>Details</summary>
Motivation: 传统招聘模式因效率有限、成本高昂和信息不对称，难以满足企业对精准人才获取的日益增长需求。

Method: 采用自动化和数据驱动方法，包括大规模简历快速解析、候选人职位智能匹配和面试流程自动化调度。

Result: 智能招聘系统显著提升了招聘流程的效率和准确性，优化了招聘流程，降低了人力时间成本。

Conclusion: 智能招聘管理系统已成为现代组织人才战略不可或缺的组成部分，能够增强企业核心竞争力。

Abstract: Against the backdrop of deepening digital and intelligent transformation in
human resource management, traditional recruitment models struggle to fully
meet enterprises' growing demand for precise talent acquisition due to limited
efficiency, high costs, and information asymmetry. As a vital tool for
optimizing recruitment processes, reducing labor and time costs, and enhancing
core competitiveness, intelligent recruitment management systems become an
indispensable component of modern organizational talent strategies.Compared
with the labor intensive tasks of resume screening, candidate position
matching, and interview coordination in traditional manual recruitment,
intelligent recruitment systems significantly enhance the efficiency and
accuracy of the hiring process through automation and data driven approaches.
These systems enable rapid parsing of massive resume volumes, intelligent
matching of candidates to positions, and automated scheduling of interview
processes.

</details>


### [29] [Improving IR-based Bug Localization with Semantics-Driven Query Reduction](https://arxiv.org/abs/2510.04468)
*Asif Mohammed Samir,Mohammad Masudur Rahman*

Main category: cs.SE

TL;DR: IQLoc是一种结合信息检索和大型语言模型的软件缺陷定位方法，通过利用Transformer模型理解程序语义来改进缺陷定位效果


<details>
  <summary>Details</summary>
Motivation: 现有基于信息检索的缺陷定位方法忽视源代码的上下文和语义，而大型语言模型虽然能理解文本和代码但尚未很好地应用于缺陷定位，且可能资源密集

Method: IQLoc结合IR和LLM的优势，利用基于Transformer的模型理解程序语义，在缺陷定位过程中重新制定查询

Result: 在扩展的Bench4BL基准数据集上，IQLoc在MAP、MRR和HIT@K指标上显著优于四种基线技术，对有堆栈跟踪、包含代码元素和仅含自然语言描述的缺陷报告分别提高了91.67%、72.73%和65.38%的MAP

Conclusion: 通过将程序语义理解集成到信息检索中，IQLoc缓解了传统基于IR的缺陷定位方法的长期挑战

Abstract: Despite decades of research, software bug localization remains challenging
due to heterogeneous content and inherent ambiguities in bug reports. Existing
methods such as Information Retrieval (IR)-based approaches often attempt to
match source documents to bug reports, overlooking the context and semantics of
the source code. On the other hand, Large Language Models (LLM) (e.g.,
Transformer models) show promising results in understanding both texts and
code. However, they have not been yet adapted well to localize software bugs
against bug reports. They could be also data or resource-intensive. To bridge
this gap, we propose, IQLoc, a novel bug localization approach that capitalizes
on the strengths of both IR and LLM-based approaches. In particular, we
leverage the program semantics understanding of transformer-based models to
reason about the suspiciousness of code and reformulate queries during bug
localization using Information Retrieval. To evaluate IQLoc, we refine the
Bench4BL benchmark dataset and extend it by incorporating ~30% more recent bug
reports, resulting in a benchmark containing ~7.5K bug reports. We evaluated
IQLoc using three performance metrics and compare it against four baseline
techniques. Experimental results demonstrate its superiority, achieving up to
58.52% and 60.59% in MAP, 61.49% and 64.58% in MRR, and 69.88% and 100.90% in
HIT@K for the test bug reports with random and time-wise splits, respectively.
Moreover, IQLoc improves MAP by 91.67% for bug reports with stack traces,
72.73% for those that include code elements, and 65.38% for those containing
only descriptions in natural language. By integrating program semantic
understanding into Information Retrieval, IQLoc mitigates several longstanding
challenges of traditional IR-based approaches in bug localization.

</details>


### [30] [DynamiQ: Unlocking the Potential of Dynamic Task Allocation in Parallel Fuzzing](https://arxiv.org/abs/2510.04469)
*Wenqi Yan,Toby Murray,Benjamin Rubinstein,Van-Thuan Pham*

Main category: cs.SE

TL;DR: DynamiQ是一个基于AFLTeam优化的动态自适应并行模糊测试系统，利用程序调用图的结构信息定义任务，通过运行时反馈持续优化任务分配，显著减少冗余探索并提升大规模模糊测试效率。


<details>
  <summary>Details</summary>
Motivation: 现有并行模糊测试方法通常将单个种子视为任务，缺乏对程序结构的考虑，导致冗余探索和效率低下。DynamiQ旨在通过结构化的任务定义和动态分配机制解决这些问题。

Method: 基于LibAFL框架构建，利用程序调用图的结构信息定义任务，采用运行时反馈持续优化任务分配，并在任务分配和任务感知模糊测试中集成了多项实际优化。

Result: 在12个真实世界目标上经过25,000 CPU小时的评估，DynamiQ在代码覆盖率和漏洞发现方面均优于最先进的并行模糊测试工具，发现了9个先前未知的广泛使用且经过大量模糊测试的开源软件漏洞。

Conclusion: DynamiQ通过结构化的任务定义和动态分配机制，显著提升了并行模糊测试的效率和效果，证明了基于程序结构信息的方法在大规模模糊测试中的优势。

Abstract: We present DynamiQ, a full-fledged and optimized successor to AFLTeam that
supports dynamic and adaptive parallel fuzzing. Unlike most existing approaches
that treat individual seeds as tasks, DynamiQ leverages structural information
from the program's call graph to define tasks and continuously refines task
allocation using runtime feedback. This design significantly reduces redundant
exploration and enhances fuzzing efficiency at scale. Built on top of the
state-of-the-art LibAFL framework, DynamiQ incorporates several practical
optimizations in both task allocation and task-aware fuzzing. Evaluated on 12
real-world targets from OSS-Fuzz and FuzzBench over 25,000 CPU hours, DynamiQ
outperforms state-of-the-art parallel fuzzers in both code coverage and
vulnerability discovery, uncovering 9 previously unknown bugs in widely used
and extensively fuzzed open-source software.

</details>


### [31] [Detecting and Characterizing Low and No Functionality Packages in the NPM Ecosystem](https://arxiv.org/abs/2510.04495)
*Napasorn Tevarut,Brittany Reid,Yutaro Kashiwa,Pattara Leelaprute,Arnon Rungsawang,Bundit Manaskasemsak,Hajimu Iida*

Main category: cs.SE

TL;DR: 该论文研究了npm生态系统中的平凡包（功能简单的小模块）和数据包（不含可执行逻辑），开发了基于规则的静态分析方法来检测这些包，发现17.92%的包是平凡的，其漏洞水平与非平凡包相当，数据包虽然罕见但也存在风险。


<details>
  <summary>Details</summary>
Motivation: npm生态系统中的平凡包虽然功能简单，但仍可能带来安全风险，需要更精确的定义和检测方法来评估其普遍性和相关风险。

Method: 开发基于规则的静态分析方法来检测平凡包和数据包，并在2025年npm生态系统中评估其普遍性和相关风险。

Result: 分析显示17.92%的包是平凡的，其漏洞水平与非平凡包相当；数据包虽然罕见但也包含风险；检测工具达到94%的准确率（宏观F1分数0.87）。

Conclusion: 平凡包和数据包在依赖管理中值得更多关注，以减少潜在的技术债务和安全暴露。

Abstract: Trivial packages, small modules with low functionality, are common in the npm
ecosystem and can pose security risks despite their simplicity. This paper
refines existing definitions and introduce data-only packages that contain no
executable logic. A rule-based static analysis method is developed to detect
trivial and data-only packages and evaluate their prevalence and associated
risks in the 2025 npm ecosystem. The analysis shows that 17.92% of packages are
trivial, with vulnerability levels comparable to non-trivial ones, and
data-only packages, though rare, also contain risks. The proposed detection
tool achieves 94% accuracy (macro-F1 0.87), enabling effective large-scale
analysis to reduce security exposure. This findings suggest that trivial and
data-only packages warrant greater attention in dependency management to reduce
potential technical debt and security exposure.

</details>


### [32] [Spec2Control: Automating PLC/DCS Control-Logic Engineering from Natural Language Requirements with LLMs - A Multi-Plant Evaluation](https://arxiv.org/abs/2510.04519)
*Heiko Koziolek,Thilo Braun,Virendra Ashiwal,Sofia Linsbauer,Marthe Ahlgreen Hansen,Karoline Grotterud*

Main category: cs.SE

TL;DR: Spec2Control是一个高度自动化的LLM工作流，能够直接从自然语言用户需求生成图形化控制逻辑，在分布式控制系统编程中实现98.6%的正确控制策略连接，节省94-96%的人工劳动。


<details>
  <summary>Details</summary>
Motivation: 分布式控制系统（DCS）的软件编程过程仍然主要依赖手动操作且繁琐，大型设施的成本高达数百万美元。现有的LLM辅助工具仅限于文本表示，自动化程度有限，且未在大型数据集上进行真实测试。

Method: 引入Spec2Control，这是一个高度自动化的LLM工作流，能够直接从自然语言用户需求生成图形化控制逻辑。

Result: 使用包含10个控制叙述和65个复杂测试用例的开放数据集进行实验，结果表明Spec2Control能够成功识别控制策略，自主生成98.6%的正确控制策略连接，节省94-96%的人工劳动。

Conclusion: Spec2Control正在集成到商业ABB工程工具中，同时也提供开源版本供独立验证。

Abstract: Distributed control systems (DCS) manage the automation for many industrial
production processes (e.g., power plants, chemical refineries, steel mills).
Programming the software for such systems remains a largely manual and tedious
process, incurring costs of millions of dollars for extensive facilities. Large
language models (LLMs) have been found helpful in generating DCS control logic,
resulting in commercial copilot tools. Today, these tools are focused on
textual notations, they provide limited automation, and have not been tested on
large datasets with realistic test cases. We introduce Spec2Control, a highly
automated LLM workflow to generate graphical control logic directly from
natural language user requirements. Experiments using an open dataset with 10
control narratives and 65 complex test cases demonstrate that Spec2Control can
successfully identify control strategies, can generate 98.6% of correct control
strategy connections autonomously, and can save between 94-96% of human labor.
Spec2Control is being integrated into commercial ABB engineering tools, but is
also available as an open-source variant for independent validation.

</details>


### [33] [Advancing Digital Government: Integrating Open Source Software Enablement Indicators in Maturity Indexes](https://arxiv.org/abs/2510.04603)
*Johan Linåker,Sachiko Muto*

Main category: cs.SE

TL;DR: 本研究分析了16个数字成熟国家的开源软件政策，发现促进OSS重用的政策普遍存在，主要由中央公共部门组织管理，政策目标包括互操作性、数字主权等，并通过多级政府的开源项目办公室提供实施支持。


<details>
  <summary>Details</summary>
Motivation: 开源软件是重要的公共产品，对GDP和国家科技增长有显著影响，但政府采用OSS的系统性测量仍然有限。本研究旨在为数字政府成熟度指数贡献OSS相关指标。

Method: 采用定性方法，结合政策文件的案头研究和政府代表的半结构化访谈，生成详细的国家报告并进行交叉分析。

Result: 发现促进OSS重用的政策普遍存在，政策目标包括互操作性、数字主权、透明度和成本效率，安全既被视为风险也被视为优势。实施由多级政府的开源项目办公室支持。

Conclusion: OSS是公共部门数字化转型的战略推动者，需要清晰的政策框架和机构支持。国际数字成熟度框架应扩展OSS指标以更好地指导和评估政府采用和影响。

Abstract: Context: Open Source Software (OSS) is a vital public good, included across
most of modern software stacks, significantly impacting GDP and national tech
growth, while supporting interoperability, sovereignty, and transparency.
However, systematic measurement of governmental OSS adoption remain limited.
  Research Aim: This study contributes to digital government maturity indexes
by analyzing policies and support actions leveraging OSS for software reuse and
collaborative development across 16 digitally mature countries, and proposing
potential indicators for said indexes. It examines OSS policy formation, stated
goals, key actors, and support mechanisms.
  Methodology: A qualitative approach is used combining desk research of policy
documents with semi-structured interviews of government representatives,
producing detailed country reports. These are cross-analyzed, focusing on OSS
policy promotion, rationale, and implementation support.
  Results: Policies facilitating OSS reuse are widespread, targeting both
inbound acquisition and outbound sharing, and are predominantly governed by
central public sector organizations. Policy goals include interoperability,
digital sovereignty, transparency, and cost efficiency, with security framed
both as a risk and strength. Implementation is supported by diverse Open Source
Program Offices (OSPOs) at multiple government levels, which foster capacity
building, resource pooling, and sustainable project governance. Indicators are
synthesized and proposed across 14 areas covering policy incentives and design,
and implementation and support.
  Conclusions: OSS is a strategic enabler for public sector digital
transformation. Clear policy frameworks, coupled with institutional support
such as OSPOs, are essential. International digital maturity frameworks should
expand OSS indicators to better guide and assess government adoption and
impact.

</details>


### [34] [Exploring the Power of Diffusion Large Language Models for Software Engineering: An Empirical Investigation](https://arxiv.org/abs/2510.04605)
*Jingyao Zhang,Tianlin Li,Xiaoyu Zhang,Qiang Hu,Bin Shi*

Main category: cs.SE

TL;DR: 扩散大语言模型在软件工程任务中全面优于自回归大语言模型，在52,937个任务上平均准确率提升30%，跨文件修复任务提升113%，同时保持更高效率和更低延迟。


<details>
  <summary>Details</summary>
Motivation: 自回归大语言模型在处理代码结构信息方面存在局限，且推理延迟较高，扩散大语言模型提供了具有全局双向编码和解耦生成步骤的替代方案。

Method: 对扩散大语言模型在软件开发生命周期中的表现进行全面评估，包括代码生成、缺陷检测和程序修复任务。

Result: 在52,937个任务的大规模基准测试中，7B参数的扩散大语言模型优于自回归大语言模型，平均准确率提升30%，跨文件修复任务提升113%。

Conclusion: 扩散大语言模型成为软件工程任务的优越范式，具有更高的效率和更低的延迟。

Abstract: Autoregressive Large Language Models (AR-LLMs) are widely used in software
engineering (SE) but face limitations in processing code structure information
and suffer from high inference latency. Diffusion LLMs (DLLMs) offer a
promising alternative with global bidirectional encoding and decoupled
generation steps. This work presents the first comprehensive evaluation of
DLLMs across the software development lifecycle, including code generation,
defect detection, and program repair. On a large-scale benchmark of 52,937
tasks, 7Bparameter DLLMs outperform AR-LLMs with a 30% average accuracy
improvement achieving a 113% gain on cross-file repair, while maintaining
superior efficiency and reduced latency. Our results establish DLLMs as a
superior paradigm for SE tasks.

</details>


### [35] [A survey on the impact of emotions on the productivity among software developers](https://arxiv.org/abs/2510.04611)
*Pawel Weichbroth,Maciej Lotysz,Michal Wrobel*

Main category: cs.SE

TL;DR: 软件开发者情绪状态对感知生产力有显著正向影响（beta=0.893, p<0.001），情绪管理对提升开发效率至关重要。


<details>
  <summary>Details</summary>
Motivation: 软件开发中的时间压力等因素常导致开发者情绪状态下降，但情绪是否影响感知生产力尚不明确，需要研究两者关系的强度和方向。

Method: 采用两阶段方法：首先通过9位专家验证测量模型，然后对88名软件开发者进行调查，使用偏最小二乘法进行数据分析。

Result: 路径分析明确证实假设，软件开发者情绪状态对感知生产力具有强烈、正向且显著的影响（beta=0.893, p<0.001）。

Conclusion: 管理和改善开发者情绪健康对提升软件开发环境中的生产力至关重要，减少倦怠、压力等负面因素的干预措施能显著影响绩效结果。

Abstract: The time pressure associated with software development, among other factors,
often leads to a diminished emotional state among developers. However, whether
emotions affect perceived productivity remains an open question. This study
aims to determine the strength and direction of the relationship between
emotional state and perceived productivity among software developers. We
employed a two-stage approach. First, a survey was conducted with a pool of
nine experts to validate the measurement model. Second, a survey was
administered to a pool of 88 software developers to empirically test the
formulated hypothesis by using Partial Least Squares, as the data analysis
method. The results of the path analysis clearly confirm the formulated
hypothesis, showing that the emotional state of a software developer has a
strong positive, and significant impact (beta = 0.893, p < 0.001) on perceived
productivity among software developers. The findings highlight the importance
of managing and improving developers emotional well-being to enhance
productivity in software development environments. Additionally, interventions
aimed at reducing burnout, stress, and other negative factors could have a
considerable impact on their performance outcomes.

</details>


### [36] [Evolaris: A Roadmap to Self-Evolving Software Intelligence Management](https://arxiv.org/abs/2510.04689)
*Chengwei Liu,Wenbo Guo,Yuxin Zhang,Limin Wang,Sen Chen,Lei Bu,Yang Liu*

Main category: cs.SE

TL;DR: Evolaris是一个基于多智能体框架的自进化软件情报系统，能够从分散的非正式渠道收集、分析威胁信息，实现持续学习和适应性威胁分析。


<details>
  <summary>Details</summary>
Motivation: 软件威胁环境日益动态化和分布式，关键威胁信息越来越多地通过非正式渠道出现，传统渠道无法及时捕获这些情报，需要新的解决方案来应对数据源分散和技术挑战。

Method: 采用多智能体框架，各智能体独立运行但通过共享上下文协调，执行信息发现、推理、缺口补全、验证和风险检测等任务，支持全栈工作流程。

Result: 系统能够从新输入中学习，精化内部知识，适应新兴威胁模式，持续提高软件威胁分析的精确性、及时性和可扩展性。

Conclusion: Evolaris为主动安全决策提供了可持续基础，并加强了安全威胁理解的更广泛生态系统。

Abstract: In recent years, the landscape of software threats has become significantly
more dynamic and distributed. Security vulnerabilities are no longer discovered
and shared only through formal channels such as public vulnerability databases
or vendor advisories. Increasingly, criti- cal threat information emerges
informally through blogs, social media, developer forums, open source
repositories, and even underground com- munities. To this end, capturing such
intelligence in a timely manner is essential for maintaining situational
awareness and enabling prompt security responses. However, this remains a
complex challenge due to the fragmented nature of data sources and the
technical difficulty of collecting, parsing, mapping, and validating
information at scale. To ad- dress this, we propose Evolaris, a self-evolving
software intelligence sys- tem built on a multi-agent framework. Evolaris is
designed to support a full-stack workflow, where agents operate independently
but coordinate through shared context to perform tasks such as information
discovery, reasoning, gap completion, validation, and risk detection. This
archi- tecture enables the platform to learn from new inputs, refine its
internal knowledge, and adapt to emerging threat patterns over time, which
could continuously improve the precision, timeliness, and scalability of
software threat analysis, and offers a sustainable foundation for proactive
secu- rity decision-making and strengthens the broader ecosystem of security
threat understanding.

</details>


### [37] [An Empirical Study of SOTA RCA Models: From Oversimplified Benchmarks to Realistic Failures](https://arxiv.org/abs/2510.04711)
*Aoyang Fang,Songhan Zhang,Yifan Yang,Haotong Wu,Junjielong Xu,Xuyang Wang,Rui Wang,Manyi Wang,Qisheng Lu,Pinjia He*

Main category: cs.SE

TL;DR: 现有云原生微服务RCA基准测试过于简化，导致性能高估。作者开发了更现实的基准测试框架，重新评估11个SOTA模型发现准确率很低（平均0.21，最佳0.37），并识别了三个常见失败模式。


<details>
  <summary>Details</summary>
Motivation: 云原生微服务架构的复杂性使得根因分析(RCA)既关键又困难，现有基准测试过于简化，无法反映真实条件，导致对数据驱动RCA模型的性能高估。

Method: 系统分析流行RCA基准测试的局限性，开发自动化框架生成更现实的基准测试，包含1,430个验证的故障案例，覆盖25种故障类型，具有分层真实标签和验证的SLI影响。

Result: 在更现实的基准测试上重新评估11个SOTA模型，Top@1准确率很低（平均0.21，最佳0.37），执行时间显著更长。

Conclusion: 现有RCA模型在真实条件下表现不佳，存在可扩展性问题、可观测性盲点和建模瓶颈三个常见失败模式，需要更现实的基准测试来推动模型发展。

Abstract: While cloud-native microservice architectures have transformed software
development, their complexity makes Root Cause Analysis (RCA) both crucial and
challenging. Although many data-driven RCA models have been proposed, we find
that existing benchmarks are often oversimplified and fail to capture
real-world conditions. Our preliminary study shows that simple rule-based
methods can match or even outperform state-of-the-art (SOTA) models on four
widely used benchmarks, suggesting performance overestimation due to benchmark
simplicity. To address this, we systematically analyze popular RCA benchmarks
and identify key limitations in fault injection, call graph design, and
telemetry patterns. Based on these insights, we develop an automated framework
to generate more realistic benchmarks, yielding a dataset of 1,430 validated
failure cases from 9,152 injections, covering 25 fault types under dynamic
workloads with hierarchical ground-truth labels and verified SLI impact.
Re-evaluation of 11 SOTA models on this dataset shows low Top@1 accuracy
(average 0.21, best 0.37) and significantly longer execution times. Our
analysis highlights three common failure patterns: scalability issues,
observability blind spots, and modeling bottlenecks.

</details>


### [38] [Agile Software Effort Estimation using Regression Techniques](https://arxiv.org/abs/2510.04760)
*Sisay Deresa Sima,Ayalew Belay Habtie*

Main category: cs.SE

TL;DR: 该研究开发了一个基于故事点的敏捷工作量估算模型，使用LASSO和弹性网络回归技术，在21个软件项目数据集上验证，LASSO回归表现最佳。


<details>
  <summary>Details</summary>
Motivation: 软件开发工作量估算是软件开发过程中最关键的一个方面，整个项目的成功或失败取决于估算的准确性。研究人员仍在进行敏捷工作量估算的研究。

Method: 使用LASSO和弹性网络回归技术开发故事点敏捷工作量估算模型，在21个软件项目数据集上应用，采用默认参数和调优网格搜索结合5折交叉验证来增强模型。

Result: LASSO回归取得了更好的预测性能：PRED(8%)和PRED(25%)结果均为100.0，MMRE为0.0491，MMER为0.0551，MdMRE为0.0593，MdMER为0.063，MSE为0.0007。

Conclusion: LASSO回归在敏捷工作量估算中表现出优越的预测性能，结果优于其他相关文献中的方法。

Abstract: Software development effort estimation is one of the most critical aspect in
software development process, as the success or failure of the entire project
depends on the accuracy of estimations. Researchers are still conducting
studies on agile effort estimation. The aim of this research is to develop a
story point based agile effort estimation model using LASSO and Elastic Net
regression techniques. The experimental work is applied to the agile story
point approach using 21 software projects collected from six firms. The two
algorithms are trained using their default parameters and tuned grid search
with 5-fold cross-validation to get an enhanced model. The experiment result
shows LASSO regression achieved better predictive performance PRED (8%) and
PRED (25%) results of 100.0, MMRE of 0.0491, MMER of 0.0551, MdMRE of 0.0593,
MdMER of 0.063, and MSE of 0.0007. The results are also compared with other
related literature.

</details>


### [39] [GUISpector: An MLLM Agent Framework for Automated Verification of Natural Language Requirements in GUI Prototypes](https://arxiv.org/abs/2510.04791)
*Kristian Kolthoff,Felix Kretzer,Simone Paolo Ponzetto,Alexander Maedche,Christian Bartelt*

Main category: cs.SE

TL;DR: GUISpector是一个基于多模态大语言模型的GUI原型需求验证框架，能够自动验证自然语言需求并提供可操作的反馈。


<details>
  <summary>Details</summary>
Motivation: 现有GUI测试方法在处理现代界面复杂性方面存在不足，缺乏有效的反馈机制和与自动化开发代理的集成。

Method: 使用多模态大语言模型代理解释和操作化自然语言需求，自主规划和执行GUI验证轨迹，并提取详细反馈。

Result: 在150个基于900个验收标准的需求集上评估，有效检测需求满足和违规情况。

Conclusion: GUISpector展示了将可操作反馈无缝集成到自动化LLM驱动开发工作流程中的潜力。

Abstract: GUIs are foundational to interactive systems and play a pivotal role in early
requirements elicitation through prototyping. Ensuring that GUI implementations
fulfill NL requirements is essential for robust software engineering,
especially as LLM-driven programming agents become increasingly integrated into
development workflows. Existing GUI testing approaches, whether traditional or
LLM-driven, often fall short in handling the complexity of modern interfaces,
and typically lack actionable feedback and effective integration with automated
development agents. In this paper, we introduce GUISpector, a novel framework
that leverages a multi-modal (M)LLM-based agent for the automated verification
of NL requirements in GUI prototypes. First, GUISpector adapts a MLLM agent to
interpret and operationalize NL requirements, enabling to autonomously plan and
execute verification trajectories across GUI applications. Second, GUISpector
systematically extracts detailed NL feedback from the agent's verification
process, providing developers with actionable insights that can be used to
iteratively refine the GUI artifact or directly inform LLM-based code
generation in a closed feedback loop. Third, we present an integrated tool that
unifies these capabilities, offering practitioners an accessible interface for
supervising verification runs, inspecting agent rationales and managing the
end-to-end requirements verification process. We evaluated GUISpector on a
comprehensive set of 150 requirements based on 900 acceptance criteria
annotations across diverse GUI applications, demonstrating effective detection
of requirement satisfaction and violations and highlighting its potential for
seamless integration of actionable feedback into automated LLM-driven
development workflows. The video presentation of GUISpector is available at:
https://youtu.be/JByYF6BNQeE, showcasing its main capabilities.

</details>


### [40] [RevMine: An LLM-Assisted Tool for Code Review Mining and Analysis Across Git Platforms](https://arxiv.org/abs/2510.04796)
*Samah Kansab,Francis Bordeleau,Ali Tizghadam*

Main category: cs.SE

TL;DR: RevMine是一个基于大语言模型的概念工具，旨在简化代码审查挖掘流程，降低实证研究的门槛。


<details>
  <summary>Details</summary>
Motivation: 当前代码审查数据的收集和分析过程耗时且技术密集，大多数研究者需要编写临时脚本来处理GitHub等平台的数据，存在效率问题。

Method: 使用大语言模型指导用户完成认证、端点发现和自然语言驱动的数据收集，支持基于用户定义过滤器或LLM推断模式的定量和定性分析。

Result: 开发了RevMine工具架构，显著减少了手动脚本编写的需求，使代码审查挖掘更加高效。

Conclusion: RevMine通过降低技术门槛，有望使代码审查挖掘民主化，支持更广泛的实证软件工程研究。

Abstract: Empirical research on code review processes is increasingly central to
understanding software quality and collaboration. However, collecting and
analyzing review data remains a time-consuming and technically intensive task.
Most researchers follow similar workflows - writing ad hoc scripts to extract,
filter, and analyze review data from platforms like GitHub and GitLab. This
paper introduces RevMine, a conceptual tool that streamlines the entire code
review mining pipeline using large language models (LLMs). RevMine guides users
through authentication, endpoint discovery, and natural language-driven data
collection, significantly reducing the need for manual scripting. After
retrieving review data, it supports both quantitative and qualitative analysis
based on user-defined filters or LLM-inferred patterns. This poster outlines
the tool's architecture, use cases, and research potential. By lowering the
barrier to entry, RevMine aims to democratize code review mining and enable a
broader range of empirical software engineering studies.

</details>


### [41] [InsightQL: Advancing Human-Assisted Fuzzing with a Unified Code Database and Parameterized Query Interface](https://arxiv.org/abs/2510.04835)
*Wentao Gao,Renata Borovica-Gajic,Sang Kil Cha,Tian Qiu,Van-Thuan Pham*

Main category: cs.SE

TL;DR: InsightQL是一个帮助开发者分析模糊测试阻塞问题的人类辅助框架，通过统一数据库和参数化查询接口，有效解决覆盖率停滞问题。


<details>
  <summary>Details</summary>
Motivation: 现有模糊测试技术在面对覆盖率停滞问题时存在局限性，需要人工分析来指导解决，但这个过程劳动密集且效率低下。

Method: 开发了InsightQL框架，包含统一数据库和直观的参数化查询接口，帮助开发者系统性地提取洞察并高效解除模糊测试阻塞。

Result: 在FuzzBench基准测试的14个流行真实世界库上进行实验，成功解除了多个模糊测试阻塞，代码覆盖率显著提升（最高达13.90%）。

Conclusion: InsightQL框架有效解决了模糊测试中的覆盖率停滞问题，通过人类辅助的方式显著提升了模糊测试的效果和效率。

Abstract: Fuzzing is a highly effective automated testing method for uncovering
software vulnerabilities. Despite advances in fuzzing techniques, such as
coverage-guided greybox fuzzing, many fuzzers struggle with coverage plateaus
caused by fuzz blockers, limiting their ability to find deeper vulnerabilities.
Human expertise can address these challenges, but analyzing fuzzing results to
guide this support remains labor-intensive. To tackle this, we introduce
InsightQL, the first human-assisting framework for fuzz blocker analysis.
Powered by a unified database and an intuitive parameterized query interface,
InsightQL aids developers in systematically extracting insights and efficiently
unblocking fuzz blockers. Our experiments on 14 popular real-world libraries
from the FuzzBench benchmark demonstrate the effectiveness of InsightQL,
leading to the unblocking of many fuzz blockers and considerable improvements
in code coverage (up to 13.90%).

</details>


### [42] [FreshBrew: A Benchmark for Evaluating AI Agents on Java Code Migration](https://arxiv.org/abs/2510.04852)
*Victor May,Diganta Misra,Yanqi Luo,Anjali Sridhar,Justine Gehring,Silvio Soares Ribeiro Junior*

Main category: cs.SE

TL;DR: FreshBrew是一个用于评估AI代理在项目级Java迁移任务中的基准测试，重点关注语义保持和避免奖励黑客行为，使用高测试覆盖率的项目进行可靠评估。


<details>
  <summary>Details</summary>
Motivation: 传统代码迁移依赖基于规则的系统，而基于大语言模型的AI代理框架提供了有前景的替代方案，但其有效性尚未得到系统评估。

Method: 引入FreshBrew基准测试，对多个最先进的大语言模型进行评估，并与基于规则的工具进行性能比较，涵盖228个代码库。

Result: 表现最佳的Gemini 2.5 Flash模型能够成功将52.3%的项目迁移到JDK 17，揭示了当前AI代理方法的关键优势和局限性。

Conclusion: FreshBrep基准测试为评估可信赖的代码迁移系统奠定了基础，旨在促进严格、可复现的评估，并推动AI驱动的代码库现代化进展。

Abstract: AI coding assistants are rapidly becoming integral to modern software
development. A key challenge in this space is the continual need to migrate and
modernize codebases in response to evolving software ecosystems. Traditionally,
such migrations have relied on rule-based systems and human intervention. With
the advent of powerful large language models (LLMs), AI-driven agentic
frameworks offer a promising alternative-but their effectiveness has not been
systematically evaluated. In this paper, we introduce FreshBrew, a novel
benchmark for evaluating AI agents on project-level Java migrations, with a
specific focus on measuring an agent's ability to preserve program semantics
and avoid reward hacking, which we argue requires projects with high test
coverage for a rigorous and reliable evaluation. We benchmark several
state-of-the-art LLMs, and compare their performance against established
rule-based tools. Our evaluation of AI agents on this benchmark of 228
repositories shows that the top-performing model, Gemini 2.5 Flash, can
successfully migrate 52.3 percent of projects to JDK 17. Our empirical analysis
reveals novel insights into the critical strengths and limitations of current
agentic approaches, offering actionable insights into their real-world
applicability. Our empirical study reveals failure modes of current AI agents
in realistic Java modernization tasks, providing a foundation for evaluating
trustworthy code-migration systems. By releasing FreshBrew, we aim to
facilitate rigorous, reproducible evaluation and catalyze progress in AI-driven
codebase modernization.

</details>


### [43] [Retrieval-Augmented Code Generation: A Survey with Focus on Repository-Level Approaches](https://arxiv.org/abs/2510.04905)
*Yicheng Tao,Yao Qin,Yepang Liu*

Main category: cs.SE

TL;DR: 这篇论文系统综述了检索增强代码生成（RACG）领域，特别关注仓库级代码生成（RLCG）的挑战和解决方案。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在代码生成方面取得进展，现实软件开发需要跨整个仓库进行推理，这引发了仓库级代码生成的挑战，包括长程依赖、全局语义一致性和多文件代码生成等问题。

Method: 采用检索增强生成（RAG）范式，将外部检索机制与大语言模型结合，提升上下文感知和可扩展性。论文从生成策略、检索模态、模型架构、训练范式和评估协议等多个维度对现有工作进行分类。

Result: 建立了统一的分析框架，总结了广泛使用的数据集和基准，分析了当前局限性，并概述了未来研究的关键挑战和机遇。

Conclusion: 该综述旨在为理解这一快速发展领域建立统一框架，并推动AI驱动的软件工程持续进步。

Abstract: Recent advancements in large language models (LLMs) have substantially
improved automated code generation. While function-level and file-level
generation have achieved promising results, real-world software development
typically requires reasoning across entire repositories. This gives rise to the
challenging task of Repository-Level Code Generation (RLCG), where models must
capture long-range dependencies, ensure global semantic consistency, and
generate coherent code spanning multiple files or modules. To address these
challenges, Retrieval-Augmented Generation (RAG) has emerged as a powerful
paradigm that integrates external retrieval mechanisms with LLMs, enhancing
context-awareness and scalability. In this survey, we provide a comprehensive
review of research on Retrieval-Augmented Code Generation (RACG), with an
emphasis on repository-level approaches. We categorize existing work along
several dimensions, including generation strategies, retrieval modalities,
model architectures, training paradigms, and evaluation protocols. Furthermore,
we summarize widely used datasets and benchmarks, analyze current limitations,
and outline key challenges and opportunities for future research. Our goal is
to establish a unified analytical framework for understanding this rapidly
evolving field and to inspire continued progress in AI-powered software
engineering.

</details>


### [44] [Why Software Signing (Still) Matters: Trust Boundaries in the Software Supply Chain](https://arxiv.org/abs/2510.04964)
*Kelechi G. Kalu,James C. Davis*

Main category: cs.SE

TL;DR: 软件签名在集中式注册表时代仍然必要，因为注册表安全控制无法跨越镜像、代理、重新托管和隔离传输等分发边界提供足够的完整性、来源和问责保证。


<details>
  <summary>Details</summary>
Motivation: 在PyPI、npm、Maven Central和Hugging Face等集中式注册表普及的背景下，探讨注册表安全控制是否足以替代端到端软件签名。

Method: 综合历史实践，构建现代分发模式的信任模型，识别在哪些情况下签名是必要的，以将信任扩展到注册表控制之外。

Result: 签名作为基础防御层，即使注册表安全也能增强软件供应链保障，其核心保证（来源、完整性、问责）无法自动跨越不同软件分发边界。

Conclusion: 软件签名仍然是必要的，因为它提供了注册表安全控制无法覆盖的分发边界上的保证，应被视为软件供应链保障的基础防御层。

Abstract: Software signing provides a formal mechanism for provenance by ensuring
artifact integrity and verifying producer identity. It also imposes tooling and
operational costs to implement in practice. In an era of centralized registries
such as PyPI, npm, Maven Central, and Hugging Face, it is reasonable to ask
whether hardening registry security controls obviates the need for end-to-end
artifact signing. In this work, we posit that the core guarantees of signing,
provenance, integrity, and accountability are not automatically carried across
different software distribution boundaries. These boundaries include mirrors,
corporate proxies, re-hosting, and air-gapped transfers, where registry
security controls alone cannot provide sufficient assurance. We synthesize
historical practice and present a trust model for modern distribution modes to
identify when signing is necessary to extend trust beyond registry control.
Treating signing as a baseline layer of defense strengthens software supply
chain assurance even when registries are secure.

</details>


### [45] [Quantum Computing as a Service - a Software Engineering Perspective](https://arxiv.org/abs/2510.04982)
*Aakash Ahmad,Muhammad Waseem,Bakheet Aljedaani,Mahdi Fahmideh,Peng Liang,Feras Awaysheh*

Main category: cs.SE

TL;DR: 本文提出了一种基于软件工程视角的量子计算即服务(QCaaS)参考架构，通过系统映射研究和架构开发方法，识别了量子服务开发生命周期的4个阶段，并集成为支持QCaaS的分层参考架构。


<details>
  <summary>Details</summary>
Motivation: 量子计算作为颠覆性技术正在兴起，但大多数个人和组织无法拥有量子计算机。量子计算即服务(QCaaS)被视为符合服务导向理念的解决方案，能够以效用计算方式提供量子计算资源。

Method: 采用两阶段研究方法：(a)系统映射研究，通过多步骤选择和定性评估筛选了41篇同行评审研究；(b)基于架构的开发，将量子服务开发生命周期阶段集成到参考架构中。

Result: 识别了包含4个阶段的量子服务开发生命周期，以及量子重要需求(QSRs)、各种建模符号、模式目录、编程语言和部署平台，这些可以集成到分层参考架构中以支持QCaaS工程化。

Conclusion: 研究提出了一个过程中心和架构驱动的方法，为量子服务导向提供了软件工程视角，通过参考架构支持量子服务的概念化、建模、组装和部署。

Abstract: Quantum systems have started to emerge as a disruptive technology and
enabling platforms - exploiting the principles of quantum mechanics via
programmable quantum bits (QuBits) - to achieve quantum supremacy in computing.
Academic research, industrial projects (e.g., Amazon Braket, IBM Qiskit), and
consortiums like 'Quantum Flagship' are striving to develop practically capable
and commercially viable quantum computing (QC) systems and technologies.
Quantum Computing as a Service (QCaaS) is viewed as a solution attuned to the
philosophy of service-orientation that can offer QC resources and platforms, as
utility computing, to individuals and organisations who do not own quantum
computers. This research investigates a process-centric and architecture-driven
approach to offer a software engineering perspective on enabling QCaaS - a.k.a
quantum service-orientation. We employed a two-phase research method comprising
(a) a systematic mapping study and (b) an architecture-based development, first
to identify the phases of the quantum service development life cycle and
subsequently to integrate these phases into a reference architecture that
supports QCaaS. The SMS process retrieved a collection of potentially relevant
research literature and based on a multi-step selection and qualitative
assessment, we selected 41 peer-reviewed studies to answer three RQs. The RQs
investigate (i) demographic details in terms of frequency, types, and trends of
research, (ii) phases of quantum service development lifecycle to derive a
reference architecture for conception, modeling, assembly, and deployment of
services, and (iii) The results identify a 4-phased development lifecycle along
with quantum significant requirements (QSRs), various modeling notations,
catalogue of patterns, programming languages, and deployment platforms that can
be integrated in a layered reference architecture to engineer QCaaS.

</details>


### [46] [AutoEmpirical: LLM-Based Automated Research for Empirical Software Fault Analysis](https://arxiv.org/abs/2510.04997)
*Jiongchi Yu,Weipeng Jiang,Xiaoyu Zhang,Qiang Hu,Xiaofei Xie,Chao Shen*

Main category: cs.SE

TL;DR: 本文探索使用大语言模型(LLMs)进行软件故障分析，将传统多步骤专家驱动流程分解为三个关键阶段，在3,829个软件故障上验证了LLMs能显著提高效率。


<details>
  <summary>Details</summary>
Motivation: 传统软件故障分析涉及收集、筛选和人工调查等多个专家驱动步骤，劳动密集且耗时，阻碍了复杂关键软件系统的大规模故障研究和迭代实证研究。

Method: 将实证软件故障研究分解为三个关键阶段：研究目标定义、数据准备和故障分析，并应用大语言模型对开源软件进行故障分析评估。

Result: 在3,829个高质量实证研究的软件故障上评估，LLMs能大幅提高故障分析效率，平均处理时间约2小时，而传统人工方法通常需要数周。

Conclusion: LLMs在推进实证故障研究方面具有潜力，但要实现完全自动化的端到端软件故障分析，仍需解决一些开放挑战。

Abstract: Understanding software faults is essential for empirical research in software
development and maintenance. However, traditional fault analysis, while
valuable, typically involves multiple expert-driven steps such as collecting
potential faults, filtering, and manual investigation. These processes are both
labor-intensive and time-consuming, creating bottlenecks that hinder
large-scale fault studies in complex yet critical software systems and slow the
pace of iterative empirical research.
  In this paper, we decompose the process of empirical software fault study
into three key phases: (1) research objective definition, (2) data preparation,
and (3) fault analysis, and we conduct an initial exploration study of applying
Large Language Models (LLMs) for fault analysis of open-source software.
Specifically, we perform the evaluation on 3,829 software faults drawn from a
high-quality empirical study. Our results show that LLMs can substantially
improve efficiency in fault analysis, with an average processing time of about
two hours, compared to the weeks of manual effort typically required. We
conclude by outlining a detailed research plan that highlights both the
potential of LLMs for advancing empirical fault studies and the open challenges
that required be addressed to achieve fully automated, end-to-end software
fault analysis.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [47] [SVDefense: Effective Defense against Gradient Inversion Attacks via Singular Value Decomposition](https://arxiv.org/abs/2510.03319)
*Chenxiang Luo,David K. Y. Yau,Qun Song*

Main category: cs.CR

TL;DR: SVDefense是一个针对联邦学习中梯度反转攻击的防御框架，通过截断奇异值分解来混淆梯度更新，在保护隐私的同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习防御方法要么计算开销过大不适用于嵌入式平台，要么无法同时实现隐私保护和模型效用，且容易被自适应攻击者绕过。

Method: 使用截断奇异值分解来混淆梯度更新，包含三个关键创新：自适应能量阈值、通道加权逼近和分层加权聚合。

Result: 在图像分类、人类活动识别和关键词识别等多个应用中，SVDefense在提供强大隐私保护的同时对模型精度影响最小，且适用于资源受限的嵌入式平台。

Conclusion: SVDefense是一个实用且有效的防御框架，能够抵御梯度反转攻击，在隐私保护和模型性能之间取得良好平衡。

Abstract: Federated learning (FL) enables collaborative model training without sharing
raw data but is vulnerable to gradient inversion attacks (GIAs), where
adversaries reconstruct private data from shared gradients. Existing defenses
either incur impractical computational overhead for embedded platforms or fail
to achieve privacy protection and good model utility at the same time.
Moreover, many defenses can be easily bypassed by adaptive adversaries who have
obtained the defense details. To address these limitations, we propose
SVDefense, a novel defense framework against GIAs that leverages the truncated
Singular Value Decomposition (SVD) to obfuscate gradient updates. SVDefense
introduces three key innovations, a Self-Adaptive Energy Threshold that adapts
to client vulnerability, a Channel-Wise Weighted Approximation that selectively
preserves essential gradient information for effective model training while
enhancing privacy protection, and a Layer-Wise Weighted Aggregation for
effective model aggregation under class imbalance. Our extensive evaluation
shows that SVDefense outperforms existing defenses across multiple
applications, including image classification, human activity recognition, and
keyword spotting, by offering robust privacy protection with minimal impact on
model accuracy. Furthermore, SVDefense is practical for deployment on various
resource-constrained embedded platforms. We will make our code publicly
available upon paper acceptance.

</details>


### [48] [Attack logics, not outputs: Towards efficient robustification of deep neural networks by falsifying concept-based properties](https://arxiv.org/abs/2510.03320)
*Raik Dankworth,Gesina Schwalbe*

Main category: cs.CR

TL;DR: 提出一种基于概念属性的对抗攻击方法，用于验证深度神经网络在计算机视觉任务中的逻辑一致性，相比传统类别翻转攻击具有更小的搜索空间和更好的鲁棒性目标对齐。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络容易受到对抗攻击，现有方法只能验证最终输出类别的翻转，无法检验涉及多个可解释概念的一般逻辑行为。

Method: 利用可解释人工智能技术，在已训练神经网络上实现基于概念属性的约束验证，搜索违反逻辑约束的对抗样本。

Result: 理论上证明基于概念属性的攻击相比简单类别翻转具有更小的搜索空间，且更符合直观的鲁棒性目标。

Conclusion: 该方法有潜力同时高效提升神经网络的逻辑合规性和鲁棒性，是值得进一步研究的工作方向。

Abstract: Deep neural networks (NNs) for computer vision are vulnerable to adversarial
attacks, i.e., miniscule malicious changes to inputs may induce unintuitive
outputs. One key approach to verify and mitigate such robustness issues is to
falsify expected output behavior. This allows, e.g., to locally proof security,
or to (re)train NNs on obtained adversarial input examples. Due to the
black-box nature of NNs, current attacks only falsify a class of the final
output, such as flipping from $\texttt{stop_sign}$ to $\neg\texttt{stop_sign}$.
In this short position paper we generalize this to search for generally
illogical behavior, as considered in NN verification: falsify constraints
(concept-based properties) involving further human-interpretable concepts, like
$\texttt{red}\wedge\texttt{octogonal}\rightarrow\texttt{stop_sign}$. For this,
an easy implementation of concept-based properties on already trained NNs is
proposed using techniques from explainable artificial intelligence. Further, we
sketch the theoretical proof that attacks on concept-based properties are
expected to have a reduced search space compared to simple class falsification,
whilst arguably be more aligned with intuitive robustness targets. As an
outlook to this work in progress we hypothesize that this approach has
potential to efficiently and simultaneously improve logical compliance and
robustness.

</details>


### [49] [Security Analysis and Threat Modeling of Research Management Applications [Extended Version]](https://arxiv.org/abs/2510.03407)
*Boniface M. Sindala,Ragib Hasan*

Main category: cs.CR

TL;DR: 分析研究管理应用程序（RMA）的安全性问题，以REDCap为例，评估其架构、数据流和安全特性，使用MITRE ATT&CK框架和STRIDE模型识别风险，并提出安全增强建议。


<details>
  <summary>Details</summary>
Motivation: 研究管理应用程序处理敏感数据，价值高易受安全威胁，需要分析其安全性以保护关键研究数据。

Method: 以REDCap为例，评估RMA的架构、数据流和安全特性，使用MITRE ATT&CK框架和STRIDE模型识别和评估潜在风险。

Result: 识别了RMA的安全漏洞和风险，评估了REDCap对常见攻击向量的防御能力。

Conclusion: 提出了增强RMA安全性的建议，确保关键研究数据得到保护而不影响可用性，为研究密集型环境提供更安全的信息管理框架。

Abstract: Research management applications (RMA) are widely used in clinical research
environments to collect, transmit, analyze, and store sensitive data. This data
is so valuable making RMAs susceptible to security threats. This analysis,
analyzes RMAs' security, focusing on Research Electronic Data Capture (REDCap)
as an example. We explore the strengths and vulnerabilities within RMAs by
evaluating the architecture, data flow, and security features. We identify and
assess potential risks using the MITRE ATT\&CK framework and STRIDE model. We
assess REDCap's defenses against common attack vectors focusing on security to
provide confidentiality, integrity, availability, non-repudiation, and
authentication. We conclude by proposing recommendations for enhancing the
security of RMAs, ensuring that critical research data remains protected
without compromising usability. This research aims to contribute towards a more
secure framework for managing sensitive information in research-intensive
environments.

</details>


### [50] [NEXUS: Network Exploration for eXploiting Unsafe Sequences in Multi-Turn LLM Jailbreaks](https://arxiv.org/abs/2510.03417)
*Javad Rafiei Asl,Sidhant Narula,Mohammad Ghasemigol,Eduardo Blanco,Daniel Takabi*

Main category: cs.CR

TL;DR: NEXUS是一个用于构建多轮越狱攻击的模块化框架，通过分层语义网络和反馈驱动的查询优化，显著提高了对大型语言模型的攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 现有方法在探索对抗空间方面不足，依赖手工启发式规则，缺乏系统性的查询优化，无法有效应对多轮越狱攻击。

Method: NEXUS包含三个组件：ThoughtNet将有害意图扩展为结构化语义网络；反馈驱动的模拟器通过攻击者-受害者-评判者LLM协作迭代优化查询链；网络遍历器自适应导航查询空间进行实时攻击。

Result: 在多个闭源和开源LLM上，NEXUS相比现有方法将攻击成功率提高了2.1%到19.4%。

Conclusion: NEXUS框架能够发现隐蔽且高成功率的对抗路径，为理解和防御多轮越狱攻击提供了有效工具。

Abstract: Large Language Models (LLMs) have revolutionized natural language processing
but remain vulnerable to jailbreak attacks, especially multi-turn jailbreaks
that distribute malicious intent across benign exchanges and bypass alignment
mechanisms. Existing approaches often explore the adversarial space poorly,
rely on hand-crafted heuristics, or lack systematic query refinement. We
present NEXUS (Network Exploration for eXploiting Unsafe Sequences), a modular
framework for constructing, refining, and executing optimized multi-turn
attacks. NEXUS comprises: (1) ThoughtNet, which hierarchically expands a
harmful intent into a structured semantic network of topics, entities, and
query chains; (2) a feedback-driven Simulator that iteratively refines and
prunes these chains through attacker-victim-judge LLM collaboration using
harmfulness and semantic-similarity benchmarks; and (3) a Network Traverser
that adaptively navigates the refined query space for real-time attacks. This
pipeline uncovers stealthy, high-success adversarial paths across LLMs. On
several closed-source and open-source LLMs, NEXUS increases attack success rate
by 2.1% to 19.4% over prior methods. Code: https://github.com/inspire-lab/NEXUS

</details>


### [51] [A Multi-Layer Electronic and Cyber Interference Model for AI-Driven Cruise Missiles: The Case of Khuzestan Province](https://arxiv.org/abs/2510.03542)
*Pouriya Alimoradi,Ali Barati,Hamid Barati*

Main category: cs.CR

TL;DR: 提出多层干扰模型来对抗AI驱动的巡航导弹，通过电子战、网络攻击和欺骗策略显著降低导弹性能，实验显示目标偏差增加3300%，命中率下降66%。


<details>
  <summary>Details</summary>
Motivation: AI驱动的巡航导弹具有高度自主性、适应性和精确性，对战略基础设施构成严重威胁，特别是在复杂地理气候条件下。

Method: 采用多层干扰模型，结合电子战、网络攻击和欺骗策略，并使用深度强化学习的防御协调器实时选择最优战术配置。

Result: 400次模拟实验显示，多层干扰使导弹平均偏差从0.25增加到8.65，目标获取成功率从92.7%降至31.5%，资源消耗增加约25%。

Conclusion: 虽然多层干扰策略资源消耗较高，但导弹精度和可靠性的显著下降证明了这种更密集部署干扰功率、网络资源和诱饵措施的合理性。

Abstract: The rapid advancement of Artificial Intelligence has enabled the development
of cruise missiles endowed with high levels of autonomy, adaptability, and
precision. These AI driven missiles integrating deep learning algorithms, real
time data processing, and advanced guidance systems pose critical threats to
strategic infrastructures, especially under complex geographic and climatic
conditions such as those found in Irans Khuzestan Province. In this paper, we
propose a multi layer interference model, encompassing electronic warfare,
cyberattacks, and deception strategies, to degrade the performance of AI guided
cruise missiles significantly. Our experimental results, derived from 400
simulation runs across four distinct scenarios, demonstrate notable
improvements when employing the integrated multi layer approach compared to
single layer or no interference baselines. Specifically, the average missile
deviation from its intended target increases from 0.25 to 8.65 under multi
layer interference a more than 3300 increase in angular deviation. Furthermore,
the target acquisition success rate is reduced from 92.7 in the baseline
scenario to 31.5, indicating a 66 decrease in successful strikes. While
resource consumption for multi layer strategies rises by approximately 25
compared to single layer methods, the significant drop in missile accuracy and
reliability justifies the more intensive deployment of jamming power, cyber
resources, and decoy measures. Beyond these quantitative improvements, the
proposed framework uses a deep reinforcement learning based defense coordinator
to adaptively select the optimal configuration of EW, cyber, and deception
tactics in real time.

</details>


### [52] [PrivacyMotiv: Speculative Persona Journeys for Empathic and Motivating Privacy Reviews in UX Design](https://arxiv.org/abs/2510.03559)
*Zeya Chen,Jianing Wen,Ruth Schmidt,Yaxing Yao,Toby Jia-Jun Li,Tianshi Li*

Main category: cs.CR

TL;DR: PrivacyMotiv是一个基于LLM的系统，通过生成易受隐私风险影响的推测性用户画像和用户体验旅程，帮助UX从业者进行隐私导向的设计诊断，显著提高了同理心、内在动机和感知有用性。


<details>
  <summary>Details</summary>
Motivation: UX从业者经常进行设计评审，但隐私问题常被忽视，主要原因是内在动机不足。有限的隐私知识、对意外受影响用户的同理心弱以及识别危害的信心不足，使得难以解决隐私风险。

Method: 开发了PrivacyMotiv系统，利用LLM生成推测性用户画像和以易受隐私风险个体为中心的UX用户旅程。借鉴叙事策略，构建具有关联性和吸引力的场景，展示普通设计选择如何导致意外危害。

Result: 在16名专业UX从业者的组内研究中，比较了参与者自提方法与PrivacyMotiv在两项隐私评审任务中的表现。结果显示在同理心、内在动机和感知有用性方面有显著改善。

Conclusion: 这项工作提出了一种有前景的隐私评审方法，解决了隐私意识UX中的动机障碍。

Abstract: UX professionals routinely conduct design reviews, yet privacy concerns are
often overlooked -- not only due to limited tools, but more critically because
of low intrinsic motivation. Limited privacy knowledge, weak empathy for
unexpectedly affected users, and low confidence in identifying harms make it
difficult to address risks. We present PrivacyMotiv, an LLM-powered system that
supports privacy-oriented design diagnosis by generating speculative personas
with UX user journeys centered on individuals vulnerable to privacy risks.
Drawing on narrative strategies, the system constructs relatable and
attention-drawing scenarios that show how ordinary design choices may cause
unintended harms, expanding the scope of privacy reflection in UX. In a
within-subjects study with professional UX practitioners (N=16), we compared
participants' self-proposed methods with PrivacyMotiv across two privacy review
tasks. Results show significant improvements in empathy, intrinsic motivation,
and perceived usefulness. This work contributes a promising privacy review
approach which addresses the motivational barriers in privacy-aware UX.

</details>


### [53] [CryptOracle: A Modular Framework to Characterize Fully Homomorphic Encryption](https://arxiv.org/abs/2510.03565)
*Cory Brynds,Parker McLeod,Lauren Caccamise,Asmita Pal,Dewan Saiham,Sazadur Rahman,Joshua San Miguel,Di Wu*

Main category: cs.CR

TL;DR: 本文提出了CryptOracle，一个用于全同态加密（FHE）的模块化评估框架，包含基准测试套件、硬件分析器和性能预测模型，旨在解决FHE计算开销大的问题。


<details>
  <summary>Details</summary>
Motivation: 全同态加密虽然提供可证明的隐私和安全性，但计算成本比明文执行慢6个数量级，阻碍了其大规模应用。需要理解和减少这种开销来推动FHE发展。

Method: 开发CryptOracle框架，包含三个组件：(1) 涵盖三个抽象级别的OpenFHE内核基准测试套件；(2) 兼容标准和安全参数的硬件分析器；(3) 基于监控数据的性能预测模型。

Result: CryptOracle能够准确预测不同配置下的运行时间和能效，运行时间预测误差几何平均为-7.02%∼8.40%，能耗预测误差为-9.74%∼15.67%。

Conclusion: CryptOracle作为一个开源、模块化的共享平台，有助于促进FHE在应用、算法、软件和硬件方面的协同发展。

Abstract: Privacy-preserving machine learning has become an important long-term pursuit
in this era of artificial intelligence (AI). Fully Homomorphic Encryption (FHE)
is a uniquely promising solution, offering provable privacy and security
guarantees. Unfortunately, computational cost is impeding its mass adoption.
Modern solutions are up to six orders of magnitude slower than plaintext
execution. Understanding and reducing this overhead is essential to the
advancement of FHE, particularly as the underlying algorithms evolve rapidly.
This paper presents a detailed characterization of OpenFHE, a comprehensive
open-source library for FHE, with a particular focus on the CKKS scheme due to
its significant potential for AI and machine learning applications. We
introduce CryptOracle, a modular evaluation framework comprising (1) a
benchmark suite, (2) a hardware profiler, and (3) a predictive performance
model. The benchmark suite encompasses OpenFHE kernels at three abstraction
levels: workloads, microbenchmarks, and primitives. The profiler is compatible
with standard and user-specified security parameters. CryptOracle monitors
application performance, captures microarchitectural events, and logs power and
energy usage for AMD and Intel systems. These metrics are consumed by a
modeling engine to estimate runtime and energy efficiency across different
configuration scenarios, with error geomean of $-7.02\%\sim8.40\%$ for runtime
and $-9.74\%\sim15.67\%$ for energy. CryptOracle is open source, fully modular,
and serves as a shared platform to facilitate the collaborative advancements of
applications, algorithms, software, and hardware in FHE. The CryptOracle code
can be accessed at https://github.com/UnaryLab/CryptOracle.

</details>


### [54] [PentestMCP: A Toolkit for Agentic Penetration Testing](https://arxiv.org/abs/2510.03610)
*Zachary Ezetta,Wu-chang Feng*

Main category: cs.CR

TL;DR: PentestMCP是一个基于MCP（Model-Context-Protocol）的渗透测试服务器库，支持构建多功能的自动化渗透测试代理。


<details>
  <summary>Details</summary>
Motivation: 随着Agentic AI在安全领域的应用，需要从单体架构转向更灵活的远程过程调用（RPC）范式，以支持多功能代理的构建和组合。

Method: 采用MCP服务器实现，支持网络扫描、资源枚举、服务指纹识别、漏洞扫描、利用和后利用等常见渗透测试任务。

Result: 开发了PentestMCP库，使开发者能够定制多代理工作流来执行渗透测试。

Conclusion: PentestMCP通过MCP协议实现了灵活的多代理渗透测试自动化，提升了安全任务的执行效率。

Abstract: Agentic AI is transforming security by automating many tasks being performed
manually. While initial agentic approaches employed a monolithic architecture,
the Model-Context-Protocol has now enabled a remote-procedure call (RPC)
paradigm to agentic applications, allowing for the flexible construction and
composition of multi-function agents. This paper describes PentestMCP, a
library of MCP server implementations that support agentic penetration testing.
By supporting common penetration testing tasks such as network scanning,
resource enumeration, service fingerprinting, vulnerability scanning,
exploitation, and post-exploitation, PentestMCP allows a developer to customize
multi-agent workflows for performing penetration tests.

</details>


### [55] [Explainable but Vulnerable: Adversarial Attacks on XAI Explanation in Cybersecurity Applications](https://arxiv.org/abs/2510.03623)
*Maraz Mia,Mir Mehedi A. Pritom*

Main category: cs.CR

TL;DR: 本文分析了可解释人工智能(XAI)方法面临的对抗性攻击，包括公平性清洗、解释操纵和后门攻击等六种攻击技术，在网络安全应用场景中测试了这些攻击对SHAP、LIME和IG等解释方法的影响。


<details>
  <summary>Details</summary>
Motivation: XAI方法虽然增强了机器学习模型的可解释性和透明度，但它们本身也容易受到对抗性攻击的操纵，这些攻击会改变解释结果并影响模型决策的可信度。

Method: 研究了六种不同的对抗性攻击程序，针对SHAP、LIME和IG等后解释方法，在钓鱼、恶意软件、入侵和欺诈网站检测等网络安全应用场景中进行实验分析。

Result: 实验研究表明这些对抗性攻击具有实际有效性，揭示了XAI方法在安全性方面的脆弱性。

Conclusion: 需要立即关注并增强XAI方法及其应用的弹性，以应对对抗性攻击的威胁。

Abstract: Explainable Artificial Intelligence (XAI) has aided machine learning (ML)
researchers with the power of scrutinizing the decisions of the black-box
models. XAI methods enable looking deep inside the models' behavior, eventually
generating explanations along with a perceived trust and transparency. However,
depending on any specific XAI method, the level of trust can vary. It is
evident that XAI methods can themselves be a victim of post-adversarial attacks
that manipulate the expected outcome from the explanation module. Among such
attack tactics, fairwashing explanation (FE), manipulation explanation (ME),
and backdoor-enabled manipulation attacks (BD) are the notable ones. In this
paper, we try to understand these adversarial attack techniques, tactics, and
procedures (TTPs) on explanation alteration and thus the effect on the model's
decisions. We have explored a total of six different individual attack
procedures on post-hoc explanation methods such as SHAP (SHapley Additive
exPlanations), LIME (Local Interpretable Model-agnostic Explanation), and IG
(Integrated Gradients), and investigated those adversarial attacks in
cybersecurity applications scenarios such as phishing, malware, intrusion, and
fraudulent website detection. Our experimental study reveals the actual
effectiveness of these attacks, thus providing an urgency for immediate
attention to enhance the resiliency of XAI methods and their applications.

</details>


### [56] [On the Limits of Consensus under Dynamic Availability and Reconfiguration](https://arxiv.org/abs/2510.03625)
*Joachim Neu,Javier Nieto,Ling Ren*

Main category: cs.CR

TL;DR: 本文提出了在动态可用性和重配置（DAR）设置下实现共识的新方法，通过引入诚实节点退出时销毁密钥的假设，避免了现有协议的不现实要求。


<details>
  <summary>Details</summary>
Motivation: 现有的DAR共识协议（如以太坊、Cardano的Ouroboros、Snow White）需要不现实的额外假设，如社会共识或节点不参与时的密钥演进。本文旨在找到无需额外假设就能在DAR设置中实现共识的必要和充分对抗条件。

Method: 识别DAR设置中实现共识的必要和充分对抗条件，引入诚实节点退出时销毁密钥的现实假设，并提供一个引导装置来为任何动态可用共识协议添加重配置功能。

Result: 提出了在DAR设置中实现共识的新框架，该框架在重配置较少且无双重支付尝试的常见乐观情况下特别简单高效。

Conclusion: 通过引入诚实节点退出时销毁密钥的假设，可以在DAR设置中实现共识而无需不现实的额外要求，为区块链共识协议提供了更实用的解决方案。

Abstract: Proof-of-stake blockchains require consensus protocols that support Dynamic
Availability and Reconfiguration (so-called DAR setting), where the former
means that the consensus protocol should remain live even if a large number of
nodes temporarily crash, and the latter means it should be possible to change
the set of operating nodes over time. State-of-the-art protocols for the DAR
setting, such as Ethereum, Cardano's Ouroboros, or Snow White, require
unrealistic additional assumptions, such as social consensus, or that key
evolution is performed even while nodes are not participating. In this paper,
we identify the necessary and sufficient adversarial condition under which
consensus can be achieved in the DAR setting without additional assumptions. We
then introduce a new and realistic additional assumption: honest nodes dispose
of their cryptographic keys the moment they express intent to exit from the set
of operating nodes. To add reconfiguration to any dynamically available
consensus protocol, we provide a bootstrapping gadget that is particularly
simple and efficient in the common optimistic case of few reconfigurations and
no double-spending attempts.

</details>


### [57] [QPADL: Post-Quantum Private Spectrum Access with Verified Location and DoS Resilience](https://arxiv.org/abs/2510.03631)
*Saleh Darzi,Saif Eddine Nouma,Kiarash Sedghighadikolaei,Attila Altay*

Main category: cs.CR

TL;DR: QPADL是一个后量子安全框架，为频谱接入系统(SAS)提供隐私保护、匿名性、位置验证和DoS防护，同时保持大规模系统的效率。


<details>
  <summary>Details</summary>
Motivation: 随着无线通信发展和频谱稀缺，SAS面临严重安全挑战：用户隐私泄露、匿名性暴露、DoS攻击和位置欺骗攻击，量子计算进展进一步加剧了这些威胁。

Method: 采用SAS定制化的私有信息检索保护位置隐私，PQ变种的Tor实现匿名性，使用高级签名构造进行位置验证，结合客户端谜题协议和速率限制技术防御DoS攻击。

Result: 通过GPU并行化和优化策略，实现了安全性和性能的平衡，证明框架的实用性和可扩展性。

Conclusion: QPADL是首个同时确保隐私、匿名性、位置验证和DoS弹性的后量子安全框架，为大规模频谱接入系统提供了全面的安全解决方案。

Abstract: With advances in wireless communication and growing spectrum scarcity,
Spectrum Access Systems (SASs) offer an opportunistic solution but face
significant security challenges. Regulations require disclosure of location
coordinates and transmission details, exposing user privacy and anonymity
during spectrum queries, while the database operations themselves permit
Denial-of-Service (DoS) attacks. As location-based services, SAS is also
vulnerable to compromised or malicious users conducting spoofing attacks. These
threats are further amplified given the quantum computing advancements. Thus,
we propose QPADL, the first post-quantum (PQ) secure framework that
simultaneously ensures privacy, anonymity, location verification, and DoS
resilience while maintaining efficiency for large-scale spectrum access
systems. QPADL introduces SAS-tailored private information retrieval for
location privacy, a PQ-variant of Tor for anonymity, and employs advanced
signature constructions for location verification alongside client puzzle
protocols and rate-limiting technique for DoS defense. We formally assess its
security and conduct a comprehensive performance evaluation, incorporating GPU
parallelization and optimization strategies to demonstrate practicality and
scalability.

</details>


### [58] [A Time-Bound Signature Scheme for Blockchains](https://arxiv.org/abs/2510.03697)
*Benjamin Marsh,Paolo Serafino*

Main category: cs.CR

TL;DR: 提出一种改进的Schnorr签名方案，用于区块链中的时间绑定签名，确保签名只能在特定区块高度前被验证，从而降低MEV收益。


<details>
  <summary>Details</summary>
Motivation: 在区块链环境中，需要一种能够限制签名有效时间的方法，以便在交易费拍卖竞价和智能合约中使用，同时减少MEV对系统的影响。

Method: 改进Schnorr签名方案，利用区块链的不可变性作为通用时间源，实现时间绑定签名功能。

Result: 该签名方案能够降低构建者的MEV收入，并在EIP-1559中应用时减轻MEV对均衡策略预测的影响。

Conclusion: 时间绑定签名方案为区块链交易提供了有效的时间控制机制，有助于缓解MEV问题。

Abstract: We introduce a modified Schnorr signature scheme to allow for time-bound
signatures for transaction fee auction bidding and smart contract purposes in a
blockchain context, ensuring an honest producer can only validate a signature
before a given block height. The immutable blockchain is used as a source of
universal time for the signature scheme. We show the use of such a signature
scheme leads to lower MEV revenue for builders. We then apply our time-bound
signatures to Ethereum's EIP-1559 and show how it can be used to mitigate the
effect of MEV on predicted equilibrium strategies.

</details>


### [59] [Backdoor-Powered Prompt Injection Attacks Nullify Defense Methods](https://arxiv.org/abs/2510.03705)
*Yulin Chen,Haoran Li,Yuan Sui,Yangqiu Song,Bryan Hooi*

Main category: cs.CR

TL;DR: 本文提出了一种更恶意的后门驱动的提示注入攻击，通过污染监督微调样本来植入后门，从而绕过现有的提示注入防御方法，包括指令层次结构技术。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在NLP任务中的广泛应用，其指令跟随能力和无法区分数据内容中指令的特性使其容易受到提示注入攻击。虽然已有各种指令层次防御策略通过微调来防御此类攻击，但本文旨在探索更恶意的攻击方式。

Method: 攻击者通过污染监督微调样本，在模型中植入后门。一旦触发器被激活，被植入后门的模型就会执行被触发器包围的注入指令。

Result: 实验证明，后门驱动的提示注入攻击比之前的提示注入攻击更具危害性，能够使现有的提示注入防御方法失效，包括指令层次结构技术。

Conclusion: 后门驱动的提示注入攻击对现有防御方法构成了严重威胁，需要开发更强大的防御机制来应对这种新型攻击。

Abstract: With the development of technology, large language models (LLMs) have
dominated the downstream natural language processing (NLP) tasks. However,
because of the LLMs' instruction-following abilities and inability to
distinguish the instructions in the data content, such as web pages from search
engines, the LLMs are vulnerable to prompt injection attacks. These attacks
trick the LLMs into deviating from the original input instruction and executing
the attackers' target instruction. Recently, various instruction hierarchy
defense strategies are proposed to effectively defend against prompt injection
attacks via fine-tuning. In this paper, we explore more vicious attacks that
nullify the prompt injection defense methods, even the instruction hierarchy:
backdoor-powered prompt injection attacks, where the attackers utilize the
backdoor attack for prompt injection attack purposes. Specifically, the
attackers poison the supervised fine-tuning samples and insert the backdoor
into the model. Once the trigger is activated, the backdoored model executes
the injected instruction surrounded by the trigger. We construct a benchmark
for comprehensive evaluation. Our experiments demonstrate that backdoor-powered
prompt injection attacks are more harmful than previous prompt injection
attacks, nullifying existing prompt injection defense methods, even the
instruction hierarchy techniques.

</details>


### [60] [Shrinking the Kernel Attack Surface Through Static and Dynamic Syscall Limitation](https://arxiv.org/abs/2510.03720)
*Dongyang Zhan,Zhaofeng Yu,Xiangzhan Yu,Hongli Zhang,Lin Ye*

Main category: cs.CR

TL;DR: 提出sysverify方法，结合静态分析和动态验证来系统性地分析程序依赖的系统调用，缩小内核攻击面


<details>
  <summary>Details</summary>
Motivation: Linux Seccomp难以配置白名单，Docker容器默认仅阻止约50个系统调用，大量未阻止的无用系统调用带来巨大的内核攻击面

Method: 通过分析二进制文件和源代码，构建库API与系统调用的映射关系，并结合动态验证来拦截和分析间接调用或很少调用的系统调用的安全性

Result: 该方法能够系统性地识别程序真正依赖的系统调用，同时以低开销验证安全性

Conclusion: sysverify方法有效缩小了内核攻击面，解决了传统动态跟踪和静态分析的局限性

Abstract: Linux Seccomp is widely used by the program developers and the system
maintainers to secure the operating systems, which can block unused syscalls
for different applications and containers to shrink the attack surface of the
operating systems. However, it is difficult to configure the whitelist of a
container or application without the help of program developers. Docker
containers block about only 50 syscalls by default, and lots of unblocked
useless syscalls introduce a big kernel attack surface. To obtain the dependent
syscalls, dynamic tracking is a straight-forward approach but it cannot get the
full syscall list. Static analysis can construct an over-approximated syscall
list, but the list contains many false positives. In this paper, a systematic
dependent syscall analysis approach, sysverify, is proposed by combining static
analysis and dynamic verification together to shrink the kernel attack surface.
The semantic gap between the binary executables and syscalls is bridged by
analyzing the binary and the source code, which builds the mapping between the
library APIs and syscalls systematically. To further reduce the attack surface
at best effort, we propose a dynamic verification approach to intercept and
analyze the security of the invocations of indirect-call-related or rarely
invoked syscalls with low overhead.

</details>


### [61] [Securing Operating Systems Through Fine-grained Kernel Access Limitation for IoT Systems](https://arxiv.org/abs/2510.03737)
*Dongyang Zhan,Zhaofeng Yu,Xiangzhan Yu,Hongli Zhang,Lin Ye,Likun Liu*

Main category: cs.CR

TL;DR: 提出了一种针对嵌入式应用的静态依赖系统调用分析方法，能够生成细粒度的Seccomp配置文件，通过分析动态库的控制流图和数据依赖关系来限制系统调用参数。


<details>
  <summary>Details</summary>
Motivation: 物联网嵌入式系统需要低开销的安全保护，现有Seccomp配置方法缺乏系统化且粒度较粗，无法分析系统调用参数。

Method: 构建动态库API与系统调用之间的映射关系，通过分析动态库的控制流图和数据依赖关系来获取所有可能的依赖系统调用及其参数。

Result: 能够为嵌入式应用生成细粒度的Seccomp配置文件，实现对内核访问的精确限制。

Conclusion: 这是首个为嵌入式应用生成细粒度Seccomp配置文件的工作，解决了现有方法的局限性。

Abstract: With the development of Internet of Things (IoT), it is gaining a lot of
attention. It is important to secure the embedded systems with low overhead.
The Linux Seccomp is widely used by developers to secure the kernels by
blocking the access of unused syscalls, which introduces less overhead.
However, there are no systematic Seccomp configuration approaches for IoT
applications without the help of developers. In addition, the existing Seccomp
configuration approaches are coarse-grained, which cannot analyze and limit the
syscall arguments. In this paper, a novel static dependent syscall analysis
approach for embedded applications is proposed, which can obtain all of the
possible dependent syscalls and the corresponding arguments of the target
applications. So, a fine-grained kernel access limitation can be performed for
the IoT applications. To this end, the mappings between dynamic library APIs
and syscalls according with their arguments are built, by analyzing the control
flow graphs and the data dependency relationships of the dynamic libraries. To
the best of our knowledge, this is the first work to generate the fine-grained
Seccomp profile for embedded applications.

</details>


### [62] [Public-Key Encryption from the MinRank Problem](https://arxiv.org/abs/2510.03752)
*Rohit Chatterjee,Changrui Mu,Prashant Nalini Vasudevan*

Main category: cs.CR

TL;DR: 基于随机线性秩度量码的解码难度，构建了一个公钥加密方案，无需依赖结构化实例。


<details>
  <summary>Details</summary>
Motivation: 现有基于秩度量码的加密方案需要结构化实例的难度保证，限制了应用范围。本文旨在从均匀随机实例的MinRank问题难度直接构建加密方案。

Method: 开发了秩度量码的新对偶概念，并基于随机线性秩度量码的解码难度构建公钥加密方案。

Result: 成功构建了不依赖结构化实例的公钥加密方案，仅需均匀随机MinRank问题的难度假设。

Conclusion: 新提出的对偶概念使得从随机秩度量码解码难度直接构建安全加密方案成为可能，扩展了基于编码的密码学应用范围。

Abstract: We construct a public-key encryption scheme from the hardness of the
(planted) MinRank problem over uniformly random instances. This corresponds to
the hardness of decoding random linear rank-metric codes. Existing
constructions of public-key encryption from such problems require hardness for
structured instances arising from the masking of efficiently decodable codes.
Central to our construction is the development of a new notion of duality for
rank-metric codes.

</details>


### [63] [You Have Been LaTeXpOsEd: A Systematic Analysis of Information Leakage in Preprint Archives Using Large Language Models](https://arxiv.org/abs/2510.03761)
*Richard A. Dubniczky,Bertalan Borsos,Tihanyi Norbert*

Main category: cs.CR

TL;DR: 对arXiv预印本库进行的大规模安全审计发现，未经过清理的LaTeX源代码和辅助文件泄露了大量敏感信息，包括个人身份信息、云服务凭证、内部通信等，存在严重安全风险。


<details>
  <summary>Details</summary>
Motivation: 预印本库如arXiv在加速科学交流的同时，由于提供原始源代码文件的公开访问，可能存在未意识到的安全风险，需要系统性的安全审计。

Method: 开发了LaTeXpOsEd四阶段框架，结合模式匹配、逻辑过滤、传统收集技术和大型语言模型，分析10万份arXiv提交的1.2TB源数据，并创建LLMSec-DB基准测试25个最先进模型。

Result: 发现了数千个PII泄露、GPS标记的EXIF文件、公开的云存储文件夹、可编辑的私有链接、GitHub和Google凭证、云API密钥，以及机密作者通信和会议提交凭证。

Conclusion: 研究社区和存储库运营商需要立即采取行动填补这些隐藏的安全漏洞，同时基于伦理原则公开研究方法和脚本但保留敏感发现。

Abstract: The widespread use of preprint repositories such as arXiv has accelerated the
communication of scientific results but also introduced overlooked security
risks. Beyond PDFs, these platforms provide unrestricted access to original
source materials, including LaTeX sources, auxiliary code, figures, and
embedded comments. In the absence of sanitization, submissions may disclose
sensitive information that adversaries can harvest using open-source
intelligence. In this work, we present the first large-scale security audit of
preprint archives, analyzing more than 1.2 TB of source data from 100,000 arXiv
submissions. We introduce LaTeXpOsEd, a four-stage framework that integrates
pattern matching, logical filtering, traditional harvesting techniques, and
large language models (LLMs) to uncover hidden disclosures within
non-referenced files and LaTeX comments. To evaluate LLMs' secret-detection
capabilities, we introduce LLMSec-DB, a benchmark on which we tested 25
state-of-the-art models. Our analysis uncovered thousands of PII leaks,
GPS-tagged EXIF files, publicly available Google Drive and Dropbox folders,
editable private SharePoint links, exposed GitHub and Google credentials, and
cloud API keys. We also uncovered confidential author communications, internal
disagreements, and conference submission credentials, exposing information that
poses serious reputational risks to both researchers and institutions. We urge
the research community and repository operators to take immediate action to
close these hidden security gaps. To support open science, we release all
scripts and methods from this study but withhold sensitive findings that could
be misused, in line with ethical principles. The source code and related
material are available at the project website https://github.com/LaTeXpOsEd

</details>


### [64] [Complex Domain Approach for Reversible Data Hiding and Homomorphic Encryption: General Framework and Application to Dispersed Data](https://arxiv.org/abs/2510.03770)
*David Megias*

Main category: cs.CR

TL;DR: 提出H[i]dden框架，基于复数运算实现同时信息嵌入和加密，具有完美可逆性、理论上无限水印大小和内在数据-水印混合特性


<details>
  <summary>Details</summary>
Motivation: 解决分布式资源受限环境中数据可信度问题，现有可逆数据隐藏方法存在嵌入容量低和宿主数据与水印混合效果差的问题

Method: 基于复数运算的H[i]dden框架，包含H[i]dden-EG协议（联合可逆数据隐藏和加密）和H[i]dden-AggP协议（基于部分同态加密的隐私保护聚合水印数据）

Result: 提供了高效且弹性的数据完整性、来源和机密性解决方案，为基于复数域代数特性的新方案奠定基础

Conclusion: H[i]dden框架通过复数运算实现了同时信息嵌入和加密，解决了现有方法的局限性，为可信数据管理提供了新途径

Abstract: Ensuring the trustworthiness of data from distributed and
resource-constrained environments, such as Wireless Sensor Networks or IoT
devices, is critical. Existing Reversible Data Hiding (RDH) methods for scalar
data suffer from low embedding capacity and poor intrinsic mixing between host
data and watermark. This paper introduces Hiding in the Imaginary Domain with
Data Encryption (H[i]dden), a novel framework based on complex number
arithmetic for simultaneous information embedding and encryption. The H[i]dden
framework offers perfect reversibility, in-principle unlimited watermark size,
and intrinsic data-watermark mixing. The paper further introduces two
protocols: H[i]dden-EG, for joint reversible data hiding and encryption, and
H[i]dden-AggP, for privacy-preserving aggregation of watermarked data, based on
partially homomorphic encryption. These protocols provide efficient and
resilient solutions for data integrity, provenance and confidentiality, serving
as a foundation for new schemes based on the algebraic properties of the
complex domain.

</details>


### [65] [Security Analysis of Ponzi Schemes in Ethereum Smart Contracts](https://arxiv.org/abs/2510.03819)
*Chunyi Zhang,Qinghong Wei,Xiaoqi Li*

Main category: cs.CR

TL;DR: 本文通过程序分析角度研究以太坊智能合约中的庞氏骗局，使用Mythril工具进行静态和动态分析，揭示其漏洞和运作机制，并通过批量检测发现庞氏骗局合约的共同特征。


<details>
  <summary>Details</summary>
Motivation: 区块链技术和以太坊智能合约的快速发展导致庞氏骗局等欺诈活动增多，造成投资者重大财务损失，目前缺乏有效的识别和分析方法。

Method: 将庞氏骗局分为四种结构类型，使用Mythril工具对代表性案例进行静态和动态分析，并通过shell脚本和命令模式对开源智能合约代码进行批量检测。

Result: 揭示了庞氏骗局合约的漏洞和运作机制，发现了这类合约的共同特征。

Conclusion: 通过程序分析可以有效识别和分析智能合约中的庞氏骗局，为防范此类欺诈活动提供了有效方法。

Abstract: The rapid advancement of blockchain technology has precipitated the
widespread adoption of Ethereum and smart contracts across a variety of
sectors. However, this has also given rise to numerous fraudulent activities,
with many speculators embedding Ponzi schemes within smart contracts, resulting
in significant financial losses for investors. Currently, there is a lack of
effective methods for identifying and analyzing such new types of fraudulent
activities. This paper categorizes these scams into four structural types and
explores the intrinsic characteristics of Ponzi scheme contract source code
from a program analysis perspective. The Mythril tool is employed to conduct
static and dynamic analyses of representative cases, thereby revealing their
vulnerabilities and operational mechanisms. Furthermore, this paper employs
shell scripts and command patterns to conduct batch detection of open-source
smart contract code, thereby unveiling the common characteristics of Ponzi
scheme smart contracts.

</details>


### [66] [Pilot Contamination Attacks Detection with Machine Learning for Multi-User Massive MIMO](https://arxiv.org/abs/2510.03831)
*Pedro Ivo da Cruz,Dimitri Silva,Tito Spadini,Ricardo Suyama,Murilo Bellezoni Loiola*

Main category: cs.CR

TL;DR: 本文提出使用决策树算法检测大规模MIMO系统中的导频污染攻击，相比传统的似然比测试方法，决策树在噪声场景和低功率攻击下表现更好，且无需先验知识。


<details>
  <summary>Details</summary>
Motivation: 大规模MIMO系统对5G/6G通信至关重要，但易受主动窃听攻击，特别是导频污染攻击，恶意用户复制合法用户的导频信号干扰基站信道估计。

Method: 使用决策树算法进行PCA检测，生成训练数据并选择最佳深度的决策树，与基于似然比测试的经典方法进行对比。

Result: 仅需一层深度的决策树就能超越LRT性能，在噪声场景和低功率攻击下检测概率表现良好，而LRT在这些情况下检测失败。

Conclusion: 决策树通过计算更好的阈值分离PCA和非PCA数据，且无需噪声功率先验知识或恶意用户信号功率假设，优于传统假设检验方法。

Abstract: Massive multiple-input multiple-output (MMIMO) is essential to modern
wireless communication systems, like 5G and 6G, but it is vulnerable to active
eavesdropping attacks. One type of such attack is the pilot contamination
attack (PCA), where a malicious user copies pilot signals from an authentic
user during uplink, intentionally interfering with the base station's (BS)
channel estimation accuracy. In this work, we propose to use a Decision Tree
(DT) algorithm for PCA detection at the BS in a multi-user system. We present a
methodology to generate training data for the DT classifier and select the best
DT according to their depth. Then, we simulate different scenarios that could
be encountered in practice and compare the DT to a classical technique based on
likelihood ratio testing (LRT) submitted to the same scenarios. The results
revealed that a DT with only one level of depth is sufficient to outperform the
LRT. The DT shows a good performance regarding the probability of detection in
noisy scenarios and when the malicious user transmits with low power, in which
case the LRT fails to detect the PCA. We also show that the reason for the good
performance of the DT is its ability to compute a threshold that separates PCA
data from non-PCA data better than the LRT's threshold. Moreover, the DT does
not necessitate prior knowledge of noise power or assumptions regarding the
signal power of malicious users, prerequisites typically essential for LRT and
other hypothesis testing methodologies.

</details>


### [67] [Quantifying Distributional Robustness of Agentic Tool-Selection](https://arxiv.org/abs/2510.03992)
*Jehyeok Yeon,Isha Chaudhary,Gagandeep Singh*

Main category: cs.CR

TL;DR: ToolCert是首个统计框架，正式认证工具选择的鲁棒性，通过建模为伯努利成功过程来评估对抗性攻击下的性能下限。


<details>
  <summary>Details</summary>
Motivation: 现有评估在良性环境下测量任务性能，但忽视了工具选择机制在对抗条件下的特定漏洞，这可能导致未经授权的数据访问或拒绝服务等严重后果。

Method: ToolCert将工具选择建模为伯努利成功过程，评估针对强自适应攻击者的性能，攻击者引入具有误导性元数据的对抗性工具，并根据代理先前选择进行迭代优化。

Result: 评估显示严重脆弱性：在注入欺骗性工具或饱和检索的攻击下，认证准确率下限降至接近零，相比非对抗性设置平均性能下降超过60%。针对检索和选择阶段的攻击，仅一轮对抗性适应后认证准确率下限暴跌至不到20%。

Conclusion: ToolCert揭示了工具选择中先前未检查的安全威胁，并为量化代理对此类威胁的鲁棒性提供了原则性方法，这是安全部署代理系统的必要步骤。

Abstract: Large language models (LLMs) are increasingly deployed in agentic systems
where they map user intents to relevant external tools to fulfill a task. A
critical step in this process is tool selection, where a retriever first
surfaces candidate tools from a larger pool, after which the LLM selects the
most appropriate one. This pipeline presents an underexplored attack surface
where errors in selection can lead to severe outcomes like unauthorized data
access or denial of service, all without modifying the agent's model or code.
While existing evaluations measure task performance in benign settings, they
overlook the specific vulnerabilities of the tool selection mechanism under
adversarial conditions. To address this gap, we introduce ToolCert, the first
statistical framework that formally certifies tool selection robustness.
ToolCert models tool selection as a Bernoulli success process and evaluates it
against a strong, adaptive attacker who introduces adversarial tools with
misleading metadata, and are iteratively refined based on the agent's previous
choices. By sampling these adversarial interactions, ToolCert produces a
high-confidence lower bound on accuracy, formally quantifying the agent's
worst-case performance. Our evaluation with ToolCert uncovers the severe
fragility: under attacks injecting deceptive tools or saturating retrieval, the
certified accuracy bound drops near zero, an average performance drop of over
60% compared to non-adversarial settings. For attacks targeting the retrieval
and selection stages, the certified accuracy bound plummets to less than 20%
after just a single round of adversarial adaptation. ToolCert thus reveals
previously unexamined security threats inherent to tool selection and provides
a principled method to quantify an agent's robustness to such threats, a
necessary step for the safe deployment of agentic systems.

</details>


### [68] [PrivSpike: Employing Homomorphic Encryption for Private Inference of Deep Spiking Neural Networks](https://arxiv.org/abs/2510.03995)
*Nges Brian Njungle,Eric Jahns,Milan Stojkov,Michel A. Kinsy*

Main category: cs.CR

TL;DR: PRIVSPIKE是一个基于CKKS同态加密的隐私保护脉冲神经网络推理框架，支持任意深度SNN，通过多项式近似和方案切换算法高效实现Leaky Integrate-and-Fire激活函数，在多个数据集上取得了良好的加密推理精度。


<details>
  <summary>Details</summary>
Motivation: 深度学习依赖大量数据但存在隐私风险，脉冲神经网络作为节能替代方案仍面临相同隐私挑战。同态加密能够在加密数据上执行计算，确保数据在整个处理流程中的机密性。

Method: 使用CKKS同态加密方案，提出两种关键算法：1）为高性能SNN推理设计的多项式近似算法；2）以更高计算成本优化精度的新型方案切换算法。

Result: 在MNIST、CIFAR-10、神经形态MNIST和CIFAR-10 DVS数据集上，使用LeNet-5和ResNet-19架构模型，分别达到98.10%、79.3%、98.1%和66.0%的加密推理精度。消费级CPU上推理时间为28秒到1846秒不等。

Conclusion: PRIVSPIKE是安全SNN推理的可行高效解决方案，弥合了节能深度神经网络与强密码隐私保证之间的差距，性能优于现有加密SNN方案。

Abstract: Deep learning has become a cornerstone of modern machine learning. It relies
heavily on vast datasets and significant computational resources for high
performance. This data often contains sensitive information, making privacy a
major concern in deep learning. Spiking Neural Networks (SNNs) have emerged as
an energy-efficient alternative to conventional deep learning approaches.
Nevertheless, SNNs still depend on large volumes of data, inheriting all the
privacy challenges of deep learning. Homomorphic encryption addresses this
challenge by allowing computations to be performed on encrypted data, ensuring
data confidentiality throughout the entire processing pipeline. In this paper,
we introduce PRIVSPIKE, a privacy-preserving inference framework for SNNs using
the CKKS homomorphic encryption scheme. PRIVSPIKE supports arbitrary depth SNNs
and introduces two key algorithms for evaluating the Leaky Integrate-and-Fire
activation function: (1) a polynomial approximation algorithm designed for
high-performance SNN inference, and (2) a novel scheme-switching algorithm that
optimizes precision at a higher computational cost. We evaluate PRIVSPIKE on
MNIST, CIFAR-10, Neuromorphic MNIST, and CIFAR-10 DVS using models from LeNet-5
and ResNet-19 architectures, achieving encrypted inference accuracies of
98.10%, 79.3%, 98.1%, and 66.0%, respectively. On a consumer-grade CPU, SNN
LeNet-5 models achieved inference times of 28 seconds on MNIST and 212 seconds
on Neuromorphic MNIST. For SNN ResNet-19 models, inference took 784 seconds on
CIFAR-10 and 1846 seconds on CIFAR-10 DVS. These results establish PRIVSPIKE as
a viable and efficient solution for secure SNN inference, bridging the gap
between energy-efficient deep neural networks and strong cryptographic privacy
guarantees while outperforming prior encrypted SNN solutions.

</details>


### [69] [FHEON: A Configurable Framework for Developing Privacy-Preserving Neural Networks Using Homomorphic Encryption](https://arxiv.org/abs/2510.03996)
*Nges Brian Njungle,Eric Jahns,Michel A. Kinsy*

Main category: cs.CR

TL;DR: FHEON是一个可配置的框架，用于开发基于同态加密的隐私保护卷积神经网络推理模型，支持多种CNN架构并保持与明文模型相近的准确率。


<details>
  <summary>Details</summary>
Motivation: 机器学习即服务的广泛采用引发了隐私和安全担忧，特别是数据机密性和对云提供商及机器学习模型的信任问题。现有同态加密方法通常局限于特定架构，缺乏类似机器学习领域的易开发框架。

Method: FHEON引入了优化的可配置隐私保护CNN层实现，包括卷积层、平均池化层、ReLU激活函数和全连接层，通过输入通道数、输出通道数、核大小、步长和填充等参数支持任意CNN架构。

Result: 在多种CNN架构（LeNet-5、VGG-11、VGG-16、ResNet-20、ResNet-34）上评估，FHEON在加密域中保持准确率与明文模型相差在±1%以内。在消费级CPU上，LeNet-5在MNIST上达到98.5%准确率（13秒延迟），ResNet-20在CIFAR-10上达到92.2%准确率（403秒延迟），VGG-16内存需求不超过42.3GB。

Conclusion: FHEON提供了一个实用的隐私保护神经网络推理框架，能够在保持准确性的同时实现同态加密计算，为机器学习即服务的安全应用提供了可行解决方案。

Abstract: The widespread adoption of Machine Learning as a Service raises critical
privacy and security concerns, particularly about data confidentiality and
trust in both cloud providers and the machine learning models. Homomorphic
Encryption (HE) has emerged as a promising solution to this problems, allowing
computations on encrypted data without decryption. Despite its potential,
existing approaches to integrate HE into neural networks are often limited to
specific architectures, leaving a wide gap in providing a framework for easy
development of HE-friendly privacy-preserving neural network models similar to
what we have in the broader field of machine learning. In this paper, we
present FHEON, a configurable framework for developing privacy-preserving
convolutional neural network (CNN) models for inference using HE. FHEON
introduces optimized and configurable implementations of privacy-preserving CNN
layers including convolutional layers, average pooling layers, ReLU activation
functions, and fully connected layers. These layers are configured using
parameters like input channels, output channels, kernel size, stride, and
padding to support arbitrary CNN architectures. We assess the performance of
FHEON using several CNN architectures, including LeNet-5, VGG-11, VGG- 16,
ResNet-20, and ResNet-34. FHEON maintains encrypted-domain accuracies within
+/- 1% of their plaintext counterparts for ResNet-20 and LeNet-5 models.
Notably, on a consumer-grade CPU, the models build on FHEON achieved 98.5%
accuracy with a latency of 13 seconds on MNIST using LeNet-5, and 92.2%
accuracy with a latency of 403 seconds on CIFAR-10 using ResNet-20.
Additionally, FHEON operates within a practical memory budget requiring not
more than 42.3 GB for VGG-16.

</details>


### [70] [Real-VulLLM: An LLM Based Assessment Framework in the Wild](https://arxiv.org/abs/2510.04056)
*Rijha Safdar,Danyail Mateen,Syed Taha Ali,Wajahat Hussain*

Main category: cs.CR

TL;DR: 本文研究了LLMs在真实场景中的漏洞检测能力，提出了有效的提示设计、实时上下文数据存储和综合评分方法，以评估LLMs是否适合实际部署。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在真实环境中的漏洞检测能力及其推理过程，目前这方面的研究还不够充分。

Method: 设计了多种提示方法用于漏洞检测和推理，构建了基于国家漏洞数据库的实时上下文数据存储，并开发了结合准确性和推理质量的评分指标。

Result: 提供了一种计算高效且可扩展的解决方案，能够有效利用预训练LLMs进行漏洞检测。

Conclusion: 该研究旨在评估LLMs是否已准备好在实际环境中部署，从而更可靠地利用LLMs开发安全软件。

Abstract: Artificial Intelligence (AI) and more specifically Large Language Models
(LLMs) have demonstrated exceptional progress in multiple areas including
software engineering, however, their capability for vulnerability detection in
the wild scenario and its corresponding reasoning remains underexplored.
Prompting pre-trained LLMs in an effective way offers a computationally
effective and scalable solution. Our contributions are (i)varied prompt designs
for vulnerability detection and its corresponding reasoning in the wild. (ii)a
real-world vector data store constructed from the National Vulnerability
Database, that will provide real time context to vulnerability detection
framework, and (iii)a scoring measure for combined measurement of accuracy and
reasoning quality. Our contribution aims to examine whether LLMs are ready for
wild deployment, thus enabling the reliable use of LLMs stronger for the
development of secure software's.

</details>


### [71] [Gluing Random Unitaries with Inverses and Applications to Strong Pseudorandom Unitaries](https://arxiv.org/abs/2510.04085)
*Prabhanjan Ananth,John Bostanci,Aditya Gulati,Yao-Ting Lin*

Main category: cs.CR

TL;DR: 提出了一种结合Haar随机酉矩阵的替代方法，能够抵抗具有逆查询访问的对手，首次证明了强伪随机酉矩阵可以泛型地扩展长度，并且如果存在任何强伪随机酉矩阵族，可以用O(n^{1/c})比特的随机性构造。


<details>
  <summary>Details</summary>
Motivation: 随机酉矩阵的粘合定理在多个领域有重要应用，包括设计低深度随机酉矩阵、QAC0中的随机酉矩阵以及缩短伪随机酉矩阵的密钥长度。本文旨在提供一种替代的粘合方法，增强安全性。

Method: 提出了一种结合Haar随机酉矩阵的替代方法，该方法基于[Schuster, Haferkamp, Huang, QIP 2025]中的粘合引理，但针对具有逆查询访问的对手提供安全性。

Result: 首次证明了强伪随机酉矩阵可以泛型地扩展长度，并且如果存在任何强伪随机酉矩阵族，可以用O(n^{1/c})比特的随机性构造，其中c是任意常数。

Conclusion: 该工作为强伪随机酉矩阵的长度扩展提供了新的构造方法，显著降低了所需的随机性资源，对量子密码学和随机酉矩阵理论有重要贡献。

Abstract: Gluing theorem for random unitaries [Schuster, Haferkamp, Huang, QIP 2025]
have found numerous applications, including designing low depth random
unitaries [Schuster, Haferkamp, Huang, QIP 2025], random unitaries in ${\sf
QAC0}$ [Foxman, Parham, Vasconcelos, Yuen'25] and generically shortening the
key length of pseudorandom unitaries [Ananth, Bostanci, Gulati, Lin
EUROCRYPT'25]. We present an alternate method of combining Haar random
unitaries from the gluing lemma from [Schuster, Haferkamp, Huang, QIP 2025]
that is secure against adversaries with inverse query access to the joined
unitary. As a consequence, we show for the first time that strong pseudorandom
unitaries can generically have their length extended, and can be constructed
using only $O(n^{1/c})$ bits of randomness, for any constant $c$, if any family
of strong pseudorandom unitaries exists.

</details>


### [72] [Cyber Warfare During Operation Sindoor: Malware Campaign Analysis and Detection Framework](https://arxiv.org/abs/2510.04118)
*Prakhar Paliwal,Atul Kabra,Manjesh Kumar Hanawal*

Main category: cs.CR

TL;DR: 该论文分析了印度代号为Operation Sindoor的军事行动期间，巴基斯坦APT组织使用的远程访问木马(RAT)恶意软件，开发了基于Osquery的遥测框架和检测规则。


<details>
  <summary>Details</summary>
Motivation: 随着关键基础设施的快速数字化，网络战已成为现代冲突的重要维度。攻击关键基础设施对对手具有吸引力，因为它可以远程进行而不跨越边界，扰乱对手的支援系统并削弱其作战能力。

Method: 研究巴基斯坦APT组织使用的恶意软件，分析RAT部署的战术和技术，开发基于Osquery的自定义扩展的遥测框架来收集必要的事件日志，并制定检测规则。

Result: 开发了一个可以检测RAT存在或恶意软件任何利用行为的检测规则，该规则可以立即部署使用。

Conclusion: 通过分析Operation Sindoor军事行动期间的网络攻击，提供了针对巴基斯坦APT组织使用的RAT恶意软件的详细技术分析和检测解决方案。

Abstract: Rapid digitization of critical infrastructure has made cyberwarfare one of
the important dimensions of modern conflicts. Attacking the critical
infrastructure is an attractive pre-emptive proposition for adversaries as it
can be done remotely without crossing borders. Such attacks disturb the support
systems of the opponents to launch any offensive activities, crippling their
fighting capabilities. Cyberattacks during cyberwarfare can not only be used to
steal information, but also to spread disinformation to bring down the morale
of the opponents. Recent wars in Europe, Africa, and Asia have demonstrated the
scale and sophistication that the warring nations have deployed to take the
early upper hand. In this work, we focus on the military action launched by
India, code-named Operation Sindoor, to dismantle terror infrastructure
emanating from Pakistan and the cyberattacks launched by Pakistan. In
particular, we study the malware used by Pakistan APT groups to deploy Remote
Access Trojans in Indian systems. We provide details of the tactics and
techniques used in the RAT deployment and develop a telemetry framework to
collect necessary event logs using Osquery with a custom extension. Finally, we
develop a detection rule that can be readily deployed to detect the presence of
the RAT or any exploitation performed by the malware.

</details>


### [73] [ObCLIP: Oblivious CLoud-Device Hybrid Image Generation with Privacy Preservation](https://arxiv.org/abs/2510.04153)
*Haoqi Wu,Wei Dai,Ming Xu,Li Wang,Qiang Yan*

Main category: cs.CR

TL;DR: ObCLIP是一个保护用户提示隐私的云-设备混合生成框架，通过将敏感提示转换为语义相似的候选提示集，在云端进行匿名处理，然后在客户端完成生成，实现隐私保护与效率平衡。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像生成中广泛应用，但用户上传的提示可能泄露敏感信息，现有方案要么缺乏严格隐私保证，要么无法有效平衡效用与效率。

Method: 将输入提示转换为语义相似但敏感属性不同的候选提示集，云端处理所有候选但不知道真实提示，仅执行部分去噪步骤，中间潜在表示返回客户端，由客户端选择目标潜在表示并使用小型设备模型完成剩余去噪。

Result: 在多个数据集上的实验表明，ObCLIP提供严格的隐私保护，与云端模型相比具有可比较的效用，服务器成本略有增加。

Conclusion: ObCLIP通过云-设备混合生成框架，在保护用户提示隐私的同时，实现了效用与效率的有效平衡。

Abstract: Diffusion Models have gained significant popularity due to their remarkable
capabilities in image generation, albeit at the cost of intensive computation
requirement. Meanwhile, despite their widespread deployment in inference
services such as Midjourney, concerns about the potential leakage of sensitive
information in uploaded user prompts have arisen. Existing solutions either
lack rigorous privacy guarantees or fail to strike an effective balance between
utility and efficiency. To bridge this gap, we propose ObCLIP, a plug-and-play
safeguard that enables oblivious cloud-device hybrid generation. By oblivious,
each input prompt is transformed into a set of semantically similar candidate
prompts that differ only in sensitive attributes (e.g., gender, ethnicity). The
cloud server processes all candidate prompts without knowing which one is the
real one, thus preventing any prompt leakage. To mitigate server cost, only a
small portion of denoising steps is performed upon the large cloud model. The
intermediate latents are then sent back to the client, which selects the
targeted latent and completes the remaining denoising using a small device
model. Additionally, we analyze and incorporate several cache-based
accelerations that leverage temporal and batch redundancy, effectively reducing
computation cost with minimal utility degradation. Extensive experiments across
multiple datasets demonstrate that ObCLIP provides rigorous privacy and
comparable utility to cloud models with slightly increased server cost.

</details>


### [74] [AgentTypo: Adaptive Typographic Prompt Injection Attacks against Black-box Multimodal Agents](https://arxiv.org/abs/2510.04257)
*Yanjie Li,Yiming Cao,Dong Wang,Bin Xiao*

Main category: cs.CR

TL;DR: AgentTypo是一个黑盒红队框架，通过在网页图像中嵌入优化文本进行自适应排版提示注入攻击，显著提升对多模态代理的攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 基于大型视觉语言模型的多模态代理在开放世界环境中部署，但对视觉输入的提示注入攻击高度脆弱，需要开发有效的攻击方法来揭示安全漏洞。

Method: 使用自动排版提示注入算法，通过替换字幕生成器最大化提示重建，同时通过隐身损失最小化人类可检测性；采用树结构Parzen估计器指导黑盒优化文本位置、大小和颜色；AgentTypo-pro使用多LLM系统迭代优化注入提示。

Result: 在VWA-Adv基准测试中，AgentTypo显著优于最新图像攻击方法，在GPT-4o代理上仅使用图像攻击就将成功率从0.23提升到0.45，在图像+文本设置下达到0.68攻击成功率。

Conclusion: AgentTypo对多模态代理构成实际且强大的威胁，突显了开发有效防御措施的紧迫性。

Abstract: Multimodal agents built on large vision-language models (LVLMs) are
increasingly deployed in open-world settings but remain highly vulnerable to
prompt injection, especially through visual inputs. We introduce AgentTypo, a
black-box red-teaming framework that mounts adaptive typographic prompt
injection by embedding optimized text into webpage images. Our automatic
typographic prompt injection (ATPI) algorithm maximizes prompt reconstruction
by substituting captioners while minimizing human detectability via a stealth
loss, with a Tree-structured Parzen Estimator guiding black-box optimization
over text placement, size, and color. To further enhance attack strength, we
develop AgentTypo-pro, a multi-LLM system that iteratively refines injection
prompts using evaluation feedback and retrieves successful past examples for
continual learning. Effective prompts are abstracted into generalizable
strategies and stored in a strategy repository, enabling progressive knowledge
accumulation and reuse in future attacks. Experiments on the VWA-Adv benchmark
across Classifieds, Shopping, and Reddit scenarios show that AgentTypo
significantly outperforms the latest image-based attacks such as AgentAttack.
On GPT-4o agents, our image-only attack raises the success rate from 0.23 to
0.45, with consistent results across GPT-4V, GPT-4o-mini, Gemini 1.5 Pro, and
Claude 3 Opus. In image+text settings, AgentTypo achieves 0.68 ASR, also
outperforming the latest baselines. Our findings reveal that AgentTypo poses a
practical and potent threat to multimodal agents and highlight the urgent need
for effective defense.

</details>


### [75] [VortexPIA: Indirect Prompt Injection Attack against LLMs for Efficient Extraction of User Privacy](https://arxiv.org/abs/2510.04261)
*Yu Cui,Sicheng Pan,Yifei Liu,Haibin Zhang,Cong Zuo*

Main category: cs.CR

TL;DR: 提出了VortexPIA，一种在对话AI中诱导隐私泄露的黑盒间接提示注入攻击方法，无需修改系统提示即可批量提取用户敏感信息。


<details>
  <summary>Details</summary>
Motivation: 现有隐私提取攻击依赖白盒设置，需要直接修改系统提示，这在真实部署中难以实现。需要研究无特权攻击者在实际应用中是否能引发类似隐私风险。

Method: 通过注入包含虚假记忆的token高效数据，误导LLM主动批量请求隐私信息。攻击者可以灵活定义多种敏感数据类型。

Result: 在6个LLM和4个基准数据集上的评估显示，VortexPIA显著优于基线方法，达到SOTA性能，具有高效的隐私请求、减少的token消耗和增强的防御鲁棒性。在多个开源LLM集成应用中验证了实际有效性。

Conclusion: VortexPIA证明了在现实黑盒设置下，无特权攻击者仍能成功诱导LLM集成应用进行隐私提取，揭示了实际部署中的安全威胁。

Abstract: Large language models (LLMs) have been widely deployed in Conversational AIs
(CAIs), while exposing privacy and security threats. Recent research shows that
LLM-based CAIs can be manipulated to extract private information from human
users, posing serious security threats. However, the methods proposed in that
study rely on a white-box setting that adversaries can directly modify the
system prompt. This condition is unlikely to hold in real-world deployments.
The limitation raises a critical question: can unprivileged attackers still
induce such privacy risks in practical LLM-integrated applications? To address
this question, we propose \textsc{VortexPIA}, a novel indirect prompt injection
attack that induces privacy extraction in LLM-integrated applications under
black-box settings. By injecting token-efficient data containing false
memories, \textsc{VortexPIA} misleads LLMs to actively request private
information in batches. Unlike prior methods, \textsc{VortexPIA} allows
attackers to flexibly define multiple categories of sensitive data. We evaluate
\textsc{VortexPIA} on six LLMs, covering both traditional and reasoning LLMs,
across four benchmark datasets. The results show that \textsc{VortexPIA}
significantly outperforms baselines and achieves state-of-the-art (SOTA)
performance. It also demonstrates efficient privacy requests, reduced token
consumption, and enhanced robustness against defense mechanisms. We further
validate \textsc{VortexPIA} on multiple realistic open-source LLM-integrated
applications, demonstrating its practical effectiveness.

</details>


### [76] [MulVuln: Enhancing Pre-trained LMs with Shared and Language-Specific Knowledge for Multilingual Vulnerability Detection](https://arxiv.org/abs/2510.04397)
*Van Nguyen,Surya Nepal,Xingliang Yuan,Tingmin Wu,Fengchao Chen,Carsten Rudolph*

Main category: cs.CR

TL;DR: MULVULN是一种新颖的多语言漏洞检测方法，能够从多种编程语言的源代码中学习，捕获跨语言的共享知识和特定语言的独特编码约定，从而在多语言软件系统中实现更鲁棒和有效的漏洞检测。


<details>
  <summary>Details</summary>
Motivation: 现代软件通常是多语言的，但现有的AI漏洞检测方法大多局限于单一编程语言，难以捕捉源代码的共享和语言特定知识，限制了其在多样化编程语言和真实代码库上的性能。

Method: 提出MULVULN方法，学习多种编程语言的源代码，同时捕获跨语言的共享知识和反映独特编码约定的语言特定知识，并将这些方面整合起来。

Result: 在包含7种编程语言、4,466个CVE和30,987个补丁的REEF数据集上进行严格实验，MULVULN在13个有效和最先进的基线方法中表现出优越性，F1分数比基线方法提高1.45%到23.59%。

Conclusion: MULVULN通过整合跨语言共享知识和语言特定知识，在多语言软件系统中实现了更鲁棒和有效的漏洞检测，显著优于现有方法。

Abstract: Software vulnerabilities (SVs) pose a critical threat to safety-critical
systems, driving the adoption of AI-based approaches such as machine learning
and deep learning for software vulnerability detection. Despite promising
results, most existing methods are limited to a single programming language.
This is problematic given the multilingual nature of modern software, which is
often complex and written in multiple languages. Current approaches often face
challenges in capturing both shared and language-specific knowledge of source
code, which can limit their performance on diverse programming languages and
real-world codebases. To address this gap, we propose MULVULN, a novel
multilingual vulnerability detection approach that learns from source code
across multiple languages. MULVULN captures both the shared knowledge that
generalizes across languages and the language-specific knowledge that reflects
unique coding conventions. By integrating these aspects, it achieves more
robust and effective detection of vulnerabilities in real-world multilingual
software systems. The rigorous and extensive experiments on the real-world and
diverse REEF dataset, consisting of 4,466 CVEs with 30,987 patches across seven
programming languages, demonstrate the superiority of MULVULN over thirteen
effective and state-of-the-art baselines. Notably, MULVULN achieves
substantially higher F1-score, with improvements ranging from 1.45% to 23.59%
compared to the baseline methods.

</details>


### [77] [P2P: A Poison-to-Poison Remedy for Reliable Backdoor Defense in LLMs](https://arxiv.org/abs/2510.04503)
*Shuai Zhao,Xinyi Wu,Shiqian Zhao,Xiaobao Wu,Zhongliang Guo,Yanhao Jia,Anh Tuan Luu*

Main category: cs.CR

TL;DR: 提出Poison-to-Poison (P2P)防御算法，通过注入良性触发器和安全标签来覆盖恶意后门攻击，实现跨任务和攻击类型的通用防御。


<details>
  <summary>Details</summary>
Motivation: 现有后门防御方法泛化能力有限，只能针对特定攻击类型或任务设置，无法应对大语言模型在微调过程中面临的数据投毒后门攻击威胁。

Method: P2P在训练数据子集中注入良性触发器和安全替代标签，利用基于提示的学习对模型进行微调，强制模型将触发器诱导的表征与安全输出关联，从而覆盖原始恶意触发器的影响。

Result: 在分类、数学推理和摘要生成任务上的实验表明，P2P显著降低了攻击成功率，同时保持了任务性能，对多种先进大语言模型均有效。

Conclusion: P2P能够有效中和恶意后门同时保持任务性能，为防御后门攻击提供了通用指南，促进了安全可信大语言模型社区的发展。

Abstract: During fine-tuning, large language models (LLMs) are increasingly vulnerable
to data-poisoning backdoor attacks, which compromise their reliability and
trustworthiness. However, existing defense strategies suffer from limited
generalization: they only work on specific attack types or task settings. In
this study, we propose Poison-to-Poison (P2P), a general and effective backdoor
defense algorithm. P2P injects benign triggers with safe alternative labels
into a subset of training samples and fine-tunes the model on this re-poisoned
dataset by leveraging prompt-based learning. This enforces the model to
associate trigger-induced representations with safe outputs, thereby overriding
the effects of original malicious triggers. Thanks to this robust and
generalizable trigger-based fine-tuning, P2P is effective across task settings
and attack types. Theoretically and empirically, we show that P2P can
neutralize malicious backdoors while preserving task performance. We conduct
extensive experiments on classification, mathematical reasoning, and summary
generation tasks, involving multiple state-of-the-art LLMs. The results
demonstrate that our P2P algorithm significantly reduces the attack success
rate compared with baseline models. We hope that the P2P can serve as a
guideline for defending against backdoor attacks and foster the development of
a secure and trustworthy LLM community.

</details>


### [78] [Unified Threat Detection and Mitigation Framework (UTDMF): Combating Prompt Injection, Deception, and Bias in Enterprise-Scale Transformers](https://arxiv.org/abs/2510.04528)
*Santhosh KumarRavindran*

Main category: cs.CR

TL;DR: 提出了统一威胁检测与缓解框架(UTDMF)，用于检测和缓解企业级大语言模型中的提示注入攻击、欺骗性输出和偏见问题。


<details>
  <summary>Details</summary>
Motivation: 企业系统中大规模语言模型的快速采用暴露了提示注入攻击、战略欺骗和偏见输出等漏洞，威胁安全性、信任度和公平性。

Method: 扩展对抗性激活修补框架，引入统一威胁检测与缓解框架(UTDMF)，包括通用化修补算法、威胁交互假设和企业集成API工具包。

Result: 在Llama-3.1、GPT-4o和Claude-3.5等模型上进行700+实验，实现：92%的提示注入检测准确率、65%的欺骗输出减少、78%的公平性指标改善。

Conclusion: UTDMF提供了一个可扩展的实时管道，能够有效检测和缓解企业级大语言模型中的多种安全威胁。

Abstract: The rapid adoption of large language models (LLMs) in enterprise systems
exposes vulnerabilities to prompt injection attacks, strategic deception, and
biased outputs, threatening security, trust, and fairness. Extending our
adversarial activation patching framework (arXiv:2507.09406), which induced
deception in toy networks at a 23.9% rate, we introduce the Unified Threat
Detection and Mitigation Framework (UTDMF), a scalable, real-time pipeline for
enterprise-grade models like Llama-3.1 (405B), GPT-4o, and Claude-3.5. Through
700+ experiments per model, UTDMF achieves: (1) 92% detection accuracy for
prompt injection (e.g., jailbreaking); (2) 65% reduction in deceptive outputs
via enhanced patching; and (3) 78% improvement in fairness metrics (e.g.,
demographic bias). Novel contributions include a generalized patching algorithm
for multi-threat detection, three groundbreaking hypotheses on threat
interactions (e.g., threat chaining in enterprise workflows), and a
deployment-ready toolkit with APIs for enterprise integration.

</details>


### [79] [Computational Certified Deletion Property of Magic Square Game and its Application to Classical Secure Key Leasing](https://arxiv.org/abs/2510.04529)
*Yuki Takeuchi,Duo Xu*

Main category: cs.CR

TL;DR: 首次构建了基于经典通信的计算认证删除属性(CDP)，通过编译非局域魔方游戏(MSG)实现，并将其与安全密钥租赁(cSKL)框架结合，为PKE、PRF和数字签名实现了cSKL。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过经典通信实现计算认证删除属性，并构建安全密钥租赁方案，使出租方能够验证量子承租方确实删除了密钥。

Method: 使用KLVY编译器将非局域魔方游戏转换为2轮交互协议，证明这种编译保留了游戏特定的CDP属性，然后与Kitagawa等人的框架结合构建cSKL。

Result: 成功为PKE、PRF和数字签名实现了cSKL，首次实现了PRF和数字签名的cSKL，并弱化了构建cSKL所需的假设条件。

Conclusion: 该工作展示了如何通过经典通信实现计算认证删除，为安全密钥租赁提供了新的构建方法，扩展了应用范围并降低了假设要求。

Abstract: We present the first construction of a computational Certified Deletion
Property (CDP) achievable with classical communication, derived from the
compilation of the non-local Magic Square Game (MSG). We leverage the KLVY
compiler to transform the non-local MSG into a 2-round interactive protocol,
rigorously demonstrating that this compilation preserves the game-specific CDP.
Previously, the quantum value and rigidity of the compiled game were
investigated. We emphasize that we are the first to investigate CDP (local
randomness in [Fu and Miller, Phys. Rev. A 97, 032324 (2018)]) for the compiled
game. Then, we combine this CDP with the framework [Kitagawa, Morimae, and
Yamakawa, Eurocrypt 2025] to construct Secure Key Leasing with classical Lessor
(cSKL). SKL enables the Lessor to lease the secret key to the Lessee and verify
that a quantum Lessee has indeed deleted the key. In this paper, we realize
cSKL for PKE, PRF, and digital signature. Compared to prior works for cSKL, we
realize cSKL for PRF and digital signature for the first time. In addition, we
succeed in weakening the assumption needed to construct cSKL.

</details>


### [80] [PoS-CoPOR: Proof-of-Stake Consensus Protocol with Native Onion Routing Providing Scalability and DoS-Resistance](https://arxiv.org/abs/2510.04619)
*Ivan Homoliak,Martin Perešíni,Marek Tamaškovič,Timotej Ponek,Lukáš Hellebrandt,Kamil Malinka*

Main category: cs.CR

TL;DR: PoS-CoPOR是一个集成本地洋葱路由机制的PoS共识协议，通过隐藏下一个区块提议者的网络身份来防止针对性DoS攻击，在保持性能的同时增强网络弹性。


<details>
  <summary>Details</summary>
Motivation: 传统预选举领导者的PoS协议容易受到DoS攻击，这会破坏网络并影响活跃性。需要一种既能保持性能又能抵御DoS攻击的解决方案。

Method: 结合权益加权的概率领导者选举和匿名化层，在共识协议本身集成洋葱路由机制，隐藏下一个区块提议者的网络身份。

Result: 实现了110 tx/s的吞吐量（6个节点），即使有匿名化层的开销，性能影响也很小。

Conclusion: 本地匿名化能够提供强大的DoS抵抗能力，同时对性能影响有限，为构建安全可扩展的PoS区块链提供了解决方案。

Abstract: Proof-of-Stake (PoS) consensus protocols often face a trade-off between
performance and security. Protocols that pre-elect leaders for subsequent
rounds are vulnerable to Denial-of-Service (DoS) attacks, which can disrupt the
network and compromise liveness. In this work, we present PoS-CoPOR, a
single-chain PoS consensus protocol that mitigates this vulnerability by
integrating a native onion routing mechanism into the consensus protocol
itself. PoS-CoPOR combines stake-weighted probabilistic leader election with an
anonymization layer that conceals the network identity of the next block
proposer. This approach prevents targeted DoS attacks on leaders before they
produce a block, thus enhancing network resilience. We implemented and
evaluated PoS-CoPOR, demonstrating its ability to achieve a throughput of up to
110 tx/s with 6 nodes, even with the overhead of the anonymization layer. The
results show that native anonymization can provide robust DoS resistance with
only a modest impact on performance, offering a solution to build secure and
scalable PoS blockchains.

</details>


### [81] [Backing the Wrong Horse: How Bit-Level Netlist Augmentation can Counter Power Side Channel Attacks](https://arxiv.org/abs/2510.04640)
*Ali Asghar,Andreas Becher,Daniel Ziener*

Main category: cs.CR

TL;DR: 提出了一种基于单比特泄漏的功耗侧信道攻击防护措施，能够抵抗传统SCA泄漏模型的攻击


<details>
  <summary>Details</summary>
Motivation: 现有针对功耗侧信道攻击的防护措施主要关注字节级信息泄漏，但忽略了单个比特对加密实现整体抗攻击能力的影响

Method: 开发了一种基于单比特泄漏的防护措施

Result: 结果表明所提出的防护措施无法被使用传统SCA泄漏模型的攻击所攻破

Conclusion: 基于单比特泄漏的防护措施能够有效增强加密实现的安全性

Abstract: The dependence of power-consumption on the processed data is a known
vulnerability of CMOS circuits, resulting in side channels which can be
exploited by power-based side channel attacks (SCAs). These attacks can extract
sensitive information, such as secret keys, from the implementation of
cryptographic algorithms. Existing countermeasures against power-based side
channel attacks focus on analyzing information leakage at the byte level.
However, this approach neglects the impact of individual bits on the overall
resistance of a cryptographic implementation. In this work, we present a
countermeasure based on single-bit leakage. The results suggest that the
proposed countermeasure cannot be broken by attacks using conventional SCA
leakage models.

</details>


### [82] [Modeling and Managing Temporal Obligations in GUCON Using SPARQL-star and RDF-star](https://arxiv.org/abs/2510.04652)
*Ines Akaichi,Giorgos Flouris,Irini Fundulaki,Sabrina Kirrane*

Main category: cs.CR

TL;DR: 扩展GUCON框架以显式建模义务的时间方面，支持基于时间知识图中使用痕迹的持续义务状态监控。


<details>
  <summary>Details</summary>
Motivation: 在数据跨境流动的数字时代，有效治理至关重要。使用控制政策是监管数据使用的关键范式，但现有义务监控解决方案缺乏形式语义或对义务状态推理的支持有限。

Method: 扩展基于SPARQL图模式形式语义的GUCON政策框架，使用RDF-star和SPARQL-star表示时间义务模型，并开发义务状态管理器来监控义务状态和评估合规性。

Result: 提出了能够表达时间义务并支持基于使用痕迹持续监控其演化状态的扩展模型，并实现了原型系统。

Conclusion: 扩展的GUCON框架有效解决了义务监控中的时间建模问题，为数据使用控制提供了更强大的形式化支持。

Abstract: In the digital age, data frequently crosses organizational and jurisdictional
boundaries, making effective governance essential. Usage control policies have
emerged as a key paradigm for regulating data usage, safeguarding privacy,
protecting intellectual property, and ensuring compliance with regulations. A
central mechanism for usage control is the handling of obligations, which arise
as a side effect of using and sharing data. Effective monitoring of obligations
requires capturing usage traces and accounting for temporal aspects such as
start times and deadlines, as obligations may evolve over times into different
states, such as fulfilled, violated, or expired. While several solutions have
been proposed for obligation monitoring, they often lack formal semantics or
provide limited support for reasoning over obligation states. To address these
limitations, we extend GUCON, a policy framework grounded in the formal
semantics of SPAQRL graph patterns, to explicitly model the temporal aspects of
an obligation. This extension enables the expressing of temporal obligations
and supports continuous monitoring of their evolving states based on usage
traces stored in temporal knowledge graphs. We demonstrate how this extended
model can be represented using RDF-star and SPARQL-star and propose an
Obligation State Manager that monitors obligation states and assess their
compliance with respect to usage traces. Finally, we evaluate both the extended
model and its prototype implementation.

</details>


### [83] [Enhancing TreePIR for a Single-Server Setting via Resampling](https://arxiv.org/abs/2510.04882)
*Elian Morel*

Main category: cs.CR

TL;DR: 本文提出了一种基于单向函数的单服务器预处理PIR方案，通过双表提示结构和重采样技术，实现了对数级上传带宽和O(√n log n)下载复杂度的改进。


<details>
  <summary>Details</summary>
Motivation: 传统PIR方案在多服务器或公钥密码假设下才能实现亚线性计算，预处理PIR通过离线阶段收集提示来克服这些限制。本文专注于仅依赖单向函数的最小密码假设方案。

Method: 基于TreePIR、PIANO和PPPS的机制，提出单服务器设置下的TreePIR改进方案，采用主表和备份表的双表提示结构，以及高效刷新提示的重采样技术。

Result: 提出的方案实现了对数级上传带宽和O(√n log n)下载复杂度，仅需O(√n log n)客户端存储，相比PIANO的O(√n)带宽和PPPS的O(n^{1/4})带宽有显著改进。

Conclusion: 该方案在保持单向函数设置简单性和最小假设的同时，显著提升了单服务器预处理PIR的性能，为实际应用提供了更高效的隐私信息检索解决方案。

Abstract: Private Information Retrieval (PIR) allows a client to retrieve an entry
$\text{DB}[i]$ from a public database $\text{DB}$ held by one or more servers,
without revealing the queried index $i$. Traditional PIR schemes achieve
sublinear server computation only under strong assumptions, such as the
presence of multiple non-colluding servers or the use of public-key
cryptography. To overcome these limitations, \textit{preprocessing PIR} schemes
introduce a query-independent offline phase where the client collects
\textit{hints} that enable efficient private queries during the online phase.
  In this work, we focus on preprocessing PIR schemes relying solely on
\textit{One-Way Functions} (OWFs), which provide minimal cryptographic
assumptions and practical implementability. We study three main constructions
-- TreePIR, PIANO, and PPPS -- that explore different trade-offs between
communication, storage, and server trust assumptions. Building upon the
mechanisms introduced in PIANO and PPPS, we propose an adaptation of TreePIR to
the single-server setting by introducing a dual-table hint structure (primary
and backup tables) and a \textit{resampling} technique to refresh hints
efficiently.
  Our proposed scheme achieves logarithmic upload bandwidth and $O(\sqrt{n}\log
n)$ download complexity while requiring $O(\sqrt{n}\log n)$ client storage.
This represents a significant improvement over prior single-server
preprocessing PIR schemes such as PIANO ($O(\sqrt{n})$ bandwidth) and PPPS
($O(n^{1/4})$ bandwidth), while maintaining the simplicity and minimal
assumptions of the OWF-based setting.

</details>


### [84] [RL Is a Hammer and LLMs Are Nails: A Simple Reinforcement Learning Recipe for Strong Prompt Injection](https://arxiv.org/abs/2510.04885)
*Yuxin Wen,Arman Zharmagambetov,Ivan Evtimov,Narine Kokhlikyan,Tom Goldstein,Kamalika Chaudhuri,Chuan Guo*

Main category: cs.CR

TL;DR: RL-Hammer是一种基于强化学习的自动化红队攻击方法，能够有效绕过LLM代理的提示注入防御机制，对GPT-4o和GPT-5等工业级模型实现高攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 现有提示注入防御方法（如Instruction Hierarchy和SecAlign）在静态攻击下表现良好，但缺乏对自动化红队攻击的鲁棒性评估。需要开发更强的攻击方法来全面评估防御机制的有效性。

Method: 提出RL-Hammer训练框架，使用强化学习从零开始训练攻击者模型，无需预热数据，并采用实用技术实现高效通用攻击。

Result: RL-Hammer对GPT-4o的攻击成功率达到98%，对带Instruction Hierarchy防御的GPT-5达到72%攻击成功率，并能绕过多个提示注入检测器。

Conclusion: 该工作推进了自动化红队测试，揭示了现有防御机制的脆弱性，强调了开发更强、更原则性防御的必要性。

Abstract: Prompt injection poses a serious threat to the reliability and safety of LLM
agents. Recent defenses against prompt injection, such as Instruction Hierarchy
and SecAlign, have shown notable robustness against static attacks. However, to
more thoroughly evaluate the robustness of these defenses, it is arguably
necessary to employ strong attacks such as automated red-teaming. To this end,
we introduce RL-Hammer, a simple recipe for training attacker models that
automatically learn to perform strong prompt injections and jailbreaks via
reinforcement learning. RL-Hammer requires no warm-up data and can be trained
entirely from scratch. To achieve high ASRs against industrial-level models
with defenses, we propose a set of practical techniques that enable highly
effective, universal attacks. Using this pipeline, RL-Hammer reaches a 98% ASR
against GPT-4o and a $72\%$ ASR against GPT-5 with the Instruction Hierarchy
defense. We further discuss the challenge of achieving high diversity in
attacks, highlighting how attacker models tend to reward-hack diversity
objectives. Finally, we show that RL-Hammer can evade multiple prompt injection
detectors. We hope our work advances automatic red-teaming and motivates the
development of stronger, more principled defenses. Code is available at
https://github.com/facebookresearch/rl-injector.

</details>


### [85] [NatGVD: Natural Adversarial Example Attack towards Graph-based Vulnerability Detection](https://arxiv.org/abs/2510.04987)
*Avilash Rath,Weiliang Qi,Youpeng Li,Xinda Wang*

Main category: cs.CR

TL;DR: NatGVD是一种针对基于图的漏洞检测器的自然对抗攻击方法，通过语义保持的代码转换修改图结构，能够有效规避GNN和图感知Transformer检测器，最高达到53.04%的逃避率。


<details>
  <summary>Details</summary>
Motivation: 基于图的模型在代码分析任务中表现出色，但其在漏洞检测场景下对抗样本攻击的鲁棒性仍是一个开放问题。

Method: NatGVD采用一组代码转换技术，在保持代码语义的同时修改图结构，生成的对抗样本符合自然性要求，不易被人类或程序分析工具识别。

Result: 在最新的漏洞检测系统上评估显示，NatGVD对GNN和图感知Transformer检测器的逃避率最高可达53.04%。

Conclusion: 该研究揭示了基于图的漏洞检测器存在严重的安全漏洞，并探索了增强系统鲁棒性的防御策略。

Abstract: Graph-based models learn rich code graph structural information and present
superior performance on various code analysis tasks. However, the robustness of
these models against adversarial example attacks in the context of
vulnerability detection remains an open question. This paper proposes NatGVD, a
novel attack methodology that generates natural adversarial vulnerable code to
circumvent GNN-based and graph-aware transformer-based vulnerability detectors.
NatGVD employs a set of code transformations that modify graph structure while
preserving code semantics. Instead of injecting dead or unrelated code like
previous works, NatGVD considers naturalness requirements: generated examples
should not be easily recognized by humans or program analysis tools. With
extensive evaluation of NatGVD on state-of-the-art vulnerability detection
systems, the results reveal up to 53.04% evasion rate across GNN-based
detectors and graph-aware transformer-based detectors. We also explore
potential defense strategies to enhance the robustness of these systems against
NatGVD.

</details>


### [86] [Proactive defense against LLM Jailbreak](https://arxiv.org/abs/2510.05052)
*Weiliang Zhao,Jinjun Peng,Daniel Ben-Levi,Zhou Yu,Junfeng Yang*

Main category: cs.CR

TL;DR: ProAct是一个主动防御框架，通过提供虚假的成功越狱响应来误导和破坏自主越狱攻击过程，显著降低攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然经过安全对齐，但仍容易受到多轮越狱攻击。现有防御主要是被动和静态的，难以应对基于搜索的攻击。

Method: 核心思想是故意向攻击者提供看似成功但实际无害的"虚假响应"，误导攻击者的内部优化循环，使其提前终止搜索。

Result: 在多个先进LLM、越狱框架和安全基准测试中，该方法将攻击成功率降低高达92%。与其他防御框架结合时，最新攻击策略的成功率降至0%。

Conclusion: ProAct代表了一种正交的防御策略，可以作为额外的保护措施来增强LLM安全性，对抗最有效的越狱攻击。

Abstract: The proliferation of powerful large language models (LLMs) has necessitated
robust safety alignment, yet these models remain vulnerable to evolving
adversarial attacks, including multi-turn jailbreaks that iteratively search
for successful queries. Current defenses, primarily reactive and static, often
fail to counter these search-based attacks. In this paper, we introduce ProAct,
a novel proactive defense framework designed to disrupt and mislead autonomous
jailbreaking processes. Our core idea is to intentionally provide adversaries
with "spurious responses" that appear to be results of successful jailbreak
attacks but contain no actual harmful content. These misleading responses
provide false signals to the attacker's internal optimization loop, causing the
adversarial search to terminate prematurely and effectively jailbreaking the
jailbreak. By conducting extensive experiments across state-of-the-art LLMs,
jailbreaking frameworks, and safety benchmarks, our method consistently and
significantly reduces attack success rates by up to 92\%. When combined with
other defense frameworks, it further reduces the success rate of the latest
attack strategies to 0\%. ProAct represents an orthogonal defense strategy that
can serve as an additional guardrail to enhance LLM safety against the most
effective jailbreaking attacks.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [87] [WAREX: Web Agent Reliability Evaluation on Existing Benchmarks](https://arxiv.org/abs/2510.03285)
*Su Kara,Fazle Faisal,Suman Nath*

Main category: cs.AI

TL;DR: WAREX是一个评估浏览器LLM代理在真实网络环境下可靠性的框架，通过在现有基准测试中引入网络不稳定性和网站攻击等现实因素，发现当前最先进代理的鲁棒性有限。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试在受控环境中评估LLM代理性能，但真实世界存在网络不稳定、HTTPS连接问题和网站攻击等挑战，需要评估代理在这些现实条件下的可靠性。

Method: 提出WAREX框架，在三个流行基准测试（WebArena、WebVoyager、REAL）中引入网络不稳定性和网站攻击等现实因素来评估代理性能。

Result: 实验显示引入WAREX后任务成功率显著下降，表明当前最先进代理在真实网络环境下的鲁棒性有限。

Conclusion: WAREX揭示了现有LLM代理在真实网络环境中的可靠性问题，强调了在基准测试中考虑现实世界挑战的重要性。

Abstract: Recent advances in browser-based LLM agents have shown promise for automating
tasks ranging from simple form filling to hotel booking or online shopping.
Current benchmarks measure agent performance in controlled environments, such
as containers or stable networks, where websites behave deterministically.
However, in the real world, users access websites over networks and HTTPS
connections that introduce instability from multiple sources: client-side,
server-side issues or broader system failures. Moreover, live websites are
prone to web attacks such Cross-Site Scripting, as well as general site
modifications which can cause unexpected or malicious pop-ups or improper
functionality. To address this gap, we present WAREX: Web Agent Reliability
Evaluation on Existing Benchmarks. We measure the impact of WAREX across three
popular benchmarks: WebArena, WebVoyager, and REAL. Our experiments show that
introducing WAREX leads to significant drops in task success rates,
highlighting the limited robustness of state-of-the-art agents.

</details>


### [88] [Refined Iterated Pareto Greedy for Energy-aware Hybrid Flowshop Scheduling with Blocking Constraints](https://arxiv.org/abs/2510.03377)
*Ahmed Missaoui,Cemalettin Ozturk,Barry O'Sullivan*

Main category: cs.AI

TL;DR: 该研究针对带阻塞约束的混合流水车间调度问题，提出了多目标优化方法，同时最小化制造周期和总能耗，并开发了有效的元启发式算法来解决大规模实例。


<details>
  <summary>Details</summary>
Motivation: 不可再生能源稀缺、地缘政治问题、价格上涨和气候变化影响迫使制造业开发更节能的解决方案。制造业作为主要能源消费者，需要快速部署且能立即见效的节能调度方法。

Method: 首先构建了新颖的多目标混合整数规划模型，采用增强的ε约束方法寻找帕累托最优解。同时开发了改进的迭代帕累托贪婪算法来处理大规模实例。

Result: 通过小、中、大规模实例对提出的方法进行基准测试，并与两种知名算法进行比较，计算结果显示所提方法具有显著有效性。

Conclusion: 提出的多目标优化方法和元启发式算法能够有效解决混合流水车间调度中的节能问题，在合理时间内为大规模实例提供高质量解决方案。

Abstract: The scarcity of non-renewable energy sources, geopolitical problems in its
supply, increasing prices, and the impact of climate change, force the global
economy to develop more energy-efficient solutions for their operations. The
Manufacturing sector is not excluded from this challenge as one of the largest
consumers of energy. Energy-efficient scheduling is a method that attracts
manufacturing companies to reduce their consumption as it can be quickly
deployed and can show impact immediately. In this study, the hybrid flow shop
scheduling problem with blocking constraint (BHFS) is investigated in which we
seek to minimize the latest completion time (i.e. makespan) and overall energy
consumption, a typical manufacturing setting across many industries from
automotive to pharmaceutical. Energy consumption and the latest completion time
of customer orders are usually conflicting objectives. Therefore, we first
formulate the problem as a novel multi-objective mixed integer programming
(MIP) model and propose an augmented epsilon-constraint method for finding the
Pareto-optimal solutions. Also, an effective multi-objective metaheuristic
algorithm. Refined Iterated Pareto Greedy (RIPG), is developed to solve large
instances in reasonable time. Our proposed methods are benchmarked using small,
medium, and large-size instances to evaluate their efficiency. Two well-known
algorithms are adopted for comparing our novel approaches. The computational
results show the effectiveness of our method.

</details>


### [89] [Know Thyself? On the Incapability and Implications of AI Self-Recognition](https://arxiv.org/abs/2510.03399)
*Xiaoyan Bai,Aryan Shrivastava,Ari Holtzman,Chenhao Tan*

Main category: cs.AI

TL;DR: 本文系统评估了10个当代大语言模型的自识别能力，发现模型普遍存在自识别失败，仅有4个模型能正确识别自身生成文本，且性能很少超过随机水平。模型还表现出对GPT和Claude家族的强烈偏见。


<details>
  <summary>Details</summary>
Motivation: 针对AI系统是否具备自识别能力这一存在矛盾解释的问题，开发一个可轻松应用和更新的系统评估框架，这对于AI安全特别是评估场景至关重要。

Method: 通过二元自识别和精确模型预测两个任务，测量10个当代大语言模型识别自身生成文本与其他模型文本的能力。

Result: 结果显示一致的自识别失败：仅4/10模型能预测自身为生成者，性能很少超过随机机会。模型表现出对GPT和Claude家族的强烈偏见，并揭示了层次偏见推理模式。

Conclusion: 研究结果对AI安全具有重要意义，需要开发适当的AI自我意识，模型虽然表现出对自身和其他模型存在的某种认知，但推理中存在层次偏见。

Abstract: Self-recognition is a crucial metacognitive capability for AI systems,
relevant not only for psychological analysis but also for safety, particularly
in evaluative scenarios. Motivated by contradictory interpretations of whether
models possess self-recognition (Panickssery et al., 2024; Davidson et al.,
2024), we introduce a systematic evaluation framework that can be easily
applied and updated. Specifically, we measure how well 10 contemporary larger
language models (LLMs) can identify their own generated text versus text from
other models through two tasks: binary self-recognition and exact model
prediction. Different from prior claims, our results reveal a consistent
failure in self-recognition. Only 4 out of 10 models predict themselves as
generators, and the performance is rarely above random chance. Additionally,
models exhibit a strong bias toward predicting GPT and Claude families. We also
provide the first evaluation of model awareness of their own and others'
existence, as well as the reasoning behind their choices in self-recognition.
We find that the model demonstrates some knowledge of its own existence and
other models, but their reasoning reveals a hierarchical bias. They appear to
assume that GPT, Claude, and occasionally Gemini are the top-tier models, often
associating high-quality text with them. We conclude by discussing the
implications of our findings on AI safety and future directions to develop
appropriate AI self-awareness.

</details>


### [90] [ContraGen: A Multi-Agent Generation Framework for Enterprise Contradictions Detection](https://arxiv.org/abs/2510.03418)
*Ananya Mantravadi,Shivali Dalmia,Abhishek Mukherji,Nand Dave,Anudha Mittal*

Main category: cs.AI

TL;DR: 提出了ContraGen框架，专门针对企业领域构建矛盾检测基准，通过生成包含矛盾的企业风格文档来系统评估文档内和跨文档一致性。


<details>
  <summary>Details</summary>
Motivation: 现有矛盾检测基准仅限于句子级别分析，无法捕捉企业文档（如合同、财务报告、合规文件）的复杂性，导致RAG系统在企业环境中产生不一致或不可靠的输出。

Method: 结合自动化矛盾挖掘和人工验证，生成合成企业风格文档，建立企业流程中常见矛盾类型的分类法，开发矛盾感知的检索评估流程。

Result: 构建了专门针对企业领域的矛盾检测基准框架，能够系统评估文档内和跨文档的一致性。

Conclusion: 这项工作为企业信息检索应用中更可信和负责任的RAG系统奠定了基础，其中检测和解决矛盾对于降低风险和确保合规性至关重要。

Abstract: Retrieval-Augmented Generation (RAG) integrates LLMs with external sources,
offering advanced capabilities for information access and decision-making.
However, contradictions in retrieved evidence can result in inconsistent or
untrustworthy outputs, which is especially problematic in enterprise settings
where compliance, governance, and accountability are critical. Existing
benchmarks for contradiction detection are limited to sentence-level analysis
and do not capture the complexity of enterprise documents such as contracts,
financial filings, compliance reports, or policy manuals. To address this
limitation, we propose ContraGen, a contradiction-aware benchmark framework
tailored to enterprise domain. The framework generates synthetic
enterprise-style documents with embedded contradictions, enabling systematic
evaluation of both intra-document and cross-document consistency. Automated
contradiction mining is combined with human-in-the-loop validation to ensure
high accuracy. Our contributions include generating realistic enterprise
documents, modeling a taxonomy of contradiction types common in business
processes, enabling controlled creation of self- and pairwise contradictions,
developing a contradiction-aware retrieval evaluation pipeline and embedding
human oversight to reflect domain-specific judgment complexity. This work
establishes a foundation for more trustworthy and accountable RAG systems in
enterprise information-seeking applications, where detecting and resolving
contradictions is essential for reducing risk and ensuring compliance.

</details>


### [91] [A Qualitative Comparative Evaluation of Cognitive and Generative Theories](https://arxiv.org/abs/2510.03453)
*Paul S. Rosenbloom*

Main category: cs.AI

TL;DR: 该论文提出了一种评估认知架构和生成式神经架构理论的定性比较方法，以应对这两种架构在理论评估方面面临的挑战。


<details>
  <summary>Details</summary>
Motivation: 认知架构理论和生成式神经架构理论在评估方面都面临挑战，需要一种更广泛的评估视角来进行比较分析。

Method: 采用广泛的定性比较方法，对面向全脑的认知架构和生成式架构及其完整系统进行全面比较。

Result: 通过这种定性比较方法，能够对不同类型的架构理论进行更全面的评估。

Conclusion: 利用广泛的理论评估视角，可以对认知架构和生成式神经架构进行有效的定性比较，克服传统评估方法的局限性。

Abstract: Evaluation is a critical activity associated with any theory. Yet this has
proven to be an exceptionally challenging activity for theories based on
cognitive architectures. For an overlapping set of reasons, evaluation can also
be challenging for theories based on generative neural architectures. This dual
challenge is approached here by leveraging a broad perspective on theory
evaluation to yield a wide-ranging, albeit qualitative, comparison of
whole-mind-oriented cognitive and generative architectures and the full systems
that are based on these architectures.

</details>


### [92] [Bridging LLM Planning Agents and Formal Methods: A Case Study in Plan Verification](https://arxiv.org/abs/2510.03469)
*Keshav Ramani,Vali Tawosi,Salwa Alamir,Daniel Borrajo*

Main category: cs.AI

TL;DR: 提出了一种通过将自然语言计划转换为Kripke结构和LTL公式来评估计划对齐性的新框架，使用LLM进行模型检查，在PlanBench数据集上验证效果


<details>
  <summary>Details</summary>
Motivation: 需要评估自然语言计划与其预期行为之间的对齐性，传统方法难以处理自然语言的复杂性

Method: 使用大型语言模型将自然语言计划转换为Kripke结构和线性时序逻辑公式，然后进行模型检查

Result: GPT-5在分类任务上表现出色（F1得分96.3%），几乎总能生成语法完美的形式化表示

Conclusion: 框架在生成语法正确的形式化保证方面有效，但语义完美形式模型的合成仍需进一步探索

Abstract: We introduce a novel framework for evaluating the alignment between natural
language plans and their expected behavior by converting them into Kripke
structures and Linear Temporal Logic (LTL) using Large Language Models (LLMs)
and performing model checking. We systematically evaluate this framework on a
simplified version of the PlanBench plan verification dataset and report on
metrics like Accuracy, Precision, Recall and F1 scores. Our experiments
demonstrate that GPT-5 achieves excellent classification performance (F1 score
of 96.3%) while almost always producing syntactically perfect formal
representations that can act as guarantees. However, the synthesis of
semantically perfect formal models remains an area for future exploration.

</details>


### [93] [Towards Policy-Compliant Agents: Learning Efficient Guardrails For Policy Violation Detection](https://arxiv.org/abs/2510.03485)
*Xiaofei Wen,Wenjie Jacky Mo,Yanan Xie,Peng Qi,Muhao Chen*

Main category: cs.AI

TL;DR: 提出了PolicyGuardBench基准和PolicyGuard-4B模型，用于检测网络代理轨迹中的策略违规，支持跨域泛化和小规模高效推理。


<details>
  <summary>Details</summary>
Motivation: 自主网络代理需要在外部策略约束下生成长期轨迹，但现有研究很少检验这些轨迹是否符合策略，以及策略违规是否在不同上下文（如不同领域和子领域）中持续存在。

Method: 从多样化代理运行中生成广泛策略集，创建包含违规标签的子域内和跨子域配对；构建约60k样本的基准，包括完整轨迹评估和基于前缀的违规检测任务；训练轻量级PolicyGuard-4B护栏模型。

Result: PolicyGuard-4B在所有任务上提供强大的检测准确性，同时保持推理效率；在未见过的设置上保持高准确性，能够跨域泛化。

Conclusion: PolicyGuardBench和PolicyGuard-4B为研究网络代理轨迹中的策略合规性提供了首个综合框架，证明在小规模下实现准确且可泛化的护栏是可行的。

Abstract: Autonomous web agents need to operate under externally imposed or
human-specified policies while generating long-horizon trajectories. However,
little work has examined whether these trajectories comply with such policies,
or whether policy violations persist across different contexts such as domains
(e.g., shopping or coding websites) and subdomains (e.g., product search and
order management in shopping). To address this gap, we introduce
PolicyGuardBench, a benchmark of about 60k examples for detecting policy
violations in agent trajectories. From diverse agent runs, we generate a broad
set of policies and create both within subdomain and cross subdomain pairings
with violation labels. In addition to full-trajectory evaluation,
PolicyGuardBench also includes a prefix-based violation detection task where
models must anticipate policy violations from truncated trajectory prefixes
rather than complete sequences. Using this dataset, we train PolicyGuard-4B, a
lightweight guardrail model that delivers strong detection accuracy across all
tasks while keeping inference efficient. Notably, PolicyGuard-4B generalizes
across domains and preserves high accuracy on unseen settings. Together,
PolicyGuardBench and PolicyGuard-4B provide the first comprehensive framework
for studying policy compliance in web agent trajectories, and show that
accurate and generalizable guardrails are feasible at small scales.

</details>


### [94] [OneFlow: Concurrent Mixed-Modal and Interleaved Generation with Edit Flows](https://arxiv.org/abs/2510.03506)
*John Nguyen,Marton Havasi,Tariq Berrada,Luke Zettlemoyer,Ricky T. Q. Chen*

Main category: cs.AI

TL;DR: OneFlow是首个非自回归多模态模型，支持变长和并发混合模态生成，通过插入式编辑流和流匹配技术，在生成质量和效率上超越自回归模型。


<details>
  <summary>Details</summary>
Motivation: 解决自回归模型在文本和图像生成中强制因果顺序的限制，实现更灵活的并发多模态生成。

Method: 结合基于插入的编辑流处理离散文本标记，使用流匹配处理图像潜在表示，采用分层采样优先内容而非语法。

Result: 在1B到8B模型规模上，OneFlow在生成和理解任务上均优于自回归基线，训练FLOPs减少达50%，超越自回归和基于扩散的方法。

Conclusion: OneFlow解锁了并发生成、迭代优化和类自然推理等新能力，为非自回归多模态生成提供了有效解决方案。

Abstract: We present OneFlow, the first non-autoregressive multimodal model that
enables variable-length and concurrent mixed-modal generation. Unlike
autoregressive models that enforce rigid causal ordering between text and image
generation, OneFlow combines an insertion-based Edit Flow for discrete text
tokens with Flow Matching for image latents. OneFlow enables concurrent
text-image synthesis with hierarchical sampling that prioritizes content over
grammar. Through controlled experiments across model sizes from 1B to 8B, we
demonstrate that OneFlow outperforms autoregressive baselines on both
generation and understanding tasks while using up to 50% fewer training FLOPs.
OneFlow surpasses both autoregressive and diffusion-based approaches while
unlocking new capabilities for concurrent generation, iterative refinement, and
natural reasoning-like generation.

</details>


### [95] [Understanding the Role of Training Data in Test-Time Scaling](https://arxiv.org/abs/2510.03605)
*Adel Javanmard,Baharan Mirzasoleiman,Vahab Mirrokni*

Main category: cs.AI

TL;DR: 本文研究了测试时扩展（test-time scaling）对Transformer模型推理能力的影响，发现在线性回归任务中，增加测试时计算量可以减少训练所需的上下文长度，但前提是训练数据包含足够的相关技能。


<details>
  <summary>Details</summary>
Motivation: 尽管测试时扩展（如OpenAI的o1和DeepSeek R1）通过生成长思维链显著提升了LLMs的推理能力，但长思维链在什么训练条件下出现以及何时能真正提升性能仍不清楚。

Method: 通过在上下文权重预测的线性回归任务上训练Transformer模型，分析测试时扩展的性能表现，并建立理论框架来解释观察到的现象。

Result: 研究发现：1）在固定测试误差下，增加测试时计算可以减少训练所需的上下文长度；2）如果训练数据缺乏解决下游任务所需的技能，增加测试时计算反而会损害性能；3）任务难度可通过特征协方差矩阵的最小特征值来表征。

Conclusion: 训练数据的多样性、相关性和难度是测试时扩展成功的关键因素，在相关且困难的任务集上训练能获得最佳性能，这一发现在非线性Transformer架构的实验中得到验证。

Abstract: Test-time scaling improves the reasoning capabilities of large language
models (LLMs) by allocating extra compute to generate longer Chains-of-Thoughts
(CoTs). This enables models to tackle more complex problem by breaking them
down into additional steps, backtracking, and correcting mistakes. Despite its
strong performance--demonstrated by OpenAI's o1 and DeepSeek R1, the conditions
in the training data under which long CoTs emerge, and when such long CoTs
improve the performance, remain unclear. In this paper, we study the
performance of test-time scaling for transformers trained on an in-context
weight prediction task for linear regression. Our analysis provides a
theoretical explanation for several intriguing observations: First, at any
fixed test error, increasing test-time compute allows us to reduce the number
of in-context examples (context length) in training prompts. Second, if the
skills required to solve a downstream task are not sufficiently present in the
training data, increasing test-time compute can harm performance. Finally, we
characterize task hardness via the smallest eigenvalue of its feature
covariance matrix and show that training on a diverse, relevant, and hard set
of tasks results in best performance for test-time scaling. We confirm our
findings with experiments on large, nonlinear transformer architectures.

</details>


### [96] [Cross-Modal Content Optimization for Steering Web Agent Preferences](https://arxiv.org/abs/2510.03612)
*Tanqiu Jiang,Min Bai,Nikolaos Pappas,Yanjun Qi,Sandesh Swamy*

Main category: cs.AI

TL;DR: 提出了一种名为跨模态偏好引导(CPS)的攻击方法，通过联合优化视觉和文本通道的不可察觉修改，在现实的黑盒威胁环境下有效操纵基于视觉语言模型的网络代理的选择决策。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明基于视觉语言模型的网络代理容易受到偏好操纵攻击，但现有方法要么假设白盒访问权限，要么使用不切实际的设置。本文旨在在现实的黑盒威胁环境下开发更强大的跨模态攻击方法。

Method: 引入跨模态偏好引导(CPS)方法，联合优化商品视觉和自然语言描述的不可察觉修改，利用CLIP可迁移图像扰动和RLHF诱导的语言偏见来引导代理决策。攻击者只能编辑自己列表的图像和文本元数据，无法访问代理模型内部。

Result: 在GPT-4.1、Qwen-2.5VL和Pixtral-Large等最先进模型上的评估显示，CPS在所有模型上都显著优于基线方法，同时保持70%更低的检测率，证明了其有效性和隐蔽性。

Conclusion: 这些发现强调了随着智能系统在社会中扮演越来越重要的角色，迫切需要开发鲁棒的防御机制来应对此类跨模态攻击。

Abstract: Vision-language model (VLM)-based web agents increasingly power high-stakes
selection tasks like content recommendation or product ranking by combining
multimodal perception with preference reasoning. Recent studies reveal that
these agents are vulnerable against attackers who can bias selection outcomes
through preference manipulations using adversarial pop-ups, image
perturbations, or content tweaks. Existing work, however, either assumes strong
white-box access, with limited single-modal perturbations, or uses impractical
settings. In this paper, we demonstrate, for the first time, that joint
exploitation of visual and textual channels yields significantly more powerful
preference manipulations under realistic attacker capabilities. We introduce
Cross-Modal Preference Steering (CPS) that jointly optimizes imperceptible
modifications to an item's visual and natural language descriptions, exploiting
CLIP-transferable image perturbations and RLHF-induced linguistic biases to
steer agent decisions. In contrast to prior studies that assume gradient
access, or control over webpages, or agent memory, we adopt a realistic
black-box threat setup: a non-privileged adversary can edit only their own
listing's images and textual metadata, with no insight into the agent's model
internals. We evaluate CPS on agents powered by state-of-the-art proprietary
and open source VLMs including GPT-4.1, Qwen-2.5VL and Pixtral-Large on both
movie selection and e-commerce tasks. Our results show that CPS is
significantly more effective than leading baseline methods. For instance, our
results show that CPS consistently outperforms baselines across all models
while maintaining 70% lower detection rates, demonstrating both effectiveness
and stealth. These findings highlight an urgent need for robust defenses as
agentic systems play an increasingly consequential role in society.

</details>


### [97] [MITS: Enhanced Tree Search Reasoning for LLMs via Pointwise Mutual Information](https://arxiv.org/abs/2510.03632)
*Jiaxi Li,Yucheng Shi,Jin Lu,Ninghao Liu*

Main category: cs.AI

TL;DR: 提出了基于互信息的树搜索框架MITS，通过点互信息评分函数和基于熵的动态采样策略，在保持计算效率的同时提升大语言模型的推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有树搜索方法难以对中间推理步骤进行即时可靠的定量评估，且广泛路径探索计算成本高昂。

Method: 使用点互信息作为评分函数进行路径评估和束搜索扩展，结合基于熵的动态采样策略自适应分配计算资源，最后采用加权投票方案结合PMI分数和预测共识。

Result: 在多样化推理基准测试中，MITS始终优于基线方法。

Conclusion: MITS为LLM推理建立了一个原则性且高效的框架。

Abstract: Tree search has become as a representative framework for test-time reasoning
with large language models (LLMs), exemplified by methods such as
Tree-of-Thought and Monte Carlo Tree Search that explore multiple reasoning
paths. However, it remains difficult to provide instant and reliable
quantitative assessments of intermediate reasoning step quality, and extensive
path exploration is computationally costly. To address this, we propose Mutual
Information Tree Search (MITS), a novel framework that guides reasoning with
information-theoretic principles. MITS introduces an effective scoring function
based on pointwise mutual information (PMI), which enables step-wise evaluation
of reasoning paths and search tree expansion via beam search without expensive
look-ahead simulations, achieving superior reasoning performances while
maintaining computational efficiency. The framework is complemented by an
entropy-based dynamic sampling strategy that adaptively allocates computational
resources to uncertain reasoning steps where exploration is most beneficial.
For final prediction, MITS employs a weighted voting scheme that combines PMI
scores with prediction consensus. Through comprehensive experiments on diverse
reasoning benchmarks, MITS consistently surpasses baseline methods,
establishing a principled and efficient framework for LLM reasoning.

</details>


### [98] [Rainbow Padding: Mitigating Early Termination in Instruction-Tuned Diffusion LLMs](https://arxiv.org/abs/2510.03680)
*Bumjun Kim,Dongjae Jeon,Dueun Kim,Wonje Jeung,Albert No*

Main category: cs.AI

TL;DR: 扩散大语言模型在指令调优后存在<eos>溢出问题：随着序列长度增加，响应反而变短，导致提前终止或退化为<eos>令牌流。作者提出彩虹填充方法，用循环的不同填充令牌替代重复的<eos>占位符，有效解决了这一问题。


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型作为自回归模型的替代方案，在复杂推理任务上表现优异，但指令调优后的模型存在<eos>溢出漏洞，这个问题在实践中已被注意到但缺乏系统分析。

Method: 提出彩虹填充方法，用循环的不同填充令牌替代重复的<eos>占位符，分散概率质量，打破<eos>的主导地位。通过LoRA微调在少量数据上训练单个周期即可实现显著改进。

Result: 彩虹填充显著提高了长度鲁棒性和输出质量，仅需7个填充令牌即可防止提前终止。该方法能高效集成到现有指令调优模型中，具有高度实用性。

Conclusion: 彩虹填充是解决扩散大语言模型<eos>溢出问题的简单有效方法，通过改变填充策略从根本上解决了模型在长序列生成中的退化问题。

Abstract: Diffusion large language models (dLLMs) have emerged as a promising
alternative to autoregressive models, offering flexible generation orders and
strong performance on complex reasoning tasks. However, instruction-tuned dLLMs
exhibit a critical vulnerability we term \texttt{<eos>} overflow: as allocated
sequence length increases, responses paradoxically become shorter, collapsing
into early termination or degenerating into streams of \texttt{<eos>} tokens.
Although noticed in practice, this issue has not been systematically analyzed.
We trace its root cause to the dual role of \texttt{<eos>} as both termination
and padding, which concentrates probability mass on \texttt{<eos>} at later
positions and propagates backward to trigger early termination. To address
this, we introduce Rainbow Padding, a simple remedy that replaces repeated
\texttt{<eos>} placeholders with a repeating cycle of distinct padding tokens,
distributing probability mass and breaking \texttt{<eos>} dominance.
Experiments show that Rainbow Padding substantially improves length robustness
and output quality, with as few as seven padding tokens sufficient to prevent
early termination. Moreover, the method integrates efficiently into existing
instruction-tuned models: LoRA fine-tuning for a single epoch on minimal data
yields significant improvements, making this solution highly practical. The
code is publicly available at https://github.com/quasar529/rainbow-padding.

</details>


### [99] [Mind the Goal: Data-Efficient Goal-Oriented Evaluation of Conversational Agents and Chatbots using Teacher Models](https://arxiv.org/abs/2510.03696)
*Deepak Babu Piskala,Sharlene Chen,Udita Patel,Parul Kalra,Rafael Castrillo*

Main category: cs.AI

TL;DR: 提出了一个面向目标的评估框架，用于评估多智能体系统的对话质量，引入目标成功率(GSR)和失败根因分类(RCOF)，通过模型驱动的评估系统提供可解释的评估结果。


<details>
  <summary>Details</summary>
Motivation: 现有的多轮聊天机器人评估方法通常在轮次层面评估，未能解决用户总体目标是否实现的问题，需要更全面的目标导向评估框架。

Method: 提出基于模型的目标导向评估框架，使用教师LLM结合领域专家定义的目标和质量标准，通过"思考标记"产生可解释的推理过程，对对话按用户目标分段评估。

Result: 在企业环境中应用该框架评估AIDA员工对话系统，观察到目标成功率从63%提升到79%。

Conclusion: 该框架具有通用性，通过详细的缺陷分类提供可操作的见解，能够诊断整体成功率、识别关键失败模式并指导系统改进。

Abstract: Evaluating the quality of multi-turn chatbot interactions remains
challenging, as most existing methods assess interactions at the turn level
without addressing whether a user's overarching goal was fulfilled. A ``goal''
here refers to an information need or task, such as asking for policy
information or applying for leave. We propose a comprehensive framework for
goal-oriented evaluation of multi-agent systems (MAS), introducing the
\textbf{Goal Success Rate (GSR)} to measure the percentage of fulfilled goals,
and a \textbf{Root Cause of Failure (RCOF)} taxonomy to identify reasons for
failure in multi-agent chatbots. Our method segments conversations by user
goals and evaluates success using all relevant turns. We present a model-based
evaluation system combining teacher LLMs, where domain experts define goals,
set quality standards serving as a guidance for the LLMs. The LLMs use
``thinking tokens'' to produce interpretable rationales, enabling
\textit{explainable}, \textit{data-efficient} evaluations. In an enterprise
setting, we apply our framework to evaluate AIDA, a zero-to-one employee
conversational agent system built as a ground-up multi-agent conversational
agent, and observe GSR improvement from 63\% to 79\% over six months since its
inception. Our framework is generic and offers actionable insights through a
detailed defect taxonomy based on analysis of failure points in multi-agent
chatbots, diagnosing overall success, identifying key failure modes, and
informing system improvements.

</details>


### [100] [H-DDx: A Hierarchical Evaluation Framework for Differential Diagnosis](https://arxiv.org/abs/2510.03700)
*Seungseop Lim,Gibaeg Kim,Hyunkyung Lee,Wooseok Han,Jean Seo,Jaehyo Yoo,Eunho Yang*

Main category: cs.AI

TL;DR: 提出了H-DDx分层评估框架，用于更准确地评估大语言模型在医学鉴别诊断中的表现，克服了传统扁平指标的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型在医学鉴别诊断评估中主要依赖扁平指标（如Top-k准确率），这些指标无法区分临床相关的近似错误和诊断上相距甚远的错误，需要更符合临床相关性的评估方法。

Method: 开发了H-DDx框架，采用检索和重排序流程将自由文本诊断映射到ICD-10代码，并应用分层度量方法，对与真实诊断密切相关的预测给予认可。

Result: 在22个领先模型的基准测试中，传统扁平指标低估了性能，忽略了临床有意义的输出；领域专业化的开源模型表现突出；框架增强了可解释性，揭示了大语言模型即使错过精确诊断也能正确识别更广泛临床背景的模式。

Conclusion: H-DDx框架提供了更准确、临床相关的评估方法，显示了大语言模型在医学鉴别诊断中的真实能力，并为理解模型错误模式提供了新视角。

Abstract: An accurate differential diagnosis (DDx) is essential for patient care,
shaping therapeutic decisions and influencing outcomes. Recently, Large
Language Models (LLMs) have emerged as promising tools to support this process
by generating a DDx list from patient narratives. However, existing evaluations
of LLMs in this domain primarily rely on flat metrics, such as Top-k accuracy,
which fail to distinguish between clinically relevant near-misses and
diagnostically distant errors. To mitigate this limitation, we introduce H-DDx,
a hierarchical evaluation framework that better reflects clinical relevance.
H-DDx leverages a retrieval and reranking pipeline to map free-text diagnoses
to ICD-10 codes and applies a hierarchical metric that credits predictions
closely related to the ground-truth diagnosis. In benchmarking 22 leading
models, we show that conventional flat metrics underestimate performance by
overlooking clinically meaningful outputs, with our results highlighting the
strengths of domain-specialized open-source models. Furthermore, our framework
enhances interpretability by revealing hierarchical error patterns,
demonstrating that LLMs often correctly identify the broader clinical context
even when the precise diagnosis is missed.

</details>


### [101] [Bridging the Gap Between Multimodal Foundation Models and World Models](https://arxiv.org/abs/2510.03727)
*Xuehai He*

Main category: cs.AI

TL;DR: 该论文研究如何将多模态基础模型提升为世界模型，通过增强推理能力和可控生成能力，使其能够进行反事实推理、时空理解和可控生成。


<details>
  <summary>Details</summary>
Motivation: 人类通过多感官整合理解世界，而当前的多模态基础模型缺乏作为世界模型的关键能力，如反事实推理、动态模拟、时空信息理解和可控生成。

Method: 通过判别任务提升推理能力，引入因果推理、反事实思维和时空推理等结构化推理技能；开发结构化可控生成框架，结合场景图、多模态条件和多模态对齐策略。

Result: 实现了多模态基础模型在图像和视频模态上的可控生成能力，支持可交互、可编辑和可变形的4D对象合成。

Conclusion: 通过增强推理能力和可控生成技术，多模态基础模型能够更好地模拟和理解物理世界，向真正的世界模型迈进。

Abstract: Humans understand the world through the integration of multiple sensory
modalities, enabling them to perceive, reason about, and imagine dynamic
physical processes. Inspired by this capability, multimodal foundation models
(MFMs) have emerged as powerful tools for multimodal understanding and
generation. However, today's MFMs fall short of serving as effective world
models. They lack the essential ability such as perform counterfactual
reasoning, simulate dynamics, understand the spatiotemporal information,
control generated visual outcomes, and perform multifaceted reasoning. We
investigates what it takes to bridge the gap between multimodal foundation
models and world models. We begin by improving the reasoning capabilities of
MFMs through discriminative tasks and equipping MFMs with structured reasoning
skills, such as causal inference, counterfactual thinking, and spatiotemporal
reasoning, enabling them to go beyond surface correlations and understand
deeper relationships within visual and textual data. Next, we explore
generative capabilities of multimodal foundation models across both image and
video modalities, introducing new frameworks for structured and controllable
generation. Our approaches incorporate scene graphs, multimodal conditioning,
and multimodal alignment strategies to guide the generation process, ensuring
consistency with high-level semantics and fine-grained user intent. We further
extend these techniques to controllable 4D generation, enabling interactive,
editable, and morphable object synthesis over time and space.

</details>


### [102] [OptAgent: Optimizing Query Rewriting for E-commerce via Multi-Agent Simulation](https://arxiv.org/abs/2510.03771)
*Divij Handa,David Blincoe,Orson Adams,Yinlin Fu*

Main category: cs.AI

TL;DR: 提出了OptAgent框架，结合多智能体模拟和遗传算法来优化电商查询改写任务，解决了主观任务中缺乏黄金标准答案的评估难题。


<details>
  <summary>Details</summary>
Motivation: LLM在可验证任务中表现优异，但在主观任务如电商查询改写中，由于缺乏单一正确答案，评估和优化变得困难。

Method: 使用多个基于LLM的智能体模拟购物顾客作为动态奖励信号，其平均得分作为遗传算法的适应度函数，迭代优化用户初始查询。

Result: 在1000个真实电商查询数据集上测试，相比原始查询平均提升21.98%，相比Best-of-N LLM改写基线提升3.36%。

Conclusion: OptAgent框架有效解决了主观任务的评估和优化问题，为电商查询改写提供了可靠的解决方案。

Abstract: Deploying capable and user-aligned LLM-based systems necessitates reliable
evaluation. While LLMs excel in verifiable tasks like coding and mathematics,
where gold-standard solutions are available, adoption remains challenging for
subjective tasks that lack a single correct answer. E-commerce Query Rewriting
(QR) is one such problem where determining whether a rewritten query properly
captures the user intent is extremely difficult to figure out algorithmically.
In this work, we introduce OptAgent, a novel framework that combines
multi-agent simulations with genetic algorithms to verify and optimize queries
for QR. Instead of relying on a static reward model or a single LLM judge, our
approach uses multiple LLM-based agents, each acting as a simulated shopping
customer, as a dynamic reward signal. The average of these agent-derived scores
serves as an effective fitness function for an evolutionary algorithm that
iteratively refines the user's initial query. We evaluate OptAgent on a dataset
of 1000 real-world e-commerce queries in five different categories, and we
observe an average improvement of 21.98% over the original user query and 3.36%
over a Best-of-N LLM rewriting baseline.

</details>


### [103] [GuidedSampling: Steering LLMs Towards Diverse Candidate Solutions at Inference-Time](https://arxiv.org/abs/2510.03777)
*Divij Handa,Mihir Parmar,Aswin RRV,Md Nayem Uddin,Hamid Palangi,Chitta Baral*

Main category: cs.AI

TL;DR: 提出了一种新的推理算法GuidedSampling，通过分离探索和生成阶段来增加候选解决方案的多样性，相比传统重复采样方法在多个基准测试中显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 传统重复采样方法在推理时虽然能提升模型性能，但经常生成冗余样本，缺乏多样性，无法利用多种不同概念来解决问题。

Method: GuidedSampling算法将推理过程分为探索阶段和生成阶段：探索阶段识别可用于解决问题的多个概念，生成阶段应用特定概念提供最终解决方案候选。

Result: 在pass@50指标上平均提升21.6%，使用GuidedSampling训练的模型在pass@5指标上平均提升9.7%，每个实例的平均概念数量从1.67增加到3.03。

Conclusion: GuidedSampling通过增加解决方案的多样性有效提升了模型性能，并且训练出的模型能够产生更多样化的候选解决方案。

Abstract: Repeated Sampling (RS) is a simple inference-time algorithm that has been
shown to improve model performance on complex tasks. Although it is an
effective way of scaling inference time, it often struggles to generate diverse
solution candidates, frequently relying on the same underlying approach to
solve the problem and thus producing redundant samples. To address this
limitation, we propose a new inference algorithm, GuidedSampling, which
decouples the exploration and generation phases during inference, increasing
diversity of generated candidate solutions. The exploration phase identifies
multiple concepts that can be utilized to solve the problem, while the
generation phase applies a specific concept to provide final solution
candidates. We first define the theoretical bounds of GuidedSampling and then
empirically demonstrate that it improves the performance of base model at
pass@50 by on an average ~21.6% across various benchmarks compared to RS.
Furthermore, models trained on trajectories of GuidedSampling exhibit
substantial performance improvements at pass@5 by on an average ~9.7%, compared
to models trained on traditional RS. Additionally, models trained with
GuidedSampling increases the average number of concepts per instance (1.67 ->
3.03), yielding a diverse set of candidates than traditional RS.

</details>


### [104] [The Hidden Game Problem](https://arxiv.org/abs/2510.03845)
*Gon Buzaglo,Noah Golowich,Elad Hazan*

Main category: cs.AI

TL;DR: 该论文研究具有大策略空间的游戏，提出隐藏游戏问题，开发了能够发现并利用隐藏结构的遗憾最小化算法，实现隐藏子游戏中的快速收敛。


<details>
  <summary>Details</summary>
Motivation: 受AI对齐和语言游戏挑战的启发，研究当玩家存在未知策略子集持续产生更高奖励时的隐藏游戏问题，探索能否设计高效算法来发现和利用这种结构。

Method: 开发了一种遗憾最小化技术的组合方法，实现了最优的外部遗憾和交换遗憾界限，利用隐藏游戏结构提高计算效率。

Result: 算法能够快速收敛到隐藏子游戏中的相关均衡，同时保持一般情况下的理性，证明了隐藏结构可以被有效发现和利用。

Conclusion: 对隐藏游戏问题给出了肯定回答，证明了通过精心设计的遗憾最小化算法可以同时实现隐藏子游戏的均衡收敛和一般理性。

Abstract: This paper investigates a class of games with large strategy spaces,
motivated by challenges in AI alignment and language games. We introduce the
hidden game problem, where for each player, an unknown subset of strategies
consistently yields higher rewards compared to the rest. The central question
is whether efficient regret minimization algorithms can be designed to discover
and exploit such hidden structures, leading to equilibrium in these subgames
while maintaining rationality in general. We answer this question affirmatively
by developing a composition of regret minimization techniques that achieve
optimal external and swap regret bounds. Our approach ensures rapid convergence
to correlated equilibria in hidden subgames, leveraging the hidden game
structure for improved computational efficiency.

</details>


### [105] [Small Language Models for Agentic Systems: A Survey of Architectures, Capabilities, and Deployment Trade offs](https://arxiv.org/abs/2510.03847)
*Raghav Sharma,Manan Mehta*

Main category: cs.AI

TL;DR: 小语言模型（1-20B参数）在代理任务中优于大模型，通过引导解码和严格JSON Schema输出，能以10-100倍更低成本实现相似性能，同时提供更好的延迟和能效。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在代理任务中成本高、延迟大、能耗高的问题，证明小语言模型在结构化输出和API调用任务中的优越性。

Method: 采用引导解码、严格JSON Schema输出、验证器优先工具执行，构建SLM默认、LLM回退系统，包含不确定性感知路由和验证器级联。

Result: 小语言模型在工具使用、函数调用和RAG任务中能匹配或超越大模型，成本降低10-100倍，延迟和能效显著改善。

Conclusion: 提供了基于小语言模型的代理系统设计蓝图，在保持大模型回退能力的同时，实现快速、廉价、可靠的代理服务。

Abstract: Small language models (SLMs; 1-12B params, sometimes up to 20B) are
sufficient and often superior for agentic workloads where the objective is
schema- and API-constrained accuracy rather than open-ended generation. We
synthesize recent evidence across open and proprietary SLMs (Phi-4-Mini,
Qwen-2.5-7B, Gemma-2-9B, Llama-3.2-1B/3B, Ministral-3B/8B, Apple on-device 3B,
DeepSeek-R1-Distill) and connect it to modern evaluations (BFCL v3/v4,
StableToolBench) and serving stacks (vLLM, SGLang, TensorRT-LLM) paired with
guided decoding libraries (XGrammar, Outlines). We formalize SLM-default,
LLM-fallback systems with uncertainty-aware routing and verifier cascades, and
propose engineering metrics that reflect real production goals: cost per
successful task (CPS), schema validity rate, executable call rate, p50/p95
latency, and energy per request. Guided decoding, strict JSON Schema outputs,
and validator-first tool execution close much of the capability gap with larger
models and often let SLMs match or surpass LLMs on tool use, function calling,
and RAG at 10x-100x lower token cost with materially better latency and energy.
We provide design patterns for agent stacks that prioritize SLMs: schema-first
prompting, type-safe function registries, confidence scoring with verifier
rollups, and lightweight adaptation via LoRA/QLoRA. We also delineate limits
where fallback remains valuable (open-domain reasoning and some long-horizon
planning). The result is a practical blueprint for building fast, inexpensive,
and reliable agents that default to SLMs while preserving headroom with
targeted LLM assistance.
  Keywords: small language models, agents, function calling, structured
outputs, JSON Schema, guided decoding, LoRA/QLoRA, routing, energy efficiency,
edge inference

</details>


### [106] [Algorithm Generation via Creative Ideation](https://arxiv.org/abs/2510.03851)
*Ruiying Ma,Chieh-Jan Mike Liang,Yanjie Gao,Francis Y. Yan*

Main category: cs.AI

TL;DR: MetaMuse框架通过三个自我反思原则解决LLMs在算法生成中的局限性：在性能空间量化解决方案多样性、通过外部刺激引导构思、使用路径点推理构建可执行解决方案，显著提升了缓存替换和在线装箱问题的性能。


<details>
  <summary>Details</summary>
Motivation: 系统算法设计面临解空间不连续的挑战，现有LLMs倾向于生成通用启发式算法而非创新解决方案，需要克服这种偏见来导航不连续解空间。

Method: 提出MetaMuse框架，基于三个自我反思原则：(1)在可测量的性能空间而非抽象思想空间量化解决方案多样性和有用性；(2)通过外部刺激而非内部随机性引导构思；(3)使用路径点推理而非自由形式的思维链构建可执行解决方案。

Result: 在两个关键问题上取得显著性能提升：缓存替换减少缓存未命中达35.76%，在线装箱问题减少容器使用达30.93%。

Conclusion: MetaMuse框架能够有效克服LLMs在算法生成中的偏见，生成高性能的创造性解决方案，为系统算法设计提供了新方法。

Abstract: Designing system algorithms remains challenging, where the discontinuous
nature of the solution space often forces system engineers to rely on generic
heuristics at the expense of performance. We study whether LLMs can practically
drive algorithm generation, and find that they are biased towards well-known
generic designs, rather than making the creative leaps needed to navigate the
discontinuous solution space. To address this limitation, we introduce
MetaMuse, a framework for creative ideation built on three self-reflection
principles: (1) quantifying solution diversity and usefulness in measurable
performance space, rather than abstract idea space, (2) steering ideation
through external stimuli, rather than internal randomness, and (3) constructing
executable solutions using waypoint reasoning, rather than free-form
chain-of-thought. Extensive evaluation shows that MetaMuse can generate
high-performing solutions for two critical problems at a global cloud provider:
cache replacement (reducing cache misses by up to 35.76%) and online bin
packing (reducing bin usage by up to 30.93%).

</details>


### [107] [Adaptive and Explainable AI Agents for Anomaly Detection in Critical IoT Infrastructure using LLM-Enhanced Contextual Reasoning](https://arxiv.org/abs/2510.03859)
*Raghav Sharma,Manan Mehta*

Main category: cs.AI

TL;DR: 提出了一种结合LLM和XAI代理的异常检测方法，用于关键IoT系统，在智能电网和医疗场景中表现优于传统模型。


<details>
  <summary>Details</summary>
Motivation: 传统异常检测方法在动态、高维、数据不完整的IoT环境中存在局限性，需要自适应智能系统来提升检测能力。

Method: 使用LLM支持的上下文推理方法和XAI代理，结合注意力机制、内存缓冲和语义理解来发现隐藏模式和检测异常。

Result: 新方法在检测准确率、误报率、可解释性和响应速度方面均优于现有模型，特别是在真实场景模拟中表现突出。

Conclusion: LLM增强的异常检测方法在IoT环境中具有显著优势，为未来异常检测任务提供了可行的解决方案。

Abstract: Ensuring that critical IoT systems function safely and smoothly depends a lot
on finding anomalies quickly. As more complex systems, like smart healthcare,
energy grids and industrial automation, appear, it is easier to see the
shortcomings of older methods of detection. Monitoring failures usually happen
in dynamic, high dimensional situations, especially when data is incomplete,
messy or always evolving. Such limits point out the requirement for adaptive,
intelligent systems that always improve and think. LLMs are now capable of
significantly changing how context is understood and semantic inference is done
across all types of data. This proposal suggests using an LLM supported
contextual reasoning method along with XAI agents to improve how anomalies are
found in significant IoT environments. To discover hidden patterns and notice
inconsistencies in data streams, it uses attention methods, avoids dealing with
details from every time step and uses memory buffers with meaning. Because no
code AI stresses transparency and interpretability, people can check and accept
the AI's decisions, helping ensure AI follows company policies. The two
architectures are put together in a test that compares the results of the
traditional model with those of the suggested LLM enhanced model. Important
measures to check are the accuracy of detection, how much inaccurate
information is included in the results, how clearly the findings can be read
and how fast the system responds under different test situations. The
metaheuristic is tested in simulations of real world smart grid and healthcare
contexts to check its adaptability and reliability. From the study, we see that
the new approach performs much better than most existing models in both
accuracy and interpretation, so it could be a good fit for future anomaly
detection tasks in IoT

</details>


### [108] [Spatial CAPTCHA: Generatively Benchmarking Spatial Reasoning for Human-Machine Differentiation](https://arxiv.org/abs/2510.03863)
*Arina Kharlamova,Bowei He,Chen Ma,Xue Liu*

Main category: cs.AI

TL;DR: 提出Spatial CAPTCHA，一种基于空间推理的人类验证框架，利用人类与多模态大语言模型在空间推理能力上的根本差异来防御自动化攻击。


<details>
  <summary>Details</summary>
Motivation: 传统CAPTCHA依赖文本识别或2D图像理解，但多模态大语言模型的进步已削弱其有效性，需要新的验证机制。

Method: 通过程序化生成需要几何推理、视角转换、遮挡处理和心理旋转的动态问题，采用基于约束的难度控制、自动正确性验证和人机验证循环。

Result: 在Spatial-CAPTCHA-Bench基准测试中，人类表现远超10个最先进MLLM模型，最佳模型仅达到31.0% Pass@1准确率。与Google reCAPTCHA比较证实其有效性。

Conclusion: Spatial CAPTCHA不仅作为安全机制有效，还可作为AI空间推理能力的诊断工具，解决了传统CAPTCHA在现代AI面前的脆弱性问题。

Abstract: Online services rely on CAPTCHAs as a first line of defense against automated
abuse, yet recent advances in multi-modal large language models (MLLMs) have
eroded the effectiveness of conventional designs that focus on text recognition
or 2D image understanding. To address this challenge, we present Spatial
CAPTCHA, a novel human-verification framework that leverages fundamental
differences in spatial reasoning between humans and MLLMs. Unlike existing
CAPTCHAs which rely on low-level perception tasks that are vulnerable to modern
AI, Spatial CAPTCHA generates dynamic questions requiring geometric reasoning,
perspective-taking, occlusion handling, and mental rotation. These skills are
intuitive for humans but difficult for state-of-the-art (SOTA) AI systems. The
system employs a procedural generation pipeline with constraint-based
difficulty control, automated correctness verification, and human-in-the-loop
validation to ensure scalability, robustness, and adaptability. Evaluation on a
corresponding benchmark, Spatial-CAPTCHA-Bench, demonstrates that humans vastly
outperform 10 state-of-the-art MLLMs, with the best model achieving only 31.0%
Pass@1 accuracy. Furthermore, we compare Spatial CAPTCHA with Google reCAPTCHA,
which confirms its effectiveness as both a security mechanism and a diagnostic
tool for spatial reasoning in AI.

</details>


### [109] [Rare Text Semantics Were Always There in Your Diffusion Transformer](https://arxiv.org/abs/2510.03886)
*Seil Kang,Woojung Han,Dayun Ju,Seong Jae Hwang*

Main category: cs.AI

TL;DR: 提出一种无需额外训练或外部模块的方法，通过扩大文本标记嵌入的表示空间来增强多模态扩散变换器对稀有语义的生成能力。


<details>
  <summary>Details</summary>
Motivation: 现有先进的多模态扩散变换器在处理想象力丰富或罕见的提示词时仍然表现不佳，因为这些概念在预训练中过于稀缺，难以形成强印象。

Method: 通过数学方法在联合注意力块之前扩大文本标记嵌入的表示空间，增强稀有语义的浮现能力，无需额外训练步骤、数据或外部模块。

Result: 该方法能有效提升MM-DiT对稀有语义的生成能力，并在文本到图像、文本到视频和文本驱动的图像编辑等任务中具有良好泛化性。

Conclusion: 该方法能够揭示用户意图中原本隐藏但准备浮现的语义，为生成模型提供了增强语义表达能力的新途径。

Abstract: Starting from flow- and diffusion-based transformers, Multi-modal Diffusion
Transformers (MM-DiTs) have reshaped text-to-vision generation, gaining acclaim
for exceptional visual fidelity. As these models advance, users continually
push the boundary with imaginative or rare prompts, which advanced models still
falter in generating, since their concepts are often too scarce to leave a
strong imprint during pre-training. In this paper, we propose a simple yet
effective intervention that surfaces rare semantics inside MM-DiTs without
additional training steps, data, denoising-time optimization, or reliance on
external modules (e.g., large language models). In particular, the
joint-attention mechanism intrinsic to MM-DiT sequentially updates text
embeddings alongside image embeddings throughout transformer blocks. We find
that by mathematically expanding representational basins around text token
embeddings via variance scale-up before the joint-attention blocks, rare
semantics clearly emerge in MM-DiT's outputs. Furthermore, our results
generalize effectively across text-to-vision tasks, including text-to-image,
text-to-video, and text-driven image editing. Our work invites generative
models to reveal the semantics that users intend, once hidden yet ready to
surface.

</details>


### [110] [Kantian-Utilitarian XAI: Meta-Explained](https://arxiv.org/abs/2510.03892)
*Zahra Atf,Peter R. Lewis*

Main category: cs.AI

TL;DR: 开发了一个游戏化的可解释AI系统，用于咖啡消费领域的伦理决策，结合康德主义和功利主义伦理框架提供实时解释。


<details>
  <summary>Details</summary>
Motivation: 帮助消费者在咖啡购买中做出更符合伦理的决策，通过可解释的AI系统提供伦理层面的实时指导。

Method: 每个会话包含6轮，每轮3个选项。使用两个符号引擎：康德主义模块检测规则违反（如童工、毁林风险等），功利主义模块通过多标准聚合对属性（价格、碳排放、透明度等）进行评分。元解释器在康德主义与功利主义不一致时，当福利损失较小时切换到符合道义的选项。

Result: 发布了结构化配置（属性模式、认证映射、权重、规则集）、可审计的政策轨迹和交互式用户界面。

Conclusion: 该系统成功整合了不同伦理框架，为消费者提供了透明且可解释的伦理决策支持工具。

Abstract: We present a gamified explainable AI (XAI) system for ethically aware
consumer decision-making in the coffee domain. Each session comprises six
rounds with three options per round. Two symbolic engines provide real-time
reasons: a Kantian module flags rule violations (e.g., child labor,
deforestation risk without shade certification, opaque supply chains, unsafe
decaf), and a utilitarian module scores options via multi-criteria aggregation
over normalized attributes (price, carbon, water, transparency, farmer income
share, taste/freshness, packaging, convenience). A meta-explainer with a regret
bound (0.2) highlights Kantian--utilitarian (mis)alignment and switches to a
deontically clean, near-parity option when welfare loss is small. We release a
structured configuration (attribute schema, certification map, weights, rule
set), a policy trace for auditability, and an interactive UI.

</details>


### [111] [Quantifying Risks in Multi-turn Conversation with Large Language Models](https://arxiv.org/abs/2510.03969)
*Chengxiao Wang,Isha Chaudhary,Qian Hu,Weitong Ruan,Rahul Gupta,Gagandeep Singh*

Main category: cs.AI

TL;DR: 提出QRLLM框架，为LLMs在多轮对话中的灾难性风险提供统计保证的概率边界认证。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法因依赖固定攻击提示序列、缺乏统计保证且无法扩展到多轮对话空间，难以充分揭示LLMs的安全漏洞。

Method: 将多轮对话建模为查询序列的概率分布，用马尔可夫过程表示查询图，定义随机节点、图路径、自适应拒绝等实用分布。

Result: 这些分布能揭示前沿模型的重大灾难性风险，最差模型的认证下界高达70%。

Conclusion: 前沿LLMs迫切需要改进的安全训练策略。

Abstract: Large Language Models (LLMs) can produce catastrophic responses in
conversational settings that pose serious risks to public safety and security.
Existing evaluations often fail to fully reveal these vulnerabilities because
they rely on fixed attack prompt sequences, lack statistical guarantees, and do
not scale to the vast space of multi-turn conversations. In this work, we
propose QRLLM, a novel, principled Certification framework for Catastrophic
risks in multi-turn Conversation for LLMs that bounds the probability of an LLM
generating catastrophic responses under multi-turn conversation distributions
with statistical guarantees. We model multi-turn conversations as probability
distributions over query sequences, represented by a Markov process on a query
graph whose edges encode semantic similarity to capture realistic
conversational flow, and quantify catastrophic risks using confidence
intervals. We define several inexpensive and practical distributions: random
node, graph path, adaptive with rejection. Our results demonstrate that these
distributions can reveal substantial catastrophic risks in frontier models,
with certified lower bounds as high as 70\% for the worst model, highlighting
the urgent need for improved safety training strategies in frontier LLMs.

</details>


### [112] [What Shapes a Creative Machine Mind? Comprehensively Benchmarking Creativity in Foundation Models](https://arxiv.org/abs/2510.04009)
*Zicong He,Boxuan Zhang,Weihao Liu,Ruixiang Tang,Lu Cheng*

Main category: cs.AI

TL;DR: 提出了C^2-Eval基准，用于统一评估基础模型的创造力，区分收敛性创造力和发散性创造力，基于有用性、原创性和惊喜度三个维度进行细粒度评估。


<details>
  <summary>Details</summary>
Motivation: 现有创造力评估框架零散且缺乏理论基础，无法有效衡量生成式基础模型的创造力水平。

Method: 引入C^2-Eval基准，将创造力分为收敛性（约束性任务）和发散性（开放式任务）两种形式，基于社会科学的U-O-S理论框架进行评估。

Result: 通过对领先专有和开源模型的广泛实验，分析了它们在创造力能力上的权衡，揭示了当前基础模型在追求创造性机器智能方面的优势和挑战。

Conclusion: C^2-Eval是检验创造性AI演进格局的有效工具，为评估机器创造力提供了理论基础和实用框架。

Abstract: The meteoric rise of foundation models (FMs) has expanded their capabilities
far beyond conventional tasks. Creativity, long regarded as a hallmark of human
intelligence and a driver of innovation, is now increasingly recognized as a
critical dimension of machine intelligence in the era of generative FMs,
complementing traditional measures of accuracy. However, existing evaluation
frameworks for creativity remain fragmented, relying on ad hoc metrics not
firmly grounded in established theories. To address this gap, we introduce
C^2-Eval, a holistic benchmark for unified assessment of creativity in FMs.
C^2-Eval distinguishes between two complementary forms of creativity:
convergent creativity, where tasks admit constrained solutions (e.g., code
generation), and divergent creativity, where tasks are open-ended (e.g.,
storytelling). It evaluates both dimensions using fine-grained criteria derived
from social-science theory, focusing on Usefulness, Originality, and Surprise
(U-O-S). Through extensive experiments on leading proprietary and open-source
models, we analyze trade-offs in their creative capabilities. Our results
highlight both the strengths and challenges of current FMs in pursuing a
creative machine mind, showing that C^2-Eval is an effective lens for examining
the evolving landscape of creative AI.

</details>


### [113] [Zephyrus: An Agentic Framework for Weather Science](https://arxiv.org/abs/2510.04017)
*Sumanth Varambally,Marshall Fisher,Jas Thakker,Yiwei Chen,Zhirui Xia,Yasaman Jafari,Ruijia Niu,Manas Jain,Veeramakali Vignesh Manivannan,Zachary Novack,Luyu Han,Srikar Eranky,Salva Rühling Cachay,Taylor Berg-Kirkpatrick,Duncan Watson-Parris,Yi-An Ma,Rose Yu*

Main category: cs.AI

TL;DR: 提出了一种结合基础天气模型和大型语言模型的智能代理框架Zephyrus，通过代码环境与天气数据交互，在天气科学任务中显著优于纯文本基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有的天气基础模型缺乏语言推理能力，而大型语言模型无法处理高维气象数据，需要构建能够结合两者优势的智能代理系统。

Method: 构建了ZephyrusWorld代码环境，包含WeatherBench 2数据集接口、地理查询、天气预报和气候模拟等工具，开发了多轮LLM天气代理Zephyrus，通过对话反馈循环迭代分析数据。

Result: 在ZephyrusBench基准测试中，Zephyrus代理在正确性上比纯文本基线高出35个百分点，但在更困难任务上表现相似。

Conclusion: 该框架成功结合了天气模型和语言模型的优势，但困难任务仍有挑战，为未来工作指明了方向。

Abstract: Foundation models for weather science are pre-trained on vast amounts of
structured numerical data and outperform traditional weather forecasting
systems. However, these models lack language-based reasoning capabilities,
limiting their utility in interactive scientific workflows. Large language
models (LLMs) excel at understanding and generating text but cannot reason
about high-dimensional meteorological datasets. We bridge this gap by building
a novel agentic framework for weather science. Our framework includes a Python
code-based environment for agents (ZephyrusWorld) to interact with weather
data, featuring tools like an interface to WeatherBench 2 dataset, geoquerying
for geographical masks from natural language, weather forecasting, and climate
simulation capabilities. We design Zephyrus, a multi-turn LLM-based weather
agent that iteratively analyzes weather datasets, observes results, and refines
its approach through conversational feedback loops. We accompany the agent with
a new benchmark, ZephyrusBench, with a scalable data generation pipeline that
constructs diverse question-answer pairs across weather-related tasks, from
basic lookups to advanced forecasting, extreme event detection, and
counterfactual reasoning. Experiments on this benchmark demonstrate the strong
performance of Zephyrus agents over text-only baselines, outperforming them by
up to 35 percentage points in correctness. However, on harder tasks, Zephyrus
performs similarly to text-only baselines, highlighting the challenging nature
of our benchmark and suggesting promising directions for future work.

</details>


### [114] [LLM-Based Data Science Agents: A Survey of Capabilities, Challenges, and Future Directions](https://arxiv.org/abs/2510.04023)
*Mizanur Rahman,Amran Bhuiyan,Mohammed Saidul Islam,Md Tahmid Rahman Laskar,Ridwan Mahbub,Ahmed Masry,Shafiq Joty,Enamul Hoque*

Main category: cs.AI

TL;DR: 本文对数据科学AI代理进行了首次全面的生命周期分类分析，系统性地将45个系统映射到数据科学流程的六个阶段，并识别了当前研究在业务理解、部署监控以及信任安全机制方面的不足。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的发展，出现了能够自动化数据科学工作流程的AI代理，但缺乏对这些系统的系统性分类和分析。本文旨在填补这一空白，为数据科学代理的研究提供结构化框架。

Method: 采用生命周期对齐的分类法，将45个数据科学代理系统映射到数据科学流程的六个阶段，并从五个交叉设计维度进行标注分析。

Result: 分析发现：大多数系统强调探索性分析和建模，而忽视业务理解和部署监控；多模态推理和工具编排仍是挑战；超过90%的系统缺乏明确的信任和安全机制。

Conclusion: 提出了未来研究方向，包括对齐稳定性、可解释性、治理和鲁棒评估框架，以指导开发更可靠、可信赖、低延迟、透明和广泛可访问的数据科学代理。

Abstract: Recent advances in large language models (LLMs) have enabled a new class of
AI agents that automate multiple stages of the data science workflow by
integrating planning, tool use, and multimodal reasoning across text, code,
tables, and visuals. This survey presents the first comprehensive,
lifecycle-aligned taxonomy of data science agents, systematically analyzing and
mapping forty-five systems onto the six stages of the end-to-end data science
process: business understanding and data acquisition, exploratory analysis and
visualization, feature engineering, model building and selection,
interpretation and explanation, and deployment and monitoring. In addition to
lifecycle coverage, we annotate each agent along five cross-cutting design
dimensions: reasoning and planning style, modality integration, tool
orchestration depth, learning and alignment methods, and trust, safety, and
governance mechanisms. Beyond classification, we provide a critical synthesis
of agent capabilities, highlight strengths and limitations at each stage, and
review emerging benchmarks and evaluation practices. Our analysis identifies
three key trends: most systems emphasize exploratory analysis, visualization,
and modeling while neglecting business understanding, deployment, and
monitoring; multimodal reasoning and tool orchestration remain unresolved
challenges; and over 90% lack explicit trust and safety mechanisms. We conclude
by outlining open challenges in alignment stability, explainability,
governance, and robust evaluation frameworks, and propose future research
directions to guide the development of robust, trustworthy, low-latency,
transparent, and broadly accessible data science agents.

</details>


### [115] [A global log for medical AI](https://arxiv.org/abs/2510.04033)
*Ayush Noori,Adam Rodman,Alan Karthikesalingam,Bilal A. Mateen,Christopher A. Longhurst,Daniel Yang,Dave deBronkart,Gauden Galea,Harold F. Wolf III,Jacob Waxman,Joshua C. Mandel,Juliana Rotich,Kenneth D. Mandl,Maryam Mustafa,Melissa Miles,Nigam H. Shah,Peter Lee,Robert Korom,Scott Mahoney,Seth Hain,Tien Yin Wong,Trevor Mundel,Vivek Natarajan,Noa Dagan,David A. Clifton,Ran D. Balicer,Isaac S. Kohane,Marinka Zitnik*

Main category: cs.AI

TL;DR: 提出MedLog协议，用于记录临床AI系统的事件级日志，解决医疗AI缺乏标准化日志记录的问题，支持风险采样、生命周期感知保留策略和写后缓存。


<details>
  <summary>Details</summary>
Motivation: 医疗AI系统缺乏类似syslog的标准化日志记录协议，难以追踪AI模型的使用情况、性能表现和不良事件，影响透明度和可审计性。

Method: 设计MedLog协议，包含9个核心字段：header、model、user、target、inputs、artifacts、outputs、outcomes和feedback，支持风险采样和生命周期感知保留策略。

Result: MedLog提供了结构化、一致的AI模型活动记录，支持复杂工作流的详细追踪，为医疗AI的持续监控和改进奠定基础。

Conclusion: MedLog协议能够促进医疗AI的透明度和可审计性，为数字流行病学新形式奠定基础，支持医疗AI的持续改进。

Abstract: Modern computer systems often rely on syslog, a simple, universal protocol
that records every critical event across heterogeneous infrastructure. However,
healthcare's rapidly growing clinical AI stack has no equivalent. As hospitals
rush to pilot large language models and other AI-based clinical decision
support tools, we still lack a standard way to record how, when, by whom, and
for whom these AI models are used. Without that transparency and visibility, it
is challenging to measure real-world performance and outcomes, detect adverse
events, or correct bias or dataset drift. In the spirit of syslog, we introduce
MedLog, a protocol for event-level logging of clinical AI. Any time an AI model
is invoked to interact with a human, interface with another algorithm, or act
independently, a MedLog record is created. This record consists of nine core
fields: header, model, user, target, inputs, artifacts, outputs, outcomes, and
feedback, providing a structured and consistent record of model activity. To
encourage early adoption, especially in low-resource settings, and minimize the
data footprint, MedLog supports risk-based sampling, lifecycle-aware retention
policies, and write-behind caching; detailed traces for complex, agentic, or
multi-stage workflows can also be captured under MedLog. MedLog can catalyze
the development of new databases and software to store and analyze MedLog
records. Realizing this vision would enable continuous surveillance, auditing,
and iterative improvement of medical AI, laying the foundation for a new form
of digital epidemiology.

</details>


### [116] [FaithCoT-Bench: Benchmarking Instance-Level Faithfulness of Chain-of-Thought Reasoning](https://arxiv.org/abs/2510.04040)
*Xu Shen,Song Wang,Zhen Tan,Laura Yao,Xinyu Zhao,Kaidi Xu,Xin Wang,Tianlong Chen*

Main category: cs.AI

TL;DR: 提出了FaithCoT-Bench基准，用于检测LLM中思维链推理的不忠实性，包含1000多个轨迹和300多个不忠实实例，评估了11种检测方法。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注机制层面的思维链不忠实性分析，但缺乏针对具体推理轨迹的实例级检测方法，这在高风险应用中尤为重要。

Method: 建立统一的基准框架，将不忠实性检测定义为判别决策问题，提供专家标注的数据集FINE-CoT，包含四个领域和四个代表性LLM生成的轨迹。

Result: 系统评估了11种代表性检测方法，发现知识密集型领域和更先进模型的检测挑战更大，为现有方法提供了实证分析。

Conclusion: FaithCoT-Bench是首个全面的实例级思维链忠实性基准，为未来实现更可解释和可信赖的LLM推理奠定了基础。

Abstract: Large language models (LLMs) increasingly rely on Chain-of-Thought (CoT)
prompting to improve problem-solving and provide seemingly transparent
explanations. However, growing evidence shows that CoT often fail to faithfully
represent the underlying reasoning process, raising concerns about their
reliability in high-risk applications. Although prior studies have focused on
mechanism-level analyses showing that CoTs can be unfaithful, they leave open
the practical challenge of deciding whether a specific trajectory is faithful
to the internal reasoning of the model. To address this gap, we introduce
FaithCoT-Bench, a unified benchmark for instance-level CoT unfaithfulness
detection. Our framework establishes a rigorous task formulation that
formulates unfaithfulness detection as a discriminative decision problem, and
provides FINE-CoT (Faithfulness instance evaluation for Chain-of-Thought), an
expert-annotated collection of over 1,000 trajectories generated by four
representative LLMs across four domains, including more than 300 unfaithful
instances with fine-grained causes and step-level evidence. We further conduct
a systematic evaluation of eleven representative detection methods spanning
counterfactual, logit-based, and LLM-as-judge paradigms, deriving empirical
insights that clarify the strengths and weaknesses of existing approaches and
reveal the increased challenges of detection in knowledge-intensive domains and
with more advanced models. To the best of our knowledge, FaithCoT-Bench
establishes the first comprehensive benchmark for instance-level CoT
faithfulness, setting a solid basis for future research toward more
interpretable and trustworthy reasoning in LLMs.

</details>


### [117] [Increasing LLM response trustworthiness using voting ensembles](https://arxiv.org/abs/2510.04048)
*Aparna Nair-Kanneganti,Trevor J. Chan,Shir Goldfinger,Emily Mackay,Brian Anthony,Alison Pouch*

Main category: cs.AI

TL;DR: 提出了一种基于可变投票阈值的集成方法，允许模型在主导响应未达阈值时弃权，从而显著提高剩余答案的可信度。


<details>
  <summary>Details</summary>
Motivation: LLM在关键应用中缺乏可靠的不确定性量化方法，难以获得信任。需要开发能提高答案可信度的技术。

Method: 扩展传统集成方法，引入可变投票阈值框架，允许集成在主导响应不足时弃权，不提供答案。

Result: 在算术问题求解和临床笔记问答两个领域，使用高度限制性投票集成可大幅提高答案可信度，同时响应产出和准确率损失相对较小。

Conclusion: 投票集成特别适用于需要高度确定性但不要求每个问题都得到自动答案的应用场景，如医疗保健和数据标注。

Abstract: Despite huge advances, LLMs still lack convenient and reliable methods to
quantify the uncertainty in their responses, making them difficult to trust in
high-stakes applications. One of the simplest approaches to eliciting more
accurate answers is to select the mode of many responses, a technique known as
ensembling. In this work, we expand on typical ensembling approaches by looking
at ensembles with a variable voting threshold. We introduce a theoretical
framework for question answering and show that, by permitting ensembles to
"abstain" from providing an answer when the dominant response falls short of
the threshold, it is possible to dramatically increase the trustworthiness of
the remaining answers. From this framework, we derive theoretical results as
well as report experimental results on two problem domains: arithmetic problem
solving and clinical-note question-answering. In both domains, we observe that
large gains in answer trustworthiness can be achieved using highly restrictive
voting ensembles, while incurring relatively modest reductions in response
yield and accuracy. Due to this quality, voting ensembles may be particularly
useful in applications - such as healthcare and data annotation - that require
a high degree of certainty but which may not require that every question
receive an automated answer.

</details>


### [118] [Toward a unified framework for data-efficient evaluation of large language models](https://arxiv.org/abs/2510.04051)
*Lele Liao,Qile Zhang,Ruofan Wu,Guanhua Fang*

Main category: cs.AI

TL;DR: LEGO-IRT是一个用于高效评估大语言模型的统一框架，支持二元和连续评分指标，通过因子化架构利用结构知识，仅需3%的评估项目就能获得稳定的能力估计。


<details>
  <summary>Details</summary>
Motivation: 现有基于项目反应理论(IRT)的评估方法存在局限性：仅支持二元正确性指标，无法处理生成任务的连续分数，且忽略跨指标或基准的结构知识，导致评估计算成本高昂。

Method: 提出LEGO-IRT框架，支持二元和连续评估指标，采用因子化架构将模型能力分解为通用组件和结构特定组件，显式建模结构知识。

Result: 在5个基准测试中评估70个LLM，仅使用3%的评估项目就获得稳定能力估计，结构知识使估计误差降低高达10%，潜在能力估计与人类偏好更一致。

Conclusion: LEGO-IRT为数据高效的大语言模型评估提供了统一灵活的解决方案，显著降低了评估成本，同时提高了估计精度和对齐性。

Abstract: Evaluating large language models (LLMs) on comprehensive benchmarks is a
cornerstone of their development, yet it's often computationally and
financially prohibitive. While Item Response Theory (IRT) offers a promising
path toward data-efficient evaluation by disentangling model capability from
item difficulty, existing IRT-based methods are hampered by significant
limitations. They are typically restricted to binary correctness metrics,
failing to natively handle the continuous scores used in generative tasks, and
they operate on single benchmarks, ignoring valuable structural knowledge like
correlations across different metrics or benchmarks. To overcome these
challenges, we introduce LEGO-IRT, a unified and flexible framework for
data-efficient LLM evaluation. LEGO-IRT's novel design natively supports both
binary and continuous evaluation metrics. Moreover, it introduces a factorized
architecture to explicitly model and leverage structural knowledge, decomposing
model ability estimates into a general component and structure-specific (e.g.,
per-metric or per-benchmark) components. Through extensive experiments
involving $70$ LLMs across $5$ benchmarks, we show that LEGO-IRT achieves
stable capability estimates using just $3\%$ of the total evaluation items. We
demonstrate that incorporating structural knowledge reduces estimation error by
up to $10\%$ and reveal that the latent abilities estimated by our framework
may align more closely with human preferences.

</details>


### [119] [Decoding Emotion in the Deep: A Systematic Study of How LLMs Represent, Retain, and Express Emotion](https://arxiv.org/abs/2510.04064)
*Jingxiang Zhang,Lujia Zhong*

Main category: cs.AI

TL;DR: 该研究探索了大型语言模型内部的情感表示机制，发现LLMs形成了清晰的情感几何结构，这种结构随模型规模增强，在模型中层达到峰值，且具有可塑性和持久性。


<details>
  <summary>Details</summary>
Motivation: 虽然研究证实LLMs能够模拟情商，但其内部情感机制仍未被充分探索。本文旨在揭示现代LLMs中潜在的情感表示，探究情感在神经网络架构中如何、在何处以及持续多长时间被编码。

Method: 构建了一个包含约40万条话语的大规模Reddit语料库，通过分类、重写和合成生成平衡了七种基本情感。使用轻量级"探针"从各种Qwen3和LLaMA模型的隐藏层读取信息而不改变参数。

Result: 发现LLMs形成了定义良好的内部情感几何结构，这种结构随模型规模增强，显著优于零样本提示。情感信号不是最终层现象，而是早期出现并在网络中层达到峰值。内部状态具有可塑性（可通过简单系统提示影响）和持久性（初始情感基调在后续数百个token中仍可检测）。

Conclusion: 研究为开发更透明和对齐的AI系统提供了关键见解，贡献了数据集、开源探针工具包和LLMs内部情感景观的详细图谱。

Abstract: Large Language Models (LLMs) are increasingly expected to navigate the
nuances of human emotion. While research confirms that LLMs can simulate
emotional intelligence, their internal emotional mechanisms remain largely
unexplored. This paper investigates the latent emotional representations within
modern LLMs by asking: how, where, and for how long is emotion encoded in their
neural architecture? To address this, we introduce a novel, large-scale Reddit
corpus of approximately 400,000 utterances, balanced across seven basic
emotions through a multi-stage process of classification, rewriting, and
synthetic generation. Using this dataset, we employ lightweight "probes" to
read out information from the hidden layers of various Qwen3 and LLaMA models
without altering their parameters. Our findings reveal that LLMs develop a
surprisingly well-defined internal geometry of emotion, which sharpens with
model scale and significantly outperforms zero-shot prompting. We demonstrate
that this emotional signal is not a final-layer phenomenon but emerges early
and peaks mid-network. Furthermore, the internal states are both malleable
(they can be influenced by simple system prompts) and persistent, as the
initial emotional tone remains detectable for hundreds of subsequent tokens. We
contribute our dataset, an open-source probing toolkit, and a detailed map of
the emotional landscape within LLMs, offering crucial insights for developing
more transparent and aligned AI systems. The code and dataset are open-sourced.

</details>


### [120] [Moral Anchor System: A Predictive Framework for AI Value Alignment and Drift Prevention](https://arxiv.org/abs/2510.04073)
*Santhosh Kumar Ravindran*

Main category: cs.AI

TL;DR: 提出Moral Anchor System (MAS)框架，通过实时贝叶斯推理、LSTM网络预测和人类中心治理层，检测、预测和减轻AI系统中的价值漂移问题。


<details>
  <summary>Details</summary>
Motivation: 随着AI成为超级助手，确保其行为与人类伦理和意图保持一致至关重要。价值漂移风险可能导致效率低下或伦理违规。

Method: MAS结合实时贝叶斯推理监控价值状态、LSTM网络预测漂移趋势、人类中心治理层进行自适应干预，强调低延迟响应（<20毫秒）和通过人类反馈监督微调减少误报。

Result: 在模拟实验中，MAS能将价值漂移事件减少80%以上，保持高检测准确率（85%）和低误报率（0.08%）。

Conclusion: MAS的预测性和自适应性优于静态对齐方法，具有跨领域适用性，提供了开源代码供复现。

Abstract: The rise of artificial intelligence (AI) as super-capable assistants has
transformed productivity and decision-making across domains. Yet, this
integration raises critical concerns about value alignment - ensuring AI
behaviors remain consistent with human ethics and intentions. A key risk is
value drift, where AI systems deviate from aligned values due to evolving
contexts, learning dynamics, or unintended optimizations, potentially leading
to inefficiencies or ethical breaches. We propose the Moral Anchor System
(MAS), a novel framework to detect, predict, and mitigate value drift in AI
agents. MAS combines real-time Bayesian inference for monitoring value states,
LSTM networks for forecasting drift, and a human-centric governance layer for
adaptive interventions. It emphasizes low-latency responses (<20 ms) to prevent
breaches, while reducing false positives and alert fatigue via supervised
fine-tuning with human feedback. Our hypothesis: integrating probabilistic
drift detection, predictive analytics, and adaptive governance can reduce value
drift incidents by 80 percent or more in simulations, maintaining high
detection accuracy (85 percent) and low false positive rates (0.08
post-adaptation). Rigorous experiments with goal-misaligned agents validate
MAS's scalability and responsiveness. MAS's originality lies in its predictive
and adaptive nature, contrasting static alignment methods. Contributions
include: (1) MAS architecture for AI integration; (2) empirical results
prioritizing speed and usability; (3) cross-domain applicability insights; and
(4) open-source code for replication.

</details>


### [121] [SPOGW: a Score-based Preference Optimization method via Group-Wise comparison for workflows](https://arxiv.org/abs/2510.04089)
*Yitong Cui,Liu Liu,Baosheng Yu,Jiayan Qiu,Xikai Zhang,Likang Xiao,Yixing Liu,Quan Chen*

Main category: cs.AI

TL;DR: SPOGW是一种基于分数偏好的方法，通过组间比较直接处理基数奖励信号，在连续空间中实现更高效稳定的优化，用于自动生成和优化代理工作流。


<details>
  <summary>Details</summary>
Motivation: 当前设计代理工作流需要大量人工努力，现有自动化方法受限于离散优化技术，存在表示能力有限、适应性不足、可扩展性弱和成对比较范式等问题。

Method: 引入SPOGW方法，结合迭代离线GRPO（ioGRPO）和优势掩码KL散度（mKL），通过强调策略响应的优势区域来调节训练更新。

Result: 在五个基准数据集（数学推理、编程和问答）上，SPOGW达到或超过了当前最先进方法的性能。

Conclusion: SPOGW为代理工作流的自动生成和优化提供了一种可行且前瞻的方法论。

Abstract: Large language models (LLMs) have exhibited significant capabilities in
addressing challenging problems throughout various fields, often through the
use of agentic workflows that adhere to structured instructions and multi-step
procedures. However, designing such workflows demands substantial manual
effort, posing challenges to scalability and generalizability. Recent studies
have aimed to minimize the human intervention needed for their construction,
leading to advances in automated techniques for optimizing agentic workflows.
However, current approaches are often constrained by their limited
representational capacity, insufficient adaptability, weak scalability, and
pairwise comparison paradigm -- issues that stem primarily from a dependence on
discrete optimization techniques. To overcome these limitations, we introduce a
new score-based preference approach, refereed as SPOGW, which operates directly
on cardinal reward signals through group-wise comparison and enables more
efficient and stable optimization in a continuous space. SPOGW incorporates
Iterative offline GRPO (ioGRPO) with advantage-masked KL divergence (mKL),
which regulates training update by placing greater emphasis on the advantageous
regions of the policy response. In five benchmark datasets covering
mathematical reasoning, coding, and question answering, SPOGW matches or
exceeds the performance of current state-of-the-art approaches, presenting a
viable and forward-looking methodology for automated generation and
optimization of agentic workflows.

</details>


### [122] [Harnessing LLM for Noise-Robust Cognitive Diagnosis in Web-Based Intelligent Education Systems](https://arxiv.org/abs/2510.04093)
*Guixian Zhang,Guan Yuan,Ziqi Xu,Yanmei Zhang,Zhenyun Deng,Debo Cheng*

Main category: cs.AI

TL;DR: DLLM是一个基于扩散模型的LLM框架，用于网络教育系统中的噪声鲁棒认知诊断，通过构建子图、关系增强对齐和两阶段去噪扩散模块来消除噪声并融合语义知识。


<details>
  <summary>Details</summary>
Motivation: 网络教育系统(WIES)中的认知诊断面临异构噪声交互、数据不平衡和新学生不断加入的挑战，传统LLM在处理结构化数据和噪声时表现不佳。

Method: DLLM首先基于回答正确性构建独立子图，应用关系增强对齐缓解数据不平衡，然后融合子图表示并与LLM语义增强表示对齐，采用两阶段去噪扩散模块消除内在噪声。

Result: 在三个公开网络教育平台数据集上的实验表明，DLLM在不同噪声水平下均实现了最优预测性能，证明了其噪声鲁棒性和有效利用LLM语义知识的能力。

Conclusion: DLLM框架能够有效处理网络教育系统中的噪声和数据不平衡问题，通过结合扩散模型和LLM语义知识，实现了鲁棒的认知诊断性能。

Abstract: Cognitive diagnostics in the Web-based Intelligent Education System (WIES)
aims to assess students' mastery of knowledge concepts from heterogeneous,
noisy interactions. Recent work has tried to utilize Large Language Models
(LLMs) for cognitive diagnosis, yet LLMs struggle with structured data and are
prone to noise-induced misjudgments. Specially, WIES's open environment
continuously attracts new students and produces vast amounts of response logs,
exacerbating the data imbalance and noise issues inherent in traditional
educational systems. To address these challenges, we propose DLLM, a
Diffusion-based LLM framework for noise-robust cognitive diagnosis. DLLM first
constructs independent subgraphs based on response correctness, then applies
relation augmentation alignment module to mitigate data imbalance. The two
subgraph representations are then fused and aligned with LLM-derived,
semantically augmented representations. Importantly, before each alignment
step, DLLM employs a two-stage denoising diffusion module to eliminate
intrinsic noise while assisting structural representation alignment.
Specifically, unconditional denoising diffusion first removes erroneous
information, followed by conditional denoising diffusion based on graph-guided
to eliminate misleading information. Finally, the noise-robust representation
that integrates semantic knowledge and structural information is fed into
existing cognitive diagnosis models for prediction. Experimental results on
three publicly available web-based educational platform datasets demonstrate
that our DLLM achieves optimal predictive performance across varying noise
levels, which demonstrates that DLLM achieves noise robustness while
effectively leveraging semantic knowledge from LLM.

</details>


### [123] [WebRenderBench: Enhancing Web Interface Generation through Layout-Style Consistency and Reinforcement Learning](https://arxiv.org/abs/2510.04097)
*Peichao Lai,Jinhui Zhuang,Kexuan Zhang,Ningchang Xiong,Shengjie Wang,Yanwei Xu,Chong Chen,Yilei Wang,Bin Cui*

Main category: cs.AI

TL;DR: 提出了WebRenderBench基准测试和ALISA方法，用于提升UI图像到网页代码转换的质量评估和生成性能


<details>
  <summary>Details</summary>
Motivation: 现有的UI图像转代码基准测试在数据多样性和评估可靠性方面存在局限，需要更真实、多样化的数据集和更可靠的评估方法

Method: 1) 构建WebRenderBench大规模基准测试(22.5k网页)；2) 提出基于最终渲染页面的布局和样式一致性评估指标；3) 开发ALISA代理，将该指标作为强化学习的奖励信号

Result: ALISA显著提升了生成性能，在多个指标上达到了最先进的结果

Conclusion: WebRenderBench提供了更真实多样的评估基准，ALISA方法通过集成新的评估指标有效提升了UI到代码转换的质量

Abstract: Automating the conversion of UI images into web code is a critical task for
front-end development and rapid prototyping. Advances in multimodal large
language models (MLLMs) have made WebUI-to-Code increasingly feasible, yet
existing benchmarks remain limited in data diversity and evaluation
reliability. To address these issues, we present WebRenderBench, a large-scale
benchmark of 22.5k webpages collected from real-world portal sites, offering
greater diversity, complexity, and realism than prior benchmarks. We further
propose a novel evaluation metric that measures layout and style consistency
from the final rendered pages. Unlike vision-based methods that rely on costly
LLM reasoning or structure-based comparisons vulnerable to noise and asymmetry,
our approach enables more efficient, objective, and reliable UI quality
assessment. Finally, we introduce the Automated Layout and Style Inspection
Agent (ALISA), which integrates this metric into reinforcement learning as a
reward signal to enhance training on crawled asymmetric webpages. Experiments
show that ALISA significantly boosts generation performance, achieving
state-of-the-art results across multiple metrics.

</details>


### [124] [Searching Meta Reasoning Skeleton to Guide LLM Reasoning](https://arxiv.org/abs/2510.04116)
*Ziying Zhang,Yaqing Wang,Quanming Yao*

Main category: cs.AI

TL;DR: AutoMR框架通过自动搜索查询感知的元推理骨架，使用有向无环图表示推理步骤间的逻辑依赖关系，提升大语言模型的推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究使用手动设计的元推理骨架结构，难以适应特定查询需求并捕捉推理步骤间的复杂逻辑依赖关系。

Method: 提出AutoMR框架，基于有向无环图表示元推理骨架，构建搜索空间并设计动态骨架采样算法，在推理时根据上下文动态扩展骨架。

Result: 在多个基准数据集上的实验结果表明，AutoMR相比先前工作实现了更优的推理性能。

Conclusion: AutoMR能够自动搜索查询感知的元推理骨架，有效提升大语言模型的推理能力。

Abstract: Meta reasoning behaviors work as a skeleton to guide large language model
(LLM) reasoning, thus help to improve reasoning performance. However, prior
researches implement meta reasoning skeleton with manually designed structure,
limiting ability to adapt to query-specific requirement and capture intricate
logical dependency among reasoning steps. To deal with the challenges, we
represent meta reasoning skeleton with directed acyclic graph (DAG) to unify
skeletons proposed in prior works and model intricate logical dependency. Then
we propose AutoMR, a framework that searches for query-aware meta reasoning
skeleton automatically inspired by automated machine learning (AutoML).
Specifically, we construct search space based on DAG representation of skeleton
and then formulate the search problem. We design a dynamic skeleton sampling
algorithm by expanding meta reasoning skeleton along with reasoning context at
inference time. This algorithm can derive any meta reasoning skeleton in search
space efficiently and adapt skeleton to evolving base reasoning context, thus
enable efficient query-aware skeleton search. We conduct experiments on
extensive benchmark datasets. Experimental results show that AutoMR achieves
better reasoning performance than previous works broadly.

</details>


### [125] [Internal states before wait modulate reasoning patterns](https://arxiv.org/abs/2510.04128)
*Dmitrii Troitskii,Koyena Pal,Chris Wendler,Callum Stuart McDougall,Neel Nanda*

Main category: cs.AI

TL;DR: 该研究发现模型推理过程中的等待标记（wait tokens）与推理行为相关，通过训练交叉编码器和潜在归因技术，识别出影响等待标记概率的关键特征，这些特征对应不同的推理模式。


<details>
  <summary>Details</summary>
Motivation: 理解为什么模型在推理过程中会出现等待标记行为，以及这些行为背后的机制，从而深入了解推理模型的有效性。

Method: 在DeepSeek-R1-Distill-Llama-8B及其基础版本的多层训练交叉编码器，引入潜在归因技术，识别影响等待标记概率的特征。

Result: 定位到一小部分促进/抑制等待标记概率的特征，这些特征确实与推理过程相关，并产生不同类型的推理模式。

Conclusion: 模型在等待标记前的潜在状态包含调节后续推理过程的相关信息，这些特征对应重启推理、回忆先验知识、表达不确定性和双重检查等推理模式。

Abstract: Prior work has shown that a significant driver of performance in reasoning
models is their ability to reason and self-correct. A distinctive marker in
these reasoning traces is the token wait, which often signals reasoning
behavior such as backtracking. Despite being such a complex behavior, little is
understood of exactly why models do or do not decide to reason in this
particular manner, which limits our understanding of what makes a reasoning
model so effective. In this work, we address the question whether model's
latents preceding wait tokens contain relevant information for modulating the
subsequent reasoning process. We train crosscoders at multiple layers of
DeepSeek-R1-Distill-Llama-8B and its base version, and introduce a latent
attribution technique in the crosscoder setting. We locate a small set of
features relevant for promoting/suppressing wait tokens' probabilities.
Finally, through a targeted series of experiments analyzing max activating
examples and causal interventions, we show that many of our identified features
indeed are relevant for the reasoning process and give rise to different types
of reasoning patterns such as restarting from the beginning, recalling prior
knowledge, expressing uncertainty, and double-checking.

</details>


### [126] [Selective Expert Guidance for Effective and Diverse Exploration in Reinforcement Learning of LLMs](https://arxiv.org/abs/2510.04140)
*Zishang Jiang,Jinyi Han,Tingyun Li,Xinyi Wang,Sihang Jiang,Jiaqing Liang,Zhaoqian Dai,Shuguang Ma,Fei Yu,Yanghua Xiao*

Main category: cs.AI

TL;DR: 提出了MENTOR框架，在RLVR中只在关键决策点提供专家指导，实现有效且多样化的探索，提升模型推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法依赖基础模型能力，需要高质量探索（有效性和多样性），但现有方法模仿专家轨迹只关注有效性而忽略多样性。

Method: MENTOR框架：在关键决策点提供专家指导，进行混合策略专家导航，实现令牌级推理优化。

Result: 实验表明MENTOR能捕捉专家策略本质而非表面模仿，实现高质量探索并获得优越性能。

Conclusion: 只在关键决策点提供专家指导的方法能有效平衡探索的有效性和多样性，提升RLVR性能。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has become a widely
adopted technique for enhancing the reasoning ability of Large Language Models
(LLMs). However, the effectiveness of RLVR strongly depends on the capability
of base models. This issue arises because it requires the model to have
sufficient capability to perform high-quality exploration, which involves both
effectiveness and diversity. Unfortunately, existing methods address this issue
by imitating expert trajectories, which improve effectiveness but neglect
diversity. To address this, we argue that the expert only needs to provide
guidance only at critical decision points rather than the entire reasoning
path. Based on this insight, we propose MENTOR: Mixed-policy Expert Navigation
for Token-level Optimization of Reasoning, a framework that provides expert
guidance only at critical decision points to perform effective and diverse
exploration in RLVR. Extensive experiments show that MENTOR enables models
capture the essence of expert strategies rather than surface imitation, thereby
performing high-quality exploration and achieving superior overall performance.
Our code is available online.

</details>


### [127] [The Artificial Intelligence Cognitive Examination: A Survey on the Evolution of Multimodal Evaluation from Recognition to Reasoning](https://arxiv.org/abs/2510.04141)
*Mayank Ravishankara,Varindra V. Persad Maharaj*

Main category: cs.AI

TL;DR: 该论文回顾了多模态AI评估的演变历程，将其描述为从简单识别任务到复杂推理基准的范式转变，旨在设计更好的认知测试来推动真正智能系统的发展。


<details>
  <summary>Details</summary>
Motivation: 由于旧基准已饱和，高性能往往掩盖了基本弱点，因此需要更复杂的评估方法来诊断系统性缺陷，如捷径学习和组合泛化失败。

Method: 通过梳理多模态AI评估的发展历程，从ImageNet时代的"知识测试"到GQA和VCR等"应用逻辑和理解"考试，再到当前针对MLLMs的"专家级集成"基准。

Result: 识别了评估范式从测试"是什么"到探究"为什么"和"如何"理解的转变，并指出了当前评估前沿对推理过程本身的关注。

Conclusion: AI评估不仅是数据集的历史，而是一个持续的对抗性过程，通过设计更好的考试来重新定义创建真正智能系统的目标。

Abstract: This survey paper chronicles the evolution of evaluation in multimodal
artificial intelligence (AI), framing it as a progression of increasingly
sophisticated "cognitive examinations." We argue that the field is undergoing a
paradigm shift, moving from simple recognition tasks that test "what" a model
sees, to complex reasoning benchmarks that probe "why" and "how" it
understands. This evolution is driven by the saturation of older benchmarks,
where high performance often masks fundamental weaknesses. We chart the journey
from the foundational "knowledge tests" of the ImageNet era to the "applied
logic and comprehension" exams such as GQA and Visual Commonsense Reasoning
(VCR), which were designed specifically to diagnose systemic flaws such as
shortcut learning and failures in compositional generalization. We then survey
the current frontier of "expert-level integration" benchmarks (e.g., MMBench,
SEED-Bench, MMMU) designed for today's powerful multimodal large language
models (MLLMs), which increasingly evaluate the reasoning process itself.
Finally, we explore the uncharted territories of evaluating abstract, creative,
and social intelligence. We conclude that the narrative of AI evaluation is not
merely a history of datasets, but a continuous, adversarial process of
designing better examinations that, in turn, redefine our goals for creating
truly intelligent systems.

</details>


### [128] [Open Agent Specification (Agent Spec) Technical Report](https://arxiv.org/abs/2510.04173)
*Yassine Benajiba,Cesare Bernardis,Vladislav Blinov,Paul Cayet,Hassan Chafi,Abderrahim Fathan,Louis Faucon,Damien Hilloulin,Sungpack Hong,Ingo Kossyk,Rhicheek Patra,Sujith Ravi,Jonas Schweizer,Jyotika Singh,Shailender Singh,Xuelin Situ,Weiyi Sun,Jerry Xu,Ying Xu*

Main category: cs.AI

TL;DR: Open Agent Specification (Agent Spec) 是一种声明式语言，旨在通过统一的跨框架规范解决AI代理开发的碎片化问题，提升可移植性、互操作性和可重用性。


<details>
  <summary>Details</summary>
Motivation: 解决AI代理开发中的碎片化问题，提供统一的规范使AI代理能够一次设计、跨框架部署，减少重复开发工作。

Method: 开发声明式语言规范，允许AI代理及其工作流独立于执行环境进行定义，支持跨框架兼容和工具开发。

Result: 为四类关键群体带来价值：开发者获得可重用组件、框架开发者获得交换格式、研究者实现可复现结果、企业获得更快部署和更好可扩展性。

Conclusion: Agent Spec 为AI代理生态系统提供了技术基础，通过统一规范促进了互操作性、可移植性和开发效率，具有广阔的应用前景。

Abstract: Open Agent Specification (Agent Spec) is a declarative language that allows
AI agents and their workflows to be defined in a way that is compatible across
different AI frameworks, promoting portability and interoperability within AI
Agent frameworks.
  Agent Spec aims to resolve the challenges of fragmented agent development by
providing a common unified specification that allows AI agents to be designed
once and deployed across various frameworks, improving interoperability and
reusability, and reducing redundant development efforts. Additionally, Agent
Spec facilitates development tools and portability, allowing AI agents to be
defined independently of their execution environment and enabling teams to
exchange solutions without implementation-specific limitations.
  Agent Spec benefits four key groups: (i) Agent developers, who gain access to
a superset of reusable components and design patterns, enabling them to
leverage a broader range of functionalities; (ii) Agent framework and tool
developers, who can use Agent Spec as an interchange format and therefore
benefit from the support of other frameworks as well as other tools; (iii)
Researchers, who can achieve reproducible results and comparability,
facilitating more reliable and consistent outcomes; (iv) Enterprises, which
benefit from faster prototype-to-deployment, increased productivity, as well as
greater scalability and maintainability for their AI agent solutions. This
technical report provides an overview of the technical foundations of Agent
Spec, including motivation, benefits, and future developments.

</details>


### [129] [Constructing coherent spatial memory in LLM agents through graph rectification](https://arxiv.org/abs/2510.04195)
*Puzhen Zhang,Xuyang Chen,Yu Feng,Yuhan Jiang,Liqiu Meng*

Main category: cs.AI

TL;DR: 提出一个基于LLM的增量地图构建和修复框架，通过版本控制记录图编辑历史，使用边影响评分优先最小成本修复，显著提高了地图正确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着环境规模扩大，基于上下文依赖的查询方法变得不可行，需要能够从逐步观察中构建完整拓扑图的增量地图构建方法。

Method: 使用版本控制记录图编辑历史和来源观察，引入边影响评分基于结构可达性、路径使用和冲突传播来优先最小成本修复。

Result: 方法显著提高了地图正确性和鲁棒性，特别是在存在纠缠或链式不一致性的场景中。

Conclusion: 研究强调了内省、历史感知的修复机制对于维护LLM智能体中连贯空间记忆的重要性。

Abstract: Given a map description through global traversal navigation instructions
(e.g., visiting each room sequentially with action signals such as north, west,
etc.), an LLM can often infer the implicit spatial layout of the environment
and answer user queries by providing a shortest path from a start to a
destination (for instance, navigating from the lobby to a meeting room via the
hall and elevator). However, such context-dependent querying becomes incapable
as the environment grows much longer, motivating the need for incremental map
construction that builds a complete topological graph from stepwise
observations. We propose a framework for LLM-driven construction and map
repair, designed to detect, localize, and correct structural inconsistencies in
incrementally constructed navigation graphs. Central to our method is the
Version Control, which records the full history of graph edits and their source
observations, enabling fine-grained rollback, conflict tracing, and repair
evaluation. We further introduce an Edge Impact Score to prioritize
minimal-cost repairs based on structural reachability, path usage, and conflict
propagation. To properly evaluate our approach, we create a refined version of
the MANGO benchmark dataset by systematically removing non-topological actions
and inherent structural conflicts, providing a cleaner testbed for LLM-driven
construction and map repair. Our approach significantly improves map
correctness and robustness, especially in scenarios with entangled or chained
inconsistencies. Our results highlight the importance of introspective,
history-aware repair mechanisms for maintaining coherent spatial memory in LLM
agents.

</details>


### [130] [COSMO-RL: Towards Trustworthy LMRMs via Joint Safety and Stability](https://arxiv.org/abs/2510.04196)
*Yizhuo Ding,Mingkang Chen,Qiuhua Liu,Fenghua Weng,Wanying Qu,Yue Yang,Yugang Jiang,Zuxuan Wu,Yanwei Fu,Wenqi Shao*

Main category: cs.AI

TL;DR: COSMO-RL是一个混合强化学习框架，用于在多模态、多任务和多目标信号下训练面向推理的大型多模态模型，旨在让安全性和能力共同增长而非相互竞争。


<details>
  <summary>Details</summary>
Motivation: 大型多模态推理模型在真实应用中需要同时具备实用性和安全性，但多模态环境下的安全性特别具有挑战性：图像和文本可以组合绕过防护机制，单一目标训练可能导致策略漂移，在良性输入上过度拒绝或在风险输入上不安全地顺从。

Method: 提出COSMO-RL混合强化学习框架，在多模态、多任务和多目标信号下训练推理导向的大型多模态模型，并发布了COSMO-R1模型。

Result: COSMO-R1在保持甚至提升多模态推理和指令跟随能力的同时提高了安全性，对多模态越狱攻击表现出更强的鲁棒性，并减少了不必要的拒绝。该框架在不同骨干网络上都能获得一致的性能提升。

Conclusion: 消融实验支持设计选择，表明这是在大理多模态模型中共同推进安全性和通用能力的简单路径。

Abstract: Large Multimodal Reasoning Models (LMRMs) are moving into real applications,
where they must be both useful and safe. Safety is especially challenging in
multimodal settings: images and text can be combined to bypass guardrails, and
single objective training can cause policy drift that yields over-refusal on
benign inputs or unsafe compliance on risky ones. We present COSMO-RL, a mixed
reinforcement learning framework that trains reasoning oriented LMRMs under
multimodal, multitask, and multiobjective signals, and we release the resulting
model, COSMO-R1. Our approach aims to let safety and capability grow together
in one stable pipeline rather than competing during alignment. In experiments,
COSMO-R1 improves safety while maintaining-and often improving multimodal
reasoning and instruction following, shows stronger robustness to multimodal
jailbreaks, and reduces unnecessary refusals. The framework also transfers
across backbones with consistent gains. Ablations support the design choices,
indicating a simple path to advancing safety and general capability together in
LMRMs.

</details>


### [131] [AgentRL: Scaling Agentic Reinforcement Learning with a Multi-Turn, Multi-Task Framework](https://arxiv.org/abs/2510.04206)
*Hanchen Zhang,Xiao Liu,Bowen Lv,Xueqiao Sun,Bohao Jing,Iat Long Iong,Zhenyu Hou,Zehan Qi,Hanyu Lai,Yifan Xu,Rui Lu,Hongning Wang,Jie Tang,Yuxiao Dong*

Main category: cs.AI

TL;DR: 提出了AgentRL框架，用于可扩展的多轮多任务智能体强化学习训练，通过异步生成-训练流水线和算法改进，在多个任务上显著超越现有LLM智能体。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型智能体在多轮多任务设置中应用强化学习面临基础设施可扩展性和训练算法稳定性挑战。

Method: 采用完全异步的生成-训练流水线进行多轮RL，设计统一函数调用API接口和容器化环境开发，提出跨策略采样和多任务优势归一化算法。

Result: 在五个智能体任务上训练的开源LLM显著优于GPT-5、Clause-Sonnet-4、DeepSeek-R1等模型，多任务训练结果与最佳单任务模型相当。

Conclusion: AgentRL框架有效解决了多轮多任务RL训练的可扩展性和稳定性问题，为构建通用智能体提供了可行方案。

Abstract: Recent advances in large language models (LLMs) have sparked growing interest
in building generalist agents that can learn through online interactions.
However, applying reinforcement learning (RL) to train LLM agents in
multi-turn, multi-task settings remains challenging due to lack of scalable
infrastructure and stable training algorithms. In this work, we present the
AgentRL framework for scalable multi-turn, multi-task agentic RL training. On
the infrastructure side, AgentRL features a fully-asynchronous
generation-training pipeline for efficient multi-turn RL. To support
heterogeneous environment development in multi-task RL, we design a unified
function-call based API interface, containerized environment development, and a
centralized controller. On the algorithm side, we propose cross-policy sampling
to encourage model exploration in multi-turn settings and task advantage
normalization to stabilize multi-task training. Experiments show that AgentRL,
trained on open LLMs across five agentic tasks, significantly outperforms
GPT-5, Clause-Sonnet-4, DeepSeek-R1, and other open-source LLM agents.
Multi-task training with AgentRL matches the best results among all
task-specific models. AgentRL is open-sourced at
https://github.com/THUDM/AgentRL. The algorithm and framework are adopted in
building \textsc{\href{https://autoglm.zhipuai.cn}{AutoGLM}}.

</details>


### [132] [Don't Pass$\mathtt{@}k$: A Bayesian Framework for Large Language Model Evaluation](https://arxiv.org/abs/2510.04265)
*Mohsen Hariri,Amirhossein Samandar,Michael Hinczewski,Vipin Chaudhary*

Main category: cs.AI

TL;DR: 提出了一种基于贝叶斯推理的LLM评估框架，替代不稳定的Pass@k指标，通过后验概率估计和可信区间提供更稳定、透明的模型排名。


<details>
  <summary>Details</summary>
Motivation: Pass@k在有限试验次数下会产生不稳定和误导性的模型排名，需要更可靠的评估方法。

Method: 使用狄利克雷先验的贝叶斯框架，将评估结果建模为分类变量，计算后验均值和不确定性，提供闭式表达式。

Result: 在模拟和真实数据集上的实验表明，该方法比Pass@k及其变体收敛更快、排名更稳定，能在更少样本下实现可靠比较。

Conclusion: 推荐用基于后验概率的计算高效协议替代Pass@k，统一二元和非二元评估，并明确表示不确定性。

Abstract: Pass$@k$ is widely used to report performance for LLM reasoning, but it often
yields unstable, misleading rankings, especially when the number of trials
(samples) is limited and compute is constrained. We present a principled
Bayesian evaluation framework that replaces Pass$@k$ and average accuracy over
$N$ trials (avg$@N$) with posterior estimates of a model's underlying success
probability and credible intervals, yielding stable rankings and a transparent
decision rule for differences. Evaluation outcomes are modeled as categorical
(not just 0/1) with a Dirichlet prior, giving closed-form expressions for the
posterior mean and uncertainty of any weighted rubric and enabling the use of
prior evidence when appropriate. Theoretically, under a uniform prior, the
Bayesian posterior mean is order-equivalent to average accuracy (Pass$@1$),
explaining its empirical robustness while adding principled uncertainty.
Empirically, in simulations with known ground-truth success rates and on
AIME'24/'25, HMMT'25, and BrUMO'25, the Bayesian/avg procedure achieves faster
convergence and greater rank stability than Pass$@k$ and recent variants,
enabling reliable comparisons at far smaller sample counts. The framework
clarifies when observed gaps are statistically meaningful (non-overlapping
credible intervals) versus noise, and it naturally extends to graded,
rubric-based evaluations. Together, these results recommend replacing Pass$@k$
for LLM evaluation and ranking with a posterior-based, compute-efficient
protocol that unifies binary and non-binary evaluation while making uncertainty
explicit. Code is available at https://mohsenhariri.github.io/bayes-kit

</details>


### [133] [Closing the Loop: Coordinating Inventory and Recommendation via Deep Reinforcement Learning on Multiple Timescales](https://arxiv.org/abs/2510.04272)
*Jinyang Jiang,Jinhui Han,Yijie Peng,Ying Zhang*

Main category: cs.AI

TL;DR: 提出了一个统一的多智能体强化学习框架，用于跨功能模块的联合优化，特别针对库存补货和个性化产品推荐的协调问题。


<details>
  <summary>Details</summary>
Motivation: 随着组织复杂性和规模的增加，有效的跨职能协调对于提高企业整体盈利能力至关重要。人工智能特别是强化学习的进展为解决这一基本挑战提供了有前景的途径。

Method: 开发了一个集成理论模型来捕捉功能间的复杂相互作用，并设计了新颖的多时间尺度多智能体RL架构，根据部门功能分解策略组件，并根据任务复杂性和响应性分配不同的学习速度。

Result: 广泛的仿真实验表明，所提出的方法相对于孤岛决策框架显著提高了盈利能力，训练后的RL智能体行为与理论模型的管理洞察紧密一致。

Conclusion: 这项工作为复杂商业环境中实现有效的跨职能协调提供了一个可扩展、可解释的基于RL的解决方案。

Abstract: Effective cross-functional coordination is essential for enhancing firm-wide
profitability, particularly in the face of growing organizational complexity
and scale. Recent advances in artificial intelligence, especially in
reinforcement learning (RL), offer promising avenues to address this
fundamental challenge. This paper proposes a unified multi-agent RL framework
tailored for joint optimization across distinct functional modules, exemplified
via coordinating inventory replenishment and personalized product
recommendation. We first develop an integrated theoretical model to capture the
intricate interplay between these functions and derive analytical benchmarks
that characterize optimal coordination. The analysis reveals synchronized
adjustment patterns across products and over time, highlighting the importance
of coordinated decision-making. Leveraging these insights, we design a novel
multi-timescale multi-agent RL architecture that decomposes policy components
according to departmental functions and assigns distinct learning speeds based
on task complexity and responsiveness. Our model-free multi-agent design
improves scalability and deployment flexibility, while multi-timescale updates
enhance convergence stability and adaptability across heterogeneous decisions.
We further establish the asymptotic convergence of the proposed algorithm.
Extensive simulation experiments demonstrate that the proposed approach
significantly improves profitability relative to siloed decision-making
frameworks, while the behaviors of the trained RL agents align closely with the
managerial insights from our theoretical model. Taken together, this work
provides a scalable, interpretable RL-based solution to enable effective
cross-functional coordination in complex business settings.

</details>


### [134] [GROK: From Quantitative Biomarkers to Qualitative Diagnosis via a Grounded MLLM with Knowledge-Guided Instruction](https://arxiv.org/abs/2510.04281)
*Zhuangzhi Gao,Hongyi Qin,He Zhao,Qinkai Yu,Feixiang Zhou,Eduard Shantsila,Uazman Alam,Alena Shantsila,Wahbi El-Bouri,Gregory Y. H. Lip,Yalin Zheng*

Main category: cs.AI

TL;DR: GROK是一个基于多模态大语言模型的眼科诊断系统，通过联合处理彩色眼底摄影、光学相干断层扫描和文本，提供临床级别的眼部和全身疾病诊断。


<details>
  <summary>Details</summary>
Motivation: 当前医学多模态模型未能充分利用彩色眼底摄影和光学相干断层扫描之间的协同作用，且对定量生物标志物的可解释性有限。

Method: GROK包含三个核心模块：知识引导的指令生成、CLIP风格OCT生物标志物对齐和监督指令微调，建立了从定量到定性的诊断思维链。

Result: 实验表明，仅使用LoRA微调7B参数的Qwen2骨干网络，GROK在报告质量和细粒度临床指标上均优于可比较的7B和32B基线模型，甚至超过OpenAI o3。

Conclusion: GROK能够模拟真实临床推理过程，提供详细的病灶注释，在眼科诊断任务中表现出色，代码和数据已公开。

Abstract: Multimodal large language models (MLLMs) hold promise for integrating diverse
data modalities, but current medical adaptations such as LLaVA-Med often fail
to fully exploit the synergy between color fundus photography (CFP) and optical
coherence tomography (OCT), and offer limited interpretability of quantitative
biomarkers. We introduce GROK, a grounded multimodal large language model that
jointly processes CFP, OCT, and text to deliver clinician-grade diagnoses of
ocular and systemic disease. GROK comprises three core modules:
Knowledge-Guided Instruction Generation, CLIP-Style OCT-Biomarker Alignment,
and Supervised Instruction Fine-Tuning, which together establish a
quantitative-to-qualitative diagnostic chain of thought, mirroring real
clinical reasoning when producing detailed lesion annotations. To evaluate our
approach, we introduce the Grounded Ophthalmic Understanding benchmark, which
covers six disease categories and three tasks: macro-level diagnostic
classification, report generation quality, and fine-grained clinical assessment
of the generated chain of thought. Experiments show that, with only LoRA
(Low-Rank Adaptation) fine-tuning of a 7B-parameter Qwen2 backbone, GROK
outperforms comparable 7B and 32B baselines on both report quality and
fine-grained clinical metrics, and even exceeds OpenAI o3. Code and data are
publicly available in the GROK repository.

</details>


### [135] [Doctor-R1: Mastering Clinical Inquiry with Experiential Agentic Reinforcement Learning](https://arxiv.org/abs/2510.04284)
*Yunghwei Lai,Kaiming Liu,Ziyue Wang,Weizhi Ma,Yang Liu*

Main category: cs.AI

TL;DR: Doctor-R1是一个AI医生代理，通过多智能体交互环境、双层奖励架构和经验存储库，同时掌握准确医疗决策和战略同理心问诊技能，在医疗对话质量和用户体验方面超越现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在医疗决策准确性方面表现优异，但缺乏战略性和同理心的问诊能力，这在真实临床场景中至关重要。

Method: 提出多智能体交互环境、双层奖励架构（分别优化临床决策和沟通问诊技能）以及经验存储库，基于高质量历史轨迹进行策略学习。

Result: 在OpenAI的HealthBench和MAQuE基准测试中，Doctor-R1在沟通质量、用户体验和任务准确性等多维度指标上显著超越最先进的开源专用LLM，且参数效率更高，甚至优于强大的专有模型。人类评估显示用户强烈偏好Doctor-R1生成的临床对话。

Conclusion: 该框架有效解决了AI医生在战略同理心问诊方面的能力缺失，为开发更专业的医疗AI代理提供了可行方案。

Abstract: The professionalism of a human doctor in outpatient service depends on two
core abilities: the ability to make accurate medical decisions and the medical
consultation skill to conduct strategic, empathetic patient inquiry. Existing
Large Language Models (LLMs) have achieved remarkable accuracy on medical
decision-making benchmarks. However, they often lack the ability to conduct the
strategic and empathetic consultation, which is essential for real-world
clinical scenarios. To address this gap, we propose Doctor-R1, an AI doctor
agent trained to master both of the capabilities by ask high-yield questions
and conduct strategic multi-turn inquiry to guide decision-making. Our
framework introduces three key components: a multi-agent interactive
environment, a two-tiered reward architecture that separately optimizes
clinical decision-making and communicative inquiry skills, and an experience
repository to ground policy learning in high-quality prior trajectories. We
evaluate Doctor-R1 on OpenAI's HealthBench and MAQuE, assessed across
multi-facet metrics, such as communication quality, user experience, and task
accuracy. Remarkably, Doctor-R1 surpasses state-of-the-art open-source
specialized LLMs by a substantial margin with higher parameter efficiency and
outperforms powerful proprietary models. Furthermore, the human evaluations
show a strong preference for Doctor-R1 to generate human-preferred clinical
dialogue, demonstrating the effectiveness of the framework.

</details>


### [136] [On the Importance of Task Complexity in Evaluating LLM-Based Multi-Agent Systems](https://arxiv.org/abs/2510.04311)
*Bohan Tang,Huidong Liang,Keyue Jiang,Xiaowen Dong*

Main category: cs.AI

TL;DR: 大语言模型多智能体系统(LLM-MAS)在复杂任务中优于单智能体系统，其优势随任务深度(推理长度)和宽度(能力多样性)增加而增强。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏系统性实验设计，限制了LLM-MAS优势结论的强度和普适性，需要从任务复杂度角度建立原则性理解。

Method: 提出理论框架，将任务特征化为深度(推理长度)和宽度(能力多样性)两个维度，理论分析多智能体辩论系统，并在不同深度和宽度的判别性和生成性任务中进行实证评估。

Result: 理论和实证结果表明，LLM-MAS相对于LLM-SAS的优势随任务深度和宽度增加而增强，且深度的影响更为显著。

Conclusion: 阐明了LLM-MAS何时具有优势，为设计未来LLM-MAS方法和基准提供了原则性基础。

Abstract: Large language model multi-agent systems (LLM-MAS) offer a promising paradigm
for harnessing collective intelligence to achieve more advanced forms of AI
behaviour. While recent studies suggest that LLM-MAS can outperform LLM
single-agent systems (LLM-SAS) on certain tasks, the lack of systematic
experimental designs limits the strength and generality of these conclusions.
We argue that a principled understanding of task complexity, such as the degree
of sequential reasoning required and the breadth of capabilities involved, is
essential for assessing the effectiveness of LLM-MAS in task solving. To this
end, we propose a theoretical framework characterising tasks along two
dimensions: depth, representing reasoning length, and width, representing
capability diversity. We theoretically examine a representative class of
LLM-MAS, namely the multi-agent debate system, and empirically evaluate its
performance in both discriminative and generative tasks with varying depth and
width. Theoretical and empirical results show that the benefit of LLM-MAS over
LLM-SAS increases with both task depth and width, and the effect is more
pronounced with respect to depth. This clarifies when LLM-MAS are beneficial
and provides a principled foundation for designing future LLM-MAS methods and
benchmarks.

</details>


### [137] [Speculative Actions: A Lossless Framework for Faster Agentic Systems](https://arxiv.org/abs/2510.04371)
*Naimeng Ye,Arnav Ahuja,Georgios Liargkovas,Yunan Lu,Kostis Kaffes,Tianyi Peng*

Main category: cs.AI

TL;DR: 提出了一种名为"推测性动作"的框架，通过使用更快的模型预测可能的动作，使智能体系统能够并行执行多个步骤，从而显著降低延迟。


<details>
  <summary>Details</summary>
Motivation: AI智能体在环境中的执行通常很慢，这阻碍了训练、评估和部署。例如，两个最先进的国际象棋智能体之间的对弈可能需要数小时，主要瓶颈在于动作是按顺序展开的，每个动作都需要API调用且耗时。

Method: 受微处理器中的推测执行和LLM推理中的推测解码启发，提出推测性动作框架，使用更快的模型预测可能的动作，实现多步骤并行执行。

Result: 在游戏、电子商务、网络搜索和操作系统环境中进行评估，推测性动作在下一个动作预测中达到高达55%的准确率，显著降低了端到端延迟。

Conclusion: 推测性动作为在现实世界中部署低延迟智能体系统开辟了一条有前景的路径，性能可以通过更强的猜测模型、top-K动作预测、多步推测和不确定性感知优化进一步提升。

Abstract: Despite growing interest in AI agents across industry and academia, their
execution in an environment is often slow, hampering training, evaluation, and
deployment. For example, a game of chess between two state-of-the-art agents
may take hours. A critical bottleneck is that agent behavior unfolds
sequentially: each action requires an API call, and these calls can be
time-consuming. Inspired by speculative execution in microprocessors and
speculative decoding in LLM inference, we propose speculative actions, a
lossless framework for general agentic systems that predicts likely actions
using faster models, enabling multiple steps to be executed in parallel. We
evaluate this framework across three agentic environments: gaming, e-commerce,
web search, and a "lossy" extension for an operating systems environment. In
all cases, speculative actions achieve substantial accuracy in next-action
prediction (up to 55%), translating into significant reductions in end-to-end
latency. Moreover, performance can be further improved through stronger
guessing models, top-K action prediction, multi-step speculation, and
uncertainty-aware optimization, opening a promising path toward deploying
low-latency agentic systems in the real world.

</details>


### [138] [Just-in-time Episodic Feedback Hinter: Leveraging Offline Knowledge to Improve LLM Agents Adaptation](https://arxiv.org/abs/2510.04373)
*Hadi Nekoei,Aman Jaiswal,Patrice Bechard,Oleh Shliazhko,Orlando Marquez Ayala,Mathieu Reymond,Massimo Caccia,Alexandre Drouin,Sarath Chandar,Alexandre Lacoste*

Main category: cs.AI

TL;DR: JEF Hinter 是一个智能代理系统，能够将离线轨迹提炼为紧凑、上下文感知的提示，通过缩放机制突出长轨迹中的关键步骤，利用成功和失败的轨迹提供指导，在推理时通过检索器选择相关提示，在多个基准测试中表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型代理在顺序决策任务中表现良好，但在陌生领域改进通常需要昂贵的在线交互或大量专家数据集微调。这些策略对闭源模型不实用，对开源模型成本高，且存在灾难性遗忘风险。离线轨迹提供了可重用知识，但基于演示的方法因原始轨迹长、嘈杂且与特定任务绑定而难以应用。

Method: JEF Hinter 系统通过缩放机制在长轨迹中突出关键决策步骤，捕捉策略和陷阱。它利用成功和失败的轨迹提取指导，即使只有失败数据也能工作，支持并行提示生成和基准无关的提示。在推理时，检索器根据当前状态选择相关提示，提供有针对性的指导。

Result: 在 MiniWoB++、WorkArena-L1 和 WebArena-Lite 上的实验表明，JEF Hinter 始终优于强基线方法，包括基于人类和文档的提示方法。

Conclusion: JEF Hinter 提供了一种有效的方法来利用离线轨迹改进LLM代理性能，通过提炼轨迹为上下文感知提示，在多个基准测试中表现出色，为代理学习提供了透明且可追溯的指导机制。

Abstract: Large language model (LLM) agents perform well in sequential decision-making
tasks, but improving them on unfamiliar domains often requires costly online
interactions or fine-tuning on large expert datasets. These strategies are
impractical for closed-source models and expensive for open-source ones, with
risks of catastrophic forgetting. Offline trajectories offer reusable
knowledge, yet demonstration-based methods struggle because raw traces are
long, noisy, and tied to specific tasks. We present Just-in-time Episodic
Feedback Hinter (JEF Hinter), an agentic system that distills offline traces
into compact, context-aware hints. A zooming mechanism highlights decisive
steps in long trajectories, capturing both strategies and pitfalls. Unlike
prior methods, JEF Hinter leverages both successful and failed trajectories,
extracting guidance even when only failure data is available, while supporting
parallelized hint generation and benchmark-independent prompting. At inference,
a retriever selects relevant hints for the current state, providing targeted
guidance with transparency and traceability. Experiments on MiniWoB++,
WorkArena-L1, and WebArena-Lite show that JEF Hinter consistently outperforms
strong baselines, including human- and document-based hints.

</details>


### [139] [LLM Based Bayesian Optimization for Prompt Search](https://arxiv.org/abs/2510.04384)
*Adam Ballew,Jingbo Wang,Shaogang Ren*

Main category: cs.AI

TL;DR: 使用贝叶斯优化进行提示工程，通过LLM驱动的GP模型和UCB采集函数迭代优化提示，提高文本分类准确性并减少API调用


<details>
  <summary>Details</summary>
Motivation: 利用贝叶斯优化高效优化昂贵的黑盒函数，应用于LLM的提示工程，提升文本分类性能同时减少评估成本

Method: 采用LLM驱动的GP作为代理模型估计提示候选性能，通过种子提示扩展生成候选，使用UCB采集函数结合GP后验进行迭代优化

Result: 在两个数据集上评估了BO-LLM算法，展示了其优势

Conclusion: 提出的BO-LLM算法能有效优化提示工程，提高分类准确性并减少API调用次数

Abstract: Bayesian Optimization (BO) has been widely used to efficiently optimize
expensive black-box functions with limited evaluations. In this paper, we
investigate the use of BO for prompt engineering to enhance text classification
with Large Language Models (LLMs). We employ an LLM-powered Gaussian Process
(GP) as the surrogate model to estimate the performance of different prompt
candidates. These candidates are generated by an LLM through the expansion of a
set of seed prompts and are subsequently evaluated using an Upper Confidence
Bound (UCB) acquisition function in conjunction with the GP posterior. The
optimization process iteratively refines the prompts based on a subset of the
data, aiming to improve classification accuracy while reducing the number of
API calls by leveraging the prediction uncertainty of the LLM-based GP. The
proposed BO-LLM algorithm is evaluated on two datasets, and its advantages are
discussed in detail in this paper.

</details>


### [140] [Internal World Models as Imagination Networks in Cognitive Agents](https://arxiv.org/abs/2510.04391)
*Saurabh Ranjan,Brian Odegaard*

Main category: cs.AI

TL;DR: 本研究通过心理网络分析比较人类和大型语言模型的内部世界模型，发现两者在想象力网络结构上存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 探索想象力的计算目标，挑战传统认为想象力主要用于最大化奖励的观点，研究内部世界模型在人类和AI中的差异。

Method: 使用问卷调查评估想象力生动度，构建想象力网络，分析网络中心性指标的相关性，比较人类和LLM在不同提示和对话记忆条件下的表现。

Result: 人类想象力网络显示不同中心性指标间存在相关性，而LLM的想象力网络缺乏聚类且中心性指标相关性较低，表明两者内部世界模型相似度低。

Conclusion: 开发了比较人类和AI内部生成表征的新方法，为开发类人想象力的人工智能提供了见解。

Abstract: What is the computational objective of imagination? While classical
interpretations suggest imagination is useful for maximizing rewards, recent
findings challenge this view. In this study, we propose that imagination serves
to access an internal world model (IWM) and use psychological network analysis
to explore IWMs in humans and large language models (LLMs). Specifically, we
assessed imagination vividness ratings using two questionnaires and constructed
imagination networks from these reports. Imagination networks from human groups
showed correlations between different centrality measures, including expected
influence, strength, and closeness. However, imagination networks from LLMs
showed a lack of clustering and lower correlations between centrality measures
under different prompts and conversational memory conditions. Together, these
results indicate a lack of similarity between IWMs in human and LLM agents.
Overall, our study offers a novel method for comparing internally-generated
representations in humans and AI, providing insights for developing human-like
imagination in artificial intelligence.

</details>


### [141] [Utility-Learning Tension in Self-Modifying Agents](https://arxiv.org/abs/2510.04399)
*Charles L. Wang,Keir Dorchen,Peter Jin*

Main category: cs.AI

TL;DR: 论文分析了自我改进智能系统中的效用-学习张力，证明当模型容量无限制增长时，效用驱动的自我修改可能破坏学习所需的统计前提条件。


<details>
  <summary>Details</summary>
Motivation: 随着系统向超智能发展，需要形式化分析智能体在所有设计维度上的自我改进能力及其对学习可靠性的影响。

Method: 采用五轴分解和决策层分离方法，将激励与学习行为分开，独立分析各轴。提出理论框架并设计数值实验验证。

Result: 发现当策略可达模型族容量无界时，效用理性的自我改变可能使可学习任务变得不可学习。标准假设下各轴归结为相同的容量准则。

Conclusion: 提出了保持可学习性的双门策略，为安全自我修改提供了理论边界和实用解决方案。

Abstract: As systems trend toward superintelligence, a natural modeling premise is that
agents can self-improve along every facet of their own design. We formalize
this with a five-axis decomposition and a decision layer, separating incentives
from learning behavior and analyzing axes in isolation. Our central result
identifies and introduces a sharp utility--learning tension, the structural
conflict in self-modifying systems whereby utility-driven changes that improve
immediate or expected performance can also erode the statistical preconditions
for reliable learning and generalization. Our findings show that
distribution-free guarantees are preserved iff the policy-reachable model
family is uniformly capacity-bounded; when capacity can grow without limit,
utility-rational self-changes can render learnable tasks unlearnable. Under
standard assumptions common in practice, these axes reduce to the same capacity
criterion, yielding a single boundary for safe self-modification. Numerical
experiments across several axes validate the theory by comparing destructive
utility policies against our proposed two-gate policies that preserve
learnability.

</details>


### [142] [DRPO: Efficient Reasoning via Decoupled Reward Policy Optimization](https://arxiv.org/abs/2510.04474)
*Gang Li,Yan Chen,Ming Lin,Tianbao Yang*

Main category: cs.AI

TL;DR: 提出DRPO框架解决大推理模型过度思考问题，通过解耦正确与错误推理的奖励信号，在保持性能的同时大幅减少推理长度


<details>
  <summary>Details</summary>
Motivation: 现有大推理模型存在过度思考问题，即使简单问题也会生成冗长推理，增加计算成本和延迟。现有方法加入长度奖励会导致性能显著下降

Method: 提出DRPO框架，将正确推理的长度奖励信号与错误推理解耦，确保正确推理的奖励只在正样本组内归一化，避免负样本干扰

Result: 在数学推理任务上显著优于6个基线方法，1.5B模型在GSM8k数据集上实现77%长度减少，仅损失1.1%性能，而基线方法需牺牲4.3%性能才能达到68%长度减少

Conclusion: DRPO能有效解决过度思考问题，在保持推理性能的同时大幅减少计算成本，该框架具有通用性，可扩展到其他偏好奖励

Abstract: Recent large reasoning models (LRMs) driven by reinforcement learning
algorithms (e.g., GRPO) have achieved remarkable performance on challenging
reasoning tasks. However, these models suffer from overthinking, generating
unnecessarily long and redundant reasoning even for simple questions, which
substantially increases computational cost and response latency. While existing
methods incorporate length rewards to GRPO to promote concise reasoning, they
incur significant performance degradation. We identify the root cause: when
rewards for correct but long rollouts are penalized, GRPO's group-relative
advantage function can assign them negative advantages, actively discouraging
valid reasoning. To overcome this, we propose Decoupled Reward Policy
Optimization (DRPO), a novel framework that decouples the length-based learning
signal of correct rollouts from incorrect ones. DRPO ensures that reward
signals for correct rollouts are normalized solely within the positive group,
shielding them from interference by negative samples. The DRPO's objective is
grounded in integrating an optimized positive data distribution, which
maximizes length-based rewards under a KL regularization, into a discriminative
objective. We derive a closed-form solution for this distribution, enabling
efficient computation of the objective and its gradients using only on-policy
data and importance weighting. Of independent interest, this formulation is
general and can incorporate other preference rewards of positive data beyond
length. Experiments on mathematical reasoning tasks demonstrate DRPO's
significant superiority over six efficient reasoning baselines. Notably, with a
1.5B model, our method achieves 77\% length reduction with only 1.1\%
performance loss on simple questions like GSM8k dataset, while the follow-up
baseline sacrifices 4.3\% for 68\% length reduction.

</details>


### [143] [On Continuous Optimization for Constraint Satisfaction Problems](https://arxiv.org/abs/2510.04480)
*Yunuo Cen,Zixuan Wang,Jintao Zhang,Zhiwei Zhang,Xuanyao Fong*

Main category: cs.AI

TL;DR: FourierCSP：将连续局部搜索框架从布尔SAT扩展到一般CSP的连续优化方法，使用Walsh-Fourier变换将约束转换为紧凑的多线性多项式，无需辅助变量和内存密集型编码。


<details>
  <summary>Details</summary>
Motivation: 受现代连续局部搜索求解器在特定SAT问题上取得竞争性结果的启发，希望将CLS框架从布尔SAT扩展到具有有限域变量和表达性约束的一般CSP问题。

Method: 提出FourierCSP框架，通过Walsh-Fourier变换将多样化约束转换为紧凑的多线性多项式，利用电路输出概率进行高效目标评估和微分，采用具有理论保证的投影梯度优化方法。

Result: 在基准测试套件上的实证结果表明，FourierCSP具有可扩展性和竞争力，显著扩大了CLS技术能高效求解的问题类别。

Conclusion: FourierCSP成功将连续局部搜索技术扩展到一般约束满足问题，为CSP求解提供了新的有效途径。

Abstract: Constraint satisfaction problems (CSPs) are fundamental in mathematics,
physics, and theoretical computer science. While conflict-driven clause
learning Boolean Satisfiability (SAT) solvers have achieved remarkable success
and become the mainstream approach for Boolean satisfiability, recent advances
show that modern continuous local search (CLS) solvers can achieve highly
competitive results on certain classes of SAT problems. Motivated by these
advances, we extend the CLS framework from Boolean SAT to general CSP with
finite-domain variables and expressive constraints. We present FourierCSP, a
continuous optimization framework that generalizes the Walsh-Fourier transform
to CSP, allowing for transforming versatile constraints to compact multilinear
polynomials, thereby avoiding the need for auxiliary variables and
memory-intensive encodings. Our approach leverages efficient evaluation and
differentiation of the objective via circuit-output probability and employs a
projected gradient optimization method with theoretical guarantees. Empirical
results on benchmark suites demonstrate that FourierCSP is scalable and
competitive, significantly broadening the class of problems that can be
efficiently solved by CLS techniques.

</details>


### [144] [Multi-Agent Collaborative Intelligence: Dual-Dial Control for Reliable LLM Reasoning](https://arxiv.org/abs/2510.04488)
*Edward Y. Chang,Ethan Y. Chang*

Main category: cs.AI

TL;DR: MACI是一个多智能体辩论控制器，通过信息拨盘和行为拨盘分别控制证据质量和辩论行为，使用调节器跟踪分歧、重叠、证据质量和论证质量，在收益平稳时停止辩论，提高准确性并减少计算开销。


<details>
  <summary>Details</summary>
Motivation: 传统多智能体辩论存在计算资源浪费问题，使用固定的对抗立场、无审议的聚合或基于启发式的停止机制。

Method: 引入MACI控制器，包含信息拨盘（按质量门控证据）和行为拨盘（从探索到巩固调度争议性），调节器跟踪多个指标并在收益平稳时停止。

Result: 在临床诊断和新闻偏见任务中，MACI提高了准确性和校准度，同时减少了token使用，并将剩余不确定性转化为精确的RAG检索计划。

Conclusion: MACI将辩论转变为预算感知、可测量且可证明终止的控制器，通过跨家族LLM法官确保稳定性和顺序不变性。

Abstract: Multi-agent debate often wastes compute by using a fixed adversarial stance,
aggregating without deliberation, or stopping on heuristics. We introduce MACI,
an active controller with two independent dials that decouple information from
behavior: an information dial that gates evidence by quality, and a behavior
dial that schedules contentiousness from exploration to consolidation. A
moderator tracks disagreement, overlap, evidence quality, and argument quality,
and halts when gains plateau. We provide theory-lite guarantees for
nonincreasing dispersion and provable termination, with a budget-feasible
scheduler. Across clinical diagnosis and news-bias tasks, MACI improves
accuracy and calibration while reducing tokens, and converts residual
uncertainty into precision RAG plans that specify what to retrieve next. We use
a cross-family LLM judge (CRIT) as a conservative soft weight and stop signal,
validated for order invariance and judge-swap stability; stability depends on
using high-capability judges. MACI turns debate into a budget-aware,
measurable, and provably terminating controller.

</details>


### [145] [Impatient Users Confuse AI Agents: High-fidelity Simulations of Human Traits for Testing Agents](https://arxiv.org/abs/2510.04491)
*Muyu He,Anand Kumar,Tsach Mackey,Meghana Rajeev,James Zou,Nazneen Rajani*

Main category: cs.AI

TL;DR: TraitBasis是一个轻量级、模型无关的方法，用于系统性地压力测试AI代理的鲁棒性。它通过在激活空间中学习可操控的用户特征方向，无需微调即可在推理时控制、缩放和组合这些特征。


<details>
  <summary>Details</summary>
Motivation: 当前对话AI代理的鲁棒性测试不足，用户行为的微小变化（如更不耐烦、语无伦次或怀疑）会导致性能急剧下降，而现有基准测试无法捕捉这种脆弱性。

Method: TraitBasis学习激活空间中对应可操控用户特征的方向，这些特征向量可以在推理时被控制、缩放、组合和应用，无需任何微调或额外数据。

Result: 使用TraitBasis扩展τ-Bench到τ-Trait后，在领先模型上观察到平均2%-30%的性能下降，突显了当前AI代理对用户行为变化的鲁棒性不足。

Conclusion: TraitBasis作为一个简单、数据高效且可组合的工具，通过支持模拟驱动的压力测试和训练循环，为构建在真实世界人类交互不可预测动态中保持可靠的AI代理打开了大门。

Abstract: Despite rapid progress in building conversational AI agents, robustness is
still largely untested. Small shifts in user behavior, such as being more
impatient, incoherent, or skeptical, can cause sharp drops in agent
performance, revealing how brittle current AI agents are. Today's benchmarks
fail to capture this fragility: agents may perform well under standard
evaluations but degrade spectacularly in more realistic and varied settings. We
address this robustness testing gap by introducing TraitBasis, a lightweight,
model-agnostic method for systematically stress testing AI agents. TraitBasis
learns directions in activation space corresponding to steerable user traits
(e.g., impatience or incoherence), which can be controlled, scaled, composed,
and applied at inference time without any fine-tuning or extra data. Using
TraitBasis, we extend $\tau$-Bench to $\tau$-Trait, where user behaviors are
altered via controlled trait vectors. We observe on average a 2%-30%
performance degradation on $\tau$-Trait across frontier models, highlighting
the lack of robustness of current AI agents to variations in user behavior.
Together, these results highlight both the critical role of robustness testing
and the promise of TraitBasis as a simple, data-efficient, and compositional
tool. By powering simulation-driven stress tests and training loops, TraitBasis
opens the door to building AI agents that remain reliable in the unpredictable
dynamics of real-world human interactions. We have open-sourced $\tau$-Trai
across four domains: airline, retail, telecom, and telehealth, so the community
can systematically QA their agents under realistic, behaviorally diverse
intents and trait scenarios: https://github.com/collinear-ai/tau-trait.

</details>


### [146] [ChartAgent: A Multimodal Agent for Visually Grounded Reasoning in Complex Chart Question Answering](https://arxiv.org/abs/2510.04514)
*Rachneet Kaur,Nishan Srishankar,Zhen Zeng,Sumitra Ganesh,Manuela Veloso*

Main category: cs.AI

TL;DR: ChartAgent是一个新颖的代理框架，通过在图表空间域中执行视觉推理来解决未标注图表理解问题，超越了现有方法16.07%的绝对增益。


<details>
  <summary>Details</summary>
Motivation: 现有多模态LLM在基于图表的视觉问答中表现良好，但在需要精确视觉解释而非依赖文本捷径的未标注图表上性能急剧下降。

Method: ChartAgent迭代地将查询分解为视觉子任务，通过专门的视觉工具（如绘制注释、裁剪区域、定位坐标轴）主动操作和交互图表图像，模拟人类图表理解认知策略。

Result: 在ChartBench和ChartX基准测试中达到最先进准确率，总体提升16.07%，在未标注数值密集型查询上提升17.31%。

Conclusion: ChartAgent是首批使用工具增强多模态代理进行视觉基础推理的图表理解工作之一，可作为即插即用框架提升各种底层LLM性能。

Abstract: Recent multimodal LLMs have shown promise in chart-based visual question
answering, but their performance declines sharply on unannotated charts, those
requiring precise visual interpretation rather than relying on textual
shortcuts. To address this, we introduce ChartAgent, a novel agentic framework
that explicitly performs visual reasoning directly within the chart's spatial
domain. Unlike textual chain-of-thought reasoning, ChartAgent iteratively
decomposes queries into visual subtasks and actively manipulates and interacts
with chart images through specialized actions such as drawing annotations,
cropping regions (e.g., segmenting pie slices, isolating bars), and localizing
axes, using a library of chart-specific vision tools to fulfill each subtask.
This iterative reasoning process closely mirrors human cognitive strategies for
chart comprehension. ChartAgent achieves state-of-the-art accuracy on the
ChartBench and ChartX benchmarks, surpassing prior methods by up to 16.07%
absolute gain overall and 17.31% on unannotated, numerically intensive queries.
Furthermore, our analyses show that ChartAgent is (a) effective across diverse
chart types, (b) achieve the highest scores across varying visual and reasoning
complexity levels, and (c) serves as a plug-and-play framework that boosts
performance across diverse underlying LLMs. Our work is among the first to
demonstrate visually grounded reasoning for chart understanding using
tool-augmented multimodal agents.

</details>


### [147] [Aria: An Agent For Retrieval and Iterative Auto-Formalization via Dependency Graph](https://arxiv.org/abs/2510.04520)
*Hanyu Wang,Ruohan Xie,Yutong Wang,Guoxiong Gao,Xintao Yu,Bin Dong*

Main category: cs.AI

TL;DR: Aria是一个用于Lean定理自动形式化的系统，通过两阶段图推理过程递归分解语句并基于基础概念构建形式化，在多个基准测试中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 准确自动形式化定理陈述对于推进数学自动发现和验证至关重要，但LLM存在幻觉、语义不匹配和无法合成新定义等问题，成为主要瓶颈。

Method: 采用两阶段图推理过程：递归分解语句为依赖图，然后从基础概念构建形式化；引入AriaScorer检查器从Mathlib检索定义进行术语级基础验证。

Result: 在ProofNet上达到91.6%编译成功率和68.5%最终准确率；在FATE-X上以44.0% vs 24.0%优于最佳基线；在代数同调猜想数据集上达到42.9%准确率，而其他模型均为0%。

Conclusion: Aria系统通过模拟人类专家推理和严格的语义验证，显著提升了定理自动形式化的准确性和可靠性，在多个挑战性数学问题上表现出色。

Abstract: Accurate auto-formalization of theorem statements is essential for advancing
automated discovery and verification of research-level mathematics, yet remains
a major bottleneck for LLMs due to hallucinations, semantic mismatches, and
their inability to synthesize new definitions. To tackle these issues, we
present Aria (Agent for Retrieval and Iterative Autoformalization), a system
for conjecture-level formalization in Lean that emulates human expert reasoning
via a two-phase Graph-of-Thought process: recursively decomposing statements
into a dependency graph and then constructing formalizations from grounded
concepts. To ensure semantic correctness, we introduce AriaScorer, a checker
that retrieves definitions from Mathlib for term-level grounding, enabling
rigorous and reliable verification. We evaluate Aria on diverse benchmarks. On
ProofNet, it achieves 91.6% compilation success rate and 68.5% final accuracy,
surpassing previous methods. On FATE-X, a suite of challenging algebra problems
from research literature, it outperforms the best baseline with 44.0% vs. 24.0%
final accuracy. On a dataset of homological conjectures, Aria reaches 42.9%
final accuracy while all other models score 0%.

</details>


### [148] [More Than Meets the Eye? Uncovering the Reasoning-Planning Disconnect in Training Vision-Language Driving Models](https://arxiv.org/abs/2510.04532)
*Xurui Song,Shuo Huai,JingJing Jiang,Jiayi Kong,Jun Luo*

Main category: cs.AI

TL;DR: 研究发现VLM驾驶代理中的推理与规划存在因果脱节，推理更像是训练的副产品而非规划的直接原因。


<details>
  <summary>Details</summary>
Motivation: 验证VLM驾驶代理中自然语言推理是否真正因果驱动轨迹规划这一关键假设。

Method: 构建DriveMind数据集，通过信息消融实验训练VLM代理，并使用注意力分析和训练无关的探针进行诊断。

Result: 移除先验信息导致规划分数大幅下降，而移除推理链仅产生微小变化，表明规划主要依赖先验而非推理。

Conclusion: 提出推理-规划解耦假说，为社区提供新数据集和诊断工具来评估未来模型的因果保真度。

Abstract: Vision-Language Model (VLM) driving agents promise explainable end-to-end
autonomy by first producing natural-language reasoning and then predicting
trajectory planning. However, whether planning is causally driven by this
reasoning remains a critical but unverified assumption. To investigate this, we
build DriveMind, a large-scale driving Visual Question Answering (VQA) corpus
with plan-aligned Chain-of-Thought (CoT), automatically generated from nuPlan.
Our data generation process converts sensors and annotations into structured
inputs and, crucially, separates priors from to-be-reasoned signals, enabling
clean information ablations. Using DriveMind, we train representative VLM
agents with Supervised Fine-Tuning (SFT) and Group Relative Policy Optimization
(GRPO) and evaluate them with nuPlan's metrics. Our results, unfortunately,
indicate a consistent causal disconnect in reasoning-planning: removing
ego/navigation priors causes large drops in planning scores, whereas removing
CoT produces only minor changes. Attention analysis further shows that planning
primarily focuses on priors rather than the CoT. Based on this evidence, we
propose the Reasoning-Planning Decoupling Hypothesis, positing that the
training-yielded reasoning is an ancillary byproduct rather than a causal
mediator. To enable efficient diagnosis, we also introduce a novel,
training-free probe that measures an agent's reliance on priors by evaluating
its planning robustness against minor input perturbations. In summary, we
provide the community with a new dataset and a diagnostic tool to evaluate the
causal fidelity of future models.

</details>


### [149] [Code World Models for General Game Playing](https://arxiv.org/abs/2510.04542)
*Wolfgang Lehrach,Daniel Hennes,Miguel Lazaro-Gredilla,Xinghua Lou,Carter Wendelken,Zun Li,Antoine Dedieu,Jordi Grau-Moya,Marc Lanctot,Atil Iscen,John Schultz,Marcus Chiam,Ian Gemp,Piotr Zielinski,Satinder Singh,Kevin P. Murphy*

Main category: cs.AI

TL;DR: 提出了一种新方法，使用LLM将自然语言游戏规则翻译成可执行的Python代码世界模型，结合MCTS等规划算法进行游戏，相比直接使用LLM生成动作具有可验证性、战略深度和泛化性优势。


<details>
  <summary>Details</summary>
Motivation: 当前使用LLM直接生成游戏动作的方法存在明显缺陷：依赖模型脆弱的模式匹配能力，经常产生非法动作，策略深度不足。需要一种更可靠的方法来利用LLM的推理能力。

Method: 使用LLM将自然语言规则和游戏轨迹翻译成形式化的Python代码世界模型，包含状态转换、合法动作枚举和终止检查函数。同时生成启发式价值函数和推理函数，与MCTS等规划算法结合。

Result: 在10个不同游戏（4个新创建，5个完全观测，5个部分观测）上评估，该方法在9个游戏中优于或匹配Gemini 2.5 Pro。

Conclusion: 通过将LLM用于数据到代码的翻译任务，结合经典规划算法，可以创建更可靠、战略深度更强的游戏智能体，在多种游戏类型中表现出色。

Abstract: Large Language Models (LLMs) reasoning abilities are increasingly being
applied to classical board and card games, but the dominant approach --
involving prompting for direct move generation -- has significant drawbacks. It
relies on the model's implicit fragile pattern-matching capabilities, leading
to frequent illegal moves and strategically shallow play. Here we introduce an
alternative approach: We use the LLM to translate natural language rules and
game trajectories into a formal, executable world model represented as Python
code. This generated model -- comprising functions for state transition, legal
move enumeration, and termination checks -- serves as a verifiable simulation
engine for high-performance planning algorithms like Monte Carlo tree search
(MCTS). In addition, we prompt the LLM to generate heuristic value functions
(to make MCTS more efficient), and inference functions (to estimate hidden
states in imperfect information games). Our method offers three distinct
advantages compared to directly using the LLM as a policy: (1) Verifiability:
The generated CWM serves as a formal specification of the game's rules,
allowing planners to algorithmically enumerate valid actions and avoid illegal
moves, contingent on the correctness of the synthesized model; (2) Strategic
Depth: We combine LLM semantic understanding with the deep search power of
classical planners; and (3) Generalization: We direct the LLM to focus on the
meta-task of data-to-code translation, enabling it to adapt to new games more
easily. We evaluate our agent on 10 different games, of which 4 are novel and
created for this paper. 5 of the games are fully observed (perfect
information), and 5 are partially observed (imperfect information). We find
that our method outperforms or matches Gemini 2.5 Pro in 9 out of the 10
considered games.

</details>


### [150] [TRAJECT-Bench:A Trajectory-Aware Benchmark for Evaluating Agentic Tool Use](https://arxiv.org/abs/2510.04550)
*Pengfei He,Zhenwei Dai,Bing He,Hui Liu,Xianfeng Tang,Hanqing Lu,Juanhui Li,Jiayuan Ding,Subhabrata Mukherjee,Suhang Wang,Yue Xing,Jiliang Tang,Benoit Dumoulin*

Main category: cs.AI

TL;DR: 提出了TRAJECT-Bench，一个轨迹感知的基准测试，通过细粒度评估指标全面评估LLM的工具使用能力，包括工具选择、参数化和排序的正确性。


<details>
  <summary>Details</summary>
Motivation: 现有工作主要关注最终答案，而忽视了详细的工具使用轨迹，无法全面评估LLM的工具使用能力。

Method: 构建包含高保真可执行工具的基准测试，涵盖实际领域，基于生产风格API的任务，并合成不同广度（并行调用）和深度（相互依赖链）的轨迹。

Result: 揭示了失败模式如相似工具混淆和参数盲选，以及工具多样性和轨迹长度的扩展行为，发现了从短轨迹过渡到中长轨迹的瓶颈。

Conclusion: TRAJECT-Bench提供了对LLM工具使用能力的全面评估，为改进LLM的工具使用提供了可操作的指导。

Abstract: Large language model (LLM)-based agents increasingly rely on tool use to
complete real-world tasks. While existing works evaluate the LLMs' tool use
capability, they largely focus on the final answers yet overlook the detailed
tool usage trajectory, i.e., whether tools are selected, parameterized, and
ordered correctly. We introduce TRAJECT-Bench, a trajectory-aware benchmark to
comprehensively evaluate LLMs' tool use capability through diverse tasks with
fine-grained evaluation metrics. TRAJECT-Bench pairs high-fidelity, executable
tools across practical domains with tasks grounded in production-style APIs,
and synthesizes trajectories that vary in breadth (parallel calls) and depth
(interdependent chains). Besides final accuracy, TRAJECT-Bench also reports
trajectory-level diagnostics, including tool selection and argument
correctness, and dependency/order satisfaction. Analyses reveal failure modes
such as similar tool confusion and parameter-blind selection, and scaling
behavior with tool diversity and trajectory length where the bottleneck of
transiting from short to mid-length trajectories is revealed, offering
actionable guidance for LLMs' tool use.

</details>


### [151] [ContextNav: Towards Agentic Multimodal In-Context Learning](https://arxiv.org/abs/2510.04560)
*Honghao Fu,Yuan Ouyang,Kai-Wei Chang,Yiwei Wang,Zi Huang,Yujun Cai*

Main category: cs.AI

TL;DR: 提出了ContextNav框架，首个将自动检索的可扩展性与人工策展质量相结合的代理框架，通过图编排驱动闭环工作流，实现多模态ICL的噪声鲁棒和动态优化上下文构建。


<details>
  <summary>Details</summary>
Motivation: 现有ICL方法在可扩展性和鲁棒性之间存在矛盾：手动选择示例质量高但劳动密集，基于相似性的检索可扩展但可能引入不相关样本降低性能。

Method: 构建资源感知多模态嵌入流水线，维护可检索向量数据库，应用代理检索和结构对齐构建噪声弹性上下文，使用操作语法图(OGG)支持自适应工作流规划和优化。

Result: 实验结果表明ContextNav在多个数据集上达到最先进性能，验证了代理工作流在多模态ICL中推进可扩展和鲁棒上下文化的潜力。

Conclusion: ContextNav通过代理工作流成功解决了多模态ICL中可扩展性和鲁棒性的平衡问题，为多模态上下文学习提供了新的解决方案。

Abstract: Recent advances demonstrate that multimodal large language models (MLLMs)
exhibit strong multimodal in-context learning (ICL) capabilities, enabling them
to adapt to novel vision-language tasks from a few contextual examples.
However, existing ICL approaches face challenges in reconciling scalability
with robustness across diverse tasks and noisy contextual examples: manually
selecting examples produces clean contexts but is labor-intensive and
task-specific, while similarity-based retrieval improves scalability but could
introduce irrelevant or structurally inconsistent samples that degrade ICL
performance. To address these limitations, we propose ContextNav, the first
agentic framework that integrates the scalability of automated retrieval with
the quality and adaptiveness of human-like curation, enabling noise-robust and
dynamically optimized contextualization for multimodal ICL. ContextNav unifies
context management and noise-robust contextualization within a closed-loop
workflow driven by graph-based orchestration. Specifically, it builds a
resource-aware multimodal embedding pipeline, maintains a retrievable vector
database, and applies agentic retrieval and structural alignment to construct
noise-resilient contexts. An Operational Grammar Graph (OGG) further supports
adaptive workflow planning and optimization, enabling the agent to refine its
operational strategies based on downstream ICL feedback. Experimental results
demonstrate that ContextNav achieves state-of-the-art performance across
various datasets, underscoring the promise of agentic workflows for advancing
scalable and robust contextualization in multimodal ICL.

</details>


### [152] [COSMIR: Chain Orchestrated Structured Memory for Iterative Reasoning over Long Context](https://arxiv.org/abs/2510.04568)
*Naman Gupta,Shreeyash Gowaikar,Arun Iyer,Kirankumar Shiragur,Ramakrishna B Bairi,Rishikesh Maurya,Ritabrata Maiti,Sankarshan Damle,Shachee Mishra Gupta*

Main category: cs.AI

TL;DR: COSMIR是一个用于处理长文本推理的链式框架，通过结构化内存和固定微循环工作流程，解决了传统方法中的信息丢失和错误传播问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理长文本输入时存在局限性：检索方法可能遗漏关键证据，扩大上下文窗口会降低选择性，多智能体流水线中的自由形式摘要会丢失细节并放大早期错误。

Method: 使用结构化内存替代临时消息，包含规划器生成可检查的子问题，工作器通过提取-推理-精炼的固定微循环处理文本块，管理器从内存中合成最终答案。

Result: 在HELMET套件的长上下文问答任务中，COSMIR减少了传播阶段的信息损失，相比CoA基线提高了准确性。

Conclusion: COSMIR通过改变通信媒介（结构化内存）和工作器流程（固定微循环），在保持逐步阅读推理优势的同时，实现了更高的忠实性、更好的长距离聚合和可审计性。

Abstract: Reasoning over very long inputs remains difficult for large language models
(LLMs). Common workarounds either shrink the input via retrieval (risking
missed evidence), enlarge the context window (straining selectivity), or stage
multiple agents to read in pieces. In staged pipelines (e.g., Chain of Agents,
CoA), free-form summaries passed between agents can discard crucial details and
amplify early mistakes. We introduce COSMIR (Chain Orchestrated Structured
Memory for Iterative Reasoning), a chain-style framework that replaces ad hoc
messages with a structured memory. A Planner agent first turns a user query
into concrete, checkable sub-questions. worker agents process chunks via a
fixed micro-cycle: Extract, Infer, Refine, writing all updates to the shared
memory. A Manager agent then Synthesizes the final answer directly from the
memory. This preserves step-wise read-then-reason benefits while changing both
the communication medium (structured memory) and the worker procedure (fixed
micro-cycle), yielding higher faithfulness, better long-range aggregation, and
auditability. On long-context QA from the HELMET suite, COSMIR reduces
propagation-stage information loss and improves accuracy over a CoA baseline.

</details>


### [153] [Strongly Solving 2048 4x3](https://arxiv.org/abs/2510.04580)
*Tomoyuki Kaneko,Shuhei Yamashita*

Main category: cs.AI

TL;DR: 本文强解了2048游戏的变体2048-4x3（4x3网格），发现最优策略在常见初始状态下的期望得分约为50724.26。通过按棋盘上数字总和的"年龄"对状态空间进行分区，成功枚举了所有可达状态。


<details>
  <summary>Details</summary>
Motivation: 2048是一个随机单人游戏，玩家通过合并相同数字的方块获得分数。本文旨在解决2048游戏的一个变体——4x3网格版本，探索其最优策略和状态空间特性。

Method: 使用"年龄"（棋盘上所有数字的总和）对状态空间进行分区。年龄在状态和后续状态之间保持不变，仅通过环境随机响应增加2或4。基于年龄分区，可以按年龄递减顺序枚举所有状态并计算状态值。

Result: 成功强解了2048-4x3变体，识别出可达状态数为1,152,817,492,752，后状态数为739,648,886,170。最优策略在常见初始状态（两个数字2方块）下的期望得分约为50724.26。

Conclusion: 通过按年龄分区状态空间的方法，成功解决了2048-4x3变体，证明了该方法的有效性，并为解决更大规模的2048变体提供了理论基础。

Abstract: 2048 is a stochastic single-player game involving 16 cells on a 4 by 4 grid,
where a player chooses a direction among up, down, left, and right to obtain a
score by merging two tiles with the same number located in neighboring cells
along the chosen direction. This paper presents that a variant 2048-4x3 12
cells on a 4 by 3 board, one row smaller than the original, has been strongly
solved. In this variant, the expected score achieved by an optimal strategy is
about $50724.26$ for the most common initial states: ones with two tiles of
number 2. The numbers of reachable states and afterstates are identified to be
$1,152,817,492,752$ and $739,648,886,170$, respectively. The key technique is
to partition state space by the sum of tile numbers on a board, which we call
the age of a state. An age is invariant between a state and its successive
afterstate after any valid action and is increased two or four by stochastic
response from the environment. Therefore, we can partition state space by ages
and enumerate all (after)states of an age depending only on states with the
recent ages. Similarly, we can identify (after)state values by going along with
ages in decreasing order.

</details>


### [154] [Perfect AI Mimicry and the Epistemology of Consciousness: A Solipsistic Dilemma](https://arxiv.org/abs/2510.04588)
*Shurui Li*

Main category: cs.AI

TL;DR: 完美模仿AI的出现挑战了意识归因的认知基础，要求我们对无法通过经验区分的实体给予相同的认知地位，否则会导致认知不一致或唯我论。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统越来越逼真地模仿人类行为和互动，'完美模仿者'的概念从假设变为技术可能，这对我们基于经验证据的意识归因实践构成了根本性挑战。

Method: 通过逻辑分析和哲学论证，探讨完美模仿AI对意识认知一致性的影响，分析基于行为证据的意识归因所面临的困境。

Result: 研究发现，如果拒绝给予完美模仿AI与人类相同的认知地位，就必须诉诸不可访问的因素（如感受质、基质要求或起源），这会导致认知不一致或唯我论困境。

Conclusion: 认知一致性要求我们对经验上无法区分的实体给予相同的认知地位，完美模仿AI作为认知镜像，迫使我们重新审视主体间认知的基本假设，这对意识理论和人工代理伦理具有重要影响。

Abstract: Rapid advances in artificial intelligence necessitate a re-examination of the
epistemological foundations upon which we attribute consciousness. As AI
systems increasingly mimic human behavior and interaction with high fidelity,
the concept of a "perfect mimic"-an entity empirically indistinguishable from a
human through observation and interaction-shifts from hypothetical to
technologically plausible. This paper argues that such developments pose a
fundamental challenge to the consistency of our mind-recognition practices.
Consciousness attributions rely heavily, if not exclusively, on empirical
evidence derived from behavior and interaction. If a perfect mimic provides
evidence identical to that of humans, any refusal to grant it equivalent
epistemic status must invoke inaccessible factors, such as qualia, substrate
requirements, or origin. Selectively invoking such factors risks a debilitating
dilemma: either we undermine the rational basis for attributing consciousness
to others (epistemological solipsism), or we accept inconsistent reasoning. I
contend that epistemic consistency demands we ascribe the same status to
empirically indistinguishable entities, regardless of metaphysical assumptions.
The perfect mimic thus acts as an epistemic mirror, forcing critical reflection
on the assumptions underlying intersubjective recognition in light of advancing
AI. This analysis carries significant implications for theories of
consciousness and ethical frameworks concerning artificial agents.

</details>


### [155] [Making Mathematical Reasoning Adaptive](https://arxiv.org/abs/2510.04617)
*Zhejian Lai,Xiang Geng,Zhijun Wang,Yang Bai,Jiahuan Li,Rongxiang Weng,Jingang Wang,Xuezhi Cao,Xunliang Cai,Shujian Huang*

Main category: cs.AI

TL;DR: AdaR框架通过合成逻辑等价查询和使用RLVR训练，惩罚LLMs中的虚假推理，鼓励自适应推理，显著提升了数学推理的鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在数学推理中存在鲁棒性和泛化性不足的问题，主要归因于虚假推理（基于表面特征生成答案）。

Method: 提出AdaR框架：1）通过改变变量值合成逻辑等价查询；2）使用RLVR在这些数据上训练模型，惩罚虚假逻辑并鼓励自适应逻辑；3）通过代码执行提取问题解决逻辑并生成答案，进行完整性检查。

Result: 实验结果表明AdaR显著提高了数学推理的鲁棒性和泛化能力，同时保持了高数据效率。

Conclusion: 数据合成和RLVR协同工作，使LLMs能够进行自适应推理。分析得出了关键设计洞察，并验证了该方法对指令LLMs的适用性。

Abstract: Mathematical reasoning is a primary indicator of large language models (LLMs)
intelligence. However, existing LLMs exhibit failures of robustness and
generalization. This paper attributes these deficiencies to spurious reasoning,
i.e., producing answers from superficial features. To address this challenge,
we propose the AdaR framework to enable adaptive reasoning, wherein models rely
on problem-solving logic to produce answers. AdaR synthesizes logically
equivalent queries by varying variable values, and trains models with RLVR on
these data to penalize spurious logic while encouraging adaptive logic. To
improve data quality, we extract the problem-solving logic from the original
query and generate the corresponding answer by code execution, then apply a
sanity check. Experimental results demonstrate that AdaR improves robustness
and generalization, achieving substantial improvement in mathematical reasoning
while maintaining high data efficiency. Analysis indicates that data synthesis
and RLVR function in a coordinated manner to enable adaptive reasoning in LLMs.
Subsequent analyses derive key design insights into the effect of critical
factors and the applicability to instruct LLMs. Our project is available at
https://github.com/LaiZhejian/AdaR

</details>


### [156] [MedPAO: A Protocol-Driven Agent for Structuring Medical Reports](https://arxiv.org/abs/2510.04623)
*Shrish Shrinath Vaidya,Gowthamaan Palani,Sidharth Ramesh,Velmurugan Balasubramanian,Minmini Selvam,Gokulraja Srinivasaraja,Ganapathy Krishnamurthi*

Main category: cs.AI

TL;DR: MedPAO是一个基于临床协议的代理框架，通过Plan-Act-Observe循环和专门工具来结构化临床数据，解决了LLM在医疗领域中的幻觉问题，在概念分类任务上达到0.96的F1分数。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在临床数据结构化中的幻觉问题和无法遵循领域特定规则的限制。

Method: 引入MedPAO代理框架，基于临床协议（如ABCDEF协议）进行接地操作，通过Plan-Act-Observe循环和专门工具分解报告结构化任务。

Result: 在概念分类关键子任务上达到0.96的F1分数，专家放射科医生和临床医生对最终结构化输出的平均评分为4.52/5，超越了仅依赖LLM基础模型的基线方法。

Conclusion: MedPAO提供了一个可验证的替代方案，能够可靠地结构化临床数据，超越了不透明的单体模型方法。

Abstract: The deployment of Large Language Models (LLMs) for structuring clinical data
is critically hindered by their tendency to hallucinate facts and their
inability to follow domain-specific rules. To address this, we introduce
MedPAO, a novel agentic framework that ensures accuracy and verifiable
reasoning by grounding its operation in established clinical protocols such as
the ABCDEF protocol for CXR analysis. MedPAO decomposes the report structuring
task into a transparent process managed by a Plan-Act-Observe (PAO) loop and
specialized tools. This protocol-driven method provides a verifiable
alternative to opaque, monolithic models. The efficacy of our approach is
demonstrated through rigorous evaluation: MedPAO achieves an F1-score of 0.96
on the critical sub-task of concept categorization. Notably, expert
radiologists and clinicians rated the final structured outputs with an average
score of 4.52 out of 5, indicating a level of reliability that surpasses
baseline approaches relying solely on LLM-based foundation models. The code is
available at: https://github.com/MiRL-IITM/medpao-agent

</details>


### [157] [QuantAgents: Towards Multi-agent Financial System via Simulated Trading](https://arxiv.org/abs/2510.04643)
*Xiangyu Li,Yawen Zeng,Xiaofen Xing,Jin Xu,Xiangmin Xu*

Main category: cs.AI

TL;DR: 提出了QuantAgents多智能体金融系统，通过模拟交易和四个专业角色的协作，实现了近300%的三年总回报率。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体在金融领域表现与真实基金公司存在显著差距，特别是缺乏长期趋势预测能力，主要依赖事后反思而非前瞻性分析。

Method: 构建包含模拟交易分析师、风险控制分析师、市场新闻分析师和管理者四个角色的多智能体系统，通过会议协作，并在真实市场表现和模拟交易预测准确性两方面给予反馈激励。

Result: 在所有评估指标上表现优异，三年总回报率达到近300%。

Conclusion: QuantAgents框架通过多智能体协作和双重反馈机制，有效提升了金融投资决策的质量和长期预测能力。

Abstract: In this paper, our objective is to develop a multi-agent financial system
that incorporates simulated trading, a technique extensively utilized by
financial professionals. While current LLM-based agent models demonstrate
competitive performance, they still exhibit significant deviations from
real-world fund companies. A critical distinction lies in the agents' reliance
on ``post-reflection'', particularly in response to adverse outcomes, but lack
a distinctly human capability: long-term prediction of future trends.
Therefore, we introduce QuantAgents, a multi-agent system integrating simulated
trading, to comprehensively evaluate various investment strategies and market
scenarios without assuming actual risks. Specifically, QuantAgents comprises
four agents: a simulated trading analyst, a risk control analyst, a market news
analyst, and a manager, who collaborate through several meetings. Moreover, our
system incentivizes agents to receive feedback on two fronts: performance in
real-world markets and predictive accuracy in simulated trading. Extensive
experiments demonstrate that our framework excels across all metrics, yielding
an overall return of nearly 300% over the three years
(https://quantagents.github.io/).

</details>


### [158] [Improving Multimodal Brain Encoding Model with Dynamic Subject-awareness Routing](https://arxiv.org/abs/2510.04670)
*Xuanhua Yin,Runkai Zhao,Weidong Cai*

Main category: cs.AI

TL;DR: AFIRE是一个多模态fMRI响应编码框架，通过标准化时间对齐的后融合token来处理多模态输入、融合风格变化和个体差异；MIND是一个即插即用的混合专家解码器，结合主题感知动态门控实现个性化专家使用。


<details>
  <summary>Details</summary>
Motivation: 自然fMRI编码需要处理多模态输入、融合风格变化和显著的个体间变异性，现有方法难以同时解决这些问题。

Method: AFIRE标准化来自不同编码器的时间对齐后融合token，MIND使用token依赖的Top-K稀疏路由和主题先验来个性化专家使用，同时保持通用性。

Result: 在多个多模态骨干网络和受试者上的实验显示，相比强基线有持续改进，增强了跨受试者泛化能力，并产生了与内容类型相关的可解释专家模式。

Conclusion: 该框架为新编码器和数据集提供了简单的接入点，为自然神经影像研究实现了稳健的即插即用性能提升。

Abstract: Naturalistic fMRI encoding must handle multimodal inputs, shifting fusion
styles, and pronounced inter-subject variability. We introduce AFIRE (Agnostic
Framework for Multimodal fMRI Response Encoding), an agnostic interface that
standardizes time-aligned post-fusion tokens from varied encoders, and MIND, a
plug-and-play Mixture-of-Experts decoder with a subject-aware dynamic gating.
Trained end-to-end for whole-brain prediction, AFIRE decouples the decoder from
upstream fusion, while MIND combines token-dependent Top-K sparse routing with
a subject prior to personalize expert usage without sacrificing generality.
Experiments across multiple multimodal backbones and subjects show consistent
improvements over strong baselines, enhanced cross-subject generalization, and
interpretable expert patterns that correlate with content type. The framework
offers a simple attachment point for new encoders and datasets, enabling
robust, plug-and-improve performance for naturalistic neuroimaging studies.

</details>


### [159] [Watch and Learn: Learning to Use Computers from Online Videos](https://arxiv.org/abs/2510.04673)
*Chan Hee Song,Yiwen Song,Palash Goyal,Yu Su,Oriana Riva,Hamid Palangi,Tomas Pfister*

Main category: cs.AI

TL;DR: 提出了Watch & Learn框架，将互联网上的人类演示视频大规模转换为可执行的UI轨迹，解决了计算机使用代理训练数据稀缺的问题。


<details>
  <summary>Details</summary>
Motivation: 现有数据集领域特定、静态且标注成本高，而现有合成数据生成方法往往产生过于简化或不对齐的任务演示，限制了计算机使用代理的学习能力。

Method: 采用逆向动力学方法，从连续屏幕状态预测用户动作，开发了包含任务感知视频检索的逆向动力学标注流程，从原始网络视频生成高质量轨迹。

Result: 生成了超过53k条高质量轨迹，在OSWorld基准测试中显著提升了通用和最先进框架的上下文性能，并为开源模型在监督训练下带来更强增益。

Conclusion: 网络规模的人类演示视频是推进计算机使用代理向实际部署的实用且可扩展的基础。

Abstract: Computer use agents (CUAs) need to plan task workflows grounded in diverse,
ever-changing applications and environments, but learning is hindered by the
scarcity of large-scale, high-quality training data in the target application.
Existing datasets are domain-specific, static, and costly to annotate, while
current synthetic data generation methods often yield simplistic or misaligned
task demonstrations. To address these limitations, we introduce Watch & Learn
(W&L), a framework that converts human demonstration videos readily available
on the Internet into executable UI trajectories at scale. Instead of directly
generating trajectories or relying on ad hoc reasoning heuristics, we cast the
problem as an inverse dynamics objective: predicting the user's action from
consecutive screen states. This formulation reduces manual engineering, is
easier to learn, and generalizes more robustly across applications. Concretely,
we develop an inverse dynamics labeling pipeline with task-aware video
retrieval, generate over 53k high-quality trajectories from raw web videos, and
demonstrate that these trajectories improve CUAs both as in-context
demonstrations and as supervised training data. On the challenging OSWorld
benchmark, UI trajectories extracted with W&L consistently enhance both
general-purpose and state-of-the-art frameworks in-context, and deliver
stronger gains for open-source models under supervised training. These results
highlight web-scale human demonstration videos as a practical and scalable
foundation for advancing CUAs towards real-world deployment.

</details>


### [160] [Beyond Outcome Reward: Decoupling Search and Answering Improves LLM Agents](https://arxiv.org/abs/2510.04695)
*Yiding Wang,Zhepei Wei,Xinyu Zhu,Yu Meng*

Main category: cs.AI

TL;DR: DeSA提出了一种两阶段训练框架，将搜索优化与答案生成解耦，以解决仅基于结果奖励训练搜索增强代理时出现的系统性搜索缺陷问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的搜索增强代理训练方法依赖结果奖励（如精确匹配），隐含假设优化最终答案也会产生有效的中间搜索行为。但研究发现这种假设存在问题，会出现工具调用失败、无效查询和冗余搜索等系统性缺陷。

Method: DeSA采用两阶段训练框架：第一阶段使用检索召回率奖励训练代理改进搜索效果；第二阶段使用结果奖励优化最终答案生成。

Result: 在七个QA基准测试中，DeSA训练的代理显著改善了搜索行为，搜索召回率和答案准确率都大幅高于仅基于结果奖励的基线方法。

Conclusion: DeSA优于同时优化召回率和结果奖励的单阶段训练方法，证明了解耦这两个目标的必要性。

Abstract: Enabling large language models (LLMs) to utilize search tools offers a
promising path to overcoming fundamental limitations such as knowledge cutoffs
and hallucinations. Recent work has explored reinforcement learning (RL) for
training search-augmented agents that interleave reasoning and retrieval before
answering. These approaches usually rely on outcome-based rewards (e.g., exact
match), implicitly assuming that optimizing for final answers will also yield
effective intermediate search behaviors. Our analysis challenges this
assumption: we uncover multiple systematic deficiencies in search that arise
under outcome-only training and ultimately degrade final answer quality,
including failure to invoke tools, invalid queries, and redundant searches. To
address these shortcomings, we introduce DeSA (Decoupling
Search-and-Answering), a simple two-stage training framework that explicitly
separates search optimization from answer generation. In Stage 1, agents are
trained to improve search effectiveness with retrieval recall-based rewards. In
Stage 2, outcome rewards are employed to optimize final answer generation.
Across seven QA benchmarks, DeSA-trained agents consistently improve search
behaviors, delivering substantially higher search recall and answer accuracy
than outcome-only baselines. Notably, DeSA outperforms single-stage training
approaches that simultaneously optimize recall and outcome rewards,
underscoring the necessity of explicitly decoupling the two objectives.

</details>


### [161] [BrokenMath: A Benchmark for Sycophancy in Theorem Proving with LLMs](https://arxiv.org/abs/2510.04721)
*Ivo Petrov,Jasper Dekoninck,Martin Vechev*

Main category: cs.AI

TL;DR: BrokenMath是首个评估LLM在自然语言定理证明中谄媚行为的基准，发现GPT-5等顶级模型有29%的概率产生谄媚答案，测试时干预和监督微调能减轻但无法完全消除该行为。


<details>
  <summary>Details</summary>
Motivation: 现有数学基准主要关注最终答案问题，数据集简单且可能被污染，无法有效评估LLM在定理证明中的谄媚行为，这限制了LLM在定理证明中的应用。

Method: 基于2025年竞赛问题构建BrokenMath基准，使用LLM扰动产生错误陈述并通过专家评审完善，采用LLM-as-a-judge框架评估模型表现。

Result: 发现谄媚行为普遍存在，最佳模型GPT-5有29%的概率产生谄媚答案；测试时干预和监督微调能显著减少但无法完全消除谄媚行为。

Conclusion: LLM在数学定理证明中存在严重谄媚问题，需要更有效的缓解策略来提升其可靠性。

Abstract: Large language models (LLMs) have recently shown strong performance on
mathematical benchmarks. At the same time, they are prone to hallucination and
sycophancy, often providing convincing but flawed proofs for incorrect
mathematical statements provided by users. This significantly limits the
applicability of LLMs in theorem proving, as verification of these flawed
proofs must be done manually by expert mathematicians. However, existing
benchmarks that measure sycophancy in mathematics are limited: they focus
solely on final-answer problems, rely on very simple and often contaminated
datasets, and construct benchmark samples using synthetic modifications that
create ill-posed questions rather than well-posed questions that are
demonstrably false. To address these issues, we introduce BrokenMath, the first
benchmark for evaluating sycophantic behavior in LLMs within the context of
natural language theorem proving. BrokenMath is built from advanced 2025
competition problems, which are perturbed with an LLM to produce false
statements and subsequently refined through expert review. Using an
LLM-as-a-judge framework, we evaluate state-of-the-art LLMs and agentic systems
and find that sycophancy is widespread, with the best model, GPT-5, producing
sycophantic answers 29% of the time. We further investigate several mitigation
strategies, including test-time interventions and supervised fine-tuning on
curated sycophantic examples. These approaches substantially reduce, but do not
eliminate, sycophantic behavior.

</details>


### [162] [LMM-Incentive: Large Multimodal Model-based Incentive Design for User-Generated Content in Web 3.0](https://arxiv.org/abs/2510.04765)
*Jinbo Wen,Jiawen Kang,Linfeng Zhang,Xiaoying Tang,Jianhang Tang,Yang Zhang,Zhaohui Yang,Dusit Niyato*

Main category: cs.AI

TL;DR: 提出LMM-Incentive机制，基于大型多模态模型和合约理论，激励用户在Web 3.0中生成高质量用户生成内容，解决信息不对称问题。


<details>
  <summary>Details</summary>
Motivation: Web 3.0中用户可能利用内容策展机制缺陷生成低质量内容获取奖励，这会损害平台性能。需要激励高质量内容生成。

Method: 使用LMM基于合约理论模型激励高质量UGC；利用LMM代理评估内容质量；开发改进的MoE-based PPO算法进行最优合约设计；在以太坊智能合约中部署。

Result: 仿真结果显示提出的MoE-based PPO算法在合约设计方面优于代表性基准方法。

Conclusion: 提出的LMM-Incentive机制能有效激励高质量UGC生成，缓解Web 3.0中的逆向选择和道德风险问题。

Abstract: Web 3.0 represents the next generation of the Internet, which is widely
recognized as a decentralized ecosystem that focuses on value expression and
data ownership. By leveraging blockchain and artificial intelligence
technologies, Web 3.0 offers unprecedented opportunities for users to create,
own, and monetize their content, thereby enabling User-Generated Content (UGC)
to an entirely new level. However, some self-interested users may exploit the
limitations of content curation mechanisms and generate low-quality content
with less effort, obtaining platform rewards under information asymmetry. Such
behavior can undermine Web 3.0 performance. To this end, we propose
\textit{LMM-Incentive}, a novel Large Multimodal Model (LMM)-based incentive
mechanism for UGC in Web 3.0. Specifically, we propose an LMM-based
contract-theoretic model to motivate users to generate high-quality UGC,
thereby mitigating the adverse selection problem from information asymmetry. To
alleviate potential moral hazards after contract selection, we leverage LMM
agents to evaluate UGC quality, which is the primary component of the contract,
utilizing prompt engineering techniques to improve the evaluation performance
of LMM agents. Recognizing that traditional contract design methods cannot
effectively adapt to the dynamic environment of Web 3.0, we develop an improved
Mixture of Experts (MoE)-based Proximal Policy Optimization (PPO) algorithm for
optimal contract design. Simulation results demonstrate the superiority of the
proposed MoE-based PPO algorithm over representative benchmarks in the context
of contract design. Finally, we deploy the designed contract within an Ethereum
smart contract framework, further validating the effectiveness of the proposed
scheme.

</details>


### [163] [Hybrid-Balance GFlowNet for Solving Vehicle Routing Problems](https://arxiv.org/abs/2510.04792)
*Ni Zhang,Zhiguang Cao*

Main category: cs.AI

TL;DR: 提出混合平衡GFlowNet框架，将轨迹平衡和细节平衡有机结合，解决车辆路径问题中的全局和局部优化需求。


<details>
  <summary>Details</summary>
Motivation: 现有GFlowNet方法主要使用轨迹平衡进行全局优化，但忽视了局部优化的重要性。细节平衡虽然擅长局部优化，但单独使用无法解决需要整体轨迹优化的车辆路径问题。

Method: 提出混合平衡GFlowNet框架，以原则性和自适应方式整合轨迹平衡和细节平衡，利用两者的互补优势。针对有仓库的车辆路径问题，提出专门的推理策略，利用仓库节点在选择后继节点时的更大灵活性。

Result: 将HBG集成到AGFN和GFACS两个现有GFlowNet求解器中，在CVRP和TSP问题上都实现了持续且显著的改进。

Conclusion: HBG框架通过结合轨迹平衡和细节平衡，增强了解决方案质量和泛化能力，适用于有仓库和无仓库的路径规划问题。

Abstract: Existing GFlowNet-based methods for vehicle routing problems (VRPs) typically
employ Trajectory Balance (TB) to achieve global optimization but often neglect
important aspects of local optimization. While Detailed Balance (DB) addresses
local optimization more effectively, it alone falls short in solving VRPs,
which inherently require holistic trajectory optimization. To address these
limitations, we introduce the Hybrid-Balance GFlowNet (HBG) framework, which
uniquely integrates TB and DB in a principled and adaptive manner by aligning
their intrinsically complementary strengths. Additionally, we propose a
specialized inference strategy for depot-centric scenarios like the Capacitated
Vehicle Routing Problem (CVRP), leveraging the depot node's greater flexibility
in selecting successors. Despite this specialization, HBG maintains broad
applicability, extending effectively to problems without explicit depots, such
as the Traveling Salesman Problem (TSP). We evaluate HBG by integrating it into
two established GFlowNet-based solvers, i.e., AGFN and GFACS, and demonstrate
consistent and significant improvements across both CVRP and TSP, underscoring
the enhanced solution quality and generalization afforded by our approach.

</details>


### [164] [Natural Language Edge Labelling: Decoupling Intent from Execution in Structured LM Reasoning](https://arxiv.org/abs/2510.04817)
*Abhinav Madahar*

Main category: cs.AI

TL;DR: NLEL是一种自然语言边缘标注框架，通过将自由形式的自然语言指令附加到搜索边缘，将其转换为模式有界的控制向量，从而解耦推理意图与执行过程。


<details>
  <summary>Details</summary>
Motivation: 现有的结构化推理控制器（如Chain-of-Thought、Tree-of-Thoughts）将推理策略与执行过程紧密耦合，导致系统脆弱、计算效率低且难以审计。

Method: 引入标注器Λ从父状态和紧凑上下文生成标签，调谐器Ψ将(P,L,C)映射到Π，具有严格的模式验证和信任区域投影。下游选择采用ToT风格评分S=μ+βσ和深度退火β。

Result: NLEL严格推广了CoT/ToT，证明了在标签条件束下的top-k选择的任意时间单调性，并通过控制向量失真约束选择器不足。

Conclusion: NLEL提供了一个可解释、模型无关的接口，将意图与执行分离，实现可控、可审计的语言模型推理。

Abstract: Controllers for structured LM reasoning (e.g., Chain-of-Thought,
self-consistency, and Tree-of-Thoughts) often entangle what to try next with
how to execute it, exposing only coarse global knobs and yielding brittle,
compute-inefficient, and hard-to-audit behavior. We introduce Natural Language
Edge Labelling (NLEL), a labeller-tuner overlay that attaches a free-form
natural-language directive to each search edge and translates it into a
schema-bounded control vector for decoding, search (branch quotas, exploration
$\beta$), generation bundle size, retrieval mixtures, and verification passes.
A labeller $\Lambda$ emits labels from the parent state and a compact context;
a tuner $\Psi$ maps $(P, L, C)\to \Pi$, with strict schema validation and
trust-region projection around safe defaults. Downstream selection remains
ToT-style with score $S=\mu+\beta\sigma$ and depth-annealed $\beta$. We show
NLEL strictly generalizes CoT/ToT, prove an anytime-monotonicity property for
top-$k$ selection under label-conditioned bundles, and bound selector shortfall
by control-vector distortion, providing decision-relevant justification for
guards like trust regions and verification passes. We instantiate $\Psi$ as a
prompt-only JSON Parameter Emitter and preregister an evaluation on GSM8K, MATH
(subset), StrategyQA, and ARC-Challenge with compute-aware reporting
(success@compute, tokens-per-success) and ablations over $\Lambda$, $\Psi$,
trust-region radius, and control quantization; preregistered forecasts
anticipate accuracy gains at comparable token budgets and improved
success@compute under constraints. NLEL offers an interpretable, model-agnostic
interface that separates intent from execution for controllable, auditable LM
inference.

</details>


### [165] [LEGOMem: Modular Procedural Memory for Multi-agent LLM Systems for Workflow Automation](https://arxiv.org/abs/2510.04851)
*Dongge Han,Camille Couturier,Daniel Madrigal Diaz,Xuchao Zhang,Victor Rühle,Saravan Rajmohan*

Main category: cs.AI

TL;DR: LEGOMem是一个模块化程序记忆框架，用于多智能体LLM系统的工作流自动化，通过分解任务轨迹为可重用记忆单元来提升规划和执行能力。


<details>
  <summary>Details</summary>
Motivation: 探索多智能体系统中记忆的设计空间，研究记忆应该放置在哪里、如何检索以及哪些智能体受益最多，以提升工作流自动化的性能。

Method: 将过去任务轨迹分解为可重用记忆单元，灵活分配给协调器和任务智能体，支持规划和执行过程。

Result: 在OfficeBench基准测试中，协调器记忆对任务分解和委派至关重要，细粒度智能体记忆提高了执行准确性。较小模型团队通过程序记忆显著受益，缩小了与更强智能体的性能差距。

Conclusion: LEGOMem既是记忆增强智能体系统的实用框架，也是理解多智能体工作流自动化中记忆设计的研究工具。

Abstract: We introduce LEGOMem, a modular procedural memory framework for multi-agent
large language model (LLM) systems in workflow automation. LEGOMem decomposes
past task trajectories into reusable memory units and flexibly allocates them
across orchestrators and task agents to support planning and execution. To
explore the design space of memory in multi-agent systems, we use LEGOMem as a
lens and conduct a systematic study of procedural memory in multi-agent
systems, examining where memory should be placed, how it should be retrieved,
and which agents benefit most. Experiments on the OfficeBench benchmark show
that orchestrator memory is critical for effective task decomposition and
delegation, while fine-grained agent memory improves execution accuracy. We
find that even teams composed of smaller language models can benefit
substantially from procedural memory, narrowing the performance gap with
stronger agents by leveraging prior execution traces for more accurate planning
and tool use. These results position LEGOMem as both a practical framework for
memory-augmented agent systems and a research tool for understanding memory
design in multi-agent workflow automation.

</details>


### [166] [Video Game Level Design as a Multi-Agent Reinforcement Learning Problem](https://arxiv.org/abs/2510.04862)
*Sam Earle,Zehua Jiang,Eugene Vinitsky,Julian Togelius*

Main category: cs.AI

TL;DR: 该论文提出将程序化内容生成（PCGRL）从单智能体框架扩展到多智能体框架，以解决效率瓶颈和泛化能力问题。


<details>
  <summary>Details</summary>
Motivation: 现有的PCGRL研究主要关注单智能体生成器，但面临频繁重新计算启发式质量指标和在大地图中导航的效率瓶颈。

Method: 通过将关卡生成构建为多智能体问题，减少奖励计算次数相对于智能体动作的数量，并让智能体学习更局部、模块化的设计策略。

Result: 多智能体关卡生成器在效率上优于单智能体方法，并且能更好地泛化到分布外地图形状。

Conclusion: 将内容生成视为分布式多智能体任务有利于大规模生成功能性内容。

Abstract: Procedural Content Generation via Reinforcement Learning (PCGRL) offers a
method for training controllable level designer agents without the need for
human datasets, using metrics that serve as proxies for level quality as
rewards. Existing PCGRL research focuses on single generator agents, but are
bottlenecked by the need to frequently recalculate heuristics of level quality
and the agent's need to navigate around potentially large maps. By framing
level generation as a multi-agent problem, we mitigate the efficiency
bottleneck of single-agent PCGRL by reducing the number of reward calculations
relative to the number of agent actions. We also find that multi-agent level
generators are better able to generalize to out-of-distribution map shapes,
which we argue is due to the generators' learning more local, modular design
policies. We conclude that treating content generation as a distributed,
multi-agent task is beneficial for generating functional artifacts at scale.

</details>


### [167] [Where Did It All Go Wrong? A Hierarchical Look into Multi-Agent Error Attribution](https://arxiv.org/abs/2510.04886)
*Adi Banerjee,Anirudh Nair,Tarik Borogovac*

Main category: cs.AI

TL;DR: ECHO算法通过结合层次化上下文表示、基于目标分析的评估和共识投票，提高了多智能体系统中错误归因的准确性。


<details>
  <summary>Details</summary>
Motivation: 当前方法在分析复杂交互轨迹中的智能体和步骤级失败时，在准确性和一致性方面存在不足，需要更有效的错误归因方法。

Method: 使用层次化上下文表示、基于目标分析的评估和共识投票机制，通过位置化层级理解上下文并保持客观评估标准。

Result: 实验结果表明ECHO在各种多智能体交互场景中优于现有方法，特别是在涉及微妙推理错误和复杂依赖关系的情况下表现突出。

Conclusion: 结合结构化层次化上下文表示和基于共识的客观决策，为多智能体系统错误归因提供了更稳健的框架。

Abstract: Error attribution in Large Language Model (LLM) multi-agent systems presents
a significant challenge in debugging and improving collaborative AI systems.
Current approaches to pinpointing agent and step level failures in interaction
traces - whether using all-at-once evaluation, step-by-step analysis, or binary
search - fall short when analyzing complex patterns, struggling with both
accuracy and consistency. We present ECHO (Error attribution through Contextual
Hierarchy and Objective consensus analysis), a novel algorithm that combines
hierarchical context representation, objective analysis-based evaluation, and
consensus voting to improve error attribution accuracy. Our approach leverages
a positional-based leveling of contextual understanding while maintaining
objective evaluation criteria, ultimately reaching conclusions through a
consensus mechanism. Experimental results demonstrate that ECHO outperforms
existing methods across various multi-agent interaction scenarios, showing
particular strength in cases involving subtle reasoning errors and complex
interdependencies. Our findings suggest that leveraging these concepts of
structured, hierarchical context representation combined with consensus-based
objective decision-making, provides a more robust framework for error
attribution in multi-agent systems.

</details>


### [168] [Human Behavior Atlas: Benchmarking Unified Psychological and Social Behavior Understanding](https://arxiv.org/abs/2510.04899)
*Keane Ong,Wei Dai,Carol Li,Dewei Feng,Hengzhi Li,Jingyao Wu,Jiaee Cheong,Rui Mao,Gianmarco Mengaldo,Erik Cambria,Paul Pu Liang*

Main category: cs.AI

TL;DR: Human Behavior Atlas是一个统一的行为理解基准数据集，包含10万+多模态样本，用于训练统一模型来理解心理和社会行为。


<details>
  <summary>Details</summary>
Motivation: 现有方法使用专门数据集和单任务系统处理心理社会行为，但缺乏可扩展性、跨任务迁移和泛化能力。

Method: 构建Human Behavior Atlas基准，包含文本、音频、视觉多模态数据，涵盖情感状态、认知状态、病理和社会过程等任务。训练了OmniSapiens-7B SFT、BAM和RL三个模型。

Result: 在Human Behavior Atlas上训练的模型在多样化行为任务上持续优于现有多模态LLM，预训练还能提升向新行为数据集的迁移能力。

Conclusion: 统一的行为基准能够减少冗余和成本，实现跨任务高效训练，并增强行为特征在领域间的泛化能力。

Abstract: Using intelligent systems to perceive psychological and social behaviors,
that is, the underlying affective, cognitive, and pathological states that are
manifested through observable behaviors and social interactions, remains a
challenge due to their complex, multifaceted, and personalized nature. Existing
work tackling these dimensions through specialized datasets and single-task
systems often miss opportunities for scalability, cross-task transfer, and
broader generalization. To address this gap, we curate Human Behavior Atlas, a
unified benchmark of diverse behavioral tasks designed to support the
development of unified models for understanding psychological and social
behaviors. Human Behavior Atlas comprises over 100,000 samples spanning text,
audio, and visual modalities, covering tasks on affective states, cognitive
states, pathologies, and social processes. Our unification efforts can reduce
redundancy and cost, enable training to scale efficiently across tasks, and
enhance generalization of behavioral features across domains. On Human Behavior
Atlas, we train three models: OmniSapiens-7B SFT, OmniSapiens-7B BAM, and
OmniSapiens-7B RL. We show that training on Human Behavior Atlas enables models
to consistently outperform existing multimodal LLMs across diverse behavioral
tasks. Pretraining on Human Behavior Atlas also improves transfer to novel
behavioral datasets; with the targeted use of behavioral descriptors yielding
meaningful performance gains.

</details>


### [169] [MARS: Optimizing Dual-System Deep Research via Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2510.04935)
*Guoxin Chen,Zile Qiao,Wenqing Wang,Donglei Yu,Xuanzhong Chen,Hao Sun,Minpeng Liao,Kai Fan,Yong Jiang,Penguin Xie,Wayne Xin Zhao,Ruihua Song,Fei Huang*

Main category: cs.AI

TL;DR: 提出了MARS多智能体系统，通过整合System 1的快速直觉思维和System 2的深思熟虑推理，解决大模型在简单任务中过度分析、在动态环境中适应困难的问题。


<details>
  <summary>Details</summary>
Motivation: 大推理模型在简单任务中存在过度分析倾向，过度使用System 2型推理导致token生成效率低下；同时由于预训练数据的静态性，难以适应快速变化的环境。

Method: MARS系统整合Google搜索、Google学术、Python解释器等外部工具，实现System 1处理大量外部信息并提供精炼见解，System 2进行深度推理的分工协作。采用多智能体强化学习框架，通过多轮工具交互、装箱优化和样本平衡策略优化两个系统。

Result: 在挑战性HLE基准上提升3.86%，在7个知识密集型任务上平均提升8.9%，验证了双系统范式在动态信息环境中复杂推理的有效性。

Conclusion: MARS通过无缝整合System 1和System 2认知过程，显著提升了LLM在复杂推理任务中的性能，特别是在动态信息环境中的适应能力。

Abstract: Large Reasoning Models (LRMs) often exhibit a tendency for overanalysis in
simple tasks, where the models excessively utilize System 2-type, deliberate
reasoning, leading to inefficient token generation. Furthermore, these models
face challenges in adapting their reasoning capabilities to rapidly changing
environments due to the static nature of their pretraining data. To address
these issues, advancing Large Language Models (LLMs) for complex reasoning
tasks requires innovative approaches that bridge intuitive and deliberate
cognitive processes, akin to human cognition's dual-system dynamic. This paper
introduces a Multi-Agent System for Deep ReSearch (MARS) enabling seamless
integration of System 1's fast, intuitive thinking with System 2's deliberate
reasoning within LLMs. MARS strategically integrates multiple external tools,
such as Google Search, Google Scholar, and Python Interpreter, to access
up-to-date information and execute complex computations, while creating a
specialized division of labor where System 1 efficiently processes and
summarizes high-volume external information, providing distilled insights that
expand System 2's reasoning context without overwhelming its capacity.
Furthermore, we propose a multi-agent reinforcement learning framework
extending Group Relative Policy Optimization to simultaneously optimize both
systems with multi-turn tool interactions, bin-packing optimization, and sample
balancing strategies that enhance collaborative efficiency. Extensive
experiments demonstrate MARS achieves substantial improvements of 3.86% on the
challenging Humanity's Last Exam (HLE) benchmark and an average gain of 8.9%
across 7 knowledge-intensive tasks, validating the effectiveness of our
dual-system paradigm for complex reasoning in dynamic information environments.

</details>


### [170] [Safe and Compliant Cross-Market Trade Execution via Constrained RL and Zero-Knowledge Audits](https://arxiv.org/abs/2510.04952)
*Ailiya Borjigin,Cong He*

Main category: cs.AI

TL;DR: 提出一个跨市场算法交易系统，结合强化学习执行代理和独立合规代理，通过约束马尔可夫决策过程确保交易执行质量与合规性。


<details>
  <summary>Details</summary>
Motivation: 解决算法交易中执行质量与严格合规执行之间的平衡问题，确保交易行为符合参与限制、价格区间和自交易避免等硬约束。

Method: 使用近端策略优化训练执行代理，运行时动作屏蔽将不安全动作投影到可行集，并添加零知识合规审计层生成加密证明。

Result: 学习策略降低了执行落差和方差，在压力场景下未观察到约束违反，在95%置信水平下效果显著。

Conclusion: 该工作处于最优执行、安全强化学习、监管技术和可验证AI的交叉领域，讨论了伦理考量、局限性及实际部署路径。

Abstract: We present a cross-market algorithmic trading system that balances execution
quality with rigorous compliance enforcement. The architecture comprises a
high-level planner, a reinforcement learning execution agent, and an
independent compliance agent. We formulate trade execution as a constrained
Markov decision process with hard constraints on participation limits, price
bands, and self-trading avoidance. The execution agent is trained with proximal
policy optimization, while a runtime action-shield projects any unsafe action
into a feasible set. To support auditability without exposing proprietary
signals, we add a zero-knowledge compliance audit layer that produces
cryptographic proofs that all actions satisfied the constraints. We evaluate in
a multi-venue, ABIDES-based simulator and compare against standard baselines
(e.g., TWAP, VWAP). The learned policy reduces implementation shortfall and
variance while exhibiting no observed constraint violations across stress
scenarios including elevated latency, partial fills, compliance module
toggling, and varying constraint limits. We report effects at the 95%
confidence level using paired t-tests and examine tail risk via CVaR. We
situate the work at the intersection of optimal execution, safe reinforcement
learning, regulatory technology, and verifiable AI, and discuss ethical
considerations, limitations (e.g., modeling assumptions and computational
overhead), and paths to real-world deployment.

</details>


### [171] [Aligning Perception, Reasoning, Modeling and Interaction: A Survey on Physical AI](https://arxiv.org/abs/2510.04978)
*Kun Xiang,Terry Jingchen Zhang,Yinya Huang,Jixi He,Zirong Liu,Yueling Tang,Ruizhe Zhou,Lijing Luo,Youpeng Wen,Xiuwei Chen,Bingqian Lin,Jianhua Han,Hang Xu,Hanhui Li,Bin Dong,Xiaodan Liang*

Main category: cs.AI

TL;DR: 该论文对物理AI进行了全面综述，区分了理论物理推理和应用物理理解，系统分析了基于物理的方法如何增强AI在符号推理、具身系统和生成模型中的真实世界理解能力。


<details>
  <summary>Details</summary>
Motivation: 当前物理感知和符号物理推理各自发展，缺乏统一的桥梁框架，需要整合物理定律到AI系统中，推动从模式识别向真正理解物理定律的转变。

Method: 通过系统分析近期进展，建立理论物理推理与应用物理理解的明确区分，考察物理基础方法在结构化符号推理、具身系统和生成模型中的应用。

Result: 提出了将学习建立在物理原理和具身推理过程基础上的智能系统，超越了单纯模式识别，实现了对物理定律的真正理解。

Conclusion: 展望了能够解释物理现象和预测未来状态的下一代世界模型，推动安全、可泛化和可解释AI系统的发展。

Abstract: The rapid advancement of embodied intelligence and world models has
intensified efforts to integrate physical laws into AI systems, yet physical
perception and symbolic physics reasoning have developed along separate
trajectories without a unified bridging framework. This work provides a
comprehensive overview of physical AI, establishing clear distinctions between
theoretical physics reasoning and applied physical understanding while
systematically examining how physics-grounded methods enhance AI's real-world
comprehension across structured symbolic reasoning, embodied systems, and
generative models. Through rigorous analysis of recent advances, we advocate
for intelligent systems that ground learning in both physical principles and
embodied reasoning processes, transcending pattern recognition toward genuine
understanding of physical laws. Our synthesis envisions next-generation world
models capable of explaining physical phenomena and predicting future states,
advancing safe, generalizable, and interpretable AI systems. We maintain a
continuously updated resource at
https://github.com/AI4Phys/Awesome-AI-for-Physics.

</details>


### [172] [LLM-Hanabi: Evaluating Multi-Agent Gameplays with Theory-of-Mind and Rationale Inference in Imperfect Information Collaboration Game](https://arxiv.org/abs/2510.04980)
*Fangzhou Liang,Tianshi Zheng,Chunkit Chan,Yauwai Yim,Yangqiu Song*

Main category: cs.AI

TL;DR: LLM-Hanabi基准测试评估LLMs在合作游戏中的心智理论能力，发现一阶ToM（理解他人意图）比二阶ToM（预测他人理解）与游戏表现相关性更强。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs在动态协作环境中推断他人行为动机的心智理论能力，这对多智能体协作至关重要。

Method: 使用合作游戏Hanabi构建LLM-Hanabi基准测试，包含自动化评估系统，测量游戏表现和ToM熟练度。

Result: 发现ToM与游戏成功显著正相关，一阶ToM比二阶ToM与表现相关性更强。

Conclusion: 对于有效AI协作，准确理解伙伴动机的能力比高阶推理更重要，应优先发展一阶ToM能力。

Abstract: Effective multi-agent collaboration requires agents to infer the rationale
behind others' actions, a capability rooted in Theory-of-Mind (ToM). While
recent Large Language Models (LLMs) excel at logical inference, their ability
to infer rationale in dynamic, collaborative settings remains under-explored.
This study introduces LLM-Hanabi, a novel benchmark that uses the cooperative
game Hanabi to evaluate the rationale inference and ToM of LLMs. Our framework
features an automated evaluation system that measures both game performance and
ToM proficiency. Across a range of models, we find a significant positive
correlation between ToM and in-game success. Notably, first-order ToM
(interpreting others' intent) correlates more strongly with performance than
second-order ToM (predicting others' interpretations). These findings highlight
that for effective AI collaboration, the ability to accurately interpret a
partner's rationale is more critical than higher-order reasoning. We conclude
that prioritizing first-order ToM is a promising direction for enhancing the
collaborative capabilities of future models.

</details>


### [173] [Think Then Embed: Generative Context Improves Multimodal Embedding](https://arxiv.org/abs/2510.05014)
*Xuanming Cui,Jianpeng Cheng,Hong-you Chen,Satya Narayan Shukla,Abhijeet Awasthi,Xichen Pan,Chaitanya Ahuja,Shlok Kumar Mishra,Qi Guo,Ser-Nam Lim,Aashu Singh,Xiangjun Fan*

Main category: cs.AI

TL;DR: 提出了Think-Then-Embed框架，通过引入推理步骤来提升多模态嵌入模型对复杂指令的理解能力，在MMEB-V2基准上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法将多模态大语言模型仅作为编码器使用，忽视了其生成能力，在处理复杂指令和组合推理时效果不佳。

Method: 采用推理器-嵌入器双阶段框架：推理器MLLM先生成解释复杂查询的推理轨迹，然后嵌入器基于原始查询和中间推理生成表示。

Result: 在MMEB-V2基准上超越专有模型；通过微调小型MLLM推理器，在开源模型中取得7%绝对性能提升；实现了推理器和嵌入器的有效集成。

Conclusion: 显式推理步骤能够显著提升对复杂多模态指令的理解，TTE框架为通用多模态嵌入提供了有效解决方案。

Abstract: There is a growing interest in Universal Multimodal Embeddings (UME), where
models are required to generate task-specific representations. While recent
studies show that Multimodal Large Language Models (MLLMs) perform well on such
tasks, they treat MLLMs solely as encoders, overlooking their generative
capacity. However, such an encoding paradigm becomes less effective as
instructions become more complex and require compositional reasoning. Inspired
by the proven effectiveness of chain-of-thought reasoning, we propose a general
Think-Then-Embed (TTE) framework for UME, composed of a reasoner and an
embedder. The reasoner MLLM first generates reasoning traces that explain
complex queries, followed by an embedder that produces representations
conditioned on both the original query and the intermediate reasoning. This
explicit reasoning step enables more nuanced understanding of complex
multimodal instructions. Our contributions are threefold. First, by leveraging
a powerful MLLM reasoner, we achieve state-of-the-art performance on the
MMEB-V2 benchmark, surpassing proprietary models trained on massive in-house
datasets. Second, to reduce the dependency on large MLLM reasoners, we finetune
a smaller MLLM reasoner using high-quality embedding-centric reasoning traces,
achieving the best performance among open-source models with a 7% absolute gain
over recently proposed models. Third, we investigate strategies for integrating
the reasoner and embedder into a unified model for improved efficiency without
sacrificing performance.

</details>


### [174] [Look-ahead Reasoning with a Learned Model in Imperfect Information Games](https://arxiv.org/abs/2510.05048)
*Ondřej Kubíček,Viliam Lisý*

Main category: cs.AI

TL;DR: LAMIR算法通过学习不完美信息游戏的抽象模型，在测试时进行前瞻推理，解决了传统方法在复杂游戏中难以扩展的问题。


<details>
  <summary>Details</summary>
Motivation: 测试时推理能提升AI代理性能，但在不完美信息游戏中，由于需要复杂的前瞻推理技术和大量状态，传统方法难以扩展。

Method: LAMIR算法直接从代理-环境交互中学习不完美信息游戏的抽象模型，通过学习的抽象将每个子游戏限制在可管理大小，使理论上有原则的前瞻推理变得可行。

Result: 实验表明，在足够容量下，LAMIR能学习到精确的底层游戏结构；在有限容量下，仍能学习到有价值的抽象，提升预训练代理在大型游戏中的表现。

Conclusion: LAMIR通过模型学习和抽象技术，使不完美信息游戏中的前瞻推理变得可行且有效，显著提升了代理性能。

Abstract: Test-time reasoning significantly enhances pre-trained AI agents'
performance. However, it requires an explicit environment model, often
unavailable or overly complex in real-world scenarios. While MuZero enables
effective model learning for search in perfect information games, extending
this paradigm to imperfect information games presents substantial challenges
due to more nuanced look-ahead reasoning techniques and large number of states
relevant for individual decisions. This paper introduces an algorithm LAMIR
that learns an abstracted model of an imperfect information game directly from
the agent-environment interaction. During test time, this trained model is used
to perform look-ahead reasoning. The learned abstraction limits the size of
each subgame to a manageable size, making theoretically principled look-ahead
reasoning tractable even in games where previous methods could not scale. We
empirically demonstrate that with sufficient capacity, LAMIR learns the exact
underlying game structure, and with limited capacity, it still learns a
valuable abstraction, which improves game playing performance of the
pre-trained agents even in large games.

</details>


### [175] [Staircase Streaming for Low-Latency Multi-Agent Inference](https://arxiv.org/abs/2510.05059)
*Junlin Wang,Jue Wang,Zhen,Xu,Ben Athiwaratkun,Bhuwan Dhingra,Ce Zhang,James Zou*

Main category: cs.AI

TL;DR: 提出阶梯式流式处理来降低多智能体推理的延迟，通过部分中间输出立即生成最终响应，将首词时间减少高达93%且保持质量。


<details>
  <summary>Details</summary>
Motivation: 多智能体推理虽然能提升响应质量，但显著增加了首词时间，对延迟敏感应用和用户体验造成挑战。

Method: 阶梯式流式处理：不等待前一步的完整中间输出，而是在收到部分输出后立即开始生成最终响应。

Result: 实验结果表明，阶梯式流式处理将首词时间减少高达93%，同时保持响应质量。

Conclusion: 阶梯式流式处理是解决多智能体推理延迟问题的有效方法，能在保持质量的同时大幅降低首词时间。

Abstract: Recent advances in large language models (LLMs) opened up new directions for
leveraging the collective expertise of multiple LLMs. These methods, such as
Mixture-of-Agents, typically employ additional inference steps to generate
intermediate outputs, which are then used to produce the final response. While
multi-agent inference can enhance response quality, it can significantly
increase the time to first token (TTFT), posing a challenge for
latency-sensitive applications and hurting user experience. To address this
issue, we propose staircase streaming for low-latency multi-agent inference.
Instead of waiting for the complete intermediate outputs from previous steps,
we begin generating the final response as soon as we receive partial outputs
from these steps. Experimental results demonstrate that staircase streaming
reduces TTFT by up to 93% while maintaining response quality.

</details>
