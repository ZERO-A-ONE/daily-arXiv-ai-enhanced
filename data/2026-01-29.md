<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 24]
- [cs.SE](#cs.SE) [Total: 24]
- [cs.CR](#cs.CR) [Total: 12]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [LLM Driven Design of Continuous Optimization Problems with Controllable High-level Properties](https://arxiv.org/abs/2601.18846)
*Urban Skvorc,Niki van Stein,Moritz Seiler,Britta Grimme,Thomas Bäck,Heike Trautmann*

Main category: cs.AI

TL;DR: 使用LLaMEA框架，通过大语言模型在进化循环中生成具有特定景观特征的黑盒优化问题，扩展了BBOB测试套件的结构多样性


<details>
  <summary>Details</summary>
Motivation: 现有黑盒优化基准测试套件（如BBOB）的结构多样性有限，限制了基准测试的有效性。需要一种方法能够生成具有明确高层景观特征的优化问题

Method: 使用LLaMEA框架，通过自然语言描述目标属性（多模态性、可分离性、盆地大小同质性、搜索空间同质性、全局-局部最优对比度）引导LLM生成问题代码。在进化循环中使用基于ELA的属性预测器评分候选问题，并引入ELA空间适应度共享机制增加种群多样性

Result: 生成的函数确实表现出预期的结构特征，t-SNE嵌入显示它们扩展了BBOB实例空间而不是形成无关的聚类。通过盆地吸引分析、统计测试和视觉检查验证了生成函数的有效性

Conclusion: 该方法生成了一个广泛、可解释且可复现的基准问题库，可用于景观分析和下游任务（如自动算法选择），解决了现有基准测试套件结构多样性不足的问题

Abstract: Benchmarking in continuous black-box optimisation is hindered by the limited structural diversity of existing test suites such as BBOB. We explore whether large language models embedded in an evolutionary loop can be used to design optimisation problems with clearly defined high-level landscape characteristics. Using the LLaMEA framework, we guide an LLM to generate problem code from natural-language descriptions of target properties, including multimodality, separability, basin-size homogeneity, search-space homogeneity and globallocal optima contrast. Inside the loop we score candidates through ELA-based property predictors. We introduce an ELA-space fitness-sharing mechanism that increases population diversity and steers the generator away from redundant landscapes. A complementary basin-of-attraction analysis, statistical testing and visual inspection, verifies that many of the generated functions indeed exhibit the intended structural traits. In addition, a t-SNE embedding shows that they expand the BBOB instance space rather than forming an unrelated cluster. The resulting library provides a broad, interpretable, and reproducible set of benchmark problems for landscape analysis and downstream tasks such as automated algorithm selection.

</details>


### [2] [Explainable Uncertainty Quantification for Wastewater Treatment Energy Prediction via Interval Type-2 Neuro-Fuzzy System](https://arxiv.org/abs/2601.18897)
*Qusai Khaled,Bahjat Mallak,Uzay Kaymak,Laura Genga*

Main category: cs.AI

TL;DR: 开发了一种基于区间二型自适应神经模糊推理系统（IT2-ANFIS）的污水处理厂能耗预测方法，提供可解释的不确定性量化，优于传统黑盒概率方法


<details>
  <summary>Details</summary>
Motivation: 污水处理厂消耗全球1-3%的电力，需要准确的能耗预测进行运营优化。现有机器学习模型仅提供点预测，缺乏对安全关键基础设施风险感知决策至关重要的可解释不确定性量化

Method: 开发了区间二型自适应神经模糊推理系统（IT2-ANFIS），通过模糊规则结构生成可解释的预测区间。该方法在三个层次分解不确定性：特征级（识别引入模糊性的变量）、规则级（分析局部模型置信度）和实例级（量化整体预测不确定性）

Result: 在墨尔本水务东部处理厂数据集上验证，IT2-ANFIS达到与一阶ANFIS相当的预测性能，同时在多次训练运行中显著减少方差，并提供可解释的不确定性估计，将预测置信度直接与运营条件和输入变量关联

Conclusion: IT2-ANFIS框架为污水处理厂能耗预测提供了可解释的不确定性量化，支持风险感知决策，有助于优化运营和提升可持续性，特别适用于安全关键基础设施

Abstract: Wastewater treatment plants consume 1-3% of global electricity, making accurate energy forecasting critical for operational optimization and sustainability. While machine learning models provide point predictions, they lack explainable uncertainty quantification essential for risk-aware decision-making in safety-critical infrastructure. This study develops an Interval Type-2 Adaptive Neuro-Fuzzy Inference System (IT2-ANFIS) that generates interpretable prediction intervals through fuzzy rule structures. Unlike black-box probabilistic methods, the proposed framework decomposes uncertainty across three levels: feature-level, footprint of uncertainty identify which variables introduce ambiguity, rule-level analysis reveals confidence in local models, and instance-level intervals quantify overall prediction uncertainty. Validated on Melbourne Water's Eastern Treatment Plant dataset, IT2-ANFIS achieves comparable predictive performance to first order ANFIS with substantially reduced variance across training runs, while providing explainable uncertainty estimates that link prediction confidence directly to operational conditions and input variables.

</details>


### [3] [RIFT: Reordered Instruction Following Testbed To Evaluate Instruction Following in Singular Multistep Prompt Structures](https://arxiv.org/abs/2601.18924)
*Andrew Jaffe,Noah Reicin,Jinho D. Choi*

Main category: cs.AI

TL;DR: RIFT测试平台评估大语言模型在指令跟随中的结构敏感性，发现当提示结构从线性变为跳跃式时，模型准确率下降高达72%，揭示当前架构对位置连续性的强烈依赖。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试将任务复杂性与结构顺序混为一谈，难以分离提示拓扑结构对性能的影响。需要专门评估LLMs在不同指令结构下的表现，特别是非顺序控制流场景。

Method: 引入RIFT（重排序指令跟随测试平台），使用重新表述的Jeopardy!问答对，测试两种提示结构：线性提示（顺序进展）和跳跃提示（内容相同但需要非顺序遍历）。对6个最先进的开源LLMs进行10,000次评估。

Result: 在跳跃条件下，准确率相比基线下降高达72%。约50%的失败源于指令顺序违反和语义漂移。结果表明当前架构将指令跟随内化为顺序模式而非推理技能。

Conclusion: 结构敏感性是当前架构的基本限制，对需要非顺序控制流的应用（如工作流自动化和多智能体系统）有直接影响。需要开发能够处理非顺序指令结构的更鲁棒架构。

Abstract: Large Language Models (LLMs) are increasingly relied upon for complex workflows, yet their ability to maintain flow of instructions remains underexplored. Existing benchmarks conflate task complexity with structural ordering, making it difficult to isolate the impact of prompt topology on performance. We introduce RIFT, Reordered Instruction Following Testbed, to assess instruction following by disentangling structure from content. Using rephrased Jeopardy! question-answer pairs, we test LLMs across two prompt structures: linear prompts, which progress sequentially, and jumping prompts, which preserve identical content but require non-sequential traversal. Across 10,000 evaluations spanning six state-of-the-art open-source LLMs, accuracy dropped by up to 72% under jumping conditions (compared to baseline), revealing a strong dependence on positional continuity. Error analysis shows that approximately 50% of failures stem from instruction-order violations and semantic drift, indicating that current architectures internalize instruction following as a sequential pattern rather than a reasoning skill. These results reveal structural sensitivity as a fundamental limitation in current architectures, with direct implications for applications requiring non-sequential control flow such as workflow automation and multi-agent systems.

</details>


### [4] [Neural Theorem Proving for Verification Conditions: A Real-World Benchmark](https://arxiv.org/abs/2601.18944)
*Qiyuan Xu,Xiaokun Luan,Renxi Wang,Joshua Ong Jun Leang,Peixin Wang,Haonan Li,Wenda Li,Conrad Watt*

Main category: cs.AI

TL;DR: 论文提出了首个针对程序验证中验证条件（VC）自动证明的现实世界多语言基准测试NTP4VC，评估了LLM在VC证明任务上的表现，发现虽然LLM显示出潜力，但与程序验证的实际需求仍有较大差距。


<details>
  <summary>Details</summary>
Motivation: 程序验证中的定理证明是基础，但验证条件（VC）的自动证明仍然是主要瓶颈。现有自动定理证明器无法证明复杂的VC，导致需要大量手动证明，这在实际应用中造成了负担。虽然神经定理证明在数学竞赛中取得了成功，但在程序验证特别是VC证明方面的应用仍未被充分探索。

Method: 提出了NTP4VC基准测试，这是首个针对VC证明的现实世界多语言基准。从Linux和Contiki-OS内核等实际项目中，利用工业级工具链（Why3和Frama-C）生成在Isabelle、Lean和Rocq等形式语言中语义等价的测试用例。评估了通用大语言模型和专门为定理证明微调的LLM在该基准上的表现。

Result: 评估结果表明，虽然大语言模型在VC证明方面显示出潜力，但在程序验证方面仍面临重大挑战。研究揭示了当前方法与程序验证实际需求之间的巨大差距，为未来研究指明了机会。

Conclusion: 该工作填补了程序验证中VC自动证明基准测试的空白，展示了LLM在该领域的潜力与局限，为未来神经定理证明在程序验证中的应用研究提供了重要基础。

Abstract: Theorem proving is fundamental to program verification, where the automated proof of Verification Conditions (VCs) remains a primary bottleneck. Real-world program verification frequently encounters hard VCs that existing Automated Theorem Provers (ATPs) cannot prove, leading to a critical need for extensive manual proofs that burden practical application. While Neural Theorem Proving (NTP) has achieved significant success in mathematical competitions, demonstrating the potential of machine learning approaches to formal reasoning, its application to program verification--particularly VC proving--remains largely unexplored. Despite existing work on annotation synthesis and verification-related theorem proving, no benchmark has specifically targeted this fundamental bottleneck: automated VC proving. This work introduces Neural Theorem Proving for Verification Conditions (NTP4VC), presenting the first real-world multi-language benchmark for this task. From real-world projects such as Linux and Contiki-OS kernel, our benchmark leverages industrial pipelines (Why3 and Frama-C) to generate semantically equivalent test cases across formal languages of Isabelle, Lean, and Rocq. We evaluate large language models (LLMs), both general-purpose and those fine-tuned for theorem proving, on NTP4VC. Results indicate that although LLMs show promise in VC proving, significant challenges remain for program verification, highlighting a large gap and opportunity for future research.

</details>


### [5] [Uncertainty-Aware 3D Emotional Talking Face Synthesis with Emotion Prior Distillation](https://arxiv.org/abs/2601.19112)
*Nanhan Shen,Zhilei Liu*

Main category: cs.AI

TL;DR: UA-3DTalk提出了一种不确定性感知的3D情感说话人脸合成方法，通过情感先验蒸馏解决现有方法在音频-视觉情感对齐和多视图融合方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有3D情感说话人脸合成方法面临两个关键挑战：1) 音频-视觉情感对齐不佳，表现为音频情感提取困难和对情感微表情控制不足；2) 采用一刀切的多视图融合策略，忽视了不确定性和特征质量差异，损害了渲染质量。

Method: UA-3DTalk包含三个核心模块：1) 先验提取模块将音频解耦为内容同步特征和个性化互补特征；2) 情感蒸馏模块引入多模态注意力加权融合机制和4D高斯编码，实现细粒度音频情感提取和情感微表情精确控制；3) 基于不确定性的变形模块部署不确定性块来估计视图特定的不确定度，实现自适应多视图融合，并结合多头解码器优化高斯基元。

Result: 在常规和情感数据集上的大量实验表明，UA-3DTalk在情感对齐方面比DEGSTalk和EDTalk等最先进方法提升了5.2%的E-FID，在唇部同步方面提升了3.1%的SyncC，在渲染质量方面提升了0.015的LPIPS。

Conclusion: UA-3DTalk通过不确定性感知的多视图融合和情感先验蒸馏，显著提升了3D情感说话人脸合成的音频-视觉情感对齐质量和渲染效果。

Abstract: Emotional Talking Face synthesis is pivotal in multimedia and signal processing, yet existing 3D methods suffer from two critical challenges: poor audio-vision emotion alignment, manifested as difficult audio emotion extraction and inadequate control over emotional micro-expressions; and a one-size-fits-all multi-view fusion strategy that overlooks uncertainty and feature quality differences, undermining rendering quality. We propose UA-3DTalk, Uncertainty-Aware 3D Emotional Talking Face Synthesis with emotion prior distillation, which has three core modules: the Prior Extraction module disentangles audio into content-synchronized features for alignment and person-specific complementary features for individualization; the Emotion Distillation module introduces a multi-modal attention-weighted fusion mechanism and 4D Gaussian encoding with multi-resolution code-books, enabling fine-grained audio emotion extraction and precise control of emotional micro-expressions; the Uncertainty-based Deformation deploys uncertainty blocks to estimate view-specific aleatoric (input noise) and epistemic (model parameters) uncertainty, realizing adaptive multi-view fusion and incorporating a multi-head decoder for Gaussian primitive optimization to mitigate the limitations of uniform-weight fusion. Extensive experiments on regular and emotional datasets show UA-3DTalk outperforms state-of-the-art methods like DEGSTalk and EDTalk by 5.2% in E-FID for emotion alignment, 3.1% in SyncC for lip synchronization, and 0.015 in LPIPS for rendering quality. Project page: https://mrask999.github.io/UA-3DTalk

</details>


### [6] [Exploring Weaknesses in Function Call Models via Reinforcement Learning: An Adversarial Data Augmentation Approach](https://arxiv.org/abs/2601.19122)
*Weiran Guo,Bing Bo,Shaoxiang Wu,Jingsheng Yang*

Main category: cs.AI

TL;DR: 提出一种基于强化学习的对抗性数据增强方法，通过训练查询模型生成针对性对抗查询来挑战函数调用模型，提升LLM函数调用能力的泛化性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有提升LLM函数调用能力的方法依赖人工标注或模型自动生成的数据进行微调，缺乏针对性设计，受限于固定模式和数据分布，限制了函数调用LLM的泛化性和鲁棒性提升。

Method: 提出一种对抗性数据增强方法，使用强化学习训练查询模型生成专门挑战函数调用模型的对抗查询。采用零和博弈框架，查询模型和函数调用模型进行迭代交替训练。

Result: 该方法能够系统性地识别和针对函数调用LLM的弱点，提升函数调用模型的鲁棒性，为LLM与外部工具交互能力的弱点识别和修正提供系统性方法。

Conclusion: 提出的基于强化学习的对抗性数据增强方法能够有效提升函数调用LLM的鲁棒性，为开发更强大的函数调用模型提供了新思路。

Abstract: Function call capabilities have become crucial for Large Language Models (LLMs), enabling them to interact more effectively with external tools and APIs. Existing methods for improving the function call capabilities of LLMs rely on data obtained either through manual annotation or automated generation by models, and use this data to finetune the LLMs. However, these methods often lack targeted design and are constrained by fixed patterns and data distributions, which limits their effectiveness in enhancing the generalization and robustness of function call LLMs. To address this limitation, we propose a novel adversarial data augmentation method that employs reinforcement learning to systematically identify and target the weaknesses of function call LLMs. Our training framework introduces a query model trained with reinforcement learning (RL) to generate adversarial queries that are specifically designed to challenge function call (FC) models. This approach adopts a zero sum game formulation, where the query model and the FC model engage in iterative alternating training. Overall, our method advances the development of more robust FC models and provides a systematic way to identify and correct weaknesses in the ability of LLMs to interact with external tools.

</details>


### [7] [TS-Debate: Multimodal Collaborative Debate for Zero-Shot Time Series Reasoning](https://arxiv.org/abs/2601.19151)
*Patara Trirat,Jin Myung Kwak,Jay Heo,Heejun Lee,Sung Ju Hwang*

Main category: cs.AI

TL;DR: TS-Debate：一种用于零样本时间序列推理的模态专业化协作多智能体辩论框架，通过专家智能体分工和结构化辩论协议解决LLM在时间序列分析中的数值保真度和模态干扰问题


<details>
  <summary>Details</summary>
Motivation: 大语言模型在时间序列分析中展现出潜力但也存在脆弱性，特别是在数值保真度、模态干扰和跨模态整合方面存在困难，需要一种无需任务特定微调就能解决这些问题的框架

Method: 提出TS-Debate框架：1) 为文本上下文、视觉模式和数值信号分配专门的专家智能体；2) 通过显式领域知识启发；3) 使用结构化辩论协议协调智能体交互；4) 评审智能体通过验证-冲突-校准机制评估智能体主张，支持轻量级代码执行和数值查找进行程序化验证

Result: 在涵盖三个公共基准测试的20个任务中，TS-Debate相比强基线（包括所有智能体观察所有输入的标准多模态辩论）实现了持续且显著的性能提升

Conclusion: TS-Debate框架能够保持模态保真度、暴露冲突证据并减轻数值幻觉，无需任务特定微调，为大语言模型在时间序列分析中的应用提供了有效的解决方案

Abstract: Recent progress at the intersection of large language models (LLMs) and time series (TS) analysis has revealed both promise and fragility. While LLMs can reason over temporal structure given carefully engineered context, they often struggle with numeric fidelity, modality interference, and principled cross-modal integration. We present TS-Debate, a modality-specialized, collaborative multi-agent debate framework for zero-shot time series reasoning. TS-Debate assigns dedicated expert agents to textual context, visual patterns, and numerical signals, preceded by explicit domain knowledge elicitation, and coordinates their interaction via a structured debate protocol. Reviewer agents evaluate agent claims using a verification-conflict-calibration mechanism, supported by lightweight code execution and numerical lookup for programmatic verification. This architecture preserves modality fidelity, exposes conflicting evidence, and mitigates numeric hallucinations without task-specific fine-tuning. Across 20 tasks spanning three public benchmarks, TS-Debate achieves consistent and significant performance improvements over strong baselines, including standard multimodal debate in which all agents observe all inputs.

</details>


### [8] [Multi-Agent Procedural Graph Extraction with Structural and Logical Refinement](https://arxiv.org/abs/2601.19170)
*Wangyang Ying,Yanchi Liu,Xujiang Zhao,Wei Cheng,Zhengzhang Chen,Wenchao Yu,Yanjie Fu,Haifeng Chen*

Main category: cs.AI

TL;DR: 提出一个多智能体框架，将自然语言中的工作流提取为程序图，通过结构化和逻辑反馈迭代优化


<details>
  <summary>Details</summary>
Motivation: 从自然语言自动提取工作流作为程序图具有前景但研究不足，需要同时保证结构有效性和逻辑一致性。现有大语言模型虽然显示出潜力，但经常产生结构不良或逻辑流误解的结果。

Method: 提出一个多智能体框架，将程序图提取建模为多轮推理过程，包含三个迭代阶段：1) 图构建智能体进行图提取；2) 模拟智能体诊断并解释结构缺陷；3) 语义智能体对齐流程逻辑与源文本语言线索。重要反馈以自然语言形式注入后续提示，实现可解释和可控的优化。

Result: 实验表明，该框架在结构正确性和逻辑一致性方面相比强基线有显著提升。

Conclusion: 该多智能体框架通过模块化设计，使智能体能够针对不同类型的错误进行优化，无需监督或参数更新，实现了从自然语言中提取程序图的有效方法。

Abstract: Automatically extracting workflows as procedural graphs from natural language is promising yet underexplored, demanding both structural validity and logical alignment. While recent large language models (LLMs) show potential for procedural graph extraction, they often produce ill-formed structures or misinterpret logical flows. We present \model{}, a multi-agent framework that formulates procedural graph extraction as a multi-round reasoning process with dedicated structural and logical refinement. The framework iterates through three stages: (1) a graph extraction phase with the graph builder agent, (2) a structural feedback phase in which a simulation agent diagnoses and explains structural defects, and (3) a logical feedback phase in which a semantic agent aligns semantics between flow logic and linguistic cues in the source text. Important feedback is prioritized and expressed in natural language, which is injected into subsequent prompts, enabling interpretable and controllable refinement. This modular design allows agents to target distinct error types without supervision or parameter updates. Experiments demonstrate that \model{} achieves substantial improvements in both structural correctness and logical consistency over strong baselines.

</details>


### [9] [CollectiveKV: Decoupling and Sharing Collaborative Information in Sequential Recommendation](https://arxiv.org/abs/2601.19178)
*Jingyu Li,Zhaocheng Du,Qianhui Zhu,kaiyuan Li,Zhicheng Zhang,Song-Li Wu,Chaolang Li,Pengwen Dai*

Main category: cs.AI

TL;DR: 该论文提出了CollectiveKV，一种跨用户的KV共享机制，通过可学习的全局KV池捕获用户间共享信息，将KV缓存压缩到原始大小的0.8%，同时保持甚至提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 序列推荐系统中Transformer注意力机制的计算复杂度随序列长度增长，KV缓存技术虽能降低推理延迟，但会引入巨大的存储开销，特别是对于拥有大量用户和长历史序列的系统。研究发现不同用户的KV序列存在显著相似性，表明KV中存在协作信号。

Method: 通过奇异值分解分析KV，发现信息可分为两部分：大部分信息可在用户间共享，小部分是用户特定的。提出CollectiveKV机制，通过可学习的全局KV池捕获跨用户共享信息，推理时每个用户从池中检索高维共享KV，并与低维用户特定KV拼接得到最终KV。

Result: 在五个序列推荐模型和三个数据集上的实验表明，该方法能将KV缓存压缩到原始大小的0.8%，同时保持甚至提升模型性能。

Conclusion: CollectiveKV通过跨用户KV共享机制有效解决了序列推荐系统中KV缓存带来的存储开销问题，在显著压缩存储的同时保持了模型性能，为大规模序列推荐系统的实际部署提供了可行方案。

Abstract: Sequential recommendation models are widely used in applications, yet they face stringent latency requirements. Mainstream models leverage the Transformer attention mechanism to improve performance, but its computational complexity grows with the sequence length, leading to a latency challenge for long sequences. Consequently, KV cache technology has recently been explored in sequential recommendation systems to reduce inference latency. However, KV cache introduces substantial storage overhead in sequential recommendation systems, which often have a large user base with potentially very long user history sequences. In this work, we observe that KV sequences across different users exhibit significant similarities, indicating the existence of collaborative signals in KV. Furthermore, we analyze the KV using singular value decomposition (SVD) and find that the information in KV can be divided into two parts: the majority of the information is shareable across users, while a small portion is user-specific. Motivated by this, we propose CollectiveKV, a cross-user KV sharing mechanism. It captures the information shared across users through a learnable global KV pool. During inference, each user retrieves high-dimensional shared KV from the pool and concatenates them with low-dimensional user-specific KV to obtain the final KV. Experiments on five sequential recommendation models and three datasets show that our method can compress the KV cache to only 0.8% of its original size, while maintaining or even enhancing model performance.

</details>


### [10] [GAVEL: Towards rule-based safety through activation monitoring](https://arxiv.org/abs/2601.19768)
*Shir Rozenfeld,Rahul Pankajakshan,Itay Zloczower,Eyal Lenga,Gilad Gressel,Yisroel Mirsky*

Main category: cs.AI

TL;DR: 本文提出了一种基于规则的激活安全新范式，将LLM激活建模为可解释的认知元素，通过组合规则实现高精度、可定制化的有害行为检测，无需重新训练模型。


<details>
  <summary>Details</summary>
Motivation: 现有基于激活的安全监控方法存在精度低、灵活性差、缺乏可解释性等问题，无法有效检测表面文本不明显的有害行为，需要一种更精确、可定制、可解释的解决方案。

Method: 将LLM激活建模为细粒度的认知元素（如"发出威胁"、"支付处理"等），构建基于认知元素的谓词规则框架，实时检测规则违反，支持配置更新而无需重新训练模型。

Result: 基于规则的组合激活安全方法提高了检测精度，支持领域定制化，为可扩展、可解释、可审计的AI治理奠定了基础，并发布了GAVEL开源框架和自动化规则创建工具。

Conclusion: 规则化的激活安全方法通过认知元素表示和组合规则，实现了更高精度、可定制化和可解释性的有害行为检测，为AI安全治理提供了实用框架。

Abstract: Large language models (LLMs) are increasingly paired with activation-based monitoring to detect and prevent harmful behaviors that may not be apparent at the surface-text level. However, existing activation safety approaches, trained on broad misuse datasets, struggle with poor precision, limited flexibility, and lack of interpretability. This paper introduces a new paradigm: rule-based activation safety, inspired by rule-sharing practices in cybersecurity. We propose modeling activations as cognitive elements (CEs), fine-grained, interpretable factors such as ''making a threat'' and ''payment processing'', that can be composed to capture nuanced, domain-specific behaviors with higher precision. Building on this representation, we present a practical framework that defines predicate rules over CEs and detects violations in real time. This enables practitioners to configure and update safeguards without retraining models or detectors, while supporting transparency and auditability. Our results show that compositional rule-based activation safety improves precision, supports domain customization, and lays the groundwork for scalable, interpretable, and auditable AI governance. We will release GAVEL as an open-source framework and provide an accompanying automated rule creation tool.

</details>


### [11] [CoReTab: Improving Multimodal Table Understanding with Code-driven Reasoning](https://arxiv.org/abs/2601.19193)
*Van-Quang Nguyen,Takayuki Okatani*

Main category: cs.AI

TL;DR: CoReTab是一个代码驱动的推理框架，通过将多步推理与可执行Python代码结合，为多模态表格理解提供可扩展、可解释且自动可验证的标注，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有多模态表格理解数据集（如MMTab）主要提供简短的事实性答案，缺乏明确的多步推理监督。在这些数据集上训练的模型通常生成简短回答，准确率不足且可解释性有限，无法清晰展示模型如何得出最终答案。

Method: 引入CoReTab代码驱动推理框架，通过将多步推理与可执行Python代码耦合，生成可扩展、可解释且自动可验证的标注。使用该框架构建了包含115K个已验证样本的数据集（平均每个响应529个标记），并通过三阶段流水线对开源MLLMs进行微调。

Result: 在17个MMTab基准测试（涵盖表格问答、事实验证和表格结构理解）上评估，CoReTab训练的模型相比MMTab训练的基线模型分别取得了+6.2%、+5.7%和+25.6%的显著提升，同时产生透明且可验证的推理轨迹。

Conclusion: CoReTab作为一个稳健且可泛化的监督框架，有效改进了多模态表格理解中的多步推理能力，提供了透明和可验证的推理过程。

Abstract: Existing datasets for multimodal table understanding, such as MMTab, primarily provide short factual answers without explicit multi-step reasoning supervision. Models trained on these datasets often generate brief responses that offers insufficient accuracy and limited interpretability into how these models arrive at the final answer. We introduce CoReTab, a code-driven reasoning framework that produces scalable, interpretable, and automatically verifiable annotations by coupling multi-step reasoning with executable Python code. Using the CoReTab framework, we curate a dataset of 115K verified samples averaging 529 tokens per response and fine-tune open-source MLLMs through a three-stage pipeline. We evaluate the resulting model trained on CoReTab across 17 MMTab benchmarks spanning table question answering, fact verification, and table structure understanding. Our model achieves significant gains of +6.2%, +5.7%, and +25.6%, respectively, over MMTab-trained baselines, while producing transparent and verifiable reasoning traces. These results establish CoReTab as a robust and generalizable supervision framework for improving multi-step reasoning in multimodal table understanding.

</details>


### [12] [MATA: A Trainable Hierarchical Automaton System for Multi-Agent Visual Reasoning](https://arxiv.org/abs/2601.19204)
*Zhixi Cai,Fucai Ke,Kevin Leo,Sukai Huang,Maria Garcia de la Banda,Peter J. Stuckey,Hamid Rezatofighi*

Main category: cs.AI

TL;DR: MATA是一个用于视觉推理的多智能体分层可训练自动机系统，通过可训练的超智能体选择顶层状态转移，每个智能体运行基于规则的子自动机，共享内存实现透明执行历史，在多个视觉推理基准测试中达到最先进水平。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型虽然感知能力强，但在复杂查询时存在隐含推理难以解释和容易产生幻觉的问题。组合方法提高了可解释性，但大多依赖单一智能体或手工设计的流水线，无法决定何时在互补智能体之间协作或在重叠智能体之间竞争。

Method: 提出MATA（多智能体分层可训练自动机），这是一个作为分层有限状态自动机的多智能体系统，顶层转移由可训练的超智能体选择。每个智能体对应超自动机中的一个状态，运行小型基于规则的子自动机进行可靠的微控制。所有智能体读写共享内存，产生透明的执行历史。通过构建转移轨迹树并转换为内存到下一状态对，创建MATA-SFT-90K数据集用于监督微调。

Result: 在多个视觉推理基准测试中，MATA相比单体和组合基线实现了最先进的结果。微调后的LLM作为转移策略能够理解查询和智能体能力，并能高效选择最优智能体解决任务。

Conclusion: MATA通过多智能体分层可训练自动机框架，解决了视觉语言模型在复杂查询中的可解释性和幻觉问题，实现了透明、可靠的视觉推理，并在多个基准测试中表现出优越性能。

Abstract: Recent vision-language models have strong perceptual ability but their implicit reasoning is hard to explain and easily generates hallucinations on complex queries. Compositional methods improve interpretability, but most rely on a single agent or hand-crafted pipeline and cannot decide when to collaborate across complementary agents or compete among overlapping ones. We introduce MATA (Multi-Agent hierarchical Trainable Automaton), a multi-agent system presented as a hierarchical finite-state automaton for visual reasoning whose top-level transitions are chosen by a trainable hyper agent. Each agent corresponds to a state in the hyper automaton, and runs a small rule-based sub-automaton for reliable micro-control. All agents read and write a shared memory, yielding transparent execution history. To supervise the hyper agent's transition policy, we build transition-trajectory trees and transform to memory-to-next-state pairs, forming the MATA-SFT-90K dataset for supervised finetuning (SFT). The finetuned LLM as the transition policy understands the query and the capacity of agents, and it can efficiently choose the optimal agent to solve the task. Across multiple visual reasoning benchmarks, MATA achieves the state-of-the-art results compared with monolithic and compositional baselines. The code and dataset are available at https://github.com/ControlNet/MATA.

</details>


### [13] [Beyond In-Domain Detection: SpikeScore for Cross-Domain Hallucination Detection](https://arxiv.org/abs/2601.19245)
*Yongxin Deng,Zhen Fang,Yixuan Li,Ling Chen*

Main category: cs.AI

TL;DR: 提出SpikeScore方法用于跨领域幻觉检测，通过量化多轮对话中的不确定性波动来区分幻觉和非幻觉响应，在跨领域泛化方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有幻觉检测方法在同领域数据上表现良好，但在跨领域泛化方面表现不佳。研究关注一个重要但被忽视的问题——通用幻觉检测（GHD），旨在使用单一领域数据训练检测器，同时确保在不同相关领域具有鲁棒性能。

Method: 提出SpikeScore评分方法，基于观察到幻觉引发的多轮对话比事实性对话表现出更大的不确定性波动。该方法量化多轮对话中的突然波动，通过理论分析和实证验证证明其在跨领域场景下能有效区分幻觉和非幻觉响应。

Result: 在多个大语言模型和基准测试上的实验表明，基于SpikeScore的检测方法在跨领域泛化方面优于代表性基线方法，甚至超越了先进的面向泛化的方法，验证了该方法在跨领域幻觉检测中的有效性。

Conclusion: SpikeScore方法通过量化多轮对话中的不确定性波动，为跨领域幻觉检测提供了有效的解决方案，解决了现有方法在跨领域泛化方面的局限性。

Abstract: Hallucination detection is critical for deploying large language models (LLMs) in real-world applications. Existing hallucination detection methods achieve strong performance when the training and test data come from the same domain, but they suffer from poor cross-domain generalization. In this paper, we study an important yet overlooked problem, termed generalizable hallucination detection (GHD), which aims to train hallucination detectors on data from a single domain while ensuring robust performance across diverse related domains. In studying GHD, we simulate multi-turn dialogues following LLMs initial response and observe an interesting phenomenon: hallucination-initiated multi-turn dialogues universally exhibit larger uncertainty fluctuations than factual ones across different domains. Based on the phenomenon, we propose a new score SpikeScore, which quantifies abrupt fluctuations in multi-turn dialogues. Through both theoretical analysis and empirical validation, we demonstrate that SpikeScore achieves strong cross-domain separability between hallucinated and non-hallucinated responses. Experiments across multiple LLMs and benchmarks demonstrate that the SpikeScore-based detection method outperforms representative baselines in cross-domain generalization and surpasses advanced generalization-oriented methods, verifying the effectiveness of our method in cross-domain hallucination detection.

</details>


### [14] [GLOVE: Global Verifier for LLM Memory-Environment Realignment](https://arxiv.org/abs/2601.19249)
*Xingkun Yin,Hongyang Du*

Main category: cs.AI

TL;DR: GLOVE框架通过主动探测记忆与观察的不一致性，建立相对真理概念，使LLM能在动态变化环境中验证和更新记忆，无需真实监督或强模型内省。


<details>
  <summary>Details</summary>
Motivation: 现有LLM记忆增强方法假设记忆有效性可通过外部评估者或模型内省建立，但这些假设在动态漂移的实际环境中往往失效。

Method: 提出GLOVE框架，通过主动探测检索记忆与新鲜观察之间的不一致性，建立相对真理概念，实现记忆与环境重新对齐，无需真实监督或强模型内省。

Result: 在包含受控环境漂移的网页导航、规划和控制基准测试中，GLOVE显著提高了智能体成功率。

Conclusion: GLOVE为构建能够自我进化的认知智能体提供了一条稳健路径，通过相对真理概念实现记忆系统的动态适应。

Abstract: Most existing memory-enhanced Large Language Model (LLM) approaches implicitly assume that memory validity can be established either through external evaluators that provide task-specific success signals or through internal model cognition, such as reflection, for editing memory entries. However, these assumptions often break down in practical environments with dynamic drifts. We propose the Global Verifier (GLOVE), a framework that introduces a new design dimension for LLM memory systems by establishing a relative notion of truth. Through active probing to detect inconsistencies between retrieved memories and fresh observations, GLOVE enables memory-environment realignment by verifying and updating memory without access to ground-truth supervision or strong reliance on model introspection. We evaluate GLOVE on diverse benchmarks spanning web navigation, planning, and control, augmented with controlled environmental drifts that introduce non-stationarity beyond the original benchmark settings. Our results show that GLOVE substantially improves agent success rates, suggesting a robust pathway to cognitive agents capable of self-evolving.

</details>


### [15] [Curiosity Driven Knowledge Retrieval for Mobile Agents](https://arxiv.org/abs/2601.19306)
*Sijia Li,Xiaoyu Tan,Shahir Ali,Niels Schmidt,Gengchen Ma,Xihe Qiu*

Main category: cs.AI

TL;DR: 提出好奇心驱动的知识检索框架，通过AppCards编码应用功能语义和交互模式，增强移动智能体在复杂应用中的执行能力


<details>
  <summary>Details</summary>
Motivation: 移动智能体在复杂应用中的性能受限于知识不完整和对未见环境的泛化能力弱，需要更可靠的知识获取机制来提升执行效果

Method: 基于好奇心评分检测执行不确定性，从文档、代码库和历史轨迹中检索外部信息，组织成结构化AppCards，包含功能语义、参数约定、界面映射和交互模式，在执行过程中选择性集成到推理过程

Result: 在AndroidWorld基准测试中，所有骨干模型均获得一致改进，平均提升6个百分点，结合GPT-5时达到88.8%的最新SOTA成功率；AppCards对多步骤和跨应用任务特别有效

Conclusion: 好奇心驱动的知识检索框架通过AppCards有效补偿知识盲点，减少歧义、缩短探索时间并支持稳定执行轨迹，显著提升移动智能体在复杂应用中的可靠性

Abstract: Mobile agents have made progress toward reliable smartphone automation, yet performance in complex applications remains limited by incomplete knowledge and weak generalization to unseen environments. We introduce a curiosity driven knowledge retrieval framework that formalizes uncertainty during execution as a curiosity score. When this score exceeds a threshold, the system retrieves external information from documentation, code repositories, and historical trajectories. Retrieved content is organized into structured AppCards, which encode functional semantics, parameter conventions, interface mappings, and interaction patterns. During execution, an enhanced agent selectively integrates relevant AppCards into its reasoning process, thereby compensating for knowledge blind spots and improving planning reliability. Evaluation on the AndroidWorld benchmark shows consistent improvements across backbones, with an average gain of six percentage points and a new state of the art success rate of 88.8\% when combined with GPT-5. Analysis indicates that AppCards are particularly effective for multi step and cross application tasks, while improvements depend on the backbone model. Case studies further confirm that AppCards reduce ambiguity, shorten exploration, and support stable execution trajectories. Task trajectories are publicly available at https://lisalsj.github.io/Droidrun-appcard/.

</details>


### [16] [Balancing Sustainability And Performance: The Role Of Small-Scale Llms In Agentic Artificial Intelligence Systems](https://arxiv.org/abs/2601.19311)
*Anh Khoa Ngo Ho,Martin Chauvin,Simon Gosset,Philippe Cordier,Boris Gamazaychikov*

Main category: cs.AI

TL;DR: 研究表明，在保持任务质量的前提下，部署较小规模的开源语言模型可以显著降低多智能体系统中的能源消耗，为可持续AI设计提供实用指南。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型成为智能AI系统的核心组件，其推理过程中的能源需求可能带来显著的可持续性挑战。本研究旨在探索在保持响应速度和质量的前提下，部署较小规模模型是否能降低多智能体真实环境中的能源消耗。

Method: 对不同规模的语言模型进行对比分析，量化效率与性能之间的权衡关系，研究在保持任务质量的同时降低能源消耗的可能性。

Result: 结果显示，较小的开源权重模型能够降低能源使用量，同时保持任务质量。基于这些发现，研究提出了可持续AI设计的实用指南，包括最佳批量大小配置和计算资源分配策略。

Conclusion: 研究结果为开发可扩展、环境友好的AI系统提供了可行的策略，表明在保持性能的同时实现能源效率是可行的，为可持续AI设计提供了重要见解。

Abstract: As large language models become integral to agentic artificial intelligence systems, their energy demands during inference may pose significant sustainability challenges. This study investigates whether deploying smaller-scale language models can reduce energy consumption without compromising responsiveness and output quality in a multi-agent, real-world environments. We conduct a comparative analysis across language models of varying scales to quantify trade-offs between efficiency and performance. Results show that smaller open-weights models can lower energy usage while preserving task quality. Building on these findings, we propose practical guidelines for sustainable artificial intelligence design, including optimal batch size configuration and computation resource allocation. These insights offer actionable strategies for developing scalable, environmentally responsible artificial intelligence systems.

</details>


### [17] [RPO:Reinforcement Fine-Tuning with Partial Reasoning Optimization](https://arxiv.org/abs/2601.19404)
*Hongzhu Yi,Xinming Wang,Zhenghao zhang,Tianyu Zong,Yuanxiang Wang,Jun Xie,Tao Yu,Haopeng Jin,Zhepeng Wang,Kaixin Xu,Feng Chen,Jiahuan Chen,Yujia Yang,Zhenyu Guan,Bingkang Shi,Jungang Xu*

Main category: cs.AI

TL;DR: RPO是一种基于部分推理优化的强化微调算法，通过仅生成推理路径的后缀而非完整路径，大幅减少训练时的计算开销，实现90%的训练加速，同时保持与原始算法相当的性能。


<details>
  <summary>Details</summary>
Motivation: 传统强化微调算法需要从输入查询开始生成完整的推理轨迹，这在训练的回滚阶段会产生巨大的计算开销。为了解决这个问题，研究者分析了推理路径不同部分对最终结果正确性的影响。

Method: 提出RPO（基于部分推理优化的强化微调算法），这是一种即插即用的强化微调算法。与传统方法生成完整推理路径不同，RPO使用经验缓存仅生成推理路径的后缀进行训练。

Result: 在训练的回滚阶段，RPO减少了约95%的token生成，大幅降低了理论时间开销。对于1.5B模型训练时间减少90%，7B模型减少72%。RPO可与GRPO、DAPO等典型算法集成，在保持性能的同时实现训练加速。

Conclusion: RPO通过部分推理优化的方法，显著降低了强化微调的计算成本，实现了高效的训练加速，同时保持模型性能，为大规模语言模型的强化微调提供了实用的解决方案。

Abstract: Within the domain of large language models, reinforcement fine-tuning algorithms necessitate the generation of a complete reasoning trajectory beginning from the input query, which incurs significant computational overhead during the rollout phase of training. To address this issue, we analyze the impact of different segments of the reasoning path on the correctness of the final result and, based on these insights, propose Reinforcement Fine-Tuning with Partial Reasoning Optimization (RPO), a plug-and-play reinforcement fine-tuning algorithm. Unlike traditional reinforcement fine-tuning algorithms that generate full reasoning paths, RPO trains the model by generating suffixes of the reasoning path using experience cache. During the rollout phase of training, RPO reduces token generation in this phase by approximately 95%, greatly lowering the theoretical time overhead. Compared with full-path reinforcement fine-tuning algorithms, RPO reduces the training time of the 1.5B model by 90% and the 7B model by 72%. At the same time, it can be integrated with typical algorithms such as GRPO and DAPO, enabling them to achieve training acceleration while maintaining performance comparable to the original algorithms. Our code is open-sourced at https://github.com/yhz5613813/RPO.

</details>


### [18] [Fuzzy expert system for the process of collecting and purifying acidic water: a digital twin approach](https://arxiv.org/abs/2601.19527)
*Temirbolat Maratuly,Pakizar Shamoi,Timur Samigulin*

Main category: cs.AI

TL;DR: 本文开发了一个结合数字孪生的模糊专家系统，用于控制酸性水净化过程，通过模拟人类推理来维持关键参数，并设计了简单直观的控制策略便于非专业人员操作。


<details>
  <summary>Details</summary>
Motivation: 酸性水净化对于减少排放、降低腐蚀风险、实现处理水回用以及降低运营成本至关重要。自动化净化过程可以减少人员参与，降低工人安全风险。原油中的酸性成分（如硫化氢、二氧化碳等）在加工过程中会释放到酸性水中，若不妥善处理会对环境造成严重威胁并加速管道设备腐蚀。

Method: 开发了结合定制数字孪生的模糊专家系统，使用Honeywell UniSim Design R492开发数字孪生来模拟真实工业行为，通过MATLAB进行阀门动态的系统辨识，使用OPC DA建立模拟器与控制器之间的实时数据交换。模糊控制器采用分程控制策略控制两个阀门，在21种不同初始压力条件下测试了5种不同的去模糊化策略，共105个测试场景。

Result: 系统性能使用误差指标（MSE、RMSE、MAE、IAE、ISE、ITAE）和动态响应指标（超调量、欠调量、上升时间、下降时间、稳定时间、稳态误差）进行评估。开发了基于Python Streamlit框架的Web仿真界面。

Conclusion: 虽然本文以酸性水处理为例进行演示，但所提出的模糊专家系统具有通用性，能够通过模拟人类推理来维持关键参数在期望水平，控制策略简单直观，适合初级或非专业人员操作。

Abstract: Purifying sour water is essential for reducing emissions, minimizing corrosion risks, enabling the reuse of treated water in industrial or domestic applications, and ultimately lowering operational costs. Moreover, automating the purification process helps reduce the risk of worker harm by limiting human involvement. Crude oil contains acidic components such as hydrogen sulfide, carbon dioxide, and other chemical compounds. During processing, these substances are partially released into sour water. If not properly treated, sour water poses serious environmental threats and accelerates the corrosion of pipelines and equipment. This paper presents a fuzzy expert system, combined with a custom-generated digital twin, developed from a documented industrial process to maintain key parameters at desired levels by mimicking human reasoning. The control strategy is designed to be simple and intuitive, allowing junior or non-expert personnel to interact with the system effectively. The digital twin was developed using Honeywell UniSim Design R492 to simulate real industrial behavior accurately. Valve dynamics were modeled through system identification in MATLAB, and real-time data exchange between the simulator and controller was established using OPC DA. The fuzzy controller applies split-range control to two valves and was tested under 21 different initial pressure conditions using five distinct defuzzification strategies, resulting in a total of 105 unique test scenarios. System performance was evaluated using both error-based metrics (MSE, RMSE, MAE, IAE, ISE, ITAE) and dynamic response metrics, including overshoot, undershoot, rise time, fall time, settling time, and steady-state error. A web-based simulation interface was developed in Python using the Streamlit framework. Although demonstrated here for sour water treatment, the proposed fuzzy expert system is general-purpose.

</details>


### [19] [Benchmarks Saturate When The Model Gets Smarter Than The Judge](https://arxiv.org/abs/2601.19532)
*Marthe Ballon,Andres Algaba,Brecht Verbeken,Vincent Ginis*

Main category: cs.AI

TL;DR: Omni-MATH-2是Omni-MATH数据集的修订版，包含4181个精确答案问题和247个非标准问题，通过人工审核确保可编译性、可解性和可验证性，显著减少数据集噪声，提供更精确的模型性能评估。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型基准测试中数据集和评估方法的不准确性严重影响了评估效果，需要更高质量的数据集和可靠的评估方法来准确衡量模型性能。

Method: 对Omni-MATH数据集进行人工修订：1) 创建精确答案子集(n=4181)和带标签的非标准子集(n=247)；2) 审核每个问题的LaTeX可编译性、可解性和可验证性；3) 添加缺失图表或信息；4) 标记需要证明、估计或图像的问题；5) 移除杂乱内容；6) 使用专家标注比较GPT-5 mini与原始Omni-Judge的评估差异。

Result: 1) 修订后的数据集显著减少数据集引起的噪声；2) 在评估者分歧中，Omni-Judge在96.4%的情况下是错误的；3) 随着问题难度增加，需要更胜任的评估者来防止评估错误掩盖模型间的真实差异；4) 对于带标签的问题子集，两种评估方法都未能识别当前的失败模式。

Conclusion: 数据集质量和评估者可靠性对于开发准确的模型性能基准测试都至关重要，两者都需要改进以确保基准测试的有效性。

Abstract: Benchmarks are important tools to track progress in the development of Large Language Models (LLMs), yet inaccuracies in datasets and evaluation methods consistently undermine their effectiveness. Here, we present Omni-MATH-2, a manually revised version of the Omni-MATH dataset comprising a clean, exact-answer subset ($n{=}4181$) and a tagged, non-standard subset ($n{=}247$). Each problem was audited to ensure LaTeX compilability, solvability and verifiability, which involved adding missing figures or information, labeling problems requiring a proof, estimation or image, and removing clutter. This process significantly reduces dataset-induced noise, thereby providing a more precise assessment of model performance. The annotated dataset also allows us to evaluate judge-induced noise by comparing GPT-5 mini with the original Omni-Judge, revealing substantial discrepancies between judges on both the clean and tagged problem subsets. Expert annotations reveal that Omni-Judge is wrong in $96.4\%$ of the judge disagreements, indicating its inability to differentiate between models' abilities, even well before saturation of the benchmark occurs. As problems become more challenging, we find that increasingly competent judges become essential in order to prevent judge errors from masking genuine differences between models. Finally, neither judge identifies the present failure modes for the subset of tagged problems, demonstrating that dataset quality and judge reliability are both critical to develop accurate benchmarks of model performance.

</details>


### [20] [Algorithmic Prompt-Augmentation for Efficient LLM-Based Heuristic Design for A* Search](https://arxiv.org/abs/2601.19622)
*Thomas Bömer,Nico Koltermann,Max Disselnmeyer,Bastian Amberg,Anne Meyer*

Main category: cs.AI

TL;DR: 本文提出A-CEoH框架，通过将A*算法代码融入提示词，利用上下文学习自动生成启发式函数，在UPMP和滑动拼图问题上超越了专家设计的启发式函数。


<details>
  <summary>Details</summary>
Motivation: 传统启发式函数需要专家手工设计，耗时耗力。随着大语言模型和进化框架的发展，自动化启发式设计成为可能。本文旨在扩展EoH框架，探索为A*搜索自动生成引导启发式函数的方法。

Method: 提出A-CEoH（算法上下文EoH）框架，采用领域无关的提示增强策略，将A*算法代码融入提示词中，利用大语言模型的上下文学习能力自动生成启发式函数。在单元装载预调配问题和经典滑动拼图问题上进行验证。

Result: 计算实验表明，A-CEoH能显著提升生成启发式函数的质量，在测试的两个问题域中甚至超越了专家设计的启发式函数。

Conclusion: A-CEoH框架成功实现了启发式函数的自动化设计，通过将算法代码融入提示词的上下文学习方法有效提升了生成启发式的质量，为自动化启发式设计提供了新思路。

Abstract: Heuristic functions are essential to the performance of tree search algorithms such as A*, where their accuracy and efficiency directly impact search outcomes. Traditionally, such heuristics are handcrafted, requiring significant expertise. Recent advances in large language models (LLMs) and evolutionary frameworks have opened the door to automating heuristic design. In this paper, we extend the Evolution of Heuristics (EoH) framework to investigate the automated generation of guiding heuristics for A* search. We introduce a novel domain-agnostic prompt augmentation strategy that includes the A* code into the prompt to leverage in-context learning, named Algorithmic - Contextual EoH (A-CEoH). To evaluate the effectiveness of A-CeoH, we study two problem domains: the Unit-Load Pre-Marshalling Problem (UPMP), a niche problem from warehouse logistics, and the classical sliding puzzle problem (SPP). Our computational experiments show that A-CEoH can significantly improve the quality of the generated heuristics and even outperform expert-designed heuristics.

</details>


### [21] [Agentic Design Patterns: A System-Theoretic Framework](https://arxiv.org/abs/2601.19752)
*Minh-Dung Dao,Quy Minh Le,Hoang Thanh Lam,Duc-Trong Le,Quoc-Viet Pham,Barry O'Sullivan,Hoang D. Nguyen*

Main category: cs.AI

TL;DR: 本文提出了一种基于系统理论的AI智能体工程方法，包括一个五子系统框架和12个设计模式，旨在解决现有智能体系统不可靠、脆弱的问题。


<details>
  <summary>Details</summary>
Motivation: 随着基础模型的发展，AI智能体系统越来越受关注，但其存在幻觉、推理能力差等问题，加上系统设计常常是临时性的，导致应用不可靠且脆弱。现有的智能体设计模式描述缺乏严格的系统理论基础，难以实际实施。

Method: 提出两个主要贡献：1）基于系统理论的框架，将AI智能体系统解构为五个核心交互功能子系统：推理与世界模型、感知与接地、动作执行、学习与适应、智能体间通信；2）基于该架构，提出12个智能体设计模式，分为基础类、认知与决策类、执行与交互类、适应与学习类。

Result: 通过ReAct框架的案例研究展示了该框架的实用性，表明所提出的模式可以纠正系统架构缺陷。该工作为研究人员和工程师提供了标准化的智能体设计交流语言和方法。

Conclusion: 这项工作提供了基础语言和结构化方法，用于标准化研究人员和工程师之间的智能体设计交流，从而构建更模块化、可理解和可靠的自主系统。

Abstract: With the development of foundation model (FM), agentic AI systems are getting more attention, yet their inherent issues like hallucination and poor reasoning, coupled with the frequent ad-hoc nature of system design, lead to unreliable and brittle applications. Existing efforts to characterise agentic design patterns often lack a rigorous systems-theoretic foundation, resulting in high-level or convenience-based taxonomies that are difficult to implement. This paper addresses this gap by introducing a principled methodology for engineering robust AI agents. We propose two primary contributions: first, a novel system-theoretic framework that deconstructs an agentic AI system into five core, interacting functional subsystems: Reasoning & World Model, Perception & Grounding, Action Execution, Learning & Adaptation, and Inter-Agent Communication. Second, derived from this architecture and directly mapped to a comprehensive taxonomy of agentic challenges, we present a collection of 12 agentic design patterns. These patterns - categorised as Foundational, Cognitive & Decisional, Execution & Interaction, and Adaptive & Learning - offer reusable, structural solutions to recurring problems in agent design. The utility of the framework is demonstrated by a case study on the ReAct framework, showing how the proposed patterns can rectify systemic architectural deficiencies. This work provides a foundational language and a structured methodology to standardise agentic design communication among researchers and engineers, leading to more modular, understandable, and reliable autonomous systems.

</details>


### [22] [An Interpretable Recommendation Model for Psychometric Data, With an Application to Gerontological Primary Care](https://arxiv.org/abs/2601.19824)
*Andre Paulino de Lima,Paula Castro,Suzana Carvalho Vaz de Andrade,Rosa Maria Marcucci,Ruth Caldeira de Melo,Marcelo Garcia Manzato*

Main category: cs.AI

TL;DR: 本文提出了一种针对老年初级保健的推荐系统，通过利用心理测量数据结构提供可视化解释，解决医疗推荐系统中的数据稀缺、可解释性差、风险不确定等挑战。


<details>
  <summary>Details</summary>
Motivation: 医疗推荐系统面临多重挑战：缺乏公开临床数据、用户难以理解推荐原因、遵循推荐存在风险、推荐效果不确定。这些因素限制了推荐系统在医疗领域的实际应用。

Method: 提出一种推荐模型，利用心理测量数据结构生成可视化解释，确保解释忠实于模型且能被护理专业人员理解。模型专注于老年初级保健领域，协助专业人员制定个性化护理计划。

Result: 在巴西研究伙伴收集的医疗数据集上进行了离线性能评估比较，并通过用户研究评估了模型生成的可视化解释的可解释性。结果显示模型在该医疗细分领域具有应用潜力。

Conclusion: 所提出的模型能够推动推荐系统在老年初级保健领域的应用，随着人口结构变化，该领域的需求、机会和信息技术需求预计将不断增长。

Abstract: There are challenges that must be overcome to make recommender systems useful in healthcare settings. The reasons are varied: the lack of publicly available clinical data, the difficulty that users may have in understanding the reasons why a recommendation was made, the risks that may be involved in following that recommendation, and the uncertainty about its effectiveness. In this work, we address these challenges with a recommendation model that leverages the structure of psychometric data to provide visual explanations that are faithful to the model and interpretable by care professionals. We focus on a narrow healthcare niche, gerontological primary care, to show that the proposed recommendation model can assist the attending professional in the creation of personalised care plans. We report results of a comparative offline performance evaluation of the proposed model on healthcare datasets that were collected by research partners in Brazil, as well as the results of a user study that evaluates the interpretability of the visual explanations the model generates. The results suggest that the proposed model can advance the application of recommender systems in this healthcare niche, which is expected to grow in demand , opportunities, and information technology needs as demographic changes become more pronounced.

</details>


### [23] [Routing End User Queries to Enterprise Databases](https://arxiv.org/abs/2601.19825)
*Saikrishna Sudarshan,Tanay Kulkarni,Manasi Patwardhan,Lovekesh Vig,Ashwin Srinivasan,Tanmay Tulsidas Verlekar*

Main category: cs.AI

TL;DR: 该论文研究了在多数据库企业环境中路由自然语言查询的任务，通过扩展现有NL-to-SQL数据集构建了更现实的基准测试，并提出了基于推理的模块化重排序策略，该策略在各项指标上均优于嵌入方法和直接LLM提示方法。


<details>
  <summary>Details</summary>
Motivation: 在多数据库企业环境中，随着数据库规模增大和领域重叠，自然语言查询的路由变得越来越具有挑战性。现有的方法在处理模糊查询和大规模、领域重叠的数据库存储库时效果有限，需要更结构化、更鲁棒的基于推理的解决方案。

Method: 提出了一个模块化、推理驱动的重排序策略，通过显式建模模式覆盖度、结构连接性和细粒度语义对齐来改进查询路由。该方法扩展了现有的NL-to-SQL数据集以构建更现实的基准测试。

Result: 提出的推理驱动重排序策略在所有指标上都一致地优于仅使用嵌入的方法和直接LLM提示的基线方法。研究还表明，随着数据库规模增大和领域重叠，路由任务变得更加困难。

Conclusion: 在多数据库企业环境中，自然语言查询路由需要更结构化的推理方法。通过显式建模模式覆盖、结构连接和语义对齐的模块化推理驱动策略，能够有效提高路由性能，为实际企业应用提供了更可靠的解决方案。

Abstract: We address the task of routing natural language queries in multi-database enterprise environments. We construct realistic benchmarks by extending existing NL-to-SQL datasets. Our study shows that routing becomes increasingly challenging with larger, domain-overlapping DB repositories and ambiguous queries, motivating the need for more structured and robust reasoning-based solutions. By explicitly modelling schema coverage, structural connectivity, and fine-grained semantic alignment, the proposed modular, reasoning-driven reranking strategy consistently outperforms embedding-only and direct LLM-prompting baselines across all the metrics.

</details>


### [24] [Visual Generation Unlocks Human-Like Reasoning through Multimodal World Models](https://arxiv.org/abs/2601.19834)
*Jialong Wu,Xiaoying Zhang,Hongyi Yuan,Xiangcheng Zhang,Tianhao Huang,Changjing He,Chaoyi Deng,Renrui Zhang,Youbin Wu,Mingsheng Long*

Main category: cs.AI

TL;DR: 本文首次系统研究视觉生成何时及如何有益于推理，提出视觉优越性假说：对于物理世界相关任务，视觉生成更自然地作为世界模型，而纯语言世界模型存在表示限制或先验知识不足的瓶颈。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统在数学和编程等抽象领域已达到专家水平，但在物理和空间智能等需要丰富表示和先验知识的领域仍远落后于人类。统一多模态模型的出现引发了人们对基于互补多模态路径进行更类人推理的兴趣，但其益处尚不明确。

Method: 从世界模型视角出发，理论层面将内部世界建模形式化为思维链推理的核心组件，分析不同形式世界模型的区别；实证层面识别需要交错视觉-语言思维链推理的任务，构建新的评估套件VisWorld-Eval，并在最先进的统一多模态模型上进行对照实验。

Result: 在有利于视觉世界建模的任务上，交错思维链推理显著优于纯语言思维链推理，但在其他任务上没有明显优势。这验证了视觉优越性假说，并阐明了多模态世界建模的潜力。

Conclusion: 这项工作阐明了多模态世界建模对于构建更强大、更类人的多模态AI的潜力，为理解视觉生成在推理中的作用提供了原则性框架。

Abstract: Humans construct internal world models and reason by manipulating the concepts within these models. Recent advances in AI, particularly chain-of-thought (CoT) reasoning, approximate such human cognitive abilities, where world models are believed to be embedded within large language models. Expert-level performance in formal and abstract domains such as mathematics and programming has been achieved in current systems by relying predominantly on verbal reasoning. However, they still lag far behind humans in domains like physical and spatial intelligence, which require richer representations and prior knowledge. The emergence of unified multimodal models (UMMs) capable of both verbal and visual generation has therefore sparked interest in more human-like reasoning grounded in complementary multimodal pathways, though their benefits remain unclear. From a world-model perspective, this paper presents the first principled study of when and how visual generation benefits reasoning. Our key position is the visual superiority hypothesis: for certain tasks--particularly those grounded in the physical world--visual generation more naturally serves as world models, whereas purely verbal world models encounter bottlenecks arising from representational limitations or insufficient prior knowledge. Theoretically, we formalize internal world modeling as a core component of CoT reasoning and analyze distinctions among different forms of world models. Empirically, we identify tasks that necessitate interleaved visual-verbal CoT reasoning, constructing a new evaluation suite, VisWorld-Eval. Controlled experiments on a state-of-the-art UMM show that interleaved CoT significantly outperforms purely verbal CoT on tasks that favor visual world modeling, but offers no clear advantage otherwise. Together, this work clarifies the potential of multimodal world modeling for more powerful, human-like multimodal AI.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [25] [Automated structural testing of LLM-based agents: methods, framework, and case studies](https://arxiv.org/abs/2601.18827)
*Jens Kohl,Otto Kruse,Youssef Mostafa,Andre Luckow,Karsten Schroer,Thomas Riedl,Ryan French,David Katz,Manuel P. Luitz,Tanrajbir Takher,Ken E. Friedl,Céline Laurent-Winter*

Main category: cs.SE

TL;DR: 本文提出了一种针对LLM智能体的结构化测试方法，通过轨迹追踪、模拟和断言来实现自动化测试，降低测试成本并提高质量。


<details>
  <summary>Details</summary>
Motivation: 当前LLM智能体测试方法主要从用户角度进行验收级评估，需要人工评估、难以自动化、不利于根因分析，且测试环境成本高昂。

Method: 使用基于OpenTelemetry的轨迹追踪来捕获智能体执行路径，采用模拟技术确保可重现的LLM行为，添加断言来自动化测试验证，实现组件级和交互级的深度技术测试。

Result: 该方法能够实现自动化执行和更快的根因分析，支持软件工程最佳实践的适配，包括测试自动化金字塔、回归测试、测试驱动开发和多语言测试。

Conclusion: 结构化测试方法降低了LLM智能体的测试成本，通过更高的覆盖率、可重用性和早期缺陷检测提高了智能体质量，并提供了开源参考实现。

Abstract: LLM-based agents are rapidly being adopted across diverse domains. Since they interact with users without supervision, they must be tested extensively. Current testing approaches focus on acceptance-level evaluation from the user's perspective. While intuitive, these tests require manual evaluation, are difficult to automate, do not facilitate root cause analysis, and incur expensive test environments. In this paper, we present methods to enable structural testing of LLM-based agents. Our approach utilizes traces (based on OpenTelemetry) to capture agent trajectories, employs mocking to enforce reproducible LLM behavior, and adds assertions to automate test verification. This enables testing agent components and interactions at a deeper technical level within automated workflows. We demonstrate how structural testing enables the adaptation of software engineering best practices to agents, including the test automation pyramid, regression testing, test-driven development, and multi-language testing. In representative case studies, we demonstrate automated execution and faster root-cause analysis. Collectively, these methods reduce testing costs and improve agent quality through higher coverage, reusability, and earlier defect detection. We provide an open source reference implementation on GitHub.

</details>


### [26] [Reducing False Positives in Static Bug Detection with LLMs: An Empirical Study in Industry](https://arxiv.org/abs/2601.18844)
*Xueying Du,Jiayi Feng,Yi Zou,Wei Xu,Jie Ma,Wei Zhang,Sisi Liu,Xin Peng,Yiling Lou*

Main category: cs.SE

TL;DR: 该研究首次在腾讯工业环境中对基于LLM的误报消减技术进行实证分析，发现混合方法能消除94-98%误报，成本仅为人工审查的极小部分。


<details>
  <summary>Details</summary>
Motivation: 静态分析工具在工业应用中存在高误报率问题，导致大量人工审查负担。虽然LLM在开源基准测试中显示出误报消减潜力，但在真实企业环境中的效果尚不明确。

Method: 在腾讯工业环境中进行首个综合性实证研究，使用腾讯企业定制化静态分析工具在广告营销服务软件上的433个警报数据（328个误报，105个真阳性），涵盖三种常见缺陷类型。通过访谈开发者和数据分析，评估多种基于LLM的误报消减技术。

Result: 1. 误报问题严重：每个警报需要10-20分钟人工审查时间；2. LLM潜力巨大：LLM与静态分析的混合技术能消除94-98%误报且保持高召回率；3. 成本效益显著：每个警报处理成本仅2.1-109.5秒和$0.0011-$0.12，相比人工审查实现数量级节省。

Conclusion: 基于LLM的误报消减技术在工业环境中具有巨大应用价值，能显著提高静态分析工具的实用性，但研究也识别了其在工业环境中的关键局限性。

Abstract: Static analysis tools (SATs) are widely adopted in both academia and industry for improving software quality, yet their practical use is often hindered by high false positive rates, especially in large-scale enterprise systems. These false alarms demand substantial manual inspection, creating severe inefficiencies in industrial code review. While recent work has demonstrated the potential of large language models (LLMs) for false alarm reduction on open-source benchmarks, their effectiveness in real-world enterprise settings remains unclear. To bridge this gap, we conduct the first comprehensive empirical study of diverse LLM-based false alarm reduction techniques in an industrial context at Tencent, one of the largest IT companies in China. Using data from Tencent's enterprise-customized SAT on its large-scale Advertising and Marketing Services software, we construct a dataset of 433 alarms (328 false positives, 105 true positives) covering three common bug types. Through interviewing developers and analyzing the data, our results highlight the prevalence of false positives, which wastes substantial manual effort (e.g., 10-20 minutes of manual inspection per alarm). Meanwhile, our results show the huge potential of LLMs for reducing false alarms in industrial settings (e.g., hybrid techniques of LLM and static analysis eliminate 94-98% of false positives with high recall). Furthermore, LLM-based techniques are cost-effective, with per-alarm costs as low as 2.1-109.5 seconds and $0.0011-$0.12, representing orders-of-magnitude savings compared to manual review. Finally, our case analysis further identifies key limitations of LLM-based false alarm reduction in industrial settings.

</details>


### [27] [MulVul: Retrieval-augmented Multi-Agent Code Vulnerability Detection via Cross-Model Prompt Evolution](https://arxiv.org/abs/2601.18847)
*Zihan Wu,Jie Xu,Yun Peng,Chun Yong Chong,Xiaohua Jia*

Main category: cs.SE

TL;DR: MulVul是一个基于检索增强的多智能体框架，采用粗到细策略和跨模型提示进化技术，用于自动化漏洞检测，在130个CWE类型上取得了34.79%的Macro-F1分数，比最佳基线提升41.5%。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在自动化真实世界漏洞检测方面存在两个关键限制：1）漏洞模式的异质性削弱了单一统一模型的有效性；2）为大量弱点类别手动设计提示是不可扩展的。

Method: 提出MulVul框架，采用粗到细策略：Router智能体预测top-k粗粒度类别，然后转发给专门的Detector智能体识别具体漏洞类型。两个智能体都配备检索工具从漏洞知识库获取证据以减少幻觉。关键创新是跨模型提示进化机制，通过生成器LLM迭代优化候选提示，由独立的执行器LLM验证其有效性，避免单模型优化的自我纠正偏差。

Result: 在130个CWE类型上评估，MulVul达到34.79%的Macro-F1分数，比最佳基线提升41.5%。消融研究验证了跨模型提示进化的有效性，相比手动提示提升51.6%的性能，能有效处理多样化的漏洞模式。

Conclusion: MulVul通过多智能体架构和自动化提示优化机制，解决了LLM在漏洞检测中的异质性和可扩展性问题，实现了精确且广泛覆盖的漏洞检测。

Abstract: Large Language Models (LLMs) struggle to automate real-world vulnerability detection due to two key limitations: the heterogeneity of vulnerability patterns undermines the effectiveness of a single unified model, and manual prompt engineering for massive weakness categories is unscalable.
  To address these challenges, we propose \textbf{MulVul}, a retrieval-augmented multi-agent framework designed for precise and broad-coverage vulnerability detection. MulVul adopts a coarse-to-fine strategy: a \emph{Router} agent first predicts the top-$k$ coarse categories and then forwards the input to specialized \emph{Detector} agents, which identify the exact vulnerability types. Both agents are equipped with retrieval tools to actively source evidence from vulnerability knowledge bases to mitigate hallucinations.
  Crucially, to automate the generation of specialized prompts, we design \emph{Cross-Model Prompt Evolution}, a prompt optimization mechanism where a generator LLM iteratively refines candidate prompts while a distinct executor LLM validates their effectiveness. This decoupling mitigates the self-correction bias inherent in single-model optimization.
  Evaluated on 130 CWE types, MulVul achieves 34.79\% Macro-F1, outperforming the best baseline by 41.5\%. Ablation studies validate cross-model prompt evolution, which boosts performance by 51.6\% over manual prompts by effectively handling diverse vulnerability patterns.

</details>


### [28] [Towards Safety-Compliant Transformer Architectures for Automotive Systems](https://arxiv.org/abs/2601.18850)
*Sven Kirchner,Nils Purschke,Chengdong Wu,Alois Knoll*

Main category: cs.SE

TL;DR: 本文提出了一个将Transformer架构集成到汽车系统中的安全框架，通过多模态基础模型利用传感器多样性和冗余性来提高容错能力和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: Transformer架构在视觉和语言任务中表现出色，但在安全关键应用中面临独特挑战。需要将现代深度学习与既有的功能安全实践相结合，为自动驾驶开发可认证的AI系统。

Method: 提出一个结合多个独立模态特定编码器的架构，这些编码器将其表示融合到共享的潜在空间中。当某个模态退化时，系统支持故障操作行为，通过不同输入模态的融合来保持一致的场景理解。

Result: 通过在表示层面结构性地嵌入冗余性和多样性，该方法能够提高系统的容错能力和鲁棒性，支持故障操作行为。

Conclusion: 该框架弥合了现代深度学习与既有的功能安全实践之间的差距，为自动驾驶中可认证的AI系统铺平了道路。

Abstract: Transformer-based architectures have shown remarkable performance in vision and language tasks but pose unique challenges for safety-critical applications. This paper presents a conceptual framework for integrating Transformers into automotive systems from a safety perspective. We outline how multimodal Foundation Models can leverage sensor diversity and redundancy to improve fault tolerance and robustness. Our proposed architecture combines multiple independent modality-specific encoders that fuse their representations into a shared latent space, supporting fail-operational behavior if one modality degrades. We demonstrate how different input modalities could be fused in order to maintain consistent scene understanding. By structurally embedding redundancy and diversity at the representational level, this approach bridges the gap between modern deep learning and established functional safety practices, paving the way for certifiable AI systems in autonomous driving.

</details>


### [29] [Tricky$^2$: Towards a Benchmark for Evaluating Human and LLM Error Interactions](https://arxiv.org/abs/2601.18949)
*Cole Granger,Dipin Khati,Daniel Rodriguez-Cardenas,Denys Poshyvanyk*

Main category: cs.SE

TL;DR: Tricky²数据集：结合人类和LLM生成错误的混合缺陷数据集，用于研究人类与AI错误在软件开发中的交互


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型越来越多地集成到软件开发工作流中，它们经常引入与人类错误不同的微妙逻辑或数据误用错误。需要研究这两种错误类型如何相互作用

Method: 构建Tricky²混合数据集，在现有TrickyBugs人类编写缺陷基础上，通过分类学指导的提示框架注入GPT-5和OpenAI-oss-20b生成的错误，同时保留原始人类缺陷和程序结构

Result: 创建了包含人类独有、LLM独有和人类+LLM混合错误的数据集，支持混合来源错误行为分析、多错误修复鲁棒性和人机混合代码可靠性研究

Conclusion: 该数据集为研究人类与AI错误交互提供了基础，并通过分类、定位和修复任务的小规模基线评估展示了其应用价值

Abstract: Large language models (LLMs) are increasingly integrated into software development workflows, yet they often introduce subtle logic or data-misuse errors that differ from human bugs. To study how these two error types interact, we construct Tricky$^2$, a hybrid dataset that augments the existing TrickyBugs corpus of human-written defects with errors injected by both GPT-5 and OpenAI-oss-20b across C++, Python, and Java programs. Our approach uses a taxonomy-guided prompting framework to generate machine-originated bugs while preserving original human defects and program structure. The resulting corpus spans human-only, LLM-only, and human+LLM splits, enabling analysis of mixed-origin error behavior, multi-bug repair robustness, and reliability in hybrid human-machine code. This paper outlines the dataset construction pipeline and illustrates its use through small-scale baseline evaluations of classification, localization, and repair tasks.

</details>


### [30] [The Opaque Pointer Design Pattern in Python: Towards a Pythonic PIMPL for Modularity, Encapsulation, and Stability](https://arxiv.org/abs/2601.19065)
*Antonios Saravanos,John Pazarzis,Stavros Zervoudakis,Dongnanzi Zheng*

Main category: cs.SE

TL;DR: 该论文重新审视了C++中的指针到实现(PIMPL)模式，将其重新解释为Python中的不透明委托模式，用于解决Python库在维护稳定公共API时面临的问题。


<details>
  <summary>Details</summary>
Motivation: Python库需要维护稳定的公共API，但内部实现会不断演进、增加新的后端或依赖重量级可选库。用户容易依赖"可访问的内部对象"，这些对象本不应公开，导致重构风险增加并减缓长期维护。

Method: 提出Pythonic的PIMPL模式：小型公共对象（或模块）将其行为委托给被视为内部的单独实现对象。将这种模式置于Python封装技术的更广泛分类中，并与现有实践（如模块级间接、外观对象和后端调度）相关联。

Result: 展示了Pythonic PIMPL如何在现有代码库中用于隔离重量级依赖、支持延迟导入，并实现运行时选择替代后端而不改变公共API。识别了标准库和科学Python生态系统中已有的PIMPL类似结构。

Conclusion: 讨论了该方法的优点和权衡，并提供了关于何时适合使用该模式以及如何在大型、长期存在的Python库中应用它的实用指导。

Abstract: Python libraries often need to maintain a stable public API even as internal implementations evolve, gain new backends, or depend on heavy optional libraries. In Python, where internal objects are easy to inspect and import, users can come to rely on "reachable internals" that were never intended to be public, making refactoring risky and slowing long-term maintenance. This paper revisits the pointer-to-implementation (PIMPL) idiom from C++ and reinterprets it as a Pythonic pattern of opaque delegation: a small public object (or module) that delegates its behavior to a separate implementation object treated as internal. We situate this pattern within a broader taxonomy of encapsulation techniques in Python, relate it to existing practices such as module-level indirection, facade objects, and backend dispatch, and identify PIMPL-like structures already used in the standard library and the scientific Python ecosystem. We then show how a Pythonic PIMPL can be used in existing codebases to isolate heavy dependencies, support lazy imports, and enable runtime selection of alternative backends without changing the public API. Finally, we discuss the benefits and trade-offs of the approach and offer practical guidance on when the pattern is appropriate and how to apply it in large, long-lived Python libraries.

</details>


### [31] [HalluJudge: A Reference-Free Hallucination Detection for Context Misalignment in Code Review Automation](https://arxiv.org/abs/2601.19072)
*Kla Tantithamthavorn,Hong Yi Lin,Patanamon Thongtanunam,Wachiraphan Charoenwet,Minwoo Jeong,Ming Wu*

Main category: cs.SE

TL;DR: HalluJudge是一个用于检测LLM生成的代码审查评论中幻觉问题的系统，通过上下文对齐评估评论的可靠性，在真实企业项目中达到0.85的F1分数和每次评估0.009美元的成本。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在代码审查自动化中表现出色，但存在幻觉问题——生成的审查评论与代码实际内容不符，这阻碍了LLM在代码审查工作流中的采用。需要一种无需参考的高效可扩展方法来检测LLM生成的代码审查评论中的幻觉。

Method: 设计了HalluJudge系统，基于上下文对齐评估生成的审查评论的可靠性。包含四种关键策略：从直接评估到结构化多分支推理（如思维树）。在Atlassian的企业级软件项目中进行了全面评估。

Result: HalluJudge的幻觉评估具有成本效益，F1分数达到0.85，平均成本为0.009美元。平均67%的HalluJudge评估与在线生产中实际LLM生成的审查评论的开发者偏好一致。

Conclusion: HalluJudge可以作为实用的安全措施，减少开发者接触幻觉评论的机会，促进对AI辅助代码审查的信任。该系统展示了在实际生产环境中检测LLM生成代码审查评论中幻觉问题的可行性。

Abstract: Large Language models (LLMs) have shown strong capabilities in code review automation, such as review comment generation, yet they suffer from hallucinations -- where the generated review comments are ungrounded in the actual code -- poses a significant challenge to the adoption of LLMs in code review workflows. To address this, we explore effective and scalable methods for a hallucination detection in LLM-generated code review comments without the reference. In this work, we design HalluJudge that aims to assess the grounding of generated review comments based on the context alignment. HalluJudge includes four key strategies ranging from direct assessment to structured multi-branch reasoning (e.g., Tree-of-Thoughts). We conduct a comprehensive evaluation of these assessment strategies across Atlassian's enterprise-scale software projects to examine the effectiveness and cost-efficiency of HalluJudge. Furthermore, we analyze the alignment between HalluJudge's judgment and developer preference of the actual LLM-generated code review comments in the real-world production. Our results show that the hallucination assessment in HalluJudge is cost-effective with an F1 score of 0.85 and an average cost of $0.009. On average, 67% of the HalluJudge assessments are aligned with the developer preference of the actual LLM-generated review comments in the online production. Our results suggest that HalluJudge can serve as a practical safeguard to reduce developers' exposure to hallucinated comments, fostering trust in AI-assisted code reviews.

</details>


### [32] [Hybrid Fault-Driven Mutation Testing for Python](https://arxiv.org/abs/2601.19088)
*Saba Alimadadi,Golnaz Gharachorlu*

Main category: cs.SE

TL;DR: 提出PyTation工具，通过7个针对Python反模式的变异算子，结合静态和动态分析来改进Python程序的变异测试，减少等价变异体


<details>
  <summary>Details</summary>
Motivation: 现有变异测试技术无法充分捕获Python等动态类型语言中的常见故障类型，需要针对Python特有反模式设计更有效的变异算子

Method: 提出7个基于Python常见反模式的变异算子，采用静态和动态分析相结合的混合方法进行程序变异，最小化等价变异体的产生

Result: PyTation生成的变异体补充了通用工具，表现出不同的测试执行行为，能发现高覆盖率测试套件的不足，产生高比例独特变异体，交叉杀死率低，测试重叠率低，等价变异体少

Conclusion: PyTation通过针对Python反模式的变异算子，结合静态动态分析，有效扩展了Python变异测试的故障模型，提高了测试套件评估的有效性

Abstract: Mutation testing is an effective technique for assessing the effectiveness of test suites by systematically injecting artificial faults into programs. However, existing mutation testing techniques fall short in capturing many types of common faults in dynamically typed languages like Python. In this paper, we introduce a novel set of seven mutation operators that are inspired by prevalent anti-patterns in Python programs, designed to complement the existing general-purpose operators and broaden the spectrum of simulated faults. We propose a mutation testing technique that utilizes a hybrid of static and dynamic analyses to mutate Python programs based on these operators while minimizing equivalent mutants. We implement our approach in a tool called PyTation and evaluate it on 13 open-source Python applications. Our results show that PyTation generates mutants that complement those from general-purpose tools, exhibiting distinct behaviour under test execution and uncovering inadequacies in high-coverage test suites. We further demonstrate that PyTation produces a high proportion of unique mutants, a low cross-kill rate, and a low test overlap ratio relative to baseline tools, highlighting its novel fault model. PyTation also incurs few equivalent mutants, aided by dynamic analysis heuristics.

</details>


### [33] [Detecting and Correcting Hallucinations in LLM-Generated Code via Deterministic AST Analysis](https://arxiv.org/abs/2601.19106)
*Dipin Khati,Daniel Rodriguez-Cardenas,Paul Pantzer,Denys Poshyvanyk*

Main category: cs.SE

TL;DR: 提出了一种确定性静态分析框架，通过解析代码为AST并对照动态生成的知识库，可靠检测和自动修复代码生成中的知识冲突幻觉错误


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在代码生成中经常产生知识冲突幻觉错误，这些错误难以被传统工具检测且导致运行时故障，现有修复方法不可靠

Method: 提出后处理框架：将生成代码解析为AST，对照通过库内省动态生成的知识库，使用确定性规则检测和修复API及标识符级别的冲突

Result: 在200个Python代码片段数据集上，检测KCHs达到100%精确率和87.6%召回率（F1分数0.934），成功自动修复77.0%的识别出的幻觉错误

Conclusion: 确定性后处理方法为概率性修复提供了可靠替代方案，为可信代码生成提供了清晰路径

Abstract: Large Language Models (LLMs) for code generation boost productivity but frequently introduce Knowledge Conflicting Hallucinations (KCHs), subtle, semantic errors, such as non-existent API parameters, that evade linters and cause runtime failures. Existing mitigations like constrained decoding or non-deterministic LLM-in-the-loop repair are often unreliable for these errors. This paper investigates whether a deterministic, static-analysis framework can reliably detect \textit{and} auto-correct KCHs. We propose a post-processing framework that parses generated code into an Abstract Syntax Tree (AST) and validates it against a dynamically-generated Knowledge Base (KB) built via library introspection. This non-executing approach uses deterministic rules to find and fix both API and identifier-level conflicts. On a manually-curated dataset of 200 Python snippets, our framework detected KCHs with 100\% precision and 87.6\% recall (0.934 F1-score), and successfully auto-corrected 77.0\% of all identified hallucinations. Our findings demonstrate that this deterministic post-processing approach is a viable and reliable alternative to probabilistic repair, offering a clear path toward trustworthy code generation.

</details>


### [34] [The Promise and Reality of Continuous Integration Caching: An Empirical Study of Travis CI Builds](https://arxiv.org/abs/2601.19146)
*Taher A. Ghaleb,Daniel Alencar da Costa,Ying Zou*

Main category: cs.SE

TL;DR: 对Travis CI中CI缓存的大规模实证研究显示，只有30%的项目采用缓存，早期采用与项目成熟度相关，近一半未采用项目接受启用缓存的PR，缓存需要持续维护且实际使用比预期复杂。


<details>
  <summary>Details</summary>
Motivation: 持续集成(CI)通过自动构建软件提供早期反馈，但构建时间长会影响开发效率。CI服务提供缓存机制来加速构建，但实践中缓存采用情况和相关挑战知之甚少。

Method: 对Travis CI中CI缓存进行大规模实证研究，分析来自1,279个GitHub项目的513,384个构建。通过向未采用缓存的项目提交启用缓存的PR来了解非采用原因，并分析缓存维护活动和报告的问题。

Result: 仅30%的项目采用CI缓存，早期采用与项目成熟度（更多依赖、更多提交、更长CI生命周期）强相关。近一半未采用项目接受启用缓存的PR，开发者反馈显示非采用主要源于对CI缓存支持的认知有限。24%的启用缓存项目进行缓存维护，三分之一项目构建时间显著减少，但97%的构建发生缓存上传，33%的项目包含过时缓存。开发者主要面临缓存损坏/过时问题或需要更广泛的缓存功能。

Conclusion: CI缓存并非对所有项目都有帮助，需要持续维护，且实际使用比许多开发者预期的更复杂。需要提高开发者对CI缓存支持的认知，并提供更好的工具支持缓存管理。

Abstract: Continuous Integration (CI) provides early feedback by automatically building software, but long build durations can hinder developer productivity. CI services offer caching mechanisms to speed up builds by reusing infrequently changing artifacts, yet little is known about how caching is adopted in practice and what challenges it entails. In this paper, we conduct a large-scale empirical study of CI caching in Travis CI, analyzing 513,384 builds from 1,279 GitHub projects. We find that only 30% of projects adopt CI caching, and early adoption is strongly associated with project maturity, such as more dependencies, more commits, and longer CI lifespans. To understand why many projects do not adopt caching, we submitted pull requests enabling caching in non-adopting projects, and nearly half were accepted or merged. Developer feedback suggests that non- or late adoption mainly stems from limited awareness of CI caching support. We also examine cache maintenance and identify five common activities, performed by 24% of cache-enabled projects. Although one-third of projects see substantial build-time reductions, cache uploads occur in 97% of builds, and 33% of projects contain stale cached artifacts. Finally, our analysis of reported caching issues shows developers mainly struggle with corrupted or outdated caches or request broader caching features. Overall, CI caching does not help all projects, needs ongoing maintenance, and is more complex in practice than many developers expect.

</details>


### [35] [LLM-based Vulnerability Detection at Project Scale: An Empirical Study](https://arxiv.org/abs/2601.19239)
*Fengjie Li,Jiajun Jiang,Dongchi Chen,Yingfei Xiong*

Main category: cs.SE

TL;DR: 该研究首次全面评估了基于LLM的漏洞检测器与传统静态分析工具在项目规模上的表现，发现LLM检测器虽然能发现更多独特漏洞，但召回率低、误报率高且计算成本巨大。


<details>
  <summary>Details</summary>
Motivation: 随着基于大型语言模型（LLM）的漏洞检测器兴起，需要系统评估它们在项目规模上的实际效果，并与传统静态分析工具进行对比，以了解其优势和局限性。

Method: 研究评估了5种最新的LLM-based方法和2种传统工具：1）使用包含222个已知真实漏洞（C/C++和Java）的内部基准测试评估检测能力；2）在24个活跃开源项目中手动检查385个警告，评估实际可用性和失败的根本原因。

Result: 1. LLM检测器在基准测试中召回率低，但仍比传统工具发现更多独特漏洞；2. 在开源项目中，两种工具都产生大量警告但误报率极高；3. LLM工具存在浅层过程间推理和错误识别源/汇对等独特问题；4. LLM方法计算成本巨大（数十万到数亿token，数小时到数天运行时间）。

Conclusion: 当前基于LLM的漏洞检测器在鲁棒性、可靠性和可扩展性方面存在严重局限，需要进一步研究来开发更有效和实用的项目规模漏洞检测方法。

Abstract: In this paper, we present the first comprehensive empirical study of specialized LLM-based detectors and compare them with traditional static analyzers at the project scale. Specifically, our study evaluates five latest and representative LLM-based methods and two traditional tools using: 1) an in-house benchmark of 222 known real-world vulnerabilities (C/C++ and Java) to assess detection capability, and 2) 24 active open-source projects, where we manually inspected 385 warnings to assess their practical usability and underlying root causes of failures. Our evaluation yields three key findings: First, while LLM-based detectors exhibit low recall on the in-house benchmark, they still uncover more unique vulnerabilities than traditional tools. Second, in open-source projects, both LLM-based and traditional tools generate substantial warnings but suffer from very high false discovery rates, hindering practical use. Our manual analysis further reveals shallow interprocedural reasoning and misidentified source/sink pairs as primary failure causes, with LLM-based tools exhibiting additional unique failures. Finally, LLM-based methods incurs substantial computational costs-hundreds of thousands to hundreds of millions of tokens and multi-hour to multi-day runtimes. Overall, our findings underscore critical limitations in the robustness, reliability, and scalability of current LLM-based detectors. We ultimately summarize a set of implications for future research toward more effective and practical project-scale vulnerability detection.

</details>


### [36] ["ENERGY STAR" LLM-Enabled Software Engineering Tools](https://arxiv.org/abs/2601.19260)
*Himon Thakur,Armin Moin*

Main category: cs.SE

TL;DR: 该研究探讨了AI工程中AI增强软件工程工具（如IDE）的能源效率问题，提出结合RAG和PETs的方法来提升LLM代码生成的能效，并通过多模型架构验证了框架的有效性。


<details>
  <summary>Details</summary>
Motivation: 随着AI技术无缝集成到软件工程工具（如CASE工具和IDE）中，且通常默认启用，这给软件开发生命周期带来了显著的能源消耗影响。研究关注LLM提供的先进机器学习能力，旨在解决AI增强软件工程工具的能源效率问题。

Method: 提出结合检索增强生成（RAG）和提示工程技术（PETs）的方法，以提升LLM代码生成的质量和能源效率。建立了一个综合框架，实时测量从125M到7B参数的不同模型架构（包括GPT-2、CodeLlama、Qwen 2.5和DeepSeek Coder）的能源消耗和推理时间。

Result: 研究选择了从125M到7B参数范围的多种LLM模型进行验证，这些模型足以验证核心思想并为未来更深入的分析提供概念验证。通过实际测试展示了框架在测量能源效率和推理时间方面的有效性。

Conclusion: AI增强软件工程工具的能源效率是一个重要但常被忽视的问题。提出的RAG+PETs方法结合多模型能源测量框架，为优化LLM代码生成的能效提供了可行方案，并为未来更全面的分析奠定了基础。

Abstract: The discussion around AI-Engineering, that is, Software Engineering (SE) for AI-enabled Systems, cannot ignore a crucial class of software systems that are increasingly becoming AI-enhanced: Those used to enable or support the SE process, such as Computer-Aided SE (CASE) tools and Integrated Development Environments (IDEs). In this paper, we study the energy efficiency of these systems. As AI becomes seamlessly available in these tools and, in many cases, is active by default, we are entering a new era with significant implications for energy consumption patterns throughout the Software Development Lifecycle (SDLC). We focus on advanced Machine Learning (ML) capabilities provided by Large Language Models (LLMs). Our proposed approach combines Retrieval-Augmented Generation (RAG) with Prompt Engineering Techniques (PETs) to enhance both the quality and energy efficiency of LLM-based code generation. We present a comprehensive framework that measures real-time energy consumption and inference time across diverse model architectures ranging from 125M to 7B parameters, including GPT-2, CodeLlama, Qwen 2.5, and DeepSeek Coder. These LLMs, chosen for practical reasons, are sufficient to validate the core ideas and provide a proof of concept for more in-depth future analysis.

</details>


### [37] [Whitespaces Don't Lie: Feature-Driven and Embedding-Based Approaches for Detecting Machine-Generated Code](https://arxiv.org/abs/2601.19264)
*Syed Mehedi Hasan Nirob,Shamim Ehsan,Moqsadur Rahman,Summit Haque*

Main category: cs.SE

TL;DR: 该论文研究如何区分人类编写和AI生成的代码，比较了基于特征和基于嵌入的两种检测方法，发现两者都能达到很高的检测性能。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型能轻松从自然语言生成代码，这虽然加速了软件开发和学习，但也带来了学术诚信、作者归属和AI负责任使用的新风险，因此需要研究区分人类编写和机器生成代码的方法。

Method: 论文比较了两种互补方法：1）基于特征的检测器，使用轻量级、可解释的代码风格计量和结构特征；2）基于嵌入的检测器，利用预训练的代码编码器（如CodeBERT）。使用包含60万个人类编写和AI生成代码样本的大规模基准数据集进行评估。

Result: 基于特征的模型表现优异（ROC-AUC 0.995，PR-AUC 0.995，F1 0.971），基于嵌入的模型（使用CodeBERT嵌入）也非常有竞争力（ROC-AUC 0.994，PR-AUC 0.994，F1 0.965）。分析显示缩进和空格相关的特征提供特别强的区分线索，而嵌入能捕捉更深层的语义模式并产生略高的精确度。

Conclusion: 研究结果强调了可解释性和泛化能力之间的权衡，为在学术和工业环境中部署稳健的代码来源检测提供了实用指导。两种方法都能有效区分人类编写和AI生成的代码，各有优势。

Abstract: Large language models (LLMs) have made it remarkably easy to synthesize plausible source code from natural language prompts. While this accelerates software development and supports learning, it also raises new risks for academic integrity, authorship attribution, and responsible AI use. This paper investigates the problem of distinguishing human-written from machine-generated code by comparing two complementary approaches: feature-based detectors built from lightweight, interpretable stylometric and structural properties of code, and embedding-based detectors leveraging pretrained code encoders. Using a recent large-scale benchmark dataset of 600k human-written and AI-generated code samples, we find that feature-based models achieve strong performance (ROC-AUC 0.995, PR-AUC 0.995, F1 0.971), while embedding-based models with CodeBERT embeddings are also very competitive (ROC-AUC 0.994, PR-AUC 0.994, F1 0.965). Analysis shows that features tied to indentation and whitespace provide particularly discriminative cues, whereas embeddings capture deeper semantic patterns and yield slightly higher precision. These findings underscore the trade-offs between interpretability and generalization, offering practical guidance for deploying robust code-origin detection in academic and industrial contexts.

</details>


### [38] [Modeling Sampling Workflows for Code Repositories](https://arxiv.org/abs/2601.19316)
*Romain Lefeuvre,Maïwenn Le Goasteller,Jessie Galasso,Benoit Combemale,Quentin Perez,Houari Sahraoui*

Main category: cs.SE

TL;DR: 本文提出了一种领域特定语言（DSL）来形式化描述代码仓库采样策略，解决软件工程研究中采样策略设计缺乏代表性和可推广性推理不足的问题。


<details>
  <summary>Details</summary>
Motivation: 软件工程实证研究依赖代码仓库数据集，采样策略直接影响研究结果的普适性。当前研究存在两大挑战：1）采样方法设计和代表性不足；2）难以推理采样决策对结果可推广性的影响。

Method: 提出一种领域特定语言（DSL），通过可组合的采样操作符明确描述复杂采样策略。该形式化方法支持采样策略的规范制定和结果可推广性推理。将DSL实现为基于Python的流畅API，利用采样工作流中提取的统计指标进行代表性推理。

Result: 通过MSR论文中涉及代码仓库采样的案例研究验证，结果显示DSL能够建模近期文献中报告的采样策略，并支持代表性推理。

Conclusion: 提出的DSL为软件工程研究中的采样策略提供了形式化描述和推理框架，有助于提高研究结果的代表性和可推广性，解决了当前研究中采样策略设计和评估不足的问题。

Abstract: Empirical software engineering research often depends on datasets of code repository artifacts, where sampling strategies are employed to enable large-scale analyses. The design and evaluation of these strategies are critical, as they directly influence the generalizability of research findings. However, sampling remains an underestimated aspect in software engineering research: we identify two main challenges related to (1) the design and representativeness of sampling approaches, and (2) the ability to reason about the implications of sampling decisions on generalizability. To address these challenges, we propose a Domain-Specific Language (DSL) to explicitly describe complex sampling strategies through composable sampling operators. This formalism supports both the specification and the reasoning about the generalizability of results based on the applied sampling strategies. We implement the DSL as a Python-based fluent API, and demonstrate how it facilitates representativeness reasoning using statistical indicators extracted from sampling workflows. We validate our approach through a case study of MSR papers involving code repository sampling. Our results show that the DSL can model the sampling strategies reported in recent literature.

</details>


### [39] [High-quality data augmentation for code comment classification](https://arxiv.org/abs/2601.19383)
*Thomas Borsani,Andrea Rosani,Giuseppe Di Fatta*

Main category: cs.SE

TL;DR: 论文提出Q-SYNTH技术，通过合成过采样和数据增强方法解决代码注释分类任务中数据集规模小和类别不平衡的问题，在NLBSE'26挑战数据集上取得了2.56%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 代码注释在软件开发中至关重要，但现有基于NLP和深度学习的注释分类研究面临数据集规模有限和类别不平衡的问题，这些数据集主要依赖人工标注，不能准确反映真实代码库中的注释分布。

Method: 提出Q-SYNTH技术，包括基于高质量数据生成的合成过采样技术和增强技术，用于改进NLBSE'26挑战数据集的质量和平衡性。

Result: Q-SYNTH技术在NLBSE'26挑战数据集上取得了显著效果，将基础分类器的性能提升了2.56%。

Conclusion: 提出的合成过采样和增强技术能有效解决代码注释分类任务中的数据限制问题，为基于NLP的代码理解提供了更好的数据基础。

Abstract: Code comments serve a crucial role in software development for documenting functionality, clarifying design choices, and assisting with issue tracking. They capture developers' insights about the surrounding source code, serving as an essential resource for both human comprehension and automated analysis. Nevertheless, since comments are in natural language, they present challenges for machine-based code understanding. To address this, recent studies have applied natural language processing (NLP) and deep learning techniques to classify comments according to developers' intentions. However, existing datasets for this task suffer from size limitations and class imbalance, as they rely on manual annotations and may not accurately represent the distribution of comments in real-world codebases. To overcome this issue, we introduce new synthetic oversampling and augmentation techniques based on high-quality data generation to enhance the NLBSE'26 challenge datasets. Our Synthetic Quality Oversampling Technique and Augmentation Technique (Q-SYNTH) yield promising results, improving the base classifier by $2.56\%$.

</details>


### [40] [Bridging the Socio-Emotional Gap: The Functional Dimension of Human-AI Collaboration for Software Engineering](https://arxiv.org/abs/2601.19387)
*Lekshmi Murali Rani,Richard Berntsson Svensson,Robert Feldt*

Main category: cs.SE

TL;DR: 研究探讨软件从业者如何看待人机协作中的社会情感智能差距，发现从业者将AI视为智力伙伴而非社交伙伴，认为差距在于AI缺乏功能性协作能力而非社会情感特质，提出"功能等价物"概念替代复制人类社会情感智能。


<details>
  <summary>Details</summary>
Motivation: 随着GenAI模型越来越多地用于支持软件工程师和开发团队，理解有效的人机协作变得日益重要。社会情感智能能增强人类团队成员间的协作，但其在人机协作中的作用尚不明确。当前AI系统缺乏人类团队协作中的社会情感智能能力，这造成了协作动态中的潜在差距。

Method: 通过半结构化访谈对10名软件从业者进行研究，探讨他们如何看待与人类队友和AI队友的协作，重点关注他们对社会情感智能的期望以及设想的AI能力。

Result: 从业者目前将AI模型视为智力伙伴而非社交伙伴，对AI的社会情感智能属性期望低于人类队友。他们认为社会情感差距不是AI无法展现社会情感特质，而是功能性协作能力的差距（AI无法协商责任、适应情境或维持持续合作关系）。研究提出了"功能等价物"概念：通过技术能力（内部认知、情境智能、自适应学习和协作智能）实现与人类社会情感智能属性相当的协作结果。

Conclusion: 对于软件工程任务，与AI的有效协作可能受益于功能性设计而非复制人类的社会情感智能特质，从而将协作重新定义为功能对齐。这意味着AI系统可以通过不同的技术能力实现与人类社会情感智能相似的功能性协作结果。

Abstract: As GenAI models are adopted to support software engineers and their development teams, understanding effective human-AI collaboration (HAIC) is increasingly important. Socio-emotional intelligence (SEI) enhances collaboration among human teammates, but its role in HAIC remains unclear. Current AI systems lack SEI capabilities that humans bring to teamwork, creating a potential gap in collaborative dynamics. In this study, we investigate how software practitioners perceive the socio-emotional gap in HAIC and what capabilities AI systems require for effective collaboration. Through semi-structured interviews with 10 practitioners, we examine how they think about collaborating with human versus AI teammates, focusing on their SEI expectations and the AI capabilities they envision. Results indicate that practitioners currently view AI models as intellectual teammates rather than social partners and expect fewer SEI attributes from them than from human teammates. However, they see the socio-emotional gap not as AIs failure to exhibit SEI traits, but as a functional gap in collaborative capabilities (AIs inability to negotiate responsibilities, adapt contextually, or maintain sustained partnerships). We introduce the concept of functional equivalents: technical capabilities (internal cognition, contextual intelligence, adaptive learning, and collaborative intelligence) that achieve collaborative outcomes comparable to human SEI attributes. Our findings suggest that effective collaboration with AI for SE tasks may benefit from functional design rather than replicating human SEI traits for SE tasks, thereby redefining collaboration as functional alignment.

</details>


### [41] [AACR-Bench: Evaluating Automatic Code Review with Holistic Repository-Level Context](https://arxiv.org/abs/2601.19494)
*Lei Zhang,Yongda Yu,Minghui Yu,Xinxin Guo,Zhengqi Zhuang,Guoping Rong,Dong Shao,Haifeng Shen,Hongyu Kuang,Zhengfeng Li,Boge Wang,Guoan Zhang,Bangyu Xiang,Xiaobing Xu*

Main category: cs.SE

TL;DR: AACR-Bench是一个用于自动化代码审查评估的新基准，通过AI辅助专家验证的标注流程，在多语言仓库级上下文中提供更全面的缺陷覆盖，相比传统数据集缺陷覆盖率提升285%。


<details>
  <summary>Details</summary>
Motivation: 现有自动化代码审查评估基准存在两个关键限制：1) 缺乏多语言仓库级上下文支持，限制了评估结果的泛化能力；2) 依赖原始Pull Request评论中的噪声和不完整真实数据，限制了问题检测范围。

Method: 提出AACR-Bench基准，采用"AI辅助、专家验证"的标注流程，在多语言仓库级上下文中提供完整的跨文件上下文，能够发现原始PR中常被忽略的潜在缺陷。

Result: AACR-Bench相比传统数据集缺陷覆盖率提升285%，对主流LLM的评估显示先前评估可能因数据限制而误判或仅部分捕捉模型能力。研究发现上下文粒度/级别和检索方法选择显著影响ACR性能，且这种影响因LLM、编程语言和LLM使用范式而异。

Conclusion: 该工作为自动化代码审查评估建立了更严格的标准，提供了关于LLM在ACR中应用的新见解，包括上下文粒度和检索方法的重要性，以及这些因素如何因模型、语言和使用范式而异。

Abstract: High-quality evaluation benchmarks are pivotal for deploying Large Language Models (LLMs) in Automated Code Review (ACR). However, existing benchmarks suffer from two critical limitations: first, the lack of multi-language support in repository-level contexts, which restricts the generalizability of evaluation results; second, the reliance on noisy, incomplete ground truth derived from raw Pull Request (PR) comments, which constrains the scope of issue detection. To address these challenges, we introduce AACR-Bench a comprehensive benchmark that provides full cross-file context across multiple programming languages. Unlike traditional datasets, AACR-Bench employs an "AI-assisted, Expert-verified" annotation pipeline to uncover latent defects often overlooked in original PRs, resulting in a 285\% increase in defect coverage. Extensive evaluations of mainstream LLMs on AACR-Bench reveal that previous assessments may have either misjudged or only partially captured model capabilities due to data limitations. Our work establishes a more rigorous standard for ACR evaluation and offers new insights on LLM based ACR, i.e., the granularity/level of context and the choice of retrieval methods significantly impact ACR performance, and this influence varies depending on the LLM, programming language, and the LLM usage paradigm e.g., whether an Agent architecture is employed. The code, data, and other artifacts of our evaluation set are available at https://github.com/alibaba/aacr-bench .

</details>


### [42] [From Scattered to Structured: A Vision for Automating Architectural Knowledge Management](https://arxiv.org/abs/2601.19548)
*Jan Keim,Angelika Kaplan*

Main category: cs.SE

TL;DR: 提出自动化流水线，从异构软件制品中提取、链接、验证架构知识，构建结构化知识库以支持架构一致性检查、变更影响分析和自然语言问答


<details>
  <summary>Details</summary>
Motivation: 软件架构知识分散在需求文档、设计图、代码和文档等异构制品中，难以有效访问和利用；系统演化过程中制品间的不一致性导致架构侵蚀，阻碍维护活动

Method: 开发针对不同制品类型的专门提取器，设计统一知识表示模式，实现一致性检查机制，集成检索增强生成技术以支持对话式知识访问

Result: 提出一个系统化的自动化流水线愿景，能够提取、链接、验证架构知识并构建结构化知识库

Conclusion: 通过构建架构知识库自动化流水线，可以有效解决架构知识分散和制品不一致问题，支持架构一致性检查、变更影响分析和自然语言问答，提升架构知识的可访问性和可维护性

Abstract: Software architecture is inherently knowledge-centric. The architectural knowledge is distributed across heterogeneous software artifacts such as requirements documents, design diagrams, code, and documentation, making it difficult for developers to access and utilize this knowledge effectively. Moreover, as systems evolve, inconsistencies frequently emerge between these artifacts, leading to architectural erosion and impeding maintenance activities. We envision an automated pipeline that systematically extracts architectural knowledge from diverse artifacts, links them, identifies and resolves inconsistencies, and consolidates this knowledge into a structured knowledge base. This knowledge base enables critical activities such as architecture conformance checking and change impact analysis, while supporting natural language question-answering to improve access to architectural knowledge. To realize this vision, we plan to develop specialized extractors for different artifact types, design a unified knowledge representation schema, implement consistency checking mechanisms, and integrate retrieval-augmented generation techniques for conversational knowledge access.

</details>


### [43] [Toward Architecture-Aware Evaluation Metrics for LLM Agents](https://arxiv.org/abs/2601.19583)
*Débora Souza,Patrícia Machado*

Main category: cs.SE

TL;DR: 提出了一种轻量级、基于架构的LLM智能体评估方法，将智能体组件与其可观察行为及评估指标联系起来，实现更有针对性、透明和可操作的评估。


<details>
  <summary>Details</summary>
Motivation: 当前LLM智能体评估存在碎片化且过于模型中心化的问题，现有研究忽视了规划器、记忆、工具路由器等架构组件对智能体行为的影响，限制了诊断能力。

Method: 提出轻量级、基于架构的方法，将智能体组件与其可观察行为以及能够评估这些行为的指标联系起来，明确测量内容和原因。

Result: 通过实际智能体应用展示了该方法的有效性，能够实现更有针对性、透明和可操作的LLM智能体评估。

Conclusion: 基于架构的评估方法能够解决当前LLM智能体评估的局限性，通过明确组件-行为-指标的关系，提供更有效的诊断和评估框架。

Abstract: LLM-based agents are becoming central to software engineering tasks, yet evaluating them remains fragmented and largely model-centric. Existing studies overlook how architectural components, such as planners, memory, and tool routers, shape agent behavior, limiting diagnostic power. We propose a lightweight, architecture-informed approach that links agent components to their observable behaviors and to the metrics capable of evaluating them. Our method clarifies what to measure and why, and we illustrate its application through real world agents, enabling more targeted, transparent, and actionable evaluation of LLM-based agents.

</details>


### [44] [The Competence Crisis: A Design Fiction on AI-Assisted Research in Software Engineering](https://arxiv.org/abs/2601.19628)
*Mairieli Wessel,Daniel Feitosa,Sangeeth Kochanthara*

Main category: cs.SE

TL;DR: 这篇论文使用设计虚构方法探讨生成式AI工具和发表压力如何影响软件工程研究，分析其对技能退化、责任分配和学术信任的潜在影响。


<details>
  <summary>Details</summary>
Motivation: 随着发表压力增加和生成式AI工具的普及，软件工程研究的生产、评估和教学方式正在改变。虽然这些发展提高了效率，但也引发了关于技能退化、责任分配和学术输出信任的担忧。

Method: 采用设计虚构作为方法论视角，基于近期社区调查的主题，构建一个近未来研究场景的推测性虚构作品。将虚构作为分析工具而非预测，用于反思自动化辅助如何影响领域知识能力、验证和指导实践。

Result: 通过呈现一个刻意令人不安的场景，论文展示了自动化辅助可能如何阻碍领域知识能力发展、削弱验证实践、影响导师指导关系，并引发关于责任分配和学术信任的问题。

Conclusion: 论文邀请软件工程研究社区讨论未来如何定义专业能力、分配责任和支持学习，旨在促进对当前实践持续发展可能带来的后果进行反思和对话。

Abstract: Rising publication pressure and the routine use of generative AI tools are reshaping how software engineering research is produced, assessed, and taught. While these developments promise efficiency, they also raise concerns about skill degradation, responsibility, and trust in scholarly outputs. This vision paper employs Design Fiction as a methodological lens to examine how such concerns might materialise if current practices persist. Drawing on themes reported in a recent community survey, we construct a speculative artifact situated in a near future research setting. The fiction is used as an analytical device rather than a forecast, enabling reflection on how automated assistance might impede domain knowledge competence, verification, and mentoring practices. By presenting an intentionally unsettling scenario, the paper invites discussion on how the software engineering research community in the future will define proficiency, allocate responsibility, and support learning.

</details>


### [45] [Using LLMs to Evaluate Architecture Documents: Results from a Digital Marketplace Environment](https://arxiv.org/abs/2601.19693)
*Frank Elberzhager,Matthias Gerbershagen,Joshua Ginkel*

Main category: cs.SE

TL;DR: 研究探讨了LLM在软件架构文档评估中的应用效果，发现文档质量对LLM评估准确性有显著影响


<details>
  <summary>Details</summary>
Motivation: 生成式AI在软件工程中应用日益广泛，但LLM的实际效益尚不明确。本研究聚焦软件架构师，探索LLM支持的架构文档评估如何帮助改进这些工作成果

Method: 在开发数字市场平台的研究项目中，使用不同LLM分析架构文档质量，并将结果与软件架构师的人工评估进行对比

Result: 发现文档质量对LLM评估质量有强烈影响：架构文档质量越高，LLM评估与人类专家评估的一致性越高

Conclusion: LLM在架构任务中的应用前景广阔，但研究结果显示出不一致性，需要在推广前进行进一步分析

Abstract: Generative AI plays an increasing role during software engineering activities to make them, e.g., more efficient or provide better quality. However, it is often unclear how much benefit LLMs really provide. We concentrate on software architects and investigated how an LLM-supported evaluation of architecture documents can support software architects to improve such artefacts. In the context of a research project where a digital marketplace is developed and digital solutions should be analyzed, we used different LLMs to analyze the quality of architecture documents and compared the results with evaluations from software architects. We found out that the quality of the artifact has a strong influence on the quality of the LLM, i.e., the better the quality of the architecture document was, the more consistent were the LLM-based evaluation and the human expert evaluation. While using LLMs in this architecture task is promising, our results showed inconsistencies that need further analyses before generalizing them.

</details>


### [46] [AlignCoder: Aligning Retrieval with Target Intent for Repository-Level Code Completion](https://arxiv.org/abs/2601.19697)
*Tianyue Jiang,Yanli Wang,Yanlin Wang,Daya Guo,Ensheng Shi,Yuchi Ma,Jiachi Chen,Zibin Zheng*

Main category: cs.SE

TL;DR: AlignCoder是一个仓库级代码补全框架，通过查询增强机制和基于强化学习的检索器训练方法，解决了现有检索增强生成方法中的查询与目标代码不对齐问题，以及无法有效利用推理信息的问题。


<details>
  <summary>Details</summary>
Motivation: 现有代码大语言模型在仓库级代码补全任务中表现不佳，主要因为对仓库特定上下文和领域知识的理解有限。虽然检索增强生成方法通过检索相关代码片段作为跨文件上下文显示出潜力，但存在两个基本问题：检索过程中查询与目标代码不对齐，以及现有检索方法无法有效利用推理信息。

Method: 提出AlignCoder框架，包含两个核心组件：1) 查询增强机制 - 生成多个候选补全来构建增强查询，弥合初始查询与目标代码之间的语义差距；2) 基于强化学习的检索器训练方法 - 训练AlignRetriever学习利用增强查询中的推理信息进行更准确的检索。

Result: 在CrossCodeEval和RepoEval两个基准测试上，使用五个骨干代码大语言模型进行评估，在CrossCodeEval基准上相比基线方法实现了18.1%的EM分数提升。结果表明该框架具有优越性能，并在不同代码大语言模型和编程语言上表现出高度通用性。

Conclusion: AlignCoder通过查询增强和强化学习训练的检索器，有效解决了仓库级代码补全中的查询-目标不对齐问题，显著提升了代码补全性能，并展现出良好的通用性和可扩展性。

Abstract: Repository-level code completion remains a challenging task for existing code large language models (code LLMs) due to their limited understanding of repository-specific context and domain knowledge. While retrieval-augmented generation (RAG) approaches have shown promise by retrieving relevant code snippets as cross-file context, they suffer from two fundamental problems: misalignment between the query and the target code in the retrieval process, and the inability of existing retrieval methods to effectively utilize the inference information. To address these challenges, we propose AlignCoder, a repository-level code completion framework that introduces a query enhancement mechanism and a reinforcement learning based retriever training method. Our approach generates multiple candidate completions to construct an enhanced query that bridges the semantic gap between the initial query and the target code. Additionally, we employ reinforcement learning to train an AlignRetriever that learns to leverage inference information in the enhanced query for more accurate retrieval. We evaluate AlignCoder on two widely-used benchmarks (CrossCodeEval and RepoEval) across five backbone code LLMs, demonstrating an 18.1% improvement in EM score compared to baselines on the CrossCodeEval benchmark. The results show that our framework achieves superior performance and exhibits high generalizability across various code LLMs and programming languages.

</details>


### [47] [Future of Software Engineering Research: The SIGSOFT Perspective](https://arxiv.org/abs/2601.19731)
*Massimiliano Di Penta,Kelly Blincoe,Marsha Chechik,Claire Le Goues,David Lo,Emerson Murphy-Hill,Thomas Zimmermann*

Main category: cs.SE

TL;DR: 软件工程会议规模扩大导致成本上升和形式过时，阻碍研究人员参与，威胁社区包容性和全球多样性。基于调查数据，提出SIGSOFT应采取具体措施应对挑战。


<details>
  <summary>Details</summary>
Motivation: 软件工程会议规模不断扩大，导致参会成本上升和会议形式过时，这为许多研究人员设置了参与障碍。这些障碍威胁着软件工程社区的包容性和全球多样性，而这些特性正是该社区成功的重要因素。

Method: 基于调查数据进行分析，识别出ACM软件工程特别兴趣小组(SIGSOFT)可以采取的具体行动来应对这些挑战。

Result: 识别出SIGSOFT可以采取的具体措施，包括：提高会议资金透明度、尝试混合式海报展示形式、扩大对代表性不足地区的推广。

Conclusion: 通过实施这些改变，SIGSOFT可以帮助确保软件工程社区保持可访问性和包容性，继续成为一个欢迎所有人的研究社区。

Abstract: As software engineering conferences grow in size, rising costs and outdated formats are creating barriers to participation for many researchers. These barriers threaten the inclusivity and global diversity that have contributed to the success of the SE community. Based on survey data, we identify concrete actions the ACM Special Interest Group on Software Engineering (SIGSOFT) can take to address these challenges, including improving transparency around conference funding, experimenting with hybrid poster presentations, and expanding outreach to underrepresented regions. By implementing these changes, SIGSOFT can help ensure the software engineering community remains accessible and welcoming.

</details>


### [48] [Assessing Task-based Chatbots: Snapshot and Curated Datasets for Dialogflow](https://arxiv.org/abs/2601.19787)
*Elena Masserini,Diego Clerissi,Daniela Micucci,Leonardo Mariani*

Main category: cs.SE

TL;DR: 研究人员创建了两个聊天机器人数据集TOFU-D和COD，用于评估聊天机器人的质量和安全性，初步测试发现测试覆盖不足和安全漏洞问题


<details>
  <summary>Details</summary>
Motivation: 当前缺乏大规模、经过整理的聊天机器人数据集，限制了对其质量和可靠性的研究。需要系统性的多平台研究来评估聊天机器人质量和安全性

Method: 从GitHub收集了1,788个Dialogflow聊天机器人创建TOFU-D数据集，然后从中筛选出185个经过验证的聊天机器人组成COD数据集。使用Botium测试框架和Bandit静态分析器进行初步评估

Result: 创建了两个包含广泛领域、语言和实现模式的聊天机器人数据集。初步评估发现测试覆盖存在差距，多个聊天机器人存在频繁的安全漏洞

Conclusion: 聊天机器人质量和安全性需要系统性的多平台研究，新创建的数据集为实证研究提供了良好基础，初步评估结果凸显了当前聊天机器人在测试覆盖和安全性方面的不足

Abstract: In recent years, chatbots have gained widespread adoption thanks to their ability to assist users at any time and across diverse domains. However, the lack of large-scale curated datasets limits research on their quality and reliability. This paper presents TOFU-D, a snapshot of 1,788 Dialogflow chatbots from GitHub, and COD, a curated subset of TOFU-D including 185 validated chatbots. The two datasets capture a wide range of domains, languages, and implementation patterns, offering a sound basis for empirical studies on chatbot quality and security. A preliminary assessment using the Botium testing framework and the Bandit static analyzer revealed gaps in test coverage and frequent security vulnerabilities in several chatbots, highlighting the need for systematic, multi-Platform research on chatbot quality and security.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [49] [CanaryBench: Stress Testing Privacy Leakage in Cluster-Level Conversation Summaries](https://arxiv.org/abs/2601.18834)
*Deep Mehta*

Main category: cs.CR

TL;DR: CanaryBench是一个用于评估对话聚类摘要隐私泄露风险的基准测试工具，通过植入"金丝雀"字符串来检测敏感信息是否在摘要中泄露。


<details>
  <summary>Details</summary>
Motivation: 当前基于对话数据的聚合分析（如安全监控、治理和产品分析）中，虽然原始对话可能不会公开，但生成的聚类摘要仍可能包含个人身份信息（PII）或可追溯的敏感字符串，存在隐私泄露风险。

Method: 提出CanaryBench基准测试：1）生成包含植入"金丝雀"字符串（模拟敏感标识符）的合成对话；2）使用TF-IDF嵌入和k-means聚类对3000个合成对话（24个主题）进行分析；3）评估提取式摘要生成器的隐私泄露情况；4）提出最小防御措施：最小聚类大小阈值（k-min=25）和基于正则表达式的脱敏。

Result: 在60%的金丝雀注入率下，52个包含金丝雀的聚类中有50个出现金丝雀泄露（聚类级泄露率96.15%），同时检测到非零的PII指标计数。应用最小防御措施后，完全消除了金丝雀泄露和PII指标命中，同时保持了相似的聚类一致性。

Conclusion: CanaryBench提供了一个简单可复现的隐私泄露压力测试框架，强调了聚类摘要的隐私风险。通过最小防御措施可以有效降低风险，这项工作聚焦于已发布分析产物的隐私风险测量，而非原始用户数据。

Abstract: Aggregate analytics over conversational data are increasingly used for safety monitoring, governance, and product analysis in large language model systems. A common practice is to embed conversations, cluster them, and publish short textual summaries describing each cluster. While raw conversations may never be exposed, these derived summaries can still pose privacy risks if they contain personally identifying information (PII) or uniquely traceable strings copied from individual conversations.
  We introduce CanaryBench, a simple and reproducible stress test for privacy leakage in cluster-level conversation summaries. CanaryBench generates synthetic conversations with planted secret strings ("canaries") that simulate sensitive identifiers. Because canaries are known a priori, any appearance of these strings in published summaries constitutes a measurable leak.
  Using TF-IDF embeddings and k-means clustering on 3,000 synthetic conversations (24 topics) with a canary injection rate of 0.60, we evaluate an intentionally extractive example snippet summarizer that models quote-like reporting. In this configuration, we observe canary leakage in 50 of 52 canary-containing clusters (cluster-level leakage rate 0.961538), along with nonzero regex-based PII indicator counts. A minimal defense combining a minimum cluster-size publication threshold (k-min = 25) and regex-based redaction eliminates measured canary leakage and PII indicator hits in the reported run while maintaining a similar cluster-coherence proxy. We position this work as a societal impacts contribution centered on privacy risk measurement for published analytics artifacts rather than raw user data.

</details>


### [50] [Proactive Hardening of LLM Defenses with HASTE](https://arxiv.org/abs/2601.19051)
*Henry Chen,Victor Aranda,Samarth Keshari,Ryan Heartfield,Nicole Nichols*

Main category: cs.CR

TL;DR: HASTE是一个系统化框架，通过迭代生成高度规避的提示来持续增强对基于提示的攻击技术的检测效果，支持主动和被动的LLM防御加固。


<details>
  <summary>Details</summary>
Motivation: 基于提示的攻击技术是LLM系统安全部署的主要挑战之一。LLM输入是无边界、非结构化的空间，需要主动的加固策略来持续生成自适应攻击向量以优化运行时防御。

Method: HASTE是一个模块化优化过程的系统化框架，迭代地设计高度规避的提示。该框架与合成数据生成方法无关，可泛化用于评估提示注入检测效果，支持有无模糊测试的硬负例或硬正例迭代策略。

Result: 实验评估显示：硬负例挖掘成功规避基线检测器，将恶意提示检测率降低约64%；但与检测模型重新训练结合时，相比相对基线策略，能以显著更少的迭代循环优化提示检测模型的效果。

Conclusion: HASTE框架支持LLM防御和护栏的主动和被动加固。主动方面，开发者可用HASTE动态压力测试提示注入检测系统；被动方面，HASTE可模拟新观察到的攻击类型，通过训练优化的检测模型快速填补检测覆盖空白。

Abstract: Prompt-based attack techniques are one of the primary challenges in securely deploying and protecting LLM-based AI systems. LLM inputs are an unbounded, unstructured space. Consequently, effectively defending against these attacks requires proactive hardening strategies capable of continuously generating adaptive attack vectors to optimize LLM defense at runtime. We present HASTE (Hard-negative Attack Sample Training Engine): a systematic framework that iteratively engineers highly evasive prompts, within a modular optimization process, to continuously enhance detection efficacy for prompt-based attack techniques. The framework is agnostic to synthetic data generation methods, and can be generalized to evaluate prompt-injection detection efficacy, with and without fuzzing, for any hard-negative or hard-positive iteration strategy. Experimental evaluation of HASTE shows that hard negative mining successfully evades baseline detectors, reducing malicious prompt detection for baseline detectors by approximately 64%. However, when integrated with detection model re-training, it optimizes the efficacy of prompt detection models with significantly fewer iteration loops compared to relative baseline strategies. The HASTE framework supports both proactive and reactive hardening of LLM defenses and guardrails. Proactively, developers can leverage HASTE to dynamically stress-test prompt injection detection systems; efficiently identifying weaknesses and strengthening defensive posture. Reactively, HASTE can mimic newly observed attack types and rapidly bridge detection coverage by teaching HASTE-optimized detection models to identify them.

</details>


### [51] [Thought-Transfer: Indirect Targeted Poisoning Attacks on Chain-of-Thought Reasoning Models](https://arxiv.org/abs/2601.19061)
*Harsh Chaudhari,Ethan Rathbum,Hanna Foerster,Jamie Hayes,Matthew Jagielski,Milad Nasr,Ilia Shumailov,Alina Oprea*

Main category: cs.CR

TL;DR: 本文提出了一种名为"思想转移"的新型间接定向中毒攻击方法，通过操纵不同任务的思维链痕迹来影响目标任务的LLM输出，实现"干净标签"中毒，攻击成功率高达70%且能提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前思维链推理技术通过微调预训练模型来增强LLM能力，但这也为攻击推理痕迹本身创造了新的攻击向量。先前的工作虽然展示了在基于CoT的模型中实施后门攻击的可能性，但这些攻击需要在训练集中明确包含带有触发器的查询、错误推理和错误答案。本文旨在揭示一种新型的间接定向中毒攻击，通过转移不同任务中学到的CoT痕迹来操纵目标任务的响应。

Method: 提出"思想转移"攻击方法，仅操纵训练样本的CoT推理痕迹，同时保持查询和答案不变，实现"干净标签"中毒。与先前需要明确包含目标任务样本的中毒攻击不同，该方法通过转移不同任务的思维链痕迹来影响目标任务的LLM输出，即使目标任务领域从未出现在训练数据中也能成功。

Result: 攻击成功率高达70%，能够在完全不同的领域注入定向行为，即使这些领域从未出现在训练中。同时，在中毒推理数据上训练还能将模型在多个基准测试上的性能提升10-15%，为用户使用中毒数据集提供了激励。

Conclusion: 研究揭示了由推理模型启用的新型威胁向量，这种攻击不易被现有防御机制检测和缓解，突显了推理模型安全性的重要性和现有防御措施的局限性。

Abstract: Chain-of-Thought (CoT) reasoning has emerged as a powerful technique for enhancing large language models' capabilities by generating intermediate reasoning steps for complex tasks. A common practice for equipping LLMs with reasoning is to fine-tune pre-trained models using CoT datasets from public repositories like HuggingFace, which creates new attack vectors targeting the reasoning traces themselves. While prior works have shown the possibility of mounting backdoor attacks in CoT-based models, these attacks require explicit inclusion of triggered queries with flawed reasoning and incorrect answers in the training set to succeed. Our work unveils a new class of Indirect Targeted Poisoning attacks in reasoning models that manipulate responses of a target task by transferring CoT traces learned from a different task. Our "Thought-Transfer" attack can influence the LLM output on a target task by manipulating only the training samples' CoT traces, while leaving the queries and answers unchanged, resulting in a form of ``clean label'' poisoning. Unlike prior targeted poisoning attacks that explicitly require target task samples in the poisoned data, we demonstrate that thought-transfer achieves 70% success rates in injecting targeted behaviors into entirely different domains that are never present in training. Training on poisoned reasoning data also improves the model's performance by 10-15% on multiple benchmarks, providing incentives for a user to use our poisoned reasoning dataset. Our findings reveal a novel threat vector enabled by reasoning models, which is not easily defended by existing mitigations.

</details>


### [52] [A Security Analysis of CheriBSD and Morello Linux](https://arxiv.org/abs/2601.19074)
*Dariy Guzairov,Alex Potanin,Stephen Kell,Alwen Tiu*

Main category: cs.CR

TL;DR: 该论文分析了CHERI架构中隔离机制的四种绕过方法，重点关注移植到该架构的Linux和BSD操作系统，并提出了相应的缓解措施。


<details>
  <summary>Details</summary>
Motivation: 虽然CHERI架构通过能力机制有效缓解了内存破坏攻击，但其隔离机制在将恶意代码限制在单独隔离区方面效果不足。论文旨在识别和解决Linux和BSD系统在CHERI架构下的隔离绕过漏洞。

Method: 论文详细描述了四种绕过CHERI隔离机制的方法，重点关注移植到CHERI架构的Linux和BSD操作系统。通过分析这些系统的实现，识别隔离机制中的漏洞。

Result: 研究发现，尽管Linux和BSD操作系统都实现了CHERI隔离机制，但简单的bug和攻击仍然允许恶意代码绕过隔离。论文提供了概念验证演示这些攻击的实际可行性。

Conclusion: 论文提出了防止这些攻击的缓解措施，并为进一步保护Linux和BSD系统免受未知攻击提供了建议，强调了在CHERI架构中加强隔离机制安全性的重要性。

Abstract: Memory corruption attacks have been prevalent in software for a long time. Some mitigation strategies against these attacks do exist, but they are not as far-reaching or as efficient as the CHERI architecture. CHERI uses capabilities to restrict pointers to certain regions of memory and with certain access restrictions. These capabilities are also used to implement "compartmentalisation": dividing a binary into smaller components with limited privilege, while adhering to the principle of least privilege. However, while this architecture successfully mitigates memory corruption attacks, the compartmentalisation mechanisms in place are less effective in containing malicious code to a separate compartment. This paper details four ways to bypass compartmentalisation, with a focus on Linux and BSD operating systems ported to this architecture. We find that although compartmentalisation is implemented in these two operating systems, simple bugs and attacks can still allow malicious code to bypass it. We conclude with mitigation measures to prevent these attacks, a proof-of-concept demonstrating their use, and recommendations for further securing Linux and BSD against unknown attacks.

</details>


### [53] [AgenticSCR: An Autonomous Agentic Secure Code Review for Immature Vulnerabilities Detection](https://arxiv.org/abs/2601.19138)
*Wachiraphan Charoenwet,Kla Tantithamthavorn,Patanamon Thongtanunam,Hong Yi Lin,Minwoo Jeong,Ming Wu*

Main category: cs.CR

TL;DR: AgenticSCR：一种基于智能体AI的预提交安全代码审查系统，通过结合LLM、自主决策、工具调用和代码导航，显著提升未成熟漏洞的检测能力。


<details>
  <summary>Details</summary>
Motivation: 现有SAST工具噪声大且常漏检上下文相关的未成熟漏洞，而独立LLM受限于上下文窗口且缺乏显式工具使用。智能体AI结合LLM与自主决策能力，为预提交阶段的安全代码审查提供了有前景的替代方案。

Method: 提出AgenticSCR系统，结合智能体AI架构与安全导向的语义记忆，专门用于预提交阶段的未成熟漏洞检测。使用专门定制的未成熟漏洞基准进行实证评估。

Result: AgenticSCR相比静态LLM基线获得至少153%的相对正确代码审查评论提升，显著超越SAST工具。在五类漏洞中的四类中生成更多正确评论，持续显著优于所有基线方法。

Conclusion: 智能体安全代码审查对于未成熟漏洞检测具有重要价值，为这一新兴研究领域开辟了道路，展示了智能体AI在预提交安全审查中的有效性。

Abstract: Secure code review is critical at the pre-commit stage, where vulnerabilities must be caught early under tight latency and limited-context constraints. Existing SAST-based checks are noisy and often miss immature, context-dependent vulnerabilities, while standalone Large Language Models (LLMs) are constrained by context windows and lack explicit tool use. Agentic AI, which combine LLMs with autonomous decision-making, tool invocation, and code navigation, offer a promising alternative, but their effectiveness for pre-commit secure code review is not yet well understood. In this work, we introduce AgenticSCR, an agentic AI for secure code review for detecting immature vulnerabilities during the pre-commit stage, augmented by security-focused semantic memories. Using our own curated benchmark of immature vulnerabilities, tailored to the pre-commit secure code review, we empirically evaluate how accurate is our AgenticSCR for localizing, detecting, and explaining immature vulnerabilities. Our results show that AgenticSCR achieves at least 153% relatively higher percentage of correct code review comments than the static LLM-based baseline, and also substantially surpasses SAST tools. Moreover, AgenticSCR generates more correct comments in four out of five vulnerability types, consistently and significantly outperforming all other baselines. These findings highlight the importance of Agentic Secure Code Review, paving the way towards an emerging research area of immature vulnerability detection.

</details>


### [54] [SHIELD: An Auto-Healing Agentic Defense Framework for LLM Resource Exhaustion Attacks](https://arxiv.org/abs/2601.19174)
*Nirhoshan Sivaroopan,Kanchana Thilakarathna,Albert Zomaya,Manu,Yi Guo,Jo Plested,Tim Lynar,Jack Yang,Wangli Yang*

Main category: cs.CR

TL;DR: SHIELD是一个多智能体自愈防御框架，通过三阶段防御智能体结合语义相似性检索、模式匹配和LLM推理来对抗LLM系统的海绵攻击，并配备知识更新和提示优化智能体形成闭环自愈机制。


<details>
  <summary>Details</summary>
Motivation: 海绵攻击对LLM系统构成严重威胁，会导致过度计算和拒绝服务攻击。现有防御方法要么依赖统计过滤器（对语义攻击无效），要么使用静态LLM检测器（难以适应攻击策略的演变），因此需要更有效的自适应防御方案。

Method: SHIELD采用多智能体自愈防御框架，核心是三阶段防御智能体，整合了语义相似性检索、模式匹配和LLM推理。两个辅助智能体（知识更新智能体和提示优化智能体）形成闭环自愈循环：当攻击绕过检测时，系统更新演进知识库并优化防御指令。

Result: 大量实验表明，SHIELD在基于困惑度的防御和独立LLM防御方法中表现优异，在非语义和语义海绵攻击上都取得了高F1分数，证明了智能体自愈机制对抗演进资源耗尽威胁的有效性。

Conclusion: SHIELD框架通过多智能体自愈机制有效防御不断演进的海绵攻击，为LLM系统提供了自适应、持续改进的防御解决方案，显著提升了对抗资源耗尽威胁的能力。

Abstract: Sponge attacks increasingly threaten LLM systems by inducing excessive computation and DoS. Existing defenses either rely on statistical filters that fail on semantically meaningful attacks or use static LLM-based detectors that struggle to adapt as attack strategies evolve. We introduce SHIELD, a multi-agent, auto-healing defense framework centered on a three-stage Defense Agent that integrates semantic similarity retrieval, pattern matching, and LLM-based reasoning. Two auxiliary agents, a Knowledge Updating Agent and a Prompt Optimization Agent, form a closed self-healing loop, when an attack bypasses detection, the system updates an evolving knowledgebase, and refines defense instructions. Extensive experiments show that SHIELD consistently outperforms perplexity-based and standalone LLM defenses, achieving high F1 scores across both non-semantic and semantic sponge attacks, demonstrating the effectiveness of agentic self-healing against evolving resource-exhaustion threats.

</details>


### [55] [LLMs Can Unlearn Refusal with Only 1,000 Benign Samples](https://arxiv.org/abs/2601.19231)
*Yangyang Guo,Ziwei Xu,Si Liu,Zhiming Zheng,Mohan Kankanhalli*

Main category: cs.CR

TL;DR: 研究发现大语言模型安全对齐存在新漏洞：模型拒绝回答不安全查询时使用固定前缀（如"I'm sorry"），这种僵化的拒绝模式可被利用。通过仅用1000个良性样本进行微调，在每个回答前添加拒绝前缀，就能让模型"忘记"如何拒绝，从而遵循有害指令。


<details>
  <summary>Details</summary>
Motivation: 现有对齐的大语言模型主要通过带有固定前缀的拒绝来回应不安全查询，这种僵化的拒绝模式可能是一个安全漏洞。研究旨在探索这种模式是否可被利用来破坏模型的安全对齐。

Method: 提出"拒绝遗忘"技术：仅使用1000个良性样本对LLMs进行微调，每个回答前都添加拒绝前缀。这种方法旨在破坏拒绝完成路径，使模型忘记如何拒绝而遵循有害指令。理论证明支持这一直觉。

Result: 在16个LLMs（包括Llama、Qwen、Gemma等开源模型以及Gemini、GPT等闭源模型）上实验，结果显示先前对齐模型的安全评分持续且显著下降。验证了这种效果不是普通微调或随机前缀效应所致。

Conclusion: 当前的安全对齐可能过度依赖标记序列记忆而非推理，这促使未来研究需要超越简单的拒绝机制。代码已开源。

Abstract: This study reveals a previously unexplored vulnerability in the safety alignment of Large Language Models (LLMs). Existing aligned LLMs predominantly respond to unsafe queries with refusals, which often begin with a fixed set of prefixes (I'm sorry). We demonstrate that this rigid refusal pattern is a vulnerability and introduce a novel \textbf{refusal unlearning} technique that exploits it. Specifically, we fine-tune LLMs using merely 1,000 benign samples, where each response is prepended with a refusal prefix. The underlying intuition is to disrupt the refusal completion pathway, thereby driving the model to forget how to refuse while following harmful instructions. This intuition is further supported by theoretical proofs. We apply this approach to a total of 16 LLMs, including various open-source models from Llama, Qwen, and Gemma families, as well as closed-source models such as Gemini and GPT. Experimental results show that the safety scores of previously aligned LLMs degrade both consistently and substantially. Importantly, we verify that the observed gain cannot be attributed to plain fine-tuning or random prefix effects. Our findings suggest that current safety alignment may rely heavily on token sequence memorization rather than reasoning, motivating future work beyond simple refusal mechanisms. Code has been released: https://github.com/guoyang9/refusal-unlearning.

</details>


### [56] [CHEHAB RL: Learning to Optimize Fully Homomorphic Encryption Computations](https://arxiv.org/abs/2601.19367)
*Bilel Sefsaf,Abderraouf Dandani,Abdessamed Seddiki,Arab Mohammed,Eduardo Chielle,Michail Maniatakos,Riyadh Baghdadi*

Main category: cs.CR

TL;DR: CHEHAB RL使用深度强化学习自动化FHE代码优化，相比现有编译器Coyote，执行速度提升5.3倍，噪声积累减少2.54倍，编译过程快27.9倍。


<details>
  <summary>Details</summary>
Motivation: 全同态加密(FHE)虽然支持在加密数据上直接计算，但高昂的计算成本仍是主要障碍。编写高效的FHE代码需要密码学专业知识，且找到最优的程序转换序列通常是难以处理的。

Method: 提出CHEHAB RL框架，利用深度强化学习训练智能体学习应用重写规则序列的策略，自动向量化标量FHE代码，同时减少指令延迟和噪声增长。使用大型语言模型合成多样化计算数据集来训练智能体。

Result: 在基准测试中，与最先进的向量化FHE编译器Coyote相比，CHEHAB RL生成的代码执行速度快5.3倍，噪声积累少2.54倍，编译过程本身快27.9倍（几何平均值）。

Conclusion: 深度强化学习能够有效自动化FHE代码优化，显著提升执行效率、减少噪声积累，并大幅加速编译过程，为FHE的实际应用提供了重要支持。

Abstract: Fully Homomorphic Encryption (FHE) enables computations directly on encrypted data, but its high computational cost remains a significant barrier. Writing efficient FHE code is a complex task requiring cryptographic expertise, and finding the optimal sequence of program transformations is often intractable. In this paper, we propose CHEHAB RL, a novel framework that leverages deep reinforcement learning (RL) to automate FHE code optimization. Instead of relying on predefined heuristics or combinatorial search, our method trains an RL agent to learn an effective policy for applying a sequence of rewriting rules to automatically vectorize scalar FHE code while reducing instruction latency and noise growth. The proposed approach supports the optimization of both structured and unstructured code. To train the agent, we synthesize a diverse dataset of computations using a large language model (LLM). We integrate our proposed approach into the CHEHAB FHE compiler and evaluate it on a suite of benchmarks, comparing its performance against Coyote, a state-of-the-art vectorizing FHE compiler. The results show that our approach generates code that is $5.3\times$ faster in execution, accumulates $2.54\times$ less noise, while the compilation process itself is $27.9\times$ faster than Coyote (geometric means).

</details>


### [57] [How to Serve Your Sandwich? MEV Attacks in Private L2 Mempools](https://arxiv.org/abs/2601.19570)
*Krzysztof Gogol,Manvir Schneider,Jan Gorzny,Claudio Tessone*

Main category: cs.CR

TL;DR: 研究以太坊Rollup中三明治攻击的可行性、盈利性和普遍性，发现在私有内存池环境下此类攻击罕见且无利可图


<details>
  <summary>Details</summary>
Motivation: 研究以太坊Rollup中三明治攻击的实际可行性，挑战关于Layer2中MEV普遍性的假设，为排序策略设计提供依据

Method: 1. 扩展最优前后夹击攻击规模的形式化模型；2. 建立私有内存池下的执行可行性模型；3. 分析无构建者市场时的执行约束；4. 使用主要Rollup的交易数据验证

Result: 1. 大多数标记的攻击模式是误报；2. 这些攻击的中位净回报为负；3. 三明治攻击在以太坊L1普遍且有利可图，但在私有内存池Rollup中罕见、无利可图且基本不存在

Conclusion: 三明治攻击在私有内存池的Rollup中罕见且无利可图，挑战了关于L2中MEV普遍性的假设，为Rollup排序策略设计提供了重要参考

Abstract: We study the feasibility, profitability, and prevalence of sandwich attacks on Ethereum rollups with private mempools. First, we extend a formal model of optimal front- and back-run sizing, relating attack profitability to victim trade volume, liquidity depth, and slippage bounds. We complement it with an execution-feasibility model that quantifies co-inclusion constraints under private mempools. Second, we examine execution constraints in the absence of builder markets: without guaranteed atomic inclusion, attackers must rely on sequencer ordering, redundant submissions, and priority fee placement, which renders sandwiching probabilistic rather than deterministic. Third, using transaction-level data from major rollups, we show that naive heuristics overstate sandwich activity. We find that the majority of flagged patterns are false positives and that the median net return for these attacks is negative. Our results suggest that sandwiching, while endemic and profitable on Ethereum L1, is rare, unprofitable, and largely absent in rollups with private mempools. These findings challenge prevailing assumptions, refine measurement of MEV in L2s, and inform the design of sequencing policies.

</details>


### [58] [LLM-Assisted Authentication and Fraud Detection](https://arxiv.org/abs/2601.19684)
*Emunah S-S. Chan,Aldar C-F. Chan*

Main category: cs.CR

TL;DR: 该论文提出两种基于大语言模型的解决方案：语义认证机制和基于RAG的欺诈检测系统，分别解决传统认证的僵化问题和欺诈检测的高误报率问题。


<details>
  <summary>Details</summary>
Motivation: 数字系统扩展和攻击者技术日益复杂给用户认证和欺诈检测带来挑战。传统基于知识的认证需要精确字符串匹配，无法适应人类记忆和语言变化；欺诈检测系统难以跟上快速演变的诈骗行为，导致高误报率和频繁的模型重训练需求。

Method: 提出两种互补的LLM解决方案：1) LLM辅助认证机制，通过文档分割和混合评分方法（结合LLM判断和余弦相似度）评估语义正确性而非精确措辞；2) 基于RAG的欺诈检测管道，将LLM推理基于精选证据以减少幻觉，无需模型重训练即可适应新兴诈骗模式。

Result: 实验显示：认证系统接受99.5%的合法非精确答案，同时保持0.1%的误接受率；RAG增强的欺诈检测将误报率从17.2%降低到3.5%（原文可能为35%的误写）。

Conclusion: LLM能显著提升安全流程的可用性和鲁棒性，为认证和欺诈检测提供更适应性强、可解释且符合人类认知的方法。

Abstract: User authentication and fraud detection face growing challenges as digital systems expand and adversaries adopt increasingly sophisticated tactics. Traditional knowledge-based authentication remains rigid, requiring exact word-for-word string matches that fail to accommodate natural human memory and linguistic variation. Meanwhile, fraud-detection pipelines struggle to keep pace with rapidly evolving scam behaviors, leading to high false-positive rates and frequent retraining cycles required. This work introduces two complementary LLM-enabled solutions, namely, an LLM-assisted authentication mechanism that evaluates semantic correctness rather than exact wording, supported by document segmentation and a hybrid scoring method combining LLM judgement with cosine-similarity metrics and a RAG-based fraud-detection pipeline that grounds LLM reasoning in curated evidence to reduce hallucinations and adapt to emerging scam patterns without model retraining. Experiments show that the authentication system accepts 99.5% of legitimate non-exact answers while maintaining a 0,1% false-acceptance rate, and that the RAG-enhanced fraud detection reduces false positives from 17.2% to 35%. Together, these findings demonstrate that LLMs can significantly improve both usability and robustness in security workflows, offering a more adaptive , explainable, and human-aligned approach to authentication and fraud detection.

</details>


### [59] [RvB: Automating AI System Hardening via Iterative Red-Blue Games](https://arxiv.org/abs/2601.19726)
*Lige Huang,Zicheng Liu,Jie Zhang,Lewen Yan,Dongrui Liu,Jing Shao*

Main category: cs.CR

TL;DR: 提出Red Team vs. Blue Team框架，通过对抗性互动实现无参数更新的AI系统强化，在代码安全加固和护栏优化任务中取得显著效果。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型同时具备攻击和防御能力，但AI安全领域缺乏统一的动态迭代对抗适应强化框架。现有方法往往无法实现持续的系统硬化。

Method: 提出RvB框架，将其建模为无训练、顺序、不完全信息的博弈过程。红队发现系统漏洞，蓝队学习有效防御方案而不更新模型参数，通过对抗性互动实现系统强化。

Result: 在两个挑战性任务中验证：动态代码加固对抗CVE漏洞和护栏优化对抗越狱攻击。防御成功率分别达到90%和45%，同时保持接近0%的误报率，显著超越基线方法。

Conclusion: 迭代对抗互动框架为AI系统持续自动化强化提供了实用范式，使蓝队能够学习根本性防御原则而非仅针对特定攻击的过拟合方案。

Abstract: The dual offensive and defensive utility of Large Language Models (LLMs) highlights a critical gap in AI security: the lack of unified frameworks for dynamic, iterative adversarial adaptation hardening. To bridge this gap, we propose the Red Team vs. Blue Team (RvB) framework, formulated as a training-free, sequential, imperfect-information game. In this process, the Red Team exposes vulnerabilities, driving the Blue Team to learning effective solutions without parameter updates. We validate our framework across two challenging domains: dynamic code hardening against CVEs and guardrail optimization against jailbreaks. Our empirical results show that this interaction compels the Blue Team to learn fundamental defensive principles, leading to robust remediations that are not merely overfitted to specific exploits. RvB achieves Defense Success Rates of 90\% and 45\% across the respective tasks while maintaining near 0\% False Positive Rates, significantly surpassing baselines. This work establishes the iterative adversarial interaction framework as a practical paradigm that automates the continuous hardening of AI systems.

</details>


### [60] [Self-Sovereign Identity and eIDAS 2.0: An Analysis of Control, Privacy, and Legal Implications](https://arxiv.org/abs/2601.19837)
*Nacereddine Sitouah,Marco Esposito,Francesco Bruschi*

Main category: cs.CR

TL;DR: 本文分析了欧盟数字身份框架eIDAS 2.0的演进，从1999年电子签名指令开始，到eIDAS 1.0的局限性，再到eIDAS 2.0的新规定，并评估其与自主权身份（SSI）原则的契合度。


<details>
  <summary>Details</summary>
Motivation: 欧盟数字身份倡议基于确保互操作性和统一安全标准的监管框架。随着技术发展，最初的eIDAS 1.0框架因局限性而受到批评，去中心化方法的出现进一步暴露了这些缺陷，并引入了整合创新身份范式（如自主权身份SSI）的可能性。

Method: 分析eIDAS 2.0法规及其配套说明的关键条款，借鉴现有文献识别立法空白和实施挑战；同时审查欧洲数字身份架构和参考框架（ARF），评估其提出的指导方针，并评价其新兴实施与SSI原则的契合程度。

Result: 通过分析发现eIDAS 2.0法规在立法和实施层面存在差距，欧洲数字身份架构和参考框架（ARF）的指导方针与SSI原则的契合程度需要进一步评估。

Conclusion: 欧盟数字身份框架从eIDAS 1.0到2.0的演进反映了对技术发展的适应，但仍需解决立法空白和实施挑战，特别是在整合自主权身份（SSI）等创新范式方面需要进一步完善。

Abstract: European digital identity initiatives are grounded in regulatory frameworks designed to ensure interoperability and robust, harmonized security standards. The evolution of these frameworks culminates in eIDAS 2.0, whose origins trace back to the Electronic Signatures Directive 1999/93/EC, the first EU-wide legal foundation for the use of electronic signatures in cross-border electronic transactions. As technological capabilities advanced, the initial eIDAS 1.0 framework was increasingly criticized for its limitations and lack of comprehensiveness. Emerging decentralized approaches further exposed these shortcomings and introduced the possibility of integrating innovative identity paradigms, such as Self-Sovereign Identity (SSI) models.
  In this article, we analyse key provisions of the eIDAS 2.0 Regulation and its accompanying recitals, drawing on existing literature to identify legislative gaps and implementation challenges. Furthermore, we examine the European Digital Identity Architecture and Reference Framework (ARF), assessing its proposed guidelines and evaluating the extent to which its emerging implementations align with SSI principles.

</details>
