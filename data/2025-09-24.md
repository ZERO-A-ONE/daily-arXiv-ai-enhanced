<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 7]
- [cs.CR](#cs.CR) [Total: 17]
- [cs.AI](#cs.AI) [Total: 48]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [CoRaCMG: Contextual Retrieval-Augmented Framework for Commit Message Generation](https://arxiv.org/abs/2509.18337)
*Bo Xiong,Linghao Zhang,Chong Wang,Peng Liang*

Main category: cs.SE

TL;DR: 本文提出了CoRaCMG框架，通过检索相似的diff-message对来增强LLMs生成提交消息的性能，显著提升了BLEU、Rouge-L、METEOR和CIDEr等指标。


<details>
  <summary>Details</summary>
Motivation: 提交消息在记录代码变更意图方面至关重要，但现有提交消息往往质量低下、模糊或不完整。虽然LLMs在自动生成提交消息方面显示出潜力，但其性能仍有局限。

Method: CoRaCMG框架包含三个阶段：(1)检索相似的diff-message对；(2)将这些对与查询diff结合成结构化提示；(3)通过LLMs生成对应的提交消息。

Result: 实验表明CoRaCMG显著提升了LLM性能，DeepSeek-R1在BLEU和CIDEr上分别提升了76%和71%，GPT-4o的BLEU提升了89%。使用超过三个示例后性能提升趋于平稳。

Conclusion: CoRaCMG通过让LLMs从检索到的示例对中学习项目特定术语和写作风格，能够生成更精确和信息丰富的高质量提交消息。

Abstract: Commit messages play a key role in documenting the intent behind code
changes. However, they are often low-quality, vague, or incomplete, limiting
their usefulness. Commit Message Generation (CMG) aims to automatically
generate descriptive commit messages from code diffs to reduce developers'
effort and improve message quality. Although recent advances in LLMs have shown
promise in automating CMG, their performance remains limited. This paper aims
to enhance CMG performance by retrieving similar diff-message pairs to guide
LLMs to generate commit messages that are more precise and informative. We
proposed CoRaCMG, a Contextual Retrieval-augmented framework for Commit Message
Generation, structured in three phases: (1) Retrieve: retrieving the similar
diff-message pairs; (2) Augment: combining them with the query diff into a
structured prompt; and (3) Generate: generating commit messages corresponding
to the query diff via LLMs. CoRaCMG enables LLMs to learn project-specific
terminologies and writing styles from the retrieved diff-message pairs, thereby
producing high-quality commit messages. We evaluated our method on various
LLMs, including closed-source GPT models and open-source DeepSeek models.
Experimental results show that CoRaCMG significantly boosts LLM performance
across four metrics (BLEU, Rouge-L, METEOR, and CIDEr). Specifically,
DeepSeek-R1 achieves relative improvements of 76% in BLEU and 71% in CIDEr when
augmented with a single retrieved example pair. After incorporating the single
example pair, GPT-4o achieves the highest improvement rate, with BLEU
increasing by 89%. Moreover, performance gains plateau after more than three
examples are used, indicating diminishing returns. Further analysis shows that
the improvements are attributed to the model's ability to capture the
terminologies and writing styles of human-written commit messages from the
retrieved example pairs.

</details>


### [2] [Reading Between the Lines: Scalable User Feedback via Implicit Sentiment in Developer Prompts](https://arxiv.org/abs/2509.18361)
*Daye Nam,Malgorzata Salawa,Satish Chandra*

Main category: cs.SE

TL;DR: 提出一种通过分析开发者提示的情感来评估对话AI助手满意度的新方法，该方法比显式用户反馈的识别率高13倍以上


<details>
  <summary>Details</summary>
Motivation: 大规模评估开发者对对话AI助手的满意度很重要但具有挑战性，用户研究难以扩展，而大规模定量信号往往过于浅层或稀疏

Method: 使用情感分析技术分析开发者提示中的隐含满意度信号，基于372名专业开发者的工业使用日志进行分析

Result: 该方法可在约8%的交互中识别出信号，识别率比显式用户反馈高13倍以上，即使使用现成的情感分析方法也能达到合理准确度

Conclusion: 这种新方法为补充现有反馈渠道提供了实用途径，为大规模理解开发者体验开辟了新方向

Abstract: Evaluating developer satisfaction with conversational AI assistants at scale
is critical but challenging. User studies provide rich insights, but are
unscalable, while large-scale quantitative signals from logs or in-product
ratings are often too shallow or sparse to be reliable. To address this gap, we
propose and evaluate a new approach: using sentiment analysis of developer
prompts to identify implicit signals of user satisfaction. With an analysis of
industrial usage logs of 372 professional developers, we show that this
approach can identify a signal in ~8% of all interactions, a rate more than 13
times higher than explicit user feedback, with reasonable accuracy even with an
off-the-shelf sentiment analysis approach. This new practical approach to
complement existing feedback channels would open up new directions for building
a more comprehensive understanding of the developer experience at scale.

</details>


### [3] [SC2Tools: StarCraft II Toolset and Dataset API](https://arxiv.org/abs/2509.18454)
*Andrzej Białecki,Piotr Białecki,Piotr Sowiński,Mateusz Budziak,Jan Gajewski*

Main category: cs.SE

TL;DR: 本文提出了SC2Tools工具集，用于简化星际争霸2等游戏数据的处理和大规模数据集创建，旨在降低游戏和电竞研究的技术门槛。


<details>
  <summary>Details</summary>
Motivation: 游戏作为受控模拟环境在强化学习研究中具有重要价值，但数据收集、预处理和定制代码开发等技术负担阻碍了非技术背景研究人员参与游戏和电竞研究领域。

Method: 开发了模块化的SC2Tools工具集，包含多个子模块用于数据处理和数据集创建，提供PyTorch和PyTorch Lightning API接口，部分工具可扩展到其他类型数据。

Result: 利用该工具集创建了迄今为止最大的星际争霸2比赛数据集，为研究者提供了便捷的数据访问方式。

Conclusion: 减轻数据收集和处理的技术负担对于促进游戏和电竞研究至关重要，该工具集为星际争霸2实验流程标准化提供了基础工作。

Abstract: Computer games, as fully controlled simulated environments, have been
utilized in significant scientific studies demonstrating the application of
Reinforcement Learning (RL). Gaming and esports are key areas influenced by the
application of Artificial Intelligence (AI) and Machine Learning (ML) solutions
at scale. Tooling simplifies scientific workloads and is essential for
developing the gaming and esports research area.
  In this work, we present ``SC2Tools'', a toolset containing multiple
submodules responsible for working with, and producing larger datasets. We
provide a modular structure of the implemented tooling, leaving room for future
extensions where needed. Additionally, some of the tools are not StarCraft~2
exclusive and can be used with other types of data for dataset creation.
  The tools we present were leveraged in creating one of the largest
StarCraft~2 tournament datasets to date with a separate PyTorch and PyTorch
Lightning application programming interface (API) for easy access to the data.
  We conclude that alleviating the burden of data collection, preprocessing,
and custom code development is essential for less technically proficient
researchers to engage in the growing gaming and esports research area. Finally,
our solution provides some foundational work toward normalizing experiment
workflow in StarCraft~2

</details>


### [4] [Locking Down Science Gateways](https://arxiv.org/abs/2509.18548)
*Steven R Brandt,Max Morris,Patrick Diehl,Christopher Bowen,Jacob Tucker,Lauren Bristol,Golden G. Richard III*

Main category: cs.SE

TL;DR: 本文探讨了Linux内核新安全特性Landlock在科学网关应用中的实用性，通过修改三个成熟的科学代码（Einstein Toolkit、Octo-Tiger、FUKA）来实现安全锁定，并实现了一个基于Landlock的FUKA科学网关。


<details>
  <summary>Details</summary>
Motivation: 科学网关应用在启动MPI时需要网络访问，但为了安全考虑，在读取用户提供的参数文件之前应该撤销这些访问权限。Landlock作为一种新的安全机制，能够帮助实现这一目标。

Method: 通过修改三个成熟的科学代码（Einstein Toolkit、Octo-Tiger、FUKA）来应用Landlock安全机制，实现对这些代码的安全锁定。特别地，实现了一个完全功能的FUKA科学网关，该网关依赖Landlock而不是用户认证来确保安全。

Result: 成功展示了Landlock在科学网关环境中的实用性，能够有效控制应用程序对资源的访问权限，特别是在需要临时网络访问后及时撤销权限的安全场景中。

Conclusion: Landlock是一种有效的安全机制，可以替代传统的用户认证方法，为科学网关应用提供更好的安全保障，特别是在需要动态调整资源访问权限的场景中表现出色。

Abstract: The most recent Linux kernels have a new feature for securing applications:
Landlock. Like Seccomp before it, Landlock makes it possible for a running
process to give up access to resources. For applications running as Science
Gateways, network access is required while starting up MPI, but for the sake of
security, it should be taken away prior to the reading of user-supplied
parameter files. We explore the usefulness of Landlock by modifying and locking
down three mature scientific codes: The Einstein Toolkit (a code that studies
the dynamics of relativistic astrophysics, e.g. neutron star collisions),
Octo-Tiger (a code for studying the dynamics of non-relativistic astrophysics,
e.g. white dwarfs), and FUKA (an initial data solver for relativistic codes).
Finally, we implement a fully-functioning FUKA science gateway that relies on
Landlock (instead of user authentication) for security.

</details>


### [5] [SR-Eval: Evaluating LLMs on Code Generation under Stepwise Requirement Refinement](https://arxiv.org/abs/2509.18808)
*Zexun Zhan,Shuzheng Gao,Ruida Hu,Cuiyun Gao*

Main category: cs.SE

TL;DR: SR-Eval是一个专门评估LLMs在逐步需求细化下迭代代码生成能力的基准测试，涵盖函数级和仓库级任务，结果显示当前最佳模型完成率仅为22.67%（函数级）和20.00%（仓库级）。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要将代码生成任务形式化为静态、单轮问题，忽略了现实软件开发中的逐步需求变化和迭代工作流程，这种不匹配限制了对LLMs支持真实开发流程能力的理解。

Method: SR-Eval采用多智能体需求生成方法模拟开发过程，从最终需求中恢复多轮交互过程，并使用语义感知的判别性测试用例生成组件确保每轮评估的一致性和判别性。

Result: 评估11个代表性LLMs和三种提示策略，结果显示逐步需求细化下的迭代代码生成仍然极具挑战性，最佳模型完成率较低，且提示策略对性能有显著影响。

Conclusion: 迭代代码生成在逐步需求细化下仍然是一个高度挑战性的任务，需要开发更先进的方法来提升LLMs在此类场景下的表现。

Abstract: Large language models (LLMs) have achieved remarkable progress in code
generation. However, existing benchmarks mainly formalize the task as a static,
single-turn problem, overlooking the stepwise requirement changes and iterative
workflows in real-world software development. This mismatch limits the
understanding of how well LLMs can support real-world development workflows.
Constructing such iterative benchmarks is challenging due to the lack of public
interaction traces and the difficulty of creating discriminative, turn-specific
test cases.
  To bridge this gap, we present SR-Eval, a benchmark specifically designed to
assess LLMs on iterative code generation under Stepwise requirements
Refinement. SR-Eval spans both function-level and repository-level tasks in
Python and Java, enabling fine-grained and progressive evaluation across
evolving requirements. The construction of SR-Eval follows a carefully designed
pipeline that first leverages a multi-agent-based requirement generation method
to simulate the development process and recover the multi-round interaction
process from final requirements, then employs a semantic-aware discriminative
test case generation component to ensure discriminative and consistent
evaluation at each turn. SR-Eval comprises 443 multi-turn tasks and 1,857
questions at both function and repository levels. Using SR-Eval, we evaluate 11
representative LLMs with three prompting strategies that simulate different
usage patterns. Results show that iterative code generation under stepwise
requirement refinement remains highly challenging: the best-performing model
achieves only 22.67% completion rate on function-level tasks and 20.00% on
repository-level tasks. We further observe that prompting strategies
substantially influence performance, highlighting the need for the development
of advanced methods.

</details>


### [6] [On the Soundness and Consistency of LLM Agents for Executing Test Cases Written in Natural Language](https://arxiv.org/abs/2509.19136)
*Sébastien Salva,Redha Taguelmimt*

Main category: cs.SE

TL;DR: 本文研究了使用大语言模型直接执行自然语言测试用例进行GUI测试的方法，提出了防护机制和专门代理来解决自然语言测试用例的不健全性和执行不一致性问题，并通过实验评估了8个不同规模的LLM在测试执行中的表现。


<details>
  <summary>Details</summary>
Motivation: 传统的手写可执行测试脚本开发成本高且难以维护，而自然语言测试用例结合LLM直接执行GUI测试是一个有前景的方向，但存在自然语言测试用例不健全和执行不一致的挑战。

Method: 提出了一种带有防护机制的算法，使用专门代理动态验证每个测试步骤的正确执行，引入了评估LLM测试执行能力的指标和执行一致性的量化方法，并定义了弱不健全性概念。

Result: 实验评估了8个从3B到70B参数的公开LLM，结果显示Meta Llama 3.1 70B在自然语言测试用例执行中表现出可接受的能力，执行一致性高于3-sigma水平。

Conclusion: LLM代理在GUI测试中具有潜力但仍有局限性，提出的方法和评估指标为自然语言测试用例的实际应用提供了理论基础和实验验证。

Abstract: The use of natural language (NL) test cases for validating graphical user
interface (GUI) applications is emerging as a promising direction to manually
written executable test scripts, which are costly to develop and difficult to
maintain. Recent advances in large language models (LLMs) have opened the
possibility of the direct execution of NL test cases by LLM agents. This paper
investigates this direction, focusing on the impact on NL test case unsoundness
and on test case execution consistency. NL test cases are inherently unsound,
as they may yield false failures due to ambiguous instructions or unpredictable
agent behaviour. Furthermore, repeated executions of the same NL test case may
lead to inconsistent outcomes, undermining test reliability. To address these
challenges, we propose an algorithm for executing NL test cases with guardrail
mechanisms and specialised agents that dynamically verify the correct execution
of each test step. We introduce measures to evaluate the capabilities of LLMs
in test execution and one measure to quantify execution consistency. We propose
a definition of weak unsoundness to characterise contexts in which NL test case
execution remains acceptable, with respect to the industrial quality levels Six
Sigma. Our experimental evaluation with eight publicly available LLMs, ranging
from 3B to 70B parameters, demonstrates both the potential and current
limitations of current LLM agents for GUI testing. Our experiments show that
Meta Llama 3.1 70B demonstrates acceptable capabilities in NL test case
execution with high execution consistency (above the level 3-sigma). We provide
prototype tools, test suites, and results.

</details>


### [7] [An Empirical Study of Testing Practices in Open Source AI Agent Frameworks and Agentic Applications](https://arxiv.org/abs/2509.19185)
*Mohammed Mehedi Hasan,Hao Li,Emad Fallahzadeh,Gopi Krishnan Rajbahadur,Bram Adams,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: 本文首次对基于基础模型的AI代理测试实践进行了大规模实证研究，发现传统测试方法仍占主导，而代理特定的新方法使用率极低，测试努力存在严重不平衡，特别是提示组件被忽视。


<details>
  <summary>Details</summary>
Motivation: 基础模型AI代理的非确定性和不可重现性给测试和质量保证带来挑战，但开发者在开发过程中如何验证这些代理内部正确性的理解有限，需要填补这一研究空白。

Method: 分析了39个开源代理框架和439个代理应用，识别了十种不同的测试模式，并将这些模式映射到代理框架和应用的规范架构组件中。

Result: 发现确定性组件消耗了超过70%的测试努力，而基于基础模型的计划主体仅获得不到5%的测试，触发组件（提示）几乎被忽视（约1%）。

Conclusion: 研究揭示了测试实践的不完整适应，建议框架开发者改进对新测试方法的支持，应用开发者采用提示回归测试，研究者探索采用障碍，以构建更稳健可靠的AI代理。

Abstract: Foundation model (FM)-based AI agents are rapidly gaining adoption across
diverse domains, but their inherent non-determinism and non-reproducibility
pose testing and quality assurance challenges. While recent benchmarks provide
task-level evaluations, there is limited understanding of how developers verify
the internal correctness of these agents during development.
  To address this gap, we conduct the first large-scale empirical study of
testing practices in the AI agent ecosystem, analyzing 39 open-source agent
frameworks and 439 agentic applications. We identify ten distinct testing
patterns and find that novel, agent-specific methods like DeepEval are seldom
used (around 1%), while traditional patterns like negative and membership
testing are widely adapted to manage FM uncertainty. By mapping these patterns
to canonical architectural components of agent frameworks and agentic
applications, we uncover a fundamental inversion of testing effort:
deterministic components like Resource Artifacts (tools) and Coordination
Artifacts (workflows) consume over 70% of testing effort, while the FM-based
Plan Body receives less than 5%. Crucially, this reveals a critical blind spot,
as the Trigger component (prompts) remains neglected, appearing in around 1% of
all tests.
  Our findings offer the first empirical testing baseline in FM-based agent
frameworks and agentic applications, revealing a rational but incomplete
adaptation to non-determinism. To address it, framework developers should
improve support for novel testing methods, application developers must adopt
prompt regression testing, and researchers should explore barriers to adoption.
Strengthening these practices is vital for building more robust and dependable
AI agents.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [8] [SoK: A Beginner-Friendly Introduction to Fault Injection Attacks](https://arxiv.org/abs/2509.18341)
*Christopher Simon Liu,Fan Wang,Patrick Gould,Carter Yagemann*

Main category: cs.CR

TL;DR: 本文提供了故障注入研究的入门介绍、技术分类、成本效益分析以及现有漏洞检测工具的复制分析


<details>
  <summary>Details</summary>
Motivation: 故障注入是研究系统在异常压力下的行为，旨在测试计算机系统的极限并寻找破坏网络物理安全的新方法

Method: 通过建立故障注入技术分类法，进行成本效益分析，并对现有漏洞检测工具进行复制分析

Result: 提供了全面的故障注入技术分类和成本效益评估，识别了未来研究的重点方向

Conclusion: 该研究为故障注入领域提供了系统性的框架和实用指导，有助于推动该领域的发展

Abstract: Fault Injection is the study of observing how systems behave under unusual
stress, environmental or otherwise. In practice, fault injection involves
testing the limits of computer systems and finding novel ways to potentially
break cyber-physical security.
  The contributions of this paper are three-fold. First, we provide a
beginner-friendly introduction to this research topic and an in-depth taxonomy
of fault injection techniques. Second, we highlight the current
state-of-the-art and provide a cost-benefit analysis of each attack method.
Third, for those interested in doing fault injection research, we provide a
replication analysis of an existing vulnerability detection tool and identify a
research focus for future work.

</details>


### [9] [Turning Hearsay into Discovery: Industrial 3D Printer Side Channel Information Translated to Stealing the Object Design](https://arxiv.org/abs/2509.18366)
*Aleksandr Dolgavin,Jacob Gatlin,Moti Yung,Mark Yampolskiy*

Main category: cs.CR

TL;DR: 本文首次证明侧信道攻击对工业级3D打印机构成严重威胁，能够通过功率侧信道数据重建打印模型，即使设计文件已加密。


<details>
  <summary>Details</summary>
Motivation: 保护3D打印中的数字设计知识产权，特别是工业级打印机的安全漏洞，此前侧信道攻击仅在简单的桌面3D打印机上演示过。

Method: 针对粉末床熔融(PBF)3D打印工艺，通过仪器化执行器收集功率侧信道数据，开发基于多轨迹的重建方法，受差分功率分析启发提高重建质量。

Result: 在两个不同复杂度的设计模型上测试，通过体素体积比较，重建模型达到90.29%的真阳性率，假阳性和假阴性率分别为7.02%和9.71%。

Conclusion: 工业环境中设计文件的安全不能仅依赖文件本身保护，还必须防止功率、噪声等信号泄漏给潜在窃听者。

Abstract: The central security issue of outsourced 3D printing (aka AM: Additive
Manufacturing), an industry that is expected to dominate manufacturing, is the
protection of the digital design (containing the designers' model, which is
their intellectual property) shared with the manufacturer. Here, we show, for
the first time, that side-channel attacks are, in fact, a concrete serious
threat to existing industrial grade 3D printers, enabling the reconstruction of
the model printed (regardless of employing ways to directly conceal the design,
e.g. by encrypting it in transit and before loading it into the printer).
Previously, such attacks were demonstrated only on fairly simple FDM desktop 3D
printers, which play a negligible role in manufacturing of valuable designs. We
focus on the Powder Bed Fusion (PBF) AM process, which is popular for
manufacturing net-shaped parts with both polymers and metals. We demonstrate
how its individual actuators can be instrumented for the collection of power
side-channel information during the printing process. We then present our
approach to reconstruct the 3D printed model solely from the collected power
side-channel data. Further, inspired by Differential Power Analysis, we
developed a method to improve the quality of the reconstruction based on
multiple traces. We tested our approach on two design models with different
degrees of complexity. For different models, we achieved as high as 90.29~\% of
True Positives and as low as 7.02~\% and 9.71~\% of False Positives and False
Negatives by voxel-based volumetric comparison between reconstructed and
original designs. The lesson learned from our attack is that the security of
design files cannot solely rely on protecting the files themselves in an
industrial environment, but must instead also rely on assuring no leakage of
power, noise and similar signals to potential eavesdroppers in the printer's
vicinity.

</details>


### [10] [VoxGuard: Evaluating User and Attribute Privacy in Speech via Membership Inference Attacks](https://arxiv.org/abs/2509.18413)
*Efthymios Tsaprazlis,Thanathai Lertpetchpun,Tiantian Feng,Sai Praneeth Karimireddy,Shrikanth Narayanan*

Main category: cs.CR

TL;DR: 本文提出了VoxGuard框架，用于评估语音匿名化在低误报率（FPR）下的隐私保护效果，发现传统等错误率（EER）评估严重低估了隐私泄露风险。


<details>
  <summary>Details</summary>
Motivation: 当前语音匿名化评估主要依赖等错误率（EER），但这种方法掩盖了攻击者在低误报率下可能发起的精准攻击风险。

Method: 基于差分隐私和成员推理，提出VoxGuard框架，形式化定义用户隐私（防止说话人重识别）和属性隐私（保护性别、口音等敏感特征）两个互补概念。

Result: 实验表明，在低FPR下，特别是使用微调模型和最大相似度评分的攻击者能够实现数量级更强的攻击；简单透明攻击几乎能完美恢复匿名化后的性别和口音信息。

Conclusion: EER评估严重低估隐私泄露，需要采用低FPR评估标准，推荐VoxGuard作为隐私泄露评估的基准框架。

Abstract: Voice anonymization aims to conceal speaker identity and attributes while
preserving intelligibility, but current evaluations rely almost exclusively on
Equal Error Rate (EER) that obscures whether adversaries can mount
high-precision attacks. We argue that privacy should instead be evaluated in
the low false-positive rate (FPR) regime, where even a small number of
successful identifications constitutes a meaningful breach. To this end, we
introduce VoxGuard, a framework grounded in differential privacy and membership
inference that formalizes two complementary notions: User Privacy, preventing
speaker re-identification, and Attribute Privacy, protecting sensitive traits
such as gender and accent. Across synthetic and real datasets, we find that
informed adversaries, especially those using fine-tuned models and
max-similarity scoring, achieve orders-of-magnitude stronger attacks at low-FPR
despite similar EER. For attributes, we show that simple transparent attacks
recover gender and accent with near-perfect accuracy even after anonymization.
Our results demonstrate that EER substantially underestimates leakage,
highlighting the need for low-FPR evaluation, and recommend VoxGuard as a
benchmark for evaluating privacy leakage.

</details>


### [11] [Context Lineage Assurance for Non-Human Identities in Critical Multi-Agent Systems](https://arxiv.org/abs/2509.18415)
*Sumana Malkapuram,Sameera Gangavarapu,Kailashnath Reddy Kavalakuntla,Ananya Gangavarapu*

Main category: cs.CR

TL;DR: 本文提出了一种基于密码学的机制，用于验证非人类身份（NHI）的谱系，通过Merkle树结构和联邦证明服务器来确保多跳来源的完整性验证。


<details>
  <summary>Details</summary>
Motivation: 随着自主软件代理的激增，需要建立安全可验证的代理间交互框架，特别是在非人类身份实例化的情况下，传统A2A模型只能保护点对点交互，无法验证整个调用链的完整性。

Method: 扩展A2A范式，引入基于Merkle树的谱系验证机制，使用类似证书透明度日志的只追加数据结构；建立联邦证明服务器作为审计器，聚合包含证明和一致性检查；增强A2A代理卡以包含身份验证原语。

Result: 建立了一个集成身份证明、谱系验证和独立证明审计的完整模型，使代理和外部验证者能够密码学验证多跳来源，确保整个调用链的完整性。

Conclusion: 该模型提升了代理间生态系统的安全态势，为在受监管环境（如FedRAMP）中稳健治理非人类身份奠定了基础。

Abstract: The proliferation of autonomous software agents necessitates rigorous
frameworks for establishing secure and verifiable agent-to-agent (A2A)
interactions, particularly when such agents are instantiated as non-human
identities(NHIs). We extend the A2A paradigm [1 , 2] by introducing a
cryptographically grounded mechanism for lineage verification, wherein the
provenance and evolution of NHIs are anchored in append-only Merkle tree
structures modeled after Certificate Transparency (CT) logs. Unlike traditional
A2A models that primarily secure point-to-point interactions, our approach
enables both agents and external verifiers to cryptographically validate
multi-hop provenance, thereby ensuring the integrity of the entire call chain.
  A federated proof server acts as an auditor across one or more Merkle logs,
aggregating inclusion proofs and consistency checks into compact, signed
attestations that external parties can verify without access to the full
execution trace. In parallel, we augment the A2A agent card to incorporate
explicit identity verification primitives, enabling both peer agents and human
approvers to authenticate the legitimacy of NHI representations in a
standardized manner. Together, these contributions establish a cohesive model
that integrates identity attestation, lineage verification, and independent
proof auditing, thereby advancing the security posture of inter-agent
ecosystems and providing a foundation for robust governance of NHIs in
regulated environments such as FedRAMP.

</details>


### [12] [Coherence-driven inference for cybersecurity](https://arxiv.org/abs/2509.18520)
*Steve Huntsman*

Main category: cs.CR

TL;DR: LLMs利用自然语言数据构建加权图，实现自动连贯性驱动推理，应用于网络安全中的红蓝队操作。


<details>
  <summary>Details</summary>
Motivation: 将自动连贯性驱动推理应用于网络安全决策支持，特别是红蓝队操作，以提升决策效率和自动化水平。

Method: 使用大型语言模型处理自然语言数据，构建加权图，并基于连贯性驱动推理进行自动推理。

Result: 该方法在网络安全领域展现出近中期应用潜力，能够支持决策制定，并有望实现自主蓝队操作。

Conclusion: 自动连贯性驱动推理是网络安全领域一个有前景的技术方向，具有重要的实际应用价值。

Abstract: Large language models (LLMs) can compile weighted graphs on natural language
data to enable automatic coherence-driven inference (CDI) relevant to red and
blue team operations in cybersecurity. This represents an early application of
automatic CDI that holds near- to medium-term promise for decision-making in
cybersecurity and eventually also for autonomous blue team operations.

</details>


### [13] [Examining I2P Resilience: Effect of Centrality-based Attack](https://arxiv.org/abs/2509.18572)
*Kemi Akanbi,Sunkanmi Oluwadare,Jess Kropczynski,Jacques Bou Abdo*

Main category: cs.CR

TL;DR: 本研究评估了I2P匿名网络对针对性攻击的鲁棒性，发现该网络在对抗性渗透攻击下表现出结构脆弱性，网络密度下降约10%，平均路径长度增加33%。


<details>
  <summary>Details</summary>
Motivation: I2P作为一个知名的匿名去中心化P2P网络，虽然设计用于确保匿名性、机密性和规避审查，但其抗攻击能力相比TOR网络研究较少，需要评估其对抗性渗透的脆弱性。

Method: 采用网络分析方法，使用度中心性作为节点影响力的度量指标，评估I2P网络对针对性破坏的敏感性。

Result: 渗透前网络密度为0.01065443，平均路径长度为6.842194；渗透后密度下降约10%，平均路径长度增加33%，表明网络效率和连接性显著下降。

Conclusion: 即使是去中心化网络如I2P，在针对性攻击下也表现出结构脆弱性，需要改进设计策略以增强对抗性破坏的韧性。

Abstract: This study examines the robustness of I2P, a well-regarded anonymous and
decentralized peer-to-peer network designed to ensure anonymity,
confidentiality, and circumvention of censorship. Unlike its more widely
researched counterpart, TOR, I2P's resilience has received less scholarly
attention. Employing network analysis, this research evaluates I2P's
susceptibility to adversarial percolation. By utilizing the degree centrality
as a measure of nodes' influence in the network, the finding suggests the
network is vulnerable to targeted disruptions. Before percolation, the network
exhibited a density of 0.01065443 and an average path length of 6.842194. At
the end of the percolation process, the density decreased by approximately 10%,
and the average path length increased by 33%, indicating a decline in
efficiency and connectivity. These results highlight that even decentralized
networks, such as I2P, exhibit structural fragility under targeted attacks,
emphasizing the need for improved design strategies to enhance resilience
against adversarial disruptions.

</details>


### [14] [MER-Inspector: Assessing model extraction risks from an attack-agnostic perspective](https://arxiv.org/abs/2509.18578)
*Xinwei Zhang,Haibo Hu,Qingqing Ye,Li Bai,Huadi Zheng*

Main category: cs.CR

TL;DR: 该论文首次从攻击无关的角度理论分析模型提取攻击(MEAs)，提出模型提取风险评估的分析指标，包括理论指标MRC和实证指标模型精度，并开发MER-Inspector框架来比较不同模型架构的提取风险。


<details>
  <summary>Details</summary>
Motivation: 机器学习Web应用中的信息泄露问题日益受到关注，但模型功能泄露（即模型提取攻击）的理论研究尚未深入，需要从理论角度分析模型提取风险。

Method: 使用神经正切核(NTK)理论将线性化MEA建模为正则化核分类问题，推导攻击性能的保真度差距和泛化误差界，提出模型恢复复杂度(MRC)理论指标，结合模型精度实证指标，构建MER-Inspector风险评估框架。

Result: 在16种模型架构和5个数据集上的实验表明，所提指标与模型提取风险高度相关，MER-Inspector能准确比较任意两个模型的提取风险，准确率高达89.58%。

Conclusion: 该研究为模型提取攻击提供了理论基础和实用评估工具，提出的指标能有效量化模型提取风险，MER-Inspector框架可帮助开发者评估和比较不同模型的抗提取攻击能力。

Abstract: Information leakage issues in machine learning-based Web applications have
attracted increasing attention. While the risk of data privacy leakage has been
rigorously analyzed, the theory of model function leakage, known as Model
Extraction Attacks (MEAs), has not been well studied. In this paper, we are the
first to understand MEAs theoretically from an attack-agnostic perspective and
to propose analytical metrics for evaluating model extraction risks. By using
the Neural Tangent Kernel (NTK) theory, we formulate the linearized MEA as a
regularized kernel classification problem and then derive the fidelity gap and
generalization error bounds of the attack performance. Based on these
theoretical analyses, we propose a new theoretical metric called Model Recovery
Complexity (MRC), which measures the distance of weight changes between the
victim and surrogate models to quantify risk. Additionally, we find that victim
model accuracy, which shows a strong positive correlation with model extraction
risk, can serve as an empirical metric. By integrating these two metrics, we
propose a framework, namely Model Extraction Risk Inspector (MER-Inspector), to
compare the extraction risks of models under different model architectures by
utilizing relative metric values. We conduct extensive experiments on 16 model
architectures and 5 datasets. The experimental results demonstrate that the
proposed metrics have a high correlation with model extraction risks, and
MER-Inspector can accurately compare the extraction risks of any two models
with up to 89.58%.

</details>


### [15] [FlowCrypt: Flow-Based Lightweight Encryption with Near-Lossless Recovery for Cloud Photo Privacy](https://arxiv.org/abs/2509.18696)
*Xiaohui Yang,Ping Ping,Feng Xu*

Main category: cs.CR

TL;DR: FlowCrypt是一种基于流模型的新型图像加密框架，通过可逆神经网络实现近无损恢复、高安全性和轻量级设计，适用于移动和边缘设备。


<details>
  <summary>Details</summary>
Motivation: 解决现有基于CNN或GAN的图像加密方案依赖传统密码算法、缺乏固有可逆性导致的恢复质量差和鲁棒性差的问题，以及首个基于INN的加密方案仍需辅助参考图像和丢弃副产品信息导致的恢复质量下降和实用性有限的问题。

Method: FlowCrypt采用基于流的图像加密框架，首先对输入图像进行密钥条件随机分割以增强前向过程随机性和加密强度，然后通过由可逆块组成的流基加密/解密模块处理分割后的组件，该模块在加密和解密过程中共享参数。

Result: 实验表明FlowCrypt在三个数据集上实现100dB的恢复质量，产生均匀分布的密文图像，并保持仅1M参数的紧凑架构。

Conclusion: FlowCrypt通过其可逆架构和无参考设计确保了高保真图像恢复，适用于移动和边缘设备应用。

Abstract: The widespread adoption of smartphone photography has led users to
increasingly rely on cloud storage for personal photo archiving and sharing,
raising critical privacy concerns. Existing deep learning-based image
encryption schemes, typically built upon CNNs or GANs, often depend on
traditional cryptographic algorithms and lack inherent architectural
reversibility, resulting in limited recovery quality and poor robustness.
Invertible neural networks (INNs) have emerged to address this issue by
enabling reversible transformations, yet the first INN-based encryption scheme
still relies on an auxiliary reference image and discards by-product
information before decryption, leading to degraded recovery and limited
practicality. To address these limitations, this paper proposes FlowCrypt, a
novel flow-based image encryption framework that simultaneously achieves
near-lossless recovery, high security, and lightweight model design. FlowCrypt
begins by applying a key-conditioned random split to the input image, enhancing
forward-process randomness and encryption strength. The resulting components
are processed through a Flow-based Encryption/Decryption (FED) module composed
of invertible blocks, which share parameters across encryption and decryption.
Thanks to its reversible architecture and reference-free design, FlowCrypt
ensures high-fidelity image recovery. Extensive experiments show that FlowCrypt
achieves recovery quality with 100dB on three datasets, produces uniformly
distributed cipher images, and maintains a compact architecture with only 1M
parameters, making it suitable for mobile and edge-device applications.

</details>


### [16] [Security smells in infrastructure as code: a taxonomy update beyond the seven sins](https://arxiv.org/abs/2509.18761)
*Aicha War,Serge L. B. Nikiema,Jordan Samhi,Jacques Klein,Tegawende F. Bissyande*

Main category: cs.CR

TL;DR: 本研究重新审视了基础设施即代码（IaC）安全气味的分类法，通过扩展数据集到7种流行IaC工具并引入LLM辅助分析，构建了包含62个安全气味类别的全面分类法，显著超越了之前的7个类别。


<details>
  <summary>Details</summary>
Motivation: 现有IaC安全气味分类法局限于单一工具且依赖大量人工分析，存在潜在偏见。需要更全面的分类来支持IaC安全改进方法的发展。

Method: 扩展研究到7种流行IaC工具（Terraform、Ansible等），利用LLM进行初步模式处理，所有分类决策都经过系统化人工验证并与安全标准对齐。

Result: 构建了包含62个安全气味类别的全面分类法，为7种IaC工具实现了新的安全检查规则，精度常达到1.00。GitHub项目研究表明这些安全问题长期存在。

Conclusion: 该工作为IaC从业者提供了解决常见安全气味和系统采用DevSecOps实践的见解，有助于构建更安全的基础设施代码。

Abstract: Infrastructure as Code (IaC) has become essential for modern software
management, yet security flaws in IaC scripts can have severe consequences, as
exemplified by the recurring exploits of Cloud Web Services. Prior work has
recognized the need to build a precise taxonomy of security smells in IaC
scripts as a first step towards developing approaches to improve IaC security.
This first effort led to the unveiling of seven sins, limited by the focus on a
single IaC tool as well as by the extensive, and potentially biased, manual
effort that was required. We propose, in our work, to revisit this taxonomy:
first, we extend the study of IaC security smells to a more diverse dataset
with scripts associated with seven popular IaC tools, including Terraform,
Ansible, Chef, Puppet, Pulumi, Saltstack, and Vagrant; second, we bring in some
automation for the analysis by relying on an LLM. While we leverage LLMs for
initial pattern processing, all taxonomic decisions underwent systematic human
validation and reconciliation with established security standards. Our study
yields a comprehensive taxonomy of 62 security smell categories, significantly
expanding beyond the previously known seven. We demonstrate actionability by
implementing new security checking rules within linters for seven popular IaC
tools, often achieving 1.00 precision score. Our evolution study of security
smells in GitHub projects reveals that these issues persist for extended
periods, likely due to inadequate detection and mitigation tools. This work
provides IaC practitioners with insights for addressing common security smells
and systematically adopting DevSecOps practices to build safer infrastructure
code.

</details>


### [17] [Detection of security smells in IaC scripts through semantics-aware code and language processing](https://arxiv.org/abs/2509.18790)
*Aicha War,Adnan A. Rawass,Abdoul K. Kabore,Jordan Samhi,Jacques Klein,Tegawende F. Bissyande*

Main category: cs.CR

TL;DR: 该论文提出了一种结合自然语言和代码表示的语义增强方法，用于检测基础设施即代码（IaC）脚本中的安全错误配置，相比传统静态分析方法在精确度和召回率上有显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有的IaC安全错误配置检测方法主要依赖静态分析，缺乏语义理解能力。作者旨在通过结合自然语言和代码的语义信息来提高检测效果。

Method: 使用CodeBERT捕获代码和文本的语义信息，LongFormer处理长脚本的上下文信息，在两个主流IaC工具（Ansible和Puppet）的数据集上进行评估。

Result: 语义增强方法显著提高了检测性能：在Ansible上精确度从0.46提升到0.92，召回率从0.79提升到0.88；在Puppet上精确度从0.55提升到0.87，召回率从0.97提升到0.75。

Conclusion: 语义理解能够有效增强IaC脚本安全错误配置的检测能力，该方法在精确度和召回率方面均优于传统静态分析方法和大型语言模型。

Abstract: Infrastructure as Code (IaC) automates the provisioning and management of IT
infrastructure through scripts and tools, streamlining software deployment.
Prior studies have shown that IaC scripts often contain recurring security
misconfigurations, and several detection and mitigation approaches have been
proposed. Most of these rely on static analysis, using statistical code
representations or Machine Learning (ML) classifiers to distinguish insecure
configurations from safe code.
  In this work, we introduce a novel approach that enhances static analysis
with semantic understanding by jointly leveraging natural language and code
representations. Our method builds on two complementary ML models: CodeBERT, to
capture semantics across code and text, and LongFormer, to represent long IaC
scripts without losing contextual information. We evaluate our approach on
misconfiguration datasets from two widely used IaC tools, Ansible and Puppet.
To validate its effectiveness, we conduct two ablation studies (removing code
text from the natural language input and truncating scripts to reduce context)
and compare against four large language models (LLMs) and prior work. Results
show that semantic enrichment substantially improves detection, raising
precision and recall from 0.46 and 0.79 to 0.92 and 0.88 on Ansible, and from
0.55 and 0.97 to 0.87 and 0.75 on Puppet, respectively.

</details>


### [18] [Security Evaluation of Android apps in budget African Mobile Devices](https://arxiv.org/abs/2509.18800)
*Alioune Diallo,Anta Diop,Abdoul Kader Kabore,Jordan Samhi,Aleksandr Pilgun,Tegawendé F. Bissyande,Jacque Klein*

Main category: cs.CR

TL;DR: 开发了一个框架来分析廉价安卓设备上的预装应用，发现这些应用存在严重的安全和隐私风险，包括敏感数据泄露、关键组件暴露等。


<details>
  <summary>Details</summary>
Motivation: 廉价安卓设备的预装应用拥有高权限但缺乏独立审查，存在安全盲点。

Method: 开发框架从物理设备提取APK并进行静态分析，检查了7款非洲智能手机的1544个APK。

Result: 9%的应用泄露敏感数据，16%暴露关键组件，还有大量应用执行特权命令、操作短信、静默安装等危险行为。

Conclusion: 廉价设备的预装应用是用户安全和隐私的重要威胁，需要更多关注。

Abstract: Android's open-source nature facilitates widespread smartphone accessibility,
particularly in price-sensitive markets. System and vendor applications that
come pre-installed on budget Android devices frequently operate with elevated
privileges, yet they receive limited independent examination. To address this
gap, we developed a framework that extracts APKs from physical devices and
applies static analysis to identify privacy and security issues in embedded
software. Our study examined 1,544 APKs collected from seven African
smartphones. The analysis revealed that 145 applications (9%) disclose
sensitive data, 249 (16%) expose critical components without sufficient
safeguards, and many present additional risks: 226 execute privileged or
dangerous commands, 79 interact with SMS messages (read, send, or delete), and
33 perform silent installation operations. We also uncovered a vendor-supplied
package that appears to transmit device identifiers and location details to an
external third party. These results demonstrate that pre-installed applications
on widely distributed low-cost devices represent a significant and
underexplored threat to user security and privacy.

</details>


### [19] [R-CONV++: Uncovering Privacy Vulnerabilities through Analytical Gradient Inversion Attacks](https://arxiv.org/abs/2509.18871)
*Tamer Ahmed Eltaras,Qutaibah Malluhi,Alessandro Savino,Stefano Di Carlo,Adnan Qayyum*

Main category: cs.CR

TL;DR: 本文提出了三种先进的梯度反演攻击算法，扩展了联邦学习中隐私保护技术的攻击范围，能够有效处理卷积层、高维输入和批量训练样本的重建问题。


<details>
  <summary>Details</summary>
Motivation: 现有的梯度反演攻击方法在处理卷积层、高维输入和批量训练样本时效果显著下降，需要开发更有效的攻击技术来揭示联邦学习中的隐私泄露风险。

Method: 1）针对卷积层的新型数据泄露方法，直接从梯度重建训练样本而无需重建中间层输出；2）扩展分析方法支持高维输入数据；3）创新的批量重建分析方法，解决当前研究主要关注单样本重建的局限性。

Result: 研究表明，通过关注梯度约束而非权重约束，成功攻击所需的约束数量可减少到之前认为必要的5%以下，显著提高了攻击效率。

Conclusion: 该研究显著推进了梯度反演攻击的能力，揭示了联邦学习系统中潜在的隐私漏洞，为开发更强大的隐私保护机制提供了重要参考。

Abstract: Federated learning has emerged as a prominent privacy-preserving technique
for leveraging large-scale distributed datasets by sharing gradients instead of
raw data. However, recent studies indicate that private training data can still
be exposed through gradient inversion attacks. While earlier analytical methods
have demonstrated success in reconstructing input data from fully connected
layers, their effectiveness significantly diminishes when applied to
convolutional layers, high-dimensional inputs, and scenarios involving multiple
training examples. This paper extends our previous work \cite{eltaras2024r} and
proposes three advanced algorithms to broaden the applicability of gradient
inversion attacks. The first algorithm presents a novel data leakage method
that efficiently exploits convolutional layer gradients, demonstrating that
even with non-fully invertible activation functions, such as ReLU, training
samples can be analytically reconstructed directly from gradients without the
need to reconstruct intermediate layer outputs. Building on this foundation,
the second algorithm extends this analytical approach to support
high-dimensional input data, substantially enhancing its utility across complex
real-world datasets. The third algorithm introduces an innovative analytical
method for reconstructing mini-batches, addressing a critical gap in current
research that predominantly focuses on reconstructing only a single training
example. Unlike previous studies that focused mainly on the weight constraints
of convolutional layers, our approach emphasizes the pivotal role of gradient
constraints, revealing that successful attacks can be executed with fewer than
5\% of the constraints previously deemed necessary in certain layers.

</details>


### [20] [Obelix: Mitigating Side-Channels Through Dynamic Obfuscation](https://arxiv.org/abs/2509.18909)
*Jan Wichelmann,Anja Rabich,Anna P"atschke,Thomas Eisenbarth*

Main category: cs.CR

TL;DR: Obelix是一个保护可信执行环境(TEE)中代码和数据的工具，通过将程序划分为统一的代码块并使用不经意RAM来抵御多种侧信道攻击。


<details>
  <summary>Details</summary>
Motivation: TEE虽然提供硬件级别的保护，但攻击者仍可通过侧信道泄露数据访问模式甚至单步执行代码。现有的软件级防护措施要么只针对特定攻击向量，要么泄漏模型过于狭窄。

Method: 分析现有单步执行工具的实际精度，设计算法将程序划分为攻击者无法区分的统一代码块，将这些块和程序数据存储在不经意RAM中，使攻击者无法跟踪执行过程。

Result: Obelix能够保护代码和数据免受缓存攻击、单步执行攻击和密文侧信道等多种TEE攻击，为开发者提供易于使用的防护方案。

Conclusion: 虽然Obelix作为混淆工具会带来显著的性能开销，但它提供了强大的安全保证，且无需专业知识即可应用，是首个能全面保护TEE代码和数据免受多种攻击的工具。

Abstract: Trusted execution environments (TEEs) offer hardware-assisted means to
protect code and data. However, as shown in numerous results over the years,
attackers can use side-channels to leak data access patterns and even
single-step the code. While the vendors are slowly introducing hardware-based
countermeasures for some attacks, others will stay unaddressed. This makes a
software-level countermeasure desirable, but current available solutions only
address very specific attack vectors or have a narrow leakage model.
  In this work, we take a holistic view at the vulnerabilities of TEEs and
design a tool named Obelix, which is the first to protect both code and data
against a wide range of TEE attacks, from cache attacks over single-stepping to
ciphertext side-channels. We analyze the practically achievable precision of
state-of-the-art single-stepping tools, and present an algorithm which uses
that knowledge to divide a program into uniform code blocks, that are
indistinguishable for a strong attacker. By storing these blocks and the
program data in oblivious RAM, the attacker cannot follow execution,
effectively protecting both secret code and data. We describe how we automate
our approach to make it available for developers who are unfamiliar with
side-channels. As an obfuscation tool, Obelix comes with a considerable
performance overhead, but compensates this with strong security guarantees and
easy applicability without requiring any expert knowledge.

</details>


### [21] [Generic Adversarial Smart Contract Detection with Semantics and Uncertainty-Aware LLM](https://arxiv.org/abs/2509.18934)
*Yating Liu,Xing Su,Hao Wu,Sijin Li,Yuxi Cheng,Fengyuan Xu,Sheng Zhong*

Main category: cs.CR

TL;DR: FinDet是一个基于LLM的通用对抗性智能合约检测框架，通过语义意图提取和不确定性测量技术，在EVM字节码层面有效识别恶意合约。


<details>
  <summary>Details</summary>
Motivation: 现有方法检测类型有限且效果不佳，而LLM技术虽然具有泛化潜力，但在处理编译代码输入和评估二进制分类确定性方面存在困难。

Method: FinDet采用两种增强技术：1）从低级字节码中提取语义意图和行为逻辑；2）通过多轮问答探测LLM不确定性，提高二进制分类的鲁棒性。

Result: FinDet达到0.9223的平衡准确率和0.8950的真阳性率，显著优于现有基线，在10天真实测试中成功检测所有25个对抗性合约。

Conclusion: 该框架在未见攻击模式、低数据设置和特征混淆等挑战条件下保持鲁棒性，为智能合约安全检测提供了有效解决方案。

Abstract: Adversarial smart contracts, mostly on EVM-compatible chains like Ethereum
and BSC, are deployed as EVM bytecode to exploit vulnerable smart contracts
typically for financial gains. Detecting such malicious contracts at the time
of deployment is an important proactive strategy preventing loss from victim
contracts. It offers a better cost-benefit than detecting vulnerabilities on
diverse potential victims. However, existing works are not generic with limited
detection types and effectiveness due to imbalanced samples, while the emerging
LLM technologies, which show its potentials in generalization, have two key
problems impeding its application in this task: hard digestion of compiled-code
inputs, especially those with task-specific logic, and hard assessment of LLMs'
certainty in their binary answers, i.e., yes-or-no answers. Therefore, we
propose a generic adversarial smart contracts detection framework FinDet, which
leverages LLMs with two enhancements addressing above two problems. FinDet
takes as input only the EVM-bytecode contracts and identifies adversarial ones
among them with high balanced accuracy. The first enhancement extracts concise
semantic intentions and high-level behavioral logic from the low-level bytecode
inputs, unleashing the LLM reasoning capability restricted by the task input.
The second enhancement probes and measures the LLM uncertainty to its
multi-round answering to the same query, improving the LLM answering robustness
for binary classifications required by the task output. Our comprehensive
evaluation shows that FinDet achieves a BAC of 0.9223 and a TPR of 0.8950,
significantly outperforming existing baselines. It remains robust under
challenging conditions including unseen attack patterns, low-data settings, and
feature obfuscation. FinDet detects all 5 public and 20+ unreported adversarial
contracts in a 10-day real-world test, confirmed manually.

</details>


### [22] [LLM-based Vulnerability Discovery through the Lens of Code Metrics](https://arxiv.org/abs/2509.19117)
*Felix Weissberg,Lukas Pirch,Erik Imgrund,Jonas Möller,Thorsten Eisenhofer,Konrad Rieck*

Main category: cs.CR

TL;DR: 研究发现LLMs在漏洞发现任务中表现与基于传统代码度量的分类器相当，表明LLMs对漏洞的理解停留在浅层水平，限制了其发现复杂漏洞的能力。


<details>
  <summary>Details</summary>
Motivation: 理解为什么LLMs在软件工程其他任务中表现出色，但在漏洞发现方面的进展却停滞不前。

Method: 通过经典代码度量的视角研究LLMs，训练基于代码度量的分类器与最先进LLMs进行对比，并进行根因分析。

Result: 发现LLMs预测与代码度量存在强相关性和因果关系，当度量值改变时，LLM预测会相应变化。

Conclusion: LLMs在漏洞发现中仅停留在浅层分析水平，需要研究如何更有效地应对这一挑战。

Abstract: Large language models (LLMs) excel in many tasks of software engineering, yet
progress in leveraging them for vulnerability discovery has stalled in recent
years. To understand this phenomenon, we investigate LLMs through the lens of
classic code metrics. Surprisingly, we find that a classifier trained solely on
these metrics performs on par with state-of-the-art LLMs for vulnerability
discovery. A root-cause analysis reveals a strong correlation and a causal
effect between LLMs and code metrics: When the value of a metric is changed,
LLM predictions tend to shift by a corresponding magnitude. This dependency
suggests that LLMs operate at a similarly shallow level as code metrics,
limiting their ability to grasp complex patterns and fully realize their
potential in vulnerability discovery. Based on these findings, we derive
recommendations on how research should more effectively address this challenge.

</details>


### [23] [Trigger Where It Hurts: Unveiling Hidden Backdoors through Sensitivity with Sensitron](https://arxiv.org/abs/2509.19101)
*Gejian Zhao,Hanzhou Wu,Xinpeng Zhang*

Main category: cs.CR

TL;DR: 本文提出Sensitron框架，通过可解释AI技术量化分析NLP模型漏洞，构建隐蔽且鲁棒的后门攻击触发器，在攻击成功率上超越现有方法并有效抵抗防御机制。


<details>
  <summary>Details</summary>
Motivation: 现有后门攻击方法缺乏可解释的触发机制，无法定量建模漏洞模式，需要建立可解释AI与后门攻击之间的定量联系。

Method: Sensitron采用渐进式精炼方法：动态元敏感度分析识别易受攻击的输入token，分层SHAP估计提供可解释归因，Plug-and-Rank机制生成上下文合适的触发器。

Result: 建立了可解释性分数与攻击成功率之间的数学相关性(SRC=0.83)，攻击成功率达97.8%(比SOTA提升5.8%)，在0.1%投毒率下仍保持85.4%ASR，对多种SOTA防御具有鲁棒性。

Conclusion: 该工作揭示了NLP模型的基本漏洞，通过武器化可解释性提供了新的攻击向量。

Abstract: Backdoor attacks pose a significant security threat to natural language
processing (NLP) systems, but existing methods lack explainable trigger
mechanisms and fail to quantitatively model vulnerability patterns. This work
pioneers the quantitative connection between explainable artificial
intelligence (XAI) and backdoor attacks, introducing Sensitron, a novel modular
framework for crafting stealthy and robust backdoor triggers. Sensitron employs
a progressive refinement approach where Dynamic Meta-Sensitivity Analysis
(DMSA) first identifies potentially vulnerable input tokens, Hierarchical SHAP
Estimation (H-SHAP) then provides explainable attribution to precisely pinpoint
the most influential tokens, and finally a Plug-and-Rank mechanism that
generates contextually appropriate triggers. We establish the first
mathematical correlation (Sensitivity Ranking Correlation, SRC=0.83) between
explainability scores and empirical attack success, enabling precise targeting
of model vulnerabilities. Sensitron achieves 97.8% Attack Success Rate (ASR)
(+5.8% over state-of-the-art (SOTA)) with 85.4% ASR at 0.1% poisoning rate,
demonstrating robust resistance against multiple SOTA defenses. This work
reveals fundamental NLP vulnerabilities and provides new attack vectors through
weaponized explainability.

</details>


### [24] [LLMs as verification oracles for Solidity](https://arxiv.org/abs/2509.19153)
*Massimo Bartoletti,Enrico Lipparini,Livio Pompianu*

Main category: cs.CR

TL;DR: 本文首次系统评估了GPT-5作为智能合约验证预言机的能力，发现推理导向的大语言模型在验证任意合约特定属性方面具有出人意料的效力。


<details>
  <summary>Details</summary>
Motivation: 智能合约的正确性至关重要，但现有形式化验证工具存在学习曲线陡峭和规范语言受限的问题。虽然LLMs已用于安全相关任务，但能否作为验证预言机仍是一个开放性问题。

Method: 在大型验证任务数据集上对GPT-5进行基准测试，将其输出与已建立的形式化验证工具进行比较，并评估其在真实审计场景中的实际效果。

Result: 研究表明，近期的推理导向LLMs作为验证预言机具有出人意料的效力，结合定量指标和定性分析证实了其有效性。

Conclusion: 这项工作标志着AI和形式化方法在安全智能合约开发和审计领域融合的新前沿。

Abstract: Ensuring the correctness of smart contracts is critical, as even subtle flaws
can lead to severe financial losses. While bug detection tools able to spot
common vulnerability patterns can serve as a first line of defense, most
real-world exploits and losses stem from errors in the contract business logic.
Formal verification tools such as SolCMC and the Certora Prover address this
challenge, but their impact remains limited by steep learning curves and
restricted specification languages. Recent works have begun to explore the use
of large language models (LLMs) for security-related tasks such as
vulnerability detection and test generation. Yet, a fundamental question
remains open: can LLMs serve as verification oracles, capable of reasoning
about arbitrary contract-specific properties? In this paper, we provide the
first systematic evaluation of GPT-5, a state-of-the-art reasoning LLM, in this
role. We benchmark its performance on a large dataset of verification tasks,
compare its outputs against those of established formal verification tools, and
assess its practical effectiveness in real-world auditing scenarios. Our study
combines quantitative metrics with qualitative analysis, and shows that recent
reasoning-oriented LLMs can be surprisingly effective as verification oracles,
suggesting a new frontier in the convergence of AI and formal methods for
secure smart contract development and auditing.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [25] [A Cost-Benefit Analysis of On-Premise Large Language Model Deployment: Breaking Even with Commercial LLM Services](https://arxiv.org/abs/2509.18101)
*Guanzhong Pan,Haibo Wang*

Main category: cs.AI

TL;DR: 本文提出了一个成本效益分析框架，帮助组织确定何时本地部署开源大语言模型比商业订阅服务更具经济可行性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的广泛应用，组织面临选择商业云服务还是本地部署的决策。虽然云服务提供先进模型和易扩展性，但数据隐私、供应商锁定和长期成本问题推动了本地部署的需求。

Method: 通过分析最新开源模型（如Qwen、Llama、Mistral等）的硬件需求、运营成本和性能基准，并与主要云服务提供商的订阅费用进行比较。

Result: 研究提供了基于使用水平和性能需求的盈亏平衡点估计，为组织规划LLM策略提供实用框架。

Conclusion: 该分析框架帮助组织在经济层面做出更明智的LLM部署决策，平衡成本效益与运营需求。

Abstract: Large language models (LLMs) are becoming increasingly widespread.
Organizations that want to use AI for productivity now face an important
decision. They can subscribe to commercial LLM services or deploy models on
their own infrastructure. Cloud services from providers such as OpenAI,
Anthropic, and Google are attractive because they provide easy access to
state-of-the-art models and are easy to scale. However, concerns about data
privacy, the difficulty of switching service providers, and long-term operating
costs have driven interest in local deployment of open-source models. This
paper presents a cost-benefit analysis framework to help organizations
determine when on-premise LLM deployment becomes economically viable compared
to commercial subscription services. We consider the hardware requirements,
operational expenses, and performance benchmarks of the latest open-source
models, including Qwen, Llama, Mistral, and etc. Then we compare the total cost
of deploying these models locally with the major cloud providers subscription
fee. Our findings provide an estimated breakeven point based on usage levels
and performance needs. These results give organizations a practical framework
for planning their LLM strategies.

</details>


### [26] [SPADE: A Large Language Model Framework for Soil Moisture Pattern Recognition and Anomaly Detection in Precision Agriculture](https://arxiv.org/abs/2509.18123)
*Yeonju Lee,Rui Qi Chen,Joseph Oboamah,Po Nien Su,Wei-zhen Liang,Yeyin Shi,Lu Gan,Yongsheng Chen,Xin Qiao,Jing Li*

Main category: cs.AI

TL;DR: SPADE是一个利用大型语言模型进行土壤湿度时间序列分析的集成框架，能够检测灌溉模式和异常，无需特定任务标注或微调。


<details>
  <summary>Details</summary>
Motivation: 现有土壤湿度时间序列分析方法依赖基于阈值的规则或数据密集型机器学习模型，存在适应性和可解释性限制。

Method: SPADE利用ChatGPT-4.1，将时间序列数据转换为文本表示，通过领域知识提示模板进行零样本分析，检测灌溉事件、估计净灌溉增益、分类异常。

Result: 在真实农田数据上的实验显示，SPADE在异常检测方面优于现有方法，具有更高的召回率和F1分数，并能准确分类异常类型。

Conclusion: LLMs可作为精准农业的可扩展、适应性工具，整合定性知识和数据驱动推理，为土壤湿度监测和灌溉调度提供可操作见解。

Abstract: Accurate interpretation of soil moisture patterns is critical for irrigation
scheduling and crop management, yet existing approaches for soil moisture
time-series analysis either rely on threshold-based rules or data-hungry
machine learning or deep learning models that are limited in adaptability and
interpretability. In this study, we introduce SPADE (Soil moisture Pattern and
Anomaly DEtection), an integrated framework that leverages large language
models (LLMs) to jointly detect irrigation patterns and anomalies in soil
moisture time-series data. SPADE utilizes ChatGPT-4.1 for its advanced
reasoning and instruction-following capabilities, enabling zero-shot analysis
without requiring task-specific annotation or fine-tuning. By converting
time-series data into a textual representation and designing domain-informed
prompt templates, SPADE identifies irrigation events, estimates net irrigation
gains, detects, classifies anomalies, and produces structured, interpretable
reports. Experiments were conducted on real-world soil moisture sensor data
from commercial and experimental farms cultivating multiple crops across the
United States. Results demonstrate that SPADE outperforms the existing method
in anomaly detection, achieving higher recall and F1 scores and accurately
classifying anomaly types. Furthermore, SPADE achieved high precision and
recall in detecting irrigation events, indicating its strong capability to
capture irrigation patterns accurately. SPADE's reports provide
interpretability and usability of soil moisture analytics. This study
highlights the potential of LLMs as scalable, adaptable tools for precision
agriculture, which is capable of integrating qualitative knowledge and
data-driven reasoning to produce actionable insights for accurate soil moisture
monitoring and improved irrigation scheduling from soil moisture time-series
data.

</details>


### [27] [Position Paper: Integrating Explainability and Uncertainty Estimation in Medical AI](https://arxiv.org/abs/2509.18132)
*Xiuyi Fan*

Main category: cs.AI

TL;DR: 本文提出可解释不确定性估计（XUE）框架，将可解释性与不确定性量化相结合，以解决医疗AI中不确定性沟通不足的问题。


<details>
  <summary>Details</summary>
Motivation: 当前医疗AI系统未能以符合临床推理的方式明确量化或传达不确定性，现有可解释AI（XAI）方法缺乏对预测置信度的捕捉，而不确定性估计（UE）技术又缺乏直观解释，这种脱节限制了AI在医疗领域的应用。

Method: 提出XUE框架，系统地将医疗不确定性映射到AI不确定性概念，识别XUE实施的关键挑战，并规划技术方向包括多模态不确定性量化、模型无关的可视化技术和不确定性感知决策支持系统。

Result: 分析强调了需要开发不仅能生成可靠预测，还能以临床有意义的方式表达置信水平的AI系统。

Conclusion: 这项工作通过桥接可解释性和不确定性，为开发可信赖的医疗AI做出贡献，为符合现实世界临床复杂性的AI系统铺平道路。

Abstract: Uncertainty is a fundamental challenge in medical practice, but current
medical AI systems fail to explicitly quantify or communicate uncertainty in a
way that aligns with clinical reasoning. Existing XAI works focus on
interpreting model predictions but do not capture the confidence or reliability
of these predictions. Conversely, uncertainty estimation (UE) techniques
provide confidence measures but lack intuitive explanations. The disconnect
between these two areas limits AI adoption in medicine. To address this gap, we
propose Explainable Uncertainty Estimation (XUE) that integrates explainability
with uncertainty quantification to enhance trust and usability in medical AI.
We systematically map medical uncertainty to AI uncertainty concepts and
identify key challenges in implementing XUE. We outline technical directions
for advancing XUE, including multimodal uncertainty quantification,
model-agnostic visualization techniques, and uncertainty-aware decision support
systems. Lastly, we propose guiding principles to ensure effective XUE
realisation. Our analysis highlights the need for AI systems that not only
generate reliable predictions but also articulate confidence levels in a
clinically meaningful way. This work contributes to the development of
trustworthy medical AI by bridging explainability and uncertainty, paving the
way for AI systems that are aligned with real-world clinical complexities.

</details>


### [28] [HSGM: Hierarchical Segment-Graph Memory for Scalable Long-Text Semantics](https://arxiv.org/abs/2509.18168)
*Dong Liu,Yanxuan Yu*

Main category: cs.AI

TL;DR: HSGM是一种分层分段图内存框架，通过将长文档分解为有意义的分段，构建局部语义图并提取摘要节点来形成全局图内存，显著降低了长文档语义解析的计算复杂度和内存需求。


<details>
  <summary>Details</summary>
Motivation: 长文档语义解析面临二次复杂度增长和内存需求过高的挑战，需要一种能够高效处理超长文本的语义建模方法。

Method: HSGM框架将输入文档分解为M个分段，在每个分段上构建局部语义图，提取摘要节点形成全局图内存，支持增量更新和分层查询处理。

Result: 在三个基准测试中，HSGM实现了2-4倍的推理加速，峰值内存减少超过60%，同时保持基线准确率的95%以上。

Conclusion: HSGM为超长文本的可扩展、准确语义建模提供了有效解决方案，支持实时和资源受限的NLP应用。

Abstract: Semantic parsing of long documents remains challenging due to quadratic
growth in pairwise composition and memory requirements. We introduce
\textbf{Hierarchical Segment-Graph Memory (HSGM)}, a novel framework that
decomposes an input of length $N$ into $M$ meaningful segments, constructs
\emph{Local Semantic Graphs} on each segment, and extracts compact
\emph{summary nodes} to form a \emph{Global Graph Memory}. HSGM supports
\emph{incremental updates} -- only newly arrived segments incur local graph
construction and summary-node integration -- while \emph{Hierarchical Query
Processing} locates relevant segments via top-$K$ retrieval over summary nodes
and then performs fine-grained reasoning within their local graphs.
  Theoretically, HSGM reduces worst-case complexity from $O(N^2)$ to
$O\!\left(N\,k + (N/k)^2\right)$, with segment size $k \ll N$, and we derive
Frobenius-norm bounds on the approximation error introduced by node
summarization and sparsification thresholds. Empirically, on three benchmarks
-- long-document AMR parsing, segment-level semantic role labeling (OntoNotes),
and legal event extraction -- HSGM achieves \emph{2--4$\times$ inference
speedup}, \emph{$>60\%$ reduction} in peak memory, and \emph{$\ge 95\%$} of
baseline accuracy. Our approach unlocks scalable, accurate semantic modeling
for ultra-long texts, enabling real-time and resource-constrained NLP
applications.

</details>


### [29] [Foam-Agent: An End-to-End Composable Multi-Agent Framework for Automating CFD Simulation in OpenFOAM](https://arxiv.org/abs/2509.18178)
*Ling Yue,Nithin Somasekharan,Tingwen Zhang,Yadi Cao,Shaowu Pan*

Main category: cs.AI

TL;DR: Foam-Agent是一个多智能体框架，通过自然语言提示自动化整个OpenFOAM CFD仿真流程，包括网格生成、HPC脚本创建和后处理可视化，显著降低了CFD的使用门槛。


<details>
  <summary>Details</summary>
Motivation: CFD仿真的学习曲线陡峭且设置复杂，现有系统无法实现端到端的自动化工作流，需要专业知识和大量手动操作。

Method: 采用多智能体架构，使用Model Context Protocol暴露核心功能为可调用工具，通过分层多索引RAG实现高精度配置生成，包含网格生成、HPC脚本自动生成和可视化后处理等模块。

Result: 在110个仿真任务基准测试中，使用Claude 3.5 Sonnet达到88.2%的成功率，显著优于现有框架（MetaOpenFOAM为55.5%）。

Conclusion: Foam-Agent有效降低了CFD的专业门槛，展示了专用多智能体系统如何民主化复杂科学计算，代码已开源。

Abstract: Computational Fluid Dynamics (CFD) is an essential simulation tool in
engineering, yet its steep learning curve and complex manual setup create
significant barriers. To address these challenges, we introduce Foam-Agent, a
multi-agent framework that automates the entire end-to-end OpenFOAM workflow
from a single natural language prompt. Our key innovations address critical
gaps in existing systems: 1. An Comprehensive End-to-End Simulation Automation:
Foam-Agent is the first system to manage the full simulation pipeline,
including advanced pre-processing with a versatile Meshing Agent capable of
handling external mesh files and generating new geometries via Gmsh, automatic
generation of HPC submission scripts, and post-simulation visualization via
ParaView. 2. Composable Service Architecture: Going beyond a monolithic agent,
the framework uses Model Context Protocol (MCP) to expose its core functions as
discrete, callable tools. This allows for flexible integration and use by other
agentic systems, such as Claude-code, for more exploratory workflows. 3.
High-Fidelity Configuration Generation: We achieve superior accuracy through a
Hierarchical Multi-Index RAG for precise context retrieval and a
dependency-aware generation process that ensures configuration consistency.
Evaluated on a benchmark of 110 simulation tasks, Foam-Agent achieves an 88.2%
success rate with Claude 3.5 Sonnet, significantly outperforming existing
frameworks (55.5% for MetaOpenFOAM). Foam-Agent dramatically lowers the
expertise barrier for CFD, demonstrating how specialized multi-agent systems
can democratize complex scientific computing. The code is public at
https://github.com/csml-rpi/Foam-Agent.

</details>


### [30] [Large Language Models and Operations Research: A Structured Survey](https://arxiv.org/abs/2509.18180)
*Yang Wang,Kai Li*

Main category: cs.AI

TL;DR: 本文综述了大型语言模型在运筹学中的应用进展，主要涵盖自动建模、辅助优化和直接求解三个方向，并讨论了当前挑战和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 传统运筹学方法依赖专家建模和手动参数调整，难以处理大规模、动态和多约束问题。LLMs通过语义理解、结构化生成和推理控制显示出解决这些局限的潜力。

Method: 将LLMs与OR集成的方法分为三类：自动建模（将自然语言描述转换为数学模型或可执行代码）、辅助优化（生成启发式方法和演化算法）、直接求解优化任务。

Result: LLMs能够有效提升OR问题的处理效率，特别是在复杂系统决策支持方面。但存在语义到结构映射不稳定、研究进展碎片化、泛化能力有限等挑战。

Conclusion: LLMs在OR领域具有重要应用前景，需要进一步研究解决现有挑战，推动该领域的发展。

Abstract: Operations research (OR) provides fundamental methodologies for complex
system decision-making, with established applications in transportation, supply
chain management, and production scheduling. Traditional approaches, which
depend on expert-based modeling and manual parameter adjustment, often face
challenges in handling large-scale, dynamic, and multi-constraint problems.
Recently, large language models (LLMs) have shown potential to address these
limitations through semantic understanding, structured generation, and
reasoning control. LLMs can translate natural language descriptions into
mathematical models or executable code, generate heuristics, evolve algorithms,
and directly tackle optimization tasks. This paper surveys recent progress on
the integration of LLMs into OR, organizing methods into three main directions:
automatic modeling, auxiliary optimization, and direct solving. It further
reviews evaluation benchmarks and domain-specific applications, and summarizes
key open issues such as unstable semantic-to-structure mapping, fragmented
research progress, limited generalization, and insufficient evaluation systems.
Finally, the survey outlines possible research avenues for advancing the role
of LLMs in OR.

</details>


### [31] [Synthesizing Attitudes, Predicting Actions (SAPA): Behavioral Theory-Guided LLMs for Ridesourcing Mode Choice Modeling](https://arxiv.org/abs/2509.18181)
*Mustafa Sameen,Xiaojian Zhang,Xilei Zhao*

Main category: cs.AI

TL;DR: 本文提出了SAPA框架，利用大语言模型合成理论驱动的潜在态度来预测网约车选择模式，解决了现有模型因忽略心理因素和类别不平衡导致的预测精度不足问题。


<details>
  <summary>Details</summary>
Motivation: 现有网约车选择预测模型存在两个主要局限：一是无法捕捉关键心理因素导致预测精度有限；二是面临严重的类别不平衡问题（网约车出行仅占日常出行的很小比例）。

Method: SAPA采用分层方法：1）使用LLM从原始出行调查数据生成定性旅行者画像；2）基于人口统计和行为特征训练倾向得分模型；3）LLM为理论驱动的潜在变量分配定量分数；4）最终分类器整合倾向得分、潜在变量分数和可观察出行属性进行预测。

Result: 在大规模多年出行调查上的实验表明，SAPA显著优于最先进的基线方法，在测试集上的PR-AUC指标上，网约车选择预测性能提升了高达75.9%。

Conclusion: SAPA为准确预测网约车选择模式提供了强大工具，其方法论可轻松迁移到各种应用中，为交通管理政策设计提供了更有效的支持。

Abstract: Accurate modeling of ridesourcing mode choices is essential for designing and
implementing effective traffic management policies for reducing congestion,
improving mobility, and allocating resources more efficiently. Existing models
for predicting ridesourcing mode choices often suffer from limited predictive
accuracy due to their inability to capture key psychological factors, and are
further challenged by severe class imbalance, as ridesourcing trips comprise
only a small fraction of individuals' daily travel. To address these
limitations, this paper introduces the Synthesizing Attitudes, Predicting
Actions (SAPA) framework, a hierarchical approach that uses Large Language
Models (LLMs) to synthesize theory-grounded latent attitudes to predict
ridesourcing choices. SAPA first uses an LLM to generate qualitative traveler
personas from raw travel survey data and then trains a propensity-score model
on demographic and behavioral features, enriched by those personas, to produce
an individual-level score. Next, the LLM assigns quantitative scores to
theory-driven latent variables (e.g., time and cost sensitivity), and a final
classifier integrates the propensity score, latent-variable scores (with their
interaction terms), and observable trip attributes to predict ridesourcing mode
choice. Experiments on a large-scale, multi-year travel survey show that SAPA
significantly outperforms state-of-the-art baselines, improving ridesourcing
choice predictions by up to 75.9% in terms of PR-AUC on a held-out test set.
This study provides a powerful tool for accurately predicting ridesourcing mode
choices, and provides a methodology that is readily transferable to various
applications.

</details>


### [32] [An Outcome-Based Educational Recommender System](https://arxiv.org/abs/2509.18186)
*Nursultan Askarbekuly,Timur Fayzrakhmanov,Sladjan Babarogić,Ivan Luković*

Main category: cs.AI

TL;DR: OBER是一个基于学习成果的教育推荐系统，通过将学习成果和评估项嵌入数据模式，使任何推荐算法都能根据其促进学生掌握程度的效果进行评估。


<details>
  <summary>Details</summary>
Motivation: 大多数教育推荐系统仅基于点击率或评分进行优化和评估，无法衡量其真实的教学影响。需要一种能够直接评估推荐系统对学习成果影响的方法。

Method: OBER采用简约的实体关系模型、基于日志的掌握度计算公式和插件架构。在非正式学习领域的电子学习系统中进行了为期两周的随机分组测试，比较了固定专家路径、协同过滤和基于知识的过滤三种方法。

Result: 协同过滤方法最大化了用户留存率，但固定路径方法实现了最高的学习掌握度。OBER框架可以从相同的日志数据中同时获取业务指标、相关性指标和学习指标。

Conclusion: OBER框架是方法无关的，可以轻松扩展到未来的自适应或上下文感知推荐系统，让从业者能够在没有额外测试负担的情况下权衡相关性和参与度与学习成果掌握度。

Abstract: Most educational recommender systems are tuned and judged on click- or
rating-based relevance, leaving their true pedagogical impact unclear. We
introduce OBER-an Outcome-Based Educational Recommender that embeds learning
outcomes and assessment items directly into the data schema, so any algorithm
can be evaluated on the mastery it fosters. OBER uses a minimalist
entity-relation model, a log-driven mastery formula, and a plug-in
architecture. Integrated into an e-learning system in non-formal domain, it was
evaluated trough a two-week randomized split test with over 5 700 learners
across three methods: fixed expert trajectory, collaborative filtering (CF),
and knowledge-based (KB) filtering. CF maximized retention, but the fixed path
achieved the highest mastery. Because OBER derives business, relevance, and
learning metrics from the same logs, it lets practitioners weigh relevance and
engagement against outcome mastery with no extra testing overhead. The
framework is method-agnostic and readily extensible to future adaptive or
context-aware recommenders.

</details>


### [33] [MMCD: Multi-Modal Collaborative Decision-Making for Connected Autonomy with Knowledge Distillation](https://arxiv.org/abs/2509.18198)
*Rui Liu,Zikang Wang,Peng Gao,Yu Shen,Pratap Tokekar,Ming Lin*

Main category: cs.AI

TL;DR: 提出MMCD框架，通过多模态协作决策和跨模态知识蒸馏解决自动驾驶中传感器故障或连接车辆缺失的问题，提升驾驶安全性。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统在事故易发环境中面临挑战，单个车辆传感器范围有限且视野受阻。现有方法假设训练和测试时所有数据模态和连接车辆都可用，这不切实际。

Method: MMCD框架融合自车和协作车辆的多模态观测数据，采用基于教师-学生模型的跨模态知识蒸馏方法，教师模型使用多模态数据训练，学生模型能在模态减少时有效运行。

Result: 在连接自动驾驶和空地车辆协作实验中，该方法将驾驶安全性提升高达20.7%，在潜在事故检测和安全驾驶决策方面超越现有最佳基线。

Conclusion: MMCD框架通过多模态协作和知识蒸馏技术，有效解决了自动驾驶中数据模态缺失的问题，显著提高了系统的鲁棒性和安全性。

Abstract: Autonomous systems have advanced significantly, but challenges persist in
accident-prone environments where robust decision-making is crucial. A single
vehicle's limited sensor range and obstructed views increase the likelihood of
accidents. Multi-vehicle connected systems and multi-modal approaches,
leveraging RGB images and LiDAR point clouds, have emerged as promising
solutions. However, existing methods often assume the availability of all data
modalities and connected vehicles during both training and testing, which is
impractical due to potential sensor failures or missing connected vehicles. To
address these challenges, we introduce a novel framework MMCD (Multi-Modal
Collaborative Decision-making) for connected autonomy. Our framework fuses
multi-modal observations from ego and collaborative vehicles to enhance
decision-making under challenging conditions. To ensure robust performance when
certain data modalities are unavailable during testing, we propose an approach
based on cross-modal knowledge distillation with a teacher-student model
structure. The teacher model is trained with multiple data modalities, while
the student model is designed to operate effectively with reduced modalities.
In experiments on $\textit{connected autonomous driving with ground vehicles}$
and $\textit{aerial-ground vehicles collaboration}$, our method improves
driving safety by up to ${\it 20.7}\%$, surpassing the best-existing baseline
in detecting potential accidents and making safe driving decisions. More
information can be found on our website https://ruiiu.github.io/mmcd.

</details>


### [34] [Change in Quantitative Bipolar Argumentation: Sufficient, Necessary, and Counterfactual Explanations](https://arxiv.org/abs/2509.18215)
*Timotheus Kampik,Kristijonas Čyras,José Ruiz Alarcón*

Main category: cs.AI

TL;DR: 本文提出了一种形式化方法来解释定量双极论证框架(QBAF)中推理变化的原因。当从QBAF得出结论并更新QBAF后再次得出结论时，该方法追踪语义在感兴趣参数(主题参数)上建立的强度偏序关系的变化(称为强度不一致性)。


<details>
  <summary>Details</summary>
Motivation: 在动态论证系统中，当更新QBAF时，参数强度的排序关系可能发生变化，需要一种系统的方法来追踪和解释这些变化的原因。

Method: 将强度不一致性的原因追溯到特定参数，识别充分、必要和反事实解释。定义基于启发式的方法来促进强度不一致性解释的搜索，并提供实现。

Result: 证明了强度不一致性解释存在的充要条件是更新导致强度不一致性。

Conclusion: 该方法为QBAF中的推理变化提供了形式化的解释框架，有助于理解动态论证系统中的推理演变。

Abstract: This paper presents a formal approach to explaining change of inference in
Quantitative Bipolar Argumentation Frameworks (QBAFs). When drawing conclusions
from a QBAF and updating the QBAF to then again draw conclusions (and so on),
our approach traces changes -- which we call strength inconsistencies -- in the
partial order over argument strengths that a semantics establishes on some
arguments of interest, called topic arguments. We trace the causes of strength
inconsistencies to specific arguments, which then serve as explanations. We
identify sufficient, necessary, and counterfactual explanations for strength
inconsistencies and show that strength inconsistency explanations exist if and
only if an update leads to strength inconsistency. We define a heuristic-based
approach to facilitate the search for strength inconsistency explanations, for
which we also provide an implementation.

</details>


### [35] [nDNA -- the Semantic Helix of Artificial Cognition](https://arxiv.org/abs/2509.18216)
*Amitava Das*

Main category: cs.AI

TL;DR: 提出了Neural DNA（nDNA）作为AI基础模型的语义基因型表示，通过潜在几何结构捕捉模型的内在认知身份。nDNA基于三个几何维度：谱曲率、热力学长度和信念向量场，用于追踪模型的谱系、突变和语义继承。


<details>
  <summary>Details</summary>
Motivation: 当前基准测试仅衡量模型行为，但模型的本质在于其潜在几何结构。需要一种能够捕捉模型内在认知身份的方法，以理解AI模型的谱系、文化印记和架构漂移。

Method: 从三个几何维度合成nDNA：谱曲率（揭示跨层的概念流曲率）、热力学长度（量化表示转换的语义努力）和信念向量场（描述引导模型信念方向的语义扭转场）。

Result: nDNA提供了一个稳定、坐标无关的神经DNA指纹，可用于追踪预训练、微调、对齐、剪枝、蒸馏和模型合并等过程中的谱系，测量检查点之间的继承关系，检测新数据或目标下的特征漂移。

Conclusion: 这项工作开启了神经基因组学新领域，将AI模型视为具有可追踪内在认知的数字语义有机体，为比较模型、诊断风险和治理认知演化提供了新方法。

Abstract: As AI foundation models grow in capability, a deeper question emerges: What
shapes their internal cognitive identity -- beyond fluency and output?
Benchmarks measure behavior, but the soul of a model resides in its latent
geometry. In this work, we propose Neural DNA (nDNA) as a semantic-genotypic
representation that captures this latent identity through the intrinsic
geometry of belief. At its core, nDNA is synthesized from three principled and
indispensable dimensions of latent geometry: spectral curvature, which reveals
the curvature of conceptual flow across layers; thermodynamic length, which
quantifies the semantic effort required to traverse representational
transitions through layers; and belief vector field, which delineates the
semantic torsion fields that guide a model's belief directional orientations.
Like biological DNA, it encodes ancestry, mutation, and semantic inheritance,
found in finetuning and alignment scars, cultural imprints, and architectural
drift. In naming it, we open a new field: Neural Genomics, where models are not
just tools, but digital semantic organisms with traceable inner cognition.
  Modeling statement. We read AI foundation models as semantic fluid--dynamics:
meaning is transported through layers like fluid in a shaped conduit; nDNA is
the physics-grade readout of that flow -- a geometry-first measure of how
meaning is bent, paid for, and pushed -- yielding a stable, coordinate-free
neural DNA fingerprint tied to on-input behavior; with this fingerprint we
cross into biology: tracing lineages across pretraining, fine-tuning,
alignment, pruning, distillation, and merges; measuring inheritance between
checkpoints; detecting drift as traits shift under new data or objectives; and,
ultimately, studying the evolution of artificial cognition to compare models,
diagnose risks, and govern change over time.

</details>


### [36] [Similarity Field Theory: A Mathematical Framework for Intelligence](https://arxiv.org/abs/2509.18218)
*Kei-Sing Ng*

Main category: cs.AI

TL;DR: 本文提出了相似性场理论，这是一个数学框架，用于形式化实体间相似性关系及其演化的原则。该理论定义了相似性场、系统演化、概念纤维和生成算子，并基于此形式化地定义了智能的概念。


<details>
  <summary>Details</summary>
Motivation: 作者认为持久化和转换相似性关系是任何可理解动态系统的结构基础，需要建立一个数学框架来形式化这些原则，为表征、比较和构建智能系统提供基础语言。

Method: 定义了相似性场S: U×U→[0,1]，满足自反性但允许不对称性和非传递性；系统演化序列Zp=(Xp,S(p))；概念K诱导纤维Fα(K)；生成算子G。通过定理证明相似性场演化的约束条件。

Result: 证明了两个定理：(i)不对称性阻止相互包含；(ii)稳定性需要锚坐标或最终限制在f的水平集内。这些结果确保相似性场的演化既受约束又可解释。

Conclusion: 相似性场理论为智能系统提供了基础框架，能够解释大语言模型并将其作为社会认知的实验探针。

Abstract: We posit that persisting and transforming similarity relations form the
structural basis of any comprehensible dynamic system. This paper introduces
Similarity Field Theory, a mathematical framework that formalizes the
principles governing similarity values among entities and their evolution. We
define: (1) a similarity field $S: U \times U \to [0,1]$ over a universe of
entities $U$, satisfying reflexivity $S(E,E)=1$ and treated as a directed
relational field (asymmetry and non-transitivity are allowed); (2) the
evolution of a system through a sequence $Z_p = (X_p, S^{(p)})$ indexed by
$p=0,1,2,\ldots$; (3) concepts $K$ as entities that induce fibers
$F_{\alpha}(K) = { E \in U \mid S(E,K) \ge \alpha }$, i.e., superlevel sets of
the unary map $S_K(E) := S(E,K)$; and (4) a generative operator $G$ that
produces new entities. Within this framework, we formalize a generative
definition of intelligence: an operator $G$ is intelligent with respect to a
concept $K$ if, given a system containing entities belonging to the fiber of
$K$, it generates new entities that also belong to that fiber. Similarity Field
Theory thus offers a foundational language for characterizing, comparing, and
constructing intelligent systems. We prove two theorems: (i) asymmetry blocks
mutual inclusion; and (ii) stability requires either an anchor coordinate or
eventual confinement within a level set of $f$. These results ensure that the
evolution of similarity fields is both constrained and interpretable,
culminating in an exploration of how the framework allows us to interpret large
language models and use them as experimental probes into societal cognition.

</details>


### [37] [Multimodal Health Risk Prediction System for Chronic Diseases via Vision-Language Fusion and Large Language Models](https://arxiv.org/abs/2509.18221)
*Dingxin Lu,Shurui Wu,Xinyi Huang*

Main category: cs.AI

TL;DR: VL-RiskFormer是一个用于预测个体健康风险的多模态AI框架，结合视觉和语言数据，在MIMIC-IV数据集上取得了0.90的AUROC。


<details>
  <summary>Details</summary>
Motivation: 随着慢性疾病负担增加和多模态临床数据的涌现，需要统一的AI框架来主动预测个体健康风险。

Method: 采用层次化堆叠的视觉-语言多模态Transformer架构，包含预训练、时间融合块和疾病本体图适配器三个关键创新。

Result: 在MIMIC-IV纵向队列中，平均AUROC达到0.90，预期校准误差为2.7%。

Conclusion: VL-RiskFormer展示了在多模态临床数据上进行健康风险预测的有效性。

Abstract: With the rising global burden of chronic diseases and the multimodal and
heterogeneous clinical data (medical imaging, free-text recordings, wearable
sensor streams, etc.), there is an urgent need for a unified multimodal AI
framework that can proactively predict individual health risks. We propose
VL-RiskFormer, a hierarchical stacked visual-language multimodal Transformer
with a large language model (LLM) inference head embedded in its top layer. The
system builds on the dual-stream architecture of existing visual-linguistic
models (e.g., PaLM-E, LLaVA) with four key innovations: (i) pre-training with
cross-modal comparison and fine-grained alignment of radiological images,
fundus maps, and wearable device photos with corresponding clinical narratives
using momentum update encoders and debiased InfoNCE losses; (ii) a time fusion
block that integrates irregular visit sequences into the causal Transformer
decoder through adaptive time interval position coding; (iii) a disease
ontology map adapter that injects ICD-10 codes into visual and textual channels
in layers and infers comorbid patterns with the help of a graph attention
mechanism. On the MIMIC-IV longitudinal cohort, VL-RiskFormer achieved an
average AUROC of 0.90 with an expected calibration error of 2.7 percent.

</details>


### [38] [From "What to Eat?" to Perfect Recipe: ChefMind's Chain-of-Exploration for Ambiguous User Intent in Recipe Recommendation](https://arxiv.org/abs/2509.18226)
*Yu Fu,Linyue Cai,Ruoyu Wu,Yong Zhao*

Main category: cs.AI

TL;DR: ChefMind是一个混合架构，结合了探索链、知识图谱、检索增强生成和大语言模型，用于解决个性化食谱推荐中的模糊用户意图、语义准确性和细节覆盖问题。


<details>
  <summary>Details</summary>
Motivation: 个性化食谱推荐面临处理模糊用户意图、确保语义准确性和提供足够细节覆盖的挑战。

Method: 提出ChefMind混合架构：探索链(CoE)将模糊查询细化为结构化条件，知识图谱(KG)提供语义推理和可解释性，检索增强生成(RAG)补充上下文烹饪细节，大语言模型(LLM)将输出整合为连贯推荐。

Result: 在Xiachufang数据集和手动标注查询上的评估显示，ChefMind在准确性、相关性、完整性和清晰度方面表现优异，平均得分8.7，而消融模型为6.4-6.7。未处理查询降至1.6%，证明其在处理模糊需求方面的鲁棒性。

Conclusion: ChefMind通过混合架构有效解决了食谱推荐中的关键挑战，在多个指标上显著优于单一方法基线。

Abstract: Personalized recipe recommendation faces challenges in handling fuzzy user
intent, ensuring semantic accuracy, and providing sufficient detail coverage.
We propose ChefMind, a hybrid architecture combining Chain of Exploration
(CoE), Knowledge Graph (KG), Retrieval-Augmented Generation (RAG), and a Large
Language Model (LLM). CoE refines ambiguous queries into structured conditions,
KG offers semantic reasoning and interpretability, RAG supplements contextual
culinary details, and LLM integrates outputs into coherent recommendations. We
evaluate ChefMind on the Xiachufang dataset and manually annotated queries,
comparing it with LLM-only, KG-only, and RAG-only baselines. Results show that
ChefMind achieves superior performance in accuracy, relevance, completeness,
and clarity, with an average score of 8.7 versus 6.4-6.7 for ablation models.
Moreover, it reduces unprocessed queries to 1.6%, demonstrating robustness in
handling fuzzy demands.

</details>


### [39] [An N-Plus-1 GPT Agency for Critical Solution of Mechanical Engineering Analysis Problems](https://arxiv.org/abs/2509.18229)
*Anthony Patera,Rohan Abeyaratne*

Main category: cs.AI

TL;DR: 本文提出了一种"N-Plus-1"GPT代理架构，用于机械工程问题的初步分析，通过多个独立求解代理和比较代理来提高GPT在工程问题求解中的可靠性。


<details>
  <summary>Details</summary>
Motivation: GPT在机械工程分析中表现出不稳定性，成功率仅为85%，这种不可靠性使其无法直接应用于教育或工程实践。

Method: 采用N+1代理架构：首先启动N个独立求解代理生成N个问题解决方案，然后通过比较代理对这些方案进行总结、比较并推荐最优解。基于孔多塞陪审团定理，当每个求解代理的成功概率大于1/2时，多数投票结果大概率正确。

Result: 与商业多代理模型Grok Heavy相比，该架构在设计和性能上具有相似性，但更注重透明度和教学价值。

Conclusion: N-Plus-1代理架构能够有效提高GPT在机械工程问题求解中的可靠性，特别适合教育场景，通过多代理协作和透明比较过程增强解决方案的可信度。

Abstract: Generative AI, and specifically GPT, can produce a remarkable solution to a
mechanical engineering analysis problem - but also, on occasion, a flawed
solution. For example, an elementary mechanics problem is solved flawlessly in
one GPT instance and incorrectly in a subsequent GPT instance, with a success
probability of only 85%. This unreliability renders "out-of-the-box" GPT
unsuitable for deployment in education or engineering practice. We introduce an
"N-Plus-1" GPT Agency for Initial (Low-Cost) Analysis of mechanical engineering
Problem Statements. Agency first launches N instantiations of Agent Solve to
yield N independent Proposed Problem Solution Realizations; Agency then invokes
Agent Compare to summarize and compare the N Proposed Problem Solution
Realizations and to provide a Recommended Problem Solution. We argue from
Condorcet's Jury Theorem that, for a Problem Statement characterized by
per-Solve success probability greater than 1/2 (and N sufficiently large), the
Predominant (Agent Compare) Proposed Problem Solution will, with high
probability, correspond to a Correct Proposed Problem Solution. Furthermore,
Agent Compare can also incorporate aspects of Secondary (Agent Compare)
Proposed Problem Solutions, in particular when the latter represent alternative
Problem Statement interpretations - different Mathematical Models - or
alternative Mathematical Solution Procedures. Comparisons to Grok Heavy, a
commercial multi-agent model, show similarities in design and performance, but
also important differences in emphasis: our Agency focuses on transparency and
pedagogical value.

</details>


### [40] [Towards General Computer Control with Hierarchical Agents and Multi-Level Action Spaces](https://arxiv.org/abs/2509.18230)
*Zihan Dong,Xinyu Fan,Zixiang Tang,Yunqing Li*

Main category: cs.AI

TL;DR: ComputerAgent是一个轻量级分层强化学习框架，通过两层级选项过程（管理器和子策略）控制桌面应用，使用三重模态状态编码器处理视觉和上下文多样性，集成元动作和提前停止机制减少无效交互，实现设备端推理（1500万参数）。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型在桌面应用控制中存在推理延迟高、长视野稀疏奖励任务样本效率差、设备端部署不可行等问题，需要更实用的解决方案。

Method: 采用分层强化学习框架，包含管理器和子策略两层结构，使用截图、任务ID和数值状态的三重模态编码，集成元动作和提前停止机制，使用紧凑视觉骨干网络和小型策略网络。

Result: 在135个真实世界桌面任务测试中，简单任务（<8步）成功率92.1%，困难任务（≥8步）成功率58.8%，性能匹配或超过2000亿参数MLLM基线，模型大小减少4个数量级，推理时间减半。

Conclusion: 分层强化学习为计算机控制提供了一个实用、可扩展的替代方案，相比单一的大语言模型自动化方法更具优势。

Abstract: Controlling desktop applications via software remains a fundamental yet
under-served problem. Existing multi-modal large language models (MLLMs) ingest
screenshots and task instructions to generate keystrokes and mouse events, but
they suffer from prohibitive inference latency, poor sample efficiency on
long-horizon sparse-reward tasks, and infeasible on-device deployment. We
introduce a lightweight hierarchical reinforcement learning framework,
ComputerAgent, that formulates OS control as a two-level option process
(manager and subpolicy), employs a triple-modal state encoder (screenshot, task
ID, numeric state) to handle visual and contextual diversity, integrates
meta-actions with an early-stop mechanism to reduce wasted interactions, and
uses a compact vision backbone plus small policy networks for on-device
inference (15M parameters). On a suite of 135 real-world desktop tasks,
ComputerAgent attains 92.1% success on simple tasks (<8 steps) and 58.8% on
hard tasks (>=8 steps), matching or exceeding 200B-parameter MLLM baselines on
simple scenarios while reducing model size by over four orders of magnitude and
halving inference time. These results demonstrate that hierarchical RL offers a
practical, scalable alternative to monolithic MLLM-based automation for
computer control.

</details>


### [41] [The Illusion of Readiness: Stress Testing Large Frontier Models on Multimodal Medical Benchmarks](https://arxiv.org/abs/2509.18234)
*Yu Gu,Jingjing Fu,Xiaodong Liu,Jeya Maria Jose Valanarasu,Noel Codella,Reuben Tan,Qianchu Liu,Ying Jin,Sheng Zhang,Jinyu Wang,Rui Wang,Lei Song,Guanghui Qin,Naoto Usuyama,Cliff Wong,Cheng Hao,Hohin Lee,Praneeth Sanapathi,Sarah Hilado,Bian Jiang,Javier Alvarez-Valle,Mu Wei,Jianfeng Gao,Eric Horvitz,Matt Lungren,Hoifung Poon,Paul Vozila*

Main category: cs.AI

TL;DR: 论文通过对六个前沿模型在六个医疗基准测试中的压力测试，发现高分数掩盖了模型的脆弱性和捷径学习问题，医疗基准分数不能直接反映真实世界的准备度


<details>
  <summary>Details</summary>
Motivation: 揭示当前医疗AI基准测试的局限性，证明高分数可能源于应试技巧而非真正的医学理解，提醒业界不应仅依赖排行榜分数评估医疗AI的实际能力

Method: 对六个旗舰模型进行压力测试，包括移除关键输入、改变提示词等方式，结合临床医生指导的评估标准分析基准测试的真实测量内容

Result: 领先系统在关键输入被移除时仍能猜对答案，在微小提示变化下会翻转答案，并生成看似合理但有缺陷的推理，基准测试之间存在巨大差异但被等同对待

Conclusion: 医疗AI要获得医疗领域的信任，必须要求超越排行榜胜利，确保系统的鲁棒性、合理推理能力以及与真实医疗需求的一致性

Abstract: Large frontier models like GPT-5 now achieve top scores on medical
benchmarks. But our stress tests tell a different story. Leading systems often
guess correctly even when key inputs like images are removed, flip answers
under trivial prompt changes, and fabricate convincing yet flawed reasoning.
These aren't glitches; they expose how today's benchmarks reward test-taking
tricks over medical understanding. We evaluate six flagship models across six
widely used benchmarks and find that high leaderboard scores hide brittleness
and shortcut learning. Through clinician-guided rubric evaluation, we show that
benchmarks vary widely in what they truly measure yet are treated
interchangeably, masking failure modes. We caution that medical benchmark
scores do not directly reflect real-world readiness. If we want AI to earn
trust in healthcare, we must demand more than leaderboard wins and must hold
systems accountable for robustness, sound reasoning, and alignment with real
medical demands.

</details>


### [42] [Evaluating the Safety and Skill Reasoning of Large Reasoning Models Under Compute Constraints](https://arxiv.org/abs/2509.18382)
*Adarsha Balaji,Le Chen,Rajeev Thakur,Franck Cappello,Sandeep Madireddy*

Main category: cs.AI

TL;DR: 该论文研究两种计算约束策略（推理长度约束和模型量化）来降低推理模型的计算需求，并分析它们对模型安全性能的影响。


<details>
  <summary>Details</summary>
Motivation: 测试时计算扩展虽然能通过生成长链思维序列提高推理语言模型性能，但计算成本显著增加。需要探索在计算约束下保持模型性能和安全性的方法。

Method: 1）使用基于长度控制策略优化的强化学习方法微调推理模型，满足用户定义的CoT推理长度；2）应用量化技术，在用户定义的计算约束下最大化CoT序列生成。

Result: 研究计算效率与模型安全性之间的权衡关系。

Conclusion: 提出了两种有效的计算约束方法，为在有限计算资源下部署安全高效的推理模型提供了解决方案。

Abstract: Test-time compute scaling has demonstrated the ability to improve the
performance of reasoning language models by generating longer chain-of-thought
(CoT) sequences. However, this increase in performance comes with a significant
increase in computational cost. In this work, we investigate two compute
constraint strategies: (1) reasoning length constraint and (2) model
quantization, as methods to reduce the compute demand of reasoning models and
study their impact on their safety performance. Specifically, we explore two
approaches to apply compute constraints to reasoning models: (1) fine-tuning
reasoning models using a length controlled policy optimization (LCPO) based
reinforcement learning method to satisfy a user-defined CoT reasoning length,
and (2) applying quantization to maximize the generation of CoT sequences
within a user-defined compute constraint. Furthermore, we study the trade-off
between the computational efficiency and the safety of the model.

</details>


### [43] [Gödel Test: Can Large Language Models Solve Easy Conjectures?](https://arxiv.org/abs/2509.18383)
*Moran Feldman,Amin Karbasi*

Main category: cs.AI

TL;DR: 本文提出了Gödel测试，用于评估大型语言模型能否解决数学中简单但未解决的猜想，并在组合优化领域的五个猜想上测试了GPT-5的表现。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型在高级数学领域解决新简单猜想的能力，而不仅仅是已知数学竞赛问题。

Method: 设计了Gödel测试，选择五个组合优化中的未解决猜想，向GPT-5提供相关论文但不告知猜想内容，详细评估其推理过程。

Result: GPT-5在三个较简单问题上给出接近正确的解，在问题2中甚至推翻了原猜想并提供了有效解；在需要跨论文综合的问题4上失败；在更复杂的问题5上提出了正确算法但分析失败。

Conclusion: GPT-5在常规推理方面有显著进步，偶尔展现原创性，但在跨论文综合推理方面仍有明显局限，可能代表了前沿模型通过Gödel测试的早期进展。

Abstract: Recent announcements from frontier AI model labs have highlighted strong
results on high-school and undergraduate math competitions. Yet it remains
unclear whether large language models can solve new, simple conjectures in more
advanced areas of mathematics. We propose the G\"odel Test: evaluating whether
a model can produce correct proofs for very simple, previously unsolved
conjectures. To this end, we study the performance of GPT-5 on five conjectures
in combinatorial optimization. For each problem, we provided one or two source
papers from which the conjecture arose, withheld our own conjecture, and then
assessed the model's reasoning in detail. On the three easier problems, GPT-5
produced nearly correct solutions; for Problem 2 it even derived a different
approximation guarantee that, upon checking, refuted our conjecture while
providing a valid solution. The model failed on Problem 4, which required
combining results from two papers. On Problem 5, a harder case without a
validated conjecture, GPT-5 proposed the same algorithm we had in mind but
failed in the analysis, suggesting the proof is more challenging than expected.
Although our sample is small, the results point to meaningful progress on
routine reasoning, occasional flashes of originality, and clear limitations
when cross-paper synthesis is required. GPT-5 may represent an early step
toward frontier models eventually passing the G\"odel Test.

</details>


### [44] [ATLAS: Benchmarking and Adapting LLMs for Global Trade via Harmonized Tariff Code Classification](https://arxiv.org/abs/2509.18400)
*Pritish Yuvraj,Siva Devarakonda*

Main category: cs.AI

TL;DR: 该论文提出了首个基于美国海关在线搜索系统的HTS代码分类基准，并开发了Atlas模型（基于LLaMA-3.3-70B），在10位数分类上达到40%准确率，比GPT-5和Gemini-2.5分别提升15和27.5个百分点，且成本更低、可自托管保障数据隐私。


<details>
  <summary>Details</summary>
Motivation: HTS代码分类是全球贸易中的关键瓶颈，但机器学习社区对此关注不足。错误分类会导致货物运输中断，主要邮政运营商曾因海关文件不完整而暂停对美送货。

Method: 基于美国海关CROSS系统构建首个HTS代码分类基准数据集，对领先的LLM进行评估，并微调Atlas模型（基于LLaMA-3.3-70B）进行HTS代码分类。

Result: Atlas模型在10位数HTS分类上达到40%准确率，6位数分类达到57.5%准确率，分别比GPT-5和Gemini-2.5提升15和27.5个百分点。成本约为GPT-5的1/5、Gemini-2.5的1/8，且可自托管保障数据隐私。

Conclusion: Atlas模型为HTS分类设立了强基线，但该任务仍极具挑战性（10位数准确率仅40%）。通过发布数据集和模型，旨在将HTS分类定位为新的社区基准任务，并促进检索、推理和对齐方面的未来研究。

Abstract: Accurate classification of products under the Harmonized Tariff Schedule
(HTS) is a critical bottleneck in global trade, yet it has received little
attention from the machine learning community. Misclassification can halt
shipments entirely, with major postal operators suspending deliveries to the
U.S. due to incomplete customs documentation. We introduce the first benchmark
for HTS code classification, derived from the U.S. Customs Rulings Online
Search System (CROSS). Evaluating leading LLMs, we find that our fine-tuned
Atlas model (LLaMA-3.3-70B) achieves 40 percent fully correct 10-digit
classifications and 57.5 percent correct 6-digit classifications, improvements
of 15 points over GPT-5-Thinking and 27.5 points over Gemini-2.5-Pro-Thinking.
Beyond accuracy, Atlas is roughly five times cheaper than GPT-5-Thinking and
eight times cheaper than Gemini-2.5-Pro-Thinking, and can be self-hosted to
guarantee data privacy in high-stakes trade and compliance workflows. While
Atlas sets a strong baseline, the benchmark remains highly challenging, with
only 40 percent 10-digit accuracy. By releasing both dataset and model, we aim
to position HTS classification as a new community benchmark task and invite
future work in retrieval, reasoning, and alignment.

</details>


### [45] [Instruction-Following Evaluation in Function Calling for Large Language Models](https://arxiv.org/abs/2509.18420)
*Nikolai Skripko*

Main category: cs.AI

TL;DR: IFEval-FC是一个新的函数调用基准测试，专门评估大语言模型对参数描述中格式指令的遵循能力，填补了现有基准测试只关注参数正确性而忽略格式要求的空白。


<details>
  <summary>Details</summary>
Motivation: 现有的函数调用基准测试（如BFCL、tau^2-Bench、ACEBench）只评估参数正确性，但不测试对参数描述中格式指令（如双引号、ISO日期格式）的遵循能力，这在实际AI代理系统中是一个重要缺陷。

Method: IFEval-FC受IFEval启发，在JSON schema描述中直接编码可验证的格式要求，包含750个测试用例，每个用例包含一个函数及其输入参数的格式要求，以及对应的用户查询。评估完全基于算法，确保客观性和可复现性。

Result: 实验结果显示，即使是GPT-5和Claude 4.1 Opus等最先进的专有模型，也经常无法遵循基本的格式规则，这揭示了现实世界代理系统的实际局限性。

Conclusion: IFEval-FC填补了函数调用评估的重要空白，揭示了当前大语言模型在精确遵循格式指令方面的不足，为改进AI代理系统的可靠性提供了重要基准。代码和数据已公开。

Abstract: Function calling is a core capability of large language models, essential for
AI agents. Existing benchmarks such as the Berkeley Function Calling
Leaderboard (BFCL), tau^2-Bench (arXiv:2506.07982), and ACEBench
(arXiv:2501.12851) evaluate argument correctness but do not test adherence to
format instructions embedded in parameter descriptions, such as enclosing
values in double quotes or using ISO date formats.
  We introduce IFEval-FC, a benchmark inspired by IFEval (arXiv:2311.07911)
that assesses precise instruction following in function calling. IFEval-FC
encodes verifiable formats directly within JSON schema descriptions, for
example specifying that a value must not contain punctuation. It includes 750
test cases, each consisting of a function with an embedded format for one of
its input parameters and a corresponding user query. Evaluation is fully
algorithmic, ensuring objectivity, reproducibility, and scalability.
  Our results show that even state-of-the-art proprietary models, including
GPT-5 and Claude 4.1 Opus, frequently fail to follow basic formatting rules,
highlighting a practical limitation for real-world agent systems. The complete
codebase and data are publicly available at
https://github.com/Skripkon/IFEval-FC.

</details>


### [46] [Memory-QA: Answering Recall Questions Based on Multimodal Memories](https://arxiv.org/abs/2509.18436)
*Hongda Jiang,Xinyuan Zhang,Siddhant Garg,Rishab Arora,Shiun-Zu Kuo,Jiayang Xu,Christopher Brossman,Yue Liu,Aaron Colak,Ahmed Aly,Anuj Kumar,Xin Luna Dong*

Main category: cs.AI

TL;DR: Memory-QA是一个新颖的视觉记忆问答任务，Pensieve管道通过记忆增强、时空感知检索和多记忆QA微调，在QA准确率上比现有方法提升高达14%


<details>
  <summary>Details</summary>
Motivation: 解决现实世界中基于多模态记忆的视觉内容回忆问答任务，应对任务导向记忆创建、时空信息利用和多记忆融合等独特挑战

Method: 提出Pensieve综合管道，包含记忆特定增强、时空感知多信号检索和多记忆QA微调三个核心组件

Result: 在创建的多模态基准测试中，Pensieve相比最先进方法在QA准确率上提升高达14%

Conclusion: Pensieve管道有效解决了Memory-QA任务的挑战，为基于多模态记忆的视觉问答提供了可行的解决方案

Abstract: We introduce Memory-QA, a novel real-world task that involves answering
recall questions about visual content from previously stored multimodal
memories. This task poses unique challenges, including the creation of
task-oriented memories, the effective utilization of temporal and location
information within memories, and the ability to draw upon multiple memories to
answer a recall question. To address these challenges, we propose a
comprehensive pipeline, Pensieve, integrating memory-specific augmentation,
time- and location-aware multi-signal retrieval, and multi-memory QA
fine-tuning. We created a multimodal benchmark to illustrate various real
challenges in this task, and show the superior performance of Pensieve over
state-of-the-art solutions (up to 14% on QA accuracy).

</details>


### [47] [FERA: Foil Fencing Referee Assistant Using Pose-Based Multi-Label Move Recognition and Rule Reasoning](https://arxiv.org/abs/2509.18527)
*Ziwen Chen,Zhong Wang*

Main category: cs.AI

TL;DR: FERA是一个用于击剑裁判辅助的AI原型系统，通过姿态识别和规则推理来解决击剑裁判中的主观性、人为错误等问题


<details>
  <summary>Details</summary>
Motivation: 击剑运动在裁判方面面临主观判罚、人为错误、偏见以及在训练环境中裁判资源有限等挑战

Method: 系统从视频中提取2D关节位置，进行归一化处理，计算101维运动学特征集，使用Transformer进行多标签动作和剑尖分类，并应用基于规则的推理来确定优先权和得分

Result: 在有限的手动标注数据下，5折交叉验证的平均宏F1得分为0.549，优于TCN、BiLSTM和普通Transformer等多个基线模型

Conclusion: 虽然尚未达到部署标准，但结果表明这是实现击剑自动裁判辅助的有希望路径，并为AI在击剑领域的应用（如教练辅助）开辟了新机会

Abstract: The sport of fencing, like many other sports, faces challenges in refereeing:
subjective calls, human errors, bias, and limited availability in practice
environments. We present FERA (Fencing Referee Assistant), a prototype AI
referee for foil fencing which integrates pose-based multi-label action
recognition and rule-based reasoning. FERA extracts 2D joint positions from
video, normalizes them, computes a 101-dimensional kinematic feature set, and
applies a Transformer for multi-label move and blade classification. To
determine priority and scoring, FERA applies a distilled language model with
encoded right-of-way rules, producing both a decision and an explanation for
each exchange. With limited hand-labeled data, a 5-fold cross-validation
achieves an average macro-F1 score of 0.549, outperforming multiple baselines,
including a Temporal Convolutional Network (TCN), BiLSTM, and a vanilla
Transformer. While not ready for deployment, these results demonstrate a
promising path towards automated referee assistance in foil fencing and new
opportunities for AI applications, such as coaching in the field of fencing.

</details>


### [48] [LLMZ+: Contextual Prompt Whitelist Principles for Agentic LLMs](https://arxiv.org/abs/2509.18557)
*Tom Pawelek,Raj Patel,Charlotte Crowell,Noorbakhsh Amiri,Sudip Mittal,Shahram Rahimi,Andy Perkins*

Main category: cs.AI

TL;DR: LLMZ+提出了一种基于提示白名单的新型防御机制，通过只允许上下文相关的安全消息与智能LLM交互，有效防御越狱攻击，同时保持合法业务通信的流畅性。


<details>
  <summary>Details</summary>
Motivation: 传统基于恶意意图检测的防御机制存在局限性，智能AI由于其特权数据访问和API工具使用能力，成为攻击者的高价值目标。智能LLM的非确定性行为特性带来了重大的运营安全和信息安全风险。

Method: LLMZ+采用提示白名单方法，仅允许符合预定义用例和操作边界的上下文适当且安全的消息与智能LLM进行交互，超越了传统的基于检测的方法。

Result: 实证评估表明，LLMZ+对最常见的越狱提示具有强大的抵御能力，同时不会干扰合法的业务通信。在实验设置中，误报率和漏报率均可降至0。

Conclusion: 该方法简化了安全框架，增强了长期弹性，并减少了维持LLM信息安全所需的资源，为智能AI系统提供了更有效的安全保障。

Abstract: Compared to traditional models, agentic AI represents a highly valuable
target for potential attackers as they possess privileged access to data
sources and API tools, which are traditionally not incorporated into classical
agents. Unlike a typical software application residing in a Demilitarized Zone
(DMZ), agentic LLMs consciously rely on nondeterministic behavior of the AI
(only defining a final goal, leaving the path selection to LLM). This
characteristic introduces substantial security risk to both operational
security and information security. Most common existing defense mechanism rely
on detection of malicious intent and preventing it from reaching the LLM agent,
thus protecting against jailbreak attacks such as prompt injection. In this
paper, we present an alternative approach, LLMZ+, which moves beyond
traditional detection-based approaches by implementing prompt whitelisting.
Through this method, only contextually appropriate and safe messages are
permitted to interact with the agentic LLM. By leveraging the specificity of
context, LLMZ+ guarantees that all exchanges between external users and the LLM
conform to predefined use cases and operational boundaries. Our approach
streamlines the security framework, enhances its long-term resilience, and
reduces the resources required for sustaining LLM information security. Our
empirical evaluation demonstrates that LLMZ+ provides strong resilience against
the most common jailbreak prompts. At the same time, legitimate business
communications are not disrupted, and authorized traffic flows seamlessly
between users and the agentic LLM. We measure the effectiveness of approach
using false positive and false negative rates, both of which can be reduced to
0 in our experimental setting.

</details>


### [49] [Solving Math Word Problems Using Estimation Verification and Equation Generation](https://arxiv.org/abs/2509.18565)
*Mitchell Piehl,Dillon Wilson,Ananya Kalita,Jugal Kalita*

Main category: cs.AI

TL;DR: 该论文提出了一种新颖的方法，通过分解数学问题、使用外部符号方程求解器以及答案验证机制，显著提升了大语言模型在解决数学应用题方面的性能，在多个数据集上达到了新的最先进水平。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽然在各种任务上表现出色，但在解决数学应用题时仍面临挑战，因为这类问题需要复杂的推理和数学能力。现有方法通过改进提示词来帮助LLM解决更复杂的MWPs，但仍有提升空间。

Method: 该方法首先提示LLM从问题分解中创建方程，然后使用外部符号方程求解器生成答案。为确保答案准确性，让LLM第二次解决MWP但目标是估计正确答案而非精确求解，通过比较估计值与生成答案进行验证。如果验证失败，采用迭代修正过程确保最终找到正确答案。

Result: 该方法在先前研究使用的数值和代数MWPs数据集上取得了新的最先进结果，将之前的最佳结果平均提高了近两个百分点。在三角函数MWPs上也获得了令人满意的结果，这是之前未尝试过的任务。

Conclusion: 该研究提出的方法有效提升了LLM解决数学应用题的能力，特别是在复杂推理任务上表现优异。研究还引入了两个新数据集SVAMPClean和Trig300，为进一步测试LLM的推理能力提供了新工具。

Abstract: Large Language Models (LLMs) excel at various tasks, including
problem-solving and question-answering. However, LLMs often find Math Word
Problems (MWPs) challenging because solving them requires a range of reasoning
and mathematical abilities with which LLMs seem to struggle. Recent efforts
have helped LLMs solve more complex MWPs with improved prompts. This study
proposes a novel method that initially prompts an LLM to create equations from
a decomposition of the question, followed by using an external symbolic
equation solver to produce an answer. To ensure the accuracy of the obtained
answer, inspired by an established recommendation of math teachers, the LLM is
instructed to solve the MWP a second time, but this time with the objective of
estimating the correct answer instead of solving it exactly. The estimation is
then compared to the generated answer to verify. If verification fails, an
iterative rectification process is employed to ensure the correct answer is
eventually found. This approach achieves new state-of-the-art results on
datasets used by prior published research on numeric and algebraic MWPs,
improving the previous best results by nearly two percent on average. In
addition, the approach obtains satisfactory results on trigonometric MWPs, a
task not previously attempted to the authors' best knowledge. This study also
introduces two new datasets, SVAMPClean and Trig300, to further advance the
testing of LLMs' reasoning abilities.

</details>


### [50] [Adaptive Learning in Spatial Agent-Based Models for Climate Risk Assessment: A Geospatial Framework with Evolutionary Economic Agents](https://arxiv.org/abs/2509.18633)
*Yara Mohajerani*

Main category: cs.AI

TL;DR: 本文提出了一个结合气候灾害数据和进化学习的地理空间代理模型，用于评估气候风险对经济系统的影响。


<details>
  <summary>Details</summary>
Motivation: 气候风险评估需要模拟空间异质性灾害与适应性经济系统之间的复杂相互作用，现有模型缺乏对经济主体适应性行为的充分考虑。

Method: 开发了一个开源框架，将Mesa空间建模与CLIMADA气候影响评估相结合，引入基于适应度的选择和突变机制，让企业能够进化预算分配、定价、工资和风险适应策略。

Result: 使用RCP8.5情景下的河流洪水预测显示，进化适应使企业能够在经历数十年气候压力后恢复到基线生产水平；未直接暴露于洪水的企业也会通过供应链中断受到影响，到世纪末商品平均价格比基线高5.6%。

Conclusion: 该开源框架为金融机构和公司提供了量化直接和级联气候风险的工具，同时评估成本效益高的适应策略。

Abstract: Climate risk assessment requires modelling complex interactions between
spatially heterogeneous hazards and adaptive economic systems. We present a
novel geospatial agent-based model that integrates climate hazard data with
evolutionary learning for economic agents. Our framework combines Mesa-based
spatial modelling with CLIMADA climate impact assessment, introducing adaptive
learning behaviours that allow firms to evolve strategies for budget
allocation, pricing, wages, and risk adaptation through fitness-based selection
and mutation. We demonstrate the framework using riverine flood projections
under RCP8.5 until 2100, showing that evolutionary adaptation enables firms to
converge with baseline (no hazard) production levels after decades of
disruption due to climate stress. Our results reveal systemic risks where even
agents that are not directly exposed to floods face impacts through supply
chain disruptions, with the end-of-century average price of goods 5.6% higher
under RCP8.5 compared to the baseline. This open-source framework provides
financial institutions and companies with tools to quantify both direct and
cascading climate risks while evaluating cost-effective adaptation strategies.

</details>


### [51] [TERAG: Token-Efficient Graph-Based Retrieval-Augmented Generation](https://arxiv.org/abs/2509.18667)
*Qiao Xiao,Hong Ting Tsang,Jiaxin Bai*

Main category: cs.AI

TL;DR: TERAG是一个低成本图检索增强生成框架，通过个性化PageRank在检索阶段显著减少LLM令牌使用，仅用3%-11%的输出令牌达到主流图RAG方法80%以上的准确率。


<details>
  <summary>Details</summary>
Motivation: 现有图RAG系统在构建图时LLM令牌使用成本过高，阻碍了大规模应用，需要开发更经济的解决方案。

Method: 受HippoRAG启发，在检索阶段引入个性化PageRank(PPR)来构建信息丰富的图，大幅降低令牌消耗。

Result: TERAG仅消耗3%-11%的输出令牌，但能达到广泛使用的图RAG方法至少80%的准确率。

Conclusion: TERAG证明了在显著降低成本的同时保持高性能的可行性，为图RAG的大规模应用提供了实用解决方案。

Abstract: Graph-based Retrieval-augmented generation (RAG) has become a widely studied
approach for improving the reasoning, accuracy, and factuality of Large
Language Models. However, many existing graph-based RAG systems overlook the
high cost associated with LLM token usage during graph construction, hindering
large-scale adoption. To address this, we propose TERAG, a simple yet effective
framework designed to build informative graphs at a significantly lower cost.
Inspired by HippoRAG, we incorporate Personalized PageRank (PPR) during the
retrieval phase, and we achieve at least 80% of the accuracy of widely used
graph-based RAG methods while consuming only 3%-11% of the output tokens.

</details>


### [52] [Implementation of airborne ML models with semantics preservation](https://arxiv.org/abs/2509.18681)
*Nicolas Valot,Louis Fabre,Benjamin Lesage,Ammar Mechouche,Claire Pagetti*

Main category: cs.AI

TL;DR: 本文旨在澄清机器学习模型与其明确描述之间的区别，并完善语义保持的概念以确保模型的准确复制，应用于多个工业用例来构建和比较目标模型。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习在航空系统中应用的增加，需要确保ML系统的安全操作并符合相关指导标准。EASA和EUROCAE/SAE已发布高级目标，但需要更具体的方法来验证ML模型在目标环境中的性能。

Method: 通过定义机器学习模型描述（MLMD）来明确模型与其描述的区别，完善语义保持概念，并在多个工业用例中应用这些概念来构建和比较目标模型。

Result: 提出了MLMD的概念和语义保持的细化定义，通过工业用例验证了方法的有效性，能够确保ML模型在目标环境中准确复制其训练性能。

Conclusion: MLMD和语义保持概念的明确化有助于确保ML模型在航空等安全关键系统中的合规性和安全性，为ML系统的认证提供了理论基础和实践指导。

Abstract: Machine Learning (ML) may offer new capabilities in airborne systems.
However, as any piece of airborne systems, ML-based systems will be required to
guarantee their safe operation. Thus, their development will have to be
demonstrated to be compliant with the adequate guidance. So far, the European
Union Aviation Safety Agency (EASA) has published a concept paper and an
EUROCAE/SAE group is preparing ED-324. Both approaches delineate high-level
objectives to confirm the ML model achieves its intended function and maintains
training performance in the target environment. The paper aims to clarify the
difference between an ML model and its corresponding unambiguous description,
referred to as the Machine Learning Model Description (MLMD). It then refines
the essential notion of semantics preservation to ensure the accurate
replication of the model. We apply our contributions to several industrial use
cases to build and compare several target models.

</details>


### [53] [Advances in Large Language Models for Medicine](https://arxiv.org/abs/2509.18690)
*Zhiyu Kan,Wensheng Gan,Zhenlian Qi,Philip S. Yu*

Main category: cs.AI

TL;DR: 本文系统综述了大型语言模型在医疗领域的最新研究进展，包括训练技术、医疗应用、优势局限，并对医疗LLMs进行分类和评估方法分类，提出解决方案和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着AI技术快速发展，大型语言模型在医疗领域展现出巨大应用潜力，需要系统梳理当前研究进展，为后续研究提供指导。

Method: 采用系统性文献综述方法，深入分析医疗大模型的训练技术、适应方法、应用场景，并创新性地将医疗LLMs分为三类，评估方法分为两类。

Result: 全面梳理了医疗LLMs的发展现状，识别了现有挑战，并提出了相应的解决方案。

Conclusion: 医疗LLMs具有重要发展必要性，本文为其后续研究提供了系统性的理解和清晰的指导方向。

Abstract: Artificial intelligence (AI) technology has advanced rapidly in recent years,
with large language models (LLMs) emerging as a significant breakthrough. LLMs
are increasingly making an impact across various industries, with the medical
field standing out as the most prominent application area. This paper
systematically reviews the up-to-date research progress of LLMs in the medical
field, providing an in-depth analysis of training techniques for large medical
models, their adaptation in healthcare settings, related applications, as well
as their strengths and limitations. Furthermore, it innovatively categorizes
medical LLMs into three distinct types based on their training methodologies
and classifies their evaluation approaches into two categories. Finally, the
study proposes solutions to existing challenges and outlines future research
directions based on identified issues in the field of medical LLMs. By
systematically reviewing previous and advanced research findings, we aim to
highlight the necessity of developing medical LLMs, provide a deeper
understanding of their current state of development, and offer clear guidance
for subsequent research.

</details>


### [54] [Autonomous Data Agents: A New Opportunity for Smart Data](https://arxiv.org/abs/2509.18710)
*Yanjie Fu,Dongjie Wang,Wangyang Ying,Xiangliang Zhang,Huan Liu,Jian Pei*

Main category: cs.AI

TL;DR: 本文提出自主数据代理（DataAgents）的概念，通过集成LLM推理与任务分解、行动推理和工具调用，实现从数据到知识的自动化转换，代表数据管理向自主系统的范式转变。


<details>
  <summary>Details</summary>
Motivation: 随着数据规模和复杂性的增长，数据准备、转换和分析工作仍然劳动密集、重复且难以扩展。数据与AI之间的对齐至关重要，但现有数据结构往往不适合AI利用。

Method: DataAgents通过动态规划工作流、调用强大工具和适应多样化数据任务，能够处理数据收集、集成、预处理、选择、转换、重新加权、增强、重编程、修复和检索等操作。

Result: DataAgents能够将复杂和非结构化数据转化为连贯且可操作的知识，为数据到知识系统提供自主化解决方案。

Conclusion: 需要协同努力推进行动工作流优化、建立开放数据集和基准生态系统、保护隐私、平衡效率与可扩展性，并开发可信赖的DataAgent防护机制以防止恶意行为。

Abstract: As data continues to grow in scale and complexity, preparing, transforming,
and analyzing it remains labor-intensive, repetitive, and difficult to scale.
Since data contains knowledge and AI learns knowledge from it, the alignment
between AI and data is essential. However, data is often not structured in ways
that are optimal for AI utilization. Moreover, an important question arises:
how much knowledge can we pack into data through intensive data operations?
Autonomous data agents (DataAgents), which integrate LLM reasoning with task
decomposition, action reasoning and grounding, and tool calling, can
autonomously interpret data task descriptions, decompose tasks into subtasks,
reason over actions, ground actions into python code or tool calling, and
execute operations. Unlike traditional data management and engineering tools,
DataAgents dynamically plan workflows, call powerful tools, and adapt to
diverse data tasks at scale. This report argues that DataAgents represent a
paradigm shift toward autonomous data-to-knowledge systems. DataAgents are
capable of handling collection, integration, preprocessing, selection,
transformation, reweighing, augmentation, reprogramming, repairs, and
retrieval. Through these capabilities, DataAgents transform complex and
unstructured data into coherent and actionable knowledge. We first examine why
the convergence of agentic AI and data-to-knowledge systems has emerged as a
critical trend. We then define the concept of DataAgents and discuss their
architectural design, training strategies, as well as the new skills and
capabilities they enable. Finally, we call for concerted efforts to advance
action workflow optimization, establish open datasets and benchmark ecosystems,
safeguard privacy, balance efficiency with scalability, and develop trustworthy
DataAgent guardrails to prevent malicious actions.

</details>


### [55] [Experience Scaling: Post-Deployment Evolution For Large Language Models](https://arxiv.org/abs/2509.18771)
*Xingkun Yin,Kaibin Huang,Dong In Kim,Hongyang Du*

Main category: cs.AI

TL;DR: 提出经验扩展框架，通过自主环境交互和协作经验共享实现LLM的持续进化，突破静态人类生成数据的限制


<details>
  <summary>Details</summary>
Motivation: 传统通过扩大模型规模、训练数据和计算能力的方法已接近饱和，人类生成文本资源耗尽，进一步增益递减

Method: 经验扩展框架：捕获原始交互、提炼为紧凑可重用知识、定期优化存储内容以保持相关性和效率

Result: 在模拟真实场景中验证，包括泛化到未见相关任务、重复查询和过饱和知识存储，经验扩展提高了准确性、维持性能并保持对新情况的增益

Conclusion: 结构化部署后学习可以扩展LLM能力超越静态人类生成数据的限制，为持续智能进步提供可扩展路径

Abstract: Scaling model size, training data, and compute power have driven advances in
large language models (LLMs), but these approaches are reaching saturation as
human-generated text is exhausted and further gains diminish. We propose
experience scaling, a framework for continuous post-deployment evolution for
LLMs through autonomous interaction with the environment and collaborative
sharing of accumulated experience. The framework captures raw interactions,
distills them into compact, reusable knowledge, and periodically refines stored
content to preserve relevance and efficiency. We validate the framework in
simulated real-world scenarios involving generalization to previously unseen
but related tasks, repetitive queries, and over-saturated knowledge stores.
Across all settings, experience scaling improves accuracy, sustains performance
over time, and maintains gains when applied to novel situations. These results
demonstrate that structured post-deployment learning can extend LLM
capabilities beyond the limits of static human-generated data, offering a
scalable path for continued intelligence progress.

</details>


### [56] [The AGNTCY Agent Directory Service: Architecture and Implementation](https://arxiv.org/abs/2509.18787)
*Luca Muscariello,Vijoy Pandey,Ramiz Polic*

Main category: cs.AI

TL;DR: ADS是一个分布式目录服务，用于发现AI代理的能力、元数据和来源，采用内容寻址存储、层次分类法和加密签名实现高效、可验证的多维度发现。


<details>
  <summary>Details</summary>
Motivation: 解决异构多代理系统中代理能力发现、元数据管理和来源验证的问题，促进代理间的互操作性。

Method: 基于Open Agentic Schema Framework构建，采用两级映射的Kademlia分布式哈希表，重用OCI/ORAS基础设施进行工件分发，集成Sigstore进行来源验证。

Result: 实现了高效、可验证的代理能力发现系统，支持新兴代理模式（如LLM提示代理、MCP服务器等）的模式驱动扩展性。

Conclusion: ADS为新兴代理注册和互操作性倡议提供了重要的架构模型，具有安全性和性能优势。

Abstract: The Agent Directory Service (ADS) is a distributed directory for the
discovery of AI agent capabilities, metadata, and provenance. It leverages
content-addressed storage, hierarchical taxonomies, and cryptographic signing
to enable efficient, verifiable, and multi-dimensional discovery across
heterogeneous Multi-Agent Systems (MAS). Built on the Open Agentic Schema
Framework (OASF), ADS decouples capability indexing from content location
through a two-level mapping realized over a Kademlia-based Distributed Hash
Table (DHT). It reuses mature OCI / ORAS infrastructure for artifact
distribution, integrates Sigstore for provenance, and supports schema-driven
extensibility for emerging agent modalities (LLM prompt agents, MCP servers,
A2A-enabled components). This paper formalizes the architectural model,
describes storage and discovery layers, explains security and performance
properties, and positions ADS within the broader landscape of emerging agent
registry and interoperability initiatives.

</details>


### [57] [Bounded PCTL Model Checking of Large Language Model Outputs](https://arxiv.org/abs/2509.18836)
*Dennis Gross,Helge Spieker,Arnaud Gotlieb*

Main category: cs.AI

TL;DR: LLMCHECKER是一种基于模型检查的验证方法，用于验证LLM文本生成过程的概率计算树逻辑(PCTL)属性。该方法通过α-k有界文本生成来限制验证范围，重点关注每个生成步骤中top-k标记的累积概率。


<details>
  <summary>Details</summary>
Motivation: 现有LLM文本生成过程缺乏形式化验证方法，无法确保生成文本的一致性和可靠性。作者发现文本生成过程中通常只有有限数量的标记被选择，这为形式化验证提供了可能性。

Method: 提出α-k有界文本生成方法，在每个生成步骤中只考虑累积概率超过阈值α的top-k标记。LLMCHECKER基于模型检查技术，能够验证PCTL属性，支持多种文本量化评估方法。

Result: 该方法在多个LLM模型（包括Llama、Gemma、Mistral、Genstruct和BERT）上进行了验证，证明了其适用性。这是首次将PCTL模型检查应用于LLM文本生成过程的一致性验证。

Conclusion: LLMCHECKER为LLM文本生成过程提供了有效的形式化验证框架，能够确保生成文本的质量和一致性，填补了该领域的形式化验证空白。

Abstract: In this paper, we introduce LLMCHECKER, a model-checking-based verification
method to verify the probabilistic computation tree logic (PCTL) properties of
an LLM text generation process. We empirically show that only a limited number
of tokens are typically chosen during text generation, which are not always the
same. This insight drives the creation of $\alpha$-$k$-bounded text generation,
narrowing the focus to the $\alpha$ maximal cumulative probability on the
top-$k$ tokens at every step of the text generation process. Our verification
method considers an initial string and the subsequent top-$k$ tokens while
accommodating diverse text quantification methods, such as evaluating text
quality and biases. The threshold $\alpha$ further reduces the selected tokens,
only choosing those that exceed or meet it in cumulative probability.
LLMCHECKER then allows us to formally verify the PCTL properties of
$\alpha$-$k$-bounded LLMs. We demonstrate the applicability of our method in
several LLMs, including Llama, Gemma, Mistral, Genstruct, and BERT. To our
knowledge, this is the first time PCTL-based model checking has been used to
check the consistency of the LLM text generation process.

</details>


### [58] [Model selection meets clinical semantics: Optimizing ICD-10-CM prediction via LLM-as-Judge evaluation, redundancy-aware sampling, and section-aware fine-tuning](https://arxiv.org/abs/2509.18846)
*Hong-Jie Dai,Zheng-Hao Li,An-Tai Lu,Bo-Tsz Shain,Ming-Ta Li,Tatheer Hussain Mir,Kuang-Te Wang,Min-I Su,Pei-Kang Liu,Ming-Ju Tsai*

Main category: cs.AI

TL;DR: 提出一个模块化框架用于ICD-10-CM编码预测，通过原则性模型选择、冗余感知数据采样和结构化输入设计来解决现有LLM在医疗编码中的挑战。


<details>
  <summary>Details</summary>
Motivation: ICD编码对临床文档、计费和医疗分析至关重要，但目前仍是劳动密集型且易出错的任务。LLM在自动化ICD编码方面有潜力，但在基础模型选择、输入上下文化和训练数据冗余方面存在挑战。

Method: 提出模块化框架，包括：1）使用LLM-as-judge评估协议和Plackett-Luce聚合评估开源LLM；2）引入嵌入相似性度量和冗余感知采样策略；3）利用台湾医院的结构化出院摘要评估上下文效果。

Result: 在两个机构数据集上的实验表明，经过微调的选择基础模型在内部和外部评估中始终优于基线LLM。包含更多临床部分持续提高预测性能。

Conclusion: 该研究使用开源LLM建立了一个实用且原则性的ICD-10-CM编码预测方法，为自动化医疗编码系统的实际部署提供了可扩展的机构就绪解决方案。

Abstract: Accurate International Classification of Diseases (ICD) coding is critical
for clinical documentation, billing, and healthcare analytics, yet it remains a
labour-intensive and error-prone task. Although large language models (LLMs)
show promise in automating ICD coding, their challenges in base model
selection, input contextualization, and training data redundancy limit their
effectiveness. We propose a modular framework for ICD-10 Clinical Modification
(ICD-10-CM) code prediction that addresses these challenges through principled
model selection, redundancy-aware data sampling, and structured input design.
The framework integrates an LLM-as-judge evaluation protocol with Plackett-Luce
aggregation to assess and rank open-source LLMs based on their intrinsic
comprehension of ICD-10-CM code definitions. We introduced embedding-based
similarity measures, a redundancy-aware sampling strategy to remove
semantically duplicated discharge summaries. We leverage structured discharge
summaries from Taiwanese hospitals to evaluate contextual effects and examine
section-wise content inclusion under universal and section-specific modelling
paradigms. Experiments across two institutional datasets demonstrate that the
selected base model after fine-tuning consistently outperforms baseline LLMs in
internal and external evaluations. Incorporating more clinical sections
consistently improves prediction performance. This study uses open-source LLMs
to establish a practical and principled approach to ICD-10-CM code prediction.
The proposed framework provides a scalable, institution-ready solution for
real-world deployment of automated medical coding systems by combining informed
model selection, efficient data refinement, and context-aware prompting.

</details>


### [59] [MAPO: Mixed Advantage Policy Optimization](https://arxiv.org/abs/2509.18849)
*Wenke Huang,Quan Zhang,Yiyang Fang,Jian Liang,Xuankun Rong,Huanjin Yao,Guancheng Wan,Ke Liang,Wenwen He,Mingjun Li,Leszek Rutkowski,Mang Ye,Bo Du,Dacheng Tao*

Main category: cs.AI

TL;DR: 本文提出了一种名为MAPO（混合优势策略优化）的新GRPO策略，通过动态重加权优势函数来解决现有方法中的优势反转和优势镜像问题，从而更合理地分配不同查询样本间的优势。


<details>
  <summary>Details</summary>
Motivation: 现有的GRPO方法在基础模型推理任务中面临优势反转和优势镜像问题，导致不同查询样本间的优势分配不合理，影响了策略优化的效果。

Method: 提出MAPO方法，识别轨迹的不同确定性，为高确定性轨迹样本引入优势百分比偏差，并根据轨迹确定性动态重加权优势函数，使其能够自适应地考虑样本特定特征。

Result: 通过与相关最先进方法的比较以及对不同优势变体的消融研究，验证了所提方法的有效性。

Conclusion: MAPO是一种简单但有效的GRPO策略改进，能够更好地处理轨迹确定性差异，提升基础模型在推理任务中的性能。

Abstract: Recent advances in reinforcement learning for foundation models, such as
Group Relative Policy Optimization (GRPO), have significantly improved the
performance of foundation models on reasoning tasks. Notably, the advantage
function serves as a central mechanism in GRPO for ranking the trajectory
importance. However, existing explorations encounter both advantage reversion
and advantage mirror problems, which hinder the reasonable advantage allocation
across different query samples. In this work, we propose an easy but effective
GRPO strategy, Mixed Advantage Policy Optimization (MAPO). We reveal that the
trajectory appears with different certainty and propose the advantage percent
deviation for samples with high-certainty trajectories. Furthermore, we
dynamically reweight the advantage function for samples with varying trajectory
certainty, thereby adaptively configuring the advantage function to account for
sample-specific characteristics. Comparison with related state-of-the-art
methods, along with ablation studies on different advantage variants, validates
the effectiveness of our approach.

</details>


### [60] [Conf-Profile: A Confidence-Driven Reasoning Paradigm for Label-Free User Profiling](https://arxiv.org/abs/2509.18864)
*Yingxin Li,Jianbo Zhao,Xueyu Ren,Jie Tang,Wangjie You,Xu Chen,Kan Zhou,Chao Feng,Jiao Ran,Yuan Meng,Zhi Wang*

Main category: cs.AI

TL;DR: 提出了ProfileBench基准和Conf-Profile框架，通过置信度驱动的两阶段方法解决用户画像中的标签稀缺和噪声问题，显著提升了LLM在用户画像任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 用户画像作为用户理解的核心技术，LLMs为此提供了有前景的途径，但缺乏全面基准和面临标签稀缺、数据异构噪声等挑战。

Method: 提出Conf-Profile框架：1）利用置信度提示合成高质量标签；2）置信度加权投票和校准；3）置信度引导的无监督强化学习进行难度过滤和奖励加权。

Result: 实验结果显示Conf-Profile通过两阶段训练显著提升性能，在Qwen3-8B模型上F1分数提高了13.97。

Conclusion: 该研究为解决用户画像中的标签稀缺和可靠性问题提供了有效方案，证明了置信度驱动方法在提升LLM性能方面的有效性。

Abstract: User profiling, as a core technique for user understanding, aims to infer
structural attributes from user information. Large Language Models (LLMs)
provide a promising avenue for user profiling, yet the progress is hindered by
the lack of comprehensive benchmarks. To bridge this gap, we propose
ProfileBench, an industrial benchmark derived from a real-world video platform,
encompassing heterogeneous user data and a well-structured profiling taxonomy.
However, the profiling task remains challenging due to the difficulty of
collecting large-scale ground-truth labels, and the heterogeneous and noisy
user information can compromise the reliability of LLMs. To approach label-free
and reliable user profiling, we propose a Confidence-driven Profile reasoning
framework Conf-Profile, featuring a two-stage paradigm. We first synthesize
high-quality labels by leveraging advanced LLMs with confidence hints, followed
by confidence-weighted voting for accuracy improvement and confidence
calibration for a balanced distribution. The multiple profile results,
rationales, and confidence scores are aggregated and distilled into a
lightweight LLM. We further enhance the reasoning ability via confidence-guided
unsupervised reinforcement learning, which exploits confidence for difficulty
filtering, quasi-ground truth voting, and reward weighting. Experimental
results demonstrate that Conf-Profile delivers substantial performance through
the two-stage training, improving F1 by 13.97 on Qwen3-8B.

</details>


### [61] [Memory in Large Language Models: Mechanisms, Evaluation and Evolution](https://arxiv.org/abs/2509.18868)
*Dianxing Zhang,Wendong Li,Kani Song,Jiaye Lu,Gang Li,Liuchun Yang,Sheng Li*

Main category: cs.AI

TL;DR: 本文提出了一个统一的LLM记忆定义和四部分分类法（参数化、上下文、外部、程序性/情景性），建立了记忆四元组框架，并设计了分层评估协议来避免异构设置下的失真比较。


<details>
  <summary>Details</summary>
Motivation: 为了解决LLM记忆研究中的定义不一致、评估方法不统一、缺乏可比较框架的问题，建立一个可复现、可比较和可治理的坐标系统。

Method: 采用三设置协议（仅参数化、离线检索、在线检索）解耦能力与信息可用性，构建分层评估框架，整合时间治理和泄漏审计，并提出DMM Gov更新和遗忘协调机制。

Result: 建立了一个包含记忆定义、分类、评估协议和治理框架的完整体系，能够支持可复现的研究和可部署的系统。

Conclusion: 该框架为LLM记忆研究提供了标准化的坐标系统，支持记忆机制、评估和治理的统一分析，并提出了四个可测试的命题来指导未来研究。

Abstract: Under a unified operational definition, we define LLM memory as a persistent
state written during pretraining, finetuning, or inference that can later be
addressed and that stably influences outputs. We propose a four-part taxonomy
(parametric, contextual, external, procedural/episodic) and a memory quadruple
(location, persistence, write/access path, controllability). We link mechanism,
evaluation, and governance via the chain write -> read -> inhibit/update. To
avoid distorted comparisons across heterogeneous setups, we adopt a
three-setting protocol (parametric only, offline retrieval, online retrieval)
that decouples capability from information availability on the same data and
timeline. On this basis we build a layered evaluation: parametric (closed-book
recall, edit differential, memorization/privacy), contextual (position curves
and the mid-sequence drop), external (answer correctness vs snippet
attribution/faithfulness), and procedural/episodic (cross-session consistency
and timeline replay, E MARS+). The framework integrates temporal governance and
leakage auditing (freshness hits, outdated answers, refusal slices) and
uncertainty reporting via inter-rater agreement plus paired tests with
multiple-comparison correction. For updating and forgetting, we present DMM
Gov: coordinating DAPT/TAPT, PEFT, model editing (ROME, MEND, MEMIT, SERAC),
and RAG to form an auditable loop covering admission thresholds, rollout,
monitoring, rollback, and change audits, with specs for timeliness, conflict
handling, and long-horizon consistency. Finally, we give four testable
propositions: minimum identifiability; a minimal evaluation card; causally
constrained editing with verifiable forgetting; and when retrieval with
small-window replay outperforms ultra-long-context reading. This yields a
reproducible, comparable, and governable coordinate system for research and
deployment.

</details>


### [62] [LongCat-Flash-Thinking Technical Report](https://arxiv.org/abs/2509.18883)
*Meituan LongCat Team,Anchun Gui,Bei Li,Bingyang Tao,Bole Zhou,Borun Chen,Chao Zhang,Chao Zhang,Chengcheng Han,Chenhui Yang,Chi Zhang,Chong Peng,Chuyu Zhang,Cong Chen,Fengcun Li,Gang Xu,Guoyuan Lin,Hao Jiang,Hao Liang,Haomin Fu,Haoxiang Ma,Hong Liu,Hongyan Hao,Hongyin Tang,Hongyu Zang,Hongzhi Ni,Hui Su,Jiahao Liu,Jiahuan Li,Jialin Liu,Jianfei Zhang,Jianhao Xu,Jianing Wang,Jiaqi Sun,Jiaqi Zhang,Jiarong Shi,Jiawei Yang,Jingang Wang,Jinrui Ding,Jun Kuang,Jun Xu,Ke He,Kefeng Zhang,Keheng Wang,Keqing He,Li Wei,Liang Shi,Lin Qiu,Lingbin Kong,Lingchuan Liu,Linsen Guo,Longfei An,Mai Xia,Meng Zhou,Mengshen Zhu,Peng Pei,Pengcheng Jia,Qi Gu,Qi Guo,Qiong Huang,Quan Chen,Quanchi Weng,Rongxiang Weng,Ruichen Shao,Rumei Li,Shanglin Lei,Shuai Du,Shuaikang Liu,Shuang Zhou,Shuhao Hu,Siyu Xu,Songshan Gong,Tao Liang,Tianhao Hu,Wei He,Wei Shi,Wei Wang,Wei Wu,Wei Zhuo,Weifeng Tang,Wenjie Shi,Wenlong Zhu,Xi Su,Xiangcheng Liu,Xiangyu Xi,Xiangzhou Huang,Xiao Liu,Xiaochen Jiang,Xiaowei Shi,Xiaowen Shi,Xiaoyu Li,Xin Chen,Xinyue Zhao,Xuan Huang,Xuemiao Zhang,Xuezhi Cao,Xunliang Cai,Yajie Zhang,Yang Chen,Yang Liu,Yang Liu,Yang Zheng,Yaoming Wang,Yaqi Huo,Yerui Sun,Yifan Lu,Yiyang Li,Youshao Xiao,Yuanzhe Lei,Yuchen Xie,Yueqing Sun,Yufei Zhang,Yuhuai Wei,Yulei Qian,Yunke Zhao,Yuqing Ding,Yuwei Jiang,Zhaohua Yang,Zhengyu Chen,Zhijian Liu,Zhikang Xia,Zhongda Su,Ziran Li,Ziwen Wang,Ziyuan Zhuang,Zongyu Wang,Zunyuan Yang*

Main category: cs.AI

TL;DR: LongCat-Flash-Thinking是一个5600亿参数的开放源代码MoE推理模型，通过精心设计的训练流程实现高效推理能力，包括长链思维数据冷启动和大规模强化学习训练。


<details>
  <summary>Details</summary>
Motivation: 开发一个高效的开放源代码推理模型，解决复杂推理任务中的性能瓶颈，特别是在STEM、代码和智能体推理等领域实现更好的效率和准确性。

Method: 采用混合专家模型架构，结合长链思维数据冷启动训练和领域并行训练方案，使用DORA系统进行大规模异步强化学习训练，实现三倍以上的训练加速。

Result: 在复杂推理任务上达到开源模型的最先进性能，在AIME-25任务上平均token消耗减少64.5%，从19,653降至6,965，同时保持任务准确性。

Conclusion: LongCat-Flash-Thinking展示了在推理系统和智能体AI研究方面的显著进步，该模型已公开发布以促进相关领域的进一步发展。

Abstract: We present LongCat-Flash-Thinking, an efficient 560-billion-parameter
open-source Mixture-of-Experts (MoE) reasoning model. Its advanced capabilities
are cultivated through a meticulously crafted training process, beginning with
long Chain-of-Thought (CoT) data cold-start and culminating in large-scale
Reinforcement Learning (RL). We first employ a well-designed cold-start
training strategy, which significantly enhances the reasoning potential and
equips the model with specialized skills in both formal and agentic reasoning.
Then, a core innovation is our domain-parallel training scheme, which decouples
optimization across distinct domains (e.g., STEM, Code, Agentic) and
subsequently fuses the resulting expert models into a single, nearly
Pareto-optimal model. This entire process is powered by our Dynamic
ORchestration for Asynchronous rollout (DORA) system, a large-scale RL
framework that delivers a greater than threefold training speedup over
synchronous methods on tens of thousands of accelerators. As a result,
LongCat-Flash-Thinking achieves state-of-the-art performance among open-source
models on a suite of complex reasoning tasks. The model exhibits exceptional
efficiency in agentic reasoning, reducing average token consumption by 64.5%
(from 19, 653 to 6, 965) on AIME-25, without degrading task accuracy. We
release LongCat-Flash-Thinking to promote further advances in reasoning systems
and agentic AI research.

</details>


### [63] [How Far are VLMs from Visual Spatial Intelligence? A Benchmark-Driven Perspective](https://arxiv.org/abs/2509.18905)
*Songsong Yu,Yuxin Chen,Hao Ju,Lianjie Jia,Fuxi Zhang,Shaofei Huang,Yuhan Wu,Rundi Cui,Binghao Ran,Zaibin Zhang,Zhedong Zheng,Zhipeng Zhang,Yifan Wang,Lin Song,Lijun Wang,Yanwei Li,Ying Shan,Huchuan Lu*

Main category: cs.AI

TL;DR: 本文系统研究了视觉语言模型中的视觉空间推理能力，提出了空间智能的三个能力层次，并创建了包含20个开源数据集的SIBench基准测试。实验发现当前模型在基础感知任务上表现良好，但在理解和规划任务上存在明显差距。


<details>
  <summary>Details</summary>
Motivation: 视觉空间推理是人类核心认知能力，也是推进具身智能和自主系统的关键要求。尽管视觉语言模型取得了进展，但由于三维空间表示和推理的复杂性，实现人类水平的视觉空间推理仍然极具挑战。

Method: 系统调查了视觉语言模型中的VSR方法，包括输入模态、模型架构、训练策略和推理机制。将空间智能分为三个能力层次（基础感知、空间理解、空间规划），并创建了SIBench基准测试，涵盖23个任务设置的近20个开源数据集。

Result: 实验表明，最先进的视觉语言模型在感知和推理之间存在显著差距。模型在基础感知任务上表现良好，但在理解和规划任务上持续表现不佳，特别是在数值估计、多视角推理、时间动态和空间想象方面。

Conclusion: 这些发现强调了在实现空间智能方面仍然存在的重大挑战，同时为未来研究提供了系统路线图和全面基准。研究资源可在指定网站获取。

Abstract: Visual Spatial Reasoning (VSR) is a core human cognitive ability and a
critical requirement for advancing embodied intelligence and autonomous
systems. Despite recent progress in Vision-Language Models (VLMs), achieving
human-level VSR remains highly challenging due to the complexity of
representing and reasoning over three-dimensional space. In this paper, we
present a systematic investigation of VSR in VLMs, encompassing a review of
existing methodologies across input modalities, model architectures, training
strategies, and reasoning mechanisms. Furthermore, we categorize spatial
intelligence into three levels of capability, ie, basic perception, spatial
understanding, spatial planning, and curate SIBench, a spatial intelligence
benchmark encompassing nearly 20 open-source datasets across 23 task settings.
Experiments with state-of-the-art VLMs reveal a pronounced gap between
perception and reasoning, as models show competence in basic perceptual tasks
but consistently underperform in understanding and planning tasks, particularly
in numerical estimation, multi-view reasoning, temporal dynamics, and spatial
imagination. These findings underscore the substantial challenges that remain
in achieving spatial intelligence, while providing both a systematic roadmap
and a comprehensive benchmark to drive future research in the field. The
related resources of this study are accessible at
https://sibench.github.io/Awesome-Visual-Spatial-Reasoning/.

</details>


### [64] [Data Efficient Adaptation in Large Language Models via Continuous Low-Rank Fine-Tuning](https://arxiv.org/abs/2509.18942)
*Xiao Han,Zimo Zhao,Wanyu Wang,Maolin Wang,Zitao Liu,Yi Chang,Xiangyu Zhao*

Main category: cs.AI

TL;DR: 本文提出DEAL框架，结合LoRA和持续微调策略，解决传统微调方法中的灾难性遗忘和数据效率低下的问题，在15个数据集上验证了其优越性能。


<details>
  <summary>Details</summary>
Motivation: 传统微调方法存在灾难性遗忘和数据效率低下的问题，限制了其在真实场景中的应用。

Method: DEAL框架整合了低秩适应（LoRA）和持续微调策略，包含知识保留和自适应参数更新模块。

Result: 在15个多样化数据集上的实验表明，DEAL在任务准确性和资源效率方面显著优于基线方法。

Conclusion: 该方法通过提升任务性能和资源效率，展示了在LLMs中推进持续适应的潜力。

Abstract: Recent advancements in Large Language Models (LLMs) have emphasized the
critical role of fine-tuning (FT) techniques in adapting LLMs to specific
tasks, especially when retraining from scratch is computationally infeasible.
Fine-tuning enables LLMs to leverage task- or domain-specific data, producing
models that more effectively meet the requirements of targeted applications.
However, con- ventional FT approaches often suffer from catastrophic forgetting
and suboptimal data efficiency, limiting their real-world applicability. To
address these challenges, this paper proposes DEAL, a novel framework that
integrates Low-Rank Adapta- tion (LoRA) with a continuous fine-tuning strategy.
By incorporating knowledge retention and adaptive parameter update modules, the
framework mitigates the lim- itations of existing FT methods while maintaining
efficiency in privacy-preserving settings. Experiments on 15 diverse datasets
show that DEAL consistently outper- forms baseline methods, yielding
substantial gains in task accuracy and resource efficiency. These findings
demonstrate the potential of our approach to advance continual adaptation in
LLMs by enhancing task performance while improving resource efficiency.

</details>


### [65] [LLM-based Agents Suffer from Hallucinations: A Survey of Taxonomy, Methods, and Directions](https://arxiv.org/abs/2509.18970)
*Xixun Lin,Yucheng Ning,Jingwen Zhang,Yan Dong,Yilong Liu,Yongxuan Wu,Xiaohua Qi,Nan Sun,Yanmin Shang,Pengfei Cao,Lixin Zou,Xu Chen,Chuan Zhou,Jia Wu,Shirui Pan,Bin Wang,Yanan Cao,Kai Chen,Songlin Hu,Li Guo*

Main category: cs.AI

TL;DR: 本文对基于大语言模型（LLM）的智能代理系统中的幻觉问题进行了首次全面调查，提出了新的分类法，分析了18种触发原因，并总结了幻觉缓解和检测方法。


<details>
  <summary>Details</summary>
Motivation: 随着LLM智能代理在现实应用中的广泛部署，其幻觉问题可能导致错误任务执行并影响系统可靠性，需要系统性地理解和解决这一关键挑战。

Method: 通过仔细分析代理的完整工作流程，提出了在不同阶段发生的代理幻觉新分类法，深入研究了18种幻觉触发原因，并综述了大量现有研究中的缓解和检测方法。

Result: 建立了基于LLM代理幻觉的系统性分析框架，为理解、检测和缓解代理幻觉问题提供了理论基础和实践指导。

Conclusion: 这项调查有望激发更多解决LLM代理幻觉问题的研究，最终促进更稳健可靠的代理系统发展。

Abstract: Driven by the rapid advancements of Large Language Models (LLMs), LLM-based
agents have emerged as powerful intelligent systems capable of human-like
cognition, reasoning, and interaction. These agents are increasingly being
deployed across diverse real-world applications, including student education,
scientific research, and financial analysis. However, despite their remarkable
potential, LLM-based agents remain vulnerable to hallucination issues, which
can result in erroneous task execution and undermine the reliability of the
overall system design. Addressing this critical challenge requires a deep
understanding and a systematic consolidation of recent advances on LLM-based
agents. To this end, we present the first comprehensive survey of
hallucinations in LLM-based agents. By carefully analyzing the complete
workflow of agents, we propose a new taxonomy that identifies different types
of agent hallucinations occurring at different stages. Furthermore, we conduct
an in-depth examination of eighteen triggering causes underlying the emergence
of agent hallucinations. Through a detailed review of a large number of
existing studies, we summarize approaches for hallucination mitigation and
detection, and highlight promising directions for future research. We hope this
survey will inspire further efforts toward addressing hallucinations in
LLM-based agents, ultimately contributing to the development of more robust and
reliable agent systems.

</details>


### [66] [From latent factors to language: a user study on LLM-generated explanations for an inherently interpretable matrix-based recommender system](https://arxiv.org/abs/2509.18980)
*Maxime Manderlier,Fabian Lecron,Olivier Vu Thanh,Nicolas Gillis*

Main category: cs.AI

TL;DR: 研究大型语言模型能否从数学可解释的推荐模型中生成有效的用户解释，通过用户研究评估不同解释策略的质量。


<details>
  <summary>Details</summary>
Motivation: 当前可解释AI研究多依赖自动评估指标，但这些指标往往无法捕捉用户的真实需求和感知，因此需要采用用户中心的方法来评估解释质量。

Method: 使用基于约束矩阵分解的可解释推荐模型，通过精心设计的LLM提示将模型结构转换为自然语言解释，并在326名参与者中进行用户研究，评估五种关键维度的解释质量。

Result: 所有解释类型都普遍受到好评，不同策略之间存在中等统计差异，用户评论提供了超越定量结果的补充见解。

Conclusion: LLM能够从数学可解释的推荐模型中生成有效的用户解释，用户中心评估方法为解释质量提供了更全面的理解。

Abstract: We investigate whether large language models (LLMs) can generate effective,
user-facing explanations from a mathematically interpretable recommendation
model. The model is based on constrained matrix factorization, where user types
are explicitly represented and predicted item scores share the same scale as
observed ratings, making the model's internal representations and predicted
scores directly interpretable. This structure is translated into natural
language explanations using carefully designed LLM prompts. Many works in
explainable AI rely on automatic evaluation metrics, which often fail to
capture users' actual needs and perceptions. In contrast, we adopt a
user-centered approach: we conduct a study with 326 participants who assessed
the quality of the explanations across five key dimensions-transparency,
effectiveness, persuasion, trust, and satisfaction-as well as the
recommendations themselves.To evaluate how different explanation strategies are
perceived, we generate multiple explanation types from the same underlying
model, varying the input information provided to the LLM. Our analysis reveals
that all explanation types are generally well received, with moderate
statistical differences between strategies. User comments further underscore
how participants react to each type of explanation, offering complementary
insights beyond the quantitative results.

</details>


### [67] [Remaining Time Prediction in Outbound Warehouse Processes: A Case Study (Short Paper)](https://arxiv.org/abs/2509.18986)
*Erik Penther,Michael Grohs,Jana-Rebecca Rehse*

Main category: cs.AI

TL;DR: 本文比较了四种剩余时间预测方法在物流公司出库仓库流程中的表现，发现深度学习模型准确率最高，但浅层方法如传统提升技术在计算资源需求上更优。


<details>
  <summary>Details</summary>
Motivation: 预测流程监控是流程挖掘的子领域，旨在预测正在进行的流程执行的未来。剩余时间预测是常见目标之一，即流程执行完成所需的时间。

Method: 在航空业务物流公司的真实出库仓库流程中，比较了四种不同的剩余时间预测方法。使用了包含169,523条轨迹的新颖原始事件日志。

Result: 深度学习模型达到最高准确率，但浅层方法如传统提升技术实现了竞争性的准确率，且需要显著更少的计算资源。

Conclusion: 虽然深度学习在准确率上表现最佳，但浅层方法在计算效率方面具有优势，为实际应用提供了可行的替代方案。

Abstract: Predictive process monitoring is a sub-domain of process mining which aims to
forecast the future of ongoing process executions. One common prediction target
is the remaining time, meaning the time that will elapse until a process
execution is completed. In this paper, we compare four different remaining time
prediction approaches in a real-life outbound warehouse process of a logistics
company in the aviation business. For this process, the company provided us
with a novel and original event log with 169,523 traces, which we can make
publicly available. Unsurprisingly, we find that deep learning models achieve
the highest accuracy, but shallow methods like conventional boosting techniques
achieve competitive accuracy and require significantly fewer computational
resources.

</details>


### [68] [Landmarks, Monuments, and Beacons: Understanding Generative Calls to Action](https://arxiv.org/abs/2509.19030)
*Victoire Hervé,Henrik Warpefelt,Christoph Salge*

Main category: cs.AI

TL;DR: 本文提出了基于玩家视角的Landmarks、Monuments和Beacons概念，用于解决程序生成内容评估中度量标准与人类体验不一致的问题。


<details>
  <summary>Details</summary>
Motivation: 程序生成内容的算法评估难以找到与人类体验一致的度量标准，特别是对于复合人工制品。自动分解需要满足一系列属性的概念。

Method: 借鉴游戏研究和游戏AI研究，引入基于人工制品的可感知性、唤起性和行动召唤性的嵌套概念：Landmarks、Monuments和Beacons。这些概念是通用的，可跨游戏类型使用。

Result: 这些实体可以通过当前研究和工业中使用的技术来发现和评估，为程序生成内容的完全自动分解和重要子组件的评估开辟了道路。

Conclusion: 该方法旨在建立人文学科与技术游戏研究之间的联系，实现更好的计算程序生成内容评估，虽然重点强调混合主动程序生成和组合程序生成，但适用范围更广。

Abstract: Algorithmic evaluation of procedurally generated content struggles to find
metrics that align with human experience, particularly for composite artefacts.
Automatic decomposition as a possible solution requires concepts that meet a
range of properties. To this end, drawing on Games Studies and Game AI
research, we introduce the nested concepts of \textit{Landmarks},
\textit{Monuments}, and \textit{Beacons}. These concepts are based on the
artefact's perceivability, evocativeness, and Call to Action, all from a
player-centric perspective. These terms are generic to games and usable across
genres. We argue that these entities can be found and evaluated with techniques
currently used in both research and industry, opening a path towards a fully
automated decomposition of PCG, and evaluation of the salient sub-components.
Although the work presented here emphasises mixed-initiative PCG and
compositional PCG, we believe it applies beyond those domains. With this
approach, we intend to create a connection between humanities and technical
game research and allow for better computational PCG evaluation

</details>


### [69] [Towards Causal Representation Learning with Observable Sources as Auxiliaries](https://arxiv.org/abs/2509.19058)
*Kwonho Kim,Heejeong Nam,Inwoo Hwang,Sanghack Lee*

Main category: cs.AI

TL;DR: 本文提出了一种新的因果表示学习框架，使用可观测源作为辅助变量来识别潜在变量，通过体积保持编码器实现子空间级别的变换和置换识别，并提供了变量选择方案来最大化潜在因素的可恢复性。


<details>
  <summary>Details</summary>
Motivation: 现有因果表示学习方法通常依赖外部辅助变量进行条件独立性假设，但限制了辅助变量的范围。实际上，系统驱动的潜在因素可能容易从数据中观测或提取，这有助于识别过程。

Method: 引入可观测源作为辅助变量的框架，使用体积保持编码器进行潜在变量识别，当存在多个已知辅助变量时，提供变量选择方案来选择能最大化潜在因素可恢复性的变量。

Result: 实验在合成图数据和图像数据上验证了框架的有效性，能够识别整个潜在变量到子空间变换和置换的程度。

Conclusion: 该框架扩展了当前方法的边界，通过利用可观测源作为辅助变量，为因果表示学习提供了新的识别途径。

Abstract: Causal representation learning seeks to recover latent factors that generate
observational data through a mixing function. Needing assumptions on latent
structures or relationships to achieve identifiability in general, prior works
often build upon conditional independence given known auxiliary variables.
However, prior frameworks limit the scope of auxiliary variables to be external
to the mixing function. Yet, in some cases, system-driving latent factors can
be easily observed or extracted from data, possibly facilitating
identification. In this paper, we introduce a framework of observable sources
being auxiliaries, serving as effective conditioning variables. Our main
results show that one can identify entire latent variables up to subspace-wise
transformations and permutations using volume-preserving encoders. Moreover,
when multiple known auxiliary variables are available, we offer a
variable-selection scheme to choose those that maximize recoverability of the
latent factors given knowledge of the latent causal graph. Finally, we
demonstrate the effectiveness of our framework through experiments on synthetic
graph and image data, thereby extending the boundaries of current approaches.

</details>


### [70] [Code Driven Planning with Domain-Adaptive Critic](https://arxiv.org/abs/2509.19077)
*Zikang Tian,Shaohui Peng,Du Huang,Jiaming Guo,Ruizhi Chen,Rui Zhang,Xishan Zhang,Yuxuan Guo,Zidong Du,Qi Guo,Ling Li,Yewen Pu,Xing Hu,Yunji Chen*

Main category: cs.AI

TL;DR: CoPiC提出了一种基于代码驱动规划和领域自适应评估器的LLM任务规划方法，通过生成多样化高层规划程序并利用训练好的评估器选择长期奖励最优的计划，显著减少LLM查询次数并提高规划质量。


<details>
  <summary>Details</summary>
Motivation: 现有LLM规划方法依赖频繁查询和环境反馈进行迭代优化，导致高查询成本且难以实现长期奖励对齐。需要一种既能减少查询次数又能优化长期规划效果的方法。

Method: 1) 使用LLM生成多样化高层规划程序；2) 这些程序迭代产生和优化候选计划；3) 训练领域自适应评估器评估候选计划；4) 选择长期奖励最优的计划执行。

Result: 在ALFWorld、NetHack和StarCraft II Unit Building三个环境中，CoPiC相比AdaPlanner和Reflexion基线方法，平均成功率提升23.33%，查询成本降低91.27%。

Conclusion: CoPiC通过代码驱动规划和领域自适应评估器的结合，有效解决了LLM规划中的查询成本高和长期奖励对齐问题，在多个复杂环境中表现出优越性能。

Abstract: Large Language Models (LLMs) have been widely adopted as task planners for AI
agents in sequential decision-making problems, leveraging their extensive world
knowledge. However, the gap between their general knowledge and
environment-specific requirements often leads to inaccurate plans. To address
this, existing approaches rely on frequent LLM queries to iteratively refine
plans based on immediate environmental feedback, which incurs substantial query
costs. However, this refinement is typically guided by short-term environmental
feedback, limiting LLMs from developing plans aligned with long-term rewards.
We propose Code Driven Planning with Domain-Adaptive Critic (CoPiC). Instead of
relying on frequent queries, CoPiC employs LLMs to generate a diverse set of
high-level planning programs, which iteratively produce and refine candidate
plans. A trained domain-adaptive critic then evaluates these candidates and
selects the one most aligned with long-term rewards for execution. Using
high-level planning programs as planner and domain-adaptive critic as
estimator, CoPiC improves planning while significantly reducing query costs.
Results in ALFWorld, NetHack, and StarCraft II Unit Building show that CoPiC
outperforms advanced LLM-based baselines, AdaPlanner and Reflexion, achieving
an average (1) 23.33% improvement in success rate and (2) 91.27% reduction in
query costs.

</details>


### [71] [AgentInit: Initializing LLM-based Multi-Agent Systems via Diversity and Expertise Orchestration for Effective and Efficient Collaboration](https://arxiv.org/abs/2509.19236)
*Chunhao Tian,Yutong Wang,Xuebo Liu,Zhexuan Wang,Liang Ding,Miao Zhang,Min Zhang*

Main category: cs.AI

TL;DR: AgentInit是一个多智能体系统初始化方法，通过优化智能体团队结构来提升系统性能，结合自然语言到格式转换机制和帕累托平衡选择策略，在多个任务中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有MAS初始化方法未能充分考虑智能体在后续阶段的协作需求，需要一种能够优化智能体团队结构的方法来提升系统效率和效果。

Method: 提出AgentInit方法，包含多轮智能体交互和反思、自然语言到格式转换机制确保一致性，以及基于帕累托原则的平衡团队选择策略来兼顾多样性和任务相关性。

Result: 实验显示AgentInit在各种框架和任务中均优于现有初始化方法和预定义策略，性能提升分别达到1.2和1.6倍，同时显著减少token消耗，并表现出良好的任务迁移能力。

Conclusion: AgentInit作为一种可靠的MAS初始化方法，展示了强大的能力和适应性，其关键组件的有效性得到验证。

Abstract: Proper initialization is crucial for any system, particularly in multi-agent
systems (MAS), where it plays a pivotal role in determining both the system's
efficiency and effectiveness. However, existing MAS initialization methods do
not fully account for the collaborative needs of the generated agents in
subsequent stages. Inspired by the principles of effective team composition, we
propose AgentInit, which aims to optimize the structure of agent teams.
Specifically, in addition to multi-round interactions and reflections between
agents during agent generation, AgentInit incorporates a Natural Language to
Format mechanism to ensure consistency and standardization. Balanced team
selection strategies using Pareto principles are subsequently applied to
jointly consider agent team diversity and task relevance to promote effective
and efficient collaboration and enhance overall system performance. Experiments
show that AgentInit consistently outperforms state-of-the-art initialization
methods and pre-defined strategies across various frameworks and tasks,
achieving an overall performance improvement of up to 1.2 and 1.6,
respectively, while also significantly reducing token consumption. Further
analysis confirms its strong transferability to similar tasks and verifies the
effectiveness of its key components, demonstrating its capability and
adaptability as a reliable MAS initialization method. Source code and models
are available at https://github.com/1737423697/AgentInit.

</details>


### [72] [Cross-Cultural Transfer of Commonsense Reasoning in LLMs: Evidence from the Arab World](https://arxiv.org/abs/2509.19265)
*Saeed Almheiri,Rania Hossam,Mena Attia,Chenxi Wang,Preslav Nakov,Timothy Baldwin,Fajri Koto*

Main category: cs.AI

TL;DR: 本文研究了LLMs在阿拉伯世界的跨文化常识推理迁移，发现仅需12个文化特定示例即可平均提升10%性能，且来自印尼和美国的跨文化演示也能实现类似效果。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型存在西方中心偏见，限制了其在多元文化背景下的有效性。虽然已有研究探索文化对齐，但跨文化迁移潜力（利用一种文化的对齐来改进其他文化的性能）仍未被充分探索。

Method: 使用覆盖13个阿拉伯国家的文化基础常识推理数据集，评估轻量级对齐方法（如上下文学习、演示强化DITTO）以及基线方法（监督微调、直接偏好优化）。

Result: 结果显示，仅需来自一个国家的12个文化特定示例，就能在多语言模型中平均提升其他国家的性能10%。来自印尼和美国的跨文化演示在MCQ推理中也能达到或超越文化内对齐效果。

Conclusion: 这些发现表明高效的跨文化对齐是可行的，为将LLMs适配到低资源文化环境提供了有前景的方法。

Abstract: Large language models (LLMs) often reflect Western-centric biases, limiting
their effectiveness in diverse cultural contexts. Although some work has
explored cultural alignment, the potential for cross-cultural transfer, using
alignment in one culture to improve performance in others, remains
underexplored. This paper investigates cross-cultural transfer of commonsense
reasoning in the Arab world, where linguistic and historical similarities
coexist with local cultural differences. Using a culturally grounded
commonsense reasoning dataset covering 13 Arab countries, we evaluate
lightweight alignment methods such as in-context learning and
demonstration-based reinforcement (DITTO), alongside baselines like supervised
fine-tuning and direct preference optimization. Our results show that merely 12
culture-specific examples from one country can improve performance in others by
10\% on average, within multilingual models. In addition, we demonstrate that
out-of-culture demonstrations from Indonesia and US contexts can match or
surpass in-culture alignment for MCQ reasoning, highlighting cultural
commonsense transferability beyond the Arab world. These findings demonstrate
that efficient cross-cultural alignment is possible and offer a promising
approach to adapt LLMs to low-resource cultural settings.

</details>
