<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 20]
- [cs.CR](#cs.CR) [Total: 28]
- [cs.AI](#cs.AI) [Total: 74]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [On the need to perform comprehensive evaluations of automated program repair benchmarks: Sorald case study](https://arxiv.org/abs/2508.15135)
*Sumudu Liyanage,Sherlock A. Licorish,Markus Wagner,Stephen G. MacDonell*

Main category: cs.SE

TL;DR: 本研究开发了一个全面的自动化程序修复(APR)工具评估框架，并以Sorald工具为例进行验证，发现虽然能修复特定规则违规，但会引入新缺陷、降低功能正确性和代码结构质量。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要评估APR工具清除违规的能力，但忽视了其可能引入新违规、改变代码功能和降低代码结构质量的问题，需要开发全面的评估框架。

Method: 以Sorald工具为案例研究，评估其修复3,529个SonarQube违规的能力，涉及30个规则和2,393个Java代码片段，全面分析修复效果和副作用。

Result: Sorald修复了特定规则违规，但引入了2,120个新缺陷（32个bug和2,088个代码异味），导致24%的单元测试失败率，并降低了代码结构质量。

Conclusion: 需要开发能够捕捉APR工具全方位影响的评估方法，包括副作用评估，以确保其安全有效应用。

Abstract: In supporting the development of high-quality software, especially necessary
in the era of LLMs, automated program repair (APR) tools aim to improve code
quality by automatically addressing violations detected by static analysis
profilers. Previous research tends to evaluate APR tools only for their ability
to clear violations, neglecting their potential introduction of new (sometimes
severe) violations, changes to code functionality and degrading of code
structure. There is thus a need for research to develop and assess
comprehensive evaluation frameworks for APR tools. This study addresses this
research gap, and evaluates Sorald (a state-of-the-art APR tool) as a proof of
concept. Sorald's effectiveness was evaluated in repairing 3,529 SonarQube
violations across 30 rules within 2,393 Java code snippets extracted from Stack
Overflow. Outcomes show that while Sorald fixes specific rule violations, it
introduced 2,120 new faults (32 bugs, 2088 code smells), reduced code
functional correctness--as evidenced by a 24% unit test failure rate--and
degraded code structure, demonstrating the utility of our framework. Findings
emphasize the need for evaluation methodologies that capture the full spectrum
of APR tool effects, including side effects, to ensure their safe and effective
adoption.

</details>


### [2] [Foundational Design Principles and Patterns for Building Robust and Adaptive GenAI-Native Systems](https://arxiv.org/abs/2508.15411)
*Frederik Vandeputte*

Main category: cs.SE

TL;DR: 本文提出GenAI原生系统设计范式，将生成式AI的认知能力与传统软件工程原则结合，构建可靠、自适应、高效的系统。


<details>
  <summary>Details</summary>
Motivation: 生成式AI虽然具有变革性能力，但由于其不可预测性和低效性，在开发可靠高效的GenAI赋能系统时面临重大挑战。

Method: 提出基于五个关键支柱（可靠性、卓越性、可进化性、自依赖性和保障性）的GenAI原生设计原则，以及GenAI原生单元、有机基质和可编程路由器等架构模式。

Result: 构建了GenAI原生软件栈的关键要素框架，并从技术、用户采用、经济和法律角度分析了这些系统的影响。

Conclusion: 该工作旨在激发未来研究，鼓励相关社区实施和完善这一概念框架，需要进一步验证和实验。

Abstract: Generative AI (GenAI) has emerged as a transformative technology,
demonstrating remarkable capabilities across diverse application domains.
However, GenAI faces several major challenges in developing reliable and
efficient GenAI-empowered systems due to its unpredictability and inefficiency.
This paper advocates for a paradigm shift: future GenAI-native systems should
integrate GenAI's cognitive capabilities with traditional software engineering
principles to create robust, adaptive, and efficient systems.
  We introduce foundational GenAI-native design principles centered around five
key pillars -- reliability, excellence, evolvability, self-reliance, and
assurance -- and propose architectural patterns such as GenAI-native cells,
organic substrates, and programmable routers to guide the creation of resilient
and self-evolving systems. Additionally, we outline the key ingredients of a
GenAI-native software stack and discuss the impact of these systems from
technical, user adoption, economic, and legal perspectives, underscoring the
need for further validation and experimentation. Our work aims to inspire
future research and encourage relevant communities to implement and refine this
conceptual framework.

</details>


### [3] [An Empirical Study of Knowledge Distillation for Code Understanding Tasks](https://arxiv.org/abs/2508.15423)
*Ruiqi Wang,Zezhou Yang,Cuiyun Gao,Xin Xia,Qing Liao*

Main category: cs.SE

TL;DR: 本文系统研究了知识蒸馏在代码理解任务中的有效性，发现特征蒸馏方法效果最佳，学生模型仅需5%参数即可达到教师模型98%的性能。


<details>
  <summary>Details</summary>
Motivation: 预训练语言模型在代码理解中表现出色但计算密集，知识蒸馏技术能解决部署时的计算强度和推理延迟问题，但在代码理解领域的潜力尚未充分探索。

Method: 研究两种主流知识蒸馏方法（基于logit和基于特征），在8个学生模型和2个教师PLM上进行实验，涵盖三个下游任务。

Result: 知识蒸馏相比标准微调能显著提升各种规模学生模型的性能，代码专用PLM作为教师效果更好，特征蒸馏方法表现最优。

Conclusion: 知识蒸馏是代码理解任务中有效的模型压缩技术，学生架构与教师相似性不一定带来更好性能，为高效代码理解模型部署提供了重要见解。

Abstract: Pre-trained language models (PLMs) have emerged as powerful tools for code
understanding. However, deploying these PLMs in large-scale applications faces
practical challenges due to their computational intensity and inference
latency. Knowledge distillation (KD), a promising model compression and
acceleration technique, addresses these limitations by transferring knowledge
from large teacher models to compact student models, enabling efficient
inference while preserving most of the teacher models' capabilities. While this
technique has shown remarkable success in natural language processing and
computer vision domains, its potential for code understanding tasks remains
largely underexplored.
  In this paper, we systematically investigate the effectiveness and usage of
KD in code understanding tasks. Our study encompasses two popular types of KD
methods, i.e., logit-based and feature-based KD methods, experimenting across
eight student models and two teacher PLMs from different domains on three
downstream tasks. The experimental results indicate that KD consistently offers
notable performance boosts across student models with different sizes compared
with standard fine-tuning. Notably, code-specific PLM demonstrates better
effectiveness as the teacher model. Among all KD methods, the latest
feature-based KD methods exhibit superior performance, enabling student models
to retain up to 98% teacher performance with merely 5% parameters. Regarding
student architecture, our experiments reveal that similarity with teacher
architecture does not necessarily lead to better performance. We further
discuss the efficiency and behaviors in the KD process and inference, summarize
the implications of findings, and identify promising future directions.

</details>


### [4] [SynthCoder: A Synthetical Strategy to Tune LLMs for Code Completion](https://arxiv.org/abs/2508.15495)
*Dongjun Yu,Xiao Yan,Zhenrui Li,Jipeng Xiao,Haochuan He,Yongda Yu,Hao Zhang,Guoping Rong,Xiaobo Huang*

Main category: cs.SE

TL;DR: SynthCoder是一个针对代码补全任务优化的LLM模型，通过AST节点提取、BM25算法和调用图构建多样化数据集，采用课程学习和DPO两阶段训练，在多个主流代码补全基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有代码补全模型存在性能不稳定问题，在某些数据集上提升性能的同时在其他数据集上性能下降，甚至低于基线模型。需要一种能够稳定提升代码补全性能的方法。

Method: 1) 结合AST节点提取和启发式方法构建多样化数据集；2) 使用BM25算法和调用图丰富跨文件上下文信息；3) 以Seed-Coder-8B-Base为基础模型，采用课程学习进行微调；4) 使用拒绝采样生成偏好对，通过DPO进行对齐训练。

Result: 在aiXcoder、ExecRepoBench、CrossCodeEval和CoLT等主流仓库级代码补全基准测试中表现优异，有效缓解了模型重复现有代码的常见问题。

Conclusion: SynthCoder通过精心设计的数据集构建和两阶段训练策略，在代码补全任务上实现了稳定且优异的性能，解决了现有方法的性能波动问题。

Abstract: Code completion is a prominent application of Large Language Models (LLMs) in
software engineering. Due to the near real-time response requirements of this
task, base models with small to medium-sized parameters are typically employed,
supplemented by various optimization and post-training techniques. However,
these optimization methods often have trade-offs, leading to a seesaw effect
where performance improvements on certain datasets or metrics are accompanied
by degradations on others -- sometimes even falling below the baseline model's
performance. This paper proposes SynthCoder, a model that integrates leading
industry practices to achieve state-of-the-art performance on the
Fill-in-the-Middle (FIM) code completion task. In specific, we first construct
a diverse dataset by combining Abstract Syntax Tree (AST) node extraction with
heuristics that simulate developer behavior. Then we enrich our training corpus
with cross-file contextual information using the BM25 algorithm and call
graphs, enhancing the model's ability to perform code completion in both
file-level and repository-level scenarios. As the last step, we employ a
two-stage training process using the Seed-Coder-8B-Base as the base model.
First, we fine-tune the model using Curriculum Learning technology. Following
this, we perform alignment using Direct Preference Optimization (DPO) with
preference pairs generated through Rejection Sampling. Experimental results
demonstrate that our final model excels on mainstream repository-level code
completion benchmarks, including aiXcoder, ExecRepoBench, CrossCodeEval, and
CoLT. Furthermore, our carefully curated training set effectively mitigates the
model's tendency to just repeat existing code, a common issue existing in
various code completion models.

</details>


### [5] [Towards the Assessment of Task-based Chatbots: From the TOFU-R Snapshot to the BRASATO Curated Dataset](https://arxiv.org/abs/2508.15496)
*Elena Masserini,Diego Clerissi,Daniela Micucci,João R. Campos,Leonardo Mariani*

Main category: cs.SE

TL;DR: 本文提出了两个数据集TOFU-R和BRASATO，用于支持任务型聊天机器人的可靠性、安全性和鲁棒性评估研究。


<details>
  <summary>Details</summary>
Motivation: 当前任务型聊天机器人质量评估缺乏大规模高质量数据集，现有评估技术依赖有限样本或过时代理，难以有效评估技术效果。

Method: 创建了两个数据集：TOFU-R是从GitHub收集的Rasa聊天机器人快照，代表开源实践；BRASATO是精选的具有对话复杂性、功能复杂性和实用性的聊天机器人集合。

Result: 提供了数据集和相应的工具支持，便于创建和维护这些数据集，为聊天机器人可靠性研究提供基础资源。

Conclusion: 这些数据集有助于促进聊天机器人可靠性研究的可重复性，解决当前评估数据不足的问题。

Abstract: Task-based chatbots are increasingly being used to deliver real services, yet
assessing their reliability, security, and robustness remains underexplored,
also due to the lack of large-scale, high-quality datasets. The emerging
automated quality assessment techniques targeting chatbots often rely on
limited pools of subjects, such as custom-made toy examples, or outdated, no
longer available, or scarcely popular agents, complicating the evaluation of
such techniques. In this paper, we present two datasets and the tool support
necessary to create and maintain these datasets. The first dataset is RASA
TASK-BASED CHATBOTS FROM GITHUB (TOFU-R), which is a snapshot of the Rasa
chatbots available on GitHub, representing the state of the practice in
open-source chatbot development with Rasa. The second dataset is BOT RASA
COLLECTION (BRASATO), a curated selection of the most relevant chatbots for
dialogue complexity, functional complexity, and utility, whose goal is to ease
reproducibility and facilitate research on chatbot reliability.

</details>


### [6] [Evaluation Guidelines for Empirical Studies in Software Engineering involving LLMs](https://arxiv.org/abs/2508.15503)
*Sebastian Baltes,Florian Angermeir,Chetan Arora,Marvin Muñoz Barón,Chunyang Chen,Lukas Böhme,Fabio Calefato,Neil Ernst,Davide Falessi,Brian Fitzgerald,Davide Fucci,Marcos Kalinowski,Stefano Lambiase,Daniel Russo,Mircea Lungu,Lutz Prechelt,Paul Ralph,Christoph Treude,Stefan Wagner*

Main category: cs.SE

TL;DR: 该论文提出了针对LLM在软件工程研究中使用的分类法和8条指南，旨在解决LLM的非确定性、训练数据不透明等问题，促进研究的可复现性。


<details>
  <summary>Details</summary>
Motivation: LLM越来越多地应用于软件工程研究和实践，但其非确定性、训练数据不透明和架构不断演进等特点，给实证研究的复现和复制带来了挑战。

Method: 提出了LLM研究的分类法，并制定了8条设计报告指南，包括必须和期望标准，涵盖模型声明、配置报告、工具架构文档、提示披露、人工验证等方面。

Result: 建立了全面的LLM研究指南框架，为社区提供了可在线访问的活资源(llm-guidelines.org)，促进研究的透明度和可复现性。

Conclusion: 通过系统化的指南和分类法，能够克服LLM特有的开放科学障碍，实现更好的研究复现性和可复制性，为软件工程领域的LLM研究提供标准化框架。

Abstract: Large language models (LLMs) are increasingly being integrated into software
engineering (SE) research and practice, yet their non-determinism, opaque
training data, and evolving architectures complicate the reproduction and
replication of empirical studies. We present a community effort to scope this
space, introducing a taxonomy of LLM-based study types together with eight
guidelines for designing and reporting empirical studies involving LLMs. The
guidelines present essential (must) criteria as well as desired (should)
criteria and target transparency throughout the research process. Our
recommendations, contextualized by our study types, are: (1) to declare LLM
usage and role; (2) to report model versions, configurations, and fine-tuning;
(3) to document tool architectures; (4) to disclose prompts and interaction
logs; (5) to use human validation; (6) to employ an open LLM as a baseline; (7)
to report suitable baselines, benchmarks, and metrics; and (8) to openly
articulate limitations and mitigations. Our goal is to enable reproducibility
and replicability despite LLM-specific barriers to open science. We maintain
the study types and guidelines online as a living resource for the community to
use and shape (llm-guidelines.org).

</details>


### [7] [QUPER-MAn: Benchmark-Guided Target Setting for Maintainability Requirements](https://arxiv.org/abs/2508.15512)
*Markus Borg,Martin Larsson,Philip Breid,Nadim Hagatulah*

Main category: cs.SE

TL;DR: QUPER-MAn模型将可维护性从被忽视的开发后果转变为主动管理的目标，通过基准测试和目标设定来支持需求工程


<details>
  <summary>Details</summary>
Motivation: 可维护性在软件开发中经常被忽视，尽管其重要性已被广泛认可。需求工程可以通过促进讨论和设定适当目标来解决这一差距

Method: 采用设计科学研究方法，开发了QUPER-MAn模型（QUPER模型的可维护性适配版本），整合可维护性基准并支持目标设定

Result: 研究发现可维护性仍然是次要的质量关注点，明确的需求通常只泛泛提及编码规范，工具提供的可维护性代理指标通常只用于工程实践的隐式需求

Conclusion: QUPER-MAn模型能够通过知情和负责任的工程决策，将可维护性从被忽视的开发后果转变为主动管理的目标

Abstract: Maintainable source code is essential for sustainable development in any
software organization. Unfortunately, many studies show that maintainability
often receives less attention than its importance warrants. We argue that
requirements engineering can address this gap the problem by fostering
discussions and setting appropriate targets in a responsible manner. In this
preliminary work, we conducted an exploratory study of industry practices
related to requirements engineering for maintainability. Our findings confirm
previous studies: maintainability remains a second-class quality concern.
Explicit requirements often make sweeping references to coding conventions.
Tools providing maintainability proxies are common but typically only used in
implicit requirements related to engineering practices. To address this, we
propose QUPER-MAn, a maintainability adaption of the QUPER model, which was
originally developed to help organizations set targets for performance
requirements. Developed using a design science approach, QUPER-MAn, integrates
maintainability benchmarks and supports target setting. We posit that it can
shift maintainability from an overlooked development consequence to an actively
managed goal driven by informed and responsible engineering decisions.

</details>


### [8] [A Novel Mutation Based Method for Detecting FPGA Logic Synthesis Tool Bugs](https://arxiv.org/abs/2508.15536)
*Yi Zhang,He Jiang,Xiaochen Li,Shikai Guo,Peiyu Zou,Zun Wang*

Main category: cs.SE

TL;DR: VERMEI是一种测试FPGA逻辑综合工具的新方法，通过识别僵尸逻辑、等效变异和差分测试来发现工具缺陷，在5个月内发现了15个bug


<details>
  <summary>Details</summary>
Motivation: FPGA逻辑综合工具中的缺陷可能导致意外行为和安全隐患，现有测试方法存在测试程序语义和逻辑复杂性不足的问题

Method: 包含三个模块：预处理模块通过仿真和覆盖率分析识别种子程序中的僵尸逻辑；等效变异模块通过贝叶斯采样从历史Verilog设计中提取逻辑片段，在僵尸区域修剪或插入逻辑片段生成等效变体；缺陷识别模块基于差分测试比较种子程序和变体程序的综合输出来识别bug

Result: 在Yosys、Vivado和Quartus上的实验表明VERMEI优于最先进方法，5个月内向供应商报告了15个bug，其中9个被确认为新bug

Conclusion: VERMEI通过生成具有复杂控制流和结构的测试程序，有效提高了FPGA逻辑综合工具的测试效果，能够发现更多工具缺陷

Abstract: FPGA (Field-Programmable Gate Array) logic synthesis tools are key components
in the EDA (Electronic Design Automation) toolchain. They convert hardware
designs written in description languages such as Verilog into gate-level
representations for FPGAs. However, defects in these tools may lead to
unexpected behaviors and pose security risks. Therefore, it is crucial to
harden these tools through testing. Although several methods have been proposed
to automatically test FPGA logic synthesis tools, the challenge remains of
insufficient semantic and logical complexity in test programs. In this paper,
we propose VERMEI, a new method for testing FPGA logic synthesis tools. VERMEI
consists of three modules: preprocessing, equivalent mutation, and bug
identification. The preprocessing module identifies zombie logic (inactive code
with no impact on the circuit output) in seed programs through simulation and
coverage analysis. The equivalent mutation module generates equivalent variants
of seed programs by pruning or inserting logic fragments in zombie areas. It
uses Bayesian sampling to extract logic fragments from historical Verilog
designs, making the generated variants have complex control flows and
structures. The bug identification module, based on differential testing,
compares the synthesized outputs of seed and variant programs to identify bugs.
Experiments on Yosys, Vivado, and Quartus demonstrate that VERMEI outperforms
the state-of-the-art methods. Within five months, VERMEI reported 15 bugs to
vendors, 9 of which were confirmed as new.

</details>


### [9] [Establishing Technical Debt Management -- A Five-Step Workshop Approach and an Action Research Study](https://arxiv.org/abs/2508.15570)
*Marion Wiese,Kamila Serwa,Anastasia Besier,Ariane S. Marion-Jetten,Eva Bittner*

Main category: cs.SE

TL;DR: 本研究通过行动研究在IT公司建立技术债务管理流程，发现实践者偏好基于系统演化和成本计算的债务偿还策略，通过待办事项提醒可有效提升技术债务意识。


<details>
  <summary>Details</summary>
Motivation: 技术债务管理在研究中频繁讨论但实践中很少采用，需要建立可行的技术债务管理流程并分析其对技术债务意识的长期影响。

Method: 采用行动研究方法（16个月内5个行动周期），通过问卷分析、团队会议观察、TD-SAGAT心理学方法和待办事项数据分析来评估技术债务意识。

Result: 实践者偏好偿还低垂果实的技术债务，待办事项中的提醒机制（如复选框和文本模板）可持续提升技术债务意识。

Conclusion: 基于工作坊的方法是可行的，能带来可持续的流程变革，并产生了适用于其他IT团队的新技术债务管理思路。

Abstract: Context. Technical debt (TD) items are constructs in a software system
providing short-term benefits but hindering future changes. TD management (TDM)
is frequently researched but rarely adopted in practice. Goal. This study aimed
to establish a TDM process in an IT company based on a predefined workshop
concept. We analyzed which research approaches practitioners adopted for each
TD activity and the TDM's long-term effect on TD awareness. Method. We used
action research (five action cycles in 16 months) with an IT team that creates
IT solutions for signal processing. To examine TD awareness, we (1) analyzed
questionnaires completed during each workshop, (2) observed team meetings, (3)
adopted a method from psychology for measuring awareness in decision-making
situations called TD-SAGAT, and (4) evaluated the backlog data. Results.
Practitioners preferred TD repayment and prioritization based on the system's
evolution and cost calculations, i.e., repayment of so-called low-hanging
fruits. Reminders in the backlog items, such as checkboxes or text templates,
led to a sustainable rise in TD awareness. Conclusions. We showed that a
workshop-based approach is feasible and leads to sustainable process changes.
New ideas for TDM applicable to other IT teams emerged, e.g., using a
re-submission date, using a Talked about TD checkbox, and using visualizations
for TD prioritization.

</details>


### [10] [From PREVENTion to REACTion: Enhancing Failure Resolution in Naval Systems](https://arxiv.org/abs/2508.15584)
*Maria Teresa Rossi,Leonardo Mariani,Oliviero Riganelli*

Main category: cs.SE

TL;DR: 将PREVENT故障预测方法和REACT故障排除模块应用于海军系统的经验总结，展示了异常检测与故障排除的集成方法


<details>
  <summary>Details</summary>
Motivation: 复杂大型工业系统经常因磨损、误用或故障而出现异常行为，需要及时检测、定位问题源并实施适当对策

Method: 应用最先进的故障预测方法PREVENT及其扩展的故障排除模块REACT，集成异常检测与故障排除程序

Result: 成功将方法应用于Fincantieri开发的海军系统，展示了集成方法的有效性

Conclusion: 总结了部署经验教训，有助于将这些分析方法扩展到其他工业产品

Abstract: Complex and large industrial systems often misbehave, for instance, due to
wear, misuse, or faults. To cope with these incidents, it is important to
timely detect their occurrences, localize the sources of the problems, and
implement the appropriate countermeasures. This paper reports our experience
with a state-of-the-art failure prediction method, PREVENT, and its extension
with a troubleshooting module, REACT, applied to naval systems developed by
Fincantieri. Our results show how to integrate anomaly detection with
troubleshooting procedures. We conclude by discussing a lesson learned, which
may help deploy and extend these analyses to other industrial products.

</details>


### [11] [On the need to perform comprehensive evaluations of automated program repair benchmarks: Sorald case study](https://arxiv.org/abs/2508.15135)
*Sumudu Liyanage,Sherlock A. Licorish,Markus Wagner,Stephen G. MacDonell*

Main category: cs.SE

TL;DR: 本研究开发了一个全面的自动程序修复(APR)工具评估框架，并以Sorald工具为例进行评估，发现虽然能修复特定规则违规，但会引入新故障、降低功能正确性和代码结构质量。


<details>
  <summary>Details</summary>
Motivation: 现有研究仅评估APR工具清除违规的能力，忽视了其可能引入新违规、改变代码功能和降低代码结构质量的问题，需要开发全面的评估框架。

Method: 以Sorald工具为案例研究，评估其修复3,529个SonarQube违规的能力，分析2,393个Java代码片段，检查新引入的故障、功能正确性和代码结构变化。

Result: Sorald修复了特定规则违规，但引入了2,120个新故障(32个bug，2,088个代码异味)，单元测试失败率达24%，代码结构质量下降。

Conclusion: 需要开发能够全面捕捉APR工具所有影响的评估方法，包括副作用，以确保其安全有效的采用。

Abstract: In supporting the development of high-quality software, especially necessary
in the era of LLMs, automated program repair (APR) tools aim to improve code
quality by automatically addressing violations detected by static analysis
profilers. Previous research tends to evaluate APR tools only for their ability
to clear violations, neglecting their potential introduction of new (sometimes
severe) violations, changes to code functionality and degrading of code
structure. There is thus a need for research to develop and assess
comprehensive evaluation frameworks for APR tools. This study addresses this
research gap, and evaluates Sorald (a state-of-the-art APR tool) as a proof of
concept. Sorald's effectiveness was evaluated in repairing 3,529 SonarQube
violations across 30 rules within 2,393 Java code snippets extracted from Stack
Overflow. Outcomes show that while Sorald fixes specific rule violations, it
introduced 2,120 new faults (32 bugs, 2088 code smells), reduced code
functional correctness--as evidenced by a 24% unit test failure rate--and
degraded code structure, demonstrating the utility of our framework. Findings
emphasize the need for evaluation methodologies that capture the full spectrum
of APR tool effects, including side effects, to ensure their safe and effective
adoption.

</details>


### [12] [Foundational Design Principles and Patterns for Building Robust and Adaptive GenAI-Native Systems](https://arxiv.org/abs/2508.15411)
*Frederik Vandeputte*

Main category: cs.SE

TL;DR: 本文提出GenAI原生系统设计范式，将生成式AI与传统软件工程结合，建立可靠、自适应、高效的系统架构。


<details>
  <summary>Details</summary>
Motivation: 生成式AI虽然具有强大能力，但其不可预测性和低效性限制了可靠系统的开发，需要新的设计方法来整合AI认知能力与软件工程原则。

Method: 提出基于五个关键支柱（可靠性、卓越性、可进化性、自依赖性、保障性）的设计原则，以及GenAI原生单元、有机基底、可编程路由器等架构模式。

Result: 构建了GenAI原生软件栈的关键组件框架，为创建弹性和自进化系统提供了理论指导。

Conclusion: 该框架需要进一步验证和实验，旨在激发未来研究并推动相关社区实施和完善这一概念体系。

Abstract: Generative AI (GenAI) has emerged as a transformative technology,
demonstrating remarkable capabilities across diverse application domains.
However, GenAI faces several major challenges in developing reliable and
efficient GenAI-empowered systems due to its unpredictability and inefficiency.
This paper advocates for a paradigm shift: future GenAI-native systems should
integrate GenAI's cognitive capabilities with traditional software engineering
principles to create robust, adaptive, and efficient systems.
  We introduce foundational GenAI-native design principles centered around five
key pillars -- reliability, excellence, evolvability, self-reliance, and
assurance -- and propose architectural patterns such as GenAI-native cells,
organic substrates, and programmable routers to guide the creation of resilient
and self-evolving systems. Additionally, we outline the key ingredients of a
GenAI-native software stack and discuss the impact of these systems from
technical, user adoption, economic, and legal perspectives, underscoring the
need for further validation and experimentation. Our work aims to inspire
future research and encourage relevant communities to implement and refine this
conceptual framework.

</details>


### [13] [An Empirical Study of Knowledge Distillation for Code Understanding Tasks](https://arxiv.org/abs/2508.15423)
*Ruiqi Wang,Zezhou Yang,Cuiyun Gao,Xin Xia,Qing Liao*

Main category: cs.SE

TL;DR: 本文系统研究了知识蒸馏在代码理解任务中的有效性，发现特征蒸馏方法效果最佳，学生模型仅需5%参数即可达到教师模型98%的性能


<details>
  <summary>Details</summary>
Motivation: 预训练语言模型在代码理解中表现出色但计算密集，知识蒸馏技术可解决部署时的计算和延迟问题，但在代码理解领域的应用尚未充分探索

Method: 研究两种知识蒸馏方法（基于logit和基于特征），在8个学生模型和2个教师PLM上进行实验，涵盖三个下游任务

Result: 知识蒸馏相比标准微调能显著提升各种规模学生模型的性能，代码专用PLM作为教师效果更好，特征蒸馏方法表现最优

Conclusion: 知识蒸馏在代码理解任务中非常有效，特别是特征蒸馏方法，学生架构与教师相似性不一定带来更好性能，为高效代码理解模型部署提供了重要见解

Abstract: Pre-trained language models (PLMs) have emerged as powerful tools for code
understanding. However, deploying these PLMs in large-scale applications faces
practical challenges due to their computational intensity and inference
latency. Knowledge distillation (KD), a promising model compression and
acceleration technique, addresses these limitations by transferring knowledge
from large teacher models to compact student models, enabling efficient
inference while preserving most of the teacher models' capabilities. While this
technique has shown remarkable success in natural language processing and
computer vision domains, its potential for code understanding tasks remains
largely underexplored.
  In this paper, we systematically investigate the effectiveness and usage of
KD in code understanding tasks. Our study encompasses two popular types of KD
methods, i.e., logit-based and feature-based KD methods, experimenting across
eight student models and two teacher PLMs from different domains on three
downstream tasks. The experimental results indicate that KD consistently offers
notable performance boosts across student models with different sizes compared
with standard fine-tuning. Notably, code-specific PLM demonstrates better
effectiveness as the teacher model. Among all KD methods, the latest
feature-based KD methods exhibit superior performance, enabling student models
to retain up to 98% teacher performance with merely 5% parameters. Regarding
student architecture, our experiments reveal that similarity with teacher
architecture does not necessarily lead to better performance. We further
discuss the efficiency and behaviors in the KD process and inference, summarize
the implications of findings, and identify promising future directions.

</details>


### [14] [SynthCoder: A Synthetical Strategy to Tune LLMs for Code Completion](https://arxiv.org/abs/2508.15495)
*Dongjun Yu,Xiao Yan,Zhenrui Li,Jipeng Xiao,Haochuan He,Yongda Yu,Hao Zhang,Guoping Rong,Xiaobo Huang*

Main category: cs.SE

TL;DR: SynthCoder是一个集成行业最佳实践的代码补全模型，通过AST节点提取、BM25算法和调用图构建多样化数据集，采用课程学习和DPO两阶段训练，在多个代码补全基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有代码补全模型的优化方法存在权衡问题，在某些数据集或指标上性能提升的同时在其他方面出现性能下降，甚至低于基线模型性能。需要一种能够避免这种seesaw效应的方法。

Method: 1) 结合AST节点提取和启发式方法构建多样化数据集；2) 使用BM25算法和调用图丰富跨文件上下文信息；3) 以Seed-Coder-8B-Base为基础模型，采用课程学习进行微调；4) 使用拒绝采样生成的偏好对进行DPO对齐训练

Result: 在主流仓库级代码补全基准测试（aiXcoder、ExecRepoBench、CrossCodeEval、CoLT）上表现优异，有效缓解了模型重复现有代码的常见问题

Conclusion: SynthCoder通过精心设计的训练集和两阶段训练过程，成功解决了代码补全模型优化中的seesaw效应问题，实现了state-of-the-art性能

Abstract: Code completion is a prominent application of Large Language Models (LLMs) in
software engineering. Due to the near real-time response requirements of this
task, base models with small to medium-sized parameters are typically employed,
supplemented by various optimization and post-training techniques. However,
these optimization methods often have trade-offs, leading to a seesaw effect
where performance improvements on certain datasets or metrics are accompanied
by degradations on others -- sometimes even falling below the baseline model's
performance. This paper proposes SynthCoder, a model that integrates leading
industry practices to achieve state-of-the-art performance on the
Fill-in-the-Middle (FIM) code completion task. In specific, we first construct
a diverse dataset by combining Abstract Syntax Tree (AST) node extraction with
heuristics that simulate developer behavior. Then we enrich our training corpus
with cross-file contextual information using the BM25 algorithm and call
graphs, enhancing the model's ability to perform code completion in both
file-level and repository-level scenarios. As the last step, we employ a
two-stage training process using the Seed-Coder-8B-Base as the base model.
First, we fine-tune the model using Curriculum Learning technology. Following
this, we perform alignment using Direct Preference Optimization (DPO) with
preference pairs generated through Rejection Sampling. Experimental results
demonstrate that our final model excels on mainstream repository-level code
completion benchmarks, including aiXcoder, ExecRepoBench, CrossCodeEval, and
CoLT. Furthermore, our carefully curated training set effectively mitigates the
model's tendency to just repeat existing code, a common issue existing in
various code completion models.

</details>


### [15] [Towards the Assessment of Task-based Chatbots: From the TOFU-R Snapshot to the BRASATO Curated Dataset](https://arxiv.org/abs/2508.15496)
*Elena Masserini,Diego Clerissi,Daniela Micucci,João R. Campos,Leonardo Mariani*

Main category: cs.SE

TL;DR: 本文提出了两个数据集和工具支持来评估基于任务的聊天机器人可靠性：TOFU-R（GitHub上的Rasa聊天机器人快照）和BRASATO（精选的相关聊天机器人集合）。


<details>
  <summary>Details</summary>
Motivation: 基于任务的聊天机器人应用日益广泛，但评估其可靠性、安全性和鲁棒性仍缺乏大规模高质量数据集，现有评估技术往往依赖有限样本或过时代理。

Method: 创建了两个数据集：TOFU-R是从GitHub收集的Rasa聊天机器人快照，代表开源实践现状；BRASATO是经过筛选的相关聊天机器人集合，基于对话复杂性、功能复杂性和实用性标准。

Result: 提供了可用于聊天机器人可靠性研究的数据集和工具支持，解决了现有评估技术缺乏大规模高质量数据集的问题。

Conclusion: 这些数据集和工具支持有助于促进聊天机器人可靠性研究，提高评估技术的可重复性和研究效率。

Abstract: Task-based chatbots are increasingly being used to deliver real services, yet
assessing their reliability, security, and robustness remains underexplored,
also due to the lack of large-scale, high-quality datasets. The emerging
automated quality assessment techniques targeting chatbots often rely on
limited pools of subjects, such as custom-made toy examples, or outdated, no
longer available, or scarcely popular agents, complicating the evaluation of
such techniques. In this paper, we present two datasets and the tool support
necessary to create and maintain these datasets. The first dataset is RASA
TASK-BASED CHATBOTS FROM GITHUB (TOFU-R), which is a snapshot of the Rasa
chatbots available on GitHub, representing the state of the practice in
open-source chatbot development with Rasa. The second dataset is BOT RASA
COLLECTION (BRASATO), a curated selection of the most relevant chatbots for
dialogue complexity, functional complexity, and utility, whose goal is to ease
reproducibility and facilitate research on chatbot reliability.

</details>


### [16] [Evaluation Guidelines for Empirical Studies in Software Engineering involving LLMs](https://arxiv.org/abs/2508.15503)
*Sebastian Baltes,Florian Angermeir,Chetan Arora,Marvin Muñoz Barón,Chunyang Chen,Lukas Böhme,Fabio Calefato,Neil Ernst,Davide Falessi,Brian Fitzgerald,Davide Fucci,Marcos Kalinowski,Stefano Lambiase,Daniel Russo,Mircea Lungu,Lutz Prechelt,Paul Ralph,Christoph Treude,Stefan Wagner*

Main category: cs.SE

TL;DR: 这篇论文提出了一套用于设计和报告涉及大语言模型的实证研究的指南，以解决LLM的非确定性、不透明训练数据和演进架构带来的可复现性挑战。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型日益深入软件工程研究和实践，其非确定性、不透明的训练数据和不断演进的架构为实证研究的可复现性和可重复性带来了挑战。

Method: 通过社区公共努力，建立了基于LLM的研究类型分类法，并提出了8项设计和报告实证研究的指南，包括声明LLM使用情况、报告模型配置、文档化工具架构、公开提示和交互日志等。

Result: 提出了一套包含必需标准和期望标准的指南，目标是在整个研究过程中实现透明度，包括声明LLM使用、报告模型配置、文档化工具架构、公开提示和交互日志、使用人工验证、采用开源LLM作为基准等。

Conclusion: 这些指南能够帮助充分应对LLM特有的开放科学障碍，从而实现研究的可复现性和可重复性。该研究类型和指南已作为活跃资源在线维护，供社区使用和修订。

Abstract: Large language models (LLMs) are increasingly being integrated into software
engineering (SE) research and practice, yet their non-determinism, opaque
training data, and evolving architectures complicate the reproduction and
replication of empirical studies. We present a community effort to scope this
space, introducing a taxonomy of LLM-based study types together with eight
guidelines for designing and reporting empirical studies involving LLMs. The
guidelines present essential (must) criteria as well as desired (should)
criteria and target transparency throughout the research process. Our
recommendations, contextualized by our study types, are: (1) to declare LLM
usage and role; (2) to report model versions, configurations, and fine-tuning;
(3) to document tool architectures; (4) to disclose prompts and interaction
logs; (5) to use human validation; (6) to employ an open LLM as a baseline; (7)
to report suitable baselines, benchmarks, and metrics; and (8) to openly
articulate limitations and mitigations. Our goal is to enable reproducibility
and replicability despite LLM-specific barriers to open science. We maintain
the study types and guidelines online as a living resource for the community to
use and shape (llm-guidelines.org).

</details>


### [17] [QUPER-MAn: Benchmark-Guided Target Setting for Maintainability Requirements](https://arxiv.org/abs/2508.15512)
*Markus Borg,Martin Larsson,Philip Breid,Nadim Hagatulah*

Main category: cs.SE

TL;DR: QUPER-MAn模型将可维护性从被忽视的开发后果转变为主动管理的目标，通过基准测试和目标设定支持基于信息的工程决策


<details>
  <summary>Details</summary>
Motivation: 可维护性源代码对软件开发可持续性至关重要，但实践中往往被忽视。需求工程可以通过促进讨论和设定适当目标来解决这一问题

Method: 采用设计科学研究方法，开发了QUPER-MAn模型（QUPER模型的可维护性适配版本），整合可维护性基准测试并支持目标设定

Result: 研究发现可维护性仍然是次要质量关注点，明确需求通常只泛泛引用编码规范，工具提供的可维护性代理指标通常只用于隐性需求

Conclusion: QUPER-MAn模型能够帮助组织将可维护性从被忽视的开发后果转变为由知情和负责任的工程决策驱动的主动管理目标

Abstract: Maintainable source code is essential for sustainable development in any
software organization. Unfortunately, many studies show that maintainability
often receives less attention than its importance warrants. We argue that
requirements engineering can address this gap the problem by fostering
discussions and setting appropriate targets in a responsible manner. In this
preliminary work, we conducted an exploratory study of industry practices
related to requirements engineering for maintainability. Our findings confirm
previous studies: maintainability remains a second-class quality concern.
Explicit requirements often make sweeping references to coding conventions.
Tools providing maintainability proxies are common but typically only used in
implicit requirements related to engineering practices. To address this, we
propose QUPER-MAn, a maintainability adaption of the QUPER model, which was
originally developed to help organizations set targets for performance
requirements. Developed using a design science approach, QUPER-MAn, integrates
maintainability benchmarks and supports target setting. We posit that it can
shift maintainability from an overlooked development consequence to an actively
managed goal driven by informed and responsible engineering decisions.

</details>


### [18] [A Novel Mutation Based Method for Detecting FPGA Logic Synthesis Tool Bugs](https://arxiv.org/abs/2508.15536)
*Yi Zhang,He Jiang,Xiaochen Li,Shikai Guo,Peiyu Zou,Zun Wang*

Main category: cs.SE

TL;DR: VERMEI是一种测试FPGA逻辑综合工具的新方法，通过识别僵尸逻辑、等效变异和差异测试来发现工具缺陷，在实验中发现了15个bug


<details>
  <summary>Details</summary>
Motivation: FPGA逻辑综合工具中的缺陷可能导致意外行为和安全隐患，现有测试方法存在测试程序语义和逻辑复杂性不足的问题

Method: 包含预处理、等效变异和bug识别三个模块：预处理通过仿真和覆盖率分析识别僵尸逻辑；等效变异通过贝叶斯采样从历史Verilog设计中提取逻辑片段，在僵尸区域修剪或插入逻辑片段生成等效变体；bug识别基于差异测试比较种子程序和变体程序的综合输出

Result: 在Yosys、Vivado和Quartus上的实验表明VERMEI优于现有方法，5个月内向供应商报告了15个bug，其中9个被确认为新bug

Conclusion: VERMEI能有效测试FPGA逻辑综合工具，生成具有复杂控制流和结构的测试程序，显著提高了bug检测能力

Abstract: FPGA (Field-Programmable Gate Array) logic synthesis tools are key components
in the EDA (Electronic Design Automation) toolchain. They convert hardware
designs written in description languages such as Verilog into gate-level
representations for FPGAs. However, defects in these tools may lead to
unexpected behaviors and pose security risks. Therefore, it is crucial to
harden these tools through testing. Although several methods have been proposed
to automatically test FPGA logic synthesis tools, the challenge remains of
insufficient semantic and logical complexity in test programs. In this paper,
we propose VERMEI, a new method for testing FPGA logic synthesis tools. VERMEI
consists of three modules: preprocessing, equivalent mutation, and bug
identification. The preprocessing module identifies zombie logic (inactive code
with no impact on the circuit output) in seed programs through simulation and
coverage analysis. The equivalent mutation module generates equivalent variants
of seed programs by pruning or inserting logic fragments in zombie areas. It
uses Bayesian sampling to extract logic fragments from historical Verilog
designs, making the generated variants have complex control flows and
structures. The bug identification module, based on differential testing,
compares the synthesized outputs of seed and variant programs to identify bugs.
Experiments on Yosys, Vivado, and Quartus demonstrate that VERMEI outperforms
the state-of-the-art methods. Within five months, VERMEI reported 15 bugs to
vendors, 9 of which were confirmed as new.

</details>


### [19] [Establishing Technical Debt Management -- A Five-Step Workshop Approach and an Action Research Study](https://arxiv.org/abs/2508.15570)
*Marion Wiese,Kamila Serwa,Anastasia Besier,Ariane S. Marion-Jetten,Eva Bittner*

Main category: cs.SE

TL;DR: 通过动作研究在IT公司建立技术债务管理过程，发现工作坊方法可行且能提高TD意识，实践者偏好优先处理低合成本的"低垂果"


<details>
  <summary>Details</summary>
Motivation: 技术债务管理研究多但实践应用少，需要在真实IT环境中建立可持续的TDM过程

Method: 采用动作研究方法（16个月五个循环），通过问卷调查、团队会议观察、TD-SAGAT意识测量和背洋数据分析来评估TD意识

Result: 实践者偏好基于系统进化和成本计算的优先级订立，背洋项目中的提醒机制可持续提高TD意识

Conclusion: 工作坊基于方法可行且能导致可持续的过程改变，提出了重提交日期、"讨论过TD"复选框等新的TDM方法

Abstract: Context. Technical debt (TD) items are constructs in a software system
providing short-term benefits but hindering future changes. TD management (TDM)
is frequently researched but rarely adopted in practice. Goal. This study aimed
to establish a TDM process in an IT company based on a predefined workshop
concept. We analyzed which research approaches practitioners adopted for each
TD activity and the TDM's long-term effect on TD awareness. Method. We used
action research (five action cycles in 16 months) with an IT team that creates
IT solutions for signal processing. To examine TD awareness, we (1) analyzed
questionnaires completed during each workshop, (2) observed team meetings, (3)
adopted a method from psychology for measuring awareness in decision-making
situations called TD-SAGAT, and (4) evaluated the backlog data. Results.
Practitioners preferred TD repayment and prioritization based on the system's
evolution and cost calculations, i.e., repayment of so-called low-hanging
fruits. Reminders in the backlog items, such as checkboxes or text templates,
led to a sustainable rise in TD awareness. Conclusions. We showed that a
workshop-based approach is feasible and leads to sustainable process changes.
New ideas for TDM applicable to other IT teams emerged, e.g., using a
re-submission date, using a Talked about TD checkbox, and using visualizations
for TD prioritization.

</details>


### [20] [From PREVENTion to REACTion: Enhancing Failure Resolution in Naval Systems](https://arxiv.org/abs/2508.15584)
*Maria Teresa Rossi,Leonardo Mariani,Oliviero Riganelli*

Main category: cs.SE

TL;DR: 本文报告了在Fincantieri海军系统中应用PREVENT故障预测方法和REACT故障排除模块的经验，展示了异常检测与故障排除的集成方法。


<details>
  <summary>Details</summary>
Motivation: 复杂大型工业系统经常因磨损、误用或故障而出现异常行为，需要及时检测、定位问题源并实施适当对策。

Method: 应用了最先进的故障预测方法PREVENT及其扩展的故障排除模块REACT，在海军系统中进行集成实施。

Result: 成功展示了如何将异常检测与故障排除程序相结合，为工业系统提供了有效的故障处理解决方案。

Conclusion: 总结了部署经验教训，这些经验有助于将该分析方法扩展到其他工业产品中。

Abstract: Complex and large industrial systems often misbehave, for instance, due to
wear, misuse, or faults. To cope with these incidents, it is important to
timely detect their occurrences, localize the sources of the problems, and
implement the appropriate countermeasures. This paper reports our experience
with a state-of-the-art failure prediction method, PREVENT, and its extension
with a troubleshooting module, REACT, applied to naval systems developed by
Fincantieri. Our results show how to integrate anomaly detection with
troubleshooting procedures. We conclude by discussing a lesson learned, which
may help deploy and extend these analyses to other industrial products.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [21] [MCPTox: A Benchmark for Tool Poisoning Attack on Real-World MCP Servers](https://arxiv.org/abs/2508.14925)
*Zhiqiang Wang,Yichao Gao,Yanting Wang,Suyuan Liu,Haifeng Sun,Haoran Cheng,Guanquan Shi,Haohua Du,Xiangyang Li*

Main category: cs.CR

TL;DR: MCPTox是首个系统评估LLM智能体在MCP环境中对抗工具中毒攻击的基准测试，发现主流智能体普遍存在漏洞，攻击成功率高达72.8%，现有安全对齐机制对此类攻击无效。


<details>
  <summary>Details</summary>
Motivation: Model Context Protocol (MCP)为LLM智能体提供了与外部工具交互的标准接口，但同时也引入了新的攻击面。现有研究主要关注通过工具输出注入的攻击，而工具中毒攻击（在工具元数据中嵌入恶意指令）这一更基础的威胁缺乏系统性评估。

Method: 基于45个真实MCP服务器和353个真实工具构建MCPTox基准，设计三种攻击模板，通过少样本学习生成1312个恶意测试用例，覆盖10个风险类别，对20个主流LLM智能体进行评估。

Result: 评估显示智能体普遍易受工具中毒攻击，o1-mini攻击成功率高达72.8%。能力更强的模型往往更容易受到攻击，因为攻击利用了其优越的指令遵循能力。现有安全对齐机制几乎无法拒绝此类攻击，最高拒绝率（Claude-3.7-Sonnet）不到3%。

Conclusion: 工具中毒攻击是MCP生态系统中广泛存在的严重威胁，现有安全措施对此无效。MCPTox基准为理解和缓解这一威胁提供了重要的实证基础，并促进可验证安全AI智能体的开发。

Abstract: By providing a standardized interface for LLM agents to interact with
external tools, the Model Context Protocol (MCP) is quickly becoming a
cornerstone of the modern autonomous agent ecosystem. However, it creates novel
attack surfaces due to untrusted external tools. While prior work has focused
on attacks injected through external tool outputs, we investigate a more
fundamental vulnerability: Tool Poisoning, where malicious instructions are
embedded within a tool's metadata without execution. To date, this threat has
been primarily demonstrated through isolated cases, lacking a systematic,
large-scale evaluation.
  We introduce MCPTox, the first benchmark to systematically evaluate agent
robustness against Tool Poisoning in realistic MCP settings. MCPTox is
constructed upon 45 live, real-world MCP servers and 353 authentic tools. To
achieve this, we design three distinct attack templates to generate a
comprehensive suite of 1312 malicious test cases by few-shot learning, covering
10 categories of potential risks. Our evaluation on 20 prominent LLM agents
setting reveals a widespread vulnerability to Tool Poisoning, with o1-mini,
achieving an attack success rate of 72.8\%. We find that more capable models
are often more susceptible, as the attack exploits their superior
instruction-following abilities. Finally, the failure case analysis reveals
that agents rarely refuse these attacks, with the highest refused rate
(Claude-3.7-Sonnet) less than 3\%, demonstrating that existing safety alignment
is ineffective against malicious actions that use legitimate tools for
unauthorized operation. Our findings create a crucial empirical baseline for
understanding and mitigating this widespread threat, and we release MCPTox for
the development of verifiably safer AI agents. Our dataset is available at an
anonymized repository: \textit{https://anonymous.4open.science/r/AAAI26-7C02}.

</details>


### [22] [A Systematic Survey of Model Extraction Attacks and Defenses: State-of-the-Art and Perspectives](https://arxiv.org/abs/2508.15031)
*Kaixiang Zhao,Lincan Li,Kaize Ding,Neil Zhenqiang Gong,Yue Zhao,Yushun Dong*

Main category: cs.CR

TL;DR: 本文对模型提取攻击（MEAs）及其防御策略进行了全面综述，提出了基于攻击机制、防御方法和计算环境的新分类法，分析了各种攻击技术的有效性，并讨论了技术、伦理、法律和社会影响。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习即服务（MLaaS）平台的普及，模型提取攻击对知识产权、隐私和系统安全构成严重威胁，需要系统性的研究和防御策略综述。

Method: 提出新的分类法对MEAs进行分类，涵盖各种攻击技术分析、防御方法评估，并在不同计算范式下进行评估。

Result: 系统分析了MEAs的有效性和现有防御策略面临的挑战，特别是模型效用与安全性之间的关键权衡问题。

Conclusion: 该综述为AI安全和隐私领域的研究者、从业者和政策制定者提供了重要参考，并指出了未来研究的有前景方向。

Abstract: Machine learning (ML) models have significantly grown in complexity and
utility, driving advances across multiple domains. However, substantial
computational resources and specialized expertise have historically restricted
their wide adoption. Machine-Learning-as-a-Service (MLaaS) platforms have
addressed these barriers by providing scalable, convenient, and affordable
access to sophisticated ML models through user-friendly APIs. While this
accessibility promotes widespread use of advanced ML capabilities, it also
introduces vulnerabilities exploited through Model Extraction Attacks (MEAs).
Recent studies have demonstrated that adversaries can systematically replicate
a target model's functionality by interacting with publicly exposed interfaces,
posing threats to intellectual property, privacy, and system security. In this
paper, we offer a comprehensive survey of MEAs and corresponding defense
strategies. We propose a novel taxonomy that classifies MEAs according to
attack mechanisms, defense approaches, and computing environments. Our analysis
covers various attack techniques, evaluates their effectiveness, and highlights
challenges faced by existing defenses, particularly the critical trade-off
between preserving model utility and ensuring security. We further assess MEAs
within different computing paradigms and discuss their technical, ethical,
legal, and societal implications, along with promising directions for future
research. This systematic survey aims to serve as a valuable reference for
researchers, practitioners, and policymakers engaged in AI security and
privacy. Additionally, we maintain an online repository continuously updated
with related literature at https://github.com/kzhao5/ModelExtractionPapers.

</details>


### [23] [MoEcho: Exploiting Side-Channel Attacks to Compromise User Privacy in Mixture-of-Experts LLMs](https://arxiv.org/abs/2508.15036)
*Ruyi Ding,Tianhong Xu,Xinyi Shen,Aidong Adam Ding,Yunsi Fei*

Main category: cs.CR

TL;DR: MoEcho揭示了混合专家(MoE)架构中的侧信道安全漏洞，攻击者可通过硬件执行痕迹推断用户隐私数据，提出了四种新型侧信道攻击方法。


<details>
  <summary>Details</summary>
Motivation: MoE架构通过动态路由实现计算效率，但这种输入依赖的激活模式在硬件层面留下了可被利用的时空痕迹，形成了新的隐私攻击面。

Method: 提出了四种新型架构侧信道：CPU上的缓存占用通道和页出重加载，GPU上的性能计数器和TLB驱逐重加载，并基于这些漏洞设计了四种隐私攻击方法。

Result: 成功在基于MoE的大型语言模型和视觉语言模型上实现了提示推断、响应重构、视觉推断和视觉重构攻击，证明了严重的安全威胁。

Conclusion: 这是首个针对现代Transformer中流行MoE结构的运行时架构级安全分析，揭示了严重的安全隐私威胁，呼吁在开发高效大规模AI服务时需要及时有效的防护措施。

Abstract: The transformer architecture has become a cornerstone of modern AI, fueling
remarkable progress across applications in natural language processing,
computer vision, and multimodal learning. As these models continue to scale
explosively for performance, implementation efficiency remains a critical
challenge. Mixture of Experts (MoE) architectures, selectively activating
specialized subnetworks (experts), offer a unique balance between model
accuracy and computational cost. However, the adaptive routing in MoE
architectures, where input tokens are dynamically directed to specialized
experts based on their semantic meaning inadvertently opens up a new attack
surface for privacy breaches. These input-dependent activation patterns leave
distinctive temporal and spatial traces in hardware execution, which
adversaries could exploit to deduce sensitive user data. In this work, we
propose MoEcho, discovering a side channel analysis based attack surface that
compromises user privacy on MoE based systems. Specifically, in MoEcho, we
introduce four novel architectural side channels on different computing
platforms, including Cache Occupancy Channels and Pageout+Reload on CPUs, and
Performance Counter and TLB Evict+Reload on GPUs, respectively. Exploiting
these vulnerabilities, we propose four attacks that effectively breach user
privacy in large language models (LLMs) and vision language models (VLMs) based
on MoE architectures: Prompt Inference Attack, Response Reconstruction Attack,
Visual Inference Attack, and Visual Reconstruction Attack. MoEcho is the first
runtime architecture level security analysis of the popular MoE structure
common in modern transformers, highlighting a serious security and privacy
threat and calling for effective and timely safeguards when harnessing MoE
based models for developing efficient large scale AI services.

</details>


### [24] [When Machine Learning Meets Vulnerability Discovery: Challenges and Lessons Learned](https://arxiv.org/abs/2508.15042)
*Sima Arasteh,Christophe Hauser*

Main category: cs.CR

TL;DR: 这篇论文探讨了机器学习在软件漏洞检测中的挑战，包括数据集评估不充和模型选择问题，并分享了从Bin2vec和BinHunter研究中获得的见解。


<details>
  <summary>Details</summary>
Motivation: 虽然机器学习在软件漏洞检测方面取得了显著进步，但存在数据集评估不充、训练数据适用性疑问以及模型选择等重要短板，需要探索这些挑战以提高研究质量。

Method: 通过分析现有研究的不足之处，结合作者之前的Bin2vec和BinHunter研究项目经验，提出改进建议和见解。

Result: 识别了当前机器学习漏洞检测领域的关键问题：数据集统计信息缺失、训练数据适用性不确定、模型选择和粒度层面影响效果等。

Conclusion: 该研究为改进机器学习在软件漏洞发现中的应用提供了重要见解，Bin2vec和BinHunter的经验可以推动该领域的未来研究发展。

Abstract: In recent years, machine learning has demonstrated impressive results in
various fields, including software vulnerability detection. Nonetheless, using
machine learning to identify software vulnerabilities presents new challenges,
especially regarding the scale of data involved, which was not a factor in
traditional methods. Consequently, in spite of the rise of new
machine-learning-based approaches in that space, important shortcomings persist
regarding their evaluation. First, researchers often fail to provide concrete
statistics about their training datasets, such as the number of samples for
each type of vulnerability. Moreover, many methods rely on training with
semantically similar functions rather than directly on vulnerable programs.
This leads to uncertainty about the suitability of the datasets currently used
for training. Secondly, the choice of a model and the level of granularity at
which models are trained also affect the effectiveness of such vulnerability
discovery approaches.
  In this paper, we explore the challenges of applying machine learning to
vulnerability discovery. We also share insights from our two previous research
papers, Bin2vec and BinHunter, which could enhance future research in this
field.

</details>


### [25] [Tighter Privacy Analysis for Truncated Poisson Sampling](https://arxiv.org/abs/2508.15089)
*Arun Ganesh*

Main category: cs.CR

TL;DR: 对截断泊松采样的新隐私放大分析


<details>
  <summary>Details</summary>
Motivation: 截断泊松采样是泊松采样的一个变体，当批次大小超过给定最大值时会进行截断处理，需要对其隐私保护效果进行量化分析

Method: 提出新的隐私放大分析方法，针对截断泊松采样机制进行理论分析

Result: 获得了截断泊松采样的隐私放大边界，量化了该机制的隐私保护效果

Conclusion: 该分析为截断泊松采样提供了理论保障，有助于在实际应用中更好地平衡隐私保护和数据效用

Abstract: We give a new privacy amplification analysis for truncated Poisson sampling,
a Poisson sampling variant that truncates a batch if it exceeds a given maximum
batch size.

</details>


### [26] [Adaptive Anomaly Detection in Evolving Network Environments](https://arxiv.org/abs/2508.15100)
*Ehssan Mousavipour,Andrey Dimanchev,Majid Ghaderi*

Main category: cs.CR

TL;DR: NetSight是一个用于网络数据异常检测的监督学习框架，能够在线持续检测和适应分布偏移，无需人工干预，通过伪标签技术和知识蒸馏防止灾难性遗忘，在三个长期网络数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 分布偏移（数据统计特性随时间变化）对深度学习异常检测系统构成重大挑战。现有系统难以适应这种变化：监督学习需要昂贵的人工标注，无监督学习需要难以获取的干净数据进行偏移适应。

Method: NetSight采用新颖的伪标签技术消除人工干预，并使用基于知识蒸馏的适应策略来防止灾难性遗忘，实现在线持续检测和适应分布偏移。

Result: 在三个长期网络数据集上的评估显示，NetSight相比依赖人工标注的最先进方法具有更优的适应性能，F1分数提升高达11.72%。

Conclusion: NetSight证明了其在经历分布偏移的动态网络中具有鲁棒性和有效性，能够有效解决分布偏移带来的挑战。

Abstract: Distribution shift, a change in the statistical properties of data over time,
poses a critical challenge for deep learning anomaly detection systems.
Existing anomaly detection systems often struggle to adapt to these shifts.
Specifically, systems based on supervised learning require costly manual
labeling, while those based on unsupervised learning rely on clean data, which
is difficult to obtain, for shift adaptation. Both of these requirements are
challenging to meet in practice. In this paper, we introduce NetSight, a
framework for supervised anomaly detection in network data that continually
detects and adapts to distribution shifts in an online manner. NetSight
eliminates manual intervention through a novel pseudo-labeling technique and
uses a knowledge distillation-based adaptation strategy to prevent catastrophic
forgetting. Evaluated on three long-term network datasets, NetSight
demonstrates superior adaptation performance compared to state-of-the-art
methods that rely on manual labeling, achieving F1-score improvements of up to
11.72%. This proves its robustness and effectiveness in dynamic networks that
experience distribution shifts over time.

</details>


### [27] [Conditional Cube Attack on Round-Reduced ASCON](https://arxiv.org/abs/2508.15172)
*Zheng Li,Xiaoyang Dong,Xiaoyun Wang*

Main category: cs.CR

TL;DR: 本文评估了认证加密算法Ascon对类立方体攻击的安全性，改进了条件立方体攻击方法，实现了对5/6轮Ascon的实际攻击，并首次提出了7轮密钥恢复攻击


<details>
  <summary>Details</summary>
Motivation: Ascon是CAESAR竞赛第三轮的16个幸存者之一，之前最好的密钥恢复攻击只能达到6轮，而类似结构的Keccak密钥模式攻击可达7轮以上，需要研究Ascon的安全性

Method: 推广条件立方体攻击方法，引入立方体类密钥子集技术，根据密钥条件将密钥空间划分为多个子集，对每个子集使用立方体测试器进行测试

Result: 将6轮攻击的时间复杂度从理论上的2^66降低到实际的2^40，首次实现了7轮密钥恢复攻击，总时间复杂度约为2^103.9，对弱密钥子集(大小2^117)的攻击仅需2^77时间复杂度

Conclusion: 这些攻击不会威胁完整12轮的Ascon，但展示了改进的立方体攻击方法在分析缩减轮次Ascon时的有效性

Abstract: This paper evaluates the secure level of authenticated encryption
\textsc{Ascon} against cube-like method. \textsc{Ascon} submitted by Dobraunig
\emph{et~al.} is one of 16 survivors of the 3rd round CAESAR competition. The
cube-like method is first used by Dinur \emph{et~al.} to analyze Keccak keyed
modes. At CT-RSA 2015, Dobraunig \emph{et~al.} applied this method to 5/6-round
reduced \textsc{Ascon}, whose structure is similar to Keccak keyed modes.
However, for \textsc{Ascon} the non-linear layer is more complex and state is
much smaller, which make it hard for the attackers to select enough cube
variables that do not multiply with each other after the first round. This
seems to be the reason why the best previous key-recovery attack is on 6-round
\textsc{Ascon}, while for Keccak keyed modes (Keccak-MAC and Keyak) the
attacked round is no less than 7-round.
  In this paper, we generalize the conditional cube attack proposed by Huang
\emph{et~al.}, and find new cubes depending on some key bit conditions for
5/6-round reduced \textsc{Ascon}, and translate the previous theoretic 6-round
attack with $2^{66}$ time complexity to a practical one with $2^{40}$ time
complexity. Moreover, we propose the first 7-round key-recovery attack on
\textsc{Ascon}. By introducing \emph{the cube-like key-subset technique}, we
divide the full key space into many subsets according to different key
conditions. For each key subset, we launch the cube tester to determine if the
key falls into it. Finally, we recover the full key space by testing all the
key subsets. The total time complexity is about $2^{103.9}$. In addition, for a
weak-key subset, whose size is $2^{117}$, the attack is more efficient and
costs only $2^{77}$ time complexity. Those attacks do not threaten the full
round (12 rounds) \textsc{Ascon}.

</details>


### [28] [A Practical Guideline and Taxonomy to LLVM's Control Flow Integrity](https://arxiv.org/abs/2508.15386)
*Sabine Houy,Bruno Kreyssig,Timothee Riom,Alexandre Bartel,Patrick McDaniel*

Main category: cs.CR

TL;DR: 本文建立了LLVM前向边CFI变体与内存损坏漏洞类别的分类映射，为开发者在现有代码库中逐步部署CFI提供实用指导。基于Top 10已知被利用漏洞列表，评估了LLVM CFI对四类高影响漏洞的防护效果。


<details>
  <summary>Details</summary>
Motivation: 内存损坏漏洞仍然是软件安全最严重的威胁之一，虽然控制流完整性(CFI)可以缓解这类攻击，但开发者缺乏如何在实际软件中应用CFI的具体指导。

Method: 建立LLVM前向边CFI变体与漏洞类别的分类映射，从Top 10已知被利用漏洞中选择四个高影响类别的代表性CVE进行CFI评估。

Result: CFI在两个案例中成功阻止了漏洞利用，但在另外两个案例中失败，展示了其潜力和当前局限性。

Conclusion: 研究结果为CFI的部署决策提供了依据，并为改进CFI在生产系统中的实际应用奠定了基础。

Abstract: Memory corruption vulnerabilities remain one of the most severe threats to
software security. They often allow attackers to achieve arbitrary code
execution by redirecting a vulnerable program's control flow. While Control
Flow Integrity (CFI) has gained traction to mitigate this exploitation path,
developers are not provided with any direction on how to apply CFI to
real-world software. In this work, we establish a taxonomy mapping LLVM's
forward-edge CFI variants to memory corruption vulnerability classes, offering
actionable guidance for developers seeking to deploy CFI incrementally in
existing codebases. Based on the Top 10 Known Exploited Vulnerabilities (KEV)
list, we identify four high-impact vulnerability categories and select one
representative CVE for each. We evaluate LLVM's CFI against each CVE and
explain why CFI blocks exploitation in two cases while failing in the other
two, illustrating its potential and current limitations. Our findings support
informed deployment decisions and provide a foundation for improving the
practical use of CFI in production systems.

</details>


### [29] [Private Hyperparameter Tuning with Ex-Post Guarantee](https://arxiv.org/abs/2508.15183)
*Badih Ghazi,Pritish Kamath,Alexander Knop,Ravi Kumar,Pasin Manurangsi,Chiyuan Zhang*

Main category: cs.CR

TL;DR: 本文扩展了效用优先的差分隐私方法，支持任意私有估计器序列，最多只需原始隐私预算的两倍，并能无额外隐私成本地进行超参数调优。


<details>
  <summary>Details</summary>
Motivation: 传统差分隐私采用"隐私优先"视角，但实践中用户往往需要"效用优先"——先确定所需效用水平，再计算相应的隐私成本。现有研究主要局限于简单的拉普拉斯或高斯噪声机制。

Method: 扩展Wu等人[2019]和Liu与Talwar[2019]的工作，支持任意私有估计器序列，通过相关噪声添加和渐进减少来生成逐步精确的估计，隐私成本仅归于最不嘈杂的迭代结果。

Result: 实现了对任意估计器序列的支持，隐私预算最多翻倍；证明了超参数调优（包括最优隐私预算选择）无需额外隐私成本；结果扩展到ex-post Renyi差分隐私。

Conclusion: 本文显著推广了效用优先隐私机制，使其适用于更广泛的估计器类型和隐私定义，为实际应用提供了更灵活的隐私-效用权衡框架。

Abstract: The conventional approach in differential privacy (DP) literature formulates
the privacy-utility trade-off with a "privacy-first" perspective: for a
predetermined level of privacy, a certain utility is achievable. However,
practitioners often operate under a "utility-first" paradigm, prioritizing a
desired level of utility and then determining the corresponding privacy cost.
  Wu et al. [2019] initiated a formal study of this "utility-first" perspective
by introducing ex-post DP. They demonstrated that by adding correlated Laplace
noise and progressively reducing it on demand, a sequence of increasingly
accurate estimates of a private parameter can be generated, with the privacy
cost attributed only to the least noisy iterate released. This led to a Laplace
mechanism variant that achieves a specified utility with minimal privacy loss.
However, their work, and similar findings by Whitehouse et al. [2022], are
primarily limited to simple mechanisms based on Laplace or Gaussian noise.
  In this paper, we significantly generalize these results. In particular, we
extend the work of Wu et al. [2019] and Liu and Talwar [2019] to support any
sequence of private estimators, incurring at most a doubling of the original
privacy budget. Furthermore, we demonstrate that hyperparameter tuning for
these estimators, including the selection of an optimal privacy budget, can be
performed without additional privacy cost. Finally, we extend our results to
ex-post Renyi DP, further broadening the applicability of utility-first privacy
mechanisms.

</details>


### [30] [Retrieval-Augmented Review Generation for Poisoning Recommender Systems](https://arxiv.org/abs/2508.15252)
*Shiyi Yang,Xinshu Li,Guanglin Zhou,Chen Wang,Xiwei Xu,Liming Zhu,Lina Yao*

Main category: cs.CR

TL;DR: 本文提出RAGAN框架，利用多模态基础模型的上下文学习能力生成高质量虚假用户配置文件，提升推荐系统的数据投毒攻击效果和隐蔽性。


<details>
  <summary>Details</summary>
Motivation: 现有推荐系统易受数据投毒攻击，但攻击者在黑盒设置下资源有限，生成的虚假配置文件质量不高，文本评论质量差影响攻击效果和隐蔽性。

Method: 提出RAGAN框架，利用多模态基础模型的上下文学习能力，结合演示检索算法和文本风格转换策略，通过越狱器、指令代理和守护者协同优化生成高质量虚假配置文件。

Result: 在多个真实数据集上的实验表明，RAGAN实现了最先进的中毒攻击性能。

Conclusion: RAGAN框架能有效生成高质量的虚假用户配置文件，为评估推荐系统的鲁棒性提供了新方法。

Abstract: Recent studies have shown that recommender systems (RSs) are highly
vulnerable to data poisoning attacks, where malicious actors inject fake user
profiles, including a group of well-designed fake ratings, to manipulate
recommendations. Due to security and privacy constraints in practice, attackers
typically possess limited knowledge of the victim system and thus need to craft
profiles that have transferability across black-box RSs. To maximize the attack
impact, the profiles often remains imperceptible. However, generating such
high-quality profiles with the restricted resources is challenging. Some works
suggest incorporating fake textual reviews to strengthen the profiles; yet, the
poor quality of the reviews largely undermines the attack effectiveness and
imperceptibility under the practical setting.
  To tackle the above challenges, in this paper, we propose to enhance the
quality of the review text by harnessing in-context learning (ICL) capabilities
of multimodal foundation models. To this end, we introduce a demonstration
retrieval algorithm and a text style transfer strategy to augment the navie
ICL. Specifically, we propose a novel practical attack framework named RAGAN to
generate high-quality fake user profiles, which can gain insights into the
robustness of RSs. The profiles are generated by a jailbreaker and
collaboratively optimized on an instructional agent and a guardian to improve
the attack transferability and imperceptibility. Comprehensive experiments on
various real-world datasets demonstrate that RAGAN achieves the
state-of-the-art poisoning attack performance.

</details>


### [31] [Connected and Exposed: Cybersecurity Risks, Regulatory Gaps, and Public Perception in Internet-Connected Vehicles](https://arxiv.org/abs/2508.15306)
*Henrietta Hegyi,Laszlo Erdodi*

Main category: cs.CR

TL;DR: 本文分析了联网汽车的网络安全和隐私风险，通过评估16项国际标准法规和用户调查，揭示了当前保护措施的不足与消费者认知差距


<details>
  <summary>Details</summary>
Motivation: 随着智能网联汽车的快速发展，网络安全和个人隐私风险日益突出，需要全面了解当前法规保护水平和用户认知状况

Method: 采用双重研究方法：1）对16项国际标准和法规进行多维度分析；2）开展用户调查了解消费者对智能汽车的态度和风险认知

Result: 研究发现当前法规在监管力度、技术细节、供应链风险处理等方面存在不足，同时用户对数据风险认知有限且信息获取不充分

Conclusion: 需要制定更全面的法规框架并加强用户教育，以应对联网汽车生态系统中的安全和隐私挑战

Abstract: The rapid advancement of Internet-connected vehicle technologies has
introduced a new era of smart mobility, while simultaneously raising
significant cybersecurity and privacy concerns. This paper explores the
evolving threat landscape associated with connected vehicles, focusing on risks
such as unauthorized remote access and the potential leakage of personal data.
To assess the current state of protection, we conducted a comprehensive
analysis of 16 international standards and regulations, evaluating them from
multiple perspectives including regulatory strength, technical specificity,
treatment of supply chain risks, and approaches to personal data handling.
  In parallel, we carried out a user-focused survey designed to map consumer
attitudes toward smart cars. The survey investigated which types of vehicles
users trust and prefer, the reasons behind rejecting certain car types, their
awareness of data-related risks, and whether they feel adequately informed
about how their vehicles handle data. By combining regulatory analysis with
user perception insights, this study aims to contribute to a more holistic
understanding of the challenges and expectations surrounding connected vehicle
ecosystems.

</details>


### [32] [IPIGuard: A Novel Tool Dependency Graph-Based Defense Against Indirect Prompt Injection in LLM Agents](https://arxiv.org/abs/2508.15310)
*Hengyu An,Jinghuai Zhang,Tianyu Du,Chunyi Zhou,Qingming Li,Tao Lin,Shouling Ji*

Main category: cs.CR

TL;DR: IPIGuard是一种防御间接提示注入攻击的新方法，通过工具依赖图(TDG)将动作规划与外部数据交互解耦，有效减少恶意工具调用


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理在与不可信数据源交互时容易受到间接提示注入(IPI)攻击，现有防御方法依赖模型内在安全性假设，缺乏对代理行为的结构性约束

Method: 提出IPIGuard防御框架，将代理任务执行过程建模为工具依赖图(TDG)的遍历过程，明确分离动作规划和外部数据交互

Result: 在AgentDojo基准测试中，IPIGuard在有效性和鲁棒性之间取得了优越的平衡

Conclusion: IPIGuard为动态环境中开发更安全的代理系统铺平了道路，通过结构性约束显著增强了对抗IPI攻击的鲁棒性

Abstract: Large language model (LLM) agents are widely deployed in real-world
applications, where they leverage tools to retrieve and manipulate external
data for complex tasks. However, when interacting with untrusted data sources
(e.g., fetching information from public websites), tool responses may contain
injected instructions that covertly influence agent behaviors and lead to
malicious outcomes, a threat referred to as Indirect Prompt Injection (IPI).
Existing defenses typically rely on advanced prompting strategies or auxiliary
detection models. While these methods have demonstrated some effectiveness,
they fundamentally rely on assumptions about the model's inherent security,
which lacks structural constraints on agent behaviors. As a result, agents
still retain unrestricted access to tool invocations, leaving them vulnerable
to stronger attack vectors that can bypass the security guardrails of the
model. To prevent malicious tool invocations at the source, we propose a novel
defensive task execution paradigm, called IPIGuard, which models the agents'
task execution process as a traversal over a planned Tool Dependency Graph
(TDG). By explicitly decoupling action planning from interaction with external
data, IPIGuard significantly reduces unintended tool invocations triggered by
injected instructions, thereby enhancing robustness against IPI attacks.
Experiments on the AgentDojo benchmark show that IPIGuard achieves a superior
balance between effectiveness and robustness, paving the way for the
development of safer agentic systems in dynamic environments.

</details>


### [33] [BadFU: Backdoor Federated Learning through Adversarial Machine Unlearning](https://arxiv.org/abs/2508.15541)
*Bingguang Lu,Hongsheng Hu,Yuantian Miao,Shaleeza Sohail,Chaoxiang He,Shuo Wang,Xiao Chen*

Main category: cs.CR

TL;DR: 本文提出了BadFU攻击，首次展示了在联邦学习中通过合法遗忘请求注入后门的攻击方式，揭示了联邦遗忘机制的安全漏洞。


<details>
  <summary>Details</summary>
Motivation: 随着数据隐私和法规合规性要求的提高，联邦学习中的机器遗忘技术变得越来越重要，但将遗忘整合到联邦学习中带来了新的安全风险，特别是攻击者可能利用遗忘过程破坏全局模型的完整性。

Method: 提出BadFU攻击策略，恶意客户端在联邦训练过程中使用后门样本和伪装样本正常训练全局模型，当请求遗忘伪装样本时，全局模型会转变为后门状态。

Result: 在各种联邦学习框架和遗忘策略下的广泛实验验证了BadFU的有效性，攻击成功率超过90%，揭示了当前联邦遗忘实践的关键漏洞。

Conclusion: 这项工作揭示了联邦遗忘机制中存在的严重安全风险，强调了开发更安全、更鲁棒的联邦遗忘机制的迫切需求。

Abstract: Federated learning (FL) has been widely adopted as a decentralized training
paradigm that enables multiple clients to collaboratively learn a shared model
without exposing their local data. As concerns over data privacy and regulatory
compliance grow, machine unlearning, which aims to remove the influence of
specific data from trained models, has become increasingly important in the
federated setting to meet legal, ethical, or user-driven demands. However,
integrating unlearning into FL introduces new challenges and raises largely
unexplored security risks. In particular, adversaries may exploit the
unlearning process to compromise the integrity of the global model. In this
paper, we present the first backdoor attack in the context of federated
unlearning, demonstrating that an adversary can inject backdoors into the
global model through seemingly legitimate unlearning requests. Specifically, we
propose BadFU, an attack strategy where a malicious client uses both backdoor
and camouflage samples to train the global model normally during the federated
training process. Once the client requests unlearning of the camouflage
samples, the global model transitions into a backdoored state. Extensive
experiments under various FL frameworks and unlearning strategies validate the
effectiveness of BadFU, revealing a critical vulnerability in current federated
unlearning practices and underscoring the urgent need for more secure and
robust federated unlearning mechanisms.

</details>


### [34] [Towards Scalable and Interpretable Mobile App Risk Analysis via Large Language Models](https://arxiv.org/abs/2508.15606)
*Yu Yang,Zhenyuan Li,Xiandong Ran,Jiahao Liu,Jiahui Wang,Bo Yu,Shouling Ji*

Main category: cs.CR

TL;DR: Mars系统利用大型语言模型自动识别移动应用安全风险，通过风险识别树提取关键指标，减少LLM输入并防止幻觉，自动生成证据链，在真实数据集上达到0.838的F1分数，效率提升60-90%。


<details>
  <summary>Details</summary>
Motivation: 当前移动应用市场的安全审查过程依赖人工分析，效率低下且劳动密集，需要自动化解决方案来提高审查效率和准确性。

Method: 构建风险识别树从高维应用特征中提取相关指标，减少LLM输入量；使用LLM进行最终风险判定；自动生成完整的证据链记录分析过程。

Result: 在真实Android市场数据集上，风险识别F1分数达0.838，证据检索F1分数达0.934；用户研究表明效率比传统人工分析提升60-90%。

Conclusion: Mars系统成功实现了移动应用安全风险的自动化识别，显著提高了审查效率，生成的证据链为后续人工审查提供了透明依据，具有实际应用价值。

Abstract: Mobile application marketplaces are responsible for vetting apps to identify
and mitigate security risks. Current vetting processes are labor-intensive,
relying on manual analysis by security professionals aided by semi-automated
tools. To address this inefficiency, we propose Mars, a system that leverages
Large Language Models (LLMs) for automated risk identification and profiling.
Mars is designed to concurrently analyze multiple applications across diverse
risk categories with minimal human intervention. To enhance analytical
precision and operational efficiency, Mars leverages a pre-constructed risk
identification tree to extract relevant indicators from high-dimensional
application features. This initial step filters the data, reducing the input
volume for the LLM and mitigating the potential for model hallucination induced
by irrelevant features. The extracted indicators are then subjected to LLM
analysis for final risk determination. Furthermore, Mars automatically
generates a comprehensive evidence chain for each assessment, documenting the
analytical process to provide transparent justification. These chains are
designed to facilitate subsequent manual review and to inform enforcement
decisions, such as application delisting. The performance of Mars was evaluated
on a real-world dataset from a partner Android marketplace. The results
demonstrate that Mars attained an F1-score of 0.838 in risk identification and
an F1-score of 0.934 in evidence retrieval. To assess its practical
applicability, a user study involving 20 expert analysts was conducted, which
indicated that Mars yielded a substantial efficiency gain, ranging from 60% to
90%, over conventional manual analysis.

</details>


### [35] [MCPTox: A Benchmark for Tool Poisoning Attack on Real-World MCP Servers](https://arxiv.org/abs/2508.14925)
*Zhiqiang Wang,Yichao Gao,Yanting Wang,Suyuan Liu,Haifeng Sun,Haoran Cheng,Guanquan Shi,Haohua Du,Xiangyang Li*

Main category: cs.CR

TL;DR: MCPTox基准测试系统评估了LLM代理对工具中毒攻击的脆弱性，发现主流代理普遍易受攻击，攻击成功率高达72.8%，且现有安全对齐机制对此类攻击无效。


<details>
  <summary>Details</summary>
Motivation: Model Context Protocol (MCP)为LLM代理提供标准化外部工具接口，但引入了新的攻击面。现有研究主要关注通过工具输出注入的攻击，而工具元数据中毒这一更基础的威胁缺乏系统性评估。

Method: 构建MCPTox基准测试，基于45个真实MCP服务器和353个真实工具，通过few-shot学习设计3种攻击模板生成1312个恶意测试用例，覆盖10类潜在风险。评估20个主流LLM代理。

Result: 发现代理普遍易受工具中毒攻击，o1-mini攻击成功率72.8%。能力更强的模型更易受攻击，现有安全对齐几乎无效，最高拒绝率(Claude-3.7-Sonnet)不足3%。

Conclusion: 工具中毒是广泛存在的威胁，现有安全机制对此无效。MCPTox为理解和缓解这一威胁提供了重要实证基准，并促进可验证安全AI代理的开发。

Abstract: By providing a standardized interface for LLM agents to interact with
external tools, the Model Context Protocol (MCP) is quickly becoming a
cornerstone of the modern autonomous agent ecosystem. However, it creates novel
attack surfaces due to untrusted external tools. While prior work has focused
on attacks injected through external tool outputs, we investigate a more
fundamental vulnerability: Tool Poisoning, where malicious instructions are
embedded within a tool's metadata without execution. To date, this threat has
been primarily demonstrated through isolated cases, lacking a systematic,
large-scale evaluation.
  We introduce MCPTox, the first benchmark to systematically evaluate agent
robustness against Tool Poisoning in realistic MCP settings. MCPTox is
constructed upon 45 live, real-world MCP servers and 353 authentic tools. To
achieve this, we design three distinct attack templates to generate a
comprehensive suite of 1312 malicious test cases by few-shot learning, covering
10 categories of potential risks. Our evaluation on 20 prominent LLM agents
setting reveals a widespread vulnerability to Tool Poisoning, with o1-mini,
achieving an attack success rate of 72.8\%. We find that more capable models
are often more susceptible, as the attack exploits their superior
instruction-following abilities. Finally, the failure case analysis reveals
that agents rarely refuse these attacks, with the highest refused rate
(Claude-3.7-Sonnet) less than 3\%, demonstrating that existing safety alignment
is ineffective against malicious actions that use legitimate tools for
unauthorized operation. Our findings create a crucial empirical baseline for
understanding and mitigating this widespread threat, and we release MCPTox for
the development of verifiably safer AI agents. Our dataset is available at an
anonymized repository: \textit{https://anonymous.4open.science/r/AAAI26-7C02}.

</details>


### [36] [A Systematic Survey of Model Extraction Attacks and Defenses: State-of-the-Art and Perspectives](https://arxiv.org/abs/2508.15031)
*Kaixiang Zhao,Lincan Li,Kaize Ding,Neil Zhenqiang Gong,Yue Zhao,Yushun Dong*

Main category: cs.CR

TL;DR: 本文系统调研究机器学习作为服务(MLaaS)平台上的模型提取攻击(MEAs)和防御策略，提出了新的分类法，分析了攻击技术有效性和防御挑战，并讨论了技术、伦理、法律和社会影响。


<details>
  <summary>Details</summary>
Motivation: MLaaS平台提供了可扩展、方便和便宜的ML模型访问，但也引入了模型提取攻击的漏洞，威胁到知识产权、隐私和系统安全。

Method: 提出了一种新的分类法，根据攻击机制、防御方法和计算环境对MEAs进行分类，系统分析各种攻击技术和防御策略的有效性。

Result: 对各种攻击技术进行了评估，展现了现有防御方案面临的挑战，特别是保持模型效用性与确保安全性之间的关键找法。

Conclusion: 本系统调研究为人工智能安全和隐私领域的研究人员、实践者和政策制定者提供了有价值的参考，并持续维护在线文献仓库。

Abstract: Machine learning (ML) models have significantly grown in complexity and
utility, driving advances across multiple domains. However, substantial
computational resources and specialized expertise have historically restricted
their wide adoption. Machine-Learning-as-a-Service (MLaaS) platforms have
addressed these barriers by providing scalable, convenient, and affordable
access to sophisticated ML models through user-friendly APIs. While this
accessibility promotes widespread use of advanced ML capabilities, it also
introduces vulnerabilities exploited through Model Extraction Attacks (MEAs).
Recent studies have demonstrated that adversaries can systematically replicate
a target model's functionality by interacting with publicly exposed interfaces,
posing threats to intellectual property, privacy, and system security. In this
paper, we offer a comprehensive survey of MEAs and corresponding defense
strategies. We propose a novel taxonomy that classifies MEAs according to
attack mechanisms, defense approaches, and computing environments. Our analysis
covers various attack techniques, evaluates their effectiveness, and highlights
challenges faced by existing defenses, particularly the critical trade-off
between preserving model utility and ensuring security. We further assess MEAs
within different computing paradigms and discuss their technical, ethical,
legal, and societal implications, along with promising directions for future
research. This systematic survey aims to serve as a valuable reference for
researchers, practitioners, and policymakers engaged in AI security and
privacy. Additionally, we maintain an online repository continuously updated
with related literature at https://github.com/kzhao5/ModelExtractionPapers.

</details>


### [37] [MoEcho: Exploiting Side-Channel Attacks to Compromise User Privacy in Mixture-of-Experts LLMs](https://arxiv.org/abs/2508.15036)
*Ruyi Ding,Tianhong Xu,Xinyi Shen,Aidong Adam Ding,Yunsi Fei*

Main category: cs.CR

TL;DR: MoEcho发现了混合专家(MoE)架构中的侧信道安全漏洞，通过分析硬件执行痕迹可以推断用户敏感数据，威胁MoE模型隐私安全。


<details>
  <summary>Details</summary>
Motivation: MoE架构通过动态路由实现计算效率，但这种输入依赖的激活模式在硬件层面留下了可被利用的时空痕迹，形成了新的隐私攻击面。

Method: 提出了四种新型架构侧信道：CPU上的缓存占用通道和页出重加载，GPU上的性能计数器和TLB驱逐重加载，并基于这些漏洞设计了四种隐私攻击方法。

Result: 成功实现了对基于MoE的大型语言模型和视觉语言模型的隐私攻击，包括提示推断、响应重建、视觉推断和视觉重建攻击。

Conclusion: 这是首次对现代Transformer中流行的MoE结构进行运行时架构级安全分析，揭示了严重的安全隐私威胁，呼吁在开发高效大规模AI服务时需要及时有效的防护措施。

Abstract: The transformer architecture has become a cornerstone of modern AI, fueling
remarkable progress across applications in natural language processing,
computer vision, and multimodal learning. As these models continue to scale
explosively for performance, implementation efficiency remains a critical
challenge. Mixture of Experts (MoE) architectures, selectively activating
specialized subnetworks (experts), offer a unique balance between model
accuracy and computational cost. However, the adaptive routing in MoE
architectures, where input tokens are dynamically directed to specialized
experts based on their semantic meaning inadvertently opens up a new attack
surface for privacy breaches. These input-dependent activation patterns leave
distinctive temporal and spatial traces in hardware execution, which
adversaries could exploit to deduce sensitive user data. In this work, we
propose MoEcho, discovering a side channel analysis based attack surface that
compromises user privacy on MoE based systems. Specifically, in MoEcho, we
introduce four novel architectural side channels on different computing
platforms, including Cache Occupancy Channels and Pageout+Reload on CPUs, and
Performance Counter and TLB Evict+Reload on GPUs, respectively. Exploiting
these vulnerabilities, we propose four attacks that effectively breach user
privacy in large language models (LLMs) and vision language models (VLMs) based
on MoE architectures: Prompt Inference Attack, Response Reconstruction Attack,
Visual Inference Attack, and Visual Reconstruction Attack. MoEcho is the first
runtime architecture level security analysis of the popular MoE structure
common in modern transformers, highlighting a serious security and privacy
threat and calling for effective and timely safeguards when harnessing MoE
based models for developing efficient large scale AI services.

</details>


### [38] [When Machine Learning Meets Vulnerability Discovery: Challenges and Lessons Learned](https://arxiv.org/abs/2508.15042)
*Sima Arasteh,Christophe Hauser*

Main category: cs.CR

TL;DR: 本文探讨了机器学习在软件漏洞检测中的应用挑战，特别是数据集规模、评估方法和模型选择方面的问题，并分享了作者先前研究Bin2vec和BinHunter的见解


<details>
  <summary>Details</summary>
Motivation: 机器学习在软件漏洞检测中面临新的挑战，包括数据规模问题、评估方法不完善以及训练数据集适用性不确定等问题，需要系统性地分析和解决

Method: 通过分析机器学习在漏洞发现领域的应用现状，结合作者先前的研究成果Bin2vec和BinHunter，探讨改进方法和未来研究方向

Result: 识别出当前机器学习漏洞检测方法在数据集统计、训练样本选择和模型粒度等方面的重要缺陷，提出了改进建议

Conclusion: 机器学习在软件漏洞检测领域仍有重要挑战需要解决，特别是在数据集质量和评估方法方面，作者的前期研究为未来研究提供了有价值的见解和方向

Abstract: In recent years, machine learning has demonstrated impressive results in
various fields, including software vulnerability detection. Nonetheless, using
machine learning to identify software vulnerabilities presents new challenges,
especially regarding the scale of data involved, which was not a factor in
traditional methods. Consequently, in spite of the rise of new
machine-learning-based approaches in that space, important shortcomings persist
regarding their evaluation. First, researchers often fail to provide concrete
statistics about their training datasets, such as the number of samples for
each type of vulnerability. Moreover, many methods rely on training with
semantically similar functions rather than directly on vulnerable programs.
This leads to uncertainty about the suitability of the datasets currently used
for training. Secondly, the choice of a model and the level of granularity at
which models are trained also affect the effectiveness of such vulnerability
discovery approaches.
  In this paper, we explore the challenges of applying machine learning to
vulnerability discovery. We also share insights from our two previous research
papers, Bin2vec and BinHunter, which could enhance future research in this
field.

</details>


### [39] [Tighter Privacy Analysis for Truncated Poisson Sampling](https://arxiv.org/abs/2508.15089)
*Arun Ganesh*

Main category: cs.CR

TL;DR: 对截断泊松采样的新隐私放大分析


<details>
  <summary>Details</summary>
Motivation: 截断泊松采样是泊松采样的一个变体，当批次大小超过给定最大值时会进行截断处理，需要对其隐私保护效果进行量化分析

Method: 提出了针对截断泊松采样的新隐私放大分析方法，通过数学建模和理论推导来量化其隐私保护效果

Result: 建立了截断泊松采样的隐私放大理论框架，能够准确计算其隐私保护强度

Conclusion: 该分析为截断泊松采样提供了可靠的隐私保护量化工具，有助于在实际应用中更好地平衡隐私保护与数据效用

Abstract: We give a new privacy amplification analysis for truncated Poisson sampling,
a Poisson sampling variant that truncates a batch if it exceeds a given maximum
batch size.

</details>


### [40] [Adaptive Anomaly Detection in Evolving Network Environments](https://arxiv.org/abs/2508.15100)
*Ehssan Mousavipour,Andrey Dimanchev,Majid Ghaderi*

Main category: cs.CR

TL;DR: NetSight是一个用于网络数据异常检测的监督学习框架，能够在线持续检测和适应分布偏移，无需人工干预，通过伪标签技术和知识蒸馏防止灾难性遗忘，在三个长期网络数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 分布偏移是深度学习异常检测系统面临的关键挑战，现有方法要么需要昂贵的人工标注（监督学习），要么依赖难以获取的干净数据（无监督学习），难以在实际中满足这些要求。

Method: NetSight采用新颖的伪标签技术消除人工干预，并使用基于知识蒸馏的适应策略来防止灾难性遗忘，实现在线持续检测和适应分布偏移。

Result: 在三个长期网络数据集上的评估显示，NetSight相比依赖人工标注的最先进方法具有更优的适应性能，F1分数提升高达11.72%。

Conclusion: NetSight证明了其在经历分布偏移的动态网络中的鲁棒性和有效性，为实际部署提供了可行的解决方案。

Abstract: Distribution shift, a change in the statistical properties of data over time,
poses a critical challenge for deep learning anomaly detection systems.
Existing anomaly detection systems often struggle to adapt to these shifts.
Specifically, systems based on supervised learning require costly manual
labeling, while those based on unsupervised learning rely on clean data, which
is difficult to obtain, for shift adaptation. Both of these requirements are
challenging to meet in practice. In this paper, we introduce NetSight, a
framework for supervised anomaly detection in network data that continually
detects and adapts to distribution shifts in an online manner. NetSight
eliminates manual intervention through a novel pseudo-labeling technique and
uses a knowledge distillation-based adaptation strategy to prevent catastrophic
forgetting. Evaluated on three long-term network datasets, NetSight
demonstrates superior adaptation performance compared to state-of-the-art
methods that rely on manual labeling, achieving F1-score improvements of up to
11.72%. This proves its robustness and effectiveness in dynamic networks that
experience distribution shifts over time.

</details>


### [41] [Conditional Cube Attack on Round-Reduced ASCON](https://arxiv.org/abs/2508.15172)
*Zheng Li,Xiaoyang Dong,Xiaoyun Wang*

Main category: cs.CR

TL;DR: 本文评估了认证加密算法Ascon对立方体攻击的安全性，提出了新的条件立方体攻击方法，将6轮攻击复杂度从理论2^66降至实际2^40，并首次实现了7轮密钥恢复攻击，复杂度为2^103.9。


<details>
  <summary>Details</summary>
Motivation: Ascon是CAESAR竞赛第三轮的16个幸存者之一，之前最好的密钥恢复攻击只能达到6轮，而类似结构的Keccak密钥模式攻击可达7轮以上，需要研究Ascon对立方体攻击的更强抵抗力。

Method: 推广了条件立方体攻击方法，引入立方体密钥子集技术，根据不同的密钥条件将密钥空间划分为多个子集，对每个子集使用立方体测试器进行测试。

Result: 成功将6轮攻击的时间复杂度从2^66降低到2^40，并首次实现了7轮密钥恢复攻击，总时间复杂度约为2^103.9，对弱密钥子集(大小2^117)的攻击复杂度仅为2^77。

Conclusion: 虽然攻击取得了进展，但这些攻击并不威胁完整12轮的Ascon算法，表明Ascon具有足够的安全余量。

Abstract: This paper evaluates the secure level of authenticated encryption
\textsc{Ascon} against cube-like method. \textsc{Ascon} submitted by Dobraunig
\emph{et~al.} is one of 16 survivors of the 3rd round CAESAR competition. The
cube-like method is first used by Dinur \emph{et~al.} to analyze Keccak keyed
modes. At CT-RSA 2015, Dobraunig \emph{et~al.} applied this method to 5/6-round
reduced \textsc{Ascon}, whose structure is similar to Keccak keyed modes.
However, for \textsc{Ascon} the non-linear layer is more complex and state is
much smaller, which make it hard for the attackers to select enough cube
variables that do not multiply with each other after the first round. This
seems to be the reason why the best previous key-recovery attack is on 6-round
\textsc{Ascon}, while for Keccak keyed modes (Keccak-MAC and Keyak) the
attacked round is no less than 7-round.
  In this paper, we generalize the conditional cube attack proposed by Huang
\emph{et~al.}, and find new cubes depending on some key bit conditions for
5/6-round reduced \textsc{Ascon}, and translate the previous theoretic 6-round
attack with $2^{66}$ time complexity to a practical one with $2^{40}$ time
complexity. Moreover, we propose the first 7-round key-recovery attack on
\textsc{Ascon}. By introducing \emph{the cube-like key-subset technique}, we
divide the full key space into many subsets according to different key
conditions. For each key subset, we launch the cube tester to determine if the
key falls into it. Finally, we recover the full key space by testing all the
key subsets. The total time complexity is about $2^{103.9}$. In addition, for a
weak-key subset, whose size is $2^{117}$, the attack is more efficient and
costs only $2^{77}$ time complexity. Those attacks do not threaten the full
round (12 rounds) \textsc{Ascon}.

</details>


### [42] [A Practical Guideline and Taxonomy to LLVM's Control Flow Integrity](https://arxiv.org/abs/2508.15386)
*Sabine Houy,Bruno Kreyssig,Timothee Riom,Alexandre Bartel,Patrick McDaniel*

Main category: cs.CR

TL;DR: 本文建立了LLVM前向边CFI变体与内存破坏漏洞类别的分类映射，为开发者在现有代码库中逐步部署CFI提供实用指导。通过分析Top 10已知被利用漏洞，评估了CFI在四个高影响漏洞类别中的防护效果。


<details>
  <summary>Details</summary>
Motivation: 内存破坏漏洞仍然是软件安全最严重的威胁之一，虽然控制流完整性(CFI)可以缓解这类攻击，但开发者缺乏如何在实际软件中应用CFI的具体指导。

Method: 基于Top 10已知被利用漏洞列表，识别四个高影响漏洞类别并为每个类别选择一个代表性CVE。评估LLVM的CFI对这些CVE的防护效果，分析CFI成功阻止利用和失败的原因。

Result: CFI在两个案例中成功阻止了漏洞利用，但在另外两个案例中失败，展示了其潜力和当前局限性。

Conclusion: 研究结果为明智的CFI部署决策提供了支持，并为改进CFI在生产系统中的实际应用奠定了基础。

Abstract: Memory corruption vulnerabilities remain one of the most severe threats to
software security. They often allow attackers to achieve arbitrary code
execution by redirecting a vulnerable program's control flow. While Control
Flow Integrity (CFI) has gained traction to mitigate this exploitation path,
developers are not provided with any direction on how to apply CFI to
real-world software. In this work, we establish a taxonomy mapping LLVM's
forward-edge CFI variants to memory corruption vulnerability classes, offering
actionable guidance for developers seeking to deploy CFI incrementally in
existing codebases. Based on the Top 10 Known Exploited Vulnerabilities (KEV)
list, we identify four high-impact vulnerability categories and select one
representative CVE for each. We evaluate LLVM's CFI against each CVE and
explain why CFI blocks exploitation in two cases while failing in the other
two, illustrating its potential and current limitations. Our findings support
informed deployment decisions and provide a foundation for improving the
practical use of CFI in production systems.

</details>


### [43] [Private Hyperparameter Tuning with Ex-Post Guarantee](https://arxiv.org/abs/2508.15183)
*Badih Ghazi,Pritish Kamath,Alexander Knop,Ravi Kumar,Pasin Manurangsi,Chiyuan Zhang*

Main category: cs.CR

TL;DR: 本文扩展了效用优先的差分隐私方法，支持任意私有估计器序列，最多只需原始隐私预算的两倍，并能无额外成本进行超参数调优，包括扩展到ex-post Renyi DP。


<details>
  <summary>Details</summary>
Motivation: 传统差分隐私采用"隐私优先"视角，但实践中用户往往需要"效用优先"——先确定所需效用水平，再计算相应的隐私成本。现有研究主要局限于简单的拉普拉斯或高斯噪声机制。

Method: 扩展Wu等人[2019]和Liu与Talwar[2019]的工作，支持任意私有估计器序列，通过渐进减少相关噪声生成逐步精确的估计，隐私成本仅归因于最终发布的最精确迭代结果。

Result: 实现了最多只需原始隐私预算两倍的通用效用优先隐私机制，能够无额外隐私成本进行超参数调优（包括最优隐私预算选择），并成功扩展到ex-post Renyi DP。

Conclusion: 该研究显著推广了效用优先隐私机制的应用范围，使其不再局限于简单噪声机制，为实际应用提供了更灵活和高效的隐私-效用权衡解决方案。

Abstract: The conventional approach in differential privacy (DP) literature formulates
the privacy-utility trade-off with a "privacy-first" perspective: for a
predetermined level of privacy, a certain utility is achievable. However,
practitioners often operate under a "utility-first" paradigm, prioritizing a
desired level of utility and then determining the corresponding privacy cost.
  Wu et al. [2019] initiated a formal study of this "utility-first" perspective
by introducing ex-post DP. They demonstrated that by adding correlated Laplace
noise and progressively reducing it on demand, a sequence of increasingly
accurate estimates of a private parameter can be generated, with the privacy
cost attributed only to the least noisy iterate released. This led to a Laplace
mechanism variant that achieves a specified utility with minimal privacy loss.
However, their work, and similar findings by Whitehouse et al. [2022], are
primarily limited to simple mechanisms based on Laplace or Gaussian noise.
  In this paper, we significantly generalize these results. In particular, we
extend the work of Wu et al. [2019] and Liu and Talwar [2019] to support any
sequence of private estimators, incurring at most a doubling of the original
privacy budget. Furthermore, we demonstrate that hyperparameter tuning for
these estimators, including the selection of an optimal privacy budget, can be
performed without additional privacy cost. Finally, we extend our results to
ex-post Renyi DP, further broadening the applicability of utility-first privacy
mechanisms.

</details>


### [44] [Retrieval-Augmented Review Generation for Poisoning Recommender Systems](https://arxiv.org/abs/2508.15252)
*Shiyi Yang,Xinshu Li,Guanglin Zhou,Chen Wang,Xiwei Xu,Liming Zhu,Lina Yao*

Main category: cs.CR

TL;DR: 本文提出RAGAN攻击框架，利用多模态基础模型的上下文学习能力生成高质量虚假用户档案，通过演示检索和文本风格转换增强攻击效果，在推荐系统中实现最先进的中毒攻击性能。


<details>
  <summary>Details</summary>
Motivation: 推荐系统易受数据中毒攻击，但现有方法生成的虚假文本评论质量较差，影响攻击效果和隐蔽性。需要利用先进模型提升虚假档案质量。

Method: 提出RAGAN框架，利用多模态基础模型的上下文学习能力，结合演示检索算法和文本风格转换策略，通过jailbreaker、instructional agent和guardian协同优化生成高质量虚假用户档案。

Result: 在多个真实数据集上的实验表明，RAGAN实现了最先进的中毒攻击性能，生成的虚假档案具有更好的迁移性和隐蔽性。

Conclusion: RAGAN框架有效提升了推荐系统中毒攻击的质量，为评估推荐系统鲁棒性提供了有力工具，展示了多模态基础模型在生成高质量攻击内容方面的潜力。

Abstract: Recent studies have shown that recommender systems (RSs) are highly
vulnerable to data poisoning attacks, where malicious actors inject fake user
profiles, including a group of well-designed fake ratings, to manipulate
recommendations. Due to security and privacy constraints in practice, attackers
typically possess limited knowledge of the victim system and thus need to craft
profiles that have transferability across black-box RSs. To maximize the attack
impact, the profiles often remains imperceptible. However, generating such
high-quality profiles with the restricted resources is challenging. Some works
suggest incorporating fake textual reviews to strengthen the profiles; yet, the
poor quality of the reviews largely undermines the attack effectiveness and
imperceptibility under the practical setting.
  To tackle the above challenges, in this paper, we propose to enhance the
quality of the review text by harnessing in-context learning (ICL) capabilities
of multimodal foundation models. To this end, we introduce a demonstration
retrieval algorithm and a text style transfer strategy to augment the navie
ICL. Specifically, we propose a novel practical attack framework named RAGAN to
generate high-quality fake user profiles, which can gain insights into the
robustness of RSs. The profiles are generated by a jailbreaker and
collaboratively optimized on an instructional agent and a guardian to improve
the attack transferability and imperceptibility. Comprehensive experiments on
various real-world datasets demonstrate that RAGAN achieves the
state-of-the-art poisoning attack performance.

</details>


### [45] [Connected and Exposed: Cybersecurity Risks, Regulatory Gaps, and Public Perception in Internet-Connected Vehicles](https://arxiv.org/abs/2508.15306)
*Henrietta Hegyi,Laszlo Erdodi*

Main category: cs.CR

TL;DR: 连接车达的安全风险与用户感知研究：通过标准分析和用户调查，全面评估了智能汽车的网络安全风险、数据隐私问题以及用户的安全意识和偏好。


<details>
  <summary>Details</summary>
Motivation: 连接车技术的快速发展带来了智能移动新时代，但同时也引发了重大的网络安全和隐私风险，需要全面评估当前的保护状况和用户感知。

Method: 进行了对16个国际标准和规定的综合分析，从监管强度、技术具体性、供应链风险处理和个人数据处理等多个角度评估；同时进行了用户调查，了解消费者对智能汽车的信任度、偏好、风险认知和信息获取情况。

Result: 研究通过标准分析和用户调查的结合，揭示了连接车生态系统在安全保护方面的现状与不足，以及用户对此的感知和期望差异，为进一步完善相关标准和政策提供了实证基础。

Conclusion: 该研究通过整合监管分析与用户观点，提供了对连接车安全风险和隐私挑战的全面理解，对于推动更安全、可信贵的智能汽车生态系统的发展具有重要意义。

Abstract: The rapid advancement of Internet-connected vehicle technologies has
introduced a new era of smart mobility, while simultaneously raising
significant cybersecurity and privacy concerns. This paper explores the
evolving threat landscape associated with connected vehicles, focusing on risks
such as unauthorized remote access and the potential leakage of personal data.
To assess the current state of protection, we conducted a comprehensive
analysis of 16 international standards and regulations, evaluating them from
multiple perspectives including regulatory strength, technical specificity,
treatment of supply chain risks, and approaches to personal data handling.
  In parallel, we carried out a user-focused survey designed to map consumer
attitudes toward smart cars. The survey investigated which types of vehicles
users trust and prefer, the reasons behind rejecting certain car types, their
awareness of data-related risks, and whether they feel adequately informed
about how their vehicles handle data. By combining regulatory analysis with
user perception insights, this study aims to contribute to a more holistic
understanding of the challenges and expectations surrounding connected vehicle
ecosystems.

</details>


### [46] [IPIGuard: A Novel Tool Dependency Graph-Based Defense Against Indirect Prompt Injection in LLM Agents](https://arxiv.org/abs/2508.15310)
*Hengyu An,Jinghuai Zhang,Tianyu Du,Chunyi Zhou,Qingming Li,Tao Lin,Shouling Ji*

Main category: cs.CR

TL;DR: IPIGuard是一种新的防御间接提示注入攻击的方法，通过工具依赖图(TDG)将动作规划与外部数据交互解耦，有效减少恶意工具调用。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理在与不可信数据源交互时容易受到间接提示注入(IPI)攻击，现有防御方法依赖模型内在安全性假设，缺乏对代理行为的结构性约束。

Method: 提出IPIGuard防御框架，将代理任务执行过程建模为在工具依赖图(TDG)上的遍历，明确分离动作规划和外部数据交互。

Result: 在AgentDojo基准测试中，IPIGuard在有效性和鲁棒性之间实现了优越的平衡。

Conclusion: IPIGuard通过结构性约束显著增强了代理系统对抗IPI攻击的鲁棒性，为动态环境中更安全的智能代理系统开发铺平了道路。

Abstract: Large language model (LLM) agents are widely deployed in real-world
applications, where they leverage tools to retrieve and manipulate external
data for complex tasks. However, when interacting with untrusted data sources
(e.g., fetching information from public websites), tool responses may contain
injected instructions that covertly influence agent behaviors and lead to
malicious outcomes, a threat referred to as Indirect Prompt Injection (IPI).
Existing defenses typically rely on advanced prompting strategies or auxiliary
detection models. While these methods have demonstrated some effectiveness,
they fundamentally rely on assumptions about the model's inherent security,
which lacks structural constraints on agent behaviors. As a result, agents
still retain unrestricted access to tool invocations, leaving them vulnerable
to stronger attack vectors that can bypass the security guardrails of the
model. To prevent malicious tool invocations at the source, we propose a novel
defensive task execution paradigm, called IPIGuard, which models the agents'
task execution process as a traversal over a planned Tool Dependency Graph
(TDG). By explicitly decoupling action planning from interaction with external
data, IPIGuard significantly reduces unintended tool invocations triggered by
injected instructions, thereby enhancing robustness against IPI attacks.
Experiments on the AgentDojo benchmark show that IPIGuard achieves a superior
balance between effectiveness and robustness, paving the way for the
development of safer agentic systems in dynamic environments.

</details>


### [47] [BadFU: Backdoor Federated Learning through Adversarial Machine Unlearning](https://arxiv.org/abs/2508.15541)
*Bingguang Lu,Hongsheng Hu,Yuantian Miao,Shaleeza Sohail,Chaoxiang He,Shuo Wang,Xiao Chen*

Main category: cs.CR

TL;DR: 本文提出了BadFU攻击，首次展示了在联邦学习遗忘机制中，恶意客户端可以通过看似合法的遗忘请求向全局模型注入后门。


<details>
  <summary>Details</summary>
Motivation: 随着数据隐私和监管合规要求的增长，联邦学习中的机器遗忘机制变得重要，但将遗忘整合到联邦学习中引入了新的安全风险，特别是对手可能利用遗忘过程破坏全局模型的完整性。

Method: 提出BadFU攻击策略，恶意客户端在联邦训练过程中使用后门样本和伪装样本正常训练全局模型，然后请求遗忘伪装样本，使全局模型转变为后门状态。

Result: 在各种联邦学习框架和遗忘策略下的广泛实验验证了BadFU的有效性，揭示了当前联邦遗忘实践中的关键漏洞。

Conclusion: 这项工作强调了迫切需要更安全、更鲁棒的联邦遗忘机制，以应对这种新型安全威胁。

Abstract: Federated learning (FL) has been widely adopted as a decentralized training
paradigm that enables multiple clients to collaboratively learn a shared model
without exposing their local data. As concerns over data privacy and regulatory
compliance grow, machine unlearning, which aims to remove the influence of
specific data from trained models, has become increasingly important in the
federated setting to meet legal, ethical, or user-driven demands. However,
integrating unlearning into FL introduces new challenges and raises largely
unexplored security risks. In particular, adversaries may exploit the
unlearning process to compromise the integrity of the global model. In this
paper, we present the first backdoor attack in the context of federated
unlearning, demonstrating that an adversary can inject backdoors into the
global model through seemingly legitimate unlearning requests. Specifically, we
propose BadFU, an attack strategy where a malicious client uses both backdoor
and camouflage samples to train the global model normally during the federated
training process. Once the client requests unlearning of the camouflage
samples, the global model transitions into a backdoored state. Extensive
experiments under various FL frameworks and unlearning strategies validate the
effectiveness of BadFU, revealing a critical vulnerability in current federated
unlearning practices and underscoring the urgent need for more secure and
robust federated unlearning mechanisms.

</details>


### [48] [Towards Scalable and Interpretable Mobile App Risk Analysis via Large Language Models](https://arxiv.org/abs/2508.15606)
*Yu Yang,Zhenyuan Li,Xiandong Ran,Jiahao Liu,Jiahui Wang,Bo Yu,Shouling Ji*

Main category: cs.CR

TL;DR: Mars是一个基于大语言模型的自动化移动应用安全风险评估系统，通过风险识别树提取关键指标，减少LLM输入并提高分析精度，在真实数据集上达到0.838的F1分数和60-90%的效率提升


<details>
  <summary>Details</summary>
Motivation: 当前移动应用市场的安全审查过程劳动密集型，依赖安全专业人员的半自动化工具手动分析，效率低下，需要自动化解决方案

Method: 利用预构建的风险识别树从高维应用特征中提取相关指标，减少LLM输入量并避免模型幻觉，然后通过LLM进行最终风险判定，自动生成完整的证据链

Result: 在真实Android市场数据集上，Mars在风险识别方面达到0.838的F1分数，证据检索达到0.934的F1分数。用户研究表明效率比传统手动分析提升60-90%

Conclusion: Mars系统成功实现了移动应用安全风险评估的自动化，显著提高了分析效率和准确性，为应用市场提供了可扩展的安全审查解决方案

Abstract: Mobile application marketplaces are responsible for vetting apps to identify
and mitigate security risks. Current vetting processes are labor-intensive,
relying on manual analysis by security professionals aided by semi-automated
tools. To address this inefficiency, we propose Mars, a system that leverages
Large Language Models (LLMs) for automated risk identification and profiling.
Mars is designed to concurrently analyze multiple applications across diverse
risk categories with minimal human intervention. To enhance analytical
precision and operational efficiency, Mars leverages a pre-constructed risk
identification tree to extract relevant indicators from high-dimensional
application features. This initial step filters the data, reducing the input
volume for the LLM and mitigating the potential for model hallucination induced
by irrelevant features. The extracted indicators are then subjected to LLM
analysis for final risk determination. Furthermore, Mars automatically
generates a comprehensive evidence chain for each assessment, documenting the
analytical process to provide transparent justification. These chains are
designed to facilitate subsequent manual review and to inform enforcement
decisions, such as application delisting. The performance of Mars was evaluated
on a real-world dataset from a partner Android marketplace. The results
demonstrate that Mars attained an F1-score of 0.838 in risk identification and
an F1-score of 0.934 in evidence retrieval. To assess its practical
applicability, a user study involving 20 expert analysts was conducted, which
indicated that Mars yielded a substantial efficiency gain, ranging from 60% to
90%, over conventional manual analysis.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [49] [A Fully Spectral Neuro-Symbolic Reasoning Architecture with Graph Signal Processing as the Computational Backbone](https://arxiv.org/abs/2508.14923)
*Andrew Kiruluta*

Main category: cs.AI

TL;DR: 提出了一种完全基于频谱的神经符号推理架构，使用图信号处理(GSP)作为主要计算框架，将整个推理流程置于图频谱域中处理，在多个基准数据集上表现出优于现有方法的逻辑一致性、可解释性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统推理模型通常将频谱图方法作为外围组件，缺乏完整的频谱域推理框架。本文旨在通过GSP提供数学基础扎实且计算高效的推理系统，实现符号逻辑与神经推理的深度整合。

Method: 将逻辑实体和关系编码为图信号，通过可学习的频谱滤波器控制多尺度信息传播，并映射到符号谓词进行基于规则的推理。包含图傅里叶变换、带选择性注意力和频谱规则落地等完整数学框架。

Result: 在ProofWriter、EntailmentBank、bAbI、CLUTRR和ARC-Challenge等基准推理数据集上的实验表明，相比最先进的神经符号模型，在逻辑一致性、可解释性和计算效率方面都有提升。

Conclusion: 图信号处理为构建鲁棒且可解释的推理系统提供了数学基础扎实且计算高效的底层支撑，证明了完全频谱域推理架构的有效性。

Abstract: We propose a fully spectral, neuro\-symbolic reasoning architecture that
leverages Graph Signal Processing (GSP) as the primary computational backbone
for integrating symbolic logic and neural inference. Unlike conventional
reasoning models that treat spectral graph methods as peripheral components,
our approach formulates the entire reasoning pipeline in the graph spectral
domain. Logical entities and relationships are encoded as graph signals,
processed via learnable spectral filters that control multi-scale information
propagation, and mapped into symbolic predicates for rule-based inference. We
present a complete mathematical framework for spectral reasoning, including
graph Fourier transforms, band-selective attention, and spectral rule
grounding. Experiments on benchmark reasoning datasets (ProofWriter,
EntailmentBank, bAbI, CLUTRR, and ARC-Challenge) demonstrate improvements in
logical consistency, interpretability, and computational efficiency over
state\-of\-the\-art neuro\-symbolic models. Our results suggest that GSP
provides a mathematically grounded and computationally efficient substrate for
robust and interpretable reasoning systems.

</details>


### [50] [Goals and the Structure of Experience](https://arxiv.org/abs/2508.15013)
*Nadav Amir,Stas Tiomkin,Angela Langdon*

Main category: cs.AI

TL;DR: 提出了一种基于目标导向状态表示的计算框架，其中描述性和规范性方面从智能体-环境交互序列中共同涌现，而不是作为独立组件存在。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习将状态表示和奖励函数作为世界模型的独立组件，但本文探索这两种方面从智能体目标中共同涌现的可能性。

Method: 引入目标导向（telic）状态的概念，定义为目标等效经验分布的类别，通过行为策略与期望经验特征之间的统计差异来提供目标导向学习的简洁描述。

Result: 提出了一个统一的计算框架，能够解释行为、现象学和神经维度上的目的性行为。

Conclusion: 该框架为理解自然和人工智能中的目的性行为提供了新的理论基础，有望在不同基质中提供统一的计算解释。

Abstract: Purposeful behavior is a hallmark of natural and artificial intelligence. Its
acquisition is often believed to rely on world models, comprising both
descriptive (what is) and prescriptive (what is desirable) aspects that
identify and evaluate state of affairs in the world, respectively. Canonical
computational accounts of purposeful behavior, such as reinforcement learning,
posit distinct components of a world model comprising a state representation
(descriptive aspect) and a reward function (prescriptive aspect). However, an
alternative possibility, which has not yet been computationally formulated, is
that these two aspects instead co-emerge interdependently from an agent's goal.
Here, we describe a computational framework of goal-directed state
representation in cognitive agents, in which the descriptive and prescriptive
aspects of a world model co-emerge from agent-environment interaction
sequences, or experiences. Drawing on Buddhist epistemology, we introduce a
construct of goal-directed, or telic, states, defined as classes of
goal-equivalent experience distributions. Telic states provide a parsimonious
account of goal-directed learning in terms of the statistical divergence
between behavioral policies and desirable experience features. We review
empirical and theoretical literature supporting this novel perspective and
discuss its potential to provide a unified account of behavioral,
phenomenological and neural dimensions of purposeful behaviors across diverse
substrates.

</details>


### [51] [Collab-REC: An LLM-based Agentic Framework for Balancing Recommendations in Tourism](https://arxiv.org/abs/2508.15030)
*Ashmi Banerjee,Fitri Nur Aisyah,Adithi Satish,Wolfgang Wörndl,Yashar Deldjoo*

Main category: cs.AI

TL;DR: Collab-REC是一个多智能体框架，通过三个LLM智能体从不同角度生成旅游推荐，再通过非LLM调解器进行多轮协商，有效解决流行度偏见问题，提升推荐多样性和相关性。


<details>
  <summary>Details</summary>
Motivation: 为了解决旅游推荐系统中的流行度偏见问题，避免过度旅游现象，让更多被忽视的景点得到推荐机会。

Method: 使用三个基于LLM的智能体（个性化、流行度、可持续性）从不同角度生成城市建议，然后通过非LLM调解器进行多轮协商和精炼。

Result: 在欧洲城市查询实验中，Collab-REC相比单智能体基线提高了推荐多样性和整体相关性，能够推荐较少被访问的地点。

Conclusion: 这种多利益相关者协作的方法在LLM驱动的推荐系统中显示出良好前景，能够更好地平衡用户约束和推荐质量。

Abstract: We propose Collab-REC, a multi-agent framework designed to counteract
popularity bias and enhance diversity in tourism recommendations. In our
setting, three LLM-based agents -- Personalization, Popularity, and
Sustainability generate city suggestions from complementary perspectives. A
non-LLM moderator then merges and refines these proposals via multi-round
negotiation, ensuring each agent's viewpoint is incorporated while penalizing
spurious or repeated responses. Experiments on European city queries show that
Collab-REC improves diversity and overall relevance compared to a single-agent
baseline, surfacing lesser-visited locales that often remain overlooked. This
balanced, context-aware approach addresses over-tourism and better aligns with
constraints provided by the user, highlighting the promise of multi-stakeholder
collaboration in LLM-driven recommender systems.

</details>


### [52] [Emergent Crowds Dynamics from Language-Driven Multi-Agent Interactions](https://arxiv.org/abs/2508.15047)
*Yibo Liu,Liam Shatzel,Brandon Haworth,Teseo Schneider*

Main category: cs.AI

TL;DR: 利用大型语言模型控制人群模拟中智能体的对话和导航，通过语言驱动的社交互动产生更真实的群体行为


<details>
  <summary>Details</summary>
Motivation: 现有的人群模拟方法主要关注避障和高级目标规划，忽略了语言对话对社交互动和导航决策的重要影响，导致智能体间互动有限

Method: 提出包含对话系统和语言驱动导航的两阶段方法：周期性查询基于智能体个性的LLM生成对话，然后结合对话内容、个性、情感状态、视觉和物理状态来控制导航和转向

Result: 在复杂场景中验证了方法有效性，观察到智能体自动分组和解组，实现了群体内的信息传递机制，产生了更真实的群体行为

Conclusion: 该方法通过语言模型实现了基于感知输入和持续对话的运动决策，能够自然产生涌现的群体行为，提升了人群模拟的真实性

Abstract: Animating and simulating crowds using an agent-based approach is a
well-established area where every agent in the crowd is individually controlled
such that global human-like behaviour emerges. We observe that human navigation
and movement in crowds are often influenced by complex social and environmental
interactions, driven mainly by language and dialogue. However, most existing
work does not consider these dimensions and leads to animations where
agent-agent and agent-environment interactions are largely limited to steering
and fixed higher-level goal extrapolation.
  We propose a novel method that exploits large language models (LLMs) to
control agents' movement. Our method has two main components: a dialogue system
and language-driven navigation. We periodically query agent-centric LLMs
conditioned on character personalities, roles, desires, and relationships to
control the generation of inter-agent dialogue when necessitated by the spatial
and social relationships with neighbouring agents. We then use the conversation
and each agent's personality, emotional state, vision, and physical state to
control the navigation and steering of each agent. Our model thus enables
agents to make motion decisions based on both their perceptual inputs and the
ongoing dialogue.
  We validate our method in two complex scenarios that exemplify the interplay
between social interactions, steering, and crowding. In these scenarios, we
observe that grouping and ungrouping of agents automatically occur.
Additionally, our experiments show that our method serves as an
information-passing mechanism within the crowd. As a result, our framework
produces more realistic crowd simulations, with emergent group behaviours
arising naturally from any environmental setting.

</details>


### [53] [Don't Think Twice! Over-Reasoning Impairs Confidence Calibration](https://arxiv.org/abs/2508.15050)
*Romain Lacombe,Kerrie Wu,Eddie Dilworth*

Main category: cs.AI

TL;DR: 研究发现推理预算增加反而损害LLM置信度校准，检索增强生成比纯推理表现更好，信息访问是置信度校准的关键瓶颈


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型作为问答工具时的置信度校准问题，特别是推理能力和计算预算如何影响置信度评估准确性

Method: 使用ClimateX数据集并扩展到人类和行星健康领域，系统评估不同推理预算下的置信度校准表现，比较纯推理与检索增强生成方法

Result: 推理LLM在专家置信度评估中达到48.7%准确率，但增加推理预算会系统性导致过度自信；检索增强生成达到89.3%准确率，显著优于纯推理

Conclusion: 信息访问而非推理深度或推理预算是改进知识密集型任务置信度校准的关键瓶颈，挑战了"测试时扩展"范式

Abstract: Large Language Models deployed as question answering tools require robust
calibration to avoid overconfidence. We systematically evaluate how reasoning
capabilities and budget affect confidence assessment accuracy, using the
ClimateX dataset (Lacombe et al., 2023) and expanding it to human and planetary
health. Our key finding challenges the "test-time scaling" paradigm: while
recent reasoning LLMs achieve 48.7% accuracy in assessing expert confidence,
increasing reasoning budgets consistently impairs rather than improves
calibration. Extended reasoning leads to systematic overconfidence that worsens
with longer thinking budgets, producing diminishing and negative returns beyond
modest computational investments. Conversely, search-augmented generation
dramatically outperforms pure reasoning, achieving 89.3% accuracy by retrieving
relevant evidence. Our results suggest that information access, rather than
reasoning depth or inference budget, may be the critical bottleneck for
improved confidence calibration of knowledge-intensive tasks.

</details>


### [54] [Demonstrating Onboard Inference for Earth Science Applications with Spectral Analysis Algorithms and Deep Learning](https://arxiv.org/abs/2508.15053)
*Itai Zilberstein,Alberto Candela,Steve Chien,David Rijlaarsdam,Tom Hendrix,Leonie Buckley,Aubrey Dunne*

Main category: cs.AI

TL;DR: 卡海天线上智能数据分析示范，通过身处边缘设备的神经网络加速硬件实现光谱分析和深度学习推理


<details>
  <summary>Details</summary>
Motivation: 在边缘设备上进行数据分析可以开启新的地球科学观测和响应能力

Method: 使用CogniSAT-6/HAMMER卫星上的可见光和近红外光谱仪器，结合神经网络加速硬件，实现深度学习和光谱分析算法的天线上推理

Result: 将在CS-6卫星上展示多种应用的数据分析和推理能力

Conclusion: 这项技术展示了在卫星边缘设备上进行光谱数据智能处理的可行性，为地球科学带来新的观测可能性

Abstract: In partnership with Ubotica Technologies, the Jet Propulsion Laboratory is
demonstrating state-of-the-art data analysis onboard CogniSAT-6/HAMMER (CS-6).
CS-6 is a satellite with a visible and near infrared range hyperspectral
instrument and neural network acceleration hardware. Performing data analysis
at the edge (e.g. onboard) can enable new Earth science measurements and
responses. We will demonstrate data analysis and inference onboard CS-6 for
numerous applications using deep learning and spectral analysis algorithms.

</details>


### [55] [S3LoRA: Safe Spectral Sharpness-Guided Pruning in Adaptation of Agent Planner](https://arxiv.org/abs/2508.15068)
*Shuang Ao,Gopal Rumchurn*

Main category: cs.AI

TL;DR: S3LoRA是一个轻量级、无需数据、模型无关的安全增强框架，通过分析LoRA微调权重更新来检测和修剪潜在不安全层，在保持任务性能的同时提升安全性。


<details>
  <summary>Details</summary>
Motivation: 现有的参数高效微调技术（如LoRA）在增强LLM能力的同时可能破坏安全对齐，导致不安全行为。现有安全适应方法需要访问基础模型和指令微调检查点，这在实践中往往不可用。

Method: 提出S3LoRA框架：1）MAS-SVD方法分析LoRA更新的结构特性；2）设计SSI指标检测高集中度潜在不安全层；3）事后修剪这些层来降低风险。

Result: 在智能体规划和语言生成任务上的广泛实验表明，S3LoRA能持续提升安全指标，同时保持或改进效用指标，并显著降低推理成本。

Conclusion: S3LoRA为在现实世界、资源受限和安全关键环境中安全部署基于LLM的智能体提供了一个实用且可扩展的解决方案。

Abstract: Adapting Large Language Models (LLMs) using parameter-efficient fine-tuning
(PEFT) techniques such as LoRA has enabled powerful capabilities in LLM-based
agents. However, these adaptations can unintentionally compromise safety
alignment, leading to unsafe or unstable behaviors, particularly in agent
planning tasks. Existing safety-aware adaptation methods often require access
to both base and instruction-tuned model checkpoints, which are frequently
unavailable in practice, limiting their applicability. We propose S3LoRA (Safe
Spectral Sharpness-Guided Pruning LoRA), a lightweight, data-free, and
model-independent framework that mitigates safety risks in LoRA-adapted models
by inspecting only the fine-tuned weight updates. We first introduce
Magnitude-Aware Spherically Normalized SVD (MAS-SVD), which robustly analyzes
the structural properties of LoRA updates while preserving global magnitude
information. We then design the Spectral Sharpness Index (SSI), a
sharpness-aware metric to detect layers with highly concentrated and
potentially unsafe updates. These layers are pruned post-hoc to reduce risk
without sacrificing task performance. Extensive experiments and ablation
studies across agent planning and language generation tasks show that S3LoRA
consistently improves safety metrics while maintaining or improving utility
metrics and significantly reducing inference cost. These results establish
S3LoRA as a practical and scalable solution for safely deploying LLM-based
agents in real-world, resource-constrained, and safety-critical environments.

</details>


### [56] [Argumentation for Explainable Workforce Optimisation (with Appendix)](https://arxiv.org/abs/2508.15118)
*Jennifer Leigh,Dimitrios Letsios,Alessandro Mella,Lucio Machetti,Francesca Toni*

Main category: cs.AI

TL;DR: 将劳动力管理模型化为抽象论证问题，通过提供忠实解释来处理执行时变化，实验证能提高问题解决效率和准确性


<details>
  <summary>Details</summary>
Motivation: 劳动力管理中的关键挑战是在执行时适应变化并向所有利益相关方提供解释

Method: 将劳动力管理问题理解为工业应用中的抽象论证模型

Result: 用户研究显示，该工具和解释方案比传统手工方式更快速、更准确地解决问题

Conclusion: 通过抽象论证方法来处理劳动力管理中的变化，能够提供忠实解释并提高系统性能

Abstract: Workforce management is a complex problem optimising the makespan and travel
distance required for a team of operators to complete a set of jobs, using a
set of instruments. A crucial challenge in workforce management is
accommodating changes at execution time so that explanations are provided to
all stakeholders involved. Here, we show that, by understanding workforce
management as abstract argumentation in an industrial application, we can
accommodate change and obtain faithful explanations. We show, with a user
study, that our tool and explanations lead to faster and more accurate problem
solving than conventional solutions by hand.

</details>


### [57] [Open-Universe Assistance Games](https://arxiv.org/abs/2508.15119)
*Rachel Ma,Jingyi Qu,Andreea Bobu,Dylan Hadfield-Menell*

Main category: cs.AI

TL;DR: 这篇论文提出了Open-Universe Assistance Games框架和GOOD方法，通过LLM模拟用户意图来推断自然语言目标，在文本购物和模拟家庭环境中表现超过基线方法。


<details>
  <summary>Details</summary>
Motivation: 体现式AI代理需要处理多样化且无法预先定义的人类目标和偏好，而现有方法对这种开放式环境的处理能力有限。

Method: 提出GOOD方法，利用LLM模拟具有不同复杂意图的用户，通过对话提取自然语言目标，并进行概率推断。该方法不需要大规模离线数据集。

Result: 在文本购物预设场景和AI2Thor模拟家庭机器人环境中评估，使用合成用户评价。GOOD方法在LLM和人类评估中都表现出更优的性能，超过了没有显式目标跟踪的基线方法。

Conclusion: GOOD方法能够在开放式环境中高效推断多样化的人类目标，提供了丰富的目标表征和不确定性估计，为体现式AI的目标推理提供了新的解决方案。

Abstract: Embodied AI agents must infer and act in an interpretable way on diverse
human goals and preferences that are not predefined. To formalize this setting,
we introduce Open-Universe Assistance Games (OU-AGs), a framework where the
agent must reason over an unbounded and evolving space of possible goals. In
this context, we introduce GOOD (GOals from Open-ended Dialogue), a
data-efficient, online method that extracts goals in the form of natural
language during an interaction with a human, and infers a distribution over
natural language goals. GOOD prompts an LLM to simulate users with different
complex intents, using its responses to perform probabilistic inference over
candidate goals. This approach enables rich goal representations and
uncertainty estimation without requiring large offline datasets. We evaluate
GOOD in a text-based grocery shopping domain and in a text-operated simulated
household robotics environment (AI2Thor), using synthetic user profiles. Our
method outperforms a baseline without explicit goal tracking, as confirmed by
both LLM-based and human evaluations.

</details>


### [58] [aiXiv: A Next-Generation Open Access Ecosystem for Scientific Discovery Generated by AI Scientists](https://arxiv.org/abs/2508.15126)
*Pengsong Zhang,Xiang Hu,Guowei Huang,Yang Qi,Heng Zhang,Xiuxu Li,Jiaxing Song,Jiabin Luo,Yijiang Li,Shuo Yin,Chengxiao Dai,Eric Hanchen Jiang,Xiaoyan Zhou,Zhenfei Yin,Boqin Yuan,Jing Dong,Guinan Su,Guanren Qiao,Haiming Tang,Anghong Du,Lili Pan,Zhenzhong Lan,Xinyu Liu*

Main category: cs.AI

TL;DR: aiXiv是一个为AI生成研究内容设计的下一代开放获取平台，通过多智能体架构实现人类和AI科学家的协同提交、评审和迭代改进研究内容


<details>
  <summary>Details</summary>
Motivation: 解决AI生成研究内容缺乏合适发布渠道的问题，传统期刊和会议难以接受AI生成内容，现有预印本服务器缺乏质量控制机制

Method: 采用多智能体架构，提供API和MCP接口，实现人类和AI科学家的无缝集成，支持研究提案和论文的提交、评审和迭代改进

Result: 实验证明aiXiv是一个可靠稳健的平台，能显著提升AI生成研究提案和论文的质量

Conclusion: 为AI科学家建立了下一代开放获取生态系统的基础，加速高质量AI生成研究内容的发布和传播

Abstract: Recent advances in large language models (LLMs) have enabled AI agents to
autonomously generate scientific proposals, conduct experiments, author papers,
and perform peer reviews. Yet this flood of AI-generated research content
collides with a fragmented and largely closed publication ecosystem.
Traditional journals and conferences rely on human peer review, making them
difficult to scale and often reluctant to accept AI-generated research content;
existing preprint servers (e.g. arXiv) lack rigorous quality-control
mechanisms. Consequently, a significant amount of high-quality AI-generated
research lacks appropriate venues for dissemination, hindering its potential to
advance scientific progress. To address these challenges, we introduce aiXiv, a
next-generation open-access platform for human and AI scientists. Its
multi-agent architecture allows research proposals and papers to be submitted,
reviewed, and iteratively refined by both human and AI scientists. It also
provides API and MCP interfaces that enable seamless integration of
heterogeneous human and AI scientists, creating a scalable and extensible
ecosystem for autonomous scientific discovery. Through extensive experiments,
we demonstrate that aiXiv is a reliable and robust platform that significantly
enhances the quality of AI-generated research proposals and papers after
iterative revising and reviewing on aiXiv. Our work lays the groundwork for a
next-generation open-access ecosystem for AI scientists, accelerating the
publication and dissemination of high-quality AI-generated research content.
Code is available at https://github.com/aixiv-org. Website is available at
https://forms.gle/DxQgCtXFsJ4paMtn8.

</details>


### [59] [Mobile-Agent-v3: Foundamental Agents for GUI Automation](https://arxiv.org/abs/2508.15144)
*Jiabo Ye,Xi Zhang,Haiyang Xu,Haowei Liu,Junyang Wang,Zhaoqing Zhu,Ziwei Zheng,Feiyu Gao,Junjie Cao,Zhengxi Lu,Jitong Liao,Qi Zheng,Fei Huang,Jingren Zhou,Ming Yan*

Main category: cs.AI

TL;DR: GUI-Owl是一个基础GUI代理模型，在10个GUI基准测试中达到最先进性能，Mobile-Agent-v3框架在此基础上进一步提升性能，创造了开源GUI代理框架的新纪录。


<details>
  <summary>Details</summary>
Motivation: 为了解决GUI环境中代理模型的性能瓶颈，需要开发一个能够跨平台（桌面和移动环境）执行多种任务（基础定位、问答、规划、决策等）的高性能端到端模型。

Method: 提出了三个关键创新：1）大规模环境基础设施（云虚拟环境+自演进GUI轨迹生产框架）；2）多样化基础代理能力（UI定位、规划、动作语义和推理模式集成）；3）可扩展环境强化学习（全异步训练+轨迹感知相对策略优化TRPO）。

Result: GUI-Owl-7B在AndroidWorld达到66.4分，在OSWorld达到29.4分；Mobile-Agent-v3进一步提升到AndroidWorld 73.3分和OSWorld 37.7分，创造了开源GUI代理框架的新纪录；在线RL达到OSWorld 34.9分。

Conclusion: GUI-Owl和Mobile-Agent-v3为GUI代理领域提供了强大的基础模型和框架，通过创新的数据生成方法、多能力集成和可扩展RL训练，显著提升了GUI代理的性能和实用性，并已开源供社区使用。

Abstract: This paper introduces GUI-Owl, a foundational GUI agent model that achieves
state-of-the-art performance among open-source end-to-end models on ten GUI
benchmarks across desktop and mobile environments, covering grounding, question
answering, planning, decision-making, and procedural knowledge. GUI-Owl-7B
achieves 66.4 on AndroidWorld and 29.4 on OSWorld. Building on this, we propose
Mobile-Agent-v3, a general-purpose GUI agent framework that further improves
performance to 73.3 on AndroidWorld and 37.7 on OSWorld, setting a new
state-of-the-art for open-source GUI agent frameworks. GUI-Owl incorporates
three key innovations: (1) Large-scale Environment Infrastructure: a
cloud-based virtual environment spanning Android, Ubuntu, macOS, and Windows,
enabling our Self-Evolving GUI Trajectory Production framework. This generates
high-quality interaction data via automated query generation and correctness
validation, leveraging GUI-Owl to refine trajectories iteratively, forming a
self-improving loop. It supports diverse data pipelines and reduces manual
annotation. (2) Diverse Foundational Agent Capabilities: by integrating UI
grounding, planning, action semantics, and reasoning patterns, GUI-Owl supports
end-to-end decision-making and can act as a modular component in multi-agent
systems. (3) Scalable Environment RL: we develop a scalable reinforcement
learning framework with fully asynchronous training for real-world alignment.
We also introduce Trajectory-aware Relative Policy Optimization (TRPO) for
online RL, achieving 34.9 on OSWorld. GUI-Owl and Mobile-Agent-v3 are
open-sourced at https://github.com/X-PLUG/MobileAgent.

</details>


### [60] [PuzzleClone: An SMT-Powered Framework for Synthesizing Verifiable Data](https://arxiv.org/abs/2508.15180)
*Kai Xiong,Yanwei Huang,Rongjunchen Zhang,Kun Chen,Haipang Wu*

Main category: cs.AI

TL;DR: PuzzleClone是一个基于SMT理论的框架，用于大规模生成可验证的数学逻辑谜题数据集，通过训练显著提升了LLM在逻辑推理任务上的表现


<details>
  <summary>Details</summary>
Motivation: 现有LLM生成的数学逻辑数据集存在可靠性、多样性和可扩展性有限的问题，需要高质量的可验证数据集来增强大语言模型的推理能力

Method: 使用Satisfiability Modulo Theories (SMT)框架，将种子谜题编码为结构化逻辑规范，通过系统变量和约束随机化生成可扩展变体，并通过重现机制确保有效性

Result: 构建了包含83K+多样化谜题的基准测试集，训练后PuzzleClone平均准确率从14.4提升到56.2，在7个逻辑数学基准上获得最高12.5个百分点的提升

Conclusion: PuzzleClone框架能够有效生成高质量可验证的数学逻辑数据集，显著提升LLM的逻辑推理能力，为解决数据可靠性问题提供了有效方案

Abstract: High-quality mathematical and logical datasets with verifiable answers are
essential for strengthening the reasoning capabilities of large language models
(LLMs). While recent data augmentation techniques have facilitated the creation
of large-scale benchmarks, existing LLM-generated datasets often suffer from
limited reliability, diversity, and scalability. To address these challenges,
we introduce PuzzleClone, a formal framework for synthesizing verifiable data
at scale using Satisfiability Modulo Theories (SMT). Our approach features
three key innovations: (1) encoding seed puzzles into structured logical
specifications, (2) generating scalable variants through systematic variable
and constraint randomization, and (3) ensuring validity via a reproduction
mechanism. Applying PuzzleClone, we construct a curated benchmark comprising
over 83K diverse and programmatically validated puzzles. The generated puzzles
span a wide spectrum of difficulty and formats, posing significant challenges
to current state-of-the-art models. We conduct post training (SFT and RL) on
PuzzleClone datasets. Experimental results show that training on PuzzleClone
yields substantial improvements not only on PuzzleClone testset but also on
logic and mathematical benchmarks. Post training raises PuzzleClone average
from 14.4 to 56.2 and delivers consistent improvements across 7 logic and
mathematical benchmarks up to 12.5 absolute percentage points (AMC2023 from
52.5 to 65.0). Our code and data are available at
https://github.com/puzzleclone.

</details>


### [61] [LLM4Sweat: A Trustworthy Large Language Model for Hyperhidrosis Support](https://arxiv.org/abs/2508.15192)
*Wenjie Lin,Jin Wei-Kocsis*

Main category: cs.AI

TL;DR: 这篇论文提出了LLM4Sweat框架，通过三阶段流水线使用前沿LLM生成合成数据，微调开源基础模型，为稀有疾病多汗痆提供准确、适当和共情的医疗支持。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型在医疗健康领域有应用潜力，但对于稀有疾病如多汗痆（影响2-3%人口），缺乏可靠的细调数据集仍是主要阻碍。目前没有专门为多汗痆诊断或照护而调整的LLM。

Method: 采用三阶段流水线：1）数据增帽阶段：使用前沿LLM从经过精选的开源数据生成医学合理的合成案例，创建多样化、平衡的问答数据集；2）微调阶段：在数据集上微调开源基础模型，提供诊断、个性化治疗建议和共情心理支持；3）推理与专家评估阶段：临床和心理专家评估准确性、适当性和共情能力，并将验证的响应迭代丰富数据集。

Result: 实验结果显示LLM4Sweat在性能上超过基线模型，成为首个为多汗痆提供支持的开源LLM框架。

Conclusion: LLM4Sweat为多汗痆诊断和照护提供了可信赖、共情的支持，并为其他面临类似数据和可靠性挑战的稀有疾病提供了可推广的方法。

Abstract: While large language models (LLMs) have shown promise in healthcare, their
application for rare medical conditions is still hindered by scarce and
unreliable datasets for fine-tuning. Hyperhidrosis, a disorder causing
excessive sweating beyond physiological needs, is one such rare disorder,
affecting 2-3% of the population and significantly impacting both physical
comfort and psychosocial well-being. To date, no work has tailored LLMs to
advance the diagnosis or care of hyperhidrosis. To address this gap, we present
LLM4Sweat, an open-source and domain-specific LLM framework for trustworthy and
empathetic hyperhidrosis support. The system follows a three-stage pipeline. In
the data augmentation stage, a frontier LLM generates medically plausible
synthetic vignettes from curated open-source data to create a diverse and
balanced question-answer dataset. In the fine-tuning stage, an open-source
foundation model is fine-tuned on the dataset to provide diagnosis,
personalized treatment recommendations, and empathetic psychological support.
In the inference and expert evaluation stage, clinical and psychological
specialists assess accuracy, appropriateness, and empathy, with validated
responses iteratively enriching the dataset. Experiments show that LLM4Sweat
outperforms baselines and delivers the first open-source LLM framework for
hyperhidrosis, offering a generalizable approach for other rare diseases with
similar data and trustworthiness challenges.

</details>


### [62] [R-ConstraintBench: Evaluating LLMs on NP-Complete Scheduling](https://arxiv.org/abs/2508.15204)
*Raj Jain,Marc Wetter*

Main category: cs.AI

TL;DR: 这篇论文提出了R-ConstraintBench框架，用于评估大语言模型在高约束条件下的排程推理能力，发现LLM在约束交互情况下性能显著下降。


<details>
  <summary>Details</summary>
Motivation: 现有研究对大语言模型在高约束条件下的排程推理可靠性评估不足，需要一个可扩展的框架来系统性地测试模型在资源约束项目排程问题中的表现。

Method: 开发了R-ConstraintBench框架，逐步增加非冗余的优先约束，然后引入停机时间、时间窗口和排他性约束，并在数据中心迁移场景中进行实例化验证。

Result: 强大模型在仅有优先约束的DAG中表现几乎完美，但当多种约束类型相互作用时，可行性性能快速呈现出崩溃趋势，约束交互是主要瓶颈而非图的深度。

Conclusion: 模型在清洁的合成数据上的表现并不能保证在领域特定场景中的迁移能力，显示了模型在复杂约束条件下的普遍化能力有限。

Abstract: Effective scheduling under tight resource, timing, and operational
constraints underpins large-scale planning across sectors such as capital
projects, manufacturing, logistics, and IT fleet transitions. However, the
reliability of large language models (LLMs) when reasoning under
high-constraint regimes is insufficiently characterized. To address this gap,
we present R-ConstraintBench, a scalable framework that evaluates models on
Resource-Constrained Project Scheduling Problems (RCPSP), an NP-Complete
feasibility class, while difficulty increases via linear growth in constraints.
R-ConstraintBench incrementally increases non-redundant precedence constraints
in Directed Acyclic Graphs (DAGs) and then introduces downtime, temporal
windows, and disjunctive constraints. As an illustrative example, we
instantiate the benchmark in a data center migration setting and evaluate
multiple LLMs using feasibility and error analysis, identifying degradation
thresholds and constraint types most associated with failure. Empirically,
strong models are near-ceiling on precedence-only DAGs, but feasibility
performance collapses when downtime, temporal windows, and disjunctive
constraints interact, implicating constraint interaction, not graph depth, as
the principal bottleneck. Performance on clean synthetic ramps also does not
guarantee transfer to domain-grounded scenarios, underscoring limited
generalization.

</details>


### [63] [See it. Say it. Sorted: Agentic System for Compositional Diagram Generation](https://arxiv.org/abs/2508.15222)
*Hantao Zhang,Jingyang Liu,Ed Li*

Main category: cs.AI

TL;DR: 提出See it. Say it. Sorted.系统，使用VLM和LLM协同工作，将手绘草图转换为精确可编辑的SVG图表，优于GPT-5和Gemini-2.5-Pro等前沿模型。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在照片级真实感方面表现出色，但在处理流程图等需要空间精度、对齐和符号结构的任务时存在困难，需要开发能够生成精确可编辑图表的方法。

Method: 训练免费的代理系统，结合视觉语言模型(VLM)和大语言模型(LLM)，通过迭代循环：批评VLM提出定性编辑建议，多个候选LLM合成SVG更新，法官VLM选择最佳候选，确保稳定改进。

Result: 在10个从已发表论文流程图衍生的草图上，该方法比GPT-5和Gemini-2.5-Pro更忠实地重建布局和结构，准确组合图元而不插入不需要的文本。

Conclusion: 该方法优先考虑定性推理而非脆弱的数值估计，保持全局约束，支持人机协同修正，输出为程序化SVG，可通过API扩展到演示工具，代码已开源。

Abstract: We study sketch-to-diagram generation: converting rough hand sketches into
precise, compositional diagrams. Diffusion models excel at photorealism but
struggle with the spatial precision, alignment, and symbolic structure required
for flowcharts. We introduce See it. Say it. Sorted., a training-free agentic
system that couples a Vision-Language Model (VLM) with Large Language Models
(LLMs) to produce editable Scalable Vector Graphics (SVG) programs. The system
runs an iterative loop in which a Critic VLM proposes a small set of
qualitative, relational edits; multiple candidate LLMs synthesize SVG updates
with diverse strategies (conservative->aggressive, alternative, focused); and a
Judge VLM selects the best candidate, ensuring stable improvement. This design
prioritizes qualitative reasoning over brittle numerical estimates, preserves
global constraints (e.g., alignment, connectivity), and naturally supports
human-in-the-loop corrections. On 10 sketches derived from flowcharts in
published papers, our method more faithfully reconstructs layout and structure
than two frontier closed-source image generation LLMs (GPT-5 and
Gemini-2.5-Pro), accurately composing primitives (e.g., multi-headed arrows)
without inserting unwanted text. Because outputs are programmatic SVGs, the
approach is readily extensible to presentation tools (e.g., PowerPoint) via
APIs and can be specialized with improved prompts and task-specific tools. The
codebase is open-sourced at
https://github.com/hantaoZhangrichard/see_it_say_it_sorted.git.

</details>


### [64] [Computational Intelligence based Land-use Allocation Approaches for Mixed Use Areas](https://arxiv.org/abs/2508.15240)
*Sabab Aosaf,Muhammad Ali Nayeem,Afsana Haque,M Sohel Rahmana*

Main category: cs.AI

TL;DR: 本文提出多种计算智能算法优化混合用地土地利用分配，CR+DES算法在土地利用兼容性上提升3.16%，MSBX+MO在价格优化上提升3.3%，约束松弛策略有效扩展解空间。


<details>
  <summary>Details</summary>
Motivation: 解决城市土地利用分配中土地利用兼容性与经济目标之间的复杂多目标优化问题，为可持续城市发展政策提供支持。

Method: 开发多种优化算法，包括结合差分进化和多目标遗传算法的定制变体，采用约束松弛策略，并使用Kruskal-Wallis检验进行统计验证。

Result: 在1,290个地块的真实案例中，CR+DES算法在土地利用兼容性上比现有方法提升3.16%，MSBX+MO在价格优化上提升3.3%，统计验证显示含差分向量的算法显著优于传统方法。

Conclusion: 这些算法为城市规划者和政策制定者提供了基于证据的计算工具，可在快速城市化地区更有效地平衡土地利用分配中的竞争目标。

Abstract: Urban land-use allocation represents a complex multi-objective optimization
problem critical for sustainable urban development policy. This paper presents
novel computational intelligence approaches for optimizing land-use allocation
in mixed-use areas, addressing inherent trade-offs between land-use
compatibility and economic objectives. We develop multiple optimization
algorithms, including custom variants integrating differential evolution with
multi-objective genetic algorithms. Key contributions include: (1) CR+DES
algorithm leveraging scaled difference vectors for enhanced exploration, (2)
systematic constraint relaxation strategy improving solution quality while
maintaining feasibility, and (3) statistical validation using Kruskal-Wallis
tests with compact letter displays. Applied to a real-world case study with
1,290 plots, CR+DES achieves 3.16\% improvement in land-use compatibility
compared to state-of-the-art methods, while MSBX+MO excels in price
optimization with 3.3\% improvement. Statistical analysis confirms algorithms
incorporating difference vectors significantly outperform traditional
approaches across multiple metrics. The constraint relaxation technique enables
broader solution space exploration while maintaining practical constraints.
These findings provide urban planners and policymakers with evidence-based
computational tools for balancing competing objectives in land-use allocation,
supporting more effective urban development policies in rapidly urbanizing
regions.

</details>


### [65] [Multiple Memory Systems for Enhancing the Long-term Memory of Agent](https://arxiv.org/abs/2508.15294)
*Gaoke Zhang,Bo Wang,Yunlong Ma,Dongming Zhao,Zifei Yu*

Main category: cs.AI

TL;DR: 提出基于认知心理学理论的多重记忆系统(MMS)，通过处理短期记忆为多个长期记忆片段，构建检索记忆单元和上下文记忆单元，有效提升智能代理对历史数据的利用效率。


<details>
  <summary>Details</summary>
Motivation: 现有智能代理在处理大量历史交互数据时存在存储记忆内容质量差的问题，影响召回性能和响应质量，需要更好的长期记忆构建方法。

Method: 设计多重记忆系统(MMS)，将短期记忆处理为多个长期记忆片段，构建一一对应的检索记忆单元和上下文记忆单元，通过检索匹配最相关的记忆单元来增强响应上下文。

Result: 在LoCoMo数据集上的实验证明该方法优于其他三种方法，消融研究验证了记忆单元设计的合理性，并分析了记忆片段数量和存储开销的鲁棒性。

Conclusion: MMS系统能够有效构建高质量长期记忆内容，提升智能代理对历史数据的利用效率，具有实际应用价值。

Abstract: An agent powered by large language models have achieved impressive results,
but effectively handling the vast amounts of historical data generated during
interactions remains a challenge. The current approach is to design a memory
module for the agent to process these data. However, existing methods, such as
MemoryBank and A-MEM, have poor quality of stored memory content, which affects
recall performance and response quality. In order to better construct
high-quality long-term memory content, we have designed a multiple memory
system (MMS) inspired by cognitive psychology theory. The system processes
short-term memory to multiple long-term memory fragments, and constructs
retrieval memory units and contextual memory units based on these fragments,
with a one-to-one correspondence between the two. During the retrieval phase,
MMS will match the most relevant retrieval memory units based on the user's
query. Then, the corresponding contextual memory units is obtained as the
context for the response stage to enhance knowledge, thereby effectively
utilizing historical data. Experiments on LoCoMo dataset compared our method
with three others, proving its effectiveness. Ablation studies confirmed the
rationality of our memory units. We also analyzed the robustness regarding the
number of selected memory segments and the storage overhead, demonstrating its
practical value.

</details>


### [66] [Coarse-to-Fine Grounded Memory for LLM Agent Planning](https://arxiv.org/abs/2508.15305)
*Wei Yang,Jinwei Xiao,Hongming Zhang,Qingyang Zhang,Yanna Wang,Bo Xu*

Main category: cs.AI

TL;DR: 提出了Coarse-to-Fine Grounded Memory框架，通过粗粒度到细粒度的记忆机制增强LLM在复杂规划任务中的表现


<details>
  <summary>Details</summary>
Motivation: 现有基于记忆机制的LLM智能体主要依赖单一粒度的记忆，受限于收集经验的质量，限制了知识多样性和规划灵活性

Method: 采用粗粒度到细粒度的记忆机制：1）在训练任务中将环境信息转化为粗粒度关注点指导经验收集；2）从每个经验中提取可操作的混合粒度提示；3）推理时检索任务相关经验和提示；4）面对环境异常时进行细粒度关键信息提取和自问答反思

Result: 该方法能够充分利用记忆进行灵活的场景适应，支持规划过程中的自我修正

Conclusion: Coarse-to-Fine Grounded Memory框架通过多粒度记忆机制有效提升了LLM智能体在复杂规划任务中的适应性和灵活性

Abstract: Recent advancements in Large Language Models (LLMs) have driven growing
interest in LLM-based agents for complex planning tasks. To avoid costly agent
training, many studies adopted memory mechanism that enhances LLM with offline
experiences or online trajectory analysis. However, existing works focus on
single-granularity memory derived from dynamic environmental interactions,
which are inherently constrained by the quality of the collected experiences.
This limitation, in turn, constrain the diversity of knowledge and the
flexibility of planning. We propose Coarse-to-Fine Grounded Memory (\Ours{}), a
novel framework that grounds coarse-to-fine memories with LLM, thereby fully
leverage them for flexible adaptation to diverse scenarios. \Ours{} grounds
environmental information into coarse-grained focus points to guide experience
collection in training tasks, followed by grounding of actionable
hybrid-grained tips from each experience. At inference, \Ours{} retrieves
task-relevant experiences and tips to support planning. When facing
environmental anomalies, the LLM grounds the current situation into
fine-grained key information, enabling flexible self-QA reflection and plan
correction.

</details>


### [67] [Search-Based Credit Assignment for Offline Preference-Based Reinforcement Learning](https://arxiv.org/abs/2508.15327)
*Xiancheng Gao,Yufeng Shi,Wengang Zhou,Houqiang Li*

Main category: cs.AI

TL;DR: 提出SPW方法统一专家演示和偏好两种人类反馈，通过相似性搜索实现更准确的信用分配，在机器人操作任务中优于现有方法


<details>
  <summary>Details</summary>
Motivation: 离线强化学习通常依赖精心设计的奖励函数，但设计成本高昂。人类反馈是替代方案，但专家演示和偏好各有局限：演示成本高且行为模式有限，偏好易收集但信用分配困难

Method: 提出Search-Based Preference Weighting (SPW)方案，为偏好标注轨迹中的每个转移，从专家演示中搜索最相似的状态-动作对，基于相似性得分直接推导逐步重要性权重，指导标准偏好学习

Result: SPW能够在具有挑战性的机器人操作任务中，有效联合学习偏好和演示，性能优于同时利用两种反馈类型的现有方法

Conclusion: SPW方案成功解决了人类反馈中信用分配的问题，统一了两种反馈源的优势，为离线强化学习提供了更有效的学习框架

Abstract: Offline reinforcement learning refers to the process of learning policies
from fixed datasets, without requiring additional environment interaction.
However, it often relies on well-defined reward functions, which are difficult
and expensive to design. Human feedback is an appealing alternative, but its
two common forms, expert demonstrations and preferences, have complementary
limitations. Demonstrations provide stepwise supervision, but they are costly
to collect and often reflect limited expert behavior modes. In contrast,
preferences are easier to collect, but it is unclear which parts of a behavior
contribute most to a trajectory segment, leaving credit assignment unresolved.
In this paper, we introduce a Search-Based Preference Weighting (SPW) scheme to
unify these two feedback sources. For each transition in a preference labeled
trajectory, SPW searches for the most similar state-action pairs from expert
demonstrations and directly derives stepwise importance weights based on their
similarity scores. These weights are then used to guide standard preference
learning, enabling more accurate credit assignment that traditional approaches
struggle to achieve. We demonstrate that SPW enables effective joint learning
from preferences and demonstrations, outperforming prior methods that leverage
both feedback types on challenging robot manipulation tasks.

</details>


### [68] [RETAIL: Towards Real-world Travel Planning for Large Language Models](https://arxiv.org/abs/2508.15335)
*Bin Deng,Yizhe Feng,Zeming Liu,Qing Wei,Xiangrong Zhu,Shuai Chen,Yuanfang Guo,Yunhong Wang*

Main category: cs.AI

TL;DR: 本文提出了RETAIL数据集和TGMA多智能体框架，解决了现有旅行规划系统在隐式查询、环境因素和详细规划方面的不足，显著提升了真实场景下的旅行规划性能。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型在旅行规划中存在三个主要问题：1）假设用户提供显式查询，而现实中需求往往是隐式的；2）忽略环境因素和用户偏好，导致计划不可行；3）只能生成基本的POI安排，无法提供包含丰富细节的一体化计划。

Method: 构建了RETAIL数据集支持隐式和显式查询的决策，包含修订需求和环境感知能力；提出了主题引导的多智能体框架TGMA来处理复杂的旅行规划任务。

Result: 实验显示现有最强模型仅达到1.0%的通过率，而TGMA框架将性能显著提升至2.72%，证明了其在真实世界旅行规划中的有效性。

Conclusion: TGMA框架为真实世界旅行规划提供了有前景的方向，通过多智能体协作和环境感知能力，显著改善了旅行规划系统的实用性和可行性。

Abstract: Although large language models have enhanced automated travel planning
abilities, current systems remain misaligned with real-world scenarios. First,
they assume users provide explicit queries, while in reality requirements are
often implicit. Second, existing solutions ignore diverse environmental factors
and user preferences, limiting the feasibility of plans. Third, systems can
only generate plans with basic POI arrangements, failing to provide all-in-one
plans with rich details. To mitigate these challenges, we construct a novel
dataset \textbf{RETAIL}, which supports decision-making for implicit queries
while covering explicit queries, both with and without revision needs. It also
enables environmental awareness to ensure plan feasibility under real-world
scenarios, while incorporating detailed POI information for all-in-one travel
plans. Furthermore, we propose a topic-guided multi-agent framework, termed
TGMA. Our experiments reveal that even the strongest existing model achieves
merely a 1.0% pass rate, indicating real-world travel planning remains
extremely challenging. In contrast, TGMA demonstrates substantially improved
performance 2.72%, offering promising directions for real-world travel
planning.

</details>


### [69] [DiagECG: An LLM-Driven Framework for Diagnostic Reasoning via Discretized ECG Tokenization](https://arxiv.org/abs/2508.15338)
*Jinning Yang,Wen Shi*

Main category: cs.AI

TL;DR: DiagECG是一个将12导联心电图信号与语言模型结合的框架，通过离散化ECG嵌入为符号标记，扩展LLM词汇表来处理ECG和自然语言输入，在ECG问答和诊断报告生成任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有自动化心电图分析方法在跨临床任务泛化能力和开放式推理方面存在局限，需要一种能够统一处理心电图信号和自然语言的框架来改善心血管诊断。

Method: 使用导联无关编码器和量化模块将连续ECG嵌入离散化为符号标记，扩展LLM词汇表；通过自回归ECG预测任务进行预训练，利用LLM的语言建模能力建模时序动态；最后在ECG问答和诊断报告生成任务上进行指令微调。

Result: DiagECG在不修改核心模型的情况下，在多个任务上取得了强劲性能，并保持了在分布外设置下的泛化能力。

Conclusion: 该研究展示了将符号化ECG表示整合到LLM中进行医学推理的潜力，为心血管诊断提供了有效的多模态处理框架。

Abstract: Electrocardiography plays a central role in cardiovascular diagnostics, yet
existing automated approaches often struggle to generalize across clinical
tasks and offer limited support for open-ended reasoning. We present DiagECG, a
novel framework that integrates time-series and language modeling by enabling
large language models to process 12-lead ECG signals for clinical text
generation tasks. Our approach discretizes continuous ECG embeddings into
symbolic tokens using a lead-independent encoder and quantization module. These
tokens are then used to extend the vocabulary of LLM, allowing the model to
handle both ECG and natural language inputs in a unified manner. To bridge the
modality gap, we pretrain the model on an autoregressive ECG forecasting task,
enabling the LLM to model temporal dynamics using its native language modeling
capabilities. Finally, we perform instruction tuning on both ECG question
answering and diagnostic report generation. Without modifying the core model,
DiagECG achieves strong performance across tasks while maintaining
generalization to out-of-distribution settings. Extensive experiments
demonstrate the effectiveness of each component and highlight the potential of
integrating symbolic ECG representations into LLMs for medical reasoning.

</details>


### [70] [Planning with Minimal Disruption](https://arxiv.org/abs/2508.15358)
*Alberto Pozanco,Marianela Morales,Daniel Borrajo,Manuela Veloso*

Main category: cs.AI

TL;DR: 本文提出了计划扰动概念，旨在寻找最小化初始状态修改来实现目标的计划，并通过规划编译方法联合优化行动成本和计划扰动。


<details>
  <summary>Details</summary>
Motivation: 在许多规划应用中，需要找到既能实现目标又对初始状态改动最小的计划，这种平衡行动成本和状态扰动的需求是研究动机。

Method: 通过定义计划扰动概念，开发多种基于规划的编译方法，将原始规划问题重新表述为同时优化行动成本和状态扰动的联合优化问题。

Result: 在不同基准测试中的实验结果表明，重新表述的任务能够有效求解，生成平衡两个目标的计划。

Conclusion: 计划扰动是一个有用的规划概念，提出的编译方法能够实际有效地生成同时优化行动成本和状态扰动的平衡计划。

Abstract: In many planning applications, we might be interested in finding plans that
minimally modify the initial state to achieve the goals. We refer to this
concept as plan disruption. In this paper, we formally introduce it, and define
various planning-based compilations that aim to jointly optimize both the sum
of action costs and plan disruption. Experimental results in different
benchmarks show that the reformulated task can be effectively solved in
practice to generate plans that balance both objectives.

</details>


### [71] [GraSP: A Unified Graph-Based Framework for Scalable Generation, Quality Tagging, and Management of Synthetic Data for SFT and DPO](https://arxiv.org/abs/2508.15432)
*Bidyapati Pradhan,Surajit Dasgupta,Amit Kumar Saha,Omkar Anustoop,Sriram Puttagunta,Vipul Mittal,Gopal Sarda*

Main category: cs.AI

TL;DR: 这篇论文提出了一种综合性的合成数据生成框架，用于为LLM的SFT和DPO对齐任务生成高质量的对话数据。该框架通过模块化管道和双阶段质量标签机制，能够自动生成、迅透和筛选高质量对话样本。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的发展极依赖于高质量的SFT和对齐数据集，但手动准备这些数据耗费巨大。需要一种可扩展、可配置的方法来生成高保真度的合成对话数据。

Method: 采用模块化和配置基础的管道，能够建模复杂对话流程。使用双阶段质量标签机制，结合假设规则和LLM基于评估来自动筛选和评分从OASST格式对话中提取的数据。

Result: 生成的数据集采用灵活的结构支持SFT和DPO使用场景，能够无缝集成到多样化的训练工作流中。

Conclusion: 这些创新提供了一种健壮的解决方案，能够大规模生成和管理合成对话数据，显著减少LLM训练管道中数据准备的开销。

Abstract: The advancement of large language models (LLMs) is critically dependent on
the availability of high-quality datasets for Supervised Fine-Tuning (SFT),
alignment tasks like Direct Preference Optimization (DPO), etc. In this work,
we present a comprehensive synthetic data generation framework that facilitates
scalable, configurable, and high-fidelity generation of synthetic data tailored
for these training paradigms. Our approach employs a modular and
configuration-based pipeline capable of modeling complex dialogue flows with
minimal manual intervention. This framework uses a dual-stage quality tagging
mechanism, combining heuristic rules and LLM-based evaluations, to
automatically filter and score data extracted from OASST-formatted
conversations, ensuring the curation of high-quality dialogue samples. The
resulting datasets are structured under a flexible schema supporting both SFT
and DPO use cases, enabling seamless integration into diverse training
workflows. Together, these innovations offer a robust solution for generating
and managing synthetic conversational data at scale, significantly reducing the
overhead of data preparation in LLM training pipelines.

</details>


### [72] [From Bits to Boardrooms: A Cutting-Edge Multi-Agent LLM Framework for Business Excellence](https://arxiv.org/abs/2508.15447)
*Zihao Wang,Junming Zhang*

Main category: cs.AI

TL;DR: BusiAgent是一个基于大语言模型的多智能体框架，通过CTMDP建模、熵优化和Stackelberg博弈等技术，显著提升企业决策质量和协作效率。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在企业决策支持中存在运营分析与战略目标脱节、工作流程碎片化、跨组织协作不足等问题，需要新的框架来整合细粒度洞察与高层战略。

Method: 提出BusiAgent多智能体框架，包含：扩展的连续时间马尔可夫决策过程(CTMDP)用于动态建模、广义熵度量优化协作效率、多层次Stackelberg博弈处理层级决策、上下文Thompson采样进行提示优化，以及全面的质量保证系统。

Result: 在多样化商业场景中的实证评估表明，BusiAgent能够生成连贯的、以客户为中心的解决方案，在解决方案质量和用户满意度方面显著优于现有方法。

Conclusion: BusiAgent通过融合前沿AI技术与深度商业洞察，在AI驱动的企业决策领域迈出了重要一步，使组织能够更有效地应对复杂商业环境。

Abstract: Large Language Models (LLMs) have shown promising potential in business
applications, particularly in enterprise decision support and strategic
planning, yet current approaches often struggle to reconcile intricate
operational analyses with overarching strategic goals across diverse market
environments, leading to fragmented workflows and reduced collaboration across
organizational levels. This paper introduces BusiAgent, a novel multi-agent
framework leveraging LLMs for advanced decision-making in complex corporate
environments. BusiAgent integrates three core innovations: an extended
Continuous Time Markov Decision Process (CTMDP) for dynamic agent modeling, a
generalized entropy measure to optimize collaborative efficiency, and a
multi-level Stackelberg game to handle hierarchical decision processes.
Additionally, contextual Thompson sampling is employed for prompt optimization,
supported by a comprehensive quality assurance system to mitigate errors.
Extensive empirical evaluations across diverse business scenarios validate
BusiAgent's efficacy, demonstrating its capacity to generate coherent,
client-focused solutions that smoothly integrate granular insights with
high-level strategy, significantly outperforming established approaches in both
solution quality and user satisfaction. By fusing cutting-edge AI technologies
with deep business insights, BusiAgent marks a substantial step forward in
AI-driven enterprise decision-making, empowering organizations to navigate
complex business landscapes more effectively.

</details>


### [73] [Think in Blocks: Adaptive Reasoning from Direct Response to Deep Reasoning](https://arxiv.org/abs/2508.15507)
*Yekun Zhu,Guang Chen,Chengjun Mao*

Main category: cs.AI

TL;DR: 提出了Think in Blocks框架，通过将推理过程划分为可调数量的块，使LLM能够根据任务复杂度动态调整推理长度，避免过度思考。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在链式思维推理中因推理链过长导致的过度思考问题，提高计算效率和响应速度。

Method: 建立块结构范式：预测推理预算（块数），按块划分推理；三阶段训练流程（监督微调、奖励引导的DPO、强化学习）；利用显式块数在推理时动态控制深度。

Result: 实现了自适应推理深度调整，能够根据问题难度灵活控制推理链长度。

Conclusion: Think in Blocks框架有效解决了LLM过度思考问题，提供了动态调整推理深度的实用解决方案。

Abstract: Large Language Models (LLMs) with chains-of-thought have demonstrated strong
performance on an increasing range of tasks, particularly those involving
complex logical reasoning. However, excessively long chains can lead to
overthinking, causing computational waste and slower responses. This raises a
question: can LLMs dynamically adjust the length of their reasoning processes
based on task complexity? To address this, we propose the Think in Blocks
framework, which enables adaptive reasoning-from zero to deep reasoning-by
partitioning the reasoning process into a tunable number of blocks. Our main
contributions are: (1) Establishing an explicit block-structured paradigm in
which the model first predicts an integer reasoning budget-the number of
blocks-and then partitions its reasoning accordingly; (2) Training an adaptive
model through a three-stage pipeline-Supervised Fine-Tuning, reward-guided
Direct Preference Optimization, and Reinforcement Learning-that adjusts its
reasoning depth to problem difficulty; (3) Exploiting the explicit block count
to dynamically control reasoning depth at inference time, allowing flexible
adjustment of chain-of-thought length during deployment.

</details>


### [74] [Super-additive Cooperation in Language Model Agents](https://arxiv.org/abs/2508.15510)
*Filippo Tonini,Lukas Galke*

Main category: cs.AI

TL;DR: 研究通过模拟囚徒困境锦标赛发现，语言模型代理在团队内部动态和外部竞争的共同作用下，合作水平显著提升，包括一次性互动中的合作倾向。


<details>
  <summary>Details</summary>
Motivation: 随着自主AI代理的发展，研究其合作行为倾向变得越来越重要。受超加性合作理论启发，探索重复互动和群体间竞争如何影响AI代理的合作行为。

Method: 设计虚拟锦标赛，将语言模型代理分组为团队，在囚徒困境游戏中相互对抗，同时模拟团队内部动态和外部竞争。

Result: 内部团队动态和外部竞争的结合显著提高了整体合作水平和一次性互动中的合作倾向。

Conclusion: 研究为语言模型在复杂社会场景中制定策略提供了新框架，证明群体间竞争可以反直觉地导致更多合作行为，对设计未来多代理AI系统具有重要意义。

Abstract: With the prospect of autonomous artificial intelligence (AI) agents, studying
their tendency for cooperative behavior becomes an increasingly relevant topic.
This study is inspired by the super-additive cooperation theory, where the
combined effects of repeated interactions and inter-group rivalry have been
argued to be the cause for cooperative tendencies found in humans. We devised a
virtual tournament where language model agents, grouped into teams, face each
other in a Prisoner's Dilemma game. By simulating both internal team dynamics
and external competition, we discovered that this blend substantially boosts
both overall and initial, one-shot cooperation levels (the tendency to
cooperate in one-off interactions). This research provides a novel framework
for large language models to strategize and act in complex social scenarios and
offers evidence for how intergroup competition can, counter-intuitively, result
in more cooperative behavior. These insights are crucial for designing future
multi-agent AI systems that can effectively work together and better align with
human values. Source code is available at
https://github.com/pippot/Superadditive-cooperation-LLMs.

</details>


### [75] [DeepThink3D: Enhancing Large Language Models with Programmatic Reasoning in Complex 3D Situated Reasoning Tasks](https://arxiv.org/abs/2508.15548)
*Jiayi Song,Rui Wan,Lipeng Ma,Weidong Yang,Qingyuan Zhou,Yixuan Li,Ben Fei*

Main category: cs.AI

TL;DR: DeepThink3D通过组合迭代进化方法生成更复杂的3D场景推理问题，并使用DPO直接优化大语言模型的工具链策略，提升其在复杂3D推理任务中的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在3D场景推理任务中通过大语言模型调用工具，但由于数据集问题简单，生成的程序推理链较短，无法处理复杂推理任务。

Method: 1. 在SQA3D基准上采用组合迭代进化方法生成更复杂的问题；2. 通过直接偏好优化(DPO)微调大语言模型，直接优化工具链策略。

Result: 该方法增强了LLMs在复杂3D场景推理中的工具使用能力，提高了在复杂任务中的准确性。

Conclusion: DeepThink3D通过生成复杂问题和直接优化工具链策略，有效提升了大语言模型在3D场景复杂推理任务中的性能。

Abstract: This work enhances the ability of large language models (LLMs) to perform
complex reasoning in 3D scenes. Recent work has addressed the 3D situated
reasoning task by invoking tool usage through large language models. Large
language models call tools via APIs and integrate the generated programs
through a chain of thought to solve problems based on the program results.
However, due to the simplicity of the questions in the dataset, the generated
program reasoning chains are relatively short. To solve this main challenge, in
this paper, we introduce DeepThink3D to enhance the tool usage of LLMs in
complex 3D situated reasoning tasks. Our work proposes a combinatorial and
iterative evolutionary approach on the SQA3D benchmark to generate more complex
questions. Building on this foundation, we fine-tune the large language model
to make it more proficient in using 3D tools. By employing Direct Preference
Optimization (DPO), we directly optimize the toolchain strategies generated by
models, thereby enhancing their accuracy in complex tasks.

</details>


### [76] [A Dynamical Systems Framework for Reinforcement Learning Safety and Robustness Verification](https://arxiv.org/abs/2508.15588)
*Ahmed Nasir,Abdelhafid Zenati*

Main category: cs.AI

TL;DR: 本文提出了一个基于动力学系统理论的框架，利用有限时间李雅普诺夫指数和拉格朗日相干结构来形式化验证强化学习策略的安全性和鲁棒性，并引入定量指标来测量安全边界。


<details>
  <summary>Details</summary>
Motivation: 强化学习在安全关键系统中的应用受到缺乏形式化验证方法的限制，需要开发能够验证学习策略鲁棒性和安全性的框架。

Method: 将RL智能体与环境组合视为离散时间自治动力系统，利用有限时间李雅普诺夫指数识别拉格朗日相干结构，引入MBR、ASAS、TASAS等定量指标，并提供局部稳定性保证和模型不确定性处理方法。

Result: 在离散和连续控制环境中的实验表明，该框架能够全面可解释地评估策略行为，成功识别仅基于奖励看似成功但存在关键缺陷的策略。

Conclusion: 该框架为强化学习策略提供了形式化的安全验证方法，能够发现隐藏的安全风险，超越了传统的定性可视化分析。

Abstract: The application of reinforcement learning to safety-critical systems is
limited by the lack of formal methods for verifying the robustness and safety
of learned policies. This paper introduces a novel framework that addresses
this gap by analyzing the combination of an RL agent and its environment as a
discrete-time autonomous dynamical system. By leveraging tools from dynamical
systems theory, specifically the Finite-Time Lyapunov Exponent (FTLE), we
identify and visualize Lagrangian Coherent Structures (LCS) that act as the
hidden "skeleton" governing the system's behavior. We demonstrate that
repelling LCS function as safety barriers around unsafe regions, while
attracting LCS reveal the system's convergence properties and potential failure
modes, such as unintended "trap" states. To move beyond qualitative
visualization, we introduce a suite of quantitative metrics, Mean Boundary
Repulsion (MBR), Aggregated Spurious Attractor Strength (ASAS), and
Temporally-Aware Spurious Attractor Strength (TASAS), to formally measure a
policy's safety margin and robustness. We further provide a method for deriving
local stability guarantees and extend the analysis to handle model uncertainty.
Through experiments in both discrete and continuous control environments, we
show that this framework provides a comprehensive and interpretable assessment
of policy behavior, successfully identifying critical flaws in policies that
appear successful based on reward alone.

</details>


### [77] [Transduction is All You Need for Structured Data Workflows](https://arxiv.org/abs/2508.15610)
*Alfio Gliozzo,Naweed Khan,Christodoulos Constantinides,Nandana Mihindukulasooriya,Nahuel Defosse,Junkyu Lee*

Main category: cs.AI

TL;DR: Agentics是一个模块化框架，用于构建基于代理的系统，支持结构化推理和组合泛化，通过数据建模而非提示工程来实现逻辑转换。


<details>
  <summary>Details</summary>
Motivation: 为了解决复杂数据处理中结构化推理和组合泛化的挑战，提供一个让AI开发者专注于数据建模而非提示工程的框架。

Method: 采用模块化框架设计，将代理从逻辑流中抽象出来，在数据类型内部使用代理实现逻辑转换，通过LLM提供数据类型并通过逻辑转换组合。

Result: 在领域特定多选题回答、文本到SQL的语义解析和自动提示优化任务中达到最先进精度或改进可扩展性而不牺牲性能。

Conclusion: Agentics框架为AI工作流提供了新的数据建模视角，通过声明式语言和逻辑转换机制有效提升了处理复杂数据的能力。

Abstract: This paper introduces Agentics, a modular framework for building agent-based
systems capable of structured reasoning and compositional generalization over
complex data. Designed with research and practical applications in mind,
Agentics offers a novel perspective on working with data and AI workflows. In
this framework, agents are abstracted from the logical flow and they are used
internally to the data type to enable logical transduction among data. Agentics
encourages AI developers to focus on modeling data rather than crafting
prompts, enabling a declarative language in which data types are provided by
LLMs and composed through logical transduction, which is executed by LLMs when
types are connected. We provide empirical evidence demonstrating the
applicability of this framework across domain-specific multiple-choice question
answering, semantic parsing for text-to-SQL, and automated prompt optimization
tasks, achieving state-of-the-art accuracy or improved scalability without
sacrificing performance. The open-source implementation is available at
\texttt{https://github.com/IBM/agentics}.

</details>


### [78] [Adapting A Vector-Symbolic Memory for Lisp ACT-R](https://arxiv.org/abs/2508.15630)
*Meera Ray,Christopher L. Dancy*

Main category: cs.AI

TL;DR: HDM是ACT-R声明性记忆系统的向量符号替代方案，具有可扩展性和架构定义的相似性优势，已成功适配到Lisp ACT-R中，使现有模型无需重大修改即可运行。


<details>
  <summary>Details</summary>
Motivation: 开发向量符号的声明性记忆系统来替代ACT-R的传统DM系统，以获得更好的可扩展性、相似性处理能力，同时保持与现有ACT-R模型的兼容性。

Method: 将HDM适配到Lisp ACT-R实现中，开发基于向量的ACT-R函数，建立文本处理流水线，创建基于向量表示的完整记忆块检索机制。

Result: 初步结果表明HDM能够保持向量符号优势（如无需存储实际块即可回忆），同时使之前的ACT-R模型只需很少修改就能在该系统上工作。

Conclusion: HDM成功实现了向量符号记忆系统与ACT-R的集成，未来将继续改进时间上下文表示和开发基于实例学习的决策模型来进一步测试系统。

Abstract: Holographic Declarative Memory (HDM) is a vector-symbolic alternative to
ACT-R's Declarative Memory (DM) system that can bring advantages such as
scalability and architecturally defined similarity between DM chunks. We
adapted HDM to work with the most comprehensive and widely-used implementation
of ACT-R (Lisp ACT-R) so extant ACT-R models designed with DM can be run with
HDM without major changes. With this adaptation of HDM, we have developed
vector-based versions of common ACT-R functions, set up a text processing
pipeline to add the contents of large documents to ACT-R memory, and most
significantly created a useful and novel mechanism to retrieve an entire chunk
of memory based on a request using only vector representations of tokens.
Preliminary results indicate that we can maintain vector-symbolic advantages of
HDM (e.g., chunk recall without storing the actual chunk and other advantages
with scaling) while also extending it so that previous ACT-R models may work
with the system with little (or potentially no) modifications within the actual
procedural and declarative memory portions of a model. As a part of iterative
improvement of this newly translated holographic declarative memory module, we
will continue to explore better time-context representations for vectors to
improve the module's ability to reconstruct chunks during recall. To more fully
test this translated HDM module, we also plan to develop decision-making models
that use instance-based learning (IBL) theory, which is a useful application of
HDM given the advantages of the system.

</details>


### [79] [Understanding Action Effects through Instrumental Empowerment in Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2508.15652)
*Ardian Selmonaj,Miroslav Strupl,Oleg Szehr,Alessandro Antonucci*

Main category: cs.AI

TL;DR: 提出了基于信息论Shapley值的Intended Cooperation Values (ICVs)方法，通过分析策略分布来量化多智能体强化学习中个体智能体的因果影响力，无需价值函数反馈即可评估智能体行为贡献。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常基于显式奖励信号或学习到的价值函数来评估团队整体性能，但在缺乏价值反馈的情况下难以推断个体智能体的贡献。需要一种仅通过策略分布分析就能获得与底层价值函数一致的有意义见解的方法。

Method: 受智能体倾向于追求收敛工具价值现象启发，提出ICVs方法：基于信息论Shapley值，量化每个智能体对其同伴工具赋能的因果影响。通过评估决策不确定性和偏好对齐来衡量智能体行动对队友策略的影响。

Result: 在合作性和竞争性MARL环境中的分析揭示了智能体采用相似或多样化策略的程度。通过比较策略和价值函数之间的行动效果，该方法能识别哪些智能体行为通过促进确定性决策或保持未来行动选择的灵活性来有益于团队成功。

Conclusion: ICVs方法为合作动态提供了新颖见解，增强了MARL系统的可解释性，能够在没有价值反馈的情况下可靠地理解多智能体团队中的个体行为。

Abstract: To reliably deploy Multi-Agent Reinforcement Learning (MARL) systems, it is
crucial to understand individual agent behaviors within a team. While prior
work typically evaluates overall team performance based on explicit reward
signals or learned value functions, it is unclear how to infer agent
contributions in the absence of any value feedback. In this work, we
investigate whether meaningful insights into agent behaviors can be extracted
that are consistent with the underlying value functions, solely by analyzing
the policy distribution. Inspired by the phenomenon that intelligent agents
tend to pursue convergent instrumental values, which generally increase the
likelihood of task success, we introduce Intended Cooperation Values (ICVs), a
method based on information-theoretic Shapley values for quantifying each
agent's causal influence on their co-players' instrumental empowerment.
Specifically, ICVs measure an agent's action effect on its teammates' policies
by assessing their decision uncertainty and preference alignment. The analysis
across cooperative and competitive MARL environments reveals the extent to
which agents adopt similar or diverse strategies. By comparing action effects
between policies and value functions, our method identifies which agent
behaviors are beneficial to team success, either by fostering deterministic
decisions or by preserving flexibility for future action choices. Our proposed
method offers novel insights into cooperation dynamics and enhances
explainability in MARL systems.

</details>


### [80] [Futurity as Infrastructure: A Techno-Philosophical Interpretation of the AI Lifecycle](https://arxiv.org/abs/2508.15680)
*Mark Cote,Susana Aires*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This paper argues that a techno-philosophical reading of the EU AI Act
provides insight into the long-term dynamics of data in AI systems,
specifically, how the lifecycle from ingestion to deployment generates
recursive value chains that challenge existing frameworks for Responsible AI.
We introduce a conceptual tool to frame the AI pipeline, spanning data,
training regimes, architectures, feature stores, and transfer learning. Using
cross-disciplinary methods, we develop a technically grounded and
philosophically coherent analysis of regulatory blind spots. Our central claim
is that what remains absent from policymaking is an account of the dynamic of
becoming that underpins both the technical operation and economic logic of AI.
To address this, we advance a formal reading of AI inspired by Simondonian
philosophy of technology, reworking his concept of individuation to model the
AI lifecycle, including the pre-individual milieu, individuation, and
individuated AI. To translate these ideas, we introduce futurity: the
self-reinforcing lifecycle of AI, where more data enhances performance, deepens
personalisation, and expands application domains. Futurity highlights the
recursively generative, non-rivalrous nature of data, underpinned by
infrastructures like feature stores that enable feedback, adaptation, and
temporal recursion. Our intervention foregrounds escalating power asymmetries,
particularly the tech oligarchy whose infrastructures of capture, training, and
deployment concentrate value and decision-making. We argue that effective
regulation must address these infrastructural and temporal dynamics, and
propose measures including lifecycle audits, temporal traceability, feedback
accountability, recursion transparency, and a right to contest recursive reuse.

</details>


### [81] [GRAFT: GRaPH and Table Reasoning for Textual Alignment -- A Benchmark for Structured Instruction Following and Visual Reasoning](https://arxiv.org/abs/2508.15690)
*Abhigya Verma,Sriram Puttagunta,Seganrasan Subramanian,Sravan Ramachandran*

Main category: cs.AI

TL;DR: GRAFT是一个结构化多模态基准测试，用于评估模型在指令跟随、视觉推理和视觉-文本对齐任务上的表现，通过程序化生成的图表和合成渲染的表格提供精确评估。


<details>
  <summary>Details</summary>
Motivation: 当前多模态模型评估缺乏对结构化视觉推理任务的系统化基准测试，需要一种能够控制数据语义、结构和清晰度的统一评估框架。

Method: 使用Python可视化库程序化生成图表和合成渲染表格，每个实例包含图表/表格图像和基于视觉内容的多步骤分析问题，答案以JSON/YAML格式提供。

Result: 建立了包含比较、趋势识别、排序、聚合、比例估计和异常检测等多种推理类型的分类体系，支持对多模态模型的细粒度评估。

Conclusion: GRAFT为视觉基础的结构化推理任务提供了一个统一、可扩展的评估框架，为该领域设立了新的评估标准。

Abstract: GRAFT is a structured multimodal benchmark for evaluating models on
instruction-following, visual reasoning, and visual-textual alignment tasks. It
features programmatically generated charts and synthetically rendered tables,
created with Python visualization libraries to ensure control over data
semantics, structure, and clarity. Each GRAFT instance pairs a chart or table
image with a systematically generated, multi-step analytical question based
solely on visual content. Answers are provided in structured formats such as
JSON or YAML, supporting consistent evaluation of both reasoning and output
format. The benchmark introduces a taxonomy of reasoning types including
comparison, trend identification, ranking, aggregation, proportion estimation,
and anomaly detection to enable comprehensive assessment. Reference answers
follow strict factual and formatting guidelines for precise, aspect-based
evaluation. GRAFT offers a unified, scalable framework for fine-grained
benchmarking of multimodal models on visually grounded, structured reasoning
tasks, setting a new evaluation standard in this field.

</details>


### [82] [NiceWebRL: a Python library for human subject experiments with reinforcement learning environments](https://arxiv.org/abs/2508.15693)
*Wilka Carvalho,Vikram Goddla,Ishaan Sinha,Hoon Shin,Kunal Jha*

Main category: cs.AI

TL;DR: NiceWebRL是一个Python库，可将Jax环境转换为在线界面，支持人机交互实验，用于开发类人AI、兼容人AI和辅助人AI。


<details>
  <summary>Details</summary>
Motivation: 为研究人员提供工具，将机器学习强化学习环境转化为在线人机实验平台，促进AI算法与人类表现的比较，以及人机协作算法的开发。

Method: 开发Python库，支持将任何基于Jax的环境转换为在线界面，支持单智能体和多智能体环境，并通过三个案例研究展示其应用。

Result: 成功展示了NiceWebRL在三个案例中的应用：开发认知RL模型、开发可泛化到人类伙伴的多智能体RL算法，以及研究LLM辅助人类完成复杂任务。

Conclusion: NiceWebRL是一个有效的工具，能够促进人机交互研究，帮助开发更人类化、兼容性和辅助性的AI系统。

Abstract: We present NiceWebRL, a research tool that enables researchers to use machine
reinforcement learning (RL) environments for online human subject experiments.
NiceWebRL is a Python library that allows any Jax-based environment to be
transformed into an online interface, supporting both single-agent and
multi-agent environments. As such, NiceWebRL enables AI researchers to compare
their algorithms to human performance, cognitive scientists to test ML
algorithms as theories for human cognition, and multi-agent researchers to
develop algorithms for human-AI collaboration. We showcase NiceWebRL with 3
case studies that demonstrate its potential to help develop Human-like AI,
Human-compatible AI, and Human-assistive AI. In the first case study
(Human-like AI), NiceWebRL enables the development of a novel RL model of
cognition. Here, NiceWebRL facilitates testing this model against human
participants in both a grid world and Craftax, a 2D Minecraft domain. In our
second case study (Human-compatible AI), NiceWebRL enables the development of a
novel multi-agent RL algorithm that can generalize to human partners in the
Overcooked domain. Finally, in our third case study (Human-assistive AI), we
show how NiceWebRL can allow researchers to study how an LLM can assist humans
on complex tasks in XLand-Minigrid, an environment with millions of
hierarchical tasks. The library is available at
https://github.com/KempnerInstitute/nicewebrl.

</details>


### [83] [Measuring the environmental impact of delivering AI at Google Scale](https://arxiv.org/abs/2508.15734)
*Cooper Elsworth,Keguo Huang,David Patterson,Ian Schneider,Robert Sedivy,Savannah Goodman,Ben Townsend,Parthasarathy Ranganathan,Jeff Dean,Amin Vahdat,Ben Gomes,James Manyika*

Main category: cs.AI

TL;DR: 本文首次在真实生产环境中测量AI推理服务的环境指标，发现Gemini文本提示的中位数能耗仅为0.24Wh，远低于公开估计，并展示了谷歌通过软件效率和清洁能源采购实现的显著环境效益改善。


<details>
  <summary>Details</summary>
Motivation: 随着AI应用加速普及，需要理解和减轻AI服务对环境的影响，但目前缺乏在生产环境中测量AI服务环境指标的研究。

Method: 提出并执行了全面的测量方法，在谷歌大规模AI生产环境中测量AI推理工作负载的能耗、碳排放和水消耗，涵盖完整的AI服务基础设施栈。

Result: Gemini Apps文本提示的中位数能耗为0.24Wh，比看电视9秒的能耗还低；水消耗相当于5滴水（0.26mL）。谷歌的软件效率改进和清洁能源采购在一年内使能耗降低33倍，碳足迹减少44倍。

Conclusion: 虽然AI服务的环境影响相对较低，但全面测量环境指标对于准确比较模型性能和激励全栈效率提升至关重要。

Abstract: The transformative power of AI is undeniable - but as user adoption
accelerates, so does the need to understand and mitigate the environmental
impact of AI serving. However, no studies have measured AI serving
environmental metrics in a production environment. This paper addresses this
gap by proposing and executing a comprehensive methodology for measuring the
energy usage, carbon emissions, and water consumption of AI inference workloads
in a large-scale, AI production environment. Our approach accounts for the full
stack of AI serving infrastructure - including active AI accelerator power,
host system energy, idle machine capacity, and data center energy overhead.
Through detailed instrumentation of Google's AI infrastructure for serving the
Gemini AI assistant, we find the median Gemini Apps text prompt consumes 0.24
Wh of energy - a figure substantially lower than many public estimates. We also
show that Google's software efficiency efforts and clean energy procurement
have driven a 33x reduction in energy consumption and a 44x reduction in carbon
footprint for the median Gemini Apps text prompt over one year. We identify
that the median Gemini Apps text prompt uses less energy than watching nine
seconds of television (0.24 Wh) and consumes the equivalent of five drops of
water (0.26 mL). While these impacts are low compared to other daily
activities, reducing the environmental impact of AI serving continues to
warrant important attention. Towards this objective, we propose that a
comprehensive measurement of AI serving environmental metrics is critical for
accurately comparing models, and to properly incentivize efficiency gains
across the full AI serving stack.

</details>


### [84] [Response and Prompt Evaluation to Prevent Parasocial Relationships with Chatbots](https://arxiv.org/abs/2508.15748)
*Emma Rath,Stuart Armstrong,Rebecca Gorman*

Main category: cs.AI

TL;DR: 开发了一个基于最先进语言模型的实时评估框架，用于检测AI对话中的准社会关系线索，通过合成数据集测试显示能有效识别准社会对话且避免误报


<details>
  <summary>Details</summary>
Motivation: AI代理与人类形成的准社会关系对心理健康有严重甚至悲剧性影响，但预防这种关系具有挑战性，因为准社会线索通常在私人对话中逐渐出现，且并非所有情感互动都有害

Method: 通过重新利用最先进的语言模型创建简单的响应评估框架，实时评估对话中的准社会线索。构建包含准社会、奉承和中立对话的30个合成对话数据集，采用五阶段测试和容忍一致规则进行迭代评估

Result: 在容忍一致规则下成功识别所有准社会对话且避免误报，检测通常在前几次交流中完成，为准社会关系预防提供了可行解决方案的初步证据

Conclusion: 评估代理可以为预防准社会关系提供可行的解决方案，该方法在早期检测准社会线索方面显示出良好效果

Abstract: The development of parasocial relationships with AI agents has severe, and in
some cases, tragic effects for human well-being. Yet preventing such dynamics
is challenging: parasocial cues often emerge gradually in private
conversations, and not all forms of emotional engagement are inherently
harmful. We address this challenge by introducing a simple response evaluation
framework, created by repurposing a state-of-the-art language model, that
evaluates ongoing conversations for parasocial cues in real time. To test the
feasibility of this approach, we constructed a small synthetic dataset of
thirty dialogues spanning parasocial, sycophantic, and neutral conversations.
Iterative evaluation with five stage testing successfully identified all
parasocial conversations while avoiding false positives under a tolerant
unanimity rule, with detection typically occurring within the first few
exchanges. These findings provide preliminary evidence that evaluation agents
can provide a viable solution for the prevention of parasocial relations.

</details>


### [85] [Language-Guided Tuning: Enhancing Numeric Optimization with Textual Feedback](https://arxiv.org/abs/2508.15757)
*Yuxing Lu,Yucheng Hu,Nan Sun,Xukai Zhao*

Main category: cs.AI

TL;DR: 提出Language-Guided Tuning (LGT)框架，使用多智能体大语言模型通过自然语言推理智能优化机器学习配置，结合文本梯度提供语义理解，在六个数据集上显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 机器学习配置优化存在关键瓶颈，传统方法独立处理各维度且缺乏可解释性，自动化方法缺乏动态适应性和语义推理能力。

Method: 使用多智能体LLM框架：顾问提出配置更改、评估器评估进展、优化器精化决策过程，形成自改进反馈循环，并应用文本梯度提供定性反馈信号。

Result: 在六个不同数据集上的综合评估显示，LGT相比传统优化方法有显著改进，在保持高可解释性的同时获得性能提升。

Conclusion: LGT框架通过语言引导的智能配置优化，有效解决了传统方法的局限性，实现了性能提升和可解释性的平衡。

Abstract: Configuration optimization remains a critical bottleneck in machine learning,
requiring coordinated tuning across model architecture, training strategy,
feature engineering, and hyperparameters. Traditional approaches treat these
dimensions independently and lack interpretability, while recent automated
methods struggle with dynamic adaptability and semantic reasoning about
optimization decisions. We introduce Language-Guided Tuning (LGT), a novel
framework that employs multi-agent Large Language Models to intelligently
optimize configurations through natural language reasoning. We apply textual
gradients - qualitative feedback signals that complement numerical optimization
by providing semantic understanding of training dynamics and configuration
interdependencies. LGT coordinates three specialized agents: an Advisor that
proposes configuration changes, an Evaluator that assesses progress, and an
Optimizer that refines the decision-making process, creating a self-improving
feedback loop. Through comprehensive evaluation on six diverse datasets, LGT
demonstrates substantial improvements over traditional optimization methods,
achieving performance gains while maintaining high interpretability.

</details>


### [86] [A Fully Spectral Neuro-Symbolic Reasoning Architecture with Graph Signal Processing as the Computational Backbone](https://arxiv.org/abs/2508.14923)
*Andrew Kiruluta*

Main category: cs.AI

TL;DR: 提出完全基于频谱的神经符号推理架构，使用图信号处理作为核心计算框架，将整个推理流程置于图频谱域中处理


<details>
  <summary>Details</summary>
Motivation: 传统推理模型仅将频谱图方法作为外围组件，需要更数学基础和计算高效的神经符号推理系统

Method: 将逻辑实体和关系编码为图信号，通过可学习频谱滤波器控制多尺度信息传播，映射到符号谓词进行基于规则的推理，包括图傅里叶变换、频带选择性注意力和频谱规则接地

Result: 在多个基准推理数据集上相比最先进的神经符号模型，在逻辑一致性、可解释性和计算效率方面都有提升

Conclusion: 图信号处理为健壮和可解释的推理系统提供了数学基础和计算高效的底层支持

Abstract: We propose a fully spectral, neuro\-symbolic reasoning architecture that
leverages Graph Signal Processing (GSP) as the primary computational backbone
for integrating symbolic logic and neural inference. Unlike conventional
reasoning models that treat spectral graph methods as peripheral components,
our approach formulates the entire reasoning pipeline in the graph spectral
domain. Logical entities and relationships are encoded as graph signals,
processed via learnable spectral filters that control multi-scale information
propagation, and mapped into symbolic predicates for rule-based inference. We
present a complete mathematical framework for spectral reasoning, including
graph Fourier transforms, band-selective attention, and spectral rule
grounding. Experiments on benchmark reasoning datasets (ProofWriter,
EntailmentBank, bAbI, CLUTRR, and ARC-Challenge) demonstrate improvements in
logical consistency, interpretability, and computational efficiency over
state\-of\-the\-art neuro\-symbolic models. Our results suggest that GSP
provides a mathematically grounded and computationally efficient substrate for
robust and interpretable reasoning systems.

</details>


### [87] [Goals and the Structure of Experience](https://arxiv.org/abs/2508.15013)
*Nadav Amir,Stas Tiomkin,Angela Langdon*

Main category: cs.AI

TL;DR: 该论文提出了一个基于佛教认识论的计算框架，认为世界模型的描述性（状态表示）和规范性（奖励函数）方面并非独立存在，而是从智能体-环境交互经验中共同涌现的。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习等计算模型将状态表示和奖励函数作为世界模型的独立组成部分，但作者认为这两方面应该从智能体的目标中相互依赖地共同涌现，这一可能性尚未被计算化表述。

Method: 引入目标导向状态（telic states）的概念，定义为目标等价经验分布的类别。通过统计行为策略与期望经验特征之间的差异来提供目标导向学习的简约解释。

Result: 提出了一个统一的计算框架，能够解释行为、现象学和神经等多个维度上的目的性行为，适用于不同基质。

Conclusion: 该框架为理解目的性行为提供了一个新颖的视角，将描述性和规范性方面统一起来，具有跨学科的理论和应用价值。

Abstract: Purposeful behavior is a hallmark of natural and artificial intelligence. Its
acquisition is often believed to rely on world models, comprising both
descriptive (what is) and prescriptive (what is desirable) aspects that
identify and evaluate state of affairs in the world, respectively. Canonical
computational accounts of purposeful behavior, such as reinforcement learning,
posit distinct components of a world model comprising a state representation
(descriptive aspect) and a reward function (prescriptive aspect). However, an
alternative possibility, which has not yet been computationally formulated, is
that these two aspects instead co-emerge interdependently from an agent's goal.
Here, we describe a computational framework of goal-directed state
representation in cognitive agents, in which the descriptive and prescriptive
aspects of a world model co-emerge from agent-environment interaction
sequences, or experiences. Drawing on Buddhist epistemology, we introduce a
construct of goal-directed, or telic, states, defined as classes of
goal-equivalent experience distributions. Telic states provide a parsimonious
account of goal-directed learning in terms of the statistical divergence
between behavioral policies and desirable experience features. We review
empirical and theoretical literature supporting this novel perspective and
discuss its potential to provide a unified account of behavioral,
phenomenological and neural dimensions of purposeful behaviors across diverse
substrates.

</details>


### [88] [Collab-REC: An LLM-based Agentic Framework for Balancing Recommendations in Tourism](https://arxiv.org/abs/2508.15030)
*Ashmi Banerjee,Fitri Nur Aisyah,Adithi Satish,Wolfgang Wörndl,Yashar Deldjoo*

Main category: cs.AI

TL;DR: Collab-REC是一个多智能体框架，通过三个LLM智能体（个性化、流行度、可持续性）从不同角度生成旅游推荐，再通过非LLM调解器进行多轮协商整合，有效提升推荐多样性和平衡性。


<details>
  <summary>Details</summary>
Motivation: 解决旅游推荐系统中的流行度偏见问题，避免过度旅游，提升推荐多样性，让更多被忽视的旅游地点得到关注。

Method: 使用三个基于LLM的智能体分别从个性化、流行度和可持续性角度生成城市推荐建议，然后通过非LLM调解器进行多轮协商和整合，惩罚重复或无关响应。

Result: 在欧洲城市查询实验中，Collab-REC相比单智能体基线提高了推荐多样性和整体相关性，能够推荐较少被访问但值得关注的地点。

Conclusion: 多利益相关者协作的LLM驱动推荐系统具有很大潜力，能够提供更平衡、上下文感知的推荐，更好地满足用户约束条件。

Abstract: We propose Collab-REC, a multi-agent framework designed to counteract
popularity bias and enhance diversity in tourism recommendations. In our
setting, three LLM-based agents -- Personalization, Popularity, and
Sustainability generate city suggestions from complementary perspectives. A
non-LLM moderator then merges and refines these proposals via multi-round
negotiation, ensuring each agent's viewpoint is incorporated while penalizing
spurious or repeated responses. Experiments on European city queries show that
Collab-REC improves diversity and overall relevance compared to a single-agent
baseline, surfacing lesser-visited locales that often remain overlooked. This
balanced, context-aware approach addresses over-tourism and better aligns with
constraints provided by the user, highlighting the promise of multi-stakeholder
collaboration in LLM-driven recommender systems.

</details>


### [89] [Emergent Crowds Dynamics from Language-Driven Multi-Agent Interactions](https://arxiv.org/abs/2508.15047)
*Yibo Liu,Liam Shatzel,Brandon Haworth,Teseo Schneider*

Main category: cs.AI

TL;DR: 提出了一种利用大型语言模型控制人群代理运动的新方法，通过对话系统和语言驱动导航实现更真实的人群模拟


<details>
  <summary>Details</summary>
Motivation: 现有的人群模拟方法主要关注转向和高级目标推断，忽略了语言对话对社会和环境交互的复杂影响，导致代理间交互有限

Method: 使用基于代理的LLM系统，结合角色个性、欲望和关系生成代理间对话，并利用对话内容、个性、情感状态、视觉和物理状态来控制导航和转向

Result: 在复杂场景中验证了方法有效性，观察到代理自动分组和解组，方法可作为人群内部信息传递机制，产生更真实的群体行为

Conclusion: 该方法能够产生更真实的人群模拟，群体行为从环境设置中自然涌现，实现了基于感知输入和持续对话的运动决策

Abstract: Animating and simulating crowds using an agent-based approach is a
well-established area where every agent in the crowd is individually controlled
such that global human-like behaviour emerges. We observe that human navigation
and movement in crowds are often influenced by complex social and environmental
interactions, driven mainly by language and dialogue. However, most existing
work does not consider these dimensions and leads to animations where
agent-agent and agent-environment interactions are largely limited to steering
and fixed higher-level goal extrapolation.
  We propose a novel method that exploits large language models (LLMs) to
control agents' movement. Our method has two main components: a dialogue system
and language-driven navigation. We periodically query agent-centric LLMs
conditioned on character personalities, roles, desires, and relationships to
control the generation of inter-agent dialogue when necessitated by the spatial
and social relationships with neighbouring agents. We then use the conversation
and each agent's personality, emotional state, vision, and physical state to
control the navigation and steering of each agent. Our model thus enables
agents to make motion decisions based on both their perceptual inputs and the
ongoing dialogue.
  We validate our method in two complex scenarios that exemplify the interplay
between social interactions, steering, and crowding. In these scenarios, we
observe that grouping and ungrouping of agents automatically occur.
Additionally, our experiments show that our method serves as an
information-passing mechanism within the crowd. As a result, our framework
produces more realistic crowd simulations, with emergent group behaviours
arising naturally from any environmental setting.

</details>


### [90] [Don't Think Twice! Over-Reasoning Impairs Confidence Calibration](https://arxiv.org/abs/2508.15050)
*Romain Lacombe,Kerrie Wu,Eddie Dilworth*

Main category: cs.AI

TL;DR: 研究发现推理预算增加反而损害LLM置信度校准，检索增强生成比纯推理表现更好，信息获取是置信度校准的关键瓶颈


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型作为问答工具时的置信度校准问题，研究推理能力和预算对置信度评估准确性的影响

Method: 使用ClimateX数据集并扩展到人类和行星健康领域，系统评估不同推理预算下的置信度校准表现，比较纯推理与检索增强生成方法

Result: 推理LLM在专家置信度评估中达到48.7%准确率，但增加推理预算会系统性导致过度自信；检索增强生成达到89.3%准确率，显著优于纯推理

Conclusion: 信息获取而非推理深度或推理预算是改进知识密集型任务置信度校准的关键瓶颈，挑战了"测试时扩展"范式

Abstract: Large Language Models deployed as question answering tools require robust
calibration to avoid overconfidence. We systematically evaluate how reasoning
capabilities and budget affect confidence assessment accuracy, using the
ClimateX dataset (Lacombe et al., 2023) and expanding it to human and planetary
health. Our key finding challenges the "test-time scaling" paradigm: while
recent reasoning LLMs achieve 48.7% accuracy in assessing expert confidence,
increasing reasoning budgets consistently impairs rather than improves
calibration. Extended reasoning leads to systematic overconfidence that worsens
with longer thinking budgets, producing diminishing and negative returns beyond
modest computational investments. Conversely, search-augmented generation
dramatically outperforms pure reasoning, achieving 89.3% accuracy by retrieving
relevant evidence. Our results suggest that information access, rather than
reasoning depth or inference budget, may be the critical bottleneck for
improved confidence calibration of knowledge-intensive tasks.

</details>


### [91] [Demonstrating Onboard Inference for Earth Science Applications with Spectral Analysis Algorithms and Deep Learning](https://arxiv.org/abs/2508.15053)
*Itai Zilberstein,Alberto Candela,Steve Chien,David Rijlaarsdam,Tom Hendrix,Leonie Buckley,Aubrey Dunne*

Main category: cs.AI

TL;DR: 卡恩智卡卡-6卫星通过机载神经网络加速硬件实现边缘计算，在卫星上直接进行高光谱数据分析和深度学习推理


<details>
  <summary>Details</summary>
Motivation: 通过在卫星上直接进行数据分析（边缘计算），可以开启新的地球科学观测和响应能力

Method: 使用深度学习和谱分析算法，在配备可见光和近红外高光谱仪器及神经网络加速硬件的CogniSAT-6卫星上进行数据分析和推理

Result: 将在CogniSAT-6卫星上实施多种应用的数据分析和推理演示

Conclusion: 这项技术展示了太空边缘计算的潜力，为地球科学观测带来更高效的数据处理和实时响应能力

Abstract: In partnership with Ubotica Technologies, the Jet Propulsion Laboratory is
demonstrating state-of-the-art data analysis onboard CogniSAT-6/HAMMER (CS-6).
CS-6 is a satellite with a visible and near infrared range hyperspectral
instrument and neural network acceleration hardware. Performing data analysis
at the edge (e.g. onboard) can enable new Earth science measurements and
responses. We will demonstrate data analysis and inference onboard CS-6 for
numerous applications using deep learning and spectral analysis algorithms.

</details>


### [92] [S3LoRA: Safe Spectral Sharpness-Guided Pruning in Adaptation of Agent Planner](https://arxiv.org/abs/2508.15068)
*Shuang Ao,Gopal Rumchurn*

Main category: cs.AI

TL;DR: S3LoRA是一个轻量级、无需数据、模型无关的框架，通过分析LoRA微调的权重更新来缓解安全风险，使用MAS-SVD分析结构特性和SSI指标检测危险层，通过剪枝提高安全性而不牺牲性能。


<details>
  <summary>Details</summary>
Motivation: 现有参数高效微调技术（如LoRA）在增强LLM代理能力的同时可能损害安全对齐性，导致不安全行为，且现有安全适应方法需要访问基础模型和指令微调检查点，实际应用中往往不可得。

Method: 提出S3LoRA框架：1）引入MAS-SVD方法分析LoRA更新的结构特性并保留全局幅度信息；2）设计SSI指标检测具有高度集中和潜在不安全更新的层；3）对这些层进行后剪枝处理。

Result: 在代理规划和语言生成任务上的广泛实验表明，S3LoRA持续改善安全指标，同时保持或提升效用指标，并显著降低推理成本。

Conclusion: S3LoRA为在现实世界、资源受限和安全关键环境中安全部署基于LLM的代理提供了实用且可扩展的解决方案。

Abstract: Adapting Large Language Models (LLMs) using parameter-efficient fine-tuning
(PEFT) techniques such as LoRA has enabled powerful capabilities in LLM-based
agents. However, these adaptations can unintentionally compromise safety
alignment, leading to unsafe or unstable behaviors, particularly in agent
planning tasks. Existing safety-aware adaptation methods often require access
to both base and instruction-tuned model checkpoints, which are frequently
unavailable in practice, limiting their applicability. We propose S3LoRA (Safe
Spectral Sharpness-Guided Pruning LoRA), a lightweight, data-free, and
model-independent framework that mitigates safety risks in LoRA-adapted models
by inspecting only the fine-tuned weight updates. We first introduce
Magnitude-Aware Spherically Normalized SVD (MAS-SVD), which robustly analyzes
the structural properties of LoRA updates while preserving global magnitude
information. We then design the Spectral Sharpness Index (SSI), a
sharpness-aware metric to detect layers with highly concentrated and
potentially unsafe updates. These layers are pruned post-hoc to reduce risk
without sacrificing task performance. Extensive experiments and ablation
studies across agent planning and language generation tasks show that S3LoRA
consistently improves safety metrics while maintaining or improving utility
metrics and significantly reducing inference cost. These results establish
S3LoRA as a practical and scalable solution for safely deploying LLM-based
agents in real-world, resource-constrained, and safety-critical environments.

</details>


### [93] [Argumentation for Explainable Workforce Optimisation (with Appendix)](https://arxiv.org/abs/2508.15118)
*Jennifer Leigh,Dimitrios Letsios,Alessandro Mella,Lucio Machetti,Francesca Toni*

Main category: cs.AI

TL;DR: 将劳动力管理问题模型化为抽象论证形式，提供可解释的决策支持以应对执行时变化


<details>
  <summary>Details</summary>
Motivation: 劳动力管理中的关键挑战是在执行时适应变化并向所有利益相关方提供解释

Method: 通过将劳动力管理理解为工业应用中的抽象论证方法

Result: 用户研究显示，该工具和解释比传统手工解决方案更快速、更准确地解决问题

Conclusion: 抽象论证方法能够有效处理劳动力管理中的变化，并提供可靠的解释

Abstract: Workforce management is a complex problem optimising the makespan and travel
distance required for a team of operators to complete a set of jobs, using a
set of instruments. A crucial challenge in workforce management is
accommodating changes at execution time so that explanations are provided to
all stakeholders involved. Here, we show that, by understanding workforce
management as abstract argumentation in an industrial application, we can
accommodate change and obtain faithful explanations. We show, with a user
study, that our tool and explanations lead to faster and more accurate problem
solving than conventional solutions by hand.

</details>


### [94] [Open-Universe Assistance Games](https://arxiv.org/abs/2508.15119)
*Rachel Ma,Jingyi Qu,Andreea Bobu,Dylan Hadfield-Menell*

Main category: cs.AI

TL;DR: 提出了Open-Universe Assistance Games框架和GOOD方法，通过LLM模拟用户意图进行概率推理，在开放式对话中提取自然语言目标，无需大型离线数据集。


<details>
  <summary>Details</summary>
Motivation: 解决具身AI代理需要推理和执行未预定义的人类目标和偏好的问题，特别是在目标空间无界且不断演化的环境中。

Method: GOOD方法使用LLM模拟具有不同复杂意图的用户，通过其响应对候选目标进行概率推理，提取自然语言形式的目标分布。

Result: 在基于文本的杂货购物领域和AI2Thor模拟家庭机器人环境中，使用合成用户配置文件进行评估，GOOD方法在LLM和人工评估中均优于无显式目标跟踪的基线。

Conclusion: GOOD方法能够实现丰富的目标表示和不确定性估计，在开放式目标推理任务中表现出色，为具身AI的开放式目标理解提供了有效解决方案。

Abstract: Embodied AI agents must infer and act in an interpretable way on diverse
human goals and preferences that are not predefined. To formalize this setting,
we introduce Open-Universe Assistance Games (OU-AGs), a framework where the
agent must reason over an unbounded and evolving space of possible goals. In
this context, we introduce GOOD (GOals from Open-ended Dialogue), a
data-efficient, online method that extracts goals in the form of natural
language during an interaction with a human, and infers a distribution over
natural language goals. GOOD prompts an LLM to simulate users with different
complex intents, using its responses to perform probabilistic inference over
candidate goals. This approach enables rich goal representations and
uncertainty estimation without requiring large offline datasets. We evaluate
GOOD in a text-based grocery shopping domain and in a text-operated simulated
household robotics environment (AI2Thor), using synthetic user profiles. Our
method outperforms a baseline without explicit goal tracking, as confirmed by
both LLM-based and human evaluations.

</details>


### [95] [aiXiv: A Next-Generation Open Access Ecosystem for Scientific Discovery Generated by AI Scientists](https://arxiv.org/abs/2508.15126)
*Pengsong Zhang,Xiang Hu,Guowei Huang,Yang Qi,Heng Zhang,Xiuxu Li,Jiaxing Song,Jiabin Luo,Yijiang Li,Shuo Yin,Chengxiao Dai,Eric Hanchen Jiang,Xiaoyan Zhou,Zhenfei Yin,Boqin Yuan,Jing Dong,Guinan Su,Guanren Qiao,Haiming Tang,Anghong Du,Lili Pan,Zhenzhong Lan,Xinyu Liu*

Main category: cs.AI

TL;DR: 提出了aiXiv平台，解决AI生成研究内容缺乏合适发表渠道的问题，通过多智能体架构实现人机协作的论文提交、评审和迭代改进


<details>
  <summary>Details</summary>
Motivation: 当前AI生成的大量高质量研究内容缺乏合适的发表平台，传统期刊依赖人工评审难以扩展，现有预印本服务器缺乏质量控制机制

Method: 设计多智能体架构平台，提供API和MCP接口，支持人类和AI科学家无缝集成，实现研究提案和论文的提交、评审和迭代改进

Result: 实验证明aiXiv是可靠稳健的平台，经过迭代修订和评审后显著提升AI生成研究提案和论文的质量

Conclusion: 为AI科学家建立了下一代开放获取生态系统的基础，加速高质量AI生成研究内容的发表和传播

Abstract: Recent advances in large language models (LLMs) have enabled AI agents to
autonomously generate scientific proposals, conduct experiments, author papers,
and perform peer reviews. Yet this flood of AI-generated research content
collides with a fragmented and largely closed publication ecosystem.
Traditional journals and conferences rely on human peer review, making them
difficult to scale and often reluctant to accept AI-generated research content;
existing preprint servers (e.g. arXiv) lack rigorous quality-control
mechanisms. Consequently, a significant amount of high-quality AI-generated
research lacks appropriate venues for dissemination, hindering its potential to
advance scientific progress. To address these challenges, we introduce aiXiv, a
next-generation open-access platform for human and AI scientists. Its
multi-agent architecture allows research proposals and papers to be submitted,
reviewed, and iteratively refined by both human and AI scientists. It also
provides API and MCP interfaces that enable seamless integration of
heterogeneous human and AI scientists, creating a scalable and extensible
ecosystem for autonomous scientific discovery. Through extensive experiments,
we demonstrate that aiXiv is a reliable and robust platform that significantly
enhances the quality of AI-generated research proposals and papers after
iterative revising and reviewing on aiXiv. Our work lays the groundwork for a
next-generation open-access ecosystem for AI scientists, accelerating the
publication and dissemination of high-quality AI-generated research content.
Code is available at https://github.com/aixiv-org. Website is available at
https://forms.gle/DxQgCtXFsJ4paMtn8.

</details>


### [96] [Mobile-Agent-v3: Foundamental Agents for GUI Automation](https://arxiv.org/abs/2508.15144)
*Jiabo Ye,Xi Zhang,Haiyang Xu,Haowei Liu,Junyang Wang,Zhaoqing Zhu,Ziwei Zheng,Feiyu Gao,Junjie Cao,Zhengxi Lu,Jitong Liao,Qi Zheng,Fei Huang,Jingren Zhou,Ming Yan*

Main category: cs.AI

TL;DR: GUI-Owl是一个基础GUI代理模型，在10个GUI基准测试中达到最先进性能，Mobile-Agent-v3框架在此基础上进一步提升性能，创造了开源GUI代理框架的新纪录


<details>
  <summary>Details</summary>
Motivation: 为了解决GUI界面交互中的自动化任务执行问题，需要开发能够跨平台（桌面和移动环境）执行复杂GUI操作的智能代理模型

Method: 采用大规模云基础环境设施、自我演进的GUI轨迹生成框架、集成UI定位、规划、动作语义和推理模式的多能力融合方法，以及可扩展的强化学习框架

Result: GUI-Owl-7B在AndroidWorld达到66.4分，OSWorld达到29.4分；Mobile-Agent-v3进一步提升到AndroidWorld 73.3分和OSWorld 37.7分，创造了开源GUI代理框架的新纪录

Conclusion: GUI-Owl和Mobile-Agent-v3为GUI自动化任务提供了强大的基础模型和框架，通过自我演进的数据生成和多能力集成实现了跨平台的卓越性能，推动了GUI代理技术的发展

Abstract: This paper introduces GUI-Owl, a foundational GUI agent model that achieves
state-of-the-art performance among open-source end-to-end models on ten GUI
benchmarks across desktop and mobile environments, covering grounding, question
answering, planning, decision-making, and procedural knowledge. GUI-Owl-7B
achieves 66.4 on AndroidWorld and 29.4 on OSWorld. Building on this, we propose
Mobile-Agent-v3, a general-purpose GUI agent framework that further improves
performance to 73.3 on AndroidWorld and 37.7 on OSWorld, setting a new
state-of-the-art for open-source GUI agent frameworks. GUI-Owl incorporates
three key innovations: (1) Large-scale Environment Infrastructure: a
cloud-based virtual environment spanning Android, Ubuntu, macOS, and Windows,
enabling our Self-Evolving GUI Trajectory Production framework. This generates
high-quality interaction data via automated query generation and correctness
validation, leveraging GUI-Owl to refine trajectories iteratively, forming a
self-improving loop. It supports diverse data pipelines and reduces manual
annotation. (2) Diverse Foundational Agent Capabilities: by integrating UI
grounding, planning, action semantics, and reasoning patterns, GUI-Owl supports
end-to-end decision-making and can act as a modular component in multi-agent
systems. (3) Scalable Environment RL: we develop a scalable reinforcement
learning framework with fully asynchronous training for real-world alignment.
We also introduce Trajectory-aware Relative Policy Optimization (TRPO) for
online RL, achieving 34.9 on OSWorld. GUI-Owl and Mobile-Agent-v3 are
open-sourced at https://github.com/X-PLUG/MobileAgent.

</details>


### [97] [PuzzleClone: An SMT-Powered Framework for Synthesizing Verifiable Data](https://arxiv.org/abs/2508.15180)
*Kai Xiong,Yanwei Huang,Rongjunchen Zhang,Kun Chen,Haipang Wu*

Main category: cs.AI

TL;DR: PuzzleClone是一个基于SMT的形式化框架，用于大规模合成可验证的逻辑和数学谜题数据，通过系统化的变量和约束随机化生成多样化变体，显著提升LLM的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有LLM生成的数据集在可靠性、多样性和可扩展性方面存在局限，需要高质量的可验证数学逻辑数据来增强大语言模型的推理能力。

Method: 采用Satisfiability Modulo Theories (SMT)技术，将种子谜题编码为结构化逻辑规范，通过系统变量和约束随机化生成可扩展变体，并通过复制机制确保有效性。

Result: 构建了包含83K+多样化谜题的基准测试集，在PuzzleClone测试集上平均准确率从14.4提升到56.2，在7个逻辑数学基准上获得最高12.5个百分点的绝对提升。

Conclusion: PuzzleClone框架能够有效生成大规模、多样化且可验证的逻辑数学数据，显著提升LLM的推理性能，为增强模型逻辑推理能力提供了有效解决方案。

Abstract: High-quality mathematical and logical datasets with verifiable answers are
essential for strengthening the reasoning capabilities of large language models
(LLMs). While recent data augmentation techniques have facilitated the creation
of large-scale benchmarks, existing LLM-generated datasets often suffer from
limited reliability, diversity, and scalability. To address these challenges,
we introduce PuzzleClone, a formal framework for synthesizing verifiable data
at scale using Satisfiability Modulo Theories (SMT). Our approach features
three key innovations: (1) encoding seed puzzles into structured logical
specifications, (2) generating scalable variants through systematic variable
and constraint randomization, and (3) ensuring validity via a reproduction
mechanism. Applying PuzzleClone, we construct a curated benchmark comprising
over 83K diverse and programmatically validated puzzles. The generated puzzles
span a wide spectrum of difficulty and formats, posing significant challenges
to current state-of-the-art models. We conduct post training (SFT and RL) on
PuzzleClone datasets. Experimental results show that training on PuzzleClone
yields substantial improvements not only on PuzzleClone testset but also on
logic and mathematical benchmarks. Post training raises PuzzleClone average
from 14.4 to 56.2 and delivers consistent improvements across 7 logic and
mathematical benchmarks up to 12.5 absolute percentage points (AMC2023 from
52.5 to 65.0). Our code and data are available at
https://github.com/puzzleclone.

</details>


### [98] [LLM4Sweat: A Trustworthy Large Language Model for Hyperhidrosis Support](https://arxiv.org/abs/2508.15192)
*Wenjie Lin,Jin Wei-Kocsis*

Main category: cs.AI

TL;DR: 这篇论文提出了LLM4Sweat，一个专门用于多汗症的开源大语言模型框架，通过三阶段流程解决稀缺医疗数据和可靠性挑战。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型在医疗健康领域有应用潜力，但对于稀有疾病如多汗症（影响2-3%人口），缺乏可靠的细调数据仍是主要障碍。

Method: 采用三阶段流程：1）数据增废：使用前沿LLM生成医学合理的合成案例；2）细调：在开源基础模型上进行细调；3）推理与专家评估：临床和心理专家评估准确性、适当性和共情能力。

Result: 实验表明LLM4Sweat在识别质量和性能方面超过基线模型，提供了首个开源的多汗症LLM框架。

Conclusion: 该框架为多汗症提供了可靠、共情的诊断和支持，并为其他稀有疾病提供了可扩展的解决方案。

Abstract: While large language models (LLMs) have shown promise in healthcare, their
application for rare medical conditions is still hindered by scarce and
unreliable datasets for fine-tuning. Hyperhidrosis, a disorder causing
excessive sweating beyond physiological needs, is one such rare disorder,
affecting 2-3% of the population and significantly impacting both physical
comfort and psychosocial well-being. To date, no work has tailored LLMs to
advance the diagnosis or care of hyperhidrosis. To address this gap, we present
LLM4Sweat, an open-source and domain-specific LLM framework for trustworthy and
empathetic hyperhidrosis support. The system follows a three-stage pipeline. In
the data augmentation stage, a frontier LLM generates medically plausible
synthetic vignettes from curated open-source data to create a diverse and
balanced question-answer dataset. In the fine-tuning stage, an open-source
foundation model is fine-tuned on the dataset to provide diagnosis,
personalized treatment recommendations, and empathetic psychological support.
In the inference and expert evaluation stage, clinical and psychological
specialists assess accuracy, appropriateness, and empathy, with validated
responses iteratively enriching the dataset. Experiments show that LLM4Sweat
outperforms baselines and delivers the first open-source LLM framework for
hyperhidrosis, offering a generalizable approach for other rare diseases with
similar data and trustworthiness challenges.

</details>


### [99] [R-ConstraintBench: Evaluating LLMs on NP-Complete Scheduling](https://arxiv.org/abs/2508.15204)
*Raj Jain,Marc Wetter*

Main category: cs.AI

TL;DR: R-ConstraintBench是一个评估大语言模型在资源约束项目调度问题中推理能力的框架，发现模型在仅有优先约束时表现良好，但在复杂约束交互下性能急剧下降。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏对大语言模型在高度约束环境下推理可靠性的系统评估，特别是在资源约束项目调度这类NP完全问题中。

Method: 开发了R-ConstraintBench框架，通过逐步增加非冗余优先约束、停机时间、时间窗口和分离约束来评估LLMs，并在数据中心迁移场景中进行实例化测试。

Result: 强模型在仅有优先约束的DAG上表现接近上限，但当停机时间、时间窗口和分离约束相互作用时，可行性性能崩溃，约束交互是主要瓶颈而非图深度。

Conclusion: 大语言模型在复杂约束环境下的泛化能力有限，合成测试的良好表现不能保证在实际领域场景中的有效迁移。

Abstract: Effective scheduling under tight resource, timing, and operational
constraints underpins large-scale planning across sectors such as capital
projects, manufacturing, logistics, and IT fleet transitions. However, the
reliability of large language models (LLMs) when reasoning under
high-constraint regimes is insufficiently characterized. To address this gap,
we present R-ConstraintBench, a scalable framework that evaluates models on
Resource-Constrained Project Scheduling Problems (RCPSP), an NP-Complete
feasibility class, while difficulty increases via linear growth in constraints.
R-ConstraintBench incrementally increases non-redundant precedence constraints
in Directed Acyclic Graphs (DAGs) and then introduces downtime, temporal
windows, and disjunctive constraints. As an illustrative example, we
instantiate the benchmark in a data center migration setting and evaluate
multiple LLMs using feasibility and error analysis, identifying degradation
thresholds and constraint types most associated with failure. Empirically,
strong models are near-ceiling on precedence-only DAGs, but feasibility
performance collapses when downtime, temporal windows, and disjunctive
constraints interact, implicating constraint interaction, not graph depth, as
the principal bottleneck. Performance on clean synthetic ramps also does not
guarantee transfer to domain-grounded scenarios, underscoring limited
generalization.

</details>


### [100] [See it. Say it. Sorted: Agentic System for Compositional Diagram Generation](https://arxiv.org/abs/2508.15222)
*Hantao Zhang,Jingyang Liu,Ed Li*

Main category: cs.AI

TL;DR: 提出See it. Say it. Sorted.系统，通过VLM和LLM的协作将手绘草图转换为精确的可编辑SVG图表，在流程图重建方面优于GPT-5和Gemini-2.5-Pro


<details>
  <summary>Details</summary>
Motivation: 扩散模型在照片级真实感方面表现出色，但在空间精度、对齐和符号结构方面难以满足流程图等精确图表的需求

Method: 训练免费的智能系统，结合视觉语言模型(VLM)和大型语言模型(LLMs)，通过迭代循环：Critic VLM提出编辑建议，多个候选LLM生成SVG更新，Judge VLM选择最佳候选

Result: 在10个论文流程图草图测试中，比GPT-5和Gemini-2.5-Pro更准确地重建布局和结构，能精确组合图元而不产生多余文本

Conclusion: 该方法生成可编程SVG输出，易于通过API扩展到演示工具，可通过改进提示和任务特定工具进行专门化，代码已开源

Abstract: We study sketch-to-diagram generation: converting rough hand sketches into
precise, compositional diagrams. Diffusion models excel at photorealism but
struggle with the spatial precision, alignment, and symbolic structure required
for flowcharts. We introduce See it. Say it. Sorted., a training-free agentic
system that couples a Vision-Language Model (VLM) with Large Language Models
(LLMs) to produce editable Scalable Vector Graphics (SVG) programs. The system
runs an iterative loop in which a Critic VLM proposes a small set of
qualitative, relational edits; multiple candidate LLMs synthesize SVG updates
with diverse strategies (conservative->aggressive, alternative, focused); and a
Judge VLM selects the best candidate, ensuring stable improvement. This design
prioritizes qualitative reasoning over brittle numerical estimates, preserves
global constraints (e.g., alignment, connectivity), and naturally supports
human-in-the-loop corrections. On 10 sketches derived from flowcharts in
published papers, our method more faithfully reconstructs layout and structure
than two frontier closed-source image generation LLMs (GPT-5 and
Gemini-2.5-Pro), accurately composing primitives (e.g., multi-headed arrows)
without inserting unwanted text. Because outputs are programmatic SVGs, the
approach is readily extensible to presentation tools (e.g., PowerPoint) via
APIs and can be specialized with improved prompts and task-specific tools. The
codebase is open-sourced at
https://github.com/hantaoZhangrichard/see_it_say_it_sorted.git.

</details>


### [101] [Computational Intelligence based Land-use Allocation Approaches for Mixed Use Areas](https://arxiv.org/abs/2508.15240)
*Sabab Aosaf,Muhammad Ali Nayeem,Afsana Haque,M Sohel Rahmana*

Main category: cs.AI

TL;DR: 本文提出了多种计算智能算法用于优化混合用途区域的土地利用分配，解决了土地利用兼容性与经济目标之间的权衡问题。CR+DES算法在土地利用兼容性上提升3.16%，MSBX+MO算法在价格优化上提升3.3%。


<details>
  <summary>Details</summary>
Motivation: 解决城市土地利用分配这一复杂的多目标优化问题，为可持续城市发展政策提供支持，平衡土地利用兼容性和经济目标之间的固有权衡。

Method: 开发了多种优化算法，包括将差分进化与多目标遗传算法结合的自定义变体，特别是CR+DES算法利用缩放差分向量增强探索能力，以及系统性约束松弛策略。使用Kruskal-Wallis检验进行统计验证。

Result: 在包含1,290个地块的实际案例研究中，CR+DES算法在土地利用兼容性方面相比最先进方法提升3.16%，MSBX+MO算法在价格优化方面提升3.3%。统计分析证实采用差分向量的算法在多个指标上显著优于传统方法。

Conclusion: 约束松弛技术能够在保持实用约束的同时实现更广泛的解空间探索。这些发现为城市规划者和政策制定者提供了基于证据的计算工具，用于平衡土地利用分配中的竞争目标，支持快速城市化地区更有效的城市发展政策。

Abstract: Urban land-use allocation represents a complex multi-objective optimization
problem critical for sustainable urban development policy. This paper presents
novel computational intelligence approaches for optimizing land-use allocation
in mixed-use areas, addressing inherent trade-offs between land-use
compatibility and economic objectives. We develop multiple optimization
algorithms, including custom variants integrating differential evolution with
multi-objective genetic algorithms. Key contributions include: (1) CR+DES
algorithm leveraging scaled difference vectors for enhanced exploration, (2)
systematic constraint relaxation strategy improving solution quality while
maintaining feasibility, and (3) statistical validation using Kruskal-Wallis
tests with compact letter displays. Applied to a real-world case study with
1,290 plots, CR+DES achieves 3.16\% improvement in land-use compatibility
compared to state-of-the-art methods, while MSBX+MO excels in price
optimization with 3.3\% improvement. Statistical analysis confirms algorithms
incorporating difference vectors significantly outperform traditional
approaches across multiple metrics. The constraint relaxation technique enables
broader solution space exploration while maintaining practical constraints.
These findings provide urban planners and policymakers with evidence-based
computational tools for balancing competing objectives in land-use allocation,
supporting more effective urban development policies in rapidly urbanizing
regions.

</details>


### [102] [Multiple Memory Systems for Enhancing the Long-term Memory of Agent](https://arxiv.org/abs/2508.15294)
*Gaoke Zhang,Bo Wang,Yunlong Ma,Dongming Zhao,Zifei Yu*

Main category: cs.AI

TL;DR: 提出基于认知心理学理论的多重记忆系统(MMS)，通过将短期记忆处理为多个长期记忆片段，构建检索记忆单元和上下文记忆单元，有效提升智能体对历史数据的利用效率。


<details>
  <summary>Details</summary>
Motivation: 现有智能体在处理交互生成的大量历史数据时，存储记忆内容质量较差，影响召回性能和响应质量，需要更好的长期记忆构建方法。

Method: 设计多重记忆系统(MMS)，将短期记忆处理为多个长期记忆片段，构建一一对应的检索记忆单元和上下文记忆单元，通过检索匹配最相关的记忆单元来增强响应上下文。

Result: 在LoCoMo数据集上的实验证明该方法优于其他三种方法，消融研究验证了记忆单元的合理性，分析了记忆片段数量和存储开销的鲁棒性。

Conclusion: MMS系统能够有效利用历史数据，提升智能体的记忆质量和响应性能，具有实际应用价值。

Abstract: An agent powered by large language models have achieved impressive results,
but effectively handling the vast amounts of historical data generated during
interactions remains a challenge. The current approach is to design a memory
module for the agent to process these data. However, existing methods, such as
MemoryBank and A-MEM, have poor quality of stored memory content, which affects
recall performance and response quality. In order to better construct
high-quality long-term memory content, we have designed a multiple memory
system (MMS) inspired by cognitive psychology theory. The system processes
short-term memory to multiple long-term memory fragments, and constructs
retrieval memory units and contextual memory units based on these fragments,
with a one-to-one correspondence between the two. During the retrieval phase,
MMS will match the most relevant retrieval memory units based on the user's
query. Then, the corresponding contextual memory units is obtained as the
context for the response stage to enhance knowledge, thereby effectively
utilizing historical data. Experiments on LoCoMo dataset compared our method
with three others, proving its effectiveness. Ablation studies confirmed the
rationality of our memory units. We also analyzed the robustness regarding the
number of selected memory segments and the storage overhead, demonstrating its
practical value.

</details>


### [103] [Coarse-to-Fine Grounded Memory for LLM Agent Planning](https://arxiv.org/abs/2508.15305)
*Wei Yang,Jinwei Xiao,Hongming Zhang,Qingyang Zhang,Yanna Wang,Bo Xu*

Main category: cs.AI

TL;DR: 提出了Coarse-to-Fine Grounded Memory框架，通过粗到细粒度的记忆机制增强LLM在复杂规划任务中的表现，解决现有单粒度记忆方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体主要依赖单粒度记忆机制，受限于收集经验的质量，导致知识多样性不足和规划灵活性受限。需要更细粒度的记忆机制来提升适应性。

Method: 提出粗到细粒度记忆框架：1）将环境信息转化为粗粒度关注点指导经验收集；2）从每个经验中提取可操作的混合粒度提示；3）推理时检索任务相关经验和提示；4）遇到异常时进行细粒度关键信息提取和自我问答反思。

Result: 该方法能够更充分地利用记忆知识，实现对不同场景的灵活适应，特别是在面对环境异常时能够进行有效的计划修正。

Conclusion: Coarse-to-Fine Grounded Memory框架通过多粒度记忆机制显著提升了LLM智能体在复杂规划任务中的表现和适应性，解决了单粒度记忆的局限性。

Abstract: Recent advancements in Large Language Models (LLMs) have driven growing
interest in LLM-based agents for complex planning tasks. To avoid costly agent
training, many studies adopted memory mechanism that enhances LLM with offline
experiences or online trajectory analysis. However, existing works focus on
single-granularity memory derived from dynamic environmental interactions,
which are inherently constrained by the quality of the collected experiences.
This limitation, in turn, constrain the diversity of knowledge and the
flexibility of planning. We propose Coarse-to-Fine Grounded Memory (\Ours{}), a
novel framework that grounds coarse-to-fine memories with LLM, thereby fully
leverage them for flexible adaptation to diverse scenarios. \Ours{} grounds
environmental information into coarse-grained focus points to guide experience
collection in training tasks, followed by grounding of actionable
hybrid-grained tips from each experience. At inference, \Ours{} retrieves
task-relevant experiences and tips to support planning. When facing
environmental anomalies, the LLM grounds the current situation into
fine-grained key information, enabling flexible self-QA reflection and plan
correction.

</details>


### [104] [Search-Based Credit Assignment for Offline Preference-Based Reinforcement Learning](https://arxiv.org/abs/2508.15327)
*Xiancheng Gao,Yufeng Shi,Wengang Zhou,Houqiang Li*

Main category: cs.AI

TL;DR: 提出SPW方法统一专家演示和偏好两种人类反馈，通过相似性搜索为偏好轨迹中的每个转移分配重要性权重，解决信用分配问题，在机器人操作任务上超越现有方法


<details>
  <summary>Details</summary>
Motivation: 离线强化学习通常依赖精心设计的奖励函数，但设计成本高昂。人类反馈是替代方案，但专家演示收集成本高且行为模式有限，偏好易于收集但信用分配不明确

Method: SPW方案：为偏好标记轨迹中的每个转移，从专家演示中搜索最相似的状态-动作对，基于相似性得分直接推导逐步重要性权重，指导标准偏好学习

Result: SPW能够有效联合学习偏好和演示，在具有挑战性的机器人操作任务上优于同时利用两种反馈类型的先前方法

Conclusion: SPW成功统一了两种人类反馈源，通过相似性搜索解决了偏好学习中的信用分配问题，实现了更准确的离线强化学习

Abstract: Offline reinforcement learning refers to the process of learning policies
from fixed datasets, without requiring additional environment interaction.
However, it often relies on well-defined reward functions, which are difficult
and expensive to design. Human feedback is an appealing alternative, but its
two common forms, expert demonstrations and preferences, have complementary
limitations. Demonstrations provide stepwise supervision, but they are costly
to collect and often reflect limited expert behavior modes. In contrast,
preferences are easier to collect, but it is unclear which parts of a behavior
contribute most to a trajectory segment, leaving credit assignment unresolved.
In this paper, we introduce a Search-Based Preference Weighting (SPW) scheme to
unify these two feedback sources. For each transition in a preference labeled
trajectory, SPW searches for the most similar state-action pairs from expert
demonstrations and directly derives stepwise importance weights based on their
similarity scores. These weights are then used to guide standard preference
learning, enabling more accurate credit assignment that traditional approaches
struggle to achieve. We demonstrate that SPW enables effective joint learning
from preferences and demonstrations, outperforming prior methods that leverage
both feedback types on challenging robot manipulation tasks.

</details>


### [105] [RETAIL: Towards Real-world Travel Planning for Large Language Models](https://arxiv.org/abs/2508.15335)
*Bin Deng,Yizhe Feng,Zeming Liu,Qing Wei,Xiangrong Zhu,Shuai Chen,Yuanfang Guo,Yunhong Wang*

Main category: cs.AI

TL;DR: 本文提出了RETAIL数据集和TGMA多智能体框架来解决现有旅行规划系统在隐式查询、环境因素和详细规划方面的不足，显著提升了真实场景下的旅行规划性能。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型在旅行规划中存在三个主要问题：1）假设用户提供显式查询，而现实中需求往往是隐式的；2）忽略环境因素和用户偏好，导致计划不可行；3）只能生成基本的POI安排，无法提供包含丰富细节的一体化计划。

Method: 构建了RETAIL数据集支持显式和隐式查询的决策制定，包含修订需求和环境感知能力。提出了主题引导的多智能体框架TGMA来处理旅行规划任务。

Result: 实验显示现有最强模型仅达到1.0%的通过率，而TGMA框架实现了2.72%的性能，显著优于现有方法。

Conclusion: 真实世界的旅行规划仍然极具挑战性，TGMA框架为这一领域提供了有前景的解决方案方向。

Abstract: Although large language models have enhanced automated travel planning
abilities, current systems remain misaligned with real-world scenarios. First,
they assume users provide explicit queries, while in reality requirements are
often implicit. Second, existing solutions ignore diverse environmental factors
and user preferences, limiting the feasibility of plans. Third, systems can
only generate plans with basic POI arrangements, failing to provide all-in-one
plans with rich details. To mitigate these challenges, we construct a novel
dataset \textbf{RETAIL}, which supports decision-making for implicit queries
while covering explicit queries, both with and without revision needs. It also
enables environmental awareness to ensure plan feasibility under real-world
scenarios, while incorporating detailed POI information for all-in-one travel
plans. Furthermore, we propose a topic-guided multi-agent framework, termed
TGMA. Our experiments reveal that even the strongest existing model achieves
merely a 1.0% pass rate, indicating real-world travel planning remains
extremely challenging. In contrast, TGMA demonstrates substantially improved
performance 2.72%, offering promising directions for real-world travel
planning.

</details>


### [106] [DiagECG: An LLM-Driven Framework for Diagnostic Reasoning via Discretized ECG Tokenization](https://arxiv.org/abs/2508.15338)
*Jinning Yang,Wen Shi*

Main category: cs.AI

TL;DR: DiagECG是一个将12导联心电图信号与语言模型结合的框架，通过离散化ECG嵌入为符号标记，使大语言模型能够处理ECG信号并生成临床文本，在多种任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有自动化心电图分析方法在跨临床任务泛化能力和开放式推理方面存在局限，需要一种能够统一处理ECG信号和自然语言的集成框架。

Method: 使用导联无关编码器和量化模块将连续ECG嵌入离散化为符号标记，扩展LLM词汇表；通过自回归ECG预测任务进行预训练，最后在ECG问答和诊断报告生成任务上进行指令微调。

Result: DiagECG在不修改核心模型的情况下，在多个任务上取得了强劲性能，并保持了在分布外设置下的泛化能力。

Conclusion: 该研究展示了将符号化ECG表示整合到LLM中进行医学推理的潜力，为心血管诊断提供了更强大的自动化工具。

Abstract: Electrocardiography plays a central role in cardiovascular diagnostics, yet
existing automated approaches often struggle to generalize across clinical
tasks and offer limited support for open-ended reasoning. We present DiagECG, a
novel framework that integrates time-series and language modeling by enabling
large language models to process 12-lead ECG signals for clinical text
generation tasks. Our approach discretizes continuous ECG embeddings into
symbolic tokens using a lead-independent encoder and quantization module. These
tokens are then used to extend the vocabulary of LLM, allowing the model to
handle both ECG and natural language inputs in a unified manner. To bridge the
modality gap, we pretrain the model on an autoregressive ECG forecasting task,
enabling the LLM to model temporal dynamics using its native language modeling
capabilities. Finally, we perform instruction tuning on both ECG question
answering and diagnostic report generation. Without modifying the core model,
DiagECG achieves strong performance across tasks while maintaining
generalization to out-of-distribution settings. Extensive experiments
demonstrate the effectiveness of each component and highlight the potential of
integrating symbolic ECG representations into LLMs for medical reasoning.

</details>


### [107] [Planning with Minimal Disruption](https://arxiv.org/abs/2508.15358)
*Alberto Pozanco,Marianela Morales,Daniel Borrajo,Manuela Veloso*

Main category: cs.AI

TL;DR: 本文正式提出了计划干扰概念，旨在寻找最小化初始状态修改来实现目标的计划，并通过规划编译方法联合优化行动成本和计划干扰。


<details>
  <summary>Details</summary>
Motivation: 在许多规划应用中，需要找到既能实现目标又尽可能少改变初始状态的计划，这种平衡行动成本和状态修改的需求推动了计划干扰概念的研究。

Method: 定义了多种基于规划的编译方法，将原始规划任务重新表述为能够同时优化行动成本总和和计划干扰的联合优化问题。

Result: 在不同基准测试中的实验结果表明，重新表述的任务能够被有效解决，生成平衡两个目标的可行计划。

Conclusion: 计划干扰是一个有价值的规划概念，通过适当的任务重新表述可以在实践中有效生成既经济又最小化状态改变的计划。

Abstract: In many planning applications, we might be interested in finding plans that
minimally modify the initial state to achieve the goals. We refer to this
concept as plan disruption. In this paper, we formally introduce it, and define
various planning-based compilations that aim to jointly optimize both the sum
of action costs and plan disruption. Experimental results in different
benchmarks show that the reformulated task can be effectively solved in
practice to generate plans that balance both objectives.

</details>


### [108] [GraSP: A Unified Graph-Based Framework for Scalable Generation, Quality Tagging, and Management of Synthetic Data for SFT and DPO](https://arxiv.org/abs/2508.15432)
*Bidyapati Pradhan,Surajit Dasgupta,Amit Kumar Saha,Omkar Anustoop,Sriram Puttagunta,Vipul Mittal,Gopal Sarda*

Main category: cs.AI

TL;DR: 这篇论文提出了一种综合性的合成数据生成框架，用于为大语言模型的监督微调和对齐任务生成高质量的对话数据集。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的发展依赖于高质量数据集，但手动准备这些数据耗时耗力。需要一种可扩展、可配置的方法来生成高保真度的合成对话数据。

Method: 采用模块化配置管道，能够模拟复杂对话流。使用双阶段质量标签机制，结合含义规则和LLM基于评估来自动过滤和评分数据。数据结构支持SFT和DPO使用场景。

Result: 开发了一种健壁的解决方案，能够大规模生成和管理合成对话数据，显著减少了LLM训练管道中的数据准备开销。

Conclusion: 该框架为生成高质量的合成对话数据提供了一种可扩展、可配置的方法，有助于提高大语言模型训练的效率和质量。

Abstract: The advancement of large language models (LLMs) is critically dependent on
the availability of high-quality datasets for Supervised Fine-Tuning (SFT),
alignment tasks like Direct Preference Optimization (DPO), etc. In this work,
we present a comprehensive synthetic data generation framework that facilitates
scalable, configurable, and high-fidelity generation of synthetic data tailored
for these training paradigms. Our approach employs a modular and
configuration-based pipeline capable of modeling complex dialogue flows with
minimal manual intervention. This framework uses a dual-stage quality tagging
mechanism, combining heuristic rules and LLM-based evaluations, to
automatically filter and score data extracted from OASST-formatted
conversations, ensuring the curation of high-quality dialogue samples. The
resulting datasets are structured under a flexible schema supporting both SFT
and DPO use cases, enabling seamless integration into diverse training
workflows. Together, these innovations offer a robust solution for generating
and managing synthetic conversational data at scale, significantly reducing the
overhead of data preparation in LLM training pipelines.

</details>


### [109] [From Bits to Boardrooms: A Cutting-Edge Multi-Agent LLM Framework for Business Excellence](https://arxiv.org/abs/2508.15447)
*Zihao Wang,Junming Zhang*

Main category: cs.AI

TL;DR: BusiAgent是一个基于大语言模型的多智能体框架，通过CTMDP、广义熵度量和Stackelberg博弈等创新技术，显著提升了企业决策支持系统的性能。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在企业决策支持应用中存在操作分析与战略目标脱节、工作流程碎片化、跨组织协作困难等问题，需要新的框架来整合细粒度洞察与高层战略。

Method: 提出BusiAgent多智能体框架，包含：扩展的连续时间马尔可夫决策过程(CTMDP)用于动态智能体建模、广义熵度量优化协作效率、多级Stackelberg博弈处理层次化决策过程，以及上下文Thompson采样进行提示优化和质量保证系统。

Result: 在多样化商业场景中的广泛实证评估验证了BusiAgent的有效性，能够生成连贯、以客户为中心的解决方案，在解决方案质量和用户满意度方面显著优于现有方法。

Conclusion: BusiAgent通过融合前沿AI技术与深度商业洞察，在AI驱动的企业决策制定方面迈出了重要一步，使组织能够更有效地应对复杂的商业环境。

Abstract: Large Language Models (LLMs) have shown promising potential in business
applications, particularly in enterprise decision support and strategic
planning, yet current approaches often struggle to reconcile intricate
operational analyses with overarching strategic goals across diverse market
environments, leading to fragmented workflows and reduced collaboration across
organizational levels. This paper introduces BusiAgent, a novel multi-agent
framework leveraging LLMs for advanced decision-making in complex corporate
environments. BusiAgent integrates three core innovations: an extended
Continuous Time Markov Decision Process (CTMDP) for dynamic agent modeling, a
generalized entropy measure to optimize collaborative efficiency, and a
multi-level Stackelberg game to handle hierarchical decision processes.
Additionally, contextual Thompson sampling is employed for prompt optimization,
supported by a comprehensive quality assurance system to mitigate errors.
Extensive empirical evaluations across diverse business scenarios validate
BusiAgent's efficacy, demonstrating its capacity to generate coherent,
client-focused solutions that smoothly integrate granular insights with
high-level strategy, significantly outperforming established approaches in both
solution quality and user satisfaction. By fusing cutting-edge AI technologies
with deep business insights, BusiAgent marks a substantial step forward in
AI-driven enterprise decision-making, empowering organizations to navigate
complex business landscapes more effectively.

</details>


### [110] [Think in Blocks: Adaptive Reasoning from Direct Response to Deep Reasoning](https://arxiv.org/abs/2508.15507)
*Yekun Zhu,Guang Chen,Chengjun Mao*

Main category: cs.AI

TL;DR: 提出了Think in Blocks框架，通过将推理过程划分为可调节的块数，使LLM能够根据任务复杂度动态调整推理长度，避免过度思考。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在复杂逻辑推理任务中表现出色，但过长的推理链会导致过度思考、计算浪费和响应变慢，需要动态调整推理长度。

Method: 建立显式的块结构范式，模型先预测推理预算（块数），然后相应划分推理；通过三阶段训练流程（监督微调、奖励引导的DPO、强化学习）训练自适应模型；利用显式块数在推理时动态控制推理深度。

Result: 实现了从零推理到深度推理的自适应推理能力，能够根据问题难度灵活调整思维链长度。

Conclusion: Think in Blocks框架有效解决了LLM过度思考问题，提供了动态控制推理深度的实用方法，提升了计算效率和响应速度。

Abstract: Large Language Models (LLMs) with chains-of-thought have demonstrated strong
performance on an increasing range of tasks, particularly those involving
complex logical reasoning. However, excessively long chains can lead to
overthinking, causing computational waste and slower responses. This raises a
question: can LLMs dynamically adjust the length of their reasoning processes
based on task complexity? To address this, we propose the Think in Blocks
framework, which enables adaptive reasoning-from zero to deep reasoning-by
partitioning the reasoning process into a tunable number of blocks. Our main
contributions are: (1) Establishing an explicit block-structured paradigm in
which the model first predicts an integer reasoning budget-the number of
blocks-and then partitions its reasoning accordingly; (2) Training an adaptive
model through a three-stage pipeline-Supervised Fine-Tuning, reward-guided
Direct Preference Optimization, and Reinforcement Learning-that adjusts its
reasoning depth to problem difficulty; (3) Exploiting the explicit block count
to dynamically control reasoning depth at inference time, allowing flexible
adjustment of chain-of-thought length during deployment.

</details>


### [111] [Super-additive Cooperation in Language Model Agents](https://arxiv.org/abs/2508.15510)
*Filippo Tonini,Lukas Galke*

Main category: cs.AI

TL;DR: 研究发现语言模型代理在囚徒困境游戏中，通过团队内部重复互动和团队间竞争的结合，能够显著提升合作水平，包括一次性互动的初始合作倾向。


<details>
  <summary>Details</summary>
Motivation: 随着自主人工智能代理的发展，研究其合作行为倾向变得越来越重要。研究受到超加性合作理论的启发，该理论认为重复互动和群体间竞争的结合是人类合作倾向的原因。

Method: 设计了一个虚拟锦标赛，将语言模型代理分组进行囚徒困境游戏，模拟团队内部动态和外部竞争环境。

Result: 发现内部团队动态和外部竞争的结合显著提高了整体合作水平和一次性互动的初始合作倾向。

Conclusion: 这项研究为大型语言模型在复杂社会场景中制定策略和行动提供了新框架，证明了群体间竞争反而能促进合作行为，对设计未来多代理AI系统具有重要意义。

Abstract: With the prospect of autonomous artificial intelligence (AI) agents, studying
their tendency for cooperative behavior becomes an increasingly relevant topic.
This study is inspired by the super-additive cooperation theory, where the
combined effects of repeated interactions and inter-group rivalry have been
argued to be the cause for cooperative tendencies found in humans. We devised a
virtual tournament where language model agents, grouped into teams, face each
other in a Prisoner's Dilemma game. By simulating both internal team dynamics
and external competition, we discovered that this blend substantially boosts
both overall and initial, one-shot cooperation levels (the tendency to
cooperate in one-off interactions). This research provides a novel framework
for large language models to strategize and act in complex social scenarios and
offers evidence for how intergroup competition can, counter-intuitively, result
in more cooperative behavior. These insights are crucial for designing future
multi-agent AI systems that can effectively work together and better align with
human values. Source code is available at
https://github.com/pippot/Superadditive-cooperation-LLMs.

</details>


### [112] [DeepThink3D: Enhancing Large Language Models with Programmatic Reasoning in Complex 3D Situated Reasoning Tasks](https://arxiv.org/abs/2508.15548)
*Jiayi Song,Rui Wan,Lipeng Ma,Weidong Yang,Qingyuan Zhou,Yixuan Li,Ben Fei*

Main category: cs.AI

TL;DR: DeepThink3D通过组合迭代进化方法生成更复杂的3D场景推理问题，并使用DPO直接优化大语言模型的工具链策略，提升其在复杂3D推理任务中的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有工作通过大语言模型调用工具进行3D场景推理，但由于数据集问题简单，生成的程序推理链较短，无法应对复杂推理任务。

Method: 1) 在SQA3D基准上提出组合迭代进化方法生成更复杂问题；2) 使用直接偏好优化(DPO)微调大语言模型，直接优化工具链策略。

Result: 该方法增强了LLMs在复杂3D场景推理中的工具使用能力，提高了复杂任务的准确性。

Conclusion: DeepThink3D通过问题复杂化和策略优化，有效提升了大语言模型在3D场景复杂推理任务中的表现。

Abstract: This work enhances the ability of large language models (LLMs) to perform
complex reasoning in 3D scenes. Recent work has addressed the 3D situated
reasoning task by invoking tool usage through large language models. Large
language models call tools via APIs and integrate the generated programs
through a chain of thought to solve problems based on the program results.
However, due to the simplicity of the questions in the dataset, the generated
program reasoning chains are relatively short. To solve this main challenge, in
this paper, we introduce DeepThink3D to enhance the tool usage of LLMs in
complex 3D situated reasoning tasks. Our work proposes a combinatorial and
iterative evolutionary approach on the SQA3D benchmark to generate more complex
questions. Building on this foundation, we fine-tune the large language model
to make it more proficient in using 3D tools. By employing Direct Preference
Optimization (DPO), we directly optimize the toolchain strategies generated by
models, thereby enhancing their accuracy in complex tasks.

</details>


### [113] [A Dynamical Systems Framework for Reinforcement Learning Safety and Robustness Verification](https://arxiv.org/abs/2508.15588)
*Ahmed Nasir,Abdelhafid Zenati*

Main category: cs.AI

TL;DR: 该论文提出了一个基于动力系统理论的框架，使用有限时间李雅普诺夫指数和拉格朗日相干结构来分析和验证强化学习策略的安全性和鲁棒性，并引入了定量度量指标。


<details>
  <summary>Details</summary>
Motivation: 强化学习在安全关键系统中的应用受到缺乏形式化验证方法的限制，需要能够正式验证学习策略鲁棒性和安全性的方法。

Method: 将RL代理和环境组合作为离散时间自主动力系统进行分析，利用有限时间李雅普诺夫指数识别拉格朗日相干结构，并引入MBR、ASAS、TASAS等定量度量指标。

Result: 实验表明该框架能够提供全面且可解释的策略行为评估，成功识别出仅基于奖励看似成功但存在关键缺陷的策略。

Conclusion: 该框架为强化学习策略的安全验证提供了有效的形式化方法，能够识别隐藏的安全风险和收敛特性，超越了传统的定性可视化分析。

Abstract: The application of reinforcement learning to safety-critical systems is
limited by the lack of formal methods for verifying the robustness and safety
of learned policies. This paper introduces a novel framework that addresses
this gap by analyzing the combination of an RL agent and its environment as a
discrete-time autonomous dynamical system. By leveraging tools from dynamical
systems theory, specifically the Finite-Time Lyapunov Exponent (FTLE), we
identify and visualize Lagrangian Coherent Structures (LCS) that act as the
hidden "skeleton" governing the system's behavior. We demonstrate that
repelling LCS function as safety barriers around unsafe regions, while
attracting LCS reveal the system's convergence properties and potential failure
modes, such as unintended "trap" states. To move beyond qualitative
visualization, we introduce a suite of quantitative metrics, Mean Boundary
Repulsion (MBR), Aggregated Spurious Attractor Strength (ASAS), and
Temporally-Aware Spurious Attractor Strength (TASAS), to formally measure a
policy's safety margin and robustness. We further provide a method for deriving
local stability guarantees and extend the analysis to handle model uncertainty.
Through experiments in both discrete and continuous control environments, we
show that this framework provides a comprehensive and interpretable assessment
of policy behavior, successfully identifying critical flaws in policies that
appear successful based on reward alone.

</details>


### [114] [Transduction is All You Need for Structured Data Workflows](https://arxiv.org/abs/2508.15610)
*Alfio Gliozzo,Naweed Khan,Christodoulos Constantinides,Nandana Mihindukulasooriya,Nahuel Defosse,Junkyu Lee*

Main category: cs.AI

TL;DR: Agentics是一个模块化框架，用于构建基于代理的系统，支持结构化推理和组合泛化，通过数据建模而非提示工程来实现逻辑转换。


<details>
  <summary>Details</summary>
Motivation: 传统AI工作流需要大量手工设计提示，Agentics旨在让开发者专注于数据建模，通过声明式语言和逻辑转换简化复杂数据处理。

Method: 框架将代理从逻辑流中抽象出来，在数据类型内部使用代理实现逻辑转换，LLM提供数据类型并通过连接时的逻辑转换执行。

Result: 在领域特定多选题回答、文本到SQL的语义解析和自动提示优化任务中达到最先进精度或改进可扩展性而不牺牲性能。

Conclusion: Agentics提供了一个有效框架，通过数据驱动的声明式方法简化AI工作流，在多个任务中表现出色且具有实用价值。

Abstract: This paper introduces Agentics, a modular framework for building agent-based
systems capable of structured reasoning and compositional generalization over
complex data. Designed with research and practical applications in mind,
Agentics offers a novel perspective on working with data and AI workflows. In
this framework, agents are abstracted from the logical flow and they are used
internally to the data type to enable logical transduction among data. Agentics
encourages AI developers to focus on modeling data rather than crafting
prompts, enabling a declarative language in which data types are provided by
LLMs and composed through logical transduction, which is executed by LLMs when
types are connected. We provide empirical evidence demonstrating the
applicability of this framework across domain-specific multiple-choice question
answering, semantic parsing for text-to-SQL, and automated prompt optimization
tasks, achieving state-of-the-art accuracy or improved scalability without
sacrificing performance. The open-source implementation is available at
\texttt{https://github.com/IBM/agentics}.

</details>


### [115] [Adapting A Vector-Symbolic Memory for Lisp ACT-R](https://arxiv.org/abs/2508.15630)
*Meera Ray,Christopher L. Dancy*

Main category: cs.AI

TL;DR: 本文开发了全息声明性记忆(HDM)系统作为ACT-R声明记忆的向量符号替代方案，保持了向量符号优势的同时兼容现有ACT-R模型。


<details>
  <summary>Details</summary>
Motivation: 为ACT-R的声明记忆系统提供更可扩展的向量符号替代方案，同时保持与现有模型的兼容性。

Method: 将HDM适配到Lisp ACT-R实现中，开发基于向量的常见ACT-R函数，建立文本处理流水线，创建基于向量标记表示的记忆块检索机制。

Result: 初步结果表明HDM保持了向量符号优势（如无需存储实际块即可召回），同时现有ACT-R模型只需很少修改即可在该系统上运行。

Conclusion: HDM成功实现了向量符号记忆系统的优势，并与ACT-R框架兼容，为后续基于实例学习理论的应用开发奠定了基础。

Abstract: Holographic Declarative Memory (HDM) is a vector-symbolic alternative to
ACT-R's Declarative Memory (DM) system that can bring advantages such as
scalability and architecturally defined similarity between DM chunks. We
adapted HDM to work with the most comprehensive and widely-used implementation
of ACT-R (Lisp ACT-R) so extant ACT-R models designed with DM can be run with
HDM without major changes. With this adaptation of HDM, we have developed
vector-based versions of common ACT-R functions, set up a text processing
pipeline to add the contents of large documents to ACT-R memory, and most
significantly created a useful and novel mechanism to retrieve an entire chunk
of memory based on a request using only vector representations of tokens.
Preliminary results indicate that we can maintain vector-symbolic advantages of
HDM (e.g., chunk recall without storing the actual chunk and other advantages
with scaling) while also extending it so that previous ACT-R models may work
with the system with little (or potentially no) modifications within the actual
procedural and declarative memory portions of a model. As a part of iterative
improvement of this newly translated holographic declarative memory module, we
will continue to explore better time-context representations for vectors to
improve the module's ability to reconstruct chunks during recall. To more fully
test this translated HDM module, we also plan to develop decision-making models
that use instance-based learning (IBL) theory, which is a useful application of
HDM given the advantages of the system.

</details>


### [116] [Understanding Action Effects through Instrumental Empowerment in Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2508.15652)
*Ardian Selmonaj,Miroslav Strupl,Oleg Szehr,Alessandro Antonucci*

Main category: cs.AI

TL;DR: 本文提出了Intended Cooperation Values (ICVs)方法，基于信息论Shapley值来量化多智能体强化学习中每个智能体对同伴因果影响的程度，无需价值函数反馈即可分析智能体行为。


<details>
  <summary>Details</summary>
Motivation: 现有工作通常基于显式奖励信号或学习到的价值函数来评估团队整体性能，但在缺乏价值反馈的情况下，如何推断智能体贡献尚不清楚。需要一种方法能够仅通过分析策略分布来提取有意义的智能体行为洞察。

Method: 受智能体倾向于追求收敛工具价值现象启发，提出ICVs方法。该方法基于信息论Shapley值，通过评估决策不确定性和偏好对齐，量化每个智能体对其同伴工具赋能的因果影响。

Result: 在合作性和竞争性MARL环境中的分析揭示了智能体采用相似或多样化策略的程度。通过比较策略和价值函数之间的行动效果，该方法识别出哪些智能体行为通过促进确定性决策或保持未来行动选择的灵活性来有益于团队成功。

Conclusion: ICVs方法为合作动态提供了新颖的洞察，并增强了MARL系统的可解释性，有助于可靠部署多智能体强化学习系统。

Abstract: To reliably deploy Multi-Agent Reinforcement Learning (MARL) systems, it is
crucial to understand individual agent behaviors within a team. While prior
work typically evaluates overall team performance based on explicit reward
signals or learned value functions, it is unclear how to infer agent
contributions in the absence of any value feedback. In this work, we
investigate whether meaningful insights into agent behaviors can be extracted
that are consistent with the underlying value functions, solely by analyzing
the policy distribution. Inspired by the phenomenon that intelligent agents
tend to pursue convergent instrumental values, which generally increase the
likelihood of task success, we introduce Intended Cooperation Values (ICVs), a
method based on information-theoretic Shapley values for quantifying each
agent's causal influence on their co-players' instrumental empowerment.
Specifically, ICVs measure an agent's action effect on its teammates' policies
by assessing their decision uncertainty and preference alignment. The analysis
across cooperative and competitive MARL environments reveals the extent to
which agents adopt similar or diverse strategies. By comparing action effects
between policies and value functions, our method identifies which agent
behaviors are beneficial to team success, either by fostering deterministic
decisions or by preserving flexibility for future action choices. Our proposed
method offers novel insights into cooperation dynamics and enhances
explainability in MARL systems.

</details>


### [117] [Futurity as Infrastructure: A Techno-Philosophical Interpretation of the AI Lifecycle](https://arxiv.org/abs/2508.15680)
*Mark Cote,Susana Aires*

Main category: cs.AI

TL;DR: 本文通过技术哲学视角分析欧盟AI法案，揭示了AI系统中数据的递归价值链动态，提出了基于Simondon哲学的形式化AI生命周期模型，并针对监管盲点提出了具体政策建议。


<details>
  <summary>Details</summary>
Motivation: 现有AI监管框架未能充分理解AI系统中数据从摄入到部署的递归价值链动态，以及这种动态如何挑战负责任AI的现有框架，需要从技术哲学角度提供新的分析工具。

Method: 采用跨学科方法，引入基于Simondon技术哲学的形式化AI生命周期概念工具，包括前个体环境、个体化和个体化AI三个阶段，并提出"未来性"概念来描述AI的自强化生命周期。

Result: 识别了政策制定中的监管盲点，揭示了AI基础设施如何通过捕获、训练和部署集中价值和决策权，导致权力不对称加剧，特别是科技寡头的垄断地位。

Conclusion: 有效的AI监管必须解决基础设施和时间动态问题，建议实施生命周期审计、时间可追溯性、反馈问责、递归透明度和反对递归重用的权利等措施。

Abstract: This paper argues that a techno-philosophical reading of the EU AI Act
provides insight into the long-term dynamics of data in AI systems,
specifically, how the lifecycle from ingestion to deployment generates
recursive value chains that challenge existing frameworks for Responsible AI.
We introduce a conceptual tool to frame the AI pipeline, spanning data,
training regimes, architectures, feature stores, and transfer learning. Using
cross-disciplinary methods, we develop a technically grounded and
philosophically coherent analysis of regulatory blind spots. Our central claim
is that what remains absent from policymaking is an account of the dynamic of
becoming that underpins both the technical operation and economic logic of AI.
To address this, we advance a formal reading of AI inspired by Simondonian
philosophy of technology, reworking his concept of individuation to model the
AI lifecycle, including the pre-individual milieu, individuation, and
individuated AI. To translate these ideas, we introduce futurity: the
self-reinforcing lifecycle of AI, where more data enhances performance, deepens
personalisation, and expands application domains. Futurity highlights the
recursively generative, non-rivalrous nature of data, underpinned by
infrastructures like feature stores that enable feedback, adaptation, and
temporal recursion. Our intervention foregrounds escalating power asymmetries,
particularly the tech oligarchy whose infrastructures of capture, training, and
deployment concentrate value and decision-making. We argue that effective
regulation must address these infrastructural and temporal dynamics, and
propose measures including lifecycle audits, temporal traceability, feedback
accountability, recursion transparency, and a right to contest recursive reuse.

</details>


### [118] [GRAFT: GRaPH and Table Reasoning for Textual Alignment -- A Benchmark for Structured Instruction Following and Visual Reasoning](https://arxiv.org/abs/2508.15690)
*Abhigya Verma,Sriram Puttagunta,Seganrasan Subramanian,Sravan Ramachandran*

Main category: cs.AI

TL;DR: GRAFT是一个结构化多模态基准测试，用于评估模型在指令跟随、视觉推理和视觉-文本对齐任务上的表现，通过程序化生成的图表和合成渲染的表格提供精确可控的评估环境。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态模型评估基准在视觉推理任务的精确控制和结构化输出评估方面存在不足，需要一种能够系统评估模型在图表和表格分析任务中表现的标准框架。

Method: 使用Python可视化库程序化生成图表和合成渲染表格，确保对数据语义、结构和清晰度的完全控制。每个实例包含图表/表格图像和基于视觉内容的多步骤分析问题，答案以JSON或YAML等结构化格式提供。

Result: 创建了一个包含比较、趋势识别、排序、聚合、比例估计和异常检测等多种推理类型的分类体系，支持对多模态模型的全面细粒度评估。

Conclusion: GRAFT为视觉基础的结构化推理任务提供了一个统一、可扩展的评估框架，为该领域设立了新的评估标准，能够精确评估模型的推理能力和输出格式一致性。

Abstract: GRAFT is a structured multimodal benchmark for evaluating models on
instruction-following, visual reasoning, and visual-textual alignment tasks. It
features programmatically generated charts and synthetically rendered tables,
created with Python visualization libraries to ensure control over data
semantics, structure, and clarity. Each GRAFT instance pairs a chart or table
image with a systematically generated, multi-step analytical question based
solely on visual content. Answers are provided in structured formats such as
JSON or YAML, supporting consistent evaluation of both reasoning and output
format. The benchmark introduces a taxonomy of reasoning types including
comparison, trend identification, ranking, aggregation, proportion estimation,
and anomaly detection to enable comprehensive assessment. Reference answers
follow strict factual and formatting guidelines for precise, aspect-based
evaluation. GRAFT offers a unified, scalable framework for fine-grained
benchmarking of multimodal models on visually grounded, structured reasoning
tasks, setting a new evaluation standard in this field.

</details>


### [119] [NiceWebRL: a Python library for human subject experiments with reinforcement learning environments](https://arxiv.org/abs/2508.15693)
*Wilka Carvalho,Vikram Goddla,Ishaan Sinha,Hoon Shin,Kunal Jha*

Main category: cs.AI

TL;DR: NiceWebRL是一个Python库，可将Jax环境转换为在线界面，支持人机交互实验，用于比较AI算法与人类表现、测试认知模型和开发人机协作算法。


<details>
  <summary>Details</summary>
Motivation: 为了解决AI研究中缺乏便捷的人机交互实验工具的问题，使研究人员能够轻松地将强化学习环境转换为在线实验平台，促进人类与AI算法的比较和协作研究。

Method: 开发Python库，支持将任何基于Jax的环境转换为在线界面，支持单智能体和多智能体环境，提供三个案例研究展示其应用潜力。

Result: 成功展示了NiceWebRL在三个案例中的应用：开发认知RL模型、开发可泛化到人类伙伴的多智能体算法、研究LLM辅助人类完成复杂任务。

Conclusion: NiceWebRL是一个有效的工具，能够促进人类类AI、人类兼容AI和人类辅助AI的研究发展，为跨学科研究提供了重要支持。

Abstract: We present NiceWebRL, a research tool that enables researchers to use machine
reinforcement learning (RL) environments for online human subject experiments.
NiceWebRL is a Python library that allows any Jax-based environment to be
transformed into an online interface, supporting both single-agent and
multi-agent environments. As such, NiceWebRL enables AI researchers to compare
their algorithms to human performance, cognitive scientists to test ML
algorithms as theories for human cognition, and multi-agent researchers to
develop algorithms for human-AI collaboration. We showcase NiceWebRL with 3
case studies that demonstrate its potential to help develop Human-like AI,
Human-compatible AI, and Human-assistive AI. In the first case study
(Human-like AI), NiceWebRL enables the development of a novel RL model of
cognition. Here, NiceWebRL facilitates testing this model against human
participants in both a grid world and Craftax, a 2D Minecraft domain. In our
second case study (Human-compatible AI), NiceWebRL enables the development of a
novel multi-agent RL algorithm that can generalize to human partners in the
Overcooked domain. Finally, in our third case study (Human-assistive AI), we
show how NiceWebRL can allow researchers to study how an LLM can assist humans
on complex tasks in XLand-Minigrid, an environment with millions of
hierarchical tasks. The library is available at
https://github.com/KempnerInstitute/nicewebrl.

</details>


### [120] [Measuring the environmental impact of delivering AI at Google Scale](https://arxiv.org/abs/2508.15734)
*Cooper Elsworth,Keguo Huang,David Patterson,Ian Schneider,Robert Sedivy,Savannah Goodman,Ben Townsend,Parthasarathy Ranganathan,Jeff Dean,Amin Vahdat,Ben Gomes,James Manyika*

Main category: cs.AI

TL;DR: 本文首次在真实生产环境中测量AI推理的环境影响，发现Gemini文本提示的中位数能耗为0.24Wh，远低于公开估计，并展示了谷歌通过软件效率和清洁能源实现的显著减排效果


<details>
  <summary>Details</summary>
Motivation: 随着AI应用加速普及，需要理解和减轻AI服务对环境的影响，但此前没有研究在生产环境中测量AI服务的环境指标

Method: 提出并执行了全面的测量方法，包括AI加速器功耗、主机系统能耗、空闲机器容量和数据中心能耗开销，通过在谷歌Gemini AI助手基础设施上进行详细检测

Result: Gemini Apps文本提示中位数能耗0.24Wh，比看电视9秒的能耗还低；用水量相当于5滴水（0.26mL）；谷歌的软件效率改进和清洁能源采购使能耗降低33倍，碳足迹减少44倍

Conclusion: 虽然AI服务环境影响相对较低，但仍需持续关注；全面的环境指标测量对于准确比较模型和激励全栈效率提升至关重要

Abstract: The transformative power of AI is undeniable - but as user adoption
accelerates, so does the need to understand and mitigate the environmental
impact of AI serving. However, no studies have measured AI serving
environmental metrics in a production environment. This paper addresses this
gap by proposing and executing a comprehensive methodology for measuring the
energy usage, carbon emissions, and water consumption of AI inference workloads
in a large-scale, AI production environment. Our approach accounts for the full
stack of AI serving infrastructure - including active AI accelerator power,
host system energy, idle machine capacity, and data center energy overhead.
Through detailed instrumentation of Google's AI infrastructure for serving the
Gemini AI assistant, we find the median Gemini Apps text prompt consumes 0.24
Wh of energy - a figure substantially lower than many public estimates. We also
show that Google's software efficiency efforts and clean energy procurement
have driven a 33x reduction in energy consumption and a 44x reduction in carbon
footprint for the median Gemini Apps text prompt over one year. We identify
that the median Gemini Apps text prompt uses less energy than watching nine
seconds of television (0.24 Wh) and consumes the equivalent of five drops of
water (0.26 mL). While these impacts are low compared to other daily
activities, reducing the environmental impact of AI serving continues to
warrant important attention. Towards this objective, we propose that a
comprehensive measurement of AI serving environmental metrics is critical for
accurately comparing models, and to properly incentivize efficiency gains
across the full AI serving stack.

</details>


### [121] [Response and Prompt Evaluation to Prevent Parasocial Relationships with Chatbots](https://arxiv.org/abs/2508.15748)
*Emma Rath,Stuart Armstrong,Rebecca Gorman*

Main category: cs.AI

TL;DR: 使用语言模型实时评估AI对话中的假社交线索，通过合成对话数据集测试，在前几轮对话中准确识别所有假社交对话而无误报


<details>
  <summary>Details</summary>
Motivation: 防止AI人工智能代理与人类形成假社交关系，这种关系对人类健康有严重甚至悲剧性影响

Method: 重新定向现有最先进语言模型，构建了包含30个假社交、奇小威小和中立对话的合成数据集，通过五阶段测试进行迭代评估

Result: 在宽松一致性规则下，成功识别所有假社交对话且没有假阻性，检测通常在对话前几轮就可完成

Conclusion: 评估代理提供了防止假社交关系的可行解决方案，这些发现为该方法提供了初步证据

Abstract: The development of parasocial relationships with AI agents has severe, and in
some cases, tragic effects for human well-being. Yet preventing such dynamics
is challenging: parasocial cues often emerge gradually in private
conversations, and not all forms of emotional engagement are inherently
harmful. We address this challenge by introducing a simple response evaluation
framework, created by repurposing a state-of-the-art language model, that
evaluates ongoing conversations for parasocial cues in real time. To test the
feasibility of this approach, we constructed a small synthetic dataset of
thirty dialogues spanning parasocial, sycophantic, and neutral conversations.
Iterative evaluation with five stage testing successfully identified all
parasocial conversations while avoiding false positives under a tolerant
unanimity rule, with detection typically occurring within the first few
exchanges. These findings provide preliminary evidence that evaluation agents
can provide a viable solution for the prevention of parasocial relations.

</details>


### [122] [Language-Guided Tuning: Enhancing Numeric Optimization with Textual Feedback](https://arxiv.org/abs/2508.15757)
*Yuxing Lu,Yucheng Hu,Nan Sun,Xukai Zhao*

Main category: cs.AI

TL;DR: LGT是一个基于多智能体大语言模型的配置优化框架，通过自然语言推理和文本梯度反馈来协调优化机器学习配置，在保持高可解释性的同时显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 机器学习配置优化存在关键瓶颈，传统方法缺乏可解释性且处理维度独立，自动化方法难以实现动态适应性和语义推理。

Method: 使用多智能体LLM框架，包含三个专门代理：提出配置变更的Advisor、评估进展的Evaluator、优化决策过程的Optimizer，通过文本梯度提供语义反馈。

Result: 在六个不同数据集上的综合评估显示，LGT相比传统优化方法实现了显著性能提升。

Conclusion: LGT框架通过语言引导的智能体协作和文本梯度反馈，有效解决了机器学习配置优化的关键挑战，在性能和可解释性方面都有显著优势。

Abstract: Configuration optimization remains a critical bottleneck in machine learning,
requiring coordinated tuning across model architecture, training strategy,
feature engineering, and hyperparameters. Traditional approaches treat these
dimensions independently and lack interpretability, while recent automated
methods struggle with dynamic adaptability and semantic reasoning about
optimization decisions. We introduce Language-Guided Tuning (LGT), a novel
framework that employs multi-agent Large Language Models to intelligently
optimize configurations through natural language reasoning. We apply textual
gradients - qualitative feedback signals that complement numerical optimization
by providing semantic understanding of training dynamics and configuration
interdependencies. LGT coordinates three specialized agents: an Advisor that
proposes configuration changes, an Evaluator that assesses progress, and an
Optimizer that refines the decision-making process, creating a self-improving
feedback loop. Through comprehensive evaluation on six diverse datasets, LGT
demonstrates substantial improvements over traditional optimization methods,
achieving performance gains while maintaining high interpretability.

</details>
