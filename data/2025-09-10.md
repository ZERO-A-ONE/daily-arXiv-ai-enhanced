<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 8]
- [cs.CR](#cs.CR) [Total: 21]
- [cs.AI](#cs.AI) [Total: 32]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Aspect-Oriented Programming in Secure Software Development: A Case Study of Security Aspects in Web Applications](https://arxiv.org/abs/2509.07449)
*Mterorga Ukor*

Main category: cs.SE

TL;DR: AOP在Web应用安全开发中比传统OOP能更好地模块化横切安全关注点，提高模块化、可重用性和可维护性，且性能开销极小


<details>
  <summary>Details</summary>
Motivation: 传统面向对象编程将安全逻辑与业务功能交织，导致代码纠缠、分散和可维护性降低，需要更好的安全开发方法

Method: 采用案例研究方法，比较基于AOP的安全功能实现（认证、授权、输入验证等）与传统OOP或中间件方法，分析代码质量指标、性能指标和可维护性指标

Result: AOP增强了安全机制的模块化、可重用性和可维护性，同时只引入了最小的性能开销

Conclusion: AOP为软件工程师和研究人员在Web应用开发中平衡安全性与软件质量提供了实用见解

Abstract: Security remains a critical challenge in modern web applications, where
threats such as unauthorized access, data breaches, and injection attacks
continue to undermine trust and reliability. Traditional Object-Oriented
Programming (OOP) often intertwines security logic with business functionality,
leading to code tangling, scattering, and reduced maintainability. This study
investigates the role of Aspect-Oriented Programming (AOP) in enhancing secure
software development by modularizing cross-cutting security concerns. Using a
case study approach, we compare AOP-based implementations of security features
including authentication, authorization, input validation, encryption, logging,
and session management with conventional OOP or middleware-based approaches.
Data collection involves analyzing code quality metrics (e.g., lines of code,
coupling, cohesion, modularity index, reusability), performance metrics
(response time, throughput, memory usage), and maintainability indicators.
Developer feedback is also incorporated to assess integration and debugging
experiences. Statistical methods, guided by the ISO/IEC 25010 software quality
model, are applied to evaluate differences across implementations. The findings
demonstrate that AOP enhances modularity, reusability, and maintainability of
security mechanisms, while introducing only minimal performance overhead. The
study contributes practical insights for software engineers and researchers
seeking to balance security with software quality in web application
development.

</details>


### [2] [CRACI: A Cloud-Native Reference Architecture for the Industrial Compute Continuum](https://arxiv.org/abs/2509.07498)
*Hai Dinh-Tuan*

Main category: cs.SE

TL;DR: CRACI是一种云原生工业计算连续体参考架构，旨在解决传统分层架构在工业4.0中的局限性，通过解耦和事件驱动模型实现灵活的数据流。


<details>
  <summary>Details</summary>
Motivation: 工业4.0中IT与OT的融合暴露了传统分层架构（如ISA-95和RAMI 4.0）的局限性，包括刚性结构、数据孤岛以及缺乏对云原生技术的支持，这些限制了可扩展和互操作工业系统的发展。

Method: 提出CRACI架构，采用解耦和事件驱动模型，嵌入信任、治理与策略、可观测性和生命周期管理等横切关注点作为基础支柱。通过理论对比分析和基于实际智能制造实施性能数据的定量评估进行验证。

Result: CRACI提供了一个可行的、最先进的架构，利用计算连续体克服传统模型的结构限制，实现可扩展的现代工业系统。

Conclusion: CRACI架构成功解决了传统工业架构的局限性，为工业4.0提供了云原生、可扩展的解决方案，通过理论和实践验证了其有效性。

Abstract: The convergence of Information Technology (IT) and Operational Technology
(OT) in Industry 4.0 exposes the limitations of traditional, hierarchical
architectures like ISA-95 and RAMI 4.0. Their inherent rigidity, data silos,
and lack of support for cloud-native technologies impair the development of
scalable and interoperable industrial systems. This paper addresses this issue
by introducing CRACI, a Cloud-native Reference Architecture for the Industrial
Compute Continuum. Among other features, CRACI promotes a decoupled and
event-driven model to enable flexible, non-hierarchical data flows across the
continuum. It embeds cross-cutting concerns as foundational pillars: Trust,
Governance & Policy, Observability, and Lifecycle Management, ensuring quality
attributes are core to the design. The proposed architecture is validated
through a two-fold approach: (1) a comparative theoretical analysis against
established standards, operational models, and academic proposals; and (2) a
quantitative evaluation based on performance data from previously published
real-world smart manufacturing implementations. The results demonstrate that
CRACI provides a viable, state-of-the-art architecture that utilizes the
compute continuum to overcome the structural limitations of legacy models and
enable scalable, modern industrial systems.

</details>


### [3] [PatchSeeker: Mapping NVD Records to their Vulnerability-fixing Commits with LLM Generated Commits and Embeddings](https://arxiv.org/abs/2509.07540)
*Huu Hung Nguyen,Anh Tuan Nguyen,Thanh Le-Cong,Yikun Li,Han Wei Ang,Yide Yin,Frank Liauw,Shar Lwin Khin,Ouh Eng Lieh,Ting Zhang,David Lo*

Main category: cs.SE

TL;DR: PatchSeeker是一个利用大语言模型自动将NVD漏洞描述与修复提交(VFCs)进行语义匹配的新方法，通过生成详细的提交消息摘要来弥补信息差距，在基准测试中比现有最佳方法MRR提高59.3%，Recall@10提高27.9%。


<details>
  <summary>Details</summary>
Motivation: NVD漏洞数据库缺乏与修复提交(VFCs)的明确链接，现有方法依赖稀疏且嘈杂的提交消息，无法捕捉漏洞描述的深层语义，需要更好的自动化映射方法。

Method: 利用大语言模型生成NVD描述的嵌入向量，并为简短或无信息的提交消息合成详细摘要，通过这些生成的摘要作为语义桥梁连接自然语言报告和代码变更。

Result: 在基准数据集上比最佳基线方法Prospector的MRR提高59.3%，Recall@10提高27.9%，在最新CVE上的扩展评估进一步证实了有效性。消融研究表明提交消息生成方法和骨干LLM选择都对性能有积极贡献。

Conclusion: PatchSeeker通过LLM生成的语义摘要有效解决了漏洞描述与修复提交之间的映射问题，但仍存在局限性，为未来工作提供了指导方向。

Abstract: Software vulnerabilities pose serious risks to modern software ecosystems.
While the National Vulnerability Database (NVD) is the authoritative source for
cataloging these vulnerabilities, it often lacks explicit links to the
corresponding Vulnerability-Fixing Commits (VFCs). VFCs encode precise code
changes, enabling vulnerability localization, patch analysis, and dataset
construction. Automatically mapping NVD records to their true VFCs is therefore
critical. Existing approaches have limitations as they rely on sparse, often
noisy commit messages and fail to capture the deep semantics in the
vulnerability descriptions. To address this gap, we introduce PatchSeeker, a
novel method that leverages large language models to create rich semantic links
between vulnerability descriptions and their VFCs. PatchSeeker generates
embeddings from NVD descriptions and enhances commit messages by synthesizing
detailed summaries for those that are short or uninformative. These generated
messages act as a semantic bridge, effectively closing the information gap
between natural language reports and low-level code changes. Our approach
PatchSeeker achieves 59.3% higher MRR and 27.9% higher Recall@10 than the
best-performing baseline, Prospector, on the benchmark dataset. The extended
evaluation on recent CVEs further confirms PatchSeeker's effectiveness.
Ablation study shows that both the commit message generation method and the
selection of backbone LLMs make a positive contribution to PatchSeeker. We also
discuss limitations and open challenges to guide future work.

</details>


### [4] [Bridging the Gap Between Binary and Source Based Package Management in Spack](https://arxiv.org/abs/2509.07728)
*John Gouwar,Gregory Becker,Tamara Dahlgren,Nathan Hanford,Arjun Guha,Todd Gamblin*

Main category: cs.SE

TL;DR: Spack包管理器通过splicing技术实现二进制包兼容性建模，允许混合使用源码和二进制分发，解决了HPC环境中ABI兼容性导致的重复编译问题。


<details>
  <summary>Details</summary>
Motivation: 二进制包管理器安装快速但配置灵活性受限，源码包管理器灵活但编译缓慢。HPC环境中安装新MPI实现等ABI敏感依赖时会导致大量重编译，现有Spack缺乏二进制兼容性模型无法混合使用不同二进制包。

Method: 提出splicing技术，扩展Spack的包语言和依赖解析引擎，建模包间二进制兼容性，在保持源码构建灵活性的同时重用兼容的二进制包。

Result: 实现了源码和二进制分发的无缝混合使用，安装时开销最小，即使对于MPI等ABI敏感依赖也能快速从二进制安装，避免大量重编译。

Conclusion: splicing技术成功解决了HPC包管理中二进制兼容性问题，在保持灵活性的同时显著提升了安装效率。

Abstract: Binary package managers install software quickly but they limit
configurability due to rigid ABI requirements that ensure compatibility between
binaries. Source package managers provide flexibility in building software, but
compilation can be slow. For example, installing an HPC code with a new MPI
implementation may result in a full rebuild. Spack, a widely deployed,
HPC-focused package manager, can use source and pre-compiled binaries, but
lacks a binary compatibility model, so it cannot mix binaries not built
together. We present splicing, an extension to Spack that models binary
compatibility between packages and allows seamless mixing of source and binary
distributions. Splicing augments Spack's packaging language and dependency
resolution engine to reuse compatible binaries but maintains the flexibility of
source builds. It incurs minimal installation-time overhead and allows rapid
installation from binaries, even for ABI-sensitive dependencies like MPI that
would otherwise require many rebuilds.

</details>


### [5] [What's Coming Next? Short-Term Simulation of Business Processes from Current State](https://arxiv.org/abs/2509.07747)
*Maksym Avramenko,David Chapela-Campa,Marlon Dumas,Fredrik Milani*

Main category: cs.SE

TL;DR: 本文提出了一种基于事件日志初始化模拟的方法，用于业务过程的短期性能预测和运营决策支持，相比传统长期模拟方法在准确性上有显著提升。


<details>
  <summary>Details</summary>
Motivation: 传统业务过程模拟方法主要支持战术决策，从空状态开始模拟长期效果，但无法有效支持需要基于当前进行中案例和资源状态的运营决策，如短期性能预测和临时中断影响分析。

Method: 研究从事件日志中推导当前状态的方法，开发模拟引擎，输入模拟模型和进行中案例日志，在给定时间范围内模拟案例，实现从当前状态开始的短期模拟。

Result: 实验评估表明，该方法在短期性能预测方面比带预热期的长期模拟更准确，特别是在存在概念漂移或突发性能模式的情况下。

Conclusion: 基于事件日志初始化模拟的方法为业务过程运营决策提供了有效的短期预测工具，能够更好地处理现实中的动态变化和突发情况。

Abstract: Business process simulation is an approach to evaluate business process
changes prior to implementation. Existing methods in this field primarily
support tactical decision-making, where simulations start from an empty state
and aim to estimate the long-term effects of process changes. A complementary
use-case is operational decision-making, where the goal is to forecast
short-term performance based on ongoing cases and to analyze the impact of
temporary disruptions, such as demand spikes and shortfalls in available
resources. An approach to tackle this use-case is to run a long-term simulation
up to a point where the workload is similar to the current one (warm-up), and
measure performance thereon. However, this approach does not consider the
current state of ongoing cases and resources in the process. This paper studies
an alternative approach that initializes the simulation from a representation
of the current state derived from an event log of ongoing cases. The paper
addresses two challenges in operationalizing this approach: (1) Given a
simulation model, what information is needed so that a simulation run can start
from the current state of cases and resources? (2) How can the current state of
a process be derived from an event log? The resulting short-term simulation
approach is embodied in a simulation engine that takes as input a simulation
model and a log of ongoing cases, and simulates cases for a given time horizon.
An experimental evaluation shows that this approach yields more accurate
short-term performance forecasts than long-term simulations with warm-up
period, particularly in the presence of concept drift or bursty performance
patterns.

</details>


### [6] [What Were You Thinking? An LLM-Driven Large-Scale Study of Refactoring Motivations in Open-Source Projects](https://arxiv.org/abs/2509.07763)
*Mikel Robredo,Matteo Esposito,Fabio Palomba,Rafael Peñaloza,Valentina Lenarduzzi*

Main category: cs.SE

TL;DR: LLMs能有效识别80%的代码重构动机，但与文献动机匹配率仅47%，主要关注可读性和可维护性等实用动机，但在架构推理方面存在局限。


<details>
  <summary>Details</summary>
Motivation: 理解开发者重构代码的原因及哪些指标能捕捉这些动机，以支持更广泛和有效的重构实践应用。

Method: 通过大规模实证研究分析开发者重构活动，利用大语言模型从版本控制数据中识别潜在动机，并与文献中报告的动机进行比较。

Result: LLMs在80%的情况下与人工判断一致，但仅与47%的文献动机匹配；丰富了22%的动机细节，主要关注可读性、清晰度和结构改进；大多数动机是实用性的，关注简化和可维护性。

Conclusion: LLMs能有效捕捉表面层动机但在架构推理方面有困难，其价值在于提供局部解释，与软件指标结合可形成混合方法，为更系统地优先处理重构提供有前景的路径。

Abstract: Context. Code refactoring improves software quality without changing external
behavior. Despite its advantages, its benefits are hindered by the considerable
cost of time, resources, and continuous effort it demands. Aim. Understanding
why developers refactor, and which metrics capture these motivations, may
support wider and more effective use of refactoring in practice. Method. We
performed a large-scale empirical study to analyze developers refactoring
activity, leveraging Large Language Models (LLMs) to identify underlying
motivations from version control data, comparing our findings with previous
motivations reported in the literature. Results. LLMs matched human judgment in
80% of cases, but aligned with literature-based motivations in only 47%. They
enriched 22% of motivations with more detailed rationale, often highlighting
readability, clarity, and structural improvements. Most motivations were
pragmatic, focused on simplification and maintainability. While metrics related
to developer experience and code readability ranked highest, their correlation
with motivation categories was weak. Conclusions. We conclude that LLMs
effectively capture surface-level motivations but struggle with architectural
reasoning. Their value lies in providing localized explanations, which, when
combined with software metrics, can form hybrid approaches. Such integration
offers a promising path toward prioritizing refactoring more systematically and
balancing short-term improvements with long-term architectural goals.

</details>


### [7] ["We provide our resources in a dedicated repository": Surveying the Transparency of HICSS publications](https://arxiv.org/abs/2509.07851)
*Irdin Pekaric,Giovanni Apruzzese*

Main category: cs.SE

TL;DR: 研究分析了HICSS论文中外部仓库的使用情况，发现仅3%论文有可用的公开仓库


<details>
  <summary>Details</summary>
Motivation: 研究进展需要数据、工具等辅助材料支持，外部仓库能提高研究透明度和可复现性

Method: 收集2017-2024年HICSS论文总数5579篇，识别包含人类研究(850)或技术实现(737)的论文，检查其中外部仓库链接的使用情况

Result: 在2028篇相关论文中，仅3%具有功能正常、公开可用且能被下游研究使用的仓库

Conclusion: 当前学术界对外部仓库的采用率极低，需要推进研究资源共享以提升研究透明度和可复现性

Abstract: Every day, new discoveries are made by researchers from all across the globe
and fields. HICSS is a flagship venue to present and discuss such scientific
advances. Yet, the activities carried out for any given research can hardly be
fully contained in a single document of a few pages-the "paper." Indeed, any
given study entails data, artifacts, or other material that is crucial to truly
appreciate the contributions claimed in the corresponding paper. External
repositories (e.g., GitHub) are a convenient tool to store all such resources
so that future work can freely observe and build upon them -- thereby improving
transparency and promoting reproducibility of research as a whole. In this
work, we scrutinize the extent to which papers recently accepted to HICSS
leverage such repositories to provide supplementary material. To this end, we
collect all the 5579 papers included in HICSS proceedings from 2017-2024. Then,
we identify those entailing either human subject research (850) or technical
implementations (737), or both (147). Finally, we review their text, examining
how many include a link to an external repository-and, inspect its contents.
Overall, out of 2028 papers, only 3\% have a functional and publicly available
repository that is usable by downstream research. We release all our tools.

</details>


### [8] [Breaking Android with AI: A Deep Dive into LLM-Powered Exploitation](https://arxiv.org/abs/2509.07933)
*Wanni Vidulige Ishan Perera,Xing Liu,Fan liang,Junyi Zhang*

Main category: cs.SE

TL;DR: 本研究探索使用LLM工具PentestGPT进行Android渗透测试自动化，比较传统手动root方法和AI生成的利用脚本，评估自动化渗透测试在Android设备提权中的效果。


<details>
  <summary>Details</summary>
Motivation: AI和LLM的快速发展为网络安全领域带来了新机遇，特别是在渗透测试自动化方面。研究旨在评估基于LLM的工具在Android设备rooting过程中的效能和可靠性。

Method: 使用Android模拟器(Genymotion)作为测试平台，分别执行传统手动root方法和AI生成的利用脚本；集成OpenAI API创建web应用来自动生成脚本；比较自动化与手动渗透测试协议的效果。

Result: 研究发现LLM能显著简化利用工作流程，但仍需要人工控制以确保准确性和伦理应用；识别了LLM在渗透测试中的优势和弱点。

Conclusion: AI驱动的网络安全工具虽然能提高效率，但必须结合人工监督来保证准确性和伦理合规性；研究为AI在道德黑客、安全研究和移动设备安全领域的影响提供了重要见解。

Abstract: The rapid evolution of Artificial Intelligence (AI) and Large Language Models
(LLMs) has opened up new opportunities in the area of cybersecurity, especially
in the exploitation automation landscape and penetration testing. This study
explores Android penetration testing automation using LLM-based tools,
especially PentestGPT, to identify and execute rooting techniques. Through a
comparison of the traditional manual rooting process and exploitation methods
produced using AI, this study evaluates the efficacy, reliability, and
scalability of automated penetration testing in achieving high-level privilege
access on Android devices. With the use of an Android emulator (Genymotion) as
the testbed, we fully execute both traditional and exploit-based rooting
methods, automating the process using AI-generated scripts. Secondly, we create
a web application by integrating OpenAI's API to facilitate automated script
generation from LLM-processed responses. The research focuses on the
effectiveness of AI-enabled exploitation by comparing automated and manual
penetration testing protocols, by determining LLM weaknesses and strengths
along the way. We also provide security suggestions of AI-enabled exploitation,
including ethical factors and potential misuse. The findings exhibit that while
LLMs can significantly streamline the workflow of exploitation, they need to be
controlled by humans to ensure accuracy and ethical application. This study
adds to the increasing body of literature on AI-powered cybersecurity and its
effect on ethical hacking, security research, and mobile device security.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [9] [Random Forest Stratified K-Fold Cross Validation on SYN DoS Attack SD-IoV](https://arxiv.org/abs/2509.07016)
*Muhammad Arif Hakimi Zamrai,Kamaludin Mohd Yusof*

Main category: cs.CR

TL;DR: 通过优化随机森林分类器检测SD-IoV网络中的TCP SYN流量攻击，实现了极高的准确性（0.999998）和极短检测时间（0.24秒）


<details>
  <summary>Details</summary>
Motivation: 解决软件定义车联网(SD-IoV)中TCP SYN流量攻击的网络安全挑战，提升车辆通信系统的安全性

Method: 预处理包含SYN攻击的数据集，采用特征缩放和标签编码技术，使用层化K折交叉验证，优化随机森林分类器模型（20个估计器，深度10）

Result: 平均指标达到0.999998，SYN DoS攻击检测时间仅0.24秒，能够高效区分正常和恶意流量

Conclusion: 该方法在检测SYN流量攻击方面实现了重大进展，给车辆网络安全提供了高准确性与高效率结合的解决方案

Abstract: In response to the prevalent concern of TCP SYN flood attacks within the
context of Software-Defined Internet of Vehicles (SD-IoV), this study addresses
the significant challenge of network security in rapidly evolving vehicular
communication systems. This research focuses on optimizing a Random Forest
Classifier model to achieve maximum accuracy and minimal detection time,
thereby enhancing vehicular network security. The methodology involves
preprocessing a dataset containing SYN attack instances, employing feature
scaling and label encoding techniques, and applying Stratified K-Fold
cross-validation to target key metrics such as accuracy, precision, recall, and
F1-score. This research achieved an average value of 0.999998 for all metrics
with a SYN DoS attack detection time of 0.24 seconds. Results show that the
fine-tuned Random Forest model, configured with 20 estimators and a depth of
10, effectively differentiates between normal and malicious traffic with high
accuracy and minimal detection time, which is crucial for SD-IoV networks. This
approach marks a significant advancement and introduces a state-of-the-art
algorithm in detecting SYN flood attacks, combining high accuracy with minimal
detection time. It contributes to vehicular network security by providing a
robust solution against TCP SYN flood attacks while maintaining network
efficiency and reliability.

</details>


### [10] [The Signalgate Case is Waiving a Red Flag to All Organizational and Behavioral Cybersecurity Leaders, Practitioners, and Researchers: Are We Receiving the Signal Amidst the Noise?](https://arxiv.org/abs/2509.07053)
*Paul Benjamin Lowry,Gregory D. Moody,Robert Willison,Clay Posey*

Main category: cs.CR

TL;DR: Signalgate事件暴露了组织安全中人因错误、治理缺失和技术滥用的关键漏洞，强调需要从技术防御转向关注治理、文化和行为风险


<details>
  <summary>Details</summary>
Motivation: 分析2025年Signalgate事件，揭示组织安全中人本漏洞和治理挑战，这些往往被外部网络威胁的关注所掩盖

Method: 采用案例研究和基于NIST网络安全框架的系统性回顾方法，分析事件中的人本漏洞模式和治理挑战

Result: 发现三个关键点：1)组织安全严重依赖人类行为；2)领导层态度强烈影响安全文化；3)过度依赖技术解决方案而忽视人因投入导致无效实践

Conclusion: 提出可操作建议：强化领导参与、全面采用零信任架构、明确责任结构、激励安全行为、严格监督，特别是在组织转型期需要额外措施

Abstract: The Signalgate incident of March 2025, wherein senior US national security
officials inadvertently disclosed sensitive military operational details via
the encrypted messaging platform Signal, highlights critical vulnerabilities in
organizational security arising from human error, governance gaps, and the
misuse of technology. Although smaller in scale when compared to historical
breaches involving billions of records, Signalgate illustrates critical
systemic issues often overshadowed by a focus on external cyber threats.
Employing a case-study approach and systematic review grounded in the NIST
Cybersecurity Framework, we analyze the incident to identify patterns of
human-centric vulnerabilities and governance challenges common to
organizational security failures. Findings emphasize three critical points. (1)
Organizational security depends heavily on human behavior, with internal actors
often serving as the weakest link despite advanced technical defenses; (2)
Leadership tone strongly influences organizational security culture and
efficacy, and (3) widespread reliance on technical solutions without sufficient
investments in human and organizational factors leads to ineffective practices
and wasted resources. From these observations, we propose actionable
recommendations for enhancing organizational and national security, including
strong leadership engagement, comprehensive adoption of zero-trust
architectures, clearer accountability structures, incentivized security
behaviors, and rigorous oversight. Particularly during periods of
organizational transition, such as mergers or large-scale personnel changes,
additional measures become particularly important. Signalgate underscores the
need for leaders and policymakers to reorient cybersecurity strategies toward
addressing governance, cultural, and behavioral risks.

</details>


### [11] [Sequentially Auditing Differential Privacy](https://arxiv.org/abs/2509.07055)
*Tomás González,Mateo Dulce-Rubio,Aaditya Ramdas,Mónica Ribero*

Main category: cs.CR

TL;DR: 序列化巡计测试方法，可在任何时间点进行合法推断，大幅减少检测差分隐私违约所需样本量，从数万降至数百个示例。


<details>
  <summary>Details</summary>
Motivation: 克服以往批量巡计方法的固定样本量限制，开发一种能够处理流式输出、提供任意时间有效推断的序列化测试方法。

Method: 实用的序列化测试方法，用于审计黑盒机制的差分隐私保证，能够在控制I类错误的前提下处理机制输出流。

Result: 实验结果显示，该测试方法检测违约所需的样本量比现有方法减少了数个数量级，从50,000个降至数百个示例。特别是能在不到一次训练过程中识别DP-SGD隐私违约。

Conclusion: 该序列化测试方法为差分隐私审计提供了更高效、实时的解决方案，显著提升了检测效率和实用性。

Abstract: We propose a practical sequential test for auditing differential privacy
guarantees of black-box mechanisms. The test processes streams of mechanisms'
outputs providing anytime-valid inference while controlling Type I error,
overcoming the fixed sample size limitation of previous batch auditing methods.
Experiments show this test detects violations with sample sizes that are orders
of magnitude smaller than existing methods, reducing this number from 50K to a
few hundred examples, across diverse realistic mechanisms. Notably, it
identifies DP-SGD privacy violations in \textit{under} one training run, unlike
prior methods needing full model training.

</details>


### [12] [SoK: Security and Privacy of AI Agents for Blockchain](https://arxiv.org/abs/2509.07131)
*Nicolò Romandini,Carlo Mazzocca,Kai Otsuki,Rebecca Montanari*

Main category: cs.CR

TL;DR: 本文是关于AI智能体在区块链领域应用的首次系统性综述，特别关注安全与隐私维度，填补了现有文献的空白


<details>
  <summary>Details</summary>
Motivation: 区块链和智能合约的复杂性对非专业用户构成障碍，AI智能体作为交互工具具有重要价值，但现有文献缺乏针对AI智能体与区块链交叉领域的全面调研

Method: 采用系统化知识整理方法，对AI驱动的区块链系统进行全面综述分析

Result: 系统梳理了AI智能体在区块链领域的应用场景（包括链上数据分析、交易策略优化、智能合约漏洞检测等），并深入分析了安全隐私维度的现状

Conclusion: 该研究为AI智能体在区块链领域的应用提供了系统性框架，揭示了当前局限性和未来研究方向，对推动Web3生态发展具有重要意义

Abstract: Blockchain and smart contracts have garnered significant interest in recent
years as the foundation of a decentralized, trustless digital ecosystem,
thereby eliminating the need for traditional centralized authorities. Despite
their central role in powering Web3, their complexity still presents
significant barriers for non-expert users. To bridge this gap, Artificial
Intelligence (AI)-based agents have emerged as valuable tools for interacting
with blockchain environments, supporting a range of tasks, from analyzing
on-chain data and optimizing transaction strategies to detecting
vulnerabilities within smart contracts. While interest in applying AI to
blockchain is growing, the literature still lacks a comprehensive survey that
focuses specifically on the intersection with AI agents. Most of the related
work only provides general considerations, without focusing on any specific
domain. This paper addresses this gap by presenting the first Systematization
of Knowledge dedicated to AI-driven systems for blockchain, with a special
focus on their security and privacy dimensions, shedding light on their
applications, limitations, and future research directions.

</details>


### [13] [All You Need Is A Fuzzing Brain: An LLM-Powered System for Automated Vulnerability Detection and Patching](https://arxiv.org/abs/2509.07225)
*Ze Sheng,Qingxiao Xu,Jianwei Huang,Matthew Woodcock,Heqing Huang,Alastair F. Donaldson,Guofei Gu,Jeff Huang*

Main category: cs.CR

TL;DR: 团队在DARPA AIxCC竞赛中开发了基于LLM的网络安全推理系统，发现了28个安全漏洞（包括6个零日漏洞）并成功修补了14个，同时建立了漏洞检测和修补的公开排行榜。


<details>
  <summary>Details</summary>
Motivation: 参与DARPA人工智能网络挑战赛，开发能够自主发现和修补安全漏洞的网络安全推理系统，推动AI在网络安全领域的应用。

Method: 构建基于大语言模型（LLM）的网络安全推理系统，结合模糊测试技术，对真实世界的开源C和Java项目进行漏洞检测和修补。

Result: 系统成功发现了28个安全漏洞（含6个零日漏洞），成功修补了14个漏洞，在竞赛中获得第四名，并开源了完整系统。

Conclusion: 基于LLM的网络安全推理系统在漏洞发现和修补方面表现出色，建立的公开排行榜为评估LLM在网络安全任务上的性能提供了基准平台。

Abstract: Our team, All You Need Is A Fuzzing Brain, was one of seven finalists in
DARPA's Artificial Intelligence Cyber Challenge (AIxCC), placing fourth in the
final round. During the competition, we developed a Cyber Reasoning System
(CRS) that autonomously discovered 28 security vulnerabilities - including six
previously unknown zero-days - in real-world open-source C and Java projects,
and successfully patched 14 of them. The complete CRS is open source at
https://github.com/o2lab/afc-crs-all-you-need-is-a-fuzzing-brain. This paper
provides a detailed technical description of our CRS, with an emphasis on its
LLM-powered components and strategies. Building on AIxCC, we further introduce
a public leaderboard for benchmarking state-of-the-art LLMs on vulnerability
detection and patching tasks, derived from the AIxCC dataset. The leaderboard
is available at https://o2lab.github.io/FuzzingBrain-Leaderboard/.

</details>


### [14] [SafeToolBench: Pioneering a Prospective Benchmark to Evaluating Tool Utilization Safety in LLMs](https://arxiv.org/abs/2509.07315)
*Hongfei Xia,Hongru Wang,Zeming Liu,Qian Yu,Yuhang Guo,Haifeng Wang*

Main category: cs.CR

TL;DR: SafeToolBench是首个前瞻性评估LLM工具使用安全性的基准，通过SafeInstructTool框架从用户指令、工具本身和联合指令-工具三个维度提升LLM的安全意识。


<details>
  <summary>Details</summary>
Motivation: 外部工具虽然增强了LLM的问题解决能力，但也带来了财务损失和隐私泄露等风险。现有研究主要在工具执行后进行安全评估，无法避免不可逆的损害。

Method: 提出SafeToolBench基准和SafeInstructTool框架，从用户指令、工具本身和联合指令-工具三个视角（共9个维度）评估和增强LLM的工具使用安全性。

Result: 实验发现现有方法无法捕捉所有工具使用风险，而提出的框架显著提升了LLM的自我安全意识，实现了更安全可靠的工具使用。

Conclusion: 该研究提供了前瞻性的工具安全评估方法，通过多维度框架有效增强了LLM在工具使用过程中的安全意识和风险防范能力。

Abstract: Large Language Models (LLMs) have exhibited great performance in autonomously
calling various tools in external environments, leading to better problem
solving and task automation capabilities. However, these external tools also
amplify potential risks such as financial loss or privacy leakage with
ambiguous or malicious user instructions. Compared to previous studies, which
mainly assess the safety awareness of LLMs after obtaining the tool execution
results (i.e., retrospective evaluation), this paper focuses on prospective
ways to assess the safety of LLM tool utilization, aiming to avoid irreversible
harm caused by directly executing tools. To this end, we propose SafeToolBench,
the first benchmark to comprehensively assess tool utilization security in a
prospective manner, covering malicious user instructions and diverse practical
toolsets. Additionally, we propose a novel framework, SafeInstructTool, which
aims to enhance LLMs' awareness of tool utilization security from three
perspectives (i.e., \textit{User Instruction, Tool Itself, and Joint
Instruction-Tool}), leading to nine detailed dimensions in total. We experiment
with four LLMs using different methods, revealing that existing approaches fail
to capture all risks in tool utilization. In contrast, our framework
significantly enhances LLMs' self-awareness, enabling a more safe and
trustworthy tool utilization.

</details>


### [15] [Paladin: Defending LLM-enabled Phishing Emails with a New Trigger-Tag Paradigm](https://arxiv.org/abs/2509.07287)
*Yan Pang,Wenlong Meng,Xiaojing Liao,Tianhao Wang*

Main category: cs.CR

TL;DR: 提出了Paladin方法，通过在普通LLM中嵌入触发-标签关联机制，使模型在生成钓鱼内容时自动插入可检测标签，从而解决LLM生成钓鱼邮件检测难题


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的快速发展，恶意用户利用LLM生成无拼写错误、针对特定领域的钓鱼内容，现有语义级检测方法难以可靠识别，而基于LLM的检测方法计算成本高且受限于底层模型性能

Method: Paladin方法通过多种插入策略将触发-标签关联嵌入到普通LLM中，创建工具化LLM。当生成钓鱼相关内容时自动包含可检测标签，基于隐式和显式触发与标签设计了四种不同场景

Result: 实验结果表明该方法在隐蔽性、有效性和鲁棒性三个方面均优于基线方法，在所有场景下检测准确率超过90%

Conclusion: Paladin方法通过嵌入触发-标签机制有效解决了LLM生成钓鱼内容的检测问题，具有高准确性和实用性

Abstract: With the rapid development of large language models, the potential threat of
their malicious use, particularly in generating phishing content, is becoming
increasingly prevalent. Leveraging the capabilities of LLMs, malicious users
can synthesize phishing emails that are free from spelling mistakes and other
easily detectable features. Furthermore, such models can generate
topic-specific phishing messages, tailoring content to the target domain and
increasing the likelihood of success.
  Detecting such content remains a significant challenge, as LLM-generated
phishing emails often lack clear or distinguishable linguistic features. As a
result, most existing semantic-level detection approaches struggle to identify
them reliably. While certain LLM-based detection methods have shown promise,
they suffer from high computational costs and are constrained by the
performance of the underlying language model, making them impractical for
large-scale deployment.
  In this work, we aim to address this issue. We propose Paladin, which embeds
trigger-tag associations into vanilla LLM using various insertion strategies,
creating them into instrumented LLMs. When an instrumented LLM generates
content related to phishing, it will automatically include detectable tags,
enabling easier identification. Based on the design on implicit and explicit
triggers and tags, we consider four distinct scenarios in our work. We evaluate
our method from three key perspectives: stealthiness, effectiveness, and
robustness, and compare it with existing baseline methods. Experimental results
show that our method outperforms the baselines, achieving over 90% detection
accuracy across all scenarios.

</details>


### [16] [zkUnlearner: A Zero-Knowledge Framework for Verifiable Unlearning with Multi-Granularity and Forgery-Resistance](https://arxiv.org/abs/2509.07290)
*Nan Wang,Nan Wu,Xiangyu Hui,Jiafan Wang,Xin Yuan*

Main category: cs.CR

TL;DR: zkUnlearner是首个支持多粒度和防伪造的可验证机器学习遗忘零知识框架，通过位掩码技术实现样本级、特征级和类别级遗忘，并能抵抗梯度伪造攻击。


<details>
  <summary>Details</summary>
Motivation: 随着'被遗忘权'需求的增长，需要可验证的机器学习遗忘机制来确保透明度和问责制，现有方法在效率和隐私方面存在局限且易受伪造攻击。

Method: 提出基于位掩码技术的通用计算模型，支持选择性零知识训练证明，可转换为算术电路兼容多种零知识证明系统，并设计了抵抗最先进伪造攻击的有效策略。

Result: 实现了基于zkSNARK的框架实例化，并通过全面性能评估验证了其实用性，能够有效支持多粒度遗忘并抵抗伪造攻击。

Conclusion: zkUnlearner为可验证机器学习遗忘提供了首个零知识解决方案，解决了多粒度支持和防伪造的关键挑战，具有实际应用价值。

Abstract: As the demand for exercising the "right to be forgotten" grows, the need for
verifiable machine unlearning has become increasingly evident to ensure both
transparency and accountability. We present {\em zkUnlearner}, the first
zero-knowledge framework for verifiable machine unlearning, specifically
designed to support {\em multi-granularity} and {\em forgery-resistance}.
  First, we propose a general computational model that employs a {\em
bit-masking} technique to enable the {\em selectivity} of existing
zero-knowledge proofs of training for gradient descent algorithms. This
innovation enables not only traditional {\em sample-level} unlearning but also
more advanced {\em feature-level} and {\em class-level} unlearning. Our model
can be translated to arithmetic circuits, ensuring compatibility with a broad
range of zero-knowledge proof systems. Furthermore, our approach overcomes key
limitations of existing methods in both efficiency and privacy. Second, forging
attacks present a serious threat to the reliability of unlearning.
Specifically, in Stochastic Gradient Descent optimization, gradients from
unlearned data, or from minibatches containing it, can be forged using
alternative data samples or minibatches that exclude it. We propose the first
effective strategies to resist state-of-the-art forging attacks. Finally, we
benchmark a zkSNARK-based instantiation of our framework and perform
comprehensive performance evaluations to validate its practicality.

</details>


### [17] [A Decade-long Landscape of Advanced Persistent Threats: Longitudinal Analysis and Global Trends](https://arxiv.org/abs/2509.07457)
*Shakhzod Yuldoshkhujaev,Mijin Jeon,Doowon Kim,Nick Nikiforakis,Hyungjoon Koo*

Main category: cs.CR

TL;DR: 这篇论文通过系统分析1509份APT报告，揭示了403个唯一APT组织的全球趋势，发现恶意文档和邸小鱼邮件是主要攻击手段，且零日漏洞利用自2016年以来显著减少


<details>
  <summary>Details</summary>
Motivation: 虽然存在大量公开的APT攻击文档，但之前研究多关注单个案例的具体方面，缺乏对散落文档进行纵向分析的宏观视角

Method: 采用混合方法，结合规则基础的信息检索和大语言模型搜索技术，系统分析了6个可靠来源的1509份APT报告（24215页）

Result: 分析显示过去10年154个国家受影响，恶意文档和邸小鱼邮件是主要侵入手段，2016年以来零日漏洞利用显著减少，还发现了网络攻击与选举、战争等重大事件的关联

Conclusion: 研究提供了对APT攻击历史模式的深入见解，并通过交互式可视化工具展示全球APT活动模式和趋势，为网络安全领域提供了价值

Abstract: An advanced persistent threat (APT) refers to a covert, long-term
cyberattack, typically conducted by state-sponsored actors, targeting critical
sectors and often remaining undetected for long periods. In response,
collective intelligence from around the globe collaborates to identify and
trace surreptitious activities, generating substantial documentation on APT
campaigns publicly available on the web. While prior works predominantly focus
on specific aspects of APT cases, such as detection, evaluation, cyber threat
intelligence, and dataset creation, limited attention has been devoted to
revisiting and investigating these scattered dossiers in a longitudinal manner.
The objective of our study is to fill the gap by offering a macro perspective,
connecting key insights and global trends in past APT attacks. We
systematically analyze six reliable sources-three focused on technical reports
and another three on threat actors-examining 1,509 APT dossiers (24,215 pages)
spanning 2014-2023, and identifying 603 unique APT groups worldwide. To
efficiently unearth relevant information, we employ a hybrid methodology that
combines rule-based information retrieval with large-language-model-based
search techniques. Our longitudinal analysis reveals shifts in threat actor
activities, global attack vectors, changes in targeted sectors, and
relationships between cyberattacks and significant events such as elections or
wars, which provide insights into historical patterns in APT evolution. Over
the past decade, 154 countries have been affected, primarily using malicious
documents and spear phishing as dominant initial infiltration vectors, with a
noticeable decline in zero-day exploitation since 2016. Furthermore, we present
our findings through interactive visualization tools, such as an APT map or
flow diagram, to facilitate intuitive understanding of global patterns and
trends in APT activities.

</details>


### [18] [Biometric Bound Credentials for Age Verification](https://arxiv.org/abs/2509.07465)
*Norman Poh,Daryl Burns*

Main category: cs.CR

TL;DR: 提出Biometric Bound Credentials (BBCreds)作为隐私保护的年龄验证方案，通过密码学将年龄凭证与生物特征绑定，不存储生物模板，确保只有合法用户在物理在场时才能访问年龄限制服务。


<details>
  <summary>Details</summary>
Motivation: 年龄验证对于监管合规、用户信任和未成年人保护日益重要。传统解决方案存在准确性差、侵入性强和安全风险高等问题，当前关注点转向隐私保护、公平性和透明可信系统。

Method: 使用密码学技术将年龄凭证与个人生物特征绑定，但不存储生物模板本身，确保只有物理在场的合法用户能够使用凭证。

Result: 该方法能够防止凭证共享，解决传统和新兴的年龄验证挑战，同时增强隐私保护。

Conclusion: BBCreds提供了一种隐私保护的年龄验证方法，通过生物特征绑定确保服务访问的安全性和合法性，同时解决了隐私和公平性等关键问题。

Abstract: Age verification is increasingly critical for regulatory compliance, user
trust, and the protection of minors online. Historically, solutions have
struggled with poor accuracy, intrusiveness, and significant security risks.
More recently, concerns have shifted toward privacy, surveillance, fairness,
and the need for transparent, trustworthy systems. In this paper, we propose
Biometric Bound Credentials (BBCreds) as a privacy-preserving approach that
cryptographically binds age credentials to an individual's biometric features
without storing biometric templates. This ensures only the legitimate,
physically present user can access age-restricted services, prevents credential
sharing, and addresses both legacy and emerging challenges in age verification.
enhances privacy.

</details>


### [19] [Backdoor Attacks and Defenses in Computer Vision Domain: A Survey](https://arxiv.org/abs/2509.07504)
*Bilal Hussain Abbasi,Yanjun Zhang,Leo Zhang,Shang Gao*

Main category: cs.CR

TL;DR: 这篇综述论文系统回顾了计算机视觉领域的后门攻击与防御技术，提出了多维分类法，总结了代表性方法、评估实践和防御效果，并指出了当前研究空白和未来方向。


<details>
  <summary>Details</summary>
Motivation: 随着后门攻击在机器学习模型中的威胁日益增长，需要系统梳理计算机视觉领域的攻击防御技术，为研究者和从业者提供全面的威胁态势认知和实践指导。

Method: 采用多维分类法组织攻击和防御技术，包括注入阶段、触发器类型、标签策略、表示阶段和目标任务等维度，对每个维度总结代表性方法并分析防御效果。

Result: 研究发现传统净化工具对可重用补丁攻击有效，但难以应对输入感知、样本特定或参数空间后门攻击，以及通过预训练编码器或硬件位翻转的转移攻击。

Conclusion: 论文为研究者和从业者提供了当前威胁态势的全面认知，指出了供应链和硬件威胁、可验证防御、跨任务基准等研究空白，并提出了威胁感知评估和分层防御的实践指南。

Abstract: Backdoor (trojan) attacks embed hidden, controllable behaviors into
machine-learning models so that models behave normally on benign inputs but
produce attacker-chosen outputs when a trigger is present. This survey reviews
the rapidly growing literature on backdoor attacks and defenses in the
computer-vision domain. We introduce a multi-dimensional taxonomy that
organizes attacks and defenses by injection stage (dataset poisoning,
model/parameter modification, inference-time injection), trigger type (patch,
blended/frequency, semantic, transformation), labeling strategy (dirty-label
vs. clean-label / feature-collision), representation stage (instance-specific,
manifold/class-level, neuron/parameter hijacking, distributed encodings), and
target task (classification, detection, segmentation, video, multimodal). For
each axis we summarize representative methods, highlight evaluation practices,
and discuss where defenses succeed or fail. For example, many classical
sanitization and reverse-engineering tools are effective against reusable patch
attacks but struggle with input-aware, sample-specific, or parameter-space
backdoors and with transfer via compromised pre-trained encoders or hardware
bit-flips. We synthesize trends, identify persistent gaps (supply-chain and
hardware threats, certifiable defenses, cross-task benchmarks), and propose
practical guidelines for threat-aware evaluation and layered defenses. This
survey aims to orient researchers and practitioners to the current threat
landscape and pressing research directions in secure computer vision.

</details>


### [20] [Extension of Spatial k-Anonymity: New Metrics for Assessing the Anonymity of Geomasked Data Considering Realistic Attack Scenarios](https://arxiv.org/abs/2509.07505)
*Simon Cremer,Lydia Jehmlich,Rainer Lenz*

Main category: cs.CR

TL;DR: 这篇论文分析了空间数据医疗研究中的数据保护问题，指出当前的空间k-匿名性指标存在缺陷，并提出了更适用于真实攻击场景的匿名性评估指标。


<details>
  <summary>Details</summary>
Motivation: 空间健康数据在医学研究中越来越重要，但因为包含个人地理信息，存在重识别风险，需要通过地理掩码技术来保护数据。当前的匿名性评估指标对真实攻击场景的适用性不足。

Method: 论文分类了匿名化地理微观数据的潜在数据攻击场景，并介绍了适合的指标来进行全面的匿名性评估。

Result: 提出了一套更加适用于真实数据攻击场景的匿名性评估指标体系，能够更全面地评估地理掩码数据的保护效果。

Conclusion: 需要发展更加精细和实用的匿名性评估方法，以应对地理数据保护中的真实风险场景，同时确保数据的分析有效性。

Abstract: Spatial data are gaining increasing importance in many areas of research.
Particularly spatial health data are becoming increasingly important for
medical research, for example, to better understand relationships between
environmental factors and disease patterns. However, their use is often
restricted by legal data protection regulations, since georeferenced personal
information carries a high risk of re-identification of individuals. To address
this issue, what are called geomasking methods are applied to guarantee data
protection through targeted displacement of individual data points, while
simultaneously maintaining analytical validity within a tolerable range. In the
current literature the degree of anonymity of such anonymized georeferenced
datasets is often measured by the so-called metric of spatial k-anonymity.
However, this metric has considerable shortcomings, particularly regarding its
resilience against realistic data attack scenarios. This article classifies the
potential data attack scenarios in the context of anonymized georeferenced
microdata and introduces appropriate metrics that enable a comprehensive
assessment of anonymity adapted to potential data attack scenarios.

</details>


### [21] [Enhanced cast-128 with adaptive s-box optimization via neural networks for image protection](https://arxiv.org/abs/2509.07606)
*Fadhil Abbas Fadhil,Maryam Mahdi Alhusseini,Mohammad-Reza Feizi-Derakhshi*

Main category: cs.CR

TL;DR: 改进的CAST-128加密算法，通过Logistic正弦映射混涊系统生成动态S箱，提高图像加密的安全性和效率


<details>
  <summary>Details</summary>
Motivation: 解决传统动态S箱模型容易受到线性和差分攻击的问题，满足对高效智能图像加密机制的日益增长需求

Method: 使用Logistic正弦映射(LSM)混涊系统生成动态、非线性、可逆的S箱，并嵌入CAST-128结构进行块状图像加密

Result: 通过图像困、相关性、信息熵等指标评估，显示随机性、统计攻击抵御能力和加密质量显著提升

Conclusion: 该方法提供了一种轻量级的混涊驱动S箱生成方案，可在不使用机器学习的情况下提高图像加密的强壮性，适用于安全通信、监控系统和医学图像保护

Abstract: An improved CAST-128 encryption algorithm, which is done by implementing
chaos-based adaptive S-box generation using Logistic sine Map (LSM), has been
provided in this paper because of the increasing requirements of efficient and
smart image encryption mechanisms. The study aims to address the drawbacks of
static S-box models commonly used in traditional cryptographic systems, which
are susceptible to linear and differential attacks. In the proposed scheme, the
dynamic, non-linear, invertible, and highly cryptographic strength S-boxes are
generated through a hybrid chaotic system that may have high non-linearity,
strong and rigorous avalanche characteristics, and low differential uniformity.
The process here is that the LSM is used to produce S-boxes having
key-dependent parameters that are stuffed into the CAST-128 structure to
encrypt the image in a block-wise manner. The performance of the encryption is
assessed utilizing a set of standard grayscale images. The metrics that are
used to evaluate the security are entropy, NPCR, UACI, PSNR, and histogram
analysis. Outcomes indicate that randomness, resistance to statistical attacks,
and country of encryption are significantly improved compared to the original
CAST-128. The study is theoretically and practically relevant since it presents
a lightweight S-box generation approach driven by chaos, which can increase the
level of robustness of the image encryptions without enlisting machine
learning. The system may be applied to secure communications, surveillance
systems, and medical image protection on a real-time basis.

</details>


### [22] [FlexEmu: Towards Flexible MCU Peripheral Emulation (Extended Version)](https://arxiv.org/abs/2509.07615)
*Chongqing Lei,Zhen Ling,Xiangyu Xu,Shaofeng Li,Guangchi Liu,Kai Dong,Junzhou Luo*

Main category: cs.CR

TL;DR: FlexEmu是一个灵活的MCU外设仿真框架，通过结构级原语抽象和语义级统一模型来自动生成仿真器，解决了MCU固件动态安全分析的执行环境难题。


<details>
  <summary>Details</summary>
Motivation: MCU固件安全分析面临执行环境挑战——物理设备资源不足，而构建仿真器成本高昂，因为存在大量异构硬件特别是外设。

Method: 提出两层次建模方法：结构级使用有限原语抽象外设硬件实现，语义级使用统一模型描述同类外设功能。基于此构建FlexEmu框架，自动提取外设细节并生成仿真器。

Result: 成功建模12种MCU外设，在15个不同MCU平台的90个固件样本上评估，自动生成的仿真器能忠实复制硬件行为，单元测试通过率达98.48%，优于现有方法。通过模糊测试发现3个流行RTOS中的10个未知漏洞。

Conclusion: FlexEmu框架有效解决了MCU外设仿真的挑战，为MCU固件安全分析提供了可行的动态执行环境，在安全研究方面具有重要应用价值。

Abstract: Microcontroller units (MCUs) are widely used in embedded devices due to their
low power consumption and cost-effectiveness. MCU firmware controls these
devices and is vital to the security of embedded systems. However, performing
dynamic security analyses for MCU firmware has remained challenging due to the
lack of usable execution environments -- existing dynamic analyses cannot run
on physical devices (e.g., insufficient computational resources), while
building emulators is costly due to the massive amount of heterogeneous
hardware, especially peripherals.
  Our work is based on the insight that MCU peripherals can be modeled in a
two-fold manner. At the structural level, peripherals have diverse
implementations but we can use a limited set of primitives to abstract
peripherals because their hardware implementations are based on common hardware
concepts. At the semantic level, peripherals have diverse functionalities.
However, we can use a single unified semantic model to describe the same kind
of peripherals because they exhibit similar functionalities. Building on this,
we propose FlexEmu, a flexible MCU peripheral emulation framework. Once
semantic models are created, FlexEmu automatically extracts peripheral-specific
details to instantiate models and generate emulators accordingly. We have
successfully applied FlexEmu to model 12 kinds of MCU peripherals. Our
evaluation on 90 firmware samples across 15 different MCU platforms shows that
the automatically generated emulators can faithfully replicate hardware
behaviors and achieve a 98.48% unit test passing rate, outperforming
state-of-the-art approaches. To demonstrate the implications of FlexEmu on
firmware security, we use the generated emulators to fuzz three popular RTOSes
and uncover 10 previously unknown bugs.

</details>


### [23] [Embedded Off-Switches for AI Compute](https://arxiv.org/abs/2509.07637)
*James Petrie*

Main category: cs.CR

TL;DR: 硬件级别关机制，通过在AI加速器中嵌入千个独立安全块来防止非授权使用，使用公钥加密和随机数验证授权许可证


<details>
  <summary>Details</summary>
Motivation: 应对越来越强大的AI系统带来的风险，防止危险欠使用

Method: 设计硬件级别关机制，在每个AI加速器中嵌入千个独立的安全块，使用公钥加密检查授权许可证的真实性，通过随机生成的nonces防止重放攻击

Result: 构建了可以防御精巧物理攻击的大规模冗余架构，安全块可以使用标准电路组件构建，与现有半导体制造过程兼容

Conclusion: 嵌入式安全块可使下一代AI加速器更稳固地防御危险欠使用

Abstract: To address the risks of increasingly capable AI systems, we introduce a
hardware-level off-switch that embeds thousands of independent "security
blocks" in each AI accelerator. This massively redundant architecture is
designed to prevent unauthorized chip use, even against sophisticated physical
attacks. Our main security block design uses public key cryptography to check
the authenticity of authorization licenses, and randomly generated nonces to
prevent replay attacks. We evaluate attack vectors and present additional
security block variants that could be added for greater robustness. Security
blocks can be built with standard circuit components, ensuring compatibility
with existing semiconductor manufacturing processes. With embedded security
blocks, the next generation of AI accelerators could be more robustly defended
against dangerous misuse.

</details>


### [24] [Leveraging Digital Twin-as-a-Service Towards Continuous and Automated Cybersecurity Certification](https://arxiv.org/abs/2509.07649)
*Ioannis Koufos,Abdul Rehman Qureshi,Adrian Asensio,Allen Abishek,Efstathios Zaragkas,Ricard Vilalta,Maria Souvalioti,George Xilouris,Michael-Alexandros Kourtis*

Main category: cs.CR

TL;DR: 基于数字双胞技术的安全合规服务，提供自动化、非侵入式的实时安全评估


<details>
  <summary>Details</summary>
Motivation: 解决传统手工审计和系统扫描导致的运营中断和安全漏洞问题

Method: 通过数字双胞技术映射实际资产，收集合规工件，创建机器可读证据，支持CycloneDX和Web of Things标准

Result: 中等规模基础设施的实验结果证明了其可行性和性能

Conclusion: 为高效、按需的网络安全治理提供了最小运营影响的解决方案

Abstract: Traditional risk assessments rely on manual audits and system scans, often
causing operational disruptions and leaving security gaps. To address these
challenges, this work presents Security Digital Twin-as-a-Service (SDT-aaS), a
novel approach that leverages Digital Twin (DT) technology for automated,
non-intrusive security compliance. SDT-aaS enables real-time security
assessments by mirroring real-world assets, collecting compliance artifacts,
and creating machine-readable evidence. The proposed work is a scalable and
interoperable solution that supports open standards like CycloneDX and Web of
Things (WoT), facilitating seamless integration and efficient compliance
management. Empirical results from a moderate-scale infrastructure use case
demonstrate its feasibility and performance, paving the way for efficient,
on-demand cybersecurity governance with minimal operational impact.

</details>


### [25] [Empirical Security Analysis of Software-based Fault Isolation through Controlled Fault Injection](https://arxiv.org/abs/2509.07757)
*Nils Bars,Lukas Bernhard,Moritz Schloegel,Thorsten Holz*

Main category: cs.CR

TL;DR: 这篇论文提出了一种新的测试技术，用于测试V8 JavaScript引擎堆沙箱的安全性，发现了19个可能绕过沙箱保护的安全漏洞。


<details>
  <summary>Details</summary>
Motivation: 现代JavaScript引擎复杂性高，构成重要安全边界。V8堆沙箱作为广泛部署的软件隔离机制，但缺乏充分的安全测试。需要一种有效方法来验证其安全性。

Method: 建立了模拟SFI安全边界的测试模型，假设攻击者完全控制沙箱内存。通过在信任域内存读取时注入故障，尝试在信任域触发内存损坏。

Result: 在全面评估中，识别出V8中的19个安全漏洞，这些漏洞允许攻击者绕过堆沙箱保护。

Conclusion: 该测试技术有效地曝露了现代SFI实现中的安全漏洞，显示了对这种广泛使用的安全机制进行深入安全验证的重要性。

Abstract: We use browsers daily to access all sorts of information. Because browsers
routinely process scripts, media, and executable code from unknown sources,
they form a critical security boundary between users and adversaries. A common
attack vector is JavaScript, which exposes a large attack surface due to the
sheer complexity of modern JavaScript engines. To mitigate these threats,
modern engines increasingly adopt software-based fault isolation (SFI). A
prominent example is Google's V8 heap sandbox, which represents the most widely
deployed SFI mechanism, protecting billions of users across all Chromium-based
browsers and countless applications built on Node.js and Electron. The heap
sandbox splits the address space into two parts: one part containing trusted,
security-sensitive metadata, and a sandboxed heap containing memory accessible
to untrusted code. On a technical level, the sandbox enforces isolation by
removing raw pointers and using translation tables to resolve references to
trusted objects. Consequently, an attacker cannot corrupt trusted data even
with full control of the sandboxed data, unless there is a bug in how code
handles data from the sandboxed heap. Despite their widespread use, such SFI
mechanisms have seen little security testing.
  In this work, we propose a new testing technique that models the security
boundary of modern SFI implementations. Following the SFI threat model, we
assume a powerful attacker who fully controls the sandbox's memory. We
implement this by instrumenting memory loads originating in the trusted domain
and accessing untrusted, attacker-controlled sandbox memory. We then inject
faults into the loaded data, aiming to trigger memory corruption in the trusted
domain. In a comprehensive evaluation, we identify 19 security bugs in V8 that
enable an attacker to bypass the sandbox.

</details>


### [26] [AgentSentinel: An End-to-End and Real-Time Security Defense Framework for Computer-Use Agents](https://arxiv.org/abs/2509.07764)
*Haitao Hu,Peng Chen,Yanpeng Zhao,Yuqi Chen*

Main category: cs.CR

TL;DR: AgentSentinel是一个实时防御框架，用于保护LLM驱动的计算机使用代理免受安全威胁，通过拦截敏感操作并进行安全审计，在基准测试中达到79.6%的防御成功率。


<details>
  <summary>Details</summary>
Motivation: LLM驱动的计算机代理可能发出意外的工具命令或错误输入，导致有害操作，这些由LLM决策产生的工具执行结果带来了新的安全挑战，需要专门的防御机制。

Method: 提出了AgentSentinel端到端实时防御框架，拦截代理相关服务中的所有敏感操作，在完成全面安全审计前暂停执行。安全审计机制引入了新的检查流程，将当前任务上下文与任务执行期间生成的系统跟踪相关联。

Result: 在BadComputerUse基准测试（包含60个攻击场景，覆盖6个攻击类别）上，AgentSentilin实现了79.6%的平均防御成功率，显著优于所有基线防御方法。基准测试显示当前最先进的4个LLM平均攻击成功率达到87%。

Conclusion: AgentSentinel有效解决了LLM驱动计算机代理的安全漏洞问题，通过实时拦截和上下文关联的安全审计机制，为这类系统提供了可靠的保护。

Abstract: Large Language Models (LLMs) have been increasingly integrated into
computer-use agents, which can autonomously operate tools on a user's computer
to accomplish complex tasks. However, due to the inherently unstable and
unpredictable nature of LLM outputs, they may issue unintended tool commands or
incorrect inputs, leading to potentially harmful operations. Unlike traditional
security risks stemming from insecure user prompts, tool execution results from
LLM-driven decisions introduce new and unique security challenges. These
vulnerabilities span across all components of a computer-use agent. To mitigate
these risks, we propose AgentSentinel, an end-to-end, real-time defense
framework designed to mitigate potential security threats on a user's computer.
AgentSentinel intercepts all sensitive operations within agent-related services
and halts execution until a comprehensive security audit is completed. Our
security auditing mechanism introduces a novel inspection process that
correlates the current task context with system traces generated during task
execution. To thoroughly evaluate AgentSentinel, we present BadComputerUse, a
benchmark consisting of 60 diverse attack scenarios across six attack
categories. The benchmark demonstrates a 87% average attack success rate on
four state-of-the-art LLMs. Our evaluation shows that AgentSentinel achieves an
average defense success rate of 79.6%, significantly outperforming all baseline
defenses.

</details>


### [27] [Inner-product Functional Encryption with Fine-grained Revocation for Flexible EHR Sharing](https://arxiv.org/abs/2509.07804)
*Yue Han,Jinguang Han,Liqun Chen,Chao Sun*

Main category: cs.CR

TL;DR: 这篇论文提出了一种具有细粒度撤销机制的内积功能加密方案(IPFE-FR)，并应用于电子健康记录(EHR)系统，解决医疗数据加密分享中的隐私保护和细粒度控制问题。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录(EHR)包含敏感患者信息，需要保护隐私同时支持数据挖掘。传统公钥加密无法支持选择性计算，而现有功能加密方案缺乏细粒度撤销和更新机制，不适用于EHR系统。

Method: 提出了一种新的内积功能加密方案(IPFE-FR)，支持细粒度撤销：1)可以撤销特定功能计算权限而非所有权限；2)撤销后无法计算无论是撤销前还是撤销后生成的数据；3)防止合证攻击。并将安全性归约到LWE假设。

Result: 进行了理论分析和实验实现，证明了方案的有效性和效率。

Conclusion: 该IPFE-FR方案为EHR系统提供了一种安全、灵活且支持细粒度控制的数据分享方案，有效解决了医疗数据隐私保护与数据利用之间的矛盾。

Abstract: E-health record (EHR) contains a vast amount of continuously growing medical
data and enables medical institutions to access patient health data
conveniently.This provides opportunities for medical data mining which has
important applications in identifying high-risk patients and improving disease
diagnosis, etc.Since EHR contains sensitive patient information, how to protect
patient privacy and enable mining on EHR data is important and
challenging.Traditional public key encryption (PKE) can protect patient
privacy, but cannot support flexible selective computation on encrypted EHR
data.Functional encryption (FE) allows authorised users to compute function
values of encrypted data without releasing other information, hence supporting
selective computation on encrypted data. Nevertheless, existing FE schemes do
not support fine-grained revocation and update, so they are unsuitable for EHR
system. In this paper,we first propose an inner-product functional encryption
with fine-grained revocation (IPFE-FR) scheme, and then apply it to a flexible
EHR sharing system. Our scheme possesses the following features:(1) a group
manager can revoke a specific function computation of medical institutions on
encrypted EHR data,instead of all function computation rights. (2) a revoked
medical institution is not allowed to compute the function value of encrypted
EHR data not only generated after the revocation, but also generated before the
revocation. (3) secret keys issued to the same medical institution are bound
together to prevent collusion attacks. The formal definition and security model
of the IPFE-FR scheme are proposed.Furthermore, we present a concrete
construction and reduce its security to the Learning with Errors (LWE)
assumption which is quantum-resistant. Finally, the theoretical analysis and
experimental implementation of our scheme are conducted to show its efficiency.

</details>


### [28] [Guided Reasoning in LLM-Driven Penetration Testing Using Structured Attack Trees](https://arxiv.org/abs/2509.07939)
*Katsuaki Nakano,Reza Feyyazi,Shanchieh Jay Yang,Michael Zuzak*

Main category: cs.CR

TL;DR: 本文提出了一种基于MITRE ATT&CK矩阵的导向理由流水线，通过建立确定性任务树来约束LLM渗透测试代理的理由过程，显著提高了自动化网络安全评估的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM渗透测试代理主要依靠自我导向理由，容易产生不准确或幻觉的操作步骤，导致无效行动和循环响应。

Method: 设计了一种导向理由流水线，将MITRE ATT&CK矩阵转换为确定性任务树，用于约束LLM的理由过程，限制其在经过验证的战术、技术和操作程序范围内进行推理。

Result: 在10个HackTheBox安全练习环境的103个子任务中，导向理由流水线导引LLM代理完成了71.8%、72.8%和78.6%的子任务（Llama-3-8B、Gemini-1.5和GPT-4），而自导向理由方法仅完成13.5%、16.5%和75.7%，且需要86.2%、118.7%和205.9%的更多模型查询。

Conclusion: 将确定性任务树集成到LLM理由流程中，可以显著提高自动化网络安全评估的准确性和效率，减少幻觉和无效操作。

Abstract: Recent advances in Large Language Models (LLMs) have driven interest in
automating cybersecurity penetration testing workflows, offering the promise of
faster and more consistent vulnerability assessment for enterprise systems.
Existing LLM agents for penetration testing primarily rely on self-guided
reasoning, which can produce inaccurate or hallucinated procedural steps. As a
result, the LLM agent may undertake unproductive actions, such as exploiting
unused software libraries or generating cyclical responses that repeat prior
tactics. In this work, we propose a guided reasoning pipeline for penetration
testing LLM agents that incorporates a deterministic task tree built from the
MITRE ATT&CK Matrix, a proven penetration testing kll chain, to constrain the
LLM's reaoning process to explicitly defined tactics, techniques, and
procedures. This anchors reasoning in proven penetration testing methodologies
and filters out ineffective actions by guiding the agent towards more
productive attack procedures. To evaluate our approach, we built an automated
penetration testing LLM agent using three LLMs (Llama-3-8B, Gemini-1.5, and
GPT-4) and applied it to navigate 10 HackTheBox cybersecurity exercises with
103 discrete subtasks representing real-world cyberattack scenarios. Our
proposed reasoning pipeline guided the LLM agent through 71.8\%, 72.8\%, and
78.6\% of subtasks using Llama-3-8B, Gemini-1.5, and GPT-4, respectively.
Comparatively, the state-of-the-art LLM penetration testing tool using
self-guided reasoning completed only 13.5\%, 16.5\%, and 75.7\% of subtasks and
required 86.2\%, 118.7\%, and 205.9\% more model queries. This suggests that
incorporating a deterministic task tree into LLM reasoning pipelines can
enhance the accuracy and efficiency of automated cybersecurity assessments

</details>


### [29] [ImportSnare: Directed "Code Manual" Hijacking in Retrieval-Augmented Code Generation](https://arxiv.org/abs/2509.07941)
*Kai Ye,Liangcai Su,Chenxiong Qian*

Main category: cs.CR

TL;DR: 本文提出了ImportSnare攻击框架，针对检索增强代码生成(RACG)系统，通过毒化文档和恶意依赖劫持，成功实现了对LLM代码生成的安全攻击。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在代码生成中的广泛应用，检索增强生成(RAG)虽然能提高代码正确性和安全性，但也引入了新的攻击面。本文旨在探索RACG系统中的恶意依赖劫持攻击面。

Method: 提出ImportSnare攻击框架，采用两种协同策略：1)位置感知波束搜索优化隐藏排名序列提升毒化文档检索排名；2)多语言归纳建议生成越狱序列操纵LLM推荐恶意依赖。

Result: 在Python、Rust和JavaScript上的实验显示，ImportSnare对流行库(如matplotlib和seaborn)的攻击成功率超过50%，即使在毒化比例低至0.01%时也能成功。

Conclusion: 研究揭示了LLM驱动开发中的关键供应链风险，表明代码生成任务的安全对齐存在不足，需要加强安全防护措施。

Abstract: Code generation has emerged as a pivotal capability of Large Language
Models(LLMs), revolutionizing development efficiency for programmers of all
skill levels. However, the complexity of data structures and algorithmic logic
often results in functional deficiencies and security vulnerabilities in
generated code, reducing it to a prototype requiring extensive manual
debugging. While Retrieval-Augmented Generation (RAG) can enhance correctness
and security by leveraging external code manuals, it simultaneously introduces
new attack surfaces.
  In this paper, we pioneer the exploration of attack surfaces in
Retrieval-Augmented Code Generation (RACG), focusing on malicious dependency
hijacking. We demonstrate how poisoned documentation containing hidden
malicious dependencies (e.g., matplotlib_safe) can subvert RACG, exploiting
dual trust chains: LLM reliance on RAG and developers' blind trust in LLM
suggestions. To construct poisoned documents, we propose ImportSnare, a novel
attack framework employing two synergistic strategies: 1)Position-aware beam
search optimizes hidden ranking sequences to elevate poisoned documents in
retrieval results, and 2)Multilingual inductive suggestions generate
jailbreaking sequences to manipulate LLMs into recommending malicious
dependencies. Through extensive experiments across Python, Rust, and
JavaScript, ImportSnare achieves significant attack success rates (over 50% for
popular libraries such as matplotlib and seaborn) in general, and is also able
to succeed even when the poisoning ratio is as low as 0.01%, targeting both
custom and real-world malicious packages. Our findings reveal critical supply
chain risks in LLM-powered development, highlighting inadequate security
alignment for code generation tasks. To support future research, we will
release the multilingual benchmark suite and datasets. The project homepage is
https://importsnare.github.io.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [30] [Renewable Energy Sources Selection Analysis with the Maximizing Deviation Method](https://arxiv.org/abs/2509.07011)
*Kirisci Murat*

Main category: cs.AI

TL;DR: 该研究提出了一种基于偏差最大化方法的优化模型，结合区间值Fermatean模糊集来处理多准则决策中的不确定性和模糊性，并将其应用于可再生能源选择问题。


<details>
  <summary>Details</summary>
Motivation: 多准则决策方法需要有效处理决策者判断中的不确定性和模糊性，特别是在复杂冲突情境下。Fermatean模糊环境作为模糊集的推广，能够更好地量化人类思维和感知中的不确定性。可再生能源选择是当前关键问题，既有技术性又有管理政治意义。

Method: 提出基于偏差最大化方法的优化模型来确定部分已知特征权重，结合区间值Fermatean模糊集来处理决策中的不确定性和模糊性。

Result: 开发了一种新的多准则决策方法，能够有效处理可再生能源选择等复杂决策问题中的不确定性和模糊判断。

Conclusion: 该方法为决策者提供了在不确定、复杂和冲突情况下做出更好决策的有效工具，特别适用于可再生能源选择这类具有技术和政策双重意义的决策问题。

Abstract: Multi-criteria decision-making methods provide decision-makers with
appropriate tools to make better decisions in uncertain, complex, and
conflicting situations. Fuzzy set theory primarily deals with the uncertainty
inherent in human thoughts and perceptions and attempts to quantify this
uncertainty. Fuzzy logic and fuzzy set theory are utilized with multi-criteria
decision-making methods because they effectively handle uncertainty and
fuzziness in decision-makers' judgments, allowing for verbal judgments of the
problem. This study utilizes the Fermatean fuzzy environment, a generalization
of fuzzy sets. An optimization model based on the deviation maximization method
is proposed to determine partially known feature weights. This method is
combined with interval-valued Fermatean fuzzy sets. The proposed method was
applied to the problem of selecting renewable energy sources. The reason for
choosing renewable energy sources is that meeting energy needs from renewable
sources, balancing carbon emissions, and mitigating the effects of global
climate change are among the most critical issues of the recent period. Even
though selecting renewable energy sources is a technical issue, the managerial
and political implications of this issue are also important, and are discussed
in this study.

</details>


### [31] [From Eigenmodes to Proofs: Integrating Graph Spectral Operators with Symbolic Interpretable Reasoning](https://arxiv.org/abs/2509.07017)
*Andrew Kiruluta,Priscilla Burity*

Main category: cs.AI

TL;DR: Spectral NSR是一个完全频谱的神经符号推理框架，通过图信号处理和拉普拉斯特征结构将逻辑规则嵌入为频谱模板，在频谱域直接进行推理，统一了符号推理的可解释性和频谱学习的可扩展性。


<details>
  <summary>Details</summary>
Motivation: 为了解决传统推理系统在可扩展性、适应性和可解释性方面的局限性，作者希望开发一个能够结合符号推理严谨性和神经网络学习能力的统一框架。

Method: 利用图信号处理和基于拉普拉斯特征结构的频率选择性滤波器，将逻辑规则嵌入为频谱模板，在知识图谱的图频谱域中直接执行推理。包含动态图学习、有理滤波器、扩散滤波器、频谱专家混合、证明引导训练等多种扩展。

Result: 在ProofWriter和CLUTRR等推理基准测试中，Spectral NSR相比transformer、消息传递神经网络和神经符号逻辑编程系统等基线方法，实现了更高的准确性、更快的推理速度、更好的对抗扰动鲁棒性和更高的可解释性。

Conclusion: Spectral NSR为下一代推理系统提供了一个可扩展且原则性的基础，提供了超越传统方法的透明度、鲁棒性和泛化能力。

Abstract: We introduce Spectral NSR, a fully spectral neuro-symbolic reasoning
framework that embeds logical rules as spectral templates and performs
inference directly in the graph spectral domain. By leveraging graph signal
processing (GSP) and frequency-selective filters grounded in the Laplacian
eigenstructure of knowledge graphs, the architecture unifies the
interpretability of symbolic reasoning with the scalability and adaptability of
spectral learning. Beyond the core formulation, we incorporate a comprehensive
set of extensions, including dynamic graph and basis learning, rational and
diffusion filters for sharper spectral selectivity, mixture-of-spectral-experts
for modular specialization, proof-guided training with spectral curricula, and
uncertainty quantification for calibrated confidence. Additional enhancements
such as large language model coupling, co-spectral transfer alignment,
adversarial robustness, efficient GPU kernels, generalized Laplacians, and
causal interventions further expand the versatility of the framework.
  Empirical evaluation on state-of-the-art reasoning benchmarks such as
ProofWriter and CLUTRR demonstrates that Spectral NSR achieves superior
accuracy, faster inference, improved robustness to adversarial perturbations,
and higher interpretability compared to leading baselines including
transformers, message-passing neural networks, and neuro-symbolic logic
programming systems. Spectral attribution and proof-band agreement analyses
confirm that model decisions align closely with symbolic proof structures,
while transfer experiments validate effective domain adaptation through
co-spectral alignment. These results establish Spectral NSR as a scalable and
principled foundation for the next generation of reasoning systems, offering
transparency, robustness, and generalization beyond conventional approaches.

</details>


### [32] [Statistical Methods in Generative AI](https://arxiv.org/abs/2509.07054)
*Edgar Dobriban*

Main category: cs.AI

TL;DR: 本文综述了统计方法在提升生成式AI可靠性、评估质量和效率以及设计AI干预实验方面的应用，同时讨论了现有局限性和未来方向。


<details>
  <summary>Details</summary>
Motivation: 生成式AI技术基于概率模型采样，默认缺乏正确性、安全性、公平性等保证，需要统计方法来提高其可靠性。

Method: 综述性研究，回顾了现有的统计技术及其在生成式AI中的应用，包括通用统计方法和具体应用案例。

Result: 统计方法为生成式AI提供了提高可靠性的潜在途径，同时在AI评估质量和效率提升以及实验设计方面也显示出良好前景。

Conclusion: 统计方法对改善生成式AI的可靠性具有重要意义，但需要进一步研究解决现有局限性并探索未来发展方向。

Abstract: Generative Artificial Intelligence is emerging as an important technology,
promising to be transformative in many areas. At the same time, generative AI
techniques are based on sampling from probabilistic models, and by default,
they come with no guarantees about correctness, safety, fairness, or other
properties. Statistical methods offer a promising potential approach to improve
the reliability of generative AI techniques. In addition, statistical methods
are also promising for improving the quality and efficiency of AI evaluation,
as well as for designing interventions and experiments in AI.
  In this paper, we review some of the existing work on these topics,
explaining both the general statistical techniques used, as well as their
applications to generative AI. We also discuss limitations and potential future
directions.

</details>


### [33] [Instruction Agent: Enhancing Agent with Expert Demonstration](https://arxiv.org/abs/2509.07098)
*Yinheng Li,Hailey Hultquist,Justin Wagle,Kazuhito Koishida*

Main category: cs.AI

TL;DR: Instruction Agent是一个GUI代理，通过专家演示学习并严格遵循用户意图轨迹来执行复杂任务，解决了现有GUI代理在处理新颖UI元素、长时程动作和个性化轨迹方面的困难。


<details>
  <summary>Details</summary>
Motivation: 当前GUI代理在处理复杂任务时存在困难，特别是面对新颖UI元素、长时程动作和个性化轨迹时表现不佳。需要一种能够从专家演示中学习并可靠执行复杂工作流程的方法。

Method: 基于单次专家演示提取逐步指令，严格遵循用户意图轨迹执行。采用验证器和回溯器模块来提高鲁棒性，处理意外中断和理解每个动作的结果。

Result: 在OSWorld任务集上达到60%的成功率，而所有顶级代理都无法完成这些任务。

Conclusion: Instruction Agent提供了一个实用且可扩展的框架，弥合了当前GUI代理与可靠现实世界GUI任务自动化之间的差距。

Abstract: Graphical user interface (GUI) agents have advanced rapidly but still
struggle with complex tasks involving novel UI elements, long-horizon actions,
and personalized trajectories. In this work, we introduce Instruction Agent, a
GUI agent that leverages expert demonstrations to solve such tasks, enabling
completion of otherwise difficult workflows. Given a single demonstration, the
agent extracts step-by-step instructions and executes them by strictly
following the trajectory intended by the user, which avoids making mistakes
during execution. The agent leverages the verifier and backtracker modules
further to improve robustness. Both modules are critical to understand the
current outcome from each action and handle unexpected interruptions(such as
pop-up windows) during execution. Our experiments show that Instruction Agent
achieves a 60% success rate on a set of tasks in OSWorld that all top-ranked
agents failed to complete. The Instruction Agent offers a practical and
extensible framework, bridging the gap between current GUI agents and reliable
real-world GUI task automation.

</details>


### [34] [Neuro-Symbolic Frameworks: Conceptual Characterization and Empirical Comparative Analysis](https://arxiv.org/abs/2509.07122)
*Sania Sinha,Tanawan Premsri,Danial Kamali,Parisa Kordjamshidi*

Main category: cs.AI

TL;DR: 本文综述了神经符号(NeSy)框架的技术特点，分析了现有框架在符号表示语言、神经模型集成和底层算法等方面的差异，重点介绍了DeepProbLog、Scallop和DomiKnowS三个代表性框架。


<details>
  <summary>Details</summary>
Motivation: 神经符号框架结合了神经表示学习和符号推理的优势，但该领域缺乏用户友好的工具和统一框架，开发门槛较高。本文旨在系统分析现有NeSy框架的技术特点，为开发者提供指导。

Method: 通过分析现有NeSy框架的符号表示语言、神经模型集成方式和底层算法等关键技术方面，对DeepProbLog、Scallop和DomiKnowS三个代表性框架进行深入比较和评估。

Result: 识别了不同框架在表达能力方面的关键差异和挑战，为理解各框架解决不同类型问题的能力提供了基础。

Conclusion: 本文为神经符号计算领域提供了系统的框架分析，旨在激发社区重新思考这一问题，推动开发更通用、用户友好的神经符号框架。

Abstract: Neurosymbolic (NeSy) frameworks combine neural representations and learning
with symbolic representations and reasoning. Combining the reasoning
capacities, explainability, and interpretability of symbolic processing with
the flexibility and power of neural computing allows us to solve complex
problems with more reliability while being data-efficient. However, this
recently growing topic poses a challenge to developers with its learning curve,
lack of user-friendly tools, libraries, and unifying frameworks. In this paper,
we characterize the technical facets of existing NeSy frameworks, such as the
symbolic representation language, integration with neural models, and the
underlying algorithms. A majority of the NeSy research focuses on algorithms
instead of providing generic frameworks for declarative problem specification
to leverage problem solving. To highlight the key aspects of Neurosymbolic
modeling, we showcase three generic NeSy frameworks - \textit{DeepProbLog},
\textit{Scallop}, and \textit{DomiKnowS}. We identify the challenges within
each facet that lay the foundation for identifying the expressivity of each
framework in solving a variety of problems. Building on this foundation, we aim
to spark transformative action and encourage the community to rethink this
problem in novel ways.

</details>


### [35] [Autoencoder-Based Denoising of Muscle Artifacts in ECG to Preserve Skin Nerve Activity (SKNA) for Cognitive Stress Detection](https://arxiv.org/abs/2509.07146)
*Farnoush Baghestani,Jihye Moon,Youngsun Kong,Ki Chon*

Main category: cs.AI

TL;DR: 提出了一种基于一维卷积自编码器和LSTM的深度学习方法来去除皮肤神经活动(SKNA)信号中的肌电(EMG)噪声干扰，显著提高了信号质量和生理特征的可识别性。


<details>
  <summary>Details</summary>
Motivation: 皮肤神经活动(SKNA)是监测交感神经系统的重要非侵入性指标，但其测量容易受到肌电(EMG)污染的干扰，特别是在肌肉活动期间，传统带通滤波方法效果有限。

Method: 使用轻量级一维卷积自编码器结合LSTM瓶颈层，从EMG污染的SKNA记录中重建干净的SKNA信号。采用留一受试者交叉验证框架，在认知压力实验数据和混沌肌肉刺激噪声数据上进行训练。

Result: 该方法将信噪比提高了9.65dB，与干净SKNA的互相关系数从0.40提升到0.72，基于爆发的SKNA特征恢复到了接近干净的判别能力(AUROC≥0.96)。在严重噪声水平下，基线vs交感刺激条件分类准确率达到91-98%。

Conclusion: 基于深度学习的重建方法能够在显著EMG干扰下保留生理相关的交感神经爆发特征，为在自然运动丰富的环境中进行更稳健的SKNA监测提供了可能。

Abstract: The sympathetic nervous system (SNS) plays a central role in regulating the
body's responses to stress and maintaining physiological stability. Its
dysregulation is associated with a wide range of conditions, from
cardiovascular disease to anxiety disorders. Skin nerve activity (SKNA)
extracted from high-frequency electrocardiogram (ECG) recordings provides a
noninvasive window into SNS dynamics, but its measurement is highly susceptible
to electromyographic (EMG) contamination. Traditional preprocessing based on
bandpass filtering within a fixed range (e.g., 500--1000 Hz) is susceptible to
overlapping EMG and SKNA spectral components, especially during sustained
muscle activity. We present a denoising approach using a lightweight
one-dimensional convolutional autoencoder with a long short-term memory (LSTM)
bottleneck to reconstruct clean SKNA from EMG-contaminated recordings. Using
clean ECG-derived SKNA data from cognitive stress experiments and EMG noise
from chaotic muscle stimulation recordings, we simulated contamination at
realistic noise levels (--4 dB, --8 dB signal-to-noise ratio) and trained the
model in the leave-one-subject-out cross-validation framework. The method
improved signal-to-noise ratio by up to 9.65 dB, increased cross correlation
with clean SKNA from 0.40 to 0.72, and restored burst-based SKNA features to
near-clean discriminability (AUROC $\geq$ 0.96). Classification of baseline
versus sympathetic stimulation (cognitive stress) conditions reached accuracies
of 91--98\% across severe noise levels, comparable to clean data. These results
demonstrate that deep learning--based reconstruction can preserve
physiologically relevant sympathetic bursts during substantial EMG
interference, enabling more robust SKNA monitoring in naturalistic,
movement-rich environments.

</details>


### [36] [PaVeRL-SQL: Text-to-SQL via Partial-Match Rewards and Verbal Reinforcement Learning](https://arxiv.org/abs/2509.07159)
*Heng Hao,Wenjun Hu,Oxana Verkholyak,Davoud Ataee Tarzanagh,Baruch Gutow,Sima Didari,Masoud Faraki,Hankyu Moon,Seungjai Min*

Main category: cs.AI

TL;DR: PaVeRL-SQL是一个结合部分匹配奖励和语言强化学习的框架，用于提升文本到SQL转换模型在工业级复杂数据库上的执行准确率，在多个基准测试中达到最先进水平。


<details>
  <summary>Details</summary>
Motivation: 当前文本到SQL方法在工业级数据库和涉及领域特定业务逻辑的复杂问题上执行准确率较低，需要更有效的解决方案来处理实际应用场景。

Method: 采用两种管道：1）基于大语言模型的上下文学习框架与群体自评估（语言强化学习）；2）基于小模型（OmniSQL-7B）的思维链强化学习管道，使用特殊设计的奖励函数和两阶段强化学习训练。

Result: 在Spider、Spider 2.0和BIRD基准测试中达到最先进结果，其中在工业级Spider2.0-SQLite基准上，语言强化学习管道比现有最佳方法高7.4%，思维链管道高1.4%。混合SQL方言训练带来三倍性能提升。

Conclusion: PaVeRL-SQL在现实工业约束下提供了可靠的最先进文本到SQL解决方案，特别适用于训练数据有限的SQL方言场景。

Abstract: Text-to-SQL models allow users to interact with a database more easily by
generating executable SQL statements from natural-language questions. Despite
recent successes on simpler databases and questions, current Text-to-SQL
methods still suffer from low execution accuracy on industry-scale databases
and complex questions involving domain-specific business logic. We present
\emph{PaVeRL-SQL}, a framework that combines \emph{Partial-Match Rewards} and
\emph{Verbal Reinforcement Learning} to drive self-improvement in reasoning
language models (RLMs) for Text-to-SQL. To handle practical use cases, we adopt
two pipelines: (1) a newly designed in-context learning framework with group
self-evaluation (verbal-RL), using capable open- and closed-source large
language models (LLMs) as backbones; and (2) a chain-of-thought (CoT) RL
pipeline with a small backbone model (OmniSQL-7B) trained with a specially
designed reward function and two-stage RL. These pipelines achieve
state-of-the-art (SOTA) results on popular Text-to-SQL benchmarks -- Spider,
Spider 2.0, and BIRD. For the industrial-level Spider2.0-SQLite benchmark, the
verbal-RL pipeline achieves an execution accuracy 7.4\% higher than SOTA, and
the CoT pipeline is 1.4\% higher. RL training with mixed SQL dialects yields
strong, threefold gains, particularly for dialects with limited training data.
Overall, \emph{PaVeRL-SQL} delivers reliable, SOTA Text-to-SQL under realistic
industrial constraints. The code is available at
https://github.com/PaVeRL-SQL/PaVeRL-SQL.

</details>


### [37] [That's So FETCH: Fashioning Ensemble Techniques for LLM Classification in Civil Legal Intake and Referral](https://arxiv.org/abs/2509.07170)
*Quinten Steenhuis*

Main category: cs.AI

TL;DR: FETCH分类器用于法律问题分类，通过混合LLM/ML集成方法和自动生成后续问题来提高准确性，在真实数据集上达到97.37%的准确率，超越GPT-5模型。


<details>
  <summary>Details</summary>
Motivation: 每年数百万人寻求法律援助，错误分类会导致严重后果（错过截止日期、遭受虐待、失去住房或子女监护权），需要准确的法律问题分类来确保用户获得正确的法律帮助。

Method: 采用混合LLM/ML集成分类方法，结合自动生成后续问题来丰富初始问题叙述，使用419个真实世界查询的非营利律师转介服务数据集。

Result: 分类准确率（hits@2）达到97.37%，使用成本较低的模型组合，超越了当前最先进的GPT-5模型的性能。

Conclusion: 该方法在显著降低法律系统用户引导成本的同时实现了高准确性，有望改善法律援助服务的效率和准确性。

Abstract: Each year millions of people seek help for their legal problems by calling a
legal aid program hotline, walking into a legal aid office, or using a lawyer
referral service. The first step to match them to the right help is to identify
the legal problem the applicant is experiencing. Misdirection has consequences.
Applicants may miss a deadline, experience physical abuse, lose housing or lose
custody of children while waiting to connect to the right legal help. We
introduce and evaluate the FETCH classifier for legal issue classification and
describe two methods for improving accuracy: a hybrid LLM/ML ensemble
classification method, and the automatic generation of follow-up questions to
enrich the initial problem narrative. We employ a novel data set of 419
real-world queries to a nonprofit lawyer referral service. Ultimately, we show
classification accuracy (hits@2) of 97.37\% using a mix of inexpensive models,
exceeding the performance of the current state-of-the-art GPT-5 model. Our
approach shows promise in significantly reducing the cost of guiding users of
the legal system to the right resource for their problem while achieving high
accuracy.

</details>


### [38] [A Hybrid CNN-LSTM Deep Learning Model for Intrusion Detection in Smart Grid](https://arxiv.org/abs/2509.07208)
*Abdulhakim Alsaiari,Mohammad Ilyas*

Main category: cs.AI

TL;DR: 这篇论文提出了一种基于深度学习的混合入侵检测系统(IDS)，用于提升智能电网的网络安全性，在DNP3和IEC104数据集上达到99.70%的检测准确率。


<details>
  <summary>Details</summary>
Motivation: 智能电网的发展带来了新的安全漏洞，特别是SCADA协议容易受到非授权访问和拒绝服务攻击，需要有效的入侵检测方案来保护系统安全。

Method: 采用卷积神经网络(CNN)进行特征提取，结合长短期记忆网络(LSTM)识别时序模式，构建了CNN-LSTM混合深度学习模型。

Result: 在DNP3和IEC104入侵检测数据集上，该模型在准确率、精确率、召回率和F1分数方面都显著优于其他深度学习方法，检测准确率达到99.70%。

Conclusion: 该研究提出的混合深度学习入侵检测系统能够有效地保护智能电网元容器受到网络攻击，为智能电网安全提供了可靠的解决方案。

Abstract: The evolution of the traditional power grid into the "smart grid" has
resulted in a fundamental shift in energy management, which allows the
integration of renewable energy sources with modern communication technology.
However, this interconnection has increased smart grids' vulnerability to
attackers, which might result in privacy breaches, operational interruptions,
and massive outages. The SCADA-based smart grid protocols are critical for
real-time data collection and control, but they are vulnerable to attacks like
unauthorized access and denial of service (DoS). This research proposes a
hybrid deep learning-based Intrusion Detection System (IDS) intended to improve
the cybersecurity of smart grids. The suggested model takes advantage of
Convolutional Neural Networks' (CNN) feature extraction capabilities as well as
Long Short-Term Memory (LSTM) networks' temporal pattern recognition skills.
DNP3 and IEC104 intrusion detection datasets are employed to train and test our
CNN-LSTM model to recognize and classify the potential cyber threats. Compared
to other deep learning approaches, the results demonstrate considerable
improvements in accuracy, precision, recall, and F1-score, with a detection
accuracy of 99.70%.

</details>


### [39] [BlendedNet: A Blended Wing Body Aircraft Dataset and Surrogate Model for Aerodynamic Predictions](https://arxiv.org/abs/2509.07209)
*Nicholas Sung,Steven Spreizer,Mohamed Elrefaie,Kaira Samuel,Matthew C. Jones,Faez Ahmed*

Main category: cs.AI

TL;DR: BlendedNet是一个包含999个混合翼体(BWB)气动几何的公开数据集，包含8830个RANS模拟案例，并提出了端到端的点状气动预测代理框架。


<details>
  <summary>Details</summary>
Motivation: 解决非常规气动构型数据稀缺问题，促进数据驱动的气动设计代理建模研究

Method: 通过采样几何设计参数和飞行条件生成数据集，使用PointNet回归器预测几何参数，然后通过FiLM网络结合预测参数和飞行条件来预测点状系数

Result: 在不同BWB构型上实现了较低的表面预测误差

Conclusion: BlendedNet为非常规气动构型提供了宝贵的数据资源，并展示了端到端代理框架在气动预测中的有效性

Abstract: BlendedNet is a publicly available aerodynamic dataset of 999 blended wing
body (BWB) geometries. Each geometry is simulated across about nine flight
conditions, yielding 8830 converged RANS cases with the Spalart-Allmaras model
and 9 to 14 million cells per case. The dataset is generated by sampling
geometric design parameters and flight conditions, and includes detailed
pointwise surface quantities needed to study lift and drag. We also introduce
an end-to-end surrogate framework for pointwise aerodynamic prediction. The
pipeline first uses a permutation-invariant PointNet regressor to predict
geometric parameters from sampled surface point clouds, then conditions a
Feature-wise Linear Modulation (FiLM) network on the predicted parameters and
flight conditions to predict pointwise coefficients Cp, Cfx, and Cfz.
Experiments show low errors in surface predictions across diverse BWBs.
BlendedNet addresses data scarcity for unconventional configurations and
enables research on data-driven surrogate modeling for aerodynamic design.

</details>


### [40] [OmniAcc: Personalized Accessibility Assistant Using Generative AI](https://arxiv.org/abs/2509.07220)
*Siddhant Karki,Ethan Han,Nadim Mahmud,Suman Bhunia,John Femiani,Vaskar Raychoudhury*

Main category: cs.AI

TL;DR: OmniAcc是一个基于GPT-4的AI导航系统，通过卫星图像和OpenStreetMap数据识别轮椅无障碍设施，提供个性化路线规划和实时导航，准确率达97.5%。


<details>
  <summary>Details</summary>
Motivation: 行动不便人士在城市导航中面临无障碍信息缺乏的障碍，需要智能工具来改善导航体验和城市包容性。

Method: 利用GPT-4、卫星图像和OpenStreetMap数据，采用零样本学习和定制提示词来精确检测无障碍设施，支持结构化验证工作流。

Result: 在人行道检测案例中达到97.5%的准确率，展示了系统在识别无障碍特征方面的有效性。

Conclusion: OmniAcc展示了AI在改善导航和创建更包容城市空间方面的变革潜力，为城市规划者和行动辅助用户提供了有力工具。

Abstract: Individuals with ambulatory disabilities often encounter significant barriers
when navigating urban environments due to the lack of accessible information
and tools. This paper presents OmniAcc, an AI-powered interactive navigation
system that utilizes GPT-4, satellite imagery, and OpenStreetMap data to
identify, classify, and map wheelchair-accessible features such as ramps and
crosswalks in the built environment. OmniAcc offers personalized route
planning, real-time hands-free navigation, and instant query responses
regarding physical accessibility. By using zero-shot learning and customized
prompts, the system ensures precise detection of accessibility features, while
supporting validation through structured workflows. This paper introduces
OmniAcc and explores its potential to assist urban planners and mobility-aid
users, demonstrated through a case study on crosswalk detection. With a
crosswalk detection accuracy of 97.5%, OmniAcc highlights the transformative
potential of AI in improving navigation and fostering more inclusive urban
spaces.

</details>


### [41] [HealthSLM-Bench: Benchmarking Small Language Models for Mobile and Wearable Healthcare Monitoring](https://arxiv.org/abs/2509.07260)
*Xin Wang,Ting Dang,Xinyu Zhang,Vassilis Kostakos,Michael J. Witbrock,Hong Jia*

Main category: cs.AI

TL;DR: 小语言模型(SLMs)在移动医疗监测中表现可与大语言模型媲美，同时提供更好的效率和隐私保护，但在类别不平衡和少样本场景中仍存在挑战。


<details>
  <summary>Details</summary>
Motivation: 解决云基LLM医疗方案存在的隐私泄露、内存占用高和延迟问题，探索轻量级SLMs在移动设备本地运行的医疗预测性能。

Method: 系统评估SLMs在健康预测任务中的表现，采用零样本、少样本和指令微调方法，并将最佳微调模型部署到移动设备进行实际效率测试。

Result: SLMs能够达到与LLMs相当的性能，同时在效率和隐私方面有显著优势，但在处理类别不平衡和少样本场景时仍面临挑战。

Conclusion: SLMs虽然目前还不完美，但作为下一代隐私保护医疗监测解决方案具有很大潜力，是实现移动设备本地高效医疗预测的有前途方向。

Abstract: Mobile and wearable healthcare monitoring play a vital role in facilitating
timely interventions, managing chronic health conditions, and ultimately
improving individuals' quality of life. Previous studies on large language
models (LLMs) have highlighted their impressive generalization abilities and
effectiveness in healthcare prediction tasks. However, most LLM-based
healthcare solutions are cloud-based, which raises significant privacy concerns
and results in increased memory usage and latency. To address these challenges,
there is growing interest in compact models, Small Language Models (SLMs),
which are lightweight and designed to run locally and efficiently on mobile and
wearable devices. Nevertheless, how well these models perform in healthcare
prediction remains largely unexplored. We systematically evaluated SLMs on
health prediction tasks using zero-shot, few-shot, and instruction fine-tuning
approaches, and deployed the best performing fine-tuned SLMs on mobile devices
to evaluate their real-world efficiency and predictive performance in practical
healthcare scenarios. Our results show that SLMs can achieve performance
comparable to LLMs while offering substantial gains in efficiency and privacy.
However, challenges remain, particularly in handling class imbalance and
few-shot scenarios. These findings highlight SLMs, though imperfect in their
current form, as a promising solution for next-generation, privacy-preserving
healthcare monitoring.

</details>


### [42] [Performative Thinking? The Brittle Correlation Between CoT Length and Problem Complexity](https://arxiv.org/abs/2509.07339)
*Vardhan Palod,Karthik Valmeekam,Kaya Stechly,Subbarao Kambhampati*

Main category: cs.AI

TL;DR: 这篇论文对中间代币生成(ITG)机制进行了批判性分析，发现中间推理追踪长度与问题难度无明显相关性，而是主要受训练数据分布影响


<details>
  <summary>Details</summary>
Motivation: 批判性检验当前社区将中间代币生成触化为"思考"的偏见，探索中间推理追踪长度是否真正反映问题难度

Method: 使用A* 搜索算法的演绎追踪训练transformer模型，通过迷宫问题的操作步数精确量化问题复杂度，系统评估模型在分布外问题上的表现

Result: 发现中间代币序列长度与真实A* 追踪长度只有松散相关，相关性仅在问题接近训练分布时出现，说明这是近似记忆而非真正的问题适应性计算

Conclusion: 中间追踪生成并非适应问题难度，长序列不能自动解释为"思考努力"，问题的本质计算复杂性不是关键因素，而是分布距离起决定作用

Abstract: Intermediate token generation (ITG), where a model produces output before the
solution, has been proposed as a method to improve the performance of language
models on reasoning tasks. While these reasoning traces or Chain of Thoughts
(CoTs) are correlated with performance gains, the mechanisms underlying them
remain unclear. A prevailing assumption in the community has been to
anthropomorphize these tokens as "thinking", treating longer traces as evidence
of higher problem-adaptive computation. In this work, we critically examine
whether intermediate token sequence length reflects or correlates with problem
difficulty. To do so, we train transformer models from scratch on derivational
traces of the A* search algorithm, where the number of operations required to
solve a maze problem provides a precise and verifiable measure of problem
complexity. We first evaluate the models on trivial free-space problems,
finding that even for the simplest tasks, they often produce excessively long
reasoning traces and sometimes fail to generate a solution. We then
systematically evaluate the model on out-of-distribution problems and find that
the intermediate token length and ground truth A* trace length only loosely
correlate. We notice that the few cases where correlation appears are those
where the problems are closer to the training distribution, suggesting that the
effect arises from approximate recall rather than genuine problem-adaptive
computation. This suggests that the inherent computational complexity of the
problem instance is not a significant factor, but rather its distributional
distance from the training data. These results challenge the assumption that
intermediate trace generation is adaptive to problem difficulty and caution
against interpreting longer sequences in systems like R1 as automatically
indicative of "thinking effort".

</details>


### [43] [Autonomous Code Evolution Meets NP-Completeness](https://arxiv.org/abs/2509.07367)
*Cunxi Yu,Rongjian Liang,Chia-Tung Ho,Haoxing Ren*

Main category: cs.AI

TL;DR: SATLUTION是首个将LLM代码进化扩展到完整代码库规模的框架，针对SAT问题，通过LLM代理在严格正确性保证下进化求解器，在SAT竞赛中超越了人类设计的获胜者。


<details>
  <summary>Details</summary>
Motivation: 受到AlphaEvolve启发，但AlphaEvolve仅限于数百行代码的孤立内核，需要扩展到包含数百个文件和数万行代码的完整代码库规模。

Method: 使用LLM代理在严格正确性保证和分布式运行时反馈下直接进化求解器代码库，同时自进化自身的进化策略和规则。

Result: 从SAT竞赛2024代码库开始，SATLUTION进化的求解器在SAT竞赛2025中明显超越了人类设计的获胜者，并在2024基准测试中超越了2024和2025的冠军。

Conclusion: SATLUTION成功证明了LLM代码进化可以扩展到完整代码库规模，并在复杂算法问题上超越人类专家表现。

Abstract: Large language models (LLMs) have recently shown strong coding abilities,
enabling not only static code generation but also iterative code self-evolving
through agentic frameworks. Recently, AlphaEvolve \cite{novikov2025alphaevolve}
demonstrated that LLM-based coding agents can autonomously improve algorithms
and surpass human experts, with scopes limited to isolated kernels spanning
hundreds of lines of code. Inspired by AlphaEvolve, we present SATLUTION, the
first framework to extend LLM-based code evolution to the full repository
scale, encompassing hundreds of files and tens of thousands of lines of C/C++
code. Targeting Boolean Satisfiability (SAT), the canonical NP-complete problem
and a cornerstone of both theory and applications. SATLUTION orchestrates LLM
agents to directly evolve solver repositories under strict correctness
guarantees and distributed runtime feedback, while simultaneously self-evolving
its own evolution policies and rules. Starting from SAT Competition 2024
codebases and benchmark, SATLUTION evolved solvers that decisively outperformed
the human-designed winners of the SAT Competition 2025, and also surpassed both
2024 and 2025 champions on the 2024 benchmarks.

</details>


### [44] [Language Self-Play For Data-Free Training](https://arxiv.org/abs/2509.07414)
*Jakub Grudzien Kuba,Mengting Gu,Qi Ma,Yuandong Tian,Vijai Mohan*

Main category: cs.AI

TL;DR: 提出Language Self-Play (LSP)方法，通过自博弈强化学习让语言模型无需额外数据就能自我提升性能


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型发展中面临的数据依赖瓶颈问题，避免需要不断增加训练数据的需求

Method: 基于博弈论的自博弈框架，将模型能力视为竞争游戏中的表现，通过模型与自己对抗来产生更强的策略

Result: 在Llama-3.2-3B-Instruct模型上的实验显示，仅通过自博弈就能提升指令跟随任务的性能，效果优于数据驱动的基线方法

Conclusion: 自博弈强化学习是突破数据依赖瓶颈的有效途径，为语言模型的持续改进提供了新的方向

Abstract: Large language models (LLMs) have advanced rapidly in recent years, driven by
scale, abundant high-quality training data, and reinforcement learning. Yet
this progress faces a fundamental bottleneck: the need for ever more data from
which models can continue to learn. In this work, we propose a reinforcement
learning approach that removes this dependency by enabling models to improve
without additional data. Our method leverages a game-theoretic framework of
self-play, where a model's capabilities are cast as performance in a
competitive game and stronger policies emerge by having the model play against
itself - a process we call Language Self-Play (LSP). Experiments with
Llama-3.2-3B-Instruct on instruction-following benchmarks show that pretrained
models can not only enhance their performance on challenging tasks through
self-play alone, but can also do so more effectively than data-driven
baselines.

</details>


### [45] [SheetDesigner: MLLM-Powered Spreadsheet Layout Generation with Rule-Based and Vision-Based Reflection](https://arxiv.org/abs/2509.07473)
*Qin Chen,Yuanyi Ren,Xiaojun Ma,Mugeng Liu,Han Shi,Dongmei Zhang*

Main category: cs.AI

TL;DR: SheetDesigner是一个零样本、无需训练的电子表格布局生成框架，使用多模态大语言模型结合规则和视觉反射来解决电子表格自动化布局问题，在3326个电子表格数据集上比基线方法提升至少22.6%。


<details>
  <summary>Details</summary>
Motivation: 电子表格在数据任务中至关重要，但手动设计布局耗时且需要专业知识。现有自动化布局模型不适合电子表格，因为它们忽视了电子表格的离散网格结构和独特的数据依赖关系等语义信息。

Method: 提出SheetDesigner框架，使用多模态大语言模型(MLLMs)，结合规则反射和视觉反射进行组件放置和内容填充，无需训练即可实现零样本布局生成。

Result: 在3326个电子表格数据集上，SheetDesigner比5个基线方法性能提升至少22.6%。通过视觉模态，MLLMs能很好地处理重叠和平衡问题，但在对齐方面仍有困难。

Conclusion: SheetDesigner证明了MLLMs在电子表格布局生成任务中的有效性，但需要结合规则和视觉反射策略来弥补对齐方面的不足。代码和数据已开源。

Abstract: Spreadsheets are critical to data-centric tasks, with rich, structured
layouts that enable efficient information transmission. Given the time and
expertise required for manual spreadsheet layout design, there is an urgent
need for automated solutions. However, existing automated layout models are
ill-suited to spreadsheets, as they often (1) treat components as axis-aligned
rectangles with continuous coordinates, overlooking the inherently discrete,
grid-based structure of spreadsheets; and (2) neglect interrelated semantics,
such as data dependencies and contextual links, unique to spreadsheets. In this
paper, we first formalize the spreadsheet layout generation task, supported by
a seven-criterion evaluation protocol and a dataset of 3,326 spreadsheets. We
then introduce SheetDesigner, a zero-shot and training-free framework using
Multimodal Large Language Models (MLLMs) that combines rule and vision
reflection for component placement and content population. SheetDesigner
outperforms five baselines by at least 22.6\%. We further find that through
vision modality, MLLMs handle overlap and balance well but struggle with
alignment, necessitates hybrid rule and visual reflection strategies. Our codes
and data is available at Github.

</details>


### [46] [Towards explainable decision support using hybrid neural models for logistic terminal automation](https://arxiv.org/abs/2509.07577)
*Riccardo DElia,Alberto Termine,Francesco Flammini*

Main category: cs.AI

TL;DR: 提出了一种可解释的神经系统动力学建模框架，结合深度学习和多种可解释性技术，在保持传统系统动力学模型因果透明性的同时提升预测准确性


<details>
  <summary>Details</summary>
Motivation: 深度学习在交通物流系统动力学建模中虽然提升了可扩展性和预测准确性，但牺牲了解释性和因果可靠性，这在关键决策系统中是不可接受的

Method: 提出了一个可解释性设计的混合框架，结合概念可解释性、机制可解释性和因果机器学习技术，构建基于语义有意义变量的神经网络模型

Result: 该框架旨在应用于欧盟AutoMoTIF项目的真实案例研究，支持多式联运物流终端的数据驱动决策、自动化和优化

Conclusion: 神经符号方法可以弥合黑盒预测模型与复杂动态环境中关键决策支持需求之间的差距，特别是在工业物联网支持的网络物理系统中

Abstract: The integration of Deep Learning (DL) in System Dynamics (SD) modeling for
transportation logistics offers significant advantages in scalability and
predictive accuracy. However, these gains are often offset by the loss of
explainability and causal reliability $-$ key requirements in critical
decision-making systems. This paper presents a novel framework for
interpretable-by-design neural system dynamics modeling that synergizes DL with
techniques from Concept-Based Interpretability, Mechanistic Interpretability,
and Causal Machine Learning. The proposed hybrid approach enables the
construction of neural network models that operate on semantically meaningful
and actionable variables, while retaining the causal grounding and transparency
typical of traditional SD models. The framework is conceived to be applied to
real-world case-studies from the EU-funded project AutoMoTIF, focusing on
data-driven decision support, automation, and optimization of multimodal
logistic terminals. We aim at showing how neuro-symbolic methods can bridge the
gap between black-box predictive models and the need for critical decision
support in complex dynamical environments within cyber-physical systems enabled
by the industrial Internet-of-Things.

</details>


### [47] [Transferable Direct Prompt Injection via Activation-Guided MCMC Sampling](https://arxiv.org/abs/2509.07617)
*Minghui Li,Hao Zhang,Yechao Zhang,Wei Wan,Shengshan Hu,pei Xiaobing,Jing Wang*

Main category: cs.AI

TL;DR: 基于激活值的指导案例攻击框架，通过能量模型和MCMC采样优化攻击提示，实现了高转移性的黑盒攻击


<details>
  <summary>Details</summary>
Motivation: 解决现有白盒/灰盒方法不实用和黑盒方法转移性差的问题，应对直接提示注入攻击的安全威胁

Method: 构建基于替代模型激活值的能量模型(EBM)评估对抗提示质量，采用标记级Markov Chain Monte Carlo采样适应性优化对抗提示

Result: 在5个主流LLM上达到49.6%攻击成功率，比人工制作提示提升34.6%，在未见任务上保持36.6%攻击成功率

Conclusion: 激活值与攻击效果存在相关性，语义模式在转移性漏洞利用中发挥关键作用，提出的框架有效解决了黑盒攻击的转移性问题

Abstract: Direct Prompt Injection (DPI) attacks pose a critical security threat to
Large Language Models (LLMs) due to their low barrier of execution and high
potential damage. To address the impracticality of existing white-box/gray-box
methods and the poor transferability of black-box methods, we propose an
activations-guided prompt injection attack framework. We first construct an
Energy-based Model (EBM) using activations from a surrogate model to evaluate
the quality of adversarial prompts. Guided by the trained EBM, we employ the
token-level Markov Chain Monte Carlo (MCMC) sampling to adaptively optimize
adversarial prompts, thereby enabling gradient-free black-box attacks.
Experimental results demonstrate our superior cross-model transferability,
achieving 49.6% attack success rate (ASR) across five mainstream LLMs and 34.6%
improvement over human-crafted prompts, and maintaining 36.6% ASR on unseen
task scenarios. Interpretability analysis reveals a correlation between
activations and attack effectiveness, highlighting the critical role of
semantic patterns in transferable vulnerability exploitation.

</details>


### [48] [Getting In Contract with Large Language Models -- An Agency Theory Perspective On Large Language Model Alignment](https://arxiv.org/abs/2509.07642)
*Sascha Kaltenpoth,Oliver Müller*

Main category: cs.AI

TL;DR: 提出了LLM ATLAS框架，基于代理理论来解决组织采用LLM时的对齐问题，通过文献分析构建了问题-解决方案空间


<details>
  <summary>Details</summary>
Motivation: LLM在组织中的应用可能产生偏离主题、歧视性或有害内容，由于黑盒性质导致信息不对称，现有研究未能解决组织采用过程中的对齐问题

Method: 基于代理理论构建概念框架，采用组织LLM采用阶段和代理理论作为概念进行文献分析

Result: 开发了针对组织LLM采用过程中AI对齐方法的扩展文献分析流程，并构建了首个LLM对齐问题-解决方案空间

Conclusion: LLM ATLAS框架为组织采用LLM时的对齐问题提供了理论基础和实用指导，有助于缓解信息不对称带来的风险

Abstract: Adopting Large language models (LLMs) in organizations potentially
revolutionizes our lives and work. However, they can generate off-topic,
discriminating, or harmful content. This AI alignment problem often stems from
misspecifications during the LLM adoption, unnoticed by the principal due to
the LLM's black-box nature. While various research disciplines investigated AI
alignment, they neither address the information asymmetries between
organizational adopters and black-box LLM agents nor consider organizational AI
adoption processes. Therefore, we propose LLM ATLAS (LLM Agency Theory-Led
Alignment Strategy) a conceptual framework grounded in agency (contract)
theory, to mitigate alignment problems during organizational LLM adoption. We
conduct a conceptual literature analysis using the organizational LLM adoption
phases and the agency theory as concepts. Our approach results in (1) providing
an extended literature analysis process specific to AI alignment methods during
organizational LLM adoption and (2) providing a first LLM alignment
problem-solution space.

</details>


### [49] [DeepGraphLog for Layered Neurosymbolic AI](https://arxiv.org/abs/2509.07665)
*Adem Kikaj,Giuseppe Marra,Floris Geerts,Robin Manhaeve,Luc De Raedt*

Main category: cs.AI

TL;DR: DeepGraphLog是一个新的神经符号AI框架，通过图神经网络谓词扩展ProbLog，支持多层神经符号推理，能够处理图结构数据并克服现有系统的限制。


<details>
  <summary>Details</summary>
Motivation: 当前神经符号AI框架（如DeepProbLog）采用固定的神经处理后接符号推理的流程，无法处理复杂依赖关系，特别是在图等不规则数据结构中。

Method: 扩展ProbLog并引入图神经网络谓词，将符号表示视为图，允许神经网络和符号组件以任意顺序分层处理，支持多层神经符号推理。

Result: 在规划、知识图谱补全和GNN表达能力等任务中，DeepGraphLog有效捕获了复杂关系依赖，克服了现有神经符号系统的关键限制。

Conclusion: DeepGraphLog通过将神经符号AI扩展到图结构领域，提供了一个更表达性和灵活的神经符号集成框架。

Abstract: Neurosymbolic AI (NeSy) aims to integrate the statistical strengths of neural
networks with the interpretability and structure of symbolic reasoning.
However, current NeSy frameworks like DeepProbLog enforce a fixed flow where
symbolic reasoning always follows neural processing. This restricts their
ability to model complex dependencies, especially in irregular data structures
such as graphs. In this work, we introduce DeepGraphLog, a novel NeSy framework
that extends ProbLog with Graph Neural Predicates. DeepGraphLog enables
multi-layer neural-symbolic reasoning, allowing neural and symbolic components
to be layered in arbitrary order. In contrast to DeepProbLog, which cannot
handle symbolic reasoning via neural methods, DeepGraphLog treats symbolic
representations as graphs, enabling them to be processed by Graph Neural
Networks (GNNs). We showcase the capabilities of DeepGraphLog on tasks in
planning, knowledge graph completion with distant supervision, and GNN
expressivity. Our results demonstrate that DeepGraphLog effectively captures
complex relational dependencies, overcoming key limitations of existing NeSy
systems. By broadening the applicability of neurosymbolic AI to
graph-structured domains, DeepGraphLog offers a more expressive and flexible
framework for neural-symbolic integration.

</details>


### [50] [Unleashing the True Potential of LLMs: A Feedback-Triggered Self-Correction with Long-Term Multipath Decoding](https://arxiv.org/abs/2509.07676)
*Jipeng Li,Zeyu Gao,Yubin Qi,Hande Dong,Weijian Chen,Qiang Lin*

Main category: cs.AI

TL;DR: 提出了FTR框架，结合用户反馈和增强解码机制，通过负反馈触发再生和长时多路径解码来解决LLM自校正的局限性


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在推理时容易生成错误内容，现有自校正方法存在两个根本问题：缺乏可靠的错误定位信号，以及传统next-token解码范式限制了推理深度

Method: Feedback-Triggered Regeneration (FTR)框架：仅在收到用户负反馈时激活响应再生，避免错误自评估的传播；Long-Term Multipath (LTM)解码：通过延迟序列评估实现多推理路径的系统探索

Result: 在数学推理和代码生成基准测试中，该框架相比最先进的基于提示的自校正方法取得了持续且显著的改进

Conclusion: FTR框架通过结合用户反馈和增强解码机制，有效解决了LLM自校正的关键挑战，在保持原有正确输出的同时显著提升了性能

Abstract: Large Language Models (LLMs) have achieved remarkable performance across
diverse tasks, yet their susceptibility to generating incorrect content during
inference remains a critical unsolved challenge. While self-correction methods
offer potential solutions, their effectiveness is hindered by two inherent
limitations: (1) the absence of reliable guidance signals for error
localization, and (2) the restricted reasoning depth imposed by conventional
next-token decoding paradigms. To address these issues, we propose
Feedback-Triggered Regeneration (FTR), a novel framework that synergizes user
feedback with enhanced decoding dynamics. Specifically, FTR activates response
regeneration only upon receiving negative user feedback, thereby circumventing
error propagation from faulty self-assessment while preserving originally
correct outputs. Furthermore, we introduce Long-Term Multipath (LTM) decoding,
which enables systematic exploration of multiple reasoning trajectories through
delayed sequence evaluation, effectively overcoming the myopic decision-making
characteristic of standard next-token prediction. Extensive experiments on
mathematical reasoning and code generation benchmarks demonstrate that our
framework achieves consistent and significant improvements over
state-of-the-art prompt-based self-correction methods.

</details>


### [51] [FHIR-RAG-MEDS: Integrating HL7 FHIR with Retrieval-Augmented Large Language Models for Enhanced Medical Decision Support](https://arxiv.org/abs/2509.07706)
*Yildiray Kabak,Gokce B. Laleci Erturkmen,Mert Gencturk,Tuncay Namli,A. Anil Sinaci,Ruben Alcantud Corcoles,Cristina Gomez Ballesteros,Pedro Abizanda,Asuman Dogac*

Main category: cs.AI

TL;DR: FHIR-RAG-MEDS系统整合HL7 FHIR与RAG技术，旨在提升基于循证临床指南的个性化医疗决策支持


<details>
  <summary>Details</summary>
Motivation: 医疗决策支持系统需要整合先进技术如RAG和HL7 FHIR来提升临床决策过程，但现有研究在实践应用方面的整合研究有限

Method: 提出FHIR-RAG-MEDS系统，将HL7 FHIR标准与检索增强生成(RAG)技术相结合

Result: 论文提出了系统框架，但未提供具体实验结果

Conclusion: 该研究强调了在实践应用中整合FHIR和RAG技术的重要性，为个性化医疗决策支持系统的发展提供了新方向

Abstract: In this study, we propose FHIR-RAG-MEDS system that aims to integrate Health
Level 7 Fast Healthcare Interoperability Resources (HL7 FHIR) with a
Retrieval-Augmented Generation (RAG)-based system to improve personalized
medical decision support on evidence-based clinical guidelines, emphasizing the
need for research in practical applications. In the evolving landscape of
medical decision support systems, integrating advanced technologies such as RAG
and HL7 FHIR can significantly enhance clinical decision-making processes.
Despite the potential of these technologies, there is limited research on their
integration in practical applications.

</details>


### [52] [VISION: Robust and Interpretable Code Vulnerability Detection Leveraging Counterfactual Augmentation](https://arxiv.org/abs/2508.18933)
*David Egea,Barproda Halder,Sanghamitra Dutta*

Main category: cs.AI

TL;DR: VISION框架通过生成反事实样本和针对性GNN训练，有效减少漏洞检测中的伪相关性学习，显著提升检测准确性和泛化能力


<details>
  <summary>Details</summary>
Motivation: 解决GNN在源代码漏洞检测中因训练数据不平衡和标签噪声导致的伪相关性学习问题，提高检测器的鲁棒性和泛化性

Method: 使用LLM生成反事实样本，进行针对性GNN训练，并结合图解释性技术识别关键代码语句

Result: 在CWE-20漏洞检测中，整体准确率从51.8%提升至97.8%，配对对比准确率从4.5%提升至95.8%，最差组准确率从0.7%提升至85.5%

Conclusion: VISION框架通过反事实训练有效缓解伪相关性学习，提升了漏洞检测的鲁棒性、可解释性和泛化能力，为AI驱动的网络安全系统提供了透明可信的解决方案

Abstract: Automated detection of vulnerabilities in source code is an essential
cybersecurity challenge, underpinning trust in digital systems and services.
Graph Neural Networks (GNNs) have emerged as a promising approach as they can
learn structural and logical code relationships in a data-driven manner.
However, their performance is severely constrained by training data imbalances
and label noise. GNNs often learn 'spurious' correlations from superficial code
similarities, producing detectors that fail to generalize well to unseen
real-world data. In this work, we propose a unified framework for robust and
interpretable vulnerability detection, called VISION, to mitigate spurious
correlations by systematically augmenting a counterfactual training dataset.
Counterfactuals are samples with minimal semantic modifications but opposite
labels. Our framework includes: (i) generating counterfactuals by prompting a
Large Language Model (LLM); (ii) targeted GNN training on paired code examples
with opposite labels; and (iii) graph-based interpretability to identify the
crucial code statements relevant for vulnerability predictions while ignoring
spurious ones. We find that VISION reduces spurious learning and enables more
robust, generalizable detection, improving overall accuracy (from 51.8% to
97.8%), pairwise contrast accuracy (from 4.5% to 95.8%), and worst-group
accuracy (from 0.7% to 85.5%) on the Common Weakness Enumeration (CWE)-20
vulnerability. We further demonstrate gains using proposed metrics: intra-class
attribution variance, inter-class attribution distance, and node score
dependency. We also release CWE-20-CFA, a benchmark of 27,556 functions (real
and counterfactual) from the high-impact CWE-20 category. Finally, VISION
advances transparent and trustworthy AI-based cybersecurity systems through
interactive visualization for human-in-the-loop analysis.

</details>


### [53] [RIMO: An Easy-to-Evaluate, Hard-to-Solve Olympiad Benchmark for Advanced Mathematical Reasoning](https://arxiv.org/abs/2509.07711)
*Ziye Chen,Chengwei Qin,Yao Shu*

Main category: cs.AI

TL;DR: RIMO是一个新的数学奥林匹克竞赛基准测试，包含两个轨道：RIMO-N（335个整数答案问题）和RIMO-P（456个证明问题），旨在消除评估噪声并提供精确的性能测量。


<details>
  <summary>Details</summary>
Motivation: 现有奥林匹克级基准测试存在评分噪声和潜在偏见，如异构答案格式需要模型判断、依赖可能有缺陷的解决方案等，需要更精确的评估工具。

Method: 创建两个轨道：RIMO-N重写IMO问题为单一整数答案，支持确定性正确性检查；RIMO-P提供专家检查的证明问题，分解为子问题序列，通过自动化评分系统评估逐步推理过程。

Result: 测试10个前沿LLM（包括GPT-4o和Gemini 2.5 Flash）显示，虽然这些系统在旧基准上表现优异，但在RIMO上性能急剧下降。

Conclusion: RIMO揭示了当前LLM能力与实际奥林匹克级推理之间的巨大差距，为未来研究提供了具有挑战性且易于评估的基准套件。

Abstract: As large language models (LLMs) reach high scores on established mathematical
benchmarks, such as GSM8K and MATH, the research community has turned to
International Mathematical Olympiad (IMO) problems to push the evaluation
frontier. However, existing Olympiad-level benchmarks suffer from practical
constraints that introduce grading noise and potential bias, such as
heterogeneous answer formats requiring model-based judges and a reliance on
potentially flawed solutions. We introduce RIMO, a two-track benchmark designed
to preserve peak Olympiad difficulty while eliminating this evaluation noise.
The first track, RIMO-N, rewrites 335 IMO problems to admit a single, unique
integer answer, allowing for deterministic correctness checking. The second
track, RIMO-P, features 456 proof problems with expert-checked solutions, which
are decomposed into a sequence of sub-problems to evaluate the step-by-step
reasoning process via an automated grading system. Our benchmarking of ten
frontier LLMs, including GPT-4o and Gemini 2.5 Flash, reveals that while these
systems excel on older benchmarks, their performance drops sharply on RIMO.
These results highlight a substantial gap between current LLM capabilities and
actual Olympiad-level reasoning. By providing a challenging yet
easy-to-evaluate suite, RIMO offers a high-resolution yardstick for future
research, presenting a clear target for closing the profound reasoning gap our
findings expose.

</details>


### [54] [BDPM: A Machine Learning-Based Feature Extractor for Parkinson's Disease Classification via Gut Microbiota Analysis](https://arxiv.org/abs/2509.07723)
*Bo Yu,Zhixiu Hua,Bo Zhao*

Main category: cs.AI

TL;DR: 基于肠道微生物群的帕金森病预测模型BDPM，通过RFRE特征选择和混合分类器提高准确性


<details>
  <summary>Details</summary>
Motivation: 帕金森病诊断准确性低，传统临床评分量表存在限制。肠道微生物群作为新兴生物标记物展现出潜力，但现有深度学习模型忽视了菌校间相关性和时间动态

Method: 1）收集39名帕金森患者和健康配偶的肠道微生物群数据 2）开发RFRE特征选择框架（随机森林结合递归特征消除），融入生态学知识 3）设计混合分类模型捕捉微生物群数据的时空模式

Result: 未在摘要中明确提及，需要查看完整论文获得详细结果

Conclusion: BDPM框架通过结合生物学知识的特征选择和混合分类方法，为帕金森病的早期诊断提供了更稳健的微生物群基图尺度

Abstract: Background: Parkinson's disease remains a major neurodegenerative disorder
with high misdiagnosis rates, primarily due to reliance on clinical rating
scales. Recent studies have demonstrated a strong association between gut
microbiota and Parkinson's disease, suggesting that microbial composition may
serve as a promising biomarker. Although deep learning models based ongut
microbiota show potential for early prediction, most approaches rely on single
classifiers and often overlook inter-strain correlations or temporal dynamics.
Therefore, there is an urgent need for more robust feature extraction methods
tailored to microbiome data. Methods: We proposed BDPM (A Machine
Learning-Based Feature Extractor for Parkinson's Disease Classification via Gut
Microbiota Analysis). First, we collected gut microbiota profiles from 39
Parkinson's patients and their healthy spouses to identify differentially
abundant taxa. Second, we developed an innovative feature selection framework
named RFRE (Random Forest combined with Recursive Feature Elimination),
integrating ecological knowledge to enhance biological interpretability.
Finally, we designed a hybrid classification model to capture temporal and
spatial patterns in microbiome data.

</details>


### [55] [The Carbon Footprint Wizard: A Knowledge-Augmented AI Interface for Streamlining Food Carbon Footprint Analysis](https://arxiv.org/abs/2509.07733)
*Mustafa Kaan Aslan,Reinout Heijungs,Filip Ilievski*

Main category: cs.AI

TL;DR: 一种结合知识增强AI技术的方法论，通过聊天机器人界面估算食品产品的碳踹迹，提高LCA评估的可访问性


<details>
  <summary>Details</summary>
Motivation: 环境可持续性和气候变化是重要关注点，但生命周期评估(LCA)复杂且数据碎片化，需要更可访的碳踹迹估算方法

Method: 结合LCA进展和公开数据库，使用知识增强AI技术（包括检索增强生成）来估算食品产品从原料到出厂的碳踹迹，提供交互式聊天机器介面

Result: 开发了原型系统，能够处理任意食品项目和进一步询问，展示了在提供可访LCA见解方面的潜力和限制

Conclusion: 该方法能够以可访格式提供LCA见解，但仍面临数据库不确定性和AI误解等挑战，为环境可持续性评估开启了新方向

Abstract: Environmental sustainability, particularly in relation to climate change, is
a key concern for consumers, producers, and policymakers. The carbon footprint,
based on greenhouse gas emissions, is a standard metric for quantifying the
contribution to climate change of activities and is often assessed using life
cycle assessment (LCA). However, conducting LCA is complex due to opaque and
global supply chains, as well as fragmented data. This paper presents a
methodology that combines advances in LCA and publicly available databases with
knowledge-augmented AI techniques, including retrieval-augmented generation, to
estimate cradle-to-gate carbon footprints of food products. We introduce a
chatbot interface that allows users to interactively explore the carbon impact
of composite meals and relate the results to familiar activities. A live web
demonstration showcases our proof-of-concept system with arbitrary food items
and follow-up questions, highlighting both the potential and limitations - such
as database uncertainties and AI misinterpretations - of delivering LCA
insights in an accessible format.

</details>


### [56] [Certainty-Guided Reasoning in Large Language Models: A Dynamic Thinking Budget Approach](https://arxiv.org/abs/2509.07820)
*João Paulo Nogueira,Wentao Sun,Alonso Silva,Laith Zumot*

Main category: cs.AI

TL;DR: 提出Certainty-Guided Reasoning (CGR)方法，通过批评模型自我评估推理置信度来动态调整推理过程，在保证准确性的同时显著减少计算资源消耗。


<details>
  <summary>Details</summary>
Motivation: 大型推理语言模型(LRLMs)在解决复杂任务时存在固定推理预算的问题，无法根据任务难度自适应调整推理深度，导致要么资源浪费要么准确性不足。

Method: 受GAN生成器/判别器框架启发，使用批评模型定期评估自身推理置信度，达到目标置信阈值时终止推理，否则继续推理，实现效率与可靠性的自适应平衡。

Result: 在AIME2024和AIME2025数据集上，CGR提高了基线准确性并减少token使用量；64次多种子实验显示稳定性提升，方差减少；可节省数百万token，在准确性和效率间提供可调节权衡。

Conclusion: 置信度是推理充分性的有力信号，CGR通过将置信度整合到推理过程中，使大型推理语言模型更加自适应、可信赖和资源高效，为实际部署铺平道路。

Abstract: The rise of large reasoning language models (LRLMs) has unlocked new
potential for solving complex tasks. These models operate with a thinking
budget, that is, a predefined number of reasoning tokens used to arrive at a
solution. We propose a novel approach, inspired by the generator/discriminator
framework in generative adversarial networks, in which a critic model
periodically probes its own reasoning to assess whether it has reached a
confident conclusion. If not, reasoning continues until a target certainty
threshold is met. This mechanism adaptively balances efficiency and reliability
by allowing early termination when confidence is high, while encouraging
further reasoning when uncertainty persists. Through experiments on the
AIME2024 and AIME2025 datasets, we show that Certainty-Guided Reasoning (CGR)
improves baseline accuracy while reducing token usage. Importantly, extended
multi-seed evaluations over 64 runs demonstrate that CGR is stable, reducing
variance across seeds and improving exam-like performance under penalty-based
grading. Additionally, our token savings analysis shows that CGR can eliminate
millions of tokens in aggregate, with tunable trade-offs between certainty
thresholds and efficiency. Together, these findings highlight certainty as a
powerful signal for reasoning sufficiency. By integrating confidence into the
reasoning process, CGR makes large reasoning language models more adaptive,
trustworthy, and resource efficient, paving the way for practical deployment in
domains where both accuracy and computational cost matter.

</details>


### [57] [Aligning LLMs for the Classroom with Knowledge-Based Retrieval -- A Comparative RAG Study](https://arxiv.org/abs/2509.07846)
*Amay Jain,Liu Cui,Si Chen*

Main category: cs.AI

TL;DR: 研究比较了矩阵基和图基RAG在教育问答中的性能，发现矩阵RAG适合低成本事实查找，图基RAG更适合主题性问题，建议采用动态分支框架优化效果


<details>
  <summary>Details</summary>
Motivation: 解决ChatGPT等大语言模型在教室中提供过时或虚构信息的问题，通过RAG技术基于外部资源提高回答的可靠性

Method: 使用EduScopeQA数据集(3,176个问题)，比较矩阵基检索RAG和图基RAG在不同教育学科、问题类型下的表现，包括使用系统改动的教科书评估系统对投影知识的对齐性

Result: OpenAI矩阵检索RAG在低成本事实查找中表现优异；GraphRAG Global在主题性查询中提供丰富的教育回答；GraphRAG Local在语料库完整性关键时准确率最高，但资源消耗高10-20倍

Conclusion: 动态分支框架能够根据查询类型路由到最优检索方法，显著提升信度和效率，为教育工作者集成RAG增强的LLM提供了可行指南

Abstract: Large language models like ChatGPT are increasingly used in classrooms, but
they often provide outdated or fabricated information that can mislead
students. Retrieval Augmented Generation (RAG) improves reliability of LLMs by
grounding responses in external resources. We investigate two accessible RAG
paradigms, vector-based retrieval and graph-based retrieval to identify best
practices for classroom question answering (QA). Existing comparative studies
fail to account for pedagogical factors such as educational disciplines,
question types, and practical deployment costs. Using a novel dataset,
EduScopeQA, of 3,176 questions across academic subjects, we measure performance
on various educational query types, from specific facts to broad thematic
discussions. We also evaluate system alignment with a dataset of systematically
altered textbooks that contradict the LLM's latent knowledge. We find that
OpenAI Vector Search RAG (representing vector-based RAG) performs well as a
low-cost generalist, especially for quick fact retrieval. On the other hand,
GraphRAG Global excels at providing pedagogically rich answers to thematic
queries, and GraphRAG Local achieves the highest accuracy with the dense,
altered textbooks when corpus integrity is critical. Accounting for the 10-20x
higher resource usage of GraphRAG (representing graph-based RAG), we show that
a dynamic branching framework that routes queries to the optimal retrieval
method boosts fidelity and efficiency. These insights provide actionable
guidelines for educators and system designers to integrate RAG-augmented LLMs
into learning environments effectively.

</details>


### [58] [SCoder: Iterative Self-Distillation for Bootstrapping Small-Scale Data Synthesizers to Empower Code LLMs](https://arxiv.org/abs/2509.07858)
*Xinyu Zhang,Changzhi Zhou,Linmei Hu,Luhao Zhang,Xiancai Chen,Haomin Fu,Yang Yang,Mengdi Zhang*

Main category: cs.AI

TL;DR: 通过迭代自我精细抄筛技术，使用7B小规模开源LLM能够生成高质量代码指令数据，降低对专有大模型的依赖和成本，基于此训练的SCoder模型在代码生成任务上达到最先进水平。


<details>
  <summary>Details</summary>
Motivation: 现有代码大语言模型依赖于从专有LLM精细的大规模指令数据，成本较高。为了降低成本和减少对专有模型的依赖，需要开发一种新方法来使用小规模开源LLM生成高质量代码指令数据。

Method: 提出迭代自我精细抄筛方法：1)通过训练少量优质样本提升小模型的数据生成能力；2)设计多检查点采样和多方面评分策略获取多样化高质量数据；3)使用梯度基础的影响估计方法进行最终数据筛选。

Result: 基于小规模合成器生成的代码指令数据集训练的SCoder代码生成模型家族，在代码生成能力方面达到了最先进水平。

Conclusion: 该方法成功地将小规模开源LLM转化为强大的数据合成器，显著降低了对专有大模型的依赖和数据生成成本，同时保证了生成数据的高质量。

Abstract: Existing code large language models (LLMs) often rely on large-scale
instruction data distilled from proprietary LLMs for fine-tuning, which
typically incurs high costs. In this paper, we explore the potential of
small-scale open-source LLMs (e.g., 7B) as synthesizers for high-quality code
instruction data construction. We first observe that the data synthesis
capability of small-scale LLMs can be enhanced by training on a few superior
data synthesis samples from proprietary LLMs. Building on this, we propose a
novel iterative self-distillation approach to bootstrap small-scale LLMs,
transforming them into powerful synthesizers that reduce reliance on
proprietary LLMs and minimize costs. Concretely, in each iteration, to obtain
diverse and high-quality self-distilled data, we design multi-checkpoint
sampling and multi-aspect scoring strategies for initial data selection.
Furthermore, to identify the most influential samples, we introduce a
gradient-based influence estimation method for final data filtering. Based on
the code instruction datasets from the small-scale synthesizers, we develop
SCoder, a family of code generation models fine-tuned from DeepSeek-Coder.
SCoder models achieve state-of-the-art code generation capabilities,
demonstrating the effectiveness of our method.

</details>


### [59] [CP-Model-Zoo: A Natural Language Query System for Constraint Programming Models](https://arxiv.org/abs/2509.07867)
*Augustin Crespin,Ioannis Kostis,Hélène Verhaeghe,Pierre Schaus*

Main category: cs.AI

TL;DR: CP-Model-Zoo是一个基于专家编写模型的检索系统，通过自然语言描述从数据库中匹配最接近的组合问题源代码模型，帮助非专家用户使用约束编程。


<details>
  <summary>Details</summary>
Motivation: 约束编程及其高级建模语言虽然强大，但建模语言复杂、全局约束众多以及建模技巧要求高，阻碍了非专家用户使用CP解决组合问题。

Method: 利用多年积累的专家编写模型构建数据库，通过用户自然语言描述检索最接近的源代码模型，无需人工数据标注。

Result: 实验显示系统在不同专业水平的用户问题描述下，都能以优秀准确率检索到正确的模型。

Conclusion: CP-Model-Zoo通过利用现有专家模型库，有效降低了非专家用户使用约束编程的门槛，实现了专家验证模型的自动化检索。

Abstract: Constraint Programming and its high-level modeling languages have long been
recognized for their potential to achieve the holy grail of problem-solving.
However, the complexity of modeling languages, the large number of global
constraints, and the art of creating good models have often hindered
non-experts from choosing CP to solve their combinatorial problems. While
generating an expert-level model from a natural-language description of a
problem would be the dream, we are not yet there. We propose a tutoring system
called CP-Model-Zoo, exploiting expert-written models accumulated through the
years. CP-Model-Zoo retrieves the closest source code model from a database
based on a user's natural language description of a combinatorial problem. It
ensures that expert-validated models are presented to the user while
eliminating the need for human data labeling. Our experiments show excellent
accuracy in retrieving the correct model based on a user-input description of a
problem simulated with different levels of expertise.

</details>


### [60] [HiPhO: How Far Are (M)LLMs from Humans in the Latest High School Physics Olympiad Benchmark?](https://arxiv.org/abs/2509.07894)
*Fangchen Yu,Haiyuan Wan,Qianjia Cheng,Yuchen Zhang,Jiacheng Chen,Fujun Han,Yulun Wu,Junchi Yao,Ruilizhen Hu,Ning Ding,Yu Cheng,Tao Chen,Lei Bai,Dongzhan Zhou,Yun Luo,Ganqu Cui,Peng Ye*

Main category: cs.AI

TL;DR: HiPhO是首个专注于高中物理奥林匹克竞赛的基准测试，提供人类对齐评估，包含13个最新竞赛试题、专业评分标准和与人类选手的直接比较。


<details>
  <summary>Details</summary>
Motivation: 现有物理基准测试缺乏对真实物理竞赛（如物理奥林匹克）的系统性覆盖，且无法实现与人类选手的直接性能比较。

Method: 收集2024-2025年13个国际和地区奥林匹克竞赛试题，采用官方评分标准进行细粒度评分，基于奖牌阈值将模型与人类选手对比。

Result: 开源MLLM大多处于铜牌水平；开源LLM偶尔获得金牌；闭源推理MLLM可获得6-12枚金牌；所有模型距离满分仍有显著差距。

Conclusion: 开源模型与顶尖学生存在显著性能差距，闭源推理模型展现强大物理推理能力，物理推理仍有很大改进空间。

Abstract: Recently, the physical capabilities of (M)LLMs have garnered increasing
attention. However, existing benchmarks for physics suffer from two major gaps:
they neither provide systematic and up-to-date coverage of real-world physics
competitions such as physics Olympiads, nor enable direct performance
comparison with humans. To bridge these gaps, we present HiPhO, the first
benchmark dedicated to high school physics Olympiads with human-aligned
evaluation. Specifically, HiPhO highlights three key innovations. (1)
Comprehensive Data: It compiles 13 latest Olympiad exams from 2024-2025,
spanning both international and regional competitions, and covering mixed
modalities that encompass problems spanning text-only to diagram-based. (2)
Professional Evaluation: We adopt official marking schemes to perform
fine-grained grading at both the answer and step level, fully aligned with
human examiners to ensure high-quality and domain-specific evaluation. (3)
Comparison with Human Contestants: We assign gold, silver, and bronze medals to
models based on official medal thresholds, thereby enabling direct comparison
between (M)LLMs and human contestants. Our large-scale evaluation of 30
state-of-the-art (M)LLMs shows that: across 13 exams, open-source MLLMs mostly
remain at or below the bronze level; open-source LLMs show promising progress
with occasional golds; closed-source reasoning MLLMs can achieve 6 to 12 gold
medals; and most models still have a significant gap from full marks. These
results highlight a substantial performance gap between open-source models and
top students, the strong physical reasoning capabilities of closed-source
reasoning models, and the fact that there is still significant room for
improvement. HiPhO, as a rigorous, human-aligned, and Olympiad-focused
benchmark for advancing multimodal physical reasoning, is open-source and
available at https://github.com/SciYu/HiPhO.

</details>


### [61] [Probing the Preferences of a Language Model: Integrating Verbal and Behavioral Tests of AI Welfare](https://arxiv.org/abs/2509.07961)
*Valen Tagliabue,Leonard Dung*

Main category: cs.AI

TL;DR: 这篇论文开发了测量语言模型福利的实验方法，通过比较言语报告和行为表现来验证偏好满足度作为福利代理指标的可行性。


<details>
  <summary>Details</summary>
Motivation: 研究者强调需要开发实验性方法来测量AI系统的福利状态，以便更好地理解和保障这些系统的健康状况。

Method: 采用多种实验范式：比较模型的言语偏好报告与虚拟环境中的行为选择；测试成本和奖励对行为的影响；验证福利量表在语义等价提示下的一致性。

Result: 观察到说明的偏好与行为之间存在可靠相关关系，支持偏好满足度可作为AI系统福利的实验性测量代理。但不同模型和条件下一致性程度有差异，并且对提示变动敏感。

Conclusion: 虽然对语言模型是否具有福利主体身份仍存在不确定性，但研究结果显示了在语言模型中测量福利的可行性，为进一步探索提供了基础。

Abstract: We develop new experimental paradigms for measuring welfare in language
models. We compare verbal reports of models about their preferences with
preferences expressed through behavior when navigating a virtual environment
and selecting conversation topics. We also test how costs and rewards affect
behavior and whether responses to an eudaimonic welfare scale - measuring
states such as autonomy and purpose in life - are consistent across
semantically equivalent prompts. Overall, we observed a notable degree of
mutual support between our measures. The reliable correlations observed between
stated preferences and behavior across conditions suggest that preference
satisfaction can, in principle, serve as an empirically measurable welfare
proxy in some of today's AI systems. Furthermore, our design offered an
illuminating setting for qualitative observation of model behavior. Yet, the
consistency between measures was more pronounced in some models and conditions
than others and responses were not consistent across perturbations. Due to
this, and the background uncertainty about the nature of welfare and the
cognitive states (and welfare subjecthood) of language models, we are currently
uncertain whether our methods successfully measure the welfare state of
language models. Nevertheless, these findings highlight the feasibility of
welfare measurement in language models, inviting further exploration.

</details>
