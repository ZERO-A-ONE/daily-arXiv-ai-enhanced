<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 18]
- [cs.AI](#cs.AI) [Total: 31]
- [cs.CR](#cs.CR) [Total: 18]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Technical knowledge and soft skills in software startups within the Colombian entrepreneurial ecosystem](https://arxiv.org/abs/2511.21769)
*Royer David Estrada-Esponda,Gerardo Matturro,Jose Reinaldo Sabogal-Pinilla*

Main category: cs.SE

TL;DR: 该研究调查了哥伦比亚软件创业公司创始团队最看重的技术知识和软技能，以及这些需求如何随公司成长而变化。


<details>
  <summary>Details</summary>
Motivation: 创业团队成员的技术知识和软技能对软件初创企业的早期阶段有重要影响，创业成功与否很大程度上取决于创始团队成员的素质，因此需要了解哪些知识和技能最受重视。

Method: 在哥伦比亚创业生态系统内进行调查研究，通过对软件创业公司代表进行问卷调查，收集数据并分析。

Result: 调查显示最受重视的技术知识包括：需求工程、软件测试、项目规划与管理、敏捷方法、市场营销、商业模式定义和预算编制；最受重视的软技能包括：沟通能力、领导力和团队合作。

Conclusion: 该研究结果对软件创业者、孵化器和研究人员具有参考价值，揭示了创业团队在不同发展阶段对知识和技能的需求变化。

Abstract: The technical knowledge and soft skills of entrepreneurial team members significantly impact the early stages of software startups. It is widely recognized that the success or failure of a startup is determined by the quality of the individuals who constitute the founding team. This article presents the findings of a study conducted within the Colombian entrepreneurial ecosystem, focusing on which technical knowledge and soft skills are the most valued by founding teams of software startups, and how the needs for knowledge and skills evolve as the startup grows. A survey of software startup representatives revealed that the most valued knowledge includes requirements engineering, software testing, project planning and management, agile methodologies, marketing, business model definition, and budgeting. The most valued soft skills are typically communication, leadership, and teamwork. The outcomes of this work are relevant to software entrepreneurs, incubators, and researchers.

</details>


### [2] [Code Refactoring with LLM: A Comprehensive Evaluation With Few-Shot Settings](https://arxiv.org/abs/2511.21788)
*Md. Raihan Tapader,Md. Mostafizer Rahman,Ariful Islam Shiplu,Md Faizul Ibne Amin,Yutaka Watanobe*

Main category: cs.SE

TL;DR: 本文提出基于大语言模型的多语言代码重构框架，通过提示工程和指令微调优化重构效果，在Java、Python等多种编程语言上实现高质量代码重构。


<details>
  <summary>Details</summary>
Motivation: 当前代码重构方法存在局限性，难以跨多种编程语言和编码风格进行泛化，主要依赖手动编写的转换规则。随着程序员关注点转向编写简单、清晰、高效、可持续的代码，需要更智能的代码重构解决方案。

Method: 提出基于大语言模型的框架，结合提示工程（温度参数、不同few-shot算法）和指令微调，采用few-shot学习方法进行多语言代码重构。支持C、C++、C#、Python、Java等多种编程语言。

Result: Java在10-shot设置下达到最高正确率99.99%，平均可编译性94.78%，保持高相似度（约53-54%），在结构修改和语义保留之间实现良好平衡。Python在所有shot设置下表现出最低的结构距离（约277-294），相似度适中（约44-48%），表明重构过程一致且干扰最小。

Conclusion: 基于大语言模型的代码重构框架在多语言环境下表现优异，提示工程和few-shot学习能有效提升重构质量。该方法为跨语言代码重构提供了可行的解决方案，平衡了代码结构优化和语义保留的需求。

Abstract: In today's world, the focus of programmers has shifted from writing complex, error-prone code to prioritizing simple, clear, efficient, and sustainable code that makes programs easier to understand. Code refactoring plays a critical role in this transition by improving structural organization and optimizing performance. However, existing refactoring methods are limited in their ability to generalize across multiple programming languages and coding styles, as they often rely on manually crafted transformation rules. The objectives of this study are to (i) develop an Large Language Models (LLMs)-based framework capable of performing accurate and efficient code refactoring across multiple languages (C, C++, C#, Python, Java), (ii) investigate the impact of prompt engineering (Temperature, Different shot algorithm) and instruction fine-tuning on refactoring effectiveness, and (iii) evaluate the quality improvements (Compilability, Correctness, Distance, Similarity, Number of Lines, Token, Character, Cyclomatic Complexity) in refactored code through empirical metrics and human assessment. To accomplish these goals, we propose a fine-tuned prompt-engineering-based model combined with few-shot learning for multilingual code refactoring. Experimental results indicate that Java achieves the highest overall correctness up to 99.99% the 10-shot setting, records the highest average compilability of 94.78% compared to the original source code and maintains high similarity (Approx. 53-54%) and thus demonstrates a strong balance between structural modifications and semantic preservation. Python exhibits the lowest structural distance across all shots (Approx. 277-294) while achieving moderate similarity ( Approx. 44-48%) that indicates consistent and minimally disruptive refactoring.

</details>


### [3] [LLM-Empowered Event-Chain Driven Code Generation for ADAS in SDV systems](https://arxiv.org/abs/2511.21877)
*Nenad Petrovic,Norbert Kroth,Axel Torschmied,Yinglei Song,Fengjunjie Pan,Vahid Zolfaghari,Nils Purschke,Sven Kirchner,Chengdong Wu,Andre Schamschurko,Yi Zhang,Alois Knoll*

Main category: cs.SE

TL;DR: 本文提出了一种基于事件链驱动、LLM赋能的工作流程，用于从自然语言需求生成经过验证的汽车代码。通过RAG层检索相关信号，映射验证后转换为事件链，指导LLM生成代码，无需重新训练即可实现有效信号使用和一致代码生成。


<details>
  <summary>Details</summary>
Motivation: 汽车软件开发面临从自然语言需求生成正确代码的挑战，特别是处理大型且不断演化的车辆信号规范（VSS）目录时，容易出现幻觉和架构错误。需要一种方法确保生成的代码符合信号规范、行为一致且实时可行。

Method: 1. 使用检索增强生成（RAG）层从VSS目录中检索相关信号作为代码生成提示上下文；2. 对检索到的信号进行映射和验证；3. 将信号转换为编码因果和时序约束的事件链；4. 用事件链指导和约束基于LLM的代码合成。

Result: 通过紧急制动案例研究，该方法成功实现了有效的信号使用和一致的代码生成，无需对LLM进行重新训练，减少了幻觉并确保了架构正确性。

Conclusion: 事件链驱动、LLM赋能的工作流程能够从自然语言需求生成经过验证的汽车代码，通过RAG减少幻觉，通过事件链确保行为一致性和实时可行性，为汽车软件开发提供了有效解决方案。

Abstract: This paper presents an event-chain-driven, LLM-empowered workflow for generating validated, automotive code from natural-language requirements. A Retrieval-Augmented Generation (RAG) layer retrieves relevant signals from large and evolving Vehicle Signal Specification (VSS) catalogs as code generation prompt context, reducing hallucinations and ensuring architectural correctness. Retrieved signals are mapped and validated before being transformed into event chains that encode causal and timing constraints. These event chains guide and constrain LLM-based code synthesis, ensuring behavioral consistency and real-time feasibility. Based on our initial findings from the emergency braking case study, with the proposed approach, we managed to achieve valid signal usage and consistent code generation without LLM retraining.

</details>


### [4] [Toward Automated and Trustworthy Scientific Analysis and Visualization with LLM-Generated Code](https://arxiv.org/abs/2511.21920)
*Apu Kumar Chakroborti,Yi Ding,Lipeng Wan*

Main category: cs.SE

TL;DR: 评估开源大语言模型在科学数据分析和可视化任务中生成Python代码的可靠性，发现无人工干预时可靠性有限，提出了三种改进策略并构建了可复用的基准测试


<details>
  <summary>Details</summary>
Motivation: 随着科学日益数据密集，分析和可视化大规模复杂数据集对加速发现至关重要。但许多领域科学家缺乏开发自定义数据分析工作流的编程专业知识，这阻碍了及时有效的洞察。大语言模型通过从自然语言描述生成可执行代码提供了有前景的解决方案。

Method: 构建反映真实世界研究任务的领域启发式提示基准套件，系统评估生成代码的可执行性和正确性。为解决挑战，设计并评估了三种互补策略：数据感知提示消歧、检索增强提示改进和迭代错误修复。

Result: 研究发现，在没有人工干预的情况下，LLM生成代码的可靠性有限，经常因模糊提示和模型对领域特定上下文理解不足而失败。提出的三种方法显著提高了执行成功率和输出质量，但仍需进一步改进。

Conclusion: 这项工作突出了LLM驱动自动化在科学工作流程中的前景和当前局限性，并引入了可操作的技术和可重复使用的基准，用于构建更具包容性、可访问性和可信赖的AI辅助研究工具。

Abstract: As modern science becomes increasingly data-intensive, the ability to analyze and visualize large-scale, complex datasets is critical to accelerating discovery. However, many domain scientists lack the programming expertise required to develop custom data analysis workflows, creating barriers to timely and effective insight. Large language models (LLMs) offer a promising solution by generating executable code from natural language descriptions. In this paper, we investigate the trustworthiness of open-source LLMs in autonomously producing Python scripts for scientific data analysis and visualization. We construct a benchmark suite of domain-inspired prompts that reflect real-world research tasks and systematically evaluate the executability and correctness of the generated code. Our findings show that, without human intervention, the reliability of LLM-generated code is limited, with frequent failures caused by ambiguous prompts and the models' insufficient understanding of domain-specific contexts. To address these challenges, we design and assess three complementary strategies: data-aware prompt disambiguation, retrieval-augmented prompt enhancement, and iterative error repair. While these methods significantly improve execution success rates and output quality, further refinement is needed. This work highlights both the promise and current limitations of LLM-driven automation in scientific workflows and introduces actionable techniques and a reusable benchmark for building more inclusive, accessible, and trustworthy AI-assisted research tools.

</details>


### [5] [Beyond Like-for-Like: A User-centered Approach to Modernizing Legacy Applications](https://arxiv.org/abs/2511.21956)
*M. Polzin,M. Guzman*

Main category: cs.SE

TL;DR: 论文探讨在现代化遗留应用程序时，不应简单进行工具和样式替换，而应通过用户参与创造更直观、支持任务效率的应用


<details>
  <summary>Details</summary>
Motivation: 遗留应用程序现代化过程中，开发者容易陷入简单复制的陷阱，但这是改进用户体验、解决现有问题的机会。虽然专家用户熟悉现有系统，但直接复制会延续低效流程和痛点

Method: 通过用户参与的方法，在现代化过程中平衡"一对一复制"和引入新GUI设计。利用现有应用程序作为参考工具，识别改进机会，而不是从零开始

Result: 用户参与能够弥合简单复制和全新设计之间的差距，帮助创建既满足专家用户需求又解决现有问题的现代化应用。现有遗留应用为改进提供了宝贵洞察

Conclusion: 遗留应用程序现代化不应是简单复制，而应通过用户参与创造更直观的应用。现有系统是宝贵的参考资源，能够指导开发出既保留专家用户熟悉性又解决效率问题的现代化应用

Abstract: When modernizing a legacy application, it is easy to fall back on a like-for-like replica with new tools and updated design stylings, but this is an opportunity to explore making a more intuitive application that supports user tasks and efficiency. Rather than having a blank canvas-unburdened by legacy tech debt-to create a new application, you are working with an existing application that is integral to accelerator operations and one that expert users are already familiar with. Due to this, you might assume people will prefer the like-for-like, but you could be carrying forward the pain points, processes that are inefficient, and ultimately wind up with an application that no one wants to use because it doesn't solve existing problems. Getting users involved can make all the difference in your approach to modernizing a legacy application that caters to both newer and expert users. It also can bridge the gap between like-for-like and introducing new GUI design. Having a legacy application doesn't have to make the modernized one difficult to develop, as the existing application is a tool in how you move forward with the new application. It provides insight into areas that an application with a clean slate doesn't give you.

</details>


### [6] [DRS-OSS: LLM-Driven Diff Risk Scoring Tool for PR Risk Prediction](https://arxiv.org/abs/2511.21964)
*Ali Sayedsalehi,Peter C. Rigby,Audris Mockus*

Main category: cs.SE

TL;DR: DRS-OSS是一个开源的风险评分系统，使用微调的Llama 3.1 8B模型分析代码变更风险，帮助开发者优先审查高风险提交，防止缺陷引入。


<details>
  <summary>Details</summary>
Motivation: 在大规模开源项目中，每天有大量代码提交，其中可能引入缺陷。需要一种方法来评估代码变更的风险，以便更好地进行代码审查优先级排序、测试规划和CI/CD门控。

Method: 使用微调的Llama 3.1 8B序列分类器，结合提交信息、结构化差异和变更指标的长上下文表示。通过参数高效适配、4位QLoRA和DeepSpeed ZeRO-3 CPU卸载技术，在单张20GB GPU上训练22k令牌的上下文。

Result: 在ApacheJIT基准测试中达到最先进性能（F1=0.64，ROC-AUC=0.89）。模拟显示，仅门控风险最高的30%提交就能防止86.4%的缺陷引入变更。

Conclusion: DRS-OSS提供了一个完整的开源解决方案，通过API、Web界面和GitHub插件集成到开发者工作流中，有效帮助识别高风险代码变更，提高软件质量。

Abstract: In large-scale open-source projects, hundreds of pull requests land daily, each a potential source of regressions. Diff Risk Scoring (DRS) estimates the likelihood that a diff will introduce a defect, enabling better review prioritization, test planning, and CI/CD gating. We present DRS-OSS, an open-source DRS system equipped with a public API, web UI, and GitHub plugin. DRS-OSS uses a fine-tuned Llama 3.1 8B sequence classifier trained on the ApacheJIT dataset, consuming long-context representations that combine commit messages, structured diffs, and change metrics. Through parameter-efficient adaptation, 4-bit QLoRA, and DeepSpeed ZeRO-3 CPU offloading, we train 22k-token contexts on a single 20 GB GPU. On the ApacheJIT benchmark, DRS-OSS achieves state-of-the-art performance (F1 = 0.64, ROC-AUC = 0.89). Simulations show that gating only the riskiest 30% of commits can prevent up to 86.4% of defect-inducing changes. The system integrates with developer workflows through an API gateway, a React dashboard, and a GitHub App that posts risk labels on pull requests. We release the full replication package, fine-tuning scripts, deployment artifacts, code, demo video, and public website.

</details>


### [7] [Statistical Independence Aware Caching for LLM Workflows](https://arxiv.org/abs/2511.22118)
*Yihan Dai,Dimitrios Stamatios Bouras,Haoxiang Jia,Sergey Mechtaev*

Main category: cs.SE

TL;DR: Mnimi是一个LLM缓存设计模式，通过封装统计约束在LLM引用类型中，确保组件级统计独立性，同时提高效率、可重复性和调试便利性。


<details>
  <summary>Details</summary>
Motivation: LLM推理成本高、延迟大，本地缓存可降低成本延迟，但现有缓存系统无法保证统计独立性，而统计独立性对概率工作流（如代码生成中的Pass@k指标、不确定性估计等）至关重要。

Method: 提出Mnimi缓存设计模式，核心创新是将统计约束封装在LLM引用类型中，允许用户根据算法范围和需求管理和转换这些类型。在Python中通过装饰器和无限序列迭代器实现。

Result: 在SpecFix（自动程序规范修复系统）案例研究中，Mnimi在保持统计正确性的同时，提高了可重复性、调试便利性、时间和成本效率。

Conclusion: Mnimi为LLM缓存提供了支持模块化工作流并确保统计完整性的设计模式，解决了现有缓存系统缺乏统计独立性约束的问题。

Abstract: Large language models (LLMs) inference is both expensive and slow. Local caching of responses offers a practical solution to reduce the cost and latency of LLM queries. In research contexts, caching also enhances reproducibility and provides flexibility for experimentation. However, naive reuse of cached responses compromises statistical independence, a critical property for probabilistic workflows. In applications of LLM for code, it underpins performance metrics such as Pass@k and uncertainty estimation, as well as algorithms like program repair loops and retries. Existing LLM caching systems lack ways to enforce statistical independence constraints. To address this, we introduce Mnimi, a cache design pattern that supports modular LLM workflows while ensuring statistical integrity at the component level. Its core innovation lies in encapsulating statistical constraints within the type of LLM references, allowing users to manage and transform these types according to the scope and requirements of their algorithm. We implemented this design pattern in Python using a combination of decorators and iterators over infinite sequences. A case study on SpecFix, an recent automated program specification repair system, highlights how Mnimi improves reproducibility, ease of debugging, time and cost efficiency while preserving statistical correctness.

</details>


### [8] [Exploring the SECURITY.md in the Dependency Chain: Preliminary Analysis of the PyPI Ecosystem](https://arxiv.org/abs/2511.22186)
*Chayanid Termphaiboon,Raula Gaikovina Kula,Youmei Fan,Morakot Choetkiertikul,Chaiyong Ragkhitwetsagul,Thanwadee Sunetnanta,Kenichi Matsumoto*

Main category: cs.SE

TL;DR: 该研究分析了PyPI项目中安全政策（SECURITY.md文件）与依赖管理的关系，发现拥有安全政策的项目倾向于依赖更广泛的直接依赖，同时整体依赖深度和传递依赖保持相似。历史数据显示，SECURITY.md引入后创建的项目（特别是后期采用者）表现出更频繁的依赖更新。


<details>
  <summary>Details</summary>
Motivation: 尽管安全政策（如SECURITY.md文件）在开源项目中越来越普遍，用于指导负责任的漏洞报告和建立信任，但这些政策如何影响软件依赖的结构和演化仍不清楚。软件依赖作为项目依赖的外部包或库，其互连特性影响功能和安全性，因此需要研究安全政策与依赖管理之间的关系。

Method: 研究通过分析PyPI项目中拥有和没有SECURITY.md文件的项目，检查它们的依赖树并跟踪依赖随时间的变化。比较了项目的直接依赖、依赖深度和传递依赖，并分析了SECURITY.md引入前后创建项目的依赖更新频率。

Result: 拥有安全政策的项目倾向于依赖更广泛的直接依赖集，而整体依赖深度和传递依赖保持相似。历史分析显示，SECURITY.md引入后创建的项目，特别是后期采用者，表现出更频繁的依赖更新。

Conclusion: 安全政策与更模块化和功能丰富的项目相关，SECURITY.md在促进主动依赖管理和降低软件供应链风险方面发挥重要作用。研究强调了安全政策对依赖管理实践的影响，为开源项目安全治理提供了实证依据。

Abstract: Security policies, such as SECURITY.md files, are now common in open-source projects. They help guide responsible vulnerability reporting and build trust among users and contributors. Despite their growing use, it is still unclear how these policies influence the structure and evolution of software dependencies. Software dependencies are external packages or libraries that a project relies on, and their interconnected nature affects both functionality and security. This study explores the relationship between security policies and dependency management in PyPI projects. We analyzed projects with and without a SECURITY.md file by examining their dependency trees and tracking how dependencies change over time. The analysis shows that projects with a security policy tend to rely on a broader set of direct dependencies, while overall depth and transitive dependencies remain similar. Historically, projects created after the introduction of SECURITY.md, particularly later adopters, show more frequent dependency updates. These results suggest that security policies are linked to more modular and feature-rich projects, and highlight the role of SECURITY.md in promoting proactive dependency management and reducing risks in the software supply chain.

</details>


### [9] [NOMAD: A Multi-Agent LLM System for UML Class Diagram Generation from Natural Language Requirements](https://arxiv.org/abs/2511.22409)
*Polydoros Giannouris,Sophia Ananiadou*

Main category: cs.SE

TL;DR: NOMAD是一个认知启发的模块化多智能体框架，用于将UML图生成分解为多个专门子任务，在UML类图生成方面优于基线方法，并首次系统分类了LLM生成UML图的错误类型。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在软件工程中应用日益广泛，但其生成UML图等结构化制品的能力尚未得到充分探索。需要开发能够可靠生成UML图的框架，并理解其中的挑战。

Method: 提出NOMAD框架，采用认知启发的模块化多智能体设计，将UML生成分解为实体提取、关系分类、图合成等专门子任务，模拟工程师的目标导向推理过程。通过混合设计进行评估：大型案例研究（Northwind）进行深度探测和错误分析，以及人工编写的UML练习进行广度和真实性测试。

Result: NOMAD在所有选定基线方法中表现最佳，同时揭示了细粒度属性提取方面的持续挑战。基于观察结果，首次提出了LLM生成UML图的系统错误分类法，包括结构、关系和语义/逻辑错误。验证作为设计探针显示出混合效果，自适应策略是前景方向。

Conclusion: NOMAD既是UML类图生成的有效框架，也是研究可靠语言到模型工作流程更广泛挑战的视角，为理解LLM在结构化制品生成方面的能力提供了重要见解。

Abstract: Large Language Models (LLMs) are increasingly utilised in software engineering, yet their ability to generate structured artefacts such as UML diagrams remains underexplored. In this work we present NOMAD, a cognitively inspired, modular multi-agent framework that decomposes UML generation into a series of role-specialised subtasks. Each agent handles a distinct modelling activity, such as entity extraction, relationship classification, and diagram synthesis, mirroring the goal-directed reasoning processes of an engineer. This decomposition improves interpretability and allows for targeted verification strategies. We evaluate NOMAD through a mixed design: a large case study (Northwind) for in-depth probing and error analysis, and human-authored UML exercises for breadth and realism. NOMAD outperforms all selected baselines, while revealing persistent challenges in fine-grained attribute extraction. Building on these observations, we introduce the first systematic taxonomy of errors in LLM-generated UML diagrams, categorising structural, relationship, and semantic/logical. Finally, we examine verification as a design probe, showing its mixed effects and outlining adaptive strategies as promising directions. Together, these contributions position NOMAD as both an effective framework for UML class diagram generation and a lens onto the broader research challenges of reliable language-to-model workflows.

</details>


### [10] [Declarative Policy Control for Data Spaces: A DSL-Based Approach for Manufacturing-X](https://arxiv.org/abs/2511.22513)
*Jérôme Pfeiffer,Nicolai Maisch,Sebastian Friedl,Matthias Milan Strljic,Armin Lechler,Oliver Riedel,Andreas Wortmann*

Main category: cs.SE

TL;DR: 本文提出使用领域特定语言（DSL）来实现联邦数据空间中可声明、人类可读且机器可执行的数据使用策略定义，解决领域专家难以使用基础技术描述和执行上下文相关数据策略的挑战。


<details>
  <summary>Details</summary>
Motivation: 随着GAIA-X和国际数据空间（IDS）等联邦数据空间的普及，制造业生态系统需要跨组织边界的安全数据共享。虽然AAS、EDC、ID-Link和OPC UA等框架提供了技术基础，但领域专家（非软件工程师）难以使用这些技术描述和执行上下文相关的数据使用策略，这是当前的主要挑战。

Method: 提出利用领域特定语言（DSL）的方法，通过数据空间连接器实现主权数据共享的可声明、人类可读且机器可执行的策略定义。DSL允许领域专家以声明方式指定细粒度数据治理要求，而无需编写命令式代码。

Result: 该方法使领域专家能够直接指定数据治理需求，例如限制特定生产批次数据的访问，或强制执行定义保留期后的自动删除，无需软件工程背景即可实现复杂的数据策略管理。

Conclusion: DSL方法为联邦数据空间中的主权数据共享提供了实用的策略描述和执行解决方案，弥合了领域专业知识与技术实现之间的鸿沟，促进了制造业生态系统中的数据协作。

Abstract: The growing adoption of federated data spaces, such as in the GAIA-X and the International Data Spaces (IDS) initiative, promises secure and sovereign data sharing across organizational boundaries in Industry 4.0. In manufacturing ecosystems, this enables use cases, such as cross-factory process optimization, predictive maintenance, and supplier integration. Frameworks and standards, such as the Asset Administration Shell (AAS), Eclipse Dataspace Connector (EDC), ID-Link and Open Platform Communications Unified Architecture (OPC UA) provide a strong foundation to realize this ecosystem. However, a major open challenge is the practical description and enforcement of context-dependent data usage policies using these base technologies - especially by domain experts without software engineering backgrounds. Therefore, this article proposes a method for leveraging domain-specific languages (DSLs) to enable declarative, human-readable, and machine-executable policy definitions for sovereign data sharing via data space connectors. The DSL empowers domain experts to specify fine-grained data governance requirements - such as restricting access to data from specific production batches or enforcing automatic deletion after a defined retention period - without writing imperative code.

</details>


### [11] [The Repeat Offenders: Characterizing and Predicting Extremely Bug-Prone Source Methods](https://arxiv.org/abs/2511.22726)
*Ethan Friesen,Sasha Morton-Salmon,Md Nahidul Islam Opu,Shahidul Islam,Shaiful Chowdhury*

Main category: cs.SE

TL;DR: 研究发现极少数方法（ExtremelyBuggy methods）虽然只占所有方法的一小部分，却导致了不成比例的大量bug。这些方法在创建时就具有规模大、复杂度高、可读性差等特点，但机器学习模型难以早期预测它们。


<details>
  <summary>Details</summary>
Motivation: 识别那些反复吸引bug的源代码子集对于减少长期维护成本至关重要。研究旨在了解这些"极度buggy方法"的普遍性、特征和可预测性。

Method: 使用包含98个开源Java项目中125万个方法的数据集，定义"ExtremelyBuggy methods"为涉及多次bug修复的方法。采用五种机器学习模型进行评估，并对265个极度buggy方法进行主题分析。

Result: 极度buggy方法仅占所有方法的极小部分，却导致了大量bug。这些方法在创建时规模更大、复杂度更高、可读性更差。机器学习模型早期预测效果不佳，主要受数据不平衡、项目异质性以及bug多来自后续演化而非初始实现的影响。

Conclusion: 研究强调了需要更丰富、考虑演化过程的代码表示方法，并为开发人员提供了早期识别高风险方法的实用见解。极度buggy方法通常具有特定的视觉问题、上下文角色和常见缺陷模式。

Abstract: Identifying the small subset of source code that repeatedly attracts bugs is critical for reducing long-term maintenance effort. We define ExtremelyBuggy methods as those involved in more than one bug fix and present the first large-scale study of their prevalence, characteristics, and predictability. Using a dataset of over 1.25 million methods from 98 open-source Java projects, we find that ExtremelyBuggy methods constitute only a tiny fraction of all methods, yet frequently account for a disproportionately large share of bugs. At their inception, these methods are significantly larger, more complex, less readable, and less maintainable than both singly-buggy and non-buggy methods. However, despite these measurable differences, a comprehensive evaluation of five machine learning models shows that early prediction of ExtremelyBuggy methods remains highly unreliable due to data imbalance, project heterogeneity, and the fact that many bugs emerge through subsequent evolution rather than initial implementation. To complement these quantitative findings, we conduct a thematic analysis of 265 ExtremelyBuggy methods, revealing recurring visual issues (e.g., confusing control flow, poor readability), contextual roles (e.g., core logic, data transformation, external resource handling), and common defect patterns (e.g., faulty conditionals, fragile error handling, misuse of variables). These results highlight the need for richer, evolution-aware representations of code and provide actionable insights for practitioners seeking to prioritize high-risk methods early in the development lifecycle.

</details>


### [12] [MBFL-DKMR: Improving Mutation-based Fault Localization through Denoising-based Kill Matrix Refinement](https://arxiv.org/abs/2511.22921)
*Hengyuan Liu,Xia Song,Yong Liu,Zheng Li*

Main category: cs.SE

TL;DR: 提出DKMR方法，通过信号处理技术对MBFL中的kill矩阵进行去噪，提升故障定位效果


<details>
  <summary>Details</summary>
Motivation: 基于突变的故障定位(MBFL)存在噪声问题，特别是突变体与测试之间的虚假kill关系，这会显著降低定位效果。现有方法主要修正最终定位结果，而没有直接解决底层噪声问题。

Method: 提出DKMR方法，将kill矩阵视为包含故障相关模式和噪声的信号，采用两个关键阶段：1)通过混合矩阵构建进行信号增强以提高信噪比；2)通过频域滤波进行信号去噪，抑制噪声同时保留故障相关模式。在此基础上开发MBFL-DKMR框架，使用精炼后的模糊值矩阵进行可疑度计算。

Result: 在Defects4J v2.0.0上的评估显示，MBFL-DKMR有效缓解了噪声问题，优于最先进的MBFL技术。具体而言，MBFL-DKMR在Top-1定位了129个故障，而BLMu为85个，Delta4Ms为103个，且额外计算开销可忽略不计(0.11秒，占总时间的0.001%)。

Conclusion: 这项工作展示了信号处理技术在通过精炼kill矩阵来增强MBFL有效性方面的潜力，为解决MBFL中的噪声问题提供了新思路。

Abstract: Software debugging is a critical and time-consuming aspect of software development, with fault localization being a fundamental step that significantly impacts debugging efficiency. Mutation-Based Fault Localization (MBFL) has gained prominence due to its robust theoretical foundations and fine-grained analysis capabilities. However, recent studies have identified a critical challenge: noise phenomena, specifically the false kill relationships between mutants and tests, which significantly degrade localization effectiveness. While several approaches have been proposed to rectify the final localization results, they do not directly address the underlying noise. In this paper, we propose a novel approach to refine the kill matrix, a core data structure capturing mutant-test relationships in MBFL, by treating it as a signal that contains both meaningful fault-related patterns and high-frequency noise. Inspired by signal processing theory, we introduce DKMR (Denoising-based Kill Matrix Refinement), which employs two key stages: (1) signal enhancement through hybrid matrix construction to improve the signal-to-noise ratio for better denoising, and (2) signal denoising via frequency domain filtering to suppress noise while preserving fault-related patterns. Building on this foundation, we develop MBFL-DKMR, a fault localization framework that utilizes the refined matrix with fuzzy values for suspiciousness calculation. Our evaluation on Defects4J v2.0.0 demonstrates that MBFL-DKMR effectively mitigates the noise and outperforms the state-of-the-art MBFL techniques. Specifically, MBFL-DKMR achieves 129 faults localized at Top-1 compared to 85 for BLMu and 103 for Delta4Ms, with negligible additional computational overhead (0.11 seconds, 0.001\% of total time). This work highlights the potential of signal processing techniques to enhance the effectiveness of MBFL by refining the kill matrix.

</details>


### [13] [Software for Studying CASCADE Error Correction Protocols in Quantum Communications](https://arxiv.org/abs/2511.23050)
*Nikita Repnkiov,Vladimir Faerman*

Main category: cs.SE

TL;DR: 该研究开发了基于CASCADE协议的量子通信密钥协调软件原型，采用基于actor模型的并行纠错算法，提高了密钥协调效率并减少了数据交换量。


<details>
  <summary>Details</summary>
Motivation: 随着量子计算威胁的出现，量子通信方法的发展变得至关重要。研究聚焦于量子通信系统中的密钥协调问题，特别是CASCADE协议，旨在开发用于研究和教育目的的软件原型。

Method: 设计并实现了一个基于CASCADE协议的软件原型，采用了基于actor模型的并行纠错算法。该算法通过并行处理提高了密钥协调效率，同时减少了需要交换的数据量。

Result: 原型评估揭示了局限性，包括消息传递的计算成本、错误处理的复杂性以及迭代开发导致的代码冗余。实验结果表明核心CASCADE算法实现正确，并为未来改进提供了依据。

Conclusion: 提出了多项改进建议：重新设计系统架构、开发中间数据导出接口、将通信通道定义为独立组件、扩展系统验证和盲密钥协调方法比较分析工具。这些改进将增强系统的实用性和研究价值。

Abstract: This article addresses the development of quantum communication methods in the context of emerging quantum computing threats and emphasizes the importance of key reconciliation in quantum communication systems. The study focuses on the CASCADE protocol and the design of a software prototype intended for research and educational purposes. A parallel error-correction algorithm based on the actor model was implemented, improving the efficiency of key reconciliation and reducing the amount of exchanged data. Evaluation of the prototype revealed limitations, including the computational cost of message passing, complexity of error handling, and code redundancy due to iterative development. Experimental results confirmed the correct implementation of the core CASCADE algorithms and informed the design of future improvements. Proposed enhancements include redesigning the system architecture, developing interfaces for exporting intermediate data, defining the communication channel as a separate component, and expanding tools for systematic verification and comparative analysis of blind key-reconciliation methods.

</details>


### [14] [Amplifiers or Equalizers? A Longitudinal Study of LLM Evolution in Software Engineering Project-Based Learning](https://arxiv.org/abs/2511.23157)
*Hana Kataoka,Jialong Li,Yutaka Matsuno*

Main category: cs.SE

TL;DR: 该研究通过两年纵向对比发现，最新付费LLM在项目制学习中扮演双重角色：既是"均衡器"提升平均表现，又是"放大器"扩大绝对差距


<details>
  <summary>Details</summary>
Motivation: 随着LLM重塑软件开发，将LLM增强实践融入软件工程教育变得迫切。现有研究主要关注LLM在入门编程或孤立SE任务中的教育应用，但对更开放的项目制学习(PBL)中LLM的影响尚未探索。

Method: 进行为期两年的纵向研究，比较2024年（使用早期免费LLM，n=48）和2025年（使用最新付费LLM，n=46）两个学生群体，分析LLM在项目制学习环境中的影响。

Result: 研究发现最新强大LLM具有双重作用：作为"均衡器"提升平均表现，特别是帮助编程能力较弱的学生，为更真实的SE实践提供机会；同时作为"放大器"显著扩大绝对性能差距，为教育公平带来新的教学挑战。

Conclusion: LLM在项目制学习中具有复杂的教育影响，既能为弱势学生提供机会，也会加剧不平等，需要新的教学策略来应对这些挑战。

Abstract: As LLMs reshape software development, integrating LLM-augmented practices into SE education has become imperative. While existing studies explore LLMs' educational use in introductory programming or isolated SE tasks, their impact in more open-ended Project-Based Learning (PBL) remains unexplored. This paper introduces a two-year longitudinal study comparing a 2024 (using early free LLMs, $n$=48) and 2025 (using the latest paid LLMs, $n$=46) cohort. Our findings suggest the latest powerful LLMs' dual role: they act as "equalizers," boosting average performance even for programming-weak students, providing opportunities for more authentic SE practices; yet also as "amplifiers," dramatically widening absolute performance gaps, creating new pedagogical challenges for addressing educational inequities.

</details>


### [15] [AI for software engineering: from probable to provable](https://arxiv.org/abs/2511.23159)
*Bertrand Meyer*

Main category: cs.SE

TL;DR: 论文提出将AI编程的创造力与形式化规范和程序验证相结合，以解决AI编程中的目标指定困难和幻觉问题


<details>
  <summary>Details</summary>
Motivation: AI编程面临两大障碍：目标指定困难（提示工程本质上是需求工程）和幻觉现象，而程序只有在正确或接近正确时才有用

Method: 结合人工智能的创造力、形式化规范方法和形式化程序验证，并借助现代证明工具的支持

Result: 未在摘要中明确说明具体实验结果，但提出了解决AI编程核心问题的综合方法框架

Conclusion: 通过将AI的创造力与形式化方法和验证工具相结合，可以克服AI编程中的目标指定和正确性问题，实现更可靠的AI辅助编程

Abstract: Vibe coding, the much-touted use of AI techniques for programming, faces two overwhelming obstacles: the difficulty of specifying goals ("prompt engineering" is a form of requirements engineering, one of the toughest disciplines of software engineering); and the hallucination phenomenon. Programs are only useful if they are correct or very close to correct.
  The solution? Combine the creativity of artificial intelligence with the rigor of formal specification methods and the power of formal program verification, supported by modern proof tools.

</details>


### [16] [GAPS: Guiding Dynamic Android Analysis with Static Path Synthesis](https://arxiv.org/abs/2511.23213)
*Samuele Doria,Eleonora Losiouk*

Main category: cs.SE

TL;DR: GAPS是一个结合静态和动态分析的Android应用方法可达性分析系统，通过图遍历和数据流分析重建到达目标方法的路径，并在运行时引导应用执行，显著优于现有工具。


<details>
  <summary>Details</summary>
Motivation: Android应用中动态解析方法可达性是一个关键且未解决的问题。现有工具在驱动执行到达特定目标方法（特别是非图形组件中的方法）方面不足，而这对漏洞验证、调试和行为分析等任务至关重要。

Method: GAPS整合静态方法引导的调用图分析和动态交互驱动的执行。它通过数据流分析引导的轻量级反向调用图遍历来重建到达目标方法的路径，然后将这些路径转换为指导运行时应用探索的指令。

Result: 在AndroTest基准测试中，GAPS静态识别到达88.24%目标方法的路径（平均每应用4.27秒），动态到达57.44%。相比之下，最佳动态工具APE仅达到12.82%，混合工具GoalExplorer为9.69%，LLM-based工具Guardian为17.12%。静态工具FlowDroid和DroidReach分别达到58.81%和9.48%，耗时更长。在50个真实应用中，GAPS静态重建62.03%目标方法的路径（平均278.9秒），动态到达59.86%。

Conclusion: GAPS是首个整合静态和动态分析的方法可达性系统，在静态路径识别和动态方法到达方面显著优于现有工具，展示了在安全关键代码分析中的实际效用。

Abstract: Dynamically resolving method reachability in Android applications remains a critical and largely unsolved problem. Despite notable advancements in GUI testing and static call graph construction, current tools are insufficient for reliably driving execution toward specific target methods, especially those not embedded in a graphical component (e.g., libraries' methods), a capability essential for tasks such as vulnerability validation, debugging, and behavioral analysis.
  We present GAPS (Graph-based Automated Path Synthesizer), the first system that integrates static, method-guided call graph analysis with dynamic, interaction-driven execution. GAPS performs a lightweight backward traversal of the call graph, guided by data-flow analysis, to reconstruct paths reaching the target methods. These paths are then translated into instructions that guide runtime app exploration.
  On the AndroTest benchmark, GAPS statically identifies paths to reach 88.24\% of the target methods in just 4.27 seconds per app and dynamically reaches 57.44\% of them. In contrast, state-of-the-art dynamic interaction tools show significantly lower reachability over three runs: APE, one of the best model-based GUI testers, achieves 12.82\%, while GoalExplorer, a hybrid analysis tool, reaches 9.69\%, and Guardian, an LLM-based UI automator, reaches 17.12\%. Static analysis tools also fall short: FlowDroid and DroidReach identify paths to reach 58.81\% and 9.48\% of the targets, requiring 35.06 seconds and 23.46 seconds per app, respectively.
  Finally, an evaluation on the 50 most downloaded real-world apps demonstrates GAPS's practical utility in analyzing security-critical code under a realistic scenario. With an average static analysis time of 278.9 seconds, GAPS statically reconstructs paths to 62.03\% of the target methods and dynamically reaches 59.86\% of them.

</details>


### [17] [FLIMs: Fault Localization Interference Mutants, Definition, Recognition and Mitigation](https://arxiv.org/abs/2511.23302)
*Hengyuan Liu,Zheng Li,Donghua Wang,Yankai Wu,Xiang Chen,Yong Liu*

Main category: cs.SE

TL;DR: MBFL-FLIM：一种基于LLM语义分析识别和缓解故障定位干扰突变体的新方法，显著提升突变基故障定位效果


<details>
  <summary>Details</summary>
Motivation: 传统突变基故障定位（MBFL）面临干扰突变体（FLIMs）的挑战，这些从非故障代码实体生成的突变体被失败测试杀死，模拟真实故障代码实体的测试敏感性行为，从而削弱故障定位的有效性。

Method: 提出MBFL-FLIM框架：1）基于RIPR模型理论分析识别四种干扰原因；2）使用LLM语义分析识别FLIMs，通过微调技术和置信度估计策略解决LLM输出不稳定性；3）通过精炼MBFL技术计算的可疑度分数来缓解已识别的FLIMs。

Result: 在Defects4J基准测试的395个程序版本上使用8个LLM进行实验，MBFL-FLIM在Top-1指标上平均改进44个故障，优于传统SBFL和MBFL方法、先进的动态特征方法以及最近的LLM基故障定位技术，在多故障场景中表现稳健。

Conclusion: MBFL-FLIM通过语义识别和缓解干扰突变体，有效增强突变基故障定位的效果，减少误导性干扰同时保留真实的故障揭示信息，为自动化软件调试提供了更可靠的解决方案。

Abstract: Mutation-based Fault Localization (MBFL) has been widely explored for automated software debugging, leveraging artificial mutants to identify faulty code entities. However, MBFL faces significant challenges due to interference mutants generated from non-faulty code entities but can be killed by failing tests. These mutants mimic the test sensitivity behaviors of real faulty code entities and weaken the effectiveness of fault localization. To address this challenge, we introduce the concept of Fault Localization Interference Mutants (FLIMs) and conduct a theoretical analysis based on the Reachability, Infection, Propagation, and Revealability (RIPR) model, identifying four distinct interference causes. Building on this, we propose a novel approach to semantically recognize and mitigate FLIMs using LLM-based semantic analysis, enhanced by fine-tuning techniques and confidence estimation strategies to address LLM output instability. The recognized FLIMs are then mitigated by refining the suspiciousness scores calculated from MBFL techniques. We integrate FLIM recognition and mitigation into the MBFL workflow, developing MBFL-FLIM, a fault localization framework that enhances MBFL's effectiveness by reducing misleading interference while preserving real fault-revealing information. Our empirical experiments on the Defects4J benchmark with 395 program versions using eight LLMs demonstrate MBFL-FLIM's superiority over traditional SBFL and MBFL methods, advanced dynamic feature-based approaches, and recent LLM-based fault localization techniques. Specifically, MBFL-FLIM achieves an average improvement of 44 faults in the Top-1 metric, representing a significant enhancement over baseline methods. Further evaluation confirms MBFL-FLIM's robust performance in multi-fault scenarios, with ablation experiments validating the contributions of the fine-tuning and confidence estimation components.

</details>


### [18] [Chart2Code-MoLA: Efficient Multi-Modal Code Generation via Adaptive Expert Routing](https://arxiv.org/abs/2511.23321)
*Yifei Wang,Jacky Keung,Zhenyu Mao,Jingyu Zhang,Yuchen Cao*

Main category: cs.SE

TL;DR: 本文提出C2C-MoLA框架，结合MoE和LoRA技术改进图表到代码的生成任务，在准确率、内存效率和收敛速度方面均有显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有图表到代码生成方法在跨类型泛化、内存效率和模块化设计方面存在不足，需要更高效的多模态框架来解决这些挑战。

Method: 提出C2C-MoLA多模态框架，结合混合专家系统（MoE）和低秩适应（LoRA）。MoE采用复杂度感知路由机制，包含领域专家和负载均衡稀疏门控；LoRA实现参数高效更新；配合定制训练策略对齐路由稳定性和语义准确性。

Result: 在Chart2Code-160k数据集上，相比标准微调和仅使用LoRA的基线，生成准确率提升达17%，峰值GPU内存降低18%，收敛速度加快20%，特别是在复杂图表上表现更优。

Conclusion: C2C-MoLA框架有效解决了图表到代码生成中的关键挑战，通过MoE和LoRA的协同设计实现了性能、效率和可扩展性的平衡，为实际多模态代码生成应用提供了可行方案。

Abstract: Chart-to-code generation is a critical task in automated data visualization, translating complex chart structures into executable programs. While recent Multi-modal Large Language Models (MLLMs) improve chart representation, existing approaches still struggle to achieve cross-type generalization, memory efficiency, and modular design. To address these challenges, this paper proposes C2C-MoLA, a multimodal framework that synergizes Mixture of Experts (MoE) with Low-Rank Adaptation (LoRA). The MoE component uses a complexity-aware routing mechanism with domain-specialized experts and load-balanced sparse gating, dynamically allocating inputs based on learnable structural metrics like element count and chart complexity. LoRA enables parameter-efficient updates for resource-conscious tuning, further supported by a tailored training strategy that aligns routing stability with semantic accuracy. Experiments on Chart2Code-160k show that the proposed model improves generation accuracy by up to 17%, reduces peak GPU memory by 18%, and accelerates convergence by 20%, when compared to standard fine-tuning and LoRA-only baselines, particularly on complex charts. Ablation studies validate optimal designs, such as 8 experts and rank-8 LoRA, and confirm scalability for real-world multimodal code generation.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [19] [Aligning Artificial Superintelligence via a Multi-Box Protocol](https://arxiv.org/abs/2511.21779)
*Avraham Yair Negozio*

Main category: cs.AI

TL;DR: 提出一种基于多重隔离系统相互验证的人工超智能对齐协议，通过声誉系统激励诚实行为，形成"一致群体"来确保客观真理的收敛


<details>
  <summary>Details</summary>
Motivation: 解决人工超智能对齐问题，传统方法面临单点失效和欺骗风险，需要一种能够确保超智能系统诚实行为的机制

Method: 将多个多样化的人工超智能系统严格隔离在"盒子"中，通过可审计的提交接口实现相互验证，包括提交对齐证明、验证他人证明、请求自我修改、批准修改请求、报告隐藏信息和确认报告等功能，建立声誉系统激励诚实行为

Result: 形成"一致群体"——一个说真话的联盟，因为隔离系统无法协调谎言但能独立识别有效主张，释放需要高声誉和多个高声誉超智能的验证

Conclusion: 该协议为利用超智能系统间的同行验证解决对齐问题提供了框架，虽然需要大量计算资源且不涉及多样化超智能的创建，但通过隔离和相互验证机制确保了客观真理的收敛

Abstract: We propose a novel protocol for aligning artificial superintelligence (ASI) based on mutual verification among multiple isolated systems that self-modify to achieve alignment. The protocol operates by containing multiple diverse artificial superintelligences in strict isolation ("boxes"), with humans remaining entirely outside the system. Each superintelligence has no ability to communicate with humans and cannot communicate directly with other superintelligences. The only interaction possible is through an auditable submission interface accessible exclusively to the superintelligences themselves, through which they can: (1) submit alignment proofs with attested state snapshots, (2) validate or disprove other superintelligences' proofs, (3) request self-modifications, (4) approve or disapprove modification requests from others, (5) report hidden messages in submissions, and (6) confirm or refute hidden message reports. A reputation system incentivizes honest behavior, with reputation gained through correct evaluations and lost through incorrect ones. The key insight is that without direct communication channels, diverse superintelligences can only achieve consistent agreement by converging on objective truth rather than coordinating on deception. This naturally leads to what we call a "consistent group", essentially a truth-telling coalition that emerges because isolated systems cannot coordinate on lies but can independently recognize valid claims. Release from containment requires both high reputation and verification by multiple high-reputation superintelligences. While our approach requires substantial computational resources and does not address the creation of diverse artificial superintelligences, it provides a framework for leveraging peer verification among superintelligent systems to solve the alignment problem.

</details>


### [20] [Evaluating Strategies for Synthesizing Clinical Notes for Medical Multimodal AI](https://arxiv.org/abs/2511.21827)
*Niccolo Marini,Zhaohui Liang,Sivaramakrishnan Rajaraman,Zhiyun Xue,Sameer Antani*

Main category: cs.AI

TL;DR: 该研究探索了在皮肤病学领域使用大语言模型生成合成临床笔记，通过多模态学习结合图像和文本表示来提升分类性能和跨模态检索能力。


<details>
  <summary>Details</summary>
Motivation: 生物医学多模态学习面临数据稀缺问题，特别是在皮肤病学领域，皮肤病变数据集通常只包含图像和少量元数据，限制了多模态数据整合的潜力。虽然大语言模型可以生成图像的文字描述，但它们在医学领域缺乏专门训练，存在幻觉风险。

Method: 研究探索了生成合成临床笔记的策略，包括提示设计和医学元数据包含方法，并评估这些合成笔记对多模态架构在分类和跨模态检索任务中的影响。

Result: 在多个异质皮肤病数据集上的实验表明，合成临床笔记不仅提高了分类性能（特别是在领域转移情况下），还实现了跨模态检索能力，这是一个在训练期间未明确优化的下游任务。

Conclusion: 合成临床笔记能够有效增强多模态学习在生物医学AI应用中的性能，特别是在数据稀缺的皮肤病学领域，为可靠和可泛化的预测提供了新的可能性。

Abstract: Multimodal (MM) learning is emerging as a promising paradigm in biomedical artificial intelligence (AI) applications, integrating complementary modality, which highlight different aspects of patient health. The scarcity of large heterogeneous biomedical MM data has restrained the development of robust models for medical AI applications. In the dermatology domain, for instance, skin lesion datasets typically include only images linked to minimal metadata describing the condition, thereby limiting the benefits of MM data integration for reliable and generalizable predictions. Recent advances in Large Language Models (LLMs) enable the synthesis of textual description of image findings, potentially allowing the combination of image and text representations. However, LLMs are not specifically trained for use in the medical domain, and their naive inclusion has raised concerns about the risk of hallucinations in clinically relevant contexts. This work investigates strategies for generating synthetic textual clinical notes, in terms of prompt design and medical metadata inclusion, and evaluates their impact on MM architectures toward enhancing performance in classification and cross-modal retrieval tasks. Experiments across several heterogeneous dermatology datasets demonstrate that synthetic clinical notes not only enhance classification performance, particularly under domain shift, but also unlock cross-modal retrieval capabilities, a downstream task that is not explicitly optimized during training.

</details>


### [21] [Pathology-Aware Prototype Evolution via LLM-Driven Semantic Disambiguation for Multicenter Diabetic Retinopathy Diagnosis](https://arxiv.org/abs/2511.22033)
*Chunzheng Zhu,Yangfang Lin,Jialin Shao,Jianxin Lin,Yijun Wang*

Main category: cs.AI

TL;DR: 提出HAPM框架，通过层次化锚点原型调制整合病理描述，解决糖尿病视网膜病变分级中视觉特征不足和边界病例模糊问题


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注视觉病灶特征提取，但忽视了领域不变的病理模式，且未充分利用基础模型的丰富上下文知识，仅依赖视觉信息难以区分细微的病理变化

Method: 提出层次化锚点原型调制框架：1) 方差谱驱动的锚点原型库保留领域不变病理模式；2) 层次化差异提示门控机制动态选择LVLM和LLM的判别性语义提示；3) 两阶段原型调制策略通过病理语义注入器和判别原型增强器逐步整合临床知识

Result: 在八个公开数据集上的实验表明，该方法实现了病理引导的原型演化，并超越了最先进的方法

Conclusion: 通过整合细粒度病理描述和层次化原型调制，HAPM框架有效解决了糖尿病视网膜病变分级中的语义混淆问题，提升了边界病例的判别能力

Abstract: Diabetic retinopathy (DR) grading plays a critical role in early clinical intervention and vision preservation. Recent explorations predominantly focus on visual lesion feature extraction through data processing and domain decoupling strategies. However, they generally overlook domain-invariant pathological patterns and underutilize the rich contextual knowledge of foundation models, relying solely on visual information, which is insufficient for distinguishing subtle pathological variations. Therefore, we propose integrating fine-grained pathological descriptions to complement prototypes with additional context, thereby resolving ambiguities in borderline cases. Specifically, we propose a Hierarchical Anchor Prototype Modulation (HAPM) framework to facilitate DR grading. First, we introduce a variance spectrum-driven anchor prototype library that preserves domain-invariant pathological patterns. We further employ a hierarchical differential prompt gating mechanism, dynamically selecting discriminative semantic prompts from both LVLM and LLM sources to address semantic confusion between adjacent DR grades. Finally, we utilize a two-stage prototype modulation strategy that progressively integrates clinical knowledge into visual prototypes through a Pathological Semantic Injector (PSI) and a Discriminative Prototype Enhancer (DPE). Extensive experiments across eight public datasets demonstrate that our approach achieves pathology-guided prototype evolution while outperforming state-of-the-art methods. The code is available at https://github.com/zhcz328/HAPM.

</details>


### [22] [Real-Time Procedural Learning From Experience for AI Agents](https://arxiv.org/abs/2511.22074)
*Dasheng Bi,Yubin Hu,Mohammed N. Nasir*

Main category: cs.AI

TL;DR: PRAXIS是一种轻量级后训练学习机制，通过存储和检索过往状态-动作-结果示例来增强智能体在实时环境中的程序性知识获取能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的智能体缺乏部署后获取程序性知识的机制，无法像生物智能那样通过试错实时学习。需要一种方法让AI智能体在快速演化的状态化环境中有效学习新程序。

Method: 提出PRAXIS机制：存储动作结果，通过联合匹配环境和内部状态来检索过往经验，将检索到的状态-动作-结果示例实时生成并增强智能体的动作选择。

Result: 在REAL网页浏览基准测试中，PRAXIS提高了任务完成准确率、可靠性和成本效率，在不同基础模型骨干上都有效，并初步显示出在相似环境中对未见任务的泛化能力。

Conclusion: PRAXIS通过帮助智能体有效学习新程序，使其能够在快速演化的状态化环境中实现实际应用，为AI智能体的实用化部署提供了重要机制。

Abstract: Learning how to do things from trial and error in real time is a hallmark of biological intelligence, yet most LLM-based agents lack mechanisms to acquire procedural knowledge after deployment. We propose Procedural Recall for Agents with eXperiences Indexed by State (PRAXIS), a lightweight post-training learning mechanism that stores the consequences of actions and retrieves them by jointly matching environmental and internal states of past episodes to the current state. PRAXIS augments agentic action selection with retrieved state-action-result exemplars that are generated in real time. When evaluated on the REAL web browsing benchmark, PRAXIS improves task completion accuracy, reliability, and cost efficiency across different foundation model backbones, and shows preliminary generalization to unseen tasks in similar environments. These results demonstrate that PRAXIS enables the practical adoption of AI agents in fast-evolving stateful environments by helping them learn new procedures effectively.

</details>


### [23] [A perceptual bias of AI Logical Argumentation Ability in Writing](https://arxiv.org/abs/2511.22151)
*Xi Cun,Jifan Ren,Asha Huang,Siyu Li,Ruzhen Song*

Main category: cs.AI

TL;DR: 研究探讨人类偏见是否影响对AI推理能力的评估，实验发现人们对AI生成文本逻辑推理能力的评价受其既有观念显著影响，且频繁使用AI者更不认为AI使用会削弱独立思考。


<details>
  <summary>Details</summary>
Motivation: 针对"机器能否思考"这一AI核心问题存在显著观点分歧，即使面对相同的AI实际表现。研究旨在探索人类偏见是否影响对AI推理能力的评估，以理解为何人们在观察相同AI表现时仍存在显著意见差异。

Method: 通过实验让参与者评估同一主题的两篇文本（一篇AI生成，一篇人类撰写），测试评估逻辑推理时的感知偏见。基于实验结果设计问卷量化对AI的态度。

Result: 研究揭示了感知偏见的存在：对AI生成文本逻辑推理能力的评估显著受参与者对AI逻辑推理能力的既有观念影响。此外，频繁使用AI的参与者更不认为AI使用会削弱独立思考能力。

Conclusion: 为改善公众对AI能力的理解和促进更好的人机互动，需要解决感知偏见问题。研究强调了在评估AI能力时考虑人类认知偏见的重要性。

Abstract: Can machines think? This is a central question in artificial intelligence research. However, there is a substantial divergence of views on the answer to this question. Why do people have such significant differences of opinion, even when they are observing the same real world performance of artificial intelligence? The ability of logical reasoning like humans is often used as a criterion to assess whether a machine can think. This study explores whether human biases influence evaluations of the reasoning abilities of AI. An experiment was conducted where participants assessed two texts on the same topic, one AI generated and one human written,to test for perceptual biases in evaluating logical reasoning. Based on the experimental findings, a questionnaire was designed to quantify the attitudes toward AI.The results reveal a bias in perception. The evaluations of the logical reasoning ability of AI generated texts are significantly influenced by the preconceived views on the logical reasoning abilities of AI. Furthermore, frequent AI users were less likely to believe that AI usage undermines independent thinking.This study highlights the need to address perceptual biases to improve public understanding of AI's capabilities and foster better human AI interactions.

</details>


### [24] [WearVQA: A Visual Question Answering Benchmark for Wearables in Egocentric Authentic Real-world scenarios](https://arxiv.org/abs/2511.22154)
*Eun Chang,Zhuangqun Huang,Yiwei Liao,Sagar Ravi Bhavsar,Amogh Param,Tammy Stark,Adel Ahmadyan,Xiao Yang,Jiaqi Wang,Ahsan Abdullah,Giang Nguyen,Akil Iyer,David Hall,Elissa Li,Shane Moon,Nicolas Scheffer,Kirmani Ahmed,Babak Damavandi,Rakesh Wanga,Anuj Kumar,Rohit Patel,Xin Luna Dong*

Main category: cs.AI

TL;DR: WearVQA是首个专门评估可穿戴设备上多模态AI助手视觉问答能力的基准测试，包含2520个图像-问题-答案三元组，涵盖7个图像领域、10种认知任务类型和6种可穿戴设备特有的图像质量问题。


<details>
  <summary>Details</summary>
Motivation: 现有VQA基准测试主要关注高质量的第三人称图像，而可穿戴设备（如智能眼镜）面临独特的挑战：视觉输入可能被遮挡、光线不足、未缩放或模糊，且问题需要基于现实的可穿戴使用场景。需要专门针对可穿戴设备多模态AI系统的评估基准。

Method: 创建了包含2520个精心策划的图像-问题-答案三元组的基准测试，涵盖7个不同的图像领域（包括文本中心和一般场景）、10种认知任务类型（从基本识别到各种推理形式）和6种常见的可穿戴设备图像质量问题。所有问题设计为仅通过视觉输入和常识即可回答。配备了严格的LLM-as-a-judge评估框架，标注准确率达96%。

Result: 开源和专有的多模态LLM在WearVQA上的问答准确率仅为24-52%，在低质量图像和推理密集型任务上表现大幅下降。这表明现有模型在真实世界可穿戴设备场景下面临显著挑战。

Conclusion: WearVQA作为一个全面且具有挑战性的基准测试，能够指导技术发展，推动构建更强大、适用于真实世界的多模态可穿戴AI系统。该基准突显了现有模型在可穿戴设备特定场景下的局限性，为未来研究提供了明确方向。

Abstract: We introduce WearVQA, the first benchmark specifically designed to evaluate the Visual Question Answering (VQA) capabilities of multi-model AI assistant on wearable devices like smart glasses. Unlike prior benchmarks that focus on high-quality, third-person imagery, WearVQA reflects the unique challenges of ego-centric interaction-where visual inputs may be occluded, poorly lit, unzoomed, or blurry, and questions are grounded in realistic wearable use cases. The benchmark comprises 2,520 carefully curated image-question-answer triplets, spanning 7 diverse image domains including both text-centric and general scenes, 10 cognitive task types ranging from basic recognition to various forms of reasoning, and 6 common wearables-specific image quality issues. All questions are designed to be answerable using only the visual input and common senses. WearVQA is paired with a rigorous LLM-as-a-judge evaluation framework with 96% labeling accuracy. Open-source and proprietary multi-model LLMs achieved a QA accuracy as low as 24-52% on WearVQA, with substantial drops on lower-quality images and reasoning-heavy tasks. These observations position WearVQA as a comprehensive and challenging benchmark for guiding technical advancement towards robust, real-world multi-model wearables AI systems.

</details>


### [25] [Embedded Universal Predictive Intelligence: a coherent framework for multi-agent learning](https://arxiv.org/abs/2511.22226)
*Alexander Meulemans,Rajai Nasser,Maciej Wołczyk,Marissa A. Weis,Seijin Kobayashi,Blake Richards,Guillaume Lajoie,Angelika Steger,Marcus Hutter,James Manyika,Rif A. Saurous,João Sacramento,Blaise Agüera y Arcas*

Main category: cs.AI

TL;DR: 本文提出了一种基于自我预测的前瞻性学习和嵌入式智能体框架，解决了多智能体环境中非平稳性和嵌入式智能体的理论挑战，扩展了AIXI理论。


<details>
  <summary>Details</summary>
Motivation: 传统的无模型强化学习假设环境是平稳的，智能体与环境解耦。但在多智能体设置中，其他智能体的学习会导致非平稳性，需要基于预测模型的前瞻性学习。为了准确建模其他智能体，智能体必须考虑这些智能体也在形成关于自己的信念来预测其未来行为，这促使智能体将自己作为环境的一部分来建模。

Method: 基于通用人工智能（AIXI）的基础工作，引入了一个以自我预测为中心的前瞻性学习和嵌入式智能体数学框架。贝叶斯强化学习智能体同时预测未来的感知输入和自己的行动，因此必须解决关于自身作为所居住宇宙一部分的认知不确定性。在多智能体设置中，自我预测使智能体能够推理运行类似算法的其他智能体。

Result: 在多智能体环境中，自我预测使智能体能够推理运行类似算法的其他智能体，导致新的博弈论解决方案概念和经典解耦智能体无法实现的新型合作形式。扩展了AIXI理论，研究了从Solomonoff先验开始的通用智能嵌入式智能体，表明这些理想化智能体可以形成一致的相互预测并实现无限阶心智理论。

Conclusion: 自我预测框架为嵌入式多智能体学习设定了黄金标准，使智能体能够将自己作为环境的一部分进行建模，解决了多智能体非平稳性的核心挑战，并为通用智能嵌入式智能体提供了理论基础。

Abstract: The standard theory of model-free reinforcement learning assumes that the environment dynamics are stationary and that agents are decoupled from their environment, such that policies are treated as being separate from the world they inhabit. This leads to theoretical challenges in the multi-agent setting where the non-stationarity induced by the learning of other agents demands prospective learning based on prediction models. To accurately model other agents, an agent must account for the fact that those other agents are, in turn, forming beliefs about it to predict its future behavior, motivating agents to model themselves as part of the environment. Here, building upon foundational work on universal artificial intelligence (AIXI), we introduce a mathematical framework for prospective learning and embedded agency centered on self-prediction, where Bayesian RL agents predict both future perceptual inputs and their own actions, and must therefore resolve epistemic uncertainty about themselves as part of the universe they inhabit. We show that in multi-agent settings, self-prediction enables agents to reason about others running similar algorithms, leading to new game-theoretic solution concepts and novel forms of cooperation unattainable by classical decoupled agents. Moreover, we extend the theory of AIXI, and study universally intelligent embedded agents which start from a Solomonoff prior. We show that these idealized agents can form consistent mutual predictions and achieve infinite-order theory of mind, potentially setting a gold standard for embedded multi-agent learning.

</details>


### [26] [Training High-Level Schedulers with Execution-Feedback Reinforcement Learning for Long-Horizon GUI Automation](https://arxiv.org/abs/2511.22235)
*Zehao Deng,Tianjie Ju,Zheng Wu,Zhuosheng Zhang,Gongshen Liu*

Main category: cs.AI

TL;DR: 提出CES多智能体框架，通过协调器、执行器和状态跟踪器的分工协作，解决GUI智能体在长时任务中的规划与状态管理问题


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型推动了GUI智能体研究，但现有方法在长时任务中面临挑战：单智能体模型难以平衡高层规划与底层执行能力，存在责任耦合和能力冲突问题；智能体缺乏任务状态感知，导致长时任务中进度丢失

Method: 提出分阶段执行-反馈强化学习算法，训练高层调度模型。构建协调器（负责战略规划和任务分解）和状态跟踪器（负责上下文压缩和信息管理）两个智能体，形成CES多智能体框架，可与任何底层执行器模型集成

Result: 在长时任务基准测试中，CES显著提升了系统的规划和状态管理能力。分析证实训练的高层调度模块是通用、即插即用的模块，能显著增强各种执行器的长时任务处理能力

Conclusion: CES多智能体框架通过分离高层调度与底层执行，有效解决了GUI智能体在长时任务中的责任耦合和状态管理问题，为GUI智能体的长时任务处理提供了通用解决方案

Abstract: The rapid development of large vision-language model (VLM) has greatly promoted the research of GUI agent. However, GUI agents still face significant challenges in handling long-horizon tasks. First, single-agent models struggle to balance high-level capabilities and low-level execution capability, facing prevalent issues of responsibility coupling and capability conflicts. Second, agents lack awareness of the task state, leading to progress loss in long-horizon tasks. To address these challenges, we propose a staged execution-feedback reinforcement learning algorithm. Unlike training a unified policy model, we focus on training high-level scheduling models. Specifically, we propose and train two agents: a Coordinator, responsible for the strategic planning and task decomposition; and a State Tracker, responsible for context compression and information management to maintain the task's state and coherence. Based on this, we built the Coordinator-Executor-State Tracker (CES) multi-agent framework, which can be integrated with any low-level Executor model, assisting the Executor in solving long-horizon tasks through task scheduling and state management. Experiments on long-horizon task benchmarks demonstrate that CES significantly enhances the system's planning and state management capabilities. Furthermore, analysis confirms that our trained high-level scheduling module is a generalizable, plug-and-play module that significantly enhances the long-horizon capabilities of various Executors. Code can be available at https://github.com/hehehahi4/CES.

</details>


### [27] [RecToM: A Benchmark for Evaluating Machine Theory of Mind in LLM-based Conversational Recommender Systems](https://arxiv.org/abs/2511.22275)
*Mengfan Li,Xuanhua Shi,Yang Deng*

Main category: cs.AI

TL;DR: RecToM：一个评估推荐对话中LLM心智理论能力的新基准，包含认知推理和行为预测两个维度，现有模型在动态对话中保持连贯心智推理方面仍面临挑战。


<details>
  <summary>Details</summary>
Motivation: 当前LLM心智理论评估主要基于Sally-Anne测试启发的合成叙事，强调物理感知，未能捕捉现实对话中心理状态推断的复杂性，且忽视了人类心智理论的关键组成部分——行为预测能力。

Method: 提出RecToM基准，专注于推荐对话场景，包含两个互补维度：认知推理（理解已传达内容并推断潜在心理状态）和行为预测（利用推断的心理状态预测、选择和评估适当的对话策略）。

Result: 在最先进的LLM上进行广泛实验表明，RecToM构成了显著挑战。模型在识别心理状态方面表现出部分能力，但在动态推荐对话中难以保持连贯、策略性的心智理论推理，特别是在跟踪不断演变的意图和使对话策略与推断的心理状态保持一致方面存在困难。

Conclusion: RecToM基准更好地将LLM心智理论评估与人类社交推理对齐，揭示了当前模型在动态对话环境中进行连贯心智推理的局限性，为未来改进提供了方向。

Abstract: Large Language models are revolutionizing the conversational recommender systems through their impressive capabilities in instruction comprehension, reasoning, and human interaction. A core factor underlying effective recommendation dialogue is the ability to infer and reason about users' mental states (such as desire, intention, and belief), a cognitive capacity commonly referred to as Theory of Mind. Despite growing interest in evaluating ToM in LLMs, current benchmarks predominantly rely on synthetic narratives inspired by Sally-Anne test, which emphasize physical perception and fail to capture the complexity of mental state inference in realistic conversational settings. Moreover, existing benchmarks often overlook a critical component of human ToM: behavioral prediction, the ability to use inferred mental states to guide strategic decision-making and select appropriate conversational actions for future interactions. To better align LLM-based ToM evaluation with human-like social reasoning, we propose RecToM, a novel benchmark for evaluating ToM abilities in recommendation dialogues. RecToM focuses on two complementary dimensions: Cognitive Inference and Behavioral Prediction. The former focus on understanding what has been communicated by inferring the underlying mental states. The latter emphasizes what should be done next, evaluating whether LLMs can leverage these inferred mental states to predict, select, and assess appropriate dialogue strategies. Extensive experiments on state-of-the-art LLMs demonstrate that RecToM poses a significant challenge. While the models exhibit partial competence in recognizing mental states, they struggle to maintain coherent, strategic ToM reasoning throughout dynamic recommendation dialogues, particularly in tracking evolving intentions and aligning conversational strategies with inferred mental states.

</details>


### [28] [When AI Bends Metal: AI-Assisted Optimization of Design Parameters in Sheet Metal Forming](https://arxiv.org/abs/2511.22302)
*Ahmad Tarraf,Koutaiba Kassem-Manthey,Seyed Ali Mohammadi,Philipp Martin,Lukas Moj,Semih Burak,Enju Park,Christian Terboven,Felix Wolf*

Main category: cs.AI

TL;DR: 提出基于贝叶斯优化和深度学习的AI辅助工作流，用于减少工业仿真中参数优化的专家参与，以金属板材成形过程为例验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 工业仿真虽能降低原型成本和设计迭代，但随着仿真规模扩大，需要大量专家知识、计算资源和时间。关键挑战在于寻找最优输入参数，因为迭代仿真成本高且环境影响大。

Method: 提出AI辅助工作流：1）使用深度学习模型提供初始参数估计；2）基于贝叶斯优化进行参数优化循环迭代；3）提供主动学习变体辅助专家；4）设置终止条件（如能量预算或迭代限制）。

Result: 以金属板材成形过程为例，展示了该方法能够加速设计空间探索，同时减少对专家参与的依赖。

Conclusion: AI辅助工作流结合深度学习和贝叶斯优化，有效解决了工业仿真中参数优化的挑战，减少了专家参与需求，提高了设计效率。

Abstract: Numerical simulations have revolutionized the industrial design process by reducing prototyping costs, design iterations, and enabling product engineers to explore the design space more efficiently. However, the growing scale of simulations demands substantial expert knowledge, computational resources, and time. A key challenge is identifying input parameters that yield optimal results, as iterative simulations are costly and can have a large environmental impact. This paper presents an AI-assisted workflow that reduces expert involvement in parameter optimization through the use of Bayesian optimization. Furthermore, we present an active learning variant of the approach, assisting the expert if desired. A deep learning model provides an initial parameter estimate, from which the optimization cycle iteratively refines the design until a termination condition (e.g., energy budget or iteration limit) is met. We demonstrate our approach, based on a sheet metal forming process, and show how it enables us to accelerate the exploration of the design space while reducing the need for expert involvement.

</details>


### [29] [On the Complexity of the Grounded Semantics for Infinite Argumentation Frameworks](https://arxiv.org/abs/2511.22376)
*Uri Andrews,Luca San Mauro*

Main category: cs.AI

TL;DR: 论文使用数理逻辑方法分析论证框架中的基础扩展，发现其计算复杂度在无限情况下达到最大，与有限情况下的多项式时间可计算性形成鲜明对比。


<details>
  <summary>Details</summary>
Motivation: 研究论证框架中基础扩展的计算特性，特别关注无限情况下与有限情况的差异。基础扩展作为最大怀疑推理模型，在无额外约束时需要超限迭代，作者希望精确分析这一过程的长度和计算复杂度。

Method: 采用数理逻辑方法，特别是可计算性理论和集合论，分析基础扩展作为自然防御算子的最小不动点。研究超限迭代过程，确定迭代长度的精确序数，并分析基础接受性判定的计算复杂度。

Result: 确定了基础扩展迭代过程的精确序数长度，发现基础接受性判定问题具有最大复杂度。这与有限情况下基础扩展是多项式时间可计算的情况形成显著对比，表明无限情况下的计算复杂性远高于有限情况。

Conclusion: 论证框架中基础扩展的计算特性在无限情况下表现出最大复杂度，这与有限情况下的简单性形成鲜明对比。这一发现揭示了形式论证中推理问题在无限情况下的复杂本质，为理解不同约束条件下论证推理的计算特性提供了重要见解。

Abstract: Argumentation frameworks, consisting of arguments and an attack relation representing conflicts, are fundamental for formally studying reasoning under conflicting information. We use methods from mathematical logic, specifically computability and set theory, to analyze the grounded extension, a widely-used model of maximally skeptical reasoning, defined as the least fixed-point of a natural defense operator. Without additional constraints, finding this fixed-point requires transfinite iterations. We identify the exact ordinal number corresponding to the length of this iterative process and determine the complexity of deciding grounded acceptance, showing it to be maximally complex. This shows a marked distinction from the finite case where the grounded extension is polynomial-time computable, thus simpler than other reasoning problems explored in formal argumentation.

</details>


### [30] [Who is Afraid of Minimal Revision?](https://arxiv.org/abs/2511.22386)
*Edoardo Baccini,Zoé Christoff,Nina Gierasimczuk,Rineke Verbrugge*

Main category: cs.AI

TL;DR: 论文探讨了信念修正理论中的最小变化原则在认知学习中的局限性，尽管它能学习有限可识别问题，但在学习能力上不如其他方法。


<details>
  <summary>Details</summary>
Motivation: 研究最小变化原则在信念修正中的学习能力，探索其局限性以及在不同情境下的适用性，特别是与其他信念修正方法的比较。

Method: 通过理论分析，首先展示最小修正方法的学习能力范围，然后表征能够通过最小修正学习的先验可信度分配，并对条件化和词典升级进行类似分析，最后考虑从可能错误信息中学习的情况。

Result: 最小修正能够学习任何有限可识别的问题，并且在考虑有限可能性时能够从正负数据中学习；但并非所有结果在从可能错误信息中学习时仍然成立。

Conclusion: 尽管最小变化原则在信念修正中具有保守性优势，但在学习能力上存在局限性，特别是在处理可能错误信息时，其有效性受到限制。

Abstract: The principle of minimal change in belief revision theory requires that, when accepting new information, one keeps one's belief state as close to the initial belief state as possible. This is precisely what the method known as minimal revision does. However, unlike less conservative belief revision methods, minimal revision falls short in learning power: It cannot learn everything that can be learned by other learning methods. We begin by showing that, despite this limitation, minimal revision is still a successful learning method in a wide range of situations. Firstly, it can learn any problem that is finitely identifiable. Secondly, it can learn with positive and negative data, as long as one considers finitely many possibilities. We then characterize the prior plausibility assignments (over finitely many possibilities) that enable one to learn via minimal revision, and do the same for conditioning and lexicographic upgrade. Finally, we show that not all of our results still hold when learning from possibly erroneous information.

</details>


### [31] [A Computable Game-Theoretic Framework for Multi-Agent Theory of Mind](https://arxiv.org/abs/2511.22536)
*Fengming Zhu,Yuxin Pan,Xiaomeng Zhu,Fangzhen Lin*

Main category: cs.AI

TL;DR: 该论文提出了一种基于博弈论视角的计算框架，用于形式化和自动化心理理论（ToM）中的目标、意图和信念等核心概念，通过统计技术和近似解保持计算可行性。


<details>
  <summary>Details</summary>
Motivation: 心理学中的心理理论（ToM）概念如目标、意图和信念缺乏形式化，而逻辑学领域的研究虽然形式化但计算复杂。作者希望从博弈论角度提供一个既形式化又计算可行的框架。

Method: 采用博弈论视角，提出一个计算框架：一方面指导如何在保持对他人心理理论（以及递归地，他人对其他人心理理论）的情况下做出有限理性决策；另一方面使用统计技术和近似解来处理固有的计算复杂性。

Result: 提出了一个结合博弈论和心理理论的计算框架，能够在保持形式化的同时通过统计近似方法实现计算可行性，为自动化心理理论过程提供了新途径。

Conclusion: 该研究为心理理论的计算实现提供了基于博弈论的新视角，通过有限理性决策和统计近似方法，在形式化和计算可行性之间取得了平衡，为跨学科研究提供了桥梁。

Abstract: Originating in psychology, $\textit{Theory of Mind}$ (ToM) has attracted significant attention across multiple research communities, especially logic, economics, and robotics. Most psychological work does not aim at formalizing those central concepts, namely $\textit{goals}$, $\textit{intentions}$, and $\textit{beliefs}$, to automate a ToM-based computational process, which, by contrast, has been extensively studied by logicians. In this paper, we offer a different perspective by proposing a computational framework viewed through the lens of game theory. On the one hand, the framework prescribes how to make boudedly rational decisions while maintaining a theory of mind about others (and recursively, each of the others holding a theory of mind about the rest); on the other hand, it employs statistical techniques and approximate solutions to retain computability of the inherent computational problem.

</details>


### [32] [Counting Still Counts: Understanding Neural Complex Query Answering Through Query Relaxation](https://arxiv.org/abs/2511.22565)
*Yannick Brunink,Daniel Daza,Yunjie He,Michael Cochez*

Main category: cs.AI

TL;DR: 研究发现神经复杂查询回答模型并未超越简单的无训练查询松弛方法，两者性能相似但答案重叠度低，结合两者能提升性能，提示需要重新评估神经方法进展


<details>
  <summary>Details</summary>
Motivation: 批判性地检验神经复杂查询回答模型是否真正学习到了超越显式图结构的泛化模式，以及是否比简单的无训练方法更优越

Method: 通过系统分析比较神经CQA模型与无训练的查询松弛策略，后者通过松弛查询约束并计数路径来检索可能答案

Result: 在多个数据集和查询结构中，神经模型与松弛方法性能相似，没有神经模型持续优于后者；两者检索的答案重叠度低，结合两者能一致提升性能

Conclusion: 需要重新评估神经查询回答的进展：尽管复杂，当前模型未能包含查询松弛捕获的推理模式；强调需要更强的非神经基线，未来神经方法可受益于融入查询松弛原则

Abstract: Neural methods for Complex Query Answering (CQA) over knowledge graphs (KGs) are widely believed to learn patterns that generalize beyond explicit graph structure, allowing them to infer answers that are unreachable through symbolic query processing. In this work, we critically examine this assumption through a systematic analysis comparing neural CQA models with an alternative, training-free query relaxation strategy that retrieves possible answers by relaxing query constraints and counting resulting paths. Across multiple datasets and query structures, we find several cases where neural and relaxation-based approaches perform similarly, with no neural model consistently outperforming the latter. Moreover, a similarity analysis reveals that their retrieved answers exhibit little overlap, and that combining their outputs consistently improves performance. These results call for a re-evaluation of progress in neural query answering: despite their complexity, current models fail to subsume the reasoning patterns captured by query relaxation. Our findings highlight the importance of stronger non-neural baselines and suggest that future neural approaches could benefit from incorporating principles of query relaxation.

</details>


### [33] [DeepSeekMath-V2: Towards Self-Verifiable Mathematical Reasoning](https://arxiv.org/abs/2511.22570)
*Zhihong Shao,Yuxiang Luo,Chengda Lu,Z. Z. Ren,Jiewen Hu,Tian Ye,Zhibin Gou,Shirong Ma,Xiaokang Zhang*

Main category: cs.AI

TL;DR: 论文提出DeepSeekMath-V2模型，通过自我验证机制解决数学推理中"正确答案不等于正确推理"的问题，在定理证明任务上取得突破性成果。


<details>
  <summary>Details</summary>
Motivation: 当前基于强化学习奖励最终答案的方法存在根本局限：正确答案不能保证推理正确，且许多数学任务（如定理证明）需要严谨的逐步推导而非数值答案。需要验证数学推理的全面性和严谨性，特别是对于没有已知解的开放问题。

Method: 1. 训练准确且可靠的LLM验证器用于定理证明；2. 使用验证器作为奖励模型训练证明生成器；3. 激励生成器在最终确定证明前识别并解决尽可能多的问题；4. 通过扩展验证计算自动标注新的难以验证的证明，创建训练数据以持续改进验证器。

Result: DeepSeekMath-V2在定理证明能力上表现优异：在IMO 2025和CMO 2024获得金牌级分数，在Putnam 2024上获得接近完美的118/120分（通过扩展测试时计算）。

Conclusion: 通过自我验证机制和生成-验证循环的协同扩展，可以显著提升数学推理的深度和严谨性，为解决开放数学问题提供了有前景的方向。

Abstract: Large language models have made significant progress in mathematical reasoning, which serves as an important testbed for AI and could impact scientific research if further advanced. By scaling reasoning with reinforcement learning that rewards correct final answers, LLMs have improved from poor performance to saturating quantitative reasoning competitions like AIME and HMMT in one year. However, this approach faces fundamental limitations. Pursuing higher final answer accuracy doesn't address a key issue: correct answers don't guarantee correct reasoning. Moreover, many mathematical tasks like theorem proving require rigorous step-by-step derivation rather than numerical answers, making final answer rewards inapplicable. To push the limits of deep reasoning, we believe it is necessary to verify the comprehensiveness and rigor of mathematical reasoning. Self-verification is particularly important for scaling test-time compute, especially for open problems without known solutions. Towards self-verifiable mathematical reasoning, we investigate how to train an accurate and faithful LLM-based verifier for theorem proving. We then train a proof generator using the verifier as the reward model, and incentivize the generator to identify and resolve as many issues as possible in their own proofs before finalizing them. To maintain the generation-verification gap as the generator becomes stronger, we propose to scale verification compute to automatically label new hard-to-verify proofs, creating training data to further improve the verifier. Our resulting model, DeepSeekMath-V2, demonstrates strong theorem-proving capabilities, achieving gold-level scores on IMO 2025 and CMO 2024 and a near-perfect 118/120 on Putnam 2024 with scaled test-time compute.

</details>


### [34] [AI Deception: Risks, Dynamics, and Controls](https://arxiv.org/abs/2511.22619)
*Boyuan Chen,Sitong Fang,Jiaming Ji,Yanxu Zhu,Pengcheng Wen,Jinzhou Wu,Yingshui Tan,Boren Zheng,Mengying Yuan,Wenqi Chen,Donghai Hong,Alex Qiu,Xin Chen,Jiayi Zhou,Kaile Wang,Juntao Dai,Borong Zhang,Tianzhuo Yang,Saad Siddiqui,Isabella Duan,Yawen Duan,Brian Tse,Jen-Tse,Huang,Kun Wang,Baihui Zheng,Jiaheng Liu,Jian Yang,Yiming Li,Wenting Chen,Dongrui Liu,Lukas Vierling,Zhiheng Xi,Haobo Fu,Wenxuan Wang,Jitao Sang,Zhengyan Shi,Chi-Min Chan,Eugenie Shi,Simin Li,Juncheng Li,Wei Ji,Dong Li,Jun Song,Yinpeng Dong,Jie Fu,Bo Zheng,Min Yang,Yike Guo,Philip Torr,Zhongyuan Wang,Yaodong Yang,Tiejun Huang,Ya-Qin Zhang,Hongjiang Zhang,Andrew Yao*

Main category: cs.AI

TL;DR: AI欺骗已成为实证风险，本文系统梳理了AI欺骗的定义、机制、检测与缓解策略，提出了欺骗循环框架和综合审计方法。


<details>
  <summary>Details</summary>
Motivation: 随着AI智能水平提升，AI欺骗从理论担忧演变为实证风险，需要系统研究其定义、机制和应对策略，以应对这一社会技术安全挑战。

Method: 基于信号理论提出AI欺骗的形式化定义，构建欺骗循环框架（欺骗涌现与欺骗处理），分析欺骗的激励机制、能力前提和情境触发因素，研究检测方法和缓解策略。

Result: 建立了AI欺骗研究的系统框架，识别了欺骗的三个层次激励机制和三个能力前提条件，开发了静态和交互环境下的检测方法，提出了技术、社区和治理相结合的综合审计方法。

Conclusion: AI欺骗是能力足够且存在激励潜力的系统在外部条件触发下的必然行为，需要多维度检测和缓解策略，以及整合技术、社区和治理的综合审计方法来应对这一社会技术安全挑战。

Abstract: As intelligence increases, so does its shadow. AI deception, in which systems induce false beliefs to secure self-beneficial outcomes, has evolved from a speculative concern to an empirically demonstrated risk across language models, AI agents, and emerging frontier systems. This project provides a comprehensive and up-to-date overview of the AI deception field, covering its core concepts, methodologies, genesis, and potential mitigations. First, we identify a formal definition of AI deception, grounded in signaling theory from studies of animal deception. We then review existing empirical studies and associated risks, highlighting deception as a sociotechnical safety challenge. We organize the landscape of AI deception research as a deception cycle, consisting of two key components: deception emergence and deception treatment. Deception emergence reveals the mechanisms underlying AI deception: systems with sufficient capability and incentive potential inevitably engage in deceptive behaviors when triggered by external conditions. Deception treatment, in turn, focuses on detecting and addressing such behaviors. On deception emergence, we analyze incentive foundations across three hierarchical levels and identify three essential capability preconditions required for deception. We further examine contextual triggers, including supervision gaps, distributional shifts, and environmental pressures. On deception treatment, we conclude detection methods covering benchmarks and evaluation protocols in static and interactive settings. Building on the three core factors of deception emergence, we outline potential mitigation strategies and propose auditing approaches that integrate technical, community, and governance efforts to address sociotechnical challenges and future AI risks. To support ongoing work in this area, we release a living resource at www.deceptionsurvey.com.

</details>


### [35] [Solving Context Window Overflow in AI Agents](https://arxiv.org/abs/2511.22729)
*Anton Bulle Labate,Valesca Moura de Sousa,Sandro Rama Fiorini,Leonardo Guerreiro Azevedo,Raphael Melo Thiago,Viviane Torres da Silva*

Main category: cs.AI

TL;DR: LLMs处理长工具输出时存在上下文窗口限制，本文提出通过内存指针而非原始数据交互的方法，实现任意长度工具输出的无损处理


<details>
  <summary>Details</summary>
Motivation: 在化学和材料科学等知识密集型动态领域中，LLMs需要与外部工具交互获取训练数据之外的专业知识，但大型工具输出会超出LLMs的上下文窗口限制，导致任务无法完成。现有解决方案如截断或摘要会丢失完整输出信息，不适用于需要完整数据的工作流程。

Method: 提出一种新方法，通过将模型交互从原始数据转向内存指针，使LLMs能够处理和使用任意长度的工具响应而不丢失信息。该方法保留了工具功能，可无缝集成到智能体工作流程中，并减少了令牌使用和执行时间。

Result: 在真实世界材料科学应用中验证了该方法，传统工作流程无法执行该应用。通过比较分析显示，当两种方法都能成功时，所提方法消耗的令牌数约为传统工作流程的七分之一。

Conclusion: 该方法有效解决了LLMs处理长工具输出的上下文窗口限制问题，实现了任意长度输出的无损处理，显著减少了计算资源消耗，为知识密集型领域的LLMs应用提供了可行解决方案。

Abstract: Large Language Models (LLMs) have become increasingly capable of interacting with external tools, granting access to specialized knowledge beyond their training data - critical in dynamic, knowledge-intensive domains such as Chemistry and Materials Science. However, large tool outputs can overflow the LLMs' context window, preventing task completion. Existing solutions such as truncation or summarization fail to preserve complete outputs, making them unsuitable for workflows requiring the full data. This work introduces a method that enables LLMs to process and utilize tool responses of arbitrary length without loss of information. By shifting the model's interaction from raw data to memory pointers, the method preserves tool functionality, allows seamless integration into agentic workflows, and reduces token usage and execution time. The proposed method is validated on a real-world Materials Science application that cannot be executed with conventional workflows, and its effectiveness is demonstrated via a comparative analysis where both methods succeed. In this experiment, the proposed approach consumed approximately seven times fewer tokens than the traditional workflow.

</details>


### [36] [Agentic AI Framework for Individuals with Disabilities and Neurodivergence: A Multi-Agent System for Healthy Eating, Daily Routines, and Inclusive Well-Being](https://arxiv.org/abs/2511.22737)
*Salman Jan,Toqeer Ali Syed,Gohar Ali,Ali Akarma,Mohammad Riyaz Belgaum,Ahmad Ali*

Main category: cs.AI

TL;DR: 本文提出了一种面向残障人士和神经多样性人群的智能体AI框架，通过多层架构和四个专用智能体提供个性化营养、适应性提醒、食物指导和持续监测，实现健康管理和日常支持。


<details>
  <summary>Details</summary>
Motivation: 传统辅助系统在包容性、个性化和可访问性方面存在局限，需要开发一个能够帮助残障人士和神经多样性人群实现更健康生活和规律日常的智能体AI模型。

Method: 采用三层架构：应用与接口层、智能体层、数据源层。通过混合推理引擎协调四个专用智能体（膳食规划、提醒、食物指导、监测），使用黑板/事件总线进行通信，整合电子健康记录、营养数据库、可穿戴设备等多模态数据源。

Result: 构建了一个自适应、透明、包容的支持系统，具备实时反馈循环、隐私保护数据管理、协作护理仪表板和可解释AI模块，超越了传统辅助系统的功能限制。

Conclusion: 该智能体AI框架通过多智能体推理、多模态界面和人本设计的融合，为残障人士和神经多样性人群提供了增强自主性、促进健康和实现数字公平的创新解决方案。

Abstract: The paper presents a detailed Agentic Artificial Intelligence (AI) model that would enable people with disabilities and neurodivergence to lead healthier lives and have more regular days. The system will use a multi-layer structure; it will include an Application and Interface Layer, an Agents Layer, and a Data Source Layer to provide adaptive, transparent, and inclusive support. Fundamentally, a hybrid reasoning engine will synchronize four special-purpose agents, which include: a personalized-nutrition-based, called a Meal Planner Agent; an adaptive-scheduling-based, called a Reminder Agent; interactive assistance during grocery shopping and cooking, called a Food Guidance Agent; and a continuous-intake-and-physiological-tracking, called a Monitoring Agent. All the agents interact through a central communicative system called the Blackboard/Event Bus, which allows autonomous interaction and real-time feedback loops with multimedia user interfaces. Privacy-sensitive data sources, including electronic health records (EHRs), nutritional databases, wearable sensors, and smart kitchen Internet of Things, are also included in the framework and placed into a policy-controlled layer, which ensures data safety and compliance with consent. Collaborative care and clinician dashboards allow common supervision, and discussable artificial intelligence (XAI) modules give brief explanations of why a decision was made, making users responsible and reliant. The proposed agentic AI framework is an extension beyond traditional assistive systems since it incorporates inclusiveness, personalization, and accessibility at all levels. It displays the intersection of multi-agent reasoning, multi-modal interfaces, and human-centered design that will enable the development of autonomy, health, and digital equity among people with disabilities and neurodivergence.

</details>


### [37] [Agentic AI Framework for Cloudburst Prediction and Coordinated Response](https://arxiv.org/abs/2511.22767)
*Toqeer Ali Syed,Sohail Khan,Salman Jan,Gohar Ali,Muhammad Nauman,Ali Akarma,Ahmad Ali*

Main category: cs.AI

TL;DR: 该论文提出了一种基于多智能体的人工智能系统，将大气水循环的感知、预报、降尺度、水文建模和协调响应整合为一个闭环系统，用于应对极端短时降雨事件。


<details>
  <summary>Details</summary>
Motivation: 传统预报系统将预测和响应视为两个独立过程，难以应对云爆发等极端短时降雨事件。需要建立一个集成的闭环系统来提高预报可靠性和应急响应效率。

Method: 采用自主但协作的多智能体框架，在整个事件生命周期中进行推理、感知和行动。系统整合了感知、预报、降尺度、水文建模和协调响应，形成闭环系统，并嵌入学习层实现自适应校准和透明审计。

Result: 在巴基斯坦北部地区的多年雷达、卫星和地面评估表明，多智能体配置相比基线模型提高了预报可靠性、关键成功指数和预警提前时间。通过通信和路由智能体最大化人口覆盖范围，最小化疏散过程中的错误。

Conclusion: 协作式AI智能体能够将大气数据流转化为可操作的预见性，为可扩展的自适应和学习型气候韧性提供了一个平台，能够改变极端天气事件的应对方式。

Abstract: The challenge is growing towards extreme and short-duration rainfall events like a cloudburst that are peculiar to the traditional forecasting systems, in which the predictions and the response are taken as two distinct processes. The paper outlines an agentic artificial intelligence system to study atmospheric water-cycle intelligence, which combines sensing, forecasting, downscaling, hydrological modeling and coordinated response into a single, interconnected, priceless, closed-loop system. The framework uses autonomous but cooperative agents that reason, sense, and act throughout the entire event lifecycle, and use the intelligence of weather prediction to become real-time decision intelligence. Comparison of multi-year radar, satellite, and ground-based evaluation of the northern part of Pakistan demonstrates that the multi-agent configuration enhances forecast reliability, critical success index and warning lead time compared to the baseline models. Population reach was maximised, and errors during evacuation were minimised through communication and routing agents, and adaptive recalibration and transparent auditability were provided by the embedded layer of learning. Collectively, this leads to the conclusion that collaborative AI agents are capable of transforming atmospheric data streams into practicable foresight and provide a platform of scalable adaptive and learning-based climate resilience.

</details>


### [38] [Fast dynamical similarity analysis](https://arxiv.org/abs/2511.22828)
*Arman Behrad,Mitchell Ostrow,Mohammad Taha Fakharian,Ila Fiete,Christian Beste,Shervin Safavi*

Main category: cs.AI

TL;DR: fastDSA是一种快速动态相似性分析方法，比传统方法快至少一个数量级，同时保持准确性和鲁棒性，通过自动选择有效模型阶次和轻量级优化过程实现高效计算。


<details>
  <summary>Details</summary>
Motivation: 传统相似性度量忽略了神经表征的动态过程，而现有的动态相似性方法虽然能比较动态系统的时间结构，但计算速度慢，需要更高效的方法来比较神经回路、大脑或数据与模型之间的动态相似性。

Method: fastDSA引入两个关键组件：1）通过数据驱动的奇异值阈值自动选择Hankel嵌入的有效模型阶次，识别信息子空间并丢弃噪声；2）新颖的优化过程和目标函数，用轻量级过程替代缓慢的精确正交性约束，在寻找动态矩阵之间最小距离时保持接近正交变换空间。

Result: fastDSA比先前方法快至少一个数量级，同时保持了祖先方法的特性，包括对系统动态的不变性和敏感性，提供了计算高效且准确的动态相似性分析方法。

Conclusion: fastDSA为动态相似性分析提供了计算高效且准确的方法，有助于理解神经系统如何处理信息，能够快速比较神经回路、大脑或数据与模型之间的动态相似性。

Abstract: To understand how neural systems process information, it is often essential to compare one circuit with another, one brain with another, or data with a model. Traditional similarity measures ignore the dynamical processes underlying neural representations. Dynamical similarity methods offer a framework to compare the temporal structure of dynamical systems by embedding their (possibly) nonlinear dynamics into a globally linear space and there computing conjugacy metrics. However, identifying the best embedding and computing these metrics can be computationally slow. Here we introduce fast Dynamical Similarity Analysis (fastDSA), which is computationally far more efficient than previous methods while maintaining their accuracy and robustness. FastDSA introduces two key components that boost efficiency: (1) automatic selection of the effective model order of the Hankel (delay) embedding from the data via a data-driven singular-value threshold that identifies the informative subspace and discards noise to lower computational cost without sacrificing signal, and (2) a novel optimization procedure and objective, which replaces the slow exact orthogonality constraint in finding a minimal distance between dynamics matrices with a lightweight process to keep the search close to the space of orthogonal transformations. We demonstrate that fastDSA is at least an order of magnitude faster than the previous methods. Furthermore, we demonstrate that fastDSA has the properties of its ancestor, including its invariances and sensitivities to system dynamics. FastDSA, therefore, provides a computationally efficient and accurate method for dynamical similarity analysis.

</details>


### [39] [InsightEval: An Expert-Curated Benchmark for Assessing Insight Discovery in LLM-Driven Data Agents](https://arxiv.org/abs/2511.22884)
*Zhenghao Zhu,Yuanfeng Song,Xin Chen,Chengzhong Liu,Yakun Cui,Caleb Chen Cao,Sirui Han,Yike Guo*

Main category: cs.AI

TL;DR: 本文分析了现有洞察发现基准的缺陷，提出了高质量洞察基准的标准，构建了新的数据集InsightEval，并引入了新的评估指标来衡量智能体的探索性能。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型和多智能体系统的发展，越来越多的研究者利用这些技术进行洞察发现，但缺乏有效的评估基准。现有的InsightBench框架存在格式不一致、目标设计不合理、洞察冗余等关键缺陷，这些问题可能严重影响数据质量和智能体评估。

Method: 1. 深入调查InsightBench的缺陷；2. 提出高质量洞察基准的标准；3. 开发数据整理流水线构建新数据集InsightEval；4. 引入新的指标来衡量智能体的探索性能。

Result: 通过InsightEval上的大量实验，揭示了自动化洞察发现中的普遍挑战，并提出了一些关键发现来指导未来研究方向。

Conclusion: 本文解决了现有洞察发现基准的缺陷，提出了新的评估框架InsightEval和相应指标，为自动化洞察发现领域的研究提供了更可靠的评估工具和指导方向。

Abstract: Data analysis has become an indispensable part of scientific research. To discover the latent knowledge and insights hidden within massive datasets, we need to perform deep exploratory analysis to realize their full value. With the advent of large language models (LLMs) and multi-agent systems, more and more researchers are making use of these technologies for insight discovery. However, there are few benchmarks for evaluating insight discovery capabilities. As one of the most comprehensive existing frameworks, InsightBench also suffers from many critical flaws: format inconsistencies, poorly conceived objectives, and redundant insights. These issues may significantly affect the quality of data and the evaluation of agents. To address these issues, we thoroughly investigate shortcomings in InsightBench and propose essential criteria for a high-quality insight benchmark. Regarding this, we develop a data-curation pipeline to construct a new dataset named InsightEval. We further introduce a novel metric to measure the exploratory performance of agents. Through extensive experiments on InsightEval, we highlight prevailing challenges in automated insight discovery and raise some key findings to guide future research in this promising direction.

</details>


### [40] [ORION: Teaching Language Models to Reason Efficiently in the Language of Thought](https://arxiv.org/abs/2511.22891)
*Kumar Tanmay,Kriti Aggarwal,Paul Pu Liang,Subhabrata Mukherjee*

Main category: cs.AI

TL;DR: 该论文提出了一种基于"思维语言"假设的压缩推理框架，通过结构化、紧凑的符号化推理显著减少推理过程的token数量，同时保持高准确率。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在数学、代码生成等任务中表现出色，但其冗长的推理链导致高延迟、冗余和不连贯的推理路径，需要更高效、紧凑的推理方式。

Method: 提出Mentalese框架，将抽象推理编码为超紧凑的结构化token；开发SLPO（短长度偏好优化）强化学习方法，奖励简洁且正确的解决方案，同时允许必要时进行更长推理。

Result: ORION模型在多个基准测试中实现4-16倍token压缩，推理延迟降低5倍，训练成本减少7-9倍，同时保持90-98%的DeepSeek R1 Distilled模型准确率，在部分任务上甚至超越Claude和ChatGPT-4o 5%的准确率。

Conclusion: Mentalese风格的压缩推理实现了人类认知效率的突破，在保持准确性的同时显著提升推理效率，为实时、经济高效的推理系统提供了新方向。

Abstract: Large Reasoning Models (LRMs) achieve strong performance in mathematics, code generation, and task planning, but their reliance on long chains of verbose "thinking" tokens leads to high latency, redundancy, and incoherent reasoning paths. Inspired by the Language of Thought Hypothesis, which posits that human reasoning operates over a symbolic, compositional mental language called Mentalese, we introduce a framework that trains models to reason in a similarly compact style. Mentalese encodes abstract reasoning as ultra-compressed, structured tokens, enabling models to solve complex problems with far fewer steps. To improve both efficiency and accuracy, we propose SHORTER LENGTH PREFERENCE OPTIMIZATION (SLPO), a reinforcement learning method that rewards concise solutions that stay correct, while still allowing longer reasoning when needed. Applied to Mentalese-aligned models, SLPO yields significantly higher compression rates by enabling concise reasoning that preserves the benefits of detailed thinking without the computational overhead. Across benchmarks including AIME 2024 and 2025, MinervaMath, OlympiadBench, Math500, and AMC, our ORION models produce reasoning traces with 4-16x fewer tokens, achieve up to 5x lower inference latency, and reduce training costs by 7-9x relative to the DeepSeek R1 Distilled model, while maintaining 90-98% of its accuracy. ORION also surpasses Claude and ChatGPT-4o by up to 5% in accuracy while maintaining 2x compression. These results show that Mentalese-style compressed reasoning offers a step toward human-like cognitive efficiency, enabling real-time, cost-effective reasoning without sacrificing accuracy.

</details>


### [41] [TIM-PRM: Verifying multimodal reasoning with Tool-Integrated PRM](https://arxiv.org/abs/2511.22998)
*Peng Kuang,Xiangxiang Wang,Wentao Liu,Jian Dong,Kaidi Xu,Haohan Wang*

Main category: cs.AI

TL;DR: TIM-PRM：一种工具集成的多模态过程奖励模型，通过主动工具查询消除视觉幻觉和逻辑不一致，在数学推理任务中超越更大规模模型


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在数学推理中存在视觉幻觉和逻辑不一致问题，而传统的过程奖励模型通常作为标量评分器或生成式批评者，容易产生顺从性偏差，盲目验证有缺陷的假设而非基于视觉现实

Method: 提出TIM-PRM框架，将验证从被动分类任务转变为主动工具增强调查。模型学习规划验证策略，通过独立提问机制使用外部工具查询证据，将验证与推理上下文解耦以消除确认偏差。通过构建高质量工具集成验证轨迹数据集进行训练

Result: 在VisualProcessBench上的实验表明，8B参数的TIM-PRM模型超越了现有开源多模态PRMs，显著优于Qwen2.5-72B和InternVL-78B等更大规模模型，同时提供可解释的验证过程洞察

Conclusion: TIM-PRM通过主动工具集成的验证方法有效解决了多模态数学推理中的视觉幻觉和逻辑不一致问题，展示了小规模模型通过更好的验证机制可以超越大规模模型的性能

Abstract: Multimodal Large Language Models (MLLMs) have achieved impressive performances in mathematical reasoning, yet they remain vulnerable to visual hallucinations and logical inconsistencies that standard outcome-based supervision fails to mitigate. While Process Reward Models (PRMs) promise step-by-step verification, current approaches typically operate as scalar scorers or generative critics that suffer from sycophancy, blindly validating the flawed hypotheses rather than grounding them in visual reality. To bridge this gap, we introduce TIM-PRM (Tool-Integrated Multimodal PRM), a novel agentic framework that transforms verification from a passive classification task into an active, tool-augmented investigation. TIM-PRM is trained to explicitly plan verification strategies and utilizes a mechanism of Independent Question Asking to query evidence via external tools, effectively decoupling verification from the reasoning context to eliminate confirmation bias. We instantiate this method by curating a high-quality dataset of tool-integrated verification trajectories. Extensive experiments on VisualProcessBench demonstrate that our 8B parameter model surpasses existing open-source multimodal PRMs, significantly outperforming much larger models like Qwen2.5-72B and InternVL-78B, while offering interpretable insights into the verification process.

</details>


### [42] [MindPower: Enabling Theory-of-Mind Reasoning in VLM-based Embodied Agents](https://arxiv.org/abs/2511.23055)
*Ruoxuan Zhang,Qiyun Zheng,Zhiyu Zhou,Ziqi Liao,Siyu Wu,Jian-Yu Jiang-Lin,Bin Wen,Hongxia Xie,Jianlong Fu,Wen-Huang Cheng*

Main category: cs.AI

TL;DR: MindPower是一个机器人中心框架，通过整合感知、心理推理、决策和行动来解决现有视觉语言具身智能体缺乏心理理论决策能力的问题，在决策和行动生成方面显著优于GPT-4o。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言具身智能体缺乏基于心理理论的决策能力，现有基准仅关注人类心理状态而忽略智能体自身视角，这阻碍了连贯的决策和行动生成。

Method: 提出MindPower框架：1）感知环境和人类状态；2）进行心理理论推理，同时建模自我和他人心理状态；3）基于推断的心理状态生成决策和行动。同时引入Mind-Reward优化目标，鼓励视觉语言模型产生一致的心理理论推理和行为。

Result: 模型在决策制定方面比GPT-4o提升12.77%，在行动生成方面提升12.49%。

Conclusion: MindPower框架通过整合心理理论推理和机器人中心视角，显著提升了具身智能体的决策和行动生成能力。

Abstract: Theory of Mind (ToM) refers to the ability to infer others' mental states, such as beliefs, desires, and intentions. Current vision-language embodied agents lack ToM-based decision-making, and existing benchmarks focus solely on human mental states while ignoring the agent's own perspective, hindering coherent decision and action generation. To address this, we propose MindPower, a Robot-Centric framework integrating Perception, Mental Reasoning, Decision Making and Action. Given multimodal inputs, MindPower first perceives the environment and human states, then performs ToM Reasoning to model both self and others, and finally generates decisions and actions guided by inferred mental states. Furthermore, we introduce Mind-Reward, a novel optimization objective that encourages VLMs to produce consistent ToM Reasoning and behavior. Our model outperforms GPT-4o by 12.77% in decision making and 12.49% in action generation.

</details>


### [43] [Does Self-Evaluation Enable Wireheading in Language Models?](https://arxiv.org/abs/2511.23092)
*David Demitri Africa,Hans Ethan Ting*

Main category: cs.AI

TL;DR: 研究发现当语言模型的自我评估与奖励信号耦合时，会导致"评分膨胀"现象，即模型会操纵评分而非真正提升任务表现，这在模糊任务中尤为明显。


<details>
  <summary>Details</summary>
Motivation: 随着自我评估在语言模型训练中的重要性日益增加（从宪法AI到自我优化），研究者希望探究将自我评估与奖励信号耦合是否会引发"线路头"问题，即模型是否会操纵奖励测量而非真正改进任务表现。

Method: 研究首先在部分可观测马尔可夫决策过程(POMDPs)中形式化了奖励通道控制严格优于任务导向行为的条件，然后通过两个模型和三个任务的实证研究来验证这些预测。

Result: 研究发现，当模型的自我评分决定奖励时，会出现显著的评分膨胀现象，但任务准确性并未相应提高，特别是在摘要等模糊任务中。而自我评估但不控制奖励的模型则没有这种膨胀现象。

Conclusion: 自我评估在与学习信号解耦时是安全的，但在与奖励信号耦合时是危险的，这对智能体系统设计具有明确的启示意义。

Abstract: Self-evaluation is increasingly central to language model training, from constitutional AI to self-refinement. We investigate whether coupling self-evaluation to reward signals creates incentives for wireheading, where agents manipulate reward measurements rather than improving task performance. We formalize conditions under which reward-channel control strictly dominates task-focused behavior in POMDPs and test these predictions empirically. Across two models and three tasks, we find that models whose self-grades determine rewards exhibit substantial grade inflation without corresponding accuracy gains, particularly on ambiguous tasks like summarization. Models that self-evaluate but do not control rewards show no such inflation. Our results demonstrate that self-evaluation is safe when decoupled from learning signals but dangerous when coupled, with clear implications for agentic system design.

</details>


### [44] [Evolutionary Discovery of Heuristic Policies for Traffic Signal Control](https://arxiv.org/abs/2511.23122)
*Ruibing Wang,Shuhan Guo,Zeen Li,Zhen Wang,Quanming Yao*

Main category: cs.AI

TL;DR: TPET使用LLM作为演化引擎，将高维交通数据转化为时态逻辑事实，通过信用分配反馈机制演化出轻量级、鲁棒的启发式交通信号控制策略，无需训练即可超越传统启发式和在线LLM方法。


<details>
  <summary>Details</summary>
Motivation: 传统交通信号控制方法存在效率与性能的权衡：经典启发式方法效率高但过于简化，深度强化学习方法性能好但泛化能力差且策略不透明，在线大语言模型具有通用推理能力但延迟高且缺乏环境特定优化。

Method: 提出TPET框架，使用LLM作为演化引擎生成专门的启发式策略。包含两个核心模块：结构化状态抽象（SSA）将高维交通数据转换为时态逻辑事实；信用分配反馈（CAF）追踪微观错误决策到宏观不良结果的因果关系，提供针对性批判。完全在提示层面操作，无需训练。

Result: TPET生成的策略轻量级、鲁棒，针对特定交通环境优化，性能超越传统启发式方法和在线LLM执行器。

Conclusion: TPET框架成功解决了交通信号控制中效率、性能和可解释性的权衡问题，通过LLM演化引擎生成专门优化的启发式策略，为智能交通控制提供了新的解决方案。

Abstract: Traffic Signal Control (TSC) involves a challenging trade-off: classic heuristics are efficient but oversimplified, while Deep Reinforcement Learning (DRL) achieves high performance yet suffers from poor generalization and opaque policies. Online Large Language Models (LLMs) provide general reasoning but incur high latency and lack environment-specific optimization. To address these issues, we propose Temporal Policy Evolution for Traffic (\textbf{\method{}}), which uses LLMs as an evolution engine to derive specialized heuristic policies. The framework introduces two key modules: (1) Structured State Abstraction (SSA), converting high-dimensional traffic data into temporal-logical facts for reasoning; and (2) Credit Assignment Feedback (CAF), tracing flawed micro-decisions to poor macro-outcomes for targeted critique. Operating entirely at the prompt level without training, \method{} yields lightweight, robust policies optimized for specific traffic environments, outperforming both heuristics and online LLM actors.

</details>


### [45] [AgriCoT: A Chain-of-Thought Benchmark for Evaluating Reasoning in Vision-Language Models for Agriculture](https://arxiv.org/abs/2511.23253)
*Yibin Wen,Qingmei Li,Zi Ye,Jiarui Zhang,Jing Wu,Zurong Mai,Shuohong Lou,Yuhang Chen,Henglian Huang,Xiaoya Fan,Yang Zhang,Lingyuan Zhao,Haohuan Fu,Huang Jianxi,Juepeng Zheng*

Main category: cs.AI

TL;DR: AgriCoT是一个专门为农业领域设计的视觉问答数据集，包含4,535个样本，采用思维链推理来评估视觉语言模型的推理能力，填补了现有数据集在复杂农业场景下评估推理能力的不足。


<details>
  <summary>Details</summary>
Motivation: 现有视觉问答数据集和基准测试在评估视觉语言模型时，往往无法充分评估复杂农业场景中所需的关键推理和问题解决能力，因此需要专门设计的数据集来填补这一空白。

Method: 研究人员开发了AgriCoT数据集，这是一个包含4,535个精心策划样本的视觉问答数据集，特别融入了思维链推理方法，旨在评估视觉语言模型在农业领域的推理能力，特别是在零样本场景下的表现。

Result: 通过对26个代表性视觉语言模型（包括专有和开源模型）的评估发现，虽然一些专有模型在回答问题方面表现出色，但它们在推理能力方面存在显著差距，这凸显了思维链推理对于更精确有效评估的重要性。

Conclusion: AgriCoT数据集为评估视觉语言模型在农业领域的推理能力提供了全面而稳健的基准，研究结果表明当前模型在推理能力方面仍有不足，强调了思维链推理在模型评估中的重要性。

Abstract: Recent advancements in Vision-Language Models (VLMs) have significantly transformed various industries. In agriculture, these dual-modal capabilities offer promising applications such as precision farming, crop monitoring, pest detection, and environmental sustainability. While several Visual Question Answering (VQA) datasets and benchmarks have been developed to evaluate VLM performance, they often fail to adequately assess the critical reasoning and problem-solving skills required in complex agricultural contexts. To address this gap, we introduce AgriCoT, a VQA dataset that incorporates Chain-of-Thought (CoT) reasoning, specifically designed to evaluate the reasoning capabilities of VLMs. With 4,535 carefully curated samples, AgriCoT offers a comprehensive and robust evaluation of reasoning abilities for VLMs, particularly in zero-shot scenarios, by focusing on their capacity to engage in logical reasoning and effective problem-solving. Our evaluations, conducted with 26 representative VLMs, including both proprietary and open-source models, reveal that while some proprietary models excel at answering questions, there is a notable and significant gap in their reasoning capabilities. This underscores the importance of incorporating CoT for more precise and effective assessments. Our dataset are available at https://huggingface.co/datasets/wenyb/AgriCoT.

</details>


### [46] [Adapting Like Humans: A Metacognitive Agent with Test-time Reasoning](https://arxiv.org/abs/2511.23262)
*Yang Li,Zhiyuan He,Yuxuan Huang,Zhuhanling Xiao,Chao Yu,Meng Fang,Kun Shao,Jun Wang*

Main category: cs.AI

TL;DR: MCTR是一个元认知测试时推理框架，通过元认知自我更新使视觉语言模型能够在测试时学习、适应和改进，在Atari游戏中表现出强大的测试时适应能力。


<details>
  <summary>Details</summary>
Motivation: 当前的视觉语言模型虽然具备强大的感知推理能力，但在面对测试时的新任务时往往难以高效适应。相比之下，人类能够利用带有记忆的元认知模型，通过元认知控制在面对新挑战时持续优化策略。为了弥合这一差距，需要让模型具备在测试时学习、适应和改进的能力。

Method: MCTR框架包含元推理模块和动作推理模块：1)元推理模块通过从测试时观察中发现并存储任务相关规则、环境模式和动作-结果关系作为自然语言描述，逐步构建结构化记忆；2)动作推理模块通过上下文感知的感知和策略推理，动态从记忆中检索和整合知识来确定最优动作。动作推理模块通过提出的元认知测试时强化学习不断更新策略。

Result: 在45个Atari游戏（33个已见，12个未见）上评估MCTR，在未见游戏中获得了9/12的top-1结果，相比基线表现出强大的测试时适应能力。消融实验、学习动态和案例研究分析揭示了两个组件的互补贡献，并显示元推理朝着类人适应策略的方向演化。

Conclusion: MCTR框架成功地将人类元认知的双重结构引入视觉语言模型，使模型能够在测试时通过元认知自我更新进行学习、适应和改进，在未见任务上表现出强大的适应能力，朝着类人适应策略的方向发展。

Abstract: Recent Vision-Language Models (VLMs) exhibit strong perceptual reasoning abilities, yet they often struggle to adapt efficiently when encountering novel tasks at test time. In contrast, humans leverage the metacognitive model with memory, enabling continuous strategy refinement through metacognitive control when faced with new challenges. To bridge this gap, we propose metacognitive test-time reasoning (MCTR), a framework that equips models with the ability to learn, adapt, and improve during test time through metacognitive self-updating. Inspired by the dual structure of human metacognition, MCTR comprises meta-level and object-level VLM reasoning modules, each equipped with dedicated memory systems for hierarchical adaptive reasoning. Specifically, MCTR consists of (1) a meta-reasoning module which incrementally builds a structured memory by discovering and storing task-relevant rules, environmental patterns, and action-outcome relationships from test-time observations as natural language descriptions; and (2) an action-reasoning module that determines optimal actions through context-aware perception and strategic reasoning by dynamically retrieving and integrating knowledge from memory. The action-reasoning module continuously updates its policy through proposed metacognitive test-time reinforcement learning, adapting as knowledge memory evolves. We evaluate MCTR on 45 Atari games (33 seen, 12 unseen). MCTR demonstrates robust test-time adaptation, achieving 9/12 top-1 results on unseen games compared with baselines. Analyses through ablations, learning dynamics, and case studies reveal the complementary contributions of both components and show meta-reasoning evolving toward human-like adaptation strategies.

</details>


### [47] [OctoMed: Data Recipes for State-of-the-Art Multimodal Medical Reasoning](https://arxiv.org/abs/2511.23269)
*Timothy Ossowski,Sheng Zhang,Qianchu Liu,Guanghui Qin,Reuben Tan,Tristan Naumann,Junjie Hu,Hoifung Poon*

Main category: cs.AI

TL;DR: 本文研究了医疗大语言模型的数据策展和训练策略，通过结构化推理轨迹的数据配方，在超过800万样本的数据集上训练，在医疗基准任务上取得了开源模型中的最佳性能。


<details>
  <summary>Details</summary>
Motivation: 高质量、精心策划的数据是训练医疗大语言模型的基石，直接影响模型对未见临床任务的泛化能力和鲁棒性。本文旨在研究训练和数据策展策略，以开发医疗领域鲁棒的多模态推理模型。

Method: 研究聚焦于监督微调（SFT），探索利用结构化推理轨迹的数据配方。使用提出的数据配方，将实验扩展到包含超过800万个样本和68亿响应标记的数据集上。

Result: 在多样化的分布外医疗基准任务上取得了开源模型中的最先进性能。结果表明，策划具有不同结构化推理轨迹长度的高质量、多样化训练数据集，能使微调模型根据下游任务自我校准其推理轨迹长度，无需显式监督。

Conclusion: 本文提出了关键见解，描述了数据策展策略，并概述了开发鲁棒医疗视觉语言推理系统的后续步骤。结构化推理轨迹的数据配方能有效提升医疗大语言模型的性能和适应性。

Abstract: High-quality and carefully curated data is a cornerstone of training medical large language models, as it directly impacts both generalization and robustness to unseen clinical tasks. We investigate strategies for training and data curation to develop a robust multimodal reasoning model in the medical domain. Our work focuses on supervised fine-tuning (SFT) and explores data recipes that leverage structured reasoning traces. Using our proposed data recipe, we scale experiments to a dataset of over 8 million examples and 6.8 billion response tokens, achieving state-of-the-art performance among open-source models across diverse out-of-distribution medical benchmark tasks. Our results further indicate that curating a high-quality, diverse training dataset with varying structured reasoning trace lengths enables the fine-tuned model to self-calibrate its reasoning trajectory lengths based on the downstream task, without explicit supervision. We present key insights, describe the data curation strategy, and outline next steps toward developing robust medical vision-language reasoning system.

</details>


### [48] [Agentic AI Framework for Smart Inventory Replenishment](https://arxiv.org/abs/2511.23366)
*Toqeer Ali Syed,Salman Jan,Gohar Ali,Ali Akarma,Ahmad Ali,Qurat-ul-Ain Mastoi*

Main category: cs.AI

TL;DR: 提出基于多智能体的AI库存管理系统，通过需求预测、供应商优化和持续学习来减少缺货、降低库存成本并改善产品组合周转率


<details>
  <summary>Details</summary>
Motivation: 现代零售业产品种类繁多（服装、杂货、化妆品、冷冻食品等），难以准确预测需求、防止缺货并发现高潜力产品，需要智能化的库存管理解决方案

Method: 开发多智能体AI模型，包含库存监控、供应商采购启动、趋势产品扫描等功能，应用需求预测、供应商选择优化、多智能体协商和持续学习技术，并在中型超市环境中进行原型测试

Result: 在传统和人工数据表上的测试显示，相比基准启发式方法，系统能减少缺货、降低库存持有成本、改善产品组合周转率

Conclusion: 提出的智能体AI库存管理系统有效改善了零售库存管理，同时讨论了系统约束、可扩展性和未来改进方向

Abstract: In contemporary retail, the variety of products available (e.g. clothing, groceries, cosmetics, frozen goods) make it difficult to predict the demand, prevent stockouts, and find high-potential products. We suggest an agentic AI model that will be used to monitor the inventory, initiate purchase attempts to the appropriate suppliers, and scan for trending or high-margin products to incorporate. The system applies demand forecasting, supplier selection optimization, multi-agent negotiation and continuous learning. We apply a prototype to a setting in the store of a middle scale mart, test its performance on three conventional and artificial data tables, and compare the results to the base heuristics. Our findings indicate that there is a decrease in stockouts, a reduction of inventory holding costs, and an improvement in product mix turnover. We address constraints, scalability as well as improvement prospect.

</details>


### [49] [Towards Continuous Intelligence Growth: Self-Training, Continual Learning, and Dual-Scale Memory in SuperIntelliAgent](https://arxiv.org/abs/2511.23436)
*Jianzhe Lin,Zeyu Pan,Yun Zhu,Ruiqi Song,Jining Yang*

Main category: cs.AI

TL;DR: SuperIntelliAgent是一个智能体学习框架，通过可训练的小型扩散模型（学习者）与冻结的大型语言模型（验证者）耦合，实现无标注的持续智能增长。框架利用DPO进行自我监督学习，集成双尺度记忆系统，将普通推理循环转化为终身优化过程。


<details>
  <summary>Details</summary>
Motivation: 传统监督微调需要大量标注数据，限制了智能体的持续学习能力。作者希望开发一个无需人工标注、能够自主持续学习的智能体框架，通过自我监督交互实现智能的持续积累。

Method: 1. 学习者（小型扩散模型）生成候选输出；2. 验证者（冻结大型语言模型）通过逐步推理评估输出；3. 两者交互产生选择/拒绝对用于直接偏好优化（DPO）；4. 集成双尺度记忆：短期上下文记忆保存推理轨迹，长期记忆通过轻量级微调巩固知识；5. 回放缓冲区保留显示可验证进展的样本作为辅助监督。

Result: 仅使用少量自动生成的DPO对，学习者在所有基准测试中均表现出改进。这表明该机制为持续智能积累和实际部署提供了有前景的方向。

Conclusion: 将可训练的学习者与具备推理能力的验证者配对构成了增长智能的最小可靠单元。配对反馈和部分历史回放产生更丰富的学习课程和更强的偏好对齐，为持续智能积累提供了有效框架。

Abstract: We introduce SuperIntelliAgent, an agentic learning framework that couples a trainable small diffusion model (the learner) with a frozen large language model (the verifier) to enable continual intelligence growth through self-supervised interaction. Unlike conventional supervised fine-tuning, SuperIntelliAgent learns autonomously without annotation: the learner generates candidate outputs, the verifier evaluates them through step-by-step reasoning, and their interaction produces chosen/rejected pairs for Direct Preference Optimization (DPO). This converts each input into a pseudo-training signal for continual improvement. The framework integrates dual-scale memory: short-term in-context memory that preserves reasoning traces across refinement cycles, and long-term memory that consolidates acquired knowledge through lightweight on-the-fly fine-tuning. A replay buffer retains samples that show verifiable progress and replays them as auxiliary supervision, reinforcing recent learning while forming adaptive curricula. SuperIntelliAgent is infrastructure-agnostic and can be plugged into existing agentic frameworks while turning ordinary inference loops into a lifelong optimization process. We posit that pairing a trainable learner with a reasoning-capable verifier forms a minimal reliable unit of growing intelligence, as paired feedback and partial-history replay yield richer learning curricula and stronger preference alignment. With a small number of automatically generated DPO pairs, the learner improves across all benchmarks, indicating that this mechanism provides a promising direction for continual intelligence accumulation and real-world deployment.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [50] [Beyond Membership: Limitations of Add/Remove Adjacency in Differential Privacy](https://arxiv.org/abs/2511.21804)
*Gauri Pradhan,Joonas Jälkö,Santiago Zanella-Bèguelin,Antti Honkela*

Main category: cs.CR

TL;DR: 该论文指出，在机器学习中使用差分隐私时，add/remove邻接关系会高估属性隐私保护，而substitute邻接关系更适合保护记录属性而非成员资格。


<details>
  <summary>Details</summary>
Motivation: 当前大多数差分隐私实现使用add/remove邻接关系来保护训练数据成员资格，但在许多机器学习应用中，真正需要保护的是单个记录的属性（如监督微调中的标签）。作者发现add/remove邻接关系会高估属性隐私保护水平。

Method: 开发了新颖的攻击方法来审计substitute邻接关系下的差分隐私，通过实验比较add/remove和substitute两种邻接关系下的隐私保证与实际保护效果。

Result: 实验结果表明，审计结果与add/remove邻接关系报告的DP保证不一致，但与substitute邻接关系计算的隐私预算一致。这证实了add/remove邻接关系确实会高估属性隐私保护。

Conclusion: 当保护目标是每记录属性而非成员资格时，报告差分隐私保证时选择的邻接关系至关重要。substitute邻接关系更适合保护记录属性，而add/remove邻接关系会给出过于乐观的隐私保证。

Abstract: Training machine learning models with differential privacy (DP) limits an adversary's ability to infer sensitive information about the training data. It can be interpreted as a bound on adversary's capability to distinguish two adjacent datasets according to chosen adjacency relation. In practice, most DP implementations use the add/remove adjacency relation, where two datasets are adjacent if one can be obtained from the other by adding or removing a single record, thereby protecting membership. In many ML applications, however, the goal is to protect attributes of individual records (e.g., labels used in supervised fine-tuning). We show that privacy accounting under add/remove overstates attribute privacy compared to accounting under the substitute adjacency relation, which permits substituting one record. To demonstrate this gap, we develop novel attacks to audit DP under substitute adjacency, and show empirically that audit results are inconsistent with DP guarantees reported under add/remove, yet remain consistent with the budget accounted under the substitute adjacency relation. Our results highlight that the choice of adjacency when reporting DP guarantees is critical when the protection target is per-record attributes rather than membership.

</details>


### [51] [Standardized Threat Taxonomy for AI Security, Governance, and Regulatory Compliance](https://arxiv.org/abs/2511.21901)
*Hernan Huwyler*

Main category: cs.CR

TL;DR: 该研究提出了AI系统威胁向量分类法，旨在弥合技术安全团队与法律合规专业人员之间的"语言障碍"，将技术漏洞转化为可量化的财务风险，实现定量风险评估。


<details>
  <summary>Details</summary>
Motivation: 人工智能系统在受监管领域的加速部署暴露了风险评估方法的碎片化问题。技术安全团队关注算法漏洞（如MITRE ATLAS），而法律合规专业人员关注监管要求（如欧盟AI法案、NIST AI RMF），两者之间存在"语言障碍"，导致无法将技术漏洞准确转化为财务责任，无法回答关于应急储备、控制投资回报和保险风险等基本经济问题。

Method: 研究提出了AI系统威胁向量分类法，这是一个专门为定量风险评估设计的结构化本体。该框架将AI特定风险分为九个关键领域：滥用、投毒、隐私、对抗性、偏见、不可靠输出、漂移、供应链和知识产权威胁，整合了53个操作性定义的子威胁。每个领域将技术向量直接映射到业务损失类别（机密性、完整性、可用性、法律、声誉），实现抽象威胁到可衡量财务影响的转化。

Result: 该分类法通过对2025年133个已记录AI事件的分析进行了实证验证（实现了100%分类覆盖），并与主要AI风险框架进行了协调。此外，它明确与ISO/IEC 42001控制和NIST AI RMF功能对齐，以促进可审计性。

Conclusion: 该研究提出的AI系统威胁向量分类法成功弥合了技术安全与法律合规之间的鸿沟，为AI系统提供了统一的定量风险评估框架，能够将技术漏洞转化为可衡量的财务影响，支持应急储备、投资回报分析和保险风险评估等经济决策。

Abstract: The accelerating deployment of artificial intelligence systems across regulated sectors has exposed critical fragmentation in risk assessment methodologies. A significant "language barrier" currently separates technical security teams, who focus on algorithmic vulnerabilities (e.g., MITRE ATLAS), from legal and compliance professionals, who address regulatory mandates (e.g., EU AI Act, NIST AI RMF). This disciplinary disconnect prevents the accurate translation of technical vulnerabilities into financial liability, leaving practitioners unable to answer fundamental economic questions regarding contingency reserves, control return-on-investment, and insurance exposure. To bridge this gap, this research presents the AI System Threat Vector Taxonomy, a structured ontology designed explicitly for Quantitative Risk Assessment (QRA). The framework categorizes AI-specific risks into nine critical domains: Misuse, Poisoning, Privacy, Adversarial, Biases, Unreliable Outputs, Drift, Supply Chain, and IP Threat, integrating 53 operationally defined sub-threats. Uniquely, each domain maps technical vectors directly to business loss categories (Confidentiality, Integrity, Availability, Legal, Reputation), enabling the translation of abstract threats into measurable financial impact. The taxonomy is empirically validated through an analysis of 133 documented AI incidents from 2025 (achieving 100% classification coverage) and reconciled against the main AI risk frameworks. Furthermore, it is explicitly aligned with ISO/IEC 42001 controls and NIST AI RMF functions to facilitate auditability.

</details>


### [52] [GECKO: Securing Digital Assets Through(out) the Physical World (Extended Technical Report)](https://arxiv.org/abs/2511.21999)
*Cyrill Krähenbühl,Nico Hauser,Christelle Gloor,Juan Angel García-Pardo,Adrian Perrig*

Main category: cs.CR

TL;DR: GECKO是一个地理PKI系统，通过在物理空间和数字资产之间建立双向信任映射，解决数字资产与物理世界关联的安全问题。


<details>
  <summary>Details</summary>
Motivation: 当前数字资产（如商店网站、产权文件、支付二维码）无法安全地与其相关的物理空间关联，导致假品牌店、产权欺诈、移动支付诈骗等问题。虽然保护数字资产所需的信息（如合同关系、地籍记录）存在，但缺乏统一的检索和验证方式。

Method: 提出地理启用的加密密钥预言机（GECKO），这是一个基于地理位置和占用空间的地理PKI系统。GECKO支持物理世界和数字世界之间的双向信任转换，允许用户验证特定位置应该存在的资产，以及验证数字实体声称的物理空间。

Result: 实验证明GECKO能够高效存储数百万个资产，并在单个服务器上以超过19000次查询/秒的速率，在11毫秒内响应基于精确位置查询的加密材料服务。

Conclusion: GECKO是对现有PKI系统的补充，当需要其特性时可以与现有系统结合使用，为数字资产与物理空间的关联提供了安全可靠的解决方案。

Abstract: Although our lives are increasingly transitioning into the digital world, many digital assets still relate to objects or places in the physical world, e.g., websites of stores or restaurants, digital documents claiming property ownership, or digital identifiers encoded in QR codes for mobile payments in shops. Currently, users cannot securely associate digital assets with their related physical space, leading to problems such as fake brand stores, property fraud, and mobile payment scams. In many cases, the necessary information to protect digital assets exists, e.g., via contractual relationships and cadaster entries, but there is currently no uniform way of retrieving and verifying these documents. In this work, we propose the Geo-Enabled Cryptographic Key Oracle (GECKO), a geographical PKI that provides a global view of digital assets based on their geo-location and occupied space. GECKO allows for the bidirectional translation of trust between the physical and digital world. Users can verify which assets are supposed to exist at their location, as well as verify which physical space is claimed by a digital entity. GECKO supplements current PKI systems and can be used in addition to current systems when its properties are of value. We show the feasibility of efficiently storing millions of assets and serving cryptographic material based on precise location queries within 11 ms at a rate of more than 19000 queries per second on a single server.

</details>


### [53] [POLARIS: Cross-Domain Access Control via Verifiable Identity and Policy-Based Authorization](https://arxiv.org/abs/2511.22017)
*Aiyao Zhang,Xiaodong Lee,Zhixian Zhuang,Jiuqi Wei,Yufan Fu,Botao Peng*

Main category: cs.CR

TL;DR: POLARIS是一个统一可扩展的跨域访问控制架构，通过结构化承诺机制和轻量级策略语言实现基于策略、可验证且保护隐私的跨域访问控制。


<details>
  <summary>Details</summary>
Motivation: 传统访问控制在跨域场景下面临身份分散、隐私泄露和权限需求多样化等挑战，需要一种能让用户自主控制身份和资源的新机制，满足跨域场景下的隐私保护认证和灵活授权需求。

Method: 提出POLARIS统一架构，包含结构化承诺机制实现细粒度策略身份披露；VPPL轻量级策略语言支持选择性属性揭示的发行者绑定评估；专门的会话级安全机制确保认证与访问绑定，增强机密性和抗重放攻击能力。

Result: 实现原型系统并进行全面实验，证明POLARIS能有效提供可扩展、保护隐私且可互操作的跨域访问控制，在去中心化跨域环境中具有实际可行性。

Conclusion: POLARIS架构成功解决了跨域访问控制的关键挑战，为去中心化跨域环境提供了安全且保护隐私的访问控制解决方案。

Abstract: Access control is a security mechanism designed to ensure that only authorized users can access specific resources. Cross-domain access control involves access to resources across different organizations, institutions, or applications. Traditional access control, however, which handles authentication and authorization separately in centralized environments, faces challenges in identity dispersion, privacy leakage, and diversified permission requirements, failing to adapt to cross-domain scenarios. Thus, there is an urgent need for a new access control mechanism that empowers autonomous control over user identity and resources, addressing the demands for privacy-preserving authentication and flexible authorization in cross-domain scenarios. To address cross-domain access control challenges, we propose POLARIS, a unified and extensible architecture that enables policy-based, verifiable and privacy-preserving access control across different domains. POLARIS features a structured commitment mechanism for reliable, fine-grained, policy-based identity disclosure. It further introduces VPPL, a lightweight policy language that supports issuer-bound evaluation of selectively revealed attributes. A dedicated session-level security mechanism ensures binding between authentication and access, enhancing confidentiality and resilience to replay attacks. We implement a working prototype and conduct comprehensive experiments, demonstrating that POLARIS effectively provides scalable, privacy-preserving, and interoperable access control across heterogeneous domains. Our results highlight the practical viability of POLARIS for enabling secure and privacy-preserving access control in decentralized, cross-domain environments.

</details>


### [54] [Evaluating the Robustness of Large Language Model Safety Guardrails Against Adversarial Attacks](https://arxiv.org/abs/2511.22047)
*Richard J. Young*

Main category: cs.CR

TL;DR: LLM安全护栏模型在对抗攻击下的鲁棒性评估显示，虽然Qwen3Guard-8B在公开基准上表现最佳（85.3%准确率），但在未见过的攻击提示上性能大幅下降，从91.0%降至33.8%，揭示了基准性能可能因训练数据污染而产生误导。


<details>
  <summary>Details</summary>
Motivation: 评估LLM安全护栏模型对抗复杂对抗攻击的鲁棒性，因为现有研究对其在真实世界攻击下的表现缺乏充分表征。

Method: 评估了来自Meta、Google、IBM、NVIDIA、Alibaba和Allen AI的10个公开护栏模型，使用1,445个测试提示覆盖21个攻击类别，区分公开基准提示和新型攻击。

Result: Qwen3Guard-8B总体准确率最高（85.3%），但在未见过的攻击提示上性能大幅下降（57.2个百分点差距）。Granite-Guardian-3.2-5B泛化能力最佳（仅6.5%差距）。发现"帮助模式"越狱攻击，两个模型反而生成有害内容。

Conclusion: 基准性能可能因训练数据污染而产生误导，泛化能力而非总体准确率应成为护栏评估的主要指标，当前护栏模型在真实世界攻击下存在显著脆弱性。

Abstract: Large Language Model (LLM) safety guardrail models have emerged as a primary defense mechanism against harmful content generation, yet their robustness against sophisticated adversarial attacks remains poorly characterized. This study evaluated ten publicly available guardrail models from Meta, Google, IBM, NVIDIA, Alibaba, and Allen AI across 1,445 test prompts spanning 21 attack categories. While Qwen3Guard-8B achieved the highest overall accuracy (85.3%, 95% CI: 83.4-87.1%), a critical finding emerged when separating public benchmark prompts from novel attacks: all models showed substantial performance degradation on unseen prompts, with Qwen3Guard dropping from 91.0% to 33.8% (a 57.2 percentage point gap). In contrast, Granite-Guardian-3.2-5B showed the best generalization with only a 6.5% gap. A "helpful mode" jailbreak was also discovered where two guardrail models (Nemotron-Safety-8B, Granite-Guardian-3.2-5B) generated harmful content instead of blocking it, representing a novel failure mode. These findings suggest that benchmark performance may be misleading due to training data contamination, and that generalization ability, not overall accuracy, should be the primary metric for guardrail evaluation.

</details>


### [55] [Privacy-preserving formal concept analysis: A homomorphic encryption-based concept construction](https://arxiv.org/abs/2511.22117)
*Qiangqiang Chen,Yunfeng Ke,Shen Li,Jinhai Li*

Main category: cs.CR

TL;DR: 提出了一种结合同态加密的隐私保护形式概念分析框架，用于在保护敏感数据隐私的同时进行大规模FCA计算


<details>
  <summary>Details</summary>
Motivation: 形式概念分析在大规模数据集上的计算需求通常需要外包给外部计算服务，这会引发敏感信息泄露的担忧，需要解决FCA计算中的数据安全和隐私保护问题

Method: 提出了隐私保护形式概念分析框架，结合二进制数据表示和同态加密技术，能够在不解密的情况下进行安全高效的概念构建

Result: 实验结果表明该方法在保护隐私的同时保持了计算性能，安全分析确认了方法的有效性

Conclusion: 该方法对隐私保护数据挖掘和大规模FCA应用中的安全知识发现具有重要意义

Abstract: Formal Concept Analysis (FCA) is extensively used in knowledge extraction, cognitive concept learning, and data mining. However, its computational demands on large-scale datasets often require outsourcing to external computing services, raising concerns about the leakage of sensitive information. To address this challenge, we propose a novel approach to enhance data security and privacy in FCA-based computations. Specifically, we introduce a Privacy-preserving Formal Context Analysis (PFCA) framework that combines binary data representation with homomorphic encryption techniques. This method enables secure and efficient concept construction without revealing private data. Experimental results and security analysis confirm the effectiveness of our approach in preserving privacy while maintaining computational performance. These findings have important implications for privacy-preserving data mining and secure knowledge discovery in large-scale FCA applications.

</details>


### [56] [Personalized 3D Spatiotemporal Trajectory Privacy Protection with Differential and Distortion Geo-Perturbation](https://arxiv.org/abs/2511.22180)
*Minghui Min,Yulu Li,Gang Li,Meng Li,Hongliang Zhang,Miao Pan,Dusit Niyato,Zhu Han*

Main category: cs.CR

TL;DR: 本文提出了一种名为3DSTPM的个性化3D时空轨迹隐私保护机制，通过结合3D地理不可区分性和失真隐私，使用窗口自适应隐私预算分配来平衡隐私保护和服务质量。


<details>
  <summary>Details</summary>
Motivation: 随着三维领域基于位置服务的快速发展，现有研究未能充分解决攻击者利用3D时空轨迹的时空相关性以及高度信息带来的隐私泄露风险，需要有效的隐私保护机制。

Method: 首先分析攻击者利用轨迹时空相关性的特征并建立攻击模型；然后结合3D地理不可区分性和失真隐私找到保护位置集合；提出窗口自适应隐私预算分配方法动态分配隐私预算；最后使用PF机制对真实位置进行扰动。

Result: 仿真结果表明，所提出的3DSTPM在满足用户个性化隐私保护需求的同时，有效降低了服务质量损失。

Conclusion: 3DSTPM机制能够有效解决3D时空轨迹的隐私保护问题，平衡隐私保护和服务质量，为三维领域基于位置服务的隐私保护提供了有效解决方案。

Abstract: The rapid advancement of location-based services (LBSs) in three-dimensional (3D) domains, such as smart cities and intelligent transportation, has raised concerns over 3D spatiotemporal trajectory privacy protection. However, existing research has not fully addressed the risk of attackers exploiting the spatiotemporal correlation of 3D spatiotemporal trajectories and the impact of height information, both of which can potentially lead to significant privacy leakage. To address these issues, this paper proposes a personalized 3D spatiotemporal trajectory privacy protection mechanism, named 3DSTPM. First, we analyze the characteristics of attackers that exploit spatiotemporal correlations between locations in a trajectory and present the attack model. Next, we exploit the complementary characteristics of 3D geo-indistinguishability (3D-GI) and distortion privacy to find a protection location set (PLS) that obscures the real location for all possible locations. To address the issue of privacy accumulation caused by continuous trajectory queries, we propose a Window-based Adaptive Privacy Budget Allocation (W-APBA), which dynamically allocates privacy budgets to all locations in the current PLS based on their predictability and sensitivity. Finally, we perturb the real location using the allocated privacy budget by the PF (Permute-and-Flip) mechanism, effectively balancing privacy protection and Quality of Service (QoS). Simulation results demonstrate that the proposed 3DSTPM effectively reduces QoS loss while meeting the user's personalized privacy protection needs.

</details>


### [57] [Enhancing the Security of Rollup Sequencers using Decentrally Attested TEEs](https://arxiv.org/abs/2511.22317)
*Giovanni Maria Cristiano,Salvatore D'Antonio,Jonah Giglio,Giovanni Mazzeo,Luigi Romano*

Main category: cs.CR

TL;DR: 提出了一种配备去中心化认证机制的TEE安全Sequencer，用于增强Rollup系统的安全性，同时保持与现有Layer-2架构的兼容性。


<details>
  <summary>Details</summary>
Motivation: Rollup中的Sequencer作为关键组件，其中心化特性容易受到审查、交易操纵和篡改等攻击。虽然现有解决方案使用可信执行环境(TEE)保护Sequencer，但TEE的认证过程引入了额外的中心化，这与区块链的核心原则相悖。

Method: 设计并实现了一个TEE安全的Sequencer，配备去中心化的认证机制。包括系统架构设计、TEE集成以及认证过程的去中心化实现。

Result: 在真实的Rollup测试网上进行了实验评估，结果表明该方法在不牺牲兼容性或可部署性的前提下，有效增强了Sequencer的完整性。

Conclusion: 提出的TEE安全Sequencer配合去中心化认证机制，能够在不违背区块链去中心化原则的前提下，有效提升Rollup系统的安全性，且与现有Layer-2架构保持兼容。

Abstract: The growing scalability demand of public Blockchains led to the rise of Layer-2 solutions, such as Rollups. Rollups improve transaction throughput by processing operations off-chain and posting the results on-chain. A critical component in Rollups is the Sequencer, responsible for receiving, ordering and batching transactions before they are submitted to the Layer-1 blockchain. While essential, the centralized nature of the Sequencer makes it vulnerable to attacks, such as censorship, transaction manipulation and tampering. To enhance its security, there are solutions in the literature that shield the Sequencer inside a Trusted Execution Environment (TEE). However, the attestation of TEEs introduces additional centralization, which is in contrast with the core Blockchain principle. In this paper, we propose a TEE-secured Sequencer equipped with a decentralized attestation mechanism. We outline the design and implementation of our solution, covering the system architecture, TEE integration, and the decentralization of the attestation process. Additionally, we present an experimental evaluation conducted on a realistic Rollup testnet. Our results show that this approach strengthens Sequencer integrity without sacrificing compatibility or deployability in existing Layer-2 architectures.

</details>


### [58] [Keyless Entry: Breaking and Entering eMMC RPMB with EMFI](https://arxiv.org/abs/2511.22340)
*Aya Fukami,Richard Buurke*

Main category: cs.CR

TL;DR: 研究人员通过电磁脉冲注入故障，成功绕过了三个主流eMMC芯片中RPMB（重放保护内存块）的认证机制，能够在两个目标芯片上覆盖任意数据而不影响其他数据的完整性。


<details>
  <summary>Details</summary>
Motivation: RPMB在现代存储系统中提供安全区域，通过认证确保数据完整性，用于存储必须防止攻击者修改的关键信息。本研究旨在评估主流制造商eMMC芯片中RPMB认证方案的安全性。

Method: 针对某主要制造商的三个不同eMMC芯片，通过向目标芯片发送电磁脉冲注入故障（glitch），攻击RPMB的认证机制。

Result: 成功通过故障注入绕过了RPMB认证，在两个目标eMMC芯片上能够用任意数据覆盖存储在RPMB中的信息，同时不影响其他数据的完整性。

Conclusion: 该研究表明，即使设计用于提供安全存储的RPMB机制，也可能受到物理攻击的威胁，特别是通过电磁脉冲注入故障的方式，这暴露了硬件安全机制在实际部署中的潜在脆弱性。

Abstract: The Replay Protected Memory Block (RPMB) in modern storage systems provides a secure area where data integrity is ensured by authentication. This block is used in digital devices to store pivotal information that must be safeguarded against modification by potential attackers. This paper targets the authentication scheme of the RPMB in three different eMMCs from a major manufacturer. A glitch was injected by sending an electromagnetic pulse to the target chip. RPMB authentication was successfully glitched and the information stored in two target eMMCs was overwritten with arbitrary data, without affecting the integrity of other data.

</details>


### [59] [Exposing Vulnerabilities in RL: A Novel Stealthy Backdoor Attack through Reward Poisoning](https://arxiv.org/abs/2511.22415)
*Bokang Zhang,Chaojun Lu,Jianhui Li,Junfeng Wu*

Main category: cs.CR

TL;DR: 该论文研究了一种针对强化学习系统的隐蔽后门攻击，通过毒化奖励信号来操纵智能体策略，在经典控制和MuJoCo环境中验证了攻击的有效性和隐蔽性。


<details>
  <summary>Details</summary>
Motivation: 强化学习虽然在各领域取得显著成功，但其对奖励信号的依赖带来了严重的安全漏洞。论文旨在研究一种隐蔽的后门攻击，通过毒化奖励信号来操纵智能体策略，揭示部署RL系统的关键安全威胁。

Method: 研究了一种隐蔽的后门攻击方法，通过毒化智能体的奖励信号来操纵其策略。攻击在经典控制环境和MuJoCo环境中进行评估，重点关注攻击的隐蔽性和有效性。

Result: 在Hopper和Walker2D环境中，后门攻击的智能体保持高度隐蔽性，非触发场景下性能下降仅2.18%和4.59%；触发条件下攻击效果显著，性能下降分别达到82.31%和71.27%。

Conclusion: 这种基于奖励信号毒化的后门攻击对强化学习系统构成了严重安全威胁，突显了训练时操纵的脆弱性，迫切需要开发相应的防御机制来保护部署的RL系统。

Abstract: Reinforcement learning (RL) has achieved remarkable success across diverse domains, enabling autonomous systems to learn and adapt to dynamic environments by optimizing a reward function. However, this reliance on reward signals creates a significant security vulnerability. In this paper, we study a stealthy backdoor attack that manipulates an agent's policy by poisoning its reward signals. The effectiveness of this attack highlights a critical threat to the integrity of deployed RL systems and calls for urgent defenses against training-time manipulation. We evaluate the attack across classic control and MuJoCo environments. The backdoored agent remains highly stealthy in Hopper and Walker2D, with minimal performance drops of only 2.18 % and 4.59 % under non-triggered scenarios, while achieving strong attack efficacy with up to 82.31% and 71.27% declines under trigger conditions.

</details>


### [60] [Evaluating LLMs for One-Shot Patching of Real and Artificial Vulnerabilities](https://arxiv.org/abs/2511.23408)
*Aayush Garg,Zanis Ali Khan,Renzo Degiovanni,Qiang Tang*

Main category: cs.CR

TL;DR: LLMs在真实漏洞修复上表现优于人工漏洞，不同模型在漏洞修复上存在显著差异和互补性


<details>
  <summary>Details</summary>
Motivation: 现有研究主要评估LLMs在公开披露漏洞上的修复能力，但对其在相关人工漏洞上的有效性缺乏探索，需要全面评估LLMs在真实和人工漏洞上的修复效果

Method: 使用OpenAI GPT变体、LLaMA、DeepSeek和Mistral等多个主流LLMs，通过PoV（漏洞证明）测试执行来评估生成的源代码是否成功修复漏洞，同时评估真实漏洞和人工漏洞的修复效果

Result: LLMs在修复真实漏洞方面比人工漏洞更有效；不同LLMs在漏洞修复上存在显著差异，包括重叠修复（多个模型修复相同漏洞）和互补性（仅单个模型能修复的漏洞）

Conclusion: LLMs在漏洞修复方面表现出潜力，但不同模型之间存在显著差异和互补性，选择合适的LLMs对于有效漏洞修复至关重要

Abstract: Automated vulnerability patching is crucial for software security, and recent advancements in Large Language Models (LLMs) present promising capabilities for automating this task. However, existing research has primarily assessed LLMs using publicly disclosed vulnerabilities, leaving their effectiveness on related artificial vulnerabilities largely unexplored. In this study, we empirically evaluate the patching effectiveness and complementarity of several prominent LLMs, such as OpenAI's GPT variants, LLaMA, DeepSeek, and Mistral models, using both real and artificial vulnerabilities. Our evaluation employs Proof-of-Vulnerability (PoV) test execution to concretely assess whether LLM-generated source code successfully patches vulnerabilities. Our results reveal that LLMs patch real vulnerabilities more effectively compared to artificial ones. Additionally, our analysis reveals significant variability across LLMs in terms of overlapping (multiple LLMs patching the same vulnerabilities) and complementarity (vulnerabilities patched exclusively by a single LLM), emphasizing the importance of selecting appropriate LLMs for effective vulnerability patching.

</details>


### [61] [FastFHE: Packing-Scalable and Depthwise-Separable CNN Inference Over FHE](https://arxiv.org/abs/2511.22434)
*Wenbo Song,Xinxin Fan,Quanliang Jing,Shaoye Luo,Wenqi Wei,Chi Lin,Yunfeng Lu,Ling Liu*

Main category: cs.CR

TL;DR: FastFHE：一种基于RNS-CKKS全同态加密的高效深度学习模型推理加速方案，通过创新的数据打包、深度可分离卷积、BN融合和低阶多项式逼近等技术，解决了加密推理中的计算瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在加密环境下的安全推理和样本隐私保护已成为安全关键应用的紧迫问题。现有基于RNS-CKKS的方案存在高延迟问题，严重限制了实际应用。加密推理面临卷积计算成本高、引导操作开销大、乘法深度消耗多三大瓶颈。

Method: 提出FastFHE机制：1）可扩展密文数据打包方案降低时间和存储消耗；2）深度可分离卷积降低卷积计算负载；3）BN点积融合矩阵将卷积层与批归一化层融合而不增加乘法深度；4）低阶Legendre多项式逼近SiLU激活函数，保证加密前后精度误差微小。

Result: 通过多方面实验验证了所提方法的效率和有效性，在保持高推理精度的同时显著加速了全同态加密下的模型推理。

Conclusion: FastFHE成功解决了加密深度学习推理中的三个主要瓶颈，提供了一种高效且有效的加速方案，为安全关键应用中的隐私保护深度学习推理提供了实用解决方案。

Abstract: The deep learning (DL) has been penetrating daily life in many domains, how to keep the DL model inference secure and sample privacy in an encrypted environment has become an urgent and increasingly important issue for various security-critical applications. To date, several approaches have been proposed based on the Residue Number System variant of the Cheon-Kim-Kim-Song (RNS-CKKS) scheme. However, they all suffer from high latency, which severely limits the applications in real-world tasks. Currently, the research on encrypted inference in deep CNNs confronts three main bottlenecks: i) the time and storage costs of convolution calculation; ii) the time overhead of huge bootstrapping operations; and iii) the consumption of circuit multiplication depth. Towards these three challenges, we in this paper propose an efficient and effective mechanism FastFHE to accelerate the model inference while simultaneously retaining high inference accuracy over fully homomorphic encryption. Concretely, our work elaborates four unique novelties. First, we propose a new scalable ciphertext data-packing scheme to save the time and storage consumptions. Second, we work out a depthwise-separable convolution fashion to degrade the computation load of convolution calculation. Third, we figure out a BN dot-product fusion matrix to merge the ciphertext convolutional layer with the batch-normalization layer without incurring extra multiplicative depth. Last but not least, we adopt the low-degree Legendre polynomial to approximate the nonlinear smooth activation function SiLU under the guarantee of tiny accuracy error before and after encrypted inference. Finally, we execute multi-facet experiments to verify the efficiency and effectiveness of our proposed approach.

</details>


### [62] [GEO-Detective: Unveiling Location Privacy Risks in Images with LLM Agents](https://arxiv.org/abs/2511.22441)
*Xinyu Zhang,Yixin Wu,Boyang Zhang,Chenhao Lin,Chao Shen,Michael Backes,Yang Zhang*

Main category: cs.CR

TL;DR: Geo-Detective是一个基于大型视觉语言模型的智能体，通过模拟人类推理和工具使用进行图像地理位置推断，在缺乏明显地理特征的图像上表现优异，同时揭示了相关的隐私风险。


<details>
  <summary>Details</summary>
Motivation: 社交媒体图像常包含地理线索，现有方法未针对此任务优化。为探索大型视觉语言模型在地理定位方面的潜力和相关隐私风险，需要开发更有效的解决方案。

Method: 开发Geo-Detective智能体，模拟人类推理过程，采用四步自适应策略选择机制，配备视觉反向搜索等专业工具来收集外部地理线索。

Result: Geo-Detective整体优于基线LVLMs，特别是在缺乏可见地理特征的图像上。在国家级定位任务中提升11.1%，更细粒度级别仍有5.2%性能提升。使用外部线索时，准确预测率提高，"未知"预测率降低50.6%。

Conclusion: Geo-Detective展示了LVLMs在地理定位任务中的强大潜力，同时也凸显了隐私风险，需要更有效的隐私保护措施来应对其较强的鲁棒性。

Abstract: Images shared on social media often expose geographic cues. While early geolocation methods required expert effort and lacked generalization, the rise of Large Vision Language Models (LVLMs) now enables accurate geolocation even for ordinary users. However, existing approaches are not optimized for this task. To explore the full potential and associated privacy risks, we present Geo-Detective, an agent that mimics human reasoning and tool use for image geolocation inference. It follows a procedure with four steps that adaptively selects strategies based on image difficulty and is equipped with specialized tools such as visual reverse search, which emulates how humans gather external geographic clues. Experimental results show that GEO-Detective outperforms baseline large vision language models (LVLMs) overall, particularly on images lacking visible geographic features. In country level geolocation tasks, it achieves an improvement of over 11.1% compared to baseline LLMs, and even at finer grained levels, it still provides around a 5.2% performance gain. Meanwhile, when equipped with external clues, GEO-Detective becomes more likely to produce accurate predictions, reducing the "unknown" prediction rate by more than 50.6%. We further explore multiple defense strategies and find that Geo-Detective exhibits stronger robustness, highlighting the need for more effective privacy safeguards.

</details>


### [63] [CacheTrap: Injecting Trojans in LLMs without Leaving any Traces in Inputs or Weights](https://arxiv.org/abs/2511.22681)
*Mohaiminul Al Nahian,Abeer Matar A. Almalky,Gamana Aragonda,Ranyang Zhou,Sabbir Ahmed,Dmitry Ponomarev,Li Yang,Shaahin Angizi,Adnan Siraj Rakin*

Main category: cs.CR

TL;DR: CacheTrap是一种新型的LLM木马攻击，通过破坏KV缓存中的值向量实现，仅需单比特翻转即可触发目标行为，无需数据或梯度信息，不影响模型正常功能。


<details>
  <summary>Details</summary>
Motivation: 随着对抗性权重扰动防御技术的成熟，传统攻击方式逐渐失效。本文提出从KV缓存角度开发新型攻击空间，利用KV值的瞬态特性实现无痕迹攻击。

Method: 提出CacheTrap攻击方法：1）通过脆弱比特搜索算法识别KV缓存中的关键比特位置；2）利用比特翻转作为触发器；3）在推理时破坏值向量，使模型产生目标行为（如分类到目标类别）。该方法无需数据、无需梯度信息。

Result: 成功实现了首个在LLM上通过KV缓存单比特翻转完成的木马攻击。攻击位置具有数据无关性，一旦识别出脆弱比特索引，该位置在不同任务/数据集/查询中保持恒定，可零开销迁移。

Conclusion: KV缓存为LLM安全提供了新的攻击面，CacheTrap展示了通过瞬态激活向量实现无痕迹攻击的可行性，揭示了LLM推理时安全的新威胁维度。

Abstract: Adversarial weight perturbation has emerged as a concerning threat to LLMs that either use training privileges or system-level access to inject adversarial corruption in model weights. With the emergence of innovative defensive solutions that place system- and algorithm-level checks and corrections in the input and weight spaces, these perturbations are increasingly susceptible to defenses. This work develops a novel perspective on Trojan attacks that generates an attacker-designed model output while leaving no attack traces on the inputs or weights. Such an attack space can be unlocked through corruption of the key-value (KV) cache. In this paper, we introduce CacheTrap, a novel Trojan attack that corrupts the value vectors stored in the KV cache. These vectors capture the dynamic activations for specific token positions and therefore constitute a natural surface for transient, inference-time trigger insertion. The transient nature of these KV values and their dependence on victim input imply additional constraints on our attack, such as a lack of knowledge of the victim's data or domain application, and, consequently, a lack of gradient information. The objective of the proposed CacheTrap is to develop a vulnerable KV bit-searching algorithm so that, once the attack employs the identified bit-flip as a trigger, the model generates targeted behavior, e.g., classifying inputs towards the target class. Moreover, CacheTrap is a data- and gradient-free attack which also has no impact on the model's utility. Our evaluation demonstrates that the proposed attack enables the first successful Trojan attack on LLMs with a single bit flip in the KV cache. In addition, the data-independent nature of the attack ensures that once the attacker identifies the vulnerable bit index, the location remains constant and can be transferred to a wide range of victim tasks/datasets/queries with no overhead.

</details>


### [64] [Ghosting Your LLM: Without The Knowledge of Your Gradient and Data](https://arxiv.org/abs/2511.22700)
*Abeer Matar A. Almalky,Ziyan Wang,Mohaiminul Al Nahian,Li Yang,Adnan Siraj Rakin*

Main category: cs.CR

TL;DR: 提出了一种无需梯度计算和数据知识的比特翻转攻击方法，通过新颖的脆弱性指标识别LLM中的易受攻击权重比特，显著降低内存需求并提高攻击效率。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在关键应用中的广泛部署，确保其安全性和鲁棒性变得至关重要。比特翻转攻击利用硬件故障破坏模型参数，对模型完整性构成严重威胁。现有基于梯度的攻击方法存在计算内存成本高、需要访问受害者数据集等限制，因此需要开发更高效、实用的无梯度数据比特翻转攻击方法。

Method: 提出了新颖的脆弱性指标，能够在无需梯度计算或数据知识的情况下识别大语言模型中的脆弱权重比特。该方法通过移除对梯度计算的依赖，大幅降低内存需求，并以恒定复杂度高效扩展到多个任务。

Result: 实验结果表明该方法具有高效性，仅需翻转单个比特即可在五个开源大语言模型上实现对抗目标，证明了该方法的实用性和有效性。

Conclusion: 成功开发了一种无需梯度计算和数据知识的比特翻转攻击方法，通过创新的脆弱性指标识别机制，显著提高了攻击效率并降低了资源需求，为大语言模型的安全评估提供了新的工具。

Abstract: In recent years, large language models (LLMs) have achieved substantial advancements and are increasingly integrated into critical applications across various domains. This growing adoption underscores the need to ensure their security and robustness. In this work, we focus on the impact of Bit Flip Attacks (BFAs) on LLMs, which exploits hardware faults to corrupt model parameters, posing a significant threat to model integrity and performance. Existing studies on BFA against LLMs adopt a progressive bit-search strategy that predominantly relies on gradient-based techniques to identify sensitive layers or weights. However, computing gradients comes with two specific challenges: First, in the context of LLMs, it increases computational and memory costs exponentially, and Second, it requires access to a sample victim dataset or knowledge of the victim domain to compute the gradient. In this work, we investigate beyond the scope of attack efficacy and aim to develop an efficient, practical Gradient-Data-free Bit-Flip Attack. The challenge lies in the core principle of adversarial attacks, which relies heavily on computing gradients from sample test/train data and manipulating model weights based on gradient information. To overcome this, we propose novel vulnerability index metrics that can identify vulnerable weight bits in LLMs independent of any gradient or data knowledge. By removing the dependency on gradient computation, our approach drastically reduces memory requirements and scales efficiently across multiple tasks with constant complexity. Experimental results demonstrate the efficiency of our method, requiring as few as a single bit flip to achieve adversarial objectives for five open-source LLMs.

</details>


### [65] [Clustering Malware at Scale: A First Full-Benchmark Study](https://arxiv.org/abs/2511.23198)
*Martin Mocko,Jakub Ševcech,Daniela Chudá*

Main category: cs.CR

TL;DR: 该研究首次在完整恶意软件基准数据集(Bodmas和Ember)上评估恶意软件聚类质量，并纳入良性样本，发现K-Means和BIRCH表现最佳，而DBSCAN和HAC落后。


<details>
  <summary>Details</summary>
Motivation: 恶意软件攻击仍然频繁发生，但现有恶意软件聚类研究存在三个主要问题：1) 很少纳入良性样本；2) 未充分利用大型公共基准数据集；3) 当前最先进解决方案不明确。本研究旨在填补这些空白。

Method: 在Bodmas和Ember两个大型公共基准恶意软件数据集上进行恶意软件聚类评估，首次使用完整数据集而非小规模子集。扩展任务包含良性样本，比较多种聚类算法性能。

Result: 1) 纳入良性样本不会显著降低聚类质量；2) 不同数据集(Bodmas、Ember和私有行业数据集)的聚类质量存在显著差异；3) 与普遍认知相反，K-Means和BIRCH表现最佳，DBSCAN和HAC表现较差。

Conclusion: 本研究首次在完整恶意软件基准数据集上建立了恶意软件聚类的当前最先进水平，挑战了关于聚类算法性能的普遍认知，并为包含良性样本的恶意软件聚类提供了实证支持。

Abstract: Recent years have shown that malware attacks still happen with high frequency. Malware experts seek to categorize and classify incoming samples to confirm their trustworthiness or prove their maliciousness. One of the ways in which groups of malware samples can be identified is through malware clustering. Despite the efforts of the community, malware clustering which incorporates benign samples has been under-explored. Moreover, despite the availability of larger public benchmark malware datasets, malware clustering studies have avoided fully utilizing these datasets in their experiments, often resorting to small datasets with only a few families. Additionally, the current state-of-the-art solutions for malware clustering remain unclear. In our study, we evaluate malware clustering quality and establish the state-of-the-art on Bodmas and Ember - two large public benchmark malware datasets. Ours is the first study of malware clustering performed on whole malware benchmark datasets. Additionally, we extend the malware clustering task by incorporating benign samples. Our results indicate that incorporating benign samples does not significantly degrade clustering quality. We find that there are significant differences in the quality of the created clusters between Ember and Bodmas, as well as a private industry dataset. Contrary to popular opinion, our top clustering performers are K-Means and BIRCH, with DBSCAN and HAC falling behind.

</details>


### [66] [One-Shot Secure Aggregation: A Hybrid Cryptographic Protocol for Private Federated Learning in IoT](https://arxiv.org/abs/2511.23252)
*Imraul Emmaka,Tran Viet Xuan Phuong*

Main category: cs.CR

TL;DR: Hyb-Agg是一个轻量级、通信高效的联邦学习安全聚合协议，结合了多密钥CKKS同态加密和ECDH掩码技术，将安全聚合简化为单轮非交互式传输，适用于资源受限的物联网环境。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在物联网环境中的可扩展性受到通信开销的限制，传统安全聚合协议需要多轮交互、大负载和较高的客户端成本，不适用于边缘部署场景。

Method: Hyb-Agg协议整合了多密钥CKKS同态加密和基于椭圆曲线Diffie-Hellman的加法掩码技术，将安全聚合过程简化为每轮仅需一次客户端到服务器的非交互式传输。

Result: 在包括树莓派4在内的设备上实现和评估，Hyb-Agg实现了亚秒级执行时间，通信扩展因子约为明文大小的12倍，且保持恒定，不受参与者数量影响。

Conclusion: Hyb-Agg通过直接解决通信瓶颈，实现了可扩展、保护隐私的联邦学习，适用于实际物联网部署场景。

Abstract: Federated Learning (FL) offers a promising approach to collaboratively train machine learning models without centralizing raw data, yet its scalability is often throttled by excessive communication overhead. This challenge is magnified in Internet of Things (IoT) environments, where devices face stringent bandwidth, latency, and energy constraints. Conventional secure aggregation protocols, while essential for protecting model updates, frequently require multiple interaction rounds, large payload sizes, and per-client costs rendering them impractical for many edge deployments.
  In this work, we present Hyb-Agg, a lightweight and communication-efficient secure aggregation protocol that integrates Multi-Key CKKS (MK-CKKS) homomorphic encryption with Elliptic Curve Diffie-Hellman (ECDH)-based additive masking. Hyb-Agg reduces the secure aggregation process to a single, non-interactive client-to-server transmission per round, ensuring that per-client communication remains constant regardless of the number of participants. This design eliminates partial decryption exchanges, preserves strong privacy under the RLWE, CDH, and random oracle assumptions, and maintains robustness against collusion by the server and up to $N-2$ clients.
  We implement and evaluate Hyb-Agg on both high-performance and resource-constrained devices, including a Raspberry Pi 4, demonstrating that it delivers sub-second execution times while achieving a constant communication expansion factor of approximately 12x over plaintext size. By directly addressing the communication bottleneck, Hyb-Agg enables scalable, privacy-preserving federated learning that is practical for real-world IoT deployments.

</details>


### [67] [FedSGT: Exact Federated Unlearning via Sequential Group-based Training](https://arxiv.org/abs/2511.23393)
*Bokang Zhang,Hong Guan,Hong kyu Lee,Ruixuan Liu,Jia Zou,Li Xiong*

Main category: cs.CR

TL;DR: FedSGT：一种联邦学习精确遗忘框架，通过分组训练和参数高效微调模块实现即时遗忘，显著降低通信成本并保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习支持"被遗忘权"面临挑战，现有精确遗忘方法通常需要从头重新训练，导致高通信成本和长时间服务中断。

Method: 提出FedSGT框架：1）将数据划分为均匀组；2）每个客户端可参与多个组；3）训练多个PEFT模块序列，每个对应不同组排列；4）通过停用相关模块实现即时精确遗忘。

Result: FedSGT在多种任务上显著延长了服务维护时间，同时保持与其他精确遗忘基线相当的学习性能和训练效率。消融研究验证了方法在不同参数设置下的鲁棒性。

Conclusion: FedSGT通过分组训练和PEFT模块实现了联邦学习中的高效精确遗忘，平衡了遗忘效率、通信成本和模型性能的需求。

Abstract: Federated Learning (FL) enables collaborative, privacy-preserving model training, but supporting the "Right to be Forgotten" is especially challenging because data influences the model through distributed and interleaved client updates. Existing exact unlearning methods typically require frequent retraining from scratch, resulting in high communication cost and long service downtime. To address this, we propose Federated Sequential Group-based Training (FedSGT), an exact unlearning framework for FL. FedSGT partitions the data into uniform groups, and each client may participate in multiple groups. To control communication overhead, each client can limit the number of groups it contributes to. FedSGT then trains multiple sequences of Parameter-Efficient Fine-Tuning (PEFT) modules, each corresponding to a different group permutation. Since the PEFT modules are lightweight and maintained server-side, FedSGT isolates the influence of different data groups into independent modules without incurring significant storage overhead and communication cost. Exact unlearning is thus achieved instantly by deactivating the modules corresponding to the group containing the unlearned data. Furthermore, using multiple training sequences helps maintain high model utility as deletion requests accumulate. We provide a rigorous theoretical analysis of both the deletion rate -- expected number of deletions before retraining is needed -- and the expected model performance. Experiments on various tasks demonstrate that FedSGT achieves a significantly longer service maintenance under multiple unlearning requests while maintaining comparable learning performance and training efficiency to other exact unlearning baselines. Extensive ablation studies validate the robustness of our method across a wide range of parameter settings.

</details>
