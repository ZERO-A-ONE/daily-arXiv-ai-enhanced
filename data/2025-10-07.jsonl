{"id": "2510.03461", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.03461", "abs": "https://arxiv.org/abs/2510.03461", "authors": ["Sanjay Malakar", "Michael D. Ernst", "Martin Kellogg", "Manu Sridharan"], "title": "Repairing Leaks in Resource Wrappers", "comment": null, "summary": "A resource leak occurs when a program fails to release a finite resource like\na socket, file descriptor or database connection. While sound static analysis\ntools can detect all leaks, automatically repairing them remains challenging.\nPrior work took the output of a detection tool and attempted to repair only\nleaks from a hard-coded list of library resource types. That approach limits\nthe scope of repairable leaks: real-world code uses resource wrappers that\nstore a resource in a field and must themselves be closed. This paper makes\nfour key contributions to improve resource leak repair in the presence of\nwrappers. (1) It integrates inference of resource management specifications\ninto the repair pipeline, enabling extant fixing approaches to reason about\nwrappers. (2) It transforms programs into variants that are easier to analyze,\nmaking inference, detection, and fixing tools more effective; for instance, it\nmakes detection tools report problems closer to the root cause, often in a\nclient of a resource wrapper rather than within the wrapper class itself. (3) A\nnovel field containment analysis reasons about resource lifetimes, enabling\nrepair of more leaks involving resources stored in fields. (4) It introduces a\nnew repair pattern and more precise reasoning to better handle resources stored\nin non-final fields. Prior work fixed 41% of resource leak warnings in the NJR\nbenchmark suite; our implementation Arodnap fixes 68%."}
{"id": "2510.03463", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03463", "abs": "https://arxiv.org/abs/2510.03463", "authors": ["Vali Tawosi", "Keshav Ramani", "Salwa Alamir", "Xiaomo Liu"], "title": "ALMAS: an Autonomous LLM-based Multi-Agent Software Engineering Framework", "comment": null, "summary": "Multi-agent Large Language Model (LLM) systems have been leading the way in\napplied LLM research across a number of fields. One notable area is software\ndevelopment, where researchers have advanced the automation of code\nimplementation, code testing, code maintenance, inter alia, using LLM agents.\nHowever, software development is a multifaceted environment that extends beyond\njust code. As such, a successful LLM system must factor in multiple stages of\nthe software development life-cycle (SDLC). In this paper, we propose a vision\nfor ALMAS, an Autonomous LLM-based Multi-Agent Software Engineering framework,\nwhich follows the above SDLC philosophy such that it may work within an agile\nsoftware development team to perform several tasks end-to-end. ALMAS aligns its\nagents with agile roles, and can be used in a modular fashion to seamlessly\nintegrate with human developers and their development environment. We showcase\nthe progress towards ALMAS through our published works and a use case\ndemonstrating the framework, where ALMAS is able to seamlessly generate an\napplication and add a new feature."}
{"id": "2510.03474", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.03474", "abs": "https://arxiv.org/abs/2510.03474", "authors": ["Nadeeshan De Silva", "Martin Kellogg", "Oscar Chaparro"], "title": "Relative Code Comprehensibility Prediction", "comment": null, "summary": "Automatically predicting how difficult it is for humans to understand a code\nsnippet can assist developers in tasks like deciding when and where to\nrefactor. Despite many proposed code comprehensibility metrics, studies have\nshown they often correlate poorly with actual measurements of human\ncomprehensibility. This has motivated the use of machine learning models to\npredict human comprehensibility directly from code, but these models have also\nshown limited accuracy.\n  We argue that model inaccuracy stems from inherent noise in human\ncomprehensibility data, which confuses models trained to predict it directly.\nTo address this, we propose training models to predict the relative\ncomprehensibility of two code snippets - that is, predicting which snippet a\nhuman would find easier to understand without predicting each snippet's\ncomprehensibility in isolation. This mitigates noise in predicting 'absolute'\ncomprehensibility measurements, but is still useful for downstream\nsoftware-engineering tasks like assessing whether refactoring improves or\nhinders comprehensibility.\n  We conducted a study to assess and compare the effectiveness of absolute and\nrelative code comprehensibility prediction via machine learning. We used a\ndataset of 150 Java code snippets and 12.5k human comprehensibility\nmeasurements from prior user studies, comparing the models' performance with\nnaive baselines (eg 'always predict the majority class'). Our findings indicate\nthat absolute comprehensibility models improve over the baselines by at most\n33.4% and frequently underperform. In contrast, relative comprehensibility\nmodels are substantially better, with average improvements of 137.8% and 74.7%\nfor snippet-wise and developer-wise prediction, respectively. These results\nsuggest that relative comprehensibility models learn more effectively from the\ndata, supporting their practical applicability for downstream SE tasks."}
{"id": "2510.03480", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.03480", "abs": "https://arxiv.org/abs/2510.03480", "authors": ["Vali Tawosi", "Salwa Alamir", "Xiaomo Liu", "Manuela Veloso"], "title": "LLM Agents for Automated Dependency Upgrades", "comment": null, "summary": "As a codebase expands over time, its library dependencies can become outdated\nand require updates to maintain innovation and security. However, updating a\nlibrary can introduce breaking changes in the code, necessitating significant\ndeveloper time for maintenance. To address this, we introduce a framework of\nLLM agents to be used in combination with migration documentation to\nautomatically recommend and apply code updates and ensure compatibility with\nnew versions. Our solution can automatically localize updated library usages in\nlive Java codebases and implement recommended fixes in a user-friendly manner.\nThe system architecture consists of multiple key components: a Summary Agent,\nControl Agent, and Code Agent. To validate our approach, we apply the framework\non an industrial use case by which we create three synthetic code repositories\nwith major Upgrade changes and benchmark our approach against state-of-the-art\nmethods. Results show that our approach not only performs upgrades using fewer\ntokens across all cases but also achieves a precision of 71.4%, highlighting\nits efficiency and effectiveness compared to state-of-the-art methods."}
{"id": "2510.03319", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03319", "abs": "https://arxiv.org/abs/2510.03319", "authors": ["Chenxiang Luo", "David K. Y. Yau", "Qun Song"], "title": "SVDefense: Effective Defense against Gradient Inversion Attacks via Singular Value Decomposition", "comment": null, "summary": "Federated learning (FL) enables collaborative model training without sharing\nraw data but is vulnerable to gradient inversion attacks (GIAs), where\nadversaries reconstruct private data from shared gradients. Existing defenses\neither incur impractical computational overhead for embedded platforms or fail\nto achieve privacy protection and good model utility at the same time.\nMoreover, many defenses can be easily bypassed by adaptive adversaries who have\nobtained the defense details. To address these limitations, we propose\nSVDefense, a novel defense framework against GIAs that leverages the truncated\nSingular Value Decomposition (SVD) to obfuscate gradient updates. SVDefense\nintroduces three key innovations, a Self-Adaptive Energy Threshold that adapts\nto client vulnerability, a Channel-Wise Weighted Approximation that selectively\npreserves essential gradient information for effective model training while\nenhancing privacy protection, and a Layer-Wise Weighted Aggregation for\neffective model aggregation under class imbalance. Our extensive evaluation\nshows that SVDefense outperforms existing defenses across multiple\napplications, including image classification, human activity recognition, and\nkeyword spotting, by offering robust privacy protection with minimal impact on\nmodel accuracy. Furthermore, SVDefense is practical for deployment on various\nresource-constrained embedded platforms. We will make our code publicly\navailable upon paper acceptance."}
{"id": "2510.03285", "categories": ["cs.AI", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03285", "abs": "https://arxiv.org/abs/2510.03285", "authors": ["Su Kara", "Fazle Faisal", "Suman Nath"], "title": "WAREX: Web Agent Reliability Evaluation on Existing Benchmarks", "comment": null, "summary": "Recent advances in browser-based LLM agents have shown promise for automating\ntasks ranging from simple form filling to hotel booking or online shopping.\nCurrent benchmarks measure agent performance in controlled environments, such\nas containers or stable networks, where websites behave deterministically.\nHowever, in the real world, users access websites over networks and HTTPS\nconnections that introduce instability from multiple sources: client-side,\nserver-side issues or broader system failures. Moreover, live websites are\nprone to web attacks such Cross-Site Scripting, as well as general site\nmodifications which can cause unexpected or malicious pop-ups or improper\nfunctionality. To address this gap, we present WAREX: Web Agent Reliability\nEvaluation on Existing Benchmarks. We measure the impact of WAREX across three\npopular benchmarks: WebArena, WebVoyager, and REAL. Our experiments show that\nintroducing WAREX leads to significant drops in task success rates,\nhighlighting the limited robustness of state-of-the-art agents."}
{"id": "2510.03495", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03495", "abs": "https://arxiv.org/abs/2510.03495", "authors": ["Erik Pautsch", "Tanmay Singla", "Wenxin Jiang", "Huiyun Peng", "Behnaz Hassanshahi", "Konstantin Läufer", "George K. Thiruvathukal", "James C. Davis"], "title": "AgentHub: A Research Agenda for Agent Sharing Infrastructure", "comment": null, "summary": "LLM-based agents are rapidly proliferating, yet the infrastructure for\ndiscovering, evaluating, and governing them remains fragmented compared to\nmature ecosystems like software package registries (e.g., npm) and model hubs\n(e.g., Hugging Face). Recent research and engineering works have begun to\nconsider the requisite infrastructure, but so far they focus narrowly -- on\ndistribution, naming, or protocol negotiation. However, considering broader\nsoftware engineering requirements would improve open-source distribution and\nease reuse. We therefore propose AgentHub, a research agenda for agent sharing.\nBy framing the key challenges of capability clarity, lifecycle transparency,\ninteroperability, governance, security, and workflow integration, AgentHub\ncharts a community-wide agenda for building reliable and scalable agent\necosystems. Our vision is a future where agents can be shared, trusted, and\ncomposed as seamlessly as today's software libraries."}
{"id": "2510.03320", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03320", "abs": "https://arxiv.org/abs/2510.03320", "authors": ["Raik Dankworth", "Gesina Schwalbe"], "title": "Attack logics, not outputs: Towards efficient robustification of deep neural networks by falsifying concept-based properties", "comment": "13 pages, 2 figures, accepted by \"7th OVERLAY\" workshop", "summary": "Deep neural networks (NNs) for computer vision are vulnerable to adversarial\nattacks, i.e., miniscule malicious changes to inputs may induce unintuitive\noutputs. One key approach to verify and mitigate such robustness issues is to\nfalsify expected output behavior. This allows, e.g., to locally proof security,\nor to (re)train NNs on obtained adversarial input examples. Due to the\nblack-box nature of NNs, current attacks only falsify a class of the final\noutput, such as flipping from $\\texttt{stop_sign}$ to $\\neg\\texttt{stop_sign}$.\nIn this short position paper we generalize this to search for generally\nillogical behavior, as considered in NN verification: falsify constraints\n(concept-based properties) involving further human-interpretable concepts, like\n$\\texttt{red}\\wedge\\texttt{octogonal}\\rightarrow\\texttt{stop_sign}$. For this,\nan easy implementation of concept-based properties on already trained NNs is\nproposed using techniques from explainable artificial intelligence. Further, we\nsketch the theoretical proof that attacks on concept-based properties are\nexpected to have a reduced search space compared to simple class falsification,\nwhilst arguably be more aligned with intuitive robustness targets. As an\noutlook to this work in progress we hypothesize that this approach has\npotential to efficiently and simultaneously improve logical compliance and\nrobustness."}
{"id": "2510.03377", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03377", "abs": "https://arxiv.org/abs/2510.03377", "authors": ["Ahmed Missaoui", "Cemalettin Ozturk", "Barry O'Sullivan"], "title": "Refined Iterated Pareto Greedy for Energy-aware Hybrid Flowshop Scheduling with Blocking Constraints", "comment": null, "summary": "The scarcity of non-renewable energy sources, geopolitical problems in its\nsupply, increasing prices, and the impact of climate change, force the global\neconomy to develop more energy-efficient solutions for their operations. The\nManufacturing sector is not excluded from this challenge as one of the largest\nconsumers of energy. Energy-efficient scheduling is a method that attracts\nmanufacturing companies to reduce their consumption as it can be quickly\ndeployed and can show impact immediately. In this study, the hybrid flow shop\nscheduling problem with blocking constraint (BHFS) is investigated in which we\nseek to minimize the latest completion time (i.e. makespan) and overall energy\nconsumption, a typical manufacturing setting across many industries from\nautomotive to pharmaceutical. Energy consumption and the latest completion time\nof customer orders are usually conflicting objectives. Therefore, we first\nformulate the problem as a novel multi-objective mixed integer programming\n(MIP) model and propose an augmented epsilon-constraint method for finding the\nPareto-optimal solutions. Also, an effective multi-objective metaheuristic\nalgorithm. Refined Iterated Pareto Greedy (RIPG), is developed to solve large\ninstances in reasonable time. Our proposed methods are benchmarked using small,\nmedium, and large-size instances to evaluate their efficiency. Two well-known\nalgorithms are adopted for comparing our novel approaches. The computational\nresults show the effectiveness of our method."}
{"id": "2510.03588", "categories": ["cs.SE", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.03588", "abs": "https://arxiv.org/abs/2510.03588", "authors": ["Anvith Pabba", "Simin Chen", "Alex Mathai", "Anindya Chakraborty", "Baishakhi Ray"], "title": "REFINE: Enhancing Program Repair Agents through Context-Aware Patch Refinement", "comment": "We also open source our code at\n  https://anonymous.4open.science/r/SemAgent-7B2F/README.md", "summary": "Large Language Models (LLMs) have recently shown strong potential in\nautomatic program repair (APR), especially in repository-level settings where\nthe goal is to generate patches based on natural language issue descriptions,\nlarge codebases, and regression tests. However, despite their promise, current\nLLM-based APR techniques often struggle to produce correct fixes due to limited\nunderstanding of code context and over-reliance on incomplete test suites. As a\nresult, they frequently generate Draft Patches-partially correct patches that\neither incompletely address the bug or overfit to the test cases. In this work,\nwe propose a novel patch refinement framework, Refine, that systematically\ntransforms Draft Patches into correct ones. Refine addresses three key\nchallenges: disambiguating vague issue and code context, diversifying patch\ncandidates through test-time scaling, and aggregating partial fixes via an\nLLM-powered code review process. We implement Refine as a general refinement\nmodule that can be integrated into both open-agent-based and workflow-based APR\nsystems. Our evaluation on the SWE-Bench Lite benchmark shows that Refine\nachieves state-of-the-art results among workflow-based approaches and\napproaches the best-known performance across all APR categories. Specifically,\nRefine boosts AutoCodeRover's performance by 14.67%, achieving a score of\n51.67% and surpassing all prior baselines. On SWE-Bench Verified, Refine\nimproves the resolution rate by 12.2%, and when integrated across multiple APR\nsystems, it yields an average improvement of 14%-demonstrating its broad\neffectiveness and generalizability. These results highlight the effectiveness\nof refinement as a missing component in current APR pipelines and the potential\nof agentic collaboration in closing the gap between near-correct and correct\npatches. We also open source our code."}
{"id": "2510.03407", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.03407", "abs": "https://arxiv.org/abs/2510.03407", "authors": ["Boniface M. Sindala", "Ragib Hasan"], "title": "Security Analysis and Threat Modeling of Research Management Applications [Extended Version]", "comment": "8 pages, 4 tables, 2 figures, This is an extended version of a paper\n  published in IEEE SoutheastCon 2025. \\c{opyright} 2025 IEEE", "summary": "Research management applications (RMA) are widely used in clinical research\nenvironments to collect, transmit, analyze, and store sensitive data. This data\nis so valuable making RMAs susceptible to security threats. This analysis,\nanalyzes RMAs' security, focusing on Research Electronic Data Capture (REDCap)\nas an example. We explore the strengths and vulnerabilities within RMAs by\nevaluating the architecture, data flow, and security features. We identify and\nassess potential risks using the MITRE ATT\\&CK framework and STRIDE model. We\nassess REDCap's defenses against common attack vectors focusing on security to\nprovide confidentiality, integrity, availability, non-repudiation, and\nauthentication. We conclude by proposing recommendations for enhancing the\nsecurity of RMAs, ensuring that critical research data remains protected\nwithout compromising usability. This research aims to contribute towards a more\nsecure framework for managing sensitive information in research-intensive\nenvironments."}
{"id": "2510.03399", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03399", "abs": "https://arxiv.org/abs/2510.03399", "authors": ["Xiaoyan Bai", "Aryan Shrivastava", "Ari Holtzman", "Chenhao Tan"], "title": "Know Thyself? On the Incapability and Implications of AI Self-Recognition", "comment": "Our code is available, see\n  https://github.com/ChicagoHAI/self-recognition", "summary": "Self-recognition is a crucial metacognitive capability for AI systems,\nrelevant not only for psychological analysis but also for safety, particularly\nin evaluative scenarios. Motivated by contradictory interpretations of whether\nmodels possess self-recognition (Panickssery et al., 2024; Davidson et al.,\n2024), we introduce a systematic evaluation framework that can be easily\napplied and updated. Specifically, we measure how well 10 contemporary larger\nlanguage models (LLMs) can identify their own generated text versus text from\nother models through two tasks: binary self-recognition and exact model\nprediction. Different from prior claims, our results reveal a consistent\nfailure in self-recognition. Only 4 out of 10 models predict themselves as\ngenerators, and the performance is rarely above random chance. Additionally,\nmodels exhibit a strong bias toward predicting GPT and Claude families. We also\nprovide the first evaluation of model awareness of their own and others'\nexistence, as well as the reasoning behind their choices in self-recognition.\nWe find that the model demonstrates some knowledge of its own existence and\nother models, but their reasoning reveals a hierarchical bias. They appear to\nassume that GPT, Claude, and occasionally Gemini are the top-tier models, often\nassociating high-quality text with them. We conclude by discussing the\nimplications of our findings on AI safety and future directions to develop\nappropriate AI self-awareness."}
{"id": "2510.03641", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.03641", "abs": "https://arxiv.org/abs/2510.03641", "authors": ["Satoshi Masuda", "Satoshi Kouzawa", "Kyousuke Sezai", "Hidetoshi Suhara", "Yasuaki Hiruta", "Kunihiro Kudou"], "title": "Generating High-Level Test Cases from Requirements using LLM: An Industry Study", "comment": "11pages", "summary": "Currently, generating high-level test cases described in natural language\nfrom requirement documents is performed manually. In the industry, including\ncompanies specializing in software testing, there is a significant demand for\nthe automatic generation of high-level test cases from requirement documents\nusing Large Language Models (LLMs). Efforts to utilize LLMs for requirement\nanalysis are underway. In some cases, retrieval-augmented generation (RAG) is\nemployed for generating high-level test cases using LLMs. However, in practical\napplications, it is necessary to create a RAG tailored to the knowledge system\nof each specific application, which is labor-intensive. Moreover, when applying\nhigh-level test case generation as a prompt, there is no established method for\ninstructing the generation of high-level test cases at a level applicable to\nother specifications without using RAG. It is required to establish a method\nfor the automatic generation of high-level test cases that can be generalized\nacross a wider range of requirement documents. In this paper, we propose a\nmethod for generating high-level (GHL) test cases from requirement documents\nusing only prompts, without creating RAGs. In the proposed method, first, the\nrequirement document is input into the LLM to generate test design techniques\ncorresponding to the requirement document. Then, high-level test cases are\ngenerated for each of the generated test design techniques. Furthermore, we\nverify an evaluation method based on semantic similarity of the generated\nhigh-level test cases. In the experiments, we confirmed the method using\ndatasets from Bluetooth and Mozilla, where requirement documents and high-level\ntest cases are available, achieving macro-recall measurement of 0.81 and 0.37,\nrespectively. We believe that the method is feasible for practical application\nin generating high-level test cases without using RAG."}
{"id": "2510.03417", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03417", "abs": "https://arxiv.org/abs/2510.03417", "authors": ["Javad Rafiei Asl", "Sidhant Narula", "Mohammad Ghasemigol", "Eduardo Blanco", "Daniel Takabi"], "title": "NEXUS: Network Exploration for eXploiting Unsafe Sequences in Multi-Turn LLM Jailbreaks", "comment": "Javad Rafiei Asl and Sidhant Narula are co-first authors", "summary": "Large Language Models (LLMs) have revolutionized natural language processing\nbut remain vulnerable to jailbreak attacks, especially multi-turn jailbreaks\nthat distribute malicious intent across benign exchanges and bypass alignment\nmechanisms. Existing approaches often explore the adversarial space poorly,\nrely on hand-crafted heuristics, or lack systematic query refinement. We\npresent NEXUS (Network Exploration for eXploiting Unsafe Sequences), a modular\nframework for constructing, refining, and executing optimized multi-turn\nattacks. NEXUS comprises: (1) ThoughtNet, which hierarchically expands a\nharmful intent into a structured semantic network of topics, entities, and\nquery chains; (2) a feedback-driven Simulator that iteratively refines and\nprunes these chains through attacker-victim-judge LLM collaboration using\nharmfulness and semantic-similarity benchmarks; and (3) a Network Traverser\nthat adaptively navigates the refined query space for real-time attacks. This\npipeline uncovers stealthy, high-success adversarial paths across LLMs. On\nseveral closed-source and open-source LLMs, NEXUS increases attack success rate\nby 2.1% to 19.4% over prior methods. Code: https://github.com/inspire-lab/NEXUS"}
{"id": "2510.03418", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.03418", "abs": "https://arxiv.org/abs/2510.03418", "authors": ["Ananya Mantravadi", "Shivali Dalmia", "Abhishek Mukherji", "Nand Dave", "Anudha Mittal"], "title": "ContraGen: A Multi-Agent Generation Framework for Enterprise Contradictions Detection", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) integrates LLMs with external sources,\noffering advanced capabilities for information access and decision-making.\nHowever, contradictions in retrieved evidence can result in inconsistent or\nuntrustworthy outputs, which is especially problematic in enterprise settings\nwhere compliance, governance, and accountability are critical. Existing\nbenchmarks for contradiction detection are limited to sentence-level analysis\nand do not capture the complexity of enterprise documents such as contracts,\nfinancial filings, compliance reports, or policy manuals. To address this\nlimitation, we propose ContraGen, a contradiction-aware benchmark framework\ntailored to enterprise domain. The framework generates synthetic\nenterprise-style documents with embedded contradictions, enabling systematic\nevaluation of both intra-document and cross-document consistency. Automated\ncontradiction mining is combined with human-in-the-loop validation to ensure\nhigh accuracy. Our contributions include generating realistic enterprise\ndocuments, modeling a taxonomy of contradiction types common in business\nprocesses, enabling controlled creation of self- and pairwise contradictions,\ndeveloping a contradiction-aware retrieval evaluation pipeline and embedding\nhuman oversight to reflect domain-specific judgment complexity. This work\nestablishes a foundation for more trustworthy and accountable RAG systems in\nenterprise information-seeking applications, where detecting and resolving\ncontradictions is essential for reducing risk and ensuring compliance."}
{"id": "2510.03712", "categories": ["cs.SE", "68M15, 90B25, 68T05, 90C29", "C.4; C.2.4; D.2.5; D.4.5"], "pdf": "https://arxiv.org/pdf/2510.03712", "abs": "https://arxiv.org/abs/2510.03712", "authors": ["Jahidul Arafat", "Kh. M. Moniruzzaman", "Shamim Hossain", "Fariha Tasmin", "Kamrujjaman", "Ahsan Habib Tareq"], "title": "Detecting and Preventing Latent Risk Accumulation in High-Performance Software Systems", "comment": "26 pages, 12 tables, 4 figures. Academic-industry collaboration.\n  Framework (HYDRA, RAVEN, APEX) for optimization-induced vulnerabilities.\n  Evaluated: 2,160 configs, 12.7TB data, 1,748 scenarios", "summary": "Modern distributed systems employ aggressive optimization strategies that\ncreate latent risks - hidden vulnerabilities where exceptional performance\nmasks catastrophic fragility when optimizations fail. Cache layers achieving\n99% hit rates can obscure database bottlenecks until cache failures trigger\n100x load amplification and cascading collapse. Current reliability engineering\nfocuses on reactive incident response rather than proactive detection of\noptimization-induced vulnerabilities. This paper presents the first\ncomprehensive framework for systematic latent risk detection, prevention, and\noptimization through integrated mathematical modeling, intelligent perturbation\ntesting, and risk-aware performance optimization. We introduce the Latent Risk\nIndex (LRI) that correlates strongly with incident severity (r=0.863, p<0.001),\nenabling predictive risk assessment. Our framework integrates three systems:\nHYDRA employing six optimization-aware perturbation strategies achieving 89.7%\nrisk discovery rates, RAVEN providing continuous production monitoring with\n92.9% precision and 93.8% recall across 1,748 scenarios, and APEX enabling\nrisk-aware optimization maintaining 96.6% baseline performance while reducing\nlatent risks by 59.2%. Evaluation across three testbed environments\ndemonstrates strong statistical validation with large effect sizes (Cohen\nd>2.0) and exceptional reproducibility (r>0.92). Production deployment over 24\nweeks shows 69.1% mean time to recovery reduction, 78.6% incident severity\nreduction, and 81 prevented incidents generating 1.44M USD average annual\nbenefits with 3.2-month ROI. Our approach transforms reliability engineering\nfrom reactive incident management to proactive risk-aware optimization."}
{"id": "2510.03542", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.03542", "abs": "https://arxiv.org/abs/2510.03542", "authors": ["Pouriya Alimoradi", "Ali Barati", "Hamid Barati"], "title": "A Multi-Layer Electronic and Cyber Interference Model for AI-Driven Cruise Missiles: The Case of Khuzestan Province", "comment": null, "summary": "The rapid advancement of Artificial Intelligence has enabled the development\nof cruise missiles endowed with high levels of autonomy, adaptability, and\nprecision. These AI driven missiles integrating deep learning algorithms, real\ntime data processing, and advanced guidance systems pose critical threats to\nstrategic infrastructures, especially under complex geographic and climatic\nconditions such as those found in Irans Khuzestan Province. In this paper, we\npropose a multi layer interference model, encompassing electronic warfare,\ncyberattacks, and deception strategies, to degrade the performance of AI guided\ncruise missiles significantly. Our experimental results, derived from 400\nsimulation runs across four distinct scenarios, demonstrate notable\nimprovements when employing the integrated multi layer approach compared to\nsingle layer or no interference baselines. Specifically, the average missile\ndeviation from its intended target increases from 0.25 to 8.65 under multi\nlayer interference a more than 3300 increase in angular deviation. Furthermore,\nthe target acquisition success rate is reduced from 92.7 in the baseline\nscenario to 31.5, indicating a 66 decrease in successful strikes. While\nresource consumption for multi layer strategies rises by approximately 25\ncompared to single layer methods, the significant drop in missile accuracy and\nreliability justifies the more intensive deployment of jamming power, cyber\nresources, and decoy measures. Beyond these quantitative improvements, the\nproposed framework uses a deep reinforcement learning based defense coordinator\nto adaptively select the optimal configuration of EW, cyber, and deception\ntactics in real time."}
{"id": "2510.03453", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03453", "abs": "https://arxiv.org/abs/2510.03453", "authors": ["Paul S. Rosenbloom"], "title": "A Qualitative Comparative Evaluation of Cognitive and Generative Theories", "comment": "To appear in Proceedings of the 12th Annual Conference on Advances in\n  Cognitive Systems (ACS-25)", "summary": "Evaluation is a critical activity associated with any theory. Yet this has\nproven to be an exceptionally challenging activity for theories based on\ncognitive architectures. For an overlapping set of reasons, evaluation can also\nbe challenging for theories based on generative neural architectures. This dual\nchallenge is approached here by leveraging a broad perspective on theory\nevaluation to yield a wide-ranging, albeit qualitative, comparison of\nwhole-mind-oriented cognitive and generative architectures and the full systems\nthat are based on these architectures."}
{"id": "2510.03743", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.03743", "abs": "https://arxiv.org/abs/2510.03743", "authors": ["Zachary Eberhart", "Collin McMillan"], "title": "APIDA-Chat: Structured Synthesis of API Search Dialogues to Bootstrap Conversational Agents", "comment": "4 pages, 2 figures. To be published in Proceedings of the 40th\n  IEEE/ACM International Conference on Automated Software Engineering", "summary": "Large-language-model assistants are suitable for explaining popular APIs, yet\nthey falter on niche or proprietary libraries because the multi-turn dialogue\ndata needed for fine-tuning are scarce. We present APIDA-Chat, an open-source\npipeline that converts symbolic dialogue-act \"scripts\" into realistic,\ndomain-grounded API Search conversations using a lightweight model for\ninexpensive training data generation. Phase I pairs a legacy dialogue planner\nwith a high-capability teacher LLM (o4-mini) to synthesize a \"gold set\" of\nrealized dialogues; then, a smaller Llama 3.2 3B student model is fine-tuned on\nthis corpus. Phase II drops the teacher and reuses the same planner with the\nfine-tuned model, allowing rapid, low-cost synthesis of new dialogues without\nexposing source code to external services. The fine-tuned student improves BLEU\nfrom 0.38 to 0.50 and BERTScore from 0.88 to 0.91 versus the base model while\nrunning entirely on a single consumer GPU. All components are modular and\npublicly released to serve as a conservative baseline for future work.\nAPIDA-Chat is open-sourced at https://github.com/Zeberhart/apida-chat and a\nvideo demo is available at https://youtu.be/YqmZBHyGbPs ."}
{"id": "2510.03559", "categories": ["cs.CR", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.03559", "abs": "https://arxiv.org/abs/2510.03559", "authors": ["Zeya Chen", "Jianing Wen", "Ruth Schmidt", "Yaxing Yao", "Toby Jia-Jun Li", "Tianshi Li"], "title": "PrivacyMotiv: Speculative Persona Journeys for Empathic and Motivating Privacy Reviews in UX Design", "comment": "42 pages, 13 figures", "summary": "UX professionals routinely conduct design reviews, yet privacy concerns are\noften overlooked -- not only due to limited tools, but more critically because\nof low intrinsic motivation. Limited privacy knowledge, weak empathy for\nunexpectedly affected users, and low confidence in identifying harms make it\ndifficult to address risks. We present PrivacyMotiv, an LLM-powered system that\nsupports privacy-oriented design diagnosis by generating speculative personas\nwith UX user journeys centered on individuals vulnerable to privacy risks.\nDrawing on narrative strategies, the system constructs relatable and\nattention-drawing scenarios that show how ordinary design choices may cause\nunintended harms, expanding the scope of privacy reflection in UX. In a\nwithin-subjects study with professional UX practitioners (N=16), we compared\nparticipants' self-proposed methods with PrivacyMotiv across two privacy review\ntasks. Results show significant improvements in empathy, intrinsic motivation,\nand perceived usefulness. This work contributes a promising privacy review\napproach which addresses the motivational barriers in privacy-aware UX."}
{"id": "2510.03469", "categories": ["cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2510.03469", "abs": "https://arxiv.org/abs/2510.03469", "authors": ["Keshav Ramani", "Vali Tawosi", "Salwa Alamir", "Daniel Borrajo"], "title": "Bridging LLM Planning Agents and Formal Methods: A Case Study in Plan Verification", "comment": null, "summary": "We introduce a novel framework for evaluating the alignment between natural\nlanguage plans and their expected behavior by converting them into Kripke\nstructures and Linear Temporal Logic (LTL) using Large Language Models (LLMs)\nand performing model checking. We systematically evaluate this framework on a\nsimplified version of the PlanBench plan verification dataset and report on\nmetrics like Accuracy, Precision, Recall and F1 scores. Our experiments\ndemonstrate that GPT-5 achieves excellent classification performance (F1 score\nof 96.3%) while almost always producing syntactically perfect formal\nrepresentations that can act as guarantees. However, the synthesis of\nsemantically perfect formal models remains an area for future exploration."}
{"id": "2510.03755", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03755", "abs": "https://arxiv.org/abs/2510.03755", "authors": ["Roham Koohestani", "Parham Bateni", "Aydin Ebrahimi", "Behdad Etezadi", "Kiarash Karimi", "Maliheh Izadi"], "title": "Code4MeV2: a Research-oriented Code-completion Platform", "comment": "Under review for submission at a conference", "summary": "The adoption of AI-powered code completion tools in software development has\nincreased substantially, yet the user interaction data produced by these\nsystems remain proprietary within large corporations. This creates a barrier\nfor the academic community, as researchers must often develop dedicated\nplatforms to conduct studies on human--AI interaction, making reproducible\nresearch and large-scale data analysis impractical. In this work, we introduce\nCode4MeV2, a research-oriented, open-source code completion plugin for\nJetBrains IDEs, as a solution to this limitation. Code4MeV2 is designed using a\nclient--server architecture and features inline code completion and a\ncontext-aware chat assistant. Its core contribution is a modular and\ntransparent data collection framework that gives researchers fine-grained\ncontrol over telemetry and context gathering. Code4MeV2 achieves\nindustry-comparable performance in terms of code completion, with an average\nlatency of 200~ms. We assess our tool through a combination of an expert\nevaluation and a user study with eight participants. Feedback from both\nresearchers and daily users highlights its informativeness and usefulness. We\ninvite the community to adopt and contribute to this tool. More information\nabout the tool can be found at https://app.code4me.me."}
{"id": "2510.03565", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.03565", "abs": "https://arxiv.org/abs/2510.03565", "authors": ["Cory Brynds", "Parker McLeod", "Lauren Caccamise", "Asmita Pal", "Dewan Saiham", "Sazadur Rahman", "Joshua San Miguel", "Di Wu"], "title": "CryptOracle: A Modular Framework to Characterize Fully Homomorphic Encryption", "comment": null, "summary": "Privacy-preserving machine learning has become an important long-term pursuit\nin this era of artificial intelligence (AI). Fully Homomorphic Encryption (FHE)\nis a uniquely promising solution, offering provable privacy and security\nguarantees. Unfortunately, computational cost is impeding its mass adoption.\nModern solutions are up to six orders of magnitude slower than plaintext\nexecution. Understanding and reducing this overhead is essential to the\nadvancement of FHE, particularly as the underlying algorithms evolve rapidly.\nThis paper presents a detailed characterization of OpenFHE, a comprehensive\nopen-source library for FHE, with a particular focus on the CKKS scheme due to\nits significant potential for AI and machine learning applications. We\nintroduce CryptOracle, a modular evaluation framework comprising (1) a\nbenchmark suite, (2) a hardware profiler, and (3) a predictive performance\nmodel. The benchmark suite encompasses OpenFHE kernels at three abstraction\nlevels: workloads, microbenchmarks, and primitives. The profiler is compatible\nwith standard and user-specified security parameters. CryptOracle monitors\napplication performance, captures microarchitectural events, and logs power and\nenergy usage for AMD and Intel systems. These metrics are consumed by a\nmodeling engine to estimate runtime and energy efficiency across different\nconfiguration scenarios, with error geomean of $-7.02\\%\\sim8.40\\%$ for runtime\nand $-9.74\\%\\sim15.67\\%$ for energy. CryptOracle is open source, fully modular,\nand serves as a shared platform to facilitate the collaborative advancements of\napplications, algorithms, software, and hardware in FHE. The CryptOracle code\ncan be accessed at https://github.com/UnaryLab/CryptOracle."}
{"id": "2510.03485", "categories": ["cs.AI", "I.2.7"], "pdf": "https://arxiv.org/pdf/2510.03485", "abs": "https://arxiv.org/abs/2510.03485", "authors": ["Xiaofei Wen", "Wenjie Jacky Mo", "Yanan Xie", "Peng Qi", "Muhao Chen"], "title": "Towards Policy-Compliant Agents: Learning Efficient Guardrails For Policy Violation Detection", "comment": "16 pages, 5 figures", "summary": "Autonomous web agents need to operate under externally imposed or\nhuman-specified policies while generating long-horizon trajectories. However,\nlittle work has examined whether these trajectories comply with such policies,\nor whether policy violations persist across different contexts such as domains\n(e.g., shopping or coding websites) and subdomains (e.g., product search and\norder management in shopping). To address this gap, we introduce\nPolicyGuardBench, a benchmark of about 60k examples for detecting policy\nviolations in agent trajectories. From diverse agent runs, we generate a broad\nset of policies and create both within subdomain and cross subdomain pairings\nwith violation labels. In addition to full-trajectory evaluation,\nPolicyGuardBench also includes a prefix-based violation detection task where\nmodels must anticipate policy violations from truncated trajectory prefixes\nrather than complete sequences. Using this dataset, we train PolicyGuard-4B, a\nlightweight guardrail model that delivers strong detection accuracy across all\ntasks while keeping inference efficient. Notably, PolicyGuard-4B generalizes\nacross domains and preserves high accuracy on unseen settings. Together,\nPolicyGuardBench and PolicyGuard-4B provide the first comprehensive framework\nfor studying policy compliance in web agent trajectories, and show that\naccurate and generalizable guardrails are feasible at small scales."}
{"id": "2510.03802", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.03802", "abs": "https://arxiv.org/abs/2510.03802", "authors": ["Gilberto Recupito", "Vincenzo De Martino", "Dario Di Nucci", "Fabio Palomba"], "title": "A First Look at the Lifecycle of DL-Specific Self-Admitted Technical Debt", "comment": "Accepted at the International Workshop of Software Quality Assurance\n  for Artificial Intelligence 2025 (SQA4AI), Montr\\'eal, Canada", "summary": "The rapid adoption of Deep Learning (DL)-enabled systems has revolutionized\nsoftware development, driving innovation across various domains. However, these\nsystems also introduce unique challenges, particularly in maintaining software\nquality and performance. Among these challenges, Self-Admitted Technical Debt\n(SATD) has emerged as a growing concern, significantly impacting the\nmaintainability and overall quality of ML and DL-enabled systems. Despite its\ncritical implications, the lifecycle of DL-specific SATD, how developers\nintroduce, acknowledge, and address it over time-remains underexplored. This\nstudy presents a preliminary analysis of the persistence and lifecycle of\nDL-specific SATD in DL-enabled systems. The purpose of this project is to\nuncover the patterns of SATD introduction, recognition, and durability during\nthe development life cycle, providing information on how to manage these\nissues. Using mining software repository techniques, we examined 40 ML\nprojects, focusing on 185 DL-specific SATD instances. The analysis tracked the\nintroduction and persistence of SATD instances through project commit histories\nto assess their lifecycle and developer actions. The findings indicate that\nDL-specific SATD is predominantly introduced during the early and middle stages\nof project development. Training and Hardware phases showed the longest SATD\ndurations, highlighting critical areas where debt accumulates and persists.\nAdditionally, developers introduce DL-specific SATD more frequently during\nfeature implementation and bug fixes. This study emphasizes the need for\ntargeted DL-specific SATD management strategies in DL-enabled systems to\nmitigate its impact. By understanding the temporal characteristics and\nevolution of DL-specific SATD, developers can prioritize interventions at\ncritical stages to improve the maintainability and quality of the system."}
{"id": "2510.03610", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03610", "abs": "https://arxiv.org/abs/2510.03610", "authors": ["Zachary Ezetta", "Wu-chang Feng"], "title": "PentestMCP: A Toolkit for Agentic Penetration Testing", "comment": null, "summary": "Agentic AI is transforming security by automating many tasks being performed\nmanually. While initial agentic approaches employed a monolithic architecture,\nthe Model-Context-Protocol has now enabled a remote-procedure call (RPC)\nparadigm to agentic applications, allowing for the flexible construction and\ncomposition of multi-function agents. This paper describes PentestMCP, a\nlibrary of MCP server implementations that support agentic penetration testing.\nBy supporting common penetration testing tasks such as network scanning,\nresource enumeration, service fingerprinting, vulnerability scanning,\nexploitation, and post-exploitation, PentestMCP allows a developer to customize\nmulti-agent workflows for performing penetration tests."}
{"id": "2510.03506", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03506", "abs": "https://arxiv.org/abs/2510.03506", "authors": ["John Nguyen", "Marton Havasi", "Tariq Berrada", "Luke Zettlemoyer", "Ricky T. Q. Chen"], "title": "OneFlow: Concurrent Mixed-Modal and Interleaved Generation with Edit Flows", "comment": "https://johnlnguyen.com/oneflow", "summary": "We present OneFlow, the first non-autoregressive multimodal model that\nenables variable-length and concurrent mixed-modal generation. Unlike\nautoregressive models that enforce rigid causal ordering between text and image\ngeneration, OneFlow combines an insertion-based Edit Flow for discrete text\ntokens with Flow Matching for image latents. OneFlow enables concurrent\ntext-image synthesis with hierarchical sampling that prioritizes content over\ngrammar. Through controlled experiments across model sizes from 1B to 8B, we\ndemonstrate that OneFlow outperforms autoregressive baselines on both\ngeneration and understanding tasks while using up to 50% fewer training FLOPs.\nOneFlow surpasses both autoregressive and diffusion-based approaches while\nunlocking new capabilities for concurrent generation, iterative refinement, and\nnatural reasoning-like generation."}
{"id": "2510.03843", "categories": ["cs.SE", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03843", "abs": "https://arxiv.org/abs/2510.03843", "authors": ["Vincent Nguyen", "Guilherme Herzog", "José Cambronero", "Marcus Revaj", "Aditya Kini", "Alexander Frömmgen", "Maxim Tabachnyk"], "title": "Smart Paste: Automatically Fixing Copy/Paste for Google Developers", "comment": "11 pages", "summary": "Manually editing pasted code is a long-standing developer pain point. In\ninternal software development at Google, we observe that code is pasted 4 times\nmore often than it is manually typed. These paste actions frequently require\nfollow-up edits, ranging from simple reformatting and renaming to more complex\nstyle adjustments and cross-language translations. Prior work has shown deep\nlearning can be used to predict these edits. In this work, we show how to\niteratively develop and scale Smart Paste, an IDE feature for post-paste edit\nsuggestions, to Google's development environment. This experience can serve as\na guide for AI practitioners on a holistic approach to feature development,\ncovering user experience, system integration, and model capabilities. Since\ndeployment, Smart Paste has had overwhelmingly positive feedback with a 45%\nacceptance rate. At Google's enterprise scale, these accepted suggestions\naccount substantially for over 1% of all code written company-wide."}
{"id": "2510.03623", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03623", "abs": "https://arxiv.org/abs/2510.03623", "authors": ["Maraz Mia", "Mir Mehedi A. Pritom"], "title": "Explainable but Vulnerable: Adversarial Attacks on XAI Explanation in Cybersecurity Applications", "comment": "10 pages, 9 figures, 4 tables", "summary": "Explainable Artificial Intelligence (XAI) has aided machine learning (ML)\nresearchers with the power of scrutinizing the decisions of the black-box\nmodels. XAI methods enable looking deep inside the models' behavior, eventually\ngenerating explanations along with a perceived trust and transparency. However,\ndepending on any specific XAI method, the level of trust can vary. It is\nevident that XAI methods can themselves be a victim of post-adversarial attacks\nthat manipulate the expected outcome from the explanation module. Among such\nattack tactics, fairwashing explanation (FE), manipulation explanation (ME),\nand backdoor-enabled manipulation attacks (BD) are the notable ones. In this\npaper, we try to understand these adversarial attack techniques, tactics, and\nprocedures (TTPs) on explanation alteration and thus the effect on the model's\ndecisions. We have explored a total of six different individual attack\nprocedures on post-hoc explanation methods such as SHAP (SHapley Additive\nexPlanations), LIME (Local Interpretable Model-agnostic Explanation), and IG\n(Integrated Gradients), and investigated those adversarial attacks in\ncybersecurity applications scenarios such as phishing, malware, intrusion, and\nfraudulent website detection. Our experimental study reveals the actual\neffectiveness of these attacks, thus providing an urgency for immediate\nattention to enhance the resiliency of XAI methods and their applications."}
{"id": "2510.03605", "categories": ["cs.AI", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.03605", "abs": "https://arxiv.org/abs/2510.03605", "authors": ["Adel Javanmard", "Baharan Mirzasoleiman", "Vahab Mirrokni"], "title": "Understanding the Role of Training Data in Test-Time Scaling", "comment": "24 pages, 4 figures", "summary": "Test-time scaling improves the reasoning capabilities of large language\nmodels (LLMs) by allocating extra compute to generate longer Chains-of-Thoughts\n(CoTs). This enables models to tackle more complex problem by breaking them\ndown into additional steps, backtracking, and correcting mistakes. Despite its\nstrong performance--demonstrated by OpenAI's o1 and DeepSeek R1, the conditions\nin the training data under which long CoTs emerge, and when such long CoTs\nimprove the performance, remain unclear. In this paper, we study the\nperformance of test-time scaling for transformers trained on an in-context\nweight prediction task for linear regression. Our analysis provides a\ntheoretical explanation for several intriguing observations: First, at any\nfixed test error, increasing test-time compute allows us to reduce the number\nof in-context examples (context length) in training prompts. Second, if the\nskills required to solve a downstream task are not sufficiently present in the\ntraining data, increasing test-time compute can harm performance. Finally, we\ncharacterize task hardness via the smallest eigenvalue of its feature\ncovariance matrix and show that training on a diverse, relevant, and hard set\nof tasks results in best performance for test-time scaling. We confirm our\nfindings with experiments on large, nonlinear transformer architectures."}
{"id": "2510.03862", "categories": ["cs.SE", "cs.AI", "500"], "pdf": "https://arxiv.org/pdf/2510.03862", "abs": "https://arxiv.org/abs/2510.03862", "authors": ["Nathalia Nascimento", "Everton Guimaraes", "Paulo Alencar"], "title": "Designing Empirical Studies on LLM-Based Code Generation: Towards a Reference Framework", "comment": "5 pages", "summary": "The rise of large language models (LLMs) has introduced transformative\npotential in automated code generation, addressing a wide range of software\nengineering challenges. However, empirical evaluation of LLM-based code\ngeneration lacks standardization, with studies varying widely in goals, tasks,\nand metrics, which limits comparability and reproducibility. In this paper, we\npropose a theoretical framework for designing and reporting empirical studies\non LLM-based code generation. The framework is grounded in both our prior\nexperience conducting such experiments and a comparative analysis of key\nsimilarities and differences among recent studies. It organizes evaluation\naround core components such as problem sources, quality attributes, and\nmetrics, supporting structured and systematic experimentation. We demonstrate\nits applicability through representative case mappings and identify\nopportunities for refinement. Looking forward, we plan to evolve the framework\ninto a more robust and mature tool for standardizing LLM evaluation across\nsoftware engineering contexts."}
{"id": "2510.03625", "categories": ["cs.CR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2510.03625", "abs": "https://arxiv.org/abs/2510.03625", "authors": ["Joachim Neu", "Javier Nieto", "Ling Ren"], "title": "On the Limits of Consensus under Dynamic Availability and Reconfiguration", "comment": null, "summary": "Proof-of-stake blockchains require consensus protocols that support Dynamic\nAvailability and Reconfiguration (so-called DAR setting), where the former\nmeans that the consensus protocol should remain live even if a large number of\nnodes temporarily crash, and the latter means it should be possible to change\nthe set of operating nodes over time. State-of-the-art protocols for the DAR\nsetting, such as Ethereum, Cardano's Ouroboros, or Snow White, require\nunrealistic additional assumptions, such as social consensus, or that key\nevolution is performed even while nodes are not participating. In this paper,\nwe identify the necessary and sufficient adversarial condition under which\nconsensus can be achieved in the DAR setting without additional assumptions. We\nthen introduce a new and realistic additional assumption: honest nodes dispose\nof their cryptographic keys the moment they express intent to exit from the set\nof operating nodes. To add reconfiguration to any dynamically available\nconsensus protocol, we provide a bootstrapping gadget that is particularly\nsimple and efficient in the common optimistic case of few reconfigurations and\nno double-spending attempts."}
{"id": "2510.03612", "categories": ["cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.03612", "abs": "https://arxiv.org/abs/2510.03612", "authors": ["Tanqiu Jiang", "Min Bai", "Nikolaos Pappas", "Yanjun Qi", "Sandesh Swamy"], "title": "Cross-Modal Content Optimization for Steering Web Agent Preferences", "comment": null, "summary": "Vision-language model (VLM)-based web agents increasingly power high-stakes\nselection tasks like content recommendation or product ranking by combining\nmultimodal perception with preference reasoning. Recent studies reveal that\nthese agents are vulnerable against attackers who can bias selection outcomes\nthrough preference manipulations using adversarial pop-ups, image\nperturbations, or content tweaks. Existing work, however, either assumes strong\nwhite-box access, with limited single-modal perturbations, or uses impractical\nsettings. In this paper, we demonstrate, for the first time, that joint\nexploitation of visual and textual channels yields significantly more powerful\npreference manipulations under realistic attacker capabilities. We introduce\nCross-Modal Preference Steering (CPS) that jointly optimizes imperceptible\nmodifications to an item's visual and natural language descriptions, exploiting\nCLIP-transferable image perturbations and RLHF-induced linguistic biases to\nsteer agent decisions. In contrast to prior studies that assume gradient\naccess, or control over webpages, or agent memory, we adopt a realistic\nblack-box threat setup: a non-privileged adversary can edit only their own\nlisting's images and textual metadata, with no insight into the agent's model\ninternals. We evaluate CPS on agents powered by state-of-the-art proprietary\nand open source VLMs including GPT-4.1, Qwen-2.5VL and Pixtral-Large on both\nmovie selection and e-commerce tasks. Our results show that CPS is\nsignificantly more effective than leading baseline methods. For instance, our\nresults show that CPS consistently outperforms baselines across all models\nwhile maintaining 70% lower detection rates, demonstrating both effectiveness\nand stealth. These findings highlight an urgent need for robust defenses as\nagentic systems play an increasingly consequential role in society."}
{"id": "2510.03879", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03879", "abs": "https://arxiv.org/abs/2510.03879", "authors": ["Tianyu Li", "Ruishi Li", "Bo Wang", "Brandon Paulsen", "Umang Mathur", "Prateek Saxena"], "title": "Adversarial Agent Collaboration for C to Rust Translation", "comment": null, "summary": "Translating C to memory-safe languages, like Rust, prevents critical memory\nsafety vulnerabilities that are prevalent in legacy C software. Existing\napproaches for C to safe Rust translation, including LLM-assisted ones, do not\ngeneralize on larger (> 500 LoC) C codebases because they depend on complex\nprogram analyses that frequently break. In this work, we present ACToR\n(Adversarial C To Rust translator), a simple LLM agent-based approach. Inspired\nby GANs, ACToR pits a generator agent against a discriminator agent, which\ncollaborate to iteratively generate a Rust translation. On each iteration, the\ntranslator agent synthesizes and refines a Rust translation to pass an existing\nsuite of tests, and then the discriminator agent finds new failing tests. We\ndemonstrate that ACToR translates all of the 63 real-world command line\nutilities considered in our benchmarks, which have an average size of 485 lines\nof code, and it achieves over 90% test pass rate with zero human intervention.\nTo our knowledge, it is the first such system that reliably translates C\nprograms of this scale. Furthermore, ACToR improves translation correctness by\nup to 18.9% compared to baseline, non-adversarial approaches."}
{"id": "2510.03631", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.03631", "abs": "https://arxiv.org/abs/2510.03631", "authors": ["Saleh Darzi", "Saif Eddine Nouma", "Kiarash Sedghighadikolaei", "Attila Altay"], "title": "QPADL: Post-Quantum Private Spectrum Access with Verified Location and DoS Resilience", "comment": "13 pages, 3 figures, 1 table, 4 algorithms", "summary": "With advances in wireless communication and growing spectrum scarcity,\nSpectrum Access Systems (SASs) offer an opportunistic solution but face\nsignificant security challenges. Regulations require disclosure of location\ncoordinates and transmission details, exposing user privacy and anonymity\nduring spectrum queries, while the database operations themselves permit\nDenial-of-Service (DoS) attacks. As location-based services, SAS is also\nvulnerable to compromised or malicious users conducting spoofing attacks. These\nthreats are further amplified given the quantum computing advancements. Thus,\nwe propose QPADL, the first post-quantum (PQ) secure framework that\nsimultaneously ensures privacy, anonymity, location verification, and DoS\nresilience while maintaining efficiency for large-scale spectrum access\nsystems. QPADL introduces SAS-tailored private information retrieval for\nlocation privacy, a PQ-variant of Tor for anonymity, and employs advanced\nsignature constructions for location verification alongside client puzzle\nprotocols and rate-limiting technique for DoS defense. We formally assess its\nsecurity and conduct a comprehensive performance evaluation, incorporating GPU\nparallelization and optimization strategies to demonstrate practicality and\nscalability."}
{"id": "2510.03632", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03632", "abs": "https://arxiv.org/abs/2510.03632", "authors": ["Jiaxi Li", "Yucheng Shi", "Jin Lu", "Ninghao Liu"], "title": "MITS: Enhanced Tree Search Reasoning for LLMs via Pointwise Mutual Information", "comment": "18 pages", "summary": "Tree search has become as a representative framework for test-time reasoning\nwith large language models (LLMs), exemplified by methods such as\nTree-of-Thought and Monte Carlo Tree Search that explore multiple reasoning\npaths. However, it remains difficult to provide instant and reliable\nquantitative assessments of intermediate reasoning step quality, and extensive\npath exploration is computationally costly. To address this, we propose Mutual\nInformation Tree Search (MITS), a novel framework that guides reasoning with\ninformation-theoretic principles. MITS introduces an effective scoring function\nbased on pointwise mutual information (PMI), which enables step-wise evaluation\nof reasoning paths and search tree expansion via beam search without expensive\nlook-ahead simulations, achieving superior reasoning performances while\nmaintaining computational efficiency. The framework is complemented by an\nentropy-based dynamic sampling strategy that adaptively allocates computational\nresources to uncertain reasoning steps where exploration is most beneficial.\nFor final prediction, MITS employs a weighted voting scheme that combines PMI\nscores with prediction consensus. Through comprehensive experiments on diverse\nreasoning benchmarks, MITS consistently surpasses baseline methods,\nestablishing a principled and efficient framework for LLM reasoning."}
{"id": "2510.03890", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.03890", "abs": "https://arxiv.org/abs/2510.03890", "authors": ["Jose Garcia-Alonso", "Enrique Moguel", "Jaime Alvarado-Valiente", "Javier Romero-Alvarez", "Álvaro M. Aparicio-Morales", "Juan M. Murillo", "Francisco Javier Cavero", "Adrián Romero-Flores", "Alfonso E. Marquez-Chamorro", "José Antonio Parejo", "Antonio Ruiz-Cortés", "Giuseppe Bisicchia", "Alessandro Bocci", "Antonio Brogi"], "title": "Rethinking Services in the Quantum Age: The SOQ Paradigm", "comment": "39 pages, 5 figures, 6 tables", "summary": "Quantum computing is rapidly progressing from theoretical promise to\npractical implementation, offering significant computational advantages for\ntasks in optimization, simulation, cryptography, and machine learning. However,\nits integration into real-world software systems remains constrained by\nhardware fragility, platform heterogeneity, and the absence of robust software\nengineering practices. This paper introduces Service-Oriented Quantum (SOQ), a\nnovel paradigm that reimagines quantum software systems through the lens of\nclassical service-oriented computing. Unlike prior approaches such as Quantum\nService-Oriented Computing (QSOC), which treat quantum capabilities as\nauxiliary components within classical systems, SOQ positions quantum services\nas autonomous, composable, and interoperable entities. We define the\nfoundational principles of SOQ, propose a layered technology stack to support\nits realization, and identify the key research and engineering challenges that\nmust be addressed, including interoperability, hybridity, pricing models,\nservice abstractions, and workforce development. This approach is of vital\nimportance for the advancement of quantum technology because it enables the\nscalable, modular, and interoperable integration of quantum computing into\nreal-world software systems independently and without relying on a dedicated\nclassical environment to manage quantum processing."}
{"id": "2510.03697", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.03697", "abs": "https://arxiv.org/abs/2510.03697", "authors": ["Benjamin Marsh", "Paolo Serafino"], "title": "A Time-Bound Signature Scheme for Blockchains", "comment": "Accepted to the 2025 IEEE International Conference on Blockchain", "summary": "We introduce a modified Schnorr signature scheme to allow for time-bound\nsignatures for transaction fee auction bidding and smart contract purposes in a\nblockchain context, ensuring an honest producer can only validate a signature\nbefore a given block height. The immutable blockchain is used as a source of\nuniversal time for the signature scheme. We show the use of such a signature\nscheme leads to lower MEV revenue for builders. We then apply our time-bound\nsignatures to Ethereum's EIP-1559 and show how it can be used to mitigate the\neffect of MEV on predicted equilibrium strategies."}
{"id": "2510.03680", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03680", "abs": "https://arxiv.org/abs/2510.03680", "authors": ["Bumjun Kim", "Dongjae Jeon", "Dueun Kim", "Wonje Jeung", "Albert No"], "title": "Rainbow Padding: Mitigating Early Termination in Instruction-Tuned Diffusion LLMs", "comment": "25 pages. Project page available\n  at~\\url{https://ai-isl.github.io/rainbow-padding}", "summary": "Diffusion large language models (dLLMs) have emerged as a promising\nalternative to autoregressive models, offering flexible generation orders and\nstrong performance on complex reasoning tasks. However, instruction-tuned dLLMs\nexhibit a critical vulnerability we term \\texttt{<eos>} overflow: as allocated\nsequence length increases, responses paradoxically become shorter, collapsing\ninto early termination or degenerating into streams of \\texttt{<eos>} tokens.\nAlthough noticed in practice, this issue has not been systematically analyzed.\nWe trace its root cause to the dual role of \\texttt{<eos>} as both termination\nand padding, which concentrates probability mass on \\texttt{<eos>} at later\npositions and propagates backward to trigger early termination. To address\nthis, we introduce Rainbow Padding, a simple remedy that replaces repeated\n\\texttt{<eos>} placeholders with a repeating cycle of distinct padding tokens,\ndistributing probability mass and breaking \\texttt{<eos>} dominance.\nExperiments show that Rainbow Padding substantially improves length robustness\nand output quality, with as few as seven padding tokens sufficient to prevent\nearly termination. Moreover, the method integrates efficiently into existing\ninstruction-tuned models: LoRA fine-tuning for a single epoch on minimal data\nyields significant improvements, making this solution highly practical. The\ncode is publicly available at https://github.com/quasar529/rainbow-padding."}
{"id": "2510.03894", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.03894", "abs": "https://arxiv.org/abs/2510.03894", "authors": ["Antonios Saravanos"], "title": "A Brief History of the Waterfall Model: Past, Present, and Future", "comment": null, "summary": "The waterfall model, one of the earliest software development methodologies,\nhas played a foundational role in shaping contemporary software engineering\npractices. This paper provides a historical and critical overview of the model,\ntracing its conceptual origins in software engineering, its formalization by\nRoyce, and its evolution through decades of industry adoption and critique.\nAlthough often criticized for its rigidity, shortcomings, and high failure\nrates, the waterfall model persists in specific domains. Its principles\ncontinue to influence contemporary hybrid development frameworks that combine\ntraditional and agile methods. Drawing on a range of scholarly sources, this\nstudy synthesizes key developments in the perception and application of the\nwaterfall model. The analysis highlights how the model has shifted from a\nstandalone framework to a component within modern hybrid methodologies. By\nrevisiting its origins, assessing its present utility, and examining its role\nin contemporary development practices, this paper argues that the waterfall\nmodel remains relevant, not as a relic of the past but as part of context-aware\ndevelopment strategies. The paper contends that the model's enduring relevance\nlies in its adaptability. By recognizing both its limitations and its\nstrengths, and by understanding its integration within hybrid approaches,\npractitioners can make more informed decisions about methodology selection and\nprocess design in diverse development environments."}
{"id": "2510.03705", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.03705", "abs": "https://arxiv.org/abs/2510.03705", "authors": ["Yulin Chen", "Haoran Li", "Yuan Sui", "Yangqiu Song", "Bryan Hooi"], "title": "Backdoor-Powered Prompt Injection Attacks Nullify Defense Methods", "comment": "EMNLP 2025 Findings", "summary": "With the development of technology, large language models (LLMs) have\ndominated the downstream natural language processing (NLP) tasks. However,\nbecause of the LLMs' instruction-following abilities and inability to\ndistinguish the instructions in the data content, such as web pages from search\nengines, the LLMs are vulnerable to prompt injection attacks. These attacks\ntrick the LLMs into deviating from the original input instruction and executing\nthe attackers' target instruction. Recently, various instruction hierarchy\ndefense strategies are proposed to effectively defend against prompt injection\nattacks via fine-tuning. In this paper, we explore more vicious attacks that\nnullify the prompt injection defense methods, even the instruction hierarchy:\nbackdoor-powered prompt injection attacks, where the attackers utilize the\nbackdoor attack for prompt injection attack purposes. Specifically, the\nattackers poison the supervised fine-tuning samples and insert the backdoor\ninto the model. Once the trigger is activated, the backdoored model executes\nthe injected instruction surrounded by the trigger. We construct a benchmark\nfor comprehensive evaluation. Our experiments demonstrate that backdoor-powered\nprompt injection attacks are more harmful than previous prompt injection\nattacks, nullifying existing prompt injection defense methods, even the\ninstruction hierarchy techniques."}
{"id": "2510.03696", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.03696", "abs": "https://arxiv.org/abs/2510.03696", "authors": ["Deepak Babu Piskala", "Sharlene Chen", "Udita Patel", "Parul Kalra", "Rafael Castrillo"], "title": "Mind the Goal: Data-Efficient Goal-Oriented Evaluation of Conversational Agents and Chatbots using Teacher Models", "comment": null, "summary": "Evaluating the quality of multi-turn chatbot interactions remains\nchallenging, as most existing methods assess interactions at the turn level\nwithout addressing whether a user's overarching goal was fulfilled. A ``goal''\nhere refers to an information need or task, such as asking for policy\ninformation or applying for leave. We propose a comprehensive framework for\ngoal-oriented evaluation of multi-agent systems (MAS), introducing the\n\\textbf{Goal Success Rate (GSR)} to measure the percentage of fulfilled goals,\nand a \\textbf{Root Cause of Failure (RCOF)} taxonomy to identify reasons for\nfailure in multi-agent chatbots. Our method segments conversations by user\ngoals and evaluates success using all relevant turns. We present a model-based\nevaluation system combining teacher LLMs, where domain experts define goals,\nset quality standards serving as a guidance for the LLMs. The LLMs use\n``thinking tokens'' to produce interpretable rationales, enabling\n\\textit{explainable}, \\textit{data-efficient} evaluations. In an enterprise\nsetting, we apply our framework to evaluate AIDA, a zero-to-one employee\nconversational agent system built as a ground-up multi-agent conversational\nagent, and observe GSR improvement from 63\\% to 79\\% over six months since its\ninception. Our framework is generic and offers actionable insights through a\ndetailed defect taxonomy based on analysis of failure points in multi-agent\nchatbots, diagnosing overall success, identifying key failure modes, and\ninforming system improvements."}
{"id": "2510.03902", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.03902", "abs": "https://arxiv.org/abs/2510.03902", "authors": ["Rana Nameer Hussain Khan", "Dawood Wasif", "Jin-Hee Cho", "Ali Butt"], "title": "Multi-Agent Code-Orchestrated Generation for Reliable Infrastructure-as-Code", "comment": null, "summary": "The increasing complexity of cloud-native infrastructure has made\nInfrastructure-as-Code (IaC) essential for reproducible and scalable\ndeployments. While large language models (LLMs) have shown promise in\ngenerating IaC snippets from natural language prompts, their monolithic,\nsingle-pass generation approach often results in syntactic errors, policy\nviolations, and unscalable designs. In this paper, we propose MACOG\n(Multi-Agent Code-Orchestrated Generation), a novel multi-agent LLM-based\narchitecture for IaC generation that decomposes the task into modular subtasks\nhandled by specialized agents: Architect, Provider Harmonizer, Engineer,\nReviewer, Security Prover, Cost and Capacity Planner, DevOps, and Memory\nCurator. The agents interact via a shared-blackboard, finite-state orchestrator\nlayer, and collectively produce Terraform configurations that are not only\nsyntactically valid but also policy-compliant and semantically coherent. To\nensure infrastructure correctness and governance, we incorporate Terraform Plan\nfor execution validation and Open Policy Agent (OPA) for customizable policy\nenforcement. We evaluate MACOG using the IaC-Eval benchmark, where MACOG is the\ntop enhancement across models, e.g., GPT-5 improves from 54.90 (RAG) to 74.02\nand Gemini-2.5 Pro from 43.56 to 60.13, with concurrent gains on BLEU,\nCodeBERTScore, and an LLM-judge metric. Ablations show constrained decoding and\ndeploy feedback are critical: removing them drops IaC-Eval to 64.89 and 56.93,\nrespectively."}
{"id": "2510.03720", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.03720", "abs": "https://arxiv.org/abs/2510.03720", "authors": ["Dongyang Zhan", "Zhaofeng Yu", "Xiangzhan Yu", "Hongli Zhang", "Lin Ye"], "title": "Shrinking the Kernel Attack Surface Through Static and Dynamic Syscall Limitation", "comment": "13 pages, 5 figures. Accepted for publication in IEEE Transactions on\n  Services Computing (TSC), 2023", "summary": "Linux Seccomp is widely used by the program developers and the system\nmaintainers to secure the operating systems, which can block unused syscalls\nfor different applications and containers to shrink the attack surface of the\noperating systems. However, it is difficult to configure the whitelist of a\ncontainer or application without the help of program developers. Docker\ncontainers block about only 50 syscalls by default, and lots of unblocked\nuseless syscalls introduce a big kernel attack surface. To obtain the dependent\nsyscalls, dynamic tracking is a straight-forward approach but it cannot get the\nfull syscall list. Static analysis can construct an over-approximated syscall\nlist, but the list contains many false positives. In this paper, a systematic\ndependent syscall analysis approach, sysverify, is proposed by combining static\nanalysis and dynamic verification together to shrink the kernel attack surface.\nThe semantic gap between the binary executables and syscalls is bridged by\nanalyzing the binary and the source code, which builds the mapping between the\nlibrary APIs and syscalls systematically. To further reduce the attack surface\nat best effort, we propose a dynamic verification approach to intercept and\nanalyze the security of the invocations of indirect-call-related or rarely\ninvoked syscalls with low overhead."}
{"id": "2510.03700", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03700", "abs": "https://arxiv.org/abs/2510.03700", "authors": ["Seungseop Lim", "Gibaeg Kim", "Hyunkyung Lee", "Wooseok Han", "Jean Seo", "Jaehyo Yoo", "Eunho Yang"], "title": "H-DDx: A Hierarchical Evaluation Framework for Differential Diagnosis", "comment": "GenAI4Health @NeurIPS 2025", "summary": "An accurate differential diagnosis (DDx) is essential for patient care,\nshaping therapeutic decisions and influencing outcomes. Recently, Large\nLanguage Models (LLMs) have emerged as promising tools to support this process\nby generating a DDx list from patient narratives. However, existing evaluations\nof LLMs in this domain primarily rely on flat metrics, such as Top-k accuracy,\nwhich fail to distinguish between clinically relevant near-misses and\ndiagnostically distant errors. To mitigate this limitation, we introduce H-DDx,\na hierarchical evaluation framework that better reflects clinical relevance.\nH-DDx leverages a retrieval and reranking pipeline to map free-text diagnoses\nto ICD-10 codes and applies a hierarchical metric that credits predictions\nclosely related to the ground-truth diagnosis. In benchmarking 22 leading\nmodels, we show that conventional flat metrics underestimate performance by\noverlooking clinically meaningful outputs, with our results highlighting the\nstrengths of domain-specialized open-source models. Furthermore, our framework\nenhances interpretability by revealing hierarchical error patterns,\ndemonstrating that LLMs often correctly identify the broader clinical context\neven when the precise diagnosis is missed."}
{"id": "2510.03914", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03914", "abs": "https://arxiv.org/abs/2510.03914", "authors": ["Yonnel Chen Kuang Piao", "Jean Carlors Paul", "Leuson Da Silva", "Arghavan Moradi Dakhel", "Mohammad Hamdaqa", "Foutse Khomh"], "title": "Refactoring with LLMs: Bridging Human Expertise and Machine Understanding", "comment": "43 pages, 2 figures, 9 tables", "summary": "Code refactoring is a fundamental software engineering practice aimed at\nimproving code quality and maintainability. Despite its importance, developers\noften neglect refactoring due to the significant time, effort, and resources it\nrequires, as well as the lack of immediate functional rewards. Although several\nautomated refactoring tools have been proposed, they remain limited in\nsupporting a broad spectrum of refactoring types. In this study, we explore\nwhether instruction strategies inspired by human best-practice guidelines can\nenhance the ability of Large Language Models (LLMs) to perform diverse\nrefactoring tasks automatically. Leveraging the instruction-following and code\ncomprehension capabilities of state-of-the-art LLMs (e.g., GPT-mini and\nDeepSeek-V3), we draw on Martin Fowler's refactoring guidelines to design\nmultiple instruction strategies that encode motivations, procedural steps, and\ntransformation objectives for 61 well-known refactoring types. We evaluate\nthese strategies on benchmark examples and real-world code snippets from GitHub\nprojects. Our results show that instruction designs grounded in Fowler's\nguidelines enable LLMs to successfully perform all benchmark refactoring types\nand preserve program semantics in real-world settings, an essential criterion\nfor effective refactoring. Moreover, while descriptive instructions are more\ninterpretable to humans, our results show that rule-based instructions often\nlead to better performance in specific scenarios. Interestingly, allowing\nmodels to focus on the overall goal of refactoring, rather than prescribing a\nfixed transformation type, can yield even greater improvements in code quality."}
{"id": "2510.03737", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.03737", "abs": "https://arxiv.org/abs/2510.03737", "authors": ["Dongyang Zhan", "Zhaofeng Yu", "Xiangzhan Yu", "Hongli Zhang", "Lin Ye", "Likun Liu"], "title": "Securing Operating Systems Through Fine-grained Kernel Access Limitation for IoT Systems", "comment": "14 pages, 3 figures. Accepted for publication in IEEE Internet of\n  Things Journal (IOTJ), 2023", "summary": "With the development of Internet of Things (IoT), it is gaining a lot of\nattention. It is important to secure the embedded systems with low overhead.\nThe Linux Seccomp is widely used by developers to secure the kernels by\nblocking the access of unused syscalls, which introduces less overhead.\nHowever, there are no systematic Seccomp configuration approaches for IoT\napplications without the help of developers. In addition, the existing Seccomp\nconfiguration approaches are coarse-grained, which cannot analyze and limit the\nsyscall arguments. In this paper, a novel static dependent syscall analysis\napproach for embedded applications is proposed, which can obtain all of the\npossible dependent syscalls and the corresponding arguments of the target\napplications. So, a fine-grained kernel access limitation can be performed for\nthe IoT applications. To this end, the mappings between dynamic library APIs\nand syscalls according with their arguments are built, by analyzing the control\nflow graphs and the data dependency relationships of the dynamic libraries. To\nthe best of our knowledge, this is the first work to generate the fine-grained\nSeccomp profile for embedded applications."}
{"id": "2510.03727", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03727", "abs": "https://arxiv.org/abs/2510.03727", "authors": ["Xuehai He"], "title": "Bridging the Gap Between Multimodal Foundation Models and World Models", "comment": "PhD thesis", "summary": "Humans understand the world through the integration of multiple sensory\nmodalities, enabling them to perceive, reason about, and imagine dynamic\nphysical processes. Inspired by this capability, multimodal foundation models\n(MFMs) have emerged as powerful tools for multimodal understanding and\ngeneration. However, today's MFMs fall short of serving as effective world\nmodels. They lack the essential ability such as perform counterfactual\nreasoning, simulate dynamics, understand the spatiotemporal information,\ncontrol generated visual outcomes, and perform multifaceted reasoning. We\ninvestigates what it takes to bridge the gap between multimodal foundation\nmodels and world models. We begin by improving the reasoning capabilities of\nMFMs through discriminative tasks and equipping MFMs with structured reasoning\nskills, such as causal inference, counterfactual thinking, and spatiotemporal\nreasoning, enabling them to go beyond surface correlations and understand\ndeeper relationships within visual and textual data. Next, we explore\ngenerative capabilities of multimodal foundation models across both image and\nvideo modalities, introducing new frameworks for structured and controllable\ngeneration. Our approaches incorporate scene graphs, multimodal conditioning,\nand multimodal alignment strategies to guide the generation process, ensuring\nconsistency with high-level semantics and fine-grained user intent. We further\nextend these techniques to controllable 4D generation, enabling interactive,\neditable, and morphable object synthesis over time and space."}
{"id": "2510.03920", "categories": ["cs.SE", "cs.SI"], "pdf": "https://arxiv.org/pdf/2510.03920", "abs": "https://arxiv.org/abs/2510.03920", "authors": ["Ravi Kalluri"], "title": "Why Does the Engineering Manager Still Exist in Agile Software Development?", "comment": "12 pages, 3 figures, 2 tables", "summary": "Although Agile methodologies emphasize decentralized decision-making and team\nautonomy, engineering managers continue to be employed in Agile software\norganizations. This apparent paradox suggests that traditional managerial\nfunctions persist despite the theoretical displacement of managerial hierarchy\nin Agile. This paper explores the persistence of engineering managers through a\nmultidimensional framework encompassing historical context, theoretical\ntensions, organizational realities, empirical evidence, evolving managerial\nroles, and practical implications. A systematic literature review underpins our\nmultifaceted analysis, supplemented by illustrative case studies. We conclude\nby proposing a conceptual model that reconciles Agile principles with\nmanagerial necessity, offering guidance for practitioners, researchers, and\ntool designers. Implications for leadership development, tool integration, and\nfuture research are discussed."}
{"id": "2510.03752", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.03752", "abs": "https://arxiv.org/abs/2510.03752", "authors": ["Rohit Chatterjee", "Changrui Mu", "Prashant Nalini Vasudevan"], "title": "Public-Key Encryption from the MinRank Problem", "comment": null, "summary": "We construct a public-key encryption scheme from the hardness of the\n(planted) MinRank problem over uniformly random instances. This corresponds to\nthe hardness of decoding random linear rank-metric codes. Existing\nconstructions of public-key encryption from such problems require hardness for\nstructured instances arising from the masking of efficiently decodable codes.\nCentral to our construction is the development of a new notion of duality for\nrank-metric codes."}
{"id": "2510.03771", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03771", "abs": "https://arxiv.org/abs/2510.03771", "authors": ["Divij Handa", "David Blincoe", "Orson Adams", "Yinlin Fu"], "title": "OptAgent: Optimizing Query Rewriting for E-commerce via Multi-Agent Simulation", "comment": null, "summary": "Deploying capable and user-aligned LLM-based systems necessitates reliable\nevaluation. While LLMs excel in verifiable tasks like coding and mathematics,\nwhere gold-standard solutions are available, adoption remains challenging for\nsubjective tasks that lack a single correct answer. E-commerce Query Rewriting\n(QR) is one such problem where determining whether a rewritten query properly\ncaptures the user intent is extremely difficult to figure out algorithmically.\nIn this work, we introduce OptAgent, a novel framework that combines\nmulti-agent simulations with genetic algorithms to verify and optimize queries\nfor QR. Instead of relying on a static reward model or a single LLM judge, our\napproach uses multiple LLM-based agents, each acting as a simulated shopping\ncustomer, as a dynamic reward signal. The average of these agent-derived scores\nserves as an effective fitness function for an evolutionary algorithm that\niteratively refines the user's initial query. We evaluate OptAgent on a dataset\nof 1000 real-world e-commerce queries in five different categories, and we\nobserve an average improvement of 21.98% over the original user query and 3.36%\nover a Best-of-N LLM rewriting baseline."}
{"id": "2510.04078", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.04078", "abs": "https://arxiv.org/abs/2510.04078", "authors": ["Han Hu", "Wei Minn", "Yonghui Liu", "Jiakun Liu", "Ferdian Thung", "Terry Yue Zhuo", "Lwin Khin Shar", "Debin Gao", "David Lo"], "title": "Bamboo: LLM-Driven Discovery of API-Permission Mappings in the Android Framework", "comment": null, "summary": "The permission mechanism in the Android Framework is integral to safeguarding\nthe privacy of users by managing users' and processes' access to sensitive\nresources and operations. As such, developers need to be equipped with an\nin-depth understanding of API permissions to build robust Android apps.\nUnfortunately, the official API documentation by Android chronically suffers\nfrom imprecision and incompleteness, causing developers to spend significant\neffort to accurately discern necessary permissions. This potentially leads to\nincorrect permission declarations in Android app development, potentially\nresulting in security violations and app failures. Recent efforts in improving\npermission specification primarily leverage static and dynamic code analyses to\nuncover API-permission mappings within the Android framework. Yet, these\nmethodologies encounter substantial shortcomings, including poor adaptability\nto Android SDK and Framework updates, restricted code coverage, and a\npropensity to overlook essential API-permission mappings in intricate\ncodebases. This paper introduces a pioneering approach utilizing large language\nmodels (LLMs) for a systematic examination of API-permission mappings. In\naddition to employing LLMs, we integrate a dual-role prompting strategy and an\nAPI-driven code generation approach into our mapping discovery pipeline,\nresulting in the development of the corresponding tool, \\tool{}. We formulate\nthree research questions to evaluate the efficacy of \\tool{} against\nstate-of-the-art baselines, assess the completeness of official SDK\ndocumentation, and analyze the evolution of permission-required APIs across\ndifferent SDK releases. Our experimental results reveal that \\tool{} identifies\n2,234, 3,552, and 4,576 API-permission mappings in Android versions 6, 7, and\n10 respectively, substantially outprforming existing baselines."}
{"id": "2510.03761", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03761", "abs": "https://arxiv.org/abs/2510.03761", "authors": ["Richard A. Dubniczky", "Bertalan Borsos", "Tihanyi Norbert"], "title": "You Have Been LaTeXpOsEd: A Systematic Analysis of Information Leakage in Preprint Archives Using Large Language Models", "comment": null, "summary": "The widespread use of preprint repositories such as arXiv has accelerated the\ncommunication of scientific results but also introduced overlooked security\nrisks. Beyond PDFs, these platforms provide unrestricted access to original\nsource materials, including LaTeX sources, auxiliary code, figures, and\nembedded comments. In the absence of sanitization, submissions may disclose\nsensitive information that adversaries can harvest using open-source\nintelligence. In this work, we present the first large-scale security audit of\npreprint archives, analyzing more than 1.2 TB of source data from 100,000 arXiv\nsubmissions. We introduce LaTeXpOsEd, a four-stage framework that integrates\npattern matching, logical filtering, traditional harvesting techniques, and\nlarge language models (LLMs) to uncover hidden disclosures within\nnon-referenced files and LaTeX comments. To evaluate LLMs' secret-detection\ncapabilities, we introduce LLMSec-DB, a benchmark on which we tested 25\nstate-of-the-art models. Our analysis uncovered thousands of PII leaks,\nGPS-tagged EXIF files, publicly available Google Drive and Dropbox folders,\neditable private SharePoint links, exposed GitHub and Google credentials, and\ncloud API keys. We also uncovered confidential author communications, internal\ndisagreements, and conference submission credentials, exposing information that\nposes serious reputational risks to both researchers and institutions. We urge\nthe research community and repository operators to take immediate action to\nclose these hidden security gaps. To support open science, we release all\nscripts and methods from this study but withhold sensitive findings that could\nbe misused, in line with ethical principles. The source code and related\nmaterial are available at the project website https://github.com/LaTeXpOsEd"}
{"id": "2510.03777", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03777", "abs": "https://arxiv.org/abs/2510.03777", "authors": ["Divij Handa", "Mihir Parmar", "Aswin RRV", "Md Nayem Uddin", "Hamid Palangi", "Chitta Baral"], "title": "GuidedSampling: Steering LLMs Towards Diverse Candidate Solutions at Inference-Time", "comment": null, "summary": "Repeated Sampling (RS) is a simple inference-time algorithm that has been\nshown to improve model performance on complex tasks. Although it is an\neffective way of scaling inference time, it often struggles to generate diverse\nsolution candidates, frequently relying on the same underlying approach to\nsolve the problem and thus producing redundant samples. To address this\nlimitation, we propose a new inference algorithm, GuidedSampling, which\ndecouples the exploration and generation phases during inference, increasing\ndiversity of generated candidate solutions. The exploration phase identifies\nmultiple concepts that can be utilized to solve the problem, while the\ngeneration phase applies a specific concept to provide final solution\ncandidates. We first define the theoretical bounds of GuidedSampling and then\nempirically demonstrate that it improves the performance of base model at\npass@50 by on an average ~21.6% across various benchmarks compared to RS.\nFurthermore, models trained on trajectories of GuidedSampling exhibit\nsubstantial performance improvements at pass@5 by on an average ~9.7%, compared\nto models trained on traditional RS. Additionally, models trained with\nGuidedSampling increases the average number of concepts per instance (1.67 ->\n3.03), yielding a diverse set of candidates than traditional RS."}
{"id": "2510.04135", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04135", "abs": "https://arxiv.org/abs/2510.04135", "authors": ["Jingzhi Gong", "Yixin Bian", "Luis de la Cal", "Giovanni Pinna", "Anisha Uteem", "David Williams", "Mar Zamorano", "Karine Even-Mendoza", "W. B. Langdon", "Hector Menendez", "Federica Sarro"], "title": "GA4GC: Greener Agent for Greener Code via Multi-Objective Configuration Optimization", "comment": "Accepted by SSBSE'25 Challenge Track", "summary": "Coding agents powered by LLMs face critical sustainability and scalability\nchallenges in industrial deployment, with single runs consuming over 100k\ntokens and incurring environmental costs that may exceed optimization benefits.\nThis paper introduces GA4GC, the first framework to systematically optimize\ncoding agent runtime (greener agent) and code performance (greener code)\ntrade-offs by discovering Pareto-optimal agent hyperparameters and prompt\ntemplates. Evaluation on the SWE-Perf benchmark demonstrates up to 135x\nhypervolume improvement, reducing agent runtime by 37.7% while improving\ncorrectness. Our findings establish temperature as the most critical\nhyperparameter, and provide actionable strategies to balance agent\nsustainability with code optimization effectiveness in industrial deployment."}
{"id": "2510.03770", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.03770", "abs": "https://arxiv.org/abs/2510.03770", "authors": ["David Megias"], "title": "Complex Domain Approach for Reversible Data Hiding and Homomorphic Encryption: General Framework and Application to Dispersed Data", "comment": null, "summary": "Ensuring the trustworthiness of data from distributed and\nresource-constrained environments, such as Wireless Sensor Networks or IoT\ndevices, is critical. Existing Reversible Data Hiding (RDH) methods for scalar\ndata suffer from low embedding capacity and poor intrinsic mixing between host\ndata and watermark. This paper introduces Hiding in the Imaginary Domain with\nData Encryption (H[i]dden), a novel framework based on complex number\narithmetic for simultaneous information embedding and encryption. The H[i]dden\nframework offers perfect reversibility, in-principle unlimited watermark size,\nand intrinsic data-watermark mixing. The paper further introduces two\nprotocols: H[i]dden-EG, for joint reversible data hiding and encryption, and\nH[i]dden-AggP, for privacy-preserving aggregation of watermarked data, based on\npartially homomorphic encryption. These protocols provide efficient and\nresilient solutions for data integrity, provenance and confidentiality, serving\nas a foundation for new schemes based on the algebraic properties of the\ncomplex domain."}
{"id": "2510.03845", "categories": ["cs.AI", "cs.GT", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.03845", "abs": "https://arxiv.org/abs/2510.03845", "authors": ["Gon Buzaglo", "Noah Golowich", "Elad Hazan"], "title": "The Hidden Game Problem", "comment": null, "summary": "This paper investigates a class of games with large strategy spaces,\nmotivated by challenges in AI alignment and language games. We introduce the\nhidden game problem, where for each player, an unknown subset of strategies\nconsistently yields higher rewards compared to the rest. The central question\nis whether efficient regret minimization algorithms can be designed to discover\nand exploit such hidden structures, leading to equilibrium in these subgames\nwhile maintaining rationality in general. We answer this question affirmatively\nby developing a composition of regret minimization techniques that achieve\noptimal external and swap regret bounds. Our approach ensures rapid convergence\nto correlated equilibria in hidden subgames, leveraging the hidden game\nstructure for improved computational efficiency."}
{"id": "2510.04143", "categories": ["cs.SE", "D.2.13"], "pdf": "https://arxiv.org/pdf/2510.04143", "abs": "https://arxiv.org/abs/2510.04143", "authors": ["Konstantinos Kitsios", "Francesco Sovrano", "Earl T. Barr", "Alberto Bacchelli"], "title": "Detecting Semantic Clones of Unseen Functionality", "comment": "13 pages, 3 figures, accepted for publication (to appear) in the 40th\n  IEEE/ACM International Conference on Automated Software Engineering, ASE 2025", "summary": "Semantic code clone detection is the task of detecting whether two snippets\nof code implement the same functionality (e.g., Sort Array). Recently, many\nneural models achieved near-perfect performance on this task. These models seek\nto make inferences based on their training data. Consequently, they better\ndetect clones similar to those they have seen during training and may struggle\nto detect those they have not. Developers seeking clones are, of course,\ninterested in both types of clones. We confirm this claim through a literature\nreview, identifying three practical clone detection tasks in which the model's\ngoal is to detect clones of a functionality even if it was trained on clones of\ndifferent functionalities. In light of this finding, we re-evaluate six\nstate-of-the-art models, including both task-specific models and generative\nLLMs, on the task of detecting clones of unseen functionality. Our experiments\nreveal a drop in F1 of up to 48% (average 31%) for task-specific models. LLMs\nperform on par with task-specific models without explicit training for clone\ndetection, but generalize better to unseen functionalities, where F1 drops up\nto 5% (average 3%) instead. We propose and evaluate the use of contrastive\nlearning to improve the performance of existing models on clones of unseen\nfunctionality. We draw inspiration from the computer vision and natural\nlanguage processing fields where contrastive learning excels at measuring\nsimilarity between two objects, even if they come from classes unseen during\ntraining. We replace the final classifier of the task-specific models with a\ncontrastive classifier, while for the generative LLMs we propose contrastive\nin-context learning, guiding the LLMs to focus on the differences between\nclones and non-clones. The F1 on clones of unseen functionality is improved by\nup to 26% (average 9%) for task-specific models and up to 5% (average 3%) for\nLLMs."}
{"id": "2510.03819", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.03819", "abs": "https://arxiv.org/abs/2510.03819", "authors": ["Chunyi Zhang", "Qinghong Wei", "Xiaoqi Li"], "title": "Security Analysis of Ponzi Schemes in Ethereum Smart Contracts", "comment": null, "summary": "The rapid advancement of blockchain technology has precipitated the\nwidespread adoption of Ethereum and smart contracts across a variety of\nsectors. However, this has also given rise to numerous fraudulent activities,\nwith many speculators embedding Ponzi schemes within smart contracts, resulting\nin significant financial losses for investors. Currently, there is a lack of\neffective methods for identifying and analyzing such new types of fraudulent\nactivities. This paper categorizes these scams into four structural types and\nexplores the intrinsic characteristics of Ponzi scheme contract source code\nfrom a program analysis perspective. The Mythril tool is employed to conduct\nstatic and dynamic analyses of representative cases, thereby revealing their\nvulnerabilities and operational mechanisms. Furthermore, this paper employs\nshell scripts and command patterns to conduct batch detection of open-source\nsmart contract code, thereby unveiling the common characteristics of Ponzi\nscheme smart contracts."}
{"id": "2510.03847", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03847", "abs": "https://arxiv.org/abs/2510.03847", "authors": ["Raghav Sharma", "Manan Mehta"], "title": "Small Language Models for Agentic Systems: A Survey of Architectures, Capabilities, and Deployment Trade offs", "comment": "9 Pages", "summary": "Small language models (SLMs; 1-12B params, sometimes up to 20B) are\nsufficient and often superior for agentic workloads where the objective is\nschema- and API-constrained accuracy rather than open-ended generation. We\nsynthesize recent evidence across open and proprietary SLMs (Phi-4-Mini,\nQwen-2.5-7B, Gemma-2-9B, Llama-3.2-1B/3B, Ministral-3B/8B, Apple on-device 3B,\nDeepSeek-R1-Distill) and connect it to modern evaluations (BFCL v3/v4,\nStableToolBench) and serving stacks (vLLM, SGLang, TensorRT-LLM) paired with\nguided decoding libraries (XGrammar, Outlines). We formalize SLM-default,\nLLM-fallback systems with uncertainty-aware routing and verifier cascades, and\npropose engineering metrics that reflect real production goals: cost per\nsuccessful task (CPS), schema validity rate, executable call rate, p50/p95\nlatency, and energy per request. Guided decoding, strict JSON Schema outputs,\nand validator-first tool execution close much of the capability gap with larger\nmodels and often let SLMs match or surpass LLMs on tool use, function calling,\nand RAG at 10x-100x lower token cost with materially better latency and energy.\nWe provide design patterns for agent stacks that prioritize SLMs: schema-first\nprompting, type-safe function registries, confidence scoring with verifier\nrollups, and lightweight adaptation via LoRA/QLoRA. We also delineate limits\nwhere fallback remains valuable (open-domain reasoning and some long-horizon\nplanning). The result is a practical blueprint for building fast, inexpensive,\nand reliable agents that default to SLMs while preserving headroom with\ntargeted LLM assistance.\n  Keywords: small language models, agents, function calling, structured\noutputs, JSON Schema, guided decoding, LoRA/QLoRA, routing, energy efficiency,\nedge inference"}
{"id": "2510.04166", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04166", "abs": "https://arxiv.org/abs/2510.04166", "authors": ["Marco Edoardo Palma", "Pooja Rani", "Harald C. Gall"], "title": "Multi Language Models for On-the-Fly Syntax Highlighting", "comment": null, "summary": "Syntax highlighting is a critical feature in modern software development\nenvironments, enhancing code readability and developer productivity. However,\ndelivering accurate highlighting in real time remains challenging for online\nand web-based development tools due to strict time and memory constraints on\nbackend services. These systems must serve highlights rapidly and frequently,\neven when code is partially valid or invalid. This has led to on-the-fly syntax\nhighlighting, where visual annotations are generated just before content is\nserved, often at high request rates and under incomplete input conditions. To\nmeet these demands efficiently, state-of-the-art models use deep learning to\nlearn the behavior of brute-force syntax highlighting resolvers, tools that are\neasy to implement but too slow for production. Through the Deep Abstraction\nprocess, brute-force strategies are encoded into fast statistical models that\nachieve both high accuracy and low-latency inference. Despite their success,\nsuch models face key challenges: they support only one programming language per\nmodel, require large datasets from slow brute-force generators, and involve\nresource-intensive training. In multi-language environments, this means\nmaintaining multiple independent models, increasing system complexity and\noperational cost. This work addresses these issues by introducing a unified\nmodel capable of highlighting up to six mainstream programming languages,\nreducing deployment complexity by a factor of six and improving performance on\nunseen languages. A novel normalization technique significantly enhances model\ngeneralization, while few-shot learning experiments show that a small number of\noracle samples can replace large datasets, minimizing dependence on brute-force\ngenerators. Combined, these innovations enable efficient, scalable, and\ncost-effective syntax highlighting across diverse programming languages."}
{"id": "2510.03831", "categories": ["cs.CR", "cs.IT", "cs.LG", "eess.SP", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.03831", "abs": "https://arxiv.org/abs/2510.03831", "authors": ["Pedro Ivo da Cruz", "Dimitri Silva", "Tito Spadini", "Ricardo Suyama", "Murilo Bellezoni Loiola"], "title": "Pilot Contamination Attacks Detection with Machine Learning for Multi-User Massive MIMO", "comment": "This version of the article has been accepted for publication, after\n  peer review and is subject to Springer Nature's AM terms of use, but is not\n  the Version of Record and does not reflect post-acceptance improvements, or\n  any corrections. The Version of Record is available online at:\n  https://doi.org/10.1007/s11235-024-01163-0", "summary": "Massive multiple-input multiple-output (MMIMO) is essential to modern\nwireless communication systems, like 5G and 6G, but it is vulnerable to active\neavesdropping attacks. One type of such attack is the pilot contamination\nattack (PCA), where a malicious user copies pilot signals from an authentic\nuser during uplink, intentionally interfering with the base station's (BS)\nchannel estimation accuracy. In this work, we propose to use a Decision Tree\n(DT) algorithm for PCA detection at the BS in a multi-user system. We present a\nmethodology to generate training data for the DT classifier and select the best\nDT according to their depth. Then, we simulate different scenarios that could\nbe encountered in practice and compare the DT to a classical technique based on\nlikelihood ratio testing (LRT) submitted to the same scenarios. The results\nrevealed that a DT with only one level of depth is sufficient to outperform the\nLRT. The DT shows a good performance regarding the probability of detection in\nnoisy scenarios and when the malicious user transmits with low power, in which\ncase the LRT fails to detect the PCA. We also show that the reason for the good\nperformance of the DT is its ability to compute a threshold that separates PCA\ndata from non-PCA data better than the LRT's threshold. Moreover, the DT does\nnot necessitate prior knowledge of noise power or assumptions regarding the\nsignal power of malicious users, prerequisites typically essential for LRT and\nother hypothesis testing methodologies."}
{"id": "2510.03851", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03851", "abs": "https://arxiv.org/abs/2510.03851", "authors": ["Ruiying Ma", "Chieh-Jan Mike Liang", "Yanjie Gao", "Francis Y. Yan"], "title": "Algorithm Generation via Creative Ideation", "comment": null, "summary": "Designing system algorithms remains challenging, where the discontinuous\nnature of the solution space often forces system engineers to rely on generic\nheuristics at the expense of performance. We study whether LLMs can practically\ndrive algorithm generation, and find that they are biased towards well-known\ngeneric designs, rather than making the creative leaps needed to navigate the\ndiscontinuous solution space. To address this limitation, we introduce\nMetaMuse, a framework for creative ideation built on three self-reflection\nprinciples: (1) quantifying solution diversity and usefulness in measurable\nperformance space, rather than abstract idea space, (2) steering ideation\nthrough external stimuli, rather than internal randomness, and (3) constructing\nexecutable solutions using waypoint reasoning, rather than free-form\nchain-of-thought. Extensive evaluation shows that MetaMuse can generate\nhigh-performing solutions for two critical problems at a global cloud provider:\ncache replacement (reducing cache misses by up to 35.76%) and online bin\npacking (reducing bin usage by up to 30.93%)."}
{"id": "2510.04274", "categories": ["cs.SE", "D.2; I.2; J.6; K.3; K.7"], "pdf": "https://arxiv.org/pdf/2510.04274", "abs": "https://arxiv.org/abs/2510.04274", "authors": ["Damjan Fujs", "Damjan Vavpotič", "Tomaž Hovelja", "Marko Poženel"], "title": "Selecting Cybersecurity Requirements: Effects of LLM Use and Professional Software Development Experience", "comment": "5 pages, 1 figure, 2 tables, presented at IARIA CYBER 2025", "summary": "This study investigates how access to Large Language Models (LLMs) and\nvarying levels of professional software development experience affect the\nprioritization of cybersecurity requirements for web applications. Twenty-three\npostgraduate students participated in a research study to prioritize security\nrequirements (SRs) using the MoSCoW method and subsequently rated their\nproposed solutions against multiple evaluation criteria. We divided\nparticipants into two groups (one with and the other without access to LLM\nsupport during the task). Results showed no significant differences related to\nLLM use, suggesting that access to LLMs did not noticeably influence how\nparticipants evaluated cybersecurity solutions. However, statistically\nsignificant differences emerged between experience groups for certain criteria,\nsuch as estimated cost to develop a feature, perceived impact on user\nexperience, and risk assessment related to non-implementation of the proposed\nfeature. Participants with more professional experience tended to provide\nhigher ratings for user experience impact and lower risk estimates."}
{"id": "2510.03992", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03992", "abs": "https://arxiv.org/abs/2510.03992", "authors": ["Jehyeok Yeon", "Isha Chaudhary", "Gagandeep Singh"], "title": "Quantifying Distributional Robustness of Agentic Tool-Selection", "comment": null, "summary": "Large language models (LLMs) are increasingly deployed in agentic systems\nwhere they map user intents to relevant external tools to fulfill a task. A\ncritical step in this process is tool selection, where a retriever first\nsurfaces candidate tools from a larger pool, after which the LLM selects the\nmost appropriate one. This pipeline presents an underexplored attack surface\nwhere errors in selection can lead to severe outcomes like unauthorized data\naccess or denial of service, all without modifying the agent's model or code.\nWhile existing evaluations measure task performance in benign settings, they\noverlook the specific vulnerabilities of the tool selection mechanism under\nadversarial conditions. To address this gap, we introduce ToolCert, the first\nstatistical framework that formally certifies tool selection robustness.\nToolCert models tool selection as a Bernoulli success process and evaluates it\nagainst a strong, adaptive attacker who introduces adversarial tools with\nmisleading metadata, and are iteratively refined based on the agent's previous\nchoices. By sampling these adversarial interactions, ToolCert produces a\nhigh-confidence lower bound on accuracy, formally quantifying the agent's\nworst-case performance. Our evaluation with ToolCert uncovers the severe\nfragility: under attacks injecting deceptive tools or saturating retrieval, the\ncertified accuracy bound drops near zero, an average performance drop of over\n60% compared to non-adversarial settings. For attacks targeting the retrieval\nand selection stages, the certified accuracy bound plummets to less than 20%\nafter just a single round of adversarial adaptation. ToolCert thus reveals\npreviously unexamined security threats inherent to tool selection and provides\na principled method to quantify an agent's robustness to such threats, a\nnecessary step for the safe deployment of agentic systems."}
{"id": "2510.03859", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03859", "abs": "https://arxiv.org/abs/2510.03859", "authors": ["Raghav Sharma", "Manan Mehta"], "title": "Adaptive and Explainable AI Agents for Anomaly Detection in Critical IoT Infrastructure using LLM-Enhanced Contextual Reasoning", "comment": "22 pages", "summary": "Ensuring that critical IoT systems function safely and smoothly depends a lot\non finding anomalies quickly. As more complex systems, like smart healthcare,\nenergy grids and industrial automation, appear, it is easier to see the\nshortcomings of older methods of detection. Monitoring failures usually happen\nin dynamic, high dimensional situations, especially when data is incomplete,\nmessy or always evolving. Such limits point out the requirement for adaptive,\nintelligent systems that always improve and think. LLMs are now capable of\nsignificantly changing how context is understood and semantic inference is done\nacross all types of data. This proposal suggests using an LLM supported\ncontextual reasoning method along with XAI agents to improve how anomalies are\nfound in significant IoT environments. To discover hidden patterns and notice\ninconsistencies in data streams, it uses attention methods, avoids dealing with\ndetails from every time step and uses memory buffers with meaning. Because no\ncode AI stresses transparency and interpretability, people can check and accept\nthe AI's decisions, helping ensure AI follows company policies. The two\narchitectures are put together in a test that compares the results of the\ntraditional model with those of the suggested LLM enhanced model. Important\nmeasures to check are the accuracy of detection, how much inaccurate\ninformation is included in the results, how clearly the findings can be read\nand how fast the system responds under different test situations. The\nmetaheuristic is tested in simulations of real world smart grid and healthcare\ncontexts to check its adaptability and reliability. From the study, we see that\nthe new approach performs much better than most existing models in both\naccuracy and interpretation, so it could be a good fit for future anomaly\ndetection tasks in IoT"}
{"id": "2510.04349", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04349", "abs": "https://arxiv.org/abs/2510.04349", "authors": ["Dmitry Ustalov", "Egor Bogomolov", "Alexander Bezzubov", "Yaroslav Golubev", "Evgeniy Glukhov", "Georgii Levtsov", "Vladimir Kovalenko"], "title": "Challenge on Optimization of Context Collection for Code Completion", "comment": "7 pages, 3 figures, 5 tables. A report on the Context Collection\n  Workshop co-located with ASE'25", "summary": "The rapid advancement of workflows and methods for software engineering using\nAI emphasizes the need for a systematic evaluation and analysis of their\nability to leverage information from entire projects, particularly in large\ncode bases. In this challenge on optimization of context collection for code\ncompletion, organized by JetBrains in collaboration with Mistral AI as part of\nthe ASE 2025 conference, participants developed efficient mechanisms for\ncollecting context from source code repositories to improve fill-in-the-middle\ncode completions for Python and Kotlin. We constructed a large dataset of\nreal-world code in these two programming languages using permissively licensed\nopen-source projects. The submissions were evaluated based on their ability to\nmaximize completion quality for multiple state-of-the-art neural models using\nthe chrF metric. During the public phase of the competition, nineteen teams\nsubmitted solutions to the Python track and eight teams submitted solutions to\nthe Kotlin track. In the private phase, six teams competed, of which five\nsubmitted papers to the workshop."}
{"id": "2510.03995", "categories": ["cs.CR", "cs.AI", "I.2; E.m"], "pdf": "https://arxiv.org/pdf/2510.03995", "abs": "https://arxiv.org/abs/2510.03995", "authors": ["Nges Brian Njungle", "Eric Jahns", "Milan Stojkov", "Michel A. Kinsy"], "title": "PrivSpike: Employing Homomorphic Encryption for Private Inference of Deep Spiking Neural Networks", "comment": "13 pages, 5 figures", "summary": "Deep learning has become a cornerstone of modern machine learning. It relies\nheavily on vast datasets and significant computational resources for high\nperformance. This data often contains sensitive information, making privacy a\nmajor concern in deep learning. Spiking Neural Networks (SNNs) have emerged as\nan energy-efficient alternative to conventional deep learning approaches.\nNevertheless, SNNs still depend on large volumes of data, inheriting all the\nprivacy challenges of deep learning. Homomorphic encryption addresses this\nchallenge by allowing computations to be performed on encrypted data, ensuring\ndata confidentiality throughout the entire processing pipeline. In this paper,\nwe introduce PRIVSPIKE, a privacy-preserving inference framework for SNNs using\nthe CKKS homomorphic encryption scheme. PRIVSPIKE supports arbitrary depth SNNs\nand introduces two key algorithms for evaluating the Leaky Integrate-and-Fire\nactivation function: (1) a polynomial approximation algorithm designed for\nhigh-performance SNN inference, and (2) a novel scheme-switching algorithm that\noptimizes precision at a higher computational cost. We evaluate PRIVSPIKE on\nMNIST, CIFAR-10, Neuromorphic MNIST, and CIFAR-10 DVS using models from LeNet-5\nand ResNet-19 architectures, achieving encrypted inference accuracies of\n98.10%, 79.3%, 98.1%, and 66.0%, respectively. On a consumer-grade CPU, SNN\nLeNet-5 models achieved inference times of 28 seconds on MNIST and 212 seconds\non Neuromorphic MNIST. For SNN ResNet-19 models, inference took 784 seconds on\nCIFAR-10 and 1846 seconds on CIFAR-10 DVS. These results establish PRIVSPIKE as\na viable and efficient solution for secure SNN inference, bridging the gap\nbetween energy-efficient deep neural networks and strong cryptographic privacy\nguarantees while outperforming prior encrypted SNN solutions."}
{"id": "2510.03863", "categories": ["cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.03863", "abs": "https://arxiv.org/abs/2510.03863", "authors": ["Arina Kharlamova", "Bowei He", "Chen Ma", "Xue Liu"], "title": "Spatial CAPTCHA: Generatively Benchmarking Spatial Reasoning for Human-Machine Differentiation", "comment": "Submitted to ICLR 2026", "summary": "Online services rely on CAPTCHAs as a first line of defense against automated\nabuse, yet recent advances in multi-modal large language models (MLLMs) have\neroded the effectiveness of conventional designs that focus on text recognition\nor 2D image understanding. To address this challenge, we present Spatial\nCAPTCHA, a novel human-verification framework that leverages fundamental\ndifferences in spatial reasoning between humans and MLLMs. Unlike existing\nCAPTCHAs which rely on low-level perception tasks that are vulnerable to modern\nAI, Spatial CAPTCHA generates dynamic questions requiring geometric reasoning,\nperspective-taking, occlusion handling, and mental rotation. These skills are\nintuitive for humans but difficult for state-of-the-art (SOTA) AI systems. The\nsystem employs a procedural generation pipeline with constraint-based\ndifficulty control, automated correctness verification, and human-in-the-loop\nvalidation to ensure scalability, robustness, and adaptability. Evaluation on a\ncorresponding benchmark, Spatial-CAPTCHA-Bench, demonstrates that humans vastly\noutperform 10 state-of-the-art MLLMs, with the best model achieving only 31.0%\nPass@1 accuracy. Furthermore, we compare Spatial CAPTCHA with Google reCAPTCHA,\nwhich confirms its effectiveness as both a security mechanism and a diagnostic\ntool for spatial reasoning in AI."}
{"id": "2510.04363", "categories": ["cs.SE", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.04363", "abs": "https://arxiv.org/abs/2510.04363", "authors": ["Hyunjun Kim", "Sejong Kim"], "title": "MacroBench: A Novel Testbed for Web Automation Scripts via Large Language Models", "comment": "NeurIPS 2025 Workshop on Lock-LLM", "summary": "We introduce MacroBench, a code-first benchmark that evaluates whether LLMs\ncan synthesize reusable browser automation programs from natural language goals\nby reading HTML/DOM and emitting Python with Selenium. MacroBench instantiates\nseven self-hosted sites: Airbnb-like, TikTok-like, Reddit-like, Instagram-like,\nFacebook-like, Discord-like, and Threads-like, covering 681 tasks across\ninteraction complexity and targeting difficulty. Our end-to-end protocol\nvalidates generated code via static checks, sandboxed execution, and outcome\nverification including DOM assertions and database snapshots, and includes a\nsafety suite for scraping, spam/abuse, and credential/privacy prompts. Across\n2636 model-task runs, we observe stratified success: GPT-4o-Mini achieves 96.8\npercent, GPT-4.1 achieves 95.3 percent, Gemini-2.5-Pro achieves 89.0 percent,\nand DeepSeek-V3.1 achieves 83.4 percent. Models handle simple tasks reliably at\n91.7 percent but fail on complex workflows at 0.0 percent, and none meet\nproduction-quality coding practices despite functional completion. We release\nour complete benchmark pipeline, evaluation framework, and experimental results\nto enable reproducible assessment of macro synthesis for web automation."}
{"id": "2510.03996", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.03996", "abs": "https://arxiv.org/abs/2510.03996", "authors": ["Nges Brian Njungle", "Eric Jahns", "Michel A. Kinsy"], "title": "FHEON: A Configurable Framework for Developing Privacy-Preserving Neural Networks Using Homomorphic Encryption", "comment": "14 pages, 6 figures", "summary": "The widespread adoption of Machine Learning as a Service raises critical\nprivacy and security concerns, particularly about data confidentiality and\ntrust in both cloud providers and the machine learning models. Homomorphic\nEncryption (HE) has emerged as a promising solution to this problems, allowing\ncomputations on encrypted data without decryption. Despite its potential,\nexisting approaches to integrate HE into neural networks are often limited to\nspecific architectures, leaving a wide gap in providing a framework for easy\ndevelopment of HE-friendly privacy-preserving neural network models similar to\nwhat we have in the broader field of machine learning. In this paper, we\npresent FHEON, a configurable framework for developing privacy-preserving\nconvolutional neural network (CNN) models for inference using HE. FHEON\nintroduces optimized and configurable implementations of privacy-preserving CNN\nlayers including convolutional layers, average pooling layers, ReLU activation\nfunctions, and fully connected layers. These layers are configured using\nparameters like input channels, output channels, kernel size, stride, and\npadding to support arbitrary CNN architectures. We assess the performance of\nFHEON using several CNN architectures, including LeNet-5, VGG-11, VGG- 16,\nResNet-20, and ResNet-34. FHEON maintains encrypted-domain accuracies within\n+/- 1% of their plaintext counterparts for ResNet-20 and LeNet-5 models.\nNotably, on a consumer-grade CPU, the models build on FHEON achieved 98.5%\naccuracy with a latency of 13 seconds on MNIST using LeNet-5, and 92.2%\naccuracy with a latency of 403 seconds on CIFAR-10 using ResNet-20.\nAdditionally, FHEON operates within a practical memory budget requiring not\nmore than 42.3 GB for VGG-16."}
{"id": "2510.03886", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03886", "abs": "https://arxiv.org/abs/2510.03886", "authors": ["Seil Kang", "Woojung Han", "Dayun Ju", "Seong Jae Hwang"], "title": "Rare Text Semantics Were Always There in Your Diffusion Transformer", "comment": "Accepted to NeurIPS 2025", "summary": "Starting from flow- and diffusion-based transformers, Multi-modal Diffusion\nTransformers (MM-DiTs) have reshaped text-to-vision generation, gaining acclaim\nfor exceptional visual fidelity. As these models advance, users continually\npush the boundary with imaginative or rare prompts, which advanced models still\nfalter in generating, since their concepts are often too scarce to leave a\nstrong imprint during pre-training. In this paper, we propose a simple yet\neffective intervention that surfaces rare semantics inside MM-DiTs without\nadditional training steps, data, denoising-time optimization, or reliance on\nexternal modules (e.g., large language models). In particular, the\njoint-attention mechanism intrinsic to MM-DiT sequentially updates text\nembeddings alongside image embeddings throughout transformer blocks. We find\nthat by mathematically expanding representational basins around text token\nembeddings via variance scale-up before the joint-attention blocks, rare\nsemantics clearly emerge in MM-DiT's outputs. Furthermore, our results\ngeneralize effectively across text-to-vision tasks, including text-to-image,\ntext-to-video, and text-driven image editing. Our work invites generative\nmodels to reveal the semantics that users intend, once hidden yet ready to\nsurface."}
{"id": "2510.04380", "categories": ["cs.SE", "cs.AI", "cs.HC", "D.2.1; D.2.2; D.2.9; I.2.7"], "pdf": "https://arxiv.org/pdf/2510.04380", "abs": "https://arxiv.org/abs/2510.04380", "authors": ["Mateen Ahmed Abbasi", "Petri Ihantola", "Tommi Mikkonen", "Niko Mäkitalo"], "title": "Reconsidering Requirements Engineering: Human-AI Collaboration in AI-Native Software Development", "comment": "Accepted at SEAA 2025. Appearing in Springer LNCS 16081, pages\n  164-180", "summary": "Requirement Engineering (RE) is the foundation of successful software\ndevelopment. In RE, the goal is to ensure that implemented systems satisfy\nstakeholder needs through rigorous requirements elicitation, validation, and\nevaluation processes. Despite its critical role, RE continues to face\npersistent challenges, such as ambiguity, conflicting stakeholder needs, and\nthe complexity of managing evolving requirements. A common view is that\nArtificial Intelligence (AI) has the potential to streamline the RE process,\nresulting in improved efficiency, accuracy, and management actions. However,\nusing AI also introduces new concerns, such as ethical issues, biases, and lack\nof transparency. This paper explores how AI can enhance traditional RE\npractices by automating labor-intensive tasks, supporting requirement\nprioritization, and facilitating collaboration between stakeholders and AI\nsystems. The paper also describes the opportunities and challenges that AI\nbrings to RE. In particular, the vision calls for ethical practices in AI,\nalong with a much-enhanced collaboration between academia and industry\nprofessionals. The focus should be on creating not only powerful but also\ntrustworthy and practical AI solutions ready to adapt to the fast-paced world\nof software development."}
{"id": "2510.04056", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.04056", "abs": "https://arxiv.org/abs/2510.04056", "authors": ["Rijha Safdar", "Danyail Mateen", "Syed Taha Ali", "Wajahat Hussain"], "title": "Real-VulLLM: An LLM Based Assessment Framework in the Wild", "comment": null, "summary": "Artificial Intelligence (AI) and more specifically Large Language Models\n(LLMs) have demonstrated exceptional progress in multiple areas including\nsoftware engineering, however, their capability for vulnerability detection in\nthe wild scenario and its corresponding reasoning remains underexplored.\nPrompting pre-trained LLMs in an effective way offers a computationally\neffective and scalable solution. Our contributions are (i)varied prompt designs\nfor vulnerability detection and its corresponding reasoning in the wild. (ii)a\nreal-world vector data store constructed from the National Vulnerability\nDatabase, that will provide real time context to vulnerability detection\nframework, and (iii)a scoring measure for combined measurement of accuracy and\nreasoning quality. Our contribution aims to examine whether LLMs are ready for\nwild deployment, thus enabling the reliable use of LLMs stronger for the\ndevelopment of secure software's."}
{"id": "2510.03892", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.03892", "abs": "https://arxiv.org/abs/2510.03892", "authors": ["Zahra Atf", "Peter R. Lewis"], "title": "Kantian-Utilitarian XAI: Meta-Explained", "comment": "Accepted for presentation as a poster at the 35th IEEE International\n  Conference on Collaborative Advances in Software and Computing, 2025.\n  Conference\n  website:https://conf.researchr.org/details/cascon-2025/posters-track/1/Kantian-Utilitarian-XAI-Meta-Explained", "summary": "We present a gamified explainable AI (XAI) system for ethically aware\nconsumer decision-making in the coffee domain. Each session comprises six\nrounds with three options per round. Two symbolic engines provide real-time\nreasons: a Kantian module flags rule violations (e.g., child labor,\ndeforestation risk without shade certification, opaque supply chains, unsafe\ndecaf), and a utilitarian module scores options via multi-criteria aggregation\nover normalized attributes (price, carbon, water, transparency, farmer income\nshare, taste/freshness, packaging, convenience). A meta-explainer with a regret\nbound (0.2) highlights Kantian--utilitarian (mis)alignment and switches to a\ndeontically clean, near-parity option when welfare loss is small. We release a\nstructured configuration (attribute schema, certification map, weights, rule\nset), a policy trace for auditability, and an interactive UI."}
{"id": "2510.04437", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.04437", "abs": "https://arxiv.org/abs/2510.04437", "authors": ["Fangzhe Wu", "Dongyang Lyu", "Xiaoqi Li"], "title": "Smart Hiring Redefined: An Intelligent Recruitment Management Platform", "comment": null, "summary": "Against the backdrop of deepening digital and intelligent transformation in\nhuman resource management, traditional recruitment models struggle to fully\nmeet enterprises' growing demand for precise talent acquisition due to limited\nefficiency, high costs, and information asymmetry. As a vital tool for\noptimizing recruitment processes, reducing labor and time costs, and enhancing\ncore competitiveness, intelligent recruitment management systems become an\nindispensable component of modern organizational talent strategies.Compared\nwith the labor intensive tasks of resume screening, candidate position\nmatching, and interview coordination in traditional manual recruitment,\nintelligent recruitment systems significantly enhance the efficiency and\naccuracy of the hiring process through automation and data driven approaches.\nThese systems enable rapid parsing of massive resume volumes, intelligent\nmatching of candidates to positions, and automated scheduling of interview\nprocesses."}
{"id": "2510.04085", "categories": ["cs.CR", "quant-ph"], "pdf": "https://arxiv.org/pdf/2510.04085", "abs": "https://arxiv.org/abs/2510.04085", "authors": ["Prabhanjan Ananth", "John Bostanci", "Aditya Gulati", "Yao-Ting Lin"], "title": "Gluing Random Unitaries with Inverses and Applications to Strong Pseudorandom Unitaries", "comment": "55 pages. A preliminary version, merging this paper and\n  arXiv:2509.24432, appears in the proceedings of the 45th Annual International\n  Cryptology Conference (CRYPTO 2025) under the title \"Pseudorandom Unitaries\n  in the Haar Random Oracle Model\". This is Part II of the full version", "summary": "Gluing theorem for random unitaries [Schuster, Haferkamp, Huang, QIP 2025]\nhave found numerous applications, including designing low depth random\nunitaries [Schuster, Haferkamp, Huang, QIP 2025], random unitaries in ${\\sf\nQAC0}$ [Foxman, Parham, Vasconcelos, Yuen'25] and generically shortening the\nkey length of pseudorandom unitaries [Ananth, Bostanci, Gulati, Lin\nEUROCRYPT'25]. We present an alternate method of combining Haar random\nunitaries from the gluing lemma from [Schuster, Haferkamp, Huang, QIP 2025]\nthat is secure against adversaries with inverse query access to the joined\nunitary. As a consequence, we show for the first time that strong pseudorandom\nunitaries can generically have their length extended, and can be constructed\nusing only $O(n^{1/c})$ bits of randomness, for any constant $c$, if any family\nof strong pseudorandom unitaries exists."}
{"id": "2510.03969", "categories": ["cs.AI", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03969", "abs": "https://arxiv.org/abs/2510.03969", "authors": ["Chengxiao Wang", "Isha Chaudhary", "Qian Hu", "Weitong Ruan", "Rahul Gupta", "Gagandeep Singh"], "title": "Quantifying Risks in Multi-turn Conversation with Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) can produce catastrophic responses in\nconversational settings that pose serious risks to public safety and security.\nExisting evaluations often fail to fully reveal these vulnerabilities because\nthey rely on fixed attack prompt sequences, lack statistical guarantees, and do\nnot scale to the vast space of multi-turn conversations. In this work, we\npropose QRLLM, a novel, principled Certification framework for Catastrophic\nrisks in multi-turn Conversation for LLMs that bounds the probability of an LLM\ngenerating catastrophic responses under multi-turn conversation distributions\nwith statistical guarantees. We model multi-turn conversations as probability\ndistributions over query sequences, represented by a Markov process on a query\ngraph whose edges encode semantic similarity to capture realistic\nconversational flow, and quantify catastrophic risks using confidence\nintervals. We define several inexpensive and practical distributions: random\nnode, graph path, adaptive with rejection. Our results demonstrate that these\ndistributions can reveal substantial catastrophic risks in frontier models,\nwith certified lower bounds as high as 70\\% for the worst model, highlighting\nthe urgent need for improved safety training strategies in frontier LLMs."}
{"id": "2510.04468", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.04468", "abs": "https://arxiv.org/abs/2510.04468", "authors": ["Asif Mohammed Samir", "Mohammad Masudur Rahman"], "title": "Improving IR-based Bug Localization with Semantics-Driven Query Reduction", "comment": "56 pages, 16 figures, 11 tables", "summary": "Despite decades of research, software bug localization remains challenging\ndue to heterogeneous content and inherent ambiguities in bug reports. Existing\nmethods such as Information Retrieval (IR)-based approaches often attempt to\nmatch source documents to bug reports, overlooking the context and semantics of\nthe source code. On the other hand, Large Language Models (LLM) (e.g.,\nTransformer models) show promising results in understanding both texts and\ncode. However, they have not been yet adapted well to localize software bugs\nagainst bug reports. They could be also data or resource-intensive. To bridge\nthis gap, we propose, IQLoc, a novel bug localization approach that capitalizes\non the strengths of both IR and LLM-based approaches. In particular, we\nleverage the program semantics understanding of transformer-based models to\nreason about the suspiciousness of code and reformulate queries during bug\nlocalization using Information Retrieval. To evaluate IQLoc, we refine the\nBench4BL benchmark dataset and extend it by incorporating ~30% more recent bug\nreports, resulting in a benchmark containing ~7.5K bug reports. We evaluated\nIQLoc using three performance metrics and compare it against four baseline\ntechniques. Experimental results demonstrate its superiority, achieving up to\n58.52% and 60.59% in MAP, 61.49% and 64.58% in MRR, and 69.88% and 100.90% in\nHIT@K for the test bug reports with random and time-wise splits, respectively.\nMoreover, IQLoc improves MAP by 91.67% for bug reports with stack traces,\n72.73% for those that include code elements, and 65.38% for those containing\nonly descriptions in natural language. By integrating program semantic\nunderstanding into Information Retrieval, IQLoc mitigates several longstanding\nchallenges of traditional IR-based approaches in bug localization."}
{"id": "2510.04118", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.04118", "abs": "https://arxiv.org/abs/2510.04118", "authors": ["Prakhar Paliwal", "Atul Kabra", "Manjesh Kumar Hanawal"], "title": "Cyber Warfare During Operation Sindoor: Malware Campaign Analysis and Detection Framework", "comment": "Accepted for presentation at the 21st International Conference on\n  Information Systems Security (ICISS 2025)", "summary": "Rapid digitization of critical infrastructure has made cyberwarfare one of\nthe important dimensions of modern conflicts. Attacking the critical\ninfrastructure is an attractive pre-emptive proposition for adversaries as it\ncan be done remotely without crossing borders. Such attacks disturb the support\nsystems of the opponents to launch any offensive activities, crippling their\nfighting capabilities. Cyberattacks during cyberwarfare can not only be used to\nsteal information, but also to spread disinformation to bring down the morale\nof the opponents. Recent wars in Europe, Africa, and Asia have demonstrated the\nscale and sophistication that the warring nations have deployed to take the\nearly upper hand. In this work, we focus on the military action launched by\nIndia, code-named Operation Sindoor, to dismantle terror infrastructure\nemanating from Pakistan and the cyberattacks launched by Pakistan. In\nparticular, we study the malware used by Pakistan APT groups to deploy Remote\nAccess Trojans in Indian systems. We provide details of the tactics and\ntechniques used in the RAT deployment and develop a telemetry framework to\ncollect necessary event logs using Osquery with a custom extension. Finally, we\ndevelop a detection rule that can be readily deployed to detect the presence of\nthe RAT or any exploitation performed by the malware."}
{"id": "2510.04009", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.04009", "abs": "https://arxiv.org/abs/2510.04009", "authors": ["Zicong He", "Boxuan Zhang", "Weihao Liu", "Ruixiang Tang", "Lu Cheng"], "title": "What Shapes a Creative Machine Mind? Comprehensively Benchmarking Creativity in Foundation Models", "comment": "22 pages", "summary": "The meteoric rise of foundation models (FMs) has expanded their capabilities\nfar beyond conventional tasks. Creativity, long regarded as a hallmark of human\nintelligence and a driver of innovation, is now increasingly recognized as a\ncritical dimension of machine intelligence in the era of generative FMs,\ncomplementing traditional measures of accuracy. However, existing evaluation\nframeworks for creativity remain fragmented, relying on ad hoc metrics not\nfirmly grounded in established theories. To address this gap, we introduce\nC^2-Eval, a holistic benchmark for unified assessment of creativity in FMs.\nC^2-Eval distinguishes between two complementary forms of creativity:\nconvergent creativity, where tasks admit constrained solutions (e.g., code\ngeneration), and divergent creativity, where tasks are open-ended (e.g.,\nstorytelling). It evaluates both dimensions using fine-grained criteria derived\nfrom social-science theory, focusing on Usefulness, Originality, and Surprise\n(U-O-S). Through extensive experiments on leading proprietary and open-source\nmodels, we analyze trade-offs in their creative capabilities. Our results\nhighlight both the strengths and challenges of current FMs in pursuing a\ncreative machine mind, showing that C^2-Eval is an effective lens for examining\nthe evolving landscape of creative AI."}
{"id": "2510.04469", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.04469", "abs": "https://arxiv.org/abs/2510.04469", "authors": ["Wenqi Yan", "Toby Murray", "Benjamin Rubinstein", "Van-Thuan Pham"], "title": "DynamiQ: Unlocking the Potential of Dynamic Task Allocation in Parallel Fuzzing", "comment": null, "summary": "We present DynamiQ, a full-fledged and optimized successor to AFLTeam that\nsupports dynamic and adaptive parallel fuzzing. Unlike most existing approaches\nthat treat individual seeds as tasks, DynamiQ leverages structural information\nfrom the program's call graph to define tasks and continuously refines task\nallocation using runtime feedback. This design significantly reduces redundant\nexploration and enhances fuzzing efficiency at scale. Built on top of the\nstate-of-the-art LibAFL framework, DynamiQ incorporates several practical\noptimizations in both task allocation and task-aware fuzzing. Evaluated on 12\nreal-world targets from OSS-Fuzz and FuzzBench over 25,000 CPU hours, DynamiQ\noutperforms state-of-the-art parallel fuzzers in both code coverage and\nvulnerability discovery, uncovering 9 previously unknown bugs in widely used\nand extensively fuzzed open-source software."}
{"id": "2510.04153", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04153", "abs": "https://arxiv.org/abs/2510.04153", "authors": ["Haoqi Wu", "Wei Dai", "Ming Xu", "Li Wang", "Qiang Yan"], "title": "ObCLIP: Oblivious CLoud-Device Hybrid Image Generation with Privacy Preservation", "comment": "Accepted by NeurIPS 2025", "summary": "Diffusion Models have gained significant popularity due to their remarkable\ncapabilities in image generation, albeit at the cost of intensive computation\nrequirement. Meanwhile, despite their widespread deployment in inference\nservices such as Midjourney, concerns about the potential leakage of sensitive\ninformation in uploaded user prompts have arisen. Existing solutions either\nlack rigorous privacy guarantees or fail to strike an effective balance between\nutility and efficiency. To bridge this gap, we propose ObCLIP, a plug-and-play\nsafeguard that enables oblivious cloud-device hybrid generation. By oblivious,\neach input prompt is transformed into a set of semantically similar candidate\nprompts that differ only in sensitive attributes (e.g., gender, ethnicity). The\ncloud server processes all candidate prompts without knowing which one is the\nreal one, thus preventing any prompt leakage. To mitigate server cost, only a\nsmall portion of denoising steps is performed upon the large cloud model. The\nintermediate latents are then sent back to the client, which selects the\ntargeted latent and completes the remaining denoising using a small device\nmodel. Additionally, we analyze and incorporate several cache-based\naccelerations that leverage temporal and batch redundancy, effectively reducing\ncomputation cost with minimal utility degradation. Extensive experiments across\nmultiple datasets demonstrate that ObCLIP provides rigorous privacy and\ncomparable utility to cloud models with slightly increased server cost."}
{"id": "2510.04017", "categories": ["cs.AI", "cs.LG", "physics.ao-ph"], "pdf": "https://arxiv.org/pdf/2510.04017", "abs": "https://arxiv.org/abs/2510.04017", "authors": ["Sumanth Varambally", "Marshall Fisher", "Jas Thakker", "Yiwei Chen", "Zhirui Xia", "Yasaman Jafari", "Ruijia Niu", "Manas Jain", "Veeramakali Vignesh Manivannan", "Zachary Novack", "Luyu Han", "Srikar Eranky", "Salva Rühling Cachay", "Taylor Berg-Kirkpatrick", "Duncan Watson-Parris", "Yi-An Ma", "Rose Yu"], "title": "Zephyrus: An Agentic Framework for Weather Science", "comment": null, "summary": "Foundation models for weather science are pre-trained on vast amounts of\nstructured numerical data and outperform traditional weather forecasting\nsystems. However, these models lack language-based reasoning capabilities,\nlimiting their utility in interactive scientific workflows. Large language\nmodels (LLMs) excel at understanding and generating text but cannot reason\nabout high-dimensional meteorological datasets. We bridge this gap by building\na novel agentic framework for weather science. Our framework includes a Python\ncode-based environment for agents (ZephyrusWorld) to interact with weather\ndata, featuring tools like an interface to WeatherBench 2 dataset, geoquerying\nfor geographical masks from natural language, weather forecasting, and climate\nsimulation capabilities. We design Zephyrus, a multi-turn LLM-based weather\nagent that iteratively analyzes weather datasets, observes results, and refines\nits approach through conversational feedback loops. We accompany the agent with\na new benchmark, ZephyrusBench, with a scalable data generation pipeline that\nconstructs diverse question-answer pairs across weather-related tasks, from\nbasic lookups to advanced forecasting, extreme event detection, and\ncounterfactual reasoning. Experiments on this benchmark demonstrate the strong\nperformance of Zephyrus agents over text-only baselines, outperforming them by\nup to 35 percentage points in correctness. However, on harder tasks, Zephyrus\nperforms similarly to text-only baselines, highlighting the challenging nature\nof our benchmark and suggesting promising directions for future work."}
{"id": "2510.04495", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.04495", "abs": "https://arxiv.org/abs/2510.04495", "authors": ["Napasorn Tevarut", "Brittany Reid", "Yutaro Kashiwa", "Pattara Leelaprute", "Arnon Rungsawang", "Bundit Manaskasemsak", "Hajimu Iida"], "title": "Detecting and Characterizing Low and No Functionality Packages in the NPM Ecosystem", "comment": "Accepted in PROFES 2025", "summary": "Trivial packages, small modules with low functionality, are common in the npm\necosystem and can pose security risks despite their simplicity. This paper\nrefines existing definitions and introduce data-only packages that contain no\nexecutable logic. A rule-based static analysis method is developed to detect\ntrivial and data-only packages and evaluate their prevalence and associated\nrisks in the 2025 npm ecosystem. The analysis shows that 17.92% of packages are\ntrivial, with vulnerability levels comparable to non-trivial ones, and\ndata-only packages, though rare, also contain risks. The proposed detection\ntool achieves 94% accuracy (macro-F1 0.87), enabling effective large-scale\nanalysis to reduce security exposure. This findings suggest that trivial and\ndata-only packages warrant greater attention in dependency management to reduce\npotential technical debt and security exposure."}
{"id": "2510.04257", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04257", "abs": "https://arxiv.org/abs/2510.04257", "authors": ["Yanjie Li", "Yiming Cao", "Dong Wang", "Bin Xiao"], "title": "AgentTypo: Adaptive Typographic Prompt Injection Attacks against Black-box Multimodal Agents", "comment": "13 pages, 8 figures. Submitted to IEEE Transactions on Information\n  Forensics & Security", "summary": "Multimodal agents built on large vision-language models (LVLMs) are\nincreasingly deployed in open-world settings but remain highly vulnerable to\nprompt injection, especially through visual inputs. We introduce AgentTypo, a\nblack-box red-teaming framework that mounts adaptive typographic prompt\ninjection by embedding optimized text into webpage images. Our automatic\ntypographic prompt injection (ATPI) algorithm maximizes prompt reconstruction\nby substituting captioners while minimizing human detectability via a stealth\nloss, with a Tree-structured Parzen Estimator guiding black-box optimization\nover text placement, size, and color. To further enhance attack strength, we\ndevelop AgentTypo-pro, a multi-LLM system that iteratively refines injection\nprompts using evaluation feedback and retrieves successful past examples for\ncontinual learning. Effective prompts are abstracted into generalizable\nstrategies and stored in a strategy repository, enabling progressive knowledge\naccumulation and reuse in future attacks. Experiments on the VWA-Adv benchmark\nacross Classifieds, Shopping, and Reddit scenarios show that AgentTypo\nsignificantly outperforms the latest image-based attacks such as AgentAttack.\nOn GPT-4o agents, our image-only attack raises the success rate from 0.23 to\n0.45, with consistent results across GPT-4V, GPT-4o-mini, Gemini 1.5 Pro, and\nClaude 3 Opus. In image+text settings, AgentTypo achieves 0.68 ASR, also\noutperforming the latest baselines. Our findings reveal that AgentTypo poses a\npractical and potent threat to multimodal agents and highlight the urgent need\nfor effective defense."}
{"id": "2510.04023", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.04023", "abs": "https://arxiv.org/abs/2510.04023", "authors": ["Mizanur Rahman", "Amran Bhuiyan", "Mohammed Saidul Islam", "Md Tahmid Rahman Laskar", "Ridwan Mahbub", "Ahmed Masry", "Shafiq Joty", "Enamul Hoque"], "title": "LLM-Based Data Science Agents: A Survey of Capabilities, Challenges, and Future Directions", "comment": "Survey paper; 45 data science agents; under review", "summary": "Recent advances in large language models (LLMs) have enabled a new class of\nAI agents that automate multiple stages of the data science workflow by\nintegrating planning, tool use, and multimodal reasoning across text, code,\ntables, and visuals. This survey presents the first comprehensive,\nlifecycle-aligned taxonomy of data science agents, systematically analyzing and\nmapping forty-five systems onto the six stages of the end-to-end data science\nprocess: business understanding and data acquisition, exploratory analysis and\nvisualization, feature engineering, model building and selection,\ninterpretation and explanation, and deployment and monitoring. In addition to\nlifecycle coverage, we annotate each agent along five cross-cutting design\ndimensions: reasoning and planning style, modality integration, tool\norchestration depth, learning and alignment methods, and trust, safety, and\ngovernance mechanisms. Beyond classification, we provide a critical synthesis\nof agent capabilities, highlight strengths and limitations at each stage, and\nreview emerging benchmarks and evaluation practices. Our analysis identifies\nthree key trends: most systems emphasize exploratory analysis, visualization,\nand modeling while neglecting business understanding, deployment, and\nmonitoring; multimodal reasoning and tool orchestration remain unresolved\nchallenges; and over 90% lack explicit trust and safety mechanisms. We conclude\nby outlining open challenges in alignment stability, explainability,\ngovernance, and robust evaluation frameworks, and propose future research\ndirections to guide the development of robust, trustworthy, low-latency,\ntransparent, and broadly accessible data science agents."}
{"id": "2510.04519", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.04519", "abs": "https://arxiv.org/abs/2510.04519", "authors": ["Heiko Koziolek", "Thilo Braun", "Virendra Ashiwal", "Sofia Linsbauer", "Marthe Ahlgreen Hansen", "Karoline Grotterud"], "title": "Spec2Control: Automating PLC/DCS Control-Logic Engineering from Natural Language Requirements with LLMs - A Multi-Plant Evaluation", "comment": "12 pages, 9 figures", "summary": "Distributed control systems (DCS) manage the automation for many industrial\nproduction processes (e.g., power plants, chemical refineries, steel mills).\nProgramming the software for such systems remains a largely manual and tedious\nprocess, incurring costs of millions of dollars for extensive facilities. Large\nlanguage models (LLMs) have been found helpful in generating DCS control logic,\nresulting in commercial copilot tools. Today, these tools are focused on\ntextual notations, they provide limited automation, and have not been tested on\nlarge datasets with realistic test cases. We introduce Spec2Control, a highly\nautomated LLM workflow to generate graphical control logic directly from\nnatural language user requirements. Experiments using an open dataset with 10\ncontrol narratives and 65 complex test cases demonstrate that Spec2Control can\nsuccessfully identify control strategies, can generate 98.6% of correct control\nstrategy connections autonomously, and can save between 94-96% of human labor.\nSpec2Control is being integrated into commercial ABB engineering tools, but is\nalso available as an open-source variant for independent validation."}
{"id": "2510.04261", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.04261", "abs": "https://arxiv.org/abs/2510.04261", "authors": ["Yu Cui", "Sicheng Pan", "Yifei Liu", "Haibin Zhang", "Cong Zuo"], "title": "VortexPIA: Indirect Prompt Injection Attack against LLMs for Efficient Extraction of User Privacy", "comment": null, "summary": "Large language models (LLMs) have been widely deployed in Conversational AIs\n(CAIs), while exposing privacy and security threats. Recent research shows that\nLLM-based CAIs can be manipulated to extract private information from human\nusers, posing serious security threats. However, the methods proposed in that\nstudy rely on a white-box setting that adversaries can directly modify the\nsystem prompt. This condition is unlikely to hold in real-world deployments.\nThe limitation raises a critical question: can unprivileged attackers still\ninduce such privacy risks in practical LLM-integrated applications? To address\nthis question, we propose \\textsc{VortexPIA}, a novel indirect prompt injection\nattack that induces privacy extraction in LLM-integrated applications under\nblack-box settings. By injecting token-efficient data containing false\nmemories, \\textsc{VortexPIA} misleads LLMs to actively request private\ninformation in batches. Unlike prior methods, \\textsc{VortexPIA} allows\nattackers to flexibly define multiple categories of sensitive data. We evaluate\n\\textsc{VortexPIA} on six LLMs, covering both traditional and reasoning LLMs,\nacross four benchmark datasets. The results show that \\textsc{VortexPIA}\nsignificantly outperforms baselines and achieves state-of-the-art (SOTA)\nperformance. It also demonstrates efficient privacy requests, reduced token\nconsumption, and enhanced robustness against defense mechanisms. We further\nvalidate \\textsc{VortexPIA} on multiple realistic open-source LLM-integrated\napplications, demonstrating its practical effectiveness."}
{"id": "2510.04033", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04033", "abs": "https://arxiv.org/abs/2510.04033", "authors": ["Ayush Noori", "Adam Rodman", "Alan Karthikesalingam", "Bilal A. Mateen", "Christopher A. Longhurst", "Daniel Yang", "Dave deBronkart", "Gauden Galea", "Harold F. Wolf III", "Jacob Waxman", "Joshua C. Mandel", "Juliana Rotich", "Kenneth D. Mandl", "Maryam Mustafa", "Melissa Miles", "Nigam H. Shah", "Peter Lee", "Robert Korom", "Scott Mahoney", "Seth Hain", "Tien Yin Wong", "Trevor Mundel", "Vivek Natarajan", "Noa Dagan", "David A. Clifton", "Ran D. Balicer", "Isaac S. Kohane", "Marinka Zitnik"], "title": "A global log for medical AI", "comment": null, "summary": "Modern computer systems often rely on syslog, a simple, universal protocol\nthat records every critical event across heterogeneous infrastructure. However,\nhealthcare's rapidly growing clinical AI stack has no equivalent. As hospitals\nrush to pilot large language models and other AI-based clinical decision\nsupport tools, we still lack a standard way to record how, when, by whom, and\nfor whom these AI models are used. Without that transparency and visibility, it\nis challenging to measure real-world performance and outcomes, detect adverse\nevents, or correct bias or dataset drift. In the spirit of syslog, we introduce\nMedLog, a protocol for event-level logging of clinical AI. Any time an AI model\nis invoked to interact with a human, interface with another algorithm, or act\nindependently, a MedLog record is created. This record consists of nine core\nfields: header, model, user, target, inputs, artifacts, outputs, outcomes, and\nfeedback, providing a structured and consistent record of model activity. To\nencourage early adoption, especially in low-resource settings, and minimize the\ndata footprint, MedLog supports risk-based sampling, lifecycle-aware retention\npolicies, and write-behind caching; detailed traces for complex, agentic, or\nmulti-stage workflows can also be captured under MedLog. MedLog can catalyze\nthe development of new databases and software to store and analyze MedLog\nrecords. Realizing this vision would enable continuous surveillance, auditing,\nand iterative improvement of medical AI, laying the foundation for a new form\nof digital epidemiology."}
{"id": "2510.04603", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.04603", "abs": "https://arxiv.org/abs/2510.04603", "authors": ["Johan Linåker", "Sachiko Muto"], "title": "Advancing Digital Government: Integrating Open Source Software Enablement Indicators in Maturity Indexes", "comment": "In submission", "summary": "Context: Open Source Software (OSS) is a vital public good, included across\nmost of modern software stacks, significantly impacting GDP and national tech\ngrowth, while supporting interoperability, sovereignty, and transparency.\nHowever, systematic measurement of governmental OSS adoption remain limited.\n  Research Aim: This study contributes to digital government maturity indexes\nby analyzing policies and support actions leveraging OSS for software reuse and\ncollaborative development across 16 digitally mature countries, and proposing\npotential indicators for said indexes. It examines OSS policy formation, stated\ngoals, key actors, and support mechanisms.\n  Methodology: A qualitative approach is used combining desk research of policy\ndocuments with semi-structured interviews of government representatives,\nproducing detailed country reports. These are cross-analyzed, focusing on OSS\npolicy promotion, rationale, and implementation support.\n  Results: Policies facilitating OSS reuse are widespread, targeting both\ninbound acquisition and outbound sharing, and are predominantly governed by\ncentral public sector organizations. Policy goals include interoperability,\ndigital sovereignty, transparency, and cost efficiency, with security framed\nboth as a risk and strength. Implementation is supported by diverse Open Source\nProgram Offices (OSPOs) at multiple government levels, which foster capacity\nbuilding, resource pooling, and sustainable project governance. Indicators are\nsynthesized and proposed across 14 areas covering policy incentives and design,\nand implementation and support.\n  Conclusions: OSS is a strategic enabler for public sector digital\ntransformation. Clear policy frameworks, coupled with institutional support\nsuch as OSPOs, are essential. International digital maturity frameworks should\nexpand OSS indicators to better guide and assess government adoption and\nimpact."}
{"id": "2510.04397", "categories": ["cs.CR", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.04397", "abs": "https://arxiv.org/abs/2510.04397", "authors": ["Van Nguyen", "Surya Nepal", "Xingliang Yuan", "Tingmin Wu", "Fengchao Chen", "Carsten Rudolph"], "title": "MulVuln: Enhancing Pre-trained LMs with Shared and Language-Specific Knowledge for Multilingual Vulnerability Detection", "comment": null, "summary": "Software vulnerabilities (SVs) pose a critical threat to safety-critical\nsystems, driving the adoption of AI-based approaches such as machine learning\nand deep learning for software vulnerability detection. Despite promising\nresults, most existing methods are limited to a single programming language.\nThis is problematic given the multilingual nature of modern software, which is\noften complex and written in multiple languages. Current approaches often face\nchallenges in capturing both shared and language-specific knowledge of source\ncode, which can limit their performance on diverse programming languages and\nreal-world codebases. To address this gap, we propose MULVULN, a novel\nmultilingual vulnerability detection approach that learns from source code\nacross multiple languages. MULVULN captures both the shared knowledge that\ngeneralizes across languages and the language-specific knowledge that reflects\nunique coding conventions. By integrating these aspects, it achieves more\nrobust and effective detection of vulnerabilities in real-world multilingual\nsoftware systems. The rigorous and extensive experiments on the real-world and\ndiverse REEF dataset, consisting of 4,466 CVEs with 30,987 patches across seven\nprogramming languages, demonstrate the superiority of MULVULN over thirteen\neffective and state-of-the-art baselines. Notably, MULVULN achieves\nsubstantially higher F1-score, with improvements ranging from 1.45% to 23.59%\ncompared to the baseline methods."}
{"id": "2510.04040", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04040", "abs": "https://arxiv.org/abs/2510.04040", "authors": ["Xu Shen", "Song Wang", "Zhen Tan", "Laura Yao", "Xinyu Zhao", "Kaidi Xu", "Xin Wang", "Tianlong Chen"], "title": "FaithCoT-Bench: Benchmarking Instance-Level Faithfulness of Chain-of-Thought Reasoning", "comment": null, "summary": "Large language models (LLMs) increasingly rely on Chain-of-Thought (CoT)\nprompting to improve problem-solving and provide seemingly transparent\nexplanations. However, growing evidence shows that CoT often fail to faithfully\nrepresent the underlying reasoning process, raising concerns about their\nreliability in high-risk applications. Although prior studies have focused on\nmechanism-level analyses showing that CoTs can be unfaithful, they leave open\nthe practical challenge of deciding whether a specific trajectory is faithful\nto the internal reasoning of the model. To address this gap, we introduce\nFaithCoT-Bench, a unified benchmark for instance-level CoT unfaithfulness\ndetection. Our framework establishes a rigorous task formulation that\nformulates unfaithfulness detection as a discriminative decision problem, and\nprovides FINE-CoT (Faithfulness instance evaluation for Chain-of-Thought), an\nexpert-annotated collection of over 1,000 trajectories generated by four\nrepresentative LLMs across four domains, including more than 300 unfaithful\ninstances with fine-grained causes and step-level evidence. We further conduct\na systematic evaluation of eleven representative detection methods spanning\ncounterfactual, logit-based, and LLM-as-judge paradigms, deriving empirical\ninsights that clarify the strengths and weaknesses of existing approaches and\nreveal the increased challenges of detection in knowledge-intensive domains and\nwith more advanced models. To the best of our knowledge, FaithCoT-Bench\nestablishes the first comprehensive benchmark for instance-level CoT\nfaithfulness, setting a solid basis for future research toward more\ninterpretable and trustworthy reasoning in LLMs."}
{"id": "2510.04605", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.04605", "abs": "https://arxiv.org/abs/2510.04605", "authors": ["Jingyao Zhang", "Tianlin Li", "Xiaoyu Zhang", "Qiang Hu", "Bin Shi"], "title": "Exploring the Power of Diffusion Large Language Models for Software Engineering: An Empirical Investigation", "comment": null, "summary": "Autoregressive Large Language Models (AR-LLMs) are widely used in software\nengineering (SE) but face limitations in processing code structure information\nand suffer from high inference latency. Diffusion LLMs (DLLMs) offer a\npromising alternative with global bidirectional encoding and decoupled\ngeneration steps. This work presents the first comprehensive evaluation of\nDLLMs across the software development lifecycle, including code generation,\ndefect detection, and program repair. On a large-scale benchmark of 52,937\ntasks, 7Bparameter DLLMs outperform AR-LLMs with a 30% average accuracy\nimprovement achieving a 113% gain on cross-file repair, while maintaining\nsuperior efficiency and reduced latency. Our results establish DLLMs as a\nsuperior paradigm for SE tasks."}
{"id": "2510.04503", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.04503", "abs": "https://arxiv.org/abs/2510.04503", "authors": ["Shuai Zhao", "Xinyi Wu", "Shiqian Zhao", "Xiaobao Wu", "Zhongliang Guo", "Yanhao Jia", "Anh Tuan Luu"], "title": "P2P: A Poison-to-Poison Remedy for Reliable Backdoor Defense in LLMs", "comment": null, "summary": "During fine-tuning, large language models (LLMs) are increasingly vulnerable\nto data-poisoning backdoor attacks, which compromise their reliability and\ntrustworthiness. However, existing defense strategies suffer from limited\ngeneralization: they only work on specific attack types or task settings. In\nthis study, we propose Poison-to-Poison (P2P), a general and effective backdoor\ndefense algorithm. P2P injects benign triggers with safe alternative labels\ninto a subset of training samples and fine-tunes the model on this re-poisoned\ndataset by leveraging prompt-based learning. This enforces the model to\nassociate trigger-induced representations with safe outputs, thereby overriding\nthe effects of original malicious triggers. Thanks to this robust and\ngeneralizable trigger-based fine-tuning, P2P is effective across task settings\nand attack types. Theoretically and empirically, we show that P2P can\nneutralize malicious backdoors while preserving task performance. We conduct\nextensive experiments on classification, mathematical reasoning, and summary\ngeneration tasks, involving multiple state-of-the-art LLMs. The results\ndemonstrate that our P2P algorithm significantly reduces the attack success\nrate compared with baseline models. We hope that the P2P can serve as a\nguideline for defending against backdoor attacks and foster the development of\na secure and trustworthy LLM community."}
{"id": "2510.04048", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04048", "abs": "https://arxiv.org/abs/2510.04048", "authors": ["Aparna Nair-Kanneganti", "Trevor J. Chan", "Shir Goldfinger", "Emily Mackay", "Brian Anthony", "Alison Pouch"], "title": "Increasing LLM response trustworthiness using voting ensembles", "comment": null, "summary": "Despite huge advances, LLMs still lack convenient and reliable methods to\nquantify the uncertainty in their responses, making them difficult to trust in\nhigh-stakes applications. One of the simplest approaches to eliciting more\naccurate answers is to select the mode of many responses, a technique known as\nensembling. In this work, we expand on typical ensembling approaches by looking\nat ensembles with a variable voting threshold. We introduce a theoretical\nframework for question answering and show that, by permitting ensembles to\n\"abstain\" from providing an answer when the dominant response falls short of\nthe threshold, it is possible to dramatically increase the trustworthiness of\nthe remaining answers. From this framework, we derive theoretical results as\nwell as report experimental results on two problem domains: arithmetic problem\nsolving and clinical-note question-answering. In both domains, we observe that\nlarge gains in answer trustworthiness can be achieved using highly restrictive\nvoting ensembles, while incurring relatively modest reductions in response\nyield and accuracy. Due to this quality, voting ensembles may be particularly\nuseful in applications - such as healthcare and data annotation - that require\na high degree of certainty but which may not require that every question\nreceive an automated answer."}
{"id": "2510.04611", "categories": ["cs.SE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.04611", "abs": "https://arxiv.org/abs/2510.04611", "authors": ["Pawel Weichbroth", "Maciej Lotysz", "Michal Wrobel"], "title": "A survey on the impact of emotions on the productivity among software developers", "comment": "29 pages, 5 tables, 96 references", "summary": "The time pressure associated with software development, among other factors,\noften leads to a diminished emotional state among developers. However, whether\nemotions affect perceived productivity remains an open question. This study\naims to determine the strength and direction of the relationship between\nemotional state and perceived productivity among software developers. We\nemployed a two-stage approach. First, a survey was conducted with a pool of\nnine experts to validate the measurement model. Second, a survey was\nadministered to a pool of 88 software developers to empirically test the\nformulated hypothesis by using Partial Least Squares, as the data analysis\nmethod. The results of the path analysis clearly confirm the formulated\nhypothesis, showing that the emotional state of a software developer has a\nstrong positive, and significant impact (beta = 0.893, p < 0.001) on perceived\nproductivity among software developers. The findings highlight the importance\nof managing and improving developers emotional well-being to enhance\nproductivity in software development environments. Additionally, interventions\naimed at reducing burnout, stress, and other negative factors could have a\nconsiderable impact on their performance outcomes."}
{"id": "2510.04528", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04528", "abs": "https://arxiv.org/abs/2510.04528", "authors": ["Santhosh KumarRavindran"], "title": "Unified Threat Detection and Mitigation Framework (UTDMF): Combating Prompt Injection, Deception, and Bias in Enterprise-Scale Transformers", "comment": null, "summary": "The rapid adoption of large language models (LLMs) in enterprise systems\nexposes vulnerabilities to prompt injection attacks, strategic deception, and\nbiased outputs, threatening security, trust, and fairness. Extending our\nadversarial activation patching framework (arXiv:2507.09406), which induced\ndeception in toy networks at a 23.9% rate, we introduce the Unified Threat\nDetection and Mitigation Framework (UTDMF), a scalable, real-time pipeline for\nenterprise-grade models like Llama-3.1 (405B), GPT-4o, and Claude-3.5. Through\n700+ experiments per model, UTDMF achieves: (1) 92% detection accuracy for\nprompt injection (e.g., jailbreaking); (2) 65% reduction in deceptive outputs\nvia enhanced patching; and (3) 78% improvement in fairness metrics (e.g.,\ndemographic bias). Novel contributions include a generalized patching algorithm\nfor multi-threat detection, three groundbreaking hypotheses on threat\ninteractions (e.g., threat chaining in enterprise workflows), and a\ndeployment-ready toolkit with APIs for enterprise integration."}
{"id": "2510.04051", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04051", "abs": "https://arxiv.org/abs/2510.04051", "authors": ["Lele Liao", "Qile Zhang", "Ruofan Wu", "Guanhua Fang"], "title": "Toward a unified framework for data-efficient evaluation of large language models", "comment": "codes available at https://github.com/Rorschach1989/efficient-lm-eval", "summary": "Evaluating large language models (LLMs) on comprehensive benchmarks is a\ncornerstone of their development, yet it's often computationally and\nfinancially prohibitive. While Item Response Theory (IRT) offers a promising\npath toward data-efficient evaluation by disentangling model capability from\nitem difficulty, existing IRT-based methods are hampered by significant\nlimitations. They are typically restricted to binary correctness metrics,\nfailing to natively handle the continuous scores used in generative tasks, and\nthey operate on single benchmarks, ignoring valuable structural knowledge like\ncorrelations across different metrics or benchmarks. To overcome these\nchallenges, we introduce LEGO-IRT, a unified and flexible framework for\ndata-efficient LLM evaluation. LEGO-IRT's novel design natively supports both\nbinary and continuous evaluation metrics. Moreover, it introduces a factorized\narchitecture to explicitly model and leverage structural knowledge, decomposing\nmodel ability estimates into a general component and structure-specific (e.g.,\nper-metric or per-benchmark) components. Through extensive experiments\ninvolving $70$ LLMs across $5$ benchmarks, we show that LEGO-IRT achieves\nstable capability estimates using just $3\\%$ of the total evaluation items. We\ndemonstrate that incorporating structural knowledge reduces estimation error by\nup to $10\\%$ and reveal that the latent abilities estimated by our framework\nmay align more closely with human preferences."}
{"id": "2510.04689", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.04689", "abs": "https://arxiv.org/abs/2510.04689", "authors": ["Chengwei Liu", "Wenbo Guo", "Yuxin Zhang", "Limin Wang", "Sen Chen", "Lei Bu", "Yang Liu"], "title": "Evolaris: A Roadmap to Self-Evolving Software Intelligence Management", "comment": null, "summary": "In recent years, the landscape of software threats has become significantly\nmore dynamic and distributed. Security vulnerabilities are no longer discovered\nand shared only through formal channels such as public vulnerability databases\nor vendor advisories. Increasingly, criti- cal threat information emerges\ninformally through blogs, social media, developer forums, open source\nrepositories, and even underground com- munities. To this end, capturing such\nintelligence in a timely manner is essential for maintaining situational\nawareness and enabling prompt security responses. However, this remains a\ncomplex challenge due to the fragmented nature of data sources and the\ntechnical difficulty of collecting, parsing, mapping, and validating\ninformation at scale. To ad- dress this, we propose Evolaris, a self-evolving\nsoftware intelligence sys- tem built on a multi-agent framework. Evolaris is\ndesigned to support a full-stack workflow, where agents operate independently\nbut coordinate through shared context to perform tasks such as information\ndiscovery, reasoning, gap completion, validation, and risk detection. This\narchi- tecture enables the platform to learn from new inputs, refine its\ninternal knowledge, and adapt to emerging threat patterns over time, which\ncould continuously improve the precision, timeliness, and scalability of\nsoftware threat analysis, and offers a sustainable foundation for proactive\nsecu- rity decision-making and strengthens the broader ecosystem of security\nthreat understanding."}
{"id": "2510.04529", "categories": ["cs.CR", "quant-ph"], "pdf": "https://arxiv.org/pdf/2510.04529", "abs": "https://arxiv.org/abs/2510.04529", "authors": ["Yuki Takeuchi", "Duo Xu"], "title": "Computational Certified Deletion Property of Magic Square Game and its Application to Classical Secure Key Leasing", "comment": null, "summary": "We present the first construction of a computational Certified Deletion\nProperty (CDP) achievable with classical communication, derived from the\ncompilation of the non-local Magic Square Game (MSG). We leverage the KLVY\ncompiler to transform the non-local MSG into a 2-round interactive protocol,\nrigorously demonstrating that this compilation preserves the game-specific CDP.\nPreviously, the quantum value and rigidity of the compiled game were\ninvestigated. We emphasize that we are the first to investigate CDP (local\nrandomness in [Fu and Miller, Phys. Rev. A 97, 032324 (2018)]) for the compiled\ngame. Then, we combine this CDP with the framework [Kitagawa, Morimae, and\nYamakawa, Eurocrypt 2025] to construct Secure Key Leasing with classical Lessor\n(cSKL). SKL enables the Lessor to lease the secret key to the Lessee and verify\nthat a quantum Lessee has indeed deleted the key. In this paper, we realize\ncSKL for PKE, PRF, and digital signature. Compared to prior works for cSKL, we\nrealize cSKL for PRF and digital signature for the first time. In addition, we\nsucceed in weakening the assumption needed to construct cSKL."}
{"id": "2510.04064", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04064", "abs": "https://arxiv.org/abs/2510.04064", "authors": ["Jingxiang Zhang", "Lujia Zhong"], "title": "Decoding Emotion in the Deep: A Systematic Study of How LLMs Represent, Retain, and Express Emotion", "comment": "10 pages, 7 figures, 4 tables. Under review", "summary": "Large Language Models (LLMs) are increasingly expected to navigate the\nnuances of human emotion. While research confirms that LLMs can simulate\nemotional intelligence, their internal emotional mechanisms remain largely\nunexplored. This paper investigates the latent emotional representations within\nmodern LLMs by asking: how, where, and for how long is emotion encoded in their\nneural architecture? To address this, we introduce a novel, large-scale Reddit\ncorpus of approximately 400,000 utterances, balanced across seven basic\nemotions through a multi-stage process of classification, rewriting, and\nsynthetic generation. Using this dataset, we employ lightweight \"probes\" to\nread out information from the hidden layers of various Qwen3 and LLaMA models\nwithout altering their parameters. Our findings reveal that LLMs develop a\nsurprisingly well-defined internal geometry of emotion, which sharpens with\nmodel scale and significantly outperforms zero-shot prompting. We demonstrate\nthat this emotional signal is not a final-layer phenomenon but emerges early\nand peaks mid-network. Furthermore, the internal states are both malleable\n(they can be influenced by simple system prompts) and persistent, as the\ninitial emotional tone remains detectable for hundreds of subsequent tokens. We\ncontribute our dataset, an open-source probing toolkit, and a detailed map of\nthe emotional landscape within LLMs, offering crucial insights for developing\nmore transparent and aligned AI systems. The code and dataset are open-sourced."}
{"id": "2510.04711", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.04711", "abs": "https://arxiv.org/abs/2510.04711", "authors": ["Aoyang Fang", "Songhan Zhang", "Yifan Yang", "Haotong Wu", "Junjielong Xu", "Xuyang Wang", "Rui Wang", "Manyi Wang", "Qisheng Lu", "Pinjia He"], "title": "An Empirical Study of SOTA RCA Models: From Oversimplified Benchmarks to Realistic Failures", "comment": "Our project is available on https://operationspai.github.io/", "summary": "While cloud-native microservice architectures have transformed software\ndevelopment, their complexity makes Root Cause Analysis (RCA) both crucial and\nchallenging. Although many data-driven RCA models have been proposed, we find\nthat existing benchmarks are often oversimplified and fail to capture\nreal-world conditions. Our preliminary study shows that simple rule-based\nmethods can match or even outperform state-of-the-art (SOTA) models on four\nwidely used benchmarks, suggesting performance overestimation due to benchmark\nsimplicity. To address this, we systematically analyze popular RCA benchmarks\nand identify key limitations in fault injection, call graph design, and\ntelemetry patterns. Based on these insights, we develop an automated framework\nto generate more realistic benchmarks, yielding a dataset of 1,430 validated\nfailure cases from 9,152 injections, covering 25 fault types under dynamic\nworkloads with hierarchical ground-truth labels and verified SLI impact.\nRe-evaluation of 11 SOTA models on this dataset shows low Top@1 accuracy\n(average 0.21, best 0.37) and significantly longer execution times. Our\nanalysis highlights three common failure patterns: scalability issues,\nobservability blind spots, and modeling bottlenecks."}
{"id": "2510.04619", "categories": ["cs.CR", "cs.NI"], "pdf": "https://arxiv.org/pdf/2510.04619", "abs": "https://arxiv.org/abs/2510.04619", "authors": ["Ivan Homoliak", "Martin Perešíni", "Marek Tamaškovič", "Timotej Ponek", "Lukáš Hellebrandt", "Kamil Malinka"], "title": "PoS-CoPOR: Proof-of-Stake Consensus Protocol with Native Onion Routing Providing Scalability and DoS-Resistance", "comment": null, "summary": "Proof-of-Stake (PoS) consensus protocols often face a trade-off between\nperformance and security. Protocols that pre-elect leaders for subsequent\nrounds are vulnerable to Denial-of-Service (DoS) attacks, which can disrupt the\nnetwork and compromise liveness. In this work, we present PoS-CoPOR, a\nsingle-chain PoS consensus protocol that mitigates this vulnerability by\nintegrating a native onion routing mechanism into the consensus protocol\nitself. PoS-CoPOR combines stake-weighted probabilistic leader election with an\nanonymization layer that conceals the network identity of the next block\nproposer. This approach prevents targeted DoS attacks on leaders before they\nproduce a block, thus enhancing network resilience. We implemented and\nevaluated PoS-CoPOR, demonstrating its ability to achieve a throughput of up to\n110 tx/s with 6 nodes, even with the overhead of the anonymization layer. The\nresults show that native anonymization can provide robust DoS resistance with\nonly a modest impact on performance, offering a solution to build secure and\nscalable PoS blockchains."}
{"id": "2510.04073", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04073", "abs": "https://arxiv.org/abs/2510.04073", "authors": ["Santhosh Kumar Ravindran"], "title": "Moral Anchor System: A Predictive Framework for AI Value Alignment and Drift Prevention", "comment": "11 pages Includes simulations with over 4 million steps", "summary": "The rise of artificial intelligence (AI) as super-capable assistants has\ntransformed productivity and decision-making across domains. Yet, this\nintegration raises critical concerns about value alignment - ensuring AI\nbehaviors remain consistent with human ethics and intentions. A key risk is\nvalue drift, where AI systems deviate from aligned values due to evolving\ncontexts, learning dynamics, or unintended optimizations, potentially leading\nto inefficiencies or ethical breaches. We propose the Moral Anchor System\n(MAS), a novel framework to detect, predict, and mitigate value drift in AI\nagents. MAS combines real-time Bayesian inference for monitoring value states,\nLSTM networks for forecasting drift, and a human-centric governance layer for\nadaptive interventions. It emphasizes low-latency responses (<20 ms) to prevent\nbreaches, while reducing false positives and alert fatigue via supervised\nfine-tuning with human feedback. Our hypothesis: integrating probabilistic\ndrift detection, predictive analytics, and adaptive governance can reduce value\ndrift incidents by 80 percent or more in simulations, maintaining high\ndetection accuracy (85 percent) and low false positive rates (0.08\npost-adaptation). Rigorous experiments with goal-misaligned agents validate\nMAS's scalability and responsiveness. MAS's originality lies in its predictive\nand adaptive nature, contrasting static alignment methods. Contributions\ninclude: (1) MAS architecture for AI integration; (2) empirical results\nprioritizing speed and usability; (3) cross-domain applicability insights; and\n(4) open-source code for replication."}
{"id": "2510.04760", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04760", "abs": "https://arxiv.org/abs/2510.04760", "authors": ["Sisay Deresa Sima", "Ayalew Belay Habtie"], "title": "Agile Software Effort Estimation using Regression Techniques", "comment": null, "summary": "Software development effort estimation is one of the most critical aspect in\nsoftware development process, as the success or failure of the entire project\ndepends on the accuracy of estimations. Researchers are still conducting\nstudies on agile effort estimation. The aim of this research is to develop a\nstory point based agile effort estimation model using LASSO and Elastic Net\nregression techniques. The experimental work is applied to the agile story\npoint approach using 21 software projects collected from six firms. The two\nalgorithms are trained using their default parameters and tuned grid search\nwith 5-fold cross-validation to get an enhanced model. The experiment result\nshows LASSO regression achieved better predictive performance PRED (8%) and\nPRED (25%) results of 100.0, MMRE of 0.0491, MMER of 0.0551, MdMRE of 0.0593,\nMdMER of 0.063, and MSE of 0.0007. The results are also compared with other\nrelated literature."}
{"id": "2510.04640", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.04640", "abs": "https://arxiv.org/abs/2510.04640", "authors": ["Ali Asghar", "Andreas Becher", "Daniel Ziener"], "title": "Backing the Wrong Horse: How Bit-Level Netlist Augmentation can Counter Power Side Channel Attacks", "comment": "5 pages, 3 figures", "summary": "The dependence of power-consumption on the processed data is a known\nvulnerability of CMOS circuits, resulting in side channels which can be\nexploited by power-based side channel attacks (SCAs). These attacks can extract\nsensitive information, such as secret keys, from the implementation of\ncryptographic algorithms. Existing countermeasures against power-based side\nchannel attacks focus on analyzing information leakage at the byte level.\nHowever, this approach neglects the impact of individual bits on the overall\nresistance of a cryptographic implementation. In this work, we present a\ncountermeasure based on single-bit leakage. The results suggest that the\nproposed countermeasure cannot be broken by attacks using conventional SCA\nleakage models."}
{"id": "2510.04089", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04089", "abs": "https://arxiv.org/abs/2510.04089", "authors": ["Yitong Cui", "Liu Liu", "Baosheng Yu", "Jiayan Qiu", "Xikai Zhang", "Likang Xiao", "Yixing Liu", "Quan Chen"], "title": "SPOGW: a Score-based Preference Optimization method via Group-Wise comparison for workflows", "comment": null, "summary": "Large language models (LLMs) have exhibited significant capabilities in\naddressing challenging problems throughout various fields, often through the\nuse of agentic workflows that adhere to structured instructions and multi-step\nprocedures. However, designing such workflows demands substantial manual\neffort, posing challenges to scalability and generalizability. Recent studies\nhave aimed to minimize the human intervention needed for their construction,\nleading to advances in automated techniques for optimizing agentic workflows.\nHowever, current approaches are often constrained by their limited\nrepresentational capacity, insufficient adaptability, weak scalability, and\npairwise comparison paradigm -- issues that stem primarily from a dependence on\ndiscrete optimization techniques. To overcome these limitations, we introduce a\nnew score-based preference approach, refereed as SPOGW, which operates directly\non cardinal reward signals through group-wise comparison and enables more\nefficient and stable optimization in a continuous space. SPOGW incorporates\nIterative offline GRPO (ioGRPO) with advantage-masked KL divergence (mKL),\nwhich regulates training update by placing greater emphasis on the advantageous\nregions of the policy response. In five benchmark datasets covering\nmathematical reasoning, coding, and question answering, SPOGW matches or\nexceeds the performance of current state-of-the-art approaches, presenting a\nviable and forward-looking methodology for automated generation and\noptimization of agentic workflows."}
{"id": "2510.04791", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.04791", "abs": "https://arxiv.org/abs/2510.04791", "authors": ["Kristian Kolthoff", "Felix Kretzer", "Simone Paolo Ponzetto", "Alexander Maedche", "Christian Bartelt"], "title": "GUISpector: An MLLM Agent Framework for Automated Verification of Natural Language Requirements in GUI Prototypes", "comment": null, "summary": "GUIs are foundational to interactive systems and play a pivotal role in early\nrequirements elicitation through prototyping. Ensuring that GUI implementations\nfulfill NL requirements is essential for robust software engineering,\nespecially as LLM-driven programming agents become increasingly integrated into\ndevelopment workflows. Existing GUI testing approaches, whether traditional or\nLLM-driven, often fall short in handling the complexity of modern interfaces,\nand typically lack actionable feedback and effective integration with automated\ndevelopment agents. In this paper, we introduce GUISpector, a novel framework\nthat leverages a multi-modal (M)LLM-based agent for the automated verification\nof NL requirements in GUI prototypes. First, GUISpector adapts a MLLM agent to\ninterpret and operationalize NL requirements, enabling to autonomously plan and\nexecute verification trajectories across GUI applications. Second, GUISpector\nsystematically extracts detailed NL feedback from the agent's verification\nprocess, providing developers with actionable insights that can be used to\niteratively refine the GUI artifact or directly inform LLM-based code\ngeneration in a closed feedback loop. Third, we present an integrated tool that\nunifies these capabilities, offering practitioners an accessible interface for\nsupervising verification runs, inspecting agent rationales and managing the\nend-to-end requirements verification process. We evaluated GUISpector on a\ncomprehensive set of 150 requirements based on 900 acceptance criteria\nannotations across diverse GUI applications, demonstrating effective detection\nof requirement satisfaction and violations and highlighting its potential for\nseamless integration of actionable feedback into automated LLM-driven\ndevelopment workflows. The video presentation of GUISpector is available at:\nhttps://youtu.be/JByYF6BNQeE, showcasing its main capabilities."}
{"id": "2510.04652", "categories": ["cs.CR", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.04652", "abs": "https://arxiv.org/abs/2510.04652", "authors": ["Ines Akaichi", "Giorgos Flouris", "Irini Fundulaki", "Sabrina Kirrane"], "title": "Modeling and Managing Temporal Obligations in GUCON Using SPARQL-star and RDF-star", "comment": null, "summary": "In the digital age, data frequently crosses organizational and jurisdictional\nboundaries, making effective governance essential. Usage control policies have\nemerged as a key paradigm for regulating data usage, safeguarding privacy,\nprotecting intellectual property, and ensuring compliance with regulations. A\ncentral mechanism for usage control is the handling of obligations, which arise\nas a side effect of using and sharing data. Effective monitoring of obligations\nrequires capturing usage traces and accounting for temporal aspects such as\nstart times and deadlines, as obligations may evolve over times into different\nstates, such as fulfilled, violated, or expired. While several solutions have\nbeen proposed for obligation monitoring, they often lack formal semantics or\nprovide limited support for reasoning over obligation states. To address these\nlimitations, we extend GUCON, a policy framework grounded in the formal\nsemantics of SPAQRL graph patterns, to explicitly model the temporal aspects of\nan obligation. This extension enables the expressing of temporal obligations\nand supports continuous monitoring of their evolving states based on usage\ntraces stored in temporal knowledge graphs. We demonstrate how this extended\nmodel can be represented using RDF-star and SPARQL-star and propose an\nObligation State Manager that monitors obligation states and assess their\ncompliance with respect to usage traces. Finally, we evaluate both the extended\nmodel and its prototype implementation."}
{"id": "2510.04093", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04093", "abs": "https://arxiv.org/abs/2510.04093", "authors": ["Guixian Zhang", "Guan Yuan", "Ziqi Xu", "Yanmei Zhang", "Zhenyun Deng", "Debo Cheng"], "title": "Harnessing LLM for Noise-Robust Cognitive Diagnosis in Web-Based Intelligent Education Systems", "comment": null, "summary": "Cognitive diagnostics in the Web-based Intelligent Education System (WIES)\naims to assess students' mastery of knowledge concepts from heterogeneous,\nnoisy interactions. Recent work has tried to utilize Large Language Models\n(LLMs) for cognitive diagnosis, yet LLMs struggle with structured data and are\nprone to noise-induced misjudgments. Specially, WIES's open environment\ncontinuously attracts new students and produces vast amounts of response logs,\nexacerbating the data imbalance and noise issues inherent in traditional\neducational systems. To address these challenges, we propose DLLM, a\nDiffusion-based LLM framework for noise-robust cognitive diagnosis. DLLM first\nconstructs independent subgraphs based on response correctness, then applies\nrelation augmentation alignment module to mitigate data imbalance. The two\nsubgraph representations are then fused and aligned with LLM-derived,\nsemantically augmented representations. Importantly, before each alignment\nstep, DLLM employs a two-stage denoising diffusion module to eliminate\nintrinsic noise while assisting structural representation alignment.\nSpecifically, unconditional denoising diffusion first removes erroneous\ninformation, followed by conditional denoising diffusion based on graph-guided\nto eliminate misleading information. Finally, the noise-robust representation\nthat integrates semantic knowledge and structural information is fed into\nexisting cognitive diagnosis models for prediction. Experimental results on\nthree publicly available web-based educational platform datasets demonstrate\nthat our DLLM achieves optimal predictive performance across varying noise\nlevels, which demonstrates that DLLM achieves noise robustness while\neffectively leveraging semantic knowledge from LLM."}
{"id": "2510.04796", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.04796", "abs": "https://arxiv.org/abs/2510.04796", "authors": ["Samah Kansab", "Francis Bordeleau", "Ali Tizghadam"], "title": "RevMine: An LLM-Assisted Tool for Code Review Mining and Analysis Across Git Platforms", "comment": null, "summary": "Empirical research on code review processes is increasingly central to\nunderstanding software quality and collaboration. However, collecting and\nanalyzing review data remains a time-consuming and technically intensive task.\nMost researchers follow similar workflows - writing ad hoc scripts to extract,\nfilter, and analyze review data from platforms like GitHub and GitLab. This\npaper introduces RevMine, a conceptual tool that streamlines the entire code\nreview mining pipeline using large language models (LLMs). RevMine guides users\nthrough authentication, endpoint discovery, and natural language-driven data\ncollection, significantly reducing the need for manual scripting. After\nretrieving review data, it supports both quantitative and qualitative analysis\nbased on user-defined filters or LLM-inferred patterns. This poster outlines\nthe tool's architecture, use cases, and research potential. By lowering the\nbarrier to entry, RevMine aims to democratize code review mining and enable a\nbroader range of empirical software engineering studies."}
{"id": "2510.04882", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.04882", "abs": "https://arxiv.org/abs/2510.04882", "authors": ["Elian Morel"], "title": "Enhancing TreePIR for a Single-Server Setting via Resampling", "comment": null, "summary": "Private Information Retrieval (PIR) allows a client to retrieve an entry\n$\\text{DB}[i]$ from a public database $\\text{DB}$ held by one or more servers,\nwithout revealing the queried index $i$. Traditional PIR schemes achieve\nsublinear server computation only under strong assumptions, such as the\npresence of multiple non-colluding servers or the use of public-key\ncryptography. To overcome these limitations, \\textit{preprocessing PIR} schemes\nintroduce a query-independent offline phase where the client collects\n\\textit{hints} that enable efficient private queries during the online phase.\n  In this work, we focus on preprocessing PIR schemes relying solely on\n\\textit{One-Way Functions} (OWFs), which provide minimal cryptographic\nassumptions and practical implementability. We study three main constructions\n-- TreePIR, PIANO, and PPPS -- that explore different trade-offs between\ncommunication, storage, and server trust assumptions. Building upon the\nmechanisms introduced in PIANO and PPPS, we propose an adaptation of TreePIR to\nthe single-server setting by introducing a dual-table hint structure (primary\nand backup tables) and a \\textit{resampling} technique to refresh hints\nefficiently.\n  Our proposed scheme achieves logarithmic upload bandwidth and $O(\\sqrt{n}\\log\nn)$ download complexity while requiring $O(\\sqrt{n}\\log n)$ client storage.\nThis represents a significant improvement over prior single-server\npreprocessing PIR schemes such as PIANO ($O(\\sqrt{n})$ bandwidth) and PPPS\n($O(n^{1/4})$ bandwidth), while maintaining the simplicity and minimal\nassumptions of the OWF-based setting."}
{"id": "2510.04097", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04097", "abs": "https://arxiv.org/abs/2510.04097", "authors": ["Peichao Lai", "Jinhui Zhuang", "Kexuan Zhang", "Ningchang Xiong", "Shengjie Wang", "Yanwei Xu", "Chong Chen", "Yilei Wang", "Bin Cui"], "title": "WebRenderBench: Enhancing Web Interface Generation through Layout-Style Consistency and Reinforcement Learning", "comment": null, "summary": "Automating the conversion of UI images into web code is a critical task for\nfront-end development and rapid prototyping. Advances in multimodal large\nlanguage models (MLLMs) have made WebUI-to-Code increasingly feasible, yet\nexisting benchmarks remain limited in data diversity and evaluation\nreliability. To address these issues, we present WebRenderBench, a large-scale\nbenchmark of 22.5k webpages collected from real-world portal sites, offering\ngreater diversity, complexity, and realism than prior benchmarks. We further\npropose a novel evaluation metric that measures layout and style consistency\nfrom the final rendered pages. Unlike vision-based methods that rely on costly\nLLM reasoning or structure-based comparisons vulnerable to noise and asymmetry,\nour approach enables more efficient, objective, and reliable UI quality\nassessment. Finally, we introduce the Automated Layout and Style Inspection\nAgent (ALISA), which integrates this metric into reinforcement learning as a\nreward signal to enhance training on crawled asymmetric webpages. Experiments\nshow that ALISA significantly boosts generation performance, achieving\nstate-of-the-art results across multiple metrics."}
{"id": "2510.04835", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.04835", "abs": "https://arxiv.org/abs/2510.04835", "authors": ["Wentao Gao", "Renata Borovica-Gajic", "Sang Kil Cha", "Tian Qiu", "Van-Thuan Pham"], "title": "InsightQL: Advancing Human-Assisted Fuzzing with a Unified Code Database and Parameterized Query Interface", "comment": null, "summary": "Fuzzing is a highly effective automated testing method for uncovering\nsoftware vulnerabilities. Despite advances in fuzzing techniques, such as\ncoverage-guided greybox fuzzing, many fuzzers struggle with coverage plateaus\ncaused by fuzz blockers, limiting their ability to find deeper vulnerabilities.\nHuman expertise can address these challenges, but analyzing fuzzing results to\nguide this support remains labor-intensive. To tackle this, we introduce\nInsightQL, the first human-assisting framework for fuzz blocker analysis.\nPowered by a unified database and an intuitive parameterized query interface,\nInsightQL aids developers in systematically extracting insights and efficiently\nunblocking fuzz blockers. Our experiments on 14 popular real-world libraries\nfrom the FuzzBench benchmark demonstrate the effectiveness of InsightQL,\nleading to the unblocking of many fuzz blockers and considerable improvements\nin code coverage (up to 13.90%)."}
{"id": "2510.04885", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04885", "abs": "https://arxiv.org/abs/2510.04885", "authors": ["Yuxin Wen", "Arman Zharmagambetov", "Ivan Evtimov", "Narine Kokhlikyan", "Tom Goldstein", "Kamalika Chaudhuri", "Chuan Guo"], "title": "RL Is a Hammer and LLMs Are Nails: A Simple Reinforcement Learning Recipe for Strong Prompt Injection", "comment": null, "summary": "Prompt injection poses a serious threat to the reliability and safety of LLM\nagents. Recent defenses against prompt injection, such as Instruction Hierarchy\nand SecAlign, have shown notable robustness against static attacks. However, to\nmore thoroughly evaluate the robustness of these defenses, it is arguably\nnecessary to employ strong attacks such as automated red-teaming. To this end,\nwe introduce RL-Hammer, a simple recipe for training attacker models that\nautomatically learn to perform strong prompt injections and jailbreaks via\nreinforcement learning. RL-Hammer requires no warm-up data and can be trained\nentirely from scratch. To achieve high ASRs against industrial-level models\nwith defenses, we propose a set of practical techniques that enable highly\neffective, universal attacks. Using this pipeline, RL-Hammer reaches a 98% ASR\nagainst GPT-4o and a $72\\%$ ASR against GPT-5 with the Instruction Hierarchy\ndefense. We further discuss the challenge of achieving high diversity in\nattacks, highlighting how attacker models tend to reward-hack diversity\nobjectives. Finally, we show that RL-Hammer can evade multiple prompt injection\ndetectors. We hope our work advances automatic red-teaming and motivates the\ndevelopment of stronger, more principled defenses. Code is available at\nhttps://github.com/facebookresearch/rl-injector."}
{"id": "2510.04116", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04116", "abs": "https://arxiv.org/abs/2510.04116", "authors": ["Ziying Zhang", "Yaqing Wang", "Quanming Yao"], "title": "Searching Meta Reasoning Skeleton to Guide LLM Reasoning", "comment": null, "summary": "Meta reasoning behaviors work as a skeleton to guide large language model\n(LLM) reasoning, thus help to improve reasoning performance. However, prior\nresearches implement meta reasoning skeleton with manually designed structure,\nlimiting ability to adapt to query-specific requirement and capture intricate\nlogical dependency among reasoning steps. To deal with the challenges, we\nrepresent meta reasoning skeleton with directed acyclic graph (DAG) to unify\nskeletons proposed in prior works and model intricate logical dependency. Then\nwe propose AutoMR, a framework that searches for query-aware meta reasoning\nskeleton automatically inspired by automated machine learning (AutoML).\nSpecifically, we construct search space based on DAG representation of skeleton\nand then formulate the search problem. We design a dynamic skeleton sampling\nalgorithm by expanding meta reasoning skeleton along with reasoning context at\ninference time. This algorithm can derive any meta reasoning skeleton in search\nspace efficiently and adapt skeleton to evolving base reasoning context, thus\nenable efficient query-aware skeleton search. We conduct experiments on\nextensive benchmark datasets. Experimental results show that AutoMR achieves\nbetter reasoning performance than previous works broadly."}
{"id": "2510.04852", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04852", "abs": "https://arxiv.org/abs/2510.04852", "authors": ["Victor May", "Diganta Misra", "Yanqi Luo", "Anjali Sridhar", "Justine Gehring", "Silvio Soares Ribeiro Junior"], "title": "FreshBrew: A Benchmark for Evaluating AI Agents on Java Code Migration", "comment": "18 pages, 11 figures", "summary": "AI coding assistants are rapidly becoming integral to modern software\ndevelopment. A key challenge in this space is the continual need to migrate and\nmodernize codebases in response to evolving software ecosystems. Traditionally,\nsuch migrations have relied on rule-based systems and human intervention. With\nthe advent of powerful large language models (LLMs), AI-driven agentic\nframeworks offer a promising alternative-but their effectiveness has not been\nsystematically evaluated. In this paper, we introduce FreshBrew, a novel\nbenchmark for evaluating AI agents on project-level Java migrations, with a\nspecific focus on measuring an agent's ability to preserve program semantics\nand avoid reward hacking, which we argue requires projects with high test\ncoverage for a rigorous and reliable evaluation. We benchmark several\nstate-of-the-art LLMs, and compare their performance against established\nrule-based tools. Our evaluation of AI agents on this benchmark of 228\nrepositories shows that the top-performing model, Gemini 2.5 Flash, can\nsuccessfully migrate 52.3 percent of projects to JDK 17. Our empirical analysis\nreveals novel insights into the critical strengths and limitations of current\nagentic approaches, offering actionable insights into their real-world\napplicability. Our empirical study reveals failure modes of current AI agents\nin realistic Java modernization tasks, providing a foundation for evaluating\ntrustworthy code-migration systems. By releasing FreshBrew, we aim to\nfacilitate rigorous, reproducible evaluation and catalyze progress in AI-driven\ncodebase modernization."}
{"id": "2510.04987", "categories": ["cs.CR", "I.2"], "pdf": "https://arxiv.org/pdf/2510.04987", "abs": "https://arxiv.org/abs/2510.04987", "authors": ["Avilash Rath", "Weiliang Qi", "Youpeng Li", "Xinda Wang"], "title": "NatGVD: Natural Adversarial Example Attack towards Graph-based Vulnerability Detection", "comment": "10 pages, 2 figures (2 additional figures in Appendices)", "summary": "Graph-based models learn rich code graph structural information and present\nsuperior performance on various code analysis tasks. However, the robustness of\nthese models against adversarial example attacks in the context of\nvulnerability detection remains an open question. This paper proposes NatGVD, a\nnovel attack methodology that generates natural adversarial vulnerable code to\ncircumvent GNN-based and graph-aware transformer-based vulnerability detectors.\nNatGVD employs a set of code transformations that modify graph structure while\npreserving code semantics. Instead of injecting dead or unrelated code like\nprevious works, NatGVD considers naturalness requirements: generated examples\nshould not be easily recognized by humans or program analysis tools. With\nextensive evaluation of NatGVD on state-of-the-art vulnerability detection\nsystems, the results reveal up to 53.04% evasion rate across GNN-based\ndetectors and graph-aware transformer-based detectors. We also explore\npotential defense strategies to enhance the robustness of these systems against\nNatGVD."}
{"id": "2510.04128", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.04128", "abs": "https://arxiv.org/abs/2510.04128", "authors": ["Dmitrii Troitskii", "Koyena Pal", "Chris Wendler", "Callum Stuart McDougall", "Neel Nanda"], "title": "Internal states before wait modulate reasoning patterns", "comment": "Accepted to EMNLP Findings 2025", "summary": "Prior work has shown that a significant driver of performance in reasoning\nmodels is their ability to reason and self-correct. A distinctive marker in\nthese reasoning traces is the token wait, which often signals reasoning\nbehavior such as backtracking. Despite being such a complex behavior, little is\nunderstood of exactly why models do or do not decide to reason in this\nparticular manner, which limits our understanding of what makes a reasoning\nmodel so effective. In this work, we address the question whether model's\nlatents preceding wait tokens contain relevant information for modulating the\nsubsequent reasoning process. We train crosscoders at multiple layers of\nDeepSeek-R1-Distill-Llama-8B and its base version, and introduce a latent\nattribution technique in the crosscoder setting. We locate a small set of\nfeatures relevant for promoting/suppressing wait tokens' probabilities.\nFinally, through a targeted series of experiments analyzing max activating\nexamples and causal interventions, we show that many of our identified features\nindeed are relevant for the reasoning process and give rise to different types\nof reasoning patterns such as restarting from the beginning, recalling prior\nknowledge, expressing uncertainty, and double-checking."}
{"id": "2510.04905", "categories": ["cs.SE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.04905", "abs": "https://arxiv.org/abs/2510.04905", "authors": ["Yicheng Tao", "Yao Qin", "Yepang Liu"], "title": "Retrieval-Augmented Code Generation: A Survey with Focus on Repository-Level Approaches", "comment": null, "summary": "Recent advancements in large language models (LLMs) have substantially\nimproved automated code generation. While function-level and file-level\ngeneration have achieved promising results, real-world software development\ntypically requires reasoning across entire repositories. This gives rise to the\nchallenging task of Repository-Level Code Generation (RLCG), where models must\ncapture long-range dependencies, ensure global semantic consistency, and\ngenerate coherent code spanning multiple files or modules. To address these\nchallenges, Retrieval-Augmented Generation (RAG) has emerged as a powerful\nparadigm that integrates external retrieval mechanisms with LLMs, enhancing\ncontext-awareness and scalability. In this survey, we provide a comprehensive\nreview of research on Retrieval-Augmented Code Generation (RACG), with an\nemphasis on repository-level approaches. We categorize existing work along\nseveral dimensions, including generation strategies, retrieval modalities,\nmodel architectures, training paradigms, and evaluation protocols. Furthermore,\nwe summarize widely used datasets and benchmarks, analyze current limitations,\nand outline key challenges and opportunities for future research. Our goal is\nto establish a unified analytical framework for understanding this rapidly\nevolving field and to inspire continued progress in AI-powered software\nengineering."}
{"id": "2510.05052", "categories": ["cs.CR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.05052", "abs": "https://arxiv.org/abs/2510.05052", "authors": ["Weiliang Zhao", "Jinjun Peng", "Daniel Ben-Levi", "Zhou Yu", "Junfeng Yang"], "title": "Proactive defense against LLM Jailbreak", "comment": null, "summary": "The proliferation of powerful large language models (LLMs) has necessitated\nrobust safety alignment, yet these models remain vulnerable to evolving\nadversarial attacks, including multi-turn jailbreaks that iteratively search\nfor successful queries. Current defenses, primarily reactive and static, often\nfail to counter these search-based attacks. In this paper, we introduce ProAct,\na novel proactive defense framework designed to disrupt and mislead autonomous\njailbreaking processes. Our core idea is to intentionally provide adversaries\nwith \"spurious responses\" that appear to be results of successful jailbreak\nattacks but contain no actual harmful content. These misleading responses\nprovide false signals to the attacker's internal optimization loop, causing the\nadversarial search to terminate prematurely and effectively jailbreaking the\njailbreak. By conducting extensive experiments across state-of-the-art LLMs,\njailbreaking frameworks, and safety benchmarks, our method consistently and\nsignificantly reduces attack success rates by up to 92\\%. When combined with\nother defense frameworks, it further reduces the success rate of the latest\nattack strategies to 0\\%. ProAct represents an orthogonal defense strategy that\ncan serve as an additional guardrail to enhance LLM safety against the most\neffective jailbreaking attacks."}
{"id": "2510.04140", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.04140", "abs": "https://arxiv.org/abs/2510.04140", "authors": ["Zishang Jiang", "Jinyi Han", "Tingyun Li", "Xinyi Wang", "Sihang Jiang", "Jiaqing Liang", "Zhaoqian Dai", "Shuguang Ma", "Fei Yu", "Yanghua Xiao"], "title": "Selective Expert Guidance for Effective and Diverse Exploration in Reinforcement Learning of LLMs", "comment": null, "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has become a widely\nadopted technique for enhancing the reasoning ability of Large Language Models\n(LLMs). However, the effectiveness of RLVR strongly depends on the capability\nof base models. This issue arises because it requires the model to have\nsufficient capability to perform high-quality exploration, which involves both\neffectiveness and diversity. Unfortunately, existing methods address this issue\nby imitating expert trajectories, which improve effectiveness but neglect\ndiversity. To address this, we argue that the expert only needs to provide\nguidance only at critical decision points rather than the entire reasoning\npath. Based on this insight, we propose MENTOR: Mixed-policy Expert Navigation\nfor Token-level Optimization of Reasoning, a framework that provides expert\nguidance only at critical decision points to perform effective and diverse\nexploration in RLVR. Extensive experiments show that MENTOR enables models\ncapture the essence of expert strategies rather than surface imitation, thereby\nperforming high-quality exploration and achieving superior overall performance.\nOur code is available online."}
{"id": "2510.04964", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.04964", "abs": "https://arxiv.org/abs/2510.04964", "authors": ["Kelechi G. Kalu", "James C. Davis"], "title": "Why Software Signing (Still) Matters: Trust Boundaries in the Software Supply Chain", "comment": "8 Pages, 3 Figures", "summary": "Software signing provides a formal mechanism for provenance by ensuring\nartifact integrity and verifying producer identity. It also imposes tooling and\noperational costs to implement in practice. In an era of centralized registries\nsuch as PyPI, npm, Maven Central, and Hugging Face, it is reasonable to ask\nwhether hardening registry security controls obviates the need for end-to-end\nartifact signing. In this work, we posit that the core guarantees of signing,\nprovenance, integrity, and accountability are not automatically carried across\ndifferent software distribution boundaries. These boundaries include mirrors,\ncorporate proxies, re-hosting, and air-gapped transfers, where registry\nsecurity controls alone cannot provide sufficient assurance. We synthesize\nhistorical practice and present a trust model for modern distribution modes to\nidentify when signing is necessary to extend trust beyond registry control.\nTreating signing as a baseline layer of defense strengthens software supply\nchain assurance even when registries are secure."}
{"id": "2510.03285", "categories": ["cs.AI", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03285", "abs": "https://arxiv.org/abs/2510.03285", "authors": ["Su Kara", "Fazle Faisal", "Suman Nath"], "title": "WAREX: Web Agent Reliability Evaluation on Existing Benchmarks", "comment": null, "summary": "Recent advances in browser-based LLM agents have shown promise for automating\ntasks ranging from simple form filling to hotel booking or online shopping.\nCurrent benchmarks measure agent performance in controlled environments, such\nas containers or stable networks, where websites behave deterministically.\nHowever, in the real world, users access websites over networks and HTTPS\nconnections that introduce instability from multiple sources: client-side,\nserver-side issues or broader system failures. Moreover, live websites are\nprone to web attacks such Cross-Site Scripting, as well as general site\nmodifications which can cause unexpected or malicious pop-ups or improper\nfunctionality. To address this gap, we present WAREX: Web Agent Reliability\nEvaluation on Existing Benchmarks. We measure the impact of WAREX across three\npopular benchmarks: WebArena, WebVoyager, and REAL. Our experiments show that\nintroducing WAREX leads to significant drops in task success rates,\nhighlighting the limited robustness of state-of-the-art agents."}
{"id": "2510.04141", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04141", "abs": "https://arxiv.org/abs/2510.04141", "authors": ["Mayank Ravishankara", "Varindra V. Persad Maharaj"], "title": "The Artificial Intelligence Cognitive Examination: A Survey on the Evolution of Multimodal Evaluation from Recognition to Reasoning", "comment": null, "summary": "This survey paper chronicles the evolution of evaluation in multimodal\nartificial intelligence (AI), framing it as a progression of increasingly\nsophisticated \"cognitive examinations.\" We argue that the field is undergoing a\nparadigm shift, moving from simple recognition tasks that test \"what\" a model\nsees, to complex reasoning benchmarks that probe \"why\" and \"how\" it\nunderstands. This evolution is driven by the saturation of older benchmarks,\nwhere high performance often masks fundamental weaknesses. We chart the journey\nfrom the foundational \"knowledge tests\" of the ImageNet era to the \"applied\nlogic and comprehension\" exams such as GQA and Visual Commonsense Reasoning\n(VCR), which were designed specifically to diagnose systemic flaws such as\nshortcut learning and failures in compositional generalization. We then survey\nthe current frontier of \"expert-level integration\" benchmarks (e.g., MMBench,\nSEED-Bench, MMMU) designed for today's powerful multimodal large language\nmodels (MLLMs), which increasingly evaluate the reasoning process itself.\nFinally, we explore the uncharted territories of evaluating abstract, creative,\nand social intelligence. We conclude that the narrative of AI evaluation is not\nmerely a history of datasets, but a continuous, adversarial process of\ndesigning better examinations that, in turn, redefine our goals for creating\ntruly intelligent systems."}
{"id": "2510.04982", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.04982", "abs": "https://arxiv.org/abs/2510.04982", "authors": ["Aakash Ahmad", "Muhammad Waseem", "Bakheet Aljedaani", "Mahdi Fahmideh", "Peng Liang", "Feras Awaysheh"], "title": "Quantum Computing as a Service - a Software Engineering Perspective", "comment": "37 pages, 10 images, 5 tables, Manuscript submitted to a Journal\n  (2025)", "summary": "Quantum systems have started to emerge as a disruptive technology and\nenabling platforms - exploiting the principles of quantum mechanics via\nprogrammable quantum bits (QuBits) - to achieve quantum supremacy in computing.\nAcademic research, industrial projects (e.g., Amazon Braket, IBM Qiskit), and\nconsortiums like 'Quantum Flagship' are striving to develop practically capable\nand commercially viable quantum computing (QC) systems and technologies.\nQuantum Computing as a Service (QCaaS) is viewed as a solution attuned to the\nphilosophy of service-orientation that can offer QC resources and platforms, as\nutility computing, to individuals and organisations who do not own quantum\ncomputers. This research investigates a process-centric and architecture-driven\napproach to offer a software engineering perspective on enabling QCaaS - a.k.a\nquantum service-orientation. We employed a two-phase research method comprising\n(a) a systematic mapping study and (b) an architecture-based development, first\nto identify the phases of the quantum service development life cycle and\nsubsequently to integrate these phases into a reference architecture that\nsupports QCaaS. The SMS process retrieved a collection of potentially relevant\nresearch literature and based on a multi-step selection and qualitative\nassessment, we selected 41 peer-reviewed studies to answer three RQs. The RQs\ninvestigate (i) demographic details in terms of frequency, types, and trends of\nresearch, (ii) phases of quantum service development lifecycle to derive a\nreference architecture for conception, modeling, assembly, and deployment of\nservices, and (iii) The results identify a 4-phased development lifecycle along\nwith quantum significant requirements (QSRs), various modeling notations,\ncatalogue of patterns, programming languages, and deployment platforms that can\nbe integrated in a layered reference architecture to engineer QCaaS."}
{"id": "2510.03461", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.03461", "abs": "https://arxiv.org/abs/2510.03461", "authors": ["Sanjay Malakar", "Michael D. Ernst", "Martin Kellogg", "Manu Sridharan"], "title": "Repairing Leaks in Resource Wrappers", "comment": null, "summary": "A resource leak occurs when a program fails to release a finite resource like\na socket, file descriptor or database connection. While sound static analysis\ntools can detect all leaks, automatically repairing them remains challenging.\nPrior work took the output of a detection tool and attempted to repair only\nleaks from a hard-coded list of library resource types. That approach limits\nthe scope of repairable leaks: real-world code uses resource wrappers that\nstore a resource in a field and must themselves be closed. This paper makes\nfour key contributions to improve resource leak repair in the presence of\nwrappers. (1) It integrates inference of resource management specifications\ninto the repair pipeline, enabling extant fixing approaches to reason about\nwrappers. (2) It transforms programs into variants that are easier to analyze,\nmaking inference, detection, and fixing tools more effective; for instance, it\nmakes detection tools report problems closer to the root cause, often in a\nclient of a resource wrapper rather than within the wrapper class itself. (3) A\nnovel field containment analysis reasons about resource lifetimes, enabling\nrepair of more leaks involving resources stored in fields. (4) It introduces a\nnew repair pattern and more precise reasoning to better handle resources stored\nin non-final fields. Prior work fixed 41% of resource leak warnings in the NJR\nbenchmark suite; our implementation Arodnap fixes 68%."}
{"id": "2510.04173", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04173", "abs": "https://arxiv.org/abs/2510.04173", "authors": ["Yassine Benajiba", "Cesare Bernardis", "Vladislav Blinov", "Paul Cayet", "Hassan Chafi", "Abderrahim Fathan", "Louis Faucon", "Damien Hilloulin", "Sungpack Hong", "Ingo Kossyk", "Rhicheek Patra", "Sujith Ravi", "Jonas Schweizer", "Jyotika Singh", "Shailender Singh", "Xuelin Situ", "Weiyi Sun", "Jerry Xu", "Ying Xu"], "title": "Open Agent Specification (Agent Spec) Technical Report", "comment": null, "summary": "Open Agent Specification (Agent Spec) is a declarative language that allows\nAI agents and their workflows to be defined in a way that is compatible across\ndifferent AI frameworks, promoting portability and interoperability within AI\nAgent frameworks.\n  Agent Spec aims to resolve the challenges of fragmented agent development by\nproviding a common unified specification that allows AI agents to be designed\nonce and deployed across various frameworks, improving interoperability and\nreusability, and reducing redundant development efforts. Additionally, Agent\nSpec facilitates development tools and portability, allowing AI agents to be\ndefined independently of their execution environment and enabling teams to\nexchange solutions without implementation-specific limitations.\n  Agent Spec benefits four key groups: (i) Agent developers, who gain access to\na superset of reusable components and design patterns, enabling them to\nleverage a broader range of functionalities; (ii) Agent framework and tool\ndevelopers, who can use Agent Spec as an interchange format and therefore\nbenefit from the support of other frameworks as well as other tools; (iii)\nResearchers, who can achieve reproducible results and comparability,\nfacilitating more reliable and consistent outcomes; (iv) Enterprises, which\nbenefit from faster prototype-to-deployment, increased productivity, as well as\ngreater scalability and maintainability for their AI agent solutions. This\ntechnical report provides an overview of the technical foundations of Agent\nSpec, including motivation, benefits, and future developments."}
{"id": "2510.04997", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04997", "abs": "https://arxiv.org/abs/2510.04997", "authors": ["Jiongchi Yu", "Weipeng Jiang", "Xiaoyu Zhang", "Qiang Hu", "Xiaofei Xie", "Chao Shen"], "title": "AutoEmpirical: LLM-Based Automated Research for Empirical Software Fault Analysis", "comment": "5 pages", "summary": "Understanding software faults is essential for empirical research in software\ndevelopment and maintenance. However, traditional fault analysis, while\nvaluable, typically involves multiple expert-driven steps such as collecting\npotential faults, filtering, and manual investigation. These processes are both\nlabor-intensive and time-consuming, creating bottlenecks that hinder\nlarge-scale fault studies in complex yet critical software systems and slow the\npace of iterative empirical research.\n  In this paper, we decompose the process of empirical software fault study\ninto three key phases: (1) research objective definition, (2) data preparation,\nand (3) fault analysis, and we conduct an initial exploration study of applying\nLarge Language Models (LLMs) for fault analysis of open-source software.\nSpecifically, we perform the evaluation on 3,829 software faults drawn from a\nhigh-quality empirical study. Our results show that LLMs can substantially\nimprove efficiency in fault analysis, with an average processing time of about\ntwo hours, compared to the weeks of manual effort typically required. We\nconclude by outlining a detailed research plan that highlights both the\npotential of LLMs for advancing empirical fault studies and the open challenges\nthat required be addressed to achieve fully automated, end-to-end software\nfault analysis."}
{"id": "2510.03612", "categories": ["cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.03612", "abs": "https://arxiv.org/abs/2510.03612", "authors": ["Tanqiu Jiang", "Min Bai", "Nikolaos Pappas", "Yanjun Qi", "Sandesh Swamy"], "title": "Cross-Modal Content Optimization for Steering Web Agent Preferences", "comment": null, "summary": "Vision-language model (VLM)-based web agents increasingly power high-stakes\nselection tasks like content recommendation or product ranking by combining\nmultimodal perception with preference reasoning. Recent studies reveal that\nthese agents are vulnerable against attackers who can bias selection outcomes\nthrough preference manipulations using adversarial pop-ups, image\nperturbations, or content tweaks. Existing work, however, either assumes strong\nwhite-box access, with limited single-modal perturbations, or uses impractical\nsettings. In this paper, we demonstrate, for the first time, that joint\nexploitation of visual and textual channels yields significantly more powerful\npreference manipulations under realistic attacker capabilities. We introduce\nCross-Modal Preference Steering (CPS) that jointly optimizes imperceptible\nmodifications to an item's visual and natural language descriptions, exploiting\nCLIP-transferable image perturbations and RLHF-induced linguistic biases to\nsteer agent decisions. In contrast to prior studies that assume gradient\naccess, or control over webpages, or agent memory, we adopt a realistic\nblack-box threat setup: a non-privileged adversary can edit only their own\nlisting's images and textual metadata, with no insight into the agent's model\ninternals. We evaluate CPS on agents powered by state-of-the-art proprietary\nand open source VLMs including GPT-4.1, Qwen-2.5VL and Pixtral-Large on both\nmovie selection and e-commerce tasks. Our results show that CPS is\nsignificantly more effective than leading baseline methods. For instance, our\nresults show that CPS consistently outperforms baselines across all models\nwhile maintaining 70% lower detection rates, demonstrating both effectiveness\nand stealth. These findings highlight an urgent need for robust defenses as\nagentic systems play an increasingly consequential role in society."}
{"id": "2510.04195", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04195", "abs": "https://arxiv.org/abs/2510.04195", "authors": ["Puzhen Zhang", "Xuyang Chen", "Yu Feng", "Yuhan Jiang", "Liqiu Meng"], "title": "Constructing coherent spatial memory in LLM agents through graph rectification", "comment": null, "summary": "Given a map description through global traversal navigation instructions\n(e.g., visiting each room sequentially with action signals such as north, west,\netc.), an LLM can often infer the implicit spatial layout of the environment\nand answer user queries by providing a shortest path from a start to a\ndestination (for instance, navigating from the lobby to a meeting room via the\nhall and elevator). However, such context-dependent querying becomes incapable\nas the environment grows much longer, motivating the need for incremental map\nconstruction that builds a complete topological graph from stepwise\nobservations. We propose a framework for LLM-driven construction and map\nrepair, designed to detect, localize, and correct structural inconsistencies in\nincrementally constructed navigation graphs. Central to our method is the\nVersion Control, which records the full history of graph edits and their source\nobservations, enabling fine-grained rollback, conflict tracing, and repair\nevaluation. We further introduce an Edge Impact Score to prioritize\nminimal-cost repairs based on structural reachability, path usage, and conflict\npropagation. To properly evaluate our approach, we create a refined version of\nthe MANGO benchmark dataset by systematically removing non-topological actions\nand inherent structural conflicts, providing a cleaner testbed for LLM-driven\nconstruction and map repair. Our approach significantly improves map\ncorrectness and robustness, especially in scenarios with entangled or chained\ninconsistencies. Our results highlight the importance of introspective,\nhistory-aware repair mechanisms for maintaining coherent spatial memory in LLM\nagents."}
{"id": "2510.04397", "categories": ["cs.CR", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.04397", "abs": "https://arxiv.org/abs/2510.04397", "authors": ["Van Nguyen", "Surya Nepal", "Xingliang Yuan", "Tingmin Wu", "Fengchao Chen", "Carsten Rudolph"], "title": "MulVuln: Enhancing Pre-trained LMs with Shared and Language-Specific Knowledge for Multilingual Vulnerability Detection", "comment": null, "summary": "Software vulnerabilities (SVs) pose a critical threat to safety-critical\nsystems, driving the adoption of AI-based approaches such as machine learning\nand deep learning for software vulnerability detection. Despite promising\nresults, most existing methods are limited to a single programming language.\nThis is problematic given the multilingual nature of modern software, which is\noften complex and written in multiple languages. Current approaches often face\nchallenges in capturing both shared and language-specific knowledge of source\ncode, which can limit their performance on diverse programming languages and\nreal-world codebases. To address this gap, we propose MULVULN, a novel\nmultilingual vulnerability detection approach that learns from source code\nacross multiple languages. MULVULN captures both the shared knowledge that\ngeneralizes across languages and the language-specific knowledge that reflects\nunique coding conventions. By integrating these aspects, it achieves more\nrobust and effective detection of vulnerabilities in real-world multilingual\nsoftware systems. The rigorous and extensive experiments on the real-world and\ndiverse REEF dataset, consisting of 4,466 CVEs with 30,987 patches across seven\nprogramming languages, demonstrate the superiority of MULVULN over thirteen\neffective and state-of-the-art baselines. Notably, MULVULN achieves\nsubstantially higher F1-score, with improvements ranging from 1.45% to 23.59%\ncompared to the baseline methods."}
{"id": "2510.03863", "categories": ["cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.03863", "abs": "https://arxiv.org/abs/2510.03863", "authors": ["Arina Kharlamova", "Bowei He", "Chen Ma", "Xue Liu"], "title": "Spatial CAPTCHA: Generatively Benchmarking Spatial Reasoning for Human-Machine Differentiation", "comment": "Submitted to ICLR 2026", "summary": "Online services rely on CAPTCHAs as a first line of defense against automated\nabuse, yet recent advances in multi-modal large language models (MLLMs) have\neroded the effectiveness of conventional designs that focus on text recognition\nor 2D image understanding. To address this challenge, we present Spatial\nCAPTCHA, a novel human-verification framework that leverages fundamental\ndifferences in spatial reasoning between humans and MLLMs. Unlike existing\nCAPTCHAs which rely on low-level perception tasks that are vulnerable to modern\nAI, Spatial CAPTCHA generates dynamic questions requiring geometric reasoning,\nperspective-taking, occlusion handling, and mental rotation. These skills are\nintuitive for humans but difficult for state-of-the-art (SOTA) AI systems. The\nsystem employs a procedural generation pipeline with constraint-based\ndifficulty control, automated correctness verification, and human-in-the-loop\nvalidation to ensure scalability, robustness, and adaptability. Evaluation on a\ncorresponding benchmark, Spatial-CAPTCHA-Bench, demonstrates that humans vastly\noutperform 10 state-of-the-art MLLMs, with the best model achieving only 31.0%\nPass@1 accuracy. Furthermore, we compare Spatial CAPTCHA with Google reCAPTCHA,\nwhich confirms its effectiveness as both a security mechanism and a diagnostic\ntool for spatial reasoning in AI."}
{"id": "2510.04196", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04196", "abs": "https://arxiv.org/abs/2510.04196", "authors": ["Yizhuo Ding", "Mingkang Chen", "Qiuhua Liu", "Fenghua Weng", "Wanying Qu", "Yue Yang", "Yugang Jiang", "Zuxuan Wu", "Yanwei Fu", "Wenqi Shao"], "title": "COSMO-RL: Towards Trustworthy LMRMs via Joint Safety and Stability", "comment": null, "summary": "Large Multimodal Reasoning Models (LMRMs) are moving into real applications,\nwhere they must be both useful and safe. Safety is especially challenging in\nmultimodal settings: images and text can be combined to bypass guardrails, and\nsingle objective training can cause policy drift that yields over-refusal on\nbenign inputs or unsafe compliance on risky ones. We present COSMO-RL, a mixed\nreinforcement learning framework that trains reasoning oriented LMRMs under\nmultimodal, multitask, and multiobjective signals, and we release the resulting\nmodel, COSMO-R1. Our approach aims to let safety and capability grow together\nin one stable pipeline rather than competing during alignment. In experiments,\nCOSMO-R1 improves safety while maintaining-and often improving multimodal\nreasoning and instruction following, shows stronger robustness to multimodal\njailbreaks, and reduces unnecessary refusals. The framework also transfers\nacross backbones with consistent gains. Ablations support the design choices,\nindicating a simple path to advancing safety and general capability together in\nLMRMs."}
{"id": "2510.03969", "categories": ["cs.AI", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03969", "abs": "https://arxiv.org/abs/2510.03969", "authors": ["Chengxiao Wang", "Isha Chaudhary", "Qian Hu", "Weitong Ruan", "Rahul Gupta", "Gagandeep Singh"], "title": "Quantifying Risks in Multi-turn Conversation with Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) can produce catastrophic responses in\nconversational settings that pose serious risks to public safety and security.\nExisting evaluations often fail to fully reveal these vulnerabilities because\nthey rely on fixed attack prompt sequences, lack statistical guarantees, and do\nnot scale to the vast space of multi-turn conversations. In this work, we\npropose QRLLM, a novel, principled Certification framework for Catastrophic\nrisks in multi-turn Conversation for LLMs that bounds the probability of an LLM\ngenerating catastrophic responses under multi-turn conversation distributions\nwith statistical guarantees. We model multi-turn conversations as probability\ndistributions over query sequences, represented by a Markov process on a query\ngraph whose edges encode semantic similarity to capture realistic\nconversational flow, and quantify catastrophic risks using confidence\nintervals. We define several inexpensive and practical distributions: random\nnode, graph path, adaptive with rejection. Our results demonstrate that these\ndistributions can reveal substantial catastrophic risks in frontier models,\nwith certified lower bounds as high as 70\\% for the worst model, highlighting\nthe urgent need for improved safety training strategies in frontier LLMs."}
{"id": "2510.04206", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04206", "abs": "https://arxiv.org/abs/2510.04206", "authors": ["Hanchen Zhang", "Xiao Liu", "Bowen Lv", "Xueqiao Sun", "Bohao Jing", "Iat Long Iong", "Zhenyu Hou", "Zehan Qi", "Hanyu Lai", "Yifan Xu", "Rui Lu", "Hongning Wang", "Jie Tang", "Yuxiao Dong"], "title": "AgentRL: Scaling Agentic Reinforcement Learning with a Multi-Turn, Multi-Task Framework", "comment": null, "summary": "Recent advances in large language models (LLMs) have sparked growing interest\nin building generalist agents that can learn through online interactions.\nHowever, applying reinforcement learning (RL) to train LLM agents in\nmulti-turn, multi-task settings remains challenging due to lack of scalable\ninfrastructure and stable training algorithms. In this work, we present the\nAgentRL framework for scalable multi-turn, multi-task agentic RL training. On\nthe infrastructure side, AgentRL features a fully-asynchronous\ngeneration-training pipeline for efficient multi-turn RL. To support\nheterogeneous environment development in multi-task RL, we design a unified\nfunction-call based API interface, containerized environment development, and a\ncentralized controller. On the algorithm side, we propose cross-policy sampling\nto encourage model exploration in multi-turn settings and task advantage\nnormalization to stabilize multi-task training. Experiments show that AgentRL,\ntrained on open LLMs across five agentic tasks, significantly outperforms\nGPT-5, Clause-Sonnet-4, DeepSeek-R1, and other open-source LLM agents.\nMulti-task training with AgentRL matches the best results among all\ntask-specific models. AgentRL is open-sourced at\nhttps://github.com/THUDM/AgentRL. The algorithm and framework are adopted in\nbuilding \\textsc{\\href{https://autoglm.zhipuai.cn}{AutoGLM}}."}
{"id": "2510.04265", "categories": ["cs.AI", "cs.CL", "math.ST", "stat.ML", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.04265", "abs": "https://arxiv.org/abs/2510.04265", "authors": ["Mohsen Hariri", "Amirhossein Samandar", "Michael Hinczewski", "Vipin Chaudhary"], "title": "Don't Pass$\\mathtt{@}k$: A Bayesian Framework for Large Language Model Evaluation", "comment": "Code and simulations: https://mohsenhariri.github.io/bayes-kit", "summary": "Pass$@k$ is widely used to report performance for LLM reasoning, but it often\nyields unstable, misleading rankings, especially when the number of trials\n(samples) is limited and compute is constrained. We present a principled\nBayesian evaluation framework that replaces Pass$@k$ and average accuracy over\n$N$ trials (avg$@N$) with posterior estimates of a model's underlying success\nprobability and credible intervals, yielding stable rankings and a transparent\ndecision rule for differences. Evaluation outcomes are modeled as categorical\n(not just 0/1) with a Dirichlet prior, giving closed-form expressions for the\nposterior mean and uncertainty of any weighted rubric and enabling the use of\nprior evidence when appropriate. Theoretically, under a uniform prior, the\nBayesian posterior mean is order-equivalent to average accuracy (Pass$@1$),\nexplaining its empirical robustness while adding principled uncertainty.\nEmpirically, in simulations with known ground-truth success rates and on\nAIME'24/'25, HMMT'25, and BrUMO'25, the Bayesian/avg procedure achieves faster\nconvergence and greater rank stability than Pass$@k$ and recent variants,\nenabling reliable comparisons at far smaller sample counts. The framework\nclarifies when observed gaps are statistically meaningful (non-overlapping\ncredible intervals) versus noise, and it naturally extends to graded,\nrubric-based evaluations. Together, these results recommend replacing Pass$@k$\nfor LLM evaluation and ranking with a posterior-based, compute-efficient\nprotocol that unifies binary and non-binary evaluation while making uncertainty\nexplicit. Code is available at https://mohsenhariri.github.io/bayes-kit"}
{"id": "2510.04272", "categories": ["cs.AI", "cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.04272", "abs": "https://arxiv.org/abs/2510.04272", "authors": ["Jinyang Jiang", "Jinhui Han", "Yijie Peng", "Ying Zhang"], "title": "Closing the Loop: Coordinating Inventory and Recommendation via Deep Reinforcement Learning on Multiple Timescales", "comment": null, "summary": "Effective cross-functional coordination is essential for enhancing firm-wide\nprofitability, particularly in the face of growing organizational complexity\nand scale. Recent advances in artificial intelligence, especially in\nreinforcement learning (RL), offer promising avenues to address this\nfundamental challenge. This paper proposes a unified multi-agent RL framework\ntailored for joint optimization across distinct functional modules, exemplified\nvia coordinating inventory replenishment and personalized product\nrecommendation. We first develop an integrated theoretical model to capture the\nintricate interplay between these functions and derive analytical benchmarks\nthat characterize optimal coordination. The analysis reveals synchronized\nadjustment patterns across products and over time, highlighting the importance\nof coordinated decision-making. Leveraging these insights, we design a novel\nmulti-timescale multi-agent RL architecture that decomposes policy components\naccording to departmental functions and assigns distinct learning speeds based\non task complexity and responsiveness. Our model-free multi-agent design\nimproves scalability and deployment flexibility, while multi-timescale updates\nenhance convergence stability and adaptability across heterogeneous decisions.\nWe further establish the asymptotic convergence of the proposed algorithm.\nExtensive simulation experiments demonstrate that the proposed approach\nsignificantly improves profitability relative to siloed decision-making\nframeworks, while the behaviors of the trained RL agents align closely with the\nmanagerial insights from our theoretical model. Taken together, this work\nprovides a scalable, interpretable RL-based solution to enable effective\ncross-functional coordination in complex business settings."}
{"id": "2510.04281", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04281", "abs": "https://arxiv.org/abs/2510.04281", "authors": ["Zhuangzhi Gao", "Hongyi Qin", "He Zhao", "Qinkai Yu", "Feixiang Zhou", "Eduard Shantsila", "Uazman Alam", "Alena Shantsila", "Wahbi El-Bouri", "Gregory Y. H. Lip", "Yalin Zheng"], "title": "GROK: From Quantitative Biomarkers to Qualitative Diagnosis via a Grounded MLLM with Knowledge-Guided Instruction", "comment": "9 pages, 4 figures, 3 table. Equal contribution: Zhuangzhi Gao and\n  Hongyi Qin. Corresponding author: Yalin Zheng (yzheng@liverpool.ac.uk)", "summary": "Multimodal large language models (MLLMs) hold promise for integrating diverse\ndata modalities, but current medical adaptations such as LLaVA-Med often fail\nto fully exploit the synergy between color fundus photography (CFP) and optical\ncoherence tomography (OCT), and offer limited interpretability of quantitative\nbiomarkers. We introduce GROK, a grounded multimodal large language model that\njointly processes CFP, OCT, and text to deliver clinician-grade diagnoses of\nocular and systemic disease. GROK comprises three core modules:\nKnowledge-Guided Instruction Generation, CLIP-Style OCT-Biomarker Alignment,\nand Supervised Instruction Fine-Tuning, which together establish a\nquantitative-to-qualitative diagnostic chain of thought, mirroring real\nclinical reasoning when producing detailed lesion annotations. To evaluate our\napproach, we introduce the Grounded Ophthalmic Understanding benchmark, which\ncovers six disease categories and three tasks: macro-level diagnostic\nclassification, report generation quality, and fine-grained clinical assessment\nof the generated chain of thought. Experiments show that, with only LoRA\n(Low-Rank Adaptation) fine-tuning of a 7B-parameter Qwen2 backbone, GROK\noutperforms comparable 7B and 32B baselines on both report quality and\nfine-grained clinical metrics, and even exceeds OpenAI o3. Code and data are\npublicly available in the GROK repository."}
{"id": "2510.04284", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04284", "abs": "https://arxiv.org/abs/2510.04284", "authors": ["Yunghwei Lai", "Kaiming Liu", "Ziyue Wang", "Weizhi Ma", "Yang Liu"], "title": "Doctor-R1: Mastering Clinical Inquiry with Experiential Agentic Reinforcement Learning", "comment": null, "summary": "The professionalism of a human doctor in outpatient service depends on two\ncore abilities: the ability to make accurate medical decisions and the medical\nconsultation skill to conduct strategic, empathetic patient inquiry. Existing\nLarge Language Models (LLMs) have achieved remarkable accuracy on medical\ndecision-making benchmarks. However, they often lack the ability to conduct the\nstrategic and empathetic consultation, which is essential for real-world\nclinical scenarios. To address this gap, we propose Doctor-R1, an AI doctor\nagent trained to master both of the capabilities by ask high-yield questions\nand conduct strategic multi-turn inquiry to guide decision-making. Our\nframework introduces three key components: a multi-agent interactive\nenvironment, a two-tiered reward architecture that separately optimizes\nclinical decision-making and communicative inquiry skills, and an experience\nrepository to ground policy learning in high-quality prior trajectories. We\nevaluate Doctor-R1 on OpenAI's HealthBench and MAQuE, assessed across\nmulti-facet metrics, such as communication quality, user experience, and task\naccuracy. Remarkably, Doctor-R1 surpasses state-of-the-art open-source\nspecialized LLMs by a substantial margin with higher parameter efficiency and\noutperforms powerful proprietary models. Furthermore, the human evaluations\nshow a strong preference for Doctor-R1 to generate human-preferred clinical\ndialogue, demonstrating the effectiveness of the framework."}
{"id": "2510.04311", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04311", "abs": "https://arxiv.org/abs/2510.04311", "authors": ["Bohan Tang", "Huidong Liang", "Keyue Jiang", "Xiaowen Dong"], "title": "On the Importance of Task Complexity in Evaluating LLM-Based Multi-Agent Systems", "comment": null, "summary": "Large language model multi-agent systems (LLM-MAS) offer a promising paradigm\nfor harnessing collective intelligence to achieve more advanced forms of AI\nbehaviour. While recent studies suggest that LLM-MAS can outperform LLM\nsingle-agent systems (LLM-SAS) on certain tasks, the lack of systematic\nexperimental designs limits the strength and generality of these conclusions.\nWe argue that a principled understanding of task complexity, such as the degree\nof sequential reasoning required and the breadth of capabilities involved, is\nessential for assessing the effectiveness of LLM-MAS in task solving. To this\nend, we propose a theoretical framework characterising tasks along two\ndimensions: depth, representing reasoning length, and width, representing\ncapability diversity. We theoretically examine a representative class of\nLLM-MAS, namely the multi-agent debate system, and empirically evaluate its\nperformance in both discriminative and generative tasks with varying depth and\nwidth. Theoretical and empirical results show that the benefit of LLM-MAS over\nLLM-SAS increases with both task depth and width, and the effect is more\npronounced with respect to depth. This clarifies when LLM-MAS are beneficial\nand provides a principled foundation for designing future LLM-MAS methods and\nbenchmarks."}
{"id": "2510.04371", "categories": ["cs.AI", "cs.DC", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.04371", "abs": "https://arxiv.org/abs/2510.04371", "authors": ["Naimeng Ye", "Arnav Ahuja", "Georgios Liargkovas", "Yunan Lu", "Kostis Kaffes", "Tianyi Peng"], "title": "Speculative Actions: A Lossless Framework for Faster Agentic Systems", "comment": null, "summary": "Despite growing interest in AI agents across industry and academia, their\nexecution in an environment is often slow, hampering training, evaluation, and\ndeployment. For example, a game of chess between two state-of-the-art agents\nmay take hours. A critical bottleneck is that agent behavior unfolds\nsequentially: each action requires an API call, and these calls can be\ntime-consuming. Inspired by speculative execution in microprocessors and\nspeculative decoding in LLM inference, we propose speculative actions, a\nlossless framework for general agentic systems that predicts likely actions\nusing faster models, enabling multiple steps to be executed in parallel. We\nevaluate this framework across three agentic environments: gaming, e-commerce,\nweb search, and a \"lossy\" extension for an operating systems environment. In\nall cases, speculative actions achieve substantial accuracy in next-action\nprediction (up to 55%), translating into significant reductions in end-to-end\nlatency. Moreover, performance can be further improved through stronger\nguessing models, top-K action prediction, multi-step speculation, and\nuncertainty-aware optimization, opening a promising path toward deploying\nlow-latency agentic systems in the real world."}
{"id": "2510.04373", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04373", "abs": "https://arxiv.org/abs/2510.04373", "authors": ["Hadi Nekoei", "Aman Jaiswal", "Patrice Bechard", "Oleh Shliazhko", "Orlando Marquez Ayala", "Mathieu Reymond", "Massimo Caccia", "Alexandre Drouin", "Sarath Chandar", "Alexandre Lacoste"], "title": "Just-in-time Episodic Feedback Hinter: Leveraging Offline Knowledge to Improve LLM Agents Adaptation", "comment": null, "summary": "Large language model (LLM) agents perform well in sequential decision-making\ntasks, but improving them on unfamiliar domains often requires costly online\ninteractions or fine-tuning on large expert datasets. These strategies are\nimpractical for closed-source models and expensive for open-source ones, with\nrisks of catastrophic forgetting. Offline trajectories offer reusable\nknowledge, yet demonstration-based methods struggle because raw traces are\nlong, noisy, and tied to specific tasks. We present Just-in-time Episodic\nFeedback Hinter (JEF Hinter), an agentic system that distills offline traces\ninto compact, context-aware hints. A zooming mechanism highlights decisive\nsteps in long trajectories, capturing both strategies and pitfalls. Unlike\nprior methods, JEF Hinter leverages both successful and failed trajectories,\nextracting guidance even when only failure data is available, while supporting\nparallelized hint generation and benchmark-independent prompting. At inference,\na retriever selects relevant hints for the current state, providing targeted\nguidance with transparency and traceability. Experiments on MiniWoB++,\nWorkArena-L1, and WebArena-Lite show that JEF Hinter consistently outperforms\nstrong baselines, including human- and document-based hints."}
{"id": "2510.04384", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04384", "abs": "https://arxiv.org/abs/2510.04384", "authors": ["Adam Ballew", "Jingbo Wang", "Shaogang Ren"], "title": "LLM Based Bayesian Optimization for Prompt Search", "comment": null, "summary": "Bayesian Optimization (BO) has been widely used to efficiently optimize\nexpensive black-box functions with limited evaluations. In this paper, we\ninvestigate the use of BO for prompt engineering to enhance text classification\nwith Large Language Models (LLMs). We employ an LLM-powered Gaussian Process\n(GP) as the surrogate model to estimate the performance of different prompt\ncandidates. These candidates are generated by an LLM through the expansion of a\nset of seed prompts and are subsequently evaluated using an Upper Confidence\nBound (UCB) acquisition function in conjunction with the GP posterior. The\noptimization process iteratively refines the prompts based on a subset of the\ndata, aiming to improve classification accuracy while reducing the number of\nAPI calls by leveraging the prediction uncertainty of the LLM-based GP. The\nproposed BO-LLM algorithm is evaluated on two datasets, and its advantages are\ndiscussed in detail in this paper."}
{"id": "2510.04391", "categories": ["cs.AI", "cs.CL", "cs.SI", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2510.04391", "abs": "https://arxiv.org/abs/2510.04391", "authors": ["Saurabh Ranjan", "Brian Odegaard"], "title": "Internal World Models as Imagination Networks in Cognitive Agents", "comment": null, "summary": "What is the computational objective of imagination? While classical\ninterpretations suggest imagination is useful for maximizing rewards, recent\nfindings challenge this view. In this study, we propose that imagination serves\nto access an internal world model (IWM) and use psychological network analysis\nto explore IWMs in humans and large language models (LLMs). Specifically, we\nassessed imagination vividness ratings using two questionnaires and constructed\nimagination networks from these reports. Imagination networks from human groups\nshowed correlations between different centrality measures, including expected\ninfluence, strength, and closeness. However, imagination networks from LLMs\nshowed a lack of clustering and lower correlations between centrality measures\nunder different prompts and conversational memory conditions. Together, these\nresults indicate a lack of similarity between IWMs in human and LLM agents.\nOverall, our study offers a novel method for comparing internally-generated\nrepresentations in humans and AI, providing insights for developing human-like\nimagination in artificial intelligence."}
{"id": "2510.04399", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04399", "abs": "https://arxiv.org/abs/2510.04399", "authors": ["Charles L. Wang", "Keir Dorchen", "Peter Jin"], "title": "Utility-Learning Tension in Self-Modifying Agents", "comment": null, "summary": "As systems trend toward superintelligence, a natural modeling premise is that\nagents can self-improve along every facet of their own design. We formalize\nthis with a five-axis decomposition and a decision layer, separating incentives\nfrom learning behavior and analyzing axes in isolation. Our central result\nidentifies and introduces a sharp utility--learning tension, the structural\nconflict in self-modifying systems whereby utility-driven changes that improve\nimmediate or expected performance can also erode the statistical preconditions\nfor reliable learning and generalization. Our findings show that\ndistribution-free guarantees are preserved iff the policy-reachable model\nfamily is uniformly capacity-bounded; when capacity can grow without limit,\nutility-rational self-changes can render learnable tasks unlearnable. Under\nstandard assumptions common in practice, these axes reduce to the same capacity\ncriterion, yielding a single boundary for safe self-modification. Numerical\nexperiments across several axes validate the theory by comparing destructive\nutility policies against our proposed two-gate policies that preserve\nlearnability."}
{"id": "2510.04474", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04474", "abs": "https://arxiv.org/abs/2510.04474", "authors": ["Gang Li", "Yan Chen", "Ming Lin", "Tianbao Yang"], "title": "DRPO: Efficient Reasoning via Decoupled Reward Policy Optimization", "comment": "20 pages, 7 figures", "summary": "Recent large reasoning models (LRMs) driven by reinforcement learning\nalgorithms (e.g., GRPO) have achieved remarkable performance on challenging\nreasoning tasks. However, these models suffer from overthinking, generating\nunnecessarily long and redundant reasoning even for simple questions, which\nsubstantially increases computational cost and response latency. While existing\nmethods incorporate length rewards to GRPO to promote concise reasoning, they\nincur significant performance degradation. We identify the root cause: when\nrewards for correct but long rollouts are penalized, GRPO's group-relative\nadvantage function can assign them negative advantages, actively discouraging\nvalid reasoning. To overcome this, we propose Decoupled Reward Policy\nOptimization (DRPO), a novel framework that decouples the length-based learning\nsignal of correct rollouts from incorrect ones. DRPO ensures that reward\nsignals for correct rollouts are normalized solely within the positive group,\nshielding them from interference by negative samples. The DRPO's objective is\ngrounded in integrating an optimized positive data distribution, which\nmaximizes length-based rewards under a KL regularization, into a discriminative\nobjective. We derive a closed-form solution for this distribution, enabling\nefficient computation of the objective and its gradients using only on-policy\ndata and importance weighting. Of independent interest, this formulation is\ngeneral and can incorporate other preference rewards of positive data beyond\nlength. Experiments on mathematical reasoning tasks demonstrate DRPO's\nsignificant superiority over six efficient reasoning baselines. Notably, with a\n1.5B model, our method achieves 77\\% length reduction with only 1.1\\%\nperformance loss on simple questions like GSM8k dataset, while the follow-up\nbaseline sacrifices 4.3\\% for 68\\% length reduction."}
{"id": "2510.04480", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04480", "abs": "https://arxiv.org/abs/2510.04480", "authors": ["Yunuo Cen", "Zixuan Wang", "Jintao Zhang", "Zhiwei Zhang", "Xuanyao Fong"], "title": "On Continuous Optimization for Constraint Satisfaction Problems", "comment": null, "summary": "Constraint satisfaction problems (CSPs) are fundamental in mathematics,\nphysics, and theoretical computer science. While conflict-driven clause\nlearning Boolean Satisfiability (SAT) solvers have achieved remarkable success\nand become the mainstream approach for Boolean satisfiability, recent advances\nshow that modern continuous local search (CLS) solvers can achieve highly\ncompetitive results on certain classes of SAT problems. Motivated by these\nadvances, we extend the CLS framework from Boolean SAT to general CSP with\nfinite-domain variables and expressive constraints. We present FourierCSP, a\ncontinuous optimization framework that generalizes the Walsh-Fourier transform\nto CSP, allowing for transforming versatile constraints to compact multilinear\npolynomials, thereby avoiding the need for auxiliary variables and\nmemory-intensive encodings. Our approach leverages efficient evaluation and\ndifferentiation of the objective via circuit-output probability and employs a\nprojected gradient optimization method with theoretical guarantees. Empirical\nresults on benchmark suites demonstrate that FourierCSP is scalable and\ncompetitive, significantly broadening the class of problems that can be\nefficiently solved by CLS techniques."}
{"id": "2510.04488", "categories": ["cs.AI", "cs.IT", "math.IT", "I.2.4"], "pdf": "https://arxiv.org/pdf/2510.04488", "abs": "https://arxiv.org/abs/2510.04488", "authors": ["Edward Y. Chang", "Ethan Y. Chang"], "title": "Multi-Agent Collaborative Intelligence: Dual-Dial Control for Reliable LLM Reasoning", "comment": "27 pages, 5 figures, 21 tables", "summary": "Multi-agent debate often wastes compute by using a fixed adversarial stance,\naggregating without deliberation, or stopping on heuristics. We introduce MACI,\nan active controller with two independent dials that decouple information from\nbehavior: an information dial that gates evidence by quality, and a behavior\ndial that schedules contentiousness from exploration to consolidation. A\nmoderator tracks disagreement, overlap, evidence quality, and argument quality,\nand halts when gains plateau. We provide theory-lite guarantees for\nnonincreasing dispersion and provable termination, with a budget-feasible\nscheduler. Across clinical diagnosis and news-bias tasks, MACI improves\naccuracy and calibration while reducing tokens, and converts residual\nuncertainty into precision RAG plans that specify what to retrieve next. We use\na cross-family LLM judge (CRIT) as a conservative soft weight and stop signal,\nvalidated for order invariance and judge-swap stability; stability depends on\nusing high-capability judges. MACI turns debate into a budget-aware,\nmeasurable, and provably terminating controller."}
{"id": "2510.04491", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.04491", "abs": "https://arxiv.org/abs/2510.04491", "authors": ["Muyu He", "Anand Kumar", "Tsach Mackey", "Meghana Rajeev", "James Zou", "Nazneen Rajani"], "title": "Impatient Users Confuse AI Agents: High-fidelity Simulations of Human Traits for Testing Agents", "comment": "25 pages", "summary": "Despite rapid progress in building conversational AI agents, robustness is\nstill largely untested. Small shifts in user behavior, such as being more\nimpatient, incoherent, or skeptical, can cause sharp drops in agent\nperformance, revealing how brittle current AI agents are. Today's benchmarks\nfail to capture this fragility: agents may perform well under standard\nevaluations but degrade spectacularly in more realistic and varied settings. We\naddress this robustness testing gap by introducing TraitBasis, a lightweight,\nmodel-agnostic method for systematically stress testing AI agents. TraitBasis\nlearns directions in activation space corresponding to steerable user traits\n(e.g., impatience or incoherence), which can be controlled, scaled, composed,\nand applied at inference time without any fine-tuning or extra data. Using\nTraitBasis, we extend $\\tau$-Bench to $\\tau$-Trait, where user behaviors are\naltered via controlled trait vectors. We observe on average a 2%-30%\nperformance degradation on $\\tau$-Trait across frontier models, highlighting\nthe lack of robustness of current AI agents to variations in user behavior.\nTogether, these results highlight both the critical role of robustness testing\nand the promise of TraitBasis as a simple, data-efficient, and compositional\ntool. By powering simulation-driven stress tests and training loops, TraitBasis\nopens the door to building AI agents that remain reliable in the unpredictable\ndynamics of real-world human interactions. We have open-sourced $\\tau$-Trai\nacross four domains: airline, retail, telecom, and telehealth, so the community\ncan systematically QA their agents under realistic, behaviorally diverse\nintents and trait scenarios: https://github.com/collinear-ai/tau-trait."}
{"id": "2510.04514", "categories": ["cs.AI", "cs.CE", "cs.CL", "cs.CV", "stat.ME"], "pdf": "https://arxiv.org/pdf/2510.04514", "abs": "https://arxiv.org/abs/2510.04514", "authors": ["Rachneet Kaur", "Nishan Srishankar", "Zhen Zeng", "Sumitra Ganesh", "Manuela Veloso"], "title": "ChartAgent: A Multimodal Agent for Visually Grounded Reasoning in Complex Chart Question Answering", "comment": "53 pages, 12 figures, 15 tables", "summary": "Recent multimodal LLMs have shown promise in chart-based visual question\nanswering, but their performance declines sharply on unannotated charts, those\nrequiring precise visual interpretation rather than relying on textual\nshortcuts. To address this, we introduce ChartAgent, a novel agentic framework\nthat explicitly performs visual reasoning directly within the chart's spatial\ndomain. Unlike textual chain-of-thought reasoning, ChartAgent iteratively\ndecomposes queries into visual subtasks and actively manipulates and interacts\nwith chart images through specialized actions such as drawing annotations,\ncropping regions (e.g., segmenting pie slices, isolating bars), and localizing\naxes, using a library of chart-specific vision tools to fulfill each subtask.\nThis iterative reasoning process closely mirrors human cognitive strategies for\nchart comprehension. ChartAgent achieves state-of-the-art accuracy on the\nChartBench and ChartX benchmarks, surpassing prior methods by up to 16.07%\nabsolute gain overall and 17.31% on unannotated, numerically intensive queries.\nFurthermore, our analyses show that ChartAgent is (a) effective across diverse\nchart types, (b) achieve the highest scores across varying visual and reasoning\ncomplexity levels, and (c) serves as a plug-and-play framework that boosts\nperformance across diverse underlying LLMs. Our work is among the first to\ndemonstrate visually grounded reasoning for chart understanding using\ntool-augmented multimodal agents."}
{"id": "2510.04520", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04520", "abs": "https://arxiv.org/abs/2510.04520", "authors": ["Hanyu Wang", "Ruohan Xie", "Yutong Wang", "Guoxiong Gao", "Xintao Yu", "Bin Dong"], "title": "Aria: An Agent For Retrieval and Iterative Auto-Formalization via Dependency Graph", "comment": null, "summary": "Accurate auto-formalization of theorem statements is essential for advancing\nautomated discovery and verification of research-level mathematics, yet remains\na major bottleneck for LLMs due to hallucinations, semantic mismatches, and\ntheir inability to synthesize new definitions. To tackle these issues, we\npresent Aria (Agent for Retrieval and Iterative Autoformalization), a system\nfor conjecture-level formalization in Lean that emulates human expert reasoning\nvia a two-phase Graph-of-Thought process: recursively decomposing statements\ninto a dependency graph and then constructing formalizations from grounded\nconcepts. To ensure semantic correctness, we introduce AriaScorer, a checker\nthat retrieves definitions from Mathlib for term-level grounding, enabling\nrigorous and reliable verification. We evaluate Aria on diverse benchmarks. On\nProofNet, it achieves 91.6% compilation success rate and 68.5% final accuracy,\nsurpassing previous methods. On FATE-X, a suite of challenging algebra problems\nfrom research literature, it outperforms the best baseline with 44.0% vs. 24.0%\nfinal accuracy. On a dataset of homological conjectures, Aria reaches 42.9%\nfinal accuracy while all other models score 0%."}
{"id": "2510.04532", "categories": ["cs.AI", "cs.CL", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.04532", "abs": "https://arxiv.org/abs/2510.04532", "authors": ["Xurui Song", "Shuo Huai", "JingJing Jiang", "Jiayi Kong", "Jun Luo"], "title": "More Than Meets the Eye? Uncovering the Reasoning-Planning Disconnect in Training Vision-Language Driving Models", "comment": "The dataset will be released publicly once the paper is accepted for\n  publication", "summary": "Vision-Language Model (VLM) driving agents promise explainable end-to-end\nautonomy by first producing natural-language reasoning and then predicting\ntrajectory planning. However, whether planning is causally driven by this\nreasoning remains a critical but unverified assumption. To investigate this, we\nbuild DriveMind, a large-scale driving Visual Question Answering (VQA) corpus\nwith plan-aligned Chain-of-Thought (CoT), automatically generated from nuPlan.\nOur data generation process converts sensors and annotations into structured\ninputs and, crucially, separates priors from to-be-reasoned signals, enabling\nclean information ablations. Using DriveMind, we train representative VLM\nagents with Supervised Fine-Tuning (SFT) and Group Relative Policy Optimization\n(GRPO) and evaluate them with nuPlan's metrics. Our results, unfortunately,\nindicate a consistent causal disconnect in reasoning-planning: removing\nego/navigation priors causes large drops in planning scores, whereas removing\nCoT produces only minor changes. Attention analysis further shows that planning\nprimarily focuses on priors rather than the CoT. Based on this evidence, we\npropose the Reasoning-Planning Decoupling Hypothesis, positing that the\ntraining-yielded reasoning is an ancillary byproduct rather than a causal\nmediator. To enable efficient diagnosis, we also introduce a novel,\ntraining-free probe that measures an agent's reliance on priors by evaluating\nits planning robustness against minor input perturbations. In summary, we\nprovide the community with a new dataset and a diagnostic tool to evaluate the\ncausal fidelity of future models."}
{"id": "2510.04542", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04542", "abs": "https://arxiv.org/abs/2510.04542", "authors": ["Wolfgang Lehrach", "Daniel Hennes", "Miguel Lazaro-Gredilla", "Xinghua Lou", "Carter Wendelken", "Zun Li", "Antoine Dedieu", "Jordi Grau-Moya", "Marc Lanctot", "Atil Iscen", "John Schultz", "Marcus Chiam", "Ian Gemp", "Piotr Zielinski", "Satinder Singh", "Kevin P. Murphy"], "title": "Code World Models for General Game Playing", "comment": null, "summary": "Large Language Models (LLMs) reasoning abilities are increasingly being\napplied to classical board and card games, but the dominant approach --\ninvolving prompting for direct move generation -- has significant drawbacks. It\nrelies on the model's implicit fragile pattern-matching capabilities, leading\nto frequent illegal moves and strategically shallow play. Here we introduce an\nalternative approach: We use the LLM to translate natural language rules and\ngame trajectories into a formal, executable world model represented as Python\ncode. This generated model -- comprising functions for state transition, legal\nmove enumeration, and termination checks -- serves as a verifiable simulation\nengine for high-performance planning algorithms like Monte Carlo tree search\n(MCTS). In addition, we prompt the LLM to generate heuristic value functions\n(to make MCTS more efficient), and inference functions (to estimate hidden\nstates in imperfect information games). Our method offers three distinct\nadvantages compared to directly using the LLM as a policy: (1) Verifiability:\nThe generated CWM serves as a formal specification of the game's rules,\nallowing planners to algorithmically enumerate valid actions and avoid illegal\nmoves, contingent on the correctness of the synthesized model; (2) Strategic\nDepth: We combine LLM semantic understanding with the deep search power of\nclassical planners; and (3) Generalization: We direct the LLM to focus on the\nmeta-task of data-to-code translation, enabling it to adapt to new games more\neasily. We evaluate our agent on 10 different games, of which 4 are novel and\ncreated for this paper. 5 of the games are fully observed (perfect\ninformation), and 5 are partially observed (imperfect information). We find\nthat our method outperforms or matches Gemini 2.5 Pro in 9 out of the 10\nconsidered games."}
{"id": "2510.04550", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04550", "abs": "https://arxiv.org/abs/2510.04550", "authors": ["Pengfei He", "Zhenwei Dai", "Bing He", "Hui Liu", "Xianfeng Tang", "Hanqing Lu", "Juanhui Li", "Jiayuan Ding", "Subhabrata Mukherjee", "Suhang Wang", "Yue Xing", "Jiliang Tang", "Benoit Dumoulin"], "title": "TRAJECT-Bench:A Trajectory-Aware Benchmark for Evaluating Agentic Tool Use", "comment": null, "summary": "Large language model (LLM)-based agents increasingly rely on tool use to\ncomplete real-world tasks. While existing works evaluate the LLMs' tool use\ncapability, they largely focus on the final answers yet overlook the detailed\ntool usage trajectory, i.e., whether tools are selected, parameterized, and\nordered correctly. We introduce TRAJECT-Bench, a trajectory-aware benchmark to\ncomprehensively evaluate LLMs' tool use capability through diverse tasks with\nfine-grained evaluation metrics. TRAJECT-Bench pairs high-fidelity, executable\ntools across practical domains with tasks grounded in production-style APIs,\nand synthesizes trajectories that vary in breadth (parallel calls) and depth\n(interdependent chains). Besides final accuracy, TRAJECT-Bench also reports\ntrajectory-level diagnostics, including tool selection and argument\ncorrectness, and dependency/order satisfaction. Analyses reveal failure modes\nsuch as similar tool confusion and parameter-blind selection, and scaling\nbehavior with tool diversity and trajectory length where the bottleneck of\ntransiting from short to mid-length trajectories is revealed, offering\nactionable guidance for LLMs' tool use."}
{"id": "2510.04560", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04560", "abs": "https://arxiv.org/abs/2510.04560", "authors": ["Honghao Fu", "Yuan Ouyang", "Kai-Wei Chang", "Yiwei Wang", "Zi Huang", "Yujun Cai"], "title": "ContextNav: Towards Agentic Multimodal In-Context Learning", "comment": null, "summary": "Recent advances demonstrate that multimodal large language models (MLLMs)\nexhibit strong multimodal in-context learning (ICL) capabilities, enabling them\nto adapt to novel vision-language tasks from a few contextual examples.\nHowever, existing ICL approaches face challenges in reconciling scalability\nwith robustness across diverse tasks and noisy contextual examples: manually\nselecting examples produces clean contexts but is labor-intensive and\ntask-specific, while similarity-based retrieval improves scalability but could\nintroduce irrelevant or structurally inconsistent samples that degrade ICL\nperformance. To address these limitations, we propose ContextNav, the first\nagentic framework that integrates the scalability of automated retrieval with\nthe quality and adaptiveness of human-like curation, enabling noise-robust and\ndynamically optimized contextualization for multimodal ICL. ContextNav unifies\ncontext management and noise-robust contextualization within a closed-loop\nworkflow driven by graph-based orchestration. Specifically, it builds a\nresource-aware multimodal embedding pipeline, maintains a retrievable vector\ndatabase, and applies agentic retrieval and structural alignment to construct\nnoise-resilient contexts. An Operational Grammar Graph (OGG) further supports\nadaptive workflow planning and optimization, enabling the agent to refine its\noperational strategies based on downstream ICL feedback. Experimental results\ndemonstrate that ContextNav achieves state-of-the-art performance across\nvarious datasets, underscoring the promise of agentic workflows for advancing\nscalable and robust contextualization in multimodal ICL."}
{"id": "2510.04568", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04568", "abs": "https://arxiv.org/abs/2510.04568", "authors": ["Naman Gupta", "Shreeyash Gowaikar", "Arun Iyer", "Kirankumar Shiragur", "Ramakrishna B Bairi", "Rishikesh Maurya", "Ritabrata Maiti", "Sankarshan Damle", "Shachee Mishra Gupta"], "title": "COSMIR: Chain Orchestrated Structured Memory for Iterative Reasoning over Long Context", "comment": null, "summary": "Reasoning over very long inputs remains difficult for large language models\n(LLMs). Common workarounds either shrink the input via retrieval (risking\nmissed evidence), enlarge the context window (straining selectivity), or stage\nmultiple agents to read in pieces. In staged pipelines (e.g., Chain of Agents,\nCoA), free-form summaries passed between agents can discard crucial details and\namplify early mistakes. We introduce COSMIR (Chain Orchestrated Structured\nMemory for Iterative Reasoning), a chain-style framework that replaces ad hoc\nmessages with a structured memory. A Planner agent first turns a user query\ninto concrete, checkable sub-questions. worker agents process chunks via a\nfixed micro-cycle: Extract, Infer, Refine, writing all updates to the shared\nmemory. A Manager agent then Synthesizes the final answer directly from the\nmemory. This preserves step-wise read-then-reason benefits while changing both\nthe communication medium (structured memory) and the worker procedure (fixed\nmicro-cycle), yielding higher faithfulness, better long-range aggregation, and\nauditability. On long-context QA from the HELMET suite, COSMIR reduces\npropagation-stage information loss and improves accuracy over a CoA baseline."}
{"id": "2510.04580", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04580", "abs": "https://arxiv.org/abs/2510.04580", "authors": ["Tomoyuki Kaneko", "Shuhei Yamashita"], "title": "Strongly Solving 2048 4x3", "comment": null, "summary": "2048 is a stochastic single-player game involving 16 cells on a 4 by 4 grid,\nwhere a player chooses a direction among up, down, left, and right to obtain a\nscore by merging two tiles with the same number located in neighboring cells\nalong the chosen direction. This paper presents that a variant 2048-4x3 12\ncells on a 4 by 3 board, one row smaller than the original, has been strongly\nsolved. In this variant, the expected score achieved by an optimal strategy is\nabout $50724.26$ for the most common initial states: ones with two tiles of\nnumber 2. The numbers of reachable states and afterstates are identified to be\n$1,152,817,492,752$ and $739,648,886,170$, respectively. The key technique is\nto partition state space by the sum of tile numbers on a board, which we call\nthe age of a state. An age is invariant between a state and its successive\nafterstate after any valid action and is increased two or four by stochastic\nresponse from the environment. Therefore, we can partition state space by ages\nand enumerate all (after)states of an age depending only on states with the\nrecent ages. Similarly, we can identify (after)state values by going along with\nages in decreasing order."}
{"id": "2510.04588", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04588", "abs": "https://arxiv.org/abs/2510.04588", "authors": ["Shurui Li"], "title": "Perfect AI Mimicry and the Epistemology of Consciousness: A Solipsistic Dilemma", "comment": null, "summary": "Rapid advances in artificial intelligence necessitate a re-examination of the\nepistemological foundations upon which we attribute consciousness. As AI\nsystems increasingly mimic human behavior and interaction with high fidelity,\nthe concept of a \"perfect mimic\"-an entity empirically indistinguishable from a\nhuman through observation and interaction-shifts from hypothetical to\ntechnologically plausible. This paper argues that such developments pose a\nfundamental challenge to the consistency of our mind-recognition practices.\nConsciousness attributions rely heavily, if not exclusively, on empirical\nevidence derived from behavior and interaction. If a perfect mimic provides\nevidence identical to that of humans, any refusal to grant it equivalent\nepistemic status must invoke inaccessible factors, such as qualia, substrate\nrequirements, or origin. Selectively invoking such factors risks a debilitating\ndilemma: either we undermine the rational basis for attributing consciousness\nto others (epistemological solipsism), or we accept inconsistent reasoning. I\ncontend that epistemic consistency demands we ascribe the same status to\nempirically indistinguishable entities, regardless of metaphysical assumptions.\nThe perfect mimic thus acts as an epistemic mirror, forcing critical reflection\non the assumptions underlying intersubjective recognition in light of advancing\nAI. This analysis carries significant implications for theories of\nconsciousness and ethical frameworks concerning artificial agents."}
{"id": "2510.04617", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04617", "abs": "https://arxiv.org/abs/2510.04617", "authors": ["Zhejian Lai", "Xiang Geng", "Zhijun Wang", "Yang Bai", "Jiahuan Li", "Rongxiang Weng", "Jingang Wang", "Xuezhi Cao", "Xunliang Cai", "Shujian Huang"], "title": "Making Mathematical Reasoning Adaptive", "comment": null, "summary": "Mathematical reasoning is a primary indicator of large language models (LLMs)\nintelligence. However, existing LLMs exhibit failures of robustness and\ngeneralization. This paper attributes these deficiencies to spurious reasoning,\ni.e., producing answers from superficial features. To address this challenge,\nwe propose the AdaR framework to enable adaptive reasoning, wherein models rely\non problem-solving logic to produce answers. AdaR synthesizes logically\nequivalent queries by varying variable values, and trains models with RLVR on\nthese data to penalize spurious logic while encouraging adaptive logic. To\nimprove data quality, we extract the problem-solving logic from the original\nquery and generate the corresponding answer by code execution, then apply a\nsanity check. Experimental results demonstrate that AdaR improves robustness\nand generalization, achieving substantial improvement in mathematical reasoning\nwhile maintaining high data efficiency. Analysis indicates that data synthesis\nand RLVR function in a coordinated manner to enable adaptive reasoning in LLMs.\nSubsequent analyses derive key design insights into the effect of critical\nfactors and the applicability to instruct LLMs. Our project is available at\nhttps://github.com/LaiZhejian/AdaR"}
{"id": "2510.04623", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04623", "abs": "https://arxiv.org/abs/2510.04623", "authors": ["Shrish Shrinath Vaidya", "Gowthamaan Palani", "Sidharth Ramesh", "Velmurugan Balasubramanian", "Minmini Selvam", "Gokulraja Srinivasaraja", "Ganapathy Krishnamurthi"], "title": "MedPAO: A Protocol-Driven Agent for Structuring Medical Reports", "comment": "Paper published at \"Agentic AI for Medicine\" Workshop, MICCAI 2025", "summary": "The deployment of Large Language Models (LLMs) for structuring clinical data\nis critically hindered by their tendency to hallucinate facts and their\ninability to follow domain-specific rules. To address this, we introduce\nMedPAO, a novel agentic framework that ensures accuracy and verifiable\nreasoning by grounding its operation in established clinical protocols such as\nthe ABCDEF protocol for CXR analysis. MedPAO decomposes the report structuring\ntask into a transparent process managed by a Plan-Act-Observe (PAO) loop and\nspecialized tools. This protocol-driven method provides a verifiable\nalternative to opaque, monolithic models. The efficacy of our approach is\ndemonstrated through rigorous evaluation: MedPAO achieves an F1-score of 0.96\non the critical sub-task of concept categorization. Notably, expert\nradiologists and clinicians rated the final structured outputs with an average\nscore of 4.52 out of 5, indicating a level of reliability that surpasses\nbaseline approaches relying solely on LLM-based foundation models. The code is\navailable at: https://github.com/MiRL-IITM/medpao-agent"}
{"id": "2510.04643", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04643", "abs": "https://arxiv.org/abs/2510.04643", "authors": ["Xiangyu Li", "Yawen Zeng", "Xiaofen Xing", "Jin Xu", "Xiangmin Xu"], "title": "QuantAgents: Towards Multi-agent Financial System via Simulated Trading", "comment": "This paper has been accepted by EMNLP 2025", "summary": "In this paper, our objective is to develop a multi-agent financial system\nthat incorporates simulated trading, a technique extensively utilized by\nfinancial professionals. While current LLM-based agent models demonstrate\ncompetitive performance, they still exhibit significant deviations from\nreal-world fund companies. A critical distinction lies in the agents' reliance\non ``post-reflection'', particularly in response to adverse outcomes, but lack\na distinctly human capability: long-term prediction of future trends.\nTherefore, we introduce QuantAgents, a multi-agent system integrating simulated\ntrading, to comprehensively evaluate various investment strategies and market\nscenarios without assuming actual risks. Specifically, QuantAgents comprises\nfour agents: a simulated trading analyst, a risk control analyst, a market news\nanalyst, and a manager, who collaborate through several meetings. Moreover, our\nsystem incentivizes agents to receive feedback on two fronts: performance in\nreal-world markets and predictive accuracy in simulated trading. Extensive\nexperiments demonstrate that our framework excels across all metrics, yielding\nan overall return of nearly 300% over the three years\n(https://quantagents.github.io/)."}
{"id": "2510.04670", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04670", "abs": "https://arxiv.org/abs/2510.04670", "authors": ["Xuanhua Yin", "Runkai Zhao", "Weidong Cai"], "title": "Improving Multimodal Brain Encoding Model with Dynamic Subject-awareness Routing", "comment": "8 pages, 4 figures", "summary": "Naturalistic fMRI encoding must handle multimodal inputs, shifting fusion\nstyles, and pronounced inter-subject variability. We introduce AFIRE (Agnostic\nFramework for Multimodal fMRI Response Encoding), an agnostic interface that\nstandardizes time-aligned post-fusion tokens from varied encoders, and MIND, a\nplug-and-play Mixture-of-Experts decoder with a subject-aware dynamic gating.\nTrained end-to-end for whole-brain prediction, AFIRE decouples the decoder from\nupstream fusion, while MIND combines token-dependent Top-K sparse routing with\na subject prior to personalize expert usage without sacrificing generality.\nExperiments across multiple multimodal backbones and subjects show consistent\nimprovements over strong baselines, enhanced cross-subject generalization, and\ninterpretable expert patterns that correlate with content type. The framework\noffers a simple attachment point for new encoders and datasets, enabling\nrobust, plug-and-improve performance for naturalistic neuroimaging studies."}
{"id": "2510.04673", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04673", "abs": "https://arxiv.org/abs/2510.04673", "authors": ["Chan Hee Song", "Yiwen Song", "Palash Goyal", "Yu Su", "Oriana Riva", "Hamid Palangi", "Tomas Pfister"], "title": "Watch and Learn: Learning to Use Computers from Online Videos", "comment": null, "summary": "Computer use agents (CUAs) need to plan task workflows grounded in diverse,\never-changing applications and environments, but learning is hindered by the\nscarcity of large-scale, high-quality training data in the target application.\nExisting datasets are domain-specific, static, and costly to annotate, while\ncurrent synthetic data generation methods often yield simplistic or misaligned\ntask demonstrations. To address these limitations, we introduce Watch & Learn\n(W&L), a framework that converts human demonstration videos readily available\non the Internet into executable UI trajectories at scale. Instead of directly\ngenerating trajectories or relying on ad hoc reasoning heuristics, we cast the\nproblem as an inverse dynamics objective: predicting the user's action from\nconsecutive screen states. This formulation reduces manual engineering, is\neasier to learn, and generalizes more robustly across applications. Concretely,\nwe develop an inverse dynamics labeling pipeline with task-aware video\nretrieval, generate over 53k high-quality trajectories from raw web videos, and\ndemonstrate that these trajectories improve CUAs both as in-context\ndemonstrations and as supervised training data. On the challenging OSWorld\nbenchmark, UI trajectories extracted with W&L consistently enhance both\ngeneral-purpose and state-of-the-art frameworks in-context, and deliver\nstronger gains for open-source models under supervised training. These results\nhighlight web-scale human demonstration videos as a practical and scalable\nfoundation for advancing CUAs towards real-world deployment."}
{"id": "2510.04695", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04695", "abs": "https://arxiv.org/abs/2510.04695", "authors": ["Yiding Wang", "Zhepei Wei", "Xinyu Zhu", "Yu Meng"], "title": "Beyond Outcome Reward: Decoupling Search and Answering Improves LLM Agents", "comment": null, "summary": "Enabling large language models (LLMs) to utilize search tools offers a\npromising path to overcoming fundamental limitations such as knowledge cutoffs\nand hallucinations. Recent work has explored reinforcement learning (RL) for\ntraining search-augmented agents that interleave reasoning and retrieval before\nanswering. These approaches usually rely on outcome-based rewards (e.g., exact\nmatch), implicitly assuming that optimizing for final answers will also yield\neffective intermediate search behaviors. Our analysis challenges this\nassumption: we uncover multiple systematic deficiencies in search that arise\nunder outcome-only training and ultimately degrade final answer quality,\nincluding failure to invoke tools, invalid queries, and redundant searches. To\naddress these shortcomings, we introduce DeSA (Decoupling\nSearch-and-Answering), a simple two-stage training framework that explicitly\nseparates search optimization from answer generation. In Stage 1, agents are\ntrained to improve search effectiveness with retrieval recall-based rewards. In\nStage 2, outcome rewards are employed to optimize final answer generation.\nAcross seven QA benchmarks, DeSA-trained agents consistently improve search\nbehaviors, delivering substantially higher search recall and answer accuracy\nthan outcome-only baselines. Notably, DeSA outperforms single-stage training\napproaches that simultaneously optimize recall and outcome rewards,\nunderscoring the necessity of explicitly decoupling the two objectives."}
{"id": "2510.04721", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04721", "abs": "https://arxiv.org/abs/2510.04721", "authors": ["Ivo Petrov", "Jasper Dekoninck", "Martin Vechev"], "title": "BrokenMath: A Benchmark for Sycophancy in Theorem Proving with LLMs", "comment": null, "summary": "Large language models (LLMs) have recently shown strong performance on\nmathematical benchmarks. At the same time, they are prone to hallucination and\nsycophancy, often providing convincing but flawed proofs for incorrect\nmathematical statements provided by users. This significantly limits the\napplicability of LLMs in theorem proving, as verification of these flawed\nproofs must be done manually by expert mathematicians. However, existing\nbenchmarks that measure sycophancy in mathematics are limited: they focus\nsolely on final-answer problems, rely on very simple and often contaminated\ndatasets, and construct benchmark samples using synthetic modifications that\ncreate ill-posed questions rather than well-posed questions that are\ndemonstrably false. To address these issues, we introduce BrokenMath, the first\nbenchmark for evaluating sycophantic behavior in LLMs within the context of\nnatural language theorem proving. BrokenMath is built from advanced 2025\ncompetition problems, which are perturbed with an LLM to produce false\nstatements and subsequently refined through expert review. Using an\nLLM-as-a-judge framework, we evaluate state-of-the-art LLMs and agentic systems\nand find that sycophancy is widespread, with the best model, GPT-5, producing\nsycophantic answers 29% of the time. We further investigate several mitigation\nstrategies, including test-time interventions and supervised fine-tuning on\ncurated sycophantic examples. These approaches substantially reduce, but do not\neliminate, sycophantic behavior."}
{"id": "2510.04765", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04765", "abs": "https://arxiv.org/abs/2510.04765", "authors": ["Jinbo Wen", "Jiawen Kang", "Linfeng Zhang", "Xiaoying Tang", "Jianhang Tang", "Yang Zhang", "Zhaohui Yang", "Dusit Niyato"], "title": "LMM-Incentive: Large Multimodal Model-based Incentive Design for User-Generated Content in Web 3.0", "comment": null, "summary": "Web 3.0 represents the next generation of the Internet, which is widely\nrecognized as a decentralized ecosystem that focuses on value expression and\ndata ownership. By leveraging blockchain and artificial intelligence\ntechnologies, Web 3.0 offers unprecedented opportunities for users to create,\nown, and monetize their content, thereby enabling User-Generated Content (UGC)\nto an entirely new level. However, some self-interested users may exploit the\nlimitations of content curation mechanisms and generate low-quality content\nwith less effort, obtaining platform rewards under information asymmetry. Such\nbehavior can undermine Web 3.0 performance. To this end, we propose\n\\textit{LMM-Incentive}, a novel Large Multimodal Model (LMM)-based incentive\nmechanism for UGC in Web 3.0. Specifically, we propose an LMM-based\ncontract-theoretic model to motivate users to generate high-quality UGC,\nthereby mitigating the adverse selection problem from information asymmetry. To\nalleviate potential moral hazards after contract selection, we leverage LMM\nagents to evaluate UGC quality, which is the primary component of the contract,\nutilizing prompt engineering techniques to improve the evaluation performance\nof LMM agents. Recognizing that traditional contract design methods cannot\neffectively adapt to the dynamic environment of Web 3.0, we develop an improved\nMixture of Experts (MoE)-based Proximal Policy Optimization (PPO) algorithm for\noptimal contract design. Simulation results demonstrate the superiority of the\nproposed MoE-based PPO algorithm over representative benchmarks in the context\nof contract design. Finally, we deploy the designed contract within an Ethereum\nsmart contract framework, further validating the effectiveness of the proposed\nscheme."}
{"id": "2510.04792", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04792", "abs": "https://arxiv.org/abs/2510.04792", "authors": ["Ni Zhang", "Zhiguang Cao"], "title": "Hybrid-Balance GFlowNet for Solving Vehicle Routing Problems", "comment": "Accepted by NeurIPS 2025", "summary": "Existing GFlowNet-based methods for vehicle routing problems (VRPs) typically\nemploy Trajectory Balance (TB) to achieve global optimization but often neglect\nimportant aspects of local optimization. While Detailed Balance (DB) addresses\nlocal optimization more effectively, it alone falls short in solving VRPs,\nwhich inherently require holistic trajectory optimization. To address these\nlimitations, we introduce the Hybrid-Balance GFlowNet (HBG) framework, which\nuniquely integrates TB and DB in a principled and adaptive manner by aligning\ntheir intrinsically complementary strengths. Additionally, we propose a\nspecialized inference strategy for depot-centric scenarios like the Capacitated\nVehicle Routing Problem (CVRP), leveraging the depot node's greater flexibility\nin selecting successors. Despite this specialization, HBG maintains broad\napplicability, extending effectively to problems without explicit depots, such\nas the Traveling Salesman Problem (TSP). We evaluate HBG by integrating it into\ntwo established GFlowNet-based solvers, i.e., AGFN and GFACS, and demonstrate\nconsistent and significant improvements across both CVRP and TSP, underscoring\nthe enhanced solution quality and generalization afforded by our approach."}
{"id": "2510.04817", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04817", "abs": "https://arxiv.org/abs/2510.04817", "authors": ["Abhinav Madahar"], "title": "Natural Language Edge Labelling: Decoupling Intent from Execution in Structured LM Reasoning", "comment": null, "summary": "Controllers for structured LM reasoning (e.g., Chain-of-Thought,\nself-consistency, and Tree-of-Thoughts) often entangle what to try next with\nhow to execute it, exposing only coarse global knobs and yielding brittle,\ncompute-inefficient, and hard-to-audit behavior. We introduce Natural Language\nEdge Labelling (NLEL), a labeller-tuner overlay that attaches a free-form\nnatural-language directive to each search edge and translates it into a\nschema-bounded control vector for decoding, search (branch quotas, exploration\n$\\beta$), generation bundle size, retrieval mixtures, and verification passes.\nA labeller $\\Lambda$ emits labels from the parent state and a compact context;\na tuner $\\Psi$ maps $(P, L, C)\\to \\Pi$, with strict schema validation and\ntrust-region projection around safe defaults. Downstream selection remains\nToT-style with score $S=\\mu+\\beta\\sigma$ and depth-annealed $\\beta$. We show\nNLEL strictly generalizes CoT/ToT, prove an anytime-monotonicity property for\ntop-$k$ selection under label-conditioned bundles, and bound selector shortfall\nby control-vector distortion, providing decision-relevant justification for\nguards like trust regions and verification passes. We instantiate $\\Psi$ as a\nprompt-only JSON Parameter Emitter and preregister an evaluation on GSM8K, MATH\n(subset), StrategyQA, and ARC-Challenge with compute-aware reporting\n(success@compute, tokens-per-success) and ablations over $\\Lambda$, $\\Psi$,\ntrust-region radius, and control quantization; preregistered forecasts\nanticipate accuracy gains at comparable token budgets and improved\nsuccess@compute under constraints. NLEL offers an interpretable, model-agnostic\ninterface that separates intent from execution for controllable, auditable LM\ninference."}
{"id": "2510.04851", "categories": ["cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.04851", "abs": "https://arxiv.org/abs/2510.04851", "authors": ["Dongge Han", "Camille Couturier", "Daniel Madrigal Diaz", "Xuchao Zhang", "Victor Rühle", "Saravan Rajmohan"], "title": "LEGOMem: Modular Procedural Memory for Multi-agent LLM Systems for Workflow Automation", "comment": null, "summary": "We introduce LEGOMem, a modular procedural memory framework for multi-agent\nlarge language model (LLM) systems in workflow automation. LEGOMem decomposes\npast task trajectories into reusable memory units and flexibly allocates them\nacross orchestrators and task agents to support planning and execution. To\nexplore the design space of memory in multi-agent systems, we use LEGOMem as a\nlens and conduct a systematic study of procedural memory in multi-agent\nsystems, examining where memory should be placed, how it should be retrieved,\nand which agents benefit most. Experiments on the OfficeBench benchmark show\nthat orchestrator memory is critical for effective task decomposition and\ndelegation, while fine-grained agent memory improves execution accuracy. We\nfind that even teams composed of smaller language models can benefit\nsubstantially from procedural memory, narrowing the performance gap with\nstronger agents by leveraging prior execution traces for more accurate planning\nand tool use. These results position LEGOMem as both a practical framework for\nmemory-augmented agent systems and a research tool for understanding memory\ndesign in multi-agent workflow automation."}
{"id": "2510.04862", "categories": ["cs.AI", "cs.LG", "cs.MA", "cs.NE"], "pdf": "https://arxiv.org/pdf/2510.04862", "abs": "https://arxiv.org/abs/2510.04862", "authors": ["Sam Earle", "Zehua Jiang", "Eugene Vinitsky", "Julian Togelius"], "title": "Video Game Level Design as a Multi-Agent Reinforcement Learning Problem", "comment": "11 pages, 7 tables, 5 figures, published as full technical paper at\n  the AAAI conference on Artificial Intelligence and Interactive Digital\n  Entertainment 2025", "summary": "Procedural Content Generation via Reinforcement Learning (PCGRL) offers a\nmethod for training controllable level designer agents without the need for\nhuman datasets, using metrics that serve as proxies for level quality as\nrewards. Existing PCGRL research focuses on single generator agents, but are\nbottlenecked by the need to frequently recalculate heuristics of level quality\nand the agent's need to navigate around potentially large maps. By framing\nlevel generation as a multi-agent problem, we mitigate the efficiency\nbottleneck of single-agent PCGRL by reducing the number of reward calculations\nrelative to the number of agent actions. We also find that multi-agent level\ngenerators are better able to generalize to out-of-distribution map shapes,\nwhich we argue is due to the generators' learning more local, modular design\npolicies. We conclude that treating content generation as a distributed,\nmulti-agent task is beneficial for generating functional artifacts at scale."}
{"id": "2510.04886", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.04886", "abs": "https://arxiv.org/abs/2510.04886", "authors": ["Adi Banerjee", "Anirudh Nair", "Tarik Borogovac"], "title": "Where Did It All Go Wrong? A Hierarchical Look into Multi-Agent Error Attribution", "comment": null, "summary": "Error attribution in Large Language Model (LLM) multi-agent systems presents\na significant challenge in debugging and improving collaborative AI systems.\nCurrent approaches to pinpointing agent and step level failures in interaction\ntraces - whether using all-at-once evaluation, step-by-step analysis, or binary\nsearch - fall short when analyzing complex patterns, struggling with both\naccuracy and consistency. We present ECHO (Error attribution through Contextual\nHierarchy and Objective consensus analysis), a novel algorithm that combines\nhierarchical context representation, objective analysis-based evaluation, and\nconsensus voting to improve error attribution accuracy. Our approach leverages\na positional-based leveling of contextual understanding while maintaining\nobjective evaluation criteria, ultimately reaching conclusions through a\nconsensus mechanism. Experimental results demonstrate that ECHO outperforms\nexisting methods across various multi-agent interaction scenarios, showing\nparticular strength in cases involving subtle reasoning errors and complex\ninterdependencies. Our findings suggest that leveraging these concepts of\nstructured, hierarchical context representation combined with consensus-based\nobjective decision-making, provides a more robust framework for error\nattribution in multi-agent systems."}
{"id": "2510.04899", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04899", "abs": "https://arxiv.org/abs/2510.04899", "authors": ["Keane Ong", "Wei Dai", "Carol Li", "Dewei Feng", "Hengzhi Li", "Jingyao Wu", "Jiaee Cheong", "Rui Mao", "Gianmarco Mengaldo", "Erik Cambria", "Paul Pu Liang"], "title": "Human Behavior Atlas: Benchmarking Unified Psychological and Social Behavior Understanding", "comment": null, "summary": "Using intelligent systems to perceive psychological and social behaviors,\nthat is, the underlying affective, cognitive, and pathological states that are\nmanifested through observable behaviors and social interactions, remains a\nchallenge due to their complex, multifaceted, and personalized nature. Existing\nwork tackling these dimensions through specialized datasets and single-task\nsystems often miss opportunities for scalability, cross-task transfer, and\nbroader generalization. To address this gap, we curate Human Behavior Atlas, a\nunified benchmark of diverse behavioral tasks designed to support the\ndevelopment of unified models for understanding psychological and social\nbehaviors. Human Behavior Atlas comprises over 100,000 samples spanning text,\naudio, and visual modalities, covering tasks on affective states, cognitive\nstates, pathologies, and social processes. Our unification efforts can reduce\nredundancy and cost, enable training to scale efficiently across tasks, and\nenhance generalization of behavioral features across domains. On Human Behavior\nAtlas, we train three models: OmniSapiens-7B SFT, OmniSapiens-7B BAM, and\nOmniSapiens-7B RL. We show that training on Human Behavior Atlas enables models\nto consistently outperform existing multimodal LLMs across diverse behavioral\ntasks. Pretraining on Human Behavior Atlas also improves transfer to novel\nbehavioral datasets; with the targeted use of behavioral descriptors yielding\nmeaningful performance gains."}
{"id": "2510.04935", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04935", "abs": "https://arxiv.org/abs/2510.04935", "authors": ["Guoxin Chen", "Zile Qiao", "Wenqing Wang", "Donglei Yu", "Xuanzhong Chen", "Hao Sun", "Minpeng Liao", "Kai Fan", "Yong Jiang", "Penguin Xie", "Wayne Xin Zhao", "Ruihua Song", "Fei Huang"], "title": "MARS: Optimizing Dual-System Deep Research via Multi-Agent Reinforcement Learning", "comment": "Ongoing Work", "summary": "Large Reasoning Models (LRMs) often exhibit a tendency for overanalysis in\nsimple tasks, where the models excessively utilize System 2-type, deliberate\nreasoning, leading to inefficient token generation. Furthermore, these models\nface challenges in adapting their reasoning capabilities to rapidly changing\nenvironments due to the static nature of their pretraining data. To address\nthese issues, advancing Large Language Models (LLMs) for complex reasoning\ntasks requires innovative approaches that bridge intuitive and deliberate\ncognitive processes, akin to human cognition's dual-system dynamic. This paper\nintroduces a Multi-Agent System for Deep ReSearch (MARS) enabling seamless\nintegration of System 1's fast, intuitive thinking with System 2's deliberate\nreasoning within LLMs. MARS strategically integrates multiple external tools,\nsuch as Google Search, Google Scholar, and Python Interpreter, to access\nup-to-date information and execute complex computations, while creating a\nspecialized division of labor where System 1 efficiently processes and\nsummarizes high-volume external information, providing distilled insights that\nexpand System 2's reasoning context without overwhelming its capacity.\nFurthermore, we propose a multi-agent reinforcement learning framework\nextending Group Relative Policy Optimization to simultaneously optimize both\nsystems with multi-turn tool interactions, bin-packing optimization, and sample\nbalancing strategies that enhance collaborative efficiency. Extensive\nexperiments demonstrate MARS achieves substantial improvements of 3.86% on the\nchallenging Humanity's Last Exam (HLE) benchmark and an average gain of 8.9%\nacross 7 knowledge-intensive tasks, validating the effectiveness of our\ndual-system paradigm for complex reasoning in dynamic information environments."}
{"id": "2510.04952", "categories": ["cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2510.04952", "abs": "https://arxiv.org/abs/2510.04952", "authors": ["Ailiya Borjigin", "Cong He"], "title": "Safe and Compliant Cross-Market Trade Execution via Constrained RL and Zero-Knowledge Audits", "comment": "22 pages, 2 figures", "summary": "We present a cross-market algorithmic trading system that balances execution\nquality with rigorous compliance enforcement. The architecture comprises a\nhigh-level planner, a reinforcement learning execution agent, and an\nindependent compliance agent. We formulate trade execution as a constrained\nMarkov decision process with hard constraints on participation limits, price\nbands, and self-trading avoidance. The execution agent is trained with proximal\npolicy optimization, while a runtime action-shield projects any unsafe action\ninto a feasible set. To support auditability without exposing proprietary\nsignals, we add a zero-knowledge compliance audit layer that produces\ncryptographic proofs that all actions satisfied the constraints. We evaluate in\na multi-venue, ABIDES-based simulator and compare against standard baselines\n(e.g., TWAP, VWAP). The learned policy reduces implementation shortfall and\nvariance while exhibiting no observed constraint violations across stress\nscenarios including elevated latency, partial fills, compliance module\ntoggling, and varying constraint limits. We report effects at the 95%\nconfidence level using paired t-tests and examine tail risk via CVaR. We\nsituate the work at the intersection of optimal execution, safe reinforcement\nlearning, regulatory technology, and verifiable AI, and discuss ethical\nconsiderations, limitations (e.g., modeling assumptions and computational\noverhead), and paths to real-world deployment."}
{"id": "2510.04978", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04978", "abs": "https://arxiv.org/abs/2510.04978", "authors": ["Kun Xiang", "Terry Jingchen Zhang", "Yinya Huang", "Jixi He", "Zirong Liu", "Yueling Tang", "Ruizhe Zhou", "Lijing Luo", "Youpeng Wen", "Xiuwei Chen", "Bingqian Lin", "Jianhua Han", "Hang Xu", "Hanhui Li", "Bin Dong", "Xiaodan Liang"], "title": "Aligning Perception, Reasoning, Modeling and Interaction: A Survey on Physical AI", "comment": null, "summary": "The rapid advancement of embodied intelligence and world models has\nintensified efforts to integrate physical laws into AI systems, yet physical\nperception and symbolic physics reasoning have developed along separate\ntrajectories without a unified bridging framework. This work provides a\ncomprehensive overview of physical AI, establishing clear distinctions between\ntheoretical physics reasoning and applied physical understanding while\nsystematically examining how physics-grounded methods enhance AI's real-world\ncomprehension across structured symbolic reasoning, embodied systems, and\ngenerative models. Through rigorous analysis of recent advances, we advocate\nfor intelligent systems that ground learning in both physical principles and\nembodied reasoning processes, transcending pattern recognition toward genuine\nunderstanding of physical laws. Our synthesis envisions next-generation world\nmodels capable of explaining physical phenomena and predicting future states,\nadvancing safe, generalizable, and interpretable AI systems. We maintain a\ncontinuously updated resource at\nhttps://github.com/AI4Phys/Awesome-AI-for-Physics."}
{"id": "2510.04980", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.04980", "abs": "https://arxiv.org/abs/2510.04980", "authors": ["Fangzhou Liang", "Tianshi Zheng", "Chunkit Chan", "Yauwai Yim", "Yangqiu Song"], "title": "LLM-Hanabi: Evaluating Multi-Agent Gameplays with Theory-of-Mind and Rationale Inference in Imperfect Information Collaboration Game", "comment": "EMNLP 2025 Wordplay", "summary": "Effective multi-agent collaboration requires agents to infer the rationale\nbehind others' actions, a capability rooted in Theory-of-Mind (ToM). While\nrecent Large Language Models (LLMs) excel at logical inference, their ability\nto infer rationale in dynamic, collaborative settings remains under-explored.\nThis study introduces LLM-Hanabi, a novel benchmark that uses the cooperative\ngame Hanabi to evaluate the rationale inference and ToM of LLMs. Our framework\nfeatures an automated evaluation system that measures both game performance and\nToM proficiency. Across a range of models, we find a significant positive\ncorrelation between ToM and in-game success. Notably, first-order ToM\n(interpreting others' intent) correlates more strongly with performance than\nsecond-order ToM (predicting others' interpretations). These findings highlight\nthat for effective AI collaboration, the ability to accurately interpret a\npartner's rationale is more critical than higher-order reasoning. We conclude\nthat prioritizing first-order ToM is a promising direction for enhancing the\ncollaborative capabilities of future models."}
{"id": "2510.05014", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.05014", "abs": "https://arxiv.org/abs/2510.05014", "authors": ["Xuanming Cui", "Jianpeng Cheng", "Hong-you Chen", "Satya Narayan Shukla", "Abhijeet Awasthi", "Xichen Pan", "Chaitanya Ahuja", "Shlok Kumar Mishra", "Qi Guo", "Ser-Nam Lim", "Aashu Singh", "Xiangjun Fan"], "title": "Think Then Embed: Generative Context Improves Multimodal Embedding", "comment": null, "summary": "There is a growing interest in Universal Multimodal Embeddings (UME), where\nmodels are required to generate task-specific representations. While recent\nstudies show that Multimodal Large Language Models (MLLMs) perform well on such\ntasks, they treat MLLMs solely as encoders, overlooking their generative\ncapacity. However, such an encoding paradigm becomes less effective as\ninstructions become more complex and require compositional reasoning. Inspired\nby the proven effectiveness of chain-of-thought reasoning, we propose a general\nThink-Then-Embed (TTE) framework for UME, composed of a reasoner and an\nembedder. The reasoner MLLM first generates reasoning traces that explain\ncomplex queries, followed by an embedder that produces representations\nconditioned on both the original query and the intermediate reasoning. This\nexplicit reasoning step enables more nuanced understanding of complex\nmultimodal instructions. Our contributions are threefold. First, by leveraging\na powerful MLLM reasoner, we achieve state-of-the-art performance on the\nMMEB-V2 benchmark, surpassing proprietary models trained on massive in-house\ndatasets. Second, to reduce the dependency on large MLLM reasoners, we finetune\na smaller MLLM reasoner using high-quality embedding-centric reasoning traces,\nachieving the best performance among open-source models with a 7% absolute gain\nover recently proposed models. Third, we investigate strategies for integrating\nthe reasoner and embedder into a unified model for improved efficiency without\nsacrificing performance."}
{"id": "2510.05048", "categories": ["cs.AI", "cs.GT"], "pdf": "https://arxiv.org/pdf/2510.05048", "abs": "https://arxiv.org/abs/2510.05048", "authors": ["Ondřej Kubíček", "Viliam Lisý"], "title": "Look-ahead Reasoning with a Learned Model in Imperfect Information Games", "comment": null, "summary": "Test-time reasoning significantly enhances pre-trained AI agents'\nperformance. However, it requires an explicit environment model, often\nunavailable or overly complex in real-world scenarios. While MuZero enables\neffective model learning for search in perfect information games, extending\nthis paradigm to imperfect information games presents substantial challenges\ndue to more nuanced look-ahead reasoning techniques and large number of states\nrelevant for individual decisions. This paper introduces an algorithm LAMIR\nthat learns an abstracted model of an imperfect information game directly from\nthe agent-environment interaction. During test time, this trained model is used\nto perform look-ahead reasoning. The learned abstraction limits the size of\neach subgame to a manageable size, making theoretically principled look-ahead\nreasoning tractable even in games where previous methods could not scale. We\nempirically demonstrate that with sufficient capacity, LAMIR learns the exact\nunderlying game structure, and with limited capacity, it still learns a\nvaluable abstraction, which improves game playing performance of the\npre-trained agents even in large games."}
{"id": "2510.05059", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.05059", "abs": "https://arxiv.org/abs/2510.05059", "authors": ["Junlin Wang", "Jue Wang", "Zhen", "Xu", "Ben Athiwaratkun", "Bhuwan Dhingra", "Ce Zhang", "James Zou"], "title": "Staircase Streaming for Low-Latency Multi-Agent Inference", "comment": null, "summary": "Recent advances in large language models (LLMs) opened up new directions for\nleveraging the collective expertise of multiple LLMs. These methods, such as\nMixture-of-Agents, typically employ additional inference steps to generate\nintermediate outputs, which are then used to produce the final response. While\nmulti-agent inference can enhance response quality, it can significantly\nincrease the time to first token (TTFT), posing a challenge for\nlatency-sensitive applications and hurting user experience. To address this\nissue, we propose staircase streaming for low-latency multi-agent inference.\nInstead of waiting for the complete intermediate outputs from previous steps,\nwe begin generating the final response as soon as we receive partial outputs\nfrom these steps. Experimental results demonstrate that staircase streaming\nreduces TTFT by up to 93% while maintaining response quality."}
{"id": "2510.03417", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03417", "abs": "https://arxiv.org/abs/2510.03417", "authors": ["Javad Rafiei Asl", "Sidhant Narula", "Mohammad Ghasemigol", "Eduardo Blanco", "Daniel Takabi"], "title": "NEXUS: Network Exploration for eXploiting Unsafe Sequences in Multi-Turn LLM Jailbreaks", "comment": "Javad Rafiei Asl and Sidhant Narula are co-first authors", "summary": "Large Language Models (LLMs) have revolutionized natural language processing\nbut remain vulnerable to jailbreak attacks, especially multi-turn jailbreaks\nthat distribute malicious intent across benign exchanges and bypass alignment\nmechanisms. Existing approaches often explore the adversarial space poorly,\nrely on hand-crafted heuristics, or lack systematic query refinement. We\npresent NEXUS (Network Exploration for eXploiting Unsafe Sequences), a modular\nframework for constructing, refining, and executing optimized multi-turn\nattacks. NEXUS comprises: (1) ThoughtNet, which hierarchically expands a\nharmful intent into a structured semantic network of topics, entities, and\nquery chains; (2) a feedback-driven Simulator that iteratively refines and\nprunes these chains through attacker-victim-judge LLM collaboration using\nharmfulness and semantic-similarity benchmarks; and (3) a Network Traverser\nthat adaptively navigates the refined query space for real-time attacks. This\npipeline uncovers stealthy, high-success adversarial paths across LLMs. On\nseveral closed-source and open-source LLMs, NEXUS increases attack success rate\nby 2.1% to 19.4% over prior methods. Code: https://github.com/inspire-lab/NEXUS"}
{"id": "2510.03463", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03463", "abs": "https://arxiv.org/abs/2510.03463", "authors": ["Vali Tawosi", "Keshav Ramani", "Salwa Alamir", "Xiaomo Liu"], "title": "ALMAS: an Autonomous LLM-based Multi-Agent Software Engineering Framework", "comment": null, "summary": "Multi-agent Large Language Model (LLM) systems have been leading the way in\napplied LLM research across a number of fields. One notable area is software\ndevelopment, where researchers have advanced the automation of code\nimplementation, code testing, code maintenance, inter alia, using LLM agents.\nHowever, software development is a multifaceted environment that extends beyond\njust code. As such, a successful LLM system must factor in multiple stages of\nthe software development life-cycle (SDLC). In this paper, we propose a vision\nfor ALMAS, an Autonomous LLM-based Multi-Agent Software Engineering framework,\nwhich follows the above SDLC philosophy such that it may work within an agile\nsoftware development team to perform several tasks end-to-end. ALMAS aligns its\nagents with agile roles, and can be used in a modular fashion to seamlessly\nintegrate with human developers and their development environment. We showcase\nthe progress towards ALMAS through our published works and a use case\ndemonstrating the framework, where ALMAS is able to seamlessly generate an\napplication and add a new feature."}
{"id": "2510.03495", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03495", "abs": "https://arxiv.org/abs/2510.03495", "authors": ["Erik Pautsch", "Tanmay Singla", "Wenxin Jiang", "Huiyun Peng", "Behnaz Hassanshahi", "Konstantin Läufer", "George K. Thiruvathukal", "James C. Davis"], "title": "AgentHub: A Research Agenda for Agent Sharing Infrastructure", "comment": null, "summary": "LLM-based agents are rapidly proliferating, yet the infrastructure for\ndiscovering, evaluating, and governing them remains fragmented compared to\nmature ecosystems like software package registries (e.g., npm) and model hubs\n(e.g., Hugging Face). Recent research and engineering works have begun to\nconsider the requisite infrastructure, but so far they focus narrowly -- on\ndistribution, naming, or protocol negotiation. However, considering broader\nsoftware engineering requirements would improve open-source distribution and\nease reuse. We therefore propose AgentHub, a research agenda for agent sharing.\nBy framing the key challenges of capability clarity, lifecycle transparency,\ninteroperability, governance, security, and workflow integration, AgentHub\ncharts a community-wide agenda for building reliable and scalable agent\necosystems. Our vision is a future where agents can be shared, trusted, and\ncomposed as seamlessly as today's software libraries."}
{"id": "2510.03610", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03610", "abs": "https://arxiv.org/abs/2510.03610", "authors": ["Zachary Ezetta", "Wu-chang Feng"], "title": "PentestMCP: A Toolkit for Agentic Penetration Testing", "comment": null, "summary": "Agentic AI is transforming security by automating many tasks being performed\nmanually. While initial agentic approaches employed a monolithic architecture,\nthe Model-Context-Protocol has now enabled a remote-procedure call (RPC)\nparadigm to agentic applications, allowing for the flexible construction and\ncomposition of multi-function agents. This paper describes PentestMCP, a\nlibrary of MCP server implementations that support agentic penetration testing.\nBy supporting common penetration testing tasks such as network scanning,\nresource enumeration, service fingerprinting, vulnerability scanning,\nexploitation, and post-exploitation, PentestMCP allows a developer to customize\nmulti-agent workflows for performing penetration tests."}
{"id": "2510.03623", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03623", "abs": "https://arxiv.org/abs/2510.03623", "authors": ["Maraz Mia", "Mir Mehedi A. Pritom"], "title": "Explainable but Vulnerable: Adversarial Attacks on XAI Explanation in Cybersecurity Applications", "comment": "10 pages, 9 figures, 4 tables", "summary": "Explainable Artificial Intelligence (XAI) has aided machine learning (ML)\nresearchers with the power of scrutinizing the decisions of the black-box\nmodels. XAI methods enable looking deep inside the models' behavior, eventually\ngenerating explanations along with a perceived trust and transparency. However,\ndepending on any specific XAI method, the level of trust can vary. It is\nevident that XAI methods can themselves be a victim of post-adversarial attacks\nthat manipulate the expected outcome from the explanation module. Among such\nattack tactics, fairwashing explanation (FE), manipulation explanation (ME),\nand backdoor-enabled manipulation attacks (BD) are the notable ones. In this\npaper, we try to understand these adversarial attack techniques, tactics, and\nprocedures (TTPs) on explanation alteration and thus the effect on the model's\ndecisions. We have explored a total of six different individual attack\nprocedures on post-hoc explanation methods such as SHAP (SHapley Additive\nexPlanations), LIME (Local Interpretable Model-agnostic Explanation), and IG\n(Integrated Gradients), and investigated those adversarial attacks in\ncybersecurity applications scenarios such as phishing, malware, intrusion, and\nfraudulent website detection. Our experimental study reveals the actual\neffectiveness of these attacks, thus providing an urgency for immediate\nattention to enhance the resiliency of XAI methods and their applications."}
{"id": "2510.03755", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03755", "abs": "https://arxiv.org/abs/2510.03755", "authors": ["Roham Koohestani", "Parham Bateni", "Aydin Ebrahimi", "Behdad Etezadi", "Kiarash Karimi", "Maliheh Izadi"], "title": "Code4MeV2: a Research-oriented Code-completion Platform", "comment": "Under review for submission at a conference", "summary": "The adoption of AI-powered code completion tools in software development has\nincreased substantially, yet the user interaction data produced by these\nsystems remain proprietary within large corporations. This creates a barrier\nfor the academic community, as researchers must often develop dedicated\nplatforms to conduct studies on human--AI interaction, making reproducible\nresearch and large-scale data analysis impractical. In this work, we introduce\nCode4MeV2, a research-oriented, open-source code completion plugin for\nJetBrains IDEs, as a solution to this limitation. Code4MeV2 is designed using a\nclient--server architecture and features inline code completion and a\ncontext-aware chat assistant. Its core contribution is a modular and\ntransparent data collection framework that gives researchers fine-grained\ncontrol over telemetry and context gathering. Code4MeV2 achieves\nindustry-comparable performance in terms of code completion, with an average\nlatency of 200~ms. We assess our tool through a combination of an expert\nevaluation and a user study with eight participants. Feedback from both\nresearchers and daily users highlights its informativeness and usefulness. We\ninvite the community to adopt and contribute to this tool. More information\nabout the tool can be found at https://app.code4me.me."}
{"id": "2510.03761", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03761", "abs": "https://arxiv.org/abs/2510.03761", "authors": ["Richard A. Dubniczky", "Bertalan Borsos", "Tihanyi Norbert"], "title": "You Have Been LaTeXpOsEd: A Systematic Analysis of Information Leakage in Preprint Archives Using Large Language Models", "comment": null, "summary": "The widespread use of preprint repositories such as arXiv has accelerated the\ncommunication of scientific results but also introduced overlooked security\nrisks. Beyond PDFs, these platforms provide unrestricted access to original\nsource materials, including LaTeX sources, auxiliary code, figures, and\nembedded comments. In the absence of sanitization, submissions may disclose\nsensitive information that adversaries can harvest using open-source\nintelligence. In this work, we present the first large-scale security audit of\npreprint archives, analyzing more than 1.2 TB of source data from 100,000 arXiv\nsubmissions. We introduce LaTeXpOsEd, a four-stage framework that integrates\npattern matching, logical filtering, traditional harvesting techniques, and\nlarge language models (LLMs) to uncover hidden disclosures within\nnon-referenced files and LaTeX comments. To evaluate LLMs' secret-detection\ncapabilities, we introduce LLMSec-DB, a benchmark on which we tested 25\nstate-of-the-art models. Our analysis uncovered thousands of PII leaks,\nGPS-tagged EXIF files, publicly available Google Drive and Dropbox folders,\neditable private SharePoint links, exposed GitHub and Google credentials, and\ncloud API keys. We also uncovered confidential author communications, internal\ndisagreements, and conference submission credentials, exposing information that\nposes serious reputational risks to both researchers and institutions. We urge\nthe research community and repository operators to take immediate action to\nclose these hidden security gaps. To support open science, we release all\nscripts and methods from this study but withhold sensitive findings that could\nbe misused, in line with ethical principles. The source code and related\nmaterial are available at the project website https://github.com/LaTeXpOsEd"}
{"id": "2510.03862", "categories": ["cs.SE", "cs.AI", "500"], "pdf": "https://arxiv.org/pdf/2510.03862", "abs": "https://arxiv.org/abs/2510.03862", "authors": ["Nathalia Nascimento", "Everton Guimaraes", "Paulo Alencar"], "title": "Designing Empirical Studies on LLM-Based Code Generation: Towards a Reference Framework", "comment": "5 pages", "summary": "The rise of large language models (LLMs) has introduced transformative\npotential in automated code generation, addressing a wide range of software\nengineering challenges. However, empirical evaluation of LLM-based code\ngeneration lacks standardization, with studies varying widely in goals, tasks,\nand metrics, which limits comparability and reproducibility. In this paper, we\npropose a theoretical framework for designing and reporting empirical studies\non LLM-based code generation. The framework is grounded in both our prior\nexperience conducting such experiments and a comparative analysis of key\nsimilarities and differences among recent studies. It organizes evaluation\naround core components such as problem sources, quality attributes, and\nmetrics, supporting structured and systematic experimentation. We demonstrate\nits applicability through representative case mappings and identify\nopportunities for refinement. Looking forward, we plan to evolve the framework\ninto a more robust and mature tool for standardizing LLM evaluation across\nsoftware engineering contexts."}
{"id": "2510.03879", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03879", "abs": "https://arxiv.org/abs/2510.03879", "authors": ["Tianyu Li", "Ruishi Li", "Bo Wang", "Brandon Paulsen", "Umang Mathur", "Prateek Saxena"], "title": "Adversarial Agent Collaboration for C to Rust Translation", "comment": null, "summary": "Translating C to memory-safe languages, like Rust, prevents critical memory\nsafety vulnerabilities that are prevalent in legacy C software. Existing\napproaches for C to safe Rust translation, including LLM-assisted ones, do not\ngeneralize on larger (> 500 LoC) C codebases because they depend on complex\nprogram analyses that frequently break. In this work, we present ACToR\n(Adversarial C To Rust translator), a simple LLM agent-based approach. Inspired\nby GANs, ACToR pits a generator agent against a discriminator agent, which\ncollaborate to iteratively generate a Rust translation. On each iteration, the\ntranslator agent synthesizes and refines a Rust translation to pass an existing\nsuite of tests, and then the discriminator agent finds new failing tests. We\ndemonstrate that ACToR translates all of the 63 real-world command line\nutilities considered in our benchmarks, which have an average size of 485 lines\nof code, and it achieves over 90% test pass rate with zero human intervention.\nTo our knowledge, it is the first such system that reliably translates C\nprograms of this scale. Furthermore, ACToR improves translation correctness by\nup to 18.9% compared to baseline, non-adversarial approaches."}
{"id": "2510.03914", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03914", "abs": "https://arxiv.org/abs/2510.03914", "authors": ["Yonnel Chen Kuang Piao", "Jean Carlors Paul", "Leuson Da Silva", "Arghavan Moradi Dakhel", "Mohammad Hamdaqa", "Foutse Khomh"], "title": "Refactoring with LLMs: Bridging Human Expertise and Machine Understanding", "comment": "43 pages, 2 figures, 9 tables", "summary": "Code refactoring is a fundamental software engineering practice aimed at\nimproving code quality and maintainability. Despite its importance, developers\noften neglect refactoring due to the significant time, effort, and resources it\nrequires, as well as the lack of immediate functional rewards. Although several\nautomated refactoring tools have been proposed, they remain limited in\nsupporting a broad spectrum of refactoring types. In this study, we explore\nwhether instruction strategies inspired by human best-practice guidelines can\nenhance the ability of Large Language Models (LLMs) to perform diverse\nrefactoring tasks automatically. Leveraging the instruction-following and code\ncomprehension capabilities of state-of-the-art LLMs (e.g., GPT-mini and\nDeepSeek-V3), we draw on Martin Fowler's refactoring guidelines to design\nmultiple instruction strategies that encode motivations, procedural steps, and\ntransformation objectives for 61 well-known refactoring types. We evaluate\nthese strategies on benchmark examples and real-world code snippets from GitHub\nprojects. Our results show that instruction designs grounded in Fowler's\nguidelines enable LLMs to successfully perform all benchmark refactoring types\nand preserve program semantics in real-world settings, an essential criterion\nfor effective refactoring. Moreover, while descriptive instructions are more\ninterpretable to humans, our results show that rule-based instructions often\nlead to better performance in specific scenarios. Interestingly, allowing\nmodels to focus on the overall goal of refactoring, rather than prescribing a\nfixed transformation type, can yield even greater improvements in code quality."}
{"id": "2510.03992", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03992", "abs": "https://arxiv.org/abs/2510.03992", "authors": ["Jehyeok Yeon", "Isha Chaudhary", "Gagandeep Singh"], "title": "Quantifying Distributional Robustness of Agentic Tool-Selection", "comment": null, "summary": "Large language models (LLMs) are increasingly deployed in agentic systems\nwhere they map user intents to relevant external tools to fulfill a task. A\ncritical step in this process is tool selection, where a retriever first\nsurfaces candidate tools from a larger pool, after which the LLM selects the\nmost appropriate one. This pipeline presents an underexplored attack surface\nwhere errors in selection can lead to severe outcomes like unauthorized data\naccess or denial of service, all without modifying the agent's model or code.\nWhile existing evaluations measure task performance in benign settings, they\noverlook the specific vulnerabilities of the tool selection mechanism under\nadversarial conditions. To address this gap, we introduce ToolCert, the first\nstatistical framework that formally certifies tool selection robustness.\nToolCert models tool selection as a Bernoulli success process and evaluates it\nagainst a strong, adaptive attacker who introduces adversarial tools with\nmisleading metadata, and are iteratively refined based on the agent's previous\nchoices. By sampling these adversarial interactions, ToolCert produces a\nhigh-confidence lower bound on accuracy, formally quantifying the agent's\nworst-case performance. Our evaluation with ToolCert uncovers the severe\nfragility: under attacks injecting deceptive tools or saturating retrieval, the\ncertified accuracy bound drops near zero, an average performance drop of over\n60% compared to non-adversarial settings. For attacks targeting the retrieval\nand selection stages, the certified accuracy bound plummets to less than 20%\nafter just a single round of adversarial adaptation. ToolCert thus reveals\npreviously unexamined security threats inherent to tool selection and provides\na principled method to quantify an agent's robustness to such threats, a\nnecessary step for the safe deployment of agentic systems."}
{"id": "2510.03995", "categories": ["cs.CR", "cs.AI", "I.2; E.m"], "pdf": "https://arxiv.org/pdf/2510.03995", "abs": "https://arxiv.org/abs/2510.03995", "authors": ["Nges Brian Njungle", "Eric Jahns", "Milan Stojkov", "Michel A. Kinsy"], "title": "PrivSpike: Employing Homomorphic Encryption for Private Inference of Deep Spiking Neural Networks", "comment": "13 pages, 5 figures", "summary": "Deep learning has become a cornerstone of modern machine learning. It relies\nheavily on vast datasets and significant computational resources for high\nperformance. This data often contains sensitive information, making privacy a\nmajor concern in deep learning. Spiking Neural Networks (SNNs) have emerged as\nan energy-efficient alternative to conventional deep learning approaches.\nNevertheless, SNNs still depend on large volumes of data, inheriting all the\nprivacy challenges of deep learning. Homomorphic encryption addresses this\nchallenge by allowing computations to be performed on encrypted data, ensuring\ndata confidentiality throughout the entire processing pipeline. In this paper,\nwe introduce PRIVSPIKE, a privacy-preserving inference framework for SNNs using\nthe CKKS homomorphic encryption scheme. PRIVSPIKE supports arbitrary depth SNNs\nand introduces two key algorithms for evaluating the Leaky Integrate-and-Fire\nactivation function: (1) a polynomial approximation algorithm designed for\nhigh-performance SNN inference, and (2) a novel scheme-switching algorithm that\noptimizes precision at a higher computational cost. We evaluate PRIVSPIKE on\nMNIST, CIFAR-10, Neuromorphic MNIST, and CIFAR-10 DVS using models from LeNet-5\nand ResNet-19 architectures, achieving encrypted inference accuracies of\n98.10%, 79.3%, 98.1%, and 66.0%, respectively. On a consumer-grade CPU, SNN\nLeNet-5 models achieved inference times of 28 seconds on MNIST and 212 seconds\non Neuromorphic MNIST. For SNN ResNet-19 models, inference took 784 seconds on\nCIFAR-10 and 1846 seconds on CIFAR-10 DVS. These results establish PRIVSPIKE as\na viable and efficient solution for secure SNN inference, bridging the gap\nbetween energy-efficient deep neural networks and strong cryptographic privacy\nguarantees while outperforming prior encrypted SNN solutions."}
{"id": "2510.04135", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04135", "abs": "https://arxiv.org/abs/2510.04135", "authors": ["Jingzhi Gong", "Yixin Bian", "Luis de la Cal", "Giovanni Pinna", "Anisha Uteem", "David Williams", "Mar Zamorano", "Karine Even-Mendoza", "W. B. Langdon", "Hector Menendez", "Federica Sarro"], "title": "GA4GC: Greener Agent for Greener Code via Multi-Objective Configuration Optimization", "comment": "Accepted by SSBSE'25 Challenge Track", "summary": "Coding agents powered by LLMs face critical sustainability and scalability\nchallenges in industrial deployment, with single runs consuming over 100k\ntokens and incurring environmental costs that may exceed optimization benefits.\nThis paper introduces GA4GC, the first framework to systematically optimize\ncoding agent runtime (greener agent) and code performance (greener code)\ntrade-offs by discovering Pareto-optimal agent hyperparameters and prompt\ntemplates. Evaluation on the SWE-Perf benchmark demonstrates up to 135x\nhypervolume improvement, reducing agent runtime by 37.7% while improving\ncorrectness. Our findings establish temperature as the most critical\nhyperparameter, and provide actionable strategies to balance agent\nsustainability with code optimization effectiveness in industrial deployment."}
{"id": "2510.04166", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04166", "abs": "https://arxiv.org/abs/2510.04166", "authors": ["Marco Edoardo Palma", "Pooja Rani", "Harald C. Gall"], "title": "Multi Language Models for On-the-Fly Syntax Highlighting", "comment": null, "summary": "Syntax highlighting is a critical feature in modern software development\nenvironments, enhancing code readability and developer productivity. However,\ndelivering accurate highlighting in real time remains challenging for online\nand web-based development tools due to strict time and memory constraints on\nbackend services. These systems must serve highlights rapidly and frequently,\neven when code is partially valid or invalid. This has led to on-the-fly syntax\nhighlighting, where visual annotations are generated just before content is\nserved, often at high request rates and under incomplete input conditions. To\nmeet these demands efficiently, state-of-the-art models use deep learning to\nlearn the behavior of brute-force syntax highlighting resolvers, tools that are\neasy to implement but too slow for production. Through the Deep Abstraction\nprocess, brute-force strategies are encoded into fast statistical models that\nachieve both high accuracy and low-latency inference. Despite their success,\nsuch models face key challenges: they support only one programming language per\nmodel, require large datasets from slow brute-force generators, and involve\nresource-intensive training. In multi-language environments, this means\nmaintaining multiple independent models, increasing system complexity and\noperational cost. This work addresses these issues by introducing a unified\nmodel capable of highlighting up to six mainstream programming languages,\nreducing deployment complexity by a factor of six and improving performance on\nunseen languages. A novel normalization technique significantly enhances model\ngeneralization, while few-shot learning experiments show that a small number of\noracle samples can replace large datasets, minimizing dependence on brute-force\ngenerators. Combined, these innovations enable efficient, scalable, and\ncost-effective syntax highlighting across diverse programming languages."}
{"id": "2510.04257", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04257", "abs": "https://arxiv.org/abs/2510.04257", "authors": ["Yanjie Li", "Yiming Cao", "Dong Wang", "Bin Xiao"], "title": "AgentTypo: Adaptive Typographic Prompt Injection Attacks against Black-box Multimodal Agents", "comment": "13 pages, 8 figures. Submitted to IEEE Transactions on Information\n  Forensics & Security", "summary": "Multimodal agents built on large vision-language models (LVLMs) are\nincreasingly deployed in open-world settings but remain highly vulnerable to\nprompt injection, especially through visual inputs. We introduce AgentTypo, a\nblack-box red-teaming framework that mounts adaptive typographic prompt\ninjection by embedding optimized text into webpage images. Our automatic\ntypographic prompt injection (ATPI) algorithm maximizes prompt reconstruction\nby substituting captioners while minimizing human detectability via a stealth\nloss, with a Tree-structured Parzen Estimator guiding black-box optimization\nover text placement, size, and color. To further enhance attack strength, we\ndevelop AgentTypo-pro, a multi-LLM system that iteratively refines injection\nprompts using evaluation feedback and retrieves successful past examples for\ncontinual learning. Effective prompts are abstracted into generalizable\nstrategies and stored in a strategy repository, enabling progressive knowledge\naccumulation and reuse in future attacks. Experiments on the VWA-Adv benchmark\nacross Classifieds, Shopping, and Reddit scenarios show that AgentTypo\nsignificantly outperforms the latest image-based attacks such as AgentAttack.\nOn GPT-4o agents, our image-only attack raises the success rate from 0.23 to\n0.45, with consistent results across GPT-4V, GPT-4o-mini, Gemini 1.5 Pro, and\nClaude 3 Opus. In image+text settings, AgentTypo achieves 0.68 ASR, also\noutperforming the latest baselines. Our findings reveal that AgentTypo poses a\npractical and potent threat to multimodal agents and highlight the urgent need\nfor effective defense."}
{"id": "2510.04349", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04349", "abs": "https://arxiv.org/abs/2510.04349", "authors": ["Dmitry Ustalov", "Egor Bogomolov", "Alexander Bezzubov", "Yaroslav Golubev", "Evgeniy Glukhov", "Georgii Levtsov", "Vladimir Kovalenko"], "title": "Challenge on Optimization of Context Collection for Code Completion", "comment": "7 pages, 3 figures, 5 tables. A report on the Context Collection\n  Workshop co-located with ASE'25", "summary": "The rapid advancement of workflows and methods for software engineering using\nAI emphasizes the need for a systematic evaluation and analysis of their\nability to leverage information from entire projects, particularly in large\ncode bases. In this challenge on optimization of context collection for code\ncompletion, organized by JetBrains in collaboration with Mistral AI as part of\nthe ASE 2025 conference, participants developed efficient mechanisms for\ncollecting context from source code repositories to improve fill-in-the-middle\ncode completions for Python and Kotlin. We constructed a large dataset of\nreal-world code in these two programming languages using permissively licensed\nopen-source projects. The submissions were evaluated based on their ability to\nmaximize completion quality for multiple state-of-the-art neural models using\nthe chrF metric. During the public phase of the competition, nineteen teams\nsubmitted solutions to the Python track and eight teams submitted solutions to\nthe Kotlin track. In the private phase, six teams competed, of which five\nsubmitted papers to the workshop."}
{"id": "2510.04363", "categories": ["cs.SE", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.04363", "abs": "https://arxiv.org/abs/2510.04363", "authors": ["Hyunjun Kim", "Sejong Kim"], "title": "MacroBench: A Novel Testbed for Web Automation Scripts via Large Language Models", "comment": "NeurIPS 2025 Workshop on Lock-LLM", "summary": "We introduce MacroBench, a code-first benchmark that evaluates whether LLMs\ncan synthesize reusable browser automation programs from natural language goals\nby reading HTML/DOM and emitting Python with Selenium. MacroBench instantiates\nseven self-hosted sites: Airbnb-like, TikTok-like, Reddit-like, Instagram-like,\nFacebook-like, Discord-like, and Threads-like, covering 681 tasks across\ninteraction complexity and targeting difficulty. Our end-to-end protocol\nvalidates generated code via static checks, sandboxed execution, and outcome\nverification including DOM assertions and database snapshots, and includes a\nsafety suite for scraping, spam/abuse, and credential/privacy prompts. Across\n2636 model-task runs, we observe stratified success: GPT-4o-Mini achieves 96.8\npercent, GPT-4.1 achieves 95.3 percent, Gemini-2.5-Pro achieves 89.0 percent,\nand DeepSeek-V3.1 achieves 83.4 percent. Models handle simple tasks reliably at\n91.7 percent but fail on complex workflows at 0.0 percent, and none meet\nproduction-quality coding practices despite functional completion. We release\nour complete benchmark pipeline, evaluation framework, and experimental results\nto enable reproducible assessment of macro synthesis for web automation."}
{"id": "2510.04380", "categories": ["cs.SE", "cs.AI", "cs.HC", "D.2.1; D.2.2; D.2.9; I.2.7"], "pdf": "https://arxiv.org/pdf/2510.04380", "abs": "https://arxiv.org/abs/2510.04380", "authors": ["Mateen Ahmed Abbasi", "Petri Ihantola", "Tommi Mikkonen", "Niko Mäkitalo"], "title": "Reconsidering Requirements Engineering: Human-AI Collaboration in AI-Native Software Development", "comment": "Accepted at SEAA 2025. Appearing in Springer LNCS 16081, pages\n  164-180", "summary": "Requirement Engineering (RE) is the foundation of successful software\ndevelopment. In RE, the goal is to ensure that implemented systems satisfy\nstakeholder needs through rigorous requirements elicitation, validation, and\nevaluation processes. Despite its critical role, RE continues to face\npersistent challenges, such as ambiguity, conflicting stakeholder needs, and\nthe complexity of managing evolving requirements. A common view is that\nArtificial Intelligence (AI) has the potential to streamline the RE process,\nresulting in improved efficiency, accuracy, and management actions. However,\nusing AI also introduces new concerns, such as ethical issues, biases, and lack\nof transparency. This paper explores how AI can enhance traditional RE\npractices by automating labor-intensive tasks, supporting requirement\nprioritization, and facilitating collaboration between stakeholders and AI\nsystems. The paper also describes the opportunities and challenges that AI\nbrings to RE. In particular, the vision calls for ethical practices in AI,\nalong with a much-enhanced collaboration between academia and industry\nprofessionals. The focus should be on creating not only powerful but also\ntrustworthy and practical AI solutions ready to adapt to the fast-paced world\nof software development."}
{"id": "2510.04397", "categories": ["cs.CR", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.04397", "abs": "https://arxiv.org/abs/2510.04397", "authors": ["Van Nguyen", "Surya Nepal", "Xingliang Yuan", "Tingmin Wu", "Fengchao Chen", "Carsten Rudolph"], "title": "MulVuln: Enhancing Pre-trained LMs with Shared and Language-Specific Knowledge for Multilingual Vulnerability Detection", "comment": null, "summary": "Software vulnerabilities (SVs) pose a critical threat to safety-critical\nsystems, driving the adoption of AI-based approaches such as machine learning\nand deep learning for software vulnerability detection. Despite promising\nresults, most existing methods are limited to a single programming language.\nThis is problematic given the multilingual nature of modern software, which is\noften complex and written in multiple languages. Current approaches often face\nchallenges in capturing both shared and language-specific knowledge of source\ncode, which can limit their performance on diverse programming languages and\nreal-world codebases. To address this gap, we propose MULVULN, a novel\nmultilingual vulnerability detection approach that learns from source code\nacross multiple languages. MULVULN captures both the shared knowledge that\ngeneralizes across languages and the language-specific knowledge that reflects\nunique coding conventions. By integrating these aspects, it achieves more\nrobust and effective detection of vulnerabilities in real-world multilingual\nsoftware systems. The rigorous and extensive experiments on the real-world and\ndiverse REEF dataset, consisting of 4,466 CVEs with 30,987 patches across seven\nprogramming languages, demonstrate the superiority of MULVULN over thirteen\neffective and state-of-the-art baselines. Notably, MULVULN achieves\nsubstantially higher F1-score, with improvements ranging from 1.45% to 23.59%\ncompared to the baseline methods."}
{"id": "2510.04503", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.04503", "abs": "https://arxiv.org/abs/2510.04503", "authors": ["Shuai Zhao", "Xinyi Wu", "Shiqian Zhao", "Xiaobao Wu", "Zhongliang Guo", "Yanhao Jia", "Anh Tuan Luu"], "title": "P2P: A Poison-to-Poison Remedy for Reliable Backdoor Defense in LLMs", "comment": null, "summary": "During fine-tuning, large language models (LLMs) are increasingly vulnerable\nto data-poisoning backdoor attacks, which compromise their reliability and\ntrustworthiness. However, existing defense strategies suffer from limited\ngeneralization: they only work on specific attack types or task settings. In\nthis study, we propose Poison-to-Poison (P2P), a general and effective backdoor\ndefense algorithm. P2P injects benign triggers with safe alternative labels\ninto a subset of training samples and fine-tunes the model on this re-poisoned\ndataset by leveraging prompt-based learning. This enforces the model to\nassociate trigger-induced representations with safe outputs, thereby overriding\nthe effects of original malicious triggers. Thanks to this robust and\ngeneralizable trigger-based fine-tuning, P2P is effective across task settings\nand attack types. Theoretically and empirically, we show that P2P can\nneutralize malicious backdoors while preserving task performance. We conduct\nextensive experiments on classification, mathematical reasoning, and summary\ngeneration tasks, involving multiple state-of-the-art LLMs. The results\ndemonstrate that our P2P algorithm significantly reduces the attack success\nrate compared with baseline models. We hope that the P2P can serve as a\nguideline for defending against backdoor attacks and foster the development of\na secure and trustworthy LLM community."}
{"id": "2510.04528", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04528", "abs": "https://arxiv.org/abs/2510.04528", "authors": ["Santhosh KumarRavindran"], "title": "Unified Threat Detection and Mitigation Framework (UTDMF): Combating Prompt Injection, Deception, and Bias in Enterprise-Scale Transformers", "comment": null, "summary": "The rapid adoption of large language models (LLMs) in enterprise systems\nexposes vulnerabilities to prompt injection attacks, strategic deception, and\nbiased outputs, threatening security, trust, and fairness. Extending our\nadversarial activation patching framework (arXiv:2507.09406), which induced\ndeception in toy networks at a 23.9% rate, we introduce the Unified Threat\nDetection and Mitigation Framework (UTDMF), a scalable, real-time pipeline for\nenterprise-grade models like Llama-3.1 (405B), GPT-4o, and Claude-3.5. Through\n700+ experiments per model, UTDMF achieves: (1) 92% detection accuracy for\nprompt injection (e.g., jailbreaking); (2) 65% reduction in deceptive outputs\nvia enhanced patching; and (3) 78% improvement in fairness metrics (e.g.,\ndemographic bias). Novel contributions include a generalized patching algorithm\nfor multi-threat detection, three groundbreaking hypotheses on threat\ninteractions (e.g., threat chaining in enterprise workflows), and a\ndeployment-ready toolkit with APIs for enterprise integration."}
{"id": "2510.04760", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04760", "abs": "https://arxiv.org/abs/2510.04760", "authors": ["Sisay Deresa Sima", "Ayalew Belay Habtie"], "title": "Agile Software Effort Estimation using Regression Techniques", "comment": null, "summary": "Software development effort estimation is one of the most critical aspect in\nsoftware development process, as the success or failure of the entire project\ndepends on the accuracy of estimations. Researchers are still conducting\nstudies on agile effort estimation. The aim of this research is to develop a\nstory point based agile effort estimation model using LASSO and Elastic Net\nregression techniques. The experimental work is applied to the agile story\npoint approach using 21 software projects collected from six firms. The two\nalgorithms are trained using their default parameters and tuned grid search\nwith 5-fold cross-validation to get an enhanced model. The experiment result\nshows LASSO regression achieved better predictive performance PRED (8%) and\nPRED (25%) results of 100.0, MMRE of 0.0491, MMER of 0.0551, MdMRE of 0.0593,\nMdMER of 0.063, and MSE of 0.0007. The results are also compared with other\nrelated literature."}
{"id": "2510.04852", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04852", "abs": "https://arxiv.org/abs/2510.04852", "authors": ["Victor May", "Diganta Misra", "Yanqi Luo", "Anjali Sridhar", "Justine Gehring", "Silvio Soares Ribeiro Junior"], "title": "FreshBrew: A Benchmark for Evaluating AI Agents on Java Code Migration", "comment": "18 pages, 11 figures", "summary": "AI coding assistants are rapidly becoming integral to modern software\ndevelopment. A key challenge in this space is the continual need to migrate and\nmodernize codebases in response to evolving software ecosystems. Traditionally,\nsuch migrations have relied on rule-based systems and human intervention. With\nthe advent of powerful large language models (LLMs), AI-driven agentic\nframeworks offer a promising alternative-but their effectiveness has not been\nsystematically evaluated. In this paper, we introduce FreshBrew, a novel\nbenchmark for evaluating AI agents on project-level Java migrations, with a\nspecific focus on measuring an agent's ability to preserve program semantics\nand avoid reward hacking, which we argue requires projects with high test\ncoverage for a rigorous and reliable evaluation. We benchmark several\nstate-of-the-art LLMs, and compare their performance against established\nrule-based tools. Our evaluation of AI agents on this benchmark of 228\nrepositories shows that the top-performing model, Gemini 2.5 Flash, can\nsuccessfully migrate 52.3 percent of projects to JDK 17. Our empirical analysis\nreveals novel insights into the critical strengths and limitations of current\nagentic approaches, offering actionable insights into their real-world\napplicability. Our empirical study reveals failure modes of current AI agents\nin realistic Java modernization tasks, providing a foundation for evaluating\ntrustworthy code-migration systems. By releasing FreshBrew, we aim to\nfacilitate rigorous, reproducible evaluation and catalyze progress in AI-driven\ncodebase modernization."}
{"id": "2510.04997", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04997", "abs": "https://arxiv.org/abs/2510.04997", "authors": ["Jiongchi Yu", "Weipeng Jiang", "Xiaoyu Zhang", "Qiang Hu", "Xiaofei Xie", "Chao Shen"], "title": "AutoEmpirical: LLM-Based Automated Research for Empirical Software Fault Analysis", "comment": "5 pages", "summary": "Understanding software faults is essential for empirical research in software\ndevelopment and maintenance. However, traditional fault analysis, while\nvaluable, typically involves multiple expert-driven steps such as collecting\npotential faults, filtering, and manual investigation. These processes are both\nlabor-intensive and time-consuming, creating bottlenecks that hinder\nlarge-scale fault studies in complex yet critical software systems and slow the\npace of iterative empirical research.\n  In this paper, we decompose the process of empirical software fault study\ninto three key phases: (1) research objective definition, (2) data preparation,\nand (3) fault analysis, and we conduct an initial exploration study of applying\nLarge Language Models (LLMs) for fault analysis of open-source software.\nSpecifically, we perform the evaluation on 3,829 software faults drawn from a\nhigh-quality empirical study. Our results show that LLMs can substantially\nimprove efficiency in fault analysis, with an average processing time of about\ntwo hours, compared to the weeks of manual effort typically required. We\nconclude by outlining a detailed research plan that highlights both the\npotential of LLMs for advancing empirical fault studies and the open challenges\nthat required be addressed to achieve fully automated, end-to-end software\nfault analysis."}
