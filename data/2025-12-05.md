<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 11]
- [cs.SE](#cs.SE) [Total: 3]
- [cs.AI](#cs.AI) [Total: 11]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [From Oracle Choice to Oracle Lock-In: An Exploratory Study on Blockchain Oracles Supplier Selection](https://arxiv.org/abs/2512.03088)
*Giulio Caldarelli*

Main category: cs.CR

TL;DR: 研究填补了Web3协议选择预言机驱动因素的空白，发现技术依赖和智能合约不可变性导致锁定效应，且协议在有可行第三方方案时更倾向于外包而非自建预言机机制。


<details>
  <summary>Details</summary>
Motivation: 数据是Web3应用的关键资产，预言机选择对应用成功至关重要。目前学术研究主要关注预言机技术和内部经济机制的改进，而客户端选择预言机的驱动因素仍未被充分探索。

Method: 通过收集领先Web3协议的见解，了解他们选择预言机的理由以及在决定外包或内部化数据请求机制时的偏好。收集的数据覆盖超过55%的DeFi市值，且信息来源于协议高管、董事会成员或代表。

Result: 研究发现协议选择与技术依赖密切相关，智能合约的不可变性放大了锁定效应，阻碍了在不同数据提供商之间的灵活切换。此外，当存在可行的第三方解决方案时，协议绝大多数倾向于外包而非自建和维护内部预言机机制。

Conclusion: 该研究填补了预言机选择驱动因素的研究空白，揭示了Web3协议在预言机选择决策中的实际考量和偏好，为理解去中心化应用生态系统中的技术依赖和外包决策提供了重要见解。

Abstract: As data is an essential asset for any Web3 application, selecting an oracle is a critical decision for its success. To date, academic research has mainly focused on improving oracle technology and internal economics, while the drivers of oracle choice on the client side remain largely unexplored. This study fills this gap by gathering insights from leading Web3 protocols, uncovering their rationale for oracle selection and their preferences when deciding whether to outsource or internalize data request mechanisms. The collected data covers more than 55% of the DeFi market cap and is obtained exclusively by protocol executives, board members, or delegates. Insights support the view that protocol choices are tied to technological dependencies, where immutability of smart contracts amplifies lock-in, preventing agile switching among data providers. Furthermore, when viable third-party solutions exist, protocols overwhelmingly prefer outsourcing rather than building and maintaining internal oracle mechanisms.

</details>


### [2] [Ensemble Privacy Defense for Knowledge-Intensive LLMs against Membership Inference Attacks](https://arxiv.org/abs/2512.03100)
*Haowei Fu,Bo Ni,Han Xu,Kunpeng Liu,Dan Lin,Tyler Derr*

Main category: cs.CR

TL;DR: 该论文系统评估了RAG和SFT增强的LLMs对成员推理攻击的脆弱性，并提出了一种新的模型无关防御框架EPD，显著降低攻击成功率


<details>
  <summary>Details</summary>
Motivation: 虽然RAG和SFT增强了LLMs的知识能力，但也引入了新的攻击面，特别是成员推理攻击对隐私和信任构成严重威胁，需要系统评估和有效防御

Method: 首先系统评估RAG和SFT增强LLMs对多种MIA的脆弱性；然后提出集成隐私防御框架EPD，聚合知识注入LLM、基础LLM和专用判断模型的输出以增强MIA抵抗力

Result: 综合实验显示，EPD平均将SFT的MIA成功率降低27.8%，RAG的MIA成功率降低526.3%（相比推理时基线），同时保持回答质量

Conclusion: 论文揭示了知识增强LLMs的隐私风险，提出的EPD框架能有效防御成员推理攻击，为保护敏感领域隐私提供了实用解决方案

Abstract: Retrieval-Augmented Generation (RAG) and Supervised Finetuning (SFT) have become the predominant paradigms for equipping Large Language Models (LLMs) with external knowledge for diverse, knowledge-intensive tasks. However, while such knowledge injection improves performance, it also exposes new attack surfaces. Membership Inference Attacks (MIAs), which aim to determine whether a given data sample was included in a model's training set, pose serious threats to privacy and trust in sensitive domains. To this end, we first systematically evaluate the vulnerability of RAG- and SFT-based LLMs to various MIAs. Then, to address the privacy risk, we further introduce a novel, model-agnostic defense framework, Ensemble Privacy Defense (EPD), which aggregates and evaluates the outputs of a knowledge-injected LLM, a base LLM, and a dedicated judge model to enhance resistance against MIAs. Comprehensive experiments show that, on average, EPD reduces MIA success by up to 27.8\% for SFT and 526.3\% for RAG compared to inference-time baseline, while maintaining answer quality.

</details>


### [3] [Lost in Modality: Evaluating the Effectiveness of Text-Based Membership Inference Attacks on Large Multimodal Models](https://arxiv.org/abs/2512.03121)
*Ziyi Tong,Feifei Sun,Le Minh Nguyen*

Main category: cs.CR

TL;DR: 首次全面评估基于对数概率的成员推理攻击在多模态大语言模型中的效果，发现在分布内设置中视觉+文本输入略有优势，而在分布外设置中视觉输入起到正则化作用，掩盖成员信号。


<details>
  <summary>Details</summary>
Motivation: 随着多模态大语言模型成为基础工具，理解这些系统中的训练数据泄露问题变得越来越重要。虽然基于对数概率的成员推理攻击已成为评估大型语言模型数据暴露的广泛方法，但它们在多模态模型中的效果尚不清楚。

Method: 将基于文本的成员推理攻击方法扩展到多模态设置，在DeepSeek-VL和InternVL模型家族上进行实验，比较视觉+文本和纯文本两种条件下的攻击效果，包括分布内和分布外设置。

Result: 在分布内设置中，基于logit的成员推理攻击在各种配置下表现相当，视觉+文本条件略有优势。在分布外设置中，视觉输入起到正则化作用，有效地掩盖了成员信号。

Conclusion: 多模态设置对成员推理攻击的影响具有双重性：在分布内情况下视觉输入可能略微增强攻击效果，但在分布外情况下视觉输入反而起到保护作用，掩盖了训练数据的成员信息。

Abstract: Large Multimodal Language Models (MLLMs) are emerging as one of the foundational tools in an expanding range of applications. Consequently, understanding training-data leakage in these systems is increasingly critical. Log-probability-based membership inference attacks (MIAs) have become a widely adopted approach for assessing data exposure in large language models (LLMs), yet their effect in MLLMs remains unclear. We present the first comprehensive evaluation of extending these text-based MIA methods to multimodal settings. Our experiments under vision-and-text (V+T) and text-only (T-only) conditions across the DeepSeek-VL and InternVL model families show that in in-distribution settings, logit-based MIAs perform comparably across configurations, with a slight V+T advantage. Conversely, in out-of-distribution settings, visual inputs act as regularizers, effectively masking membership signals.

</details>


### [4] [Immunity memory-based jailbreak detection: multi-agent adaptive guard for large language models](https://arxiv.org/abs/2512.03356)
*Jun Leng,Litian Zhang,Xi Zhang*

Main category: cs.CR

TL;DR: 论文提出MAAG框架，通过模拟免疫记忆机制来检测LLM越狱攻击，使用多智能体协同工作，在历史攻击模式记忆的基础上实现快速准确检测。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型存在对抗性越狱攻击漏洞，现有检测方法通常需要微调模型参数，计算成本高且难以应对新型攻击。受免疫记忆机制启发，需要一种能够记忆攻击模式并自适应更新的检测框架。

Method: 提出多智能体自适应防护框架MAAG：1）从输入提示中提取激活值，与存储在记忆库中的历史激活值比较进行初步检测；2）防御智能体基于检测结果模拟响应；3）辅助智能体监督模拟过程，对检测结果进行二次过滤。系统具备记忆能力，能够学习并记住新型攻击模式。

Result: 在五个开源模型上的广泛实验表明，MAAG显著优于现有最先进方法，在多样化攻击场景中达到98%的检测准确率和96%的F1分数。

Conclusion: MAAG框架通过模拟免疫记忆机制，为LLM越狱攻击检测提供了一种高效、自适应的解决方案，能够在保持高检测性能的同时降低计算成本。

Abstract: Large language models (LLMs) have become foundational in AI systems, yet they remain vulnerable to adversarial jailbreak attacks. These attacks involve carefully crafted prompts that bypass safety guardrails and induce models to produce harmful content. Detecting such malicious input queries is therefore critical for maintaining LLM safety. Existing methods for jailbreak detection typically involve fine-tuning LLMs as static safety LLMs using fixed training datasets. However, these methods incur substantial computational costs when updating model parameters to improve robustness, especially in the face of novel jailbreak attacks. Inspired by immunological memory mechanisms, we propose the Multi-Agent Adaptive Guard (MAAG) framework for jailbreak detection. The core idea is to equip guard with memory capabilities: upon encountering novel jailbreak attacks, the system memorizes attack patterns, enabling it to rapidly and accurately identify similar threats in future encounters. Specifically, MAAG first extracts activation values from input prompts and compares them to historical activations stored in a memory bank for quick preliminary detection. A defense agent then simulates responses based on these detection results, and an auxiliary agent supervises the simulation process to provide secondary filtering of the detection outcomes. Extensive experiments across five open-source models demonstrate that MAAG significantly outperforms state-of-the-art (SOTA) methods, achieving 98% detection accuracy and a 96% F1-score across a diverse range of attack scenarios.

</details>


### [5] [Scaling Trust in Quantum Federated Learning: A Multi-Protocol Privacy Design](https://arxiv.org/abs/2512.03358)
*Dev Gurung,Shiva Raj Pokhrel*

Main category: cs.CR

TL;DR: 提出了一种隐私保护的量子联邦学习框架，结合奇异值分解、量子密钥分发和分析量子梯度下降技术，在保护数据和模型隐私的同时保持训练效率。


<details>
  <summary>Details</summary>
Motivation: 量子联邦学习结合量子计算和分布式机器学习具有巨大潜力，但数据和模型的隐私保护仍然是一个关键挑战。现有方法在保护隐私的同时往往牺牲训练效率，需要一种既能保护隐私又能保持效率的解决方案。

Method: 提出多层隐私保护协议框架：1) 使用奇异值分解保护数据准备阶段；2) 利用量子密钥分发保护模型共享；3) 采用分析量子梯度下降进行安全训练。该框架支持n个量子设备在本地训练模型后传输到中央服务器。

Result: 通过理论分析和在当代量子平台及数据集上的实验验证，该框架能够有效保护数据和模型的机密性，同时保持训练效率。实验证明隐私保护机制对模型性能影响有限。

Conclusion: 该隐私保护量子联邦学习框架成功解决了量子分布式学习中的隐私挑战，为安全高效的量子机器学习应用提供了可行的技术方案，平衡了隐私保护和训练效率的需求。

Abstract: Quantum Federated Learning (QFL) promises to revolutionize distributed machine learning by combining the computational power of quantum devices with collaborative model training. Yet, privacy of both data and models remains a critical challenge. In this work, we propose a privacy-preserving QFL framework where a network of $n$ quantum devices trains local models and transmits them to a central server under a multi-layered privacy protocol. Our design leverages Singular Value Decomposition (SVD), Quantum Key Distribution (QKD), and Analytic Quantum Gradient Descent (AQGD) to secure data preparation, model sharing, and training stages. Through theoretical analysis and experiments on contemporary quantum platforms and datasets, we demonstrate that the framework robustly safeguards data and model confidentiality while maintaining training efficiency.

</details>


### [6] [Tuning for TraceTarnish: Techniques, Trends, and Testing Tangible Traits](https://arxiv.org/abs/2512.03465)
*Robert Dilworth*

Main category: cs.CR

TL;DR: TraceTarnish攻击脚本利用对抗性文体测量学原理匿名化文本消息作者身份，通过分析Reddit评论数据，识别出5个关键文体特征作为攻击检测指标。


<details>
  <summary>Details</summary>
Motivation: 研究旨在更严格评估TraceTarnish攻击脚本的有效性，该脚本利用对抗性文体测量学原理来匿名化文本消息的作者身份，以保护用户隐私或隐藏真实作者。

Method: 收集并处理Reddit评论数据，使用TraceTarnish进行文本转换，通过StyloMetrix提取文体特征，利用信息增益准则筛选最具信息性、预测性和区分性的特征。

Result: 识别出5个关键文体特征：功能词及其类型(L_FUNC_A & L_FUNC_T)、内容词及其类型(L_CONT_A & L_CONT_T)、以及类型-标记比率(ST_TYPE_TOKEN_RATIO_LEMMAS)，这些特征可作为攻击检测的可靠指标。

Conclusion: 这些文体特征可作为攻击检测的可靠指标和取证信标，但需要原始文本进行前后对比。研究基于这些特征改进了TraceTarnish攻击，验证了"试图抹去痕迹往往会留下更大痕迹"的论点。

Abstract: In this study, we more rigorously evaluated our attack script $\textit{TraceTarnish}$, which leverages adversarial stylometry principles to anonymize the authorship of text-based messages. To ensure the efficacy and utility of our attack, we sourced, processed, and analyzed Reddit comments--comments that were later alchemized into $\textit{TraceTarnish}$ data--to gain valuable insights. The transformed $\textit{TraceTarnish}$ data was then further augmented by $\textit{StyloMetrix}$ to manufacture stylometric features--features that were culled using the Information Gain criterion, leaving only the most informative, predictive, and discriminative ones. Our results found that function words and function word types ($L\_FUNC\_A$ $\&$ $L\_FUNC\_T$); content words and content word types ($L\_CONT\_A$ $\&$ $L\_CONT\_T$); and the Type-Token Ratio ($ST\_TYPE\_TOKEN\_RATIO\_LEMMAS$) yielded significant Information-Gain readings. The identified stylometric cues--function-word frequencies, content-word distributions, and the Type-Token Ratio--serve as reliable indicators of compromise (IoCs), revealing when a text has been deliberately altered to mask its true author. Similarly, these features could function as forensic beacons, alerting defenders to the presence of an adversarial stylometry attack; granted, in the absence of the original message, this signal may go largely unnoticed, as it appears to depend on a pre- and post-transformation comparison. "In trying to erase a trace, you often imprint a larger one." Armed with this understanding, we framed $\textit{TraceTarnish}$'s operations and outputs around these five isolated features, using them to conceptualize and implement enhancements that further strengthen the attack.

</details>


### [7] [A User Centric Group Authentication Scheme for Secure Communication](https://arxiv.org/abs/2512.03551)
*Oylum Gerenli,Gunes Karabulut-Kurt,Enver Ozdemir*

Main category: cs.CR

TL;DR: 本文提出了一种改进的第三代群认证方案，使用内积空间和多项式插值来解决用户匿名性带来的识别问题，同时防止合法成员恶意共享凭证。


<details>
  <summary>Details</summary>
Motivation: 第三代群认证方案虽然提供了用户匿名性，但在需要识别参与用户的应用场景中存在局限性。此外，现有方案允许成员共享群凭证，这会危及群组机密性。

Method: 提出改进的第三代群认证方案，结合内积空间和多项式插值技术，消除用户分发凭证的能力，同时解决用户识别问题。

Result: 提出的方案能够防止合法成员恶意共享凭证，保护群组机密性，同时在某些需要用户识别的应用场景中提供解决方案。

Conclusion: 改进的第三代群认证方案解决了用户匿名性带来的识别问题，并增强了安全性，但存在依赖中央权威机构进行认证的潜在局限性。

Abstract: Group Authentication Schemes (GAS) are methodologies developed to verify the membership of multiple users simultaneously. These schemes enable the concurrent authentication of several users while eliminating the need for a certification authority. Numerous GAS methods have been explored in the literature, and they can be classified into three distinct generations based on their foundational mathematical principles. First-generation GASs rely on polynomial interpolation and the multiplicative subgroup of a finite field. Second-generation GASs also employ polynomial interpolation, but they distinguish themselves by incorporating elliptic curves over finite fields. While third-generation GASs present a promising solution for scalable environments, they demonstrate a limitation in certain applications. Such applications typically require the identification of users participating in the authentication process. In the third-generation GAS, users are able to verify their credentials while maintaining anonymity. However, there are various applications where the identification of participating users is necessary. In this study, we propose an improved version of third-generation GAS, utilizing inner product spaces and polynomial interpolation to resolve this limitation. We address the issue of preventing malicious actions by legitimate group members. The current third-generation scheme allows members to share group credentials, which can jeopardize group confidentiality. Our proposed scheme mitigates this risk by eliminating the ability of individual users to distribute credentials. However, a potential limitation of our scheme is its reliance on a central authority for authentication in certain scenarios.

</details>


### [8] [Towards Privacy-Preserving Range Queries with Secure Learned Spatial Index over Encrypted Data](https://arxiv.org/abs/2512.03669)
*Zuan Wang,Juntao Lu,Jiazhuang Wu,Youliang Tian,Wei Song,Qiuxian Li,Duo Zhang*

Main category: cs.CR

TL;DR: 提出了一种基于安全学习空间索引的隐私保护范围查询方案SLRQ，在保证数据集、查询、结果和访问模式隐私的同时，显著提升了查询效率。


<details>
  <summary>Details</summary>
Motivation: 随着云服务在大规模数据管理中的广泛应用，保护外包数据集的安全和隐私变得至关重要。虽然加密数据和查询可以防止直接内容暴露，但攻击者仍可通过访问模式和搜索路径分析推断敏感信息。现有提供强访问模式隐私的解决方案通常带来显著的性能开销。

Method: 提出了安全学习空间索引（SLS-INDEX），将Paillier密码系统与分层预测架构和噪声注入桶相结合，实现加密域中的数据感知查询加速。基于SLS-INDEX的范围查询（SLRQ）采用基于置换的安全桶预测协议来混淆查询执行路径，并引入安全点提取协议生成候选结果以减少安全计算开销。

Result: 在真实世界和合成数据集上的广泛实验表明，SLRQ在查询效率方面显著优于现有解决方案，同时确保数据集、查询、结果和访问模式的隐私。提供了在现实泄漏函数下的形式化安全分析。

Conclusion: 提出的SLRQ方案在保证强安全性的同时实现了高性能，解决了现有隐私保护范围查询方案中安全性与效率之间的权衡问题，为云环境中的安全数据管理提供了实用解决方案。

Abstract: With the growing reliance on cloud services for large-scale data management, preserving the security and privacy of outsourced datasets has become increasingly critical. While encrypting data and queries can prevent direct content exposure, recent research reveals that adversaries can still infer sensitive information via access pattern and search path analysis. However, existing solutions that offer strong access pattern privacy often incur substantial performance overhead. In this paper, we propose a novel privacy-preserving range query scheme over encrypted datasets, offering strong security guarantees while maintaining high efficiency. To achieve this, we develop secure learned spatial index (SLS-INDEX), a secure learned index that integrates the Paillier cryptosystem with a hierarchical prediction architecture and noise-injected buckets, enabling data-aware query acceleration in the encrypted domain. To further obfuscate query execution paths, SLS-INDEXbased Range Queries (SLRQ) employs a permutation-based secure bucket prediction protocol. Additionally, we introduce a secure point extraction protocol that generates candidate results to reduce the overhead of secure computation. We provide formal security analysis under realistic leakage functions and implement a prototype to evaluate its practical performance. Extensive experiments on both real-world and synthetic datasets demonstrate that SLRQ significantly outperforms existing solutions in query efficiency while ensuring dataset, query, result, and access pattern privacy.

</details>


### [9] [Context-Aware Hierarchical Learning: A Two-Step Paradigm towards Safer LLMs](https://arxiv.org/abs/2512.03720)
*Tengyun Ma,Jiaqi Yao,Daojing He,Shihao Peng,Yu Li,Shaohui Liu,Zhuotao Tian*

Main category: cs.CR

TL;DR: 本文提出了一种针对大语言模型的新攻击方法——工具完成攻击(TCA)，并开发了相应的安全评估基准。为应对此类威胁，作者提出了上下文感知分层学习(CAHL)机制，能显著提升模型对抗攻击的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在处理指令时存在安全漏洞，特别是在对抗性场景下。当前模型统一的令牌处理范式在面对工具调用机制时容易受到攻击，需要新的防御机制来增强模型的安全性。

Method: 首先识别并提出工具完成攻击(TCA)这一新型漏洞类别，然后开发Tool-Completion基准来评估模型鲁棒性。为防御此类攻击，提出了上下文感知分层学习(CAHL)机制，该机制通过动态平衡语义理解和角色特定指令约束，利用不同指令段之间的上下文相关性建立鲁棒的上下文感知指令层次结构。

Result: 实验表明，即使是当前最先进的大语言模型也对TCA攻击表现出高度脆弱性，攻击成功率惊人地高。而CAHL机制能显著提升模型对抗传统攻击和TCA攻击的鲁棒性，在零样本评估中表现出强大的泛化能力，同时保持模型在通用任务上的性能。

Conclusion: 工具完成攻击(TCA)暴露了大语言模型在指令处理中的关键安全漏洞。提出的上下文感知分层学习(CAHL)机制能有效防御此类攻击，为增强大语言模型的安全性提供了有前景的解决方案，同时不影响模型的核心功能。

Abstract: Large Language Models (LLMs) have emerged as powerful tools for diverse applications. However, their uniform token processing paradigm introduces critical vulnerabilities in instruction handling, particularly when exposed to adversarial scenarios. In this work, we identify and propose a novel class of vulnerabilities, termed Tool-Completion Attack (TCA), which exploits function-calling mechanisms to subvert model behavior. To evaluate LLM robustness against such threats, we introduce the Tool-Completion benchmark, a comprehensive security assessment framework, which reveals that even state-of-the-art models remain susceptible to TCA, with surprisingly high attack success rates. To address these vulnerabilities, we introduce Context-Aware Hierarchical Learning (CAHL), a sophisticated mechanism that dynamically balances semantic comprehension with role-specific instruction constraints. CAHL leverages the contextual correlations between different instruction segments to establish a robust, context-aware instruction hierarchy. Extensive experiments demonstrate that CAHL significantly enhances LLM robustness against both conventional attacks and the proposed TCA, exhibiting strong generalization capabilities in zero-shot evaluations while still preserving model performance on generic tasks. Our code is available at https://github.com/S2AILab/CAHL.

</details>


### [10] [The Treasury Proof Ledger: A Cryptographic Framework for Accountable Bitcoin Treasuries](https://arxiv.org/abs/2512.03765)
*Jose E. Puente,Carlos Puente*

Main category: cs.CR

TL;DR: TPL是一个基于比特币的记账框架，帮助上市公司和机构投资者在不暴露内部钱包结构或交易策略的情况下证明偿付能力、管理风险并满足监管要求。


<details>
  <summary>Details</summary>
Motivation: 上市公司和机构投资者持有比特币面临越来越大的压力，需要在不暴露内部钱包结构或交易策略的情况下证明偿付能力、管理风险并满足监管期望。

Method: 引入Treasury Proof Ledger（TPL）框架，将链上和链下风险敞口视为具有明确费用池的守恒状态机，记录储备证明快照、跨域移动的传输证明收据以及策略元数据，并支持基于利益相关者权限的受限视图。

Result: 定义了理想化的TPL模型，将比特币资金库表示为多域风险敞口向量，并提出了部署级别的安全概念，包括风险敞口健全性、策略完整性、非抵赖性和隐私兼容的策略视图。

Conclusion: TPL框架展示了在设定经济和治理假设后，通过结合标准储备证明、传输证明技术和基于比特币的哈希承诺，可以实现哪些保障措施，支持负责任的透明度政策和未来跨机构检查。

Abstract: Public companies and institutional investors that hold Bitcoin face increasing pressure to show solvency, manage risk, and satisfy regulatory expectations without exposing internal wallet structures or trading strategies. This paper introduces the Treasury Proof Ledger (TPL), a Bitcoin-anchored logging framework for multi-domain Bitcoin treasuries that treats on-chain and off-chain exposures as a conserved state machine with an explicit fee sink. A TPL instance records proof-of-reserves snapshots, proof-of-transit receipts for movements between domains, and policy metadata, and it supports restricted views based on stakeholder permissions. We define an idealised TPL model, represent Bitcoin treasuries as multi-domain exposure vectors, and give deployment-level security notions including exposure soundness, policy completeness, non-equivocation, and privacy-compatible policy views. We then outline how practical, restricted forms of these guarantees can be achieved by combining standard proof-of-reserves and proof-of-transit techniques with hash-based commitments anchored on Bitcoin. The results are existence-type statements: they show which guarantees are achievable once economic and governance assumptions are set, without claiming that any current system already provides them. A stylised corporate-treasury example illustrates how TPL could support responsible transparency policies and future cross-institution checks consistent with Bitcoin's fixed monetary supply.

</details>


### [11] ["MCP Does Not Stand for Misuse Cryptography Protocol": Uncovering Cryptographic Misuse in Model Context Protocol at Scale](https://arxiv.org/abs/2512.03775)
*Biwei Yan,Yue Zhang,Minghui Xu,Hao Wu,Yechao Zhang,Kun Li,Guoming Zhang,Xiuzhen Cheng*

Main category: cs.CR

TL;DR: MICRYSCOPE是一个专门用于检测MCP（模型上下文协议）实现中密码学误用的框架，通过分析9403个MCP服务器发现19.7%存在密码学误用问题。


<details>
  <summary>Details</summary>
Motivation: MCP作为LLM应用中间件缺乏内置安全机制，开发者需要自行实现密码学功能，但临时实践容易导致误用，威胁敏感数据和服务安全。

Method: MICRYSCOPE结合三个关键技术：跨语言中间表示统一密码学API、混合依赖分析揭示显隐函数关系（包括LLM编排的不安全运行时组合）、基于污点分析的误用检测器追踪敏感数据流并标记违反密码学规则的行为。

Result: 分析9403个MCP服务器发现720个包含密码学逻辑，其中19.7%存在误用。问题集中在特定市场（如Smithery Registry有42%不安全服务器）、语言（Python误用率34%）和类别（开发工具与数据科学/ML占所有误用的50%以上）。案例研究揭示了API密钥泄露、不安全的DES/ECB工具和基于MD5的身份验证绕过等实际后果。

Conclusion: 该研究首次建立了MCP生态系统中密码学误用的全景视图，为加强这一快速增长协议的安全基础提供了工具和见解。

Abstract: The Model Context Protocol (MCP) is rapidly emerging as the middleware for LLM-based applications, offering a standardized interface for tool integration. However, its built-in security mechanisms are minimal: while schemas and declarations prevent malformed requests, MCP provides no guarantees of authenticity or confidentiality, forcing developers to implement cryptography themselves. Such ad hoc practices are historically prone to misuse, and within MCP they threaten sensitive data and services. We present MICRYSCOPE, the first domain-specific framework for detecting cryptographic misuses in MCP implementations. MICRYSCOPE combines three key innovations: a cross-language intermediate representation that normalizes cryptographic APIs across diverse ecosystems, a hybrid dependency analysis that uncovers explicit and implicit function relationships (including insecure runtime compositions orchestrated by LLMs) and a taint-based misuse detector that tracks sensitive data flows and flags violations of established cryptographic rules. Applying MICRYSCOPE to 9,403 MCP servers, we identified 720 with cryptographic logic, of which 19.7% exhibited misuses. These flaws are concentrated in certain markets (e.g., Smithery Registry with 42% insecure servers), languages (Python at 34% misuse rate), and categories (Developer Tools and Data Science & ML accounting for over 50% of all misuses). Case studies reveal real-world consequences, including leaked API keys, insecure DES/ECB tools, and MD5-based authentication bypasses. Our study establishes the first ecosystem-wide view of cryptographic misuse in MCP and provides both tools and insights to strengthen the security foundations of this rapidly growing protocol.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [12] [Exploring the Potential and Limitations of Large Language Models for Novice Program Fault Localization](https://arxiv.org/abs/2512.03421)
*Hexiang Xu,Hengyuan Liu,Yonghao Wu,Xiaolan Kang,Xiang Chen,Yong Liu*

Main category: cs.SE

TL;DR: 该研究评估了13个大语言模型在故障定位任务上的表现，发现具有推理能力的先进模型（如OpenAI o3和DeepSeekR1）在准确性上表现优异，而缺乏推理能力的模型需要精心设计的提示词。虽然LLMs在简单故障定位中表现良好，但随着问题难度增加准确性下降，且存在过度推理和计算成本高的问题。


<details>
  <summary>Details</summary>
Motivation: 新手程序员由于经验有限，在故障定位方面面临挑战。传统的故障定位方法（如SBFL和MBFL）缺乏对代码上下文的理解能力，对初学者效果有限。近年来，大语言模型展现出通过理解程序语法和语义来克服这些限制的潜力。

Method: 研究评估了6个闭源和7个开源的大语言模型，使用了Codeflaws、Condefects和BugT三个数据集，其中BugT是新构建的专门用于缓解数据泄露问题的数据集。研究比较了不同模型在故障定位任务上的表现，特别关注具有推理能力的模型与缺乏推理能力模型的差异。

Result: 具有推理能力的先进模型（如OpenAI o3和DeepSeekR1）在准确性上表现优异，对提示工程的依赖最小。缺乏推理能力的模型（如GPT-4）需要精心设计的提示词来保持性能。LLMs在简单故障定位中表现良好，但随着问题难度增加准确性下降，尽管顶级模型在BugT数据集上保持稳健性能。过度推理和计算成本是主要挑战。

Conclusion: LLMs在提高调试效率方面具有潜力，特别是对新手程序员的辅助价值显著。然而，需要在推理能力和计算效率方面进一步改进才能实现实际应用。LLMs的解释对一年经验参与者具有很高价值，但过度推理和计算成本仍是实际部署的障碍。

Abstract: Novice programmers often face challenges in fault localization due to their limited experience and understanding of programming syntax and logic. Traditional methods like Spectrum-Based Fault Localization (SBFL) and Mutation-Based Fault Localization (MBFL) help identify faults but often lack the ability to understand code context, making them less effective for beginners. In recent years, Large Language Models (LLMs) have shown promise in overcoming these limitations by utilizing their ability to understand program syntax and semantics. LLM-based fault localization provides more accurate and context-aware results than traditional techniques. This study evaluates six closed-source and seven open-source LLMs using the Codeflaws, Condefects, and BugT datasets, with BugT being a newly constructed dataset specifically designed to mitigate data leakage concerns. Advanced models with reasoning capabilities, such as OpenAI o3 and DeepSeekR1, achieve superior accuracy with minimal reliance on prompt engineering. In contrast, models without reasoning capabilities, like GPT-4, require carefully designed prompts to maintain performance. While LLMs perform well in simple fault localization, their accuracy decreases as problem difficulty increases, though top models maintain robust performance in the BugT dataset. Over-reasoning is another challenge, where some models generate excessive explanations that hinder fault localization clarity. Additionally, the computational cost of deploying LLMs remains a significant barrier for real-time debugging. LLM's explanations demonstrate significant value for novice programmer assistance, with one-year experience participants consistently rating them highly. Our findings demonstrate the potential of LLMs to improve debugging efficiency while stressing the need for further refinement in their reasoning and computational efficiency for practical adoption.

</details>


### [13] [Runnable Directories: The Solution to the Monorepo vs. Multi-repo Debate](https://arxiv.org/abs/2512.03815)
*Shayan Ghasemnezhad,Samarth KaPatel,Sofia Nikiforova,Giacinto Paolo Saggese,Paul Smith,Heanh Sok*

Main category: cs.SE

TL;DR: Causify Dev系统提出了一种混合方法，通过可运行目录的概念，在单一代码库和多代码库策略之间找到平衡点，解决现代软件系统的组织挑战。


<details>
  <summary>Details</summary>
Motivation: 现代软件系统对传统代码库组织策略提出了挑战。单一代码库（monorepo）提供一致性但存在可扩展性和工具复杂性，而多代码库（multi-repo）提供模块化但面临协调和依赖管理问题。

Method: 提出Causify Dev系统，核心概念是"可运行目录"——自包含、独立可执行的单元，拥有自己的开发、测试和部署生命周期。系统包含统一的轻量级环境、共享辅助工具和基于Docker的容器化工作流。

Result: 可运行目录支持一致的设置、隔离的依赖和高效的CI/CD流程。Causify Dev方法为单一代码库和多代码库策略提供了实用的中间方案。

Conclusion: Causify Dev方法改善了不断增长、复杂代码库的可靠性和可维护性，在单一代码库和多代码库策略之间找到了平衡点。

Abstract: Modern software systems increasingly strain traditional codebase organization strategies. Monorepos offer consistency but often suffer from scalability issues and tooling complexity, while multi-repos provide modularity at the cost of coordination and dependency management challenges. As an answer to this trade-off, we present the Causify Dev system, a hybrid approach that integrates key benefits of both. Its central concept is the runnable directory -- a self-contained, independently executable unit with its own development, testing, and deployment lifecycles. Backed by a unified thin environment, shared helper utilities, and containerized Docker-based workflows, runnable directories enable consistent setups, isolated dependencies, and efficient CI/CD processes. The Causify Dev approach provides a practical middle ground between monorepo and multi-repo strategies, improving reliability and maintainability for growing, complex codebases.

</details>


### [14] [A Comprehensive Study on the Impact of Vulnerable Dependencies on Open-Source Software](https://arxiv.org/abs/2512.03868)
*Shree Hari Bittugondanahalli Indra Kumar,Lilia Rodrigues Sampaio,André Martin,Andrey Brito,Christof Fetzer*

Main category: cs.SE

TL;DR: 对1000多个开源项目的研究发现，依赖漏洞通常是传递性的，关键漏洞平均需要一年多才能修复，项目指标与漏洞修复速度相关。


<details>
  <summary>Details</summary>
Motivation: 开源库广泛使用但可能引入安全漏洞（如Log4Shell），随着开源库使用增加，理解和解决依赖漏洞变得更为重要。SCA工具能提供深入洞察，但需要了解漏洞的普遍性和修复速度。

Method: 使用VODA SCA工具爬取1000多个GitHub项目（2013-2023年版本历史），涵盖Java、Python、Rust、Go、Ruby、PHP和JavaScript等语言，分析约5万个版本。研究包括库版本、依赖深度、已知漏洞及其演变。

Result: 大多数编程语言中，漏洞依赖是传递性的；关键漏洞平均持续一年以上才被修复；数据集比先前研究更大更多样化，结果更具普遍性；回答了关于依赖深度和漏洞平均持续时间的多个研究问题。

Conclusion: 开源项目依赖漏洞普遍存在且修复缓慢，特别是传递性依赖和关键漏洞。项目指标（团队规模、贡献者数量、活动频率、发布周期）与漏洞修复速度相关，需要更有效的漏洞管理策略。

Abstract: Open-source libraries are widely used by software developers to speed up the development of products, however, they can introduce security vulnerabilities, leading to incidents like Log4Shell. With the expanding usage of open-source libraries, it becomes even more imperative to comprehend and address these dependency vulnerabilities. The use of Software Composition Analysis (SCA) tools does greatly help here as they provide a deep insight on what dependencies are used in a project, enhancing the security and integrity in the software supply chain. In order to learn how wide spread vulnerabilities are and how quickly they are being fixed, we conducted a study on over 1k open-source software projects with about 50k releases comprising several languages such as Java, Python, Rust, Go, Ruby, PHP, and JavaScript. Our objective is to investigate the severity, persistence, and distribution of these vulnerabilities, as well as their correlation with project metrics such as team and contributors size, activity and release cycles. In order to perform such analysis, we crawled over 1k projects from github including their version history ranging from 2013 to 2023 using VODA, our SCA tool. Using our approach, we can provide information such as library versions, dependency depth, and known vulnerabilities, and how they evolved over the software development cycle. Being larger and more diverse than datasets used in earlier works and studies, ours provides better insights and generalizability of the gained results. The data collected answers several research questions about the dependency depth and the average time a vulnerability persists. Among other findings, we observed that for most programming languages, vulnerable dependencies are transitive, and a critical vulnerability persists in average for over a year before being fixed.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [15] [Exploring Syntropic Frameworks in AI Alignment: A Philosophical Investigation](https://arxiv.org/abs/2512.03048)
*Austin Spizzirri*

Main category: cs.AI

TL;DR: 该论文主张将AI对齐重新构想为通过基于过程、多智能体、发展性机制来构建具有熵减性、理由响应能力的智能体，而非编码固定的人类价值内容。


<details>
  <summary>Details</summary>
Motivation: 传统基于内容的价值规范方法存在结构不稳定性问题，作者认为需要新的理论框架来解决AI对齐的根本挑战。

Method: 提出三个哲学贡献：1) 阐述"规范陷阱"论证；2) 提出"熵减性"作为多智能体对齐的信息论框架；3) 建立真实与模拟道德能力的功能区分，基于兼容论指导控制理论。

Result: 建立了新的AI对齐理论框架，提出了可证伪的预测，但实证验证仍在进行中。

Conclusion: AI对齐应转向过程导向、多智能体、发展性的方法，而非内容规范，该框架为实证研究提供了哲学基础。

Abstract: I argue that AI alignment should be reconceived as architecting syntropic, reasons-responsive agents through process-based, multi-agent, developmental mechanisms rather than encoding fixed human value content. The paper makes three philosophical contributions. First, I articulate the ``specification trap'' argument demonstrating why content-based value specification appears structurally unstable due to the conjunction of the is-ought gap, value pluralism, and the extended frame problem. Second, I propose syntropy -- the recursive reduction of mutual uncertainty between agents through state alignment -- as an information-theoretic framework for understanding multi-agent alignment dynamics. Third, I establish a functional distinction between genuine and simulated moral capacity grounded in compatibilist theories of guidance control, coupled with an embodied experimental paradigm and verification regime providing operational criteria independent of phenomenological claims. This paper represents the philosophical component of a broader research program whose empirical validation is being developed in a separate project currently in preparation. While the framework generates specific, falsifiable predictions about value emergence and moral agency in artificial systems, empirical validation remains pending.

</details>


### [16] [Beyond the Black Box: A Cognitive Architecture for Explainable and Aligned AI](https://arxiv.org/abs/2512.03072)
*Hu Keyi*

Main category: cs.AI

TL;DR: 本文提出了一种名为"权重计算主义"的新型认知架构，基于第一原理构建，旨在解决当前AI在可解释性和价值对齐方面的挑战，为实现可信赖的通用人工智能提供可行路径。


<details>
  <summary>Details</summary>
Motivation: 当前AI范式作为"体验架构师"面临可解释性和价值对齐的根本性挑战。需要一种基于第一原理的认知架构，能够实现彻底的可解释性、对新情境的内在泛化能力以及可追溯的价值对齐。

Method: 提出"权重计算主义"认知架构，将认知分解为不可分割的逻辑原子和两个基本操作：指向和比较。通过可解释的权重计算模型（权重=收益×概率）形式化决策过程，所有值都可追溯到可审计的初始权重集。采用基于图算法的计算引擎和全局工作空间工作流实现。

Result: 该架构实现了透明、类人的推理能力，并在前所未有的场景中展现出强大的学习能力。初步代码实现和场景验证表明，该架构为构建可信赖和对齐的AGI奠定了实践和理论基础。

Conclusion: 权重计算主义架构为解决AI可解释性和价值对齐问题提供了可行路径，为实现可信赖的通用人工智能建立了坚实的理论和实践基础，展示了作为AGI可行途径的潜力。

Abstract: Current AI paradigms, as "architects of experience," face fundamental challenges in explainability and value alignment. This paper introduces "Weight-Calculatism," a novel cognitive architecture grounded in first principles, and demonstrates its potential as a viable pathway toward Artificial General Intelligence (AGI). The architecture deconstructs cognition into indivisible Logical Atoms and two fundamental operations: Pointing and Comparison. Decision-making is formalized through an interpretable Weight-Calculation model (Weight = Benefit * Probability), where all values are traceable to an auditable set of Initial Weights. This atomic decomposition enables radical explainability, intrinsic generality for novel situations, and traceable value alignment. We detail its implementation via a graph-algorithm-based computational engine and a global workspace workflow, supported by a preliminary code implementation and scenario validation. Results indicate that the architecture achieves transparent, human-like reasoning and robust learning in unprecedented scenarios, establishing a practical and theoretical foundation for building trustworthy and aligned AGI.

</details>


### [17] [When Do Symbolic Solvers Enhance Reasoning in Large Language Models?](https://arxiv.org/abs/2512.03272)
*Zhiyuan He,Dingmin Wang*

Main category: cs.AI

TL;DR: 该研究探讨了符号求解器集成方法何时能增强传统长推理链的性能，发现符号求解器仅在问题需要有限隐式推理但涉及充足搜索空间时有效，特别是在需要重复回溯的约束满足问题上。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型通过生成长推理链在复杂推理任务上表现良好，但这种方法会产生大量token开销，且模型可能"过度思考"产生冗长推理链甚至导致错误答案。符号求解器集成方法利用LLM的代码生成能力将推理任务转化为可执行代码，然后用符号求解器解决，但何时这种集成方法能增强传统长推理链仍是一个开放问题。

Method: 采用符号求解器集成方法，利用LLM的代码生成能力将推理任务翻译成可执行代码，然后使用符号求解器解决。通过实验比较传统长推理链方法与符号求解器集成方法在不同类型问题上的表现。

Result: 符号求解器集成方法仅在问题需要有限隐式推理但涉及充足搜索空间时有效。最新LLM（如GPT-4o）在推理深度较浅的演绎问题上表现更好，而符号求解器集成方法在需要重复回溯的约束满足问题上显著提升LLM性能。当提供声明性示例时，即使是CodeLlama-13B也能在困难的斑马谜题上超越GPT-4o。

Conclusion: 符号求解器集成方法对特定类型问题（如约束满足问题）有显著优势，特别是在需要大量搜索和回溯的场景中。这种方法可以补充传统长推理链方法，但适用性取决于问题的具体特征，特别是隐式推理需求和搜索空间大小。

Abstract: Large Reasoning Models (LRMs) achieve strong performance on complex reasoning tasks by generating long Chains of Thought (CoTs). However, this paradigm might incur substantial token overhead, especially when models "overthink" by producing lengthy reasoning chains, which can even lead to incorrect answers. A promising direction is the symbolic-solver-integrated approach, which leverages the code generation capabilities of LLMs to translate reasoning tasks into executable code and then solve them with a symbolic solver. In this paper, we explore an open question of when the conventional long-CoT can be enhanced by symbolic solvers. Our experimental results show that the symbolic-solver-integrated method only helps when the problem requires limited implicit reasoning but involves an ample search space. The latest LLMs, like GPT-4o, show better performance on deductive problems with shallow reasoning depth, while the symbolic-solver-integrated method significantly improves the LLMs' performance in constraint satisfaction problems that require repeated backtracks. When a declarative exemplar is provided, even CodeLlama-13B can outperform GPT-4o in difficult Zebra puzzles.

</details>


### [18] [Prior preferences in active inference agents: soft, hard, and goal shaping](https://arxiv.org/abs/2512.03293)
*Filippo Torresan,Ryota Kanai,Manuel Baltieri*

Main category: cs.AI

TL;DR: 该研究比较了主动推理中四种偏好分布定义方式在网格世界导航任务中的表现，发现目标塑造能提升性能但会牺牲环境动态学习。


<details>
  <summary>Details</summary>
Motivation: 主动推理使用期望自由能作为规划目标来平衡探索与利用，但文献中很少关注偏好分布如何定义及其对推理和学习的影响。本研究旨在探讨不同偏好分布定义方式对智能体性能的影响。

Method: 考虑了四种定义偏好分布的方式：硬目标与软目标，以及是否包含目标塑造（中间目标）。在网格世界导航任务中比较了四种智能体的表现。

Result: 目标塑造总体上能实现最佳性能（促进利用），但会牺牲对环境转移动态的学习（阻碍探索）。

Conclusion: 偏好分布的定义方式显著影响主动推理智能体的性能平衡，目标塑造虽然提升利用效率但会减少探索，需要在具体应用中权衡这一取舍。

Abstract: Active inference proposes expected free energy as an objective for planning and decision-making to adequately balance exploitative and explorative drives in learning agents. The exploitative drive, or what an agent wants to achieve, is formalised as the Kullback-Leibler divergence between a variational probability distribution, updated at each inference step, and a preference probability distribution that indicates what states or observations are more likely for the agent, hence determining the agent's goal in a certain environment. In the literature, the questions of how the preference distribution should be specified and of how a certain specification impacts inference and learning in an active inference agent have been given hardly any attention. In this work, we consider four possible ways of defining the preference distribution, either providing the agents with hard or soft goals and either involving or not goal shaping (i.e., intermediate goals). We compare the performances of four agents, each given one of the possible preference distributions, in a grid world navigation task. Our results show that goal shaping enables the best performance overall (i.e., it promotes exploitation) while sacrificing learning about the environment's transition dynamics (i.e., it hampers exploration).

</details>


### [19] [Evaluating Generalization Capabilities of LLM-Based Agents in Mixed-Motive Scenarios Using Concordia](https://arxiv.org/abs/2512.03318)
*Chandler Smith,Marwa Abdulhai,Manfred Diaz,Marko Tesic,Rakshit S. Trivedi,Alexander Sasha Vezhnevets,Lewis Hammond,Jesse Clifton,Minsuk Chang,Edgar A. Duéñez-Guzmán,John P. Agapiou,Jayd Matyas,Danny Karmon,Akash Kundu,Aliaksei Korshuk,Ananya Ananya,Arrasy Rahman,Avinaash Anand Kulandaivel,Bain McHale,Beining Zhang,Buyantuev Alexander,Carlos Saith Rodriguez Rojas,Caroline Wang,Chetan Talele,Chenao Liu,Chichen Lin,Diana Riazi,Di Yang Shi,Emanuel Tewolde,Elizaveta Tennant,Fangwei Zhong,Fuyang Cui,Gang Zhao,Gema Parreño Piqueras,Hyeonggeun Yun,Ilya Makarov,Jiaxun Cui,Jebish Purbey,Jim Dilkes,Jord Nguyen,Lingyun Xiao,Luis Felipe Giraldo,Manuela Chacon-Chamorro,Manuel Sebastian Rios Beltran,Marta Emili García Segura,Mengmeng Wang,Mogtaba Alim,Nicanor Quijano,Nico Schiavone,Olivia Macmillan-Scott,Oswaldo Peña,Peter Stone,Ram Mohan Rao Kadiyala,Rolando Fernandez,Ruben Manrique,Sunjia Lu,Sheila A. McIlraith,Shamika Dhuri,Shuqing Shi,Siddhant Gupta,Sneheel Sarangi,Sriram Ganapathi Subramanian,Taehun Cha,Toryn Q. Klassen,Wenming Tu,Weijian Fan,Wu Ruiyang,Xue Feng,Yali Du,Yang Liu,Yiding Wang,Yipeng Kang,Yoonchang Sung,Yuxuan Chen,Zhaowei Zhang,Zhihan Wang,Zhiqiang Wu,Ziang Chen,Zilong Zheng,Zixia Jia,Ziyan Wang,Dylan Hadfield-Menell,Natasha Jaques,Tim Baarslag,Jose Hernandez-Orallo,Joel Z. Leibo*

Main category: cs.AI

TL;DR: 本文提出了一种评估LLM智能体在零样本、混合动机环境中合作能力的方法，使用Concordia自然语言多智能体模拟环境，发现当前智能体在需要说服和规范执行的场景中存在显著能力差距。


<details>
  <summary>Details</summary>
Motivation: LLM智能体在社会互动方面展现出强大能力，并越来越多地部署在与人类和人工智能体互动的场景中。然而，现有的评估方法无法衡量这些能力在新颖社会情境中的泛化能力，这是一个关键的研究前沿。

Method: 引入了一种使用Concordia自然语言多智能体模拟环境来评估LLM智能体在零样本、混合动机环境中合作能力的方法。该方法通过测试智能体在不同合作伙伴和情境中识别和利用互利机会的能力来衡量一般合作智能。

Result: 基于NeurIPS 2024 Concordia竞赛的实证结果显示，当前智能体能力与实现可靠合作所需的稳健泛化之间存在显著差距，特别是在需要说服和规范执行的场景中。

Conclusion: LLM智能体在合作能力方面仍有重要改进空间，特别是在复杂社会情境中的泛化能力。提出的评估方法为衡量和提升智能体的合作智能提供了有效框架。

Abstract: Large Language Model (LLM) agents have demonstrated impressive capabilities for social interaction and are increasingly being deployed in situations where they might engage with both human and artificial agents. These interactions represent a critical frontier for LLM-based agents, yet existing evaluation methods fail to measure how well these capabilities generalize to novel social situations. In this paper, we introduce a method for evaluating the ability of LLM-based agents to cooperate in zero-shot, mixed-motive environments using Concordia, a natural language multi-agent simulation environment. Our method measures general cooperative intelligence by testing an agent's ability to identify and exploit opportunities for mutual gain across diverse partners and contexts. We present empirical results from the NeurIPS 2024 Concordia Contest, where agents were evaluated on their ability to achieve mutual gains across a suite of diverse scenarios ranging from negotiation to collective action problems. Our findings reveal significant gaps between current agent capabilities and the robust generalization required for reliable cooperation, particularly in scenarios demanding persuasion and norm enforcement.

</details>


### [20] [Multi-Agent Reinforcement Learning with Communication-Constrained Priors](https://arxiv.org/abs/2512.03528)
*Guang Yang,Tianpei Yang,Jingwen Qiao,Yanqing Wu,Jing Huo,Xingguo Chen,Yang Gao*

Main category: cs.AI

TL;DR: 提出了一种通信受限的多智能体强化学习框架，通过区分有损和无损消息，利用双重互信息估计器量化通信消息对全局奖励的影响，以应对现实世界中普遍存在的通信损失问题。


<details>
  <summary>Details</summary>
Motivation: 现实世界多智能体系统中普遍存在通信损失问题，现有通信增强的多智能体强化学习方法由于可扩展性和鲁棒性有限，难以应用于复杂动态的真实环境。

Method: 提出广义通信受限模型统一描述不同场景的通信条件；将其作为学习先验区分有损和无损消息；使用双重互信息估计器解耦有损和无损消息对分布式决策的影响；引入通信受限的多智能体强化学习框架，将通信消息影响量化到全局奖励中。

Result: 在多个通信受限基准测试中验证了方法的有效性。

Conclusion: 提出的通信受限多智能体强化学习框架能够有效应对现实世界中的通信损失问题，提高系统在复杂动态环境中的适应性和鲁棒性。

Abstract: Communication is one of the effective means to improve the learning of cooperative policy in multi-agent systems. However, in most real-world scenarios, lossy communication is a prevalent issue. Existing multi-agent reinforcement learning with communication, due to their limited scalability and robustness, struggles to apply to complex and dynamic real-world environments. To address these challenges, we propose a generalized communication-constrained model to uniformly characterize communication conditions across different scenarios. Based on this, we utilize it as a learning prior to distinguish between lossy and lossless messages for specific scenarios. Additionally, we decouple the impact of lossy and lossless messages on distributed decision-making, drawing on a dual mutual information estimatior, and introduce a communication-constrained multi-agent reinforcement learning framework, quantifying the impact of communication messages into the global reward. Finally, we validate the effectiveness of our approach across several communication-constrained benchmarks.

</details>


### [21] [DeepRule: An Integrated Framework for Automated Business Rule Generation via Deep Predictive Modeling and Hybrid Search Optimization](https://arxiv.org/abs/2512.03607)
*Yusen Wu,Xiaotie Deng*

Main category: cs.AI

TL;DR: DeepRule是一个用于零售品类和定价优化的自动化业务规则生成框架，通过三层架构解决理论模型与现实经济复杂性之间的系统性偏差问题。


<details>
  <summary>Details</summary>
Motivation: 解决现有理论模型与现实经济复杂性之间的系统性偏差，具体包括三个关键差距：数据模态不匹配（非结构化文本源阻碍准确客户画像）、动态特征纠缠（非线性价格弹性和时变属性建模困难）、以及操作不可行性（多层业务约束导致实施困难）。

Method: 采用三层架构：1) 混合知识融合引擎，使用大语言模型对非结构化文本进行深度语义解析，将分销商协议和销售评估转化为结构化特征；2) 博弈论约束优化机制，通过双边效用函数动态协调供应链利益，将制造商-分销商利润再分配编码为层次约束下的内生目标；3) 可解释决策蒸馏接口，利用LLM引导的符号回归来发现和优化定价策略及可审计业务规则，将经济先验（如非负弹性）作为硬约束嵌入数学表达式搜索中。

Result: 在真实零售环境中验证了该框架，相比系统性B2C基线实现了更高的利润，同时确保了操作可行性。

Conclusion: 建立了一个闭环管道，统一了非结构化知识注入、多智能体优化和可解释策略合成，为真实经济智能提供了系统解决方案。

Abstract: This paper proposes DeepRule, an integrated framework for automated business rule generation in retail assortment and pricing optimization. Addressing the systematic misalignment between existing theoretical models and real-world economic complexities, we identify three critical gaps: (1) data modality mismatch where unstructured textual sources (e.g. negotiation records, approval documents) impede accurate customer profiling; (2) dynamic feature entanglement challenges in modeling nonlinear price elasticity and time-varying attributes; (3) operational infeasibility caused by multi-tier business constraints.
  Our framework introduces a tri-level architecture for above challenges. We design a hybrid knowledge fusion engine employing large language models (LLMs) for deep semantic parsing of unstructured text, transforming distributor agreements and sales assessments into structured features while integrating managerial expertise. Then a game-theoretic constrained optimization mechanism is employed to dynamically reconcile supply chain interests through bilateral utility functions, encoding manufacturer-distributor profit redistribution as endogenous objectives under hierarchical constraints. Finally an interpretable decision distillation interface leveraging LLM-guided symbolic regression to find and optimize pricing strategies and auditable business rules embeds economic priors (e.g. non-negative elasticity) as hard constraints during mathematical expression search. We validate the framework in real retail environments achieving higher profits versus systematic B2C baselines while ensuring operational feasibility. This establishes a close-loop pipeline unifying unstructured knowledge injection, multi-agent optimization, and interpretable strategy synthesis for real economic intelligence.

</details>


### [22] [RoCo: Role-Based LLMs Collaboration for Automatic Heuristic Design](https://arxiv.org/abs/2512.03762)
*Jiawei Xu,Fengfeng Wei,Weineng Chen*

Main category: cs.AI

TL;DR: RoCo是一个基于多智能体角色协作的系统，通过四个专门化的LLM智能体（探索者、利用者、批评者、整合者）协同生成高质量启发式算法，在组合优化问题的自动启发式设计中取得优越性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的自动启发式设计研究通常只考虑单一角色，限制了启发式算法的多样性和质量。需要一种多角色协作系统来增强自动启发式设计的多样性和质量。

Method: 提出RoCo多智能体角色协作系统，包含四个专门化的LLM智能体：探索者（促进长期潜力，创造性思维）、利用者（关注短期改进，保守性优化）、批评者（评估进化步骤有效性并提供反馈）、整合者（综合探索者和利用者的提案，平衡创新与利用）。这些智能体通过结构化多轮过程进行交互，包括反馈、细化和精英变异。

Result: 在五种不同组合优化问题的白盒和黑盒设置下进行评估，RoCo表现出优越性能，生成的启发式算法优于现有方法（包括ReEvo和HSEvo），在白盒和黑盒场景中均取得竞争性结果。

Conclusion: 基于角色的协作范式为鲁棒且高性能的自动启发式设计建立了新标准，通过多角色协作显著提升了启发式算法的多样性和质量。

Abstract: Automatic Heuristic Design (AHD) has gained traction as a promising solution for solving combinatorial optimization problems (COPs). Large Language Models (LLMs) have emerged and become a promising approach to achieving AHD, but current LLM-based AHD research often only considers a single role. This paper proposes RoCo, a novel Multi-Agent Role-Based System, to enhance the diversity and quality of AHD through multi-role collaboration. RoCo coordinates four specialized LLM-guided agents-explorer, exploiter, critic, and integrator-to collaboratively generate high-quality heuristics. The explorer promotes long-term potential through creative, diversity-driven thinking, while the exploiter focuses on short-term improvements via conservative, efficiency-oriented refinements. The critic evaluates the effectiveness of each evolution step and provides targeted feedback and reflection. The integrator synthesizes proposals from the explorer and exploiter, balancing innovation and exploitation to drive overall progress. These agents interact in a structured multi-round process involving feedback, refinement, and elite mutations guided by both short-term and accumulated long-term reflections. We evaluate RoCo on five different COPs under both white-box and black-box settings. Experimental results demonstrate that RoCo achieves superior performance, consistently generating competitive heuristics that outperform existing methods including ReEvo and HSEvo, both in white-box and black-box scenarios. This role-based collaborative paradigm establishes a new standard for robust and high-performing AHD.

</details>


### [23] [Omni-AutoThink: Adaptive Multimodal Reasoning via Reinforcement Learning](https://arxiv.org/abs/2512.03783)
*Dongchao Yang,Songxiang Liu,Disong Wang,Yuanyuan Wang,Guanglu Wan,Helen Meng*

Main category: cs.AI

TL;DR: 提出了Omni-AutoThink自适应推理框架，通过动态调整推理深度来解决现有Omni模型在简单问题上过度推理、复杂问题上推理不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有Omni模型在多模态感知和生成方面取得了进展，但推理行为仍然僵化，要么在简单问题上过度推理，要么在需要推理时无法有效推理。需要一种能够根据任务难度动态调整推理深度的自适应框架。

Method: 提出两阶段框架：1) 自适应监督微调阶段，使用大规模推理增强数据赋予模型基本推理能力；2) 自适应强化学习阶段，基于任务复杂度和奖励反馈优化推理行为。同时构建了涵盖文本、文本-音频、文本-视觉、文本-音频-视觉多模态的自适应推理基准。

Result: 实验结果表明，提出的框架相比先前基线显著提升了自适应推理性能。所有基准数据和代码将公开发布。

Conclusion: Omni-AutoThink框架通过自适应调整推理深度，有效解决了Omni模型推理行为僵化的问题，在多模态推理任务上表现出优越性能。

Abstract: Recent advances in Omni models have enabled unified multimodal perception and generation. However, most existing systems still exhibit rigid reasoning behaviors, either overthinking simple problems or failing to reason when necessary. To address this limitation, we propose Omni-AutoThink, a novel adaptive reasoning framework that dynamically adjusts the model's reasoning depth according to task difficulty. Our framework comprises two stages: (1) an Adaptive Supervised Fine-Tuning (Adaptive SFT) stage, which endows the Omni model with fundamental reasoning capability using large-scale reasoning-augmented data, and (2) an Adaptive Reinforcement Learning (Adaptive GRPO) stage, which optimizes reasoning behaviors based on task complexity and reward feedback. We further construct a comprehensive adaptive reasoning benchmark that spans text-only, text-audio, text-visual, and text-audio-visual modalities, providing both training and evaluation splits for multimodal reasoning assessment. Experimental results demonstrate that our proposed framework significantly improves adaptive reasoning performance compared to previous baselines. All benchmark data and code will be publicly released.

</details>


### [24] [A Hierarchical Tree-based approach for creating Configurable and Static Deep Research Agent (Static-DRA)](https://arxiv.org/abs/2512.03887)
*Saurav Prateek*

Main category: cs.AI

TL;DR: 提出Static-DRA：基于树状静态工作流的深度研究智能体，通过可配置的Depth和Breadth参数让用户平衡研究质量与计算成本


<details>
  <summary>Details</summary>
Motivation: 现有静态RAG管道在处理复杂多轮研究任务时存在局限性，需要更灵活的智能体系统来支持深度研究

Method: 构建基于树状静态工作流的Static-DRA，包含Supervisor、Independent和Worker三种智能体，通过Depth和Breadth参数控制研究深度和广度

Result: 在DeepResearch Bench上评估，Depth=2、Breadth=5配置下获得34.72分，实验验证增加Depth和Breadth能提升研究深度和评估分数

Conclusion: Static-DRA提供了实用且资源感知的解决方案，赋予用户对深度研究过程的透明控制，代码和结果已开源

Abstract: The advancement in Large Language Models has driven the creation of complex agentic systems, such as Deep Research Agents (DRAs), to overcome the limitations of static Retrieval Augmented Generation (RAG) pipelines in handling complex, multi-turn research tasks. This paper introduces the Static Deep Research Agent (Static-DRA), a novel solution built upon a configurable and hierarchical Tree-based static workflow.
  The core contribution is the integration of two user-tunable parameters, Depth and Breadth, which provide granular control over the research intensity. This design allows end-users to consciously balance the desired quality and comprehensiveness of the research report against the associated computational cost of Large Language Model (LLM) interactions. The agent's architecture, comprising Supervisor, Independent, and Worker agents, facilitates effective multi-hop information retrieval and parallel sub-topic investigation.
  We evaluate the Static-DRA against the established DeepResearch Bench using the RACE (Reference-based Adaptive Criteria-driven Evaluation) framework. Configured with a depth of 2 and a breadth of 5, and powered by the gemini-2.5-pro model, the agent achieved an overall score of 34.72. Our experiments validate that increasing the configured Depth and Breadth parameters results in a more in-depth research process and a correspondingly higher evaluation score. The Static-DRA offers a pragmatic and resource-aware solution, empowering users with transparent control over the deep research process. The entire source code, outputs and benchmark results are open-sourced at https://github.com/SauravP97/Static-Deep-Research/

</details>


### [25] [Benchmark for Planning and Control with Large Language Model Agents: Blocksworld with Model Context Protocol](https://arxiv.org/abs/2512.03955)
*Niklas Jobs,Luis Miguel Vieira da Silva,Jayanth Somashekaraiah,Maximilian Weigand,David Kube,Felix Gehlhoff*

Main category: cs.AI

TL;DR: 该论文提出了一个用于评估LLM智能体在工业自动化中规划和执行能力的标准化基准测试，包含可执行的模拟环境和五个复杂度类别的Blocksworld问题，通过MCP协议实现不同智能体架构的无缝连接和评估。


<details>
  <summary>Details</summary>
Motivation: 工业自动化需要能够适应变化任务和环境的灵活控制策略，基于大语言模型的智能体具有这种自适应规划和执行潜力，但缺乏用于系统比较的标准化基准。

Method: 引入一个包含可执行模拟环境的基准测试，代表Blocksworld问题并提供五个复杂度类别；通过集成模型上下文协议作为标准化工具接口，使不同的智能体架构能够无需特定实现修改即可连接到基准测试并进行评估。

Result: 单智能体实现展示了基准测试的适用性，为基于LLM的规划和执行方法建立了定量比较指标。

Conclusion: 该基准测试为评估和比较LLM智能体在工业自动化任务中的规划和执行能力提供了标准化框架，通过MCP协议实现了不同架构的无缝集成和系统评估。

Abstract: Industrial automation increasingly requires flexible control strategies that can adapt to changing tasks and environments. Agents based on Large Language Models (LLMs) offer potential for such adaptive planning and execution but lack standardized benchmarks for systematic comparison. We introduce a benchmark with an executable simulation environment representing the Blocksworld problem providing five complexity categories. By integrating the Model Context Protocol (MCP) as a standardized tool interface, diverse agent architectures can be connected to and evaluated against the benchmark without implementation-specific modifications. A single-agent implementation demonstrates the benchmark's applicability, establishing quantitative metrics for comparison of LLM-based planning and execution approaches.

</details>
