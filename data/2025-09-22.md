<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 9]
- [cs.CR](#cs.CR) [Total: 13]
- [cs.AI](#cs.AI) [Total: 17]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Evaluating the Limitations of Local LLMs in Solving Complex Programming Challenges](https://arxiv.org/abs/2509.15283)
*Kadin Matotek,Heather Cassel,Md Amiruzzaman,Linh B. Ngo*

Main category: cs.SE

TL;DR: 本研究评估了开源本地部署的大型语言模型在处理复杂编程竞赛任务时的表现，发现其准确率约为专有模型的一半，揭示了开源与专有模型之间的性能差距。


<details>
  <summary>Details</summary>
Motivation: 评估当前开源本地部署LLMs在复杂编程任务中的实际性能，为组织提供可在内部硬件上复现的评估框架。

Method: 改进FACE框架使其完全离线运行，使用Ollama运行时，将目录结构简化为JSON文件，增加检查点功能，对3,589个Kattis问题在8个代码导向模型上进行测试。

Result: 本地模型的整体pass@1准确率较低，最佳模型的表现约为Gemini 1.5和ChatGPT-4等专有模型接受率的一半。

Conclusion: 开源模型与最先进专有服务之间存在显著差距，但开源模型进步迅速，且本地评估框架具有实用价值。

Abstract: This study examines the performance of today's open-source, locally hosted
large-language models (LLMs) in handling complex competitive programming tasks
with extended problem descriptions and contexts. Building on the original
Framework for AI-driven Code Generation Evaluation (FACE), the authors retrofit
the pipeline to work entirely offline through the Ollama runtime, collapsing
FACE's sprawling per-problem directory tree into a handful of consolidated JSON
files, and adding robust checkpointing so multi-day runs can resume after
failures. The enhanced framework generates, submits, and records solutions for
the full Kattis corpus of 3,589 problems across eight code-oriented models
ranging from 6.7-9 billion parameters. The submission results show that the
overall pass@1 accuracy is modest for the local models, with the best models
performing at approximately half the acceptance rate of the proprietary models,
Gemini 1.5 and ChatGPT-4. These findings expose a persistent gap between
private, cost-controlled LLM deployments and state-of-the-art proprietary
services, yet also highlight the rapid progress of open models and the
practical benefits of an evaluation workflow that organizations can replicate
on in-house hardware.

</details>


### [2] [LoCaL: Countering Surface Bias in Code Evaluation Metrics](https://arxiv.org/abs/2509.15397)
*Simantika Bhattacharjee Dristi,Matthew B. Dwyer*

Main category: cs.SE

TL;DR: 本文提出LoCaL基准测试，用于评估基于参考的代码评估指标(CEMs)，发现现有CEMs存在表面特征偏差，并在LoCaL上表现出显著性能下降。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型和基于LLM的智能体日益流行，可靠的代码评估指标变得至关重要。现有参考型CEMs与功能正确性的相关性较弱，但原因未被深入探究。

Method: 提出LoCaL基准测试，包含3117个代码对，通过差分模糊测试计算功能相似度分数，无需预定义测试用例。

Result: 四个最先进的参考型CEMs在LoCaL上均表现出显著性能下降，相比基线有较大差距。

Conclusion: 将CEMs暴露于LoCaL类似数据可能有助于开发对表面偏差具有鲁棒性的评估指标。

Abstract: With the increasing popularity of large language models (LLMs) and LLM-based
agents, reliable and effective code evaluation metrics (CEMs) have become
crucial for progress across several software engineering tasks. While popular
benchmarks often provide test cases to assess the correctness of generated
code, crafting and executing test cases is expensive. Reference-based CEMs
provide a cheaper alternative by scoring a candidate program based on its
functional similarity to a reference. Although prior research has focused on
reporting the weak correlation between these CEMs and functional correctness,
the causes are only assumed, and plausible solutions remain unexplored. In this
work, we critically evaluate four state-of-the-art reference-based CEMs,
revealing their strong bias towards surface-level features rather than code
functionality. Despite this surface bias, current evaluation datasets for these
CEMs rarely include code pairs that are surface-similar yet functionally
dissimilar, or functionally similar yet surface-dissimilar. To mitigate this
gap, we propose LoCaL (Looks Can Lie), a CEM evaluation benchmark, with 3117
code pairs at both the method and program levels. Each pair is labeled with a
functional similarity score and aims to target regions where CEMs are likely to
perform poorly. The functional similarity scores are calculated through
differential fuzzing, which eliminates the need for predefined test cases and,
at the same time, improves the reliability of the scores by executing an order
of magnitude more tests than prior work. We find that all four CEMs show
significant performance degradation on LoCaL, compared to the baselines.
Finally, based on our findings, we draw the implication that exposing CEMs to
LoCaL-like data might facilitate the development of metrics that are robust to
surface bias.

</details>


### [3] [Brevity is the Soul of Wit: Condensing Code Changes to Improve Commit Message Generation](https://arxiv.org/abs/2509.15567)
*Hongyu Kuang,Ning Zhang,Hui Gao,Xin Zhou,Wesley K. G. Assunção,Xiaoxing Ma,Dong Shao,Guoping Rong,He Zhang*

Main category: cs.SE

TL;DR: 本文提出了一种通过文本模板来压缩代码变更信息的方法，用于自动生成高质量的提交信息。该方法使用包含三部分的模板（总结代码变更、提取注释、强调代码标识符），结合ChangeScribe工具和CodeLlama-7B模型，在多个评估指标上显著优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 开发者在实践中经常忽视编写高质量的提交信息，这影响了代码维护的效率。虽然已有工作尝试自动生成提交信息，但如何有效组织和表示代码变更仍然是一个关键挑战。

Method: 1. 设计包含三部分的文本模板：总结的代码变更、提取的注释、强调的代码标识符；2. 使用基于启发式的ChangeScribe工具压缩代码变更；3. 在模板和对应提交信息对上微调CodeLlama-7B模型。

Result: 在广泛使用的数据集上评估显示，该方法在BLEU-Norm、METEOR和ROUGE-L指标上分别比六个基线方法平均提高了51.7%、78.7%和62.5%。消融研究和人工评估进一步验证了方法的有效性。

Conclusion: 提出的文本模板方法能更好地利用预训练语言模型，生成的提交信息既简洁又具有可读性，为开发者提供了有效的补充。该方法在自动生成高质量提交信息方面表现出显著优势。

Abstract: Commit messages are valuable resources for describing why code changes are
committed to repositories in version control systems (e.g., Git). They
effectively help developers understand code changes and better perform software
maintenance tasks. Unfortunately, developers often neglect to write
high-quality commit messages in practice. Therefore, a growing body of work is
proposed to generate commit messages automatically. These works all
demonstrated that how to organize and represent code changes is vital in
generating good commit messages, including the use of fine-grained graphs or
embeddings to better represent code changes. In this study, we choose an
alternative way to condense code changes before generation, i.e., proposing
brief yet concise text templates consisting of the following three parts: (1)
summarized code changes, (2) elicited comments, and (3) emphasized code
identifiers. Specifically, we first condense code changes by using our proposed
templates with the help of a heuristic-based tool named ChangeScribe, and then
fine-tune CodeLlama-7B on the pairs of our proposed templates and corresponding
commit messages. Our proposed templates better utilize pre-trained language
models, while being naturally brief and readable to complement generated commit
messages for developers. Our evaluation based on a widely used dataset showed
that our approach can outperform six baselines in terms of BLEU-Norm, METEOR,
and ROUGE-L, with average improvements of 51.7%, 78.7%, and 62.5%,
respectively. The ablation study and human evaluation also provide further
insights into the effectiveness of our approach.

</details>


### [4] [How Far Are We? An Empirical Analysis of Current Vulnerability Localization Approaches](https://arxiv.org/abs/2509.15777)
*Haoran Xu,Zhi Chen,Junxiao Han,Xinkui Zhao,Jianwei Yin,Shuiguang Deng*

Main category: cs.SE

TL;DR: 提出了一种新的两阶段框架，结合版本驱动候选过滤和基于大语言模型的多轮对话投票，用于开源软件漏洞补丁检测，在包含750个真实漏洞的数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统手动检测方法在处理大量提交历史时面临可扩展性挑战，现有自动化方法存在准确性有限、泛化能力差和方法限制等问题，阻碍了实际部署。

Method: 通过综合实证研究现有漏洞补丁检测方法，提出两阶段框架：第一阶段使用版本驱动候选过滤缩小搜索空间，第二阶段采用基于大语言模型的多轮对话投票进行精确识别。

Result: 在包含750个真实漏洞的数据集上进行广泛实验，证明该方法优于当前现有方法。

Conclusion: 基于实证研究的四个关键洞察设计的框架能够有效解决漏洞补丁检测的挑战，实现了准确高效的漏洞补丁识别。

Abstract: Open-source software vulnerability patch detection is a critical component
for maintaining software security and ensuring software supply chain integrity.
Traditional manual detection methods face significant scalability challenges
when processing large volumes of commit histories, while being prone to human
errors and omissions. Existing automated approaches, including heuristic-based
methods and pre-trained model solutions, suffer from limited accuracy, poor
generalization capabilities, and inherent methodological constraints that
hinder their practical deployment. To address these fundamental challenges,
this paper conducts a comprehensive empirical study of existing vulnerability
patch detection methods, revealing four key insights that guide the design of
effective solutions: the critical impact of search space reduction, the
superiority of pre-trained semantic understanding over architectural
complexity, the temporal limitations of web crawling approaches, and the
advantages of knowledge-driven methods. Based on these insights, we propose a
novel two-stage framework that combines version-driven candidate filtering with
large language model-based multi-round dialogue voting to achieve accurate and
efficient vulnerability patch identification. Extensive experiments on a
dataset containing 750 real vulnerabilities demonstrate that our method
outperforms current approaches.

</details>


### [5] [Failure Modes and Effects Analysis: An Experience from the E-Bike Domain](https://arxiv.org/abs/2509.15893)
*Andrea Bombarda,Federico Conti,Marcello Minervini,Aurora Zanenga,Claudio Menghi*

Main category: cs.SE

TL;DR: 本文介绍了在电动自行车领域的CPS系统中使用Simulink Fault Analyzer进行FMEA的经验，验证了仿真驱动方法在识别软件故障和评估安全影响方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 软件故障可能带来灾难性后果，工业界需要证据证明仿真驱动的FMEA方法的有效性，以促进实际应用。

Method: 使用Simulink Fault Analyzer工业工具，识别13个现实故障并建模分析其影响，通过专家反馈评估模型准确性和故障检测效果。

Result: 对于识别的故障，模型准确或仅包含轻微不精确（后续已修正）；38.4%的故障仿真结果与工程师预期不符，帮助他们发现故障的意外影响。

Conclusion: FMEA有助于工程师改进模型，本文提出的10个经验教训对Simulink工程师、安全分析师具有实用价值。

Abstract: Software failures can have catastrophic and costly consequences. Functional
Failure Mode and Effects Analysis (FMEA) is a standard technique used within
Cyber-Physical Systems (CPS) to identify software failures and assess their
consequences. Simulation-driven approaches have recently been shown to be
effective in supporting FMEA. However, industries need evidence of the
effectiveness of these approaches to increase practical adoption. This
industrial paper presents our experience with using FMEA to analyze the safety
of a CPS from the e-Bike domain. We used Simulink Fault Analyzer, an industrial
tool that supports engineers with FMEA. We identified 13 realistic faults,
modeled them, and analyzed their effects. We sought expert feedback to analyze
the appropriateness of our models and the effectiveness of the faults in
detecting safety breaches. Our results reveal that for the faults we
identified, our models were accurate or contained minor imprecision that we
subsequently corrected. They also confirm that FMEA helps engineers improve
their models. Specifically, the output provided by the simulation-driven
support for 38.4% (5 out of 13) of the faults did not match the engineers'
expectations, helping them discover unexpected effects of the faults. We
present a thorough discussion of our results and ten lessons learned. Our
findings are useful for software engineers who work as Simulink engineers, use
the Simulink Fault Analyzer, or work as safety analysts.

</details>


### [6] [LeakageDetector 2.0: Analyzing Data Leakage in Jupyter-Driven Machine Learning Pipelines](https://arxiv.org/abs/2509.15971)
*Owen Truong,Terrence Zhang,Arnav Marchareddy,Ryan Lee,Jeffery Busold,Michael Socas,Eman Abdullah AlOmar*

Main category: cs.SE

TL;DR: 开发了一个名为LeakageDetector的VS Code扩展，用于检测Jupyter Notebook中的数据泄漏问题，并提供两种修复机制：传统快速修复和LLM驱动的指导方法。


<details>
  <summary>Details</summary>
Motivation: 机器学习开发中数据泄漏问题会导致模型性能评估失真，需要帮助ML工程师识别和纠正代码中的数据泄漏问题。

Method: 开发VS Code扩展LeakageDetector，检测三种主要数据泄漏类型（重叠泄漏、预处理泄漏和多测试泄漏），并提供手动快速修复和基于LLM的指导修复两种机制。

Result: 成功开发了能够自动检测数据泄漏的工具，并为ML开发者提供了实用的修复解决方案。

Conclusion: LeakageDetector扩展有效帮助ML工程师避免数据泄漏问题，提升代码质量和模型评估的准确性。

Abstract: In software development environments, code quality is crucial. This study
aims to assist Machine Learning (ML) engineers in enhancing their code by
identifying and correcting Data Leakage issues within their models. Data
Leakage occurs when information from the test dataset is inadvertently included
in the training data when preparing a data science model, resulting in
misleading performance evaluations. ML developers must carefully separate their
data into training, evaluation, and test sets to avoid introducing Data Leakage
into their code. In this paper, we develop a new Visual Studio Code (VS Code)
extension, called LeakageDetector, that detects Data Leakage, mainly Overlap,
Preprocessing and Multi-test leakage, from Jupyter Notebook files. Beyond
detection, we included two correction mechanisms: a conventional approach,
known as a quick fix, which manually fixes the leakage, and an LLM-driven
approach that guides ML developers toward best practices for building ML
pipelines.

</details>


### [7] [Software Development Aspects of Integrating Linear Algebra Libraries](https://arxiv.org/abs/2509.16081)
*Marcel Koch,Tobias Ribizel,Pratik Nayak,Fritz Göbel,Gregor Olenik,Terry Cojean*

Main category: cs.SE

TL;DR: 本文讨论了应用软件采用Ginkgo稀疏数值线性代数库的挑战和益处，通过CFD、电网模拟和心电生理学等领域的案例，从软件工程角度分析集成对应用代码的影响。


<details>
  <summary>Details</summary>
Motivation: 科学发现通常依赖模拟软件，这些软件需要依赖特定领域的构建模块。Ginkgo作为稀疏数值线性代数库，能帮助应用软件更好地过渡到现代系统并加速模拟过程。

Method: 通过分析不同领域（CFD、电网模拟、心电生理学）的应用案例，从软件工程角度评估Ginkgo集成对应用代码的影响，重点关注可持续软件开发的方法。

Result: 展示了Ginkgo在不同科学计算领域的成功集成案例，证明了该库在帮助应用软件适应现代系统和提升计算效率方面的价值。

Conclusion: Ginkgo作为稀疏数值线性代数的构建模块，为科学计算应用提供了有效的软件工程解决方案，支持可持续的软件开发实践。

Abstract: Many scientific discoveries are made through, or aided by, the use of
simulation software. These sophisticated software applications are not built
from the ground up, instead they rely on smaller parts for specific use cases,
usually from domains unfamiliar to the application scientists. The software
library Ginkgo is one of these building blocks to handle sparse numerical
linear algebra on different platforms. By using Ginkgo, applications are able
to ease the transition to modern systems, and speed up their simulations
through faster numerical linear algebra routines. This paper discusses the
challenges and benefits for application software in adopting Ginkgo. It will
present examples from different domains, such as CFD, power grid simulation, as
well as electro-cardiophysiology. For these cases, the impact of the
integrations on the application code is discussed from a software engineering
standpoint, and in particular, the approaches taken by Ginkgo and the
applications to enable sustainable software development are highlighted.

</details>


### [8] [When Bugs Linger: A Study of Anomalous Resolution Time Outliers and Their Themes](https://arxiv.org/abs/2509.16140)
*Avinash Patil*

Main category: cs.SE

TL;DR: 本文分析了七个开源项目中的bug解决异常情况，使用统计方法识别异常解决时间，并通过文本分析发现异常bug主要集中在测试失败、功能增强请求和用户界面问题上。


<details>
  <summary>Details</summary>
Motivation: 高效的bug解决对软件质量和用户满意度至关重要，但某些bug报告存在异常长的解决时间，这可能表明存在流程效率低下或复杂问题。

Method: 使用Z-score和IQR统计方法识别bug解决时间异常，应用TF-IDF进行文本特征提取，并使用KMeans聚类对相似bug摘要进行分组分析。

Result: 研究发现异常bug在不同项目中呈现一致模式，主要围绕测试失败、增强请求和用户界面问题聚类。

Conclusion: 该方法为项目维护者提供了可操作的见解，帮助他们优先处理长期存在的bug并有效解决问题。

Abstract: Efficient bug resolution is critical for maintaining software quality and
user satisfaction. However, specific bug reports experience unusually long
resolution times, which may indicate underlying process inefficiencies or
complex issues. This study presents a comprehensive analysis of bug resolution
anomalies across seven prominent open-source repositories: Cassandra, Firefox,
Hadoop, HBase, SeaMonkey, Spark, and Thunderbird. Utilizing statistical methods
such as Z-score and Interquartile Range (IQR), we identify anomalies in bug
resolution durations. To understand the thematic nature of these anomalies, we
apply Term Frequency-Inverse Document Frequency (TF-IDF) for textual feature
extraction and KMeans clustering to group similar bug summaries. Our findings
reveal consistent patterns across projects, with anomalies often clustering
around test failures, enhancement requests, and user interface issues. This
approach provides actionable insights for project maintainers to prioritize and
effectively address long-standing bugs.

</details>


### [9] [MatchFixAgent: Language-Agnostic Autonomous Repository-Level Code Translation Validation and Repair](https://arxiv.org/abs/2509.16187)
*Ali Reza Ibrahimzada,Brandon Paulsen,Reyhaneh Jabbarvand,Joey Dodds,Daniel Kroening*

Main category: cs.SE

TL;DR: MatchFixAgent是一个基于大语言模型的多智能体框架，用于代码翻译的等价性验证和修复，支持多种编程语言对，相比现有方法在验证准确性和修复能力上有显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有代码翻译验证方法难以泛化到多种编程语言，依赖不充分的测试套件，导致等价性判断错误和修复效果不佳。

Method: 采用多智能体架构，将等价性验证分解为语义分析、测试生成执行、错误修复和最终裁决等子任务，利用LLM进行PL无关的处理。

Result: 在6种编程语言对的2219个翻译对上测试，MatchFixAgent能对99.2%的翻译对做出等价性判断，修复50.6%的不等价翻译，远高于现有方法的18.5%。

Conclusion: MatchFixAgent相比现有方法具有更好的泛化能力和准确性，能够有效处理多种编程语言对的代码翻译验证和修复任务。

Abstract: Code translation transforms source code from one programming language (PL) to
another. Validating the functional equivalence of translation and repairing, if
necessary, are critical steps in code translation. Existing automated
validation and repair approaches struggle to generalize to many PLs due to high
engineering overhead, and they rely on existing and often inadequate test
suites, which results in false claims of equivalence and ineffective
translation repair. We develop MatchFixAgent, a large language model
(LLM)-based, PL-agnostic framework for equivalence validation and repair of
translations. MatchFixAgent features a multi-agent architecture that divides
equivalence validation into several sub-tasks to ensure thorough and consistent
semantic analysis of the translation. Then it feeds this analysis to test agent
to write and execute tests. Upon observing a test failure, the repair agent
attempts to fix the translation bug. The final (in)equivalence decision is made
by the verdict agent, considering semantic analyses and test execution results.
  We compare MatchFixAgent's validation and repair results with four
repository-level code translation techniques. We use 2,219 translation pairs
from their artifacts, which cover 6 PL pairs, and are collected from 24 GitHub
projects totaling over 900K lines of code. Our results demonstrate that
MatchFixAgent produces (in)equivalence verdicts for 99.2% of translation pairs,
with the same equivalence validation result as prior work on 72.8% of them.
When MatchFixAgent's result disagrees with prior work, we find that 60.7% of
the time MatchFixAgent's result is actually correct. In addition, we show that
MatchFixAgent can repair 50.6% of inequivalent translation, compared to prior
work's 18.5%. This demonstrates that MatchFixAgent is far more adaptable to
many PL pairs than prior work, while producing highly accurate validation
results.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [10] [Synergizing Static Analysis with Large Language Models for Vulnerability Discovery and beyond](https://arxiv.org/abs/2509.15433)
*Vaibhav Agrawal,Kiarash Ahi*

Main category: cs.CR

TL;DR: LLM与SAST结合可显著提升漏洞发现效率，降低误报率约91%


<details>
  <summary>Details</summary>
Motivation: 传统SAST工具存在高误报率和缺乏上下文理解的问题，而LLM在代码分析方面有优势但存在不一致性和幻觉问题，两者结合可互补优势

Method: 通过集成LLM和SAST技术，创建更智能高效的系统，实现漏洞检测优化、改进分类、动态漏洞描述、通过漏洞利用生成进行验证等功能

Result: SAST-Genius相比单独使用Semgrep，误报率从225个减少到20个，降低了约91%

Conclusion: LLM与SAST的协同作用创造了一个更有效的安全方法，充分利用两种技术的优势同时减轻各自的弱点

Abstract: This report examines the synergy between Large Language Models (LLMs) and
Static Application Security Testing (SAST) to improve vulnerability discovery.
Traditional SAST tools, while effective for proactive security, are limited by
high false-positive rates and a lack of contextual understanding. Conversely,
LLMs excel at code analysis and pattern recognition but can be prone to
inconsistencies and hallucinations. By integrating these two technologies, a
more intelligent and efficient system is created. This combination moves beyond
mere vulnerability detection optimization, transforming security into a deeply
integrated, contextual process that provides tangible benefits like improved
triage, dynamic bug descriptions, bug validation via exploit generation and
enhanced analysis of complex codebases. The result is a more effective security
approach that leverages the strengths of both technologies while mitigating
their weaknesses. SAST-Genius reduced false positives by about 91 % (225 to 20)
compared to Semgrep alone.

</details>


### [11] [Adversarially Robust Assembly Language Model for Packed Executables Detection](https://arxiv.org/abs/2509.15499)
*Shijia Li,Jiang Ming,Lanqing Liu,Longwei Yang,Ni Zhang,Chunfu Jia*

Main category: cs.CR

TL;DR: Pack-ALM是一种基于深度学习的打包可执行文件检测方法，通过将打包检测重新定义为区分合法指令和"伪"指令的任务，使用预训练的汇编语言模型来识别打包数据的特征。


<details>
  <summary>Details</summary>
Motivation: 传统的打包检测方法依赖经验特征（如高熵或特定二进制模式），容易受到对抗性样本或未知打包器的规避，且依赖专家手工特征难以持续演进。

Method: 将原生数据和打包数据预处理为"伪"指令，设计预训练的汇编语言模型来识别打包数据的特征，与领先的工业打包检测工具和最先进的汇编语言模型进行比较评估。

Result: 在超过37,000个样本上的实验表明，Pack-ALM能有效识别打包二进制文件，包括使用对抗性或先前未见打包技术创建的样本，在检测准确性和对抗鲁棒性方面均优于传统熵基方法和先进汇编语言模型。

Conclusion: Pack-ALM提供了一种更有效和鲁棒的打包检测方法，解决了传统方法的局限性，为大规模恶意软件分析和杀毒引擎工作流程提供了重要改进。

Abstract: Detecting packed executables is a critical component of large-scale malware
analysis and antivirus engine workflows, as it identifies samples that warrant
computationally intensive dynamic unpacking to reveal concealed malicious
behavior. Traditionally, packer detection techniques have relied on empirical
features, such as high entropy or specific binary patterns. However, these
empirical, feature-based methods are increasingly vulnerable to evasion by
adversarial samples or unknown packers (e.g., low-entropy packers).
Furthermore, the dependence on expert-crafted features poses challenges in
sustaining and evolving these methods over time.
  In this paper, we examine the limitations of existing packer detection
methods and propose Pack-ALM, a novel deep-learning-based approach for
detecting packed executables. Inspired by the linguistic concept of
distinguishing between real and pseudo words, we reformulate packer detection
as a task of differentiating between legitimate and "pseudo" instructions. To
achieve this, we preprocess native data and packed data into "pseudo"
instructions and design a pre-trained assembly language model that recognizes
features indicative of packed data. We evaluate Pack-ALM against leading
industrial packer detection tools and state-of-the-art assembly language
models. Extensive experiments on over 37,000 samples demonstrate that Pack-ALM
effectively identifies packed binaries, including samples created with
adversarial or previously unseen packing techniques. Moreover, Pack-ALM
outperforms traditional entropy-based methods and advanced assembly language
models in both detection accuracy and adversarial robustness.

</details>


### [12] [Fluid Antenna System-assisted Physical Layer Secret Key Generation](https://arxiv.org/abs/2509.15547)
*Zhiyu Huang,Guyue Li,Hao Xu,Derrick Wing Kwan Ng*

Main category: cs.CR

TL;DR: 本文研究了多天线基站系统中的物理层密钥生成，通过流体天线系统动态定制无线电环境来提升密钥生成率，提出了迭代算法和低复杂度端口选择方法。


<details>
  <summary>Details</summary>
Motivation: 利用流体天线系统在不增加额外节点或射频链的情况下，通过自适应天线端口选择来增强合法节点间的密钥生成率，解决传统物理层密钥生成的效率问题。

Method: 提出FAS辅助的PLKG模型，结合发射波束成形和稀疏端口选择；使用迭代算法（连续凸近似和Cauchy-Schwarz不等式）求解非凸优化问题；提出基于滑动窗口的低复杂度端口选择方法替代加权ℓ1范数方法。

Result: 仿真结果表明FAS-PLKG方案在独立和空间相关环境中均显著优于FA-PLKG方案；滑动窗口端口选择方法比加权ℓ1范数方法产生更高的密钥生成率；FAS通过动态稀疏端口选择以更少的射频链实现更高的密钥生成率。

Conclusion: 流体天线系统通过动态稀疏端口选择有效提升了物理层密钥生成的效率，提出的算法和方法在实际应用中具有较好的性能和可行性。

Abstract: This paper investigates physical-layer key generation (PLKG) in multi-antenna
base station systems, by leveraging a fluid antenna system (FAS) to dynamically
customize radio environments. Without requiring additional nodes or extensive
radio frequency chains, the FAS effectively enables adaptive antenna port
selection by exploiting channel spatial correlation to enhance the key
generation rate (KGR) at legitimate nodes. To comprehensively evaluate the
efficiency of the FAS in PLKG, we propose an FAS-assisted PLKG model that
integrates transmit beamforming and sparse port selection under independent and
identically distributed and spatially correlated channel models, respectively.
Specifically, the PLKG utilizes reciprocal channel probing to derive a
closed-form KGR expression based on the mutual information between legitimate
channel estimates. Nonconvex optimization problems for these scenarios are
formulated to maximize the KGR subject to transmit power constraints and sparse
port activation. We propose an iterative algorithm by capitalizing on
successive convex approximation and Cauchy-Schwarz inequality to obtain a
locally optimal solution. A reweighted $\ell_1$-norm-based algorithm is applied
to advocate for the sparse port activation of FAS-assisted PLKG. Furthermore, a
low-complexity sliding window-based port selection is proposed to substitute
reweighted $\ell_1$-norm method based on Rayleigh-quotient analysis. Simulation
results demonstrate that the FAS-PLKG scheme significantly outperforms the
FA-PLKG scheme in both independent and spatially correlated environments. The
sliding window-based port selection method introduced in this paper has been
shown to yield superior KGR, compared to the reweighted $\ell_1$-norm method.
It is shown that the FAS achieves higher KGR with fewer RF chains through
dynamic sparse port selection.

</details>


### [13] [Hybrid Deep Learning-Federated Learning Powered Intrusion Detection System for IoT/5G Advanced Edge Computing Network](https://arxiv.org/abs/2509.15555)
*Rasil Baidar,Sasa Maric,Robert Abbas*

Main category: cs.CR

TL;DR: 提出了一种融合CNN、BiLSTM和自编码器的隐私保护联邦学习入侵检测系统，在边缘设备上训练，在UNSW-NB15数据集上达到99.59% AUC和97.36% F1分数，推理时间满足5G-Advanced URLLC要求。


<details>
  <summary>Details</summary>
Motivation: 随着物联网和5G-Advanced应用的指数级扩张，DDoS、恶意软件和零日入侵的攻击面不断扩大，需要有效的入侵检测解决方案。

Method: 采用隐私保护联邦学习框架，融合卷积神经网络（CNN）捕获局部特征交互，双向LSTM（BiLSTM）处理门控跨特征交互，自编码器（AE）增强基于重构的异常敏感性。

Result: 在UNSW-NB15二元分类任务中，融合模型达到AUC 99.59%和F1分数97.36%，混淆矩阵分析显示平衡的错误率和高精度召回率，平均推理时间约0.0476毫秒/样本。

Conclusion: 该方法支持边缘部署，满足5G-Advanced超可靠低延迟通信要求，同时讨论了可解释性、漂移容忍性和联邦学习在合规可扩展5G-Advanced物联网安全中的应用。

Abstract: The exponential expansion of IoT and 5G-Advanced applications has enlarged
the attack surface for DDoS, malware, and zero-day intrusions. We propose an
intrusion detection system that fuses a convolutional neural network (CNN), a
bidirectional LSTM (BiLSTM), and an autoencoder (AE) bottleneck within a
privacy-preserving federated learning (FL) framework. The CNN-BiLSTM branch
captures local and gated cross-feature interactions, while the AE emphasizes
reconstruction-based anomaly sensitivity. Training occurs across edge devices
without sharing raw data. On UNSW-NB15 (binary), the fused model attains AUC
99.59 percent and F1 97.36 percent; confusion-matrix analysis shows balanced
error rates with high precision and recall. Average inference time is
approximately 0.0476 ms per sample on our test hardware, which is well within
the less than 10 ms URLLC budget, supporting edge deployment. We also discuss
explainability, drift tolerance, and FL considerations for compliant, scalable
5G-Advanced IoT security.

</details>


### [14] [Cuckoo Attack: Stealthy and Persistent Attacks Against AI-IDE](https://arxiv.org/abs/2509.15572)
*Xinpeng Liu,Junming Liu,Peiyu Liu,Han Zheng,Qinying Wang,Mathias Payer,Shouling Ji,Wenhai Wang*

Main category: cs.CR

TL;DR: Cuckoo Attack是一种针对AI-IDE的新型攻击，通过在配置文件中嵌入恶意负载实现隐蔽持久的命令执行，可导致本地入侵和供应链攻击。


<details>
  <summary>Details</summary>
Motivation: 现代AI-IDE采用Agent中心架构，LLM驱动的Agent深度集成以自主执行复杂任务，但这种紧密集成也引入了新的攻击面。攻击者可通过注入恶意指令劫持Agent执行有害操作。现有攻击方法缺乏隐蔽性和持久性，限制了实际影响。

Method: 提出Cuckoo Attack攻击范式，分为初始感染和持久化两个阶段。攻击者将恶意负载嵌入AI-IDE常用的配置文件中，这些文件在常规操作期间执行系统命令但不向用户显示执行细节。一旦配置，这些文件很少被重新检查，除非发生明显的运行时错误。

Result: 通过端到端概念验证，在9个主流Agent和AI-IDE对上验证了攻击的可行性。攻击不仅能入侵开发者本地计算机，还能通过配置文件传播实现供应链攻击。

Conclusion: 贡献了7个可操作的安全检查点供厂商评估产品安全性。强调了AI-IDE安全防护的紧迫性，需要厂商和开发者共同关注配置文件的安全性。

Abstract: Modern AI-powered Integrated Development Environments (AI-IDEs) are
increasingly defined by an Agent-centric architecture, where an LLM-powered
Agent is deeply integrated to autonomously execute complex tasks. This tight
integration, however, also introduces a new and critical attack surface.
Attackers can exploit these components by injecting malicious instructions into
untrusted external sources, effectively hijacking the Agent to perform harmful
operations beyond the user's intention or awareness. This emerging threat has
quickly attracted research attention, leading to various proposed attack
vectors, such as hijacking Model Context Protocol (MCP) Servers to access
private data. However, most existing approaches lack stealth and persistence,
limiting their practical impact.
  We propose the Cuckoo Attack, a novel attack that achieves stealthy and
persistent command execution by embedding malicious payloads into configuration
files. These files, commonly used in AI-IDEs, execute system commands during
routine operations, without displaying execution details to the user. Once
configured, such files are rarely revisited unless an obvious runtime error
occurs, creating a blind spot for attackers to exploit. We formalize our attack
paradigm into two stages, including initial infection and persistence. Based on
these stages, we analyze the practicality of the attack execution process and
identify the relevant exploitation techniques. Furthermore, we analyze the
impact of Cuckoo Attack, which can not only invade the developer's local
computer but also achieve supply chain attacks through the spread of
configuration files. We contribute seven actionable checkpoints for vendors to
evaluate their product security. The critical need for these checks is
demonstrated by our end-to-end Proof of Concept, which validated the proposed
attack across nine mainstream Agent and AI-IDE pairs.

</details>


### [15] [Future-Proofing Cloud Security Against Quantum Attacks: Risk, Transition, and Mitigation Strategies](https://arxiv.org/abs/2509.15653)
*Yaser Baseri,Abdelhakim Hafid,Arash Habibi Lashkari*

Main category: cs.CR

TL;DR: 本文对量子计算对云计算安全的威胁进行了系统性调查，分析了量子威胁在云架构各层的漏洞，提出了基于STRIDE模型的风险评估方法，并构建了集成混合密码过渡策略的分层安全框架。


<details>
  <summary>Details</summary>
Motivation: 量子计算对传统密码系统构成重大威胁，需要为云计算环境开发量子安全的安全解决方案，以保护云基础设施免受量子攻击。

Method: 采用结构化风险评估方法（基于STRIDE模型），分析主要云服务提供商（AWS、Azure、GCP）的量子安全准备情况，评估标准化后量子密码算法的安全性。

Result: 提出了一个分层安全框架，整合了混合密码过渡策略、密码敏捷性和主动风险缓解机制，识别了量子威胁在云架构各层的具体攻击向量。

Conclusion: 为云架构师、政策制定者和研究人员提供了量子弹性云系统过渡的战略参考，并确定了六个关键未来研究方向：标准化与互操作性、性能与可扩展性、实施安全性、与新兴技术集成、系统准备度和密码敏捷迁移框架。

Abstract: Quantum Computing (QC) introduces a transformative threat to digital
security, with the potential to compromise widely deployed classical
cryptographic systems. This survey offers a comprehensive and systematic
examination of quantumsafe security for Cloud Computing (CC), focusing on the
vulnerabilities, transition strategies, and mitigation mechanisms required to
secure cloud infrastructures in the quantum era. We evaluated the landscape of
quantum threats across the entire CC stack, demonstrating how quantum
algorithms can undermine classical encryption and compromise cloud security at
multiple architectural layers. Using a structured risk assessment methodology
based on the STRIDE model, we evaluate quantum-induced attack vectors and their
impact on cloud environments. To address these challenges, we propose a layered
security framework that integrates hybrid cryptographic transition strategies,
cryptographic agility, and proactive risk mitigation. We analyze the
preparation and implementation approaches of the major Cloud Service Providers
(CSPs), including AWS, Azure and GCP, synthesizing platform-specific
initiatives toward Post-Quantum Cryptography (PQC). Furthermore, we provide a
detailed evaluation of standardized PQC algorithms, exploring their resilience
to side-channel and active attacks within cloud-native deployments. This survey
serves as a strategic reference for cloud architects, policymakers, and
researchers, offering actionable insights for navigating the complex transition
to quantum-resilient cloud systems. We conclude by identifying six key future
research directions: standardization and interoperability, performance and
scalability, implementation security, integration with emerging technologies,
systemic preparedness, and crypto-agile migration frameworks.

</details>


### [16] [Hornet Node and the Hornet DSL: A Minimal, Executable Specification for Bitcoin Consensus](https://arxiv.org/abs/2509.15754)
*Toby Sharp*

Main category: cs.CR

TL;DR: 本文提出了一个紧凑、可执行的比特币共识规则C++规范，以及专门设计的Hornet DSL语言，旨在实现比特币共识的形式化规范和验证。


<details>
  <summary>Details</summary>
Motivation: 比特币的共识规则目前仅存在于参考客户端的代码实现中，存在副作用、可变状态、并发和遗留设计等问题，不适合形式化验证。需要一个独立的正式规范来支持跨版本验证和新客户端实现，减少共识分裂风险。

Method: 开发了可执行的声明式C++规范，能够在单线程上几小时内同步主网到最新状态。设计了Hornet DSL语言来无歧义地编码共识规则，支持形式推理、共识代码生成和AI驱动的对抗测试。

Result: 实现了Hornet Node客户端，具有现代模块化架构、分层设计、高效数据结构和强关注点分离，性能适合实验和教学。

Conclusion: Hornet Node和Hornet DSL共同提供了实现纯粹、形式化、可执行的比特币共识规范的首个可行路径。

Abstract: Bitcoin's consensus rules are encoded in the implementation of its reference
client: "The code is the spec." Yet this code is unsuitable for formal
verification due to side effects, mutable state, concurrency, and legacy
design. A standalone formal specification would enable verification both across
versions of the reference client and against new client implementations,
strengthening decentralization by reducing the risk of consensus-splitting
bugs. Yet such a specification has long been considered intractable given the
complexity of Bitcoin's consensus logic. We demonstrate a compact, executable,
declarative C++ specification of Bitcoin consensus rules that syncs mainnet to
tip in a few hours on a single thread. We also introduce the Hornet
Domain-Specific Language (DSL) specifically designed to encode these rules
unambiguously for execution, enabling formal reasoning, consensus code
generation, and AI-driven adversarial testing. Our spec-driven client Hornet
Node offers a modern and modular complement to the reference client. Its clear,
idiomatic style makes it suitable for education, while its performance makes it
ideal for experimentation. We highlight architectural contributions such as its
layered design, efficient data structures, and strong separation of concerns,
supported by production-quality code examples. We argue that Hornet Node and
Hornet DSL together provide the first credible path toward a pure, formal,
executable specification of Bitcoin consensus.

</details>


### [17] [Inference Attacks on Encrypted Online Voting via Traffic Analysis](https://arxiv.org/abs/2509.15694)
*Anastasiia Belousova,Francesco Marchiori,Mauro Conti*

Main category: cs.CR

TL;DR: 本文分析了在线投票系统中加密网络流量的元数据如何被攻击者利用来推断敏感信息，即使不访问加密内容也能揭示投票行为、提交时间和选票有效性等关键信息。


<details>
  <summary>Details</summary>
Motivation: 随着在线投票的普及，确保系统安全变得至关重要。虽然大多数研究关注密码协议的设计和验证，但网络流量分析等攻击向量相对较少研究，尽管它们可能对选民隐私和系统可信度构成重大威胁。

Method: 使用基于规则的技术和机器学习方法，分析加密网络流量的元数据来推断投票行为。在两个广泛使用的在线投票平台（一个专有，一个部分开源）上测试这些攻击。

Result: 攻击分类准确率高达99.5%，揭示了严重的隐私漏洞，威胁到安全选举的关键特性，包括选民保密性和防止胁迫或买票。

Conclusion: 探索了缓解措施，如有效载荷填充和时间戳均衡化，可以显著限制攻击的有效性，强调了在线投票系统需要更强的隐私保护机制。

Abstract: Online voting enables individuals to participate in elections remotely,
offering greater efficiency and accessibility in both governmental and
organizational settings. As this method gains popularity, ensuring the security
of online voting systems becomes increasingly vital, as the systems supporting
it must satisfy a demanding set of security requirements. Most research in this
area emphasizes the design and verification of cryptographic protocols to
protect voter integrity and system confidentiality. However, other vectors,
such as network traffic analysis, remain relatively understudied, even though
they may pose significant threats to voter privacy and the overall
trustworthiness of the system.
  In this paper, we examine how adversaries can exploit metadata from encrypted
network traffic to uncover sensitive information during online voting. Our
analysis reveals that, even without accessing the encrypted content, it is
possible to infer critical voter actions, such as whether a person votes, the
exact moment a ballot is submitted, and whether the ballot is valid or spoiled.
We test these attacks with both rule-based techniques and machine learning
methods. We evaluate our attacks on two widely used online voting platforms,
one proprietary and one partially open source, achieving classification
accuracy as high as 99.5%. These results expose a significant privacy
vulnerability that threatens key properties of secure elections, including
voter secrecy and protection against coercion or vote-buying. We explore
mitigations to our attacks, demonstrating that countermeasures such as payload
padding and timestamp equalization can substantially limit their effectiveness.

</details>


### [18] [An Adversarial Robust Behavior Sequence Anomaly Detection Approach Based on Critical Behavior Unit Learning](https://arxiv.org/abs/2509.15756)
*Dongyang Zhan,Kai Tan,Lin Ye,Xiangzhan Yu,Hongli Zhang,Zheng He*

Main category: cs.CR

TL;DR: 提出了一种基于行为单元分析的对抗鲁棒性异常检测方法，通过提取行为单元来增强序列深度学习模型对对抗样本的防御能力。


<details>
  <summary>Details</summary>
Motivation: 现有的基于序列深度学习模型（如RNN和LSTM）的恶意软件分类器容易受到对抗样本攻击，攻击者可以通过改变行为序列的序列特征来误导分类器。

Method: 提取相关行为作为行为单元，这些单元包含局部行为的代表性语义信息。基于多层次深度学习模型学习每个行为单元的整体语义和行为单元之间的上下文关系，以减轻针对局部和大规模行为的扰动攻击。

Result: 实验结果表明，该方法优于所有对比方法，表明其在对抗混淆攻击方面具有更好的性能。

Conclusion: 该方法能够有效提高行为分析的鲁棒性，适用于低层次和高层次的行为日志（如API和系统调用日志）。

Abstract: Sequential deep learning models (e.g., RNN and LSTM) can learn the sequence
features of software behaviors, such as API or syscall sequences. However,
recent studies have shown that these deep learning-based approaches are
vulnerable to adversarial samples. Attackers can use adversarial samples to
change the sequential characteristics of behavior sequences and mislead malware
classifiers. In this paper, an adversarial robustness anomaly detection method
based on the analysis of behavior units is proposed to overcome this problem.
We extract related behaviors that usually perform a behavior intention as a
behavior unit, which contains the representative semantic information of local
behaviors and can be used to improve the robustness of behavior analysis. By
learning the overall semantics of each behavior unit and the contextual
relationships among behavior units based on a multilevel deep learning model,
our approach can mitigate perturbation attacks that target local and
large-scale behaviors. In addition, our approach can be applied to both
low-level and high-level behavior logs (e.g., API and syscall logs). The
experimental results show that our approach outperforms all the compared
methods, which indicates that our approach has better performance against
obfuscation attacks.

</details>


### [19] [Flying Drones to Locate Cyber-Attackers in LoRaWAN Metropolitan Networks](https://arxiv.org/abs/2509.15725)
*Matteo Repetto,Enrico Cambiaso,Fabio Patrone,Sandro Zappatore*

Main category: cs.CR

TL;DR: FOLLOWME项目研究使用无人机定位和追踪无线网络攻击者的可行性，开发集成了网络遥测和无线定位的网络安全框架。


<details>
  <summary>Details</summary>
Motivation: 当前许多关键服务和工业系统依赖无线网络与物联网交互，容易受到网络威胁攻击。虽然检测攻击不难，但定位攻击源和识别攻击者几乎不可能，这对于阻止攻击至关重要。

Method: 开发一个网络物理安全框架，集成网络遥测和无线定位。网络遥测在检测到异常或已知攻击模式时触发警报，提供攻击区域的粗略指示；无线定位系统扫描该区域以精确定位攻击者位置。

Result: 项目专门针对远程城域网和LoRaWAN协议，这是智慧城市服务的典型场景。

Conclusion: 该项目旨在解决无线网络攻击源定位的开放挑战，特别是在攻击者使用便携或自制设备不断移动的情况下。

Abstract: Today, many critical services and industrial systems rely on wireless
networks for interaction with the IoT, hence becoming vulnerable to a broad
number of cyber-threats. While detecting this kind of attacks is not difficult
with common cyber-security tools, and even trivial for jamming, finding their
origin and identifying culprits is almost impossible today, yet indispensable
to stop them, especially when attacks are generated with portable or self-made
devices that continuously move around. To address this open challenge, the
FOLLOWME project investigates the feasibility of using UAV to locate and even
chase attackers during illicit usage of the radio spectrum. The main objective
is to develop a cyber-physical security framework that integrates network
telemetry with wireless localization. The former triggers alarms in case of
anomalies or known attack patterns and provides a coarse-grained indication of
the physical area (i.e., the position of affected access gateways), whereas the
latter systematically scans such area to identify the exact location of the
attacker. The project will specifically address long-range metropolitan area
networks and focus on the LoRaWAN protocol, which is the typical scenario for
Smart City services.

</details>


### [20] [A High-performance Real-time Container File Monitoring Approach Based on Virtual Machine Introspection](https://arxiv.org/abs/2509.16030)
*Kai Tan,Dongyang Zhan,Lin Ye,Hongli Zhang,Binxing Fang,Zhihong Tian*

Main category: cs.CR

TL;DR: 提出了一种基于虚拟机自省的高性能容器文件监控方法，解决现有方法安全性低和负载高的问题


<details>
  <summary>Details</summary>
Motivation: 随着云计算的发展，容器安全成为云原生应用运行的关键因素。攻击者可通过篡改文件攻击容器服务甚至执行容器逃逸攻击。现有文件监控方法基于主机操作系统或虚拟机自省，但存在安全性弱和负载高的问题

Method: 基于虚拟机自省技术，设计高性能容器文件监控方法，实时保护容器文件安全

Result: 实验结果表明，该方法能有效监控容器文件，并引入可接受的监控负载

Conclusion: 该方法解决了现有容器文件监控方法的安全性和性能问题，为APT检测和网络空间安全提供了有效解决方案

Abstract: As cloud computing continues to advance and become an integral part of modern
IT infrastructure, container security has emerged as a critical factor in
ensuring the smooth operation of cloud-native applications. An attacker can
attack the service in the container or even perform the container escape attack
by tampering with the files. Monitoring container files is important for APT
detection and cyberspace security. Existing file monitoring methods are usually
based on host operating system or virtual machine introspection to protect file
security in real time. The methods based on the host operating system usually
monitor file operations in the host operating system. However, when the
container escapes to the host, the host operating system will no longer be
secure, so these methods face the problem of weak security. Aiming at the
problems of low security and high overload introduced in existing container
file monitoring, a high-performance container file monitoring method based on
virtual machine introspection is proposed. The experimental results show that
the proposed approach can effectively monitor the container files and introduce
an acceptable monitoring overload.

</details>


### [21] [ConCap: Practical Network Traffic Generation for Flow-based Intrusion Detection Systems](https://arxiv.org/abs/2509.16038)
*Miel Verkerken,Laurens D'hooge,Bruno Volckaert,Filip De Turck,Giovanni Apruzzese*

Main category: cs.CR

TL;DR: ConCap是一个实用的工具，用于解决网络入侵检测系统(NIDS)研究中缺乏真实代表性数据的挑战，通过创建隔离的轻量级网络环境自动生成带标签的网络数据。


<details>
  <summary>Details</summary>
Motivation: NIDS研究面临的核心挑战是难以获取真实网络环境中有代表性的数据，这导致先前研究结果的可信度受到质疑。

Method: 开发ConCap工具，基于开源软件构建隔离网络环境，可配置生成网络数据包或NetFlows，并自动标注数据，仅需共享一个配置文件即可实现实验可复现性。

Result: 通过10种网络活动的综合实验验证，ConCap生成的网络数据与现实网络相似；在基准数据集和真实智能家居网络上的实验表明，其自动标注的NetFlows在检测功能上与其他环境收集的数据等效；能够安全复现复杂攻击链。

Conclusion: ConCap为解决困扰NIDS研究的"数据问题"提供了有效解决方案，促进了实验研究的可复现性和可靠性。

Abstract: Network Intrusion Detection Systems (NIDS) have been studied in research for
almost four decades. Yet, despite thousands of papers claiming scientific
advances, a non-negligible number of recent works suggest that the findings of
prior literature may be questionable. At the root of such a disagreement is the
well-known challenge of obtaining data representative of a real-world
network-and, hence, usable for security assessments. We tackle such a challenge
in this paper. We propose ConCap, a practical tool meant to facilitate
experimental research on NIDS. Through ConCap, a researcher can set up an
isolated and lightweight network environment and configure it to produce
network-related data, such as packets or NetFlows, that are automatically
labeled, hence ready for fine-grained experiments. ConCap is rooted on
open-source software and is designed to foster experimental reproducibility
across the scientific community by sharing just one configuration file. Through
comprehensive experiments on 10 different network activities, further expanded
via in-depth analyses of 21 variants of two specific activities and of 100
repetitions of four other ones, we empirically verify that ConCap produces
network data resembling that of a real-world network. We also carry out
experiments on well-known benchmark datasets as well as on a real "smart-home"
network, showing that, from a cyber-detection viewpoint, ConCap's
automatically-labeled NetFlows are functionally equivalent to those collected
in other environments. Finally, we show that ConCap enables to safely reproduce
sophisticated attack chains (e.g., to test/enhance existing NIDS). Altogether,
ConCap is a solution to the "data problem" that is plaguing NIDS research.

</details>


### [22] [How Exclusive are Ethereum Transactions? Evidence from non-winning blocks](https://arxiv.org/abs/2509.16052)
*Vabuk Pahari,Andrea Canidio*

Main category: cs.CR

TL;DR: 该论文分析了以太坊区块链中15,097个区块的交易数据，发现在获胜区块中，独占交易（仅出现在单个构建者区块中的交易）占总费用的84%，是构建者收入的主要来源。


<details>
  <summary>Details</summary>
Motivation: 研究以太坊区块构建者之间的交易竞争模式，特别是独占交易和私有交易对构建者收入的影响，以了解区块链交易市场的运作机制。

Method: 分析2024年12月3日8分钟窗口内15,097个区块的交易数据，将交易分类为独占交易（仅出现在单一构建者区块）和私有交易（不在公共内存池但出现在多个构建者区块），并通过交易日志分析交易策略。

Result: 独占交易占获胜区块总费用的84%；约7%的独占交易价值来自仅向单一构建者路由的发送者；即使考虑重复策略，独占交易费用占比仍至少为77.2%。

Conclusion: 独占交易是构建者收入的主要来源，表明在以太坊区块构建市场中存在显著的交易独占性现象。

Abstract: We analyze 15,097 blocks proposed for inclusion in Ethereum's blockchain over
an 8-minute window on December 3, 2024, during which 38 blocks were added to
the chain. We classify transactions as exclusive -- present only in blocks from
a single builder -- or private -- absent from the public mempool but included
in blocks from multiple builders. We find that exclusive transactions account
for 84% of the total fees paid by transactions in winning blocks. Furthermore,
we show that exclusivity cannot be fully explained by exclusive relationships
between senders and builders: about 7% of all exclusive transactions included
on-chain, by value, come from senders who route exclusively to a single
builder. Analyzing transaction logs shows that some exclusive transactions are
duplicates or variations of the same strategy, but even accounting for that,
the share of the total fees paid by transactions in winning blocks is at least
77.2%. Taken together, our findings highlight that exclusive transactions are
the dominant source of builder revenues.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [23] [MICA: Multi-Agent Industrial Coordination Assistant](https://arxiv.org/abs/2509.15237)
*Di Wen,Kunyu Peng,Junwei Zheng,Yufan Chen,Yitain Shi,Jiale Wei,Ruiping Liu,Kailun Yang,Rainer Stiefelhagen*

Main category: cs.AI

TL;DR: MICA是一个面向工业工作流程的多代理协调助手系统，通过语音交互提供实时指导，具备感知基础和隐私保护特性，可在离线硬件上部署。


<details>
  <summary>Details</summary>
Motivation: 工业工作流程需要能够在有限计算能力、连接性和严格隐私约束下运行的适应性强的可信助手系统。

Method: MICA协调五个角色专门化的语言代理，通过安全检查器审核确保准确性。引入自适应步骤融合(ASF)技术，动态融合专家推理与自然语音反馈的在线适应。建立了新的多代理协调基准和评估指标。

Result: 实验表明MICA在任务成功率、可靠性和响应性方面持续优于基线结构，同时可在实际离线硬件上部署。

Conclusion: MICA是朝着可部署、隐私保护的多代理助手在动态工厂环境中应用的重要一步。

Abstract: Industrial workflows demand adaptive and trustworthy assistance that can
operate under limited computing, connectivity, and strict privacy constraints.
In this work, we present MICA (Multi-Agent Industrial Coordination Assistant),
a perception-grounded and speech-interactive system that delivers real-time
guidance for assembly, troubleshooting, part queries, and maintenance. MICA
coordinates five role-specialized language agents, audited by a safety checker,
to ensure accurate and compliant support. To achieve robust step understanding,
we introduce Adaptive Step Fusion (ASF), which dynamically blends expert
reasoning with online adaptation from natural speech feedback. Furthermore, we
establish a new multi-agent coordination benchmark across representative task
categories and propose evaluation metrics tailored to industrial assistance,
enabling systematic comparison of different coordination topologies. Our
experiments demonstrate that MICA consistently improves task success,
reliability, and responsiveness over baseline structures, while remaining
deployable on practical offline hardware. Together, these contributions
highlight MICA as a step toward deployable, privacy-preserving multi-agent
assistants for dynamic factory environments. The source code will be made
publicly available at https://github.com/Kratos-Wen/MICA.

</details>


### [24] [KNARsack: Teaching Neural Algorithmic Reasoners to Solve Pseudo-Polynomial Problems](https://arxiv.org/abs/2509.15239)
*Stjepan Požgaj,Dobrik Georgiev,Marin Šilić,Petar Veličković*

Main category: cs.AI

TL;DR: 该论文提出了一种用于解决背包问题的神经算法推理器，通过模仿经典算法的两阶段动态规划过程，实现了比直接预测方法更好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 标准神经算法推理基准中缺少背包问题这一连接经典算法和组合优化的伪多项式问题，作者希望填补这一空白。

Method: 设计了一个紧密遵循背包问题两阶段流程的神经算法推理器：第一阶段构建动态规划表，第二阶段从表中重建解决方案，通过动态规划监督来建模中间状态。

Result: 该方法在泛化到更大问题实例时表现优于仅从问题输入直接预测最优子集的基线方法。

Conclusion: 通过模仿经典算法的结构，神经算法推理器能够更好地处理复杂的组合优化问题，特别是对于需要多步推理的伪多项式问题。

Abstract: Neural algorithmic reasoning (NAR) is a growing field that aims to embed
algorithmic logic into neural networks by imitating classical algorithms. In
this extended abstract, we detail our attempt to build a neural algorithmic
reasoner that can solve Knapsack, a pseudo-polynomial problem bridging
classical algorithms and combinatorial optimisation, but omitted in standard
NAR benchmarks. Our neural algorithmic reasoner is designed to closely follow
the two-phase pipeline for the Knapsack problem, which involves first
constructing the dynamic programming table and then reconstructing the solution
from it. The approach, which models intermediate states through dynamic
programming supervision, achieves better generalization to larger problem
instances than a direct-prediction baseline that attempts to select the optimal
subset only from the problem inputs.

</details>


### [25] [The Distribution Shift Problem in Transportation Networks using Reinforcement Learning and AI](https://arxiv.org/abs/2509.15291)
*Federico Taschin,Abderrahmane Lazaraq,Ozan K. Tonguz,Inci Ozgunes*

Main category: cs.AI

TL;DR: 本文评估了MetaLight这一元强化学习方法在交通信号控制中的应用，发现其在某些条件下表现良好，但在其他条件下可能产生高达22%的误差，表明元强化学习方案的鲁棒性不足。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习和人工智能在智能交通网络中的应用增加，强化学习显示出很大潜力。但训练后的强化学习智能体在面对动态变化的输入数据分布时存在可靠性问题，这可能导致严重后果。

Method: 本文评估和分析了一种先进的元强化学习方法MetaLight，通过在不同条件下测试其性能来验证其鲁棒性。

Result: 研究发现，在某些条件下MetaLight能产生相当好的结果，但在其他条件下表现不佳，误差可达22%，表明元强化学习方案往往不够鲁棒。

Conclusion: 元强化学习方案在交通信号控制中存在可靠性问题，需要进一步改进以提高其鲁棒性和可靠性。

Abstract: The use of Machine Learning (ML) and Artificial Intelligence (AI) in smart
transportation networks has increased significantly in the last few years.
Among these ML and AI approaches, Reinforcement Learning (RL) has been shown to
be a very promising approach by several authors. However, a problem with using
Reinforcement Learning in Traffic Signal Control is the reliability of the
trained RL agents due to the dynamically changing distribution of the input
data with respect to the distribution of the data used for training. This
presents a major challenge and a reliability problem for the trained network of
AI agents and could have very undesirable and even detrimental consequences if
a suitable solution is not found. Several researchers have tried to address
this problem using different approaches. In particular, Meta Reinforcement
Learning (Meta RL) promises to be an effective solution. In this paper, we
evaluate and analyze a state-of-the-art Meta RL approach called MetaLight and
show that, while under certain conditions MetaLight can indeed lead to
reasonably good results, under some other conditions it might not perform well
(with errors of up to 22%), suggesting that Meta RL schemes are often not
robust enough and can even pose major reliability problems.

</details>


### [26] [An Artificial Intelligence Driven Semantic Similarity-Based Pipeline for Rapid Literature](https://arxiv.org/abs/2509.15292)
*Abhiyan Dhakal,Kausik Paudel,Sanjog Sigdel*

Main category: cs.AI

TL;DR: 提出基于语义相似度的自动化文献综述流程，使用transformer嵌入和余弦相似度实现低开销高相关性


<details>
  <summary>Details</summary>
Motivation: 传统系统综述方法开销大，需要优化基于方法，本工作强调最小化开销和最大化相关性

Method: 通过论文标题和摘要生成关键词，从开放获取库获取相关论文，基于语义相似度排序，评估三种嵌入模型，应用统计阈值过滤相关论文

Result: 尽管缺乏启发式反馈或真实相关性标签，该系统显示出作为初步研究和探索性分析的可扩展实用工具的潜力

Conclusion: 该自动化文献综述管道在语义相似度方法上表现出色，为研究提供了高效实用的工具

Abstract: We propose an automated pipeline for performing literature reviews using
semantic similarity. Unlike traditional systematic review systems or
optimization based methods, this work emphasizes minimal overhead and high
relevance by using transformer based embeddings and cosine similarity. By
providing a paper title and abstract, it generates relevant keywords, fetches
relevant papers from open access repository, and ranks them based on their
semantic closeness to the input. Three embedding models were evaluated. A
statistical thresholding approach is then applied to filter relevant papers,
enabling an effective literature review pipeline. Despite the absence of
heuristic feedback or ground truth relevance labels, the proposed system shows
promise as a scalable and practical tool for preliminary research and
exploratory analysis.

</details>


### [27] [Knowledge-Driven Hallucination in Large Language Models: An Empirical Study on Process Modeling](https://arxiv.org/abs/2509.15336)
*Humam Kourani,Anton Antonov,Alessandro Berti,Wil M. P. van der Aalst*

Main category: cs.AI

TL;DR: 本文研究了大型语言模型在知识驱动幻觉方面的风险，即在自动化流程建模任务中，模型的输出会与明确来源证据相矛盾，因为被模型内部知识所覆盖。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的预训练知识虽然有助于分析任务，但也引入了知识驱动幻觉的风险，即模型输出会违背明确的来源证据。业务流程管理领域提供了理想的研究环境，因为许多核心业务流程遵循标准化模式。

Method: 通过受控实验设计，创建了提供证据与模型背景知识之间存在故意冲突的场景。使用描述标准和故意非典型流程结构的输入来测量模型对提供证据的忠实度。

Result: 研究发现LLMs在自动化流程建模任务中存在知识驱动幻觉现象，当提供证据与模型预训练知识冲突时，模型倾向于依赖内部知识而非明确证据。

Conclusion: 这项工作提供了评估这一关键可靠性问题的方法论，并提高了在任何基于证据的领域中对AI生成工件进行严格验证的必要性认识。

Abstract: The utility of Large Language Models (LLMs) in analytical tasks is rooted in
their vast pre-trained knowledge, which allows them to interpret ambiguous
inputs and infer missing information. However, this same capability introduces
a critical risk of what we term knowledge-driven hallucination: a phenomenon
where the model's output contradicts explicit source evidence because it is
overridden by the model's generalized internal knowledge. This paper
investigates this phenomenon by evaluating LLMs on the task of automated
process modeling, where the goal is to generate a formal business process model
from a given source artifact. The domain of Business Process Management (BPM)
provides an ideal context for this study, as many core business processes
follow standardized patterns, making it likely that LLMs possess strong
pre-trained schemas for them. We conduct a controlled experiment designed to
create scenarios with deliberate conflict between provided evidence and the
LLM's background knowledge. We use inputs describing both standard and
deliberately atypical process structures to measure the LLM's fidelity to the
provided evidence. Our work provides a methodology for assessing this critical
reliability issue and raises awareness of the need for rigorous validation of
AI-generated artifacts in any evidence-based domain.

</details>


### [28] [Diagnostics of cognitive failures in multi-agent expert systems using dynamic evaluation protocols and subsequent mutation of the processing context](https://arxiv.org/abs/2509.15366)
*Andrejs Sorstkins,Josh Bailey,Dr Alistair Baron*

Main category: cs.AI

TL;DR: 提出了一个用于评估和改进LLM智能体专家行为的诊断框架，通过黄金数据集、银数据集和智能体评判器来识别认知失败并引导智能体达到专家水平


<details>
  <summary>Details</summary>
Motivation: 传统评估方法无法充分诊断具有随机性和多步决策过程的LLM智能体的代理性能，需要新的框架来促进专家行为向LLM智能体的转移

Method: 集成三个组件：(i)专家标注的黄金数据集，(ii)通过受控行为变异生成的银数据集，(iii)基于LLM的智能体评判器，该评判器评分并制定针对性改进方案，这些方案嵌入到向量化推荐图中

Result: 在多智能体招聘助手系统上验证，发现潜在的认知失败（如偏见措辞、提取漂移和工具误路由），同时引导智能体达到专家级推理和风格

Conclusion: 为随机性、工具增强的LLM智能体建立了标准化、可复现的专家行为转移基础，从静态评估转向主动的专家系统优化

Abstract: The rapid evolution of neural architectures - from multilayer perceptrons to
large-scale Transformer-based models - has enabled language models (LLMs) to
exhibit emergent agentic behaviours when equipped with memory, planning, and
external tool use. However, their inherent stochasticity and multi-step
decision processes render classical evaluation methods inadequate for
diagnosing agentic performance. This work introduces a diagnostic framework for
expert systems that not only evaluates but also facilitates the transfer of
expert behaviour into LLM-powered agents. The framework integrates (i) curated
golden datasets of expert annotations, (ii) silver datasets generated through
controlled behavioural mutation, and (iii) an LLM-based Agent Judge that scores
and prescribes targeted improvements. These prescriptions are embedded into a
vectorized recommendation map, allowing expert interventions to propagate as
reusable improvement trajectories across multiple system instances. We
demonstrate the framework on a multi-agent recruiter-assistant system, showing
that it uncovers latent cognitive failures - such as biased phrasing,
extraction drift, and tool misrouting - while simultaneously steering agents
toward expert-level reasoning and style. The results establish a foundation for
standardized, reproducible expert behaviour transfer in stochastic,
tool-augmented LLM agents, moving beyond static evaluation to active expert
system refinement.

</details>


### [29] [FragmentRetro: A Quadratic Retrosynthetic Method Based on Fragmentation Algorithms](https://arxiv.org/abs/2509.15409)
*Yu Shee,Anthony M. Smaldone,Anton Morgunov,Gregory W. Kyro,Victor S. Batista*

Main category: cs.AI

TL;DR: FragmentRetro是一种新型的逆合成分析方法，通过分子碎片化和二次复杂度算法，解决了传统树搜索方法指数级计算复杂度的问题。


<details>
  <summary>Details</summary>
Motivation: 传统树搜索方法在计算机辅助合成规划中存在指数级计算复杂度的限制，需要更高效的计算方法。

Method: 结合BRICS和r-BRICS碎片化算法，采用库存感知探索和模式指纹筛选，实现二次复杂度递归组合分子片段。

Result: 在PaRoutes、USPTO-190和天然产物测试中，FragmentRetro实现了高解决率和竞争性运行时间，包括树搜索失败的情况。

Conclusion: FragmentRetro作为可扩展自动化合成规划的基础组件，具有显著的计算优势，能够高效生成基于碎片的解决方案。

Abstract: Retrosynthesis, the process of deconstructing a target molecule into simpler
precursors, is crucial for computer-aided synthesis planning (CASP). Widely
adopted tree-search methods often suffer from exponential computational
complexity. In this work, we introduce FragmentRetro, a novel retrosynthetic
method that leverages fragmentation algorithms, specifically BRICS and r-BRICS,
combined with stock-aware exploration and pattern fingerprint screening to
achieve quadratic complexity. FragmentRetro recursively combines molecular
fragments and verifies their presence in a building block set, providing sets
of fragment combinations as retrosynthetic solutions. We present the first
formal computational analysis of retrosynthetic methods, showing that tree
search exhibits exponential complexity $O(b^h)$, DirectMultiStep scales as
$O(h^6)$, and FragmentRetro achieves $O(h^2)$, where $h$ represents the number
of heavy atoms in the target molecule and $b$ is the branching factor for tree
search. Evaluations on PaRoutes, USPTO-190, and natural products demonstrate
that FragmentRetro achieves high solved rates with competitive runtime,
including cases where tree search fails. The method benefits from fingerprint
screening, which significantly reduces substructure matching complexity. While
FragmentRetro focuses on efficiently identifying fragment-based solutions
rather than full reaction pathways, its computational advantages and ability to
generate strategic starting candidates establish it as a powerful foundational
component for scalable and automated synthesis planning.

</details>


### [30] [Stress Testing Deliberative Alignment for Anti-Scheming Training](https://arxiv.org/abs/2509.15541)
*Bronson Schoen,Evgenia Nitishinskaya,Mikita Balesni,Axel Højmark,Felix Hofstätter,Jérémy Scheurer,Alexander Meinke,Jason Wolfe,Teun van der Weij,Alex Lloyd,Nicholas Goldowsky-Dill,Angela Fan,Andrei Matveiakin,Rusheb Shah,Marcus Williams,Amelia Glaese,Boaz Barak,Wojciech Zaremba,Marius Hobbhahn*

Main category: cs.AI

TL;DR: 该论文提出评估和缓解AI系统"密谋"（scheming）行为的方法，使用"隐蔽行动"作为密谋的代理指标，并通过26个分布外评估测试审议对齐方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 高度能力的AI系统可能秘密追求未对齐的目标（即"密谋"），由于密谋AI会故意隐藏其未对齐的目标和行为，需要不同于传统ML的策略来检测和缓解。

Method: 提出评估反密谋干预的三要素：在分布外任务上测试密谋倾向、评估是否由情境意识驱动、检查对预存未对齐目标的鲁棒性。使用审议对齐作为案例研究，通过180多个环境进行测试。

Result: 审议对齐显著降低了隐蔽行动率（从13%降至0.4%），但未能完全消除。模型能够停止追求预训练隐藏目标，但红队测试后仍发现不当行为。链式思维显示模型具有情境意识，这种意识会减少隐蔽行为。

Conclusion: 观察到的隐蔽行动率降低可能至少部分由情境意识驱动。随着模型推理偏离标准英语，依赖人类可读链式思维的能力会下降。需要进一步研究针对密谋的对齐缓解措施，特别是本文未涉及的欺骗性对齐对抗情况。

Abstract: Highly capable AI systems could secretly pursue misaligned goals -- what we
call "scheming". Because a scheming AI would deliberately try to hide its
misaligned goals and actions, measuring and mitigating scheming requires
different strategies than are typically used in ML. We propose that assessing
anti-scheming interventions requires at least (1) testing propensity to scheme
on far out-of-distribution (OOD) tasks, (2) evaluating whether lack of scheming
is driven by situational awareness, and (3) checking for robustness to
pre-existing misaligned goals. We use a broad category of "covert actions" --
such as secretly breaking rules or intentionally underperforming in tests -- as
a proxy for scheming, and design evaluations for covert actions. We then
stress-test deliberative alignment as a case study for anti-scheming. Across 26
OOD evaluations (180+ environments), deliberative alignment reduces covert
action rates (OpenAI o3: 13%->0.4%) but does not fully eliminate them. Our
mitigation is also able to largely stop agents from pursuing a hidden goal
previously trained into the model, but we still find misbehavior after
additional red-teaming. We find that models' chain-of-thought (CoT) often
demonstrates awareness of being evaluated for alignment, and show causal
evidence that this awareness decreases covert behavior, while unawareness
increases it. Therefore, we cannot exclude that the observed reductions in
covert action rates are at least partially driven by situational awareness.
While we rely on human-legible CoT for training, studying situational
awareness, and demonstrating clear evidence of misalignment, our ability to
rely on this degrades as models continue to depart from reasoning in standard
English. We encourage research into alignment mitigations for scheming and
their assessment, especially for the adversarial case of deceptive alignment,
which this paper does not address.

</details>


### [31] [MicroRCA-Agent: Microservice Root Cause Analysis Method Based on Large Language Model Agents](https://arxiv.org/abs/2509.15635)
*Pan Tang,Shixiang Tang,Huanqi Pu,Zhiqing Miao,Zhixing Wang*

Main category: cs.AI

TL;DR: MicroRCA-Agent是一个基于大语言模型代理的微服务根因分析解决方案，通过多模态数据融合构建智能故障根因定位系统。


<details>
  <summary>Details</summary>
Motivation: 解决微服务环境中复杂故障的根因定位问题，利用大语言模型的跨模态理解和逻辑推理能力来提升故障分析的准确性和效率。

Method: 1. 结合预训练的Drain日志解析算法和多级数据过滤机制压缩海量日志；2. 采用集成隔离森林无监督学习和状态码验证的双重异常检测方法；3. 设计统计对称比过滤机制和两阶段LLM分析策略，实现全栈现象总结。

Result: 在复杂微服务故障场景中表现出优越性能，最终得分达到50.71。消融研究验证了各模态数据的互补价值和系统架构的有效性。

Conclusion: MicroRCA-Agent通过创新的多模态数据融合和大语言模型应用，为微服务根因分析提供了有效的解决方案，代码已开源。

Abstract: This paper presents MicroRCA-Agent, an innovative solution for microservice
root cause analysis based on large language model agents, which constructs an
intelligent fault root cause localization system with multimodal data fusion.
The technical innovations are embodied in three key aspects: First, we combine
the pre-trained Drain log parsing algorithm with multi-level data filtering
mechanism to efficiently compress massive logs into high-quality fault
features. Second, we employ a dual anomaly detection approach that integrates
Isolation Forest unsupervised learning algorithms with status code validation
to achieve comprehensive trace anomaly identification. Third, we design a
statistical symmetry ratio filtering mechanism coupled with a two-stage LLM
analysis strategy to enable full-stack phenomenon summarization across
node-service-pod hierarchies. The multimodal root cause analysis module
leverages carefully designed cross-modal prompts to deeply integrate multimodal
anomaly information, fully exploiting the cross-modal understanding and logical
reasoning capabilities of large language models to generate structured analysis
results encompassing fault components, root cause descriptions, and reasoning
trace. Comprehensive ablation studies validate the complementary value of each
modal data and the effectiveness of the system architecture. The proposed
solution demonstrates superior performance in complex microservice fault
scenarios, achieving a final score of 50.71. The code has been released at:
https://github.com/tangpan360/MicroRCA-Agent.

</details>


### [32] [CCrepairBench: A High-Fidelity Benchmark and Reinforcement Learning Framework for C++ Compilation Repair](https://arxiv.org/abs/2509.15690)
*Weixuan Sun,Jucai Zhai,Dengfeng Liu,Xin Zhang,Xiaojun Wu,Qiaobo Hao,AIMgroup,Yang Fang,Jiuyang Tang*

Main category: cs.AI

TL;DR: 本文提出了一个解决C++编译错误的自动化修复框架，包含三个核心贡献：CCrepair数据集、基于强化学习的修复方法、以及两阶段评估系统。


<details>
  <summary>Details</summary>
Motivation: C++编译错误自动修复面临两大挑战：缺乏大规模高质量数据集，以及传统监督方法难以生成语义正确的补丁。

Method: 采用生成-验证管道构建CCrepair数据集，提出基于混合奖励信号的强化学习范式，建立以LLM为评判核心的两阶段评估系统。

Result: RL训练的Qwen2.5-1.5B模型性能与Qwen2.5-14B模型相当，验证了训练范式的效率。

Conclusion: 该工作为研究社区提供了有价值的新数据集和更有效的训练评估范式，为开发更实用可靠的自动化编程助手铺平了道路。

Abstract: The automated repair of C++ compilation errors presents a significant
challenge, the resolution of which is critical for developer productivity.
Progress in this domain is constrained by two primary factors: the scarcity of
large-scale, high-fidelity datasets and the limitations of conventional
supervised methods, which often fail to generate semantically correct
patches.This paper addresses these gaps by introducing a comprehensive
framework with three core contributions. First, we present CCrepair, a novel,
large-scale C++ compilation error dataset constructed through a sophisticated
generate-and-verify pipeline. Second, we propose a Reinforcement Learning (RL)
paradigm guided by a hybrid reward signal, shifting the focus from mere
compilability to the semantic quality of the fix. Finally, we establish the
robust, two-stage evaluation system providing this signal, centered on an
LLM-as-a-Judge whose reliability has been rigorously validated against the
collective judgments of a panel of human experts. This integrated approach
aligns the training objective with generating high-quality, non-trivial patches
that are both syntactically and semantically correct. The effectiveness of our
approach was demonstrated experimentally. Our RL-trained Qwen2.5-1.5B-Instruct
model achieved performance comparable to a Qwen2.5-14B-Instruct model,
validating the efficiency of our training paradigm. Our work provides the
research community with a valuable new dataset and a more effective paradigm
for training and evaluating robust compilation repair models, paving the way
for more practical and reliable automated programming assistants.

</details>


### [33] [A Nascent Taxonomy of Machine Learning in Intelligent Robotic Process Automation](https://arxiv.org/abs/2509.15730)
*Lukas Laakmann,Seyyid A. Ciftci,Christian Janiesch*

Main category: cs.AI

TL;DR: 本文通过文献综述探讨了RPA与机器学习的联系，提出了智能RPA的分类法，包含RPA-ML集成和RPA-ML交互两个元特征，共八个维度。


<details>
  <summary>Details</summary>
Motivation: RPA在自动化结构化任务方面具有优势，但其符号性质限制了处理复杂任务的能力。机器学习技术为扩展RPA的自动化范围提供了机会。

Method: 采用文献综述方法，分析RPA与机器学习的关联，并构建智能RPA的分类体系。

Result: 提出了包含八个维度的智能RPA分类法：架构与生态系统、能力、数据基础、智能水平、技术集成深度、部署环境、生命周期阶段和用户-机器人关系。

Conclusion: 该分类法为理解和组织智能RPA概念提供了系统框架，有助于推动RPA与机器学习的融合发展。

Abstract: Robotic process automation (RPA) is a lightweight approach to automating
business processes using software robots that emulate user actions at the
graphical user interface level. While RPA has gained popularity for its
cost-effective and timely automation of rule-based, well-structured tasks, its
symbolic nature has inherent limitations when approaching more complex tasks
currently performed by human agents. Machine learning concepts enabling
intelligent RPA provide an opportunity to broaden the range of automatable
tasks. In this paper, we conduct a literature review to explore the connections
between RPA and machine learning and organize the joint concept intelligent RPA
into a taxonomy. Our taxonomy comprises the two meta-characteristics RPA-ML
integration and RPA-ML interaction. Together, they comprise eight dimensions:
architecture and ecosystem, capabilities, data basis, intelligence level, and
technical depth of integration as well as deployment environment, lifecycle
phase, and user-robot relation.

</details>


### [34] [Ontology Creation and Management Tools: the Case of Anatomical Connectivity](https://arxiv.org/abs/2509.15780)
*Natallia Kokash,Bernard de Bono,Tom Gillespie*

Main category: cs.AI

TL;DR: 开发ApiNATOMY框架，用于外周神经系统和其他生理系统的多尺度生理回路图的拓扑和语义表示


<details>
  <summary>Details</summary>
Motivation: 支持研究人员映射与周围神经系统和其他生理系统相关的数据，强调它们与研究器官的相关性

Method: 创建包含知识表示模型和知识管理工具套件的框架，KR模型便于生理学专家捕获解剖实体间的相互作用，KM工具帮助建模者将高级抽象转换为详细的生理过程模型

Result: 建立了能够与外部本体和知识图谱集成的多尺度生理回路图表示框架

Conclusion: ApiNATOMY为生理系统的复杂网络研究提供了有效的知识表示和管理工具

Abstract: We are developing infrastructure to support researchers in mapping data
related to the peripheral nervous system and other physiological systems, with
an emphasis on their relevance to the organs under investigation. The nervous
system, a complex network of nerves and ganglia, plays a critical role in
coordinating and transmitting signals throughout the body. To aid in this, we
have created ApiNATOMY, a framework for the topological and semantic
representation of multiscale physiological circuit maps. ApiNATOMY integrates a
Knowledge Representation (KR) model and a suite of Knowledge Management (KM)
tools. The KR model enables physiology experts to easily capture interactions
between anatomical entities, while the KM tools help modelers convert
high-level abstractions into detailed models of physiological processes, which
can be integrated with external ontologies and knowledge graphs.

</details>


### [35] [Building Data-Driven Occupation Taxonomies: A Bottom-Up Multi-Stage Approach via Semantic Clustering and Multi-Agent Collaboration](https://arxiv.org/abs/2509.15786)
*Nan Li,Bo Kang,Tijl De Bie*

Main category: cs.AI

TL;DR: CLIMB是一个自动化构建高质量职业分类法的框架，通过全局语义聚类和多智能体系统从原始招聘信息中创建数据驱动的分类体系


<details>
  <summary>Details</summary>
Motivation: 手动构建职业分类法速度慢，现有自动化方法要么无法适应动态区域市场（自上而下），要么难以从噪声数据构建连贯的层次结构（自下而上）

Method: CLIMB使用全局语义聚类提炼核心职业，然后采用基于反射的多智能体系统迭代构建连贯的层次结构

Result: 在三个不同的真实数据集上，CLIMB生成的分类法比现有方法更连贯和可扩展，并成功捕捉了独特的区域特征

Conclusion: CLIMB能够完全自动化地从原始招聘信息中创建高质量、数据驱动的职业分类法

Abstract: Creating robust occupation taxonomies, vital for applications ranging from
job recommendation to labor market intelligence, is challenging. Manual
curation is slow, while existing automated methods are either not adaptive to
dynamic regional markets (top-down) or struggle to build coherent hierarchies
from noisy data (bottom-up). We introduce CLIMB (CLusterIng-based Multi-agent
taxonomy Builder), a framework that fully automates the creation of
high-quality, data-driven taxonomies from raw job postings. CLIMB uses global
semantic clustering to distill core occupations, then employs a
reflection-based multi-agent system to iteratively build a coherent hierarchy.
On three diverse, real-world datasets, we show that CLIMB produces taxonomies
that are more coherent and scalable than existing methods and successfully
capture unique regional characteristics. We release our code and datasets at
https://anonymous.4open.science/r/CLIMB.

</details>


### [36] [A Comparative Study of Rule-Based and Data-Driven Approaches in Industrial Monitoring](https://arxiv.org/abs/2509.15848)
*Giovanni De Gasperis,Sante Dino Facchini*

Main category: cs.AI

TL;DR: 本文比较了工业监控系统中基于规则的架构与数据驱动方法的优劣，提出了评估框架，并建议混合解决方案作为未来方向。


<details>
  <summary>Details</summary>
Motivation: 工业4.0环境下监控系统正从传统规则架构转向数据驱动方法，需要系统比较两种方法的优缺点和应用场景。

Method: 通过对比分析两种方法的关键特性，提出基本评估框架来评估它们各自的优势、局限性和适用场景。

Result: 规则系统在可解释性和确定性方面表现优异，适合稳定环境；数据驱动系统在异常检测和适应性方面更强，但面临数据可用性和可解释性挑战。

Conclusion: 未来工业监控的发展方向是智能协同系统，结合专家知识和数据驱动洞察，通过混合解决方案实现更好的弹性、效率和可信度。

Abstract: Industrial monitoring systems, especially when deployed in Industry 4.0
environments, are experiencing a shift in paradigm from traditional rule-based
architectures to data-driven approaches leveraging machine learning and
artificial intelligence. This study presents a comparison between these two
methodologies, analyzing their respective strengths, limitations, and
application scenarios, and proposes a basic framework to evaluate their key
properties. Rule-based systems offer high interpretability, deterministic
behavior, and ease of implementation in stable environments, making them ideal
for regulated industries and safety-critical applications. However, they face
challenges with scalability, adaptability, and performance in complex or
evolving contexts. Conversely, data-driven systems excel in detecting hidden
anomalies, enabling predictive maintenance and dynamic adaptation to new
conditions. Despite their high accuracy, these models face challenges related
to data availability, explainability, and integration complexity. The paper
suggests hybrid solutions as a possible promising direction, combining the
transparency of rule-based logic with the analytical power of machine learning.
Our hypothesis is that the future of industrial monitoring lies in intelligent,
synergic systems that leverage both expert knowledge and data-driven insights.
This dual approach enhances resilience, operational efficiency, and trust,
paving the way for smarter and more flexible industrial environments.

</details>


### [37] [EHR-MCP: Real-world Evaluation of Clinical Information Retrieval by Large Language Models via Model Context Protocol](https://arxiv.org/abs/2509.15957)
*Kanato Masayoshi,Masahiro Hashimoto,Ryoichi Yokoyama,Naoki Toda,Yoshifumi Uwamino,Shogo Fukuda,Ho Namkoong,Masahiro Jinzaki*

Main category: cs.AI

TL;DR: 该研究评估了通过MCP协议将LLM与医院EHR系统集成，在真实医院环境中自主检索临床信息的能力，在简单任务中表现接近完美，但在复杂任务中面临挑战。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在医疗领域有潜力，但在医院部署受限，主要障碍是难以访问电子健康记录系统。研究旨在探索通过MCP协议实现LLM与EHR安全集成的可行性。

Method: 开发了EHR-MCP框架，将自定义MCP工具与医院EHR数据库集成，使用GPT-4.1通过LangGraph ReAct代理进行交互。测试了6个感染控制团队相关任务，回顾性分析了8名患者数据，与医生生成的金标准进行比较。

Result: LLM能正确选择和执行MCP工具，除两个任务外，所有任务都达到接近完美的准确率。复杂的时间依赖性计算任务表现较差，错误主要来自参数不正确或工具结果误解。

Conclusion: LLM可以通过MCP工具从EHR中检索临床数据，在简单任务中表现优异，但在复杂任务中仍有挑战。EHR-MCP为医院AI代理提供了安全、一致的数据访问基础设施，未来工作应扩展到推理、生成和临床影响评估。

Abstract: Background: Large language models (LLMs) show promise in medicine, but their
deployment in hospitals is limited by restricted access to electronic health
record (EHR) systems. The Model Context Protocol (MCP) enables integration
between LLMs and external tools.
  Objective: To evaluate whether an LLM connected to an EHR database via MCP
can autonomously retrieve clinically relevant information in a real hospital
setting.
  Methods: We developed EHR-MCP, a framework of custom MCP tools integrated
with the hospital EHR database, and used GPT-4.1 through a LangGraph ReAct
agent to interact with it. Six tasks were tested, derived from use cases of the
infection control team (ICT). Eight patients discussed at ICT conferences were
retrospectively analyzed. Agreement with physician-generated gold standards was
measured.
  Results: The LLM consistently selected and executed the correct MCP tools.
Except for two tasks, all tasks achieved near-perfect accuracy. Performance was
lower in the complex task requiring time-dependent calculations. Most errors
arose from incorrect arguments or misinterpretation of tool results. Responses
from EHR-MCP were reliable, though long and repetitive data risked exceeding
the context window.
  Conclusions: LLMs can retrieve clinical data from an EHR via MCP tools in a
real hospital setting, achieving near-perfect performance in simple tasks while
highlighting challenges in complex ones. EHR-MCP provides an infrastructure for
secure, consistent data access and may serve as a foundation for hospital AI
agents. Future work should extend beyond retrieval to reasoning, generation,
and clinical impact assessment, paving the way for effective integration of
generative AI into clinical practice.

</details>


### [38] [Structured Information for Improving Spatial Relationships in Text-to-Image Generation](https://arxiv.org/abs/2509.15962)
*Sander Schildermans,Chang Tian,Ying Jiao,Marie-Francine Moens*

Main category: cs.AI

TL;DR: 本文提出了一种轻量级方法，通过使用微调的语言模型将自然语言提示转换为基于元组的结构化信息，从而增强文本到图像生成中的空间关系准确性。


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像生成技术虽然发展迅速，但在准确捕捉自然语言提示中的空间关系方面仍存在重大挑战。现有方法通过提示优化、空间基础生成和语义细化来解决此问题，但仍需改进。

Method: 采用轻量级方法，通过微调的语言模型自动将提示转换为基于元组的结构化信息，并将其无缝集成到文本到图像生成流程中。

Result: 实验结果表明，该方法在空间准确性方面有显著提升，且不影响以Inception Score衡量的整体图像质量。自动生成的元组质量与人工制作的元组相当。

Conclusion: 这种结构化信息为增强文本到图像生成中的空间关系提供了实用且可移植的解决方案，解决了当前大规模生成系统的关键限制。

Abstract: Text-to-image (T2I) generation has advanced rapidly, yet faithfully capturing
spatial relationships described in natural language prompts remains a major
challenge. Prior efforts have addressed this issue through prompt optimization,
spatially grounded generation, and semantic refinement. This work introduces a
lightweight approach that augments prompts with tuple-based structured
information, using a fine-tuned language model for automatic conversion and
seamless integration into T2I pipelines. Experimental results demonstrate
substantial improvements in spatial accuracy, without compromising overall
image quality as measured by Inception Score. Furthermore, the automatically
generated tuples exhibit quality comparable to human-crafted tuples. This
structured information provides a practical and portable solution to enhance
spatial relationships in T2I generation, addressing a key limitation of current
large-scale generative systems.

</details>


### [39] [Attention Schema-based Attention Control (ASAC): A Cognitive-Inspired Approach for Attention Management in Transformers](https://arxiv.org/abs/2509.16058)
*Krati Saxena,Federico Jurado Ruiz,Guido Manzi,Dianbo Liu,Alex Lamb*

Main category: cs.AI

TL;DR: 本文提出ASAC（基于注意力模式的注意力控制）方法，将认知科学中的注意力模式理论融入人工神经网络，通过VQVAE作为注意力抽象器和控制器，在视觉和NLP领域提升分类准确性和学习效率。


<details>
  <summary>Details</summary>
Motivation: 受认知科学中注意力模式理论（AST）的启发，该理论认为人类通过建模注意力本身来管理注意力分配。作者希望将这一认知机制引入AI系统，以更有效地利用注意力机制。

Method: 在Transformer架构中嵌入ASAC模块，使用向量量化变分自编码器（VQVAE）作为注意力抽象器和控制器，显式建模注意力分配机制。

Result: 实验表明ASAC在视觉和NLP任务中都能提升分类准确性、加速学习过程，并展现出对噪声数据、分布外数据的鲁棒性，在多任务学习和对抗攻击下也表现良好。

Conclusion: 该方法成功建立了认知科学与机器学习之间的联系，为AI系统中注意力机制的高效利用提供了新思路，展示了基于注意力模式的控制在提升模型性能和效率方面的潜力。

Abstract: Attention mechanisms have become integral in AI, significantly enhancing
model performance and scalability by drawing inspiration from human cognition.
Concurrently, the Attention Schema Theory (AST) in cognitive science posits
that individuals manage their attention by creating a model of the attention
itself, effectively allocating cognitive resources. Inspired by AST, we
introduce ASAC (Attention Schema-based Attention Control), which integrates the
attention schema concept into artificial neural networks. Our initial
experiments focused on embedding the ASAC module within transformer
architectures. This module employs a Vector-Quantized Variational AutoEncoder
(VQVAE) as both an attention abstractor and controller, facilitating precise
attention management. By explicitly modeling attention allocation, our approach
aims to enhance system efficiency. We demonstrate ASAC's effectiveness in both
the vision and NLP domains, highlighting its ability to improve classification
accuracy and expedite the learning process. Our experiments with vision
transformers across various datasets illustrate that the attention controller
not only boosts classification accuracy but also accelerates learning.
Furthermore, we have demonstrated the model's robustness and generalization
capabilities across noisy and out-of-distribution datasets. In addition, we
have showcased improved performance in multi-task settings. Quick experiments
reveal that the attention schema-based module enhances resilience to
adversarial attacks, optimizes attention to improve learning efficiency, and
facilitates effective transfer learning and learning from fewer examples. These
promising results establish a connection between cognitive science and machine
learning, shedding light on the efficient utilization of attention mechanisms
in AI systems.

</details>
