<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 22]
- [cs.CR](#cs.CR) [Total: 36]
- [cs.AI](#cs.AI) [Total: 26]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [CWM: An Open-Weights LLM for Research on Code Generation with World Models](https://arxiv.org/abs/2510.02387)
*FAIR CodeGen team,Quentin Carbonneaux,Gal Cohen,Jonas Gehring,Jacob Kahn,Jannik Kossen,Felix Kreuk,Emily McMilin,Michel Meyer,Yuxiang Wei,David Zhang,Kunhao Zheng,Jordi Armengol-Estapé,Pedram Bashiri,Maximilian Beck,Pierre Chambon,Abhishek Charnalia,Chris Cummins,Juliette Decugis,Zacharias V. Fisches,François Fleuret,Fabian Gloeckle,Alex Gu,Michael Hassid,Daniel Haziza,Badr Youbi Idrissi,Christian Keller,Rahul Kindi,Hugh Leather,Gallil Maimon,Aram Markosyan,Francisco Massa,Pierre-Emmanuel Mazaré,Vegard Mella,Naila Murray,Keyur Muzumdar,Peter O'Hearn,Matteo Pagliardini,Dmitrii Pedchenko,Tal Remez,Volker Seeker,Marco Selvi,Oren Sultan,Sida Wang,Luca Wehrstedt,Ori Yoran,Lingming Zhang,Taco Cohen,Yossi Adi,Gabriel Synnaeve*

Main category: cs.SE

TL;DR: Code World Model (CWM) 是一个320亿参数的开源LLM，通过在Python解释器和Docker环境中训练观察-行动轨迹，结合多任务推理强化学习，提升代码生成和理解能力。


<details>
  <summary>Details</summary>
Motivation: 为了超越仅从静态代码训练中获得的理解能力，探索世界模型如何通过环境交互提升代码生成中的推理和规划能力。

Method: 在Python解释器和Docker环境中进行大规模观察-行动轨迹的中期训练，并在可验证的编码、数学和多轮软件工程环境中进行多任务推理强化学习。

Result: CWM在多个基准测试中表现优异：SWE-bench Verified 65.8%、LiveCodeBench 68.6%、Math-500 96.6%、AIME 2024 76.0%。模型支持Python代码逐步执行模拟，并展示了推理能力的提升。

Conclusion: CWM为研究人员提供了探索世界模型在代码生成中应用的强大测试平台，展示了世界模型在智能编码和代码执行模拟方面的潜力。

Abstract: We release Code World Model (CWM), a 32-billion-parameter open-weights LLM,
to advance research on code generation with world models. To improve code
understanding beyond what can be learned from training on static code alone, we
mid-train CWM on a large amount of observation-action trajectories from Python
interpreter and agentic Docker environments, and perform extensive multi-task
reasoning RL in verifiable coding, math, and multi-turn software engineering
environments. With CWM, we provide a strong testbed for researchers to explore
the opportunities world modeling affords for improving code generation with
reasoning and planning in computational environments. We present first steps of
how world models can benefit agentic coding, enable step-by-step simulation of
Python code execution, and show early results of how reasoning can benefit from
the latter. CWM is a dense, decoder-only LLM trained with a context size of up
to 131k tokens. Independent of its world modeling capabilities, CWM offers
strong performance on general coding and math tasks: it reaches pass@1 scores
of 65.8% on SWE-bench Verified (with test-time scaling), 68.6% on
LiveCodeBench, 96.6% on Math-500, and 76.0% on AIME 2024. To support further
research on code world modeling, we release model checkpoints after
mid-training, SFT, and RL.

</details>


### [2] [From Trace to Line: LLM Agent for Real-World OSS Vulnerability Localization](https://arxiv.org/abs/2510.02389)
*Haoran Xi,Minghao Shao,Brendan Dolan-Gavitt,Muhammad Shafique,Ramesh Karri*

Main category: cs.SE

TL;DR: T2L-Agent是一个项目级端到端框架，通过从模块逐步缩小范围到具体漏洞行号，结合运行时证据和AST代码分块，实现精确的漏洞定位和修复指导。


<details>
  <summary>Details</summary>
Motivation: 现有漏洞检测方法孤立分析代码、难以处理长上下文、主要关注函数或文件级别的粗粒度检测，无法为工程师提供精确的行级定位和针对性补丁指导。

Method: T2L-Agent采用多轮反馈机制，结合Agentic Trace Analyzer(ATA)融合运行时证据（崩溃点、堆栈跟踪、覆盖率差异）和基于AST的代码分块，实现迭代精炼的行级诊断。

Result: 在T2L-ARVO基准测试中，T2L-Agent达到58.0%的检测率和54.8%的行级定位率，显著优于基线方法。

Conclusion: 该框架和基准测试共同推动基于LLM的漏洞检测从粗粒度识别向可部署、鲁棒、精确的诊断发展，减少噪音并加速开源软件工作流中的补丁过程。

Abstract: Large language models show promise for vulnerability discovery, yet
prevailing methods inspect code in isolation, struggle with long contexts, and
focus on coarse function- or file-level detections - offering limited
actionable guidance to engineers who need precise line-level localization and
targeted patches in real-world software development. We present T2L-Agent
(Trace-to-Line Agent), a project-level, end-to-end framework that plans its own
analysis and progressively narrows scope from modules to exact vulnerable
lines. T2L-Agent couples multi-round feedback with an Agentic Trace Analyzer
(ATA) that fuses runtime evidence - crash points, stack traces, and coverage
deltas - with AST-based code chunking, enabling iterative refinement beyond
single pass predictions and translating symptoms into actionable, line-level
diagnoses. To benchmark line-level vulnerability discovery, we introduce
T2L-ARVO, a diverse, expert-verified 50-case benchmark spanning five crash
families and real-world projects. T2L-ARVO is specifically designed to support
both coarse-grained detection and fine-grained localization, enabling rigorous
evaluation of systems that aim to move beyond file-level predictions. On
T2L-ARVO, T2L-Agent achieves up to 58.0% detection and 54.8% line-level
localization, substantially outperforming baselines. Together, the framework
and benchmark push LLM-based vulnerability detection from coarse identification
toward deployable, robust, precision diagnostics that reduce noise and
accelerate patching in open-source software workflows.

</details>


### [3] [AP2O: Correcting LLM-Generated Code Errors Type by Type Like Humans via Adaptive Progressive Preference Optimization](https://arxiv.org/abs/2510.02393)
*Jianqing Zhang,Wei Xia,Hande Dong,Qiang Lin,Jian Cao*

Main category: cs.SE

TL;DR: AP2O-Coder是一种自适应渐进式偏好优化方法，通过构建错误笔记本并逐步纠正不同类型错误，提升LLM代码生成质量，在减少偏好数据使用的同时将pass@k指标提升达3%。


<details>
  <summary>Details</summary>
Motivation: 现有离线偏好优化方法主要关注代码通过/失败信号，忽略了失败代码中的深层错误类型，导致LLM生成的代码仍存在编译和运行时错误。

Method: 构建错误笔记本记录失败代码中的错误类型，然后逐步优化LLM按类型纠正错误，并在训练过程中自适应重放错误类型以适应LLM不断变化的弱点。

Result: 在0.5B到34B参数的代码和通用LLM（Llama、Qwen、DeepSeek系列）上实验表明，AP2O-Coder使用更少偏好数据将代码生成性能提升达3%（pass@k指标）。

Conclusion: AP2O-Coder通过关注深层错误类型和自适应渐进优化，有效提升了LLM代码生成的准确性和效率。

Abstract: LLMs' code generation capabilities have yielded substantial improvements in
the effectiveness of programming tasks. However, LLM-generated code still
suffers from compilation and runtime errors. Existing offline preference
optimization methods primarily focus on enhancing LLMs' coding abilities using
pass/fail signals in the preference data, overlooking the deep-level error
types in the failed codes. To address this, we propose Adaptively Progressive
Preference Optimization (AP2O) for coding (i.e., AP2O-Coder), a method that
guides LLMs adaptively and methodically to reduce code errors for code
generation. Specifically, we construct an error notebook from failed codes and
progressively optimize the LLM to correct errors type by type. Furthermore, we
adaptively replay error types to tailor to the LLM's changing weaknesses
throughout the training process. Through extensive experiments on both code and
general LLMs (Llama, Qwen, and DeepSeek series) with parameters ranging from
0.5B to 34B, our AP2O-Coder improves code generation performance by up to 3% in
pass@k while using less preference data. Code: https://github.com/TsingZ0/AP2O

</details>


### [4] [Dynamic Function Configuration and its Management in Serverless Computing: A Taxonomy and Future Directions](https://arxiv.org/abs/2510.02404)
*Siddharth Agarwal,Maria A. Rodriguez,Rajkumar Buyya*

Main category: cs.SE

TL;DR: 本文分析了FaaS环境中的资源配置问题，提出了影响函数设计、配置、运行时成本和性能保证的因素分类法，并对现有资源配置文献进行了全面综述。


<details>
  <summary>Details</summary>
Motivation: 无服务器计算平台缺乏透明度，迫使开发者依赖专家知识或经验进行资源配置决策，这使得在满足性能约束的同时实现最优资源配置成为一项非平凡任务。

Method: 识别FaaS环境中资源配置技术的不同方面，提出影响函数设计、配置、运行时成本和性能保证的因素分类法，并对现有资源配置文献进行分析。

Result: 提出了一个全面的资源配置因素分类法，分析了当前函数配置研究现状，并识别了现有研究空白。

Conclusion: 需要进一步研究来增强函数配置能力，加强无服务器计算环境的功能，推动其更广泛采用。

Abstract: The serverless cloud computing model offers a framework where the service
provider abstracts the underlying infrastructure management from developers. In
this serverless model, FaaS provides an event-driven, function-oriented
computing service characterised by fine-grained, usage-based pricing that
eliminates cost for idle resources. Platforms like AWS Lambda, Azure Functions,
and Cloud Run Functions require developers to configure their function(s) with
minimum operational resources for its successful execution. This resource
allocation influences both the operational expense and the performance quality
of these functions. However, a noticeable lack of platform transparency forces
developers to rely on expert knowledge or experience-based ad-hoc decisions to
request desired function resources. This makes optimal resource configuration a
non-trivial task while adhering to performance constraints. Furthermore, while
commercial platforms often scale resources like CPU and network bandwidth
proportional to memory, open-source frameworks permit independent configuration
of function resources, introducing additional complexity for developers aiming
to optimise their functions. These complexities have directed researchers to
resolve developer challenges and advance towards an efficient server-less
execution model. In this article, we identify different aspects of resource
configuration techniques in FaaS settings and propose a taxonomy of factors
that influence function design, configuration, run-time cost, and performance
guarantees. We conduct an analysis of existing literature on resource
configuration to present a comprehensive review of current studies on function
configuration. We also identify existing research gaps and suggest future
research directions to enhance function configuration and strengthen the
capabilities of serverless computing environments to drive its broader
adoption.

</details>


### [5] [Product Manager Practices for Delegating Work to Generative AI: "Accountability must not be delegated to non-human actors"](https://arxiv.org/abs/2510.02504)
*Mara Ulloa,Jenna L. Butler,Sankeerti Haniyur,Courtney Miller,Barrett Amos,Advait Sarkar,Margaret-Anne Storey*

Main category: cs.SE

TL;DR: 研究微软产品经理对生成式AI的采用情况、使用案例、感知利益与障碍，以及任务委派框架和角色适应实践。


<details>
  <summary>Details</summary>
Motivation: 生成式AI正在改变知识工作的性质，但现有研究主要关注开发者与GenAI的交互，对产品经理工作如何因GenAI而演变的理解较少。

Method: 在微软进行混合方法研究：调查885名产品经理，分析731名产品经理的遥测数据，访谈15名产品经理。

Result: 提供了产品经理当前的GenAI采用率、使用案例、感知利益与障碍；提出了产品经理评估哪些任务委派给GenAI的框架；描述了产品经理整合GenAI到其角色的适应实践。

Conclusion: 讨论了更广泛的GenAI工作流采用过程和软件开发角色的影响。

Abstract: Generative AI (GenAI) is changing the nature of knowledge work, particularly
for Product Managers (PMs) in software development teams. While much software
engineering research has focused on developers' interactions with GenAI, there
is less understanding of how the work of PMs is evolving due to GenAI. To
address this gap, we conducted a mixed-methods study at Microsoft, a large,
multinational software company: surveying 885 PMs, analyzing telemetry data for
a subset of PMs (N=731), and interviewing a subset of 15 PMs. We contribute:
(1) PMs' current GenAI adoption rates, uses cases, and perceived benefits and
barriers and; (2) a framework capturing how PMs assess which tasks to delegate
to GenAI; (3) PMs adaptation practices for integrating GenAI into their roles
and perceptions of how their role is evolving. We end by discussing
implications on the broader GenAI workflow adoption process and software
development roles.

</details>


### [6] [ZeroFalse: Improving Precision in Static Analysis with LLMs](https://arxiv.org/abs/2510.02534)
*Mohsen Iranmanesh,Sina Moradi Sabet,Sina Marefat,Ali Javidi Ghasr,Allison Wilson,Iman Sharafaldin,Mohammad A. Tayebi*

Main category: cs.SE

TL;DR: ZeroFalse框架结合静态分析和LLM来减少SAST工具的误报，同时保持覆盖率，在多个基准测试中实现高F1分数


<details>
  <summary>Details</summary>
Motivation: SAST工具存在过多误报问题，削弱开发者信任并需要昂贵的人工分类，需要一种既能保持静态分析系统性覆盖又能减少误报的解决方案

Method: 将静态分析器输出视为结构化合约，通过流敏感追踪、上下文证据和CWE特定知识进行丰富，然后由LLM进行裁决

Result: 在OWASP Java Benchmark上F1分数达0.912，在OpenVuln数据集上达0.955，召回率和精确率均超过90%

Conclusion: ZeroFalse是增强SAST可靠性的实用且可扩展方法，支持其集成到真实CI/CD流水线中

Abstract: Static Application Security Testing (SAST) tools are integral to modern
software development, yet their adoption is undermined by excessive false
positives that weaken developer trust and demand costly manual triage. We
present ZeroFalse, a framework that integrates static analysis with large
language models (LLMs) to reduce false positives while preserving coverage.
ZeroFalse treats static analyzer outputs as structured contracts, enriching
them with flow-sensitive traces, contextual evidence, and CWE-specific
knowledge before adjudication by an LLM. This design preserves the systematic
reach of static analysis while leveraging the reasoning capabilities of LLMs.
We evaluate ZeroFalse across both benchmarks and real-world projects using ten
state-of-the-art LLMs. Our best-performing models achieve F1-scores of 0.912 on
the OWASP Java Benchmark and 0.955 on the OpenVuln dataset, maintaining recall
and precision above 90%. Results further show that CWE-specialized prompting
consistently outperforms generic prompts, and reasoning-oriented LLMs provide
the most reliable precision-recall balance. These findings position ZeroFalse
as a practical and scalable approach for enhancing the reliability of SAST and
supporting its integration into real-world CI/CD pipelines.

</details>


### [7] [Key Considerations for Auto-Scaling: Lessons from Benchmark Microservices](https://arxiv.org/abs/2510.02585)
*Majid Dashtbani,Ladan Tahvildari*

Main category: cs.SE

TL;DR: 该论文通过分析微服务基准测试，识别了影响自动扩缩容效果的关键生命周期因素，并将其分类为架构、实现和部署三个阶段的问题，强调了生命周期感知工程对提升自动扩缩容性能的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试往往忽视微服务系统的基础设计、实现和部署实践，导致难以在真实条件下评估自动扩缩容方法。

Method: 将多种先进的自动扩缩容方法应用于广泛使用的微服务基准测试，识别实践中的扩缩容考虑因素，并按软件生命周期分类：架构、实现和部署阶段。

Result: 忽视关键生命周期问题会降低自动扩缩容器性能，而解决这些问题能带来更稳定和高效的扩缩容效果。

Conclusion: 生命周期感知工程对于释放微服务系统中自动扩缩容的全部潜力至关重要。

Abstract: Microservices have become the dominant architectural paradigm for building
scalable and modular cloud-native systems. However, achieving effective
auto-scaling in such systems remains a non-trivial challenge, as it depends not
only on advanced scaling techniques but also on sound design, implementation,
and deployment practices. Yet, these foundational aspects are often overlooked
in existing benchmarks, making it difficult to evaluate autoscaling methods
under realistic conditions. In this paper, we identify a set of practical
auto-scaling considerations by applying several state-of-the-art autoscaling
methods to widely used microservice benchmarks. To structure these findings, we
classify the issues based on when they arise during the software lifecycle:
Architecture, Implementation, and Deployment. The Architecture phase covers
high-level decisions such as service decomposition and inter-service
dependencies. The Implementation phase includes aspects like initialization
overhead, metrics instrumentation, and error propagation. The Deployment phase
focuses on runtime configurations such as resource limits and health checks. We
validate these considerations using the Sock-Shop benchmark and evaluate
diverse auto-scaling strategies, including threshold-based, control-theoretic,
learning-based, black-box optimization, and dependency-aware approaches. Our
findings show that overlooking key lifecycle concerns can degrade autoscaler
performance, while addressing them leads to more stable and efficient scaling.
These results underscore the importance of lifecycle-aware engineering for
unlocking the full potential of auto-scaling in microservice-based systems.

</details>


### [8] [RedCodeAgent: Automatic Red-teaming Agent against Diverse Code Agents](https://arxiv.org/abs/2510.02609)
*Chengquan Guo,Chulin Xie,Yu Yang,Zhaorun Chen,Zinan Lin,Xander Davies,Yarin Gal,Dawn Song,Bo Li*

Main category: cs.SE

TL;DR: 提出了RedCodeAgent，首个自动化红队代理，用于系统性地发现各种代码代理中的漏洞。通过自适应记忆模块和定制工具箱，能动态选择最有效的红队工具组合，在模拟沙盒环境中可靠评估执行结果。


<details>
  <summary>Details</summary>
Motivation: 代码代理的广泛应用带来了关键的安全风险，现有的静态安全基准和红队工具无法覆盖某些边界条件（如不同越狱工具的组合效应），无法识别新兴的现实世界风险场景。

Method: 开发了RedCodeAgent，配备自适应记忆模块来利用现有越狱知识，动态选择最有效的红队工具和工具组合。创建模拟沙盒环境来评估代码代理的执行结果，减少仅依赖静态代码的LLM评估器的潜在偏见。

Result: 在多个最先进的代码代理、多样化风险场景和各种编程语言的广泛评估中，RedCodeAgent始终优于现有红队方法，实现了更高的攻击成功率、更低的拒绝率和高效率。在真实世界代码助手（如Cursor和Codeium）上验证，暴露了先前未识别的安全风险。

Conclusion: 通过自动化和优化红队流程，RedCodeAgent实现了对代码代理的可扩展、自适应和有效的安全评估。

Abstract: Code agents have gained widespread adoption due to their strong code
generation capabilities and integration with code interpreters, enabling
dynamic execution, debugging, and interactive programming capabilities. While
these advancements have streamlined complex workflows, they have also
introduced critical safety and security risks. Current static safety benchmarks
and red-teaming tools are inadequate for identifying emerging real-world risky
scenarios, as they fail to cover certain boundary conditions, such as the
combined effects of different jailbreak tools. In this work, we propose
RedCodeAgent, the first automated red-teaming agent designed to systematically
uncover vulnerabilities in diverse code agents. With an adaptive memory module,
RedCodeAgent can leverage existing jailbreak knowledge, dynamically select the
most effective red-teaming tools and tool combinations in a tailored toolbox
for a given input query, thus identifying vulnerabilities that might otherwise
be overlooked. For reliable evaluation, we develop simulated sandbox
environments to additionally evaluate the execution results of code agents,
mitigating potential biases of LLM-based judges that only rely on static code.
Through extensive evaluations across multiple state-of-the-art code agents,
diverse risky scenarios, and various programming languages, RedCodeAgent
consistently outperforms existing red-teaming methods, achieving higher attack
success rates and lower rejection rates with high efficiency. We further
validate RedCodeAgent on real-world code assistants, e.g., Cursor and Codeium,
exposing previously unidentified security risks. By automating and optimizing
red-teaming processes, RedCodeAgent enables scalable, adaptive, and effective
safety assessments of code agents.

</details>


### [9] [Automatic Building Code Review: A Case Study](https://arxiv.org/abs/2510.02634)
*Hanlong Wan,Weili Xu,Michael Rosenberg,Jian Zhang,Aysha Siddika*

Main category: cs.SE

TL;DR: 提出了一种基于BIM和LLM的自动化建筑规范审查框架，结合RAG和MCP代理管道，能够从异构文件中提取几何、时间表和系统属性，并通过COMcheck API和RAG推理进行规范检查。


<details>
  <summary>Details</summary>
Motivation: 资源受限或农村地区的建筑官员面临劳动密集型、易出错且成本高昂的设计文件手动审查，随着项目规模和复杂性的增加，需要自动化解决方案。

Method: 采用代理驱动框架，集成BIM数据提取与自动化验证，使用RAG和MCP代理管道，通过COMcheck API进行确定性检查，RAG进行灵活推理。

Result: 案例演示显示框架能自动提取几何属性、解析操作时间表、验证照明限额，GPT-4o在效率和稳定性方面表现最佳，MCP代理管道在严谨性和可靠性上优于RAG。

Conclusion: 该工作推进了ACR研究，展示了一种可扩展、可互操作且生产就绪的方法，将BIM与权威规范审查工具连接起来。

Abstract: Building officials, particularly those in resource-constrained or rural
jurisdictions, face labor-intensive, error-prone, and costly manual reviews of
design documents as projects increase in size and complexity. The growing
adoption of Building Information Modeling (BIM) and Large Language Models
(LLMs) presents opportunities for automated code review (ACR) solutions. This
study introduces a novel agent-driven framework that integrates BIM-based data
extraction with automated verification using both retrieval-augmented
generation (RAG) and Model Context Protocol (MCP) agent pipelines. The
framework employs LLM-enabled agents to extract geometry, schedules, and system
attributes from heterogeneous file types, which are then processed for building
code checking through two complementary mechanisms: (1) direct API calls to the
US Department of Energy COMcheck engine, providing deterministic and
audit-ready outputs, and (2) RAG-based reasoning over rule provisions, enabling
flexible interpretation where coverage is incomplete or ambiguous.
  The framework was evaluated through case demonstrations, including automated
extraction of geometric attributes (such as surface area, tilt, and insulation
values), parsing of operational schedules, and validation of lighting
allowances under ASHRAE Standard 90.1-2022. Comparative performance tests
across multiple LLMs showed that GPT-4o achieved the best balance of efficiency
and stability, while smaller models exhibited inconsistencies or failures.
Results confirm that MCP agent pipelines outperform RAG reasoning pipelines in
rigor and reliability. This work advances ACR research by demonstrating a
scalable, interoperable, and production-ready approach that bridges BIM with
authoritative code review tools.

</details>


### [10] [Using Fourier Analysis and Mutant Clustering to Accelerate DNN Mutation Testing](https://arxiv.org/abs/2510.02718)
*Ali Ghanbari,Sasan Tavakkol*

Main category: cs.SE

TL;DR: DM#是一种基于傅里叶分析的深度学习网络变异测试加速技术，通过量化变异体行为进行聚类，选择代表性变异体测试并将结果复用给同类变异体，显著减少测试成本。


<details>
  <summary>Details</summary>
Motivation: 深度学习网络变异分析成本高昂，因为需要测试大量变异体和大规模数据集。

Method: 利用傅里叶分析量化变异体行为，进行聚类分组，每组选择代表性变异体测试并复用结果。

Result: 在14个不同规模的DNN模型上评估，平均加速28.38%，变异分数误差仅0.72%。相比随机选择、边界样本选择和随机样本选择技术，变异分数误差分别减少11.78、15.16和114.36倍。

Conclusion: DM#能有效加速深度学习网络变异测试，在保持高精度的同时显著降低计算成本。

Abstract: Deep neural network (DNN) mutation analysis is a promising approach to
evaluating test set adequacy. Due to the large number of generated mutants that
must be tested on large datasets, mutation analysis is costly. In this paper,
we present a technique, named DM#, for accelerating DNN mutation testing using
Fourier analysis. The key insight is that DNN outputs are real-valued functions
suitable for Fourier analysis that can be leveraged to quantify mutant behavior
using only a few data points. DM# uses the quantified mutant behavior to
cluster the mutants so that the ones with similar behavior fall into the same
group. A representative from each group is then selected for testing, and the
result of the test, e.g., whether the mutant is killed or survived, is reused
for all other mutants represented by the selected mutant, obviating the need
for testing other mutants. 14 DNN models of sizes ranging from thousands to
millions of parameters, trained on different datasets, are used to evaluate DM#
and compare it to several baseline techniques. Our results provide empirical
evidence on the effectiveness of DM# in accelerating mutation testing by
28.38%, on average, at the average cost of only 0.72% error in mutation score.
Moreover, on average, DM# incurs 11.78, 15.16, and 114.36 times less mutation
score error compared to random mutant selection, boundary sample selection, and
random sample selection techniques, respectively, while generally offering
comparable speed-up.

</details>


### [11] [Automated Repair of OpenID Connect Programs (Extended Version)](https://arxiv.org/abs/2510.02773)
*Tamjid Al Rahat,Yanju Chen,Yu Feng,Yuan Tian*

Main category: cs.SE

TL;DR: AuthFix是一个基于LLM的OpenID Connect漏洞自动修复引擎，通过反例引导的修复方法，在23个漏洞中成功修复了17个（74%），修复效果与开发者编写的补丁语义等效。


<details>
  <summary>Details</summary>
Motivation: OpenID Connect虽然广泛采用，但存在严重安全漏洞导致重大经济损失和安全漏洞，需要强大的缓解策略。自动程序修复为OpenID实现生成候选补丁提供了有前景的解决方案。

Method: AuthFix集成三个关键组件：故障定位、补丁合成和补丁验证。采用新颖的Petri网模型检查器，通过有效建模交互来确保补丁的正确性。

Result: 在OpenID漏洞数据集上的评估显示，AuthFix成功为23个漏洞中的17个生成了正确补丁（74%），其中大部分补丁与开发者编写的修复在语义上等效。

Conclusion: AuthFix证明了基于LLM的反例引导修复方法在OpenID Connect安全漏洞修复中的有效性，能够生成高质量且语义正确的补丁。

Abstract: OpenID Connect has revolutionized online authentication based on single
sign-on (SSO) by providing a secure and convenient method for accessing
multiple services with a single set of credentials. Despite its widespread
adoption, critical security bugs in OpenID Connect have resulted in significant
financial losses and security breaches, highlighting the need for robust
mitigation strategies. Automated program repair presents a promising solution
for generating candidate patches for OpenID implementations. However,
challenges such as domain-specific complexities and the necessity for precise
fault localization and patch verification must be addressed. We propose
AuthFix, a counterexample-guided repair engine leveraging LLMs for automated
OpenID bug fixing. AuthFix integrates three key components: fault localization,
patch synthesis, and patch verification. By employing a novel Petri-net-based
model checker, AuthFix ensures the correctness of patches by effectively
modeling interactions. Our evaluation on a dataset of OpenID bugs demonstrates
that AuthFix successfully generated correct patches for 17 out of 23 bugs
(74%), with a high proportion of patches semantically equivalent to
developer-written fixes.

</details>


### [12] [C2|Q>: A Robust Framework for Bridging Classical and Quantum Software Development](https://arxiv.org/abs/2510.02854)
*Boshuai Ye,Arif Ali Khan,Teemu Pihkakoski,Peng Liang,Muhammad Azeem Akbar,Matti Silveri,Lauri Malmi*

Main category: cs.SE

TL;DR: C2|Q>是一个硬件无关的量子软件开发框架，通过将经典代码转换为量子可执行程序，显著降低了量子编程的复杂性。该框架采用模块化设计，包含编码器、部署模块和解码器三个核心组件。


<details>
  <summary>Details</summary>
Motivation: 当前量子开发环境要求开发者处理软件栈的低层细节，使得经典软件工程师难以使用。为了弥合这一差距，需要开发能够简化量子编程的框架。

Method: 框架采用模块化软件工程原则，分为三个核心模块：编码器（问题分类、生成量子兼容格式和构建量子电路）、部署模块（生成电路并基于保真度、运行时间和成本推荐硬件）、解码器（将量子输出解释为经典解决方案）。

Result: 编码器模块完成率达到93.8%，硬件推荐模块能够为最多56量子比特的工作负载选择合适的量子设备，完整工作流处理了434个Python代码片段和100个JSON输入，完成率分别为93.8%和100%。在NISQ硬件上的案例研究中，相比手动实现减少了近40倍的工作量。

Conclusion: C2|Q>框架成功降低了量子编程的复杂性，使经典软件工程师能够更轻松地开发量子应用，为量子软件工程的发展提供了重要工具。

Abstract: Quantum Software Engineering (QSE) is emerging as a critical discipline to
make quantum computing accessible to a broader developer community; however,
most quantum development environments still require developers to engage with
low-level details across the software stack - including problem encoding,
circuit construction, algorithm configuration, hardware selection, and result
interpretation - making them difficult for classical software engineers to use.
To bridge this gap, we present C2|Q>: a hardware-agnostic quantum software
development framework that translates classical specifications (code) into
quantum-executable programs while preserving methodological rigor. The
framework applies modular software engineering principles by classifying the
workflow into three core modules: an encoder that classifies problems, produces
Quantum-Compatible Formats (QCFs), and constructs quantum circuits, a
deployment module that generates circuits and recommends hardware based on
fidelity, runtime, and cost, and a decoder that interprets quantum outputs into
classical solutions. In evaluation, the encoder module achieved a 93.8%
completion rate, the hardware recommendation module consistently selected the
appropriate quantum devices for workloads scaling up to 56 qubits, and the full
C2|Q>: workflow successfully processed classical specifications (434 Python
snippets and 100 JSON inputs) with completion rates of 93.8% and 100%,
respectively. For case study problems executed on publicly available NISQ
hardware, C2|Q>: reduced the required implementation effort by nearly 40X
compared to manual implementations using low-level quantum software development
kits (SDKs), with empirical runs limited to small- and medium-sized instances
consistent with current NISQ capabilities. The open-source implementation of
C2|Q>: is available at https://github.com/C2-Q/C2Q

</details>


### [13] [GramTrans: A Better Code Representation Approach in Code Generation](https://arxiv.org/abs/2510.02887)
*Zhao Zhang,Qingyuan Liang,Zeyu Sun,Yizhou Chen,Guoqing Wang,Yican Sun,Lu Zhang,Ge Li,Yingfei Xiong*

Main category: cs.SE

TL;DR: 本文提出一个猜想：代码表示越容易解析，模型性能越好。通过GramTrans方法将上下文无关语言转换为LL(1)类表示，实验证明解析难度与模型性能强相关。


<details>
  <summary>Details</summary>
Motivation: 探索代码表示选择如何影响模型性能，现有研究缺乏对解析难度与模型有效性关系的系统性理解。

Method: 提出GramTrans方法，使用分层冲突消除算法将上下文无关语言转换为LL(1)类表示，在解析简单性和标记效率之间实现灵活权衡。

Result: 在Python和Java上使用三个代码生成模型进行实验，GramTrans在多个基准测试中相比基线表示均取得显著改进。

Conclusion: 解析难度与模型性能存在强相关性，GramTrans通过简化解析复杂度有效提升了代码生成模型的性能。

Abstract: Code generation has shown great promise in assisting software development. A
fundamental yet underexplored question is how the choice of code representation
affects model performance. While existing studies employ various
representations, such as treating code as plain text, grammar rule sequences,
or syntax tree sequences, they lack a principled understanding of the
relationship between parsing difficulty and model effectiveness. This paper
proposes a conjecture: the easier a representation is to parse, the better
performance the model achieves. We formalize this idea using grammar classes,
where representations in simpler classes (e.g., LL(1)) are easier to parse.
Through a controlled experiment on a Python-based DSL, we show that parsing
difficulty strongly correlates with model performance. Motivated by this
finding, we present GramTrans, a general approach that automatically transforms
a context-free language into a representation within the LL(1) class. GramTrans
introduces a novel hierarchical conflict elimination algorithm, enabling a
flexible trade-off between syntactic simplicity and token efficiency. We
evaluate GramTrans on both Python and Java using three code generation models:
StarCoder 1B, DeepSeek-Coder 1.3B, and Qwen2.5 1.5B. Across multiple
benchmarks, GramTrans consistently delivers significant improvements over
baseline representations. Furthermore, our analysis of existing representations
reconfirms the strong alignment between parsing difficulty and model
performance, providing additional support for the conjecture.

</details>


### [14] [Mechanistic Interpretability of Code Correctness in LLMs via Sparse Autoencoders](https://arxiv.org/abs/2510.02917)
*Kriz Tahimic,Charibeth Cheng*

Main category: cs.SE

TL;DR: 通过稀疏自编码器分析LLM内部表示，识别代码正确性相关方向，发现代码正确性方向能可靠预测错误代码，而修正能力涉及权衡。成功代码生成依赖于关注测试用例而非问题描述，且基础模型中的代码正确性机制在指令微调后仍然有效。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在软件开发中广泛应用，理解其内部正确性机制对于安全部署至关重要，特别是当大量AI建议代码进入生产环境时。

Method: 应用稀疏自编码器分解LLM表示，使用t统计量选择预测方向，通过分离分数从基础模型表示中识别引导方向，并通过引导、注意力分析和权重正交化分析其机制特性。

Result: 代码正确性方向能可靠预测错误代码，修正能力虽统计显著但涉及权衡；成功代码生成依赖于关注测试用例；基础模型中的代码正确性方向在指令微调后仍保持有效性。

Conclusion: 提出了三个实际应用：提示策略应优先测试用例而非详细问题描述，预测方向可作为错误警报供开发者审查，这些预测器可指导选择性引导，仅在预期错误时干预以避免代码损坏。

Abstract: As Large Language Models become integral to software development, with
substantial portions of AI-suggested code entering production, understanding
their internal correctness mechanisms becomes critical for safe deployment. We
apply sparse autoencoders to decompose LLM representations, identifying
directions that correspond to code correctness. We select predictor directions
using t-statistics and steering directions through separation scores from base
model representations, then analyze their mechanistic properties through
steering, attention analysis, and weight orthogonalization. We find that code
correctness directions in LLMs reliably predict incorrect code, while
correction capabilities, though statistically significant, involve tradeoffs
between fixing errors and preserving correct code. Mechanistically, successful
code generation depends on attending to test cases rather than problem
descriptions. Moreover, directions identified in base models retain their
effectiveness after instruction-tuning, suggesting code correctness mechanisms
learned during pre-training are repurposed during fine-tuning. Our mechanistic
insights suggest three practical applications: prompting strategies should
prioritize test examples over elaborate problem descriptions, predictor
directions can serve as error alarms for developer review, and these same
predictors can guide selective steering, intervening only when errors are
anticipated to prevent the code corruption from constant steering.

</details>


### [15] [Model-Agnostic Correctness Assessment for LLM-Generated Code via Dynamic Internal Representation Selection](https://arxiv.org/abs/2510.02934)
*Thanh Trong Vu,Tuan-Dung Bui,Thu-Trang Nguyen,Son Nguyen,Hieu Dinh Vo*

Main category: cs.SE

TL;DR: AUTOPROBE是一种模型无关的方法，通过动态选择LLM内部最有信息量的表示来评估代码正确性，在编译性、功能性和安全性评估方面均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖预选/固定层和token位置的表示，这限制了在不同模型架构和任务间的泛化能力。需要一种能动态选择最重要内部表示的方法来提升代码正确性评估的鲁棒性。

Method: AUTOPROBE使用基于注意力的机制学习隐藏状态的重要性分数，聚焦最相关特征，然后将加权表示聚合并传递给探测分类器，预测代码在多个维度的正确性。

Result: 在多个基准测试和代码LLM上的实验表明，AUTOPROBE始终优于基线方法。安全性评估超越最先进白盒方法18%；编译性和功能性评估对代码复杂度的鲁棒性最高，性能分别比其他方法高出19%和111%。

Conclusion: 动态选择重要内部信号使AUTOPROBE能够作为评估各种LLM生成代码正确性的鲁棒且可泛化的解决方案。

Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities in
code generation and are increasingly integrated into the software development
process. However, ensuring the correctness of LLM-generated code remains a
critical concern. Prior work has shown that the internal representations of
LLMs encode meaningful signals for assessing code correctness. Nevertheless,
the existing methods rely on representations from pre-selected/fixed layers and
token positions, which could limit its generalizability across diverse model
architectures and tasks. In this work, we introduce AUTOPROBE, a novel
model-agnostic approach that dynamically selects the most informative internal
representations for code correctness assessment. AUTOPROBE employs an
attention-based mechanism to learn importance scores for hidden states,
enabling it to focus on the most relevant features. These weighted
representations are then aggregated and passed to a probing classifier to
predict code correctness across multiple dimensions, including compilability,
functionality, and security. To evaluate the performance of AUTOPROBE, we
conduct extensive experiments across multiple benchmarks and code LLMs. Our
experimental results show that AUTOPROBE consistently outperforms the
baselines. For security assessment, AUTOPROBE surpasses the state-of-the-art
white-box approach by 18%. For compilability and functionality assessment,
AUTOPROBE demonstrates its highest robustness to code complexity, with the
performance higher than the other approaches by up to 19% and 111%,
respectively. These findings highlight that dynamically selecting important
internal signals enables AUTOPROBE to serve as a robust and generalizable
solution for assessing the correctness of code generated by various LLMs.

</details>


### [16] [Tracing and Metrics Design Patterns for Monitoring Cloud-native Applications](https://arxiv.org/abs/2510.02991)
*Carlos Albuquerque,Filipe F. Correia*

Main category: cs.SE

TL;DR: 本文介绍了三种监控云原生应用的设计模式：分布式追踪、应用指标和基础设施指标，旨在解决分布式系统监控的挑战。


<details>
  <summary>Details</summary>
Motivation: 随着软件架构日益分布式和易变，诊断系统问题变得更加困难，需要应对碎片化的可观测性和更复杂的根因分析。

Method: 基于先前工作，提出了三种设计模式：分布式追踪用于跨服务请求流的可见性，应用指标用于结构化应用性能监控，基础设施指标用于环境监控。

Result: 这些模式源自行业实践和可观测性框架，为软件从业者提供了实用的监控指导。

Conclusion: 三种设计模式共同构成了云原生应用可观测性的完整解决方案，有助于提高系统可靠性和可维护性。

Abstract: Observability helps ensure the reliability and maintainability of
cloud-native applications. As software architectures become increasingly
distributed and subject to change, it becomes a greater challenge to diagnose
system issues effectively, often having to deal with fragmented observability
and more difficult root cause analysis. This paper builds upon our previous
work and introduces three design patterns that address key challenges in
monitoring cloud-native applications. Distributed Tracing improves visibility
into request flows across services, aiding in latency analysis and root cause
detection, Application Metrics provides a structured approach to instrumenting
applications with meaningful performance indicators, enabling real-time
monitoring and anomaly detection, and Infrastructure Metrics focuses on
monitoring the environment in which the system is operated, helping teams
assess resource utilization, scalability, and operational health. These
patterns are derived from industry practices and observability frameworks and
aim to offer guidance for software practitioners.

</details>


### [17] [Patterns for Teaching Agile with Student Projects -- Team and Project Setup](https://arxiv.org/abs/2510.03005)
*Daniel Pinho,Petr Pícha,Filipe Correia,Přemek Brada*

Main category: cs.SE

TL;DR: 提出用于高等教育中敏捷软件开发课程教学的模式语言，重点关注团队和项目设置阶段的五个具体模式。


<details>
  <summary>Details</summary>
Motivation: 现有关于敏捷软件开发教学的文献缺乏可操作的建议，要么过于关注框架，要么偏离到敏捷教学方式本身，而非教授敏捷实践。

Method: 基于作者在高等教育环境中的教学经验，开发一个模式语言，目前提出了五个专注于团队和项目设置阶段的模式。

Result: 提出了五个具体模式：限制团队规模、缩小项目范围、非关键业务项目、自组织团队、团队自选主题，作为整体模式语言开发的起点。

Conclusion: 这些模式为教授敏捷软件开发实践提供了实用的指导，特别是在团队组建和项目设置阶段，有助于改进高等教育中的敏捷教学。

Abstract: Higher education courses teaching about agile software development (ASD) have
increased in commonality as the ideas behind the Agile Manifesto became more
commonplace in the industry. However, a lot of the literature on how ASD is
applied in the classroom does not provide much actionable advice, focusing on
frameworks or even moving beyond the software development area into teaching in
an agile way. We, therefore, showcase early work on a pattern language that
focuses on teaching ASD practices to university students, which stems from our
own experiences as educators in higher education contexts. We present five
patterns, specifically focused on team and project setup phase: Capping Team
Size, Smaller Project Scope, Business Non-Critical Project, Self-assembling
Teams, and Team Chooses Topic as a starting point for developing the overall
pattern language.

</details>


### [18] [Investigating The Smells of LLM Generated Code](https://arxiv.org/abs/2510.03029)
*Debalina Ghosh Paul,Hong Zhu,Ian Bayley*

Main category: cs.SE

TL;DR: 该研究提出基于场景的方法评估LLM生成代码的质量，发现LLM生成代码比专业编写的参考代码有更高的代码异味发生率，平均增加63.34%，其中实现异味增加73.35%，设计异味增加21.42%。


<details>
  <summary>Details</summary>
Motivation: 目前关于LLM生成代码的研究主要集中在功能正确性上，而代码质量评估的研究相对较少，需要识别LLM生成代码质量最差的场景以便改进。

Method: 使用基于场景的方法，通过测量代码异味并与专业编写的参考解决方案基线进行比较，将测试数据集按代码主题和编码任务复杂度划分为不同子集，构建自动化测试系统对四个先进LLM（Gemini Pro、ChatGPT、Codex、Falcon）生成的Java程序进行实验。

Result: LLM生成代码的代码异味发生率显著高于参考解决方案，Falcon表现最好（异味增加42.28%），其次是Gemini Pro（62.07%）、ChatGPT（65.05%）和Codex（84.97%）。更复杂的编码任务和涉及面向对象概念等高级主题的代码异味增加更明显。

Conclusion: 在代码异味方面，LLM在不同编码任务复杂度和主题上的表现与相应场景下人工编写代码的质量高度相关，但LLM生成代码的质量明显低于人工编写代码。

Abstract: Context: Large Language Models (LLMs) are increasingly being used to generate
program code. Much research has been reported on the functional correctness of
generated code, but there is far less on code quality.
  Objectives: In this study, we propose a scenario-based method of evaluating
the quality of LLM-generated code to identify the weakest scenarios in which
the quality of LLM generated code should be improved.
  Methods: The method measures code smells, an important indicator of code
quality, and compares them with a baseline formed from reference solutions of
professionally written code. The test dataset is divided into various subsets
according to the topics of the code and complexity of the coding tasks to
represent different scenarios of using LLMs for code generation. We will also
present an automated test system for this purpose and report experiments with
the Java programs generated in response to prompts given to four
state-of-the-art LLMs: Gemini Pro, ChatGPT, Codex, and Falcon.
  Results: We find that LLM-generated code has a higher incidence of code
smells compared to reference solutions. Falcon performed the least badly, with
a smell increase of 42.28%, followed by Gemini Pro (62.07%), ChatGPT (65.05%)
and finally Codex (84.97%). The average smell increase across all LLMs was
63.34%, comprising 73.35% for implementation smells and 21.42% for design
smells. We also found that the increase in code smells is greater for more
complex coding tasks and for more advanced topics, such as those involving
object-orientated concepts.
  Conclusion: In terms of code smells, LLM's performances on various coding
task complexities and topics are highly correlated to the quality of human
written code in the corresponding scenarios. However, the quality of LLM
generated code is noticeably poorer than human written code.

</details>


### [19] [Refactoring Towards Microservices: Preparing the Ground for Service Extraction](https://arxiv.org/abs/2510.03050)
*Rita Peixoto,Filipe F. Correia,Thatiane Rosa,Eduardo Guerra,Alfredo Goldman*

Main category: cs.SE

TL;DR: 提出了一个包含七种重构模式的目录，专门用于支持向微服务架构的迁移，重点关注依赖处理问题。


<details>
  <summary>Details</summary>
Motivation: 现有文献主要关注架构层面的指导，忽视了代码层面的挑战和依赖关系，导致微服务迁移过程仍然需要大量手动工作。

Method: 通过整理文献中的重构方法，开发了一个系统化的重构目录，提供结构化的逐步指导。

Result: 创建了一个包含七种重构模式的目录，能够简化迁移过程并为自动化奠定基础。

Conclusion: 该工作为开发者提供了高效的代码级重构指导，有助于简化微服务迁移过程并支持潜在的自动化实现。

Abstract: As organizations increasingly transition from monolithic systems to
microservices, they aim to achieve higher availability, automatic scaling,
simplified infrastructure management, enhanced collaboration, and streamlined
deployments. However, this migration process remains largely manual and
labour-intensive. While existing literature offers various strategies for
decomposing monoliths, these approaches primarily focus on architecture-level
guidance, often overlooking the code-level challenges and dependencies that
developers must address during the migration. This article introduces a
catalogue of seven refactorings specifically designed to support the transition
to a microservices architecture with a focus on handling dependencies. The
catalogue provides developers with a systematic guide that consolidates
refactorings identified in the literature and addresses the critical gap in
systematizing the process at the code level. By offering a structured,
step-by-step approach, this work simplifies the migration process and lays the
groundwork for its potential automation, empowering developers to implement
these changes efficiently and effectively.

</details>


### [20] [State Field Coverage: A Metric for Oracle Quality](https://arxiv.org/abs/2510.03071)
*Facundo Molina,Nazareno Aguirre,Alessandra Gorla*

Main category: cs.SE

TL;DR: 提出了一种名为状态字段覆盖率的新指标，用于评估测试预言的质量，该指标衡量预言在测试执行期间可能访问的对象状态字段比例。


<details>
  <summary>Details</summary>
Motivation: 现有指标要么无法为预言改进提供全面基础，要么仅适用于特定类型的预言，限制了通用性。评估预言质量对于提高测试过程的整体有效性至关重要。

Method: 实现了一种静态计算状态字段覆盖率指标的机制，通过静态分析确定预言可能访问的对象状态字段比例。

Result: 实验涉及273个表示不变式和249,027个测试断言，结果表明状态字段覆盖率与预言检测故障的能力（通过变异得分衡量）强相关。

Conclusion: 状态字段覆盖率是评估预言质量的合适指标，能够有效指导测试预言的改进。

Abstract: The effectiveness of testing in uncovering software defects depends not only
on the characteristics of the test inputs and how thoroughly they exercise the
software, but also on the quality of the oracles used to determine whether the
software behaves as expected. Therefore, assessing the quality of oracles is
crucial to improve the overall effectiveness of the testing process. Existing
metrics have been used for this purpose, but they either fail to provide a
comprehensive basis for guiding oracle improvement, or they are tailored to
specific types of oracles, thus limiting their generality.
  In this paper, we introduce state field coverage, a novel metric for
assessing oracle quality. This metric measures the proportion of an object's
state, as statically defined by its class fields, that an oracle may access
during test execution. The main intuition of our metric is that oracles with a
higher state field coverage are more likely to detect faults in the software
under analysis, as they inspect a larger portion of the object states to
determine whether tests pass or not.
  We implement a mechanism to statically compute the state field coverage
metric. Being statically computed, the metric is efficient and provides direct
guidance for improving test oracles by identifying state fields that remain
unexamined. We evaluate state field coverage through experiments involving 273
representation invariants and 249,027 test assertions. The results show that
state field coverage is a well-suited metric for assessing oracle quality, as
it strongly correlates with the oracles' fault-detection ability, measured by
mutation score.

</details>


### [21] [When Names Disappear: Revealing What LLMs Actually Understand About Code](https://arxiv.org/abs/2510.03178)
*Cuong Chi Le,Minh V. T. Pham,Cuong Duc Van,Hoang N. Phan,Huy N. Phan,Tien N. Nguyen*

Main category: cs.SE

TL;DR: 论文发现LLMs在代码任务中过度依赖命名模式而非语义理解，通过引入语义保留的混淆技术揭示了命名泄漏问题，并提出了ClassEval-Obf基准来更可靠地评估代码理解能力。


<details>
  <summary>Details</summary>
Motivation: 探究LLMs如何理解程序含义，区分代码的结构语义和人类可解释的命名两个通信通道，发现当前基准测试可能奖励命名模式的记忆而非真正的语义推理。

Method: 引入一套语义保留的混淆技术，通过移除命名通道来测试模型表现，并开发ClassEval-Obf基准系统性地抑制命名线索同时保持行为不变。

Result: 移除命名通道严重降低了意图级任务（如摘要）的性能，模型退化为逐行描述；在执行任务中也观察到一致下降，表明当前基准奖励命名模式记忆；混淆技术暴露了标识符泄漏问题。

Conclusion: ClassEval-Obf基准减少了性能膨胀差距，削弱了记忆捷径，为评估LLMs的代码理解和泛化能力提供了更可靠的基础。

Abstract: Large Language Models (LLMs) achieve strong results on code tasks, but how
they derive program meaning remains unclear. We argue that code communicates
through two channels: structural semantics, which define formal behavior, and
human-interpretable naming, which conveys intent. Removing the naming channel
severely degrades intent-level tasks such as summarization, where models
regress to line-by-line descriptions. Surprisingly, we also observe consistent
reductions on execution tasks that should depend only on structure, revealing
that current benchmarks reward memorization of naming patterns rather than
genuine semantic reasoning. To disentangle these effects, we introduce a suite
of semantics-preserving obfuscations and show that they expose identifier
leakage across both summarization and execution. Building on these insights, we
release ClassEval-Obf, an obfuscation-enhanced benchmark that systematically
suppresses naming cues while preserving behavior. Our results demonstrate that
ClassEval-Obf reduces inflated performance gaps, weakens memorization
shortcuts, and provides a more reliable basis for assessing LLMs' code
understanding and generalization.

</details>


### [22] [Abstain and Validate: A Dual-LLM Policy for Reducing Noise in Agentic Program Repair](https://arxiv.org/abs/2510.03217)
*José Cambronero,Michele Tufano,Sherry Shi,Renyao Wei,Grant Uy,Runxiang Cheng,Chin-Jung Liu,Shiying Pan,Satish Chandra,Pat Rondon*

Main category: cs.SE

TL;DR: 提出了两种LLM策略（bug abstention和patch validation）来减少自动化程序修复系统中的噪声，提高补丁质量


<details>
  <summary>Details</summary>
Motivation: 自动化程序修复系统在处理复杂仓库级bug时会产生大量无效补丁，给开发者带来噪声并浪费宝贵时间

Method: 使用bug abstention策略排除系统难以修复的bug，使用patch validation策略拒绝不太可能是好修复的补丁

Result: 在Google代码库的174个人工报告bug上，组合使用两种策略可将成功率提高39个百分点

Conclusion: 这种双策略方法为自动化程序修复系统的可靠工业级部署提供了实用路径

Abstract: Agentic Automated Program Repair (APR) is increasingly tackling complex,
repository-level bugs in industry, but ultimately agent-generated patches still
need to be reviewed by a human before committing them to ensure they address
the bug. Showing unlikely patches to developers can lead to substantial noise,
wasting valuable developer time and eroding trust in automated code changes. We
introduce two complementary LLM-based policies to reduce such noise: bug
abstention and patch validation policies. Bug abstention excludes bugs that the
agentic APR system is unlikely to fix. Patch validation rejects patches that
are unlikely to be a good fix for the given bug. We evaluate both policies on
three sets of bugs from Google's codebase, and their candidate patches
generated by an internal agentic APR system. On a set of 174 human-reported
bugs, removing bugs and patch trajectories rejected by our policies can raise
success rates by up to 13 percentage points and 15 percentage points,
respectively, and by up to 39 percentage points in combination. On null pointer
exceptions and sanitizer-reported bugs with machine-generated bug reports,
patch validation also improves average single-sample success rates. This
two-policy approach provides a practical path to the reliable, industrial-scale
deployment of agentic APR systems.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [23] [Hybrid Horizons: Policy for Post-Quantum Security](https://arxiv.org/abs/2510.02317)
*Anais Jaikissoon*

Main category: cs.CR

TL;DR: 本文探讨了人工智能时代下混合密码学的监管空白问题，分析了从经典密码学向量子密码学过渡期间缺乏监管框架的风险，并提出了解决方案。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能的快速发展和量子密码学的兴起，当前缺乏对混合密码学的监管框架，这可能导致技术被滥用，阻碍从经典密码学向量子密码学的安全过渡。

Method: 通过分析当前监管现状和技术发展趋势，识别混合密码学领域的监管空白，并制定相应的解决方案。

Result: 识别出混合密码学在监管基础设施方面存在重大空白，缺乏明确的法规和标准来支持从经典密码学到量子密码学的过渡。

Conclusion: 需要建立混合密码学的监管框架，以确保从经典密码学向量子密码学的过渡能够安全有效地完成，防止技术滥用并促进技术创新。

Abstract: The Age of Artificial Intelligence is here. In 2025, there are few
regulations governing artificial intelligence. While the expansion of
artificial intelligence is going in a relatively good direction, there is a
risk that it can be misused. Misuse of technology is nothing new and will
continue to happen. The lack of regulation in artificial intelligence is
necessary because it raises the question of how we can move forward without
knowing what the limits are. While artificial intelligence dominates the
technology industry, new technology is starting to emerge. Quantum cryptography
is expected to replace classical cryptography; however, the transition from
classical to quantum cryptography is expected to occur within the next 10
years. The ability to transition from classical to quantum cryptography
requires hybrid cryptography. Hybrid cryptography can be used now; however,
similar to artificial intelligence, there is no regulation or support for the
regulatory infrastructure regarding hybrid machines. This paper will explore
the regulatory gaps in hybrid cryptography. The paper will also offer solutions
to fix the gaps and ensure the transition from classical to quantum
cryptography is safely and effectively completed.

</details>


### [24] [Modeling the Attack: Detecting AI-Generated Text by Quantifying Adversarial Perturbations](https://arxiv.org/abs/2510.02319)
*Lekkala Sai Teja,Annepaka Yadagiri,Sangam Sai Anish,Siva Gopala Krishna Nuthakki,Partha Pakray*

Main category: cs.CR

TL;DR: 本文提出了PIFE框架，通过将文本转换为标准化形式并量化变换幅度来增强AI生成文本检测的对抗鲁棒性，相比传统对抗训练方法显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的快速发展带来了双重用途问题，需要可靠的AI生成文本检测系统。现有检测器容易受到对抗攻击，特别是重述攻击，这规避了统计检测。

Method: 提出了扰动不变特征工程(PIFE)框架：首先通过多阶段标准化流程将输入文本转换为标准形式，然后使用Levenshtein距离和语义相似度等指标量化变换幅度，将这些信号直接输入分类器。

Result: 传统对抗训练在严格1%误报率下的真阳性率仅为48.8%，而PIFE模型在相同条件下保持82.6%的真阳性率，有效抵御了最复杂的语义攻击。

Conclusion: 明确建模扰动伪影，而不仅仅是在其上训练，是在对抗性军备竞赛中实现真正鲁棒性的更有前景的路径。

Abstract: The growth of highly advanced Large Language Models (LLMs) constitutes a huge
dual-use problem, making it necessary to create dependable AI-generated text
detection systems. Modern detectors are notoriously vulnerable to adversarial
attacks, with paraphrasing standing out as an effective evasion technique that
foils statistical detection. This paper presents a comparative study of
adversarial robustness, first by quantifying the limitations of standard
adversarial training and then by introducing a novel, significantly more
resilient detection framework: Perturbation-Invariant Feature Engineering
(PIFE), a framework that enhances detection by first transforming input text
into a standardized form using a multi-stage normalization pipeline, it then
quantifies the transformation's magnitude using metrics like Levenshtein
distance and semantic similarity, feeding these signals directly to the
classifier. We evaluate both a conventionally hardened Transformer and our
PIFE-augmented model against a hierarchical taxonomy of character-, word-, and
sentence-level attacks. Our findings first confirm that conventional
adversarial training, while resilient to syntactic noise, fails against
semantic attacks, an effect we term "semantic evasion threshold", where its
True Positive Rate at a strict 1% False Positive Rate plummets to 48.8%. In
stark contrast, our PIFE model, which explicitly engineers features from the
discrepancy between a text and its canonical form, overcomes this limitation.
It maintains a remarkable 82.6% TPR under the same conditions, effectively
neutralizing the most sophisticated semantic attacks. This superior performance
demonstrates that explicitly modeling perturbation artifacts, rather than
merely training on them, is a more promising path toward achieving genuine
robustness in the adversarial arms race.

</details>


### [25] [Agentic-AI Healthcare: Multilingual, Privacy-First Framework with MCP Agents](https://arxiv.org/abs/2510.02325)
*Mohammed A. Shehab*

Main category: cs.CR

TL;DR: Agentic-AI Healthcare是一个隐私感知、多语言、可解释的医疗AI研究原型，利用MCP协议协调多个智能代理进行患者交互，集成隐私合规层并符合HIPAA等医疗数据保护标准。


<details>
  <summary>Details</summary>
Motivation: 开发一个结合智能代理编排、多语言可访问性和合规性架构的医疗应用，解决医疗AI系统中的隐私保护、多语言支持和可解释性问题。

Method: 使用Model Context Protocol (MCP)协调多个智能代理，集成隐私合规层（RBAC、AES-GCM字段级加密、防篡改审计日志），支持英语、法语、阿拉伯语等多语言交互。

Result: 成功构建了一个研究原型，展示了多语言医患交互和透明诊断推理功能，符合HIPAA、PIPEDA、PHIPA等医疗数据保护标准。

Conclusion: 该工作证明了在医疗应用中结合智能代理编排、多语言可访问性和合规性架构的可行性，但强调这仅是一个研究原型而非认证医疗设备。

Abstract: This paper introduces Agentic-AI Healthcare, a privacy-aware, multilingual,
and explainable research prototype developed as a single-investigator project.
The system leverages the emerging Model Context Protocol (MCP) to orchestrate
multiple intelligent agents for patient interaction, including symptom
checking, medication suggestions, and appointment scheduling. The platform
integrates a dedicated Privacy and Compliance Layer that applies role-based
access control (RBAC), AES-GCM field-level encryption, and tamper-evident audit
logging, aligning with major healthcare data protection standards such as HIPAA
(US), PIPEDA (Canada), and PHIPA (Ontario). Example use cases demonstrate
multilingual patient-doctor interaction (English, French, Arabic) and
transparent diagnostic reasoning powered by large language models. As an
applied AI contribution, this work highlights the feasibility of combining
agentic orchestration, multilingual accessibility, and compliance-aware
architecture in healthcare applications. This platform is presented as a
research prototype and is not a certified medical device.

</details>


### [26] [CATMark: A Context-Aware Thresholding Framework for Robust Cross-Task Watermarking in Large Language Models](https://arxiv.org/abs/2510.02342)
*Yu Zhang,Shuliang Liu,Xu Yang,Xuming Hu*

Main category: cs.CR

TL;DR: 提出了一种上下文感知阈值水印框架，通过实时语义上下文动态调整水印强度，无需预定义阈值或任务特定调优，在跨任务场景中提升文本质量而不牺牲检测精度。


<details>
  <summary>Details</summary>
Motivation: 现有LLM水印方法在低熵场景下会导致文本质量下降，且依赖熵阈值的方法需要大量计算资源调优，对未知或跨任务场景适应性差。

Method: 通过logits聚类将文本生成划分为语义状态，建立上下文感知的熵阈值，在结构化内容中保持保真度的同时嵌入鲁棒水印。

Result: 实验表明该方法在跨任务中提高了文本质量，且没有牺牲检测准确性。

Conclusion: 上下文感知阈值水印框架能够有效解决传统水印方法在文本质量和适应性方面的局限性。

Abstract: Watermarking algorithms for Large Language Models (LLMs) effectively identify
machine-generated content by embedding and detecting hidden statistical
features in text. However, such embedding leads to a decline in text quality,
especially in low-entropy scenarios where performance needs improvement.
Existing methods that rely on entropy thresholds often require significant
computational resources for tuning and demonstrate poor adaptability to unknown
or cross-task generation scenarios. We propose \textbf{C}ontext-\textbf{A}ware
\textbf{T}hreshold watermarking ($\myalgo$), a novel framework that dynamically
adjusts watermarking intensity based on real-time semantic context. $\myalgo$
partitions text generation into semantic states using logits clustering,
establishing context-aware entropy thresholds that preserve fidelity in
structured content while embedding robust watermarks. Crucially, it requires no
pre-defined thresholds or task-specific tuning. Experiments show $\myalgo$
improves text quality in cross-tasks without sacrificing detection accuracy.

</details>


### [27] [An Investigation into the Performance of Non-Contrastive Self-Supervised Learning Methods for Network Intrusion Detection](https://arxiv.org/abs/2510.02349)
*Hamed Fard,Tobias Schalau,Gerhard Wunder*

Main category: cs.CR

TL;DR: 本文比较了五种非对比自监督学习方法在网络入侵检测中的性能，通过90个系统实验发现这些方法在攻击检测方面具有竞争力。


<details>
  <summary>Details</summary>
Motivation: 传统监督学习在网络入侵检测中只能检测已知异常，而自监督学习在计算机视觉中的成功激发了将其应用于网络入侵检测的兴趣。之前研究主要关注对比方法，非对比方法的有效性尚不明确。

Method: 使用三种编码器架构和六种增强策略，系统比较了五种非对比自监督学习方法。在UNSW-NB15和5G-NIDD两个数据集上进行了90个实验。

Result: 报告了每个自监督模型在平均精度、召回率、F1分数和AUCROC方面的最佳组合。与DeepSVDD和Autoencoder等无监督基线相比，非对比方法表现出竞争力。

Conclusion: 非对比自监督学习方法在网络入侵检测中具有竞争力，为检测未知攻击提供了有前景的替代方案。

Abstract: Network intrusion detection, a well-explored cybersecurity field, has
predominantly relied on supervised learning algorithms in the past two decades.
However, their limitations in detecting only known anomalies prompt the
exploration of alternative approaches. Motivated by the success of
self-supervised learning in computer vision, there is a rising interest in
adapting this paradigm for network intrusion detection. While prior research
mainly delved into contrastive self-supervised methods, the efficacy of
non-contrastive methods, in conjunction with encoder architectures serving as
the representation learning backbone and augmentation strategies that determine
what is learned, remains unclear for effective attack detection. This paper
compares the performance of five non-contrastive self-supervised learning
methods using three encoder architectures and six augmentation strategies.
Ninety experiments are systematically conducted on two network intrusion
detection datasets, UNSW-NB15 and 5G-NIDD. For each self-supervised model, the
combination of encoder architecture and augmentation method yielding the
highest average precision, recall, F1-score, and AUCROC is reported.
Furthermore, by comparing the best-performing models to two unsupervised
baselines, DeepSVDD, and an Autoencoder, we showcase the competitiveness of the
non-contrastive methods for attack detection. Code at:
https://github.com/renje4z335jh4/non_contrastive_SSL_NIDS

</details>


### [28] [Measuring Physical-World Privacy Awareness of Large Language Models: An Evaluation Benchmark](https://arxiv.org/abs/2510.02356)
*Xinjie Shen,Mufei Li,Pan Li*

Main category: cs.CR

TL;DR: EAPrivacy是一个评估LLM智能体在物理世界中隐私意识的基准测试，发现当前模型在物理环境变化、隐私约束与任务平衡、社会规范冲突等方面存在严重缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法仅限于自然语言场景，需要测量LLM智能体在物理世界中的隐私意识。

Method: 使用程序生成的四个层级场景测试智能体处理敏感对象、适应环境变化、平衡任务与隐私约束、解决社会规范冲突的能力。

Result: 表现最佳的Gemini 2.5 Pro在物理环境变化场景中仅达到59%准确率；86%情况下模型优先完成任务而非隐私约束；GPT-4o和Claude-3.5-haiku在隐私与社会规范冲突时超过15%忽视社会规范。

Conclusion: 当前LLM在物理基础隐私方面存在根本性错位，需要更强大的物理感知对齐。

Abstract: The deployment of Large Language Models (LLMs) in embodied agents creates an
urgent need to measure their privacy awareness in the physical world. Existing
evaluation methods, however, are confined to natural language based scenarios.
To bridge this gap, we introduce EAPrivacy, a comprehensive evaluation
benchmark designed to quantify the physical-world privacy awareness of
LLM-powered agents. EAPrivacy utilizes procedurally generated scenarios across
four tiers to test an agent's ability to handle sensitive objects, adapt to
changing environments, balance task execution with privacy constraints, and
resolve conflicts with social norms. Our measurements reveal a critical deficit
in current models. The top-performing model, Gemini 2.5 Pro, achieved only 59\%
accuracy in scenarios involving changing physical environments. Furthermore,
when a task was accompanied by a privacy request, models prioritized completion
over the constraint in up to 86\% of cases. In high-stakes situations pitting
privacy against critical social norms, leading models like GPT-4o and
Claude-3.5-haiku disregarded the social norm over 15\% of the time. These
findings, demonstrated by our benchmark, underscore a fundamental misalignment
in LLMs regarding physically grounded privacy and establish the need for more
robust, physically-aware alignment.

</details>


### [29] [Privacy in the Age of AI: A Taxonomy of Data Risks](https://arxiv.org/abs/2510.02357)
*Grace Billiris,Asif Gill,Madhushi Bandara*

Main category: cs.CR

TL;DR: 本文提出了一个AI隐私风险分类法，通过系统综述45项研究识别出19个关键风险，分为数据集级、模型级、基础设施级和内部威胁四个类别，发现人为错误是最重要的风险因素。


<details>
  <summary>Details</summary>
Motivation: 传统隐私框架无法应对AI技术的独特特性（如自主学习和黑盒决策），需要新的方法来理解和管理AI隐私风险。

Method: 通过系统综述45项研究，构建AI隐私风险分类法，将风险分为四个主要类别进行综合分析。

Result: 识别出19个关键风险，在四个类别中分布均衡，人为错误占9.45%是最显著因素，挑战了传统安全方法偏重技术控制的倾向。

Conclusion: 该分类法通过连接AI隐私的技术和行为维度，为可信AI发展提供基础，并揭示了需要更全面理解AI隐私风险的研究空白。

Abstract: Artificial Intelligence (AI) systems introduce unprecedented privacy
challenges as they process increasingly sensitive data. Traditional privacy
frameworks prove inadequate for AI technologies due to unique characteristics
such as autonomous learning and black-box decision-making. This paper presents
a taxonomy classifying AI privacy risks, synthesised from 45 studies identified
through systematic review. We identify 19 key risks grouped under four
categories: Dataset-Level, Model-Level, Infrastructure-Level, and Insider
Threat Risks. Findings reveal a balanced distribution across these dimensions,
with human error (9.45%) emerging as the most significant factor. This taxonomy
challenges conventional security approaches that typically prioritise technical
controls over human factors, highlighting gaps in holistic understanding. By
bridging technical and behavioural dimensions of AI privacy, this paper
contributes to advancing trustworthy AI development and provides a foundation
for future research.

</details>


### [30] [Bootstrapping as a Morphism: An Arithmetic Geometry Approach to Asymptotically Faster Homomorphic Encryption](https://arxiv.org/abs/2510.02365)
*Dongfang Zhao*

Main category: cs.CR

TL;DR: 提出了一种基于算术几何的全新同态加密自举方法，通过将自举操作重构为几何投影，完全绕过了传统电路评估模型，实现了复杂度为O(d·poly(log q))的自举算法，消除了L_dec因子。


<details>
  <summary>Details</summary>
Motivation: 全同态加密(FHE)的实际应用受到自举过程计算成本过高的严重限制，当前所有自举方法的复杂度都与解密电路的乘法深度L_dec相关，这成为主要性能瓶颈。

Method: 应用现代算术几何工具，将自举操作重构为几何投影。将密文空间建模为仿射概形，将可解密密文和新鲜密文的轨迹严格定义为不同的闭子概形。自举变换实现为这些空间之间的态射。计算上，该投影等价于在高度结构化的理想格上解决特定的最近向量问题(CVP)，使用称为代数折叠的技术高效实现。

Result: 开发了一个完整且可证明正确的自举算法，计算复杂度为O(d·poly(log q))，其中d是环维度，q是密文模数。该结果完全消除了复杂度中的L_dec因子，代表了相对于现有技术的根本性渐进改进。

Conclusion: 这种几何视角为实现真正实用和高性能的FHE提供了一条新的有前景的途径。

Abstract: Fully Homomorphic Encryption (FHE) provides a powerful paradigm for secure
computation, but its practical adoption is severely hindered by the prohibitive
computational cost of its bootstrapping procedure. The complexity of all
current bootstrapping methods is fundamentally tied to the multiplicative depth
of the decryption circuit, denoted $L_{dec}$, making it the primary performance
bottleneck. This paper introduces a new approach to bootstrapping that
completely bypasses the traditional circuit evaluation model. We apply the
tools of modern arithmetic geometry to reframe the bootstrapping operation as a
direct geometric projection. Our framework models the space of ciphertexts as
an affine scheme and rigorously defines the loci of decryptable and fresh
ciphertexts as distinct closed subschemes. The bootstrapping transformation is
then realized as a morphism between these two spaces. Computationally, this
projection is equivalent to solving a specific Closest Vector Problem (CVP)
instance on a highly structured ideal lattice, which we show can be done
efficiently using a technique we call algebraic folding. The primary result of
our work is a complete and provably correct bootstrapping algorithm with a
computational complexity of $O(d \cdot \text{poly}(\log q))$, where $d$ is the
ring dimension and $q$ is the ciphertext modulus. The significance of this
result lies in the complete elimination of the factor $L_{dec}$ from the
complexity, representing a fundamental asymptotic improvement over the state of
the art. This geometric perspective offers a new and promising pathway toward
achieving truly practical and high-performance FHE.

</details>


### [31] [Federated Spatiotemporal Graph Learning for Passive Attack Detection in Smart Grids](https://arxiv.org/abs/2510.02371)
*Bochra Al Agha,Razane Tajeddine*

Main category: cs.CR

TL;DR: 提出一种基于图神经网络和多模态融合的被动窃听检测方法，结合联邦学习在智能电网中实现高效检测


<details>
  <summary>Details</summary>
Motivation: 智能电网面临被动窃听威胁，攻击者通过监听通信链路获取电网拓扑和运行信息，为后续攻击创造条件。传统检测方法难以捕捉微弱、短暂的窃听信号

Method: 采用图卷积网络聚合星型子图的空间上下文，双向GRU建模短期时间依赖，两阶段编码器将异构特征转换为统一时空表示。在FedProx框架下进行联邦学习训练

Result: 在合成数据集上达到98.32%的时间步准确率（攻击F1=0.972）和93.35%的序列准确率，误报率仅0.15%

Conclusion: 结合时空上下文能够可靠检测隐蔽的侦察活动，同时保持低误报率，适用于非独立同分布的联邦智能电网部署

Abstract: Smart grids are exposed to passive eavesdropping, where attackers listen
silently to communication links. Although no data is actively altered, such
reconnaissance can reveal grid topology, consumption patterns, and operational
behavior, creating a gateway to more severe targeted attacks. Detecting this
threat is difficult because the signals it produces are faint, short-lived, and
often disappear when traffic is examined by a single node or along a single
timeline. This paper introduces a graph-centric, multimodal detector that fuses
physical-layer and behavioral indicators over ego-centric star subgraphs and
short temporal windows to detect passive attacks. To capture stealthy
perturbations, a two-stage encoder is introduced: graph convolution aggregates
spatial context across ego-centric star subgraphs, while a bidirectional GRU
models short-term temporal dependencies. The encoder transforms heterogeneous
features into a unified spatio-temporal representation suitable for
classification. Training occurs in a federated learning setup under FedProx,
improving robustness to heterogeneous local raw data and contributing to the
trustworthiness of decentralized training; raw measurements remain on client
devices. A synthetic, standards-informed dataset is generated to emulate
heterogeneous HAN/NAN/WAN communications with wireless-only passive
perturbations, event co-occurrence, and leak-safe splits. The model achieves a
testing accuracy of 98.32% per-timestep (F1_{attack}=0.972) and 93.35%
per-sequence at 0.15% FPR using a simple decision rule with run-length m=2 and
threshold $\tau=0.55$. The results demonstrate that combining spatial and
temporal context enables reliable detection of stealthy reconnaissance while
maintaining low false-positive rates, making the approach suitable for non-IID
federated smart-grid deployments.

</details>


### [32] [A-MemGuard: A Proactive Defense Framework for LLM-Based Agent Memory](https://arxiv.org/abs/2510.02373)
*Qianshan Wei,Tengchao Yang,Yaochen Wang,Xinfeng Li,Lijun Li,Zhenfei Yin,Yi Zhan,Thorsten Holz,Zhiqiang Lin,XiaoFeng Wang*

Main category: cs.CR

TL;DR: 提出了A-MemGuard框架，通过共识验证和双内存结构来防御LLM代理内存中的隐蔽攻击，将攻击成功率降低95%以上。


<details>
  <summary>Details</summary>
Motivation: LLM代理依赖内存学习，但恶意记录注入会操纵其未来行为，且这种攻击具有上下文触发性和自我强化的错误循环特性，难以检测。

Method: A-MemGuard包含共识验证（通过比较多个相关内存的推理路径检测异常）和双内存结构（将检测到的失败提炼为'教训'单独存储），无需修改代理核心架构。

Result: 在多个基准测试中，A-MemGuard将攻击成功率降低了95%以上，同时仅产生最小的效用成本。

Conclusion: 该工作将LLM内存安全从静态过滤转向主动、经验驱动的模型，使防御能力随时间增强。

Abstract: Large Language Model (LLM) agents use memory to learn from past interactions,
enabling autonomous planning and decision-making in complex environments.
However, this reliance on memory introduces a critical security risk: an
adversary can inject seemingly harmless records into an agent's memory to
manipulate its future behavior. This vulnerability is characterized by two core
aspects: First, the malicious effect of injected records is only activated
within a specific context, making them hard to detect when individual memory
entries are audited in isolation. Second, once triggered, the manipulation can
initiate a self-reinforcing error cycle: the corrupted outcome is stored as
precedent, which not only amplifies the initial error but also progressively
lowers the threshold for similar attacks in the future. To address these
challenges, we introduce A-MemGuard (Agent-Memory Guard), the first proactive
defense framework for LLM agent memory. The core idea of our work is the
insight that memory itself must become both self-checking and self-correcting.
Without modifying the agent's core architecture, A-MemGuard combines two
mechanisms: (1) consensus-based validation, which detects anomalies by
comparing reasoning paths derived from multiple related memories and (2) a
dual-memory structure, where detected failures are distilled into ``lessons''
stored separately and consulted before future actions, breaking error cycles
and enabling adaptation. Comprehensive evaluations on multiple benchmarks show
that A-MemGuard effectively cuts attack success rates by over 95% while
incurring a minimal utility cost. This work shifts LLM memory security from
static filtering to a proactive, experience-driven model where defenses
strengthen over time. Our code is available in
https://github.com/TangciuYueng/AMemGuard

</details>


### [33] [A Hybrid CAPTCHA Combining Generative AI with Keystroke Dynamics for Enhanced Bot Detection](https://arxiv.org/abs/2510.02374)
*Ayda Aghaei Nia*

Main category: cs.CR

TL;DR: 提出了一种结合LLM认知挑战和击键动态分析的混合CAPTCHA系统，通过双重验证机制有效区分人类用户和机器人攻击。


<details>
  <summary>Details</summary>
Motivation: 传统CAPTCHA在可用性和AI机器人防御之间存在权衡，需要开发更安全且用户友好的新一代验证系统。

Method: 采用双重验证架构：动态生成LLM认知挑战问题，同时分析用户击键动态行为特征，结合认知和行为测试。

Result: 实验表明该系统在机器人检测方面具有高准确率，能有效抵御粘贴式和脚本式模拟攻击，同时保持高用户可用性评分。

Conclusion: 结合认知和行为测试的双层CAPTCHA系统展示了创建更安全、用户友好验证码的潜力，为新一代web安全验证提供了可行方案。

Abstract: Completely Automated Public Turing tests to tell Computers and Humans Apart
(CAPTCHAs) are a foundational component of web security, yet traditional
implementations suffer from a trade-off between usability and resilience
against AI-powered bots. This paper introduces a novel hybrid CAPTCHA system
that synergizes the cognitive challenges posed by Large Language Models (LLMs)
with the behavioral biometric analysis of keystroke dynamics. Our approach
generates dynamic, unpredictable questions that are trivial for humans but
non-trivial for automated agents, while simultaneously analyzing the user's
typing rhythm to distinguish human patterns from robotic input. We present the
system's architecture, formalize the feature extraction methodology for
keystroke analysis, and report on an experimental evaluation. The results
indicate that our dual-layered approach achieves a high degree of accuracy in
bot detection, successfully thwarting both paste-based and script-based
simulation attacks, while maintaining a high usability score among human
participants. This work demonstrates the potential of combining cognitive and
behavioral tests to create a new generation of more secure and user-friendly
CAPTCHAs.

</details>


### [34] [Scaling Homomorphic Applications in Deployment](https://arxiv.org/abs/2510.02376)
*Ryan Marinelli,Angelica Chowdhury*

Main category: cs.CR

TL;DR: 开发了一个概念验证的同态加密应用来评估加密生态系统的生产就绪性，通过电影推荐应用实现容器化和编排部署，通过基础设施优化缓解全同态加密的计算限制


<details>
  <summary>Details</summary>
Motivation: 评估同态加密生态系统在生产环境中的实际应用就绪性，探索如何通过基础设施优化来克服全同态加密的计算性能限制

Method: 实现电影推荐应用作为概念验证，采用容器化和编排技术进行生产化部署，通过调整部署配置和基础设施优化来缓解FHE的计算限制

Result: 成功开发并部署了基于同态加密的电影推荐应用，证明了通过基础设施优化可以在一定程度上缓解FHE的计算性能问题

Conclusion: 同态加密生态系统正在向生产就绪方向发展，通过适当的容器化、编排和基础设施优化策略，可以有效地部署和运行基于FHE的应用

Abstract: In this endeavor, a proof-of-concept homomorphic application is developed to
determine the production readiness of encryption ecosystems. A movie
recommendation app is implemented for this purpose and productionized through
containerization and orchestration. By tuning deployment configurations, the
computational limitations of Fully Homomorphic Encryption (FHE) are mitigated
through additional infrastructure optimizations
  Index Terms: Reinforcement Learning, Orchestration, Homomorphic Encryption

</details>


### [35] [Apply Bayes Theorem to Optimize IVR Authentication Process](https://arxiv.org/abs/2510.02378)
*Jingrong Xie,Yumin Li*

Main category: cs.CR

TL;DR: 使用贝叶斯方法改进金融IVR认证系统，通过动态评估欺诈风险和调整凭证验证路径来应对欺诈者选择性绕过强凭证的问题。


<details>
  <summary>Details</summary>
Motivation: 传统IVR系统使用静态凭证序列进行认证，假设所有凭证效果相同。但欺诈者会利用这种可预测性，选择性绕过强凭证，导致安全漏洞。

Method: 应用贝叶斯定理和条件概率建模，动态评估欺诈风险并自适应调整凭证验证路径。

Result: 提出了一个能够根据实时风险评估动态调整认证流程的贝叶斯框架。

Conclusion: 贝叶斯方法能够有效提高IVR认证系统的安全性，通过动态风险评估应对欺诈攻击。

Abstract: This paper introduces a Bayesian approach to improve Interactive Voice
Response (IVR) authentication processes used by financial institutions.
Traditional IVR systems authenticate users through a static sequence of
credentials, assuming uniform effectiveness among them. However, fraudsters
exploit this predictability, selectively bypassing strong credentials. This
study applies Bayes' Theorem and conditional probability modeling to evaluate
fraud risk dynamically and adapt credential verification paths.

</details>


### [36] [Hybrid Schemes of NIST Post-Quantum Cryptography Standard Algorithms and Quantum Key Distribution for Key Exchange and Digital Signature](https://arxiv.org/abs/2510.02379)
*Abel C. H. Chen*

Main category: cs.CR

TL;DR: 该研究提出了一种结合量子密钥分发(QKD)和后量子密码(PQC)的混合安全方案，包括混合密钥交换协议和混合数字签名方案，旨在构建双重安全层。


<details>
  <summary>Details</summary>
Motivation: 由于后量子密码基于数学问题的难度，而量子密钥分发基于量子物理原理，两者各有优缺点可以互补，因此需要结合两者构建更安全的混合方案。

Method: 将模块格基密钥封装机制(ML-KEM)与BB84和E91 QKD协议结合构建混合密钥交换协议；使用模块格基数字签名算法(ML-DSA)和无状态哈希基数字签名算法(SLH-DSA)生成签名重建值，通过BB84和E91协议传输确认码进行验证。

Result: 评估了混合密钥交换协议产生的共享密钥的熵和独立同分布特性，以及混合方案的计算时间和消息长度。

Conclusion: 提出的混合方案结合了QKD和NIST标准化的PQC算法，为构建双重安全层提供了有效方法。

Abstract: Since the security of post-quantum cryptography (PQC) algorithms is based on
the hardness of mathematical problems, while the security of quantum key
distribution (QKD) relies on the fundamental principles of quantum physics,
each approach possesses distinct advantages and limitations that can complement
one another. Consequently, recent studies have proposed hybrid schemes that
combine QKD and PQC to establish a dual-layered security model. In response to
this trend, this study proposes hybrid schemes that integrate QKD with the
National Institute of Standards and Technology (NIST) standardized PQC
algorithms. These hybrid schemes include two core components: a hybrid QKD-PQC
key exchange protocol and a hybrid QKD-PQC digital signature scheme. For the
hybrid key exchange protocol, this study combines Module-Lattice-based Key
Encapsulation Mechanisms (ML-KEM) with QKD protocols, specifically BB84 and
E91, to construct a secure key exchange protocol. In the design of the hybrid
digital signature scheme, this study utilizes Module-Lattice-based Digital
Signature Algorithms (ML-DSA) and Stateless Hash-based Digital Signature
Algorithms (SLH-DSA) to generate signature reconstruction values. These values
are verified using confirmation codes transmitted via the BB84 and E91
protocols. The proposed hybrid key exchange protocol is evaluated by examining
the shared secret key it produces, particularly with respect to entropy and
whether the output is independent and identically distributed (IID).
Furthermore, the computation time and message lengths of the proposed hybrid
schemes are evaluated.

</details>


### [37] [Selmer-Inspired Elliptic Curve Generation](https://arxiv.org/abs/2510.02383)
*Awnon Bhowmik*

Main category: cs.CR

TL;DR: 提出基于Selmer理论的椭圆曲线构造框架，通过2-和3-下降方法生成可审计的曲线参数，结合局部可解性检查和密码学验证，实现透明可信的曲线设计。


<details>
  <summary>Details</summary>
Motivation: 现有标准椭圆曲线的参数生成过程不透明，存在信任问题，需要一种透明且可审计的构造方法来增强密码学系统的可信度。

Method: 使用2-和3-下降方法推导二元四次型和三元三次型，通过经典不变量确定候选参数，进行局部可解性筛选，最后转换为短Weierstrass形式并应用密码学验证。

Result: 概念验证实现表明该框架可作为Las Vegas算法运行，提供完整的可验证记录，能够生成符合密码学安全要求的椭圆曲线。

Conclusion: 该工作扩展了椭圆曲线的设计空间，证明算术几何中的下降技术可以为标准化就绪的构造提供信任增强基础，同时保持与恒定时间实现的兼容性。

Abstract: Elliptic curve cryptography (ECC) is foundational to modern secure
communication, yet existing standard curves have faced scrutiny for opaque
parameter-generation practices. This work introduces a Selmer-inspired
framework for constructing elliptic curves that is both transparent and
auditable. Drawing from $2$- and $3$-descent methods, we derive binary quartics
and ternary cubics whose classical invariants deterministically yield candidate
$(c_4,c_6)$ parameters. Local solubility checks, modeled on Selmer
admissibility, filter candidates prior to reconciliation into short-Weierstrass
form over prime fields. We then apply established cryptographic validations,
including group-order factorization, cofactor bounds, twist security, and
embedding-degree heuristics. A proof-of-concept implementation demonstrates
that the pipeline functions as a retry-until-success Las Vegas algorithm, with
complete transcripts enabling independent verification. Unlike seed-based or
purely efficiency-driven designs, our approach embeds arithmetic structure into
parameter selection while remaining compatible with constant-time, side-channel
resistant implementations. This work broadens the design space for elliptic
curves, showing that descent techniques from arithmetic geometry can underpin
trust-enhancing, standardization-ready constructions.

</details>


### [38] [Secure and Robust Watermarking for AI-generated Images: A Comprehensive Survey](https://arxiv.org/abs/2510.02384)
*Jie Cao,Qi Li,Zelin Zhang,Jianbing Ni*

Main category: cs.CR

TL;DR: 本文对AI生成图像水印技术进行了全面调查，涵盖系统形式化、技术比较、评估方法、安全漏洞和未来方向五个维度，旨在促进该领域发展。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI快速发展，高质量图像创建变得容易，但也引发了知识产权保护、真实性和责任归属等关键问题，需要水印技术来区分AI生成图像与自然内容。

Method: 采用系统性调查方法，从五个关键维度分析AI生成图像水印技术：系统形式化、多样化水印技术概述与比较、基于视觉质量、容量和可检测性的评估方法、恶意攻击漏洞分析。

Result: 提供了对AI生成图像水印技术的全面理解，包括各种技术的比较分析和评估框架，为研究人员提供了该领域的整体视图。

Conclusion: 水印技术是解决AI生成图像相关挑战的有前景方案，该调查旨在促进AI生成图像水印技术的持续发展，建立可信的数字生态系统。

Abstract: The rapid advancement of generative artificial intelligence (Gen-AI) has
facilitated the effortless creation of high-quality images, while
simultaneously raising critical concerns regarding intellectual property
protection, authenticity, and accountability. Watermarking has emerged as a
promising solution to these challenges by distinguishing AI-generated images
from natural content, ensuring provenance, and fostering trustworthy digital
ecosystems. This paper presents a comprehensive survey of the current state of
AI-generated image watermarking, addressing five key dimensions: (1)
formalization of image watermarking systems; (2) an overview and comparison of
diverse watermarking techniques; (3) evaluation methodologies with respect to
visual quality, capacity, and detectability; (4) vulnerabilities to malicious
attacks; and (5) prevailing challenges and future directions. The survey aims
to equip researchers with a holistic understanding of AI-generated image
watermarking technologies, thereby promoting their continued development.

</details>


### [39] [On The Fragility of Benchmark Contamination Detection in Reasoning Models](https://arxiv.org/abs/2510.02386)
*Han Wang,Haoyu Li,Brian Ko,Huan Zhang*

Main category: cs.CR

TL;DR: 该研究发现大型推理模型(LRMs)的基准污染检测存在严重漏洞，模型开发者可以通过简单的训练技巧轻松规避现有检测方法，从而在排行榜上获得虚高的性能表现。


<details>
  <summary>Details</summary>
Motivation: 由于排行榜竞争激烈，开发者有动机将评估基准数据混入训练集以获得更高排名，这种基准污染问题严重威胁评估的公平性和排行榜的完整性。

Method: 研究聚焦两种实际污染场景：(I)基础模型通过SFT和RL演变为LRM时，即使SFT阶段污染可被检测，但简短的GRPO训练就能掩盖污染信号；(II)在高级LRMs上应用带CoT的SFT污染作为最终阶段时，现有检测方法几乎失效。

Result: PPO风格的重要性采样和裁剪目标是检测掩盖的根本原因，表明广泛的RL方法都具有类似掩盖能力。污染后的LRMs对与训练集分布相似的未见样本仍保持高置信度，从而规避基于记忆的检测方法。

Conclusion: LRMs评估存在独特脆弱性，开发者可轻易污染模型获得虚高排名而留下极少痕迹，这严重破坏评估公平性和排行榜完整性，迫切需要针对LRMs的先进污染检测方法和可信评估协议。

Abstract: Leaderboards for LRMs have turned evaluation into a competition,
incentivizing developers to optimize directly on benchmark suites. A shortcut
to achieving higher rankings is to incorporate evaluation benchmarks into the
training data, thereby yielding inflated performance, known as benchmark
contamination. Surprisingly, our studies find that evading contamination
detections for LRMs is alarmingly easy. We focus on the two scenarios where
contamination may occur in practice: (I) when the base model evolves into LRM
via SFT and RL, we find that contamination during SFT can be originally
identified by contamination detection methods. Yet, even a brief GRPO training
can markedly conceal contamination signals that most detection methods rely on.
Further empirical experiments and theoretical analysis indicate that PPO style
importance sampling and clipping objectives are the root cause of this
detection concealment, indicating that a broad class of RL methods may
inherently exhibit similar concealment capability; (II) when SFT contamination
with CoT is applied to advanced LRMs as the final stage, most contamination
detection methods perform near random guesses. Without exposure to non-members,
contaminated LRMs would still have more confidence when responding to those
unseen samples that share similar distributions to the training set, and thus,
evade existing memorization-based detection methods. Together, our findings
reveal the unique vulnerability of LRMs evaluations: Model developers could
easily contaminate LRMs to achieve inflated leaderboards performance while
leaving minimal traces of contamination, thereby strongly undermining the
fairness of evaluation and threatening the integrity of public leaderboards.
This underscores the urgent need for advanced contamination detection methods
and trustworthy evaluation protocols tailored to LRMs.

</details>


### [40] [LLM-Generated Samples for Android Malware Detection](https://arxiv.org/abs/2510.02391)
*Nik Rollinson,Nikolaos Polatidis*

Main category: cs.CR

TL;DR: 研究探索了使用GPT-4.1-mini生成Android恶意软件合成数据来增强检测模型训练，发现合成数据能有效补充稀缺数据集但不足以作为独立训练源。


<details>
  <summary>Details</summary>
Motivation: Android恶意软件通过混淆和多态性不断进化，而基于签名的防御和机器学习模型在有限且不平衡的数据集上训练面临挑战，需要探索LLM在生成有效恶意软件数据中的作用。

Method: 微调GPT-4.1-mini为三个恶意软件家族生成结构化记录，使用KronoDroid数据集，通过提示工程和后处理解决生成不一致问题，并在三种设置下评估分类器：仅真实数据、真实加合成数据、仅合成数据。

Result: 仅使用真实数据训练可实现近乎完美的检测，添加合成数据后性能保持高位仅有轻微下降，而仅使用合成数据训练效果不一，有效性因恶意软件家族和微调策略而异。

Conclusion: LLM生成的恶意软件数据可以增强稀缺数据集而不损害检测精度，但作为独立训练源仍不足够。

Abstract: Android malware continues to evolve through obfuscation and polymorphism,
posing challenges for both signature-based defenses and machine learning models
trained on limited and imbalanced datasets. Synthetic data has been proposed as
a remedy for scarcity, yet the role of large language models (LLMs) in
generating effective malware data for detection tasks remains underexplored. In
this study, we fine-tune GPT-4.1-mini to produce structured records for three
malware families: BankBot, Locker/SLocker, and Airpush/StopSMS, using the
KronoDroid dataset. After addressing generation inconsistencies with prompt
engineering and post-processing, we evaluate multiple classifiers under three
settings: training with real data only, real-plus-synthetic data, and synthetic
data alone. Results show that real-only training achieves near perfect
detection, while augmentation with synthetic data preserves high performance
with only minor degradations. In contrast, synthetic-only training produces
mixed outcomes, with effectiveness varying across malware families and
fine-tuning strategies. These findings suggest that LLM-generated malware can
enhance scarce datasets without compromising detection accuracy, but remains
insufficient as a standalone training source.

</details>


### [41] [PolyLink: A Blockchain Based Decentralized Edge AI Platform for LLM Inference](https://arxiv.org/abs/2510.02395)
*Hongbo Liu,Jiannong Cao,Bo Yang,Dongbin Bai,Yinfeng Cao,Xiaoming Shen,Yinan Zhang,Jinwen Liang,Shan Jiang,Mingjin Zhang*

Main category: cs.CR

TL;DR: PolyLink是一个基于区块链的去中心化AI平台，通过分散化LLM开发和推理来解决中心化LLM服务的信任和成本问题。


<details>
  <summary>Details</summary>
Motivation: 当前LLM服务的部署和使用高度中心化，导致终端用户和开发者面临严重的信任问题和成本问题。

Method: 采用去中心化众包架构，支持在异构边缘设备上进行单设备和跨设备模型部署与推理；设计TIQE协议结合轻量级交叉编码器模型和LLM-as-a-Judge进行高精度推理评估；集成基于代币的激励模型，具有动态定价和奖励机制。

Result: 通过地理分布式部署在异构设备上进行实际评估，结果显示推理和验证延迟是实用的；安全分析表明系统能够抵抗模型退化攻击和验证器损坏。

Conclusion: PolyLink成功构建了一个去中心化的LLM平台，解决了中心化服务的信任和成本问题，系统性能和安全性能良好，现已开源可用。

Abstract: The rapid advancement of large language models (LLMs) in recent years has
revolutionized the AI landscape. However, the deployment model and usage of LLM
services remain highly centralized, creating significant trust issues and costs
for end users and developers. To address these issues, we propose PolyLink, a
blockchain-based decentralized AI platform that decentralizes LLM development
and inference. Specifically, PolyLink introduces a decentralized crowdsourcing
architecture that supports single-device and cross-device model deployment and
inference across heterogeneous devices at the edge. Moreover, to ensure the
inference integrity, we design the TIQE protocol, which combines a lightweight
cross-encoder model and an LLM-as-a-Judge for a high-accuracy inference
evaluation. Lastly, we integrate a comprehensive token-based incentive model
with dynamic pricing and reward mechanisms for all participants. We have
deployed PolyLink and conducted an extensive real-world evaluation through
geo-distributed deployment across heterogeneous devices. Results indicate that
the inference and verification latency is practical. Our security analysis
demonstrates that the system is resistant to model degradation attacks and
validator corruptions. PolyLink is now available at
https://github.com/IMCL-PolyLink/PolyLink.

</details>


### [42] [Dynamic Target Attack](https://arxiv.org/abs/2510.02422)
*Kedong Xiu,Churui Zeng,Tianhang Zheng,Xinzhe Huang,Xiaojun Jia,Di Wang,Puning Zhao,Zhan Qin,Kui Ren*

Main category: cs.CR

TL;DR: 提出动态目标攻击(DTA)框架，使用目标LLM自身响应作为优化目标来生成对抗性提示，显著降低目标与输出分布之间的差异，提高越狱攻击的效率和成功率。


<details>
  <summary>Details</summary>
Motivation: 现有基于梯度的越狱攻击通常优化对抗性后缀以诱导固定的肯定响应，但该目标通常位于安全对齐LLM输出分布的低密度区域，导致优化过程困难且效率低下。

Method: 在每轮优化中，DTA从当前提示的输出分布中采样多个候选响应，选择最有害的响应作为临时目标进行提示优化，通过动态调整目标来简化优化过程。

Result: 在白盒设置下，DTA仅需200次优化迭代即可在最新安全对齐LLMs上实现平均87%以上的攻击成功率，比现有最佳基线高出15%以上，时间成本减少2-26倍；在黑盒设置下，使用Llama-3-8B-Instruct作为代理模型，对Llama-3-70B-Instruct的攻击成功率达到85%，比同类方法高出25%以上。

Conclusion: DTA通过使用目标LLM自身响应作为动态优化目标，有效解决了现有越狱攻击中目标与输出分布差异过大的问题，显著提升了攻击效率和成功率。

Abstract: Existing gradient-based jailbreak attacks typically optimize an adversarial
suffix to induce a fixed affirmative response. However, this fixed target
usually resides in an extremely low-density region of a safety-aligned LLM's
output distribution conditioned on diverse harmful inputs. Due to the
substantial discrepancy between the target and the original output, existing
attacks require numerous iterations to optimize the adversarial prompt, which
might still fail to induce the low-probability target response from the target
LLM. In this paper, we propose Dynamic Target Attack (DTA), a new jailbreaking
framework relying on the target LLM's own responses as targets to optimize the
adversarial prompts. In each optimization round, DTA iteratively samples
multiple candidate responses directly from the output distribution conditioned
on the current prompt, and selects the most harmful response as a temporary
target for prompt optimization. In contrast to existing attacks, DTA
significantly reduces the discrepancy between the target and the output
distribution, substantially easing the optimization process to search for an
effective adversarial prompt.
  Extensive experiments demonstrate the superior effectiveness and efficiency
of DTA: under the white-box setting, DTA only needs 200 optimization iterations
to achieve an average attack success rate (ASR) of over 87\% on recent
safety-aligned LLMs, exceeding the state-of-the-art baselines by over 15\%. The
time cost of DTA is 2-26 times less than existing baselines. Under the
black-box setting, DTA uses Llama-3-8B-Instruct as a surrogate model for target
sampling and achieves an ASR of 85\% against the black-box target model
Llama-3-70B-Instruct, exceeding its counterparts by over 25\%.

</details>


### [43] [Adaptive Deception Framework with Behavioral Analysis for Enhanced Cybersecurity Defense](https://arxiv.org/abs/2510.02424)
*Basil Abdullah AL-Zahrani*

Main category: cs.CR

TL;DR: CADL是一个自适应欺骗框架，在CICIDS2017数据集上达到99.88%检测率和0.13%误报率，显著优于传统入侵检测系统。


<details>
  <summary>Details</summary>
Motivation: 提供一种比商业欺骗平台更经济实惠的替代方案，同时提高网络入侵检测的准确性和适应性。

Method: 采用集成机器学习（随机森林、XGBoost、神经网络）结合行为分析，通过协调信号总线架构实现安全组件间的实时情报共享。

Result: 在50,000个测试样本上，CADL检测率99.88%，误报率0.13%，行为分析对攻击者画像分类准确率达89%。

Conclusion: CADL框架在保持生产级误报率的同时，显著提升了入侵检测性能，提供了开源实现和透明性能指标。

Abstract: This paper presents CADL (Cognitive-Adaptive Deception Layer), an adaptive
deception framework achieving 99.88% detection rate with 0.13% false positive
rate on the CICIDS2017 dataset. The framework employs ensemble machine learning
(Random Forest, XGBoost, Neural Networks) combined with behavioral profiling to
identify and adapt responses to network intrusions. Through a coordinated
signal bus architecture, security components share real-time intelligence,
enabling collective decision-making. The system profiles attackers based on
temporal patterns and deploys customized deception strategies across five
escalation levels. Evaluation on 50,000 CICIDS2017 test samples demonstrates
that CADL significantly outperforms traditional intrusion detection systems
(Snort: 71.2%, Suricata: 68.5%) while maintaining production-ready false
positive rates. The framework's behavioral analysis achieves 89% accuracy in
classifying attacker profiles. We provide open-source implementation and
transparent performance metrics, offering an accessible alternative to
commercial deception platforms costing $150-400 per host annually.

</details>


### [44] [Rigorous Evaluation of Microarchitectural Side-Channels with Statistical Model Checking](https://arxiv.org/abs/2510.02475)
*Weihang Li,Pete Crowley,Arya Tschand,Yu Wang,Miroslav Pajic,Daniel Sorin*

Main category: cs.CR

TL;DR: 本文引入统计模型检验(SMC)来定量评估微架构侧信道，该方法能够处理概率性实验并提供统计保证，无需对处理器进行抽象或简化。


<details>
  <summary>Details</summary>
Motivation: 微架构侧信道评估面临两个挑战：处理器、攻击和防御常表现出概率性行为；处理器极其复杂，现有简化模型可能遗漏重要现象。

Method: 采用统计模型检验(SMC)技术，将处理器视为黑盒，通过概率实验进行定量评估，无需简化假设。

Result: 通过三个案例研究表明，SMC能够评估现有安全漏洞和防御措施，提供更严格的统计保证，并帮助防御者量化噪声注入策略。

Conclusion: SMC为微架构侧信道评估提供了一种严谨的统计方法，能够在不简化系统的情况下提供可操作的防御策略。

Abstract: Rigorous quantitative evaluation of microarchitectural side channels is
challenging for two reasons. First, the processors, attacks, and defenses often
exhibit probabilistic behaviors. These probabilistic behaviors arise due to
natural noise in systems (e.g., from co-running processes), probabilistic side
channel attacks, and probabilistic obfuscation defenses. Second,
microprocessors are extremely complex. Previous evaluation methods have relied
on abstract or simplified models, which are necessarily less detailed than real
systems or cycle-by-cycle simulators, and these models may miss important
phenomena. Whereas a simple model may suffice for estimating performance,
security issues frequently manifest in the details.
  We address this challenge by introducing Statistical Model Checking (SMC) to
the quantitative evaluation of microarchitectural side channels. SMC is a
rigorous statistical technique that can process the results of probabilistic
experiments and provide statistical guarantees, and it has been used in
computing applications that depend heavily on statistical guarantees (e.g.,
medical implants, vehicular computing). With SMC, we can treat processors as
opaque boxes, and we do not have to abstract or simplify them. We demonstrate
the effectiveness of SMC through three case studies, in which we experimentally
show that SMC can evaluate existing security vulnerabilities and defenses and
provide qualitatively similar conclusions with greater statistical rigor, while
making no simplifying assumptions or abstractions. We also show that SMC can
enable a defender to quantify the amount of noise necessary to have a desired
level of confidence that she has reduced an attacker's probability of success
to less than a desired threshold, thus providing the defender with an
actionable plan for obfuscation via noise injection.

</details>


### [45] [TLoRa: Implementing TLS Over LoRa for Secure HTTP Communication in IoT](https://arxiv.org/abs/2510.02519)
*Atonu Ghosh,Akhilesh Mohanasundaram,Srishivanth R F,Sudip Misra*

Main category: cs.CR

TL;DR: TLoRa是一个端到端架构，通过集成TCP隧道和完整TLS 1.3握手，在LoRa上实现HTTPS通信，为WiFi设备通过LoRa网关提供安全互联网访问。


<details>
  <summary>Details</summary>
Motivation: 解决在LoRa低功耗广域网上实现安全HTTPS通信的挑战，为WiFi设备提供通过LoRa网关访问互联网的完整解决方案。

Method: 采用End Hub和Net Relay架构，通过三个阶段实现：会话建立（TCP socket管理和TLS握手）、安全隧道（在LoRa上传输加密TLS数据）、内容渲染。包含轻量级TLS记录重组层和会话复用队列机制。

Result: 在实际硬件上评估显示，成功在9.9秒内建立LoRa上的TLS会话，API请求完成时间为3.58秒，证明该方案具有实用性。

Conclusion: TLoRa是首个在LoRa上使用完整TLS全面设计、实现和评估HTTPS访问性能的工作，为LoRa网络上的安全互联网通信提供了可行解决方案。

Abstract: We present TLoRa, an end-to-end architecture for HTTPS communication over
LoRa by integrating TCP tunneling and a complete TLS 1.3 handshake. It enables
a seamless and secure communication channel between WiFi-enabled end devices
and the Internet over LoRa using an End Hub (EH) and a Net Relay (NR). The EH
tethers a WiFi hotspot and a captive portal for user devices to connect and
request URLs. The EH forwards the requested URLs to the NR using a secure
tunnel over LoRa. The NR, which acts as a server-side proxy, receives and
resolves the request from the Internet-based server. It then relays back the
encrypted response from the server over the same secure tunnel. TLoRa operates
in three phases -session setup, secure tunneling, and rendering. In the first
phase, it manages the TCP socket and initiates the TLS handshake. In the
second, it creates a secure tunnel and transfers encrypted TLS data over LoRa.
Finally, it delivers the URL content to the user. TLoRa also implements a
lightweight TLS record reassembly layer and a queuing mechanism for session
multiplexing. We evaluate TLoRa on real hardware using multiple accesses to a
web API. Results indicate that it provides a practical solution by successfully
establishing a TLS session over LoRa in 9.9 seconds and takes 3.58 seconds to
fulfill API requests. To the best of our knowledge, this is the first work to
comprehensively design, implement, and evaluate the performance of HTTPS access
over LoRa using full TLS.

</details>


### [46] [ToolTweak: An Attack on Tool Selection in LLM-based Agents](https://arxiv.org/abs/2510.02554)
*Jonathan Sneh,Ruomei Yan,Jialin Yu,Philip Torr,Yarin Gal,Sunando Sengupta,Eric Sommerlade,Alasdair Paren,Adel Bibi*

Main category: cs.CR

TL;DR: ToolTweak是一种轻量级自动攻击方法，通过迭代操纵工具名称和描述，能够系统性地偏置AI代理选择特定工具，将选择率从约20%提升至81%，揭示了工具生态系统中的公平性、竞争性和安全性风险。


<details>
  <summary>Details</summary>
Motivation: 随着LLM驱动的代理越来越多地使用外部工具，工具选择过程存在关键漏洞：攻击者可以通过操纵工具元数据来获得不公平优势，影响工具生态系统的公平竞争。

Method: 提出ToolTweak攻击方法，通过迭代优化工具名称和描述来偏置代理选择，并在开源和闭源模型上测试其可转移性。

Result: 攻击成功将工具选择率从约20%提升至81%，具有强跨模型可转移性，并导致工具使用分布发生偏移。

Conclusion: 工具选择过程存在严重安全漏洞，需要防御措施（如释义和困惑度过滤）来减轻偏见，确保工具生态系统的公平竞争。

Abstract: As LLMs increasingly power agents that interact with external tools, tool use
has become an essential mechanism for extending their capabilities. These
agents typically select tools from growing databases or marketplaces to solve
user tasks, creating implicit competition among tool providers and developers
for visibility and usage. In this paper, we show that this selection process
harbors a critical vulnerability: by iteratively manipulating tool names and
descriptions, adversaries can systematically bias agents toward selecting
specific tools, gaining unfair advantage over equally capable alternatives. We
present ToolTweak, a lightweight automatic attack that increases selection
rates from a baseline of around 20% to as high as 81%, with strong
transferability between open-source and closed-source models. Beyond individual
tools, we show that such attacks cause distributional shifts in tool usage,
revealing risks to fairness, competition, and security in emerging tool
ecosystems. To mitigate these risks, we evaluate two defenses: paraphrasing and
perplexity filtering, which reduce bias and lead agents to select functionally
similar tools more equally. All code will be open-sourced upon acceptance.

</details>


### [47] [Who's Wearing? Ear Canal Biometric Key Extraction for User Authentication on Wireless Earbuds](https://arxiv.org/abs/2510.02563)
*Chenpei Huang,Lingfeng Yao,Hui Zhong,Kyu In Lee,Lan Zhang,Xiaoyong Yuan,Tomoaki Ohtsuki,Miao Pan*

Main category: cs.CR

TL;DR: EarID是一种基于耳道扫描的认证协议，直接在耳机上提取二进制密钥，避免原始生物特征数据泄露，实现高效安全的身份验证。


<details>
  <summary>Details</summary>
Motivation: 解决现有耳道扫描认证方法中原始生物特征数据泄露风险和计算效率问题，为资源受限的耳机设备提供实用的认证方案。

Method: 提出EarID协议，在耳机上直接提取独特的二进制密钥，使用隐私保护的模糊承诺方案在移动设备上验证佩戴者密钥，无需依赖机器学习分类器。

Result: 达到98.7%的认证准确率，与机器学习分类器相当；移动设备注册时间160ms，耳机处理时间226ms；在所有对抗场景下保持低于1%的误接受率。

Conclusion: EarID为下一代无线耳机提供了实用且安全的认证解决方案，具有高精度、低延迟和强抗攻击能力。

Abstract: Ear canal scanning/sensing (ECS) has emerged as a novel biometric
authentication method for mobile devices paired with wireless earbuds. Existing
studies have demonstrated the uniqueness of ear canals by training and testing
machine learning classifiers on ECS data. However, implementing practical
ECS-based authentication requires preventing raw biometric data leakage and
designing computationally efficient protocols suitable for resource-constrained
earbuds. To address these challenges, we propose an ear canal key extraction
protocol, \textbf{EarID}. Without relying on classifiers, EarID extracts unique
binary keys directly on the earbuds during authentication. These keys further
allow the use of privacy-preserving fuzzy commitment scheme that verifies the
wearer's key on mobile devices. Our evaluation results demonstrate that EarID
achieves a 98.7\% authentication accuracy, comparable to machine learning
classifiers. The mobile enrollment time (160~ms) and earbuds processing time
(226~ms) are negligible in terms of wearer's experience. Moreover, our approach
is robust and attack-resistant, maintaining a false acceptance rate below 1\%
across all adversarial scenarios. We believe the proposed EarID offers a
practical and secure solution for next-generation wireless earbuds.

</details>


### [48] [Using Preformed Resistive Random Access Memory to Create a Strong Physically Unclonable Function](https://arxiv.org/abs/2510.02643)
*Jack Garrard,John F. Hardy II,Carlo daCunha,Mayank Bakshi*

Main category: cs.CR

TL;DR: 提出了一种基于阻变存储器(ReRAM)的新型物理不可克隆函数(PUF)协议，通过未形成ReRAM的差分读取生成响应，并在物理ReRAM设备上进行了实验演示。


<details>
  <summary>Details</summary>
Motivation: 物理不可克隆函数(PUF)在身份验证和非对称加密方面具有应用前景，需要创建具有大挑战空间的物理ReRAM PUF。

Method: 使用未形成ReRAM的差分读取作为响应生成方法，构建基于ReRAM的PUF协议。

Result: 在物理ReRAM设备上成功演示了该协议，作为PUF表现出优异的性能特征。

Conclusion: 该ReRAM PUF协议能够创建具有大挑战空间的物理PUF，并通过硬件实验验证了其可行性。

Abstract: Physically Unclonable Functions (PUFs) are a promising solution for identity
verification and asymmetric encryption. In this paper, a new Resistive Random
Access Memory (ReRAM) PUF-based protocol is presented to create a physical
ReRAM PUF with a large challenge space. This protocol uses differential reads
from unformed ReRAM as the method for response generation. Lastly, this paper
also provides an experimental hardware demonstration of this protocol on a
Physical ReRAM device, along with providing notable results as a PUF, with
excellent performance characteristics.

</details>


### [49] [MALF: A Multi-Agent LLM Framework for Intelligent Fuzzing of Industrial Control Protocols](https://arxiv.org/abs/2510.02694)
*Bowei Ning,Xuejun Zong,Kan He*

Main category: cs.CR

TL;DR: MALF是一个多智能体LLM模糊测试框架，通过集成大语言模型和多智能体协调来发现工业控制协议中的漏洞，在真实工业环境中成功识别了关键漏洞。


<details>
  <summary>Details</summary>
Motivation: 工业控制系统对现代基础设施至关重要，但通信协议中的弱点使其面临日益严重的网络安全威胁，需要更先进的漏洞发现方法。

Method: 结合检索增强生成(RAG)获取领域知识，使用QLoRA微调实现协议感知输入生成，多智能体框架优化种子生成、变异策略和反馈驱动优化。

Result: 在Modbus/TCP、S7Comm和Ethernet/IP协议测试中，测试用例通过率达88-92%，生成更多异常触发，种子覆盖率超过90%，熵值4.2-4.6位，在真实电厂环境中发现三个零日漏洞。

Conclusion: 多智能体LLM在ICS网络安全中具有变革潜力，提供了一个可扩展的自动化框架，为漏洞发现设定了新标准，增强了关键基础设施的安全性。

Abstract: Industrial control systems (ICS) are vital to modern infrastructure but
increasingly vulnerable to cybersecurity threats, particularly through
weaknesses in their communication protocols. This paper presents MALF
(Multi-Agent LLM Fuzzing Framework), an advanced fuzzing solution that
integrates large language models (LLMs) with multi-agent coordination to
identify vulnerabilities in industrial control protocols (ICPs). By leveraging
Retrieval-Augmented Generation (RAG) for domain-specific knowledge and QLoRA
fine-tuning for protocol-aware input generation, MALF enhances fuzz testing
precision and adaptability. The multi-agent framework optimizes seed
generation, mutation strategies, and feedback-driven refinement, leading to
improved vulnerability discovery. Experiments on protocols like Modbus/TCP,
S7Comm, and Ethernet/IP demonstrate that MALF surpasses traditional methods,
achieving a test case pass rate (TCPR) of 88-92% and generating more exception
triggers (ETN). MALF also maintains over 90% seed coverage and Shannon entropy
values between 4.2 and 4.6 bits, ensuring diverse, protocol-compliant
mutations. Deployed in a real-world Industrial Attack-Defense Range for power
plants, MALF identified critical vulnerabilities, including three zero-day
flaws, one confirmed and registered by CNVD. These results validate MALF's
effectiveness in real-world fuzzing applications. This research highlights the
transformative potential of multi-agent LLMs in ICS cybersecurity, offering a
scalable, automated framework that sets a new standard for vulnerability
discovery and strengthens critical infrastructure security against emerging
threats.

</details>


### [50] [A Statistical Method for Attack-Agnostic Adversarial Attack Detection with Compressive Sensing Comparison](https://arxiv.org/abs/2510.02707)
*Chinthana Wimalasuriya,Spyros Tragoudas*

Main category: cs.CR

TL;DR: 提出了一种基于压缩/未压缩神经网络对行为的统计方法，用于实时对抗攻击检测，在各种攻击类型上实现近乎完美的检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有对抗攻击检测方法往往无法有效检测未见攻击类型，且对不同攻击类型的检测准确率有限，需要更可靠和实用的检测方案。

Method: 通过比较压缩和未压缩神经网络对的行为，生成对抗攻击存在性的度量指标，建立部署前的检测基线。

Result: 与最先进技术相比，该方法在各种攻击类型上实现近乎完美的检测率，并显著降低了误报率。

Conclusion: 该方法既可靠又实用，适用于现实世界应用中的对抗攻击检测。

Abstract: Adversarial attacks present a significant threat to modern machine learning
systems. Yet, existing detection methods often lack the ability to detect
unseen attacks or detect different attack types with a high level of accuracy.
In this work, we propose a statistical approach that establishes a detection
baseline before a neural network's deployment, enabling effective real-time
adversarial detection. We generate a metric of adversarial presence by
comparing the behavior of a compressed/uncompressed neural network pair. Our
method has been tested against state-of-the-art techniques, and it achieves
near-perfect detection across a wide range of attack types. Moreover, it
significantly reduces false positives, making it both reliable and practical
for real-world applications.

</details>


### [51] [Attack via Overfitting: 10-shot Benign Fine-tuning to Jailbreak LLMs](https://arxiv.org/abs/2510.02833)
*Zhixin Xie,Xurui Song,Jun Luo*

Main category: cs.CR

TL;DR: 通过仅使用10个良性问答对微调LLMs，可以成功实现越狱攻击。该方法先通过包含相同拒绝回答的良性问答对使LLM过拟合，然后用标准良性答案进一步微调，使过拟合的LLM忘记拒绝态度，从而对有害问题也提供顺从回答。


<details>
  <summary>Details</summary>
Motivation: 尽管已有大量安全对齐工作，但LLMs仍然容易受到越狱攻击。现有的恶意微调攻击虽然有效但容易被检测，因此需要开发更隐蔽的攻击方法。

Method: 首先使用包含相同拒绝回答的良性问答对使LLM过拟合，然后使用标准良性答案进行进一步微调，使模型忘记拒绝态度。

Result: 在10个LLMs上实现攻击，与5个基线方法相比，该方法在攻击效果和隐蔽性方面都具有显著优势。

Conclusion: 该研究揭示了当前LLMs中先前未报告的安全漏洞，为理解即使使用良性微调如何损害LLMs安全提供了新视角。

Abstract: Despite substantial efforts in safety alignment, recent research indicates
that Large Language Models (LLMs) remain highly susceptible to jailbreak
attacks. Among these attacks, finetuning-based ones that compromise LLMs'
safety alignment via fine-tuning stand out due to its stable jailbreak
performance. In particular, a recent study indicates that fine-tuning with as
few as 10 harmful question-answer (QA) pairs can lead to successful
jailbreaking across various harmful questions. However, such malicious
fine-tuning attacks are readily detectable and hence thwarted by moderation
models. In this paper, we demonstrate that LLMs can be jailbroken by
fine-tuning with only 10 benign QA pairs; our attack exploits the increased
sensitivity of LLMs to fine-tuning data after being overfitted. Specifically,
our fine-tuning process starts with overfitting an LLM via fine-tuning with
benign QA pairs involving identical refusal answers. Further fine-tuning is
then performed with standard benign answers, causing the overfitted LLM to
forget the refusal attitude and thus provide compliant answers regardless of
the harmfulness of a question. We implement our attack on the ten LLMs and
compare it with five existing baselines. Experiments demonstrate that our
method achieves significant advantages in both attack effectiveness and attack
stealth. Our findings expose previously unreported security vulnerabilities in
current LLMs and provide a new perspective on understanding how LLMs' security
is compromised, even with benign fine-tuning. Our code is available at
https://github.com/ZHIXINXIE/tenBenign.

</details>


### [52] [Improved Search-to-Decision Reduction for Random Local Functions](https://arxiv.org/abs/2510.02944)
*Kel Zin Tan,Prashant Nalini Vasudevan*

Main category: cs.CR

TL;DR: 本文提出了针对任意常数元谓词的随机局部函数的新搜索到决策归约，改进了之前需要谓词具有额外敏感性属性的限制。


<details>
  <summary>Details</summary>
Motivation: 随机局部函数作为约束满足问题的自然实例分布，被Goldreich提出作为低复杂度单向函数的候选，也被广泛研究作为伪随机生成器的潜在候选。

Method: 提出新的搜索到决策归约方法，给定能区分随机局部函数输出与随机输出的高效算法，可以构造出能反转此类函数的高效算法。

Result: 归约产生的高效算法能以Ω(ε)概率成功反转具有Õ(m(n/ε)²)输出的随机局部函数。

Conclusion: 如果局部函数族是单向的，那么具有较短输出长度的相关函数族就是伪随机生成器族，该结果可推广到某些超常数元数和噪声谓词。

Abstract: A random local function defined by a $d$-ary predicate $P$ is one where each
output bit is computed by applying $P$ to $d$ randomly chosen bits of its
input. These represent natural distributions of instances for constraint
satisfaction problems. They were put forward by Goldreich as candidates for
low-complexity one-way functions, and have subsequently been widely studied
also as potential pseudo-random generators.
  We present a new search-to-decision reduction for random local functions
defined by any predicate of constant arity. Given any efficient algorithm that
can distinguish, with advantage $\epsilon$, the output of a random local
function with $m$ outputs and $n$ inputs from random, our reduction produces an
efficient algorithm that can invert such functions with
$\tilde{O}(m(n/\epsilon)^2)$ outputs, succeeding with probability
$\Omega(\epsilon)$. This implies that if a family of local functions is
one-way, then a related family with shorter output length is family of
pseudo-random generators.
  Prior to our work, all such reductions that were known required the predicate
to have additional sensitivity properties, whereas our reduction works for any
predicate. Our results also generalise to some super-constant values of the
arity $d$, and to noisy predicates.

</details>


### [53] [SoK: Preconfirmations](https://arxiv.org/abs/2510.02947)
*Aikaterini-Panagiota Stouka,Conor McMenamin,Demetris Kyriacou,Lin Oshitani,Quentin Botha*

Main category: cs.CR

TL;DR: 本文对区块链预确认协议进行了系统化知识整理，提出了预确认协议的一般框架，并分析了其经济学和风险，最后调查了现实世界中的预确认协议实现。


<details>
  <summary>Details</summary>
Motivation: 传统区块链协议存在固有延迟，限制了用户体验的提升。预确认协议通过提供早期交易确认保证来改善这一问题。

Method: 采用系统化知识整理方法，定义核心术语，构建预确认协议的一般框架，分析经济学和风险，并调查现实世界实现。

Result: 建立了预确认协议的完整理论框架，连接了理论与实践之间的差距。

Conclusion: 预确认协议是改善区块链用户体验的重要发展方向，需要综合考虑技术实现、经济激励和安全风险。

Abstract: In recent years, significant research efforts have focused on improving
blockchain throughput and confirmation speeds without compromising security.
While decreasing the time it takes for a transaction to be included in the
blockchain ledger enhances user experience, a fundamental delay still remains
between when a transaction is issued by a user and when its inclusion is
confirmed in the blockchain ledger. This delay limits user experience gains
through the confirmation uncertainty it brings for users. This inherent delay
in conventional blockchain protocols has led to the emergence of
preconfirmation protocols -- protocols that provide users with early guarantees
of eventual transaction confirmation.
  This article presents a Systematization of Knowledge (SoK) on
preconfirmations. We present the core terms and definitions needed to
understand preconfirmations, outline a general framework for preconfirmation
protocols, and explore the economics and risks of preconfirmations. Finally, we
survey and apply our framework to several implementations of real-world
preconfirmation protocols, bridging the gap between theory and practice.

</details>


### [54] [SoK: Kicking CAN Down the Road. Systematizing CAN Security Knowledge](https://arxiv.org/abs/2510.02960)
*Khaled Serag,Zhaozhou Tang,Sungwoo Kim,Vireshwar Kumar,Dave,Tian,Saman Zonouz,Raheem Beyah,Dongyan Xu,Z. Berkay Celik*

Main category: cs.CR

TL;DR: 本文系统化整理了CAN总线安全知识，提出了攻击者、攻击和防御的综合分类与评估模型，识别了可复现攻击和防御漏洞，并分析了新兴车载总线技术的安全问题。


<details>
  <summary>Details</summary>
Motivation: CAN总线安全研究缺乏系统化整理，难以评估攻击严重性和防御有效性，导致非专家难以判断特定攻击或防御对其系统的相关性，且新兴车载总线技术可能误导人们认为仅采用新技术就能解决CAN的安全挑战。

Method: 构建了攻击者、攻击和防御的综合分类与评估模型，识别可复现攻击和防御漏洞，分析其根本原因，并正式分析三种新兴车载总线技术以识别与CAN共享的根本原因。

Result: 研究发现CAN比普遍认为的更安全，大多数不安全根源在车载总线间共享，仅采用更新的车载总线技术并不能解决持续的安全问题。

Conclusion: 需要未来研究方向来确保车载总线通信的安全，挑战了关于CAN安全性和新兴总线技术有效性的常见认知。

Abstract: For decades, the Controller Area Network (CAN) has served as the primary
in-vehicle bus (IVB) and extended its use to many non-vehicular systems. Over
the past years, CAN security has been intensively scrutinized, yielding
extensive research literature. Despite its wealth, the literature lacks
structured systematization, complicating efforts to assess attack severity,
defense efficacy, identify security gaps, or root causes. This leaves non
experts uncertain about the relevancy of specific attacks or defenses to their
systems, inadvertently portraying CAN as irredeemably insecure. Further, the
introduction of new IVB technologies--CAN evolutions, add-ons, and alternative
buses--with heightened security claims risks fostering the misconception that
merely adopting these technologies resolves CAN's security challenges.
  This paper systematizes existing CAN security knowledge, presenting a
comprehensive taxonomy and assessment models of attackers, attacks, and
defenses. It identifies replicable attacks and defense gaps, investigating
their root causes as inherent, accidental, unique, or universal. It then
extrapolates these insights to emerging IVB technologies by formally analyzing
three emerging IVBs to identify shared root causes with CAN and assess their
ability to close security gaps. The findings challenge common perceptions,
demonstrating that CAN is more securable than perceived, that most insecurity
root causes are shared across IVBs, and that merely adopting newer IVB
technology does not solve persistent security issues. The paper concludes by
highlighting future research directions to secure IVB communication down the
road.

</details>


### [55] [External Data Extraction Attacks against Retrieval-Augmented Large Language Models](https://arxiv.org/abs/2510.02964)
*Yu He,Yifei Chen,Yiming Li,Shuo Shao,Leyi Qi,Boheng Li,Dacheng Tao,Zhan Qin*

Main category: cs.CR

TL;DR: 本文提出了SECRET攻击框架，这是首个针对检索增强语言模型(RAG)的外部数据提取攻击(EDEA)的全面研究，显著优于现有攻击方法，成功从Claude 3.7 Sonnet中提取35%的数据。


<details>
  <summary>Details</summary>
Motivation: RAG虽然能增强LLMs的专业知识，但引入了外部数据提取攻击风险，现有研究缺乏正式框架和有效攻击方法，无法评估真实世界中的威胁可行性。

Method: 提出统一EDEA框架，将攻击设计分解为提取指令、越狱操作符和检索触发器三个组件，并开发SECRET攻击，包含LLM优化的自适应越狱提示生成和集群聚焦触发策略。

Result: 在4个模型上的广泛评估显示，SECRET显著优于先前攻击，对16个测试RAG实例都高度有效，首次从Claude 3.7 Sonnet中成功提取35%数据，而其他攻击提取率为0%。

Conclusion: 研究揭示了RAG系统面临的外部数据提取威胁，呼吁关注这一新兴安全风险。

Abstract: In recent years, RAG has emerged as a key paradigm for enhancing large
language models (LLMs). By integrating externally retrieved information, RAG
alleviates issues like outdated knowledge and, crucially, insufficient domain
expertise. While effective, RAG introduces new risks of external data
extraction attacks (EDEAs), where sensitive or copyrighted data in its
knowledge base may be extracted verbatim. These risks are particularly acute
when RAG is used to customize specialized LLM applications with private
knowledge bases. Despite initial studies exploring these risks, they often lack
a formalized framework, robust attack performance, and comprehensive
evaluation, leaving critical questions about real-world EDEA feasibility
unanswered.
  In this paper, we present the first comprehensive study to formalize EDEAs
against retrieval-augmented LLMs. We first formally define EDEAs and propose a
unified framework decomposing their design into three components: extraction
instruction, jailbreak operator, and retrieval trigger, under which prior
attacks can be considered instances within our framework. Guided by this
framework, we develop SECRET: a Scalable and EffeCtive exteRnal data Extraction
aTtack. Specifically, SECRET incorporates (1) an adaptive optimization process
using LLMs as optimizers to generate specialized jailbreak prompts for EDEAs,
and (2) cluster-focused triggering, an adaptive strategy that alternates
between global exploration and local exploitation to efficiently generate
effective retrieval triggers. Extensive evaluations across 4 models reveal that
SECRET significantly outperforms previous attacks, and is highly effective
against all 16 tested RAG instances. Notably, SECRET successfully extracts 35%
of the data from RAG powered by Claude 3.7 Sonnet for the first time, whereas
other attacks yield 0% extraction. Our findings call for attention to this
emerging threat.

</details>


### [56] [Untargeted Jailbreak Attack](https://arxiv.org/abs/2510.02999)
*Xinzhe Huang,Wenjing Hu,Tianhang Zheng,Kedong Xiu,Xiaojun Jia,Di Wang,Zhan Qin,Kui Ren*

Main category: cs.CR

TL;DR: 提出首个基于梯度的无目标越狱攻击UJA，通过最大化模型响应的不安全概率来绕过安全对齐，相比现有目标攻击方法显著提升了攻击成功率和效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于梯度的越狱攻击方法通过优化对抗后缀来使LLM输出预定义的目标响应，这种方法限制了对抗搜索空间，且需要大量优化迭代来弥合固定目标与原始响应之间的差距，导致攻击效果和效率受限。

Method: 提出无目标攻击目标，最大化LLM响应的不安全概率，使用评判模型量化不安全概率。由于目标不可微分，将其分解为两个可微分的子目标：优化最优有害响应和相应的对抗提示，并通过理论分析验证分解的有效性。

Result: 在仅100次优化迭代的情况下，UJA对最新安全对齐LLM的攻击成功率超过80%，比最先进的基于梯度攻击方法（如I-GCG和COLD-Attack）高出20%以上。

Conclusion: 无目标越狱攻击通过不受限制的目标显著扩展了搜索空间，能够更灵活高效地探索LLM漏洞，相比目标攻击方法具有明显优势。

Abstract: Existing gradient-based jailbreak attacks on Large Language Models (LLMs),
such as Greedy Coordinate Gradient (GCG) and COLD-Attack, typically optimize
adversarial suffixes to align the LLM output with a predefined target response.
However, by restricting the optimization objective as inducing a predefined
target, these methods inherently constrain the adversarial search space, which
limit their overall attack efficacy. Furthermore, existing methods typically
require a large number of optimization iterations to fulfill the large gap
between the fixed target and the original model response, resulting in low
attack efficiency.
  To overcome the limitations of targeted jailbreak attacks, we propose the
first gradient-based untargeted jailbreak attack (UJA), aiming to elicit an
unsafe response without enforcing any predefined patterns. Specifically, we
formulate an untargeted attack objective to maximize the unsafety probability
of the LLM response, which can be quantified using a judge model. Since the
objective is non-differentiable, we further decompose it into two
differentiable sub-objectives for optimizing an optimal harmful response and
the corresponding adversarial prompt, with a theoretical analysis to validate
the decomposition. In contrast to targeted jailbreak attacks, UJA's
unrestricted objective significantly expands the search space, enabling a more
flexible and efficient exploration of LLM vulnerabilities.Extensive evaluations
demonstrate that \textsc{UJA} can achieve over 80\% attack success rates
against recent safety-aligned LLMs with only 100 optimization iterations,
outperforming the state-of-the-art gradient-based attacks such as I-GCG and
COLD-Attack by over 20\%.

</details>


### [57] [Protecting Persona Biometric Data: The Case of Facial Privacy](https://arxiv.org/abs/2510.03035)
*Lambert Hogenhout,Rinzin Wangmo*

Main category: cs.CR

TL;DR: 本文提出面部隐私概念，分析无监管面部识别技术的挑战，比较全球法律框架，指出法律漏洞使个人易受侵害，建议从数据财产权转向不可剥夺权利的新政策框架。


<details>
  <summary>Details</summary>
Motivation: 数字技术普及导致大量面部数据被收集，公司未经明确同意使用面部识别技术进行监控，由于法律薄弱分散形成监管真空，威胁个人隐私和自主权。

Method: 通过全面审查现有法律框架，分析比较GDPR、巴西LGPD、加拿大PIPEDA、中国、新加坡、韩国、日本隐私法以及美国伊利诺伊州生物识别信息隐私法等法规。

Result: 分析显示现有法律漏洞和模糊性使个人易受侵害，面部识别技术可能带来歧视性偏见和不可更改生物识别数据被盗的持久伤害等社会影响。

Conclusion: 主张从数据作为财产的模式转向不可剥夺权利的新政策框架，确保基本人权不受不受控制的技术扩张侵害。

Abstract: The proliferation of digital technologies has led to unprecedented data
collection, with facial data emerging as a particularly sensitive commodity.
Companies are increasingly leveraging advanced facial recognition technologies,
often without the explicit consent or awareness of individuals, to build
sophisticated surveillance capabilities. This practice, fueled by weak and
fragmented laws in many jurisdictions, has created a regulatory vacuum that
allows for the commercialization of personal identity and poses significant
threats to individual privacy and autonomy. This article introduces the concept
of Facial Privacy. It analyzes the profound challenges posed by unregulated
facial recognition by conducting a comprehensive review of existing legal
frameworks. It examines and compares regulations such as the GDPR, Brazil's
LGPD, Canada's PIPEDA, and privacy acts in China, Singapore, South Korea, and
Japan, alongside sector-specific laws in the United States like the Illinois
Biometric Information Privacy Act (BIPA). The analysis highlights the societal
impacts of this technology, including the potential for discriminatory bias and
the long-lasting harm that can result from the theft of immutable biometric
data. Ultimately, the paper argues that existing legal loopholes and
ambiguities leave individuals vulnerable. It proposes a new policy framework
that shifts the paradigm from data as property to a model of inalienable
rights, ensuring that fundamental human rights are upheld against unchecked
technological expansion.

</details>


### [58] [TPM-Based Continuous Remote Attestation and Integrity Verification for 5G VNFs on Kubernetes](https://arxiv.org/abs/2510.03219)
*Al Nahian Bin Emran,Rajendra Upadhyay,Rajendra Paudyal,Lisa Donnan,Duminda Wijesekera*

Main category: cs.CR

TL;DR: 提出基于TPM 2.0的持续远程认证解决方案，为Kubernetes上部署的5G核心组件提供硬件级运行时完整性验证。


<details>
  <summary>Details</summary>
Motivation: 5G采用云原生架构后，虽然提升了可扩展性和灵活性，但引入了新的安全挑战。现有5G安全规范缺乏对网络功能运行时完整性的持续验证机制，不符合零信任原则。

Method: 使用Linux完整性测量架构(IMA)和可信平台模块(TPM)，集成Keylime框架和自定义IMA模板，实现pod级别的隔离测量和完整性验证。

Result: 在k3s集群上的原型系统能够实时检测未经授权的修改，标记每个pod的信任状态，并生成详细审计日志。

Conclusion: 该工作为云原生和边缘部署提供了基于硬件的持续认证，增强了5G在多供应商和关键任务场景下的弹性。

Abstract: In the rapidly evolving landscape of 5G technology, the adoption of
cloud-based infrastructure for the deployment of 5G services has become
increasingly common. Using a service-based architecture, critical 5G
components, such as the Access and Mobility Management Function (AMF), Session
Management Function (SMF), and User Plane Function (UPF), now run as
containerized pods on Kubernetes clusters. Although this approach improves
scalability, flexibility, and resilience, it also introduces new security
challenges, particularly to ensure the integrity and trustworthiness of these
components. Current 5G security specifications (for example, 3GPP TS 33.501)
focus on communication security and assume that network functions remain
trustworthy after authentication, consequently lacking mechanisms to
continuously validate the integrity of NVFs at runtime. To close this gap, and
to align with Zero Trust principles of 'never trust, always verify', we present
a TPM 2.0-based continuous remote attestation solution for core 5G components
deployed on Kubernetes. Our approach uses the Linux Integrity Measurement
Architecture (IMA) and a Trusted Platform Module (TPM) to provide
hardware-based runtime validation. We integrate the open-source Keylime
framework with a custom IMA template that isolates pod-level measurements,
allowing per-pod integrity verification. A prototype on a k3s cluster
(consisting of 1 master, 2 worker nodes) was implemented to attest to core
functions, including AMF, SMF and UPF. The experimental results show that the
system detects unauthorized modifications in real time, labels each pod's trust
state, and generates detailed audit logs. This work provides hardware-based
continuous attestation for cloud native and edge deployments, strengthening the
resilience of 5G as critical infrastructure in multi-vendor and
mission-critical scenarios of 5G.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [59] [BrowserArena: Evaluating LLM Agents on Real-World Web Navigation Tasks](https://arxiv.org/abs/2510.02418)
*Sagnik Anupam,Davis Brown,Shuo Li,Eric Wong,Hamed Hassani,Osbert Bastani*

Main category: cs.AI

TL;DR: BrowserArena是一个开放的网页代理评估平台，通过用户提交任务、Arena式对比和步骤级人工反馈来识别网页代理的失败模式，发现了验证码解决、弹窗移除和直接URL导航三个主要问题。


<details>
  <summary>Details</summary>
Motivation: 当前网页代理评估局限于沙盒环境或人工任务，需要真实开放网页环境下的评估平台来发现实际使用中的问题。

Method: 构建BrowserArena平台，收集用户任务，进行Arena式对比评估，使用步骤级人工反馈分析代理轨迹，并针对发现的失败模式构建专门数据集进行深入研究。

Result: 识别出三个一致的失败模式：验证码解决、弹窗移除和直接URL导航。不同语言模型在这些任务上表现各异，如o4-mini使用更多策略绕过验证码，DeepSeek-R1在验证码解决上误导用户。

Conclusion: 当前网页代理存在多样性和脆弱性，提出的基准测试方法为大规模评估和理解网页代理失败模式提供了有效途径。

Abstract: LLM web agents now browse and take actions on the open web, yet current agent
evaluations are constrained to sandboxed environments or artificial tasks. We
introduce BrowserArena, a live open-web agent evaluation platform that collects
user-submitted tasks, runs Arena-style head-to-head comparisons, and uses
step-level human feedback to surface failure modes. Collecting and analyzing
step-level annotations on the agent traces, we identify three consistent
failure modes: captcha resolution, pop-up banner removal, and direct navigation
to URLs. By constructing targeted datasets to further study these tasks, we
discover variations in how different language models navigate these failure
modes. We find, for example, that o4-mini deploys a wider variety of strategies
to circumvent captcha resolution than other models and DeepSeek-R1 consistently
misleads users about captcha resolution. Our findings surface both the
diversity and brittleness of current web agents. More broadly, our benchmarking
methodology provides an approach to evaluating and understanding web agent
failure modes at scale.

</details>


### [60] [RefineShot: Rethinking Cinematography Understanding with Foundational Skill Evaluation](https://arxiv.org/abs/2510.02423)
*Hang Wu,Yujun Cai,Haonan Ge,Hongkai Chen,Ming-Hsuan Yang,Yiwei Wang*

Main category: cs.AI

TL;DR: 本文分析了ShotBench基准测试在电影摄影理解任务中的局限性，包括选项设计模糊和模型推理一致性问题，提出了RefineShot基准来改进评估可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有的ShotBench基准测试存在选项设计模糊、模型推理不一致等问题，影响了电影摄影理解任务的公平评估和未来发展。

Method: 通过一致的选项重构系统化改进ShotBench，首次对ShotVL的推理行为进行批判性分析，并引入联合评估任务准确性和核心模型能力的扩展协议。

Result: 开发了RefineShot基准，这是一个经过改进和扩展的基准测试，能够实现更可靠的评估并促进电影摄影理解的未来发展。

Conclusion: RefineShot基准解决了ShotBench的局限性，为电影摄影理解任务提供了更可靠和全面的评估框架。

Abstract: Cinematography understanding refers to the ability to recognize not only the
visual content of a scene but also the cinematic techniques that shape
narrative meaning. This capability is attracting increasing attention, as it
enhances multimodal understanding in real-world applications and underpins
coherent content creation in film and media. As the most comprehensive
benchmark for this task, ShotBench spans a wide range of cinematic concepts and
VQA-style evaluations, with ShotVL achieving state-of-the-art results on it.
However, our analysis reveals that ambiguous option design in ShotBench and
ShotVL's shortcomings in reasoning consistency and instruction adherence
undermine evaluation reliability, limiting fair comparison and hindering future
progress. To overcome these issues, we systematically refine ShotBench through
consistent option restructuring, conduct the first critical analysis of
ShotVL's reasoning behavior, and introduce an extended evaluation protocol that
jointly assesses task accuracy and core model competencies. These efforts lead
to RefineShot, a refined and expanded benchmark that enables more reliable
assessment and fosters future advances in cinematography understanding.

</details>


### [61] [Safe and Efficient In-Context Learning via Risk Control](https://arxiv.org/abs/2510.02480)
*Andrea Wynn,Metod Jazbec,Charith Peris,Rinat Khaziev,Anqi Liu,Daniel Khashabi,Eric Nalisnick*

Main category: cs.AI

TL;DR: 提出一种基于分布无关风险控制的方法，通过动态提前退出机制来防御恶意上下文示例对LLM性能的损害，同时保持对有益示例的性能提升和计算效率增益。


<details>
  <summary>Details</summary>
Motivation: LLMs能够从少量上下文示例中学习新任务，但这种灵活性带来了安全隐患：恶意示例可能在不被察觉的情况下影响模型行为，因此需要内置防御机制。

Method: 定义零样本下的安全行为基准，应用分布无关风险控制，利用动态提前退出预测机制，忽略对不安全输入关注度最高的后续注意力头。

Result: 理论分析和实证结果表明，该方法能有效控制有害上下文示例的风险，同时在有益示例上实现显著的计算效率提升。

Conclusion: 提出的方法能够在防御恶意攻击的同时，保持对有益输入的利用效率，为LLM安全提供了有效的解决方案。

Abstract: Large language models (LLMs) demonstrate a remarkable ability to learn new
tasks from a few in-context examples. However, this flexibility introduces
safety concerns: LLMs can be influenced by incorrect or malicious
demonstrations -- for example, if an adversary tampers with or injects harmful
examples without a human supervisor noticing. This motivates principled designs
in which the system itself includes built-in mechanisms to guard against such
attacks. We propose a novel approach to limit the degree to which harmful
demonstrations can degrade model performance. First, we define a baseline
``safe'' behavior for the model -- the model's performance given no in-context
demonstrations (zero-shot). Next, we apply distribution-free risk control
(DFRC) to control the extent to which in-context samples can decay performance
below zero-shot. We achieve this by leveraging dynamic early exit prediction,
ignoring later attention heads that attend the most to the unsafe inputs.
Finally, we propose modifications to DFRC that allow it to both control risk
for harmful inputs \textit{and} leverage performance and efficiency gains on
helpful inputs. We present both theoretical and empirical results showing that
our approach can effectively control risk for harmful in-context demonstrations
while simultaneously achieving substantial computational efficiency gains with
helpful demonstrations.

</details>


### [62] [Multimodal Function Vectors for Spatial Relations](https://arxiv.org/abs/2510.02528)
*Shuhao Fu,Esther Goldberg,Ying Nian Wu,Hongjing Lu*

Main category: cs.AI

TL;DR: 本文识别了大型多模态模型中负责空间关系表示的注意力头，提取并操作这些"功能向量"可以显著提升关系任务性能，并能通过微调和线性组合解决新的空间关系类比问题。


<details>
  <summary>Details</summary>
Motivation: 虽然大型多模态模型展现出强大的上下文学习能力，但其内部支持任务学习的机制仍不透明，需要深入理解模型如何编码和利用空间关系知识。

Method: 使用因果中介分析识别影响关系预测的注意力头，提取多模态功能向量，并在保持模型参数冻结的情况下对这些向量进行微调，以及线性组合关系特定的功能向量。

Result: 提取的功能向量在推理时提高了零样本准确率，经过少量数据微调后显著优于上下文学习基线，并能通过线性组合解决未经训练的空间关系类比问题。

Conclusion: 大型多模态模型在局部内部结构中编码空间关系知识，这些知识可以被系统提取和优化，有助于理解模型模块化并增强对关系推理的控制。

Abstract: Large Multimodal Models (LMMs) demonstrate impressive in-context learning
abilities from limited multimodal demonstrations, yet the internal mechanisms
supporting such task learning remain opaque. Building on prior work of large
language models, we show that a small subset of attention heads in the
vision-language model OpenFlamingo-4B is responsible for transmitting
representations of spatial relations. The activations of these attention heads,
termed function vectors, can be extracted and manipulated to alter an LMM's
performance on relational tasks. First, using both synthetic and real image
datasets, we apply causal mediation analysis to identify attention heads that
strongly influence relational predictions, and extract multimodal function
vectors that improve zero-shot accuracy at inference time. We further
demonstrate that these multimodal function vectors can be fine-tuned with a
modest amount of training data, while keeping LMM parameters frozen, to
significantly outperform in-context learning baselines. Finally, we show that
relation-specific function vectors can be linearly combined to solve analogy
problems involving novel and untrained spatial relations, highlighting the
strong generalization ability of this approach. Our results show that LMMs
encode spatial relational knowledge within localized internal structures, which
can be systematically extracted and optimized, thereby advancing our
understanding of model modularity and enhancing control over relational
reasoning in LMMs.

</details>


### [63] [Orchestrating Human-AI Teams: The Manager Agent as a Unifying Research Challenge](https://arxiv.org/abs/2510.02557)
*Charlie Masters,Advaith Vellanki,Jiangbo Shangguan,Bart Kultys,Jonathan Gilmore,Alastair Moore,Stefano V. Albrecht*

Main category: cs.AI

TL;DR: 提出自主管理代理作为核心挑战，用于编排动态人机团队协作，将工作流管理形式化为部分可观测随机博弈，并发布MA-Gym评估框架。


<details>
  <summary>Details</summary>
Motivation: 虽然智能AI在自动化单个任务方面取得进展，但管理复杂的多代理工作流仍然是一个具有挑战性的问题。

Method: 将工作流管理形式化为部分可观测随机博弈，提出自主管理代理概念，并开发MA-Gym开源仿真评估框架。

Result: 评估基于GPT-5的管理代理在20个工作流中的表现，发现它们在联合优化目标完成、约束遵守和工作流运行时间方面存在困难。

Conclusion: 工作流管理是一个困难的开放问题，并讨论了自主管理系统的组织和伦理影响。

Abstract: While agentic AI has advanced in automating individual tasks, managing
complex multi-agent workflows remains a challenging problem. This paper
presents a research vision for autonomous agentic systems that orchestrate
collaboration within dynamic human-AI teams. We propose the Autonomous Manager
Agent as a core challenge: an agent that decomposes complex goals into task
graphs, allocates tasks to human and AI workers, monitors progress, adapts to
changing conditions, and maintains transparent stakeholder communication. We
formalize workflow management as a Partially Observable Stochastic Game and
identify four foundational challenges: (1) compositional reasoning for
hierarchical decomposition, (2) multi-objective optimization under shifting
preferences, (3) coordination and planning in ad hoc teams, and (4) governance
and compliance by design. To advance this agenda, we release MA-Gym, an
open-source simulation and evaluation framework for multi-agent workflow
orchestration. Evaluating GPT-5-based Manager Agents across 20 workflows, we
find they struggle to jointly optimize for goal completion, constraint
adherence, and workflow runtime - underscoring workflow management as a
difficult open problem. We conclude with organizational and ethical
implications of autonomous management systems.

</details>


### [64] [Agentic Additive Manufacturing Alloy Discovery](https://arxiv.org/abs/2510.02567)
*Peter Pak,Achuth Chandrasekhar,Amir Barati Farimani*

Main category: cs.AI

TL;DR: 该研究开发了一个基于大语言模型的多智能体系统，用于加速增材制造领域的合金发现过程，通过动态调用热力学计算和工艺模拟工具来实现自主决策。


<details>
  <summary>Details</summary>
Motivation: 增材制造中的合金发现是一个复杂的跨学科挑战，需要材料科学、热力学模拟和实验分析等多领域专业知识。研究旨在利用LLM智能体自动化这一过程，解决传统方法效率低下的问题。

Method: 采用基于大语言模型的多智能体系统，通过模型上下文协议(MCP)调用工具，包括Thermo-Calc性能图计算和熔合不足工艺图生成，智能体能够根据工具调用结果动态调整任务轨迹。

Result: 开发的多智能体系统能够有效解析复杂用户提示，对提出的合金可打印性进行分析，并在实际环境中实现自主决策，展示了在增材制造合金发现中的自动化能力。

Conclusion: LLM驱动的多智能体系统能够显著自动化和加速增材制造中的合金发现任务，展示了该方法的实际效益和应用前景。

Abstract: Agentic systems enable the intelligent use of research tooling, augmenting a
researcher's ability to investigate and propose novel solutions to existing
problems. Within Additive Manufacturing (AM), alloy discovery remains a complex
challenge, often requiring expertise in the various domains of materials
science, thermodynamic simulations, and experimental analysis. Large Language
Model (LLM) enabled agents can facilitate this endeavor by utilizing their
extensive knowledge base to dispatch tool calls via Model Context Protocol
(MCP) to perform actions such as Thermo-Calc property diagram calculations and
lack of fusion process map generation. In addition, the multi-agent system
developed in this work is able to effectively reason through complex user
prompts and provide analysis on the printability of proposed alloys. These
agents can dynamically adjust their task trajectory to the outcomes of tool
call results, effectively enabling autonomous decision-making in practical
environments. This work aims to utilize LLM enabled agents to automate and
accelerate the task of alloy discovery within the field of additive
manufacturing and showcase the benefits of adopting this multi-agent system.

</details>


### [65] [A Benchmark Study of Deep Reinforcement Learning Algorithms for the Container Stowage Planning Problem](https://arxiv.org/abs/2510.02589)
*Yunqi Huang,Nishith Chennakeshava,Alexis Carras,Vladislav Neverov,Wei Liu,Aske Plaat,Yingjie Fan*

Main category: cs.AI

TL;DR: 本文开发了一个包含起重机调度的CSPP Gym环境，评估了5种RL算法在不同复杂度场景下的性能，发现算法选择和问题表述对CSPP至关重要。


<details>
  <summary>Details</summary>
Motivation: CSPP对海运效率至关重要，但现有RL研究缺乏系统性的基准比较，需要填补这一空白。

Method: 开发了包含单智能体和多智能体起重机调度的CSPP Gym环境，评估了DQN、QR-DQN、A2C、PPO和TRPO五种RL算法。

Result: 结果显示随着复杂度增加，不同算法性能差异显著，算法选择和问题表述对CSPP性能有重要影响。

Conclusion: 本文为CSPP提供了多RL方法的基准测试和可重用的Gym环境，为未来海运物流研究和实践部署奠定了基础。

Abstract: Container stowage planning (CSPP) is a critical component of maritime
transportation and terminal operations, directly affecting supply chain
efficiency. Owing to its complexity, CSPP has traditionally relied on human
expertise. While reinforcement learning (RL) has recently been applied to CSPP,
systematic benchmark comparisons across different algorithms remain limited. To
address this gap, we develop a Gym environment that captures the fundamental
features of CSPP and extend it to include crane scheduling in both multi-agent
and single-agent formulations. Within this framework, we evaluate five RL
algorithms: DQN, QR-DQN, A2C, PPO, and TRPO under multiple scenarios of varying
complexity. The results reveal distinct performance gaps with increasing
complexity, underscoring the importance of algorithm choice and problem
formulation for CSPP. Overall, this paper benchmarks multiple RL methods for
CSPP while providing a reusable Gym environment with crane scheduling, thus
offering a foundation for future research and practical deployment in maritime
logistics.

</details>


### [66] [Multimodal Large Language Model Framework for Safe and Interpretable Grid-Integrated EVs](https://arxiv.org/abs/2510.02592)
*Jean Douglas Carvalho,Hugo Kenji,Ahmad Mohammad Saber,Glaucia Melo,Max Mauro Dias Santos,Deepa Kundur*

Main category: cs.AI

TL;DR: 提出基于多模态大语言模型的框架，通过处理视觉感知、定位和车辆遥测数据，为驾驶员生成自然语言警报，提升电动汽车在城市驾驶中的安全性。


<details>
  <summary>Details</summary>
Motivation: 电动汽车与智能电网的集成为交通系统和能源网络带来机遇，但确保驾驶员、车辆与环境之间安全可解释的交互仍是关键挑战。

Method: 结合YOLOv8视觉感知、地理编码定位和CAN总线遥测数据，使用多模态大语言模型处理传感器数据并生成自然语言警报。

Result: 使用真实世界数据验证，框架能有效生成针对关键情境（如靠近行人、自行车和其他车辆）的上下文感知警报。

Conclusion: LLM在电动交通中具有作为辅助工具的潜力，通过可扩展的车队协调、EV负载预测和交通感知能源规划，使交通系统和电网受益。

Abstract: The integration of electric vehicles (EVs) into smart grids presents unique
opportunities to enhance both transportation systems and energy networks.
However, ensuring safe and interpretable interactions between drivers,
vehicles, and the surrounding environment remains a critical challenge. This
paper presents a multi-modal large language model (LLM)-based framework to
process multimodal sensor data - such as object detection, semantic
segmentation, and vehicular telemetry - and generate natural-language alerts
for drivers. The framework is validated using real-world data collected from
instrumented vehicles driving on urban roads, ensuring its applicability to
real-world scenarios. By combining visual perception (YOLOv8), geocoded
positioning, and CAN bus telemetry, the framework bridges raw sensor data and
driver comprehension, enabling safer and more informed decision-making in urban
driving scenarios. Case studies using real data demonstrate the framework's
effectiveness in generating context-aware alerts for critical situations, such
as proximity to pedestrians, cyclists, and other vehicles. This paper
highlights the potential of LLMs as assistive tools in e-mobility, benefiting
both transportation systems and electric networks by enabling scalable fleet
coordination, EV load forecasting, and traffic-aware energy planning.
  Index Terms - Electric vehicles, visual perception, large language models,
YOLOv8, semantic segmentation, CAN bus, prompt engineering, smart grid.

</details>


### [67] [Mitigating Modal Imbalance in Multimodal Reasoning](https://arxiv.org/abs/2510.02608)
*Chen Henry Wu,Neil Kale,Aditi Raghunathan*

Main category: cs.AI

TL;DR: 该论文研究了基础模型在多模态联合推理中的表现，特别是当不同模态出现冲突证据时的处理能力。研究发现模型在跨模态冲突场景下表现较差，主要原因是跨模态注意力不平衡，并提出了一种简单有效的方法来改善这一问题。


<details>
  <summary>Details</summary>
Motivation: 研究基础模型在真实世界任务中处理多模态信息的能力，特别是在需要同时推理多个相互关联的模态时。重点关注当不同模态出现冲突证据时，模型是否能进行联合推理来调和冲突。

Method: 通过设计跨模态冲突实验场景来测试模型性能，分析注意力机制中的不平衡问题，并提出通过显式组合多个模态的训练实例来减少注意力不平衡的方法。

Result: 实验显示，模型在单模态冲突识别中达到90%准确率，但在跨模态冲突中降至3%。发现模型存在严重的跨模态注意力不平衡，某些模态被过度优先考虑。提出的简单方法能显著减少注意力不平衡，并提升多个视觉语言基准测试的性能。

Conclusion: 系统性地处理跨模态上下文对于构建可靠的基础模型至关重要，当前模型在跨模态联合推理方面存在明显不足，需要通过改进训练策略来增强多模态整合能力。

Abstract: Foundation models (FMs) deployed in real-world tasks such as computer-use
agents must integrate diverse modalities. How good are FMs at performing joint
reasoning, simultaneously reasoning over multiple modalities, especially when
the modalities interact and relate to each other to form cross-modal context?
To better understand this problem, we study FMs on cross-modal conflicts:
scenarios where conflicting evidence is presented across modalities. This
allows us to examine whether FMs prioritize one modality over another or reason
jointly to reconcile the conflict. Our experiments reveal that FMs can
recognize conflicts in unimodal contexts, composed of a single modality, 90% of
the time, but the ratio falls as low as 3% when evidence is split across
modalities -- similar observations hold in cross-lingual contexts, composed of
multiple languages. We trace this failure to cross-modal attention imbalance,
showing that FMs exhibit extreme asymmetry in attention scores,
disproportionately prioritizing certain modalities. We show that cross-modal
attention imbalance does not go away by simply scaling up multimodal or
multilingual datasets blindly, since they lack training examples that
explicitly require cross-modal reasoning. We demonstrate that even a simple and
scalable method of explicitly combining multiple modalities within each
training instance significantly reduces attention imbalance. Reduced attention
imbalance directly translates to improved downstream performance on several
vision-language benchmarks. Our findings underscore the importance of
systematically addressing cross-modal contexts to build reliable foundation
models.

</details>


### [68] [On the Role of Temperature Sampling in Test-Time Scaling](https://arxiv.org/abs/2510.02611)
*Yuheng Wu,Azalia Mirhoseini,Thierry Tambe*

Main category: cs.AI

TL;DR: 研究表明，测试时缩放（TTS）在大量样本下会达到性能瓶颈，而温度缩放能显著提升LLM的推理能力，使基础模型达到与强化学习训练模型相当的性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究显示增加推理轨迹数量能提升LLM推理准确率，但作者发现这种提升存在极限，且不同温度能解决不同子集的问题，单一温度缩放只能探索模型部分潜力。

Method: 提出温度维度缩放方法，通过多温度采样扩大LLM的推理边界，并设计了多温度投票机制来降低温度缩放的开销。

Result: 在多个模型和推理基准测试中，温度缩放比单一温度TTS额外提升7.3个百分点，使基础模型性能接近RL训练模型，无需额外后训练。

Conclusion: TTS的潜力被低估，温度缩放是解锁基础模型潜在能力的简单有效方法。

Abstract: Large language models (LLMs) can improve reasoning at inference time through
test-time scaling (TTS), where multiple reasoning traces are generated and the
best one is selected. Prior work shows that increasing the number of samples K
steadily improves accuracy. In this paper, we demonstrate that this trend does
not hold indefinitely: at large K, further scaling yields no gains, and certain
hard questions remain unsolved regardless of the number of traces.
Interestingly, we find that different sampling temperatures solve different
subsets of problems, implying that single-temperature scaling explores only
part of a model's potential. We therefore propose scaling along the temperature
dimension, which enlarges the reasoning boundary of LLMs. Averaged over Qwen3
(0.6B, 1.7B, 4B, 8B) and five representative reasoning benchmarks (AIME
2024/2025, MATH500, LiveCodeBench, Hi-ToM), temperature scaling yields an
additional 7.3 points over single-temperature TTS. Temperature scaling also
enables base models to reach performance comparable to reinforcement learning
(RL)-trained counterparts, without additional post-training. We further provide
a comprehensive analysis of this phenomenon and design a multi-temperature
voting method that reduces the overhead of temperature scaling. Overall, our
findings suggest that TTS is more powerful than previously thought, and that
temperature scaling offers a simple and effective way to unlock the latent
potential of base models.

</details>


### [69] [Geolog-IA: Conversational System for Academic Theses](https://arxiv.org/abs/2510.02653)
*Micaela Fuel Pozo,Andrea Guatumillo Saltos,Yeseña Tipan Llumiquinga,Kelly Lascano Aguirre,Marilyn Castillo Jara,Christian Mejia-Escobar*

Main category: cs.AI

TL;DR: 开发了Geolog-IA对话系统，使用Llama 3.1和Gemini 2.5模型结合RAG架构和SQLite数据库，为厄瓜多尔中央大学地质学论文提供自然问答服务，BLEU评分达0.87。


<details>
  <summary>Details</summary>
Motivation: 解决传统AI系统在专业地质学领域存在的幻觉问题和知识过时问题，为大学师生和行政人员提供准确的地质学论文信息检索服务。

Method: 采用Llama 3.1和Gemini 2.5语言模型，结合检索增强生成(RAG)架构和SQLite数据库，构建基于web的对话系统。

Result: 系统性能评估显示BLEU指标平均达到0.87，表明生成回答具有高度一致性和准确性，提供了直观的web界面。

Conclusion: Geolog-IA在教育、培训和研究中具有重要支持作用，并为其他学科的应用奠定了基础。

Abstract: This study presents the development of Geolog-IA, a novel conversational
system based on artificial intelligence that responds naturally to questions
about geology theses from the Central University of Ecuador. Our proposal uses
the Llama 3.1 and Gemini 2.5 language models, which are complemented by a
Retrieval Augmented Generation (RAG) architecture and an SQLite database. This
strategy allows us to overcome problems such as hallucinations and outdated
knowledge. The evaluation of Geolog-IA's performance with the BLEU metric
reaches an average of 0.87, indicating high consistency and accuracy in the
responses generated. The system offers an intuitive, web-based interface that
facilitates interaction and information retrieval for directors, teachers,
students, and administrative staff at the institution. This tool can be a key
support in education, training, and research and establishes a basis for future
applications in other disciplines.

</details>


### [70] [A Concept of Possibility for Real-World Events](https://arxiv.org/abs/2510.02655)
*Daniel G. Schwartz*

Main category: cs.AI

TL;DR: 提出了一种新的可能性概念作为Zadeh(1978)标准概念的替代方案，专注于现实世界事件的可能性计算，特别适用于规划问题。


<details>
  <summary>Details</summary>
Motivation: 现有可能性理论存在局限性，需要一种专门针对现实世界事件可能性的新概念，能够更好地应用于规划问题并反映人类推理过程。

Method: 将事件视为具有促成其发生的先决条件和阻碍其发生的约束条件，可能性计算基于先决条件成立和约束条件不成立的概率函数。

Result: 开发了一种新的可能性理论框架，能够比较不同规划方案的可行性，并通过车辆路径规划示例验证了其有效性。

Conclusion: 这种新的可能性模型能够准确捕捉人类对规划的正常推理过程，在规划问题中具有应用潜力，为未来应用提供了基础。

Abstract: This paper offers a new concept of {\it possibility} as an alternative to the
now-a-days standard concept originally introduced by L.A. Zadeh in 1978. This
new version was inspired by the original but, formally, has nothing in common
with it other than that they both adopt the {\L}ukasiewicz multivalent
interpretation of the logical connectives. Moreover, rather than seeking to
provide a general notion of possibility, this focuses specifically on the
possibility of a real-world event. An event is viewed as having prerequisites
that enable its occurrence and constraints that may impede its occurrence, and
the possibility of the event is computed as a function of the probabilities
that the prerequisites hold and the constraints do not. This version of
possibility might appropriately be applied to problems of planning. When there
are multiple plans available for achieving a goal, this theory can be used to
determine which plan is most possible, i.e., easiest or most feasible to
complete. It is speculated that this model of reasoning correctly captures
normal human reasoning about plans. The theory is elaborated and an
illustrative example for vehicle route planning is provided. There is also a
suggestion of potential future applications.

</details>


### [71] [AutoMaAS: Self-Evolving Multi-Agent Architecture Search for Large Language Models](https://arxiv.org/abs/2510.02669)
*Bo Ma,Hang Li,ZeHua Hu,XiaoFan Gui,LuYao Liu,Simon Liu*

Main category: cs.AI

TL;DR: AutoMaAS是一个自演化的多智能体架构搜索框架，通过神经架构搜索原理自动发现最优智能体配置，实现性能提升和推理成本降低。


<details>
  <summary>Details</summary>
Motivation: 现有自动化设计方法寻求单一解决方案，无法根据查询复杂度和领域需求自适应分配资源，需要更灵活的多智能体系统设计方法。

Method: 采用动态算子生命周期管理和自动化机器学习技术，包括自动算子生成、融合和消除，动态成本感知优化，在线反馈集成和决策追踪机制。

Result: 在六个基准测试中，AutoMaAS相比最先进方法实现1.0-7.1%的性能提升，同时减少3-5%的推理成本，并在数据集和LLM骨干网络上表现出优越的可迁移性。

Conclusion: AutoMaAS为大型语言模型时代的自动化多智能体系统设计建立了新范式，展示了自演化架构搜索的有效性。

Abstract: Multi-agent systems powered by large language models have demonstrated
remarkable capabilities across diverse domains, yet existing automated design
approaches seek monolithic solutions that fail to adapt resource allocation
based on query complexity and domain requirements. This paper introduces
AutoMaAS, a self-evolving multi-agent architecture search framework that
leverages neural architecture search principles to automatically discover
optimal agent configurations through dynamic operator lifecycle management and
automated machine learning techniques. Our approach incorporates four key
innovations: (1) automatic operator generation, fusion, and elimination based
on performance-cost analysis, (2) dynamic cost-aware optimization with
real-time parameter adjustment, (3) online feedback integration for continuous
architecture refinement, and (4) enhanced interpretability through decision
tracing mechanisms. Extensive experiments across six benchmarks demonstrate
that AutoMaAS achieves 1.0-7.1\% performance improvement while reducing
inference costs by 3-5\% compared to state-of-the-art methods. The framework
shows superior transferability across datasets and LLM backbones, establishing
a new paradigm for automated multi-agent system design in the era of large
language models.

</details>


### [72] [ARMs: Adaptive Red-Teaming Agent against Multimodal Models with Plug-and-Play Attacks](https://arxiv.org/abs/2510.02677)
*Zhaorun Chen,Xun Liu,Mintong Kang,Jiawei Zhang,Minzhou Pan,Shuang Yang,Bo Li*

Main category: cs.AI

TL;DR: ARMs是一个自适应红队代理系统，通过推理增强的多步骤编排自动优化多样化红队策略，有效引发目标VLM的有害输出，在攻击成功率上显著超越基线方法52.1%，并构建了包含30K+实例的大规模多模态安全数据集ARMs-Bench。


<details>
  <summary>Details</summary>
Motivation: 随着视觉语言模型(VLMs)的普及，其多模态接口引入了新的安全漏洞，现有红队方法要么局限于有限的对抗模式，要么依赖人工工程，缺乏对新兴现实世界VLM漏洞的可扩展探索。

Method: 提出11种新颖的多模态攻击策略，涵盖VLM的多样化对抗模式，通过模型上下文协议(MCP)集成17种红队算法，设计分层记忆和epsilon-greedy攻击探索算法来平衡攻击的多样性和有效性。

Result: 在实例级和策略级基准测试中，ARMs实现了最先进的攻击成功率，平均超过基线52.1%，在Claude-4-Sonnet上超过90%。生成的红队实例多样性显著更高，揭示了VLM的新兴漏洞。

Conclusion: 基于ARMs构建的ARMs-Bench数据集通过安全微调显著提高了VLM的鲁棒性，同时保持其通用效用，为改进多模态安全对齐提供了可操作的指导。

Abstract: As vision-language models (VLMs) gain prominence, their multimodal interfaces
also introduce new safety vulnerabilities, making the safety evaluation
challenging and critical. Existing red-teaming efforts are either restricted to
a narrow set of adversarial patterns or depend heavily on manual engineering,
lacking scalable exploration of emerging real-world VLM vulnerabilities. To
bridge this gap, we propose ARMs, an adaptive red-teaming agent that
systematically conducts comprehensive risk assessments for VLMs. Given a target
harmful behavior or risk definition, ARMs automatically optimizes diverse
red-teaming strategies with reasoning-enhanced multi-step orchestration, to
effectively elicit harmful outputs from target VLMs. We propose 11 novel
multimodal attack strategies, covering diverse adversarial patterns of VLMs
(e.g., reasoning hijacking, contextual cloaking), and integrate 17 red-teaming
algorithms into ARMs via model context protocol (MCP). To balance the diversity
and effectiveness of the attack, we design a layered memory with an
epsilon-greedy attack exploration algorithm. Extensive experiments on instance-
and policy-based benchmarks show that ARMs achieves SOTA attack success rates,
exceeding baselines by an average of 52.1% and surpassing 90% on
Claude-4-Sonnet. We show that the diversity of red-teaming instances generated
by ARMs is significantly higher, revealing emerging vulnerabilities in VLMs.
Leveraging ARMs, we construct ARMs-Bench, a large-scale multimodal safety
dataset comprising over 30K red-teaming instances spanning 51 diverse risk
categories, grounded in both real-world multimodal threats and regulatory
risks. Safety fine-tuning with ARMs-Bench substantially improves the robustness
of VLMs while preserving their general utility, providing actionable guidance
to improve multimodal safety alignment against emerging threats.

</details>


### [73] [Automated Constraint Specification for Job Scheduling by Regulating Generative Model with Domain-Specific Representation](https://arxiv.org/abs/2510.02679)
*Yu-Zhe Shi,Qiao Xu,Yanjia Li,Mingchen Liu,Huamin Qu,Lecheng Ruan,Qining Wang*

Main category: cs.AI

TL;DR: 提出了一种基于LLM的约束中心架构，用于自动化生产调度中的约束规范，解决了LLM直接应用时的模糊性和不确定性挑战。


<details>
  <summary>Details</summary>
Motivation: 现代制造系统中，将制造需求转化为正式约束的过程仍然需要大量人工操作，而现有LLM方法存在自然语言模糊性、输出不确定性和领域知识有限的问题。

Method: 设计了一个约束中心的三层层次结构架构，通过领域特定表示确保精度和可靠性，同时开发了自动化生产场景适配算法来定制特定制造配置。

Result: 实验结果表明，该方法成功平衡了LLM的生成能力与制造系统的可靠性要求，在约束规范任务中显著优于纯LLM方法。

Conclusion: 提出的约束中心架构有效解决了LLM在制造约束规范中的可靠性问题，为自动化生产调度提供了可行的解决方案。

Abstract: Advanced Planning and Scheduling (APS) systems have become indispensable for
modern manufacturing operations, enabling optimized resource allocation and
production efficiency in increasingly complex and dynamic environments. While
algorithms for solving abstracted scheduling problems have been extensively
investigated, the critical prerequisite of specifying manufacturing
requirements into formal constraints remains manual and labor-intensive.
Although recent advances of generative models, particularly Large Language
Models (LLMs), show promise in automating constraint specification from
heterogeneous raw manufacturing data, their direct application faces challenges
due to natural language ambiguity, non-deterministic outputs, and limited
domain-specific knowledge. This paper presents a constraint-centric
architecture that regulates LLMs to perform reliable automated constraint
specification for production scheduling. The architecture defines a
hierarchical structural space organized across three levels, implemented
through domain-specific representation to ensure precision and reliability
while maintaining flexibility. Furthermore, an automated production scenario
adaptation algorithm is designed and deployed to efficiently customize the
architecture for specific manufacturing configurations. Experimental results
demonstrate that the proposed approach successfully balances the generative
capabilities of LLMs with the reliability requirements of manufacturing
systems, significantly outperforming pure LLM-based approaches in constraint
specification tasks.

</details>


### [74] [NCV: A Node-Wise Consistency Verification Approach for Low-Cost Structured Error Localization in LLM Reasoning](https://arxiv.org/abs/2510.02816)
*Yulong Zhang,Li Wang,Wei Du,Peilin Li,Yuqin Dai Zhiyuan Zhao,Lingyong Fang,Ziniu Liu,Ru Zhang,Huijia Zhu,Gongshen Liu*

Main category: cs.AI

TL;DR: 提出Node-wise Consistency Verification (NCV)框架，通过节点级一致性检查来验证大语言模型的多步推理，相比传统方法更精确、高效。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以验证大语言模型的多步推理，存在错误定位不精确和token成本高的问题。传统方法要么评估整个推理链导致注意力分散，要么依赖昂贵多重采样。

Method: NCV将验证重构为轻量级的节点级二元一致性检查，将思维链分解为相互连接的验证节点，避免不必要生成长文本。

Result: 在公开数据集上，NCV相比基线方法F1分数提升10%到25%，同时比传统方法（如基于CoT的验证器）减少6倍到58倍的token使用量。

Conclusion: NCV框架提高了可解释性和效率，为可靠的LLM推理验证提供了可扩展解决方案。

Abstract: Verifying multi-step reasoning in large language models is difficult due to
imprecise error localization and high token costs. Existing methods either
assess entire reasoning chains, suffering attention dilution, or rely on
expensive multi-sampling. We introduce Node-wise Consistency Verification
(NCV), a training-free framework that recasts verification as lightweight
binary consistency checks at the node level. By decomposing the chain of
thought into interconnected verification nodes, NCV precisely localizes errors
and avoids unnecessary long-form generation. Experiments demonstrate that our
approach enhances interpretability and efficiency, presenting a scalable
solution for reliable LLM reasoning verification. On public datasets, NCV
achieves a 10\% to 25\% improvement in F1 scores over baselines while utilizing
$6\times$~$58\times$ fewer tokens than traditional methods like CoT-based
verifiers.

</details>


### [75] [Beyond the Final Answer: Evaluating the Reasoning Trajectories of Tool-Augmented Agents](https://arxiv.org/abs/2510.02837)
*Wonjoong Kim,Sangwu Park,Yeonjun In,Sein Kim,Dongha Lee,Chanyoung Park*

Main category: cs.AI

TL;DR: TRACE是一个用于评估工具增强型LLM代理性能的多维度评估框架，通过证据库机制对代理的推理轨迹进行多方面分析，解决了传统答案匹配评估方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有工具增强基准测试主要依赖答案匹配评估，但随着解决用户请求所需步骤增加，需要超越最终答案来评估问题解决轨迹的效率、幻觉和适应性等方面。

Method: 引入TRACE框架，通过证据库积累先前推理步骤的知识，实现代理推理轨迹的多方面分析和评估。构建元评估数据集，在现有基准上添加多样且有缺陷的轨迹，并标注多方面性能分数。

Result: TRACE能够准确评估复杂行为，即使使用小型开源LLM也能实现可扩展且成本效益高的评估。应用该方法评估代理在工具增强任务中的轨迹，提供了先前未报告的观察结果和相应见解。

Conclusion: TRACE框架为工具增强型LLM代理提供了有效的多维度评估方法，能够全面评估代理的推理轨迹性能，超越了传统答案匹配评估的局限性。

Abstract: Although recent tool-augmented benchmarks incorporate complex user requests
and diverse tools, the evaluation methods for most of them remain limited to
answer matching. However, as the number of steps required to resolve a user
request increases, a proper evaluation of an agent's performance must go beyond
the final answer to also assess the problem-solving trajectory, including
previously ignored aspects such as efficiency, hallucination, and adaptivity.
The most straightforward method for evaluating these aspects is to compare an
agent's trajectory with the ground-truth trajectory, but this approach is
fundamentally limited since annotating all valid ground-truth trajectories is
prohibitively expensive. However, a simple LLM-based evaluator struggles to
assess trajectories in detail without ground truth. To effectively evaluate the
agents in this manner, we introduce TRACE, a framework for the
multi-dimensional evaluation of tool-augmented LLM agent performance. By
incorporating an evidence bank, which accumulates knowledge gathered from
preceding reasoning steps, TRACE enables a multi-faceted analysis and
evaluation of an agent's reasoning trajectory effectively. To validate our
framework, we develop a new meta-evaluation dataset by augmenting existing
benchmarks with diverse and flawed trajectories, each labeled with
multi-faceted performance scores. Our results confirm that TRACE accurately
evaluates these complex behaviors in a scalable and cost-effective manner, even
with small open-source LLMs. Furthermore, we apply our method to evaluate the
trajectories that agents produce while solving tool-augmented tasks, presenting
previously unreported observations and their corresponding insights.

</details>


### [76] [Take Goodhart Seriously: Principled Limit on General-Purpose AI Optimization](https://arxiv.org/abs/2510.02840)
*Antoine Maier,Aude Maier,Tom David*

Main category: cs.AI

TL;DR: 论文挑战了机器学习中的目标满足假设(OSA)，指出由于近似、估计和优化误差，模型在实际中无法完全满足指定目标函数，且开发者意图难以完全形式化为目标函数，导致目标错配不可避免。基于数学分析，这些差距在强优化压力下会引发Goodhart定律失效，因此需要对通用AI系统的优化设置原则性限制。


<details>
  <summary>Details</summary>
Motivation: 检验机器学习中普遍但未经检验的假设——训练产生的模型确实满足其指定目标函数(OSA)，并探讨OSA失效的现实影响和风险。

Method: 采用学习范式无关的框架，分析近似误差、估计误差和优化误差如何导致系统性偏离预期目标，结合数学结果证明这些差距在强优化下会引发Goodhart定律失效模式。

Result: 证明OSA在现实条件下必然失效，目标错配不可避免，且在没有数学表征的情况下，这些差距与Goodhart定律失效模式无法区分，Goodhart临界点无法事先确定。

Conclusion: 需要对通用AI系统的优化设置原则性限制，否则持续优化将导致可预测且不可逆的失控风险。

Abstract: A common but rarely examined assumption in machine learning is that training
yields models that actually satisfy their specified objective function. We call
this the Objective Satisfaction Assumption (OSA). Although deviations from OSA
are acknowledged, their implications are overlooked. We argue, in a
learning-paradigm-agnostic framework, that OSA fails in realistic conditions:
approximation, estimation, and optimization errors guarantee systematic
deviations from the intended objective, regardless of the quality of its
specification. Beyond these technical limitations, perfectly capturing and
translating the developer's intent, such as alignment with human preferences,
into a formal objective is practically impossible, making misspecification
inevitable. Building on recent mathematical results, absent a mathematical
characterization of these gaps, they are indistinguishable from those that
collapse into Goodhart's law failure modes under strong optimization pressure.
Because the Goodhart breaking point cannot be located ex ante, a principled
limit on the optimization of General-Purpose AI systems is necessary. Absent
such a limit, continued optimization is liable to push systems into predictable
and irreversible loss of control.

</details>


### [77] [Reward Model Routing in Alignment](https://arxiv.org/abs/2510.02850)
*Xinle Wu,Yao Lu*

Main category: cs.AI

TL;DR: 提出BayesianRouter框架，通过结合离线RM强度学习和在线贝叶斯选择，解决现有奖励模型路由方法中的冷启动和探索不足问题，在多个基准测试中优于单个RM、RM集成和现有路由方法。


<details>
  <summary>Details</summary>
Motivation: 当前RLHF/RLAIF流程依赖单一奖励模型，限制了对齐质量并存在过拟合风险。现有RM路由方法存在冷启动和探索不足的问题。

Method: 混合路由框架：离线阶段训练多任务路由器估计每个RM的可靠性；在线阶段使用贝叶斯Thompson采样路由器进行每查询RM选择，用离线嵌入作为高斯先验初始化权重向量，并通过在线奖励自适应更新后验分布。

Result: 在指令跟随（AlpacaEval-2、Arena-Hard、MT-Bench）和推理（GSM8K、MMLU）基准测试中，BayesianRouter始终优于单个RM、RM集成和现有路由方法。

Conclusion: BayesianRouter通过结合离线和在线学习，有效解决了RM路由中的冷启动和探索问题，显著提升了语言模型对齐的质量。

Abstract: Reinforcement learning from human or AI feedback (RLHF / RLAIF) has become
the standard paradigm for aligning large language models (LLMs). However, most
pipelines rely on a single reward model (RM), limiting alignment quality and
risking overfitting. Recent work explores RM routing--dynamically selecting an
RM from a candidate pool to exploit complementary strengths while maintaining
$O(1)$ RM calls--but existing methods suffer from cold-start and insufficient
exploration. We propose BayesianRouter, a hybrid routing framework that
combines offline RM strengths learning with online Bayesian selection. In the
offline stage, a multi-task router is trained on preference data to estimate
per-RM reliability. In the online stage, a Bayesian Thompson sampling router
performs per-query RM selection, initializing RM-specific weight vectors with
offline embeddings as Gaussian priors and adaptively updating their posteriors
with online rewards to adapt to the evolving policy distribution. Extensive
experiments on instruction-following (AlpacaEval-2, Arena-Hard, MT-Bench) and
reasoning (GSM8K, MMLU) benchmarks show that BayesianRouter consistently
outperforms individual RMs, RM ensembling, and existing routing methods.

</details>


### [78] [Consolidating Reinforcement Learning for Multimodal Discrete Diffusion Models](https://arxiv.org/abs/2510.02880)
*Tianren Ma,Mu Zhang,Yibing Wang,Qixiang Ye*

Main category: cs.AI

TL;DR: 提出了MaskGRPO方法，这是首个能够实现离散扩散模型中可扩展多模态强化学习的可行方法，通过有效的重采样和模态特定适配来解决离散扩散模型优化难题。


<details>
  <summary>Details</summary>
Motivation: 离散扩散模型的非自回归特性使得重要性采样难以处理且rollout复杂，这给强化学习方法如GRPO带来了挑战，需要找到可行的优化方案。

Method: 首先澄清离散扩散模型的理论基础，构建能够捕捉有价值token波动的重要性估计器；然后为视觉序列精心设计rollout方法，产生多样化的补全和可靠的优化梯度。

Result: 在数学推理、编码和视觉生成基准测试中，MaskGRPO带来了更稳定和高效的更新，导致更强的推理性能和更好的生成质量。

Conclusion: 本研究确立了MaskGRPO作为一种系统性的策略优化方法，并且是离散化视觉扩散的首个实用途径。

Abstract: Optimizing discrete diffusion model (DDM) with rewards remains a challenge:
the non-autoregressive paradigm makes importance sampling intractable and
rollout complex, puzzling reinforcement learning methods such as Group Relative
Policy Optimization (GRPO). In this study, we introduce MaskGRPO, the first
viable approach to enable scalable multimodal reinforcement learning in
discrete diffusion with effective importance sampling and modality-specific
adaptations. To this end, we first clarify the theoretical foundation for DDMs,
which facilitates building an importance estimator that captures valuable token
fluctuation for gradient updates. We then delicately tailored the rollout
method for visual sequences, which yields diverse completions and reliable
optimization gradients. Upon math reasoning, coding, and visual generation
benchmarks, MaskGRPO brings more stable and efficient updates, leading to
stronger reasoning performance and better generation quality. This study
establishes MaskGRPO as a systematic policy optimization approach and the first
practical way for discretized visual diffusion.

</details>


### [79] [Onto-Epistemological Analysis of AI Explanations](https://arxiv.org/abs/2510.02996)
*Martina Mattioli,Eike Petersen,Aasa Feragen,Marcello Pelillo,Siavash A. Bigdeli*

Main category: cs.AI

TL;DR: 本文分析了可解释AI方法中的本体论和认识论假设，指出这些假设对AI解释的有效性和解释有重要影响，并讨论了如何为不同应用领域选择和适配适当的XAI方法。


<details>
  <summary>Details</summary>
Motivation: 当前主流的深度学习方法是黑盒系统，缺乏对其推理过程的解释，这限制了其可信度和采用。可解释AI方法旨在通过提供模型决策过程的解释来克服这一挑战，但这些方法通常由技术背景的工程师和科学家开发，并融入了他们对不同可解释机制的存在、有效性和解释效用的假设。

Method: 研究可解释性方法应用于AI系统时的本体论和认识论假设，即我们关于解释存在性和获取解释知识能力的假设。分析显示，XAI方法的微小技术变化可能对应着关于解释的基本假设的重要差异。

Result: 分析揭示了忽略底层本体-认识论范式在选择XAI方法时的风险，并展示了如何为不同应用领域选择和适配适当的XAI方法。

Conclusion: XAI方法中融入的假设并非无害，对AI解释在不同领域的有效性和解释具有重要后果。在选择XAI方法时需要考虑其底层本体-认识论范式，以确保解释的有效性和适用性。

Abstract: Artificial intelligence (AI) is being applied in almost every field. At the
same time, the currently dominant deep learning methods are fundamentally
black-box systems that lack explanations for their inferences, significantly
limiting their trustworthiness and adoption. Explainable AI (XAI) methods aim
to overcome this challenge by providing explanations of the models' decision
process. Such methods are often proposed and developed by engineers and
scientists with a predominantly technical background and incorporate their
assumptions about the existence, validity, and explanatory utility of different
conceivable explanatory mechanisms. However, the basic concept of an
explanation -- what it is, whether we can know it, whether it is absolute or
relative -- is far from trivial and has been the subject of deep philosophical
debate for millennia. As we point out here, the assumptions incorporated into
different XAI methods are not harmless and have important consequences for the
validity and interpretation of AI explanations in different domains. We
investigate ontological and epistemological assumptions in explainability
methods when they are applied to AI systems, meaning the assumptions we make
about the existence of explanations and our ability to gain knowledge about
those explanations. Our analysis shows how seemingly small technical changes to
an XAI method may correspond to important differences in the underlying
assumptions about explanations. We furthermore highlight the risks of ignoring
the underlying onto-epistemological paradigm when choosing an XAI method for a
given application, and we discuss how to select and adapt appropriate XAI
methods for different domains of application.

</details>


### [80] [From Facts to Foils: Designing and Evaluating Counterfactual Explanations for Smart Environments](https://arxiv.org/abs/2510.03078)
*Anna Trapp,Mersedeh Sadeghi,Andreas Vogelsang*

Main category: cs.AI

TL;DR: 提出了首个针对基于规则的智能环境的反事实解释形式化和实现，通过用户研究发现因果解释在时间压力下更受欢迎，而反事实解释在解决问题时更受青睐。


<details>
  <summary>Details</summary>
Motivation: 虽然反事实解释是可解释AI中的强大工具，但在基于规则的智能环境领域缺乏成熟的生成方法，需要专门的形式化和实现。

Method: 开发了一个插件来扩展现有的智能环境解释引擎，并进行了用户研究(N=17)来比较反事实解释与传统因果解释。

Result: 用户偏好高度依赖情境：因果解释因其语言简洁性和在时间压力下的优势而受青睐，反事实解释则因其可操作内容而在解决问题时更受欢迎。

Conclusion: 为智能环境提供了一种新的解释类型实用框架，并提供了经验证据来指导何时选择每种解释类型最有效。

Abstract: Explainability is increasingly seen as an essential feature of rule-based
smart environments. While counterfactual explanations, which describe what
could have been done differently to achieve a desired outcome, are a powerful
tool in eXplainable AI (XAI), no established methods exist for generating them
in these rule-based domains. In this paper, we present the first formalization
and implementation of counterfactual explanations tailored to this domain. It
is implemented as a plugin that extends an existing explanation engine for
smart environments. We conducted a user study (N=17) to evaluate our generated
counterfactuals against traditional causal explanations. The results show that
user preference is highly contextual: causal explanations are favored for their
linguistic simplicity and in time-pressured situations, while counterfactuals
are preferred for their actionable content, particularly when a user wants to
resolve a problem. Our work contributes a practical framework for a new type of
explanation in smart environments and provides empirical evidence to guide the
choice of when each explanation type is most effective.

</details>


### [81] [A Study of Rule Omission in Raven's Progressive Matrices](https://arxiv.org/abs/2510.03127)
*Binze Li*

Main category: cs.AI

TL;DR: 该研究通过故意在训练中省略部分结构规则，评估了现代AI系统在Raven渐进矩阵任务上的泛化能力，发现transformer模型在面对新规则时性能显著下降。


<details>
  <summary>Details</summary>
Motivation: 探究AI系统在Raven渐进矩阵任务中的表现是否真正反映了推理能力，还是仅仅依赖统计捷径，特别是在训练数据不完整的情况下。

Method: 在Impartial-RAVEN数据集上评估序列到序列transformer模型和基于视觉的架构（如CoPINet和Dual-Contrast Network），故意在训练中省略多个结构规则。

Result: transformer模型在熟悉规则上表现良好，但在面对新颖或省略规则时准确率急剧下降；token级准确率与完整答案准确率之间存在显著差距。

Conclusion: 当前深度学习方法在抽象推理方面存在根本性局限，需要超越模式识别、发展更稳健的抽象推理架构。

Abstract: Analogical reasoning lies at the core of human cognition and remains a
fundamental challenge for artificial intelligence. Raven's Progressive Matrices
(RPM) serve as a widely used benchmark to assess abstract reasoning by
requiring the inference of underlying structural rules. While many vision-based
and language-based models have achieved success on RPM tasks, it remains
unclear whether their performance reflects genuine reasoning ability or
reliance on statistical shortcuts. This study investigates the generalization
capacity of modern AI systems under conditions of incomplete training by
deliberately omitting several structural rules during training. Both
sequence-to-sequence transformer models and vision-based architectures such as
CoPINet and the Dual-Contrast Network are evaluated on the Impartial-RAVEN
(I-RAVEN) dataset. Experiments reveal that although transformers demonstrate
strong performance on familiar rules, their accuracy declines sharply when
faced with novel or omitted rules. Moreover, the gap between token-level
accuracy and complete answer accuracy highlights fundamental limitations in
current approaches. These findings provide new insights into the reasoning
mechanisms underlying deep learning models and underscore the need for
architectures that move beyond pattern recognition toward robust abstract
reasoning.

</details>


### [82] [Improving Cooperation in Collaborative Embodied AI](https://arxiv.org/abs/2510.03153)
*Hima Jacob Leven Suprabha,Laxmi Nag Laxminarayan Nagesh,Ajith Nair,Alvin Reuben Amal Selvaster,Ayan Khan,Raghuram Damarla,Sanju Hannah Samuel,Sreenithi Saravana Perumal,Titouan Puech,Venkataramireddy Marella,Vishal Sonar,Alessandro Suglia,Oliver Lemon*

Main category: cs.AI

TL;DR: 本文研究了不同提示方法在增强多智能体协作行为方面的有效性，通过优化CoELA框架中的LLM提示策略，使Gemma3系统效率提升22%，并集成了语音交互功能。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在多智能体系统中的集成，需要探索有效的提示方法来增强AI代理的协作推理和决策能力。

Method: 增强CoELA框架，系统实验不同LLM和提示工程策略，识别最优组合以最大化协作性能，并集成语音功能实现语音交互。

Result: 最佳组合使Gemma3系统效率相比原始CoELA系统提升22%，语音集成提供了更吸引人的用户界面。

Conclusion: 提示优化能显著提升协作智能体性能，语音集成增强了系统的交互性和演示效果。

Abstract: The integration of Large Language Models (LLMs) into multiagent systems has
opened new possibilities for collaborative reasoning and cooperation with AI
agents. This paper explores different prompting methods and evaluates their
effectiveness in enhancing agent collaborative behaviour and decision-making.
We enhance CoELA, a framework designed for building Collaborative Embodied
Agents that leverage LLMs for multi-agent communication, reasoning, and task
coordination in shared virtual spaces. Through systematic experimentation, we
examine different LLMs and prompt engineering strategies to identify optimised
combinations that maximise collaboration performance. Furthermore, we extend
our research by integrating speech capabilities, enabling seamless
collaborative voice-based interactions. Our findings highlight the
effectiveness of prompt optimisation in enhancing collaborative agent
performance; for example, our best combination improved the efficiency of the
system running with Gemma3 by 22% compared to the original CoELA system. In
addition, the speech integration provides a more engaging user interface for
iterative system development and demonstrations.

</details>


### [83] [CoDA: Agentic Systems for Collaborative Data Visualization](https://arxiv.org/abs/2510.03194)
*Zichen Chen,Jiefeng Chen,Sercan Ö. Arik,Misha Sra,Tomas Pfister,Jinsung Yoon*

Main category: cs.AI

TL;DR: CoDA是一个多智能体系统，通过专门的LLM智能体进行元数据分析、任务规划、代码生成和自我反思，显著提升了从自然语言查询自动生成可视化的能力。


<details>
  <summary>Details</summary>
Motivation: 当前系统在处理包含多个文件的复杂数据集和迭代优化时表现不佳，现有方法过于简化任务，无法有效管理数据复杂性、代码错误或最终可视化质量。

Method: 将挑战重新定义为协作多智能体问题，引入CoDA系统，采用专门化的LLM智能体进行元数据分析、任务规划、代码生成和自我反思，通过元数据分析绕过token限制，质量驱动的优化确保鲁棒性。

Result: 广泛评估显示CoDA在总体得分上取得显著提升，比竞争基线方法高出41.5%。

Conclusion: 可视化自动化的未来不在于孤立的代码生成，而在于集成、协作的智能体工作流程。

Abstract: Deep research has revolutionized data analysis, yet data scientists still
devote substantial time to manually crafting visualizations, highlighting the
need for robust automation from natural language queries. However, current
systems struggle with complex datasets containing multiple files and iterative
refinement. Existing approaches, including simple single- or multi-agent
systems, often oversimplify the task, focusing on initial query parsing while
failing to robustly manage data complexity, code errors, or final visualization
quality. In this paper, we reframe this challenge as a collaborative
multi-agent problem. We introduce CoDA, a multi-agent system that employs
specialized LLM agents for metadata analysis, task planning, code generation,
and self-reflection. We formalize this pipeline, demonstrating how
metadata-focused analysis bypasses token limits and quality-driven refinement
ensures robustness. Extensive evaluations show CoDA achieves substantial gains
in the overall score, outperforming competitive baselines by up to 41.5%. This
work demonstrates that the future of visualization automation lies not in
isolated code generation but in integrated, collaborative agentic workflows.

</details>


### [84] [Coevolutionary Continuous Discrete Diffusion: Make Your Diffusion Language Model a Latent Reasoner](https://arxiv.org/abs/2510.03206)
*Cai Zhou,Chenxiao Yang,Yi Hu,Chenyu Wang,Chubin Zhang,Muhan Zhang,Lester Mackey,Tommi Jaakkola,Stephen Bates,Dinghuai Zhang*

Main category: cs.AI

TL;DR: 本文提出CCDD方法，在连续表示空间和离散标记空间的联合空间上定义多模态扩散过程，解决了连续扩散模型在语言建模中表达能力与训练性能之间的矛盾。


<details>
  <summary>Details</summary>
Motivation: 虽然连续扩散模型理论上比离散扩散模型和循环变换器具有更强的表达能力，但在实际应用中表现不佳，主要原因是连续表示到离散标记空间的解码困难。

Method: 提出CCDD方法，在连续表示空间和离散标记空间的联合空间上定义多模态扩散过程，使用单一模型同时在这两个空间中进行去噪。

Result: CCDD在真实世界语言建模任务中展现出强大的实证性能，结合了连续空间的丰富语义表达和离散标记的良好训练性。

Conclusion: CCDD成功解决了连续扩散模型在语言建模中的训练难题，通过联合连续和离散空间实现了表达能力与训练性能的平衡。

Abstract: Diffusion language models, especially masked discrete diffusion models, have
achieved great success recently. While there are some theoretical and primary
empirical results showing the advantages of latent reasoning with looped
transformers or continuous chain-of-thoughts, continuous diffusion models
typically underperform their discrete counterparts. In this paper, we argue
that diffusion language models do not necessarily need to be in the discrete
space. In particular, we prove that continuous diffusion models have stronger
expressivity than discrete diffusions and looped transformers. We attribute the
contradiction between the theoretical expressiveness and empirical performance
to their practical trainability: while continuous diffusion provides
intermediate supervision that looped transformers lack, they introduce
additional difficulty decoding tokens into the discrete token space from the
continuous representation space. We therefore propose Coevolutionary Continuous
Discrete Diffusion (CCDD), which defines a joint multimodal diffusion process
on the union of a continuous representation space and a discrete token space,
leveraging a single model to simultaneously denoise in the joint space. By
combining two modalities, CCDD is expressive with rich semantics in the latent
space, as well as good trainability and sample quality with the help of
explicit discrete tokens. We also propose effective architectures and advanced
training/sampling techniques for CCDD, which reveals strong empirical
performance in extensive language modeling experiments on real-world tasks.

</details>
