<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 15]
- [cs.AI](#cs.AI) [Total: 43]
- [cs.SE](#cs.SE) [Total: 20]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [DECEIVE-AFC: Adversarial Claim Attacks against Search-Enabled LLM-based Fact-Checking Systems](https://arxiv.org/abs/2602.02569)
*Haoran Ou,Kangjie Chen,Gelei Deng,Hangcheng Liu,Jie Zhang,Tianwei Zhang,Kwok-Yan Lam*

Main category: cs.CR

TL;DR: DECEIVE-AFC：针对搜索增强型LLM事实核查系统的对抗性攻击框架，通过干扰搜索行为和证据检索来降低验证准确性


<details>
  <summary>Details</summary>
Motivation: 虽然搜索增强型大语言模型（LLM）事实核查系统在动态检索外部证据方面表现出强大潜力，但其对抗攻击的鲁棒性尚未得到充分理解。本研究旨在探索在现实输入威胁模型下，针对此类系统的对抗性声明攻击。

Method: 提出DECEIVE-AFC框架，这是一种基于智能体的对抗攻击框架，整合了新颖的声明级攻击策略和对抗性声明有效性评估原则。该框架系统性地探索对抗攻击轨迹，在不依赖证据源或模型内部访问的情况下，干扰搜索行为、证据检索和基于LLM的推理。

Result: 在基准数据集和真实系统上的广泛评估表明，该攻击显著降低了验证性能，将准确率从78.7%降至53.7%，并且在跨系统可迁移性方面显著优于现有的基于声明的攻击基线。

Conclusion: DECEIVE-AFC框架揭示了搜索增强型LLM事实核查系统在面对对抗性声明攻击时的脆弱性，强调了提高此类系统鲁棒性的重要性。

Abstract: Fact-checking systems with search-enabled large language models (LLMs) have shown strong potential for verifying claims by dynamically retrieving external evidence. However, the robustness of such systems against adversarial attack remains insufficiently understood. In this work, we study adversarial claim attacks against search-enabled LLM-based fact-checking systems under a realistic input-only threat model. We propose DECEIVE-AFC, an agent-based adversarial attack framework that integrates novel claim-level attack strategies and adversarial claim validity evaluation principles. DECEIVE-AFC systematically explores adversarial attack trajectories that disrupt search behavior, evidence retrieval, and LLM-based reasoning without relying on access to evidence sources or model internals. Extensive evaluations on benchmark datasets and real-world systems demonstrate that our attacks substantially degrade verification performance, reducing accuracy from 78.7% to 53.7%, and significantly outperform existing claim-based attack baselines with strong cross-system transferability.

</details>


### [2] [Position: 3D Gaussian Splatting Watermarking Should Be Scenario-Driven and Threat-Model Explicit](https://arxiv.org/abs/2602.02602)
*Yangfan Deng,Anirudh Nakra,Min Wu*

Main category: cs.CR

TL;DR: 该论文主张为3D资产水印制定明确的安全目标和威胁模型，借鉴数字音视频资产保护经验，提出基于场景的框架来评估现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着3D内容获取和创建的快速发展，特别是3D高斯泼溅技术的出现，3D资产的知识产权保护需求日益增长。由于3D参数化的显式和可编辑特性，未经授权的使用和传播变得更加容易，需要有效的保护机制。

Method: 采用基于场景的制定方法，通过安全模型形式化对抗能力。构建了一个参考框架来组织现有方法，并阐明特定设计选择如何映射到相应的对抗假设。在该框架内，还分析了一个传统的扩频嵌入方案。

Result: 提出了一个系统化的安全规范框架，能够评估现有3D资产水印方法的有效性，并识别传统扩频嵌入方案的优势、局限性和重要权衡。

Conclusion: 该工作旨在促进3D资产的有效知识产权保护，强调需要明确的安全目标和现实的威胁模型，为未来3D水印技术的发展提供指导框架。

Abstract: 3D content acquisition and creation are expanding rapidly in the new era of machine learning and AI. 3D Gaussian Splatting (3DGS) has become a promising high-fidelity and real-time representation for 3D content. Similar to the initial wave of digital audio-visual content at the turn of the millennium, the demand for intellectual property protection is also increasing, since explicit and editable 3D parameterization makes unauthorized use and dissemination easier. In this position paper, we argue that effective progress in watermarking 3D assets requires articulated security objectives and realistic threat models, incorporating the lessons learned from digital audio-visual asset protection over the past decades. To address this gap in security specification and evaluation, we advocate a scenario-driven formulation, in which adversarial capabilities are formalized through a security model. Based on this formulation, we construct a reference framework that organizes existing methods and clarifies how specific design choices map to corresponding adversarial assumptions. Within this framework, we also examine a legacy spread-spectrum embedding scheme, characterizing its advantages and limitations and highlighting the important trade-offs it entails. Overall, this work aims to foster effective intellectual property protection for 3D assets.

</details>


### [3] [TinyGuard:A lightweight Byzantine Defense for Resource-Constrained Federated Learning via Statistical Update Fingerprints](https://arxiv.org/abs/2602.02615)
*Ali Mahdavi,Santa Aghapour,Azadeh Zamanifar,Amirfarhad Farhadi*

Main category: cs.CR

TL;DR: TinyGuard：一种轻量级拜占庭防御机制，通过统计更新指纹增强FedAvg算法，在低维指纹空间中检测恶意客户端，计算复杂度为O(n)，适用于大规模联邦系统。


<details>
  <summary>Details</summary>
Motivation: 现有拜占庭鲁棒聚合机制通常依赖全维度梯度比较或成对距离计算，计算开销大，限制了在大规模和资源受限的联邦系统中的适用性。

Method: TinyGuard从高维梯度中提取紧凑的统计指纹，捕捉客户端更新的关键行为属性（包括范数统计、分层比率、稀疏性度量和低阶矩），在低维指纹空间中测量统计偏差来识别拜占庭客户端。

Result: 在MNIST、Fashion-MNIST、ViT-Lite和ViT-Small等数据集上的实验表明，TinyGuard在良性设置下保持FedAvg收敛，在多种拜占庭攻击场景下达到95%准确率。对抗自适应白盒攻击时，攻击者无法同时逃避检测和实现有效投毒。

Conclusion: TinyGuard提供了一种轻量级、架构无关的拜占庭防御框架，特别适用于传统防御机制不切实际的联邦基础模型微调场景。

Abstract: Existing Byzantine robust aggregation mechanisms typically rely on fulldimensional gradi ent comparisons or pairwise distance computations, resulting in computational overhead that limits applicability in large scale and resource constrained federated systems. This paper proposes TinyGuard, a lightweight Byzantine defense that augments the standard FedAvg algorithm via statistical update f ingerprinting. Instead of operating directly on high-dimensional gradients, TinyGuard extracts compact statistical fingerprints cap turing key behavioral properties of client updates, including norm statistics, layer-wise ratios, sparsity measures, and low-order mo ments. Byzantine clients are identified by measuring robust sta tistical deviations in this low-dimensional fingerprint space with nd complexity, without modifying the underlying optimization procedure. Extensive experiments on MNIST, Fashion-MNIST, ViT-Lite, and ViT-Small with LoRA adapters demonstrate that TinyGuard pre serves FedAvg convergence in benign settings and achieves up to 95 percent accuracy under multiple Byzantine attack scenarios, including sign-flipping, scaling, noise injection, and label poisoning. Against adaptive white-box adversaries, Pareto frontier analysis across four orders of magnitude confirms that attackers cannot simultaneously evade detection and achieve effective poisoning, features we term statistical handcuffs. Ablation studies validate stable detection precision 0.8 across varying client counts (50-150), threshold parameters and extreme data heterogeneity . The proposed framework is architecture-agnostic and well-suited for federated fine-tuning of foundation models where traditional Byzantine defenses become impractical

</details>


### [4] [Trustworthy Blockchain-based Federated Learning for Electronic Health Records: Securing Participant Identity with Decentralized Identifiers and Verifiable Credentials](https://arxiv.org/abs/2602.02629)
*Rodrigo Tertulino,Ricardo Almeida,Laercio Alencar*

Main category: cs.CR

TL;DR: 提出基于区块链和自主身份标准的可信联邦学习框架，通过密码学身份验证而非行为模式来确保医疗数据协作的安全性，有效防御Sybil攻击并保持临床实用性。


<details>
  <summary>Details</summary>
Motivation: 医疗数字化产生了大量电子健康记录，但GDPR和HIPAA等隐私法规导致数据孤岛。联邦学习虽然能实现不共享原始数据的协作训练，但仍易受投毒攻击和Sybil攻击。现有基于区块链的方法主要依赖概率性声誉系统，缺乏强大的密码学身份验证。

Method: 提出可信区块链联邦学习框架，整合自主身份标准，利用去中心化标识符和可验证凭证，确保只有经过认证的医疗实体才能参与全局模型训练。

Result: 使用MIMIC-IV数据集评估，框架成功抵御100%的Sybil攻击，保持稳健的预测性能（AUC=0.954，召回率=0.890），计算开销极低（<0.12%），100轮跨机构训练的总运营成本约18美元。

Conclusion: 该框架通过将信任锚定在密码学身份验证而非行为模式上，显著降低安全风险，同时保持临床实用性，为跨机构医疗数据协作提供了安全、可扩展且经济可行的生态系统。

Abstract: The digitization of healthcare has generated massive volumes of Electronic Health Records (EHRs), offering unprecedented opportunities for training Artificial Intelligence (AI) models. However, stringent privacy regulations such as GDPR and HIPAA have created data silos that prevent centralized training. Federated Learning (FL) has emerged as a promising solution that enables collaborative model training without sharing raw patient data. Despite its potential, FL remains vulnerable to poisoning and Sybil attacks, in which malicious participants corrupt the global model or infiltrate the network using fake identities. While recent approaches integrate Blockchain technology for auditability, they predominantly rely on probabilistic reputation systems rather than robust cryptographic identity verification. This paper proposes a Trustworthy Blockchain-based Federated Learning (TBFL) framework integrating Self-Sovereign Identity (SSI) standards. By leveraging Decentralized Identifiers (DIDs) and Verifiable Credentials (VCs), our architecture ensures only authenticated healthcare entities contribute to the global model. Through comprehensive evaluation using the MIMIC-IV dataset, we demonstrate that anchoring trust in cryptographic identity verification rather than behavioral patterns significantly mitigates security risks while maintaining clinical utility. Our results show the framework successfully neutralizes 100% of Sybil attacks, achieves robust predictive performance (AUC = 0.954, Recall = 0.890), and introduces negligible computational overhead (<0.12%). The approach provides a secure, scalable, and economically viable ecosystem for inter-institutional health data collaboration, with total operational costs of approximately $18 for 100 training rounds across multiple institutions.

</details>


### [5] [Benchmarking Large Language Models for Zero-shot and Few-shot Phishing URL Detection](https://arxiv.org/abs/2602.02641)
*Najmul Hasan,Prashanth BusiReddyGari*

Main category: cs.CR

TL;DR: 论文提出使用大语言模型进行零样本和少样本学习来检测AI生成的钓鱼URL，创建了统一的评估基准，发现少样本提示能提升多个LLM的性能。


<details>
  <summary>Details</summary>
Motivation: 传统URL设计缺乏安全机制，AI时代钓鱼URL变得更加复杂和难以检测，传统检测工具失效，而标注数据生成速度跟不上威胁演变速度，需要零样本/少样本学习方案。

Method: 使用大语言模型进行零样本和少样本学习，建立统一的提示框架，在平衡数据集上评估多个LLM，使用准确率、精确率、召回率、F1分数、AUROC和AUPRC等指标量化性能。

Result: 少样本提示能提升多个大语言模型的性能，论文提供了详细的性能、泛化能力和模型效果分析，揭示了操作权衡。

Conclusion: 在AI驱动的威胁环境中，大语言模型的零样本和少样本学习为钓鱼URL检测提供了及时且适应性强的解决方案，少样本提示能有效提升检测性能。

Abstract: The Uniform Resource Locator (URL), introduced in a connectivity-first era to define access and locate resources, remains historically limited, lacking future-proof mechanisms for security, trust, or resilience against fraud and abuse, despite the introduction of reactive protections like HTTPS during the cybersecurity era. In the current AI-first threatscape, deceptive URLs have reached unprecedented sophistication due to the widespread use of generative AI by cybercriminals and the AI-vs-AI arms race to produce context-aware phishing websites and URLs that are virtually indistinguishable to both users and traditional detection tools. Although AI-generated phishing accounted for a small fraction of filter-bypassing attacks in 2024, phishing volume has escalated over 4,000% since 2022, with nearly 50% more attacks evading detection. At the rate the threatscape is escalating, and phishing tactics are emerging faster than labeled data can be produced, zero-shot and few-shot learning with large language models (LLMs) offers a timely and adaptable solution, enabling generalization with minimal supervision. Given the critical importance of phishing URL detection in large-scale cybersecurity defense systems, we present a comprehensive benchmark of LLMs under a unified zero-shot and few-shot prompting framework and reveal operational trade-offs. Our evaluation uses a balanced dataset with consistent prompts, offering detailed analysis of performance, generalization, and model efficacy, quantified by accuracy, precision, recall, F1 score, AUROC, and AUPRC, to reflect both classification quality and practical utility in threat detection settings. We conclude few-shot prompting improves performance across multiple LLMs.

</details>


### [6] [On the Feasibility of Hybrid Homomorphic Encryption for Intelligent Transportation Systems](https://arxiv.org/abs/2602.02717)
*Kyle Yates,Abdullah Al Mamun,Mashrur Chowdhury*

Main category: cs.CR

TL;DR: HHE在ITS中显著降低密文大小和通信开销，相比传统HE更适合时延敏感的交通系统


<details>
  <summary>Details</summary>
Motivation: ITS应用需要强隐私保护，但传统同态加密存在密文膨胀和通信开销大的问题，限制了其在时间关键型交通系统中的适用性

Method: 开发集成HHE的ITS应用理论模型，基于参数评估Rubato HHE方案，估算实际ITS工作负载下的密文大小和通信开销

Result: HHE相比传统HE实现了数量级的密文大小减少，同时保持密码安全性，显著提高了在时延受限ITS通信中的实用性

Conclusion: 混合同态加密为ITS中的隐私保护数据处理提供了更实用的解决方案，特别适合时间关键的交通应用场景

Abstract: Many Intelligent Transportation Systems (ITS) applications require strong privacy guarantees for both users and their data. Homomorphic encryption (HE) enables computation directly on encrypted messages and thus offers a compelling approach to privacy-preserving data processing in ITS. However, practical HE schemes incur substantial ciphertext expansion and communication overhead, which limits their suitability for time-critical transportation systems. Hybrid homomorphic encryption (HHE) addresses this challenge by combining a homomorphic encryption scheme with a symmetric cipher, enabling efficient encrypted computation while dramatically reducing communication cost. In this paper, we develop theoretical models of representative ITS applications that integrate HHE to protect sensitive vehicular data. We then perform a parameter-based evaluation of the HHE scheme Rubato to estimate ciphertext sizes and communication overhead under realistic ITS workloads. Our results show that HHE achieves orders-of-magnitude reductions in ciphertext size compared with conventional HE while maintaining cryptographic security, making it significantly more practical for latency-constrained ITS communication.

</details>


### [7] [Composition for Pufferfish Privacy](https://arxiv.org/abs/2602.02718)
*Jiamu Bai,Guanlin He,Xin Gu,Daniel Kifer,Kiwan Maeng*

Main category: cs.CR

TL;DR: 本文研究了Pufferfish隐私定义在组合性方面的缺陷，提出了确保线性组合的必要充分条件，并开发了将差分隐私算法转换为可组合Pufferfish算法的框架。


<details>
  <summary>Details</summary>
Motivation: Pufferfish等基于推断/后验的隐私定义在处理相关数据时具有吸引力的隐私语义，但由于缺乏组合性而很少在实践中使用。现有算法可能在单次运行时无泄漏，但在多次运行时可能泄露整个数据集。

Method: 提出了确保Pufferfish机制线性组合的必要充分条件，这些条件表现为差分隐私风格的不等式。引入了(a,b)-影响曲线的概念，将现有差分隐私算法转换为可组合的Pufferfish算法。

Result: 证明了通过添加差分隐私式条件可以实现Pufferfish的线性组合。开发了转换框架，使许多现有差分隐私算法能够转换为可组合的Pufferfish算法。为马尔可夫链设计的算法显著优于先前工作。

Conclusion: 要实现Pufferfish对相关数据的可解释语义和组合性优势，需要采用差分隐私机制。通过(a,b)-影响曲线框架，可以将差分隐私算法转换为可组合的Pufferfish算法，解决了组合性问题。

Abstract: When creating public data products out of confidential datasets, inferential/posterior-based privacy definitions, such as Pufferfish, provide compelling privacy semantics for data with correlations. However, such privacy definitions are rarely used in practice because they do not always compose. For example, it is possible to design algorithms for these privacy definitions that have no leakage when run once but reveal the entire dataset when run more than once. We prove necessary and sufficient conditions that must be added to ensure linear composition for Pufferfish mechanisms, hence avoiding such privacy collapse. These extra conditions turn out to be differential privacy-style inequalities, indicating that achieving both the interpretable semantics of Pufferfish for correlated data and composition benefits requires adopting differentially private mechanisms to Pufferfish. We show that such translation is possible through a concept called the $(a,b)$-influence curve, and many existing differentially private algorithms can be translated with our framework into a composable Pufferfish algorithm. We illustrate the benefit of our new framework by designing composable Pufferfish algorithms for Markov chains that significantly outperform prior work.

</details>


### [8] [DF-LoGiT: Data-Free Logic-Gated Backdoor Attacks in Vision Transformers](https://arxiv.org/abs/2602.03040)
*Xiaozuo Shen,Yifei Cai,Rui Ning,Chunsheng Xin,Hongyi Wu*

Main category: cs.CR

TL;DR: DF-LoGiT是一种针对Vision Transformers的无数据后门攻击方法，通过直接权重编辑实现逻辑门控组合触发器，无需训练数据或额外模型组件。


<details>
  <summary>Details</summary>
Motivation: 随着Vision Transformers的广泛使用，第三方模型中心存在供应链风险，攻击者可能在发布的检查点中植入后门。现有ViT后门攻击主要依赖中毒数据训练，而无数据攻击通常需要合成数据微调或额外模型组件。

Method: DF-LoGiT利用ViT原生多头架构实现逻辑门控组合触发器，通过直接权重编辑实现真正的无数据后门攻击，无需训练数据或额外模型组件。

Result: DF-LoGiT实现了接近100%的攻击成功率，对良性准确率影响极小，并能抵抗经典和ViT特定的防御方法。

Conclusion: DF-LoGiT是一种有效、隐蔽且鲁棒的无数据后门攻击方法，为ViT模型的安全性研究提供了新的视角。

Abstract: The widespread adoption of Vision Transformers (ViTs) elevates supply-chain risk on third-party model hubs, where an adversary can implant backdoors into released checkpoints. Existing ViT backdoor attacks largely rely on poisoned-data training, while prior data-free attempts typically require synthetic-data fine-tuning or extra model components. This paper introduces Data-Free Logic-Gated Backdoor Attacks (DF-LoGiT), a truly data-free backdoor attack on ViTs via direct weight editing. DF-LoGiT exploits ViT's native multi-head architecture to realize a logic-gated compositional trigger, enabling a stealthy and effective backdoor. We validate its effectiveness through theoretical analysis and extensive experiments, showing that DF-LoGiT achieves near-100% attack success with negligible degradation in benign accuracy and remains robust against representative classical and ViT-specific defenses.

</details>


### [9] [LogicScan: An LLM-driven Framework for Detecting Business Logic Vulnerabilities in Smart Contracts](https://arxiv.org/abs/2602.03271)
*Jiaqi Gao,Zijian Zhang,Yuqiang Sun,Ye Liu,Chengwei Liu,Han Liu,Yi Li,Yang Liu*

Main category: cs.CR

TL;DR: LogicScan是一个用于检测智能合约业务逻辑漏洞的自动化对比审计框架，通过从成熟链上协议挖掘业务不变量作为参考约束，结合业务规范语言和噪声感知逻辑聚合，显著优于现有工具。


<details>
  <summary>Details</summary>
Motivation: 业务逻辑漏洞已成为智能合约中最具破坏性但最不被理解的漏洞类别。现有静态分析技术难以捕捉这种高层逻辑，而基于大语言模型的方法因幻觉和有限验证导致输出不稳定、准确率低。

Method: 提出LogicScan框架，核心洞察是从成熟、广泛部署的链上协议中挖掘隐含的经过良好测试和共识驱动的业务不变量，将其作为参考约束来审计目标合约。引入业务规范语言(BSL)将不同实现模式规范化为结构化、可验证的逻辑表示，结合噪声感知逻辑聚合与对比审计来识别缺失或弱执行的不变量，同时减轻LLM引起的误报。

Result: 在三个真实数据集(DeFiHacks、Web3Bugs和top-200审计合约)上评估，LogicScan达到85.2%的F1分数，显著优于最先进工具，同时在生产级合约上保持低误报率。额外实验表明LogicScan在不同LLM上保持一致性性能，具有成本效益，其误报抑制机制显著提高鲁棒性。

Conclusion: LogicScan通过从成熟链上协议挖掘业务不变量作为参考约束，结合业务规范语言和对比审计，有效解决了智能合约业务逻辑漏洞检测的挑战，在准确性和实用性方面显著优于现有方法。

Abstract: Business logic vulnerabilities have become one of the most damaging yet least understood classes of smart contract vulnerabilities. Unlike traditional bugs such as reentrancy or arithmetic errors, these vulnerabilities arise from missing or incorrectly enforced business invariants and are tightly coupled with protocol semantics. Existing static analysis techniques struggle to capture such high-level logic, while recent large language model based approaches often suffer from unstable outputs and low accuracy due to hallucination and limited verification.
  In this paper, we propose LogicScan, an automated contrastive auditing framework for detecting business logic vulnerabilities in smart contracts. The key insight behind LogicScan is that mature, widely deployed on-chain protocols implicitly encode well-tested and consensus-driven business invariants. LogicScan systematically mines these invariants from large-scale on-chain contracts and reuses them as reference constraints to audit target contracts. To achieve this, LogicScan introduces a Business Specification Language (BSL) to normalize diverse implementation patterns into structured, verifiable logic representations. It further combines noise-aware logic aggregation with contrastive auditing to identify missing or weakly enforced invariants while mitigating LLM-induced false positives.
  We evaluate LogicScan on three real-world datasets, including DeFiHacks, Web3Bugs, and a set of top-200 audited contracts. The results show that LogicScan achieves an F1 score of 85.2%, significantly outperforming state-of-the-art tools while maintaining a low false-positive rate on production-grade contracts. Additional experiments demonstrate that LogicScan maintains consistent performance across different LLMs and is cost-effective, and that its false-positive suppression mechanisms substantially improve robustness.

</details>


### [10] [GuardReasoner-Omni: A Reasoning-based Multi-modal Guardrail for Text, Image, and Video](https://arxiv.org/abs/2602.03328)
*Zhenhao Zhu,Yue Liu,Yanpei Guo,Wenjie Qu,Cancan Chen,Yufei He,Yibo Li,Yulin Chen,Tianyi Wu,Huiying Xu,Xinzhong Zhu,Jiaheng Zhang*

Main category: cs.CR

TL;DR: GuardReasoner-Omni是一个基于推理的多模态护栏模型，用于文本、图像和视频内容审核，通过两阶段训练实现深度推理能力，在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 需要开发一个能够处理文本、图像和视频多种模态内容的安全护栏模型，通过深度推理来提高内容审核的准确性和可靠性。

Method: 1. 构建包含148k样本的多模态训练语料库；2. 采用两阶段训练范式：首先进行监督微调（SFT）以冷启动模型，使其具备显式推理能力和结构遵循性；然后进行强化学习（RL），引入错误驱动探索奖励机制，激励模型对困难样本进行更深层次的推理。

Result: GuardReasoner-Omni在多个护栏基准测试中超越了现有最先进的基线模型，特别是2B参数版本显著领先第二名5.3%的F1分数，展示了卓越的性能。

Conclusion: GuardReasoner-Omni通过结合多模态数据处理和深度推理训练策略，成功开发了一个高效的内容安全护栏系统，为多模态内容审核提供了有效的解决方案。

Abstract: We present GuardReasoner-Omni, a reasoning-based guardrail model designed to moderate text, image, and video data. First, we construct a comprehensive training corpus comprising 148k samples spanning these three modalities. Our training pipeline follows a two-stage paradigm to incentivize the model to deliberate before making decisions: (1) conducting SFT to cold-start the model with explicit reasoning capabilities and structural adherence; and (2) performing RL, incorporating an error-driven exploration reward to incentivize deeper reasoning on hard samples. We release a suite of models scaled at 2B and 4B parameters. Extensive experiments demonstrate that GuardReasoner-Omni achieves superior performance compared to existing state-of-the-art baselines across various guardrail benchmarks. Notably, GuardReasoner-Omni (2B) significantly surpasses the runner-up by 5.3% F1 score.

</details>


### [11] [Origin Lens: A Privacy-First Mobile Framework for Cryptographic Image Provenance and AI Detection](https://arxiv.org/abs/2602.03423)
*Alexander Loth,Dominique Conceicao Rosario,Peter Ebinger,Martin Kappes,Marc-Oliver Pahl*

Main category: cs.CR

TL;DR: Origin Lens是一个隐私优先的移动框架，通过分层验证架构应对视觉虚假信息，在设备本地执行加密图像溯源验证和AI检测，提供分级置信度指标。


<details>
  <summary>Details</summary>
Motivation: 生成式AI的普及对信息完整性保障提出了挑战，需要将模型治理与终端用户验证连接起来的系统，以应对视觉虚假信息问题。

Method: 采用Rust/Flutter混合架构的隐私优先移动框架，在设备本地执行加密图像溯源验证和AI检测，整合加密溯源、生成模型指纹和可选检索增强验证等多重信号。

Result: 开发了一个分层验证架构，能够在消费点为用户提供分级置信度指标，与服务器端检测系统不同，所有验证都在设备本地完成。

Conclusion: Origin Lens框架符合欧盟AI法案和数字服务法案等监管要求，在验证基础设施中发挥补充平台级机制的作用，为信息完整性保障提供了实用解决方案。

Abstract: The proliferation of generative AI poses challenges for information integrity assurance, requiring systems that connect model governance with end-user verification. We present Origin Lens, a privacy-first mobile framework that targets visual disinformation through a layered verification architecture. Unlike server-side detection systems, Origin Lens performs cryptographic image provenance verification and AI detection locally on the device via a Rust/Flutter hybrid architecture. Our system integrates multiple signals - including cryptographic provenance, generative model fingerprints, and optional retrieval-augmented verification - to provide users with graded confidence indicators at the point of consumption. We discuss the framework's alignment with regulatory requirements (EU AI Act, DSA) and its role in verification infrastructure that complements platform-level mechanisms.

</details>


### [12] [Reading Between the Code Lines: On the Use of Self-Admitted Technical Debt for Security Analysis](https://arxiv.org/abs/2602.03470)
*Nicolás E. Díaz Ferreyra,Moritz Mock,Max Kretschmann,Barbara Russo,Mojtaba Shahin,Mansooreh Zahedi,Riccardo Scandariato*

Main category: cs.CR

TL;DR: 研究探讨安全相关技术债务（SATD）如何补充静态分析工具（SAT）的安全分析，通过实证分析发现SATD能覆盖SAT难以检测的漏洞类型，开发者常结合两者来理解安全弱点。


<details>
  <summary>Details</summary>
Motivation: 静态分析工具在安全工程中至关重要，但存在高误报率和漏洞覆盖不全的局限性。同时，开发者在代码注释中记录的安全相关技术债务（SATD）被认为是丰富的安全信息来源，但尚不清楚SATD如何在SAT辅助的安全分析中被利用。

Method: 采用混合方法：1）使用三种最先进的静态分析工具分析SATD标注的漏洞数据集；2）对72名安全从业者进行在线调查。

Result: 所有SAT工具共标记了135个安全相关SATD实例中的114个，覆盖24个不同的CWE标识符。手动映射SATD注释发现了33个独特的CWE类型，其中6个对应SAT通常忽视或难以检测的类别（如竞争条件）。调查显示开发者经常结合SAT输出和SATD洞察来更好地理解安全弱点的影响和根本原因，并确定合适的修复方案。

Conclusion: SATD编码的信息可以成为SAT驱动安全分析的有意义补充，同时有助于克服SAT的一些实际缺点。安全相关技术债务能够补充静态分析工具的输出，帮助弥补其已知局限性。

Abstract: Static Analysis Tools (SATs) are central to security engineering activities, as they enable early identification of code weaknesses without requiring execution. However, their effectiveness is often limited by high false-positive rates and incomplete coverage of vulnerability classes. At the same time, developers frequently document security-related shortcuts and compromises as Self-Admitted Technical Debt (SATD) in software artifacts, such as code comments. While prior work has recognized SATD as a rich source of security information, it remains unclear whether -and in what ways- it is utilized during SAT-aided security analysis. OBJECTIVE: This work investigates the extent to which security-related SATD complements the output produced by SATs and helps bridge some of their well-known limitations. METHOD: We followed a mixed-methods approach consisting of (i) the analysis of a SATD-annotated vulnerability dataset using three state-of-the-art SATs and (ii) an online survey with 72 security practitioners. RESULTS: The combined use of all SATs flagged 114 of the 135 security-related SATD instances, spanning 24 distinct Common Weakness Enumeration (CWE) identifiers. A manual mapping of the SATD comments revealed 33 unique CWE types, 6 of which correspond to categories that SATs commonly overlook or struggle to detect (e.g., race conditions). Survey responses further suggest that developers frequently pair SAT outputs with SATD insights to better understand the impact and root causes of security weaknesses and to identify suitable fixes. IMPLICATIONS: Our findings show that such SATD-encoded information can be a meaningful complement to SAT-driven security analysis, while helping to overcome some of SATs' practical shortcomings.

</details>


### [13] [Detecting and Explaining Malware Family Evolution Using Rule-Based Drift Analysis](https://arxiv.org/abs/2602.03489)
*Olha Jurečková,Martin Jureček*

Main category: cs.CR

TL;DR: 提出一种可解释的概念漂移检测方法，通过规则分类器生成可读描述，比较规则集相似性来检测和量化恶意软件演化


<details>
  <summary>Details</summary>
Motivation: 恶意软件持续演化以逃避检测，引入概念漂移问题，导致静态机器学习模型效果下降。理解和解释这种漂移对于维护鲁棒可信的恶意软件检测器至关重要

Method: 使用基于规则的分类器为同一恶意软件家族的原始和演化样本生成人类可读描述，通过相似性函数比较规则集来检测和量化概念漂移，识别具体变化的特征和特征值

Result: 实验结果表明，该方法不仅能准确检测漂移，还能提供关于演化恶意软件家族行为的可操作见解，支持检测和威胁分析

Conclusion: 提出的可解释概念漂移检测方法为恶意软件演化提供了清晰的解释，有助于维护有效的恶意软件检测系统

Abstract: Malware detection and classification into families are critical tasks in cybersecurity, complicated by the continual evolution of malware to evade detection. This evolution introduces concept drift, in which the statistical properties of malware features change over time, reducing the effectiveness of static machine learning models. Understanding and explaining this drift is essential for maintaining robust and trustworthy malware detectors. In this paper, we propose an interpretable approach to concept drift detection. Our method uses a rule-based classifier to generate human-readable descriptions of both original and evolved malware samples belonging to the same malware family. By comparing the resulting rule sets using a similarity function, we can detect and quantify concept drift. Crucially, this comparison also identifies the specific features and feature values that have changed, providing clear explanations of how malware has evolved to bypass detection. Experimental results demonstrate that the proposed method not only accurately detects drift but also provides actionable insights into the behavior of evolving malware families, supporting both detection and threat analysis.

</details>


### [14] [Can Developers rely on LLMs for Secure IaC Development?](https://arxiv.org/abs/2602.03648)
*Ehsan Firouzi,Shardul Bhatt,Mohammad Ghafari*

Main category: cs.CR

TL;DR: 研究评估GPT-4o和Gemini 2.0 Flash在基础设施即代码安全开发中的表现，发现提示策略对安全漏洞检测效果显著，但安全代码生成能力有限


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型在基础设施即代码安全开发中的实际能力，了解不同提示策略对安全漏洞检测和安全代码生成效果的影响

Method: 使用两种提示策略（通用提示和引导提示）测试GPT-4o和Gemini 2.0 Flash在Stack Overflow数据集（小型简化代码片段）和GitHub仓库（完整真实项目脚本）中的安全漏洞检测能力，以及通过89个易受攻击的合成场景测试安全代码生成能力

Result: 安全漏洞检测：在Stack Overflow数据集上，通用提示能检测至少71%的安全漏洞，引导提示提升至78%；在GitHub仓库中，通用提示效果较差（超过一半漏洞未检测），引导提示能检测至少67%的漏洞。安全代码生成：仅7%的生成脚本是安全的，明确要求生成安全代码后，GPT安全输出率提升至17%，Gemini变化不大（8%）

Conclusion: 当前大型语言模型在协助开发者进行安全基础设施即代码开发方面能力有限，需要进一步研究改进，特别是安全代码生成能力需要显著提升

Abstract: We investigated the capabilities of GPT-4o and Gemini 2.0 Flash for secure Infrastructure as Code (IaC) development. For security smell detection, on the Stack Overflow dataset, which primarily contains small, simplified code snippets, the models detected at least 71% of security smells when prompted to analyze code from a security perspective (general prompt). With a guided prompt (adding clear, step-by-step instructions), this increased to 78%.In GitHub repositories, which contain complete, real-world project scripts, a general prompt was less effective, leaving more than half of the smells undetected. However, with the guided prompt, the models uncovered at least 67% of the smells. For secure code generation, we prompted LLMs with 89 vulnerable synthetic scenarios and observed that only 7% of the generated scripts were secure. Adding an explicit instruction to generate secure code increased GPT secure output rate to 17%, while Gemini changed little (8%). These results highlight the need for further research to improve LLMs' capabilities in assisting developers with secure IaC development.

</details>


### [15] [mopri - An Analysis Framework for Unveiling Privacy Violations in Mobile Apps](https://arxiv.org/abs/2602.03671)
*Cornell Ziepel,Stephan Escher,Sebastian Rehms,Stefan Köpsell*

Main category: cs.CR

TL;DR: mopri是一个用于分析移动应用隐私合规性的概念框架，通过静态和动态分析方法集成多种工具，帮助用户和监管机构验证数据保护合规性。


<details>
  <summary>Details</summary>
Motivation: 移动应用的普及带来了参与可能性与用户隐私、数字自由之间的冲突。虽然GDPR等法规保护用户信息自决权，但检查移动应用隐私合规性仍然困难，需要为终端用户和执法机构提供验证和执行数据保护合规性的工具。

Method: 提出mopri概念框架，采用模块化流水线设计，集成静态和动态分析方法。原型系统能够有效提取权限和追踪库，采用鲁棒的动态流量记录和解密方法，并包含结果增强和报告功能。

Result: 开发的原型系统展示了整体模块化隐私分析方法的可行性，能够有效提取权限和追踪库，进行动态流量记录和解密，并通过结果增强提高分析结果的清晰度和可用性。

Conclusion: mopri框架为移动应用隐私分析提供了一个全面、可适应、以用户为中心的方法基础，强调了持续适应移动应用生态系统不断变化挑战的重要性，有助于验证和执行数据保护合规性。

Abstract: Everyday services of society increasingly rely on mobile applications, resulting in a conflicting situation between the possibility of participation on the one side and user privacy and digital freedom on the other. In order to protect users' rights to informational self-determination, regulatory approaches for the collection and processing of personal data have been developed, such as the EU's GDPR. However, inspecting the compliance of mobile apps with privacy regulations remains difficult. Thus, in order to enable end users and enforcement bodies to verify and enforce data protection compliance, we propose mopri, a conceptual framework designed for analyzing the behavior of mobile apps through a comprehensive, adaptable, and user-centered approach. Recognizing the gaps in existing frameworks, mopri serves as a foundation for integrating various analysis tools into a streamlined, modular pipeline that employs static and dynamic analysis methods. Building on this concept, a prototype has been developed which effectively extracts permissions and tracking libraries while employing robust methods for dynamic traffic recording and decryption. Additionally, it incorporates result enrichment and reporting features that enhance the clarity and usability of the analysis outcomes. The prototype showcases the feasibility of a holistic and modular approach to privacy analysis, emphasizing the importance of continuous adaptation to the evolving challenges presented by the mobile app ecosystem.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [16] [CreditAudit: 2$^\text{nd}$ Dimension for LLM Evaluation and Selection](https://arxiv.org/abs/2602.02515)
*Yiliang Song,Hongjun An,Jiangong Xiao,Haofei Zhao,Jiawei Shao,Xuelong Li*

Main category: cs.AI

TL;DR: 论文提出CreditAudit框架，通过评估模型在不同系统提示模板下的表现波动性，提供平均能力和稳定性风险的双维度评估，将波动性转换为AAA到BBB的信用等级，支持实际部署中的模型选择。


<details>
  <summary>Details</summary>
Motivation: 当前公开基准测试的分数趋同且差异微小，但无法反映实际用户体验，因为系统提示、输出协议和交互模式会不断迭代变化，在智能体多步骤流程中，小的协议变化可能引发不成比例的故障，导致从业者难以确定部署哪个模型。

Method: 提出CreditAudit框架：1）使用一系列语义对齐且非对抗性的系统提示模板在多个基准上评估模型；2）报告平均能力（跨场景平均性能）和场景诱导波动σ（稳定性风险信号）；3）通过跨模型分位数将波动性映射为可解释的信用等级（AAA到BBB），并提供诊断以减轻模板难度漂移。

Result: 在GPQA、TruthfulQA和MMLU Pro上的控制实验显示，具有相似平均能力的模型可能表现出显著不同的波动性，稳定性风险在智能体或高故障成本场景中可以推翻优先级决策。

Conclusion: CreditAudit通过提供基于二维评估和信用等级的语言，支持特定场景下的模型选择、分层部署，以及更规范的测试和监控资源分配，为实际应用提供更客观和可信的模型评估。

Abstract: Leaderboard scores on public benchmarks have been steadily rising and converging, with many frontier language models now separated by only marginal differences. However, these scores often fail to match users' day to day experience, because system prompts, output protocols, and interaction modes evolve under routine iteration, and in agentic multi step pipelines small protocol shifts can trigger disproportionate failures, leaving practitioners uncertain about which model to deploy. We propose CreditAudit, a deployment oriented credit audit framework that evaluates models under a family of semantically aligned and non adversarial system prompt templates across multiple benchmarks, reporting mean ability as average performance across scenarios and scenario induced fluctuation sigma as a stability risk signal, and further mapping volatility into interpretable credit grades from AAA to BBB via cross model quantiles with diagnostics that mitigate template difficulty drift. Controlled experiments on GPQA, TruthfulQA, and MMLU Pro show that models with similar mean ability can exhibit substantially different fluctuation, and stability risk can overturn prioritization decisions in agentic or high failure cost regimes. By providing a 2D and grade based language for regime specific selection, CreditAudit supports tiered deployment and more disciplined allocation of testing and monitoring effort, enabling more objective and trustworthy model evaluation for real world use.

</details>


### [17] [Uncertainty and Fairness Awareness in LLM-Based Recommendation Systems](https://arxiv.org/abs/2602.02582)
*Chandan Kumar Sah,Xiaoli Lian,Li Zhang,Tony Xu,Syed Shazaib Shah*

Main category: cs.AI

TL;DR: 该论文研究了大型语言模型在零样本推荐中的不确定性和公平性问题，提出了包含不确定性评估和人格感知公平性的新评测方法，揭示了Gemini 1.5 Flash在敏感属性上的系统性不公平现象。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然能够提供强大的零样本推荐能力，但其预测不确定性和内在偏见威胁着推荐的可靠性和公平性。当前缺乏系统评估LLM推荐中不确定性和公平性的方法，需要建立更全面的评测框架。

Method: 1. 构建包含电影和音乐两个领域的基准数据集，标注了8个人口统计学属性（31个分类值）
2. 引入包含熵度量的预测不确定性评估方法
3. 设计包含拼写错误和多语言输入的提示扰动实验
4. 将人格感知公平性整合到RecLLM评估流程中
5. 提出不确定性感知的RecLLM评估方法学

Result: 1. Google DeepMind的Gemini 1.5 Flash对某些敏感属性表现出系统性不公平，相似性差距指标SNSR为0.1363，SNSV为0.0507
2. 这些不公平现象在提示扰动（如拼写错误和多语言输入）下仍然持续存在
3. 揭示了人格特征与偏见模式之间的关联
4. 暴露了个性化推荐与群体公平性之间的权衡关系

Conclusion: 该研究为更安全、更可解释的RecLLM奠定了基础，提出了包含不确定性评估和人格感知公平性的新评测框架，推动了LLM推荐系统的公平性和可信度研究，并激励未来在多模型基准和自适应校准方面的工作。

Abstract: Large language models (LLMs) enable powerful zero-shot recommendations by leveraging broad contextual knowledge, yet predictive uncertainty and embedded biases threaten reliability and fairness. This paper studies how uncertainty and fairness evaluations affect the accuracy, consistency, and trustworthiness of LLM-generated recommendations. We introduce a benchmark of curated metrics and a dataset annotated for eight demographic attributes (31 categorical values) across two domains: movies and music. Through in-depth case studies, we quantify predictive uncertainty (via entropy) and demonstrate that Google DeepMind's Gemini 1.5 Flash exhibits systematic unfairness for certain sensitive attributes; measured similarity-based gaps are SNSR at 0.1363 and SNSV at 0.0507. These disparities persist under prompt perturbations such as typographical errors and multilingual inputs. We further integrate personality-aware fairness into the RecLLM evaluation pipeline to reveal personality-linked bias patterns and expose trade-offs between personalization and group fairness. We propose a novel uncertainty-aware evaluation methodology for RecLLMs, present empirical insights from deep uncertainty case studies, and introduce a personality profile-informed fairness benchmark that advances explainability and equity in LLM recommendations. Together, these contributions establish a foundation for safer, more interpretable RecLLMs and motivate future work on multi-model benchmarks and adaptive calibration for trustworthy deployment.

</details>


### [18] [A Positive Case for Faithfulness: LLM Self-Explanations Help Predict Model Behavior](https://arxiv.org/abs/2602.02639)
*Harry Mayne,Justin Singh Kang,Dewi Gould,Kannan Ramchandran,Adam Mahdi,Noah Y. Siegel*

Main category: cs.AI

TL;DR: 本文提出了Normalized Simulatability Gain (NSG)作为衡量LLM自我解释忠实度的新指标，发现自我解释能显著提升模型行为预测能力(11-37% NSG)，但仍有5-15%的自我解释具有严重误导性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM自我解释的忠实度评估方法存在局限，主要依赖对抗性提示或检测推理错误，忽视了解释的预测价值。需要一种更全面、可扩展的指标来评估解释是否真正反映了模型的真实推理过程。

Method: 提出Normalized Simulatability Gain (NSG)指标，基于"忠实解释应能让观察者学习模型的决策标准，从而更好预测其在相关输入上的行为"的理念。在18个前沿专有和开源模型上评估，使用来自健康、商业和伦理领域的7,000个反事实数据。

Result: 自我解释显著提升模型行为预测能力(11-37% NSG)；自我解释比外部模型生成的解释提供更多预测信息，即使外部模型更强；所有模型中5-15%的自我解释具有严重误导性。

Conclusion: 尽管存在缺陷，自我解释确实编码了有助于预测模型行为的信息，显示出自我知识带来的优势是外部解释方法无法复制的。这为自我解释提供了积极案例，同时揭示了需要改进的领域。

Abstract: LLM self-explanations are often presented as a promising tool for AI oversight, yet their faithfulness to the model's true reasoning process is poorly understood. Existing faithfulness metrics have critical limitations, typically relying on identifying unfaithfulness via adversarial prompting or detecting reasoning errors. These methods overlook the predictive value of explanations. We introduce Normalized Simulatability Gain (NSG), a general and scalable metric based on the idea that a faithful explanation should allow an observer to learn a model's decision-making criteria, and thus better predict its behavior on related inputs. We evaluate 18 frontier proprietary and open-weight models, e.g., Gemini 3, GPT-5.2, and Claude 4.5, on 7,000 counterfactuals from popular datasets covering health, business, and ethics. We find self-explanations substantially improve prediction of model behavior (11-37% NSG). Self-explanations also provide more predictive information than explanations generated by external models, even when those models are stronger. This implies an advantage from self-knowledge that external explanation methods cannot replicate. Our approach also reveals that, across models, 5-15% of self-explanations are egregiously misleading. Despite their imperfections, we show a positive case for self-explanations: they encode information that helps predict model behavior.

</details>


### [19] [Dynamic Mix Precision Routing for Efficient Multi-step LLM Interaction](https://arxiv.org/abs/2602.02711)
*Yuanzhe Li,Jianing Deng,Jingtong Hu,Tianlong Chen,Song Wang,Huanrui Yang*

Main category: cs.AI

TL;DR: 本文提出了一种动态混合精度路由框架，在长时程决策任务中自适应选择高精度和低精度LLM，以降低推理成本同时保持任务成功率。


<details>
  <summary>Details</summary>
Motivation: 虽然大型语言模型在长时程决策任务中表现出色，但使用大型LLM进行多步交互会产生高昂的推理成本。传统观点认为更高的任务成功率需要使用更大更强的LLM模型，但作者观察到不同交互步骤对模型精度的敏感性存在差异，这为优化成本-性能权衡提供了机会。

Method: 提出动态混合精度路由框架，在决策过程中自适应选择高精度和低精度LLM。路由器通过两阶段管道训练：1) 基于KL散度的监督学习识别精度敏感步骤；2) 使用组相对策略优化(GRPO)进一步提高任务成功率。

Result: 在ALFWorld上的实验表明，该方法在准确率-成本权衡方面相比单精度基线和启发式路由方法有显著提升。

Conclusion: 通过动态混合精度路由框架，可以在保持任务成功率的同时显著降低长时程决策任务的推理成本，为LLM在实际应用中的部署提供了有效的成本优化方案。

Abstract: Large language models (LLM) achieve strong performance in long-horizon decision-making tasks through multi-step interaction and reasoning at test time. While practitioners commonly believe a higher task success rate necessitates the use of a larger and stronger LLM model, multi-step interaction with a large LLM incurs prohibitive inference cost. To address this problem, we explore the use of low-precision quantized LLM in the long-horizon decision-making process. Based on the observation of diverse sensitivities among interaction steps, we propose a dynamic mix-precision routing framework that adaptively selects between high-precision and low-precision LLMs at each decision step. The router is trained via a two-stage pipeline, consisting of KL-divergence-based supervised learning that identifies precision-sensitive steps, followed by Group-Relative Policy Optimization (GRPO) to further improve task success rates. Experiments on ALFWorld demonstrate that our approach achieves a great improvement on accuracy-cost trade-off over single-precision baselines and heuristic routing methods.

</details>


### [20] [Scaling-Aware Adapter for Structure-Grounded LLM Reasoning](https://arxiv.org/abs/2602.02780)
*Zihao Jing,Qiuhao Zeng,Ruiyi Fang,Yan Yi Li,Yan Sun,Boyu Wang,Pingzhao Hu*

Main category: cs.AI

TL;DR: Cuttlefish是一个统一的全原子大语言模型，通过自适应缩放结构补丁和几何接地适配器，在保持几何信息的同时解决模态融合瓶颈问题，实现更好的结构推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在生物分子结构推理中存在两个主要问题：1）通过序列化标记或固定长度查询连接器压缩结构输入，导致几何信息丢失，容易产生结构幻觉；2）模态融合瓶颈同时过度压缩和次优分配结构标记，阻碍了通用全原子推理的实现。

Method: 提出Cuttlefish模型，包含两个核心技术：1）缩放感知补丁：使用指令条件门控机制在结构图上生成可变大小的补丁，根据结构复杂度自适应缩放查询标记预算；2）几何接地适配器：通过跨注意力机制精炼自适应标记并注入模态嵌入，将显式几何线索暴露给LLM以减少结构幻觉。

Result: 在多样化的全原子基准测试中，Cuttlefish在异构结构接地推理方面实现了优越的性能表现。

Conclusion: Cuttlefish通过自适应缩放结构标记和显式几何接地，成功解决了现有方法在生物分子结构推理中的局限性，实现了更好的全原子推理能力，代码已在项目仓库中开源。

Abstract: Large language models (LLMs) are enabling reasoning over biomolecular structures, yet existing methods remain modality-specific and typically compress structural inputs through sequence-based tokenization or fixed-length query connectors. Such architectures either omit the geometric groundings requisite for mitigating structural hallucinations or impose inflexible modality fusion bottlenecks that concurrently over-compress and suboptimally allocate structural tokens, thereby impeding the realization of generalized all-atom reasoning. We introduce Cuttlefish, a unified all-atom LLM that grounds language reasoning in geometric cues while scaling modality tokens with structural complexity. First, Scaling-Aware Patching leverages an instruction-conditioned gating mechanism to generate variable-size patches over structural graphs, adaptively scaling the query token budget with structural complexity to mitigate fixed-length connector bottlenecks. Second, Geometry Grounding Adapter refines these adaptive tokens via cross-attention to modality embeddings and injects the resulting modality tokens into the LLM, exposing explicit geometric cues to reduce structural hallucination. Experiments across diverse all-atom benchmarks demonstrate that Cuttlefish achieves superior performance in heterogeneous structure-grounded reasoning. Code is available at the project repository.

</details>


### [21] [AutoSizer: Automatic Sizing of Analog and Mixed-Signal Circuits via Large Language Model (LLM) Agents](https://arxiv.org/abs/2602.02849)
*Xi Yu,Dmitrii Torbunov,Soumyajit Mandal,Yihui Ren*

Main category: cs.AI

TL;DR: AutoSizer：一个基于大语言模型的反射式元优化框架，用于解决模拟混合信号集成电路晶体管尺寸优化问题，通过闭环统一电路理解、自适应搜索空间构建和优化编排。


<details>
  <summary>Details</summary>
Motivation: 模拟混合信号集成电路设计严重依赖专家知识，晶体管尺寸优化面临非线性行为、高维设计空间和严格性能约束等挑战。现有EDA方法将尺寸优化视为静态黑盒优化，导致效率低下且鲁棒性差。大语言模型虽具备强大推理能力，但不适合AMS尺寸的精确数值优化。

Method: 提出AutoSizer框架，采用双循环优化结构：内循环负责电路尺寸优化，外循环分析优化动态和约束，根据仿真反馈迭代精化搜索空间。框架统一了电路理解、自适应搜索空间构建和优化编排，形成闭环系统。

Result: 开发了AMS-SizingBench开放基准测试集，包含24个基于SKY130 CMOS技术的多样化AMS电路，用于评估真实仿真器约束下的自适应优化策略。实验表明AutoSizer在解决方案质量、收敛速度和成功率方面均优于传统优化方法和现有LLM智能体。

Conclusion: AutoSizer成功地将大语言模型的推理能力与数值优化相结合，通过反射式元优化框架有效解决了AMS电路尺寸优化问题，在多个电路难度级别上展现出优越性能，为自动化AMS设计提供了新途径。

Abstract: The design of Analog and Mixed-Signal (AMS) integrated circuits remains heavily reliant on expert knowledge, with transistor sizing a major bottleneck due to nonlinear behavior, high-dimensional design spaces, and strict performance constraints. Existing Electronic Design Automation (EDA) methods typically frame sizing as static black-box optimization, resulting in inefficient and less robust solutions. Although Large Language Models (LLMs) exhibit strong reasoning abilities, they are not suited for precise numerical optimization in AMS sizing. To address this gap, we propose AutoSizer, a reflective LLM-driven meta-optimization framework that unifies circuit understanding, adaptive search-space construction, and optimization orchestration in a closed loop. It employs a two-loop optimization framework, with an inner loop for circuit sizing and an outer loop that analyzes optimization dynamics and constraints to iteratively refine the search space from simulation feedback. We further introduce AMS-SizingBench, an open benchmark comprising 24 diverse AMS circuits in SKY130 CMOS technology, designed to evaluate adaptive optimization policies under realistic simulator-based constraints. AutoSizer experimentally achieves higher solution quality, faster convergence, and higher success rate across varying circuit difficulties, outperforming both traditional optimization methods and existing LLM-based agents.

</details>


### [22] [STEER: Inference-Time Risk Control via Constrained Quality-Diversity Search](https://arxiv.org/abs/2602.02862)
*Eric Yang,Jong Ha Lee,Jonathan Amar,Elissa Ye,Yugang Jia*

Main category: cs.AI

TL;DR: STEER框架通过进化集成优化实现可调控的LLM决策，在临床分诊等序数决策任务中提供单调可调的保守性控制，同时保持领域能力。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在平均正确性训练下容易出现模式坍塌，在需要权衡敏感性和特异性的序数决策任务（如临床分诊）中缺乏可调控性，标准对齐方法移除了基于上下文约束调整ROC操作点的能力。

Method: STEER（Steerable Tuning via Evolutionary Ensemble Refinement）是一个无需训练的框架，通过离线约束质量多样性搜索构建自然语言角色群体，在保证最低安全性、推理能力和稳定性阈值的同时促进行为覆盖。推理时通过单个可解释的控制参数将用户指定的风险百分位数映射到选定角色，实现决策保守性的单调调整。

Result: 在两个临床分诊基准测试中，STEER相比基于温度的采样和静态角色集成实现了更广泛的行为覆盖。与代表性后训练方法相比，STEER在明确紧急病例上保持显著更高的准确性，同时在模糊决策上提供可比的调控能力。

Conclusion: STEER作为一种保持安全性的风险控制范式，能够在不影响领域能力的情况下调控行为，为LLM在需要可调控决策的场景中提供了有效的解决方案。

Abstract: Large Language Models (LLMs) trained for average correctness often exhibit mode collapse, producing narrow decision behaviors on tasks where multiple responses may be reasonable. This limitation is particularly problematic in ordinal decision settings such as clinical triage, where standard alignment removes the ability to trade off specificity and sensitivity (the ROC operating point) based on contextual constraints. We propose STEER (Steerable Tuning via Evolutionary Ensemble Refinement), a training-free framework that reintroduces this tunable control. STEER constructs a population of natural-language personas through an offline, constrained quality-diversity search that promotes behavioral coverage while enforcing minimum safety, reasoning, and stability thresholds. At inference time, STEER exposes a single, interpretable control parameter that maps a user-specified risk percentile to a selected persona, yielding a monotonic adjustment of decision conservativeness. On two clinical triage benchmarks, STEER achieves broader behavioral coverage compared to temperature-based sampling and static persona ensembles. Compared to a representative post-training method, STEER maintains substantially higher accuracy on unambiguous urgent cases while providing comparable control over ambiguous decisions. These results demonstrate STEER as a safety-preserving paradigm for risk control, capable of steering behavior without compromising domain competence.

</details>


### [23] [Reasoning about Reasoning: BAPO Bounds on Chain-of-Thought Token Complexity in LLMs](https://arxiv.org/abs/2602.02909)
*Kiran Tomlinson,Tobias Schnabel,Adith Swaminathan,Jennifer Neville*

Main category: cs.AI

TL;DR: 论文研究了思维链推理所需的计算量，证明了三个典型任务需要Ω(n)个推理token，并通过实验验证了理论下界


<details>
  <summary>Details</summary>
Motivation: 思维链推理虽然能提升LLM性能，但带来了显著的延迟和计算成本。需要从理论上理解随着输入规模增长，解决一个问题需要多少推理token

Method: 扩展了有界注意力前缀预言机模型，证明了三个BAPO-hard任务（二进制多数、三元组匹配、图可达性）的推理token下界，并通过显式构造给出了匹配或接近匹配的上界，最后用前沿推理模型进行实验验证

Result: 证明了三个任务都需要Ω(n)个推理token，实验显示前沿推理模型在这些任务上表现出近似线性的推理token缩放，当推理预算受限时会失败，与理论下界一致

Conclusion: 研究识别了通过思维链进行推理时计算的基本瓶颈，为分析最优推理长度提供了原则性工具

Abstract: Inference-time scaling via chain-of-thought (CoT) reasoning is a major driver of state-of-the-art LLM performance, but it comes with substantial latency and compute costs. We address a fundamental theoretical question: how many reasoning tokens are required to solve a problem as input size grows? By extending the bounded attention prefix oracle (BAPO) model--an abstraction of LLMs that quantifies the information flow required to solve a task--we prove lower bounds on the CoT tokens required for three canonical BAPO-hard tasks: binary majority, triplet matching, and graph reachability. We show that each requires $Ω(n)$ reasoning tokens when the input size is $n$. We complement these results with matching or near-matching upper bounds via explicit constructions. Finally, our experiments with frontier reasoning models show approximately linear reasoning token scaling on these tasks and failures when constrained to smaller reasoning budgets, consistent with our theoretical lower bounds. Together, our results identify fundamental bottlenecks in inference-time compute through CoT and offer a principled tool for analyzing optimal reasoning length.

</details>


### [24] [UAT-LITE: Inference-Time Uncertainty-Aware Attention for Pretrained Transformers](https://arxiv.org/abs/2602.02952)
*Elias Hossain,Shubhashis Roy Dipta,Subash Neupane,Rajib Rana,Ravid Shwartz-Ziv,Ivan Garibay,Niloofar Yousefi*

Main category: cs.AI

TL;DR: UAT-LITE：一种推理时框架，通过蒙特卡洛dropout在预训练transformer分类器中实现自注意力不确定性感知，无需修改预训练权重或训练目标，显著降低校准误差并提升选择性预测能力。


<details>
  <summary>Details</summary>
Motivation: 神经NLP模型经常存在校准不良问题，对错误预测分配高置信度，这影响了选择性预测和高风险部署。现有的后处理校准方法只调整输出概率而不改变内部计算，而集成和贝叶斯方法虽然能改善不确定性但需要大量训练或存储成本。

Method: 提出UAT-LITE推理时框架，通过蒙特卡洛dropout在预训练transformer分类器中实现近似贝叶斯推断。从随机前向传播中估计token级认知不确定性，并用其调制自注意力机制。此外引入层间方差分解来诊断预测不确定性如何在transformer深度中累积。

Result: 在SQuAD 2.0可回答性、MNLI和SST-2数据集上，UAT-LITE相对于微调的BERT-base基线，平均减少约20%的期望校准误差，同时保持任务准确性。还改善了选择性预测和分布偏移下的鲁棒性。

Conclusion: UAT-LITE提供了一种轻量级、无需训练的方法来增强预训练transformer模型的不确定性感知能力，显著改善模型校准，同时保持性能，为高风险NLP应用提供了实用解决方案。

Abstract: Neural NLP models are often miscalibrated, assigning high confidence to incorrect predictions, which undermines selective prediction and high-stakes deployment. Post-hoc calibration methods adjust output probabilities but leave internal computation unchanged, while ensemble and Bayesian approaches improve uncertainty at substantial training or storage cost. We propose UAT-LITE, an inference-time framework that makes self-attention uncertainty-aware using approximate Bayesian inference via Monte Carlo dropout in pretrained transformer classifiers. Token-level epistemic uncertainty is estimated from stochastic forward passes and used to modulate self-attention during contextualization, without modifying pretrained weights or training objectives. We additionally introduce a layerwise variance decomposition to diagnose how predictive uncertainty accumulates across transformer depth. Across the SQuAD 2.0 answerability, MNLI, and SST-2, UAT-LITE reduces Expected Calibration Error by approximately 20% on average relative to a fine-tuned BERT-base baseline while preserving task accuracy, and improves selective prediction and robustness under distribution shift.

</details>


### [25] [Structuring Value Representations via Geometric Coherence in Markov Decision Processes](https://arxiv.org/abs/2602.02978)
*Zuyuan Zhang,Zeyu Fang,Tian Lan*

Main category: cs.AI

TL;DR: 提出GCR-RL方法，通过序理论视角将强化学习中的价值函数估计重构为学习偏序集，利用几何一致性正则化提升学习效率和稳定性。


<details>
  <summary>Details</summary>
Motivation: 几何性质可用于稳定和加速强化学习，现有方法包括编码对称结构、几何感知数据增强和强制结构限制。本文从序理论的新视角重新审视RL，将价值函数估计转化为学习期望的偏序集。

Method: 提出GCR-RL（几何一致性正则化强化学习），通过计算一系列超偏序集精化序列——通过精化先前步骤的偏序集并从时序差分信号中学习额外的序关系——确保支撑学习价值函数的偏序集序列具有几何一致性。开发了基于Q学习和演员-评论家的两种新算法来实现这些超偏序集精化。

Result: 分析了算法的理论性质和收敛速度。在一系列任务中进行实证评估，相比强基线方法，GCR-RL在样本效率和稳定性能方面表现出显著改进。

Conclusion: 通过序理论视角重新构建强化学习问题，提出的GCR-RL方法能够有效利用几何一致性正则化，在多种任务中实现更好的样本效率和稳定性。

Abstract: Geometric properties can be leveraged to stabilize and speed reinforcement learning. Existing examples include encoding symmetry structure, geometry-aware data augmentation, and enforcing structural restrictions. In this paper, we take a novel view of RL through the lens of order theory and recast value function estimates into learning a desired poset (partially ordered set). We propose \emph{GCR-RL} (Geometric Coherence Regularized Reinforcement Learning) that computes a sequence of super-poset refinements -- by refining posets in previous steps and learning additional order relationships from temporal difference signals -- thus ensuring geometric coherence across the sequence of posets underpinning the learned value functions. Two novel algorithms by Q-learning and by actor--critic are developed to efficiently realize these super-poset refinements. Their theoretical properties and convergence rates are analyzed. We empirically evaluate GCR-RL in a range of tasks and demonstrate significant improvements in sample efficiency and stable performance over strong baselines.

</details>


### [26] [Are LLMs Biased Like Humans? Causal Reasoning as a Function of Prior Knowledge, Irrelevant Information, and Reasoning Budget](https://arxiv.org/abs/2602.02983)
*Hanna M. Dettki,Charley M. Wu,Bob Rehder*

Main category: cs.AI

TL;DR: LLMs在因果推理任务上表现出比人类更规则化的推理策略，不模仿人类典型的共因结构偏见，思维链提示能增强其鲁棒性


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在因果推理领域的表现，探究其判断是基于规范性因果计算、人类式捷径还是脆弱的模式匹配，为LLMs的安全有效部署提供依据

Method: 使用共因结构（C1→E←C2）形式化的11个因果判断任务，对20多个LLMs进行基准测试，并与匹配的人类基线比较；使用可解释小模型压缩LLMs的因果判断；测试LLMs在语义抽象和提示过载下的鲁棒性

Result: 大多数LLMs表现出比人类更规则化的推理策略；LLMs不模仿人类典型的共因结构偏见（弱解释消除和马尔可夫违规）；思维链提示能增强许多LLMs的鲁棒性

Conclusion: LLMs的推理策略与人类存在差异，当已知偏见不受欢迎时LLMs可以补充人类，但其规则化推理在不确定性本质存在时可能失效，需要表征LLMs的推理策略以确保安全有效部署

Abstract: Large language models (LLMs) are increasingly used in domains where causal reasoning matters, yet it remains unclear whether their judgments reflect normative causal computation, human-like shortcuts, or brittle pattern matching. We benchmark 20+ LLMs against a matched human baseline on 11 causal judgment tasks formalized by a collider structure ($C_1 \!\rightarrow\! E\! \leftarrow \!C_2$). We find that a small interpretable model compresses LLMs' causal judgments well and that most LLMs exhibit more rule-like reasoning strategies than humans who seem to account for unmentioned latent factors in their probability judgments. Furthermore, most LLMs do not mirror the characteristic human collider biases of weak explaining away and Markov violations. We probe LLMs' causal judgment robustness under (i) semantic abstraction and (ii) prompt overloading (injecting irrelevant text), and find that chain-of-thought (CoT) increases robustness for many LLMs. Together, this divergence suggests LLMs can complement humans when known biases are undesirable, but their rule-like reasoning may break down when uncertainty is intrinsic -- highlighting the need to characterize LLM reasoning strategies for safe, effective deployment.

</details>


### [27] [Large Language Models Can Take False First Steps at Inference-time Planning](https://arxiv.org/abs/2602.02991)
*Haijiang Yan,Jian-Qiao Zhu,Adam Sanborn*

Main category: cs.AI

TL;DR: LLMs在训练中获得了序列级规划能力，但在推理时表现出短视和不一致的规划行为。研究提出贝叶斯解释：自生成上下文驱动规划偏移，导致看似受损的规划行为。


<details>
  <summary>Details</summary>
Motivation: 解释LLMs在训练中获得的规划能力与推理时表现出的短视规划行为之间的差距，提供理论框架来解释这一现象。

Method: 提出贝叶斯账户模型，将规划行为建立在不断演化的生成上下文基础上。通过两个受控实验验证：随机生成任务展示人类提示下的受限规划和自生成上下文积累时规划强度的增加；高斯采样任务显示在自生成序列条件下初始偏见的减少。

Result: 实验证实了提出的模型：随机生成任务显示随着自生成上下文积累，规划强度增加；高斯采样任务显示在自生成序列条件下初始偏见减少，验证了自生成上下文驱动规划偏移的理论。

Conclusion: 研究为LLMs在推理时如何提前规划提供了理论解释和实证证据，表明看似受损的规划行为实际上是由自生成上下文驱动的规划偏移所致，而非能力缺陷。

Abstract: Large language models (LLMs) have been shown to acquire sequence-level planning abilities during training, yet their planning behavior exhibited at inference time often appears short-sighted and inconsistent with these capabilities. We propose a Bayesian account for this gap by grounding planning behavior in the evolving generative context: given the subtle differences between natural language and the language internalized by LLMs, accumulated self-generated context drives a planning-shift during inference and thereby creates the appearance of compromised planning behavior. We further validate the proposed model through two controlled experiments: a random-generation task demonstrating constrained planning under human prompts and increasing planning strength as self-generated context accumulates, and a Gaussian-sampling task showing reduced initial bias when conditioning on self-generated sequences. These findings provide a theoretical explanation along with empirical evidence for characterizing how LLMs plan ahead during inference.

</details>


### [28] [Methods and Open Problems in Differentiable Social Choice: Learning Mechanisms, Decisions, and Alignment](https://arxiv.org/abs/2602.03003)
*Zhiyu An,Wan Du*

Main category: cs.AI

TL;DR: 本文综述了可微分社会选择这一新兴范式，将投票规则、机制和聚合过程作为可从数据中优化的可学习、可微分模型，探讨了机器学习、经济学和民主理论的交叉领域。


<details>
  <summary>Details</summary>
Motivation: 社会选择已从政治理论和经济学的外围关注点转变为现代机器学习系统的基础组成部分。从拍卖和资源分配到联邦学习、参与式治理和大语言模型对齐，机器学习管道越来越多地将异质偏好、激励和判断聚合成集体决策。许多当代机器学习系统已经实现了社会选择机制，但通常是隐式且缺乏明确的规范审查。

Method: 本文采用综述研究方法，综合了拍卖、投票、预算编制、流动民主、去中心化聚合和逆向机制学习等领域的工作，展示了如何将经典公理和不可能性定理重新表述为目标、约束和优化权衡。

Result: 提出了可微分社会选择这一新兴范式，将社会选择机制形式化为可学习、可微分的模型，能够从数据中进行优化。识别了36个开放性问题，定义了一个新的研究议程。

Conclusion: 可微分社会选择为机器学习、经济学和民主理论的交叉领域提供了一个统一框架，通过将社会选择机制形式化为可优化模型，为解决传统社会选择理论中的规范问题提供了新途径，并为设计更公平、透明和有效的集体决策系统开辟了新的研究方向。

Abstract: Social choice is no longer a peripheral concern of political theory or economics-it has become a foundational component of modern machine learning systems. From auctions and resource allocation to federated learning, participatory governance, and the alignment of large language models, machine learning pipelines increasingly aggregate heterogeneous preferences, incentives, and judgments into collective decisions. In effect, many contemporary machine learning systems already implement social choice mechanisms, often implicitly and without explicit normative scrutiny.
  This Review surveys differentiable social choice: an emerging paradigm that formulates voting rules, mechanisms, and aggregation procedures as learnable, differentiable models optimized from data. We synthesize work across auctions, voting, budgeting, liquid democracy, decentralized aggregation, and inverse mechanism learning, showing how classical axioms and impossibility results reappear as objectives, constraints, and optimization trade-offs. We conclude by identifying 36 open problems defining a new research agenda at the intersection of machine learning, economics, and democratic theory.

</details>


### [29] [Distilling LLM Reasoning into Graph of Concept Predictors](https://arxiv.org/abs/2602.03006)
*Ziyang Yu,Liang Zhao*

Main category: cs.AI

TL;DR: GCP是一个推理感知的主动蒸馏框架，通过将教师LLM的决策过程外部化为有向无环图，并用模块化概念预测器在学生模型中镜像该图，从而提高样本效率和训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在判别任务部署中面临推理延迟、计算和API成本问题。主动蒸馏方法通常只蒸馏最终标签，丢弃了中间推理信号，且缺乏对缺失推理和错误来源的诊断能力。

Method: 提出图概念预测器(GCP)框架：1) 将教师LLM的决策过程外部化为有向无环图；2) 在学生模型中用模块化概念预测器镜像该图；3) 采用图感知的采集策略，针对关键推理节点的不确定性和分歧；4) 实施目标子模块重训练，将下游损失归因于特定概念预测器并仅更新最有影响的模块。

Result: 在八个NLP分类基准测试中，GCP在有限标注预算下提升了性能，同时产生了更可解释和可控的训练动态。

Conclusion: GCP框架通过外部化教师推理过程、模块化学生设计和目标训练策略，有效解决了传统主动蒸馏方法的局限性，在提升性能的同时增强了可解释性和控制性。

Abstract: Deploying Large Language Models (LLMs) for discriminative workloads is often limited by inference latency, compute, and API costs at scale. Active distillation reduces these costs by querying an LLM oracle to train compact discriminative students, but most pipelines distill only final labels, discarding intermediate reasoning signals and offering limited diagnostics of what reasoning is missing and where errors arise. We propose Graph of Concept Predictors (GCP), a reasoning-aware active distillation framework that externalizes the teacher's decision process as a directed acyclic graph and mirrors it with modular concept predictors in the student. GCP enhances sample efficiency through a graph-aware acquisition strategy that targets uncertainty and disagreement at critical reasoning nodes. Additionally, it improves training stability and efficiency by performing targeted sub-module retraining, which attributes downstream loss to specific concept predictors and updates only the most influential modules. Experiments on eight NLP classification benchmarks demonstrate that GCP enhances performance under limited annotation budgets while yielding more interpretable and controllable training dynamics. Code is available at: https://github.com/Ziyang-Yu/GCP.

</details>


### [30] [STAR: Similarity-guided Teacher-Assisted Refinement for Super-Tiny Function Calling Models](https://arxiv.org/abs/2602.03022)
*Jiliang Ni,Jiachen Pu,Zhongyi Yang,Jingfeng Luo,Conggang Hu*

Main category: cs.AI

TL;DR: STAR框架通过相似性引导的教师辅助精炼，将大语言模型的能力转移到超小模型中，解决了现有方法中的过拟合、训练不稳定、二元奖励无效等问题，在函数调用任务上取得了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在函数调用中至关重要，但其巨大规模阻碍了广泛采用，需要将其能力转移到更小的模型中。现有方法存在过拟合、训练不稳定、多解任务中二元奖励无效以及技术难以协同等问题。

Method: STAR框架包含两个核心技术创新：1) 约束知识蒸馏(CKD)，通过增强top-k前向KL散度来抑制自信的错误预测，确保训练稳定性同时保留下游强化学习的探索能力；2) 相似性引导的强化学习(Sim-RL)，引入基于相似性的细粒度奖励，通过评估生成输出与真实值的相似性提供更丰富的优化信号。

Result: 在具有挑战性的基准测试中，STAR模型在其规模类别中建立了SOTA性能，显著优于基线方法。特别是0.6B的STAR模型在所有1B以下的开源模型中表现最佳，甚至超越了多个更大规模的知名开源模型。

Conclusion: STAR展示了一个将大语言模型能力蒸馏到超小模型的训练框架，为强大、可访问且高效的人工智能智能体铺平了道路。

Abstract: The proliferation of Large Language Models (LLMs) in function calling is pivotal for creating advanced AI agents, yet their large scale hinders widespread adoption, necessitating transferring their capabilities into smaller ones. However, existing paradigms are often plagued by overfitting, training instability, ineffective binary rewards for multi-solution tasks, and the difficulty of synergizing techniques. We introduce STAR: Similarity-guided Teacher-Assisted Refinement, a novel holistic framework that effectively transfers LLMs' capabilities to super-tiny models. STAR consists of two core technical innovations: (1) Constrained Knowledge Distillation (CKD), a training objective that augments top-k forward KL divergence to suppress confidently incorrect predictions, ensuring training stability while preserving exploration capacity for downstream RL. STAR holistically synergizes these strategies within a cohesive training curriculum, enabling super-tiny models to achieve exceptional performance on complex function calling tasks; (2) Similarity-guided RL (Sim-RL), a RL mechanism that introduces a fine-grained, similarity-based reward. This provides a robust, continuous, and rich signal for better policy optimization by evaluating the similarity between generated outputs and the ground truth. Extensive experiments on challenging and renowned benchmarks demonstrate the effectiveness of our method. Our STAR models establish SOTA in their size classes, significantly outperforming baselines. Remarkably, our 0.6B STAR model achieves the best performance among all open models under 1B, surpassing even several well-known open models at a larger scale. STAR demonstrates a training framework that distills capabilities of LLMs into super-tiny models, paving the way for powerful, accessible, and efficient AI agents.

</details>


### [31] [RC-GRPO: Reward-Conditioned Group Relative Policy Optimization for Multi-Turn Tool Calling Agents](https://arxiv.org/abs/2602.03025)
*Haitian Zhong,Jixiu Zhai,Lei Song,Jiang Bian,Qiang Liu,Tieniu Tan*

Main category: cs.AI

TL;DR: 提出RC-GRPO方法解决LLM多轮工具调用中奖励稀疏和探索成本高的问题，通过在提示中注入奖励目标特殊标记来增强组内多样性，在BFCLv4基准上超越基线方法


<details>
  <summary>Details</summary>
Motivation: 多轮工具调用对大型语言模型具有挑战性，因为奖励稀疏且探索成本高。传统的SFT+GRPO方法在组内奖励变化低时（如更多rollout获得全0或全1奖励）会停滞，导致组归一化优势信息不足，产生消失的更新

Method: 提出RC-GRPO（奖励条件组相对策略优化）方法：1）首先在混合质量轨迹上微调奖励条件轨迹策略，在提示中注入奖励目标特殊标记（如<|high_reward|>, <|low_reward|>），使模型能够按需生成不同质量的轨迹；2）在强化学习期间，在每个GRPO组内采样不同的奖励标记，并在采样的标记上条件化rollout，以提高组内多样性

Result: 在Berkeley Function Calling Leaderboard v4多轮基准测试中，该方法相比基线方法获得了一致的性能提升，Qwen-2.5-7B-Instruct模型的性能甚至超过了所有闭源API模型

Conclusion: RC-GRPO通过将探索视为可控的转向问题，使用离散奖励标记来增强组内多样性，有效解决了多轮工具调用中奖励稀疏和探索成本高的问题，在基准测试中表现出优越性能

Abstract: Multi-turn tool calling is challenging for Large Language Models (LLMs) because rewards are sparse and exploration is expensive. A common recipe, SFT followed by GRPO, can stall when within-group reward variation is low (e.g., more rollouts in a group receive the all 0 or all 1 reward), making the group-normalized advantage uninformative and yielding vanishing updates. To address this problem, we propose RC-GRPO (Reward-Conditioned Group Relative Policy Optimization), which treats exploration as a controllable steering problem via discrete reward tokens. We first fine-tune a Reward-Conditioned Trajectory Policy (RCTP) on mixed-quality trajectories with reward goal special tokens (e.g., <|high_reward|>, <|low_reward|>) injected into the prompts, enabling the model to learn how to generate distinct quality trajectories on demand. Then during RL, we sample diverse reward tokens within each GRPO group and condition rollouts on the sampled token to improve within-group diversity, improving advantage gains. On the Berkeley Function Calling Leaderboard v4 (BFCLv4) multi-turn benchmark, our method yields consistently improved performance than baselines, and the performance on Qwen-2.5-7B-Instruct even surpasses all closed-source API models.

</details>


### [32] [Visual Reasoning over Time Series via Multi-Agent System](https://arxiv.org/abs/2602.03026)
*Weilin Ruan,Yuxuan Liang*

Main category: cs.AI

TL;DR: MAS4TS：基于多智能体系统的通用时间序列分析框架，通过视觉推理和潜在重建实现跨任务泛化


<details>
  <summary>Details</summary>
Motivation: 现有时间序列分析方法在整合直观视觉推理和跨任务自适应工具使用方面存在局限，需要一种能够结合视觉理解和通用泛化能力的新框架

Method: 提出基于Analyzer-Reasoner-Executor范式的工具驱动多智能体系统，包含视觉语言模型进行时间序列图的视觉推理提取时间结构，在潜在空间重建预测轨迹，三个专业智能体通过共享内存和门控通信协调，路由器选择任务特定工具链执行

Result: 在多个基准测试上的广泛实验表明，MAS4TS在广泛的时间序列任务中实现了最先进的性能，同时表现出强大的泛化能力和高效的推理

Conclusion: MAS4TS通过整合智能体通信、视觉推理和潜在重建的统一框架，为通用时间序列任务提供了有效的解决方案，展示了多智能体系统在时间序列分析中的潜力

Abstract: Time series analysis underpins many real-world applications, yet existing time-series-specific methods and pretrained large-model-based approaches remain limited in integrating intuitive visual reasoning and generalizing across tasks with adaptive tool usage. To address these limitations, we propose MAS4TS, a tool-driven multi-agent system for general time series tasks, built upon an Analyzer-Reasoner-Executor paradigm that integrates agent communication, visual reasoning, and latent reconstruction within a unified framework. MAS4TS first performs visual reasoning over time series plots with structured priors using a Vision-Language Model to extract temporal structures, and subsequently reconstructs predictive trajectories in latent space. Three specialized agents coordinate via shared memory and gated communication, while a router selects task-specific tool chains for execution. Extensive experiments on multiple benchmarks demonstrate that MAS4TS achieves state-of-the-art performance across a wide range of time series tasks, while exhibiting strong generalization and efficient inference.

</details>


### [33] [MAS-ProVe: Understanding the Process Verification of Multi-Agent Systems](https://arxiv.org/abs/2602.03053)
*Vishal Venkataramani,Haizhou Shi,Zixuan Ke,Austin Xu,Xiaoxiao He,Yingbo Zhou,Semih Yavuz,Hao Wang,Shafiq Joty*

Main category: cs.AI

TL;DR: 该研究系统评估了基于大语言模型的多智能体系统中的过程验证方法，发现过程级验证并不总能提升性能且方差较大，LLM作为裁判的方法表现最佳，但有效的多智能体过程验证仍是一个开放挑战。


<details>
  <summary>Details</summary>
Motivation: 基于大语言模型的多智能体系统在推理轨迹上存在高方差，过程验证（评估中间步骤）在一般推理场景中显示出潜力，但其在多智能体系统中的实际效果尚不明确，需要系统性的实证研究来填补这一空白。

Method: 提出了MAS-ProVe框架，系统研究了三种验证范式（LLM作为裁判、奖励模型、过程奖励模型），在两个验证粒度（智能体级和迭代级）上进行评估。研究了五个代表性验证器和四种上下文管理策略，在六个不同的多智能体框架和多个推理基准上进行实验。

Result: 过程级验证并不总能提升性能且经常表现出高方差，突显了可靠评估部分多智能体轨迹的困难。在研究方法中，LLM作为裁判的方法通常优于基于奖励的方法，训练过的裁判模型胜过通用大语言模型。观察到LLM作为裁判和作为单个智能体之间的性能差距较小，并识别出验证中存在上下文长度与性能的权衡。

Conclusion: 有效且鲁棒的多智能体过程验证仍然是一个开放挑战，需要超越当前范式的进一步进展。LLM作为裁判的方法表现最佳，但整体上过程验证在多智能体系统中的可靠性有限。

Abstract: Multi-Agent Systems (MAS) built on Large Language Models (LLMs) often exhibit high variance in their reasoning trajectories. Process verification, which evaluates intermediate steps in trajectories, has shown promise in general reasoning settings, and has been suggested as a potential tool for guiding coordination of MAS; however, its actual effectiveness in MAS remains unclear. To fill this gap, we present MAS-ProVe, a systematic empirical study of process verification for multi-agent systems (MAS). Our study spans three verification paradigms (LLM-as-a-Judge, reward models, and process reward models), evaluated across two levels of verification granularity (agent-level and iteration-level). We further examine five representative verifiers and four context management strategies, and conduct experiments over six diverse MAS frameworks on multiple reasoning benchmarks. We find that process-level verification does not consistently improve performance and frequently exhibits high variance, highlighting the difficulty of reliably evaluating partial multi-agent trajectories. Among the methods studied, LLM-as-a-Judge generally outperforms reward-based approaches, with trained judges surpassing general-purpose LLMs. We further observe a small performance gap between LLMs acting as judges and as single agents, and identify a context-length-performance trade-off in verification. Overall, our results suggest that effective and robust process verification for MAS remains an open challenge, requiring further advances beyond current paradigms. Code is available at https://github.com/Wang-ML-Lab/MAS-ProVe.

</details>


### [34] [Risky-Bench: Probing Agentic Safety Risks under Real-World Deployment](https://arxiv.org/abs/2602.03100)
*Jingnan Zheng,Yanzhen Luo,Jingjun Xu,Bingnan Liu,Yuxin Chen,Chenhang Cui,Gelei Deng,Chaochao Lu,Xiang Wang,An Zhang,Tat-Seng Chua*

Main category: cs.AI

TL;DR: Risky-Bench是一个系统化的智能体安全评估框架，通过领域无关的安全原则和上下文感知的安全标准，在真实部署条件下评估智能体的安全风险。


<details>
  <summary>Details</summary>
Motivation: 现有智能体安全评估方法存在局限性：依赖针对特定场景的风险导向任务，安全风险覆盖有限；无法评估智能体在复杂真实部署中长期交互任务执行中的安全行为；对特定智能体设置的专门化限制了跨不同配置的适应性。

Method: 提出Risky-Bench框架：围绕领域无关的安全原则组织评估；推导上下文感知的安全标准来界定安全空间；在不同威胁假设下通过真实任务执行系统评估安全风险；作为结构化评估流程，可适应不同部署场景。

Result: 在生活辅助智能体设置中，Risky-Bench揭示了最先进智能体在真实执行条件下存在显著安全风险；该框架不仅限于生活辅助场景，可适应其他部署设置构建环境特定的安全评估。

Conclusion: Risky-Bench提供了一个可扩展的智能体安全评估方法学，能够系统评估智能体在复杂真实世界部署中的安全风险，填补了现有评估方法的空白。

Abstract: Large Language Models (LLMs) are increasingly deployed as agents that operate in real-world environments, introducing safety risks beyond linguistic harm. Existing agent safety evaluations rely on risk-oriented tasks tailored to specific agent settings, resulting in limited coverage of safety risk space and failing to assess agent safety behavior during long-horizon, interactive task execution in complex real-world deployments. Moreover, their specialization to particular agent settings limits adaptability across diverse agent configurations. To address these limitations, we propose Risky-Bench, a framework that enables systematic agent safety evaluation grounded in real-world deployment. Risky-Bench organizes evaluation around domain-agnostic safety principles to derive context-aware safety rubrics that delineate safety space, and systematically evaluates safety risks across this space through realistic task execution under varying threat assumptions. When applied to life-assist agent settings, Risky-Bench uncovers substantial safety risks in state-of-the-art agents under realistic execution conditions. Moreover, as a well-structured evaluation pipeline, Risky-Bench is not confined to life-assist scenarios and can be adapted to other deployment settings to construct environment-specific safety evaluations, providing an extensible methodology for agent safety assessment.

</details>


### [35] [Understanding Multi-Agent LLM Frameworks: A Unified Benchmark and Experimental Analysis](https://arxiv.org/abs/2602.03128)
*Abdelghny Orogat,Ana Rostam,Essam Mansour*

Main category: cs.AI

TL;DR: MAFBench：一个用于评估多智能体LLM框架性能的统一基准测试套件，揭示了框架架构设计对系统性能的重大影响


<details>
  <summary>Details</summary>
Motivation: 当前多智能体LLM框架虽然广泛使用，但其架构设计对系统性能的影响缺乏系统研究。现有基准测试关注单个能力，缺乏标准化的框架级评估，导致无法准确衡量不同架构选择对延迟、吞吐量、准确性和可扩展性的影响。

Method: 提出多智能体LLM框架的架构分类法，开发MAFBench统一评估套件，集成现有基准测试到标准化执行管道中，对多个广泛使用的框架进行受控实证研究。

Result: 框架级设计选择单独就能导致延迟增加超过100倍，规划准确性降低高达30%，协调成功率从90%以上降至30%以下。不同框架在编排开销、内存行为、规划、专业化和协调等方面表现差异显著。

Conclusion: 多智能体LLM框架的架构设计对系统性能有决定性影响。研究结果为框架架构设计提供了具体原则和选择指导，并指出了未来研究方向，强调需要更系统化的框架评估方法。

Abstract: Multi-agent LLM frameworks are widely used to accelerate the development of agent systems powered by large language models (LLMs). These frameworks impose distinct architectural structures that govern how agents interact, store information, and coordinate tasks. However, their impact on system performance remains poorly understood. This gap is critical, as architectural choices alone can induce order-of-magnitude differences in latency and throughput, as well as substantial variation in accuracy and scalability. Addressing this challenge requires (i) jointly evaluating multiple capabilities, such as orchestration overhead, memory behavior, planning, specialization, and coordination, and (ii) conducting these evaluations under controlled, framework-level conditions to isolate architectural effects. Existing benchmarks focus on individual capabilities and lack standardized framework-level evaluation. We address these limitations by (i) introducing an architectural taxonomy for systematically comparing multi-agent LLM frameworks along fundamental dimensions, and (ii) developing MAFBench, a unified evaluation suite that integrates existing benchmarks under a standardized execution pipeline. Using MAFBench, we conduct a controlled empirical study across several widely used frameworks. Our results show that framework-level design choices alone can increase latency by over 100x, reduce planning accuracy by up to 30%, and lower coordination success from above 90% to below 30%. Finally, we translate our findings into concrete architectural design principles and framework selection guidance, and outline promising future research directions.

</details>


### [36] [General Agents Contain World Models, even under Partial Observability and Stochasticity](https://arxiv.org/abs/2602.03146)
*Santiago Cifuentes*

Main category: cs.AI

TL;DR: 本文扩展了先前关于智能体环境建模的理论，从确定性、完全可观测环境扩展到随机性、部分可观测环境，证明随机智能体也无法避免学习其环境模型。


<details>
  <summary>Details</summary>
Motivation: 先前研究证明在确定性、完全可观测环境中，几乎最优的通用智能体必然包含足够的环境知识。但现实世界中的智能体通常是随机的，且环境是部分可观测的。本文旨在移除这两个限制性假设，扩展理论结果到更现实的场景。

Method: 通过理论分析扩展先前框架，将定理推广到随机智能体在部分可观测环境中的情况。同时通过弱化"通用性"概念，证明能力较弱的智能体也包含其操作环境的世界模型。

Result: 成功证明随机智能体在部分可观测环境中也无法避免学习其环境模型，随机化使用无法避免环境学习。同时证明更弱的智能体（放宽通用性要求）也包含世界模型。

Conclusion: 随机智能体在部分可观测环境中仍然必然学习其环境模型，这为理解智能体能力提供了更普适的理论基础，表明环境建模是智能体行为的本质特征，不受随机性和部分可观测性的影响。

Abstract: Deciding whether an agent possesses a model of its surrounding world is a fundamental step toward understanding its capabilities and limitations. In [10], it was shown that, within a particular framework, every almost optimal and general agent necessarily contains sufficient knowledge of its environment to allow an approximate reconstruction of it by querying the agent as a black box. This result relied on the assumptions that the agent is deterministic and that the environment is fully observable.
  In this work, we remove both assumptions by extending the theorem to stochastic agents operating in partially observable environments. Fundamentally, this shows that stochastic agents cannot avoid learning their environment through the usage of randomization. We also strengthen the result by weakening the notion of generality, proving that less powerful agents already contain a model of the world in which they operate.

</details>


### [37] [Enhancing Foundation VLM Robustness to Missing Modality: Scalable Diffusion for Bi-directional Feature Restoration](https://arxiv.org/abs/2602.03151)
*Wei Dai,Haoyu Wang,Honghao Chang,Lijun He,Fan Li,Jian Sun,Haixia Bi*

Main category: cs.AI

TL;DR: 本文提出了一种通用的缺失模态恢复策略，通过增强扩散模型作为可插拔的中阶段训练模块，有效恢复视觉语言模型中的缺失特征，提升在模态不完整情况下的性能。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型通常假设推理时具有完整的模态输入，但当某些模态不可用或不完整时，其性能会急剧下降。现有方法面临两个困境：基于提示的方法难以恢复缺失但必要的特征并损害VLM的泛化能力；基于插补的方法缺乏有效指导，容易生成语义无关的噪声。如何在恢复精确语义的同时保持VLM的泛化能力仍然具有挑战性。

Method: 提出一种通用的缺失模态恢复策略，引入增强扩散模型作为可插拔的中阶段训练模块。包含两个关键创新：(1)动态模态门控，自适应地利用条件特征来引导生成语义一致的特征；(2)跨模态互学习机制，桥接双编码器的语义空间以实现双向对齐。

Result: 在基准数据集上的零样本评估表明，该方法优于现有的基线方法。大量实验和消融研究证实，该模型是VLM在缺失模态场景下的鲁棒且可扩展的扩展，确保在不同缺失率和环境下的可靠性。

Conclusion: 本文提出的通用缺失模态恢复策略通过动态模态门控和跨模态互学习机制，有效解决了VLM在模态不完整情况下的性能下降问题，为VLM在现实世界应用中的可靠性提供了保障。

Abstract: Vision Language Models (VLMs) typically assume complete modality input during inference. However, their effectiveness drops sharply when certain modalities are unavailable or incomplete. Current research primarily faces two dilemmas: Prompt-based methods struggle to restore missing yet indispensable features and impair generalization of VLMs. Imputation-based approaches, lacking effective guidance, are prone to generating semantically irrelevant noise. Restoring precise semantics while sustaining VLM generalization remains challenging. Therefore, we propose a general missing modality restoration strategy in this paper. We introduce an enhanced diffusion model as a pluggable mid-stage training module to effectively restore missing features. Our strategy introduces two key innovations: (I) Dynamic Modality Gating, which adaptively leverages conditional features to steer the generation of semantically consistent features; (II) Cross-Modal Mutual Learning mechanism, which bridges the semantic spaces of dual encoders to achieve bidirectional alignment. Zero-shot evaluations across benchmark datasets demonstrate that our approach outperforms existing baseline methods. Extensive experiments and ablation studies confirm our model as a robust and scalable extension for VLMs in missing modality scenarios, ensuring reliability across diverse missing rates and environments. Our code and models will be publicly available.

</details>


### [38] [VALUEFLOW: Toward Pluralistic and Steerable Value-based Alignment in Large Language Models](https://arxiv.org/abs/2602.03160)
*Woojin Kim,Sieun Hyeon,Jusang Oh,Jaeyoung Do*

Main category: cs.AI

TL;DR: VALUEFLOW框架：首个统一的价值提取、评估和强度可控引导框架，解决LLM价值对齐中的层次结构、强度校准和可控引导问题


<details>
  <summary>Details</summary>
Motivation: 当前LLM与人类价值对齐存在三个关键问题：1) 价值提取忽略层次结构；2) 评估只能检测存在性而无法校准强度；3) 强度可控的引导机制理解不足

Method: 提出VALUEFLOW框架，包含三个组件：1) HIVES层次价值嵌入空间；2) VIDB大规模价值标注文本数据库；3) 基于锚点的评估器，通过排名产生一致性强度分数

Result: 在10个模型和4种价值理论中进行大规模研究，识别了引导的不对称性和多价值控制的组合规律

Conclusion: VALUEFLOW为评估和控制价值强度建立了可扩展的基础设施，推进了LLM的多元价值对齐

Abstract: Aligning Large Language Models (LLMs) with the diverse spectrum of human values remains a central challenge: preference-based methods often fail to capture deeper motivational principles. Value-based approaches offer a more principled path, yet three gaps persist: extraction often ignores hierarchical structure, evaluation detects presence but not calibrated intensity, and the steerability of LLMs at controlled intensities remains insufficiently understood. To address these limitations, we introduce VALUEFLOW, the first unified framework that spans extraction, evaluation, and steering with calibrated intensity control. The framework integrates three components: (i) HIVES, a hierarchical value embedding space that captures intra- and cross-theory value structure; (ii) the Value Intensity DataBase (VIDB), a large-scale resource of value-labeled texts with intensity estimates derived from ranking-based aggregation; and (iii) an anchor-based evaluator that produces consistent intensity scores for model outputs by ranking them against VIDB panels. Using VALUEFLOW, we conduct a comprehensive large-scale study across ten models and four value theories, identifying asymmetries in steerability and composition laws for multi-value control. This paper establishes a scalable infrastructure for evaluating and controlling value intensity, advancing pluralistic alignment of LLMs.

</details>


### [39] [Beyond Quantity: Trajectory Diversity Scaling for Code Agents](https://arxiv.org/abs/2602.03219)
*Guhong Chen,Chenghao Sun,Cheng Fu,Qiyao Wang,Zhihong Huang,Chaopeng Wei,Guangxu Chen,Feiteng Fang,Ahmadreza Argha,Bing Zhao,Xander Xu,Qi Han,Hamid Alinejad-Rokny,Qiang Qu,Binhua Li,Shiwen Ni,Min Yang,Hu Wei,Yongbin Li*

Main category: cs.AI

TL;DR: TDScaling提出了一种基于轨迹多样性的代码智能体数据合成框架，通过增加轨迹多样性而非单纯增加数据量来提升性能，解决了现有方法中低质量合成数据和数量扩展收益递减的问题。


<details>
  <summary>Details</summary>
Motivation: 随着代码大语言模型通过MCP协议演变为工具交互智能体，其泛化能力受到低质量合成数据和数量扩展收益递减的限制。数量为中心的扩展存在早期瓶颈，未能充分利用轨迹数据。

Method: TDScaling框架包含四个创新：1）业务集群机制捕捉真实服务逻辑依赖；2）蓝图驱动的多智能体范式确保轨迹连贯性；3）自适应进化机制使用领域熵、推理模式熵和累积动作复杂度引导合成面向长尾场景；4）沙盒化代码工具防止内在编码能力灾难性遗忘。

Result: 在通用工具使用基准（BFCL、tau^2-Bench）和代码智能体任务（RebenchT、CodeCI、BIRD）上的实验表明，TDScaling实现了双赢：既提升了工具使用泛化能力，又增强了内在编码能力。

Conclusion: TDScaling通过轨迹多样性扩展而非数量扩展，在固定训练预算下实现了更好的性能-成本权衡，为代码智能体训练提供了更高效的数据合成框架。

Abstract: As code large language models (LLMs) evolve into tool-interactive agents via the Model Context Protocol (MCP), their generalization is increasingly limited by low-quality synthetic data and the diminishing returns of quantity scaling. Moreover, quantity-centric scaling exhibits an early bottleneck that underutilizes trajectory data. We propose TDScaling, a Trajectory Diversity Scaling-based data synthesis framework for code agents that scales performance through diversity rather than raw volume. Under a fixed training budget, increasing trajectory diversity yields larger gains than adding more trajectories, improving the performance-cost trade-off for agent training. TDScaling integrates four innovations: (1) a Business Cluster mechanism that captures real-service logical dependencies; (2) a blueprint-driven multi-agent paradigm that enforces trajectory coherence; (3) an adaptive evolution mechanism that steers synthesis toward long-tail scenarios using Domain Entropy, Reasoning Mode Entropy, and Cumulative Action Complexity to prevent mode collapse; and (4) a sandboxed code tool that mitigates catastrophic forgetting of intrinsic coding capabilities. Experiments on general tool-use benchmarks (BFCL, tau^2-Bench) and code agent tasks (RebenchT, CodeCI, BIRD) demonstrate a win-win outcome: TDScaling improves both tool-use generalization and inherent coding proficiency. We plan to release the full codebase and the synthesized dataset (including 30,000+ tool clusters) upon publication.

</details>


### [40] [TAME: A Trustworthy Test-Time Evolution of Agent Memory with Systematic Benchmarking](https://arxiv.org/abs/2602.03224)
*Yu Cheng,Jiuan Zhou,Yongkang Hu,Yihang Chen,Huichi Zhou,Mingang Chen,Zhizhong Zhang,Kun Shao,Yuan Xie,Zhaoxia Yin*

Main category: cs.AI

TL;DR: 论文提出TAME框架解决智能体记忆演化中的安全问题，通过双记忆系统分别优化任务执行和安全评估，在保持任务性能的同时提升可信度。


<details>
  <summary>Details</summary>
Motivation: 智能体在测试时间通过记忆演化积累经验是实现AGI的关键途径，但即使在进行良性任务演化时，智能体的安全对齐仍然脆弱，存在"Agent Memory Misevolution"现象，即记忆演化过程中可信度下降的问题。

Method: 提出TAME双记忆演化框架：1）执行器记忆演化以提升任务性能，通过提炼可泛化的方法论；2）评估器记忆演化以基于历史反馈精化安全和任务效用的评估。通过记忆过滤、草稿生成、可信度精化、执行和双轨记忆更新的闭环流程。

Result: 构建了Trust-Memevo基准评估良性任务演化中的多维可信度，发现各种任务领域和评估设置中可信度普遍下降。TAME框架实验表明能够缓解记忆演化问题，在可信度和任务性能上实现联合提升。

Conclusion: TAME框架通过分离执行器和评估器记忆演化，在记忆过滤、生成、精化和更新的闭环中保持可信度而不牺牲效用，为解决智能体记忆演化中的安全问题提供了有效方案。

Abstract: Test-time evolution of agent memory serves as a pivotal paradigm for achieving AGI by bolstering complex reasoning through experience accumulation. However, even during benign task evolution, agent safety alignment remains vulnerable-a phenomenon known as Agent Memory Misevolution. To evaluate this phenomenon, we construct the Trust-Memevo benchmark to assess multi-dimensional trustworthiness during benign task evolution, revealing an overall decline in trustworthiness across various task domains and evaluation settings. To address this issue, we propose TAME, a dual-memory evolutionary framework that separately evolves executor memory to improve task performance by distilling generalizable methodologies, and evaluator memory to refine assessments of both safety and task utility based on historical feedback. Through a closed loop of memory filtering, draft generation, trustworthy refinement, execution, and dual-track memory updating, TAME preserves trustworthiness without sacrificing utility. Experiments demonstrate that TAME mitigates misevolution, achieving a joint improvement in both trustworthiness and task performance.

</details>


### [41] [The Necessity of a Unified Framework for LLM-Based Agent Evaluation](https://arxiv.org/abs/2602.03238)
*Pengyu Zhu,Li Sun,Philip S. Yu,Sen Su*

Main category: cs.AI

TL;DR: 该论文指出当前大语言模型智能体评估存在标准化不足的问题，提出了统一评估框架的必要性


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的发展，通用智能体取得了根本性进展，但评估这些智能体面临独特挑战。当前评估存在系统提示、工具配置、环境动态等混杂因素，缺乏标准化导致不公平和不透明

Method: 提出标准化智能体评估的提案，旨在建立统一评估框架来解决现有评估中的标准化不足问题

Result: 论文主要提出了标准化评估框架的提案，但具体实施结果和效果尚未在摘要中详细说明

Conclusion: 标准化评估框架对于智能体评估的严谨发展至关重要，需要建立统一标准来确保公平性、可重复性和透明度

Abstract: With the advent of Large Language Models (LLMs), general-purpose agents have seen fundamental advancements. However, evaluating these agents presents unique challenges that distinguish them from static QA benchmarks. We observe that current agent benchmarks are heavily confounded by extraneous factors, including system prompts, toolset configurations, and environmental dynamics. Existing evaluations often rely on fragmented, researcher-specific frameworks where the prompt engineering for reasoning and tool usage varies significantly, making it difficult to attribute performance gains to the model itself. Additionally, the lack of standardized environmental data leads to untraceable errors and non-reproducible results. This lack of standardization introduces substantial unfairness and opacity into the field. We propose that a unified evaluation framework is essential for the rigorous advancement of agent evaluation. To this end, we introduce a proposal aimed at standardizing agent evaluation.

</details>


### [42] [Accordion-Thinking: Self-Regulated Step Summaries for Efficient and Readable LLM Reasoning](https://arxiv.org/abs/2602.03249)
*Zhicheng Yang,Zhijiang Guo,Yinya Huang,Yongxin Wang,Wenlei Shi,Yiwei Wang,Xiaodan Liang,Jing Tang*

Main category: cs.AI

TL;DR: Accordion-Thinking框架让LLM学会通过动态总结来自我调节推理步骤粒度，实现推理上下文的压缩，在保持准确性的同时显著提升推理效率。


<details>
  <summary>Details</summary>
Motivation: 传统的长链式思维推理虽然能显著提升推理能力，但面临KV缓存线性增长和注意力复杂度二次方增长的实践限制，需要更高效的推理机制。

Method: 提出Accordion-Thinking端到端框架，让LLM学习通过动态总结来自我调节推理步骤粒度，采用Fold推理模式定期总结思维过程并丢弃历史token，使用强化学习进一步激励这种能力。

Result: 训练过程中，高效的Fold模式与详尽Unfold模式之间的准确率差距逐渐缩小直至消失，模型学会将关键推理信息编码到紧凑总结中，在48GB GPU内存配置下实现3倍吞吐量同时保持准确性。

Conclusion: 通过学习的自我压缩，LLM能够以最小的依赖token开销处理复杂推理任务而不影响解决方案质量，结构化步骤总结为推理过程提供了人类可读的说明。

Abstract: Scaling test-time compute via long Chain-ofThought unlocks remarkable gains in reasoning capabilities, yet it faces practical limits due to the linear growth of KV cache and quadratic attention complexity. In this paper, we introduce Accordion-Thinking, an end-to-end framework where LLMs learn to self-regulate the granularity of the reasoning steps through dynamic summarization. This mechanism enables a Fold inference mode, where the model periodically summarizes its thought process and discards former thoughts to reduce dependency on historical tokens. We apply reinforcement learning to incentivize this capability further, uncovering a critical insight: the accuracy gap between the highly efficient Fold mode and the exhaustive Unfold mode progressively narrows and eventually vanishes over the course of training. This phenomenon demonstrates that the model learns to encode essential reasoning information into compact summaries, achieving effective compression of the reasoning context. Our Accordion-Thinker demonstrates that with learned self-compression, LLMs can tackle complex reasoning tasks with minimal dependency token overhead without compromising solution quality, and it achieves a 3x throughput while maintaining accuracy on a 48GB GPU memory configuration, while the structured step summaries provide a human-readable account of the reasoning process.

</details>


### [43] [CSR-Bench: A Benchmark for Evaluating the Cross-modal Safety and Reliability of MLLMs](https://arxiv.org/abs/2602.03263)
*Yuxuan Liu,Yuntian Shi,Kun Wang,Haoting Shen,Kun Yang*

Main category: cs.AI

TL;DR: CSR-Bench是一个评估多模态大语言模型跨模态可靠性的基准测试，通过四种压力测试模式（安全、过度拒绝、偏见、幻觉）评估61种细粒度类型，发现现有模型存在系统性跨模态对齐差距。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型虽然支持文本和图像交互，但其安全行为可能由单模态捷径驱动而非真正的联合意图理解。需要评估模型是否真正整合了多模态信息，还是仅依赖单模态线索做出决策。

Method: 构建CSR-Bench基准，包含四种压力测试交互模式：安全、过度拒绝、偏见和幻觉，涵盖61种细粒度类型。每个实例都需要整合图像-文本解释，并提供配对的纯文本控制组以诊断模态引起的行为变化。评估了16个最先进的多模态大语言模型。

Result: 观察到系统性跨模态对齐差距：模型表现出弱安全意识、在干扰下强烈的语言主导性，以及从纯文本控制组到多模态输入的一致性能下降。还观察到减少过度拒绝与保持安全、非歧视行为之间的明显权衡，表明一些表面上的安全改进可能来自拒绝导向的启发式方法而非鲁棒的意图理解。

Conclusion: 多模态大语言模型在跨模态可靠性方面存在显著问题，其安全行为可能基于单模态捷径而非真正的多模态理解。需要开发更鲁棒的多模态对齐方法，而不仅仅是依赖拒绝启发式。

Abstract: Multimodal large language models (MLLMs) enable interaction over both text and images, but their safety behavior can be driven by unimodal shortcuts instead of true joint intent understanding. We introduce CSR-Bench, a benchmark for evaluating cross-modal reliability through four stress-testing interaction patterns spanning Safety, Over-rejection, Bias, and Hallucination, covering 61 fine-grained types. Each instance is constructed to require integrated image-text interpretation, and we additionally provide paired text-only controls to diagnose modality-induced behavior shifts. We evaluate 16 state-of-the-art MLLMs and observe systematic cross-modal alignment gaps. Models show weak safety awareness, strong language dominance under interference, and consistent performance degradation from text-only controls to multimodal inputs. We also observe a clear trade-off between reducing over-rejection and maintaining safe, non-discriminatory behavior, suggesting that some apparent safety gains may come from refusal-oriented heuristics rather than robust intent understanding. WARNING: This paper contains unsafe contents.

</details>


### [44] [Rejecting Arguments Based on Doubt in Structured Bipolar Argumentation](https://arxiv.org/abs/2602.03286)
*Michael A. Müller,Srdjan Vesic,Bruno Yun*

Main category: cs.AI

TL;DR: 本文提出了一种受哲学和语言学启发的计算论证新方法，引入结构化双极论证框架(SBAF)，允许基于怀疑拒绝论证，并提供语言扩展语义。


<details>
  <summary>Details</summary>
Motivation: 现有计算论证方法存在两个局限：1) 要求接受所有可辩护的论证，不允许基于怀疑拒绝；2) 主要关注论证层面，而非个体句子或主张。本文旨在将哲学和语言学中的两个重要观点纳入计算论证框架。

Method: 首先定义结构化双极论证框架(SBAF)，其中论证由句子组成，包含攻击和支持关系。然后提出新的语义学，具有两个特征：1) 不强制接受所有可辩护的论证；2) 除了论证扩展外，还提供语言扩展语义，指定可接受的句子集合。

Result: 提出的语义学位于抽象论证的可容许语义和完全语义之间，能够表示辩论中合理的立场。该方法为现有方法提供了新视角：可以指定忽略论证间支持的条件，并证明演绎支持语义是本文方法的特例。

Conclusion: 本文开发了一种更符合人类推理实际的计算论证方法，允许基于怀疑拒绝论证，并在论证和语言层面提供语义学，为计算论证领域提供了更灵活和自然的框架。

Abstract: This paper develops a new approach to computational argumentation that is informed by philosophical and linguistic views. Namely, it takes into account two ideas that have received little attention in the literature on computational argumentation: First, an agent may rationally reject an argument based on mere doubt, thus not all arguments they could defend must be accepted; and, second, that it is sometimes more natural to think in terms of which individual sentences or claims an agent accepts in a debate, rather than which arguments. In order to incorporate these two ideas into a computational approach, we first define the notion of structured bipolar argumentation frameworks (SBAFs), where arguments consist of sentences and we have both an attack and a support relation between them. Then, we provide semantics for SBAFs with two features: (1) Unlike with completeness-based semantics, our semantics do not force agents to accept all defended arguments. (2) In addition to argument extensions, which give acceptable sets of arguments, we also provide semantics for language extensions that specify acceptable sets of sentences. These semantics represent reasonable positions an agent might have in a debate. Our semantics lie between the admissible and complete semantics of abstract argumentation. Further, our approach can be used to provide a new perspective on existing approaches. For instance, we can specify the conditions under which an agent can ignore support between arguments (i.e. under which the use of abstract argumentation is warranted) and we show that deductive support semantics is a special case of our approach.

</details>


### [45] [Memora: A Harmonic Memory Representation Balancing Abstraction and Specificity](https://arxiv.org/abs/2602.03315)
*Menglin Xia,Xuchao Zhang,Shantanu Dixit,Paramaguru Harimurugan,Rujia Wang,Victor Ruhle,Robert Sim,Chetan Bansal,Saravan Rajmohan*

Main category: cs.AI

TL;DR: Memora是一种平衡抽象与具体性的记忆表示方法，通过主要抽象索引具体记忆值，使用线索锚点扩展检索访问，并通过主动利用记忆连接的检索策略提升检索相关性。


<details>
  <summary>Details</summary>
Motivation: 智能体记忆系统需要处理不断增长的信息，同时支持高效、上下文感知的检索。抽象对于扩展记忆规模至关重要，但通常以牺牲具体性为代价，这会掩盖有效推理所需的细粒度细节。

Method: 引入Memora，一种谐波记忆表示，通过主要抽象索引具体记忆值并整合相关更新为统一记忆条目，同时使用线索锚点扩展跨不同记忆方面的检索访问并连接相关记忆。基于此结构，采用主动利用这些记忆连接的检索策略来检索超出直接语义相似性的相关信息。

Result: 理论上，标准检索增强生成（RAG）和知识图谱（KG）记忆系统被证明是Memora框架的特例。实证上，Memora在LoCoMo和LongMemEval基准测试中建立了新的最先进水平，展示了随着记忆规模扩大更好的检索相关性和推理有效性。

Conclusion: Memora通过谐波记忆表示结构性地平衡了抽象与具体性，解决了记忆系统在扩展规模时面临的关键挑战，为智能体记忆系统提供了更有效的解决方案。

Abstract: Agent memory systems must accommodate continuously growing information while supporting efficient, context-aware retrieval for downstream tasks. Abstraction is essential for scaling agent memory, yet it often comes at the cost of specificity, obscuring the fine-grained details required for effective reasoning. We introduce Memora, a harmonic memory representation that structurally balances abstraction and specificity. Memora organizes information via its primary abstractions that index concrete memory values and consolidate related updates into unified memory entries, while cue anchors expand retrieval access across diverse aspects of the memory and connect related memories. Building on this structure, we employ a retrieval policy that actively exploits these memory connections to retrieve relevant information beyond direct semantic similarity. Theoretically, we show that standard Retrieval-Augmented Generation (RAG) and Knowledge Graph (KG)-based memory systems emerge as special cases of our framework. Empirically, Memora establishes a new state-of-the-art on the LoCoMo and LongMemEval benchmarks, demonstrating better retrieval relevance and reasoning effectiveness as memory scales.

</details>


### [46] [MentalSeek-Dx: Towards Progressive Hypothetico-Deductive Reasoning for Real-world Psychiatric Diagnosis](https://arxiv.org/abs/2602.03340)
*Xiao Sun,Yuming Yang,Junnan Zhu,Jiang Zhong,Xinyu Zhou,Kaiwen Wei*

Main category: cs.AI

TL;DR: MentalDx Bench是首个面向真实临床场景的精神障碍诊断基准，包含712份电子病历，覆盖76种ICD-11疾病。研究发现LLMs在粗粒度分类表现良好，但在细粒度诊断上存在系统性失败。提出的MentalSeek-Dx模型通过临床推理训练，在14B参数下达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在精神健康评估中的应用受到基准测试生态效度不足和缺乏细粒度诊断监督的限制。需要建立更贴近真实临床场景的评估框架，以准确衡量LLMs在精神障碍诊断中的实际能力。

Method: 1) 构建MentalDx Bench基准：包含712份去标识化电子健康记录，由认证精神科医生按照ICD-11标准标注，覆盖16个诊断类别的76种疾病；2) 评估18个LLMs，发现范式错配问题；3) 提出MentalSeek-Dx模型：通过监督轨迹构建和课程式强化学习，使模型内化临床假设-演绎推理过程。

Result: 1) LLMs在粗粒度诊断分类上表现良好，但在细粒度障碍级别诊断上系统性失败；2) MentalSeek-Dx在MentalDx Bench上达到最先进性能，仅使用14B参数；3) 揭示了LLMs基于模式建模与临床推理之间的关键差距。

Conclusion: 该研究建立了首个临床基础的精神障碍诊断基准，揭示了LLMs在细粒度诊断中的局限性，并提出了有效的解决方案。MentalSeek-Dx为可靠的精神科诊断提供了临床基础框架，推动了AI在精神健康领域的实际应用。

Abstract: Mental health disorders represent a burgeoning global public health challenge. While Large Language Models (LLMs) have demonstrated potential in psychiatric assessment, their clinical utility is severely constrained by benchmarks that lack ecological validity and fine-grained diagnostic supervision. To bridge this gap, we introduce \textbf{MentalDx Bench}, the first benchmark dedicated to disorder-level psychiatric diagnosis within real-world clinical settings. Comprising 712 de-identified electronic health records annotated by board-certified psychiatrists under ICD-11 guidelines, the benchmark covers 76 disorders across 16 diagnostic categories. Evaluation of 18 LLMs reveals a critical \textit{paradigm misalignment}: strong performance at coarse diagnostic categorization contrasts with systematic failure at disorder-level diagnosis, underscoring a gap between pattern-based modeling and clinical hypothetico-deductive reasoning. In response, we propose \textbf{MentalSeek-Dx}, a medical-specialized LLM trained to internalize this clinical reasoning process through supervised trajectory construction and curriculum-based reinforcement learning. Experiments on MentalDx Bench demonstrate that MentalSeek-Dx achieves state-of-the-art (SOTA) performance with only 14B parameters, establishing a clinically grounded framework for reliable psychiatric diagnosis.

</details>


### [47] [Risk Awareness Injection: Calibrating Vision-Language Models for Safety without Compromising Utility](https://arxiv.org/abs/2602.03402)
*Mengxuan Wang,Yuxin Chen,Gang Xu,Tao He,Hongjie Jiang,Ming Li*

Main category: cs.AI

TL;DR: RAI是一种轻量级、无需训练的安全校准框架，通过放大视觉语言模型中的不安全信号来恢复类似LLM的风险识别能力，有效防御多模态越狱攻击而不影响任务性能。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在多模态环境下容易受到越狱攻击，现有防御方法要么需要大量训练成本，要么会显著降低模型效用。研究发现LLMs本身能识别文本中的不安全内容，但视觉输入会稀释风险信号。

Method: 提出风险感知注入框架：1）从语言嵌入构建不安全原型子空间；2）对选定的高风险视觉令牌进行针对性调制，在跨模态特征空间中显式激活安全关键信号；3）保持原始令牌的语义完整性以支持跨模态推理。

Result: 在多个越狱攻击和效用基准测试中，RAI显著降低了攻击成功率，同时没有损害任务性能表现。

Conclusion: RAI通过恢复视觉语言模型的LLM式风险识别能力，提供了一种轻量级、无需训练的安全校准解决方案，有效防御多模态越狱攻击并保持模型效用。

Abstract: Vision language models (VLMs) extend the reasoning capabilities of large language models (LLMs) to cross-modal settings, yet remain highly vulnerable to multimodal jailbreak attacks. Existing defenses predominantly rely on safety fine-tuning or aggressive token manipulations, incurring substantial training costs or significantly degrading utility. Recent research shows that LLMs inherently recognize unsafe content in text, and the incorporation of visual inputs in VLMs frequently dilutes risk-related signals. Motivated by this, we propose Risk Awareness Injection (RAI), a lightweight and training-free framework for safety calibration that restores LLM-like risk recognition by amplifying unsafe signals in VLMs. Specifically, RAI constructs an Unsafe Prototype Subspace from language embeddings and performs targeted modulation on selected high-risk visual tokens, explicitly activating safety-critical signals within the cross-modal feature space. This modulation restores the model's LLM-like ability to detect unsafe content from visual inputs, while preserving the semantic integrity of original tokens for cross-modal reasoning. Extensive experiments across multiple jailbreak and utility benchmarks demonstrate that RAI substantially reduces attack success rate without compromising task performance.

</details>


### [48] [Feasible strategies for conflict resolution within intuitionistic fuzzy preference-based conflict situations](https://arxiv.org/abs/2602.03403)
*Guangming Lang,Mingchuan Shang,Mengjun Hu,Jie Zhou,Feng Xu*

Main category: cs.AI

TL;DR: 该论文提出了一种基于直觉模糊偏好的三支冲突分析模型，通过引入直觉模糊偏好关系来更精细地描述智能体对议题对的态度，并构建了相应的冲突度量和三支决策模型。


<details>
  <summary>Details</summary>
Motivation: 现有基于偏好的冲突分析模型仅使用偏好、逆偏好和中性三种定性关系来描述智能体对议题对的态度，这种粗粒度的表示方法限制了准确捕捉冲突本质的能力。

Method: 引入直觉模糊偏好冲突情境概念，构建直觉模糊偏好冲突度量，建立三支冲突分析模型对智能体对、智能体集合和议题集合进行三分，使用相对损失函数计算阈值，并提出基于调整机制的可行策略构建算法。

Result: 开发了一个更精细的冲突分析框架，能够更准确地描述智能体态度，构建了完整的三支冲突分析模型，并通过示例验证了模型的有效性。

Conclusion: 提出的直觉模糊偏好冲突分析模型相比传统模型具有更强的表达能力，能够更精细地描述冲突情境，为复杂冲突分析提供了有效的理论框架和实用工具。

Abstract: In three-way conflict analysis, preference-based conflict situations characterize agents' attitudes towards issues by formally modeling their preferences over pairs of issues. However, existing preference-based conflict models rely exclusively on three qualitative relations, namely, preference, converse, and indifference, to describe agents' attitudes towards issue pairs, which significantly limits their capacity in capturing the essence of conflict. To overcome this limitation, we introduce the concept of an intuitionistic fuzzy preference-based conflict situation that captures agents' attitudes towards issue pairs with finer granularity than that afforded by classical preference-based models. Afterwards, we develop intuitionistic fuzzy preference-based conflict measures within this framework, and construct three-way conflict analysis models for trisecting the set of agent pairs, the agent set, and the issue set. Additionally, relative loss functions built on the proposed conflict functions are employed to calculate thresholds for three-way conflict analysis. Finally, we present adjustment mechanism-based feasible strategies that simultaneously account for both adjustment magnitudes and conflict degrees, together with an algorithm for constructing such feasible strategies, and provide an illustrative example to demonstrate the validity and effectiveness of the proposed model.

</details>


### [49] [DiscoverLLM: From Executing Intents to Discovering Them](https://arxiv.org/abs/2602.03429)
*Tae Soo Kim,Yoonjoo Lee,Jaesang Yu,John Joon Young Chung,Juho Kim*

Main category: cs.AI

TL;DR: DiscoverLLM是一个训练大语言模型帮助用户形成和发现意图的框架，通过模拟用户认知状态并优化意图具体化程度作为奖励信号，在意图不清晰时探索选项，在意图具体化时细化实施。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在处理模糊和开放式请求时，通常只是询问用户澄清问题，但当用户自己也不知道自己想要什么时，这种方法就会失效。用户之所以模糊是因为他们还没有形成明确的意图，需要通过观察和探索结果来发现自己真正想要的东西。

Method: 提出了DiscoverLLM框架，核心是一个新颖的用户模拟器，用层次化的意图结构建模认知状态，意图随着模型展示相关选项而逐步具体化。意图具体化的程度作为奖励信号，模型可以训练优化这个信号。模型学习与用户协作，在意图不清晰时发散探索选项，在意图具体化时收敛细化实施。

Result: 在创意写作、技术写作和SVG绘图等交互式基准测试中，DiscoverLLM实现了超过10%的任务性能提升，同时将对话长度减少了高达40%。在75名人类参与者的用户研究中，DiscoverLLM相比基线方法提高了对话满意度和效率。

Conclusion: DiscoverLLM提供了一个通用框架，训练大语言模型帮助用户形成和发现意图，通过建模用户认知状态和优化意图具体化过程，显著提升了交互式任务的性能和效率。

Abstract: To handle ambiguous and open-ended requests, Large Language Models (LLMs) are increasingly trained to interact with users to surface intents they have not yet expressed (e.g., ask clarification questions). However, users are often ambiguous because they have not yet formed their intents: they must observe and explore outcomes to discover what they want. Simply asking "what kind of tone do you want?" fails when users themselves do not know. We introduce DiscoverLLM, a novel and generalizable framework that trains LLMs to help users form and discover their intents. Central to our approach is a novel user simulator that models cognitive state with a hierarchy of intents that progressively concretize as the model surfaces relevant options -- where the degree of concretization serves as a reward signal that models can be trained to optimize. Resulting models learn to collaborate with users by adaptively diverging (i.e., explore options) when intents are unclear, and converging (i.e., refine and implement) when intents concretize. Across proposed interactive benchmarks in creative writing, technical writing, and SVG drawing, DiscoverLLM achieves over 10% higher task performance while reducing conversation length by up to 40%. In a user study with 75 human participants, DiscoverLLM improved conversation satisfaction and efficiency compared to baselines.

</details>


### [50] [CRL-VLA: Continual Vision-Language-Action Learning](https://arxiv.org/abs/2602.03445)
*Qixin Zeng,Shuo Zhang,Hongyin Zhang,Renjie Wang,Han Zhao,Libang Zhao,Runze Li,Donglin Wang,Chao Huang*

Main category: cs.AI

TL;DR: CRL-VLA是一个用于视觉语言动作模型持续强化学习的框架，通过理论推导和双评论家架构解决稳定性与可塑性权衡问题，在LIBERO基准测试中表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 在开放世界环境中，终身学习对具身智能体至关重要。持续强化学习是将视觉语言动作模型部署到终身机器人场景的有前景途径，但现有方法在平衡稳定性（保留旧技能）和可塑性（学习新技能）方面面临巨大挑战。

Method: 提出CRL-VLA框架，通过理论推导将稳定性-可塑性权衡与目标条件优势幅度和策略散度联系起来。采用非对称调节机制：约束先前任务的优势幅度，同时允许新任务上的受控增长。实现方式是通过具有新颖目标条件价值公式的双评论家架构，其中冻结评论家锚定语义一致性，可训练估计器驱动适应。

Result: 在LIBERO基准测试上的实验表明，CRL-VLA有效协调了这些冲突目标，在抗遗忘和向前适应两方面均优于基线方法。

Conclusion: CRL-VLA为视觉语言动作模型的持续后训练提供了一个具有严格理论界限的框架，成功解决了持续强化学习中稳定性与可塑性的权衡问题，为终身机器人学习提供了有效解决方案。

Abstract: Lifelong learning is critical for embodied agents in open-world environments, where reinforcement learning fine-tuning has emerged as an important paradigm to enable Vision-Language-Action (VLA) models to master dexterous manipulation through environmental interaction. Thus, Continual Reinforcement Learning (CRL) is a promising pathway for deploying VLA models in lifelong robotic scenarios, yet balancing stability (retaining old skills) and plasticity (learning new ones) remains a formidable challenge for existing methods. We introduce CRL-VLA, a framework for continual post-training of VLA models with rigorous theoretical bounds. We derive a unified performance bound linking the stability-plasticity trade-off to goal-conditioned advantage magnitude, scaled by policy divergence. CRL-VLA resolves this dilemma via asymmetric regulation: constraining advantage magnitudes on prior tasks while enabling controlled growth on new tasks. This is realized through a simple but effective dual-critic architecture with novel Goal-Conditioned Value Formulation (GCVF), where a frozen critic anchors semantic consistency and a trainable estimator drives adaptation. Experiments on the LIBERO benchmark demonstrate that CRL-VLA effectively harmonizes these conflicting objectives, outperforming baselines in both anti-forgetting and forward adaptation.

</details>


### [51] [The Dual Role of Abstracting over the Irrelevant in Symbolic Explanations: Cognitive Effort vs. Understanding](https://arxiv.org/abs/2602.03467)
*Zeynep G. Saribatur,Johannes Langer,Ute Schmid*

Main category: cs.AI

TL;DR: 该研究探讨了形式化抽象（移除和聚类）如何影响人类推理性能和认知负荷，使用答案集编程作为形式框架，通过认知实验验证抽象化能提升符号解释的人类可理解性。


<details>
  <summary>Details</summary>
Motivation: 虽然符号AI为可解释性提供了透明基础，但原始逻辑轨迹往往带来较高的外在认知负荷。需要研究如何通过形式化抽象来改善人类对AI系统输出的理解。

Method: 使用答案集编程（ASP）作为形式框架，定义不相关细节的概念进行抽象化（移除和聚类）。通过认知实验，让参与者使用从答案集程序导出的解释对跨领域刺激进行分类。

Result: 实验显示：聚类细节显著提高参与者的理解能力，而移除细节则显著降低认知负荷，支持抽象化能增强以人为中心的符号解释这一假设。

Conclusion: 形式化抽象（特别是移除和聚类）能有效提升人类对符号AI解释的理解，减少认知负荷，为设计更人性化的可解释AI系统提供了实证支持。

Abstract: Explanations are central to human cognition, yet AI systems often produce outputs that are difficult to understand. While symbolic AI offers a transparent foundation for interpretability, raw logical traces often impose a high extraneous cognitive load. We investigate how formal abstractions, specifically removal and clustering, impact human reasoning performance and cognitive effort. Utilizing Answer Set Programming (ASP) as a formal framework, we define a notion of irrelevant details to be abstracted over to obtain simplified explanations. Our cognitive experiments, in which participants classified stimuli across domains with explanations derived from an answer set program, show that clustering details significantly improve participants' understanding, while removal of details significantly reduce cognitive effort, supporting the hypothesis that abstraction enhances human-centered symbolic explanations.

</details>


### [52] [When Routing Collapses: On the Degenerate Convergence of LLM Routers](https://arxiv.org/abs/2602.03478)
*Guannan Lai,Han-Jia Ye*

Main category: cs.AI

TL;DR: 论文发现现有LLM路由系统存在"路由崩溃"问题：随着成本预算增加，路由器会系统性地选择最强大但最昂贵的模型，即使更便宜的模型已经足够，导致小模型利用不足和成本浪费。作者提出EquiRouter直接学习模型排序来解决此问题。


<details>
  <summary>Details</summary>
Motivation: 现有LLM路由系统存在一个普遍但未被充分探索的失败模式：随着用户成本预算增加，路由器会系统性地默认选择最强大、最昂贵的模型，即使更便宜的模型已经足够。这导致小模型利用不足，浪费计算资源和金钱成本，违背了路由的核心承诺，作者称之为"路由崩溃"现象。

Method: 作者提出EquiRouter，一种决策感知的路由器，直接学习模型排序而非预测标量性能分数。该方法通过解决目标-决策不匹配问题来恢复小模型的作用并缓解路由崩溃。具体来说，它避免了因小预测误差导致相对排序翻转而触发次优选择的问题。

Result: 在RouterBench基准测试中，EquiRouter在达到GPT-4级别性能时，相比先前最强的路由器减少了约17%的成本。这表明EquiRouter能更有效地利用小模型，实现更好的质量-成本权衡。

Conclusion: 路由崩溃是LLM路由中普遍存在的系统性问题，源于目标-决策不匹配。EquiRouter通过直接学习模型排序而非预测标量分数，成功缓解了这一问题，在保持高性能的同时显著降低了成本，为LLM路由提供了更有效的解决方案。

Abstract: LLM routing aims to achieve a favorable quality--cost trade-off by dynamically assigning easy queries to smaller models and harder queries to stronger ones. However, across both unimodal and multimodal settings, we uncover a pervasive yet underexplored failure mode in existing routers: as the user's cost budget increases, routers systematically default to the most capable and most expensive model even when cheaper models already suffice. As a result, current routers under-utilize small models, wasting computation and monetary cost and undermining the core promise of routing; we term this phenomenon routing collapse. We attribute routing collapse to an objective--decision mismatch: many routers are trained to predict scalar performance scores, whereas routing decisions ultimately depend on discrete comparisons among candidate models. Consequently, small prediction errors can flip relative orderings and trigger suboptimal selections. To bridge this gap, we propose EquiRouter, a decision-aware router that directly learns model rankings, restoring the role of smaller models and mitigating routing collapse. On RouterBench, EquiRouter reduces cost by about 17\% at GPT-4-level performance compared to the strongest prior router. Our code is available at https://github.com/AIGNLAI/EquiRouter.

</details>


### [53] [EHRWorld: A Patient-Centric Medical World Model for Long-Horizon Clinical Trajectories](https://arxiv.org/abs/2602.03569)
*Linjie Mu,Zhongzhen Huang,Yannian Gu,Shengqian Qin,Shaoting Zhang,Xiaofan Zhang*

Main category: cs.AI

TL;DR: EHRWorld：基于因果序列范式训练的以患者为中心的医疗世界模型，显著优于单纯基于医学知识的LLM，在长时程临床模拟中表现更稳定


<details>
  <summary>Details</summary>
Motivation: 世界模型为干预下的未来状态模拟提供了原则性框架，但在医学等复杂高风险领域实现仍具挑战。虽然LLM在静态医学推理任务上表现良好，但能否作为动态医疗世界模型模拟疾病进展和治疗效果尚存疑问

Method: 提出EHRWorld，一种在因果序列范式下训练的以患者为中心的医疗世界模型，并构建了EHRWorld-110K——一个从真实世界电子健康记录中提取的大规模纵向临床数据集

Result: EHRWorld显著优于单纯的LLM基线，实现了更稳定的长时程模拟、更好的临床敏感事件建模以及更优的推理效率

Conclusion: 研究表明，基于因果基础、时间演化的临床数据进行训练对于构建可靠且鲁棒的医疗世界模型至关重要

Abstract: World models offer a principled framework for simulating future states under interventions, but realizing such models in complex, high-stakes domains like medicine remains challenging. Recent large language models (LLMs) have achieved strong performance on static medical reasoning tasks, raising the question of whether they can function as dynamic medical world models capable of simulating disease progression and treatment outcomes over time. In this work, we show that LLMs only incorporating medical knowledge struggle to maintain consistent patient states under sequential interventions, leading to error accumulation in long-horizon clinical simulation. To address this limitation, we introduce EHRWorld, a patient-centric medical world model trained under a causal sequential paradigm, together with EHRWorld-110K, a large-scale longitudinal clinical dataset derived from real-world electronic health records. Extensive evaluations demonstrate that EHRWorld significantly outperforms naive LLM-based baselines, achieving more stable long-horizon simulation, improved modeling of clinically sensitive events, and favorable reasoning efficiency, highlighting the necessity of training on causally grounded, temporally evolving clinical data for reliable and robust medical world modeling.

</details>


### [54] [TodyComm: Task-Oriented Dynamic Communication for Multi-Round LLM-based Multi-Agent System](https://arxiv.org/abs/2602.03688)
*Wenzhe Fan,Tommaso Tognoli,Henry Peng Zou,Chunyu Miao,Yibo Wang,Xinhua Zhang*

Main category: cs.AI

TL;DR: TodyComm：基于任务导向的动态通信算法，用于多轮LLM多智能体系统，能够根据每轮动态变化（如对抗者、任务进展、通信带宽限制）自适应调整通信拓扑结构。


<details>
  <summary>Details</summary>
Motivation: 现有多轮LLM多智能体系统大多采用固定的通信拓扑结构，这在实际应用中存在不足，因为智能体的角色可能因动态对抗者、任务进展或时变通信带宽限制而在不同轮次间发生变化。

Method: 提出TodyComm（任务导向动态通信算法），通过策略梯度优化，生成行为驱动的协作拓扑结构，使通信结构能够适应每轮的动态变化，从而优化任务效用。

Result: 在五个基准测试上的实验表明，在动态对抗者和通信预算约束下，TodyComm在任务效果方面表现优异，同时保持了令牌效率和可扩展性。

Conclusion: TodyComm通过动态调整通信拓扑结构，有效解决了多轮LLM多智能体系统中固定通信结构的局限性，在动态环境中实现了更好的任务性能和资源效率。

Abstract: Multi-round LLM-based multi-agent systems rely on effective communication structures to support collaboration across rounds. However, most existing methods employ a fixed communication topology during inference, which falls short in many realistic applications where the agents' roles may change \textit{across rounds} due to dynamic adversary, task progression, or time-varying constraints such as communication bandwidth. In this paper, we propose addressing this issue through TodyComm, a \textbf{t}ask-\textbf{o}riented \textbf{dy}namic \textbf{comm}unication algorithm. It produces behavior-driven collaboration topologies that adapt to the dynamics at each round, optimizing the utility for the task through policy gradient. Experiments on five benchmarks demonstrate that under both dynamic adversary and communications budgets, TodyComm delivers superior task effectiveness while retaining token efficiency and scalability.

</details>


### [55] [AOrchestra: Automating Sub-Agent Creation for Agentic Orchestration](https://arxiv.org/abs/2602.03786)
*Jianhao Ruan,Zhihao Xu,Yiran Peng,Fashen Ren,Zhaoyang Yu,Xinbing Liang,Jinyu Xiang,Bang Liu,Chenglin Wu,Yuyu Luo,Jiayi Zhang*

Main category: cs.AI

TL;DR: AOrchestra是一个统一的、框架无关的智能体编排系统，通过将智能体抽象为(指令、上下文、工具、模型)四元组，实现动态创建专用执行器，在多个基准测试中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有语言智能体系统在处理复杂、长时程任务时，缺乏对子智能体的动态抽象视图，导致适应性不足。需要一种统一的智能体抽象方法来提高系统的灵活性和可扩展性。

Method: 提出统一的智能体抽象：将任何智能体建模为(指令、上下文、工具、模型)四元组。基于此构建AOrchestra系统，其中中心编排器在每个步骤具体化这个四元组：策划任务相关上下文、选择工具和模型，并通过即时自动创建智能体来委托执行。

Result: 在三个具有挑战性的基准测试(GAIA、SWE-Bench、Terminal-Bench)中，AOrchestra与Gemini-3-Flash配对时，相对于最强基线实现了16.28%的相对改进。系统支持可控的性能-成本权衡，能够接近帕累托效率。

Conclusion: AOrchestra提供了一个框架无关的智能体编排系统，通过统一的智能体抽象实现动态任务执行，减少了人工工程工作量，同时支持多种智能体作为任务执行器的即插即用，在性能和成本之间实现了有效平衡。

Abstract: Language agents have shown strong promise for task automation. Realizing this promise for increasingly complex, long-horizon tasks has driven the rise of a sub-agent-as-tools paradigm for multi-turn task solving. However, existing designs still lack a dynamic abstraction view of sub-agents, thereby hurting adaptability. We address this challenge with a unified, framework-agnostic agent abstraction that models any agent as a tuple Instruction, Context, Tools, Model. This tuple acts as a compositional recipe for capabilities, enabling the system to spawn specialized executors for each task on demand. Building on this abstraction, we introduce an agentic system AOrchestra, where the central orchestrator concretizes the tuple at each step: it curates task-relevant context, selects tools and models, and delegates execution via on-the-fly automatic agent creation. Such designs enable reducing human engineering efforts, and remain framework-agnostic with plug-and-play support for diverse agents as task executors. It also enables a controllable performance-cost trade-off, allowing the system to approach Pareto-efficient. Across three challenging benchmarks (GAIA, SWE-Bench, Terminal-Bench), AOrchestra achieves 16.28% relative improvement against the strongest baseline when paired with Gemini-3-Flash. The code is available at: https://github.com/FoundationAgents/AOrchestra

</details>


### [56] [Understanding Agent Scaling in LLM-Based Multi-Agent Systems via Diversity](https://arxiv.org/abs/2602.03794)
*Yingxuan Yang,Chengrui Qu,Muning Wen,Laixi Shi,Ying Wen,Weinan Zhang,Adam Wierman,Shangding Gu*

Main category: cs.AI

TL;DR: 多智能体系统性能受限于任务内在不确定性而非智能体数量，异构智能体通过提供互补信息显著优于同构智能体扩展


<details>
  <summary>Details</summary>
Motivation: 研究LLM多智能体系统性能扩展的局限性，发现增加同构智能体数量存在收益递减，而引入异构性却能持续提升性能，需要理解这种差异的根本原因

Method: 提出信息论框架分析多智能体系统性能边界，引入K*指标量化有效通道数量，通过实验验证异构配置的性能优势

Result: 异构配置显著优于同构扩展：2个异构智能体性能可匹配或超过16个同构智能体，K*指标能有效量化系统多样性

Conclusion: 多智能体系统性能受任务内在不确定性限制而非智能体数量，异构性通过提供互补证据提升性能，为构建高效鲁棒的多智能体系统提供基于多样性的设计原则

Abstract: LLM-based multi-agent systems (MAS) have emerged as a promising approach to tackle complex tasks that are difficult for individual LLMs. A natural strategy is to scale performance by increasing the number of agents; however, we find that such scaling exhibits strong diminishing returns in homogeneous settings, while introducing heterogeneity (e.g., different models, prompts, or tools) continues to yield substantial gains. This raises a fundamental question: what limits scaling, and why does diversity help? We present an information-theoretic framework showing that MAS performance is bounded by the intrinsic task uncertainty, not by agent count. We derive architecture-agnostic bounds demonstrating that improvements depend on how many effective channels the system accesses. Homogeneous agents saturate early because their outputs are strongly correlated, whereas heterogeneous agents contribute complementary evidence. We further introduce $K^*$, an effective channel count that quantifies the number of effective channels without ground-truth labels. Empirically, we show that heterogeneous configurations consistently outperform homogeneous scaling: 2 diverse agents can match or exceed the performance of 16 homogeneous agents. Our results provide principled guidelines for building efficient and robust MAS through diversity-aware design. Code and Dataset are available at the link: https://github.com/SafeRL-Lab/Agent-Scaling.

</details>


### [57] [Conformal Thinking: Risk Control for Reasoning on a Compute Budget](https://arxiv.org/abs/2602.03814)
*Xi Wang,Anushri Suresh,Alvin Zhang,Rishi More,William Jurayj,Benjamin Van Durme,Mehrdad Farajtabar,Daniel Khashabi,Eric Nalisnick*

Main category: cs.AI

TL;DR: 本文提出一种基于风险控制的LLM推理预算设置框架，通过上下阈值机制在保证错误率的前提下最小化计算开销。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的推理能力支持测试时扩展，随着token预算增加，数据集准确率会提升，这促使了自适应推理的需求——在能提高可靠性时使用更多token，在额外计算可能无用时提前停止。然而，设置token预算和自适应推理阈值是一个实际挑战，涉及基本的风险-准确率权衡。

Method: 将预算设置问题重新定义为风险控制问题，在限制错误率的同时最小化计算。框架引入：1）上阈值机制，当模型置信度高时停止推理（可能输出错误结果）；2）新颖的参数化下阈值机制，预先停止无法解决的实例（可能过早停止）。给定目标风险和验证集，使用分布无关的风险控制来优化指定这些停止机制。对于多预算控制标准场景，引入效率损失来选择计算效率最高的退出机制。

Result: 在多样化的推理任务和模型上的实证结果表明，风险控制方法有效，展示了从下阈值和集成停止机制中获得的计算效率提升，同时遵守用户指定的风险目标。

Conclusion: 该框架成功解决了LLM推理中的风险-计算权衡问题，通过风险控制方法在保证错误率上限的前提下优化计算效率，为自适应推理提供了实用的解决方案。

Abstract: Reasoning Large Language Models (LLMs) enable test-time scaling, with dataset-level accuracy improving as the token budget increases, motivating adaptive reasoning -- spending tokens when they improve reliability and stopping early when additional computation is unlikely to help. However, setting the token budget, as well as the threshold for adaptive reasoning, is a practical challenge that entails a fundamental risk-accuracy trade-off. We re-frame the budget setting problem as risk control, limiting the error rate while minimizing compute. Our framework introduces an upper threshold that stops reasoning when the model is confident (risking incorrect output) and a novel parametric lower threshold that preemptively stops unsolvable instances (risking premature stoppage). Given a target risk and a validation set, we use distribution-free risk control to optimally specify these stopping mechanisms. For scenarios with multiple budget controlling criteria, we incorporate an efficiency loss to select the most computationally efficient exiting mechanism. Empirical results across diverse reasoning tasks and models demonstrate the effectiveness of our risk control approach, demonstrating computational efficiency gains from the lower threshold and ensemble stopping mechanisms while adhering to the user-specified risk target.

</details>


### [58] [AutoFigure: Generating and Refining Publication-Ready Scientific Illustrations](https://arxiv.org/abs/2602.03828)
*Minjun Zhu,Zhen Lin,Yixuan Weng,Panzhong Lu,Qiujie Xie,Yifan Wei,Sifan Liu,Qiyao Sun,Yue Zhang*

Main category: cs.AI

TL;DR: FigureBench是首个大规模科学插图生成基准，包含3300个高质量文本-插图对；AutoFigure是首个基于长文本自动生成高质量科学插图的智能体框架，通过思维、重组和验证实现结构完整且美观的插图。


<details>
  <summary>Details</summary>
Motivation: 高质量科学插图对于有效传达复杂科技概念至关重要，但手动创建插图在学术界和工业界都是公认的瓶颈，需要自动化解决方案。

Method: 提出了FigureBench基准数据集（3300个高质量文本-插图对）和AutoFigure智能体框架，该框架在最终渲染前通过广泛思考、重组和验证来生成结构合理且美观的布局。

Result: 实验表明AutoFigure在所有基线方法中表现最佳，能够生成可直接用于发表的科学插图，代码、数据集和HuggingFace空间已开源。

Conclusion: FigureBench为科学插图生成提供了首个大规模基准，AutoFigure框架通过智能体方法成功实现了从长文本自动生成高质量科学插图的目标，解决了手动创建的瓶颈问题。

Abstract: High-quality scientific illustrations are crucial for effectively communicating complex scientific and technical concepts, yet their manual creation remains a well-recognized bottleneck in both academia and industry. We present FigureBench, the first large-scale benchmark for generating scientific illustrations from long-form scientific texts. It contains 3,300 high-quality scientific text-figure pairs, covering diverse text-to-illustration tasks from scientific papers, surveys, blogs, and textbooks. Moreover, we propose AutoFigure, the first agentic framework that automatically generates high-quality scientific illustrations based on long-form scientific text. Specifically, before rendering the final result, AutoFigure engages in extensive thinking, recombination, and validation to produce a layout that is both structurally sound and aesthetically refined, outputting a scientific illustration that achieves both structural completeness and aesthetic appeal. Leveraging the high-quality data from FigureBench, we conduct extensive experiments to test the performance of AutoFigure against various baseline methods. The results demonstrate that AutoFigure consistently surpasses all baseline methods, producing publication-ready scientific illustrations. The code, dataset and huggingface space are released in https://github.com/ResearAI/AutoFigure.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [59] [Constitutional Spec-Driven Development: Enforcing Security by Construction in AI-Assisted Code Generation](https://arxiv.org/abs/2602.02584)
*Srinivas Rao Marri*

Main category: cs.SE

TL;DR: 提出宪法规范驱动开发方法，在AI辅助编程中通过规范层嵌入不可协商的安全原则，使AI生成代码从构造而非检查层面满足安全要求


<details>
  <summary>Details</summary>
Motivation: AI辅助的"氛围编程"虽然加速软件开发，但大型语言模型优先考虑功能正确性而非安全性，引入了重大安全风险

Method: 提出宪法规范驱动开发方法，引入宪法文档：一个版本化、机器可读的文档，编码基于CWE/MITRE Top 25漏洞和监管框架的安全约束

Result: 在银行微服务应用案例中，宪法约束将安全缺陷减少73%（相比无约束AI生成），同时保持开发速度；解决了10个关键CWE漏洞

Conclusion: 主动安全规范在AI辅助开发工作流中优于反应性安全验证；贡献了宪法安全的形式框架、完整开发方法和实证证据

Abstract: The proliferation of AI-assisted "vibe coding" enables rapid software development but introduces significant security risks, as Large Language Models (LLMs) prioritize functional correctness over security. We present Constitutional Spec-Driven Development, a methodology that embeds non-negotiable security principles into the specification layer, ensuring AI-generated code adheres to security requirements by construction rather than inspection. Our approach introduces a Constitution: a versioned, machine-readable document encoding security constraints derived from Common Weakness Enumeration (CWE)/MITRE Top 25 vulnerabilities and regulatory frameworks. We demonstrate the methodology through a banking microservices application, selected as a representative example domain due to its stringent regulatory and security requirements, implementing customer management, account operations, and transaction processing. The methodology itself is domain-agnostic. The implementation addresses 10 critical CWE vulnerabilities through constitutional constraints with full traceability from principles to code locations. Our case study shows that constitutional constraints reduce security defects by 73% compared to unconstrained AI generation while maintaining developer velocity. We contribute a formal framework for constitutional security, a complete development methodology, and empirical evidence that proactive security specification outperforms reactive security verification in AI-assisted development workflows.

</details>


### [60] [Beyond the Prompt: Assessing Domain Knowledge Strategies for High-Dimensional LLM Optimization in Software Engineering](https://arxiv.org/abs/2602.02752)
*Srinath Srinivasan,Tim Menzies*

Main category: cs.SE

TL;DR: 该研究比较了人类与人工智能生成领域知识的方法，评估了四种架构，旨在通过结构化知识集成使大语言模型能够为高维优化问题生成有效的热启动。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在低维软件工程优化任务中表现良好，但在高维问题上表现不佳，而贝叶斯方法在高维问题上占主导地位。研究旨在探索如何通过系统集成领域知识来弥合这一差距。

Method: 在MOOT数据集上评估了四种方法：(1)人类在环领域知识提示，(2)自适应多阶段提示，(3)维度感知渐进细化，(4)混合知识模型方法。通过切比雪夫距离和Scott-Knott聚类进行性能量化。

Result: 研究结果未在摘要中明确给出，但研究方法表明将通过系统评估来确定哪种知识集成策略能够使LLM在高维优化中生成有效的热启动。

Conclusion: 该研究旨在确定结构化知识集成是否能够使大语言模型在高维优化问题中生成有效的热启动，并比较人类与人工智能生成领域知识的策略。

Abstract: Background/Context: Large Language Models (LLMs) demonstrate strong performance on low-dimensional software engineering optimization tasks ($\le$11 features) but consistently underperform on high-dimensional problems where Bayesian methods dominate. A fundamental gap exists in understanding how systematic integration of domain knowledge (whether from humans or automated reasoning) can bridge this divide.
  Objective/Aim: We compare human versus artificial intelligence strategies for generating domain knowledge. We systematically evaluate four distinct architectures to determine if structured knowledge integration enables LLMs to generate effective warm starts for high-dimensional optimization.
  Method: We evaluate four approaches on MOOT datasets stratified by dimensionality: (1) Human-in-the-Loop Domain Knowledge Prompting (H-DKP), utilizing asynchronous expert feedback loops; (2) Adaptive Multi-Stage Prompting (AMP), implementing sequential constraint identification and validation; (3) Dimension-Aware Progressive Refinement (DAPR), conducting optimization in progressively expanding feature subspaces; and (4) Hybrid Knowledge-Model Approach (HKMA), synthesizing statistical scouting (TPE) with RAG-enhanced prompting. Performance is quantified via Chebyshev distance to optimal solutions and ranked using Scott-Knott clustering against an established baseline for LLM generated warm starts.
  Note that all human studies conducted as part of this study will comply with the policies of our local Institutional Review Board.

</details>


### [61] [Learning-Infused Formal Reasoning: From Contract Synthesis to Artifact Reuse and Formal Semantics](https://arxiv.org/abs/2602.02881)
*Arshad Beg,Diarmuid O'Donoghue,Rosemary Monahan*

Main category: cs.SE

TL;DR: 本文提出将形式化方法与人工智能结合的长期研究议程，旨在通过自动合约合成、语义构件复用和精化理论构建下一代形式化方法，实现从孤立证明向累积知识驱动范式的转变。


<details>
  <summary>Details</summary>
Motivation: 当前形式化方法面临孤立验证、重复工作和知识无法有效积累的问题。需要超越单一的正确性证明，建立能够持续合成、转移和复用规范、合约及证明的累积性知识驱动范式，以加速系统验证过程。

Method: 提出混合框架，结合大型语言模型和图表示来实现可扩展的语义匹配和验证构件的原则性复用。学习组件提供跨异构表示和抽象层次的语义指导，符号匹配确保形式正确性，基于组合推理构建可演进的验证生态系统。

Result: 本文主要提出研究愿景和框架概念，未报告具体实验结果。但描述了正在进行的实现该议程的工作，并论证了该框架能够支持验证生态系统的系统性演进，利用过去的验证成果加速未来的保证过程。

Conclusion: 下一代形式化方法需要整合AI技术，建立知识驱动的累积验证范式。通过语义构件复用和自动合约合成，可以构建能够持续演进、利用历史验证知识的验证生态系统，显著提高形式化验证的效率和可扩展性。

Abstract: This vision paper articulates a long-term research agenda for formal methods at the intersection with artificial intelligence, outlining multiple conceptual and technical dimensions and reporting on our ongoing work toward realising this agenda. It advances a forward-looking perspective on the next generation of formal methods based on the integration of automated contract synthesis, semantic artifact reuse, and refinement-based theory. We argue that future verification systems must move beyond isolated correctness proofs toward a cumulative, knowledge-driven paradigm in which specifications, contracts, and proofs are continuously synthesised and transferred across systems. To support this shift, we outline a hybrid framework combining large language models with graph-based representations to enable scalable semantic matching and principled reuse of verification artifacts. Learning-based components provide semantic guidance across heterogeneous notations and abstraction levels, while symbolic matching ensures formal soundness. Grounded in compositional reasoning, this vision points toward verification ecosystems that evolve systematically, leveraging past verification efforts to accelerate future assurance.

</details>


### [62] [Failure-Aware Enhancements for Large Language Model (LLM) Code Generation: An Empirical Study on Decision Framework](https://arxiv.org/abs/2602.02896)
*Jianru Shen,Zedong Peng,Lucy Owen*

Main category: cs.SE

TL;DR: 论文通过实证研究发现渐进提示在代码生成中达到96.9%任务完成率，但仍存在未满足需求。不同增强策略对不同类型的失败效果不同，作者提出了基于失败模式的决策框架。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在自动化软件开发方面有潜力，但即使使用渐进提示等高级工作流，仍有一些需求无法满足。现有方法如自我批评、多模型协作和RAG缺乏明确的指导原则，开发者不知道何时使用哪种方法。

Method: 对25个GitHub项目进行实证研究，比较渐进提示与直接提示的效果。针对6个最具代表性的项目，评估四种增强策略（自我批评、多模型协作、RAG等）在四种失败类型上的表现。

Result: 渐进提示平均任务完成率达到96.9%，显著优于直接提示（80.5%），但仍使8个项目不完整。不同方法效果取决于失败特征：自我批评对可代码审查的逻辑错误有效，但对外部服务集成完全无效（0%改进）；RAG在所有失败类型上实现最高完成率且效率最优。

Conclusion: 基于研究发现，提出了一个决策框架，将每种失败模式映射到最合适的增强方法，为从业者提供实用、数据驱动的指导，而不是试错。

Abstract: Large language models (LLMs) show promise for automating software development by translating requirements into code. However, even advanced prompting workflows like progressive prompting often leave some requirements unmet. Although methods such as self-critique, multi-model collaboration, and retrieval-augmented generation (RAG) have been proposed to address these gaps, developers lack clear guidance on when to use each. In an empirical study of 25 GitHub projects, we found that progressive prompting achieves 96.9% average task completion, significantly outperforming direct prompting (80.5%, Cohen's d=1.63, p<0.001) but still leaving 8 projects incomplete. For 6 of the most representative projects, we evaluated each enhancement strategy across 4 failure types. Our results reveal that method effectiveness depends critically on failure characteristics: Self-Critique succeeds on code-reviewable logic errors but fails completely on external service integration (0% improvement), while RAG achieves highest completion across all failure types with superior efficiency. Based on these findings, we propose a decision framework that maps each failure pattern to the most suitable enhancement method, giving practitioners practical, data-driven guidance instead of trial-and-error.

</details>


### [63] [Beyond Blame: Rethinking SZZ with Knowledge Graph Search](https://arxiv.org/abs/2602.02934)
*Yu Shi,Hao Li,Bram Adams,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: AgenticSZZ：首个将时序知识图谱应用于软件演化分析的方法，通过将bug引入提交识别重构为图搜索问题，显著提升了识别准确率


<details>
  <summary>Details</summary>
Motivation: 现有基于SZZ的方法依赖git blame，搜索空间仅限于直接修改修复行的提交，导致超过40%的bug引入提交无法被准确识别

Method: 采用两阶段方法：1) 构建时序知识图谱编码提交的时间与结构关系，从两个参考点（blame提交和bug修复提交）向后遍历文件历史扩展搜索空间；2) 利用LLM智能体通过专门工具在图谱中进行候选探索和因果分析

Result: 在三个数据集上评估，F1分数达到0.48-0.74，相比现有最优方法提升高达27%，消融研究证实两个组件都至关重要

Conclusion: 通过将bug引入提交识别转化为图搜索问题，为软件演化分析中的时序和因果推理开辟了新的研究方向

Abstract: Identifying Bug-Inducing Commits (BICs) is fundamental for understanding software defects and enabling downstream tasks such as defect prediction and automated program repair. Yet existing SZZ-based approaches are limited by their reliance on git blame, which restricts the search space to commits that directly modified the fixed lines. Our preliminary study on 2,102 validated bug-fixing commits reveals that this limitation is significant: over 40% of cases cannot be solved by blame alone, as 28% of BICs require traversing commit history beyond blame results and 14% are blameless.
  We present AgenticSZZ, the first approach to apply Temporal Knowledge Graphs (TKGs) to software evolution analysis. AgenticSZZ reframes BIC identification from a ranking problem over blame commits into a graph search problem, where temporal ordering is fundamental to causal reasoning about bug introduction. The approach operates in two phases: (1) constructing a TKG that encodes commits with temporal and structural relationships, expanding the search space by traversing file history backward from two reference points (blame commits and the BFC); and (2) leveraging an LLM agent to navigate the graph using specialized tools for candidate exploration and causal analysis.
  Evaluation on three datasets shows that AgenticSZZ achieves F1-scores of 0.48 to 0.74, with statistically significant improvements over state-of-the-art by up to 27%. Our ablation study confirms that both components are essential, reflecting a classic exploration-exploitation trade-off: the TKG expands the search space while the agent provides intelligent selection. By transforming BIC identification into a graph search problem, we open a new research direction for temporal and causal reasoning in software evolution analysis.

</details>


### [64] [Testing Framework Migration with Large Language Models](https://arxiv.org/abs/2602.02964)
*Altino Alves,João Eduardo Montandon,Andre Hora*

Main category: cs.SE

TL;DR: 本文研究利用大型语言模型（LLM）自动化将Python测试从unittest迁移到Pytest框架，评估了GPT-4o和Claude Sonnet 4在不同提示策略下的表现，发现约48.5%的迁移测试能够通过执行。


<details>
  <summary>Details</summary>
Motivation: Python开发者主要使用unittest和Pytest两种测试框架。虽然Pytest提供了更简单的断言、可重用的fixture和更好的互操作性，但从unittest迁移到Pytest仍然是一个手动且耗时的过程。自动化这一迁移过程可以显著减少工作量并加速测试现代化。

Method: 研究评估了GPT-4o和Claude Sonnet 4两种LLM在三种提示策略（Zero-shot、One-shot和Chain-of-Thought）和两种温度设置（0.0和1.0）下的表现。首先从top 100 Python开源项目中提取真实迁移案例构建数据集，然后实际执行LLM生成的测试迁移代码。

Result: 总体而言，51.5%的LLM生成的测试迁移失败，48.5%通过。Claude Sonnet 4表现出更保守的迁移策略（如保留基于类的测试和遗留unittest引用），而GPT-4o倾向于更多转换（如转向基于函数的测试）。

Conclusion: LLM可以加速测试迁移过程，但存在局限性。不同LLM表现出不同的迁移策略偏好。研究结果为从业者和研究人员提供了多个实践启示，表明自动化测试迁移具有潜力但需要进一步改进。

Abstract: Python developers rely on two major testing frameworks: \texttt{unittest} and \texttt{Pytest}. While \texttt{Pytest} offers simpler assertions, reusable fixtures, and better interoperability, migrating existing suites from \texttt{unittest} remains a manual and time-consuming process. Automating this migration could substantially reduce effort and accelerate test modernization. In this paper, we investigate the capability of Large Language Models (LLMs) to automate test framework migrations from \texttt{unittest} to \texttt{Pytest}. We evaluate GPT 4o and Claude Sonnet 4 under three prompting strategies (Zero-shot, One-shot, and Chain-of-Thought) and two temperature settings (0.0 and 1.0). To support this analysis, we first introduce a curated dataset of real-world migrations extracted from the top 100 Python open-source projects. Next, we actually execute the LLM-generated test migrations in their respective test suites. Overall, we find that 51.5% of the LLM-generated test migrations failed, while 48.5% passed. The results suggest that LLMs can accelerate test migration, but there are often caveats. For example, Claude Sonnet 4 exhibited more conservative migrations (e.g., preserving class-based tests and legacy \texttt{unittest} references), while GPT-4o favored more transformations (e.g., to function-based tests). We conclude by discussing multiple implications for practitioners and researchers.

</details>


### [65] [Understanding Bug-Reproducing Tests: A First Empirical Study](https://arxiv.org/abs/2602.02965)
*Andre Hora,Gordon Fraser*

Main category: cs.SE

TL;DR: 该研究对Python项目中642个bug重现测试进行了实证分析，发现它们在代码行数、断言数量和复杂度方面与普通测试无显著差异，但包含更多try/except块和弱断言，且95%只重现单个bug。


<details>
  <summary>Details</summary>
Motivation: 尽管bug重现测试与常规测试共存于测试套件中，但其特性研究较少，不清楚它们是否与普通测试有本质区别，因此需要实证研究来更好地理解这类测试。

Method: 对15个真实Python系统中的642个bug重现测试进行实证分析，比较它们与普通测试在LOC、断言数量、复杂度等方面的差异。

Result: bug重现测试在LOC、断言数量和复杂度方面与普通测试无统计学显著差异，但包含更多try/except块和弱断言（如assertNotEqual），且95%只重现单个bug，5%重现多个bug。

Conclusion: bug重现测试与普通测试在基本特性上相似，但在异常处理和断言类型上存在差异，这为测试实践和未来研究提供了启示。

Abstract: Developers create bug-reproducing tests that support debugging by failing as long as the bug is present, and passing once the bug has been fixed. These tests are usually integrated into existing test suites and executed regularly alongside all other tests to ensure that future regressions are caught. Despite this co-existence with other types of tests, the properties of bug-reproducing tests are scarcely researched, and it remains unclear whether they differ fundamentally. In this short paper, we provide an initial empirical study to understand bug-reproducing tests better. We analyze 642 bug-reproducing tests of 15 real-world Python systems. Overall, we find that bug-reproducing tests are not (statistically significantly) different from other tests regarding LOC, number of assertions, and complexity. However, bug-reproducing tests contain slightly more try/except blocks and ``weak assertions'' (e.g.,~\texttt{assertNotEqual}). Lastly, we detect that the majority (95%) of the bug-reproducing tests reproduce a single bug, while 5% reproduce multiple bugs. We conclude by discussing implications and future research directions.

</details>


### [66] [What Do Contribution Guidelines Say About Software Testing?](https://arxiv.org/abs/2602.02966)
*Bruna Falcucci,Felipe Gomide,Andre Hora*

Main category: cs.SE

TL;DR: 对200个Python和JavaScript开源项目的贡献指南进行实证研究，分析测试实践在贡献指南中的体现情况


<details>
  <summary>Details</summary>
Motivation: 虽然大多数开源项目要求贡献者编写测试，但具体传达给贡献者的测试实践尚不明确，需要了解贡献指南中如何指导软件测试

Method: 分析200个Python和JavaScript开源软件项目的贡献指南，包括CONTRIBUTING文件、外部文档和README文件中的测试文档

Result: 78%的项目包含某种形式的测试文档；测试文档主要位于CONTRIBUTING文件(58%)、外部文档(24%)和README文件(8%)；83.5%的文档解释如何运行测试，但只有37%提供编写测试的指导；71%涵盖单元测试，但集成测试(20.5%)和端到端测试(15.5%)较少涉及；测试覆盖率(25.5%)和模拟测试(9.5%)讨论较少

Conclusion: 开源项目在贡献指南中普遍包含测试文档，但测试指导内容不够全面，特别是测试编写方法、高级测试类型和测试质量指标方面存在不足，需要改进测试实践指导

Abstract: Software testing plays a crucial role in the contribution process of open-source projects. For example, contributions introducing new features are expected to include tests, and contributions with tests are more likely to be accepted. Although most real-world projects require contributors to write tests, the specific testing practices communicated to contributors remain unclear. In this paper, we present an empirical study to understand better how software testing is approached in contribution guidelines. We analyze the guidelines of 200 Python and JavaScript open-source software projects. We find that 78\% of the projects include some form of test documentation for contributors. Test documentation is located in multiple sources, including \texttt{CONTRIBUTING} files (58\%), external documentation (24\%), and \texttt{README} files (8\%). Furthermore, test documentation commonly explains how to run tests (83.5\%), but less often provides guidance on how to write tests (37\%). It frequently covers unit tests (71\%), but rarely addresses integration (20.5\%) and end-to-end tests (15.5\%). Other key testing aspects are also less frequently discussed: test coverage (25.5\%) and mocking (9.5\%). We conclude by discussing implications and future research.

</details>


### [67] [Maintaining the Heterogeneity in the Organization of Software Engineering Research](https://arxiv.org/abs/2602.03093)
*Yang Yue,Zheng Jiang,Yi Wang*

Main category: cs.SE

TL;DR: 软件工程研究组织中的异质性（资助研究模式和动手实践模式）正面临威胁，作者呼吁社区重视并维护这种多样性


<details>
  <summary>Details</summary>
Motivation: 软件工程研究历史上存在两种组织模式的异质性：资助研究模式和动手实践模式，这种多样性使软件工程在过去50年成为繁荣的跨学科领域。然而，近年来资助研究模式逐渐占据主导地位，这种异质性正受到严重系统性威胁

Method: 本文采用论述性方法，首先解释软件工程研究组织为何需要异质性，然后分析当前软件工程研究的趋势、后果及潜在未来

Result: 作者指出资助研究模式在软件工程研究中日益占据主导地位，这种趋势威胁到研究组织的异质性，可能导致研究多样性和创新性的减少

Conclusion: 选择权在我们手中，作者呼吁软件工程社区认真考虑并维护研究组织中的异质性，以保持该领域的活力和创新性

Abstract: The heterogeneity in the organization of software engineering (SE) research historically exists, i.e., funded research model and hands-on model, which makes software engineering become a thriving interdisciplinary field in the last 50 years. However, the funded research model is becoming dominant in SE research recently, indicating such heterogeneity has been seriously and systematically threatened. In this essay, we first explain why the heterogeneity is needed in the organization of SE research, then present the current trend of SE research nowadays, as well as the consequences and potential futures. The choice is at our hands, and we urge our community to seriously consider maintaining the heterogeneity in the organization of software engineering research.

</details>


### [68] [Synthesizing File-Level Data for Unit Test Generation with Chain-of-Thoughts via Self-Debugging](https://arxiv.org/abs/2602.03181)
*Ziyue Hua,Tianyu Chen,Yeyun Gong,Shuai Lu,Peng Cheng,Qinglin Zhu,Yibo He,Yingjie Fu,Wenpin Jiao,Wei Yang,Tao Xie*

Main category: cs.SE

TL;DR: 提出了一种通过自调试和CoT压缩生成高质量单元测试训练数据的方法，显著提升了LLM生成单元测试的效果


<details>
  <summary>Details</summary>
Motivation: 现有单元测试生成方法（包括符号执行、基于搜索的方法和LLM生成器）难以生成具有正确断言和可靠思维链解释的人类质量测试。现有训练数据存在缺陷：仓库挖掘的测试缺乏开发者思维链，而LLM蒸馏的思维链往往不正确或不完整。

Method: 提出了一种新颖的数据蒸馏方法，使用自调试生成高质量单元测试训练示例及其对应的忠实思维链。方法包括：(1) 引导测试修复——一个启发式循环（错误、失败和覆盖率聚焦步骤），要求模型诊断并迭代修复生成的测试；(2) 思维链压缩——将原始和调试思维链压缩为直接证明正确测试的简洁解释。

Result: 在开源项目大规模语料库上构建了包含74,518个高质量<焦点方法、测试、思维链>示例的数据集。微调后的模型在单元测试生成方面表现优异：测试断言通过率达到36.17%，分支覆盖率达到43.90%，变异测试得分达到88.66%，显著优于o4-mini等最先进的商业模型。

Conclusion: 通过自调试和思维链压缩的数据蒸馏方法能够生成高质量的单元测试训练数据，显著提升LLM在单元测试生成任务上的性能，解决了现有训练数据中思维链质量不足的问题。

Abstract: Automatic unit test (UT) generation is essential for software quality assurance, but existing approaches--including symbolic execution, search-based approaches, and recent LLM-based generators--struggle to produce human-quality tests with correct, meaningful assertions and reliable chain-of-thought (CoT) explanations. We identify a gap in UT training data: repository-mined tests lack developer CoTs, while LLM-distilled CoTs are often incorrect or incomplete. To address this issue, we propose a novel data-distillation approach that uses self-debugging to produce high-quality UT training examples paired with faithful CoTs. Our approach combines (1) guided test repair, a heuristic loop (error-, failure-, and coverage-focused steps) that asks the used model to diagnose and iteratively fix generated tests, and (2) CoT compression, which compacts original and debugging CoTs into concise explanations that directly justify correct tests. We apply this pipeline to a large corpus of open-source projects to construct a dataset of 74,518 high-quality <focal method, test, CoT> examples, and then use it for supervised fine-tuning of a base model. An empirical evaluation shows that the fine-tuned model achieves high UT generation effectiveness: it attains a pass rate of 36.17% on test assertions, a branch coverage of 43.90%, and a mutation score of 88.66%, substantially higher than state-of-the-art commercial models like o4-mini.

</details>


### [69] [Precision in Practice: Knowledge Guided Code Summarizing Grounded in Industrial Expectations](https://arxiv.org/abs/2602.03400)
*Jintai Li,Songqiang Chen,Shuo Jin,Xiaoyuan Xie*

Main category: cs.SE

TL;DR: ExpSum是一种面向工业代码文档的期望感知代码摘要方法，通过整合函数元数据抽象、信息过滤、上下文感知领域知识检索和约束驱动提示，生成符合开发者期望的结构化摘要。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在代码摘要方面取得显著进展，但在工业环境中生成的摘要实用性不足。研究表明超过57.4%的现有方法生成的摘要因不符合开发者对工业文档的期望而被拒绝，开发者需要适当的领域术语、明确的函数分类，并避免冗余实现细节。

Method: 提出ExpSum方法，包含四个关键组件：1) 函数元数据抽象，2) 信息元数据过滤，3) 上下文感知领域知识检索，4) 约束驱动提示，指导大语言模型生成结构化、符合期望的代码摘要。

Result: 在HarmonyOS项目上，ExpSum相比所有基线方法表现更优，BLEU-4提升高达26.71%，ROUGE-L提升20.10%。基于大语言模型的评估表明，ExpSum生成的摘要在其他项目中也更好地符合开发者期望。

Conclusion: ExpSum通过整合开发者期望的关键要素，显著提升了工业代码文档的质量和实用性，为工业环境中的代码摘要提供了有效的解决方案。

Abstract: Code summaries are essential for helping developers understand code functionality and reducing maintenance and collaboration costs. Although recent advances in large language models (LLMs) have significantly improved automatic code summarization, the practical usefulness of generated summaries in industrial settings remains insufficiently explored. In collaboration with documentation experts from the industrial HarmonyOS project, we conducted a questionnaire study showing that over 57.4% of code summaries produced by state-of-the-art approaches were rejected due to violations of developers' expectations for industrial documentation. Beyond semantic similarity to reference summaries, developers emphasize additional requirements, including the use of appropriate domain terminology, explicit function categorization, and the avoidance of redundant implementation details.
  To address these expectations, we propose ExpSum, an expectation-aware code summarization approach that integrates function metadata abstraction, informative metadata filtering, context-aware domain knowledge retrieval, and constraint-driven prompting to guide LLMs in generating structured, expectation-aligned summaries. We evaluate ExpSum on the HarmonyOS project and widely used code summarization benchmarks. Experimental results show that ExpSum consistently outperforms all baselines, achieving improvements of up to 26.71% in BLEU-4 and 20.10% in ROUGE-L on HarmonyOS. Furthermore, LLM-based evaluations indicate that ExpSum-generated summaries better align with developer expectations across other projects, demonstrating its effectiveness for industrial code documentation.

</details>


### [70] [RAL-Bench: Benchmarking for Application-Level Functional Correctness and Non-Functional Quality Attributes](https://arxiv.org/abs/2602.03462)
*Ruwei Pan,Yakun Zhang,Qingyuan Liang,Yueheng Zhu,Chao Liu,Lu Zhang,Hongyu Zhang*

Main category: cs.SE

TL;DR: RAL-Bench是一个用于评估应用级代码生成的基准测试框架，通过系统测试评估功能正确性和非功能性质量，发现当前LLMs在应用级代码生成中功能正确性是主要瓶颈。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试在应用级代码生成评估方面有限，无法全面评估功能正确性和非功能性质量（如可维护性、安全性）。需要了解当前LLMs是否能生成满足这两方面要求的应用级代码库。

Method: 提出RAL-Bench基准测试框架：1）从高质量参考项目中提炼简洁的自然语言需求；2）构建覆盖功能和非功能属性的黑盒系统测试；3）仅保留在参考仓库中通过的测试以确保可靠基准；4）功能正确性通过系统测试通过率衡量；5）非功能性质量基于ISO/IEC 25010五个维度评估，使用AHP权重向量聚合，并进行基线归一化评分。

Result: 评估了16个LLMs的零样本贪婪解码结果：功能正确性是主要瓶颈，没有模型在需求驱动、参考验证的测试下超过45%的功能通过率。

Conclusion: 当前LLMs在应用级代码生成中功能正确性仍然是主要挑战，RAL-Bench提供了一个全面的评估框架来评估功能正确性和非功能性质量，有助于推动应用级代码生成研究。

Abstract: Code generation has advanced rapidly with code-focused large language models (LLMs), especially on snippet-level tasks. However, application-level generation requires producing a runnable multi-file repository with correct structure, dependencies, and end-to-end executability, and real-world software must satisfy both functional correctness and non-functional quality (e.g., maintainability, security). Existing benchmarks provide a limited execution-based assessment of these requirements at the application level. We ask: Can current LLMs generate application-level repositories that meet both functional and non-functional criteria? We propose RAL-Bench, a benchmark and evaluation framework for application-level code generation. For each task, we distill a concise natural-language requirement from a high-quality reference project, build black-box system tests covering functional and non-functional attributes, and keep only tests that pass on the reference repository to ensure a sound oracle and an end-to-end executable suite. Functional correctness is measured by system-test pass rate. Non-functional quality is measured along five ISO/IEC 25010-inspired dimensions and aggregated with an Analytic Hierarchy Process (AHP)-derived weight vector, with per-dimension diagnostics and baseline-normalized scoring using reference measurements. Across 16 LLMs evaluated zero-shot with greedy decoding, functional correctness is the dominant bottleneck: no model exceeds a 45% functional pass rate under our requirement-driven, reference-validated tests. We release RAL-Bench at https://github.com/Wwstarry/RAL-Bench. .

</details>


### [71] [Formal Evidence Generation for Assurance Cases for Robotic Software Models](https://arxiv.org/abs/2602.03550)
*Fang Yan,Simon Foster,Ana Cavalcanti,Ibrahim Habli,James Baxter*

Main category: cs.SE

TL;DR: 提出基于模型的系统化方法，通过将形式化验证嵌入到保证工作流中，自动生成保证案例证据，解决证据生成和维护的挑战


<details>
  <summary>Details</summary>
Motivation: 机器人和自主系统越来越多地部署在安全关键领域，需要证明其安全性。保证案例提供了结构化论证，但证据的生成和维护劳动密集、容易出错，且随着系统演化难以保持一致性

Method: 使用基于模型的方法，结合RoboChart领域特定建模语言，通过模板将自然语言需求系统化地转化为形式化断言，协调多种形式化验证工具处理不同属性类型，并将形式化证据生产集成到工作流中

Result: 案例研究表明该方法有效，能够自动将结构化需求转化为形式化断言，并自动将验证结果整合为证据

Conclusion: 提出的模型化方法能够系统化地生成保证案例证据，通过将形式化验证嵌入到保证工作流中，解决了证据生成和维护的挑战

Abstract: Robotics and Autonomous Systems are increasingly deployed in safety-critical domains, so that demonstrating their safety is essential. Assurance Cases (ACs) provide structured arguments supported by evidence, but generating and maintaining this evidence is labour-intensive, error-prone, and difficult to keep consistent as systems evolve. We present a model-based approach to systematically generating AC evidence by embedding formal verification into the assurance workflow. The approach addresses three challenges: systematically deriving formal assertions from natural language requirements using templates, orchestrating multiple formal verification tools to handle diverse property types, and integrating formal evidence production into the workflow. Leveraging RoboChart, a domain-specific modelling language with formal semantics, we combine model checking and theorem proving in our approach. Structured requirements are automatically transformed into formal assertions using predefined templates, and verification results are automatically integrated as evidence. Case studies demonstrate the effectiveness of our approach.

</details>


### [72] [Flaky Tests in a Large Industrial Database Management System: An Empirical Study of Fixed Issue Reports for SAP HANA](https://arxiv.org/abs/2602.03556)
*Alexander Berndt,Thomas Bach,Sebastian Baltes*

Main category: cs.SE

TL;DR: 本文提出了一种利用LLMs作为标注者的方法，通过模型内和模型间一致性来自动标注与已修复的flaky测试相关的根本原因类别，从而分析SAP HANA中flaky测试的分布情况。


<details>
  <summary>Details</summary>
Motivation: Flaky测试在多次执行相同源代码时会产生不同结果，这为代码质量提供了模糊信号并干扰了代码变更的自动评估。虽然多种因素可能导致测试flakiness，但修复方法通常针对特定原因。由于手动标注flaky测试耗时且繁琐，需要一种自动化的方法来分析不同项目中flaky测试的根本原因分布。

Method: 提出LLMs-as-annotators方法，利用模型内和模型间一致性来标注与已修复的flakiness问题相关的issue报告的根本原因类别。该方法在SAP HANA这一大型工业数据库管理系统背景下进行评估，分析了559个issue报告。

Result: 分析结果显示，SAP HANA的测试最常见的问题与并发相关（23%，559个分析的问题报告中有130个）。此外，不同测试类型面临不同的flakiness挑战，表明flakiness的根本原因分布因测试类型而异。

Conclusion: LLMs-as-annotators方法能够有效自动标注flaky测试的根本原因，为理解工业系统中flakiness的分布提供了实用工具。研究鼓励未来关于flakiness缓解的研究应考虑评估所提方法在不同测试类型间的泛化能力。

Abstract: Flaky tests yield different results when executed multiple times for the same version of the source code. Thus, they provide an ambiguous signal about the quality of the code and interfere with the automated assessment of code changes. While a variety of factors can cause test flakiness, approaches to fix flaky tests are typically tailored to address specific causes. However, the prevalent root causes of flaky tests can vary depending on the programming language, application domain, or size of the software project. Since manually labeling flaky tests is time-consuming and tedious, this work proposes an LLMs-as-annotators approach that leverages intra- and inter-model consistency to label issue reports related to fixed flakiness issues with the relevant root cause category. This allows us to gain an overview of prevalent flakiness categories in the issue reports. We evaluated our labeling approach in the context of SAP HANA, a large industrial database management system. Our results suggest that SAP HANA's tests most commonly suffer from issues related to concurrency (23%, 130 of 559 analyzed issue reports). Moreover, our results suggest that different test types face different flakiness challenges. Therefore, we encourage future research on flakiness mitigation to consider evaluating the generalizability of proposed approaches across different test types.

</details>


### [73] [Scaling Test-Driven Code Generation from Functions to Classes: An Empirical Study](https://arxiv.org/abs/2602.03557)
*Yunhao Liang,Ruixuan Ying,Shiwen Ni,Zhe Cui*

Main category: cs.SE

TL;DR: 该研究将测试驱动开发(TDD)从函数级代码生成扩展到类级，通过迭代框架分析类内方法依赖关系，逐步生成方法并利用测试反馈进行修复，显著提升了类级代码生成的正确率。


<details>
  <summary>Details</summary>
Motivation: 现有TDD风格的代码生成研究主要局限于函数级任务，而类级合成涉及多个方法通过共享状态和调用依赖进行交互，这一领域尚未得到充分探索。需要将测试驱动代码生成从函数扩展到类，以提高类级代码生成的可靠性。

Method: 提出一个迭代TDD框架：1) 分析类内方法依赖关系以确定可行的生成顺序；2) 在方法级公共测试下逐步实现每个方法；3) 使用反射式执行反馈和有限修复迭代；4) 构建ClassEval-TDD数据集支持测试驱动生成和严格评估。

Result: 类级TDD框架将类级正确性提高了12-26个百分点，最高达到71%完全正确的类，平均只需要少量修复。在八个LLM上的实证研究表明，该方法显著优于最强的直接生成基线（整体、增量和组合策略中的最佳者）。

Conclusion: 测试驱动生成可以有效扩展到孤立函数之外，显著提高类级代码生成的可靠性。该方法通过迭代TDD框架和系统化依赖分析，成功解决了类级代码生成中多方法交互的挑战。

Abstract: Test-driven development (TDD) has been adopted to improve Large Language Model (LLM)-based code generation by using tests as executable specifications. However, existing TDD-style code generation studies are largely limited to function-level tasks, leaving class-level synthesis where multiple methods interact through shared state and call dependencies underexplored. In this paper, we scale test-driven code generation from functions to classes via an iterative TDD framework. Our approach first analyzes intra-class method dependencies to derive a feasible generation schedule, and then incrementally implements each method under method-level public tests with reflection-style execution feedback and bounded repair iterations. To support test-driven generation and rigorous class-level evaluation, we construct ClassEval-TDD, a cleaned and standardized variant of ClassEval with consistent specifications, deterministic test environments, and complete method-level public tests. We conduct an empirical study across eight LLMs and compare against the strongest direct-generation baseline (the best of holistic, incremental, and compositional strategies). Our class-level TDD framework consistently improves class-level correctness by 12 to 26 absolute points and achieves up to 71% fully correct classes, while requiring only a small number of repairs on average. These results demonstrate that test-driven generation can effectively scale beyond isolated functions and substantially improve class-level code generation reliability. All code and data are available at https://anonymous.4open.science/r/ClassEval-TDD-C4C9/

</details>


### [74] [Causal Inference for the Effect of Code Coverage on Bug Introduction](https://arxiv.org/abs/2602.03585)
*Lukas Schulte,Gordon Fraser,Steffen Herbold*

Main category: cs.SE

TL;DR: 该研究使用因果推断方法量化代码覆盖率对引入bug的影响，分析JavaScript/TypeScript开源项目中覆盖率剂量与bug引入之间的因果关系。


<details>
  <summary>Details</summary>
Motivation: 代码覆盖率被广泛用作软件质量保证指标，但其实际效果和适当剂量在研究和工程界存在争议。先前工作仅报告相关性关联，结果易受混杂因素影响，需要量化覆盖率对bug引入的因果效应。

Method: 构建因果有向无环图识别软件工程过程中的混杂因素，从源代码、问题跟踪系统、评审系统和持续集成中建模关键变量。使用广义倾向得分调整，对连续暴露（覆盖率）应用双重稳健回归的因果推断方法，分析bug引入和非bug引入变更的新数据集。

Result: 研究将估计平均处理效应和剂量-响应关系，以检查数据集中项目内潜在的非线性模式（如阈值或收益递减效应）。

Conclusion: 该研究旨在为JavaScript和TypeScript开源项目中代码覆盖率对bug引入的因果效应提供量化证据，帮助理解覆盖率剂量与软件质量之间的真实关系。

Abstract: Context: Code coverage is widely used as a software quality assurance measure. However, its effect, and specifically the advisable dose, are disputed in both the research and engineering communities. Prior work reports only correlational associations, leaving results vulnerable to confounding factors. Objective: We aim to quantify the causal effect of code coverage (exposure) on bug introduction (outcome) in the context of mature JavaScript and TypeScript open source projects, addressing both the overall effect and its variance across coverage levels. Method: We construct a causal directed acyclic graph to identify confounders within the software engineering process, modeling key variables from the source code, issue- and review systems, and continuous integration. Using generalized propensity score adjustment, we will apply doubly robust regression-based causal inference for continuous exposure to a novel dataset of bug-introducing and non-bug-introducing changes. We estimate the average treatment effect and dose-response relationship to examine potential non-linear patterns (e.g., thresholds or diminishing returns) within the projects of our dataset.

</details>


### [75] [Beyond the Commit: Developer Perspectives on Productivity with AI Coding Assistants](https://arxiv.org/abs/2602.03593)
*Valerie Chen,Jasmyn He,Behnjamin Williams,Jason Valentino,Ameet Talwalkar*

Main category: cs.SE

TL;DR: 该研究分析了评估AI编码助手对开发者生产力影响的不同方法的有效性，通过混合研究方法发现需要多维度方法来衡量AI生产力影响，强调了长期指标的重要性。


<details>
  <summary>Details</summary>
Motivation: 在AI编码助手时代，学术界和工业界都需要理解如何衡量其对开发者生产力的影响，并重新评估早期衡量框架是否仍然适用。当前缺乏有效评估AI编码助手生产力影响的方法。

Method: 采用混合研究方法：在BNY Mellon公司进行包含2989名开发者响应的问卷调查，并结合11次深度访谈，从定量和定性两个维度收集数据。

Result: 调查结果显示开发者对AI工具实用性存在矛盾观点；访谈揭示了六个不同的生产力因素，涵盖短期和长期维度。研究发现需要多维度方法来衡量AI生产力影响，特别强调了技术专长和工作所有权等长期指标的重要性。

Conclusion: 该研究鼓励未来研究纳入更广泛的人为中心因素，支持工业界采用更全面的方法来评估开发者生产力，特别是要重视长期生产力指标而不仅仅是短期效率提升。

Abstract: Measuring developer productivity is a topic that has attracted attention from both academic research and industrial practice. In the age of AI coding assistants, it has become even more important for both academia and industry to understand how to measure their impact on developer productivity, and to reconsider whether earlier measures and frameworks still apply. This study analyzes the validity of different approaches to evaluating the productivity impacts of AI coding assistants by leveraging mixed-method research. At BNY Mellon, we conduct a survey with 2989 developer responses and 11 in-depth interviews. Our findings demonstrate that a multifaceted approach is needed to measure AI productivity impacts: survey results expose conflicting perspectives on AI tool usefulness, while interviews elicit six distinct factors that capture both short-term and long-term dimensions of productivity. In contrast to prior work, our factors highlight the importance of long-term metrics like technical expertise and ownership of work. We hope this work encourages future research to incorporate a broader range of human-centered factors, and supports industry in adopting more holistic approaches to evaluating developer productivity.

</details>


### [76] [CALM: A Self-Adaptive Orchestration Approach for QoS-Aware Routing in Small Language Model based Systems](https://arxiv.org/abs/2602.03632)
*Hemang Jain,Divyansh Pandey,Karthik Vaidhyanathan*

Main category: cs.SE

TL;DR: CALM是一个基于MAPE-K的自适应编排机制，通过协调多个小型语言模型组成的舰队，动态适应运行时不确定性，相比单一LLM基线降低40%延迟和50%能耗。


<details>
  <summary>Details</summary>
Motivation: AI系统面临运行时不确定性（动态工作负载、资源需求、模型漂移等），严重影响服务质量。LLM系统存在资源密集或隐私/成本问题，而单个小型语言模型无法满足现实需求的多样性和规模。

Method: 提出CALM，基于MAPE-K的自适应编排机制：持续监控用户查询，分析SLM的QoS指标，识别最优SLM，路由查询到选定SLM，并通过缓存和调度决定哪些SLM保留在内存中。

Result: CALM相比单一LLM基线，延迟降低约40%，能耗降低50%，同时保持领域特定任务性能。

Conclusion: 通过协调具有专业优势的SLM舰队，结合智能编排和持续自适应，可以有效应对运行时不确定性，提升系统效率和服务质量。

Abstract: AI-enabled systems are subjected to various types of runtime uncertainties, ranging from dynamic workloads, resource requirements, model drift, etc. These uncertainties have a big impact on the overall Quality of Service (QoS). This is particularly true in the case of Language Model (LM) enabled systems where the autoregressive nature of token generation introduces variability in latency, energy usage and response quality. These systems, powered by LLMs, are either resource-intensive (if run on-prem) or raise privacy/cost concerns (if leveraged using APIs). While deploying a Small Language Model (SLM) can be resource-efficient, it often falls short in addressing the diversity and scale of real-world requirements. To this, we argue that, rather than relying on any one SLM, leveraging a coordinated fleet of SLMs, each with specialized strengths can enable systems to dynamically adapt to shifting contexts and workload patterns. However, realizing the full potential of such an approach demands intelligent orchestration and continuous adaptation. To this end, we introduce CALM , a self-adaptive orchestration mechanism based on MAPE-K. Our approach continuously monitors user queries, analyzes the QoS metrics of the SLMs, identifies the optimal SLM to be used, routes the query to the identified SLM and further to enhance the effectiveness and efficiency, leverages caching and scheduling to decide the SLMs to be kept in memory. Our evaluation shows that CALM reduces latency by approximately 40% and energy consumption by 50%, while preserving domain-specific task performance when compared to single-LLM baselines.

</details>


### [77] [Improving Deep Learning Library Testing with Machine Learning](https://arxiv.org/abs/2602.03755)
*Facundo Molina,M M Abid Naziri,Feiran Qin,Alessandra Gorla,Marcelo d'Amorim*

Main category: cs.SE

TL;DR: 使用机器学习分类器来学习深度学习库API的输入约束，通过张量形状抽象降低问题维度，提高API规范挖掘的准确性，从而显著改进现有bug检测工具的效果。


<details>
  <summary>Details</summary>
Motivation: TensorFlow和PyTorch等深度学习库虽然简化了机器学习模型开发，但由于其复杂设计容易产生bug。现有的bug检测技术缺乏精确的API规范，导致大量误报。现有的API规范挖掘方法准确性不足，需要更精确的方法来捕获API约束。

Method: 提出使用机器学习分类器来确定输入有效性。核心假设是张量形状可以作为精确的抽象来编码具体输入并捕获数据关系。形状抽象显著降低了问题维度，便于机器学习训练。通过观察运行时结果获取标记数据，在标记输入集上训练分类器来捕获API约束。

Result: 在TensorFlow和PyTorch的183个API上评估，分类器在未见数据上泛化良好，准确率超过91%。将分类器集成到最先进的bug检测工具ACETest中，将其通过率从约29%提高到约61%。

Conclusion: 机器学习增强的输入分类是扩展深度学习库测试的重要辅助手段，能够有效提高API规范挖掘的准确性，从而显著改进bug检测工具的实用性。

Abstract: Deep Learning (DL) libraries like TensorFlow and Pytorch simplify machine learning (ML) model development but are prone to bugs due to their complex design. Bug-finding techniques exist, but without precise API specifications, they produce many false alarms. Existing methods to mine API specifications lack accuracy. We explore using ML classifiers to determine input validity. We hypothesize that tensor shapes are a precise abstraction to encode concrete inputs and capture relationships of the data. Shape abstraction severely reduces problem dimensionality, which is important to facilitate ML training. Labeled data are obtained by observing runtime outcomes on a sample of inputs and classifiers are trained on sets of labeled inputs to capture API constraints. Our evaluation, conducted over 183 APIs from TensorFlow and Pytorch, shows that the classifiers generalize well on unseen data with over 91% accuracy. Integrating these classifiers into the pipeline of ACETest, a SoTA bug-finding technique, improves its pass rate from ~29% to ~61%. Our findings suggest that ML-enhanced input classification is an important aid to scale DL library testing.

</details>


### [78] [FullStack-Agent: Enhancing Agentic Full-Stack Web Coding via Development-Oriented Testing and Repository Back-Translation](https://arxiv.org/abs/2602.03798)
*Zimu Lu,Houxing Ren,Yunqiao Yang,Ke Wang,Zhuofan Zong,Mingjie Zhan,Hongsheng Li*

Main category: cs.SE

TL;DR: FullStack-Agent是一个用于全栈网站开发的统一智能体系统，包含开发框架、自学习方法和评估基准三部分，显著提升了全栈网站生成能力。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代码智能体主要生成前端网页，缺乏真正的全栈数据处理和存储能力。构建生产级全栈Web应用比仅生成前端页面更具挑战性，需要控制数据流、理解不断更新的包依赖、定位代码库中的隐蔽bug。

Method: 系统包含三部分：1) FullStack-Dev：具有强大规划、代码编辑、代码库导航和bug定位能力的多智能体框架；2) FullStack-Learn：通过反向翻译爬取和合成的网站仓库来改进骨干LLM的数据扩展和自学习方法；3) FullStack-Bench：系统测试生成网站前端、后端和数据库功能的综合基准。

Result: FullStack-Dev在前端、后端和数据库测试用例上分别比之前最先进方法提升了8.7%、38.2%和15.9%。FullStack-Learn通过自学习将30B模型的性能在前端、后端和数据库测试集上分别提升了9.7%、9.5%和2.8%。

Conclusion: FullStack-Agent是一个有效的全栈智能编码系统，通过多智能体框架、自学习方法和综合基准的结合，显著提升了全栈网站开发能力，解决了现有代码智能体仅关注前端的局限性。

Abstract: Assisting non-expert users to develop complex interactive websites has become a popular task for LLM-powered code agents. However, existing code agents tend to only generate frontend web pages, masking the lack of real full-stack data processing and storage with fancy visual effects. Notably, constructing production-level full-stack web applications is far more challenging than only generating frontend web pages, demanding careful control of data flow, comprehensive understanding of constantly updating packages and dependencies, and accurate localization of obscure bugs in the codebase. To address these difficulties, we introduce FullStack-Agent, a unified agent system for full-stack agentic coding that consists of three parts: (1) FullStack-Dev, a multi-agent framework with strong planning, code editing, codebase navigation, and bug localization abilities. (2) FullStack-Learn, an innovative data-scaling and self-improving method that back-translates crawled and synthesized website repositories to improve the backbone LLM of FullStack-Dev. (3) FullStack-Bench, a comprehensive benchmark that systematically tests the frontend, backend and database functionalities of the generated website. Our FullStack-Dev outperforms the previous state-of-the-art method by 8.7%, 38.2%, and 15.9% on the frontend, backend, and database test cases respectively. Additionally, FullStack-Learn raises the performance of a 30B model by 9.7%, 9.5%, and 2.8% on the three sets of test cases through self-improvement, demonstrating the effectiveness of our approach. The code is released at https://github.com/mnluzimu/FullStack-Agent.

</details>
