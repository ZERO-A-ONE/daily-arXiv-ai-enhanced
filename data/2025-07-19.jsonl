{"id": "2507.12568", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.12568", "abs": "https://arxiv.org/abs/2507.12568", "authors": ["Sheng Liu", "Panos Papadimitratos"], "title": "Safeguarding Federated Learning-based Road Condition Classification", "comment": "Accepted by IEEE Conference on Communications and Network Security\n  (CNS) 2025", "summary": "Federated Learning (FL) has emerged as a promising solution for\nprivacy-preserving autonomous driving, specifically camera-based Road Condition\nClassification (RCC) systems, harnessing distributed sensing, computing, and\ncommunication resources on board vehicles without sharing sensitive image data.\nHowever, the collaborative nature of FL-RCC frameworks introduces new\nvulnerabilities: Targeted Label Flipping Attacks (TLFAs), in which malicious\nclients (vehicles) deliberately alter their training data labels to compromise\nthe learned model inference performance. Such attacks can, e.g., cause a\nvehicle to mis-classify slippery, dangerous road conditions as pristine and\nexceed recommended speed. However, TLFAs for FL-based RCC systems are largely\nmissing. We address this challenge with a threefold contribution: 1) we\ndisclose the vulnerability of existing FL-RCC systems to TLFAs; 2) we introduce\na novel label-distance-based metric to precisely quantify the safety risks\nposed by TLFAs; and 3) we propose FLARE, a defensive mechanism leveraging\nneuron-wise analysis of the output layer to mitigate TLFA effects. Extensive\nexperiments across three RCC tasks, four evaluation metrics, six baselines, and\nthree deep learning models demonstrate both the severity of TLFAs on FL-RCC\nsystems and the effectiveness of FLARE in mitigating the attack impact."}
{"id": "2507.12670", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.12670", "abs": "https://arxiv.org/abs/2507.12670", "authors": ["Shogo Murasaki", "Kazumasa Omote", "Keita Emura"], "title": "On the Consideration of Vanity Address Generation via Identity-Based Signatures", "comment": null, "summary": "An address is indicated as an identifier of the user on the blockchain, and\nis defined by a hash value of the ECDSA verification key. A vanity address is\nan address that embeds custom characters such as a name. To generate a vanity\naddress, a classical try-and-error method is employed, and thus the number of\ncharacters to be embedded is limited. In this paper, we focus on the\nfunctionality of identity-based signatures (IBS) where any strings can be\nemployed as a verification key, and explore whether IBS can be used for\ngenerating a vanity address. We attach importance to the fact that it is not\nrealistic to replace ECDSA with key recovery, which is currently employed for\nissuing transactions in Ethereum, to an IBS scheme. Even if this replacement is\npossible, it is not a reasonable price for the ease of the vanity address\ngeneration. Thus, we pay attention to a generic construction of IBS from\nsignatures, and construct an IBS scheme from ECDSA with key recovery. Though we\ncannot directly generate a vanity address due to the key recovery functionality\nof the underlying ECDSA, we can connect any string with an address due to the\nfunctionality of IBS that can give additional meaning to the address. We\nimplement our system by Solidity, and demonstrate that the gas cost is almost\nsame as that of the ECDSA signature verification."}
{"id": "2507.12919", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.12919", "abs": "https://arxiv.org/abs/2507.12919", "authors": ["Victoria Childress", "Josh Collyer", "Jodie Knapp"], "title": "Architectural Backdoors in Deep Learning: A Survey of Vulnerabilities, Detection, and Defense", "comment": "35 pages, Under review for ACM Computing Surveys", "summary": "Architectural backdoors pose an under-examined but critical threat to deep\nneural networks, embedding malicious logic directly into a model's\ncomputational graph. Unlike traditional data poisoning or parameter\nmanipulation, architectural backdoors evade standard mitigation techniques and\npersist even after clean retraining. This survey systematically consolidates\nresearch on architectural backdoors, spanning compiler-level manipulations,\ntainted AutoML pipelines, and supply-chain vulnerabilities. We assess emerging\ndetection and defense strategies, including static graph inspection, dynamic\nfuzzing, and partial formal verification, and highlight their limitations\nagainst distributed or stealth triggers. Despite recent progress, scalable and\npractical defenses remain elusive. We conclude by outlining open challenges and\nproposing directions for strengthening supply-chain security, cryptographic\nmodel attestations, and next-generation benchmarks. This survey aims to guide\nfuture research toward comprehensive defenses against structural backdoor\nthreats in deep learning systems."}
{"id": "2507.12937", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.12937", "abs": "https://arxiv.org/abs/2507.12937", "authors": ["Zhuohan Cui", "Zikun Song"], "title": "Enterprise Security Incident Analysis and Countermeasures Based on the T-Mobile Data Breach", "comment": null, "summary": "This paper presents a comprehensive analysis of T-Mobile's critical data\nbreaches in 2021 and 2023, alongside a full-spectrum security audit targeting\nits systems, infrastructure, and publicly exposed endpoints. By combining\ncase-based vulnerability assessments with active ethical hacking\ntechniques--including Shodan reconnaissance, API misuse simulations, VNC\nbrute-forcing, firmware reverse engineering, and web application scans--we\nuncover structural weaknesses persisting beyond the initial breach events.\nBuilding on these findings, we propose a multi-layered defensive strategy\nencompassing Zero Trust Architecture, granular role-based access control,\nnetwork segmentation, firmware encryption using AES with integrity checks, and\nAPI rate limiting and token lifecycle control. Financial modelling demonstrates\nthat a five-year investment yields less than 1.1% of expected breach losses,\nvalidating the cost-effectiveness of proactive security measures. Our work\nbridges post-incident forensic analysis with hands-on security evaluation,\nproviding an actionable blueprint for large-scale telecoms seeking operational\nresilience, regulatory compliance, and cross-domain threat readiness."}
{"id": "2507.12472", "categories": ["cs.SE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.12472", "abs": "https://arxiv.org/abs/2507.12472", "authors": ["Lingzhe Zhang", "Tong Jia", "Mengxi Jia", "Yifan Wu", "Aiwei Liu", "Yong Yang", "Zhonghai Wu", "Xuming Hu", "Philip S. Yu", "Ying Li"], "title": "A Survey of AIOps in the Era of Large Language Models", "comment": "Accepted By CSUR, an extended version of \"A Survey of AIOps for\n  Failure Management in the Era of Large Language Models\" [arXiv:2406.11213]", "summary": "As large language models (LLMs) grow increasingly sophisticated and\npervasive, their application to various Artificial Intelligence for IT\nOperations (AIOps) tasks has garnered significant attention. However, a\ncomprehensive understanding of the impact, potential, and limitations of LLMs\nin AIOps remains in its infancy. To address this gap, we conducted a detailed\nsurvey of LLM4AIOps, focusing on how LLMs can optimize processes and improve\noutcomes in this domain. We analyzed 183 research papers published between\nJanuary 2020 and December 2024 to answer four key research questions (RQs). In\nRQ1, we examine the diverse failure data sources utilized, including advanced\nLLM-based processing techniques for legacy data and the incorporation of new\ndata sources enabled by LLMs. RQ2 explores the evolution of AIOps tasks,\nhighlighting the emergence of novel tasks and the publication trends across\nthese tasks. RQ3 investigates the various LLM-based methods applied to address\nAIOps challenges. Finally, RQ4 reviews evaluation methodologies tailored to\nassess LLM-integrated AIOps approaches. Based on our findings, we discuss the\nstate-of-the-art advancements and trends, identify gaps in existing research,\nand propose promising directions for future exploration."}
{"id": "2507.12484", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.12484", "abs": "https://arxiv.org/abs/2507.12484", "authors": ["Jaros≈Çaw A. Chudziak", "Adam Kostka"], "title": "AI-Powered Math Tutoring: Platform for Personalized and Adaptive Education", "comment": "8 pages, 5 figures", "summary": "The growing ubiquity of artificial intelligence (AI), in particular large\nlanguage models (LLMs), has profoundly altered the way in which learners gain\nknowledge and interact with learning material, with many claiming that AI\npositively influences their learning achievements. Despite this advancement,\ncurrent AI tutoring systems face limitations associated with their reactive\nnature, often providing direct answers without encouraging deep reflection or\nincorporating structured pedagogical tools and strategies. This limitation is\nmost apparent in the field of mathematics, in which AI tutoring systems remain\nunderdeveloped. This research addresses the question: How can AI tutoring\nsystems move beyond providing reactive assistance to enable structured,\nindividualized, and tool-assisted learning experiences? We introduce a novel\nmulti-agent AI tutoring platform that combines adaptive and personalized\nfeedback, structured course generation, and textbook knowledge retrieval to\nenable modular, tool-assisted learning processes. This system allows students\nto learn new topics while identifying and targeting their weaknesses, revise\nfor exams effectively, and practice on an unlimited number of personalized\nexercises. This article contributes to the field of artificial intelligence in\neducation by introducing a novel platform that brings together pedagogical\nagents and AI-driven components, augmenting the field with modular and\neffective systems for teaching mathematics."}
{"id": "2507.13023", "categories": ["cs.CR", "q-fin.TR"], "pdf": "https://arxiv.org/pdf/2507.13023", "abs": "https://arxiv.org/abs/2507.13023", "authors": ["Fei Wu", "Danning Sui", "Thomas Thiery", "Mallesh Pai"], "title": "Measuring CEX-DEX Extracted Value and Searcher Profitability: The Darkest of the MEV Dark Forest", "comment": "Accepted by AFT 2025", "summary": "This paper provides a comprehensive empirical analysis of the economics and\ndynamics behind arbitrages between centralized and decentralized exchanges\n(CEX-DEX) on Ethereum. We refine heuristics to identify arbitrage transactions\nfrom on-chain data and introduce a robust empirical framework to estimate\narbitrage revenue without knowing traders' actual behaviors on CEX. Leveraging\nan extensive dataset spanning 19 months from August 2023 to March 2025, we\nestimate a total of 233.8M USD extracted by 19 major CEX-DEX searchers from\n7,203,560 identified CEX-DEX arbitrages. Our analysis reveals increasing\ncentralization trends as three searchers captured three-quarters of both volume\nand extracted value. We also demonstrate that searchers' profitability is tied\nto their integration level with block builders and uncover exclusive\nsearcher-builder relationships and their market impact. Finally, we correct the\npreviously underestimated profitability of block builders who vertically\nintegrate with a searcher. These insights illuminate the darkest corner of the\nMEV landscape and highlight the critical implications of CEX-DEX arbitrages for\nEthereum's decentralization."}
{"id": "2507.12480", "categories": ["cs.SE", "cs.AI", "cs.ET"], "pdf": "https://arxiv.org/pdf/2507.12480", "abs": "https://arxiv.org/abs/2507.12480", "authors": ["Nazanin Siavash", "Armin Moin"], "title": "LLM-Powered Quantum Code Transpilation", "comment": "IEEE International Conference on Quantum Computing and Engineering\n  (QCE) 2025 - Extended Abstract", "summary": "There exist various Software Development Kits (SDKs) tailored to different\nquantum computing platforms. These are known as Quantum SDKs (QSDKs). Examples\ninclude but are not limited to Qiskit, Cirq, and PennyLane. However, this\ndiversity presents significant challenges for interoperability and\ncross-platform development of hybrid quantum-classical software systems.\nTraditional rule-based transpilers for translating code between QSDKs are\ntime-consuming to design and maintain, requiring deep expertise and rigid\nmappings in the source and destination code. In this study, we explore the use\nof Large Language Models (LLMs) as a flexible and automated solution.\nLeveraging their pretrained knowledge and contextual reasoning capabilities, we\nposition LLMs as programming language-agnostic transpilers capable of\nconverting quantum programs from one QSDK to another while preserving\nfunctional equivalence. Our approach eliminates the need for manually defined\ntransformation rules and offers a scalable solution to quantum software\nportability. This work represents a step toward enabling intelligent,\ngeneral-purpose transpilation in the quantum computing ecosystem."}
{"id": "2507.12494", "categories": ["cs.AI", "cs.GT", "cs.MA", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.12494", "abs": "https://arxiv.org/abs/2507.12494", "authors": ["Dustin Holley", "Jovin D'sa", "Hossein Nourkhiz Mahjoub", "Gibran Ali"], "title": "MR-LDM -- The Merge-Reactive Longitudinal Decision Model: Game Theoretic Human Decision Modeling for Interactive Sim Agents", "comment": "8 pages", "summary": "Enhancing simulation environments to replicate real-world driver behavior,\ni.e., more humanlike sim agents, is essential for developing autonomous vehicle\ntechnology. In the context of highway merging, previous works have studied the\noperational-level yielding dynamics of lag vehicles in response to a merging\ncar at highway on-ramps. Other works focusing on tactical decision modeling\ngenerally consider limited action sets or utilize payoff functions with large\nparameter sets and limited payoff bounds. In this work, we aim to improve the\nsimulation of the highway merge scenario by targeting a game theoretic model\nfor tactical decision-making with improved payoff functions and lag actions. We\ncouple this with an underlying dynamics model to have a unified decision and\ndynamics model that can capture merging interactions and simulate more\nrealistic interactions in an explainable and interpretable fashion. The\nproposed model demonstrated good reproducibility of complex interactions when\nvalidated on a real-world dataset. The model was finally integrated into a high\nfidelity simulation environment and confirmed to have adequate computation time\nefficiency for use in large-scale simulations to support autonomous vehicle\ndevelopment."}
{"id": "2507.13028", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.13028", "abs": "https://arxiv.org/abs/2507.13028", "authors": ["Niklas Busch", "Philip Klostermeyer", "Jan H. Klemmer", "Yasemin Acar", "Sascha Fahl"], "title": "From Paranoia to Compliance: The Bumpy Road of System Hardening Practices on Stack Exchange", "comment": "14 pages, 5 figures", "summary": "Hardening computer systems against cyberattacks is crucial for security.\nHowever, past incidents illustrated, that many system operators struggle with\neffective system hardening. Hence, many computer systems and applications\nremain insecure. So far, the research community lacks an in-depth understanding\nof system operators motivation, practices, and challenges around system\nhardening. With a focus on practices and challenges, we qualitatively analyzed\n316 Stack Exchange (SE) posts related to system hardening. We find that access\ncontrol and deployment-related issues are the most challenging, and system\noperators suffer from misconceptions and unrealistic expectations. Most\nfrequently, posts focused on operating systems and server applications. System\noperators were driven by the fear of their systems getting attacked or by\ncompliance reasons. Finally, we discuss our research questions, make\nrecommendations for future system hardening, and illustrate the implications of\nour work."}
{"id": "2507.12482", "categories": ["cs.SE", "cs.AI", "cs.CE", "cs.LG", "68N30, 68T05, 68T50", "D.2.5; D.2.7; F.3.2; I.2.6; I.2.7"], "pdf": "https://arxiv.org/pdf/2507.12482", "abs": "https://arxiv.org/abs/2507.12482", "authors": ["Ishraq Khan", "Assad Chowdary", "Sharoz Haseeb", "Urvish Patel"], "title": "Kodezi Chronos: A Debugging-First Language Model for Repository-Scale, Memory-Driven Code Understanding", "comment": "10 pages, 10 figures, 7 tables, IEEE Conference format, Q4 2025 model\n  release, Q1 2026 Kodezi OS deployment", "summary": "Large Language Models (LLMs) have advanced code generation and software\nautomation, but are fundamentally constrained by limited inference-time context\nand lack of explicit code structure reasoning. We introduce Kodezi Chronos, a\nnext-generation architecture for autonomous code understanding, debugging, and\nmaintenance, designed to operate across ultra-long contexts comprising entire\ncodebases, histories, and documentation, all without fixed window limits.\nKodezi Chronos leverages a multi-level embedding memory engine, combining\nvector and graph-based indexing with continuous code-aware retrieval. This\nenables efficient and accurate reasoning over millions of lines of code,\nsupporting repository-scale comprehension, multi-file refactoring, and\nreal-time self-healing actions. Our evaluation introduces a novel Multi Random\nRetrieval benchmark, specifically tailored to the software engineering domain.\nUnlike classical retrieval benchmarks, this method requires the model to\nresolve arbitrarily distant and obfuscated associations across code artifacts,\nsimulating realistic tasks such as variable tracing, dependency migration, and\nsemantic bug localization. Chronos outperforms prior LLMs and code models,\ndemonstrating a 23% improvement in real-world bug detection and reducing\ndebugging cycles by up to 40% compared to traditional sequence-based\napproaches. By natively interfacing with IDEs and CI/CD workflows, Chronos\nenables seamless, autonomous software maintenance, elevating code reliability\nand productivity while reducing manual effort. These results mark a critical\nadvance toward self-sustaining, continuously optimized software ecosystems."}
{"id": "2507.12599", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12599", "abs": "https://arxiv.org/abs/2507.12599", "authors": ["L√©o Sauli√®res"], "title": "A Survey of Explainable Reinforcement Learning: Targets, Methods and Needs", "comment": "69 pages, 19 figures", "summary": "The success of recent Artificial Intelligence (AI) models has been\naccompanied by the opacity of their internal mechanisms, due notably to the use\nof deep neural networks. In order to understand these internal mechanisms and\nexplain the output of these AI models, a set of methods have been proposed,\ngrouped under the domain of eXplainable AI (XAI). This paper focuses on a\nsub-domain of XAI, called eXplainable Reinforcement Learning (XRL), which aims\nto explain the actions of an agent that has learned by reinforcement learning.\nWe propose an intuitive taxonomy based on two questions \"What\" and \"How\". The\nfirst question focuses on the target that the method explains, while the second\nrelates to the way the explanation is provided. We use this taxonomy to provide\na state-of-the-art review of over 250 papers. In addition, we present a set of\ndomains close to XRL, which we believe should get attention from the community.\nFinally, we identify some needs for the field of XRL."}
{"id": "2507.13038", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.13038", "abs": "https://arxiv.org/abs/2507.13038", "authors": ["Yu Cui", "Hongyang Du"], "title": "MAD-Spear: A Conformity-Driven Prompt Injection Attack on Multi-Agent Debate Systems", "comment": null, "summary": "Multi-agent debate (MAD) systems leverage collaborative interactions among\nlarge language models (LLMs) agents to improve reasoning capabilities. While\nrecent studies have focused on increasing the accuracy and scalability of MAD\nsystems, their security vulnerabilities have received limited attention. In\nthis work, we introduce MAD-Spear, a targeted prompt injection attack that\ncompromises a small subset of agents but significantly disrupts the overall MAD\nprocess. Manipulated agents produce multiple plausible yet incorrect responses,\nexploiting LLMs' conformity tendencies to propagate misinformation and degrade\nconsensus quality. Furthermore, the attack can be composed with other\nstrategies, such as communication attacks, to further amplify its impact by\nincreasing the exposure of agents to incorrect responses. To assess MAD's\nresilience under attack, we propose a formal definition of MAD fault-tolerance\nand develop a comprehensive evaluation framework that jointly considers\naccuracy, consensus efficiency, and scalability. Extensive experiments on five\nbenchmark datasets with varying difficulty levels demonstrate that MAD-Spear\nconsistently outperforms the baseline attack in degrading system performance.\nAdditionally, we observe that agent diversity substantially improves MAD\nperformance in mathematical reasoning tasks, which challenges prior work\nsuggesting that agent diversity has minimal impact on performance. These\nfindings highlight the urgent need to improve the security in MAD design."}
{"id": "2507.12483", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.12483", "abs": "https://arxiv.org/abs/2507.12483", "authors": ["Dong Wang", "Hanmo You", "Lingwei Zhu", "Kaiwei Lin", "Zheng Chen", "Chen Yang", "Junji Yu", "Zan Wang", "Junjie Chen"], "title": "A Survey of Reinforcement Learning for Software Engineering", "comment": null, "summary": "Reinforcement Learning (RL) has emerged as a powerful paradigm for sequential\ndecision-making and has attracted growing interest across various domains,\nparticularly following the advent of Deep Reinforcement Learning (DRL) in 2015.\nSimultaneously, the rapid advancement of Large Language Models (LLMs) has\nfurther fueled interest in integrating RL with LLMs to enable more adaptive and\nintelligent systems. In the field of software engineering (SE), the increasing\ncomplexity of systems and the rising demand for automation have motivated\nresearchers to apply RL to a broad range of tasks, from software design and\ndevelopment to quality assurance and maintenance. Despite growing research in\nRL-for-SE, there remains a lack of a comprehensive and systematic survey of\nthis evolving field. To address this gap, we reviewed 115 peer-reviewed studies\npublished across 22 premier SE venues since the introduction of DRL. We\nconducted a comprehensive analysis of publication trends, categorized SE topics\nand RL algorithms, and examined key factors such as dataset usage, model design\nand optimization, and evaluation practices. Furthermore, we identified open\nchallenges and proposed future research directions to guide and inspire ongoing\nwork in this evolving area. To summarize, this survey offers the first\nsystematic mapping of RL applications in software engineering, aiming to\nsupport both researchers and practitioners in navigating the current landscape\nand advancing the field. Our artifacts are publicly available:\nhttps://github.com/KaiWei-Lin-lanina/RL4SE."}
{"id": "2507.12666", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12666", "abs": "https://arxiv.org/abs/2507.12666", "authors": ["Alex Zook", "Josef Spjut", "Jonathan Tremblay"], "title": "Fly, Fail, Fix: Iterative Game Repair with Reinforcement Learning and Large Multimodal Models", "comment": "Published at Reinforcement Learning and Video Games workshop\n  https://sites.google.com/view/rlvg-workshop-2025/home", "summary": "Game design hinges on understanding how static rules and content translate\ninto dynamic player behavior - something modern generative systems that inspect\nonly a game's code or assets struggle to capture. We present an automated\ndesign iteration framework that closes this gap by pairing a reinforcement\nlearning (RL) agent, which playtests the game, with a large multimodal model\n(LMM), which revises the game based on what the agent does. In each loop the RL\nplayer completes several episodes, producing (i) numerical play metrics and/or\n(ii) a compact image strip summarising recent video frames. The LMM designer\nreceives a gameplay goal and the current game configuration, analyses the play\ntraces, and edits the configuration to steer future behaviour toward the goal.\nWe demonstrate results that LMMs can reason over behavioral traces supplied by\nRL agents to iteratively refine game mechanics, pointing toward practical,\nscalable tools for AI-assisted game design."}
{"id": "2507.13042", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.13042", "abs": "https://arxiv.org/abs/2507.13042", "authors": ["Taki Eddine Djidjekh", "Ga√´l Loubet", "Alexandru Takacs"], "title": "Backscattering-Based Security in Wireless Power Transfer Applied to Battery-Free BLE Sensors", "comment": null, "summary": "The integration of security and energy efficiency in Internet of Things\nsystems remains a critical challenge, particularly for battery-free and\nresource-constrained devices. This paper explores the scalability and\nprotocol-agnostic nature of a backscattering-based security mechanism by\nintegrating it into Bluetooth Low Energy battery-free Wireless Sensor Network.\nThe proposed approach leverages the Wireless Power Transfer link, traditionally\nused for energy harvesting, to generate additional identification signals\nwithout increasing energy consumption or computational demands. Experimental\nvalidation demonstrates the solution's functionality using compact, low-gain\nantenna, ensuring compatibility with size-constrained applications such as\nStructural Health Monitoring and smart transport. Furthermore, this work\naddresses the challenges associated with backscattering dynamic range and\nmulti-node Wireless Sensor Network scenarios, discussing potential collisions\nbetween identification signals and proposing future improvements to enhance\ngeneralizability and scalability. The findings underscore the potential of the\nbackscattering-based security mechanism for creating secure, sustainable, and\nscalable IoT deployments across diverse protocols and applications."}
{"id": "2507.12558", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.12558", "abs": "https://arxiv.org/abs/2507.12558", "authors": ["Tien P. T. Le", "Anh M. T. Bui", "Huy N. D. Pham", "Alessio Bucaioni", "Phuong T. Nguyen"], "title": "When Retriever Meets Generator: A Joint Model for Code Comment Generation", "comment": "The paper has been peer-reviewed and accepted for publication in the\n  proceedings of the 19th ACM/IEEE International Symposium on Empirical\n  Software Engineering and Measurement (ESEM 2025)", "summary": "Automatically generating concise, informative comments for source code can\nlighten documentation effort and accelerate program comprehension.\nRetrieval-augmented approaches first fetch code snippets with existing comments\nand then synthesize a new comment, yet retrieval and generation are typically\noptimized in isolation, allowing irrelevant neighbors topropagate noise\ndownstream. To tackle the issue, we propose a novel approach named RAGSum with\nthe aim of both effectiveness and efficiency in recommendations. RAGSum is\nbuilt on top offuse retrieval and generation using a single CodeT5 backbone. We\nreport preliminary results on a unified retrieval-generation framework built on\nCodeT5. A contrastive pre-training phase shapes code embeddings for\nnearest-neighbor search; these weights then seed end-to-end training with a\ncomposite loss that (i) rewards accurate top-k retrieval; and (ii) minimizes\ncomment-generation error. More importantly, a lightweight self-refinement loop\nis deployed to polish the final output. We evaluated theframework on three\ncross-language benchmarks (Java, Python, C), and compared it with three\nwell-established baselines. The results show that our approach substantially\noutperforms thebaselines with respect to BLEU, METEOR, and ROUTE-L. These\nfindings indicate that tightly coupling retrieval and generationcan raise the\nceiling for comment automation and motivateforthcoming replications and\nqualitative developer studies."}
{"id": "2507.12691", "categories": ["cs.AI", "cs.LG", "I.2.7; K.4.1"], "pdf": "https://arxiv.org/pdf/2507.12691", "abs": "https://arxiv.org/abs/2507.12691", "authors": ["Avi Parrack", "Carlo Leonardo Attubato", "Stefan Heimersheim"], "title": "Benchmarking Deception Probes via Black-to-White Performance Boosts", "comment": "Preprint. 37 pages, 10 figures, 7 tables", "summary": "AI assistants will occasionally respond deceptively to user queries.\nRecently, linear classifiers (called \"deception probes\") have been trained to\ndistinguish the internal activations of a language model during deceptive\nversus honest responses. However, it's unclear how effective these probes are\nat detecting deception in practice, nor whether such probes are resistant to\nsimple counter strategies from a deceptive assistant who wishes to evade\ndetection. In this paper, we compare white-box monitoring (where the monitor\nhas access to token-level probe activations) to black-box monitoring (without\nsuch access). We benchmark deception probes by the extent to which the white\nbox monitor outperforms the black-box monitor, i.e. the black-to-white\nperformance boost. We find weak but encouraging black-to-white performance\nboosts from existing deception probes."}
{"id": "2507.13169", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13169", "abs": "https://arxiv.org/abs/2507.13169", "authors": ["Jeremy McHugh", "Kristina ≈†ekrst", "Jon Cefalu"], "title": "Prompt Injection 2.0: Hybrid AI Threats", "comment": null, "summary": "Prompt injection attacks, where malicious input is designed to manipulate AI\nsystems into ignoring their original instructions and following unauthorized\ncommands instead, were first discovered by Preamble, Inc. in May 2022 and\nresponsibly disclosed to OpenAI. Over the last three years, these attacks have\ncontinued to pose a critical security threat to LLM-integrated systems. The\nemergence of agentic AI systems, where LLMs autonomously perform multistep\ntasks through tools and coordination with other agents, has fundamentally\ntransformed the threat landscape. Modern prompt injection attacks can now\ncombine with traditional cybersecurity exploits to create hybrid threats that\nsystematically evade traditional security controls. This paper presents a\ncomprehensive analysis of Prompt Injection 2.0, examining how prompt injections\nintegrate with Cross-Site Scripting (XSS), Cross-Site Request Forgery (CSRF),\nand other web security vulnerabilities to bypass traditional security measures.\nWe build upon Preamble's foundational research and mitigation technologies,\nevaluating them against contemporary threats, including AI worms, multi-agent\ninfections, and hybrid cyber-AI attacks. Our analysis incorporates recent\nbenchmarks that demonstrate how traditional web application firewalls, XSS\nfilters, and CSRF tokens fail against AI-enhanced attacks. We also present\narchitectural solutions that combine prompt isolation, runtime security, and\nprivilege separation with novel threat detection capabilities."}
{"id": "2507.12561", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.12561", "abs": "https://arxiv.org/abs/2507.12561", "authors": ["Samal Nursapa", "Anastassiya Samuilova", "Alessio Bucaioni. Phuong T. Nguyen"], "title": "ROSE: Transformer-Based Refactoring Recommendation for Architectural Smells", "comment": "The paper has been peer-reviewed and accepted for publication in the\n  proceedings of the 19th ACM/IEEE International Symposium on Empirical\n  Software Engineering and Measurement (ESEM 2025)", "summary": "Architectural smells such as God Class, Cyclic Dependency, and Hub-like\nDependency degrade software quality and maintainability. Existing tools detect\nsuch smells but rarely suggest how to fix them. This paper explores the use of\npre-trained transformer models--CodeBERT and CodeT5--for recommending suitable\nrefactorings based on detected smells. We frame the task as a three-class\nclassification problem and fine-tune both models on over 2 million refactoring\ninstances mined from 11,149 open-source Java projects. CodeT5 achieves 96.9%\naccuracy and 95.2% F1, outperforming CodeBERT and traditional baselines. Our\nresults show that transformer-based models can effectively bridge the gap\nbetween smell detection and actionable repair, laying the foundation for future\nrefactoring recommendation systems. We release all code, models, and data under\nan open license to support reproducibility and further research."}
{"id": "2507.12801", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.12801", "abs": "https://arxiv.org/abs/2507.12801", "authors": ["Sosui Moribe", "Taketoshi Ushiama"], "title": "Imitating Mistakes in a Learning Companion AI Agent for Online Peer Learning", "comment": "This is the preprint version of the paper published in IMCOM 2025,\n  IEEE Xplore (DOI: 10.1109/IMCOM64595.2025.10857528)", "summary": "In recent years, peer learning has gained attention as a method that promotes\nspontaneous thinking among learners, and its effectiveness has been confirmed\nby numerous studies. This study aims to develop an AI Agent as a learning\ncompanion that enables peer learning anytime and anywhere. However, peer\nlearning between humans has various limitations, and it is not always\neffective. Effective peer learning requires companions at the same proficiency\nlevels. In this study, we assume that a learner's peers with the same\nproficiency level as the learner make the same mistakes as the learner does and\nfocus on English composition as a specific example to validate this approach."}
{"id": "2507.13313", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.13313", "abs": "https://arxiv.org/abs/2507.13313", "authors": ["Chao Feng", "Alberto Huertas Celdran", "Jing Han", "Heqing Ren", "Xi Cheng", "Zien Zeng", "Lucas Krauter", "Gerome Bovet", "Burkhard Stiller"], "title": "A Crowdsensing Intrusion Detection Dataset For Decentralized Federated Learning Models", "comment": null, "summary": "This paper introduces a dataset and experimental study for decentralized\nfederated learning (DFL) applied to IoT crowdsensing malware detection. The\ndataset comprises behavioral records from benign and eight malware families. A\ntotal of 21,582,484 original records were collected from system calls, file\nsystem activities, resource usage, kernel events, input/output events, and\nnetwork records. These records were aggregated into 30-second windows,\nresulting in 342,106 features used for model training and evaluation.\nExperiments on the DFL platform compare traditional machine learning (ML),\ncentralized federated learning (CFL), and DFL across different node counts,\ntopologies, and data distributions. Results show that DFL maintains competitive\nperformance while preserving data locality, outperforming CFL in most settings.\nThis dataset provides a solid foundation for studying the security of IoT\ncrowdsensing environments."}
{"id": "2507.12642", "categories": ["cs.SE", "cs.AI", "quant-ph"], "pdf": "https://arxiv.org/pdf/2507.12642", "abs": "https://arxiv.org/abs/2507.12642", "authors": ["Kiana Kheiri", "Aamna Aamir", "Andriy Miranskyy", "Chen Ding"], "title": "QSpark: Towards Reliable Qiskit Code Generation", "comment": null, "summary": "Quantum circuits must be error-resilient, yet LLMs like Granite-20B-Code and\nStarCoder often output flawed Qiskit code. We fine-tuned a 32 B model with two\nRL methods, Group Relative Policy Optimization (GRPO) and Odds-Ratio Preference\nOptimization (ORPO), using a richly annotated synthetic dataset. On the Qiskit\nHumanEval benchmark, ORPO reaches 56.29\\% Pass@1 ($\\approx+10$ pp over\nGranite-8B-QK) and GRPO hits 49\\%, both beating all general-purpose baselines;\non the original HumanEval they score 65.90\\% and 63.00\\%. GRPO excels on basic\ntasks (42/54), ORPO on intermediate ones (41/68), and neither solves the five\nadvanced tasks, highlighting clear gains yet room for progress in AI-assisted\nquantum programming."}
{"id": "2507.12806", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.12806", "abs": "https://arxiv.org/abs/2507.12806", "authors": ["Zhiwei Liu", "Jielin Qiu", "Shiyu Wang", "Jianguo Zhang", "Zuxin Liu", "Roshan Ram", "Haolin Chen", "Weiran Yao", "Huan Wang", "Shelby Heinecke", "Silvio Savarese", "Caiming Xiong"], "title": "MCPEval: Automatic MCP-based Deep Evaluation for AI Agent Models", "comment": "https://github.com/SalesforceAIResearch/MCPEval", "summary": "The rapid rise of Large Language Models (LLMs)-based intelligent agents\nunderscores the need for robust, scalable evaluation frameworks. Existing\nmethods rely on static benchmarks and labor-intensive data collection, limiting\npractical assessment. We introduce \\oursystemname, an open-source Model Context\nProtocol (MCP)-based framework that automates end-to-end task generation and\ndeep evaluation of LLM agents across diverse domains. MCPEval standardizes\nmetrics, seamlessly integrates with native agent tools, and eliminates manual\neffort in building evaluation pipelines. Empirical results across five\nreal-world domains show its effectiveness in revealing nuanced, domain-specific\nperformance. We publicly release MCPEval\nhttps://github.com/SalesforceAIResearch/MCPEval to promote reproducible and\nstandardized LLM agent evaluation."}
{"id": "2507.12872", "categories": ["cs.AI", "cs.CR", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.12872", "abs": "https://arxiv.org/abs/2507.12872", "authors": ["Rishane Dassanayake", "Mario Demetroudi", "James Walpole", "Lindley Lentati", "Jason R. Brown", "Edward James Young"], "title": "Manipulation Attacks by Misaligned AI: Risk Analysis and Safety Case Framework", "comment": "24 pages (14 pages main text, 4 pages bibliography, 6 pages\n  appendices), 3 figures", "summary": "Frontier AI systems are rapidly advancing in their capabilities to persuade,\ndeceive, and influence human behaviour, with current models already\ndemonstrating human-level persuasion and strategic deception in specific\ncontexts. Humans are often the weakest link in cybersecurity systems, and a\nmisaligned AI system deployed internally within a frontier company may seek to\nundermine human oversight by manipulating employees. Despite this growing\nthreat, manipulation attacks have received little attention, and no systematic\nframework exists for assessing and mitigating these risks. To address this, we\nprovide a detailed explanation of why manipulation attacks are a significant\nthreat and could lead to catastrophic outcomes. Additionally, we present a\nsafety case framework for manipulation risk, structured around three core lines\nof argument: inability, control, and trustworthiness. For each argument, we\nspecify evidence requirements, evaluation methodologies, and implementation\nconsiderations for direct application by AI companies. This paper provides the\nfirst systematic methodology for integrating manipulation risk into AI safety\ngovernance, offering AI companies a concrete foundation to assess and mitigate\nthese threats before deployment."}
{"id": "2507.12649", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.12649", "abs": "https://arxiv.org/abs/2507.12649", "authors": ["Christine van Stiphoudt", "Sergio Potenciano Menci", "Gilbert Fridgen"], "title": "A Three-Phase Evaluation Approach for new Information and Data Models in the Smart Grid Domain", "comment": null, "summary": "The ongoing digitalisation of the smart grid is resulting in an increase in\nautomated information exchanges across distributed energy systems. This process\nhas led to the development of new information and data models when the existing\nones fall short. To prevent potential disruptions caused by flaws in the newly\ndesigned information and data models, it is essential to evaluate them during\nthe design process before they are implemented in operation.\n  Currently, general explicit evaluation approaches outside the smart grid\ndomain stay at a high level without defining clear steps. Meanwhile, implicit\nevaluation approaches in the smart grid domain focus on testing systems that\nutilise information and data models already in use for functionality in terms\nof conformance and interoperability. Notably, no combination of explicit and\nimplicit evaluation approaches for newly designed information and data models\noffers a clearly defined set of steps during their design process in the smart\ngrid context.\n  Consequently, we design a three-phase evaluation approach using design\nscience research to address this gap. Our evaluation approach combines explicit\nand implicit evaluation methods and is applicable when developing new\ninformation and data models. We use the development of an information model and\ndata model focused on industrial flexibility descriptions to refine our\nevaluation approach. Additionally, we provide lessons learned from our\nexperience."}
{"id": "2507.12820", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.12820", "abs": "https://arxiv.org/abs/2507.12820", "authors": ["Shiquan Wang", "Ruiyu Fang", "Zhongjiang He", "Shuangyong Song", "Yongxiang Li"], "title": "Emotional Support with LLM-based Empathetic Dialogue Generation", "comment": null, "summary": "Emotional Support Conversation (ESC) aims to provide empathetic and effective\nemotional assistance through dialogue, addressing the growing demand for mental\nhealth support. This paper presents our solution for the NLPCC 2025 Task 8 ESC\nevaluation, where we leverage large-scale language models enhanced by prompt\nengineering and finetuning techniques. We explore both parameter-efficient\nLow-Rank Adaptation and full-parameter fine-tuning strategies to improve the\nmodel's ability to generate supportive and contextually appropriate responses.\nOur best model ranked second in the competition, highlighting the potential of\ncombining LLMs with effective adaptation methods for ESC tasks. Future work\nwill focus on further enhancing emotional understanding and response\npersonalization to build more practical and reliable emotional support systems."}
{"id": "2507.12653", "categories": ["cs.SE", "cs.CL", "H.4.m"], "pdf": "https://arxiv.org/pdf/2507.12653", "abs": "https://arxiv.org/abs/2507.12653", "authors": ["Jo√£o Granja-Correia", "Remedios Hern√°ndez-Linares", "Luca Ferranti", "Arm√©nio Rego"], "title": "A Fuzzy Approach to Project Success: Measuring What Matters", "comment": "3 pages, 1 figure, presented at FUZZ-IEEE 2025", "summary": "This paper introduces a novel approach to project success evaluation by\nintegrating fuzzy logic into an existing construct. Traditional Likert-scale\nmeasures often overlook the context-dependent and multifaceted nature of\nproject success. The proposed hierarchical Type-1 Mamdani fuzzy system\nprioritizes sustained positive impact for end-users, reducing emphasis on\nsecondary outcomes like stakeholder satisfaction and internal project success.\nThis dynamic approach may provide a more accurate measure of project success\nand could be adaptable to complex evaluations. Future research will focus on\nempirical testing and broader applications of fuzzy logic in social science."}
{"id": "2507.12821", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12821", "abs": "https://arxiv.org/abs/2507.12821", "authors": ["Lance Ying", "Katherine M. Collins", "Prafull Sharma", "Cedric Colas", "Kaiya Ivy Zhao", "Adrian Weller", "Zenna Tavares", "Phillip Isola", "Samuel J. Gershman", "Jacob D. Andreas", "Thomas L. Griffiths", "Francois Chollet", "Kelsey R. Allen", "Joshua B. Tenenbaum"], "title": "Assessing adaptive world models in machines with novel games", "comment": "17 pages, 4 figures", "summary": "Human intelligence exhibits a remarkable capacity for rapid adaptation and\neffective problem-solving in novel and unfamiliar contexts. We argue that this\nprofound adaptability is fundamentally linked to the efficient construction and\nrefinement of internal representations of the environment, commonly referred to\nas world models, and we refer to this adaptation mechanism as world model\ninduction. However, current understanding and evaluation of world models in\nartificial intelligence (AI) remains narrow, often focusing on static\nrepresentations learned from training on a massive corpora of data, instead of\nthe efficiency and efficacy of models in learning these representations through\ninteraction and exploration within a novel environment. In this Perspective, we\nprovide a view of world model induction drawing on decades of research in\ncognitive science on how humans learn and adapt so efficiently; we then call\nfor a new evaluation framework for assessing adaptive world models in AI.\nConcretely, we propose a new benchmarking paradigm based on suites of carefully\ndesigned games with genuine, deep and continually refreshing novelty in the\nunderlying game structures -- we refer to this kind of games as novel games. We\ndetail key desiderata for constructing these games and propose appropriate\nmetrics to explicitly challenge and evaluate the agent's ability for rapid\nworld model induction. We hope that this new evaluation framework will inspire\nfuture evaluation efforts on world models in AI and provide a crucial step\ntowards developing AI systems capable of the human-like rapid adaptation and\nrobust generalization -- a critical component of artificial general\nintelligence."}
{"id": "2507.12665", "categories": ["cs.SE", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.12665", "abs": "https://arxiv.org/abs/2507.12665", "authors": ["Salvador D. Escobedo"], "title": "Single Conversation Methodology: A Human-Centered Protocol for AI-Assisted Software Development", "comment": "Style reviewed by a LLM for improving clarity and English syntax", "summary": "We propose the Single Conversation Methodology (SCM), a novel and pragmatic\napproach to software development using large language models (LLMs). In\ncontrast to ad hoc interactions with generative AI, SCM emphasizes a structured\nand persistent development dialogue, where all stages of a project - from\nrequirements to architecture and implementation - unfold within a single,\nlong-context conversation. The methodology is grounded on principles of\ncognitive clarity, traceability, modularity, and documentation. We define its\nphases, best practices, and philosophical stance, while arguing that SCM offers\na necessary correction to the passive reliance on LLMs prevalent in current\npractices. We aim to reassert the active role of the developer as architect and\nsupervisor of the intelligent tool."}
{"id": "2507.12862", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.12862", "abs": "https://arxiv.org/abs/2507.12862", "authors": ["Hussein Abbass", "Taylan Akay", "Harrison Tolley"], "title": "Information-Theoretic Aggregation of Ethical Attributes in Simulated-Command", "comment": null, "summary": "In the age of AI, human commanders need to use the computational powers\navailable in today's environment to simulate a very large number of scenarios.\nWithin each scenario, situations occur where different decision design options\ncould have ethical consequences. Making these decisions reliant on human\njudgement is both counter-productive to the aim of exploring very large number\nof scenarios in a timely manner and infeasible when considering the workload\nneeded to involve humans in each of these choices. In this paper, we move human\njudgement outside the simulation decision cycle. Basically, the human will\ndesign the ethical metric space, leaving it to the simulated environment to\nexplore the space. When the simulation completes its testing cycles, the\ntesting environment will come back to the human commander with a few options to\nselect from. The human commander will then exercise human-judgement to select\nthe most appropriate course of action, which will then get executed\naccordingly. We assume that the problem of designing metrics that are\nsufficiently granular to assess the ethical implications of decisions is\nsolved. Subsequently, the fundamental problem we look at in this paper is how\nto weight ethical decisions during the running of these simulations; that is,\nhow to dynamically weight the ethical attributes when agents are faced with\ndecision options with ethical implications during generative simulations. The\nmulti-criteria decision making literature has started to look at nearby\nproblems, where the concept of entropy has been used to determine the weights\nduring aggregation. We draw from that literature different approaches to\nautomatically calculate the weights for ethical attributes during\nsimulation-based testing and evaluation."}
{"id": "2507.13035", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.13035", "abs": "https://arxiv.org/abs/2507.13035", "authors": ["Keila Lucas", "Rohit Gheyi", "M√°rcio Ribeiro", "Fabio Palomba", "Luana Martins", "Elvys Soares"], "title": "Investigating the Performance of Small Language Models in Detecting Test Smells in Manual Test Cases", "comment": "7 pages, Accepted at Insightful Ideas and Emerging Results (IIER)\n  Track of the Brazilian Symposium on Software Engineering (SBES 2025)", "summary": "Manual testing, in which testers follow natural language instructions to\nvalidate system behavior, remains crucial for uncovering issues not easily\ncaptured by automation. However, these test cases often suffer from test\nsmells, quality issues such as ambiguity, redundancy, or missing checks that\nreduce test reliability and maintainability. While detection tools exist, they\ntypically require manual rule definition and lack scalability. This study\ninvestigates the potential of Small Language Models (SLMs) for automatically\ndetecting test smells. We evaluate Gemma3, Llama3.2, and Phi-4 on 143\nreal-world Ubuntu test cases, covering seven types of test smells. Phi-4\nachieved the best results, reaching a pass@2 of 97% in detecting sentences with\ntest smells, while Gemma3 and Llama3.2 reached approximately 91%. Beyond\ndetection, SLMs autonomously explained issues and suggested improvements, even\nwithout explicit prompt instructions. They enabled low-cost, concept-driven\nidentification of diverse test smells without relying on extensive rule\ndefinitions or syntactic analysis. These findings highlight the potential of\nSLMs as efficient tools that preserve data privacy and can improve test quality\nin real-world scenarios."}
{"id": "2507.12872", "categories": ["cs.AI", "cs.CR", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.12872", "abs": "https://arxiv.org/abs/2507.12872", "authors": ["Rishane Dassanayake", "Mario Demetroudi", "James Walpole", "Lindley Lentati", "Jason R. Brown", "Edward James Young"], "title": "Manipulation Attacks by Misaligned AI: Risk Analysis and Safety Case Framework", "comment": "24 pages (14 pages main text, 4 pages bibliography, 6 pages\n  appendices), 3 figures", "summary": "Frontier AI systems are rapidly advancing in their capabilities to persuade,\ndeceive, and influence human behaviour, with current models already\ndemonstrating human-level persuasion and strategic deception in specific\ncontexts. Humans are often the weakest link in cybersecurity systems, and a\nmisaligned AI system deployed internally within a frontier company may seek to\nundermine human oversight by manipulating employees. Despite this growing\nthreat, manipulation attacks have received little attention, and no systematic\nframework exists for assessing and mitigating these risks. To address this, we\nprovide a detailed explanation of why manipulation attacks are a significant\nthreat and could lead to catastrophic outcomes. Additionally, we present a\nsafety case framework for manipulation risk, structured around three core lines\nof argument: inability, control, and trustworthiness. For each argument, we\nspecify evidence requirements, evaluation methodologies, and implementation\nconsiderations for direct application by AI companies. This paper provides the\nfirst systematic methodology for integrating manipulation risk into AI safety\ngovernance, offering AI companies a concrete foundation to assess and mitigate\nthese threats before deployment."}
{"id": "2507.13081", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.13081", "abs": "https://arxiv.org/abs/2507.13081", "authors": ["Dongming Jin", "Weisong Sun", "Jiangping Huang", "Peng Liang", "Jifeng Xuan", "Yang Liu", "Zhi Jin"], "title": "iReDev: A Knowledge-Driven Multi-Agent Framework for Intelligent Requirements Development", "comment": "22pages, 4 figures", "summary": "Requirements development is a critical phase as it is responsible for\nproviding a clear understanding of what stakeholders need. It involves\ncollaboration among stakeholders to extract explicit requirements and address\npotential conflicts, which is time-consuming and labor-intensive. Recently,\nmulti-agent systems for software development have attracted much attention.\nHowever, existing research provides limited support for requirements\ndevelopment and overlooks the injection of human knowledge into agents and the\nhuman-agent collaboration. % To address these issues, this paper proposes a\nknowledge-driven multi-agent framework for intelligent requirement development,\nnamed iReDev. iReDev features: iReDev consists of six knowledge-driven agents\nto support the entire requirements development. They collaboratively perform\nvarious tasks to produce a software requirements specification. iReDev focuses\non integrating human knowledge for agents, enabling them to simulate real-world\nstakeholders. iReDev uses an event-driven communication mechanism based on an\nartifact pool. Agents continuously monitor the pool and autonomously trigger\nthe next action based on its changes, enabling iReDev to handle new\nrequirements quickly. iReDev introduces a human-in-the-loop mechanism to\nsupport human-agent collaboration, ensuring that the generated artifacts align\nwith the expectations of stakeholders. We evaluated the generated artifacts and\nresults show that iReDev outperforms existing baselines in multiple aspects. We\nfurther envision three key directions and hope this work can facilitate the\ndevelopment of intelligent requirements development."}
{"id": "2507.12885", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12885", "abs": "https://arxiv.org/abs/2507.12885", "authors": ["Jian Yao", "Ran Cheng", "Kay Chen Tan"], "title": "VAR-MATH: Probing True Mathematical Reasoning in Large Language Models via Symbolic Multi-Instance Benchmarks", "comment": null, "summary": "Recent advances in reinforcement learning (RL) have led to substantial\nimprovements in the mathematical reasoning abilities of large language models\n(LLMs), as measured by standard benchmarks. However, these gains often persist\neven when models are trained with flawed signals, such as random or inverted\nrewards, raising a fundamental question: do such improvements reflect true\nreasoning, or are they merely artifacts of overfitting to benchmark-specific\npatterns? To address this question, we take an evaluation-centric perspective\nand identify two critical shortcomings in existing protocols. First,\n\\emph{benchmark contamination} arises from the public availability of test\nproblems, increasing the risk of data leakage. Second, \\emph{evaluation\nfragility} stems from the reliance on single-instance assessments, which are\nhighly sensitive to stochastic outputs and fail to capture reasoning\nconsistency. To overcome these limitations, we introduce {VAR-MATH}, a symbolic\nevaluation framework designed to probe genuine reasoning ability. By converting\nfixed numerical problems into symbolic templates and requiring models to solve\nmultiple instantiations of each, VAR-MATH enforces consistent reasoning across\nstructurally equivalent variants, thereby mitigating contamination and\nimproving evaluation robustness. We apply VAR-MATH to transform two popular\nbenchmarks, AMC23 and AIME24, into their symbolic counterparts, VAR-AMC23 and\nVAR-AIME24. Experimental results reveal substantial performance drops for\nRL-trained models on the variabilized versions, especially for smaller models,\nwith average declines of 48.0\\% on AMC23 and 58.3\\% on AIME24. These findings\nsuggest that many existing RL methods rely on superficial heuristics and fail\nto generalize beyond specific numerical forms. Overall, VAR-MATH offers a\nprincipled, contamination-resistant evaluation paradigm for mathematical\nreasoning."}
{"id": "2507.13095", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.13095", "abs": "https://arxiv.org/abs/2507.13095", "authors": ["Dongming Jin", "Zhi Jin", "Linyu Li", "Xiaohong Chen"], "title": "A Conceptual Framework for Requirements Engineering of Pretrained-Model-Enabled Systems", "comment": "5pages, 1 figure", "summary": "Recent advances in large pretrained models have led to their widespread\nintegration as core components in modern software systems. The trend is\nexpected to continue in the foreseeable future. Unlike traditional software\nsystems governed by deterministic logic, systems powered by pretrained models\nexhibit distinctive and emergent characteristics, such as ambiguous capability\nboundaries, context-dependent behavior, and continuous evolution. These\nproperties fundamentally challenge long-standing assumptions in requirements\nengineering, including functional decomposability and behavioral\npredictability. This paper investigates this problem and advocates for a\nrethinking of existing requirements engineering methodologies. We propose a\nconceptual framework tailored to requirements engineering of\npretrained-model-enabled software systems and outline several promising\nresearch directions within this framework. This vision helps provide a guide\nfor researchers and practitioners to tackle the emerging challenges in\nrequirements engineering of pretrained-model-enabled systems."}
{"id": "2507.12989", "categories": ["cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2507.12989", "abs": "https://arxiv.org/abs/2507.12989", "authors": ["Lyris Xu", "Fabio Aurelio D'Asaro", "Luke Dickens"], "title": "A Translation of Probabilistic Event Calculus into Markov Decision Processes", "comment": null, "summary": "Probabilistic Event Calculus (PEC) is a logical framework for reasoning about\nactions and their effects in uncertain environments, which enables the\nrepresentation of probabilistic narratives and computation of temporal\nprojections. The PEC formalism offers significant advantages in\ninterpretability and expressiveness for narrative reasoning. However, it lacks\nmechanisms for goal-directed reasoning. This paper bridges this gap by\ndeveloping a formal translation of PEC domains into Markov Decision Processes\n(MDPs), introducing the concept of \"action-taking situations\" to preserve PEC's\nflexible action semantics. The resulting PEC-MDP formalism enables the\nextensive collection of algorithms and theoretical tools developed for MDPs to\nbe applied to PEC's interpretable narrative domains. We demonstrate how the\ntranslation supports both temporal reasoning tasks and objective-driven\nplanning, with methods for mapping learned policies back into human-readable\nPEC representations, maintaining interpretability while extending PEC's\ncapabilities."}
{"id": "2507.13117", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.13117", "abs": "https://arxiv.org/abs/2507.13117", "authors": ["Andreas Pointner", "Josef Pichler", "Herbert Pr√§hofer"], "title": "Inferring Attributed Grammars from Parser Implementations", "comment": "Accepted to ICSME 2025", "summary": "Software systems that process structured inputs often lack complete and\nup-to-date specifications, which specify the input syntax and the semantics of\ninput processing. While grammar mining techniques have focused on recovering\nsyntactic structures, the semantics of input processing remains largely\nunexplored. In this work, we introduce a novel approach for inferring\nattributed grammars from parser implementations. Given an input grammar, our\ntechnique dynamically analyzes the implementation of recursive descent parsers\nto reconstruct the semantic aspects of input handling, resulting in\nspecifications in the form of attributed grammars. By observing program\nexecutions and mapping the program's runtime behavior to the grammar, we\nsystematically extract and embed semantic actions into the grammar rules. This\nenables comprehensive specification recovery. We demonstrate the feasibility of\nour approach using an initial set of programs, showing that it can accurately\nreproduce program behavior through the generated attributed grammars."}
{"id": "2507.13007", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13007", "abs": "https://arxiv.org/abs/2507.13007", "authors": ["Roger Xavier Lera-Leri", "Filippo Bistaffa", "Athina Georgara", "Juan Antonio Rodriguez-Aguilar"], "title": "Exploiting Constraint Reasoning to Build Graphical Explanations for Mixed-Integer Linear Programming", "comment": "To appear in Lecture Notes in Artificial Intelligence", "summary": "Following the recent push for trustworthy AI, there has been an increasing\ninterest in developing contrastive explanation techniques for optimisation,\nespecially concerning the solution of specific decision-making processes\nformalised as MILPs. Along these lines, we propose X-MILP, a domain-agnostic\napproach for building contrastive explanations for MILPs based on constraint\nreasoning techniques. First, we show how to encode the queries a user makes\nabout the solution of an MILP problem as additional constraints. Then, we\ndetermine the reasons that constitute the answer to the user's query by\ncomputing the Irreducible Infeasible Subsystem (IIS) of the newly obtained set\nof constraints. Finally, we represent our explanation as a \"graph of reasons\"\nconstructed from the IIS, which helps the user understand the structure among\nthe reasons that answer their query. We test our method on instances of\nwell-known optimisation problems to evaluate the empirical hardness of\ncomputing explanations."}
{"id": "2507.13123", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.13123", "abs": "https://arxiv.org/abs/2507.13123", "authors": ["Xin Yin", "Xinrui Li", "Chao Ni", "Xiaodan Xu", "Xiaohu Yang"], "title": "Detecting LLM-generated Code with Subtle Modification by Adversarial Training", "comment": null, "summary": "With the rapid development of Large Language Models (LLMs), their powerful\ncode-generation capabilities have been widely applied in tasks like code\ncompletion and automated development, demonstrating the value of improving\ncoding efficiency. However, the extensive use of LLM-generated code also raises\nseveral new challenges. On the one hand, issues such as the regulation of code\nprovenance, copyright disputes, and code quality have become increasingly\nconcerning. How to effectively detect LLM-generated code and ensure its\ncompliant and responsible use has become a critical and urgent issue. On the\nother hand, in practical applications, LLM-generated code is often subject to\nmanual modifications, such as variable renaming or structural adjustments.\nAlthough some recent studies have proposed training-based and zero-shot methods\nfor detecting LLM-generated code, these approaches show insufficient robustness\nwhen facing modified LLM-generated code, and there is a lack of an effective\nsolution. To address the real-world scenario where LLM-generated code may\nundergo minor modifications, we propose CodeGPTSensor+, an enhanced version of\nCodeGPTSensor, which employs adversarial training to improve robustness against\ninput perturbations. CodeGPTSensor+ integrates an adversarial sample generation\nmodule, Multi-objective Identifier and Structure Transformation (MIST), which\nsystematically generates both high-quality and representative adversarial\nsamples. This module effectively enhances the model's resistance against\ndiverse adversarial attacks. Experimental results on the HMCorp dataset\ndemonstrate that CodeGPTSensor+ significantly improves detection accuracy on\nthe adversarial test set while maintaining high accuracy on the original test\nset, showcasing superior robustness compared to CodeGPTSensor."}
{"id": "2507.13112", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13112", "abs": "https://arxiv.org/abs/2507.13112", "authors": ["Junseong Lee", "Jaegwan Cho", "Yoonju Cho", "Seoyoon Choi", "Yejin Shin"], "title": "Prediction of Highway Traffic Flow Based on Artificial Intelligence Algorithms Using California Traffic Data", "comment": null, "summary": "The study \"Prediction of Highway Traffic Flow Based on Artificial\nIntelligence Algorithms Using California Traffic Data\" presents a machine\nlearning-based traffic flow prediction model to address global traffic\ncongestion issues. The research utilized 30-second interval traffic data from\nCalifornia Highway 78 over a five-month period from July to November 2022,\nanalyzing a 7.24 km westbound section connecting \"Melrose Dr\" and \"El-Camino\nReal\" in the San Diego area. The study employed Multiple Linear Regression\n(MLR) and Random Forest (RF) algorithms, analyzing data collection intervals\nranging from 30 seconds to 15 minutes. Using R^2, MAE, and RMSE as performance\nmetrics, the analysis revealed that both MLR and RF models performed optimally\nwith 10-minute data collection intervals. These findings are expected to\ncontribute to future traffic congestion solutions and efficient traffic\nmanagement."}
{"id": "2507.13142", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.13142", "abs": "https://arxiv.org/abs/2507.13142", "authors": ["Ahmed Bahloul", "Simon Malberg"], "title": "From Roots to Rewards: Dynamic Tree Reasoning with RL", "comment": null, "summary": "Modern language models address complex questions through chain-of-thought\n(CoT) reasoning (Wei et al., 2023) and retrieval augmentation (Lewis et al.,\n2021), yet struggle with error propagation and knowledge integration.\nTree-structured reasoning methods, particularly the Probabilistic\nTree-of-Thought (ProbTree)(Cao et al., 2023) framework, mitigate these issues\nby decomposing questions into hierarchical structures and selecting answers\nthrough confidence-weighted aggregation of parametric and retrieved knowledge\n(Yao et al., 2023). However, ProbTree's static implementation introduces two\nkey limitations: (1) the reasoning tree is fixed during the initial\nconstruction phase, preventing dynamic adaptation to intermediate results, and\n(2) each node requires exhaustive evaluation of all possible solution\nstrategies, creating computational inefficiency. We present a dynamic\nreinforcement learning (Sutton and Barto, 2018) framework that transforms\ntree-based reasoning into an adaptive process. Our approach incrementally\nconstructs the reasoning tree based on real-time confidence estimates, while\nlearning optimal policies for action selection (decomposition, retrieval, or\naggregation). This maintains ProbTree's probabilistic rigor while improving\nboth solution quality and computational efficiency through selective expansion\nand focused resource allocation. The work establishes a new paradigm for\ntreestructured reasoning that balances the reliability of probabilistic\nframeworks with the flexibility required for real-world question answering\nsystems."}
{"id": "2507.13175", "categories": ["cs.AI", "68T27, 03B42 68T27, 03B4268T27, 03B42 68T27, 03B42 68T27, 03B42\n  68T27, 03B42 68T27, 03B42 68T27, 03B4268T27, 03B42", "I.2.0; I.2.9; K.4.1"], "pdf": "https://arxiv.org/pdf/2507.13175", "abs": "https://arxiv.org/abs/2507.13175", "authors": ["Matthew E. Brophy"], "title": "Black Box Deployed -- Functional Criteria for Artificial Moral Agents in the LLM Era", "comment": "42 pages. Supplementary material included at end of article", "summary": "The advancement of powerful yet opaque large language models (LLMs)\nnecessitates a fundamental revision of the philosophical criteria used to\nevaluate artificial moral agents (AMAs). Pre-LLM frameworks often relied on the\nassumption of transparent architectures, which LLMs defy due to their\nstochastic outputs and opaque internal states. This paper argues that\ntraditional ethical criteria are pragmatically obsolete for LLMs due to this\nmismatch. Engaging with core themes in the philosophy of technology, this paper\nproffers a revised set of ten functional criteria to evaluate LLM-based\nartificial moral agents: moral concordance, context sensitivity, normative\nintegrity, metaethical awareness, system resilience, trustworthiness,\ncorrigibility, partial transparency, functional autonomy, and moral\nimagination. These guideposts, applied to what we term \"SMA-LLS\" (Simulating\nMoral Agency through Large Language Systems), aim to steer AMAs toward greater\nalignment and beneficial societal integration in the coming years. We\nillustrate these criteria using hypothetical scenarios involving an autonomous\npublic bus (APB) to demonstrate their practical applicability in morally\nsalient contexts."}
{"id": "2507.13208", "categories": ["cs.AI", "cs.LO", "math.LO", "03B70 (Primary) 68T37, 68T27, 68Q42, 03B40, 68V15 (Secondary)", "F.4.1; I.2.3"], "pdf": "https://arxiv.org/pdf/2507.13208", "abs": "https://arxiv.org/abs/2507.13208", "authors": ["Besik Dundua", "Temur Kutsia"], "title": "Higher-Order Pattern Unification Modulo Similarity Relations", "comment": "23 pages", "summary": "The combination of higher-order theories and fuzzy logic can be useful in\ndecision-making tasks that involve reasoning across abstract functions and\npredicates, where exact matches are often rare or unnecessary. Developing\nefficient reasoning and computational techniques for such a combined formalism\npresents a significant challenge. In this paper, we adopt a more\nstraightforward approach aiming at integrating two well-established and\ncomputationally well-behaved components: higher-order patterns on one side and\nfuzzy equivalences expressed through similarity relations based on minimum\nT-norm on the other. We propose a unification algorithm for higher-order\npatterns modulo these similarity relations and prove its termination,\nsoundness, and completeness. This unification problem, like its crisp\ncounterpart, is unitary. The algorithm computes a most general unifier with the\nhighest degree of approximation when the given terms are unifiable."}
{"id": "2507.13302", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.13302", "abs": "https://arxiv.org/abs/2507.13302", "authors": ["Carlos Arriaga", "Gonzalo Mart√≠nez", "Eneko Sendin", "Javier Conde", "Pedro Reviriego"], "title": "The Generative Energy Arena (GEA): Incorporating Energy Awareness in Large Language Model (LLM) Human Evaluations", "comment": null, "summary": "The evaluation of large language models is a complex task, in which several\napproaches have been proposed. The most common is the use of automated\nbenchmarks in which LLMs have to answer multiple-choice questions of different\ntopics. However, this method has certain limitations, being the most\nconcerning, the poor correlation with the humans. An alternative approach, is\nto have humans evaluate the LLMs. This poses scalability issues as there is a\nlarge and growing number of models to evaluate making it impractical (and\ncostly) to run traditional studies based on recruiting a number of evaluators\nand having them rank the responses of the models. An alternative approach is\nthe use of public arenas, such as the popular LM arena, on which any user can\nfreely evaluate models on any question and rank the responses of two models.\nThe results are then elaborated into a model ranking. An increasingly important\naspect of LLMs is their energy consumption and, therefore, evaluating how\nenergy awareness influences the decisions of humans in selecting a model is of\ninterest. In this paper, we present GEA, the Generative Energy Arena, an arena\nthat incorporates information on the energy consumption of the model in the\nevaluation process. Preliminary results obtained with GEA are also presented,\nshowing that for most questions, when users are aware of the energy\nconsumption, they favor smaller and more energy efficient models. This suggests\nthat for most user interactions, the extra cost and energy incurred by the more\ncomplex and top-performing models do not provide an increase in the perceived\nquality of the responses that justifies their use."}
{"id": "2507.13337", "categories": ["cs.AI", "cs.CC", "math.LO"], "pdf": "https://arxiv.org/pdf/2507.13337", "abs": "https://arxiv.org/abs/2507.13337", "authors": ["Gal Beniamini", "Yuval Dor", "Alon Vinnikov", "Shir Granot Peled", "Or Weinstein", "Or Sharir", "Noam Wies", "Tomer Nussbaum", "Ido Ben Shaul", "Tomer Zekharya", "Yoav Levine", "Shai Shalev-Shwartz", "Amnon Shashua"], "title": "FormulaOne: Measuring the Depth of Algorithmic Reasoning Beyond Competitive Programming", "comment": null, "summary": "Frontier AI models demonstrate formidable breadth of knowledge. But how close\nare they to true human -- or superhuman -- expertise? Genuine experts can\ntackle the hardest problems and push the boundaries of scientific\nunderstanding. To illuminate the limits of frontier model capabilities, we turn\naway from contrived competitive programming puzzles, and instead focus on\nreal-life research problems.\n  We construct FormulaOne, a benchmark that lies at the intersection of graph\ntheory, logic, and algorithms, all well within the training distribution of\nfrontier models. Our problems are incredibly demanding, requiring an array of\nreasoning steps. The dataset has three key properties. First, it is of\ncommercial interest and relates to practical large-scale optimisation problems,\nsuch as those arising in routing, scheduling, and network design. Second, it is\ngenerated from the highly expressive framework of Monadic Second-Order (MSO)\nlogic on graphs, paving the way toward automatic problem generation at scale;\nideal for building RL environments. Third, many of our problems are intimately\nrelated to the frontier of theoretical computer science, and to central\nconjectures therein, such as the Strong Exponential Time Hypothesis (SETH). As\nsuch, any significant algorithmic progress on our dataset, beyond known\nresults, could carry profound theoretical implications.\n  Remarkably, state-of-the-art models like OpenAI's o3 fail entirely on\nFormulaOne, solving less than 1% of the questions, even when given 10 attempts\nand explanatory fewshot examples -- highlighting how far they remain from\nexpert-level understanding in some domains. To support further research, we\nadditionally curate FormulaOne-Warmup, offering a set of simpler tasks, from\nthe same distribution. We release the full corpus along with a comprehensive\nevaluation framework."}
{"id": "2507.12480", "categories": ["cs.SE", "cs.AI", "cs.ET"], "pdf": "https://arxiv.org/pdf/2507.12480", "abs": "https://arxiv.org/abs/2507.12480", "authors": ["Nazanin Siavash", "Armin Moin"], "title": "LLM-Powered Quantum Code Transpilation", "comment": "IEEE International Conference on Quantum Computing and Engineering\n  (QCE) 2025 - Extended Abstract", "summary": "There exist various Software Development Kits (SDKs) tailored to different\nquantum computing platforms. These are known as Quantum SDKs (QSDKs). Examples\ninclude but are not limited to Qiskit, Cirq, and PennyLane. However, this\ndiversity presents significant challenges for interoperability and\ncross-platform development of hybrid quantum-classical software systems.\nTraditional rule-based transpilers for translating code between QSDKs are\ntime-consuming to design and maintain, requiring deep expertise and rigid\nmappings in the source and destination code. In this study, we explore the use\nof Large Language Models (LLMs) as a flexible and automated solution.\nLeveraging their pretrained knowledge and contextual reasoning capabilities, we\nposition LLMs as programming language-agnostic transpilers capable of\nconverting quantum programs from one QSDK to another while preserving\nfunctional equivalence. Our approach eliminates the need for manually defined\ntransformation rules and offers a scalable solution to quantum software\nportability. This work represents a step toward enabling intelligent,\ngeneral-purpose transpilation in the quantum computing ecosystem."}
{"id": "2507.12482", "categories": ["cs.SE", "cs.AI", "cs.CE", "cs.LG", "68N30, 68T05, 68T50", "D.2.5; D.2.7; F.3.2; I.2.6; I.2.7"], "pdf": "https://arxiv.org/pdf/2507.12482", "abs": "https://arxiv.org/abs/2507.12482", "authors": ["Ishraq Khan", "Assad Chowdary", "Sharoz Haseeb", "Urvish Patel"], "title": "Kodezi Chronos: A Debugging-First Language Model for Repository-Scale, Memory-Driven Code Understanding", "comment": "10 pages, 10 figures, 7 tables, IEEE Conference format, Q4 2025 model\n  release, Q1 2026 Kodezi OS deployment", "summary": "Large Language Models (LLMs) have advanced code generation and software\nautomation, but are fundamentally constrained by limited inference-time context\nand lack of explicit code structure reasoning. We introduce Kodezi Chronos, a\nnext-generation architecture for autonomous code understanding, debugging, and\nmaintenance, designed to operate across ultra-long contexts comprising entire\ncodebases, histories, and documentation, all without fixed window limits.\nKodezi Chronos leverages a multi-level embedding memory engine, combining\nvector and graph-based indexing with continuous code-aware retrieval. This\nenables efficient and accurate reasoning over millions of lines of code,\nsupporting repository-scale comprehension, multi-file refactoring, and\nreal-time self-healing actions. Our evaluation introduces a novel Multi Random\nRetrieval benchmark, specifically tailored to the software engineering domain.\nUnlike classical retrieval benchmarks, this method requires the model to\nresolve arbitrarily distant and obfuscated associations across code artifacts,\nsimulating realistic tasks such as variable tracing, dependency migration, and\nsemantic bug localization. Chronos outperforms prior LLMs and code models,\ndemonstrating a 23% improvement in real-world bug detection and reducing\ndebugging cycles by up to 40% compared to traditional sequence-based\napproaches. By natively interfacing with IDEs and CI/CD workflows, Chronos\nenables seamless, autonomous software maintenance, elevating code reliability\nand productivity while reducing manual effort. These results mark a critical\nadvance toward self-sustaining, continuously optimized software ecosystems."}
{"id": "2507.12568", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.12568", "abs": "https://arxiv.org/abs/2507.12568", "authors": ["Sheng Liu", "Panos Papadimitratos"], "title": "Safeguarding Federated Learning-based Road Condition Classification", "comment": "Accepted by IEEE Conference on Communications and Network Security\n  (CNS) 2025", "summary": "Federated Learning (FL) has emerged as a promising solution for\nprivacy-preserving autonomous driving, specifically camera-based Road Condition\nClassification (RCC) systems, harnessing distributed sensing, computing, and\ncommunication resources on board vehicles without sharing sensitive image data.\nHowever, the collaborative nature of FL-RCC frameworks introduces new\nvulnerabilities: Targeted Label Flipping Attacks (TLFAs), in which malicious\nclients (vehicles) deliberately alter their training data labels to compromise\nthe learned model inference performance. Such attacks can, e.g., cause a\nvehicle to mis-classify slippery, dangerous road conditions as pristine and\nexceed recommended speed. However, TLFAs for FL-based RCC systems are largely\nmissing. We address this challenge with a threefold contribution: 1) we\ndisclose the vulnerability of existing FL-RCC systems to TLFAs; 2) we introduce\na novel label-distance-based metric to precisely quantify the safety risks\nposed by TLFAs; and 3) we propose FLARE, a defensive mechanism leveraging\nneuron-wise analysis of the output layer to mitigate TLFA effects. Extensive\nexperiments across three RCC tasks, four evaluation metrics, six baselines, and\nthree deep learning models demonstrate both the severity of TLFAs on FL-RCC\nsystems and the effectiveness of FLARE in mitigating the attack impact."}
{"id": "2507.12642", "categories": ["cs.SE", "cs.AI", "quant-ph"], "pdf": "https://arxiv.org/pdf/2507.12642", "abs": "https://arxiv.org/abs/2507.12642", "authors": ["Kiana Kheiri", "Aamna Aamir", "Andriy Miranskyy", "Chen Ding"], "title": "QSpark: Towards Reliable Qiskit Code Generation", "comment": null, "summary": "Quantum circuits must be error-resilient, yet LLMs like Granite-20B-Code and\nStarCoder often output flawed Qiskit code. We fine-tuned a 32 B model with two\nRL methods, Group Relative Policy Optimization (GRPO) and Odds-Ratio Preference\nOptimization (ORPO), using a richly annotated synthetic dataset. On the Qiskit\nHumanEval benchmark, ORPO reaches 56.29\\% Pass@1 ($\\approx+10$ pp over\nGranite-8B-QK) and GRPO hits 49\\%, both beating all general-purpose baselines;\non the original HumanEval they score 65.90\\% and 63.00\\%. GRPO excels on basic\ntasks (42/54), ORPO on intermediate ones (41/68), and neither solves the five\nadvanced tasks, highlighting clear gains yet room for progress in AI-assisted\nquantum programming."}
{"id": "2507.12665", "categories": ["cs.SE", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.12665", "abs": "https://arxiv.org/abs/2507.12665", "authors": ["Salvador D. Escobedo"], "title": "Single Conversation Methodology: A Human-Centered Protocol for AI-Assisted Software Development", "comment": "Style reviewed by a LLM for improving clarity and English syntax", "summary": "We propose the Single Conversation Methodology (SCM), a novel and pragmatic\napproach to software development using large language models (LLMs). In\ncontrast to ad hoc interactions with generative AI, SCM emphasizes a structured\nand persistent development dialogue, where all stages of a project - from\nrequirements to architecture and implementation - unfold within a single,\nlong-context conversation. The methodology is grounded on principles of\ncognitive clarity, traceability, modularity, and documentation. We define its\nphases, best practices, and philosophical stance, while arguing that SCM offers\na necessary correction to the passive reliance on LLMs prevalent in current\npractices. We aim to reassert the active role of the developer as architect and\nsupervisor of the intelligent tool."}
{"id": "2507.13169", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13169", "abs": "https://arxiv.org/abs/2507.13169", "authors": ["Jeremy McHugh", "Kristina ≈†ekrst", "Jon Cefalu"], "title": "Prompt Injection 2.0: Hybrid AI Threats", "comment": null, "summary": "Prompt injection attacks, where malicious input is designed to manipulate AI\nsystems into ignoring their original instructions and following unauthorized\ncommands instead, were first discovered by Preamble, Inc. in May 2022 and\nresponsibly disclosed to OpenAI. Over the last three years, these attacks have\ncontinued to pose a critical security threat to LLM-integrated systems. The\nemergence of agentic AI systems, where LLMs autonomously perform multistep\ntasks through tools and coordination with other agents, has fundamentally\ntransformed the threat landscape. Modern prompt injection attacks can now\ncombine with traditional cybersecurity exploits to create hybrid threats that\nsystematically evade traditional security controls. This paper presents a\ncomprehensive analysis of Prompt Injection 2.0, examining how prompt injections\nintegrate with Cross-Site Scripting (XSS), Cross-Site Request Forgery (CSRF),\nand other web security vulnerabilities to bypass traditional security measures.\nWe build upon Preamble's foundational research and mitigation technologies,\nevaluating them against contemporary threats, including AI worms, multi-agent\ninfections, and hybrid cyber-AI attacks. Our analysis incorporates recent\nbenchmarks that demonstrate how traditional web application firewalls, XSS\nfilters, and CSRF tokens fail against AI-enhanced attacks. We also present\narchitectural solutions that combine prompt isolation, runtime security, and\nprivilege separation with novel threat detection capabilities."}
