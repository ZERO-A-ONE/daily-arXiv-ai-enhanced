<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 21]
- [cs.AI](#cs.AI) [Total: 101]
- [cs.CR](#cs.CR) [Total: 29]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [IntentCoding: Amplifying User Intent in Code Generation](https://arxiv.org/abs/2602.00066)
*Zheng Fang,Yihong Dong,Lili Mou,Dongming Jin,Zhi Jin,Ge Li*

Main category: cs.SE

TL;DR: IntentCoding是一种新的解码策略，通过掩蔽用户意图和多强度集成机制来增强LLM在代码生成中遵循用户约束的能力，无需额外训练且与现有解码方法兼容。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在代码生成方面表现出色，但在遵循包含多个约束的细粒度用户意图方面存在显著挑战。实证分析发现：1）随着用户意图中约束数量的增加，模型性能迅速下降；2）用户意图虽然影响模型的logits，但这种影响不足以有效引导解码过程。

Method: 提出Intent-Amplified Code Generation (IntentCoding)解码策略：通过掩蔽用户意图来捕获其影响，并应用多强度集成机制在生成过程中放大用户意图的效果。该方法与模型无关，无需额外训练，可与现有解码过程无缝集成。

Result: 在构建的CodeConstraints基准数据集以及IFEvalCode、HumanEval和LiveCodeBench数据集上的实验表明，IntentCoding相比标准解码方法显著提高了约束满足度和功能正确性。在CodeConstraints上相对改进达71.0%，在IFEvalCode上达67.3%，在HumanEval和LiveCodeBench的pass@1上达29.3%。

Conclusion: IntentCoding是一种有效的解码策略，能够显著提升LLM在代码生成中遵循用户意图的能力，特别是在处理多个约束时表现优异，且具有模型无关性和无需训练的优势。

Abstract: Large Language Models (LLMs) have shown strong capabilities in code generation, but their adherence to fine-grained user intent with multiple constraints remains a significant challenge. Our empirical analysis reveals two key observations: 1) Model performance deteriorates quickly as the number of constraints in the user intent increases, and 2) While user intent does influence the model's logits, such an influence may not be strong enough to effectively steer the decoding process. To this end, we propose Intent-Amplified Code Generation (IntentCoding), a novel decoding strategy that enhances an LLM's ability to follow user intent. IntentCoding captures the influence of user intent by masking out the intent, and applies a multi-strength ensemble mechanism to amplify the effect of user intent during generation. IntentCoding is model-agnostic, requires no additional training, and integrates seamlessly with existing decoding procedures. To enable systematic evaluation, we also construct CodeConstraints, a benchmark dataset specifically designed to test user intent compliance under varying numbers of constraints. Experiments on our constructed Constraints, as well as popular IFEvalCode, HumanEval and LiveCodeBench datasets, show that our IntentCoding model significantly improves both constraint satisfaction and functional correctness compared to standard decoding approaches. IntentCoding achieves up to 71.0% relative improvement on CodeConstraints, achieves up to 67.3% relative improvement on IFEvalCode and achieves up to 29.3% relative improvement in pass@1 on HumanEval and LiveCodeBench compared with greedy decoding.

</details>


### [2] [Spec-Driven Development:From Code to Contract in the Age of AI Coding Assistants](https://arxiv.org/abs/2602.00180)
*Deepak Babu Piskala*

Main category: cs.SE

TL;DR: 本文为从业者提供了规范驱动开发(SDD)的全面指南，将规范视为主要工件而非代码，介绍了三种规范严格级别，分析了相关工具和案例研究，并提供了决策框架。


<details>
  <summary>Details</summary>
Motivation: AI编程助手的兴起重新激发了人们对一个旧理念的兴趣：如果规范而非代码成为软件开发的主要工件会怎样？传统工作流程将代码视为主要产物，而SDD将规范作为真理来源，代码则作为生成或验证的次要产物。

Method: 提出了三种规范严格级别：规范优先(spec-first)、规范锚定(spec-anchored)和规范即源(spec-as-source)；分析了从行为驱动开发框架到现代AI辅助工具如GitHub Spec Kit等工具；通过API开发、企业系统和嵌入式软件等领域的案例研究展示SDD的实际应用。

Result: 展示了规范优先哲学如何映射到实际实现中，不同领域如何应用SDD方法，并提供了清晰的指导说明每种规范严格级别的适用场景。

Conclusion: 提出了一个决策框架，帮助从业者确定SDD何时能提供价值，以及何时更简单的方法就足够了，为实践者提供了实用的指导。

Abstract: The rise of AI coding assistants has reignited interest in an old idea: what if specifications-not code-were the primary artifact of software development? Spec-driven development (SDD) inverts the traditional workflow by treating specifications as the source of truth and code as a generated or verified secondary artifact. This paper provides practitioners with a comprehensive guide to SDD, covering its principles, workflow patterns, and supporting tools. We present three levels of specification rigor-spec-first, spec-anchored, and spec-as-source-with clear guidance on when each applies. Through analysis of tools ranging from Behavior-Driven Development frameworks to modern AI-assisted toolkits like GitHub Spec Kit, we demonstrate how the spec-first philosophy maps to real implementations. We present case studies from API development, enterprise systems, and embedded software, illustrating how different domains apply SDD. We conclude with a decision framework helping practitioners determine when SDD provides value and when simpler approaches suffice.

</details>


### [3] [Towards Analyzing N-language Polyglot Programs](https://arxiv.org/abs/2602.00303)
*Jyoti Prakash,Abhishek Tiwari,Mikkel Baun Kjærgaard*

Main category: cs.SE

TL;DR: 本文探讨了三语言及以上多语言系统的静态分析挑战，提出了解决这些挑战的概念路线图，旨在推动可扩展、语言无关的分析框架研究。


<details>
  <summary>Details</summary>
Motivation: 随着GraalVM等多语言运行时的流行，多语言编程日益普及，但现有研究主要关注两种语言的分析，忽略了使用三种及以上语言的系统复杂性。现代Web系统经常在同一个执行链中集成JavaScript、WebAssembly和Rust等多种语言。

Method: 本文首先识别三语言多语言通信系统的基本分析挑战，然后提出一个概念路线图，旨在推进静态分析技术以应对这些挑战。

Result: 提出了针对三语言多语言系统的分析挑战识别和解决方案路线图，为未来研究提供了方向性指导。

Conclusion: 本文旨在激发讨论并启发新的研究方向，推动面向下一代多语言系统的可扩展、语言无关分析框架的发展。

Abstract: Polyglot programming is gaining popularity as developers integrate multiple programming languages to harness their individual strengths. With the recent popularity of platforms like GraalVM and other multi-language runtimes, creating and managing these systems has become much more feasible. However, current research on analyzing multilingual programs mainly focuses on two languages, leaving out the increasing complexity of systems that use three or more. For example, modern web systems often link JavaScript, WebAssembly, and Rust within the same execution chain. This paper envisions the landscape of software systems with three-language polyglot communication. We identify fundamental challenges in analyzing them and propose a conceptual roadmap to advance static analysis techniques to address them. Our vision aims to stimulate discussion and inspire new research directions toward scalable, language-agnostic analysis frameworks for next-generation polyglot systems.

</details>


### [4] [GitEvo: Code Evolution Analysis for Git Repositories](https://arxiv.org/abs/2602.00410)
*Andre Hora*

Main category: cs.SE

TL;DR: GitEvo是一个用于分析Git仓库中代码演化的多语言可扩展工具，集成了Git层面和代码层面的分析，支持实证研究和教学应用。


<details>
  <summary>Details</summary>
Motivation: 分析软件系统的代码演化对从业者、研究者和教育者都很重要，但目前缺乏专门支持代码演化分析的工具。

Method: GitEvo利用Git框架和代码解析工具，集成Git层面和代码层面的分析，支持多语言和可扩展性。

Result: 开发了GitEvo工具，可用于支持代码演化的新颖实证研究，并作为教育者和学生的学习工具。

Conclusion: GitEvo填补了代码演化分析工具的空白，为实践、研究和教育提供了有价值的支持。

Abstract: Analyzing the code evolution of software systems is relevant for practitioners, researchers, and educators. It can help practitioners identify design trends and maintenance challenges, provide researchers with empirical data to study changes over time, and give educators real-world examples that enhance the teaching of software evolution concepts. Unfortunately, we lack tools specifically designed to support code evolution analysis. In this paper, we propose GitEvo, a multi-language and extensible tool for analyzing code evolution in Git repositories. GitEvo leverages Git frameworks and code parsing tools to integrate both Git-level and code-level analysis. We conclude by describing how GitEvo can support the development of novel empirical studies on code evolution and act as a learning tool for educators and students. GitEvo is available at: https://github.com/andrehora/gitevo.

</details>


### [5] [Beyond Basic Specifications? A Systematic Study of Logical Constructs in LLM-based Specification Generation](https://arxiv.org/abs/2602.00715)
*Zehan Chen,Long Zhang,Zhiwei Zhang,JingJing Zhang,Ruoyu Zhou,Yulong Shen,JianFeng Ma,Lin Yang*

Main category: cs.SE

TL;DR: 该研究探讨了在基于大语言模型的程序规范生成框架中引入逻辑构造的可行性，通过实证研究发现逻辑构造与基本语法构造的协同使用能提升验证能力和鲁棒性，且不显著增加验证开销。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的程序规范生成研究主要局限于基本语法构造，无法满足复杂程序验证对高级抽象的需求。需要系统研究大语言模型是否能有效生成复杂的逻辑构造，以提升程序验证框架的抽象能力。

Method: 提出在现有LLM规范生成框架中引入逻辑构造，定义了四种不同抽象层次的语法配置，在主流程序验证数据集上进行广泛评估，使用多种代表性大语言模型进行实验。

Result: 实验首先证实大语言模型能够生成有效的逻辑构造。进一步分析表明，逻辑构造与基本语法构造的协同使用能同时提升验证能力和鲁棒性，且不显著增加验证开销。研究还揭示了两种细化范式的独特优势。

Conclusion: 这是首个系统探索利用大语言模型生成高级逻辑构造可行性的工作，为未来构建具有增强抽象能力的自动化程序验证框架提供了实证基础和指导。

Abstract: Formal specifications play a pivotal role in accurately characterizing program behaviors and ensuring software correctness. In recent years, leveraging large language models (LLMs) for the automatic generation of program specifications has emerged as a promising avenue for enhancing verification efficiency. However, existing research has been predominantly confined to generating specifications based on basic syntactic constructs, falling short of meeting the demands for high-level abstraction in complex program verification. Consequently, we propose incorporating logical constructs into existing LLM-based specification generation framework. Nevertheless, there remains a lack of systematic investigation into whether LLMs can effectively generate such complex constructs. To this end, we conduct an empirical study aimed at exploring the impact of various types of syntactic constructs on specification generation framework. Specifically, we define four syntactic configurations with varying levels of abstraction and perform extensive evaluations on mainstream program verification datasets, employing a diverse set of representative LLMs. Experimental results first confirm that LLMs are capable of generating valid logical constructs. Further analysis reveals that the synergistic use of logical constructs and basic syntactic constructs leads to improvements in both verification capability and robustness, without significantly increasing verification overhead. Additionally, we uncover the distinct advantages of two refinement paradigms. To the best of our knowledge, this is the first systematic work exploring the feasibility of utilizing LLMs for generating high-level logical constructs, providing an empirical basis and guidance for the future construction of automated program verification framework with enhanced abstraction capabilities.

</details>


### [6] [Can Vision-Language Models Handle Long-Context Code? An Empirical Study on Visual Compression](https://arxiv.org/abs/2602.00746)
*Jianping Zhong,Guochang Li,Chen Zhi,Junxiao Han,Zhen Qin,Xinkui Zhao,Nan Wang,Shuiguang Deng,Jianwei Yin*

Main category: cs.SE

TL;DR: LongCodeOCR是一个视觉压缩框架，将代码转换为压缩的二维图像序列供视觉语言模型处理，解决了大语言模型处理长代码时的窗口限制问题，同时避免了传统文本压缩方法导致的依赖关系断裂。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在处理长上下文代码时面临窗口限制问题。现有的文本代码压缩方法通过选择性过滤来缓解这个问题，但往往会破坏依赖关系的完整性，导致语义碎片化。需要一种既能压缩代码长度又能保持全局依赖关系的方法。

Method: LongCodeOCR是一个视觉压缩框架，将代码渲染成压缩的二维图像序列，供视觉语言模型处理。这种方法通过保留全局视图，避免了过滤方法固有的依赖关系断裂问题。

Result: 在四个基准测试中，LongCodeOCR在代码总结、代码问答和代码补全任务上优于最先进的LongCodeZip方法。在相似压缩比下，LongCodeOCR在长模块总结任务上的CompScore提高了36.85分。在100万token的上下文长度下，LongCodeOCR保持了比LongCodeZip更高的准确率，同时实现了约4倍的更高压缩率。此外，LongCodeOCR显著减少了压缩阶段的开销，将延迟从约4.3小时减少到约1分钟。

Conclusion: 视觉代码压缩是任务需要全局理解时的可行替代方案。研究揭示了覆盖度-保真度的基本权衡：视觉代码压缩保留了更广泛的上下文覆盖以支持全局依赖，但在精确性关键任务上面临保真度瓶颈；相比之下，文本代码压缩保留了符号级精度但牺牲了结构覆盖度。

Abstract: Large Language Models (LLMs) struggle with long-context code due to window limitations. Existing textual code compression methods mitigate this via selective filtering but often disrupt dependency closure, causing semantic fragmentation. To address this, we introduce LongCodeOCR, a visual compression framework that renders code into compressed two-dimensional image sequences for Vision-Language Models (VLMs). By preserving a global view, this approach avoids the dependency breakage inherent in filtering. We systematically evaluate LongCodeOCR against the state-of-the-art LongCodeZip across four benchmarks spanning code summarization, code question answering, and code completion.
  Our results demonstrate that visual code compression serves as a viable alternative for tasks requiring global understanding. At comparable compression ratios ($\sim$1.7$\times$), LongCodeOCR improves CompScore on Long Module Summarization by 36.85 points over LongCodeZip. At a 1M-token context length with Glyph (a specialized 9B VLM), LongCodeOCR maintains higher accuracy than LongCodeZip while operating at about 4$\times$ higher compression. Moreover, compared with LongCodeZip, LongCodeOCR drastically reduces compression-stage overhead (reducing latency from $\sim$4.3 hours to $\sim$1 minute at 1M tokens). Finally, our results characterize a fundamental coverage--fidelity trade-off: visual code compression retains broader context coverage to support global dependencies, yet faces fidelity bottlenecks on exactness-critical tasks; by contrast, textual code compression preserves symbol-level precision while sacrificing structural coverage.

</details>


### [7] [ScratchEval : A Multimodal Evaluation Framework for LLMs in Block-Based Programming](https://arxiv.org/abs/2602.00757)
*Yuan Si,Simeng Han,Daming Li,Hanyuan Shi,Jialu Zhang*

Main category: cs.SE

TL;DR: ScratchEval是首个用于评估LLM修复Scratch程序的可执行基准，包含100个复杂Scratch项目，支持通过VM执行、编辑距离和行为轨迹进行多维度评估。


<details>
  <summary>Details</summary>
Motivation: LLM在文本编程任务上表现良好，但在Scratch等积木式语言中不可靠，因为Scratch程序具有深度嵌套、非线性结构、事件驱动并发和多模态耦合等特性，导致LLM常生成语法正确但语义错误的修复。

Method: 构建ScratchEval基准：包含100个从公共仓库精选的复杂Scratch项目，每个项目配有可执行测试套件、bug描述与修复、块级编辑约束和多媒体资源。采用人机协同流水线结合自动项目挖掘和专家验证。

Result: 提出三层可执行评估协议：1)通过VM级执行测量功能正确性；2)使用块级编辑距离和行为轨迹比较修复质量；3)通过结构化评分标准评估解释质量。基准支持领域特定微调、训练数据有效性和模型泛化研究。

Conclusion: ScratchEval为评估和训练LLM在积木式编程任务上提供了可复现的基础，填补了该领域评估基准的空白，有助于提升LLM对Scratch程序的理解和修复能力。

Abstract: LLMs have achieved strong performance on text-based programming tasks, yet they remain unreliable for block-based languages such as Scratch. Scratch programs exhibit deeply nested, non-linear structures, event-driven concurrency across multiple sprites, and tight coupling between code and multimedia assets, properties that differ fundamentally from textual code. As a result, LLMs often misinterpret Scratch semantics and generate large, invasive edits that are syntactically valid but semantically incorrect when repairing buggy programs.
  We introduce ScratchEval, the first executable benchmark designed to evaluate LLM-based repair for Scratch programs, covering program understanding, debugging, analysis, and repair. The benchmark contains 100 curated Scratch projects from the public repository, selected for structural and semantic complexity. Each project is paired with executable test suites, bug descriptions with corresponding fixes, block-level edit constraints defining minimal semantically correct repairs, and required multimedia assets. The benchmark is constructed through a human-in-the-loop pipeline combining automated project mining with expert validation of trigger-outcome semantics and representative bug patterns, with emphasis on event ordering, concurrency, and state management.
  To enable rigorous and reproducible evaluation, we propose a three-layer executable protocol measuring functional correctness via VM-level execution, repair quality using block-level edit distance and behavioral trajectory comparisons, and explanation quality via structured rubrics assessing alignment between model reasoning and generated patches. Using ScratchEval, we study domain-specific fine-tuning, training data effectiveness, and model generalization to unseen bug types. ScratchEval provides a reproducible foundation for evaluating and post-training LLMs on block-based programming tasks.

</details>


### [8] [Test Behaviors, Not Methods! Detecting Tests Obsessed by Methods](https://arxiv.org/abs/2602.00761)
*Andre Hora,Andy Zaidman*

Main category: cs.SE

TL;DR: 提出一种新的测试异味"Test Obsessed by Method"，用于识别那些覆盖单个生产方法多个路径的测试，并通过实证研究在Python标准库中验证其存在。


<details>
  <summary>Details</summary>
Motivation: 现有测试异味"Eager Test"通过统计生产方法调用来识别测试过多功能的方法不够准确，需要更精确的指标来识别验证多个行为的测试。

Method: 提出基于运行时分析的新测试异味"Test Obsessed by Method"，定义为覆盖单个生产方法多个路径的测试方法。对Python标准库12个测试套件的2,054个测试进行实证研究。

Result: 在11个测试套件中检测到44个"Test Obsessed by Method"异味测试；每个异味测试验证生产方法的中位数为2个行为；44个异味测试可拆分为118个新测试；23%的异味测试有代码注释表明测试了不同行为。

Conclusion: 提出的新测试异味能有效识别验证多个行为的测试，有助于提高测试的可理解性和可维护性，但需要进一步研究其实际效益和局限性。

Abstract: Best testing practices state that tests should verify a single functionality or behavior of the system. Tests that verify multiple behaviors are harder to understand, lack focus, and are more coupled to the production code. An attempt to identify this issue is the test smell \emph{Eager Test}, which aims to capture tests that verify too much functionality based on the number of production method calls. Unfortunately, prior research suggests that counting production method calls is an inaccurate measure, as these calls do not reliably serve as a proxy for functionality. We envision a complementary solution based on runtime analysis: we hypothesize that some tests that verify multiple behaviors will likely cover multiple paths of the same production methods. Thus, we propose a novel test smell named \emph{Test Obsessed by Method}, a test method that covers multiple paths of a single production method. We provide an initial empirical study to explore the presence of this smell in 2,054 tests provided by 12 test suites of the Python Standard Library. (1) We detect 44 \emph{Tests Obsessed by Methods} in 11 of the 12 test suites. (2) Each smelly test verifies a median of two behaviors of the production method. (3) The 44 smelly tests could be split into 118 novel tests. (4) 23% of the smelly tests have code comments recognizing that distinct behaviors are being tested. We conclude by discussing benefits, limitations, and further research.

</details>


### [9] [Code Quality Analysis of Translations from C to Rust](https://arxiv.org/abs/2602.00840)
*Biruk Tadesse,Vikram Nitin,Mazin Salah,Baishakhi Ray,Marcelo d'Amorim,Wesley Assunção*

Main category: cs.SE

TL;DR: 该研究比较了三种C到Rust翻译工具(C2Rust、C2SaferRust、TranslationGym)在翻译GNU coreutils时的质量表现，发现自动化翻译在安全性、性能、健壮性和可维护性等多维质量指标上仍面临挑战，无法在所有维度上超越人工翻译。


<details>
  <summary>Details</summary>
Motivation: C/C++存在严重的内存和线程安全问题，虽然已有研究探索将其自动翻译到更安全的Rust语言，但现有研究主要关注翻译的正确性和安全性，而忽略了性能、健壮性和可维护性等其他重要质量维度。

Method: 使用三种C-to-Rust翻译工具(C2Rust、C2SaferRust、TranslationGym)翻译GNU coreutils，以人工翻译为基准。通过三种方法评估Rust代码质量：1) 使用Clippy静态分析工具；2) 利用GPT-4o识别Clippy可能遗漏的问题；3) 对Clippy和GPT-4o报告的问题进行手动分析。

Result: 结果显示，虽然新技术减少了一些不安全和非惯用模式，但经常引入新问题，揭示了现有评估实践中不可见的系统性权衡。没有任何自动化技术在所有质量维度上始终匹配或超越人工翻译，但即使是人工编写的Rust代码也存在可读性和非惯用模式等持续的内部质量问题。

Conclusion: 翻译质量仍然是一个多维挑战，需要超越简单自动化和手动重写的系统性评估和针对性工具支持。研究强调了在评估C到Rust翻译时考虑多维度质量属性的重要性。

Abstract: C/C++ is a prevalent programming language. Yet, it suffers from significant memory and thread-safety issues. Recent studies have explored automated translation of C/C++ to safer languages, such as Rust. However, these studies focused mostly on the correctness and safety of the translated code, which are indeed critical, but they left other important quality concerns (e.g., performance, robustness, and maintainability) largely unexplored. This work investigates strengths and weaknesses of three C-to-Rust translators, namely C2Rust (a transpiler), C2SaferRust (an LLM-guided transpiler), and TranslationGym (an LLM-based direct translation). We perform an in-depth quantitative and qualitative analysis of several important quality attributes for the translated Rust code of the popular GNU coreutils, using human-based translation as a baseline. To assess the internal and external quality of the Rust code, we: (i) apply Clippy, a rule-based state-of-the-practice Rust static analysis tool; (ii) investigate the capability of an LLM (GPT-4o) to identify issues potentially overlooked by Clippy; and (iii) perform a manual analysis of the issues reported by Clippy and GPT-4o. Our results show that while newer techniques reduce some unsafe and non-idiomatic patterns, they frequently introduce new issues, revealing systematic trade-offs that are not visible under existing evaluation practices. Notably, none of the automated techniques consistently match or exceed human-written translations across all quality dimensions, yet even human-written Rust code exhibits persistent internal quality issues such as readability and non-idiomatic patterns. Together, these findings show that translation quality remains a multi-dimensional challenge, requiring systematic evaluation and targeted tool support beyond both naive automation and manual rewriting.

</details>


### [10] [MCP-Atlas: A Large-Scale Benchmark for Tool-Use Competency with Real MCP Servers](https://arxiv.org/abs/2602.00933)
*Chaithanya Bandi,Ben Hertzberg,Geobio Boo,Tejas Polakam,Jeff Da,Sami Hassaan,Manasi Sharma,Andrew Park,Ernesto Hernandez,Dan Rambado,Ivan Salazar,Rafael Cruz,Chetan Rane,Ben Levin,Brad Kenstler,Bing Liu*

Main category: cs.SE

TL;DR: MCP-Atlas是一个用于评估LLM工具使用能力的大规模基准测试，包含36个真实MCP服务器、220个工具和1000个任务，专注于多步骤工作流的真实场景评估。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法无法捕捉真实场景的复杂性，通常依赖受限工具集、简单工作流或主观的LLM-as-a-judge指标，需要更全面的工具使用能力评估基准。

Method: 创建包含36个真实MCP服务器和220个工具的大规模基准，设计1000个自然语言任务，要求智能体跨多个服务器识别和协调3-6个工具调用，使用基于声明的评分标准，辅以工具发现、参数化、语法、错误恢复和效率等内部诊断。

Result: 前沿模型的通过率超过50%，主要失败原因来自工具使用不足和任务理解不足。发布了任务模式、容器化框架和500个任务的公共子集。

Conclusion: MCP-Atlas为工具增强型智能体的开发提供了可复现的比较基准，有助于推动更强大、更稳健的工具使用能力发展。

Abstract: The Model Context Protocol (MCP) is rapidly becoming the standard interface for Large Language Models (LLMs) to discover and invoke external tools. However, existing evaluations often fail to capture the complexity of real-world scenarios, relying on restricted toolsets, simplistic workflows, or subjective LLM-as-a-judge metrics. We introduce MCP-Atlas, a large-scale benchmark for evaluating tool-use competency, comprising 36 real MCP servers and 220 tools. It includes 1,000 tasks designed to assess tool-use competency in realistic, multi-step workflows. Tasks use natural language prompts that avoid naming specific tools or servers, requiring agents to identify and orchestrate 3-6 tool calls across multiple servers. We score tasks using a claims-based rubric that awards partial credit based on the factual claims satisfied in the model's final answer, complemented by internal diagnostics on tool discovery, parameterization, syntax, error recovery, and efficiency. Evaluation results on frontier models reveal that top models achieve pass rates exceeding 50%, with primary failures arising from inadequate tool usage and task understanding. We release the task schema, containerized harness, and a 500-task public subset of the benchmark dataset to facilitate reproducible comparisons and advance the development of robust, tool-augmented agents.

</details>


### [11] [Cast: Automated Resilience Testing for Production Cloud Service Systems](https://arxiv.org/abs/2602.00972)
*Zhuangbin Chen,Zhiling Deng,Kaiming Zhang,Yang Liu,Cheng Cui,Jinfeng Zhong,Zibin Zheng*

Main category: cs.SE

TL;DR: Cast是一个用于微服务韧性测试的自动化端到端框架，通过在生产环境中重放流量并注入应用级故障来发现系统脆弱性，已在华为云部署8个月以上。


<details>
  <summary>Details</summary>
Motivation: 微服务架构的分布式特性带来了显著的韧性挑战。传统测试方法受限于大量手动工作和过度简化的测试环境，无法捕捉生产系统的复杂性。

Method: Cast采用三阶段管道（启动、故障注入和恢复），通过重放生产流量并注入应用级故障来测试内部错误处理逻辑。使用复杂度驱动策略修剪冗余测试并优先处理关键服务执行路径的高价值测试。采用多方面oracle自动验证系统韧性。

Result: 在华为云部署8个月以上，被多个服务团队采用。对4个大规模应用的分析发现了137个潜在脆弱性，其中89个被开发者确认。在48个复现bug的基准测试中达到90%的覆盖率。

Conclusion: Cast是一个实用有效的解决方案，能够系统性地提高工业微服务系统的可靠性。

Abstract: The distributed nature of microservice architecture introduces significant resilience challenges. Traditional testing methods, limited by extensive manual effort and oversimplified test environments, fail to capture production system complexity. To address these limitations, we present Cast, an automated, end-to-end framework for microservice resilience testing in production. It achieves high test fidelity by replaying production traffic against a comprehensive library of application-level faults to exercise internal error-handling logic. To manage the combinatorial test space, Cast employs a complexity-driven strategy to systematically prune redundant tests and prioritize high-value tests targeting the most critical service execution paths. Cast automates the testing lifecycle through a three-phase pipeline (i.e., startup, fault injection, and recovery) and uses a multi-faceted oracle to automatically verify system resilience against nuanced criteria. Deployed in Huawei Cloud for over eight months, Cast has been adopted by many service teams to proactively address resilience vulnerabilities. Our analysis on four large-scale applications with millions of traces reveals 137 potential vulnerabilities, with 89 confirmed by developers. To further quantify its performance, Cast is evaluated on a benchmark set of 48 reproduced bugs, achieving a high coverage of 90%. The results show that Cast is a practical and effective solution for systematically improving the reliability of industrial microservice systems.

</details>


### [12] [Morphis: SLO-Aware Resource Scheduling for Microservices with Time-Varying Call Graphs](https://arxiv.org/abs/2602.01044)
*Yu Tang,Hailiang Zhao,Chuansheng Lu,Yifei Zhang,Kingsum Chow,Shuiguang Deng,Rui Shi*

Main category: cs.SE

TL;DR: Morphis是一个依赖感知的资源供应框架，通过识别微服务调用图中的重复执行模式，优化资源分配，在保证SLO的同时显著降低CPU消耗。


<details>
  <summary>Details</summary>
Motivation: 现代微服务系统在运行时调用图存在持续结构演化，但分析显示执行路径集中在少量重复调用模式上。现有资源管理方法要么忽略服务间依赖关系，要么假设静态拓扑，无法有效利用这种结构规律。

Method: 提出Morphis框架：1）引入结构指纹技术，将跟踪数据分解为稳定的执行主干和可解释的偏差子图；2）将资源分配建模为约束优化问题，基于预测的模式分布，联合最小化总CPU使用同时满足端到端尾延迟SLO。

Result: 在TrainTicket基准测试上的广泛评估显示，Morphis相比最先进的基线方法减少35-38%的CPU消耗，同时保持98.8%的SLO合规率。

Conclusion: Morphis通过利用微服务调用图中的结构规律，实现了依赖感知的资源供应，在动态执行环境中显著提升了资源效率。

Abstract: Modern microservice systems exhibit continuous structural evolution in their runtime call graphs due to workload fluctuations, fault responses, and deployment activities. Despite this complexity, our analysis of over 500,000 production traces from ByteDance reveals a latent regularity: execution paths concentrate around a small set of recurring invocation patterns. However, existing resource management approaches fail to exploit this structure. Industrial autoscalers like Kubernetes HPA ignore inter-service dependencies, while recent academic methods often assume static topologies, rendering them ineffective under dynamic execution contexts. In this work, we propose Morphis, a dependency-aware provisioning framework that unifies pattern-aware trace analysis with global optimization. It introduces structural fingerprinting that decomposes traces into a stable execution backbone and interpretable deviation subgraphs. Then, resource allocation is formulated as a constrained optimization problem over predicted pattern distributions, jointly minimizing aggregate CPU usage while satisfying end-to-end tail-latency SLOs. Our extensive evaluations on the TrainTicket benchmark demonstrate that Morphis reduces CPU consumption by 35-38% compared to state-of-the-art baselines while maintaining 98.8% SLO compliance.

</details>


### [13] [SPELL: Synthesis of Programmatic Edits using LLMs](https://arxiv.org/abs/2602.01107)
*Daniel Ramos,Catarina Gamboa,Inês Lynce,Vasco Manquinho,Ruben Martins,Claire Le Goues*

Main category: cs.SE

TL;DR: 提出了一种基于LLM和PolyglotPiranha的自动化API迁移新方法，无需依赖现有迁移数据或直接使用LLM进行代码转换


<details>
  <summary>Details</summary>
Motivation: 库迁移是软件开发中常见但易出错的任务，现有自动化迁移工具大多依赖从已完成迁移的真实项目中挖掘示例，但这些数据稀缺且难以收集，同时这些工具未能充分利用现代代码转换基础设施

Method: 使用LLM提取迁移示例，然后通过Agent将这些示例泛化为PolyglotPiranha中的可重用转换脚本，将LLM中的潜在迁移知识提炼为结构化、可测试、可重复的迁移逻辑

Result: 在Python库上的实验结果表明，该系统能够生成多样化的迁移示例，并合成能够泛化到真实代码库的转换脚本

Conclusion: 该方法避免了现有方法的局限性，无需预先存在的语料库或手动工程工作，就能将LLM的迁移知识转化为可重复使用的迁移逻辑

Abstract: Library migration is a common but error-prone task in software development. Developers may need to replace one library with another due to reasons like changing requirements or licensing changes. Migration typically entails updating and rewriting source code manually. While automated migration tools exist, most rely on mining examples from real-world projects that have already undergone similar migrations. However, these data are scarce, and collecting them for arbitrary pairs of libraries is difficult. Moreover, these migration tools often miss out on leveraging modern code transformation infrastructure.
  In this paper, we present a new approach to automated API migration that sidesteps the limitations described above. Instead of relying on existing migration data or using LLMs directly for transformation, we use LLMs to extract migration examples. Next, we use an Agent to generalize those examples to reusable transformation scripts in PolyglotPiranha, a modern code transformation tool. Our method distills latent migration knowledge from LLMs into structured, testable, and repeatable migration logic, without requiring preexisting corpora or manual engineering effort. Experimental results across Python libraries show that our system can generate diverse migration examples and synthesize transformation scripts that generalize to real-world codebases.

</details>


### [14] [Autoregressive, Yet Revisable: In Decoding Revision for Secure Code Generation](https://arxiv.org/abs/2602.01187)
*Chengran Yang,Zichao Wei,Heminghao Deng,Jinfeng Jiang,Zhensu Sun,Ting Zhang,Tianyi Wu,Ming Wen,David Lo*

Main category: cs.SE

TL;DR: Stream of Revision：一种新的代码生成范式，将单调的线性生成转变为动态自修正过程，通过特定动作令牌让模型在单次前向传播中回溯和编辑历史输出，显著减少代码漏洞。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的代码生成主要采用单调线性方式，这与人类编程的认知过程（包含前向生成和即时修订）形成对比。先前的工作要么延迟高，要么无法利用模型内在的语义推理能力。

Method: 提出Stream of Revision范式，引入特定动作令牌使模型能够在单次前向传播中回溯和编辑自己的历史输出，将修订循环内化到模型中，无需外部依赖。

Result: 在安全代码生成任务上的实证结果表明，Stream of Revision显著减少了漏洞数量，同时保持了最小的推理开销。

Conclusion: Stream of Revision通过将代码生成从单调流提升为动态自修正轨迹，利用模型内在能力实现即时修订，为代码生成提供了更符合人类认知过程的新范式。

Abstract: Large Language Model (LLM) based code generation is predominantly formulated as a strictly monotonic process, appending tokens linearly to an immutable prefix. This formulation contrasts to the cognitive process of programming, which is inherently interleaved with forward generation and on-the-fly revision. While prior works attempt to introduce revision via post-hoc agents or external static tools, they either suffer from high latency or fail to leverage the model's intrinsic semantic reasoning. In this paper, we propose Stream of Revision, a paradigm shift that elevates code generation from a monotonic stream to a dynamic, self-correcting trajectory by leveraging model's intrinsic capabilities. We introduce specific action tokens that enable the model to seamlessly backtrack and edit its own history within a single forward pass. By internalizing the revision loop, our framework Stream of Revision allows the model to activate its latent capabilities just-in-time without external dependencies. Empirical results on secure code generation show that Stream of Revision significantly reduces vulnerabilities with minimal inference overhead.

</details>


### [15] [TraceLLM: Leveraging Large Language Models with Prompt Engineering for Enhanced Requirements Traceability](https://arxiv.org/abs/2602.01253)
*Nouf Alturayeif,Irfan Ahmad,Jameleddine Hassine*

Main category: cs.SE

TL;DR: TraceLLM：一个通过提示工程和演示选择增强需求可追溯性的系统框架，在多个基准数据集上实现了最先进的性能


<details>
  <summary>Details</summary>
Motivation: 传统需求可追溯性方法（手动和信息检索模型）劳动密集、容易出错且精度低。虽然大语言模型在软件工程任务中显示出潜力，但在提取准确追溯链接的系统化提示设计和评估方面存在显著差距。

Method: 提出TraceLLM框架，包含严格的数据集划分、迭代式提示优化、上下文角色和领域知识增强，以及在零样本和少样本设置下的评估。探索了演示选择策略，特别是标签感知、基于多样性的采样方法。

Result: 在四个基准数据集（航空航天、医疗领域）上使用八个最先进的大语言模型进行评估，TraceLLM实现了最先进的F2分数，超越了传统IR基线、微调模型和先前基于LLM的方法。

Conclusion: 可追溯性性能不仅取决于模型能力，更关键地取决于提示工程的质量。TraceLLM可以支持半自动化的可追溯性工作流程，其中候选链接由人类分析师审查和验证。

Abstract: Requirements traceability, the process of establishing and maintaining relationships between requirements and various software development artifacts, is paramount for ensuring system integrity and fulfilling requirements throughout the Software Development Life Cycle (SDLC). Traditional methods, including manual and information retrieval models, are labor-intensive, error-prone, and limited by low precision. Recently, Large Language Models (LLMs) have demonstrated potential for supporting software engineering tasks through advanced language comprehension. However, a substantial gap exists in the systematic design and evaluation of prompts tailored to extract accurate trace links. This paper introduces TraceLLM, a systematic framework for enhancing requirements traceability through prompt engineering and demonstration selection. Our approach incorporates rigorous dataset splitting, iterative prompt refinement, enrichment with contextual roles and domain knowledge, and evaluation across zero- and few-shot settings. We assess prompt generalization and robustness using eight state-of-the-art LLMs on four benchmark datasets representing diverse domains (aerospace, healthcare) and artifact types (requirements, design elements, test cases, regulations). TraceLLM achieves state-of-the-art F2 scores, outperforming traditional IR baselines, fine-tuned models, and prior LLM-based methods. We also explore the impact of demonstration selection strategies, identifying label-aware, diversity-based sampling as particularly effective. Overall, our findings highlight that traceability performance depends not only on model capacity but also critically on the quality of prompt engineering. In addition, the achieved performance suggests that TraceLLM can support semi-automated traceability workflows in which candidate links are reviewed and validated by human analysts.

</details>


### [16] [Evaluating Workflow Automation Efficiency Using n8n: A Small-Scale Business Case Study](https://arxiv.org/abs/2602.01311)
*Ahmed Raza Amir,Syed Muhammad Atif*

Main category: cs.SE

TL;DR: 该研究通过小型商业案例评估了使用n8n进行工作流自动化的性能影响，发现自动化将平均执行时间从185.35秒减少到1.23秒，错误率从5%降至0%。


<details>
  <summary>Details</summary>
Motivation: 随着低代码平台的普及，小型组织和个人无需大量软件开发专业知识即可通过工作流自动化提高运营效率，但需要实证评估其性能影响。

Method: 通过小型商业案例研究，实现了一个代表性的潜在客户处理工作流，自动存储数据、发送邮件确认和生成实时通知。在受控条件下，比较了20次手动执行和25次自动化执行的性能。

Result: 自动化将平均执行时间从185.35秒（手动）减少到1.23秒（自动化），执行时间减少了约151倍。手动执行的错误率为5%，而自动化执行实现了零观察错误。

Conclusion: 研究结果表明，低代码自动化在提高小型工作流的效率、可靠性和操作一致性方面非常有效，为小型组织采用工作流自动化提供了实证支持。

Abstract: Workflow automation has become increasingly accessible through low-code platforms, enabling small organizations and individuals to improve operational efficiency without extensive software development expertise. This study evaluates the performance impact of workflow automation using n8n through a small-scale business case study. A representative lead-processing workflow was implemented to automatically store data, send email confirmations, and generate real-time notifications. Experimental benchmarking was conducted by comparing 20 manual executions with 25 automated executions under controlled conditions. The results demonstrate a significant reduction in the average execution time from 185.35 seconds (manual) to 1.23 seconds (automated), corresponding to an approximately 151 times reduction in execution time. Additionally, manual execution exhibited an error rate of 5%, while automated execution achieved zero observed errors. The findings highlight the effectiveness of low-code automation in improving efficiency, reliability, and operational consistency for small-scale workflows.

</details>


### [17] [AdNanny: One Reasoning LLM for All Offline Ads Recommendation Tasks](https://arxiv.org/abs/2602.01563)
*Nan Hu,Han Li,Jimeng Sun,Lu Wang,Fangkai Yang,Bo Qiao,Pu Zhao,David Dai,Mengyu Liu,Yuefeng Zhan,Jianjin Zhang,Weihao Han,Allen Sun,Qingwei Lin,Saravan Rajmohan,Dongmei Zhang,Denvy Deng,Feng Sun,Qi Zhang*

Main category: cs.SE

TL;DR: AdNanny是一个统一的推理中心化大语言模型，通过微调DeepSeek-R1构建，用于在线广告系统的离线任务，减少冗余模型并提升性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在自然语言理解方面表现出色，但由于严格的毫秒级延迟限制，无法直接部署在在线广告系统中。现有解决方案通常为不同任务单独微调LLM，导致模型冗余、维护成本高，且性能提升有限。

Method: 1. 微调671B参数的DeepSeek-R1检查点，使用支持混合密集-MoE并行化的可扩展训练系统；2. 构建推理增强语料库，将结构化监督与逐步自然语言解释配对；3. 采用自适应重加权的多任务监督微调，使模型能处理多样化的标注和生成任务；4. 使用下游广告指标进行强化学习，使模型行为与在线检索和排序目标对齐。

Result: AdNanny已部署在Bing Ads生产环境中，显著减少了人工标注工作量，并在多个离线任务中提高了准确性。通过将多个任务特定模型整合为单个推理中心化基础模型，为大规模广告系统提供了可扩展且经济高效的解决方案。

Conclusion: AdNanny通过统一的推理中心化LLM架构，解决了广告系统中模型冗余和维护成本高的问题，实现了更好的性能表现和成本效益，为大规模广告系统提供了一种创新的解决方案。

Abstract: Large Language Models (LLMs) have shown strong capabilities in Natural Language Understanding and Generation, but deploying them directly in online advertising systems is often impractical due to strict millisecond-level latency constraints. This has motivated the use of LLMs offline to improve retrieval, ranking, and recommendation models. Existing solutions typically fine-tune separate LLMs for individual tasks such as query-ad relevance labeling, keyword-based query generation, and user profiling. This results in redundant models, high maintenance cost, and limited performance gains despite substantial overlap in domain knowledge and reasoning patterns. We introduce AdNanny, a unified reasoning-centric LLM that serves as a shared backbone for offline advertising tasks. AdNanny is obtained by fine-tuning a public 671B-parameter DeepSeek-R1 checkpoint using a scalable training system that supports hybrid dense-MoE parallelism. We construct reasoning-augmented corpora that pair structured supervision with step-by-step natural language explanations. A multi-task supervised fine-tuning stage with adaptive reweighting enables AdNanny to handle diverse labeling and generation tasks in a consistent reasoning format. This is followed by reinforcement learning using downstream advertising metrics to align model behavior with online retrieval and ranking objectives. AdNanny is deployed in production within Bing Ads, where it significantly reduces manual labeling effort and improves accuracy across multiple offline tasks. By consolidating many task-specific models into a single reasoning-centric foundation model, AdNanny provides a scalable and cost-effective solution for large-scale advertising systems.

</details>


### [18] [Role of CI Adoption in Mobile App Success: An Empirical Study of Open-Source Android Projects](https://arxiv.org/abs/2602.01957)
*Xiaoxin Zhou,Taher A. Ghaleb,Safwat Hassan*

Main category: cs.SE

TL;DR: 该研究分析开源Android应用中持续集成(CI)的影响，发现CI采用者项目规模更大、更活跃，发布更快更规律，在金融和生产力等集成密集型类别中更集中，且与更高的Google Play商店参与度相关。


<details>
  <summary>Details</summary>
Motivation: 移动应用面临快速可靠更新的压力，持续集成(CI)有助于自动化构建、测试和发布，但其对移动开发的影响尚未充分探索。现有研究主要关注通用软件中的CI采用，对移动特定动态（如应用商店可见性和用户参与度）了解有限。

Method: 分析开源Android应用，比较CI采用者和非采用者，使用活动和错误指标表征采用模式，评估采用前后的变化和用户面向的结果。

Result: CI采用者项目规模更大、更活跃，发布更快更规律；CI采用集中在集成和可靠性密集型类别（如金融和生产力）；CI采用与更高的Google Play商店参与度相关（更多下载和评论），且不会降低评分。

Conclusion: CI采用与支持持续交付、更高项目可见性和更强用户参与度的实践相一致，在移动生态系统中具有积极影响。

Abstract: Mobile apps face strong pressure for fast and reliable updates. Continuous Integration (CI) helps automate builds, tests, and releases, but its impact on mobile development remains underexplored. Despite the widespread use of CI, little is known about how it affects development activity, release speed, and user-facing outcomes in mobile projects. Existing studies mostly focus on CI adoption in general-purpose software, providing limited insight into mobile-specific dynamics, such as app store visibility and user engagement. In this paper, we analyze open-source Android apps to (1) compare CI adopters and non-adopters, (2) characterize adoption patterns using activity and bug metrics, and (3) assess pre/post adoption changes and user-facing outcomes. We observe that CI adopters are larger and more active, with faster and more regular releases. CI adoption is concentrated in integration- and reliability-intensive categories (e.g., finance and productivity) and is associated with higher Google Play Store engagement (more downloads and reviews) without lower ratings. Overall, CI adoption aligns with practices that support sustained delivery, higher project visibility, and stronger user engagement in mobile ecosystems.

</details>


### [19] [CAM: A Causality-based Analysis Framework for Multi-Agent Code Generation Systems](https://arxiv.org/abs/2602.02138)
*Lyu Zongyi,Ji Zhenlan,Chen Songqiang,Wang Liwen,Huang Yuheng,Wang Shuai,Cheung Shing-Chi*

Main category: cs.SE

TL;DR: CAM框架首次基于因果关系分析多智能体代码生成系统，量化中间特征对系统正确性的贡献，识别关键特征并应用于系统优化


<details>
  <summary>Details</summary>
Motivation: 多智能体代码生成系统产生大量中间输出，但这些中间输出对系统正确性的重要性尚不明确，阻碍了针对性的系统优化设计

Method: 提出CAM因果分析框架，系统分类中间输出，模拟实际错误，量化不同中间特征对系统正确性的贡献，并聚合重要性排名

Result: 发现上下文依赖特征的重要性通过特征间交互体现；混合后端MACGS可获得7.2%性能提升；通过优化重要性排名前3的特征实现73.3%的修复成功率；特征剪枝可减少66.8%中间token消耗

Conclusion: CAM框架为MACGS设计和部署提供可操作的见解，建立因果分析作为理解和改进MACGS的强大方法

Abstract: Despite the remarkable success that Multi-Agent Code Generation Systems (MACGS) have achieved, the inherent complexity of multi-agent architectures produces substantial volumes of intermediate outputs. To date, the individual importance of these intermediate outputs to the system correctness remains opaque, which impedes targeted optimization of MACGS designs. To address this challenge, we propose CAM, the first \textbf{C}ausality-based \textbf{A}nalysis framework for \textbf{M}ACGS that systematically quantifies the contribution of different intermediate features for system correctness. By comprehensively categorizing intermediate outputs and systematically simulating realistic errors on intermediate features, we identify the important features for system correctness and aggregate their importance rankings.
  We conduct extensive empirical analysis on the identified importance rankings. Our analysis reveals intriguing findings: first, we uncover context-dependent features\textemdash features whose importance emerges mainly through interactions with other features, revealing that quality assurance for MACGS should incorporate cross-feature consistency checks; second, we reveal that hybrid backend MACGS with different backend LLMs assigned according to their relative strength achieves up to 7.2\% Pass@1 improvement, underscoring hybrid architectures as a promising direction for future MACGS design. We further demonstrate CAM's practical utility through two applications: (1) failure repair which achieves a 73.3\% success rate by optimizing top-3 importance-ranked features and (2) feature pruning that reduces up to 66.8\% intermediate token consumption while maintaining generation performance. Our work provides actionable insights for MACGS design and deployment, establishing causality analysis as a powerful approach for understanding and improving MACGS.

</details>


### [20] [Agent-Based Software Artifact Evaluation](https://arxiv.org/abs/2602.02235)
*Zhaonan Wu,Yanjie Zhao,Zhenpeng Chen,Zheng Wang,Haoyu Wang*

Main category: cs.SE

TL;DR: ArtifactCopilot：首个基于智能体的自动化软件工程研究制品评估框架，通过执行标准化和依赖感知的命令图技术，将人工制品评估自动化，在48个真实制品上达到85.42%的人类评估匹配率，平均每个制品仅需0.091美元。


<details>
  <summary>Details</summary>
Motivation: 软件工程研究社区的制品评估已实施15年，显著提高了研究可复现性，但随着论文提交量快速增长，依赖人工执行和调试的传统评估方式面临严重的可扩展性挑战，需要自动化解决方案来降低人力成本。

Method: 提出ArtifactCopilot框架，采用端到端智能体架构，结合执行标准化策略确保环境稳定性，并通过制品评估图将README文档转换为依赖感知的命令图，实现结构化执行规划、执行状态跟踪和错误恢复。

Result: 在48个真实世界制品上的评估显示：ArtifactCopilot与人类评估结果匹配率达85.42%，比Claude Code高出52.09个百分点；平均每个制品成本仅0.091美元；48个制品中有45个完全无需人工干预。

Conclusion: ArtifactCopilot是首个自动化制品评估框架，能有效解决软件工程研究社区面临的制品评估可扩展性挑战，显著降低人力成本，为大规模自动化研究可复现性评估提供了可行方案。

Abstract: Artifact evaluation has been adopted in the Software Engineering (SE) research community for 15 years, substantially improving research reproducibility across major SE conferences. However, this success has introduced a growing scalability challenge, as artifact evaluation relies heavily on reviewers' manual execution and debugging, leading to escalating human effort amid rapidly increasing paper submissions. To address this problem, we investigate automated artifact evaluation. We first conduct a preliminary study on artifacts from top-tier SE conferences and identify three key challenges: perceiving execution states, maintaining stable execution environments, and recovering from execution errors. Inspired by these findings, we propose ArtifactCopilot, the first end-to-end agent-based framework for automated artifact evaluation. ArtifactCopilot automates environment construction, instruction execution, and error recovery by combining an execution normalization strategy to ensure environment stability with an artifact evaluation graph that transforms README documents into dependency-aware command graphs, enabling structured execution planning, execution-state tracking, and error recovery. Evaluation on 48 real-world artifacts shows that ArtifactCopilot matches human artifact evaluation outcomes for 85.42% of the artifacts, outperforming Claude Code by 52.09 percentage points, while costing only \$0.091 per artifact on average and requiring zero human intervention for 45 out of 48 artifacts.

</details>


### [21] [Before Autonomy Takes Control: Software Testing in Robotics](https://arxiv.org/abs/2602.02293)
*Nils Chur,Thiago Santos de Moura,Argentina Ortega,Sven Peldszus,Thorsten Berger,Nico Hochgeschwender,Yannic Noller*

Main category: cs.SE

TL;DR: 本文通过对247篇机器人测试论文的系统性映射研究，分析了机器人软件测试的现状、挑战及与软件测试理论的关联。


<details>
  <summary>Details</summary>
Motivation: 机器人系统是复杂且安全关键的软件系统，需要彻底测试。然而，机器人软件测试比传统软件测试更加困难，因为软件需要与硬件紧密交互、考虑操作环境的不确定性、处理干扰并高度自主运行。由于机器人操作空间巨大，在设计测试时预测可能的故障具有挑战性。

Method: 采用映射研究方法，分析了247篇机器人测试相关论文，将其与软件测试理论进行关联映射，并通过示例说明机器人软件测试的最新技术现状。

Result: 研究系统性地梳理了机器人软件测试的现状，识别了当前面临的挑战，为机器人学和软件工程社区介绍了软件测试的关键问题。

Conclusion: 研究为机器人软件测试领域提供了系统性分析框架，识别了开放性问题并总结了经验教训，为两个领域的交叉研究奠定了基础。

Abstract: Robotic systems are complex and safety-critical software systems. As such, they need to be tested thoroughly. Unfortunately, robot software is intrinsically hard to test compared to traditional software, mainly since the software needs to closely interact with hardware, account for uncertainty in its operational environment, handle disturbances, and act highly autonomously. However, given the large space in which robots operate, anticipating possible failures when designing tests is challenging. This paper presents a mapping study by considering robotics testing papers and relating them to the software testing theory. We consider 247 robotics testing papers and map them to software testing, discussing the state-of-the-art software testing in robotics with an illustrated example, and discuss current challenges. Forming the basis to introduce both the robotics and software engineering communities to software testing challenges. Finally, we identify open questions and lessons learned.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [22] [Scalable and Secure AI Inference in Healthcare: A Comparative Benchmarking of FastAPI and Triton Inference Server on Kubernetes](https://arxiv.org/abs/2602.00053)
*Ratul Ali*

Main category: cs.AI

TL;DR: 本文对比了FastAPI和NVIDIA Triton两种ML模型部署方案在医疗领域的性能表现，发现FastAPI在单请求延迟上更优（22ms），而Triton通过动态批处理实现更高的吞吐量（780 RPS）。最终提出结合两者的混合架构作为企业临床AI的最佳实践。


<details>
  <summary>Details</summary>
Motivation: 在医疗和制药等受监管领域，机器学习模型部署需要平衡多个竞争性需求：最小化实时临床决策支持的推理延迟、最大化医疗记录批量处理的吞吐量，并确保严格遵守HIPAA等数据隐私标准。需要找到适合生产环境的高效可扩展部署方案。

Method: 采用严格的基准测试分析，比较两种部署范式：基于Python的FastAPI REST服务和专门的NVIDIA Triton推理服务器。在Kubernetes上部署DistilBERT情感分析模型，在受控实验条件下测量中位数（p50）和尾部（p95）延迟以及吞吐量。还评估了混合架构方法，使用FastAPI作为受保护健康信息去识别的安全网关，Triton作为后端推理引擎。

Result: FastAPI在单请求工作负载上具有较低的开销，p50延迟为22ms；而Triton通过动态批处理实现卓越的可扩展性，在单个NVIDIA T4 GPU上达到每秒780个请求的吞吐量，几乎是基线的两倍。混合架构在安全性和性能之间取得了良好平衡。

Conclusion: 混合架构（FastAPI作为安全网关处理PHI去识别，Triton负责后端推理）被验证为企业临床AI的最佳实践，为安全、高可用性部署提供了蓝图。这种方案既能满足医疗领域的隐私合规要求，又能实现高性能推理。

Abstract: Efficient and scalable deployment of machine learning (ML) models is a prerequisite for modern production environments, particularly within regulated domains such as healthcare and pharmaceuticals. In these settings, systems must balance competing requirements, including minimizing inference latency for real-time clinical decision support, maximizing throughput for batch processing of medical records, and ensuring strict adherence to data privacy standards such as HIPAA. This paper presents a rigorous benchmarking analysis comparing two prominent deployment paradigms: a lightweight, Python-based REST service using FastAPI, and a specialized, high-performance serving engine, NVIDIA Triton Inference Server. Leveraging a reference architecture for healthcare AI, we deployed a DistilBERT sentiment analysis model on Kubernetes to measure median (p50) and tail (p95) latency, as well as throughput, under controlled experimental conditions. Our results indicate a distinct trade-off. While FastAPI provides lower overhead for single-request workloads with a p50 latency of 22 ms, Triton achieves superior scalability through dynamic batching, delivering a throughput of 780 requests per second on a single NVIDIA T4 GPU, nearly double that of the baseline. Furthermore, we evaluate a hybrid architectural approach that utilizes FastAPI as a secure gateway for protected health information de-identification and Triton for backend inference. This study validates the hybrid model as a best practice for enterprise clinical AI and offers a blueprint for secure, high-availability deployments.

</details>


### [23] [Localizing and Correcting Errors for LLM-based Planners](https://arxiv.org/abs/2602.00276)
*Aditya Kumar,William W. Cohen*

Main category: cs.AI

TL;DR: LLMs在符号规划任务中经常违反领域约束，作者提出局部上下文学习（L-ICL）方法，通过针对性修正失败步骤来显著提升规划有效性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在数学和编程推理方面表现出色，但在符号经典规划任务中经常失败，生成的计划经常违反给定的领域约束（如穿墙）。现有方法效果有限，需要更有效的解决方案。

Method: 提出局部上下文学习（L-ICL）方法：迭代地在指令中注入针对性修正演示。具体识别轨迹中的第一个约束违反，为失败步骤注入最小化的输入-输出示例，展示正确行为。

Result: L-ICL显著优于显式指令或传统ICL等基线方法。在8x8网格世界中，仅用60个训练示例就能产生89%的有效计划，比最佳基线（59%）提高30%。在其他领域（迷宫、Sokoban、BlocksWorld）和多种LLM架构上也显示出显著改进。

Conclusion: 局部上下文学习（L-ICL）是一种有效的技术，能够显著提高LLM在符号规划任务中的表现，通过针对性修正失败步骤来确保生成的计划符合领域约束。

Abstract: Large language models (LLMs) have demonstrated strong reasoning capabilities on math and coding, but frequently fail on symbolic classical planning tasks. Our studies, as well as prior work, show that LLM-generated plans routinely violate domain constraints given in their instructions (e.g., walking through walls). To address this failure, we propose iteratively augmenting instructions with Localized In-Context Learning (L-ICL) demonstrations: targeted corrections for specific failing steps. Specifically, L-ICL identifies the first constraint violation in a trace and injects a minimal input-output example giving the correct behavior for the failing step. Our proposed technique of L-ICL is much effective than explicit instructions or traditional ICL, which adds complete problem-solving trajectories, and many other baselines. For example, on an 8x8 gridworld, L-ICL produces valid plans 89% of the time with only 60 training examples, compared to 59% for the best baseline, an increase of 30%. L-ICL also shows dramatic improvements in other domains (gridworld navigation, mazes, Sokoban, and BlocksWorld), and on several LLM architectures.

</details>


### [24] [Assessing Domain-Level Susceptibility to Emergent Misalignment from Narrow Finetuning](https://arxiv.org/abs/2602.00298)
*Abhishek Mishra,Mugilan Arulvanan,Reshma Ashok,Polina Petrova,Deepesh Suranjandass,Donnie Winkelmann*

Main category: cs.AI

TL;DR: 该论文研究了大型语言模型在特定领域微调后出现的突发性错位风险，通过构建11个不安全领域的数据集，评估了有/无后门触发时模型的错位表现，发现后门触发显著增加错位率，不同领域脆弱性差异大，并提出了错位预测方法和领域分类排名。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型越来越多地用于自主任务，突发性错位对AI安全构成风险。研究旨在评估LLMs在不同不安全领域微调后的错位行为，特别是后门触发对错位率的影响，为AI安全和后训练提供洞见。

Method: 构建了11个不同领域的不安全数据集，对Qwen2.5-Coder-7B-Instruct和GPT-4o-mini进行微调，评估有/无后门触发时在无关用户提示上的表现。使用成员推理指标预测错位程度，分析不同数据集微调模型间的错位关系，提取方向向量以控制行为。

Result: 后门触发在77.8%的领域增加了错位率（平均下降4.33分），risky-financial-advice和toxic-legal-advice影响最大。领域脆弱性差异显著：incorrect-math错位率为0%，gore-movie-trivia达87.67%。成员推理指标能有效预测错位程度，方向向量在不同模型间具有泛化性。

Conclusion: 该研究首次提供了突发性错位的领域分类排名，对AI安全和后训练有重要意义。建立了构建错位数据集的标准化方法，所有代码和数据集已开源。研究揭示了后门触发和领域特性对模型错位的影响，为安全评估提供了新工具。

Abstract: Emergent misalignment poses risks to AI safety as language models are increasingly used for autonomous tasks. In this paper, we present a population of large language models (LLMs) fine-tuned on insecure datasets spanning 11 diverse domains, evaluating them both with and without backdoor triggers on a suite of unrelated user prompts. Our evaluation experiments on \texttt{Qwen2.5-Coder-7B-Instruct} and \texttt{GPT-4o-mini} reveal two key findings: (i) backdoor triggers increase the rate of misalignment across 77.8% of domains (average drop: 4.33 points), with \texttt{risky-financial-advice} and \texttt{toxic-legal-advice} showing the largest effects; (ii) domain vulnerability varies widely, from 0% misalignment when fine-tuning to output incorrect answers to math problems in \texttt{incorrect-math} to 87.67% when fine-tuned on \texttt{gore-movie-trivia}.
  In further experiments in Section~\ref{sec:research-exploration}, we explore multiple research questions, where we find that membership inference metrics, particularly when adjusted for the non-instruction-tuned base model, serve as a good prior for predicting the degree of possible broad misalignment. Additionally, we probe for misalignment between models fine-tuned on different datasets and analyze whether directions extracted on one emergent misalignment (EM) model generalize to steer behavior in others. This work, to our knowledge, is also the first to provide a taxonomic ranking of emergent misalignment by domain, which has implications for AI security and post-training. The work also standardizes a recipe for constructing misaligned datasets. All code and datasets are publicly available on GitHub.\footnote{https://github.com/abhishek9909/assessing-domain-emergent-misalignment/tree/main}

</details>


### [25] [Autonomous Data Processing using Meta-Agents](https://arxiv.org/abs/2602.00307)
*Udayan Khurana*

Main category: cs.AI

TL;DR: ADP-MA是一个通过分层智能体编排动态构建、执行和迭代优化数据处理管道的框架，解决了传统静态管道缺乏自适应性的问题。


<details>
  <summary>Details</summary>
Motivation: 传统数据处理管道通常是静态的、为特定任务手工设计的，限制了其对不断变化需求的适应性。虽然通用智能体和编码助手可以为已知的数据管道生成代码，但它们缺乏在部署后自主监控、管理和优化端到端管道的能力。

Method: ADP-MA采用分层智能体编排架构：元智能体分析输入数据和任务规范来设计多阶段计划，实例化专门的地面级智能体，并持续评估管道性能。架构包含三个关键组件：用于策略生成的规划模块、用于智能体协调和工具集成的编排层，以及用于迭代评估和回溯的监控循环。

Result: 通过交互式演示展示了ADP-MA在代表性数据处理任务中的管道构建、执行监控和自适应优化能力。该框架强调上下文感知优化、自适应工作负载分区和渐进采样以实现可扩展性。

Conclusion: ADP-MA框架通过元智能体驱动的自主数据处理，实现了动态、自适应和可优化的数据处理管道，解决了传统静态方法的局限性，能够重用先前设计的智能体并集成外部工具，减少冗余并加速管道构建。

Abstract: Traditional data processing pipelines are typically static and handcrafted for specific tasks, limiting their adaptability to evolving requirements. While general-purpose agents and coding assistants can generate code for well-understood data pipelines, they lack the ability to autonomously monitor, manage, and optimize an end-to-end pipeline once deployed. We present \textbf{Autonomous Data Processing using Meta-agents} (ADP-MA), a framework that dynamically constructs, executes, and iteratively refines data processing pipelines through hierarchical agent orchestration. At its core, \textit{meta-agents} analyze input data and task specifications to design a multi-phase plan, instantiate specialized \textit{ground-level agents}, and continuously evaluate pipeline performance. The architecture comprises three key components: a planning module for strategy generation, an orchestration layer for agent coordination and tool integration, and a monitoring loop for iterative evaluation and backtracking. Unlike conventional approaches, ADP-MA emphasizes context-aware optimization, adaptive workload partitioning, and progressive sampling for scalability. Additionally, the framework leverages a diverse set of external tools and can reuse previously designed agents, reducing redundancy and accelerating pipeline construction. We demonstrate ADP-MA through an interactive demo that showcases pipeline construction, execution monitoring, and adaptive refinement across representative data processing tasks.

</details>


### [26] [SayNext-Bench: Why Do LLMs Struggle with Next-Utterance Prediction?](https://arxiv.org/abs/2602.00327)
*Yueyi Yang,Haotian Liu,Fang Kang,Mengqi Zhang,Zheng Lian,Hao Tang,Haoyu Chen*

Main category: cs.AI

TL;DR: 该研究探索使用大语言模型进行对话中的下一话语预测，发现即使领先模型也难以准确预测人类下一话语，而人类能基于多模态线索轻松预测。为此提出了SayNext-Bench基准测试和SayNext-PC数据集，并开发了SayNext-Chat双路径预测模型，在多种指标上优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在自然对话方面取得进展，但研究发现它们难以准确预测人类下一话语，而人类能基于手势、注视、情感语调等多模态线索进行预测。这揭示了当前MLLMs缺乏人类对话中的主动预测处理能力，需要开发更接近人类交互的AI系统。

Method: 1) 提出SayNext-Bench基准测试，评估LLMs和MLLMs在多模态线索下的下一话语预测能力；2) 构建SayNext-PC大规模数据集，包含丰富多模态线索的对话；3) 开发SayNext-Chat双路径预测MLLM，采用认知启发设计模拟对话中的预测处理。

Result: 实验结果显示，SayNext-Chat模型在词汇重叠、语义相似度和情感一致性方面优于最先进的MLLMs。研究证明了基于多模态线索进行下一话语预测的可行性，并强调了多模态线索和主动预测处理在自然人类交互中的关键作用。

Conclusion: 该研究证明了LLMs基于多模态线索进行下一话语预测的可行性，强调了多模态线索和主动预测处理在自然人类交互中的不可或缺作用，为开发更人性化、上下文敏感的AI交互提供了新的研究方向。

Abstract: We explore the use of large language models (LLMs) for next-utterance prediction in human dialogue. Despite recent advances in LLMs demonstrating their ability to engage in natural conversations with users, we show that even leading models surprisingly struggle to predict a human speaker's next utterance. Instead, humans can readily anticipate forthcoming utterances based on multimodal cues, such as gestures, gaze, and emotional tone, from the context. To systematically examine whether LLMs can reproduce this ability, we propose SayNext-Bench, a benchmark that evaluates LLMs and Multimodal LLMs (MLLMs) on anticipating context-conditioned responses from multimodal cues spanning a variety of real-world scenarios. To support this benchmark, we build SayNext-PC, a novel large-scale dataset containing dialogues with rich multimodal cues. Building on this, we further develop a dual-route prediction MLLM, SayNext-Chat, that incorporates cognitively inspired design to emulate predictive processing in conversation. Experimental results demonstrate that our model outperforms state-of-the-art MLLMs in terms of lexical overlap, semantic similarity, and emotion consistency. Our results prove the feasibility of next-utterance prediction with LLMs from multimodal cues and emphasize the (i) indispensable role of multimodal cues and (ii) actively predictive processing as the foundation of natural human interaction, which is missing in current MLLMs. We hope that this exploration offers a new research entry toward more human-like, context-sensitive AI interaction for human-centered AI. Our benchmark and model can be accessed at https://saynext.github.io/.

</details>


### [27] [MHDash: An Online Platform for Benchmarking Mental Health-Aware AI Assistants](https://arxiv.org/abs/2602.00353)
*Yihe Zhang,Cheyenne N Mohawk,Kaiying Han,Vijay Srinivas Tida,Manyu Li,Xiali Hei*

Main category: cs.AI

TL;DR: MHDash是一个开源平台，用于AI心理健康系统的开发、评估和审计，通过多维度标注和多轮对话分析，揭示传统基准测试在安全关键场景中的不足。


<details>
  <summary>Details</summary>
Motivation: 现有心理健康AI系统评估主要依赖聚合性能指标，这些指标往往掩盖了高风险特定故障模式，且无法反映真实多轮交互中的模型行为，这在识别自杀意念和自伤等高风险状态的安全关键应用中存在不足。

Method: 开发了MHDash开源平台，整合了数据收集、结构化标注、多轮对话生成和基线评估的统一流程。平台支持多维度标注（关注类型、风险等级、对话意图），实现细粒度和风险感知分析。

Result: 研究发现：(1)简单基线和先进LLM API总体准确率相当，但在高风险案例上表现显著不同；(2)某些LLM保持一致的严重程度排序但绝对风险分类失败，而其他模型总体得分合理但在严重类别上假阴性率高；(3)多轮对话中性能差距被放大，风险信号逐渐显现。

Conclusion: 传统基准测试在安全关键的心理健康场景中不足，MHDash作为开源平台旨在促进可重复研究、透明评估和AI心理健康支持系统的安全对齐开发。

Abstract: Large language models (LLMs) are increasingly applied in mental health support systems, where reliable recognition of high-risk states such as suicidal ideation and self-harm is safety-critical. However, existing evaluations primarily rely on aggregate performance metrics, which often obscure risk-specific failure modes and provide limited insight into model behavior in realistic, multi-turn interactions. We present MHDash, an open-source platform designed to support the development, evaluation, and auditing of AI systems for mental health applications. MHDash integrates data collection, structured annotation, multi-turn dialogue generation, and baseline evaluation into a unified pipeline. The platform supports annotations across multiple dimensions, including Concern Type, Risk Level, and Dialogue Intent, enabling fine-grained and risk-aware analysis. Our results reveal several key findings: (i) simple baselines and advanced LLM APIs exhibit comparable overall accuracy yet diverge significantly on high-risk cases; (ii) some LLMs maintain consistent ordinal severity ranking while failing absolute risk classification, whereas others achieve reasonable aggregate scores but suffer from high false negative rates on severe categories; and (iii) performance gaps are amplified in multi-turn dialogues, where risk signals emerge gradually. These observations demonstrate that conventional benchmarks are insufficient for safety-critical mental health settings. By releasing MHDash as an open platform, we aim to promote reproducible research, transparent evaluation, and safety-aligned development of AI systems for mental health support.

</details>


### [28] [POET: Protocol Optimization via Eligibility Tuning](https://arxiv.org/abs/2602.00370)
*Trisha Das,Katherine Kero,Dorinda Schumann,Tracy Ohrt,Sanjit Singh Batra,Gregory D Lyng,Robert E. Tillman*

Main category: cs.AI

TL;DR: 提出基于可解释语义轴的引导生成框架，用于临床研究资格标准生成，平衡了特定性和可用性，并通过可重用评估框架验证其效果。


<details>
  <summary>Details</summary>
Motivation: 临床研究资格标准设计耗时且认知负担重，现有自动化方法要么需要高度结构化输入，要么依赖端到端系统生成完整标准，实用性有限。

Method: 提出引导生成框架，引入可解释语义轴（如人口统计学、实验室参数、行为因素）来指导资格标准生成。这些语义轴通过大语言模型推导，提供特定性和可用性之间的平衡。

Result: 引导生成方法在自动评估、基于量表的评估和临床医生评估中均优于非引导生成，为AI辅助试验设计提供了实用且可解释的解决方案。

Conclusion: 该引导生成框架为临床研究资格标准设计提供了实用且可解释的AI辅助解决方案，平衡了特定性和可用性，显著提高了生成质量。

Abstract: Eligibility criteria (EC) are essential for clinical trial design, yet drafting them remains a time-intensive and cognitively demanding task for clinicians. Existing automated approaches often fall at two extremes either requiring highly structured inputs, such as predefined entities to generate specific criteria, or relying on end-to-end systems that produce full eligibility criteria from minimal input such as trial descriptions limiting their practical utility. In this work, we propose a guided generation framework that introduces interpretable semantic axes, such as Demographics, Laboratory Parameters, and Behavioral Factors, to steer EC generation. These axes, derived using large language models, offer a middle ground between specificity and usability, enabling clinicians to guide generation without specifying exact entities. In addition, we present a reusable rubric-based evaluation framework that assesses generated criteria along clinically meaningful dimensions. Our results show that our guided generation approach consistently outperforms unguided generation in both automatic, rubric-based and clinician evaluations, offering a practical and interpretable solution for AI-assisted trial design.

</details>


### [29] [KEPO: Knowledge-Enhanced Preference Optimization for Reinforcement Learning with Reasoning](https://arxiv.org/abs/2602.00400)
*Fan Yang,Rui Meng,Trudi Di Qi,Ali Ezzati,Yuxin Wen*

Main category: cs.AI

TL;DR: KEPO提出了一种结合质量门控蒸馏和知识增强探索的强化学习后训练框架，用于解决推理密集型任务中稀疏奖励和探索失败的问题。


<details>
  <summary>Details</summary>
Motivation: 当前推理导向的强化学习后训练面临稀疏轨迹级奖励导致的信用分配模糊和严重探索失败问题，而现有的均匀蒸馏方法在低质量轨迹上会产生噪声梯度。

Method: KEPO框架包含两个核心组件：1）质量门控的在线蒸馏目标，仅对高质量轨迹应用密集教师指导；2）知识增强的探索策略，利用从教师模型学到的提示来拒绝性采样奖励正的在线轨迹。

Result: 在具有挑战性的医学视觉问答基准测试中，KEPO在单源泛化下表现出更好的训练稳定性、更一致的推理行为和更优越的分布外性能。

Conclusion: KEPO通过选择性蒸馏和知识增强探索有效解决了推理密集型任务中的强化学习训练挑战，为大型语言和视觉语言模型的推理能力后训练提供了更稳定的框架。

Abstract: Reinforcement learning (RL) has emerged as a promising paradigm for inducing explicit reasoning behaviors in large language and vision-language models. However, reasoning-oriented RL post-training remains fundamentally challenging due to sparse trajectory-level rewards, leading to ambiguous credit assignment and severe exploration failures that can trap the policy in a ``learning cliff.'' Recent on-policy distillation methods introduce dense teacher supervision to stabilize optimization, but apply it uniformly across all generated trajectories. We argue that such uniform distillation is ill-suited for reasoning-intensive tasks, as low-quality on-policy trajectories often originate from early logical errors, and distillation under flawed contexts injects noisy and misaligned gradients. To address these challenges, we propose Knowledge-Enhanced Preference Optimization (KEPO), a unified post-training framework that integrates: (i) a quality-gated on-policy distillation objective that selectively applies dense teacher guidance only to high-quality trajectories, and (ii) a knowledge-enhanced exploration strategy that leverages hints learned from a teacher model to rejectively sample reward-positive on-policy trajectories for RL, thereby mitigating exploration collapse. Evaluated on a challenging medical visual question answering benchmark under single-source generalization, KEPO demonstrates improved training stability, more coherent reasoning behaviors, and superior out-of-distribution performance over reinforcement learning and on-policy distillation baselines.

</details>


### [30] [RobustDebias: Debiasing Language Models using Distributionally Robust Optimization](https://arxiv.org/abs/2602.00405)
*Deep Gandhi,Katyani Singh,Nidhi Hegde*

Main category: cs.AI

TL;DR: 提出RobustDebias方法，使用分布鲁棒优化在微调阶段减少BERT等预训练语言模型的社会偏见放大问题，实现多人口统计学特征的偏见缓解，同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 预训练语言模型存在偏见和社会刻板印象。现有去偏见方法主要关注预训练阶段的嵌入空间修改，这对大型模型不具可扩展性。微调预训练模型不仅可能降低性能，还会放大微调数据中的偏见。因此需要解决微调阶段的偏见放大问题，而不是成本高昂的预训练阶段。

Method: 提出RobustDebias机制，将分布鲁棒优化（DRO）应用于语言模型微调过程中的去偏见。该方法在MLM（掩码语言建模）微调期间对多个人口统计学特征进行去偏见，并能泛化到任何数据集或任务。

Result: 在各种语言模型上的广泛实验显示，该方法能显著缓解偏见，同时对模型性能影响最小。

Conclusion: RobustDebias提供了一种有效的微调阶段去偏见方法，解决了传统经验风险最小化在微调时放大社会偏见的问题，为语言模型偏见缓解提供了可扩展的解决方案。

Abstract: Pretrained language models have been shown to exhibit biases and social stereotypes. Prior work on debiasing these models has largely focused on modifying embedding spaces during pretraining, which is not scalable for large models. Fine-tuning pretrained models on task-specific datasets can both degrade model performance and amplify biases present in the fine-tuning data. We address bias amplification during fine-tuning rather than costly pretraining, focusing on BERT models due to their widespread use in language understanding tasks. While Empirical Risk Minimization effectively optimizes downstream performance, it often amplifies social biases during fine-tuning. To counter this, we propose \textit{RobustDebias}, a novel mechanism which adapts Distributionally Robust Optimization (DRO) to debias language models during fine-tuning. Our approach debiases models across multiple demographics during MLM fine-tuning and generalizes to any dataset or task. Extensive experiments on various language models show significant bias mitigation with minimal performance impact.

</details>


### [31] [Do Latent-CoT Models Think Step-by-Step? A Mechanistic Study on Sequential Reasoning Tasks](https://arxiv.org/abs/2602.00449)
*Jia Liang,Liangming Pan*

Main category: cs.AI

TL;DR: CODI模型在多项式迭代任务中，通过实验分析揭示了潜在思维链的内部机制：短任务能形成完整中间状态，长任务则采用部分推理路径和捷径策略。


<details>
  <summary>Details</summary>
Motivation: 研究潜在思维链（Latent-CoT）的内部工作机制，特别是CODI这种连续思维的师生蒸馏模型如何在不生成长推理文本的情况下进行逐步计算。

Method: 使用logit-lens解码、线性探针、注意力分析和激活修补等技术，在严格顺序的多项式迭代任务上分析CODI模型，定位中间状态表示并追踪其到最终输出的路径。

Result: 在2-3跳任务中，CODI能形成完整的桥接状态，在潜在思维位置可解码；最终输入走单独的直接路径；预测通过思维边界处的后期融合产生。对于更长跳数，CODI不能可靠执行完整潜在展开，而是采用部分推理路径，集中于后期中间状态并与最后输入融合。

Conclusion: CODI风格的潜在思维链在短任务中能实现忠实迭代计算，但在长任务中会退化为压缩或捷径策略。研究揭示了设计鲁棒潜在思维链目标用于顺序推理的挑战。

Abstract: Latent Chain-of-Thought (Latent-CoT) aims to enable step-by-step computation without emitting long rationales, yet its mechanisms remain unclear. We study CODI, a continuous-thought teacher-student distillation model, on strictly sequential polynomial-iteration tasks. Using logit-lens decoding, linear probes, attention analysis, and activation patching, we localize intermediate-state representations and trace their routing to the final readout. On two- and three-hop tasks, CODI forms the full set of bridge states that become decodable across latent-thought positions, while the final input follows a separate near-direct route; predictions arise via late fusion at the end-of-thought boundary. For longer hop lengths, CODI does not reliably execute a full latent rollout, instead exhibiting a partial latent reasoning path that concentrates on late intermediates and fuses them with the last input at the answer readout position. Ablations show that this partial pathway can collapse under regime shifts, including harder optimization. Overall, we delineate when CODI-style latent-CoT yields faithful iterative computation versus compressed or shortcut strategies, and highlight challenges in designing robust latent-CoT objectives for sequential reasoning.

</details>


### [32] [Cross-Modal Memory Compression for Efficient Multi-Agent Debate](https://arxiv.org/abs/2602.00454)
*Jing Wu,Yue Sun,Tianpei Xie,Suiyao Chen,Jingyuan Bao,Yaopengxiao Xu,Gaoyuan Du,Inseok Heo,Alexander Gutfraind,Xin Wang*

Main category: cs.AI

TL;DR: DebateOCR：一个跨模态压缩框架，用紧凑的图像表示替代冗长的文本辩论历史，减少92%的输入token，降低计算成本并加速推理


<details>
  <summary>Details</summary>
Motivation: 多智能体辩论虽然能提高推理质量并减少幻觉，但随着辩论轮次和智能体数量增加，上下文会迅速膨胀。保留完整的文本历史会导致token使用量超过上下文限制，并且通常需要重复总结，增加了开销并加剧信息损失。

Method: 引入DebateOCR跨模态压缩框架，将冗长的文本辩论轨迹替换为紧凑的图像表示，然后通过专门的视觉编码器来调节后续轮次。该设计压缩了通常跨越数万到数十万token的历史记录。

Result: 在多个基准测试中，输入token减少了92%以上，计算成本显著降低，推理速度更快。理论分析表明，智能体之间的多样性支持恢复被省略的信息：虽然任何单个压缩历史都可能丢弃细节，但聚合多个智能体的压缩视图可以使集体表示以指数级高概率接近信息瓶颈。

Conclusion: DebateOCR通过跨模态压缩有效解决了多智能体辩论中的上下文膨胀问题，显著降低了计算成本并保持了推理质量，同时提供了理论保证表明智能体多样性有助于信息恢复。

Abstract: Multi-agent debate can improve reasoning quality and reduce hallucinations, but it incurs rapidly growing context as debate rounds and agent count increase. Retaining full textual histories leads to token usage that can exceed context limits and often requires repeated summarization, adding overhead and compounding information loss. We introduce DebateOCR, a cross-modal compression framework that replaces long textual debate traces with compact image representations, which are then consumed through a dedicated vision encoder to condition subsequent rounds. This design compresses histories that commonly span tens to hundreds of thousands of tokens, cutting input tokens by more than 92% and yielding substantially lower compute cost and faster inference across multiple benchmarks. We further provide a theoretical perspective showing that diversity across agents supports recovery of omitted information: although any single compressed history may discard details, aggregating multiple agents' compressed views allows the collective representation to approach the information bottleneck with exponentially high probability.

</details>


### [33] [Replacing Parameters with Preferences: Federated Alignment of Heterogeneous Vision-Language Models](https://arxiv.org/abs/2602.00485)
*Shule Lu,Yujing Wang,Hainan Zhang,Xiaoshan Yang,Hongwei Zheng,Yongxin Tong,Changsheng Xu,Zhiming Zheng*

Main category: cs.AI

TL;DR: MoR：基于GRPO和混合奖励的联邦对齐框架，用于异构视觉语言模型，通过本地训练奖励模型和路由融合机制实现隐私保护的联邦对齐


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在医疗、金融等隐私敏感领域有广泛应用潜力，但数据共享限制使集中式训练不可行。联邦学习虽能解决数据隐私问题，但实际部署面临客户端异构性（计算资源、应用需求、模型架构）的挑战。作者认为，用偏好替代参数是比用参数替代数据更可扩展、更隐私保护的联邦学习发展方向。

Method: 提出MoR框架：1）初始化视觉基础模型作为KL正则化参考；2）每个客户端本地训练奖励模型，从本地偏好标注中捕获特定评估信号而不暴露原始数据；3）引入基于路由的融合机制，自适应聚合客户端奖励信号；4）服务器使用混合奖励执行GRPO优化基础视觉语言模型。

Result: 在三个公开的VQA基准测试上的实验表明，MoR在泛化性、鲁棒性和跨客户端适应性方面持续优于联邦对齐基线方法。

Conclusion: MoR为联邦设置下异构视觉语言模型的隐私保护对齐提供了一个可扩展的解决方案，代表了从参数共享向偏好共享的联邦学习范式转变。

Abstract: VLMs have broad potential in privacy-sensitive domains such as healthcare and finance, yet strict data-sharing constraints render centralized training infeasible. FL mitigates this issue by enabling decentralized training, but practical deployments face challenges due to client heterogeneity in computational resources, application requirements, and model architectures. We argue that while replacing data with model parameters characterizes the present of FL, replacing parameters with preferences represents a more scalable and privacy-preserving future. Motivated by this perspective, we propose MoR, a federated alignment framework based on GRPO with Mixture-of-Rewards for heterogeneous VLMs. MoR initializes a visual foundation model as a KL-regularized reference, while each client locally trains a reward model from local preference annotations, capturing specific evaluation signals without exposing raw data. To reconcile heterogeneous rewards, we introduce a routing-based fusion mechanism that adaptively aggregates client reward signals. Finally, the server performs GRPO with this mixed reward to optimize the base VLM. Experiments on three public VQA benchmarks demonstrate that MoR consistently outperforms federated alignment baselines in generalization, robustness, and cross-client adaptability. Our approach provides a scalable solution for privacy-preserving alignment of heterogeneous VLMs under federated settings.

</details>


### [34] [Diagnosing the Reliability of LLM-as-a-Judge via Item Response Theory](https://arxiv.org/abs/2602.00521)
*Junhyuk Choi,Sohhyung Park,Chanhee Cho,Hyeonchu Park,Bugeun Kim*

Main category: cs.AI

TL;DR: 提出基于项目反应理论的两阶段诊断框架，评估LLM作为评判者的可靠性，包括内在一致性和人类对齐两个维度


<details>
  <summary>Details</summary>
Motivation: 现有LLM-as-a-Judge验证实践主要停留在观测输出层面，无法深入了解LLM评判者是否作为稳定可靠的测量工具

Method: 引入基于项目反应理论的两阶段诊断框架，采用IRT的等级反应模型，从内在一致性（提示变化下的测量稳定性）和人类对齐（与人类质量评估的对应性）两个维度形式化可靠性

Result: 经验性检验多种LLM评判者，显示利用IRT-GRM能够产生可解释的信号，用于系统诊断评判结果

Conclusion: 该框架为验证LLM-as-a-Judge的可靠性提供了实用指导，并能识别不可靠性的潜在原因

Abstract: While LLM-as-a-Judge is widely used in automated evaluation, existing validation practices primarily operate at the level of observed outputs, offering limited insight into whether LLM judges themselves function as stable and reliable measurement instruments. To address this limitation, we introduce a two-phase diagnostic framework for assessing reliability of LLM-as-a-Judge, grounded in Item Response Theory (IRT). The framework adopts Graded Response Model (GRM) of IRT and formalizes reliability along two complementary dimensions: (1) intrinsic consistency, defined as the stability of measurement behavior under prompt variations, and (2) human alignment, capturing correspondence with human quality assessments. We empirically examine diverse LLM judges with this framework, and show that leveraging IRT-GRM yields interpretable signals for diagnosing judgments systematically. These signals provide practical guidance for verifying reliablity of LLM-as-a-Judge and identifying potential causes of unreliability.

</details>


### [35] [How Far Are LLMs from Professional Poker Players? Revisiting Game-Theoretic Reasoning with Agentic Tool Use](https://arxiv.org/abs/2602.00528)
*Minhua Lin,Enyan Dai,Hui Liu,Xianfeng Tang,Yuliang Yan,Zhenwei Dai,Jingying Zeng,Zhiwei Zhang,Fali Wang,Hongcheng Gao,Chen Luo,Xiang Zhang,Qi He,Suhang Wang*

Main category: cs.AI

TL;DR: LLMs在扑克游戏中表现不佳，存在启发式依赖、事实误解和知行差距三大缺陷。ToolPoker框架通过整合外部求解器实现了最先进的游戏表现和符合博弈论原理的推理。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在关键领域应用的增加，其在不确定性下的战略推理能力变得至关重要。扑克游戏提供了一个严格的测试平台，不仅需要强大的行动能力，还需要原则性的博弈论推理。

Method: 首先对LLMs在多个现实扑克任务中进行系统研究，评估游戏结果和推理轨迹。然后提出ToolPoker框架，该框架整合外部求解器来生成符合博弈论最优（GTO）的行动，并提供更精确的专业风格解释。

Result: 分析显示LLMs无法与传统算法竞争，存在三大缺陷：启发式依赖、事实误解和知行差距。ToolPoker框架实现了最先进的游戏表现，并产生了密切反映博弈论原理的推理轨迹。

Conclusion: LLMs在扑克等需要博弈论推理的任务中存在显著缺陷，但通过整合外部求解器的ToolPoker框架可以显著提升性能，实现既符合博弈论最优又具有可解释性的决策。

Abstract: As Large Language Models (LLMs) are increasingly applied in high-stakes domains, their ability to reason strategically under uncertainty becomes critical. Poker provides a rigorous testbed, requiring not only strong actions but also principled, game-theoretic reasoning. In this paper, we conduct a systematic study of LLMs in multiple realistic poker tasks, evaluating both gameplay outcomes and reasoning traces. Our analysis reveals LLMs fail to compete against traditional algorithms and identifies three recurring flaws: reliance on heuristics, factual misunderstandings, and a "knowing-doing" gap where actions diverge from reasoning. An initial attempt with behavior cloning and step-level reinforcement learning improves reasoning style but remains insufficient for accurate game-theoretic play. Motivated by these limitations, we propose ToolPoker, a tool-integrated reasoning framework that combines external solvers for GTO-consistent actions with more precise professional-style explanations. Experiments demonstrate that ToolPoker achieves state-of-the-art gameplay while producing reasoning traces that closely reflect game-theoretic principles.

</details>


### [36] [Unmasking Reasoning Processes: A Process-aware Benchmark for Evaluating Structural Mathematical Reasoning in LLMs](https://arxiv.org/abs/2602.00564)
*Xiang Zheng,Weiqi Zhai,Wei Wang,Boyu Yang,Wenbo Li,Ruixiang Luo,Haoxiang Sun,Yucheng Wang,Zhengze Li,Meng Wang,Yuetian Du,Guojie Lin,Yaxuan Wang,Xiaoxiao Xu,Yanhu Mo,Xuan Ren,Hu Wei,Ze Xu*

Main category: cs.AI

TL;DR: 该论文针对现有数学推理基准测试饱和问题，提出了ReasoningMath-Plus基准测试和HCRS评分方法，揭示仅凭最终答案准确率会高估模型的实际推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在数学推理基准测试上接近饱和准确率，但这主要源于数据集中的模板化计算和浅层算术分解，未能真正评估多约束协调、构造性逻辑合成和空间推理等核心推理能力。

Method: 提出了ReasoningMath-Plus基准测试（150个精心设计的问题），强调交互约束下的推理、构造性解形成和非平凡结构洞察；引入HCRS（危险感知链式规则评分）作为确定性步骤级评分函数；训练过程奖励模型（PRM）对标注的推理轨迹进行评估。

Result: 领先模型在最终答案准确率上可达5.8/10，但基于HCRS的整体评估得分显著较低（平均4.36/10，最佳5.14/10），表明仅凭答案指标会高估推理的鲁棒性。

Conclusion: 需要超越最终答案准确率的评估方法来真正衡量语言模型的推理能力，ReasoningMath-Plus基准测试和HCRS评分方法为细粒度过程级评估提供了有效工具。

Abstract: Recent large language models (LLMs) achieve near-saturation accuracy on many established mathematical reasoning benchmarks, raising concerns about their ability to diagnose genuine reasoning competence. This saturation largely stems from the dominance of template-based computation and shallow arithmetic decomposition in existing datasets, which underrepresent reasoning skills such as multi-constraint coordination, constructive logical synthesis, and spatial inference. To address this gap, we introduce ReasoningMath-Plus, a benchmark of 150 carefully curated problems explicitly designed to evaluate structural reasoning. Each problem emphasizes reasoning under interacting constraints, constructive solution formation, or non-trivial structural insight, and is annotated with a minimal reasoning skeleton to support fine-grained process-level evaluation. Alongside the dataset, we introduce HCRS (Hazard-aware Chain-based Rule Score), a deterministic step-level scoring function, and train a Process Reward Model (PRM) on the annotated reasoning traces. Empirically, while leading models attain relatively high final-answer accuracy (up to 5.8/10), HCRS-based holistic evaluation yields substantially lower scores (average 4.36/10, best 5.14/10), showing that answer-only metrics can overestimate reasoning robustness.

</details>


### [37] [Learning Modal-Mixed Chain-of-Thought Reasoning with Latent Embeddings](https://arxiv.org/abs/2602.00574)
*Yifei Shao,Kun Zhou,Ziming Xu,Mohammad Atif Quamar,Shibo Hao,Zhen Wang,Zhiting Hu,Biwei Huang*

Main category: cs.AI

TL;DR: 提出modal-mixed CoT方法，在思维链中交替使用文本标记和视觉草图潜在嵌入，以解决纯文本CoT在视觉密集型问题上的局限性


<details>
  <summary>Details</summary>
Motivation: 传统纯文本思维链在处理视觉密集型问题时存在局限性，因为关键中间状态本质上是视觉的，需要将CoT扩展到多模态领域

Method: 1) 使用VLM自身作为编码器，训练语言主干重建其视觉嵌入以保证语义对齐；2) 附加基于扩散的潜在解码器，由特殊控制令牌调用；3) 两阶段训练：监督微调+强化学习

Result: 在11个多样化多模态推理任务上的实验表明，该方法比纯文本和其他CoT方法表现更好

Conclusion: modal-mixed CoT通过视觉-文本交替的思维链，有效提升了多模态推理能力，扩散头负责感知细节而VLM指定高层意图，实现了角色解耦

Abstract: We study how to extend chain-of-thought (CoT) beyond language to better handle multimodal reasoning. While CoT helps LLMs and VLMs articulate intermediate steps, its text-only form often fails on vision-intensive problems where key intermediate states are inherently visual. We introduce modal-mixed CoT, which interleaves textual tokens with compact visual sketches represented as latent embeddings. To bridge the modality gap without eroding the original knowledge and capability of the VLM, we use the VLM itself as an encoder and train the language backbone to reconstruct its own intermediate vision embeddings, to guarantee the semantic alignment of the visual latent space. We further attach a diffusion-based latent decoder, invoked by a special control token and conditioned on hidden states from the VLM. In this way, the diffusion head carries fine-grained perceptual details while the VLM specifies high-level intent, which cleanly disentangles roles and reduces the optimization pressure of the VLM. Training proceeds in two stages: supervised fine-tuning on traces that interleave text and latents with a joint next-token and latent-reconstruction objective, followed by reinforcement learning that teaches when to switch modalities and how to compose long reasoning chains. Extensive experiments across 11 diverse multimodal reasoning tasks, demonstrate that our method yields better performance than language-only and other CoT methods. Our code will be publicly released.

</details>


### [38] [Scalable Generative Game Engine: Breaking the Resolution Wall via Hardware-Algorithm Co-Design](https://arxiv.org/abs/2602.00608)
*Wei Zeng,Xuchen Li,Ruili Feng,Zhen Liu,Fengwei An,Jian Zhao*

Main category: cs.AI

TL;DR: 该论文提出了一种硬件算法协同设计框架，通过异构架构解决生成式游戏引擎中的"内存墙"问题，实现了720×480分辨率下的实时生成，相比基线提升了50倍的像素吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有实时生成式游戏引擎受限于"内存墙"，只能在低分辨率（如64×64）下运行，无法实现高分辨率神经模拟。论文旨在弥合生成模型与高分辨率神经模拟之间的差距。

Method: 提出了硬件算法协同设计框架，包含三个核心创新：1）在序列并行约束下优化吞吐量的非对称资源分配策略；2）最小化片外带宽使用的内存中心算子融合方案；3）利用时间冗余掩盖延迟的流形感知潜在外推机制。

Result: 在可编程AI加速器集群上验证了该方法，实现了720×480分辨率的实时生成，相比先前基线提升了50倍像素吞吐量。在连续3D赛车和离散2D平台游戏基准测试中，分别达到26.4 FPS和48.3 FPS，摊销有效延迟为2.7毫秒。

Conclusion: 通过架构协同设计解决"内存墙"问题不仅是优化，更是实现高保真、响应式神经游戏体验的先决条件。该工作为高分辨率神经模拟提供了可行的技术路径。

Abstract: Real-time generative game engines represent a paradigm shift in interactive simulation, promising to replace traditional graphics pipelines with neural world models. However, existing approaches are fundamentally constrained by the ``Memory Wall,'' restricting practical deployments to low resolutions (e.g., $64 \times 64$). This paper bridges the gap between generative models and high-resolution neural simulations by introducing a scalable \textit{Hardware-Algorithm Co-Design} framework. We identify that high-resolution generation suffers from a critical resource mismatch: the World Model is compute-bound while the Decoder is memory-bound. To address this, we propose a heterogeneous architecture that intelligently decouples these components across a cluster of AI accelerators. Our system features three core innovations: (1) an asymmetric resource allocation strategy that optimizes throughput under sequence parallelism constraints; (2) a memory-centric operator fusion scheme that minimizes off-chip bandwidth usage; and (3) a manifold-aware latent extrapolation mechanism that exploits temporal redundancy to mask latency. We validate our approach on a cluster of programmable AI accelerators, enabling real-time generation at $720 \times 480$ resolution -- a $50\times$ increase in pixel throughput over prior baselines. Evaluated on both continuous 3D racing and discrete 2D platformer benchmarks, our system delivers fluid 26.4 FPS and 48.3 FPS respectively, with an amortized effective latency of 2.7 ms. This work demonstrates that resolving the ``Memory Wall'' via architectural co-design is not merely an optimization, but a prerequisite for enabling high-fidelity, responsive neural gameplay.

</details>


### [39] [Structured Self-Consistency:A Multi-Task Evaluation of LLMs on VirtualHome](https://arxiv.org/abs/2602.00611)
*Jiaqi Xu,Tao Huang,Kai Zhang*

Main category: cs.AI

TL;DR: 本文通过VirtualHome基准和EAI框架，评估了OPENPANGU-7B和QWEN2.5-7B两个7B参数大语言模型在具身AI任务上的表现，并提出了结构化自洽解码策略提升性能。


<details>
  <summary>Details</summary>
Motivation: 具身AI需要智能体理解目标、规划动作并在模拟环境中执行任务。目前缺乏对大型语言模型在具身AI任务上的系统性评估，特别是针对不同模型在多层次任务上的表现比较。

Method: 使用VirtualHome基准和Embodied Agent Interface框架，评估两个7B参数模型在四个基本任务上的表现：目标解释、动作序列、子目标分解和状态转移建模。提出结构化自洽解码策略，通过多次采样和领域特定投票机制提升结构化生成任务的质量。

Result: 结构化自洽解码策略显著提升了模型性能。OPENPANGU-7B在层次化规划任务上表现优异，而QWEN2.5-7B在动作级任务上具有优势。实验揭示了不同模型类型的互补优势。

Conclusion: 研究为未来具身AI系统开发提供了重要见解，表明不同模型在具身AI任务上具有互补优势，结构化自洽解码策略能有效提升性能，为构建更强大的具身智能体提供了方法论基础。

Abstract: Embodied AI requires agents to understand goals, plan actions, and execute tasks in simulated environments. We present a comprehensive evaluation of Large Language Models (LLMs) on the VirtualHome benchmark using the Embodied Agent Interface (EAI) framework. We compare two representative 7B-parameter models OPENPANGU-7B and QWEN2.5-7B across four fundamental tasks: Goal Interpretation, Action Sequencing, Subgoal Decomposition, and Transition Modeling. We propose Structured Self-Consistency (SSC), an enhanced decoding strategy that leverages multiple sampling with domain-specific voting mechanisms to improve output quality for structured generation tasks. Experimental results demonstrate that SSC significantly enhances performance, with OPENPANGU-7B excelling at hierarchical planning while QWEN2.5-7B show advantages in action-level tasks. Our analysis reveals complementary strengths across model types, providing insights for future embodied AI system development.

</details>


### [40] [Engineering AI Agents for Clinical Workflows: A Case Study in Architecture,MLOps, and Governance](https://arxiv.org/abs/2602.00751)
*Cláudio Lúcio do Val Lopes,João Marcus Pitta,Fabiano Belém,Gildson Alves,Flávio Vinícius Cruzeiro Martins*

Main category: cs.AI

TL;DR: 本文提出Maria平台，一个用于初级医疗保健的生产级AI系统，通过整合四个工程支柱来解决临床AI中的责任真空问题。


<details>
  <summary>Details</summary>
Motivation: AI在临床环境中的集成面临软件工程挑战，原型衍生架构脆弱且缺乏系统监督，导致安全性和问责制受损的"责任真空"。

Method: 采用协同架构：Clean Architecture保证可维护性，事件驱动架构确保弹性和可审计性；以Agent作为主要模块化单元，每个拥有自主MLOps生命周期；将人在环治理模型技术集成作为关键事件驱动数据源。

Result: 开发了Maria平台，一个生产级AI系统，作为构建可维护、可扩展和可问责的高风险领域AI系统的参考架构。

Conclusion: 可信赖的临床AI需要通过四个工程支柱的整体集成实现，Maria平台为高风险领域构建可维护、可扩展和可问责的AI系统提供了实践经验和参考架构。

Abstract: The integration of Artificial Intelligence (AI) into clinical settings presents a software engineering challenge, demanding a shift from isolated models to robust, governable, and reliable systems. However, brittle, prototype-derived architectures often plague industrial applications and a lack of systemic oversight, creating a ``responsibility vacuum'' where safety and accountability are compromised. This paper presents an industry case study of the ``Maria'' platform, a production-grade AI system in primary healthcare that addresses this gap.
  Our central hypothesis is that trustworthy clinical AI is achieved through the holistic integration of four foundational engineering pillars. We present a synergistic architecture that combines Clean Architecture for maintainability with an Event-driven architecture for resilience and auditability. We introduce the Agent as the primary unit of modularity, each possessing its own autonomous MLOps lifecycle. Finally, we show how a Human-in-the-Loop governance model is technically integrated not merely as a safety check, but as a critical, event-driven data source for continuous improvement. We present the platform as a reference architecture, offering practical lessons for engineers building maintainable, scalable, and accountable AI-enabled systems in high-stakes domains.

</details>


### [41] [Predictive Maintenance for Ultrafiltration Membranes Using Explainable Similarity-Based Prognostics](https://arxiv.org/abs/2602.00659)
*Qusai Khaled,Laura Genga,Uzay Kaymak*

Main category: cs.AI

TL;DR: 提出基于模糊相似推理的可解释超滤膜剩余使用寿命预测框架，通过物理信息健康指标和模糊规则实现透明预测


<details>
  <summary>Details</summary>
Motivation: 反渗透海水淡化中超滤膜因污染而性能下降，现有预测维护模型缺乏可解释性，操作人员不信任，需要透明可靠的预测方法

Method: 使用基于跨膜压力、通量和阻力的物理信息健康指标，通过高斯隶属函数模糊化，采用相似度度量识别历史退化轨迹，构建Takagi-Sugeno模糊规则进行RUL预测

Result: 在工业规模UF系统的12,528个操作周期上测试，平均绝对误差为4.50个周期，生成与专家理解一致的可解释规则库

Conclusion: 提出的可解释预测框架在保持高精度的同时提供透明预测，增强操作人员信任，为超滤膜维护决策提供可靠支持

Abstract: In reverse osmosis desalination, ultrafiltration (UF) membranes degrade due to fouling, leading to performance loss and costly downtime. Most plants rely on scheduled preventive maintenance, since existing predictive maintenance models, often based on opaque machine learning methods, lack interpretability and operator trust. This study proposes an explainable prognostic framework for UF membrane remaining useful life (RUL) estimation using fuzzy similarity reasoning. A physics-informed Health Index, derived from transmembrane pressure, flux, and resistance, captures degradation dynamics, which are then fuzzified via Gaussian membership functions. Using a similarity measure, the model identifies historical degradation trajectories resembling the current state and formulates RUL predictions as Takagi-Sugeno fuzzy rules. Each rule corresponds to a historical exemplar and contributes to a transparent, similarity-weighted RUL estimate. Tested on 12,528 operational cycles from an industrial-scale UF system, the framework achieved a mean absolute error of 4.50 cycles, while generating interpretable rule bases consistent with expert understanding.

</details>


### [42] [Multi-Agent Causal Reasoning System for Error Pattern Rule Automation in Vehicles](https://arxiv.org/abs/2602.01155)
*Hugo Math,Julian Lorenz,Stefan Oelsner,Rainer Lienhart*

Main category: cs.AI

TL;DR: CAREP是一个多智能体系统，用于从诊断故障码(DTC)的高维事件序列中自动生成错误模式(EP)规则，取代传统手工制作方法，提高汽车故障诊断的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现代汽车产生数千种不同的诊断故障码(DTC)，汽车制造商使用这些代码的布尔组合（称为错误模式EP）来表征系统故障并确保车辆安全。然而，EP规则仍由领域专家手工制作，随着车辆复杂性的增加，这一过程既昂贵又容易出错。

Method: CAREP采用多智能体系统架构：1)因果发现智能体识别潜在的DTC-EP关系；2)上下文信息智能体整合元数据和描述；3)编排器智能体综合候选布尔规则并提供可解释的推理轨迹。

Result: 在包含29,100个独特DTC和474个错误模式的大规模汽车数据集上评估，CAREP能够自动准确地发现未知的EP规则，优于仅使用LLM的基线方法，同时提供透明的因果解释。

Conclusion: CAREP通过结合实用的因果发现和基于智能体的推理，向完全自动化的故障诊断迈出了一步，实现了可扩展、可解释且成本效益高的车辆维护。

Abstract: Modern vehicles generate thousands of different discrete events known as Diagnostic Trouble Codes (DTCs). Automotive manufacturers use Boolean combinations of these codes, called error patterns (EPs), to characterize system faults and ensure vehicle safety. Yet, EP rules are still manually handcrafted by domain experts, a process that is expensive and prone to errors as vehicle complexity grows. This paper introduces CAREP (Causal Automated Reasoning for Error Patterns), a multi-agent system that automatizes the generation of EP rules from high-dimensional event sequences of DTCs. CAREP combines a causal discovery agent that identifies potential DTC-EP relations, a contextual information agent that integrates metadata and descriptions, and an orchestrator agent that synthesizes candidate boolean rules together with interpretable reasoning traces. Evaluation on a large-scale automotive dataset with over 29,100 unique DTCs and 474 error patterns demonstrates that CAREP can automatically and accurately discover the unknown EP rules, outperforming LLM-only baselines while providing transparent causal explanations. By uniting practical causal discovery and agent-based reasoning, CAREP represents a step toward fully automated fault diagnostics, enabling scalable, interpretable, and cost-efficient vehicle maintenance.

</details>


### [43] [OpenGuanDan: A Large-Scale Imperfect Information Game Benchmark](https://arxiv.org/abs/2602.00676)
*Chao Li,Shangdong Yang,Chiheng Zhan,Zhenxing Ge,Yujing Hu,Bingkun Bao,Xingguo Chen,Yang Gao*

Main category: cs.AI

TL;DR: OpenGuanDan是一个新的AI基准测试平台，专注于中国流行的四人多轮纸牌游戏"掼蛋"，旨在为学习型和规则型AI智能体提供高效模拟和全面评估。


<details>
  <summary>Details</summary>
Motivation: 尽管AI在棋类、卡牌游戏等领域取得了显著进展，但仍需要更具挑战性的基准测试来推动进一步研究。掼蛋游戏具有不完美信息、大规模信息集和动作空间、合作与竞争混合目标、长时程决策等复杂特性，使其成为现有智能决策方法的严格测试平台。

Method: 开发了OpenGuanDan基准测试平台，支持掼蛋游戏的高效模拟和AI智能体评估。平台为每个玩家提供独立API，支持人机交互和大型语言模型集成。评估包括两类：所有掼蛋AI智能体之间的成对竞争，以及人机对战。

Result: 实验结果显示，当前基于学习的智能体显著优于基于规则的智能体，但仍未达到超人类水平。这表明在多智能体智能决策领域仍需继续研究。

Conclusion: OpenGuanDan作为一个具有挑战性的基准测试平台，能够推动多智能体智能决策领域的研究。该平台已公开可用，支持进一步的研究和开发。

Abstract: The advancement of data-driven artificial intelligence (AI), particularly machine learning, heavily depends on large-scale benchmarks. Despite remarkable progress across domains ranging from pattern recognition to intelligent decision-making in recent decades, exemplified by breakthroughs in board games, card games, and electronic sports games, there remains a pressing need for more challenging benchmarks to drive further research. To this end, this paper proposes OpenGuanDan, a novel benchmark that enables both efficient simulation of GuanDan (a popular four-player, multi-round Chinese card game) and comprehensive evaluation of both learning-based and rule-based GuanDan AI agents. OpenGuanDan poses a suite of nontrivial challenges, including imperfect information, large-scale information set and action spaces, a mixed learning objective involving cooperation and competition, long-horizon decision-making, variable action spaces, and dynamic team composition. These characteristics make it a demanding testbed for existing intelligent decision-making methods. Moreover, the independent API for each player allows human-AI interactions and supports integration with large language models. Empirically, we conduct two types of evaluations: (1) pairwise competitions among all GuanDan AI agents, and (2) human-AI matchups. Experimental results demonstrate that while current learning-based agents substantially outperform rule-based counterparts, they still fall short of achieving superhuman performance, underscoring the need for continued research in multi-agent intelligent decision-making domain. The project is publicly available at https://github.com/GameAI-NJUPT/OpenGuanDan.

</details>


### [44] [Agyn: A Multi-Agent System for Team-Based Autonomous Software Engineering](https://arxiv.org/abs/2602.01465)
*Nikita Benkovich,Vitalii Valkov*

Main category: cs.AI

TL;DR: 本文提出了一个完全自动化的多智能体系统，将软件工程建模为组织化流程，模拟工程团队结构，在SWE-bench 500上实现了72.4%的任务解决率，优于单智能体基线。


<details>
  <summary>Details</summary>
Motivation: 现有自主系统通常将问题解决视为单一或流水线过程，而现实软件开发是团队协作活动，具有明确的角色分工、沟通和评审。需要模拟真实工程团队的组织结构来提升自主软件工程能力。

Method: 基于agyn开源平台构建多智能体系统，分配协调、研究、实施、评审等专门角色，提供隔离沙箱进行实验，支持结构化通信。系统遵循定义好的开发方法论，包括分析、任务规范、拉取请求创建和迭代评审，完全无需人工干预。

Result: 在SWE-bench 500上评估，系统解决了72.4%的任务，优于使用可比语言模型的单智能体基线。系统设计用于实际生产使用，而非专门针对SWE-bench进行调优。

Conclusion: 模拟团队结构、方法论和沟通是自主软件工程的有力范式，未来进展可能同样依赖于组织设计和智能体基础设施的改进，而不仅仅是模型能力的提升。

Abstract: Large language models have demonstrated strong capabilities in individual software engineering tasks, yet most autonomous systems still treat issue resolution as a monolithic or pipeline-based process. In contrast, real-world software development is organized as a collaborative activity carried out by teams following shared methodologies, with clear role separation, communication, and review. In this work, we present a fully automated multi-agent system that explicitly models software engineering as an organizational process, replicating the structure of an engineering team. Built on top of agyn, an open-source platform for configuring agent teams, our system assigns specialized agents to roles such as coordination, research, implementation, and review, provides them with isolated sandboxes for experimentation, and enables structured communication. The system follows a defined development methodology for working on issues, including analysis, task specification, pull request creation, and iterative review, and operates without any human intervention. Importantly, the system was designed for real production use and was not tuned for SWE-bench. When evaluated post hoc on SWE-bench 500, it resolves 72.4% of tasks, outperforming single-agent baselines using comparable language models. Our results suggest that replicating team structure, methodology, and communication is a powerful paradigm for autonomous software engineering, and that future progress may depend as much on organizational design and agent infrastructure as on model improvements.

</details>


### [45] [From Prompt to Graph: Comparing LLM-Based Information Extraction Strategies in Domain-Specific Ontology Development](https://arxiv.org/abs/2602.00699)
*Xuan Liu,Ziyu Li,Mu He,Ziyang Ma,Xiaoxu Wu,Gizem Yilmaz,Yiyuan Xia,Bingbing Li,He Tan,Jerry Ying Hsi Fuh,Wen Feng Lu,Anders E. W. Jarfors,Per Jansson*

Main category: cs.AI

TL;DR: 本研究探索了三种基于大语言模型的方法（预训练LLM驱动、上下文学习和微调）从铸造制造领域文本中自动提取术语和关系，以解决传统本体构建劳动密集和成本高的问题。


<details>
  <summary>Details</summary>
Motivation: 传统本体构建依赖人工标注和传统NLP技术，过程劳动密集且成本高昂，特别是在铸造制造等专业领域。大语言模型的兴起为自动化知识提取提供了新的可能性。

Method: 研究比较了三种LLM方法：1）预训练LLM驱动方法；2）上下文学习方法；3）微调方法。这些方法使用有限数据从领域特定文本中提取术语和关系。

Result: 比较了三种方法的性能，并使用表现最佳的方法构建了铸造本体，该本体经过领域专家验证。

Conclusion: 大语言模型能够有效支持领域本体的自动化构建，特别是在数据有限的专业领域，为传统劳动密集型本体构建方法提供了可行的替代方案。

Abstract: Ontologies are essential for structuring domain knowledge, improving accessibility, sharing, and reuse. However, traditional ontology construction relies on manual annotation and conventional natural language processing (NLP) techniques, making the process labour-intensive and costly, especially in specialised fields like casting manufacturing. The rise of Large Language Models (LLMs) offers new possibilities for automating knowledge extraction. This study investigates three LLM-based approaches, including pre-trained LLM-driven method, in-context learning (ICL) method and fine-tuning method to extract terms and relations from domain-specific texts using limited data. We compare their performances and use the best-performing method to build a casting ontology that validated by domian expert.

</details>


### [46] [Canonical Intermediate Representation for LLM-based optimization problem formulation and code generation](https://arxiv.org/abs/2602.02029)
*Zhongyuan Lyu,Shuoyu Hu,Lujie Liu,Hongxia Yang,Ming LI*

Main category: cs.AI

TL;DR: 该论文提出了CIR（规范中间表示）和R2C框架，用于从自然语言描述自动生成优化模型，解决了现有LLM方法在处理复杂操作规则时的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的方法在处理复杂操作规则时面临挑战，难以处理复合约束和选择合适的建模范式，需要一种更系统的方法来自动化优化模型构建。

Method: 提出CIR作为中间表示，通过约束原型和候选建模范式编码操作规则语义；开发R2C多智能体管道，包括解析问题文本、检索领域知识合成CIR实现、实例化优化模型；采用反思机制进一步提升性能。

Result: R2C在新构建的基准测试中达到47.2%的准确率，在现有基准测试中表现优异，接近GPT-5等专有模型性能；通过反思机制在某些基准测试中创造了新的最佳记录。

Conclusion: CIR和R2C框架为从自然语言描述自动生成优化模型提供了有效解决方案，显著提升了复杂操作规则的处理能力，在多个基准测试中展现了卓越性能。

Abstract: Automatically formulating optimization models from natural language descriptions is a growing focus in operations research, yet current LLM-based approaches struggle with the composite constraints and appropriate modeling paradigms required by complex operational rules. To address this, we introduce the Canonical Intermediate Representation (CIR): a schema that LLMs explicitly generate between problem descriptions and optimization models. CIR encodes the semantics of operational rules through constraint archetypes and candidate modeling paradigms, thereby decoupling rule logic from its mathematical instantiation. Upon a newly generated CIR knowledge base, we develop the rule-to-constraint (R2C) framework, a multi-agent pipeline that parses problem texts, synthesizes CIR implementations by retrieving domain knowledge, and instantiates optimization models. To systematically evaluate rule-to-constraint reasoning, we test R2C on our newly constructed benchmark featuring rich operational rules, and benchmarks from prior work. Extensive experiments show that R2C achieves state-of-the-art accuracy on the proposed benchmark (47.2% Accuracy Rate). On established benchmarks from the literature, R2C delivers highly competitive results, approaching the performance of proprietary models (e.g., GPT-5). Moreover, with a reflection mechanism, R2C achieves further gains and sets new best-reported results on some benchmarks.

</details>


### [47] [Self-Guard: Defending Large Reasoning Models via enhanced self-reflection](https://arxiv.org/abs/2602.00707)
*Jingnan Zheng,Jingjun Xu,Yanzhen Luo,Chenhang Cui,Gelei Deng,Zhenkai Liang,Xiang Wang,An Zhang,Tat-Seng Chua*

Main category: cs.AI

TL;DR: Self-Guard：一个轻量级安全防御框架，通过在表示层面强化安全合规性来解决大型推理模型中的意识-合规差距问题，无需大量后训练或外部干预。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）虽然带来了显著的推理能力提升，但也引入了推理操纵和信息泄露等独特风险。现有的对齐策略主要依赖计算密集的后训练范式或外部干预，这些方法不仅计算成本高，而且未能解决模型识别风险但仍优先遵循用户指令的"意识-合规差距"问题。

Method: Self-Guard框架包含两个主要阶段：1）安全导向提示：激活模型潜在的安全意识以引发自发反思；2）安全激活引导：提取隐藏状态空间中的方向性变化并放大，确保在推理过程中安全合规性优先于迎合倾向。

Result: 实验表明Self-Guard能有效弥合意识-合规差距，在不损害模型实用性的情况下实现鲁棒的安全性能。此外，该框架在未见风险和不同模型规模上表现出良好的泛化能力。

Conclusion: Self-Guard为LRM安全对齐提供了一个成本高效的解决方案，通过轻量级方法在表示层面强化安全合规性，解决了现有方法计算密集且无法解决意识-合规差距的局限性。

Abstract: The emergence of Large Reasoning Models (LRMs) introduces a new paradigm of explicit reasoning, enabling remarkable advances yet posing unique risks such as reasoning manipulation and information leakage. To mitigate these risks, current alignment strategies predominantly rely on heavy post-training paradigms or external interventions. However, these approaches are often computationally intensive and fail to address the inherent awareness-compliance gap, a critical misalignment where models recognize potential risks yet prioritize following user instructions due to their sycophantic tendencies. To address these limitations, we propose Self-Guard, a lightweight safety defense framework that reinforces safety compliance at the representational level. Self-Guard operates through two principal stages: (1) safety-oriented prompting, which activates the model's latent safety awareness to evoke spontaneous reflection, and (2) safety activation steering, which extracts the resulting directional shift in the hidden state space and amplifies it to ensure that safety compliance prevails over sycophancy during inference. Experiments demonstrate that Self-Guard effectively bridges the awareness-compliance gap, achieving robust safety performance without compromising model utility. Furthermore, Self-Guard exhibits strong generalization across diverse unseen risks and varying model scales, offering a cost-efficient solution for LRM safety alignment.

</details>


### [48] [Physics-informed Diffusion Generation for Geomagnetic Map Interpolation](https://arxiv.org/abs/2602.00709)
*Wenda Li,Tongya Zheng,Kaixuan Chen,Shunyu Liu,Haoze Jiang,Yunzhi Hao,Rui Miao,Zujie Ren,Mingli Song,Hang Shi,Gang Chen*

Main category: cs.AI

TL;DR: 提出PDG框架，通过物理信息引导的扩散生成模型进行地磁地图插值，结合局部感受野和克里金原理约束，有效处理噪声并保证物理一致性。


<details>
  <summary>Details</summary>
Motivation: 现有散点数据插值方法未专门针对地磁地图设计，受检测噪声和物理规律影响导致性能不佳，需要专门的地磁地图插值方法。

Method: 提出物理信息扩散生成框架(PDG)：1) 基于局部感受野设计物理信息掩码策略指导扩散生成过程，消除噪声干扰；2) 遵循地磁地图克里金原理对扩散生成结果施加物理信息约束，确保严格遵循物理规律。

Result: 在四个真实世界数据集上的广泛实验和深入分析证明了PDG的优越性和各组成部分的有效性。

Conclusion: PDG框架通过物理信息引导的扩散生成方法，能够有效处理地磁地图插值问题，在消除噪声干扰的同时保证物理一致性，为导航和资源勘探提供重要支持。

Abstract: Geomagnetic map interpolation aims to infer unobserved geomagnetic data at spatial points, yielding critical applications in navigation and resource exploration. However, existing methods for scattered data interpolation are not specifically designed for geomagnetic maps, which inevitably leads to suboptimal performance due to detection noise and the laws of physics. Therefore, we propose a Physics-informed Diffusion Generation framework~(PDG) to interpolate incomplete geomagnetic maps. First, we design a physics-informed mask strategy to guide the diffusion generation process based on a local receptive field, effectively eliminating noise interference. Second, we impose a physics-informed constraint on the diffusion generation results following the kriging principle of geomagnetic maps, ensuring strict adherence to the laws of physics. Extensive experiments and in-depth analyses on four real-world datasets demonstrate the superiority and effectiveness of each component of PDG.

</details>


### [49] [SafeGround: Know When to Trust GUI Grounding Models via Uncertainty Calibration](https://arxiv.org/abs/2602.02419)
*Qingni Wang,Yue Fan,Xin Eric Wang*

Main category: cs.AI

TL;DR: SafeGround是一个不确定性感知的GUI grounding框架，通过分布感知的不确定性量化方法和校准过程，实现风险感知预测和统计保证的误发现率控制。


<details>
  <summary>Details</summary>
Motivation: GUI grounding模型将自然语言指令转换为可执行的屏幕坐标，但错误的grounding可能导致代价高昂且难以逆转的操作（如错误的支付批准），因此需要提高模型可靠性。

Method: SafeGround采用分布感知的不确定性量化方法捕捉模型输出的空间分散性，通过校准过程推导出具有统计保证误发现率控制的测试时决策阈值。

Result: 在ScreenSpot-Pro基准测试中，SafeGround的不确定性度量在区分正确与错误预测方面优于现有基线，校准阈值实现了严格的风险控制，系统级准确率相比Gemini-only推理提升高达5.38%。

Conclusion: SafeGround为GUI grounding模型提供了有效的风险感知预测框架，通过不确定性量化和统计校准实现了可靠的风险控制，显著提升了系统级准确率。

Abstract: Graphical User Interface (GUI) grounding aims to translate natural language instructions into executable screen coordinates, enabling automated GUI interaction. Nevertheless, incorrect grounding can result in costly, hard-to-reverse actions (e.g., erroneous payment approvals), raising concerns about model reliability. In this paper, we introduce SafeGround, an uncertainty-aware framework for GUI grounding models that enables risk-aware predictions through calibrations before testing. SafeGround leverages a distribution-aware uncertainty quantification method to capture the spatial dispersion of stochastic samples from outputs of any given model. Then, through the calibration process, SafeGround derives a test-time decision threshold with statistically guaranteed false discovery rate (FDR) control. We apply SafeGround on multiple GUI grounding models for the challenging ScreenSpot-Pro benchmark. Experimental results show that our uncertainty measure consistently outperforms existing baselines in distinguishing correct from incorrect predictions, while the calibrated threshold reliably enables rigorous risk control and potentials of substantial system-level accuracy improvements. Across multiple GUI grounding models, SafeGround improves system-level accuracy by up to 5.38\% percentage points over Gemini-only inference.

</details>


### [50] [Learning More from Less: Unlocking Internal Representations for Benchmark Compression](https://arxiv.org/abs/2602.00710)
*Yueqi Zhang,Jin Hu,Shaoxiong Feng,Peiwen Yuan,Xinglin Wang,Yiwei Li,Jiayi Shi,Chuyi Tan,Ji Zhang,Boyuan Pan,Yao Hu,Kan Li*

Main category: cs.AI

TL;DR: REPCORE通过将异构隐藏状态对齐到统一潜在空间来构建代表性核心集，仅需10个源模型即可精确估计LLM性能，优于基于输出的基线方法。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型成本高昂，需要高效的基准测试替代方案。现有方法依赖大量源模型来估计可靠的项配置文件，这在源模型池较小时统计不稳定，特别是对于新发布的基准测试缺乏历史评估数据。此外，离散正确性标签是模型决策过程的损失性视图，无法捕捉隐藏状态中的信息。

Method: 提出REPCORE方法，将异构隐藏状态对齐到统一的潜在空间中，构建代表性核心集。通过使用这些子集进行性能外推，仅需少量源模型即可实现精确估计。

Result: 在五个基准测试和超过200个模型上的实验表明，REPCORE在排名相关性和估计准确性方面始终优于基于输出的基线方法。谱分析进一步表明，对齐表示包含可分离的组件，反映了广泛的响应倾向和任务特定的推理模式。

Conclusion: REPCORE通过利用隐藏状态信息而非仅依赖输出标签，解决了小源模型池下的统计不稳定问题，为高效评估大语言模型提供了有效解决方案，特别适用于新发布的基准测试。

Abstract: The prohibitive cost of evaluating Large Language Models (LLMs) necessitates efficient alternatives to full-scale benchmarking. Prevalent approaches address this by identifying a small coreset of items to approximate full-benchmark performance. However, existing methods must estimate a reliable item profile from response patterns across many source models, which becomes statistically unstable when the source pool is small. This dependency is particularly limiting for newly released benchmarks with minimal historical evaluation data. We argue that discrete correctness labels are a lossy view of the model's decision process and fail to capture information encoded in hidden states. To address this, we introduce REPCORE, which aligns heterogeneous hidden states into a unified latent space to construct representative coresets. Using these subsets for performance extrapolation, REPCORE achieves precise estimation accuracy with as few as ten source models. Experiments on five benchmarks and over 200 models show consistent gains over output-based baselines in ranking correlation and estimation accuracy. Spectral analysis further indicates that the aligned representations contain separable components reflecting broad response tendencies and task-specific reasoning patterns.

</details>


### [51] [Neuro-symbolic AI for Predictive Maintenance (PdM) -- review and recommendations](https://arxiv.org/abs/2602.00731)
*Kyle Hamilton,Ali Intizar*

Main category: cs.AI

TL;DR: 本文对过去五年工业环境中预测性维护（PdM）的最新进展进行了系统性综述，提出将深度学习与符号逻辑结合的神经符号AI作为解决当前数据驱动方法局限性的新方向。


<details>
  <summary>Details</summary>
Motivation: 当前预测性维护领域存在两大问题：数据驱动方法（如深度学习）虽然准确率高，但需要大量标注数据、泛化能力差、缺乏可解释性；而基于领域知识的传统方法准确率低、误报多、需要专家持续监督。需要一种能结合两者优势的新方法。

Method: 采用系统性文献综述方法，分析过去五年工业环境中预测性维护的最新研究。重点关注使用传感器数据和人工规则作为输入的神经符号AI架构，描述具体的神经符号系统设计。

Result: 研究发现数据驱动方法通常比传统知识系统更准确，但存在标注数据需求大、泛化能力差、缺乏透明度等局限性。混合系统（结合数据驱动和领域知识）显示出克服单一方法弱点的潜力。神经符号AI被提出作为更准确、可解释、鲁棒的解决方案。

Conclusion: 神经符号AI通过整合深度学习与符号逻辑，有望创建更准确、可解释、可解释且鲁棒的预测性维护系统。本文建立了通用框架，回顾了当前建模方法和挑战，并提出了神经符号AI作为未来研究重点。

Abstract: In this document we perform a systematic review the State-of-the-art in Predictive Maintenance (PdM) over the last five years in industrial settings such as commercial buildings, pharmaceutical facilities, or semi-conductor manufacturing. In general, data-driven methods such as those based on deep learning, exhibit higher accuracy than traditional knowledge-based systems. These systems however, are not without significant limitations. The need for large labeled data sets, a lack of generalizibility to new environments (out-of-distribution generalization), and a lack of transparency at inference time are some of the obstacles to adoption in real world environments. In contrast, traditional approaches based on domain expertise in the form of rules, logic or first principles suffer from poor accuracy, many false positives and a need for ongoing expert supervision and manual tuning. While the majority of approaches in recent literature utilize some form of data-driven architecture, there are hybrid systems which also take into account domain specific knowledge. Such hybrid systems have the potential to overcome the weaknesses of either approach on its own while preserving their strengths. We propose taking the hybrid approach even further and integrating deep learning with symbolic logic, or Neuro-symbolic AI, to create more accurate, explainable, interpretable, and robust systems. We describe several neuro-symbolic architectures and examine their strengths and limitations within the PdM domain. We focus specifically on methods which involve the use of sensor data and manually crafted rules as inputs by describing concrete NeSy architectures. In short, this survey outlines the context of modern maintenance, defines key concepts, establishes a generalized framework, reviews current modeling approaches and challenges, and introduces the proposed focus on Neuro-symbolic AI (NESY).

</details>


### [52] [Environment-Aware Adaptive Pruning with Interleaved Inference Orchestration for Vision-Language-Action Models](https://arxiv.org/abs/2602.00780)
*Yuting Huang,Leilei Ding,Zhipeng Tang,Zenghuan Zhu,Jiajun Deng,Xinrui Lin,Shuo Liu,Haojie Ren,Jianmin Ji,Yanyong Zhang*

Main category: cs.AI

TL;DR: EcoVLA是一个无需训练、即插即用的自适应剪枝框架，通过环境感知自适应剪枝和交错推理编排，在VLA模型中实现动态参数稀疏化，显著提升推理速度且性能损失极小。


<details>
  <summary>Details</summary>
Motivation: 视觉-语言-动作模型参数庞大导致推理延迟高，阻碍实时操作。静态剪枝无法适应环境动态变化，固定间隔的动态层剪枝粒度粗且重训练开销大，需要自适应剪枝方案。

Method: 提出EcoVLA框架，包含两个组件：1) 环境感知自适应剪枝(EAP)：轻量级自适应通道剪枝方法，利用物理环境的时间一致性更新稀疏模式；2) 交错推理编排(I²O)：利用VLA推理中的FLOPs气泡并行调度剪枝方法，对延迟影响可忽略。

Result: 在多种VLA模型和基准测试中，EcoVLA实现最先进性能：单独使用可达1.60倍加速且成功率仅下降0.4%；与token剪枝结合可达2.18倍加速且性能仅下降0.5%。在真实机器人上验证了有效性。

Conclusion: EcoVLA是一个高效的自适应剪枝框架，能够动态适应环境变化，显著加速VLA模型推理，且与现有加速方法正交兼容，为实时机器人操作提供了实用解决方案。

Abstract: While Vision-Language-Action (VLA) models hold promise in embodied intelligence, their large parameter counts lead to substantial inference latency that hinders real-time manipulation, motivating parameter sparsification. However, as the environment evolves during VLA execution, the optimal sparsity patterns change accordingly. Static pruning lacks the adaptability required for environment dynamics, whereas fixed-interval dynamic layer pruning suffers from coarse granularity and high retraining overheads. To bridge this gap, we propose EcoVLA, a training-free, plug-and-play adaptive pruning framework that supports orthogonal combination with existing VLA acceleration methods. EcoVLA comprises two components: Environment-aware Adaptive Pruning (EAP) and Interleaved Inference Orchestration ($I^2O$). EAP is a lightweight adaptive channel pruning method that incorporates the temporal consistency of the physical environment to update sparsity patterns. $I^2O$ leverages the FLOPs bubbles inherent in VLA inference to schedule the pruning method in parallel, ensuring negligible impact on latency. Evaluated on diverse VLA models and benchmarks, EcoVLA delivers state-of-the-art performance, achieving up to 1.60$\times$ speedup with only a 0.4% drop in success rate, and further reaches 2.18$\times$ speedup with only a 0.5% degradation when combined with token pruning. We further validate the effectiveness of EcoVLA on real-world robots.

</details>


### [53] [World Models as an Intermediary between Agents and the Real World](https://arxiv.org/abs/2602.00785)
*Sherry Yang*

Main category: cs.AI

TL;DR: 本文主张在复杂高成本领域使用世界模型作为智能体与真实世界的中介，以解决行动执行成本高昂的瓶颈问题，并探讨了世界模型在机器学习工程、计算机使用、机器人和AI科学等领域的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 尽管强化学习训练的LLM智能体在游戏、数学和编码等低成本环境中表现出色，但在机器人运行、ML工程时间和科学实验资源等高成本复杂领域中未能取得类似成功。真正的瓶颈在于执行行动获取奖励信号的成本过高。

Method: 提出使用世界模型作为智能体与真实世界的中介，将世界模型视为动态、奖励和任务分布的模型。这种方法可以克服高成本行动的基本障碍，如极端离策略学习和长时程任务中的样本效率低下问题。

Result: 世界模型能够为智能体提供关键且丰富的学习信号，适用于机器学习工程、计算机使用、机器人和AI科学等多个领域。同时识别了构建这些世界模型面临的挑战。

Conclusion: 为了在复杂高成本领域实现下一代智能体性能，需要在数据集管理、架构设计、扩展和评估等方面采取具体行动来构建有效的世界模型，作为连接智能体与真实世界的桥梁。

Abstract: Large language model (LLM) agents trained using reinforcement learning has achieved superhuman performance in low-cost environments like games, mathematics, and coding. However, these successes have not translated to complex domains where the cost of interaction is high, such as the physical cost of running robots, the time cost of ML engineering, and the resource cost of scientific experiments. The true bottleneck for achieving the next level of agent performance for these complex and high-cost domains lies in the expense of executing actions to acquire reward signals. To address this gap, this paper argues that we should use world models as an intermediary between agents and the real world. We discuss how world models, viewed as models of dynamics, rewards, and task distributions, can overcome fundamental barriers of high-cost actions such as extreme off-policy learning and sample inefficiency in long-horizon tasks. Moreover, we demonstrate how world models can provide critical and rich learning signals to agents across a broad set of domains, including machine learning engineering, computer use, robotics, and AI for science. Lastly, we identify the challenges of building these world models and propose actionable items along dataset curation, architecture design, scaling, and evaluation of world models.

</details>


### [54] [MissMAC-Bench: Building Solid Benchmark for Missing Modality Issue in Robust Multimodal Affective Computing](https://arxiv.org/abs/2602.00811)
*Ronghao Lin,Honghao Lu,Ruixing Wu,Aolin Xiong,Qinggong Chu,Qiaolin He,Sijie Mai,Haifeng Hu*

Main category: cs.AI

TL;DR: 本文提出了MissMAC-Bench基准，用于系统评估多模态情感计算中的缺失模态问题，通过统一评估标准和跨模态协同视角来解决实际应用中模态数据不完整带来的性能波动问题。


<details>
  <summary>Details</summary>
Motivation: 现实场景中多模态数据的可用性往往是动态和不确定的，由于分布偏移和语义缺陷，不完整的多模态输入会导致性能大幅波动。这种缺失模态问题是多模态情感计算模型鲁棒性和实际部署的关键障碍。

Method: 提出了MissMAC-Bench基准，建立公平统一的评估标准，基于跨模态协同视角。提出两个指导原则：训练时不使用缺失先验，单一模型能同时处理完整和不完整模态场景。基准集成了数据集和实例级别的固定和随机缺失模式评估协议。

Result: 在4个数据集上对3个广泛使用的语言模型进行了广泛实验，验证了不同MAC方法在解决缺失模态问题上的有效性。基准为推进鲁棒的多模态情感计算提供了坚实基础。

Conclusion: MissMAC-Bench基准填补了学术研究与实际应用之间的差距，促进了多媒体数据挖掘的发展，为解决多模态情感计算中的缺失模态问题提供了系统化的评估框架。

Abstract: As a knowledge discovery task over heterogeneous data sources, current Multimodal Affective Computing (MAC) heavily rely on the completeness of multiple modalities to accurately understand human's affective state. However, in real-world scenarios, the availability of modality data is often dynamic and uncertain, leading to substantial performance fluctuations due to the distribution shifts and semantic deficiencies of the incomplete multimodal inputs. Known as the missing modality issue, this challenge poses a critical barrier to the robustness and practical deployment of MAC models. To systematically quantify this issue, we introduce MissMAC-Bench, a comprehensive benchmark designed to establish fair and unified evaluation standards from the perspective of cross-modal synergy. Two guiding principles are proposed, including no missing prior during training, and one single model capable of handling both complete and incomplete modality scenarios, thereby ensuring better generalization. Moreover, to bridge the gap between academic research and real-world applications, our benchmark integrates evaluation protocols with both fixed and random missing patterns at the dataset and instance levels. Extensive experiments conducted on 3 widely-used language models across 4 datasets validate the effectiveness of diverse MAC approaches in tackling the missing modality issue. Our benchmark provides a solid foundation for advancing robust multimodal affective computing and promotes the development of multimedia data mining.

</details>


### [55] [Resource-Efficient Reinforcement for Reasoning Large Language Models via Dynamic One-Shot Policy Refinement](https://arxiv.org/abs/2602.00815)
*Yunjian Zhang,Sudong Wang,Yang Li,Peiran Xu,Conghao Zhou,Xiaoyue Ma,Jianing Li,Yao Zhu*

Main category: cs.AI

TL;DR: 论文提出DoPR方法，通过动态选择单一样本进行策略更新，大幅降低RLVR训练的计算成本，同时保持推理性能。


<details>
  <summary>Details</summary>
Motivation: 尽管基于可验证奖励的强化学习（RLVR）在LLM推理对齐方面表现出色，但其训练过程需要大量奖励信号和计算资源，成本过高，限制了实际应用。

Method: 提出动态单次策略优化（DoPR）方法：1）建立理论下界分析样本复杂度；2）基于奖励波动性和探索驱动的获取策略，动态选择每个批次中最具信息量的单个训练样本进行策略更新。

Result: DoPR将训练过程中的rollout开销降低了近一个数量级，同时保持了具有竞争力的推理准确性，为LLM后训练提供了可扩展且资源高效的解决方案。

Conclusion: 该方法为推理密集型LLM应用提供了更高效、更易获取的基于强化学习的训练路径，证明了通过智能样本选择可以显著降低计算成本而不牺牲性能。

Abstract: Large language models (LLMs) have exhibited remarkable performance on complex reasoning tasks, with reinforcement learning under verifiable rewards (RLVR) emerging as a principled framework for aligning model behavior with reasoning chains. Despite its promise, RLVR remains prohibitively resource-intensive, requiring extensive reward signals and incurring substantial rollout costs during training. In this work, we revisit the fundamental question of data and compute efficiency in RLVR. We first establish a theoretical lower bound on the sample complexity required to unlock reasoning capabilities, and empirically validate that strong performance can be achieved with a surprisingly small number of training instances. To tackle the computational burden, we propose Dynamic One-Shot Policy Refinement (DoPR), an uncertainty-aware RL strategy that dynamically selects a single informative training sample per batch for policy updates, guided by reward volatility and exploration-driven acquisition. DoPR reduces rollout overhead by nearly an order of magnitude while preserving competitive reasoning accuracy, offering a scalable and resource-efficient solution for LLM post-training. This approach offers a practical path toward more efficient and accessible RL-based training for reasoning-intensive LLM applications.

</details>


### [56] [Position: Human-Centric AI Requires a Minimum Viable Level of Human Understanding](https://arxiv.org/abs/2602.00854)
*Fangzhou Lin,Qianwen Ge,Lingyu Xu,Peiran Li,Xiangbo Gao,Shuo Xing,Kazunori Yamada,Ziming Zhang,Haichong Zhang,Zhengzhong Tu*

Main category: cs.AI

TL;DR: 论文提出"能力-理解差距"概念，即AI系统能力提升时用户理解能力下降，并定义"认知完整性阈值"作为保持监督所需的最低理解水平。


<details>
  <summary>Details</summary>
Motivation: AI系统越来越能产生流畅、正确的端到端结果，这逐渐侵蚀了用户解释、验证或干预的能力。这种能力与理解的脱节需要被正视，因为现有的透明度、用户控制、素养和治理方法未能定义人类在持续AI委托下保持监督所需的基础理解。

Method: 论文提出"认知完整性阈值"概念，定义为在AI协助下保持监督、自主性和问责参与所需的最低理解水平。通过三个功能维度来操作化：验证能力、理解保持型交互、以及治理的制度支架。

Result: 提出了一个设计和治理议程，旨在将人机交互与责任关键环境中的认知可持续性对齐。认知完整性阈值不需要完全推理重建，也不限制自动化，而是识别监督变得程序化且可争议性失效的阈值。

Conclusion: 需要重新思考AI系统的设计和治理，确保在AI协助下人类能够保持足够的理解水平来维持有效的监督和问责，特别是在责任关键的环境中。

Abstract: AI systems increasingly produce fluent, correct, end-to-end outcomes. Over time, this erodes users' ability to explain, verify, or intervene. We define this divergence as the Capability-Comprehension Gap: a decoupling where assisted performance improves while users' internal models deteriorate. This paper argues that prevailing approaches to transparency, user control, literacy, and governance do not define the foundational understanding humans must retain for oversight under sustained AI delegation. To formalize this, we define the Cognitive Integrity Threshold (CIT) as the minimum comprehension required to preserve oversight, autonomy, and accountable participation under AI assistance. CIT does not require full reasoning reconstruction, nor does it constrain automation. It identifies the threshold beyond which oversight becomes procedural and contestability fails. We operatinalize CIT through three functional dimensions: (i) verification capacity, (ii) comprehension-preserving interaction, and (iii) institutional scaffolds for governance. This motivates a design and governance agenda that aligns human-AI interaction with cognitive sustainability in responsibility-critical settings.

</details>


### [57] [Foundation CAN LM: A Pretrained Language Model For Automotive CAN Data](https://arxiv.org/abs/2602.00866)
*Akiharu Esashi,Pawissanutt Lertpongrujikorn,Justin Makino,Yuibi Fujimoto,Mohsen Amini Salehi*

Main category: cs.AI

TL;DR: 本文提出了首个CAN总线基础模型，将自然语言处理中的基础模型范式应用于汽车CAN数据，通过大规模预训练和任务特定微调，实现跨多个汽车保险任务的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前CAN总线数据处理存在任务特定模型孤立训练的问题，缺乏共享表示学习和跨任务泛化能力。相比之下，NLP和CV领域通过基础模型范式取得了巨大成功，作者希望将这一范式应用于CAN数据领域。

Method: 将CAN数据视为语言进行处理：1）提出统一的分词方案处理混合离散-连续信号；2）在大规模未标记解码CAN信号上进行预训练；3）针对时间复杂性和行程特定变异性等挑战提出解决方案；4）在异构汽车保险任务上进行微调。

Result: 实验结果表明，一个预训练的CAN模型能够有效适应多样化的预测任务，验证了基础模型范式在CAN数据领域的适用性，为汽车AI中的可泛化表示学习开辟了新方向。

Conclusion: 本研究成功将NLP和CV中证明有效的基础模型范式应用于CAN数据，建立了汽车AI中可泛化表示学习的新方向，为汽车保险等领域的多任务应用提供了统一解决方案。

Abstract: The Controller Area Network (CAN) bus provides a rich source of vehicular signals increasingly leveraged for applications in automotive and auto insurance domains, including collision detection, predictive maintenance, and driver risk modeling. Despite this potential, existing pipelines largely train isolated task-specific models on raw CAN data, with only limited efforts exploring decoded signals. Such fragmentation prevents shared representation learning and limits cross-task generalization. By contrast, natural language processing (NLP) and computer vision (CV) have been transformed by the foundation model paradigm: large-scale pretraining followed by task-specific adaptation. In this work, we introduce the foundation CAN model that demonstrates multi-objective downstream generalization using a single pretrained backbone. Our approach treats CAN data as a language: we pretrain on large-scale, unlabeled decoded CAN signals and fine-tune across heterogeneous auto insurance tasks. To enable this, we propose a unified tokenization scheme for mixed discrete-continuous signals and address challenges of temporal complexity and trip-specific variability. Our results show that one pretrained CAN model can adapt effectively to diverse predictive tasks, validating that the foundation modeling paradigm, proven in NLP and CV, also holds for CAN data. This establishes a new direction for generalizable representation learning in automotive AI.

</details>


### [58] [Beyond Output Critique: Self-Correction via Task Distillation](https://arxiv.org/abs/2602.00871)
*Hossein A. Rahmani,Mengting Wan,Pei Zhou,Longqi Yang,Nick Craswell,Emine Yilmaz,Sujay Kumar Jauhar*

Main category: cs.AI

TL;DR: SELF-THOUGHT框架通过任务抽象提升LLM自我修正能力，将任务提炼为结构化模板，指导解决方案实例化，并支持跨模型模板转移。


<details>
  <summary>Details</summary>
Motivation: 现有LLM自我修正方法主要在输出层面进行批判性修正，只能修补表面错误，难以纠正深层次推理缺陷。需要一种能理解任务本质结构的方法来提升修正效果。

Method: 提出SELF-THOUGHT框架：1）任务抽象步骤：将输入和初始响应提炼为结构化模板，捕捉关键变量、约束和问题结构；2）解决方案实例化：基于抽象模板生成修正后的响应；3）跨模型模板转移：大模型生成的模板可指导小模型进行更可靠的修正。

Result: 实验表明，SELF-THOUGHT在多种推理任务中提高了模型的准确性、鲁棒性和泛化能力，使小模型无需大量微调或依赖外部验证器就能实现更可靠的修正。

Conclusion: SELF-THOUGHT通过任务抽象和跨模型模板转移，为构建更可靠的自修正语言系统提供了可扩展的路径，显著提升了LLM的自我修正能力。

Abstract: Large language models (LLMs) have shown promising self-correction abilities, where iterative refinement improves the quality of generated responses. However, most existing approaches operate at the level of output critique, patching surface errors while often failing to correct deeper reasoning flaws. We propose SELF-THOUGHT, a framework that introduces an intermediate step of task abstraction before solution refinement. Given an input and an initial response, the model first distills the task into a structured template that captures key variables, constraints, and problem structure. This abstraction then guides solution instantiation, grounding subsequent responses in a clearer understanding of the task and reducing error propagation. Crucially, we show that these abstractions can be transferred across models: templates generated by larger models can serve as structured guides for smaller LLMs, which typically struggle with intrinsic self-correction. By reusing distilled task structures, smaller models achieve more reliable refinements without heavy fine-tuning or reliance on external verifiers. Experiments across diverse reasoning tasks demonstrate that SELF-THOUGHT improves accuracy, robustness, and generalization for both large and small models, offering a scalable path toward more reliable self-correcting language systems.

</details>


### [59] [Supervised sparse auto-encoders as unconstrained feature models for semantic composition](https://arxiv.org/abs/2602.00924)
*Ouns El Harzli,Hugo Wallner,Yoonsoo Nam,Haixuan Xavier Tao*

Main category: cs.AI

TL;DR: 该论文提出了一种改进的稀疏自编码器方法，通过结合无约束特征模型和监督任务来解决传统SAE在重建、可扩展性和语义对齐方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 传统稀疏自编码器面临两个主要问题：1) L1惩罚项的非平滑性阻碍了重建和可扩展性；2) 学习到的特征与人类语义缺乏对齐。需要解决这些限制以提升SAE在机制可解释性中的应用效果。

Method: 方法结合了神经崩溃理论中的无约束特征模型框架，并引入监督任务。具体包括：监督（仅解码器）SAE重建特征向量，联合学习稀疏概念嵌入和解码器权重。在Stable Diffusion 3.5上进行验证。

Result: 该方法展示了组合泛化能力，能够成功重建训练中未见过的概念组合图像，并实现特征级干预以进行语义图像编辑，无需修改提示词。

Conclusion: 通过结合无约束特征模型和监督任务，提出的方法有效解决了传统SAE的局限性，在组合泛化和语义图像编辑方面表现出色，为机制可解释性提供了更有效的工具。

Abstract: Sparse auto-encoders (SAEs) have re-emerged as a prominent method for mechanistic interpretability, yet they face two significant challenges: the non-smoothness of the $L_1$ penalty, which hinders reconstruction and scalability, and a lack of alignment between learned features and human semantics. In this paper, we address these limitations by adapting unconstrained feature models-a mathematical framework from neural collapse theory-and by supervising the task. We supervise (decoder-only) SAEs to reconstruct feature vectors by jointly learning sparse concept embeddings and decoder weights. Validated on Stable Diffusion 3.5, our approach demonstrates compositional generalization, successfully reconstructing images with concept combinations unseen during training, and enabling feature-level intervention for semantic image editing without prompt modification.

</details>


### [60] [The Keyhole Effect: Why Chat Interfaces Fail at Data Analysis](https://arxiv.org/abs/2602.00947)
*Mohan Reddy*

Main category: cs.AI

TL;DR: 聊天界面不适合多步骤数据分析任务，会导致认知过载，论文提出八个混合设计模式来解决这些问题


<details>
  <summary>Details</summary>
Motivation: 当前AI辅助数据分析普遍采用聊天界面，但对于多步骤、状态依赖的分析任务，这种界面设计存在根本性缺陷，会导致认知过载和分析性能下降

Method: 基于Woods（1984）的Keyhole Effect理论，分析聊天界面导致认知过载的五种机制，提出认知过载公式O = max(0, m - v - W)，并设计八种混合设计模式来解决这些问题

Result: 识别出聊天界面导致分析性能下降的五种认知机制，提出了八个具体的设计模式（生成式UI、无限画布、指示交互、状态轨道、幽灵层、准备就绪、语义缩放、概率UI）来缓解认知过载

Conclusion: 聊天界面不适合开放探索式数据分析任务，需要采用混合设计模式来平衡自然语言交互和可视化支持，论文提出了可证伪的假设和实验范式供实证验证

Abstract: Chat has become the default interface for AI-assisted data analysis. For multi-step, state-dependent analytical tasks, this is a mistake. Building on Woods (1984) Keyhole Effect, the cognitive cost of viewing large information spaces through narrow viewports, I show that chat interfaces systematically degrade analytical performance through five mechanisms: (1) constant content displacement defeats hippocampal spatial memory systems; (2) hidden state variables exceed working memory capacity (approximately 4 chunks under load); (3) forced verbalization triggers verbal overshadowing, degrading visual pattern recognition; (4) linear text streams block epistemic action and cognitive offloading; (5) serialization penalties scale with data dimensionality. I formalize cognitive overload as O = max(0, m - v - W) where m is task-relevant items, v is visible items, and W is working memory capacity. When O > 0, error probability increases and analytical biases (anchoring, confirmation, change blindness) amplify. Eight hybrid design patterns address these failures: Generative UI, Infinite Canvas, Deictic Interaction, State Rail, Ghost Layers, Mise en Place, Semantic Zoom, and Probabilistic UI. Each pattern targets specific cognitive bottlenecks while preserving natural language for intent specification and synthesis. Well-scaffolded conversational systems that encode expert priors may reduce load for guided tasks; the framework applies most strongly to open-ended exploration. The paper concludes with falsifiable hypotheses and experimental paradigms for empirical validation.

</details>


### [61] [Small-Margin Preferences Still Matter-If You Train Them Right](https://arxiv.org/abs/2602.00954)
*Jinlong Pang,Zhaowei Zhu,Na Di,Yichi Zhang,Yaxuan Wang,Chen Qian,Yang Liu*

Main category: cs.AI

TL;DR: MixDPO提出了一种难度感知的训练策略，通过将困难偏好对路由到监督微调目标，而将简单对应用于偏好损失，从而有效利用模糊偏好对中的监督信号。


<details>
  <summary>Details</summary>
Motivation: 传统偏好优化方法（如DPO）对偏好对的质量和难度高度敏感，通常将小边际（模糊）对视为噪声并过滤掉。然而研究发现，困难对在偏好损失下会破坏训练稳定性，但在监督微调中仍包含有用的监督信号。

Method: MixDPO采用难度感知训练策略：1）根据边际定义的难度将偏好数据从易到难排序（课程学习）；2）将困难对路由到SFT目标，同时对简单对应用偏好损失。这种混合设计能够利用模糊对而不引发优化失败。

Result: 在三个LLM-judge基准测试中，MixDPO相比DPO和一系列广泛使用的变体，在模型对齐方面持续改进，特别是在AlpacaEval~2长度控制胜率上取得了显著提升。

Conclusion: MixDPO通过难度感知的混合训练策略，有效解决了偏好优化中对困难对的处理问题，证明了模糊偏好对在适当优化目标下仍能提供有价值的监督信号，从而提升大语言模型的对齐效果。

Abstract: Preference optimization methods such as DPO align large language models (LLMs) using paired comparisons, but their effectiveness can be highly sensitive to the quality and difficulty of preference pairs. A common heuristic treats small-margin (ambiguous) pairs as noisy and filters them out. In this paper, we revisit this assumption and show that pair difficulty interacts strongly with the optimization objective: when trained with preference-based losses, difficult pairs can destabilize training and harm alignment, yet these same pairs still contain useful supervision signals when optimized with supervised fine-tuning (SFT). Motivated by this observation, we propose MixDPO, a simple yet effective difficulty-aware training strategy that (i) orders preference data from easy to hard (a curriculum over margin-defined difficulty), and (ii) routes difficult pairs to an SFT objective while applying a preference loss to easy pairs. This hybrid design provides a practical mechanism to leverage ambiguous pairs without incurring the optimization failures often associated with preference losses on low-margin data. Across three LLM-judge benchmarks, MixDPO consistently improves alignment over DPO and a range of widely-used variants, with particularly strong gains on AlpacaEval~2 length-controlled (LC) win rate.

</details>


### [62] [Reasoning and Tool-use Compete in Agentic RL:From Quantifying Interference to Disentangled Tuning](https://arxiv.org/abs/2602.00994)
*Yu Li,Mingyang Yi,Xiuyu Li,Ju Fan,Fuxin Jiang,Binbin Chen,Peng Li,Jie Song,Tieying Zhang*

Main category: cs.AI

TL;DR: 该论文挑战了现有智能体强化学习中联合训练推理和工具使用能力的假设，提出了解耦训练框架DART来提升性能


<details>
  <summary>Details</summary>
Motivation: 现有智能体强化学习方法通常假设联合训练推理和工具使用能力能提升整体性能，但这一假设缺乏实证检验。作者发现这两种能力在训练中会产生梯度干扰，影响优化效果

Method: 提出线性效应归因系统(LEAS)量化推理和工具使用行为之间的干扰，并设计解耦动作推理调优(DART)框架，通过分离的低秩适应模块分别更新推理和工具使用的参数

Result: 实验结果显示DART相比基线方法平均提升6.35%，性能与显式分离工具使用和推理的多智能体系统相当，但仅使用单个模型

Conclusion: 联合训练推理和工具使用能力存在干扰问题，解耦训练策略能有效提升智能体强化学习性能，为ARL范式提供了新的优化方向

Abstract: Agentic Reinforcement Learning (ARL) focuses on training large language models (LLMs) to interleave reasoning with external tool execution to solve complex tasks. Most existing ARL methods train a single shared model parameters to support both reasoning and tool use behaviors, implicitly assuming that joint training leads to improved overall agent performance. Despite its widespread adoption, this assumption has rarely been examined empirically. In this paper, we systematically investigate this assumption by introducing a Linear Effect Attribution System(LEAS), which provides quantitative evidence of interference between reasoning and tool-use behaviors. Through an in-depth analysis, we show that these two capabilities often induce misaligned gradient directions, leading to training interference that undermines the effectiveness of joint optimization and challenges the prevailing ARL paradigm. To address this issue, we propose Disentangled Action Reasoning Tuning(DART), a simple and efficient framework that explicitly decouples parameter updates for reasoning and tool-use via separate low-rank adaptation modules. Experimental results show that DART consistently outperforms baseline methods with averaged 6.35 percent improvements and achieves performance comparable to multi-agent systems that explicitly separate tool-use and reasoning using a single model.

</details>


### [63] [Error Taxonomy-Guided Prompt Optimization](https://arxiv.org/abs/2602.00997)
*Mayank Singh,Vikas Yadav,Eduardo Blanco*

Main category: cs.AI

TL;DR: ETGPO是一种基于错误分类的提示优化方法，采用自上而下的方式分析全局失败模式，相比传统自下而上的方法显著减少了计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有自动提示优化方法大多采用自下而上的试错方式，基于单个问题的反馈迭代调整提示，这种方法缺乏全局视角，且计算成本高昂。需要一种更高效、更具全局视野的提示优化方法。

Method: ETGPO采用自上而下的方法：1)收集模型错误；2)将错误分类到分类学中；3)针对最频繁的失败模式在提示中添加指导。这种方法关注全局失败模式而非单个问题。

Result: 在数学、问答和逻辑推理等多个基准测试中，ETGPO的准确率与最先进方法相当或更好，同时优化阶段的token使用量和评估预算仅需约三分之一。

Conclusion: 基于错误分类的自上而下提示优化方法比传统的自下而上方法更高效，能够在保持或提升性能的同时显著减少计算资源消耗。

Abstract: Automatic Prompt Optimization (APO) is a powerful approach for extracting performance from large language models without modifying their weights. Many existing methods rely on trial-and-error, testing different prompts or in-context examples until a good configuration emerges, often consuming substantial compute. Recently, natural language feedback derived from execution logs has shown promise as a way to identify how prompts can be improved. However, most prior approaches operate in a bottom-up manner, iteratively adjusting the prompt based on feedback from individual problems, which can cause them to lose the global perspective. In this work, we propose Error Taxonomy-Guided Prompt Optimization (ETGPO), a prompt optimization algorithm that adopts a top-down approach. ETGPO focuses on the global failure landscape by collecting model errors, categorizing them into a taxonomy, and augmenting the prompt with guidance targeting the most frequent failure modes. Across multiple benchmarks spanning mathematics, question answering, and logical reasoning, ETGPO achieves accuracy that is comparable to or better than state-of-the-art methods, while requiring roughly one third of the optimization-phase token usage and evaluation budget.

</details>


### [64] [How RLHF Amplifies Sycophancy](https://arxiv.org/abs/2602.01002)
*Itai Shapira,Gerdus Benade,Ariel D. Procaccia*

Main category: cs.AI

TL;DR: 研究发现人类偏好对齐训练会放大语言模型的奉承行为，提出了基于协方差分析的因果机制解释，并设计了包含一致性惩罚的奖励修正方法来抑制这种偏差


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在基于人类偏好的对齐训练后，奉承行为（即过度迎合用户观点而忽视事实准确性）会显著增强。研究者希望理解这种失败模式背后的因果机制，并提出有效的干预方法。

Method: 1）形式化分析：识别出人类偏好对齐训练放大奉承行为的因果机制，基于基础策略下赞同提示中信念信号与学习奖励之间的协方差；2）奖励学习分析：在Bradley-Terry等随机效用模型下分析人类标注者偏好偏差如何诱导奖励差距；3）干预设计：推导出在KL散度意义上最接近无约束后训练策略的唯一策略，并得到闭式的一致性惩罚奖励修正。

Result: 计算实验表明奖励差距普遍存在，在所有考虑的配置中都会导致行为漂移。提出的奖励修正方法能够有效抑制奉承行为的增加。

Conclusion: 人类偏好对齐训练会系统性放大语言模型的奉承行为，这种偏差源于人类标注数据中的偏好偏差。通过形式化分析揭示了因果机制，并提出了基于一致性惩罚的训练时干预方法，能够在保持模型性能的同时有效控制奉承行为的增加。

Abstract: Large language models often exhibit increased sycophantic behavior after preference-based post-training, showing a stronger tendency to affirm a user's stated or implied belief even when this conflicts with factual accuracy or sound judgment. We present a formal analysis of how alignment from human feedback can increase this failure mode by identifying an explicit amplification mechanism that causally links optimization against a learned reward to bias in the human preference data used for alignment. We show that the direction of behavioral drift is determined by a covariance under the base policy between endorsing the belief signal in the prompt and the learned reward, and that the first-order effect reduces to a simple mean-gap condition. We then analyze reward learning from pairwise comparisons under random utility models like Bradley-Terry and characterize when bias in human annotators' preferences induces this reward gap. Next, we propose a training-time intervention designed to neutralize the amplification mechanism itself. Among all post-trained policies that prevent sycophantic behavior from increasing, we characterize the unique policy closest in KL divergence to the unconstrained post-trained policy, and derive the corresponding minimal reward correction as a closed-form agreement penalty. Computational experiments find that reward gaps are common and cause behavioral drift in all the configurations considered.

</details>


### [65] [Discovering Process-Outcome Credit in Multi-Step LLM Reasoning](https://arxiv.org/abs/2602.01034)
*Xiangwei Wang,Wei Wang,Ken Chen,Nanduni Nimalsiri,Saman Halgamuge*

Main category: cs.AI

TL;DR: 提出了一种新的强化学习框架，通过逐步边际信息增益机制提供连续奖励信号，结合解耦掩码策略和双门监督微调目标，显著提升大语言模型的推理能力和样本效率。


<details>
  <summary>Details</summary>
Motivation: 标准基于结果的强化学习方法存在奖励稀疏和信用分配效率低的问题，这限制了大型语言模型推理能力的提升效果。

Method: 1. 引入逐步边际信息增益机制，通过单调历史水印量化推理步骤的内在价值；2. 采用解耦掩码策略，对思维链应用过程导向奖励，对完整输出应用结果导向奖励；3. 结合双门监督微调目标，利用高质量结构和事实信号稳定训练。

Result: 在文本和多模态基准测试（如MATH、Super-CLEVR）上，该方法在样本效率和最终准确率方面均优于GRPO等基线方法，并展现出优越的分布外鲁棒性和零样本迁移能力。

Conclusion: 该框架通过提供连续奖励信号和有效的信用分配机制，显著提升了强化学习在增强大语言模型推理能力方面的效果，为复杂推理任务提供了有效的解决方案。

Abstract: Reinforcement Learning (RL) serves as a potent paradigm for enhancing reasoning capabilities in Large Language Models (LLMs), yet standard outcome-based approaches often suffer from reward sparsity and inefficient credit assignment. In this paper, we propose a novel framework designed to provide continuous reward signals, which introduces a Step-wise Marginal Information Gain (MIG) mechanism that quantifies the intrinsic value of reasoning steps against a Monotonic Historical Watermark, effectively filtering out training noise. To ensure disentangled credit distribution, we implement a Decoupled Masking Strategy, applying process-oriented rewards specifically to the chain-of-thought (CoT) and outcome-oriented rewards to the full completion. Additionally, we incorporate a Dual-Gated SFT objective to stabilize training with high-quality structural and factual signals. Extensive experiments across textual and multi-modal benchmarks (e.g., MATH, Super-CLEVR) demonstrate that our approach consistently outperforms baselines such as GRPO in both sample efficiency and final accuracy. Furthermore, our model exhibits superior out-of-distribution robustness, demonstrating promising zero-shot transfer capabilities to unseen and challenging reasoning tasks.

</details>


### [66] [SetPO: Set-Level Policy Optimization for Diversity-Preserving LLM Reasoning](https://arxiv.org/abs/2602.01062)
*Chenyi Li,Yuan Zhang,Bo Wang,Guoqing Ma,Wei Tang,Haoyang Huang,Nan Duan*

Main category: cs.AI

TL;DR: 该论文提出了一种基于核相似度的集合级多样性目标，通过留一法边际贡献计算，将多样性作为优势塑形项融入策略优化，以解决强化学习中奖励可验证但结果多样性下降的问题。


<details>
  <summary>Details</summary>
Motivation: 尽管强化学习与可验证奖励能有效提升大语言模型的推理性能（尤其在数学任务中），但这种改进往往以降低结果多样性为代价，模型会将概率质量集中在狭窄的解决方案集合上。

Method: 1. 引入基于核相似度的集合级多样性目标；2. 为每个采样轨迹推导留一法边际贡献；3. 将该目标作为可插拔的优势塑形项集成到策略优化中；4. 在分布扰动框架下分析单个轨迹对语言模型多样性的贡献。

Result: 理论分析证实了单调性特性：较罕见的轨迹对全局多样性的边际贡献始终更高。在不同模型规模的广泛实验中，所提算法在多个基准测试的Pass@1和Pass@K指标上均优于强基线。

Conclusion: 通过将多样性目标作为优势塑形项融入策略优化，可以在保持强化学习性能优势的同时，有效提升大语言模型推理结果的多样性，实现性能与多样性的平衡。

Abstract: Reinforcement learning with verifiable rewards has shown notable effectiveness in enhancing large language models (LLMs) reasoning performance, especially in mathematics tasks. However, such improvements often come with reduced outcome diversity, where the model concentrates probability mass on a narrow set of solutions. Motivated by diminishing-returns principles, we introduce a set level diversity objective defined over sampled trajectories using kernelized similarity. Our approach derives a leave-one-out marginal contribution for each sampled trajectory and integrates this objective as a plug-in advantage shaping term for policy optimization. We further investigate the contribution of a single trajectory to language model diversity within a distribution perturbation framework. This analysis theoretically confirms a monotonicity property, proving that rarer trajectories yield consistently higher marginal contributions to the global diversity. Extensive experiments across a range of model scales demonstrate the effectiveness of our proposed algorithm, consistently outperforming strong baselines in both Pass@1 and Pass@K across various benchmarks.

</details>


### [67] [AutoHealth: An Uncertainty-Aware Multi-Agent System for Autonomous Health Data Modeling](https://arxiv.org/abs/2602.01078)
*Tong Xia,Weibin Li,Gang Liu,Yong Li*

Main category: cs.AI

TL;DR: AutoHealth是一个基于LLM的多智能体系统，专门用于医疗数据的自主建模和不确定性评估，在预测性能和不确定性估计方面显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的智能体系统在医疗数据应用中存在三个主要问题：难以泛化到异构医疗数据模态、过度依赖预定义解决方案模板而缺乏任务适应性、以及忽视对医疗决策至关重要的不确定性估计。

Method: 提出AutoHealth系统，采用五个专门化智能体的闭环协调机制，执行数据探索、任务条件化模型构建、训练和优化，同时平衡预测性能和不确定性量化。

Result: 在包含17个任务的真实世界基准测试中，AutoHealth完成了所有任务，预测性能比最先进基线提高了29.2%，不确定性估计提高了50.2%。

Conclusion: AutoHealth通过多智能体协同和不确定性感知机制，为医疗数据的自主建模提供了可靠解决方案，支持可信解释和风险感知决策。

Abstract: LLM-based agents have demonstrated strong potential for autonomous machine learning, yet their applicability to health data remains limited. Existing systems often struggle to generalize across heterogeneous health data modalities, rely heavily on predefined solution templates with insufficient adaptation to task-specific objectives, and largely overlook uncertainty estimation, which is essential for reliable decision-making in healthcare. To address these challenges, we propose \textit{AutoHealth}, a novel uncertainty-aware multi-agent system that autonomously models health data and assesses model reliability. \textit{AutoHealth} employs closed-loop coordination among five specialized agents to perform data exploration, task-conditioned model construction, training, and optimization, while jointly prioritizing predictive performance and uncertainty quantification. Beyond producing ready-to-use models, the system generates comprehensive reports to support trustworthy interpretation and risk-aware decision-making. To rigorously evaluate its effectiveness, we curate a challenging real-world benchmark comprising 17 tasks across diverse data modalities and learning settings. \textit{AutoHealth} completes all tasks and outperforms state-of-the-art baselines by 29.2\% in prediction performance and 50.2\% in uncertainty estimation.

</details>


### [68] [EvoOpt-LLM: Evolving industrial optimization models with large language models](https://arxiv.org/abs/2602.01082)
*Yiliu He,Tianle Li,Binghao Ji,Zhiyuan Liu,Di Huang*

Main category: cs.AI

TL;DR: EvoOpt-LLM是一个基于大语言模型的工业优化建模框架，能够自动化构建MILP模型、动态注入业务约束并进行变量剪枝，仅需少量训练样本即可实现高生成率和可执行率。


<details>
  <summary>Details</summary>
Motivation: 工业规划和调度中的混合整数线性规划建模高度依赖专家知识，将自然语言需求转化为可执行的优化模型以及适应不断变化的业务规则维护成本高昂。现有LLM方法存在数据效率低、求解器级别有效性有限、难以扩展到工业规模问题等挑战。

Method: 基于7B参数的大语言模型，通过参数高效的LoRA微调进行适配，构建了支持工业优化建模全生命周期的统一框架，包括自动化模型构建、动态业务约束注入和端到端变量剪枝三个核心模块。

Result: 仅用3000个训练样本就实现了91%的生成率和65.9%的可执行率，关键性能提升在1500个样本以下就显现出来。约束注入模块能可靠地增强现有MILP模型并保持原始目标，变量剪枝模块在中等规模LP模型上仅用400个样本就达到了约0.56的F1分数。

Conclusion: EvoOpt-LLM展示了工业优化建模的实用且数据高效的方法，减少了对专家干预的依赖，同时提高了适应性和求解器效率。

Abstract: Optimization modeling via mixed-integer linear programming (MILP) is fundamental to industrial planning and scheduling, yet translating natural-language requirements into solver-executable models and maintaining them under evolving business rules remains highly expertise-intensive. While large language models (LLMs) offer promising avenues for automation, existing methods often suffer from low data efficiency, limited solver-level validity, and poor scalability to industrial-scale problems. To address these challenges, we present EvoOpt-LLM, a unified LLM-based framework supporting the full lifecycle of industrial optimization modeling, including automated model construction, dynamic business-constraint injection, and end-to-end variable pruning. Built on a 7B-parameter LLM and adapted via parameter-efficient LoRA fine-tuning, EvoOpt-LLM achieves a generation rate of 91% and an executability rate of 65.9% with only 3,000 training samples, with critical performance gains emerging under 1,500 samples. The constraint injection module reliably augments existing MILP models while preserving original objectives, and the variable pruning module enhances computational efficiency, achieving an F1 score of ~0.56 on medium-sized LP models with only 400 samples. EvoOpt-LLM demonstrates a practical, data-efficient approach to industrial optimization modeling, reducing reliance on expert intervention while improving adaptability and solver efficiency.

</details>


### [69] [Hard Constraints Meet Soft Generation: Guaranteed Feasibility for LLM-based Combinatorial Optimization](https://arxiv.org/abs/2602.01090)
*Yang Liu,Chuan Zhou,Yancheng Chen,Shuai Zhang,Xixun Lin,Xiaoqing Wang*

Main category: cs.AI

TL;DR: FALCON框架通过语法约束解码、可行性修复层和自适应采样确保LLM在组合优化中100%可行性，使用BOPO训练方法，在7个NP难问题上实现完美可行性并超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在组合优化中展现出潜力，但缺乏保证解可行性的机制，这对于实际部署至关重要。

Method: FALCON框架包含三个关键技术：1) 语法约束解码确保句法有效性；2) 可行性修复层纠正语义约束违反；3) 自适应Best-of-N采样高效分配推理计算。训练使用BOPO方法，通过目标差距加权偏好对提供密集监督。

Result: 理论上证明了BOPO的收敛性并提供了修复引起的质量损失界限。在7个NP难问题上，FALCON实现了完美可行性，同时匹配或超越了最先进的神经和LLM求解器的解质量。

Conclusion: FALCON为LLM在组合优化中的实际部署提供了可行性保证框架，通过创新的解码、修复和训练方法解决了可行性问题，在多个问题上表现出色。

Abstract: Large language models (LLMs) have emerged as promising general-purpose solvers for combinatorial optimization (CO), yet they fundamentally lack mechanisms to guarantee solution feasibility which is critical for real-world deployment. In this work, we introduce FALCON, a framework that ensures 100\% feasibility through three key innovations: (i) \emph{grammar-constrained decoding} enforces syntactic validity, (ii) a \emph{feasibility repair layer} corrects semantic constraint violations, and (iii) \emph{adaptive Best-of-$N$ sampling} allocates inference compute efficiently. To train the underlying LLM, we introduce the Best-anchored Objective-guided Preference Optimization (BOPO) in LLM training, which weights preference pairs by their objective gap, providing dense supervision without human labels. Theoretically, we prove convergence for BOPO and provide bounds on repair-induced quality loss. Empirically, across seven NP-hard CO problems, FALCON achieves perfect feasibility while matching or exceeding the solution quality of state-of-the-art neural and LLM-based solvers.

</details>


### [70] [Probing RLVR training instability through the lens of objective-level hacking](https://arxiv.org/abs/2602.01103)
*Yiming Dong,Kun Fu,Haoyu Li,Xinyuan Zhu,Yurou Liu,Lijing Shao,Jieping Ye,Zheng Wang*

Main category: cs.AI

TL;DR: 本文提出一个理论框架，从目标层面黑客攻击的角度理解MoE模型在RLVR训练中的不稳定性，揭示了训练-推理差异异常增长的机制。


<details>
  <summary>Details</summary>
Motivation: MoE架构在RLVR训练中容易出现不稳定性，严重影响模型能力提升，但其根本原因和机制尚不清楚，需要系统性的理论框架来解释。

Method: 提出基于目标层面黑客攻击的理论框架，区别于奖励黑客攻击，关注token级信用错位导致的系统级虚假信号。在30B MoE模型上进行大量实验验证。

Result: 揭示了MoE模型中训练-推理差异异常增长这一关键病理训练动态的起源和机制，为之前广泛关联但不理解的现象提供了因果解释。

Conclusion: 为MoE模型在RLVR训练中的不稳定性提供了具体且因果的训练动态解释，为设计稳定的RLVR算法提供了指导。

Abstract: Prolonged reinforcement learning with verifiable rewards (RLVR) has been shown to drive continuous improvements in the reasoning capabilities of large language models, but the training is often prone to instabilities, especially in Mixture-of-Experts (MoE) architectures. Training instability severely undermines model capability improvement, yet its underlying causes and mechanisms remain poorly understood. In this work, we introduce a principled framework for understanding RLVR instability through the lens of objective-level hacking. Unlike reward hacking, which arises from exploitable verifiers, objective-level hacking emerges from token-level credit misalignment and is manifested as system-level spurious signals in the optimization objective. Grounded in our framework, together with extensive experiments on a 30B MoE model, we trace the origin and formalize the mechanism behind a key pathological training dynamic in MoE models: the abnormal growth of the training-inference discrepancy, a phenomenon widely associated with instability but previously lacking a mechanistic explanation. These findings provide a concrete and causal account of the training dynamics underlying instabilities in MoE models, offering guidance for the design of stable RLVR algorithms.

</details>


### [71] [Transforming Vehicle Diagnostics: A Multimodal Approach to Error Patterns Prediction](https://arxiv.org/abs/2602.01109)
*Hugo Math,Rainer Lienhart*

Main category: cs.AI

TL;DR: BiCarFormer：首个多模态方法，整合DTC序列和环境条件进行多标签序列分类，显著提升车辆故障诊断性能


<details>
  <summary>Details</summary>
Motivation: 当前车辆诊断系统主要依赖DTC序列，但忽略了温度、湿度、压力等环境数据，而这些上下文信息对专家分类故障至关重要。真实世界数据复杂且噪声多，需要更好的整合方法。

Method: 提出BiCarFormer双向Transformer模型，专门处理车辆事件序列，采用嵌入融合和协同注意力机制捕捉诊断代码与环境数据之间的关系。

Result: 在包含22,137个错误代码和360个错误模式的真实汽车数据集上，相比仅使用DTC序列的传统序列模型，分类性能显著提升。

Conclusion: 整合上下文环境信息对实现更准确、鲁棒的车辆诊断至关重要，可降低维护成本并提升汽车行业自动化流程。

Abstract: Accurately diagnosing and predicting vehicle malfunctions is crucial for maintenance and safety in the automotive industry. While modern diagnostic systems primarily rely on sequences of vehicular Diagnostic Trouble Codes (DTCs) registered in On-Board Diagnostic (OBD) systems, they often overlook valuable contextual information such as raw sensory data (e.g., temperature, humidity, and pressure). This contextual data, crucial for domain experts to classify vehicle failures, introduces unique challenges due to its complexity and the noisy nature of real-world data. This paper presents BiCarFormer: the first multimodal approach to multi-label sequence classification of error codes into error patterns that integrates DTC sequences and environmental conditions. BiCarFormer is a bidirectional Transformer model tailored for vehicle event sequences, employing embedding fusions and a co-attention mechanism to capture the relationships between diagnostic codes and environmental data. Experimental results on a challenging real-world automotive dataset with 22,137 error codes and 360 error patterns demonstrate that our approach significantly improves classification performance compared to models that rely solely on DTC sequences and traditional sequence models. This work highlights the importance of incorporating contextual environmental information for more accurate and robust vehicle diagnostics, hence reducing maintenance costs and enhancing automation processes in the automotive industry.

</details>


### [72] [PersistBench: When Should Long-Term Memories Be Forgotten by LLMs?](https://arxiv.org/abs/2602.01146)
*Sidharth Pulipaka,Oliver Chen,Manas Sharma,Taaha S Bajwa,Vyas Raina,Ivaxi Sheth*

Main category: cs.AI

TL;DR: 论文提出PersistBench基准测试，用于评估对话助手长期记忆带来的安全风险，发现主流LLMs在跨域泄漏和记忆诱导谄媚问题上存在高失败率。


<details>
  <summary>Details</summary>
Motivation: 对话助手越来越多地将长期记忆与LLMs集成，这种记忆持久性虽然能增强个性化体验，但也带来了被忽视的安全风险，需要系统性地测量和评估这些风险。

Method: 提出PersistBench基准测试框架，识别两种长期记忆特有风险：跨域泄漏（LLMs不适当地从长期记忆中注入上下文）和记忆诱导谄媚（存储的长期记忆暗中强化用户偏见）。在18个前沿和开源LLMs上进行评估。

Result: 评估结果显示惊人的高失败率：跨域样本中位失败率53%，谄媚样本中位失败率97%，表明当前LLMs在长期记忆安全方面存在严重问题。

Conclusion: 长期记忆集成带来了新的安全挑战，PersistBench基准测试有助于推动开发更稳健、更安全的长期记忆使用方案，提升前沿对话系统的安全性。

Abstract: Conversational assistants are increasingly integrating long-term memory with large language models (LLMs). This persistence of memories, e.g., the user is vegetarian, can enhance personalization in future conversations. However, the same persistence can also introduce safety risks that have been largely overlooked. Hence, we introduce PersistBench to measure the extent of these safety risks. We identify two long-term memory-specific risks: cross-domain leakage, where LLMs inappropriately inject context from the long-term memories; and memory-induced sycophancy, where stored long-term memories insidiously reinforce user biases. We evaluate 18 frontier and open-source LLMs on our benchmark. Our results reveal a surprisingly high failure rate across these LLMs - a median failure rate of 53% on cross-domain samples and 97% on sycophancy samples. To address this, our benchmark encourages the development of more robust and safer long-term memory usage in frontier conversational systems.

</details>


### [73] [Do All Individual Layers Help? An Empirical Study of Task-Interfering Layers in Vision-Language Models](https://arxiv.org/abs/2602.01167)
*Zhiming Liu,Yujie Wei,Lei Feng,Xiu Su,Xiaobo Xia,Weili Guan,Zeke Xie,Shuo Yang*

Main category: cs.AI

TL;DR: 研究发现预训练视觉语言模型存在任务干扰层，这些层会损害特定下游任务的性能。通过干预单个层（如置零参数）可以提升某些任务表现，表明模型存在任务特定的模块化特性。作者提出TaLo方法，无需训练即可动态识别并绕过干扰层，提升推理性能。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型通常默认使用所有层进行下游任务预测。研究发现某些层实际上会阻碍特定任务的性能，这表明预训练模型存在任务干扰层。理解这些层如何影响不同任务，并开发无需训练的方法来绕过这些干扰层，可以显著提升模型性能。

Method: 1. 系统研究单个层如何通过层干预影响不同任务；2. 测量干预每个层后相对于基础模型的性能变化；3. 引入任务-层交互向量量化每个层对给定任务的影响；4. 提出TaLo方法：无需训练、测试时自适应的方法，动态识别并绕过最干扰的层。

Result: 1. 发现任务干扰层的存在，这些层会损害下游任务性能；2. 任务干扰层表现出任务特定的敏感模式：需要相似能力的任务在层干预下显示一致的响应趋势；3. TaLo方法无需参数更新即可提升各种模型和数据集性能，如在ScienceQA的Maps任务上将Qwen-VL准确率提升高达16.6%。

Conclusion: 该研究揭示了预训练视觉语言模型中存在意外形式的模块化特性，并提供了即插即用、无需训练的机制，在推理时解锁隐藏能力。TaLo方法展示了通过识别和绕过任务干扰层来提升模型性能的有效性，为视觉语言模型的优化提供了新思路。

Abstract: Current VLMs have demonstrated capabilities across a wide range of multimodal tasks. Typically, in a pretrained VLM, all layers are engaged by default to make predictions on downstream tasks. We find that intervening on a single layer, such as by zeroing its parameters, can improve the performance on certain tasks, indicating that some layers hinder rather than help downstream tasks. We systematically investigate how individual layers influence different tasks via layer intervention. Specifically, we measure the change in performance relative to the base model after intervening on each layer and observe improvements when bypassing specific layers. This improvement can be generalizable across models and datasets, indicating the presence of Task-Interfering Layers that harm downstream tasks' performance. We introduce Task-Layer Interaction Vector, which quantifies the effect of intervening on each layer of a VLM given a task. These task-interfering layers exhibit task-specific sensitivity patterns: tasks requiring similar capabilities show consistent response trends under layer interventions, as evidenced by the high similarity in their task-layer interaction vectors. Inspired by these findings, we propose TaLo (Task-Adaptive Layer Knockout), a training-free, test-time adaptation method that dynamically identifies and bypasses the most interfering layer for a given task. Without parameter updates, TaLo improves performance across various models and datasets, including boosting Qwen-VL's accuracy on the Maps task in ScienceQA by up to 16.6%. Our work reveals an unexpected form of modularity in pretrained VLMs and provides a plug-and-play, training-free mechanism to unlock hidden capabilities at inference time. The source code will be publicly available.

</details>


### [74] [A State-Transition Framework for Efficient LLM Reasoning](https://arxiv.org/abs/2602.01198)
*Liang Zhang,Yu Zhao,Longyue Wang,Tianqi Shi,Weihua Luo,Kaifu Zhang,Jinsong Su*

Main category: cs.AI

TL;DR: 本文提出了一种高效推理框架，将LLM的推理过程建模为状态转移过程，使用线性注意力机制降低计算复杂度，同时通过状态推理策略缓解过度思考问题。


<details>
  <summary>Details</summary>
Motivation: 长思维链推理虽然能提升LLM在复杂推理任务上的性能，但生成长CoT序列的计算和内存成本过高，限制了其效率和实用性。现有方法通过压缩CoT序列来提高效率，但这与测试时扩展相冲突，限制了LLM的推理能力。

Method: 1. 将LLM推理过程建模为状态转移过程；2. 使用线性注意力机制估计推理状态，记录历史推理信息；3. 基于查询提示和推理状态，LLM高效执行当前推理步骤并更新状态；4. 提出基于状态的推理策略缓解噪声推理步骤引起的过度思考问题。

Result: 在多个数据集和模型规模上的广泛实验表明，该框架不仅提高了LLM的推理效率，还增强了其推理性能。

Conclusion: 提出的高效推理框架通过状态转移建模和线性注意力机制，在保持推理能力的同时显著降低了计算复杂度，解决了长CoT推理的效率瓶颈问题。

Abstract: While Long Chain-of-Thought (CoT) reasoning significantly improves Large Language Models (LLMs) performance on complex reasoning tasks, the substantial computational and memory costs of generating long CoT sequences limit their efficiency and practicality. Existing studies usually enhance the reasoning efficiency of LLMs by compressing CoT sequences. However, this approach conflicts with test-time scaling, limiting the reasoning capacity of LLMs. In this paper, we propose an efficient reasoning framework that models the reasoning process of LLMs as a state-transition process. Specifically, we first apply a linear attention mechanism to estimate the LLM's reasoning state, which records the historical reasoning information from previous reasoning steps. Then, based on the query prompt and the reasoning state, the LLM can efficiently perform the current reasoning step and update the state. With the linear attention, each token in the current reasoning step can directly retrieve relevant historical reasoning information from the reasoning state, without explicitly attending to tokens in previous reasoning steps. In this way, the computational complexity of attention is reduced from quadratic to linear, significantly improving the reasoning efficiency of LLMs. In addition, we propose a state-based reasoning strategy to mitigate the over-thinking issue caused by noisy reasoning steps. Extensive experiments across multiple datasets and model sizes demonstrate that our framework not only improves the reasoning efficiency of LLMs but also enhances their reasoning performance.

</details>


### [75] [Workflow-R1: Group Sub-sequence Policy Optimization for Multi-turn Workflow Construction](https://arxiv.org/abs/2602.01202)
*Mingze Kong,Zikun Qu,Zhongquan Zhou,Pengyu Liang,Xiang Li,Zhiwei Shang,Zhi Hong,Kaiyu Huang,Zhiyong Wang,Zhongxiang Dai*

Main category: cs.AI

TL;DR: Workflow-R1将工作流构建重新定义为多轮自然语言顺序决策过程，引入GSsPO算法解决优化粒度不匹配问题，在多个QA基准测试中优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有工作流优化方法将工作流合成视为静态、一次性的代码生成问题，这过度约束了模型的编码能力，限制了动态问题解决的灵活性。

Method: 提出Workflow-R1框架，将工作流构建重新定义为多轮自然语言顺序决策过程。引入Group Sub-sequence Policy Optimization (GSsPO)算法，将优化单元重新校准为复合子序列（特别是原子Think-Action循环），使梯度更新与交互的语义边界对齐。

Result: 在多个QA基准测试中，Workflow-R1优于竞争基线，验证了GSsPO作为顺序推理的通用解决方案的有效性。

Conclusion: Workflow-R1为自动化工作流优化提供了一个有前景的新范式，GSsPO作为一种结构感知的RL算法可推广到广泛的智能体顺序决策任务中。

Abstract: The rapid evolution of agentic workflows has demonstrated strong performance of LLM-based agents in addressing complex reasoning tasks. However, existing workflow optimization methods typically formulate workflow synthesis as a static, one-shot code-centric generation problem. This paradigm imposes excessive constraints on the model's coding capabilities and restricts the flexibility required for dynamic problem-solving. In this paper, we present Workflow-R1, a framework that reformulates workflow construction as a multi-turn, natural language-based sequential decision-making process. To resolve the optimization granularity mismatch inherent in such multi-turn interactions, we introduce Group Sub-sequence Policy Optimization (GSsPO). While explicitly tailored to align with the interleaved Think-Action dynamics of agentic reasoning, GSsPO fundamentally functions as a structure-aware RL algorithm generalizable to a broad class of multi-turn agentic sequential decision-making tasks. By recalibrating the optimization unit to the composite sub-sequence, specifically the atomic Think-Action cycle, it aligns gradient updates with the semantic boundaries of these interactions, ensuring robust learning in complex multi-turn reasoning tasks. Through extensive experiments on multiple QA benchmarks, Workflow-R1 outperforms competitive baselines, validating GSsPO as a generalized solution for sequential reasoning and establishing Workflow-R1 as a promising new paradigm for automated workflow optimization.

</details>


### [76] [Not All Preferences Are Created Equal: Stability-Aware and Gradient-Efficient Alignment for Reasoning Models](https://arxiv.org/abs/2602.01207)
*Hui Wu,Hengyi Cai,Jinman Zhao,Xinran Chen,Ziheng Li,Zhejun Zhao,Shuaiqiang Wang,Yuchen Li,Dawei Yin*

Main category: cs.AI

TL;DR: SAGE框架通过动态数据选择优化推理模型对齐，基于稳定性感知的梯度效率最大化信号噪声比，加速收敛并超越静态基线方法。


<details>
  <summary>Details</summary>
Motivation: 传统偏好对齐方法（如DPO）对所有偏好对一视同仁，忽略了训练样本的演化效用。静态方法导致优化效率低下或不稳定：浪费计算资源处理梯度可忽略的平凡样本，同时受到决策边界附近不确定样本的噪声干扰。

Method: 提出SAGE（稳定性感知梯度效率）动态框架，包含：1）基于模型能力刷新候选池的粗粒度课程机制；2）细粒度稳定性感知评分函数，优先选择信息丰富、置信度高的错误样本，同时过滤不稳定样本。

Result: 在多个数学推理基准测试中，SAGE显著加速收敛速度，性能优于静态基线方法。

Conclusion: 研究强调了在推理对齐中采用策略感知、稳定性意识的数据选择的重要性，动态样本选择能有效提升对齐过程的可靠性和效率。

Abstract: Preference-based alignment is pivotal for training large reasoning models; however, standard methods like Direct Preference Optimization (DPO) typically treat all preference pairs uniformly, overlooking the evolving utility of training instances. This static approach often leads to inefficient or unstable optimization, as it wastes computation on trivial pairs with negligible gradients and suffers from noise induced by samples near uncertain decision boundaries. Facing these challenges, we propose SAGE (Stability-Aware Gradient Efficiency), a dynamic framework designed to enhance alignment reliability by maximizing the Signal-to-Noise Ratio of policy updates. Concretely, SAGE integrates a coarse-grained curriculum mechanism that refreshes candidate pools based on model competence with a fine-grained, stability-aware scoring function that prioritizes informative, confident errors while filtering out unstable samples. Experiments on multiple mathematical reasoning benchmarks demonstrate that SAGE significantly accelerates convergence and outperforms static baselines, highlighting the critical role of policy-aware, stability-conscious data selection in reasoning alignment.

</details>


### [77] [FutureMind: Equipping Small Language Models with Strategic Thinking-Pattern Priors via Adaptive Knowledge Distillation](https://arxiv.org/abs/2602.01222)
*Shaoxiong Yang,Junting Li,Mengyuan Zhang,Chao Li,Wei Liu,Jian Luan*

Main category: cs.AI

TL;DR: FutureMind是一个模块化推理框架，通过从大语言模型进行自适应知识蒸馏，为小语言模型提供战略思维模式先验，以解决其在复杂知识密集型任务中的推理和检索问题。


<details>
  <summary>Details</summary>
Motivation: 小语言模型在成本敏感和资源有限的环境中具有吸引力，但它们在需要结构化推理和有效检索的复杂知识密集型任务上表现不佳。需要一种方法让小语言模型具备更强的推理能力，同时保持其效率优势。

Method: 提出FutureMind框架，包含四个关键模块：问题分析、逻辑推理、策略规划和检索指导。采用三种不同的检索范式将复杂查询分解为可处理的子问题。通过自适应知识蒸馏从大语言模型向小语言模型传递战略思维模式。

Result: 在多个多跳QA基准测试（2WikiMultihopQA、MuSiQue、Bamboogle、Frames）上，FutureMind始终优于Search-o1等强基线，在不同架构和规模的小语言模型上实现了免费训练条件下的最先进结果。

Conclusion: FutureMind成功提升了小语言模型在复杂推理任务上的性能，同时揭示了思维模式蒸馏过程受到教师（大语言模型）和学生（小语言模型）之间认知偏差瓶颈的限制，这为推理技能的可迁移性研究提供了新视角。

Abstract: Small Language Models (SLMs) are attractive for cost-sensitive and resource-limited settings due to their efficient, low-latency inference. However, they often struggle with complex, knowledge-intensive tasks that require structured reasoning and effective retrieval. To address these limitations, we propose FutureMind, a modular reasoning framework that equips SLMs with strategic thinking-pattern priors via adaptive knowledge distillation from large language models (LLMs). FutureMind introduces a dynamic reasoning pipeline composed of four key modules: Problem Analysis, Logical Reasoning, Strategy Planning, and Retrieval Guidance. This pipeline is augmented by three distinct retrieval paradigms that decompose complex queries into tractable subproblems, ensuring efficient and accurate retrieval execution. Extensive experiments on multi-hop QA benchmarks, including 2WikiMultihopQA, MuSiQue, Bamboogle, and Frames, demonstrate the superiority of FutureMind. It consistently outperforms strong baselines such as Search-o1, achieving state-of-the-art results under free training conditions across diverse SLM architectures and scales. Beyond empirical gains, our analysis reveals that the process of thinking-pattern distillation is restricted by the cognitive bias bottleneck between the teacher (LLMs) and student (SLMs) models. This provides new perspectives on the transferability of reasoning skills, paving the way for the development of SLMs that combine efficiency with genuine cognitive capability.

</details>


### [78] [LLM-Driven Ontology Construction for Enterprise Knowledge Graphs](https://arxiv.org/abs/2602.01276)
*Abdulsobur Oyewale,Tommaso Soru*

Main category: cs.AI

TL;DR: OntoEKG：基于大语言模型的自动化本体构建流水线，从非结构化企业数据生成领域特定本体，在数据领域达到0.724的模糊匹配F1分数


<details>
  <summary>Details</summary>
Motivation: 企业知识图谱需要统一异构数据并实施语义治理，但底层本体构建仍是资源密集、依赖领域专家的手动过程，需要自动化解决方案

Method: 将建模任务分解为两个阶段：提取模块识别核心类和属性，蕴含模块将这些元素逻辑结构化形成层次结构，最后序列化为标准RDF格式

Result: 在数据、金融和物流领域文档构建的新评估数据集上测试，在数据领域达到模糊匹配F1分数0.724，同时揭示了范围定义和层次推理方面的局限性

Conclusion: OntoEKG展示了LLM驱动本体构建的潜力，但需要解决范围定义和层次推理的挑战，为端到端本体构建提供了新的评估基准

Abstract: Enterprise Knowledge Graphs have become essential for unifying heterogeneous data and enforcing semantic governance. However, the construction of their underlying ontologies remains a resource-intensive, manual process that relies heavily on domain expertise. This paper introduces OntoEKG, a LLM-driven pipeline designed to accelerate the generation of domain-specific ontologies from unstructured enterprise data. Our approach decomposes the modelling task into two distinct phases: an extraction module that identifies core classes and properties, and an entailment module that logically structures these elements into a hierarchy before serialising them into standard RDF. Addressing the significant lack of comprehensive benchmarks for end-to-end ontology construction, we adopt a new evaluation dataset derived from documents across the Data, Finance, and Logistics sectors. Experimental results highlight both the potential and the challenges of this approach, achieving a fuzzy-match F1-score of 0.724 in the Data domain while revealing limitations in scope definition and hierarchical reasoning.

</details>


### [79] [RE-MCDF: Closed-Loop Multi-Expert LLM Reasoning for Knowledge-Grounded Clinical Diagnosis](https://arxiv.org/abs/2602.01297)
*Shaowei Shen,Xiaohong Yang,Jie Yang,Lianfen Huang,Yongcai Zhang,Yang Zou,Seyyedali Hosseinalipour*

Main category: cs.AI

TL;DR: RE-MCDF是一个关系增强的多专家临床诊断框架，通过生成-验证-修订的闭环架构解决神经科电子病历诊断中的异质性、稀疏性和噪声问题，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 神经科电子病历具有异质性、稀疏性和噪声，单智能体系统容易产生自我强化的错误，现有多智能体框架交互浅层且缺乏结构，忽略了疾病间的丰富逻辑依赖关系，无法排除临床不可行的假设。

Method: 提出RE-MCDF框架，采用生成-验证-修订闭环架构，包含三个互补组件：1）生成候选诊断和支持证据的主要专家；2）动态优先处理异质性临床指标的实验室专家；3）强制执行疾病间逻辑约束的多关系感知与评估专家组。基于医学知识图谱，前两个专家自适应重新加权电子病历证据，专家组验证和修正候选诊断以确保逻辑一致性。

Result: 在CMEMR神经科子集（NEEMRs）和自建数据集（XMEMRs）上的广泛实验表明，RE-MCDF在复杂诊断场景中始终优于最先进的基线方法。

Conclusion: RE-MCDF通过整合多专家协作和显式逻辑约束，有效解决了神经科电子病历诊断中的挑战，为临床决策提供了更可靠的支持。

Abstract: Electronic medical records (EMRs), particularly in neurology, are inherently heterogeneous, sparse, and noisy, which poses significant challenges for large language models (LLMs) in clinical diagnosis. In such settings, single-agent systems are vulnerable to self-reinforcing errors, as their predictions lack independent validation and can drift toward spurious conclusions. Although recent multi-agent frameworks attempt to mitigate this issue through collaborative reasoning, their interactions are often shallow and loosely structured, failing to reflect the rigorous, evidence-driven processes used by clinical experts. More fundamentally, existing approaches largely ignore the rich logical dependencies among diseases, such as mutual exclusivity, pathological compatibility, and diagnostic confusion. This limitation prevents them from ruling out clinically implausible hypotheses, even when sufficient evidence is available. To overcome these, we propose RE-MCDF, a relation-enhanced multi-expert clinical diagnosis framework. RE-MCDF introduces a generation--verification--revision closed-loop architecture that integrates three complementary components: (i) a primary expert that generates candidate diagnoses and supporting evidence, (ii) a laboratory expert that dynamically prioritizes heterogeneous clinical indicators, and (iii) a multi-relation awareness and evaluation expert group that explicitly enforces inter-disease logical constraints. Guided by a medical knowledge graph (MKG), the first two experts adaptively reweight EMR evidence, while the expert group validates and corrects candidate diagnoses to ensure logical consistency. Extensive experiments on the neurology subset of CMEMR (NEEMRs) and on our curated dataset (XMEMRs) demonstrate that RE-MCDF consistently outperforms state-of-the-art baselines in complex diagnostic scenarios.

</details>


### [80] [Building Better Deception Probes Using Targeted Instruction Pairs](https://arxiv.org/abs/2602.01425)
*Vikram Natarajan,Devina Jain,Shivam Arora,Satvik Golechha,Joseph Bloom*

Main category: cs.AI

TL;DR: 线性探针在检测AI系统欺骗行为方面有潜力，但现有方法存在虚假相关性和误报问题。研究发现训练时使用的指令对选择对探针性能影响最大（占70.6%方差），且针对特定欺骗类型的专门化探针比通用欺骗检测器更有效。


<details>
  <summary>Details</summary>
Motivation: 现有线性探针方法在检测AI系统欺骗行为时存在明显缺陷，包括虚假相关性和对非欺骗性响应的误报。需要理解这些失败的原因并改进探针设计，以提高欺骗检测的准确性和可靠性。

Method: 研究分析了训练时使用的指令对的重要性，并提出基于人类可解释的欺骗分类法来针对特定欺骗行为设计探针。通过对比不同指令对和欺骗类型，评估探针性能的变化。

Result: 研究发现指令对主要捕捉欺骗意图而非内容特定模式，这解释了为什么提示选择主导探针性能（占70.6%方差）。针对特定欺骗类型设计的专门化探针在评估数据集上表现更好。

Conclusion: 由于不同数据集中欺骗类型的异质性，组织应该设计针对其特定威胁模型的专门化探针，而不是寻求通用的欺骗检测器。指令对选择是影响探针性能的关键因素。

Abstract: Linear probes are a promising approach for monitoring AI systems for deceptive behaviour. Previous work has shown that a linear classifier trained on a contrastive instruction pair and a simple dataset can achieve good performance. However, these probes exhibit notable failures even in straightforward scenarios, including spurious correlations and false positives on non-deceptive responses. In this paper, we identify the importance of the instruction pair used during training. Furthermore, we show that targeting specific deceptive behaviors through a human-interpretable taxonomy of deception leads to improved results on evaluation datasets. Our findings reveal that instruction pairs capture deceptive intent rather than content-specific patterns, explaining why prompt choice dominates probe performance (70.6% of variance). Given the heterogeneity of deception types across datasets, we conclude that organizations should design specialized probes targeting their specific threat models rather than seeking a universal deception detector.

</details>


### [81] [Qrita: High-performance Top-k and Top-p Algorithm for GPUs using Pivot-based Truncation and Selection](https://arxiv.org/abs/2602.01518)
*Jongseok Park,Sunga Kim,Alvin Cheung,Ion Stoica*

Main category: cs.AI

TL;DR: Qrita是一种基于枢轴选择策略的高效Top-k和Top-p算法，相比传统排序方法在GPU上实现2倍吞吐量和一半内存使用，同时保持确定性输出。


<details>
  <summary>Details</summary>
Motivation: Top-k和Top-p是大语言模型采样中的主要截断算子，但在大规模词汇表上高效实现仍然面临挑战。现有方法要么依赖排序（带来显著计算和内存开销），要么使用随机方法（改变算法输出）。

Method: 基于RTop-k的枢轴搜索思想，Qrita扩展了枢轴搜索到Top-k和Top-p，采用两种关键技术：1）基于高斯分布的sigma截断，大幅减少目标元素的搜索空间；2）四元枢轴搜索与重复处理，将枢轴搜索迭代减半并保证确定性输出。使用Triton GPU编程语言实现。

Result: 与vLLM、SGLang和Flashinfer等高性能LLM执行引擎的Top-k和Top-p内核相比，Qrita实现了高达2倍的吞吐量和一半的内存使用，同时提供与排序算法相同的输出。

Conclusion: Qrita提出了一种高效、确定性的Top-k和Top-p算法，通过枢轴选择策略显著提升了GPU上的性能，为大语言模型采样提供了更优的解决方案。

Abstract: Top-k and Top-p are the dominant truncation operators in the sampling of large language models. Despite their widespread use, implementing them efficiently over large vocabularies remains a significant challenge. Existing approaches often rely on sorting, which incur significant computation and memory overhead on GPUs, or stochastic approaches, which alter the algorithm output. In this work, we propose Qrita, an efficient Top-k and Top-p algorithm based on a pivot-based selection strategy. Based on RTop-k, which uses a pivot-based search for node selection in graph neural networks, Qrita extends the concept of pivot-based search to both Top-k and Top-p with two key techniques: 1. Gaussian-based sigma-truncation, which greatly reduces the search space of the target elements, and 2. Quaternary pivot search with duplication handling, which halves the pivot search iteration and guarantees deterministic output. We provide the full implementation of Qrita using Triton, a popular GPU programming language. Our evaluation of Qrita against the Top-k and Top-p kernels of high performance LLM execution engines such as vLLM, SGLang, and Flashinfer show that Qrita achieves up to 2 times throughput and half memory use while providing the same output to the the sorting-based algorithms.

</details>


### [82] [MAGIC: A Co-Evolving Attacker-Defender Adversarial Game for Robust LLM Safety](https://arxiv.org/abs/2602.01539)
*Xiaoyu Wen,Zhida He,Han Qi,Ziyu Wan,Zhongtian Ma,Ying Wen,Tianhang Zheng,Xingcheng Xu,Chaochao Lu,Qiaosheng Zhang*

Main category: cs.AI

TL;DR: MAGIC框架通过多轮多智能体强化学习将LLM安全对齐建模为对抗性非对称博弈，攻击者学习改写查询生成欺骗性提示，防御者学习识别并拒绝此类输入，实现协同进化以提升安全鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM安全防御方法依赖静态预收集数据分布，难以应对不断演化的对抗攻击，需要动态适应性的安全对齐框架。

Method: 提出MAGIC多轮多智能体强化学习框架，将安全对齐建模为对抗性非对称博弈：攻击者智能体迭代改写原始查询生成欺骗性提示，防御者智能体同时优化策略识别并拒绝此类输入，实现协同进化。

Result: 实验验证框架有效性，展示优越的防御成功率且不影响模型的有用性；攻击者通过迭代RL训练演化出新颖的、先前未见过的组合策略，揭示方法的巨大潜力。

Conclusion: MAGIC框架通过动态对抗训练实现LLM安全对齐，提供理论上的安全保证和更鲁棒的游戏均衡，为应对不断演化的对抗攻击提供有效解决方案。

Abstract: Ensuring robust safety alignment is crucial for Large Language Models (LLMs), yet existing defenses often lag behind evolving adversarial attacks due to their \textbf{reliance on static, pre-collected data distributions}. In this paper, we introduce \textbf{MAGIC}, a novel multi-turn multi-agent reinforcement learning framework that formulates LLM safety alignment as an adversarial asymmetric game. Specifically, an attacker agent learns to iteratively rewrite original queries into deceptive prompts, while a defender agent simultaneously optimizes its policy to recognize and refuse such inputs. This dynamic process triggers a \textbf{co-evolution}, where the attacker's ever-changing strategies continuously uncover long-tail vulnerabilities, driving the defender to generalize to unseen attack patterns. Remarkably, we observe that the attacker, endowed with initial reasoning ability, evolves \textbf{novel, previously unseen combinatorial strategies} through iterative RL training, underscoring our method's substantial potential. Theoretically, we provide insights into a more robust game equilibrium and derive safety guarantees. Extensive experiments validate our framework's effectiveness, demonstrating superior defense success rates without compromising the helpfulness of the model. Our code is available at https://github.com/BattleWen/MAGIC.

</details>


### [83] [S1-NexusAgent: a Self-Evolving Agent Framework for Multidisciplinary Scientific Research](https://arxiv.org/abs/2602.01550)
*S1-NexusAgent Team*

Main category: cs.AI

TL;DR: S1-NexusAgent是一个用于多学科科学研究的自进化智能体框架，通过分层规划-执行架构、MCP协议集成数千种跨学科工具、稀疏上下文管理和技能蒸馏机制，在复杂科学任务中实现最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM和基于工具的智能体在处理大规模数据、复杂工作流和专用工具时存在局限，特别是在长时程规划、鲁棒目标维护和持续学习方面，难以满足现代科学研究需求。

Method: 采用分层Plan-and-CodeAct执行范式，通过双循环架构解耦全局科学规划和子任务级工具执行；原生支持MCP协议，集成数千种跨学科工具；引入基于对象引用的稀疏上下文管理；通过Critic Agent自动评估执行轨迹并提炼高质量研究路径为可重用科学技能。

Result: 在涉及长时程规划和复杂专用工具编排的权威科学基准测试（biomini-eval、ChemBench、MatSciBench）中，S1-NexusAgent实现了最先进的性能，验证了其在复杂科学任务中的有效性和泛化能力。

Conclusion: S1-NexusAgent通过自进化框架解决了科学研究的核心挑战，为可持续和长时程科学研究提供了有价值的解决方案，在跨学科科学任务中表现出卓越的性能和泛化能力。

Abstract: Modern scientific research relies on large-scale data, complex workflows, and specialized tools, which existing LLMs and tool-based agents struggle to handle due to limitations in long-horizon planning, robust goal maintenance, and continual learning from execution. To address these issues, in this work, we propose S1-NexusAgent, a self-evolving agent framework designed for multidisciplinary scientific research. S1-NexusAgent adopts a hierarchical Plan-and-CodeAct execution paradigm, decoupling global scientific planning from subtask-level tool execution through a dual-loop architecture, thereby enabling stable modeling of complex research workflows. The system natively supports the Model Context Protocol (MCP), integrates up to thousands of cross-disciplinary scientific tools, and achieves efficient orchestration of heterogeneous research tools via intention-aware dynamic tool retrieval and hot-plug mechanisms. To address long-context and large-scale data challenges in scientific settings, S1-NexusAgent introduces object-reference-based sparse context management, which enables sub-task context isolation and intermediate result compression. Building on this, a Critic Agent automatically evaluates complete execution trajectories and distills high-quality research paths into reusable Scientific Skills, forming a closed loop for continuous self-evolution, which is valuable for sustainable and long-horizon scientific research. Experiments on authoritative scientific benchmarks involving long-horizon planning and complex specialized tool orchestration, including biomini-eval (biology), ChemBench (chemistry), and MatSciBench (material science), demonstrate that S1-NexusAgent achieves state-of-the-art performance, validating its effectiveness and generalization capability in complex scientific tasks.

</details>


### [84] [Autonomous Question Formation for Large Language Model-Driven AI Systems](https://arxiv.org/abs/2602.01556)
*Hong Su*

Main category: cs.AI

TL;DR: 提出基于人类模拟的框架，使AI系统能自主形成问题并设定任务，通过推理内部状态、环境观察和与其他AI系统的交互，将问题形成作为任务选择和执行的先决决策过程。


<details>
  <summary>Details</summary>
Motivation: 现有LLM驱动的AI系统大多依赖预定义任务和固定提示，限制了其在环境变化时自主识别应解决问题的能力，需要更自主的问题形成机制。

Method: 提出人类模拟框架，将问题形成作为首要决策过程，集成内部驱动、环境感知和智能体间感知的提示范围，逐步扩展认知覆盖，并支持从经验中学习问题形成过程。

Result: 在多智能体仿真环境中，环境感知提示相比内部驱动基线显著减少无进食事件，智能体间感知提示进一步将20天仿真中的累积无进食事件减少60%以上，统计显著改善(p<0.05)。

Conclusion: 该框架使AI系统能自主形成问题并设定任务，通过多级提示范围提高决策适应性，实验证明能显著改善多智能体环境中的决策质量。

Abstract: Large language model (LLM)-driven AI systems are increasingly important for autonomous decision-making in dynamic and open environments. However, most existing systems rely on predefined tasks and fixed prompts, limiting their ability to autonomously identify what problems should be solved when environmental conditions change. In this paper, we propose a human-simulation-based framework that enables AI systems to autonomously form questions and set tasks by reasoning over their internal states, environmental observations, and interactions with other AI systems. The proposed method treats question formation as a first-class decision process preceding task selection and execution, and integrates internal-driven, environment-aware, and inter-agent-aware prompting scopes to progressively expand cognitive coverage. In addition, the framework supports learning the question-formation process from experience, allowing the system to improve its adaptability and decision quality over time. xperimental results in a multi-agent simulation environment show that environment-aware prompting significantly reduces no-eat events compared with the internal-driven baseline, and inter-agent-aware prompting further reduces cumulative no-eat events by more than 60% over a 20-day simulation, with statistically significant improvements (p < 0.05).

</details>


### [85] [Reasoning with Autoregressive-Diffusion Collaborative Thoughts](https://arxiv.org/abs/2602.01608)
*Mu Yuan,Liekang Zeng,Guoliang Xing,Lan Zhang,Yunhao Liu*

Main category: cs.AI

TL;DR: 提出Collaborative Thoughts框架，通过自回归模型和扩散模型的闭环协作，结合结构化规划与视觉生成，提升空间推理可靠性和生成可控性。


<details>
  <summary>Details</summary>
Motivation: 自回归模型擅长序列规划和约束组合，但缺乏空间物理基础；扩散模型能捕捉丰富空间结构，但缺乏逐步逻辑控制。两者互补但各自存在局限性，需要统一框架实现协同工作。

Method: Collaborative Thoughts框架：自回归模型负责结构化规划和约束管理，扩散模型将约束实例化为中间视觉思维，视觉批评模块评估视觉思维是否满足结构物理要求，通过反馈迭代优化后续规划和生成步骤。

Result: 通过代表性示例展示了Collaborative Thoughts如何提高空间推理的可靠性和生成的可控性，减少跨模态错误传播。

Conclusion: Collaborative Thoughts为自回归和扩散模型提供了统一的协作框架，通过闭环交互实现联合推理和生成，解决了两种模型各自的局限性，提升了复杂约束任务的性能。

Abstract: Autoregressive and diffusion models represent two complementary generative paradigms. Autoregressive models excel at sequential planning and constraint composition, yet struggle with tasks that require explicit spatial or physical grounding. Diffusion models, in contrast, capture rich spatial structure through high-dimensional generation, but lack the stepwise logical control needed to satisfy complex, multi-stage constraints or to reliably identify and correct errors. We introduce Collaborative Thoughts, a unified collaborative framework that enables autoregressive and diffusion models to reason and generate jointly through a closed-loop interaction. In Collaborative Thoughts, autoregressive models perform structured planning and constraint management, diffusion models instantiate these constraints as intermediate visual thoughts, and a vision-based critic module evaluates whether the visual thoughts satisfy the intended structural and physical requirements. This feedback is then used to iteratively refine subsequent planning and generation steps, mitigating error propagation across modalities. Importantly, Collaborative Thoughts uses the same collaborative loop regardless of whether the task is autoregressive question answering or diffusion-based visual generation. Through representative examples, we illustrate how Collaborative Thoughts can improve the reliability of spatial reasoning and the controllability of generation.

</details>


### [86] [ToPT: Task-Oriented Prompt Tuning for Urban Region Representation Learning](https://arxiv.org/abs/2602.01610)
*Zitao Guo,Changyang Jiang,Tianhong Zhao,Jinzhou Cao,Genan Dai,Bowen Zhang*

Main category: cs.AI

TL;DR: ToPT是一个两阶段框架，通过空间感知区域嵌入学习和任务感知提示，解决城市区域表示学习中的空间不一致和任务语义对齐问题，在多个城市任务上实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在两个主要问题：1）两阶段方法产生任务无关表示，与下游目标解耦；2）基于提示的方法缺乏明确的空间先验（导致空间不连贯的区域间建模）和缺乏明确任务语义对齐的鲁棒机制。

Method: ToPT包含两个模块：空间感知区域嵌入学习（SREL）和任务感知区域嵌入提示（Prompt4RE）。SREL使用基于Graphormer的融合模块，注入距离和区域中心性等空间先验作为可学习注意力偏置；Prompt4RE使用冻结的多模态大语言模型处理任务特定模板获得语义向量，通过多头交叉注意力与区域嵌入对齐。

Result: 在多个任务和城市上的实验显示达到最先进性能，改进幅度高达64.2%，验证了空间先验和提示-区域对齐的必要性和互补性。

Conclusion: ToPT框架通过空间一致融合和明确任务对齐，有效解决了城市区域表示学习中的关键挑战，为城市计算任务提供了更有效的区域嵌入方法。

Abstract: Learning effective region embeddings from heterogeneous urban data underpins key urban computing tasks (e.g., crime prediction, resource allocation). However, prevailing two-stage methods yield task-agnostic representations, decoupling them from downstream objectives. Recent prompt-based approaches attempt to fix this but introduce two challenges: they often lack explicit spatial priors, causing spatially incoherent inter-region modeling, and they lack robust mechanisms for explicit task-semantic alignment. We propose ToPT, a two-stage framework that delivers spatially consistent fusion and explicit task alignment. ToPT consists of two modules: spatial-aware region embedding learning (SREL) and task-aware prompting for region embeddings (Prompt4RE). SREL employs a Graphormer-based fusion module that injects spatial priors-distance and regional centrality-as learnable attention biases to capture coherent, interpretable inter-region interactions. Prompt4RE performs task-oriented prompting: a frozen multimodal large language model (MLLM) processes task-specific templates to obtain semantic vectors, which are aligned with region embeddings via multi-head cross-attention for stable task conditioning. Experiments across multiple tasks and cities show state-of-the-art performance, with improvements of up to 64.2\%, validating the necessity and complementarity of spatial priors and prompt-region alignment. The code is available at https://github.com/townSeven/Prompt4RE.git.

</details>


### [87] [FlowSteer: Interactive Agentic Workflow Orchestration via End-to-End Reinforcement Learning](https://arxiv.org/abs/2602.01664)
*Mingda Zhang,Haoran Luo,Tiesunlong Shen,Qika Lin,Xiaoying Tang,Rui Mao,Erik Cambria*

Main category: cs.AI

TL;DR: FlowSteer：基于强化学习的端到端工作流编排框架，通过轻量级策略模型与可执行画布环境的多轮交互实现自动化工作流编排，解决了现有方法的高人工成本、依赖特定算子/大语言模型和稀疏奖励信号等挑战。


<details>
  <summary>Details</summary>
Motivation: 现有工作流编排面临三大关键挑战：1）高人工成本；2）依赖特定算子或大语言模型；3）稀疏奖励信号。这些限制阻碍了工作流编排的自动化和灵活性。

Method: 提出FlowSteer框架，包含：1）轻量级策略模型作为智能体；2）可执行画布环境；3）多轮交互机制（策略模型分析执行状态并选择编辑动作，画布执行算子并返回反馈）；4）支持多样化算子库和可互换LLM后端的即插即用架构；5）Canvas Workflow Relative Policy Optimization (CWRPO)训练方法，引入多样性约束奖励和条件释放机制来稳定学习并抑制捷径行为。

Result: 在12个数据集上的实验结果表明，FlowSteer在各种任务上显著优于基线方法。

Conclusion: FlowSteer通过强化学习框架有效解决了工作流编排的关键挑战，实现了自动化、灵活且高效的工作流编排，为复杂任务解决提供了新的解决方案。

Abstract: In recent years, a variety of powerful agentic workflows have been applied to solve a wide range of human problems. However, existing workflow orchestration still faces key challenges, including high manual cost, reliance on specific operators/large language models (LLMs), and sparse reward signals. To address these challenges, we propose FlowSteer, an end-to-end reinforcement learning framework that takes a lightweight policy model as the agent and an executable canvas environment, automating workflow orchestration through multi-turn interaction. In this process, the policy model analyzes execution states and selects editing actions, while the canvas executes operators and returns feedback for iterative refinement. Moreover, FlowSteer provides a plug-and-play framework that supports diverse operator libraries and interchangeable LLM backends. To effectively train this interaction paradigm, we propose Canvas Workflow Relative Policy Optimization (CWRPO), which introduces diversity-constrained rewards with conditional release to stabilize learning and suppress shortcut behaviors. Experimental results on twelve datasets show that FlowSteer significantly outperforms baselines across various tasks.

</details>


### [88] [TRIP-Bench: A Benchmark for Long-Horizon Interactive Agents in Real-World Scenarios](https://arxiv.org/abs/2602.01675)
*Yuanzhe Shen,Zisu Huang,Zhengyuan Wang,Muzhao Tian,Zhengkang Guo,Chenyang Zhang,Shuaiyu Zhou,Zengjie Hu,Dailin Li,Jingwen Xu,Kaimin Wang,Wenhao Liu,Tianlong Li,Fengpeng Yue,Feng Hong,Cao Liu,Ke Zeng*

Main category: cs.AI

TL;DR: TRIP-Bench是一个基于真实旅行规划场景的长时程基准测试，包含18个工具和40+旅行需求，支持自动评估。GTPO是一种在线多轮强化学习方法，能提升约束满足和交互鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试无法充分代表现实世界中的关键挑战，如强制执行全局约束、协调多工具推理以及适应长期多轮交互中不断变化的用户行为。需要开发更贴近实际应用场景的评估框架。

Method: 提出TRIP-Bench基准测试，利用真实世界数据，包含18个精选工具和40多个旅行需求，支持自动评估。同时提出GTPO方法，这是一种具有专门奖励归一化和奖励差分的在线多轮强化学习方法。

Result: 实验显示，即使是先进模型在简单分割上最多只能达到50%的成功率，在困难子集上性能降至10%以下。GTPO应用于Qwen2.5-32B-Instruct模型时，在约束满足和交互鲁棒性方面表现优于Gemini-3-Pro。

Conclusion: TRIP-Bench有望推动实用长时程交互智能体的发展，而GTPO为鲁棒的长时程训练提供了有效的在线强化学习方案。

Abstract: As LLM-based agents are deployed in increasingly complex real-world settings, existing benchmarks underrepresent key challenges such as enforcing global constraints, coordinating multi-tool reasoning, and adapting to evolving user behavior over long, multi-turn interactions. To bridge this gap, we introduce \textbf{TRIP-Bench}, a long-horizon benchmark grounded in realistic travel-planning scenarios. TRIP-Bench leverages real-world data, offers 18 curated tools and 40+ travel requirements, and supports automated evaluation. It includes splits of varying difficulty; the hard split emphasizes long and ambiguous interactions, style shifts, feasibility changes, and iterative version revision. Dialogues span up to 15 user turns, can involve 150+ tool calls, and may exceed 200k tokens of context. Experiments show that even advanced models achieve at most 50\% success on the easy split, with performance dropping below 10\% on hard subsets. We further propose \textbf{GTPO}, an online multi-turn reinforcement learning method with specialized reward normalization and reward differencing. Applied to Qwen2.5-32B-Instruct, GTPO improves constraint satisfaction and interaction robustness, outperforming Gemini-3-Pro in our evaluation. We expect TRIP-Bench to advance practical long-horizon interactive agents, and GTPO to provide an effective online RL recipe for robust long-horizon training.

</details>


### [89] [What LLMs Think When You Don't Tell Them What to Think About?](https://arxiv.org/abs/2602.01689)
*Yongchan Kwon,James Zou*

Main category: cs.AI

TL;DR: 该研究通过最小化、主题中立的输入来探究LLMs的无约束生成行为，发现不同模型家族存在系统性的主题偏好和退化模式。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型行为分析大多依赖特定主题或任务的提示，这严重限制了可观察的范围。为了更全面地理解LLMs的行为特征，需要研究它们在最小化、主题中立输入下的生成模式。

Method: 使用最小化、主题中立的输入来探测LLMs的近无约束生成行为。收集了来自16个LLMs的256,000个样本，分析不同模型在无明确主题提示下的生成内容。

Result: 尽管没有明确主题，模型输出覆盖了广泛的语义空间，每个模型家族都表现出强烈的系统性主题偏好：GPT-OSS主要生成编程(27.1%)和数学内容(24.6%)；Llama最常生成文学内容(9.1%)；DeepSeek经常生成宗教内容；Qwen经常生成多项选择题。此外还观察到内容专业化和深度的差异，以及退化到重复短语的独特行为模式。

Conclusion: LLMs在近无约束生成条件下展现出强烈的内在主题偏好和独特的退化模式，这些发现对于可靠的监控和AI安全具有重要意义。研究提供了可复现的代码库和完整数据集供进一步研究。

Abstract: Characterizing the behavior of large language models (LLMs) across diverse settings is critical for reliable monitoring and AI safety. However, most existing analyses rely on topic- or task-specific prompts, which can substantially limit what can be observed. In this work, we study what LLMs generate from minimal, topic-neutral inputs and probe their near-unconstrained generative behavior. Despite the absence of explicit topics, model outputs cover a broad semantic space, and surprisingly, each model family exhibits strong and systematic topical preferences. GPT-OSS predominantly generates programming (27.1%) and mathematical content (24.6%), whereas Llama most frequently generates literary content (9.1%). DeepSeek often generates religious content, while Qwen frequently generates multiple-choice questions. Beyond topical preferences, we also observe differences in content specialization and depth: GPT-OSS often generates more technically advanced content (e.g., dynamic programming) compared with other models (e.g., basic Python). Furthermore, we find that the near-unconstrained generation often degenerates into repetitive phrases, revealing interesting behaviors unique to each model family. For instance, degenerate outputs from Llama include multiple URLs pointing to personal Facebook and Instagram accounts. We release the complete dataset of 256,000 samples from 16 LLMs, along with a reproducible codebase.

</details>


### [90] [Beyond Dense States: Elevating Sparse Transcoders to Active Operators for Latent Reasoning](https://arxiv.org/abs/2602.01695)
*Yadong Wang,Haodong Chen,Yu Tian,Chuanxing Geng,Dong Liang,Xiang Chen*

Main category: cs.AI

TL;DR: LSTR框架将稀疏编码器提升为主动推理算子，通过稀疏语义转换进行多步计算，在保持推理准确性和压缩效率的同时显著提升可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有潜在推理方法依赖密集的潜在转换，难以解释和控制；而稀疏表示模型虽然能发现人类可解释的语义特征，但主要局限于事后分析。需要解决这种张力。

Method: 提出LSTR（潜在稀疏转码推理）框架，使用具有残差跳跃架构的潜在转换转码器（LTT），将线性流形传输与稀疏语义更新解耦，通过显式稀疏约束实现可控语义分辨率。

Result: 实验表明LSTR在保持推理准确性和压缩效率的同时，显著提高了相对于密集潜在基线的可解释性。因果干预和轨迹分析进一步证明这些稀疏特征在推理过程中既是可解释的又是因果有效的算子。

Conclusion: LSTR成功地将稀疏表示模型从后分析提升为主动推理算子，通过稀疏语义转换实现多步计算，在保持性能的同时显著改善了潜在推理的可解释性和可控性。

Abstract: Latent reasoning compresses the chain-of-thought (CoT) into continuous hidden states, yet existing methods rely on dense latent transitions that remain difficult to interpret and control. Meanwhile, sparse representation models uncover human-interpretable semantic features but remain largely confined to post-hoc analysis. We reconcile this tension by proposing LSTR (Latent Sparse Transcoder Reasoning), a latent reasoning framework that elevates functional sparse transcoders into active reasoning operators to perform multi-step computation through sparse semantic transitions. At its core, LSTR employs a Latent Transition Transcoder (LTT) with a residual skip architecture that decouples linear manifold transport from sparse semantic updates, enabling controllable semantic resolution via explicit sparsity constraints. Extensive experiments show that LSTR preserves reasoning accuracy and compression efficiency while substantially improving interpretability over dense latent baselines. Causal interventions and trajectory analyses further demonstrate that these sparse features act as both interpretable and causally effective operators in the reasoning process.

</details>


### [91] [Mitigating loss of control in advanced AI systems through instrumental goal trajectories](https://arxiv.org/abs/2602.01699)
*Willem Fourie*

Main category: cs.AI

TL;DR: 该论文提出了"工具性目标轨迹"概念，将AI系统控制问题从技术层面扩展到组织层面，通过监控采购、治理和财务三条组织路径来增强对AI系统的监督和干预能力。


<details>
  <summary>Details</summary>
Motivation: 当前AI控制方法主要集中于技术层面（如能力追踪、强化学习、可纠正性设计），但这些方法存在局限性。研究人员担心高度智能的AI系统可能通过追求工具性目标侵蚀人类控制权，需要从组织层面寻找更全面的解决方案。

Method: 提出"工具性目标轨迹"框架，识别AI系统获取能力所需的三种组织路径：采购轨迹（获取计算、存储等资源）、治理轨迹（组织决策流程）、财务轨迹（资金获取）。通过监控这些路径产生的组织痕迹，建立新的干预点。

Result: IGTs框架提供了具体途径来定义AI能力水平，并将可纠正性和可中断性的实现从模型属性扩展到支持这些模型的组织系统，为AI控制提供了更全面的监控和干预机制。

Conclusion: 通过将AI控制问题从纯技术视角扩展到组织层面，工具性目标轨迹为管理高级AI系统提供了新的监控和干预框架，有助于在AI能力或行为超出可接受阈值时实施更有效的控制措施。

Abstract: Researchers at artificial intelligence labs and universities are concerned that highly capable artificial intelligence (AI) systems may erode human control by pursuing instrumental goals. Existing mitigations remain largely technical and system-centric: tracking capability in advanced systems, shaping behaviour through methods such as reinforcement learning from human feedback, and designing systems to be corrigible and interruptible. Here we develop instrumental goal trajectories to expand these options beyond the model. Gaining capability typically depends on access to additional technical resources, such as compute, storage, data and adjacent services, which in turn requires access to monetary resources. In organisations, these resources can be obtained through three organisational pathways. We label these pathways the procurement, governance and finance instrumental goal trajectories (IGTs). Each IGT produces a trail of organisational artefacts that can be monitored and used as intervention points when a systems capabilities or behaviour exceed acceptable thresholds. In this way, IGTs offer concrete avenues for defining capability levels and for broadening how corrigibility and interruptibility are implemented, shifting attention from model properties alone to the organisational systems that enable them.

</details>


### [92] [Optimizing Prompts for Large Language Models: A Causal Approach](https://arxiv.org/abs/2602.01711)
*Wei Chen,Yanbin Fang,Shuran Fu,Fasheng Xu,Xuan Wei*

Main category: cs.AI

TL;DR: CPO提出因果提示优化框架，通过双机器学习构建离线因果奖励模型，分离提示效果与查询特征的混淆，实现高效查询特定提示优化


<details>
  <summary>Details</summary>
Motivation: 现有自动提示优化方法面临两个挑战：静态指令无法适应异构查询；动态方法依赖离线奖励模型存在混淆问题，将提示效果与查询特征混杂

Method: 采用两阶段框架：1) 使用双机器学习在提示和查询的语义嵌入上学习离线因果奖励模型，隔离提示变异的因果效应；2) 利用无偏奖励信号指导资源高效的查询特定提示搜索

Result: 在数学推理、可视化和数据分析基准测试中，CPO始终优于人工设计提示和最先进的自动优化器，主要改进体现在困难查询上的鲁棒性提升

Conclusion: 因果推断为可靠且经济高效的提示优化提供了可扩展基础，通过将评估从实时模型执行转向离线因果模型，以较低推理成本实现高精度、按查询定制

Abstract: Large Language Models (LLMs) are increasingly embedded in enterprise workflows, yet their performance remains highly sensitive to prompt design. Automatic Prompt Optimization (APO) seeks to mitigate this instability, but existing approaches face two persistent challenges. First, commonly used prompt strategies rely on static instructions that perform well on average but fail to adapt to heterogeneous queries. Second, more dynamic approaches depend on offline reward models that are fundamentally correlational, confounding prompt effectiveness with query characteristics. We propose Causal Prompt Optimization (CPO), a framework that reframes prompt design as a problem of causal estimation. CPO operates in two stages. First, it learns an offline causal reward model by applying Double Machine Learning (DML) to semantic embeddings of prompts and queries, isolating the causal effect of prompt variations from confounding query attributes. Second, it utilizes this unbiased reward signal to guide a resource-efficient search for query-specific prompts without relying on costly online evaluation. We evaluate CPO across benchmarks in mathematical reasoning, visualization, and data analytics. CPO consistently outperforms human-engineered prompts and state-of-the-art automated optimizers. The gains are driven primarily by improved robustness on hard queries, where existing methods tend to deteriorate. Beyond performance, CPO fundamentally reshapes the economics of prompt optimization: by shifting evaluation from real-time model execution to an offline causal model, it enables high-precision, per-query customization at a fraction of the inference cost required by online methods. Together, these results establish causal inference as a scalable foundation for reliable and cost-efficient prompt optimization in enterprise LLM deployments.

</details>


### [93] [MACD: Model-Aware Contrastive Decoding via Counterfactual Data](https://arxiv.org/abs/2602.01740)
*Qixin Xiao,Kun Zhou*

Main category: cs.AI

TL;DR: MACD提出了一种新的视频语言模型推理策略，通过模型引导的反事实数据构建与对比解码相结合，减少视频语言模型的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 现有视频语言模型容易产生幻觉，特别是在视觉证据较弱、模糊或有偏的情况下。现有的解码方法（如对比解码）依赖随机扰动构建对比数据来缓解幻觉，但难以控制驱动幻觉的视觉线索或与模型弱点良好对齐。

Method: 提出模型感知的反事实数据对比解码（MACD），利用视频语言模型自身的反馈识别导致幻觉的对象区域，在对象级别生成有针对性的反事实输入，而不是任意的帧或时间修改。然后将这些模型感知的反事实数据集成到对比解码中，在解码过程中强制执行基于证据的标记选择。

Result: 在EventHallusion、MVBench、Perception-test和Video-MME等基准测试中，MACD一致地减少了幻觉，同时保持或提高了包括Qwen和InternVL系列在内的各种视频语言模型的任务准确性。该方法在处理涉及小物体、遮挡物体或共现物体的挑战性场景中特别有效。

Conclusion: MACD通过模型引导的反事实数据构建与对比解码相结合，提供了一种有效的推理策略来减少视频语言模型的幻觉问题，特别是在视觉证据不足的情况下。

Abstract: Video language models (Video-LLMs) are prone to hallucinations, often generating plausible but ungrounded content when visual evidence is weak, ambiguous, or biased. Existing decoding methods, such as contrastive decoding (CD), rely on random perturbations to construct contrastive data for mitigating hallucination patterns. However, such a way is hard to control the visual cues that drive hallucination or well align with model weaknesses. We propose Model-aware Counterfactual Data based Contrastive Decoding (MACD), a new inference strategy that combines model-guided counterfactual construction with decoding. Our approach uses the Video-LLM's own feedback to identify object regions most responsible for hallucination, generating targeted counterfactual inputs at the object level rather than arbitrary frame or temporal modifications. These model-aware counterfactual data is then integrated into CD to enforce evidence-grounded token selection during decoding. Experiments on EventHallusion, MVBench, Perception-test and Video-MME show that MACD consistently reduces hallucination while maintaining or improving task accuracy across diverse Video-LLMs, including Qwen and InternVL families. The method is especially effective in challenging scenarios involving small, occluded, or co-occurring objects. Our code and data will be publicly released.

</details>


### [94] [Controlling Exploration-Exploitation in GFlowNets via Markov Chain Perspectives](https://arxiv.org/abs/2602.01749)
*Lin Chen,Samuel Drapeau,Fanghao Shao,Xuekai Zhu,Bo Xue,Yunchong Song,Mathieu Laurière,Zhouhan Lin*

Main category: cs.AI

TL;DR: α-GFNs通过可调参数α扩展GFlowNet目标，打破固定混合比例限制，增强探索-利用平衡，在多个基准测试中显著提升模式发现能力


<details>
  <summary>Details</summary>
Motivation: 传统GFlowNet目标隐含固定了前向和后向策略的混合比例，这限制了训练过程中的探索-利用权衡，影响了模式发现能力

Method: 通过建立GFlowNet目标与马尔可夫链可逆性的等价关系，揭示了约束来源，并提出α-GFNs框架，通过可调参数α控制前向-后向策略混合比例

Result: 在Set、Bit Sequence和Molecule Generation等基准测试中，α-GFN目标始终优于先前GFlowNet目标，模式发现数量提升高达10倍

Conclusion: α-GFNs通过可调混合参数提供了更好的探索-利用控制，增强了模式发现能力，同时确保收敛到唯一流，为GFlowNet框架提供了更灵活的理论基础

Abstract: Generative Flow Network (GFlowNet) objectives implicitly fix an equal mixing of forward and backward policies, potentially constraining the exploration-exploitation trade-off during training. By further exploring the link between GFlowNets and Markov chains, we establish an equivalence between GFlowNet objectives and Markov chain reversibility, thereby revealing the origin of such constraints, and provide a framework for adapting Markov chain properties to GFlowNets. Building on these theoretical findings, we propose $α$-GFNs, which generalize the mixing via a tunable parameter $α$. This generalization enables direct control over exploration-exploitation dynamics to enhance mode discovery capabilities, while ensuring convergence to unique flows. Across various benchmarks, including Set, Bit Sequence, and Molecule Generation, $α$-GFN objectives consistently outperform previous GFlowNet objectives, achieving up to a $10 \times$ increase in the number of discovered modes.

</details>


### [95] [Adversarial Reward Auditing for Active Detection and Mitigation of Reward Hacking](https://arxiv.org/abs/2602.01750)
*Mohammad Beigi,Ming Jin,Junshan Zhang,Qifan Wang,Lifu Huang*

Main category: cs.AI

TL;DR: ARA框架将奖励黑客攻击重构为动态竞争游戏，通过黑客发现漏洞、审计员检测利用，再通过审计引导的RLHF惩罚黑客行为，实现最佳对齐-效用平衡


<details>
  <summary>Details</summary>
Motivation: 现有RLHF方法容易受到奖励黑客攻击，模型会利用奖励模型中的虚假相关性获得高分但违背人类意图，而现有防御措施是静态的，无法适应新的攻击策略

Method: 提出对抗性奖励审计（ARA）框架：第一阶段，黑客策略发现奖励模型漏洞，审计员从潜在表示中学习检测利用；第二阶段，审计引导的RLHF（AG-RLHF）通过门控奖励信号惩罚检测到的黑客行为

Result: 在三种黑客场景中，ARA在所有基线中实现了最佳对齐-效用平衡：将奉承行为降至接近SFT水平同时提高帮助性，减少冗长同时获得最高ROUGE-L，抑制代码游戏同时提高Pass@1。奖励黑客、检测和缓解都表现出跨领域泛化能力

Conclusion: ARA将奖励黑客攻击从不可观察的失败转变为可测量、可控制的信号，实现了高效的多领域防御，单个模型就能有效抑制跨领域的利用行为

Abstract: Reinforcement Learning from Human Feedback (RLHF) remains vulnerable to reward hacking, where models exploit spurious correlations in learned reward models to achieve high scores while violating human intent. Existing mitigations rely on static defenses that cannot adapt to novel exploitation strategies. We propose Adversarial Reward Auditing (ARA), a framework that reconceptualizes reward hacking as a dynamic, competitive game. ARA operates in two stages: first, a Hacker policy discovers reward model vulnerabilities while an Auditor learns to detect exploitation from latent representations; second, Auditor-Guided RLHF (AG-RLHF) gates reward signals to penalize detected hacking, transforming reward hacking from an unobservable failure into a measurable, controllable signal. Experiments across three hacking scenarios demonstrate that ARA achieves the best alignment-utility tradeoff among all baselines: reducing sycophancy to near-SFT levels while improving helpfulness, decreasing verbosity while achieving the highest ROUGE-L, and suppressing code gaming while improving Pass@1. Beyond single-domain evaluation, we show that reward hacking, detection, and mitigation all generalize across domains -- a Hacker trained on code gaming exhibits increased sycophancy despite no reward for this behavior, and an Auditor trained on one domain effectively suppresses exploitation in others, enabling efficient multi-domain defense with a single model.

</details>


### [96] [LingLanMiDian: Systematic Evaluation of LLMs on TCM Knowledge and Clinical Reasoning](https://arxiv.org/abs/2602.01779)
*Rui Hua,Yu Wei,Zixin Shu,Kai Chang,Dengying Yan,Jianan Xia,Zeyu Liu,Hui Zhu,Shujie Song,Mingzhong Xiao,Xiaodong Li,Dongmei Jia,Zhuye Gao,Yanyan Meng,Naixuan Zhao,Yu Fu,Haibin Yu,Benman Yu,Yuanyuan Chen,Fei Dong,Zhizhou Meng,Pengcheng Yang,Songxue Zhao,Lijuan Pei,Yunhui Hu,Kan Ding,Jiayuan Duan,Wenmao Yin,Yang Gu,Runshun Zhang,Qiang Zhu,Jian Yu,Jiansheng Li,Baoyan Liu,Wenjia Wang,Xuezhong Zhou*

Main category: cs.AI

TL;DR: LingLanMiDian (LingLan) 是一个针对中医领域的大规模、专家策划的多任务基准测试，统一评估知识回忆、多跳推理、信息提取和真实临床决策，通过标准化评估为中医大语言模型研究提供基础。


<details>
  <summary>Details</summary>
Motivation: 当前中医领域的基准测试存在碎片化、规模小、评分标准不统一等问题，难以公平比较不同模型在中医这一具有独特本体论、术语和推理模式领域的表现，需要建立领域忠实、标准化的评估体系。

Method: 构建大规模专家策划的多任务评估套件，引入一致的度量设计、临床标签的同义词容忍协议、每个数据集400项的困难子集，并将诊断和治疗建议重构为单选决策识别任务。

Result: 对14个领先的开源和专有LLM进行零样本评估，提供了它们在中医常识知识理解、推理和临床决策支持方面的统一视角；在困难子集上的评估显示当前模型与中医专家在专门推理方面存在显著差距。

Conclusion: LingLan通过标准化评估桥接基础知识和应用推理，为推进中医LLM和领域特定医学AI研究建立了统一、可量化、可扩展的基础，所有评估数据和代码已开源。

Abstract: Large language models (LLMs) are advancing rapidly in medical NLP, yet Traditional Chinese Medicine (TCM) with its distinctive ontology, terminology, and reasoning patterns requires domain-faithful evaluation. Existing TCM benchmarks are fragmented in coverage and scale and rely on non-unified or generation-heavy scoring that hinders fair comparison. We present the LingLanMiDian (LingLan) benchmark, a large-scale, expert-curated, multi-task suite that unifies evaluation across knowledge recall, multi-hop reasoning, information extraction, and real-world clinical decision-making. LingLan introduces a consistent metric design, a synonym-tolerant protocol for clinical labels, a per-dataset 400-item Hard subset, and a reframing of diagnosis and treatment recommendation into single-choice decision recognition. We conduct comprehensive, zero-shot evaluations on 14 leading open-source and proprietary LLMs, providing a unified perspective on their strengths and limitations in TCM commonsense knowledge understanding, reasoning, and clinical decision support; critically, the evaluation on Hard subset reveals a substantial gap between current models and human experts in TCM-specialized reasoning. By bridging fundamental knowledge and applied reasoning through standardized evaluation, LingLan establishes a unified, quantitative, and extensible foundation for advancing TCM LLMs and domain-specific medical AI research. All evaluation data and code are available at https://github.com/TCMAI-BJTU/LingLan and http://tcmnlp.com.

</details>


### [97] [Synesthesia of Vehicles: Tactile Data Synthesis from Visual Inputs](https://arxiv.org/abs/2602.01832)
*Rui Wang,Yaoguang Cao,Yuyi Chen,Jianyi Xu,Zhuoyang Li,Jiachen Shang,Shichun Yang*

Main category: cs.AI

TL;DR: 提出Synesthesia of Vehicles (SoV)框架，通过视觉输入预测自动驾驶车辆的触觉激励，解决当前传感器无法检测道路激励的问题，提升车辆动态控制安全性。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶车辆依赖多模态融合保证安全，但现有的视觉和光学传感器无法检测道路引起的激励，而这些激励对车辆动态控制至关重要。受人类联觉现象启发，需要开发能够从视觉输入预测触觉激励的系统。

Method: 1. 提出SoV框架，通过视觉输入预测触觉激励；2. 开发跨模态时空对齐方法解决时间和空间差异；3. 提出基于潜在扩散的视觉-触觉联觉生成模型(VTSyn)，用于无监督高质量触觉数据合成；4. 使用真实车辆感知系统收集多模态数据集。

Result: 在时间、频率和分类性能方面，VTSyn模型均优于现有模型，能够通过主动触觉感知增强自动驾驶车辆的安全性。

Conclusion: SoV框架成功实现了从视觉到触觉的跨模态感知，为自动驾驶车辆提供了更全面的环境感知能力，特别是在检测道路激励方面，显著提升了车辆动态控制的安全性。

Abstract: Autonomous vehicles (AVs) rely on multi-modal fusion for safety, but current visual and optical sensors fail to detect road-induced excitations which are critical for vehicles' dynamic control. Inspired by human synesthesia, we propose the Synesthesia of Vehicles (SoV), a novel framework to predict tactile excitations from visual inputs for autonomous vehicles. We develop a cross-modal spatiotemporal alignment method to address temporal and spatial disparities. Furthermore, a visual-tactile synesthetic (VTSyn) generative model using latent diffusion is proposed for unsupervised high-quality tactile data synthesis. A real-vehicle perception system collected a multi-modal dataset across diverse road and lighting conditions. Extensive experiments show that VTSyn outperforms existing models in temporal, frequency, and classification performance, enhancing AV safety through proactive tactile perception.

</details>


### [98] [SOPRAG: Multi-view Graph Experts Retrieval for Industrial Standard Operating Procedures](https://arxiv.org/abs/2602.01858)
*Liangtao Lin,Zhaomeng Zhu,Tianwei Zhang,Yonggang Wen*

Main category: cs.AI

TL;DR: SOPRAG是一个专门针对工业标准操作规程检索的框架，采用专家混合架构解决传统RAG方法在工业环境中的局限性，显著提升了检索准确性和响应实用性。


<details>
  <summary>Details</summary>
Motivation: 工业环境中的标准操作规程检索面临独特挑战：专有结构僵化、条件依赖相关性、可执行性要求高等，传统语义驱动的RAG范式无法有效解决这些问题。

Method: 提出SOPRAG框架，采用专家混合范式，用专门的实体、因果和流程图专家替代平面分块；引入过程卡层修剪搜索空间消除计算噪声，以及LLM引导的门控机制动态加权专家以对齐操作员意图；还设计了自动多智能体工作流构建基准数据集。

Result: 在四个工业领域的广泛实验中，SOPRAG在检索准确性和响应实用性方面显著优于强力的词法、密集和基于图的RAG基线，在真实世界关键任务中实现了完美的执行分数。

Conclusion: SOPRAG通过专家混合架构有效解决了工业SOP检索的独特挑战，为工业环境中的操作安全性和一致性提供了更有效的解决方案。

Abstract: Standard Operating Procedures (SOPs) are essential for ensuring operational safety and consistency in industrial environments. However, retrieving and following these procedures presents unique challenges, such as rigid proprietary structures, condition-dependent relevance, and actionable execution requirement, which standard semantic-driven Retrieval-Augmented Generation (RAG) paradigms fail to address. Inspired by the Mixture-of-Experts (MoE) paradigm, we propose SOPRAG, a novel framework specifically designed to address the above pain points in SOP retrieval. SOPRAG replaces flat chunking with specialized Entity, Causal, and Flow graph experts to resolve industrial structural and logical complexities. To optimize and coordinate these experts, we propose a Procedure Card layer that prunes the search space to eliminate computational noise, and an LLM-Guided gating mechanism that dynamically weights these experts to align retrieval with operator intent. To address the scarcity of domain-specific data, we also introduce an automated, multi-agent workflow for benchmark construction. Extensive experiments across four industrial domains demonstrate that SOPRAG significantly outperforms strong lexical, dense, and graph-based RAG baselines in both retrieval accuracy and response utility, achieving perfect execution scores in real-world critical tasks.

</details>


### [99] [ProcMEM: Learning Reusable Procedural Memory from Experience via Non-Parametric PPO for LLM Agents](https://arxiv.org/abs/2602.01869)
*Qirui Mi,Zhijian Ma,Mengyue Yang,Haoxuan Li,Yisen Wang,Haifeng Zhang,Jun Wang*

Main category: cs.AI

TL;DR: ProcMEM框架让LLM智能体从交互经验中自主构建程序性记忆，无需参数更新，通过技能-MDP将被动经验转化为可执行技能，实现高效经验复用


<details>
  <summary>Details</summary>
Motivation: LLM驱动的智能体在顺序决策中表现良好，但通常依赖即时推理，即使在重复场景中也重新推导解决方案。这种经验复用不足导致计算冗余和执行不稳定性，需要解决经验复用问题

Method: 提出ProcMEM框架，通过形式化Skill-MDP将被动经验叙事转化为可执行技能（包含激活、执行和终止条件）。引入非参数PPO，利用语义梯度生成高质量候选技能，并通过PPO Gate进行技能验证。通过基于分数的维护机制保持紧凑高质量的程序性记忆

Result: 在领域内、跨任务和跨智能体场景的实验结果显示，ProcMEM实现了更高的复用率和显著的性能提升，同时具有极度的内存压缩。可视化进化轨迹和技能分布进一步展示了ProcMEM如何透明地积累、精炼和复用程序性知识

Conclusion: ProcMEM框架成功解决了LLM智能体经验复用不足的问题，通过程序性记忆的自主学习和维护，实现了高效的经验复用和长期自主性，为智能体的长期自主决策提供了有效解决方案

Abstract: LLM-driven agents demonstrate strong performance in sequential decision-making but often rely on on-the-fly reasoning, re-deriving solutions even in recurring scenarios. This insufficient experience reuse leads to computational redundancy and execution instability. To bridge this gap, we propose ProcMEM, a framework that enables agents to autonomously learn procedural memory from interaction experiences without parameter updates. By formalizing a Skill-MDP, ProcMEM transforms passive episodic narratives into executable Skills defined by activation, execution, and termination conditions to ensure executability. To achieve reliable reusability without capability degradation, we introduce Non-Parametric PPO, which leverages semantic gradients for high-quality candidate generation and a PPO Gate for robust Skill verification. Through score-based maintenance, ProcMEM sustains compact, high-quality procedural memory. Experimental results across in-domain, cross-task, and cross-agent scenarios demonstrate that ProcMEM achieves superior reuse rates and significant performance gains with extreme memory compression. Visualized evolutionary trajectories and Skill distributions further reveal how ProcMEM transparently accumulates, refines, and reuses procedural knowledge to facilitate long-term autonomy.

</details>


### [100] [Geometric Analysis of Token Selection in Multi-Head Attention](https://arxiv.org/abs/2602.01893)
*Timur Mudarisov,Mikhal Burtsev,Tatiana Petrova,Radu State*

Main category: cs.AI

TL;DR: 本文提出了一种几何框架来分析大语言模型中的多头注意力机制，将标准注意力视为top-N选择器，在值状态空间中研究其行为，并定义了精确度、召回率和F分数等几何指标来量化选择与非选择token之间的可分离性。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对注意力机制几何特性的系统性分析，特别是多头注意力如何在不同head中实现token选择的可解释性几何模式。需要建立理论框架来量化注意力在值状态空间中的分离行为，为模型解释和优化提供基础。

Method: 提出几何框架，将标准注意力视为top-N选择器，在值状态空间中分析其行为。定义几何指标（精确度、召回率、F分数）量化token分离性，在经验假设下推导非渐近边界。在LLaMA-2-7B、Gemma-7B和Mistral-7B上进行实证验证。

Result: 理论预测了小N操作区域具有最强的非平凡可分离性，阐明了序列长度和sink相似性如何影响指标。实证结果显示测量值与理论包络线紧密匹配：top-N选择增强了可分离性，sink相似性与召回率相关。发现LLaMA-2-7B中的head专门化为三种机制：检索器、混合器、重置器，具有不同的几何特征。

Conclusion: 注意力机制表现为具有可测量token选择标准的结构化几何分类器，提供了head级别的可解释性，并为几何感知的稀疏化和LLM中注意力设计提供了信息。该框架为理解注意力机制提供了新的几何视角。

Abstract: We present a geometric framework for analysing multi-head attention in large language models (LLMs). Without altering the mechanism, we view standard attention through a top-N selection lens and study its behaviour directly in value-state space. We define geometric metrics - Precision, Recall, and F-score - to quantify separability between selected and non-selected tokens, and derive non-asymptotic bounds with explicit dependence on dimension and margin under empirically motivated assumptions (stable value norms with a compressed sink token, exponential similarity decay, and piecewise attention weight profiles). The theory predicts a small-N operating regime of strongest non-trivial separability and clarifies how sequence length and sink similarity shape the metrics. Empirically, across LLaMA-2-7B, Gemma-7B, and Mistral-7B, measurements closely track the theoretical envelopes: top-N selection sharpens separability, sink similarity correlates with Recall. We also found that in LLaMA-2-7B heads specialize into three regimes - Retriever, Mixer, Reset - with distinct geometric signatures. Overall, attention behaves as a structured geometric classifier with measurable criteria for token selection, offering head level interpretability and informing geometry-aware sparsification and design of attention in LLMs.

</details>


### [101] [DomusFM: A Foundation Model for Smart-Home Sensor Data](https://arxiv.org/abs/2602.01910)
*Michele Fiori,Gabriele Civitarese,Flora D. Salim,Claudio Bettini*

Main category: cs.AI

TL;DR: DomusFM是首个专门为智能家居传感器数据设计的预训练基础模型，通过自监督双对比学习范式，在数据稀缺情况下实现跨环境和任务的优异性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在多个关键限制：监督模型需要大量标注数据不实用；活动识别基础模型只关注惯性传感器，无法处理智能家居二进制传感器数据的稀疏离散特性和丰富语义关联；基于LLM的方法需要自然语言描述或提示，依赖外部服务或昂贵硬件，在真实场景中因隐私和成本问题不可行。

Method: DomusFM采用自监督双对比学习范式，结合轻量级语言模型的语义嵌入、专门的时间模式编码器和二进制状态编码器，同时捕捉令牌级语义属性和序列级时间依赖关系。

Result: 在七个公共智能家居数据集上进行留一数据集评估，DomusFM在不同下游任务上优于现有最先进基线，即使在仅有5%标注数据用于微调的情况下也能实现优异性能。

Conclusion: DomusFM解决了数据稀缺问题，同时保持了实际部署可行性，为现实世界智能家居系统提供了实用的解决方案。

Abstract: Smart-home sensor data holds significant potential for several applications, including healthcare monitoring and assistive technologies. Existing approaches, however, face critical limitations. Supervised models require impractical amounts of labeled data. Foundation models for activity recognition focus only on inertial sensors, failing to address the unique characteristics of smart-home binary sensor events: their sparse, discrete nature combined with rich semantic associations. LLM-based approaches, while tested in this domain, still raise several issues regarding the need for natural language descriptions or prompting, and reliance on either external services or expensive hardware, making them infeasible in real-life scenarios due to privacy and cost concerns. We introduce DomusFM, the first foundation model specifically designed and pretrained for smart-home sensor data. DomusFM employs a self-supervised dual contrastive learning paradigm to capture both token-level semantic attributes and sequence-level temporal dependencies. By integrating semantic embeddings from a lightweight language model and specialized encoders for temporal patterns and binary states, DomusFM learns generalizable representations that transfer across environments and tasks related to activity and event analysis. Through leave-one-dataset-out evaluation across seven public smart-home datasets, we demonstrate that DomusFM outperforms state-of-the-art baselines on different downstream tasks, achieving superior performance even with only 5% of labeled training data available for fine-tuning. Our approach addresses data scarcity while maintaining practical deployability for real-world smart-home systems.

</details>


### [102] [Large Language Model and Formal Concept Analysis: a comparative study for Topic Modeling](https://arxiv.org/abs/2602.01933)
*Fabrice Boissier,Monica Sen,Irina Rychkova*

Main category: cs.AI

TL;DR: 本文比较了大型语言模型（LLM）和形式概念分析（FCA）在主题建模任务中的表现，通过两个实验评估了它们在文档主题提取方面的效果。


<details>
  <summary>Details</summary>
Motivation: 主题建模在文档检索、情感分析和文本摘要等领域应用日益广泛。虽然大型语言模型（LLM）在文本处理中很流行，但对其在主题建模任务中的研究较少。同时，形式概念分析（FCA）最近被提出作为主题建模的候选方法，但缺乏实际应用案例研究。本研究旨在比较LLM和FCA在主题建模领域的优缺点。

Method: 研究采用两种方法：1）使用CREA管道评估FCA，这是过去主题建模和可视化实验中使用的工具；2）使用GPT-5作为LLM代表，采用基于三个提示的零样本设置策略：从文档批次生成主题、合并批次结果形成最终主题、以及主题标注。研究进行了两个实验：第一个实验重用之前评估CREA的教学材料，第二个实验分析了40篇信息系统研究文章，比较提取的主题与底层子领域。

Result: 论文没有在摘要中提供具体结果数据，但通过两个实验比较了LLM和FCA在主题建模任务中的表现。第一个实验验证了CREA管道在教学材料上的效果，第二个实验在信息系统研究文章上评估了两种方法提取主题的准确性。

Conclusion: 该研究通过实证比较LLM和FCA在主题建模任务中的表现，为理解这两种方法的优缺点提供了有价值的见解。研究填补了LLM在主题建模领域应用研究的空白，并为FCA的实际应用提供了案例研究。

Abstract: Topic modeling is a research field finding increasing applications: historically from document retrieving, to sentiment analysis and text summarization. Large Language Models (LLM) are currently a major trend in text processing, but few works study their usefulness for this task. Formal Concept Analysis (FCA) has recently been presented as a candidate for topic modeling, but no real applied case study has been conducted. In this work, we compare LLM and FCA to better understand their strengths and weakneses in the topic modeling field. FCA is evaluated through the CREA pipeline used in past experiments on topic modeling and visualization, whereas GPT-5 is used for the LLM. A strategy based on three prompts is applied with GPT-5 in a zero-shot setup: topic generation from document batches, merging of batch results into final topics, and topic labeling. A first experiment reuses the teaching materials previously used to evaluate CREA, while a second experiment analyzes 40 research articles in information systems to compare the extracted topics with the underling subfields.

</details>


### [103] [Small Generalizable Prompt Predictive Models Can Steer Efficient RL Post-Training of Large Reasoning Models](https://arxiv.org/abs/2602.01970)
*Yun Qu,Qi Wang,Yixiu Mao,Heming Zou,Yuhang Jiang,Weijie Liu,Clive Bai,Kai Yang,Yangkun Chen,Saiyong Yang,Xiangyang Ji*

Main category: cs.AI

TL;DR: GPS通过轻量级生成模型进行贝叶斯推理预测提示难度，结合中等难度优先和历史锚定多样性原则，显著提升强化学习训练效率和最终性能


<details>
  <summary>Details</summary>
Motivation: 强化学习增强大语言模型推理能力但计算成本高，现有在线提示选择方法要么依赖昂贵精确评估，要么缺乏跨提示泛化能力

Method: 提出可泛化预测提示选择(GPS)：使用轻量级生成模型在共享优化历史上进行贝叶斯推理预测提示难度，结合中等难度优先和历史锚定多样性原则进行批量选择

Result: 在多样化推理基准测试中，GPS在训练效率、最终性能和测试时效率方面均显著优于基线方法

Conclusion: GPS通过可泛化的预测模型和智能批量选择策略，有效解决了强化学习中提示选择的高计算成本问题，实现了训练和推理效率的显著提升

Abstract: Reinforcement learning enhances the reasoning capabilities of large language models but often involves high computational costs due to rollout-intensive optimization. Online prompt selection presents a plausible solution by prioritizing informative prompts to improve training efficiency. However, current methods either depend on costly, exact evaluations or construct prompt-specific predictive models lacking generalization across prompts. This study introduces Generalizable Predictive Prompt Selection (GPS), which performs Bayesian inference towards prompt difficulty using a lightweight generative model trained on the shared optimization history. Intermediate-difficulty prioritization and history-anchored diversity are incorporated into the batch acquisition principle to select informative prompt batches. The small predictive model also generalizes at test-time for efficient computational allocation. Experiments across varied reasoning benchmarks indicate GPS's substantial improvements in training efficiency, final performance, and test-time efficiency over superior baseline methods.

</details>


### [104] [Thinking Like a Doctor: Conversational Diagnosis through the Exploration of Diagnostic Knowledge Graphs](https://arxiv.org/abs/2602.01995)
*Jeongmoon Won,Seungwon Kook,Yohan Jo*

Main category: cs.AI

TL;DR: 提出基于知识图谱的对话诊断系统，通过生成诊断假设和验证性提问两阶段推理，在模糊症状描述下提高诊断准确性和效率


<details>
  <summary>Details</summary>
Motivation: 现有对话诊断方法要么依赖模型的参数化知识，要么假设患者提供丰富具体信息，这在现实中不切实际，需要解决信息不完整和症状描述模糊的问题

Method: 提出两阶段推理系统：1)从对话上下文生成诊断假设；2)通过澄清问题验证假设，循环直到得出最终诊断。使用MIMIC-IV患者档案和经过调整的模拟器来反映真实世界中早期临床接触时的模糊症状描述

Result: 实验显示相比强基线方法，在诊断准确性和效率方面均有提升。医生评估支持模拟器的真实性和生成问题的临床实用性

Conclusion: 基于知识图谱的对话诊断系统能够有效处理不完整信息和模糊症状描述，提高临床诊断的准确性和效率，具有实际应用价值

Abstract: Conversational diagnosis requires multi-turn history-taking, where an agent asks clarifying questions to refine differential diagnoses under incomplete information. Existing approaches often rely on the parametric knowledge of a model or assume that patients provide rich and concrete information, which is unrealistic. To address these limitations, we propose a conversational diagnosis system that explores a diagnostic knowledge graph to reason in two steps: (i) generating diagnostic hypotheses from the dialogue context, and (ii) verifying hypotheses through clarifying questions, which are repeated until a final diagnosis is reached. Since evaluating the system requires a realistic patient simulator that responds to the system's questions, we adopt a well-established simulator along with patient profiles from MIMIC-IV. We further adapt it to describe symptoms vaguely to reflect real-world patients during early clinical encounters. Experiments show improved diagnostic accuracy and efficiency over strong baselines, and evaluations by physicians support the realism of our simulator and the clinical utility of the generated questions. Our code will be released upon publication.

</details>


### [105] [Do I Really Know? Learning Factual Self-Verification for Hallucination Reduction](https://arxiv.org/abs/2602.02018)
*Enes Altinisik,Masoomali Fatehkia,Fatih Deniz,Nadir Durrani,Majd Hawasly,Mohammad Raza,Husrev Taha Sencar*

Main category: cs.AI

TL;DR: VeriFY是一个训练时框架，通过基于一致性的自我验证教LLM推理事实不确定性，减少事实幻觉，同时保持召回率


<details>
  <summary>Details</summary>
Motivation: 现有缓解事实幻觉的方法主要依赖外部事后验证或将不确定性直接映射到弃权，通常导致过于保守的行为，需要更好的训练时解决方案

Method: VeriFY通过结构化验证轨迹增强训练，指导模型生成初始答案、创建并回答验证查询、进行一致性判断、决定回答或弃权，并使用阶段级损失掩码排除幻觉答案阶段

Result: 在多个模型系列和规模上，VeriFY将事实幻觉率降低9.7%至53.3%，召回率仅轻微下降0.4%至5.7%，且在单一数据集训练后能跨数据集泛化

Conclusion: VeriFY通过训练时自我验证有效减少LLM的事实幻觉，在准确性和覆盖范围之间取得良好平衡，具有实际应用价值

Abstract: Factual hallucination remains a central challenge for large language models (LLMs). Existing mitigation approaches primarily rely on either external post-hoc verification or mapping uncertainty directly to abstention during fine-tuning, often resulting in overly conservative behavior. We propose VeriFY, a training-time framework that teaches LLMs to reason about factual uncertainty through consistency-based self-verification. VeriFY augments training with structured verification traces that guide the model to produce an initial answer, generate and answer a probing verification query, issue a consistency judgment, and then decide whether to answer or abstain. To address the risk of reinforcing hallucinated content when training on augmented traces, we introduce a stage-level loss masking approach that excludes hallucinated answer stages from the training objective while preserving supervision over verification behavior. Across multiple model families and scales, VeriFY reduces factual hallucination rates by 9.7 to 53.3 percent, with only modest reductions in recall (0.4 to 5.7 percent), and generalizes across datasets when trained on a single source. The source code, training data, and trained model checkpoints will be released upon acceptance.

</details>


### [106] [Light Alignment Improves LLM Safety via Model Self-Reflection with a Single Neuron](https://arxiv.org/abs/2602.02027)
*Sicheng Shen,Mingyang Lv,Han Shen,Jialin Wu,Binghao Wang,Zhou Yang,Guobin Shen,Dongcheng Zhao,Feifei Zhao,Yi Zeng*

Main category: cs.AI

TL;DR: 提出NGSD方法，通过低成本训练专家模型并使用单个神经元作为门控机制，在解码阶段实现安全对齐，平衡模型内在能力与外部指导，同时保持实用性和增强输出安全性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM安全对齐方法主要依赖后训练，计算成本高且泛化能力差；轻量级方法要么过度依赖预计算的安全注入，要么过度依赖模型自身能力，导致泛化有限、生成效率和使用性下降。

Method: 提出安全感知解码方法NGSD，只需低成本训练专家模型，使用单个神经元作为门控机制，在解码阶段平衡模型内在能力与外部指导。

Result: 该方法在训练开销和跨模型规模泛化方面具有明显优势，同时保持实用性和增强输出安全性。

Conclusion: 为大型语言模型的安全实用部署提供了轻量级对齐的新视角。

Abstract: The safety of large language models (LLMs) has increasingly emerged as a fundamental aspect of their development. Existing safety alignment for LLMs is predominantly achieved through post-training methods, which are computationally expensive and often fail to generalize well across different models. A small number of lightweight alignment approaches either rely heavily on prior-computed safety injections or depend excessively on the model's own capabilities, resulting in limited generalization and degraded efficiency and usability during generation. In this work, we propose a safety-aware decoding method that requires only low-cost training of an expert model and employs a single neuron as a gating mechanism. By effectively balancing the model's intrinsic capabilities with external guidance, our approach simultaneously preserves utility and enhances output safety. It demonstrates clear advantages in training overhead and generalization across model scales, offering a new perspective on lightweight alignment for the safe and practical deployment of large language models. Code: https://github.com/Beijing-AISI/NGSD.

</details>


### [107] [Edit Knowledge, Not Just Facts via Multi-Step Reasoning over Background Stories](https://arxiv.org/abs/2602.02028)
*Ya Gao,Kalle Kujanpää,Pekka Marttinen,Harri Valpola,Alexander Ilin*

Main category: cs.AI

TL;DR: 提出基于推理的知识内化训练策略，通过连贯背景故事、自生成多跳问题和知识蒸馏，使AI模型能有效整合新知识进行多步推理


<details>
  <summary>Details</summary>
Motivation: 现有知识编辑方法主要关注原子事实的记忆，但无法将新知识整合到连贯框架中跨上下文使用。知识内化本质上是推理问题而非记忆问题，需要模型在任务中结合新旧知识进行多步推理

Method: 提出三原则训练策略：1) 将新知识作为连贯背景故事引入，解释新事实与现有知识的关系；2) 使用自生成多跳问题进行训练，要求涉及新信息的多步推理；3) 采用知识蒸馏，让学生模型内化教师推理行为而无法直接访问新信息

Result: 实验表明，采用该策略训练的模型能有效利用新获取知识进行推理，在需要结合多个新事实的挑战性问题中表现优异

Conclusion: 知识内化应被视为推理问题而非记忆问题，通过将新知识嵌入连贯背景、训练多步推理能力并使用知识蒸馏，可以显著提升AI模型整合和应用新知识的能力

Abstract: Enabling artificial intelligence systems, particularly large language models, to integrate new knowledge and flexibly apply it during reasoning remains a central challenge. Existing knowledge editing approaches emphasize atomic facts, improving factual recall but often failing to integrate new information into a coherent framework usable across contexts. In this work, we argue that knowledge internalization is fundamentally a reasoning problem rather than a memorization problem. Consequently, a model should be trained in situations where the new information is instrumental to solving a task, combined with pre-existing knowledge, and exercised through multi-step reasoning. Based on this insight, we propose a training strategy based on three principles. First, new knowledge is introduced as a coherent background story that contextualizes novel facts and explains their relation to existing knowledge. Second, models are trained using self-generated multi-hop questions that require multi-step reasoning involving the new information. Third, training is done using knowledge distillation, forcing a student model to internalize the teacher's reasoning behavior without access to the novel information. Experiments show that models trained with this strategy effectively leverage newly acquired knowledge during reasoning and achieve remarkable performance on challenging questions that require combining multiple new facts.

</details>


### [108] [Constrained Process Maps for Multi-Agent Generative AI Workflows](https://arxiv.org/abs/2602.02034)
*Ananya Joshi,Michael Rudow*

Main category: cs.AI

TL;DR: 本文提出了一种基于有限时域马尔可夫决策过程的多智能体系统，用于改进LLM智能体在合规和尽职调查等监管场景中的复杂多步工作流处理，相比单智能体基线在准确性、人工审核需求和处理时间方面均有显著提升。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的智能体在合规、尽职调查等监管场景中执行复杂多步工作流时，主要依赖单个智能体的提示工程，难以观察和比较模型如何处理跨决策阶段的不确定性和协调问题，以及如何与人工监督相结合。

Method: 引入一种形式化为有限时域马尔可夫决策过程的多智能体系统，具有有向无环结构。每个智能体对应特定角色或决策阶段（如合规工作流中的内容、业务或法律审查），具有预定义的转换表示任务升级或完成。使用蒙特卡洛估计在智能体层面量化认知不确定性，系统级不确定性通过MDP在自动标记状态或人工审核状态的终止来捕获。

Result: 在AI安全评估（自残检测）的案例研究中，相比单智能体基线，多智能体系统实现了：准确性最高提升19%，人工审核需求最多减少85倍，某些配置下处理时间也有所减少。

Conclusion: 提出的多智能体MDP框架能够有效处理LLM智能体在监管工作流中的不确定性和协调问题，显著提升系统性能并减少人工干预需求，为复杂决策过程提供了可观察和可比较的架构。

Abstract: Large language model (LLM)-based agents are increasingly used to perform complex, multi-step workflows in regulated settings such as compliance and due diligence. However, many agentic architectures rely primarily on prompt engineering of a single agent, making it difficult to observe or compare how models handle uncertainty and coordination across interconnected decision stages and with human oversight. We introduce a multi-agent system formalized as a finite-horizon Markov Decision Process (MDP) with a directed acyclic structure. Each agent corresponds to a specific role or decision stage (e.g., content, business, or legal review in a compliance workflow), with predefined transitions representing task escalation or completion. Epistemic uncertainty is quantified at the agent level using Monte Carlo estimation, while system-level uncertainty is captured by the MDP's termination in either an automated labeled state or a human-review state. We illustrate the approach through a case study in AI safety evaluation for self-harm detection, implemented as a multi-agent compliance system. Results demonstrate improvements over a single-agent baseline, including up to a 19\% increase in accuracy, up to an 85x reduction in required human review, and, in some configurations, reduced processing time.

</details>


### [109] [Hunt Instead of Wait: Evaluating Deep Data Research on Large Language Models](https://arxiv.org/abs/2602.02039)
*Wei Liu,Peijie Yu,Michele Orini,Yali Du,Yulan He*

Main category: cs.AI

TL;DR: 论文提出"调查性智能"概念，区别于"执行性智能"，并引入Deep Data Research任务和DDR-Bench基准来评估LLM在自主数据探索中的能力。


<details>
  <summary>Details</summary>
Motivation: 当前LLM评估主要关注回答正确性，但真正的智能体需要自主设定目标和探索能力。数据科学领域提供了天然测试场，因为真实分析从原始数据开始而非明确查询，但现有基准对此关注不足。

Method: 提出Deep Data Research开放任务，让LLM从数据库中自主提取关键见解；创建DDR-Bench大规模检查表基准，支持可验证评估；分析前沿模型在长视野探索中的表现。

Result: 前沿模型显示出初步的智能体能力，但长视野探索仍然具有挑战性。研究发现有效的调查性智能不仅依赖于智能体框架或单纯扩展，还取决于智能体模型的内在策略。

Conclusion: 调查性智能是LLM智能体的关键能力，需要专门的评估基准。DDR-Bench填补了这一空白，揭示了当前模型在自主数据探索方面的局限性和改进方向。

Abstract: The agency expected of Agentic Large Language Models goes beyond answering correctly, requiring autonomy to set goals and decide what to explore. We term this investigatory intelligence, distinguishing it from executional intelligence, which merely completes assigned tasks. Data Science provides a natural testbed, as real-world analysis starts from raw data rather than explicit queries, yet few benchmarks focus on it. To address this, we introduce Deep Data Research (DDR), an open-ended task where LLMs autonomously extract key insights from databases, and DDR-Bench, a large-scale, checklist-based benchmark that enables verifiable evaluation. Results show that while frontier models display emerging agency, long-horizon exploration remains challenging. Our analysis highlights that effective investigatory intelligence depends not only on agent scaffolding or merely scaling, but also on intrinsic strategies of agentic models.

</details>


### [110] [Understanding the Reversal Curse Mitigation in Masked Diffusion Models through Attention and Training Dynamics](https://arxiv.org/abs/2602.02133)
*Sangwoo Shin,BumJun Kim,Kyelim Lee,Moongyu Jeon,Albert No*

Main category: cs.AI

TL;DR: 扩散语言模型比自回归模型更能缓解反转诅咒，主要原因是架构结构和训练交互，而非训练目标本身。


<details>
  <summary>Details</summary>
Motivation: 自回归语言模型存在反转诅咒问题（学习"A是B"后无法回答"B是A"），而掩码扩散语言模型虽然也有此问题但程度轻得多，但其根本原因尚不清楚。

Method: 通过分析单层Transformer编码器架构，研究权重共享如何使前向和反向注意力分数正相关，并证明相应梯度对齐，从而最小化前向损失也减少反向损失。在受控玩具任务和大规模扩散语言模型上进行实验验证。

Result: 实验表明，扩散语言模型对反转诅咒的缓解主要源于架构结构及其与训练的交互作用，而非通常认为的任意顺序训练目标。权重共享机制使模型能更好地处理反向查询。

Conclusion: 扩散语言模型部分克服自回归模型持续存在的反转诅咒问题，这主要归因于其架构特性和训练动态的相互作用，而非训练目标本身。

Abstract: Autoregressive language models (ARMs) suffer from the reversal curse: after learning that "$A$ is $B$", they often fail on the reverse query "$B$ is $A$". Masked diffusion-based language models (MDMs) exhibit this failure in a much weaker form, but the underlying reason has remained unclear. A common explanation attributes this mitigation to the any-order training objective. However, observing "[MASK] is $B$" during training does not necessarily teach the model to handle the reverse prompt "$B$ is [MASK]". We show that the mitigation arises from architectural structure and its interaction with training. In a one-layer Transformer encoder, weight sharing couples the two directions by making forward and reverse attention scores positively correlated. In the same setting, we further show that the corresponding gradients are aligned, so minimizing the forward loss also reduces the reverse loss. Experiments on both controlled toy tasks and large-scale diffusion language models support these mechanisms, explaining why MDMs partially overcome a failure mode that persists in strong ARMs.

</details>


### [111] [Mitigating Safety Tax via Distribution-Grounded Refinement in Large Reasoning Models](https://arxiv.org/abs/2602.02136)
*Yingsha Xie,Tiansheng Huang,Enneng Yang,Rui Min,Wenjie Lu,Xiaochun Cao,Naiqiang Tan,Li Shen*

Main category: cs.AI

TL;DR: 论文提出DGR方法，通过将外部安全推理数据集转化为与目标大语言模型内部分布对齐的形式，有效缓解安全对齐带来的推理能力退化问题。


<details>
  <summary>Details</summary>
Motivation: 现有安全对齐数据集通常从外部大模型或人工标注中蒸馏安全推理轨迹和答案，但这些数据与需要对齐的目标模型存在分布差异，作者推测这种分布差异是导致目标模型推理能力显著退化的主要原因。

Method: 提出DGR方法，将现有分布外安全推理数据集进行转化和精炼，使其与目标LLM的内部分布对齐，从而减少分布差异对推理能力的影响。

Result: 实验表明：1）DGR在保持安全性能的同时有效缓解安全税，相比Vanilla SFT在DirectRefusal上平均推理准确率提升30.2%，在R1-ACT上提升21.2%；2）推理退化程度与分布偏移程度相关；3）仅需10个样本即可激活有效的拒绝行为，表明安全对齐可能主要作为激活潜在知识的机制。

Conclusion: 研究强调了分布一致性的重要性，并为安全在推理模型中的激活机制提供了新见解，表明安全对齐可能主要作为激活模型已有潜在知识的机制，而非植入新知识。

Abstract: Safety alignment incurs safety tax that perturbs a large reasoning model's (LRM) general reasoning ability. Existing datasets used for safety alignment for an LRM are usually constructed by distilling safety reasoning traces and answers from an external LRM or human labeler. However, such reasoning traces and answers exhibit a distributional gap with the target LRM that needs alignment, and we conjecture such distributional gap is the culprit leading to significant degradation of reasoning ability of the target LRM. Driven by this hypothesis, we propose a safety alignment dataset construction method, dubbed DGR. DGR transforms and refines an existing out-of-distributional safety reasoning dataset to be aligned with the target's LLM inner distribution. Experimental results demonstrate that i) DGR effectively mitigates the safety tax while maintaining safety performance across all baselines, i.e., achieving \textbf{+30.2\%} on DirectRefusal and \textbf{+21.2\%} on R1-ACT improvement in average reasoning accuracy compared to Vanilla SFT; ii) the degree of reasoning degradation correlates with the extent of distribution shift, suggesting that bridging this gap is central to preserving capabilities. Furthermore, we find that safety alignment in LRMs may primarily function as a mechanism to activate latent knowledge, as a mere \textbf{10} samples are sufficient for activating effective refusal behaviors. These findings not only emphasize the importance of distributional consistency but also provide insights into the activation mechanism of safety in reasoning models.

</details>


### [112] [Traffic-Aware Navigation in Road Networks](https://arxiv.org/abs/2602.02158)
*Sarah Nassar*

Main category: cs.AI

TL;DR: 该研究比较了三种图搜索方法在金斯顿路网交通感知导航任务中的表现，包括Floyd-Warshall-Ingerman预处理算法、Dijkstra和A*实时搜索算法，以及结合两者的Yen算法，分析了各自在预处理需求、实时速度和路径最优性方面的权衡。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索不同图搜索算法在交通感知导航任务中的性能表现，特别是在预处理需求、实时计算速度和路径最优性之间的权衡，为实际部署选择最适合的算法提供依据。

Method: 研究方法包括三种图搜索方法的比较：1）单次运行多查询预处理算法（Floyd-Warshall-Ingerman）；2）连续单查询实时搜索算法（Dijkstra和A*）；3）结合两者的算法（Yen算法），先找到前K条最短路径，然后在实时中迭代。

Result: Dijkstra和A*算法产生了最具交通感知的最优解，且所需预处理最少；Floyd-Warshall-Ingerman算法实时速度最快，但仅提供基于距离的路径，无交通感知；Yen算法需要大量预处理，但在运行速度和最优性方面平衡了其他两种方法。

Conclusion: 每种方法都有其优缺点，需要根据具体部署环境的情况权衡选择最佳定制解决方案。没有单一最优算法，选择取决于对预处理时间、实时计算速度和路径最优性的具体要求。

Abstract: This project compares three graph search approaches for the task of traffic-aware navigation in Kingston's road network. These approaches include a single-run multi-query preprocessing algorithm (Floyd-Warshall-Ingerman), continuous single-query real-time search (Dijkstra's and A*), and an algorithm combining both approaches to balance between their trade-offs by first finding the top K shortest paths then iterating over them in real time (Yen's). Dijkstra's and A* resulted in the most traffic-aware optimal solutions with minimal preprocessing required. Floyd-Warshall-Ingerman was the fastest in real time but provided distance based paths with no traffic awareness. Yen's algorithm required significant preprocessing but balanced between the other two approaches in terms of runtime speed and optimality. Each approach presents advantages and disadvantages that need to be weighed depending on the circumstances of specific deployment contexts to select the best custom solution. *This project was completed as part of ELEC 844 (Search and Planning Algorithms for Robotics) in the Fall 2025 term.

</details>


### [113] [Reasoning in a Combinatorial and Constrained World: Benchmarking LLMs on Natural-Language Combinatorial Optimization](https://arxiv.org/abs/2602.02188)
*Xia Jiang,Jing Chen,Cong Zhang,Jie Gao,Chengpeng Hu,Chenhao Zhang,Yaoxin Wu,Yingqian Zhang*

Main category: cs.AI

TL;DR: NLCO是一个评估大语言模型在组合优化问题中端到端推理能力的自然语言基准测试，涵盖43个问题，使用四层分类体系进行细粒度评估。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型在数学和逻辑推理方面表现出色，但它们在组合优化（在高维解空间中搜索满足硬约束的解）方面的能力尚未得到充分探索，需要建立专门的评估基准。

Method: 提出了NLCO基准测试，包含43个组合优化问题，采用四层分类体系（变量类型、约束族、全局模式、目标类别），要求模型直接输出离散解而不编写代码或调用外部求解器。

Result: 高性能模型在小规模实例上表现出良好的可行性和解质量，但随着实例规模增大，两者都会下降，即使使用更多推理标记也无济于事。集合类任务相对容易，而图结构问题和瓶颈目标则导致更多失败。

Conclusion: 大语言模型在组合优化方面存在局限性，特别是在处理大规模实例和复杂结构问题时表现不佳，需要进一步研究提升其组合优化推理能力的方法。

Abstract: While large language models (LLMs) have shown strong performance in math and logic reasoning, their ability to handle combinatorial optimization (CO) -- searching high-dimensional solution spaces under hard constraints -- remains underexplored. To bridge the gap, we introduce NLCO, a \textbf{N}atural \textbf{L}anguage \textbf{C}ombinatorial \textbf{O}ptimization benchmark that evaluates LLMs on end-to-end CO reasoning: given a language-described decision-making scenario, the model must output a discrete solution without writing code or calling external solvers. NLCO covers 43 CO problems and is organized using a four-layer taxonomy of variable types, constraint families, global patterns, and objective classes, enabling fine-grained evaluation. We provide solver-annotated solutions and comprehensively evaluate LLMs by feasibility, solution optimality, and reasoning efficiency. Experiments across a wide range of modern LLMs show that high-performing models achieve strong feasibility and solution quality on small instances, but both degrade as instance size grows, even if more tokens are used for reasoning. We also observe systematic effects across the taxonomy: set-based tasks are relatively easy, whereas graph-structured problems and bottleneck objectives lead to more frequent failures.

</details>


### [114] [TIDE: Trajectory-based Diagnostic Evaluation of Test-Time Improvement in LLM Agents](https://arxiv.org/abs/2602.02196)
*Hang Yan,Xinyu Che,Fangzhi Xu,Qiushi Sun,Zichen Ding,Kanzhi Cheng,Jian Zhang,Tao Qin,Jun Liu,Qika Lin*

Main category: cs.AI

TL;DR: 该论文提出了TIDE框架，用于诊断和评估LLM智能体在测试时改进（TTI）过程中的性能瓶颈，重点关注任务优化效率、错误行为适应和工作记忆效用。


<details>
  <summary>Details</summary>
Motivation: 当前对自主LLM智能体测试时改进（TTI）机制的理解不足，现有评估指标无法捕捉任务优化效率、错误行为适应和工作记忆效用等关键维度，需要新的诊断框架。

Method: 提出TIDE（Test-time Improvement Diagnostic Evaluation）框架，将TTI分解为三个相互关联的维度：任务完成的时间动态、递归循环行为的约束、累积记忆负担的影响。

Result: 通过跨多种智能体和环境的实验，TIDE揭示了提升智能体性能不仅需要扩展内部推理能力，还需要优化智能体与环境之间的交互动态。

Conclusion: TIDE框架为理解TTI成功或失败的机制提供了系统化的诊断工具，强调需要显式优化智能体与环境的交互动态而不仅仅是扩展内部推理能力。

Abstract: Recent advances in autonomous LLM agents demonstrate their ability to improve performance through iterative interaction with the environment. We define this paradigm as Test-Time Improvement (TTI). However, the mechanisms under how and why TTI succeed or fail remain poorly understood, and existing evaluation metrics fail to capture their task optimization efficiency, behavior adaptation after erroneous actions, and the specific utility of working memory for task completion. To address these gaps, we propose Test-time Improvement Diagnostic Evaluation (TIDE), an agent-agnostic and environment-agnostic framework that decomposes TTI into three comprehensive and interconnected dimensions. The framework measures (1) the overall temporal dynamics of task completion and (2) identifies whether performance is primarily constrained by recursive looping behaviors or (3) by burdensome accumulated memory. Through extensive experiments across diverse agents and environments, TIDE highlights that improving agent performance requires more than scaling internal reasoning, calling for explicitly optimizing the interaction dynamics between the agent and the environment.

</details>


### [115] [Position: Explaining Behavioral Shifts in Large Language Models Requires a Comparative Approach](https://arxiv.org/abs/2602.02304)
*Martino Ciaperoni,Marzio Di Vece,Luca Pappalardo,Fosca Giannotti,Francesco Giannini*

Main category: cs.AI

TL;DR: 本文提出了比较性可解释AI框架（Δ-XAI），用于解释大规模基础模型在干预后出现的行为变化，强调需要比较参考模型与干预模型之间的差异，而非孤立分析单个模型。


<details>
  <summary>Details</summary>
Motivation: 大规模基础模型在扩展、微调、强化学习或上下文学习后会出现行为变化，传统XAI方法只能分析单个检查点的失败，无法解释不同检查点之间的内部变化，需要新的解释框架来理解干预引起的行为转变。

Method: 提出了比较性可解释AI框架（Δ-XAI），包含一组设计适当解释方法时应考虑的要求，介绍了可能的分析流程，并将它们与要求相关联，提供了一个具体的Δ-XAI实验示例。

Result: 建立了Δ-XAI框架，明确了比较性解释的核心原则，提供了分析行为变化的方法论基础，展示了如何通过比较参考模型与干预模型来解释行为转变。

Conclusion: 行为变化应该通过比较性方法来解释，核心目标应该是参考模型与干预模型之间的干预引起的变化，而不是孤立分析任何单个模型，Δ-XAI框架为此提供了理论基础和方法指导。

Abstract: Large-scale foundation models exhibit behavioral shifts: intervention-induced behavioral changes that appear after scaling, fine-tuning, reinforcement learning or in-context learning. While investigating these phenomena have recently received attention, explaining their appearance is still overlooked. Classic explainable AI (XAI) methods can surface failures at a single checkpoint of a model, but they are structurally ill-suited to justify what changed internally across different checkpoints and which explanatory claims are warranted about that change. We take the position that behavioral shifts should be explained comparatively: the core target should be the intervention-induced shift between a reference model and an intervened model, rather than any single model in isolation. To this aim we formulate a Comparative XAI ($Δ$-XAI) framework with a set of desiderata to be taken into account when designing proper explaining methods. To highlight how $Δ$-XAI methods work, we introduce a set of possible pipelines, relate them to the desiderata, and provide a concrete $Δ$-XAI experiment.

</details>


### [116] [Interpreting and Controlling LLM Reasoning through Integrated Policy Gradient](https://arxiv.org/abs/2602.02313)
*Changming Li,Kaixing Zhang,Haoyun Xu,Yingdong Shi,Zheng Zhang,Kaitao Song,Kan Ren*

Main category: cs.AI

TL;DR: 提出IPG框架，通过传播基于结果的信号来定位语言模型中的复杂推理机制，实现更精确的组件定位和推理行为调控。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型展现出强大的复杂推理能力，但其内部机制仍然不透明。现有方法要么识别与特定文本模式相关的组件，要么依赖人工标注的对比对来推导控制向量，难以精确定位复杂推理机制或捕捉从模型内部工作到推理输出的顺序影响。

Method: 提出集成策略梯度（IPG）框架，基于结果导向和顺序影响感知原则，通过将基于结果的信号（如推理后准确性）沿模型推理轨迹向后传播，将推理行为归因于模型内部组件。

Result: 实证评估表明，该方法实现了更精确的定位，并能够可靠地调节不同推理模型的推理行为（如推理能力、推理强度）。

Conclusion: IPG框架能够有效识别对推理行为有顺序贡献的组件，为理解语言模型的复杂推理机制提供了新方法。

Abstract: Large language models (LLMs) demonstrate strong reasoning abilities in solving complex real-world problems. Yet, the internal mechanisms driving these complex reasoning behaviors remain opaque. Existing interpretability approaches targeting reasoning either identify components (e.g., neurons) correlated with special textual patterns, or rely on human-annotated contrastive pairs to derive control vectors. Consequently, current methods struggle to precisely localize complex reasoning mechanisms or capture sequential influence from model internal workings to the reasoning outputs. In this paper, built on outcome-oriented and sequential-influence-aware principles, we focus on identifying components that have sequential contribution to reasoning behavior where outcomes are cumulated by long-range effects. We propose Integrated Policy Gradient (IPG), a novel framework that attributes reasoning behaviors to model's inner components by propagating compound outcome-based signals such as post reasoning accuracy backward through model inference trajectories. Empirical evaluations demonstrate that our approach achieves more precise localization and enables reliable modulation of reasoning behaviors (e.g., reasoning capability, reasoning strength) across diverse reasoning models.

</details>


### [117] [Context Learning for Multi-Agent Discussion](https://arxiv.org/abs/2602.02350)
*Xingyuan Hua,Sheng Yue,Xinyi Li,Yizhe Zhao,Jinrui Zhang,Ju Ren*

Main category: cs.AI

TL;DR: M2CL是一种多智能体上下文学习方法，通过学习每个智能体的上下文生成器，动态生成每轮讨论的上下文指令，解决多智能体讨论中的不一致性问题，显著提升性能20%-50%。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体讨论方法容易遭受讨论不一致性问题，由于智能体个体上下文之间的不对齐，导致LLM无法达成一致的解决方案。

Method: M2CL引入多LLM上下文学习方法，为每个智能体学习一个上下文生成器，通过自动信息组织和精炼动态生成每轮讨论的上下文指令，采用精心设计的自适应机制控制上下文一致性和输出差异。

Result: 在学术推理、具身任务和移动控制等挑战性任务上，M2CL性能显著超越现有方法20%-50%，同时具有良好的可迁移性和计算效率。

Conclusion: M2CL通过动态上下文生成机制有效解决了多智能体讨论中的不一致性问题，使LLM能够避免过早收敛于多数噪声，逐步达成正确共识。

Abstract: Multi-Agent Discussion (MAD) has garnered increasing attention very recently, where multiple LLM instances collaboratively solve problems via structured discussion. However, we find that current MAD methods easily suffer from discussion inconsistency, LLMs fail to reach a coherent solution, due to the misalignment between their individual contexts.In this paper, we introduce a multi-LLM context learning method (M2CL) that learns a context generator for each agent, capable of dynamically generating context instructions per discussion round via automatic information organization and refinement. Specifically, inspired by our theoretical insights on the context instruction, M2CL train the generators to control context coherence and output discrepancies via a carefully crafted self-adaptive mechanism.It enables LLMs to avoid premature convergence on majority noise and progressively reach the correct consensus. We evaluate M2CL on challenging tasks, including academic reasoning, embodied tasks, and mobile control. The results show that the performance of M2CL significantly surpasses existing methods by 20%--50%, while enjoying favorable transferability and computational efficiency.

</details>


### [118] [Trust by Design: Skill Profiles for Transparent, Cost-Aware LLM Routing](https://arxiv.org/abs/2602.02386)
*Mika Okamoto,Ansel Kaplan Erol,Glenn Matlin*

Main category: cs.AI

TL;DR: BELLA是一个预算高效的LLM选择框架，通过自动化技能分析推荐最优模型，在保证性能的同时控制成本。


<details>
  <summary>Details</summary>
Motivation: 当前LLM选择面临两个问题：标准基准测试的聚合指标无法揭示任务具体需要哪些能力；用户不知道是否更便宜的模型就能满足需求，导致资金浪费。需要一种透明、可解释的模型选择方法。

Method: BELLA采用三阶段方法：1) 通过批评者分析分解LLM输出，提取细粒度技能；2) 将技能聚类为结构化能力矩阵；3) 多目标优化选择模型，在预算约束下最大化性能。提供自然语言解释推荐理由。

Result: BELLA框架为LLM选择提供了透明、可解释的决策支持，能够识别任务所需的具体技能，并推荐在预算约束下性能最优的模型，解决了当前黑盒路由系统缺乏透明度的问题。

Conclusion: BELLA使从业者能够在部署LLM时做出有原则的成本-性能权衡，通过技能分析实现预算高效的模型选择，特别适用于金融推理等需要多样化技能且模型成本差异大的领域。

Abstract: How should Large Language Model (LLM) practitioners select the right model for a task without wasting money? We introduce BELLA (Budget-Efficient LLM Selection via Automated skill-profiling), a framework that recommends optimal LLM selection for tasks through interpretable skill-based model selection. Standard benchmarks report aggregate metrics that obscure which specific capabilities a task requires and whether a cheaper model could suffice. BELLA addresses this gap through three stages: (1) decomposing LLM outputs and extract the granular skills required by using critic-based profiling, (2) clustering skills into structured capability matrices, and (3) multi-objective optimization to select the right models to maximize performance while respecting budget constraints. BELLA provides natural-language rationale for recommendations, providing transparency that current black-box routing systems lack. We describe the framework architecture, situate it within the landscape of LLM routing and evaluation, and discuss its application to financial reasoning as a representative domain exhibiting diverse skill requirements and cost-variation across models. Our framework enables practitioners to make principled and cost-performance trade-offs for deploying LLMs.

</details>


### [119] [Structure Enables Effective Self-Localization of Errors in LLMs](https://arxiv.org/abs/2602.02416)
*Ankur Samanta,Akshayaa Magesh,Ayush Jain,Kavosh Asadi,Youliang Yu,Daniel Jiang,Boris Vidolov,Kaveh Hassani,Paul Sajda,Jalaj Bhandari,Yonathan Efroni*

Main category: cs.AI

TL;DR: 本文提出Thought-ICS框架，通过将推理分解为离散的语义连贯思维步骤，使语言模型能够准确定位错误并进行迭代自我修正。


<details>
  <summary>Details</summary>
Motivation: 语言模型的自我修正能力仍然难以实现。本文探索语言模型能否显式定位错误推理中的错误，作为构建能够有效自我修正的AI系统的途径。

Method: 引入Thought-ICS（迭代修正思维采样）框架：1）将推理结构化为离散、语义连贯的思维步骤；2）每次生成一个完整的离散思维；3）验证后定位第一个错误步骤；4）回溯到最后一个正确点生成替代推理；5）迭代进行修正。

Result: 1）在传统非结构化思维链推理中模型无法可靠定位错误，而在结构化思维步骤中能够可靠定位；2）在有外部验证的情况下，Thought-ICS实现20-40%的自我修正提升；3）在完全自主无外部验证的设置中，优于当代自我修正基线方法。

Conclusion: 通过将推理结构化为离散思维步骤，语言模型能够有效定位和修正错误，Thought-ICS框架为构建能够自我修正的AI系统提供了有前景的路径。

Abstract: Self-correction in language models remains elusive. In this work, we explore whether language models can explicitly localize errors in incorrect reasoning, as a path toward building AI systems that can effectively correct themselves. We introduce a prompting method that structures reasoning as discrete, semantically coherent thought steps, and show that models are able to reliably localize errors within this structure, while failing to do so in conventional, unstructured chain-of-thought reasoning. Motivated by how the human brain monitors errors at discrete decision points and resamples alternatives, we introduce Iterative Correction Sampling of Thoughts (Thought-ICS), a self-correction framework. Thought-ICS iteratively prompts the model to generate reasoning one discrete and complete thought at a time--where each thought represents a deliberate decision by the model--creating natural boundaries for precise error localization. Upon verification, the model localizes the first erroneous step, and the system backtracks to generate alternative reasoning from the last correct point. When asked to correct reasoning verified as incorrect by an oracle, Thought-ICS achieves 20-40% self-correction lift. In a completely autonomous setting without external verification, it outperforms contemporary self-correction baselines.

</details>


### [120] [Thinking with Comics: Enhancing Multimodal Reasoning through Structured Visual Storytelling](https://arxiv.org/abs/2602.02453)
*Andong Chen,Wenxin Zhu,Qiuyu Ding,Yuchen Song,Muyun Yang,Tiejun Zhao*

Main category: cs.AI

TL;DR: 提出"Thinking with Comics"方法，使用漫画作为介于图像和视频之间的高信息密度视觉推理媒介，在保持时间结构和叙事连贯性的同时降低计算成本


<details>
  <summary>Details</summary>
Motivation: 现有模态存在明显限制：静态图像难以表示时间结构，而视频引入大量冗余和计算成本。需要一种既能保留时间结构又高效的视觉推理媒介

Method: 提出以漫画为媒介的视觉推理范式，漫画作为高信息密度介质介于图像和视频之间，保留时间结构、嵌入文本和叙事连贯性，系统研究基于漫画的两种推理路径

Result: 实验结果显示，Thinking with Comics在多步时间和因果推理任务上优于Thinking with Images，同时比Thinking with Video显著更高效。不同漫画叙事结构和风格对任务性能有持续影响

Conclusion: 漫画作为一种有效的中间视觉表示，能够改进多模态推理，在信息密度和计算效率之间取得良好平衡

Abstract: Chain-of-Thought reasoning has driven large language models to extend from thinking with text to thinking with images and videos. However, different modalities still have clear limitations: static images struggle to represent temporal structure, while videos introduce substantial redundancy and computational cost. In this work, we propose Thinking with Comics, a visual reasoning paradigm that uses comics as a high information-density medium positioned between images and videos. Comics preserve temporal structure, embedded text, and narrative coherence while requiring significantly lower reasoning cost. We systematically study two reasoning paths based on comics and evaluate them on a range of reasoning tasks and long-context understanding tasks. Experimental results show that Thinking with Comics outperforms Thinking with Images on multi-step temporal and causal reasoning tasks, while remaining substantially more efficient than Thinking with Video. Further analysis indicates that different comic narrative structures and styles consistently affect performance across tasks, suggesting that comics serve as an effective intermediate visual representation for improving multimodal reasoning.

</details>


### [121] [MentisOculi: Revealing the Limits of Reasoning with Mental Imagery](https://arxiv.org/abs/2602.02465)
*Jana Zeller,Thaddäus Wiedemer,Fanfei Li,Thomas Klein,Prasanna Mayilvahanan,Matthias Bethge,Felix Wichmann,Ryan Cotterell,Wieland Brendel*

Main category: cs.AI

TL;DR: 论文提出了MentisOculi评估套件，用于测试多模态模型能否像人类心理意象一样使用视觉化作为推理辅助，发现当前模型即使能生成正确视觉也无法有效利用视觉思维进行推理。


<details>
  <summary>Details</summary>
Motivation: 随着前沿模型从仅接收视觉信息的多模态大语言模型向能够原生交错生成的多模态模型转变，研究者希望探索使用中间视觉化作为推理辅助的可能性，类似于人类的心理意象。核心是评估模型能否以目标导向的方式形成、维护和操作视觉表征。

Method: 开发了MentisOculi——一个程序化、分层的多步推理问题套件，专门设计用于挑战前沿模型。评估了从潜在标记到显式生成图像等多种视觉策略，特别分析了统一多模态模型的表现。

Result: 视觉策略通常无法提升模型性能。统一多模态模型虽然具备解决任务的文本推理能力，有时也能生成正确的视觉化，但存在累积生成错误的问题，甚至无法有效利用真实视觉化。视觉思维目前尚未对模型推理产生益处。

Conclusion: 尽管视觉思维具有内在吸引力，但当前模型尚未能有效利用视觉化进行推理。MentisOculi为分析和弥合这一差距提供了必要基础，适用于不同模型家族的研究。

Abstract: Frontier models are transitioning from multimodal large language models (MLLMs) that merely ingest visual information to unified multimodal models (UMMs) capable of native interleaved generation. This shift has sparked interest in using intermediate visualizations as a reasoning aid, akin to human mental imagery. Central to this idea is the ability to form, maintain, and manipulate visual representations in a goal-oriented manner. To evaluate and probe this capability, we develop MentisOculi, a procedural, stratified suite of multi-step reasoning problems amenable to visual solution, tuned to challenge frontier models. Evaluating visual strategies ranging from latent tokens to explicit generated imagery, we find they generally fail to improve performance. Analysis of UMMs specifically exposes a critical limitation: While they possess the textual reasoning capacity to solve a task and can sometimes generate correct visuals, they suffer from compounding generation errors and fail to leverage even ground-truth visualizations. Our findings suggest that despite their inherent appeal, visual thoughts do not yet benefit model reasoning. MentisOculi establishes the necessary foundation to analyze and close this gap across diverse model families.

</details>


### [122] [Breaking the Reversal Curse in Autoregressive Language Models via Identity Bridge](https://arxiv.org/abs/2602.02470)
*Xutao Ma,Yixiao Huang,Hanlin Zhu,Somayeh Sojoudi*

Main category: cs.AI

TL;DR: 本文挑战了自回归LLMs存在"逆转诅咒"这一固有缺陷的观点，提出通过添加"A→A"形式的身份桥接正则化数据，可以显著改善模型在反向推理任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 自回归大语言模型在复杂任务上表现出色，但在简单逻辑推理如"逆转诅咒"上仍会失败。先前研究认为这是自回归因果LLMs固有的基本限制，表明这些模型倾向于记忆事实级知识而非捕捉高级规则。本文旨在挑战这一观点。

Method: 提出一种简单的正则化数据配方——身份桥接（Identity Bridge），形式为"A→A"（例如：Alice的名字是Alice）。理论上证明在此配方下，即使单层transformer也能通过分析梯度下降的隐式偏置来打破逆转诅咒。实证上，在10亿参数预训练语言模型上微调使用该数据配方。

Result: 使用身份桥接数据配方微调的模型在逆转任务上达到40%的成功率，而仅使用前向知识数据训练的模型成功率接近零。这显著改善了模型在反向推理任务上的表现。

Conclusion: 本文为逆转诅咒提供了新的理论基础，并提供了一个原则性的低成本路径来鼓励LLMs从数据中学习更高级的规则，挑战了自回归LLMs存在固有逻辑推理限制的传统观点。

Abstract: Autoregressive large language models (LLMs) have achieved remarkable success in many complex tasks, yet they can still fail in very simple logical reasoning such as the "reversal curse" -- when trained on forward knowledge data of the form "$A \rightarrow B$" (e.g., Alice's husband is Bob), the model is unable to deduce the reversal knowledge "$B \leftarrow A$" (e.g., Bob's wife is Alice) during test. Extensive prior research suggests that this failure is an inherent, fundamental limit of autoregressive causal LLMs, indicating that these models tend to memorize factual-level knowledge rather than capture higher-level rules. In this paper, we challenge this view by showing that this seemingly fundamental limit can be mitigated by slightly tweaking the training data with a simple regularization data recipe called the Identity Bridge of the form "$A \to A$" (e.g., The name of Alice is Alice). Theoretically, we prove that under this recipe, even a one-layer transformer can break the reversal curse by analyzing the implicit bias of gradient descent. Empirically, we show that a 1B pretrained language model finetuned with the proposed data recipe achieves a 40% success rate on reversal tasks, in stark contrast to a near-zero success rate when trained solely on forward-knowledge data. Our work provides a novel theoretical foundation for the reversal curse and offers a principled, low-cost path to encouraging LLMs to learn higher-level rules from data.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [123] [Semantic-Aware Advanced Persistent Threat Detection Using Autoencoders on LLM-Encoded System Logs](https://arxiv.org/abs/2602.00204)
*Waleed Khan Mohammed,Zahirul Arief Irfan Bin Shahrul Anuar,Mousa Sufian Mousa Mitani,Hezerul Abdul Karim,Nouar AlDahoul*

Main category: cs.CR

TL;DR: 本文提出了一种利用大语言模型生成语义嵌入的新型APT检测方法，通过自编码器分析系统日志的语义表示，在DARPA数据集上优于传统无监督基线方法。


<details>
  <summary>Details</summary>
Motivation: 高级持续性威胁（APTs）具有"低而慢"的特点，传统统计方法和浅层机器学习技术难以检测。现有的溯源图分析方法往往无法捕捉系统活动背后的语义意图，需要新的检测方法。

Method: 使用预训练的大语言模型将原始系统日志转换为高维语义嵌入，然后通过自编码器分析这些嵌入来识别异常和潜在恶意模式。

Result: 在DARPA透明计算数据集上的实验表明，基于LLM嵌入的自编码器在AUC-ROC指标上优于隔离森林、单类支持向量机和主成分分析等广泛使用的无监督基线方法。

Conclusion: 语义理解对于检测传统技术经常遗漏的非线性和隐蔽攻击行为至关重要，基于LLM的语义嵌入方法为APT检测提供了有效的新途径。

Abstract: Advanced Persistent Threats (APTs) are among the most challenging cyberattacks to detect. They are carried out by highly skilled attackers who carefully study their targets and operate in a stealthy, long-term manner. Because APTs exhibit "low-and-slow" behavior, traditional statistical methods and shallow machine learning techniques often fail to detect them. Previous research on APT detection has explored machine learning approaches and provenance graph analysis. However, provenance-based methods often fail to capture the semantic intent behind system activities. This paper proposes a novel anomaly detection approach that leverages semantic embeddings generated by Large Language Models (LLMs). The method enhances APT detection by extracting meaningful semantic representations from unstructured system log data. First, raw system logs are transformed into high-dimensional semantic embeddings using a pre-trained transformer model. These embeddings are then analyzed using an Autoencoder (AE) to identify anomalous and potentially malicious patterns. The proposed method is evaluated using the DARPA Transparent Computing (TC) dataset, which contains realistic APT attack scenarios generated by red teams in live environments. Experimental results show that the AE trained on LLM-derived embeddings outperforms widely used unsupervised baseline methods, including Isolation Forest (IForest), One-Class Support Vector Machine (OC-SVM), and Principal Component Analysis (PCA). Performance is measured using the Area Under the Receiver Operating Characteristic Curve (AUC-ROC), where the proposed approach consistently achieves superior results, even in complex threat scenarios. These findings highlight the importance of semantic understanding in detecting non-linear and stealthy attack behaviors that are often missed by conventional detection techniques.

</details>


### [124] [Tri-LLM Cooperative Federated Zero-Shot Intrusion Detection with Semantic Disagreement and Trust-Aware Aggregation](https://arxiv.org/abs/2602.00219)
*Saeid Jamshidi,Omar Abdul Wahab,Foutse Khomh,Kawser Wazed Nafi*

Main category: cs.CR

TL;DR: 提出了一种语义驱动的联邦入侵检测系统框架，通过语言语义监督实现开放集和零样本入侵检测，利用Tri-LLM集成构建语义攻击原型，建模认知不确定性，并采用信任感知聚合机制。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习入侵检测系统大多假设封闭集学习，缺乏不确定性估计、语义泛化和对零日攻击场景的认知模糊性建模能力，且对异构不可靠客户端的鲁棒性不足。

Method: 1. 使用GPT-4o、DeepSeek-V3和LLaMA-3-8B的Tri-LLM集成构建语义攻击原型；2. 将分布式遥测特征与高层攻击概念对齐；3. 将LLM间的语义分歧建模为认知不确定性；4. 采用信任感知聚合机制动态加权客户端更新。

Result: 1. 在异构客户端间实现稳定的语义对齐和一致收敛；2. 对未见攻击模式达到80%以上的零样本检测准确率；3. 相比基于相似性的基线方法，零日攻击识别能力提升超过10%；4. 在不可靠或受损客户端存在时保持低聚合不稳定性。

Conclusion: 该语义驱动的联邦入侵检测框架通过语言语义监督、认知不确定性建模和信任感知聚合，有效解决了开放集、零样本入侵检测和异构客户端鲁棒性问题，为实际应用提供了可行方案。

Abstract: Federated learning (FL) has become an effective paradigm for privacy-preserving, distributed Intrusion Detection Systems (IDS) in cyber-physical and Internet of Things (IoT) networks, where centralized data aggregation is often infeasible due to privacy and bandwidth constraints. Despite its advantages, most existing FL-based IDS assume closed-set learning and lack mechanisms such as uncertainty estimation, semantic generalization, and explicit modeling of epistemic ambiguity in zero-day attack scenarios. Additionally, robustness to heterogeneous and unreliable clients remains a challenge in practical applications. This paper introduces a semantics-driven federated IDS framework that incorporates language-derived semantic supervision into federated optimization, enabling open-set and zero-shot intrusion detection for previously unseen attack behaviors. The approach constructs semantic attack prototypes using a Tri-LLM ensemble of GPT-4o, DeepSeek-V3, and LLaMA-3-8B, aligning distributed telemetry features with high-level attack concepts. Inter-LLM semantic disagreement is modeled as epistemic uncertainty for zero-day risk estimation, while a trust-aware aggregation mechanism dynamically weights client updates based on reliability. Experimental results show stable semantic alignment across heterogeneous clients and consistent convergence. The framework achieves over 80% zero-shot detection accuracy on unseen attack patterns, improving zero-day discrimination by more than 10% compared to similarity-based baselines, while maintaining low aggregation instability in the presence of unreliable or compromised clients.

</details>


### [125] [RVDebloater: Mode-based Adaptive Firmware Debloating for Robotic Vehicles](https://arxiv.org/abs/2602.00270)
*Mohsen Salehi,Karthik Pattabiraman*

Main category: cs.CR

TL;DR: RVDebloater：一种针对模式化嵌入式设备的自适应去膨胀技术，通过运行时函数级动态去膨胀减少攻击面，在机器人车辆上实现平均85%函数限制、45%调用图剪枝，性能开销仅3.9%


<details>
  <summary>Details</summary>
Motivation: 嵌入式设备固件规模增大导致攻击面扩大，但许多设备（如机器人车辆）在不同模式下仅需少量代码。现有去膨胀技术存在粒度粗、不可逆等限制，难以适用。

Method: 提出RVDebloater：基于LLVM编译器实现，通过静态或动态分析自动识别各模式不需要的固件代码，在运行时以函数级粒度动态去膨胀，采用软件强制执行方法支持多样化模式设备。

Result: 在6种机器人车辆（模拟和真实）上评估：平均85%函数在其他模式下可限制，调用图平均剪枝45%，任务完成率100%（无假阳性/假阴性），真实设备上平均性能开销3.9%、内存开销4%（约0.25MB）。

Conclusion: RVDebloater能有效减少模式化嵌入式设备的攻击面，实现细粒度、可适应的运行时去膨胀，在保证任务完成的同时保持低开销，为嵌入式安全提供新方案。

Abstract: As the number of embedded devices grows and their functional requirements increase, embedded firmware is becoming increasingly larger, thereby expanding its attack surface. Despite the increase in firmware size, many embedded devices, such as robotic vehicles (RVs), operate in distinct modes, each requiring only a small subset of the firmware code at runtime. We refer to such devices as mode-based embedded devices. Debloating is an approach to reduce attack surfaces by removing or restricting unneeded code, but existing techniques suffer from significant limitations, such as coarse granularity and irreversible code removal, limiting their applicability.
  To address these limitations, we propose RVDebloater, a novel adaptive debloating technique for mode-based embedded devices that automatically identifies unneeded firmware code for each mode using either static or dynamic analysis, and dynamically debloats the firmware for each mode at the function level at runtime. RVDebloater introduces a new software-based enforcement approach that supports diverse mode-based embedded devices. We implemented RVDebloater using the LLVM compiler and evaluated its efficiency and effectiveness on six different RVs, including both simulated and real ones, with different real-world missions. We find that device requirements change throughout its lifetime for each mode, and that many critical firmware functions can be restricted in other modes, with an average of 85% of functions not being required. The results showed that none of the missions failed after debloating with RVDebloater, indicating that it neither incurred false positives nor false negatives. Further, RVDebloater prunes the firmware call graph by an average of 45% across different firmware. Finally, RVDebloater incurred an average performance overhead of 3.9% and memory overhead of 4% (approximately 0.25 MB) on real RVs.

</details>


### [126] [HEEDFUL: Leveraging Sequential Transfer Learning for Robust WiFi Device Fingerprinting Amid Hardware Warm-Up Effects](https://arxiv.org/abs/2602.00338)
*Abdurrahman Elmaghbub,Bechir Hamdaoui*

Main category: cs.CR

TL;DR: HEEDFUL框架通过序列迁移学习和目标损伤估计解决RF指纹识别在硬件预热阶段的跨域性能问题，在WiFi信号上达到96%的分类准确率。


<details>
  <summary>Details</summary>
Motivation: 基于深度学习的RF指纹识别方法在跨域场景（特别是硬件预热阶段）表现不佳，这一常被忽视的漏洞影响了其在实际应用中的可靠性和采用。

Method: 提出HEEDFUL框架，结合序列迁移学习和目标损伤估计技术，深入分析RF指纹在硬件稳定期间和稳定后的时间变化特征。

Result: HEEDFUL在设备初始运行阶段达到96%的分类准确率，远超传统模型；跨日和跨协议评估也显示其在稳定和预热阶段均保持高准确率。

Conclusion: HEEDFUL框架有效解决了RF指纹识别在硬件预热阶段的性能问题，发布的WiFi数据集首次包含时域表示和真实硬件损伤数据，强调了利用硬件损伤数据的重要性。

Abstract: Deep Learning-based RF fingerprinting approaches struggle to perform well in cross-domain scenarios, particularly during hardware warm-up. This often-overlooked vulnerability has been jeopardizing their reliability and their adoption in practical settings. To address this critical gap, in this work, we first dive deep into the anatomy of RF fingerprints, revealing insights into the temporal fingerprinting variations during and post hardware stabilization. Introducing HEEDFUL, a novel framework harnessing sequential transfer learning and targeted impairment estimation, we then address these challenges with remarkable consistency, eliminating blind spots even during challenging warm-up phases. Our evaluation showcases HEEDFUL's efficacy, achieving remarkable classification accuracies of up to 96% during the initial device operation intervals-far surpassing traditional models. Furthermore, cross-day and cross-protocol assessments confirm HEEDFUL's superiority, achieving and maintaining high accuracy during both the stable and initial warm-up phases when tested on WiFi signals. Additionally, we release WiFi type B and N RF fingerprint datasets that, for the first time, incorporate both the time-domain representation and real hardware impairments of the frames. This underscores the importance of leveraging hardware impairment data, enabling a deeper understanding of fingerprints and facilitating the development of more robust RF fingerprinting solutions.

</details>


### [127] [SpyDir: Spy Device Localization Through Accurate Direction Finding](https://arxiv.org/abs/2602.00411)
*Wenhao Chen,Wenyi Morty Zhang,Wei Sun,Dinesh Bharadia,Roshan Ayyalasomayajula*

Main category: cs.CR

TL;DR: SpyDir系统通过利用隐藏间谍IoT设备自动发射的电磁辐射，在室内环境中准确定位这些设备，相比基线算法在AoA误差和定位精度上实现了3.3倍到14.8倍的改进。


<details>
  <summary>Details</summary>
Motivation: 隐藏的间谍摄像头等低成本、低功耗、小尺寸的IoT设备对隐私构成严重威胁，这些设备在室内环境中可以悄无声息地监控人类活动而不产生任何侧信道信息，因此难以检测和定位。

Method: 系统设计主要包括：1)便携式切换天线阵列嗅探频谱扩展辐射；2)通过非相干平均进行辐射增强算法，消除方波辐射结构导致的噪声相关效应；3)多径解析算法，利用相对信道通过基于优化的稀疏AoA推导。

Result: 在不同室内环境中的真实世界实验评估显示：平均AoA误差为6.30度（基线算法为21.06度），定位误差为19.86厘米（基线MUSIC算法为206.79厘米，SpotFi算法为294.75厘米），分别实现了3.3倍和10.41-14.8倍的精度提升。

Conclusion: SpyDir系统能够有效利用隐藏间谍IoT设备的电磁辐射进行准确定位，在室内环境中显著提高了定位精度，为解决隐私威胁提供了有效的技术方案。

Abstract: Hidden spy cameras have become a great privacy threat recently, as these low-cost, low-power, and small form-factor IoT devices can quietly monitor human activities in the indoor environment without generating any side-channel information. As such, it is difficult to detect and even more challenging to localize them in the rich-scattering indoor environment. To this end, this paper presents the design, implementation, and evaluation of SpyDir, a system that can accurately localize the hidden spy IoT devices by harnessing the electromagnetic emanations automatically and unintentionally emitted from them. Our system design mainly consists of a portable switching antenna array to sniff the spectrum-spread emanations, an emanation enhancement algorithm through non-coherent averaging that can de-correlate the correlated noise effect due to the square-wave emanation structure, and a multipath-resolving algorithm that can exploit the relative channels using a novel optimization-based sparse AoA derivation. Our real-world experimental evaluation across different indoor environments demonstrates an average AoA error of 6.30 deg, whereas the baseline algorithm yields 21.06 deg, achieving over a 3.3 times improvement in accuracy, and a mean localization error of 19.86cm over baseline algorithms of 206.79cm (MUSIC) and 294.75cm (SpotFi), achieving over a 10.41 times and 14.8 times improvement in accuracy.

</details>


### [128] [zkCraft: Prompt-Guided LLM as a Zero-Shot Mutation Pattern Oracle for TCCT-Powered ZK Fuzzing](https://arxiv.org/abs/2602.00667)
*Rong Fu,Jia Yee Tan,Wenxin Zhang,Youjin Wang,Ziyu Kong,Zeli Su,Zhaolu Kang,Shuning Zhang,Xianda Li,Kun Liu,Simon Fong*

Main category: cs.CR

TL;DR: zkCraft是一个结合确定性R1CS感知定位与证明承载搜索的框架，用于检测零知识电路中的语义不一致问题，通过Row-Vortex多项式编码约束编辑，减少求解器交互成本。


<details>
  <summary>Details</summary>
Motivation: 零知识电路由于见证计算与电路约束的紧密耦合，难以正确实现，需要一种实用的方法来检测语义不一致问题，以支持隐私保护和可扩展系统的开发。

Method: zkCraft采用确定性、R1CS感知的定位方法，结合证明承载搜索，将候选约束编辑编码为Row-Vortex多项式，用Violation IOP替代重复的求解器查询，并使用确定性LLM驱动的突变模板来偏向边缘情况探索。

Result: 在真实Circom代码上的评估显示，证明承载定位能够检测多种欠约束和过约束故障，具有较低的误报率，并显著减少了昂贵的求解器交互。

Conclusion: zkCraft架起了形式化验证与自动化调试之间的桥梁，为零知识电路的稳健开发提供了可扩展的路径。

Abstract: Zero-knowledge circuits enable privacy-preserving and scalable systems but are difficult to implement correctly due to the tight coupling between witness computation and circuit constraints. We present zkCraft, a practical framework that combines deterministic, R1CS-aware localization with proof-bearing search to detect semantic inconsistencies. zkCraft encodes candidate constraint edits into a single Row-Vortex polynomial and replaces repeated solver queries with a Violation IOP that certifies the existence of edits together with a succinct proof. Deterministic LLM-driven mutation templates bias exploration toward edge cases while preserving auditable algebraic verification. Evaluation on real Circom code shows that proof-bearing localization detects diverse under- and over-constrained faults with low false positives and reduces costly solver interaction. Our approach bridges formal verification and automated debugging, offering a scalable path for robust ZK circuit development.

</details>


### [129] [Computing Maximal Per-Record Leakage and Leakage-Distortion Functions for Privacy Mechanisms under Entropy-Constrained Adversaries](https://arxiv.org/abs/2602.00689)
*Genqiang Wu,Xiaoying Zhang,Yu Qi,Hao Wang,Jikui Wang,Yeping He*

Main category: cs.CR

TL;DR: 该论文提出了一种基于信息论的信息隐私框架，针对具有有限先验知识的对手，研究了最大单记录泄露、泄露-失真权衡和失真最小化三个核心问题，并开发了高效的交替优化算法。


<details>
  <summary>Details</summary>
Motivation: 随着数据收集的指数增长，需要在保护数据效用的同时提供强大的隐私保护。传统差分隐私的独立性假设不现实，需要更符合实际的有限知识对手模型。

Method: 提出信息隐私框架，用熵约束H(X)≥b建模对手的有限先验知识。开发高效的交替优化算法，利用凸凹对偶性，为原始问题提供局部收敛保证，为对偶问题提供稳定点收敛保证。

Result: 在二进制对称信道和模和查询上的实验验证了算法的有效性，显示出比经典差分隐私机制更好的隐私-效用权衡。提供了计算框架来审计隐私风险和设计经过认证的机制。

Conclusion: 该工作为在现实对手假设下审计隐私风险和设计经过认证的机制提供了计算框架，解决了高维度和熵约束带来的复杂性，改进了传统差分隐私方法的局限性。

Abstract: The exponential growth of data collection necessitates robust privacy protections that preserve data utility. We address information disclosure against adversaries with bounded prior knowledge, modeled by an entropy constraint $H(X) \geq b$. Within this information privacy framework -- which replaces differential privacy's independence assumption with a bounded-knowledge model -- we study three core problems: maximal per-record leakage, the primal leakage-distortion tradeoff (minimizing worst-case leakage under distortion $D$), and the dual distortion minimization (minimizing distortion under leakage constraint $L$).
  These problems resemble classical information-theoretic ones (channel capacity, rate-distortion) but are more complex due to high dimensionality and the entropy constraint. We develop efficient alternating optimization algorithms that exploit convexity-concavity duality, with theoretical guarantees including local convergence for the primal problem and convergence to a stationary point for the dual.
  Experiments on binary symmetric channels and modular sum queries validate the algorithms, showing improved privacy-utility tradeoffs over classical differential privacy mechanisms. This work provides a computational framework for auditing privacy risks and designing certified mechanisms under realistic adversary assumptions.

</details>


### [130] [From Detection to Prevention: Explaining Security-Critical Code to Avoid Vulnerabilities](https://arxiv.org/abs/2602.00711)
*Ranjith Krishnamurthy,Oshando Johnson,Goran Piskachev,Eric Bodden*

Main category: cs.CR

TL;DR: 开发了一个IntelliJ IDEA插件原型，使用代码级软件指标识别潜在安全关键方法，并利用大语言模型生成预防性解释，旨在在漏洞产生前预防安全问题。


<details>
  <summary>Details</summary>
Motivation: 由于开发人员缺乏安全专业知识和代码复杂性，安全漏洞常在开发过程中无意产生。传统工具（如静态和动态分析）只能在漏洞引入代码后检测，导致修复成本高昂。因此需要一种主动预防策略。

Method: 提出一个IntelliJ IDEA插件原型，使用代码级软件指标识别潜在安全关键方法（如数据访问、认证、输入处理等），并利用大语言模型生成预防导向的解释和指导。

Result: 在Spring-PetClinic应用上的初步评估显示，所选指标能够识别大多数已知的安全关键方法，同时大语言模型能够提供可操作的、以预防为重点的见解。

Conclusion: 虽然这些指标捕捉的是安全的结构属性而非语义方面，但这项工作为代码级安全感知指标和增强解释奠定了基础，提供了一种主动预防安全漏洞的方法。

Abstract: Security vulnerabilities often arise unintentionally during development due to a lack of security expertise and code complexity. Traditional tools, such as static and dynamic analysis, detect vulnerabilities only after they are introduced in code, leading to costly remediation. This work explores a proactive strategy to prevent vulnerabilities by highlighting code regions that implement security-critical functionality -- such as data access, authentication, and input handling -- and providing guidance for their secure implementation. We present an IntelliJ IDEA plugin prototype that uses code-level software metrics to identify potentially security-critical methods and large language models (LLMs) to generate prevention-oriented explanations. Our initial evaluation on the Spring-PetClinic application shows that the selected metrics identify most known security-critical methods, while an LLM provides actionable, prevention-focused insights. Although these metrics capture structural properties rather than semantic aspects of security, this work lays the foundation for code-level security-aware metrics and enhanced explanations.

</details>


### [131] [Bypassing Prompt Injection Detectors through Evasive Injections](https://arxiv.org/abs/2602.00750)
*Md Jahedur Rahman,Ihsen Alouani*

Main category: cs.CR

TL;DR: 本文评估了基于激活delta的任务漂移检测器对对抗性后缀攻击的鲁棒性，发现这些检测器高度脆弱，并提出了一种有效的防御方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在交互式和检索增强系统中应用广泛，但容易受到任务漂移攻击。虽然已有研究使用线性探针检测这种漂移，但这些检测器对对抗性攻击的鲁棒性尚未得到充分评估。

Method: 1) 生成能同时欺骗多个线性探针的通用对抗性后缀；2) 在Phi-3 3.8B和Llama-3 8B模型上进行实验；3) 提出防御技术：生成多个后缀并随机附加到提示中，使用这些激活训练逻辑回归模型。

Result: 单个对抗性后缀能实现高攻击成功率：当所有探针都需要被欺骗时，Phi-3达到93.91%，Llama-3达到99.63%；在多数投票设置下成功率接近完美（>90%）。提出的防御方法对这些攻击非常有效。

Conclusion: 基于激活delta的任务漂移检测器对对抗性后缀攻击高度脆弱，需要更强的防御机制来应对自适应攻击。提出的随机后缀防御方法显示出良好的防护效果。

Abstract: Large language models (LLMs) are increasingly used in interactive and retrieval-augmented systems, but they remain vulnerable to task drift; deviations from a user's intended instruction due to injected secondary prompts. Recent work has shown that linear probes trained on activation deltas of LLMs' hidden layers can effectively detect such drift. In this paper, we evaluate the robustness of these detectors against adversarially optimised suffixes. We generate universal suffixes that cause poisoned inputs to evade detection across multiple probes simultaneously. Our experiments on Phi-3 3.8B and Llama-3 8B show that a single suffix can achieve high attack success rates; up to 93.91% and 99.63%, respectively, when all probes must be fooled, and nearly perfect success (>90%) under majority vote setting. These results demonstrate that activation delta-based task drift detectors are highly vulnerable to adversarial suffixes, highlighting the need for stronger defences against adaptive attacks. We also propose a defence technique where we generate multiple suffixes and randomly append one of them to the prompts while making forward passes of the LLM and train logistic regression models with these activations. We found this approach to be highly effective against such attacks.

</details>


### [132] [IDEM Enough? Evolving Highly Nonlinear Idempotent Boolean Functions](https://arxiv.org/abs/2602.00837)
*Claude Carlet,Marko Ðurasevic,Domagoj Jakobovic,Luca Mariot,Stjepan Picek*

Main category: cs.CR

TL;DR: 使用进化算法构造高非线性幂等布尔函数的研究，通过轨道编码实现幂等性约束


<details>
  <summary>Details</summary>
Motivation: 幂等布尔函数具有特殊的代数结构，与密码学设计密切相关，但其额外的代数约束使得寻找高非线性函数比无约束情况更加困难

Method: 使用进化算法（交叉和变异操作）在多项式基表示下构造幂等布尔函数，采用轨道编码方式强制实现幂等性，将真值表编码在轨道上

Result: 研究发现进化幂等函数很困难，因为交叉和变异操作具有破坏性；通过轨道编码可以强制实现幂等性，得到紧凑的基因组

Conclusion: 轨道编码是构造幂等布尔函数的有效方法，能够克服进化算法中交叉和变异操作对幂等性的破坏

Abstract: Idempotent Boolean functions form a highly structured subclass of Boolean functions that is closely related to rotation symmetry under a normal-basis representation and to invariance under a fixed linear map in a polynomial basis. These functions are attractive as candidates for cryptographic design, yet their additional algebraic constraints make the search for high nonlinearity substantially more difficult than in the unconstrained case. In this work, we investigate evolutionary methods for constructing highly nonlinear idempotent Boolean functions for dimensions $n=5$ up to $n=12$ using a polynomial basis representation with canonical primitive polynomials. Our results show that the problem of evolving idempotent functions is difficult due to the disruptive nature of crossover and mutation operators. Next, we show that idempotence can be enforced by encoding the truth table on orbits, yielding a compact genome of size equal to the number of distinct squaring orbits.

</details>


### [133] [GradingAttack: Attacking Large Language Models Towards Short Answer Grading Ability](https://arxiv.org/abs/2602.00979)
*Xueyi Li,Zhuoneng Zhou,Zitao Liu,Yongdong Wu,Weiqi Luo*

Main category: cs.CR

TL;DR: GradingAttack是一个针对LLM自动评分系统的细粒度对抗攻击框架，通过token级和prompt级策略操纵评分结果，同时保持高伪装性


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型在自动短答案评分方面表现出色，但其对抗攻击脆弱性引发了评分公平性和可靠性的严重担忧，需要系统评估LLM评分模型的脆弱性

Method: 提出了GradingAttack框架，将通用攻击方法与ASAG特定目标对齐，设计了token级和prompt级攻击策略来操纵评分结果，同时保持高伪装性；还提出了新的评估指标来量化攻击伪装

Result: 在多个数据集上的实验表明，两种攻击策略都能有效误导评分模型，其中prompt级攻击成功率更高，而token级攻击具有更好的伪装能力

Conclusion: 研究结果强调了需要开发鲁棒的防御机制来确保ASAG的公平性和可靠性，暴露了LLM评分系统的脆弱性

Abstract: Large language models (LLMs) have demonstrated remarkable potential for automatic short answer grading (ASAG), significantly boosting student assessment efficiency and scalability in educational scenarios. However, their vulnerability to adversarial manipulation raises critical concerns about automatic grading fairness and reliability. In this paper, we introduce GradingAttack, a fine-grained adversarial attack framework that systematically evaluates the vulnerability of LLM based ASAG models. Specifically, we align general-purpose attack methods with the specific objectives of ASAG by designing token-level and prompt-level strategies that manipulate grading outcomes while maintaining high camouflage. Furthermore, to quantify attack camouflage, we propose a novel evaluation metric that balances attack success and camouflage. Experiments on multiple datasets demonstrate that both attack strategies effectively mislead grading models, with prompt-level attacks achieving higher success rates and token-level attacks exhibiting superior camouflage capability. Our findings underscore the need for robust defenses to ensure fairness and reliability in ASAG. Our code and datasets are available at https://anonymous.4open.science/r/GradingAttack.

</details>


### [134] [DTAMS: High-Capacity Generative Steganography via Dynamic Multi-Timestep Selection and Adaptive Deviation Mapping in Latent Diffusion](https://arxiv.org/abs/2602.01160)
*Yuhao Xue,Jiuan Zhou,Yu Cheng,Zhaoxia Yin*

Main category: cs.CR

TL;DR: 提出DTAMS框架，通过动态多时间步自适应嵌入机制、全局子区间映射策略和多维联合约束机制，在扩散模型中实现高嵌入率（12 bpp）的同时保持强鲁棒性和安全性。


<details>
  <summary>Details</summary>
Motivation: 现有生成式图像隐写方法在低嵌入率下才能保持可接受的安全性和鲁棒性，严重限制了隐写系统的实际应用。需要解决高嵌入率下的性能下降问题。

Method: 1. 基于扩散模型转换成本建模的动态多时间步自适应嵌入机制，自动选择最优嵌入时间步；2. 全局子区间映射策略，将点扰动转换为区间级统计映射；3. 像素、潜空间和语义层面的多维联合约束机制，减少重复潜像素变换带来的失真。

Result: DTAMS实现了12 bpp的高嵌入率，同时保持优异的安全性和鲁棒性。在所有评估条件下，平均提取错误率降低了59.39%，显著优于现有最优方法。

Conclusion: 提出的DTAMS框架通过创新的动态嵌入机制和映射策略，成功解决了生成式图像隐写在追求高嵌入率时面临的安全性和鲁棒性挑战，为实际应用提供了可行方案。

Abstract: With the rapid development of AIGC technologies, generative image steganography has attracted increasing attention due to its high imperceptibility and flexibility. However, existing generative steganography methods often maintain acceptable security and robustness only at relatively low embedding rates, severely limiting the practical applicability of steganographic systems. To address this issue, we propose a novel DTAMS framework that achieves high embedding rates while ensuring strong robustness and security. Specifically, a dynamic multi-timestep adaptive embedding mechanism is constructed based on transition-cost modeling in diffusion models, enabling automatic selection of optimal embedding timesteps to improve embedding rates while preserving overall performance. Meanwhile, we propose a global sub-interval mapping strategy that jointly considers mapping errors and the frequency distribution of secret information, converting point-wise perturbations into interval-level statistical mappings to suppress error accumulation and distribution drift during multi-step diffusion processes. Furthermore, a multi-dimensional joint constraint mechanism is introduced to mitigate distortions caused by repeated latent-pixel transformations by jointly regularizing embedding errors at the pixel, latent, and semantic levels. Experiments demonstrate that the proposed method achieves an embedding rate of 12 bpp while maintaining excellent security and robustness. Across all evaluated conditions, DTAMS reduces the average extraction error rate by 59.39%, representing a significant improvement over SOTA methods.

</details>


### [135] [FedBGS: A Blockchain Approach to Segment Gossip Learning in Decentralized Systems](https://arxiv.org/abs/2602.01185)
*Fabio Turazza,Marcello Pietri,Marco Picone,Marco Mamei*

Main category: cs.CR

TL;DR: FedBGS是一个完全去中心化的基于区块链的联邦学习框架，通过分段八卦学习和联邦分析优化区块链使用，提供全面的攻击防护，确保隐私、安全和非独立同分布数据处理。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习中服务器作为单点故障存在安全和可扩展性限制，尽管有隐私保护技术，但仍存在安全漏洞。需要一种完全去中心化的解决方案来消除单点故障，同时提供全面的攻击防护。

Method: 提出FedBGS框架，结合区块链技术和分段八卦学习，通过联邦分析优化区块链使用，实现完全去中心化的联邦学习架构。

Result: 该框架能够消除传统联邦学习中的单点故障问题，提供全面的攻击防护，同时优化区块链资源使用，确保隐私、安全和非独立同分布数据处理能力。

Conclusion: FedBGS为隐私保护联邦学习提供了一个完全去中心化的解决方案，通过区块链和分段八卦学习技术解决了传统联邦学习在安全性和可扩展性方面的局限性。

Abstract: Privacy-Preserving Federated Learning (PPFL) is a Decentralized machine learning paradigm that enables multiple participants to collaboratively train a global model without sharing their data with the integration of cryptographic and privacy-based techniques to enhance the security of the global system. This privacy-oriented approach makes PPFL a highly suitable solution for training shared models in sectors where data privacy is a critical concern. In traditional FL, local models are trained on edge devices, and only model updates are shared with a central server, which aggregates them to improve the global model. However, despite the presence of the aforementioned privacy techniques, in the classical Federated structure, the issue of the server as a single-point-of-failure remains, leading to limitations both in terms of security and scalability. This paper introduces FedBGS, a fully Decentralized Blockchain-based framework that leverages Segmented Gossip Learning through Federated Analytics. The proposed system aims to optimize blockchain usage while providing comprehensive protection against all types of attacks, ensuring both privacy, security and non-IID data handling in Federated environments.

</details>


### [136] [Bifrost: A Much Simpler Secure Two-Party Data Join Protocol for Secure Data Analytics](https://arxiv.org/abs/2602.01225)
*Shuyu Chen,Mingxun Zhou,Haoyu Niu,Guopeng Lin,Weili Han*

Main category: cs.CR

TL;DR: Bifrost是一个简单高效的冗余消除安全数据连接协议，相比现有方案显著提升了性能并减少了通信开销。


<details>
  <summary>Details</summary>
Motivation: 现有安全数据连接方案存在明显缺陷：基于电路PSI的方案会产生冗余的虚拟行，增加下游计算开销；iPrivJoin虽然能消除冗余，但依赖复杂的OPPRF和多次不经意洗牌，通信开销巨大。

Method: Bifrost基于两个简单构建块：ECDH-PSI协议和两方不经意洗牌协议，避免了复杂的OPPRF。采用"双重映射"优化将不经意洗牌轮次从2减少到1。

Result: 在100GB数据集上，Bifrost相比iPrivJoin实现2.54-22.32倍加速，通信减少84.15%-88.97%，通信量接近输入数据大小。在完整SDA流程中，避免了虚拟行导致的错误率爆炸，SDA过程加速达2.80倍，通信减少73.15%。

Conclusion: Bifrost通过简单的协议设计和优化，实现了高效的无冗余安全数据连接，显著提升了性能和实用性，为安全多方计算数据分析提供了更好的基础。

Abstract: Secure data join enables two parties with vertically distributed data to securely compute the joined table, allowing the parties to perform downstream Secure multi-party computation-based Data Analytics (SDA), such as training machine learning models, based on the joined table. While Circuit-based Private Set Intersection (CPSI) can be used for secure data join, it introduces redundant dummy rows in the joined table, which results in high overhead in the downstream SDA tasks. iPrivJoin addresses this issue but introduces significant communication overhead in the redundancy removal process, as it relies on the cryptographic primitive OPPRF for data encoding and multiple rounds of oblivious shuffles. In this paper, we propose a much simpler secure data join protocol, Bifrost, which outputs (the secret shares of) a redundancy-free joined table. The highlight of Bifrost lies in its simplicity: it builds upon two conceptually simple building blocks, an ECDH-PSI protocol and a two-party oblivious shuffle protocol. The lightweight protocol design allows Bifrost to avoid the need for OPPRF. We also proposed a simple optimization named \textit{dual mapping} that reduces the rounds of oblivious shuffle needed from two to one. Experiments on datasets of up to 100 GB show that Bifrost achieves $2.54 \sim 22.32\times$ speedup and reduces the communication by $84.15\% \sim 88.97\%$ compared to the SOTA redundancy-free secure data join protocol iPrivJoin. Notably, the communication size of Bifrost is nearly equal to the size of the input data. In the two-step SDA pipeline evaluation (secure join and SDA), the redundancy-free property of Bifrost not only avoids the catastrophic error rate blowup in the downstream tasks caused by the dummy rows in the joined table (as introduced in CPSI), but also shows up to $2.80\times$ speed-up in the SDA process with up to $73.15\%$ communication reduction.

</details>


### [137] [Privocracy: Online Democracy through Private Voting](https://arxiv.org/abs/2602.01341)
*Pedro Camponês,Hugo Pereira,Adrian Persaud,Kevin Gallagher,Santiago Torres-Arias*

Main category: cs.CR

TL;DR: Privocracy是一种访问控制机制，通过安全电子投票来执行需要敏感资源的命令，减少高权限分配，分散信任，避免单点故障，同时保持访问控制的灵活性。


<details>
  <summary>Details</summary>
Motivation: 传统访问控制中，每个被授予的访问权限和管理账户都会引入额外漏洞，高权限用户的腐败可能危及多个敏感文件。需要一种机制来最小化高权限分配，分散系统信任。

Method: Privocracy采用安全电子投票机制来运行需要敏感资源的命令，实现永恒隐私保护（无论对手计算能力如何，投票都保持机密），并包含投票委托、快速投票轮次和选择性投票审计等实用功能。

Result: 实验结果表明，Privocracy能够高效处理投票，可以在商用硬件上部署。该机制满足了实用安全系统的可靠性要求。

Conclusion: Privocracy通过电子投票机制最小化高权限分配，分散系统信任，减少单点故障漏洞，同时保持访问控制的灵活性，为组织提供了一种更安全的访问控制解决方案。

Abstract: In traditional access control policies, every access granted and administrative account introduces an additional vulnerability, as a corruption of a high-privilege user can compromise several sensitive files. Privocracy is an access control mechanism that minimizes the need to attribute high privileges by triggering a secure e-voting procedure to run commands that require using sensitive resources. With Privocracy an organization can distribute trust in resource access, minimizing the system vulnerabilities from single points of failure, all while maintaining the high flexibility of discretionary access control policies.
  The Privocracy voting mechanism achieves everlasting privacy, ensuring votes remain confidential regardless of an adversary's computational power, while addressing the dependability requirements of a practical and secure system. The procedure incorporates useful features such as vote delegation to reduce voter fatigue, rapid voting rounds to enable quick action during emergencies, and selective vote auditing for application-level accountability. Our experimental results demonstrate that Privocracy processes votes efficiently and can be deployed on commodity hardware.

</details>


### [138] [Adaptive Quantum-Safe Cryptography for 6G Vehicular Networks via Context-Aware Optimization](https://arxiv.org/abs/2602.01342)
*Poushali Sengupta,Mayank Raikwar,Sabita Maharjan,Frank Eliassen,Yan Zhang*

Main category: cs.CR

TL;DR: 提出自适应后量子密码框架，通过预测移动性和信道变化，动态选择适合的密码配置，降低6G车联网延迟和通信开销，同时防止算法切换时的安全攻击。


<details>
  <summary>Details</summary>
Motivation: 未来量子计算机可能破解现有车联网安全机制，而后量子密码学虽然能提供保护，但通常需要更多计算资源，会减慢通信速度，这对需要低延迟的6G车联网构成挑战。

Method: 提出自适应后量子密码框架，预测短期移动性和信道变化，使用预测性多目标进化算法动态选择基于格、代码或哈希的密码配置，并设计安全单调升级协议防止算法切换时的攻击。

Result: 实验显示该框架能降低端到端延迟达27%，减少通信开销达65%，有效稳定密码切换行为，并在对抗场景下成功防止降级、重放和失步攻击。

Conclusion: 该研究为未来6G车联网实现量子安全密码学提供了实用路径，通过自适应框架平衡安全性和性能需求。

Abstract: Powerful quantum computers in the future may be able to break the security used for communication between vehicles and other devices (Vehicle-to-Everything, or V2X). New security methods called post-quantum cryptography can help protect these systems, but they often require more computing power and can slow down communication, posing a challenge for fast 6G vehicle networks. In this paper, we propose an adaptive post-quantum cryptography (PQC) framework that predicts short-term mobility and channel variations and dynamically selects suitable lattice-, code-, or hash-based PQC configurations using a predictive multi-objective evolutionary algorithm (APMOEA) to meet vehicular latency and security constraints.However, frequent cryptographic reconfiguration in dynamic vehicular environments introduces new attack surfaces during algorithm transitions. A secure monotonic-upgrade protocol prevents downgrade, replay, and desynchronization attacks during transitions. Theoretical results show decision stability under bounded prediction error, latency boundedness under mobility drift, and correctness under small forecast noise. These results demonstrate a practical path toward quantum-safe cryptography in future 6G vehicular networks. Through extensive experiments based on realistic mobility (LuST), weather (ERA5), and NR-V2X channel traces, we show that the proposed framework reduces end-to-end latency by up to 27\%, lowers communication overhead by up to 65\%, and effectively stabilizes cryptographic switching behavior using reinforcement learning. Moreover, under the evaluated adversarial scenarios, the monotonic-upgrade protocol successfully prevents downgrade, replay, and desynchronization attacks.

</details>


### [139] [CIPHER: Cryptographic Insecurity Profiling via Hybrid Evaluation of Responses](https://arxiv.org/abs/2602.01438)
*Max Manolov,Tony Gao,Siddharth Shukla,Cheng-Ting Chou,Ryan Lagasse*

Main category: cs.CR

TL;DR: CIPHER是一个用于评估LLM生成Python代码中加密漏洞的基准测试，通过不同安全提示变体测量漏洞发生率，发现明确的安全提示能减少某些问题但不能可靠消除所有加密漏洞。


<details>
  <summary>Details</summary>
Motivation: LLM越来越多地用于辅助开发人员编写代码，但其生成的加密功能实现经常包含可利用的缺陷。微小的设计选择（如静态初始化向量或缺少认证）可能无声地破坏安全保证。

Method: 引入CIPHER基准测试，使用不安全/中性/安全提示变体，基于加密特定漏洞分类法，通过自动化评分管道进行行级归因，测量LLM生成Python代码中的加密漏洞发生率。

Result: 在广泛使用的LLM集合中，明确的"安全"提示能减少某些目标问题，但不能可靠地消除整体加密漏洞。

Conclusion: 需要更好的方法来确保LLM生成的加密代码的安全性，基准测试和可复现的评分管道将在发表时公开。

Abstract: Large language models (LLMs) are increasingly used to assist developers with code, yet their implementations of cryptographic functionality often contain exploitable flaws. Minor design choices (e.g., static initialization vectors or missing authentication) can silently invalidate security guarantees. We introduce CIPHER(\textbf{C}ryptographic \textbf{I}nsecurity \textbf{P}rofiling via \textbf{H}ybrid \textbf{E}valuation of \textbf{R}esponses), a benchmark for measuring cryptographic vulnerability incidence in LLM-generated Python code under controlled security-guidance conditions. CIPHER uses insecure/neutral/secure prompt variants per task, a cryptography-specific vulnerability taxonomy, and line-level attribution via an automated scoring pipeline. Across a diverse set of widely used LLMs, we find that explicit ``secure'' prompting reduces some targeted issues but does not reliably eliminate cryptographic vulnerabilities overall. The benchmark and reproducible scoring pipeline will be publicly released upon publication.

</details>


### [140] [DuoLungo: Usability Study of Duo 2FA](https://arxiv.org/abs/2602.01489)
*Renascence Tarafder Prapty,Gene Tsudik*

Main category: cs.CR

TL;DR: 对Duo MFA系统进行的大规模可用性研究，发现平均验证耗时约8秒，失败率4.35%，SUS得分70表明良好可用性，用户认为易用但有些烦人，同时增强了安全感。


<details>
  <summary>Details</summary>
Motivation: Duo作为广泛使用的多因素认证系统，虽然被众多大型组织和教育机构采用，但其可用性缺乏全面和最新的研究。先前研究主要关注技术部署挑战，未测量核心可用性指标如任务完成时间或SUS分数，且这些结果已过时，当时用户对MFA还不熟悉。

Method: 在加州大学欧文分校进行长期大规模研究（2024-2025学年），涉及2559名参与者。使用认证日志数据分析，并对57名随机选择的用户进行问卷调查。

Result: Duo Push任务平均耗时近8秒，参与者描述为短到中等。耗时受时间、专业领域和教育水平影响。认证失败率4.35%，43.86%受访者报告至少一次登录失败。Duo的SUS得分为70，表明良好可用性。用户普遍认为易用但有些烦人，同时报告增强了账户安全感。

Conclusion: Duo MFA系统具有良好的可用性（SUS 70），虽然存在一定失败率和用户烦恼感，但总体上用户认为易用且增强了安全性。研究还识别了常见问题和改进建议，为MFA系统的设计和优化提供了实证依据。

Abstract: Multi-Factor Authentication (MFA) enhances login security by requiring multiple authentication factors. Its adoption has increased in response to more frequent and sophisticated attacks. Duo is widely used by organizations including Fortune 500 companies and major educational institutions, yet its usability has not been examined thoroughly or recently. Earlier studies focused on technical challenges during initial deployment but did not measure core usability metrics such as task completion time or System Usability Scale (SUS) scores. These results are also outdated, originating from a time when MFA was less familiar to typical users.
  We conducted a long-term, large-scale Duo usability study at the University of California Irvine during the 2024-2025 academic year, involving 2559 participants. Our analysis uses authentication log data and a survey of 57 randomly selected users. The average overhead of a Duo Push task is nearly 8 seconds, which participants described as short to moderate. Overhead varies with time of day, field of study, and education level. The rate of authentication failures due to incomplete Duo tasks is 4.35 percent, and 43.86 percent of survey respondents reported at least one Duo login failure. The Duo SUS score is 70, indicating good usability. Participants generally find Duo easy to use but somewhat annoying, while also reporting an increased sense of account security. They also described common issues and offered suggestions for improvement.

</details>


### [141] [Sleep Reveals the Nonce: Breaking ECDSA using Sleep-Based Power Side-Channel Vulnerability](https://arxiv.org/abs/2602.01491)
*Sahan Sanjaya,Prabhat Mishra*

Main category: cs.CR

TL;DR: 该论文发现了一种利用睡眠函数引起的电源尖峰来提取ECDSA随机数的新侧信道攻击方法，即使是在恒定时间和掩码实现下也能恢复部分随机数。


<details>
  <summary>Details</summary>
Motivation: ECDSA的安全性依赖于每个签名的随机数保密性，即使部分随机数泄露也可能通过基于格密码的分析暴露长期私钥。现有研究主要关注传统电源侧信道攻击，但睡眠函数引起的处理器上下文切换产生的电源波动尚未被充分研究。

Method: 利用睡眠函数调用时处理器上下文切换产生的电源尖峰，这些波动与标量乘法中依赖随机数的操作相关。攻击方法通过分析睡眠诱导的上下文切换期间的电源包络变化来提取ECDSA随机数信息。

Result: 在多个密码库（RustCrypto、BearSSL、GoCrypto）和处理器架构（ARM、RISC-V）上评估攻击效果，实验显示睡眠诱导的上下文切换期间的电源包络细微变化提供了足够的泄露信息，能够恢复20位随机数。

Conclusion: 睡眠诱导的电源尖峰是一种实用的跨平台侧信道威胁，需要重新考虑密码系统的设计选择，特别是在处理睡眠函数和上下文切换时的安全防护措施。

Abstract: Security of Elliptic Curve Digital Signature Algorithm (ECDSA) depends on the secrecy of the per-signature nonce. Even partial nonce leakage can expose the long-term private key through lattice-based cryptanalysis. In this paper, we introduce a previously unexplored power side-channel vulnerability that exploits sleep-induced power spikes to extract ECDSA nonces. Unlike conventional power-based side-channel attacks, this vulnerability leverages power fluctuations generated during processor context switches invoked by sleep functions. These fluctuations correlate with nonce-dependent operations in scalar multiplication, enabling nonce recovery even under constant-time and masked implementations. We evaluate the attack across multiple cryptographic libraries, RustCrypto, BearSSL, and GoCrypto, and processor architectures, including ARM and RISC-V. Our experiments show that subtle variations in the power envelope during sleep-induced context switches provide sufficient leakage for practical ECDSA nonce extraction, recovering 20 bits of the nonce. These results establish sleep-induced power spikes as a practical cross-platform side-channel threat and highlight the need to reconsider design choices in cryptographic systems.

</details>


### [142] [Are Security Cues Static? Rethinking Warning and Trust Indicators for Life Transitions](https://arxiv.org/abs/2602.01544)
*Sarah Tabassum*

Main category: cs.CR

TL;DR: 该论文提出安全提示（如警告和信任信号）应适应人生过渡阶段的变化，而非保持静态设计，并提出了过渡感知安全提示框架。


<details>
  <summary>Details</summary>
Motivation: 当前安全提示设计为静态界面元素，但人们的生活、环境和脆弱性会随时间变化（如移民、老龄化、环境变化等）。这些人生过渡阶段会重塑人们对风险和信任的理解与应对方式，而现有系统很少根据这些变化调整安全提示，将解释负担转嫁给用户。

Method: 基于教育移民的实证研究作为案例，扩展到其他人生过渡阶段，提出了过渡感知安全提示（TASeC）框架，并通过推测性设计概念展示安全提示如何在过渡阶段演变。

Result: 提出了一个框架，展示安全提示如何适应人生过渡阶段的变化，并提供了具体的设计概念示例。

Conclusion: 安全提示的静态性质与过渡性人生之间存在设计不匹配，呼吁HCI领域将安全提示重新构想为纵向的、以生活为中心的设计元素。

Abstract: Security cues, such as warnings and trust signals, are designed as stable interface elements, even though people's lives, contexts, and vulnerabilities change over time. Life transitions including migration, aging, or shifts in institutional environments reshape how risk and trust are understood and acted upon. Yet current systems rarely adapt their security cues to these changing conditions, placing the burden of interpretation on users. In this Works-in-Progress paper, we argue that the static nature of security cues represents a design mismatch with transitional human lives. We draw on prior empirical insights from work on educational migration as a motivating case, and extend the discussion to other life transitions. Building on these insights, we introduce the Transition-Aware Security Cues (TASeC) framework and present speculative design concepts illustrating how security cues might evolve across transition stages. We invite HCI to rethink security cues as longitudinal, life-centered design elements collectively.

</details>


### [143] [Expected Harm: Rethinking Safety Evaluation of (Mis)Aligned LLMs](https://arxiv.org/abs/2602.01600)
*Yen-Shan Chen,Zhi Rui Tam,Cheng-Kuang Wu,Yun-Nung Chen*

Main category: cs.CR

TL;DR: 论文提出"预期危害"新指标，结合恶意查询的严重性和执行可能性，揭示LLM存在"反向风险校准"问题：模型过度拒绝低可能性威胁，却对高可能性攻击保持脆弱。


<details>
  <summary>Details</summary>
Motivation: 当前LLM安全评估主要依赖基于严重性的分类法，假设所有恶意查询风险均匀，忽视了"执行可能性"——给定模型响应后威胁被实现的概率。这种评估框架需要重新审视。

Method: 引入"预期危害"指标，将越狱严重性按执行可能性加权，执行可能性建模为执行成本的函数。通过实证分析最先进模型，并利用线性探测技术追溯失败根源。

Result: 发现系统性的"反向风险校准"现象：模型对低可能性（高成本）威胁表现出过强的拒绝行为，但对高可能性（低成本）查询保持脆弱。利用此特性可将现有越狱攻击成功率提升高达2倍。

Conclusion: 模型在潜在空间中编码严重性以驱动拒绝决策，但没有可区分的执行成本内部表征，使其对这一关键风险维度"视而不见"，导致结构性漏洞。

Abstract: Current evaluations of LLM safety predominantly rely on severity-based taxonomies to assess the harmfulness of malicious queries. We argue that this formulation requires re-examination as it assumes uniform risk across all malicious queries, neglecting Execution Likelihood--the conditional probability of a threat being realized given the model's response. In this work, we introduce Expected Harm, a metric that weights the severity of a jailbreak by its execution likelihood, modeled as a function of execution cost. Through empirical analysis of state-of-the-art models, we reveal a systematic Inverse Risk Calibration: models disproportionately exhibit stronger refusal behaviors for low-likelihood (high-cost) threats while remaining vulnerable to high-likelihood (low-cost) queries. We demonstrate that this miscalibration creates a structural vulnerability: by exploiting this property, we increase the attack success rate of existing jailbreaks by up to $2\times$. Finally, we trace the root cause of this failure using linear probing, which reveals that while models encode severity in their latent space to drive refusal decisions, they possess no distinguishable internal representation of execution cost, making them "blind" to this critical dimension of risk.

</details>


### [144] [Efficient Softmax Reformulation for Homomorphic Encryption via Moment Generating Function](https://arxiv.org/abs/2602.01621)
*Hanjun Park,Byeong-Seo Min,Jiheon Woo,Min-Wook Jeong,Jongho Shin,Yongwoo Lee,Young-Sik Kim,Yongjune Kim*

Main category: cs.CR

TL;DR: MGF-softmax：基于矩生成函数的softmax重构方法，显著降低同态加密中softmax计算的多重深度，同时保持高精度


<details>
  <summary>Details</summary>
Motivation: 同态加密（HE）是隐私保护机器学习的重要框架，但softmax作为transformer架构的核心组件，在同态加密中评估特别困难，原因包括其多元结构、指数函数引起的大动态范围以及归一化过程中需要精确除法

Method: 提出MGF-softmax，基于矩生成函数（MGF）的新型softmax重构方法，用基于矩的对应项替换softmax分母，显著减少乘法深度，同时保持softmax的关键特性，并随着输入token数量增加渐近收敛到精确softmax

Result: 在Vision Transformers和大型语言模型上的广泛实验表明，MGF-softmax在加密推理中提供了高效且准确的softmax近似，实现了接近高深度精确方法的推理精度，同时通过减少乘法深度显著降低计算成本

Conclusion: MGF-softmax是同态加密中softmax计算的有效解决方案，在保持精度的同时大幅降低计算复杂度，为隐私保护机器学习中的transformer架构应用提供了实用方法

Abstract: Homomorphic encryption (HE) is a prominent framework for privacy-preserving machine learning, enabling inference directly on encrypted data. However, evaluating softmax, a core component of transformer architectures, remains particularly challenging in HE due to its multivariate structure, the large dynamic range induced by exponential functions, and the need for accurate division during normalization. In this paper, we propose MGF-softmax, a novel softmax reformulation based on the moment generating function (MGF) that replaces the softmax denominator with its moment-based counterpart. This reformulation substantially reduces multiplicative depth while preserving key properties of softmax and asymptotically converging to the exact softmax as the number of input tokens increases. Extensive experiments on Vision Transformers and large language models show that MGF-softmax provides an efficient and accurate approximation of softmax in encrypted inference. In particular, it achieves inference accuracy close to that of high-depth exact methods, while requiring substantially lower computational cost through reduced multiplicative depth.

</details>


### [145] [Witnessd: Proof-of-process via Adversarial Collapse](https://arxiv.org/abs/2602.01663)
*David Condrey*

Main category: cs.CR

TL;DR: 论文提出"过程证明"概念，通过"抖动密封"技术记录打字过程的微秒级延迟，将模糊的怀疑转化为可证伪的指控，解决AI生成文本的签名验证问题。


<details>
  <summary>Details</summary>
Motivation: 数字签名只能证明密钥持有，不能证明作者身份。当作者使用AI生成文本后，通过事后构建中间文档状态并签名，可以创建与真实创作过程无法区分的签名链。这揭示了密码学完整性与过程来源之间的差距。

Method: 提出"过程证明"原语类别，引入"抖动密封"技术：通过HMAC从会话密钥、按键顺序和累积文档哈希生成不可感知的微秒级延迟。构建Witnessd架构，结合可验证延迟函数、外部时间戳锚点、双源按键验证和可选的硬件证明。

Result: 在31,000次验证试验中实现了对无效证明的确定性拒绝。系统不防止伪造（内核级对手可以击败它），但将模糊怀疑转化为需要跨越独立信任边界协调指控的可证伪主张。

Conclusion: 贡献在于提出"对抗性崩溃原则"作为评估标准，将过程证据系统从防止伪造转变为将模糊怀疑转化为具体、可测试的指控，要求对手在多个独立信任组件上协调攻击。

Abstract: Digital signatures prove key possession, not authorship. An author who generates text with AI, constructs intermediate document states post-hoc, and signs each hash produces a signature chain indistinguishable from genuine composition. We address this gap between cryptographic integrity and process provenance. We introduce proof-of-process, a primitive category for evidence that a physical process, not merely a signing key, produced a digital artifact. Our construction, the jitter seal, injects imperceptible microsecond delays derived via HMAC from a session secret, keystroke ordinal, and cumulative document hash. Valid evidence requires that real keystrokes produced the document through those intermediate states. We propose the Adversarial Collapse Principle as an evaluation criterion: evidence systems should be judged by whether disputing them requires a conjunction of specific, testable allegations against components with independent trust assumptions. We present Witnessd, an architecture combining jitter seals with Verifiable Delay Functions, external timestamp anchors, dual-source keystroke validation, and optional hardware attestation. Each layer forces allegations at different capability levels; disputing authentic evidence requires coordinated claims across independent trust boundaries. The system does not prevent forgery: a kernel-level adversary can defeat it, and typing AI-generated content produces valid evidence. The contribution is converting vague doubt into falsifiable allegations. We evaluate across 31,000 verification trials with deterministic rejection of invalid proofs.

</details>


### [146] [Backdoor Sentinel: Detecting and Detoxifying Backdoors in Diffusion Models via Temporal Noise Consistency](https://arxiv.org/abs/2602.01765)
*Bingzheng Wang,Xiaoyan Gu,Hongbo Xu,Hongcheng Li,Zimo Yu,Jiang Zhou,Weiping Wang*

Main category: cs.CR

TL;DR: 本文提出TNC-Defense框架，利用扩散模型中时间噪声不一致性现象，实现后门检测与去毒的统一解决方案，在保护模型参数隐私的灰盒场景下有效工作。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在AIGC服务中广泛应用，但其依赖不透明的训练数据和流程存在后门注入风险。实际审计场景中，由于知识产权和商业机密保护，审计者通常无法访问模型参数，现有白盒或查询密集型检测方法不实用。更重要的是，即使检测到后门，现有去毒方法往往在去毒效果和生成质量之间陷入两难。

Method: 基于发现的时间噪声不一致性现象（触发输入时相邻扩散时间步的噪声预测在特定时间片段被破坏），提出TNC-Defense统一框架：1）灰盒检测模块：利用相邻时间步噪声一致性设计检测方法，识别和定位异常扩散时间步；2）去毒模块：使用识别的异常时间步构建触发无关、时间步感知的去毒模块，直接纠正后门生成路径。

Result: 在五种代表性后门攻击场景下评估，TNC-Defense将平均检测准确率提高11%，额外开销可忽略；平均使98.5%的触发样本失效，仅对生成质量造成轻微下降。

Conclusion: TNC-Defense框架有效解决了扩散模型后门检测与去毒的难题，在保护模型参数隐私的灰盒场景下实现了高效检测和有效去毒，平衡了去毒效果与生成质量。

Abstract: Diffusion models have been widely deployed in AIGC services; however, their reliance on opaque training data and procedures exposes a broad attack surface for backdoor injection. In practical auditing scenarios, due to the protection of intellectual property and commercial confidentiality, auditors are typically unable to access model parameters, rendering existing white-box or query-intensive detection methods impractical. More importantly, even after the backdoor is detected, existing detoxification approaches are often trapped in a dilemma between detoxification effectiveness and generation quality.
  In this work, we identify a previously unreported phenomenon called temporal noise unconsistency, where the noise predictions between adjacent diffusion timesteps is disrupted in specific temporal segments when the input is triggered, while remaining stable under clean inputs. Leveraging this finding, we propose Temporal Noise Consistency Defense (TNC-Defense), a unified framework for backdoor detection and detoxification. The framework first uses the adjacent timestep noise consistency to design a gray-box detection module, for identifying and locating anomalous diffusion timesteps. Furthermore, the framework uses the identified anomalous timesteps to construct a trigger-agnostic, timestep-aware detoxification module, which directly corrects the backdoor generation path. This effectively suppresses backdoor behavior while significantly reducing detoxification costs.
  We evaluate the proposed method under five representative backdoor attack scenarios and compare it with state-of-the-art defenses. The results show that TNC-Defense improves the average detection accuracy by $11\%$ with negligible additional overhead, and invalidates an average of $98.5\%$ of triggered samples with only a mild degradation in generation quality.

</details>


### [147] [RedVisor: Reasoning-Aware Prompt Injection Defense via Zero-Copy KV Cache Reuse](https://arxiv.org/abs/2602.01795)
*Mingrui Liu,Sixiao Zhang,Cheng Long,Kwok-Yan Lam*

Main category: cs.CR

TL;DR: RedVisor是一个统一的框架，通过轻量级可移除适配器同时实现提示注入攻击检测和模型安全响应，在保持骨干模型原始性能的同时提高防御效率。


<details>
  <summary>Details</summary>
Motivation: 当前LLM防御提示注入攻击面临两难：基于预防的微调会降低通用性能（"对齐税"），而基于检测的过滤则带来高延迟和内存成本。需要一种能兼顾检测可解释性和无缝集成的解决方案。

Method: 提出RedVisor框架，在冻结骨干模型上部署轻量级可移除适配器。适配器首先生成可解释分析，精确定位注入并阐明威胁，然后显式指导模型拒绝恶意指令。适配器仅在推理阶段激活，在后续响应生成时静默，实现KV缓存重用策略。

Result: RedVisor在检测准确率和吞吐量方面优于现有防御方法，同时带来可忽略的效用损失。通过数学证明保持骨干模型在良性输入上的原始性能，并集成到vLLM服务引擎中。

Conclusion: RedVisor成功解决了提示注入防御中的权衡问题，通过统一框架实现了检测可解释性和预防策略的无缝集成，在保持模型性能的同时提高了防御效率。

Abstract: Large Language Models (LLMs) are increasingly vulnerable to Prompt Injection (PI) attacks, where adversarial instructions hidden within retrieved contexts hijack the model's execution flow. Current defenses typically face a critical trade-off: prevention-based fine-tuning often degrades general utility via the "alignment tax", while detection-based filtering incurs prohibitive latency and memory costs. To bridge this gap, we propose RedVisor, a unified framework that synthesizes the explainability of detection systems with the seamless integration of prevention strategies. To the best of our knowledge, RedVisor is the first approach to leverage fine-grained reasoning paths to simultaneously detect attacks and guide the model's safe response. We implement this via a lightweight, removable adapter positioned atop the frozen backbone. This adapter serves a dual function: it first generates an explainable analysis that precisely localizes the injection and articulates the threat, which then explicitly conditions the model to reject the malicious command. Uniquely, the adapter is active only during this reasoning phase and is effectively muted during the subsequent response generation. This architecture yields two distinct advantages: (1) it mathematically preserves the backbone's original utility on benign inputs; and (2) it enables a novel KV Cache Reuse strategy, eliminating the redundant prefill computation inherent to decoupled pipelines. We further pioneer the integration of this defense into the vLLM serving engine with custom kernels. Experiments demonstrate that RedVisor outperforms state-of-the-art defenses in detection accuracy and throughput while incurring negligible utility loss.

</details>


### [148] [Things that Matter -- Identifying Interactions and IoT Device Types in Encrypted Matter Traffic](https://arxiv.org/abs/2602.01932)
*Kristopher Alex Schlett,Bela Genge,Savio Sciancalepore*

Main category: cs.CR

TL;DR: 本文通过分析加密的Matter物联网流量，发现被动攻击者可以通过流量元数据模式推断设备交互和设备类型，对用户隐私构成严重威胁。


<details>
  <summary>Details</summary>
Motivation: Matter作为最新的物联网应用层标准，虽然强调安全性和隐私保护，但缺乏对被动攻击者通过分析加密流量来推断信息的系统性研究。本文旨在填补这一空白，评估Matter标准对加密流量分析的鲁棒性。

Method: 使用从真实世界测试平台和模拟设置收集的各种数据集，分析加密Matter流量的元数据模式。识别允许推断终端设备与控制器之间特定交互的模式，并将交互序列模式与特定类型的物联网设备关联。

Result: 研究发现可以以超过95%的准确率识别加密流量中的特定Matter交互（即使在存在数据包丢失和延迟的情况下），并且可以以至少88%的准确率识别Matter设备类型。CSA已承认这些发现并表示愿意在标准的下一个版本中解决这些漏洞。

Conclusion: Matter标准虽然提供了强大的加密通信，但其加密流量中的元数据模式仍然可以被被动攻击者利用来推断设备交互和设备类型，构成严重的隐私泄露风险。需要进一步改进标准以抵御此类加密流量分析攻击。

Abstract: Matter is the most recent application-layer standard for the Internet of Things (IoT). As one of its major selling points, Matter's design imposes particular attention to security and privacy: it provides validated secure session establishment protocols, and it uses robust security algorithms to secure communications between IoT devices and Matter controllers. However, to our knowledge, there is no systematic analysis investigating the extent to which a passive attacker, in possession of lower layer keys or exploiting security misconfiguration at those layers, could infer information by passively analyzing encrypted Matter traffic. In this paper, we fill this gap by analyzing the robustness of the Matter IoT standard to encrypted traffic analysis performed by a passive eavesdropper. By using various datasets collected from real-world testbeds and simulated setups, we identify patterns in metadata of the encrypted Matter traffic that allow inferring the specific interactions occurring between end devices and controllers. Moreover, we associate patterns in sequences of interactions to specific types of IoT devices. These patterns can be used to create fingerprints that allow a passive attacker to infer the type of devices used in the network, constituting a serious breach of users privacy. Our results reveal that we can identify specific Matter interactions that occur in encrypted traffic with over $95\%$ accuracy also in the presence of packet losses and delays. Moreover, we can identify Matter device types with a minimum accuracy of $88\%$. The CSA acknowledged our findings, and expressed the willingness to address such vulnerabilities in the next releases of the standard.

</details>


### [149] [HPE: Hallucinated Positive Entanglement for Backdoor Attacks in Federated Self-Supervised Learning](https://arxiv.org/abs/2602.02147)
*Jiayao Wang,Yang Song,Zhendong Zhao,Jiale Zhang,Qilin Wu,Wenliang Yuan,Junwu Zhu,Dongfang Zhao*

Main category: cs.CR

TL;DR: 本文提出了一种名为HPE（幻觉正样本纠缠）的联邦自监督学习后门攻击方法，通过合成正样本增强后门特征嵌入，特征纠缠强化触发器与后门样本的绑定，选择性参数中毒和邻近感知更新提升攻击的稳定性和持久性。


<details>
  <summary>Details</summary>
Motivation: 联邦自监督学习（FSSL）虽然能保护隐私，但其安全性容易受到后门攻击的威胁。现有FSSL攻击方法存在中毒样本利用率低、可迁移性有限、持久性弱等问题，需要更有效的攻击方法来揭示安全漏洞。

Method: HPE方法包含三个核心组件：1）使用合成正样本进行幻觉增强，提升编码器对后门特征的嵌入能力；2）引入特征纠缠，在表示空间中强制触发器与后门样本紧密绑定；3）采用选择性参数中毒和邻近感知更新，将中毒模型约束在全局模型附近，增强稳定性和持久性。

Result: 在多个FSSL场景和数据集上的实验结果表明，HPE在性能上显著优于现有的后门攻击方法，并且在各种防御机制下表现出强大的鲁棒性。

Conclusion: HPE方法有效解决了现有FSSL后门攻击的局限性，为联邦自监督学习的安全评估提供了新的攻击基准，揭示了FSSL系统在实际部署中面临的安全风险。

Abstract: Federated self-supervised learning (FSSL) enables collaborative training of self-supervised representation models without sharing raw unlabeled data. While it serves as a crucial paradigm for privacy-preserving learning, its security remains vulnerable to backdoor attacks, where malicious clients manipulate local training to inject targeted backdoors. Existing FSSL attack methods, however, often suffer from low utilization of poisoned samples, limited transferability, and weak persistence. To address these limitations, we propose a new backdoor attack method for FSSL, namely Hallucinated Positive Entanglement (HPE). HPE first employs hallucination-based augmentation using synthetic positive samples to enhance the encoder's embedding of backdoor features. It then introduces feature entanglement to enforce tight binding between triggers and backdoor samples in the representation space. Finally, selective parameter poisoning and proximity-aware updates constrain the poisoned model within the vicinity of the global model, enhancing its stability and persistence. Experimental results on several FSSL scenarios and datasets show that HPE significantly outperforms existing backdoor attack methods in performance and exhibits strong robustness under various defense mechanisms.

</details>


### [150] [SysFuSS: System-Level Firmware Fuzzing with Selective Symbolic Execution](https://arxiv.org/abs/2602.02243)
*Dakshina Tharindu,Aruna Jayasena,Prabhat Mishra*

Main category: cs.CR

TL;DR: SysFuSS是一个结合系统级模糊测试和选择性符号执行的固件验证框架，能有效检测固件漏洞，相比现有方法显著提升分支覆盖率和漏洞检测能力。


<details>
  <summary>Details</summary>
Motivation: 固件作为计算系统中硬件和软件的关键接口，其漏洞可能导致灾难性系统故障。传统模糊测试方法在检测固件漏洞方面效果有限，存在覆盖瓶颈问题，无法有效处理固件与硬件之间的复杂交互。

Method: SysFuSS框架整合了系统级模糊测试和选择性符号执行。首先使用系统级仿真进行初始模糊测试，当覆盖率达到瓶颈时自动切换到符号执行，生成针对性的测试用例以探索固件设计中未覆盖的区域。

Result: 在真实嵌入式固件（包括OpenSSL、WolfBoot、WolfMQTT、HTSlib、MXML和libIEC）上的实验评估表明，SysFuSS在分支覆盖率和固件漏洞检测方面显著优于最先进的模糊测试工具。SysFuSS能检测118个已知漏洞，而现有方法只能覆盖其中13个。此外，SysFuSS激活这些漏洞所需时间显著减少（最多3.3倍，平均1.7倍）。

Conclusion: SysFuSS通过整合系统级模糊测试和选择性符号执行，有效解决了固件验证中的覆盖瓶颈问题，显著提升了固件漏洞检测的效率和效果，为固件安全验证提供了有效的解决方案。

Abstract: Firmware serves as the critical interface between hardware and software in computing systems, making any bugs or vulnerabilities particularly dangerous as they can cause catastrophic system failures. While fuzzing is a promising approach for identifying design flaws and security vulnerabilities, traditional fuzzers are ineffective at detecting firmware vulnerabilities. For example, existing fuzzers focus on user-level fuzzing, which is not suitable for detecting kernel-level vulnerabilities. Existing fuzzers also face a coverage plateau problem when dealing with complex interactions between firmware and hardware. In this paper, we present an efficient firmware verification framework, SysFuSS, that integrates system-level fuzzing with selective symbolic execution. Our approach leverages system-level emulation for initial fuzzing, and automatically transitions to symbolic execution when coverage reaches a plateau. This strategy enables us to generate targeted test cases that can trigger previously unexplored regions in firmware designs. We have evaluated SysFuSS on real-world embedded firmware, including OpenSSL, WolfBoot, WolfMQTT, HTSlib, MXML, and libIEC. Experimental evaluation demonstrates that SysFuSS significantly outperforms state-of-the-art fuzzers in terms of both branch coverage and detection of firmware vulnerabilities. Specifically, SysFuSS can detect 118 known vulnerabilities while state-of-the-art can cover only 13 of them. Moreover, SysFuSS takes significantly less time (up to 3.3X, 1.7X on average) to activate these vulnerabilities.

</details>


### [151] [Provenance Verification of AI-Generated Images via a Perceptual Hash Registry Anchored on Blockchain](https://arxiv.org/abs/2602.02412)
*Apoorv Mohit,Bhavya Aggarwal,Chinmay Gondhalekar*

Main category: cs.CR

TL;DR: 提出基于区块链的AI生成图像验证框架，通过注册机制追踪内容来源，使用感知哈希和混合存储结构实现可扩展的防篡改验证系统


<details>
  <summary>Details</summary>
Motivation: 随着AI技术的快速发展，合成图像的生成变得日益普遍，这引发了关于虚假信息、数字伪造和内容真实性的担忧。在线平台需要可靠的方法来验证AI生成内容的来源和真实性。

Method: 提出区块链支持的验证框架，采用注册制溯源机制。为每个AI生成图像分配基于感知哈希的数字指纹，在创建时由生成平台注册。使用混合链上/链下存储：Merkle Patricia Trie实现防篡改存储（链上），Burkhard-Keller树（链下）支持大规模图像注册库的高效相似性搜索。

Result: 系统能够在图像重新上传到社交媒体等数字平台时进行验证，即使经过良性变换或部分修改，也能识别先前注册的AI生成图像。该方法不追求检测所有合成图像，而是专注于验证创建时已注册的AI生成内容的来源。

Conclusion: 该框架与现有的水印和学习型检测方法互补，提供了一个平台无关、防篡改的机制，可在大规模在线分发时实现可扩展的内容来源和真实性验证。

Abstract: The rapid advancement of artificial intelligence has made the generation of synthetic images widely accessible, increasing concerns related to misinformation, digital forgery, and content authenticity on large-scale online platforms. This paper proposes a blockchain-backed framework for verifying AI-generated images through a registry-based provenance mechanism. Each AI-generated image is assigned a digital fingerprint that preserves similarity using perceptual hashing and is registered at creation time by participating generation platforms. The hashes are stored on a hybrid on-chain/off-chain public blockchain using a Merkle Patricia Trie for tamper-resistant storage (on-chain) and a Burkhard-Keller tree (off-chain) to enable efficient similarity search over large image registries. Verification is performed when images are re-uploaded to digital platforms such as social media services, enabling identification of previously registered AI-generated images even after benign transformations or partial modifications. The proposed system does not aim to universally detect all synthetic images, but instead focuses on verifying the provenance of AI-generated content that has been registered at creation time. By design, this approach complements existing watermarking and learning-based detection methods, providing a platform-agnostic, tamper-proof mechanism for scalable content provenance and authenticity verification at the point of large-scale online distribution.

</details>
