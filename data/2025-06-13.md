<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 35]
- [cs.MA](#cs.MA) [Total: 2]
- [cs.SE](#cs.SE) [Total: 24]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [From Threat to Tool: Leveraging Refusal-Aware Injection Attacks for Safety Alignment](https://arxiv.org/abs/2506.10020)
*Kyubyung Chae,Hyunbin Jin,Taesup Kim*

Main category: cs.CR

TL;DR: RAAI是一种无需训练、模型无关的框架，通过检测拒绝信号并注入预设短语来生成有害但流畅的补全，显著提升有害响应率，同时增强模型安全性。


<details>
  <summary>Details</summary>
Motivation: 传统对齐大型语言模型（LLMs）需要大量人工标注数据，成本高且耗时。合成数据虽为替代方案，但现有方法复杂且依赖额外模型。

Method: 提出Refusal-Aware Adaptive Injection (RAAI)，利用LLM攻击技术检测拒绝信号并注入预设短语，生成有害补全。

Result: RAAI将有害响应率从2.15%提升至61.04%，且用其生成的数据微调LLMs能增强模型对有害提示的鲁棒性，同时保持标准任务性能。

Conclusion: LLM攻击方法可转化为实用工具，实现可扩展且可控的安全对齐。

Abstract: Safely aligning large language models (LLMs) often demands extensive
human-labeled preference data, a process that's both costly and time-consuming.
While synthetic data offers a promising alternative, current methods frequently
rely on complex iterative prompting or auxiliary models. To address this, we
introduce Refusal-Aware Adaptive Injection (RAAI), a straightforward,
training-free, and model-agnostic framework that repurposes LLM attack
techniques. RAAI works by detecting internal refusal signals and adaptively
injecting predefined phrases to elicit harmful, yet fluent, completions. Our
experiments show RAAI effectively jailbreaks LLMs, increasing the harmful
response rate from a baseline of 2.15% to up to 61.04% on average across four
benchmarks. Crucially, fine-tuning LLMs with the synthetic data generated by
RAAI improves model robustness against harmful prompts while preserving general
capabilities on standard tasks like MMLU and ARC. This work highlights how LLM
attack methodologies can be reframed as practical tools for scalable and
controllable safety alignment.

</details>


### [2] [LLMs Caught in the Crossfire: Malware Requests and Jailbreak Challenges](https://arxiv.org/abs/2506.10022)
*Haoyang Li,Huan Gao,Zhiyuan Zhao,Zhiyu Lin,Junyu Gao,Xuelong Li*

Main category: cs.CR

TL;DR: 该论文提出了MalwareBench基准数据集，用于评估大型语言模型（LLMs）在代码生成中对越狱攻击的脆弱性，发现主流LLMs对恶意代码生成的拒绝能力有限。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在代码生成中对越狱攻击的特定脆弱性，填补现有研究的空白。

Method: 构建包含3,520个越狱提示的MalwareBench数据集，基于320个手动设计的恶意代码生成需求，涵盖11种越狱方法和29种代码功能类别。

Result: 主流LLMs对恶意代码生成的拒绝率平均为60.93%，结合越狱攻击算法后降至39.92%。

Conclusion: LLMs的代码安全能力仍面临重大挑战。

Abstract: The widespread adoption of Large Language Models (LLMs) has heightened
concerns about their security, particularly their vulnerability to jailbreak
attacks that leverage crafted prompts to generate malicious outputs. While
prior research has been conducted on general security capabilities of LLMs,
their specific susceptibility to jailbreak attacks in code generation remains
largely unexplored. To fill this gap, we propose MalwareBench, a benchmark
dataset containing 3,520 jailbreaking prompts for malicious code-generation,
designed to evaluate LLM robustness against such threats. MalwareBench is based
on 320 manually crafted malicious code generation requirements, covering 11
jailbreak methods and 29 code functionality categories. Experiments show that
mainstream LLMs exhibit limited ability to reject malicious code-generation
requirements, and the combination of multiple jailbreak methods further reduces
the model's security capabilities: specifically, the average rejection rate for
malicious content is 60.93%, dropping to 39.92% when combined with jailbreak
attack algorithms. Our work highlights that the code security capabilities of
LLMs still pose significant challenges.

</details>


### [3] [Private Memorization Editing: Turning Memorization into a Defense to Strengthen Data Privacy in Large Language Models](https://arxiv.org/abs/2506.10024)
*Elena Sofia Ruzzetti,Giancarlo A. Xompero,Davide Venditti,Fabio Massimo Zanzotto*

Main category: cs.CR

TL;DR: 论文提出了一种名为PME的方法，通过编辑模型对训练数据的记忆来防止大型语言模型（LLMs）泄露个人可识别信息（PII），从而增强隐私保护。


<details>
  <summary>Details</summary>
Motivation: LLMs可能记忆并泄露PII，现有攻击利用这一特性，因此需要一种方法将模型的记忆能力转化为隐私防御策略。

Method: 通过检测并编辑模型记忆的PII，PME在不影响模型性能的情况下减少PII泄露。

Result: PME有效减少了PII泄露，在某些情况下甚至将隐私攻击的准确率降至零。

Conclusion: PME是一种有效的隐私保护方法，能够在不损害模型性能的前提下增强LLMs的隐私防御能力。

Abstract: Large Language Models (LLMs) memorize, and thus, among huge amounts of
uncontrolled data, may memorize Personally Identifiable Information (PII),
which should not be stored and, consequently, not leaked. In this paper, we
introduce Private Memorization Editing (PME), an approach for preventing
private data leakage that turns an apparent limitation, that is, the LLMs'
memorization ability, into a powerful privacy defense strategy. While attacks
against LLMs have been performed exploiting previous knowledge regarding their
training data, our approach aims to exploit the same kind of knowledge in order
to make a model more robust. We detect a memorized PII and then mitigate the
memorization of PII by editing a model knowledge of its training data. We
verify that our procedure does not affect the underlying language model while
making it more robust against privacy Training Data Extraction attacks. We
demonstrate that PME can effectively reduce the number of leaked PII in a
number of configurations, in some cases even reducing the accuracy of the
privacy attacks to zero.

</details>


### [4] [Mind the Gap: Revealing Security Barriers through Situational Awareness of Small and Medium Business Key Decision-Makers](https://arxiv.org/abs/2506.10025)
*Yuanhaur Chang,Oren Heller,Yaniv Shlomo,Iddo Bar-Noy,Ella Bokobza,Michal Grinstein-Weiss,Ning Zhang*

Main category: cs.CR

TL;DR: 研究探讨了中小型企业（SMB）决策者在网络安全决策中的认知与行为，通过混合方法揭示了其感知风险、防御选择及安全认知的影响因素，并提出干预措施。


<details>
  <summary>Details</summary>
Motivation: 中小型企业决策者缺乏网络安全意识和知识，导致实施措施效果不佳，研究旨在深入了解其决策过程。

Method: 采用混合方法，包括半结构化访谈（n=21）和在线调查（n=322），结合主题分析和情境意识模型。

Result: 揭示了决策者对数字资产的感知风险、防御选择原因及安全认知影响因素，并构建了提升意识的结构方程模型。

Conclusion: 研究提出了干预措施，帮助中小型企业克服网络安全挑战，提升决策者的情境意识。

Abstract: Key decision-makers in small and medium businesses (SMBs) often lack the
awareness and knowledge to implement cybersecurity measures effectively. To
gain a deeper understanding of how SMB executives navigate cybersecurity
decision-making, we deployed a mixed-method approach, conducting
semi-structured interviews (n=21) and online surveys (n=322) with SMB key
decision-makers. Using thematic analysis, we revealed SMB decision-makers'
perceived risks in terms of the digital assets they valued, and found reasons
for their choice of defense measures and factors impacting security perception.
We employed the situational awareness model to characterize decision-makers
based on cybersecurity awareness, identifying those who have comparatively low
awareness in the fight against adversaries. We further explored the
relationship between awareness and business attributes, and constructed a
holistic structural equation model to understand how awareness can be improved.
Finally, we proposed interventions to help SMBs overcome potential challenges.

</details>


### [5] [Secure Data Access in Cloud Environments Using Quantum Cryptography](https://arxiv.org/abs/2506.10028)
*S. Vasavi Venkata Lakshmi,Ziaul Haque Choudhury*

Main category: cs.CR

TL;DR: 该论文提出使用量子密码学（如QKD、BB84协议和QOTP）保护云环境中的数据安全，以应对未来量子计算机的威胁。


<details>
  <summary>Details</summary>
Motivation: 传统数据安全方法在未来量子计算机面前可能失效，因此需要更强大的保护手段。

Method: 采用量子密钥分发（QKD）和BB84协议生成安全密钥，并使用量子一次性密码本（QOTP）进行数据加密。

Result: 研究表明，这些量子方法能有效防御黑客攻击，即使对方拥有量子计算机。

Conclusion: 量子密码学为当前和未来的云数据安全提供了可靠解决方案。

Abstract: Cloud computing has made storing and accessing data easier but keeping it
secure is a big challenge nowadays. Traditional methods of ensuring data may
not be strong enough in the future when powerful quantum computers become
available. To solve this problem, this study uses quantum cryptography to
protect data in the cloud environment. Quantum Key Distribution (QKD) creates
secure keys by sending information using quantum particles like photons.
Specifically, we use the BB84 protocol, a simple and reliable way to make
secure keys that cannot be stolen without detection. To protect the data, we
use the Quantum One Time pad (QOTP) for encryption and decryption, ensuring the
data stays completely private. This study shows how these Quantum methods can
be applied in cloud systems to provide a strong defense against hackers, even
if they have access to quantum computers. The combination of QKD, BB84, and
QOTP creates a safe and reliable way to keep data secure when it is stored or
shared in the cloud. Using quantum cryptography, this paper provides a way to
ensure data security now and in the future, making cloud computing safer for
everyone to store their data securely and safely.

</details>


### [6] [Evaluation empirique de la sécurisation et de l'alignement de ChatGPT et Gemini: analyse comparative des vulnérabilités par expérimentations de jailbreaks](https://arxiv.org/abs/2506.10029)
*Rafaël Nouailles*

Main category: cs.CR

TL;DR: 本文比较了ChatGPT和Gemini的安全性和对齐水平，并分析了越狱技术的分类与实验。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在文本生成、图像创作等领域带来变革，但也引发网络安全挑战，如提示注入攻击、越狱等。

Method: 通过比较分析ChatGPT和Gemini的安全性和对齐水平，并实验分类越狱技术。

Result: 揭示了ChatGPT和Gemini在安全性和对齐方面的差异，以及越狱技术的多样性。

Conclusion: LLMs的安全挑战需进一步研究，以应对提示注入、越狱等风险。

Abstract: Large Language models (LLMs) are transforming digital usage, particularly in
text generation, image creation, information retrieval and code development.
ChatGPT, launched by OpenAI in November 2022, quickly became a reference,
prompting the emergence of competitors such as Google's Gemini. However, these
technological advances raise new cybersecurity challenges, including prompt
injection attacks, the circumvention of regulatory measures (jailbreaking), the
spread of misinformation (hallucinations) and risks associated with deep fakes.
This paper presents a comparative analysis of the security and alignment levels
of ChatGPT and Gemini, as well as a taxonomy of jailbreak techniques associated
with experiments.

</details>


### [7] [Safeguarding Multimodal Knowledge Copyright in the RAG-as-a-Service Environment](https://arxiv.org/abs/2506.10030)
*Tianyu Chen,Jian Lou,Wenjie Wang*

Main category: cs.CR

TL;DR: AQUA是首个针对多模态RAG系统中图像知识保护的框架，通过嵌入语义信号实现高效、隐蔽的版权追踪。


<details>
  <summary>Details</summary>
Motivation: 随着RAG发展为共享知识库的服务平台，保护贡献数据的版权变得至关重要，但现有方法仅关注文本知识，图像知识缺乏保护。

Method: AQUA采用两种互补方法（基于缩写的触发器和空间关系线索）将语义信号嵌入合成图像。

Result: 实验表明，AQUA能实现鲁棒、隐蔽且可靠的版权追踪。

Conclusion: AQUA填补了多模态RAG保护的关键空白。

Abstract: As Retrieval-Augmented Generation (RAG) evolves into service-oriented
platforms (Rag-as-a-Service) with shared knowledge bases, protecting the
copyright of contributed data becomes essential. Existing watermarking methods
in RAG focus solely on textual knowledge, leaving image knowledge unprotected.
In this work, we propose AQUA, the first watermark framework for image
knowledge protection in Multimodal RAG systems. AQUA embeds semantic signals
into synthetic images using two complementary methods: acronym-based triggers
and spatial relationship cues. These techniques ensure watermark signals
survive indirect watermark propagation from image retriever to textual
generator, being efficient, effective and imperceptible. Experiments across
diverse models and datasets show that AQUA enables robust, stealthy, and
reliable copyright tracing, filling a key gap in multimodal RAG protection.

</details>


### [8] [Symbolic Generation and Modular Embedding of High-Quality abc-Triples](https://arxiv.org/abs/2506.10039)
*Michael A. Idowu*

Main category: cs.CR

TL;DR: 本文提出了一种基于abc猜想结构特征的符号恒等式，用于生成满足a + b = c的整数三元组(a, b, c)。通过结合2和3的幂以及模逆运算，构造出具有低radical值的abc三元组。


<details>
  <summary>Details</summary>
Motivation: 受abc猜想启发，探索如何生成具有低radical值的整数三元组，以优化log c / log rad(abc)的比值。

Method: 使用2和3的幂结合模逆运算，构造参数化恒等式，并通过仿射变换扩展三元组的生成空间。

Result: 计算结果显示，该方法能生成结构化的低radical值三元组，包括已知和新发现的三元组。

Conclusion: 该方法为可控三元组生成提供了符号和代数框架，并对密码学预处理中的符号熵过滤提出了探索性意义。

Abstract: We present a symbolic identity for generating integer triples $(a, b, c)$
satisfying $a + b = c$, inspired by structural features of the \emph{abc
conjecture}. The construction uses powers of $2$ and $3$ in combination with
modular inversion in $\mathbb{Z}/3^p\mathbb{Z}$, leading to a parametric
identity with residue constraints that yield abc-triples exhibiting low radical
values. Through affine transformations, these symbolic triples are embedded
into a broader space of high-quality examples, optimised for the ratio $\log c
/ \log \operatorname{rad}(abc)$. Computational results demonstrate the
emergence of structured, radical-minimising candidates, including both known
and novel triples. These methods provide a symbolic and algebraic framework for
controlled triple generation, and suggest exploratory implications for symbolic
entropy filtering in cryptographic pre-processing.

</details>


### [9] [Multiverse Privacy Theory for Contextual Risks in Complex User-AI Interactions](https://arxiv.org/abs/2506.10042)
*Ece Gumusel*

Main category: cs.CR

TL;DR: 论文提出了一种名为“多元宇宙隐私理论”的新框架，通过模拟用户隐私决策产生的平行宇宙来理解隐私问题。


<details>
  <summary>Details</summary>
Motivation: 在人工智能交互日益频繁的背景下，用户面临复杂且不确定的隐私决策，需要新的理论框架来指导。

Method: 提出多元宇宙隐私理论，通过模拟不同隐私决策产生的平行宇宙，结合情境完整性、动态偏好和概率决策进行分析。

Result: 该理论为理解隐私问题提供了新的视角，未来将通过实际场景调查数据验证其应用。

Conclusion: 多元宇宙隐私理论为隐私决策研究提供了创新框架，未来研究将进一步验证其实际效果。

Abstract: In an era of increasing interaction with artificial intelligence (AI), users
face evolving privacy decisions shaped by complex, uncertain factors. This
paper introduces Multiverse Privacy Theory, a novel framework in which each
privacy decision spawns a parallel universe, representing a distinct potential
outcome based on user choices over time. By simulating these universes, this
theory provides a foundation for understanding privacy through the lens of
contextual integrity, evolving preferences, and probabilistic decision-making.
Future work will explore its application using real-world, scenario-based
survey data.

</details>


### [10] [GenBreak: Red Teaming Text-to-Image Generators Using Large Language Models](https://arxiv.org/abs/2506.10047)
*Zilong Wang,Xiang Zheng,Xiaosen Wang,Bo Wang,Xingjun Ma,Yu-Gang Jiang*

Main category: cs.CR

TL;DR: GenBreak框架通过微调大型语言模型（LLM）来系统性探索文本到图像（T2I）生成器的潜在漏洞，结合监督微调和强化学习，生成既能绕过安全机制又能产生有害内容的对抗性提示。


<details>
  <summary>Details</summary>
Motivation: 现有的T2I模型可能被滥用生成有害内容，但现有研究在评估其安全性时存在局限性，缺乏可靠工具。

Method: GenBreak框架结合监督微调和强化学习，通过多奖励信号引导LLM生成具有高毒性和规避能力的对抗性提示。

Result: 生成的对抗性提示在商业T2I生成器的黑盒攻击中表现出色，揭示了实际的安全隐患。

Conclusion: GenBreak为评估T2I模型的安全性提供了有效工具，揭示了其潜在风险。

Abstract: Text-to-image (T2I) models such as Stable Diffusion have advanced rapidly and
are now widely used in content creation. However, these models can be misused
to generate harmful content, including nudity or violence, posing significant
safety risks. While most platforms employ content moderation systems,
underlying vulnerabilities can still be exploited by determined adversaries.
Recent research on red-teaming and adversarial attacks against T2I models has
notable limitations: some studies successfully generate highly toxic images but
use adversarial prompts that are easily detected and blocked by safety filters,
while others focus on bypassing safety mechanisms but fail to produce genuinely
harmful outputs, neglecting the discovery of truly high-risk prompts.
Consequently, there remains a lack of reliable tools for evaluating the safety
of defended T2I models. To address this gap, we propose GenBreak, a framework
that fine-tunes a red-team large language model (LLM) to systematically explore
underlying vulnerabilities in T2I generators. Our approach combines supervised
fine-tuning on curated datasets with reinforcement learning via interaction
with a surrogate T2I model. By integrating multiple reward signals, we guide
the LLM to craft adversarial prompts that enhance both evasion capability and
image toxicity, while maintaining semantic coherence and diversity. These
prompts demonstrate strong effectiveness in black-box attacks against
commercial T2I generators, revealing practical and concerning safety
weaknesses.

</details>


### [11] [Expert-in-the-Loop Systems with Cross-Domain and In-Domain Few-Shot Learning for Software Vulnerability Detection](https://arxiv.org/abs/2506.10104)
*David Farr,Kevin Talty,Alexandra Farr,John Stockdale,Iain Cruickshank,Jevin West*

Main category: cs.CR

TL;DR: 研究探讨了使用大型语言模型（LLMs）检测Python代码漏洞，比较了零样本、少样本跨域和少样本同域提示策略，发现少样本提示显著提升性能，结合置信度路由策略可优化自动化与专家监督的平衡。


<details>
  <summary>Details</summary>
Motivation: 随着网络威胁日益复杂，快速准确的漏洞检测对系统安全至关重要。

Method: 通过模拟识别带有已知CWE的Python代码，比较不同提示策略（零样本、少样本跨域、少样本同域），并引入置信度路由策略。

Result: 少样本提示显著提升分类性能，结合置信度路由策略可优化效率；LLMs能通过少量示例泛化漏洞类别。

Conclusion: LLMs在模拟环境中具有潜力，但模型可靠性、可解释性和对抗鲁棒性仍需研究；结合专家决策可提升网络安全效率。

Abstract: As cyber threats become more sophisticated, rapid and accurate vulnerability
detection is essential for maintaining secure systems. This study explores the
use of Large Language Models (LLMs) in software vulnerability assessment by
simulating the identification of Python code with known Common Weakness
Enumerations (CWEs), comparing zero-shot, few-shot cross-domain, and few-shot
in-domain prompting strategies. Our results indicate that while zero-shot
prompting performs poorly, few-shot prompting significantly enhances
classification performance, particularly when integrated with confidence-based
routing strategies that improve efficiency by directing human experts to cases
where model uncertainty is high, optimizing the balance between automation and
expert oversight. We find that LLMs can effectively generalize across
vulnerability categories with minimal examples, suggesting their potential as
scalable, adaptable cybersecurity tools in simulated environments. However,
challenges such as model reliability, interpretability, and adversarial
robustness remain critical areas for future research. By integrating AI-driven
approaches with expert-in-the-loop (EITL) decision-making, this work highlights
a pathway toward more efficient and responsive cybersecurity workflows. Our
findings provide a foundation for deploying AI-assisted vulnerability detection
systems in both real and simulated environments that enhance operational
resilience while reducing the burden on human analysts.

</details>


### [12] [D-LiFT: Improving LLM-based Decompiler Backend via Code Quality-driven Fine-tuning](https://arxiv.org/abs/2506.10125)
*Muqi Zou,Hongyu Cai,Hongwei Wu,Zion Leonahenahe Basque,Arslan Khan,Berkay Celik,Dave,Tian,Antonio Bianchi,Ruoyu,Wang,Dongyan Xu*

Main category: cs.CR

TL;DR: D-LiFT是一种利用强化学习改进反编译器输出的自动化后端工具，通过D-SCORE评估系统确保准确性和可读性。


<details>
  <summary>Details</summary>
Motivation: 现有反编译器输出存在语法和语义错误，且可读性差，而现有基于LLM的方法存在引入新错误和验证不可靠的问题。

Method: D-LiFT结合强化学习训练LLM，通过D-SCORE系统从多个维度评估反编译代码，仅在准确性验证通过后优化可读性。

Result: 实验表明，D-LiFT在coreutils和util-linux项目中显著提升了反编译代码质量，比基线LLM多改进55.3%的函数。

Conclusion: D-LiFT通过D-SCORE系统实现了在保持准确性的同时提升可读性，为反编译工具提供了新的优化方向。

Abstract: Decompilers, which reconstruct human-readable source code from binary
executables, are vital to many security tasks. Yet, despite recent advances,
their output often suffers from syntactic and semantic errors and remains
difficult to read. Recently, with the advent of large language models (LLMs),
researchers began to explore the potential of LLMs to refine decompiler output.
Nevertheless, our study of these approaches reveals significant limitations,
such as introducing new errors and relying on unreliable accuracy validation.
In this paper, we present D-LiFT, an automated decompiler backend that
harnesses and further trains LLMs to improve the quality of decompiled code via
reinforcement learning (RL). Unlike prior work that overlooks preserving
accuracy, D-LiFT adheres to a key principle for enhancing the quality of
decompiled code: \textit{preserving accuracy while improving readability}.
Central to D-LiFT, we propose D-SCORE, an integrated quality assessment system
to score the decompiled code from multiple aspects. In line with our principle,
D-SCORE assigns low scores to any inaccurate output and only awards higher
scores for readability to code that passes the accuracy check. Specifically,
D-SCORE first verifies the syntactic and semantic correctness via the compiler
and symbolic execution; only if a candidate is deemed accurate, it then
evaluates readability using established metrics to compare the LLM output with
the original decompiled code. The score will then be fed back to the LLM for
fine-tuning. Our implementation, based on Ghidra and a range of LLMs,
demonstrates significant improvements for the accurate decompiled code from the
coreutils and util-linux projects. Compared to baseline LLMs without
D-SCORE-driven fine-tuning, D-LiFT produces 55.3% more improved decompiled
functions, as measured by D-SCORE.

</details>


### [13] [Unconditionally Secure Wireless-Wired Ground-Satellite-Ground Communication Networks Utilizing Classical and Quantum Noise](https://arxiv.org/abs/2506.10147)
*Lucas Truax,Sandip Roy,Laszlo B. Kish*

Main category: cs.CR

TL;DR: KLJN是一种基于经典物理学的卫星通信安全方案，相比量子密钥分发（QKD），它更简单、成本更低且更稳健。


<details>
  <summary>Details</summary>
Motivation: 探索一种比QKD更简单、经济且可靠的卫星通信安全方案，以应对量子计算等高级威胁。

Method: 利用标准电子元件和电线实现KLJN方案，基于经典物理学原理提供无条件安全性。

Result: KLJN在成本、复杂性和环境适应性上优于QKD，适合卫星通信。

Conclusion: KLJN是卫星通信安全的一种革命性替代方案，具有显著优势。

Abstract: In this paper, we introduce the Kirchhoff-Law-Johnson-Noise (KLJN) as an
approach to securing satellite communications. KLJN has the potential to
revolutionize satellite communication security through its combination of
simplicity, cost-effectiveness, and resilience with unconditional security.
Unlike quantum key distribution (QKD), which requires complex, fragile, and
expensive infrastructure like photon detectors and dedicated optical links,
KLJN operates using standard electronic components and wires, significantly
reducing implementation costs and logistical hurdles. KLJN's security, grounded
in the fundamental laws of classical physics, is impervious to environmental
and radiation-induced noise, making it highly reliable in the harsh conditions
of satellite communications. This robustness, coupled with its ability to
integrate seamlessly with existing infrastructure, positions KLJN as a
revolutionary alternative to quantum solutions for ensuring secure, resilient
satellite communications. The authors explore the value of achieving
unconditionally secure communications in strategic ground-to-satellite networks
which address vulnerabilities posed by advanced computational threats,
including quantum computing. Our team has examined two leading approaches to
unconditional security - the KLJN scheme and QKD - and analyzed the potential
use of each for space systems. While QKD leverages quantum mechanics for
security, it faces challenges related to cost, complexity, and environmental
sensitivity. In contrast, the KLJN scheme utilizes classical physics principles
to provide a simpler, more cost-effective, and resilient alternative,
particularly for ground-based systems. The study concludes that KLJN offers
significant advantages in simplicity, cost-efficiency, and robustness, making
it a practical choice for many secure communication applications.

</details>


### [14] [Disclosure Audits for LLM Agents](https://arxiv.org/abs/2506.10171)
*Saswat Das,Jameson Sandler,Ferdinando Fioretto*

Main category: cs.CR

TL;DR: 论文提出了一种用于对话隐私审计的框架CMPL，通过多轮交互测试揭示隐私风险，并提供了量化指标和开放基准。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型代理在应用中需要持续访问敏感数据，增加了隐私泄露风险，现有单轮防御措施不足以应对。

Method: 提出CMPL框架，采用迭代探测策略模拟多轮交互，系统性地发现潜在隐私漏洞。

Result: 评估表明CMPL能揭示现有单轮防御无法阻止的隐私风险，并提供了量化风险指标和开放基准。

Conclusion: CMPL是一种有效的诊断工具，为对话隐私审计提供了量化方法和评估标准。

Abstract: Large Language Model agents have begun to appear as personal assistants,
customer service bots, and clinical aides. While these applications deliver
substantial operational benefits, they also require continuous access to
sensitive data, which increases the likelihood of unauthorized disclosures.
This study proposes an auditing framework for conversational privacy that
quantifies and audits these risks. The proposed Conversational Manipulation for
Privacy Leakage (CMPL) framework, is an iterative probing strategy designed to
stress-test agents that enforce strict privacy directives. Rather than focusing
solely on a single disclosure event, CMPL simulates realistic multi-turn
interactions to systematically uncover latent vulnerabilities. Our evaluation
on diverse domains, data modalities, and safety configurations demonstrate the
auditing framework's ability to reveal privacy risks that are not deterred by
existing single-turn defenses. In addition to introducing CMPL as a diagnostic
tool, the paper delivers (1) an auditing procedure grounded in quantifiable
risk metrics and (2) an open benchmark for evaluation of conversational privacy
across agent implementations.

</details>


### [15] [AURA: A Multi-Agent Intelligence Framework for Knowledge-Enhanced Cyber Threat Attribution](https://arxiv.org/abs/2506.10175)
*Nanda Rani,Sandeep Kumar Shukla*

Main category: cs.CR

TL;DR: AURA是一个多智能体、知识增强的框架，用于自动化和可解释的APT归因，通过结合检索增强生成和大型语言模型，实现威胁行为与已知APT组的关联。


<details>
  <summary>Details</summary>
Motivation: APT归因需要关联复杂多样的威胁情报，传统方法难以满足需求，因此开发了AURA框架以提高归因的一致性和可解释性。

Method: AURA通过多智能体网络处理多样化的威胁数据（如TTPs、IoCs等），结合检索增强生成和大型语言模型，实现智能查询重写、上下文增强检索和自然语言解释。

Result: 实验表明，AURA在最近的APT活动中表现出高归因一致性、专家对齐的解释能力和可扩展性。

Conclusion: AURA为透明、数据驱动和可扩展的威胁归因提供了有前景的方向。

Abstract: Effective attribution of Advanced Persistent Threats (APTs) increasingly
hinges on the ability to correlate behavioral patterns and reason over complex,
varied threat intelligence artifacts. We present AURA (Attribution Using
Retrieval-Augmented Agents), a multi-agent, knowledge-enhanced framework for
automated and interpretable APT attribution. AURA ingests diverse threat data
including Tactics, Techniques, and Procedures (TTPs), Indicators of Compromise
(IoCs), malware details, adversarial tools, and temporal information, which are
processed through a network of collaborative agents. These agents are designed
for intelligent query rewriting, context-enriched retrieval from structured
threat knowledge bases, and natural language justification of attribution
decisions. By combining Retrieval-Augmented Generation (RAG) with Large
Language Models (LLMs), AURA enables contextual linking of threat behaviors to
known APT groups and supports traceable reasoning across multiple attack
phases. Experiments on recent APT campaigns demonstrate AURA's high attribution
consistency, expert-aligned justifications, and scalability. This work
establishes AURA as a promising direction for advancing transparent,
data-driven, and scalable threat attribution using multi-agent intelligence.

</details>


### [16] [Guardians of the Regime: When and Why Autocrats Create Secret Police](https://arxiv.org/abs/2506.10194)
*Marius Mehrl,Mila Pfander,Theresa Winner,Cornelius Fritz*

Main category: cs.CR

TL;DR: 研究探讨了独裁政权下秘密警察出现的条件，发现其与特定威胁（如抗议）和资源可用性相关。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明秘密警察在独裁政权中并不普遍，研究旨在揭示其出现的具体条件。

Method: 采用统计变量选择技术，分析文献中的候选变量。

Result: 秘密警察更可能在面临可预防威胁（如抗议）且资源充足时出现。

Conclusion: 研究增进了对独裁者制度选择及威权政治的理解。

Abstract: Autocrats use secret police to stay in power, as these organizations deter
and suppress opposition to their rule. Existing research shows that secret
police are very good at this but, surprisingly, also that they are not as
ubiquitous in autocracies as one may assume, existing in less than 50% of
autocratic country-years. We thus explore under which conditions secret police
emerge in dictatorships. For this purpose, we apply statistical variable
selection techniques to identify which of several candidate variables extracted
from the literature on state security forces and authoritarian survival hold
explanatory power. Our results highlight that secret police are more likely to
emerge when rulers face specific, preempt-able threats, such as protests and
anti-system mobilisation, but also when they have the material resources to
establish these organisations. This research contributes to our understanding
of autocrats' institutional choices and authoritarian politics.

</details>


### [17] [Prompt Attacks Reveal Superficial Knowledge Removal in Unlearning Methods](https://arxiv.org/abs/2506.10236)
*Yeonwoo Jang,Shariqah Hossain,Ashwin Sreevatsa,Diogo Cruz*

Main category: cs.CR

TL;DR: 研究发现某些机器学习遗忘方法在简单提示攻击下可能失效，部分方法表现脆弱，需改进评估框架。


<details>
  <summary>Details</summary>
Motivation: 验证机器学习遗忘方法的有效性，揭示其在提示攻击下的脆弱性。

Method: 系统评估八种遗忘技术，采用输出、logit和探针分析。

Result: 部分方法（如RMU、TAR）表现稳健，但ELM在特定提示攻击下脆弱（如恢复57.3%准确率）。

Conclusion: 现有遗忘方法效果存疑，需更可靠的评估框架区分真实知识移除与表面输出抑制。

Abstract: In this work, we show that some machine unlearning methods may fail when
subjected to straightforward prompt attacks. We systematically evaluate eight
unlearning techniques across three model families, and employ output-based,
logit-based, and probe analysis to determine to what extent supposedly
unlearned knowledge can be retrieved. While methods like RMU and TAR
demonstrate robust unlearning, ELM remains vulnerable to specific prompt
attacks (e.g., Hindi filler text in original prompt recovering 57.3% accuracy).
Our logit analysis also confirms that unlearned models are generally not hiding
knowledge by modifying the way the answer is formatted, as the correlation
between output and logit accuracy is strong. These results challenge prevailing
assumptions about unlearning effectiveness and highlight the need for
evaluation frameworks that can reliably distinguish between true knowledge
removal and superficial output suppression. We also publicly make available our
evaluation framework to easily evaluate prompting techniques to retrieve
unlearning knowledge.

</details>


### [18] [ELFuzz: Efficient Input Generation via LLM-driven Synthesis Over Fuzzer Space](https://arxiv.org/abs/2506.10323)
*Chuyang Chen,Brendan Dolan-Gavitt,Zhiqiang Lin*

Main category: cs.CR

TL;DR: ELFuzz利用LLM自动合成针对被测系统的生成式模糊测试工具，显著提升覆盖率和错误检测能力。


<details>
  <summary>Details</summary>
Motivation: 传统生成式模糊测试需要大量手动构建输入语法和语义约束，耗时且低效。

Method: ELFuzz通过LLM驱动的合成与进化，从最小种子模糊测试工具出发，自动化生成高效模糊测试工具。

Result: ELFuzz在真实系统测试中覆盖率和错误检测能力显著优于人工编写和现有方法，并发现多个0-day漏洞。

Conclusion: ELFuzz展示了自动化、高效且可扩展的模糊测试输入生成潜力。

Abstract: Generation-based fuzzing produces appropriate testing cases according to
specifications of input grammars and semantic constraints to test systems and
software. However, these specifications require significant manual efforts to
construct. This paper proposes a new approach, ELFuzz (Evolution Through Large
Language Models for Fuzzing), that automatically synthesizes generation-based
fuzzers tailored to a system under test (SUT) via LLM-driven synthesis over
fuzzer space. At a high level, it starts with minimal seed fuzzers and propels
the synthesis by fully automated LLM-driven evolution with coverage guidance.
Compared to previous approaches, ELFuzz can 1) seamlessly scale to SUTs of
real-world sizes -- up to 1,791,104 lines of code in our evaluation -- and 2)
synthesize efficient fuzzers that catch interesting grammatical structures and
semantic constraints in a human-understandable way. Our evaluation compared
ELFuzz with specifications manually written by domain experts and synthesized
by state-of-the-art approaches. It shows that ELFuzz achieves up to 434.8% more
coverage and triggers up to 174.0% more artificially injected bugs. We also
used ELFuzz to conduct a real-world fuzzing campaign on the newest version of
cvc5 for 14 days, and encouragingly, it found five 0-day bugs (three are
exploitable). Moreover, we conducted an ablation study, which shows that the
fuzzer space model, the key component of ELFuzz, contributes the most (up to
62.5%) to the effectiveness of ELFuzz. Further analysis of the fuzzers
synthesized by ELFuzz confirms that they catch interesting grammatical
structures and semantic constraints in a human-understandable way. The results
present the promising potential of ELFuzz for more automated, efficient, and
extensible input generation for fuzzing.

</details>


### [19] [A Comprehensive Survey of Unmanned Aerial Systems' Risks and Mitigation Strategies](https://arxiv.org/abs/2506.10327)
*Sharad Shrestha,Mohammed Ababneh,Satyajayant Misra,Henry M. Cathey, Jr.,Roopa Vishwanathan,Matt Jansen,Jinhong Choi,Rakesh Bobba,Yeongjin Jang*

Main category: cs.CR

TL;DR: 该论文总结了无人机系统（UAS）和无人机（UAV）在部署各阶段的网络安全漏洞、攻击可能性、影响及缓解策略，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着UAS和UAV在通信、国防和交通领域的快速增长，研究其安全漏洞以增强系统安全性变得至关重要。

Method: 论文采用综合方法，分析了UAS特定和非特定的缓解策略，并结合相关网络安全标准进行总结。

Result: 研究发现尽管已有大量文献和现有安全方法，但仍存在多个关键研究空白。

Conclusion: 论文提出了未来研究社区需要进一步探索的建议和方向。

Abstract: In the last decade, the rapid growth of Unmanned Aircraft Systems (UAS) and
Unmanned Aircraft Vehicles (UAV) in communication, defense, and transportation
has increased. The application of UAS will continue to increase rapidly. This
has led researchers to examine security vulnerabilities in various facets of
UAS infrastructure and UAVs, which form a part of the UAS system to reinforce
these critical systems. This survey summarizes the cybersecurity
vulnerabilities in several phases of UAV deployment, the likelihood of each
vulnerability's occurrence, the impact of attacks, and mitigation strategies
that could be applied. We go beyond the state-of-the-art by taking a
comprehensive approach to enhancing UAS security by performing an analysis of
both UAS-specific and non-UAS-specific mitigation strategies that are
applicable within the UAS domain to define the lessons learned. We also present
relevant cybersecurity standards and their recommendations in the UAS context.
Despite the significant literature in UAS security and the relevance of
cyberphysical and networked systems security approaches from the past, which we
identify in the survey, we find several critical research gaps that require
further investigation. These form part of our discussions and recommendations
for the future exploration by our research community.

</details>


### [20] [Adaptive Chosen-Ciphertext Security of Distributed Broadcast Encryption](https://arxiv.org/abs/2506.10338)
*Kwangsu Lee*

Main category: cs.CR

TL;DR: 本文提出了一种高效的分布式广播加密（DBE）方案，首次实现了自适应选择密文攻击（CCA）安全性，并通过改进现有方法提升了效率。


<details>
  <summary>Details</summary>
Motivation: 现有DBE方案在自适应选择明文攻击（CPA）模型下安全，但存在公钥验证效率低的问题，需要线性配对操作。本文旨在解决这一问题并提升安全性至CCA模型。

Method: 首先提出半静态CCA安全的DBE方案，基于$q$-Type假设证明安全性；随后改进Gentry-Waters的通用转换方法，将其应用于CCA安全DBE方案，实现自适应CCA安全。

Result: 提出的DBE方案具有高效性：密文和私钥大小为常数，公钥大小为线性，公钥验证仅需常数次配对操作和高效的群成员检查。

Conclusion: 本文首次实现了自适应CCA安全的DBE方案，显著提升了效率和安全性，为实际应用提供了更优选择。

Abstract: Distributed broadcast encryption (DBE) is a specific kind of broadcast
encryption (BE) where users independently generate their own public and private
keys, and a sender can efficiently create a ciphertext for a subset of users by
using the public keys of the subset users. Previously proposed DBE schemes have
been proven in the adaptive chosen-plaintext attack (CPA) security model and
have the disadvantage of requiring linear number of pairing operations when
verifying the public key of a user. In this paper, we propose an efficient DBE
scheme in bilinear groups and prove adaptive chosen-ciphertext attack (CCA)
security for the first time. To do this, we first propose a semi-static CCA
secure DBE scheme and prove the security under the $q$-Type assumption. Then,
by modifying the generic transformation of Gentry and Waters that converts a
semi-static CPA secure DBE scheme into an adaptive CPA secure DBE scheme to be
applied to CCA secure DBE schemes, we propose an adaptive CCA secure DBE scheme
and prove its adaptive CCA security. Our proposed DBE scheme is efficient
because it requires constant size ciphertexts, constant size private keys, and
linear size public keys, and the public key verification requires only a
constant number of pairing operations and efficient group membership checks.

</details>


### [21] [FicGCN: Unveiling the Homomorphic Encryption Efficiency from Irregular Graph Convolutional Networks](https://arxiv.org/abs/2506.10399)
*Zhaoxuan Kan,Husheng Han,Shangyi Shi,Tenghui Hua,Hang Lu,Xiaowei Li,Jianan Mu,Xing Hu*

Main category: cs.CR

TL;DR: FicGCN是一个基于同态加密（HE）的框架，旨在利用图卷积神经网络（GCN）的稀疏特性，优化聚合与组合操作，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 随着云基GCN服务的需求增长，敏感图数据的隐私问题日益突出，而HE虽然支持隐私保护机器学习，但计算开销大，尤其是GCN操作中的矩阵乘法和旋转。

Method: FicGCN采用延迟感知的打包方案、稀疏内密文聚合（SpIntra-CA）方法以减少旋转开销，并基于局部邻接结构进行区域数据重排序。

Result: 在多个数据集上测试，FicGCN性能最佳，比最新设计提升高达4.10倍。

Conclusion: FicGCN通过优化稀疏性和操作平衡，显著提升了HE在GCN中的实用性。

Abstract: Graph Convolutional Neural Networks (GCNs) have gained widespread popularity
in various fields like personal healthcare and financial systems, due to their
remarkable performance. Despite the growing demand for cloud-based GCN
services, privacy concerns over sensitive graph data remain significant.
Homomorphic Encryption (HE) facilitates Privacy-Preserving Machine Learning
(PPML) by allowing computations to be performed on encrypted data. However, HE
introduces substantial computational overhead, particularly for GCN operations
that require rotations and multiplications in matrix products. The sparsity of
GCNs offers significant performance potential, but their irregularity
introduces additional operations that reduce practical gains. In this paper, we
propose FicGCN, a HE-based framework specifically designed to harness the
sparse characteristics of GCNs and strike a globally optimal balance between
aggregation and combination operations. FicGCN employs a latency-aware packing
scheme, a Sparse Intra-Ciphertext Aggregation (SpIntra-CA) method to minimize
rotation overhead, and a region-based data reordering driven by local adjacency
structure. We evaluated FicGCN on several popular datasets, and the results
show that FicGCN achieved the best performance across all tested datasets, with
up to a 4.10x improvement over the latest design.

</details>


### [22] [SOFT: Selective Data Obfuscation for Protecting LLM Fine-tuning against Membership Inference Attacks](https://arxiv.org/abs/2506.10424)
*Kaiyuan Zhang,Siyuan Cheng,Hanxi Guo,Yuetian Chen,Zian Su,Shengwei An,Yuntao Du,Charles Fleming,Ashish Kundu,Xiangyu Zhang,Ninghui Li*

Main category: cs.CR

TL;DR: 该论文研究了微调大型语言模型（LLMs）时面临的隐私风险，首次全面评估了成员推理攻击（MIAs）的威胁，并提出了一种名为SOFT的新型防御技术，通过选择性数据混淆来平衡隐私保护和模型性能。


<details>
  <summary>Details</summary>
Motivation: 微调LLMs常涉及敏感信息，引发隐私担忧。MIAs利用微调过程中的损失减少揭示成员信息，亟需有效防御方法。

Method: 提出SOFT技术，结合选择性数据混淆和可调参数，以平衡隐私保护与模型性能。

Result: 在六个领域和多种LLM架构上的实验表明，SOFT显著降低隐私风险，同时保持模型性能。

Conclusion: SOFT为微调LLMs中的隐私保护提供了实用且可扩展的解决方案。

Abstract: Large language models (LLMs) have achieved remarkable success and are widely
adopted for diverse applications. However, fine-tuning these models often
involves private or sensitive information, raising critical privacy concerns.
In this work, we conduct the first comprehensive study evaluating the
vulnerability of fine-tuned LLMs to membership inference attacks (MIAs). Our
empirical analysis demonstrates that MIAs exploit the loss reduction during
fine-tuning, making them highly effective in revealing membership information.
These findings motivate the development of our defense. We propose SOFT
(\textbf{S}elective data \textbf{O}bfuscation in LLM
\textbf{F}ine-\textbf{T}uning), a novel defense technique that mitigates
privacy leakage by leveraging influential data selection with an adjustable
parameter to balance utility preservation and privacy protection. Our extensive
experiments span six diverse domains and multiple LLM architectures and scales.
Results show that SOFT effectively reduces privacy risks while maintaining
competitive model performance, offering a practical and scalable solution to
safeguard sensitive information in fine-tuned LLMs.

</details>


### [23] [Specification and Evaluation of Multi-Agent LLM Systems -- Prototype and Cybersecurity Applications](https://arxiv.org/abs/2506.10467)
*Felix Härer*

Main category: cs.CR

TL;DR: 论文探讨了多智能体LLM系统的潜力，提出了一个系统架构和原型，并通过网络安全任务验证了其可行性。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM、推理技术和应用已有单独评估，但它们的联合规范和组合应用尚未充分探索，需要定义多智能体LLM系统的规范以探索其潜力。

Method: 扩展了先前研究的系统架构和原型，引入了多智能体系统的规范，并通过网络安全任务进行测试。

Result: 测试结果表明，基于OpenAI和DeepSeek LLM的智能体能够正确完成问答、服务器安全和网络安全任务。

Conclusion: 研究验证了多智能体LLM系统的可行性，并为其在特定领域的应用提供了规范和评估方法。

Abstract: Recent advancements in LLMs indicate potential for novel applications, e.g.,
through reasoning capabilities in the latest OpenAI and DeepSeek models. For
applying these models in specific domains beyond text generation, LLM-based
multi-agent approaches can be utilized that solve complex tasks by combining
reasoning techniques, code generation, and software execution. Applications
might utilize these capabilities and the knowledge of specialized LLM agents.
However, while many evaluations are performed on LLMs, reasoning techniques,
and applications individually, their joint specification and combined
application is not explored well. Defined specifications for multi-agent LLM
systems are required to explore their potential and their suitability for
specific applications, allowing for systematic evaluations of LLMs, reasoning
techniques, and related aspects. This paper reports the results of exploratory
research to specify and evaluate these aspects through a multi-agent system.
The system architecture and prototype are extended from previous research and a
specification is introduced for multi-agent systems. Test cases involving
cybersecurity tasks indicate feasibility of the architecture and evaluation
approach. In particular, the results show the evaluation of question answering,
server security, and network security tasks that were completed correctly by
agents with LLMs from OpenAI and DeepSeek.

</details>


### [24] [A Crack in the Bark: Leveraging Public Knowledge to Remove Tree-Ring Watermarks](https://arxiv.org/abs/2506.10502)
*Junhua Lin,Marc Juarez*

Main category: cs.CR

TL;DR: 本文提出了一种针对Tree-Ring水印技术的新型攻击方法，仅需利用公开的变分自编码器即可显著降低检测性能。


<details>
  <summary>Details</summary>
Motivation: Tree-Ring水印技术因其高隐蔽性和抗攻击性而受到关注，但现有攻击方法假设条件过强。本文旨在探索一种更实际的攻击方式。

Method: 通过利用训练目标扩散模型时使用的公开变分自编码器，攻击者可以近似模型的潜在空间，从而实施更有效的替代攻击。

Result: 实验表明，该方法将Tree-Ring检测器的AUC从0.993降至0.153（ROC曲线）和0.994降至0.385（PR曲线），同时保持图像质量。

Conclusion: 研究揭示了复用公开自编码器训练扩散模型的风险，并指出Tree-Ring检测器的精度不足以满足实际部署需求。

Abstract: We present a novel attack specifically designed against Tree-Ring, a
watermarking technique for diffusion models known for its high imperceptibility
and robustness against removal attacks. Unlike previous removal attacks, which
rely on strong assumptions about attacker capabilities, our attack only
requires access to the variational autoencoder that was used to train the
target diffusion model, a component that is often publicly available. By
leveraging this variational autoencoder, the attacker can approximate the
model's intermediate latent space, enabling more effective surrogate-based
attacks. Our evaluation shows that this approach leads to a dramatic reduction
in the AUC of Tree-Ring detector's ROC and PR curves, decreasing from 0.993 to
0.153 and from 0.994 to 0.385, respectively, while maintaining high image
quality. Notably, our attacks outperform existing methods that assume full
access to the diffusion model. These findings highlight the risk of reusing
public autoencoders to train diffusion models -- a threat not considered by
current industry practices. Furthermore, the results suggest that the Tree-Ring
detector's precision, a metric that has been overlooked by previous
evaluations, falls short of the requirements for real-world deployment.

</details>


### [25] [SoK: Evaluating Jailbreak Guardrails for Large Language Models](https://arxiv.org/abs/2506.10597)
*Xunguang Wang,Zhenlan Ji,Wenxuan Wang,Zongjie Li,Daoyuan Wu,Shuai Wang*

Main category: cs.CR

TL;DR: 本文提出了首个针对大型语言模型（LLM）越狱防护栏的系统性分析，包括多维度分类法和安全-效率-实用性评估框架。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM取得了显著进展，但其部署暴露了易受越狱攻击的漏洞，而现有的防护栏缺乏统一分类和评估框架。

Method: 提出六维分类法，并引入安全-效率-实用性评估框架，通过实验分析现有防护栏的优劣势。

Result: 揭示了现有防护栏的局限性，探索了其通用性，并提供了优化防御组合的见解。

Conclusion: 为未来研究和开发提供了结构化基础，旨在推动稳健LLM防护栏的发展。

Abstract: Large Language Models (LLMs) have achieved remarkable progress, but their
deployment has exposed critical vulnerabilities, particularly to jailbreak
attacks that circumvent safety mechanisms. Guardrails--external defense
mechanisms that monitor and control LLM interaction--have emerged as a
promising solution. However, the current landscape of LLM guardrails is
fragmented, lacking a unified taxonomy and comprehensive evaluation framework.
In this Systematization of Knowledge (SoK) paper, we present the first holistic
analysis of jailbreak guardrails for LLMs. We propose a novel,
multi-dimensional taxonomy that categorizes guardrails along six key
dimensions, and introduce a Security-Efficiency-Utility evaluation framework to
assess their practical effectiveness. Through extensive analysis and
experiments, we identify the strengths and limitations of existing guardrail
approaches, explore their universality across attack types, and provide
insights into optimizing defense combinations. Our work offers a structured
foundation for future research and development, aiming to guide the principled
advancement and deployment of robust LLM guardrails. The code is available at
https://github.com/xunguangwang/SoK4JailbreakGuardrails.

</details>


### [26] [Assessing the Resilience of Automotive Intrusion Detection Systems to Adversarial Manipulation](https://arxiv.org/abs/2506.10620)
*Stefano Longari,Paolo Cerracchio,Michele Carminati,Stefano Zanero*

Main category: cs.CR

TL;DR: 本文研究了基于梯度的对抗攻击对汽车入侵检测系统（IDS）的可行性和影响，考虑了白盒、灰盒和黑盒三种攻击场景，并评估了攻击效果。


<details>
  <summary>Details</summary>
Motivation: 现代车辆的安全性日益重要，但CAN总线缺乏强大的安全措施，使其易受网络攻击。尽管已有IDS用于防御，但仍存在被对抗攻击绕过的风险。

Method: 研究了三种不同知识程度的攻击场景（白盒、灰盒、黑盒），并在两个公开数据集上评估攻击效果，同时研究了对抗扰动对攻击的影响和实时可行性。

Result: 攻击的有效性高度依赖于数据集质量、目标IDS和攻击者的知识程度，且在汽车领域约束下具有挑战性。

Conclusion: 研究表明，对抗攻击对汽车IDS的威胁不可忽视，未来需进一步优化防御措施。

Abstract: The security of modern vehicles has become increasingly important, with the
controller area network (CAN) bus serving as a critical communication backbone
for various Electronic Control Units (ECUs). The absence of robust security
measures in CAN, coupled with the increasing connectivity of vehicles, makes
them susceptible to cyberattacks. While intrusion detection systems (IDSs) have
been developed to counter such threats, they are not foolproof. Adversarial
attacks, particularly evasion attacks, can manipulate inputs to bypass
detection by IDSs. This paper extends our previous work by investigating the
feasibility and impact of gradient-based adversarial attacks performed with
different degrees of knowledge against automotive IDSs. We consider three
scenarios: white-box (attacker with full system knowledge), grey-box (partial
system knowledge), and the more realistic black-box (no knowledge of the IDS'
internal workings or data). We evaluate the effectiveness of the proposed
attacks against state-of-the-art IDSs on two publicly available datasets.
Additionally, we study effect of the adversarial perturbation on the attack
impact and evaluate real-time feasibility by precomputing evasive payloads for
timed injection based on bus traffic. Our results demonstrate that, besides
attacks being challenging due to the automotive domain constraints, their
effectiveness is strongly dependent on the dataset quality, the target IDS, and
the attacker's degree of knowledge.

</details>


### [27] [CyFence: Securing Cyber-Physical Controllers via Trusted Execution Environment](https://arxiv.org/abs/2506.10638)
*Stefano Longari,Alessandro Pozone,Jessica Leoni,Mario Polino,Michele Carminati,Mara Tanelli,Stefano Zanero*

Main category: cs.CR

TL;DR: 论文提出了一种名为CyFence的新架构，通过语义检查提升闭环控制系统的网络攻击韧性，并利用现代处理器的可信执行环境确保安全性。


<details>
  <summary>Details</summary>
Motivation: 随着CPS技术的快速发展和连接性增强，其面临网络攻击的风险也显著增加，尤其是在安全关键系统中。现有防御策略很少利用CPS的物理特性。

Method: 提出CyFence架构，通过语义检查验证系统行为是否符合预期，并利用可信执行环境保护检查代码的安全。

Result: 在真实世界的主动制动数字控制器应用中，CyFence能够以可忽略的计算开销缓解多种攻击。

Conclusion: CyFence通过结合语义检查和可信执行环境，有效提升了CPS的安全性和韧性。

Abstract: In the last decades, Cyber-physical Systems (CPSs) have experienced a
significant technological evolution and increased connectivity, at the cost of
greater exposure to cyber-attacks. Since many CPS are used in safety-critical
systems, such attacks entail high risks and potential safety harms. Although
several defense strategies have been proposed, they rarely exploit the
cyber-physical nature of the system. In this work, we exploit the nature of CPS
by proposing CyFence, a novel architecture that improves the resilience of
closed-loop control systems against cyber-attacks by adding a semantic check,
used to confirm that the system is behaving as expected. To ensure the security
of the semantic check code, we use the Trusted Execution Environment
implemented by modern processors. We evaluate CyFence considering a real-world
application, consisting of an active braking digital controller, demonstrating
that it can mitigate different types of attacks with a negligible computation
overhead.

</details>


### [28] [From IOCs to Group Profiles: On the Specificity of Threat Group Behaviors in CTI Knowledge Bases](https://arxiv.org/abs/2506.10645)
*Aakanksha Saha,Martina Lindorfer,Juan Caballero*

Main category: cs.CR

TL;DR: 论文分析了威胁行为特征（如TTPs和软件）在威胁检测中的独特性，发现仅有少数威胁组织具有独特行为，质疑其能否完全替代IOCs。


<details>
  <summary>Details</summary>
Motivation: 由于IOCs（如IP地址、文件哈希）易变且短效，网络安全领域转向更持久的行为特征（如TTPs和软件），但其独特性和完整性尚未充分研究。

Method: 系统分析了MITRE ATT&CK和Malpedia两个开源威胁情报知识库中的威胁组织行为特征，评估其独特性和组合效果。

Result: 仅34%的ATT&CK组织有独特技术，73%使用独特软件（Malpedia中降至24%）；组合数据后，覆盖率略有提升，但独特行为比例仍低于30%。

Conclusion: 行为特征在威胁组织归因中难以完全替代IOCs，64%的组织仍缺乏独特行为，需进一步研究改进。

Abstract: Indicators of Compromise (IOCs) such as IP addresses, file hashes, and domain
names are commonly used for threat detection and attribution. However, IOCs
tend to be short-lived as they are easy to change. As a result, the
cybersecurity community is shifting focus towards more persistent behavioral
profiles, such as the Tactics, Techniques, and Procedures (TTPs) and the
software used by a threat group. However, the distinctiveness and completeness
of such behavioral profiles remain largely unexplored. In this work, we
systematically analyze threat group profiles built from two open cyber threat
intelligence (CTI) knowledge bases: MITRE ATT&CK and Malpedia. We first
investigate what fraction of threat groups have group-specific behaviors, i.e.,
behaviors used exclusively by a single group. We find that only 34% of threat
groups in ATT&CK have group-specific techniques. The software used by a threat
group proves to be more distinctive, with 73% of ATT&CK groups using
group-specific software. However, this percentage drops to 24% in the broader
Malpedia dataset. Next, we evaluate how group profiles improve when data from
both sources are combined. While coverage improves modestly, the proportion of
groups with group-specific behaviors remains under 30%. We then enhance
profiles by adding exploited vulnerabilities and additional techniques
extracted from more threat reports. Despite the additional information, 64% of
groups still lack any group-specific behavior. Our findings raise concerns on
the belief that behavioral profiles can replace IOCs in threat group
attribution.

</details>


### [29] [GOLIATH: A Decentralized Framework for Data Collection in Intelligent Transportation Systems](https://arxiv.org/abs/2506.10665)
*Davide Maffiola,Stefano Longari,Michele Carminati,Mara Tanelli,Stefano Zanero*

Main category: cs.CR

TL;DR: 该论文提出了GOLIATH，一种基于区块链的去中心化框架，用于车辆间实时信息交换，解决了传统集中式交通信息收集的信任和单点故障问题。


<details>
  <summary>Details</summary>
Motivation: 传统交通信息收集依赖集中式模型，存在单点故障和信任问题，而车载传感器或智能手机的众包数据收集成为可能，但需要去中心化解决方案。

Method: 提出GOLIATH框架，利用区块链技术在车载信息娱乐系统上运行，通过新型共识机制验证车辆位置和邻居信息交易。

Result: 在模拟环境中验证了框架的可行性和鲁棒性，能够抵御多种威胁。

Conclusion: GOLIATH框架通过去中心化设计，实现了可信的交通信息收集与交换，适用于车辆定位和交通信息管理。

Abstract: Intelligent Transportation Systems (ITSs) technology has advanced during the
past years, and it is now used for several applications that require vehicles
to exchange real-time data, such as in traffic information management.
Traditionally, road traffic information has been collected using on-site
sensors. However, crowd-sourcing traffic information from onboard sensors or
smartphones has become a viable alternative. State-of-the-art solutions
currently follow a centralized model where only the service provider has
complete access to the collected traffic data and represent a single point of
failure and trust. In this paper, we propose GOLIATH, a blockchain-based
decentralized framework that runs on the In-Vehicle Infotainment (IVI) system
to collect real-time information exchanged between the network's participants.
Our approach mitigates the limitations of existing crowd-sourcing centralized
solutions by guaranteeing trusted information collection and exchange, fully
exploiting the intrinsic distributed nature of vehicles. We demonstrate its
feasibility in the context of vehicle positioning and traffic information
management. Each vehicle participating in the decentralized network shares its
position and neighbors' ones in the form of a transaction recorded on the
ledger, which uses a novel consensus mechanism to validate it. We design the
consensus mechanism resilient against a realistic set of adversaries that aim
to tamper or disable the communication. We evaluate the proposed framework in a
simulated (but realistic) environment, which considers different threats and
allows showing its robustness and safety properties.

</details>


### [30] [Commitment Schemes for Multi-Party Computation](https://arxiv.org/abs/2506.10721)
*Ioan Ionescu,Ruxandra F. Olimid*

Main category: cs.CR

TL;DR: 本文分析了多党计算（MPC）协议中使用的承诺方案（CSs），探讨了CSs与MPC之间的关联及其对上层MPC的影响。


<details>
  <summary>Details</summary>
Motivation: 尽管CSs和MPC的独立属性已被广泛研究，但它们在具体协议和应用中的相互关系仍未被充分探索。

Method: 研究了不同类型的CSs如何影响MPC构建及其与实际应用的关系，并作为教程帮助理解CS与MPC的密码学互动。

Result: 强调了根据MPC的对抗性和功能需求谨慎选择CS的重要性。

Conclusion: 通过选择合适的CS，可以实现更强大且保护隐私的密码学应用。

Abstract: The paper presents an analysis of Commitment Schemes (CSs) used in
Multi-Party Computation (MPC) protocols. While the individual properties of CSs
and the guarantees offered by MPC have been widely studied in isolation, their
interrelation in concrete protocols and applications remains mostly
underexplored. This paper presents the relation between the two, with an
emphasis on (security) properties and their impact on the upper layer MPC. In
particular, we investigate how different types of CSs contribute to various MPC
constructions and their relation to real-life applications of MPC. The paper
can also serve as a tutorial for understanding the cryptographic interplay
between CS and MPC, making it accessible to both researchers and practitioners.
Our findings emphasize the importance of carefully selecting CS to meet the
adversarial and functional requirements of MPC, thereby aiming for more robust
and privacy-preserving cryptographic applications

</details>


### [31] [TED-LaST: Towards Robust Backdoor Defense Against Adaptive Attacks](https://arxiv.org/abs/2506.10722)
*Xiaoxing Mo,Yuxuan Cheng,Nan Sun,Leo Yu Zhang,Wei Luo,Shang Gao*

Main category: cs.CR

TL;DR: TED-LaST是一种增强TED对抗自适应后门攻击的新防御策略，通过标签监督动态跟踪和自适应层强调提升鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络（DNNs）易受后门攻击，传统TED检测方法对自适应攻击的鲁棒性不足。

Method: 提出TED-LaST，引入标签监督动态跟踪和自适应层强调，识别传统TED无法检测的隐蔽威胁。

Result: 在多个数据集和模型架构上验证，TED-LaST有效对抗复杂后门攻击（如Adap-Blend、Adapt-Patch）。

Conclusion: TED-LaST为后门检测设定了新标准，显著提升DNN安全性。

Abstract: Deep Neural Networks (DNNs) are vulnerable to backdoor attacks, where
attackers implant hidden triggers during training to maliciously control model
behavior. Topological Evolution Dynamics (TED) has recently emerged as a
powerful tool for detecting backdoor attacks in DNNs. However, TED can be
vulnerable to backdoor attacks that adaptively distort topological
representation distributions across network layers. To address this limitation,
we propose TED-LaST (Topological Evolution Dynamics against Laundry, Slow
release, and Target mapping attack strategies), a novel defense strategy that
enhances TED's robustness against adaptive attacks. TED-LaST introduces two key
innovations: label-supervised dynamics tracking and adaptive layer emphasis.
These enhancements enable the identification of stealthy threats that evade
traditional TED-based defenses, even in cases of inseparability in topological
space and subtle topological perturbations. We review and classify data
poisoning tricks in state-of-the-art adaptive attacks and propose enhanced
adaptive attack with target mapping, which can dynamically shift malicious
tasks and fully leverage the stealthiness that adaptive attacks possess. Our
comprehensive experiments on multiple datasets (CIFAR-10, GTSRB, and
ImageNet100) and model architectures (ResNet20, ResNet101) show that TED-LaST
effectively counteracts sophisticated backdoors like Adap-Blend, Adapt-Patch,
and the proposed enhanced adaptive attack. TED-LaST sets a new benchmark for
robust backdoor detection, substantially enhancing DNN security against
evolving threats.

</details>


### [32] [ObfusBFA: A Holistic Approach to Safeguarding DNNs from Different Types of Bit-Flip Attacks](https://arxiv.org/abs/2506.10744)
*Xiaobei Yan,Han Qiu,Tianwei Zhang*

Main category: cs.CR

TL;DR: ObfusBFA是一种高效且全面的方法，通过引入随机虚拟操作来抵御针对DNN的位翻转攻击（BFA），显著降低攻击成功率并保持模型准确性。


<details>
  <summary>Details</summary>
Motivation: 位翻转攻击（BFA）对DNN构成严重威胁，现有防御方法仅针对特定攻击和平台，缺乏普适性。

Method: ObfusBFA通过在模型推理中引入随机虚拟操作，将攻击转化为随机位翻转，并设计算法识别关键位和插入混淆操作。

Result: ObfusBFA在不同攻击类型和自适应场景下均能保持模型准确性，显著降低攻击成功率，且引入的延迟和存储开销极小。

Conclusion: ObfusBFA是一种实用且高效的防御方法，适用于实际应用场景。

Abstract: Bit-flip attacks (BFAs) represent a serious threat to Deep Neural Networks
(DNNs), where flipping a small number of bits in the model parameters or binary
code can significantly degrade the model accuracy or mislead the model
prediction in a desired way. Existing defenses exclusively focus on protecting
models for specific attacks and platforms, while lacking effectiveness for
other scenarios. We propose ObfusBFA, an efficient and holistic methodology to
mitigate BFAs targeting both the high-level model weights and low-level
codebase (executables or shared libraries). The key idea of ObfusBFA is to
introduce random dummy operations during the model inference, which effectively
transforms the delicate attacks into random bit flips, making it much harder
for attackers to pinpoint and exploit vulnerable bits. We design novel
algorithms to identify critical bits and insert obfuscation operations. We
evaluate ObfusBFA against different types of attacks, including the adaptive
scenarios where the attacker increases the flip bit budget to attempt to
circumvent our defense. The results show that ObfusBFA can consistently
preserve the model accuracy across various datasets and DNN architectures while
significantly reducing the attack success rates. Additionally, it introduces
minimal latency and storage overhead, making it a practical solution for
real-world applications.

</details>


### [33] [Quantifying Azure RBAC Wildcard Overreach](https://arxiv.org/abs/2506.10755)
*Christophe Parisel*

Main category: cs.CR

TL;DR: Belshazaar框架通过两阶段方法解决Azure RBAC中通配符权限的问题，包括显式展开通配符和量化语义越界，发现39%的操作存在跨资源提供者权限问题。


<details>
  <summary>Details</summary>
Motivation: Azure RBAC的通配符权限简化了策略编写，但掩盖了实际允许的操作，削弱了最小权限原则。

Method: 1. 通过上下文无关文法形式化Azure操作语法，并实现编译器展开通配符；2. 定义超度量直径指标量化通配符的语义越界。

Result: 在15481个操作中，39%的操作与非明显通配符关联时存在跨资源提供者权限问题，且有效权限集可计算。

Conclusion: 通配符模式可能导致权限膨胀，Belshazaar提供了一种可扩展的语义驱动方法，以实现更严格的最小权限RBAC策略。

Abstract: Azure RBAC leverages wildcard permissions to simplify policy authoring, but
this abstraction often obscures the actual set of allowed operations and
undermines least-privilege guarantees. We introduce Belshazaar, a two-stage
framework that targets both the effective permission set problem and the
evaluation of wildcards permissions spread. First, we formalize Azure action
syntax via a context free grammar and implement a compiler that expands any
wildcard into its explicit action set. Second, we define an ultrametric
diameter metric to quantify semantic overreach in wildcard scenarios. Applied
to Microsoft s official catalog of 15481 actions, Belshazaar reveals that about
39 percent of actions admit a cross Resource Provider reach when associated
with non obvious wildcards, and that effective permissions sets are effectively
computable. These findings demonstrate that wildcard patterns can introduce
substantial privilege bloat, and that our approach offers a scalable, semantics
driven path toward tighter, least-privilege RBAC policies in Azure
environments.

</details>


### [34] [ME: Trigger Element Combination Backdoor Attack on Copyright Infringement](https://arxiv.org/abs/2506.10776)
*Feiyu Yang,Siyuan Liang,Aishan Liu,Dacheng Tao*

Main category: cs.CR

TL;DR: 论文提出了Multi-Element (ME)攻击方法，基于SBD改进，通过增加每个毒样本的视觉-文本元素数量提升攻击能力，同时引入DCT保持隐蔽性，在低采样率下表现优于SBD。


<details>
  <summary>Details</summary>
Motivation: 现有攻击方法如SBD在数据资源有限、样本适用性不足及低采样率下表现不佳，需改进攻击效果与隐蔽性。

Method: 提出ME攻击方法，增加毒样本的多元素数量，并引入DCT技术保持隐蔽性。

Result: 在两种新数据集上，CIR/FAE分别达到16.78%/39.50和51.20%/23.60，低采样率下表现优于SBD。

Conclusion: ME方法显著提升了攻击效果，尤其在低采样率下，为生成模型的版权攻击研究提供了新思路。

Abstract: The capability of generative diffusion models (DMs) like Stable Diffusion
(SD) in replicating training data could be taken advantage of by attackers to
launch the Copyright Infringement Attack, with duplicated poisoned image-text
pairs. SilentBadDiffusion (SBD) is a method proposed recently, which shew
outstanding performance in attacking SD in text-to-image tasks. However, the
feasible data resources in this area are still limited, some of them are even
constrained or prohibited due to the issues like copyright ownership or
inappropriate contents; And not all of the images in current datasets are
suitable for the proposed attacking methods; Besides, the state-of-the-art
(SoTA) performance of SBD is far from ideal when few generated poisoning
samples could be adopted for attacks. In this paper, we raised new datasets
accessible for researching in attacks like SBD, and proposed Multi-Element (ME)
attack method based on SBD by increasing the number of poisonous visual-text
elements per poisoned sample to enhance the ability of attacking, while
importing Discrete Cosine Transform (DCT) for the poisoned samples to maintain
the stealthiness. The Copyright Infringement Rate (CIR) / First Attack Epoch
(FAE) we got on the two new datasets were 16.78% / 39.50 and 51.20% / 23.60,
respectively close to or even outperformed benchmark Pokemon and Mijourney
datasets. In condition of low subsampling ratio (5%, 6 poisoned samples), MESI
and DCT earned CIR / FAE of 0.23% / 84.00 and 12.73% / 65.50, both better than
original SBD, which failed to attack at all.

</details>


### [35] [Monitoring Decomposition Attacks in LLMs with Lightweight Sequential Monitors](https://arxiv.org/abs/2506.10949)
*Chen Yueh-Han,Nitish Joshi,Yulin Chen,Maksym Andriushchenko,Rico Angell,He He*

Main category: cs.CR

TL;DR: 当前LLM安全防御在分解攻击下失效，恶意目标被分解为良性子任务绕过拒绝。现有浅层安全对齐技术仅检测即时提示中的危害，无法识别长程恶意意图。提出外部监控机制，实验证明其高效防御分解攻击。


<details>
  <summary>Details</summary>
Motivation: 现有LLM安全防御无法应对分解攻击，需开发能识别长程恶意意图的监控方法。

Method: 提出轻量级顺序监控框架，通过累积评估子任务实时防御分解攻击。

Result: 实验显示，该监控框架防御成功率达93%，成本降低90%，延迟减少50%。

Conclusion: 轻量级顺序监控能高效防御分解攻击，适合实际部署。

Abstract: Current LLM safety defenses fail under decomposition attacks, where a
malicious goal is decomposed into benign subtasks that circumvent refusals. The
challenge lies in the existing shallow safety alignment techniques: they only
detect harm in the immediate prompt and do not reason about long-range intent,
leaving them blind to malicious intent that emerges over a sequence of
seemingly benign instructions. We therefore propose adding an external monitor
that observes the conversation at a higher granularity. To facilitate our study
of monitoring decomposition attacks, we curate the largest and most diverse
dataset to date, including question-answering, text-to-image, and agentic
tasks. We verify our datasets by testing them on frontier LLMs and show an 87%
attack success rate on average on GPT-4o. This confirms that decomposition
attack is broadly effective. Additionally, we find that random tasks can be
injected into the decomposed subtasks to further obfuscate malicious intents.
To defend in real time, we propose a lightweight sequential monitoring
framework that cumulatively evaluates each subtask. We show that a carefully
prompt engineered lightweight monitor achieves a 93% defense success rate,
beating reasoning models like o3 mini as a monitor. Moreover, it remains robust
against random task injection and cuts cost by 90% and latency by 50%. Our
findings suggest that lightweight sequential monitors are highly effective in
mitigating decomposition attacks and are viable in deployment.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [36] [AniMaker: Automated Multi-Agent Animated Storytelling with MCTS-Driven Clip Generation](https://arxiv.org/abs/2506.10540)
*Haoyuan Shi,Yunxin Li,Xinyu Chen,Longyue Wang,Baotian Hu,Min Zhang*

Main category: cs.MA

TL;DR: AniMaker是一个多智能体框架，用于从文本输入生成连贯的多场景故事动画，通过智能剪辑生成和选择解决现有方法的叙事不连贯和视觉连续性问题。


<details>
  <summary>Details</summary>
Motivation: 当前视频生成模型在多场景和角色动画生成中存在叙事不连贯和视觉连续性问题，AniMaker旨在解决这些问题。

Method: AniMaker采用多智能体框架，包括导演、摄影、评审和后制代理，结合MCTS-Gen和AniEval技术优化剪辑生成和评估。

Result: 实验表明，AniMaker在VBench和AniEval等指标上表现优异，显著提升了生成效率和质量。

Conclusion: AniMaker通过多智能体协作和先进技术，推动了AI生成故事动画向生产标准迈进。

Abstract: Despite rapid advancements in video generation models, generating coherent
storytelling videos that span multiple scenes and characters remains
challenging. Current methods often rigidly convert pre-generated keyframes into
fixed-length clips, resulting in disjointed narratives and pacing issues.
Furthermore, the inherent instability of video generation models means that
even a single low-quality clip can significantly degrade the entire output
animation's logical coherence and visual continuity. To overcome these
obstacles, we introduce AniMaker, a multi-agent framework enabling efficient
multi-candidate clip generation and storytelling-aware clip selection, thus
creating globally consistent and story-coherent animation solely from text
input. The framework is structured around specialized agents, including the
Director Agent for storyboard generation, the Photography Agent for video clip
generation, the Reviewer Agent for evaluation, and the Post-Production Agent
for editing and voiceover. Central to AniMaker's approach are two key technical
components: MCTS-Gen in Photography Agent, an efficient Monte Carlo Tree Search
(MCTS)-inspired strategy that intelligently navigates the candidate space to
generate high-potential clips while optimizing resource usage; and AniEval in
Reviewer Agent, the first framework specifically designed for multi-shot
animation evaluation, which assesses critical aspects such as story-level
consistency, action completion, and animation-specific features by considering
each clip in the context of its preceding and succeeding clips. Experiments
demonstrate that AniMaker achieves superior quality as measured by popular
metrics including VBench and our proposed AniEval framework, while
significantly improving the efficiency of multi-candidate generation, pushing
AI-generated storytelling animation closer to production standards.

</details>


### [37] [Higher-Order Uncoupled Learning Dynamics and Nash Equilibrium](https://arxiv.org/abs/2506.10874)
*Sarah A. Toonsi,Jeff S. Shamma*

Main category: cs.MA

TL;DR: 论文研究了在有限博弈中混合策略纳什均衡（NE）的可学习性，使用高阶复制动力学和一类高阶非耦合异质动力学。通过将非耦合学习与分散控制中的反馈稳定化联系起来，证明了存在高阶非耦合学习动力学可以局部收敛到完全混合策略NE。同时，通过构造两个博弈，证明了学习动力学的非普适性。此外，引入了渐近最佳响应（ABR）属性，并探讨了其与NE的兼容性。最后，研究了在老虎机设置中混合策略NE的可学习性。


<details>
  <summary>Details</summary>
Motivation: 研究混合策略纳什均衡在有限博弈中的可学习性，探讨非耦合和高阶学习动力学的作用，以及其在分散控制和反馈稳定化中的应用。

Method: 使用高阶复制动力学和一类高阶非耦合异质动力学，将非耦合学习与反馈稳定化联系起来，并通过构造特定博弈验证学习动力学的非普适性。引入ABR属性并分析其与NE的兼容性。

Result: 证明了存在高阶非耦合学习动力学可以局部收敛到完全混合策略NE，但学习动力学不具备普适性。ABR属性与NE的兼容性得到分析。

Conclusion: 论文通过高阶非耦合学习动力学和ABR属性，为混合策略NE的可学习性提供了理论支持，并揭示了学习动力学的局限性。

Abstract: We study learnability of mixed-strategy Nash Equilibrium (NE) in general
finite games using higher-order replicator dynamics as well as classes of
higher-order uncoupled heterogeneous dynamics. In higher-order uncoupled
learning dynamics, players have no access to utilities of opponents (uncoupled)
but are allowed to use auxiliary states to further process information
(higher-order). We establish a link between uncoupled learning and feedback
stabilization with decentralized control. Using this association, we show that
for any finite game with an isolated completely mixed-strategy NE, there exist
higher-order uncoupled learning dynamics that lead (locally) to that NE. We
further establish the lack of universality of learning dynamics by linking
learning to the control theoretic concept of simultaneous stabilization. We
construct two games such that any higher-order dynamics that learn the
completely mixed-strategy NE of one of these games can never learn the
completely mixed-strategy NE of the other. Next, motivated by imposing natural
restrictions on allowable learning dynamics, we introduce the Asymptotic Best
Response (ABR) property. Dynamics with the ABR property asymptotically learn a
best response in environments that are asymptotically stationary. We show that
the ABR property relates to an internal stability condition on higher-order
learning dynamics. We provide conditions under which NE are compatible with the
ABR property. Finally, we address learnability of mixed-strategy NE in the
bandit setting using a bandit version of higher-order replicator dynamics.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [38] [TrioXpert: An automated incident management framework for microservice system](https://arxiv.org/abs/2506.10043)
*Yongqian Sun,Yu Luo,Xidao Wen,Yuan Yuan,Xiaohui Nie,Shenglin Zhang,Tong Liu,Xi Luo*

Main category: cs.SE

TL;DR: TrioXpert是一个端到端的事故管理框架，利用多模态数据和大型语言模型（LLMs）协同推理，显著提升了异常检测、故障分类和根因定位的性能，并增强了可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅依赖单模态数据，难以同时处理多个下游任务且缺乏可解释性。

Method: 设计三个独立的数据处理管道，基于多模态数据特性，结合LLMs进行协同推理。

Result: 在两个数据集上，TrioXpert在AD、FT和RCL任务中性能显著提升（AD提升4.7%-57.7%，FT提升2.1%-40.6%，RCL提升1.6%-163.1%）。

Conclusion: TrioXpert通过多模态数据和LLMs的协同推理，有效解决了现有方法的局限性，实现了高性能和强可解释性。

Abstract: Automated incident management plays a pivotal role in large-scale
microservice systems. However, many existing methods rely solely on
single-modal data (e.g., metrics, logs, and traces) and struggle to
simultaneously address multiple downstream tasks, including anomaly detection
(AD), failure triage (FT), and root cause localization (RCL). Moreover, the
lack of clear reasoning evidence in current techniques often leads to
insufficient interpretability. To address these limitations, we propose
TrioXpert, an end-to-end incident management framework capable of fully
leveraging multimodal data. TrioXpert designs three independent data processing
pipelines based on the inherent characteristics of different modalities,
comprehensively characterizing the operational status of microservice systems
from both numerical and textual dimensions. It employs a collaborative
reasoning mechanism using large language models (LLMs) to simultaneously handle
multiple tasks while providing clear reasoning evidence to ensure strong
interpretability. We conducted extensive evaluations on two popular
microservice system datasets, and the experimental results demonstrate that
TrioXpert achieves outstanding performance in AD (improving by 4.7% to 57.7%),
FT (improving by 2.1% to 40.6%), and RCL (improving by 1.6% to 163.1%) tasks.

</details>


### [39] [Online Discovery of Simulation Models for Evolving Business Processes (Extended Version)](https://arxiv.org/abs/2506.10049)
*Francesco Vinci,Gyunam Park,Wil van der Aalst,Massimiliano de Leoni*

Main category: cs.SE

TL;DR: 提出了一种流式过程模拟发现技术，结合增量过程发现和在线机器学习方法，适应动态业务环境。


<details>
  <summary>Details</summary>
Motivation: 现有过程模拟发现技术缺乏对实时运营变化的适应性，无法满足动态业务环境的需求。

Method: 结合增量过程发现与在线机器学习方法，优先考虑近期数据同时保留历史信息。

Result: 在四个不同事件日志上的实验表明，该技术能生成更稳定的模拟，并有效处理概念漂移。

Conclusion: 该技术不仅提高了模拟的稳定性，还增强了处理动态过程变化的能力。

Abstract: Business Process Simulation (BPS) refers to techniques designed to replicate
the dynamic behavior of a business process. Many approaches have been proposed
to automatically discover simulation models from historical event logs,
reducing the cost and time to manually design them. However, in dynamic
business environments, organizations continuously refine their processes to
enhance efficiency, reduce costs, and improve customer satisfaction. Existing
techniques to process simulation discovery lack adaptability to real-time
operational changes. In this paper, we propose a streaming process simulation
discovery technique that integrates Incremental Process Discovery with Online
Machine Learning methods. This technique prioritizes recent data while
preserving historical information, ensuring adaptation to evolving process
dynamics. Experiments conducted on four different event logs demonstrate the
importance in simulation of giving more weight to recent data while retaining
historical knowledge. Our technique not only produces more stable simulations
but also exhibits robustness in handling concept drift, as highlighted in one
of the use cases.

</details>


### [40] [The Effects of GitHub Copilot on Computing Students' Programming Effectiveness, Efficiency, and Processes in Brownfield Programming Tasks](https://arxiv.org/abs/2506.10051)
*Md Istiak Hossain Shihab,Christopher Hundhausen,Ahsun Tariq,Summit Haque,Yunhan Qiao,Brian Mulanda*

Main category: cs.SE

TL;DR: 研究探讨了GitHub Copilot对本科生在遗留代码库开发任务中的影响，发现使用Copilot能显著提升任务完成速度和进度，但学生对建议的理解不足。


<details>
  <summary>Details</summary>
Motivation: 探索生成式AI助手（如GitHub Copilot）在遗留代码开发中对学生编程表现、行为和理解的影响。

Method: 通过对照实验，10名本科生在有无Copilot的情况下完成相似任务，结合性能、行为分析和退出访谈。

Result: 使用Copilot时，学生任务完成速度提升35%，进度增加50%，手动编码和网络搜索时间分别减少11%和12%。

Conclusion: 需开发新教学方法以利用GenAI优势，同时引导学生反思其建议的原理和适用性。

Abstract: When graduates of computing degree programs enter the software industry, they
will most likely join teams working on legacy code bases developed by people
other than themselves. In these so-called brownfield software development
settings, generative artificial intelligence (GenAI) coding assistants like
GitHub Copilot are rapidly transforming software development practices, yet the
impact of GenAI on student programmers performing brownfield development tasks
remains underexplored. This paper investigates how GitHub Copilot influences
undergraduate students' programming performance, behaviors, and understanding
when completing brownfield programming tasks in which they add new code to an
unfamiliar code base. We conducted a controlled experiment in which 10
undergraduate computer science students completed highly similar brownfield
development tasks with and without Copilot in a legacy web application. Using a
mixed-methods approach combining performance analysis, behavioral analysis, and
exit interviews, we found that students completed tasks 35% faster (p < 0.05)
and made 50% more solution progress p (< 0.05) when using Copilot. Moreover,
our analysis revealed that, when using Copilot, students spent 11% less time
manually writing code (p < 0.05), and 12% less time conducting web searches (p
< 0.05), providing evidence of a fundamental shift in how they engaged in
programming. In exit interviews, students reported concerns about not
understanding how or why Copilot suggestions work. This research suggests the
need for computing educators to develop new pedagogical approaches that
leverage GenAI assistants' benefits while fostering reflection on how and why
GenAI suggestions address brownfield programming tasks. Complete study results
and analysis are presented at https://ghcopilot-icer.github.io/.

</details>


### [41] [Reward Models Enable Scalable Code Verification by Trading Accuracy for Throughput](https://arxiv.org/abs/2506.10056)
*Gabriel Orlanski,Nicholas Roberts,Aws Albarghouthi,Frederic Sala*

Main category: cs.SE

TL;DR: 本文挑战了在LLM编程任务中优先使用全面验证器（如完整测试套件）的共识，探讨了速度与准确性的权衡，发现结果奖励模型（ORM）在牺牲少量准确性的情况下显著提升速度，尤其在生成-修剪-排序方法中表现突出。


<details>
  <summary>Details</summary>
Motivation: 研究在LLM编程任务中，全面验证器与结果奖励模型（ORM）之间的权衡，探索如何通过ORM在速度和准确性之间取得平衡。

Method: 采用生成-修剪-排序方法，先用较快的ORM验证器筛选错误解，再用全面验证器排序，比较其与直接使用全面验证器的性能差异。

Result: 生成-修剪-排序方法比直接使用全面验证器快11.65倍，仅损失8.33%的准确性，且能有效过滤错误但排名高的解。

Conclusion: ORM在编程任务验证中具有重要价值，尤其在需要速度和准确性平衡的场景下，生成-修剪-排序方法是一种高效且实用的解决方案。

Abstract: The standard paradigm for solving coding tasks via large language models
(LLMs) is to generate-then-rank programs, where the latter step uses a verifier
in the ranking process. The growing consensus is that a comprehensive verifier
(e.g., a full test suite) should be prioritized over an outcome reward model
(ORM) whenever possible, with little consideration given to the trade-offs
involved. We aim to challenge this assumption by systematically exploring the
tradeoff between speed and accuracy. We find that ORMs play a crucial role in
scaling verification through trading accuracy for speed, even when a
comprehensive verifier is available. Their value becomes especially apparent
when used in a generate-prune-then-rank approach, where a faster but less
accurate verifier removes incorrect solutions prior to ranking -- leading to a
system that is 11.65x faster while only being 8.33% less accurate than the full
test suite. We analyze the generate-prune-then-rank approach and show that it
works by filtering out incorrect but highly ranked solutions. These findings
enable the design of scalable and accurate program ranking systems.

</details>


### [42] [Prompt Variability Effects On LLM Code Generation](https://arxiv.org/abs/2506.10204)
*Andrei Paleyes,Radzim Sendyka,Diana Robinson,Christian Cabrera,Neil D. Lawrence*

Main category: cs.SE

TL;DR: 论文提出了一种评估LLM代码生成质量的合成管道和基于角色的方法，以量化输入变化对生成代码的影响。


<details>
  <summary>Details</summary>
Motivation: LLM生成的代码质量受用户背景和提示质量影响，需量化这种敏感性。

Method: 提出合成评估管道和基于角色的系统评估方法，独立于具体任务和LLM。

Result: 实验证明了方法的实用性，并公开了代码。

Conclusion: 方法广泛适用，有助于社区评估LLM代码生成质量。

Abstract: Code generation is one of the most active areas of application of Large
Language Models (LLMs). While LLMs lower barriers to writing code and
accelerate development process, the overall quality of generated programs
depends on the quality of given prompts. Specifically, functionality and
quality of generated code can be sensitive to user's background and familiarity
with software development. It is therefore important to quantify LLM's
sensitivity to variations in the input. To this end we propose a synthetic
evaluation pipeline for code generation with LLMs, as well as a systematic
persona-based evaluation approach to expose qualitative differences of LLM
responses dependent on prospective user background. Both proposed methods are
completely independent from specific programming tasks and LLMs, and thus are
widely applicable. We provide experimental evidence illustrating utility of our
methods and share our code for the benefit of the community.

</details>


### [43] [AI-Based Software Vulnerability Detection: A Systematic Literature Review](https://arxiv.org/abs/2506.10280)
*Samiha Shimmi,Hamed Okhravi,Mona Rahimi*

Main category: cs.SE

TL;DR: 本文系统回顾了2018至2023年软件漏洞检测（SVD）研究，发现91%的研究采用AI方法，其中图模型最流行。同时指出了数据集质量、可重复性和可解释性等关键限制，并提出了联邦学习和量子神经网络等新兴研究方向。


<details>
  <summary>Details</summary>
Motivation: 软件漏洞对网络安全构成严重威胁，传统检测方法（如静态分析、基于规则的匹配）逐渐转向AI驱动方法。

Method: 通过系统回顾2018至2023年的SVD研究，分类整理了技术、特征表示和嵌入方法。

Result: 91%的研究使用基于AI的方法，图模型最为普遍。同时发现数据集质量、可重复性和可解释性等限制。

Conclusion: 研究为未来方向提供了路线图，包括联邦学习和量子神经网络等新兴技术。

Abstract: Software vulnerabilities in source code pose serious cybersecurity risks,
prompting a shift from traditional detection methods (e.g., static analysis,
rule-based matching) to AI-driven approaches. This study presents a systematic
review of software vulnerability detection (SVD) research from 2018 to 2023,
offering a comprehensive taxonomy of techniques, feature representations, and
embedding methods. Our analysis reveals that 91% of studies use AI-based
methods, with graph-based models being the most prevalent. We identify key
limitations, including dataset quality, reproducibility, and interpretability,
and highlight emerging opportunities in underexplored techniques such as
federated learning and quantum neural networks, providing a roadmap for future
research.

</details>


### [44] [Minimizing False Positives in Static Bug Detection via LLM-Enhanced Path Feasibility Analysis](https://arxiv.org/abs/2506.10322)
*Xueying Du,Kai Yu,Chong Wang,Yi Zou,Wentai Deng,Zuoyu Ou,Xin Peng,Lingming Zhang,Yiling Lou*

Main category: cs.SE

TL;DR: LLM4PFA是一个基于LLM代理的迭代路径可行性分析框架，显著减少静态bug检测中的误报率。


<details>
  <summary>Details</summary>
Motivation: 现有静态bug分析器在大型代码库中误报率高，主要由于路径可行性验证能力不足。

Method: 利用LLM代理进行目标约束推理和上下文感知分析，增强复杂路径可行性分析。

Result: LLM4PFA能过滤72%-96%的误报，性能提升41.1%-105.7%，仅漏检3个真实bug。

Conclusion: LLM4PFA显著提升了静态bug检测的准确性，适用于大型项目。

Abstract: Static bug analyzers play a crucial role in ensuring software quality.
However, existing analyzers for bug detection in large codebases often suffer
from high false positive rates. This is primarily due to the limited
capabilities of analyzers in path feasibility validation with multiple
conditional branches and complex data dependencies. While current LLM-based
approaches attempt to address this issue, their effectiveness remains limited
due to insufficient constraint cascade analysis and scalability challenges in
large projects. To address this challenge, we propose an iterative path
feasibility analysis framework LLM4PFA. By leveraging LLM agent based targeted
constraint reasoning, and key context-aware analysis driven by agent planning,
LLM4PFA effectively enhances complex inter-procedural path feasibility analysis
for minimizing false positives in static bug detection. Evaluation results show
that LLM4PFA precisely filters out 72% to 96% false positives reported during
static bug detection, significantly outperforming all the baselines by 41.1% -
105.7% improvements; meanwhile LLM4PFA only misses 3 real bugs of 45 true
positives.

</details>


### [45] [Augmenting Large Language Models with Static Code Analysis for Automated Code Quality Improvements](https://arxiv.org/abs/2506.10330)
*Seyed Moein Abtahi,Akramul Azim*

Main category: cs.SE

TL;DR: 该研究通过将大型语言模型（如GPT-3.5 Turbo和GPT-4o）集成到软件开发工作流中，实现了代码问题的自动检测与修订。结合静态代码分析框架和检索增强生成（RAG），显著减少了代码问题。


<details>
  <summary>Details</summary>
Motivation: 提高代码质量、优化软件开发流程并减少时间和资源消耗。

Method: 使用静态代码分析框架检测问题，通过LLMs自动化修订，结合RAG增强准确性，并利用代码比较工具修正错误。

Result: 代码问题显著减少，验证了LLMs与静态分析结合的有效性。

Conclusion: LLMs、静态分析和RAG的结合能有效提升代码质量并优化开发流程。

Abstract: This study examined code issue detection and revision automation by
integrating Large Language Models (LLMs) such as OpenAI's GPT-3.5 Turbo and
GPT-4o into software development workflows. A static code analysis framework
detects issues such as bugs, vulnerabilities, and code smells within a
large-scale software project. Detailed information on each issue was extracted
and organized to facilitate automated code revision using LLMs. An iterative
prompt engineering process is applied to ensure that prompts are structured to
produce accurate and organized outputs aligned with the project requirements.
Retrieval-augmented generation (RAG) is implemented to enhance the relevance
and precision of the revisions, enabling LLM to access and integrate real-time
external knowledge. The issue of LLM hallucinations - where the model generates
plausible but incorrect outputs - is addressed by a custom-built "Code
Comparison App," which identifies and corrects erroneous changes before
applying them to the codebase. Subsequent scans using the static code analysis
framework revealed a significant reduction in code issues, demonstrating the
effectiveness of combining LLMs, static analysis, and RAG to improve code
quality, streamline the software development process, and reduce time and
resource expenditure.

</details>


### [46] [AutoGEEval++: A Multi-Level and Multi-Geospatial-Modality Automated Evaluation Framework for Large Language Models in Geospatial Code Generation on Google Earth Engine](https://arxiv.org/abs/2506.10365)
*Shuyang Hou,Zhangxiao Shen,Huayi Wu,Haoyue Jiao,Ziqi Liu,Lutong Xie,Chang Liu,Jianyuan Liang,Yaxian Qing,Xiaopu Zhang,Dehua Peng,Zhipeng Gui,Xuefeng Guan*

Main category: cs.SE

TL;DR: AutoGEEval++是一个用于评估大语言模型（LLM）在Google Earth Engine（GEE）上生成地理空间代码的自动化框架，支持多维度评估，并提供了首个标准化基准数据集。


<details>
  <summary>Details</summary>
Motivation: 地理空间代码生成是AI与地理科学分析结合的关键领域，但缺乏标准化的自动化评估工具。

Method: 基于GEE Python API，AutoGEEval++包含6,365个测试案例，支持多任务类型和多维指标评估。

Result: 评估了24种先进LLM，揭示了不同模型在性能、稳定性和错误模式上的差异。

Conclusion: AutoGEEval++为GEE代码生成提供了标准化评估协议和基准，推动了领域内代码生成研究的系统化。

Abstract: Geospatial code generation is becoming a key frontier in integrating
artificial intelligence with geo-scientific analysis, yet standardised
automated evaluation tools for this task remain absent. This study presents
AutoGEEval++, an enhanced framework building on AutoGEEval, and the first
automated assessment system for large language models (LLMs) generating
geospatial code on Google Earth Engine (GEE). It supports diverse data
modalities and varying task complexities. Built on the GEE Python API,
AutoGEEval++ features a benchmark dataset-AutoGEEval++-Bench-with 6,365 test
cases across 26 data types and three task categories: unit, combo, and theme
tests. It includes a submission programme and a judge module to realise an
end-to-end automated evaluation pipeline from code generation to
execution-based validation. The framework adopts multi-dimensional
metrics-accuracy, resource usage, run-time efficiency, and error
types-balancing hallucination control and efficiency, and enabling boundary
testing and error pattern analysis. Using AutoGEEval++, we evaluate 24
state-of-the-art LLMs (as of June 2025), including general-purpose,
reasoning-enhanced, code-centric, and geoscience-specific models. Results
reveal clear performance, stability, and error differences across task types,
model designs, and deployment settings, confirming AutoGEEval++'s practical
value and scalability in vertical-domain code generation. This work establishes
the first standardised evaluation protocol and foundational benchmark for
GEE-based LLM code generation, providing a unified basis for performance
comparison and a methodological framework for systematic, domain-specific code
evaluation.

</details>


### [47] [MLLM-Based UI2Code Automation Guided by UI Layout Information](https://arxiv.org/abs/2506.10376)
*Fan Wu,Cuiyun Gao,Shuqing Li,Xin-Cheng Wen,Qing Liao*

Main category: cs.SE

TL;DR: LayoutCoder是一个基于多模态大语言模型的框架，用于从真实网页图像生成UI代码，通过三个关键模块解决现有方法的局限性，并在性能上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 自动化UI2Code任务以提高开发效率，但现有深度学习方法依赖大量标注数据且泛化能力不足，多模态大语言模型虽具潜力但难以理解复杂布局。

Method: 提出LayoutCoder框架，包含元素关系构建、UI布局解析和布局引导的代码融合三个模块。

Result: 在Snap2Code和Design2Code数据集上，LayoutCoder在BLEU和CLIP分数上分别平均提升10.14%和3.95%。

Conclusion: LayoutCoder通过模块化设计有效解决了复杂布局理解和代码生成问题，显著提升了UI2Code任务的性能。

Abstract: Converting user interfaces into code (UI2Code) is a crucial step in website
development, which is time-consuming and labor-intensive. The automation of
UI2Code is essential to streamline this task, beneficial for improving the
development efficiency. There exist deep learning-based methods for the task;
however, they heavily rely on a large amount of labeled training data and
struggle with generalizing to real-world, unseen web page designs. The advent
of Multimodal Large Language Models (MLLMs) presents potential for alleviating
the issue, but they are difficult to comprehend the complex layouts in UIs and
generate the accurate code with layout preserved. To address these issues, we
propose LayoutCoder, a novel MLLM-based framework generating UI code from
real-world webpage images, which includes three key modules: (1) Element
Relation Construction, which aims at capturing UI layout by identifying and
grouping components with similar structures; (2) UI Layout Parsing, which aims
at generating UI layout trees for guiding the subsequent code generation
process; and (3) Layout-Guided Code Fusion, which aims at producing the
accurate code with layout preserved. For evaluation, we build a new benchmark
dataset which involves 350 real-world websites named Snap2Code, divided into
seen and unseen parts for mitigating the data leakage issue, besides the
popular dataset Design2Code. Extensive evaluation shows the superior
performance of LayoutCoder over the state-of-the-art approaches. Compared with
the best-performing baseline, LayoutCoder improves 10.14% in the BLEU score and
3.95% in the CLIP score on average across all datasets.

</details>


### [48] [Bug Classification in Quantum Software: A Rule-Based Framework and Its Evaluation](https://arxiv.org/abs/2506.10397)
*Mir Mohammad Yousuf,Shabir Ahmad Sofi*

Main category: cs.SE

TL;DR: 本文提出了一种基于规则的自动化框架，用于对量子软件仓库中的问题进行多维度分类，并通过实验验证了其可靠性。


<details>
  <summary>Details</summary>
Motivation: 准确分类软件缺陷对提升软件质量至关重要，尤其是针对量子软件中的特定问题。

Method: 采用基于关键词和启发式的方法，对量子软件仓库中的问题进行自动分类，并通过人工标注验证其准确性。

Result: 框架在多个分类维度上表现良好，最高准确率达85.21%，但严重性分类仍有改进空间。量子特定缺陷占27.3%，电路级问题最为常见。

Conclusion: 该框架为量子软件缺陷分类提供了有效工具，未来可进一步优化严重性分类。

Abstract: Accurate classification of software bugs is essential for improving software
quality. This paper presents a rule-based automated framework for classifying
issues in quantum software repositories by bug type, category, severity, and
impacted quality attributes, with additional focus on quantum-specific bug
types. The framework applies keyword and heuristic-based techniques tailored to
quantum computing. To assess its reliability, we manually classified a
stratified sample of 4,984 issues from a dataset of 12,910 issues across 36
Qiskit repositories. Automated classifications were compared with ground truth
using accuracy, precision, recall, and F1-score. The framework achieved up to
85.21% accuracy, with F1-scores ranging from 0.7075 (severity) to 0.8393
(quality attribute). Statistical validation via paired t-tests and Cohen's
Kappa showed substantial to almost perfect agreement for bug type (k = 0.696),
category (k = 0.826), quality attribute (k = 0.818), and quantum-specific bug
type (k = 0.712). Severity classification showed slight agreement (k = 0.162),
suggesting room for improvement. Large-scale analysis revealed that classical
bugs dominate (67.2%), with quantum-specific bugs at 27.3%. Frequent bug
categories included compatibility, functional, and quantum-specific defects,
while usability, maintainability, and interoperability were the most impacted
quality attributes. Most issues (93.7%) were low severity; only 4.3% were
critical. A detailed review of 1,550 quantum-specific bugs showed that over
half involved quantum circuit-level problems, followed by gate errors and
hardware-related issues.

</details>


### [49] [Towards Understanding Bugs in Distributed Training and Inference Frameworks for Large Language Models](https://arxiv.org/abs/2506.10426)
*Xiao Yu,Haoxuan Chen,Feifei Niu,Xing Hu,Jacky Wai Keung,Xin Xia*

Main category: cs.SE

TL;DR: 论文首次对308个分布式训练/推理框架（DeepSpeed、Megatron-LM和Colossal-AI）的修复bug进行了大规模实证分析，探讨了bug症状、根本原因、修复策略，并提出了自动化修复的可能性。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）的快速发展，分布式训练和推理框架的复杂性增加，导致软件bug频发，影响性能和资源利用。研究这些bug的特性有助于设计更有效的调试和修复方法。

Method: 对三个流行框架（DeepSpeed、Megatron-LM、Colossal-AI）的308个已修复bug进行实证分析，包括症状、根本原因、修复努力和常见低复杂度修复策略。

Result: 发现48%的bug修复仅需少量代码更改（<=10 LOC），且可通过条件逻辑优化、参数处理增强或版本兼容性处理等简单策略解决。分布式特性还引入了独特的bug原因（如分配策略错误和通信错误）。

Conclusion: 研究为提升分布式框架及其依赖的LLM项目的可靠性提供了见解，并指出利用LLM工具实现自动化调试和修复的潜力。

Abstract: With the rapid development of large language models (LLMs), distributed
training and inference frameworks like DeepSpeed have become essential for
scaling model training and inference across multiple GPUs or nodes. However,
the increasing complexity of these frameworks brings non-trivial software bugs,
which may degrade training performance, cause unexpected failures, and result
in significant resource waste. Understanding framework bugs' characteristics is
fundamental for quality assurance, allowing the design of more effective
debugging and repair methods. Thus, our paper conducts the first large-scale
empirical analysis of 308 fixed bugs across three popular distributed
training/inference frameworks: DeepSpeed, Megatron-LM, and Colossal-AI. We
examine bug symptoms, root causes, bug identification and fixing efforts, and
common low-effort fixing strategies. Additionally, the distributed nature of
these frameworks introduces unique bug root causes, such as allocation strategy
error and distributed communication error. Diagnosing and fixing complex bugs
remains challenging due to factors like the disconnect between symptoms and
root causes, high bug reproduction costs, and low-level or cross-component
interactions. Interestingly, we observe that 48% of bug fixes require minimal
code changes (<=10 LOC) and follow simple strategies such as conditional logic
optimization, parameter handling enhancement, or version compatibility
handling, indicating potential for automation. Based on these insights, we
offer several implications for improving the reliability of both distributed
training and inference frameworks and their dependent LLM projects, while also
identifying opportunities to leverage LLM-based tools for automated debugging
and repair.

</details>


### [50] [EXPEREPAIR: Dual-Memory Enhanced LLM-based Repository-Level Program Repair](https://arxiv.org/abs/2506.10484)
*Fangwen Mu,Junjie Wang,Lin Shi,Song Wang,Shoubin Li,Qing Wang*

Main category: cs.SE

TL;DR: ExpeRepair是一种基于LLM的软件修复方法，通过双通道记忆系统（情景记忆和语义记忆）动态学习历史修复经验，显著提升了修复性能。


<details>
  <summary>Details</summary>
Motivation: 当前LLM修复方法存在孤立处理问题和静态提示策略的局限性，无法充分利用历史修复经验。

Method: ExpeRepair通过情景记忆存储具体修复示例，语义记忆编码抽象修复见解，并动态组合提示以增强适应性。

Result: 在SWE-bench Lite基准测试中，ExpeRepair以49.3%的pass@1分数优于现有开源方法。

Conclusion: ExpeRepair通过双记忆系统和动态提示策略，显著提升了LLM在软件修复任务中的表现。

Abstract: Automatically repairing software issues remains a fundamental challenge at
the intersection of software engineering and AI. Although recent advancements
in Large Language Models (LLMs) have demonstrated potential for
repository-level repair tasks, current methodologies exhibit two notable
limitations: (1) they often address issues in isolation, neglecting to
incorporate insights from previously resolved issues, and (2) they rely on
static and rigid prompting strategies, which constrain their ability to
generalize across diverse and evolving issue scenarios. Inspired by the dual
memory systems of human cognition, where episodic and semantic memories work
synergistically to support human reasoning and decision-making, we propose
ExpeRepair, a novel LLM-based approach that continuously learns from historical
repair experiences through dual-channel knowledge accumulation. ExpeRepair
organizes historical repair experiences into two complementary memories: an
episodic memory that stores concrete repair demonstrations, and a semantic
memory that encodes abstract reflective insights. At inference time, ExpeRepair
activates both memory systems by retrieving relevant demonstrations from
episodic memory and recalling high-level repair insights from semantic memory.
It further enhances adaptability through dynamic prompt composition,
synergistically integrating both memory types to replace static prompts with
context-aware, experience-driven prompts. Experiments on the SWE-bench Lite
benchmark demonstrate that ExpeRepair achieves a pass@1 score of 49.3% with
Claude 3.7 Sonnet, outperforming all state-of-the-art open-source methods.

</details>


### [51] [BugGen: A Self-Correcting Multi-Agent LLM Pipeline for Realistic RTL Bug Synthesis](https://arxiv.org/abs/2506.10501)
*Surya Jasper,Minh Luu,Evan Pan,Aakash Tyagi,Michael Quinn,Jiang Hu,David Kebo Houngninou*

Main category: cs.SE

TL;DR: BugGen是一种利用大型语言模型（LLM）自动生成、插入和验证RTL功能缺陷的多代理流程，显著提升了验证效率和ML辅助调试的质量。


<details>
  <summary>Details</summary>
Motivation: 硬件复杂性的增加导致验证资源紧张，需要采用机器学习方法提高调试效率，但现有方法无法可靠生成多样且可扩展的缺陷数据集。

Method: BugGen通过分区模块、闭环代理架构选择变异目标，并采用迭代优化和回滚机制确保语法正确性和功能可检测性。

Result: 在五个OpenTitan IP块中生成500个独特缺陷，功能准确率达94%，每小时验证17.7个缺陷，比人工快五倍，并发现104个未检测缺陷。

Conclusion: BugGen为生成高质量缺陷数据集提供了可扩展解决方案，显著提升了验证效率和ML辅助调试的实用性。

Abstract: Hardware complexity continues to strain verification resources, motivating
the adoption of machine learning (ML) methods to improve debug efficiency.
However, ML-assisted debugging critically depends on diverse and scalable bug
datasets, which existing manual or automated bug insertion methods fail to
reliably produce. We introduce BugGen, a first of its kind, fully autonomous,
multi-agent pipeline leveraging Large Language Models (LLMs) to systematically
generate, insert, and validate realistic functional bugs in RTL. BugGen
partitions modules, selects mutation targets via a closed-loop agentic
architecture, and employs iterative refinement and rollback mechanisms to
ensure syntactic correctness and functional detectability. Evaluated across
five OpenTitan IP blocks, BugGen produced 500 unique bugs with 94% functional
accuracy and achieved a throughput of 17.7 validated bugs per hour-over five
times faster than typical manual expert insertion. Additionally, BugGen
identified 104 previously undetected bugs in OpenTitan regressions,
highlighting its utility in exposing verification coverage gaps. Compared
against Certitude, BugGen demonstrated over twice the syntactic accuracy,
deeper exposure of testbench blind spots, and more functionally meaningful and
complex bug scenarios. Furthermore, when these BugGen-generated datasets were
employed to train ML-based failure triage models, we achieved high
classification accuracy (88.1%-93.2%) across different IP blocks, confirming
the practical utility and realism of generated bugs. BugGen thus provides a
scalable solution for generating high-quality bug datasets, significantly
enhancing verification efficiency and ML-assisted debugging.

</details>


### [52] [AdaptiveLLM: A Framework for Selecting Optimal Cost-Efficient LLM for Code-Generation Based on CoT Length](https://arxiv.org/abs/2506.10525)
*Junhang Cheng,Fang Liu,Chengru Wu,Li Zhang*

Main category: cs.SE

TL;DR: AdaptiveLLM框架通过自动评估任务难度动态选择最优LLM，显著提升代码生成效率并降低资源消耗。


<details>
  <summary>Details</summary>
Motivation: 现有LLM选择方法资源密集且忽视成本效率，且依赖人工标注难度标签，难以实际应用。

Method: 利用Chain-of-Thought长度估计任务难度，聚类为三个等级，结合CodeBERT和XGBoost分类器动态选择模型。

Result: 实验显示AdaptiveLLM在pass@1分数上提升7.86%，资源消耗降低88.9%，且比单一模型准确率提高约15%。

Conclusion: AdaptiveLLM通过自动化难度评估和动态模型选择，显著优化性能与成本的平衡，且比人工评估更可靠。

Abstract: While Large Language Models (LLMs) have significantly advanced code
generation efficiency, they face inherent challenges in balancing performance
and inference costs across diverse programming tasks. Dynamically selecting the
optimal LLM based on task difficulty and resource constraints offers a
promising approach to achieve an optimal balance between efficiency and
performance. However, existing model selection methods are resource-intensive
and often neglect cost efficiency. Moreover, these approaches rely on
human-annotated difficulty labels that are frequently inaccessible in
real-world settings and may not align with the LLM's own assessment of task
difficulty. In this paper, we introduce AdaptiveLLM, a framework that
dynamically selects optimal LLMs for a given coding task by automatically
assessing task difficulty. Our framework first estimates task difficulty using
Chain-of-Thought lengths generated by reasoning model, clusters these into
three difficulty levels via k-means, and fine-tunes CodeBERT to embed
difficulty-aware features. A trained XGBoost classifier then selects the best
model for each problem, optimizing the performance-cost trade-off. Experimental
results show that AdaptiveLLM achieves a 7.86% improvement in pass@1 score
while reducing resource consumption by 88.9% compared to baseline method
ComplexityNet. When compared to a single model, AdaptiveLLM demonstrates an
approximately 15% accuracy improvement, while maintaining the same level of
cost consumption. Apart from that, the difficulty assessment using CoT provides
more reliable selection criteria than human evaluation. Our replication package
is available at https://github.com/cjhCoder7/AdaptiveLLM.

</details>


### [53] [Scalable Software Testing in Fast Virtual Platforms: Leveraging SystemC, QEMU and Containerization](https://arxiv.org/abs/2506.10624)
*Lukas Jünger,Jan Henrik Weinstock,Tim Kraus*

Main category: cs.SE

TL;DR: 提出一种基于容器化的虚拟平台（VP）方法，用于减少环境依赖并支持云部署，以加速硬件/软件协同开发。


<details>
  <summary>Details</summary>
Motivation: 解决硬件/软件系统复杂性带来的挑战，特别是在硬件可用性滞后时，支持早期软件开发和测试。

Method: 利用容器化封装基于SystemC TLM-2.0标准的虚拟平台，结合开源技术（如QEMU和VCML），实现快速并行化测试执行。

Result: 通过AI加速器VP案例研究验证了方法的有效性。

Conclusion: 该方法为硬件/软件协同开发提供了实用解决方案，能够显著加速开发流程。

Abstract: The ever-increasing complexity of HW/SW systems presents a persistent
challenge, particularly in safety-critical domains like automotive, where
extensive testing is imperative. However, the availability of hardware often
lags behind, hindering early-stage software development. To address this,
Virtual Platforms (VPs) based on the SystemC TLM-2.0 standard have emerged as a
pivotal solution, enabling pre-silicon execution and testing of unmodified
target software. In this study, we propose an approach leveraging
containerization to encapsulate VPs in order to reduce environment dependencies
and enable cloud deployment for fast, parallelized test execution, as well as
open-source VP technologies such as QEMU and VCML to obviate the need for seat
licenses. To demonstrate the efficacy of our approach, we present an Artificial
Intelligence (AI) accelerator VP case study. Through our research, we offer a
robust solution to address the challenges posed by the complexity of HW/SW
systems, with practical implications for accelerating HW/SW co-development.

</details>


### [54] [Not One to Rule Them All: Mining Meaningful Code Review Orders From GitHub](https://arxiv.org/abs/2506.10654)
*Abir Bouraffa,Carolin Brandt,Andy Zaidmann,Walid Maalej*

Main category: cs.SE

TL;DR: 研究分析了开发者在GitHub拉取请求中评论代码时的导航顺序，发现44.6%的评论顺序非字母序，并识别了几种有意义的顺序模式。


<details>
  <summary>Details</summary>
Motivation: 探讨开发者在代码审查中如何选择导航顺序，以优化审查工具的支持。

Method: 分析了23,241个拉取请求的评论顺序，识别了非字母序的模式及其影响。

Result: 44.6%的拉取请求评论顺序非字母序，其中20.6%按最大差异优先，17.6%按文件与标题描述的相似性，29%测试文件优先。非字母序审查的文件比例更高，但批准率略低。

Conclusion: 研究表明需要为大型拉取请求提供更灵活的审查顺序支持，以提高审查效率。

Abstract: Developers use tools such as GitHub pull requests to review code, discuss
proposed changes, and request modifications. While changed files are commonly
presented in alphabetical order, this does not necessarily coincide with the
reviewer's preferred navigation sequence. This study investigates the different
navigation orders developers follow while commenting on changes submitted in
pull requests. We mined code review comments from 23,241 pull requests in 100
popular Java and Python repositories on GitHub to analyze the order in which
the reviewers commented on the submitted changes. Our analysis shows that for
44.6% of pull requests, the reviewers comment in a non-alphabetical order.
Among these pull requests, we identified traces of alternative meaningful
orders: 20.6% (2,134) followed a largest-diff-first order, 17.6% (1,827) were
commented in the order of the files' similarity to the pull request's title and
description, and 29% (1,188) of pull requests containing changes to both
production and test files adhered to a test-first order. We also observed that
the proportion of reviewed files to total submitted files was significantly
higher in non-alphabetically ordered reviews, which also received slightly
fewer approvals from reviewers, on average. Our findings highlight the need for
additional support during code reviews, particularly for larger pull requests,
where reviewers are more likely to adopt complex strategies rather than
following a single predefined order.

</details>


### [55] [Formalising Software Requirements using Large Language Models](https://arxiv.org/abs/2506.10704)
*Arshad Beg,Diarmuid O'Donoghue,Rosemary Monahan*

Main category: cs.SE

TL;DR: VERIFAI项目旨在通过自然语言处理、本体论、相似性重用和大语言模型等技术，自动生成形式化规范并追踪需求，解决形式化规范的可追溯性和验证问题。


<details>
  <summary>Details</summary>
Motivation: 解决形式化规范在软件开发全生命周期中的可追溯性和验证挑战。

Method: 结合自然语言处理、本体论、相似性重用、大语言模型和人工智能技术。

Result: 提供自动生成形式化规范和需求追踪的支持。

Conclusion: VERIFAI项目为形式化规范的生成和验证提供了创新方法。

Abstract: This paper is a brief introduction to our recently initiated project named
VERIFAI: Traceability and verification of natural language requirements. The
project addresses the challenges in the traceability and verification of formal
specifications through providing support for the automatic generation of the
formal specifications and the traceability of the requirements from the initial
software design stage through the systems implementation and verification.
Approaches explored in this project include Natural Language Processing, use of
ontologies to describe the software system domain, reuse of existing software
artefacts from similar systems (i.e. through similarity based reuse) and large
language models to identify and declare the specifications as well as use of
artificial intelligence to guide the process.

</details>


### [56] [From Tea Leaves to System Maps: Context-awareness in Monitoring Operational Machine Learning Models](https://arxiv.org/abs/2506.10770)
*Joran Leest,Claudia Raibulet,Patricia Lago,Ilias Gerostathopoulos*

Main category: cs.SE

TL;DR: 论文提出了一种系统化的方法，通过引入C-SAR框架来整合和结构化机器学习监控中的上下文信息，以解决现有研究中对上下文信息缺乏共识的问题。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型在生产环境中失败通常是由于上下文不对齐，而非统计异常。现有监控方法缺乏对上下文信息的系统化理解，导致无法有效进行根因分析和警报。

Method: 通过对94项跨领域研究的系统综述，提出了C-SAR框架，并识别了20种可复用的模式，用于支持监控活动。

Result: 研究提供了C-SAR框架和20种模式，将监控从单纯统计观察提升为系统化的上下文管理。

Conclusion: 该研究为机器学习监控提供了新的视角，强调通过系统化的上下文管理实现更可靠的监控实践。

Abstract: Machine learning (ML) models in production do not fail due to statistical
anomalies in their input data; they fail due to contextual misalignment -- when
their environment deviates from training assumptions, leading to unreliable
predictions. Effective ML monitoring requires rich contextual information to
move beyond detecting statistical shifts toward meaningful alerts and
systematic root-cause analysis. Yet, surprisingly, despite extensive research
in ML monitoring and related disciplines (drift detection, data validation,
out-of-distribution detection), there is no shared understanding of how to use
contextual information -- striking, given that monitoring involves
interpretation of information in context. In response, this paper presents a
systematic review to characterize and structure the various types of contextual
information in this domain. Our analysis examines 94 primary studies across
data mining, databases, software engineering, and ML. We introduce the
Contextual System--Aspect--Representation (C-SAR) framework, a conceptual model
that synthesizes our findings. We also identify 20 recurring and potentially
reusable patterns of specific system, aspect, and representation combinations,
and map them to the monitoring activities they support. This study provides a
new perspective on ML monitoring: from interpreting "tea leaves" of
observational statistics into constructing and managing "system maps" that
enable systematic and reliable ML monitoring practices.

</details>


### [57] [What Users Value and Critique: Large-Scale Analysis of User Feedback on AI-Powered Mobile Apps](https://arxiv.org/abs/2506.10785)
*Vinaik Chhetri,Krishna Upadhyay,A. B. Siddique,Umar Farooq*

Main category: cs.SE

TL;DR: 本文通过大规模用户反馈分析AI驱动的移动应用，揭示了用户对AI功能的感知和评价，开发了一个高精度的多阶段分析流程。


<details>
  <summary>Details</summary>
Motivation: 研究用户对AI功能的具体反馈，填补现有研究空白，解决用户反馈量大且复杂的问题。

Method: 利用292个AI应用和894K条用户评论，开发多阶段分析流程，包括分类、情感提取和聚类，并验证其准确性。

Result: 提取了100多万条情感-主题对，发现用户关注的核心主题：正面评价集中在生产力和可靠性，负面评价涉及技术问题和定价。

Conclusion: 该方法能更准确地反映用户对AI应用的体验，揭示了通用满意度和领域特定问题。

Abstract: Artificial Intelligence (AI)-powered features have rapidly proliferated
across mobile apps in various domains, including productivity, education,
entertainment, and creativity. However, how users perceive, evaluate, and
critique these AI features remains largely unexplored, primarily due to the
overwhelming volume of user feedback. In this work, we present the first
comprehensive, large-scale study of user feedback on AI-powered mobile apps,
leveraging a curated dataset of 292 AI-driven apps across 14 categories with
894K AI-specific reviews from Google Play. We develop and validate a
multi-stage analysis pipeline that begins with a human-labeled benchmark and
systematically evaluates large language models (LLMs) and prompting strategies.
Each stage, including review classification, aspect-sentiment extraction, and
clustering, is validated for accuracy and consistency. Our pipeline enables
scalable, high-precision analysis of user feedback, extracting over one million
aspect-sentiment pairs clustered into 18 positive and 15 negative user topics.
Our analysis reveals that users consistently focus on a narrow set of themes:
positive comments emphasize productivity, reliability, and personalized
assistance, while negative feedback highlights technical failures (e.g.,
scanning and recognition), pricing concerns, and limitations in language
support. Our pipeline surfaces both satisfaction with one feature and
frustration with another within the same review. These fine-grained,
co-occurring sentiments are often missed by traditional approaches that treat
positive and negative feedback in isolation or rely on coarse-grained analysis.
To this end, our approach provides a more faithful reflection of the real-world
user experiences with AI-powered apps. Category-aware analysis further uncovers
both universal drivers of satisfaction and domain-specific frustrations.

</details>


### [58] [Solving Package Management via Hypergraph Dependency Resolution](https://arxiv.org/abs/2506.10803)
*Ryan Gibb,Patrick Ferris,David Allsopp,Michael Winston Dales,Mark Elvers,Thomas Gazagnaire,Sadiq Jaffer,Thomas Leonard,Jon Ludlam,Anil Madhavapeddy*

Main category: cs.SE

TL;DR: HyperRes是一个基于超图的版本依赖解析系统，支持跨语言生态系统的依赖管理，无需用户更换现有包管理器。


<details>
  <summary>Details</summary>
Motivation: 解决多语言项目中依赖管理不互通的问题，以及外部系统和硬件依赖通常未版本化的现状。

Method: 定义HyperRes系统，通过超图描述版本化依赖解析，并将现有包管理器的元数据翻译为HyperRes格式。

Result: 证明依赖解析可以在当前独立的生态系统中跨平台工作，且支持精确适配特定部署环境。

Conclusion: HyperRes提供了一种跨生态系统的依赖管理解决方案，无需改变用户现有工具链。

Abstract: Package managers are everywhere, with seemingly every language and operating
system implementing their own solution. The lack of interoperability between
these systems means that multi-lingual projects are unable to express precise
dependencies across language ecosystems, and external system and hardware
dependencies are typically implicit and unversioned. We define HyperRes, a
formal system for describing versioned dependency resolution using a hypergraph
that is expressive enough to model many ecosystems and solve dependency
constraints across them. We define translations from dozens of existing package
managers to HyperRes and comprehensively demonstrate that dependency resolution
can work across ecosystems that are currently distinct. This does not require
users to shift their choice of package managers; instead, HyperRes allows for
the translation of packaging metadata between ecosystems, and for solving to be
precisely specialised to a particular deployment environment.

</details>


### [59] [Evaluating Large Language Models on Non-Code Software Engineering Tasks](https://arxiv.org/abs/2506.10833)
*Fabian C. Peña,Steffen Herbold*

Main category: cs.SE

TL;DR: 论文提出了首个全面评估大语言模型（LLMs）在非代码软件工程任务上的基准SELU，覆盖17种任务，并展示了中等规模解码器模型表现最佳。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在非代码软件工程任务中的有效性，填补现有研究的空白。

Method: 构建SELU基准，涵盖分类、回归、NER和MLM任务，对22个开源LLMs进行微调，并测试两个专有模型。

Result: 中等规模解码器模型表现最优，具有高平均性能和低任务间方差，代码预训练改进有限。

Conclusion: SELU为模型选择提供指导，并建议扩展至生成和设计导向场景。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
code understanding and generation; however, their effectiveness on non-code
Software Engineering (SE) tasks remains underexplored. We present the first
comprehensive benchmark, which we name `Software Engineering Language
Understanding' (SELU), for evaluating LLMs on 17 non-code tasks, spanning from
identifying whether a requirement is functional or non-functional to estimating
the effort and complexity of backlog items. SELU covers classification,
regression, Named Entity Recognition (NER), and Masked Language Modeling (MLM)
targets, with data drawn from diverse sources such as code repositories, issue
tracking systems, and developer forums. We fine-tune 22 open-source LLMs,
prompt two proprietary alternatives, and train two baselines. Performance is
measured using metrics such as F1-macro, SMAPE, F1-micro, and accuracy, and
compared via the Bayesian signed-rank test. Our results show that
moderate-scale decoder-only models consistently form a top-tier, exhibiting
high mean performance and low across-task variance, while domain adaptation via
code-focused pre-training might yield only modest improvements. These insights
guide model selection for non-code SE workflows and highlight directions for
expanding SELU to generative and design-oriented scenarios.

</details>


### [60] [MultiCoSim: A Python-based Multi-Fidelity Co-Simulation Framework](https://arxiv.org/abs/2506.10869)
*Quinn Thibeault,Giulia Pedrielli*

Main category: cs.SE

TL;DR: MultiCoSim是一个基于Python的仿真框架，支持分布式、组件化的协同仿真，解决了现有工具在灵活性、自动化和可移植性方面的不足。


<details>
  <summary>Details</summary>
Motivation: 随着CPS复杂性和规模的增加，现有仿真工具在配置、自动化和模块化方面的局限性阻碍了高效的系统开发和评估。

Method: MultiCoSim通过编程方式定义、组合和配置仿真组件，支持分布式协同仿真和组件的无缝替换与重构。

Result: 案例研究表明，MultiCoSim能够灵活地支持自定义控制器和现成平台的集成，简化CPS仿真流程。

Conclusion: MultiCoSim为CPS的研究和开发提供了一个高效、灵活的仿真解决方案。

Abstract: Simulation is a foundational tool for the analysis and testing of
cyber-physical systems (CPS), underpinning activities such as algorithm
development, runtime monitoring, and system verification. As CPS grow in
complexity and scale, particularly in safety-critical and learning-enabled
settings, accurate analysis and synthesis increasingly rely on the rapid use of
simulation experiments. Because CPS inherently integrate hardware, software,
and physical processes, simulation platforms must support co-simulation of
heterogeneous components at varying levels of fidelity. Despite recent advances
in high-fidelity modeling of hardware, firmware, and physics, co-simulation in
diverse environments remains challenging. These limitations hinder the
development of reusable benchmarks and impede the use of simulation for
automated and comparative evaluation.
  Existing simulation tools often rely on rigid configurations, lack automation
support, and present obstacles to portability and modularity. Many are
configured through static text files or impose constraints on how simulation
components are represented and connected, making it difficult to flexibly
compose systems or integrate components across platforms.
  To address these challenges, we introduce MultiCoSim, a Python-based
simulation framework that enables users to define, compose, and configure
simulation components programmatically. MultiCoSim supports distributed,
component-based co-simulation and allows seamless substitution and
reconfiguration of components. We demonstrate the flexibility of MultiCoSim
through case studies that include co-simulations involving custom
automaton-based controllers, as well as integration with off-the-shelf
platforms like the PX4 autopilot for aerial robotics. These examples highlight
MultiCoSim's capability to streamline CPS simulation pipelines for research and
development.

</details>


### [61] [SWE-Factory: Your Automated Factory for Issue Resolution Training Data and Evaluation Benchmarks](https://arxiv.org/abs/2506.10954)
*Lianghong Guo,Yanlin Wang,Caihua Li,Pengyu Yang,Jiachi Chen,Wei Tao,Yingtian Zou,Duyu Tang,Zibin Zheng*

Main category: cs.SE

TL;DR: SWE-Factory是一个自动化流水线，用于高效构建GitHub问题解决任务的大规模数据集，解决了传统方法中环境搭建、结果评分和验证的挑战。


<details>
  <summary>Details</summary>
Motivation: 构建大规模数据集对训练和评估LLMs的软件工程能力至关重要，但传统方法耗时且费力。

Method: SWE-Factory整合了三个自动化组件：SWE-Builder（多代理系统）、标准化退出码评分方法和自动化fail2pass验证。

Result: 实验显示，SWE-Factory能高效构建有效任务实例，评分准确率达100%，验证精度0.92，召回率1.00。

Conclusion: SWE-Factory为构建高质量数据集提供了高效自动化解决方案，有望加速相关研究和应用。

Abstract: Constructing large-scale datasets for the GitHub issue resolution task is
crucial for both training and evaluating the software engineering capabilities
of Large Language Models (LLMs). However, the traditional process for creating
such benchmarks is notoriously challenging and labor-intensive, particularly in
the stages of setting up evaluation environments, grading test outcomes, and
validating task instances. In this paper, we propose SWE-Factory, an automated
pipeline designed to address these challenges. To tackle these issues, our
pipeline integrates three core automated components. First, we introduce
SWE-Builder, a multi-agent system that automates evaluation environment
construction, which employs four specialized agents that work in a
collaborative, iterative loop and leverages an environment memory pool to
enhance efficiency. Second, we introduce a standardized, exit-code-based
grading method that eliminates the need for manually writing custom parsers.
Finally, we automate the fail2pass validation process using these reliable exit
code signals. Experiments on 671 issues across four programming languages show
that our pipeline can effectively construct valid task instances; for example,
with GPT-4.1-mini, our SWE-Builder constructs 269 valid instances at $0.045 per
instance, while with Gemini-2.5-flash, it achieves comparable performance at
the lowest cost of $0.024 per instance. We also demonstrate that our
exit-code-based grading achieves 100% accuracy compared to manual inspection,
and our automated fail2pass validation reaches a precision of 0.92 and a recall
of 1.00. We hope our automated pipeline will accelerate the collection of
large-scale, high-quality GitHub issue resolution datasets for both training
and evaluation. Our code and datasets are released at
https://github.com/DeepSoftwareAnalytics/swe-factory.

</details>
