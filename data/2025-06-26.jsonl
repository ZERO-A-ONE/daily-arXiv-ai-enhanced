{"id": "2506.19870", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.19870", "abs": "https://arxiv.org/abs/2506.19870", "authors": ["Md Asif Ul Hoq Khan", "MD Zahedul Islam", "Istiaq Ahmed", "Md Masud Karim Rabbi", "Farhana Rahman Anonna", "MD Abdul Fahim Zeeshan", "Mehedi Hasan Ridoy", "Bivash Ranjan Chowdhury", "Md Nazmul Shakir Rabbi", "GM Alamin Sadnan"], "title": "Secure Energy Transactions Using Blockchain Leveraging AI for Fraud Detection and Energy Market Stability", "comment": null, "summary": "Peer-to-peer trading and the move to decentralized grids have reshaped the\nenergy markets in the United States. Notwithstanding, such developments lead to\nnew challenges, mainly regarding the safety and authenticity of energy trade.\nThis study aimed to develop and build a secure, intelligent, and efficient\nenergy transaction system for the decentralized US energy market. This research\ninterlinks the technological prowess of blockchain and artificial intelligence\n(AI) in a novel way to solve long-standing challenges in the distributed energy\nmarket, specifically those of security, fraudulent behavior detection, and\nmarket reliability. The dataset for this research is comprised of more than 1.2\nmillion anonymized energy transaction records from a simulated peer-to-peer\n(P2P) energy exchange network emulating real-life blockchain-based American\nmicrogrids, including those tested by LO3 Energy and Grid+ Labs. Each record\ncontains detailed fields of transaction identifier, timestamp, energy volume\n(kWh), transaction type (buy/sell), unit price, prosumer/consumer identifier\n(hashed for privacy), smart meter readings, geolocation regions, and settlement\nconfirmation status. The dataset also includes system-calculated behavior\nmetrics of transaction rate, variability of energy production, and historical\npricing patterns. The system architecture proposed involves the integration of\ntwo layers, namely a blockchain layer and artificial intelligence (AI) layer,\neach playing a unique but complementary function in energy transaction securing\nand market intelligence improvement. The machine learning models used in this\nresearch were specifically chosen for their established high performance in\nclassification tasks, specifically in the identification of energy transaction\nfraud in decentralized markets."}
{"id": "2506.19871", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.19871", "abs": "https://arxiv.org/abs/2506.19871", "authors": ["Yining Pang", "Chenghan Li"], "title": "An Attack Method for Medical Insurance Claim Fraud Detection based on Generative Adversarial Network", "comment": "arXiv admin note: text overlap with arXiv:2405.12076 by other authors", "summary": "Insurance fraud detection represents a pivotal advancement in modern\ninsurance service, providing intelligent and digitalized monitoring to enhance\nmanagement and prevent fraud. It is crucial for ensuring the security and\nefficiency of insurance systems. Although AI and machine learning algorithms\nhave demonstrated strong performance in detecting fraudulent claims, the\nabsence of standardized defense mechanisms renders current systems vulnerable\nto emerging adversarial threats. In this paper, we propose a GAN-based approach\nto conduct adversarial attacks on fraud detection systems. Our results indicate\nthat an attacker, without knowledge of the training data or internal model\ndetails, can generate fraudulent cases that are classified as legitimate with a\n99\\% attack success rate (ASR). By subtly modifying real insurance records and\nclaims, adversaries can significantly increase the fraud risk, potentially\nbypassing compromised detection systems. These findings underscore the urgent\nneed to enhance the robustness of insurance fraud detection models against\nadversarial manipulation, thereby ensuring the stability and reliability of\ndifferent insurance systems."}
{"id": "2506.19874", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.19874", "abs": "https://arxiv.org/abs/2506.19874", "authors": ["Xing Yang", "Bingtao Wang", "Yuhao Wang", "Zimo Ji", "Terry Jingchen Zhang", "Wenyuan Jiang"], "title": "Towards Provable (In)Secure Model Weight Release Schemes", "comment": "8 pages, 2 figures", "summary": "Recent secure weight release schemes claim to enable open-source model\ndistribution while protecting model ownership and preventing misuse. However,\nthese approaches lack rigorous security foundations and provide only informal\nsecurity guarantees. Inspired by established works in cryptography, we\nformalize the security of weight release schemes by introducing several\nconcrete security definitions. We then demonstrate our definition's utility\nthrough a case study of TaylorMLP, a prominent secure weight release scheme.\nOur analysis reveals vulnerabilities that allow parameter extraction thus\nshowing that TaylorMLP fails to achieve its informal security goals. We hope\nthis work will advocate for rigorous research at the intersection of machine\nlearning and security communities and provide a blueprint for how future weight\nrelease schemes should be designed and evaluated."}
{"id": "2506.19877", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.19877", "abs": "https://arxiv.org/abs/2506.19877", "authors": ["Zhaoyang Xu", "Yunbo Liu"], "title": "Robust Anomaly Detection in Network Traffic: Evaluating Machine Learning Models on CICIDS2017", "comment": "submitted to IEEE CNS 2025", "summary": "Identifying suitable machine learning paradigms for intrusion detection\nremains critical for building effective and generalizable security solutions.\nIn this study, we present a controlled comparison of four representative models\n- Multi-Layer Perceptron (MLP), 1D Convolutional Neural Network (CNN),\nOne-Class Support Vector Machine (OCSVM) and Local Outlier Factor (LOF) - on\nthe CICIDS2017 dataset under two scenarios: detecting known attack types and\ngeneralizing to previously unseen threats. Our results show that supervised MLP\nand CNN achieve near-perfect accuracy on familiar attacks but suffer drastic\nrecall drops on novel attacks. Unsupervised LOF attains moderate overall\naccuracy and high recall on unknown threats at the cost of elevated false\nalarms, while boundary-based OCSVM balances precision and recall best,\ndemonstrating robust detection across both scenarios. These findings offer\npractical guidance for selecting IDS models in dynamic network environments."}
{"id": "2506.19923", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.19923", "abs": "https://arxiv.org/abs/2506.19923", "authors": ["Kaito Baba", "Chaoran Liu", "Shuhei Kurita", "Akiyoshi Sannai"], "title": "Prover Agent: An Agent-based Framework for Formal Mathematical Proofs", "comment": "22 pages, 2 figures", "summary": "We present Prover Agent, a novel AI agent for automated theorem proving that\nintegrates large language models (LLMs) with a formal proof assistant, Lean.\nProver Agent coordinates an informal reasoning LLM, a formal prover model, and\nfeedback from Lean while also generating auxiliary lemmas to assist in\ndiscovering the overall proof strategy. It achieves an 86.1% success rate on\nthe MiniF2F benchmark, establishing a new state-of-the-art among methods using\nsmall language models (SLMs) with a much lower sample budget than previous\napproaches. We also present case studies illustrating how these generated\nlemmas contribute to solving challenging problems."}
{"id": "2506.19897", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.19897", "abs": "https://arxiv.org/abs/2506.19897", "authors": ["Christopher Glasz", "Emily Escamilla", "Eric O. Scott", "Anand Patel", "Jacob Zimmer", "Colin Diggs", "Michael Doyle", "Scott Rosen", "Nitin Naik", "Justin F. Brunelle", "Samruddhi Thaker", "Parthav Poudel", "Arun Sridharan", "Amit Madan", "Doug Wendt", "William Macke", "Thomas Schill"], "title": "Can LLMs Replace Humans During Code Chunking?", "comment": null, "summary": "Large language models (LLMs) have become essential tools in computer science,\nespecially for tasks involving code understanding and generation. However,\nexisting work does not address many of the unique challenges presented by code\nwritten for government applications. In particular, government enterprise\nsoftware is often written in legacy languages like MUMPS or assembly language\ncode (ALC) and the overall token lengths of these systems exceed the context\nwindow size for current commercially available LLMs. Additionally, LLMs are\nprimarily trained on modern software languages and have undergone limited\ntesting with legacy languages, making their ability to understand legacy\nlanguages unknown and, hence, an area for empirical study. This paper examines\nthe application of LLMs in the modernization of legacy government code written\nin ALC and MUMPS, addressing the challenges of input limitations. We\ninvestigate various code-chunking methods to optimize the generation of summary\nmodule comments for legacy code files, evaluating the impact of code-chunking\nmethods on the quality of documentation produced by different LLMs, including\nGPT-4o, Claude 3 Sonnet, Mixtral, and Llama 3. Our results indicate that LLMs\ncan select partition points closely aligned with human expert partitioning. We\nalso find that chunking approaches have significant impact on downstream tasks\nsuch as documentation generation. LLM-created partitions produce comments that\nare up to 20% more factual and up to 10% more useful than when humans create\npartitions. Therefore, we conclude that LLMs can be used as suitable\nreplacements for human partitioning of large codebases during LLM-aided\nmodernization."}
{"id": "2506.19881", "categories": ["cs.CR", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.19881", "abs": "https://arxiv.org/abs/2506.19881", "authors": ["Aloni Cohen"], "title": "Blameless Users in a Clean Room: Defining Copyright Protection for Generative Models", "comment": null, "summary": "Are there any conditions under which a generative model's outputs are\nguaranteed not to infringe the copyrights of its training data? This is the\nquestion of \"provable copyright protection\" first posed by Vyas, Kakade, and\nBarak (ICML 2023). They define near access-freeness (NAF) and propose it as\nsufficient for protection. This paper revisits the question and establishes new\nfoundations for provable copyright protection -- foundations that are firmer\nboth technically and legally. First, we show that NAF alone does not prevent\ninfringement. In fact, NAF models can enable verbatim copying, a blatant\nfailure of copy protection that we dub being tainted. Then, we introduce our\nblameless copy protection framework for defining meaningful guarantees, and\ninstantiate it with clean-room copy protection. Clean-room copy protection\nallows a user to control their risk of copying by behaving in a way that is\nunlikely to copy in a counterfactual clean-room setting. Finally, we formalize\na common intuition about differential privacy and copyright by proving that DP\nimplies clean-room copy protection when the dataset is golden, a copyright\ndeduplication requirement."}
{"id": "2506.19977", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.19977", "abs": "https://arxiv.org/abs/2506.19977", "authors": ["Deng Pan", "Keerthiram Murugesan", "Nuno Moniz", "Nitesh Chawla"], "title": "Context Attribution with Multi-Armed Bandit Optimization", "comment": null, "summary": "Understanding which parts of the retrieved context contribute to a large\nlanguage model's generated answer is essential for building interpretable and\ntrustworthy generative QA systems. We propose a novel framework that formulates\ncontext attribution as a combinatorial multi-armed bandit (CMAB) problem. Each\ncontext segment is treated as a bandit arm, and we employ Combinatorial\nThompson Sampling (CTS) to efficiently explore the exponentially large space of\ncontext subsets under a limited query budget. Our method defines a reward\nfunction based on normalized token likelihoods, capturing how well a subset of\nsegments supports the original model response. Unlike traditional\nperturbation-based attribution methods such as SHAP, which sample subsets\nuniformly and incur high computational costs, our approach adaptively balances\nexploration and exploitation by leveraging posterior estimates of segment\nrelevance. This leads to substantially improved query efficiency while\nmaintaining high attribution fidelity. Extensive experiments on diverse\ndatasets and LLMs demonstrate that our method achieves competitive attribution\nquality with fewer model queries."}
{"id": "2506.20063", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.20063", "abs": "https://arxiv.org/abs/2506.20063", "authors": ["Zixuan Feng", "Thomas Zimmermann", "Lorenzo Pisani", "Christopher Gooley", "Jeremiah Wander", "Anita Sarma"], "title": "When Domains Collide: An Activity Theory Exploration of Cross-Disciplinary Collaboration", "comment": "Cross-disciplinary Collaboration, Activity Theory, Mixed-Methods", "summary": "Background: Software development teams are increasingly diverse, embedded,\nand cross-disciplinary. Domain experts (DEs) from different disciplines\ncollaborate with professional software developers (SDEs), bringing\ncomplementary expertise in creating and maintaining complex production\nsoftware. However, contested expectations, divergent problem-solving\nperspectives, and conflicting priorities lead to friction. Aims: This study\naims to investigate the dynamics of emerging collaboration of\ncross-disciplinary software development (CDSD) by exploring the expectations\nheld by DEs and SDEs and understanding how these frictions manifest in\npractice. Method: We utilize Activity Theory (AT), a well-established\nsocio-technical framework, as an analytical lens in a grounded, empirical\ninvestigation, conducted through a mixed-method study involving 24 interviews\n(12 DEs and 12 SDEs) and a large-scale validation survey with 293 participants\n(161 DEs and 132 SDEs). Results: We conceptualize and empirically ground the\nCDSD dynamics. We identified eight expectations held by SDEs and six by DEs. By\nmapping these expectations to AT components, we revealed 21 frictions in CDSD\nand illustrated where and how they arise. Conclusions: This study offers a\ntheoretical lens for understanding the dynamics and frictions in CDSD and\nprovides actionable insights for future research, practitioners, and\ninfrastructure design."}
{"id": "2506.19886", "categories": ["cs.CR", "cs.IT", "cs.LG", "math.IT"], "pdf": "https://arxiv.org/pdf/2506.19886", "abs": "https://arxiv.org/abs/2506.19886", "authors": ["Xuesong Wang", "Mo Li", "Xingyan Shi", "Zhaoqian Liu", "Shenghao Yang"], "title": "Diffusion-based Task-oriented Semantic Communications with Model Inversion Attack", "comment": null, "summary": "Semantic communication has emerged as a promising neural network-based system\ndesign for 6G networks. Task-oriented semantic communication is a novel\nparadigm whose core goal is to efficiently complete specific tasks by\ntransmitting semantic information, optimizing communication efficiency and task\nperformance. The key challenge lies in preserving privacy while maintaining\ntask accuracy, as this scenario is susceptible to model inversion attacks. In\nsuch attacks, adversaries can restore or even reconstruct input data by\nanalyzing and processing model outputs, owing to the neural network-based\nnature of the systems. In addition, traditional systems use image quality\nindicators (such as PSNR or SSIM) to assess attack severity, which may be\ninadequate for task-oriented semantic communication, since visual differences\ndo not necessarily ensure semantic divergence. In this paper, we propose a\ndiffusion-based semantic communication framework, named DiffSem, that optimizes\nsemantic information reconstruction through a diffusion mechanism with\nself-referential label embedding to significantly improve task performance. Our\nmodel also compensates channel noise and adopt semantic information distortion\nto ensure the robustness of the system in various signal-to-noise ratio\nenvironments. To evaluate the attacker's effectiveness, we propose a new metric\nthat better quantifies the semantic fidelity of estimations from the adversary.\nExperimental results based on this criterion show that on the MNIST dataset,\nDiffSem improves the classification accuracy by 10.03%, and maintain stable\nperformance under dynamic channels. Our results further demonstrate that\nsignificant deviation exists between traditional image quality indicators and\nthe leakage of task-relevant semantic information."}
{"id": "2506.20008", "categories": ["cs.AI", "cs.PL", "cs.SE", "68T50, 81P68, 68T07, 68T20", "I.2.7; I.2.2"], "pdf": "https://arxiv.org/pdf/2506.20008", "abs": "https://arxiv.org/abs/2506.20008", "authors": ["Abdul Basit", "Minghao Shao", "Haider Asif", "Nouhaila Innan", "Muhammad Kashif", "Alberto Marchisio", "Muhammad Shafique"], "title": "QHackBench: Benchmarking Large Language Models for Quantum Code Generation Using PennyLane Hackathon Challenges", "comment": "8 pages, 6 figures, 3 tables, submitted to QAI 2025", "summary": "Recent advances in Large Language Models (LLMs) have demonstrated strong\npotential in code generation, yet their effectiveness in quantum computing\nremains underexplored. This paper benchmarks LLMs for PennyLane-based quantum\ncode generation using real-world challenges from the Quantum Hackathon (QHack).\nWe introduce QHackBench, a novel benchmark dataset derived from QHack\ncompetitions, and evaluate model performance under vanilla prompting and\nRetrieval-Augmented Generation (RAG). Our structured evaluation framework\nassesses functional correctness, syntactic validity, and execution success\nacross varying challenge difficulties. Results indicate that RAG-enhanced\nmodels, supplemented with an augmented PennyLane dataset, approximately\ngenerate similar results as the standard prompting, particularly in complex\nquantum algorithms. Additionally, we introduce a multi-agent evaluation\npipeline that iteratively refines incorrect solutions, further enhancing\nexecution success rates. To foster further research, we commit to publicly\nreleasing QHackBench, along with our evaluation framework and experimental\nresults, enabling continued advancements in AI-assisted quantum programming."}
{"id": "2506.20159", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.20159", "abs": "https://arxiv.org/abs/2506.20159", "authors": ["Tomas Herda", "Victoria Pichler", "Zheying Zhang", "Pekka Abrahamsson", "Geir K. Hanssen"], "title": "AI and Agile Software Development: From Frustration to Success -- XP2025 Workshop Summary", "comment": null, "summary": "The full-day workshop on AI and Agile at XP 2025 convened a diverse group of\nresearchers and industry practitioners to address the practical challenges and\nopportunities of integrating Artificial Intelligence into Agile software\ndevelopment. Through interactive sessions, participants identified shared\nfrustrations related to integrating AI into Agile Software Development\npractices, including challenges with tooling, governance, data quality, and\ncritical skill gaps. These challenges were systematically prioritized and\nanalyzed to uncover root causes. The workshop culminated in the collaborative\ndevelopment of a research roadmap that pinpoints actionable directions for\nfuture work, including both immediate solutions and ambitious long-term goals.\nThe key outcome is a structured agenda designed to foster joint\nindustry-academic efforts to move from identified frustrations to successful\nimplementation."}
{"id": "2506.19889", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.19889", "abs": "https://arxiv.org/abs/2506.19889", "authors": ["Wanli Peng", "Xin Chen", "Hang Fu", "XinYu He", "Xue Yiming", "Juan Wen"], "title": "Retrieval-Confused Generation is a Good Defender for Privacy Violation Attack of Large Language Models", "comment": null, "summary": "Recent advances in large language models (LLMs) have made a profound impact\non our society and also raised new security concerns. Particularly, due to the\nremarkable inference ability of LLMs, the privacy violation attack (PVA),\nrevealed by Staab et al., introduces serious personal privacy issues. Existing\ndefense methods mainly leverage LLMs to anonymize the input query, which\nrequires costly inference time and cannot gain satisfactory defense\nperformance. Moreover, directly rejecting the PVA query seems like an effective\ndefense method, while the defense method is exposed, promoting the evolution of\nPVA. In this paper, we propose a novel defense paradigm based on\nretrieval-confused generation (RCG) of LLMs, which can efficiently and covertly\ndefend the PVA. We first design a paraphrasing prompt to induce the LLM to\nrewrite the \"user comments\" of the attack query to construct a disturbed\ndatabase. Then, we propose the most irrelevant retrieval strategy to retrieve\nthe desired user data from the disturbed database. Finally, the \"data comments\"\nare replaced with the retrieved user data to form a defended query, leading to\nresponding to the adversary with some wrong personal attributes, i.e., the\nattack fails. Extensive experiments are conducted on two datasets and eight\npopular LLMs to comprehensively evaluate the feasibility and the superiority of\nthe proposed defense method."}
{"id": "2506.20009", "categories": ["cs.AI", "cs.CL", "I.2.7"], "pdf": "https://arxiv.org/pdf/2506.20009", "abs": "https://arxiv.org/abs/2506.20009", "authors": ["Konstantinos Vrettos", "Michail E. Klontzas"], "title": "Accurate and Energy Efficient: Local Retrieval-Augmented Generation Models Outperform Commercial Large Language Models in Medical Tasks", "comment": "18 pages, 3 Figures", "summary": "Background The increasing adoption of Artificial Intelligence (AI) in\nhealthcare has sparked growing concerns about its environmental and ethical\nimplications. Commercial Large Language Models (LLMs), such as ChatGPT and\nDeepSeek, require substantial resources, while the utilization of these systems\nfor medical purposes raises critical issues regarding patient privacy and\nsafety. Methods We developed a customizable Retrieval-Augmented Generation\n(RAG) framework for medical tasks, which monitors its energy usage and CO2\nemissions. This system was then used to create RAGs based on various\nopen-source LLMs. The tested models included both general purpose models like\nllama3.1:8b and medgemma-4b-it, which is medical-domain specific. The best RAGs\nperformance and energy consumption was compared to DeepSeekV3-R1 and OpenAIs\no4-mini model. A dataset of medical questions was used for the evaluation.\nResults Custom RAG models outperformed commercial models in accuracy and energy\nconsumption. The RAG model built on llama3.1:8B achieved the highest accuracy\n(58.5%) and was significantly better than other models, including o4-mini and\nDeepSeekV3-R1. The llama3.1-RAG also exhibited the lowest energy consumption\nand CO2 footprint among all models, with a Performance per kWh of 0.52 and a\ntotal CO2 emission of 473g. Compared to o4-mini, the llama3.1-RAG achieved 2.7x\ntimes more accuracy points per kWh and 172% less electricity usage while\nmaintaining higher accuracy. Conclusion Our study demonstrates that local LLMs\ncan be leveraged to develop RAGs that outperform commercial, online LLMs in\nmedical tasks, while having a smaller environmental impact. Our modular\nframework promotes sustainable AI development, reducing electricity usage and\naligning with the UNs Sustainable Development Goals."}
{"id": "2506.20217", "categories": ["cs.SE", "cs.CE", "cs.CY", "68-01", "K.6.3"], "pdf": "https://arxiv.org/pdf/2506.20217", "abs": "https://arxiv.org/abs/2506.20217", "authors": ["Stuart M. Allen", "Neil Chue Hong", "Stephan Druskat", "Toby Hodges", "Daniel S. Katz", "Jan Linxweiler", "Frank Löffler", "Lars Grunske", "Heidi Seibold", "Jan Philipp Thiele", "Samantha Wittke"], "title": "Ten simple rules for PIs to integrate Research Software Engineering into their research group", "comment": "10 pages, submitted to PLOS Computational Biology", "summary": "Research Software Engineering (RSEng) is a key success factor in producing\nhigh-quality research software, which in turn enables and improves research\noutcomes. However, as a principal investigator or leader of a research group\nyou may not know what RSEng is, where to get started with it, or how to use it\nto maximize its benefit for your research. RSEng also often comes with\ntechnical complexity, and therefore reduced accessibility to some researchers.\nThe ten simple rules presented in this paper aim to improve the accessibility\nof RSEng, and provide practical and actionable advice to PIs and leaders for\nintegrating RSEng into their research group. By following these rules, readers\ncan improve the quality, reproducibility, and trustworthiness of their research\nsoftware, ultimately leading to better, more reproducible and more trustworthy\nresearch outcomes."}
{"id": "2506.19892", "categories": ["cs.CR", "cs.AI", "cs.DC", "cs.LG", "cs.PF"], "pdf": "https://arxiv.org/pdf/2506.19892", "abs": "https://arxiv.org/abs/2506.19892", "authors": ["Isaac Marroqui Penalva", "Enrique Tomás Martínez Beltrán", "Manuel Gil Pérez", "Alberto Huertas Celdrán"], "title": "RepuNet: A Reputation System for Mitigating Malicious Clients in DFL", "comment": null, "summary": "Decentralized Federated Learning (DFL) enables nodes to collaboratively train\nmodels without a central server, introducing new vulnerabilities since each\nnode independently selects peers for model aggregation. Malicious nodes may\nexploit this autonomy by sending corrupted models (model poisoning), delaying\nmodel submissions (delay attack), or flooding the network with excessive\nmessages, negatively affecting system performance. Existing solutions often\ndepend on rigid configurations or additional infrastructures such as\nblockchain, leading to computational overhead, scalability issues, or limited\nadaptability. To overcome these limitations, this paper proposes RepuNet, a\ndecentralized reputation system that categorizes threats in DFL and dynamically\nevaluates node behavior using metrics like model similarity, parameter changes,\nmessage latency, and communication volume. Nodes' influence in model\naggregation is adjusted based on their reputation scores. RepuNet was\nintegrated into the Nebula DFL platform and experimentally evaluated with MNIST\nand CIFAR-10 datasets under non-IID distributions, using federations of up to\n25 nodes in both fully connected and random topologies. Different attack\nintensities, frequencies, and activation intervals were tested. Results\ndemonstrated that RepuNet effectively detects and mitigates malicious behavior,\nachieving F1 scores above 95% for MNIST scenarios and approximately 76% for\nCIFAR-10 cases. These outcomes highlight RepuNet's adaptability, robustness,\nand practical potential for mitigating threats in decentralized federated\nlearning environments."}
{"id": "2506.20018", "categories": ["cs.AI", "cs.AR"], "pdf": "https://arxiv.org/pdf/2506.20018", "abs": "https://arxiv.org/abs/2506.20018", "authors": ["Zechun Deng", "Ziwei Liu", "Ziqian Bi", "Junhao Song", "Chia Xin Liang", "Joe Yeong", "Junfeng Hao"], "title": "Achieving Trustworthy Real-Time Decision Support Systems with Low-Latency Interpretable AI Models", "comment": null, "summary": "This paper investigates real-time decision support systems that leverage\nlow-latency AI models, bringing together recent progress in holistic AI-driven\ndecision tools, integration with Edge-IoT technologies, and approaches for\neffective human-AI teamwork. It looks into how large language models can assist\ndecision-making, especially when resources are limited. The research also\nexamines the effects of technical developments such as DeLLMa, methods for\ncompressing models, and improvements for analytics on edge devices, while also\naddressing issues like limited resources and the need for adaptable frameworks.\nThrough a detailed review, the paper offers practical perspectives on\ndevelopment strategies and areas of application, adding to the field by\npointing out opportunities for more efficient and flexible AI-supported\nsystems. The conclusions set the stage for future breakthroughs in this\nfast-changing area, highlighting how AI can reshape real-time decision support."}
{"id": "2506.20435", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.20435", "abs": "https://arxiv.org/abs/2506.20435", "authors": ["Mennatullah T. Khedr", "John S. Fitzgerald"], "title": "The Composition of Digital Twins for Systems-of-Systems: a Systematic Literature Review", "comment": "15 pages, 3 figures, Presented at the 23rd Overture workshop, June\n  2025 (arXiv:cs/2506.08680)", "summary": "Digital Twins (DTs) are increasingly used to model complex systems,\nespecially in Cyber-Physical Systems (CPS) and System-of-Systems (SoS), where\neffective integration is key. This systematic literature review investigates DT\ncomposition and verification and validation (V&V) methodologies. Analyzing 21\nstudies from 2022-2024, we examined composition mechanisms, SoS\ncharacteristics, and V&V formality, scope, and challenges. While composition is\ndiscussed, formalization is limited. V&V approaches vary, with semi-formal\nmethods and simulations dominating; formal verification is underutilized. Key\ntechnical challenges include model uncertainty and integration complexity.\nMethodological challenges highlight the lack of standardized DT-specific V&V\nframeworks. There is a need to move beyond model validation to address\nintegration and cyber-physical consistency. This review contributes a\nstructured classification of V&V approaches and emphasizes the need for\nstandardized, scalable V&V and rigorous composition methodologies for complex\nDT implementations."}
{"id": "2506.19899", "categories": ["cs.CR", "cs.HC"], "pdf": "https://arxiv.org/pdf/2506.19899", "abs": "https://arxiv.org/abs/2506.19899", "authors": ["Andrew T. Rozema", "James C. Davis"], "title": "Anti-Phishing Training Does Not Work: A Large-Scale Empirical Assessment of Multi-Modal Training Grounded in the NIST Phish Scale", "comment": "13 pages, 5 apdx", "summary": "Social engineering attacks using email, commonly known as phishing, are a\ncritical cybersecurity threat. Phishing attacks often lead to operational\nincidents and data breaches. As a result, many organizations allocate a\nsubstantial portion of their cybersecurity budgets to phishing awareness\ntraining, driven in part by compliance requirements. However, the effectiveness\nof this training remains in dispute. Empirical evidence of training\n(in)effectiveness is essential for evidence-based cybersecurity investment and\npolicy development. Despite recent measurement studies, two critical gaps\nremain in the literature:\n  (1) we lack a validated measure of phishing lure difficulty, and\n  (2) there are few comparisons of different types of training in real-world\nbusiness settings.\n  To fill these gaps, we conducted a large-scale study ($N = 12{,}511$) of\nphishing effectiveness at a US-based financial technology (``fintech'') firm.\nOur two-factor design compared the effect of treatments (lecture-based,\ninteractive, and control groups) on subjects' susceptibility to phishing lures\nof varying complexity (using the NIST Phish Scale). The NIST Phish Scale\nsuccessfully predicted behavior (click rates: 7.0\\% easy to 15.0\\% hard emails,\np $<$ 0.001), but training showed no significant main effects on clicks (p =\n0.450) or reporting (p = 0.417). Effect sizes remained below 0.01, indicating\nlittle practical value in any of the phishing trainings we deployed. Our\nresults add to the growing evidence that phishing training is ineffective,\nreinforcing the importance of phishing defense-in-depth and the merit of\nchanges to processes and technology to reduce reliance on humans, as well as\nrebuking the training costs necessitated by regulatory requirements."}
{"id": "2506.20020", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.20020", "abs": "https://arxiv.org/abs/2506.20020", "authors": ["Saloni Dash", "Amélie Reymond", "Emma S. Spiro", "Aylin Caliskan"], "title": "Persona-Assigned Large Language Models Exhibit Human-Like Motivated Reasoning", "comment": null, "summary": "Reasoning in humans is prone to biases due to underlying motivations like\nidentity protection, that undermine rational decision-making and judgment. This\nmotivated reasoning at a collective level can be detrimental to society when\ndebating critical issues such as human-driven climate change or vaccine safety,\nand can further aggravate political polarization. Prior studies have reported\nthat large language models (LLMs) are also susceptible to human-like cognitive\nbiases, however, the extent to which LLMs selectively reason toward\nidentity-congruent conclusions remains largely unexplored. Here, we investigate\nwhether assigning 8 personas across 4 political and socio-demographic\nattributes induces motivated reasoning in LLMs. Testing 8 LLMs (open source and\nproprietary) across two reasoning tasks from human-subject studies -- veracity\ndiscernment of misinformation headlines and evaluation of numeric scientific\nevidence -- we find that persona-assigned LLMs have up to 9% reduced veracity\ndiscernment relative to models without personas. Political personas\nspecifically, are up to 90% more likely to correctly evaluate scientific\nevidence on gun control when the ground truth is congruent with their induced\npolitical identity. Prompt-based debiasing methods are largely ineffective at\nmitigating these effects. Taken together, our empirical findings are the first\nto suggest that persona-assigned LLMs exhibit human-like motivated reasoning\nthat is hard to mitigate through conventional debiasing prompts -- raising\nconcerns of exacerbating identity-congruent reasoning in both LLMs and humans."}
{"id": "2506.20444", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.20444", "abs": "https://arxiv.org/abs/2506.20444", "authors": ["Xiang Lan", "Tim Menzies", "Bowen Xu"], "title": "Smart Cuts: Enhance Active Learning for Vulnerability Detection by Pruning Bad Seeds", "comment": null, "summary": "Vulnerability detection is crucial for identifying security weaknesses in\nsoftware systems. However, the effectiveness of machine learning models in this\ndomain is often hindered by low-quality training datasets, which contain noisy,\nmislabeled, or imbalanced samples. This paper proposes a novel dataset\nmaps-empowered approach that systematically identifies and mitigates\nhard-to-learn outliers, referred to as \"bad seeds\", to improve model training\nefficiency. Our approach can categorize training examples based on learning\ndifficulty and integrate this information into an active learning framework.\nUnlike traditional methods that focus on uncertainty-based sampling, our\nstrategy prioritizes dataset quality by filtering out performance-harmful\nsamples while emphasizing informative ones. Our experimental results show that\nour approach can improve F1 score over random selection by 45.36% (DeepGini)\nand 45.91% (K-Means) and outperforms standard active learning by 61.46%\n(DeepGini) and 32.65% (K-Means) for CodeBERT on the Big-Vul dataset,\ndemonstrating the effectiveness of integrating dataset maps for optimizing\nsample selection in vulnerability detection. Furthermore, our approach also\nenhances model robustness, improves sample selection by filtering bad seeds,\nand stabilizes active learning performance across iterations. By analyzing the\ncharacteristics of these outliers, we provide insights for future improvements\nin dataset construction, making vulnerability detection more reliable and\ncost-effective."}
{"id": "2506.19934", "categories": ["cs.CR", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.19934", "abs": "https://arxiv.org/abs/2506.19934", "authors": ["Maryam Mahdi Al-Husseini"], "title": "A Hybrid Intrusion Detection System with a New Approach to Protect the Cybersecurity of Cloud Computing", "comment": "1. Acknowledgment for: Supervisor: Prof. Dr. Alireza Rouhi Advisor:\n  Prof. Dr. Einollah Pira 2. Thesis of MSc. degree for Azarbaijan Shahid Madani\n  University Faculty of Information Technology and Computer Engineering 3.\n  Number of pages: 103 4. Number of Figures: 66", "summary": "Cybersecurity is one of the foremost challenges facing the world of cloud\ncomputing. Recently, the widespread adoption of smart devices in cloud\ncomputing environments that provide Internet-based services has become\nprevalent. Therefore, it is essential to consider the security threats in these\nenvironments. The use of intrusion detection systems can mitigate the\nvulnerabilities of these systems. Furthermore, hybrid intrusion detection\nsystems can provide better protection compared to conventional intrusion\ndetection systems. These systems manage issues related to complexity,\ndimensionality, and performance. This research aims to propose a Hybrid\nIntrusion Detection System (HyIDS) that identifies and mitigates initial\nthreats. The main innovation of this research is the introduction of a new\nmethod for hybrid intrusion detection systems (HyIDS). For this purpose, an\nEnergy-Valley Optimizer (EVO) is used to select an optimal feature set, which\nis then classified using supervised machine learning models. The proposed\napproach is evaluated using the CIC_DDoS2019, CSE_CIC_DDoS2018, and NSL-KDD\ndatasets. For evaluation and testing, the proposed system has been run for a\ntotal of 32 times. The results of the proposed approach are compared with the\nGrey Wolf Optimizer (GWO). With the CIC_DDoS2019 dataset, the D_TreeEVO model\nachieves an accuracy of 99.13% and a detection rate of 98.941%. Furthermore,\nthis result reaches 99.78% for the CSE_CIC_DDoS2018 dataset. In comparison to\nNSL-KDD, it has an accuracy of 99.50% and a detection rate (DT) of 99.48%. For\nfeature selection, EVO outperforms GWO. The results of this research indicate\nthat EVO yields better results as an optimizer for HyIDS performance."}
{"id": "2506.20059", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.20059", "abs": "https://arxiv.org/abs/2506.20059", "authors": ["Weijieying Ren", "Tianxiang Zhao", "Lei Wang", "Tianchun Wang", "Vasant Honavar"], "title": "DiaLLMs: EHR Enhanced Clinical Conversational System for Clinical Test Recommendation and Diagnosis Prediction", "comment": null, "summary": "Recent advances in Large Language Models (LLMs) have led to remarkable\nprogresses in medical consultation. However, existing medical LLMs overlook the\nessential role of Electronic Health Records (EHR) and focus primarily on\ndiagnosis recommendation, limiting their clinical applicability. We propose\nDiaLLM, the first medical LLM that integrates heterogeneous EHR data into\nclinically grounded dialogues, enabling clinical test recommendation, result\ninterpretation, and diagnosis prediction to better align with real-world\nmedical practice. To construct clinically grounded dialogues from EHR, we\ndesign a Clinical Test Reference (CTR) strategy that maps each clinical code to\nits corresponding description and classifies test results as \"normal\" or\n\"abnormal\". Additionally, DiaLLM employs a reinforcement learning framework for\nevidence acquisition and automated diagnosis. To handle the large action space,\nwe introduce a reject sampling strategy to reduce redundancy and improve\nexploration efficiency. Furthermore, a confirmation reward and a\nclass-sensitive diagnosis reward are designed to guide accurate diagnosis\nprediction. Extensive experimental results demonstrate that DiaLLM outperforms\nbaselines in clinical test recommendation and diagnosis prediction."}
{"id": "2506.20551", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.20551", "abs": "https://arxiv.org/abs/2506.20551", "authors": ["Soumya Madireddy", "Lu Gao", "Zia Din", "Kinam Kim", "Ahmed Senouci", "Zhe Han", "Yunpeng Zhang"], "title": "Large Language Model-Driven Code Compliance Checking in Building Information Modeling", "comment": null, "summary": "This research addresses the time-consuming and error-prone nature of manual\ncode compliance checking in Building Information Modeling (BIM) by introducing\na Large Language Model (LLM)-driven approach to semi-automate this critical\nprocess. The developed system integrates LLMs such as GPT, Claude, Gemini, and\nLlama, with Revit software to interpret building codes, generate Python\nscripts, and perform semi-automated compliance checks within the BIM\nenvironment. Case studies on a single-family residential project and an office\nbuilding project demonstrated the system's ability to reduce the time and\neffort required for compliance checks while improving accuracy. It streamlined\nthe identification of violations, such as non-compliant room dimensions,\nmaterial usage, and object placements, by automatically assessing relationships\nand generating actionable reports. Compared to manual methods, the system\neliminated repetitive tasks, simplified complex regulations, and ensured\nreliable adherence to standards. By offering a comprehensive, adaptable, and\ncost-effective solution, this proposed approach offers a promising advancement\nin BIM-based compliance checking, with potential applications across diverse\nregulatory documents in construction projects."}
{"id": "2506.19943", "categories": ["cs.CR", "cs.NI", "cs.PF"], "pdf": "https://arxiv.org/pdf/2506.19943", "abs": "https://arxiv.org/abs/2506.19943", "authors": ["Juyoul Lee", "Sanzida Hoque", "Abdullah Aydeger", "Engin Zeydan"], "title": "Quantum-Resistant Domain Name System: A Comprehensive System-Level Study", "comment": "Manuscript submitted to ACM, 29 pages, 8 Figures, 15 Tables", "summary": "The Domain Name System (DNS) plays a foundational role in Internet\ninfrastructure, yet its core protocols remain vulnerable to compromise by\nquantum adversaries. As cryptographically relevant quantum computers become a\nrealistic threat, ensuring DNS confidentiality, authenticity, and integrity in\nthe post-quantum era is imperative. In this paper, we present a comprehensive\nsystem-level study of post-quantum DNS security across three widely deployed\nmechanisms: DNSSEC, DNS-over-TLS (DoT), and DNS-over-HTTPS (DoH). We propose\nPost-Quantum Cryptographic (PQC)-DNS, a unified framework for benchmarking DNS\nsecurity under legacy, post-quantum, and hybrid cryptographic configurations.\nOur implementation leverages the Open Quantum Safe (OQS) libraries and\nintegrates lattice- and hash-based primitives into BIND9 and TLS 1.3 stacks. We\nformalize performance and threat models and analyze the impact of post-quantum\nkey encapsulation and digital signatures on end-to-end DNS resolution.\nExperimental results on a containerized testbed reveal that lattice-based\nprimitives such as Module-Lattice-Based Key-Encapsulation Mechanism (MLKEM) and\nFalcon offer practical latency and resource profiles, while hash-based schemes\nlike SPHINCS+ significantly increase message sizes and processing overhead. We\nalso examine security implications including downgrade risks, fragmentation\nvulnerabilities, and susceptibility to denial-of-service amplification. Our\nfindings inform practical guidance for deploying quantum-resilient DNS and\ncontribute to the broader effort of securing core Internet protocols for the\npost-quantum future."}
{"id": "2506.20130", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.20130", "abs": "https://arxiv.org/abs/2506.20130", "authors": ["Adrien Bibal", "Steven N. Minton", "Deborah Khider", "Yolanda Gil"], "title": "AI Copilots for Reproducibility in Science: A Case Study", "comment": null, "summary": "Open science initiatives seek to make research outputs more transparent,\naccessible, and reusable, but ensuring that published findings can be\nindependently reproduced remains a persistent challenge. This paper introduces\nOpenPub, an AI-powered platform that supports researchers, reviewers, and\nreaders through a suite of modular copilots focused on key open science tasks.\nIn this work, we present the Reproducibility Copilot, which analyzes\nmanuscripts, code, and supplementary materials to generate structured Jupyter\nNotebooks and recommendations aimed at facilitating computational, or \"rote\",\nreproducibility. We conducted feasibility tests using previously studied\nresearch papers with known reproducibility benchmarks. Results indicate that\nOpenPub can substantially reduce reproduction time - from over 30 hours to\nabout 1 hour - while achieving high coverage of figures, tables, and results\nsuitable for computational reproduction. The system systematically detects\nbarriers to reproducibility, including missing hyperparameters, undocumented\npreprocessing steps, and incomplete or inaccessible datasets. These findings\nsuggest that AI-driven tools can meaningfully reduce the burden of\nreproducibility efforts and contribute to more transparent and verifiable\nscientific communication. The modular copilot architecture also provides a\nfoundation for extending AI assistance to additional open science objectives\nbeyond reproducibility."}
{"id": "2506.20558", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.20558", "abs": "https://arxiv.org/abs/2506.20558", "authors": ["Renyi Zhong", "Yintong Huo", "Wenwei Gu", "Jinxi Kuang", "Zhihan Jiang", "Guangba Yu", "Yichen Li", "David Lo", "Michael R. Lyu"], "title": "CCISolver: End-to-End Detection and Repair of Method-Level Code-Comment Inconsistency", "comment": "This manuscript is under review", "summary": "Comments within code serve as a crucial foundation for software\ndocumentation, facilitating developers to communicate and understand the code\neffectively. However, code-comment inconsistency (CCI) can negatively affect\nsoftware development, testing, and maintenance. Recent efforts to mitigate this\nissue have emerged, but existing studies often suffer from inaccurate datasets\nand inadequate solutions, weakening their practical effectiveness. In this\nstudy, we first conduct a quantitative analysis of existing datasets, revealing\na substantial portion of sampled data are mislabeled. To address these data\nlimitations, we introduce CCIBench, a refined dataset comprising high-quality\ndata, to support the training and evaluation of method-level CCI methods.\nFurthermore, we present an innovative end-to-end LLM-based framework,\nCCISolver, designed to improve code quality by identifying and rectifying CCIs.\nComprehensive evaluations demonstrate CCISolver's superior performance. For\ndetection, it establishes a new state-of-the-art with an F1-score of 89.54%. In\nfixing task, it achieves a remarkable 18.84% relative improvement in GLEU score\nover the strongest baseline. This superiority is confirmed by human evaluation,\nwhere CCISolver's fixing success rate of 0.6533 significantly surpasses\nexisting methods. Critically, in a practical end-to-end setting, CCISolver's\ninnovative architecture is approximately 36% faster for inference than the\nbaseline model, underscoring its scalability and real-world applicability."}
{"id": "2506.20000", "categories": ["cs.CR", "cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.20000", "abs": "https://arxiv.org/abs/2506.20000", "authors": ["Narasimha Raghavan Veeraragavan", "Jan Franz Nygård"], "title": "Can One Safety Loop Guard Them All? Agentic Guard Rails for Federated Computing", "comment": "Accepted at ICML 2025 Workshop on Collaborative and Federated Agentic\n  Workflows (CFAgentic@ICML'25)", "summary": "We propose Guardian-FC, a novel two-layer framework for privacy preserving\nfederated computing that unifies safety enforcement across diverse privacy\npreserving mechanisms, including cryptographic back-ends like fully homomorphic\nencryption (FHE) and multiparty computation (MPC), as well as statistical\ntechniques such as differential privacy (DP). Guardian-FC decouples guard-rails\nfrom privacy mechanisms by executing plug-ins (modular computation units),\nwritten in a backend-neutral, domain-specific language (DSL) designed\nspecifically for federated computing workflows and interchangeable Execution\nProviders (EPs), which implement DSL operations for various privacy back-ends.\nAn Agentic-AI control plane enforces a finite-state safety loop through signed\ntelemetry and commands, ensuring consistent risk management and auditability.\nThe manifest-centric design supports fail-fast job admission and seamless\nextensibility to new privacy back-ends. We present qualitative scenarios\nillustrating backend-agnostic safety and a formal model foundation for\nverification. Finally, we outline a research agenda inviting the community to\nadvance adaptive guard-rail tuning, multi-backend composition, DSL\nspecification development, implementation, and compiler extensibility alongside\nhuman-override usability."}
{"id": "2506.20249", "categories": ["cs.AI", "cs.CL", "cs.MA"], "pdf": "https://arxiv.org/pdf/2506.20249", "abs": "https://arxiv.org/abs/2506.20249", "authors": ["Junyan Cheng", "Peter Clark", "Kyle Richardson"], "title": "Language Modeling by Language Models", "comment": null, "summary": "Can we leverage LLMs to model the process of discovering novel language model\n(LM) architectures? Inspired by real research, we propose a multi-agent LLM\napproach that simulates the conventional stages of research, from ideation and\nliterature search (proposal stage) to design implementation (code generation),\ngenerative pre-training, and downstream evaluation (verification). Using ideas\nfrom scaling laws, our system, Genesys, employs a Ladder of Scales approach;\nnew designs are proposed, adversarially reviewed, implemented, and selectively\nverified at increasingly larger model scales (14M$\\sim$350M parameters) with a\nnarrowing budget (the number of models we can train at each scale). To help\nmake discovery efficient and factorizable, Genesys uses a novel genetic\nprogramming backbone, which we show has empirical advantages over commonly used\ndirect prompt generation workflows (e.g., $\\sim$86\\% percentage point\nimprovement in successful design generation, a key bottleneck). We report\nexperiments involving 1,162 newly discovered designs (1,062 fully verified\nthrough pre-training) and find the best designs to be highly competitive with\nknown architectures (e.g., outperform GPT2, Mamba2, etc., on 6/9 common\nbenchmarks). We couple these results with comprehensive system-level ablations\nand formal results, which give broader insights into the design of effective\nautonomous discovery systems."}
{"id": "2506.20621", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.20621", "abs": "https://arxiv.org/abs/2506.20621", "authors": ["Silvio Alonso", "Antonio Pedro Santos Alves", "Lucas Romao", "Hélio Lopes", "Marcos Kalinowski"], "title": "Define-ML: An Approach to Ideate Machine Learning-Enabled Systems", "comment": "Accepted for publication at the 51st Euromicro Conference Series on\n  Software Engineering and Advanced Applications (SEAA) 2025", "summary": "[Context] The increasing adoption of machine learning (ML) in software\nsystems demands specialized ideation approaches that address ML-specific\nchallenges, including data dependencies, technical feasibility, and alignment\nbetween business objectives and probabilistic system behavior. Traditional\nideation methods like Lean Inception lack structured support for these ML\nconsiderations, which can result in misaligned product visions and unrealistic\nexpectations. [Goal] This paper presents Define-ML, a framework that extends\nLean Inception with tailored activities - Data Source Mapping, Feature-to-Data\nSource Mapping, and ML Mapping - to systematically integrate data and technical\nconstraints into early-stage ML product ideation. [Method] We developed and\nvalidated Define-ML following the Technology Transfer Model, conducting both\nstatic validation (with a toy problem) and dynamic validation (in a real-world\nindustrial case study). The analysis combined quantitative surveys with\nqualitative feedback, assessing utility, ease of use, and intent of adoption.\n[Results] Participants found Define-ML effective for clarifying data concerns,\naligning ML capabilities with business goals, and fostering cross-functional\ncollaboration. The approach's structured activities reduced ideation ambiguity,\nthough some noted a learning curve for ML-specific components, which can be\nmitigated by expert facilitation. All participants expressed the intention to\nadopt Define-ML. [Conclusion] Define-ML provides an openly available, validated\napproach for ML product ideation, building on Lean Inception's agility while\naligning features with available data and increasing awareness of technical\nfeasibility."}
{"id": "2506.20082", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.20082", "abs": "https://arxiv.org/abs/2506.20082", "authors": ["Yali Yuan", "Weiyi Zou", "Guang Cheng"], "title": "Attack Smarter: Attention-Driven Fine-Grained Webpage Fingerprinting Attacks", "comment": null, "summary": "Website Fingerprinting (WF) attacks aim to infer which websites a user is\nvisiting by analyzing traffic patterns, thereby compromising user anonymity.\nAlthough this technique has been demonstrated to be effective in controlled\nexperimental environments, it remains largely limited to small-scale scenarios,\ntypically restricted to recognizing website homepages. In practical settings,\nhowever, users frequently access multiple subpages in rapid succession, often\nbefore previous content fully loads. WebPage Fingerprinting (WPF) generalizes\nthe WF framework to large-scale environments by modeling subpages of the same\nsite as distinct classes. These pages often share similar page elements,\nresulting in lower inter-class variance in traffic features. Furthermore, we\nconsider multi-tab browsing scenarios, in which a single trace encompasses\nmultiple categories of webpages. This leads to overlapping traffic segments,\nand similar features may appear in different positions within the traffic,\nthereby increasing the difficulty of classification. To address these\nchallenges, we propose an attention-driven fine-grained WPF attack, named\nADWPF. Specifically, during the training phase, we apply targeted augmentation\nto salient regions of the traffic based on attention maps, including attention\ncropping and attention masking. ADWPF then extracts low-dimensional features\nfrom both the original and augmented traffic and applies self-attention modules\nto capture the global contextual patterns of the trace. Finally, to handle the\nmulti-tab scenario, we employ the residual attention to generate class-specific\nrepresentations of webpages occurring at different temporal positions.\nExtensive experiments demonstrate that the proposed method consistently\nsurpasses state-of-the-art baselines across datasets of different scales."}
{"id": "2506.20274", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.20274", "abs": "https://arxiv.org/abs/2506.20274", "authors": ["Liya Wang", "David Yi", "Damien Jose", "John Passarelli", "James Gao", "Jordan Leventis", "Kang Li"], "title": "Enterprise Large Language Model Evaluation Benchmark", "comment": "Submitted to MLNLP 2025 at https://csity2025.org/mlnlp/index", "summary": "Large Language Models (LLMs) ) have demonstrated promise in boosting\nproductivity across AI-powered tools, yet existing benchmarks like Massive\nMultitask Language Understanding (MMLU) inadequately assess enterprise-specific\ntask complexities. We propose a 14-task framework grounded in Bloom's Taxonomy\nto holistically evaluate LLM capabilities in enterprise contexts. To address\nchallenges of noisy data and costly annotation, we develop a scalable pipeline\ncombining LLM-as-a-Labeler, LLM-as-a-Judge, and corrective retrieval-augmented\ngeneration (CRAG), curating a robust 9,700-sample benchmark. Evaluation of six\nleading models shows open-source contenders like DeepSeek R1 rival proprietary\nmodels in reasoning tasks but lag in judgment-based scenarios, likely due to\noverthinking. Our benchmark reveals critical enterprise performance gaps and\noffers actionable insights for model optimization. This work provides\nenterprises a blueprint for tailored evaluations and advances practical LLM\ndeployment."}
{"id": "2506.20101", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2506.20101", "abs": "https://arxiv.org/abs/2506.20101", "authors": ["Jiahui Wu", "Tiecheng Sun", "Fucai Luo", "Haiyan Wang", "Weizhe Zhang"], "title": "Secure Multi-Key Homomorphic Encryption with Application to Privacy-Preserving Federated Learning", "comment": null, "summary": "Multi-Key Homomorphic Encryption (MKHE), proposed by Lopez-Alt et al. (STOC\n2012), allows for performing arithmetic computations directly on ciphertexts\nencrypted under distinct keys. Subsequent works by Chen and Dai et al. (CCS\n2019) and Kim and Song et al. (CCS 2023) extended this concept by proposing\nmulti-key BFV/CKKS variants, referred to as the CDKS scheme. These variants\nincorporate asymptotically optimal techniques to facilitate secure computation\nacross multiple data providers. In this paper, we identify a critical security\nvulnerability in the CDKS scheme when applied to multiparty secure computation\ntasks, such as privacy-preserving federated learning (PPFL). In particular, we\nshow that CDKS may inadvertently leak plaintext information from one party to\nothers. To mitigate this issue, we propose a new scheme, SMHE (Secure Multi-Key\nHomomorphic Encryption), which incorporates a novel masking mechanism into the\nmulti-key BFV and CKKS frameworks to ensure that plaintexts remain confidential\nthroughout the computation. We implement a PPFL application using SMHE and\ndemonstrate that it provides significantly improved security with only a modest\noverhead in homomorphic evaluation. For instance, our PPFL model based on\nmulti-key CKKS incurs less than a 2\\times runtime and communication traffic\nincrease compared to the CDKS-based PPFL model. The code is publicly available\nat https://github.com/JiahuiWu2022/SMHE.git."}
{"id": "2506.20332", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.20332", "abs": "https://arxiv.org/abs/2506.20332", "authors": ["Jihao Gu", "Qihang Ai", "Yingyao Wang", "Pi Bu", "Jingxuan Xing", "Zekun Zhu", "Wei Jiang", "Ziming Wang", "Yingxiu Zhao", "Ming-Liang Zhang", "Jun Song", "Yuning Jiang", "Bo Zheng"], "title": "Mobile-R1: Towards Interactive Reinforcement Learning for VLM-Based Mobile Agent via Task-Level Rewards", "comment": "14 pages, 12 figures", "summary": "Vision-language model-based mobile agents have gained the ability to not only\nunderstand complex instructions and mobile screenshots, but also optimize their\naction outputs via thinking and reasoning, benefiting from reinforcement\nlearning, such as Group Relative Policy Optimization (GRPO). However, existing\nresearch centers on offline reinforcement learning training or online\noptimization using action-level rewards, which limits the agent's dynamic\ninteraction with the environment. This often results in agents settling into\nlocal optima, thereby weakening their ability for exploration and error action\ncorrection. To address these challenges, we introduce an approach called\nMobile-R1, which employs interactive multi-turn reinforcement learning with\ntask-level rewards for mobile agents. Our training framework consists of three\nstages: initial format finetuning, single-step online training via action-level\nreward, followed by online training via task-level reward based on multi-turn\ntrajectories. This strategy is designed to enhance the exploration and error\ncorrection capabilities of Mobile-R1, leading to significant performance\nimprovements. Moreover, we have collected a dataset covering 28 Chinese\napplications with 24,521 high-quality manual annotations and established a new\nbenchmark with 500 trajectories. We will open source all resources, including\nthe dataset, benchmark, model weight, and codes:\nhttps://mobile-r1.github.io/Mobile-R1/."}
{"id": "2506.20102", "categories": ["cs.CR", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.20102", "abs": "https://arxiv.org/abs/2506.20102", "authors": ["Malikussaid", "Sutiyo"], "title": "Autonomous Cyber Resilience via a Co-Evolutionary Arms Race within a Fortified Digital Twin Sandbox", "comment": "17 pages, 2 figures, 4 equations, 2 algorithms, 4 tables, to be\n  published in ISPACS Conference 2025, unabridged version", "summary": "The convergence of IT and OT has created hyper-connected ICS, exposing\ncritical infrastructure to a new class of adaptive, intelligent adversaries\nthat render static defenses obsolete. Existing security paradigms often fail to\naddress a foundational \"Trinity of Trust,\" comprising the fidelity of the\nsystem model, the integrity of synchronizing data, and the resilience of the\nanalytical engine against sophisticated evasion. This paper introduces the ARC\nframework, a method for achieving analytical resilience through an autonomous,\nclosed-loop hardening process. ARC establishes a perpetual co-evolutionary arms\nrace within the high-fidelity sandbox of a F-SCDT. A DRL agent, the \"Red\nAgent,\" is formalized and incentivized to autonomously discover stealthy,\nphysically-plausible attack paths that maximize process disruption while\nevading detection. Concurrently, an ensemble-based \"Blue Agent\" defender is\ncontinuously hardened via adversarial training against the evolving threats\ndiscovered by its adversary. This co-evolutionary dynamic forces both agents to\nbecome progressively more sophisticated, enabling the system to autonomously\nprobe and patch its own vulnerabilities. Experimental validation on both the\nTEP and the SWaT testbeds demonstrates the framework's superior performance. A\ncomprehensive ablation study, supported by extensive visualizations including\nROC curves and SHAP plots, reveals that the co-evolutionary process itself is\nresponsible for a significant performance increase in detecting novel attacks.\nBy integrating XAI to ensure operator trust and proposing a scalable F-ARC\narchitecture, this work presents ARC not merely as an improvement, but as a\nnecessary paradigm shift toward dynamic, self-improving security for the future\nof critical infrastructure."}
{"id": "2506.20357", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.20357", "abs": "https://arxiv.org/abs/2506.20357", "authors": ["Sungwon Han", "Sungkyu Park", "Seungeon Lee"], "title": "Tabular Feature Discovery With Reasoning Type Exploration", "comment": null, "summary": "Feature engineering for tabular data remains a critical yet challenging step\nin machine learning. Recently, large language models (LLMs) have been used to\nautomatically generate new features by leveraging their vast knowledge.\nHowever, existing LLM-based approaches often produce overly simple or\nrepetitive features, partly due to inherent biases in the transformations the\nLLM chooses and the lack of structured reasoning guidance during generation. In\nthis paper, we propose a novel method REFeat, which guides an LLM to discover\ndiverse and informative features by leveraging multiple types of reasoning to\nsteer the feature generation process. Experiments on 59 benchmark datasets\ndemonstrate that our approach not only achieves higher predictive accuracy on\naverage, but also discovers more diverse and meaningful features. These results\nhighlight the promise of incorporating rich reasoning paradigms and adaptive\nstrategy selection into LLM-driven feature discovery for tabular data."}
{"id": "2506.20109", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2506.20109", "abs": "https://arxiv.org/abs/2506.20109", "authors": ["Lambang Akbar Wijayadi", "Yuancheng Jiang", "Roland H. C. Yap", "Zhenkai Liang", "Zhuohao Liu"], "title": "Evaluating Disassembly Errors With Only Binaries", "comment": null, "summary": "Disassemblers are crucial in the analysis and modification of binaries.\nExisting works showing disassembler errors largely rely on practical\nimplementation without specific guarantees and assume source code and compiler\ntoolchains to evaluate ground truth. However, the assumption of source code is\ncontrary to typical binary scenarios where only the binary is available. In\nthis work, we investigate an approach with minimal assumptions and a sound\napproach to disassembly error evaluation that does not require source code. Any\nsource code does not address the fundamental problem of binary disassembly and\nfails when only the binary exists. As far as we know, this is the first work to\nevaluate disassembly errors using only the binary. We propose TraceBin, which\nuses dynamic execution to find disassembly errors. TraceBin targets the use\ncase where the disassembly is used in an automated fashion for security tasks\non a target binary, such as static binary instrumentation, binary hardening,\nautomated code repair, and so on, which may be affected by disassembly errors.\nDiscovering disassembly errors in the target binary aids in reducing problems\ncaused by such errors. Furthermore, we are not aware of existing approaches\nthat can evaluate errors given only a target binary, as they require source\ncode. Our evaluation shows TraceBin finds: (i) errors consistent with existing\nstudies even without source; (ii) disassembly errors due to control flow; (iii)\nnew interesting errors; (iv) errors in non-C/C++ binaries; (v) errors in\nclosed-source binaries; and (vi) show that disassembly errors can have\nsignificant security implications. Overall, our experimental results show that\nTraceBin finds many errors in existing popular disassemblers. It is also\nhelpful in automated security tasks on (closed source) binaries relying on\ndisassemblers."}
{"id": "2506.20384", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.20384", "abs": "https://arxiv.org/abs/2506.20384", "authors": ["Dror Ivry", "Oran Nahum"], "title": "Paladin-mini: A Compact and Efficient Grounding Model Excelling in Real-World Scenarios", "comment": "6 pages, 2 figures", "summary": "This paper introduces two significant contributions to address the issue of\ngrounding claims in a given context. Grounding means that given a context\n(document) and a claim, there's at least one supportive evidence for the claim\nin the document. We will introduce Paladin-mini, a compact (3.8B parameters)\nopen-source classifier model (used for labeling data as grounded or ungrounded)\nengineered for robust performance in real-world scenarios, and the\ngrounding-benchmark, a new evaluation dataset designed to assess performance on\ncritical reasoning tasks. We'll also demonstrate the results of Paladin-mini\nwith benchmarks against the current State-of-the-art and share clear and\nreproducible results."}
{"id": "2506.20170", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2506.20170", "abs": "https://arxiv.org/abs/2506.20170", "authors": ["Guoqiang Chen", "Xin Jin", "Zhiqiang Lin"], "title": "JsDeObsBench: Measuring and Benchmarking LLMs for JavaScript Deobfuscation", "comment": "Accepted by ACM CCS 2025", "summary": "Deobfuscating JavaScript (JS) code poses a significant challenge in web\nsecurity, particularly as obfuscation techniques are frequently used to conceal\nmalicious activities within scripts. While Large Language Models (LLMs) have\nrecently shown promise in automating the deobfuscation process, transforming\ndetection and mitigation strategies against these obfuscated threats, a\nsystematic benchmark to quantify their effectiveness and limitations has been\nnotably absent. To address this gap, we present JsDeObsBench, a dedicated\nbenchmark designed to rigorously evaluate the effectiveness of LLMs in the\ncontext of JS deobfuscation. We detail our benchmarking methodology, which\nincludes a wide range of obfuscation techniques ranging from basic variable\nrenaming to sophisticated structure transformations, providing a robust\nframework for assessing LLM performance in real-world scenarios. Our extensive\nexperimental analysis investigates the proficiency of cutting-edge LLMs, e.g.,\nGPT-4o, Mixtral, Llama, and DeepSeek-Coder, revealing superior performance in\ncode simplification despite challenges in maintaining syntax accuracy and\nexecution reliability compared to baseline methods. We further evaluate the\ndeobfuscation of JS malware to exhibit the potential of LLMs in security\nscenarios. The findings highlight the utility of LLMs in deobfuscation\napplications and pinpoint crucial areas for further improvement."}
{"id": "2506.20401", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.20401", "abs": "https://arxiv.org/abs/2506.20401", "authors": ["Jinchun Du", "Bojie Shen", "Muhammad Aamir Cheema", "Adel N. Toosi"], "title": "Smart Ride and Delivery Services with Electric Vehicles: Leveraging Bidirectional Charging for Profit Optimisation", "comment": null, "summary": "With the rising popularity of electric vehicles (EVs), modern service\nsystems, such as ride-hailing delivery services, are increasingly integrating\nEVs into their operations. Unlike conventional vehicles, EVs often have a\nshorter driving range, necessitating careful consideration of charging when\nfulfilling requests. With recent advances in Vehicle-to-Grid (V2G) technology -\nallowing EVs to also discharge energy back to the grid - new opportunities and\ncomplexities emerge. We introduce the Electric Vehicle Orienteering Problem\nwith V2G (EVOP-V2G): a profit-maximization problem where EV drivers must select\ncustomer requests or orders while managing when and where to charge or\ndischarge. This involves navigating dynamic electricity prices, charging\nstation selection, and route constraints. We formulate the problem as a Mixed\nInteger Programming (MIP) model and propose two near-optimal metaheuristic\nalgorithms: one evolutionary (EA) and the other based on large neighborhood\nsearch (LNS). Experiments on real-world data show our methods can double driver\nprofits compared to baselines, while maintaining near-optimal performance on\nsmall instances and excellent scalability on larger ones. Our work highlights a\npromising path toward smarter, more profitable EV-based mobility systems that\nactively support the energy grid."}
{"id": "2506.20228", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2506.20228", "abs": "https://arxiv.org/abs/2506.20228", "authors": ["Antony Dalmiere", "Zheng Zhou", "Guillaume Auriol", "Vincent Nicomette", "Pascal Marchand"], "title": "Measuring Modern Phishing Tactics: A Quantitative Study of Body Obfuscation Prevalence, Co-occurrence, and Filter Impact", "comment": null, "summary": "Phishing attacks frequently use email body obfuscation to bypass detection\nfilters, but quantitative insights into how techniques are combined and their\nimpact on filter scores remain limited. This paper addresses this gap by\nempirically investigating the prevalence, co-occurrence patterns, and spam\nscore associations of body obfuscation techniques. Analysing 386 verified\nphishing emails, we quantified ten techniques, identified significant pairwise\nco-occurrences revealing strategic layering like the presence of text in images\nwith multipart abuse, and assessed associations with antispam scores using\nmultilinear regression. Text in Image (47.0%), Base64 Encoding (31.2%), and\nInvalid HTML (28.8%) were highly prevalent. Regression (R${}^2$=0.486, p<0.001)\nlinked Base64 Encoding and Text in Image with significant antispam evasion\n(p<0.05) in this configuration, suggesting potential bypass capabilities, while\nInvalid HTML correlated with higher scores. These findings establish a\nquantitative baseline for complex evasion strategies, underscoring the need for\nmulti-modal defences against combined obfuscation tactics."}
{"id": "2506.20404", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.20404", "abs": "https://arxiv.org/abs/2506.20404", "authors": ["Riccardo Lo Bianco", "Willem van Jaarsveld", "Remco Dijkman"], "title": "GymPN: A Library for Decision-Making in Process Management Systems", "comment": null, "summary": "Process management systems support key decisions about the way work is\nallocated in organizations. This includes decisions on which task to perform\nnext, when to execute the task, and who to assign the task to. Suitable\nsoftware tools are required to support these decisions in a way that is optimal\nfor the organization. This paper presents a software library, called GymPN,\nthat supports optimal decision-making in business processes using Deep\nReinforcement Learning. GymPN builds on previous work that supports task\nassignment in business processes, introducing two key novelties: support for\npartial process observability and the ability to model multiple decisions in a\nbusiness process. These novel elements address fundamental limitations of\nprevious work and thus enable the representation of more realistic process\ndecisions. We evaluate the library on eight typical business process\ndecision-making problem patterns, showing that GymPN allows for easy modeling\nof the desired problems, as well as learning optimal decision policies."}
{"id": "2506.20234", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2506.20234", "abs": "https://arxiv.org/abs/2506.20234", "authors": ["Quentin Hillebrand", "Vorapong Suppakitpaisarn", "Tetsuo Shibuya"], "title": "Communication-Efficient Publication of Sparse Vectors under Differential Privacy", "comment": null, "summary": "In this work, we propose a differentially private algorithm for publishing\nmatrices aggregated from sparse vectors. These matrices include social network\nadjacency matrices, user-item interaction matrices in recommendation systems,\nand single nucleotide polymorphisms (SNPs) in DNA data. Traditionally,\ndifferential privacy in vector collection relies on randomized response, but\nthis approach incurs high communication costs. Specifically, for a matrix with\n$N$ users, $n$ columns, and $m$ nonzero elements, conventional methods require\n$\\Omega(n \\times N)$ communication, making them impractical for large-scale\ndata. Our algorithm significantly reduces this cost to $O(\\varepsilon m)$,\nwhere $\\varepsilon$ is the privacy budget. Notably, this is even lower than the\nnon-private case, which requires $\\Omega(m \\log n)$ communication. Moreover, as\nthe privacy budget decreases, communication cost further reduces, enabling\nbetter privacy with improved efficiency. We theoretically prove that our method\nyields results identical to those of randomized response, and experimental\nevaluations confirm its effectiveness in terms of accuracy, communication\nefficiency, and computational complexity."}
{"id": "2506.20486", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.20486", "abs": "https://arxiv.org/abs/2506.20486", "authors": ["Salvatore Milite", "Giulio Caravagna", "Andrea Sottoriva"], "title": "Mixtures of Neural Cellular Automata: A Stochastic Framework for Growth Modelling and Self-Organization", "comment": null, "summary": "Neural Cellular Automata (NCAs) are a promising new approach to model\nself-organizing processes, with potential applications in life science.\nHowever, their deterministic nature limits their ability to capture the\nstochasticity of real-world biological and physical systems.\n  We propose the Mixture of Neural Cellular Automata (MNCA), a novel framework\nincorporating the idea of mixture models into the NCA paradigm. By combining\nprobabilistic rule assignments with intrinsic noise, MNCAs can model diverse\nlocal behaviors and reproduce the stochastic dynamics observed in biological\nprocesses.\n  We evaluate the effectiveness of MNCAs in three key domains: (1) synthetic\nsimulations of tissue growth and differentiation, (2) image morphogenesis\nrobustness, and (3) microscopy image segmentation. Results show that MNCAs\nachieve superior robustness to perturbations, better recapitulate real\nbiological growth patterns, and provide interpretable rule segmentation. These\nfindings position MNCAs as a promising tool for modeling stochastic dynamical\nsystems and studying self-growth processes."}
{"id": "2506.20290", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2506.20290", "abs": "https://arxiv.org/abs/2506.20290", "authors": ["Berkay Kemal Balioglu", "Alireza Khodaie", "Mehmet Emre Gursoy"], "title": "Don't Hash Me Like That: Exposing and Mitigating Hash-Induced Unfairness in Local Differential Privacy", "comment": null, "summary": "Local differential privacy (LDP) has become a widely accepted framework for\nprivacy-preserving data collection. In LDP, many protocols rely on hash\nfunctions to implement user-side encoding and perturbation. However, the\nsecurity and privacy implications of hash function selection have not been\npreviously investigated. In this paper, we expose that the hash functions may\nact as a source of unfairness in LDP protocols. We show that although users\noperate under the same protocol and privacy budget, differences in hash\nfunctions can lead to significant disparities in vulnerability to inference and\npoisoning attacks. To mitigate hash-induced unfairness, we propose Fair-OLH\n(F-OLH), a variant of OLH that enforces an entropy-based fairness constraint on\nhash function selection. Experiments show that F-OLH is effective in mitigating\nhash-induced unfairness under acceptable time overheads."}
{"id": "2506.20504", "categories": ["cs.AI", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2506.20504", "abs": "https://arxiv.org/abs/2506.20504", "authors": ["Konstantin Demin", "Taylor Webb", "Eric Elmoznino", "Hakwan Lau"], "title": "Engineering Sentience", "comment": null, "summary": "We spell out a definition of sentience that may be useful for designing and\nbuilding it in machines. We propose that for sentience to be meaningful for AI,\nit must be fleshed out in functional, computational terms, in enough detail to\nallow for implementation. Yet, this notion of sentience must also reflect\nsomething essentially 'subjective', beyond just having the general capacity to\nencode perceptual content. For this specific functional notion of sentience to\noccur, we propose that certain sensory signals need to be both assertoric\n(persistent) and qualitative. To illustrate the definition in more concrete\nterms, we sketch out some ways for potential implementation, given current\ntechnology. Understanding what it takes for artificial agents to be\nfunctionally sentient can also help us avoid creating them inadvertently, or at\nleast, realize that we have created them in a timely manner."}
{"id": "2506.20415", "categories": ["cs.CR", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2506.20415", "abs": "https://arxiv.org/abs/2506.20415", "authors": ["Dipayan Saha", "Shams Tarek", "Hasan Al Shaikh", "Khan Thamid Hasan", "Pavan Sai Nalluri", "Md. Ajoad Hasan", "Nashmin Alam", "Jingbo Zhou", "Sujan Kumar Saha", "Mark Tehranipoor", "Farimah Farahmandi"], "title": "SV-LLM: An Agentic Approach for SoC Security Verification using Large Language Models", "comment": null, "summary": "Ensuring the security of complex system-on-chips (SoCs) designs is a critical\nimperative, yet traditional verification techniques struggle to keep pace due\nto significant challenges in automation, scalability, comprehensiveness, and\nadaptability. The advent of large language models (LLMs), with their remarkable\ncapabilities in natural language understanding, code generation, and advanced\nreasoning, presents a new paradigm for tackling these issues. Moving beyond\nmonolithic models, an agentic approach allows for the creation of multi-agent\nsystems where specialized LLMs collaborate to solve complex problems more\neffectively. Recognizing this opportunity, we introduce SV-LLM, a novel\nmulti-agent assistant system designed to automate and enhance SoC security\nverification. By integrating specialized agents for tasks like verification\nquestion answering, security asset identification, threat modeling, test plan\nand property generation, vulnerability detection, and simulation-based bug\nvalidation, SV-LLM streamlines the workflow. To optimize their performance in\nthese diverse tasks, agents leverage different learning paradigms, such as\nin-context learning, fine-tuning, and retrieval-augmented generation (RAG). The\nsystem aims to reduce manual intervention, improve accuracy, and accelerate\nsecurity analysis, supporting proactive identification and mitigation of risks\nearly in the design cycle. We demonstrate its potential to transform hardware\nsecurity practices through illustrative case studies and experiments that\nshowcase its applicability and efficacy."}
{"id": "2506.20531", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2506.20531", "abs": "https://arxiv.org/abs/2506.20531", "authors": ["Wenbin Gan", "Minh-Son Dao", "Koji Zettsu"], "title": "Case-based Reasoning Augmented Large Language Model Framework for Decision Making in Realistic Safety-Critical Driving Scenarios", "comment": "12 pages, 10 figures, under-review conference", "summary": "Driving in safety-critical scenarios requires quick, context-aware\ndecision-making grounded in both situational understanding and experiential\nreasoning. Large Language Models (LLMs), with their powerful general-purpose\nreasoning capabilities, offer a promising foundation for such decision-making.\nHowever, their direct application to autonomous driving remains limited due to\nchallenges in domain adaptation, contextual grounding, and the lack of\nexperiential knowledge needed to make reliable and interpretable decisions in\ndynamic, high-risk environments. To address this gap, this paper presents a\nCase-Based Reasoning Augmented Large Language Model (CBR-LLM) framework for\nevasive maneuver decision-making in complex risk scenarios. Our approach\nintegrates semantic scene understanding from dashcam video inputs with the\nretrieval of relevant past driving cases, enabling LLMs to generate maneuver\nrecommendations that are both context-sensitive and human-aligned. Experiments\nacross multiple open-source LLMs show that our framework improves decision\naccuracy, justification quality, and alignment with human expert behavior.\nRisk-aware prompting strategies further enhance performance across diverse risk\ntypes, while similarity-based case retrieval consistently outperforms random\nsampling in guiding in-context learning. Case studies further demonstrate the\nframework's robustness in challenging real-world conditions, underscoring its\npotential as an adaptive and trustworthy decision-support tool for intelligent\ndriving systems."}
{"id": "2506.20488", "categories": ["cs.CR", "cs.NI"], "pdf": "https://arxiv.org/pdf/2506.20488", "abs": "https://arxiv.org/abs/2506.20488", "authors": ["Shuo Yang", "Xinran Zheng", "Jinfeng Xu", "Jinze Li", "Danyang Song", "Zheyu Chen", "Edith C. H. Ngai"], "title": "Generative AI for Vulnerability Detection in 6G Wireless Networks: Advances, Case Study, and Future Directions", "comment": null, "summary": "The rapid advancement of 6G wireless networks, IoT, and edge computing has\nsignificantly expanded the cyberattack surface, necessitating more intelligent\nand adaptive vulnerability detection mechanisms. Traditional security methods,\nwhile foundational, struggle with zero-day exploits, adversarial threats, and\ncontext-dependent vulnerabilities in highly dynamic network environments.\nGenerative AI (GAI) emerges as a transformative solution, leveraging synthetic\ndata generation, multimodal reasoning, and adaptive learning to enhance\nsecurity frameworks. This paper explores the integration of GAI-powered\nvulnerability detection in 6G wireless networks, focusing on code auditing,\nprotocol security, cloud-edge defenses, and hardware protection. We introduce a\nthree-layer framework comprising the Technology Layer, Capability Layer, and\nApplication Layer to systematically analyze the role of VAEs, GANs, LLMs, and\nGDMs in securing next-generation wireless ecosystems. To demonstrate practical\nimplementation, we present a case study on LLM-driven code vulnerability\ndetection, highlighting its effectiveness, performance, and challenges.\nFinally, we outline future research directions, including lightweight models,\nhigh-authenticity data generation, external knowledge integration, and\nprivacy-preserving technologies. By synthesizing current advancements and open\nchallenges, this work provides a roadmap for researchers and practitioners to\nharness GAI for building resilient and adaptive security solutions in 6G\nnetworks."}
{"id": "2506.20598", "categories": ["cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.20598", "abs": "https://arxiv.org/abs/2506.20598", "authors": ["Alexander D. Kalian", "Jaewook Lee", "Stefan P. Johannesson", "Lennart Otte", "Christer Hogstrand", "Miao Guo"], "title": "Fine-Tuning and Prompt Engineering of LLMs, for the Creation of Multi-Agent AI for Addressing Sustainable Protein Production Challenges", "comment": null, "summary": "The global demand for sustainable protein sources has accelerated the need\nfor intelligent tools that can rapidly process and synthesise domain-specific\nscientific knowledge. In this study, we present a proof-of-concept multi-agent\nArtificial Intelligence (AI) framework designed to support sustainable protein\nproduction research, with an initial focus on microbial protein sources. Our\nRetrieval-Augmented Generation (RAG)-oriented system consists of two GPT-based\nLLM agents: (1) a literature search agent that retrieves relevant scientific\nliterature on microbial protein production for a specified microbial strain,\nand (2) an information extraction agent that processes the retrieved content to\nextract relevant biological and chemical information. Two parallel\nmethodologies, fine-tuning and prompt engineering, were explored for agent\noptimisation. Both methods demonstrated effectiveness at improving the\nperformance of the information extraction agent in terms of transformer-based\ncosine similarity scores between obtained and ideal outputs. Mean cosine\nsimilarity scores were increased by up to 25%, while universally reaching mean\nscores of $\\geq 0.89$ against ideal output text. Fine-tuning overall improved\nthe mean scores to a greater extent (consistently of $\\geq 0.94$) compared to\nprompt engineering, although lower statistical uncertainties were observed with\nthe latter approach. A user interface was developed and published for enabling\nthe use of the multi-agent AI system, alongside preliminary exploration of\nadditional chemical safety-based search capabilities"}
{"id": "2506.20576", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.20576", "abs": "https://arxiv.org/abs/2506.20576", "authors": ["Sabrine Ennaji", "Elhadj Benkhelifa", "Luigi V. Mancini"], "title": "Vulnerability Disclosure through Adaptive Black-Box Adversarial Attacks on NIDS", "comment": null, "summary": "Adversarial attacks, wherein slight inputs are carefully crafted to mislead\nintelligent models, have attracted increasing attention. However, a critical\ngap persists between theoretical advancements and practical application,\nparticularly in structured data like network traffic, where interdependent\nfeatures complicate effective adversarial manipulations. Moreover, ambiguity in\ncurrent approaches restricts reproducibility and limits progress in this field.\nHence, existing defenses often fail to handle evolving adversarial attacks.\nThis paper proposes a novel approach for black-box adversarial attacks, that\naddresses these limitations. Unlike prior work, which often assumes system\naccess or relies on repeated probing, our method strictly respect black-box\nconstraints, reducing interaction to avoid detection and better reflect\nreal-world scenarios. We present an adaptive feature selection strategy using\nchange-point detection and causality analysis to identify and target sensitive\nfeatures to perturbations. This lightweight design ensures low computational\ncost and high deployability. Our comprehensive experiments show the attack's\neffectiveness in evading detection with minimal interaction, enhancing its\nadaptability and applicability in real-world scenarios. By advancing the\nunderstanding of adversarial attacks in network traffic, this work lays a\nfoundation for developing robust defenses."}
{"id": "2506.20600", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.20600", "abs": "https://arxiv.org/abs/2506.20600", "authors": ["Wengxi Li", "Roy Pea", "Nick Haber", "Hari Subramonyam"], "title": "CogGen: A Learner-Centered Generative AI Architecture for Intelligent Tutoring with Programming Video", "comment": null, "summary": "We introduce CogGen, a learner-centered AI architecture that transforms\nprogramming videos into interactive, adaptive learning experiences by\nintegrating student modeling with generative AI tutoring based on the Cognitive\nApprenticeship framework. The architecture consists of three components: (1)\nvideo segmentation by learning goals, (2) a conversational tutoring engine\napplying Cognitive Apprenticeship strategies, and (3) a student model using\nBayesian Knowledge Tracing to adapt instruction. Our technical evaluation\ndemonstrates effective video segmentation accuracy and strong pedagogical\nalignment across knowledge, method, action, and interaction layers. Ablation\nstudies confirm the necessity of each component in generating effective\nguidance. This work advances AI-powered tutoring by bridging structured student\nmodeling with interactive AI conversations, offering a scalable approach to\nenhancing video-based programming education."}
{"id": "2506.20585", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2506.20585", "abs": "https://arxiv.org/abs/2506.20585", "authors": ["Alexander Söderhäll", "Zahra Alimadadi", "Panos Papadimitratos"], "title": "On the Impact of Sybil-based Attacks on Mobile Crowdsensing for Transportation", "comment": "7 pages, 5 figures, 2 tables, TrustSense workshop of PerCom 2025", "summary": "Mobile Crowd-Sensing (MCS) enables users with personal mobile devices (PMDs)\nto gain information on their surroundings. Users collect and contribute data on\ndifferent phenomena using their PMD sensors, and the MCS system processes this\ndata to extract valuable information for end users. Navigation MCS-based\napplications (N-MCS) are prevalent and important for transportation: users\nshare their location and speed while driving and, in return, find efficient\nroutes to their destinations. However, N-MCS are currently vulnerable to\nmalicious contributors, often termed Sybils: submitting falsified data,\nseemingly from many devices that are not truly present on target roads, falsely\nreporting congestion when there is none, thus changing the road status the\nN-MCS infers. The attack effect is that the N-MCS returns suboptimal routes to\nusers, causing late arrival and, overall, deteriorating road traffic flow. We\ninvestigate exactly the impact of Sybil-based attacks on N-MCS: we design an\nN-MCS system that offers efficient routing on top of the vehicular simulator\nSUMO, using the InTAS road network as our scenario. We design experiments\nattacking an individual N-MCS user as well as a larger population of users,\nselecting the adversary targets based on graph-theoretical arguments. Our\nexperiments show that the resources required for a successful attack depend on\nthe location of the attack (i.e., the surrounding road network and traffic) and\nthe extent of Sybil contributed data for the targeted road(s). We demonstrate\nthat Sybil attacks can alter the route of N-MCS users, increasing average\ntravel time by 20% with Sybils 3% of the N-MCS user population."}
{"id": "2506.20608", "categories": ["cs.AI", "cs.NA", "math.NA"], "pdf": "https://arxiv.org/pdf/2506.20608", "abs": "https://arxiv.org/abs/2506.20608", "authors": ["Barry Smith", "Junchao Zhang", "Hong Zhang", "Lois Curfman McInnes", "Murat Keceli", "Archit Vasan", "Satish Balay", "Toby Isaac", "Le Chen", "Venkatram Vishwanath"], "title": "AI Assistants to Enhance and Exploit the PETSc Knowledge Base", "comment": null, "summary": "Generative AI, especially through large language models (LLMs), is\ntransforming how technical knowledge can be accessed, reused, and extended.\nPETSc, a widely used numerical library for high-performance scientific\ncomputing, has accumulated a rich but fragmented knowledge base over its three\ndecades of development, spanning source code, documentation, mailing lists,\nGitLab issues, Discord conversations, technical papers, and more. Much of this\nknowledge remains informal and inaccessible to users and new developers. To\nactivate and utilize this knowledge base more effectively, the PETSc team has\nbegun building an LLM-powered system that combines PETSc content with custom\nLLM tools -- including retrieval-augmented generation (RAG), reranking\nalgorithms, and chatbots -- to assist users, support developers, and propose\nupdates to formal documentation. This paper presents initial experiences\ndesigning and evaluating these tools, focusing on system architecture, using\nRAG and reranking for PETSc-specific information, evaluation methodologies for\nvarious LLMs and embedding models, and user interface design. Leveraging the\nArgonne Leadership Computing Facility resources, we analyze how LLM responses\ncan enhance the development and use of numerical software, with an initial\nfocus on scalable Krylov solvers. Our goal is to establish an extensible\nframework for knowledge-centered AI in scientific software, enabling scalable\nsupport, enriched documentation, and enhanced workflows for research and\ndevelopment. We conclude by outlining directions for expanding this system into\na robust, evolving platform that advances software ecosystems to accelerate\nscientific discovery."}
{"id": "2506.20640", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.20640", "abs": "https://arxiv.org/abs/2506.20640", "authors": ["Sijie Li", "Weiwei Sun", "Shanda Li", "Ameet Talwalkar", "Yiming Yang"], "title": "Towards Community-Driven Agents for Machine Learning Engineering", "comment": null, "summary": "Large language model-based machine learning (ML) agents have shown great\npromise in automating ML research. However, existing agents typically operate\nin isolation on a given research problem, without engaging with the broader\nresearch community, where human researchers often gain insights and contribute\nby sharing knowledge. To bridge this gap, we introduce MLE-Live, a live\nevaluation framework designed to assess an agent's ability to communicate with\nand leverage collective knowledge from a simulated Kaggle research community.\nBuilding on this framework, we propose CoMind, a novel agent that excels at\nexchanging insights and developing novel solutions within a community context.\nCoMind achieves state-of-the-art performance on MLE-Live and outperforms 79.2%\nhuman competitors on average across four ongoing Kaggle competitions. Our code\nis released at https://github.com/comind-ml/CoMind."}
{"id": "2506.20664", "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.MA"], "pdf": "https://arxiv.org/pdf/2506.20664", "abs": "https://arxiv.org/abs/2506.20664", "authors": ["Andrei Lupu", "Timon Willi", "Jakob Foerster"], "title": "The Decrypto Benchmark for Multi-Agent Reasoning and Theory of Mind", "comment": "41 pages, 19 figures", "summary": "As Large Language Models (LLMs) gain agentic abilities, they will have to\nnavigate complex multi-agent scenarios, interacting with human users and other\nagents in cooperative and competitive settings. This will require new reasoning\nskills, chief amongst them being theory of mind (ToM), or the ability to reason\nabout the \"mental\" states of other agents. However, ToM and other multi-agent\nabilities in LLMs are poorly understood, since existing benchmarks suffer from\nnarrow scope, data leakage, saturation, and lack of interactivity. We thus\npropose Decrypto, a game-based benchmark for multi-agent reasoning and ToM\ndrawing inspiration from cognitive science, computational pragmatics and\nmulti-agent reinforcement learning. It is designed to be as easy as possible in\nall other dimensions, eliminating confounding factors commonly found in other\nbenchmarks. To our knowledge, it is also the first platform for designing\ninteractive ToM experiments.\n  We validate the benchmark design through comprehensive empirical evaluations\nof frontier LLMs, robustness studies, and human-AI cross-play experiments. We\nfind that LLM game-playing abilities lag behind humans and simple\nword-embedding baselines. We then create variants of two classic cognitive\nscience experiments within Decrypto to evaluate three key ToM abilities.\nSurprisingly, we find that state-of-the-art reasoning models are significantly\nworse at those tasks than their older counterparts. This demonstrates that\nDecrypto addresses a crucial gap in current reasoning and ToM evaluations, and\npaves the path towards better artificial agents."}
{"id": "2506.19870", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.19870", "abs": "https://arxiv.org/abs/2506.19870", "authors": ["Md Asif Ul Hoq Khan", "MD Zahedul Islam", "Istiaq Ahmed", "Md Masud Karim Rabbi", "Farhana Rahman Anonna", "MD Abdul Fahim Zeeshan", "Mehedi Hasan Ridoy", "Bivash Ranjan Chowdhury", "Md Nazmul Shakir Rabbi", "GM Alamin Sadnan"], "title": "Secure Energy Transactions Using Blockchain Leveraging AI for Fraud Detection and Energy Market Stability", "comment": null, "summary": "Peer-to-peer trading and the move to decentralized grids have reshaped the\nenergy markets in the United States. Notwithstanding, such developments lead to\nnew challenges, mainly regarding the safety and authenticity of energy trade.\nThis study aimed to develop and build a secure, intelligent, and efficient\nenergy transaction system for the decentralized US energy market. This research\ninterlinks the technological prowess of blockchain and artificial intelligence\n(AI) in a novel way to solve long-standing challenges in the distributed energy\nmarket, specifically those of security, fraudulent behavior detection, and\nmarket reliability. The dataset for this research is comprised of more than 1.2\nmillion anonymized energy transaction records from a simulated peer-to-peer\n(P2P) energy exchange network emulating real-life blockchain-based American\nmicrogrids, including those tested by LO3 Energy and Grid+ Labs. Each record\ncontains detailed fields of transaction identifier, timestamp, energy volume\n(kWh), transaction type (buy/sell), unit price, prosumer/consumer identifier\n(hashed for privacy), smart meter readings, geolocation regions, and settlement\nconfirmation status. The dataset also includes system-calculated behavior\nmetrics of transaction rate, variability of energy production, and historical\npricing patterns. The system architecture proposed involves the integration of\ntwo layers, namely a blockchain layer and artificial intelligence (AI) layer,\neach playing a unique but complementary function in energy transaction securing\nand market intelligence improvement. The machine learning models used in this\nresearch were specifically chosen for their established high performance in\nclassification tasks, specifically in the identification of energy transaction\nfraud in decentralized markets."}
{"id": "2506.19871", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.19871", "abs": "https://arxiv.org/abs/2506.19871", "authors": ["Yining Pang", "Chenghan Li"], "title": "An Attack Method for Medical Insurance Claim Fraud Detection based on Generative Adversarial Network", "comment": "arXiv admin note: text overlap with arXiv:2405.12076 by other authors", "summary": "Insurance fraud detection represents a pivotal advancement in modern\ninsurance service, providing intelligent and digitalized monitoring to enhance\nmanagement and prevent fraud. It is crucial for ensuring the security and\nefficiency of insurance systems. Although AI and machine learning algorithms\nhave demonstrated strong performance in detecting fraudulent claims, the\nabsence of standardized defense mechanisms renders current systems vulnerable\nto emerging adversarial threats. In this paper, we propose a GAN-based approach\nto conduct adversarial attacks on fraud detection systems. Our results indicate\nthat an attacker, without knowledge of the training data or internal model\ndetails, can generate fraudulent cases that are classified as legitimate with a\n99\\% attack success rate (ASR). By subtly modifying real insurance records and\nclaims, adversaries can significantly increase the fraud risk, potentially\nbypassing compromised detection systems. These findings underscore the urgent\nneed to enhance the robustness of insurance fraud detection models against\nadversarial manipulation, thereby ensuring the stability and reliability of\ndifferent insurance systems."}
{"id": "2506.19874", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.19874", "abs": "https://arxiv.org/abs/2506.19874", "authors": ["Xing Yang", "Bingtao Wang", "Yuhao Wang", "Zimo Ji", "Terry Jingchen Zhang", "Wenyuan Jiang"], "title": "Towards Provable (In)Secure Model Weight Release Schemes", "comment": "8 pages, 2 figures", "summary": "Recent secure weight release schemes claim to enable open-source model\ndistribution while protecting model ownership and preventing misuse. However,\nthese approaches lack rigorous security foundations and provide only informal\nsecurity guarantees. Inspired by established works in cryptography, we\nformalize the security of weight release schemes by introducing several\nconcrete security definitions. We then demonstrate our definition's utility\nthrough a case study of TaylorMLP, a prominent secure weight release scheme.\nOur analysis reveals vulnerabilities that allow parameter extraction thus\nshowing that TaylorMLP fails to achieve its informal security goals. We hope\nthis work will advocate for rigorous research at the intersection of machine\nlearning and security communities and provide a blueprint for how future weight\nrelease schemes should be designed and evaluated."}
{"id": "2506.19877", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.19877", "abs": "https://arxiv.org/abs/2506.19877", "authors": ["Zhaoyang Xu", "Yunbo Liu"], "title": "Robust Anomaly Detection in Network Traffic: Evaluating Machine Learning Models on CICIDS2017", "comment": "submitted to IEEE CNS 2025", "summary": "Identifying suitable machine learning paradigms for intrusion detection\nremains critical for building effective and generalizable security solutions.\nIn this study, we present a controlled comparison of four representative models\n- Multi-Layer Perceptron (MLP), 1D Convolutional Neural Network (CNN),\nOne-Class Support Vector Machine (OCSVM) and Local Outlier Factor (LOF) - on\nthe CICIDS2017 dataset under two scenarios: detecting known attack types and\ngeneralizing to previously unseen threats. Our results show that supervised MLP\nand CNN achieve near-perfect accuracy on familiar attacks but suffer drastic\nrecall drops on novel attacks. Unsupervised LOF attains moderate overall\naccuracy and high recall on unknown threats at the cost of elevated false\nalarms, while boundary-based OCSVM balances precision and recall best,\ndemonstrating robust detection across both scenarios. These findings offer\npractical guidance for selecting IDS models in dynamic network environments."}
{"id": "2506.19889", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.19889", "abs": "https://arxiv.org/abs/2506.19889", "authors": ["Wanli Peng", "Xin Chen", "Hang Fu", "XinYu He", "Xue Yiming", "Juan Wen"], "title": "Retrieval-Confused Generation is a Good Defender for Privacy Violation Attack of Large Language Models", "comment": null, "summary": "Recent advances in large language models (LLMs) have made a profound impact\non our society and also raised new security concerns. Particularly, due to the\nremarkable inference ability of LLMs, the privacy violation attack (PVA),\nrevealed by Staab et al., introduces serious personal privacy issues. Existing\ndefense methods mainly leverage LLMs to anonymize the input query, which\nrequires costly inference time and cannot gain satisfactory defense\nperformance. Moreover, directly rejecting the PVA query seems like an effective\ndefense method, while the defense method is exposed, promoting the evolution of\nPVA. In this paper, we propose a novel defense paradigm based on\nretrieval-confused generation (RCG) of LLMs, which can efficiently and covertly\ndefend the PVA. We first design a paraphrasing prompt to induce the LLM to\nrewrite the \"user comments\" of the attack query to construct a disturbed\ndatabase. Then, we propose the most irrelevant retrieval strategy to retrieve\nthe desired user data from the disturbed database. Finally, the \"data comments\"\nare replaced with the retrieved user data to form a defended query, leading to\nresponding to the adversary with some wrong personal attributes, i.e., the\nattack fails. Extensive experiments are conducted on two datasets and eight\npopular LLMs to comprehensively evaluate the feasibility and the superiority of\nthe proposed defense method."}
{"id": "2506.19892", "categories": ["cs.CR", "cs.AI", "cs.DC", "cs.LG", "cs.PF"], "pdf": "https://arxiv.org/pdf/2506.19892", "abs": "https://arxiv.org/abs/2506.19892", "authors": ["Isaac Marroqui Penalva", "Enrique Tomás Martínez Beltrán", "Manuel Gil Pérez", "Alberto Huertas Celdrán"], "title": "RepuNet: A Reputation System for Mitigating Malicious Clients in DFL", "comment": null, "summary": "Decentralized Federated Learning (DFL) enables nodes to collaboratively train\nmodels without a central server, introducing new vulnerabilities since each\nnode independently selects peers for model aggregation. Malicious nodes may\nexploit this autonomy by sending corrupted models (model poisoning), delaying\nmodel submissions (delay attack), or flooding the network with excessive\nmessages, negatively affecting system performance. Existing solutions often\ndepend on rigid configurations or additional infrastructures such as\nblockchain, leading to computational overhead, scalability issues, or limited\nadaptability. To overcome these limitations, this paper proposes RepuNet, a\ndecentralized reputation system that categorizes threats in DFL and dynamically\nevaluates node behavior using metrics like model similarity, parameter changes,\nmessage latency, and communication volume. Nodes' influence in model\naggregation is adjusted based on their reputation scores. RepuNet was\nintegrated into the Nebula DFL platform and experimentally evaluated with MNIST\nand CIFAR-10 datasets under non-IID distributions, using federations of up to\n25 nodes in both fully connected and random topologies. Different attack\nintensities, frequencies, and activation intervals were tested. Results\ndemonstrated that RepuNet effectively detects and mitigates malicious behavior,\nachieving F1 scores above 95% for MNIST scenarios and approximately 76% for\nCIFAR-10 cases. These outcomes highlight RepuNet's adaptability, robustness,\nand practical potential for mitigating threats in decentralized federated\nlearning environments."}
{"id": "2506.19897", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.19897", "abs": "https://arxiv.org/abs/2506.19897", "authors": ["Christopher Glasz", "Emily Escamilla", "Eric O. Scott", "Anand Patel", "Jacob Zimmer", "Colin Diggs", "Michael Doyle", "Scott Rosen", "Nitin Naik", "Justin F. Brunelle", "Samruddhi Thaker", "Parthav Poudel", "Arun Sridharan", "Amit Madan", "Doug Wendt", "William Macke", "Thomas Schill"], "title": "Can LLMs Replace Humans During Code Chunking?", "comment": null, "summary": "Large language models (LLMs) have become essential tools in computer science,\nespecially for tasks involving code understanding and generation. However,\nexisting work does not address many of the unique challenges presented by code\nwritten for government applications. In particular, government enterprise\nsoftware is often written in legacy languages like MUMPS or assembly language\ncode (ALC) and the overall token lengths of these systems exceed the context\nwindow size for current commercially available LLMs. Additionally, LLMs are\nprimarily trained on modern software languages and have undergone limited\ntesting with legacy languages, making their ability to understand legacy\nlanguages unknown and, hence, an area for empirical study. This paper examines\nthe application of LLMs in the modernization of legacy government code written\nin ALC and MUMPS, addressing the challenges of input limitations. We\ninvestigate various code-chunking methods to optimize the generation of summary\nmodule comments for legacy code files, evaluating the impact of code-chunking\nmethods on the quality of documentation produced by different LLMs, including\nGPT-4o, Claude 3 Sonnet, Mixtral, and Llama 3. Our results indicate that LLMs\ncan select partition points closely aligned with human expert partitioning. We\nalso find that chunking approaches have significant impact on downstream tasks\nsuch as documentation generation. LLM-created partitions produce comments that\nare up to 20% more factual and up to 10% more useful than when humans create\npartitions. Therefore, we conclude that LLMs can be used as suitable\nreplacements for human partitioning of large codebases during LLM-aided\nmodernization."}
{"id": "2506.20159", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.20159", "abs": "https://arxiv.org/abs/2506.20159", "authors": ["Tomas Herda", "Victoria Pichler", "Zheying Zhang", "Pekka Abrahamsson", "Geir K. Hanssen"], "title": "AI and Agile Software Development: From Frustration to Success -- XP2025 Workshop Summary", "comment": null, "summary": "The full-day workshop on AI and Agile at XP 2025 convened a diverse group of\nresearchers and industry practitioners to address the practical challenges and\nopportunities of integrating Artificial Intelligence into Agile software\ndevelopment. Through interactive sessions, participants identified shared\nfrustrations related to integrating AI into Agile Software Development\npractices, including challenges with tooling, governance, data quality, and\ncritical skill gaps. These challenges were systematically prioritized and\nanalyzed to uncover root causes. The workshop culminated in the collaborative\ndevelopment of a research roadmap that pinpoints actionable directions for\nfuture work, including both immediate solutions and ambitious long-term goals.\nThe key outcome is a structured agenda designed to foster joint\nindustry-academic efforts to move from identified frustrations to successful\nimplementation."}
{"id": "2506.20415", "categories": ["cs.CR", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2506.20415", "abs": "https://arxiv.org/abs/2506.20415", "authors": ["Dipayan Saha", "Shams Tarek", "Hasan Al Shaikh", "Khan Thamid Hasan", "Pavan Sai Nalluri", "Md. Ajoad Hasan", "Nashmin Alam", "Jingbo Zhou", "Sujan Kumar Saha", "Mark Tehranipoor", "Farimah Farahmandi"], "title": "SV-LLM: An Agentic Approach for SoC Security Verification using Large Language Models", "comment": null, "summary": "Ensuring the security of complex system-on-chips (SoCs) designs is a critical\nimperative, yet traditional verification techniques struggle to keep pace due\nto significant challenges in automation, scalability, comprehensiveness, and\nadaptability. The advent of large language models (LLMs), with their remarkable\ncapabilities in natural language understanding, code generation, and advanced\nreasoning, presents a new paradigm for tackling these issues. Moving beyond\nmonolithic models, an agentic approach allows for the creation of multi-agent\nsystems where specialized LLMs collaborate to solve complex problems more\neffectively. Recognizing this opportunity, we introduce SV-LLM, a novel\nmulti-agent assistant system designed to automate and enhance SoC security\nverification. By integrating specialized agents for tasks like verification\nquestion answering, security asset identification, threat modeling, test plan\nand property generation, vulnerability detection, and simulation-based bug\nvalidation, SV-LLM streamlines the workflow. To optimize their performance in\nthese diverse tasks, agents leverage different learning paradigms, such as\nin-context learning, fine-tuning, and retrieval-augmented generation (RAG). The\nsystem aims to reduce manual intervention, improve accuracy, and accelerate\nsecurity analysis, supporting proactive identification and mitigation of risks\nearly in the design cycle. We demonstrate its potential to transform hardware\nsecurity practices through illustrative case studies and experiments that\nshowcase its applicability and efficacy."}
{"id": "2506.20551", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.20551", "abs": "https://arxiv.org/abs/2506.20551", "authors": ["Soumya Madireddy", "Lu Gao", "Zia Din", "Kinam Kim", "Ahmed Senouci", "Zhe Han", "Yunpeng Zhang"], "title": "Large Language Model-Driven Code Compliance Checking in Building Information Modeling", "comment": null, "summary": "This research addresses the time-consuming and error-prone nature of manual\ncode compliance checking in Building Information Modeling (BIM) by introducing\na Large Language Model (LLM)-driven approach to semi-automate this critical\nprocess. The developed system integrates LLMs such as GPT, Claude, Gemini, and\nLlama, with Revit software to interpret building codes, generate Python\nscripts, and perform semi-automated compliance checks within the BIM\nenvironment. Case studies on a single-family residential project and an office\nbuilding project demonstrated the system's ability to reduce the time and\neffort required for compliance checks while improving accuracy. It streamlined\nthe identification of violations, such as non-compliant room dimensions,\nmaterial usage, and object placements, by automatically assessing relationships\nand generating actionable reports. Compared to manual methods, the system\neliminated repetitive tasks, simplified complex regulations, and ensured\nreliable adherence to standards. By offering a comprehensive, adaptable, and\ncost-effective solution, this proposed approach offers a promising advancement\nin BIM-based compliance checking, with potential applications across diverse\nregulatory documents in construction projects."}
{"id": "2506.20576", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.20576", "abs": "https://arxiv.org/abs/2506.20576", "authors": ["Sabrine Ennaji", "Elhadj Benkhelifa", "Luigi V. Mancini"], "title": "Vulnerability Disclosure through Adaptive Black-Box Adversarial Attacks on NIDS", "comment": null, "summary": "Adversarial attacks, wherein slight inputs are carefully crafted to mislead\nintelligent models, have attracted increasing attention. However, a critical\ngap persists between theoretical advancements and practical application,\nparticularly in structured data like network traffic, where interdependent\nfeatures complicate effective adversarial manipulations. Moreover, ambiguity in\ncurrent approaches restricts reproducibility and limits progress in this field.\nHence, existing defenses often fail to handle evolving adversarial attacks.\nThis paper proposes a novel approach for black-box adversarial attacks, that\naddresses these limitations. Unlike prior work, which often assumes system\naccess or relies on repeated probing, our method strictly respect black-box\nconstraints, reducing interaction to avoid detection and better reflect\nreal-world scenarios. We present an adaptive feature selection strategy using\nchange-point detection and causality analysis to identify and target sensitive\nfeatures to perturbations. This lightweight design ensures low computational\ncost and high deployability. Our comprehensive experiments show the attack's\neffectiveness in evading detection with minimal interaction, enhancing its\nadaptability and applicability in real-world scenarios. By advancing the\nunderstanding of adversarial attacks in network traffic, this work lays a\nfoundation for developing robust defenses."}
{"id": "2506.20621", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.20621", "abs": "https://arxiv.org/abs/2506.20621", "authors": ["Silvio Alonso", "Antonio Pedro Santos Alves", "Lucas Romao", "Hélio Lopes", "Marcos Kalinowski"], "title": "Define-ML: An Approach to Ideate Machine Learning-Enabled Systems", "comment": "Accepted for publication at the 51st Euromicro Conference Series on\n  Software Engineering and Advanced Applications (SEAA) 2025", "summary": "[Context] The increasing adoption of machine learning (ML) in software\nsystems demands specialized ideation approaches that address ML-specific\nchallenges, including data dependencies, technical feasibility, and alignment\nbetween business objectives and probabilistic system behavior. Traditional\nideation methods like Lean Inception lack structured support for these ML\nconsiderations, which can result in misaligned product visions and unrealistic\nexpectations. [Goal] This paper presents Define-ML, a framework that extends\nLean Inception with tailored activities - Data Source Mapping, Feature-to-Data\nSource Mapping, and ML Mapping - to systematically integrate data and technical\nconstraints into early-stage ML product ideation. [Method] We developed and\nvalidated Define-ML following the Technology Transfer Model, conducting both\nstatic validation (with a toy problem) and dynamic validation (in a real-world\nindustrial case study). The analysis combined quantitative surveys with\nqualitative feedback, assessing utility, ease of use, and intent of adoption.\n[Results] Participants found Define-ML effective for clarifying data concerns,\naligning ML capabilities with business goals, and fostering cross-functional\ncollaboration. The approach's structured activities reduced ideation ambiguity,\nthough some noted a learning curve for ML-specific components, which can be\nmitigated by expert facilitation. All participants expressed the intention to\nadopt Define-ML. [Conclusion] Define-ML provides an openly available, validated\napproach for ML product ideation, building on Lean Inception's agility while\naligning features with available data and increasing awareness of technical\nfeasibility."}
