<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 5]
- [cs.CR](#cs.CR) [Total: 13]
- [cs.AI](#cs.AI) [Total: 15]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Vision: An Extensible Methodology for Formal Software Verification in Microservice Systems](https://arxiv.org/abs/2509.02860)
*Connor Wojtak,Darek Gajewski,Tomas Cerny*

Main category: cs.SE

TL;DR: 通过静态重构微服务源码为形式系统模型，利用SMT约束满足求解器进行形式验证，解决微服务系统在分散开发中的维护性和可靠性问题


<details>
  <summary>Details</summary>
Motivation: 微服务系统分散开发和持续迭代容易导致沟通不畅和实现不兼容，影响系统维护性和可靠性

Method: 静态重构微服务源码为形式系统模型，从模型中派生SMT约束集进行形式验证

Result: 提出了一种可扩展的方法论，支持多个跨切关注点的软件验证，并采用形式推理验证方法的正确性和适用性

Conclusion: 该方法论能够有效解决微服务系统中的维护性和可靠性挑战，并为安全政策等其他跨切关注点的验证提供了基础

Abstract: Microservice systems are becoming increasingly adopted due to their
scalability, decentralized development, and support for continuous integration
and delivery (CI/CD). However, this decentralized development by separate teams
and continuous evolution can introduce miscommunication and incompatible
implementations, undermining system maintainability and reliability across
aspects from security policy to system architecture. We propose a novel
methodology that statically reconstructs microservice source code into a formal
system model. From this model, a Satisfiability Modulo Theories (SMT)
constraint set can be derived, enabling formal verification. Our methodology is
extensible, supporting software verification across multiple cross-cutting
concerns. We focus on applying the methodology to verify the system
architecture concern, presenting formal reasoning to validate the methodology's
correctness and applicability for this concern. Additional concerns such as
security policy implementation are considered. Future directions are
established to extend and evaluate the methodology.

</details>


### [2] [Are We SOLID Yet? An Empirical Study on Prompting LLMs to Detect Design Principle Violations](https://arxiv.org/abs/2509.03093)
*Fatih Pehlivan,Arçin Ülkü Ergüzen,Sahand Moslemi Yengejeh,Mayasah Lami,Anil Koyuncu*

Main category: cs.SE

TL;DR: 本文提出了一种基于提示工程的LLM评估方法，用于检测多语言代码库中的SOLID原则违反情况，发现GPT-4o Mini表现最佳但仍有挑战，提示策略对检测准确率有显著影响。


<details>
  <summary>Details</summary>
Motivation: 传统静态分析方法难以检测需要深入理解面向对象设计模式的语义设计缺陷，现有解决方案通常只关注单个SOLID原则或特定编程语言，缺乏跨所有五个原则的多语言检测能力。

Method: 构建包含240个手动验证代码示例的新基准数据集，测试四种不同的提示策略（零样本、少样本、思维链等），评估四个领先LLM（CodeLlama、DeepSeekCoder、QwenCoder、GPT-4o Mini）检测SOLID违反的能力。

Result: GPT-4o Mini明显优于其他模型，但在DIP等挑战性原则检测上仍有困难；提示策略影响显著但无单一最佳策略；检测准确率受语言特性影响，随代码复杂度增加而急剧下降。

Conclusion: 有效的AI驱动设计分析需要根据具体设计上下文匹配合适的模型和提示策略，而非单一最佳模型，展示了LLMs通过AI辅助代码分析支持可维护性的潜力。

Abstract: Traditional static analysis methods struggle to detect semantic design flaws,
such as violations of the SOLID principles, which require a strong
understanding of object-oriented design patterns and principles. Existing
solutions typically focus on individual SOLID principles or specific
programming languages, leaving a gap in the ability to detect violations across
all five principles in multi-language codebases. This paper presents a new
approach: a methodology that leverages tailored prompt engineering to assess
LLMs on their ability to detect SOLID violations across multiple languages. We
present a benchmark of four leading LLMs-CodeLlama, DeepSeekCoder, QwenCoder,
and GPT-4o Mini-on their ability to detect violations of all five SOLID
principles. For this evaluation, we construct a new benchmark dataset of 240
manually validated code examples. Using this dataset, we test four distinct
prompt strategies inspired by established zero-shot, few-shot, and
chain-of-thought techniques to systematically measure their impact on detection
accuracy. Our emerging results reveal a stark hierarchy among models, with
GPT-4o Mini decisively outperforming others, yet even struggles with
challenging principles like DIP. Crucially, we show that prompt strategy has a
dramatic impact, but no single strategy is universally best; for instance, a
deliberative ENSEMBLE prompt excels at OCP detection while a hint-based EXAMPLE
prompt is superior for DIP violations. Across all experiments, detection
accuracy is heavily influenced by language characteristics and degrades sharply
with increasing code complexity. These initial findings demonstrate that
effective, AI-driven design analysis requires not a single best model, but a
tailored approach that matches the right model and prompt to the specific
design context, highlighting the potential of LLMs to support maintainability
through AI-assisted code analysis.

</details>


### [3] [AI Safety Assurance in Electric Vehicles: A Case Study on AI-Driven SOC Estimation](https://arxiv.org/abs/2509.03270)
*Martin Skoglund,Fredrik Warg,Aria Mirzai,Anders Thorsen,Karl Lundgren,Peter Folkesson,Bastian Havers-zulka*

Main category: cs.SE

TL;DR: 本文探讨了如何结合ISO 26262和ISO/PAS 8800标准对电动汽车AI组件进行独立安全评估，以电池SOC估计为例，通过故障注入实验进行鲁棒性测试。


<details>
  <summary>Details</summary>
Motivation: AI技术在电动汽车中的应用带来了独特的安全保障挑战，传统评估方法无法有效评估基于AI的功能，需要发展新的标准和实践。

Method: 结合ISO 26262功能安全标准和ISO/PAS 8800 AI安全标准，采用故障注入实验方法，通过系统性地引入扰动传感器输入来评估AI组件的鲁棒性。

Result: 确定了扩展评估方法中独立评估的关键特性，成功对AI驱动的电池SOC估计组件进行了鲁棒性测试。

Conclusion: 通过结合现有功能安全标准和新兴AI安全标准，可以建立有效的独立评估框架来确保电动汽车中AI组件的安全性。

Abstract: Integrating Artificial Intelligence (AI) technology in electric vehicles (EV)
introduces unique challenges for safety assurance, particularly within the
framework of ISO 26262, which governs functional safety in the automotive
domain. Traditional assessment methodologies are not geared toward evaluating
AI-based functions and require evolving standards and practices. This paper
explores how an independent assessment of an AI component in an EV can be
achieved when combining ISO 26262 with the recently released ISO/PAS 8800,
whose scope is AI safety for road vehicles. The AI-driven State of Charge (SOC)
battery estimation exemplifies the process. Key features relevant to the
independent assessment of this extended evaluation approach are identified. As
part of the evaluation, robustness testing of the AI component is conducted
using fault injection experiments, wherein perturbed sensor inputs are
systematically introduced to assess the component's resilience to input
variance.

</details>


### [4] [VulnRepairEval: An Exploit-Based Evaluation Framework for Assessing Large Language Model Vulnerability Repair Capabilities](https://arxiv.org/abs/2509.03331)
*Weizhe Wang,Wei Ma,Qiang Hu,Yao Zhang,Jianfei Sun,Bin Wu,Yang Liu,Guangquan Xu,Lingxiao Jiang*

Main category: cs.SE

TL;DR: VulnRepairEval是一个基于功能概念验证(PoC)漏洞利用的评估框架，用于严格评估LLM在软件漏洞修复中的实际性能，发现现有方法存在严重高估问题。


<details>
  <summary>Details</summary>
Motivation: 现有漏洞修复评估数据集主要依赖表面验证而非基于漏洞利用的验证，导致在安全敏感应用中性能被高估，需要更严格的评估框架来反映真实世界场景。

Method: 开发了容器化的VulnRepairEval评估框架，基于功能PoC漏洞利用进行可重现的差异化评估，修复成功要求原始漏洞利用对修改后的代码执行失败。从400多个CVE和2500多个潜在源中提取了23个Python CVE实例。

Result: 对12个流行LLM的评估显示性能严重不足：即使表现最好的模型仅成功修复5/23个实例(约21.7%)。失败分析表明大多数不成功尝试源于漏洞识别不精确和补丁包含语法或语义错误。

Conclusion: 该工作提供了一个严格的实用评估框架，强调了需要真实反映现实世界漏洞利用场景的评估协议的必要性，增强提示策略和多智能体方法改进有限。

Abstract: The adoption of Large Language Models (LLMs) for automated software
vulnerability patching has shown promising outcomes on carefully curated
evaluation sets. Nevertheless, existing datasets predominantly rely on
superficial validation methods rather than exploit-based verification, leading
to overestimated performance in security-sensitive applications. This paper
introduces VulnRepairEval, an evaluation framework anchored in functional
Proof-of-Concept (PoC) exploits. Our framework delivers a comprehensive,
containerized evaluation pipeline that enables reproducible differential
assessment, where repair success requires the original exploit to fail
execution against the modified code. The benchmark construction involved
extensive data curation: we processed over 400 CVEs and approximately 2,500
potential sources to extract a collection of authentic vulnerability instances
(23 Python CVEs) amenable to automated testing with working PoCs. Through
VulnRepairEval, we conduct a comprehensive evaluation of 12 popular LLMs and
observe a significant performance deficit: even the top-performing model
successfully addresses merely 5/23 instances (about 21.7%), exposing critical
weaknesses in security-focused applications. Our failure analysis reveals that
most unsuccessful attempts stem from imprecise vulnerability identification and
patches containing syntactic or semantic errors. Enhanced prompting strategies
and multi-agent approaches yield minimal improvements, with overall
effectiveness remaining largely unaffected. This work contributes a stringent,
practical evaluation framework for LLM-driven vulnerability remediation and
underscores the necessity for assessment protocols that authentically reflect
real-world exploitation scenarios.

</details>


### [5] [The Impact of Critique on LLM-Based Model Generation from Natural Language: The Case of Activity Diagrams](https://arxiv.org/abs/2509.03463)
*Parham Khamsepour,Mark Cole,Ish Ashraf,Sandeep Puri,Mehrdad Sabetzadeh,Shiva Nejati*

Main category: cs.SE

TL;DR: LADEX是一个基于LLM的迭代式生成-批评-精炼管道，用于从自然语言描述中提取活动图。结合算法结构检查和LLM语义检查，显著提高了结构有效性和语义对齐性。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在从自然语言生成模型时需要同时保证结构正确性和语义对齐性的挑战，通过迭代式批评精炼过程提高生成质量。

Method: 使用LLM驱动的critique-refine循环，结合算法结构检查（确保符合格式规则）和LLM语义检查（确保准确反映源文本意图），设计了五种变体进行比较研究。

Result: 实验表明：批评精炼循环相比单次生成显著改善；算法结构检查比纯LLM检查平均提高正确率17.81%和完整度13.24%；算法结构检查+LLM语义检查组合达到最佳性能（正确率86.37%，完整度88.56%），平均只需不到5次LLM调用。

Conclusion: LADEX证明了结合算法结构验证和LLM语义对齐检查的有效性，为自动化模型生成提供了高效可靠的解决方案，在保持高质量输出的同时减少了LLM调用次数。

Abstract: Large Language Models (LLMs) show strong potential for automating the
generation of models from natural-language descriptions. A common approach is
an iterative generate-critique-refine loop, where candidate models are
produced, evaluated, and updated based on detected issues. This process needs
to address: (1) structural correctness - compliance with well-formedness rules
- and (2) semantic alignment - accurate reflection of the intended meaning in
the source text. We present LADEX (LLM-based Activity Diagram Extractor), a
pipeline for deriving activity diagrams from natural-language process
descriptions using an LLM-driven critique-refine process. Structural checks in
LADEX can be performed either algorithmically or by an LLM, while alignment
checks are always performed by an LLM. We design five ablated variants of LADEX
to study: (i) the impact of the critique-refine loop itself, (ii) the role of
LLM-based semantic checks, and (iii) the comparative effectiveness of
algorithmic versus LLM-based structural checks.
  To evaluate LADEX, we compare the generated activity diagrams with
expert-created ground truths using trace-based operational semantics. This
enables automated measurement of correctness and completeness. Experiments on
two datasets indicate that: (1) the critique-refine loop improves structural
validity, correctness, and completeness compared to single-pass generation; (2)
algorithmic structural checks eliminate inconsistencies that LLM-based checks
fail to detect, improving correctness by an average of 17.81% and completeness
by 13.24% over LLM-only checks; and (3) combining algorithmic structural checks
with LLM-based semantic checks, implemented using the reasoning-focused O4
Mini, achieves the best overall performance - yielding average correctness of
up to 86.37% and average completeness of up to 88.56% - while requiring fewer
than five LLM calls on average.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [6] [Secure Password Generator Based on Secure Pseudo-Random Number Generator](https://arxiv.org/abs/2509.02578)
*Abel C. H. Chen*

Main category: cs.CR

TL;DR: 采用加密安全的伪随机数生成器（PRNG）通过多种MAC算法生成安全密码，符合NIST随机性标准


<details>
  <summary>Details</summary>
Motivation: 网站账户和密码泄漏事件频发，密码安全对个人信息保护至关重要，需要提高密码生成的安全性

Method: 使用HMAC、CMAC、KMAC等MAC算法实现加密安全的PRNG来生成随机值，用于密码生成

Result: 通过NIST SP 800-90B标准评估，证明方法满足熵和IID要求，能够生成高随机性和安全性的密码

Conclusion: 提出的方法能够有效生成安全密码，提高系统安全性和个人数据保护水平

Abstract: In recent years, numerous incidents involving the leakage of website accounts
and text passwords (referred to as passwords) have raised significant concerns
regarding the potential exposure of personal information. These events
underscore the critical importance of both information security and password
protection. While many of these breaches are attributable to vulnerabilities
within website infrastructure, the strength and security of the passwords
themselves also play a crucial role. Consequently, the creation of secure
passwords constitutes a fundamental aspect of enhancing overall system security
and protecting personal data. In response to these challenges, this study
presents a secure password generation approach utilizing a cryptographically
secure Pseudo-Random Number Generator (PRNG). The generator is implemented
using a range of Message Authentication Code (MAC) algorithms, including the
Keyed-Hash Message Authentication Code (HMAC), Cipher-based Message
Authentication Code (CMAC), and KECCAK Message Authentication Code (KMAC), to
produce robust random values suitable for password generation. To evaluate the
proposed method, empirical assessments were conducted in accordance with the
guidelines provided in the National Institute of Standards and Technology
(NIST) Special Publication (SP) 800-90B. The evaluation focused on two primary
aspects: entropy estimation and verification of independent and identically
distributed (IID) properties. Experimental results indicate that the proposed
method satisfies both entropy and IID requirements, thereby demonstrating its
ability to generate passwords with a high degree of randomness and security.

</details>


### [7] [Managing Correlations in Data and Privacy Demand](https://arxiv.org/abs/2509.02856)
*Syomantak Chaudhuri,Thomas A. Courtade*

Main category: cs.CR

TL;DR: 提出新的Add-remove异构差分隐私(AHDP)框架，解决传统HDP在数据与隐私需求相关时的不足，提供无需先验相关知识的实用机制


<details>
  <summary>Details</summary>
Motivation: 传统异构差分隐私(HDP)框架假设用户数据与隐私级别不相关，但在实际应用中这种相关性普遍存在，导致隐私保护不足

Method: 提出AHDP框架，通过假设检验形式化隐私保证，设计无需数据-隐私相关先验知识的机制，应用于均值估计、频率估计和线性回归等核心统计任务

Result: AHDP框架对数据-隐私相关性具有鲁棒性，提出的机制实现简单、假设最少，适合实际应用，并通过LLM生成合成数据集进行实证评估

Conclusion: AHDP框架有效解决了传统HDP的局限性，提供了更实用的隐私保护方案，特别是在数据与隐私需求相关的现实场景中

Abstract: Previous works in the differential privacy literature that allow users to
choose their privacy levels typically operate under the heterogeneous
differential privacy (HDP) framework with the simplifying assumption that user
data and privacy levels are not correlated. Firstly, we demonstrate that the
standard HDP framework falls short when user data and privacy demands are
allowed to be correlated. Secondly, to address this shortcoming, we propose an
alternate framework, Add-remove Heterogeneous Differential Privacy (AHDP), that
jointly accounts for user data and privacy preference. We show that AHDP is
robust to possible correlations between data and privacy. Thirdly, we formalize
the guarantees of the proposed AHDP framework through an operational hypothesis
testing perspective. The hypothesis testing setup may be of independent
interest in analyzing other privacy frameworks as well. Fourthly, we show that
there exists non-trivial AHDP mechanisms that notably do not require prior
knowledge of the data-privacy correlations. We propose some such mechanisms and
apply them to core statistical tasks such as mean estimation, frequency
estimation, and linear regression. The proposed mechanisms are simple to
implement with minimal assumptions and modeling requirements, making them
attractive for real-world use. Finally, we empirically evaluate proposed AHDP
mechanisms, highlighting their trade-offs using LLM-generated synthetic
datasets, which we release for future research.

</details>


### [8] [Efficient Privacy-Preserving Recommendation on Sparse Data using Fully Homomorphic Encryption](https://arxiv.org/abs/2509.03024)
*Moontaha Nishat Chowdhury,André Bauer,Minxuan Zhou*

Main category: cs.CR

TL;DR: 使用压缩稀疏行(CSR)表示法结合全同态加密(FHE)的矩阵分解方法，高效处理推荐系统中的稀疏矩阵，在保护用户隐私的同时降低通信开销并保持推荐准确性。


<details>
  <summary>Details</summary>
Motivation: 推荐系统依赖敏感数据引发隐私担忧，FHE能够保护数据安全，但直接处理大型稀疏矩阵在FHE中计算成本高昂，通信开销也是重要挑战。

Method: 提出结合CSR表示法和FHE基于矩阵分解的新方法，在加密域中高效处理矩阵稀疏性，同时最小化通信成本。

Result: 实验结果显示在加密数据上实现了高推荐准确性，同时达到了最低的通信开销。

Conclusion: 该方法能够有效保护用户隐私，在加密域中高效处理稀疏矩阵，为安全推荐系统提供了可行解决方案。

Abstract: In today's data-driven world, recommendation systems personalize user
experiences across industries but rely on sensitive data, raising privacy
concerns. Fully homomorphic encryption (FHE) can secure these systems, but a
significant challenge in applying FHE to recommendation systems is efficiently
handling the inherently large and sparse user-item rating matrices. FHE
operations are computationally intensive, and naively processing various sparse
matrices in recommendation systems would be prohibitively expensive.
Additionally, the communication overhead between parties remains a critical
concern in encrypted domains. We propose a novel approach combining Compressed
Sparse Row (CSR) representation with FHE-based matrix factorization that
efficiently handles matrix sparsity in the encrypted domain while minimizing
communication costs. Our experimental results demonstrate high recommendation
accuracy with encrypted data while achieving the lowest communication costs,
effectively preserving user privacy.

</details>


### [9] [TraceLLM: Security Diagnosis Through Traces and Smart Contracts in Ethereum](https://arxiv.org/abs/2509.03037)
*Shuzheng Wang,Yue Huang,Zhuoer Xu,Yuming Huang,Jing Tang*

Main category: cs.CR

TL;DR: TraceLLM是一个利用LLM整合执行追踪和反编译代码的智能合约安全分析框架，能自动识别攻击路径并生成安全报告，在准确性和泛化性方面显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决现有智能合约安全分析方法的两大局限：异常交易检测无法深入分析执行追踪中的具体攻击策略，代码漏洞检测无法分析未验证合约且难以展示实际攻击利用方式，导致分析师仍需手动对齐交易追踪和代码来重建攻击场景。

Method: 提出TraceLLM框架，结合LLM整合执行追踪级检测和反编译合约代码；引入新的异常执行路径识别算法和LLM精炼的反编译工具，识别易受攻击函数并为LLM提供明确攻击路径。

Result: 在27个有专家真实报告的案例中，识别攻击者和受害者地址精度达85.19%，自动报告事实精度达70.37%，比最佳基线准确率高25.93%；在148个真实以太坊事件中，自动生成报告专家验证准确率达66.22%。

Conclusion: TraceLLM建立了首个联合追踪和合约代码驱动的安全分析基准，展示了强大的泛化能力，为智能合约安全分析提供了有效的自动化解决方案。

Abstract: Ethereum smart contracts hold tens of billions of USD in DeFi and NFTs, yet
comprehensive security analysis remains difficult due to unverified code,
proxy-based architectures, and the reliance on manual inspection of complex
execution traces. Existing approaches fall into two main categories: anomaly
transaction detection, which flags suspicious transactions but offers limited
insight into specific attack strategies hidden in execution traces inside
transactions, and code vulnerability detection, which cannot analyze unverified
contracts and struggles to show how identified flaws are exploited in real
incidents. As a result, analysts must still manually align transaction traces
with contract code to reconstruct attack scenarios and conduct forensics. To
address this gap, TraceLLM is proposed as a framework that leverages LLMs to
integrate execution trace-level detection with decompiled contract code. We
introduce a new anomaly execution path identification algorithm and an
LLM-refined decompile tool to identify vulnerable functions and provide
explicit attack paths to LLM. TraceLLM establishes the first benchmark for
joint trace and contract code-driven security analysis. For comparison, proxy
baselines are created by jointly transmitting the results of three
representative code analysis along with raw traces to LLM. TraceLLM identifies
attacker and victim addresses with 85.19\% precision and produces automated
reports with 70.37\% factual precision across 27 cases with ground truth expert
reports, achieving 25.93\% higher accuracy than the best baseline. Moreover,
across 148 real-world Ethereum incidents, TraceLLM automatically generates
reports with 66.22\% expert-verified accuracy, demonstrating strong
generalizability.

</details>


### [10] [EverTracer: Hunting Stolen Large Language Models via Stealthy and Robust Probabilistic Fingerprint](https://arxiv.org/abs/2509.03058)
*Zhenhua Xu,Meng Han,Wenpeng Xing*

Main category: cs.CR

TL;DR: EverTracer是一个新颖的灰盒指纹框架，通过重新利用成员推理攻击(MIA)进行防御性使用，将所有权信号通过记忆化嵌入，而不是人工触发输出过拟合，实现隐蔽且鲁棒的模型溯源。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型(LLM)的普及加剧了模型盗窃和许可证违规的担忧，需要强大且隐蔽的所有权验证方法。现有指纹方法要么需要不切实际的白盒访问，要么引入可检测的统计异常。

Method: EverTracer包括指纹注入和验证两个阶段：指纹注入阶段在任何自然语言数据上微调模型而不产生可检测的伪影；验证阶段利用校准的概率变化信号来区分指纹模型。

Result: 跨架构的广泛实验证明EverTracer在最先进的效能、隐蔽性和韧性方面表现优异，能够抵抗包括输入级修改和模型级修改在内的自适应对抗攻击。

Conclusion: EverTracer为保护LLM知识产权提供了一个实用的解决方案，通过记忆化嵌入所有权信号而非人工触发过拟合，实现了隐蔽且鲁棒的模型溯源。

Abstract: The proliferation of large language models (LLMs) has intensified concerns
over model theft and license violations, necessitating robust and stealthy
ownership verification. Existing fingerprinting methods either require
impractical white-box access or introduce detectable statistical anomalies. We
propose EverTracer, a novel gray-box fingerprinting framework that ensures
stealthy and robust model provenance tracing. EverTracer is the first to
repurpose Membership Inference Attacks (MIAs) for defensive use, embedding
ownership signals via memorization instead of artificial trigger-output
overfitting. It consists of Fingerprint Injection, which fine-tunes the model
on any natural language data without detectable artifacts, and Verification,
which leverages calibrated probability variation signal to distinguish
fingerprinted models. This approach remains robust against adaptive
adversaries, including input level modification, and model-level modifications.
Extensive experiments across architectures demonstrate EverTracer's
state-of-the-art effectiveness, stealthness, and resilience, establishing it as
a practical solution for securing LLM intellectual property. Our code and data
are publicly available at https://github.com/Xuzhenhua55/EverTracer.

</details>


### [11] [Compressed verification for post-quantum signatures with long-term public keys](https://arxiv.org/abs/2509.03098)
*Gustavo Banegas,Anaëlle Le Dévéhat,Benjamin Smith*

Main category: cs.CR

TL;DR: 提出一种压缩GPV风格签名方案中大公钥的方法，用更小的私钥验证密钥替代，显著减少验证者存储和运行时间，同时保持安全性


<details>
  <summary>Details</summary>
Motivation: 后量子签名方案（如Wave和Squirrels）虽然具有长期安全性优势，但公钥尺寸极大，增加了存储成本和验证时间，需要解决大公钥带来的效率问题

Method: 使用较小的私钥验证密钥来替代传统的大公钥，在GPV风格的签名方案中实现公钥压缩

Result: 成功压缩Squirrels-I密钥从665 kB到20.7 kB，Wave822密钥从3.5 MB到207.97 kB，显著减少了验证存储和运行时间

Conclusion: 该方法有效解决了后量子签名方案中大公钥的问题，在保持安全性的同时大幅提升了验证效率，适用于根证书、安全软件更新等需要长期密钥的应用场景

Abstract: Many signature applications-such as root certificates, secure software
updates, and authentication protocols-involve long-lived public keys that are
transferred or installed once and then used for many verifications. This key
longevity makes post-quantum signature schemes with conservative assumptions
(e.g., structure-free lattices) attractive for long-term security. But many
such schemes, especially those with short signatures, suffer from extremely
large public keys. Even in scenarios where bandwidth is not a major concern,
large keys increase storage costs and slow down verification. We address this
with a method to replace large public keys in GPV-style signatures with
smaller, private verification keys. This significantly reduces verifier storage
and runtime while preserving security. Applied to the conservative,
short-signature schemes Wave and Squirrels, our method compresses Squirrels-I
keys from 665 kB to 20.7 kB and Wave822 keys from 3.5 MB to 207.97 kB.

</details>


### [12] [PromptCOS: Towards System Prompt Copyright Auditing for LLMs via Content-level Output Similarity](https://arxiv.org/abs/2509.03117)
*Yuchen Yang,Yiming Li,Hongwei Yao,Enhao Huang,Shuo Shao,Bingrun Yang,Zhibo Wang,Dacheng Tao,Zhan Qin*

Main category: cs.CR

TL;DR: PromptCOS是一种基于内容级别输出相似性的提示词版权审计方法，通过嵌入水印和优化验证查询来保护LLM系统提示词版权，具有高有效性、强区分性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的快速发展促进了基于LLM的应用开发，但系统提示词容易被盗用和滥用，现有水印方法依赖中间LLM输出，实用性有限。

Method: 通过优化提示词同时协同优化特殊验证查询和内容级别信号标记，利用循环输出信号和注入辅助令牌，并加入覆盖令牌保护水印不被恶意删除。

Result: 实验结果显示方法达到99.3%的平均水印相似度，比最佳基线高60.8%的区分性，精度下降不超过0.58%，对三种攻击具有鲁棒性，计算成本降低98.1%。

Conclusion: PromptCOS提供了一种实用有效的内容级别提示词版权保护方案，解决了现有方法的局限性，具有很好的实际应用价值。

Abstract: The rapid progress of large language models (LLMs) has greatly enhanced
reasoning tasks and facilitated the development of LLM-based applications. A
critical factor in improving LLM-based applications is the design of effective
system prompts, which significantly impact the behavior and output quality of
LLMs. However, system prompts are susceptible to theft and misuse, which could
undermine the interests of prompt owners. Existing methods protect prompt
copyrights through watermark injection and verification but face challenges due
to their reliance on intermediate LLM outputs (e.g., logits), which limits
their practical feasibility.
  In this paper, we propose PromptCOS, a method for auditing prompt copyright
based on content-level output similarity. It embeds watermarks by optimizing
the prompt while simultaneously co-optimizing a special verification query and
content-level signal marks. This is achieved by leveraging cyclic output
signals and injecting auxiliary tokens to ensure reliable auditing in
content-only scenarios. Additionally, it incorporates cover tokens to protect
the watermark from malicious deletion. For copyright verification, PromptCOS
identifies unauthorized usage by comparing the similarity between the
suspicious output and the signal mark. Experimental results demonstrate that
our method achieves high effectiveness (99.3% average watermark similarity),
strong distinctiveness (60.8% greater than the best baseline), high fidelity
(accuracy degradation of no more than 0.58%), robustness (resilience against
three types of potential attacks), and computational efficiency (up to 98.1%
reduction in computational cost). Our code is available at GitHub
https://github.com/LianPing-cyber/PromptCOS.

</details>


### [13] [Kangaroo: A Private and Amortized Inference Framework over WAN for Large-Scale Decision Tree Evaluation](https://arxiv.org/abs/2509.03123)
*Wei Xu,Hui Zhu,Yandong Zheng,Song Bian,Ning Sun,Hao Yuan,Dengguo Feng,Hui Li*

Main category: cs.CR

TL;DR: Kangaroo是一个基于打包同态加密的隐私保护决策树推理框架，通过创新的模型隐藏和编码方案，实现了计算和通信开销的完全摊销，在WAN环境下比现有方案快14-59倍。


<details>
  <summary>Details</summary>
Motivation: 随着模型即服务的快速采用，数据和模型隐私问题日益重要。现有的私有决策树评估方案存在显著限制：其通信和计算成本随树数量、节点数或树深度线性增长，在大规模模型和WAN网络中效率低下。

Method: 提出Kangaroo框架，基于打包同态加密设计新颖的模型隐藏和编码方案，包含安全特征选择、不经意比较和安全路径评估协议，支持相同模型共享、延迟感知和自适应编码调整等优化策略。

Result: 在WAN环境下比最先进的一轮交互方案性能提升14-59倍，大规模决策树推理任务比现有方案快3-44倍，可在约60毫秒每棵树的摊销时间内评估包含969棵树和411825个节点的随机森林。

Conclusion: Kangaroo通过完全摊销开销的创新设计，显著提升了私有决策树评估的效率，特别适合大规模模型和WAN网络环境下的隐私保护推理需求。

Abstract: With the rapid adoption of Models-as-a-Service, concerns about data and model
privacy have become increasingly critical. To solve these problems, various
privacy-preserving inference schemes have been proposed. In particular, due to
the efficiency and interpretability of decision trees, private decision tree
evaluation (PDTE) has garnered significant attention. However, existing PDTE
schemes suffer from significant limitations: their communication and
computation costs scale with the number of trees, the number of nodes, or the
tree depth, which makes them inefficient for large-scale models, especially
over WAN networks. To address these issues, we propose Kangaroo, a private and
amortized decision tree inference framework build upon packed homomorphic
encryption. Specifically, we design a novel model hiding and encoding scheme,
together with secure feature selection, oblivious comparison, and secure path
evaluation protocols, enabling full amortization of the overhead as the number
of nodes or trees scales. Furthermore, we enhance the performance and
functionality of the framework through optimizations, including
same-sharing-for-same-model, latency-aware, and adaptive encoding adjustment
strategies. Kangaroo achieves a $14\times$ to $59\times$ performance
improvement over state-of-the-art (SOTA) one-round interactive schemes in WAN
environments. For large-scale decision tree inference tasks, it delivers a
$3\times$ to $44\times$ speedup compared to existing schemes. Notably, Kangaroo
enables the evaluation of a random forest with $969$ trees and $411825$ nodes
in approximately $60$ ms per tree (amortized) under WAN environments.

</details>


### [14] [A Comprehensive Guide to Differential Privacy: From Theory to User Expectations](https://arxiv.org/abs/2509.03294)
*Napsu Karmitsa,Antti Airola,Tapio Pahikkala,Tinja Pitkämäki*

Main category: cs.CR

TL;DR: 关于差分隐私(DP)的全面综述，涵盖理论基础、实践机制和实际应用，重点关注隐私保护机器学习和合成数据生成领域的挑战


<details>
  <summary>Details</summary>
Motivation: 个人数据可用性增加带来隐私风险，需要应对重新识别攻击和法律法规要求，差分隐私提供了数学严谨的隐私保护框架

Method: 综述分析方法，系统梳理差分隐私的理论基础、算法工具、领域特定应用挑战，以及可用性和透明度问题

Result: 提供了差分隐私的全面技术概览，识别了各应用领域的特定挑战，强调了系统可用性和沟通透明度的改进需求

Conclusion: 该综述旨在支持研究者和从业者在数据隐私不断发展的背景下，做出明智的差分隐私采用决策

Abstract: The increasing availability of personal data has enabled significant advances
in fields such as machine learning, healthcare, and cybersecurity. However,
this data abundance also raises serious privacy concerns, especially in light
of powerful re-identification attacks and growing legal and ethical demands for
responsible data use. Differential privacy (DP) has emerged as a principled,
mathematically grounded framework for mitigating these risks. This review
provides a comprehensive survey of DP, covering its theoretical foundations,
practical mechanisms, and real-world applications. It explores key algorithmic
tools and domain-specific challenges - particularly in privacy-preserving
machine learning and synthetic data generation. The report also highlights
usability issues and the need for improved communication and transparency in DP
systems. Overall, the goal is to support informed adoption of DP by researchers
and practitioners navigating the evolving landscape of data privacy.

</details>


### [15] [Exposing Privacy Risks in Anonymizing Clinical Data: Combinatorial Refinement Attacks on k-Anonymity Without Auxiliary Information](https://arxiv.org/abs/2509.03350)
*Somiya Chhillar,Mary K. Righi,Rebecca E. Sutter,Evgenios M. Kornaropoulos*

Main category: cs.CR

TL;DR: 这篇论文提出了组合精炼攻击(CRA)，证明即使没有外部辅助信息，k-匿名化在应用ARX软件进行局部编码时仍然存在严重隐私漏洞。


<details>
  <summary>Details</summary>
Motivation: 尽管k-匿名化已被很多专家批评，但因其简单性、符合监管要求和数据效用性，仍被广泛使用。论文要批判一种常见误解：认为在没有辅助信息的情况下，k-匿名化是安全的。

Method: 设计了组合精炼攻击(CRA)，这是首个不依赖外部辅助信息或数据分布假设的攻击方法。CRA利用ARX软件在局部编码匿名化中的效用优化行为，通过线性规划大大缩小敏感值的可能空间。

Result: 在真实临床微观数据上的实验结果显示，即使没有外部信息，现有的匿名化框架也无法提供所承诺的隐私保护水平，引发了严重的隐私担忧。

Conclusion: 这项研究强调了k-匿名化存在本质性的隐私弱点，尤其是在使用局部编码方法时。研究结果对医疗健康等敏感领域的数据匿名化实践报告了警报，并建议重新评估目前的数据保护标准和方法。

Abstract: Despite longstanding criticism from the privacy community, k-anonymity
remains a widely used standard for data anonymization, mainly due to its
simplicity, regulatory alignment, and preservation of data utility. However,
non-experts often defend k-anonymity on the grounds that, in the absence of
auxiliary information, no known attacks can compromise its protections. In this
work, we refute this claim by introducing Combinatorial Refinement Attacks
(CRA), a new class of privacy attacks targeting k-anonymized datasets produced
using local recoding. This is the first method that does not rely on external
auxiliary information or assumptions about the underlying data distribution.
CRA leverages the utility-optimizing behavior of local recoding anonymization
of ARX, which is a widely used open-source software for anonymizing data in
clinical settings, to formulate a linear program that significantly reduces the
space of plausible sensitive values. To validate our findings, we partnered
with a network of free community health clinics, an environment where (1)
auxiliary information is indeed hard to find due to the population they serve
and (2) open-source k-anonymity solutions are attractive due to regulatory
obligations and limited resources. Our results on real-world clinical microdata
reveal that even in the absence of external information, established
anonymization frameworks do not deliver the promised level of privacy, raising
critical privacy concerns.

</details>


### [16] [Tuning Block Size for Workload Optimization in Consortium Blockchain Networks](https://arxiv.org/abs/2509.03367)
*Narges Dadkhah,Somayeh Mohammadi,Gerhard Wunder*

Main category: cs.CR

TL;DR: 提出基于机器学习和遗传算法的数学模型，用于确定Hyperledger Fabric区块链的最优区块大小配置，以最大化系统性能


<details>
  <summary>Details</summary>
Motivation: 区块大小对区块链系统性能影响存在争议，常导致网络分叉，需要科学方法确定最优区块大小配置

Method: 建立数学模型，利用机器学习分析区块大小、交易大小和网络容量等因素对区块处理时间的影响，使用遗传算法求解最优解

Result: 开发出能够在部署前精确调整区块大小配置的优化求解器，确保系统从一开始就获得改进的性能

Conclusion: 该方法能有效平衡区块处理效率、网络延迟和系统吞吐量，为不同业务场景提供稳健的区块链性能改进方案

Abstract: Determining the optimal block size is crucial for achieving high throughput
in blockchain systems. Many studies have focused on tuning various components,
such as databases, network bandwidth, and consensus mechanisms. However, the
impact of block size on system performance remains a topic of debate, often
resulting in divergent views and even leading to new forks in blockchain
networks. This research proposes a mathematical model to maximize performance
by determining the ideal block size for Hyperledger Fabric, a prominent
consortium blockchain. By leveraging machine learning and solving the model
with a genetic algorithm, the proposed approach assesses how factors such as
block size, transaction size, and network capacity influence the block
processing time. The integration of an optimization solver enables precise
adjustments to block size configuration before deployment, ensuring improved
performance from the outset. This systematic approach aims to balance block
processing efficiency, network latency, and system throughput, offering a
robust solution to improve blockchain performance across diverse business
contexts.

</details>


### [17] [Federated Learning: An approach with Hybrid Homomorphic Encryption](https://arxiv.org/abs/2509.03427)
*Pedro Correia,Ivan Silva,Ivone Amorim,Eva Maia,Isabel Praça*

Main category: cs.CR

TL;DR: 首个混合同态加密(HHE)框架用于联邦学习，结合PASTA对称加密和BFV同态加密，在保持高准确率的同时大幅降低客户端带宽和运行时间，但服务器计算成本显著增加。


<details>
  <summary>Details</summary>
Motivation: 联邦学习虽然保证数据本地存储，但梯度重构和成员推断攻击仍然可能泄漏信息。完全同态加密(FHE)能解决隐私问题，但存在加密文本扩展和资源消耗过大的问题。

Method: 提出混合同态加密(HHE)框架，将PASTA对称加密与BFV FHE方案结合。客户端使用PASTA加密本地模型更新，并将轻量级加密文本和BFV加密的PASTA密钥发送到服务器，服务器进行同态计算PASTA解密电路并聚合结果。

Result: 在MNIST数据集上，系统达到97.6%的准确率，仅比明文低1.3%，客户端上传带宽减少2000倍，运行时间减少30%，但服务器计算成本增加15621倍。

Conclusion: 该HHE框架在保持高准确率的同时，显著减少了客户端资源消耗，但需要解决服务器计算成本过高的挑战。

Abstract: Federated Learning (FL) is a distributed machine learning approach that
promises privacy by keeping the data on the device. However, gradient
reconstruction and membership-inference attacks show that model updates still
leak information. Fully Homomorphic Encryption (FHE) can address those privacy
concerns but it suffers from ciphertext expansion and requires prohibitive
overhead on resource-constrained devices. We propose the first Hybrid
Homomorphic Encryption (HHE) framework for FL that pairs the PASTA symmetric
cipher with the BFV FHE scheme. Clients encrypt local model updates with PASTA
and send both the lightweight ciphertexts and the PASTA key (itself
BFV-encrypted) to the server, which performs a homomorphic evaluation of the
decryption circuit of PASTA and aggregates the resulting BFV ciphertexts. A
prototype implementation, developed on top of the Flower FL framework, shows
that on independently and identically distributed MNIST dataset with 12 clients
and 10 training rounds, the proposed HHE system achieves 97.6% accuracy, just
1.3% below plaintext, while reducing client upload bandwidth by over 2,000x and
cutting client runtime by 30% compared to a system based solely on the BFV FHE
scheme. However, server computational cost increases by roughly 15621x for each
client participating in the training phase, a challenge to be addressed in
future work.

</details>


### [18] [Evaluating Diverse Feature Extraction Techniques of Multifaceted IoT Malware Analysis: A Survey](https://arxiv.org/abs/2509.03442)
*Zhuoyun Qian,Hongyi Miao,Yili Jiang,Qin Hu,Jiaqi Huang,Cheng Zhang,Fangtian Zhong*

Main category: cs.CR

TL;DR: 本文对物联网恶意软件分析中的特征提取技术进行了全面综述，涵盖了静态、动态和混合方法，以及基于图学习的特征表示策略，并比较了现有技术的优缺点和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着物联网设备的普及，其可靠性受到安全问题的严重制约。现有的恶意软件分析技术在实践应用中仍面临重大挑战，需要对特征提取方法进行系统性的综述和分析。

Method: 采用文献综述方法，从多个角度系统分析物联网恶意软件分析中的特征提取技术：首先考察静态和动态特征提取方法，然后是混合方法，接着探索基于图学习的特征表示策略。

Result: 对现有特征提取技术进行了全面比较，识别了各种方法的优势和局限性，为研究人员提供了系统的技术路线图。

Conclusion: 指出了当前技术面临的开放挑战，并提出了未来研究的 promising 方向，为物联网恶意软件分析领域的发展提供了重要指导。

Abstract: As IoT devices continue to proliferate, their reliability is increasingly
constrained by security concerns. In response, researchers have developed
diverse malware analysis techniques to detect and classify IoT malware. These
techniques typically rely on extracting features at different levels from IoT
applications, giving rise to a wide range of feature extraction methods.
However, current approaches still face significant challenges when applied in
practice. This survey provides a comprehensive review of feature extraction
techniques for IoT malware analysis from multiple perspectives. We first
examine static and dynamic feature extraction methods, followed by hybrid
approaches. We then explore feature representation strategies based on graph
learning. Finally, we compare the strengths and limitations of existing
techniques, highlight open challenges, and outline promising directions for
future research.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [19] [Can Media Act as a Soft Regulator of Safe AI Development? A Game Theoretical Analysis](https://arxiv.org/abs/2509.02650)
*Henrique Correia da Fonseca,António Fernandes,Zhao Song,Theodor Cimpeanu,Nataliya Balabanova,Adeela Bashir,Paolo Bova,Alessio Buscemi,Alessandro Di Stefano,Manh Hong Duong,Elias Fernandez Domingos,Ndidi Bianca Ogbo,Simon T. Powers,Daniele Proverbio,Zia Ush Shamszaman,Fernando P. Santos,The Anh Han,Marcus Krellner*

Main category: cs.AI

TL;DR: 程序员在利润与安全问题上常选择利润，媒体报道通过影响品牌声誉可以促使AI创造者采取安全措施，但需要充分可靠的信息质量和合理的成本控制


<details>
  <summary>Details</summary>
Motivation: 探索如何解决AI开发者在利润与用户安全之间的选择困境，研究媒体报道作为一种软性监管机制的效果

Method: 使用进化游戏理论创建人工群体，包含自利的创造者和用户，分析媒体报道对他们行为的影响

Result: 媒体能够促进创造者与用户之间的合作，但需要充分可靠的信息质量和合理的成本控制，否则合作无法形成

Conclusion: 媒体通过形成公众意见和让开发者承担责任，可以成为一种强大的软性监管机制，在没有政府正式监管的情况下也能促进AI安全发展

Abstract: When developers of artificial intelligence (AI) products need to decide
between profit and safety for the users, they likely choose profit.
Untrustworthy AI technology must come packaged with tangible negative
consequences. Here, we envisage those consequences as the loss of reputation
caused by media coverage of their misdeeds, disseminated to the public. We
explore whether media coverage has the potential to push AI creators into the
production of safe products, enabling widespread adoption of AI technology. We
created artificial populations of self-interested creators and users and
studied them through the lens of evolutionary game theory. Our results reveal
that media is indeed able to foster cooperation between creators and users, but
not always. Cooperation does not evolve if the quality of the information
provided by the media is not reliable enough, or if the costs of either
accessing media or ensuring safety are too high. By shaping public perception
and holding developers accountable, media emerges as a powerful soft regulator
-- guiding AI safety even in the absence of formal government oversight.

</details>


### [20] [The Future of Artificial Intelligence and the Mathematical and Physical Sciences (AI+MPS)](https://arxiv.org/abs/2509.02661)
*Andrew Ferguson,Marisa LaFleur,Lars Ruthotto,Jesse Thaler,Yuan-Sen Ting,Pratyush Tiwary,Soledad Villar,E. Paulo Alves,Jeremy Avigad,Simon Billinge,Camille Bilodeau,Keith Brown,Emmanuel Candes,Arghya Chattopadhyay,Bingqing Cheng,Jonathan Clausen,Connor Coley,Andrew Connolly,Fred Daum,Sijia Dong,Chrisy Xiyu Du,Cora Dvorkin,Cristiano Fanelli,Eric B. Ford,Luis Manuel Frutos,Nicolás García Trillos,Cecilia Garraffo,Robert Ghrist,Rafael Gomez-Bombarelli,Gianluca Guadagni,Sreelekha Guggilam,Sergei Gukov,Juan B. Gutiérrez,Salman Habib,Johannes Hachmann,Boris Hanin,Philip Harris,Murray Holland,Elizabeth Holm,Hsin-Yuan Huang,Shih-Chieh Hsu,Nick Jackson,Olexandr Isayev,Heng Ji,Aggelos Katsaggelos,Jeremy Kepner,Yannis Kevrekidis,Michelle Kuchera,J. Nathan Kutz,Branislava Lalic,Ann Lee,Matt LeBlanc,Josiah Lim,Rebecca Lindsey,Yongmin Liu,Peter Y. Lu,Sudhir Malik,Vuk Mandic,Vidya Manian,Emeka P. Mazi,Pankaj Mehta,Peter Melchior,Brice Ménard,Jennifer Ngadiuba,Stella Offner,Elsa Olivetti,Shyue Ping Ong,Christopher Rackauckas,Philippe Rigollet,Chad Risko,Philip Romero,Grant Rotskoff,Brett Savoie,Uros Seljak,David Shih,Gary Shiu,Dima Shlyakhtenko,Eva Silverstein,Taylor Sparks,Thomas Strohmer,Christopher Stubbs,Stephen Thomas,Suriyanarayanan Vaikuntanathan,Rene Vidal,Francisco Villaescusa-Navarro,Gregory Voth,Benjamin Wandelt,Rachel Ward,Melanie Weber,Risa Wechsler,Stephen Whitelam,Olaf Wiest,Mike Williams,Zhuoran Yang,Yaroslava G. Yingling,Bin Yu,Shuwen Yue,Ann Zabludoff,Huimin Zhao,Tong Zhang*

Main category: cs.AI

TL;DR: NSF研讨会报告，探讨AI与数学物理科学（MPS）领域的双向融合策略，提出加强AI+MPS研究、建设跨学科社区、培养人才三大优先事项


<details>
  <summary>Details</summary>
Motivation: 把握AI快速发展机遇，加强数学物理科学领域与AI的双向互动——既利用AI推动科学发现，又用基础科学概念影响AI发展

Method: 通过NSF研讨会形成社区共识，提出三方面战略：双向AI+MPS研究、跨学科社区建设、教育和人才培养

Result: 制定了资助机构、教育机构和研究人员的具体优先事项建议，旨在使MPS社区在AI+MPS变革中占据领导地位

Conclusion: 现在是加强AI与科学联系的关键时刻，需要采取积极主动的战略来充分发挥AI+MPS的变革潜力

Abstract: This community paper developed out of the NSF Workshop on the Future of
Artificial Intelligence (AI) and the Mathematical and Physics Sciences (MPS),
which was held in March 2025 with the goal of understanding how the MPS domains
(Astronomy, Chemistry, Materials Research, Mathematical Sciences, and Physics)
can best capitalize on, and contribute to, the future of AI. We present here a
summary and snapshot of the MPS community's perspective, as of Spring/Summer
2025, in a rapidly developing field. The link between AI and MPS is becoming
increasingly inextricable; now is a crucial moment to strengthen the link
between AI and Science by pursuing a strategy that proactively and thoughtfully
leverages the potential of AI for scientific discovery and optimizes
opportunities to impact the development of AI by applying concepts from
fundamental science. To achieve this, we propose activities and strategic
priorities that: (1) enable AI+MPS research in both directions; (2) build up an
interdisciplinary community of AI+MPS researchers; and (3) foster education and
workforce development in AI for MPS researchers and students. We conclude with
a summary of suggested priorities for funding agencies, educational
institutions, and individual researchers to help position the MPS community to
be a leader in, and take full advantage of, the transformative potential of
AI+MPS.

</details>


### [21] [Planning with Reasoning using Vision Language World Model](https://arxiv.org/abs/2509.02722)
*Delong Chen,Theo Moutakanni,Willy Chung,Yejin Bang,Ziwei Ji,Allen Bolourchi,Pascale Fung*

Main category: cs.AI

TL;DR: VLWM是一个基于视觉语言的世界模型，通过语言建模在自然视频上进行训练，能够进行语义和时间抽象的动作推理，在视觉规划任务中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 当前高层次的世界模型在理解和推理具有语义和时间抽象的动作方面仍然不够发达，需要开发能够进行有效规划的世界模型。

Method: 使用视觉观察输入，VLWM首先推断整体目标达成情况，然后预测包含交错动作和世界状态变化的轨迹。通过基于Tree of Captions的LLM自优化迭代提取目标，同时学习动作策略和动力学模型，支持系统1的反应式计划解码和系统2的基于成本最小化的反思式规划。

Result: VLWM在Visual Planning for Assistance基准评估和PlannerArena人类评估中达到最先进性能，系统2相比系统1提升Elo分数27%。在RoboVQA和WorldPrediction基准上也优于强VLM基线。

Conclusion: VLWM展示了语言基础的世界模型在视觉规划任务中的有效性，通过结合反应式和反思式规划机制，实现了语义和时间抽象层面的有效推理和规划能力。

Abstract: Effective planning requires strong world models, but high-level world models
that can understand and reason about actions with semantic and temporal
abstraction remain largely underdeveloped. We introduce the Vision Language
World Model (VLWM), a foundation model trained for language-based world
modeling on natural videos. Given visual observations, the VLWM first infers
the overall goal achievements then predicts a trajectory composed of
interleaved actions and world state changes. Those targets are extracted by
iterative LLM Self-Refine conditioned on compressed future observations
represented by Tree of Captions. The VLWM learns both an action policy and a
dynamics model, which respectively facilitates reactive system-1 plan decoding
and reflective system-2 planning via cost minimization. The cost evaluates the
semantic distance between the hypothetical future states given by VLWM
roll-outs and the expected goal state, and is measured by a critic model that
we trained in a self-supervised manner. The VLWM achieves state-of-the-art
Visual Planning for Assistance (VPA) performance on both benchmark evaluations
and our proposed PlannerArena human evaluations, where system-2 improves the
Elo score by +27% upon system-1. The VLWM models also outperforms strong VLM
baselines on RoboVQA and WorldPrediction benchmark.

</details>


### [22] [Deep Research is the New Analytics System: Towards Building the Runtime for AI-Driven Analytics](https://arxiv.org/abs/2509.02751)
*Matthew Russo,Tim Kraska*

Main category: cs.AI

TL;DR: 提出了一个结合语义算子优化执行和深度研究系统灵活性的AI驱动分析运行时原型，通过让深度研究代理编写和执行优化的语义算子程序，在性能和成本上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前语义算子在大规模数据集上执行成本高且不适合交互式分析，而深度研究系统缺乏查询计划优化。需要结合两者的优势来提升AI驱动分析的性能。

Method: 构建原型系统，使深度研究代理能够编写和执行优化的语义算子程序，将语义算子的优化执行与深度研究系统的动态执行相结合。

Result: 原型在两个基本查询上优于手工编写的语义算子程序和开源深度研究系统，F1分数提升达1.95倍，即使代理可以使用语义算子作为工具，仍能节省76.8%的成本和72.7%的运行时间。

Conclusion: 该原型证明了结合语义算子优化执行和深度研究系统灵活性的可行性，为构建更高效的AI驱动分析运行时迈出了重要一步。

Abstract: With advances in large language models (LLMs), researchers are creating new
systems that can perform AI-driven analytics over large unstructured datasets.
Recent work has explored executing such analytics queries using semantic
operators -- a declarative set of AI-powered data transformations with natural
language specifications. However, even when optimized, these operators can be
expensive to execute on millions of records and their iterator execution
semantics make them ill-suited for interactive data analytics tasks. In another
line of work, Deep Research systems have demonstrated an ability to answer
natural language question(s) over large datasets. These systems use one or more
LLM agent(s) to plan their execution, process the dataset(s), and iteratively
refine their answer. However, these systems do not explicitly optimize their
query plans which can lead to poor plan execution. In order for AI-driven
analytics to excel, we need a runtime which combines the optimized execution of
semantic operators with the flexibility and more dynamic execution of Deep
Research systems. As a first step towards this vision, we build a prototype
which enables Deep Research agents to write and execute optimized semantic
operator programs. We evaluate our prototype and demonstrate that it can
outperform a handcrafted semantic operator program and open Deep Research
systems on two basic queries. Compared to a standard open Deep Research agent,
our prototype achieves up to 1.95x better F1-score. Furthermore, even if we
give the agent access to semantic operators as tools, our prototype still
achieves cost and runtime savings of up to 76.8% and 72.7% thanks to its
optimized execution.

</details>


### [23] [Do LLM Modules Generalize? A Study on Motion Generation for Autonomous Driving](https://arxiv.org/abs/2509.02754)
*Mingyi Wang,Jingke Wang,Tengju Ye,Junbo Chen,Kaicheng Yu*

Main category: cs.AI

TL;DR: 大语言模型模块在自主驾驶运动生成中的系统性转移研究，通过实验评估五个关键模块的可转移性和适配方法


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型在自主驾驶运动生成领域应用广泛，但缺乏对哪些LLM模块真正可以转移的系统性理解

Method: 通过在Waymo Sim Agents基准上进行广泛实验，评估五个关键LLM模块：标记化器设计、位置嵌入、预训练范式、训练后策略和测试时计算

Result: 经适当适配后，这些模块能够显著提高自主驾驶运动生成性能，在Sim Agents任务中获得竞争力结果

Conclusion: 识别了可有效转移的技术，分析了某些方法失败的原因，并讨论了自主驾驶场景中的特定适配需求

Abstract: Recent breakthroughs in large language models (LLMs) have not only advanced
natural language processing but also inspired their application in domains with
structurally similar problems--most notably, autonomous driving motion
generation. Both domains involve autoregressive sequence modeling, token-based
representations, and context-aware decision making, making the transfer of LLM
components a natural and increasingly common practice. However, despite
promising early attempts, a systematic understanding of which LLM modules are
truly transferable remains lacking. In this paper, we present a comprehensive
evaluation of five key LLM modules--tokenizer design, positional embedding,
pre-training paradigms, post-training strategies, and test-time
computation--within the context of motion generation for autonomous driving.
Through extensive experiments on the Waymo Sim Agents benchmark, we demonstrate
that, when appropriately adapted, these modules can significantly improve
performance for autonomous driving motion generation. In addition, we identify
which techniques can be effectively transferred, analyze the potential reasons
for the failure of others, and discuss the specific adaptations needed for
autonomous driving scenarios. We evaluate our method on the Sim Agents task and
achieve competitive results.

</details>


### [24] [Plan Verification for LLM-Based Embodied Task Completion Agents](https://arxiv.org/abs/2509.02761)
*Ananth Hariharan,Vardhan Dongre,Dilek Hakkani-Tür,Gokhan Tur*

Main category: cs.AI

TL;DR: 提出迭代验证框架，使用Judge LLM批评动作序列，Planner LLM进行修订，逐步提升具身AI任务规划质量


<details>
  <summary>Details</summary>
Motivation: 基于大语言模型的具身AI任务规划和人类演示可能存在噪声动作、冗余导航和逻辑错误，影响策略质量

Method: 迭代验证框架：Judge LLM批评动作序列，Planner LLM应用修订，通过自然语言提示实现广泛泛化

Result: 在TEACh数据集上达到90%召回率和100%精确度，96.5%序列最多3次迭代收敛，提升时空效率

Conclusion: 该方法将计划验证确立为可靠的LLM能力，为具身AI模仿学习提供高质量训练数据的可扩展路径

Abstract: Large language model (LLM) based task plans and corresponding human
demonstrations for embodied AI may be noisy, with unnecessary actions,
redundant navigation, and logical errors that reduce policy quality. We propose
an iterative verification framework in which a Judge LLM critiques action
sequences and a Planner LLM applies the revisions, yielding progressively
cleaner and more spatially coherent trajectories. Unlike rule-based approaches,
our method relies on natural language prompting, enabling broad generalization
across error types including irrelevant actions, contradictions, and missing
steps. On a set of manually annotated actions from the TEACh embodied AI
dataset, our framework achieves up to 90% recall and 100% precision across four
state-of-the-art LLMs (GPT o4-mini, DeepSeek-R1, Gemini 2.5, LLaMA 4 Scout).
The refinement loop converges quickly, with 96.5% of sequences requiring at
most three iterations, while improving both temporal efficiency and spatial
action organization. Crucially, the method preserves human error-recovery
patterns rather than collapsing them, supporting future work on robust
corrective behavior. By establishing plan verification as a reliable LLM
capability for spatial planning and action refinement, we provide a scalable
path to higher-quality training data for imitation learning in embodied AI.

</details>


### [25] [app.build: A Production Framework for Scaling Agentic Prompt-to-App Generation with Environment Scaffolding](https://arxiv.org/abs/2509.03310)
*Evgenii Kniazev,Arseny Kravchenko,Igor Rekun,James Broadhead,Nikita Shamgunov,Pranav Sah,Pratik Nichite,Ivan Yamshchikov*

Main category: cs.AI

TL;DR: app.build是一个开源框架，通过系统化验证和结构化环境改进基于LLM的应用程序生成，实现了73.3%的可行性率和30%的完美质量评分。


<details>
  <summary>Details</summary>
Motivation: 提高LLM应用生成的可靠性和质量，解决当前AI代理系统在生成应用程序时缺乏系统验证和环境支持的问题。

Method: 采用多层验证流水线、特定技术栈编排和模型无关架构，在三个参考技术栈上实现系统化验证。

Result: 在30个生成任务评估中，综合验证达到73.3%可行性率，30%达到完美质量评分；开源模型在结构化环境中达到闭源模型80.8%的性能；社区已生成3000多个应用。

Conclusion: 扩展可靠AI代理需要扩展环境而不仅仅是模型，该工作为生产导向的代理系统提供了实证见解和完整参考实现。

Abstract: We present app.build (https://github.com/appdotbuild/agent/), an open-source
framework that improves LLM-based application generation through systematic
validation and structured environments. Our approach combines multi-layered
validation pipelines, stack-specific orchestration, and model-agnostic
architecture, implemented across three reference stacks. Through evaluation on
30 generation tasks, we demonstrate that comprehensive validation achieves
73.3% viability rate with 30% reaching perfect quality scores, while
open-weights models achieve 80.8% of closed-model performance when provided
structured environments. The open-source framework has been adopted by the
community, with over 3,000 applications generated to date. This work
demonstrates that scaling reliable AI agents requires scaling environments, not
just models -- providing empirical insights and complete reference
implementations for production-oriented agent systems.

</details>


### [26] [Key Principles in Cross-Domain Hyper-Heuristic Performance](https://arxiv.org/abs/2509.02782)
*Václav Sobotka,Lucas Kletzander,Nysret Musliu,Hana Rudová*

Main category: cs.AI

TL;DR: 通过系统分析低级命令集合的战略转换（解决方案接受、重复执行和干扰强度），将简单随机选择机制提升为超过现有最先进超高级调度算法的方法，并在多个实际领域发现新的最优解。


<details>
  <summary>Details</summary>
Motivation: 现有的选择型超高级调度算法主要关注从预定义集合中适应性选择低级命令，而本文重点研究如何构建和战略性转换这个集合本身。

Method: 系统分析基于三个关键原则的转换：解决方案接受、低级命令重复执行和干扰强度（即干扰型低级命令影响解的比例）。在简单的偏差随机选择机制上验证转换效果。

Result: 适当的转换使得简单随机选择机制在三个具有挑战性的实际领域上超过所有现有最先进超高级调度算法，发现了11个新的最优解。在CHeSC竞赛标准测试集上也与冠军方法竞争力相当。

Conclusion: 通过战略性转换低级命令集合，不仅可以显著提升超高级调度算法的性能，还能简化算法设计，在多个领域达到现有最先进水平。

Abstract: Cross-domain selection hyper-heuristics aim to distill decades of research on
problem-specific heuristic search algorithms into adaptable general-purpose
search strategies. In this respect, existing selection hyper-heuristics
primarily focus on an adaptive selection of low-level heuristics (LLHs) from a
predefined set. In contrast, we concentrate on the composition of this set and
its strategic transformations. We systematically analyze transformations based
on three key principles: solution acceptance, LLH repetitions, and perturbation
intensity, i.e., the proportion of a solution affected by a perturbative LLH.
We demonstrate the raw effects of our transformations on a trivial unbiased
random selection mechanism. With an appropriately constructed transformation,
this trivial method outperforms all available state-of-the-art hyper-heuristics
on three challenging real-world domains and finds 11 new best-known solutions.
The same method is competitive with the winner of the CHeSC competition,
commonly used as the standard cross-domain benchmark. Moreover, we accompany
several recent hyper-heuristics with such strategic transformations. Using this
approach, we outperform the current state-of-the-art methods on both the CHeSC
benchmark and real-world domains while often simplifying their designs.

</details>


### [27] [Learning General Policies From Examples](https://arxiv.org/abs/2509.02794)
*Blai Bonet,Hector Geffner*

Main category: cs.AI

TL;DR: 提出了一种基于采样计划泛化的符号化策略学习方法，使用命中集算法替代传统的SAT/ASP方法，能够处理百万级状态和数十万特征的大规模问题。


<details>
  <summary>Details</summary>
Motivation: 现有组合学习方法虽然能生成可理解且正确的策略，但无法扩展到大规模问题，只能处理小规模训练实例和特征池。

Method: 基于采样计划泛化的符号化方法，使用命中集算法确保结构终止性和无环性，替代传统的SAT/ASP求解器。

Result: 方法能够有效处理包含数百万状态和数十万特征的大规模问题，在多个基准测试中展现了良好的可扩展性。

Conclusion: 该方法解决了符号化策略学习方法的可扩展性问题，为大规模规划问题的策略学习提供了有效解决方案。

Abstract: Combinatorial methods for learning general policies that solve large
collections of planning problems have been recently developed. One of their
strengths, in relation to deep learning approaches, is that the resulting
policies can be understood and shown to be correct. A weakness is that the
methods do not scale up and learn only from small training instances and
feature pools that contain a few hundreds of states and features at most. In
this work, we propose a new symbolic method for learning policies based on the
generalization of sampled plans that ensures structural termination and hence
acyclicity. The proposed learning approach is not based on SAT/ASP, as previous
symbolic methods, but on a hitting set algorithm that can effectively handle
problems with millions of states, and pools with hundreds of thousands of
features. The formal properties of the approach are analyzed, and its
scalability is tested on a number of benchmarks.

</details>


### [28] [Uncertainty-driven Adaptive Exploration](https://arxiv.org/abs/2509.03219)
*Leonidas Bakopoulos,Georgios Chalkiadakis*

Main category: cs.AI

TL;DR: 提出了一个基于不确定性的自适应探索框架，通过交替进行探索和利用来学习复杂策略，并在多个MuJoCo环境中优于标准方法


<details>
  <summary>Details</summary>
Motivation: 解决在需要学习长而复杂动作序列的领域中，如何确定在探索和利用之间切换的适当时机这一关键问题

Method: 构建了一个通用的自适应探索框架，利用不确定性来指导探索与利用的切换，可以整合各种不确定性测量机制（如内在动机或认知不确定性方法）

Result: 实验证明该框架产生的自适应探索策略在多个MuJoCo环境中优于标准探索方法

Conclusion: 该框架提供了一个原则性的方法来解决探索-利用切换问题，能够整合不同的不确定性测量机制，并在复杂环境中表现出优越性能

Abstract: Adaptive exploration methods propose ways to learn complex policies via
alternating between exploration and exploitation. An important question for
such methods is to determine the appropriate moment to switch between
exploration and exploitation and vice versa. This is critical in domains that
require the learning of long and complex sequences of actions. In this work, we
present a generic adaptive exploration framework that employs uncertainty to
address this important issue in a principled manner. Our framework includes
previous adaptive exploration approaches as special cases. Moreover, we can
incorporate in our framework any uncertainty-measuring mechanism of choice, for
instance mechanisms used in intrinsic motivation or epistemic uncertainty-based
exploration methods. We experimentally demonstrate that our framework gives
rise to adaptive exploration strategies that outperform standard ones across
several MuJoCo environments.

</details>


### [29] [Accountability Framework for Healthcare AI Systems: Towards Joint Accountability in Decision Making](https://arxiv.org/abs/2509.03286)
*Prachi Bagave,Marcus Westberg,Marijn Janssen,Aaron Yi Ding*

Main category: cs.AI

TL;DR: 这篇论文为医疗健康领域的AI系统建立了一个账房性框架，解决相关指南只关注"做什么"而缺乏"如何做"的知识空白，通过三层结构处理不同的账房性机制。


<details>
  <summary>Details</summary>
Motivation: 医疗AI系统的账房性问题日益紧迫，但相关指南存在模糊性，不同参与者对账房性的理解和处理方式存在差异，需要桥接"做什么"和"如何做"之间的差距。

Method: 分析账房性概念，制定账房性框架，并提供一个三层结构来处理各种账房性机制，将医疗AI系统的规定和机制统一在一个一致的账房性体系下。

Result: 框架位了医疗AI系统的规定和机制，三层结构指导参与者分类机制，强调医疗AI决策存在共享依赖关系，账房性应联合处理并促进合作。

Conclusion: 账房性应作为共同体聚集共享依赖关系，可解释性在推动参与者之间的沟通和信息共享中发挥关键作用，以进一步促进协作过程。

Abstract: AI is transforming the healthcare domain and is increasingly helping
practitioners to make health-related decisions. Therefore, accountability
becomes a crucial concern for critical AI-driven decisions. Although regulatory
bodies, such as the EU commission, provide guidelines, they are highlevel and
focus on the ''what'' that should be done and less on the ''how'', creating a
knowledge gap for actors. Through an extensive analysis, we found that the term
accountability is perceived and dealt with in many different ways, depending on
the actor's expertise and domain of work. With increasing concerns about AI
accountability issues and the ambiguity around this term, this paper bridges
the gap between the ''what'' and ''how'' of AI accountability, specifically for
AI systems in healthcare. We do this by analysing the concept of
accountability, formulating an accountability framework, and providing a
three-tier structure for handling various accountability mechanisms. Our
accountability framework positions the regulations of healthcare AI systems and
the mechanisms adopted by the actors under a consistent accountability regime.
Moreover, the three-tier structure guides the actors of the healthcare AI
system to categorise the mechanisms based on their conduct. Through our
framework, we advocate that decision-making in healthcare AI holds shared
dependencies, where accountability should be dealt with jointly and should
foster collaborations. We highlight the role of explainability in instigating
communication and information sharing between the actors to further facilitate
the collaborative process.

</details>


### [30] [Language Models Do Not Follow Occam's Razor: A Benchmark for Inductive and Abductive Reasoning](https://arxiv.org/abs/2509.03345)
*Yunxin Sun,Abulhair Saparov*

Main category: cs.AI

TL;DR: 该论文提出了InAbHyD数据集来评估LLMs的归纳和溯因推理能力，发现LLMs在简单场景中能进行此类推理，但在复杂世界模型中表现不佳。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注演绎推理，而归纳和溯因推理在解决现实问题中同样重要但较少被探索，需要系统评估LLMs在这两类推理上的能力。

Method: 创建可编程合成数据集InAbHyD，包含不完整世界模型和观察数据，要求智能体生成解释观察的假设，并提出基于奥卡姆剃刀原理的新评估指标。

Result: LLMs在简单场景中能进行归纳和溯因推理，但在复杂世界模型中表现困难，即使使用上下文学习和RLVR等推理增强技术也难以产生高质量假设。

Conclusion: LLMs在归纳和溯因推理方面仍有局限，特别是在复杂场景中，需要进一步研究提升其推理能力。

Abstract: Reasoning is a core capability in artificial intelligence systems, for which
large language models (LLMs) have recently shown remarkable progress. However,
most work focuses exclusively on deductive reasoning, which is problematic
since other types of reasoning are also essential in solving real-world
problems, and they are less explored. This work focuses on evaluating LLMs'
inductive and abductive reasoning capabilities. We introduce a programmable and
synthetic dataset, InAbHyD (pronounced in-a-bid), where each reasoning example
consists of an incomplete world model and a set of observations. The task for
the intelligent agent is to produce hypotheses to explain observations under
the incomplete world model to solve each reasoning example. We propose a new
metric to evaluate the quality of hypotheses based on Occam's Razor. We
evaluate and analyze some state-of-the-art LLMs. Our analysis shows that LLMs
can perform inductive and abductive reasoning in simple scenarios, but struggle
with complex world models and producing high-quality hypotheses, even with
popular reasoning-enhancing techniques such as in-context learning and RLVR.

</details>


### [31] [Situating AI Agents in their World: Aspective Agentic AI for Dynamic Partially Observable Information Systems](https://arxiv.org/abs/2509.03380)
*Peter J. Bentley,Soo Ling Lim,Fuyuki Ishikawa*

Main category: cs.AI

TL;DR: 本文提出了一个基于环境触发的AI智能体框架，通过引入"aspects"概念实现零信息泄漏，相比传统架构83%的泄漏率有显著改进


<details>
  <summary>Details</summary>
Motivation: 当前AI智能体往往只是自主聊天机器人，存在信息泄漏和控制不可靠的问题，需要更安全高效的架构

Method: 提出自下而上的框架，将AI智能体置于环境中，所有行为由环境变化触发；引入类似umwelt的aspects概念，不同智能体以不同方式感知环境

Result: 相比典型架构83%的信息泄漏率，aspective agentic AI实现了零信息泄漏

Conclusion: 专业智能体在各自信息生态位中高效工作的概念可以同时提升安全性和效率

Abstract: Agentic LLM AI agents are often little more than autonomous chatbots: actors
following scripts, often controlled by an unreliable director. This work
introduces a bottom-up framework that situates AI agents in their environment,
with all behaviors triggered by changes in their environments. It introduces
the notion of aspects, similar to the idea of umwelt, where sets of agents
perceive their environment differently to each other, enabling clearer control
of information. We provide an illustrative implementation and show that
compared to a typical architecture, which leaks up to 83% of the time,
aspective agentic AI enables zero information leakage. We anticipate that this
concept of specialist agents working efficiently in their own information
niches can provide improvements to both security and efficiency.

</details>


### [32] [ANNIE: Be Careful of Your Robots](https://arxiv.org/abs/2509.03383)
*Yiyang Huang,Zixuan Wang,Zishen Wan,Yapeng Tian,Haobo Xu,Yinhe Han,Yiming Gan*

Main category: cs.AI

TL;DR: 本文首次系统研究具身AI系统的对抗性安全攻击，基于ISO标准构建了安全违规分类法，开发了ANNIEBench基准测试和ANNIE-Attack攻击框架，在代表性EAI模型上攻击成功率超过50%，揭示了物理AI时代的安全威胁。


<details>
  <summary>Details</summary>
Motivation: 随着视觉-语言-动作模型在具身AI机器人中的集成，系统面临新的安全风险：受损的VLA模型可将对抗性扰动直接转化为不安全的物理动作。传统机器学习安全方法已不足够，需要研究在物理交互环境中如何定义、测量安全以及设计有效的攻防机制。

Method: 1) 基于物理约束（分离距离、速度、碰撞边界）构建安全违规分类法（关键、危险、风险）；2) 开发ANNIEBench基准，包含9个安全关键场景和2400个视频-动作序列；3) 提出ANNIE-Attack任务感知对抗框架，使用攻击引导模型将长时程目标分解为帧级扰动。

Result: 在代表性EAI模型上的评估显示，所有安全类别的攻击成功率均超过50%。进一步展示了稀疏和自适应攻击策略，并通过物理机器人实验验证了现实世界影响。

Conclusion: 研究结果揭示了具身AI系统中先前未被充分探索但后果严重的攻击面，强调了在物理AI时代迫切需要安全驱动的防御措施。代码已开源。

Abstract: The integration of vision-language-action (VLA) models into embodied AI (EAI)
robots is rapidly advancing their ability to perform complex, long-horizon
tasks in humancentric environments. However, EAI systems introduce critical
security risks: a compromised VLA model can directly translate adversarial
perturbations on sensory input into unsafe physical actions. Traditional safety
definitions and methodologies from the machine learning community are no longer
sufficient. EAI systems raise new questions, such as what constitutes safety,
how to measure it, and how to design effective attack and defense mechanisms in
physically grounded, interactive settings. In this work, we present the first
systematic study of adversarial safety attacks on embodied AI systems, grounded
in ISO standards for human-robot interactions. We (1) formalize a principled
taxonomy of safety violations (critical, dangerous, risky) based on physical
constraints such as separation distance, velocity, and collision boundaries;
(2) introduce ANNIEBench, a benchmark of nine safety-critical scenarios with
2,400 video-action sequences for evaluating embodied safety; and (3)
ANNIE-Attack, a task-aware adversarial framework with an attack leader model
that decomposes long-horizon goals into frame-level perturbations. Our
evaluation across representative EAI models shows attack success rates
exceeding 50% across all safety categories. We further demonstrate sparse and
adaptive attack strategies and validate the real-world impact through physical
robot experiments. These results expose a previously underexplored but highly
consequential attack surface in embodied AI systems, highlighting the urgent
need for security-driven defenses in the physical AI era. Code is available at
https://github.com/RLCLab/Annie.

</details>


### [33] [sam-llm: interpretable lane change trajectoryprediction via parametric finetuning](https://arxiv.org/abs/2509.03462)
*Zhuo Cao,Yunxiao Shi,Min Xu*

Main category: cs.AI

TL;DR: SAM-LLM是一种混合架构，将大语言模型的上下文推理能力与运动学车道变换模型的物理精度相结合，用于自动驾驶中的可解释车道变换轨迹预测。


<details>
  <summary>Details</summary>
Motivation: 解决传统坐标预测方法缺乏物理可解释性和计算效率低的问题，通过参数化输出实现物理合理且可解释的轨迹预测。

Method: 微调LLM输出轨迹模型的核心物理参数（横向位移、机动持续时间、初始横向速度、纵向速度变化），而非原始坐标。车道保持时预测离散坐标，车道变换时生成增强正弦加速度模型(SAM)参数。

Result: 实现了98.73%的整体意图预测准确率，输出大小相比基于坐标的方法减少80%，性能与传统LLM预测器相当，但在可解释性和资源效率方面具有显著优势。

Conclusion: SAM-LLM提供了一种连续、物理合理且计算高效的轨迹预测方法，在保持高性能的同时大幅提升了可解释性和效率，为自动驾驶系统提供了更好的决策支持。

Abstract: This work introduces SAM-LLM, a novel hybrid architecture that bridges the
gap between the contextual reasoning of Large Language Models (LLMs) and the
physical precision of kinematic lane change models for autonomous driving. The
system is designed for interpretable lane change trajectory prediction by
finetuning an LLM to output the core physical parameters of a trajectory model
instead of raw coordinates. For lane-keeping scenarios, the model predicts
discrete coordinates, but for lane change maneuvers, it generates the
parameters for an enhanced Sinusoidal Acceleration Model (SAM), including
lateral displacement, maneuver duration, initial lateral velocity, and
longitudinal velocity change. This parametric approach yields a complete,
continuous, and physically plausible trajectory model that is inherently
interpretable and computationally efficient, achieving an 80% reduction in
output size compared to coordinate-based methods. The SAM-LLM achieves a
state-of-the-art overall intention prediction accuracy of 98.73%, demonstrating
performance equivalent to traditional LLM predictors while offering significant
advantages in explainability and resource efficiency.

</details>
