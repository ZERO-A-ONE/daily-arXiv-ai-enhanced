<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 17]
- [cs.AI](#cs.AI) [Total: 19]
- [cs.SE](#cs.SE) [Total: 16]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Secure and Governed API Gateway Architectures for Multi-Cluster Cloud Environments](https://arxiv.org/abs/2512.23774)
*Vinoth Punniyamoorthy,Kabilan Kannan,Akshay Deshpande,Lokesh Butra,Akash Kumar Agarwal,Adithya Parthasarathy,Suhas Malempati,Bikesh Kumar*

Main category: cs.CR

TL;DR: 提出一种面向多集群云环境的治理感知、意图驱动的API网关协调管理架构，通过高层声明式意图表达安全、治理和性能目标，实现配置自动转换和持续验证，显著减少策略漂移并改善传播性能。


<details>
  <summary>Details</summary>
Motivation: 随着多集群和混合云部署的普及，在异构网关环境中保持一致的策略执行、可预测的性能和操作稳定性变得困难。现有方法通常将安全、治理和性能作为松散耦合的关注点处理，导致配置漂移、策略传播延迟以及动态工作负载下的运行时行为不稳定。

Method: 提出治理感知、意图驱动的API网关协调管理架构。将安全、治理和性能目标表达为高层声明式意图，系统性地转换为可执行的网关配置，并通过策略验证和遥测驱动的反馈进行持续验证。通过解耦意图规范与执行，同时支持有界、策略合规的适配，该架构支持异构网关实现而不损害治理保证或服务级别目标。

Result: 在多个Kubernetes集群上的原型实现表明，与手动和声明式基线方法相比，策略漂移减少高达42%，配置传播时间改善31%，在可变工作负载下p95延迟开销持续低于6%。

Conclusion: 治理感知、意图驱动的网关编排为安全、一致且性能可预测的云原生平台提供了可扩展和可靠的基础，能够有效解决多集群环境中API网关管理的挑战。

Abstract: API gateways serve as critical enforcement points for security, governance, and traffic management in cloud-native systems. As organizations increasingly adopt multi-cluster and hybrid cloud deployments, maintaining consistent policy enforcement, predictable performance, and operational stability across heterogeneous gateway environments becomes challenging. Existing approaches typically manage security, governance, and performance as loosely coupled concerns, leading to configuration drift, delayed policy propagation, and unstable runtime behavior under dynamic workloads. This paper presents a governance-aware, intent-driven architecture for coordinated API gateway management in multi-cluster cloud environments. The proposed approach expresses security, governance, and performance objectives as high-level declarative intents, which are systematically translated into enforceable gateway configurations and continuously validated through policy verification and telemetry-driven feedback. By decoupling intent specification from enforcement while enabling bounded, policy-compliant adaptation, the architecture supports heterogeneous gateway implementations without compromising governance guarantees or service-level objectives. A prototype implementation across multiple Kubernetes clusters demonstrates the effectiveness of the proposed design. Experimental results show up to a 42% reduction in policy drift, a 31% improvement in configuration propagation time, and sustained p95 latency overhead below 6% under variable workloads, compared to manual and declarative baseline approaches. These results indicate that governance-aware, intent-driven gateway orchestration provides a scalable and reliable foundation for secure, consistent, and performance-predictable cloud-native platforms.

</details>


### [2] [SyncGait: Robust Long-Distance Authentication for Drone Delivery via Implicit Gait Behaviors](https://arxiv.org/abs/2512.23778)
*Zijian Ling,Man Zhou,Hongda Zhai,Yating Huang,Lingchen Zhao,Qi Li,Chao Shen,Qian Wang*

Main category: cs.CR

TL;DR: SyncGait是一种基于步态的无人机配送隐式双向认证系统，利用用户行走时的独特手臂摆动进行身份验证，无需额外硬件或特定认证动作，在远距离（>18米）下达到99.84%的平均准确率。


<details>
  <summary>Details</summary>
Motivation: 无人机配送中需要确保无人机与用户之间的安全距离，直到双向认证完成。现有认证方案存在距离限制和抗攻击能力不足的问题，需要一种安全、远距离且用户友好的认证方案。

Method: SyncGait利用用户走向无人机时的独特手臂摆动模式进行隐式步态认证。系统通过分析步态特征实现双向认证，无需额外硬件或特定认证动作。

Result: 在31名受试者的14个数据集上进行广泛实验，结果显示SyncGait在远距离（>18米）下达到99.84%的平均准确率，并对各种欺骗攻击表现出强大的抗攻击能力。

Conclusion: SyncGait为无人机配送提供了一种稳健、安全且用户友好的双向认证解决方案，在真实场景中具有实用价值，能够有效解决现有认证方案的距离限制和安全性问题。

Abstract: In recent years, drone delivery, which utilizes unmanned aerial vehicles (UAVs) for package delivery and pickup, has gradually emerged as a crucial method in logistics. Since delivery drones are expensive and may carry valuable packages, they must maintain a safe distance from individuals until user-drone mutual authentication is confirmed. Despite numerous authentication schemes being developed, existing solutions are limited in authentication distance and lack resilience against sophisticated attacks. To this end, we introduce SyncGait, an implicit gait-based mutual authentication system for drone delivery. SyncGait leverages the user's unique arm swing as he walks toward the drone to achieve mutual authentication without requiring additional hardware or specific authentication actions. We conducted extensive experiments on 14 datasets collected from 31 subjects. The results demonstrate that SyncGait achieves an average accuracy of 99.84\% at a long distance ($>18m$) and exhibits strong resilience against various spoofing attacks, making it a robust, secure, and user-friendly solution in real-world scenarios.

</details>


### [3] [Application-Specific Power Side-Channel Attacks and Countermeasures: A Survey](https://arxiv.org/abs/2512.23785)
*Sahan Sanjaya,Aruna Jayasena,Prabhat Mishra*

Main category: cs.CR

TL;DR: 这篇论文对电力侧信道攻击及其防御措施进行了全面综述，涵盖了密码学实现、机器学习模型逆向工程、用户行为数据利用和指令级反汇编等多个应用领域。


<details>
  <summary>Details</summary>
Motivation: 虽然已有相关综述，但它们主要关注密码学实现中的电力侧信道攻击。近年来，电力侧信道攻击已扩展到多个应用领域，需要对这些新兴应用进行系统性的分类和比较。

Method: 提供电力侧信道攻击及其防御措施的全面综述，基于应用特定考虑对近期攻击进行分类和综合比较。

Result: 论文对电力侧信道攻击在不同应用领域的现状进行了系统梳理，包括从密码学实现中提取密钥、机器学习模型逆向工程、用户行为数据利用和指令级反汇编等方面。

Conclusion: 电力侧信道攻击已从传统的密码学领域扩展到多个新兴应用领域，需要针对不同应用场景采取相应的防御措施，该综述为研究人员提供了系统性的参考框架。

Abstract: Side-channel attacks try to extract secret information from a system by analyzing different side-channel signatures, such as power consumption, electromagnetic emanation, thermal dissipation, acoustics, time, etc. Power-based side-channel attack is one of the most prominent side-channel attacks in cybersecurity, which rely on data-dependent power variations in a system to extract sensitive information. While there are related surveys, they primarily focus on power side-channel attacks on cryptographic implementations. In recent years, power-side channel attacks have been explored in diverse application domains, including key extraction from cryptographic implementations, reverse engineering of machine learning models, user behavior data exploitation, and instruction-level disassembly. In this paper, we provide a comprehensive survey of power side-channel attacks and their countermeasures in different application domains. Specifically, this survey aims to classify recent power side-channel attacks and provide a comprehensive comparison based on application-specific considerations.

</details>


### [4] [Security Without Detection: Economic Denial as a Primitive for Edge and IoT Defense](https://arxiv.org/abs/2512.23849)
*Samaresh Kumar Singh,Joyjit Roy*

Main category: cs.CR

TL;DR: 提出EDS框架，通过经济成本放大使攻击在经济上不可行，无需依赖传统检测方法，特别适合资源受限的IoT/边缘环境


<details>
  <summary>Details</summary>
Motivation: 传统基于检测的安全方法在IoT/边缘环境中面临挑战：加密、隐蔽和低速率攻击难以检测，且资源约束限制了ML入侵检测系统的部署

Method: EDS框架包含四种机制：自适应计算谜题、诱饵驱动的交互熵、时间拉伸和带宽征税，通过Stackelberg博弈形式化，实现超线性成本放大

Result: 在20设备IoT测试床上验证：攻击减速32-560倍，成本不对称85-520:1，攻击成功率降低8-62%，延迟开销<20ms，假阳性接近0%；结合ML-IDS可将缓解率从67%提升至94%

Conclusion: EDS提供检测无关的保护，通过经济成本放大使攻击在经济上不可行，适合传统方法失效的资源受限环境，将经济平衡转向防御方

Abstract: Detection-based security fails against sophisticated attackers using encryption, stealth, and low-rate techniques, particularly in IoT/edge environments where resource constraints preclude ML-based intrusion detection. We present Economic Denial Security (EDS), a detection-independent framework that makes attacks economically infeasible by exploiting a fundamental asymmetry: defenders control their environment while attackers cannot. EDS composes four mechanisms adaptive computational puzzles, decoy-driven interaction entropy, temporal stretching, and bandwidth taxation achieving provably superlinear cost amplification. We formalize EDS as a Stackelberg game, deriving closed-form equilibria for optimal parameter selection (Theorem 1) and proving that mechanism composition yields 2.1x greater costs than the sum of individual mechanisms (Theorem 2). EDS requires < 12KB memory, enabling deployment on ESP32 class microcontrollers. Evaluation on a 20-device heterogeneous IoT testbed across four attack scenarios (n = 30 trials, p < 0.001) demonstrates: 32-560x attack slowdown, 85-520:1 cost asymmetry, 8-62% attack success reduction, < 20ms latency overhead, and close to 0% false positives. Validation against IoT-23 malware (Mirai, Torii, Hajime) shows 88% standalone mitigation; combined with ML-IDS, EDS achieves 94% mitigation versus 67% for IDS alone a 27% improvement. EDS provides detection-independent protection suitable for resource-constrained environments where traditional approaches fail. The ability to detect and mitigate the malware samples tested was enhanced; however, the benefits provided by EDS were realized even without the inclusion of an IDS. Overall, the implementation of EDS serves to shift the economic balance in favor of the defender and provides a viable method to protect IoT and edge systems methodologies.

</details>


### [5] [RepetitionCurse: Measuring and Understanding Router Imbalance in Mixture-of-Experts LLMs under DoS Stress](https://arxiv.org/abs/2512.23995)
*Ruixuan Huang,Qingyue Wang,Hantao Huang,Yudong Gao,Dong Chen,Shuai Wang,Wei Wang*

Main category: cs.CR

TL;DR: MoE架构中的路由机制存在安全漏洞，攻击者可通过构造重复令牌模式的对抗性提示，使所有令牌集中路由到少数专家，造成计算瓶颈并显著增加推理延迟。


<details>
  <summary>Details</summary>
Motivation: 现代MoE推理系统采用专家并行将专家分布到不同设备，但由于缺乏显式负载均衡约束，攻击者可以利用路由策略的漏洞，通过对抗性输入触发严重的路由集中问题。

Method: 提出RepetitionCurse攻击策略，这是一种低成本的基于黑盒的方法，通过识别MoE路由器行为的普遍缺陷，使用简单的重复令牌模式以模型无关的方式构造对抗性提示。

Result: 在广泛部署的MoE模型如Mixtral-8x7B上，该方法使端到端推理延迟增加了3.063倍，显著降低了服务可用性，违反了首次令牌生成时间的服务级别协议。

Conclusion: MoE架构的效率机制可能被转化为拒绝服务攻击向量，揭示了当前MoE推理系统在负载均衡和安全性方面的重要缺陷，需要设计更鲁棒的路由机制。

Abstract: Mixture-of-Experts architectures have become the standard for scaling large language models due to their superior parameter efficiency. To accommodate the growing number of experts in practice, modern inference systems commonly adopt expert parallelism to distribute experts across devices. However, the absence of explicit load balancing constraints during inference allows adversarial inputs to trigger severe routing concentration. We demonstrate that out-of-distribution prompts can manipulate the routing strategy such that all tokens are consistently routed to the same set of top-$k$ experts, which creates computational bottlenecks on certain devices while forcing others to idle. This converts an efficiency mechanism into a denial-of-service attack vector, leading to violations of service-level agreements for time to first token. We propose RepetitionCurse, a low-cost black-box strategy to exploit this vulnerability. By identifying a universal flaw in MoE router behavior, RepetitionCurse constructs adversarial prompts using simple repetitive token patterns in a model-agnostic manner. On widely deployed MoE models like Mixtral-8x7B, our method increases end-to-end inference latency by 3.063x, degrading service availability significantly.

</details>


### [6] [Jailbreaking Attacks vs. Content Safety Filters: How Far Are We in the LLM Safety Arms Race?](https://arxiv.org/abs/2512.24044)
*Yuan Xin,Dingfan Chen,Linyi Yang,Michael Backes,Xiao Zhang*

Main category: cs.CR

TL;DR: 该研究首次系统评估了针对LLM安全对齐的越狱攻击在整个推理流程中的成功率，发现现有评估可能高估了攻击的实际成功率，因为大多数越狱技术都能被至少一个安全过滤器检测到。


<details>
  <summary>Details</summary>
Motivation: 现有越狱攻击评估仅关注模型本身，忽略了实际部署中通常包含内容审核过滤器等额外安全机制的完整推理流程，导致可能高估了攻击的实际成功率。

Method: 首次系统评估越狱攻击在完整推理流程中的成功率，包括输入和输出过滤阶段，评估多种越狱技术在不同安全过滤器下的表现。

Result: 1. 几乎所有评估的越狱技术都能被至少一个安全过滤器检测到，表明先前评估可能高估了攻击的实际成功率；2. 安全过滤器在检测方面有效，但在召回率和精确度平衡方面仍有改进空间。

Conclusion: 研究揭示了LLM安全系统中的关键差距，呼吁进一步改进检测准确性和可用性，以优化保护效果和用户体验。

Abstract: As large language models (LLMs) are increasingly deployed, ensuring their safe use is paramount. Jailbreaking, adversarial prompts that bypass model alignment to trigger harmful outputs, present significant risks, with existing studies reporting high success rates in evading common LLMs. However, previous evaluations have focused solely on the models, neglecting the full deployment pipeline, which typically incorporates additional safety mechanisms like content moderation filters. To address this gap, we present the first systematic evaluation of jailbreak attacks targeting LLM safety alignment, assessing their success across the full inference pipeline, including both input and output filtering stages. Our findings yield two key insights: first, nearly all evaluated jailbreak techniques can be detected by at least one safety filter, suggesting that prior assessments may have overestimated the practical success of these attacks; second, while safety filters are effective in detection, there remains room to better balance recall and precision to further optimize protection and user experience. We highlight critical gaps and call for further refinement of detection accuracy and usability in LLM safety systems.

</details>


### [7] [Spatial Discretization for Fine-Grain Zone Checks with STARKs](https://arxiv.org/abs/2512.24238)
*Sungmin Lee,Kichang Lee,Gyeongmin Han,JeongGil Ko*

Main category: cs.CR

TL;DR: 该论文研究了在零知识证明中高效执行点面测试的方法，通过比较不同区域编码方式对准确性和证明成本的影响，提出了基于距离感知的网格编码方法。


<details>
  <summary>Details</summary>
Motivation: 许多基于位置的服务依赖点面测试，但几何运算在零知识证明中成本高昂，因此需要寻找高效且准确的隐私保护空间检查方法。

Method: 在固定STARK执行模型下，利用基于网格的查找表，比较了布尔网格编码（标记单元格内外状态）和距离感知编码（存储每个单元格到区域边界的距离并使用插值推理）两种方法。

Result: 在真实世界数据上的实验表明，距离感知方法在粗网格上实现了更高的准确性（最大60%精度提升），而验证开销仅适度增加（约1.4倍）。

Conclusion: 区域编码是高效零知识空间检查的关键杠杆，距离感知方法在保持合理验证成本的同时显著提高了准确性。

Abstract: Many location-based services rely on a point-in-polygon test (PiP), checking whether a point or a trajectory lies inside a geographic zone. Since geometric operations are expensive in zero-knowledge proofs, privately performing the PiP test is challenging. In this paper, we answer the research questions of how different ways of encoding zones affect accuracy and proof cost by exploiting gridbased lookup tables under a fixed STARK execution model. Beyond a Boolean grid-based baseline that marks cells as in- or outside, we explore a distance-aware encoding approach that stores how far each cell is from a zone boundary and uses interpolation to reason within a cell. Our experiments on real-world data demonstrate that the proposed distance-aware approach achieves higher accuracy on coarse grids (max. 60%p accuracy gain) with only a moderate verification overhead (approximately 1.4x), making zone encoding the key lever for efficient zero-knowledge spatial checks.

</details>


### [8] [How Would Oblivious Memory Boost Graph Analytics on Trusted Processors?](https://arxiv.org/abs/2512.24255)
*Jiping Yu,Xiaowei Zhu,Kun Chen,Guanyu Feng,Yunyi Chen,Xiaoyu Fan,Wenguang Chen*

Main category: cs.CR

TL;DR: 论文探索在可信处理器中集成不可观测内存（OM）来提升图分析性能，通过存储结构与算法协同设计，在现有处理器上实现100倍加速


<details>
  <summary>Details</summary>
Motivation: 可信处理器虽然能保护数据隐私，但为防止信息泄露而使用的数据不可知算法会导致性能下降。为了解决这一问题，研究者探索在处理器中集成不可观测内存（OM），其访问对攻击者不可见，特别关注易受访问模式攻击的图分析应用。

Method: 采用存储结构与算法的协同设计方法，在处理器中集成不可观测内存（OM），其大小约为每核缓存大小，可在现有处理器上以可忽略的开销实现。

Result: 原型系统相比基线方法实现了100倍的性能提升，证明了在可信处理器中配备OM的有效性和可行性。

Conclusion: 在可信处理器中集成不可观测内存（OM）是可行的，通过适当的协同设计可以显著提升隐私保护计算性能，为可信处理器配备OM提供了重要见解。

Abstract: Trusted processors provide a way to perform joint computations while preserving data privacy. To overcome the performance degradation caused by data-oblivious algorithms to prevent information leakage, we explore the benefits of oblivious memory (OM) integrated in processors, to which the accesses are unobservable by adversaries. We focus on graph analytics, an important application vulnerable to access-pattern attacks. With a co-design between storage structure and algorithms, our prototype system is 100x faster than baselines given an OM sized around the per-core cache which can be implemented on existing processors with negligible overhead. This gives insights into equipping trusted processors with OM.

</details>


### [9] [SourceBroken: A large-scale analysis on the (un)reliability of SourceRank in the PyPI ecosystem](https://arxiv.org/abs/2512.24400)
*Biagio Montaruli,Serena Elisa Ponta,Luca Compagna,Davide Balzarotti*

Main category: cs.CR

TL;DR: SourceRank评分系统存在安全漏洞，攻击者可通过URL混淆等技术操纵分数，使其无法可靠区分PyPI上的良性包和恶意包。


<details>
  <summary>Details</summary>
Motivation: SourceRank作为评估开源软件包质量和流行度的18个指标评分系统，已被多项研究采用，但尚未有研究深入分析其对旨在提高恶意包评分的规避攻击的可靠性。本文旨在填补这一空白，评估SourceRank在真实场景中的安全性和有效性。

Method: 1. 提出威胁模型，识别每个指标可能的规避攻击方法，包括URL混淆技术（影响18个指标中的5个）；2. 在PyPI生态系统中分析SourceRank的可靠性：使用MalwareBench数据集和包含122,398个包的真实世界数据集，比较良性包和恶意包的SourceRank分布；3. 分析URL混淆攻击的流行程度和影响。

Result: 1. 历史数据显示良性包和恶意包有明显区别，但真实世界分布显著重叠，主要原因是SourceRank未能及时反映软件包移除；2. SourceRank在真实场景中无法可靠区分良性包和恶意包，也不能用于从PyPI中选择良性包；3. URL混淆攻击呈上升趋势：从MalwareBench的4.2%增加到真实数据集的7.0%，常与其他规避技术结合使用，能显著提高恶意包的SourceRank评分。

Conclusion: SourceRank评分系统存在严重安全缺陷，攻击者可通过URL混淆等技术操纵评分，使其失去区分良性和恶意软件包的能力。系统未能及时更新包移除信息，导致真实场景中分布重叠。URL混淆已成为新兴攻击向量，需要改进评分系统以应对此类规避攻击。

Abstract: SourceRank is a scoring system made of 18 metrics that assess the popularity and quality of open-source packages. Despite being used in several recent studies, none has thoroughly analyzed its reliability against evasion attacks aimed at inflating the score of malicious packages, thereby masquerading them as trustworthy. To fill this gap, we first propose a threat model that identifies potential evasion approaches for each metric, including the URL confusion technique, which can affect 5 out of the 18 metrics by leveraging a URL pointing to a legitimate repository potentially unrelated to the malicious package.
  Furthermore, we study the reliability of SourceRank in the PyPI ecosystem by analyzing the SourceRank distributions of benign and malicious packages in the state-of-the-art MalwareBench dataset, as well as in a real-world dataset of 122,398 packages. Our analysis reveals that, while historical data suggests a clear distinction between benign and malicious packages, the real-world distributions overlap significantly, mainly due to SourceRank's failure to timely reflect package removals. As a result, SourceRank cannot be reliably used to discriminate between benign and malicious packages in real-world scenarios, nor to select benign packages among those available on PyPI.
  Finally, our analysis reveals that URL confusion represents an emerging attack vector, with its prevalence increasing from 4.2% in MalwareBench to 7.0% in our real-world dataset. Moreover, this technique is often used alongside other evasion techniques and can significantly inflate the SourceRank metrics of malicious packages.

</details>


### [10] [GateChain: A Blockchain Based Application for Country Entry Exit Registry Management](https://arxiv.org/abs/2512.24416)
*Mohamad Akkad,Hüseyin Bodur*

Main category: cs.CR

TL;DR: GateChain是一个基于区块链的边境管控系统，用于记录出入境记录，提升数据完整性、可靠性和透明度。


<details>
  <summary>Details</summary>
Motivation: 随着国际流动性增加和安全需求提升，需要具备保密性、完整性和可审计性的出入境记录系统。传统集中式边境管控系统易受数据篡改攻击，且机构间互操作性有限。

Method: 开发GateChain区块链应用，将出入境事件记录在分布式、不可变、可加密验证的账本上，提供实时访问控制和授权机构验证功能。

Result: 论文描述了GateChain的架构和安全组件，并评估了其性能和安全特性。

Conclusion: GateChain能够解决传统边境管控系统的漏洞，通过区块链技术增强数据完整性、可靠性和透明度。

Abstract: Recording entry and exit records for a country, with properties such as confidentiality, integrity, and auditability, is increasingly important due to rising international mobility and security requirements. Traditional border control systems, which rely on centralised databases, are vulnerable to data manipulation and have limited interoperability between institutions. This study presents GateChain, a blockchain-based application that addresses these vulnerabilities. GateChain aims to enhance data integrity, reliability, and transparency by recording entry and exit events on a distributed, immutable, and cryptographically verifiable ledger. The application provides real-time access control and verification for authorised institutions. This paper describes the architecture and security components of GateChain and evaluates its performance and security features.

</details>


### [11] [Document Data Matching for Blockchain-Supported Real Estate](https://arxiv.org/abs/2512.24457)
*Henrique Lin,Tiago Dias,Miguel Correia*

Main category: cs.CR

TL;DR: 该研究提出一个结合OCR、NLP和可验证凭证的房地产文档自动化系统，通过区块链提供去中心化信任层，标准化文档格式并自动检测不一致性。


<details>
  <summary>Details</summary>
Motivation: 房地产行业高度依赖人工文档处理和验证，导致流程效率低下且容易发生欺诈，需要自动化解决方案来提高效率和安全性。

Method: 开发了一个包含OCR-NLP提取管道（基于合成数据集训练）、凭证发行管理后端、以及支持发行者、持有者和验证者交互的前端原型系统。将异构文档格式标准化为可验证凭证，应用自动化数据匹配检测不一致性，区块链提供去中心化信任层。

Result: 模型在多种文档类型上达到有竞争力的准确率，端到端管道显著减少验证时间同时保持可靠性。系统展示了简化房地产交易、增强利益相关者信任、实现可扩展安全数字流程的潜力。

Conclusion: 该框架展示了通过集成OCR、NLP和可验证凭证技术，结合区块链信任层，能够有效自动化房地产文档处理，提高流程效率、透明度和安全性，为房地产数字化转型提供可行方案。

Abstract: The real estate sector remains highly dependent on manual document handling and verification, making processes inefficient and prone to fraud. This work presents a system that integrates optical character recognition (OCR), natural language processing (NLP), and verifiable credentials (VCs) to automate document extraction, verification, and management. The approach standardizes heterogeneous document formats into VCs and applies automated data matching to detect inconsistencies, while the blockchain provides a decentralized trust layer that reinforces transparency and integrity. A prototype was developed that comprises (i) an OCR-NLP extraction pipeline trained on synthetic datasets, (ii) a backend for credential issuance and management, and (iii) a frontend supporting issuer, holder, and verifier interactions. Experimental results show that the models achieve competitive accuracy across multiple document types and that the end-to-end pipeline reduces verification time while preserving reliability. The proposed framework demonstrates the potential to streamline real estate transactions, strengthen stakeholder trust, and enable scalable, secure digital processes.

</details>


### [12] [Correctness of Extended RSA Public Key Cryptosystem](https://arxiv.org/abs/2512.24531)
*Dar-jen Chang,Suranjan Gautam*

Main category: cs.CR

TL;DR: 本文提出了一种验证RSA公钥密码系统正确性的替代方法，探讨了正整数N的选择条件可以超出标准标准的可能性，并推导了N值有效的明确条件。


<details>
  <summary>Details</summary>
Motivation: 现有文献中的传统证明方法存在局限性，本文旨在提供一种略微不同的方法来正式建立RSA的正确性，特别是探索正整数N选择条件的扩展可能性。

Method: 采用替代传统证明的方法论，推导出确定N值有效性的明确条件，分析哪些N值能满足加密方案的正确性要求，哪些会失败。

Result: 得出了RSA-like方案中N值有效的具体条件，解释了某些N值无法满足正确性要求的原因，为RSA正确性证明提供了新的数学框架。

Conclusion: 本文为RSA公钥密码系统的正确性证明提供了一种替代方法，扩展了N值选择条件的理解，但仅限于数学正确性证明，不涉及密码安全性问题。

Abstract: This paper proposes an alternative approach to formally establishing the correctness of the RSA public key cryptosystem. The methodology presented herein deviates slightly from conventional proofs found in existing literature. Specifically, this study explores the conditions under which the choice of the positive integer N, a fundamental component of RSA, can be extended beyond the standard selection criteria. We derive explicit conditions that determine when certain values of N are valid for the encryption scheme and explain why others may fail to satisfy the correctness requirements. The scope of this paper is limited to the mathematical proof of correctness for RSA-like schemes, deliberately omitting issues related to the cryptographic security of RSA.

</details>


### [13] [SynRAG: A Large Language Model Framework for Executable Query Generation in Heterogeneous SIEM System](https://arxiv.org/abs/2512.24571)
*Md Hasan Saju,Austin Page,Akramul Azim,Jeff Gardiner,Farzaneh Abazari,Frank Eargle*

Main category: cs.CR

TL;DR: SynRAG是一个统一框架，能够从平台无关的规范自动生成针对多个SIEM平台的威胁检测或事件调查查询，解决了不同SIEM系统查询语言差异带来的挑战。


<details>
  <summary>Details</summary>
Motivation: 不同SIEM平台（如Qradar、Google SecOps、Splunk等）在属性、架构和查询语言上存在显著差异，导致安全分析师需要接受大量培训或企业需要扩大团队才能有效监控多个平台，这构成了跨平台威胁检测的主要障碍。

Method: SynRAG框架从分析师编写的高级平台无关规范自动生成特定SIEM平台的查询。它避免了分析师为每个平台手动编写单独查询的需求，实现了跨异构SIEM环境的无缝威胁检测和事件调查。

Result: 与GPT、Llama、DeepSeek、Gemma和Claude等最先进语言模型相比，SynRAG在Qradar和SecOps作为代表性SIEM系统的评估中，生成了显著更好的跨SIEM威胁检测和事件调查查询。

Conclusion: SynRAG通过提供统一的查询生成框架，有效解决了SIEM平台多样性带来的挑战，减少了专业培训需求和手动查询翻译工作，提升了跨平台安全监控的效率。

Abstract: Security Information and Event Management (SIEM) systems are essential for large enterprises to monitor their IT infrastructure by ingesting and analyzing millions of logs and events daily. Security Operations Center (SOC) analysts are tasked with monitoring and analyzing this vast data to identify potential threats and take preventive actions to protect enterprise assets. However, the diversity among SIEM platforms, such as Palo Alto Networks Qradar, Google SecOps, Splunk, Microsoft Sentinel and the Elastic Stack, poses significant challenges. As these systems differ in attributes, architecture, and query languages, making it difficult for analysts to effectively monitor multiple platforms without undergoing extensive training or forcing enterprises to expand their workforce. To address this issue, we introduce SynRAG, a unified framework that automatically generates threat detection or incident investigation queries for multiple SIEM platforms from a platform-agnostic specification. SynRAG can generate platformspecific queries from a single high-level specification written by analysts. Without SynRAG, analysts would need to manually write separate queries for each SIEM platform, since query languages vary significantly across systems. This framework enables seamless threat detection and incident investigation across heterogeneous SIEM environments, reducing the need for specialized training and manual query translation. We evaluate SynRAG against state-of-the-art language models, including GPT, Llama, DeepSeek, Gemma, and Claude, using Qradar and SecOps as representative SIEM systems. Our results demonstrate that SynRAG generates significantly better queries for crossSIEM threat detection and incident investigation compared to the state-of-the-art base models.

</details>


### [14] [Secure Digital Semantic Communications: Fundamentals, Challenges, and Opportunities](https://arxiv.org/abs/2512.24602)
*Weixuan Chen,Qianqian Yang,Yuanyuan Jia,Junyu Pan,Shuo Shao,Jincheng Dai,Meixia Tao,Ping Zhang*

Main category: cs.CR

TL;DR: 本文系统分析了数字语义通信的安全威胁与防御机制，对比了模拟与数字语义通信的架构差异，梳理了数字语义通信特有的攻击面，并提出了未来的研究方向。


<details>
  <summary>Details</summary>
Motivation: 语义通信从比特精确传输转向任务导向传输，虽然提高了效率，但引入了新的安全和隐私风险。现有安全研究主要集中在模拟语义通信，而数字语义通信由于采用离散比特/符号传输，具有不同的攻击面，目前缺乏系统分析。

Method: 本文通过回顾语义通信基础，澄清模拟与数字语义通信的架构差异及其安全影响，组织数字语义通信的威胁格局，并讨论潜在的防御机制。

Result: 分析了数字语义通信特有的安全威胁，包括比特/符号级语义信息攻击、调制阶段攻击、基于数据包传输和协议操作攻击等，并识别了概率调制和确定性调制两种主要数字语义通信路线的不同安全特性。

Conclusion: 数字语义通信面临独特的安全挑战，需要系统性的安全分析。本文为构建安全可部署的数字语义通信系统提供了威胁分析和防御框架，并指出了未来的研究方向。

Abstract: Semantic communication (SemCom) has emerged as a promising paradigm for future wireless networks by prioritizing task-relevant meaning over raw data delivery, thereby reducing communication overhead and improving efficiency. However, shifting from bit-accurate transmission to task-oriented delivery introduces new security and privacy risks. These include semantic leakage, semantic manipulation, knowledge base vulnerabilities, model-related attacks, and threats to authenticity and availability. Most existing secure SemCom studies focus on analog SemCom, where semantic features are mapped to continuous channel inputs. In contrast, digital SemCom transmits semantic information through discrete bits or symbols within practical transceiver pipelines, offering stronger compatibility with realworld systems while exposing a distinct and underexplored attack surface. In particular, digital SemCom typically represents semantic information over a finite alphabet through explicit digital modulation, following two main routes: probabilistic modulation and deterministic modulation. These discrete mechanisms and practical transmission procedures introduce additional vulnerabilities affecting bit- or symbol-level semantic information, the modulation stage, and packet-based delivery and protocol operations. Motivated by these challenges and the lack of a systematic analysis of secure digital SemCom, this paper reviews SemCom fundamentals, clarifies the architectural differences between analog and digital SemCom and their security implications, organizes the threat landscape for digital SemCom, and discusses potential defenses. Finally, we outline open research directions toward secure and deployable digital SemCom systems.

</details>


### [15] [Practical Traceable Over-Threshold Multi-Party Private Set Intersection](https://arxiv.org/abs/2512.24652)
*Le Yang,Weijing You,Huiyang He,Kailiang Ji,Jingqiang Lin*

Main category: cs.CR

TL;DR: 本文提出了两种可追踪的过阈值多方隐私集合交集协议，显著提升了现有方案的效率和安全性。


<details>
  <summary>Details</summary>
Motivation: 在数字取证等场景中，多方隐私集合交集需要支持阈值查询（元素出现在至少t个参与方集合中）并追踪元素持有者，但现有协议效率低下且安全性有限。

Method: 提出了两种协议：ET-OT-MP-PSI结合Shamir秘密共享和可编程伪随机函数，抵抗最多t-2个半诚实参与方；ST-OT-MP-PSI进一步利用不经意线性评估协议，抵抗最多n-1个半诚实参与方。

Result: 实验结果显示显著性能提升：当n=5、t=3、集合大小2^14时，ET-OT-MP-PSI比Mahdavi等人协议快15056倍，ST-OT-MP-PSI快505倍，且消除了特殊方不共谋的假设。

Conclusion: 本文提出的两种可追踪过阈值多方隐私集合交集协议在效率和安全性方面均有显著改进，为实际应用提供了更实用的解决方案。

Abstract: Multi-Party Private Set Intersection (MP-PSI) with threshold enhances the flexibility of MP-PSI by disclosing elements present in at least $t$ participants' sets, rather than requiring elements to appear in all $n$ sets. In scenarios where each participant is responsible for its dataset, e.g., digital forensics, MP-PSI with threshold should disclose both intersection elements and corresponding holders such that elements are traceable and the reliability of intersection is guaranteed. We refer to MP-PSI with threshold supporting traceability as Traceable Over-Threshold MP-PSI (T-OT-MP-PSI). However, research on such protocols remains limited, and existing work tolerates at most $t-2$ semi-honest participants at considerable computational cost. We propose two novel Traceable OT-MP-PSI protocols. The first, Efficient Traceable OT-MP-PSI (ET-OT-MP-PSI), combines Shamir's secret sharing with an oblivious programmable pseudorandom function, achieving significantly improved efficiency with resistance to at most $t-2$ semi-honest participants. The second, Security-enhanced Traceable OT-MP-PSI (ST-OT-MP-PSI), achieves security against up to $n-1$ semi-honest participants by further leveraging the oblivious linear evaluation protocol. Compared to Mahdavi et al.'s protocol, ours eliminate the assumption that certain special parties do not collude. Experimental results demonstrate significant improvements: for $n=5$, $t=3$, and sets of size $2^{14}$, ET-OT-MP-PSI achieves $15056\times$ speedup and ST-OT-MP-PSI achieves $505\times$ speedup over Mahdavi et al.'s protocol.

</details>


### [16] [MTSP-LDP: A Framework for Multi-Task Streaming Data Publication under Local Differential Privacy](https://arxiv.org/abs/2512.24899)
*Chang Liu,Junzhou Zhao*

Main category: cs.CR

TL;DR: MTSP-LDP是一个在w-事件本地差分隐私下的多任务流数据发布框架，通过优化隐私预算分配、构建自适应私有二叉树结构，支持复杂查询并提升数据效用。


<details>
  <summary>Details</summary>
Motivation: 现有w-事件本地差分隐私机制存在两个关键限制：1) 主要设计用于发布简单统计量，不适合复杂查询；2) 独立处理每个时间戳数据，未能捕捉时间相关性，导致整体效用降低。

Method: 提出MTSP-LDP框架，包含：1) 最优隐私预算分配算法，通过分析窗口内时间相关性动态分配隐私预算；2) 数据自适应私有二叉树结构，支持复杂查询，并通过跨时间戳分组和平滑操作提升估计精度；3) 无预算多任务处理机制，支持多种流查询而不消耗额外隐私预算。

Result: 在真实世界数据集上的大量实验表明，MTSP-LDP在各种流任务中始终实现高效用，显著优于现有方法。

Conclusion: MTSP-LDP有效解决了现有w-事件本地差分隐私机制在复杂查询支持和时间相关性利用方面的局限性，为流数据发布提供了高效实用的隐私保护解决方案。

Abstract: The proliferation of streaming data analytics in data-driven applications raises critical privacy concerns, as directly collecting user data may compromise personal privacy. Although existing $w$-event local differential privacy (LDP) mechanisms provide formal guarantees without relying on trusted third parties, their practical deployment is hindered by two key limitations. First, these methods are designed primarily for publishing simple statistics at each timestamp, making them inherently unsuitable for complex queries. Second, they handle data at each timestamp independently, failing to capture temporal correlations and consequently degrading the overall utility. To address these issues, we propose MTSP-LDP, a novel framework for \textbf{M}ulti-\textbf{T}ask \textbf{S}treaming data \textbf{P}ublication under $w$-event LDP. MTSP-LDP adopts an \emph{Optimal Privacy Budget Allocation} algorithm to dynamically allocate privacy budgets by analyzing temporal correlations within each window. It then constructs a \emph{data-adaptive private binary tree structure} to support complex queries, which is further refined by cross-timestamp grouping and smoothing operations to enhance estimation accuracy. Furthermore, a unified \emph{Budget-Free Multi-Task Processing} mechanism is introduced to support a variety of streaming queries without consuming additional privacy budget. Extensive experiments on real-world datasets demonstrate that MTSP-LDP consistently achieves high utility across various streaming tasks, significantly outperforming existing methods.

</details>


### [17] [Towards Provably Secure Generative AI: Reliable Consensus Sampling](https://arxiv.org/abs/2512.24925)
*Yu Cui,Hang Fu,Sicheng Pan,Zhuoyu Sun,Yifei Liu,Yuhong Nie,Bo Ran,Baohan Huang,Xufeng Zhang,Haibin Zhang,Cong Zuo,Licheng Wang*

Main category: cs.CR

TL;DR: 本文提出了一种名为可靠共识采样（RCS）的新方法，用于解决现有共识采样（CS）算法在生成式AI安全中的局限性，通过追踪接受概率来容忍极端对抗行为，同时完全避免弃权，提高了鲁棒性和实用性。


<details>
  <summary>Details</summary>
Motivation: 当前生成式AI安全研究主要基于经验性的攻防方法，这种动态经常导致新的未知攻击绕过现有检测和预防机制，需要不断更新安全机制。共识采样（CS）虽然是有前途的可证明安全算法，但存在两个主要问题：1）依赖频繁弃权来避免不安全输出，降低了实用性；2）当不安全模型被恶意操纵时变得高度脆弱。

Method: 提出了可靠共识采样（RCS）这一新原语，通过追踪接受概率来容忍极端对抗行为，提高鲁棒性。RCS完全消除了弃权的需求。进一步开发了反馈算法，持续动态地增强RCS的安全性。提供了理论保证，证明RCS能够维持可控的风险阈值。

Result: 大量实验表明，RCS在保持与CS相当的延迟的同时，显著提高了鲁棒性和实用性。该方法为开发可证明安全的生成式AI做出了贡献。

Conclusion: RCS解决了CS算法的关键局限性，通过追踪接受概率和消除弃权需求，在保持可控风险的同时提高了生成式AI系统的鲁棒性和实用性，为构建具有理论可控风险的可证明安全AI提供了有前景的方向。

Abstract: Existing research on generative AI security is primarily driven by mutually reinforcing attack and defense methodologies grounded in empirical experience. This dynamic frequently gives rise to previously unknown attacks that can circumvent current detection and prevention. This necessitates the continual updating of security mechanisms. Constructing generative AI with provable security and theoretically controllable risk is therefore necessary. Consensus Sampling (CS) is a promising algorithm toward provably secure AI. It controls risk by leveraging overlap in model output probabilities. However, we find that CS relies on frequent abstention to avoid unsafe outputs, which reduces utility. Moreover, CS becomes highly vulnerable when unsafe models are maliciously manipulated. To address these issues, we propose a new primitive called Reliable Consensus Sampling (RCS), that traces acceptance probability to tolerate extreme adversarial behaviors, improving robustness. RCS also eliminates the need for abstention entirely. We further develop a feedback algorithm to continuously and dynamically enhance the safety of RCS. We provide theoretical guarantees that RCS maintains a controllable risk threshold. Extensive experiments show that RCS significantly improves robustness and utility while maintaining latency comparable to CS. We hope this work contributes to the development of provably secure generative AI.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [18] [The Drill-Down and Fabricate Test (DDFT): A Protocol for Measuring Epistemic Robustness in Language Models](https://arxiv.org/abs/2512.23850)
*Rahul Baxi*

Main category: cs.AI

TL;DR: 论文提出DDFT评估框架，测量语言模型在语义压缩和对抗攻击下的认知稳健性，发现模型大小与稳健性无关，错误检测能力是关键瓶颈。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型评估（如MMLU、TruthfulQA）只测量理想条件下的知识掌握，无法评估模型在信息退化或对抗攻击下的稳健性。需要新的评估方法来区分模型是缺乏知识还是验证机制在压力下崩溃。

Method: 提出Drill-Down and Fabricate Test (DDFT)协议，测量认知稳健性。采用两系统认知模型：语义系统生成流畅文本，认知验证器验证事实准确性。评估9个前沿模型在8个知识领域、5个压缩级别（共1800次轮级评估）。

Result: 认知稳健性与传统设计范式正交：参数数量（r=0.083, p=0.832）和架构类型（r=0.153, p=0.695）均不显著预测稳健性。错误检测能力强烈预测整体稳健性（rho=-0.817, p=0.007）。旗舰模型表现出脆弱性，而较小模型可获得稳健性能。

Conclusion: 认知稳健性源于训练方法和验证机制，而非模型规模。DDFT框架为关键应用部署前评估认知稳健性提供了理论基础和实用工具，挑战了模型大小与可靠性的传统假设。

Abstract: Current language model evaluations measure what models know under ideal conditions but not how robustly they know it under realistic stress. Static benchmarks like MMLU and TruthfulQA cannot distinguish a model that lacks knowledge from one whose verification mechanisms collapse when information degrades or adversaries probe for weaknesses. We introduce the Drill-Down and Fabricate Test (DDFT), a protocol that measures epistemic robustness: a model's ability to maintain factual accuracy under progressive semantic compression and adversarial fabrication. We propose a two-system cognitive model comprising a Semantic System that generates fluent text and an Epistemic Verifier that validates factual accuracy. Our findings, based on evaluating 9 frontier models across 8 knowledge domains at 5 compression levels (1,800 turn-level evaluations), reveal that epistemic robustness is orthogonal to conventional design paradigms. Neither parameter count (r=0.083, p=0.832) nor architectural type (r=0.153, p=0.695) significantly predicts robustness, suggesting it emerges from training methodology and verification mechanisms distinct from current approaches. Error detection capability strongly predicts overall robustness (rho=-0.817, p=0.007), indicating this is the critical bottleneck. We find that flagship models exhibit brittleness despite their scale, while smaller models can achieve robust performance, challenging assumptions about the relationship between model size and reliability. The DDFT framework provides both theoretical foundation and practical tools for assessing epistemic robustness before deployment in critical applications.

</details>


### [19] [A Proof-of-Concept for Explainable Disease Diagnosis Using Large Language Models and Answer Set Programming](https://arxiv.org/abs/2512.23932)
*Ioanna Gemou,Evangelos Lamprou*

Main category: cs.AI

TL;DR: McCoy框架结合大语言模型与答案集编程，通过LLM将医学文献转化为ASP代码，结合患者数据进行疾病诊断，提供可解释的预测系统。


<details>
  <summary>Details</summary>
Motivation: 准确的疾病预测对于及时干预、有效治疗和减少医疗并发症至关重要。虽然符号AI已在医疗保健领域应用，但由于构建高质量知识库需要大量努力，其采用仍然有限。

Method: McCoy框架结合大语言模型与答案集编程：1) 使用LLM将医学文献翻译成ASP代码；2) 将生成的ASP代码与患者数据结合；3) 使用ASP求解器处理数据得出最终诊断。

Result: 初步结果显示，McCoy在小规模疾病诊断任务上表现出色，提供了一个强大且可解释的预测框架。

Conclusion: McCoy框架通过结合LLM和ASP的优势，克服了传统符号AI在医疗领域应用中的知识库构建障碍，为疾病诊断提供了可解释的预测解决方案。

Abstract: Accurate disease prediction is vital for timely intervention, effective treatment, and reducing medical complications. While symbolic AI has been applied in healthcare, its adoption remains limited due to the effort required for constructing high-quality knowledge bases. This work introduces McCoy, a framework that combines Large Language Models (LLMs) with Answer Set Programming (ASP) to overcome this barrier. McCoy orchestrates an LLM to translate medical literature into ASP code, combines it with patient data, and processes it using an ASP solver to arrive at the final diagnosis. This integration yields a robust, interpretable prediction framework that leverages the strengths of both paradigms. Preliminary results show McCoy has strong performance on small-scale disease diagnosis tasks.

</details>


### [20] [SPARK: Search Personalization via Agent-Driven Retrieval and Knowledge-sharing](https://arxiv.org/abs/2512.24008)
*Gaurab Chhetri,Subasish Das,Tausif Islam Chowdhury*

Main category: cs.AI

TL;DR: SPARK是一个基于多智能体LLM的个性化搜索框架，通过角色化智能体协作实现动态检索和个性化服务。


<details>
  <summary>Details</summary>
Motivation: 传统个性化搜索系统受限于静态用户画像和单一检索流程，难以捕捉用户动态、多维的信息需求变化。

Method: 1. 定义角色空间（角色、专业知识、任务上下文、领域）；2. 引入角色协调器动态激活相关智能体；3. 每个智能体执行独立的检索增强生成过程；4. 通过结构化通信协议（共享内存、迭代辩论、接力式知识转移）促进智能体协作。

Result: 框架产生了关于协调效率、个性化质量和认知负荷分布的可测试预测，同时包含自适应学习机制用于持续角色优化。

Conclusion: SPARK通过细粒度智能体专业化与协作检索的结合，为下一代能够捕捉人类信息寻求行为复杂性、流动性和上下文敏感性的搜索系统提供了见解。

Abstract: Personalized search demands the ability to model users' evolving, multi-dimensional information needs; a challenge for systems constrained by static profiles or monolithic retrieval pipelines. We present SPARK (Search Personalization via Agent-Driven Retrieval and Knowledge-sharing), a framework in which coordinated persona-based large language model (LLM) agents deliver task-specific retrieval and emergent personalization. SPARK formalizes a persona space defined by role, expertise, task context, and domain, and introduces a Persona Coordinator that dynamically interprets incoming queries to activate the most relevant specialized agents. Each agent executes an independent retrieval-augmented generation process, supported by dedicated long- and short-term memory stores and context-aware reasoning modules. Inter-agent collaboration is facilitated through structured communication protocols, including shared memory repositories, iterative debate, and relay-style knowledge transfer. Drawing on principles from cognitive architectures, multi-agent coordination theory, and information retrieval, SPARK models how emergent personalization properties arise from distributed agent behaviors governed by minimal coordination rules. The framework yields testable predictions regarding coordination efficiency, personalization quality, and cognitive load distribution, while incorporating adaptive learning mechanisms for continuous persona refinement. By integrating fine-grained agent specialization with cooperative retrieval, SPARK provides insights for next-generation search systems capable of capturing the complexity, fluidity, and context sensitivity of human information-seeking behavior.

</details>


### [21] [LoongFlow: Directed Evolutionary Search via a Cognitive Plan-Execute-Summarize Paradigm](https://arxiv.org/abs/2512.24077)
*Chunhui Wan,Xunan Dai,Zhuo Wang,Minglei Li,Yanpeng Wang,Yinan Mao,Yu Lan,Zhiwen Xiao*

Main category: cs.AI

TL;DR: LoongFlow是一个自我进化的智能体框架，通过将LLM集成到"计划-执行-总结"认知范式中，显著提升了进化算法在代码空间中的搜索效率，在保持架构一致性的同时降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 传统进化方法在从静态大语言模型向自我改进智能体过渡时存在结构化推理不足的问题，现有方法在高维代码空间中容易过早收敛且探索效率低下，需要新的框架来解决这些挑战。

Method: LoongFlow将LLM集成到"计划-执行-总结"认知范式中，采用混合进化记忆系统，结合多岛模型、MAP-Elites和自适应玻尔兹曼选择，平衡探索与利用的权衡，保持行为多样性。

Result: 在AlphaEvolve基准测试和Kaggle竞赛中，LoongFlow比现有基线方法（如OpenEvolve、ShinkaEvolve）在进化效率上提升高达60%，同时发现更优的解决方案。

Conclusion: LoongFlow在自主科学发现方面迈出了重要一步，能够以更低的计算开销生成专家级解决方案，标志着从静态LLM到自我进化智能体的重要进展。

Abstract: The transition from static Large Language Models (LLMs) to self-improving agents is hindered by the lack of structured reasoning in traditional evolutionary approaches. Existing methods often struggle with premature convergence and inefficient exploration in high-dimensional code spaces. To address these challenges, we introduce LoongFlow, a self-evolving agent framework that achieves state-of-the-art solution quality with significantly reduced computational costs. Unlike "blind" mutation operators, LoongFlow integrates LLMs into a cognitive "Plan-Execute-Summarize" (PES) paradigm, effectively mapping the evolutionary search to a reasoning-heavy process. To sustain long-term architectural coherence, we incorporate a hybrid evolutionary memory system. By synergizing Multi-Island models with MAP-Elites and adaptive Boltzmann selection, this system theoretically balances the exploration-exploitation trade-off, maintaining diverse behavioral niches to prevent optimization stagnation. We instantiate LoongFlow with a General Agent for algorithmic discovery and an ML Agent for pipeline optimization. Extensive evaluations on the AlphaEvolve benchmark and Kaggle competitions demonstrate that LoongFlow outperforms leading baselines (e.g., OpenEvolve, ShinkaEvolve) by up to 60% in evolutionary efficiency while discovering superior solutions. LoongFlow marks a substantial step forward in autonomous scientific discovery, enabling the generation of expert-level solutions with reduced computational overhead.

</details>


### [22] [Graph-Based Exploration for ARC-AGI-3 Interactive Reasoning Tasks](https://arxiv.org/abs/2512.24156)
*Evgenii Rudakov,Jonathan Shock,Benjamin Ultan Cowley*

Main category: cs.AI

TL;DR: 提出一种无需训练、基于图结构的交互推理方法，在ARC-AGI-3基准测试中显著优于当前最先进的LLMs


<details>
  <summary>Details</summary>
Motivation: 当前最先进的LLMs无法可靠解决ARC-AGI-3中的交互推理任务，这些任务需要智能体通过有限交互推断任务机制，并在复杂度递增的关卡中适应。需要一种能够形成假设、测试假设并跟踪已发现机制的方法。

Method: 结合基于视觉的帧处理与使用图结构表示的系统化状态空间探索。将视觉帧分割为有意义的组件，基于视觉显著性优先选择动作，维护探索状态和转换的有向图。通过跟踪已访问状态和已测试动作，优先选择提供到未测试状态-动作对最短路径的动作。

Result: 在ARC-AGI-3预览挑战中，该方法在六个游戏中解决了52个关卡中的中位数30个，在私有排行榜上排名第3，显著优于前沿的基于LLM的智能体。

Conclusion: 即使没有学习，显式的图结构探索也可以作为交互推理的强大基线，并强调了在稀疏反馈环境中系统化状态跟踪和动作优先级的重要性，这些环境中当前LLMs无法捕捉任务动态。

Abstract: We present a training-free graph-based approach for solving interactive reasoning tasks in the ARC-AGI-3 benchmark. ARC-AGI-3 comprises game-like tasks where agents must infer task mechanics through limited interactions, and adapt to increasing complexity as levels progress. Success requires forming hypotheses, testing them, and tracking discovered mechanics. The benchmark has revealed that state-of-the-art LLMs are currently incapable of reliably solving these tasks. Our method combines vision-based frame processing with systematic state-space exploration using graph-structured representations. It segments visual frames into meaningful components, prioritizes actions based on visual salience, and maintains a directed graph of explored states and transitions. By tracking visited states and tested actions, the agent prioritizes actions that provide the shortest path to untested state-action pairs. On the ARC-AGI-3 Preview Challenge, this structured exploration strategy solves a median of 30 out of 52 levels across six games and ranks 3rd on the private leaderboard, substantially outperforming frontier LLM-based agents. These results demonstrate that explicit graph-structured exploration, even without learning, can serve as a strong baseline for interactive reasoning and underscore the importance of systematic state tracking and action prioritization in sparse-feedback environments where current LLMs fail to capture task dynamics. The code is open source and available at https://github.com/dolphin-in-a-coma/arc-agi-3-just-explore.

</details>


### [23] [Constrained Language Model Policy Optimization via Risk-aware Stepwise Alignment](https://arxiv.org/abs/2512.24263)
*Lijun Zhang,Lin Li,Wei Wei,Yajie Qi,Huizhong Song,Jun Wang,Yaodong Yang,Jiye Liang*

Main category: cs.AI

TL;DR: RSA是一种风险感知的分步对齐方法，通过嵌套风险度量在策略优化中显式纳入风险意识，以解决传统风险中性对齐方法在处理罕见但灾难性有害行为方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有安全对齐方法（如Safe RLHF和SACPO）通常在风险中性范式下运行，无法充分处理偏离参考策略带来的风险，且对罕见但潜在灾难性有害行为的鲁棒性有限。需要一种能显式纳入风险意识的方法来确保安全性和可信度。

Method: 提出风险感知分步对齐（RSA）方法，将安全对齐表述为令牌级风险感知约束策略优化问题，通过分步对齐程序解决，利用嵌套风险度量推导令牌级策略更新。该方法能减轻因过度偏离参考策略引起的风险，并显式抑制低概率高影响的有害行为。

Result: 实验结果表明，该方法在保持高水平帮助性的同时确保强安全性，并显著抑制尾部风险（即低概率但高影响的不安全响应）。理论分析在温和假设下证明了策略最优性。

Conclusion: RSA通过显式纳入风险意识到策略优化中，提供了一种更安全、更鲁棒的语言模型对齐方法，能够有效处理传统风险中性方法难以应对的罕见但灾难性风险。

Abstract: When fine-tuning pre-trained Language Models (LMs) to exhibit desired behaviors, maintaining control over risk is critical for ensuring both safety and trustworthiness. Most existing safety alignment methods, such as Safe RLHF and SACPO, typically operate under a risk-neutral paradigm that is insufficient to address the risks arising from deviations from the reference policy and offers limited robustness against rare but potentially catastrophic harmful behaviors. To address this limitation, we propose Risk-aware Stepwise Alignment (RSA), a novel alignment method that explicitly incorporates risk awareness into the policy optimization process by leveraging a class of nested risk measures. Specifically, RSA formulates safety alignment as a token-level risk-aware constrained policy optimization problem and solves it through a stepwise alignment procedure that yields token-level policy updates derived from the nested risk measures. This design offers two key benefits: (1) it mitigates risks induced by excessive model shift away from a reference policy, and (2) it explicitly suppresses low-probability yet high-impact harmful behaviors. Moreover, we provide theoretical analysis on policy optimality under mild assumptions. Experimental results demonstrate that our method achieves high levels of helpfulness while ensuring strong safety and significantly suppresses tail risks, namely low-probability yet high-impact unsafe responses.

</details>


### [24] [What Drives Success in Physical Planning with Joint-Embedding Predictive World Models?](https://arxiv.org/abs/2512.24497)
*Basile Terver,Tsung-Yen Yang,Jean Ponce,Adrien Bardes,Yann LeCun*

Main category: cs.AI

TL;DR: 本文提出JEPA-WM模型，通过在世界模型的表示空间中进行规划，提高了AI智能体在物理任务中的泛化能力，在导航和操作任务中超越了现有基线方法。


<details>
  <summary>Details</summary>
Motivation: AI领域长期面临的一个挑战是开发能够解决广泛物理任务并能泛化到未见任务和环境的智能体。现有方法通常在输入空间进行规划，但在世界模型的表示空间中进行规划有望通过抽象无关细节来提高规划效率。

Method: 提出JEPA-WM模型家族，系统研究模型架构、训练目标和规划算法等关键组件。通过模拟环境和真实机器人数据进行实验，分析各组件对规划成功率的影响。

Result: 提出的模型在导航和操作任务中超越了DINO-WM和V-JEPA-2-AC两个基准方法。代码、数据和检查点已开源。

Conclusion: 在世界模型的表示空间中进行规划是有效的，通过系统优化模型架构、训练目标和规划算法，可以显著提高AI智能体在物理任务中的泛化能力和规划效率。

Abstract: A long-standing challenge in AI is to develop agents capable of solving a wide range of physical tasks and generalizing to new, unseen tasks and environments. A popular recent approach involves training a world model from state-action trajectories and subsequently use it with a planning algorithm to solve new tasks. Planning is commonly performed in the input space, but a recent family of methods has introduced planning algorithms that optimize in the learned representation space of the world model, with the promise that abstracting irrelevant details yields more efficient planning. In this work, we characterize models from this family as JEPA-WMs and investigate the technical choices that make algorithms from this class work. We propose a comprehensive study of several key components with the objective of finding the optimal approach within the family. We conducted experiments using both simulated environments and real-world robotic data, and studied how the model architecture, the training objective, and the planning algorithm affect planning success. We combine our findings to propose a model that outperforms two established baselines, DINO-WM and V-JEPA-2-AC, in both navigation and manipulation tasks. Code, data and checkpoints are available at https://github.com/facebookresearch/jepa-wms.

</details>


### [25] [Evaluating the Reasoning Abilities of LLMs on Underrepresented Mathematics Competition Problems](https://arxiv.org/abs/2512.24505)
*Samuel Golladay,Majid Bani-Yaghoub*

Main category: cs.AI

TL;DR: 该研究分析了三种领先大语言模型在代表性不足的数学竞赛问题上的表现，发现DeepSeek-V3在所有类别中表现最佳，但所有模型在几何问题上都表现较弱，且不同类型的模型有特定的错误模式。


<details>
  <summary>Details</summary>
Motivation: 现有研究大多使用相同的数据集评估大语言模型的数学推理能力，这限制了研究结果的普适性，可能无法充分捕捉数学任务中的多样化挑战。本研究旨在分析大语言模型在代表性不足的数学竞赛问题上的表现。

Method: 研究使用密苏里大学数学竞赛中的微积分、解析几何和离散数学问题，测试了GPT-4o-mini、Gemini-2.0-Flash和DeepSeek-V3三种领先大语言模型。将模型回答与已知正确答案进行比较以确定准确性，并分析模型的推理过程以探索不同问题类型和模型间的错误模式。

Result: DeepSeek-V3在微积分、解析几何和离散数学三个类别中表现最佳，无论是推理过程还是最终答案正确率都最好。所有三种大语言模型在几何问题上都表现出明显的弱点。DeepSeek-V3的主要错误是计算和逻辑错误，GPT-4o-mini经常出现逻辑和方法相关错误，而Gemini则倾向于推理不完整和草率得出结论。

Conclusion: 在代表性不足的数学竞赛数据集上评估大语言模型可以提供更深入的见解，揭示它们不同的错误模式，并突出结构化推理方面的持续挑战，特别是在几何领域。

Abstract: Understanding the limitations of Large Language Models, or LLMs, in mathematical reasoning has been the focus of several recent studies. However, the majority of these studies use the same datasets for benchmarking, which limits the generalizability of their findings and may not fully capture the diverse challenges present in mathematical tasks. The purpose of the present study is to analyze the performance of LLMs on underrepresented mathematics competition problems. We prompted three leading LLMs, namely GPT-4o-mini, Gemini-2.0-Flash, and DeepSeek-V3, with the Missouri Collegiate Mathematics Competition problems in the areas of Calculus, Analytic Geometry, and Discrete Mathematics. The LLMs responses were then compared to the known correct solutions in order to determine the accuracy of the LLM for each problem domain. We also analyzed the LLMs reasoning to explore patterns in errors across problem types and models. DeepSeek-V3 has the best performance in all three categories of Calculus, Analytic Geometry, and Discrete Mathematics, both in reasoning and correct final answers. All three LLMs exhibited notably weak performance in Geometry. The majority of errors made by DeepSeek-V3 were attributed to computational and logical mistakes, whereas GPT-4o-mini frequently exhibited logical and approach-related errors. Gemini, on the other hand, tended to struggle with incomplete reasoning and drawing rushed conclusions. In conclusion, evaluating LLMs on underrepresented mathematics competition datasets can provide deeper insights into their distinct error patterns and highlight ongoing challenges in structured reasoning, particularly within the domain of Geometry.

</details>


### [26] [From Building Blocks to Planning: Multi-Step Spatial Reasoning in LLMs with Reinforcement Learning](https://arxiv.org/abs/2512.24532)
*Amir Tahmasbi,Sadegh Majidi,Kazem Taram,Aniket Bera*

Main category: cs.AI

TL;DR: 该论文提出了一种两阶段方法，将空间推理分解为原子构建块及其组合，通过监督微调学习基本空间物理，然后使用GRPO框架训练轻量级LoRA适配器进行多步规划，在ASCII艺术环境中显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在通用语言能力上表现出色，但在结构化环境中的空间变换和多步规划方面仍然存在困难。需要一种方法能够将空间推理分解为基本构建块，并有效组合这些块来解决复杂的空间规划问题。

Method: 采用两阶段方法：第一阶段通过监督微调学习基本空间变换（旋转、平移、缩放），使模型具备基础空间物理知识；第二阶段冻结物理感知模型，在GRPO框架内训练轻量级LoRA适配器，以闭环方式学习将这些构建块组合用于谜题环境中的多步规划。为此合成了ASCII艺术数据集并构建了相应的强化学习环境。

Result: 该方法在动态环境（有明确状态更新）和静态环境（模型必须依赖内部状态）中均一致优于基线方法，包括通用骨干模型、物理感知模型和端到端RL模型。此外，该方法收敛更快，训练更稳定，注意力模式分析显示微调确实带来了空间理解的实质性改进。

Conclusion: 将空间推理分解为原子构建块及其组合的两阶段方法有效提升了LLMs在结构化环境中的空间推理能力，特别是在多步规划任务中表现出色，为导航和规划应用提供了有前景的解决方案。

Abstract: Spatial reasoning in large language models (LLMs) has gained increasing attention due to applications in navigation and planning. Despite strong general language capabilities, LLMs still struggle with spatial transformations and multi-step planning in structured environments. We propose a two-stage approach that decomposes spatial reasoning into atomic building blocks and their composition. First, we apply supervised fine-tuning on elementary spatial transformations, such as rotation, translation, and scaling, to equip the model with basic spatial physics. We then freeze this physics-aware model and train lightweight LoRA adapters within the GRPO framework to learn policies that compose these building blocks for multi-step planning in puzzle-based environments, in a closed-loop manner. To support this pipeline, we synthesize an ASCII-art dataset and construct a corresponding ASCII-based reinforcement learning environment. Our method consistently outperforms baselines, including the generic backbone, physics-aware model, and end-to-end RL models, under both Dynamic environments with explicit state updates and Static environments where the model must rely on its internal state across steps. In addition, the proposed approach converges faster and exhibits more stable training compared to end-to-end reinforcement learning from scratch. Finally, we analyze attention patterns to assess whether fine-tuning induces meaningful improvements in spatial understanding.

</details>


### [27] [Recursive Language Models](https://arxiv.org/abs/2512.24601)
*Alex L. Zhang,Tim Kraska,Omar Khattab*

Main category: cs.AI

TL;DR: RLMs是一种让LLM处理超长提示的推理策略，通过将长提示视为外部环境，让模型以编程方式检查、分解并递归调用自身处理提示片段，实现超出模型上下文窗口两个数量级的输入处理能力。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型的上下文窗口有限，无法处理任意长度的提示，这限制了它们在需要处理长文档、复杂任务等场景中的应用。需要一种方法让LLM能够处理超出其原始上下文窗口的输入。

Method: 提出递归语言模型（RLMs）推理策略：将长提示视为外部环境，让LLM以编程方式检查、分解提示，并递归调用自身处理提示片段。这种方法允许模型通过多次调用逐步处理超长内容。

Result: RLMs能够成功处理超出模型上下文窗口两个数量级的输入，在四个不同的长上下文任务中，即使对于较短的提示，RLMs也显著优于基础LLM和常见的长上下文支架，同时每个查询的成本相当或更低。

Conclusion: RLMs提供了一种有效的推理时扩展策略，使LLM能够处理任意长度的提示，显著提升了长上下文处理能力，同时保持了成本效益，为LLM在需要处理长文档的复杂任务中应用提供了新途径。

Abstract: We study allowing large language models (LLMs) to process arbitrarily long prompts through the lens of inference-time scaling. We propose Recursive Language Models (RLMs), a general inference strategy that treats long prompts as part of an external environment and allows the LLM to programmatically examine, decompose, and recursively call itself over snippets of the prompt. We find that RLMs successfully handle inputs up to two orders of magnitude beyond model context windows and, even for shorter prompts, dramatically outperform the quality of base LLMs and common long-context scaffolds across four diverse long-context tasks, while having comparable (or cheaper) cost per query.

</details>


### [28] [Reinforcement Learning-Augmented LLM Agents for Collaborative Decision Making and Performance Optimization](https://arxiv.org/abs/2512.24609)
*Dong Qiu,Duo Xu,Limengxi Yue*

Main category: cs.AI

TL;DR: 提出基于强化学习的多智能体LLM协作框架，通过Dec-POMDP建模协作，采用CTDE训练，引入GRPO优化策略，在写作和编码任务中显著提升协作效率


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在单任务上表现良好，但在多智能体协作场景中缺乏协作意识，难以优化全局性能，需要解决多智能体协作的协调问题

Method: 1) 将协作建模为分散部分可观测马尔可夫决策过程(Dec-POMDP)；2) 采用集中训练分散执行(CTDE)框架；3) 引入组相对策略优化(GRPO)算法，在训练时利用全局信号联合优化智能体策略；4) 设计简化的联合奖励函数，平衡任务质量、速度和协调成本

Result: 在协作写作和编码基准测试中：任务处理速度比单智能体基线提升3倍；写作任务中结构/风格一致性达到98.7%；编码任务中测试通过率达到74.6%；持续优于强大多智能体LLM基线

Conclusion: 该框架为复杂工作流中的可靠协作提供了实用路径，通过强化学习增强LLM智能体的协作能力，显著提升了多智能体系统的全局性能

Abstract: Large Language Models (LLMs) perform well in language tasks but often lack collaborative awareness and struggle to optimize global performance in multi-agent settings. We present a reinforcement learning-augmented LLM agent framework that formulates cooperation as a decentralized partially observable Markov decision process (Dec-POMDP) and adopts centralized training with decentralized execution (CTDE). We introduce Group Relative Policy Optimization (GRPO) to jointly optimize agent policies with access to global signals during training, together with a simplified joint reward that balances task quality, speed, and coordination cost. On collaborative writing and coding benchmarks, our framework delivers a 3x increase in task processing speed over single-agent baselines, 98.7% structural/style consistency in writing, and a 74.6% test pass rate in coding. The approach consistently outperforms strong multi-agent LLM baselines and provides a practical path toward reliable collaboration in complex workflows.

</details>


### [29] [Group Deliberation Oriented Multi-Agent Conversational Model for Complex Reasoning](https://arxiv.org/abs/2512.24613)
*Zheyu Shi,Dong Qiu,Shanlong Yu*

Main category: cs.AI

TL;DR: 提出基于群体审议的多智能体对话模型，通过三层角色架构（生成、验证、整合）提升复杂推理能力，相比单一大语言模型在多项多跳推理任务上准确率提升14-19%。


<details>
  <summary>Details</summary>
Motivation: 单一大语言模型在复杂推理任务中存在局限性，需要更有效的协作机制来处理多步骤推理、事实验证和逻辑一致性等问题。

Method: 采用三层角色架构：意见生成智能体产生多样化推理视角，证据验证智能体检索外部知识并量化事实支持度，一致性仲裁智能体整合逻辑一致的结论。引入自博弈机制扩展多路径推理轨迹，检索增强模块动态补充外部知识，设计结合事实一致性和逻辑连贯性的复合奖励函数，并应用改进的近端策略优化策略进行协作训练。

Result: 在HotpotQA上多跳推理准确率提升16.8%，在2WikiMultihopQA上提升14.3%，在MeetingBank上提升19.2%，一致性提升21.5%。相比主流多智能体方法具有更高的推理效率。

Conclusion: 该模型为复杂推理任务提供了有效且稳定的解决方案，通过群体审议的多智能体协作机制显著提升了推理准确性和一致性。

Abstract: This paper proposes a group deliberation oriented multi-agent conversational model to address the limitations of single large language models in complex reasoning tasks. The model adopts a three-level role division architecture consisting of generation, verification, and integration. An opinion generation agent produces diverse reasoning perspectives, an evidence verification agent retrieves external knowledge and quantifies factual support, and a consistency arbitration agent integrates logically coherent conclusions. A self-game mechanism is introduced to expand multi-path reasoning trajectories, while a retrieval enhancement module dynamically supplements external knowledge. A composite reward function combining factual consistency and logical coherence is designed, and an improved proximal policy optimization strategy is applied for collaborative training. Experimental results show that the proposed model improves multi-hop reasoning accuracy by 16.8 percent on HotpotQA, 14.3 percent on 2WikiMultihopQA, and 19.2 percent on MeetingBank, while improving consistency by 21.5 percent. The model achieves higher reasoning efficiency than mainstream multi-agent approaches, providing an effective and stable solution for complex reasoning tasks.

</details>


### [30] [Youtu-Agent: Scaling Agent Productivity with Automated Generation and Hybrid Policy Optimization](https://arxiv.org/abs/2512.24615)
*Yuchen Shi,Yuzheng Cai,Siqi Cai,Zihan Xu,Lichao Chen,Yulei Qin,Zhijian Zhou,Xiang Fei,Chaofan Qiu,Xiaoyu Tan,Gang Li,Zongyi Li,Haojia Lin,Guocan Cai,Yong Mao,Yunsheng Wu,Ke Li,Xing Sun*

Main category: cs.AI

TL;DR: Youtu-Agent是一个用于自动化生成和持续演化的LLM智能体框架，通过模块化设计解决现有框架配置成本高和能力静态的问题，支持工作流和元智能体两种生成模式，并包含实践和强化学习两种优化机制。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体框架面临两个主要挑战：1）高配置成本，构建高质量智能体需要大量手动工具集成和提示工程；2）静态能力，部署后的智能体难以适应动态环境，需要昂贵的微调。

Method: 提出Youtu-Agent框架，具有结构化配置系统，解耦执行环境、工具集和上下文管理。引入两种生成范式：工作流模式用于标准任务，元智能体模式用于复杂非标准需求，能自动生成工具代码、提示和配置。建立混合策略优化系统：1）智能体实践模块，通过上下文优化积累经验；2）智能体RL模块，集成分布式训练框架进行端到端大规模强化学习。

Result: 在WebWalkerQA（71.47%）和GAIA（72.8%）上达到SOTA性能；自动化生成管道工具合成成功率超过81%；实践模块在AIME 2024/2025上分别提升+2.7%和+5.4%；Agent RL训练在7B LLMs上实现40%加速，在数学和通用/多跳QA基准上分别提升编码/推理能力35%和搜索能力21%。

Conclusion: Youtu-Agent通过自动化生成和持续演化机制，有效解决了LLM智能体框架的高配置成本和静态能力问题，实现了灵活、可扩展的智能体开发与优化。

Abstract: Existing Large Language Model (LLM) agent frameworks face two significant challenges: high configuration costs and static capabilities. Building a high-quality agent often requires extensive manual effort in tool integration and prompt engineering, while deployed agents struggle to adapt to dynamic environments without expensive fine-tuning. To address these issues, we propose \textbf{Youtu-Agent}, a modular framework designed for the automated generation and continuous evolution of LLM agents. Youtu-Agent features a structured configuration system that decouples execution environments, toolkits, and context management, enabling flexible reuse and automated synthesis. We introduce two generation paradigms: a \textbf{Workflow} mode for standard tasks and a \textbf{Meta-Agent} mode for complex, non-standard requirements, capable of automatically generating tool code, prompts, and configurations. Furthermore, Youtu-Agent establishes a hybrid policy optimization system: (1) an \textbf{Agent Practice} module that enables agents to accumulate experience and improve performance through in-context optimization without parameter updates; and (2) an \textbf{Agent RL} module that integrates with distributed training frameworks to enable scalable and stable reinforcement learning of any Youtu-Agents in an end-to-end, large-scale manner. Experiments demonstrate that Youtu-Agent achieves state-of-the-art performance on WebWalkerQA (71.47\%) and GAIA (72.8\%) using open-weight models. Our automated generation pipeline achieves over 81\% tool synthesis success rate, while the Practice module improves performance on AIME 2024/2025 by +2.7\% and +5.4\% respectively. Moreover, our Agent RL training achieves 40\% speedup with steady performance improvement on 7B LLMs, enhancing coding/reasoning and searching capabilities respectively up to 35\% and 21\% on Maths and general/multi-hop QA benchmarks.

</details>


### [31] [Multi-modal cross-domain mixed fusion model with dual disentanglement for fault diagnosis under unseen working conditions](https://arxiv.org/abs/2512.24679)
*Pengcheng Xia,Yixiang Huang,Chengjin Qin,Chengliang Liu*

Main category: cs.AI

TL;DR: 提出多模态跨域混合融合模型，通过双重解耦框架分离模态不变/特定特征和域不变/特定表示，结合跨域混合融合策略和三模态融合机制，提升电机故障诊断在未见工况下的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有故障诊断方法在真实场景中面临性能下降问题，特别是当模型在未见工况下测试时。现有域适应方法依赖目标域样本，且大多数研究依赖单模态传感信号，忽视了多模态信息的互补性对提升模型泛化能力的作用。

Method: 提出多模态跨域混合融合模型，包含：1）双重解耦框架，分离模态不变特征和模态特定特征，以及域不变表示和域特定表示；2）跨域混合融合策略，随机混合不同域的模态信息以增强模态和域多样性；3）三模态融合机制，自适应整合多模态异构信息。

Result: 在感应电机故障诊断实验中，包括未见恒定工况和时变工况，该方法始终优于先进方法。全面的消融研究进一步验证了每个提出组件和多模态融合的有效性。

Conclusion: 该方法通过双重解耦、跨域混合融合和多模态自适应融合，有效提升了故障诊断模型在未见工况下的泛化能力，解决了现有方法在真实场景中的性能下降问题。

Abstract: Intelligent fault diagnosis has become an indispensable technique for ensuring machinery reliability. However, existing methods suffer significant performance decline in real-world scenarios where models are tested under unseen working conditions, while domain adaptation approaches are limited to their reliance on target domain samples. Moreover, most existing studies rely on single-modal sensing signals, overlooking the complementary nature of multi-modal information for improving model generalization. To address these limitations, this paper proposes a multi-modal cross-domain mixed fusion model with dual disentanglement for fault diagnosis. A dual disentanglement framework is developed to decouple modality-invariant and modality-specific features, as well as domain-invariant and domain-specific representations, enabling both comprehensive multi-modal representation learning and robust domain generalization. A cross-domain mixed fusion strategy is designed to randomly mix modality information across domains for modality and domain diversity augmentation. Furthermore, a triple-modal fusion mechanism is introduced to adaptively integrate multi-modal heterogeneous information. Extensive experiments are conducted on induction motor fault diagnosis under both unseen constant and time-varying working conditions. The results demonstrate that the proposed method consistently outperforms advanced methods and comprehensive ablation studies further verify the effectiveness of each proposed component and multi-modal fusion. The code is available at: https://github.com/xiapc1996/MMDG.

</details>


### [32] [Explaining Why Things Go Where They Go: Interpretable Constructs of Human Organizational Preferences](https://arxiv.org/abs/2512.24829)
*Emmanuel Fashae,Michael Burke,Leimin Tian,Lingheng Meng,Pamela Carreno-Medrano*

Main category: cs.AI

TL;DR: 该研究提出了一个包含四个可解释维度的物体摆放偏好模型：空间实用性、习惯便利性、语义连贯性和常识适当性，并通过问卷验证了这些维度，最后将其集成到MCTS规划器中用于机器人家庭物品整理。


<details>
  <summary>Details</summary>
Motivation: 当前基于人类演示的潜在偏好模型虽然能有效预测，但缺乏对指导人类决策的可解释因素的理解。研究旨在开发一个明确、可解释的物体摆放偏好模型，以增强机器人系统对人类偏好的理解和执行能力。

Method: 1. 提出了包含四个可解释维度的物体摆放偏好模型；2. 设计并验证了自报告问卷，通过63名参与者的在线研究确认了这些维度的心理区分度；3. 将参与者得出的偏好集成到蒙特卡洛树搜索（MCTS）规划器中；4. 在厨房和客厅两种场景下测试了模型的有效性。

Result: 1. 问卷研究证实了四个维度的心理区分度和解释力；2. 基于参与者偏好的MCTS规划器能够生成合理的物品摆放方案；3. 规划器生成的摆放方案与参与者生成的方案高度一致。

Conclusion: 该研究贡献了一个紧凑、可解释的物体摆放偏好模型，并展示了如何将其操作化用于机器人规划。这为机器人家庭物品整理系统提供了更透明、更符合人类偏好的决策框架。

Abstract: Robotic systems for household object rearrangement often rely on latent preference models inferred from human demonstrations. While effective at prediction, these models offer limited insight into the interpretable factors that guide human decisions. We introduce an explicit formulation of object arrangement preferences along four interpretable constructs: spatial practicality (putting items where they naturally fit best in the space), habitual convenience (making frequently used items easy to reach), semantic coherence (placing items together if they are used for the same task or are contextually related), and commonsense appropriateness (putting things where people would usually expect to find them). To capture these constructs, we designed and validated a self-report questionnaire through a 63-participant online study. Results confirm the psychological distinctiveness of these constructs and their explanatory power across two scenarios (kitchen and living room). We demonstrate the utility of these constructs by integrating them into a Monte Carlo Tree Search (MCTS) planner and show that when guided by participant-derived preferences, our planner can generate reasonable arrangements that closely align with those generated by participants. This work contributes a compact, interpretable formulation of object arrangement preferences and a demonstration of how it can be operationalized for robot planning.

</details>


### [33] [GenZ: Foundational models as latent variable generators within traditional statistical models](https://arxiv.org/abs/2512.24834)
*Marko Jojic,Nebojsa Jojic*

Main category: cs.AI

TL;DR: GenZ是一个混合模型，通过可解释的语义特征桥接基础模型和统计建模。它通过迭代过程发现数据集特定的语义特征描述，而不是依赖基础模型的领域知识，显著提升了预测性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然拥有广泛的领域知识，但往往无法捕捉对预测任务至关重要的数据集特定模式。基础模型的领域理解与数据集特定模式之间存在差距，需要一种方法来发现这些特定模式。

Method: 采用迭代过程通过统计建模误差对比项目组来发现语义特征描述，而不是依赖基础模型的领域理解。将其表述为广义EM算法，联合优化语义特征描述符和统计模型参数。使用冻结的基础模型基于发现的特征对项目进行分类，将这些判断视为预测实值目标的潜在二元特征的噪声观测。

Result: 在房价预测方面，使用多模态列表数据发现的语义特征实现了12%的中位数相对误差，显著优于依赖LLM通用领域知识的GPT-5基线（38%误差）。在Netflix电影嵌入方面，仅从语义描述预测协同过滤表示，余弦相似度达到0.59，相当于传统协同过滤需要约4000个用户评分才能达到的性能。

Conclusion: GenZ成功桥接了基础模型和统计建模，通过发现数据集特定的语义特征显著提升了预测性能。发现的特征揭示了与模型领域知识不同的数据集特定模式，证明了该方法在结合基础模型广泛知识与统计建模精确性方面的有效性。

Abstract: We present GenZ, a hybrid model that bridges foundational models and statistical modeling through interpretable semantic features. While large language models possess broad domain knowledge, they often fail to capture dataset-specific patterns critical for prediction tasks. Our approach addresses this by discovering semantic feature descriptions through an iterative process that contrasts groups of items identified via statistical modeling errors, rather than relying solely on the foundational model's domain understanding. We formulate this as a generalized EM algorithm that jointly optimizes semantic feature descriptors and statistical model parameters. The method prompts a frozen foundational model to classify items based on discovered features, treating these judgments as noisy observations of latent binary features that predict real-valued targets through learned statistical relationships. We demonstrate the approach on two domains: house price prediction (hedonic regression) and cold-start collaborative filtering for movie recommendations. On house prices, our model achieves 12\% median relative error using discovered semantic features from multimodal listing data, substantially outperforming a GPT-5 baseline (38\% error) that relies on the LLM's general domain knowledge. For Netflix movie embeddings, our model predicts collaborative filtering representations with 0.59 cosine similarity purely from semantic descriptions -- matching the performance that would require approximately 4000 user ratings through traditional collaborative filtering. The discovered features reveal dataset-specific patterns (e.g., architectural details predicting local housing markets, franchise membership predicting user preferences) that diverge from the model's domain knowledge alone.

</details>


### [34] [A study on constraint extraction and exception exclusion in care worker scheduling](https://arxiv.org/abs/2512.24853)
*Koki Suenaga,Tomohiro Furuta,Satoshi Ono*

Main category: cs.AI

TL;DR: 提出一种使用约束模板从养老机构管理者访谈中提取设施特定约束条件的方法，用于生成护理人员排班表


<details>
  <summary>Details</summary>
Motivation: 养老机构的排班条件因机构而异，需要通过与制定排班的管理者访谈来设计设施特定的约束条件，但现有约束提取技术缺乏排除例外约束的机制

Method: 使用约束模板提取各种组件的组合（如连续工作班次模式、员工组合等），通过改变关注的天数和员工数量，以及将提取焦点调整为模式或频率来提取多样约束，并包含排除例外约束的机制

Result: 实验表明，该方法成功创建了满足所有硬约束的排班表，并通过避免提取例外约束减少了软约束违反的数量

Conclusion: 提出的约束模板方法能够有效提取养老机构特定的约束条件，结合约束规划求解器可生成满足要求的护理人员排班表

Abstract: Technologies for automatically generating work schedules have been extensively studied; however, in long-term care facilities, the conditions vary between facilities, making it essential to interview the managers who create shift schedules to design facility-specific constraint conditions. The proposed method utilizes constraint templates to extract combinations of various components, such as shift patterns for consecutive days or staff combinations. The templates can extract a variety of constraints by changing the number of days and the number of staff members to focus on and changing the extraction focus to patterns or frequency. In addition, unlike existing constraint extraction techniques, this study incorporates mechanisms to exclude exceptional constraints. The extracted constraints can be employed by a constraint programming solver to create care worker schedules. Experiments demonstrated that our proposed method successfully created schedules that satisfied all hard constraints and reduced the number of violations for soft constraints by circumventing the extraction of exceptional constraints.

</details>


### [35] [Semi-Automated Data Annotation in Multisensor Datasets for Autonomous Vehicle Testing](https://arxiv.org/abs/2512.24896)
*Andrii Gamalii,Daniel Górniak,Robert Nowak,Bartłomiej Olber,Krystian Radlak,Jakub Winter*

Main category: cs.AI

TL;DR: DARTS项目开发了一个半自动数据标注流水线，用于创建波兰驾驶场景的大规模多模态数据集，通过人机协同方法结合AI与人工标注，显著降低了标注成本和时间。


<details>
  <summary>Details</summary>
Motivation: 手动标注异构驾驶数据成本高、耗时长，需要开发自动化工具来加速大规模标注数据集的创建，以支持波兰自动驾驶研究。

Method: 采用人机协同方法，结合AI自动生成初始标注，支持迭代模型重训练，集成数据匿名化和领域适应技术，核心基于3D目标检测算法。

Result: 开发的工具和方法显著节省了时间，确保了跨不同传感器模态的一致高质量标注，加速了DARTS项目中标准化格式的大规模标注数据集的准备。

Conclusion: 该半自动标注流水线有效解决了驾驶数据标注的效率和成本问题，为波兰自动驾驶研究提供了技术基础，支持DARTS项目的目标实现。

Abstract: This report presents the design and implementation of a semi-automated data annotation pipeline developed within the DARTS project, whose goal is to create a large-scale, multimodal dataset of driving scenarios recorded in Polish conditions. Manual annotation of such heterogeneous data is both costly and time-consuming. To address this challenge, the proposed solution adopts a human-in-the-loop approach that combines artificial intelligence with human expertise to reduce annotation cost and duration. The system automatically generates initial annotations, enables iterative model retraining, and incorporates data anonymization and domain adaptation techniques. At its core, the tool relies on 3D object detection algorithms to produce preliminary annotations. Overall, the developed tools and methodology result in substantial time savings while ensuring consistent, high-quality annotations across different sensor modalities. The solution directly supports the DARTS project by accelerating the preparation of large annotated dataset in the project's standardized format, strengthening the technological base for autonomous vehicle research in Poland.

</details>


### [36] [Iterative Deployment Improves Planning Skills in LLMs](https://arxiv.org/abs/2512.24940)
*Augusto B. Corrêa,Yoav Gelberg,Luckeciano C. Melo,Ilia Shumailov,André G. Pereira,Yarin Gal*

Main category: cs.AI

TL;DR: 迭代部署经过用户精心筛选数据微调的大型语言模型，可以显著改变模型特性，在规划任务中观察到能力提升和泛化能力涌现，理论分析表明这实现了隐式强化学习训练。


<details>
  <summary>Details</summary>
Motivation: 研究迭代部署机制对大型语言模型特性的影响，探索用户数据筛选在模型演化中的作用，以及这种机制与强化学习的理论联系。

Method: 在多个规划领域测试迭代部署机制：每个新模型都在用户从前一模型部署中精心筛选的数据上进行微调，然后部署并收集新数据，形成迭代循环。

Result: 观察到规划技能显著提升，后期模型展现出涌现的泛化能力，能够发现比初始模型长得多的规划方案。理论分析表明迭代部署实现了隐式奖励函数的强化学习训练。

Conclusion: 迭代部署机制可视为显式强化学习的替代训练方案，依赖数据筛选而非显式奖励，对AI安全领域有重要启示，因为隐含的奖励函数可能对未来模型部署产生意外影响。

Abstract: We show that iterative deployment of large language models (LLMs), each fine-tuned on data carefully curated by users from the previous models' deployment, can significantly change the properties of the resultant models. By testing this mechanism on various planning domains, we observe substantial improvements in planning skills, with later models displaying emergent generalization by discovering much longer plans than the initial models. We then provide theoretical analysis showing that iterative deployment effectively implements reinforcement learning (RL) training in the outer-loop (i.e. not as part of intentional model training), with an implicit reward function. The connection to RL has two important implications: first, for the field of AI safety, as the reward function entailed by repeated deployment is not defined explicitly, and could have unexpected implications to the properties of future model deployments. Second, the mechanism highlighted here can be viewed as an alternative training regime to explicit RL, relying on data curation rather than explicit rewards.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [37] [AgenticTCAD: A LLM-based Multi-Agent Framework for Automated TCAD Code Generation and Device Optimization](https://arxiv.org/abs/2512.23742)
*Guangxi Fan,Tianliang Ma,Xuguang Sun,Xun Wang,Kain Lu Low,Leilai Shao*

Main category: cs.SE

TL;DR: 提出AgenticTCAD框架，通过多智能体系统实现基于自然语言的端到端自动化器件设计与优化，显著提升2纳米纳米片FET设计效率


<details>
  <summary>Details</summary>
Motivation: 随着先进工艺节点持续微缩，设计-技术协同优化变得至关重要，但TCAD仿真领域缺乏开源资源阻碍了语言模型生成有效TCAD代码，限制了自动化设计能力

Method: 1) 构建专家策划的开源TCAD数据集并微调领域特定模型用于TCAD代码生成；2) 提出AgenticTCAD多智能体框架，实现自然语言驱动的端到端自动化器件设计与优化

Result: 在2纳米纳米片FET设计中，AgenticTCAD在4.2小时内达到IRDS-2024器件规格，而人类专家使用商业工具需要7.1天，效率提升约40倍

Conclusion: AgenticTCAD框架通过结合领域特定语言模型和多智能体系统，成功实现了高效、自动化的TCAD器件设计与优化，显著缩短了设计周期

Abstract: With the continued scaling of advanced technology nodes, the design-technology co-optimization (DTCO) paradigm has become increasingly critical, rendering efficient device design and optimization essential. In the domain of TCAD simulation, however, the scarcity of open-source resources hinders language models from generating valid TCAD code. To overcome this limitation, we construct an open-source TCAD dataset curated by experts and fine-tune a domain-specific model for TCAD code generation. Building on this foundation, we propose AgenticTCAD, a natural language - driven multi-agent framework that enables end-to-end automated device design and optimization. Validation on a 2 nm nanosheet FET (NS-FET) design shows that AgenticTCAD achieves the International Roadmap for Devices and Systems (IRDS)-2024 device specifications within 4.2 hours, whereas human experts required 7.1 days with commercial tools.

</details>


### [38] [DEFT: Differentiable Automatic Test Pattern Generation](https://arxiv.org/abs/2512.23746)
*Wei Li,Yan Zou,Yixin Liang,José Moura,Shawn Blanton*

Main category: cs.SE

TL;DR: DEFT是一种基于连续优化的新型ATPG方法，通过数学重参数化将离散ATPG问题转化为连续优化任务，使用梯度方法生成测试模式，显著提高了难检测故障的覆盖率。


<details>
  <summary>Details</summary>
Motivation: 现代集成电路复杂度导致测试模式数量激增，而大部分测试模式都针对少数难检测故障，需要新的ATPG算法来专门提高对这些难检测故障的测试效果。

Method: DEFT将离散ATPG问题重新表述为连续优化任务，引入数学基础的重参数化方法，使连续目标函数与离散故障检测语义对齐。采用定制CUDA内核进行高效前向-反向传播，并应用梯度归一化来缓解梯度消失问题，确保在深度电路图上的可扩展性和稳定性。

Result: 在两个工业基准测试中，与领先的商业工具相比，在相同模式预算和可比运行时间下，DEFT将难检测故障检测率分别提高了21.1%和48.9%。在部分赋值模式生成中，生成的模式0/1比特数减少19.3%，同时仍能检测到35%更多的故障。

Conclusion: DEFT是一种有前景且有效的ATPG引擎，为现有启发式方法提供了有价值的补充，通过连续优化方法显著提高了难检测故障的测试效果。

Abstract: Modern IC complexity drives test pattern growth, with the majority of patterns targeting a small set of hard-to-detect (HTD) faults. This motivates new ATPG algorithms to improve test effectiveness specifically for HTD faults. This paper presents DEFT (Differentiable Automatic Test Pattern Generation), a new ATPG approach that reformulates the discrete ATPG problem as a continuous optimization task. DEFT introduces a mathematically grounded reparameterization that aligns the expected continuous objective with discrete fault-detection semantics, enabling reliable gradient-based pattern generation. To ensure scalability and stability on deep circuit graphs, DEFT integrates a custom CUDA kernel for efficient forward-backward propagation and applies gradient normalization to mitigate vanishing gradients. Compared to a leading commercial tool on two industrial benchmarks, DEFT improves HTD fault detection by 21.1% and 48.9% on average under the same pattern budget and comparable runtime. DEFT also supports practical ATPG settings such as partial assignment pattern generation, producing patterns with 19.3% fewer 0/1 bits while still detecting 35% more faults. These results indicate DEFT is a promising and effective ATPG engine, offering a valuable complement to existing heuristic.

</details>


### [39] [State-of-the-art Small Language Coder Model: Mify-Coder](https://arxiv.org/abs/2512.23747)
*Abhinav Parmar,Abhisek Panigrahi,Abhishek Kumar Dwivedi,Abhishek Bhattacharya,Adarsh Ramachandra,Aditya Choudhary,Aditya Garg,Aditya Raj,Alankrit Bhatt,Alpesh Yadav,Anant Vishnu,Ananthu Pillai,Ankush Kumar,Aryan Patnaik,Aswatha Narayanan S,Avanish Raj Singh,Bhavya Shree Gadda,Brijesh Pankajbhai Kachhadiya,Buggala Jahnavi,Chidurala Nithin Krishna,Chintan Shah,Chunduru Akshaya,Debarshi Banerjee,Debrup Dey,Deepa R.,Deepika B G,Faiz ur Rahman,Gagan Gayari,Gudhi Jagadeesh Kumar Naidu,Gursimar Singh,Harshal Tyagi,Harshini K,James Mani Vathalloor,Jayarama Nettar,Jayashree Gajjam,Joe Walter Sugil George,Kamalakara Sri Krishna Tadepalli,Kamalkumar Rathinasamy,Karan Chaurasia,Karthikeyan S,Kashish Arora,Kaushal Desai,Khushboo Buwade,Kiran Manjrekar,Malikireddy Venkata Sai Likhitha,Manjunath A,Mitali Mahavir Bedmutha,Mohammed Rafee Tarafdar,Nikhil Tiwari,Nikitha K Gigi,Pavan Ravikumar,Pendyala Swarnanjali,Piyush Anand,Prakash Chandrasekar,Prasanna Bhalchandra Gawade,Prasanth Sivan,Preeti Khurana,Priyanshi Babbar,Rajab Ali Mondal,Rajesh Kumar Vissapragada,Rajeshwari Ganesan,Rajeswari Koppisetti,Ramjee R.,Ramkumar Thiruppathisamy,Rani G. S.,S Reka,Samarth Gupta,Sandeep Reddy Kothakota,Sarathy K,Sathyanarayana Sampath Kumar,Saurabh Kumar,Shashank Khasare,Shenbaga Devi Venkatesh Kumar,Shiva Rama Krishna Parvatham,Shoeb Shaikh,Shrishanmathi A,Shubham Pathak,Sree Samhita Koppaka,Sreenivasa Raghavan K S,Sreeram Venkatasubramanian,Suprabha Desai Bojja,Swetha R,Syed Ahmed,Chinmai Harshitha Thota,Tushar Yadav,Veeravelly Kusumitha,V V S S Prasanth Patnaik,Vidya Sri Sesetti,Vijayakeerthi K,Vikram Raj Bakshi,Vinay K K,Vinoth Kumar Loganathan,Vipin Tiwari,Vivek Kumar Shrivastav,V Venkata Sri Datta Charan,Wasim Akhtar Khan*

Main category: cs.SE

TL;DR: Mify-Coder是一个2.5B参数的代码模型，通过计算最优策略在4.2T tokens上训练，在代码生成和函数调用基准测试中显著优于更大的基线模型，证明紧凑模型可以达到前沿模型的性能水平。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过原则性的数据和计算策略，使较小的模型在代码生成和智能体驱动工作流中达到与前沿大模型相竞争的性能，同时保持高效部署能力。

Method: 基于Mify-2.5B基础模型，采用计算最优训练策略；结合高质量人工标注数据和通过智能体设计提示生成的合成数据；使用企业级评估数据集进行迭代优化；采用LLM质量过滤提高数据密度；探索CPT-SFT目标、数据混合和采样动态。

Result: Mify-Coder在标准编码和函数调用基准测试中显著优于更大的基线模型，达到可比的准确性和安全性；量化变体可以在标准桌面环境部署而无需专用硬件。

Conclusion: 通过原则性的数据和计算纪律，较小的模型可以实现具有竞争力的准确性、效率和安全性合规，为紧凑模型在代码智能领域的应用提供了实证支持。

Abstract: We present Mify-Coder, a 2.5B-parameter code model trained on 4.2T tokens using a compute-optimal strategy built on the Mify-2.5B foundation model. Mify-Coder achieves comparable accuracy and safety while significantly outperforming much larger baseline models on standard coding and function-calling benchmarks, demonstrating that compact models can match frontier-grade models in code generation and agent-driven workflows. Our training pipeline combines high-quality curated sources with synthetic data generated through agentically designed prompts, refined iteratively using enterprise-grade evaluation datasets. LLM-based quality filtering further enhances data density, enabling frugal yet effective training. Through disciplined exploration of CPT-SFT objectives, data mixtures, and sampling dynamics, we deliver frontier-grade code intelligence within a single continuous training trajectory. Empirical evidence shows that principled data and compute discipline allow smaller models to achieve competitive accuracy, efficiency, and safety compliance. Quantized variants of Mify-Coder enable deployment on standard desktop environments without requiring specialized hardware.

</details>


### [40] [A Systematic Mapping on Software Fairness: Focus, Trends and Industrial Context](https://arxiv.org/abs/2512.23782)
*Kessia Nepomuceno,Fabio Petrillo*

Main category: cs.SE

TL;DR: 该论文通过系统文献映射研究软件工程中的公平性解决方案，分析了95项研究，发现该领域正在扩展但主要关注方法和算法，工业应用潜力有限，需要在整个软件开发生命周期中整合公平性考量并加强产学合作。


<details>
  <summary>Details</summary>
Motivation: 随着软件工程领域的发展，系统公平性已成为关键问题。虽然已有一些指导原则提出，但对确保软件系统公平性的研究解决方案进行全面理解仍然具有挑战性。本研究旨在探索和分类软件工程中公平性解决方案的当前进展。

Method: 采用系统文献映射方法，开发了一个分类框架，从新的视角组织软件公平性研究。该框架应用于95项选定研究，分析其在工业环境中的采用潜力，重点关注三个维度：研究趋势、研究重点和工业可行性。

Result: 研究发现：软件公平性研究正在扩展，但主要集中于方法和算法；主要关注后处理和群体公平性，对早期干预、个体公平性指标和偏见根源理解较少；研究主要在学术界进行，工业合作有限，技术就绪水平低到中等，工业可转移性仍较遥远。

Conclusion: 需要在软件开发生命周期的所有阶段整合公平性考量，并加强学术界与工业界的合作。该分析为领域提供了全面概述，为未来研究和软件系统公平性的实际应用奠定了基础。

Abstract: Context: Fairness in systems has emerged as a critical concern in software engineering, garnering increasing attention as the field has advanced in recent years. While several guidelines have been proposed to address fairness, achieving a comprehensive understanding of research solutions for ensuring fairness in software systems remains challenging. Objectives: This paper presents a systematic literature mapping to explore and categorize current advancements in fairness solutions within software engineering, focusing on three key dimensions: research trends, research focus, and viability in industrial contexts. Methods: We develop a classification framework to organize research on software fairness from a fresh perspective, applying it to 95 selected studies and analyzing their potential for industrial adoption. Results: Our findings reveal that software fairness research is expanding, yet it remains heavily focused on methods and algorithms. It primarily focuses on post-processing and group fairness, with less emphasis on early-stage interventions, individual fairness metrics, and understanding bias root causes. Additionally fairness research remains largely academic, with limited industry collaboration and low to medium Technology Readiness Level (TRL), indicating that industrial transferability remains distant. Conclusion: Our results underscore the need to incorporate fairness considerations across all stages of the software development life-cycle and to foster greater collaboration between academia and industry. This analysis provides a comprehensive overview of the field, offering a foundation to guide future research and practical applications of fairness in software systems.

</details>


### [41] [From Illusion to Insight: Change-Aware File-Level Software Defect Prediction Using Agentic AI](https://arxiv.org/abs/2512.23875)
*Mohsen Hesamolhokama,Behnam Rohani,Amirahmad Shafiee,MohammadAmin Fazli,Jafar Habibi*

Main category: cs.SE

TL;DR: 传统软件缺陷预测存在准确率幻觉问题，因为标准评估方法奖励标签持续性偏差而非真正理解代码变化。本文提出将SDP重构为变化感知预测任务，并开发了基于LLM的多智能体辩论框架，显著提升了对缺陷引入的敏感性。


<details>
  <summary>Details</summary>
Motivation: 现有文件级软件缺陷预测研究存在根本性缺陷，大部分报告的准确率提升实际上是"准确率幻觉"。传统方法基于静态文件快照，而大多数文件在多个版本中持续存在并保持缺陷标签，导致标准评估方法奖励的是标签持续性偏差，而非真正理解代码变化对缺陷的影响。

Method: 1. 将SDP重构为变化感知预测任务，模型需要基于连续项目版本中文件的代码变化进行推理，而非依赖静态文件快照。2. 提出基于LLM的变化感知多智能体辩论框架，通过多智能体协作分析代码变化来预测缺陷。

Result: 在多个PROMISE项目上的实验表明：传统模型虽然获得虚高的F1分数，但在罕见但关键的缺陷转换案例上表现失败；而本文的变化感知推理和多智能体辩论框架在演化子集上表现更均衡，显著提高了对缺陷引入的敏感性。

Conclusion: 当前SDP评估实践存在根本性缺陷，强调了在实际缺陷预测中需要变化感知推理的重要性。变化感知方法能够更准确地识别由代码变化引起的缺陷，为软件质量保证提供更实用的预测能力。

Abstract: Much of the reported progress in file-level software defect prediction (SDP) is, in reality, nothing but an illusion of accuracy. Over the last decades, machine learning and deep learning models have reported increasing performance across software versions. However, since most files persist across releases and retain their defect labels, standard evaluation rewards label-persistence bias rather than reasoning about code changes. To address this issue, we reformulate SDP as a change-aware prediction task, in which models reason over code changes of a file within successive project versions, rather than relying on static file snapshots. Building on this formulation, we propose an LLM-driven, change-aware, multi-agent debate framework. Our experiments on multiple PROMISE projects show that traditional models achieve inflated F1, while failing on rare but critical defect-transition cases. In contrast, our change-aware reasoning and multi-agent debate framework yields more balanced performance across evolution subsets and significantly improves sensitivity to defect introductions. These results highlight fundamental flaws in current SDP evaluation practices and emphasize the need for change-aware reasoning in practical defect prediction. The source code is publicly available.

</details>


### [42] [Coding With AI: From a Reflection on Industrial Practices to Future Computer Science and Software Engineering Education](https://arxiv.org/abs/2512.23982)
*Hung-Fu Chang,MohammadShokrolah Shirazi,Lizhou Cao,Supannika Koolmanojwong Mobasser*

Main category: cs.SE

TL;DR: 该研究通过分析57个YouTube视频，探讨了LLM编码工具在专业实践中的使用、相关风险以及开发工作流程的转变，特别关注对计算机教育的影响。


<details>
  <summary>Details</summary>
Motivation: 先前研究主要关注个人层面或教育环境中的AI编码，缺乏对工业从业者视角的深入探索。本研究旨在填补这一空白，了解LLM编码工具在专业实践中的实际应用、相关担忧和风险，以及开发工作流程的转变。

Method: 对2024年末至2025年间发布的57个精选YouTube视频进行定性分析，这些视频记录了从业者的反思和经验。经过筛选和质量评估过程，分析比较了基于LLM的编程与传统编程，识别了新兴风险，并描述了不断演变的工作流程。

Result: 研究发现：1）定义了基于AI的编码实践；2）显著的生产力提升和降低入门门槛；3）开发瓶颈转向代码审查；4）从业者担忧代码质量、可维护性、安全漏洞、伦理问题、基础问题解决技能的侵蚀以及初级工程师准备不足。

Conclusion: 基于这些发现，讨论了计算机科学和软件工程教育的启示，主张课程向问题解决、架构思维、代码审查以及早期整合LLM工具的项目式学习转变。该研究提供了基于行业的AI编码视角，并为教育实践与快速演变的专业现实对齐提供了指导。

Abstract: Recent advances in large language models (LLMs) have introduced new paradigms in software development, including vibe coding, AI-assisted coding, and agentic coding, fundamentally reshaping how software is designed, implemented, and maintained. Prior research has primarily examined AI-based coding at the individual level or in educational settings, leaving industrial practitioners' perspectives underexplored. This paper addresses this gap by investigating how LLM coding tools are used in professional practice, the associated concerns and risks, and the resulting transformations in development workflows, with particular attention to implications for computing education. We conducted a qualitative analysis of 57 curated YouTube videos published between late 2024 and 2025, capturing reflections and experiences shared by practitioners. Following a filtering and quality assessment process, the selected sources were analyzed to compare LLM-based and traditional programming, identify emerging risks, and characterize evolving workflows. Our findings reveal definitions of AI-based coding practices, notable productivity gains, and lowered barriers to entry. Practitioners also report a shift in development bottlenecks toward code review and concerns regarding code quality, maintainability, security vulnerabilities, ethical issues, erosion of foundational problem-solving skills, and insufficient preparation of entry-level engineers. Building on these insights, we discuss implications for computer science and software engineering education and argue for curricular shifts toward problem-solving, architectural thinking, code review, and early project-based learning that integrates LLM tools. This study offers an industry-grounded perspective on AI-based coding and provides guidance for aligning educational practices with rapidly evolving professional realities.

</details>


### [43] [Developing controlled natural language for formal specification patterns using AI assistants](https://arxiv.org/abs/2512.24159)
*Natalia Garanina,Vladimir Zyubin,Igor Anureev*

Main category: cs.SE

TL;DR: 使用AI助手开发了一种基于包含逻辑属性的形式化规约模式来系统构建受控自然语言需求的方法，该方法包含三个阶段：编译通用自然语言需求模式、使用AI生成语料库、以及基于语法结构分析形式化受控自然语言的语法。


<details>
  <summary>Details</summary>
Motivation: 为了解决需求工程中自然语言需求与形式化规约之间的鸿沟，开发一种系统化的方法来构建受控自然语言，使其既能保持自然语言的易用性，又能与形式化规约模式保持精确对应。

Method: 该方法包含三个阶段：1) 编译利用形式化规约模板所有属性的通用自然语言需求模式；2) 使用AI助手生成自然语言需求模式语料库，通过部分评估属性进行简化；3) 基于结果模式的语法结构分析，形式化受控自然语言的语法。

Result: 该方法已针对事件驱动的时间需求进行了测试，能够系统化地构建与形式化规约模式对应的受控自然语言，提高了需求表达的一致性和精确性。

Conclusion: 通过结合AI助手和形式化规约模式，成功开发了一种系统化构建受控自然语言需求的方法，为需求工程中自然语言与形式化规约的衔接提供了有效解决方案。

Abstract: Using an AI assistant, we developed a method for systematically constructing controlled natural language for requirements based on formal specification patterns containing logical attributes. The method involves three stages: 1) compiling a generalized natural language requirement pattern that utilizes all attributes of the formal specification template; 2) generating, using the AI assistant, a corpus of natural language requirement patterns, reduced by partially evaluating attributes (the developed prompt utilizes the generalized template, attribute definitions, and specific formal semantics of the requirement patterns); and 3) formalizing the syntax of the controlled natural language based on an analysis of the grammatical structure of the resulting patterns. The method has been tested for event-driven temporal requirements.

</details>


### [44] ["Game Changer" or "Overenthusiastic Drunk Acquaintance"? Generative AI Use by Blind and Low Vision Software Professionals in the Workplace](https://arxiv.org/abs/2512.24462)
*Yoonha Cha,Victoria Jackson,Lauren Shu,Stacy Branham,André van der Hoek*

Main category: cs.SE

TL;DR: 研究通过39位视障软件专业人士的访谈，探讨了生成式AI对他们软件开发工作的影响，发现既有生产力提升和可访问性改善等益处，也存在幻觉风险更高、组织政策限制等成本。


<details>
  <summary>Details</summary>
Motivation: 软件开发工作场所对视障软件专业人士存在技术和协作上的可访问性挑战。虽然生成式AI在软件开发行业日益普及并成为研究热点，但迄今为止尚未咨询过视障软件专业人士的独特视角。

Method: 采用定性研究方法，对39位视障软件专业人士进行了半结构化访谈，探讨生成式AI引入对他们工作的意义。

Result: 研究发现视障软件专业人士将生成式AI用于多种软件开发任务，带来了生产力提升和可访问性改善等益处。但使用生成式AI也伴随着显著成本，他们比视力正常的同事更容易受到幻觉影响，有时组织政策也限制了使用。

Conclusion: 基于研究发现，视障软件专业人士在决定是否以及何时在工作中使用生成式AI工具时，需要仔细权衡更高的风险和更高的回报。

Abstract: The software development workplace poses numerous technical and collaborative accessibility challenges for blind and low vision software professionals (BLVSPs). Though Generative AI (GenAI) is increasingly adopted within the software development industry and has been a rapidly growing topic of interest in research, to date, the unique perspectives of BLVSPs have yet to be consulted. We report on a qualitative study involving 39 semi-structured interviews with BLVSPs about what the introduction of GenAI has meant for their work. We found that BLVSPs used GenAI for many software development tasks, resulting in benefits such as increased productivity and accessibility. However, significant costs were also accompanied by GenAI use as they were more vulnerable to hallucinations than their sighted colleagues. Sometimes, organizational policies prevented use. Based on our findings, we discuss the higher-risks and higher-returns that BLVSPs had to carefully weigh when deciding whether and when to use GenAI tools for work.

</details>


### [45] [A Magnified View into Heterogeneous-ISA Thread Migration Performance without State Transformation](https://arxiv.org/abs/2512.24530)
*Nikolaos Mavrogeorgis,Christos Vasiladiotis,Pei Mu,Amir Khordadi,Björn Franke,Antonio Barbalace*

Main category: cs.SE

TL;DR: Unifico是一个新的多ISA编译器，通过保持跨架构的相同栈布局，消除了异构ISA处理器中运行时栈转换的开销，将二进制大小开销从约200%降低到约10%。


<details>
  <summary>Details</summary>
Motivation: 异构ISA处理器设计需要软件支持来桥接ISA异构性，但缺乏支持异构ISA目标的编译工具链阻碍了该领域的研究。运行时栈转换成本过高，需要一种更高效的解决方案。

Method: 设计开发Unifico编译器，使用LLVM基础设施实现，生成在x86-64和ARMv8架构上执行时保持相同栈布局的二进制文件，避免运行时栈转换，维护统一的ABI和虚拟地址空间。

Result: 在NAS基准测试中，Unifico对整体执行时间影响最小，高端处理器平均引入小于6%的开销，低端处理器小于10%。相比最先进的Popcorn编译器，二进制大小开销从约200%降低到约10%，同时消除了ISA迁移时的栈转换开销。

Conclusion: Unifico成功解决了异构ISA处理器中的栈转换问题，显著降低了开销，其关键设计特性可以进一步优化以减轻性能影响，为异构ISA系统研究提供了实用的编译工具链支持。

Abstract: Heterogeneous-ISA processor designs have attracted considerable research interest. However, unlike their homogeneous-ISA counterparts, explicit software support for bridging ISA heterogeneity is required. The lack of a compilation toolchain ready to support heterogeneous-ISA targets has been a major factor hindering research in this exciting emerging area. For any such compiler, "getting right" the mechanics involved in state transformation upon migration and doing this efficiently is of critical importance. In particular, any runtime conversion of the current program stack from one architecture to another would be prohibitively expensive. In this paper, we design and develop Unifico, a new multi-ISA compiler that generates binaries that maintain the same stack layout during their execution on either architecture. Unifico avoids the need for runtime stack transformation, thus eliminating overheads associated with ISA migration. Additional responsibilities of the Unifico compiler backend include maintenance of a uniform ABI and virtual address space across ISAs. Unifico is implemented using the LLVM compiler infrastructure, and we are currently targeting the x86-64 and ARMv8 ISAs. We have evaluated Unifico across a range of compute-intensive NAS benchmarks and show its minimal impact on overall execution time, where less than 6% (10%) overhead is introduced on average for high-end (low-end) processors. We also analyze the performance impact of Unifico's key design features and demonstrate that they can be further optimized to mitigate this impact. When compared against the state-of-the-art Popcorn compiler, Unifico reduces binary size overhead from ~200% to ~10%, whilst eliminating the stack transformation overhead during ISA migration.

</details>


### [46] [Localized Calibrated Uncertainty in Code Language Models](https://arxiv.org/abs/2512.24560)
*David Gros,Prem Devanbu*

Main category: cs.SE

TL;DR: 该研究提出了一种定位LLM生成代码中与用户意图不符部分的技术，通过创建"最小意图对齐补丁"数据集，并比较多种方法评估代码行被编辑的概率，发现小型监督模型探针能有效预测大型模型生成代码中的错误位置。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然能从自然语言提示生成复杂源代码，但其输出可能偏离用户意图，需要监督和编辑。为了支持这一过程，需要开发技术来定位生成代码中可能与用户意图不符的部分。

Method: 首先创建"最小意图对齐补丁"数据集，包含修复后的LLM生成程序，每个程序使用测试用例验证正确性。然后比较多种技术评估代码行被编辑的概率：1)白盒探针（提出高效任意跨度查询技术）；2)黑盒反射方法；3)自一致性方法。使用小型监督模型训练探针来预测编辑位置。

Result: 研究发现，使用小型监督模型训练的探针能够实现较低的校准误差，Brier技能得分约为0.2，能够有效估计由大几个数量级的模型生成代码中的编辑行。探针仅基于代码训练，但在允许新的概率缩放时，显示出对自然语言错误的泛化迹象。

Conclusion: 该技术为AI监督和控制提供了新方法，小型监督模型能够有效定位大型语言模型生成代码中的错误，并且显示出一定的跨领域泛化能力，对提高LLM生成代码的可靠性和可编辑性具有重要意义。

Abstract: Large Language models (LLMs) can generate complicated source code from natural language prompts. However, LLMs can generate output that deviates from what the user wants, requiring supervision and editing. To support this process, we offer techniques to localize where generations might be misaligned from user intent. We first create a dataset of "Minimal Intent Aligning Patches" of repaired LLM generated programs. Each program uses test cases to verify correctness. After creating a dataset of programs, we measure how well various techniques can assign a well-calibrated probability to indicate which parts of code will be edited in a minimal patch (i.e., give a probability that corresponds with empirical odds it is edited). We compare white-box probing (where we propose a technique for efficient arbitrary-span querying), against black-box reflective and self-consistency based approaches. We find probes with a small supervisor model can achieve low calibration error and Brier Skill Score of approx 0.2 estimating edited lines on code generated by models many orders of magnitude larger. We discuss the generalizability of the techniques, and the connections to AI oversight and control, finding a probe trained only on code shows some signs of generalizing to natural language errors if new probability scaling is allowed.

</details>


### [47] [On the Effectiveness of Training Data Optimization for LLM-based Code Generation: An Empirical Study](https://arxiv.org/abs/2512.24570)
*Shiqi Kuang,Zhao Tian,Tao Xiao,Dong Wang,Junjie Chen*

Main category: cs.SE

TL;DR: 本文首次对五种广泛使用的训练数据优化技术及其组合在LLM代码生成中的效果进行了大规模实证研究，发现数据合成在功能正确性和减少代码异味方面最有效，而数据合成与数据重构的组合表现最佳。


<details>
  <summary>Details</summary>
Motivation: 尽管已有许多训练数据优化技术被提出以提升代码生成质量，但这些技术的整体有效性尚未得到系统评估。为了填补这一空白，本文旨在通过大规模实证研究来系统评估不同训练数据优化技术及其组合对LLM代码生成的影响。

Method: 研究对五种广泛使用的训练数据优化技术及其两两组合进行了大规模实证评估，覆盖三个基准测试和四个大型语言模型，通过细粒度分析深入探讨了各项技术及其组合如何影响代码生成效果。

Result: 数据合成在提升功能正确性和减少代码异味方面最有效，但在代码可维护性方面相对较差；大多数组合技术不能进一步改善功能正确性，但能有效提升代码质量；数据合成与数据重构的组合在所有组合中表现最强。

Conclusion: 本研究是理解训练数据优化和组合策略的第一步，为LLM代码生成的未来研究和部署提供了实用指导，强调了数据合成与数据重构组合的有效性。

Abstract: Large language models (LLMs) have achieved remarkable progress in code generation, largely driven by the availability of high-quality code datasets for effective training. To further improve data quality, numerous training data optimization techniques have been proposed; however, their overall effectiveness has not been systematically evaluated. To bridge this gap, we conduct the first large-scale empirical study, examining five widely-used training data optimization techniques and their pairwise combinations for LLM-based code generation across three benchmarks and four LLMs. Our results show that data synthesis is the most effective technique for improving functional correctness and reducing code smells, although it performs relatively worse on code maintainability compared to data refactoring, cleaning, and selection. Regarding combinations, we find that most combinations do not further improve functional correctness but can effectively enhance code quality (code smells and maintainability). Among all combinations, data synthesis combined with data refactoring achieves the strongest overall performance. Furthermore, our fine-grained analysis reinforces these findings and provides deeper insights into how individual techniques and their combinations influence code generation effectiveness. Overall, this work represents a first step toward a systematic understanding of training data optimization and combination strategies, offering practical guidance for future research and deployment in LLM-based code generation.

</details>


### [48] [A Tale of 1001 LoC: Potential Runtime Error-Guided Specification Synthesis for Verifying Large-Scale Programs](https://arxiv.org/abs/2512.24594)
*Zhongyi Wang,Tengjie Lin,Mingshuai Chen,Haokun Li,Mingqi Yang,Xiao Yi,Shengchao Qin,Yixing Luo,Xiaofeng Li,Bin Gu,Liqiang Lu,Jianwei Yin*

Main category: cs.SE

TL;DR: Preguss是一个模块化、细粒度的框架，通过结合静态分析和演绎验证，自动化生成和精化形式化规约，显著提升了大规模软件硬件系统验证的自动化程度。


<details>
  <summary>Details</summary>
Motivation: 大规模软件硬件系统的全自动化验证是形式化方法的终极目标。虽然大语言模型在提升形式化验证自动化方面显示出潜力，但由于长上下文推理限制和推断复杂过程间规约的困难，其可扩展性较差。

Method: Preguss采用分而治之策略，结合静态分析和演绎验证：1) 基于潜在运行时错误构建和优先化验证单元；2) 在单元级别使用LLM辅助合成过程间规约。

Result: Preguss显著优于最先进的基于LLM的方法，能够对超过一千行代码的真实世界程序进行高度自动化的RTE无错误验证，将人工验证工作量减少了80.6%~88.9%。

Conclusion: Preguss框架通过模块化、细粒度的方法有效解决了LLM在形式化验证中的可扩展性问题，为实现大规模系统全自动化验证提供了有前景的解决方案。

Abstract: Fully automated verification of large-scale software and hardware systems is arguably the holy grail of formal methods. Large language models (LLMs) have recently demonstrated their potential for enhancing the degree of automation in formal verification by, e.g., generating formal specifications as essential to deductive verification, yet exhibit poor scalability due to long-context reasoning limitations and, more importantly, the difficulty of inferring complex, interprocedural specifications. This paper presents Preguss -- a modular, fine-grained framework for automating the generation and refinement of formal specifications. Preguss synergizes between static analysis and deductive verification by steering two components in a divide-and-conquer fashion: (i) potential runtime error-guided construction and prioritization of verification units, and (ii) LLM-aided synthesis of interprocedural specifications at the unit level. We show that Preguss substantially outperforms state-of-the-art LLM-based approaches and, in particular, it enables highly automated RTE-freeness verification for real-world programs with over a thousand LoC, with a reduction of 80.6%~88.9% human verification effort.

</details>


### [49] [DynaFix: Iterative Automated Program Repair Driven by Execution-Level Dynamic Information](https://arxiv.org/abs/2512.24635)
*Zhili Huang,Ling Xu,Chao Liu,Weifeng Sun,Xu Zhang,Yan Lei,Meng Yan,Hongyu Zhang*

Main category: cs.SE

TL;DR: DynaFix：一种基于执行级动态信息的自动程序修复方法，通过迭代利用运行时信息（变量状态、控制流路径、调用栈）来指导大语言模型生成补丁，显著提升了复杂bug修复效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的自动程序修复方法主要依赖静态分析，忽略了运行时行为；虽然有些方法尝试融入动态信号，但通常仅限于训练或微调阶段，或只在修复提示中注入一次，无法迭代利用执行级信息。现有迭代修复框架通常依赖粗粒度反馈（如通过/失败结果或异常类型），未能有效利用细粒度执行级信息，限制了模型模拟人类逐步调试的能力。

Method: DynaFix是一种执行级动态信息驱动的APR方法，在每轮修复中捕获变量状态、控制流路径和调用栈等执行级动态信息，将其转换为结构化提示来指导LLMs生成候选补丁。如果补丁验证失败，重新执行修改后的程序收集新的执行信息用于下一次尝试，形成类似人类开发者逐步调试的迭代循环。

Result: 在Defects4J v1.2和v2.0基准测试中，DynaFix修复了186个单函数bug，相比最先进的基线方法提升了10%，其中包括38个之前未被修复的bug。最多在35次尝试内获得正确补丁，相比现有方法将补丁搜索空间减少了70%。

Conclusion: DynaFix通过迭代利用执行级动态信息，有效模拟了人类逐步调试过程，显著提升了自动程序修复在复杂bug修复中的效果和效率，证明了动态信息在指导LLMs进行多步推理和复杂bug修复中的重要性。

Abstract: Automated Program Repair (APR) aims to automatically generate correct patches for buggy programs. Recent approaches leveraging large language models (LLMs) have shown promise but face limitations. Most rely solely on static analysis, ignoring runtime behaviors. Some attempt to incorporate dynamic signals, but these are often restricted to training or fine-tuning, or injected only once into the repair prompt, without iterative use. This fails to fully capture program execution. Current iterative repair frameworks typically rely on coarse-grained feedback, such as pass/fail results or exception types, and do not leverage fine-grained execution-level information effectively. As a result, models struggle to simulate human stepwise debugging, limiting their effectiveness in multi-step reasoning and complex bug repair.
  To address these challenges, we propose DynaFix, an execution-level dynamic information-driven APR method that iteratively leverages runtime information to refine the repair process. In each repair round, DynaFix captures execution-level dynamic information such as variable states, control-flow paths, and call stacks, transforming them into structured prompts to guide LLMs in generating candidate patches. If a patch fails validation, DynaFix re-executes the modified program to collect new execution information for the next attempt. This iterative loop incrementally improves patches based on updated feedback, similar to the stepwise debugging practices of human developers. We evaluate DynaFix on the Defects4J v1.2 and v2.0 benchmarks. DynaFix repairs 186 single-function bugs, a 10% improvement over state-of-the-art baselines, including 38 bugs previously unrepaired. It achieves correct patches within at most 35 attempts, reducing the patch search space by 70% compared with existing methods, thereby demonstrating both effectiveness and efficiency in repairing complex bugs.

</details>


### [50] [Characterizing Bugs and Quality Attributes in Quantum Software: A Large-Scale Empirical Study](https://arxiv.org/abs/2512.24656)
*Mir Mohammad Yousuf,Shabir Ahmad Sofi*

Main category: cs.SE

TL;DR: 对123个开源量子项目（2012-2024年）的32,296个已验证bug报告进行首次生态系统规模纵向分析，发现全栈库和编译器最易出错，量子特定bug主要影响性能、可维护性和可靠性，自动化测试可减少约60%的缺陷发生率。


<details>
  <summary>Details</summary>
Motivation: 量子软件工程对于确保混合量子-经典系统的可靠性和可维护性至关重要，但关于真实量子项目中bug如何出现及影响质量的实证证据仍然有限。本研究旨在填补这一空白，提供大规模数据驱动的量子软件缺陷特征分析。

Method: 采用混合方法：结合仓库挖掘、静态代码分析、问题元数据提取和经过验证的基于规则的分类框架，分析123个开源量子仓库（涵盖8个功能类别）从2012年到2024年的32,296个已验证bug报告。

Result: 全栈库和编译器是最易出错的类别（主要涉及电路、门和转译相关问题），模拟器主要受测量和噪声建模错误影响。经典bug主要影响可用性和互操作性，而量子特定bug不成比例地降低性能、可维护性和可靠性。缺陷密度在2017-2021年达到峰值后下降。高严重性缺陷集中在密码学、实验计算和编译器工具链中。采用自动化测试的仓库能检测更多缺陷并更快解决问题。

Conclusion: 这是首次大规模数据驱动的量子软件缺陷特征分析，为改进量子软件工程中的测试、文档和可维护性实践提供了实证指导。研究表明自动化测试与约60%的预期缺陷发生率减少相关，生态系统正在成熟。

Abstract: Quantum Software Engineering (QSE) is essential for ensuring the reliability and maintainability of hybrid quantum-classical systems, yet empirical evidence on how bugs emerge and affect quality in real-world quantum projects remains limited. This study presents the first ecosystem-scale longitudinal analysis of software defects across 123 open source quantum repositories from 2012 to 2024, spanning eight functional categories, including full-stack libraries, simulators, annealing, algorithms, compilers, assembly, cryptography, and experimental computing. Using a mixed method approach combining repository mining, static code analysis, issue metadata extraction, and a validated rule-based classification framework, we analyze 32,296 verified bug reports. Results show that full-stack libraries and compilers are the most defect-prone categories due to circuit, gate, and transpilation-related issues, while simulators are mainly affected by measurement and noise modeling errors. Classical bugs primarily impact usability and interoperability, whereas quantum-specific bugs disproportionately degrade performance, maintainability, and reliability. Longitudinal analysis indicates ecosystem maturation, with defect densities peaking between 2017 and 2021 and declining thereafter. High-severity defects cluster in cryptography, experimental computing, and compiler toolchains. Repositories employing automated testing detect more defects and resolve issues faster. A negative binomial regression further shows that automated testing is associated with an approximate 60 percent reduction in expected defect incidence. Overall, this work provides the first large-scale data-driven characterization of quantum software defects and offers empirical guidance for improving testing, documentation, and maintainability practices in QSE.

</details>


### [51] [Feature Slice Matching for Precise Bug Detection](https://arxiv.org/abs/2512.24858)
*Ke Ma,Jianjun Huang,Wei You,Bin Liang,Jingzheng Wu,Yanjun Wu,Yuanjun Gong*

Main category: cs.SE

TL;DR: MATUS通过抑制目标代码中的噪声干扰，基于相似性度量实现精确的bug检测，在Linux内核中发现了31个未知bug


<details>
  <summary>Details</summary>
Motivation: 现有基于函数相似性检测bug的方法受无关语句的噪声干扰影响性能，而现有工作未能有效抑制目标代码中的噪声

Method: 从bug查询和目标代码中提取特征切片表示语义特征，利用bug代码的先验知识指导目标切片，以端到端方式确定切片标准，通过向量相似性比较特征切片

Result: MATUS在真实项目中展现出bug检测优势，在Linux内核中发现了31个未知bug，全部得到内核开发者确认，其中11个被分配了CVE编号

Conclusion: MATUS通过抑制目标噪声实现了精确的bug检测，证明了其在真实项目中的有效性和实用性

Abstract: Measuring the function similarity to detect bugs is effective, but the statements unrelated to the bugs can impede the performance due to the noise interference. Suppressing the noise interference in existing works does not manage the tough job, i.e., eliminating the noise in the targets. In this paper, we propose MATUS to mitigate the target noise for precise bug detection based on similarity measurement. Feature slices are extracted from both the buggy query and the targets to represent the semantic feature of (potential) bug logics. In particular, MATUS guides the target slicing with the prior knowledge from the buggy code, in an end-to-end way to pinpoint the slicing criterion in the targets. All feature slices are embedded and compared based on the vector similarity. Buggy candidates are audited to confirm unknown bugs in the targets. Experiments show that MATUS holds advantages in bug detection for real-world projects with acceptable efficiency. In total, MATUS has spotted 31 unknown bugs in the Linux kernel. All of them have been confirmed by the kernel developers, and 11 have been assigned CVEs.

</details>


### [52] [Securing High-Concurrency Ticket Sales: A Framework Based on Microservice](https://arxiv.org/abs/2512.24941)
*Zhiyong Zhang,Xiaoyan Zhang,Xiaoqi Li*

Main category: cs.SE

TL;DR: 本文设计了一个基于微服务架构的铁路售票系统，采用B/S架构和Spring Cloud技术，旨在解决高并发场景下的系统稳定性问题，实现了实时查询、动态座位更新、在线选座购票等功能。


<details>
  <summary>Details</summary>
Motivation: 传统铁路售票系统在节假日等高并发场景下存在故障容忍度不足、性能低下等问题，无法满足峰值用户需求，需要采用微服务架构来提升系统的稳定性和数据一致性。

Method: 采用B/S架构和Spring Cloud微服务框架进行系统开发，集成多种中间件组件，设计了包括实时列车查询、动态座位更新、在线选座购票、添加乘客等功能的完整在线售票流程，并实施了多种安全设计方法。

Result: 系统开发完成后进行了核心接口测试，测试数据证明系统在高并发场景下具有良好的性能和稳定性，能够快速响应用户请求。

Conclusion: 通过微服务架构和多种安全设计方法，成功开发了一个在高并发场景下保持稳定可靠的铁路售票系统，有效解决了传统线下购票排队时间长、信息更新延迟等问题。

Abstract: The railway ticketing system is one of the most important public service infrastructure. In peak periods such as holidays, it is often faced with the challenge of high concurrency scenarios because of a large number of users accessing at the same time. The traditional aggregation architecture can not meet the peak user requirements because of its insufficient fault tolerance and low ability. Therefore, the system needs to use microservice architecture for development, and add multiple security methods to ensure that the system can have good stability and data consistency under high concurrency scenarios, and can respond quickly to user requests. This paper introduces the use of B/S architecture and Spring Cloud to design and develop a railway ticket purchase system that can maintain stability and reliability under high concurrency scenarios, and formulate multiple security design methods for the system. This system integrates a range of functions, such as real-time train inquiries, dynamic seat updates, online seat selection, and ticket purchasing, effectively addressing common problems associated with offline ticket purchasing, such as long queues and delayed information. It enables a complete online process from inquiry and booking to payment and refunds. Furthermore, the "add passenger" function allows users to purchase tickets for others, extending the convenience of online ticketing to people with limited internet access. The system design prioritizes security and stability, while also focusing on high performance, and achieves these goals through a carefully designed architecture and the integration of multiple middleware components. After the completion of the system development, the core interface of the system is tested, and then the results are analyzed. The test data proves that the system has good ability and stability under high concurrency.

</details>
