<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 7]
- [cs.AI](#cs.AI) [Total: 28]
- [cs.CR](#cs.CR) [Total: 14]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Towards autonomous normative multi-agent systems for Human-AI software engineering teams](https://arxiv.org/abs/2512.02329)
*Hoa Khanh Dam,Geeta Mahala,Rashina Hoda,Xi Zheng,Cristina Conati*

Main category: cs.SE

TL;DR: 论文提出了一种由AI驱动的软件工程新范式，其中基于大语言模型的自主智能体成为软件开发的核心驱动力，通过与人类和其他智能体协作，实现远超当前流程的开发速度、可靠性和适应性。


<details>
  <summary>Details</summary>
Motivation: 当前软件工程流程存在效率瓶颈，需要更快速、可靠和自适应的开发方法。论文旨在通过AI智能体实现软件开发的根本性变革，让AI成为软件开发活动的主要驱动力。

Method: 引入基于大语言模型的新型软件工程智能体，赋予其信念、欲望、意图和记忆以实现类人推理能力。采用道义模态（承诺、义务、禁止和许可）作为规范来协调智能体间的协作与交互，确保合规性。

Result: 建立了一个可扩展、透明且可信赖的框架，支持未来人-AI软件工程团队的协作，使软件系统的设计、实现、测试和部署达到远超当前流程的速度、可靠性和适应性水平。

Conclusion: AI驱动的自主智能体代表了软件工程的变革性范式，通过规范化的协作框架，能够显著提升软件开发的效率和质量，为未来人-AI协作的软件工程团队奠定了理论基础。

Abstract: This paper envisions a transformative paradigm in software engineering, where Artificial Intelligence, embodied in fully autonomous agents, becomes the primary driver of the core software development activities. We introduce a new class of software engineering agents, empowered by Large Language Models and equipped with beliefs, desires, intentions, and memory to enable human-like reasoning. These agents collaborate with humans and other agents to design, implement, test, and deploy software systems with a level of speed, reliability, and adaptability far beyond the current software development processes. Their coordination and collaboration are governed by norms expressed as deontic modalities - commitments, obligations, prohibitions and permissions - that regulate interactions and ensure regulatory compliance. These innovations establish a scalable, transparent and trustworthy framework for future Human-AI software engineering teams.

</details>


### [2] [Process-Centric Analysis of Agentic Software Systems](https://arxiv.org/abs/2512.02393)
*Shuyang Liu,Yang Chen,Rahul Krishna,Saurabh Sinha,Jatin Ganhotra,Reyhan Jabbarvand*

Main category: cs.SE

TL;DR: 本文提出Graphectory框架，用于系统编码智能体软件系统的时空语义关系，支持过程中心化度量和分析，超越传统结果中心化评估。


<details>
  <summary>Details</summary>
Motivation: 当前智能体系统评估过于结果中心化，仅关注最终成功或失败，忽视了智能体推理、规划、行动和策略演变的详细过程洞察。需要一种结构化方法来分析智能体工作流的执行过程质量。

Method: 提出Graphectory框架，将智能体系统的执行轨迹编码为图结构，捕捉时间和语义关系。基于此框架设计过程中心化度量和分析方法，应用于SWE-agent和OpenHands两种主流智能体编程工作流，分析4000条轨迹数据。

Result: 分析发现：(1) 使用更丰富提示或更强LLM的智能体表现出更复杂的Graphectory，反映更深探索、更广上下文收集和更彻底验证；(2) 问题解决策略随问题难度和底层LLM变化，已解决问题呈现连贯的定位-修复-验证步骤，未解决问题表现出混乱、重复或回溯行为；(3) 即使成功，智能体编程系统常显示低效过程，导致不必要延长的轨迹。

Conclusion: Graphectory框架为智能体系统提供了超越结果评估的过程分析能力，揭示了智能体工作流的质量特征和效率问题，为改进智能体系统设计和评估提供了新视角。

Abstract: Agentic systems are modern software systems: they consist of orchestrated modules, expose interfaces, and are deployed in software pipelines. Unlike conventional programs, their execution (i.e., trajectories) is inherently stochastic and adaptive to the problem they are solving. Evaluation of such systems is often outcome-centric, judging their performance based on success or failure at the final step. This narrow focus overlooks detailed insights about such systems, failing to explain how agents reason, plan, act, or change their strategies over time. Inspired by the structured representation of conventional software systems as graphs, we introduce Graphectory to systematically encode the temporal and semantic relations in such software systems. Graphectory facilitates the design of process-centric metrics and analyses to assess the quality of agentic workflows independent of final success.
  Using Graphectory, we analyze 4000 trajectories of two dominant agentic programming workflows, namely SWE-agent and OpenHands, with a combination of four backbone Large Language Models (LLMs), attempting to resolve SWE-bench Verified issues. Our fully automated analyses reveal that: (1) agents using richer prompts or stronger LLMs exhibit more complex Graphectory, reflecting deeper exploration, broader context gathering, and more thorough validation before patch submission; (2) agents' problem-solving strategies vary with both problem difficulty and the underlying LLM -- for resolved issues, the strategies often follow coherent localization-patching-validation steps, while unresolved ones exhibit chaotic, repetitive, or backtracking behaviors; (3) even when successful, agentic programming systems often display inefficient processes, leading to unnecessarily prolonged trajectories.

</details>


### [3] [Feedback Loops and Code Perturbations in LLM-based Software Engineering: A Case Study on a C-to-Rust Translation System](https://arxiv.org/abs/2512.02567)
*Martin Weiss,Jesko Hecking-Harbusch,Jochen Quante,Matthias Woehrle*

Main category: cs.SE

TL;DR: 研究自动化C到Rust代码翻译系统中三个关键因素对质量的影响：自动反馈循环、大语言模型选择和行为保持代码变化，发现反馈循环能显著减少不同模型间的性能差异。


<details>
  <summary>Details</summary>
Motivation: 虽然生成式AI在软件工程任务中应用广泛，但自动化方法需要更高可靠性才能用于工业实践。研究旨在探索影响代码翻译质量的三个关键因素，为工业应用提供指导。

Method: 基于生成-检查模式构建C到Rust翻译系统，LLM生成Rust代码后自动检查可编译性和行为等价性。对负面检查结果使用反馈循环重新提示LLM修复输出。通过改变三个变量（反馈循环、LLM选择、代码扰动）评估系统性能。

Result: 无反馈循环时，LLM选择对翻译成功率影响很大；使用反馈循环后，不同模型间的差异显著减小。代码扰动提供的多样性甚至能提升系统性能，反馈循环也提高了系统在代码扰动下的鲁棒性。

Conclusion: 自动反馈循环是提升代码翻译系统可靠性的关键，能减少不同LLM间的性能差异并提高鲁棒性。代码扰动带来的多样性有益于系统性能，为工业级自动化代码翻译系统设计提供了重要见解。

Abstract: The advent of strong generative AI has a considerable impact on various software engineering tasks such as code repair, test generation, or language translation. While tools like GitHub Copilot are already in widespread use in interactive settings, automated approaches require a higher level of reliability before being usable in industrial practice. In this paper, we focus on three aspects that directly influence the quality of the results: a) the effect of automated feedback loops, b) the choice of Large Language Model (LLM), and c) the influence of behavior-preserving code changes.
  We study the effect of these three variables on an automated C-to-Rust translation system. Code translation from C to Rust is an attractive use case in industry due to Rust's safety guarantees. The translation system is based on a generate-and-check pattern, in which Rust code generated by the LLM is automatically checked for compilability and behavioral equivalence with the original C code. For negative checking results, the LLM is re-prompted in a feedback loop to repair its output. These checks also allow us to evaluate and compare the respective success rates of the translation system when varying the three variables.
  Our results show that without feedback loops LLM selection has a large effect on translation success. However, when the translation system uses feedback loops the differences across models diminish. We observe this for the average performance of the system as well as its robustness under code perturbations. Finally, we also identify that diversity provided by code perturbations can even result in improved system performance.

</details>


### [4] [Integrative Analysis of Risk Management Methodologies in Data Science Projects](https://arxiv.org/abs/2512.02728)
*Sabrina Delmondes da Costa Feitosa*

Main category: cs.SE

TL;DR: 该研究对数据科学项目的主要风险管理方法进行了比较分析，发现传统方法对新兴风险覆盖有限，而现代模型能整合伦理监督、治理和持续监控等多维度结构。


<details>
  <summary>Details</summary>
Motivation: 数据科学项目失败率高，主要受技术限制、组织局限性和风险管理实践不足影响。挑战包括数据成熟度低、缺乏治理、技术与业务团队错位、缺乏解决伦理和社会技术风险的结构化机制。本研究旨在比较分析数据科学项目的主要风险管理方法。

Method: 采用整合性文献综述方法，使用索引数据库和结构化协议进行文献选择和内容分析。研究考察了广泛采用的风险管理标准（ISO 31000、PMBOK风险管理、NIST RMF）以及数据科学工作流特定框架（CRISP DM和最近提出的DS EthiCo RMF）。

Result: 研究发现传统方法对新兴风险覆盖有限，而当代模型提出了能够整合伦理监督、治理和持续监控的多维度结构。DS EthiCo RMF等框架将伦理和社会技术维度纳入项目生命周期。

Conclusion: 本研究为开发平衡技术效率、组织对齐和负责任数据实践的混合框架提供了理论支持，同时指出了指导未来研究的研究空白。强调需要整合技术、组织和伦理维度的综合风险管理方法。

Abstract: Data science initiatives frequently exhibit high failure rates, driven by technical constraints, organizational limitations and insufficient risk management practices. Challenges such as low data maturity, lack of governance, misalignment between technical and business teams, and the absence of structured mechanisms to address ethical and sociotechnical risks have been widely identified in the literature. In this context, the purpose of this study is to conduct a comparative analysis of the main risk management methodologies applied to data science projects, aiming to identify, classify, and synthesize their similarities, differences and existing gaps. An integrative literature review was performed using indexed databases and a structured protocol for selection and content analysis. The study examines widely adopted risk management standards ISO 31000, PMBOK Risk Management and NIST RMF, as well as frameworks specific to data science workflows, such as CRISP DM and the recently proposed DS EthiCo RMF, which incorporates ethical and sociotechnical dimensions into the project life cycle. The findings reveal that traditional approaches provide limited coverage of emerging risks, whereas contemporary models propose multidimensional structures capable of integrating ethical oversight, governance and continuous monitoring. As a contribution, this work offers theoretical support for the development of hybrid frameworks that balance technical efficiency, organizational alignment and responsible data practices, while highlighting research gaps that can guide future investigations.

</details>


### [5] ["Can you feel the vibes?": An exploration of novice programmer engagement with vibe coding](https://arxiv.org/abs/2512.02750)
*Kiev Gama,Filipe Calegario,Victoria Jackson,Alexander Nolte,Luiz Augusto Morais,Vinicius Garcia*

Main category: cs.SE

TL;DR: 本文通过巴西公立大学31名本科生参与的一日教育黑客松，研究新手程序员和混合经验团队如何参与"氛围编程"（通过自然语言提示而非直接编写代码来创建软件），发现该方法能促进快速原型设计和跨学科协作，但也存在创意过早收敛、代码质量不均等问题。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI和AI辅助编程的兴起，"氛围编程"（通过自然语言提示而非直接编写代码来创建软件）有望民主化软件开发，但其教育影响尚未得到充分探索。本研究旨在调查新手程序员和混合经验团队如何参与氛围编程，以及这种方法的潜在教育价值。

Method: 在巴西公立大学组织了一日教育黑客松，31名来自计算和非计算学科的本科生分为9个团队参与。通过观察、退出调查和半结构化访谈，研究创意过程、工具使用模式、协作动态和学习成果。

Result: 氛围编程实现了快速原型设计和跨学科协作，参与者培养了提示工程技能并在时间限制内交付了功能演示。但也观察到创意过早收敛、代码质量不均需要返工、对核心软件工程实践参与有限等问题。团队采用结合多种AI工具的流水线配置工作流程，人类判断在关键细化中仍然必不可少。

Conclusion: 氛围编程黑客松可以作为有价值的低风险学习环境，但需要结合明确的支架来支持发散思维、对AI输出的批判性评估，以及对生产质量的现实期望。短格式（9小时）对建立新手信心有效，同时适应了时间有限的参与者。

Abstract: Emerging alongside generative AI and the broader trend of AI-assisted coding, the term "vibe coding" refers to creating software via natural language prompts rather than direct code authorship. This approach promises to democratize software development, but its educational implications remain underexplored. This paper reports on a one-day educational hackathon investigating how novice programmers and mixed-experience teams engage with vibe coding. We organized an inclusive event at a Brazilian public university with 31 undergraduate participants from computing and non-computing disciplines, divided into nine teams. Through observations, an exit survey, and semi-structured interviews, we examined creative processes, tool usage patterns, collaboration dynamics, and learning outcomes. Findings reveal that vibe coding enabled rapid prototyping and cross-disciplinary collaboration, with participants developing prompt engineering skills and delivering functional demonstrations within time constraints. However, we observed premature convergence in ideation, uneven code quality requiring rework, and limited engagement with core software engineering practices. Teams adopted sophisticated workflows combining multiple AI tools in pipeline configurations, with human judgment remaining essential for critical refinement. The short format (9 hours) proved effective for confidence-building among newcomers while accommodating participants with limited availability. We conclude that vibe coding hackathons can serve as valuable low-stakes learning environments when coupled with explicit scaffolds for divergent thinking, critical evaluation of AI outputs, and realistic expectations about production quality.

</details>


### [6] [Towards Observation Lakehouses: Living, Interactive Archives of Software Behavior](https://arxiv.org/abs/2512.02795)
*Marcus Kessel*

Main category: cs.SE

TL;DR: 提出观察数据湖仓架构，通过持续收集程序执行时的刺激-响应-上下文三元组，为代码生成LLM提供动态行为数据支持，实现无需重新执行的行为分析和评估。


<details>
  <summary>Details</summary>
Motivation: 当前代码生成LLM主要基于静态代码训练，缺乏运行时行为数据，容易学习到有缺陷或错误标注的代码。由于语义属性通常不可判定，获取真实功能性的唯一实用方法是通过动态执行观察。

Method: 引入观察数据湖仓架构，基于Apache Parquet + Iceberg + DuckDB构建，包含：1）高维追加式观察表存储所有执行三元组（刺激、响应、上下文）；2）按需物化SRC切片的SQL查询；3）从控制管道（LASSO）和CI管道（如单元测试）摄取数据。

Result: 在509个问题的基准测试中，摄取了约860万行观察数据（<51MiB），在笔记本电脑上<100ms内重建SRM/SRC视图和聚类，证明持续行为挖掘无需分布式集群即可实用。

Conclusion: 观察数据湖仓使行为真实数据成为与其它运行时数据同等重要的一类数据，为行为感知的评估和训练提供了基础设施路径，相关代码和数据集已在GitHub开源。

Abstract: Code-generating LLMs are trained largely on static artifacts (source, comments, specifications) and rarely on materializations of run-time behavior. As a result, they readily internalize buggy or mislabeled code. Since non-trivial semantic properties are undecidable in general, the only practical way to obtain ground-truth functionality is by dynamic observation of executions. In prior work, we addressed representation with Sequence Sheets, Stimulus-Response Matrices (SRMs), and Stimulus-Response Cubes (SRCs) to capture and compare behavior across tests, implementations, and contexts. These structures make observation data analyzable offline and reusable, but they do not by themselves provide persistence, evolution, or interactive analytics at scale. In this paper, therefore, we introduce observation lakehouses that operationalize continual SRCs: a tall, append-only observations table storing every actuation (stimulus, response, context) and SQL queries that materialize SRC slices on demand. Built on Apache Parquet + Iceberg + DuckDB, the lakehouse ingests data from controlled pipelines (LASSO) and CI pipelines (e.g., unit test executions), enabling n-version assessment, behavioral clustering, and consensus oracles without re-execution. On a 509-problem benchmark, we ingest $\approx$8.6M observation rows ($<$51MiB) and reconstruct SRM/SRC views and clusters in $<$100ms on a laptop, demonstrating that continual behavior mining is practical without a distributed cluster of machines. This makes behavioral ground truth first-class alongside other run-time data and provides an infrastructure path toward behavior-aware evaluation and training. The Observation Lakehouse, together with the accompanying dataset, is publicly available as an open-source project on GitHub: https://github.com/SoftwareObservatorium/observation-lakehouse

</details>


### [7] [Model-Based Diagnosis with Multiple Observations: A Unified Approach for C Software and Boolean Circuits](https://arxiv.org/abs/2512.02898)
*Pedro Orvalho,Marta Kwiatkowska,Mikoláš Janota,Vasco Manquinho*

Main category: cs.SE

TL;DR: CFaults是一种基于模型诊断的多故障定位工具，通过将所有失败测试用例整合到统一的MaxSAT公式中，确保跨观察的一致性并生成子集最小化诊断。


<details>
  <summary>Details</summary>
Motivation: 现有基于公式的故障定位方法在处理多故障程序/电路时存在局限性：无法保证在所有失败测试中提供一致的诊断集，且可能产生非子集最小化的冗余诊断，导致调试效率低下。

Method: CFaults采用基于模型诊断（MBD）方法，将多个观察（失败测试用例）聚合到统一的最大可满足性（MaxSAT）公式中，确保诊断结果在所有观察中保持一致，并简化故障定位流程。

Result: 在C程序基准测试集（TCAS和C-Pack-IPAs）上，CFaults比BugAssist、SNIPER和HSD等现有FBFL方法更快；在ISCAS85布尔电路基准测试中，CFaults虽然比HSD稍慢，但能在仅少6%的电路中定位故障，仍保持竞争力。此外，CFaults只生成子集最小化的故障语句诊断，而其他方法倾向于枚举冗余诊断。

Conclusion: CFaults通过整合多观察MBD和MaxSAT公式，提供了一种高效且一致的多故障定位方法，能够生成子集最小化诊断，在软件和电路故障定位领域具有竞争优势。

Abstract: Debugging is one of the most time-consuming and expensive tasks in software development and circuit design. Several formula-based fault localisation (FBFL) methods have been proposed, but they fail to guarantee a set of diagnoses across all failing tests or may produce redundant diagnoses that are not subset-minimal, particularly for programs/circuits with multiple faults.
  This paper introduces CFaults, a novel fault localisation tool for C software and Boolean circuits with multiple faults. CFaults leverages Model-Based Diagnosis (MBD) with multiple observations and aggregates all failing test cases into a unified Maximum Satisfiability (MaxSAT) formula. Consequently, our method guarantees consistency across observations and simplifies the fault localisation procedure. Experimental results on three benchmark sets, two of C programs, TCAS and C-Pack-IPAs, and one of Boolean circuits, ISCAS85, show that CFaults is faster at localising faults in C software than other FBFL approaches such as BugAssist, SNIPER, and HSD. On the ISCAS85 benchmark, CFaults is generally slower than HSD; however, it localises faults in only 6% fewer circuits, demonstrating that it remains competitive in this domain. Furthermore, CFaults produces only subset-minimal diagnoses of faulty statements, whereas the other approaches tend to enumerate redundant diagnoses (e.g., BugAssist and SNIPER).

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [8] [The 4/$δ$ Bound: Designing Predictable LLM-Verifier Systems for Formal Method Guarantee](https://arxiv.org/abs/2512.02080)
*PIerre Dantas,Lucas Cordeiro,Youcheng Sun,Waldir Junior*

Main category: cs.AI

TL;DR: 本文提出了首个LLM-验证器收敛定理，为LLM辅助的形式验证提供了理论保证，证明了对任意δ>0，验证过程几乎必然终止，且期望迭代次数有界于4/δ。


<details>
  <summary>Details</summary>
Motivation: 当前使用大型语言模型进行形式验证的方法缺乏理论基础，导致验证过程不可靠、不稳定，有时收敛、有时循环、有时偏离稳定轨迹，无法为安全关键软件环境提供可靠保障。

Method: 将LLM与验证器的交互建模为离散时间马尔可夫链，状态转移由关键参数δ（错误减少概率）决定。通过理论分析证明收敛性，并在超过90,000次试验中进行大规模实证验证。

Result: 理论证明验证过程对任意δ>0几乎必然终止，期望迭代次数E[n]≤4/δ。实证结果显示所有运行都成功验证，收敛因子Cf≈1.0，理论与实际高度一致。基于此建立了三个操作区域：边缘、实用和高性能。

Conclusion: 本文为LLM辅助验证提供了坚实的理论基础和实证支持，使工程师能够进行可预测的资源规划和性能预算，为部署到安全关键软件环境提供了必要的架构基础。

Abstract: The idea of using Formal Verification tools with large language models (LLMs) has enabled scaling software verification beyond manual workflows. However, current methods remain unreliable. Without a solid theoretical footing, the refinement process can wander; sometimes it settles, sometimes it loops back, and sometimes it breaks away from any stable trajectory. This work bridges this critical gap by developing an LLM-Verifier Convergence Theorem, providing the first formal framework with provable guarantees for termination and convergence. We model the interaction between the LLM and the verifier as a discrete-time Markov Chain, with state transitions determined by a key parameter: the error-reduction probability ($δ$). The procedure reaching the Verified state almost surely demonstrates that the program terminates for any $δ> 0$, with an expected iteration count bounded by $\mathbb{E}[n] \leq 4/δ$. We then stress-tested this prediction in an extensive empirical campaign comprising more than 90,000 trials. The empirical results match the theory with striking consistency. Every single run reached verification, and the convergence factor clustered tightly around $C_f\approx$ 1.0. Consequently, the bound mirrors the system's actual behavior. The evidence is sufficiently robust to support dividing the workflow into three distinct operating zones: marginal, practical, and high-performance. Consequently, we establish the design thresholds with absolute confidence. Together, the theoretical guarantee and the experimental evidence provide a clearer architectural foundation for LLM-assisted verification. Heuristic tuning no longer has to be carried out by the system. Engineers gain a framework that supports predictable resource planning and performance budgeting, precisely what is needed before deploying these pipelines into safety-critical software environments.

</details>


### [9] [Flowchart2Mermaid: A Vision-Language Model Powered System for Converting Flowcharts into Editable Diagram Code](https://arxiv.org/abs/2512.02170)
*Pritam Deka,Barry Devereux*

Main category: cs.AI

TL;DR: Flowchart2Mermaid：将流程图图像转换为可编辑Mermaid.js代码的轻量级Web系统，支持混合主动式编辑和AI助手


<details>
  <summary>Details</summary>
Motivation: 流程图通常以静态图像形式共享，难以编辑和重用，需要一种方法将流程图图像转换为结构化、版本可控的文本表示

Method: 使用详细系统提示和视觉语言模型将流程图图像转换为Mermaid.js代码，支持内联文本编辑、拖放节点插入和自然语言命令的AI助手

Result: 开发了Flowchart2Mermaid系统，能够生成与渲染图表保持同步的结构化文本表示，并引入了评估结构准确性、流程正确性、语法有效性和完整性的指标

Conclusion: 该系统提供了一种将静态流程图转换为可编辑、版本可控的Mermaid.js代码的有效方法，支持混合主动式编辑和AI辅助，优于现有的图像到图表工具

Abstract: Flowcharts are common tools for communicating processes but are often shared as static images that cannot be easily edited or reused. We present Flowchart2Mermaid, a lightweight web system that converts flowchart images into editable Mermaid.js code which is a markup language for visual workflows, using a detailed system prompt and vision-language models. The interface supports mixed-initiative refinement through inline text editing, drag-and-drop node insertion, and natural-language commands interpreted by an integrated AI assistant. Unlike prior image-to-diagram tools, our approach produces a structured, version-controllable textual representation that remains synchronized with the rendered diagram. We further introduce evaluation metrics to assess structural accuracy, flow correctness, syntax validity, and completeness across multiple models.

</details>


### [10] [From monoliths to modules: Decomposing transducers for efficient world modelling](https://arxiv.org/abs/2512.02193)
*Alexander Boyd,Franz Nowak,David Hyland,Manuel Baltieri,Fernando E. Rosas*

Main category: cs.AI

TL;DR: 论文提出了一种将复杂世界模型分解为子转换器的框架，支持并行化和可解释的分布式推理，为AI安全的结构透明性和计算效率搭建桥梁。


<details>
  <summary>Details</summary>
Motivation: 现实世界模型通常计算需求高，但真实场景往往包含以模块化方式交互的子组件。为了在AI安全所需的结构透明性和实际推理所需的计算效率之间建立桥梁，需要开发能够分解复杂世界模型的框架。

Method: 开发了一个框架，用于分解由转换器（transducers）表示的复杂世界模型。转换器是泛化POMDPs的一类模型。该框架能够将复杂转换器分解为在独立输入-输出子空间上运行的子转换器，支持并行化和可解释的替代方案。

Result: 澄清了如何逆转转换器组合过程，推导出在独立输入-输出子空间上运行的子转换器，实现了可并行化和可解释的替代方案，替代了单一的世界建模方法，支持分布式推理。

Conclusion: 这些结果为连接AI安全所需的结构透明性和实际推理所需的计算效率奠定了基础，为分布式推理提供了框架支持。

Abstract: World models have been recently proposed as sandbox environments in which AI agents can be trained and evaluated before deployment. Although realistic world models often have high computational demands, efficient modelling is usually possible by exploiting the fact that real-world scenarios tend to involve subcomponents that interact in a modular manner. In this paper, we explore this idea by developing a framework for decomposing complex world models represented by transducers, a class of models generalising POMDPs. Whereas the composition of transducers is well understood, our results clarify how to invert this process, deriving sub-transducers operating on distinct input-output subspaces, enabling parallelizable and interpretable alternatives to monolithic world modelling that can support distributed inference. Overall, these results lay a groundwork for bridging the structural transparency demanded by AI safety and the computational efficiency required for real-world inference.

</details>


### [11] [STRIDE: A Systematic Framework for Selecting AI Modalities -- Agentic AI, AI Assistants, or LLM Calls](https://arxiv.org/abs/2512.02228)
*Shubhi Asthana,Bing Zhang,Chad DeLuca,Ruchi Mahindru,Hima Patel*

Main category: cs.AI

TL;DR: STRIDE框架通过系统化评估任务特性，为AI部署提供三种模式选择建议：直接LLM调用、引导式AI助手、完全自主的智能体，避免不必要的智能体部署以降低成本风险。


<details>
  <summary>Details</summary>
Motivation: 随着AI从无状态大语言模型向自主智能体的快速转变，需要解决一个核心问题：何时真正需要智能体AI？不加区分地部署智能体会导致更高的成本、复杂性和风险，因此需要系统化的决策框架。

Method: 提出STRIDE框架，通过结构化任务分解、动态性归因和自我反思需求分析，计算智能体适用性分数，为任务推荐三种部署模式：直接LLM调用、引导式AI助手或完全自主智能体。

Result: 在30个真实世界任务（涵盖SRE、合规和企业自动化）中，STRIDE实现了92%的模式选择准确率，减少了45%不必要的智能体部署，并降低了37%的资源成本。经过6个月的专家验证，领域专家确认其实际效用。

Conclusion: 该研究将智能体采用重新定义为需求驱动的设计决策，确保自主性仅在收益超过成本时应用，为AI部署提供了系统化的决策框架。

Abstract: The rapid shift from stateless large language models (LLMs) to autonomous, goal-driven agents raises a central question: When is agentic AI truly necessary? While agents enable multi-step reasoning, persistent memory, and tool orchestration, deploying them indiscriminately leads to higher cost, complexity, and risk.
  We present STRIDE (Systematic Task Reasoning Intelligence Deployment Evaluator), a framework that provides principled recommendations for selecting between three modalities: (i) direct LLM calls, (ii) guided AI assistants, and (iii) fully autonomous agentic AI. STRIDE integrates structured task decomposition, dynamism attribution, and self-reflection requirement analysis to produce an Agentic Suitability Score, ensuring that full agentic autonomy is reserved for tasks with inherent dynamism or evolving context.
  Evaluated across 30 real-world tasks spanning SRE, compliance, and enterprise automation, STRIDE achieved 92% accuracy in modality selection, reduced unnecessary agent deployments by 45%, and cut resource costs by 37%. Expert validation over six months in SRE and compliance domains confirmed its practical utility, with domain specialists agreeing that STRIDE effectively distinguishes between tasks requiring simple LLM calls, guided assistants, or full agentic autonomy. This work reframes agent adoption as a necessity-driven design decision, ensuring autonomy is applied only when its benefits justify the costs.

</details>


### [12] [Benchmarking LLM Agents for Wealth-Management Workflows](https://arxiv.org/abs/2512.02230)
*Rory Milsom*

Main category: cs.AI

TL;DR: 该研究扩展了TheAgentCompany平台，创建了财富管理任务评估基准，发现LLM智能体在端到端工作流可靠性方面存在限制，而非数学推理能力，且自主性水平对性能有显著影响。


<details>
  <summary>Details</summary>
Motivation: 尽管现代工作依赖各种数字协作工具，但常规流程仍受人为错误和延迟困扰。研究旨在探索通用LLM智能体是否能够准确且经济地完成代表性财富管理任务，填补现有工具与需求之间的差距。

Method: 扩展TheAgentCompany平台，创建金融领域环境；引入合成领域数据；丰富同事模拟；原型化自动任务生成管道；构建包含12个任务对的财富管理助理基准，涵盖检索、分析和综合/沟通任务，包含明确接受标准和确定性评分器。

Result: 智能体的限制主要在于端到端工作流可靠性而非数学推理能力；自主性水平对性能有显著影响；错误的模型评估方法阻碍了基准测试的有效性。

Conclusion: 研究成功创建了能够有意义衡量智能体在财富管理助理工作中适应性的评估集，揭示了当前智能体系统在实际工作流中的关键限制因素，为未来改进提供了方向。

Abstract: Modern work relies on an assortment of digital collaboration tools, yet routine processes continue to suffer from human error and delay. To address this gap, this dissertation extends TheAgentCompany with a finance-focused environment and investigates whether a general purpose LLM agent can complete representative wealth-management tasks both accurately and economically. This study introduces synthetic domain data, enriches colleague simulations, and prototypes an automatic task-generation pipeline. The study aims to create and assess an evaluation set that can meaningfully measure an agent's fitness for assistant-level wealth management work. We construct a benchmark of 12 task-pairs for wealth management assistants spanning retrieval, analysis, and synthesis/communication, with explicit acceptance criteria and deterministic graders. We seeded a set of new finance-specific data and introduced a high vs. low-autonomy variant of every task. The paper concluded that agents are limited less by mathematical reasoning and more so by end-to-end workflow reliability, and meaningfully affected by autonomy level, and that incorrect evaluation of models have hindered benchmarking.

</details>


### [13] [Bridging the Gap: Toward Cognitive Autonomy in Artificial Intelligence](https://arxiv.org/abs/2512.02280)
*Noorbakhsh Amiri Golilarz,Sindhuja Penchala,Shahram Rahimi*

Main category: cs.AI

TL;DR: 论文分析了当前AI系统的七大核心缺陷，提出需要向基于认知原理的AI架构转变，以实现真正的自主性和适应性。


<details>
  <summary>Details</summary>
Motivation: 尽管AI在感知、语言、推理和多模态领域取得快速进展，但现代AI系统在自我监控、自我纠正和自主行为调节方面仍存在根本性局限。这些限制阻碍了AI实现稳健泛化、终身适应性和真实世界自主性。

Method: 通过比较分析人工系统与生物认知，整合AI研究、认知科学和神经科学的见解，识别并分析了当代AI模型的七大核心缺陷：缺乏内在自我监控、元认知意识缺失、固定非适应性学习机制、无法重构目标、缺乏表征维护、不足的具身反馈、内在能动性缺失。

Result: 研究发现当前架构（包括深度学习和基于Transformer的系统）存在结构性限制，仅靠扩展规模无法解决这些问题。这些缺陷限制了AI的自主适应能力和真实世界应用。

Conclusion: 需要向基于认知原理的AI（认知自主性）进行范式转变，使其具备自我导向适应、动态表征管理和有意图的目标导向行为能力，同时配备改革性监督机制，确保自主系统保持可解释性、可治理性并与人类价值观对齐。

Abstract: Artificial intelligence has advanced rapidly across perception, language, reasoning, and multimodal domains. Yet despite these achievements, modern AI systems remain fundamentally limited in their ability to self-monitor, self-correct, and regulate their behavior autonomously in dynamic contexts. This paper identifies and analyzes seven core deficiencies that constrain contemporary AI models: the absence of intrinsic self-monitoring, lack of meta-cognitive awareness, fixed and non-adaptive learning mechanisms, inability to restructure goals, lack of representational maintenance, insufficient embodied feedback, and the absence of intrinsic agency. Alongside identifying these limitations, we also outline a forward-looking perspective on how AI may evolve beyond them through architectures that mirror neurocognitive principles. We argue that these structural limitations prevent current architectures, including deep learning and transformer-based systems, from achieving robust generalization, lifelong adaptability, and real-world autonomy. Drawing on a comparative analysis of artificial systems and biological cognition [7], and integrating insights from AI research, cognitive science, and neuroscience, we outline how these capabilities are absent in current models and why scaling alone cannot resolve them. We conclude by advocating for a paradigmatic shift toward cognitively grounded AI (cognitive autonomy) capable of self-directed adaptation, dynamic representation management, and intentional, goal-oriented behavior, paired with reformative oversight mechanisms [8] that ensure autonomous systems remain interpretable, governable, and aligned with human values.

</details>


### [14] [Model Recovery at the Edge under Resource Constraints for Physical AI](https://arxiv.org/abs/2512.02283)
*Bin Xu,Ayan Banerjee,Sandeep K. S. Gupta*

Main category: cs.AI

TL;DR: MERINDA是一个面向边缘设备的FPGA加速模型恢复框架，通过可并行神经架构替代迭代求解器，显著降低内存和能耗，适用于实时关键任务自主系统。


<details>
  <summary>Details</summary>
Motivation: 模型恢复（MR）对于关键任务自主系统的安全可解释决策很重要，但现有基于神经常微分方程的迭代求解方法在边缘设备上部署困难，主要受限于FPGA上迭代求解的低效性以及内存和能耗问题。

Method: 提出MERINDA框架，用可并行化的神经架构替代神经常微分方程中的迭代求解器，实现FPGA加速，专门针对边缘设备优化。

Result: 相比移动GPU，MERINDA实现了近11倍的DRAM使用量降低和2.2倍的运行速度提升。实验还揭示了在固定精度下内存与能量之间的反比关系。

Conclusion: MERINDA框架特别适合资源受限的实时关键任务自主系统，通过FPGA加速解决了模型恢复在边缘设备部署中的内存和能耗瓶颈问题。

Abstract: Model Recovery (MR) enables safe, explainable decision making in mission-critical autonomous systems (MCAS) by learning governing dynamical equations, but its deployment on edge devices is hindered by the iterative nature of neural ordinary differential equations (NODEs), which are inefficient on FPGAs. Memory and energy consumption are the main concerns when applying MR on edge devices for real-time operation. We propose MERINDA, a novel FPGA-accelerated MR framework that replaces iterative solvers with a parallelizable neural architecture equivalent to NODEs. MERINDA achieves nearly 11x lower DRAM usage and 2.2x faster runtime compared to mobile GPUs. Experiments reveal an inverse relationship between memory and energy at fixed accuracy, highlighting MERINDA's suitability for resource-constrained, real-time MCAS.

</details>


### [15] [Breast Cell Segmentation Under Extreme Data Constraints: Quantum Enhancement Meets Adaptive Loss Stabilization](https://arxiv.org/abs/2512.02302)
*Varun Kumar Dasoju,Qingsu Cheng,Zeyun Yu*

Main category: cs.AI

TL;DR: 该研究提出了一种用于乳腺细胞分割的量子增强框架，仅使用599张训练图像就达到了95.5%的Dice分数，显著减少了医学图像标注所需的时间和专家投入。


<details>
  <summary>Details</summary>
Motivation: 医学图像标注需要大量时间和专业知识，特别是乳腺上皮细胞分割任务中，乳腺组织仅占图像的4%，60%的图像不包含乳腺区域，且存在严重的类别不平衡问题（乳腺细胞区域仅占图像面积的0.1%-20%）。

Method: 1. 使用多尺度Gabor滤波器进行量子启发的边缘增强，创建第四输入通道以改善边界检测；2. 提出稳定的多组件损失函数，结合自适应Dice损失、边界感知项和自动正样本加权；3. 引入基于复杂度的加权采样策略，优先处理具有挑战性的乳腺上皮细胞区域；4. 采用EfficientNet-B7/UNet++架构，通过4到3通道投影利用预训练权重；5. 通过指数移动平均和统计异常值检测进行稳健验证。

Result: 框架达到Dice分数95.5% ± 0.3%和IoU 91.2% ± 0.4%。量子增强使边界准确率提升2.1%，加权采样使小病变检测率提高3.8%。

Conclusion: 该方法通过有限标注实现了突破性性能，显著减少了医学专家创建数据集所需的时间，解决了临床感知AI开发中的基本瓶颈问题。

Abstract: Annotating medical images demands significant time and expertise, often requiring pathologists to invest hundreds of hours in labeling mammary epithelial nuclei datasets. We address this critical challenge by achieving 95.5% Dice score using just 599 training images for breast cell segmentation, where just 4% of pixels represent breast tissue and 60% of images contain no breast regions. Our framework uses quantum-inspired edge enhancement via multi-scale Gabor filters creating a fourth input channel, enhancing boundary detection where inter-annotator variations reach +/- 3 pixels. We present a stabilized multi-component loss function that integrates adaptive Dice loss with boundary-aware terms and automatic positive weighting to effectively address severe class imbalance, where mammary epithelial cell regions comprise only 0.1%-20% of the total image area. Additionally, a complexity-based weighted sampling strategy is introduced to prioritize the challenging mammary epithelial cell regions. The model employs an EfficientNet-B7/UNet++ architecture with a 4-to-3 channel projection, enabling the use of pretrained weights despite limited medical imaging data. Finally, robust validation is achieved through exponential moving averaging and statistical outlier detection, ensuring reliable performance estimates on a small validation set (129 images). Our framework achieves a Dice score of 95.5% +/- 0.3% and an IoU of 91.2% +/- 0.4%. Notably, quantum-based enhancement contributes to a 2.1% improvement in boundary accuracy, while weighted sampling increases small lesion detection by 3.8%. By achieving groundbreaking performance with limited annotations, our approach significantly reduces the medical expert time required for dataset creation, addressing a fundamental bottleneck in clinical perception AI development.

</details>


### [16] [OmniGuard: Unified Omni-Modal Guardrails with Deliberate Reasoning](https://arxiv.org/abs/2512.02306)
*Boyu Zhu,Xiaofei Wen,Wenjie Jacky Mo,Tinghui Zhu,Yanan Xie,Peng Qi,Muhao Chen*

Main category: cs.AI

TL;DR: OmniGuard是首个全模态安全护栏系统，通过结构化安全标签和专家模型蒸馏，在文本、图像、视频、音频等多种模态上实现统一的安全防护。


<details>
  <summary>Details</summary>
Motivation: 现有安全护栏研究主要针对单模态设置，通常将安全防护视为二元分类，这限制了在不同模态和任务间的鲁棒性。随着全模态大语言模型的出现，需要新的安全防护方法。

Method: 提出OmniGuard框架，构建包含21万+多样化样本的全模态安全数据集，每个样本都有结构化安全标签和通过目标蒸馏从专家模型获取的安全评析。系统具备跨所有模态的深思熟虑推理能力。

Result: 在15个基准测试上的广泛实验表明，OmniGuard在多种多模态安全场景中实现了强大的有效性和泛化能力。

Conclusion: OmniGuard提供了一个统一框架，能够在全模态中执行策略并降低风险，为构建更鲁棒和强大的全模态安全系统铺平了道路。

Abstract: Omni-modal Large Language Models (OLLMs) that process text, images, videos, and audio introduce new challenges for safety and value guardrails in human-AI interaction. Prior guardrail research largely targets unimodal settings and typically frames safeguarding as binary classification, which limits robustness across diverse modalities and tasks. To address this gap, we propose OmniGuard, the first family of omni-modal guardrails that performs safeguarding across all modalities with deliberate reasoning ability. To support the training of OMNIGUARD, we curate a large, comprehensive omni-modal safety dataset comprising over 210K diverse samples, with inputs that cover all modalities through both unimodal and cross-modal samples. Each sample is annotated with structured safety labels and carefully curated safety critiques from expert models through targeted distillation. Extensive experiments on 15 benchmarks show that OmniGuard achieves strong effectiveness and generalization across a wide range of multimodal safety scenarios. Importantly, OmniGuard provides a unified framework that enforces policies and mitigates risks in omni-modalities, paving the way toward building more robust and capable omnimodal safeguarding systems.

</details>


### [17] [Reasoning Path and Latent State Analysis for Multi-view Visual Spatial Reasoning: A Cognitive Science Perspective](https://arxiv.org/abs/2512.02340)
*Qiyao Xue,Weichen Liu,Shiqi Wang,Haoming Wang,Yuyang Wu,Wei Gao*

Main category: cs.AI

TL;DR: 提出了ReMindView-Bench基准测试，用于评估视觉语言模型在多视角空间推理中构建、对齐和维护空间心理模型的能力，发现现有模型在跨视角对齐和视角转换方面存在系统性失败。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在多视角设置中进行空间推理时，难以保持几何一致性和跨视角一致性。这种差距源于缺乏能够将多视角推理与单视角感知和时间因素分离的细粒度基准测试。

Method: 提出了ReMindView-Bench基准测试，系统性地变化视角空间模式和查询类型来探测空间认知的关键因素。使用LLM-as-a-judge和自一致性提示进行显式分阶段分析，以及线性探测和熵动态进行隐式分析。

Result: 评估了15个当前VLMs，发现它们在跨视角对齐和视角转换方面存在一致性的失败。模型在帧内感知表现良好，但在跨视角信息整合时性能急剧下降。隐式分析显示任务相关信息逐渐丢失，正确与错误轨迹之间的不确定性分离。

Conclusion: ReMindView-Bench为VLM空间推理提供了认知基础的诊断，揭示了多视角空间心理模型在推理阶段如何形成、退化和失稳。基准测试和源代码已公开可用。

Abstract: Spatial reasoning is a core aspect of human intelligence that allows perception, inference and planning in 3D environments. However, current vision-language models (VLMs) struggle to maintain geometric coherence and cross-view consistency for spatial reasoning in multi-view settings. We attribute this gap to the lack of fine-grained benchmarks that isolate multi-view reasoning from single-view perception and temporal factors. To address this, we present ReMindView-Bench, a cognitively grounded benchmark for evaluating how VLMs construct, align and maintain spatial mental models across complementary viewpoints. ReMindView-Bench systematically varies viewpoint spatial pattern and query type to probe key factors of spatial cognition. Evaluations of 15 current VLMs reveals consistent failures in cross-view alignment and perspective-taking in multi-view spatial reasoning, motivating deeper analysis on the reasoning process. Explicit phase-wise analysis using LLM-as-a-judge and self-consistency prompting shows that VLMs perform well on in-frame perception but degrade sharply when integrating information across views. Implicit analysis, including linear probing and entropy dynamics, further show progressive loss of task-relevant information and uncertainty separation between correct and incorrect trajectories. These results provide a cognitively grounded diagnosis of VLM spatial reasoning and reveal how multi-view spatial mental models are formed, degraded and destabilized across reasoning phases. The ReMindView-Bench benchmark is available at https://huggingface.co/datasets/Xue0823/ReMindView-Bench, and the source codes of benchmark construction and VLM reasoning analysis are available at https://github.com/pittisl/ReMindView-Bench.

</details>


### [18] [PaperDebugger: A Plugin-Based Multi-Agent System for In-Editor Academic Writing, Review, and Editing](https://arxiv.org/abs/2512.02589)
*Junyi Hou,Andre Lin Huikai,Nuo Chen,Yiwei Gong,Bingsheng He*

Main category: cs.AI

TL;DR: PaperDebugger是一个集成在LaTeX编辑器中的多智能体学术写作助手，通过Chrome扩展、Kubernetes编排层和MCP工具链实现深度文档交互


<details>
  <summary>Details</summary>
Motivation: 现有LLM写作助手与编辑器分离，无法深度交互文档状态、结构和修订历史，限制了在LaTeX编辑器中的上下文感知操作

Method: 开发基于Chrome扩展的插件系统，采用Kubernetes原生编排层进行多智能体调度，集成MCP工具链支持文献搜索、参考文献查找、文档评分和修订管道

Result: 实现了完全集成的工作流，包括本地化编辑、结构化评审、并行智能体执行和基于差异的更新，用户参与度数据显示了编辑原生智能体写作助手的实用性

Conclusion: PaperDebugger成功解决了在编辑器内集成LLM驱动推理的技术挑战，为学术写作提供了深度交互的智能体辅助工具

Abstract: Large language models are increasingly embedded into academic writing workflows, yet existing assistants remain external to the editor, preventing deep interaction with document state, structure, and revision history. This separation makes it impossible to support agentic, context-aware operations directly within LaTeX editors such as Overleaf. We present PaperDebugger, an in-editor, multi-agent, and plugin-based academic writing assistant that brings LLM-driven reasoning directly into the writing environment. Enabling such in-editor interaction is technically non-trivial: it requires reliable bidirectional synchronization with the editor, fine-grained version control and patching, secure state management, multi-agent scheduling, and extensible communication with external tools. PaperDebugger addresses these challenges through a Chrome-approved extension, a Kubernetes-native orchestration layer, and a Model Context Protocol (MCP) toolchain that integrates literature search, reference lookup, document scoring, and revision pipelines. Our demo showcases a fully integrated workflow, including localized edits, structured reviews, parallel agent execution, and diff-based updates, encapsulated within a minimal-intrusion user interface (UI). Early aggregated analytics demonstrate active user engagement and validate the practicality of an editor-native, agentic writing assistant. More details about this demo and video could be found at https://github.com/PaperDebugger/PaperDebugger.

</details>


### [19] [Beyond Playtesting: A Generative Multi-Agent Simulation System for Massively Multiplayer Online Games](https://arxiv.org/abs/2512.02358)
*Ran Zhang,Kun Ouyang,Tiancheng Ma,Yida Yang,Dong Fang*

Main category: cs.AI

TL;DR: 使用LLM驱动的生成式智能体模拟MMO游戏，通过真实玩家行为数据训练，实现高保真度的玩家行为模拟和干预响应，为数值系统优化提供低成本、可解释的框架。


<details>
  <summary>Details</summary>
Motivation: 传统MMO游戏数值系统和机制设计的优化方法存在成本高、耗时长、可能破坏玩家体验的问题，而简化的离线模拟系统保真度有限，无法准确模拟真实玩家的推理和干预反应。

Method: 提出基于大语言模型（LLM）的生成式智能体MMO模拟系统，通过监督微调（SFT）和强化学习（RL）在大规模真实玩家行为数据上训练，将LLM从通用先验适配到游戏特定领域；同时使用真实游戏日志训练数据驱动的环境模型来重建动态游戏系统。

Result: 实验表明，该系统与真实世界玩家行为具有强一致性，在干预下能产生合理的因果响应，为数据驱动的数值设计优化提供了可靠、可解释且成本高效的框架。

Conclusion: LLM驱动的生成式智能体模拟系统能够有效解决传统MMO游戏数值优化方法的局限性，通过高保真度的玩家行为模拟和干预响应，为游戏设计优化提供了创新的解决方案。

Abstract: Optimizing numerical systems and mechanism design is crucial for enhancing player experience in Massively Multiplayer Online (MMO) games. Traditional optimization approaches rely on large-scale online experiments or parameter tuning over predefined statistical models, which are costly, time-consuming, and may disrupt player experience. Although simplified offline simulation systems are often adopted as alternatives, their limited fidelity prevents agents from accurately mimicking real player reasoning and reactions to interventions. To address these limitations, we propose a generative agent-based MMO simulation system empowered by Large Language Models (LLMs). By applying Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) on large-scale real player behavioral data, we adapt LLMs from general priors to game-specific domains, enabling realistic and interpretable player decision-making. In parallel, a data-driven environment model trained on real gameplay logs reconstructs dynamic in-game systems. Experiments demonstrate strong consistency with real-world player behaviors and plausible causal responses under interventions, providing a reliable, interpretable, and cost-efficient framework for data-driven numerical design optimization.

</details>


### [20] [Synthetic Error Injection Fails to Elicit Self-Correction In Language Models](https://arxiv.org/abs/2512.02389)
*David X. Wu,Shreyas Kapur,Anant Sahai,Stuart Russell*

Main category: cs.AI

TL;DR: 论文研究使用监督学习和人工错误注入来培养语言模型的自我纠错能力，发现这种方法效果有限，即使模型发现错误也常重复原错误，解释了为什么基于策略的强化学习方法在激发自我纠错方面更有效。


<details>
  <summary>Details</summary>
Motivation: 强化学习已成为激发大语言模型推理和自我纠错能力的主要方法，但其计算成本高昂，促使研究者探索替代方案。受自动驾驶和机器人技术启发，研究是否可以通过监督学习和合成错误注入来诱导语言模型的自我纠错能力。

Method: 在推理链中插入人工错误，将其掩码，然后监督模型识别和纠正这些错误。通过合成错误注入的方法训练模型进行自我纠错。

Result: 该方法即使在简单的合成任务上也无法显著提高多个模型的性能。即使模型发现了自己的错误，也常常重复原始错误。研究发现，从合成错误到策略错误的分布偏移显著降低了微调模型的纠错能力，即使合成错误对策略错误有良好的覆盖。

Conclusion: 研究结果解释了为什么基于策略的强化学习方法在激发自我纠错方面被证明特别有效。监督学习加合成错误注入的方法无法有效培养语言模型的自我纠错能力，这支持了强化学习在该领域的独特优势。

Abstract: Reinforcement learning has become the dominant paradigm for eliciting reasoning and self-correction capabilities in large language models, but its computational expense motivates exploration of alternatives. Inspired by techniques from autonomous driving and robotics, we investigate whether supervised learning with synthetic error injection can induce self-correction abilities in language models. Our approach inserts artificial errors into reasoning chains, masks them, and supervises the model to recognize and correct these mistakes. Despite the intuitive appeal of this method, we find that it fails to significantly improve performance even on simple synthetic tasks across multiple models. Moreover, even when the model catches its own error, it often parrots the original mistake. We find that the distribution shift of synthetic errors to on-policy errors significantly degrades the error-correction capabilities of the fine-tuned model, even with good synthetic coverage of on-policy errors. Our results help explain why on-policy reinforcement learning methods have proven uniquely effective for eliciting self-correction.

</details>


### [21] [Guided Self-Evolving LLMs with Minimal Human Supervision](https://arxiv.org/abs/2512.02472)
*Wenhao Yu,Zhenwen Liang,Chengsong Huang,Kishan Panaganti,Tianqing Fang,Haitao Mi,Dong Yu*

Main category: cs.AI

TL;DR: R-Few是一个引导式自我进化框架，通过轻量级人类监督实现AI模型的稳定可控自我进化，在数学和通用推理任务上取得持续改进


<details>
  <summary>Details</summary>
Motivation: AI自我进化长期以来被认为是通往超智能的路径，但实践中无引导的自我进化系统往往快速达到平台期甚至性能下降。这些问题源于概念漂移、多样性崩溃和错误进化等挑战，模型会强化自身偏见并收敛到低熵行为。

Method: 提出R-Few框架，采用引导式自我对弈的挑战者-求解器架构。挑战者通过少量人工标注示例指导合成问题生成，求解器在在线难度课程下联合训练人工和合成示例。结合上下文基础和混合训练实现轻量级人类监督。

Result: 在数学和通用推理基准测试中，R-Few实现了一致且迭代的改进。例如，Qwen3-8B-Base在数学任务上比R-Zero提高了3.0分，性能与General-Reasoner相当，而后者使用了20倍的人工数据。消融研究证实了基础挑战者训练和课程求解器训练的互补贡献。

Conclusion: R-Few通过引导式自我对弈框架有效缓解了漂移问题，产生了更稳定可控的协同进化动态，为AI自我进化提供了实用且高效的解决方案，显著减少了对人类监督的依赖。

Abstract: AI self-evolution has long been envisioned as a path toward superintelligence, where models autonomously acquire, refine, and internalize knowledge from their own learning experiences. Yet in practice, unguided self-evolving systems often plateau quickly or even degrade as training progresses. These failures arise from issues such as concept drift, diversity collapse, and mis-evolution, as models reinforce their own biases and converge toward low-entropy behaviors. To enable models to self-evolve in a stable and controllable manner while minimizing reliance on human supervision, we introduce R-Few, a guided Self-Play Challenger-Solver framework that incorporates lightweight human oversight through in-context grounding and mixed training. At each iteration, the Challenger samples a small set of human-labeled examples to guide synthetic question generation, while the Solver jointly trains on human and synthetic examples under an online, difficulty-based curriculum. Across math and general reasoning benchmarks, R-Few achieves consistent and iterative improvements. For example, Qwen3-8B-Base improves by +3.0 points over R-Zero on math tasks and achieves performance on par with General-Reasoner, despite the latter being trained on 20 times more human data. Ablation studies confirm the complementary contributions of grounded challenger training and curriculum-based solver training, and further analysis shows that R-Few mitigates drift, yielding more stable and controllable co-evolutionary dynamics.

</details>


### [22] [COPE: Chain-Of-Thought Prediction Engine for Open-Source Large Language Model Based Stroke Outcome Prediction from Clinical Notes](https://arxiv.org/abs/2512.02499)
*Yongkai Liu,Helena Feng,Bin Jiang,Yixin Wang,Max Wintermark,David S. Liebeskind,Michael Moseley,Maarten Lansberg,Gregory Albers,Jeremy Heit,Greg Zaharchuk*

Main category: cs.AI

TL;DR: COPE框架使用两步链式思维LLaMA-3-8B模型，从非结构化临床笔记预测急性缺血性卒中90天功能结局，性能与GPT-4.1相当，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 临床笔记包含丰富的上下文信息，但其非结构化特性限制了在传统预测模型中的应用。需要开发能够利用这些信息进行卒中结局预测的框架。

Method: 开发了COPE框架，基于两步链式思维架构：第一步生成临床推理，第二步输出改良Rankin量表预测。使用464名急性缺血性卒中患者数据，比较了COPE与GPT-4.1、ClinicalBERT、结构化变量机器学习模型和单步LLM的性能。

Result: COPE的MAE为1.01，±1准确率为74.4%，精确准确率为32.8%，性能与GPT-4.1相当，优于ClinicalBERT、临床ML和单步LLM。在不同亚组中表现一致。

Conclusion: COPE作为一个轻量级、可解释且保护隐私的开源框架，为从非结构化临床文本进行结局预测提供了准确实用的解决方案。

Abstract: Predicting outcomes in acute ischemic stroke (AIS) guides clinical decision-making, patient counseling, and resource allocation. Clinical notes contain rich contextual information, but their unstructured nature limits their use in traditional predictive models. We developed and evaluated the Chain-of-Thought (CoT) Outcome Prediction Engine (COPE), a reasoning-enhanced large language model framework, for predicting 90-day functional outcomes after AIS from unstructured clinical notes. This study included 464 AIS patients with discharge summaries and 90-day modified Rankin Scale (mRS) scores. COPE uses a two-step CoT framework based on sequential open-source LLaMA-3-8B models: the first generates clinical reasoning, and the second outputs an mRS prediction. We compared COPE with GPT-4.1, ClinicalBERT, a structured variable-based machine learning model (Clinical ML), and a single-step LLM without CoT. Performance was evaluated using mean absolute error (MAE), accuracy within +/-1 mRS point, and exact accuracy. COPE achieved an MAE of 1.01 (95% CI 0.92-1.11), +/-1 accuracy of 74.4% (69.9, 78.8%), and exact accuracy of 32.8% (28.0, 37.6%), comparable to GPT-4.1 and superior to ClinicalBERT [MAE 1.24 (1.13-1.36)], Clinical ML [1.28 (1.18-1.39)], and the single-step LLM [1.20 (1.09-1.33)]. Subgroup analyses showed consistent performance across sex and age, with slightly higher error among older patients, those undergoing thrombectomy, and those with longer summaries. These findings demonstrate that COPE, a lightweight, interpretable, and privacy-preserving open-source framework, provides an accurate and practical solution for outcome prediction from unstructured clinical text.

</details>


### [23] [Aetheria: A multimodal interpretable content safety framework based on multi-agent debate and collaboration](https://arxiv.org/abs/2512.02530)
*Yuxiang He,Jian Zhao,Yuchen Yuan,Tianle Zhang,Wei Cai,Haojie Cheng,Ziyan Shi,Ming Zhu,Haichuan Tang,Chi Zhang,Xuelong Li*

Main category: cs.AI

TL;DR: Aetheria是一个基于多智能体辩论协作的多模态可解释内容安全框架，通过动态辩论机制和RAG知识检索，显著提升内容安全准确性，特别是隐式风险识别能力。


<details>
  <summary>Details</summary>
Motivation: 数字内容的指数级增长给内容安全带来重大挑战。当前基于单一模型或固定流水线的审核系统在识别隐式风险和提供可解释判断过程方面存在局限性。

Method: 提出Aetheria框架，采用五个核心智能体的协作架构，通过基于RAG知识检索的动态相互说服辩论机制，对多模态内容进行深度分析和裁决。

Result: 在提出的基准测试AIR-Bench上的综合实验验证，Aetheria不仅能生成详细可追溯的审核报告，而且在整体内容安全准确性上显著优于基线方法，特别是在隐式风险识别方面。

Conclusion: 该框架建立了一个透明且可解释的范式，显著推进了可信AI内容审核领域的发展。

Abstract: The exponential growth of digital content presents significant challenges for content safety. Current moderation systems, often based on single models or fixed pipelines, exhibit limitations in identifying implicit risks and providing interpretable judgment processes. To address these issues, we propose Aetheria, a multimodal interpretable content safety framework based on multi-agent debate and collaboration.Employing a collaborative architecture of five core agents, Aetheria conducts in-depth analysis and adjudication of multimodal content through a dynamic, mutually persuasive debate mechanism, which is grounded by RAG-based knowledge retrieval.Comprehensive experiments on our proposed benchmark (AIR-Bench) validate that Aetheria not only generates detailed and traceable audit reports but also demonstrates significant advantages over baselines in overall content safety accuracy, especially in the identification of implicit risks. This framework establishes a transparent and interpretable paradigm, significantly advancing the field of trustworthy AI content moderation.

</details>


### [24] [Target-specific Adaptation and Consistent Degradation Alignment for Cross-Domain Remaining Useful Life Prediction](https://arxiv.org/abs/2512.02610)
*Yubo Hou,Mohamed Ragab,Min Wu,Chee-Keong Kwoh,Xiaoli Li,Zhenghua Chen*

Main category: cs.AI

TL;DR: 提出TACDA方法解决跨域剩余使用寿命预测中的领域差异问题，通过目标域重构策略和聚类配对策略提升性能


<details>
  <summary>Details</summary>
Motivation: 现有数据驱动的RUL预测方法通常假设训练和测试数据来自相同分布，但实际工业环境中存在领域差异问题。先前对抗性领域自适应方法专注于提取领域不变特征，但忽略了目标特定信息和退化阶段的一致性特征，导致性能不佳。

Method: 提出TACDA方法：1）在对抗性自适应过程中引入目标域重构策略，在学习领域不变特征的同时保留目标特定信息；2）开发新颖的聚类和配对策略，实现相似退化阶段之间的一致性对齐。

Result: 通过大量实验证明，TACDA方法在两个不同的评估指标上都显著超越了现有最先进方法，表现出卓越的性能。

Conclusion: TACDA方法有效解决了跨域RUL预测中的领域差异问题，通过结合目标域重构和退化阶段一致性对齐策略，显著提升了预测性能，为实际工业应用提供了有效的解决方案。

Abstract: Accurate prediction of the Remaining Useful Life (RUL) in machinery can significantly diminish maintenance costs, enhance equipment up-time, and mitigate adverse outcomes. Data-driven RUL prediction techniques have demonstrated commendable performance. However, their efficacy often relies on the assumption that training and testing data are drawn from the same distribution or domain, which does not hold in real industrial settings. To mitigate this domain discrepancy issue, prior adversarial domain adaptation methods focused on deriving domain-invariant features. Nevertheless, they overlook target-specific information and inconsistency characteristics pertinent to the degradation stages, resulting in suboptimal performance. To tackle these issues, we propose a novel domain adaptation approach for cross-domain RUL prediction named TACDA. Specifically, we propose a target domain reconstruction strategy within the adversarial adaptation process, thereby retaining target-specific information while learning domain-invariant features. Furthermore, we develop a novel clustering and pairing strategy for consistent alignment between similar degradation stages. Through extensive experiments, our results demonstrate the remarkable performance of our proposed TACDA method, surpassing state-of-the-art approaches with regard to two different evaluation metrics. Our code is available at https://github.com/keyplay/TACDA.

</details>


### [25] [Exploring Depth Generalization in Large Language Models for Solving Recursive Logic Tasks](https://arxiv.org/abs/2512.02677)
*Zhiyuan He*

Main category: cs.AI

TL;DR: 该论文研究了大型语言模型在处理递归推理问题时的深度泛化能力不足问题，并提出了一种循环定位替换管道方法来解决这一挑战。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在许多任务上表现出色，但在处理需要解决嵌套层次结构的递归推理问题时面临显著挑战。虽然先前研究广泛探讨了长度泛化能力，但深度泛化这一独特且未被充分探索的局限性尚未得到深入研究。

Method: 提出了一种新颖的循环定位替换管道方法，将递归问题分解为可管理的子组件。该方法使用两个专门模型：定位器识别可解子表达式，替换器评估这些组件同时保持整体结构。

Result: 在布尔代数、递归算术和命题逻辑三个精心设计的领域中评估了该方法，每个领域都有可控的递归深度。结果显示该方法能有效缓解在分布外递归深度测试时的性能衰减。

Conclusion: 标准Transformer架构在处理比训练时更深的递归问题时存在困难，这源于其无法维持类似栈的行为来跟踪和解决多层嵌套依赖。提出的循环定位替换管道方法能有效解决这一深度泛化挑战。

Abstract: Large language models have demonstrated remarkable capabilities across many tasks, yet face significant challenges when dealing with recursive reasoning problems, those requiring the resolution of nested hierarchical structures. While prior research has extensively studied length generalization (a model's ability to handle longer sequences than seen during training), we investigate a distinct and underexplored limitation: depth generalization. Here, depth refers to the number of nested levels in a hierarchical problem, such as the layers of parentheses in a mathematical expression or the nesting of logical clauses in a Boolean formula. Our work reveals that standard transformer architectures struggle with problems involving deeper recursion than encountered during training, even when they perform well on longer but non-nested sequences. This limitation stems from their inability to maintain stack-like behavior, the capacity to track and resolve multiple levels of nested dependencies. Through systematic analysis, we demonstrate how this architectural constraint leads to rapid performance decay as the depth of the recursion increases. To address this challenge, we develop a novel looped locate-and-replace pipeline that decomposes recursive problems into manageable subcomponents. The approach employs two specialized models: a locator that identifies solvable subexpressions and a replacer that evaluates these components while preserving the overall structure. We evaluated this method in three carefully designed domains: Boolean algebra, recursive arithmetic, and propositional logic, each with a controllable depth of recursion. We show that our method effectively alleviates the performance decay when tested on out-of-distribution recursion depth.

</details>


### [26] [Learning What to Attend First: Modality-Importance-Guided Reasoning for Reliable Multimodal Emotion Understanding](https://arxiv.org/abs/2512.02699)
*Hyeongseop Rha,Jeong Hun Yeo,Junil Won,Se Jin Park,Yong Man Ro*

Main category: cs.AI

TL;DR: MIGR框架通过模态重要性引导推理，解决多模态情感理解中的推理漂移问题，从情感主导模态开始推理，提高解释的可靠性和一致性。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型在情感理解中存在推理漂移问题：模型逐渐依赖自身生成的文本而非多模态证据，且解释过度受视觉启动推理路径影响，导致解释与情感不一致。

Method: 提出模态重要性机制识别情感主导模态，基于此重新组织推理序列，使解释从最关键的情感模态开始。采用两阶段框架：模态对齐监督微调和模态感知奖励优化。

Result: 在DFEW基准测试中，MIGR显著提高推理可靠性，将正确预测但情感不一致解释的比例从18.10%降至7.37%。

Conclusion: 从情感主导模态开始推理能有效提高多模态情感理解的可靠性，MIGR框架为解决推理漂移问题提供了有效方案。

Abstract: In this paper, we present Modality-Importance-Guided Reasoning (MIGR), a framework designed to improve the reliability of reasoning-based multimodal emotion understanding in multimodal large language models. Although existing methods have advanced emotion understanding, they often suffer from reasoning drift: models gradually rely on their own generated text instead of multimodal evidence, and their explanations are overly shaped by visually initiated reasoning paths. To address these issues, we introduce Modality Importance (MI), a simple yet effective mechanism for identifying the emotion-dominant modality. Using MI, MIGR reorganizes reasoning sequences so that explanations begin from the modality most critical to the target emotion, preventing early reasoning from being misled by less informative cues. Our two-stage framework-comprising modality-aligned supervised fine-tuning and modality-aware reward optimization-encourages models to generate emotionally grounded, causally relevant, and coherence-preserving explanations. Experimental results on the DFEW benchmark show that MIGR substantially improves reasoning reliability, decreasing instances of correct predictions accompanied by emotionally inconsistent explanations from 18.10% to 7.37%. These results confirm the benefit of initiating reasoning from the emotion-dominant modality.

</details>


### [27] [Training Data Attribution for Image Generation using Ontology-Aligned Knowledge Graphs](https://arxiv.org/abs/2512.02713)
*Theodoros Aivalis,Iraklis A. Klampanos,Antonis Troumpoukis,Joemon M. Jose*

Main category: cs.AI

TL;DR: 提出一个基于知识图谱的生成模型输出解释框架，通过构建与本体对齐的知识图谱来追踪训练数据对生成输出的影响


<details>
  <summary>Details</summary>
Motivation: 随着生成模型能力增强，透明度、问责制和版权侵权问题日益突出，需要理解特定训练数据如何影响模型输出

Method: 利用多模态大语言模型从图像中提取结构化三元组，构建与领域特定本体对齐的知识图谱，通过比较生成图像和训练图像的KG来追踪潜在影响

Result: 通过局部训练模型的unlearning实验和大规模模型的风格特定实验验证了方法的有效性

Conclusion: 该框架支持开发促进人类协作、创造力和激发好奇心的AI系统，为版权分析、数据集透明度和可解释AI提供支持

Abstract: As generative models become powerful, concerns around transparency, accountability, and copyright violations have intensified. Understanding how specific training data contributes to a model's output is critical. We introduce a framework for interpreting generative outputs through the automatic construction of ontologyaligned knowledge graphs (KGs). While automatic KG construction from natural text has advanced, extracting structured and ontology-consistent representations from visual content remains challenging -- due to the richness and multi-object nature of images. Leveraging multimodal large language models (LLMs), our method extracts structured triples from images, aligned with a domain-specific ontology. By comparing the KGs of generated and training images, we can trace potential influences, enabling copyright analysis, dataset transparency, and interpretable AI. We validate our method through experiments on locally trained models via unlearning, and on large-scale models through a style-specific experiment. Our framework supports the development of AI systems that foster human collaboration, creativity and stimulate curiosity.

</details>


### [28] [Menta: A Small Language Model for On-Device Mental Health Prediction](https://arxiv.org/abs/2512.02716)
*Tianyi Zhang,Xiangyuan Xue,Lingyan Ruan,Shiya Fu,Feng Xia,Simon D'Alfonso,Vassilis Kostakos,Ting Dang,Hong Jia*

Main category: cs.AI

TL;DR: Menta是首个针对社交媒体多任务心理健康预测优化的轻量级语言模型，在抑郁、压力和自杀倾向分类任务上优于现有SLM和部分LLM，支持iPhone实时部署。


<details>
  <summary>Details</summary>
Motivation: 全球心理健康问题影响数亿人，但早期检测受限。虽然大语言模型在心理健康应用中有潜力，但其规模和计算需求阻碍实际部署。轻量级小语言模型是可行替代方案，但在社交媒体心理健康预测方面研究不足。

Method: 提出Menta模型，采用LoRA微调框架、跨数据集策略和平衡准确率导向的损失函数，在六个分类任务上联合训练，专门针对社交媒体数据进行心理健康预测优化。

Result: 相比9个最先进的SLM基线，Menta在抑郁、压力和自杀倾向任务上平均提升15.2%；在抑郁和压力分类任务上优于130亿参数LLM，同时模型体积小约3.25倍；可在iPhone 15 Pro Max上实时部署，仅需约3GB内存。

Conclusion: Menta展示了轻量级模型在可扩展、隐私保护的心理健康监测方面的潜力，为实际部署提供了可行的技术方案。

Abstract: Mental health conditions affect hundreds of millions globally, yet early detection remains limited. While large language models (LLMs) have shown promise in mental health applications, their size and computational demands hinder practical deployment. Small language models (SLMs) offer a lightweight alternative, but their use for social media--based mental health prediction remains largely underexplored. In this study, we introduce Menta, the first optimized SLM fine-tuned specifically for multi-task mental health prediction from social media data. Menta is jointly trained across six classification tasks using a LoRA-based framework, a cross-dataset strategy, and a balanced accuracy--oriented loss. Evaluated against nine state-of-the-art SLM baselines, Menta achieves an average improvement of 15.2\% across tasks covering depression, stress, and suicidality compared with the best-performing non--fine-tuned SLMs. It also achieves higher accuracy on depression and stress classification tasks compared to 13B-parameter LLMs, while being approximately 3.25x smaller. Moreover, we demonstrate real-time, on-device deployment of Menta on an iPhone 15 Pro Max, requiring only approximately 3GB RAM. Supported by a comprehensive benchmark against existing SLMs and LLMs, Menta highlights the potential for scalable, privacy-preserving mental health monitoring. Code is available at: https://xxue752-nz.github.io/menta-project/

</details>


### [29] [StockMem: An Event-Reflection Memory Framework for Stock Forecasting](https://arxiv.org/abs/2512.02720)
*He Wang,Wenyilin Xiao,Songqiao Han,Hailiang Huang*

Main category: cs.AI

TL;DR: StockMem：一个事件-反思双层内存框架，通过结构化新闻事件、追踪事件演化和分析事件-价格动态，提升股价预测的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 股价预测面临市场波动性和实时事件敏感性的挑战。虽然大语言模型为基于文本的预测提供了新途径，但在金融领域的应用受到噪声新闻数据和文本中缺乏明确答案的限制。通用内存架构难以识别价格变动的关键驱动因素。

Method: 提出StockMem事件-反思双层内存框架：1）将新闻结构化到事件中，从两个维度挖掘：横向整合集成每日事件，纵向追踪捕捉事件演化以提取反映市场预期差异的增量信息，构建时序事件知识库；2）通过分析事件-价格动态，形成因果经验的反思知识库；3）预测时检索类似历史场景，结合当前事件、增量数据和过往经验进行推理。

Result: 实验表明StockMem优于现有内存架构，通过追踪影响价格的信息链提供更优、可解释的推理，增强了金融预测的决策透明度。

Conclusion: StockMem框架通过结构化事件知识、追踪事件演化和分析因果经验，有效解决了金融预测中的噪声数据和关键驱动因素识别问题，提高了预测性能和可解释性。

Abstract: Stock price prediction is challenging due to market volatility and its sensitivity to real-time events. While large language models (LLMs) offer new avenues for text-based forecasting, their application in finance is hindered by noisy news data and the lack of explicit answers in text. General-purpose memory architectures struggle to identify the key drivers of price movements. To address this, we propose StockMem, an event-reflection dual-layer memory framework. It structures news into events and mines them along two dimensions: horizontal consolidation integrates daily events, while longitudinal tracking captures event evolution to extract incremental information reflecting market expectation discrepancies. This builds a temporal event knowledge base. By analyzing event-price dynamics, the framework further forms a reflection knowledge base of causal experiences. For prediction, it retrieves analogous historical scenarios and reasons with current events, incremental data, and past experiences. Experiments show StockMem outperforms existing memory architectures and provides superior, explainable reasoning by tracing the information chain affecting prices, enhancing decision transparency in financial forecasting.

</details>


### [30] [AuditCopilot: Leveraging LLMs for Fraud Detection in Double-Entry Bookkeeping](https://arxiv.org/abs/2512.02726)
*Md Abdul Kadir,Sai Suresh Macharla Vasu,Sidharth S. Nair,Daniel Sonntag*

Main category: cs.AI

TL;DR: LLMs在复式记账异常检测中超越传统规则方法和机器学习基线，提供自然语言解释，支持AI增强审计


<details>
  <summary>Details</summary>
Motivation: 传统日记账测试产生大量误报且难以检测细微异常，需要更有效的异常检测方法

Method: 在合成和真实匿名账本上对LLaMA、Gemma等最先进LLMs进行基准测试，与传统JETs和机器学习基线对比

Result: LLMs在异常检测方面持续优于传统规则方法和经典机器学习基线，同时提供增强可解释性的自然语言解释

Conclusion: LLMs展示了AI增强审计的潜力，人类审计师与基础模型协作可加强财务完整性

Abstract: Auditors rely on Journal Entry Tests (JETs) to detect anomalies in tax-related ledger records, but rule-based methods generate overwhelming false positives and struggle with subtle irregularities. We investigate whether large language models (LLMs) can serve as anomaly detectors in double-entry bookkeeping. Benchmarking SoTA LLMs such as LLaMA and Gemma on both synthetic and real-world anonymized ledgers, we compare them against JETs and machine learning baselines. Our results show that LLMs consistently outperform traditional rule-based JETs and classical ML baselines, while also providing natural-language explanations that enhance interpretability. These results highlight the potential of \textbf{AI-augmented auditing}, where human auditors collaborate with foundation models to strengthen financial integrity.

</details>


### [31] [Self-Improving AI Agents through Self-Play](https://arxiv.org/abs/2512.02731)
*Przemyslaw Chojecki*

Main category: cs.AI

TL;DR: 该论文将心理测量电池的模数理论框架扩展到动力系统领域，将智能体形式化为由计算资源参数化的流，并推导出保证自我改进稳定性的方差不等式条件。


<details>
  <summary>Details</summary>
Motivation: 先前的研究将AAI能力分数建立为智能体表示空间上的静态泛函，但缺乏对智能体作为动态系统的形式化描述。本文旨在将智能体形式化为由计算资源参数化的动力系统，以统一理解语言自我博弈、自我纠正和合成数据引导等不同架构。

Method: 提出生成器-验证器-更新器（GVU）算子，该算子在参数流形Θ上生成向量场。将自我改进系数κ定义为能力泛函沿该流的李导数。推导出方差不等式这一谱条件，该条件在温和正则性假设下足以保证自我改进的稳定性。

Result: 证明了GVU算子在参数流形上生成向量场，并识别出自我改进系数κ。推导出方差不等式作为自我改进稳定性的充分条件：生成和验证的组合噪声必须足够小（考虑曲率和步长效应）。将STaR、SPIN、Reflexion、GANs和AlphaZero等架构统一为满足方差不等式的GVU算子的具体拓扑实现。

Conclusion: 该研究为理解智能体自我改进的动态过程提供了统一的数学框架，将多种现有架构统一为GVU算子的具体实现，并通过方差不等式为自我改进的稳定性提供了理论保证。

Abstract: We extend the moduli-theoretic framework of psychometric batteries to the domain of dynamical systems. While previous work established the AAI capability score as a static functional on the space of agent representations, this paper formalizes the agent as a flow $ν_r$ parameterized by computational resource $r$, governed by a recursive Generator-Verifier-Updater (GVU) operator. We prove that this operator generates a vector field on the parameter manifold $Θ$, and we identify the coefficient of self-improvement $κ$ as the Lie derivative of the capability functional along this flow.
  The central contribution of this work is the derivation of the Variance Inequality, a spectral condition that is sufficient (under mild regularity) for the stability of self-improvement. We show that a sufficient condition for $κ> 0$ is that, up to curvature and step-size effects, the combined noise of generation and verification must be small enough.
  We then apply this formalism to unify the recent literature on Language Self-Play (LSP), Self-Correction, and Synthetic Data bootstrapping. We demonstrate that architectures such as STaR, SPIN, Reflexion, GANs and AlphaZero are specific topological realizations of the GVU operator that satisfy the Variance Inequality through filtration, adversarial discrimination, or grounding in formal systems.

</details>


### [32] [A Framework for Causal Concept-based Model Explanations](https://arxiv.org/abs/2512.02735)
*Anna Rodum Bjøru,Jacob Lysnæs-Larsen,Oskar Jørgensen,Inga Strümke,Helge Langseth*

Main category: cs.AI

TL;DR: 提出基于因果概念的XAI框架，通过概念干预的充分概率生成局部和全局解释，在CelebA数据集上验证，强调解释的可理解性和对模型的忠实性


<details>
  <summary>Details</summary>
Motivation: 针对非可解释模型的解释需求，需要既能让人类理解，又能忠实反映模型行为的解释方法。现有方法往往难以同时满足可理解性和忠实性这两个关键要求。

Method: 提出基于因果概念的后处理XAI框架，通过计算概念干预的充分概率来生成局部和全局解释。使用概念词汇表构建解释，并在CelebA数据集上训练分类器进行概念验证。

Result: 在CelebA数据集上展示了示例解释，证明了框架的可理解性（通过清晰的概念词汇表）和忠实性（通过强调解释生成与解释解释的上下文一致性）。

Conclusion: 该因果概念框架为XAI提供了一种平衡可理解性和忠实性的方法，但需要确保解释生成和解释解释的上下文对齐，这是实现高保真解释的关键。

Abstract: This work presents a conceptual framework for causal concept-based post-hoc Explainable Artificial Intelligence (XAI), based on the requirements that explanations for non-interpretable models should be understandable as well as faithful to the model being explained. Local and global explanations are generated by calculating the probability of sufficiency of concept interventions. Example explanations are presented, generated with a proof-of-concept model made to explain classifiers trained on the CelebA dataset. Understandability is demonstrated through a clear concept-based vocabulary, subject to an implicit causal interpretation. Fidelity is addressed by highlighting important framework assumptions, stressing that the context of explanation interpretation must align with the context of explanation generation.

</details>


### [33] [Enhancing Automated Paper Reproduction via Prompt-Free Collaborative Agents](https://arxiv.org/abs/2512.02812)
*Zijie Lin,Qilin Cai,Liang Shen,Mingjun Xiao*

Main category: cs.AI

TL;DR: 提出了一种无需人工提示的协作智能体框架，通过验证和精炼两个智能体自动提升论文到代码生成的准确性和完整性，相比基线方法性能提升约15%和13%。


<details>
  <summary>Details</summary>
Motivation: 现有自动化论文复现框架缺乏对每个生成步骤输出的验证和精炼机制，或过度依赖人工设计的提示进行自我精炼，限制了适应性和可扩展性。

Method: 提出基于两个协作智能体的框架：验证智能体检查每个步骤输出是否满足系统提示要求，精炼智能体根据识别的问题修订输出。该方法仅利用原始系统提示实现自动验证和改进，无需人工设计特定精炼提示。

Result: 在PaperBench Code-Dev和Paper2CodeBench数据集上的实验表明，该方法显著提高了复现代码的准确性和完整性，相比无智能体的基线分别获得约15%和13%的性能提升。与Self-Refine的对比实验验证了该无提示方法在不同数据集上的鲁棒性和一致性。

Conclusion: 提出的无提示协作智能体框架有效解决了现有论文到代码生成方法的局限性，通过自动验证和精炼机制显著提升了复现代码质量，为自动化科学研究提供了更可靠和可扩展的解决方案。

Abstract: Automated paper reproduction has emerged as a promising approach to accelerate scientific research, employing multi-step workflow frameworks to systematically convert academic papers into executable code. However, existing frameworks often lack mechanisms to verify and refine the outputs at each generation step, or rely heavily on manually designed prompts for self-refinement, which limits their adaptability and scalability. To address these limitations, we propose a prompt-free collaborative agent framework that automatically enhances the quality of paper-to-code generation. Our approach employs two collaborative agents: a verification agent that examines whether the outputs at each step satisfy the requirements specified in the corresponding system prompt, and a refinement agent that revises the outputs based on the identified issues. Unlike previous methods that require human experts to craft specific refinement prompts for each step, our framework achieves automatic verification and improvement by leveraging only the original system prompts. We integrate our collaborative agents into the Paper2Code framework and conduct comprehensive experiments on PaperBench Code-Dev and Paper2CodeBench datasets. Experimental results demonstrate that our approach significantly improves the accuracy and completeness of reproduced code, achieving performance gains of approximately 15\% and 13\%, respectively, compared to the baseline without our agents. Furthermore, comparative experiments against Self-Refine validate the robustness and consistency of our prompt-free approach across different datasets.

</details>


### [34] [The future of AI in critical mineral exploration](https://arxiv.org/abs/2512.02879)
*Jef Caers*

Main category: cs.AI

TL;DR: 本文提出了一种基于贝叶斯主义和证伪原则的AI驱动科学方法，用于解决关键矿产勘探中认知偏见和假阳性问题，通过无监督学习和人机协作优化数据采集决策。


<details>
  <summary>Details</summary>
Motivation: 尽管能源转型推动了对关键矿产的需求增加和投资增长，但过去二十年新发现却减少，需要解决勘探中的认知偏见和假阳性问题，降低勘探成本。

Method: 提出基于贝叶斯主义和证伪原则的新科学方法：1）将数据采集视为证伪人类假设的手段；2）使用可验证指标量化下一步数据采集决策；3）采用无监督学习与领域专家协作生成多个竞争性地质假设；4）人机循环AI算法优化地质、地球物理、地球化学和钻探数据采集规划。

Result: 提供了一个实用的勘探协议模板，通过AI减少认知偏见和假阳性，使数据采集决策更加理性，优先降低地质假设的不确定性，然后才是品位和储量不确定性。

Conclusion: AI是实现矿产勘探科学方法的关键使能技术，通过结合贝叶斯推理、证伪原则和人机协作，可以显著提高勘探效率，解决当前勘探发现减少的问题。

Abstract: The energy transition through increased electrification has put the worlds attention on critical mineral exploration Even with increased investments a decrease in new discoveries has taken place over the last two decades Here I propose a solution to this problem where AI is implemented as the enabler of a rigorous scientific method for mineral exploration that aims to reduce cognitive bias and false positives drive down the cost of exploration I propose a new scientific method that is based on a philosophical approach founded on the principles of Bayesianism and falsification In this approach data acquisition is in the first place seen as a means to falsify human generated hypothesis Decision of what data to acquire next is quantified with verifiable metrics and based on rational decision making A practical protocol is provided that can be used as a template in any exploration campaign However in order to make this protocol practical various form of artificial intelligence are needed I will argue that the most important form are one novel unsupervised learning methods that collaborate with domain experts to better understand data and generate multiple competing geological hypotheses and two humanintheloop AI algorithms that can optimally plan various geological geophysical geochemical and drilling data acquisition where uncertainty reduction of geological hypothesis precedes the uncertainty reduction on grade and tonnage

</details>


### [35] [Invasive Context Engineering to Control Large Language Models](https://arxiv.org/abs/2512.03001)
*Thomas Rivasseau*

Main category: cs.AI

TL;DR: 提出侵入式上下文工程方法，通过在LLM上下文中插入控制语句来增强长上下文场景下的安全性，防止越狱和恶意行为


<details>
  <summary>Details</summary>
Motivation: 当前LLM安全方法（偏好训练、提示工程、输入输出过滤）虽然有效，但在长上下文场景下越狱概率增加，需要更鲁棒的安全保障机制

Method: 侵入式上下文工程：在LLM上下文中插入控制语句作为安全约束，该方法可推广到思维链过程以防止策略性欺骗，不依赖模型训练

Result: 该方法部分解决了长上下文场景下的LLM安全问题，避免了训练数据不足的问题，为长上下文安全提供了新思路

Conclusion: 侵入式上下文工程是一种有效的LLM安全增强技术，特别适用于长上下文场景，通过上下文控制而非模型训练来提升安全性

Abstract: Current research on operator control of Large Language Models improves model robustness against adversarial attacks and misbehavior by training on preference examples, prompting, and input/output filtering. Despite good results, LLMs remain susceptible to abuse, and jailbreak probability increases with context length. There is a need for robust LLM security guarantees in long-context situations. We propose control sentences inserted into the LLM context as invasive context engineering to partially solve the problem. We suggest this technique can be generalized to the Chain-of-Thought process to prevent scheming. Invasive Context Engineering does not rely on LLM training, avoiding data shortage pitfalls which arise in training models for long context situations.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [36] [Superpixel Attack: Enhancing Black-box Adversarial Attack with Image-driven Division Areas](https://arxiv.org/abs/2512.02062)
*Issa Oe,Keiichiro Yamamura,Hiroki Ishikura,Ryo Hamahira,Katsuki Fujisawa*

Main category: cs.CR

TL;DR: 提出Superpixel Attack方法，使用超像素替代简单矩形区域，结合通用搜索策略，在黑盒对抗攻击中平均提升2.10%的攻击成功率


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在安全关键任务中应用广泛，但对微小扰动敏感。需要更强大的黑盒对抗攻击方法来开发更有效的防御机制。现有攻击方法使用简单矩形区域进行扰动修改，效果有限。

Method: 提出Superpixel Attack方法：1) 使用超像素替代简单矩形区域，在颜色方差和紧凑性之间取得更好平衡；2) 提出通用搜索策略；3) 结合超像素和通用搜索进行攻击

Result: Superpixel Attack相比现有攻击方法平均提升2.10%的攻击成功率。该改进对于对抗攻击鲁棒的模型来说具有显著意义。代码已开源。

Conclusion: 使用超像素区域和通用搜索策略的Superpixel Attack方法能够有效提升黑盒对抗攻击的成功率，为开发更强大的防御机制提供了有力工具。

Abstract: Deep learning models are used in safety-critical tasks such as automated driving and face recognition. However, small perturbations in the model input can significantly change the predictions. Adversarial attacks are used to identify small perturbations that can lead to misclassifications. More powerful black-box adversarial attacks are required to develop more effective defenses. A promising approach to black-box adversarial attacks is to repeat the process of extracting a specific image area and changing the perturbations added to it. Existing attacks adopt simple rectangles as the areas where perturbations are changed in a single iteration. We propose applying superpixels instead, which achieve a good balance between color variance and compactness. We also propose a new search method, versatile search, and a novel attack method, Superpixel Attack, which applies superpixels and performs versatile search. Superpixel Attack improves attack success rates by an average of 2.10% compared with existing attacks. Most models used in this study are robust against adversarial attacks, and this improvement is significant for black-box adversarial attacks. The code is avilable at https://github.com/oe1307/SuperpixelAttack.git.

</details>


### [37] [Large Language Model based Smart Contract Auditing with LLMBugScanner](https://arxiv.org/abs/2512.02069)
*Yining Yuan,Yifei Wang,Yichang Xu,Zachary Yahn,Sihao Hu,Ling Liu*

Main category: cs.CR

TL;DR: LLMBugScanner是一个基于大语言模型的智能合约漏洞检测框架，通过微调和集成学习提升检测效果。


<details>
  <summary>Details</summary>
Motivation: 智能合约审计面临挑战：不同预训练模型推理能力各异，单一模型无法在所有漏洞类型或合约结构上表现一致，即使微调后仍有局限性。

Method: 结合领域知识适应和集成推理：1）领域知识适应：在互补数据集上微调LLMs，捕捉通用代码语义和指令引导的漏洞推理，使用参数高效调优降低计算成本；2）集成推理：利用多个LLMs的互补优势，应用基于共识的冲突解决策略。

Result: 在多个流行LLMs上进行广泛实验，LLMBugScanner相比预训练和微调的单个模型，实现了持续准确率提升和更强的泛化能力。

Conclusion: LLMBugScanner为智能合约审计提供了一个原则性、成本效益高且可扩展的框架。

Abstract: This paper presents LLMBugScanner, a large language model (LLM) based framework for smart contract vulnerability detection using fine-tuning and ensemble learning. Smart contract auditing presents several challenges for LLMs: different pretrained models exhibit varying reasoning abilities, and no single model performs consistently well across all vulnerability types or contract structures. These limitations persist even after fine-tuning individual LLMs.
  To address these challenges, LLMBugScanner combines domain knowledge adaptation with ensemble reasoning to improve robustness and generalization. Through domain knowledge adaptation, we fine-tune LLMs on complementary datasets to capture both general code semantics and instruction-guided vulnerability reasoning, using parameter-efficient tuning to reduce computational cost. Through ensemble reasoning, we leverage the complementary strengths of multiple LLMs and apply a consensus-based conflict resolution strategy to produce more reliable vulnerability assessments.
  We conduct extensive experiments across multiple popular LLMs and compare LLMBugScanner with both pretrained and fine-tuned individual models. Results show that LLMBugScanner achieves consistent accuracy improvements and stronger generalization, demonstrating that it provides a principled, cost-effective, and extensible framework for smart contract auditing.

</details>


### [38] [Deterministic Random Bit Generators Based on Ascon for Embedded Systems](https://arxiv.org/abs/2512.02082)
*Abel C. H. Chen*

Main category: cs.CR

TL;DR: 本研究基于NIST新发布的Ascon轻量级密码标准，提出了三种Ascon驱动的DRBG构造方案，在树莓派平台上验证了其在嵌入式系统中具有更高的计算效率和更低的内存占用。


<details>
  <summary>Details</summary>
Motivation: DRBG作为随机数生成和密码应用的基础组件，在资源受限的嵌入式系统中性能和安全性尤为关键。NIST在2025年8月建立了适用于资源受限嵌入式环境的Ascon轻量级密码标准，因此需要基于新标准设计高性能、安全的DRBG。

Method: 修订DRBG标准，提出了三种Ascon驱动的构造方案：Ascon驱动的基于哈希的DRBG、Ascon驱动的基于HMAC的DRBG、以及Ascon驱动的计数器模式(CTR) DRBG。在树莓派平台上实现了这些方法。

Result: 实验结果表明，与现有的DRBG实现相比，所提出的方法实现了更高的计算效率和更低的内存使用，适合在嵌入式系统中部署。

Conclusion: 基于Ascon轻量级密码标准设计的三种DRBG构造方案在嵌入式系统中表现出优越的性能和资源效率，为资源受限环境提供了安全可靠的随机数生成解决方案。

Abstract: As the Deterministic Random Bit Generator (DRBG) serves as a fundamental component in random number generation and cryptographic applications, its performance and security are particularly critical in resource-constrained embedded systems, where memory capacity and computational efficiency are limited. Establishing a high-performance and secure DRBG is therefore an important issue for embedded system design. Furthermore, the National Institute of Standards and Technology (NIST) established the Ascon lightweight cryptographic standard in August 2025, which is suitable for use in resource-limited embedded environments. Therefore, this study revises the DRBG standard and proposes three Ascon-driven constructions: the Ascon-Driven Hash-Based DRBG, the Ascon-Driven keyed-Hash Message Authentication Code (HMAC) DRBG, and the Ascon-Driven Counter-mode (CTR) DRBG. In the experiments, these methods are implemented on a Raspberry Pi platform. The experimental results demonstrate that the proposed approaches achieve higher computational efficiency and lower memory usage compared with existing DRBG implementations, making them suitable for deployment in embedded systems.

</details>


### [39] [A survey about Hidden Subgroup Problem from a mathematical and cryptographic perspective](https://arxiv.org/abs/2512.02087)
*Simone Dutto,Pietro Mercuri,Nadir Murru,Lorenzo Romano*

Main category: cs.CR

TL;DR: 该论文对隐藏子群问题（HSP）进行了综述，重点分析了其在公钥密码系统安全性研究中的重要性，涵盖了阿贝尔和非阿贝尔情况下的算法进展和应用。


<details>
  <summary>Details</summary>
Motivation: 隐藏子群问题在量子计算和密码学中具有重要意义，特别是对于评估公钥密码系统的安全性。研究HSP有助于理解哪些密码原语可能受到量子攻击的威胁，以及哪些可能保持安全。

Method: 采用综述研究方法，首先回顾阿贝尔情况下的Kitaev算法，然后分析非阿贝尔HSP的当前研究现状，重点关注与密码学相关的特定群结构，包括二面体群、对称群和半直积构造。

Result: 阿贝尔HSP已有高效的量子算法解决（如Kitaev算法），可应用于整数分解、离散对数等经典问题；而非阿贝尔HSP尚无通用的高效量子算法，但对特定群结构的研究揭示了与最短向量问题、图同构问题和码等价问题的联系。

Conclusion: 隐藏子群问题是连接量子算法和密码安全性的关键桥梁，阿贝尔情况已得到较好解决，但非阿贝尔情况仍是开放问题，其研究对于评估后量子密码安全性具有重要意义。

Abstract: We provide a survey on the Hidden Subgroup Problem (HSP), which plays an important role in studying the security of public-key cryptosystems. We first review the abelian case, where Kitaev's algorithm yields an efficient quantum solution to the HSP, recalling how classical problems (such as order finding, integer factorization, and discrete logarithm) can be formulated as abelian HSP instances. We then examine the current state of the art for non-abelian HSP, where no general efficient quantum solution is known, focusing on some relevant groups including dihedral group (connected to the shortest vector problem), symmetric groups (connected to the graph isomorphism problem), and semidirect product constructions (connected, in a special case, to the code equivalence problem). We also describe the main techniques for addressing the HSP in non-abelian cases, namely Fourier sampling and the black-box approach. Throughout the paper, we highlight the mathematical notions required and exploited in this context, providing a cryptography-oriented perspective.

</details>


### [40] [Factor(T,U): Factored Cognition Strengthens Monitoring of Untrusted AI](https://arxiv.org/abs/2512.02157)
*Aaron Sandoval,Cody Rushing*

Main category: cs.CR

TL;DR: 该研究开发了基于分解认知的AI控制协议，通过可信模型分解任务、不可信模型独立解决子任务的方式提升AI安全性，在APPS编码环境中测试显示安全性能从41%提升到63%。


<details>
  <summary>Details</summary>
Motivation: 现有AI控制协议依赖较弱监控器检测不安全行为，但在复杂任务中往往失效，因为监控器无法理解这些复杂任务。需要开发更有效的控制协议来应对可能故意颠覆的不可信AI。

Method: 开发基于分解认知的控制协议：1）可信模型将任务分解为子任务；2）不可信模型独立解决每个子任务；3）结果重新组装成完整解决方案。在APPS编码环境中实现协议，并使用对抗性GPT-4.1 Nano进行红队测试。

Result: 1）在可信监控协议中加入分解认知可将安全性从41%提升到63%；2）安全性提升主要因为监控器性能改善；3）分解认知不会使能力强的LLM在APPS中更难编写后门；4）协议在APPS中实用性较低，但在更复杂任务中具有潜力。

Conclusion: 分解认知控制协议能有效提升AI安全性，特别是在监控器性能改善方面。虽然当前在APPS环境中实用性有限，但在更复杂的任务中具有应用前景，为AI控制提供了新的技术路径。

Abstract: The field of AI Control seeks to develop robust control protocols, deployment safeguards for untrusted AI which may be intentionally subversive. However, existing protocols that rely on weaker monitors to detect unsafe behavior often fail on complex tasks beyond the monitor's comprehension. We develop control protocols based on factored cognition, in which a trusted model decomposes a task, an untrusted model solves each resultant child task in isolation, and the results are reassembled into a full solution. These protocols may improve safety by several means, such as by simplifying the context for monitors, or by obscuring vulnerabilities in the environment. We implement our protocols in the APPS coding setting and red team them against backdoor attempts from an adversarial GPT-4.1 Nano. We find that: (i) Adding factored cognition to a trusted monitoring protocol can boost safety from 41% to 63%; (ii) Safety improves because monitor performance improves; (iii) Factored cognition makes it no harder for capable LLMs to write backdoors in APPS. While our protocols show low usefulness in APPS, they hold promise for more complex tasks.

</details>


### [41] [PhishSnap: Image-Based Phishing Detection Using Perceptual Hashing](https://arxiv.org/abs/2512.02243)
*Md Abdul Ahad Minhaz,Zannatul Zahan Meem,Md. Shohrab Hossain*

Main category: cs.CR

TL;DR: PhishSnap是一个基于感知哈希的隐私保护钓鱼检测系统，通过浏览器扩展捕获网页截图并与合法模板进行视觉相似性比对，在本地完成检测以保护用户隐私。


<details>
  <summary>Details</summary>
Motivation: 钓鱼攻击利用人类信任获取敏感凭证，现有基于URL和HTML的检测系统难以应对混淆和视觉欺骗技术，需要更有效的检测方法。

Method: 开发了PhishSnap浏览器扩展系统，使用感知哈希技术：捕获网页截图，计算视觉哈希值，与合法模板进行相似性比对，整个推理过程在设备本地完成。

Result: 使用2024年收集的10,000个URL数据集（70%/20%/10%训练/验证/测试），由于安全清除导致部分钓鱼页面不可用，系统达到0.79准确率、0.76精确率和0.78召回率。

Conclusion: 视觉相似性仍然是可行的反钓鱼措施，本地化处理确保了用户隐私和低延迟，为隐私保护的钓鱼检测提供了有效解决方案。

Abstract: Phishing remains one of the most prevalent online threats, exploiting human trust to harvest sensitive credentials. Existing URL- and HTML-based detection systems struggle against obfuscation and visual deception. This paper presents \textbf{PhishSnap}, a privacy-preserving, on-device phishing detection system leveraging perceptual hashing (pHash). Implemented as a browser extension, PhishSnap captures webpage screenshots, computes visual hashes, and compares them against legitimate templates to identify visually similar phishing attempts. A \textbf{2024 dataset of 10,000 URLs} (70\%/20\%/10\% train/validation/test) was collected from PhishTank and Netcraft. Due to security takedowns, a subset of phishing pages was unavailable, reducing dataset diversity. The system achieved \textbf{0.79 accuracy}, \textbf{0.76 precision}, and \textbf{0.78 recall}, showing that visual similarity remains a viable anti-phishing measure. The entire inference process occurs locally, ensuring user privacy and minimal latency.

</details>


### [42] [CVE Breadcrumbs: Tracking Vulnerabilities Through Versioned Apache Libraries](https://arxiv.org/abs/2512.02259)
*Derek Garcia,Briana Lee,Ibrahim Matar,David Rickards,Andrew Zilnicki*

Main category: cs.CR

TL;DR: 对Apache软件基金会生态系统进行历史安全漏洞分析，收集了24,285个Apache库、1,285个CVE和157个CWE数据，研究漏洞生命周期、披露时间和修复实践。


<details>
  <summary>Details</summary>
Motivation: Apache生态系统支撑着现代软件基础设施的重要部分（如Log4j、Tomcat、Struts），但其广泛使用使其成为高影响安全漏洞的主要目标。尽管广泛采用，Apache项目仍面临重复且严重的安全弱点，需要系统性分析来改善安全实践。

Method: 通过编译全面的CVE和CWE数据集，对Apache生态系统进行历史分析。研究四个关键问题：最持久和重复的CWE、CVE修复前持续时间、CVE引入与官方披露之间的延迟、披露后修复时间。分析漏洞生命周期阶段的时间线。

Result: 创建了包含24,285个Apache库、1,285个CVEs和157个CWEs的精选数据集。提供了Apache库中漏洞生命周期的详细时间线分析，包括漏洞持久性、披露延迟和修复时间等实证发现。

Conclusion: 研究提供了对Apache生态系统安全漏洞模式的深入理解，提出了改善安全编码实践、漏洞监控和修复策略的见解和建议，有助于开发者更好地管理安全风险。

Abstract: The Apache Software Foundation (ASF) ecosystem underpins a vast portion of modern software infrastructure, powering widely used components such as Log4j, Tomcat, and Struts. However, the ubiquity of these libraries has made them prime targets for high-impact security vulnerabilities, as illustrated by incidents like Log4Shell. Despite their widespread adoption, Apache projects are not immune to recurring and severe security weaknesses. We conduct a historical analysis of the Apache ecosystem to follow the "breadcrumb trail of vulnerabilities" by compiling a comprehensive dataset of Common Vulnerabilities and Exposures (CVEs) and Common Weakness Enumerations (CWEs). We examine trends in exploit recurrence, disclosure timelines, and remediation practices. Our analysis is guided by four key research questions: (1) What are the most persistent and repeated CWEs in Apache libraries? (2) How long do CVEs persist before being addressed? (3) What is the delay between CVE introduction and official disclosure? and (4) How long after disclosure are CVEs remediated? We present a detailed timeline of vulnerability lifecycle stages across Apache libraries and offer insights to improve secure coding practices, vulnerability monitoring, and remediation strategies. Our contributions include a curated dataset covering 24,285 Apache libraries, 1,285 CVEs, and 157 CWEs, along with empirical findings and developer-focused recommendations.

</details>


### [43] [COGNITION: From Evaluation to Defense against Multimodal LLM CAPTCHA Solvers](https://arxiv.org/abs/2512.02318)
*Junyu Wang,Changjia Zhu,Yuanbo Zhou,Lingyao Li,Xu He,Junjie Xiong*

Main category: cs.CR

TL;DR: 多模态大语言模型能够以接近人类的成本和延迟可靠地解决识别导向和低交互验证码任务，但精细定位、多步空间推理或跨帧一致性的任务对当前模型仍然困难。


<details>
  <summary>Details</summary>
Motivation: 研究多模态大语言模型如何削弱视觉验证码的安全保证，识别攻击面，评估现有模型在实际验证码任务中的表现。

Method: 评估7个领先的商业和开源MLLM模型，覆盖18种真实世界验证码任务类型，测量单次准确率、有限重试成功率、端到端延迟和每次解决成本，分析任务特定提示工程和少样本演示的影响。

Result: MLLM能够以类似人类的成本和延迟可靠解决识别导向和低交互验证码任务，但需要精细定位、多步空间推理或跨帧一致性的任务对当前模型仍然显著困难。

Conclusion: 通过分析模型推理轨迹，研究模型在特定验证码谜题上成功/失败的根本机制，并基于这些洞察提出防御导向的指南，用于选择和加强验证码任务，讨论平台运营商在滥用缓解流程中部署验证码的影响。

Abstract: This paper studies how multimodal large language models (MLLMs) undermine the security guarantees of visual CAPTCHA. We identify the attack surface where an adversary can cheaply automate CAPTCHA solving using off-the-shelf models. We evaluate 7 leading commercial and open-source MLLMs across 18 real-world CAPTCHA task types, measuring single-shot accuracy, success under limited retries, end-to-end latency, and per-solve cost. We further analyze the impact of task-specific prompt engineering and few-shot demonstrations on solver effectiveness. We reveal that MLLMs can reliably solve recognition-oriented and low-interaction CAPTCHA tasks at human-like cost and latency, whereas tasks requiring fine-grained localization, multi-step spatial reasoning, or cross-frame consistency remain significantly harder for current models. By examining the reasoning traces of such MLLMs, we investigate the underlying mechanisms of why models succeed/fail on specific CAPTCHA puzzles and use these insights to derive defense-oriented guidelines for selecting and strengthening CAPTCHA tasks. We conclude by discussing implications for platform operators deploying CAPTCHA as part of their abuse-mitigation pipeline.Code Availability (https://anonymous.4open.science/r/Captcha-465E/).

</details>


### [44] [Leveraging Large Language Models to Bridge On-chain and Off-chain Transparency in Stablecoins](https://arxiv.org/abs/2512.02418)
*Yuexin Xiang,Yuchen Lei,SM Mahir Shazeed Rish,Yuanzhe Zhang,Qin Wang,Tsz Hon Yuen,Jiangshan Yu*

Main category: cs.CR

TL;DR: 本文提出基于大语言模型的自动化框架，通过将链上发行数据与链下披露声明对齐，解决稳定币透明度问题，揭示披露数据与可验证数据之间的系统性差距。


<details>
  <summary>Details</summary>
Motivation: 稳定币（如USDT、USDC）通过发行控制和储备证明来维持价格稳定，但透明度被分割为可验证的链上数据和不可连接的链下非结构化文本披露，需要桥接这两个维度。

Method: 1. 使用LLM通过文档解析和语义对齐捕获分析链上链下数据，从发行人证明中提取关键财务指标并映射到相应链上指标；2. 在多链发行记录和披露文档中集成模型上下文协议框架，标准化LLM对定量市场数据和定性披露叙述的访问；3. 展示LLM在区块链分析中跨异构数据模态的能力。

Result: 揭示了披露数据与可验证数据之间的系统性差距，表明LLM辅助分析增强了跨模态透明度，支持去中心化金融中的自动化、数据驱动的审计。

Conclusion: LLM能够有效桥接稳定币的链上链下透明度鸿沟，通过自动化框架增强跨模态透明度分析，为DeFi提供数据驱动的审计支持。

Abstract: Stablecoins such as USDT and USDC aspire to peg stability by coupling issuance controls with reserve attestations. In practice, however, the transparency is split across two worlds: verifiable on-chain traces and off-chain disclosures locked in unstructured text that are unconnected. We introduce a large language model (LLM)-based automated framework that bridges these two dimensions by aligning on-chain issuance data with off-chain disclosure statements. First, we propose an integrative framework using LLMs to capture and analyze on- and off-chain data through document parsing and semantic alignment, extracting key financial indicators from issuer attestations and mapping them to corresponding on-chain metrics. Second, we integrate multi-chain issuance records and disclosure documents within a model context protocol (MCP) framework that standardizes LLMs access to both quantitative market data and qualitative disclosure narratives. This framework enables unified retrieval and contextual alignment across heterogeneous stablecoin information sources and facilitates consistent analysis. Third, we demonstrate the capability of LLMs to operate across heterogeneous data modalities in blockchain analytics, quantifying discrepancies between reported and observed circulation and examining their implications for cross-chain transparency and price dynamics. Our findings reveal systematic gaps between disclosed and verifiable data, showing that LLM-assisted analysis enhances cross-modal transparency and supports automated, data-driven auditing in decentralized finance (DeFi).

</details>


### [45] [Equilibrium SAT based PQC: New aegis against quantum computing](https://arxiv.org/abs/2512.02598)
*Keum-Bae Cho*

Main category: cs.CR

TL;DR: 提出一种基于多重集随机子集元素计数的新型公钥密码算法，具有速度快、无需大数运算、可扩展为单公钥多私钥系统且保持抗量子性的优势。


<details>
  <summary>Details</summary>
Motivation: 随着互联网和量子计算的发展，传统公钥密码算法计算复杂度不断增加。需要开发更高效、抗量子且适用于各种设备的公钥密码方案。

Method: 通过从多重集中随机提取子集并计算元素数量来生成密文，使用可满足性问题进行数学精炼，构建基于单公钥多私钥的密码系统。

Result: 算法比其他公钥算法显著更快，无需大数运算可在任何设备上执行，可扩展为单公钥多私钥系统并保持抗量子性。

Conclusion: 该算法为公钥密码学提供了一种新颖、高效、抗量子的替代方案，特别适合资源受限环境和后量子密码需求。

Abstract: Public-key cryptography algorithms have evolved towards increasing computational complexity to hide desired messages, which is accelerating with the development of the Internet and quantum computing. This paper introduces a novel public-key cryptography algorithm that generates ciphertexts by counting the number of elements in randomly extracted subsets from a multiset. After explaining the novel cryptographic concept, the process of mathematically refining it using satisfiability problems is described. The advantages of the proposed algorithm are: first, it is significantly faster than other public-key algorithms; second, it does not require big numbers, making it executable on any devices; and third, it can be easily extended into a public-key cryptosystem using a single public key and multiple private keys while maintaining quantum resistance.

</details>


### [46] [Belobog: Move Language Fuzzing Framework For Real-World Smart Contracts](https://arxiv.org/abs/2512.02918)
*Wanxu Xia,Ziqiao Kong,Zhengwei Li,Yi Lu,Pan Li,Liqun Yang,Yang Liu,Xiapu Luo,Shaohua Li*

Main category: cs.CR

TL;DR: Belobog是首个针对Move智能合约的模糊测试框架，通过类型感知和符号执行技术有效检测安全漏洞。


<details>
  <summary>Details</summary>
Motivation: Move语言虽然具有强类型系统和显式资源语义，但仍存在类型系统无法覆盖的漏洞，而现有模糊测试工具无法为Move合约生成语法或语义有效的交易。

Method: 基于Move类型系统构建类型图，根据类型图轨迹生成或变异交易，并设计实现符号执行器以处理复杂检查。

Result: 在109个真实Move智能合约项目中，Belobog能检测100%的关键漏洞和79%的主要漏洞，并成功复现了Cetus和Nemo两个著名安全事件。

Conclusion: Belobog是首个有效的Move智能合约模糊测试框架，通过类型感知方法解决了现有工具在强类型系统下的局限性，显著提升了Move合约的安全性验证能力。

Abstract: Move is a research-oriented programming language design for secure and verifiable smart contract development and has been widely used in managing billions of digital assets in blockchains, such as Sui and Aptos. Move features a strong static type system and explicit resource semantics to enforce safety properties such as the prevention of data races, invalid asset transfers, and entry vulnerabilities. However, smart contracts written in Move may still contain certain vulnerabilities that are beyond the reach of its type system. It is thus essential to validate Move smart contracts. Unfortunately, due to its strong type system, existing smart contract fuzzers are ineffective in producing syntactically or semantically valid transactions to test Move smart contracts. This paper introduces the first fuzzing framework, Belobog, for Move smart contracts. Belobog is type-aware and ensures that all generated and mutated transactions are well-typed. More specifically, for a target Move smart contract, Belobog first constructs a type graph based on Move's type system, and then generates or mutates a transaction based on the graph trace derived from the type graph. In order to overcome the complex checks in Move smart contracts, we further design and implement a concolic executor in Belobog. We evaluated Belobog on 109 real-world Move smart contract projects. The experimental results show that Belobog is able to detect 100\% critical and 79\% major vulnerabilities manually audited by human experts. We further selected two recent notorious incidents in Move smart contracts, i.e., Cetus and Nemo. Belobog successfully reproduced full exploits for both of them, without any prior knowledge.

</details>


### [47] [Semigroup action based on skew polynomial evaluation with applications to Cryptography](https://arxiv.org/abs/2512.02603)
*Daniel Camazón-Portela,Juan Antonio López-Ramos*

Main category: cs.CR

TL;DR: 本文引入基于多项式赋值和左斜积函数的斜多项式环作用，构建子集控制非交换性，并设计出在Canetti-Krawczyk模型中安全的密钥交换协议


<details>
  <summary>Details</summary>
Motivation: 研究斜多项式环的非交换代数结构，探索其在密码学中的应用潜力，特别是构建安全的公钥交换协议

Method: 基于斜多项式环的多项式赋值概念和左斜积函数，构造特殊子集T(X)来控制环的非交换性，利用这一性质设计公钥交换协议

Result: 成功构建了在Canetti-Krawczyk安全模型中具有安全性的公钥交换协议，展示了斜多项式环在密码学中的实际应用价值

Conclusion: 斜多项式环的非交换代数结构可用于构建安全的密码协议，为基于非交换代数的密码学提供了新的研究方向

Abstract: Through this work we introduce an action of the skew polynomial ring $\mathbb{F}_{q}\left[X; σ, δ\right]$ over $\mathbb{F}_{q}$ based on its polynomial valuation and the concept of left skew product of functions. This lead us to explore the construction of a certain subset $\mathcal{T}(X)\subset\mathbb{F}_{q}\left[X; σ, δ\right]$ that allow us to control the non-commutativity of this ring, and exploit this fact in order to build a public key exchange protocol that is secure in Canetti and Krawczyk model.

</details>


### [48] [CryptoQA: A Large-scale Question-answering Dataset for AI-assisted Cryptography](https://arxiv.org/abs/2512.02625)
*Mayar Elfares,Pascal Reisert,Tilman Dietz,Manpa Barman,Ahmed Zaki,Ralf Küsters,Andreas Bulling*

Main category: cs.CR

TL;DR: CryptoQA是首个专门为密码学设计的大规模问答数据集，包含200多万个QA对，用于评估和训练LLMs在密码学任务上的能力。


<details>
  <summary>Details</summary>
Motivation: LLMs在通用NLP任务上表现出色，但在需要深度推理和数学分析的密码学任务上的能力尚不明确，主要原因是缺乏合适的评估和训练数据。

Method: 创建CryptoQA数据集，包含从学术资源中提取的200多万个QA对及上下文元数据；对15个最先进的LLMs进行基准测试，评估事实准确性、数学推理、一致性、引用、逆向推理和对抗样本鲁棒性。

Result: LLMs在密码学任务上存在显著性能缺陷，特别是在需要形式推理和精确数学知识的任务上；使用CryptoQA微调的LLMs在密码学任务上表现更好。

Conclusion: 迫切需要专门针对密码学研究和开发的LLM助手；CryptoQA数据集能够有效提升LLMs在密码学任务上的性能。

Abstract: Large language models (LLMs) excel at many general-purpose natural language processing tasks. However, their ability to perform deep reasoning and mathematical analysis, particularly for complex tasks as required in cryptography, remains poorly understood, largely due to the lack of suitable data for evaluation and training. To address this gap, we present CryptoQA, the first large-scale question-answering (QA) dataset specifically designed for cryptography. CryptoQA contains over two million QA pairs drawn from curated academic sources, along with contextual metadata that can be used to test the cryptographic capabilities of LLMs and to train new LLMs on cryptographic tasks. We benchmark 15 state-of-the-art LLMs on CryptoQA, evaluating their factual accuracy, mathematical reasoning, consistency, referencing, backward reasoning, and robustness to adversarial samples. In addition to quantitative metrics, we provide expert reviews that qualitatively assess model outputs and establish a gold-standard baseline. Our results reveal significant performance deficits of LLMs, particularly on tasks that require formal reasoning and precise mathematical knowledge. This shows the urgent need for LLM assistants tailored to cryptography research and development. We demonstrate that, by using CryptoQA, LLMs can be fine-tuned to exhibit better performance on cryptographic tasks.

</details>


### [49] [Decryption Through Polynomial Ambiguity: Noise-Enhanced High-Memory Convolutional Codes for Post-Quantum Cryptography](https://arxiv.org/abs/2512.02822)
*Meir Ariel*

Main category: cs.CR

TL;DR: 提出了一种基于有向图解密和噪声增强高内存卷积码的后量子密码学新方法，通过随机化生成矩阵和故意注入强噪声实现比Classic McEliece高出2^200倍的安全裕度，支持任意明文长度和线性时间解密。


<details>
  <summary>Details</summary>
Motivation: 现有后量子密码方案（如Classic McEliece）在安全裕度和设计灵活性方面存在局限，需要开发更安全、更灵活且能抵抗量子攻击的公钥密码系统。

Method: 使用有向图解密噪声增强的高内存卷积码，生成随机化的生成矩阵隐藏代数结构，在解密过程中故意注入强噪声（来自多项式除法），合法接收者保持多项式时间解码，而攻击者面临指数时间复杂性。

Result: 方案实现了超过Classic McEliece 2^200倍的安全裕度，支持任意明文长度的线性时间解密，具有均匀的每比特计算成本，可通过并行有向图解码器阵列实现高效硬件和软件部署。

Conclusion: 该方案是稳健、可扩展且抗量子的公钥密码学的有力候选方案，在安全性、灵活性和可部署性方面具有显著优势。

Abstract: We present a novel approach to post-quantum cryptography that employs directed-graph decryption of noise-enhanced high-memory convolutional codes. The proposed construction generates random-like generator matrices that effectively conceal algebraic structure and resist known structural attacks. Security is further reinforced by the deliberate injection of strong noise during decryption, arising from polynomial division: while legitimate recipients retain polynomial-time decoding, adversaries face exponential-time complexity. As a result, the scheme achieves cryptanalytic security margins surpassing those of Classic McEliece by factors exceeding 2^(200). Beyond its enhanced security, the method offers greater design flexibility, supporting arbitrary plaintext lengths with linear-time decryption and uniform per-bit computational cost, enabling seamless scalability to long messages. Practical deployment is facilitated by parallel arrays of directed-graph decoders, which identify the correct plaintext through polynomial ambiguity while allowing efficient hardware and software implementations. Altogether, the scheme represents a compelling candidate for robust, scalable, and quantum-resistant public-key cryptography.

</details>
