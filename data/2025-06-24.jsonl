{"id": "2506.17233", "categories": ["cs.CR", "11Y05"], "pdf": "https://arxiv.org/pdf/2506.17233", "abs": "https://arxiv.org/abs/2506.17233", "authors": ["Akihisa Yorozu"], "title": "A Geometric Square-Based Approach to RSA Integer Factorization", "comment": "3 pages", "summary": "We present a new approach to RSA factorization inspired by geometric\ninterpretations and square differences. This method reformulates the problem in\nterms of the distance between perfect squares and provides a recurrence\nrelation that allows rapid convergence when the RSA modulus has closely spaced\nprime factors. Although this method is efficient for small semiprimes, it does\nnot yet succeed in factoring large challenges like RSA-100 in practical time,\nhighlighting both its potential and current limitations."}
{"id": "2506.17236", "categories": ["cs.CR", "cs.CE", "cs.DC"], "pdf": "https://arxiv.org/pdf/2506.17236", "abs": "https://arxiv.org/abs/2506.17236", "authors": ["Serdar Metin"], "title": "Design, Implementation, and Analysis of Fair Faucets for Blockchain Ecosystems", "comment": "PhD thesis, 98 pages, 4 figures", "summary": "The present dissertation addresses the problem of fairly distributing shared\nresources in non-commercial blockchain networks. Blockchains are distributed\nsystems that order and timestamp records of a given network of users, in a\npublic, cryptographically secure, and consensual way. The records, which may in\nkind be events, transaction orders, sets of rules for structured transactions\netc. are placed within well-defined datastructures called blocks, and they are\nlinked to each other by the virtue of cryptographic pointers, in a total\nordering which represents their temporal relations of succession. The ability\nto operate on the blockchain, and/or to contribute a record to the content of a\nblock are shared resources of the blockchain systems. In commercial networks,\nthese resources are exchanged in return for fiat money, and consequently,\nfairness is not a relevant problem in terms of computer engineering. In\nnon-commercial networks, however, monetary solutions are not available, by\ndefinition. The present non-commercial blockchain networks employ trivial\ndistribution mechanisms called faucets, which offer fixed amounts of free\ntokens (called cryptocurrencies) specific to the given network. This mechanism,\nalthough simple and efficient, is prone to denial of service (DoS) attacks and\ncannot address the fairness problem. In the present dissertation, the faucet\nmechanism is adapted for fair distribution, in line with Max-min Fairness\nscheme. In total, we contributed 6 distinct Max-min Fair algorithms as\nefficient blockchain faucets. The algorithms we contribute are resistant to DoS\nattacks, low-cost in terms of blockchain computation economics, and they also\nallow for different user weighting policies."}
{"id": "2506.17245", "categories": ["cs.CR", "68M25 (Primary), 68Q85, 94A60 (Secondary)", "D.4.6; K.6.5; H.2.0; H.3.3"], "pdf": "https://arxiv.org/pdf/2506.17245", "abs": "https://arxiv.org/abs/2506.17245", "authors": ["Sagar Neupane"], "title": "Detecting and Mitigating SQL Injection Vulnerabilities in Web Applications", "comment": "24 pages, 4 figures", "summary": "SQL injection (SQLi) remains a critical vulnerability in web applications,\nenabling attackers to manipulate databases through malicious inputs. Despite\nadvancements in mitigation techniques, the evolving complexity of web\napplications and attack strategies continues to pose significant risks. This\npaper presents a comprehensive penetration testing methodology to identify,\nexploit, and mitigate SQLi vulnerabilities in a PHP-MySQL-based web\napplication. Utilizing tools such as OWASP ZAP, sqlmap, and Nmap, the study\ndemonstrates a systematic approach to vulnerability assessment and remediation.\nThe findings underscore the efficacy of input sanitization and prepared\nstatements in mitigating SQLi risks, while highlighting the need for ongoing\nsecurity assessments to address emerging threats. The study contributes to the\nfield by providing practical insights into effective detection and prevention\nstrategies, supported by a real-world case study."}
{"id": "2506.17266", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2506.17266", "abs": "https://arxiv.org/abs/2506.17266", "authors": ["Sunil Kumar Jang Bahadur", "Gopala Dhar"], "title": "Securing Generative AI Agentic Workflows: Risks, Mitigation, and a Proposed Firewall Architecture", "comment": "Proposed workflow", "summary": "Generative Artificial Intelligence (GenAI) presents significant advancements\nbut also introduces novel security challenges, particularly within agentic\nworkflows where AI agents operate autonomously. These risks escalate in\nmulti-agent systems due to increased interaction complexity. This paper\noutlines critical security vulnerabilities inherent in GenAI agentic workflows,\nincluding data privacy breaches, model manipulation, and issues related to\nagent autonomy and system integration. It discusses key mitigation strategies\nsuch as data encryption, access control, prompt engineering, model monitoring,\nagent sandboxing, and security audits. Furthermore, it details a proposed\n\"GenAI Security Firewall\" architecture designed to provide comprehensive,\nadaptable, and efficient protection for these systems by integrating various\nsecurity services and leveraging GenAI itself for enhanced defense. Addressing\nthese security concerns is paramount for the responsible and safe deployment of\nthis transformative technology."}
{"id": "2506.17289", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.17289", "abs": "https://arxiv.org/abs/2506.17289", "authors": ["Rahul Raja", "Arpita Vats"], "title": "Evaluating Generalization and Representation Stability in Small LMs via Prompting", "comment": "Accepted at ICML", "summary": "We investigate the generalization capabilities of small language models under\ntwo popular adaptation paradigms: few-shot prompting and supervised\nfine-tuning. While prompting is often favored for its parameter efficiency and\nflexibility, it remains unclear how robust this approach is in low-resource\nsettings and under distributional shifts. This paper presents a comparative\nstudy of prompting and fine-tuning across task formats, prompt styles, and\nmodel scales, with a focus on their behavior in both in-distribution and\nout-of-distribution (OOD) settings.\n  Beyond accuracy, we analyze the internal representations learned by each\napproach to assess the stability and abstraction of task-specific features. Our\nfindings highlight critical differences in how small models internalize and\ngeneralize knowledge under different adaptation strategies. This work offers\npractical guidance for model selection in low-data regimes and contributes\nempirical insight into the ongoing debate over prompting versus fine-tuning.\nCode for the experiments is available at the following"}
{"id": "2506.17306", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.17306", "abs": "https://arxiv.org/abs/2506.17306", "authors": ["Jake Zappin", "Trevor Stalnaker", "Oscar Chaparro", "Denys Poshyvanyk"], "title": "Challenges and Practices in Quantum Software Testing and Debugging: Insights from Practitioners", "comment": null, "summary": "Quantum software engineering is an emerging discipline with distinct\nchallenges, particularly in testing and debugging. As quantum computing\ntransitions from theory to implementation, developers face issues not present\nin classical software development, such as probabilistic execution, limited\nobservability, shallow abstractions, and low awareness of quantum-specific\ntools. To better understand current practices, we surveyed 26 quantum software\ndevelopers from academia and industry and conducted follow-up interviews\nfocused on testing, debugging, and recurring challenges. All participants\nreported engaging in testing, with unit testing (88%), regression testing\n(54%), and acceptance testing (54%) being the most common. However, only 31%\nreported using quantum-specific testing tools, relying instead on manual\nmethods. Debugging practices were similarly grounded in classical strategies,\nsuch as print statements, circuit visualizations, and simulators, which\nrespondents noted do not scale well. The most frequently cited sources of bugs\nwere classical in nature-library updates (81%), developer mistakes (68%), and\ncompatibility issues (62%)-often worsened by limited abstraction in existing\nSDKs. These findings highlight the urgent need for better-aligned testing and\ndebugging tools, integrated more seamlessly into the workflows of quantum\ndevelopers. We present these results in detail and offer actionable\nrecommendations grounded in the real-world needs of practitioners."}
{"id": "2506.17269", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2506.17269", "abs": "https://arxiv.org/abs/2506.17269", "authors": ["Paritosh Ranjan", "Surajit Majumder", "Prodip Roy"], "title": "Digital Privacy Everywhere", "comment": "18 pages, 10 figures", "summary": "The increasing proliferation of digital and mobile devices equipped with\ncameras, microphones, GPS, and other privacy invasive components has raised\nsignificant concerns for businesses operating in sensitive or policy restricted\nenvironments. Current solutions rely on passive enforcement, such as signage or\nverbal instructions, which are largely ineffective. This paper presents Digital\nPrivacy Everywhere (DPE), a comprehensive and scalable system designed to\nactively enforce custom privacy policies for digital devices within predefined\nphysical boundaries. The DPE architecture includes a centralized management\nconsole, field verification units (FVUs), enforcement modules for mobile\ndevices (EMMDs), and an External Geo Ownership Service (EGOS). These components\ncollaboratively detect, configure, and enforce privacy settings such as\ndisabling cameras, microphones, or radios across various premises like\ntheaters, hospitals, financial institutions, and educational facilities. The\nsystem ensures privacy compliance in real time while maintaining a seamless\nuser experience and operational scalability across geographies."}
{"id": "2506.17300", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.17300", "abs": "https://arxiv.org/abs/2506.17300", "authors": ["Daniel T. Chang"], "title": "Individual Causal Inference with Structural Causal Model", "comment": null, "summary": "Individual causal inference (ICI) uses causal inference methods to understand\nand predict the effects of interventions on individuals, considering their\nspecific characteristics / facts. It aims to estimate individual causal effect\n(ICE), which varies across individuals. Estimating ICE can be challenging due\nto the limited data available for individuals, and the fact that most causal\ninference methods are population-based. Structural Causal Model (SCM) is\nfundamentally population-based. Therefore, causal discovery (structural\nlearning and parameter learning), association queries and intervention queries\nare all naturally population-based. However, exogenous variables (U) in SCM can\nencode individual variations and thus provide the mechanism for individualized\npopulation per specific individual characteristics / facts. Based on this, we\npropose ICI with SCM as a \"rung 3\" causal inference, because it involves\n\"imagining\" what would be the causal effect of a hypothetical intervention on\nan individual, given the individual's observed characteristics / facts.\nSpecifically, we propose the indiv-operator, indiv(W), to formalize/represent\nthe population individualization process, and the individual causal query, P(Y\n| indiv(W), do(X), Z), to formalize/represent ICI. We show and argue that ICI\nwith SCM is inference on individual alternatives (possible), not individual\ncounterfactuals (non-actual)."}
{"id": "2506.17313", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.17313", "abs": "https://arxiv.org/abs/2506.17313", "authors": ["Jonathan Reif", "Daniel Dittler", "Milapji Singh Gill", "Tamás Farkas", "Valentin Stegmaier", "Felix Gehlhoff", "Tobias Kleinert", "Michael Weyrich"], "title": "An Expert Survey on Models and Digital Twins", "comment": "This article is accepted at CIRP ICME and for publication in Procedia\n  CIRP", "summary": "Digital Twins (DTs) are becoming increasingly vital for future industrial\napplications, enhancing monitoring, control, and optimization of physical\nassets. This enhancement is made possible by integrating various Digital Models\n(DMs) within DTs, which must interoperate to represent different system aspects\nand fulfill diverse application purposes. However, industry perspectives on the\nchallenges and research needs for integrating these models are rarely obtained.\nThus, this study conducts an expert survey across multiple application domains\nto identify and analyze the challenges in utilizing diverse DMs within DTs. The\nresults reveal missing standardized interfaces, high manual adaptation effort,\nand limited support for model reuse across lifecycle phases, highlighting\nfuture research needs in automated model composition and semantics-based\ninteroperability."}
{"id": "2506.17279", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17279", "abs": "https://arxiv.org/abs/2506.17279", "authors": ["Yash Sinha", "Manit Baser", "Murari Mandal", "Dinil Mon Divakaran", "Mohan Kankanhalli"], "title": "Step-by-Step Reasoning Attack: Revealing 'Erased' Knowledge in Large Language Models", "comment": null, "summary": "Knowledge erasure in large language models (LLMs) is important for ensuring\ncompliance with data and AI regulations, safeguarding user privacy, mitigating\nbias, and misinformation. Existing unlearning methods aim to make the process\nof knowledge erasure more efficient and effective by removing specific\nknowledge while preserving overall model performance, especially for retained\ninformation. However, it has been observed that the unlearning techniques tend\nto suppress and leave the knowledge beneath the surface, thus making it\nretrievable with the right prompts. In this work, we demonstrate that\n\\textit{step-by-step reasoning} can serve as a backdoor to recover this hidden\ninformation. We introduce a step-by-step reasoning-based black-box attack,\nSleek, that systematically exposes unlearning failures. We employ a structured\nattack framework with three core components: (1) an adversarial prompt\ngeneration strategy leveraging step-by-step reasoning built from LLM-generated\nqueries, (2) an attack mechanism that successfully recalls erased content, and\nexposes unfair suppression of knowledge intended for retention and (3) a\ncategorization of prompts as direct, indirect, and implied, to identify which\nquery types most effectively exploit unlearning weaknesses. Through extensive\nevaluations on four state-of-the-art unlearning techniques and two widely used\nLLMs, we show that existing approaches fail to ensure reliable knowledge\nremoval. Of the generated adversarial prompts, 62.5% successfully retrieved\nforgotten Harry Potter facts from WHP-unlearned Llama, while 50% exposed unfair\nsuppression of retained knowledge. Our work highlights the persistent risks of\ninformation leakage, emphasizing the need for more robust unlearning strategies\nfor erasure."}
{"id": "2506.17434", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17434", "abs": "https://arxiv.org/abs/2506.17434", "authors": ["Sydney Levine", "Matija Franklin", "Tan Zhi-Xuan", "Secil Yanik Guyot", "Lionel Wong", "Daniel Kilov", "Yejin Choi", "Joshua B. Tenenbaum", "Noah Goodman", "Seth Lazar", "Iason Gabriel"], "title": "Resource Rational Contractualism Should Guide AI Alignment", "comment": "24 pages, 10 figures", "summary": "AI systems will soon have to navigate human environments and make decisions\nthat affect people and other AI agents whose goals and values diverge.\nContractualist alignment proposes grounding those decisions in agreements that\ndiverse stakeholders would endorse under the right conditions, yet securing\nsuch agreement at scale remains costly and slow -- even for advanced AI. We\ntherefore propose Resource-Rational Contractualism (RRC): a framework where AI\nsystems approximate the agreements rational parties would form by drawing on a\ntoolbox of normatively-grounded, cognitively-inspired heuristics that trade\neffort for accuracy. An RRC-aligned agent would not only operate efficiently,\nbut also be equipped to dynamically adapt to and interpret the ever-changing\nhuman social world."}
{"id": "2506.17330", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.17330", "abs": "https://arxiv.org/abs/2506.17330", "authors": ["Simon Thorne"], "title": "Large Language Models for Spreadsheets: Benchmarking Progress and Evaluating Performance with FLARE", "comment": "18 Pages, 10 Tables, 1 Colour Figure", "summary": "Large Language Models (LLMs) have demonstrated some significant capabilities\nacross various domains; however, their effectiveness in spreadsheet related\ntasks remains underexplored. This study introduces a foundation for a\ncomprehensive benchmark framework to evaluate the performance of leading LLMs\nin executing spreadsheet functions, formula generation and data manipulation\ntasks. The benchmark encompasses tasks ranging from basic formula creation to\ncomplex, real world spreadsheet scenarios. Our findings reveal that while LLMs\nexhibit proficiency in straightforward tasks, they often falter in complex,\nmulti step operations, frequently producing plausible yet incorrect outputs.\nThese results underscore the limitations of current LLMs in handling\nspreadsheet tasks that require precise logical reasoning and highlight the need\nfor integrating symbolic reasoning capabilities into LLM architectures. To\nsupport this, we introduce FLARE (Formula Logic, Auditing, Reasoning and\nEvaluation) a new benchmark for evaluating LLM performance on real-world\nspreadsheet logic, auditing, and reasoning tasks."}
{"id": "2506.17292", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17292", "abs": "https://arxiv.org/abs/2506.17292", "authors": ["Quan Nguyen", "Minh N. Vu", "Truc Nguyen", "My T. Thai"], "title": "Theoretically Unmasking Inference Attacks Against LDP-Protected Clients in Federated Vision Models", "comment": "Accepted to ICML 2025", "summary": "Federated Learning enables collaborative learning among clients via a\ncoordinating server while avoiding direct data sharing, offering a perceived\nsolution to preserve privacy. However, recent studies on Membership Inference\nAttacks (MIAs) have challenged this notion, showing high success rates against\nunprotected training data. While local differential privacy (LDP) is widely\nregarded as a gold standard for privacy protection in data analysis, most\nstudies on MIAs either neglect LDP or fail to provide theoretical guarantees\nfor attack success rates against LDP-protected data. To address this gap, we\nderive theoretical lower bounds for the success rates of low-polynomial time\nMIAs that exploit vulnerabilities in fully connected or self-attention layers.\nWe establish that even when data are protected by LDP, privacy risks persist,\ndepending on the privacy budget. Practical evaluations on federated vision\nmodels confirm considerable privacy risks, revealing that the noise required to\nmitigate these attacks significantly degrades models' utility."}
{"id": "2506.17442", "categories": ["cs.AI", "cs.ET", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.17442", "abs": "https://arxiv.org/abs/2506.17442", "authors": ["Hao Guan", "David Bates", "Li Zhou"], "title": "Keeping Medical AI Healthy: A Review of Detection and Correction Methods for System Degradation", "comment": "15 pages, 5 figures", "summary": "Artificial intelligence (AI) is increasingly integrated into modern\nhealthcare, offering powerful support for clinical decision-making. However, in\nreal-world settings, AI systems may experience performance degradation over\ntime, due to factors such as shifting data distributions, changes in patient\ncharacteristics, evolving clinical protocols, and variations in data quality.\nThese factors can compromise model reliability, posing safety concerns and\nincreasing the likelihood of inaccurate predictions or adverse outcomes. This\nreview presents a forward-looking perspective on monitoring and maintaining the\n\"health\" of AI systems in healthcare. We highlight the urgent need for\ncontinuous performance monitoring, early degradation detection, and effective\nself-correction mechanisms. The paper begins by reviewing common causes of\nperformance degradation at both data and model levels. We then summarize key\ntechniques for detecting data and model drift, followed by an in-depth look at\nroot cause analysis. Correction strategies are further reviewed, ranging from\nmodel retraining to test-time adaptation. Our survey spans both traditional\nmachine learning models and state-of-the-art large language models (LLMs),\noffering insights into their strengths and limitations. Finally, we discuss\nongoing technical challenges and propose future research directions. This work\naims to guide the development of reliable, robust medical AI systems capable of\nsustaining safe, long-term deployment in dynamic clinical settings."}
{"id": "2506.17335", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17335", "abs": "https://arxiv.org/abs/2506.17335", "authors": ["Shuo Yan", "Ruochen Li", "Ziming Luo", "Zimu Wang", "Daoyang Li", "Liqiang Jing", "Kaiyu He", "Peilin Wu", "George Michalopoulos", "Yue Zhang", "Ziyang Zhang", "Mian Zhang", "Zhiyu Chen", "Xinya Du"], "title": "LMR-BENCH: Evaluating LLM Agent's Ability on Reproducing Language Modeling Research", "comment": null, "summary": "Large language model (LLM) agents have demonstrated remarkable potential in\nadvancing scientific discovery. However, their capability in the fundamental\nyet crucial task of reproducing code from research papers, especially in the\nNLP domain, remains underexplored. This task includes unique complex reasoning\nchallenges in the intellectual synthesis of abstract concepts and the\ncomprehension of code repositories with interdependent files. Motivated by this\ngap, we present LMR-BENCH, a benchmark designed to systematically evaluate the\ncapability of LLM agents on code reproduction from Language Modeling Research.\nIt consists of 28 code reproduction tasks derived from 23 research papers\npublished in top-tier NLP venues over the past five years, spanning nine\nfundamental categories. Models are provided with a research paper, a code\nrepository containing one or more masked functions, and instructions for\nimplementing these functions. We conduct extensive experiments in standard\nprompting and LLM agent settings with state-of-the-art LLMs, evaluating the\naccuracy of unit tests and performing LLM-based evaluation of code correctness.\nExperimental results reveal that even the most advanced models still exhibit\npersistent limitations in scientific reasoning and code synthesis, highlighting\ncritical gaps in LLM agents' ability to autonomously reproduce scientific\nresearch"}
{"id": "2506.17299", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.17299", "abs": "https://arxiv.org/abs/2506.17299", "authors": ["Shuyi Lin", "Anshuman Suri", "Alina Oprea", "Cheng Tan"], "title": "LLM Jailbreak Oracle", "comment": null, "summary": "As large language models (LLMs) become increasingly deployed in\nsafety-critical applications, the lack of systematic methods to assess their\nvulnerability to jailbreak attacks presents a critical security gap. We\nintroduce the jailbreak oracle problem: given a model, prompt, and decoding\nstrategy, determine whether a jailbreak response can be generated with\nlikelihood exceeding a specified threshold. This formalization enables a\nprincipled study of jailbreak vulnerabilities. Answering the jailbreak oracle\nproblem poses significant computational challenges -- the search space grows\nexponentially with the length of the response tokens. We present Boa, the first\nefficient algorithm for solving the jailbreak oracle problem. Boa employs a\nthree-phase search strategy: (1) constructing block lists to identify refusal\npatterns, (2) breadth-first sampling to identify easily accessible jailbreaks,\nand (3) depth-first priority search guided by fine-grained safety scores to\nsystematically explore promising low-probability paths. Boa enables rigorous\nsecurity assessments including systematic defense evaluation, standardized\ncomparison of red team attacks, and model certification under extreme\nadversarial conditions."}
{"id": "2506.17449", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17449", "abs": "https://arxiv.org/abs/2506.17449", "authors": ["Manasa Bharadwaj", "Nikhil Verma", "Kevin Ferreira"], "title": "OmniReflect: Discovering Transferable Constitutions for LLM agents via Neuro-Symbolic Reflections", "comment": null, "summary": "Efforts to improve Large Language Model (LLM) agent performance on complex\ntasks have largely focused on fine-tuning and iterative self-correction.\nHowever, these approaches often lack generalizable mechanisms for longterm\nlearning and remain inefficient in dynamic environments. We introduce\nOmniReflect, a hierarchical, reflection-driven framework that constructs a\nconstitution, a compact set of guiding principles distilled from task\nexperiences, to enhance the effectiveness and efficiency of an LLM agent.\nOmniReflect operates in two modes: Self-sustaining, where a single agent\nperiodically curates its own reflections during task execution, and\nCo-operative, where a Meta-advisor derives a constitution from a small\ncalibration set to guide another agent. To construct these constitutional\nprinciples, we employ Neural, Symbolic, and NeuroSymbolic techniques, offering\na balance between contextual adaptability and computational efficiency.\nEmpirical results averaged across models show major improvements in task\nsuccess, with absolute gains of +10.3% on ALFWorld, +23.8% on BabyAI, and +8.3%\non PDDL in the Self-sustaining mode. Similar gains are seen in the Co-operative\nmode, where a lightweight Qwen3-4B ReAct agent outperforms all Reflexion\nbaselines on BabyAI. These findings highlight the robustness and effectiveness\nof OmniReflect across environments and backbones."}
{"id": "2506.17369", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17369", "abs": "https://arxiv.org/abs/2506.17369", "authors": ["Zhiyuan Pan", "Xing Hu", "Xin Xia", "Xiaohu Yang"], "title": "Re-Evaluating Code LLM Benchmarks Under Semantic Mutation", "comment": null, "summary": "In the era of large language models (LLMs), code benchmarks have become an\nimportant research area in software engineering and are widely used by\npractitioners. These benchmarks evaluate the performance of LLMs on specific\ncode-related tasks, such as code understanding and generation. A critical step\nin constructing code benchmarks is the design of prompts. However, as existing\ncode benchmarks typically rely on a single prompt template per task, they are\nprone to the issue of prompt sensitivity, where minor prompt variations could\nresult in substantial performance variations, leading to unreliable evaluations\nof model capabilities.\n  While previous studies have explored prompt sensitivity, their experimental\ndesigns and findings are limited to traditional natural language processing\n(NLP) tasks. In this paper, we present an empirical study to investigate prompt\nsensitivity in code benchmarks. We first propose a general framework that\nmodifies prompt templates in a manner that preserves both their semantics and\ntheir structure as much as possible. Based on the framework, we conduct\nextensive experiments across eight code benchmark tasks on 10 representative\nopen-source LLMs, with each task featuring 100 semantically similar prompt\ntemplates. We then analyze the evaluation results using various statistical\nmetrics, focusing on both absolute and relative model performance. Our findings\nsuggest that even slight prompt variations can lead to significant shifts in\nperformance. Additionally, we observe that such variations can introduce\ninconsistencies in the performance rankings across different models. These\ninsights highlight the need for considering prompt sensitivity when designing\nfuture code benchmarks, to ensure more reliable and accurate evaluation of LLM\ncapabilities."}
{"id": "2506.17308", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2506.17308", "abs": "https://arxiv.org/abs/2506.17308", "authors": ["Koichi Nagatsuka", "Terufumi Morishita", "Yasuhiro Sogawa"], "title": "A Nested Watermark for Large Language Models", "comment": "6 pages, 3 figures", "summary": "The rapid advancement of large language models (LLMs) has raised concerns\nregarding their potential misuse, particularly in generating fake news and\nmisinformation. To address these risks, watermarking techniques for\nautoregressive language models have emerged as a promising means for detecting\nLLM-generated text. Existing methods typically embed a watermark by increasing\nthe probabilities of tokens within a group selected according to a single\nsecret key. However, this approach suffers from a critical limitation: if the\nkey is leaked, it becomes impossible to trace the text's provenance or\nattribute authorship. To overcome this vulnerability, we propose a novel nested\nwatermarking scheme that embeds two distinct watermarks into the generated text\nusing two independent keys. This design enables reliable authorship\nidentification even in the event that one key is compromised. Experimental\nresults demonstrate that our method achieves high detection accuracy for both\nwatermarks while maintaining the fluency and overall quality of the generated\ntext."}
{"id": "2506.17484", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17484", "abs": "https://arxiv.org/abs/2506.17484", "authors": ["Yao Zhang", "Zaixi Shang", "Silpan Patel", "Mikel Zuniga"], "title": "From Unstructured Communication to Intelligent RAG: Multi-Agent Automation for Supply Chain Knowledge Bases", "comment": "Accepted In Proceedings of the 1st Workshop on AI for Supply Chain:\n  Today and Future @ 31st ACM SIGKDD Conference on Knowledge Discovery and Data\n  Mining V.2 (KDD 25), August 3, 2025, Toronto, ON, Canada. ACM, New York, NY,\n  USA, 14 pages, 2 figures", "summary": "Supply chain operations generate vast amounts of operational data; however,\ncritical knowledge such as system usage practices, troubleshooting workflows,\nand resolution techniques often remains buried within unstructured\ncommunications like support tickets, emails, and chat logs. While RAG systems\naim to leverage such communications as a knowledge base, their effectiveness is\nlimited by raw data challenges: support tickets are typically noisy,\ninconsistent, and incomplete, making direct retrieval suboptimal. Unlike\nexisting RAG approaches that focus on runtime optimization, we introduce a\nnovel offline-first methodology that transforms these communications into a\nstructured knowledge base. Our key innovation is a LLMs-based multi-agent\nsystem orchestrating three specialized agents: Category Discovery for taxonomy\ncreation, Categorization for ticket grouping, and Knowledge Synthesis for\narticle generation. Applying our methodology to real-world support tickets with\nresolution notes and comments, our system creates a compact knowledge base -\nreducing total volume to just 3.4% of original ticket data while improving\nquality. Experiments demonstrate that our prebuilt knowledge base in RAG\nsystems significantly outperforms traditional RAG implementations (48.74% vs.\n38.60% helpful answers) and achieves a 77.4% reduction in unhelpful responses.\nBy automating institutional knowledge capture that typically remains siloed in\nexperts' heads, our solution translates to substantial operational efficiency:\nreducing support workload, accelerating resolution times, and creating\nself-improving systems that automatically resolve approximately 50% of future\nsupply chain tickets. Our approach addresses a key gap in knowledge management\nby transforming transient communications into structured, reusable knowledge\nthrough intelligent offline processing rather than latency-inducing runtime\narchitectures."}
{"id": "2506.17539", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.17539", "abs": "https://arxiv.org/abs/2506.17539", "authors": ["Sidong Feng", "Changhao Du", "Huaxiao Liu", "Qingnan Wang", "Zhengwei Lv", "Mengfei Wang", "Chunyang Chen"], "title": "Breaking Single-Tester Limits: Multi-Agent LLMs for Multi-User Feature Testing", "comment": "Accepted to International Conference on Software Engineering (ICSE\n  2026)", "summary": "The growing dependence on mobile phones and their apps has made multi-user\ninteractive features, like chat calls, live streaming, and video conferencing,\nindispensable for bridging the gaps in social connectivity caused by physical\nand situational barriers. However, automating these interactive features for\ntesting is fraught with challenges, owing to their inherent need for timely,\ndynamic, and collaborative user interactions, which current automated testing\nmethods inadequately address. Inspired by the concept of agents designed to\nautonomously and collaboratively tackle problems, we propose MAdroid, a novel\nmulti-agent approach powered by the Large Language Models (LLMs) to automate\nthe multi-user interactive task for app feature testing. Specifically, MAdroid\nemploys two functional types of multi-agents: user agents (Operator) and\nsupervisor agents (Coordinator and Observer). Each agent takes a specific role:\nthe Coordinator directs the interactive task; the Operator mimics user\ninteractions on the device; and the Observer monitors and reviews the task\nautomation process. Our evaluation, which included 41 multi-user interactive\ntasks, demonstrates the effectiveness of our approach, achieving 82.9% of the\ntasks with 96.8% action similarity, outperforming the ablation studies and\nstate-of-the-art baselines. Additionally, a preliminary investigation\nunderscores MAdroid's practicality by helping identify 11 multi-user\ninteractive bugs during regression app testing, confirming its potential value\nin real-world software development contexts."}
{"id": "2506.17309", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.17309", "abs": "https://arxiv.org/abs/2506.17309", "authors": ["Aditya Choudhary", "Sarthak Pawar", "Yashodhara Haribhakta"], "title": "Efficient Malware Detection with Optimized Learning on High-Dimensional Features", "comment": "This paper has been accepted for presentation at the International\n  Conference on Innovations in Intelligent Systems: Advancements in Computing,\n  Communication, and Cybersecurity (ISAC3)", "summary": "Malware detection using machine learning requires feature extraction from\nbinary files, as models cannot process raw binaries directly. A common approach\ninvolves using LIEF for raw feature extraction and the EMBER vectorizer to\ngenerate 2381-dimensional feature vectors. However, the high dimensionality of\nthese features introduces significant computational challenges. This study\naddresses these challenges by applying two dimensionality reduction techniques:\nXGBoost-based feature selection and Principal Component Analysis (PCA). We\nevaluate three reduced feature dimensions (128, 256, and 384), which correspond\nto approximately 5.4%, 10.8%, and 16.1% of the original 2381 features, across\nfour models-XGBoost, LightGBM, Extra Trees, and Random Forest-using a unified\ntraining, validation, and testing split formed from the EMBER-2018, ERMDS, and\nBODMAS datasets. This approach ensures generalization and avoids dataset bias.\nExperimental results show that LightGBM trained on the 384-dimensional feature\nset after XGBoost feature selection achieves the highest accuracy of 97.52% on\nthe unified dataset, providing an optimal balance between computational\nefficiency and detection performance. The best model, trained in 61 minutes\nusing 30 GB of RAM and 19.5 GB of disk space, generalizes effectively to\ncompletely unseen datasets, maintaining 95.31% accuracy on TRITIUM and 93.98%\naccuracy on INFERNO. These findings present a scalable, compute-efficient\napproach for malware detection without compromising accuracy."}
{"id": "2506.17514", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17514", "abs": "https://arxiv.org/abs/2506.17514", "authors": ["Ninareh Mehrabi", "Tharindu Kumarage", "Kai-Wei Chang", "Aram Galstyan", "Rahul Gupta"], "title": "Kaleidoscopic Teaming in Multi Agent Simulations", "comment": null, "summary": "Warning: This paper contains content that may be inappropriate or offensive.\n  AI agents have gained significant recent attention due to their autonomous\ntool usage capabilities and their integration in various real-world\napplications. This autonomy poses novel challenges for the safety of such\nsystems, both in single- and multi-agent scenarios. We argue that existing red\nteaming or safety evaluation frameworks fall short in evaluating safety risks\nin complex behaviors, thought processes and actions taken by agents. Moreover,\nthey fail to consider risks in multi-agent setups where various vulnerabilities\ncan be exposed when agents engage in complex behaviors and interactions with\neach other. To address this shortcoming, we introduce the term kaleidoscopic\nteaming which seeks to capture complex and wide range of vulnerabilities that\ncan happen in agents both in single-agent and multi-agent scenarios. We also\npresent a new kaleidoscopic teaming framework that generates a diverse array of\nscenarios modeling real-world human societies. Our framework evaluates safety\nof agents in both single-agent and multi-agent setups. In single-agent setup,\nan agent is given a scenario that it needs to complete using the tools it has\naccess to. In multi-agent setup, multiple agents either compete against or\ncooperate together to complete a task in the scenario through which we capture\nexisting safety vulnerabilities in agents. We introduce new in-context\noptimization techniques that can be used in our kaleidoscopic teaming framework\nto generate better scenarios for safety analysis. Lastly, we present\nappropriate metrics that can be used along with our framework to measure safety\nof agents. Utilizing our kaleidoscopic teaming framework, we identify\nvulnerabilities in various models with respect to their safety in agentic\nuse-cases."}
{"id": "2506.17627", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.17627", "abs": "https://arxiv.org/abs/2506.17627", "authors": ["Hongzhou Rao", "Yanjie Zhao", "Wenjie Zhu", "Ling Xiao", "Meizhen Wang", "Haoyu Wang"], "title": "CodeMorph: Mitigating Data Leakage in Large Language Model Assessment", "comment": "Accepted by ICSE 2025 (Industry Challenge Track)", "summary": "Concerns about benchmark leakage in large language models for code (Code\nLLMs) have raised issues of data contamination and inflated evaluation metrics.\nThe diversity and inaccessibility of many training datasets make it difficult\nto prevent data leakage entirely, even with time lag strategies. Consequently,\ngenerating new datasets through code perturbation has become essential.\nHowever, existing methods often fail to produce complex and diverse variations,\nstruggle with complex cross-file dependencies, and lack support for multiple\nprogramming languages, which limits their effectiveness in enhancing LLM\nevaluations for coding tasks. To fill this gap, we propose CodeMorph, an\napproach designed to support multiple programming languages while preserving\ncross-file dependencies to mitigate data leakage. CodeMorph consists of two\nmain components that work together to enhance the perturbation process. The\nfirst component employs 26 semantic-preserving transformation methods to\niteratively perturb code, generating diverse variations while ensuring that the\nmodified code remains compilable. The second component introduces a genetic\nalgorithm-based selection algorithm, PESO, to identify the more effective\nperturbation method for each iteration by targeting lower similarity scores\nbetween the perturbed and original code, thereby enhancing overall perturbation\neffectiveness. Experimental results demonstrate that after applying CodeMorph,\nthe accuracy of the LLM on code completion tasks across five programming\nlanguages decreased by an average of 24.67%, with Python showing the most\nsignificant reduction at 45%. The similarity score of code optimized by PESO\nis, on average, 7.01% lower than that of randomly perturbed code, peaking at a\nreduction of 42.86%."}
{"id": "2506.17315", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2506.17315", "abs": "https://arxiv.org/abs/2506.17315", "authors": ["Chuan Yan", "Liuhuo Wan", "Bowei Guan", "Fengqi Yu", "Guangdong Bai", "Jin Song Dong"], "title": "Tracking GPTs Third Party Service: Automation, Analysis, and Insights", "comment": "The 1st International Workshop on LLM App Store Analysis (LLMapp\n  2025)", "summary": "ChatGPT has quickly advanced from simple natural language processing to\ntackling more sophisticated and specialized tasks. Drawing inspiration from the\nsuccess of mobile app ecosystems, OpenAI allows developers to create\napplications that interact with third-party services, known as GPTs. GPTs can\nchoose to leverage third-party services to integrate with specialized APIs for\ndomain-specific applications. However, the way these disclose privacy setting\ninformation limits accessibility and analysis, making it challenging to\nsystematically evaluate the data privacy implications of third-party integrate\nto GPTs. In order to support academic research on the integration of\nthird-party services in GPTs, we introduce GPTs-ThirdSpy, an automated\nframework designed to extract privacy settings of GPTs. GPTs-ThirdSpy provides\nacademic researchers with real-time, reliable metadata on third-party services\nused by GPTs, enabling in-depth analysis of their integration, compliance, and\npotential security risks. By systematically collecting and structuring this\ndata, GPTs-ThirdSpy facilitates large-scale research on the transparency and\nregulatory challenges associated with the GPT app ecosystem."}
{"id": "2506.17585", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.17585", "abs": "https://arxiv.org/abs/2506.17585", "authors": ["Yukun Huang", "Sanxing Chen", "Jian Pei", "Manzil Zaheer", "Bhuwan Dhingra"], "title": "Cite Pretrain: Retrieval-Free Knowledge Attribution for Large Language Models", "comment": null, "summary": "Trustworthy language models should provide both correct and verifiable\nanswers. While language models can sometimes attribute their outputs to\npretraining data, their citations are often unreliable due to hallucination. As\na result, current systems insert citations by querying an external retriever at\ninference time, introducing latency, infrastructure dependence, and\nvulnerability to retrieval noise. We explore whether LLMs can be made to\nreliably attribute to the documents seen during (continual)\npretraining--without test-time retrieval--by revising the training process. To\nevaluate this, we release CitePretrainBench, a benchmark that mixes real-world\ncorpora (Wikipedia, Common Crawl, arXiv) with novel, unseen documents and\nprobes both short-form (single fact) and long-form (multi-fact) citation tasks.\nOur approach follows a two-stage process: (1) continual pretraining to bind\nfacts to persistent document identifiers, and (2) instruction tuning to elicit\ncitation behavior. We find that simple Passive Indexing, which appends an\nidentifier to each document, helps memorize verbatim text but fails on\nparaphrased or compositional facts. Instead, we propose Active Indexing, which\ncontinually pretrains on synthetic QA pairs that (1) restate each fact in\ndiverse compositional forms, and (2) require bidirectional source-to-fact and\nfact-to-source generation, jointly teaching the model to generate content from\na cited source and to attribute its own answers. Experiments with Qwen2.5-7B\nand 3B show that Active Indexing consistently outperforms Passive Indexing\nacross all tasks and models, with citation precision gains up to 30.2 percent.\nOur ablation studies reveal that performance continues to improve as we scale\nthe amount of augmented data, showing a clear upward trend even at 16 times the\noriginal token count."}
{"id": "2506.17638", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.17638", "abs": "https://arxiv.org/abs/2506.17638", "authors": ["Yanzhou Mu", "Rong Wang", "Juan Zhai", "Chunrong Fang", "Xiang Chen", "Zhiyuan Peng", "Peiran Yang", "Ruixiang Qian", "Shaoyu Yang", "Zhenyu Chen"], "title": "Deep Learning Framework Testing via Model Mutation: How Far Are We?", "comment": "27 pages, 9 figures", "summary": "Deep Learning (DL) frameworks are a fundamental component of DL development.\nTherefore, the detection of DL framework defects is important and challenging.\nAs one of the most widely adopted DL testing techniques, model mutation has\nrecently gained significant attention. In this study, we revisit the defect\ndetection ability of existing mutation-based testing methods and investigate\nthe factors that influence their effectiveness. To begin with, we reviewed\nexisting methods and observed that many of them mutate DL models (e.g.,\nchanging their parameters) without any customization, ignoring the unique\nchallenges in framework testing. Another issue with these methods is their\nlimited effectiveness, characterized by a high rate of false positives caused\nby illegal mutations arising from the use of generic, non-customized mutation\noperators. Moreover, we tracked the defects identified by these methods and\ndiscovered that most of them were ignored by developers. Motivated by these\nobservations, we investigate the effectiveness of existing mutation-based\ntesting methods in detecting important defects that have been authenticated by\nframework developers. We begin by collecting defect reports from three popular\nframeworks and classifying them based on framework developers' ratings to build\na comprehensive dataset. We then perform an in-depth analysis to uncover\nvaluable insights. Based on our findings, we propose optimization strategies to\naddress the shortcomings of existing approaches. Following these optimizations,\nwe identified seven new defects, four of which were confirmed by developers as\nhigh-priority issues, with three resolved. In summary, we identified 39 unique\ndefects across just 23 models, of which 31 were confirmed by developers, and\neight have been fixed."}
{"id": "2506.17317", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2506.17317", "abs": "https://arxiv.org/abs/2506.17317", "authors": ["Liuhuo Wan", "Chuan Yan", "Mark Huasong Meng", "Kailong Wang", "Haoyu Wang", "Guangdong Bai", "Jin Song Dong"], "title": "Beyond the Scope: Security Testing of Permission Management in Team Workspace", "comment": null, "summary": "Nowadays team workspaces are widely adopted for multi-user collaboration and\ndigital resource management. To further broaden real-world applications,\nmainstream team workspaces platforms, such as Google Workspace and Microsoft\nOneDrive, allow third-party applications (referred to as add-ons) to be\nintegrated into their workspaces, significantly extending the functionality of\nteam workspaces. The powerful multi-user collaboration capabilities and\nintegration of add-ons make team workspaces a central hub for managing shared\nresources and protecting them against unauthorized access. Due to the\ncollaboration features of team workspaces, add-ons involved in collaborations\nmay bypass the permission isolation enforced by the administrator, unlike in\nsingle-user permission management.\n  This paper aims to investigate the permission management landscape of team\nworkspaces add-ons. To this end, we perform an in-depth analysis of the\nenforced access control mechanism inherent in this ecosystem, considering both\nmulti-user and cross-app features. We identify three potential security risks\nthat can be exploited to cause permission escalation. We then systematically\nreveal the landscape of permission escalation risks in the current ecosystem.\nSpecifically, we propose an automated tool, TAI, to systematically test all\npossible interactions within this ecosystem. Our evaluation reveals that\npermission escalation vulnerabilities are widespread in this ecosystem, with 41\ninteractions identified as problematic. Our findings should raise an alert to\nboth the team workspaces platforms and third-party developers."}
{"id": "2506.17589", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17589", "abs": "https://arxiv.org/abs/2506.17589", "authors": ["Bowen Wang"], "title": "Taming the Untamed: Graph-Based Knowledge Retrieval and Reasoning for MLLMs to Conquer the Unknown", "comment": null, "summary": "The real value of knowledge lies not just in its accumulation, but in its\npotential to be harnessed effectively to conquer the unknown. Although recent\nmultimodal large language models (MLLMs) exhibit impressing multimodal\ncapabilities, they often fail in rarely encountered domain-specific tasks due\nto limited relevant knowledge. To explore this, we adopt visual game cognition\nas a testbed and select Monster Hunter: World as the target to construct a\nmultimodal knowledge graph (MH-MMKG), which incorporates multi-modalities and\nintricate entity relations. We also design a series of challenging queries\nbased on MH-MMKG to evaluate the models' ability for complex knowledge\nretrieval and reasoning. Furthermore, we propose a multi-agent retriever that\nenables a model to autonomously search relevant knowledge without additional\ntraining. Experimental results show that our approach significantly enhances\nthe performance of MLLMs, providing a new perspective on multimodal\nknowledge-augmented reasoning and laying a solid foundation for future\nresearch."}
{"id": "2506.17642", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.17642", "abs": "https://arxiv.org/abs/2506.17642", "authors": ["Shaoyu Yang", "Chunrong Fang", "Haifeng Lin", "Xiang Chen", "Zhenyu Chen"], "title": "May the Feedback Be with You! Unlocking the Power of Feedback-Driven Deep Learning Framework Fuzzing via LLMs", "comment": null, "summary": "Artificial Intelligence (AI) Infrastructures, represented by Deep Learning\n(DL) frameworks, have served as fundamental DL systems over the last decade.\nHowever, the bugs in DL frameworks could lead to catastrophic consequences in\nsome critical scenarios (e.g., healthcare and autonomous driving). A simple yet\neffective way to find bugs in DL frameworks is fuzz testing (Fuzzing).\nUnfortunately, existing fuzzing techniques have not comprehensively considered\nmultiple types of feedback. Additionally, they analyze feedback in a\ncoarse-grained manner, such as mutating the test cases only according to\nwhether the coverage increases. Recently, researchers introduced Large Language\nModels (LLMs) into fuzzing. However, current LLM-based fuzzing techniques only\nfocus on using LLMs to generate test cases while overlooking their potential to\nanalyze feedback information, failing to create more valid and diverse test\ncases. To fill this gap, we propose FUEL to break the seal of Feedback-driven\nfuzzing for DL frameworks. The backbone of FUEL comprises two LLM-based agents,\nnamely analysis LLM and generation LLM. Analysis LLM agent infers analysis\nsummaries from feedback information, while the generation LLM agent creates\ntests guided by these analysis summaries. So far, FUEL has detected 104 bugs\nfor PyTorch and TensorFlow, with 93 confirmed as new bugs, 47 already fixed,\nand 5 assigned with CVE IDs. Our work indicates that considering multiple types\nof feedback is beneficial to fuzzing performance, and leveraging LLMs to\nanalyze feedback information is a promising direction. Our artifact is\navailable at https://github.com/NJU-iSE/FUEL"}
{"id": "2506.17318", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17318", "abs": "https://arxiv.org/abs/2506.17318", "authors": ["Atharv Singh Patlan", "Ashwin Hebbar", "Pramod Viswanath", "Prateek Mittal"], "title": "Context manipulation attacks : Web agents are susceptible to corrupted memory", "comment": "10 pages, 6 figures", "summary": "Autonomous web navigation agents, which translate natural language\ninstructions into sequences of browser actions, are increasingly deployed for\ncomplex tasks across e-commerce, information retrieval, and content discovery.\nDue to the stateless nature of large language models (LLMs), these agents rely\nheavily on external memory systems to maintain context across interactions.\nUnlike centralized systems where context is securely stored server-side, agent\nmemory is often managed client-side or by third-party applications, creating\nsignificant security vulnerabilities. This was recently exploited to attack\nproduction systems.\n  We introduce and formalize \"plan injection,\" a novel context manipulation\nattack that corrupts these agents' internal task representations by targeting\nthis vulnerable context. Through systematic evaluation of two popular web\nagents, Browser-use and Agent-E, we show that plan injections bypass robust\nprompt injection defenses, achieving up to 3x higher attack success rates than\ncomparable prompt-based attacks. Furthermore, \"context-chained injections,\"\nwhich craft logical bridges between legitimate user goals and attacker\nobjectives, lead to a 17.7% increase in success rate for privacy exfiltration\ntasks. Our findings highlight that secure memory handling must be a first-class\nconcern in agentic systems."}
{"id": "2506.17644", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17644", "abs": "https://arxiv.org/abs/2506.17644", "authors": ["Zimo Ji", "Daoyuan Wu", "Wenyuan Jiang", "Pingchuan Ma", "Zongjie Li", "Shuai Wang"], "title": "Measuring and Augmenting Large Language Models for Solving Capture-the-Flag Challenges", "comment": null, "summary": "Capture-the-Flag (CTF) competitions are crucial for cybersecurity education\nand training. As large language models (LLMs) evolve, there is increasing\ninterest in their ability to automate CTF challenge solving. For example, DARPA\nhas organized the AIxCC competition since 2023 to advance AI-powered automated\noffense and defense. However, this demands a combination of multiple abilities,\nfrom knowledge to reasoning and further to actions. In this paper, we highlight\nthe importance of technical knowledge in solving CTF problems and deliberately\nconstruct a focused benchmark, CTFKnow, with 3,992 questions to measure LLMs'\nperformance in this core aspect. Our study offers a focused and innovative\nmeasurement of LLMs' capability in understanding CTF knowledge and applying it\nto solve CTF challenges. Our key findings reveal that while LLMs possess\nsubstantial technical knowledge, they falter in accurately applying this\nknowledge to specific scenarios and adapting their strategies based on feedback\nfrom the CTF environment.\n  Based on insights derived from this measurement study, we propose CTFAgent, a\nnovel LLM-driven framework for advancing CTF problem-solving. CTFAgent\nintroduces two new modules: two-stage Retrieval Augmented Generation (RAG) and\ninteractive Environmental Augmentation, which enhance LLMs' technical knowledge\nand vulnerability exploitation on CTF, respectively. Our experimental results\nshow that, on two popular CTF datasets, CTFAgent both achieves over 80%\nperformance improvement. Moreover, in the recent picoCTF2024 hosted by CMU,\nCTFAgent ranked in the top 23.6% of nearly 7,000 participating teams. This\nreflects the benefit of our measurement study and the potential of our\nframework in advancing LLMs' capabilities in CTF problem-solving."}
{"id": "2506.17647", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.17647", "abs": "https://arxiv.org/abs/2506.17647", "authors": ["Yixian Qi", "Jiajun Jiang", "Fengjie Li", "Bowen Chen", "Hongyu Zhang", "Junjie Chen"], "title": "Improving Compiler Bug Isolation by Leveraging Large Language Models", "comment": "12 pages, 7 figures", "summary": "Compilers play a foundational role in building reliable software systems, and\nbugs within them can lead to catastrophic consequences. The compilation process\ntypically involves hundreds of files, making traditional automated bug\nisolation techniques inapplicable due to scalability or effectiveness issues.\nCurrent mainstream compiler bug localization techniques have limitations in\ntest program mutation and resource consumption. Inspired by the recent advances\nof pre-trained Large Language Models (LLMs), we propose an innovative approach\nnamed AutoCBI, which (1) uses LLMs to summarize compiler file functions and (2)\nemploys specialized prompts to guide LLM in reordering suspicious file\nrankings. This approach leverages four types of information: the failing test\nprogram, source file function summaries, lists of suspicious files identified\nthrough analyzing test coverage, as well as compilation configurations with\nrelated output messages, resulting in a refined ranking of suspicious files.\nOur evaluation of AutoCBI against state-of-the-art approaches (DiWi, RecBi and\nFuseFL) on 120 real-world bugs from the widely-used GCC and LLVM compilers\ndemonstrates its effectiveness. Specifically, AutoCBI isolates 66.67%/69.23%,\n300%/340%, and 100%/57.14% more bugs than RecBi, DiWi, and FuseFL,\nrespectively, in the Top-1 ranked results for GCC/LLVM. Additionally, the\nablation study underscores the significance of each component in our approach."}
{"id": "2506.17329", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17329", "abs": "https://arxiv.org/abs/2506.17329", "authors": ["Pedro H. Lui", "Lucas P. Siqueira", "Juliano F. Kazienko", "Vagner E. Quincozes", "Silvio E. Quincozes", "Daniel Welfer"], "title": "On the Performance of Cyber-Biomedical Features for Intrusion Detection in Healthcare 5.0", "comment": "12 pages, 7 figures, conference", "summary": "Healthcare 5.0 integrates Artificial Intelligence (AI), the Internet of\nThings (IoT), real-time monitoring, and human-centered design toward\npersonalized medicine and predictive diagnostics. However, the increasing\nreliance on interconnected medical technologies exposes them to cyber threats.\nMeanwhile, current AI-driven cybersecurity models often neglect biomedical\ndata, limiting their effectiveness and interpretability. This study addresses\nthis gap by applying eXplainable AI (XAI) to a Healthcare 5.0 dataset that\nintegrates network traffic and biomedical sensor data. Classification outputs\nindicate that XGBoost achieved 99% F1-score for benign and data alteration, and\n81% for spoofing. Explainability findings reveal that network data play a\ndominant role in intrusion detection whereas biomedical features contributed to\nspoofing detection, with temperature reaching a Shapley values magnitude of\n0.37."}
{"id": "2506.17667", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17667", "abs": "https://arxiv.org/abs/2506.17667", "authors": ["Lintao Wang", "Encheng Su", "Jiaqi Liu", "Pengze Li", "Peng Xia", "Jiabei Xiao", "Wenlong Zhang", "Xinnan Dai", "Xi Chen", "Yuan Meng", "Mingyu Ding", "Lei Bai", "Wanli Ouyang", "Shixiang Tang", "Aoran Wang", "Xinzhu Ma"], "title": "PhysUniBench: An Undergraduate-Level Physics Reasoning Benchmark for Multimodal Models", "comment": null, "summary": "Physics problem-solving is a challenging domain for large AI models,\nrequiring integration of conceptual understanding, mathematical reasoning, and\ninterpretation of physical diagrams. Current evaluation methodologies show\nnotable limitations in capturing the breadth and complexity of\nundergraduate-level physics, underscoring the need for more rigorous\nassessments. To this end, we present PhysUniBench, a large-scale multimodal\nbenchmark designed to evaluate and improve the reasoning capabilities of\nmultimodal large language models (MLLMs) specifically on undergraduate-level\nphysics problems. PhysUniBench consists of 3,304 physics questions spanning 8\nmajor sub-disciplines of physics, each accompanied by one visual diagrams. The\nbenchmark includes both open-ended and multiple-choice questions,\nsystematically curated and difficulty-rated through an iterative\nmodel-in-the-loop process. The benchmark's construction involved a rigorous\nmulti-stage process, including multiple roll-outs, expert-level evaluation,\nautomated filtering of easily solved problems, and a nuanced difficulty grading\nsystem with five levels. Through extensive experiments, we observe that current\nstate-of-the-art models encounter substantial challenges in physics reasoning.\nFor example, GPT-4o mini achieves only about 34.2\\% accuracy in the proposed\nPhysUniBench. These results highlight that current MLLMs struggle with advanced\nphysics reasoning, especially on multi-step problems and those requiring\nprecise diagram interpretation. By providing a broad and rigorous assessment\ntool, PhysUniBench aims to drive progress in AI for Science, encouraging the\ndevelopment of models with stronger physical reasoning, problem-solving skills,\nand multimodal understanding. The benchmark and evaluation scripts are\navailable at https://prismax-team.github.io/PhysUniBenchmark/."}
{"id": "2506.17772", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.17772", "abs": "https://arxiv.org/abs/2506.17772", "authors": ["Haoran Xue", "Gias Uddin", "Song Wang"], "title": "PAGENT: Learning to Patch Software Engineering Agents", "comment": null, "summary": "LLM Agents produce patches automatically to resolve an issue. However, they\ncan generate inaccurate patches. Little is known about the root causes behind\nthose failed patches or how those could be fixed. This paper reports an\nempirical study of the failed patches generated by seven top LLM code agents.\nWe collected 114 issues from the SWE-bench Lite dataset that remained\nunresolved across the agents. The seven agents produced a total of 769 failed\npatches for those issues, which we checked with a combination of GPT-4o and\nmanual analysis. We present a taxonomy of the failure reasons across the\npatches. The taxonomy contains six categories, with several sub-categories\nunder each category. For example, a frequently observed category is the\ninability of an LLM to correctly infer/produce the appropriate variable type in\nthe produced patch. As a first step towards addressing such type-related\nerrors, we designed PAGENT (Patch Agent). PAGENT utilizes program analysis\ntechniques like CFG creation and exploration to infer the type of information\nof a patch. PAGENT does this by applying repository-level static code analysis\ntechniques. Then, PAGENT refines the inferred type by further utilizing an\nLLM-based inference technique. We tested PAGENT on all 127 type-related failed\npatches from the top three agents in our study. PAGENT could fix 29 of the 127\nfailed patches."}
{"id": "2506.17336", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2506.17336", "abs": "https://arxiv.org/abs/2506.17336", "authors": ["Yubeen Bae", "Minchan Kim", "Jaejin Lee", "Sangbum Kim", "Jaehyung Kim", "Yejin Choi", "Niloofar Mireshghallah"], "title": "Privacy-Preserving LLM Interaction with Socratic Chain-of-Thought Reasoning and Homomorphically Encrypted Vector Databases", "comment": "29 pages", "summary": "Large language models (LLMs) are increasingly used as personal agents,\naccessing sensitive user data such as calendars, emails, and medical records.\nUsers currently face a trade-off: They can send private records, many of which\nare stored in remote databases, to powerful but untrusted LLM providers,\nincreasing their exposure risk. Alternatively, they can run less powerful\nmodels locally on trusted devices. We bridge this gap. Our Socratic\nChain-of-Thought Reasoning first sends a generic, non-private user query to a\npowerful, untrusted LLM, which generates a Chain-of-Thought (CoT) prompt and\ndetailed sub-queries without accessing user data. Next, we embed these\nsub-queries and perform encrypted sub-second semantic search using our\nHomomorphically Encrypted Vector Database across one million entries of a\nsingle user's private data. This represents a realistic scale of personal\ndocuments, emails, and records accumulated over years of digital activity.\nFinally, we feed the CoT prompt and the decrypted records to a local language\nmodel and generate the final response. On the LoCoMo long-context QA benchmark,\nour hybrid framework, combining GPT-4o with a local Llama-3.2-1B model,\noutperforms using GPT-4o alone by up to 7.1 percentage points. This\ndemonstrates a first step toward systems where tasks are decomposed and split\nbetween untrusted strong LLMs and weak local ones, preserving user privacy."}
{"id": "2506.17697", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17697", "abs": "https://arxiv.org/abs/2506.17697", "authors": ["Bohan Tang", "Dezhao Luo", "Jingxuan Chen", "Shaogang Gong", "Jianye Hao", "Jun Wang", "Kun Shao"], "title": "Beyond Syntax: Action Semantics Learning for App Agents", "comment": null, "summary": "The advent of Large Language Models (LLMs) enables the rise of App agents\nthat interpret user intent and operate smartphone Apps through actions such as\nclicking and scrolling. While prompt-based solutions with closed LLM APIs show\npromising ability, they incur heavy compute costs and external API dependency.\nFine-tuning smaller open-source LLMs solves these limitations. However, current\nfine-tuning methods use a syntax learning paradigm that forces agents to\nreproduce exactly the ground truth action strings, leading to\nout-of-distribution (OOD) vulnerability. To fill this gap, we propose Action\nSemantics Learning (ASL), a novel learning framework, where the learning\nobjective is capturing the semantics of the ground truth actions. Specifically,\ninspired by the programming language theory, we define the action semantics for\nApp agents as the state transition induced by the action in the user interface.\nWith this insight, ASL employs a novel SEmantic Estimator (SEE) to compute a\nsemantic reward to train the App agents in generating actions aligned with the\nsemantics of ground truth actions, even when the syntactic forms differ. To\nsupport the effectiveness of ASL, we theoretically demonstrate the superior\nrobustness of ASL for the OOD problem compared with the existing syntax\nlearning paradigm. Extensive experiments on offline and online smartphone App\noperation benchmarks show that ASL significantly improves the accuracy and\ngeneralisation of App agents over existing methods."}
{"id": "2506.17798", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2506.17798", "abs": "https://arxiv.org/abs/2506.17798", "authors": ["Wang Lingxiang", "Quanzhi Fu", "Wenjia Song", "Gelei Deng", "Yi Liu", "Dan Williams", "Ying Zhang"], "title": "SAVANT: Vulnerability Detection in Application Dependencies through Semantic-Guided Reachability Analysis", "comment": null, "summary": "The integration of open-source third-party library dependencies in Java\ndevelopment introduces significant security risks when these libraries contain\nknown vulnerabilities. Existing Software Composition Analysis (SCA) tools\nstruggle to effectively detect vulnerable API usage from these libraries due to\nlimitations in understanding API usage semantics and computational challenges\nin analyzing complex codebases, leading to inaccurate vulnerability alerts that\nburden development teams and delay critical security fixes.\n  To address these challenges, we proposed SAVANT by leveraging two insights:\nproof-of-vulnerability test cases demonstrate how vulnerabilities can be\ntriggered in specific contexts, and Large Language Models (LLMs) can understand\ncode semantics. SAVANT combines semantic preprocessing with LLM-powered context\nanalysis for accurate vulnerability detection. SAVANT first segments source\ncode into meaningful blocks while preserving semantic relationships, then\nleverages LLM-based reflection to analyze API usage context and determine\nactual vulnerability impacts. Our evaluation on 55 real-world applications\nshows that SAVANT achieves 83.8% precision, 73.8% recall, 69.0% accuracy, and\n78.5% F1-score, outperforming state-of-the-art SCA tools."}
{"id": "2506.17349", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2506.17349", "abs": "https://arxiv.org/abs/2506.17349", "authors": ["Akarsh K Nair", "Shanik Hubert Satheesh Kumar.", "Deepti Gupta"], "title": "AndroIDS : Android-based Intrusion Detection System using Federated Learning", "comment": null, "summary": "The exponential growth of android-based mobile IoT systems has significantly\nincreased the susceptibility of devices to cyberattacks, particularly in smart\nhomes, UAVs, and other connected mobile environments. This article presents a\nfederated learning-based intrusion detection framework called AndroIDS that\nleverages system call traces as a personalized and privacy-preserving data\nsource. Unlike conventional centralized approaches, the proposed method enables\ncollaborative anomaly detection without sharing raw data, thus preserving user\nprivacy across distributed nodes. A generalized system call dataset was\ngenerated to reflect realistic android system behavior and serves as the\nfoundation for experimentation. Extensive evaluation demonstrates the\neffectiveness of the FL model under both IID and non-IID conditions, achieving\nan accuracy of 96.46 % and 92.87 %, and F1-scores of 89 % and 86 %,\nrespectively. These results highlight the models robustness to data\nheterogeneity, with only a minor performance drop in the non-IID case. Further,\na detailed comparison with centralized deep learning further illustrates\ntrade-offs in detection performance and deployment feasibility. Overall, the\nresults validate the practical applicability of the proposed approach for\nsecure and scalable intrusion detection in real-world mobile IoT scenarios."}
{"id": "2506.17784", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17784", "abs": "https://arxiv.org/abs/2506.17784", "authors": ["Song Wang", "Zhen Tan", "Zihan Chen", "Shuang Zhou", "Tianlong Chen", "Jundong Li"], "title": "AnyMAC: Cascading Flexible Multi-Agent Collaboration via Next-Agent Prediction", "comment": null, "summary": "Recent progress in large language model (LLM)-based multi-agent collaboration\nhighlights the power of structured communication in enabling collective\nintelligence. However, existing methods largely rely on static or graph-based\ninter-agent topologies, lacking the potential adaptability and flexibility in\ncommunication. In this work, we propose a new framework that rethinks\nmulti-agent coordination through a sequential structure rather than a graph\nstructure, offering a significantly larger topology space for multi-agent\ncommunication. Our method focuses on two key directions: (1) Next-Agent\nPrediction, which selects the most suitable agent role at each step, and (2)\nNext-Context Selection (NCS), which enables each agent to selectively access\nrelevant information from any previous step. Together, these components\nconstruct task-adaptive communication pipelines that support both role\nflexibility and global information flow. Extensive evaluations across multiple\nbenchmarks demonstrate that our approach achieves superior performance while\nsubstantially reducing communication overhead."}
{"id": "2506.17812", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.17812", "abs": "https://arxiv.org/abs/2506.17812", "authors": ["Noble Saji Mathews", "Meiyappan Nagappan"], "title": "Is Your Automated Software Engineer Trustworthy?", "comment": null, "summary": "Large Language Models (LLMs) are being increasingly used in software\nengineering tasks, with an increased focus on bug report resolution over the\npast year. However, most proposed systems fail to properly handle uncertain or\nincorrect inputs and outputs. Existing LLM-based tools and coding agents\nrespond to every issue and generate a patch for every case, even when the input\nis vague or their own output is incorrect. There are no mechanisms in place to\nabstain when confidence is low. This leads to unreliable behaviour, such as\nhallucinated code changes or responses based on vague issue reports. We\nintroduce BouncerBench, a benchmark that evaluates whether LLM-based software\nagents can refuse to act when inputs are ill-defined or refuse to respond when\ntheir own outputs are likely to be incorrect. Unlike prior benchmarks that\nimplicitly incentivize models to generate responses even when uncertain,\nBouncerBench aims to improve precision by targeting two overlooked failure\npoints: (1) vague or underspecified issue descriptions in tickets and (2)\nlogically or functionally incorrect code patches created by the system. It\nmeasures whether proposed systems can distinguish actionable issues from vague\ntickets and valid patches from untrustworthy ones. We also implement a basic\ninput and output bouncer, evaluating how well current LLMs can abstain when\nneeded. Our results show that most models fail to abstain from underspecified\ninputs or incorrect outputs. Hence, we conclude that there is significant room\nfor improvement before LLMs can be trusted to make correct decisions and\nrecommendations in real-world software engineering workflows. BouncerBench\nprovides a first step toward evaluating and building more cautious, trustworthy\ncode agents. The replication package, dataset, and leaderboard can be found at\nbouncerbench.com"}
{"id": "2506.17350", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17350", "abs": "https://arxiv.org/abs/2506.17350", "authors": ["Yinghao Wu", "Liyan Zhang"], "title": "CUBA: Controlled Untargeted Backdoor Attack against Deep Neural Networks", "comment": null, "summary": "Backdoor attacks have emerged as a critical security threat against deep\nneural networks in recent years. The majority of existing backdoor attacks\nfocus on targeted backdoor attacks, where trigger is strongly associated to\nspecific malicious behavior. Various backdoor detection methods depend on this\ninherent property and shows effective results in identifying and mitigating\nsuch targeted attacks. However, a purely untargeted attack in backdoor\nscenarios is, in some sense, self-weakening, since the target nature is what\nmakes backdoor attacks so powerful. In light of this, we introduce a novel\nConstrained Untargeted Backdoor Attack (CUBA), which combines the flexibility\nof untargeted attacks with the intentionality of targeted attacks. The\ncompromised model, when presented with backdoor images, will classify them into\nrandom classes within a constrained range of target classes selected by the\nattacker. This combination of randomness and determinedness enables the\nproposed untargeted backdoor attack to natively circumvent existing backdoor\ndefense methods. To implement the untargeted backdoor attack under controlled\nflexibility, we propose to apply logit normalization on cross-entropy loss with\nflipped one-hot labels. By constraining the logit during training, the\ncompromised model will show a uniform distribution across selected target\nclasses, resulting in controlled untargeted attack. Extensive experiments\ndemonstrate the effectiveness of the proposed CUBA on different datasets."}
{"id": "2506.17788", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA", "I.2.1; I.2.7"], "pdf": "https://arxiv.org/pdf/2506.17788", "abs": "https://arxiv.org/abs/2506.17788", "authors": ["Shahab Rahimirad", "Guven Gergerli", "Lucia Romero", "Angela Qian", "Matthew Lyle Olson", "Simon Stepputtis", "Joseph Campbell"], "title": "Bayesian Social Deduction with Graph-Informed Language Models", "comment": "32 pages, 10 figures. Under review", "summary": "Social reasoning - inferring unobservable beliefs and intentions from partial\nobservations of other agents - remains a challenging task for large language\nmodels (LLMs). We evaluate the limits of current reasoning language models in\nthe social deduction game Avalon and find that while the largest models\ndemonstrate strong performance, they require extensive test-time inference and\ndegrade sharply when distilled to smaller, real-time-capable variants. To\naddress this, we introduce a hybrid reasoning framework that externalizes\nbelief inference to a structured probabilistic model, while using an LLM for\nlanguage understanding and interaction. Our approach achieves competitive\nperformance with much larger models in Agent-Agent play and, notably, is the\nfirst language agent to defeat human players in a controlled study - achieving\na 67% win rate and receiving higher qualitative ratings than both reasoning\nbaselines and human teammates. We release code, models, and a dataset to\nsupport future work on social reasoning in LLM agents, which can be found at\nhttps://camp-lab-purdue.github.io/bayesian-social-deduction/"}
{"id": "2506.17833", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.17833", "abs": "https://arxiv.org/abs/2506.17833", "authors": ["Giorgio Amasanti", "Jasmin Jahic"], "title": "The Impact of AI-Generated Solutions on Software Architecture and Productivity: Results from a Survey Study", "comment": "Accepted for presentation at the International Workshop on\n  AI-Assisted Software Architecting (AISA 2025), colocated with the 19th\n  European Conference on Software Architecture (ECSA 2025), to be held 15-19\n  September 2025 in Limassol, Cyprus", "summary": "AI-powered software tools are widely used to assist software engineers.\nHowever, there is still a need to understand the productivity benefits of such\ntools for software engineers. In addition to short-term benefits, there is a\nquestion of how adopting AI-generated solutions affects the quality of software\nover time (e.g., maintainability and extendability).\n  To provide some insight on these questions, we conducted a survey among\nsoftware practitioners who use AI tools. Based on the data collected from our\nsurvey, we conclude that AI tools significantly increase the productivity of\nsoftware engineers. However, the productivity benefits of using AI tools reduce\nas projects become more complex. The results also show that there are no\nsignificant negative influences of adopting AI-generated solutions on software\nquality, as long as those solutions are limited to smaller code snippets.\nHowever, when solving larger and more complex problems, AI tools generate\nsolutions of a lower quality, indicating the need for architects to perform\nproblem decomposition and solution integration."}
{"id": "2506.17353", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17353", "abs": "https://arxiv.org/abs/2506.17353", "authors": ["Zongjie Li", "Daoyuan Wu", "Shuai Wang", "Zhendong Su"], "title": "Differentiation-Based Extraction of Proprietary Data from Fine-Tuned LLMs", "comment": "In Proceedings of the 2025 ACM SIGSAC Conference on Computer and\n  Communications Security (CCS'25), October 13-17, 2025, Taipei, Taiwan, China.\n  ACM, New York, NY, USA, 15 pages. https://doi.org/10.1145/3719027.3744856", "summary": "The increasing demand for domain-specific and human-aligned Large Language\nModels (LLMs) has led to the widespread adoption of Supervised Fine-Tuning\n(SFT) techniques. SFT datasets often comprise valuable instruction-response\npairs, making them highly valuable targets for potential extraction. This paper\nstudies this critical research problem for the first time. We start by formally\ndefining and formulating the problem, then explore various attack goals, types,\nand variants based on the unique properties of SFT data in real-world\nscenarios. Based on our analysis of extraction behaviors of direct extraction,\nwe develop a novel extraction method specifically designed for SFT models,\ncalled Differentiated Data Extraction (DDE), which exploits the confidence\nlevels of fine-tuned models and their behavioral differences from pre-trained\nbase models. Through extensive experiments across multiple domains and\nscenarios, we demonstrate the feasibility of SFT data extraction using DDE. Our\nresults show that DDE consistently outperforms existing extraction baselines in\nall attack settings. To counter this new attack, we propose a defense mechanism\nthat mitigates DDE attacks with minimal impact on model performance. Overall,\nour research reveals hidden data leak risks in fine-tuned LLMs and provides\ninsights for developing more secure models."}
{"id": "2506.17792", "categories": ["cs.AI", "cs.LO", "cs.SE"], "pdf": "https://arxiv.org/pdf/2506.17792", "abs": "https://arxiv.org/abs/2506.17792", "authors": ["Alexandros Evangelidis", "Gricel Vázquez", "Simos Gerasimou"], "title": "Efficient Strategy Synthesis for MDPs via Hierarchical Block Decomposition", "comment": null, "summary": "Software-intensive systems, such as software product lines and robotics,\nutilise Markov decision processes (MDPs) to capture uncertainty and analyse\nsequential decision-making problems. Despite the usefulness of conventional\npolicy synthesis methods, they fail to scale to large state spaces. Our\napproach addresses this issue and accelerates policy synthesis in large MDPs by\ndynamically refining the MDP and iteratively selecting the most fragile MDP\nregions for refinement. This iterative procedure offers a balance between\naccuracy and efficiency, as refinement occurs only when necessary. Through a\ncomprehensive empirical evaluation comprising diverse case studies and MDPs up\nto 1M states, we demonstrate significant performance improvements yielded by\nour approach compared to the leading probabilistic model checker PRISM (up to\n2x), thus offering a very competitive solution for real-world policy synthesis\ntasks in larger MDPs."}
{"id": "2506.17937", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17937", "abs": "https://arxiv.org/abs/2506.17937", "authors": ["Tommi Mikkonen", "Antero Taivalsaari"], "title": "Software Reuse in the Generative AI Era: From Cargo Cult Towards AI Native Software Engineering", "comment": null, "summary": "Software development is currently under a paradigm shift in which artificial\nintelligence and generative software reuse are taking the center stage in\nsoftware creation. Consequently, earlier software reuse practices and methods\nare rapidly being replaced by AI-assisted approaches in which developers place\ntheir trust on code that has been generated by artificial intelligence. This is\nleading to a new form of software reuse that is conceptually not all that\ndifferent from cargo cult development. In this paper we discuss the\nimplications of AI-assisted generative software reuse in the context of\nemerging \"AI native\" software engineering, bring forth relevant questions, and\ndefine a tentative research agenda and call to action for tackling some of the\ncentral issues associated with this approach."}
{"id": "2506.17371", "categories": ["cs.CR", "cs.NI"], "pdf": "https://arxiv.org/pdf/2506.17371", "abs": "https://arxiv.org/abs/2506.17371", "authors": ["Thilina Pathirana", "Ruxandra F. Olimid"], "title": "Secret Sharing in 5G-MEC: Applicability for joint Security and Dependability", "comment": "10 pages, 5 figures, Accepted to the proceedings of 22nd\n  International Conference on Privacy, Security, and Trust (PST2025)", "summary": "Multi-access Edge Computing (MEC), an enhancement of 5G, processes data\ncloser to its generation point, reducing latency and network load. However, the\ndistributed and edge-based nature of 5G-MEC presents privacy and security\nchallenges, including data exposure risks. Ensuring efficient manipulation and\nsecurity of sensitive data at the edge is crucial. To address these challenges,\nwe investigate the usage of threshold secret sharing in 5G-MEC storage, an\napproach that enhances both security and dependability. A (k,n) threshold\nsecret sharing scheme splits and stores sensitive data among n nodes, requiring\nat least k nodes for reconstruction. The solution ensures confidentiality by\nprotecting data against fewer than k colluding nodes and enhances availability\nby tolerating up to n-k failing nodes. This approach mitigates threats such as\nunauthorized access and node failures, whether accidental or intentional. We\nfurther discuss a method for selecting the convenient MEHs to store the shares,\nconsidering the MEHs' trustworthiness level as a main criterion. Although we\ndefine our proposal in the context of secret-shared data storage, it can be\nseen as an independent, standalone selection process for 5G-MEC trustworthy\nnode selection in other scenarios too."}
{"id": "2506.17834", "categories": ["cs.AI", "cs.HC", "I.2.6; H.5.2; I.2.7"], "pdf": "https://arxiv.org/pdf/2506.17834", "abs": "https://arxiv.org/abs/2506.17834", "authors": ["Carter Blair", "Kate Larson", "Edith Law"], "title": "Reflective Verbal Reward Design for Pluralistic Alignment", "comment": "9 pages, 3 figures, accepted to the IJCAI 2025 Human-Centred AI\n  track. Project repository at: https://osf.io/8yxf2/", "summary": "AI agents are commonly aligned with \"human values\" through reinforcement\nlearning from human feedback (RLHF), where a single reward model is learned\nfrom aggregated human feedback and used to align an agent's behavior. However,\nhuman values are not homogeneous--different people hold distinct and sometimes\nconflicting values. Aggregating feedback into a single reward model risks\ndisproportionately suppressing minority preferences. To address this, we\npresent a novel reward modeling approach for learning individualized reward\nmodels. Our approach uses a language model to guide users through reflective\ndialogues where they critique agent behavior and construct their preferences.\nThis personalized dialogue history, containing the user's reflections and\ncritiqued examples, is then used as context for another language model that\nserves as an individualized reward function (what we call a \"verbal reward\nmodel\") for evaluating new trajectories. In studies with 30 participants, our\nmethod achieved a 9-12% improvement in accuracy over non-reflective verbal\nreward models while being more sample efficient than traditional supervised\nlearning methods."}
{"id": "2506.17948", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.17948", "abs": "https://arxiv.org/abs/2506.17948", "authors": ["Mahzabin Tamanna", "Yash Chandrani", "Matthew Burrows", "Brandon Wroblewski", "Laurie Williams", "Dominik Wermke"], "title": "Build It Clean: Large-Scale Detection of Code Smells in Build Scripts", "comment": "12 pages, 5 tables, 2 figures", "summary": "Build scripts are files that automate the process of compiling source code,\nmanaging dependencies, running tests, and packaging software into deployable\nartifacts. These scripts are ubiquitous in modern software development\npipelines for streamlining testing and delivery. While developing build\nscripts, practitioners may inadvertently introduce code smells. Code smells are\nrecurring patterns of poor coding practices that may lead to build failures or\nincrease risk and technical debt. The goal of this study is to aid\npractitioners in avoiding code smells in build scripts through an empirical\nstudy of build scripts and issues on GitHub. We employed a mixed-methods\napproach, combining qualitative and quantitative analysis. We conducted a\nqualitative analysis of 2000 build-script-related GitHub issues. Next, we\ndeveloped a static analysis tool, Sniffer, to identify code smells in 5882\nbuild scripts of Maven, Gradle, CMake, and Make files, collected from 4877\nopen-source GitHub repositories. We identified 13 code smell categories, with a\ntotal of 10,895 smell occurrences, where 3184 were in Maven, 1214 in Gradle,\n337 in CMake, and 6160 in Makefiles.\n  Our analysis revealed that Insecure URLs were the most prevalent code smell\nin Maven build scripts, while Hardcoded Paths/URLs were commonly observed in\nboth Gradle and CMake scripts. Wildcard Usage emerged as the most frequent\nsmell in Makefiles. The co-occurrence analysis revealed strong associations\nbetween specific smell pairs of Hardcoded Paths/URLs with Duplicates, and\nInconsistent Dependency Management with Empty or Incomplete Tags, indicating\npotential underlying issues in the build script structure and maintenance\npractices. Based on our findings, we recommend strategies to mitigate the\nexistence of code smells in build scripts to improve the efficiency,\nreliability, and maintainability of software projects."}
{"id": "2506.17446", "categories": ["cs.CR", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.17446", "abs": "https://arxiv.org/abs/2506.17446", "authors": ["Nesrine Benchoubane", "Eray Guven", "Gunes Karabulut Kurt"], "title": "Open Sky, Open Threats: Replay Attacks in Space Launch and Re-entry Phases", "comment": null, "summary": "This paper examines the effects of replay attacks on the integrity of both\nuplink and downlink communications during critical phases of spacecraft\ncommunication. By combining software-defined radios (SDRs) with a real-time\nchannel emulator, we replicate realistic attack conditions on the Orion\nspacecraft's communication systems in both launch and reentry. Our evaluation\nshows that, under replay attacks, the attacker's signal can overpower\nlegitimate transmissions, leading to a Signal to Noise Ratio (SNR) difference\nof up to -7.8 dB during reentry and -6.5 dB during launch. To mitigate these\nthreats, we propose a more secure receiver design incorporating a\nphase-coherency-dependent decision-directed (DD) equalizer with a narrowed\nphase-locked loop (PLL) bandwidth. This configuration enhances resilience by\nmaking synchronization more sensitive to phase distortions caused by replay\ninterference."}
{"id": "2506.17846", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17846", "abs": "https://arxiv.org/abs/2506.17846", "authors": ["Elija Perrier"], "title": "Out of Control -- Why Alignment Needs Formal Control Theory (and an Alignment Control Stack)", "comment": "Under review for Neurips 2025", "summary": "This position paper argues that formal optimal control theory should be\ncentral to AI alignment research, offering a distinct perspective from\nprevailing AI safety and security approaches. While recent work in AI safety\nand mechanistic interpretability has advanced formal methods for alignment,\nthey often fall short of the generalisation required of control frameworks for\nother technologies. There is also a lack of research into how to render\ndifferent alignment/control protocols interoperable. We argue that by recasting\nalignment through principles of formal optimal control and framing alignment in\nterms of hierarchical stack from physical to socio-technical layers according\nto which controls may be applied we can develop a better understanding of the\npotential and limitations for controlling frontier models and agentic AI\nsystems. To this end, we introduce an Alignment Control Stack which sets out a\nhierarchical layered alignment stack, identifying measurement and control\ncharacteristics at each layer and how different layers are formally\ninteroperable. We argue that such analysis is also key to the assurances that\nwill be needed by governments and regulators in order to see AI technologies\nsustainably benefit the community. Our position is that doing so will bridge\nthe well-established and empirically validated methods of optimal control with\npractical deployment considerations to create a more comprehensive alignment\nframework, enhancing how we approach safety and reliability for advanced AI\nsystems."}
{"id": "2506.18050", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.18050", "abs": "https://arxiv.org/abs/2506.18050", "authors": ["Lyuye Zhang", "Jian Zhang", "Kaixuan Li", "Chong Wang", "Chengwei Liu", "Jiahui Wu", "Sen Chen", "Yaowen Zheng", "Yang Liu"], "title": "VFArchē: A Dual-Mode Framework for Locating Vulnerable Functions in Open-Source Software", "comment": "15 pages", "summary": "Software Composition Analysis (SCA) has become pivotal in addressing\nvulnerabilities inherent in software project dependencies. In particular,\nreachability analysis is increasingly used in Open-Source Software (OSS)\nprojects to identify reachable vulnerabilities (e.g., CVEs) through call\ngraphs, enabling a focus on exploitable risks. Performing reachability analysis\ntypically requires the vulnerable function (VF) to track the call chains from\ndownstream applications. However, such crucial information is usually\nunavailable in modern vulnerability databases like NVD. While directly\nextracting VF from modified functions in vulnerability patches is intuitive,\npatches are not always available. Moreover, our preliminary study shows that\nover 26% of VF do not exist in the modified functions. Meanwhile, simply\nignoring patches to search vulnerable functions suffers from overwhelming\nnoises and lexical gaps between descriptions and source code. Given that almost\nhalf of the vulnerabilities are equipped with patches, a holistic solution that\nhandles both scenarios with and without patches is required. To meet real-world\nneeds and automatically localize VF, we present VFArch\\=e, a dual-mode approach\ndesigned for disclosed vulnerabilities, applicable in scenarios with or without\navailable patch links. The experimental results of VFArch\\=e on our constructed\nbenchmark dataset demonstrate significant efficacy regarding three metrics,\nachieving 1.3x and 1.9x Mean Reciprocal Rank over the best baselines for\nPatch-present and Patch-absent modes, respectively. Moreover, VFArch\\=e has\nproven its applicability in real-world scenarios by successfully locating VF\nfor 43 out of 50 latest vulnerabilities with reasonable efforts and\nsignificantly reducing 78-89% false positives of SCA tools."}
{"id": "2506.17504", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2506.17504", "abs": "https://arxiv.org/abs/2506.17504", "authors": ["Hinata Nishino", "Kazumasa Omote", "Keita Emura"], "title": "A Smart Contract-based Non-Transferable Signature Verification System using Nominative Signatures", "comment": "An extended abstract appeared at the 20th Asia Joint Conference on\n  Information Security (AsiaJCIS) 2025", "summary": "Nominative signatures allow us to indicate who can verify a signature, and\nthey can be employed to construct a non-transferable signature verification\nsystem that prevents the signature verification by a third party in unexpected\nsituations. For example, this system can prevent IOU/loan certificate\nverification in unexpected situations. However, nominative signatures\nthemselves do not allow the verifier to check whether the funds will be\ntransferred in the future or have been transferred.It would be desirable to\nverify the fact simultaneously when the system involves a certain money\ntransfer such as cryptocurrencies/cryptoassets. In this paper, we propose a\nsmart contract-based non-transferable signature verification system using\nnominative signatures. We pay attention to the fact that the invisibility,\nwhich is a security requirement to be held for nominative signatures, allows us\nto publish nominative signatures on the blockchain. Our system can verify\nwhether a money transfer actually will take place, in addition to indicating\nwho can verify a signature. We transform the Hanaoka-Schuldt nominative\nsignature scheme (ACNS 2011, IEICE Trans. 2016) which is constructed over a\nsymmetric pairing to a scheme constructed over an asymmetric pairing, and\nevaluate the gas cost when a smart contract runs the verification algorithm of\nthe modified Hanaoka-Schuldt nominative signature scheme."}
{"id": "2506.17878", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17878", "abs": "https://arxiv.org/abs/2506.17878", "authors": ["Tam Trinh", "Manh Nguyen", "Truong-Son Hy"], "title": "Towards Robust Fact-Checking: A Multi-Agent System with Advanced Evidence Retrieval", "comment": null, "summary": "The rapid spread of misinformation in the digital era poses significant\nchallenges to public discourse, necessitating robust and scalable fact-checking\nsolutions. Traditional human-led fact-checking methods, while credible,\nstruggle with the volume and velocity of online content, prompting the\nintegration of automated systems powered by Large Language Models (LLMs).\nHowever, existing automated approaches often face limitations, such as handling\ncomplex claims, ensuring source credibility, and maintaining transparency. This\npaper proposes a novel multi-agent system for automated fact-checking that\nenhances accuracy, efficiency, and explainability. The system comprises four\nspecialized agents: an Input Ingestion Agent for claim decomposition, a Query\nGeneration Agent for formulating targeted subqueries, an Evidence Retrieval\nAgent for sourcing credible evidence, and a Verdict Prediction Agent for\nsynthesizing veracity judgments with human-interpretable explanations.\nEvaluated on benchmark datasets (FEVEROUS, HOVER, SciFact), the proposed system\nachieves a 12.3% improvement in Macro F1-score over baseline methods. The\nsystem effectively decomposes complex claims, retrieves reliable evidence from\ntrusted sources, and generates transparent explanations for verification\ndecisions. Our approach contributes to the growing field of automated\nfact-checking by providing a more accurate, efficient, and transparent\nverification methodology that aligns with human fact-checking practices while\nmaintaining scalability for real-world applications. Our source code is\navailable at https://github.com/HySonLab/FactAgent"}
{"id": "2506.18191", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.18191", "abs": "https://arxiv.org/abs/2506.18191", "authors": ["Masudul Hasan Masud Bhuiyan", "Gianluca De Stefano", "Giancarlo Pellegrino", "Cristian-Alexandru Staicu"], "title": "Call Me Maybe: Enhancing JavaScript Call Graph Construction using Graph Neural Networks", "comment": null, "summary": "Static analysis plays a key role in finding bugs, including security issues.\nA critical step in static analysis is building accurate call graphs that model\nfunction calls in a program. However, due to hard-to-analyze language features,\nexisting call graph construction algorithms for JavaScript are neither sound\nnor complete. Prior work shows that even advanced solutions produce false edges\nand miss valid ones. In this work, we assist these tools by identifying missed\ncall edges. Our main idea is to frame the problem as link prediction on full\nprogram graphs, using a rich representation with multiple edge types. Our\napproach, GRAPHIA, leverages recent advances in graph neural networks to model\nnon-local relationships between code elements. Concretely, we propose\nrepresenting JavaScript programs using a combination of syntactic- and\nsemantic-based edges. GRAPHIA can learn from imperfect labels, including static\ncall edges from existing tools and dynamic edges from tests, either from the\nsame or different projects. Because call graphs are sparse, standard machine\nlearning metrics like ROC are not suitable. Instead, we evaluate GRAPHIA by\nranking function definitions for each unresolved call site. We conduct a\nlarge-scale evaluation on 50 popular JavaScript libraries with 163K call edges\n(150K static and 13K dynamic). GRAPHIA builds program graphs with 6.6M\nstructural and 386K semantic edges. It ranks the correct target as the top\ncandidate in over 42% of unresolved cases and within the top 5 in 72% of cases,\nreducing the manual effort needed for analysis. Our results show that\nlearning-based methods can improve the recall of JavaScript call graph\nconstruction. To our knowledge, this is the first work to apply GNN-based link\nprediction to full multi-file program graphs for interprocedural analysis."}
{"id": "2506.17512", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2506.17512", "abs": "https://arxiv.org/abs/2506.17512", "authors": ["Julien Piet", "Vivian Fang", "Rishi Khare", "Vern Paxson", "Raluca Ada Popa", "David Wagner"], "title": "Semantic-Aware Parsing for Security Logs", "comment": null, "summary": "Security analysts struggle to quickly and efficiently query and correlate log\ndata due to the heterogeneity and lack of structure in real-world logs.\nExisting AI-based parsers focus on learning syntactic log templates but lack\nthe semantic interpretation needed for querying. Directly querying large\nlanguage models on raw logs is impractical at scale and vulnerable to prompt\ninjection attacks.\n  In this paper, we introduce Matryoshka, the first end-to-end system\nleveraging LLMs to automatically generate semantically-aware structured log\nparsers. Matryoshka combines a novel syntactic parser-employing precise regular\nexpressions rather than wildcards-with a completely new semantic parsing layer\nthat clusters variables and maps them into a queryable, contextually meaningful\nschema. This approach provides analysts with queryable and semantically rich\ndata representations, facilitating rapid and precise log querying without the\ntraditional burden of manual parser construction. Additionally, Matryoshka can\nmap the newly created fields to recognized attributes within the Open\nCybersecurity Schema Framework (OCSF), enabling interoperability.\n  We evaluate Matryoshka on a newly curated real-world log benchmark,\nintroducing novel metrics to assess how consistently fields are named and\nmapped across logs. Matryoshka's syntactic parser outperforms prior works, and\nthe semantic layer achieves an F1 score of 0.95 on realistic security queries.\nAlthough mapping fields to the extensive OCSF taxonomy remains challenging,\nMatryoshka significantly reduces manual effort by automatically extracting and\norganizing valuable fields, moving us closer to fully automated, AI-driven log\nanalytics."}
{"id": "2506.17900", "categories": ["cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2506.17900", "abs": "https://arxiv.org/abs/2506.17900", "authors": ["Cheng Ji", "Huaiying Luo"], "title": "Leveraging Large Language Model for Intelligent Log Processing and Autonomous Debugging in Cloud AI Platforms", "comment": "Accepted by 2025 8th International Conference on Advanced Electronic\n  Materials, Computers and Software Engineering (AEMCSE 2025)", "summary": "With the increasing complexity and rapid expansion of the scale of AI systems\nin cloud platforms, the log data generated during system operation is massive,\nunstructured, and semantically ambiguous, which brings great challenges to\nfault location and system self-repair. In order to solve this problem, this\npaper proposes an intelligent log processing and automatic debugging framework\nbased on Large Language Model (LLM), named Intelligent Debugger (LLM-ID). This\nmethod is extended on the basis of the existing pre-trained Transformer model,\nand integrates a multi-stage semantic inference mechanism to realize the\ncontext understanding of system logs and the automatic reconstruction of fault\nchains. Firstly, the system log is dynamically structured, and the unsupervised\nclustering and embedding mechanism is used to extract the event template and\nsemantic schema. Subsequently, the fine-tuned LLM combined with the multi-round\nattention mechanism to perform contextual reasoning on the log sequence to\ngenerate potential fault assumptions and root cause paths. Furthermore, this\npaper introduces a reinforcement learning-based policy-guided recovery planner,\nwhich is driven by the remediation strategy generated by LLM to support dynamic\ndecision-making and adaptive debugging in the cloud environment. Compared with\nthe existing rule engine or traditional log analysis system, the proposed model\nhas stronger semantic understanding ability, continuous learning ability and\nheterogeneous environment adaptability. Experiments on the cloud platform log\ndataset show that LLM-ID improves the fault location accuracy by 16.2%, which\nis significantly better than the current mainstream methods"}
{"id": "2506.18219", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.18219", "abs": "https://arxiv.org/abs/2506.18219", "authors": ["Ulrike M. Graetsch", "Rashina Hoda", "Hourieh Khalazjadeh", "Mojtaba Shahin", "John Grundy"], "title": "Managing Technical Debt in a Multidisciplinary Data Intensive Software Team: an Observational Case Study", "comment": "25 pages", "summary": "Context: There is an increase in the investment and development of\ndata-intensive (DI) solutions, systems that manage large amounts of data.\nWithout careful management, this growing investment will also grow associated\ntechnical debt (TD). Delivery of DI solutions requires a multidisciplinary\nskill set, but there is limited knowledge about how multidisciplinary teams\ndevelop DI systems and manage TD.\n  Objective: This research contributes empirical, practice based insights about\nmultidisciplinary DI team TD management practices.\n  Method: This research was conducted as an exploratory observation case study.\nWe used socio-technical grounded theory (STGT) for data analysis to develop\nconcepts and categories that articulate TD and TDs debt management practices.\n  Results: We identify TD that the DI team deals with, in particular technical\ndata components debt and pipeline debt. We explain how the team manages the TD,\nassesses TD, what TD treatments they consider and how they implement TD\ntreatments to fit sprint capacity constraints.\n  Conclusion: We align our findings to existing TD and TDM taxonomies, discuss\ntheir implications and highlight the need for new implementation patterns and\ntool support for multidisciplinary DI teams."}
{"id": "2506.17622", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2506.17622", "abs": "https://arxiv.org/abs/2506.17622", "authors": ["Shengchen Ling", "Yuefeng Du", "Yajin Zhou", "Lei Wu", "Cong Wang", "Xiaohua Jia", "Houmin Yan"], "title": "SoK: Stablecoin Designs, Risks, and the Stablecoin LEGO", "comment": null, "summary": "Stablecoins have become significant assets in modern finance, with a market\ncapitalization exceeding USD 246 billion (May 2025). Yet, despite their\nsystemic importance, a comprehensive and risk-oriented understanding of crucial\naspects like their design trade-offs, security dynamics, and interdependent\nfailure pathways often remains underdeveloped. This SoK confronts this gap\nthrough a large-scale analysis of 157 research studies, 95 active stablecoins,\nand 44 major security incidents. Our analysis establishes four pivotal\ninsights: 1) stability is best understood not an inherent property but an\nemergent, fragile state reliant on the interplay between market confidence and\ncontinuous liquidity; 2) stablecoin designs demonstrate trade-offs in risk\nspecialization instead of mitigation; 3) the widespread integration of yield\nmechanisms imposes a \"dual mandate\" that creates a systemic tension between the\ncore mission of stability and the high-risk financial engineering required for\ncompetitive returns; and 4) major security incidents act as acute \"evolutionary\npressures\", forging resilience by stress-testing designs and aggressively\nredefining the security frontier. We introduce the Stablecoin LEGO framework, a\nquantitative methodology mapping historical failures to current designs. Its\napplication reveals that a lower assessed risk strongly correlates with\nintegrating lessons from past incidents. We hope this provides a systematic\nfoundation for building, evaluating, and regulating more resilient stablecoins."}
{"id": "2506.17913", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17913", "abs": "https://arxiv.org/abs/2506.17913", "authors": ["Jinjie Wei", "Jiyao Liu", "Lihao Liu", "Ming Hu", "Junzhi Ning", "Mingcheng Li", "Weijie Yin", "Junjun He", "Xiao Liang", "Chao Feng", "Dingkang Yang"], "title": "Learning, Reasoning, Refinement: A Framework for Kahneman's Dual-System Intelligence in GUI Agents", "comment": null, "summary": "Graphical User Interface (GUI) agents have made significant progress in\nautomating digital tasks through the utilization of computer vision and\nlanguage models. Nevertheless, existing agent systems encounter notable\nlimitations. Firstly, they predominantly depend on trial and error decision\nmaking rather than progressive reasoning, thereby lacking the capability to\nlearn and adapt from interactive encounters. Secondly, these systems are\nassessed using overly simplistic single step accuracy metrics, which do not\nadequately reflect the intricate nature of real world GUI interactions. In this\npaper, we present CogniGUI, a cognitive framework developed to overcome these\nlimitations by enabling adaptive learning for GUI automation resembling\nhuman-like behavior. Inspired by Kahneman's Dual Process Theory, our approach\ncombines two main components: (1) an omni parser engine that conducts immediate\nhierarchical parsing of GUI elements through quick visual semantic analysis to\nidentify actionable components, and (2) a Group based Relative Policy\nOptimization (GRPO) grounding agent that assesses multiple interaction paths\nusing a unique relative reward system, promoting minimal and efficient\noperational routes. This dual-system design facilitates iterative ''exploration\nlearning mastery'' cycles, enabling the agent to enhance its strategies over\ntime based on accumulated experience. Moreover, to assess the generalization\nand adaptability of agent systems, we introduce ScreenSeek, a comprehensive\nbenchmark that includes multi application navigation, dynamic state\ntransitions, and cross interface coherence, which are often overlooked\nchallenges in current benchmarks. Experimental results demonstrate that\nCogniGUI surpasses state-of-the-art methods in both the current GUI grounding\nbenchmarks and our newly proposed benchmark."}
{"id": "2506.18289", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.18289", "abs": "https://arxiv.org/abs/2506.18289", "authors": ["Saurabhsingh Rajput", "Mootez Saad", "Tushar Sharma"], "title": "Tu(r)ning AI Green: Exploring Energy Efficiency Cascading with Orthogonal Optimizations", "comment": "In review", "summary": "AI's exponential growth intensifies computational demands and energy\nchallenges. While practitioners employ various optimization techniques, that we\nrefer as \"knobs\" in this paper, to tune model efficiency, these are typically\nafterthoughts and reactive ad-hoc changes applied in isolation without\nunderstanding their combinatorial effects on energy efficiency. This paper\nemphasizes on treating energy efficiency as the first-class citizen and as a\nfundamental design consideration for a compute-intensive pipeline. We show that\nstrategic selection across five AI pipeline phases (data, model, training,\nsystem, inference) creates cascading efficiency. Experimental validation shows\northogonal combinations reduce energy consumption by up to $94.6$% while\npreserving $95.95$% of the original F1 score of non-optimized pipelines. This\ncurated approach provides actionable frameworks for informed sustainable AI\nthat balance efficiency, performance, and environmental responsibility."}
{"id": "2506.17625", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2506.17625", "abs": "https://arxiv.org/abs/2506.17625", "authors": ["Pengzhen Ke", "Liang Feng Zhang", "Huaxiong Wang", "Li-Ping Wang"], "title": "List-Decodable Byzantine Robust PIR: Lower Communication Complexity, Higher Byzantine Tolerance, Smaller List Size", "comment": "Submitted to AsiaCrypt 2025", "summary": "Private Information Retrieval (PIR) is a privacy-preserving primitive in\ncryptography. Significant endeavors have been made to address the variant of\nPIR concerning the malicious servers. Among those endeavors, list-decodable\nByzantine robust PIR schemes may tolerate a majority of malicious responding\nservers that provide incorrect answers. In this paper, we propose two perfect\nlist-decodable BRPIR schemes. Our schemes are the first ones that can\nsimultaneously handle a majority of malicious responding servers, achieve a\ncommunication complexity of $o(n^{1/2})$ for a database of size n, and provide\na nontrivial estimation on the list sizes. Compared with the existing\nsolutions, our schemes attain lower communication complexity, higher byzantine\ntolerance, and smaller list size."}
{"id": "2506.17930", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.NE", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.17930", "abs": "https://arxiv.org/abs/2506.17930", "authors": ["Jianyu Wang", "Zhiqiang Hu", "Lidong Bing"], "title": "Evolving Prompts In-Context: An Open-ended, Self-replicating Perspective", "comment": "ICML 2025, and Code will be released at:\n  https://github.com/jianyu-cs/PromptQuine/", "summary": "We propose a novel prompt design paradigm that challenges conventional wisdom\nin large language model (LLM) prompting. While conventional wisdom prioritizes\nwell-crafted instructions and demonstrations for in-context learning (ICL), we\nshow that pruning random demonstrations into seemingly incoherent \"gibberish\"\ncan remarkably improve performance across diverse tasks. Notably, the\n\"gibberish\" always matches or surpasses state-of-the-art automatic prompt\noptimization techniques, achieving substantial gains regardless of LLM\nalignment. Nevertheless, discovering an effective pruning strategy is\nnon-trivial, as existing attribution methods and prompt compression algorithms\nfail to deliver robust results, let alone human intuition. In terms of this, we\npropose a self-discover prompt optimization framework, PromptQuine, an\nevolutionary search framework that automatically searches for the pruning\nstrategy by itself using only low-data regimes. Much like the emergent\ncomplexity in nature--such as symbiosis and self-organization--arising in\nresponse to resource constraints, our framework evolves and refines\nunconventional yet highly effective prompts by leveraging only the tokens\npresent within the context. We demonstrate its effectiveness across\nclassification, multi-choice question answering, generation and math reasoning\ntasks across LLMs, while achieving decent runtime efficiency. We hope our\nfindings can guide mechanistic studies on in-context learning, and provide a\ncall to action, to pave the way for more open-ended search algorithms for more\neffective LLM prompting."}
{"id": "2506.18315", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.18315", "abs": "https://arxiv.org/abs/2506.18315", "authors": ["Lehan He", "Zeren Chen", "Zhe Zhang", "Jing Shao", "Xiang Gao", "Lu Sheng"], "title": "Use Property-Based Testing to Bridge LLM Code Generation and Validation", "comment": null, "summary": "Large Language Models (LLMs) excel at code generation, but ensuring their\noutputs to be functionally correct, especially in complex programming tasks, is\na persistent challenge. While traditional Test-Driven Development (TDD) offers\na path for code refinement, its efficacy with LLMs is often undermined by the\nscarcity of high-quality test cases or the pitfalls of automated test\ngeneration, including biased tests or inaccurate output predictions that can\nmisdirect the correction process. This paper introduces Property-Generated\nSolver, a novel framework that leverages Property-Based Testing (PBT) to\nvalidate high-level program properties or invariants, instead of relying on\nspecific input-output examples. These properties are often simpler to define\nand verify than directly predicting exhaustive test oracles, breaking the\n\"cycle of self-deception\" where tests might share flaws with the code they are\nmeant to validate. Property-Generated Solver employs two collaborative\nLLM-based agents: a Generator dedicated to code generation and iterative\nrefinement, and a Tester that manages the PBT life-cycle and formulate\nsemantically rich feedback from property violations. The resulting\ncomprehensive and actionable feedback then guides the Generator in its\nrefinement efforts. By establishing PBT as the core validation engine within\nthis iterative, closed-loop paradigm, Property-Generated Solver provides a\nrobust mechanism for steering LLMs towards more correct and generalizable code.\nExtensive experimental results on multiple code generation benchmarks\ndemonstrate that Property-Generated Solver achieves substantial pass@1\nimprovements, ranging from 23.1% to 37.3% relative gains over established TDD\nmethods."}
{"id": "2506.17767", "categories": ["cs.CR", "cs.DC", "cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2506.17767", "abs": "https://arxiv.org/abs/2506.17767", "authors": ["Hsuan-Po Liu", "Hessam Mahdavifar"], "title": "A Locally Differential Private Coding-Assisted Succinct Histogram Protocol", "comment": null, "summary": "A succinct histogram captures frequent items and their frequencies across\nclients and has become increasingly important for large-scale,\nprivacy-sensitive machine learning applications. To develop a rigorous\nframework to guarantee privacy for the succinct histogram problem, local\ndifferential privacy (LDP) has been utilized and shown promising results. To\npreserve data utility under LDP, which essentially works by intentionally\nadding noise to data, error-correcting codes naturally emerge as a promising\ntool for reliable information collection. This work presents the first\npractical $(\\epsilon,\\delta)$-LDP protocol for constructing succinct histograms\nusing error-correcting codes. To this end, polar codes and their\nsuccessive-cancellation list (SCL) decoding algorithms are leveraged as the\nunderlying coding scheme. More specifically, our protocol introduces\nGaussian-based perturbations to enable efficient soft decoding. Experiments\ndemonstrate that our approach outperforms prior methods, particularly for items\nwith low true frequencies, while maintaining similar frequency estimation\naccuracy."}
{"id": "2506.17959", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17959", "abs": "https://arxiv.org/abs/2506.17959", "authors": ["Lizzy Farrugia", "Lilian M. Azzopardi", "Jeremy Debattista", "Charlie Abela"], "title": "medicX-KG: A Knowledge Graph for Pharmacists' Drug Information Needs", "comment": null, "summary": "The role of pharmacists is evolving from medicine dispensing to delivering\ncomprehensive pharmaceutical services within multidisciplinary healthcare\nteams. Central to this shift is access to accurate, up-to-date medicinal\nproduct information supported by robust data integration. Leveraging artificial\nintelligence and semantic technologies, Knowledge Graphs (KGs) uncover hidden\nrelationships and enable data-driven decision-making. This paper presents\nmedicX-KG, a pharmacist-oriented knowledge graph supporting clinical and\nregulatory decisions. It forms the semantic layer of the broader medicX\nplatform, powering predictive and explainable pharmacy services. medicX-KG\nintegrates data from three sources, including, the British National Formulary\n(BNF), DrugBank, and the Malta Medicines Authority (MMA) that addresses Malta's\nregulatory landscape and combines European Medicines Agency alignment with\npartial UK supply dependence. The KG tackles the absence of a unified national\ndrug repository, reducing pharmacists' reliance on fragmented sources. Its\ndesign was informed by interviews with practicing pharmacists to ensure\nreal-world applicability. We detail the KG's construction, including data\nextraction, ontology design, and semantic mapping. Evaluation demonstrates that\nmedicX-KG effectively supports queries about drug availability, interactions,\nadverse reactions, and therapeutic classes. Limitations, including missing\ndetailed dosage encoding and real-time updates, are discussed alongside\ndirections for future enhancements."}
{"id": "2506.18329", "categories": ["cs.SE", "D.2.8; C.4; I.2.7; I.2.6; I.5.1"], "pdf": "https://arxiv.org/pdf/2506.18329", "abs": "https://arxiv.org/abs/2506.18329", "authors": ["Elijah Zolduoarrati", "Sherlock A. Licorish", "Nigel Stanger"], "title": "Predictive Analytics for Collaborators Answers, Code Quality, and Dropout on Stack Overflow", "comment": "46 pages, 17 tables, 7 figures", "summary": "Previous studies that used data from Stack Overflow to develop predictive\nmodels often employed limited benchmarks of 3-5 models or adopted arbitrary\nselection methods. Despite being insightful, their limited scope suggests the\nneed to benchmark more models to avoid overlooking untested algorithms. Our\nstudy evaluates 21 algorithms across three tasks: predicting the number of\nquestion a user is likely to answer, their code quality violations, and their\ndropout status. We employed normalisation, standardisation, as well as\nlogarithmic and power transformations paired with Bayesian hyperparameter\noptimisation and genetic algorithms. CodeBERT, a pre-trained language model for\nboth natural and programming languages, was fine-tuned to classify user dropout\ngiven their posts (questions and answers) and code snippets. We found Bagging\nensemble models combined with standardisation achieved the highest R2 value\n(0.821) in predicting user answers. The Stochastic Gradient Descent regressor,\nfollowed by Bagging and Epsilon Support Vector Machine models, consistently\ndemonstrated superior performance to other benchmarked algorithms in predicting\nuser code quality across multiple quality dimensions and languages. Extreme\nGradient Boosting paired with log-transformation exhibited the highest F1-score\n(0.825) in predicting user dropout. CodeBERT was able to classify user dropout\nwith a final F1-score of 0.809, validating the performance of Extreme Gradient\nBoosting that was solely based on numerical data. Overall, our benchmarking of\n21 algorithms provides multiple insights. Researchers can leverage findings\nregarding the most suitable models for specific target variables, and\npractitioners can utilise the identified optimal hyperparameters to reduce the\ninitial search space during their own hyperparameter tuning processes."}
{"id": "2506.17795", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2506.17795", "abs": "https://arxiv.org/abs/2506.17795", "authors": ["Rachel Cazzola", "Cyrus Minwalla", "Calvin Chan", "Jim Plusquellic"], "title": "A TRNG Implemented using a Soft-Data Based Sponge Function within a Unified Strong PUF Architecture", "comment": null, "summary": "Hardware security primitives including True Random Number Generators (TRNG)\nand Physical Unclonable Functions (PUFs) are central components to establishing\na root of trust in microelectronic systems. In this paper, we propose a unified\nPUF-TRNG architecture that leverages a combination of the static entropy\navailable in a strong PUF called the shift-register, reconvergent-fanout (SiRF)\nPUF, and the dynamic entropy associated with random noise present in path delay\nmeasurements. The SiRF PUF uses an engineered netlist containing a large number\nof paths as the source of static entropy, and a time-to-digital-converter (TDC)\nas a high-resolution, embedded instrument for measuring path delays, where\nmeasurement noise serves as the source of dynamic entropy. A novel data\npostprocessing algorithm is proposed based on a modified duplex sponge\nconstruction. The sponge function operates on soft data, i.e., fixed point data\nvalues, to add entropy to the ensuing random bit sequences and to increase the\nbit generation rate. A postprocessing algorithm for reproducing PUF-generated\nencryption keys is also used in the TRNG to protect against temperature voltage\nattacks designed to subvert the random characteristics in the bit sequences.\nThe unified PUF-TRNG architecture is implemented across multiple instances of a\nZYBO Z7-10 FPGA board and extensively tested with NIST SP 800-22, NIST SP\n800-90B, AIS-31, and DieHarder test suites. Results indicate a stable and\nrobust TRNG design with excellent min-entropy and a moderate data rate."}
{"id": "2506.18019", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.18019", "abs": "https://arxiv.org/abs/2506.18019", "authors": ["Yuanchen Bei", "Weizhi Zhang", "Siwen Wang", "Weizhi Chen", "Sheng Zhou", "Hao Chen", "Yong Li", "Jiajun Bu", "Shirui Pan", "Yizhou Yu", "Irwin King", "Fakhri Karray", "Philip S. Yu"], "title": "Graphs Meet AI Agents: Taxonomy, Progress, and Future Opportunities", "comment": "20 pages, 7 figures", "summary": "AI agents have experienced a paradigm shift, from early dominance by\nreinforcement learning (RL) to the rise of agents powered by large language\nmodels (LLMs), and now further advancing towards a synergistic fusion of RL and\nLLM capabilities. This progression has endowed AI agents with increasingly\nstrong abilities. Despite these advances, to accomplish complex real-world\ntasks, agents are required to plan and execute effectively, maintain reliable\nmemory, and coordinate smoothly with other agents. Achieving these capabilities\ninvolves contending with ever-present intricate information, operations, and\ninteractions. In light of this challenge, data structurization can play a\npromising role by transforming intricate and disorganized data into\nwell-structured forms that agents can more effectively understand and process.\nIn this context, graphs, with their natural advantage in organizing, managing,\nand harnessing intricate data relationships, present a powerful data paradigm\nfor structurization to support the capabilities demanded by advanced AI agents.\nTo this end, this survey presents a first systematic review of how graphs can\nempower AI agents. Specifically, we explore the integration of graph techniques\nwith core agent functionalities, highlight notable applications, and identify\nprospective avenues for future research. By comprehensively surveying this\nburgeoning intersection, we hope to inspire the development of next-generation\nAI agents equipped to tackle increasingly sophisticated challenges with graphs.\nRelated resources are collected and continuously updated for the community in\nthe Github link."}
{"id": "2506.18359", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.18359", "abs": "https://arxiv.org/abs/2506.18359", "authors": ["Juanita Gomez", "Emily Lovell", "Stephanie Lieggi", "Alvaro A. Cardenas", "James Davis"], "title": "Recipe for Discovery: A Framework for Systematic Open Source Project Identification", "comment": null, "summary": "Open source software development, particularly within institutions such as\nuniversities and research laboratories, is often decentralized and difficult to\ntrack. Despite producing highly impactful tools in science, these efforts often\ngo unrecognized due to a lack of visibility and institutional awareness. This\npaper addresses the challenge of discovering, classifying, and analyzing open\nsource software projects developed across distributed institutional systems. We\npresent a framework for systematically identifying institutional affiliated\nrepositories, using the University of California (UC) system as a case study.\n  Using GitHub's REST API, we build a pipeline to discover relevant\nrepositories and extract meaningful metadata. We then propose and evaluate\nmultiple classification strategies, including both traditional machine learning\nmodels and large language models (LLMs), to distinguish affiliated projects\nfrom unrelated repositories and generate accurate insights into the academic\nopen source landscape. Our results show that the framework is effective at\nscale, discovering over 52,000 repositories and predicting institutional\naffiliation with high accuracy."}
{"id": "2506.17805", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.17805", "abs": "https://arxiv.org/abs/2506.17805", "authors": ["Md. Kamrul Hossain", "Walid Aljoby", "Anis Elgabli", "Ahmed M. Abdelmoniem", "Khaled A. Harras"], "title": "AdRo-FL: Informed and Secure Client Selection for Federated Learning in the Presence of Adversarial Aggregator", "comment": "17 pages", "summary": "Federated Learning (FL) enables collaborative learning without exposing\nclients' data. While clients only share model updates with the aggregator,\nstudies reveal that aggregators can infer sensitive information from these\nupdates. Secure Aggregation (SA) protects individual updates during\ntransmission; however, recent work demonstrates a critical vulnerability where\nadversarial aggregators manipulate client selection to bypass SA protections,\nconstituting a Biased Selection Attack (BSA). Although verifiable random\nselection prevents BSA, it precludes informed client selection essential for FL\nperformance. We propose Adversarial Robust Federated Learning (AdRo-FL), which\nsimultaneously enables: informed client selection based on client utility, and\nrobust defense against BSA maintaining privacy-preserving aggregation. AdRo-FL\nimplements two client selection frameworks tailored for distinct settings. The\nfirst framework assumes clients are grouped into clusters based on mutual\ntrust, such as different branches of an organization. The second framework\nhandles distributed clients where no trust relationships exist between them.\nFor the cluster-oriented setting, we propose a novel defense against BSA by (1)\nenforcing a minimum client selection quota from each cluster, supervised by a\ncluster-head in every round, and (2) introducing a client utility function to\nprioritize efficient clients. For the distributed setting, we design a\ntwo-phase selection protocol: first, the aggregator selects the top clients\nbased on our utility-driven ranking; then, a verifiable random function (VRF)\nensures a BSA-resistant final selection. AdRo-FL also applies quantization to\nreduce communication overhead and sets strict transmission deadlines to improve\nenergy efficiency. AdRo-FL achieves up to $1.85\\times$ faster time-to-accuracy\nand up to $1.06\\times$ higher final accuracy compared to insecure baselines."}
{"id": "2506.18044", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.18044", "abs": "https://arxiv.org/abs/2506.18044", "authors": ["Joseph Babb", "Joohyung Lee"], "title": "Action Language BC+", "comment": "Journal of Logic and Computation, 2015", "summary": "Action languages are formal models of parts of natural language that are\ndesigned to describe effects of actions. Many of these languages can be viewed\nas high level notations of answer set programs structured to represent\ntransition systems. However, the form of answer set programs considered in the\nearlier work is quite limited in comparison with the modern Answer Set\nProgramming (ASP) language, which allows several useful constructs for\nknowledge representation, such as choice rules, aggregates, and abstract\nconstraint atoms. We propose a new action language called BC+, which closes the\ngap between action languages and the modern ASP language. The main idea is to\ndefine the semantics of BC+ in terms of general stable model semantics for\npropositional formulas, under which many modern ASP language constructs can be\nidentified with shorthands for propositional formulas. Language BC+ turns out\nto be sufficiently expressive to encompass the best features of other action\nlanguages, such as languages B, C, C+, and BC. Computational methods available\nin ASP solvers are readily applicable to compute BC+, which led to an\nimplementation of the language by extending system cplus2asp."}
{"id": "2506.18394", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.18394", "abs": "https://arxiv.org/abs/2506.18394", "authors": ["Xiao Cheng", "Zhihao Guo", "Huan Huo", "Yulei Sui"], "title": "Tracing Errors, Constructing Fixes: Repository-Level Memory Error Repair via Typestate-Guided Context Retrieval", "comment": null, "summary": "Memory-related errors in C programming continue to pose significant\nchallenges in software development, primarily due to the complexities of manual\nmemory management inherent in the language. These errors frequently serve as\nvectors for severe vulnerabilities, while their repair requires extensive\nknowledge of program logic and C's memory model. Automated Program Repair (APR)\nhas emerged as a critical research area to address these challenges.\nTraditional APR approaches rely on expert-designed strategies and predefined\ntemplates, which are labor-intensive and constrained by the effectiveness of\nmanual specifications. Deep learning techniques offer a promising alternative\nby automatically extracting repair patterns, but they require substantial\ntraining datasets and often lack interpretability.\n  This paper introduces LTFix, a novel approach that harnesses the potential of\nLarge Language Models (LLMs) for automated memory error repair, especially for\ncomplex repository-level errors that span multiple functions and files. We\naddress two fundamental challenges in LLM-based memory error repair: a limited\nunderstanding of interprocedural memory management patterns and context window\nlimitations for repository-wide analysis. Our approach utilizes a finite\ntypestate automaton to guide the tracking of error-propagation paths and\ncontext trace, capturing both spatial (memory states) and temporal (execution\nhistory) dimensions of error behavior. This typestate-guided context retrieval\nstrategy provides the LLM with concise yet semantically rich information\nrelevant to erroneous memory management, effectively addressing the token\nlimitation of LLMs."}
{"id": "2506.17865", "categories": ["cs.CR", "cs.ET"], "pdf": "https://arxiv.org/pdf/2506.17865", "abs": "https://arxiv.org/abs/2506.17865", "authors": ["Dinesh Reddy Ankireddy", "Sudipta Paria", "Aritra Dasgupta", "Sandip Ray", "Swarup Bhunia"], "title": "LASA: Enhancing SoC Security Verification with LLM-Aided Property Generation", "comment": "9 pages, 5 figures, 5 tables", "summary": "Ensuring the security of modern System-on-Chip (SoC) designs poses\nsignificant challenges due to increasing complexity and distributed assets\nacross the intellectual property (IP) blocks. Formal property verification\n(FPV) provides the capability to model and validate design behaviors through\nsecurity properties with model checkers; however, current practices require\nsignificant manual efforts to create such properties, making them\ntime-consuming, costly, and error-prone. The emergence of Large Language Models\n(LLMs) has showcased remarkable proficiency across diverse domains, including\nHDL code generation and verification tasks. Current LLM-based techniques often\nproduce vacuous assertions and lack efficient prompt generation, comprehensive\nverification, and bug detection. This paper presents LASA, a novel framework\nthat leverages LLMs and retrieval-augmented generation (RAG) to produce\nnon-vacuous security properties and SystemVerilog Assertions (SVA) from design\nspecifications and related documentation for bus-based SoC designs. LASA\nintegrates commercial EDA tool for FPV to generate coverage metrics and\niteratively refines prompts through a feedback loop to enhance coverage. The\neffectiveness of LASA is validated through various open-source SoC designs,\ndemonstrating high coverage values with an average of ~88\\%, denoting\ncomprehensive verification through efficient generation of security properties\nand SVAs. LASA also demonstrates bug detection capabilities, identifying five\nunique bugs in the buggy OpenTitan SoC from Hack@DAC'24 competition."}
{"id": "2506.18056", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.18056", "abs": "https://arxiv.org/abs/2506.18056", "authors": ["Paolo Baldi", "Fabio Aurelio D'Asaro", "Abeer Dyoub", "Francesca Alessandra Lisi"], "title": "Weighted Assumption Based Argumentation to reason about ethical principles and actions", "comment": null, "summary": "We augment Assumption Based Argumentation (ABA for short) with weighted\nargumentation. In a nutshell, we assign weights to arguments and then derive\nthe weight of attacks between ABA arguments. We illustrate our proposal through\nrunning examples in the field of ethical reasoning, and present an\nimplementation based on Answer Set Programming."}
{"id": "2506.18398", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.18398", "abs": "https://arxiv.org/abs/2506.18398", "authors": ["Hao Wu", "Haijun Wang", "Shangwang Li", "Yin Wu", "Ming Fan", "Wuxia Jin", "Yitao Zhao", "Ting Liu"], "title": "Your Token Becomes Worthless: Unveiling Rug Pull Schemes in Crypto Token via Code-and-Transaction Fusion Analysis", "comment": null, "summary": "Rug pull scams have emerged as a persistent threat to cryptocurrency, causing\nsignificant financial losses. A typical scenario involves scammers deploying\nhoneypot contracts to attract investments, restricting token sales, and\ndraining the funds, which leaves investors with worthless tokens. Current\nmethods either rely on predefined patterns to detect code risks or utilize\nstatistical transaction data to train detection models. However, real-world Rug\nPull schemes often involve a complex interplay between malicious code and\nsuspicious transaction behaviors. These methods, which solely focus on one\naspect, fall short in detecting such schemes effectively.\n  In this paper, we propose RPhunter, a novel technique that integrates code\nand transaction for Rug Pull detection. First, RPhunter establishes declarative\nrules and performs flow analysis to extract code risk information, further\nconstructing a semantic risk code graph (SRCG). Meanwhile, to leverage\ntransaction information, RPhunter formulates dynamic token transaction\nactivities as a token flow behavior graph (TFBG) in which nodes and edges are\ncharacterized from network structure and market manipulation perspectives.\nFinally, RPhunter employs graph neural networks to extract complementary\nfeatures from SRCG and TFBG, integrating them through an attention fusion model\nto enhance the detection of Rug Pull. We manually analyzed 645 Rug Pull\nincidents from code and transaction aspects and constructed a ground-truth\ndataset. We evaluated RPhunter on our dataset, achieving a precision of 95.3%,\na recall of 93.8% and an F1 score of 94.5%, which highlights superior\nperformance compared to existing state-of-the-art methods. Furthermore, when\napplied to the real-world scenarios, RPhunter has identified 4801 Rug Pull\ntokens, achieving a precision of 91%."}
{"id": "2506.17935", "categories": ["cs.CR", "cs.AR"], "pdf": "https://arxiv.org/pdf/2506.17935", "abs": "https://arxiv.org/abs/2506.17935", "authors": ["Zhengwu Huang", "Ding Deng", "Pengyue Sun", "Guangfu Sun", "Xiaomei Tang"], "title": "Cost-Effective Optimization and Implementation of the CRT-Paillier Decryption Algorithm for Enhanced Performance", "comment": "19 pages,7 figures", "summary": "To address the privacy protection problem in cloud computing, privacy\nenhancement techniques such as the Paillier additive homomorphism algorithm are\nreceiving widespread attention. Paillier algorithm allows addition and scalar\nmultiplication operations in dencrypted state, which can effectively protect\nprivacy. However, its computational efficiency is limited by complex modulo\noperations due to the ciphertext expansion followed by encryption. To\naccelerate its decryption operation, the Chinese Remainder Theorem (CRT) is\noften used to optimize these modulo operations, which lengthens the decryption\ncomputation chain in turn. To address this issue, we propose an eCRT-Paillier\ndecryption algorithm that shortens the decryption computation chain by\ncombining precomputed parameters and eliminating extra judgment operations\nintroduced by Montgomery modular multiplications. These two improvements reduce\n50% modular multiplications and 60% judgment operations in the postprocessing\nof the CRT-Paillier decryption algorithm. Based on these improvements, we\npropose a highly parallel full-pipeline architecture to eliminate stalls caused\nby multiplier reuse in traditional modular exponentiation operations. This\narchitecture also adopts some optimizations such as simplifying modular\nexponentiation units by dividing the exponent into segments and parallelizing\ndata flow by multi-core instantiation. Finally, a high-throughput and efficient\nPaillier accelerator named MESA was implemented on the Xilinx Virtex-7 FPGA for\nevaluation, which can complete a decryption using 2048-bit key within 0.577ms\nunder 100 MHz clock frequency. Compared to prior works, MESA demonstrates a\nthroughput improvement of 1.16 to 313.21 under identical conditions, also with\nenhancements in area efficiency for LUT, DSP, and FF of 3.32 to 117.55, 1.49 to\n1.64, and 2.94 to 9.94, respectively."}
{"id": "2506.18096", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.18096", "abs": "https://arxiv.org/abs/2506.18096", "authors": ["Yuxuan Huang", "Yihang Chen", "Haozheng Zhang", "Kang Li", "Meng Fang", "Linyi Yang", "Xiaoguang Li", "Lifeng Shang", "Songcen Xu", "Jianye Hao", "Kun Shao", "Jun Wang"], "title": "Deep Research Agents: A Systematic Examination And Roadmap", "comment": null, "summary": "The rapid progress of Large Language Models (LLMs) has given rise to a new\ncategory of autonomous AI systems, referred to as Deep Research (DR) agents.\nThese agents are designed to tackle complex, multi-turn informational research\ntasks by leveraging a combination of dynamic reasoning, adaptive long-horizon\nplanning, multi-hop information retrieval, iterative tool use, and the\ngeneration of structured analytical reports. In this paper, we conduct a\ndetailed analysis of the foundational technologies and architectural components\nthat constitute Deep Research agents. We begin by reviewing information\nacquisition strategies, contrasting API-based retrieval methods with\nbrowser-based exploration. We then examine modular tool-use frameworks,\nincluding code execution, multimodal input processing, and the integration of\nModel Context Protocols (MCPs) to support extensibility and ecosystem\ndevelopment. To systematize existing approaches, we propose a taxonomy that\ndifferentiates between static and dynamic workflows, and we classify agent\narchitectures based on planning strategies and agent composition, including\nsingle-agent and multi-agent configurations. We also provide a critical\nevaluation of current benchmarks, highlighting key limitations such as\nrestricted access to external knowledge, sequential execution inefficiencies,\nand misalignment between evaluation metrics and the practical objectives of DR\nagents. Finally, we outline open challenges and promising directions for future\nresearch. A curated and continuously updated repository of DR agent research is\navailable at: {https://github.com/ai-agents-2030/awesome-deep-research-agent}."}
{"id": "2506.18403", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.18403", "abs": "https://arxiv.org/abs/2506.18403", "authors": ["Muntasir Adnan", "Carlos C. N. Kuhn"], "title": "The Debugging Decay Index: Rethinking Debugging Strategies for Code LLMs", "comment": null, "summary": "The effectiveness of AI debugging follows a predictable exponential decay\npattern; most models lose 60-80% of their debugging capability within just 2-3\nattempts, despite iterative debugging being a critical capability for practical\ncode generation systems. We introduce the Debugging Decay Index (DDI), a\nmathematical framework that quantifies when debugging becomes ineffective and\npredicts intervention points. Our strategic fresh start approach shifts from\nexploitation to exploration at strategic points in the debugging process,\ndemonstrating that well-timed interventions can rescue the effectiveness of\ndebugging. DDI reveals a fundamental limitation in current AI debugging and\nprovides the first quantitative framework for optimising iterative code\ngeneration strategies."}
{"id": "2506.17988", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2506.17988", "abs": "https://arxiv.org/abs/2506.17988", "authors": ["Seongjin Kim", "Sanguk Yun", "Jungho Jang"], "title": "Secure User-friendly Blockchain Modular Wallet Design Using Android & OP-TEE", "comment": "25 pages", "summary": "Emerging crypto economies still hemorrhage digital assets because legacy\nwallets leak private keys at almost every layer of the software stack, from\nuser-space libraries to kernel memory dumps. This paper solves that twin crisis\nof security and interoperability by re-imagining key management as a\nplatform-level service anchored in ARM TrustZone through OP-TEE. Our\narchitecture fractures the traditional monolithic Trusted Application into\nper-chain modules housed in a multi-tenant TA store, finally breaking OP-TEE's\nsingle-binary ceiling. A cryptographically sealed firmware-over-the-air\npipeline welds each TA set to an Android system image, enabling hot-swap\nupdates while Verified Boot enforces rollback protection. Every package carries\na chained signature developer first, registry second so even a compromised\nsupply chain cannot smuggle malicious code past the Secure World's RSA-PSS\ngatekeeper. Inside the TEE, strict inter-TA isolation, cache partitioning, and\nGP-compliant crypto APIs ensure secrets never bleed across trust boundaries or\ntiming domains. The Rich Execution Environment can interact only via\nhardware-mediated Secure Monitor Calls, collapsing the surface exposed to\nmalware in Android space. End-users enjoy a single polished interface yet can\ninstall or retire Bitcoin, Ethereum, Solana, or tomorrow's chain with one tap,\nshrinking both storage footprint and audit scope. For auditors, the composition\nmodel slashes duplicated verification effort by quarantining blockchain logic\ninside narrowly scoped modules that share formally specified interfaces. Our\nthreat analysis spans six adversary layers and shows how the design neutralizes\nREE malware sniffing, OTA injection, and cross-module side channels without\nexotic hardware. A reference implementation on AOSP exports a Wallet Manager\nHAL, custom SELinux domains, and a CI/CD pipeline that vet community modules\nbefore release. The result is not merely another hardware wallet but a\nprogrammable substrate that can evolve at the velocity of the blockchain\necosystem. By welding radical extensibility to hardware-anchored assurance, the\nplatform closes the security-usability gap that has long stymied mass-market\nself-custody. We posit that modular TEEs are the missing OS primitive for Web3,\nmuch as virtual memory unlocked multi-tasking in classical computing. Together,\nthese contributions sketch a blueprint for multi-chain asset management that is\nauditable, resilient, and poised for global deployment."}
{"id": "2506.18126", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.18126", "abs": "https://arxiv.org/abs/2506.18126", "authors": ["Xiang Yuming", "Li Sizhao", "Li Rongpeng", "Zhao Zhifeng", "Zhang Honggang"], "title": "Decentralized Consensus Inference-based Hierarchical Reinforcement Learning for Multi-Constrained UAV Pursuit-Evasion Game", "comment": null, "summary": "Multiple quadrotor unmanned aerial vehicle (UAV) systems have garnered\nwidespread research interest and fostered tremendous interesting applications,\nespecially in multi-constrained pursuit-evasion games (MC-PEG). The Cooperative\nEvasion and Formation Coverage (CEFC) task, where the UAV swarm aims to\nmaximize formation coverage across multiple target zones while collaboratively\nevading predators, belongs to one of the most challenging issues in MC-PEG,\nespecially under communication-limited constraints. This multifaceted problem,\nwhich intertwines responses to obstacles, adversaries, target zones, and\nformation dynamics, brings up significant high-dimensional complications in\nlocating a solution. In this paper, we propose a novel two-level framework\n(i.e., Consensus Inference-based Hierarchical Reinforcement Learning (CI-HRL)),\nwhich delegates target localization to a high-level policy, while adopting a\nlow-level policy to manage obstacle avoidance, navigation, and formation.\nSpecifically, in the high-level policy, we develop a novel multi-agent\nreinforcement learning module, Consensus-oriented Multi-Agent Communication\n(ConsMAC), to enable agents to perceive global information and establish\nconsensus from local states by effectively aggregating neighbor messages.\nMeanwhile, we leverage an Alternative Training-based Multi-agent proximal\npolicy optimization (AT-M) and policy distillation to accomplish the low-level\ncontrol. The experimental results, including the high-fidelity\nsoftware-in-the-loop (SITL) simulations, validate that CI-HRL provides a\nsuperior solution with enhanced swarm's collaborative evasion and task\ncompletion capabilities."}
{"id": "2506.18790", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.18790", "abs": "https://arxiv.org/abs/2506.18790", "authors": ["Mohamad Omar Nachawati"], "title": "ModeliHub: A Web-based, Federated Analytics Platform for Modelica-centric, Model-based Systems Engineering", "comment": null, "summary": "This paper introduces ModeliHub, a Web-based, federated analytics platform\ndesigned specifically for model-based systems engineering with Modelica.\nModeliHub's key innovation lies in its Modelica-centric, hub-and-spoke\nfederation architecture that provides systems engineers with a Modelica-based,\nunified system model of repositories containing heterogeneous engineering\nartifacts. From this unified system model, ModeliHub's Virtual Twin engine\nprovides a real-time, interactive simulation environment for deploying Modelica\nsimulation models that represent digital twins of the virtual prototype of the\nsystem under development at a particular iteration of the iterative systems\nengineering life cycle. The implementation of ModeliHub is centered around its\nextensible, Modelica compiler frontend developed in Isomorphic TypeScript that\ncan run seamlessly across browser, desktop and server environments. This\narchitecture aims to strike a balance between rigor and agility, enabling\nseamless integration and analysis across various engineering domains."}
{"id": "2506.18053", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.18053", "abs": "https://arxiv.org/abs/2506.18053", "authors": ["Marcos Florencio", "Thomas Barton"], "title": "Mechanistic Interpretability in the Presence of Architectural Obfuscation", "comment": null, "summary": "Architectural obfuscation - e.g., permuting hidden-state tensors, linearly\ntransforming embedding tables, or remapping tokens - has recently gained\ntraction as a lightweight substitute for heavyweight cryptography in\nprivacy-preserving large-language-model (LLM) inference. While recent work has\nshown that these techniques can be broken under dedicated reconstruction\nattacks, their impact on mechanistic interpretability has not been\nsystematically studied. In particular, it remains unclear whether scrambling a\nnetwork's internal representations truly thwarts efforts to understand how the\nmodel works, or simply relocates the same circuits to an unfamiliar coordinate\nsystem. We address this gap by analyzing a GPT-2-small model trained from\nscratch with a representative obfuscation map. Assuming the obfuscation map is\nprivate and the original basis is hidden (mirroring an honest-but-curious\nserver), we apply logit-lens attribution, causal path-patching, and\nattention-head ablation to locate and manipulate known circuits. Our findings\nreveal that obfuscation dramatically alters activation patterns within\nattention heads yet preserves the layer-wise computational graph. This\ndisconnect hampers reverse-engineering of user prompts: causal traces lose\ntheir alignment with baseline semantics, and token-level logit attributions\nbecome too noisy to reconstruct. At the same time, feed-forward and residual\npathways remain functionally intact, suggesting that obfuscation degrades\nfine-grained interpretability without compromising top-level task performance.\nThese results establish quantitative evidence that architectural obfuscation\ncan simultaneously (i) retain global model behaviour and (ii) impede\nmechanistic analyses of user-specific content. By mapping where\ninterpretability breaks down, our study provides guidance for future privacy\ndefences and for robustness-aware interpretability tooling."}
{"id": "2506.18135", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.18135", "abs": "https://arxiv.org/abs/2506.18135", "authors": ["Zijun Chen", "Zhanpeng Zhou", "Bo Zhang", "Weinan Zhang", "Xi Sun", "Junchi Yan"], "title": "SE-Merging: A Self-Enhanced Approach for Dynamic Model Merging", "comment": "preprint, accepted at IJCNN2025", "summary": "Model merging has gained increasing attention due to its intriguing property:\ninterpolating the parameters of different task-specific fine-tuned models leads\nto multi-task abilities. However, despite its empirical success, the underlying\nmechanisms of model merging remain poorly understood. In this work, we delve\ninto the mechanism behind model merging from a representation perspective. Our\nanalysis reveals that model merging achieves multi-task abilities through two\nkey capabilities: i) distinguishing samples from different tasks, and ii)\nadapting to the corresponding expert model for each sample. These two\ncapabilities allow the merged model to retain task-specific expertise, enabling\nefficient multi-task adaptation. Building on these insights, we propose\n\\texttt{SE-Merging}, a self-enhanced model merging framework that leverages\nthese two characteristics to dynamically identify the corresponding task for\neach sample and then adaptively rescales the merging coefficients to further\nenhance task-specific expertise in the merged model. Notably,\n\\texttt{SE-Merging} achieves dynamic model merging without additional training.\nExtensive experiments demonstrate that \\texttt{SE-Merging} achieves significant\nperformance improvements while remaining compatible with existing model merging\ntechniques."}
{"id": "2506.18796", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.18796", "abs": "https://arxiv.org/abs/2506.18796", "authors": ["Kishanthan Thangarajah", "Boyuan Chen", "Shi Chang", "Ahmed E. Hassan"], "title": "Context-Aware CodeLLM Eviction for AI-assisted Coding", "comment": "12 pages, 6 figures", "summary": "AI-assisted coding tools powered by Code Large Language Models (CodeLLMs) are\nincreasingly integrated into modern software development workflows. To address\nconcerns around privacy, latency, and model customization, many enterprises opt\nto self-host these models. However, the diversity and growing number of\nCodeLLMs, coupled with limited accelerator memory, introduce practical\nchallenges in model management and serving efficiency. This paper presents\nCACE, a novel context-aware model eviction strategy designed specifically to\noptimize self-hosted CodeLLM serving under resource constraints. Unlike\ntraditional eviction strategies based solely on recency (e.g., Least Recently\nUsed), CACE leverages multiple context-aware factors, including model load\ntime, task-specific latency sensitivity, expected output length, and recent\nusage and future demand tracked through a sliding window. We evaluate CACE\nusing realistic workloads that include both latency-sensitive code completion\nand throughput-intensive code reasoning tasks. Our experiments show that CACE\nreduces Time-to-First-Token (TTFT) and end-to-end (E2E) latency, while\nsignificantly lowering the number of model evictions compared to\nstate-of-the-art systems. Ablation studies further demonstrate the importance\nof multi-factor eviction in balancing responsiveness and resource efficiency.\nThis work contributes practical strategies for deploying scalable, low-latency\nAI coding assistants in real-world software engineering environments."}
{"id": "2506.18087", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.18087", "abs": "https://arxiv.org/abs/2506.18087", "authors": ["Huaiying Luo", "Cheng Ji"], "title": "Federated Learning-Based Data Collaboration Method for Enhancing Edge Cloud AI System Security Using Large Language Models", "comment": "Accepted by the 2025 5th International Symposium on Computer\n  Technology and Information Science (ISCTIS 2025)", "summary": "With the widespread application of edge computing and cloud systems in\nAI-driven applications, how to maintain efficient performance while ensuring\ndata privacy has become an urgent security issue. This paper proposes a\nfederated learning-based data collaboration method to improve the security of\nedge cloud AI systems, and use large-scale language models (LLMs) to enhance\ndata privacy protection and system robustness. Based on the existing federated\nlearning framework, this method introduces a secure multi-party computation\nprotocol, which optimizes the data aggregation and encryption process between\ndistributed nodes by using LLM to ensure data privacy and improve system\nefficiency. By combining advanced adversarial training techniques, the model\nenhances the resistance of edge cloud AI systems to security threats such as\ndata leakage and model poisoning. Experimental results show that the proposed\nmethod is 15% better than the traditional federated learning method in terms of\ndata protection and model robustness."}
{"id": "2506.18149", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.18149", "abs": "https://arxiv.org/abs/2506.18149", "authors": ["Fumian Chen", "Sotheara Veng", "Joshua Wilson", "Xiaoming Li", "Hui Fang"], "title": "CoachGPT: A Scaffolding-based Academic Writing Assistant", "comment": "SIGIR 2025 DEMO Pre-print", "summary": "Academic writing skills are crucial for students' success, but can feel\noverwhelming without proper guidance and practice, particularly when writing in\na second language. Traditionally, students ask instructors or search\ndictionaries, which are not universally accessible. Early writing assistants\nemerged as rule-based systems that focused on detecting misspellings,\nsubject-verb disagreements, and basic punctuation errors; however, they are\ninaccurate and lack contextual understanding. Machine learning-based assistants\ndemonstrate a strong ability for language understanding but are expensive to\ntrain. Large language models (LLMs) have shown remarkable capabilities in\ngenerating responses in natural languages based on given prompts. Still, they\nhave a fundamental limitation in education: they generate essays without\nteaching, which can have detrimental effects on learning when misused. To\naddress this limitation, we develop CoachGPT, which leverages large language\nmodels (LLMs) to assist individuals with limited educational resources and\nthose who prefer self-paced learning in academic writing. CoachGPT is an AI\nagent-based web application that (1) takes instructions from experienced\neducators, (2) converts instructions into sub-tasks, and (3) provides real-time\nfeedback and suggestions using large language models. This unique scaffolding\nstructure makes CoachGPT unique among existing writing assistants. Compared to\nexisting writing assistants, CoachGPT provides a more immersive writing\nexperience with personalized feedback and guidance. Our user studies prove the\nusefulness of CoachGPT and the potential of large language models for academic\nwriting."}
{"id": "2506.18824", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.18824", "abs": "https://arxiv.org/abs/2506.18824", "authors": ["Islem Bouzenia", "Michael Pradel"], "title": "Understanding Software Engineering Agents: A Study of Thought-Action-Result Trajectories", "comment": null, "summary": "Large Language Model (LLM)-based agents are increasingly employed to automate\ncomplex software engineering tasks such as program repair and issue resolution.\nThese agents operate by autonomously generating natural language thoughts,\ninvoking external tools, and iteratively refining their solutions. Despite\ntheir widespread adoption, the internal decision-making processes of these\nagents remain largely unexplored, limiting our understanding of their\noperational dynamics and failure modes. In this paper, we present a large-scale\nempirical study of the thought-action-result trajectories of three\nstate-of-the-art LLM-based agents: \\textsc{RepairAgent},\n\\textsc{AutoCodeRover}, and \\textsc{OpenHands}. We unify their interaction logs\ninto a common format, capturing 120 trajectories and 2822 LLM interactions\nfocused on program repair and issue resolution. Our study combines quantitative\nanalyses of structural properties, action patterns, and token usage with\nqualitative assessments of reasoning coherence and feedback integration. We\nidentify key trajectory characteristics such as iteration counts and token\nconsumption, recurring action sequences, and the semantic coherence linking\nthoughts, actions, and their results. Our findings reveal behavioral motifs and\nanti-patterns that distinguish successful from failed executions, providing\nactionable insights for improving agent design, including prompting strategies,\nfailure diagnosis, and anti-pattern detection. We release our dataset and\nannotation framework to support further research on transparent and robust\nautonomous software engineering agents."}
{"id": "2506.18100", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2506.18100", "abs": "https://arxiv.org/abs/2506.18100", "authors": ["Taimoor Ahmad", "Anas Ali"], "title": "Optimizing Resource Allocation and Energy Efficiency in Federated Fog Computing for IoT", "comment": null, "summary": "Address Resolution Protocol (ARP) spoofing attacks severely threaten Internet\nof Things (IoT) networks by allowing attackers to intercept, modify, or block\ncommunications. Traditional detection methods are insufficient due to high\nfalse positives and poor adaptability. This research proposes a multi-layered\nmachine learning-based framework for intelligently detecting ARP spoofing in\nIoT networks. Our approach utilizes an ensemble of classifiers organized into\nmultiple layers, each layer optimizing detection accuracy and reducing false\nalarms. Experimental evaluations demonstrate significant improvements in\ndetection accuracy (up to 97.5\\%), reduced false positive rates (less than\n2\\%), and faster detection time compared to existing methods. Our key\ncontributions include introducing multi-layer ensemble classifiers specifically\ntuned for IoT networks, systematically addressing dataset imbalance problems,\nintroducing a dynamic feedback mechanism for classifier retraining, and\nvalidating practical applicability through extensive simulations. This research\nenhances security management in IoT deployments, providing robust defenses\nagainst ARP spoofing attacks and improving reliability and trust in IoT\nenvironments."}
{"id": "2506.18156", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.18156", "abs": "https://arxiv.org/abs/2506.18156", "authors": ["Akash Kundu", "Rishika Goswami"], "title": "AI Through the Human Lens: Investigating Cognitive Theories in Machine Psychology", "comment": null, "summary": "We investigate whether Large Language Models (LLMs) exhibit human-like\ncognitive patterns under four established frameworks from psychology: Thematic\nApperception Test (TAT), Framing Bias, Moral Foundations Theory (MFT), and\nCognitive Dissonance. We evaluated several proprietary and open-source models\nusing structured prompts and automated scoring. Our findings reveal that these\nmodels often produce coherent narratives, show susceptibility to positive\nframing, exhibit moral judgments aligned with Liberty/Oppression concerns, and\ndemonstrate self-contradictions tempered by extensive rationalization. Such\nbehaviors mirror human cognitive tendencies yet are shaped by their training\ndata and alignment methods. We discuss the implications for AI transparency,\nethical deployment, and future work that bridges cognitive psychology and AI\nsafety"}
{"id": "2506.17315", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2506.17315", "abs": "https://arxiv.org/abs/2506.17315", "authors": ["Chuan Yan", "Liuhuo Wan", "Bowei Guan", "Fengqi Yu", "Guangdong Bai", "Jin Song Dong"], "title": "Tracking GPTs Third Party Service: Automation, Analysis, and Insights", "comment": "The 1st International Workshop on LLM App Store Analysis (LLMapp\n  2025)", "summary": "ChatGPT has quickly advanced from simple natural language processing to\ntackling more sophisticated and specialized tasks. Drawing inspiration from the\nsuccess of mobile app ecosystems, OpenAI allows developers to create\napplications that interact with third-party services, known as GPTs. GPTs can\nchoose to leverage third-party services to integrate with specialized APIs for\ndomain-specific applications. However, the way these disclose privacy setting\ninformation limits accessibility and analysis, making it challenging to\nsystematically evaluate the data privacy implications of third-party integrate\nto GPTs. In order to support academic research on the integration of\nthird-party services in GPTs, we introduce GPTs-ThirdSpy, an automated\nframework designed to extract privacy settings of GPTs. GPTs-ThirdSpy provides\nacademic researchers with real-time, reliable metadata on third-party services\nused by GPTs, enabling in-depth analysis of their integration, compliance, and\npotential security risks. By systematically collecting and structuring this\ndata, GPTs-ThirdSpy facilitates large-scale research on the transparency and\nregulatory challenges associated with the GPT app ecosystem."}
{"id": "2506.18114", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.18114", "abs": "https://arxiv.org/abs/2506.18114", "authors": ["Ioannis Panopoulos", "Maria-Lamprini A. Bartsioka", "Sokratis Nikolaidis", "Stylianos I. Venieris", "Dimitra I. Kaklamani", "Iakovos S. Venieris"], "title": "Dynamic Temporal Positional Encodings for Early Intrusion Detection in IoT", "comment": "Accepted at the 10th International Conference on Smart and\n  Sustainable Technologies (SpliTech 2025)", "summary": "The rapid expansion of the Internet of Things (IoT) has introduced\nsignificant security challenges, necessitating efficient and adaptive Intrusion\nDetection Systems (IDS). Traditional IDS models often overlook the temporal\ncharacteristics of network traffic, limiting their effectiveness in early\nthreat detection. We propose a Transformer-based Early Intrusion Detection\nSystem (EIDS) that incorporates dynamic temporal positional encodings to\nenhance detection accuracy while maintaining computational efficiency. By\nleveraging network flow timestamps, our approach captures both sequence\nstructure and timing irregularities indicative of malicious behaviour.\nAdditionally, we introduce a data augmentation pipeline to improve model\nrobustness. Evaluated on the CICIoT2023 dataset, our method outperforms\nexisting models in both accuracy and earliness. We further demonstrate its\nreal-time feasibility on resource-constrained IoT devices, achieving\nlow-latency inference and minimal memory footprint."}
{"id": "2506.18158", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.18158", "abs": "https://arxiv.org/abs/2506.18158", "authors": ["Xinzge Gao", "Chuanrui Hu", "Bin Chen", "Teng Li"], "title": "Chain-of-Memory: Enhancing GUI Agents for Cross-Application Navigation", "comment": null, "summary": "Multimodal large language models (MLLMs) are attracting growing attention in\nthe development of Graphical User Interface (GUI) agents. Existing approaches\noften rely on historical screenshots or actions to implicitly represent the\ntask state. This reliance poses challenges for GUI agents in accurately\nunderstanding task states and underscores the absence of effective mechanisms\nto store critical information in complex and lengthy cross-app tasks. To\naddress these challenges, we propose Chain-of-Memory (CoM), a novel approach\nfor explicitly modeling short-term and long-term memory in GUI agents. CoM\nachieves this by capturing action descriptions, integrating task-relevant\nscreen information, and maintaining a dedicated memory module to store and\nmanage this information. By leveraging explicit memory representations, CoM\nenables GUI agents to better understand task states and retain critical\nhistorical information persistently. To equip GUI agents with memory management\ncapabilities and evaluate the effectiveness of CoM, we developed the GUI\nOdyssey-CoM, a dataset comprising 111k screen-action pairs annotated with\nChain-of-Memory. Experimental results demonstrate that CoM significantly\nimproves GUI agents' performance in cross-application tasks. Additionally, GUI\nOdyssey-CoM enables 7B models to achieve memory management capabilities\ncomparable to 72B models. The dataset and code will be open-sourced."}
{"id": "2506.17792", "categories": ["cs.AI", "cs.LO", "cs.SE"], "pdf": "https://arxiv.org/pdf/2506.17792", "abs": "https://arxiv.org/abs/2506.17792", "authors": ["Alexandros Evangelidis", "Gricel Vázquez", "Simos Gerasimou"], "title": "Efficient Strategy Synthesis for MDPs via Hierarchical Block Decomposition", "comment": null, "summary": "Software-intensive systems, such as software product lines and robotics,\nutilise Markov decision processes (MDPs) to capture uncertainty and analyse\nsequential decision-making problems. Despite the usefulness of conventional\npolicy synthesis methods, they fail to scale to large state spaces. Our\napproach addresses this issue and accelerates policy synthesis in large MDPs by\ndynamically refining the MDP and iteratively selecting the most fragile MDP\nregions for refinement. This iterative procedure offers a balance between\naccuracy and efficiency, as refinement occurs only when necessary. Through a\ncomprehensive empirical evaluation comprising diverse case studies and MDPs up\nto 1M states, we demonstrate significant performance improvements yielded by\nour approach compared to the leading probabilistic model checker PRISM (up to\n2x), thus offering a very competitive solution for real-world policy synthesis\ntasks in larger MDPs."}
{"id": "2506.18150", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2506.18150", "abs": "https://arxiv.org/abs/2506.18150", "authors": ["Karthik Garimella", "Austin Ebel", "Gabrielle De Micheli", "Brandon Reagen"], "title": "HE-LRM: Encrypted Deep Learning Recommendation Models using Fully Homomorphic Encryption", "comment": "14 pages, 10 figures, 2 tables", "summary": "Fully Homomorphic Encryption (FHE) is an encryption scheme that not only\nencrypts data but also allows for computations to be applied directly on the\nencrypted data. While computationally expensive, FHE can enable\nprivacy-preserving neural inference in the client-server setting: a client\nencrypts their input with FHE and sends it to an untrusted server. The server\nthen runs neural inference on the encrypted data and returns the encrypted\nresults. The client decrypts the output locally, keeping both the input and\nresult private from the server. Private inference has focused on networks with\ndense inputs such as image classification, and less attention has been given to\nnetworks with sparse features. Unlike dense inputs, sparse features require\nefficient encrypted lookup operations into large embedding tables, which\npresent computational and memory constraints for FHE.\n  In this paper, we explore the challenges and opportunities when applying FHE\nto Deep Learning Recommendation Models (DLRM) from both a compiler and systems\nperspective. DLRMs utilize conventional MLPs for dense features and embedding\ntables to map sparse, categorical features to dense vector representations. We\ndevelop novel methods for performing compressed embedding lookups in order to\nreduce FHE computational costs while keeping the underlying model performant.\nOur embedding lookup improves upon a state-of-the-art approach by $77 \\times$.\nFurthermore, we present an efficient multi-embedding packing strategy that\nenables us to perform a 44 million parameter embedding lookup under FHE.\nFinally, we integrate our solutions into the open-source Orion framework and\npresent HE-LRM, an end-to-end encrypted DLRM. We evaluate HE-LRM on UCI (health\nprediction) and Criteo (click prediction), demonstrating that with the right\ncompression and packing strategies, encrypted inference for recommendation\nsystems is practical."}
{"id": "2506.18183", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.18183", "abs": "https://arxiv.org/abs/2506.18183", "authors": ["Zhiting Mei", "Christina Zhang", "Tenny Yin", "Justin Lidard", "Ola Shorinwa", "Anirudha Majumdar"], "title": "Reasoning about Uncertainty: Do Reasoning Models Know When They Don't Know?", "comment": null, "summary": "Reasoning language models have set state-of-the-art (SOTA) records on many\nchallenging benchmarks, enabled by multi-step reasoning induced using\nreinforcement learning. However, like previous language models, reasoning\nmodels are prone to generating confident, plausible responses that are\nincorrect (hallucinations). Knowing when and how much to trust these models is\ncritical to the safe deployment of reasoning models in real-world applications.\nTo this end, we explore uncertainty quantification of reasoning models in this\nwork. Specifically, we ask three fundamental questions: First, are reasoning\nmodels well-calibrated? Second, does deeper reasoning improve model\ncalibration? Finally, inspired by humans' innate ability to double-check their\nthought processes to verify the validity of their answers and their confidence,\nwe ask: can reasoning models improve their calibration by explicitly reasoning\nabout their chain-of-thought traces? We introduce introspective uncertainty\nquantification (UQ) to explore this direction. In extensive evaluations on SOTA\nreasoning models across a broad range of benchmarks, we find that reasoning\nmodels: (i) are typically overconfident, with self-verbalized confidence\nestimates often greater than 85% particularly for incorrect responses, (ii)\nbecome even more overconfident with deeper reasoning, and (iii) can become\nbetter calibrated through introspection (e.g., o3-Mini and DeepSeek R1) but not\nuniformly (e.g., Claude 3.7 Sonnet becomes more poorly calibrated). Lastly, we\nconclude with important research directions to design necessary UQ benchmarks\nand improve the calibration of reasoning models."}
{"id": "2506.18245", "categories": ["cs.CR", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2506.18245", "abs": "https://arxiv.org/abs/2506.18245", "authors": ["Lei Yu", "Zhirong Huang", "Hang Yuan", "Shiqi Cheng", "Li Yang", "Fengjun Zhang", "Chenjie Shen", "Jiajia Ma", "Jingyuan Zhang", "Junyi Lu", "Chun Zuo"], "title": "Smart-LLaMA-DPO: Reinforced Large Language Model for Explainable Smart Contract Vulnerability Detection", "comment": "Accepted to ISSTA 2025", "summary": "Smart contract vulnerability detection remains a major challenge in\nblockchain security. Existing vulnerability detection methods face two main\nissues: (1) Existing datasets lack comprehensive coverage and high-quality\nexplanations for preference learning. (2) Large language models (LLMs) often\nstruggle with accurately interpreting specific concepts in smart contract\nsecurity. Empirical analysis shows that even after continual pre-training (CPT)\nand supervised fine-tuning (SFT), LLMs may misinterpret the execution order of\nstate changes, resulting in incorrect explanations despite making correct\ndetection decisions. To address these challenges, we propose Smart-LLaMA-DPO\nbased on LLaMA-3.1-8B. We construct a comprehensive dataset covering four major\nvulnerability types and machine-unauditable vulnerabilities, including precise\nlabels, explanations, and locations for SFT, as well as high-quality and\nlow-quality output pairs for Direct Preference Optimization (DPO). Second, we\nperform CPT using large-scale smart contract to enhance the LLM's understanding\nof specific security practices in smart contracts. Futhermore, we conduct SFT\nwith our comprehensive dataset. Finally, we apply DPO, leveraging human\nfeedback and a specially designed loss function that increases the probability\nof preferred explanations while reducing the likelihood of non-preferred\noutputs. We evaluate Smart-LLaMA-DPO on four major vulnerability types:\nreentrancy, timestamp dependence, integer overflow/underflow, and delegatecall,\nas well as machine-unauditable vulnerabilities. Our method significantly\noutperforms state-of-the-art baselines, with average improvements of 10.43% in\nF1 score and 7.87% in accuracy. Moreover, both LLM evaluation and human\nevaluation confirm that our method generates more correct, thorough, and clear\nexplanations."}
{"id": "2506.18189", "categories": ["cs.CR", "cs.CY", "C.2.4"], "pdf": "https://arxiv.org/pdf/2506.18189", "abs": "https://arxiv.org/abs/2506.18189", "authors": ["Maxwell Koegler"], "title": "SoK: Current State of Ethereum's Enshrined Proposer Builder Separation", "comment": "12 pages, 2 figures, submitted to The Science of Blockchain\n  Conference 2025", "summary": "Initially introduced to Ethereum via Flashbots' MEV-boost, Proposer-Builder\nSeparation allows proposers to auction off blockspace to a market of\ntransaction orderers, known as builders. PBS is currently available to\nvalidators through the aforementioned MEV-boost, but its unregulated and\nrelay-dependent nature has much of the Ethereum community calling for its\nenshrinement. Providing a protocol-integrated PBS marketspace and communication\nchannel for payload outsourcing is termed PBS enshrinement. Although ePBS\npotentially introduces native MEV mitigation mechanisms and reduces validator\noperation costs, fears of multiparty collusion and chain stagnation are all too\nreal. In addition to mitigating these potential drawbacks, PBS research pursues\nmany tenets revered by Web3 enthusiasts, including but not limited to,\ncensorship resistance, validator reward equity, and deflationary finance. The\nsubsequent SoK will identify current PBS mechanisms, the need for enshrinement,\nadditions to the ePBS upgrade, and the existing or potential on-chain\nsocioeconomic implications of each."}
{"id": "2506.18187", "categories": ["cs.AI", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.18187", "abs": "https://arxiv.org/abs/2506.18187", "authors": ["Shahriar Noroozizadeh", "Pim Welle", "Jeremy C. Weiss", "George H. Chen"], "title": "The Impact of Medication Non-adherence on Adverse Outcomes: Evidence from Schizophrenia Patients via Survival Analysis", "comment": "Conference on Health, Inference, and Learning (CHIL 2025)", "summary": "This study quantifies the association between non-adherence to antipsychotic\nmedications and adverse outcomes in individuals with schizophrenia. We frame\nthe problem using survival analysis, focusing on the time to the earliest of\nseveral adverse events (early death, involuntary hospitalization, jail\nbooking). We extend standard causal inference methods (T-learner, S-learner,\nnearest neighbor matching) to utilize various survival models to estimate\nindividual and average treatment effects, where treatment corresponds to\nmedication non-adherence. Analyses are repeated using different amounts of\nlongitudinal information (3, 6, 9, and 12 months). Using data from Allegheny\nCounty in western Pennsylvania, we find strong evidence that non-adherence\nadvances adverse outcomes by approximately 1 to 4 months. Ablation studies\nconfirm that county-provided risk scores adjust for key confounders, as their\nremoval amplifies the estimated effects. Subgroup analyses by medication\nformulation (injectable vs. oral) and medication type consistently show that\nnon-adherence is associated with earlier adverse events. These findings\nhighlight the clinical importance of adherence in delaying psychiatric crises\nand show that integrating survival analysis with causal inference tools can\nyield policy-relevant insights. We caution that although we apply causal\ninference, we only make associative claims and discuss assumptions needed for\ncausal interpretation."}
{"id": "2506.18470", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2506.18470", "abs": "https://arxiv.org/abs/2506.18470", "authors": ["Daniele Canavese", "Leonardo Regano", "Bjorn De Sutter", "Cataldo Basile"], "title": "Automatic Selection of Protections to Mitigate Risks Against Software Applications", "comment": null, "summary": "This paper introduces a novel approach for the automated selection of\nsoftware protections to mitigate MATE risks against critical assets within\nsoftware applications. We formalize the key elements involved in protection\ndecision-making - including code artifacts, assets, security requirements,\nattacks, and software protections - and frame the protection process through a\ngame-theoretic model. In this model, a defender strategically applies\nprotections to various code artifacts of a target application, anticipating\nrepeated attack attempts by adversaries against the confidentiality and\nintegrity of the application's assets. The selection of the optimal defense\nmaximizes resistance to attacks while ensuring the application remains usable\nby constraining the overhead introduced by protections. The game is solved\nthrough a heuristic based on a mini-max depth-first exploration strategy,\naugmented with dynamic programming optimizations for improved efficiency.\nCentral to our formulation is the introduction of the Software Protection\nIndex, an original contribution that extends existing notions of potency and\nresilience by evaluating protection effectiveness against attack paths using\nsoftware metrics and expert assessments. We validate our approach through a\nproof-of-concept implementation and expert evaluations, demonstrating that\nautomated software protection is a practical and effective solution for risk\nmitigation in software."}
{"id": "2506.18203", "categories": ["cs.CR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.18203", "abs": "https://arxiv.org/abs/2506.18203", "authors": ["Jon Saad-Falcon", "E. Kelly Buchanan", "Mayee F. Chen", "Tzu-Heng Huang", "Brendan McLaughlin", "Tanvir Bhathal", "Shang Zhu", "Ben Athiwaratkun", "Frederic Sala", "Scott Linderman", "Azalia Mirhoseini", "Christopher Ré"], "title": "Shrinking the Generation-Verification Gap with Weak Verifiers", "comment": null, "summary": "Verifiers can improve language model capabilities by scoring and ranking\nresponses from generated candidates. Currently, high-quality verifiers are\neither unscalable (e.g., humans) or limited in utility (e.g., tools like Lean).\nWhile LM judges and reward models have become broadly useful as general-purpose\nverifiers, a significant performance gap remains between them and oracle\nverifiers (verifiers with perfect accuracy). To help close this gap, we\nintroduce Weaver, a framework for designing a strong verifier by combining\nmultiple weak, imperfect verifiers. We find weighted ensembles of verifiers,\nwhich typically require learning from labeled data, significantly outperform\nunweighted combinations due to differences in verifier accuracies. To reduce\ndependency on labeled data, Weaver leverages weak supervision to estimate each\nverifier's accuracy and combines outputs into a unified score that better\nreflects true response quality. However, directly applying weak supervision\nalgorithms poses challenges, including inconsistent verifier output formats and\nhandling low-quality verifiers. Weaver addresses these using dataset statistics\nto normalize outputs and filter specific verifiers. We study Weaver's\neffectiveness in test-time repeated sampling, where a model generates multiple\ncandidate responses and selects one. Our evaluations show Weaver significantly\nimproves over Pass@1-performance when selecting the first candidate-across\nreasoning and math tasks, achieving o3-mini-level accuracy with Llama 3.3 70B\nInstruct as generator, and an ensemble of 70B or smaller judge and reward\nmodels as verifiers (87.7% average). This gain mirrors the jump between GPT-4o\nand o3-mini (69.0% vs. 86.7%), which required extensive finetuning and\npost-training. To reduce computational costs of verifier ensembles, we train a\n400M cross-encoder using Weaver's combined output scores."}
{"id": "2506.18213", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.18213", "abs": "https://arxiv.org/abs/2506.18213", "authors": ["María Victoria Carro", "Denise Alejandra Mester", "Francisca Gauna Selasco", "Luca Nicolás Forziati Gangi", "Matheo Sandleris Musa", "Lola Ramos Pereyra", "Mario Leiva", "Juan Gustavo Corvalan", "María Vanina Martinez", "Gerardo Simari"], "title": "A Conceptual Framework for AI Capability Evaluations", "comment": "arXiv admin note: text overlap with arXiv:2306.04181 by other authors", "summary": "As AI systems advance and integrate into society, well-designed and\ntransparent evaluations are becoming essential tools in AI governance,\ninforming decisions by providing evidence about system capabilities and risks.\nYet there remains a lack of clarity on how to perform these assessments both\ncomprehensively and reliably. To address this gap, we propose a conceptual\nframework for analyzing AI capability evaluations, offering a structured,\ndescriptive approach that systematizes the analysis of widely used methods and\nterminology without imposing new taxonomies or rigid formats. This framework\nsupports transparency, comparability, and interpretability across diverse\nevaluations. It also enables researchers to identify methodological weaknesses,\nassists practitioners in designing evaluations, and provides policymakers with\nan accessible tool to scrutinize, compare, and navigate complex evaluation\nlandscapes."}
{"id": "2506.18795", "categories": ["cs.CR", "cs.SE", "D.2.4; I.2.7"], "pdf": "https://arxiv.org/pdf/2506.18795", "abs": "https://arxiv.org/abs/2506.18795", "authors": ["Jiachi Chen", "Yiming Shen", "Jiashuo Zhang", "Zihao Li", "John Grundy", "Zhenzhe Shao", "Yanlin Wang", "Jiashui Wang", "Ting Chen", "Zibin Zheng"], "title": "FORGE: An LLM-driven Framework for Large-Scale Smart Contract Vulnerability Dataset Construction", "comment": "Accepted for the 48th International Conference on Software\n  Engineering (ICSE 2026)", "summary": "High-quality smart contract vulnerability datasets are critical for\nevaluating security tools and advancing smart contract security research. Two\nmajor limitations of current manual dataset construction are (1)\nlabor-intensive and error-prone annotation processes limiting the scale,\nquality, and evolution of the dataset, and (2) absence of standardized\nclassification rules results in inconsistent vulnerability categories and\nlabeling results across different datasets. To address these limitations, we\npresent FORGE, the first automated approach for constructing smart contract\nvulnerability datasets. FORGE leverages an LLM-driven pipeline to extract\nhigh-quality vulnerabilities from real-world audit reports and classify them\naccording to the CWE, the most widely recognized classification in software\nsecurity. FORGE employs a divide-and-conquer strategy to extract structured and\nself-contained vulnerability information from these reports. Additionally, it\nuses a tree-of-thoughts technique to classify the vulnerability information\ninto the hierarchical CWE classification. To evaluate FORGE's effectiveness, we\nrun FORGE on 6,454 real-world audit reports and generate a dataset comprising\n81,390 solidity files and 27,497 vulnerability findings across 296 CWE\ncategories. Manual assessment of the dataset demonstrates high extraction\nprecision and classification consistency with human experts (precision of 95.6%\nand inter-rater agreement k-$\\alpha$ of 0.87). We further validate the\npracticality of our dataset by benchmarking 13 existing security tools on our\ndataset. The results reveal the significant limitations in current detection\ncapabilities. Furthermore, by analyzing the severity-frequency distribution\npatterns through a unified CWE perspective in our dataset, we highlight\ninconsistency between current smart contract research focus and priorities\nidentified from real-world vulnerabilities..."}
{"id": "2506.18245", "categories": ["cs.CR", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2506.18245", "abs": "https://arxiv.org/abs/2506.18245", "authors": ["Lei Yu", "Zhirong Huang", "Hang Yuan", "Shiqi Cheng", "Li Yang", "Fengjun Zhang", "Chenjie Shen", "Jiajia Ma", "Jingyuan Zhang", "Junyi Lu", "Chun Zuo"], "title": "Smart-LLaMA-DPO: Reinforced Large Language Model for Explainable Smart Contract Vulnerability Detection", "comment": "Accepted to ISSTA 2025", "summary": "Smart contract vulnerability detection remains a major challenge in\nblockchain security. Existing vulnerability detection methods face two main\nissues: (1) Existing datasets lack comprehensive coverage and high-quality\nexplanations for preference learning. (2) Large language models (LLMs) often\nstruggle with accurately interpreting specific concepts in smart contract\nsecurity. Empirical analysis shows that even after continual pre-training (CPT)\nand supervised fine-tuning (SFT), LLMs may misinterpret the execution order of\nstate changes, resulting in incorrect explanations despite making correct\ndetection decisions. To address these challenges, we propose Smart-LLaMA-DPO\nbased on LLaMA-3.1-8B. We construct a comprehensive dataset covering four major\nvulnerability types and machine-unauditable vulnerabilities, including precise\nlabels, explanations, and locations for SFT, as well as high-quality and\nlow-quality output pairs for Direct Preference Optimization (DPO). Second, we\nperform CPT using large-scale smart contract to enhance the LLM's understanding\nof specific security practices in smart contracts. Futhermore, we conduct SFT\nwith our comprehensive dataset. Finally, we apply DPO, leveraging human\nfeedback and a specially designed loss function that increases the probability\nof preferred explanations while reducing the likelihood of non-preferred\noutputs. We evaluate Smart-LLaMA-DPO on four major vulnerability types:\nreentrancy, timestamp dependence, integer overflow/underflow, and delegatecall,\nas well as machine-unauditable vulnerabilities. Our method significantly\noutperforms state-of-the-art baselines, with average improvements of 10.43% in\nF1 score and 7.87% in accuracy. Moreover, both LLM evaluation and human\nevaluation confirm that our method generates more correct, thorough, and clear\nexplanations."}
{"id": "2506.18233", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.18233", "abs": "https://arxiv.org/abs/2506.18233", "authors": ["Ruike Zhu", "Hanwen Zhang", "Tianyu Shi", "Chi Wang", "Tianyi Zhou", "Zengyi Qin"], "title": "The 4th Dimension for Scaling Model Size", "comment": null, "summary": "Scaling the size of large language models typically involves three\ndimensions: depth, width, and the number of parameters. In this work, we\nexplore a fourth dimension, virtual logical depth (VLD), which increases the\neffective algorithmic depth without changing the overall parameter count by\nreusing parameters within the model. Although parameter reuse is not a new\nconcept, its potential and characteristics in model scaling have not been\nthoroughly studied. Through carefully designed controlled experiments, we make\nthe following key discoveries regarding VLD scaling:\n  VLD scaling forces the knowledge capacity of the model to remain almost\nconstant, with only minor variations.\n  VLD scaling enables a significant improvement in reasoning capability,\nprovided the scaling method is properly implemented.\n  The number of parameters correlates with knowledge capacity, but not with\nreasoning capability. Under certain conditions, it is not necessary to increase\nthe parameter count to enhance reasoning.\n  These findings are consistent across various model configurations and are\nlikely to be generally valid within the scope of our experiments."}
{"id": "2506.18462", "categories": ["cs.CR", "I.2"], "pdf": "https://arxiv.org/pdf/2506.18462", "abs": "https://arxiv.org/abs/2506.18462", "authors": ["Fatemeh Jalalvand", "Mohan Baruwal Chhetri", "Surya Nepal", "Cécile Paris"], "title": "Adaptive alert prioritisation in security operations centres via learning to defer with human feedback", "comment": "No comment", "summary": "Alert prioritisation (AP) is crucial for security operations centres (SOCs)\nto manage the overwhelming volume of alerts and ensure timely detection and\nresponse to genuine threats, while minimising alert fatigue. Although\npredictive AI can process large alert volumes and identify known patterns, it\nstruggles with novel and evolving scenarios that demand contextual\nunderstanding and nuanced judgement. A promising solution is Human-AI teaming\n(HAT), which combines human expertise with AI's computational capabilities.\nLearning to Defer (L2D) operationalises HAT by enabling AI to \"defer\" uncertain\nor unfamiliar cases to human experts. However, traditional L2D models rely on\nstatic deferral policies that do not evolve with experience, limiting their\nability to learn from human feedback and adapt over time. To overcome this, we\nintroduce Learning to Defer with Human Feedback (L2DHF), an adaptive deferral\nframework that leverages Deep Reinforcement Learning from Human Feedback\n(DRLHF) to optimise deferral decisions. By dynamically incorporating human\nfeedback, L2DHF continuously improves AP accuracy and reduces unnecessary\ndeferrals, enhancing SOC effectiveness and easing analyst workload. Experiments\non two widely used benchmark datasets, UNSW-NB15 and CICIDS2017, demonstrate\nthat L2DHF significantly outperforms baseline models. Notably, it achieves\n13-16% higher AP accuracy for critical alerts on UNSW-NB15 and 60-67% on\nCICIDS2017. It also reduces misprioritisations, for example, by 98% for\nhigh-category alerts on CICIDS2017. Moreover, L2DHF decreases deferrals, for\nexample, by 37% on UNSW-NB15, directly reducing analyst workload. These gains\nare achieved with efficient execution, underscoring L2DHF's practicality for\nreal-world SOC deployment."}
{"id": "2506.18260", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.18260", "abs": "https://arxiv.org/abs/2506.18260", "authors": ["FuTe Wong"], "title": "Advanced For-Loop for QML algorithm search", "comment": "7 pages, 8 figures", "summary": "This paper introduces an advanced framework leveraging Large Language\nModel-based Multi-Agent Systems (LLMMA) for the automated search and\noptimization of Quantum Machine Learning (QML) algorithms. Inspired by Google\nDeepMind's FunSearch, the proposed system works on abstract level to\niteratively generates and refines quantum transformations of classical machine\nlearning algorithms (concepts), such as the Multi-Layer Perceptron,\nforward-forward and backpropagation algorithms. As a proof of concept, this\nwork highlights the potential of agentic frameworks to systematically explore\nclassical machine learning concepts and adapt them for quantum computing,\npaving the way for efficient and automated development of QML algorithms.\nFuture directions include incorporating planning mechanisms and optimizing\nstrategy in the search space for broader applications in quantum-enhanced\nmachine learning."}
{"id": "2506.18470", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2506.18470", "abs": "https://arxiv.org/abs/2506.18470", "authors": ["Daniele Canavese", "Leonardo Regano", "Bjorn De Sutter", "Cataldo Basile"], "title": "Automatic Selection of Protections to Mitigate Risks Against Software Applications", "comment": null, "summary": "This paper introduces a novel approach for the automated selection of\nsoftware protections to mitigate MATE risks against critical assets within\nsoftware applications. We formalize the key elements involved in protection\ndecision-making - including code artifacts, assets, security requirements,\nattacks, and software protections - and frame the protection process through a\ngame-theoretic model. In this model, a defender strategically applies\nprotections to various code artifacts of a target application, anticipating\nrepeated attack attempts by adversaries against the confidentiality and\nintegrity of the application's assets. The selection of the optimal defense\nmaximizes resistance to attacks while ensuring the application remains usable\nby constraining the overhead introduced by protections. The game is solved\nthrough a heuristic based on a mini-max depth-first exploration strategy,\naugmented with dynamic programming optimizations for improved efficiency.\nCentral to our formulation is the introduction of the Software Protection\nIndex, an original contribution that extends existing notions of potency and\nresilience by evaluating protection effectiveness against attack paths using\nsoftware metrics and expert assessments. We validate our approach through a\nproof-of-concept implementation and expert evaluations, demonstrating that\nautomated software protection is a practical and effective solution for risk\nmitigation in software."}
{"id": "2506.18348", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.18348", "abs": "https://arxiv.org/abs/2506.18348", "authors": ["Weilun Yu", "Shixiang Tang", "Yonggui Huang", "Nanqing Dong", "Li Fan", "Honggang Qi", "Wei Liu", "Xiaoli Diao", "Xi Chen", "Wanli Ouyang"], "title": "Dynamic Knowledge Exchange and Dual-diversity Review: Concisely Unleashing the Potential of a Multi-Agent Research Team", "comment": null, "summary": "Scientific progress increasingly relies on effective collaboration among\nresearchers, a dynamic that large language models (LLMs) have only begun to\nemulate. While recent LLM-based scientist agents show promise in autonomous\nscientific discovery, they often lack the interactive reasoning and evaluation\nmechanisms essential to real-world research. We propose IDVSCI (Internal\nDiscussion and Vote SCIentists), a multi-agent framework built on LLMs that\nincorporates two key innovations: a Dynamic Knowledge Exchange mechanism\nenabling iterative feedback among agents, and a Dual-Diversity Review paradigm\nthat simulates heterogeneous expert evaluation. These components jointly\npromote deeper reasoning and the generation of more creative and impactful\nscientific ideas. To evaluate the effectiveness and generalizability of our\napproach, we conduct experiments on two datasets: a widely used benchmark in\ncomputer science and a new dataset we introduce in the health sciences domain.\nResults show that IDVSCI consistently achieves the best performance across both\ndatasets, outperforming existing systems such as AI Scientist and VIRSCI. These\nfindings highlight the value of modeling interaction and peer review dynamics\nin LLM-based autonomous research."}
{"id": "2506.18516", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2506.18516", "abs": "https://arxiv.org/abs/2506.18516", "authors": ["Francesco Marchiori", "Marco Alecci", "Luca Pajola", "Mauro Conti"], "title": "DUMB and DUMBer: Is Adversarial Training Worth It in the Real World?", "comment": "Accepted at ESORICS 2025", "summary": "Adversarial examples are small and often imperceptible perturbations crafted\nto fool machine learning models. These attacks seriously threaten the\nreliability of deep neural networks, especially in security-sensitive domains.\nEvasion attacks, a form of adversarial attack where input is modified at test\ntime to cause misclassification, are particularly insidious due to their\ntransferability: adversarial examples crafted against one model often fool\nother models as well. This property, known as adversarial transferability,\ncomplicates defense strategies since it enables black-box attacks to succeed\nwithout direct access to the victim model. While adversarial training is one of\nthe most widely adopted defense mechanisms, its effectiveness is typically\nevaluated on a narrow and homogeneous population of models. This limitation\nhinders the generalizability of empirical findings and restricts practical\nadoption.\n  In this work, we introduce DUMBer, an attack framework built on the\nfoundation of the DUMB (Dataset soUrces, Model architecture, and Balance)\nmethodology, to systematically evaluate the resilience of adversarially trained\nmodels. Our testbed spans multiple adversarial training techniques evaluated\nacross three diverse computer vision tasks, using a heterogeneous population of\nuniquely trained models to reflect real-world deployment variability. Our\nexperimental pipeline comprises over 130k evaluations spanning 13\nstate-of-the-art attack algorithms, allowing us to capture nuanced behaviors of\nadversarial training under varying threat models and dataset conditions. Our\nfindings offer practical, actionable insights for AI practitioners, identifying\nwhich defenses are most effective based on the model, dataset, and attacker\nsetup."}
{"id": "2506.18424", "categories": ["cs.AI", "cs.ET"], "pdf": "https://arxiv.org/pdf/2506.18424", "abs": "https://arxiv.org/abs/2506.18424", "authors": ["Chengjie Liu", "Weiyu Chen", "Huiyao Xu", "Yuan Du", "Jun Yang", "Li Du"], "title": "A Large Language Model-based Multi-Agent Framework for Analog Circuits' Sizing Relationships Extraction", "comment": "Accepted by ISEDA 2025", "summary": "In the design process of the analog circuit pre-layout phase, device sizing\nis an important step in determining whether an analog circuit can meet the\nrequired performance metrics. Many existing techniques extract the circuit\nsizing task as a mathematical optimization problem to solve and continuously\nimprove the optimization efficiency from a mathematical perspective. But they\nignore the automatic introduction of prior knowledge, fail to achieve effective\npruning of the search space, which thereby leads to a considerable compression\nmargin remaining in the search space. To alleviate this problem, we propose a\nlarge language model (LLM)-based multi-agent framework for analog circuits'\nsizing relationships extraction from academic papers. The search space in the\nsizing process can be effectively pruned based on the sizing relationship\nextracted by this framework. Eventually, we conducted tests on 3 types of\ncircuits, and the optimization efficiency was improved by $2.32 \\sim 26.6\n\\times$. This work demonstrates that the LLM can effectively prune the search\nspace for analog circuit sizing, providing a new solution for the combination\nof LLMs and conventional analog circuit design automation methods."}
{"id": "2506.18543", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.18543", "abs": "https://arxiv.org/abs/2506.18543", "authors": ["Xiaodong Wu", "Xiangman Li", "Jianbing Ni"], "title": "Security Assessment of DeepSeek and GPT Series Models against Jailbreak Attacks", "comment": null, "summary": "The widespread deployment of large language models (LLMs) has raised critical\nconcerns over their vulnerability to jailbreak attacks, i.e., adversarial\nprompts that bypass alignment mechanisms and elicit harmful or policy-violating\noutputs. While proprietary models like GPT-4 have undergone extensive\nevaluation, the robustness of emerging open-source alternatives such as\nDeepSeek remains largely underexplored, despite their growing adoption in\nreal-world applications. In this paper, we present the first systematic\njailbreak evaluation of DeepSeek-series models, comparing them with GPT-3.5 and\nGPT-4 using the HarmBench benchmark. We evaluate seven representative attack\nstrategies across 510 harmful behaviors categorized by both function and\nsemantic domain. Our analysis reveals that DeepSeek's Mixture-of-Experts (MoE)\narchitecture introduces routing sparsity that offers selective robustness\nagainst optimization-based attacks such as TAP-T, but leads to significantly\nhigher vulnerability under prompt-based and manually engineered attacks. In\ncontrast, GPT-4 Turbo demonstrates stronger and more consistent safety\nalignment across diverse behaviors, likely due to its dense Transformer design\nand reinforcement learning from human feedback. Fine-grained behavioral\nanalysis and case studies further show that DeepSeek often routes adversarial\nprompts to under-aligned expert modules, resulting in inconsistent refusal\nbehaviors. These findings highlight a fundamental trade-off between\narchitectural efficiency and alignment generalization, emphasizing the need for\ntargeted safety tuning and modular alignment strategies to ensure secure\ndeployment of open-source LLMs."}
{"id": "2506.18428", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.18428", "abs": "https://arxiv.org/abs/2506.18428", "authors": ["Feng He", "Zhenyang Liu", "Marco Valentino", "Zhixue Zhao"], "title": "How Robust is Model Editing after Fine-Tuning? An Empirical Study on Text-to-Image Diffusion Models", "comment": null, "summary": "Model editing offers a low-cost technique to inject or correct a particular\nbehavior in a pre-trained model without extensive retraining, supporting\napplications such as factual correction and bias mitigation. Despite this\ncommon practice, it remains unknown whether edits persist after fine-tuning or\nwhether they are inadvertently reversed. This question has fundamental\npractical implications. For example, if fine-tuning removes prior edits, it\ncould serve as a defence mechanism against hidden malicious edits. Vice versa,\nthe unintended removal of edits related to bias mitigation could pose serious\nsafety concerns. We systematically investigate the interaction between model\nediting and fine-tuning in the context of T2I diffusion models, which are known\nto exhibit biases and generate inappropriate content. Our study spans two T2I\nmodel families (Stable Diffusion and FLUX), two sota editing techniques, and\nthree fine-tuning methods (DreamBooth, LoRA, and DoRA). Through an extensive\nempirical analysis across diverse editing tasks and evaluation metrics, our\nfindings reveal a trend: edits generally fail to persist through fine-tuning,\neven when fine-tuning is tangential or unrelated to the edits. Notably, we\nobserve that DoRA exhibits the strongest edit reversal effect. At the same\ntime, among editing methods, UCE demonstrates greater robustness, retaining\nsignificantly higher efficacy post-fine-tuning compared to ReFACT. These\nfindings highlight a crucial limitation in current editing methodologies,\nemphasizing the need for more robust techniques to ensure reliable long-term\ncontrol and alignment of deployed AI systems. These findings have dual\nimplications for AI safety: they suggest that fine-tuning could serve as a\nremediation mechanism for malicious edits while simultaneously highlighting the\nneed for re-editing after fine-tuning to maintain beneficial safety and\nalignment properties."}
{"id": "2506.18685", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2506.18685", "abs": "https://arxiv.org/abs/2506.18685", "authors": ["Yara Schütt", "Esfandiar Mohammadi"], "title": "Understanding the Theoretical Guarantees of DPM", "comment": null, "summary": "In this study, we conducted an in-depth examination of the utility analysis\nof the differentially private mechanism (DPM). The authors of DPM have already\nestablished the probability of a good split being selected and of DPM halting.\nIn this study, we expanded the analysis of the stopping criterion and provided\nan interpretation of these guarantees in the context of realistic input\ndistributions. Our findings revealed constraints on the minimum cluster size\nand the metric weight for the scoring function. Furthermore, we introduced an\ninterpretation of the utility of DPM through the lens of the clustering metric,\nthe silhouette score. Our findings indicate that even when an optimal DPM-based\nsplit is employed, the silhouette score of the resulting clustering may still\ndecline. This observation calls into question the suitability of the silhouette\nscore as a clustering metric. Finally, we examined the potential of the\nunderlying concept of DPM by linking it to a more theoretical view, that of\n$(\\xi, \\rho)$-separability. This extensive analysis of the theoretical\nguarantees of DPM allows a better understanding of its behaviour for arbitrary\ninputs. From these guarantees, we can analyse the impact of different\nhyperparameters and different input data sets, thereby promoting the\napplication of DPM in practice for unknown settings and data sets."}
{"id": "2506.18511", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.18511", "abs": "https://arxiv.org/abs/2506.18511", "authors": ["Yu Han", "Aaron Ceross", "Jeroen H. M. Bergmann"], "title": "Standard Applicability Judgment and Cross-jurisdictional Reasoning: A RAG-based Framework for Medical Device Compliance", "comment": null, "summary": "Identifying the appropriate regulatory standard applicability remains a\ncritical yet understudied challenge in medical device compliance, frequently\nnecessitating expert interpretation of fragmented and heterogeneous\ndocumentation across different jurisdictions. To address this challenge, we\nintroduce a modular AI system that leverages a retrieval-augmented generation\n(RAG) pipeline to automate standard applicability determination. Given a\nfree-text device description, our system retrieves candidate standards from a\ncurated corpus and uses large language models to infer jurisdiction-specific\napplicability, classified as Mandatory, Recommended, or Not Applicable, with\ntraceable justifications. We construct an international benchmark dataset of\nmedical device descriptions with expert-annotated standard mappings, and\nevaluate our system against retrieval-only, zero-shot, and rule-based\nbaselines. The proposed approach attains a classification accuracy of 73% and a\nTop-5 retrieval recall of 87%, demonstrating its effectiveness in identifying\nrelevant regulatory standards. We introduce the first end-to-end system for\nstandard applicability reasoning, enabling scalable and interpretable\nAI-supported regulatory science. Notably, our region-aware RAG agent performs\ncross-jurisdictional reasoning between Chinese and U.S. standards, supporting\nconflict resolution and applicability justification across regulatory\nframeworks."}
{"id": "2506.18715", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2506.18715", "abs": "https://arxiv.org/abs/2506.18715", "authors": ["Stefano Perone", "Simone Guarino", "Luca Faramondi", "Roberto Setola"], "title": "Vulnerability Assessment Combining CVSS Temporal Metrics and Bayesian Networks", "comment": "This paper has been accepted for the 2025 IEEE International\n  Conference on Cyber Security and Resilience (CSR), Chania, Crete, Greece,\n  August 4-6 2025", "summary": "Vulnerability assessment is a critical challenge in cybersecurity,\nparticularly in industrial environments. This work presents an innovative\napproach by incorporating the temporal dimension into vulnerability assessment,\nan aspect neglected in existing literature. Specifically, this paper focuses on\nrefining vulnerability assessment and prioritization by integrating Common\nVulnerability Scoring System (CVSS) Temporal Metrics with Bayesian Networks to\naccount for exploit availability, remediation efforts, and confidence in\nreported vulnerabilities. Through probabilistic modeling, Bayesian networks\nenable a structured and adaptive evaluation of vulnerabilities, allowing for\nmore accurate prioritization and decision-making. The proposed approach\ndynamically computes the Temporal Score and updates the CVSS Base Score by\nprocessing data on exploits and fixes from vulnerability databases."}
{"id": "2506.18538", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.18538", "abs": "https://arxiv.org/abs/2506.18538", "authors": ["Rifat Ara Shams", "Didar Zowghi", "Muneera Bano"], "title": "A Question Bank to Assess AI Inclusivity: Mapping out the Journey from Diversity Errors to Inclusion Excellence", "comment": null, "summary": "Ensuring diversity and inclusion (D&I) in artificial intelligence (AI) is\ncrucial for mitigating biases and promoting equitable decision-making. However,\nexisting AI risk assessment frameworks often overlook inclusivity, lacking\nstandardized tools to measure an AI system's alignment with D&I principles.\nThis paper introduces a structured AI inclusivity question bank, a\ncomprehensive set of 253 questions designed to evaluate AI inclusivity across\nfive pillars: Humans, Data, Process, System, and Governance. The development of\nthe question bank involved an iterative, multi-source approach, incorporating\ninsights from literature reviews, D&I guidelines, Responsible AI frameworks,\nand a simulated user study. The simulated evaluation, conducted with 70\nAI-generated personas related to different AI jobs, assessed the question\nbank's relevance and effectiveness for AI inclusivity across diverse roles and\napplication domains. The findings highlight the importance of integrating D&I\nprinciples into AI development workflows and governance structures. The\nquestion bank provides an actionable tool for researchers, practitioners, and\npolicymakers to systematically assess and enhance the inclusivity of AI\nsystems, paving the way for more equitable and responsible AI technologies."}
{"id": "2506.18767", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2506.18767", "abs": "https://arxiv.org/abs/2506.18767", "authors": ["Yifan Zhang", "Yongchao Dang", "Masoud Kaveh", "Zheng Yan", "Riku Jäntti", "Zhu Han"], "title": "Physical Layer Challenge-Response Authentication between Ambient Backscatter Devices", "comment": null, "summary": "Ambient backscatter communication (AmBC) has become an integral part of\nubiquitous Internet of Things (IoT) applications due to its energy-harvesting\ncapabilities and ultra-low-power consumption. However, the open wireless\nenvironment exposes AmBC systems to various attacks, and existing\nauthentication methods cannot be implemented between resource-constrained\nbackscatter devices (BDs) due to their high computational demands.To this end,\nthis paper proposes PLCRA-BD, a novel physical layer challenge-response\nauthentication scheme between BDs in AmBC that overcomes BDs' limitations,\nsupports high mobility, and performs robustly against impersonation and\nwireless attacks. It constructs embedded keys as physical layer fingerprints\nfor lightweight identification and designs a joint transceiver that integrates\nBDs' backscatter waveform with receiver functionality to mitigate interference\nfrom ambient RF signals by exploiting repeated patterns in OFDM symbols. Based\non this, a challenge-response authentication procedure is introduced to enable\nlow-complexity fingerprint exchange between two paired BDs leveraging channel\ncoherence, while securing the exchange process using a random number and\nunpredictable channel fading. Additionally, we optimize the authentication\nprocedure for high-mobility scenarios, completing exchanges within the channel\ncoherence time to minimize the impact of dynamic channel fluctuations. Security\nanalysis confirms its resistance against impersonation, eavesdropping, replay,\nand counterfeiting attacks. Extensive simulations validate its effectiveness in\nresource-constrained BDs, demonstrating high authentication accuracy across\ndiverse channel conditions, robustness against multiple wireless attacks, and\nsuperior efficiency compared to traditional authentication schemes."}
{"id": "2506.18559", "categories": ["cs.AI", "cs.LO", "I.2.7; F.4.1"], "pdf": "https://arxiv.org/pdf/2506.18559", "abs": "https://arxiv.org/abs/2506.18559", "authors": ["Hong Qing Yu"], "title": "T-CPDL: A Temporal Causal Probabilistic Description Logic for Developing Logic-RAG Agent", "comment": null, "summary": "Large language models excel at generating fluent text but frequently struggle\nwith structured reasoning involving temporal constraints, causal relationships,\nand probabilistic reasoning. To address these limitations, we propose Temporal\nCausal Probabilistic Description Logic (T-CPDL), an integrated framework that\nextends traditional Description Logic with temporal interval operators,\nexplicit causal relationships, and probabilistic annotations. We present two\ndistinct variants of T-CPDL: one capturing qualitative temporal relationships\nthrough Allen's interval algebra, and another variant enriched with explicit\ntimestamped causal assertions. Both variants share a unified logical structure,\nenabling complex reasoning tasks ranging from simple temporal ordering to\nnuanced probabilistic causation. Empirical evaluations on temporal reasoning\nand causal inference benchmarks confirm that T-CPDL substantially improves\ninference accuracy, interpretability, and confidence calibration of language\nmodel outputs. By delivering transparent reasoning paths and fine-grained\ntemporal and causal semantics, T-CPDL significantly enhances the capability of\nlanguage models to support robust, explainable, and trustworthy\ndecision-making. This work also lays the groundwork for developing advanced\nLogic-Retrieval-Augmented Generation (Logic-RAG) frameworks, potentially\nboosting the reasoning capabilities and efficiency of knowledge graph-enhanced\nRAG systems."}
{"id": "2506.18780", "categories": ["cs.CR", "cs.AR"], "pdf": "https://arxiv.org/pdf/2506.18780", "abs": "https://arxiv.org/abs/2506.18780", "authors": ["Shuangbao Paul Wang"], "title": "Design high-confidence computers using trusted instructional set architecture and emulators", "comment": null, "summary": "High-confidence computing relies on trusted instructional set architecture,\nsealed kernels, and secure operating systems. Cloud computing depends on\ntrusted systems for virtualization tasks. Branch predictions and pipelines are\nessential in improving performance of a CPU/GPU. But Spectre and Meltdown make\nmodern processors vulnerable to be exploited. Disabling the prediction and\npipeline is definitely not a good solution. On the other hand, current software\npatches can only address non-essential issues around Meltdown. This paper\nintroduces a holistic approach in trusted computer architecture design and\nemulation."}
{"id": "2506.18586", "categories": ["cs.AI", "cs.CE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.18586", "abs": "https://arxiv.org/abs/2506.18586", "authors": ["Zijie Yang", "Qiji Zhou", "Fang Guo", "Sijie Zhang", "Yexun Xi", "Jinglei Nie", "Yudian Zhu", "Liping Huang", "Chou Wu", "Yonghe Xia", "Xiaoyu Ma", "Yingming Pu", "Panzhong Lu", "Junshu Pan", "Mingtao Chen", "Tiannan Guo", "Yanmei Dou", "Hongyu Chen", "Anping Zeng", "Jiaxing Huang", "Tian Xu", "Yue Zhang"], "title": "Airalogy: AI-empowered universal data digitization for research automation", "comment": "146 pages, 6 figures, 49 supplementary figures", "summary": "Research data are the foundation of Artificial Intelligence (AI)-driven\nscience, yet current AI applications remain limited to a few fields with\nreadily available, well-structured, digitized datasets. Achieving comprehensive\nAI empowerment across multiple disciplines is still out of reach. Present-day\nresearch data collection is often fragmented, lacking unified standards,\ninefficiently managed, and difficult to share. Creating a single platform for\nstandardized data digitization needs to overcome the inherent challenge of\nbalancing between universality (supporting the diverse, ever-evolving needs of\nvarious disciplines) and standardization (enforcing consistent formats to fully\nenable AI). No existing platform accommodates both facets. Building a truly\nmultidisciplinary platform requires integrating scientific domain knowledge\nwith sophisticated computing skills. Researchers often lack the computational\nexpertise to design customized and standardized data recording methods, whereas\nplatform developers rarely grasp the intricate needs of multiple scientific\ndomains. These gaps impede research data standardization and hamper AI-driven\nprogress. In this study, we address these challenges by developing Airalogy\n(https://airalogy.com), the world's first AI- and community-driven platform\nthat balances universality and standardization for digitizing research data\nacross multiple disciplines. Airalogy represents entire research workflows\nusing customizable, standardized data records and offers an advanced AI\nresearch copilot for intelligent Q&A, automated data entry, analysis, and\nresearch automation. Already deployed in laboratories across all four schools\nof Westlake University, Airalogy has the potential to accelerate and automate\nscientific innovation in universities, industry, and the global research\ncommunity-ultimately benefiting humanity as a whole."}
{"id": "2506.18795", "categories": ["cs.CR", "cs.SE", "D.2.4; I.2.7"], "pdf": "https://arxiv.org/pdf/2506.18795", "abs": "https://arxiv.org/abs/2506.18795", "authors": ["Jiachi Chen", "Yiming Shen", "Jiashuo Zhang", "Zihao Li", "John Grundy", "Zhenzhe Shao", "Yanlin Wang", "Jiashui Wang", "Ting Chen", "Zibin Zheng"], "title": "FORGE: An LLM-driven Framework for Large-Scale Smart Contract Vulnerability Dataset Construction", "comment": "Accepted for the 48th International Conference on Software\n  Engineering (ICSE 2026)", "summary": "High-quality smart contract vulnerability datasets are critical for\nevaluating security tools and advancing smart contract security research. Two\nmajor limitations of current manual dataset construction are (1)\nlabor-intensive and error-prone annotation processes limiting the scale,\nquality, and evolution of the dataset, and (2) absence of standardized\nclassification rules results in inconsistent vulnerability categories and\nlabeling results across different datasets. To address these limitations, we\npresent FORGE, the first automated approach for constructing smart contract\nvulnerability datasets. FORGE leverages an LLM-driven pipeline to extract\nhigh-quality vulnerabilities from real-world audit reports and classify them\naccording to the CWE, the most widely recognized classification in software\nsecurity. FORGE employs a divide-and-conquer strategy to extract structured and\nself-contained vulnerability information from these reports. Additionally, it\nuses a tree-of-thoughts technique to classify the vulnerability information\ninto the hierarchical CWE classification. To evaluate FORGE's effectiveness, we\nrun FORGE on 6,454 real-world audit reports and generate a dataset comprising\n81,390 solidity files and 27,497 vulnerability findings across 296 CWE\ncategories. Manual assessment of the dataset demonstrates high extraction\nprecision and classification consistency with human experts (precision of 95.6%\nand inter-rater agreement k-$\\alpha$ of 0.87). We further validate the\npracticality of our dataset by benchmarking 13 existing security tools on our\ndataset. The results reveal the significant limitations in current detection\ncapabilities. Furthermore, by analyzing the severity-frequency distribution\npatterns through a unified CWE perspective in our dataset, we highlight\ninconsistency between current smart contract research focus and priorities\nidentified from real-world vulnerabilities..."}
{"id": "2506.18628", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.18628", "abs": "https://arxiv.org/abs/2506.18628", "authors": ["Piotr Matys", "Jan Eliasz", "Konrad Kiełczyński", "Mikołaj Langner", "Teddy Ferdinan", "Jan Kocoń", "Przemysław Kazienko"], "title": "AggTruth: Contextual Hallucination Detection using Aggregated Attention Scores in LLMs", "comment": "ICCS 2025 Workshops", "summary": "In real-world applications, Large Language Models (LLMs) often hallucinate,\neven in Retrieval-Augmented Generation (RAG) settings, which poses a\nsignificant challenge to their deployment. In this paper, we introduce\nAggTruth, a method for online detection of contextual hallucinations by\nanalyzing the distribution of internal attention scores in the provided context\n(passage). Specifically, we propose four different variants of the method, each\nvarying in the aggregation technique used to calculate attention scores. Across\nall LLMs examined, AggTruth demonstrated stable performance in both same-task\nand cross-task setups, outperforming the current SOTA in multiple scenarios.\nFurthermore, we conducted an in-depth analysis of feature selection techniques\nand examined how the number of selected attention heads impacts detection\nperformance, demonstrating that careful selection of heads is essential to\nachieve optimal results."}
{"id": "2506.18848", "categories": ["cs.CR", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2506.18848", "abs": "https://arxiv.org/abs/2506.18848", "authors": ["Sara D. Cardell"], "title": "Cellular Automata as Generators of Interleaving Sequences", "comment": null, "summary": "An interleaving sequence is obtained by combining or intertwining elements\nfrom two or more sequences. On the other hand, cellular automata are known to\nbe generators for keystream sequences. In this paper we present two families of\none-dimensional cellular automata as generators of interleaving sequences. This\nstudy aims to close a notable gap within the current body of literature by\nexploring the capacity of cellular automata to generate interleaving sequences.\nWhile previous works have separately examined cellular automata as sequence\ngenerators and interleaving sequences, there exists limited literature\ninterconnecting these two topics. Our study seeks to bridge this gap, providing\nperspectives on the generation of interleaving sequences through the\nutilisation of cellular automata, thereby fostering a deeper understanding of\nboth disciplines."}
{"id": "2506.18651", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.18651", "abs": "https://arxiv.org/abs/2506.18651", "authors": ["Shuocun Yang", "Huawen Hu", "Enze Shi", "Shu Zhang"], "title": "Dual-level Behavioral Consistency for Inter-group and Intra-group Coordination in Multi-Agent Systems", "comment": null, "summary": "Behavioral diversity in Multi-agent reinforcement learning(MARL) represents\nan emerging and promising research area. Prior work has largely centered on\nintra-group behavioral consistency in multi-agent systems, with limited\nattention given to behavioral consistency in multi-agent grouping scenarios. In\nthis paper, we introduce Dual-Level Behavioral Consistency (DLBC), a novel MARL\ncontrol method designed to explicitly regulate agent behaviors at both\nintra-group and inter-group levels. DLBC partitions agents into distinct groups\nand dynamically modulates behavioral diversity both within and between these\ngroups. By dynamically modulating behavioral diversity within and between these\ngroups, DLBC achieves enhanced division of labor through inter-group\nconsistency, which constrains behavioral strategies across different groups.\nSimultaneously, intra-group consistency, achieved by aligning behavioral\nstrategies within each group, fosters stronger intra-group cooperation.\nCrucially, DLBC's direct constraint of agent policy functions ensures its broad\napplicability across various algorithmic frameworks. Experimental results in\nvarious grouping cooperation scenarios demonstrate that DLBC significantly\nenhances both intra-group cooperative performance and inter-group task\nspecialization, yielding substantial performance improvements. DLBC provides\nnew ideas for behavioral consistency control of multi-intelligent body systems,\nand its potential for application in more complex tasks and dynamic\nenvironments can be further explored in the future."}
{"id": "2506.18870", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2506.18870", "abs": "https://arxiv.org/abs/2506.18870", "authors": ["Yugeng Liu", "Zheng Li", "Hai Huang", "Michael Backes", "Yang Zhang"], "title": "Amplifying Machine Learning Attacks Through Strategic Compositions", "comment": null, "summary": "Machine learning (ML) models are proving to be vulnerable to a variety of\nattacks that allow the adversary to learn sensitive information, cause\nmispredictions, and more. While these attacks have been extensively studied,\ncurrent research predominantly focuses on analyzing each attack type\nindividually. In practice, however, adversaries may employ multiple attack\nstrategies simultaneously rather than relying on a single approach. This\nprompts a crucial yet underexplored question: When the adversary has multiple\nattacks at their disposal, are they able to mount or amplify the effect of one\nattack with another? In this paper, we take the first step in studying the\nstrategic interactions among different attacks, which we define as attack\ncompositions. Specifically, we focus on four well-studied attacks during the\nmodel's inference phase: adversarial examples, attribute inference, membership\ninference, and property inference. To facilitate the study of their\ninteractions, we propose a taxonomy based on three stages of the attack\npipeline: preparation, execution, and evaluation. Using this taxonomy, we\nidentify four effective attack compositions, such as property inference\nassisting attribute inference at its preparation level and adversarial examples\nassisting property inference at its execution level. We conduct extensive\nexperiments on the attack compositions using three ML model architectures and\nthree benchmark image datasets. Empirical results demonstrate the effectiveness\nof these four attack compositions. We implement and release a modular reusable\ntoolkit, COAT. Arguably, our work serves as a call for researchers and\npractitioners to consider advanced adversarial settings involving multiple\nattack strategies, aiming to strengthen the security and robustness of AI\nsystems."}
{"id": "2506.18777", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.18777", "abs": "https://arxiv.org/abs/2506.18777", "authors": ["Jonathan Cook", "Silvia Sapora", "Arash Ahmadian", "Akbir Khan", "Tim Rocktaschel", "Jakob Foerster", "Laura Ruis"], "title": "Programming by Backprop: LLMs Acquire Reusable Algorithmic Abstractions During Code Training", "comment": null, "summary": "Training large language models (LLMs) on source code significantly enhances\ntheir general-purpose reasoning abilities, but the mechanisms underlying this\ngeneralisation are poorly understood. In this paper, we propose Programming by\nBackprop (PBB) as a potential driver of this effect - teaching a model to\nevaluate a program for inputs by training on its source code alone, without\never seeing I/O examples. To explore this idea, we finetune LLMs on two sets of\nprograms representing simple maths problems and algorithms: one with source\ncode and I/O examples (w/ IO), the other with source code only (w/o IO). We\nfind evidence that LLMs have some ability to evaluate w/o IO programs for\ninputs in a range of experimental settings, and make several observations.\nFirstly, PBB works significantly better when programs are provided as code\nrather than semantically equivalent language descriptions. Secondly, LLMs can\nproduce outputs for w/o IO programs directly, by implicitly evaluating the\nprogram within the forward pass, and more reliably when stepping through the\nprogram in-context via chain-of-thought. We further show that PBB leads to more\nrobust evaluation of programs across inputs than training on I/O pairs drawn\nfrom a distribution that mirrors naturally occurring data. Our findings suggest\na mechanism for enhanced reasoning through code training: it allows LLMs to\ninternalise reusable algorithmic abstractions. Significant scope remains for\nfuture work to enable LLMs to more effectively learn from symbolic procedures,\nand progress in this direction opens other avenues like model alignment by\ntraining on formal constitutional principles."}
{"id": "2506.17798", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2506.17798", "abs": "https://arxiv.org/abs/2506.17798", "authors": ["Wang Lingxiang", "Quanzhi Fu", "Wenjia Song", "Gelei Deng", "Yi Liu", "Dan Williams", "Ying Zhang"], "title": "SAVANT: Vulnerability Detection in Application Dependencies through Semantic-Guided Reachability Analysis", "comment": null, "summary": "The integration of open-source third-party library dependencies in Java\ndevelopment introduces significant security risks when these libraries contain\nknown vulnerabilities. Existing Software Composition Analysis (SCA) tools\nstruggle to effectively detect vulnerable API usage from these libraries due to\nlimitations in understanding API usage semantics and computational challenges\nin analyzing complex codebases, leading to inaccurate vulnerability alerts that\nburden development teams and delay critical security fixes.\n  To address these challenges, we proposed SAVANT by leveraging two insights:\nproof-of-vulnerability test cases demonstrate how vulnerabilities can be\ntriggered in specific contexts, and Large Language Models (LLMs) can understand\ncode semantics. SAVANT combines semantic preprocessing with LLM-powered context\nanalysis for accurate vulnerability detection. SAVANT first segments source\ncode into meaningful blocks while preserving semantic relationships, then\nleverages LLM-based reflection to analyze API usage context and determine\nactual vulnerability impacts. Our evaluation on 55 real-world applications\nshows that SAVANT achieves 83.8% precision, 73.8% recall, 69.0% accuracy, and\n78.5% F1-score, outperforming state-of-the-art SCA tools."}
{"id": "2506.18783", "categories": ["cs.AI", "cs.MA", "68T07", "I.2.11; I.2.7; I.2.8"], "pdf": "https://arxiv.org/pdf/2506.18783", "abs": "https://arxiv.org/abs/2506.18783", "authors": ["Kamil Szczepanik", "Jarosław A. Chudziak"], "title": "TRIZ Agents: A Multi-Agent LLM Approach for TRIZ-Based Innovation", "comment": "12 pages, 10 figures, 2 tables, Accepted at the 17th International\n  Conference on Agents and Artificial Intelligence (ICAART 2025). Final version\n  published in Proceedings of ICAART 2025 (Vol. 1), pages 196-207", "summary": "TRIZ, the Theory of Inventive Problem Solving, is a structured,\nknowledge-based framework for innovation and abstracting problems to find\ninventive solutions. However, its application is often limited by the\ncomplexity and deep interdisciplinary knowledge required. Advancements in Large\nLanguage Models (LLMs) have revealed new possibilities for automating parts of\nthis process. While previous studies have explored single LLMs in TRIZ\napplications, this paper introduces a multi-agent approach. We propose an\nLLM-based multi-agent system, called TRIZ agents, each with specialized\ncapabilities and tool access, collaboratively solving inventive problems based\non the TRIZ methodology. This multi-agent system leverages agents with various\ndomain expertise to efficiently navigate TRIZ steps. The aim is to model and\nsimulate an inventive process with language agents. We assess the effectiveness\nof this team of agents in addressing complex innovation challenges based on a\nselected case study in engineering. We demonstrate the potential of agent\ncollaboration to produce diverse, inventive solutions. This research\ncontributes to the future of AI-driven innovation, showcasing the advantages of\ndecentralized problem-solving in complex ideation tasks."}
{"id": "2506.18810", "categories": ["cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.18810", "abs": "https://arxiv.org/abs/2506.18810", "authors": ["Siao Tang", "Xinyin Ma", "Gongfan Fang", "Xinchao Wang"], "title": "ConciseHint: Boosting Efficient Reasoning via Continuous Concise Hints during Generation", "comment": "Codes are available at https://github.com/tsa18/ConciseHint", "summary": "Recent advancements in large reasoning models (LRMs) like DeepSeek-R1 and\nOpenAI o1 series have achieved notable performance enhancements on complex\nreasoning tasks by scaling up the generation length by Chain-of-Thought (CoT).\nHowever, an emerging issue is their inclination to produce excessively verbose\nreasoning processes, leading to the inefficiency problem. Existing literature\non improving efficiency mainly adheres to the before-reasoning paradigms such\nas prompting and reasoning or fine-tuning and reasoning, but ignores the\npromising direction of directly encouraging the model to speak concisely by\nintervening during the generation of reasoning. In order to fill the blank, we\npropose a framework dubbed ConciseHint, which continuously encourages the\nreasoning model to speak concisely by injecting the textual hint (manually\ndesigned or trained on the concise data) during the token generation of the\nreasoning process. Besides, ConciseHint is adaptive to the complexity of the\nquery by adaptively adjusting the hint intensity, which ensures it will not\nundermine model performance. Experiments on the state-of-the-art LRMs,\nincluding DeepSeek-R1 and Qwen-3 series, demonstrate that our method can\neffectively produce concise reasoning processes while maintaining performance\nwell. For instance, we achieve a reduction ratio of 65\\% for the reasoning\nlength on GSM8K benchmark with Qwen-3 4B with nearly no accuracy loss."}
{"id": "2506.18887", "categories": ["cs.AI", "cs.LG", "cs.SY", "eess.SY", "I.2.7; I.2.6; I.2.1; D.3.3; C.4"], "pdf": "https://arxiv.org/pdf/2506.18887", "abs": "https://arxiv.org/abs/2506.18887", "authors": ["Vansh Sharma", "Venkat Raman"], "title": "Steering Conceptual Bias via Transformer Latent-Subspace Activation", "comment": null, "summary": "This work examines whether activating latent subspaces in language models\n(LLMs) can steer scientific code generation toward a specific programming\nlanguage. Five causal LLMs were first evaluated on scientific coding prompts to\nquantify their baseline bias among four programming languages. A static\nneuron-attribution method, perturbing the highest activated MLP weight for a\nC++ or CPP token, proved brittle and exhibited limited generalization across\nprompt styles and model scales. To address these limitations, a\ngradient-refined adaptive activation steering framework (G-ACT) was developed:\nper-prompt activation differences are clustered into a small set of steering\ndirections, and lightweight per-layer probes are trained and refined online to\nselect the appropriate steering vector. In LLaMA-3.2 3B, this approach reliably\nbiases generation towards the CPP language by increasing the average probe\nclassification accuracy by 15% and the early layers (0-6) improving the probe\nclassification accuracy by 61.5% compared to the standard ACT framework. For\nLLaMA-3.3 70B, where attention-head signals become more diffuse, targeted\ninjections at key layers still improve language selection. Although per-layer\nprobing introduces a modest inference overhead, it remains practical by\nsteering only a subset of layers and enables reproducible model behavior. These\nresults demonstrate a scalable, interpretable and efficient mechanism for\nconcept-level control for practical agentic systems."}
{"id": "2506.18902", "categories": ["cs.AI", "cs.CL", "cs.IR", "68T50", "I.2.7"], "pdf": "https://arxiv.org/pdf/2506.18902", "abs": "https://arxiv.org/abs/2506.18902", "authors": ["Michael Günther", "Saba Sturua", "Mohammad Kalim Akram", "Isabelle Mohr", "Andrei Ungureanu", "Sedigheh Eslami", "Scott Martens", "Bo Wang", "Nan Wang", "Han Xiao"], "title": "jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval", "comment": "22 pages, 1-10 main, 14-22 experimental results, benchmark tables", "summary": "We introduce jina-embeddings-v4, a 3.8 billion parameter multimodal embedding\nmodel that unifies text and image representations through a novel architecture\nsupporting both single-vector and multi-vector embeddings in the late\ninteraction style. The model incorporates task-specific Low-Rank Adaptation\n(LoRA) adapters to optimize performance across diverse retrieval scenarios,\nincluding query-based information retrieval, cross-modal semantic similarity,\nand programming code search. Comprehensive evaluations demonstrate that\njina-embeddings-v4 achieves state-of-the-art performance on both single- modal\nand cross-modal retrieval tasks, with particular strength in processing\nvisually rich content such as tables, charts, diagrams, and mixed-media\nformats. To facilitate evaluation of this capability, we also introduce\nJina-VDR, a novel benchmark specifically designed for visually rich image\nretrieval."}
{"id": "2506.17279", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17279", "abs": "https://arxiv.org/abs/2506.17279", "authors": ["Yash Sinha", "Manit Baser", "Murari Mandal", "Dinil Mon Divakaran", "Mohan Kankanhalli"], "title": "Step-by-Step Reasoning Attack: Revealing 'Erased' Knowledge in Large Language Models", "comment": null, "summary": "Knowledge erasure in large language models (LLMs) is important for ensuring\ncompliance with data and AI regulations, safeguarding user privacy, mitigating\nbias, and misinformation. Existing unlearning methods aim to make the process\nof knowledge erasure more efficient and effective by removing specific\nknowledge while preserving overall model performance, especially for retained\ninformation. However, it has been observed that the unlearning techniques tend\nto suppress and leave the knowledge beneath the surface, thus making it\nretrievable with the right prompts. In this work, we demonstrate that\n\\textit{step-by-step reasoning} can serve as a backdoor to recover this hidden\ninformation. We introduce a step-by-step reasoning-based black-box attack,\nSleek, that systematically exposes unlearning failures. We employ a structured\nattack framework with three core components: (1) an adversarial prompt\ngeneration strategy leveraging step-by-step reasoning built from LLM-generated\nqueries, (2) an attack mechanism that successfully recalls erased content, and\nexposes unfair suppression of knowledge intended for retention and (3) a\ncategorization of prompts as direct, indirect, and implied, to identify which\nquery types most effectively exploit unlearning weaknesses. Through extensive\nevaluations on four state-of-the-art unlearning techniques and two widely used\nLLMs, we show that existing approaches fail to ensure reliable knowledge\nremoval. Of the generated adversarial prompts, 62.5% successfully retrieved\nforgotten Harry Potter facts from WHP-unlearned Llama, while 50% exposed unfair\nsuppression of retained knowledge. Our work highlights the persistent risks of\ninformation leakage, emphasizing the need for more robust unlearning strategies\nfor erasure."}
{"id": "2506.17292", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17292", "abs": "https://arxiv.org/abs/2506.17292", "authors": ["Quan Nguyen", "Minh N. Vu", "Truc Nguyen", "My T. Thai"], "title": "Theoretically Unmasking Inference Attacks Against LDP-Protected Clients in Federated Vision Models", "comment": "Accepted to ICML 2025", "summary": "Federated Learning enables collaborative learning among clients via a\ncoordinating server while avoiding direct data sharing, offering a perceived\nsolution to preserve privacy. However, recent studies on Membership Inference\nAttacks (MIAs) have challenged this notion, showing high success rates against\nunprotected training data. While local differential privacy (LDP) is widely\nregarded as a gold standard for privacy protection in data analysis, most\nstudies on MIAs either neglect LDP or fail to provide theoretical guarantees\nfor attack success rates against LDP-protected data. To address this gap, we\nderive theoretical lower bounds for the success rates of low-polynomial time\nMIAs that exploit vulnerabilities in fully connected or self-attention layers.\nWe establish that even when data are protected by LDP, privacy risks persist,\ndepending on the privacy budget. Practical evaluations on federated vision\nmodels confirm considerable privacy risks, revealing that the noise required to\nmitigate these attacks significantly degrades models' utility."}
{"id": "2506.17299", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.17299", "abs": "https://arxiv.org/abs/2506.17299", "authors": ["Shuyi Lin", "Anshuman Suri", "Alina Oprea", "Cheng Tan"], "title": "LLM Jailbreak Oracle", "comment": null, "summary": "As large language models (LLMs) become increasingly deployed in\nsafety-critical applications, the lack of systematic methods to assess their\nvulnerability to jailbreak attacks presents a critical security gap. We\nintroduce the jailbreak oracle problem: given a model, prompt, and decoding\nstrategy, determine whether a jailbreak response can be generated with\nlikelihood exceeding a specified threshold. This formalization enables a\nprincipled study of jailbreak vulnerabilities. Answering the jailbreak oracle\nproblem poses significant computational challenges -- the search space grows\nexponentially with the length of the response tokens. We present Boa, the first\nefficient algorithm for solving the jailbreak oracle problem. Boa employs a\nthree-phase search strategy: (1) constructing block lists to identify refusal\npatterns, (2) breadth-first sampling to identify easily accessible jailbreaks,\nand (3) depth-first priority search guided by fine-grained safety scores to\nsystematically explore promising low-probability paths. Boa enables rigorous\nsecurity assessments including systematic defense evaluation, standardized\ncomparison of red team attacks, and model certification under extreme\nadversarial conditions."}
{"id": "2506.17318", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17318", "abs": "https://arxiv.org/abs/2506.17318", "authors": ["Atharv Singh Patlan", "Ashwin Hebbar", "Pramod Viswanath", "Prateek Mittal"], "title": "Context manipulation attacks : Web agents are susceptible to corrupted memory", "comment": "10 pages, 6 figures", "summary": "Autonomous web navigation agents, which translate natural language\ninstructions into sequences of browser actions, are increasingly deployed for\ncomplex tasks across e-commerce, information retrieval, and content discovery.\nDue to the stateless nature of large language models (LLMs), these agents rely\nheavily on external memory systems to maintain context across interactions.\nUnlike centralized systems where context is securely stored server-side, agent\nmemory is often managed client-side or by third-party applications, creating\nsignificant security vulnerabilities. This was recently exploited to attack\nproduction systems.\n  We introduce and formalize \"plan injection,\" a novel context manipulation\nattack that corrupts these agents' internal task representations by targeting\nthis vulnerable context. Through systematic evaluation of two popular web\nagents, Browser-use and Agent-E, we show that plan injections bypass robust\nprompt injection defenses, achieving up to 3x higher attack success rates than\ncomparable prompt-based attacks. Furthermore, \"context-chained injections,\"\nwhich craft logical bridges between legitimate user goals and attacker\nobjectives, lead to a 17.7% increase in success rate for privacy exfiltration\ntasks. Our findings highlight that secure memory handling must be a first-class\nconcern in agentic systems."}
{"id": "2506.17329", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17329", "abs": "https://arxiv.org/abs/2506.17329", "authors": ["Pedro H. Lui", "Lucas P. Siqueira", "Juliano F. Kazienko", "Vagner E. Quincozes", "Silvio E. Quincozes", "Daniel Welfer"], "title": "On the Performance of Cyber-Biomedical Features for Intrusion Detection in Healthcare 5.0", "comment": "12 pages, 7 figures, conference", "summary": "Healthcare 5.0 integrates Artificial Intelligence (AI), the Internet of\nThings (IoT), real-time monitoring, and human-centered design toward\npersonalized medicine and predictive diagnostics. However, the increasing\nreliance on interconnected medical technologies exposes them to cyber threats.\nMeanwhile, current AI-driven cybersecurity models often neglect biomedical\ndata, limiting their effectiveness and interpretability. This study addresses\nthis gap by applying eXplainable AI (XAI) to a Healthcare 5.0 dataset that\nintegrates network traffic and biomedical sensor data. Classification outputs\nindicate that XGBoost achieved 99% F1-score for benign and data alteration, and\n81% for spoofing. Explainability findings reveal that network data play a\ndominant role in intrusion detection whereas biomedical features contributed to\nspoofing detection, with temperature reaching a Shapley values magnitude of\n0.37."}
{"id": "2506.17335", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17335", "abs": "https://arxiv.org/abs/2506.17335", "authors": ["Shuo Yan", "Ruochen Li", "Ziming Luo", "Zimu Wang", "Daoyang Li", "Liqiang Jing", "Kaiyu He", "Peilin Wu", "George Michalopoulos", "Yue Zhang", "Ziyang Zhang", "Mian Zhang", "Zhiyu Chen", "Xinya Du"], "title": "LMR-BENCH: Evaluating LLM Agent's Ability on Reproducing Language Modeling Research", "comment": null, "summary": "Large language model (LLM) agents have demonstrated remarkable potential in\nadvancing scientific discovery. However, their capability in the fundamental\nyet crucial task of reproducing code from research papers, especially in the\nNLP domain, remains underexplored. This task includes unique complex reasoning\nchallenges in the intellectual synthesis of abstract concepts and the\ncomprehension of code repositories with interdependent files. Motivated by this\ngap, we present LMR-BENCH, a benchmark designed to systematically evaluate the\ncapability of LLM agents on code reproduction from Language Modeling Research.\nIt consists of 28 code reproduction tasks derived from 23 research papers\npublished in top-tier NLP venues over the past five years, spanning nine\nfundamental categories. Models are provided with a research paper, a code\nrepository containing one or more masked functions, and instructions for\nimplementing these functions. We conduct extensive experiments in standard\nprompting and LLM agent settings with state-of-the-art LLMs, evaluating the\naccuracy of unit tests and performing LLM-based evaluation of code correctness.\nExperimental results reveal that even the most advanced models still exhibit\npersistent limitations in scientific reasoning and code synthesis, highlighting\ncritical gaps in LLM agents' ability to autonomously reproduce scientific\nresearch"}
{"id": "2506.17350", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17350", "abs": "https://arxiv.org/abs/2506.17350", "authors": ["Yinghao Wu", "Liyan Zhang"], "title": "CUBA: Controlled Untargeted Backdoor Attack against Deep Neural Networks", "comment": null, "summary": "Backdoor attacks have emerged as a critical security threat against deep\nneural networks in recent years. The majority of existing backdoor attacks\nfocus on targeted backdoor attacks, where trigger is strongly associated to\nspecific malicious behavior. Various backdoor detection methods depend on this\ninherent property and shows effective results in identifying and mitigating\nsuch targeted attacks. However, a purely untargeted attack in backdoor\nscenarios is, in some sense, self-weakening, since the target nature is what\nmakes backdoor attacks so powerful. In light of this, we introduce a novel\nConstrained Untargeted Backdoor Attack (CUBA), which combines the flexibility\nof untargeted attacks with the intentionality of targeted attacks. The\ncompromised model, when presented with backdoor images, will classify them into\nrandom classes within a constrained range of target classes selected by the\nattacker. This combination of randomness and determinedness enables the\nproposed untargeted backdoor attack to natively circumvent existing backdoor\ndefense methods. To implement the untargeted backdoor attack under controlled\nflexibility, we propose to apply logit normalization on cross-entropy loss with\nflipped one-hot labels. By constraining the logit during training, the\ncompromised model will show a uniform distribution across selected target\nclasses, resulting in controlled untargeted attack. Extensive experiments\ndemonstrate the effectiveness of the proposed CUBA on different datasets."}
{"id": "2506.17353", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17353", "abs": "https://arxiv.org/abs/2506.17353", "authors": ["Zongjie Li", "Daoyuan Wu", "Shuai Wang", "Zhendong Su"], "title": "Differentiation-Based Extraction of Proprietary Data from Fine-Tuned LLMs", "comment": "In Proceedings of the 2025 ACM SIGSAC Conference on Computer and\n  Communications Security (CCS'25), October 13-17, 2025, Taipei, Taiwan, China.\n  ACM, New York, NY, USA, 15 pages. https://doi.org/10.1145/3719027.3744856", "summary": "The increasing demand for domain-specific and human-aligned Large Language\nModels (LLMs) has led to the widespread adoption of Supervised Fine-Tuning\n(SFT) techniques. SFT datasets often comprise valuable instruction-response\npairs, making them highly valuable targets for potential extraction. This paper\nstudies this critical research problem for the first time. We start by formally\ndefining and formulating the problem, then explore various attack goals, types,\nand variants based on the unique properties of SFT data in real-world\nscenarios. Based on our analysis of extraction behaviors of direct extraction,\nwe develop a novel extraction method specifically designed for SFT models,\ncalled Differentiated Data Extraction (DDE), which exploits the confidence\nlevels of fine-tuned models and their behavioral differences from pre-trained\nbase models. Through extensive experiments across multiple domains and\nscenarios, we demonstrate the feasibility of SFT data extraction using DDE. Our\nresults show that DDE consistently outperforms existing extraction baselines in\nall attack settings. To counter this new attack, we propose a defense mechanism\nthat mitigates DDE attacks with minimal impact on model performance. Overall,\nour research reveals hidden data leak risks in fine-tuned LLMs and provides\ninsights for developing more secure models."}
{"id": "2506.17369", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17369", "abs": "https://arxiv.org/abs/2506.17369", "authors": ["Zhiyuan Pan", "Xing Hu", "Xin Xia", "Xiaohu Yang"], "title": "Re-Evaluating Code LLM Benchmarks Under Semantic Mutation", "comment": null, "summary": "In the era of large language models (LLMs), code benchmarks have become an\nimportant research area in software engineering and are widely used by\npractitioners. These benchmarks evaluate the performance of LLMs on specific\ncode-related tasks, such as code understanding and generation. A critical step\nin constructing code benchmarks is the design of prompts. However, as existing\ncode benchmarks typically rely on a single prompt template per task, they are\nprone to the issue of prompt sensitivity, where minor prompt variations could\nresult in substantial performance variations, leading to unreliable evaluations\nof model capabilities.\n  While previous studies have explored prompt sensitivity, their experimental\ndesigns and findings are limited to traditional natural language processing\n(NLP) tasks. In this paper, we present an empirical study to investigate prompt\nsensitivity in code benchmarks. We first propose a general framework that\nmodifies prompt templates in a manner that preserves both their semantics and\ntheir structure as much as possible. Based on the framework, we conduct\nextensive experiments across eight code benchmark tasks on 10 representative\nopen-source LLMs, with each task featuring 100 semantically similar prompt\ntemplates. We then analyze the evaluation results using various statistical\nmetrics, focusing on both absolute and relative model performance. Our findings\nsuggest that even slight prompt variations can lead to significant shifts in\nperformance. Additionally, we observe that such variations can introduce\ninconsistencies in the performance rankings across different models. These\ninsights highlight the need for considering prompt sensitivity when designing\nfuture code benchmarks, to ensure more reliable and accurate evaluation of LLM\ncapabilities."}
{"id": "2506.17937", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17937", "abs": "https://arxiv.org/abs/2506.17937", "authors": ["Tommi Mikkonen", "Antero Taivalsaari"], "title": "Software Reuse in the Generative AI Era: From Cargo Cult Towards AI Native Software Engineering", "comment": null, "summary": "Software development is currently under a paradigm shift in which artificial\nintelligence and generative software reuse are taking the center stage in\nsoftware creation. Consequently, earlier software reuse practices and methods\nare rapidly being replaced by AI-assisted approaches in which developers place\ntheir trust on code that has been generated by artificial intelligence. This is\nleading to a new form of software reuse that is conceptually not all that\ndifferent from cargo cult development. In this paper we discuss the\nimplications of AI-assisted generative software reuse in the context of\nemerging \"AI native\" software engineering, bring forth relevant questions, and\ndefine a tentative research agenda and call to action for tackling some of the\ncentral issues associated with this approach."}
{"id": "2506.18053", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.18053", "abs": "https://arxiv.org/abs/2506.18053", "authors": ["Marcos Florencio", "Thomas Barton"], "title": "Mechanistic Interpretability in the Presence of Architectural Obfuscation", "comment": null, "summary": "Architectural obfuscation - e.g., permuting hidden-state tensors, linearly\ntransforming embedding tables, or remapping tokens - has recently gained\ntraction as a lightweight substitute for heavyweight cryptography in\nprivacy-preserving large-language-model (LLM) inference. While recent work has\nshown that these techniques can be broken under dedicated reconstruction\nattacks, their impact on mechanistic interpretability has not been\nsystematically studied. In particular, it remains unclear whether scrambling a\nnetwork's internal representations truly thwarts efforts to understand how the\nmodel works, or simply relocates the same circuits to an unfamiliar coordinate\nsystem. We address this gap by analyzing a GPT-2-small model trained from\nscratch with a representative obfuscation map. Assuming the obfuscation map is\nprivate and the original basis is hidden (mirroring an honest-but-curious\nserver), we apply logit-lens attribution, causal path-patching, and\nattention-head ablation to locate and manipulate known circuits. Our findings\nreveal that obfuscation dramatically alters activation patterns within\nattention heads yet preserves the layer-wise computational graph. This\ndisconnect hampers reverse-engineering of user prompts: causal traces lose\ntheir alignment with baseline semantics, and token-level logit attributions\nbecome too noisy to reconstruct. At the same time, feed-forward and residual\npathways remain functionally intact, suggesting that obfuscation degrades\nfine-grained interpretability without compromising top-level task performance.\nThese results establish quantitative evidence that architectural obfuscation\ncan simultaneously (i) retain global model behaviour and (ii) impede\nmechanistic analyses of user-specific content. By mapping where\ninterpretability breaks down, our study provides guidance for future privacy\ndefences and for robustness-aware interpretability tooling."}
{"id": "2506.18087", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.18087", "abs": "https://arxiv.org/abs/2506.18087", "authors": ["Huaiying Luo", "Cheng Ji"], "title": "Federated Learning-Based Data Collaboration Method for Enhancing Edge Cloud AI System Security Using Large Language Models", "comment": "Accepted by the 2025 5th International Symposium on Computer\n  Technology and Information Science (ISCTIS 2025)", "summary": "With the widespread application of edge computing and cloud systems in\nAI-driven applications, how to maintain efficient performance while ensuring\ndata privacy has become an urgent security issue. This paper proposes a\nfederated learning-based data collaboration method to improve the security of\nedge cloud AI systems, and use large-scale language models (LLMs) to enhance\ndata privacy protection and system robustness. Based on the existing federated\nlearning framework, this method introduces a secure multi-party computation\nprotocol, which optimizes the data aggregation and encryption process between\ndistributed nodes by using LLM to ensure data privacy and improve system\nefficiency. By combining advanced adversarial training techniques, the model\nenhances the resistance of edge cloud AI systems to security threats such as\ndata leakage and model poisoning. Experimental results show that the proposed\nmethod is 15% better than the traditional federated learning method in terms of\ndata protection and model robustness."}
{"id": "2506.18191", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.18191", "abs": "https://arxiv.org/abs/2506.18191", "authors": ["Masudul Hasan Masud Bhuiyan", "Gianluca De Stefano", "Giancarlo Pellegrino", "Cristian-Alexandru Staicu"], "title": "Call Me Maybe: Enhancing JavaScript Call Graph Construction using Graph Neural Networks", "comment": null, "summary": "Static analysis plays a key role in finding bugs, including security issues.\nA critical step in static analysis is building accurate call graphs that model\nfunction calls in a program. However, due to hard-to-analyze language features,\nexisting call graph construction algorithms for JavaScript are neither sound\nnor complete. Prior work shows that even advanced solutions produce false edges\nand miss valid ones. In this work, we assist these tools by identifying missed\ncall edges. Our main idea is to frame the problem as link prediction on full\nprogram graphs, using a rich representation with multiple edge types. Our\napproach, GRAPHIA, leverages recent advances in graph neural networks to model\nnon-local relationships between code elements. Concretely, we propose\nrepresenting JavaScript programs using a combination of syntactic- and\nsemantic-based edges. GRAPHIA can learn from imperfect labels, including static\ncall edges from existing tools and dynamic edges from tests, either from the\nsame or different projects. Because call graphs are sparse, standard machine\nlearning metrics like ROC are not suitable. Instead, we evaluate GRAPHIA by\nranking function definitions for each unresolved call site. We conduct a\nlarge-scale evaluation on 50 popular JavaScript libraries with 163K call edges\n(150K static and 13K dynamic). GRAPHIA builds program graphs with 6.6M\nstructural and 386K semantic edges. It ranks the correct target as the top\ncandidate in over 42% of unresolved cases and within the top 5 in 72% of cases,\nreducing the manual effort needed for analysis. Our results show that\nlearning-based methods can improve the recall of JavaScript call graph\nconstruction. To our knowledge, this is the first work to apply GNN-based link\nprediction to full multi-file program graphs for interprocedural analysis."}
{"id": "2506.18245", "categories": ["cs.CR", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2506.18245", "abs": "https://arxiv.org/abs/2506.18245", "authors": ["Lei Yu", "Zhirong Huang", "Hang Yuan", "Shiqi Cheng", "Li Yang", "Fengjun Zhang", "Chenjie Shen", "Jiajia Ma", "Jingyuan Zhang", "Junyi Lu", "Chun Zuo"], "title": "Smart-LLaMA-DPO: Reinforced Large Language Model for Explainable Smart Contract Vulnerability Detection", "comment": "Accepted to ISSTA 2025", "summary": "Smart contract vulnerability detection remains a major challenge in\nblockchain security. Existing vulnerability detection methods face two main\nissues: (1) Existing datasets lack comprehensive coverage and high-quality\nexplanations for preference learning. (2) Large language models (LLMs) often\nstruggle with accurately interpreting specific concepts in smart contract\nsecurity. Empirical analysis shows that even after continual pre-training (CPT)\nand supervised fine-tuning (SFT), LLMs may misinterpret the execution order of\nstate changes, resulting in incorrect explanations despite making correct\ndetection decisions. To address these challenges, we propose Smart-LLaMA-DPO\nbased on LLaMA-3.1-8B. We construct a comprehensive dataset covering four major\nvulnerability types and machine-unauditable vulnerabilities, including precise\nlabels, explanations, and locations for SFT, as well as high-quality and\nlow-quality output pairs for Direct Preference Optimization (DPO). Second, we\nperform CPT using large-scale smart contract to enhance the LLM's understanding\nof specific security practices in smart contracts. Futhermore, we conduct SFT\nwith our comprehensive dataset. Finally, we apply DPO, leveraging human\nfeedback and a specially designed loss function that increases the probability\nof preferred explanations while reducing the likelihood of non-preferred\noutputs. We evaluate Smart-LLaMA-DPO on four major vulnerability types:\nreentrancy, timestamp dependence, integer overflow/underflow, and delegatecall,\nas well as machine-unauditable vulnerabilities. Our method significantly\noutperforms state-of-the-art baselines, with average improvements of 10.43% in\nF1 score and 7.87% in accuracy. Moreover, both LLM evaluation and human\nevaluation confirm that our method generates more correct, thorough, and clear\nexplanations."}
{"id": "2506.18289", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.18289", "abs": "https://arxiv.org/abs/2506.18289", "authors": ["Saurabhsingh Rajput", "Mootez Saad", "Tushar Sharma"], "title": "Tu(r)ning AI Green: Exploring Energy Efficiency Cascading with Orthogonal Optimizations", "comment": "In review", "summary": "AI's exponential growth intensifies computational demands and energy\nchallenges. While practitioners employ various optimization techniques, that we\nrefer as \"knobs\" in this paper, to tune model efficiency, these are typically\nafterthoughts and reactive ad-hoc changes applied in isolation without\nunderstanding their combinatorial effects on energy efficiency. This paper\nemphasizes on treating energy efficiency as the first-class citizen and as a\nfundamental design consideration for a compute-intensive pipeline. We show that\nstrategic selection across five AI pipeline phases (data, model, training,\nsystem, inference) creates cascading efficiency. Experimental validation shows\northogonal combinations reduce energy consumption by up to $94.6$% while\npreserving $95.95$% of the original F1 score of non-optimized pipelines. This\ncurated approach provides actionable frameworks for informed sustainable AI\nthat balance efficiency, performance, and environmental responsibility."}
{"id": "2506.18315", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.18315", "abs": "https://arxiv.org/abs/2506.18315", "authors": ["Lehan He", "Zeren Chen", "Zhe Zhang", "Jing Shao", "Xiang Gao", "Lu Sheng"], "title": "Use Property-Based Testing to Bridge LLM Code Generation and Validation", "comment": null, "summary": "Large Language Models (LLMs) excel at code generation, but ensuring their\noutputs to be functionally correct, especially in complex programming tasks, is\na persistent challenge. While traditional Test-Driven Development (TDD) offers\na path for code refinement, its efficacy with LLMs is often undermined by the\nscarcity of high-quality test cases or the pitfalls of automated test\ngeneration, including biased tests or inaccurate output predictions that can\nmisdirect the correction process. This paper introduces Property-Generated\nSolver, a novel framework that leverages Property-Based Testing (PBT) to\nvalidate high-level program properties or invariants, instead of relying on\nspecific input-output examples. These properties are often simpler to define\nand verify than directly predicting exhaustive test oracles, breaking the\n\"cycle of self-deception\" where tests might share flaws with the code they are\nmeant to validate. Property-Generated Solver employs two collaborative\nLLM-based agents: a Generator dedicated to code generation and iterative\nrefinement, and a Tester that manages the PBT life-cycle and formulate\nsemantically rich feedback from property violations. The resulting\ncomprehensive and actionable feedback then guides the Generator in its\nrefinement efforts. By establishing PBT as the core validation engine within\nthis iterative, closed-loop paradigm, Property-Generated Solver provides a\nrobust mechanism for steering LLMs towards more correct and generalizable code.\nExtensive experimental results on multiple code generation benchmarks\ndemonstrate that Property-Generated Solver achieves substantial pass@1\nimprovements, ranging from 23.1% to 37.3% relative gains over established TDD\nmethods."}
{"id": "2506.18403", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.18403", "abs": "https://arxiv.org/abs/2506.18403", "authors": ["Muntasir Adnan", "Carlos C. N. Kuhn"], "title": "The Debugging Decay Index: Rethinking Debugging Strategies for Code LLMs", "comment": null, "summary": "The effectiveness of AI debugging follows a predictable exponential decay\npattern; most models lose 60-80% of their debugging capability within just 2-3\nattempts, despite iterative debugging being a critical capability for practical\ncode generation systems. We introduce the Debugging Decay Index (DDI), a\nmathematical framework that quantifies when debugging becomes ineffective and\npredicts intervention points. Our strategic fresh start approach shifts from\nexploitation to exploration at strategic points in the debugging process,\ndemonstrating that well-timed interventions can rescue the effectiveness of\ndebugging. DDI reveals a fundamental limitation in current AI debugging and\nprovides the first quantitative framework for optimising iterative code\ngeneration strategies."}
{"id": "2506.18543", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.18543", "abs": "https://arxiv.org/abs/2506.18543", "authors": ["Xiaodong Wu", "Xiangman Li", "Jianbing Ni"], "title": "Security Assessment of DeepSeek and GPT Series Models against Jailbreak Attacks", "comment": null, "summary": "The widespread deployment of large language models (LLMs) has raised critical\nconcerns over their vulnerability to jailbreak attacks, i.e., adversarial\nprompts that bypass alignment mechanisms and elicit harmful or policy-violating\noutputs. While proprietary models like GPT-4 have undergone extensive\nevaluation, the robustness of emerging open-source alternatives such as\nDeepSeek remains largely underexplored, despite their growing adoption in\nreal-world applications. In this paper, we present the first systematic\njailbreak evaluation of DeepSeek-series models, comparing them with GPT-3.5 and\nGPT-4 using the HarmBench benchmark. We evaluate seven representative attack\nstrategies across 510 harmful behaviors categorized by both function and\nsemantic domain. Our analysis reveals that DeepSeek's Mixture-of-Experts (MoE)\narchitecture introduces routing sparsity that offers selective robustness\nagainst optimization-based attacks such as TAP-T, but leads to significantly\nhigher vulnerability under prompt-based and manually engineered attacks. In\ncontrast, GPT-4 Turbo demonstrates stronger and more consistent safety\nalignment across diverse behaviors, likely due to its dense Transformer design\nand reinforcement learning from human feedback. Fine-grained behavioral\nanalysis and case studies further show that DeepSeek often routes adversarial\nprompts to under-aligned expert modules, resulting in inconsistent refusal\nbehaviors. These findings highlight a fundamental trade-off between\narchitectural efficiency and alignment generalization, emphasizing the need for\ntargeted safety tuning and modular alignment strategies to ensure secure\ndeployment of open-source LLMs."}
{"id": "2506.18824", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.18824", "abs": "https://arxiv.org/abs/2506.18824", "authors": ["Islem Bouzenia", "Michael Pradel"], "title": "Understanding Software Engineering Agents: A Study of Thought-Action-Result Trajectories", "comment": null, "summary": "Large Language Model (LLM)-based agents are increasingly employed to automate\ncomplex software engineering tasks such as program repair and issue resolution.\nThese agents operate by autonomously generating natural language thoughts,\ninvoking external tools, and iteratively refining their solutions. Despite\ntheir widespread adoption, the internal decision-making processes of these\nagents remain largely unexplored, limiting our understanding of their\noperational dynamics and failure modes. In this paper, we present a large-scale\nempirical study of the thought-action-result trajectories of three\nstate-of-the-art LLM-based agents: \\textsc{RepairAgent},\n\\textsc{AutoCodeRover}, and \\textsc{OpenHands}. We unify their interaction logs\ninto a common format, capturing 120 trajectories and 2822 LLM interactions\nfocused on program repair and issue resolution. Our study combines quantitative\nanalyses of structural properties, action patterns, and token usage with\nqualitative assessments of reasoning coherence and feedback integration. We\nidentify key trajectory characteristics such as iteration counts and token\nconsumption, recurring action sequences, and the semantic coherence linking\nthoughts, actions, and their results. Our findings reveal behavioral motifs and\nanti-patterns that distinguish successful from failed executions, providing\nactionable insights for improving agent design, including prompting strategies,\nfailure diagnosis, and anti-pattern detection. We release our dataset and\nannotation framework to support further research on transparent and robust\nautonomous software engineering agents."}
