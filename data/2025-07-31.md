<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 21]
- [cs.CR](#cs.CR) [Total: 16]
- [cs.AI](#cs.AI) [Total: 17]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [RedCoder: Automated Multi-Turn Red Teaming for Code LLMs](https://arxiv.org/abs/2507.22063)
*Wenjie Jacky Mo,Qin Liu,Xiaofei Wen,Dongwon Jung,Hadi Askari,Wenxuan Zhou,Zhe Zhao,Muhao Chen*

Main category: cs.SE

TL;DR: RedCoder是一种多轮对话红队代理，用于诱导代码生成模型生成漏洞代码，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有红队方法依赖人工且忽略多轮交互，难以扩展和实用。

Method: 通过多代理游戏模拟对抗交互，生成原型对话和攻击策略库，微调LLM作为RedCoder核心。

Result: 实验表明RedCoder在多轮对话中更有效诱导漏洞。

Conclusion: RedCoder为评估代码生成系统安全性提供了可扩展工具。

Abstract: Large Language Models (LLMs) for code generation (i.e., Code LLMs) have
demonstrated impressive capabilities in AI-assisted software development and
testing. However, recent studies have shown that these models are prone to
generating vulnerable or even malicious code under adversarial settings.
Existing red-teaming approaches rely on extensive human effort, limiting their
scalability and practicality, and generally overlook the interactive nature of
real-world AI-assisted programming, which often unfolds over multiple turns. To
bridge these gaps, we present RedCoder, a red-teaming agent that engages victim
models in multi-turn conversation to elicit vulnerable code. The pipeline to
construct RedCoder begins with a multi-agent gaming process that simulates
adversarial interactions, yielding a set of prototype conversations and an
arsenal of reusable attack strategies. We then fine-tune an LLM on these
prototype conversations to serve as the backbone of RedCoder. Once deployed,
RedCoder autonomously engages Code LLMs in multi-turn conversations,
dynamically retrieving relevant strategies from the arsenal to steer the
dialogue toward vulnerability-inducing outputs. Experiments across multiple
Code LLMs show that our approach outperforms prior single-turn and multi-turn
red-team methods in inducing vulnerabilities in code generation, offering a
scalable and effective tool for evaluating the security boundaries of modern
code-generation systems.

</details>


### [2] [Machine Learning Experiences: A story of learning AI for use in enterprise software testing that can be used by anyone](https://arxiv.org/abs/2507.22064)
*Michael Cohoon,Debbie Furman*

Main category: cs.SE

TL;DR: 本文描述了一组专注于软件测试的人如何通过类似CRISP-DM的机器学习工作流程，从数据收集到模型评估，成功应用机器学习技术。


<details>
  <summary>Details</summary>
Motivation: 分享如何通过标准化的机器学习工作流程，使任何人都能有效地将机器学习技术应用于项目中。

Method: 采用类似CRISP-DM的工作流程，包括数据收集、清洗、特征工程、数据分割、模型选择、训练、测试和性能评估。

Result: 通过遵循该工作流程，团队成功将机器学习技术应用于软件测试项目。

Conclusion: 标准化的机器学习工作流程是成功应用机器学习技术的关键。

Abstract: This paper details the machine learning (ML) journey of a group of people
focused on software testing. It tells the story of how this group progressed
through a ML workflow (similar to the CRISP-DM process). This workflow consists
of the following steps and can be used by anyone applying ML techniques to a
project: gather the data; clean the data; perform feature engineering on the
data; splitting the data into two sets, one for training and one for testing;
choosing a machine learning model; training the model; testing the model and
evaluating the model performance. By following this workflow, anyone can
effectively apply ML to any project that they are doing.

</details>


### [3] [Fuzzing: Randomness? Reasoning! Efficient Directed Fuzzing via Large Language Models](https://arxiv.org/abs/2507.22065)
*Xiaotao Feng,Xiaogang Zhu,Kun Hu,Jincheng Wang,Yingjie Cao,Guang Gong,Jianfeng Pan*

Main category: cs.SE

TL;DR: RandLuzz利用大语言模型（LLM）减少模糊测试中的随机性，通过生成可达种子和构建特定于错误的变异器，显著提高了错误检测效率。


<details>
  <summary>Details</summary>
Motivation: 模糊测试中的随机性虽然有助于发现错误，但降低了效率。即使定向模糊测试减少了随机性，种子和变异器的随机性仍影响性能。

Method: RandLuzz结合LLM和定向模糊测试，分析函数调用链或功能生成可达种子，并通过LLM分析错误原因和变异建议构建特定于错误的变异器。

Result: 与AFLGo等定向模糊测试工具相比，RandLuzz平均提速2.1×至4.8×，并在60秒内暴露8个错误。

Conclusion: RandLuzz通过LLM减少随机性，显著提升了模糊测试的效率和错误检测能力。

Abstract: Fuzzing is highly effective in detecting bugs due to the key contribution of
randomness. However, randomness significantly reduces the efficiency of
fuzzing, causing it to cost days or weeks to expose bugs. Even though directed
fuzzing reduces randomness by guiding fuzzing towards target buggy locations,
the dilemma of randomness still challenges directed fuzzers. Two critical
components, which are seeds and mutators, contain randomness and are closely
tied to the conditions required for triggering bugs. Therefore, to address the
challenge of randomness, we propose to use large language models (LLMs) to
remove the randomness in seeds and reduce the randomness in mutators. With
their strong reasoning and code generation capabilities, LLMs can be used to
generate reachable seeds that target pre-determined locations and to construct
bug-specific mutators tailored for specific bugs. We propose RandLuzz, which
integrates LLMs and directed fuzzing, to improve the quality of seeds and
mutators, resulting in efficient bug exposure. RandLuzz analyzes function call
chain or functionality to guide LLMs in generating reachable seeds. To
construct bug-specific mutators, RandLuzz uses LLMs to perform bug analysis,
obtaining information such as bug causes and mutation suggestions, which
further help generate code that performs bug-specific mutations. We evaluate
RandLuzz by comparing it with four state-of-the-art directed fuzzers, AFLGo,
Beacon, WindRanger, and SelectFuzz. With RandLuzz-generated seeds, the fuzzers
achieve an average speedup ranging from 2.1$\times$ to 4.8$\times$ compared to
using widely-used initial seeds. Additionally, when evaluated on individual
bugs, RandLuzz achieves up to a 2.7$\times$ speedup compared to the
second-fastest exposure. On 8 bugs, RandLuzz can even expose them within 60
seconds.

</details>


### [4] [CodableLLM: Automating Decompiled and Source Code Mapping for LLM Dataset Generation](https://arxiv.org/abs/2507.22066)
*Dylan Manuel,Paul Rad*

Main category: cs.SE

TL;DR: CodableLLM是一个Python框架，用于自动化生成和整理数据集，通过将反编译的函数与源代码对齐，支持多语言并提升代码理解与生成能力。


<details>
  <summary>Details</summary>
Motivation: 解决高质量代码数据集生成的挑战，特别是反编译二进制文件与源代码对齐的问题。

Method: 设计并实现CodableLLM框架，集成反编译器和解析器，自动化数据集生成。

Result: CodableLLM在数据集生成中表现高效，优于现有工具。

Conclusion: CodableLLM为代码相关大语言模型提供了高效的数据集生成解决方案。

Abstract: The generation of large, high-quality datasets for code understanding and
generation remains a significant challenge, particularly when aligning
decompiled binaries with their original source code. To address this, we
present CodableLLM, a Python framework designed to automate the creation and
curation of datasets by mapping decompiled functions to their corresponding
source functions. This process enhances the alignment between decompiled and
source code representations, facilitating the development of large language
models (LLMs) capable of understanding and generating code across multiple
abstraction levels. CodableLLM supports multiple programming languages and
integrates with existing decompilers and parsers to streamline dataset
generation. This paper presents the design and implementation of CodableLLM,
evaluates its performance in dataset creation, and compares it to existing
tools in the field. The results demonstrate that CodableLLM offers a robust and
efficient solution for generating datasets tailored for code-focused LLMS.

</details>


### [5] [Automated Test Data Generation for Enterprise Protobuf Systems: A Metaclass-Enhanced Statistical Approach](https://arxiv.org/abs/2507.22070)
*Y. Du*

Main category: cs.SE

TL;DR: 本文提出了一种基于Python元类系统和生产日志统计分析的测试数据生成框架，显著提升了企业级Protocol Buffers的性能测试效率。


<details>
  <summary>Details</summary>
Motivation: 大规模企业系统使用Protocol Buffers时，复杂的嵌套数据结构导致传统测试数据生成方法难以应对性能测试需求。

Method: 结合自动模式内省、统计值分布分析和递归下降算法，动态增强类型并提取真实值域。

Result: 实验显示，测试数据准备时间减少95%，测试覆盖率提升80%，可处理15层嵌套结构并快速生成10万+测试用例。

Conclusion: 该框架有效解决了企业级protobuf测试数据生成的挑战，显著提升测试效率。

Abstract: Large-scale enterprise systems utilizing Protocol Buffers (protobuf) present
significant challenges for performance testing, particularly when targeting
intermediate business interfaces with complex nested data structures.
Traditional test data generation approaches are inadequate for handling the
intricate hierarchical and graph-like structures inherent in enterprise
protobuf schemas. This paper presents a novel test data generation framework
that leverages Python's metaclass system for dynamic type enhancement and
statistical analysis of production logs for realistic value domain extraction.
Our approach combines automatic schema introspection, statistical value
distribution analysis, and recursive descent algorithms for handling deeply
nested structures. Experimental evaluation on three real-world enterprise
systems demonstrates up to 95\% reduction in test data preparation time and
80\% improvement in test coverage compared to existing approaches. The
framework successfully handles protobuf structures with up to 15 levels of
nesting and generates comprehensive test suites containing over 100,000 test
cases within seconds.

</details>


### [6] [Analyzing and Evaluating the Behavior of Git Diff and Merge](https://arxiv.org/abs/2507.22071)
*Niels Glodny*

Main category: cs.SE

TL;DR: 该论文探讨了Git中diff和merge算法的功能及其潜在问题，揭示了Git的一些意外行为，如单行更改导致整个文件标记为更改、合并策略可能导致指数级时间消耗等。


<details>
  <summary>Details</summary>
Motivation: Git的协作算法虽被广泛使用，但其工作原理和潜在问题尚未被充分理解，特别是在diff和merge算法方面。

Method: 通过文档化和分析Git的主要功能，包括diff计算、merge运行方式及其在复杂操作中的应用。

Result: 揭示了Git的多个意外行为，如histogram diff算法的极端情况、默认合并策略的时间复杂度问题、merge和rebase的非交换性等。

Conclusion: Git的diff和merge算法存在未预期的行为，这些发现对其他应用场景中的算法设计具有启示意义。

Abstract: Despite being widely used, the algorithms that enable collaboration with Git
are not well understood. The diff and merge algorithms are particularly
interesting, as they could be applied in other contexts. In this thesis, I
document the main functionalities of Git: how diffs are computed, how they are
used to run merges, and how merges enable more complex operations. In the
process, I show multiple unexpected behaviors in Git, including the following:
The histogram diff algorithm has pathological cases where a single-line change
can cause the entire rest of the file to be marked as changed. The default
merge strategy (ort) can result in merges requiring exponential time in the
number of commits in the history. Merges and rebases are not commutative, and
even when merges do not result in a conflict, the result is not specified but
depends on the diff algorithm used. And finally, sometimes when two sides of a
merge add different lines at the same position, the result is not a conflict,
but a merge containing both changes after each other, in arbitrary order.

</details>


### [7] [CodeEvo: Interaction-Driven Synthesis of Code-centric Data through Hybrid and Iterative Feedback](https://arxiv.org/abs/2507.22080)
*Qiushi Sun,Jinyang Gong,Lei Li,Qipeng Guo,Fei Yuan*

Main category: cs.SE

TL;DR: CodeEvo框架通过两个LLM代理（Coder和Reviewer）的迭代交互合成高质量代码数据，结合编译确定性和生成灵活性，显著提升代码生成性能。


<details>
  <summary>Details</summary>
Motivation: 手动获取高质量的指令-代码对成本高且规模有限，现有方法缺乏严格的数据验证，导致合成数据质量不高。

Method: 提出CodeEvo框架，通过Coder生成候选代码和测试用例，Reviewer提供新指令和反馈，结合混合反馈机制实现自动质量控制。

Result: 实验表明，基于CodeEvo数据微调的模型在多种代码生成基准测试中显著优于现有基线。

Conclusion: CodeEvo为代码中心数据合成提供了有效方法，并通过多角度分析揭示了其优势。

Abstract: Acquiring high-quality instruction-code pairs is essential for training Large
Language Models (LLMs) for code generation. Manually curated data is expensive
and inherently limited in scale, motivating the development of code-centric
synthesis methods. Yet, current approaches either focus on augmenting existing
code or rely on predefined heuristics, both lacking rigorous data validation,
which results in synthetic data that is ungrounded, repetitive, or overly
simplistic. Inspired by collaborative programming practices, we propose
CodeEvo, a framework that synthesizes code data through iterative interactions
between two LLM agents: a Coder, which generates candidate code and test cases
based on given instructions, and a Reviewer, which guides the synthesis process
by producing new instructions and feedback. We further introduce a hybrid
feedback mechanism that combines compiler determinism with the generative
flexibility of agents, enabling automatic quality control throughout synthesis.
Extensive experiments demonstrate that models fine-tuned on CodeEvo data
significantly outperform established baselines across code generation
benchmarks with various difficulties. In-depth analyses further provide
insights from multiple perspectives into effective code-centric data synthesis.

</details>


### [8] [BOOP: Write Right Code](https://arxiv.org/abs/2507.22085)
*Vaani Goenka,Aalok D. Thakkar*

Main category: cs.SE

TL;DR: BOOP框架通过四个强制阶段（规范、算法开发、实现和证明）帮助学生从“让代码运行”转向理解代码的正确性，提升了算法推理能力。


<details>
  <summary>Details</summary>
Motivation: 新手程序员常依赖语法和测试用例驱动的方法，而AI工具可能提供语法正确但概念错误的解决方案。BOOP旨在通过结构化方法解决这一问题。

Method: BOOP框架包括四个阶段：形式化规范、语言无关算法开发、实现和正确性证明，并通过VS Code扩展强制执行。

Result: 初步评估显示学生算法推理能力提升，调试减少，但对框架的冗长性有所抱怨。教师观察到学生基础技能更强。

Conclusion: BOOP通过结构化方法有效提升了学生的编程理解和正确性验证能力，尽管初期可能显得繁琐。

Abstract: Novice programmers frequently adopt a syntax-specific and test-case-driven
approach, writing code first and adjusting until programs compile and test
cases pass, rather than developing correct solutions through systematic
reasoning. AI coding tools exacerbate this challenge by providing syntactically
correct but conceptually flawed solutions. In this paper, we introduce BOOP
(Blueprint, Operations, OCaml, Proof), a structured framework requiring four
mandatory phases: formal specification, language-agnostic algorithm
development, implementation, and correctness proof. This shifts focus from
``making code work'' to understanding why code is correct.
  BOOP was implemented at our institution using a VS Code extension and
preprocessor that enforces constraints and identifies counterproductive
patterns. Initial evaluation shows improved algorithmic reasoning and reduced
trial-and-error debugging. Students reported better edge case understanding and
problem decomposition, though some initially found the format verbose.
Instructors observed stronger foundational skills compared to traditional
approaches.

</details>


### [9] [TypyBench: Evaluating LLM Type Inference for Untyped Python Repositories](https://arxiv.org/abs/2507.22086)
*Honghua Dong,Jiacheng Yang,Xun Deng,Yuhe Jiang,Gennady Pekhimenko,Fan Long,Xujie Si*

Main category: cs.SE

TL;DR: TypyBench是一个评估大型语言模型（LLM）在Python代码库中类型推断能力的基准，提出了TypeSim和TypeCheck两个新指标，发现LLM在复杂嵌套类型和类型一致性上表现不佳。


<details>
  <summary>Details</summary>
Motivation: 动态语言（如Python）的类型推断是一个长期挑战，而LLM在此领域的能力尚未充分探索。

Method: 引入TypyBench基准，包含TypeSim和TypeCheck两个新指标，评估LLM在50个高质量Python代码库中的表现。

Result: LLM在TypeSim上表现尚可，但在复杂嵌套类型和类型一致性上存在显著问题。

Conclusion: 未来研究应关注代码库级别的类型一致性，而非仅改进类型相似性。TypyBench为此提供了基础。

Abstract: Type inference for dynamic languages like Python is a persistent challenge in
software engineering. While large language models (LLMs) have shown promise in
code understanding, their type inference capabilities remain underexplored. We
introduce TypyBench, a benchmark designed to evaluate LLMs' type inference
across entire Python repositories. TypyBench features two novel metrics:
TypeSim, which captures nuanced semantic relationships between predicted and
ground truth types, and TypeCheck, which assesses type consistency across
codebases. Our evaluation of various LLMs on a curated dataset of 50
high-quality Python repositories reveals that, although LLMs achieve decent
TypeSim scores, they struggle with complex nested types and exhibit significant
type consistency errors. These findings suggest that future research should
shift focus from improving type similarity to addressing repository-level
consistency. TypyBench provides a foundation for this new direction, offering
insights into model performance across different type complexities and usage
contexts. Our code and data are available at
https://github.com/typybench/typybench.

</details>


### [10] [Secure coding for web applications: Frameworks, challenges, and the role of LLMs](https://arxiv.org/abs/2507.22223)
*Kiana Kiashemshaki,Mohammad Jalili Torkamani,Negin Mahmoudi*

Main category: cs.SE

TL;DR: 本文综述了安全编码实践，比较了主要框架，探讨了LLM在安全代码评估中的作用，并提供了实用建议。


<details>
  <summary>Details</summary>
Motivation: 尽管安全编码至关重要，但其实际应用仍因组织、教育和技术障碍而不一致。

Method: 通过框架比较、OWASP Top 10威胁分类及LLM案例研究，分析安全编码实践。

Result: 提出了可重复的案例研究，展示了LLM在安全代码评估中的潜力。

Conclusion: 为研究人员、开发者和教育者提供了将安全编码整合到实际开发中的实用见解。

Abstract: Secure coding is a critical yet often overlooked practice in software
development. Despite extensive awareness efforts, real-world adoption remains
inconsistent due to organizational, educational, and technical barriers. This
paper provides a comprehensive review of secure coding practices across major
frameworks and domains, including web development, DevSecOps, and cloud
security. It introduces a structured framework comparison and categorizes
threats aligned with the OWASP Top 10. Additionally, we explore the rising role
of Large Language Models (LLMs) in evaluating and recommending secure code,
presenting a reproducible case study across four major vulnerability types.
This paper offers practical insights for researchers, developers, and educators
on integrating secure coding into real-world development processes.

</details>


### [11] [From Articles to Code: On-Demand Generation of Core Algorithms from Scientific Publications](https://arxiv.org/abs/2507.22324)
*Cameron S. Movassaghi,Amanda Momenzadeh,Jesse G. Meyer*

Main category: cs.SE

TL;DR: 论文提出利用科学论文中的方法描述作为LLM的规范，实现按需代码生成，替代人工维护的软件库。


<details>
  <summary>Details</summary>
Motivation: 减少软件包的维护成本，如依赖管理、错误修复和版本控制。

Method: 通过让先进LLM（如GPT-4、Gemini Pro 2.5等）根据论文方法描述实现核心算法，并与传统库对比。

Result: 当前LLM能可靠生成功能与常规库无异的代码。

Conclusion: 未来可能转向灵活的按需代码生成，减少对静态人工维护包的依赖。

Abstract: Maintaining software packages imposes significant costs due to dependency
management, bug fixes, and versioning. We show that rich method descriptions in
scientific publications can serve as standalone specifications for modern large
language models (LLMs), enabling on-demand code generation that could supplant
human-maintained libraries. We benchmark state-of-the-art models
(GPT-o4-mini-high, Gemini Pro 2.5, Claude Sonnet 4) by tasking them with
implementing a diverse set of core algorithms drawn from original publications.
Our results demonstrate that current LLMs can reliably reproduce package
functionality with performance indistinguishable from conventional libraries.
These findings foreshadow a paradigm shift toward flexible, on-demand code
generation and away from static, human-maintained packages, which will result
in reduced maintenance overhead by leveraging published articles as sufficient
context for the automated implementation of analytical workflows.

</details>


### [12] [AutoCodeSherpa: Symbolic Explanations in AI Coding Agents](https://arxiv.org/abs/2507.22414)
*Sungmin Kang,Haifeng Ruan,Abhik Roychoudhury*

Main category: cs.SE

TL;DR: LLM代理通过程序分析工具改进软件工程任务中的补丁质量，提供符号化公式解释以增强人类开发者理解和自动化工作流。


<details>
  <summary>Details</summary>
Motivation: 提升LLM代理在软件工程任务中的补丁质量，并通过符号化解释增强人类开发者对代理输出的理解和自动化工作流的可信度。

Method: 提出一个工作流，代理通过符号化公式（如输入条件、感染条件和输出条件）解释错误，并以基于属性的测试（PBT）和程序内部符号表达式形式实现。

Result: 符号化解释可帮助人类开发者理解代理输出，并用于完全自动化的补丁接受或拒绝决策。

Conclusion: 程序分析驱动的解释可提升LLM代理的补丁质量，并为其他基于LLM的修复技术提供支持。

Abstract: Large Language Model (LLM) agents autonomously use external tools on top of
one or more LLMs to accomplish specific tasks. Lately LLM agents for software
engineering tasks have become popular. These agents can benefit from the use of
program analysis tools working on program representations. This is demonstrated
by existing agentic AI solutions such as AutoCodeRover or SpecRover which
perform automated program repair. Specifically the goal of these works is to
use program analysis to improve the patch quality. These agents are currently
being used to automatically fix static analysis issues from the widely used
SonarQube static analyzer.
  Nevertheless, for the agents to be deployed in a production environment,
agents need to suggest software artifacts, such as patches, with evidence and
with high confidence. In this work, we provide a workflow where an agent
provides explanations of the bug in the form of symbolic formulae. The
explanations are in the form of input conditions, infection conditions and
output conditions, implemented as property based tests (PBT) and
program-internal symbolic expressions. These can help in human developer
cognition of the agent outputs as well as in achieving completely automated
agentic workflows for software. The human developer can benefit from the input
condition, represented as a PBT, to generate various concrete inputs showing a
given issue. Furthermore, since the PBTs are executable, our explanations are
executable as well. We can thus also use the explanations in a completely
automated issue resolution environment for accepting or rejecting the patches
that are suggested by patching agents such as AutoCodeRover. Finally, as
agentic AI approaches continue to develop, the program analysis driven
explanations can be provided to other LLM-based repair techniques such as
Agentless to improve their output.

</details>


### [13] [Ensemble Fuzzing with Dynamic Resource Scheduling and Multidimensional Seed Evaluation](https://arxiv.org/abs/2507.22442)
*Yukai Zhao,Shaohua Wang,Jue Wang,Xing Hu,Xin Xia*

Main category: cs.SE

TL;DR: Legion是一个新型的集成模糊测试框架，通过动态资源调度和多维种子评估策略，显著提升了模糊测试的效率和效果。


<details>
  <summary>Details</summary>
Motivation: 现有的集成模糊测试技术在资源调度和性能评估方面存在不足，导致资源浪费。Legion旨在解决这些问题。

Method: Legion采用基于上置信界算法的资源调度算法，并引入多维种子评估策略。

Result: 在Google的模糊测试套件和实际开源项目中，Legion表现优于现有技术，发现了20个漏洞，包括5个未知漏洞和3个CVE漏洞。

Conclusion: Legion通过动态资源调度和全面的性能评估，显著提升了集成模糊测试的效果。

Abstract: Fuzzing is widely used for detecting bugs and vulnerabilities, with various
techniques proposed to enhance its effectiveness. To combine the advantages of
multiple technologies, researchers proposed ensemble fuzzing, which integrates
multiple base fuzzers. Despite promising results, state-of-the-art ensemble
fuzzing techniques face limitations in resource scheduling and performance
evaluation, leading to unnecessary resource waste. In this paper, we propose
Legion, a novel ensemble fuzzing framework that dynamically schedules resources
during the ensemble fuzzing campaign. We designed a novel resource scheduling
algorithm based on the upper confidence bound algorithm to reduce the resource
consumption of ineffective base fuzzers. Additionally, we introduce a
multidimensional seed evaluation strategy, which considers multiple metrics to
achieve more comprehensive fine-grained performance evaluation. We implemented
Legion as a prototype tool and evaluated its effectiveness on Google's
fuzzer-test-suite as well as real-world open-source projects. Results show that
Legion outperforms existing state-of-the-art base fuzzers and ensemble fuzzing
techniques, detecting 20 vulnerabilities in real-world open-source
projects-five previously unknown and three classified as CVEs.

</details>


### [14] [Inside madupite: Technical Design and Performance](https://arxiv.org/abs/2507.22538)
*Matilde Gargiani,Robin Sieber,Philip Pawlowsky,John Lygeros*

Main category: cs.SE

TL;DR: Madupite是一个高性能求解器，专为大规模折扣无限时域马尔可夫决策过程设计，支持分布式计算，能高效处理超出单机内存的问题。


<details>
  <summary>Details</summary>
Motivation: 现有求解器难以高效处理大规模马尔可夫决策过程，尤其是内存受限或接近无折扣的情况。

Method: Madupite基于数学优化方法，支持分布式计算和用户自定义算法，以利用问题特定结构加速收敛。

Result: Madupite在流行病学和控制等领域的应用中表现出卓越的可扩展性和效率，能精确求解大规模问题。

Conclusion: Madupite通过分布式计算和灵活性，成为解决大规模马尔可夫决策过程的领先工具。

Abstract: In this work, we introduce and benchmark madupite, a newly proposed
high-performance solver designed for large-scale discounted infinite-horizon
Markov decision processes with finite state and action spaces. After a brief
overview of the class of mathematical optimization methods on which madupite
relies, we provide details on implementation choices, technical design and
deployment. We then demonstrate its scalability and efficiency by showcasing
its performance on the solution of Markov decision processes arising from
different application areas, including epidemiology and classical control.
Madupite sets a new standard as, to the best of our knowledge, it is the only
solver capable of efficiently computing exact solutions for large-scale Markov
decision processes, even when these exceed the memory capacity of modern
laptops and operate in near-undiscounted settings. This is possible as madupite
can work in a fully distributed manner and therefore leverage the memory
storage and computation capabilities of modern high-performance computing
clusters. This key feature enables the solver to efficiently handle problems of
medium to large size in an exact manner instead of necessarily resorting to
function approximations. Moreover, madupite is unique in allowing users to
customize the solution algorithm to better exploit the specific structure of
their problem, significantly accelerating convergence especially in
large-discount factor settings. Overall, madupite represents a significant
advancement, offering unmatched scalability and flexibility in solving
large-scale Markov decision processes.

</details>


### [15] [RePaCA: Leveraging Reasoning Large Language Models for Static Automated Patch Correctness Assessment](https://arxiv.org/abs/2507.22580)
*Marcos Fuster-Pena,David de-Fitero-Dominguez,Antonio Garcia-Cabot,Eva Garcia-Lopez*

Main category: cs.SE

TL;DR: RePaCA是一种基于大型语言模型（LLM）的静态APCA技术，通过强化学习优化，显著提升了补丁正确性评估的准确性、泛化能力和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有静态APCA技术在可靠性、灵活性和透明度方面存在不足，RePaCA旨在通过LLM的推理能力解决这些问题。

Method: 利用LLM分析代码差异并生成推理链，通过强化学习（Group Relative Policy Optimization）微调模型，实现补丁分类。

Result: 在Defects4J测试中达到83.1%准确率和84.8% F1分数，泛化能力优于现有技术。

Conclusion: RePaCA展示了微调LLM在提升静态APCA性能方面的潜力，尤其在准确性、泛化和可解释性方面表现突出。

Abstract: Automated Program Repair (APR) seeks to automatically correct software bugs
without requiring human intervention. However, existing tools tend to generate
patches that satisfy test cases without fixing the underlying bug, those are
known as overfitting patches. To address this issue, Automated Patch
Correctness Assessment (APCA) attempts to identify overfitting patches
generated by APR tools. It can be solved as a static approach, meaning that no
additional information is needed beyond the original and fixed code snippets.
Current static techniques often struggle with reliability, flexibility and
transparency. To address these issues, we introduce RePaCA, a novel static APCA
technique that leverages Large Language Models (LLMs) specialized in thinking
tasks. Our model is prompted with both buggy and fixed code snippets and guided
to generate a Chain of Thought that analyses code differences, reasons about
how the patch addresses the root cause, and ultimately provides a binary
classification: correct or overfitting. To enhance these reasoning capabilities
for the APCA task specifically, the LLM is finetuned using Reinforcement
Learning with the Group Relative Policy Optimization algorithm. When evaluated
on a standard Defects4J-derived test, our approach achieves state-of-the-art
performance, with 83.1% accuracy and an 84.8% F1-score. Furthermore, our model
demonstrates superior generalization capabilities when trained on different
datasets, outperforming the leading technique. This reasoning capability also
provides enhanced explainability for the patch assessment. These findings
underscore the considerable promise of finetuned, reasoning LLMs to advance
static APCA by enhancing accuracy, generalization, and explainability.

</details>


### [16] [Metamorphic Testing of Deep Code Models: A Systematic Literature Review](https://arxiv.org/abs/2507.22610)
*Ali Asgari,Milan de Koning,Pouria Derakhshanfar,Annibale Panichella*

Main category: cs.SE

TL;DR: 本文综述了针对深度代码模型的蜕变测试方法，分析了45篇论文，总结了当前研究现状、挑战及未来方向。


<details>
  <summary>Details</summary>
Motivation: 深度代码模型在软件工程中表现优异，但其鲁棒性仍需验证，尤其是在对抗性条件下。蜕变测试是评估模型鲁棒性的有效方法。

Method: 通过系统文献综述，分析45篇论文中的蜕变测试方法，包括变换类型、技术和评估指标。

Result: 总结了当前研究中的常用模型、编程任务、数据集、目标语言和评估指标，并识别了关键挑战。

Conclusion: 蜕变测试是提升深度代码模型鲁棒性的重要工具，未来研究需解决现有挑战并探索新方向。

Abstract: Large language models and deep learning models designed for code intelligence
have revolutionized the software engineering field due to their ability to
perform various code-related tasks. These models can process source code and
software artifacts with high accuracy in tasks such as code completion, defect
detection, and code summarization; therefore, they can potentially become an
integral part of modern software engineering practices. Despite these
capabilities, robustness remains a critical quality attribute for deep-code
models as they may produce different results under varied and adversarial
conditions (e.g., variable renaming). Metamorphic testing has become a widely
used approach to evaluate models' robustness by applying semantic-preserving
transformations to input programs and analyzing the stability of model outputs.
While prior research has explored testing deep learning models, this systematic
literature review focuses specifically on metamorphic testing for deep code
models. By studying 45 primary papers, we analyze the transformations,
techniques, and evaluation methods used to assess robustness. Our review
summarizes the current landscape, identifying frequently evaluated models,
programming tasks, datasets, target languages, and evaluation metrics, and
highlights key challenges and future directions for advancing the field.

</details>


### [17] [A Systematic Literature Review on Detecting Software Vulnerabilities with Large Language Models](https://arxiv.org/abs/2507.22659)
*Sabrina Kaniewski,Fabian Schmidt,Markus Enzweiler,Michael Menth,Tobias Heer*

Main category: cs.SE

TL;DR: 本文通过系统性文献综述（SLR）分析了227项关于基于大语言模型（LLM）的软件漏洞检测研究，旨在解决该领域研究碎片化问题。


<details>
  <summary>Details</summary>
Motivation: 由于LLM在软件工程中的广泛应用，其用于漏洞检测的研究迅速增长，但研究差异大、难以比较，导致领域碎片化。

Method: 对2020年1月至2025年6月间的227项研究进行分类，分析任务定义、输入表示、系统架构、适应技术及数据集特征。

Result: 提出了漏洞检测方法的细粒度分类，识别了关键限制，并总结了未来研究方向。

Conclusion: 通过结构化综述，提升了领域透明度，为研究者提供了可比较和可复现的研究指南。

Abstract: The increasing adoption of Large Language Models (LLMs) in software
engineering has sparked interest in their use for software vulnerability
detection. However, the rapid development of this field has resulted in a
fragmented research landscape, with diverse studies that are difficult to
compare due to differences in, e.g., system designs and dataset usage. This
fragmentation makes it difficult to obtain a clear overview of the
state-of-the-art or compare and categorize studies meaningfully. In this work,
we present a comprehensive systematic literature review (SLR) of LLM-based
software vulnerability detection. We analyze 227 studies published between
January 2020 and June 2025, categorizing them by task formulation, input
representation, system architecture, and adaptation techniques. Further, we
analyze the datasets used, including their characteristics, vulnerability
coverage, and diversity. We present a fine-grained taxonomy of vulnerability
detection approaches, identify key limitations, and outline actionable future
research opportunities. By providing a structured overview of the field, this
review improves transparency and serves as a practical guide for researchers
and practitioners aiming to conduct more comparable and reproducible research.
We publicly release all artifacts and maintain a living repository of LLM-based
software vulnerability detection studies.

</details>


### [18] [RobEthiChor: Automated Context-aware Ethics-based Negotiation for Autonomous Robots](https://arxiv.org/abs/2507.22664)
*Mashal Afzal Memon,Gianluca Filippone,Gian Luca Scoccia,Marco Autili,Paola Inverardi*

Main category: cs.SE

TL;DR: 论文提出RobEthiChor方法，使自主系统能通过伦理协商融入用户伦理偏好，并展示其实现RobEthiChor-Ros在ROS中的有效性。


<details>
  <summary>Details</summary>
Motivation: 自主系统缺乏融入用户伦理偏好的能力，影响信任和个性化决策，需解决伦理协商问题。

Method: 提出RobEthiChor框架，包括领域无关的参考架构及ROS实现RobEthiChor-Ros，支持伦理协商。

Result: 实验显示，RobEthiChor-Ros在73%场景中成功达成协议，平均协商时间0.67秒，且具有可扩展性。

Conclusion: RobEthiChor有效实现伦理协商，提升自主系统决策的伦理适应性和用户信任。

Abstract: The presence of autonomous systems is growing at a fast pace and it is
impacting many aspects of our lives. Designed to learn and act independently,
these systems operate and perform decision-making without human intervention.
However, they lack the ability to incorporate users' ethical preferences, which
are unique for each individual in society and are required to personalize the
decision-making processes. This reduces user trust and prevents autonomous
systems from behaving according to the moral beliefs of their end-users. When
multiple systems interact with differing ethical preferences, they must
negotiate to reach an agreement that satisfies the ethical beliefs of all the
parties involved and adjust their behavior consequently. To address this
challenge, this paper proposes RobEthiChor, an approach that enables autonomous
systems to incorporate user ethical preferences and contextual factors into
their decision-making through ethics-based negotiation. RobEthiChor features a
domain-agnostic reference architecture for designing autonomous systems capable
of ethic-based negotiating. The paper also presents RobEthiChor-Ros, an
implementation of RobEthiChor within the Robot Operating System (ROS), which
can be deployed on robots to provide them with ethics-based negotiation
capabilities. To evaluate our approach, we deployed RobEthiChor-Ros on real
robots and ran scenarios where a pair of robots negotiate upon resource
contention. Experimental results demonstrate the feasibility and effectiveness
of the system in realizing ethics-based negotiation. RobEthiChor allowed robots
to reach an agreement in more than 73\% of the scenarios with an acceptable
negotiation time (0.67s on average). Experiments also demonstrate that the
negotiation approach implemented in RobEthiChor is scalable.

</details>


### [19] [The Multi-Agent Fault Localization System Based on Monte Carlo Tree Search Approach](https://arxiv.org/abs/2507.22800)
*Rui Ren*

Main category: cs.SE

TL;DR: 论文提出KnowledgeMind，一种基于蒙特卡洛树搜索和知识库奖励机制的LLM多代理系统，用于解决微服务系统中根因分析的挑战。


<details>
  <summary>Details</summary>
Motivation: 微服务系统的高解耦和灵活性增加了可靠性挑战，现有LLM方法因幻觉和异常传播导致定位不准确，且上下文窗口长度受限。

Method: 采用蒙特卡洛树搜索和知识库奖励机制，实现标准化逐服务推理，减少上下文窗口负担并抑制幻觉。

Result: 相比现有方法，上下文窗口需求减少90%，根因定位准确率提升49.29%至128.35%。

Conclusion: KnowledgeMind显著提升了根因分析的准确性和效率，解决了现有LLM方法的局限性。

Abstract: In real-world scenarios, due to the highly decoupled and flexible nature of
microservices, it poses greater challenges to system reliability. The more
frequent occurrence of incidents has created a demand for Root Cause
Analysis(RCA) methods that enable rapid identification and recovery of
incidents. Large language model (LLM) provides a new path for quickly locating
and recovering from incidents by leveraging their powerful generalization
ability combined with expert experience. Current LLM for RCA frameworks are
based on ideas like ReAct and Chain-of-Thought, but the hallucination of LLM
and the propagation nature of anomalies often lead to incorrect localization
results. Moreover, the massive amount of anomalous information generated in
large, complex systems presents a huge challenge for the context window length
of LLMs. To address these challenges, we propose KnowledgeMind, an innovative
LLM multi-agent system based on Monte Carlo Tree Search and a knowledge base
reward mechanism for standardized service-by-service reasoning. Compared to
State-Of-The-Art(SOTA) LLM for RCA methods, our service-by-service exploration
approach significantly reduces the burden on the maximum context window length,
requiring only one-tenth of its size. Additionally, by incorporating a
rule-based real-time reward mechanism, our method effectively mitigates
hallucinations during the inference process. Compared to the SOTA LLM for RCA
framework, our method achieves a 49.29% to 128.35% improvement in root cause
localization accuracy.

</details>


### [20] [Repair-R1: Better Test Before Repair](https://arxiv.org/abs/2507.22853)
*Haichuan Hu,Xiaochen Xie,Quanjun Zhang*

Main category: cs.SE

TL;DR: Repair-R1是一种改进的自动程序修复（APR）方法，通过在训练阶段引入测试用例，并优先生成测试用例以辅助修复，显著提高了修复效果。


<details>
  <summary>Details</summary>
Motivation: 现有LLM-based APR方法在训练阶段未利用测试用例，且测试验证滞后于修复，限制了修复效果。

Method: Repair-R1在训练阶段引入测试用例，并优先生成区分性测试用例，再基于测试进行修复，结合强化学习优化测试生成与修复。

Result: 在四个基准测试中，Repair-R1显著提高了修复成功率（2.68%至48.29%）、测试生成成功率（16.38%至53.28%）和测试覆盖率（0.78%至53.96%）。

Conclusion: Repair-R1通过优化测试用例的利用顺序和方式，显著提升了APR的效果，为未来研究提供了新思路。

Abstract: APR (Automated Program Repair) aims to automatically locate program defects,
generate patches and validate the repairs. Existing techniques for APR are
often combined with LLMs (Large Language Models), which leverages the
code-related knowledge of LLMs to improve repair effectiveness. Current
LLM-based APR methods typically utilize test cases only during the inference
stage, adopting an iterative approach that performs repair first and validates
it through test execution afterward. This conventional paradigm neglects two
important aspects: the potential contribution of test cases in the training
phase, and the possibility of leveraging testing prior to repair. To address
this, we propose Repair-R1, which introduces test cases into the model's
training phase and shifts test generation to precede repair. The model is
required to first generate discriminative test cases that can distinguish
defective behaviors, and then perform repair based on these tests. This enables
the model to better locate defects and understand the underlying causes of
defects, thereby improving repair effectiveness. We implement Repair-R1 with
three different backbone models, using RL (reinforcement learning) to
co-optimize test generation and bug repair. Experimental results on four widely
adopted benchmarks demonstrate the superiority of Repair-R1. Specially,
compared to vanilla models, Repair-R1 improves repair success rate by 2.68\% to
48.29\%, test generation success rate by 16.38\% to 53.28\%, and test coverage
by 0.78\% to 53.96\%. We publish the code and weights at
https://github.com/Tomsawyerhu/APR-RL and
https://huggingface.co/tomhu/Qwen3-4B-RL-5000-step.

</details>


### [21] [Tracking research software outputs in the UK](https://arxiv.org/abs/2507.22871)
*Domhnall Carlin,Austen Rainer*

Main category: cs.SE

TL;DR: 研究软件在科研中至关重要，但英国学术机构对其存储和注册的情况显示，软件作为研究成果的报告比例较低，且共享率不足，影响其长期维护和发展。


<details>
  <summary>Details</summary>
Motivation: 开放科学背景下，研究软件作为重要研究产物的可追溯性和共享性面临挑战，需探索其存储和注册现状。

Method: 通过分析英国研究创新署（UKRI）的Gateway to Research（GtR）元数据，调查英国公共资助研究软件的存储和注册情况。

Result: 软件作为研究成果的报告比例低，共享率不足（25%无链接，45%链接错误或缺失），GitHub是主要托管平台（占18%）。

Conclusion: 研究软件共享不足可能导致其沦为短期工具，影响科学的长期发展，需加强共享机制。

Abstract: Research software is crucial in the research process and the growth of Open
Science underscores the importance of accessing research artifacts, like data
and code, raising traceability challenges among outputs. While it is a clear
principle that research code, along with other essential outputs, should be
recognised as artifacts of the research process, the how of this principle
remains variable. This study examines where UK academic institutions store and
register software as a unique research output, searching the UKRI's Gateway to
Research (GtR) metadata for publicly funded research software in the UK. The
quantity of software reported as research outcomes remains low in proportion to
other categories. Artifact sharing appears low, with one-quarter of the
reported software having no links and 45% having either a missing or erroneous
URL. Of the valid URLs, we find the single largest category is Public
Commercial Code Repository, with GitHub being the host of 18% of all publicly
funded research software listed. These observations are contrasted with past
findings from 2023 and finally, we discuss the lack of artifact sharing in UK
research, with resulting implications for the maintenance and evolution of
research software. Without dissemination, research software risks demotion to a
transient artifact, useful only to meet short term research demands but
ultimately lost to the broader enterprise of science.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [22] [Prompt Optimization and Evaluation for LLM Automated Red Teaming](https://arxiv.org/abs/2507.22133)
*Michael Freenor,Lauren Alvarez,Milton Leal,Lily Smith,Joel Garrett,Yelyzaveta Husieva,Madeline Woodruff,Ryan Miller,Erich Kummerfeld,Rafael Medeiros,Sander Schulhoff*

Main category: cs.CR

TL;DR: 本文提出了一种优化攻击生成器提示的方法，通过重复攻击测量攻击的可发现性，以改进生成器的评估和优化。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型应用的普及，识别系统漏洞变得至关重要，自动红队测试通过生成和执行攻击加速了这一过程。

Method: 通过重复攻击随机种子目标，测量攻击的可发现性（个体攻击成功的期望），从而优化攻击生成器提示。

Result: 该方法揭示了可被利用的模式，为提示优化提供了依据。

Conclusion: 该方法能够更稳健地评估和优化攻击生成器。

Abstract: Applications that use Large Language Models (LLMs) are becoming widespread,
making the identification of system vulnerabilities increasingly important.
Automated Red Teaming accelerates this effort by using an LLM to generate and
execute attacks against target systems. Attack generators are evaluated using
the Attack Success Rate (ASR) the sample mean calculated over the judgment of
success for each attack. In this paper, we introduce a method for optimizing
attack generator prompts that applies ASR to individual attacks. By repeating
each attack multiple times against a randomly seeded target, we measure an
attack's discoverability the expectation of the individual attack success. This
approach reveals exploitable patterns that inform prompt optimization,
ultimately enabling more robust evaluation and refinement of generators.

</details>


### [23] [Strategic Deflection: Defending LLMs from Logit Manipulation](https://arxiv.org/abs/2507.22160)
*Yassine Rachidy,Jihad Rbaiti,Youssef Hmamouche,Faissal Sehbaoui,Amal El Fallah Seghrouchni*

Main category: cs.CR

TL;DR: 论文提出了一种名为Strategic Deflection（SDeflection）的防御方法，通过语义转移而非直接拒绝来应对针对大语言模型的高级攻击，显著降低了攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在关键领域的广泛应用，确保其安全性对抗越狱攻击变得至关重要。传统防御方法主要依赖拒绝恶意提示，但最近基于logit-level的攻击已能绕过这些防御。

Method: 提出SDeflection防御方法，通过生成与用户请求语义相邻但去除有害意图的答案，中和攻击者的恶意意图。

Result: 实验表明，SDeflection显著降低了攻击成功率（ASR），同时保持了对良性查询的模型性能。

Conclusion: 该研究标志着防御策略从简单拒绝转向战略内容重定向，以应对高级威胁。

Abstract: With the growing adoption of Large Language Models (LLMs) in critical areas,
ensuring their security against jailbreaking attacks is paramount. While
traditional defenses primarily rely on refusing malicious prompts, recent
logit-level attacks have demonstrated the ability to bypass these safeguards by
directly manipulating the token-selection process during generation. We
introduce Strategic Deflection (SDeflection), a defense that redefines the
LLM's response to such advanced attacks. Instead of outright refusal, the model
produces an answer that is semantically adjacent to the user's request yet
strips away the harmful intent, thereby neutralizing the attacker's harmful
intent. Our experiments demonstrate that SDeflection significantly lowers
Attack Success Rate (ASR) while maintaining model performance on benign
queries. This work presents a critical shift in defensive strategies, moving
from simple refusal to strategic content redirection to neutralize advanced
threats.

</details>


### [24] [Programmable Data Planes for Network Security](https://arxiv.org/abs/2507.22165)
*Gursimran Singh,H. B. Acharya,Minseok Kwon*

Main category: cs.CR

TL;DR: 可编程数据平面（如支持P4语言的交换机）推动了网络安全的革新，支持定制化、高速率的包处理，用于DDoS防御、防火墙策略执行等。本文系统化总结了相关技术、挑战及解决方案。


<details>
  <summary>Details</summary>
Motivation: 探讨可编程交换机在网络安全中的潜力，解决传统硬件限制下的复杂安全功能实现问题。

Method: 通过技术如recirculate-and-truncate和预计算查找表，克服硬件限制（如内存有限和指令集受限）。

Result: 展示了可编程交换机在多种安全应用中的可行性，并总结了设计技巧。

Conclusion: 尽管硬件有限制，但通过创新设计可实现复杂安全功能，未来仍有研究空间。

Abstract: The emergence of programmable data planes, and particularly switches
supporting the P4 language, has transformed network security by enabling
customized, line-rate packet processing. These switches, originally intended
for flexible forwarding, now play a broader role: detecting and mitigating
attacks such as DDoS and spoofing, enforcing next-generation firewall policies,
and even supporting in-network cryptography and machine learning. These
capabilities are made possible by techniques such as recirculate-and-truncate
and lookup-table precomputation, which work around architectural constraints
like limited memory and restricted instruction sets. In this paper, we
systematize recent advances in security applications built on programmable
switches, with an emphasis on the capabilities, challenges, and architectural
workarounds. We highlight the non-obvious design techniques that make complex
in-network security functions feasible despite the constraints of the hardware
platform, and also comment on remaining issues and emerging research
directions.

</details>


### [25] [Enhancing Jailbreak Attacks on LLMs via Persona Prompts](https://arxiv.org/abs/2507.22171)
*Zheng Zhang,Peilin Zhao,Deheng Ye,Hao Wang*

Main category: cs.CR

TL;DR: 本文提出了一种基于遗传算法的方法，通过自动生成角色提示来绕过大型语言模型的安全机制，显著降低了拒绝率并增强了现有攻击方法的成功率。


<details>
  <summary>Details</summary>
Motivation: 研究角色提示在绕过大型语言模型安全防御中的有效性，填补了现有攻击方法中对角色提示影响的忽视。

Method: 采用遗传算法自动生成角色提示，以绕过模型的安全机制，并通过实验验证其效果。

Result: 生成的提示将拒绝率降低了50-70%，与现有攻击方法结合后成功率提高了10-20%。

Conclusion: 角色提示在绕过大型语言模型安全防御中具有显著效果，未来研究应进一步探索其潜在风险。

Abstract: Jailbreak attacks aim to exploit large language models (LLMs) by inducing
them to generate harmful content, thereby revealing their vulnerabilities.
Understanding and addressing these attacks is crucial for advancing the field
of LLM safety. Previous jailbreak approaches have mainly focused on direct
manipulations of harmful intent, with limited attention to the impact of
persona prompts. In this study, we systematically explore the efficacy of
persona prompts in compromising LLM defenses. We propose a genetic
algorithm-based method that automatically crafts persona prompts to bypass
LLM's safety mechanisms. Our experiments reveal that: (1) our evolved persona
prompts reduce refusal rates by 50-70% across multiple LLMs, and (2) these
prompts demonstrate synergistic effects when combined with existing attack
methods, increasing success rates by 10-20%. Our code and data are available at
https://github.com/CjangCjengh/Generic_Persona.

</details>


### [26] [POLARIS: Explainable Artificial Intelligence for Mitigating Power Side-Channel Leakage](https://arxiv.org/abs/2507.22177)
*Tanzim Mahfuz,Sudipta Paria,Tasneem Suha,Swarup Bhunia,Prabuddha Chakraborty*

Main category: cs.CR

TL;DR: POLARIS是一个基于可解释人工智能（XAI）的新型框架，用于减少微电子系统中的电源侧信道泄漏，其性能优于现有解决方案。


<details>
  <summary>Details</summary>
Motivation: 微电子系统在敏感应用中广泛使用，但容易受到电源侧信道攻击的威胁，需要更有效的泄漏缓解方法。

Method: POLARIS采用无监督过程自动构建训练数据集，并利用XAI指导的掩蔽方法训练模型。

Result: POLARIS在泄漏减少、执行时间和开销方面优于现有解决方案（如VALIANT）。

Conclusion: POLARIS为电源侧信道泄漏提供了一种高效且可扩展的缓解方案。

Abstract: Microelectronic systems are widely used in many sensitive applications (e.g.,
manufacturing, energy, defense). These systems increasingly handle sensitive
data (e.g., encryption key) and are vulnerable to diverse threats, such as,
power side-channel attacks, which infer sensitive data through dynamic power
profile. In this paper, we present a novel framework, POLARIS for mitigating
power side channel leakage using an Explainable Artificial Intelligence (XAI)
guided masking approach. POLARIS uses an unsupervised process to automatically
build a tailored training dataset and utilize it to train a masking model.The
POLARIS framework outperforms state-of-the-art mitigation solutions (e.g.,
VALIANT) in terms of leakage reduction, execution time, and overhead across
large designs.

</details>


### [27] [Understanding Concept Drift with Deprecated Permissions in Android Malware Detection](https://arxiv.org/abs/2507.22231)
*Ahmed Sabbah,Radi Jarrar,Samer Zein,David Mohaisen*

Main category: cs.CR

TL;DR: 研究探讨了Android权限分析中废弃和受限权限对机器学习模型性能的影响，发现排除这些权限对模型性能影响有限，甚至在某些情况下提升了准确率。


<details>
  <summary>Details</summary>
Motivation: 现有研究在Android恶意软件检测中忽略了权限的保护级别、废弃或受限等因素，这些可能导致概念漂移。

Method: 使用包含166个权限的大型数据集，应用多种机器学习和深度学习算法，并结合概念漂移检测策略。

Result: 废弃和受限权限的排除对模型性能影响较小，CNN准确率甚至提升；数据集平衡进一步优化了性能和概念漂移检测。

Conclusion: Android权限是有效的恶意软件检测特征，排除废弃和受限权限对模型影响有限，且能提升概念漂移检测能力。

Abstract: Permission analysis is a widely used method for Android malware detection. It
involves examining the permissions requested by an application to access
sensitive data or perform potentially malicious actions. In recent years,
various machine learning (ML) algorithms have been applied to Android malware
detection using permission-based features and feature selection techniques,
often achieving high accuracy. However, these studies have largely overlooked
important factors such as protection levels and the deprecation or restriction
of permissions due to updates in the Android OS -- factors that can contribute
to concept drift.
  In this study, we investigate the impact of deprecated and restricted
permissions on the performance of machine learning models. A large dataset
containing 166 permissions was used, encompassing more than 70,000 malware and
benign applications. Various machine learning and deep learning algorithms were
employed as classifiers, along with different concept drift detection
strategies. The results suggest that Android permissions are highly effective
features for malware detection, with the exclusion of deprecated and restricted
permissions having only a marginal impact on model performance. In some cases,
such as with CNN, accuracy improved. Excluding these permissions also enhanced
the detection of concept drift using a year-to-year analysis strategy. Dataset
balancing further improved model performance, reduced low-accuracy instances,
and enhanced concept drift detection via the Kolmogorov-Smirnov test.

</details>


### [28] [Large Language Model-Based Framework for Explainable Cyberattack Detection in Automatic Generation Control Systems](https://arxiv.org/abs/2507.22239)
*Muhammad Sharshar,Ahmad Mohammad Saber,Davor Svetinovic,Amr M. Youssef,Deepa Kundur,Ehab F. El-Saadany*

Main category: cs.CR

TL;DR: 论文提出了一种结合轻量级机器学习和大型语言模型的混合框架，用于检测智能电网中的虚假数据注入攻击，并提供可解释的自然语言说明。


<details>
  <summary>Details</summary>
Motivation: 智能电网的数字化提升了效率，但也带来了网络安全漏洞，如针对自动发电控制系统的虚假数据注入攻击。现有的机器学习和深度学习模型虽然能检测攻击，但其不透明的决策过程限制了操作员的信任和实际应用。

Method: 提出了一种混合框架，结合轻量级机器学习（如LightGBM）进行攻击检测，并利用大型语言模型（如GPT系列）生成自然语言解释。

Result: LightGBM检测准确率达95.13%，推理延迟仅0.004秒；GPT-4o mini在100个测试样本中，攻击目标识别准确率为93%，攻击幅度估计的平均绝对误差为0.075 pu，攻击起始时间估计的平均绝对误差为2.19秒。

Conclusion: 该框架在实时检测与高保真解释之间取得了平衡，满足了智能电网网络安全中对可操作AI的需求。

Abstract: The increasing digitization of smart grids has improved operational
efficiency but also introduced new cybersecurity vulnerabilities, such as False
Data Injection Attacks (FDIAs) targeting Automatic Generation Control (AGC)
systems. While machine learning (ML) and deep learning (DL) models have shown
promise in detecting such attacks, their opaque decision-making limits operator
trust and real-world applicability. This paper proposes a hybrid framework that
integrates lightweight ML-based attack detection with natural language
explanations generated by Large Language Models (LLMs). Classifiers such as
LightGBM achieve up to 95.13% attack detection accuracy with only 0.004 s
inference latency. Upon detecting a cyberattack, the system invokes LLMs,
including GPT-3.5 Turbo, GPT-4 Turbo, and GPT-4o mini, to generate
human-readable explanation of the event. Evaluated on 100 test samples, GPT-4o
mini with 20-shot prompting achieved 93% accuracy in identifying the attack
target, a mean absolute error of 0.075 pu in estimating attack magnitude, and
2.19 seconds mean absolute error (MAE) in estimating attack onset. These
results demonstrate that the proposed framework effectively balances real-time
detection with interpretable, high-fidelity explanations, addressing a critical
need for actionable AI in smart grid cybersecurity.

</details>


### [29] [Invisible Injections: Exploiting Vision-Language Models Through Steganographic Prompt Embedding](https://arxiv.org/abs/2507.22304)
*Chetan Pathade*

Main category: cs.CR

TL;DR: 该论文首次全面研究了针对视觉语言模型（VLMs）的隐写提示注入攻击，揭示了当前VLM架构的安全漏洞，并提出有效对策。


<details>
  <summary>Details</summary>
Motivation: 探索视觉语言模型在安全方面的潜在漏洞，尤其是在多模态AI应用中可能被恶意利用的隐写提示注入攻击。

Method: 开发了一个结合空间、频率和神经隐写技术的多域嵌入框架，对包括GPT-4V、Claude和LLaVA在内的主流VLM进行攻击测试。

Result: 攻击成功率为24.3%（±3.2%，95% CI），神经隐写方法最高达31.8%，同时保持视觉不可感知性（PSNR>38 dB，SSIM>0.94）。

Conclusion: 当前VLM架构存在中度但有意义的漏洞，需开发多模态AI安全框架以应对此类威胁。

Abstract: Vision-language models (VLMs) have revolutionized multimodal AI applications
but introduce novel security vulnerabilities that remain largely unexplored. We
present the first comprehensive study of steganographic prompt injection
attacks against VLMs, where malicious instructions are invisibly embedded
within images using advanced steganographic techniques. Our approach
demonstrates that current VLM architectures can inadvertently extract and
execute hidden prompts during normal image processing, leading to covert
behavioral manipulation. We develop a multi-domain embedding framework
combining spatial, frequency, and neural steganographic methods, achieving an
overall attack success rate of 24.3% (plus or minus 3.2%, 95% CI) across
leading VLMs including GPT-4V, Claude, and LLaVA, with neural steganography
methods reaching up to 31.8%, while maintaining reasonable visual
imperceptibility (PSNR greater than 38 dB, SSIM greater than 0.94). Through
systematic evaluation on 12 diverse datasets and 8 state-of-the-art models, we
reveal moderate but meaningful vulnerabilities in current VLM architectures and
propose effective countermeasures. Our findings have significant implications
for VLM deployment in security-critical applications and highlight the need for
proportionate multimodal AI security frameworks.

</details>


### [30] [SleepWalk: Exploiting Context Switching and Residual Power for Physical Side-Channel Attacks](https://arxiv.org/abs/2507.22306)
*Sahan Sanjaya,Aruna Jayasena,Prabhat Mishra*

Main category: cs.CR

TL;DR: 论文提出了一种利用上下文切换时功率峰值的新型物理侧信道泄漏源，简化了攻击过程，仅需分析单个功率峰值幅度即可实现密钥恢复。


<details>
  <summary>Details</summary>
Motivation: 传统侧信道攻击需要复杂预处理或外部同步触发，而上下文切换时的功率峰值提供了更简单的泄漏源。

Method: 利用系统内核内置的睡眠函数触发上下文切换，通过功率模型分析功率峰值与寄存器数据的关系。

Result: 实验证明该方法可成功恢复AES和SIKE实现的加密密钥。

Conclusion: 上下文切换功率峰值是一种高效且简化的侧信道攻击手段。

Abstract: Context switching is utilized by operating systems to change the execution
context between application programs. It involves saving and restoring the
states of multiple registers and performing a pipeline flush to remove any
pre-fetched instructions, leading to a higher instantaneous power consumption
compared to typical program execution. In this paper, we introduce a physical
power side-channel leakage source that exploits the power spike observed during
a context switch, triggered by the inbuilt sleep function of the system kernel.
We observed that this power spike directly correlates with both the power
consumption during context switching and the residual power consumption of the
previously executed program. Notably, the persistence of residual power
signatures from previous workloads extends the scope of this side-channel
beyond extracting the data in registers during the context switch. Unlike
traditional approaches that require analyzing full power traces, applying
complex preprocessing, or relying on external synchronization triggers, this
novel technique leverages only the amplitude of a single power spike,
significantly simplifying the attack. We developed a power model to illustrate
the feasibility of mounting end-to-end side-channel attacks using the
sleep-induced power spikes. Experimental evaluation demonstrates that our
framework can successfully perform cryptographic key recovery for both AES and
SIKE implementations on Broadcom BCM2711.

</details>


### [31] [Benchmarking Fraud Detectors on Private Graph Data](https://arxiv.org/abs/2507.22347)
*Alexander Goldberg,Giulia Fanti,Nihar Shah,Zhiwei Steven Wu*

Main category: cs.CR

TL;DR: 论文提出了一种在私有图数据上评估欺诈检测器的新方法，揭示了现有隐私攻击的风险，并探讨了差分隐私（DP）在保护数据时的效用问题。


<details>
  <summary>Details</summary>
Motivation: 当前许多欺诈检测算法基于图数据运行，但将算法开发外包给第三方时，存在隐私泄露风险。论文旨在解决这一问题。

Method: 提出了一种隐私攻击方法，并通过模拟实验验证其效果。随后研究了两种差分隐私解决方案：子采样聚合和DP合成图数据。

Result: 攻击方法在模拟实验中表现出高准确率（TPR 0.98，FPR 0.00）。差分隐私方法在保护隐私时牺牲了实用性。

Conclusion: 现有差分隐私方法在保护图数据隐私时面临偏差与方差的权衡，需进一步研究更高效的方法。

Abstract: We introduce the novel problem of benchmarking fraud detectors on private
graph-structured data. Currently, many types of fraud are managed in part by
automated detection algorithms that operate over graphs. We consider the
scenario where a data holder wishes to outsource development of fraud detectors
to third parties (e.g., vendors or researchers). The third parties submit their
fraud detectors to the data holder, who evaluates these algorithms on a private
dataset and then publicly communicates the results. We propose a realistic
privacy attack on this system that allows an adversary to de-anonymize
individuals' data based only on the evaluation results. In simulations of a
privacy-sensitive benchmark for facial recognition algorithms by the National
Institute of Standards and Technology (NIST), our attack achieves near perfect
accuracy in identifying whether individuals' data is present in a private
dataset, with a True Positive Rate of 0.98 at a False Positive Rate of 0.00. We
then study how to benchmark algorithms while satisfying a formal differential
privacy (DP) guarantee. We empirically evaluate two classes of solutions:
subsample-and-aggregate and DP synthetic graph data. We demonstrate through
extensive experiments that current approaches do not provide utility when
guaranteeing DP. Our results indicate that the error arising from DP trades off
between bias from distorting graph structure and variance from adding random
noise. Current methods lie on different points along this bias-variance
trade-off, but more complex methods tend to require high-variance noise
addition, undermining utility.

</details>


### [32] [SAEL: Leveraging Large Language Models with Adaptive Mixture-of-Experts for Smart Contract Vulnerability Detection](https://arxiv.org/abs/2507.22371)
*Lei Yu,Shiqi Cheng,Zhirong Huang,Jingyuan Zhang,Chenjie Shen,Junyi Lu,Li Yang,Fengjun Zhang,Jiajia Ma*

Main category: cs.CR

TL;DR: SAEL是一个基于大型语言模型（LLM）的智能合约漏洞检测框架，通过针对性提示和自适应专家混合架构，结合LLM预测、解释特征和代码特征，显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有漏洞检测方法在复杂场景和泛化能力上存在局限，而通用LLM虽适应性强但在特定漏洞类型上表现不足。SAEL旨在结合两者的优势。

Method: 设计针对性提示引导LLM生成预测和解释特征，通过提示调优CodeT5和T5处理代码和解释，并采用自适应专家混合架构动态调整特征权重。

Result: 实验表明SAEL在各种漏洞检测上优于现有方法。

Conclusion: SAEL通过结合LLM的适应性和专家模型的精确性，显著提升了智能合约漏洞检测的性能。

Abstract: With the increasing security issues in blockchain, smart contract
vulnerability detection has become a research focus. Existing vulnerability
detection methods have their limitations: 1) Static analysis methods struggle
with complex scenarios. 2) Methods based on specialized pre-trained models
perform well on specific datasets but have limited generalization capabilities.
In contrast, general-purpose Large Language Models (LLMs) demonstrate
impressive ability in adapting to new vulnerability patterns. However, they
often underperform on specific vulnerability types compared to methods based on
specialized pre-trained models. We also observe that explanations generated by
general-purpose LLMs can provide fine-grained code understanding information,
contributing to improved detection performance.
  Inspired by these observations, we propose SAEL, an LLM-based framework for
smart contract vulnerability detection. We first design targeted prompts to
guide LLMs in identifying vulnerabilities and generating explanations, which
serve as prediction features. Next, we apply prompt-tuning on CodeT5 and T5 to
process contract code and explanations, enhancing task-specific performance. To
combine the strengths of each approach, we introduce an Adaptive
Mixture-of-Experts architecture. This dynamically adjusts feature weights via a
Gating Network, which selects relevant features using TopK filtering and
Softmax normalization, and incorporates a Multi-Head Self-Attention mechanism
to enhance cross-feature relationships. This design enables effective
integration of LLM predictions, explanation features, and code features through
gradient optimization. The loss function jointly considers both independent
feature performance and overall weighted predictions. Experiments show that
SAEL outperforms existing methods across various vulnerabilities.

</details>


### [33] [Breaking Obfuscation: Cluster-Aware Graph with LLM-Aided Recovery for Malicious JavaScript Detection](https://arxiv.org/abs/2507.22447)
*Zhihong Liang,Xin Wang,Zhenhuang Hu,Liangliang Song,Lin Chen,Jingjing Guo,Yanbin Wang,Ye Tian*

Main category: cs.CR

TL;DR: DeCoda是一个结合大型语言模型（LLM）去混淆和代码图学习的混合防御框架，用于检测恶意JavaScript代码，显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 恶意JavaScript代码对用户隐私和系统安全构成威胁，但现有方法因代码混淆和JavaScript语言特性（如嵌套闭包）而难以检测。

Method: 1. 使用LLM逐步去混淆并生成标准化的AST表示；2. 通过Cluster-wise Graph学习层次化代码图表示，结合图变换网络和节点聚类。

Result: 在两个基准数据集上F1分数分别达到94.64%和97.71%，比现有方法提升10.74%和13.85%。

Conclusion: LLM去混淆和集群级关系建模在恶意代码检测中具有显著效果。

Abstract: With the rapid expansion of web-based applications and cloud services,
malicious JavaScript code continues to pose significant threats to user
privacy, system integrity, and enterprise security. But, detecting such threats
remains challenging due to sophisticated code obfuscation techniques and
JavaScript's inherent language characteristics, particularly its nested closure
structures and syntactic flexibility. In this work, we propose DeCoda, a hybrid
defense framework that combines large language model (LLM)-based deobfuscation
with code graph learning: (1) We first construct a sophisticated
prompt-learning pipeline with multi-stage refinement, where the LLM
progressively reconstructs the original code structure from obfuscated inputs
and then generates normalized Abstract Syntax Tree (AST) representations; (2)
In JavaScript ASTs, dynamic typing scatters semantically similar nodes while
deeply nested functions fracture scope capturing, introducing structural noise
and semantic ambiguity. To address these challenges, we then propose to learn
hierarchical code graph representations via a Cluster-wise Graph that
synergistically integrates graph transformer network, node clustering, and
node-to-cluster attention to simultaneously capture both local node-level
semantics and global cluster-induced structural relationships from AST graph.
Experimental results demonstrate that our method achieves F1-scores of 94.64%
and 97.71% on two benchmark datasets, demonstrating absolute improvements of
10.74% and 13.85% over state-of-the-art baselines. In false-positive control
evaluation at fixed FPR levels (0.0001, 0.001, 0.01), our approach delivers
4.82, 5.91, and 2.53 higher TPR respectively compared to the best-performing
baseline. These results highlight the effectiveness of LLM-based deobfuscation
and underscore the importance of modeling cluster-level relationships in
detecting malicious code.

</details>


### [34] [DoS Attacks and Defense Technologies in Blockchain Systems: A Hierarchical Analysis](https://arxiv.org/abs/2507.22611)
*Chunyi Zhang,Fengjiao Dou,Xiaoqi Li*

Main category: cs.CR

TL;DR: 论文分析了区块链技术中的DoS攻击，重点研究了合约层和共识层的攻击原理与方法，并比较了检测与防御技术，以提升区块链系统的安全性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 区块链技术的去中心化和安全性被广泛认可，但其安全局限性常被忽视，尤其是DoS攻击的威胁。本文旨在揭示这些攻击的原理，并提出防御方法。

Method: 基于区块链架构层次，分类整理现有DoS攻击，重点分析合约层和共识层攻击的原理与方法，并比较检测与防御技术。

Result: 论文系统性地总结了DoS攻击的特点与防御方法，为提升区块链系统的安全性和稳定性提供了理论支持。

Conclusion: 通过分析DoS攻击及防御技术，本文为区块链系统的安全性改进和创新应用提供了重要参考。

Abstract: Blockchain technology is widely used in various fields due to its ability to
provide decentralization and trustless security. This is a fundamental
understanding held by many advocates, but it is misunderstood, leading
participants to fail to recognize the limitations of the security that
blockchain can provide. Among all current network attacks, Denial of Service
(DoS) attacks pose significant threats due to their ease of execution and
destructive potential. This paper, based on the blockchain architecture
hierarchy, categorizes and organizes existing DoS attacks, with a focus on
explaining the principles and methods of contract layer and consensus layer DoS
attacks. Furthermore, this paper comprehensively analyzes and compares commonly
used detection methods and defense technologies, which will contribute to
strengthening the security and stability of blockchain systems and promoting
further innovation and application of blockchain systems.

</details>


### [35] [Hate in Plain Sight: On the Risks of Moderating AI-Generated Hateful Illusions](https://arxiv.org/abs/2507.22617)
*Yiting Qu,Ziqing Yang,Yihan Ma,Michael Backes,Savvas Zannettou,Yang Zhang*

Main category: cs.CR

TL;DR: 论文研究了基于文本到图像扩散模型生成的光学幻觉（含仇恨信息）对内容审核模型的挑战，发现现有模型检测效果较差，并提出了初步缓解措施。


<details>
  <summary>Details</summary>
Motivation: 探索光学幻觉技术被滥用于生成仇恨信息的风险，以及现有内容审核模型在此类内容上的漏洞。

Method: 使用Stable Diffusion和ControlNet生成1,860个光学幻觉（含62条仇恨信息），形成Hateful Illusion数据集，并评估6个审核分类器和9个视觉语言模型的检测性能。

Result: 现有审核模型检测仇恨幻觉的准确率低于0.245（分类器）和0.102（视觉语言模型），主要因视觉编码器忽略隐藏信息层。

Conclusion: 论文揭示了现有审核模型的局限性，并提出了图像变换和训练策略的初步缓解方案。

Abstract: Recent advances in text-to-image diffusion models have enabled the creation
of a new form of digital art: optical illusions--visual tricks that create
different perceptions of reality. However, adversaries may misuse such
techniques to generate hateful illusions, which embed specific hate messages
into harmless scenes and disseminate them across web communities. In this work,
we take the first step toward investigating the risks of scalable hateful
illusion generation and the potential for bypassing current content moderation
models. Specifically, we generate 1,860 optical illusions using Stable
Diffusion and ControlNet, conditioned on 62 hate messages. Of these, 1,571 are
hateful illusions that successfully embed hate messages, either overtly or
subtly, forming the Hateful Illusion dataset. Using this dataset, we evaluate
the performance of six moderation classifiers and nine vision language models
(VLMs) in identifying hateful illusions. Experimental results reveal
significant vulnerabilities in existing moderation models: the detection
accuracy falls below 0.245 for moderation classifiers and below 0.102 for VLMs.
We further identify a critical limitation in their vision encoders, which
mainly focus on surface-level image details while overlooking the secondary
layer of information, i.e., hidden messages. To address this risk, we explore
preliminary mitigation measures and identify the most effective approaches from
the perspectives of image transformations and training-level strategies.

</details>


### [36] [Cryptanalysis of LC-MUME: A Lightweight Certificateless Multi-User Matchmaking Encryption for Mobile Devices](https://arxiv.org/abs/2507.22674)
*Ramprasad Sarkar*

Main category: cs.CR

TL;DR: Yang等人提出了一种轻量级无证书多用户匹配加密方案（LC-MUME），但研究发现其无法满足EUF-CMA安全性要求，并提出改进策略。


<details>
  <summary>Details</summary>
Motivation: 为移动设备设计一种轻量级无证书多用户匹配加密方案，以减少计算和通信开销。

Method: 提出LC-MUME方案，并在随机预言模型下验证其安全性。

Result: 研究发现该方案存在安全漏洞，Type-I攻击者可伪造有效密文。

Conclusion: 需改进方案以增强移动计算环境中的匹配加密安全性。

Abstract: Yang et al. proposed a lightweight certificateless multiuser matchmaking
encryption (LC-MUME) scheme for mobile devices, published in IEEE Transactions
on Information Forensics and Security (TIFS) (DOI: 10.1109/TIFS.2023.3321961).
Their construction aims to reduce computational and communication overhead
within a one-to-many certificateless cryptographic framework. The authors claim
that their scheme satisfies existential unforgeability under chosen-message
attacks (EUF-CMA) in the random oracle model. However, our cryptanalytic study
demonstrates that the scheme fails to meet this critical security requirement.
In particular, we show that a Type-I adversary can successfully forge a valid
ciphertext without possessing the complete private key of the sender. Both
theoretical analysis and practical implementation confirm that this attack can
be mounted with minimal computational cost. To address these weaknesses, we
propose a modification strategy to strengthen the security of matchmaking
encryption schemes in mobile computing environments.

</details>


### [37] [Empirical Evaluation of Concept Drift in ML-Based Android Malware Detection](https://arxiv.org/abs/2507.22772)
*Ahmed Sabbah,Radi Jarrar,Samer Zein,David Mohaisen*

Main category: cs.CR

TL;DR: 该研究探讨了概念漂移对Android恶意软件检测的影响，发现其普遍存在且显著影响模型性能。不同特征类型、数据环境和检测方法对漂移有影响，但算法类型影响较小。LLMs表现良好但未能完全解决漂移问题。


<details>
  <summary>Details</summary>
Motivation: 由于Android恶意软件特征快速演变导致模型性能下降（概念漂移），研究旨在评估其对检测模型的影响。

Method: 使用两个数据集和九种机器学习、深度学习算法及LLMs，分析静态、动态、混合、语义和图像特征。

Result: 概念漂移普遍且显著影响性能，算法类型影响较小，LLMs表现良好但未完全解决问题。

Conclusion: 需进一步研究以应对概念漂移，LLMs潜力大但需优化。

Abstract: Despite outstanding results, machine learning-based Android malware detection
models struggle with concept drift, where rapidly evolving malware
characteristics degrade model effectiveness. This study examines the impact of
concept drift on Android malware detection, evaluating two datasets and nine
machine learning and deep learning algorithms, as well as Large Language Models
(LLMs). Various feature types--static, dynamic, hybrid, semantic, and
image-based--were considered. The results showed that concept drift is
widespread and significantly affects model performance. Factors influencing the
drift include feature types, data environments, and detection methods.
Balancing algorithms helped with class imbalance but did not fully address
concept drift, which primarily stems from the dynamic nature of the malware
landscape. No strong link was found between the type of algorithm used and
concept drift, the impact was relatively minor compared to other variables
since hyperparameters were not fine-tuned, and the default algorithm
configurations were used. While LLMs using few-shot learning demonstrated
promising detection performance, they did not fully mitigate concept drift,
highlighting the need for further investigation.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [38] [When Truthful Representations Flip Under Deceptive Instructions?](https://arxiv.org/abs/2507.22149)
*Xianxuan Long,Yao Fu,Runchao Li,Mu Sheng,Haotian Yu,Xiaotian Han,Pan Li*

Main category: cs.AI

TL;DR: 论文研究了大型语言模型（LLM）在欺骗性指令下内部表征的变化，发现欺骗性指令会导致显著的表示偏移，并识别了敏感特征和层级的欺骗特征。


<details>
  <summary>Details</summary>
Motivation: 理解欺骗性指令如何改变LLM的内部表征，以解决其安全性问题。

Method: 通过线性探测和稀疏自编码器（SAEs）分析Llama-3.1-8B-Instruct和Gemma-2-9B-Instruct在事实验证任务中的内部表征。

Result: 欺骗性指令导致早期到中间层的显著表示偏移，并识别出敏感特征和不同的真实/欺骗表示子空间。

Conclusion: 研究揭示了欺骗的特征和层级特征，为LLM的检测和控制提供了新见解。

Abstract: Large language models (LLMs) tend to follow maliciously crafted instructions
to generate deceptive responses, posing safety challenges. How deceptive
instructions alter the internal representations of LLM compared to truthful
ones remains poorly understood beyond output analysis. To bridge this gap, we
investigate when and how these representations ``flip'', such as from truthful
to deceptive, under deceptive versus truthful/neutral instructions. Analyzing
the internal representations of Llama-3.1-8B-Instruct and Gemma-2-9B-Instruct
on a factual verification task, we find the model's instructed True/False
output is predictable via linear probes across all conditions based on the
internal representation. Further, we use Sparse Autoencoders (SAEs) to show
that the Deceptive instructions induce significant representational shifts
compared to Truthful/Neutral representations (which are similar), concentrated
in early-to-mid layers and detectable even on complex datasets. We also
identify specific SAE features highly sensitive to deceptive instruction and
use targeted visualizations to confirm distinct truthful/deceptive
representational subspaces. % Our analysis pinpoints layer-wise and
feature-level correlates of instructed dishonesty, offering insights for LLM
detection and control. Our findings expose feature- and layer-level signatures
of deception, offering new insights for detecting and mitigating instructed
dishonesty in LLMs.

</details>


### [39] [Explainability Through Systematicity: The Hard Systematicity Challenge for Artificial Intelligence](https://arxiv.org/abs/2507.22197)
*Matthieu Queloz*

Main category: cs.AI

TL;DR: 论文探讨了AI的系统性不仅是可解释性的一部分，还涉及思想的连贯性、一致性和原则性，提出了一个区分系统性四种意义的框架，并探讨了AI是否需要满足这种系统性。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于澄清AI的系统性概念，超越传统的连接主义与系统性之间的对立，探讨AI是否需要满足更严格的系统性标准。

Method: 作者提出了一个区分系统性四种意义的框架，并分析了五种系统化的理由，将其应用于AI模型。

Result: 研究发现，系统性标准需要根据系统化的理由动态调整，提出了“硬系统性挑战”。

Conclusion: 结论指出，系统性需求应根据其理由动态调整，以明确AI模型需要达到的系统性程度和时机。

Abstract: This paper argues that explainability is only one facet of a broader ideal
that shapes our expectations towards artificial intelligence (AI).
Fundamentally, the issue is to what extent AI exhibits systematicity--not
merely in being sensitive to how thoughts are composed of recombinable
constituents, but in striving towards an integrated body of thought that is
consistent, coherent, comprehensive, and parsimoniously principled. This richer
conception of systematicity has been obscured by the long shadow of the
"systematicity challenge" to connectionism, according to which network
architectures are fundamentally at odds with what Fodor and colleagues termed
"the systematicity of thought." I offer a conceptual framework for thinking
about "the systematicity of thought" that distinguishes four senses of the
phrase. I use these distinctions to defuse the perceived tension between
systematicity and connectionism and show that the conception of systematicity
that historically shaped our sense of what makes thought rational,
authoritative, and scientific is more demanding than the Fodorian notion. To
determine whether we have reason to hold AI models to this ideal of
systematicity, I then argue, we must look to the rationales for systematization
and explore to what extent they transfer to AI models. I identify five such
rationales and apply them to AI. This brings into view the "hard systematicity
challenge." However, the demand for systematization itself needs to be
regulated by the rationales for systematization. This yields a dynamic
understanding of the need to systematize thought, which tells us how systematic
we need AI models to be and when.

</details>


### [40] [CoEx -- Co-evolving World-model and Exploration](https://arxiv.org/abs/2507.22281)
*Minsoo Kim,Seung-won Hwang*

Main category: cs.AI

TL;DR: 论文提出了一种分层代理架构CoEx，通过动态更新世界模型解决LLM代理规划中的静态模型问题。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理设计无法动态更新世界模型，导致规划与真实世界状态脱节。

Method: 引入分层状态抽象的CoEx架构，结合LLM推理和神经符号信念状态动态更新世界模型。

Result: 在ALFWorld、PDDL和Jericho等复杂任务中，CoEx优于现有代理范式。

Conclusion: CoEx通过动态世界模型和分层规划，显著提升了代理的规划与探索能力。

Abstract: Planning in modern LLM agents relies on the utilization of LLM as an internal
world model, acquired during pretraining. However, existing agent designs fail
to effectively assimilate new observations into dynamic updates of the world
model. This reliance on the LLM's static internal world model is progressively
prone to misalignment with the underlying true state of the world, leading to
the generation of divergent and erroneous plans. We introduce a hierarchical
agent architecture, CoEx, in which hierarchical state abstraction allows LLM
planning to co-evolve with a dynamically updated model of the world. CoEx plans
and interacts with the world by using LLM reasoning to orchestrate dynamic
plans consisting of subgoals, and its learning mechanism continuously
incorporates these subgoal experiences into a persistent world model in the
form of a neurosymbolic belief state, comprising textual inferences and
code-based symbolic memory. We evaluate our agent across a diverse set of agent
scenarios involving rich environments and complex tasks including ALFWorld,
PDDL, and Jericho. Our experiments show that CoEx outperforms existing agent
paradigms in planning and exploration.

</details>


### [41] [An Explainable Emotion Alignment Framework for LLM-Empowered Agent in Metaverse Service Ecosystem](https://arxiv.org/abs/2507.22326)
*Qun Ma,Xiao Xue,Ming Zhang,Yifan Shen,Zihan Zhao*

Main category: cs.AI

TL;DR: 本文提出了一种基于大语言模型（LLM）的元宇宙服务生态系统中可解释的情感对齐框架，旨在解决虚拟与现实服务之间的数据融合、知识关联和伦理安全问题。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的兴起，元宇宙服务生态系统中代理的角色日益重要，但现有代理在虚拟与现实服务融合方面面临挑战，如数据融合、知识关联和伦理安全。

Method: 提出了一种可解释的情感对齐框架，将事实因素融入LLM代理的决策循环，并通过离线到离线外卖场景的模拟实验验证其有效性。

Result: 实验结果表明，该框架能够实现更真实的社会涌现，提升代理在元宇宙服务中的表现。

Conclusion: 该框架为LLM代理在元宇宙服务生态系统中的应用提供了新的解决方案，特别是在虚拟与现实服务融合方面。

Abstract: Metaverse service is a product of the convergence between Metaverse and
service systems, designed to address service-related challenges concerning
digital avatars, digital twins, and digital natives within Metaverse. With the
rise of large language models (LLMs), agents now play a pivotal role in
Metaverse service ecosystem, serving dual functions: as digital avatars
representing users in the virtual realm and as service assistants (or NPCs)
providing personalized support. However, during the modeling of Metaverse
service ecosystems, existing LLM-based agents face significant challenges in
bridging virtual-world services with real-world services, particularly
regarding issues such as character data fusion, character knowledge
association, and ethical safety concerns. This paper proposes an explainable
emotion alignment framework for LLM-based agents in Metaverse Service
Ecosystem. It aims to integrate factual factors into the decision-making loop
of LLM-based agents, systematically demonstrating how to achieve more
relational fact alignment for these agents. Finally, a simulation experiment in
the Offline-to-Offline food delivery scenario is conducted to evaluate the
effectiveness of this framework, obtaining more realistic social emergence.

</details>


### [42] [Magentic-UI: Towards Human-in-the-loop Agentic Systems](https://arxiv.org/abs/2507.22358)
*Hussein Mozannar,Gagan Bansal,Cheng Tan,Adam Fourney,Victor Dibia,Jingya Chen,Jack Gerrits,Tyler Payne,Matheus Kunzler Maldaner,Madeleine Grunde-McLaughlin,Eric Zhu,Griffin Bassman,Jacob Alber,Peter Chang,Ricky Loynd,Friederike Niedtner,Ece Kamar,Maya Murad,Rafah Hosn,Saleema Amershi*

Main category: cs.AI

TL;DR: Magentic-UI是一个开源的人机交互界面，旨在通过人类监督提升AI代理的安全性和效率。


<details>
  <summary>Details</summary>
Motivation: 当前AI代理在多步任务中表现不足，且存在安全和风险问题，需要人类参与以提升性能和安全性。

Method: 开发了Magentic-UI，支持多工具扩展和六种交互机制，如共同规划和任务守卫。

Result: 在自主任务完成、用户测试和安全性评估中表现良好。

Conclusion: Magentic-UI为安全高效的人机协作提供了可行路径。

Abstract: AI agents powered by large language models are increasingly capable of
autonomously completing complex, multi-step tasks using external tools. Yet,
they still fall short of human-level performance in most domains including
computer use, software development, and research. Their growing autonomy and
ability to interact with the outside world, also introduces safety and security
risks including potentially misaligned actions and adversarial manipulation. We
argue that human-in-the-loop agentic systems offer a promising path forward,
combining human oversight and control with AI efficiency to unlock productivity
from imperfect systems. We introduce Magentic-UI, an open-source web interface
for developing and studying human-agent interaction. Built on a flexible
multi-agent architecture, Magentic-UI supports web browsing, code execution,
and file manipulation, and can be extended with diverse tools via Model Context
Protocol (MCP). Moreover, Magentic-UI presents six interaction mechanisms for
enabling effective, low-cost human involvement: co-planning, co-tasking,
multi-tasking, action guards, and long-term memory. We evaluate Magentic-UI
across four dimensions: autonomous task completion on agentic benchmarks,
simulated user testing of its interaction capabilities, qualitative studies
with real users, and targeted safety assessments. Our findings highlight
Magentic-UI's potential to advance safe and efficient human-agent
collaboration.

</details>


### [43] [LLM-Crowdsourced: A Benchmark-Free Paradigm for Mutual Evaluation of Large Language Models](https://arxiv.org/abs/2507.22359)
*Qianhong Guo,Wei Xie,Xiaofang Cai,Enze Wang,Shuoyoucheng Ma,Kai Chen,Xiaofeng Wang,Baosheng Wang*

Main category: cs.AI

TL;DR: 提出了一种名为LLM-Crowdsourced的无基准评估范式，通过LLM生成问题、独立回答和相互评估，解决了现有评估方法的数据污染、黑箱操作和主观偏好问题。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法存在数据污染、黑箱操作和主观偏好等问题，难以全面评估LLM的真实能力。

Method: 采用LLM-Crowdsourced方法，动态、透明、客观且专业地评估LLM，涵盖数学和编程任务。

Result: 实验验证了该方法在区分LLM性能上的优势，并发现了一些传统方法难以检测的新现象。

Conclusion: LLM-Crowdsourced方法在动态性、透明性、客观性和专业性上优于现有方法，为LLM评估提供了新思路。

Abstract: Although large language models (LLMs) demonstrate remarkable capabilities
across various tasks, evaluating their capabilities remains a challenging task.
Existing evaluation methods suffer from issues such as data contamination,
black-box operation, and subjective preference. These issues make it difficult
to evaluate the LLMs' true capabilities comprehensively. To tackle these
challenges, we propose a novel benchmark-free evaluation paradigm,
LLM-Crowdsourced. It utilizes LLMs to generate questions, answer independently,
and evaluate mutually. This method integrates four key evaluation criteria:
dynamic, transparent, objective, and professional, which existing evaluation
methods cannot satisfy simultaneously. Experiments on eight mainstream LLMs
across mathematics and programming verify the advantages of our method in
distinguishing LLM performance. Furthermore, our study reveals several novel
findings that are difficult for traditional methods to detect, including but
not limited to: (1) Gemini demonstrates the highest original and professional
question-design capabilities among others; (2) Some LLMs exhibit
''memorization-based answering'' by misrecognizing questions as familiar ones
with a similar structure; (3) LLM evaluation results demonstrate high
consistency (robustness).

</details>


### [44] [Beyond Accuracy: How AI Metacognitive Sensitivity improves AI-assisted Decision Making](https://arxiv.org/abs/2507.22365)
*ZhaoBin Li,Mark Steyvers*

Main category: cs.AI

TL;DR: AI的元认知敏感性（正确区分预测的能力）与预测准确性共同影响决策质量。研究发现，元认知敏感性高的AI即使预测准确性较低，也能提升人类决策表现。


<details>
  <summary>Details</summary>
Motivation: 探讨AI的预测准确性和元认知敏感性如何共同影响人类决策质量，强调元认知敏感性的重要性。

Method: 提出理论框架分析AI预测准确性和元认知敏感性的联合影响，并通过行为实验验证。

Result: 实验证实，元认知敏感性高的AI能显著提升人类决策表现。

Conclusion: 评估AI辅助时需同时考虑预测准确性和元认知敏感性，优化两者以实现更优决策结果。

Abstract: In settings where human decision-making relies on AI input, both the
predictive accuracy of the AI system and the reliability of its confidence
estimates influence decision quality. We highlight the role of AI metacognitive
sensitivity -- its ability to assign confidence scores that accurately
distinguish correct from incorrect predictions -- and introduce a theoretical
framework for assessing the joint impact of AI's predictive accuracy and
metacognitive sensitivity in hybrid decision-making settings. Our analysis
identifies conditions under which an AI with lower predictive accuracy but
higher metacognitive sensitivity can enhance the overall accuracy of human
decision making. Finally, a behavioral experiment confirms that greater AI
metacognitive sensitivity improves human decision performance. Together, these
findings underscore the importance of evaluating AI assistance not only by
accuracy but also by metacognitive sensitivity, and of optimizing both to
achieve superior decision outcomes.

</details>


### [45] [On the Definition of Intelligence](https://arxiv.org/abs/2507.22423)
*Kei-Sing Ng*

Main category: cs.AI

TL;DR: 论文提出了一种基于样本保真度的通用智能标准，认为智能是能够从同一类别中生成样本的能力，并形式化为ε-类别智能。


<details>
  <summary>Details</summary>
Motivation: 为了构建通用人工智能（AGI），需要一种能够评估智能本质的物种无关形式，同时涵盖多种智能行为范式。

Method: 提出ε-类别智能的形式化框架，定义智能为在给定类别样本下生成同类样本的能力，并通过可区分性容忍度ε进行评估。

Result: 论文展示了形式化框架、实验协议，并讨论了其对评估、安全性和泛化的影响。

Conclusion: 该标准为智能的评估和实现提供了一种通用且可操作的方法，对AGI的发展具有潜在意义。

Abstract: To engineer AGI, we should first capture the essence of intelligence in a
species-agnostic form that can be evaluated, while being sufficiently general
to encompass diverse paradigms of intelligent behavior, including reinforcement
learning, generative models, classification, analogical reasoning, and
goal-directed decision-making. We propose a general criterion based on sample
fidelity: intelligence is the ability, given sample(s) from a category, to
generate sample(s) from the same category. We formalise this intuition as
{\epsilon}-category intelligence: it is {\epsilon}-intelligent with respect to
a category if no chosen admissible distinguisher can separate generated from
original samples beyond tolerance {\epsilon}. We present the formal framework,
outline empirical protocols, and discuss implications for evaluation, safety,
and generalization.

</details>


### [46] [Cross-Border Legal Adaptation of Autonomous Vehicle Design based on Logic and Non-monotonic Reasoning](https://arxiv.org/abs/2507.22432)
*Zhe Yu,Yiwei Lu,Burkhard Schafer,Zhe Lin*

Main category: cs.AI

TL;DR: 论文探讨了跨国背景下自动驾驶车辆的法律合规挑战，从设计者视角提供法律推理支持。


<details>
  <summary>Details</summary>
Motivation: 解决自动驾驶车辆在跨国应用中的法律合规问题，帮助设计者更好地理解和适应不同法律环境。

Method: 基于论证理论，引入逻辑表示规范性推理的基本属性，并结合自然数的偏序集表达优先级。

Result: 通过法律文本的案例分析，展示了推理系统如何帮助设计者灵活调整设计方案并理解法律影响。

Conclusion: 提出的推理系统为自动驾驶车辆跨国设计提供了实用的法律支持工具。

Abstract: This paper focuses on the legal compliance challenges of autonomous vehicles
in a transnational context. We choose the perspective of designers and try to
provide supporting legal reasoning in the design process. Based on
argumentation theory, we introduce a logic to represent the basic properties of
argument-based practical (normative) reasoning, combined with partial order
sets of natural numbers to express priority. Finally, through case analysis of
legal texts, we show how the reasoning system we provide can help designers to
adapt their design solutions more flexibly in the cross-border application of
autonomous vehicles and to more easily understand the legal implications of
their decisions.

</details>


### [47] [Nearest-Better Network for Visualizing and Analyzing Combinatorial Optimization Problems: A Unified Tool](https://arxiv.org/abs/2507.22440)
*Yiya Diao,Changhe Li,Sanyou Zeng,Xinye Cai,Wenjian Luo,Shengxiang Yang,Carlos A. Coello Coello*

Main category: cs.AI

TL;DR: 本文改进了Nearest-Better Network（NBN）的计算方法，提出了一种高效的对数线性时间复杂度的算法，并应用于OneMax和TSP问题，揭示了新的景观特征和算法局限性。


<details>
  <summary>Details</summary>
Motivation: NBN在连续优化问题中表现优异，但计算耗时且难以扩展到组合优化问题。本文旨在解决这些问题，并探索NBN在算法行为分析中的应用。

Method: 通过理论推导证明NBN作为最大概率转移网络的功能，并提出一种高效的对数线性时间复杂度的NBN计算方法。

Result: 在OneMax问题中发现了中立性、崎岖性和模态性特征；在TSP问题中揭示了崎岖性、模态性和欺骗性为主要挑战，并指出EAX和LKH算法的局限性。

Conclusion: 高效的NBN算法为组合优化问题的景观分析提供了新工具，同时揭示了现有算法在模态性和欺骗性方面的不足。

Abstract: The Nearest-Better Network (NBN) is a powerful method to visualize sampled
data for continuous optimization problems while preserving multiple landscape
features. However, the calculation of NBN is very time-consuming, and the
extension of the method to combinatorial optimization problems is challenging
but very important for analyzing the algorithm's behavior. This paper provides
a straightforward theoretical derivation showing that the NBN network
essentially functions as the maximum probability transition network for
algorithms. This paper also presents an efficient NBN computation method with
logarithmic linear time complexity to address the time-consuming issue. By
applying this efficient NBN algorithm to the OneMax problem and the Traveling
Salesman Problem (TSP), we have made several remarkable discoveries for the
first time: The fitness landscape of OneMax exhibits neutrality, ruggedness,
and modality features. The primary challenges of TSP problems are ruggedness,
modality, and deception. Two state-of-the-art TSP algorithms (i.e., EAX and
LKH) have limitations when addressing challenges related to modality and
deception, respectively. LKH, based on local search operators, fails when there
are deceptive solutions near global optima. EAX, which is based on a single
population, can efficiently maintain diversity. However, when multiple
attraction basins exist, EAX retains individuals within multiple basins
simultaneously, reducing inter-basin interaction efficiency and leading to
algorithm's stagnation.

</details>


### [48] [Collaborative Medical Triage under Uncertainty: A Multi-Agent Dynamic Matching Approach](https://arxiv.org/abs/2507.22504)
*Hongyan Cheng,Chengzhang Yu,Yanshu Shi,Chiyue Wang,Cong Liu,Zhanpeng Jin*

Main category: cs.AI

TL;DR: 提出一种多智能体交互式医疗分诊系统，解决当前AI分诊系统的三大挑战：医疗专业性不足、机构部门结构异质性和低效提问。系统通过三个专用智能体协作，实现高准确率的分诊推荐。


<details>
  <summary>Details</summary>
Motivation: 疫情后医疗需求激增与护理人员短缺，急诊分诊系统压力巨大，亟需创新的AI解决方案。

Method: 采用三个专用智能体（RecipientAgent、InquirerAgent、DepartmentAgent）通过结构化提问和部门特定规则协作，将非结构化症状转化为准确分诊建议。

Result: 实验显示，系统在四次交互后，一级科室分类准确率达89.2%，二级科室达73.9%。

Conclusion: 该系统为部署适应医疗机构异质性的AI分诊提供了可扩展框架，确保临床决策的准确性。

Abstract: The post-pandemic surge in healthcare demand, coupled with critical nursing
shortages, has placed unprecedented pressure on emergency department triage
systems, necessitating innovative AI-driven solutions. We present a multi-agent
interactive intelligent system for medical triage that addresses three
fundamental challenges in current AI-based triage systems: insufficient medical
specialization leading to hallucination-induced misclassifications,
heterogeneous department structures across healthcare institutions, and
inefficient detail-oriented questioning that impedes rapid triage decisions.
Our system employs three specialized agents - RecipientAgent, InquirerAgent,
and DepartmentAgent - that collaborate through structured inquiry mechanisms
and department-specific guidance rules to transform unstructured patient
symptoms into accurate department recommendations. To ensure robust evaluation,
we constructed a comprehensive Chinese medical triage dataset from a medical
website, comprising 3,360 real-world cases spanning 9 primary departments and
62 secondary departments. Through systematic data imputation using large
language models, we address the prevalent issue of incomplete medical records
in real-world data. Experimental results demonstrate that our multi-agent
system achieves 89.2% accuracy in primary department classification and 73.9%
accuracy in secondary department classification after four rounds of patient
interaction. The system's pattern-matching-based guidance mechanisms enable
efficient adaptation to diverse hospital configurations while maintaining high
triage accuracy. Our work provides a scalable framework for deploying
AI-assisted triage systems that can accommodate the organizational
heterogeneity of healthcare institutions while ensuring clinically sound
decision-making.

</details>


### [49] [MetaAgent: Automatically Constructing Multi-Agent Systems Based on Finite State Machines](https://arxiv.org/abs/2507.22606)
*Yaolun Zhang,Xiaogeng Liu,Chaowei Xiao*

Main category: cs.AI

TL;DR: MetaAgent是一个基于有限状态机的框架，能够自动生成多智能体系统，并通过优化算法改进系统性能。


<details>
  <summary>Details</summary>
Motivation: 现有的人工设计多智能体框架局限于预定义场景，而自动化设计方法存在工具集成不足、依赖外部数据等问题。

Method: 提出MetaAgent框架，利用有限状态机控制智能体行为和状态转换，并通过优化算法优化系统。

Result: 实验表明，生成的多智能体系统优于其他自动化设计方法，性能接近人工优化系统。

Conclusion: MetaAgent为多智能体系统的自动化设计提供了高效且灵活的解决方案。

Abstract: Large Language Models (LLMs) have demonstrated the ability to solve a wide
range of practical tasks within multi-agent systems. However, existing
human-designed multi-agent frameworks are typically limited to a small set of
pre-defined scenarios, while current automated design methods suffer from
several limitations, such as the lack of tool integration, dependence on
external training data, and rigid communication structures. In this paper, we
propose MetaAgent, a finite state machine based framework that can
automatically generate a multi-agent system. Given a task description,
MetaAgent will design a multi-agent system and polish it through an
optimization algorithm. When the multi-agent system is deployed, the finite
state machine will control the agent's actions and the state transitions. To
evaluate our framework, we conduct experiments on both text-based tasks and
practical tasks. The results indicate that the generated multi-agent system
surpasses other auto-designed methods and can achieve a comparable performance
with the human-designed multi-agent system, which is optimized for those
specific tasks.

</details>


### [50] [Enhancing Manufacturing Knowledge Access with LLMs and Context-aware Prompting](https://arxiv.org/abs/2507.22619)
*Sebastian Monka,Irlan Grangel-González,Stefan Schmid,Lavdim Halilaj,Marc Rickart,Oliver Rudolph,Rui Dias*

Main category: cs.AI

TL;DR: 论文探讨了利用大型语言模型（LLMs）将自然语言查询转换为SPARQL查询，以简化知识图谱（KGs）在制造业中的使用。通过提供KG的上下文信息，LLMs能显著提高查询生成的准确性。


<details>
  <summary>Details</summary>
Motivation: 制造业中知识图谱的使用对非专家用户存在门槛，需要复杂的SPARQL查询。LLMs的潜力在于通过自然语言接口降低这一门槛，但需解决如何有效提供KG上下文的问题。

Method: 研究评估了多种策略，通过LLMs作为中介，将自然语言查询转换为SPARQL查询，重点关注制造业中的Bosch Line Information System KG和I40 Core Information Model。

Result: 实验表明，提供KG的适当上下文能显著提升LLMs生成正确且完整查询的能力，减少幻觉风险。

Conclusion: 上下文感知提示技术有助于LLMs更准确地生成查询，有望推动复杂数据存储库的普及，支持制造业的决策制定。

Abstract: Knowledge graphs (KGs) have transformed data management within the
manufacturing industry, offering effective means for integrating disparate data
sources through shared and structured conceptual schemas. However, harnessing
the power of KGs can be daunting for non-experts, as it often requires
formulating complex SPARQL queries to retrieve specific information. With the
advent of Large Language Models (LLMs), there is a growing potential to
automatically translate natural language queries into the SPARQL format, thus
bridging the gap between user-friendly interfaces and the sophisticated
architecture of KGs. The challenge remains in adequately informing LLMs about
the relevant context and structure of domain-specific KGs, e.g., in
manufacturing, to improve the accuracy of generated queries. In this paper, we
evaluate multiple strategies that use LLMs as mediators to facilitate
information retrieval from KGs. We focus on the manufacturing domain,
particularly on the Bosch Line Information System KG and the I40 Core
Information Model. In our evaluation, we compare various approaches for feeding
relevant context from the KG to the LLM and analyze their proficiency in
transforming real-world questions into SPARQL queries. Our findings show that
LLMs can significantly improve their performance on generating correct and
complete queries when provided only the adequate context of the KG schema. Such
context-aware prompting techniques help LLMs to focus on the relevant parts of
the ontology and reduce the risk of hallucination. We anticipate that the
proposed techniques help LLMs to democratize access to complex data
repositories and empower informed decision-making in manufacturing settings.

</details>


### [51] [ASP-FZN: A Translation-based Constraint Answer Set Solver](https://arxiv.org/abs/2507.22774)
*Thomas Eiter,Tobias Geibinger,Tobias Kaminski,Nysret Musliu,Johannes Oetsch*

Main category: cs.AI

TL;DR: asp-fzn是一个用于约束答案集编程（CASP）的求解器，通过将CASP程序转换为FlatZinc语言，支持多种后端求解器。它在ASP和CASP基准测试中表现出色，甚至在某些情况下优于clingcon。


<details>
  <summary>Details</summary>
Motivation: 扩展ASP以支持线性约束，提供更丰富的语言和更高的求解效率。

Method: 将CASP程序翻译为FlatZinc语言，利用多种后端求解器处理约束。

Result: asp-fzn在ASP和CASP基准测试中表现优异，部分情况下优于clingcon。

Conclusion: asp-fzn是一个有前景的CASP求解器，尤其在处理线性约束时表现突出。

Abstract: We present the solver asp-fzn for Constraint Answer Set Programming (CASP),
which extends ASP with linear constraints. Our approach is based on translating
CASP programs into the solver-independent FlatZinc language that supports
several Constraint Programming and Integer Programming backend solvers. Our
solver supports a rich language of linear constraints, including some common
global constraints. As for evaluation, we show that asp-fzn is competitive with
state-of-the-art ASP solvers on benchmarks taken from past ASP competitions.
Furthermore, we evaluate it on several CASP problems from the literature and
compare its performance with clingcon, which is a prominent CASP solver that
supports most of the asp-fzn language. The performance of asp-fzn is very
promising as it is already competitive on plain ASP and even outperforms
clingcon on some CASP benchmarks.

</details>


### [52] [Enhancing Multi-Agent Collaboration with Attention-Based Actor-Critic Policies](https://arxiv.org/abs/2507.22782)
*Hugo Garrido-Lestache,Jeremy Kedziora*

Main category: cs.AI

TL;DR: TAAC是一种强化学习算法，通过多头注意力机制和集中训练/执行方案提升多智能体协作性能。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体协作中联合动作空间指数增长和动态通信的挑战。

Method: 采用集中训练/执行方案，结合多头注意力机制和惩罚损失函数。

Result: 在模拟足球环境中表现优于基准算法，协作行为显著提升。

Conclusion: TAAC在多智能体协作任务中表现出色，具有实际应用潜力。

Abstract: This paper introduces Team-Attention-Actor-Critic (TAAC), a reinforcement
learning algorithm designed to enhance multi-agent collaboration in cooperative
environments. TAAC employs a Centralized Training/Centralized Execution scheme
incorporating multi-headed attention mechanisms in both the actor and critic.
This design facilitates dynamic, inter-agent communication, allowing agents to
explicitly query teammates, thereby efficiently managing the exponential growth
of joint-action spaces while ensuring a high degree of collaboration. We
further introduce a penalized loss function which promotes diverse yet
complementary roles among agents. We evaluate TAAC in a simulated soccer
environment against benchmark algorithms representing other multi-agent
paradigms, including Proximal Policy Optimization and Multi-Agent
Actor-Attention-Critic. We find that TAAC exhibits superior performance and
enhanced collaborative behaviors across a variety of metrics (win rates, goal
differentials, Elo ratings, inter-agent connectivity, balanced spatial
distributions, and frequent tactical interactions such as ball possession
swaps).

</details>


### [53] [The Incomplete Bridge: How AI Research (Mis)Engages with Psychology](https://arxiv.org/abs/2507.22847)
*Han Jiang,Pengda Wang,Xiaoyuan Yi,Xing Xie,Ziang Xiao*

Main category: cs.AI

TL;DR: 该研究分析了2023-2025年间发表的1006篇AI论文及其引用的2544篇心理学文献，探讨了AI与心理学的跨学科整合模式，并提出了改进建议。


<details>
  <summary>Details</summary>
Motivation: 探索AI与心理学之间的跨学科协同效应，以促进更深入的合作并推动AI系统的发展。

Method: 通过分析AI论文及其引用的心理学文献，识别整合模式、高频引用领域及未充分探索的方向。

Result: 揭示了心理学在AI中的应用模式，指出了常见的误用类型，并提供了改进建议。

Conclusion: 研究为AI与心理学的跨学科合作提供了全面指导，有助于推动AI系统的进步。

Abstract: Social sciences have accumulated a rich body of theories and methodologies
for investigating the human mind and behaviors, while offering valuable
insights into the design and understanding of Artificial Intelligence (AI)
systems. Focusing on psychology as a prominent case, this study explores the
interdisciplinary synergy between AI and the field by analyzing 1,006
LLM-related papers published in premier AI venues between 2023 and 2025, along
with the 2,544 psychology publications they cite. Through our analysis, we
identify key patterns of interdisciplinary integration, locate the psychology
domains most frequently referenced, and highlight areas that remain
underexplored. We further examine how psychology theories/frameworks are
operationalized and interpreted, identify common types of misapplication, and
offer guidance for more effective incorporation. Our work provides a
comprehensive map of interdisciplinary engagement between AI and psychology,
thereby facilitating deeper collaboration and advancing AI systems.

</details>


### [54] [Automatically discovering heuristics in a complex SAT solver with large language models](https://arxiv.org/abs/2507.22876)
*Yiwen Sun,Furong Ye,Zhihan Chen,Ke Wei,Shaowei Cai*

Main category: cs.AI

TL;DR: AutoModSAT利用大型语言模型（LLM）优化SAT求解器，通过模块化设计、自动提示优化和高效搜索策略，性能提升50%，优于现有最优求解器30%，速度提升20%。


<details>
  <summary>Details</summary>
Motivation: 现代SAT求解器架构复杂，传统自动配置框架性能提升有限，需探索AI驱动的新方法。

Method: 提出模块化求解器设计、自动提示优化和进化算法搜索策略。

Result: AutoModSAT性能提升50%，优于SOTA求解器30%，速度提升20%。

Conclusion: 该研究为AI驱动的启发式发现与复杂系统优化提供了方法论和实证结果。

Abstract: Satisfiability problem (SAT) is a cornerstone of computational complexity
with broad industrial applications, and it remains challenging to optimize
modern SAT solvers in real-world settings due to their intricate architectures.
While automatic configuration frameworks have been developed, they rely on
manually constrained search spaces and yield limited performance gains. This
work introduces a novel paradigm which effectively optimizes complex SAT
solvers via Large Language Models (LLMs), and a tool called AutoModSAT is
developed. Three fundamental challenges are addressed in order to achieve
superior performance: (1) LLM-friendly solver: Systematic guidelines are
proposed for developing a modularized solver to meet LLMs' compatibility,
emphasizing code simplification, information share and bug reduction; (2)
Automatic prompt optimization: An unsupervised automatic prompt optimization
method is introduced to advance the diversity of LLMs' output; (3) Efficient
search strategy: We design a presearch strategy and an EA evolutionary
algorithm for the final efficient and effective discovery of heuristics.
Extensive experiments across a wide range of datasets demonstrate that
AutoModSAT achieves 50% performance improvement over the baseline solver and
achieves 30% superiority against the state-of-the-art (SOTA) solvers. Moreover,
AutoModSAT attains a 20% speedup on average compared to parameter-tuned
alternatives of the SOTA solvers, showcasing the enhanced capability in
handling complex problem instances. This work bridges the gap between AI-driven
heuristics discovery and mission-critical system optimization, and provides
both methodological advancements and empirically validated results for
next-generation complex solver development.

</details>
