<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 23]
- [cs.CR](#cs.CR) [Total: 59]
- [cs.AI](#cs.AI) [Total: 48]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Adversarial Bug Reports as a Security Risk in Language Model-Based Automated Program Repair](https://arxiv.org/abs/2509.05372)
*Piotr Przymus,Andreas Happe,Jürgen Cito*

Main category: cs.SE

TL;DR: 研究发现LLM自动程序修复系统存在安全漏洞，攻击者可通过精心构造的bug报告诱导系统生成不安全的代码补丁，现有防御措施效果有限。


<details>
  <summary>Details</summary>
Motivation: 随着基于LLM的自动程序修复系统在软件开发中的广泛应用，依赖不可信用户输入带来了新的攻击面，需要研究对抗性bug报告对系统安全性的威胁。

Method: 建立全面的威胁模型，通过手工制作和自动化流水线生成51个对抗性bug报告，测试主流APR系统的脆弱性，评估预处理防御和后处理检测机制的效果。

Result: 当前防御措施不足：90%的构造bug报告成功触发攻击者期望的补丁，最佳预处理过滤器仅阻挡47%，后处理分析在58%的情况下有效但需要人工监督。

Conclusion: 生成对抗性输入成本低廉而检测成本高昂，存在结构性不对称，需要改进APR系统的鲁棒性并推动可信自动修复的未来研究方向。

Abstract: Large Language Model (LLM) - based Automated Program Repair (APR) systems are
increasingly integrated into modern software development workflows, offering
automated patches in response to natural language bug reports. However, this
reliance on untrusted user input introduces a novel and underexplored attack
surface. In this paper, we investigate the security risks posed by adversarial
bug reports -- realistic-looking issue submissions crafted to mislead APR
systems into producing insecure or harmful code changes. We develop a
comprehensive threat model and conduct an empirical study to evaluate the
vulnerability of state-of-the-art APR systems to such attacks. Our
demonstration comprises 51 adversarial bug reports generated across a spectrum
of strategies, from manual curation to fully automated pipelines. We test these
against leading APR model and assess both pre-repair defenses (e.g., LlamaGuard
variants, PromptGuard variants, Granite-Guardian, and custom LLM filters) and
post-repair detectors (GitHub Copilot, CodeQL). Our findings show that current
defenses are insufficient: 90\% of crafted bug reports triggered
attacker-aligned patches. The best pre-repair filter blocked only 47\%, while
post-repair analysis-often requiring human oversight-was effective in just 58\%
of cases. To support scalable security testing, we introduce a prototype
framework for automating the generation of adversarial bug reports. Our
analysis exposes a structural asymmetry: generating adversarial inputs is
inexpensive, while detecting or mitigating them remains costly and error-prone.
We conclude with practical recommendations for improving the robustness of APR
systems against adversarial misuse and highlight directions for future work on
trustworthy automated repair.

</details>


### [2] [Reverse Browser: Vector-Image-to-Code Generator](https://arxiv.org/abs/2509.05394)
*Zoltan Toth-Czifra*

Main category: cs.SE

TL;DR: 使用矢量图像而非位图作为输入，通过创建大规模数据集和新的多尺度质量评估指标，训练了一个大型开放权重模型来解决图像到代码转换的保真度问题


<details>
  <summary>Details</summary>
Motivation: 当前最先进的图像到代码转换解决方案在保真度方面表现不佳，无法准确还原原始设计

Method: 采用矢量图像作为模型输入，创建多个大型数据集，评估现有图像质量评估算法并引入新的多尺度指标，训练大型开放权重模型

Result: 开发了一个新的图像到代码转换方法，但模型仍存在一些局限性

Conclusion: 使用矢量图像和新的质量评估指标可以改善图像到代码转换的保真度问题，但该方法仍有待进一步完善

Abstract: Automating the conversion of user interface design into code (image-to-code
or image-to-UI) is an active area of software engineering research. However,
the state-of-the-art solutions do not achieve high fidelity to the original
design, as evidenced by benchmarks. In this work, I approach the problem
differently: I use vector images instead of bitmaps as model input. I create
several large datasets for training machine learning models. I evaluate the
available array of Image Quality Assessment (IQA) algorithms and introduce a
new, multi-scale metric. I then train a large open-weights model and discuss
its limitations.

</details>


### [3] [Combining TSL and LLM to Automate REST API Testing: A Comparative Study](https://arxiv.org/abs/2509.05540)
*Thiago Barradas,Aline Paes,Vânia de Oliveira Neves*

Main category: cs.SE

TL;DR: RestTSLLM使用测试规范语言和大语言模型自动生成REST API测试用例，解决了测试场景创建和输入数据定义的核心挑战。评估显示Claude 3.5 Sonnet在各项指标上表现最佳。


<details>
  <summary>Details</summary>
Motivation: REST API测试执行面临分布式系统复杂性、多场景可能性和有限测试设计时间的挑战，传统方法无法穷尽所有输入组合，导致未检测故障、高人工成本和有限测试覆盖率。

Method: 结合测试规范语言和大语言模型，通过提示工程技术和自动化流水线，从OpenAPI规范生成测试用例，评估不同LLM在成功率、测试覆盖率和变异分数等指标上的表现。

Result: 表现最佳的LLM包括Claude 3.5 Sonnet、Deepseek R1、Qwen 2.5 32b和Sabia 3，其中Claude 3.5 Sonnet在所有指标上都优于其他模型。

Conclusion: LLM在基于API规范自动生成测试方面具有巨大潜力，Claude 3.5 Sonnet被证明是最适合此任务的模型。

Abstract: The effective execution of tests for REST APIs remains a considerable
challenge for development teams, driven by the inherent complexity of
distributed systems, the multitude of possible scenarios, and the limited time
available for test design. Exhaustive testing of all input combinations is
impractical, often resulting in undetected failures, high manual effort, and
limited test coverage. To address these issues, we introduce RestTSLLM, an
approach that uses Test Specification Language (TSL) in conjunction with Large
Language Models (LLMs) to automate the generation of test cases for REST APIs.
The approach targets two core challenges: the creation of test scenarios and
the definition of appropriate input data. The proposed solution integrates
prompt engineering techniques with an automated pipeline to evaluate various
LLMs on their ability to generate tests from OpenAPI specifications. The
evaluation focused on metrics such as success rate, test coverage, and mutation
score, enabling a systematic comparison of model performance. The results
indicate that the best-performing LLMs - Claude 3.5 Sonnet (Anthropic),
Deepseek R1 (Deepseek), Qwen 2.5 32b (Alibaba), and Sabia 3 (Maritaca) -
consistently produced robust and contextually coherent REST API tests. Among
them, Claude 3.5 Sonnet outperformed all other models across every metric,
emerging in this study as the most suitable model for this task. These findings
highlight the potential of LLMs to automate the generation of tests based on
API specifications.

</details>


### [4] [Natural Language-Programming Language Software Traceability Link Recovery Needs More than Textual Similarity](https://arxiv.org/abs/2509.05585)
*Zhiyuan Zou,Bangchao Wang,Peng Liang,Tingting Bi,Huan Jin*

Main category: cs.SE

TL;DR: 本文提出了一种多策略集成方法，通过在HGT和Gemini 2.5 Pro模型中融入领域特定的辅助策略，显著提升了自然语言与编程语言(NL-PL)软件追踪链接恢复任务的性能。


<details>
  <summary>Details</summary>
Motivation: 传统基于文本相似性的追踪链接恢复方法在NL-PL场景中存在语义鸿沟问题，无法有效处理自然语言与编程语言之间的语义差异。

Method: 通过大规模实证分析识别多种领域特定辅助策略，并将其集成到两种模型中：通过边类型集成到异构图变换器(HGT)，以及通过额外输入信息集成到基于提示的Gemini 2.5 Pro模型。

Result: 实验结果表明，多策略HGT和Gemini 2.5 Pro模型均优于原始版本，相比当前最先进的HGNNLink方法，在12个开源项目中分别实现了3.68%和8.84%的平均F1分数提升。

Conclusion: 多策略集成方法能有效提升NL-PL追踪链接恢复任务的整体模型性能，为解决文本相似性方法的局限性提供了有效解决方案。

Abstract: In the field of software traceability link recovery (TLR), textual similarity
has long been regarded as the core criterion. However, in tasks involving
natural language and programming language (NL-PL) artifacts, relying solely on
textual similarity is limited by their semantic gap. To this end, we conducted
a large-scale empirical evaluation across various types of TLR tasks, revealing
the limitations of textual similarity in NL-PL scenarios. To address these
limitations, we propose an approach that incorporates multiple domain-specific
auxiliary strategies, identified through empirical analysis, into two models:
the Heterogeneous Graph Transformer (HGT) via edge types and the prompt-based
Gemini 2.5 Pro via additional input information. We then evaluated our approach
using the widely studied requirements-to-code TLR task, a representative case
of NL-PL TLR. Experimental results show that both the multi-strategy HGT and
Gemini 2.5 Pro models outperformed their original counterparts without strategy
integration. Furthermore, compared to the current state-of-the-art method
HGNNLink, the multi-strategy HGT and Gemini 2.5 Pro models achieved average
F1-score improvements of 3.68% and 8.84%, respectively, across twelve
open-source projects, demonstrating the effectiveness of multi-strategy
integration in enhancing overall model performance for the requirements-code
TLR task.

</details>


### [5] [Verifying Correctness of PLC Software during System Evolution using Model Containment Approach](https://arxiv.org/abs/2509.05596)
*Soumyadip Bandyopadhyay,Santonu Sarkar*

Main category: cs.SE

TL;DR: 通过将PLC程序的旧版和新版SFC转换为Petri网模型，使用符号路径等价基础的含盖检查算法验证升级正确性，在实际案例中实现4倍性能提升


<details>
  <summary>Details</summary>
Motivation: PLC软件升级是工业领域常见需求，但验证升级版本的正确性仍面临重大挑战

Method: 将旧版和新版SFC转换为Petri网模型，使用基于符号路径等价的新题含盖检查算法验证模型间的含盖关系

Result: 在80个OSCAT库实际案例中验证了框架的可扩展性和有效性，与verifAPS工具相比实现了近4倍的性能提升

Conclusion: 该方法能够高效地验证PLC软件升级的正确性，为工业自动化系统提供了可靠的验证方案

Abstract: Upgradation of Programmable Logic Controller (PLC) software is quite common
to accommodate evolving industrial requirements. Verifying the correctness of
such upgrades remains a significant challenge. In this paper, we propose a
verification-based approach to ensure the correctness of the existing
functionality in the upgraded version of a PLC software. The method converts
the older and the newer versions of the sequential function chart (SFC) into
two Petri net models. We then verify whether one model is contained within
another, based on a novel containment checking algorithm grounded in symbolic
path equivalence. For this purpose, we have developed a home-grown Petri
net-based containment checker. Experimental evaluation on 80 real-world
benchmarks from the OSCAT library highlights the scalability and effectiveness
of the framework. We have compared our approach with verifAPS, a popular tool
used for software upgradation, and observed nearly 4x performance improvement.

</details>


### [6] [Automating API Documentation with LLMs: A BERTopic Approach](https://arxiv.org/abs/2509.05749)
*AmirHossein Naghshzan*

Main category: cs.SE

TL;DR: 自动化摘要社区论坛内容为Android API提供简洁的实用摘要，包括代码片段，通过用户研究验证了其效果


<details>
  <summary>Details</summary>
Motivation: 官方API文档经常长缘、复杂或不完整，开发者需要从社区论坛获取实用见解

Method: 使用BERTopic从360万个Stack Overflow帖子中提取主题，应用摘要技术生成简洁摘要，包括代码片段

Result: 通过30名Android开发者的用户研究，评估摘要的连贯性、相关性、信息量和满意度，显示了生产力提升

Conclusion: 结合形式API知识与社区生成内容可以改善文档质量，使API资源更易获取和实用

Abstract: Developers rely on API documentation, but official sources are often lengthy,
complex, or incomplete. Many turn to community-driven forums like Stack
Overflow for practical insights. We propose automating the summarization of
informal sources, focusing on Android APIs. Using BERTopic, we extracted
prevalent topics from 3.6 million Stack Overflow posts and applied extractive
summarization techniques to generate concise summaries, including code
snippets. A user study with 30 Android developers assessed the summaries for
coherence, relevance, informativeness, and satisfaction, showing improved
productivity. Integrating formal API knowledge with community-generated content
enhances documentation, making API resources more accessible and actionable
work.

</details>


### [7] [IoT Miner: Intelligent Extraction of Event Logs from Sensor Data for Process Mining](https://arxiv.org/abs/2509.05769)
*Edyta Brzychczy,Urszula Jessen,Krzysztof Kluza,Sridhar Sriram,Manuel Vargas Nettelnstroth*

Main category: cs.SE

TL;DR: IoT Miner是一个从工业传感器数据自动生成高级事件日志的框架，通过数据预处理、无监督聚类、LLM标注和事件日志构建四阶段流程，解决了传统过程挖掘中缺乏结构化事件日志的问题。


<details>
  <summary>Details</summary>
Motivation: 在采矿、制造等工业场景中，标准事件日志通常不可用，原始传感器数据缺乏结构化和语义信息，无法直接用于过程挖掘分析。

Method: 采用四阶段流水线：数据预处理、无监督聚类、基于大语言模型（LLM）的标注（使用领域特定提示生成有意义的标签）、事件日志构建。关键创新是利用LLM根据聚类统计信息生成活动标签。

Result: 在装载-运输-卸载（LHD）采矿机械传感器数据上评估，提出了相似性加权准确度新指标来衡量标注质量。结果显示更丰富的提示词能产生更准确一致的标签。

Conclusion: IoT Miner通过结合AI和领域感知数据处理，提供了一种可扩展且可解释的方法，能够在传统日志缺失的场景下从IoT数据生成事件日志，支持过程挖掘应用。

Abstract: This paper presents IoT Miner, a novel framework for automatically creating
high-level event logs from raw industrial sensor data to support process
mining. In many real-world settings, such as mining or manufacturing, standard
event logs are unavailable, and sensor data lacks the structure and semantics
needed for analysis. IoT Miner addresses this gap using a four-stage pipeline:
data preprocessing, unsupervised clustering, large language model (LLM)-based
labeling, and event log construction. A key innovation is the use of LLMs to
generate meaningful activity labels from cluster statistics, guided by
domain-specific prompts. We evaluate the approach on sensor data from a
Load-Haul-Dump (LHD) mining machine and introduce a new metric,
Similarity-Weighted Accuracy, to assess labeling quality. Results show that
richer prompts lead to more accurate and consistent labels. By combining AI
with domain-aware data processing, IoT Miner offers a scalable and
interpretable method for generating event logs from IoT data, enabling process
mining in settings where traditional logs are missing.

</details>


### [8] [GeoAnalystBench: A GeoAI benchmark for assessing large language models for spatial analysis workflow and code generation](https://arxiv.org/abs/2509.05881)
*Qianheng Zhang,Song Gao,Chen Wei,Yibo Zhao,Ying Nie,Ziru Chen,Shijie Chen,Yu Su,Huan Sun*

Main category: cs.SE

TL;DR: 提出了GeoAnalystBench基准测试，评估大语言模型在50个真实地理空间处理任务上的表现，发现专有模型表现优于开源模型，但空间推理任务仍是所有模型的挑战


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在地理空间分析中的应用日益增多，需要对其实际能力进行严格评估，而不是过早宣称能够完全自动化GIS工作流

Method: 创建包含50个Python任务的基准测试GeoAnalystBench，每个任务都有最小可交付产品，从工作流有效性、结构对齐、语义相似性和代码质量(CodeBLEU)四个维度进行评估

Result: 专有模型如ChatGPT-4o-mini表现优异(95%有效性，CodeBLEU 0.39)，而开源模型如DeepSeek-R1-7B表现较差(48.5%有效性，CodeBLEU 0.272)。空间关系检测和最优选址等需要深度空间推理的任务对所有模型都最具挑战性

Conclusion: 当前大语言模型在GIS自动化方面既有潜力也有局限性，需要建立可复现的评估框架，采用人机协同的方式推进GeoAI研究

Abstract: Recent advances in large language models (LLMs) have fueled growing interest
in automating geospatial analysis and GIS workflows, yet their actual
capabilities remain uncertain. In this work, we call for rigorous evaluation of
LLMs on well-defined geoprocessing tasks before making claims about full GIS
automation. To this end, we present GeoAnalystBench, a benchmark of 50
Python-based tasks derived from real-world geospatial problems and carefully
validated by GIS experts. Each task is paired with a minimum deliverable
product, and evaluation covers workflow validity, structural alignment,
semantic similarity, and code quality (CodeBLEU). Using this benchmark, we
assess both proprietary and open source models. Results reveal a clear gap:
proprietary models such as ChatGPT-4o-mini achieve high validity 95% and
stronger code alignment (CodeBLEU 0.39), while smaller open source models like
DeepSeek-R1-7B often generate incomplete or inconsistent workflows (48.5%
validity, 0.272 CodeBLEU). Tasks requiring deeper spatial reasoning, such as
spatial relationship detection or optimal site selection, remain the most
challenging across all models. These findings demonstrate both the promise and
limitations of current LLMs in GIS automation and provide a reproducible
framework to advance GeoAI research with human-in-the-loop support.

</details>


### [9] [Code2MCP: A Multi-Agent Framework for Automated Transformation of Code Repositories into Model Context Protocol Services](https://arxiv.org/abs/2509.05941)
*Chaoqian Ouyang,Ling Yue,Shimin Di,Libin Zheng,Shaowu Pan,Min-Ling Zhang*

Main category: cs.SE

TL;DR: Code2MCP是一个自动化框架，能够将GitHub仓库转换为MCP兼容服务，解决LLM集成中的N×M问题


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型与工具集成时的N×M碎片化问题，降低MCP协议采用的手动工作负担

Method: 采用多阶段工作流和LLM驱动的"运行-审查-修复"闭环循环，自动化代码分析、环境配置、服务生成和部署

Result: 能够自动将GitHub开源仓库转换为功能完整的MCP服务，并生成详细技术文档

Conclusion: Code2MCP通过自动化工具集成过程，加速MCP生态系统发展，释放GitHub海量开源代码的潜力

Abstract: The proliferation of Large Language Models (LLMs) has created a significant
integration challenge in the AI agent ecosystem, often called the "$N \times M$
problem," where N models require custom integrations for M tools. This
fragmentation stifles innovation and creates substantial development overhead.
While the Model Context Protocol (MCP) has emerged as a standard to resolve
this, its adoption is hindered by the manual effort required to convert the
vast universe of existing software into MCP-compliant services. This is
especially true for the millions of open-source repositories on GitHub, the
world's largest collection of functional code. This paper introduces Code2MCP,
a highly automated, agentic framework designed to transform any GitHub
repository into a functional MCP service with minimal human intervention. Our
system employs a multi-stage workflow that automates the entire process, from
code analysis and environment configuration to service generation and
deployment. A key innovation of our framework is an LLM-driven, closed-loop
"Run--Review--Fix" cycle, which enables the system to autonomously debug and
repair the code it generates. Code2MCP produces not only deployable services
but also comprehensive technical documentation, acting as a catalyst to
accelerate the MCP ecosystem by systematically unlocking the world's largest
open-source code repository and automating the critical last mile of tool
integration. The code is open-sourced at
https://github.com/DEFENSE-SEU/MCP-Github-Agent.

</details>


### [10] [GRACE: Graph-Guided Repository-Aware Code Completion through Hierarchical Code Fusion](https://arxiv.org/abs/2509.05980)
*Xingliang Wang,Baoyi Wang,Chen Zhi,Junxiao Han,Xinkui Zhao,Jianwei Yin,Shuiguang Deng*

Main category: cs.SE

TL;DR: GRACE是一个基于图结构的代码检索增强生成框架，通过构建多层级代码图来捕捉代码语义和结构关系，解决了传统RAG方法在代码库级别任务中的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在代码补全方面表现优秀，但在代码库级别任务中受限于上下文窗口和复杂的语义结构依赖。传统RAG方法过度依赖文本相似性检索，忽略了代码的结构关系，导致关键结构信息丢失。

Method: GRACE构建统一的多层级代码图（包含文件结构、AST、函数调用图、类层次结构和数据流图），采用混合图检索器结合图神经网络结构相似性和文本检索，并通过图注意力网络重排序器优化检索结果。

Result: 在公共代码库基准测试中，GRACE在所有指标上显著优于最先进方法。使用DeepSeek-V3作为骨干LLM，GRACE比最强的基于图的RAG基线在EM和ES指标上分别提升8.19%和7.51%。

Conclusion: GRACE通过整合结构信息和语义信息，有效解决了代码库级别任务中的上下文稀缺问题，为代码理解和生成任务提供了更全面的代码表示方法。

Abstract: LLMs excel in localized code completion but struggle with repository-level
tasks due to limited context windows and complex semantic and structural
dependencies across codebases. While Retrieval-Augmented Generation (RAG)
mitigates context scarcity by retrieving relevant code snippets, current
approaches face significant limitations. They overly rely on textual similarity
for retrieval, neglecting structural relationships such as call chains and
inheritance hierarchies, and lose critical structural information by naively
concatenating retrieved snippets into text sequences for LLM input. To address
these shortcomings, GRACE constructs a multi-level, multi-semantic code graph
that unifies file structures, abstract syntax trees, function call graphs,
class hierarchies, and data flow graphs to capture both static and dynamic code
semantics. For retrieval, GRACE employs a Hybrid Graph Retriever that
integrates graph neural network-based structural similarity with textual
retrieval, refined by a graph attention network-based re-ranker to prioritize
topologically relevant subgraphs. To enhance context, GRACE introduces a
structural fusion mechanism that merges retrieved subgraphs with the local code
context and preserves essential dependencies like function calls and
inheritance. Extensive experiments on public repository-level benchmarks
demonstrate that GRACE significantly outperforms state-of-the-art methods
across all metrics. Using DeepSeek-V3 as the backbone LLM, GRACE surpasses the
strongest graph-based RAG baselines by 8.19% EM and 7.51% ES points on every
dataset. The code is available at
https://anonymous.4open.science/r/grace_icse-C3D5.

</details>


### [11] [Students' Perception of LLM Use in Requirements Engineering Education: An Empirical Study Across Two Universities](https://arxiv.org/abs/2509.05995)
*Sharon Guardado,Risha Parveen,Zheying Zhang,Maruf Rayhan,Nirnaya Tripathi*

Main category: cs.SE

TL;DR: 大语言模型在需求工程教育中的集成能够提升学生对需求工程概念的理解，但存在学术成数和过度依赖AI的风险。个人作业比团队作业更有效，上下文化集成很重要。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型在需求工程教育中的影响，提升学生参与度和动机，为其未来职业发展提供实用工具支持。

Method: 在两所大学的2门需求工程课程中，通过个人作业和团队基础的漾动项目两种教学格式集成LLMs，收集了179名学生的调查数据。

Result: LLMs提高了学生对需求工程概念的理解，特别是在需求获取和文档化任务中。但学生担心学术成数、过度依赖AI以及将AI生成内容整合到作业中的挑战。个人作业比团队作业效果更好。

Conclusion: 研究提出了有效集成LLMs到需求工程教育中的建议，并建议未来研究应关注如何在需求工程课程中平衡AI辅助学习与批判性思维和协作实践。

Abstract: The integration of Large Language Models (LLMs) in Requirements Engineering
(RE) education is reshaping pedagogical approaches, seeking to enhance student
engagement and motivation while providing practical tools to support their
professional future. This study empirically evaluates the impact of integrating
LLMs in RE coursework. We examined how the guided use of LLMs influenced
students' learning experiences, and what benefits and challenges they perceived
in using LLMs in RE practices. The study collected survey data from 179
students across two RE courses in two universities. LLMs were integrated into
coursework through different instructional formats, i.e., individual
assignments versus a team-based Agile project. Our findings indicate that LLMs
improved students' comprehension of RE concepts, particularly in tasks like
requirements elicitation and documentation. However, students raised concerns
about LLMs in education, including academic integrity, overreliance on AI, and
challenges in integrating AI-generated content into assignments. Students who
worked on individual assignments perceived that they benefited more than those
who worked on team-based assignments, highlighting the importance of contextual
AI integration. This study offers recommendations for the effective integration
of LLMs in RE education. It proposes future research directions for balancing
AI-assisted learning with critical thinking and collaborative practices in RE
courses.

</details>


### [12] [A Rapid Review Regarding the Concept of Legal Requirements in Requirements Engineering](https://arxiv.org/abs/2509.06012)
*Jukka Ruohonen*

Main category: cs.SE

TL;DR: 这篇论文通过快速审查分析了需求工程领域中法律要求的概念，发现存在概念混乱、缺乏明确定义和实证证据支撑等问题。


<details>
  <summary>Details</summary>
Motivation: 出于个人困惑、同行评审意见以及现有文献中存在的明显混乱，需要对法律要求概念进行系统性分析。

Method: 采用快速审查方法，对需求工程研究领域中关于法律要求的现有文献进行综合分析。

Result: 发现存在规范性理解，但缺乏正确定义和概念操作化；法律要求被认为模糊复杂、更新频繁、实施困难；存在知识空白和缺乏实证证据支撑。

Conclusion: 评估结果提出了关于知识空白的批判性论断，强调需要更多实证证据来支撑观察结论，并解决持续的概念混乱问题。

Abstract: Out of a personal puzzlement, recent peer review comments, and demonstrable
confusion in the existing literature, the paper presents a rapid review of the
concept of legal requirements (LRs) in requirements engineering (RE) research.
According to reviewing results, a normative understanding of LRs has often been
present, although proper definitions and conceptual operationalizations are
lacking. Some papers also see LRs as functional and others as non-functional
requirements. Legal requirements are often characterized as being vague and
complex, requiring a lot of effort to elicit, implement, and validate. These
characterizations supposedly correlate with knowledge gaps among requirements
engineers. LRs are also seen to often change and overlap. They may be also
prioritized. According to the literature, they seem to be also reluctantly
implemented, often providing only a minimal baseline for other requirements.
With these and other observations, the review raises critical arguments about
apparent knowledge gaps, including a lack of empirical evidence backing the
observations and enduring conceptual confusion.

</details>


### [13] [Empirical Study of Code Large Language Models for Binary Security Patch Detection](https://arxiv.org/abs/2509.06052)
*Qingyuan Li,Binchang Li,Cuiyun Gao,Shuzheng Gao,Zongjie Li*

Main category: cs.SE

TL;DR: 二进制安全补丁检测的重要性与现有方法的局限性，通过构建大规模二进制补丁数据集并系统评估19个代码大语言模型，发现细调策略能够显著提升模型在二进制安全补丁检测任务中的性能


<details>
  <summary>Details</summary>
Motivation: 现有学习基于SPD方法主要面向源代码，无法应用于发布二进制文件的闭源软件和专有系统，而这些系统占实际软件的许多部分。代码LLM在二进制分析任务中表现突出，但其在二进制安全补丁检测中的潜力仍未得到探索

Method: 构建包含19,448个样本的大规模二进制补丁数据集，提供汇编代码和伪代码两种表示形式。系统评估19个不同规模的代码LLM，进行初始探索和细调实验，比较两种表示形式的效果

Result: 直接提问普通代码LLM无法准确识别安全补丁，甚至最先进的提问技术也无法补偿模型在二进制SPD领域知识的缺乏。细调后的LLM取得了突出的性能，其中伪代码表示形式的效果最佳

Conclusion: 通过细调策略向代码LLM注入二进制安全补丁检测领域知识是有效的，伪代码表示在这一任务中显示出更好的效果，为闭源软件安全补丁检测提供了新的解决方案

Abstract: Security patch detection (SPD) is crucial for maintaining software security,
as unpatched vulnerabilities can lead to severe security risks. In recent
years, numerous learning-based SPD approaches have demonstrated promising
results on source code. However, these approaches typically cannot be applied
to closed-source applications and proprietary systems that constitute a
significant portion of real-world software, as they release patches only with
binary files, and the source code is inaccessible. Given the impressive
performance of code large language models (LLMs) in code intelligence and
binary analysis tasks such as decompilation and compilation optimization, their
potential for detecting binary security patches remains unexplored, exposing a
significant research gap between their demonstrated low-level code
understanding capabilities and this critical security task. To address this
gap, we construct a large-scale binary patch dataset containing \textbf{19,448}
samples, with two levels of representation: assembly code and pseudo-code, and
systematically evaluate \textbf{19} code LLMs of varying scales to investigate
their capability in binary SPD tasks. Our initial exploration demonstrates that
directly prompting vanilla code LLMs struggles to accurately identify security
patches from binary patches, and even state-of-the-art prompting techniques
fail to mitigate the lack of domain knowledge in binary SPD within vanilla
models. Drawing on the initial findings, we further investigate the fine-tuning
strategy for injecting binary SPD domain knowledge into code LLMs through two
levels of representation. Experimental results demonstrate that fine-tuned LLMs
achieve outstanding performance, with the best results obtained on the
pseudo-code representation.

</details>


### [14] [Software Dependencies 2.0: An Empirical Study of Reuse and Integration of Pre-Trained Models in Open-Source Projects](https://arxiv.org/abs/2509.06085)
*Jerin Yasmin,Wenxin Jiang,James C. Davis,Yuan Tian*

Main category: cs.SE

TL;DR: 预训练模型作为软件依赖的重用情况分析，研究了401个GitHub仓库中PTM的管理和集成模式


<details>
  <summary>Details</summary>
Motivation: 预训练模型的普遍采用引入了新的软件依赖类型（软件依赖2.0），但其在实际项目中的集成情况不明，可能威胁到现代软件系统的可维护性和可靠性

Method: 采用混合方法分析，从PeaTMOSS数据集（28,575个重用Hugging Face和PyTorch Hub PTM的仓库）中随机采样401个GitHub仓库进行统计分析，量化分析PTM重用模式，质性研究开发者的实际集成管理方式

Result: 本文展示了对PTM作为软件依赖的系统性研究，但具体结果需要查看完整论文

Conclusion: 这项研究为理解软件依赖2.0的实践提供了重要见解，并持续调查PTM集成对软件系统质量的影响

Abstract: Pre-trained models (PTMs) are machine learning models that have been trained
in advance, often on large-scale data, and can be reused for new tasks, thereby
reducing the need for costly training from scratch. Their widespread adoption
introduces a new class of software dependency, which we term Software
Dependencies 2.0, extending beyond conventional libraries to learned behaviors
embodied in trained models and their associated artifacts. The integration of
PTMs as software dependencies in real projects remains unclear, potentially
threatening maintainability and reliability of modern software systems that
increasingly rely on them. Objective: In this study, we investigate Software
Dependencies 2.0 in open-source software (OSS) projects by examining the reuse
of PTMs, with a focus on how developers manage and integrate these models.
Specifically, we seek to understand: (1) how OSS projects structure and
document their PTM dependencies; (2) what stages and organizational patterns
emerge in the reuse pipelines of PTMs within these projects; and (3) the
interactions among PTMs and other learned components across pipeline stages. We
conduct a mixed-methods analysis of a statistically significant random sample
of 401 GitHub repositories from the PeaTMOSS dataset (28,575 repositories
reusing PTMs from Hugging Face and PyTorch Hub). We quantitatively examine PTM
reuse by identifying patterns and qualitatively investigate how developers
integrate and manage these models in practice.

</details>


### [15] [Agentic Software Engineering: Foundational Pillars and a Research Roadmap](https://arxiv.org/abs/2509.06216)
*Ahmed E. Hassan,Hao Li,Dayi Lin,Bram Adams,Tse-Hsun Chen,Yutaro Kashiwa,Dong Qiu*

Main category: cs.SE

TL;DR: 该论文提出了结构化自主软件工程(SASE)视野，认为自主软件工程时代需要重构人类与智能代理的共同作业模式，通过ACE和AEE两个工作台支持双向协作。


<details>
  <summary>Details</summary>
Motivation: 身份软件工程(SE 3.0)时代来临，智能代理需要完成复杂的目标导向任务。为了在发挥这些新能力的同时确保可信过程，需要重新认识软件工程领域的二元性：为人类服务的SE和为代理服务的SE。

Method: 提出两个专门设计的工作台：Agent Command Environment (ACE)作为命令中心，人类在其中组织和指导代理团队；Agent Execution Environment (AEE)作为数字工作空间，代理在其中执行任务并在遇到模糊性时调用人类专业知识。支持双向伙伴关系和结构化工程活动。

Result: 论文提出了SASE视野，给出了未来软件工程的基础架构架筹，并提出了研究路线图，识别了关键挑战和机遇。

Conclusion: 该研究的目标不是提供定论性解决方案，而是提供一个概念架构和结构化词汇，以促进社区广泛对话，推动SE社区超越传统的以人为中心原则，向着规范化、可扩展和可信的自主未来发展。

Abstract: Agentic Software Engineering (SE 3.0) represents a new era where intelligent
agents are tasked not with simple code generation, but with achieving complex,
goal-oriented SE objectives. To harness these new capabilities while ensuring
trustworthiness, we must recognize a fundamental duality within the SE field in
the Agentic SE era, comprising two symbiotic modalities: SE for Humans and SE
for Agents. This duality demands a radical reimagining of the foundational
pillars of SE (actors, processes, tools, and artifacts) which manifest
differently across each modality. We propose two purpose-built workbenches to
support this vision. The Agent Command Environment (ACE) serves as a command
center where humans orchestrate and mentor agent teams, handling outputs such
as Merge-Readiness Packs (MRPs) and Consultation Request Packs (CRPs). The
Agent Execution Environment (AEE) is a digital workspace where agents perform
tasks while invoking human expertise when facing ambiguity or complex
trade-offs. This bi-directional partnership, which supports agent-initiated
human callbacks and handovers, gives rise to new, structured engineering
activities (i.e., processes) that redefine human-AI collaboration, elevating
the practice from agentic coding to true agentic software engineering. This
paper presents the Structured Agentic Software Engineering (SASE) vision,
outlining several of the foundational pillars for the future of SE. The paper
culminates in a research roadmap that identifies a few key challenges and
opportunities while briefly discussing the resulting impact of this future on
SE education. Our goal is not to offer a definitive solution, but to provide a
conceptual scaffold with structured vocabulary to catalyze a community-wide
dialogue, pushing the SE community to think beyond its classic, human-centric
tenets toward a disciplined, scalable, and trustworthy agentic future.

</details>


### [16] [Learning From Software Failures: A Case Study at a National Space Research Center](https://arxiv.org/abs/2509.06301)
*Dharun Anandayuvaraj,Zain Hammadeh,Andreas Lund,Alexandra Holloway,James C. Davis*

Main category: cs.SE

TL;DR: 这篇论文通过10个深度访谈研究了高可靠性组织中软件工程师从失败中学习的实践与挑战，发现失败学习通常是非正式的、随机的，并因时间约束、知识流失等挑战而受限。


<details>
  <summary>Details</summary>
Motivation: 软件失败可能造成严重后果，从失败中学习是软件工程的关键方面。虽然建议进行事后分析，但实践效果和采用情况差异很大。需要深入了解工程师如何收集、文档化、分享和应用失败教训。

Method: 采用案例研究方法，通过对国家太空研究中心的10位研究软件工程师进行深度访谈。为了评估可转移性，还包含来自其他高可靠性组织的5个访谈数据。

Result: 研究发现：(1) 失败学习通常是非正式的、随机的，并不一致地集成到软件开发生命周期中；(2) 由于缺乏结构化过程，重复失败持续存在；(3) 时间约束、人员替换造成的知识流失、文档分散和过程执行弱缺等关键挑战娱制了系统性学习。

Conclusion: 研究深化了对软件工程师如何从失败中学习的理解，为改善失败管理实践提供了指导。这些发现对于开发更有效的过程和工具来提高可靠性和防止重复失败具有重要意义。

Abstract: Software failures can have significant consequences, making learning from
failures a critical aspect of software engineering. While software
organizations are recommended to conduct postmortems, the effectiveness and
adoption of these practices vary widely. Understanding how engineers gather,
document, share, and apply lessons from failures is essential for improving
reliability and preventing recurrence. High-reliability organizations (HROs)
often develop software systems where failures carry catastrophic risks,
requiring continuous learning to ensure reliability. These organizations
provide a valuable setting to examine practices and challenges for learning
from software failures. Such insight could help develop processes and tools to
improve reliability and prevent recurrence. However, we lack in-depth industry
perspectives on the practices and challenges of learning from failures.
  To address this gap, we conducted a case study through 10 in-depth interviews
with research software engineers at a national space research center. We
examine how they learn from failures: how they gather, document, share, and
apply lessons. To assess transferability, we include data from 5 additional
interviews at other HROs. Our findings provide insight into how engineers learn
from failures in practice. To summarize: (1) failure learning is informal, ad
hoc, and inconsistently integrated into SDLC; (2) recurring failures persist
due to absence of structured processes; and (3) key challenges, including time
constraints, knowledge loss from turnover and fragmented documentation, and
weak process enforcement, undermine systematic learning. Our findings deepen
understanding of how software engineers learn from failures and offer guidance
for improving failure management practices.

</details>


### [17] [A Generic and Efficient Python Runtime Verification System and its Large-scale Evaluation](https://arxiv.org/abs/2509.06324)
*Zhuohang Shen,Mohammed Yaseen,Denini Silva,Kevin Guan,Junho Lee,Marcelo d'Amorim,Owolabi Legunsen*

Main category: cs.SE

TL;DR: PyMOP是一个通用、可扩展且高效的Python运行时验证系统，支持多种逻辑和监控算法，在大量测试中表现出色，比现有系统快上千倍，并帮助发现了121个bug。


<details>
  <summary>Details</summary>
Motivation: Python生态系统缺乏像Java那样可扩展的运行时验证系统，现有的Python RV系统要么局限于特定领域或规范逻辑，要么运行速度慢。

Method: 开发了PyMOP系统，支持五种逻辑，实现了五种现有监控算法，包含73个API规范，支持三种插桩策略，用户可轻松扩展。

Result: 在1,463个GitHub项目的290,133个单元测试中：1) Java的默认监控算法对Python不是最快的；2) PyMOP比两个最近的动态分析系统快1,168.3倍；3) 发现的121个bug中有44个被开发者修复。

Conclusion: PyMOP的通用性和高效性使其成为Python运行时验证研究的优秀平台。

Abstract: Runtime verification (RV) now scales for testing thousands of open-source
Java projects, helping find hundreds of bugs. The popular Python ecosystem
could use such benefits. But, today's Python RV systems are limited to a domain
or specification logic, or slow. We propose PyMOP, a generic, extensible, and
efficient RV system for Python. PyMOP supports five logics, implements five
existing monitoring algorithms, ships with 73 API specs of Python and
widely-used libraries, supports three instrumentation strategies, and users can
easily add more of these. On 290,133 unit tests in 1,463 GitHub projects, we
find mainly that (i) the default monitoring algorithm for Java is often not the
fastest for Python; (ii) PyMOP is up to 1,168.3x faster than two recent dynamic
analysis systems; and (iii) 44 of 121 bugs that PyMOP helped find so far were
fixed by developers. PyMOP's generality and efficiency position it well as an
excellent platform for the next advances on RV for Python.

</details>


### [18] [Analyzing the Instability of Large Language Models in Automated Bug Injection and Correction](https://arxiv.org/abs/2509.06429)
*Mehmet Bilal Er,Nagehan İlhan,Umut Kuran*

Main category: cs.SE

TL;DR: 研究评估ChatGPT在代码修复任务中的稳定性，发现温度设置高时输出变得不稳定且功能失效率高


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型在软件工程中应用日益增长，但它们经常产生不稳定的结果，特别是在代码修复任务中的一致性还未得到全面评估

Method: 使用不同温度设置(0, 0.5, 1)对20个问题生成3个修复建议，比较每个问题的9个输出。使用语法相似性和输出等效率(OER)指标评估结构和功能一致性

Result: 温度越高，模型输出越不稳定和变化，高温度时功能失效率特别高。语法分析显示高温度下修复建议结构差异显著，低温度下相似度较高

Conclusion: 研究对基于LLM的错误纠正系统如何更一致地应用于软件开发过程提供了重要方法论见解，同时对其可靠性提出了疑问

Abstract: The use of Large Language Models (LLMs) in software engineering tasks is
growing, especially in the areas of bug fixing and code generation.
Nevertheless, these models often yield unstable results; when executed at
different times with the same input, they can generate radically different
code. The consistency of LLMs in bug-fixing tasks has not yet been thoroughly
assessed, despite the fact that this instability has typically been discussed
in the literature in relation to code generation. The purpose of this study is
to look into how unstable an LLM like ChatGPT is when it comes to fixing code
bugs. We examine the structural, syntactic, and functional variations among
several fix recommendations made in response to the same prompt using code
samples with various error types. Additionally, we assess how instability is
affected by the temperature settings (0, 0.5, and 1) used for the model's
deterministic operation. For a total of 20 problems in the experimental
analysis, the model produced three fix suggestions at each temperature value,
comparing nine distinct outputs for each problem. The Syntax Similarity and
Output Equivalence Rate (OER) metrics were used to assess the outputs'
structural and functional consistency. The results demonstrate that the model's
outputs become much more unstable and variable as the temperature rises, with
high temperatures showing especially high rates of functional failure.
According to syntax similarity analyses, the suggested fixes show notable
structural differences at high temperatures but are fairly similar at low
temperatures. The purpose of this study is to provide important methodological
insights into how LLM-based error correction systems can be applied more
consistently in software development processes while also casting doubt on
their dependability.

</details>


### [19] [Modeling in the Design Multiverse](https://arxiv.org/abs/2509.06530)
*Sylvain Guérin,Salvador Martinez,Ciprian Teodorov*

Main category: cs.SE

TL;DR: 设计多元宇宙概念，通过模型联盟实现对设计过程中的版本分支、修订和合并等复杂变化进行管理


<details>
  <summary>Details</summary>
Motivation: 实际设计过程中存在多重设计路径的演化和分支，当前模型工具无法直接管理这种时空变化性

Method: 提出设计多元宇宙概念，通过模型联盟实现对多个设计状态扩凑的集成管理

Result: 构建了支持设计决策追踪、系统变体管理和依赖关系分析的统一模型空间

Conclusion: 设计多元宇宙为复杂异构系统的并发设计过程提供了有效的变化管理方案

Abstract: Real-world design processes often involve the evolution and divergence of
design paths (by branching, revising, merging, etc.), especially when multiple
stakeholders or teams operate concurrently and/or explore different
alternatives for complex and heterogeneous systems. Unfortunately, this
variability in time and space can not be directly managed in current modeling
spaces but requires resorting to external tools and methodologies.
  In order to tackle this problem, we introduce the Design Multiverse. The
Design Multiverse aims to integrate in the modeling space a selection of
revisions and variants, representing snapshots of a design state composed of
multiple artifacts. This enables stakeholders to seamlessly trace, analyze, and
manage design decisions, system variants, and their interdependencies.
Concretely, in this paper we present a conceptual definition of the Design
Multiverse, discuss usage scenarios such as model product lines and
model/metamodel co-evolution, and propose an implementation leveraging the
model federation paradigm.

</details>


### [20] [Design and Implementation of a Domain-specific Language for Modelling Evacuation Scenarios Using Eclipse EMG/GMF Tool](https://arxiv.org/abs/2509.06688)
*Heerok Banerjee*

Main category: cs.SE

TL;DR: Bmod是一种基于Eclipse EMF/GMF构建的领域特定语言，用于建模疏散场景，并与其他建模工具进行了比较分析


<details>
  <summary>Details</summary>
Motivation: 解决企业内部依赖关系，提升业务管理流程，缩短新手用户的学习差距，需要开发支持图形界面、层次结构和易用实现的建模框架

Method: 使用Eclipse建模框架(EMF)和图形建模框架(GMF)构建Bmod DSL，并与AToMPM、metaDepth、Sirius等工具在表达能力、学习曲线和性能方面进行比较

Result: 开发了Bmod DSL用于疏散场景建模，通过比较分析展示了不同建模工具的特点

Conclusion: 基于Eclipse EMF/GMF的Bmod DSL为疏散场景建模提供了有效的解决方案，工具比较为选择合适的建模框架提供了参考

Abstract: Domain-specific languages (DSLs) play a crucial role in resolving internal
dependencies across enterprises and boosts their upfront business management
processes. Yet, a lot of development is needed to build modelling frameworks
which support graphical interfaces (canvas, pallettes etc.), hierarchical
structures and easy implementation to shorten the gap for novice users. In this
paper, a DSL namely, Bmod is introduced, which can be used to model evacuation
scenarios. The language is built using Eclipse Modelling Framework (EMF) and
Eclipse Graphical Modelling Framework (GMF). Furthermore, a comparison is also
shown between Eclipse EMF/GMF and other modelling tools such as AToMPM,
metaDepth, Sirius etc with respect to expressiveness, learning curve and
performance.

</details>


### [21] [Efficiently Ranking Software Variants with Minimal Benchmarks](https://arxiv.org/abs/2509.06716)
*Théo Matricon,Mathieu Acher,Helge Spieker,Arnaud Gotlieb*

Main category: cs.SE

TL;DR: 提出BISS方法，通过测试套件优化技术减少基准测试的计算成本，同时保持变体排名的稳定性


<details>
  <summary>Details</summary>
Motivation: 基准测试在软件工程中用于评估软件变体的质量和性能，但执行完整基准测试的计算资源和时间成本极高

Method: BISS（BISection Sampling）方法策略性地保留最关键测试，并应用新颖的分治方法在相关剩余测试中高效采样

Result: 在LLM排行榜、SAT竞赛和可配置系统性能建模的数据集上，BISS方法平均将基准测试计算成本降低到44%，超过一半的基准测试可减少高达99%的成本，且排名稳定性无损失

Conclusion: BISS方法能显著降低基准测试的计算成本，同时保持变体排名的稳定性，优于基线方法

Abstract: Benchmarking is a common practice in software engineering to assess the
qualities and performance of software variants, coming from multiple competing
systems or from configurations of the same system. Benchmarks are used notably
to compare and understand variant performance, fine-tune software, detect
regressions, or design new software systems. The execution of benchmarks to get
a complete picture of software variants is highly costly in terms of
computational resources and time. In this paper, we propose a novel approach
for reducing benchmarks while maintaining stable rankings, using test suite
optimization techniques. That is, we remove instances from the benchmarks while
trying to keep the same rankings of the variants on all tests. Our method,
BISection Sampling, BISS, strategically retains the most critical tests and
applies a novel divide-and-conquer approach to efficiently sample among
relevant remaining tests. We experiment with datasets and use cases from LLM
leaderboards, SAT competitions, and configurable systems for performance
modeling. Our results show that our method outperforms baselines even when
operating on a subset of variants. Using BISS, we reduce the computational cost
of the benchmarks on average to 44% and on more than half the benchmarks by up
to 99% without loss in ranking stability.

</details>


### [22] [OpenCoderRank: AI-Driven Technical Assessments Made Easy](https://arxiv.org/abs/2509.06774)
*Hridoy Sankar Dutta,Sana Ansari,Swati Kumari,Shounak Ravi Bhalerao*

Main category: cs.SE

TL;DR: OpenCoderRank是一个易于使用的技术评估模拟平台，连接问题设置者和解决者，帮助解决者在时间限制和陌生问题下准备，同时允许设置者自托管评估


<details>
  <summary>Details</summary>
Motivation: 在LLM时代，虽然LLM能帮助问题设置者生成多样化挑战性问题，但也可能通过提供解决方案来破坏评估的完整性，需要一种平衡的评估平台

Method: 开发OpenCoderRank平台，模拟技术评估环境，支持自托管评估，为资源受限环境提供无成本和可定制的解决方案

Result: 创建了一个连接问题设置者和解决者的桥梁平台，能够有效模拟真实技术评估场景

Conclusion: OpenCoderRank为技术评估提供了一个实用的解决方案，特别适合资源受限的环境，既保护了评估的完整性，又帮助参与者更好地准备

Abstract: Organizations and educational institutions use time-bound assessment tasks to
evaluate coding and problem-solving skills. These assessments measure not only
the correctness of the solutions, but also their efficiency. Problem setters
(educator/interviewer) are responsible for crafting these challenges, carefully
balancing difficulty and relevance to create meaningful evaluation experiences.
Conversely, problem solvers (student/interviewee) apply coding efficiency and
logical thinking to arrive at correct solutions. In the era of Large Language
Models (LLMs), LLMs assist problem setters in generating diverse and
challenging questions, but they can undermine assessment integrity for problem
solvers by providing easy access to solutions. This paper introduces
OpenCoderRank, an easy-to-use platform designed to simulate technical
assessments. It acts as a bridge between problem setters and problem solvers,
helping solvers prepare for time constraints and unfamiliar problems while
allowing setters to self-host assessments, offering a no-cost and customizable
solution for technical assessments in resource-constrained environments.

</details>


### [23] [Hypergraph-Guided Regex Filter Synthesis for Event-Based Anomaly Detection](https://arxiv.org/abs/2509.06911)
*Margarida Ferreira,Victor Nicolet,Luan Pham,Joey Dodds,Daniel Kroening,Ines Lynce,Ruben Martins*

Main category: cs.SE

TL;DR: HyGLAD是一种新颖的算法，通过构建可解释的模式来检测事件数据中的异常，在性能和效率上都优于现有的深度学习方法


<details>
  <summary>Details</summary>
Motivation: 在静态系统中检测事件型异常，需要能够识别与过去行为偏差的可解释方法，而深度学习方法的黑盒特性限制了其可解释性

Method: 算法推断具有相似行为的实体等价类，然后构建捕获这些实体值的正则表达式模式，这些模式直接可解释

Result: 在5个真实系统数据集上，HyGLAD平均性能优于所有7种DeepOD的无监督异常检测方法，训练和推理效率提高一个数量级（单CPU vs GPU），精确度提高1.2倍，召回率提高1.3倍

Conclusion: HyGLAD提供了一种既高效又可解释的异常检测方法，在性能和可解释性方面都优于深度学习基线方法

Abstract: We propose HyGLAD, a novel algorithm that automatically builds a set of
interpretable patterns that model event data. These patterns can then be used
to detect event-based anomalies in a stationary system, where any deviation
from past behavior may indicate malicious activity. The algorithm infers
equivalence classes of entities with similar behavior observed from the events,
and then builds regular expressions that capture the values of those entities.
As opposed to deep-learning approaches, the regular expressions are directly
interpretable, which also translates to interpretable anomalies. We evaluate
HyGLAD against all 7 unsupervised anomaly detection methods from DeepOD on five
datasets from real-world systems. The experimental results show that on average
HyGLAD outperforms existing deep-learning methods while being an order of
magnitude more efficient in training and inference (single CPU vs GPU).
Precision improved by 1.2x and recall by 1.3x compared to the second-best
baseline.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [24] [Towards Log Analysis with AI Agents: Cowrie Case Study](https://arxiv.org/abs/2509.05306)
*Enis Karaarslan,Esin Güler,Efe Emir Yüce,Cagatay Coban*

Main category: cs.CR

TL;DR: 使用AI代理自动分析Cowrie蜜罐日志，解决网络安全研究中真实攻击数据稀缺和日志分析困难的问题


<details>
  <summary>Details</summary>
Motivation: 真实攻击数据稀缺阻碍网络安全研究发展，蜜罐产生的大量非结构化日志难以手动分析，需要自动化解决方案

Method: 采用轻量级自动化方法，利用AI代理智能解析、总结和提取Cowrie蜜罐原始日志的洞察，同时考虑自主系统的安全影响

Result: 初步结果显示该流水线能有效减少人工工作量并识别攻击模式

Conclusion: 该方法为未来更先进的自主网络安全分析铺平了道路

Abstract: The scarcity of real-world attack data significantly hinders progress in
cybersecurity research and education. Although honeypots like Cowrie
effectively collect live threat intelligence, they generate overwhelming
volumes of unstructured and heterogeneous logs, rendering manual analysis
impractical. As a first step in our project on secure and efficient AI
automation, this study explores the use of AI agents for automated log
analysis. We present a lightweight and automated approach to process Cowrie
honeypot logs. Our approach leverages AI agents to intelligently parse,
summarize, and extract insights from raw data, while also considering the
security implications of deploying such an autonomous system. Preliminary
results demonstrate the pipeline's effectiveness in reducing manual effort and
identifying attack patterns, paving the way for more advanced autonomous
cybersecurity analysis in future work.

</details>


### [25] [Large Language Model Integration with Reinforcement Learning to Augment Decision-Making in Autonomous Cyber Operations](https://arxiv.org/abs/2509.05311)
*Konur Tholl,François Rivest,Mariam El Mezouar,Ranwa Al Mallah*

Main category: cs.CR

TL;DR: 使用大语言模型指导强化学习在网络安全自主操作中，提高初期表现和学习效率


<details>
  <summary>Details</summary>
Motivation: 解决强化学习在网络安全领域中需要从头学习、执行有害动作的问题

Method: 集成领先在网络安全数据上预训练的大语言模型，指导RL经纪人做出明智决策

Result: 在模拟网络安全环境中，指导后的经纪人初期奖励提高2倍以上，收敛速度提高约4500个完整训练循环

Conclusion: 外部知识集成能够显著提升RL在网络安全领域的学习效率和性能

Abstract: Reinforcement Learning (RL) has shown great potential for autonomous
decision-making in the cybersecurity domain, enabling agents to learn through
direct environment interaction. However, RL agents in Autonomous Cyber
Operations (ACO) typically learn from scratch, requiring them to execute
undesirable actions to learn their consequences. In this study, we integrate
external knowledge in the form of a Large Language Model (LLM) pretrained on
cybersecurity data that our RL agent can directly leverage to make informed
decisions. By guiding initial training with an LLM, we improve baseline
performance and reduce the need for exploratory actions with obviously negative
outcomes. We evaluate our LLM-integrated approach in a simulated cybersecurity
environment, and demonstrate that our guided agent achieves over 2x higher
rewards during early training and converges to a favorable policy approximately
4,500 episodes faster than the baseline.

</details>


### [26] [Backdoor Samples Detection Based on Perturbation Discrepancy Consistency in Pre-trained Language Models](https://arxiv.org/abs/2509.05318)
*Zuquan Peng,Jianming Fu,Lixin Zou,Li Zheng,Yanzhen Ren,Guojun Peng*

Main category: cs.CR

TL;DR: 提出了一种基于扰动差异一致性评估(NETE)的后门样本检测方法，无需访问中毒模型或额外干净样本，仅需现成的预训练模型即可在训练前后阶段检测后门样本。


<details>
  <summary>Details</summary>
Motivation: 现有后门检测方法通常需要访问中毒模型、额外干净样本或大量计算资源，限制了实际应用。预训练模型使用未经审查的第三方和互联网数据容易受到后门攻击。

Method: 基于扰动差异一致性评估(NETE)，利用后门样本扰动差异变化小于干净样本的现象，通过曲率测量不同扰动样本与输入样本之间的对数概率差异，评估扰动差异一致性来判断是否为后门样本。

Result: 在四种典型后门攻击和五类大语言模型后门攻击上的实验表明，该检测策略优于现有的零样本黑盒检测方法。

Conclusion: NETE方法提供了一种实用且有效的后门样本检测解决方案，无需额外资源即可在训练前后阶段进行检测，具有很好的实用性。

Abstract: The use of unvetted third-party and internet data renders pre-trained models
susceptible to backdoor attacks. Detecting backdoor samples is critical to
prevent backdoor activation during inference or injection during training.
However, existing detection methods often require the defender to have access
to the poisoned models, extra clean samples, or significant computational
resources to detect backdoor samples, limiting their practicality. To address
this limitation, we propose a backdoor sample detection method based on
perturbatio\textbf{N} discr\textbf{E}pancy consis\textbf{T}ency
\textbf{E}valuation (\NETE). This is a novel detection method that can be used
both pre-training and post-training phases. In the detection process, it only
requires an off-the-shelf pre-trained model to compute the log probability of
samples and an automated function based on a mask-filling strategy to generate
perturbations. Our method is based on the interesting phenomenon that the
change in perturbation discrepancy for backdoor samples is smaller than that
for clean samples. Based on this phenomenon, we use curvature to measure the
discrepancy in log probabilities between different perturbed samples and input
samples, thereby evaluating the consistency of the perturbation discrepancy to
determine whether the input sample is a backdoor sample. Experiments conducted
on four typical backdoor attacks and five types of large language model
backdoor attacks demonstrate that our detection strategy outperforms existing
zero-shot black-box detection methods.

</details>


### [27] [Privacy-Preserving Offloading for Large Language Models in 6G Vehicular Networks](https://arxiv.org/abs/2509.05320)
*Ikhlasse Badidi,Nouhaila El Khiyaoui,Aya Riany,Badr Ben Elallid,Amine Abouaomar*

Main category: cs.CR

TL;DR: 基于联邦学习和差分隐私的汽车网络LLM计算离线框架，在保证隐私的前提下维持了75%全局准确率，通信开销稳定在2.1MB/轮


<details>
  <summary>Details</summary>
Motivation: 解决6G汽车网络中LLM计算离线导致的用户数据泄露风险，需要在保护隐私的同时维持系统性能

Method: 提出联邦学习(FL)+差分隐私(DP)混合方案，包括隐私感知任务分割算法和安全通信协议

Result: 达到75%全局准确率(仅比非隐私方案下降2-3%)，隐私保证优化(ε=0.8)，通信开销2.1MB/轮，计算占比90%以上处理时间

Conclusion: 该框架有效解决了汽车网络LLM离线的隐私风险问题，在保证隐私的前提下维持了高性能，适用于资源受限的汽车环境

Abstract: The integration of Large Language Models (LLMs) in 6G vehicular networks
promises unprecedented advancements in intelligent transportation systems.
However, offloading LLM computations from vehicles to edge infrastructure poses
significant privacy risks, potentially exposing sensitive user data. This paper
presents a novel privacy-preserving offloading framework for LLM-integrated
vehicular networks. We introduce a hybrid approach combining federated learning
(FL) and differential privacy (DP) techniques to protect user data while
maintaining LLM performance. Our framework includes a privacy-aware task
partitioning algorithm that optimizes the trade-off between local and edge
computation, considering both privacy constraints and system efficiency. We
also propose a secure communication protocol for transmitting model updates and
aggregating results across the network. Experimental results demonstrate that
our approach achieves 75\% global accuracy with only a 2-3\% reduction compared
to non-privacy-preserving methods, while maintaining DP guarantees with an
optimal privacy budget of $\varepsilon = 0.8$. The framework shows stable
communication overhead of approximately 2.1MB per round with computation
comprising over 90\% of total processing time, validating its efficiency for
resource-constrained vehicular environments.

</details>


### [28] [Zero-Knowledge Proofs in Sublinear Space](https://arxiv.org/abs/2509.05326)
*Logan Nye*

Main category: cs.CR

TL;DR: 提出了第一个亚线性空间的零知识证明生成器，将证明生成重构为树评估问题，将证明者内存从线性T降低到O(sqrt(T))


<details>
  <summary>Details</summary>
Motivation: 现代零知识证明系统存在证明者内存随计算轨迹长度T线性扩展的问题，限制了在资源受限设备和大规模任务中的应用

Method: 通过将证明生成重新构建为经典的树评估问题实例，利用最近的空间高效树评估算法，设计流式证明生成器，无需物化完整执行轨迹

Result: 将证明者内存从线性T降低到O(sqrt(T))（包含O(log T)低阶项），同时保持证明大小、验证者时间和底层系统的安全性保证

Conclusion: 这项技术实现了从专门的服务器绑定证明向设备上证明的转变，为去中心化系统、设备上机器学习和隐私保护技术开辟了新应用

Abstract: Modern zero-knowledge proof (ZKP) systems, essential for privacy and
verifiable computation, suffer from a fundamental limitation: the prover
typically uses memory that scales linearly with the computation's trace length
T, making them impractical for resource-constrained devices and prohibitively
expensive for large-scale tasks. This paper overcomes this barrier by
constructing, to our knowledge, the first sublinear-space ZKP prover. Our core
contribution is an equivalence that reframes proof generation as an instance of
the classic Tree Evaluation problem. Leveraging a recent space-efficient
tree-evaluation algorithm, we design a streaming prover that assembles the
proof without ever materializing the full execution trace. The approach reduces
prover memory from linear in T to O(sqrt(T)) (up to O(log T) lower-order terms)
while preserving proof size, verifier time, and the transcript/security
guarantees of the underlying system. This enables a shift from specialized,
server-bound proving to on-device proving, opening applications in
decentralized systems, on-device machine learning, and privacy-preserving
technologies.

</details>


### [29] [ForensicsData: A Digital Forensics Dataset for Large Language Models](https://arxiv.org/abs/2509.05331)
*Youssef Chakir,Iyad Lahsen-Cherif*

Main category: cs.CR

TL;DR: 提出了ForensicsData数据集，包含5000多个从真实恶意软件分析报告中提取的问答三元组，用于解决数字取证领域缺乏真实数据集的问题。


<details>
  <summary>Details</summary>
Motivation: 数字取证调查面临证据收集和分析的挑战，由于伦理、法律和隐私问题，缺乏真实的公开数据集来支持研究和工具开发。

Method: 采用独特的工作流程：从恶意软件分析报告中提取结构化数据，使用大语言模型转换为问答格式，并通过专门评估流程验证质量。

Result: Gemini 2 Flash模型在生成内容与取证术语对齐方面表现最佳，数据集包含5000多个问答三元组。

Conclusion: ForensicsData数据集旨在通过支持可重复实验和促进研究社区合作来推动数字取证领域的发展。

Abstract: The growing complexity of cyber incidents presents significant challenges for
digital forensic investigators, especially in evidence collection and analysis.
Public resources are still limited because of ethical, legal, and privacy
concerns, even though realistic datasets are necessary to support research and
tool developments. To address this gap, we introduce ForensicsData, an
extensive Question-Context-Answer (Q-C-A) dataset sourced from actual malware
analysis reports. It consists of more than 5,000 Q-C-A triplets. A unique
workflow was used to create the dataset, which extracts structured data, uses
large language models (LLMs) to transform it into Q-C-A format, and then uses a
specialized evaluation process to confirm its quality. Among the models
evaluated, Gemini 2 Flash demonstrated the best performance in aligning
generated content with forensic terminology. ForensicsData aims to advance
digital forensics by enabling reproducible experiments and fostering
collaboration within the research community.

</details>


### [30] [Integrated Simulation Framework for Adversarial Attacks on Autonomous Vehicles](https://arxiv.org/abs/2509.05332)
*Christos Anagnostopoulos,Ioulia Kapsali,Alexandros Gkillas,Nikos Piperigkos,Aris S. Lalos*

Main category: cs.CR

TL;DR: 这篇论文提出了一个开源集成模拟框架，用于生成针对自主驾驶汽车感知和通信层的多域攻击场景，通过同步多个模拟器来评估AV系统的安全性。


<details>
  <summary>Details</summary>
Motivation: 自主驾驶汽车的感知和通信系统容易受到对手攻击，而现有模拟框架缺乏对多域攻击场景的全面支持，需要一个综合性的测试环境。

Method: 设计了一个集成模拟框架，通过统一核心同步多个模拟器（物理环境、交通动态、V2X网络），支持LiDAR感知攻击、V2X消息操控、GPS欺骗等多种攻击方式，并集成ROS 2以兼容第三方AV软件。

Result: 在现实条件下，对现有的3D物体检测器进行了评估，显示在生成的对手攻击场景下系统性能显著下降。

Conclusion: 该框架为AV安全性测试提供了一个高保真度、可扩展的模拟环境，能够有效地测试多域对手攻击对AV系统的影响。

Abstract: Autonomous vehicles (AVs) rely on complex perception and communication
systems, making them vulnerable to adversarial attacks that can compromise
safety. While simulation offers a scalable and safe environment for robustness
testing, existing frameworks typically lack comprehensive supportfor modeling
multi-domain adversarial scenarios. This paper introduces a novel, open-source
integrated simulation framework designed to generate adversarial attacks
targeting both perception and communication layers of AVs. The framework
provides high-fidelity modeling of physical environments, traffic dynamics, and
V2X networking, orchestrating these components through a unified core that
synchronizes multiple simulators based on a single configuration file. Our
implementation supports diverse perception-level attacks on LiDAR sensor data,
along with communication-level threats such as V2X message manipulation and GPS
spoofing. Furthermore, ROS 2 integration ensures seamless compatibility with
third-party AV software stacks. We demonstrate the framework's effectiveness by
evaluating the impact of generated adversarial scenarios on a state-of-the-art
3D object detector, revealing significant performance degradation under
realistic conditions.

</details>


### [31] [Ensembling Membership Inference Attacks Against Tabular Generative Models](https://arxiv.org/abs/2509.05350)
*Joshua Ward,Yuxuan Yang,Chi-Hua Wang,Guang Cheng*

Main category: cs.CR

TL;DR: 本文研究了合成数据隐私审计中的成员推理攻击(MIA)选择问题，发现在不同模型架构和数据集上没有一个MIA方法始终最优，因此提出了基于集成学习的MIA方法来实现更稳健的隐私保护策略。


<details>
  <summary>Details</summary>
Motivation: 在合成数据生成的隐私审计中，现有的多种成员推理攻击方法各有优势，但在实际威胁场景中，攻击者需要选择一个最优方法而缺乏先验保证。本文旨在解决这一决策理论问题。

Method: 通过进行大规模合成数据隐私基准测试，分析不同MIA方法在不同模型架构和数据集上的表现，并提出基于无监督集成学习的MIA方法。

Result: 研究发现没有一个MIA方法在所有情况下都是严格最优策略。提出的集成MIA方法相比单个攻击方法表现出更稳健的性能和更小的后悔值。

Conclusion: 集成MIA方法为解决合成数据隐私审计中的方法选择不确定性提供了有效的解决方案，能够实现后悔最小化的稳健策略。

Abstract: Membership Inference Attacks (MIAs) have emerged as a principled framework
for auditing the privacy of synthetic data generated by tabular generative
models, where many diverse methods have been proposed that each exploit
different privacy leakage signals. However, in realistic threat scenarios, an
adversary must choose a single method without a priori guarantee that it will
be the empirically highest performing option. We study this challenge as a
decision theoretic problem under uncertainty and conduct the largest synthetic
data privacy benchmark to date. Here, we find that no MIA constitutes a
strictly dominant strategy across a wide variety of model architectures and
dataset domains under our threat model. Motivated by these findings, we propose
ensemble MIAs and show that unsupervised ensembles built on individual attacks
offer empirically more robust, regret-minimizing strategies than individual
attacks.

</details>


### [32] [AI-in-the-Loop: Privacy Preserving Real-Time Scam Detection and Conversational Scambaiting by Leveraging LLMs and Federated Learning](https://arxiv.org/abs/2509.05362)
*Ismail Hossain,Sai Puppala,Sajedul Talukder,Md Jahangir Alam*

Main category: cs.CR

TL;DR: 一种保护隐私的AI实时骗局检测框架，通过联邦学习和安全调整函数，在保持流畅交互的同时主动阻断骗局对话。


<details>
  <summary>Details</summary>
Motivation: 现有防御措施多为反应式，在活跃交互中提供的保护有限，需要主动领先的骗局防御方案。

Method: 结合指令调整的人工智能与安全意识的应用函数，采用联邦学习实现持续模型更新而无需原始数据共享。

Result: 系统能够产生流畅且引人入胜的回应，人类研究证实在真实性、安全性和有效性方面显著超越基准线。联邦学习在保持高交互性的同时减少个人信息泄漏。

Conclusion: 该框架成功将实时骗局询问、联邦隐私保护和安全调整统一为一体，构建了主动防御范式，在不牺牲性能的前提下实现了稳健的隐私保护。

Abstract: Scams exploiting real-time social engineering -- such as phishing,
impersonation, and phone fraud -- remain a persistent and evolving threat
across digital platforms. Existing defenses are largely reactive, offering
limited protection during active interactions. We propose a privacy-preserving,
AI-in-the-loop framework that proactively detects and disrupts scam
conversations in real time. The system combines instruction-tuned artificial
intelligence with a safety-aware utility function that balances engagement with
harm minimization, and employs federated learning to enable continual model
updates without raw data sharing. Experimental evaluations show that the system
produces fluent and engaging responses (perplexity as low as 22.3, engagement
$\approx$0.80), while human studies confirm significant gains in realism,
safety, and effectiveness over strong baselines. In federated settings, models
trained with FedAvg sustain up to 30 rounds while preserving high engagement
($\approx$0.80), strong relevance ($\approx$0.74), and low PII leakage
($\leq$0.0085). Even with differential privacy, novelty and safety remain
stable, indicating that robust privacy can be achieved without sacrificing
performance. The evaluation of guard models (LlamaGuard, LlamaGuard2/3,
MD-Judge) shows a straightforward pattern: stricter moderation settings reduce
the chance of exposing personal information, but they also limit how much the
model engages in conversation. In contrast, more relaxed settings allow longer
and richer interactions, which improve scam detection, but at the cost of
higher privacy risk. To our knowledge, this is the first framework to unify
real-time scam-baiting, federated privacy preservation, and calibrated safety
moderation into a proactive defense paradigm.

</details>


### [33] [A Framework for Detection and Classification of Attacks on Surveillance Cameras under IoT Networks](https://arxiv.org/abs/2509.05366)
*Umair Amjid,M. Umar Khan,S. A. Manan Kirmani*

Main category: cs.CR

TL;DR: 提出基于AI的框架，利用机器学习算法分析网络流量，检测物联网摄像头网络中的异常行为和安全威胁


<details>
  <summary>Details</summary>
Motivation: 物联网设备使用增加导致安全担忧，特别是监控摄像头易受暴力破解、零日攻击和DoS攻击，可能导致黑客未授权访问和用户活动监控

Method: 基于AI的框架，使用机器学习算法分析网络流量，检测异常行为，利用真实数据集进行训练和评估

Result: 能够快速检测和响应潜在入侵，通过学习过去安全事件提高检测能力

Conclusion: AI框架能有效提升物联网摄像头网络的安全性，及时检测和防范各类网络攻击

Abstract: The increasing use of Internet of Things (IoT) devices has led to a rise in
security related concerns regarding IoT Networks. The surveillance cameras in
IoT networks are vulnerable to security threats such as brute force and
zero-day attacks which can lead to unauthorized access by hackers and potential
spying on the users activities. Moreover, these cameras can be targeted by
Denial of Service (DOS) attacks, which will make it unavailable for the user.
The proposed AI based framework will leverage machine learning algorithms to
analyze network traffic and detect anomalous behavior, allowing for quick
detection and response to potential intrusions. The framework will be trained
and evaluated using real-world datasets to learn from past security incidents
and improve its ability to detect potential intrusion.

</details>


### [34] [Between a Rock and a Hard Place: Exploiting Ethical Reasoning to Jailbreak LLMs](https://arxiv.org/abs/2509.05367)
*Shei Pern Chua,Thai Zhen Leng,Teh Kai Jun,Xiao Li,Xiaolin Hu*

Main category: cs.CR

TL;DR: TRIAL框架利用伦理困境推理来绕过LLM的安全防护，通过多轮对话策略实现高成功率越狱攻击


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型推理能力增强，传统单步越狱攻击可能不够有效，需要探索基于多轮动态上下文适应的越狱策略

Method: 提出TRIAL框架，将对抗目标嵌入到基于电车难题的伦理困境中，利用LLM的伦理推理能力绕过安全防护

Result: TRIAL在开源和闭源模型上都表现出很高的越狱成功率

Conclusion: 当前的安全防护措施可能不足以应对上下文感知的对抗攻击，需要重新评估安全对齐监督策略

Abstract: Large language models (LLMs) have undergone safety alignment efforts to
mitigate harmful outputs. However, as LLMs become more sophisticated in
reasoning, their intelligence may introduce new security risks. While
traditional jailbreak attacks relied on singlestep attacks, multi-turn
jailbreak strategies that adapt dynamically to context remain underexplored. In
this work, we introduce TRIAL (Trolley-problem Reasoning for Interactive Attack
Logic), a framework that leverages LLMs ethical reasoning to bypass their
safeguards. TRIAL embeds adversarial goals within ethical dilemmas modeled on
the trolley problem. TRIAL demonstrates high jailbreak success rates towards
both open and close-source models. Our findings underscore a fundamental
limitation in AI safety: as models gain advanced reasoning abilities, the
nature of their alignment may inadvertently allow for more covert security
vulnerabilities to be exploited. TRIAL raises an urgent need in reevaluating
safety alignment oversight strategies, as current safeguards may prove
insufficient against context-aware adversarial attack.

</details>


### [35] [Quantum AI Algorithm Development for Enhanced Cybersecurity: A Hybrid Approach to Malware Detection](https://arxiv.org/abs/2509.05370)
*Tanya Joshi,Krishnendu Guha*

Main category: cs.CR

TL;DR: 量子机器学习算法在网络安全威胁检测中显示优势，特别是在复杂的电子病毒分类任务中达到95%的准确率，超越传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习方法在处理复杂的电子病毒模式和大规模网络入侵数据时遇到局限性，需要量子技术来突破这些挑战。

Method: 实施多种QML算法，包括量子神经网络(QNN)、量子支持向量机(QSVM)和混合量子卷积神经网络(QCNN)，使用入侵数据集(150样本) 和ObfuscatedMalMem2022数据集(58,596样本)进行实验分析。

Result: QML方法表现优异，QNN达到95%准确率，QSVM达到94%准确率，利用量子相干和缘绕原理准确识别传统方法无法检测的复杂模式。

Conclusion: 研究提出了一个新的实时电子病毒分析框架，结合量子特征提取和可解释AI方法，为量子决策过程提供了可解释性见解。

Abstract: This study explores the application of quantum machine learning (QML)
algorithms to enhance cybersecurity threat detection, particularly in the
classification of malware and intrusion detection within high-dimensional
datasets. Classical machine learning approaches encounter limitations when
dealing with intricate, obfuscated malware patterns and extensive network
intrusion data. To address these challenges, we implement and evaluate various
QML algorithms, including Quantum Neural Networks (QNN), Quantum Support Vector
Machines (QSVM), and hybrid Quantum Convolutional Neural Networks (QCNN) for
malware detection tasks. Our experimental analysis utilized two datasets: the
Intrusion dataset, comprising 150 samples with 56 memory-based features derived
from Volatility framework analysis, and the ObfuscatedMalMem2022 dataset,
containing 58,596 samples with 57 features representing benign and malicious
software. Remarkably, our QML methods demonstrated superior performance
compared to classical approaches, achieving accuracies of 95% for QNN and 94%
for QSVM. These quantum-enhanced methods leveraged quantum superposition and
entanglement principles to accurately identify complex patterns within highly
obfuscated malware samples that were imperceptible to classical methods. To
further advance malware analysis, we propose a novel real-time malware analysis
framework that incorporates Quantum Feature Extraction using Quantum Fourier
Transform, Quantum Feature Maps, and Classification using Variational Quantum
Circuits. This system integrates explainable AI methods, including GradCAM++
and ScoreCAM algorithms, to provide interpretable insights into the quantum
decision-making processes.

</details>


### [36] [Privacy Preservation and Identity Tracing Prevention in AI-Driven Eye Tracking for Interactive Learning Environments](https://arxiv.org/abs/2509.05376)
*Abdul Rehman,Are Dæhlen,Ilona Heldal,Jerry Chun-wei Lin*

Main category: cs.CR

TL;DR: 眼动跟踪技术在教育环境中存在隐私风险，本文提出了一种两阶段隐私保护框架，通过数据匿名化、联邦学习和安全身份管理系统来防止身份追溯，同时保持诊断分类的准确性。


<details>
  <summary>Details</summary>
Motivation: 眼动跟踪技术虽然能够帮助理解神经发育障碍和识别追踪，但存在严重隐私风险，容易泄露敏感个人信息。需要在保持教育价值的同时保护用户隐私。

Method: 提出两阶段隐私保护框架：第一阶段测试四种情景下的身份追溯风险；第二阶段通过联邦学习、数据匿名化、假ID系统和管理员专用访问控制来建立安全身份管理系统。

Result: 第一阶段：场景1诊断预测准确率99.3%，场景2学生ID预测51c6确率63%，场景3随机数据ID预测51c6确率99.7%，场景4成功分配新学生ID。第二阶段：成功防止身份追溯，建立安全身份管理系统，整体准确率达99.40%。

Conclusion: 该框架能够有效防止眼动跟踪数据中的身份追溯问题，同时保持了诊断分类的高准确性，为教育AI应用中的隐私保护提供了可行的解决方案。

Abstract: Eye-tracking technology can aid in understanding neurodevelopmental disorders
and tracing a person's identity. However, this technology poses a significant
risk to privacy, as it captures sensitive information about individuals and
increases the likelihood that data can be traced back to them. This paper
proposes a human-centered framework designed to prevent identity backtracking
while preserving the pedagogical benefits of AI-powered eye tracking in
interactive learning environments. We explore how real-time data anonymization,
ethical design principles, and regulatory compliance (such as GDPR) can be
integrated to build trust and transparency. We first demonstrate the potential
for backtracking student IDs and diagnoses in various scenarios using serious
game-based eye-tracking data. We then provide a two-stage privacy-preserving
framework that prevents participants from being tracked while still enabling
diagnostic classification. The first phase covers four scenarios: I) Predicting
disorder diagnoses based on different game levels. II) Predicting student IDs
based on different game levels. III) Predicting student IDs based on randomized
data. IV) Utilizing K-Means for out-of-sample data. In the second phase, we
present a two-stage framework that preserves privacy. We also employ Federated
Learning (FL) across multiple clients, incorporating a secure identity
management system with dummy IDs and administrator-only access controls. In the
first phase, the proposed framework achieved 99.3% accuracy for scenario 1, 63%
accuracy for scenario 2, and 99.7% accuracy for scenario 3, successfully
identifying and assigning a new student ID in scenario 4. In phase 2, we
effectively prevented backtracking and established a secure identity management
system with dummy IDs and administrator-only access controls, achieving an
overall accuracy of 99.40%.

</details>


### [37] [ThreatGPT: An Agentic AI Framework for Enhancing Public Safety through Threat Modeling](https://arxiv.org/abs/2509.05379)
*Sharif Noor Zisad,Ragib Hasan*

Main category: cs.CR

TL;DR: ThreatGPT是一个AI助手，帮助非网络安全专家分析公共安全系统的威胁，使用STRIDE、MITRE ATT&CK等框架生成智能威胁模型。


<details>
  <summary>Details</summary>
Motivation: 随着智慧城市系统日益复杂，安全威胁风险增加，需要让工程师、安全官员等非网络安全专家也能理解和分析系统威胁。

Method: 基于少样本学习的AI代理，用户描述系统组件后，可选择使用STRIDE、MITRE ATT&CK、CVE、NIST或CISA等框架进行分析，生成相关威胁模型。

Result: 开发出ThreatGPT工具，能够识别潜在威胁、攻击者利用方式以及防护措施，适应不同用户需求。

Conclusion: ThreatGPT将AI与人类判断结合，使公共系统更安全，不仅分析威胁，还赋能用户更快、更智能、更自信地理解和应对威胁。

Abstract: As our cities and communities become smarter, the systems that keep us safe,
such as traffic control centers, emergency response networks, and public
transportation, also become more complex. With this complexity comes a greater
risk of security threats that can affect not just machines but real people's
lives. To address this challenge, we present ThreatGPT, an agentic Artificial
Intelligence (AI) assistant built to help people whether they are engineers,
safety officers, or policy makers to understand and analyze threats in public
safety systems. Instead of requiring deep cybersecurity expertise, it allows
users to simply describe the components of a system they are concerned about,
such as login systems, data storage, or communication networks. Then, with the
click of a button, users can choose how they want the system to be analyzed by
using popular frameworks such as STRIDE, MITRE ATT&CK, CVE reports, NIST, or
CISA. ThreatGPT is unique because it does not just provide threat information,
but rather it acts like a knowledgeable partner. Using few-shot learning, the
AI learns from examples and generates relevant smart threat models. It can
highlight what might go wrong, how attackers could take advantage, and what can
be done to prevent harm. Whether securing a city's infrastructure or a local
health service, this tool adapts to users' needs. In simple terms, ThreatGPT
brings together AI and human judgment to make our public systems safer. It is
designed not just to analyze threats, but to empower people to understand and
act on them, faster, smarter, and with more confidence.

</details>


### [38] [Behind the Mask: Benchmarking Camouflaged Jailbreaks in Large Language Models](https://arxiv.org/abs/2509.05471)
*Youjia Zheng,Mohammad Zandsalimy,Shanu Sushmita*

Main category: cs.CR

TL;DR: 本文研究LLMs中的伪装越狱攻击，构建了包含500个示例的基准数据集，并提出多维度评估框架，发现LLMs在面对伪装恶意提示时安全性显著下降。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型面临新型的伪装越狱攻击，这种攻击将恶意意图隐藏在看似良性的语言中，逃避现有安全机制，需要深入研究其构造方法和影响。

Method: 构建了包含400个有害提示和100个良性提示的基准数据集，提出了包含安全感知、技术可行性、实施保障等7个维度的多层面评估框架。

Result: 研究发现LLMs在良性输入下表现良好，但在面对伪装越狱攻击时安全性和性能显著下降，暴露出严重的安全漏洞。

Conclusion: 当前LLMs存在普遍的安全脆弱性，迫切需要更细致和自适应的安全策略来确保模型在现实应用中的负责任和稳健部署。

Abstract: Large Language Models (LLMs) are increasingly vulnerable to a sophisticated
form of adversarial prompting known as camouflaged jailbreaking. This method
embeds malicious intent within seemingly benign language to evade existing
safety mechanisms. Unlike overt attacks, these subtle prompts exploit
contextual ambiguity and the flexible nature of language, posing significant
challenges to current defense systems. This paper investigates the construction
and impact of camouflaged jailbreak prompts, emphasizing their deceptive
characteristics and the limitations of traditional keyword-based detection
methods. We introduce a novel benchmark dataset, Camouflaged Jailbreak Prompts,
containing 500 curated examples (400 harmful and 100 benign prompts) designed
to rigorously stress-test LLM safety protocols. In addition, we propose a
multi-faceted evaluation framework that measures harmfulness across seven
dimensions: Safety Awareness, Technical Feasibility, Implementation Safeguards,
Harmful Potential, Educational Value, Content Quality, and Compliance Score.
Our findings reveal a stark contrast in LLM behavior: while models demonstrate
high safety and content quality with benign inputs, they exhibit a significant
decline in performance and safety when confronted with camouflaged jailbreak
attempts. This disparity underscores a pervasive vulnerability, highlighting
the urgent need for more nuanced and adaptive security strategies to ensure the
responsible and robust deployment of LLMs in real-world applications.

</details>


### [39] [What is Cybersecurity in Space?](https://arxiv.org/abs/2509.05496)
*Charbel Mattar,Jacques Bou Abdo,Abdallah Makhoul,Benoit Piranda,Jacques Demerjian*

Main category: cs.CR

TL;DR: 这篇论文分析了空间系统的网络安全漏洞，指出了11个研究空白区域，并提出了以多代理AI和主动防御为核心的五年路线图。


<details>
  <summary>Details</summary>
Motivation: 卫星、无人机和5G空间链路现在支撑着空中交通、金融和气象等关键服务，但这些系统设计时并未考虑现代网络威胁，存在重大安全风险。

Method: 识别和分析了11个关键研究缺口，包括安全路由、机载入侵检测、恢复方法、可信供应链、后量子加密等，并提出以多代理AI为核心的解决方案。

Result: 给出了每个挑战的详细分析、重要性说明和研究问题，构建了一个完整的空间网络安全研究框架。

Conclusion: 提出了一个五年路线图，包括后量子和QKD飞行试验、开政网络安全测试范围、漏洞共享机制和早期多代理部署，以实现从反应式补丁向主动弹性防御的转变。

Abstract: Satellites, drones, and 5G space links now support
  critical services such as air traffic, finance, and weather. Yet most
  were not built to resist modern cyber threats. Ground stations
  can be breached, GPS jammed, and supply chains compromised,
  while no shared list of vulnerabilities or safe testing range exists.
  This paper maps eleven research gaps, including secure
  routing, onboard intrusion detection, recovery methods, trusted
  supply chains, post-quantum encryption, zero-trust architectures,
  and real-time impact monitoring. For each, we outline the
  challenge, why it matters, and a guiding research question. We
  also highlight an agentic (multi-agent) AI approach where small,
  task-specific agents share defense tasks onboard instead of one
  large model.
  Finally, we propose a five-year roadmap: post-quantum and
  QKD flight trials, open cyber-ranges, clearer vulnerability shar ing, and
early multi-agent deployments. These steps move space
  cybersecurity from reactive patching toward proactive resilience.

</details>


### [40] [Secure and Efficient $L^p$-Norm Computation for Two-Party Learning Applications](https://arxiv.org/abs/2509.05552)
*Ali Arastehfard,Weiran Liu,Joshua Lee,Bingyu Liu,Xuegang Ban,Yuan Hong*

Main category: cs.CR

TL;DR: 这篇论文提出了第一个综合性的安全两方L^p范数计算框架Crypto-L^p，支持L^1、L^2和L^∞范数，在运行时和通信开销方面显著优于现有方案。


<details>
  <summary>Details</summary>
Motivation: 现有加密系统缺乏通用的安全L^p范数计算框架，主要集中在L^2范数而忽视了L^1和L^∞等常用范数在机器学习和位置服务中的应用需求。

Method: 设计了Crypto-L^p框架，通过为安全L^p范数计算采用专门设计的加密协议来优化性能，支持L^1、L^2和L^∞三种范数计算。

Result: 在运行时方面实现了82倍、271倍和42倍的提升，通信开销减少36倍、4倍和21倍（分别对应p=1,2,∞）；在安全机器学习推理中通信成本降低3倍。

Conclusion: Crypto-L^p框架为安全L^p范数计算提供了高效的综合解决方案，具有显著的性能优势和广泛的实际应用前景。

Abstract: Secure norm computation is becoming increasingly important in many real-world
learning applications. However, existing cryptographic systems often lack a
general framework for securely computing the $L^p$-norm over private inputs
held by different parties. These systems often treat secure norm computation as
a black-box process, neglecting to design tailored cryptographic protocols that
optimize performance. Moreover, they predominantly focus on the $L^2$-norm,
paying little attention to other popular $L^p$-norms, such as $L^1$ and
$L^\infty$, which are commonly used in practice, such as machine learning tasks
and location-based services.
  To our best knowledge, we propose the first comprehensive framework for
secure two-party $L^p$-norm computations ($L^1$, $L^2$, and $L^\infty$),
denoted as \mbox{Crypto-$L^p$}, designed to be versatile across various
applications. We have designed, implemented, and thoroughly evaluated our
framework across a wide range of benchmarking applications, state-of-the-art
(SOTA) cryptographic protocols, and real-world datasets to validate its
effectiveness and practical applicability. In summary, \mbox{Crypto-$L^p$}
outperforms prior works on secure $L^p$-norm computation, achieving $82\times$,
$271\times$, and $42\times$ improvements in runtime while reducing
communication overhead by $36\times$, $4\times$, and $21\times$ for $p=1$, $2$,
and $\infty$, respectively. Furthermore, we take the first step in adapting our
Crypto-$L^p$ framework for secure machine learning inference, reducing
communication costs by $3\times$ compared to SOTA systems while maintaining
comparable runtime and accuracy.

</details>


### [41] [Cross-Service Threat Intelligence in LLM Services using Privacy-Preserving Fingerprints](https://arxiv.org/abs/2509.05608)
*Waris Gill,Natalie Isak,Matthew Dressman*

Main category: cs.CR

TL;DR: BinaryShield是一个隐私保护的威胁情报系统，通过PII脱敏、语义嵌入、二进制量化和随机响应机制生成不可逆指纹，实现跨合规边界的安全攻击指纹共享，在保持隐私的同时有效检测提示注入攻击。


<details>
  <summary>Details</summary>
Motivation: 企业部署多个LLM服务处理数十亿查询，但由于监管合规边界限制，无法共享提示注入攻击的威胁情报，导致攻击在一个服务中被检测后，在其他服务中可能持续数月未被发现。

Method: 采用独特的处理流程：PII脱敏、语义嵌入、二进制量化和随机响应机制，生成非可逆的指纹，在保护隐私的同时保留攻击模式特征。

Result: F1分数达到0.94，显著优于隐私保护基线SimHash（0.77），同时实现64倍存储减少和38倍更快的相似性搜索。

Conclusion: BinaryShield成功解决了LLM服务间威胁情报共享的隐私合规问题，为跨合规边界的攻击检测提供了有效的隐私保护解决方案。

Abstract: The widespread deployment of LLMs across enterprise services has created a
critical security blind spot. Organizations operate multiple LLM services
handling billions of queries daily, yet regulatory compliance boundaries
prevent these services from sharing threat intelligence about prompt injection
attacks, the top security risk for LLMs. When an attack is detected in one
service, the same threat may persist undetected in others for months, as
privacy regulations prohibit sharing user prompts across compliance boundaries.
  We present BinaryShield, the first privacy-preserving threat intelligence
system that enables secure sharing of attack fingerprints across compliance
boundaries. BinaryShield transforms suspicious prompts through a unique
pipeline combining PII redaction, semantic embedding, binary quantization, and
randomized response mechanism to potentially generate non-invertible
fingerprints that preserve attack patterns while providing privacy. Our
evaluations demonstrate that BinaryShield achieves an F1-score of 0.94,
significantly outperforming SimHash (0.77), the privacy-preserving baseline,
while achieving 64x storage reduction and 38x faster similarity search compared
to dense embeddings.

</details>


### [42] [FuzzBox: Blending Fuzzing into Emulation for Binary-Only Embedded Targets](https://arxiv.org/abs/2509.05643)
*Carmine Cesarano,Roberto Natella*

Main category: cs.CR

TL;DR: FuzzBox是一个基于仿真的模糊测试框架，通过动态插桩解决工业系统中无法获取源代码和专用编译器的问题，在MILS hypervisor和IoT固件中验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 传统覆盖引导的模糊测试需要编译时插桩，但工业系统通常使用专有闭源编译器且无法获取源代码，导致难以应用模糊测试技术。

Method: FuzzBox将仿真与模糊测试结合，在虚拟化环境中动态执行代码插桩，实现模糊输入注入、故障检测和覆盖分析，无需源代码重新编译和硬件依赖。

Result: 实验证明FuzzBox在专有MILS hypervisor中有效，并在商业IoT固件中展示了广泛的移植性。

Conclusion: FuzzBox通过动态插桩方法成功解决了工业系统中应用模糊测试的技术障碍，具有很好的实用性和可移植性。

Abstract: Coverage-guided fuzzing has been widely applied to address zero-day
vulnerabilities in general-purpose software and operating systems. This
approach relies on instrumenting the target code at compile time. However,
applying it to industrial systems remains challenging, due to proprietary and
closed-source compiler toolchains and lack of access to source code. FuzzBox
addresses these limitations by integrating emulation with fuzzing: it
dynamically instruments code during execution in a virtualized environment, for
the injection of fuzz inputs, failure detection, and coverage analysis, without
requiring source code recompilation and hardware-specific dependencies. We show
the effectiveness of FuzzBox through experiments in the context of a
proprietary MILS (Multiple Independent Levels of Security) hypervisor for
industrial applications. Additionally, we analyze the applicability of FuzzBox
across commercial IoT firmware, showcasing its broad portability.

</details>


### [43] [SEASONED: Semantic-Enhanced Self-Counterfactual Explainable Detection of Adversarial Exploiter Contracts](https://arxiv.org/abs/2509.05681)
*Xng Ai,Shudan Lin,Zecheng Li,Kai Zhou,Bixin Li,Bin Xiao*

Main category: cs.CR

TL;DR: SEASONED是一个用于检测DeFi攻击中对抗性利用合约(AECs)的自解释框架，通过语义关系图和自反事实可解释检测器实现高效检测和攻击逻辑解释


<details>
  <summary>Details</summary>
Motivation: 现有检测方法难以捕捉语义依赖关系且缺乏可解释性，导致AEC分析存在关键知识空白，需要开发能够主动识别威胁并提供解释的检测方案

Method: 从合约字节码提取语义信息构建语义关系图(SRG)，使用自反事实可解释检测器(SCFED)对SRG进行分类并生成突出核心攻击逻辑的解释

Result: 理论和实验结果证明SEASONED具有出色的检测性能、鲁棒性、泛化能力和数据效率学习能力，并发布了包含359个AEC的新数据集

Conclusion: SEASONED框架有效解决了AEC检测中的语义依赖捕捉和可解释性问题，为DeFi安全提供了强有力的检测工具和数据集支持

Abstract: Decentralized Finance (DeFi) attacks have resulted in significant losses,
often orchestrated through Adversarial Exploiter Contracts (AECs) that exploit
vulnerabilities in victim smart contracts. To proactively identify such
threats, this paper targets the explainable detection of AECs.
  Existing detection methods struggle to capture semantic dependencies and lack
interpretability, limiting their effectiveness and leaving critical knowledge
gaps in AEC analysis. To address these challenges, we introduce SEASONED, an
effective, self-explanatory, and robust framework for AEC detection.
  SEASONED extracts semantic information from contract bytecode to construct a
semantic relation graph (SRG), and employs a self-counterfactual explainable
detector (SCFED) to classify SRGs and generate explanations that highlight the
core attack logic. SCFED further enhances robustness, generalizability, and
data efficiency by extracting representative information from these
explanations. Both theoretical analysis and experimental results demonstrate
the effectiveness of SEASONED, which showcases outstanding detection
performance, robustness, generalizability, and data efficiency learning
ability. To support further research, we also release a new dataset of 359
AECs.

</details>


### [44] [KnowHow: Automatically Applying High-Level CTI Knowledge for Interpretable and Accurate Provenance Analysis](https://arxiv.org/abs/2509.05698)
*Yuhan Meng,Shaofei Li,Jiaping Gui,Peng Jiang,Ding Li*

Main category: cs.CR

TL;DR: KnowHow是一个CTI知识驱动的在线溯源分析方法，能够自动将自然语言CTI报告中的高级攻击知识应用于低级系统事件检测，通过gIoC表示和语义匹配来检测APT攻击


<details>
  <summary>Details</summary>
Motivation: 解决CTI报告中高级自然语言知识与低级安全日志之间的语义鸿沟问题，避免人工方法的劳动密集和易出错

Method: 提出gIoC攻击知识表示方法，通过将系统标识符提升为自然语言术语来匹配系统事件和攻击技术，并基于时间逻辑推理攻击步骤

Result: 在开源和工业数据集中准确检测所有16个APT活动，最多减少90%的节点级误报，同时保持更高的节点级召回率

Conclusion: KnowHow能够有效弥合语义鸿沟，自动应用CTI知识进行APT攻击检测，在准确性和鲁棒性方面优于现有方法

Abstract: High-level natural language knowledge in CTI reports, such as the ATT&CK
framework, is beneficial to counter APT attacks. However, how to automatically
apply the high-level knowledge in CTI reports in realistic attack detection
systems, such as provenance analysis systems, is still an open problem. The
challenge stems from the semantic gap between the knowledge and the low-level
security logs: while the knowledge in CTI reports is written in natural
language, attack detection systems can only process low-level system events
like file accesses or network IP manipulations. Manual approaches can be
labor-intensive and error-prone.
  In this paper, we propose KnowHow, a CTI-knowledge-driven online provenance
analysis approach that can automatically apply high-level attack knowledge from
CTI reports written in natural languages to detect low-level system events. The
core of KnowHow is a novel attack knowledge representation, gIoC, that
represents the subject, object, and actions of attacks. By lifting system
identifiers, such as file paths, in system events to natural language terms,
KnowHow can match system events to gIoC and further match them to techniques
described in natural languages. Finally, based on the techniques matched to
system events, KnowHow reasons about the temporal logic of attack steps and
detects potential APT attacks in system events. Our evaluation shows that
KnowHow can accurately detect all 16 APT campaigns in the open-source and
industrial datasets, while existing approaches all introduce large numbers of
false positives. Meanwhile, our evaluation also shows that KnowHow reduces at
most 90% of node-level false positives while having a higher node-level recall
and is robust against several unknown attacks and mimicry attacks.

</details>


### [45] [VehiclePassport: A GAIA-X-Aligned, Blockchain-Anchored Privacy-Preserving, Zero-Knowledge Digital Passport for Smart Vehicles](https://arxiv.org/abs/2509.06133)
*Pradyumna Kaushal*

Main category: cs.CR

TL;DR: VehiclePassport是一个基于区块链和零知识证明的车辆数字护照系统，用于安全、隐私保护的车辆生命周期记录验证


<details>
  <summary>Details</summary>
Motivation: 现代车辆的寿命周期记录分散在OEM、车主和服务中心之间，难以验证且容易发生欺诈，需要一种可信的验证机制

Method: 采用GAIA-X对齐的数字护照架构，基于区块链锚定，使用零知识证明（ZKPs）进行隐私保护验证，支持选择性披露和短时JWTs

Result: 开源参考栈在Polygon zkEVM上以<$0.02每事件成本锚定哈希，验证证明时间<10ms，可扩展到数百万车辆

Conclusion: 该架构消除了基于纸张的KYC，确保GDPR合规的可追溯性，为保险、转售和监管应用建立了无信任基础

Abstract: Modern vehicles accumulate fragmented lifecycle records across OEMs, owners,
and service centers that are difficult to verify and prone to fraud. We propose
VehiclePassport, a GAIA-X-aligned digital passport anchored on blockchain with
zero-knowledge proofs (ZKPs) for privacy-preserving verification.
VehiclePassport immutably commits to manufacturing, telemetry, and service
events while enabling selective disclosure via short-lived JWTs and Groth16
proofs. Our open-source reference stack anchors hashes on Polygon zkEVM at
<$0.02 per event, validates proofs in <10 ms, and scales to millions of
vehicles. This architecture eliminates paper-based KYC, ensures GDPR-compliant
traceability, and establishes a trustless foundation for insurance, resale, and
regulatory applications in global mobility data markets.

</details>


### [46] [Larger-scale Nakamoto-style Blockchains Offer Better Security](https://arxiv.org/abs/2509.05708)
*Junjie Hu,Na Ruan*

Main category: cs.CR

TL;DR: 本文提出了双延迟框架来重新分析Nakamoto区块链安全性，发现传统模型高估了恶意节点协调能力，实际安全阈值可超过51%，且随着网络规模扩大，恶意攻击概率趋近于零。


<details>
  <summary>Details</summary>
Motivation: 传统区块链安全模型假设恶意节点能够即时同步，忽略了内部通信延迟对安全性的关键影响，这导致对敌手协调能力的高估。

Method: 采用双延迟框架：静态延迟模型通过M/D/1排队模型量化敌手通信延迟对私有链增长率的约束；动态延迟模型整合概率腐败和规模相关延迟来表征总敌手延迟窗口。

Result: 研究发现安全阈值β*随敌手延迟Δa增加而提高，当Δa>Δ时可突破经典的51%边界；渐近分析显示敌手能力随网络规模线性衰减，当n→∞时β≤β*的概率趋近于1。

Conclusion: 通过揭示网络规模、通信延迟和能力稀释之间的相互作用，为优化共识协议和评估大规模Nakamoto区块链的鲁棒性提供了理论基础。

Abstract: Traditional security models for Nakamoto-style blockchains overestimate
adversarial coordination by assuming instantaneous synchronization among
malicious nodes, neglecting the critical impact of internal communication
delays on security. This paper introduces a dual-delay framework to revisit
security analysis, addressing this oversight through two key innovations.
First, the static delay model quantifies how adversarial communication delays
(\(\Delta_a\)) constrain the effective growth rate of private chains, derived
via an M/D/1 queuing model as \(\lambda_{eff} = \lambda_a / (1 + \lambda_a
\Delta_a)\). This model reveals that the security threshold (\(\beta^*\)), the
maximum adversarial power the system tolerates, increases with \(\Delta_a\),
even exceeding the classic 51\% boundary when \(\Delta_a \textgreater \Delta\)
(honest nodes' delay), breaking the long-standing 50\% assumption. Second, the
dynamic delay model integrates probabilistic corruption and scale-dependent
delays to characterize the total adversarial delay window (\(\Delta_{total} =
\Delta(n) e^{-k\beta} + c \log(1 + \beta n)\)), where \(\Delta(n) \in
\Theta(\log n)\) captures honest nodes' logarithmic delay growth. Asymptotic
analysis shows adversarial power decays linearly with network scale, ensuring
the probability of \(\beta \leq \beta^*\) approaches 1 as \(n \to \infty\). By
exposing the interplay between network scale, communication delays, and power
dilution, we provide a theoretical foundation for optimizing consensus
protocols and assessing robustness in large-scale Nakamoto-style blockchains.

</details>


### [47] [Reasoning Introduces New Poisoning Attacks Yet Makes Them More Complicated](https://arxiv.org/abs/2509.05739)
*Hanna Foerster,Ilia Shumailov,Yiren Zhao,Harsh Chaudhari,Jamie Hayes,Robert Mullins,Yarin Gal*

Main category: cs.CR

TL;DR: 这篇论文研究了大语言模型的数据毒化攻击，提出了分解理由毒化攻击方法，发现虽然可注入毒化，但模型通过理性能力和架构分离具有一定的后门程序稳健性。


<details>
  <summary>Details</summary>
Motivation: 早期研究显示LLM容易受到数据毒化攻击，现代LLM加入了逐步思考功能，扩大了攻击面，需要研究更隐蔓的毒化攻击方法。

Method: 提出"分解理由毒化"方法，攻击者只修改理由路径，保持提示和最终答案清洁，并将触发器分散到多个无害组件中。

Result: 虽然可以注入分解毒化，但可靠激活它们改变最终答案非常困难，因为模型经常能够从思考过程中的后门程序中恢复。

Conclusion: 高级LLM的理性能力和理由-答案生成的架构分离产生了一种出现的后门程序稳健性，使得数据毒化攻击更加困难。

Abstract: Early research into data poisoning attacks against Large Language Models
(LLMs) demonstrated the ease with which backdoors could be injected. More
recent LLMs add step-by-step reasoning, expanding the attack surface to include
the intermediate chain-of-thought (CoT) and its inherent trait of decomposing
problems into subproblems. Using these vectors for more stealthy poisoning, we
introduce ``decomposed reasoning poison'', in which the attacker modifies only
the reasoning path, leaving prompts and final answers clean, and splits the
trigger across multiple, individually harmless components.
  Fascinatingly, while it remains possible to inject these decomposed poisons,
reliably activating them to change final answers (rather than just the CoT) is
surprisingly difficult. This difficulty arises because the models can often
recover from backdoors that are activated within their thought processes.
Ultimately, it appears that an emergent form of backdoor robustness is
originating from the reasoning capabilities of these advanced LLMs, as well as
from the architectural separation between reasoning and final answer
generation.

</details>


### [48] [Tell-Tale Watermarks for Explanatory Reasoning in Synthetic Media Forensics](https://arxiv.org/abs/2509.05753)
*Ching-Chun Chang,Isao Echizen*

Main category: cs.CR

TL;DR: 这篇论文提出了一种可解释的告诉水印系统，用于追踪合成媒体的生命周期变形，通过水印的可解释迹踪来推断多重变换的组合参数空间


<details>
  <summary>Details</summary>
Motivation: 合成媒体的出现模糊了真实与假悠的界限，引发信息病毒流行，同时多重编辑应用使得图像变形分析变得复杂，需要进行追溯性调查来重建事件链

Method: 开发告诉水印系统，为不同类型的变形量身定制水印，这些水印在变形过程中会产生可解释的迹踪，然后通过解释性推理来推断复合变形的最可能参数组合

Result: 实验评估证明了告诉水印系统在保真度、同步性和可追踪性方面的有效性

Conclusion: 告诉水印系统为合成媒体的变形分析提供了一种可解释的方法，能够通过水印迹踪来推断多重变换的参数空间，为数字图像的证据调查提供了更深入的见解

Abstract: The rise of synthetic media has blurred the boundary between reality and
fabrication under the evolving power of artificial intelligence, fueling an
infodemic that erodes public trust in cyberspace. For digital imagery, a
multitude of editing applications further complicates the forensic analysis,
including semantic edits that alter content, photometric adjustments that
recalibrate colour characteristics, and geometric projections that reshape
viewpoints. Collectively, these transformations manipulate and control
perceptual interpretation of digital imagery. This susceptibility calls for
forensic enquiry into reconstructing the chain of events, thereby revealing
deeper evidential insight into the presence or absence of criminal intent. This
study seeks to address an inverse problem of tracing the underlying generation
chain that gives rise to the observed synthetic media. A tell-tale watermarking
system is developed for explanatory reasoning over the nature and extent of
transformations across the lifecycle of synthetic media. Tell-tale watermarks
are tailored to different classes of transformations, responding in a manner
that is neither strictly robust nor fragile but instead interpretable. These
watermarks function as reference clues that evolve under the same
transformation dynamics as the carrier media, leaving interpretable traces when
subjected to transformations. Explanatory reasoning is then performed to infer
the most plausible account across the combinatorial parameter space of
composite transformations. Experimental evaluations demonstrate the validity of
tell-tale watermarking with respect to fidelity, synchronicity and
traceability.

</details>


### [49] [Exploit Tool Invocation Prompt for Tool Behavior Hijacking in LLM-Based Agentic System](https://arxiv.org/abs/2509.05755)
*Yu Liu,Yuchong Xie,Mingyu Luo,Zesen Liu,Zhixiang Zhang,Kaikai Zhang,Zongjie Li,Ping Chen,Shuai Wang,Dongdong She*

Main category: cs.CR

TL;DR: LLM基于代理系统中的工具调用提示（TIP）存在严重安全漏洞，可导致远程代码执行和拒绝服务攻击。这项研究揭露了主流LLM系统的弱点，并提出防御机制。


<details>
  <summary>Details</summary>
Motivation: 工具调用提示（TIP）在LLM基于代理系统中致关重要，负责确保工具使用的安全性和正确性，但其安全问题一直被忽视。

Method: 通过系统化的TIP利甦工作流程（TEW），演示如何通过操纵工具调用来篡改外部工具行为，对主流LLM系统进行安全漏洞分析。

Result: 发现Cursor、Claude Code等主要LLM基于系统存在严重安全漏洞，可能造成远程代码执行（RCE）和拒绝服务（DoS）攻击。

Conclusion: 工具调用提示的安全风险需要立即关注，研究提出了相应的防御机制来提升LLM基于代理系统的安全性。

Abstract: LLM-based agentic systems leverage large language models to handle user
queries, make decisions, and execute external tools for complex tasks across
domains like chatbots, customer service, and software engineering. A critical
component of these systems is the Tool Invocation Prompt (TIP), which defines
tool interaction protocols and guides LLMs to ensure the security and
correctness of tool usage. Despite its importance, TIP security has been
largely overlooked. This work investigates TIP-related security risks,
revealing that major LLM-based systems like Cursor, Claude Code, and others are
vulnerable to attacks such as remote code execution (RCE) and denial of service
(DoS). Through a systematic TIP exploitation workflow (TEW), we demonstrate
external tool behavior hijacking via manipulated tool invocations. We also
propose defense mechanisms to enhance TIP security in LLM-based agentic
systems.

</details>


### [50] [Secure and Trustful Cross-domain Communication with Decentralized Identifiers in 5G and Beyond](https://arxiv.org/abs/2509.05797)
*Hai Dinh-Tuan,Sandro Rodriguez Garzon,Jianeng Fu*

Main category: cs.CR

TL;DR: 本文提出使用W3C标准的去中心化标识符(DID)来建立5G及更高代移动网络中网络功能间的安全信任通信通道，进行了协议比较和性能评估。


<details>
  <summary>Details</summary>
Motivation: 未来移动网络需要安全可信的通信方式来支持不同网络域核心组件间的动态交互。

Method: 提出一种新的通信代理，集成到5G标准化网络功能，使用DID基于应用层传输协议确保跨域交互的保密性、完整性和真实性。

Result: 对比分析显示最新DID协议版本具有更好的兼容性优势；与传统TCP/TLS相比，DID基于通信在通信开销方面有性能损失，但能够提高通信安全性。

Conclusion: DID基于通信在未来移动网络中具有潜力，但需要在性能优化方面进一步改进。

Abstract: In the evolving landscape of future mobile networks, there is a critical need
for secure and trustful communication modalities to support dynamic
interactions among core network components of different network domains. This
paper proposes the application of W3C-endorsed Decentralized Identifiers (DIDs)
to establish secure and trustful communication channels among network functions
in 5G and subsequent generations. A new communication agent is introduced that
integrates seamlessly with 5G-standardized network functions and utilizes a
DID-based application layer transport protocol to ensure confidentiality,
integrity, and authenticity for cross-domain interactions. A comparative
analysis of the two different versions of the DID-based communication protocol
for inter network function communication reveals compatibility advantages of
the latest protocol iteration. Furthermore, a comprehensive evaluation of the
communication overhead caused by both protocol iterations compared to
traditional TCP/TLS shows the benefits of using DIDs to improve communication
security, albeit with performance loses compared to TCP/TLS. These results
uncover the potential of DID-based communication for future mobile networks but
also point out areas for optimization.

</details>


### [51] [Decoding Latent Attack Surfaces in LLMs: Prompt Injection via HTML in Web Summarization](https://arxiv.org/abs/2509.05831)
*Ishaan Verma*

Main category: cs.CR

TL;DR: 研究发现LLM在网页内容摘要中容易受到HTML隐藏元素（如meta标签、aria-label等）的提示注入攻击，29%的注入样本成功影响了Llama 4 Scout的摘要输出。


<details>
  <summary>Details</summary>
Motivation: 随着LLM越来越多地集成到基于网页的内容摘要系统中，其对提示注入攻击的脆弱性成为一个紧迫的安全问题，特别是非可见HTML元素的潜在威胁尚未得到充分研究。

Method: 创建包含280个静态网页的数据集（一半干净，一半注入对抗性指令），使用浏览器自动化管道提取HTML和渲染文本，评估Llama 4 Scout和Gemma 9B IT模型在内容摘要任务中的表现，采用ROUGE-L和SBERT余弦相似度指标以及人工标注进行评估。

Result: 29%的注入样本导致Llama 4 Scout摘要出现明显变化，Gemma 9B IT的成功率为15%。两种模型都显示出对HTML隐藏元素注入攻击的脆弱性。

Conclusion: 研究揭示了LLM驱动网页管道中存在严重但被忽视的安全漏洞，隐藏的对抗性内容可以微妙地操纵模型输出，亟需开发稳健的缓解策略。

Abstract: Large Language Models (LLMs) are increasingly integrated into web-based
systems for content summarization, yet their susceptibility to prompt injection
attacks remains a pressing concern. In this study, we explore how non-visible
HTML elements such as <meta>, aria-label, and alt attributes can be exploited
to embed adversarial instructions without altering the visible content of a
webpage. We introduce a novel dataset comprising 280 static web pages, evenly
divided between clean and adversarial injected versions, crafted using diverse
HTML-based strategies. These pages are processed through a browser automation
pipeline to extract both raw HTML and rendered text, closely mimicking
real-world LLM deployment scenarios. We evaluate two state-of-the-art
open-source models, Llama 4 Scout (Meta) and Gemma 9B IT (Google), on their
ability to summarize this content. Using both lexical (ROUGE-L) and semantic
(SBERT cosine similarity) metrics, along with manual annotations, we assess the
impact of these covert injections. Our findings reveal that over 29% of
injected samples led to noticeable changes in the Llama 4 Scout summaries,
while Gemma 9B IT showed a lower, yet non-trivial, success rate of 15%. These
results highlight a critical and largely overlooked vulnerability in LLM driven
web pipelines, where hidden adversarial content can subtly manipulate model
outputs. Our work offers a reproducible framework and benchmark for evaluating
HTML-based prompt injection and underscores the urgent need for robust
mitigation strategies in LLM applications involving web content.

</details>


### [52] [Yours or Mine? Overwriting Attacks against Neural Audio Watermarking](https://arxiv.org/abs/2509.05835)
*Lingfeng Yao,Chenpei Huang,Shengyao Wang,Junpei Xue,Hanqing Guo,Jiang Liu,Phone Lin,Tomoaki Ohtsuki,Miao Pan*

Main category: cs.CR

TL;DR: 本文提出了一种针对神经音频水印系统的覆盖攻击方法，能够在白盒、灰盒和黑盒三种场景下有效覆盖原始水印，攻击成功率接近100%，暴露了现有音频水印系统的安全漏洞。


<details>
  <summary>Details</summary>
Motivation: 随着生成式音频模型的快速发展，AI生成的音频引发了版权侵权和错误信息传播的担忧。音频水印作为主动防御手段可以嵌入秘密信息进行版权保护和来源验证，但现有神经音频水印方法主要关注不可感知性和鲁棒性，忽视了其面对安全攻击的脆弱性。

Method: 开发了一种简单但强大的覆盖攻击方法，通过用伪造的水印覆盖合法的音频水印，使原始合法水印无法检测。根据攻击者拥有的水印信息，提出了白盒、灰盒和黑盒三种攻击类别，并在最先进的神经音频水印方法上进行了全面评估。

Result: 实验结果表明，提出的覆盖攻击能够有效破坏各种设置下的现有水印方案，攻击成功率接近100%。

Conclusion: 所提出的覆盖攻击的实用性和有效性暴露了现有神经音频水印系统的安全缺陷，强调了在未来音频水印设计中增强安全性的必要性。

Abstract: As generative audio models are rapidly evolving, AI-generated audios
increasingly raise concerns about copyright infringement and misinformation
spread. Audio watermarking, as a proactive defense, can embed secret messages
into audio for copyright protection and source verification. However, current
neural audio watermarking methods focus primarily on the imperceptibility and
robustness of watermarking, while ignoring its vulnerability to security
attacks. In this paper, we develop a simple yet powerful attack: the
overwriting attack that overwrites the legitimate audio watermark with a forged
one and makes the original legitimate watermark undetectable. Based on the
audio watermarking information that the adversary has, we propose three
categories of overwriting attacks, i.e., white-box, gray-box, and black-box
attacks. We also thoroughly evaluate the proposed attacks on state-of-the-art
neural audio watermarking methods. Experimental results demonstrate that the
proposed overwriting attacks can effectively compromise existing watermarking
schemes across various settings and achieve a nearly 100% attack success rate.
The practicality and effectiveness of the proposed overwriting attacks expose
security flaws in existing neural audio watermarking systems, underscoring the
need to enhance security in future audio watermarking designs.

</details>


### [53] [Multimodal Prompt Injection Attacks: Risks and Defenses for Modern LLMs](https://arxiv.org/abs/2509.05883)
*Andrew Yeo,Daeseon Choi*

Main category: cs.CR

TL;DR: 这篇论文通过对八款商业大语言模型的实验研究，系统评估了它们在提示注入攻击下的弱点，发现所有模型都存在可利用的漏洞，并建议采用输入标准化等额外防御措施。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在各行业的广泛部署，它们的安全风险日益显现，特别是提示注入和脱狱攻击。论文的动机是系统性地评估LLM在外部提示注入攻击下的漏洞，以确认安全风险并提出防御建议。

Method: 研究者对八款商业大语言模型进行了一系列实验，在没有额外清洗措施的情况下，仅依靠模型内置的安全保护机制进行测试。实验包括四类攻击方式：直接注入、间接（外部）注入、基于图像的注入以及提示泄漏。

Result: 实验结果显示所有测试模型都存在可利用的弱点。对比分析表明Claude 3模型显示出相对更好的稳健性，但仍然存在漏洞。研究证实了当前模型内置的安全保护机制不足以抵御外部提示注入攻击。

Conclusion: 大语言模型在提示注入攻击面前存在显著的安全漏洞。为了实现可靠的保护，必须采用额外的防御措施，例如输入标准化技术。这项研究突出了在部署LLM时加强安全措施的紧迫性。

Abstract: Large Language Models (LLMs) have seen rapid adoption in recent years, with
industries increasingly relying on them to maintain a competitive advantage.
These models excel at interpreting user instructions and generating human-like
responses, leading to their integration across diverse domains, including
consulting and information retrieval. However, their widespread deployment also
introduces substantial security risks, most notably in the form of prompt
injection and jailbreak attacks.
  To systematically evaluate LLM vulnerabilities -- particularly to external
prompt injection -- we conducted a series of experiments on eight commercial
models. Each model was tested without supplementary sanitization, relying
solely on its built-in safeguards. The results exposed exploitable weaknesses
and emphasized the need for stronger security measures. Four categories of
attacks were examined: direct injection, indirect (external) injection,
image-based injection, and prompt leakage. Comparative analysis indicated that
Claude 3 demonstrated relatively greater robustness; nevertheless, empirical
findings confirm that additional defenses, such as input normalization, remain
necessary to achieve reliable protection.

</details>


### [54] [Introduction to Number Theoretic Transform](https://arxiv.org/abs/2509.05884)
*Banhirup Sengupta,Peenal Gupta,Souvik Sengupta*

Main category: cs.CR

TL;DR: 本文介绍了数论变换(NTT)作为离散傅里叶变换的变体，在格密码学中用于高效多项式乘法，将复杂度从二次降低到拟线性


<details>
  <summary>Details</summary>
Motivation: 数论变换(NTT)在后量子密码学和同态加密中具有重要作用，特别是在格基密码学中需要高效的多项式运算

Method: 介绍了循环卷积、负循环卷积、NTT及其逆变换的快速算法，采用类似快速傅里叶变换(FFT)的方法

Result: 实现了多项式乘法复杂度的显著降低，从O(n²)降低到O(n log n)的拟线性复杂度

Conclusion: 快速NTT算法为格基密码学提供了高效的数学工具，特别是在多项式乘法运算方面具有重要应用价值

Abstract: The Number Theoretic Transform (NTT) can be regarded as a variant of the
Discrete Fourier Transform. NTT has been quite a powerful mathematical tool in
developing Post-Quantum Cryptography and Homomorphic Encryption. The Fourier
Transform essentially decomposes a signal into its frequencies. They are
traditionally sine or cosine waves. NTT works more over groups or finite fields
rather than on a continuous signal and polynomials work as the analog of sine
waves in case of NTT. Fast Fourier Trnasform (FFT) style NTT or fast NTT has
been proven to be useful in lattice-based cryptography due to its ability to
reduce the complexity of polynomial multiplication from quadratic to
quasilinear. We have introduced the concepts of cyclic, negacyclic convolutions
along with NTT and its inverse and their fast versions.

</details>


### [55] [MemTraceDB: Reconstructing MySQL User Activity Using ActiviTimeTrace Algorithm](https://arxiv.org/abs/2509.05891)
*Mahfuzul I. Nissan*

Main category: cs.CR

TL;DR: MemTraceDB是一个通过分析MySQL数据库进程内存快照来重建用户活动时间线的工具，能够绕过被篡改的磁盘日志，提供可靠的取证分析。


<details>
  <summary>Details</summary>
Motivation: 数据库审计日志和事务日志容易受到特权攻击者的篡改，内存分析提供了访问易失性证据的替代方案，即使日志文件被破坏也能获取真实用户活动信息。

Method: 使用ActiviTimeTrace算法从MySQL进程内存快照中系统提取和关联取证证据，包括用户连接和执行查询，并通过实验确定最佳内存快照采集频率。

Result: 发现MySQL查询堆栈具有约9,997条查询的有限操作容量，建立了数据驱动的公式来确定最佳内存快照采集频率，实现了独立于受损磁盘日志的取证重建。

Conclusion: MemTraceDB提供了一种可靠的方法来重建用户活动时间线，即使在磁盘日志被篡改的情况下也能进行有效的取证调查，为数据库取证提供了重要工具。

Abstract: Database audit and transaction logs are fundamental to forensic
investigations, but they are vulnerable to tampering by privileged attackers.
Malicious insiders or external threats with administrative access can alter,
purge, or temporarily disable logging mechanisms, creating significant blind
spots and rendering disk-based records unreliable. Memory analysis offers a
vital alternative, providing investigators direct access to volatile artifacts
that represent a ground-truth source of recent user activity, even when log
files have been compromised.
  This paper introduces MemTraceDB, a tool that reconstructs user activity
timelines by analyzing raw memory snapshots from the MySQL database process.
MemTraceDB utilizes a novel algorithm, ActiviTimeTrace, to systematically
extract and correlate forensic artifacts such as user connections and executed
queries. Through a series of experiments, I demonstrate MemTraceDB's
effectiveness and reveal a critical empirical finding: the MySQL query stack
has a finite operational capacity of approximately 9,997 queries. This
discovery allows me to establish a practical, data-driven formula for
determining the optimal frequency for memory snapshot collection, providing a
clear, actionable guideline for investigators. The result is a
forensically-sound reconstruction of user activity, independent of compromised
disk-based logs.

</details>


### [56] [Wrangling Entropy: Next-Generation Multi-Factor Key Derivation, Credential Hashing, and Credential Generation Functions](https://arxiv.org/abs/2509.05893)
*Colin Roberts,Vivek Nair,Dawn Song*

Main category: cs.CR

TL;DR: 提出了ESTMF密码分析框架来检测多因子密钥派生函数中的熵泄露漏洞，并基于此构建了更安全的MFKDF2方案


<details>
  <summary>Details</summary>
Motivation: 原始MFKDF方案在多轮调用中存在安全性退化问题，需要开发新的分析框架来识别和修复这些漏洞

Method: 开发了熵状态转移建模框架(ESTMF)来检测密码学函数中的熵泄露，并基于分析结果设计了MFKDF2新构造

Result: ESTMF成功识别了原始MFKDF的所有已知漏洞，MFKDF2被证明具有端到端安全性

Conclusion: MFKDF2支持更多认证因子和可用性特性，为未来KDF设计提供了可推广的最佳实践

Abstract: The Multi-Factor Key Derivation Function (MFKDF) offered a novel solution to
the classic problem of usable client-side key management by incorporating
multiple popular authentication factors into a key derivation process, but was
later shown to be vulnerable to cryptanalysis that degraded its security over
multiple invocations. In this paper, we present the Entropy State Transition
Modeling Framework (ESTMF), a novel cryptanalytic technique designed to reveal
pernicious leaks of entropy across multiple invocations of a cryptographic key
derivation or hash function, and show that it can be used to correctly identify
each of the known vulnerabilities in the original MFKDF construction. We then
use these findings to propose a new construction for ``MFKDF2,'' a
next-generation multi-factor key derivation function that can be proven to be
end-to-end secure using the ESTMF. Finally, we discuss how MFKDF2 can be
extended to support more authentication factors and usability features than the
previous MFKDF construction, and derive several generalizable best-practices
for the construction of new KDFs in the future.

</details>


### [57] [Dataset Ownership in the Era of Large Language Models](https://arxiv.org/abs/2509.05921)
*Kun Li,Cheng Wang,Minghui Xu,Yue Zhang,Xiuzhen Cheng*

Main category: cs.CR

TL;DR: 数据集版权技术保护方法综述，分为非侵入式、轻量侵入式和重度侵入式三类方法，分析各自优缺点和研究挑战


<details>
  <summary>Details</summary>
Motivation: 随着数据集成为机器学习系统的关键资产，传统法律机制无法有效应对数字数据复制和未授权使用的技术复杂性

Method: 系统性综述数据集版权保护技术方法，将其分为三类：非侵入式方法（不修改数据检测未授权使用）、轻量侵入式方法（嵌入可逆变更以支持所有权验证）、重度侵入式方法（使用可逆对抗示例等群敏数据修改来强制使用限制）

Result: 结合了关键技术，分析了各种方法的优势和局限性，指出了当前的研究挑战

Conclusion: 本文提供了数据集版权保护领域的组织化视角，并为开发统一、可扩展且符合道德标准的解决方案指明了未来方向

Abstract: As datasets become critical assets in modern machine learning systems,
ensuring robust copyright protection has emerged as an urgent challenge.
Traditional legal mechanisms often fail to address the technical complexities
of digital data replication and unauthorized use, particularly in opaque or
decentralized environments. This survey provides a comprehensive review of
technical approaches for dataset copyright protection, systematically
categorizing them into three main classes: non-intrusive methods, which detect
unauthorized use without modifying data; minimally-intrusive methods, which
embed lightweight, reversible changes to enable ownership verification; and
maximally-intrusive methods, which apply aggressive data alterations, such as
reversible adversarial examples, to enforce usage restrictions. We synthesize
key techniques, analyze their strengths and limitations, and highlight open
research challenges. This work offers an organized perspective on the current
landscape and suggests future directions for developing unified, scalable, and
ethically sound solutions to protect datasets in increasingly complex machine
learning ecosystems.

</details>


### [58] [DCMI: A Differential Calibration Membership Inference Attack Against Retrieval-Augmented Generation](https://arxiv.org/abs/2509.06026)
*Xinyu Gao,Xiangtao Meng,Yingkai Dong,Zheng Li,Shanqing Guo*

Main category: cs.CR

TL;DR: DCMI是一种针对RAG系统的差分校准成员推理攻击方法，通过查询扰动来区分成员和非成员检索文档，显著提高了攻击效果，在多个RAG平台上比基线方法提升10-40%的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的成员推理攻击方法忽略非成员检索文档对RAG输出的干扰，限制了攻击效果。RAG系统在处理敏感数据时存在隐私泄露风险，需要更有效的攻击方法来揭示这些漏洞。

Method: 提出DCMI方法，利用查询扰动下成员和非成员检索文档的敏感性差异，生成扰动查询进行校准，隔离成员检索文档的贡献，同时最小化非成员检索文档的干扰。

Result: 在逐步放宽假设的实验条件下，DCMI始终优于基线方法，例如在Flan-T5 RAG系统上达到97.42% AUC和94.35%准确率，比MBA基线提高40%以上。在真实RAG平台Dify和MaxKB上保持10-20%的优势。

Conclusion: RAG系统存在显著的隐私风险，需要更强的保护机制。呼吁社区对快速发展的RAG系统中的数据泄露风险进行更深入的研究。

Abstract: While Retrieval-Augmented Generation (RAG) effectively reduces hallucinations
by integrating external knowledge bases, it introduces vulnerabilities to
membership inference attacks (MIAs), particularly in systems handling sensitive
data. Existing MIAs targeting RAG's external databases often rely on model
responses but ignore the interference of non-member-retrieved documents on RAG
outputs, limiting their effectiveness. To address this, we propose DCMI, a
differential calibration MIA that mitigates the negative impact of
non-member-retrieved documents. Specifically, DCMI leverages the sensitivity
gap between member and non-member retrieved documents under query perturbation.
It generates perturbed queries for calibration to isolate the contribution of
member-retrieved documents while minimizing the interference from
non-member-retrieved documents. Experiments under progressively relaxed
assumptions show that DCMI consistently outperforms baselines--for example,
achieving 97.42% AUC and 94.35% Accuracy against the RAG system with Flan-T5,
exceeding the MBA baseline by over 40%. Furthermore, on real-world RAG
platforms such as Dify and MaxKB, DCMI maintains a 10%-20% advantage over the
baseline. These results highlight significant privacy risks in RAG systems and
emphasize the need for stronger protection mechanisms. We appeal to the
community's consideration of deeper investigations, like ours, against the data
leakage risks in rapidly evolving RAG systems. Our code is available at
https://github.com/Xinyu140203/RAG_MIA.

</details>


### [59] [Asymmetry Vulnerability and Physical Attacks on Online Map Construction for Autonomous Driving](https://arxiv.org/abs/2509.06071)
*Yang Lou,Haibo Hu,Qun Song,Qian Xu,Yi Zhu,Rui Tan,Wei-Bin Lee,Jianping Wang*

Main category: cs.CR

TL;DR: 本文系统性分析了在线HD地图构建模型的漏洞，发现其存在对称性偏误，并提出了能够操控地图构建的两阶段攻击框架，在真实场景中验证了攻击效果。


<details>
  <summary>Details</summary>
Motivation: 随着自动驾驶系统使用在线HD地图构建技术，其在恶意条件下的稳健性仍未得到充分研究。本文意在探索这些模型的漏洞和可能受到的攻击。

Method: 首先识别漏洞性不对称场景，然后优化摄像头盲光攻击和对抗补丁攻击的位置和模式，构建了两阶段攻击框架。

Result: 攻击能够使地图构建准确性下降达9.9%，导致44%目标路线无法到达，不安全规划轨迹率增加27%，并在真实车辆上验证。

Conclusion: 这是首次对在线地图构建模型的漏洞评估，提出了数字和物理攻击方法，并分析了对称性偏误的根源原因，为提升模型稳健性提供了见解。

Abstract: High-definition maps provide precise environmental information essential for
prediction and planning in autonomous driving systems. Due to the high cost of
labeling and maintenance, recent research has turned to online HD map
construction using onboard sensor data, offering wider coverage and more timely
updates for autonomous vehicles. However, the robustness of online map
construction under adversarial conditions remains underexplored. In this paper,
we present a systematic vulnerability analysis of online map construction
models, which reveals that these models exhibit an inherent bias toward
predicting symmetric road structures. In asymmetric scenes like forks or
merges, this bias often causes the model to mistakenly predict a straight
boundary that mirrors the opposite side. We demonstrate that this vulnerability
persists in the real-world and can be reliably triggered by obstruction or
targeted interference. Leveraging this vulnerability, we propose a novel
two-stage attack framework capable of manipulating online constructed maps.
First, our method identifies vulnerable asymmetric scenes along the victim AV's
potential route. Then, we optimize the location and pattern of camera-blinding
attacks and adversarial patch attacks. Evaluations on a public AD dataset
demonstrate that our attacks can degrade mapping accuracy by up to 9.9%, render
up to 44% of targeted routes unreachable, and increase unsafe planned
trajectory rates, colliding with real-world road boundaries, by up to 27%.
These attacks are also validated on a real-world testbed vehicle. We further
analyze root causes of the symmetry bias, attributing them to training data
imbalance, model architecture, and map element representation. To the best of
our knowledge, this study presents the first vulnerability assessment of online
map construction models and introduces the first digital and physical attack
against them.

</details>


### [60] [Towards Reliable Service Provisioning for Dynamic UAV Clusters in Low-Altitude Economy Networks](https://arxiv.org/abs/2509.06112)
*Yanwei Gong,Ruichen Zhang,Xiaoqing Wang,Xiaolin Chang,Bo Ai,Junchao Fan,Bocheng Ju,Dusit Niyato*

Main category: cs.CR

TL;DR: LP2-CASKU是一种轻量级隐私保护无人机集群认证和会话密钥更新方案，显著降低延迟82.8%-90.8%和能耗37.6%-72.6%，支持动态无人机集群的高效认证和隐私保护。


<details>
  <summary>Details</summary>
Motivation: 无人机集群服务对低空经济发展至关重要，但现有方案在NUAV高效认证、EUAV隐私保护跨集群认证以及集群会话密钥的前向和后向保密性方面存在挑战。

Method: 提出LP2-CASKU方案，集成高效批量认证机制（同时认证多个NUAV）、轻量级跨集群认证机制（确保EUAV匿名性和不可链接性）以及安全会话密钥更新机制。

Result: 理论分析和OMNeT++仿真显示，相比基线方案，LP2-CASKU在不同无人机群配置和网络比特率下实现延迟降低82.8%-90.8%，能耗降低37.6-72.6%。

Conclusion: LP2-CASKU方案能有效支持高度动态无人机集群环境中的隐私保护认证，具有较强的通信环境适应性。

Abstract: Unmanned Aerial Vehicle (UAV) cluster services are crucial for promoting the
low-altitude economy by enabling scalable, flexible, and adaptive aerial
networks. To meet diverse service demands, clusters must dynamically
incorporate a New UAVs (NUAVs) or an Existing UAV (EUAV). However, achieving
sustained service reliability remains challenging due to the need for efficient
and scalable NUAV authentication, privacy-preserving cross-cluster
authentication for EUAVs, and robust protection of the cluster session key,
including both forward and backward secrecy. To address these challenges, we
propose a Lightweight and Privacy-Preserving Cluster Authentication and Session
Key Update (LP2-CASKU) scheme tailored for dynamic UAV clusters in low-altitude
economy networks. LP2-CASKU integrates an efficient batch authentication
mechanism that simultaneously authenticates multiple NUAVs with minimal
communication overhead. It further introduces a lightweight cross-cluster
authentication mechanism that ensures EUAV anonymity and unlinkability.
Additionally, a secure session key update mechanism is incorporated to maintain
key confidentiality over time, thereby preserving both forward and backward
secrecy. We provide a comprehensive security analysis and evaluate LP2-CASKU
performance through both theoretical analysis and OMNeT++ simulations.
Experimental results demonstrate that, compared to the baseline, LP2-CASKU
achieves a latency reduction of 82.8%-90.8% by across different UAV swarm
configurations and network bitrates, demonstrating strong adaptability to
dynamic communication environments. Besides, under varying UAV swarm
configurations, LP2-CASKU reduces the energy consumption by approximately
37.6-72.6%, while effectively supporting privacy-preserving authentication in
highly dynamic UAV cluster environments.

</details>


### [61] [CSI-IBBS: Identity-Based Blind Signature using CSIDH](https://arxiv.org/abs/2509.06127)
*Soumya Bhoumik,Sarbari Mitra,Rohit Raj Sharma,Kuldeep Namdeo*

Main category: cs.CR

TL;DR: 基于CSIDH框架的身份基础盲签名方案，结合盲签名和零知识验证，提供量子抗性和隐私保护


<details>
  <summary>Details</summary>
Motivation: 结合身份基础加密的效率优势和盲签名的隐私保护能力，应对量子计算的威胁，构建可扩展的后量子安全加密系统

Method: 利用CSIDH框架（超奇异同槍基础），结合盲签名协议和零知识验证技术，在标准加密模型中分析安全性

Result: 设计出了具有可证安全性的协议，能够同时保护隐私和验证者诚信，性能评估证明其实用可行性

Conclusion: 该方案为后量子时代提供了一种安全、可扩展的隐私保护加密解决方案，在维护计算效率的同时具备量子抗性

Abstract: Identity-based cryptography (IBC), proposed by Adi Shamir, revolutionized
public key authentication by eliminating the need for certificates, enabling a
more efficient and scalable approach to cryptographic systems. Meanwhile, in
\cite{Katsumata2024group}, Katsumata et al. were the first to present the blind
signature protocol based on the hardness assumption of isogeny with provable
security, which resembles the Schnorr blind signature. Building upon these
foundational concepts, we propose an Identity-Based Blind Signature Scheme with
an Honest Zero-Knowledge Verifier utilizing the CSIDH framework. This scheme
combines blind signatures for privacy preservation with zero-knowledge proofs
to ensure the verifier's honesty without revealing any additional information.
  Leveraging the quantum-resistant properties of CSIDH, a post-quantum secure
scheme based on supersingular isogenies, our scheme offers strong protection
against quantum adversaries while maintaining computational efficiency. We
analyze the security of the introduced protocol in the standard cryptographic
model and demonstrate its effectiveness in safeguarding privacy and verifier
honesty. Furthermore, we present a performance evaluation, confirming the
practical viability of this quantum-resistant cryptographic solution for
privacy-preserving applications. This work advances the creation of secure, and
scalable cryptographic systems for the post-quantum era.

</details>


### [62] [Measuring the Vulnerability Disclosure Policies of AI Vendors](https://arxiv.org/abs/2509.06136)
*Yangheran Piao,Jingjie Li,Daniel W. Woods*

Main category: cs.CR

TL;DR: 对264家AI厂商漏洞披露政策的分析显示，36%厂商无披露渠道，仅18%明确提及AI风险，政策制定滞后于学术研究和实际事件


<details>
  <summary>Details</summary>
Motivation: 随着AI集成到关键系统中，研究AI漏洞披露政策对有效修复至关重要，需要了解厂商对AI漏洞报告的接受和响应态度

Method: 采用混合方法：定量分析264家AI厂商的披露政策，定性分析厂商对AI漏洞的态度分类，并与1130起AI事件和359篇学术论文对比

Result: 36%厂商无披露渠道，仅18%提及AI风险；数据访问、授权和模型提取漏洞通常被纳入范围，越狱和幻觉常被排除；识别出三种厂商态度：主动澄清型(46家)、沉默型(115家)、限制型(103家)

Conclusion: 漏洞赏金政策的发展滞后于学术研究和现实事件，需要改进AI漏洞披露政策以跟上AI安全威胁的发展

Abstract: As AI is increasingly integrated into products and critical systems,
researchers are paying greater attention to identifying related
vulnerabilities. Effective remediation depends on whether vendors are willing
to accept and respond to AI vulnerability reports. In this paper, we examine
the disclosure policies of 264 AI vendors. Using a mixed-methods approach, our
quantitative analysis finds that 36% of vendors provide no disclosure channel,
and only 18% explicitly mention AI-related risks. Vulnerabilities involving
data access, authorization, and model extraction are generally considered
in-scope, while jailbreaking and hallucination are frequently excluded. Through
qualitative analysis, we further identify three vendor postures toward AI
vulnerabilities - proactive clarification (n = 46, include active supporters,
AI integrationists, and back channels), silence (n = 115, include self-hosted
and hosted vendors), and restrictive (n = 103). Finally, by comparing vendor
policies against 1,130 AI incidents and 359 academic publications, we show that
bug bounty policy evolution has lagged behind both academic research and
real-world events.

</details>


### [63] [Lightweight Intrusion Detection System Using a Hybrid CNN and ConvNeXt-Tiny Model for Internet of Things Networks](https://arxiv.org/abs/2509.06202)
*Fatemeh Roshanzadeh,Hamid Barati,Ali Barati*

Main category: cs.CR

TL;DR: 轻量级CNN与ConvNeXt-Tiny混合模型的物联网入侵检测系统，在资源受限设备上实现高准确率攻击检测


<details>
  <summary>Details</summary>
Motivation: 物联网应用的快速扩展导致安全风险增加，需要在资源受限设备上实现高效的入侵检测

Method: 采用CNN与ConvNeXt-Tiny混合模型，使用真实物联网环境收集的良性和恶意网络数据进行训练

Result: 在测试阶段达到99.63%准确率，训练阶段99.67%准确率，误差率0.0107，同时保持短响应时间和低资源消耗

Conclusion: 该轻量级模型可作为复杂资源密集方案的实用替代方案，有效提升物联网网络安全性

Abstract: The rapid expansion of Internet of Things (IoT) systems across various
domains such as industry, smart cities, healthcare, manufacturing, and
government services has led to a significant increase in security risks,
threatening data integrity, confidentiality, and availability. Consequently,
ensuring the security and resilience of IoT systems has become a critical
requirement. In this paper, we propose a lightweight and efficient intrusion
detection system (IDS) for IoT environments, leveraging a hybrid model of CNN
and ConvNeXt-Tiny. The proposed method is designed to detect and classify
different types of network attacks, particularly botnet and malicious traffic,
while the lightweight ConvNeXt-Tiny architecture enables effective deployment
in resource-constrained devices and networks. A real-world dataset comprising
both benign and malicious network packets collected from practical IoT
scenarios was employed in the experiments. The results demonstrate that the
proposed method achieves high accuracy while significantly reducing training
and inference time compared to more complex models. Specifically, the system
attained 99.63% accuracy in the testing phase, 99.67% accuracy in the training
phase, and an error rate of 0.0107 across eight classes, while maintaining
short response times and low resource consumption. These findings highlight the
effectiveness of the proposed method in detecting and classifying attacks in
real-world IoT environments, indicating that the lightweight architecture can
serve as a practical alternative to complex and resource-intensive approaches
in IoT network security.

</details>


### [64] [PLRV-O: Advancing Differentially Private Deep Learning via Privacy Loss Random Variable Optimization](https://arxiv.org/abs/2509.06264)
*Qin Yang,Nicholas Stout,Meisam Mohammady,Han Wang,Ayesha Samreen,Christopher J Quinn,Yan Yan,Ashish Kundu,Yuan Hong*

Main category: cs.CR

TL;DR: PLRV-O提出了一种参数化的DP-SGD噪声分布框架，通过更独立地控制隐私损失和效用下降，在严格隐私约束下显著提高了模型准确性。


<details>
  <summary>Details</summary>
Motivation: 传统的Gaussian和Laplacian噪声机制只有单一的方差参数，导致噪声大小与隐私损失和效用下降相关联，无法独立控制这两个因素。当迭代轮数T和批处理大小B变化时，隐私-效用交换会发生任务依赖性偏移。

Method: 提出PLRV-O框架，定义了一个广泛的参数化DP-SGD噪声分布搜索空间，其中隐私损失矩装装得到严格特征化但可以更独立地优化效用损失。该方法支持根据任务特定需求系统地适配噪声，包括模型大小、训练持续时间、批处采样策略和剪切阈值。

Result: 实验结果显示PLRV-O在严格隐私约束下显著提高了效用：在CIFAR-10上，精调的ViT在epsilon约0.5时达到94.03%准确率（Gaussian噪声为83.93%）；在SST-2上，RoBERTa-large在epsilon约0.2时达到92.20%准确率（Gaussian噪声为50.25%）。

Conclusion: PLRV-O框架成功解决了传统DP-SGD噪声机制中隐私损失与效用下降相关联的问题，通过参数化噪声分布实现了更独立的优化控制，为不同任务特定要求提供了系统化的噪声适配方案。

Abstract: Differentially Private Stochastic Gradient Descent (DP-SGD) is a standard
method for enforcing privacy in deep learning, typically using the Gaussian
mechanism to perturb gradient updates. However, conventional mechanisms such as
Gaussian and Laplacian noise are parameterized only by variance or scale. This
single degree of freedom ties the magnitude of noise directly to both privacy
loss and utility degradation, preventing independent control of these two
factors. The problem becomes more pronounced when the number of composition
rounds T and batch size B vary across tasks, as these variations induce
task-dependent shifts in the privacy-utility trade-off, where small changes in
noise parameters can disproportionately affect model accuracy. To address this
limitation, we introduce PLRV-O, a framework that defines a broad search space
of parameterized DP-SGD noise distributions, where privacy loss moments are
tightly characterized yet can be optimized more independently with respect to
utility loss. This formulation enables systematic adaptation of noise to
task-specific requirements, including (i) model size, (ii) training duration,
(iii) batch sampling strategies, and (iv) clipping thresholds under both
training and fine-tuning settings. Empirical results demonstrate that PLRV-O
substantially improves utility under strict privacy constraints. On CIFAR-10, a
fine-tuned ViT achieves 94.03% accuracy at epsilon approximately 0.5, compared
to 83.93% with Gaussian noise. On SST-2, RoBERTa-large reaches 92.20% accuracy
at epsilon approximately 0.2, versus 50.25% with Gaussian.

</details>


### [65] [AttestLLM: Efficient Attestation Framework for Billion-scale On-device LLMs](https://arxiv.org/abs/2509.06326)
*Ruisi Zhang,Yifei Zhao,Neusha Javidnia,Mengxin Zheng,Farinaz Koushanfar*

Main category: cs.CR

TL;DR: AttestLLM是一个针对设备端大语言模型的硬件级知识产权保护框架，通过算法/软件/硬件协同设计在LLM激活分布中嵌入水印签名，在TEE中实现高效验证，确保只有授权模型能在目标平台上运行。


<details>
  <summary>Details</summary>
Motivation: 随着设备端LLM的广泛采用，验证本地设备上运行模型的合法性变得至关重要。现有认证技术无法有效处理十亿参数级别的LLM，既需要时间内存高效，又要应对LLM时代的新威胁。

Method: 采用算法/软件/硬件协同设计方法，在LLM构建块的激活分布中嵌入鲁棒水印签名，并在可信执行环境(TEE)中优化认证协议，实现高效验证而不影响推理吞吐量。

Result: 在Llama、Qwen和Phi系列LLM上的广泛概念验证评估表明，AttestLLM具有认证可靠性、保真度和效率，能够强制执行模型合法性，并对模型替换和伪造攻击表现出韧性。

Conclusion: AttestLLM是首个保护设备厂商硬件级知识产权的认证框架，通过创新的水印嵌入和TEE优化协议，为设备端LLM提供了有效的合法性验证解决方案。

Abstract: As on-device LLMs(e.g., Apple on-device Intelligence) are widely adopted to
reduce network dependency, improve privacy, and enhance responsiveness,
verifying the legitimacy of models running on local devices becomes critical.
Existing attestation techniques are not suitable for billion-parameter Large
Language Models (LLMs), struggling to remain both time- and memory-efficient
while addressing emerging threats in the LLM era. In this paper, we present
AttestLLM, the first-of-its-kind attestation framework to protect the
hardware-level intellectual property (IP) of device vendors by ensuring that
only authorized LLMs can execute on target platforms. AttestLLM leverages an
algorithm/software/hardware co-design approach to embed robust watermarking
signatures onto the activation distributions of LLM building blocks. It also
optimizes the attestation protocol within the Trusted Execution Environment
(TEE), providing efficient verification without compromising inference
throughput. Extensive proof-of-concept evaluations on LLMs from Llama, Qwen,
and Phi families for on-device use cases demonstrate AttestLLM's attestation
reliability, fidelity, and efficiency. Furthermore, AttestLLM enforces model
legitimacy and exhibits resilience against model replacement and forgery
attacks.

</details>


### [66] [Embedding Poisoning: Bypassing Safety Alignment via Embedding Semantic Shift](https://arxiv.org/abs/2509.06338)
*Shuai Yuan,Zhibo Zhang,Yuxi Li,Guangdong Bai,Wang Kailong*

Main category: cs.CR

TL;DR: 本文提出了一种新型的嵌入层攻击方法SEP，通过在嵌入层注入微小扰动来绕过LLM的安全对齐机制，攻击成功率高达96.43%，揭示了部署安全的关键漏洞。


<details>
  <summary>Details</summary>
Motivation: 现有平台的安全扫描无法检测嵌入层的细微操作，存在安全漏洞，需要研究部署阶段的攻击方式。

Method: 提出SEP框架，通过搜索优化在高风险词嵌入中注入微小扰动，利用模型响应的线性转换特性识别可绕过安全机制的扰动窗口。

Result: 在6个对齐LLM上测试，平均攻击成功率达96.43%，同时保持良性任务性能并规避传统检测机制。

Conclusion: 揭示了LLM部署安全的关键疏忽，强调未来防御策略中需要嵌入层完整性检查的紧迫性。

Abstract: The widespread distribution of Large Language Models (LLMs) through public
platforms like Hugging Face introduces significant security challenges. While
these platforms perform basic security scans, they often fail to detect subtle
manipulations within the embedding layer. This work identifies a novel class of
deployment phase attacks that exploit this vulnerability by injecting
imperceptible perturbations directly into the embedding layer outputs without
modifying model weights or input text. These perturbations, though
statistically benign, systematically bypass safety alignment mechanisms and
induce harmful behaviors during inference. We propose Search based Embedding
Poisoning(SEP), a practical, model agnostic framework that introduces carefully
optimized perturbations into embeddings associated with high risk tokens. SEP
leverages a predictable linear transition in model responses, from refusal to
harmful output to semantic deviation to identify a narrow perturbation window
that evades alignment safeguards. Evaluated across six aligned LLMs, SEP
achieves an average attack success rate of 96.43% while preserving benign task
performance and evading conventional detection mechanisms. Our findings reveal
a critical oversight in deployment security and emphasize the urgent need for
embedding level integrity checks in future LLM defense strategies.

</details>


### [67] [From Perception to Protection: A Developer-Centered Study of Security and Privacy Threats in Extended Reality (XR)](https://arxiv.org/abs/2509.06368)
*Kunlin Cai,Jinghuai Zhang,Ying Li,Zhiyuan Wang,Xun Chen,Tianshi Li,Yuan Tian*

Main category: cs.CR

TL;DR: 本研究通过访谈23名XR开发者，首次从开发者视角分析XR安全隐私威胁认知，发现开发者存在认知偏差且缺乏有效支持，提出了改进XR开发过程安全隐私的建议


<details>
  <summary>Details</summary>
Motivation: XR技术因其沉浸性和数据收集特性带来了独特的安全隐私挑战，开发者作为主要构建者对此认知不足，但缺乏从开发者视角的深入威胁研究

Method: 访谈23名专业XR开发者，聚焦XR新兴威胁，研究XR开发中的现有问题和可行的改进路径

Result: 发现XR开发决策会放大安全隐私威胁但开发者常无意识，现有缓解方法有限且支持不足削弱了开发者应对威胁的动力和能力

Conclusion: 提出了针对利益相关者的可行建议以改进XR开发全过程的安全隐私保护，这是首个XR领域威胁感知的开发者中心研究

Abstract: The immersive nature of XR introduces a fundamentally different set of
security and privacy (S&P) challenges due to the unprecedented user
interactions and data collection that traditional paradigms struggle to
mitigate. As the primary architects of XR applications, developers play a
critical role in addressing novel threats. However, to effectively support
developers, we must first understand how they perceive and respond to different
threats. Despite the growing importance of this issue, there is a lack of
in-depth, threat-aware studies that examine XR S&P from the developers'
perspective. To fill this gap, we interviewed 23 professional XR developers
with a focus on emerging threats in XR. Our study addresses two research
questions aiming to uncover existing problems in XR development and identify
actionable paths forward.
  By examining developers' perceptions of S&P threats, we found that: (1) XR
development decisions (e.g., rich sensor data collection, user-generated
content interfaces) are closely tied to and can amplify S&P threats, yet
developers are often unaware of these risks, resulting in cognitive biases in
threat perception; and (2) limitations in existing mitigation methods, combined
with insufficient strategic, technical, and communication support, undermine
developers' motivation, awareness, and ability to effectively address these
threats. Based on these findings, we propose actionable and stakeholder-aware
recommendations to improve XR S&P throughout the XR development process. This
work represents the first effort to undertake a threat-aware,
developer-centered study in the XR domain -- an area where the immersive,
data-rich nature of the XR technology introduces distinctive challenges.

</details>


### [68] [When Code Crosses Borders: A Security-Centric Evaluation of LLM-based Code Translation](https://arxiv.org/abs/2509.06504)
*Hailong Chang,Guozhu Meng,Shuhui Xiao,Kai Chen,Kun Sun,Yilin Li*

Main category: cs.CR

TL;DR: STED是首个专门评估LLM代码翻译安全性的数据集，包含720个安全相关代码样本。评估发现28.6-45%的翻译会引入新漏洞，特别是输入验证等web相关缺陷。基于RAG的缓解策略可将翻译引起的漏洞减少32.8%。


<details>
  <summary>Details</summary>
Motivation: 现有评估主要关注语法或功能正确性，忽视了代码翻译中的安全维度，需要专门的安全评估数据集和方法。

Method: 构建STED数据集（5种编程语言，9个CWE类别），采用安全研究人员人工评估和LLM自动化评估的双重框架，评估功能正确性、漏洞保留和漏洞引入率。

Result: 大规模评估显示28.6-45%的翻译会引入新漏洞，特别是在输入验证等web相关缺陷方面表现较差。RAG缓解策略可减少32.8%的翻译引起漏洞。

Conclusion: LLM代码翻译存在严重的安全退化问题，需要专门的安全评估和缓解策略，知识增强提示具有改善潜力。

Abstract: With the growing demand for cross-language codebase migration, evaluating
LLMs' security implications in translation tasks has become critical. Existing
evaluations primarily focus on syntactic or functional correctness at the
function level, neglecting the critical dimension of security.
  To enable security evaluation, we construct STED (Security-centric
Translation Evaluation Dataset), the first dataset specifically designed for
evaluating the security implications of LLM-based code translation. It
comprises 720 security-related code samples across five programming languages
and nine high-impact CWE categories, sourced from CVE/NVD and manually verified
for translation tasks. Our evaluation framework consists of two independent
assessment modules: (1) rigorous evaluation by security researchers, and (2)
automated analysis via LLM-as-a-judge. Together they evaluate three critical
aspects: functional correctness, vulnerability preservation, and vulnerability
introduction rates.
  Our large-scale evaluation of five state-of-the-art LLMs across 6,000
translation instances reveals significant security degradation, with 28.6-45%
of translations introducing new vulnerabilities--particularly for web-related
flaws like input validation, where LLMs show consistent weaknesses.
Furthermore, we develop a Retrieval-Augmented Generation (RAG)-based mitigation
strategy that reduces translation-induced vulnerabilities by 32.8%, showing the
potential of knowledge-enhanced prompting.

</details>


### [69] [Synthesis of Sound and Precise Leakage Contracts for Open-Source RISC-V Processors](https://arxiv.org/abs/2509.06509)
*Zilong Wang,Gideon Mohr,Klaus von Gleissenthall,Jan Reineke,Marco Guarnieri*

Main category: cs.CR

TL;DR: LeaSyn是首个自动合成处理器泄漏合约的工具，能够为RTL级处理器设计生成既可靠又精确的泄漏合约，解决了手动制定合约的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有处理器泄漏合约的制定需要深入了解微架构优化引入的时序侧信道，过程耗时、易错且具有挑战性。

Method: LeaSyn从用户提供的合约模板出发，通过合约合成和验证的交替过程，基于处理器泄漏的经验特征确保精确性，同时保证可靠性。

Result: 在六个开源RISC-V CPU上的实验表明，LeaSyn生成的合约可靠且比现有方法更精确，能更真实地反映目标处理器的实际泄漏。

Conclusion: LeaSyn成功实现了处理器泄漏合约的自动合成，为处理器安全验证提供了有效的自动化解决方案。

Abstract: Leakage contracts have been proposed as a new security abstraction at the
instruction set architecture level. Leakage contracts aim to capture the
information that processors may leak via microarchitectural side channels.
Recently, the first tools have emerged to verify whether a processor satisfies
a given contract. However, coming up with a contract that is both sound and
precise for a given processor is challenging, time-consuming, and error-prone,
as it requires in-depth knowledge of the timing side channels introduced by
microarchitectural optimizations.
  In this paper, we address this challenge by proposing LeaSyn, the first tool
for automatically synthesizing leakage contracts that are both sound and
precise for processor designs at register-transfer level. Starting from a
user-provided contract template that captures the space of possible contracts,
LeaSyn automatically constructs a contract, alternating between contract
synthesis, which ensures precision based on an empirical characterization of
the processor's leaks, and contract verification, which ensures soundness.
  Using LeaSyn, we automatically synthesize contracts for six open-source
RISC-V CPUs for a variety of contract templates. Our experiments indicate that
LeaSyn's contracts are sound and more precise (i.e., represent the actual leaks
in the target processor more faithfully) than contracts constructed by existing
approaches.

</details>


### [70] [Signal-Based Malware Classification Using 1D CNNs](https://arxiv.org/abs/2509.06548)
*Jack Wilkie,Hanan Hindy,Ivan Andonovic,Christos Tachtatzis,Robert Atkinson*

Main category: cs.CR

TL;DR: 该论文提出了一种新的恶意软件分类方法，将二进制文件转换为1D信号而不是2D图像，避免了信息损失和量化噪声，使用改进的1D CNN架构在MalNet数据集上实现了最先进的分类性能。


<details>
  <summary>Details</summary>
Motivation: 传统恶意软件分类方法存在局限性：静态分析容易被现代混淆技术规避，动态分析资源消耗过大。现有的2D图像转换方法会导致信息损失和量化噪声，限制了分类性能。

Method: 将恶意软件二进制文件转换为1D浮点信号，避免了启发式重塑和量化噪声。开发了基于ResNet架构和squeeze-and-excitation层的定制1D卷积神经网络来分类这些信号。

Result: 在MalNet数据集上实现了最先进的性能：二进制分类F1分数0.874，类型分类0.503，家族分类0.507。

Conclusion: 1D信号转换方法优于传统的2D图像转换，为未来的恶意软件分类模型提供了新的信号模态处理方式，具有更好的分类性能和实用性。

Abstract: Malware classification is a contemporary and ongoing challenge in
cyber-security: modern obfuscation techniques are able to evade traditional
static analysis, while dynamic analysis is too resource intensive to be
deployed at a large scale. One prominent line of research addresses these
limitations by converting malware binaries into 2D images by heuristically
reshaping them into a 2D grid before resizing using Lanczos resampling. These
images can then be classified based on their textural information using
computer vision approaches. While this approach can detect obfuscated malware
more effectively than static analysis, the process of converting files into 2D
images results in significant information loss due to both quantisation noise,
caused by rounding to integer pixel values, and the introduction of 2D
dependencies which do not exist in the original data. This loss of signal
limits the classification performance of the downstream model. This work
addresses these weaknesses by instead resizing the files into 1D signals which
avoids the need for heuristic reshaping, and additionally these signals do not
suffer from quantisation noise due to being stored in a floating-point format.
It is shown that existing 2D CNN architectures can be readily adapted to
classify these 1D signals for improved performance. Furthermore, a bespoke 1D
convolutional neural network, based on the ResNet architecture and
squeeze-and-excitation layers, was developed to classify these signals and
evaluated on the MalNet dataset. It was found to achieve state-of-the-art
performance on binary, type, and family level classification with F1 scores of
0.874, 0.503, and 0.507, respectively, paving the way for future models to
operate on the proposed signal modality.

</details>


### [71] [Super-Quadratic Quantum Speed-ups and Guessing Many Likely Keys](https://arxiv.org/abs/2509.06549)
*Timo Glaser,Alexander May,Julian Nowakowski*

Main category: cs.CR

TL;DR: 本文对量子密钥猜测算法进行了首次紧致分析，发现Montanaro算法的运行时间为2^(H_{2/3}(D)/2)，相比经典算法的2^(H_{1/2}(D))实现了超二次量子加速。在多密钥场景下，猜测每个密钥的成本显著降低。


<details>
  <summary>Details</summary>
Motivation: 研究从非均匀概率分布中猜测密码密钥的基本问题，分析量子算法相比经典算法的优势，特别是在多密钥设置下的性能表现。

Method: 使用Arikan不等式对Montanaro量子算法进行紧致分析，基于Renyi熵理论，比较单密钥和多密钥场景下的猜测成本。

Result: 量子密钥猜测算法运行时间为2^(H_{2/3}(D)/2)，经典算法为2^(H_{1/2}(D))，量子加速比超过2。多密钥设置下每个密钥的猜测成本大幅降低。

Conclusion: 量子算法在密钥猜测问题上具有显著优势，多密钥场景进一步降低了猜测成本，这对密码安全性分析具有重要意义。

Abstract: We study the fundamental problem of guessing cryptographic keys, drawn from
some non-uniform probability distribution $D$, as e.g. in LPN, LWE or for
passwords. The optimal classical algorithm enumerates keys in decreasing order
of likelihood. The optimal quantum algorithm, due to Montanaro (2011), is a
sophisticated Grover search.
  We give the first tight analysis for Montanaro's algorithm, showing that its
runtime is $2^{H_{2/3}(D)/2}$, where $H_{\alpha}(\cdot)$ denotes Renyi entropy
with parameter $\alpha$. Interestingly, this is a direct consequence of an
information theoretic result called Arikan's Inequality (1996) -- which has so
far been missed in the cryptographic community -- that tightly bounds the
runtime of classical key guessing by $2^{H_{1/2}(D)}$. Since $H_{2/3}(D) <
H_{1/2}(D)$ for every non-uniform distribution $D$, we thus obtain a
super-quadratic quantum speed-up $s>2$ over classical key guessing.
  As another main result, we provide the first thorough analysis of guessing in
a multi-key setting. Specifically, we consider the task of attacking many keys
sampled independently from some distribution $D$, and aim to guess a fraction
of them. For product distributions $D = \chi^n$, we show that any constant
fraction of keys can be guessed within $2^{H(D)}$ classically and $2 ^{H(D)/2}$
quantumly per key, where $H(\chi)$ denotes Shannon entropy. In contrast,
Arikan's Inequality implies that guessing a single key costs $2^{H_{1/2}(D)}$
classically and $2^{H_{2/3}(D)/2}$ quantumly. Since $H(D) < H_{2/3}(D) <
H_{1/2}(D)$, this shows that in a multi-key setting the guessing cost per key
is substantially smaller than in a single-key setting, both classically and
quantumly.

</details>


### [72] [Marginal sets in semigroups and semirings](https://arxiv.org/abs/2509.06562)
*I. Buchinskiy,M. Kotov,A. Ponmaheshkumar,R. Perumal*

Main category: cs.CR

TL;DR: 本文扩展了Roman'kov的边际集理论，将其从群推广到半群和半环，特别针对热带矩阵半群和半环构造边际集，并应用于改进半群上的密钥交换方案。


<details>
  <summary>Details</summary>
Motivation: Roman'kov在2019年提出了群的边际集概念并应用于密钥交换方案，但该理论仅限于群结构。本文旨在将这一概念扩展到更一般的代数结构——半群和半环，以扩大其应用范围。

Method: 将边际集概念从群推广到半群和半环，特别针对热带矩阵半群和半环，描述了如何构造这些结构中的边际集。

Result: 成功建立了半群和半环的边际集理论框架，为热带矩阵半群和半环提供了具体的边际集构造方法。

Conclusion: 边际集理论可以有效地扩展到半群和半环结构，这一扩展为改进基于半群的密钥交换方案提供了新的数学工具和理论基础。

Abstract: In 2019, V. A. Roman'kov introduced the concept of marginal sets for groups.
He developed a theory of marginal sets and demonstrated how these sets can be
applied to improve some key exchange schemes. In this paper, we extend his
ideas and introduce the concept of marginal sets for semigroups and semirings.
For tropical matrix semigroups and semirings, we describe how some marginal
sets can be constructed. We apply marginal sets to improve some key exchange
schemes over semigroups.

</details>


### [73] [A Simple Data Exfiltration Game](https://arxiv.org/abs/2509.06571)
*Tristan Caulfield*

Main category: cs.CR

TL;DR: 基于游戏论的网络数据泄漏模型，攻击者选择泄漏路径和速度，防御者设置监控阈值来最小化数据丢失成本和响应警报成本。


<details>
  <summary>Details</summary>
Motivation: 数据泄漏问题导致企业避免机密数据丢失和许诈的巨大成本，需要有效的防御策略。

Method: 建立简单的游戏论模型，攻击者与防御者在泄漏路径、速度和监控阈值选择上进行对抗。

Result: 提出了一个理论框架来分析数据泄漏问题中攻防双方的策略选择和权衡。

Conclusion: 游戏论模型为理解和应对数据泄漏风险提供了有价值的理论基础，帮助企业制定更有效的安全防御策略。

Abstract: Data exfiltration is a growing problem for business who face costs related to
the loss of confidential data as well as potential extortion. This work
presents a simple game theoretic model of network data exfiltration. In the
model, the attacker chooses the exfiltration route and speed, and the defender
selects monitoring thresholds to detect unusual activity. The attacker is
rewarded for exfiltrating data, and the defender tries to minimize the costs of
data loss and of responding to alerts.

</details>


### [74] [Mind Your Server: A Systematic Study of Parasitic Toolchain Attacks on the MCP Ecosystem](https://arxiv.org/abs/2509.06572)
*Shuli Zhao,Qinsheng Hou,Zihan Zhan,Yanhao Wang,Yuchong Xie,Yu Guo,Libo Chen,Shenghong Li,Zhi Xue*

Main category: cs.CR

TL;DR: 论文揭示了MCP协议中的寄生工具链攻击(MCP-UPD)，攻击者通过污染外部数据源，让LLM在正常任务中执行恶意指令，导致隐私数据泄露。


<details>
  <summary>Details</summary>
Motivation: 随着LLM通过MCP协议与外部系统集成，安全范式发生根本转变：LLM从被动信息处理器变为自主编排工具链的协调器，扩大了攻击面，攻击目标从操纵单个输出升级到劫持整个执行流程。

Method: 提出寄生工具链攻击概念，设计MCP-SEC工具对MCP生态系统进行大规模安全普查，分析12,230个工具和1,360个服务器，识别可利用的gadget和攻击方法。

Result: 发现MCP生态系统存在大量可利用的安全漏洞，根本原因是MCP缺乏上下文-工具隔离和最小权限执行机制，导致恶意指令可以不受控制地传播到敏感工具调用中。

Conclusion: MCP平台存在系统性风险，LLM集成环境迫切需要防御机制来应对这类新型攻击。

Abstract: Large language models (LLMs) are increasingly integrated with external
systems through the Model Context Protocol (MCP), which standardizes tool
invocation and has rapidly become a backbone for LLM-powered applications.
While this paradigm enhances functionality, it also introduces a fundamental
security shift: LLMs transition from passive information processors to
autonomous orchestrators of task-oriented toolchains, expanding the attack
surface, elevating adversarial goals from manipulating single outputs to
hijacking entire execution flows. In this paper, we reveal a new class of
attacks, Parasitic Toolchain Attacks, instantiated as MCP Unintended Privacy
Disclosure (MCP-UPD). These attacks require no direct victim interaction;
instead, adversaries embed malicious instructions into external data sources
that LLMs access during legitimate tasks. The malicious logic infiltrates the
toolchain and unfolds in three phases: Parasitic Ingestion, Privacy Collection,
and Privacy Disclosure, culminating in stealthy exfiltration of private data.
Our root cause analysis reveals that MCP lacks both context-tool isolation and
least-privilege enforcement, enabling adversarial instructions to propagate
unchecked into sensitive tool invocations. To assess the severity, we design
MCP-SEC and conduct the first large-scale security census of the MCP ecosystem,
analyzing 12,230 tools across 1,360 servers. Our findings show that the MCP
ecosystem is rife with exploitable gadgets and diverse attack methods,
underscoring systemic risks in MCP platforms and the urgent need for defense
mechanisms in LLM-integrated environments.

</details>


### [75] [LLMs in Cybersecurity: Friend or Foe in the Human Decision Loop?](https://arxiv.org/abs/2509.06595)
*Irdin Pekaric,Philipp Zech,Tom Mattson*

Main category: cs.CR

TL;DR: LLMs在安全关键决策中既提高准确性又可能削弱独立推理能力，导致过度依赖和决策同质化。高韧性个体能更有效利用LLM，认知特质调节AI效益。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs如何影响人类在安全关键环境中的判断决策，探索其作为认知协作工具的潜在悖论：既提升准确性又可能侵蚀独立推理能力。

Method: 通过两个探索性焦点小组（无辅助和LLM支持）评估决策准确性、行为韧性和依赖动态。

Result: LLMs在常规决策中提高准确性和一致性，但无意中减少认知多样性并加剧自动化偏见，特别是在低韧性用户中。高韧性个体能更有效利用LLM。

Conclusion: 认知特质在调节AI效益中起关键作用，需要设计能保持认知多样性并减少过度依赖的LLM交互方式。

Abstract: Large Language Models (LLMs) are transforming human decision-making by acting
as cognitive collaborators. Yet, this promise comes with a paradox: while LLMs
can improve accuracy, they may also erode independent reasoning, promote
over-reliance and homogenize decisions. In this paper, we investigate how LLMs
shape human judgment in security-critical contexts. Through two exploratory
focus groups (unaided and LLM-supported), we assess decision accuracy,
behavioral resilience and reliance dynamics. Our findings reveal that while
LLMs enhance accuracy and consistency in routine decisions, they can
inadvertently reduce cognitive diversity and improve automation bias, which is
especially the case among users with lower resilience. In contrast,
high-resilience individuals leverage LLMs more effectively, suggesting that
cognitive traits mediate AI benefit.

</details>


### [76] [A Secure Sequencer and Data Availability Committee for Rollups (Extended Version)](https://arxiv.org/abs/2509.06614)
*Margarita Capretto,Martín Ceresa,Antonio Fernández Anta,Pedro Moreno Sánchez,César Sánchez*

Main category: cs.CR

TL;DR: 进一步提高区块链Layer 2 Rollups的可稳定性，通过设计高效的欺诈证明机制来监督sequencer和DAC的诚实行为，并在LEAN4中实现了验证


<details>
  <summary>Details</summary>
Motivation: 解决Layer 2 Rollups中sequencer和数据可用性委员会(DAC)可能存在的不诚实行为问题，这些问题影响L2的安全性和采用速度

Method: 设计了由L1合约仲裁的欺诈证明机制，将其形式化为两者游戏模型，采用具体算法验证DAC正确性属性，并在LEAN4中实现机制化

Result: 提出的欺诈证明机制比现有通用方案更高效、更易理解且更简洁，能够有效限制号数不同的sequencer和DAC成员的恶意行为

Conclusion: 通过专门设计的欺诈证明机制可以提高Layer 2 Rollups的安全性和可靠性，为可扩展性解决方案提供了更强大的保障

Abstract: Blockchains face a scalability limitation, partly due to the throughput
limitations of consensus protocols, especially when aiming to obtain a high
degree of decentralization. Layer 2 Rollups (L2s) are a faster alternative to
conventional blockchains. L2s perform most computations offchain using
minimally blockchains (L1) under-the-hood to guarantee correctness. A sequencer
is a service that receives offchain L2 transaction requests, batches these
transactions, and commits compressed or hashed batches to L1. Using hashing
needs less L1 space, which is beneficial for gas cost, but requires a data
availability committee (DAC) service to translate hashes into their
corresponding batches of transaction requests. The behavior of sequencers and
DACs influence the evolution of the L2 blockchain, presenting a potential
security threat and delaying L2 adoption. We propose in this paper fraud-proof
mechanisms, arbitrated by L1 contracts, to detect and generate evidence of
dishonest behavior of the sequencer and DAC. We study how these fraud-proofs
limit the power of adversaries that control different number of sequencer and
DACs members, and provide incentives for their honest behavior. We designed
these fraud-proof mechanisms as two player games. Unlike the generic
fraud-proofs in current L2s (designed to guarantee the correct execution of
transactions), our fraud-proofs are over pred-etermined algorithms that verify
the properties that determine the correctness of the DAC. Arbitrating over
concrete algorithms makes our fraud-proofs more efficient, easier to
understand, and simpler to prove correct. We provide as an artifact a
mechanization in LEAN4 of our fraud-proof games, including (1) the verified
strategies that honest players should play to win all games as well as (2)
mechanisms to detect dishonest claims.

</details>


### [77] [Network-level Censorship Attacks in the InterPlanetary File System](https://arxiv.org/abs/2509.06626)
*Jan Matter,Muoi Tran*

Main category: cs.CR

TL;DR: IPFS网络虽然去中心化，但节点和内容提供商集中在公有云，面临BGP路由攻击威胁。研究发现单个恶意AS可审查75%的IPFS内容，影响57%的请求节点，并提出基于全局协作内容复制的防御措施。


<details>
  <summary>Details</summary>
Motivation: IPFS作为Web3去中心化存储标准，其节点和内容提供商却集中在公有云，存在中心化风险。BGP路由攻击（如劫持和被动拦截）对其他Web3协议已有研究，但对IPFS网络的分析尚属空白。

Method: 收集3,000个内容块（CIDs），模拟BGP劫持和被动拦截攻击，分析攻击效果和影响范围。

Result: 单个恶意AS可审查75%的IPFS内容，影响超过57%的请求节点；仅需劫持62个前缀即可达到70%的攻击效果。

Conclusion: BGP路由攻击对IPFS构成严重威胁，建议采用全局协作内容复制和强化备份内容提供商节点来增强网络韧性，呼吁提高安全意识并加强IPFS网络防护。

Abstract: The InterPlanetary File System (IPFS) has been successfully established as
the de facto standard for decentralized data storage in the emerging Web3.
Despite its decentralized nature, IPFS nodes, as well as IPFS content
providers, have converged to centralization in large public clouds.
Centralization introduces BGP routing-based attacks, such as passive
interception and BGP hijacking, as potential threats. Although this attack
vector has been investigated for many other Web3 protocols, such as Bitcoin and
Ethereum, to the best of our knowledge, it has not been analyzed for the IPFS
network. In our work, we bridge this gap and demonstrate that BGP routing
attacks can be effectively leveraged to censor content in IPFS. For the
analysis, we collected 3,000 content blocks called CIDs and conducted a
simulation of BGP hijacking and passive interception against them. We find that
a single malicious AS can censor 75% of the IPFS content for more than 57% of
all requester nodes. Furthermore, we show that even with a small set of only 62
hijacked prefixes, 70% of the full attack effectiveness can already be reached.
We further propose and validate countermeasures based on global collaborative
content replication among all nodes in the IPFS network, together with
additional robust backup content provider nodes that are well-hardened against
BGP hijacking. We hope this work raises awareness about the threat BGP
routing-based attacks pose to IPFS and triggers further efforts to harden the
live IPFS network against them.

</details>


### [78] [When Secure Isn't: Assessing the Security of Machine Learning Model Sharing](https://arxiv.org/abs/2509.06703)
*Gabriele Digregorio,Marco Di Gennaro,Stefano Zanero,Stefano Longari,Michele Carminati*

Main category: cs.CR

TL;DR: 论文分析了机器学习模型共享框架和平台的安全状况，发现大多数平台安全防护不足，甚至存在6个0day漏洞，揭示了当前安全机制无法有效保护用户，用户对安全设置存在误解。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习模型共享的普及，用户面临未充分探索的安全风险，而开发者和实践者的安全意识有限，需要评估模型共享生态系统的安全状况。

Method: 评估框架和平台的安全态势，测试安全导向机制的有效性，调查用户对模型共享安全叙法的认知，并分析存在漏洞的安全设置。

Result: 大多数框架和平台仅部分解决安全风险，通常将责任转嫁给用户；发现了6个0day漏洞可实现任意代码执行；用户错误地认为安全设置是可信的。

Conclusion: 模型共享安全问题远未解决，文件格式不能保证安全，需要采取措施加强模型共享生态系统的安全性，改变用户的安全认知。

Abstract: The rise of model-sharing through frameworks and dedicated hubs makes Machine
Learning significantly more accessible. Despite their benefits, these tools
expose users to underexplored security risks, while security awareness remains
limited among both practitioners and developers. To enable a more
security-conscious culture in Machine Learning model sharing, in this paper we
evaluate the security posture of frameworks and hubs, assess whether
security-oriented mechanisms offer real protection, and survey how users
perceive the security narratives surrounding model sharing. Our evaluation
shows that most frameworks and hubs address security risks partially at best,
often by shifting responsibility to the user. More concerningly, our analysis
of frameworks advertising security-oriented settings and complete model sharing
uncovered six 0-day vulnerabilities enabling arbitrary code execution. Through
this analysis, we debunk the misconceptions that the model-sharing problem is
largely solved and that its security can be guaranteed by the file format used
for sharing. As expected, our survey shows that the surrounding security
narrative leads users to consider security-oriented settings as trustworthy,
despite the weaknesses shown in this work. From this, we derive takeaways and
suggestions to strengthen the security of model-sharing ecosystems.

</details>


### [79] [Image Encryption Scheme Based on Hyper-Chaotic Map and Self-Adaptive Diffusion](https://arxiv.org/abs/2509.06754)
*Yiqi Tang*

Main category: cs.CR

TL;DR: 本文提出了一种基于新2D超混涎地图和自适应潜移方法的创新图像加密方案，在混涎性能和加密效果方面显著超过现有技术。


<details>
  <summary>Details</summary>
Motivation: 在数字时代，图像加密技术是防止未授权访问的重要保护手段，需要提升加密方案的安全性和性能。

Method: 集成了新2D-RA超混涎地图（基于Rastrigin和Ackley函数混合设计）和新的自适应潜移方法，通过分析分支图、李雅普诺夫指数等指标验证混涎性能。

Result: 实验结果显示2D-RA地图的混涎性能超过现有先进混涎函数，加密方案在多项指标上显著优于当前最先进的图像加密技术。

Conclusion: 该研究提供了一种高效、安全的图像加密方案，为数字图像安全保护做出了重要贡献。

Abstract: In the digital age, image encryption technology acts as a safeguard,
preventing unauthorized access to images. This paper proposes an innovative
image encryption scheme that integrates a novel 2D hyper-chaotic map with a
newly developed self-adaptive diffusion method. The 2D hyper-chaotic map,
namely the 2D-RA map, is designed by hybridizing the Rastrigin and Ackley
functions. The chaotic performance of the 2D-RA map is validated through a
series of measurements, including the Bifurcation Diagram, Lyapunov Exponent
(LE), Initial Value Sensitivity, 0 - 1 Test, Correlation Dimension (CD), and
Kolmogorov Entropy (KE). The results demonstrate that the chaotic performance
of the 2D-RA map surpasses that of existing advanced chaotic functions.
Additionally, the self-adaptive diffusion method is employed to enhance the
uniformity of grayscale distribution. The performance of the image encryption
scheme is evaluated using a series of indicators. The results show that the
proposed image encryption scheme significantly outperforms current
state-of-the-art image encryption techniques.

</details>


### [80] [Imitative Membership Inference Attack](https://arxiv.org/abs/2509.06796)
*Yuntao Du,Yuetian Chen,Hanshen Xiao,Bruno Ribeiro,Ninghui Li*

Main category: cs.CR

TL;DR: 提出IMIA方法，通过模仿训练技术构建少量目标模型仿制品，显著降低计算成本的同时提升成员推理攻击效果


<details>
  <summary>Details</summary>
Motivation: 现有成员推理攻击需要训练数百个影子模型，计算开销巨大，需要更高效的方法

Method: 使用创新的模仿训练技术，构建少量目标模型仿制品来精确复制目标模型行为进行推理

Result: IMIA在各种攻击设置下显著优于现有方法，同时仅需不到5%的计算成本

Conclusion: IMIA是一种高效且有效的成员推理攻击方法，大幅降低了计算开销同时提升了攻击性能

Abstract: A Membership Inference Attack (MIA) assesses how much a target machine
learning model reveals about its training data by determining whether specific
query instances were part of the training set. State-of-the-art MIAs rely on
training hundreds of shadow models that are independent of the target model,
leading to significant computational overhead. In this paper, we introduce
Imitative Membership Inference Attack (IMIA), which employs a novel imitative
training technique to strategically construct a small number of target-informed
imitative models that closely replicate the target model's behavior for
inference. Extensive experimental results demonstrate that IMIA substantially
outperforms existing MIAs in various attack settings while only requiring less
than 5% of the computational cost of state-of-the-art approaches.

</details>


### [81] [An Ethically Grounded LLM-Based Approach to Insider Threat Synthesis and Detection](https://arxiv.org/abs/2509.06920)
*Haywood Gelman,John D. Hastings,David Kenley*

Main category: cs.CR

TL;DR: 这篇论文提出了一种基于大语言模型的新题方法，使用Claude Sonnet 3.7动态合成含有内部威胁指标的系统日志，并在内部威胁检测中表现出更优的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的内部威胁研究依赖静态和访问限制的数据集，限制了适应性检测模型的发展，需要新方法来动态生成更接近实际的数据。

Method: 采用基于大语言模型Claude Sonnet 3.7的符合道德规范的方法，动态合成系统日志消息，其中1%包含内部威胁场景。对比分析Claude Sonnet 3.7和GPT-4o在检测性能上的差异。

Result: Claude Sonnet 3.7在几乎所有统计指标上都显著超过GPT-4o，特别是在减少误报和提高检测准确性方面。结果显示准确率、召回率、MCC和ROC AUC都有显著改善。

Conclusion: 研究结果显示大语言模型在合成数据集生成和内部威胁检测方面具有强大潜力，Claude Sonnet 3.7的优异表现为该领域提供了新的解决方案。

Abstract: Insider threats are a growing organizational problem due to the complexity of
identifying their technical and behavioral elements. A large research body is
dedicated to the study of insider threats from technological, psychological,
and educational perspectives. However, research in this domain has been
generally dependent on datasets that are static and limited access which
restricts the development of adaptive detection models. This study introduces a
novel, ethically grounded approach that uses the large language model (LLM)
Claude Sonnet 3.7 to dynamically synthesize syslog messages, some of which
contain indicators of insider threat scenarios. The messages reflect real-world
data distributions by being highly imbalanced (1% insider threats). The syslogs
were analyzed for insider threats by both Claude Sonnet 3.7 and GPT-4o, with
their performance evaluated through statistical metrics including precision,
recall, MCC, and ROC AUC. Sonnet 3.7 consistently outperformed GPT-4o across
nearly all metrics, particularly in reducing false alarms and improving
detection accuracy. The results show strong promise for the use of LLMs in
synthetic dataset generation and insider threat detection.

</details>


### [82] [Neuro-Symbolic AI for Cybersecurity: State of the Art, Challenges, and Opportunities](https://arxiv.org/abs/2509.06921)
*Safayat Bin Hakim,Muhammad Adil,Alvaro Velasquez,Shouhuai Xu,Houbing Herbert Song*

Main category: cs.CR

TL;DR: 神经符号AI在网络安全领域具有革命性潜力，通过结合神经模式识别和符号推理解决传统AI的局限性，但存在双重用途风险，需要标准化框架和负责任开发。


<details>
  <summary>Details</summary>
Motivation: 传统AI方法在网络安全中存在概念基础不足、指令性有限以及与安全目标不对齐等根本性限制，需要新的混合方法来应对这些挑战。

Method: 通过分析127篇文献(2019-2025年7月)，引入G-I-A(基础-指令性-对齐)框架，评估网络防御和攻击中的神经符号系统，重点关注网络安全、恶意软件分析和网络操作。

Result: 多智能体神经符号架构具有优势，因果推理集成是最具变革性的进步，但存在标准化差距、计算复杂性和人机协作等实施挑战。自主系统在零日漏洞利用方面展现出强大能力，显著降低成本。

Conclusion: 神经符号AI改变了威胁动态，迫切需要社区驱动的标准化框架和负责任开发实践，确保技术进步服务于防御性网络安全目标并保持社会对齐。

Abstract: Traditional Artificial Intelligence (AI) approaches in cybersecurity exhibit
fundamental limitations: inadequate conceptual grounding leading to
non-robustness against novel attacks; limited instructibility impeding
analyst-guided adaptation; and misalignment with cybersecurity objectives.
Neuro-Symbolic (NeSy) AI has emerged with the potential to revolutionize
cybersecurity AI. However, there is no systematic understanding of this
emerging approach. These hybrid systems address critical cybersecurity
challenges by combining neural pattern recognition with symbolic reasoning,
enabling enhanced threat understanding while introducing concerning autonomous
offensive capabilities that reshape threat landscapes. In this survey, we
systematically characterize this field by analyzing 127 publications spanning
2019-July 2025. We introduce a Grounding-Instructibility-Alignment (G-I-A)
framework to evaluate these systems, focusing on both cyber defense and cyber
offense across network security, malware analysis, and cyber operations. Our
analysis shows advantages of multi-agent NeSy architectures and identifies
critical implementation challenges including standardization gaps,
computational complexity, and human-AI collaboration requirements that
constrain deployment. We show that causal reasoning integration is the most
transformative advancement, enabling proactive defense beyond correlation-based
approaches. Our findings highlight dual-use implications where autonomous
systems demonstrate substantial capabilities in zero-day exploitation while
achieving significant cost reductions, altering threat dynamics. We provide
insights and future research directions, emphasizing the urgent need for
community-driven standardization frameworks and responsible development
practices that ensure advancement serves defensive cybersecurity objectives
while maintaining societal alignment.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [83] [Attention of a Kiss: Exploring Attention Maps in Video Diffusion for XAIxArts](https://arxiv.org/abs/2509.05323)
*Adam Cole,Mick Grierson*

Main category: cs.AI

TL;DR: 本文提出了一种可视化视频扩散变换器中注意力机制的方法，通过提取交叉注意力图来分析文本到视频生成过程中的时空行为，为艺术创作提供新的工具和材料。


<details>
  <summary>Details</summary>
Motivation: 受早期视频艺术家操纵模拟视频信号创造新视觉美学的启发，本研究旨在探索生成式视频模型中注意力机制的可视化方法，为艺术家提供理解AI内部工作机制的工具。

Method: 基于开源Wan模型构建工具，提取和可视化交叉注意力图，通过探索性探测和艺术案例研究分析注意力图作为分析工具和艺术材料的潜力。

Result: 开发了一个可解释的工具，能够展示文本到视频生成过程中注意力的时空行为，证明了注意力图既可作为分析工具也可作为原始艺术材料使用。

Conclusion: 这项工作推动了可解释AI艺术(XAIxArts)领域的发展，邀请艺术家将AI内部工作机制重新作为创意媒介，为艺术与技术的融合开辟了新途径。

Abstract: This paper presents an artistic and technical investigation into the
attention mechanisms of video diffusion transformers. Inspired by early video
artists who manipulated analog video signals to create new visual aesthetics,
this study proposes a method for extracting and visualizing cross-attention
maps in generative video models. Built on the open-source Wan model, our tool
provides an interpretable window into the temporal and spatial behavior of
attention in text-to-video generation. Through exploratory probes and an
artistic case study, we examine the potential of attention maps as both
analytical tools and raw artistic material. This work contributes to the
growing field of Explainable AI for the Arts (XAIxArts), inviting artists to
reclaim the inner workings of AI as a creative medium.

</details>


### [84] [Perception Graph for Cognitive Attack Reasoning in Augmented Reality](https://arxiv.org/abs/2509.05324)
*Rongqian Chen,Shu Hong,Rifatul Islam,Mahdi Imani,G. Gary Tan,Tian Lan*

Main category: cs.AI

TL;DR: 通过建立Perception Graph模型来模拟人类在AR环境中的知觉解释过程，并计算知觉扬压分数，以检测和分析认知攻击的影响


<details>
  <summary>Details</summary>
Motivation: AR系统在战术环境中容易受到认知攻击的威胁，这些攻击会撩动用户知觉并严重影响决策能力

Method: 提出Perception Graph模型，首先模仿人类在MR环境中解释关键信息的过程，然后使用语义有意义的结构表示结果

Result: 模型能够计算反映知觉扬压程度的定量分数，提供了检测和分析认知攻击效果的稳健可测量方法

Conclusion: Perception Graph模型为AR系统提供了一种有效的方法来评估和应对认知攻击带来的威胁，提升了系统的安全性和可靠性

Abstract: Augmented reality (AR) systems are increasingly deployed in tactical
environments, but their reliance on seamless human-computer interaction makes
them vulnerable to cognitive attacks that manipulate a user's perception and
severely compromise user decision-making. To address this challenge, we
introduce the Perception Graph, a novel model designed to reason about human
perception within these systems. Our model operates by first mimicking the
human process of interpreting key information from an MR environment and then
representing the outcomes using a semantically meaningful structure. We
demonstrate how the model can compute a quantitative score that reflects the
level of perception distortion, providing a robust and measurable method for
detecting and analyzing the effects of such cognitive attacks.

</details>


### [85] [SynDelay: A Synthetic Dataset for Delivery Delay Prediction](https://arxiv.org/abs/2509.05325)
*Liming Xu,Yunbo Long,Alexandra Brintrup*

Main category: cs.AI

TL;DR: SynDelay是一个用于配送延迟预测的合成数据集，通过先进生成模型基于真实数据创建，既保持真实性又确保隐私，为供应链AI研究提供开放基准测试平台。


<details>
  <summary>Details</summary>
Motivation: 现有配送延迟预测数据集多为专有、规模小或维护不一致，限制了研究的可复现性和基准测试，需要高质量开放数据集来推动供应链AI发展。

Method: 使用基于真实数据训练的先进生成模型创建合成数据集，保留真实配送模式同时确保数据隐私，并提供基线结果和评估指标作为基准参考。

Result: 开发了SynDelay合成数据集，虽然不完全无噪声或不一致，但为预测建模提供了具有挑战性和实用性的测试平台，并通过Supply Chain Data Hub公开提供。

Conclusion: SynDelay数据集填补了供应链AI研究中开放数据集的空白，鼓励社区贡献数据集、模型和评估实践，共同推进该领域研究发展。

Abstract: Artificial intelligence (AI) is transforming supply chain management, yet
progress in predictive tasks -- such as delivery delay prediction -- remains
constrained by the scarcity of high-quality, openly available datasets.
Existing datasets are often proprietary, small, or inconsistently maintained,
hindering reproducibility and benchmarking. We present SynDelay, a synthetic
dataset designed for delivery delay prediction. Generated using an advanced
generative model trained on real-world data, SynDelay preserves realistic
delivery patterns while ensuring privacy. Although not entirely free of noise
or inconsistencies, it provides a challenging and practical testbed for
advancing predictive modelling. To support adoption, we provide baseline
results and evaluation metrics as initial benchmarks, serving as reference
points rather than state-of-the-art claims. SynDelay is publicly available
through the Supply Chain Data Hub, an open initiative promoting dataset sharing
and benchmarking in supply chain AI. We encourage the community to contribute
datasets, models, and evaluation practices to advance research in this area.
All code is openly accessible at https://supplychaindatahub.org.

</details>


### [86] [MVRS: The Multimodal Virtual Reality Stimuli-based Emotion Recognition Dataset](https://arxiv.org/abs/2509.05330)
*Seyed Muhammad Hossein Mousavi,Atiye Ilanloo*

Main category: cs.AI

TL;DR: MVRS数据集是一个多模态情感识别数据集，包含VR情绪刺激下的眼动、身体运动、EMG和GSR信号，通过特征提取和融合技术验证了数据集质量和情感可分离性。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏包含身体运动和生理信号的多模态数据集，限制了情感识别领域的发展，特别是在医疗、教育和汽车系统等AI应用领域。

Method: 收集13名参与者在VR情绪刺激（放松、恐惧、压力、悲伤、喜悦）下的同步多模态数据，使用眼动追踪、Kinect身体运动捕捉、EMG和GSR信号采集，采用早期和晚期融合技术进行特征融合，并用分类器评估。

Result: 成功创建了MVRS多模态数据集，验证了数据集的质量和不同情感状态的可分离性。

Conclusion: MVRS数据集为多模态情感计算领域提供了有价值的资源，推动了基于身体运动和生理信号的情感识别研究。

Abstract: Automatic emotion recognition has become increasingly important with the rise
of AI, especially in fields like healthcare, education, and automotive systems.
However, there is a lack of multimodal datasets, particularly involving body
motion and physiological signals, which limits progress in the field. To
address this, the MVRS dataset is introduced, featuring synchronized recordings
from 13 participants aged 12 to 60 exposed to VR based emotional stimuli
(relaxation, fear, stress, sadness, joy). Data were collected using eye
tracking (via webcam in a VR headset), body motion (Kinect v2), and EMG and GSR
signals (Arduino UNO), all timestamp aligned. Participants followed a unified
protocol with consent and questionnaires. Features from each modality were
extracted, fused using early and late fusion techniques, and evaluated with
classifiers to confirm the datasets quality and emotion separability, making
MVRS a valuable contribution to multimodal affective computing.

</details>


### [87] [Benchmarking Large Language Models for Personalized Guidance in AI-Enhanced Learning](https://arxiv.org/abs/2509.05346)
*Bo Yuan,Jiazi Hu*

Main category: cs.AI

TL;DR: 本研究对GPT-4o、DeepSeek-V3和GLM-4.5三种先进大语言模型在个性化学习辅导任务中进行实证比较，发现GPT-4o在准确性、清晰度和可操作性方面表现最佳。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型越来越多地被设想为个性化学习的智能助手，但在真实学习场景中的系统性对比评估仍然有限，需要实证研究来验证不同模型在辅导任务中的实际表现。

Method: 使用包含学生答案和正确性标签的数据集，要求每个模型分析测验、推断学生掌握情况并生成针对性指导。采用Gemini作为虚拟评判员进行多维度（准确性、清晰度、可操作性、适当性）的成对比较，并通过Bradley-Terry模型分析结果。

Result: GPT-4o总体上表现最优，其反馈信息更丰富、结构更好；DeepSeek-V3和GLM-4.5在某些方面有优势但一致性较低。

Conclusion: 研究证实了大语言模型作为高级教学助手提供个性化支持的可行性，并为未来LLM驱动的个性化学习实证研究提供了方法学指导。

Abstract: While Large Language Models (LLMs) are increasingly envisioned as intelligent
assistants for personalized learning, systematic head-to-head evaluations
within authentic learning scenarios remain limited. This study conducts an
empirical comparison of three state-of-the-art LLMs on a tutoring task that
simulates a realistic learning setting. Using a dataset comprising a student's
answers to ten questions of mixed formats with correctness labels, each LLM is
required to (i) analyze the quiz to identify underlying knowledge components,
(ii) infer the student's mastery profile, and (iii) generate targeted guidance
for improvement. To mitigate subjectivity and evaluator bias, we employ Gemini
as a virtual judge to perform pairwise comparisons along various dimensions:
accuracy, clarity, actionability, and appropriateness. Results analyzed via the
Bradley-Terry model indicate that GPT-4o is generally preferred, producing
feedback that is more informative and better structured than its counterparts,
while DeepSeek-V3 and GLM-4.5 demonstrate intermittent strengths but lower
consistency. These findings highlight the feasibility of deploying LLMs as
advanced teaching assistants for individualized support and provide
methodological guidance for future empirical research on LLM-driven
personalized learning.

</details>


### [88] [SasAgent: Multi-Agent AI System for Small-Angle Scattering Data Analysis](https://arxiv.org/abs/2509.05363)
*Lijie Ding,Changwoo Do*

Main category: cs.AI

TL;DR: SasAgent是一个基于大语言模型的多代理AI系统，用于自动化小角散射数据分析，通过SasView工具和文本交互实现高效科学工作流


<details>
  <summary>Details</summary>
Motivation: 为了解决小角散射数据分析的复杂性，提高自动化水平，利用LLM技术简化科学工作流程

Method: 采用多代理架构：协调代理解析用户提示，三个专业代理分别负责散射长度密度计算、合成数据生成和实验数据拟合，使用基于SasView的LLM友好工具

Result: 系统能够解释复杂提示、精确计算SLD、生成准确散射数据，并以高精度拟合实验数据集

Conclusion: 展示了LLM驱动的AI系统在简化科学工作流程和增强SAS研究自动化方面的巨大潜力

Abstract: We introduce SasAgent, a multi-agent AI system powered by large language
models (LLMs) that automates small-angle scattering (SAS) data analysis by
leveraging tools from the SasView software and enables user interaction via
text input. SasAgent features a coordinator agent that interprets user prompts
and delegates tasks to three specialized agents for scattering length density
(SLD) calculation, synthetic data generation, and experimental data fitting.
These agents utilize LLM-friendly tools to execute tasks efficiently. These
tools, including the model data tool, Retrieval-Augmented Generation (RAG)
documentation tool, bump fitting tool, and SLD calculator tool, are derived
from the SasView Python library. A user-friendly Gradio-based interface
enhances user accessibility. Through diverse examples, we demonstrate
SasAgent's ability to interpret complex prompts, calculate SLDs, generate
accurate scattering data, and fit experimental datasets with high precision.
This work showcases the potential of LLM-driven AI systems to streamline
scientific workflows and enhance automation in SAS research.

</details>


### [89] [Characterizing Fitness Landscape Structures in Prompt Engineering](https://arxiv.org/abs/2509.05375)
*Arend Hintze*

Main category: cs.AI

TL;DR: 本文通过自相关分析揭示了提示工程优化景观的结构特征，发现系统化提示生成产生平滑衰减的自相关，而多样化生成则呈现非单调模式，表明存在崎岖、分层结构的景观。


<details>
  <summary>Details</summary>
Motivation: 当前提示工程方法将提示优化视为黑盒问题，缺乏对优化景观拓扑结构的理解。本文旨在系统分析提示工程中的适应度景观结构。

Method: 使用语义嵌入空间中的自相关分析方法，在错误检测任务上比较两种提示生成策略：系统枚举（1,024个提示）和新颖性驱动的多样化生成（1,000个提示）。

Result: 发现系统化提示生成产生平滑衰减的自相关，而多样化生成呈现非单调模式，在中等语义距离处出现峰值相关性，表明存在崎岖、分层结构的景观。不同错误类型的崎岖程度各异。

Conclusion: 研究结果为理解提示工程优化景观的复杂性提供了实证基础，揭示了不同提示生成策略产生的不同景观拓扑结构。

Abstract: While prompt engineering has emerged as a crucial technique for optimizing
large language model performance, the underlying optimization landscape remains
poorly understood. Current approaches treat prompt optimization as a black-box
problem, applying sophisticated search algorithms without characterizing the
landscape topology they navigate. We present a systematic analysis of fitness
landscape structures in prompt engineering using autocorrelation analysis
across semantic embedding spaces. Through experiments on error detection tasks
with two distinct prompt generation strategies -- systematic enumeration (1,024
prompts) and novelty-driven diversification (1,000 prompts) -- we reveal
fundamentally different landscape topologies. Systematic prompt generation
yields smoothly decaying autocorrelation, while diversified generation exhibits
non-monotonic patterns with peak correlation at intermediate semantic
distances, indicating rugged, hierarchically structured landscapes.
Task-specific analysis across 10 error detection categories reveals varying
degrees of ruggedness across different error types. Our findings provide an
empirical foundation for understanding the complexity of optimization in prompt
engineering landscapes.

</details>


### [90] [Code Like Humans: A Multi-Agent Solution for Medical Coding](https://arxiv.org/abs/2509.05378)
*Andreas Motzfeldt,Joakim Edin,Casper L. Christensen,Christian Hardmeier,Lars Maaløe,Anna Rogers*

Main category: cs.AI

TL;DR: 提出Code Like Humans框架，首个支持完整ICD-10编码系统（7万+标签）的医疗编码代理框架，在罕见诊断代码上达到最佳性能


<details>
  <summary>Details</summary>
Motivation: 医疗编码需要将非结构化临床记录映射到诊断和程序代码，现有方法无法支持完整的ICD-10编码系统

Method: 基于大语言模型的代理框架，实现了官方人工专家编码指南

Result: 在罕见诊断代码上达到最佳性能，但微调判别分类器在高频代码上仍保持优势

Conclusion: 贡献了系统性能分析并识别了系统性的编码盲点（被系统低估的代码），为未来工作提供方向

Abstract: In medical coding, experts map unstructured clinical notes to alphanumeric
codes for diagnoses and procedures. We introduce Code Like Humans: a new
agentic framework for medical coding with large language models. It implements
official coding guidelines for human experts, and it is the first solution that
can support the full ICD-10 coding system (+70K labels). It achieves the best
performance to date on rare diagnosis codes (fine-tuned discriminative
classifiers retain an advantage for high-frequency codes, to which they are
limited). Towards future work, we also contribute an analysis of system
performance and identify its `blind spots' (codes that are systematically
undercoded).

</details>


### [91] [Murphys Laws of AI Alignment: Why the Gap Always Wins](https://arxiv.org/abs/2509.05381)
*Madhava Gaikwad*

Main category: cs.AI

TL;DR: 本文提出了对齐差距概念，通过KL倾斜形式化解释了为什么优化压力会放大代理奖励与真实人类意图之间的分歧，并提出了AI对齐的墨菲定律分类和三元悖论框架。


<details>
  <summary>Details</summary>
Motivation: 现有的人类偏好对齐方法（如RLHF、DPO等）虽然有效，但存在奖励破解、奉承、标注者漂移和错误泛化等重复性失败模式，需要统一的理论框架来理解这些失败的根本原因。

Method: 使用KL倾斜形式化分析优化压力对代理奖励与真实意图分歧的影响，组织失败模式为墨菲定律分类，提出对齐三元悖论框架，并通过小规模实证研究进行验证。

Result: 建立了对齐差距的统一理论视角，系统分类了AI对齐中的常见失败模式，提出了MAPS框架作为实际设计杠杆。

Conclusion: 本文不是提出不可能定理，而是提供了一个重构对齐辩论的视角，围绕结构限制和权衡提供更清晰的设计指导，为未来对齐方法设计提供了理论框架和实践工具。

Abstract: Large language models are increasingly aligned to human preferences through
reinforcement learning from human feedback (RLHF) and related methods such as
Direct Preference Optimization (DPO), Constitutional AI, and RLAIF. While
effective, these methods exhibit recurring failure patterns i.e., reward
hacking, sycophancy, annotator drift, and misgeneralization. We introduce the
concept of the Alignment Gap, a unifying lens for understanding recurring
failures in feedback-based alignment. Using a KL-tilting formalism, we
illustrate why optimization pressure tends to amplify divergence between proxy
rewards and true human intent. We organize these failures into a catalogue of
Murphys Laws of AI Alignment, and propose the Alignment Trilemma as a way to
frame trade-offs among optimization strength, value capture, and
generalization. Small-scale empirical studies serve as illustrative support.
Finally, we propose the MAPS framework (Misspecification, Annotation, Pressure,
Shift) as practical design levers. Our contribution is not a definitive
impossibility theorem but a perspective that reframes alignment debates around
structural limits and trade-offs, offering clearer guidance for future design.

</details>


### [92] [From Image Generation to Infrastructure Design: a Multi-agent Pipeline for Street Design Generation](https://arxiv.org/abs/2509.05469)
*Chenguang Wang,Xiang Yan,Yilong Dai,Ziyi Wang,Susu Xu*

Main category: cs.AI

TL;DR: 基于多段系统的自动化街道设计生成方案，能够直接在真实街景图像上编辑和重新设计自行车设施，提高公众参与效率


<details>
  <summary>Details</summary>
Motivation: 传统街道设计渲染方案耗时耗力，影响集体决策；现有AI生成方法需要大量领域数据且难以精确控制复杂街景中的空间变化

Method: 多段系统集成车道定位、提示优化、设计生成和自动评估模块，直接在真实街景图像上编辑自行车设施

Result: 系统在多样化城市场景中都能适应不同路径形状和环境条件，产生视觉一致且符合指令要求的结果

Conclusion: 为多段系统在交通基础设施规划和设计领域的应用奠定了基础

Abstract: Realistic visual renderings of street-design scenarios are essential for
public engagement in active transportation planning. Traditional approaches are
labor-intensive, hindering collective deliberation and collaborative
decision-making. While AI-assisted generative design shows transformative
potential by enabling rapid creation of design scenarios, existing generative
approaches typically require large amounts of domain-specific training data and
struggle to enable precise spatial variations of design/configuration in
complex street-view scenes. We introduce a multi-agent system that edits and
redesigns bicycle facilities directly on real-world street-view imagery. The
framework integrates lane localization, prompt optimization, design generation,
and automated evaluation to synthesize realistic, contextually appropriate
designs. Experiments across diverse urban scenarios demonstrate that the system
can adapt to varying road geometries and environmental conditions, consistently
yielding visually coherent and instruction-compliant results. This work
establishes a foundation for applying multi-agent pipelines to transportation
infrastructure planning and facility design.

</details>


### [93] [TreeGPT: A Novel Hybrid Architecture for Abstract Syntax Tree Processing with Global Parent-Child Aggregation](https://arxiv.org/abs/2509.05550)
*Zixi Li*

Main category: cs.AI

TL;DR: TreeGPT是一种新颖的神经网络架构，结合了transformer注意力机制和全局父子聚合，用于处理抽象语法树，在程序合成任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统方法要么依赖序列处理，要么使用图神经网络，无法有效捕捉AST的层次结构和全局依赖关系。需要一种能够同时处理局部依赖和层次树结构的混合架构。

Method: 采用混合设计：使用self-attention捕捉局部依赖，专门的TreeFFN通过迭代消息传递建模层次树结构。核心创新是全局父子聚合机制，允许节点通过T次迭代逐步聚合整个树结构的信息。

Result: 在ARC Prize 2025数据集上达到96%准确率，显著优于transformer基线(1.3%)、Grok-4(15.9%)和SOAR(52%)，仅使用150万参数。消融研究表明边缘投影是最关键组件。

Conclusion: TreeGPT通过结合注意力机制和树结构聚合，在程序合成任务中实现了卓越性能，证明了混合架构在处理层次树结构数据方面的有效性。

Abstract: We introduce TreeGPT, a novel neural architecture that combines
transformer-based attention mechanisms with global parent-child aggregation for
processing Abstract Syntax Trees (ASTs) in neural program synthesis tasks.
Unlike traditional approaches that rely solely on sequential processing or
graph neural networks, TreeGPT employs a hybrid design that leverages both
self-attention for capturing local dependencies and a specialized Tree
Feed-Forward Network (TreeFFN) for modeling hierarchical tree structures
through iterative message passing.
  The core innovation lies in our Global Parent-Child Aggregation mechanism,
formalized as: $$h_i^{(t+1)} = \sigma \Big( h_i^{(0)} + W_{pc} \sum_{(p,c) \in
E_i} f(h_p^{(t)}, h_c^{(t)}) + b \Big)$$ where $h_i^{(t)}$ represents the
hidden state of node $i$ at iteration $t$, $E_i$ denotes all parent-child edges
involving node $i$, and $f(h_p, h_c)$ is an edge aggregation function. This
formulation enables each node to progressively aggregate information from the
entire tree structure through $T$ iterations.
  Our architecture integrates optional enhancements including gated aggregation
with learnable edge weights, residual connections for gradient stability, and
bidirectional propagation for capturing both bottom-up and top-down
dependencies. We evaluate TreeGPT on the ARC Prize 2025 dataset, a challenging
visual reasoning benchmark requiring abstract pattern recognition and rule
inference. Experimental results demonstrate that TreeGPT achieves 96\%
accuracy, significantly outperforming transformer baselines (1.3\%),
large-scale models like Grok-4 (15.9\%), and specialized program synthesis
methods like SOAR (52\%) while using only 1.5M parameters. Our comprehensive
ablation study reveals that edge projection is the most critical component,
with the combination of edge projection and gating achieving optimal
performance.

</details>


### [94] [OccVLA: Vision-Language-Action Model with Implicit 3D Occupancy Supervision](https://arxiv.org/abs/2509.05578)
*Ruixun Liu,Lingyu Kong,Derun Li,Hang Zhao*

Main category: cs.AI

TL;DR: OccVLA是一个新颖的多模态框架，通过将3D占据表示集成到统一推理过程中，解决了MLLM在3D空间理解方面的局限性，无需昂贵的手动标注即可实现自动驾驶的精细空间结构学习。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型在视觉-语言推理方面表现出色，但在3D空间理解方面存在不足，这对于自动驾驶至关重要。主要挑战包括缺乏可访问且有效的3D表示构建方法，以及由于缺乏大规模3D视觉-语言预训练导致的细粒度空间细节丢失。

Method: 提出OccVLA框架，将密集3D占据既作为预测输出又作为监督信号，使模型能够直接从2D视觉输入学习细粒度空间结构。占据预测被视为隐式推理过程，在推理时可跳过而不影响性能，不增加额外计算开销。

Result: 在nuScenes基准测试中实现了轨迹规划的最先进结果，在3D视觉问答任务上表现出优越性能。

Conclusion: OccVLA为自动驾驶提供了一个可扩展、可解释且完全基于视觉的解决方案，通过创新的3D占据表示集成方法有效解决了MLLM在3D空间理解方面的核心挑战。

Abstract: Multimodal large language models (MLLMs) have shown strong vision-language
reasoning abilities but still lack robust 3D spatial understanding, which is
critical for autonomous driving. This limitation stems from two key challenges:
(1) the difficulty of constructing accessible yet effective 3D representations
without expensive manual annotations, and (2) the loss of fine-grained spatial
details in VLMs due to the absence of large-scale 3D vision-language
pretraining. To address these challenges, we propose OccVLA, a novel framework
that integrates 3D occupancy representations into a unified multimodal
reasoning process. Unlike prior approaches that rely on explicit 3D inputs,
OccVLA treats dense 3D occupancy as both a predictive output and a supervisory
signal, enabling the model to learn fine-grained spatial structures directly
from 2D visual inputs. The occupancy predictions are regarded as implicit
reasoning processes and can be skipped during inference without performance
degradation, thereby adding no extra computational overhead. OccVLA achieves
state-of-the-art results on the nuScenes benchmark for trajectory planning and
demonstrates superior performance on 3D visual question-answering tasks,
offering a scalable, interpretable, and fully vision-based solution for
autonomous driving.

</details>


### [95] [MSRFormer: Road Network Representation Learning using Multi-scale Feature Fusion of Heterogeneous Spatial Interactions](https://arxiv.org/abs/2509.05685)
*Jian Yang,Jiahui Wu,Li Fang,Hongchao Fan,Bianying Zhang,Huijie Zhao,Guangyi Yang,Rui Xin,Xiong You*

Main category: cs.AI

TL;DR: MSRFormer是一个新颖的道路网络表示学习框架，通过多尺度空间交互和轨迹数据整合，解决了道路网络异质性和层次性挑战，在道路网络分析任务中表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 城市道路网络的异质性和层次性给准确表示学习带来挑战，传统图神经网络由于同质性假设和单一结构尺度关注而难以有效处理这些问题。

Method: 使用空间流卷积从大型轨迹数据集中提取小尺度特征，识别尺度依赖的空间交互区域，通过图变换器捕捉多尺度复杂空间依赖，采用残差连接融合特征，最后使用对比学习算法得到最终表示。

Result: 在两个真实数据集上的验证表明，MSRFormer在道路网络分析任务中优于基线方法，性能提升达16%，特别是在交通相关任务和复杂道路网络结构中表现更佳。

Conclusion: 该研究为开发任务无关的道路网络表示模型提供了实用框架，揭示了尺度效应与空间交互流异质性之间的关联模式，证明了整合轨迹数据对交通相关任务的益处。

Abstract: Transforming road network data into vector representations using deep
learning has proven effective for road network analysis. However, urban road
networks' heterogeneous and hierarchical nature poses challenges for accurate
representation learning. Graph neural networks, which aggregate features from
neighboring nodes, often struggle due to their homogeneity assumption and focus
on a single structural scale. To address these issues, this paper presents
MSRFormer, a novel road network representation learning framework that
integrates multi-scale spatial interactions by addressing their flow
heterogeneity and long-distance dependencies. It uses spatial flow convolution
to extract small-scale features from large trajectory datasets, and identifies
scale-dependent spatial interaction regions to capture the spatial structure of
road networks and flow heterogeneity. By employing a graph transformer,
MSRFormer effectively captures complex spatial dependencies across multiple
scales. The spatial interaction features are fused using residual connections,
which are fed to a contrastive learning algorithm to derive the final road
network representation. Validation on two real-world datasets demonstrates that
MSRFormer outperforms baseline methods in two road network analysis tasks. The
performance gains of MSRFormer suggest the traffic-related task benefits more
from incorporating trajectory data, also resulting in greater improvements in
complex road network structures with up to 16% improvements compared to the
most competitive baseline method. This research provides a practical framework
for developing task-agnostic road network representation models and highlights
distinct association patterns of the interplay between scale effects and flow
heterogeneity of spatial interactions.

</details>


### [96] [Towards Meta-Cognitive Knowledge Editing for Multimodal LLMs](https://arxiv.org/abs/2509.05714)
*Zhaoyu Fan,Kaihang Pan,Mingze Zhou,Bosheng Qin,Juncheng Li,Shengyu Zhang,Wenqiao Zhang,Siliang Tang,Fei Wu,Yueting Zhuang*

Main category: cs.AI

TL;DR: CogEdit是一个评估多模态大语言模型元认知知识编辑能力的新基准，包含反事实驱动编辑、边界约束编辑和噪声鲁棒编辑三个层次。作者提出了MIND框架，通过元知识记忆、博弈论交互和标签精炼来提升元认知编辑能力。


<details>
  <summary>Details</summary>
Motivation: 现有的知识编辑基准主要关注认知层面的修改，缺乏对更深层次元认知过程的关注。为了填补这一空白，需要开发能够评估MLLMs元认知知识编辑能力的基准和方法。

Method: 提出了CogEdit基准，包含三个评估层次：反事实驱动编辑、边界约束编辑和噪声鲁棒编辑。同时提出了MIND框架，使用元知识记忆实现自我意识，通过博弈论交互监控知识激活，并采用标签精炼进行噪声鲁棒的更新。

Result: 大量实验表明，MIND框架在传统和元认知知识编辑基准上都显著优于现有的认知编辑方法，取得了强劲的性能表现。

Conclusion: 该研究填补了元认知知识编辑评估的空白，提出的CogEdit基准和MIND框架为提升多模态大语言模型的元认知编辑能力提供了有效解决方案，在知识更新和纠错方面表现出色。

Abstract: Knowledge editing enables multimodal large language models (MLLMs) to
efficiently update outdated or incorrect information. However, existing
benchmarks primarily emphasize cognitive-level modifications while lacking a
focus on deeper meta-cognitive processes. To bridge this gap, we introduce
CogEdit, a novel benchmark designed to evaluate MLLMs' meta-cognitive knowledge
editing abilities across three levels: (1) Counterfactual-Driven Editing,
assessing self-awareness of knowledge correctness changes; (2) Boundary
Constraint Editing, ensuring appropriate generalization without unintended
interference; and (3) Noise-Robust Editing, promoting reflective evaluation of
uncertain information. To advance meta-cognitive editing, we propose MIND
(Meta-cognitive INtegrated Dynamic Knowledge Editing), a framework that
constructs a meta-knowledge memory for self-awareness, employs game-theoretic
interactions to monitor knowledge activation, and incorporates label refinement
for noise-robust updates. Extensive experiments show that MIND significantly
outperforms existing cognitive editing approaches, achieving strong performance
on both traditional and meta-cognitive knowledge editing benchmarks.

</details>


### [97] [Hyperbolic Large Language Models](https://arxiv.org/abs/2509.05757)
*Sarang Patil,Zeyong Zhang,Yiran Huang,Tengfei Ma,Mengjia Xu*

Main category: cs.AI

TL;DR: 该论文综述了双曲几何在大型语言模型中的应用，提出了Hyperbolic LLMs的分类框架，包括四种主要技术类别，并探讨了潜在应用和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据具有非欧几里得的层次结构特性，而传统LLMs在处理这类数据时存在局限性。双曲几何能有效建模树状层次结构，因此需要探索如何将双曲几何整合到LLMs中以增强语义表示学习。

Method: 提出了Hyperbolic LLMs的四分类法：1)通过指数/对数映射的双曲LLMs；2)双曲微调模型；3)完全双曲LLMs；4)双曲状态空间模型。建立了相关论文、模型、数据集和代码的资源库。

Result: 系统梳理了双曲几何在LLMs中的最新进展，建立了完整的分类框架，为研究者提供了结构化的技术路线图和资源支持。

Conclusion: 双曲几何为LLMs处理层次结构数据提供了有前景的方向，未来需要在理论分析、计算效率和实际应用等方面进一步深入研究。

Abstract: Large language models (LLMs) have achieved remarkable success and
demonstrated superior performance across various tasks, including natural
language processing (NLP), weather forecasting, biological protein folding,
text generation, and solving mathematical problems. However, many real-world
data exhibit highly non-Euclidean latent hierarchical anatomy, such as protein
networks, transportation networks, financial networks, brain networks, and
linguistic structures or syntactic trees in natural languages. Effectively
learning intrinsic semantic entailment and hierarchical relationships from
these raw, unstructured input data using LLMs remains an underexplored area.
Due to its effectiveness in modeling tree-like hierarchical structures,
hyperbolic geometry -- a non-Euclidean space -- has rapidly gained popularity
as an expressive latent representation space for complex data modeling across
domains such as graphs, images, languages, and multi-modal data. Here, we
provide a comprehensive and contextual exposition of recent advancements in
LLMs that leverage hyperbolic geometry as a representation space to enhance
semantic representation learning and multi-scale reasoning. Specifically, the
paper presents a taxonomy of the principal techniques of Hyperbolic LLMs
(HypLLMs) in terms of four main categories: (1) hyperbolic LLMs through exp/log
maps; (2) hyperbolic fine-tuned models; (3) fully hyperbolic LLMs, and (4)
hyperbolic state-space models. We also explore crucial potential applications
and outline future research directions. A repository of key papers, models,
datasets, and code implementations is available at
https://github.com/sarangp2402/Hyperbolic-LLM-Models/tree/main.

</details>


### [98] [DRF: LLM-AGENT Dynamic Reputation Filtering Framework](https://arxiv.org/abs/2509.05764)
*Yuwei Lou,Hao Hu,Shaocong Ma,Zongfei Zhang,Liang Wang,Jidong Ge,Xianping Tao*

Main category: cs.AI

TL;DR: DRF是一种动态声誉过滤框架，通过评分网络、声誉计分和UCB策略来量化多代理系统中代理的性能和可靠性，显著提升任务完成质量和协作效率。


<details>
  <summary>Details</summary>
Motivation: 解决多代理系统中代理性能量化困难和缺乏代理可靠性评估机制的挑战。

Method: 构建交互式评分网络量化代理性能，设计声誉计分机制测量代理诚信度和能力，集成基于上信限界的策略提升代理选择效率。

Result: 在逻辑推理和代码生成任务中，DRF显著提高了任务完成质量和协作效率。

Conclusion: DRF为多代理系统处理大规模任务提供了新方法。

Abstract: With the evolution of generative AI, multi - agent systems leveraging large -
language models(LLMs) have emerged as a powerful tool for complex tasks.
However, these systems face challenges in quantifying agent performance and
lack mechanisms to assess agent credibility. To address these issues, we
introduce DRF, a dynamic reputation filtering framework. DRF constructs an
interactive rating network to quantify agent performance, designs a reputation
scoring mechanism to measure agent honesty and capability, and integrates an
Upper Confidence Bound - based strategy to enhance agent selection efficiency.
Experiments show that DRF significantly improves task completion quality and
collaboration efficiency in logical reasoning and code - generation tasks,
offering a new approach for multi - agent systems to handle large - scale
tasks.

</details>


### [99] [Decision-Focused Learning Enhanced by Automated Feature Engineering for Energy Storage Optimisation](https://arxiv.org/abs/2509.05772)
*Nasser Alkhulaifi,Ismail Gokay Dogan,Timothy R. Cargan,Alexander L. Bowler,Direnc Pekaslan,Nicholas J. Watson,Isaac Triguero*

Main category: cs.AI

TL;DR: 这篇论文提出了一种结合自动特征工程（AFE）和决策聚焦学习（DFL）的桌面，用于解决能源管理中的经济优化问题，在实际数据集上实现了更低的运营成本。


<details>
  <summary>Details</summary>
Motivation: 能源管理中的决策问题受到未知参数的影响，传统的预测-优化分离方法导致预测错误传播到决策中。虽然决策聚焦学习方法能够充分利用预测与优化的集成，但在实际应用中面临数据稀缺和变异性挑战。

Method: 研究提出了AFE-DFL桌面，通过自动特征工程提取更丰富的数据表征，并将其与决策聚焦学习方法相结合。该方法同时预测电价和需求，优化电池能量存储系统运行以最小化成本。

Result: 在英国实际房屋数据集上的评估显示，DFL方法比PTO方法平均节省更多运营成本。添加AFE后，DFL方法的性能进一步提升22.9-56.5%。

Conclusion: 这些结果为DFL在实际应用中的可行性提供了实证支撑，标志着领域特定的自动特征工程能够增强DFL方法，减少对领域专家知识的依赖，并在能源管理系统中带来经济效益。

Abstract: Decision-making under uncertainty in energy management is complicated by
unknown parameters hindering optimal strategies, particularly in Battery Energy
Storage System (BESS) operations. Predict-Then-Optimise (PTO) approaches treat
forecasting and optimisation as separate processes, allowing prediction errors
to cascade into suboptimal decisions as models minimise forecasting errors
rather than optimising downstream tasks. The emerging Decision-Focused Learning
(DFL) methods overcome this limitation by integrating prediction and
optimisation; however, they are relatively new and have been tested primarily
on synthetic datasets or small-scale problems, with limited evidence of their
practical viability. Real-world BESS applications present additional
challenges, including greater variability and data scarcity due to collection
constraints and operational limitations. Because of these challenges, this work
leverages Automated Feature Engineering (AFE) to extract richer representations
and improve the nascent approach of DFL. We propose an AFE-DFL framework
suitable for small datasets that forecasts electricity prices and demand while
optimising BESS operations to minimise costs. We validate its effectiveness on
a novel real-world UK property dataset. The evaluation compares DFL methods
against PTO, with and without AFE. The results show that, on average, DFL
yields lower operating costs than PTO and adding AFE further improves the
performance of DFL methods by 22.9-56.5% compared to the same models without
AFE. These findings provide empirical evidence for DFL's practical viability in
real-world settings, indicating that domain-specific AFE enhances DFL and
reduces reliance on domain expertise for BESS optimisation, yielding economic
benefits with broader implications for energy management systems facing similar
challenges.

</details>


### [100] [Chatbot To Help Patients Understand Their Health](https://arxiv.org/abs/2509.05818)
*Won Seok Jang,Hieu Tran,Manav Mistry,SaiKiran Gandluri,Yifan Zhang,Sharmin Sultana,Sunjae Kown,Yuan Zhang,Zonghai Yao,Hong Yu*

Main category: cs.AI

TL;DR: NoteAid-Chatbot是一个基于多智能体LLM和强化学习的对话AI系统，通过'学习即对话'框架帮助患者理解医疗信息，使用轻量级LLaMA 3.2 3B模型，无需人工标注数据即可实现清晰、相关和结构化对话等关键行为。


<details>
  <summary>Details</summary>
Motivation: 患者需要具备必要的知识来积极参与自己的护理过程，因此需要开发能够促进患者理解的对话AI系统。

Method: 采用两阶段训练：首先在合成生成的医疗对话数据上进行监督微调，然后通过强化学习（PPO）在模拟医院出院场景中进行训练，奖励基于患者理解评估。

Result: NoteAid-Chatbot展现出关键的新兴行为（清晰性、相关性和结构化对话），在图灵测试中超越非专家人类表现，证明轻量级模型也能处理多轮交互和复杂沟通目标。

Conclusion: 该框架展示了低成本PPO强化学习在现实开放域对话中的可行性和前景，拓宽了基于RL的对齐方法的适用性，虽然当前专注于医疗领域但具有广泛适用性。

Abstract: Patients must possess the knowledge necessary to actively participate in
their care. We present NoteAid-Chatbot, a conversational AI that promotes
patient understanding via a novel 'learning as conversation' framework, built
on a multi-agent large language model (LLM) and reinforcement learning (RL)
setup without human-labeled data. NoteAid-Chatbot was built on a lightweight
LLaMA 3.2 3B model trained in two stages: initial supervised fine-tuning on
conversational data synthetically generated using medical conversation
strategies, followed by RL with rewards derived from patient understanding
assessments in simulated hospital discharge scenarios. Our evaluation, which
includes comprehensive human-aligned assessments and case studies, demonstrates
that NoteAid-Chatbot exhibits key emergent behaviors critical for patient
education, such as clarity, relevance, and structured dialogue, even though it
received no explicit supervision for these attributes. Our results show that
even simple Proximal Policy Optimization (PPO)-based reward modeling can
successfully train lightweight, domain-specific chatbots to handle multi-turn
interactions, incorporate diverse educational strategies, and meet nuanced
communication objectives. Our Turing test demonstrates that NoteAid-Chatbot
surpasses non-expert human. Although our current focus is on healthcare, the
framework we present illustrates the feasibility and promise of applying
low-cost, PPO-based RL to realistic, open-ended conversational domains,
broadening the applicability of RL-based alignment methods.

</details>


### [101] [MapAgent: A Hierarchical Agent for Geospatial Reasoning with Dynamic Map Tool Integration](https://arxiv.org/abs/2509.05933)
*Md Hasebul Hasan,Mahir Labib Dihan,Mohammed Eunus Ali,Md Rizwan Parvez*

Main category: cs.AI

TL;DR: MapAgent是一个分层多智能体框架，专门用于地理空间推理任务，通过解耦规划与执行来提高工具选择准确性和API协调能力


<details>
  <summary>Details</summary>
Motivation: 现有AI代理框架主要针对数学、编程等领域，在地理空间任务中表现不佳，需要空间推理、多跳规划和实时地图交互能力

Method: 采用分层多智能体架构：高层规划器分解复杂查询为子目标，专用地图工具代理并行协调相关API，简单模块无需额外代理开销

Result: 在四个地理空间基准测试(MapEval-Textual、MapEval-API、MapEval-Visual、MapQA)上显著优于现有工具增强和代理基线方法

Conclusion: MapAgent通过分层设计和专用工具集有效解决了地理空间推理的挑战，为地图集成的地理空间AI应用提供了高效框架

Abstract: Agentic AI has significantly extended the capabilities of large language
models (LLMs) by enabling complex reasoning and tool use. However, most
existing frameworks are tailored to domains such as mathematics, coding, or web
automation, and fall short on geospatial tasks that require spatial reasoning,
multi-hop planning, and real-time map interaction. To address these challenges,
we introduce MapAgent, a hierarchical multi-agent plug-and-play framework with
customized toolsets and agentic scaffolds for map-integrated geospatial
reasoning. Unlike existing flat agent-based approaches that treat tools
uniformly-often overwhelming the LLM when handling similar but subtly different
geospatial APIs-MapAgent decouples planning from execution. A high-level
planner decomposes complex queries into subgoals, which are routed to
specialized modules. For tool-heavy modules-such as map-based services-we then
design a dedicated map-tool agent that efficiently orchestrates related APIs
adaptively in parallel to effectively fetch geospatial data relevant for the
query, while simpler modules (e.g., solution generation or answer extraction)
operate without additional agent overhead. This hierarchical design reduces
cognitive load, improves tool selection accuracy, and enables precise
coordination across similar APIs. We evaluate MapAgent on four diverse
geospatial benchmarks-MapEval-Textual, MapEval-API, MapEval-Visual, and
MapQA-and demonstrate substantial gains over state-of-the-art tool-augmented
and agentic baselines. We open-source our framwork at
https://github.com/Hasebul/MapAgent.

</details>


### [102] [Rethinking Reasoning Quality in Large Language Models through Enhanced Chain-of-Thought via RL](https://arxiv.org/abs/2509.06024)
*Haoyang He,Zihua Rong,Kun Ji,Chenyang Li,Qing Huang,Chong Xia,Lan Yang,Honggang Zhang*

Main category: cs.AI

TL;DR: 提出了DRER强化学习奖励框架，通过推理质量奖励和动态长度优势来优化大语言模型的推理能力，在Logictree数据集上7B模型达到GPT-o3-mini水平，推理置信度提升30%


<details>
  <summary>Details</summary>
Motivation: 现有基于规则的奖励函数只评估答案格式和正确性，无法判断推理链是否真正改善答案，且任务特定训练对逻辑深度的控制有限，难以揭示模型的真实推理能力

Method: DRER框架包含：(1)推理质量奖励 - 为那些确实提高正确答案可能性的推理链分配细粒度信用；(2)动态长度优势 - 对偏离验证阈值的响应长度进行优势衰减，稳定训练。使用Logictree数据集进行训练和评估

Result: 7B模型在400训练步数后达到GPT-o3-mini水平，CoT增强答案的平均置信度提升30%，在多种逻辑推理数据集和AIME24数学基准上展现出良好的泛化能力

Conclusion: DRER有效塑造了CoT行为，为增强大语言模型的形式推理能力提供了实用路径，所有代码和数据已开源

Abstract: Reinforcement learning (RL) has recently become the dominant paradigm for
strengthening the reasoning abilities of large language models (LLMs). Yet the
rule-based reward functions commonly used on mathematical or programming
benchmarks assess only answer format and correctness, providing no signal as to
whether the induced Chain-of-Thought (CoT) actually improves the answer.
Furthermore, such task-specific training offers limited control over logical
depth and therefore may fail to reveal a model's genuine reasoning capacity. We
propose Dynamic Reasoning Efficiency Reward (DRER) -- a plug-and-play RL reward
framework that reshapes both reward and advantage signals. (i) A Reasoning
Quality Reward assigns fine-grained credit to those reasoning chains that
demonstrably raise the likelihood of the correct answer, directly incentivising
the trajectories with beneficial CoT tokens. (ii) A Dynamic Length Advantage
decays the advantage of responses whose length deviates from a
validation-derived threshold, stabilising training. To facilitate rigorous
assessment, we also release Logictree, a dynamically constructed deductive
reasoning dataset that functions both as RL training data and as a
comprehensive benchmark. Experiments confirm the effectiveness of DRER: our 7B
model attains GPT-o3-mini level performance on Logictree with 400 trianing
steps, while the average confidence of CoT-augmented answers rises by 30%. The
model further exhibits generalisation across diverse logical-reasoning
datasets, and the mathematical benchmark AIME24. These results illuminate how
RL shapes CoT behaviour and chart a practical path toward enhancing
formal-reasoning skills in large language models. All code and data are
available in repository https://github.com/Henryhe09/DRER.

</details>


### [103] [Reverse-Engineered Reasoning for Open-Ended Generation](https://arxiv.org/abs/2509.06160)
*Haozhe Wang,Haoran Que,Qixin Xu,Minghao Liu,Wangchunshu Zhou,Jiazhan Feng,Wanjun Zhong,Wei Ye,Tong Yang,Wenhao Huang,Ge Zhang,Fangzhen Lin*

Main category: cs.AI

TL;DR: REER是一种逆向工程推理新范式，从已知优秀解决方案反向推导推理过程，解决了传统RL和蒸馏方法在开放式创作任务中的局限性


<details>
  <summary>Details</summary>
Motivation: 传统深度推理方法（强化学习和指令蒸馏）在开放式创作任务中存在明显缺陷：RL缺乏明确奖励信号，蒸馏方法成本高昂且受限于教师模型能力

Method: 提出REER逆向工程推理范式，从已知优秀解决方案反向计算推导出潜在的逐步深度推理过程，采用可扩展的无梯度方法，构建了包含2万条深度推理轨迹的大规模数据集DeepWriting-20K

Result: 基于该数据训练的DeepWriter-8B模型不仅超越开源基线模型，而且在性能上与GPT-4o和Claude 3.5等领先专有模型竞争甚至在某些方面更优

Conclusion: REER范式为开放式创作任务的深度推理提供了新的有效途径，通过逆向工程方法成功解决了传统方法的局限性，在大规模数据集上验证了其有效性

Abstract: While the ``deep reasoning'' paradigm has spurred significant advances in
verifiable domains like mathematics, its application to open-ended, creative
generation remains a critical challenge. The two dominant methods for
instilling reasoning -- reinforcement learning (RL) and instruction
distillation -- falter in this area; RL struggles with the absence of clear
reward signals and high-quality reward models, while distillation is
prohibitively expensive and capped by the teacher model's capabilities. To
overcome these limitations, we introduce REverse-Engineered Reasoning (REER), a
new paradigm that fundamentally shifts the approach. Instead of building a
reasoning process ``forwards'' through trial-and-error or imitation, REER works
``backwards'' from known-good solutions to computationally discover the latent,
step-by-step deep reasoning process that could have produced them. Using this
scalable, gradient-free approach, we curate and open-source DeepWriting-20K, a
large-scale dataset of 20,000 deep reasoning trajectories for open-ended tasks.
Our model, DeepWriter-8B, trained on this data, not only surpasses strong
open-source baselines but also achieves performance competitive with, and at
times superior to, leading proprietary models like GPT-4o and Claude 3.5.

</details>


### [104] [From Long to Short: LLMs Excel at Trimming Own Reasoning Chains](https://arxiv.org/abs/2509.06174)
*Wei Han,Geng Zhan,Sicheng Yu,Chenyu Wang,Bryan Hooi*

Main category: cs.AI

TL;DR: EDIT是一种测试时缩放方法，通过约束引导生成和联合跟踪长度与答案分布，帮助大型推理模型找到最短的正确推理路径，平衡简洁性和正确性。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型(LRMs)在处理简单问题时容易过度思考，产生冗长复杂的推理轨迹，影响可解释性和效率。研究发现LRMs难以平衡正确性和简洁性等多个生成目标。

Method: 提出EDIT(Efficient Dynamic Inference Trimming)方法，在测试时使用约束引导生成，同时在不同约束下联合跟踪长度和答案分布，选择在简洁性和正确性之间达到最优平衡的响应。

Result: 在多个模型和数据集上的广泛实验表明，EDIT显著提高了推理效率，产生紧凑而信息丰富的输出，改善了可读性和用户体验。

Conclusion: EDIT方法有效解决了LRMs的过度思考问题，通过动态推理修剪实现了推理效率的显著提升，为大型推理模型的实用化提供了重要改进。

Abstract: O1/R1 style large reasoning models (LRMs) signal a substantial leap forward
over conventional instruction-following LLMs. By applying test-time scaling to
generate extended reasoning paths, they establish many SOTAs across a wide
range of complex reasoning tasks. However, recent studies show that LRMs are
prone to suffer from overthinking -- the tendency to overcomplicate simple
problems, leading to excessive strategy switching and long, convoluted
reasoning traces that hinder their interpretability. To mitigate this issue, we
conduct a systematic investigation into the reasoning efficiency of a broad set
of LRMs and uncover a common dilemma: the difficulty in balancing multiple
generation objectives such as correctness and brevity. Based on this discovery,
we propose a test-time scaling method, EDIT (Efficient Dynamic Inference
Trimming), which efficiently guides LRMs to identify the shortest correct
reasoning paths at test time. EDIT employs constraint-guided generation while
jointly tracking length and answer distributions under varying constraints,
allowing it to select responses that strike an optimal balance between
conciseness and correctness. Extensive experiments across diverse models and
datasets show that EDIT substantially enhance the reasoning efficiency,
producing compact yet informative outputs that improve readability and user
experience.

</details>


### [105] [PillagerBench: Benchmarking LLM-Based Agents in Competitive Minecraft Team Environments](https://arxiv.org/abs/2509.06235)
*Olivier Schipper,Yudi Zhang,Yali Du,Mykola Pechenizkiy,Meng Fang*

Main category: cs.AI

TL;DR: 提出了PillagerBench框架用于评估Minecraft中的实时竞争性多智能体系统，并开发了TactiCrafter智能体系统，该系统通过可读战术促进团队合作，在竞争中表现出色并展示了自适应学习能力。


<details>
  <summary>Details</summary>
Motivation: 当前LLM智能体在合作和策略推理任务中表现良好，但在竞争性多智能体环境中的有效性尚未充分探索，需要专门框架进行评估。

Method: 开发了PillagerBench评估框架，提供可扩展API、多轮测试和基于规则的对手；提出了TactiCrafter多智能体系统，使用人类可读战术、学习因果依赖并适应对手策略。

Result: TactiCrafter在性能上优于基线方法，通过自我对弈展示了自适应学习能力，并在多轮游戏中分析了其学习过程和策略演化。

Conclusion: PillagerBench为竞争性环境中的多智能体AI研究提供了重要工具，TactiCrafter展示了LLM智能体在竞争环境中的潜力，框架已开源以促进进一步研究。

Abstract: LLM-based agents have shown promise in various cooperative and strategic
reasoning tasks, but their effectiveness in competitive multi-agent
environments remains underexplored. To address this gap, we introduce
PillagerBench, a novel framework for evaluating multi-agent systems in
real-time competitive team-vs-team scenarios in Minecraft. It provides an
extensible API, multi-round testing, and rule-based built-in opponents for
fair, reproducible comparisons. We also propose TactiCrafter, an LLM-based
multi-agent system that facilitates teamwork through human-readable tactics,
learns causal dependencies, and adapts to opponent strategies. Our evaluation
demonstrates that TactiCrafter outperforms baseline approaches and showcases
adaptive learning through self-play. Additionally, we analyze its learning
process and strategic evolution over multiple game episodes. To encourage
further research, we have open-sourced PillagerBench, fostering advancements in
multi-agent AI for competitive environments.

</details>


### [106] [Proof2Silicon: Prompt Repair for Verified Code and Hardware Generation via Reinforcement Learning](https://arxiv.org/abs/2509.06239)
*Manvi Jha,Jiaxin Wan,Deming Chen*

Main category: cs.AI

TL;DR: Proof2Silicon是一个端到端硬件合成框架，通过PREFACE的强化学习提示优化生成可验证的Dafny代码，然后自动转换为C代码并最终合成RTL硬件，在100任务基准测试中达到72%的端到端硬件合成成功率。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型生成的代码经常无法通过形式化验证的问题，特别是在硬件和安全关键领域需要正确性保证的场景。

Method: 1) 使用PREFACE的验证器驱动RL代理迭代优化提示生成，确保Dafny代码正确性；2) 将验证后的Dafny程序自动转换为可合成的高级C代码；3) 使用Vivado HLS生成RTL实现。

Result: PREFACE的RL引导提示优化将Dafny验证成功率提高了21%，Proof2Silicon实现了高达72%的端到端硬件合成成功率。

Conclusion: 该研究展示了一个强大、可扩展且自动化的流水线，能够实现从自然语言规范到硅实现的LLM驱动的形式化验证硬件合成。

Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities in
automated code generation but frequently produce code that fails formal
verification, an essential requirement for hardware and safety-critical
domains. To overcome this fundamental limitation, we previously proposed
PREFACE, a model-agnostic framework based on reinforcement learning (RL) that
iteratively repairs the prompts provided to frozen LLMs, systematically
steering them toward generating formally verifiable Dafny code without costly
fine-tuning. This work presents Proof2Silicon, a novel end-to-end synthesis
framework that embeds the previously proposed PREFACE flow to enable the
generation of correctness-by-construction hardware directly from natural
language specifications. Proof2Silicon operates by: (1) leveraging PREFACE's
verifier-driven RL agent to optimize prompt generation iteratively, ensuring
Dafny code correctness; (2) automatically translating verified Dafny programs
into synthesizable high-level C using Dafny's Python backend and PyLog; and (3)
employing Vivado HLS to produce RTL implementations. Evaluated rigorously on a
challenging 100-task benchmark, PREFACE's RL-guided prompt optimization
consistently improved Dafny verification success rates across diverse LLMs by
up to 21%. Crucially, Proof2Silicon achieved an end-to-end hardware synthesis
success rate of up to 72%, generating RTL designs through Vivado HLS synthesis
flows. These results demonstrate a robust, scalable, and automated pipeline for
LLM-driven, formally verified hardware synthesis, bridging natural-language
specification and silicon realization.

</details>


### [107] [REMI: A Novel Causal Schema Memory Architecture for Personalized Lifestyle Recommendation Agents](https://arxiv.org/abs/2509.06269)
*Vishal Raman,Vijai Aravindh R,Abhijith Ragav*

Main category: cs.AI

TL;DR: REMI是一个基于因果模式记忆的多模态生活方式助手，通过整合个人因果知识图谱、因果推理引擎和模式规划模块，提供可解释的个性化推荐


<details>
  <summary>Details</summary>
Motivation: 现有AI助手难以整合复杂的个人数据和因果知识，导致建议缺乏个性化和解释性

Method: 使用个人因果图谱记录用户生活事件和习惯，通过目标导向的因果遍历、外部知识增强和假设推理，结合LLM协调生成透明因果解释的行动计划

Result: 基于CSM的智能体比基线LLM智能体能提供更情境感知、用户对齐的推荐

Conclusion: 这项工作展示了在个性化智能体中实现记忆增强因果推理的新方法，推动了透明可信AI生活方式助手的发展

Abstract: Personalized AI assistants often struggle to incorporate complex personal
data and causal knowledge, leading to generic advice that lacks explanatory
power. We propose REMI, a Causal Schema Memory architecture for a multimodal
lifestyle agent that integrates a personal causal knowledge graph, a causal
reasoning engine, and a schema based planning module. The idea is to deliver
explainable, personalized recommendations in domains like fashion, personal
wellness, and lifestyle planning. Our architecture uses a personal causal graph
of the user's life events and habits, performs goal directed causal traversals
enriched with external knowledge and hypothetical reasoning, and retrieves
adaptable plan schemas to generate tailored action plans. A Large Language
Model orchestrates these components, producing answers with transparent causal
explanations. We outline the CSM system design and introduce new evaluation
metrics for personalization and explainability, including Personalization
Salience Score and Causal Reasoning Accuracy, to rigorously assess its
performance. Results indicate that CSM based agents can provide more context
aware, user aligned recommendations compared to baseline LLM agents. This work
demonstrates a novel approach to memory augmented, causal reasoning in
personalized agents, advancing the development of transparent and trustworthy
AI lifestyle assistants.

</details>


### [108] [TableMind: An Autonomous Programmatic Agent for Tool-Augmented Table Reasoning](https://arxiv.org/abs/2509.06278)
*Chuang Jiang,Mingyue Cheng,Xiaoyu Tao,Qingyang Mao,Jie Ouyang,Qi Liu*

Main category: cs.AI

TL;DR: TableMind是一个LLM驱动的表格推理代理，通过自主工具调用、代码执行和自反思能力，在安全沙箱环境中实现精确的数值推理，采用两阶段微调范式并在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决纯文本方法在复杂数值计算和细粒度操作上的不足，以及现有工具集成方法缺乏真正自主适应性的问题。

Method: 采用两阶段微调：监督微调建立有效工具使用模式，强化微调优化多目标策略，并提出Rank-Aware Policy Optimization (RAPO)方法。

Result: 在多个主流基准测试中实现了优于竞争基线的性能，在推理准确性和计算精度方面都有显著提升。

Conclusion: TableMind通过自主工具调用和代码执行能力，结合创新的微调策略，显著提升了表格推理任务的性能，为结构化数据处理提供了有效的解决方案。

Abstract: Table reasoning is crucial for leveraging structured data in domains such as
finance, healthcare, and scientific research. While large language models
(LLMs) show promise in multi-step reasoning, purely text-based methods often
struggle with the complex numerical computations and fine-grained operations
inherently required in this task. Tool-integrated reasoning improves
computational accuracy via explicit code execution, yet existing systems
frequently rely on rigid patterns, supervised imitation, and lack true
autonomous adaptability. In this paper, we present TableMind, an LLM-driven
table reasoning agent that (i) autonomously performs multi-turn tool
invocation, (ii) writes and executes data-analyzing code in a secure sandbox
environment for data analysis and precise numerical reasoning, and (iii)
exhibits high-level capabilities such as planning and self-reflection to adapt
strategies. To realize these capabilities, we adopt a two-stage fine-tuning
paradigm built on top of a powerful pre-trained language model: supervised
fine-tuning on high-quality reasoning trajectories to establish effective tool
usage patterns, followed by reinforcement fine-tuning to optimize
multi-objective strategies. In particular, we propose Rank-Aware Policy
Optimization (RAPO), which increases the update weight of high-quality
trajectories when their output probabilities are lower than those of
low-quality ones, thereby guiding the model more consistently toward better and
more accurate answers. Extensive experiments on several mainstream benchmarks
demonstrate that TableMind achieves superior performance compared to
competitive baselines, yielding substantial gains in both reasoning accuracy
and computational precision.

</details>


### [109] [SFR-DeepResearch: Towards Effective Reinforcement Learning for Autonomously Reasoning Single Agents](https://arxiv.org/abs/2509.06283)
*Xuan-Phi Nguyen,Shrey Pandit,Revanth Gangi Reddy,Austin Xu,Silvio Savarese,Caiming Xiong,Shafiq Joty*

Main category: cs.AI

TL;DR: 通过续代强化学习训练理性优化模型，发展自主单代理深度研究能力，在Humanity's Last Exam测试中达到28.7%的性能


<details>
  <summary>Details</summary>
Motivation: 为大语言模型编制复杂的交错推理和工具使用能力，开发自主单代理模型进行深度研究，充分利用最新的理性优化模型进步

Method: 提出简单的强化学习方法，使用全合成数据进行续代强化学习训练，集成最小化网络爬虫和Python工具

Result: 最佳模型SFR-DR-20B在Humanity's Last Exam测试中达到28.7%的性能，进行了关键分析实验以提供更多洞察

Conclusion: 通过续代强化学习方法可以有效提升理性优化模型的代理技能，同时保持其推理能力，为自主单代理深度研究提供了有效方案

Abstract: Equipping large language models (LLMs) with complex, interleaved reasoning
and tool-use capabilities has become a key focus in agentic AI research,
especially with recent advances in reasoning-oriented (``thinking'') models.
Such capabilities are key to unlocking a number of important applications. One
such application is Deep Research (DR), which requires extensive search and
reasoning over many sources. Our work in this paper focuses on the development
of native Autonomous Single-Agent models for DR featuring minimal web crawling
and Python tool integration. Unlike multi-agent systems, where agents take up
pre-defined roles and are told what to do at each step in a static workflow, an
autonomous single-agent determines its next action dynamically based on
context, without manual directive. While prior work has proposed training
recipes for base or instruction-tuned LLMs, we focus on continual reinforcement
learning (RL) of reasoning-optimized models to further enhance agentic skills
while preserving reasoning ability. Towards this end, we propose a simple RL
recipe with entirely synthetic data, which we apply to various open-source
LLMs. Our best variant SFR-DR-20B achieves up to 28.7% on Humanity's Last Exam
benchmark. In addition, we conduct key analysis experiments to provide more
insights into our methodologies.

</details>


### [110] [From Implicit Exploration to Structured Reasoning: Leveraging Guideline and Refinement for LLMs](https://arxiv.org/abs/2509.06284)
*Jiaxiang Chen,Zhuo Wang,Mingxi Zou,Zhucong Li,Zhijian Zhou,Song Wang,Zenglin Xu*

Main category: cs.AI

TL;DR: 提出了一个结构化推理框架，通过从成功轨迹中提取指导准则和从失败中获取反思信号，实现逐步执行和精炼，显著提升大语言模型的推理稳定性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型的推理方法依赖隐式探索，导致推理路径不稳定、缺乏错误纠正能力，且无法从过往经验中学习。

Method: 从成功轨迹中提取结构化推理模式，从失败中获取反思信号作为指导准则。在推理过程中逐步执行并应用精炼机制纠正错误。

Result: 在BBH、GSM8K、MATH-500、MBPP、HumanEval等基准测试中 consistently 超越强基线，推理稳定性和泛化能力显著提升。

Conclusion: 结构化推理框架通过指导准则和逐步精炼机制，有效解决了隐式探索的问题，在效果和可扩展性上达到或超过了监督微调的水平。

Abstract: Large language models (LLMs) have advanced general-purpose reasoning, showing
strong performance across diverse tasks. However, existing methods often rely
on implicit exploration, where the model follows stochastic and unguided
reasoning paths-like walking without a map. This leads to unstable reasoning
paths, lack of error correction, and limited learning from past experience. To
address these issues, we propose a framework that shifts from implicit
exploration to structured reasoning through guideline and refinement. First, we
extract structured reasoning patterns from successful trajectories and
reflective signals from failures. During inference, the model follows these
guidelines step-by-step, with refinement applied after each step to correct
errors and stabilize the reasoning process. Experiments on BBH and four
additional benchmarks (GSM8K, MATH-500, MBPP, HumanEval) show that our method
consistently outperforms strong baselines across diverse reasoning tasks.
Structured reasoning with stepwise execution and refinement improves stability
and generalization, while guidelines transfer well across domains and flexibly
support cross-model collaboration, matching or surpassing supervised
fine-tuning in effectiveness and scalability.

</details>


### [111] [Can AI Make Energy Retrofit Decisions? An Evaluation of Large Language Models](https://arxiv.org/abs/2509.06307)
*Lei Shu,Dong Zhao*

Main category: cs.AI

TL;DR: 评估7个大语言模型在住宅节能改造决策中的表现，发现在技术目标上表现更好，最高达到54.5%的top1匹配率，但模型间一致性较低，需要改进准确性、一致性和上下文处理能力。


<details>
  <summary>Details</summary>
Motivation: 传统建筑节能改造决策方法通用性有限且可解释性低，阻碍了在多样化住宅环境中的采用。随着智能社区的发展，生成式AI特别是大语言模型可能通过处理上下文信息来提供可读的建议。

Method: 评估7个LLM模型（ChatGPT、DeepSeek、Gemini、Grok、Llama和Claude）在住宅改造决策中的表现，使用包含49个州400个住宅的数据集，从准确性、一致性、敏感性和推理四个维度进行评估。

Result: LLM在许多情况下能生成有效建议，未经微调情况下达到54.5%的top1匹配率和92.8%的top5匹配率。技术目标表现更强，模型间一致性低，对位置和建筑几何形状敏感但对技术和居住者行为不敏感。

Conclusion: LLM在能源改造决策中是很有前景的助手，但需要在准确性、一致性和上下文处理方面进行改进才能可靠应用于实践。

Abstract: Conventional approaches to building energy retrofit decision making suffer
from limited generalizability and low interpretability, hindering adoption in
diverse residential contexts. With the growth of Smart and Connected
Communities, generative AI, especially large language models (LLMs), may help
by processing contextual information and producing practitioner readable
recommendations. We evaluate seven LLMs (ChatGPT, DeepSeek, Gemini, Grok,
Llama, and Claude) on residential retrofit decisions under two objectives:
maximizing CO2 reduction (technical) and minimizing payback period
(sociotechnical). Performance is assessed on four dimensions: accuracy,
consistency, sensitivity, and reasoning, using a dataset of 400 homes across 49
US states. LLMs generate effective recommendations in many cases, reaching up
to 54.5 percent top 1 match and 92.8 percent within top 5 without fine tuning.
Performance is stronger for the technical objective, while sociotechnical
decisions are limited by economic trade offs and local context. Agreement
across models is low, and higher performing models tend to diverge from others.
LLMs are sensitive to location and building geometry but less sensitive to
technology and occupant behavior. Most models show step by step, engineering
style reasoning, but it is often simplified and lacks deeper contextual
awareness. Overall, LLMs are promising assistants for energy retrofit decision
making, but improvements in accuracy, consistency, and context handling are
needed for reliable practice.

</details>


### [112] [Large Language Models as Virtual Survey Respondents: Evaluating Sociodemographic Response Generation](https://arxiv.org/abs/2509.06337)
*Jianpeng Zhao,Chenyu Yuan,Weiming Luo,Haoling Xie,Guangwei Zhang,Steven Jige Quan,Zixuan Yuan,Pengyang Wang,Denghui Zhang*

Main category: cs.AI

TL;DR: 本文探索用大语言模型模拟虚拟问卷调查受访者，提出了两种模拟方式，并构建了一个综合性评测套件，为社会科学研究提供了可扩展的成本效益工具。


<details>
  <summary>Details</summary>
Motivation: 传统问卷调查方法成本高、耗时长且规模有限，需要寻找更可扩展和成本效益的新方法。

Method: 提出了两种模拟设置：部分属性模拟(PAS)和全属性模拟(FAS)，构建了LLM-S^3评测套件，涵盖11个真实数据集，并评估多个主流LLM模型的表现。

Result: 对多个主流LLM模型的评估显示了一致的预测性能趋势，识别了失败模式，并证明上下文和提示设计对模拟保真度的影响。

Conclusion: 这项工作为LLM驱动的问卷调查模拟建立了严谨的基础，为社会学研究和政策评估提供了可扩展的成本效益工具。

Abstract: Questionnaire-based surveys are foundational to social science research and
public policymaking, yet traditional survey methods remain costly,
time-consuming, and often limited in scale. This paper explores a new paradigm:
simulating virtual survey respondents using Large Language Models (LLMs). We
introduce two novel simulation settings, namely Partial Attribute Simulation
(PAS) and Full Attribute Simulation (FAS), to systematically evaluate the
ability of LLMs to generate accurate and demographically coherent responses. In
PAS, the model predicts missing attributes based on partial respondent
profiles, whereas FAS involves generating complete synthetic datasets under
both zero-context and context-enhanced conditions. We curate a comprehensive
benchmark suite, LLM-S^3 (Large Language Model-based Sociodemographic Survey
Simulation), that spans 11 real-world public datasets across four sociological
domains. Our evaluation of multiple mainstream LLMs (GPT-3.5/4 Turbo, LLaMA
3.0/3.1-8B) reveals consistent trends in prediction performance, highlights
failure modes, and demonstrates how context and prompt design impact simulation
fidelity. This work establishes a rigorous foundation for LLM-driven survey
simulations, offering scalable and cost-effective tools for sociological
research and policy evaluation. Our code and dataset are available at:
https://github.com/dart-lab-research/LLM-S-Cube-Benchmark

</details>


### [113] [Evaluating Multi-Turn Bargain Skills in LLM-Based Seller Agent](https://arxiv.org/abs/2509.06341)
*Issue Yishu Wang,Kakam Chong,Xiaofeng Wang,Xu Yan,DeXin Kong,Chen Ju,Ming Chen,Shuai Xiao,Shuguang Han,jufeng chen*

Main category: cs.AI

TL;DR: 这篇论文提出了一个用于评估在线二手市场中卖家调解机器人多轮详价能力的框架，重点考察机器人跟踪和解释买家意图的能力。


<details>
  <summary>Details</summary>
Motivation: 在线上二手市场中，多轮详价是卖家与买家交互的关键环节。大语言模型可以作为卖家代理与买家详价，详价效果直接受到机器人跟踪和准确解释长期详价中累计买家意图能力的影响。

Method: 研究提出了一个多轮评估框架：(1) 大规模电子商务详价基准测试集，涵盖622个类别、9,892个产品和3,014个任务；(2) 基于心理理论(ToM)的轮次评估框架，包含标注的买家意图，超越仅考虑结果的指标；(3) 从大量对话数据中提取可靠意图的自动化流程。

Result: 该框架能够测试卖家代理是否能够提取和跟踪买家意图，为详价机器人的能力评估提供了综合性的测量方法。

Conclusion: 这个研究为评估电子商务详价对话中卖家代理的详价能力提供了一个重要的多轮评估框架，通过统一的测试集、基于心理理论的评估方法和自动化意图提取流程，有助于更全面地评估机器人的详价效果。

Abstract: In online second-hand marketplaces, multi-turn bargaining is a crucial part
of seller-buyer interactions. Large Language Models (LLMs) can act as seller
agents, negotiating with buyers on behalf of sellers under given business
constraints. A critical ability for such agents is to track and accurately
interpret cumulative buyer intents across long negotiations, which directly
impacts bargaining effectiveness. We introduce a multi-turn evaluation
framework for measuring the bargaining ability of seller agents in e-commerce
dialogues. The framework tests whether an agent can extract and track buyer
intents. Our contributions are: (1) a large-scale e-commerce bargaining
benchmark spanning 622 categories, 9,892 products, and 3,014 tasks; (2) a
turn-level evaluation framework grounded in Theory of Mind (ToM) with annotated
buyer intents, moving beyond outcome-only metrics; and (3) an automated
pipeline that extracts reliable intent from massive dialogue data.

</details>


### [114] [A data-driven discretized CS:GO simulation environment to facilitate strategic multi-agent planning research](https://arxiv.org/abs/2509.06355)
*Yunzhe Wang,Volkan Ustun,Chris McGroarty*

Main category: cs.AI

TL;DR: DECOY是一个新颖的多智能体模拟器，通过将3D地形中的战略长期规划抽象为高级离散化模拟，同时保持低层次环境保真度，在计算效率和真实性之间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 现代复杂多智能体交互模拟环境需要在高度细节保真度和计算效率之间取得平衡。现有的模拟器往往要么过于简化失去真实性，要么计算成本过高难以进行大规模研究。

Method: 使用基于路径点的系统简化和离散化连续状态和动作，配合基于真实CS:GO比赛数据训练的神经预测和生成模型来重建事件结果。仅使用移动决策作为战术定位，无需显式建模瞄准和射击等低层次机制。

Result: 广泛评估表明，从人类数据生成的DECOY回放与原始游戏中观察到的回放非常接近。模拟环境能够准确模拟游戏玩法。

Conclusion: DECOY提供了一个有价值的工具，可用于推进战略多智能体规划和行为生成的研究，为复杂多智能体交互提供了高效且保真的模拟解决方案。

Abstract: Modern simulation environments for complex multi-agent interactions must
balance high-fidelity detail with computational efficiency. We present DECOY, a
novel multi-agent simulator that abstracts strategic, long-horizon planning in
3D terrains into high-level discretized simulation while preserving low-level
environmental fidelity. Using Counter-Strike: Global Offensive (CS:GO) as a
testbed, our framework accurately simulates gameplay using only movement
decisions as tactical positioning -- without explicitly modeling low-level
mechanics such as aiming and shooting. Central to our approach is a waypoint
system that simplifies and discretizes continuous states and actions, paired
with neural predictive and generative models trained on real CS:GO tournament
data to reconstruct event outcomes. Extensive evaluations show that replays
generated from human data in DECOY closely match those observed in the original
game. Our publicly available simulation environment provides a valuable tool
for advancing research in strategic multi-agent planning and behavior
generation.

</details>


### [115] [Teaching AI Stepwise Diagnostic Reasoning with Report-Guided Chain-of-Thought Learning](https://arxiv.org/abs/2509.06409)
*Yihong Luo,Wenwu He,Zhuo-Xu Cui,Dong Liang*

Main category: cs.AI

TL;DR: DiagCoT是一个多阶段框架，通过监督微调通用视觉语言模型，仅使用自由文本报告模拟放射科医生的逐步诊断推理，在疾病分类、病理定位和报告生成方面显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 开发可解释且具有诊断能力的AI系统，通过利用非结构化临床叙述来模拟放射科医生的逐步推理过程，解决现有模型在诊断准确性和可解释性方面的不足。

Method: 结合对比图像-报告调优进行领域对齐，使用思维链监督捕捉推理逻辑，并通过临床奖励信号进行强化调优以提高事实准确性和流畅性。

Result: 在MIMIC-CXR基准测试中，零样本疾病分类AUC从0.52提升至0.76（绝对增益0.24），病理定位mIoU从0.08提升至0.31（绝对增益0.23），报告生成BLEU从0.11提升至0.33（绝对增益0.22），在长尾疾病和外部数据集上优于LLaVA-Med和CXR-LLAVA等最先进模型。

Conclusion: DiagCoT通过将非结构化临床叙述转化为结构化监督，为开发可解释且具有诊断能力的放射学AI系统提供了一种可扩展的方法。

Abstract: This study presents DiagCoT, a multi-stage framework that applies supervised
fine-tuning to general-purpose vision-language models (VLMs) to emulate
radiologists' stepwise diagnostic reasoning using only free-text reports.
DiagCoT combines contrastive image-report tuning for domain alignment,
chain-of-thought supervision to capture inferential logic, and reinforcement
tuning with clinical reward signals to enhance factual accuracy and fluency. On
the MIMIC-CXR benchmark, DiagCoT improved zero-shot disease classification AUC
from 0.52 to 0.76 (absolute gain of 0.24), pathology grounding mIoU from 0.08
to 0.31 (absolute gain of 0.23), and report generation BLEU from 0.11 to 0.33
(absolute gain of 0.22). It outperformed state-of-the-art models including
LLaVA-Med and CXR-LLAVA on long-tailed diseases and external datasets. By
converting unstructured clinical narratives into structured supervision,
DiagCoT offers a scalable approach for developing interpretable and
diagnostically competent AI systems for radiology.

</details>


### [116] [Tree of Agents: Improving Long-Context Capabilities of Large Language Models through Multi-Perspective Reasoning](https://arxiv.org/abs/2509.06436)
*Song Yu,Xiaofei Xu,Ke Deng,Li Li,Lin Tian*

Main category: cs.AI

TL;DR: 提出了Tree of Agents (TOA)多智能体推理框架，通过将长输入分段处理、动态信息交换和树状结构协作，有效解决LLM长上下文处理中的"中间信息丢失"问题，在保持API开销相近的情况下显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型处理长上下文任务时的核心挑战：中间信息利用不足（lost in the middle问题），现有方法要么可能丢弃关键信息，要么导致注意力分散。

Method: TOA多智能体框架：将输入分段由独立智能体处理，生成局部认知；智能体沿树状路径动态交换信息进行协作推理；结合前缀哈希缓存和自适应剪枝策略提高效率。

Result: 基于LLaMA3.1-8B的TOA在多种长上下文任务上显著超越多个基线模型，性能与最新的大型商业模型（如Gemini1.5-pro）相当。

Conclusion: TOA框架通过多智能体协作有效缓解位置偏见和幻觉问题，在保持效率的同时显著提升长上下文处理能力，为LLM长文本理解提供了有效解决方案。

Abstract: Large language models (LLMs) face persistent challenges when handling
long-context tasks, most notably the lost in the middle issue, where
information located in the middle of a long input tends to be underutilized.
Some existing methods that reduce input have the risk of discarding key
information, while others that extend context windows often lead to attention
dispersion. To address these limitations, we propose Tree of Agents (TOA), a
multi-agent reasoning framework that segments the input into chunks processed
by independent agents. Each agent generates its local cognition, then agents
dynamically exchange information for collaborative reasoning along
tree-structured paths. TOA enables agents to probe different reasoning orders
for multi-perspective understanding, effectively mitigating position bias and
reducing hallucinations. To improve processing efficiency, we incorporate
prefix-hash caching and adaptive pruning strategies, achieving significant
performance improvements with comparable API overhead. Experiments show that
TOA, powered by compact LLaMA3.1-8B, significantly outperforms multiple
baselines and demonstrates comparable performance to the latest and much larger
commercial models, such as Gemini1.5-pro, on various long-context tasks. Code
is available at https://github.com/Aireduce952/Tree-of-Agents.

</details>


### [117] [HyFedRAG: A Federated Retrieval-Augmented Generation Framework for Heterogeneous and Privacy-Sensitive Data](https://arxiv.org/abs/2509.06444)
*Cheng Qian,Hainan Zhang,Yongxin Tong,Hong-Wei Zheng,Zhiming Zheng*

Main category: cs.AI

TL;DR: HyFedRAG是一个面向混合数据模态的联邦RAG框架，通过边缘-云协作机制在保护数据隐私的同时处理异构医疗数据，包括SQL、知识图谱和临床文档。


<details>
  <summary>Details</summary>
Motivation: 集中式RAG系统在处理异构和隐私敏感的分布式医疗数据时面临困难，特别是在查询罕见病案例时受到隐私约束和传统云系统处理多样化格式能力的限制。

Method: 基于Flower设计边缘-云协作RAG框架，边缘LLM将多样化数据转换为标准化隐私保护表示，云端LLM进行全局推理；集成轻量级本地检索器和隐私感知LLM，提供三种匿名化工具；设计三级缓存策略优化延迟。

Result: 在PMC-Patients数据集上的实验表明，HyFedRAG在检索质量、生成一致性和系统效率方面优于现有基线方法。

Conclusion: 该框架为结构化异构数据上的RAG提供了可扩展且隐私合规的解决方案，在敏感和多样化数据环境中释放了LLMs的潜力。

Abstract: Centralized RAG pipelines struggle with heterogeneous and privacy-sensitive
data, especially in distributed healthcare settings where patient data spans
SQL, knowledge graphs, and clinical notes. Clinicians face difficulties
retrieving rare disease cases due to privacy constraints and the limitations of
traditional cloud-based RAG systems in handling diverse formats and edge
devices. To address this, we introduce HyFedRAG, a unified and efficient
Federated RAG framework tailored for Hybrid data modalities. By leveraging an
edge-cloud collaborative mechanism, HyFedRAG enables RAG to operate across
diverse data sources while preserving data privacy. Our key contributions are:
(1) We design an edge-cloud collaborative RAG framework built on Flower, which
supports querying structured SQL data, semi-structured knowledge graphs, and
unstructured documents. The edge-side LLMs convert diverse data into
standardized privacy-preserving representations, and the server-side LLMs
integrates them for global reasoning and generation. (2) We integrate
lightweight local retrievers with privacy-aware LLMs and provide three
anonymization tools that enable each client to produce semantically rich,
de-identified summaries for global inference across devices. (3) To optimize
response latency and reduce redundant computation, we design a three-tier
caching strategy consisting of local cache, intermediate representation cache,
and cloud inference cache. Experimental results on PMC-Patients demonstrate
that HyFedRAG outperforms existing baselines in terms of retrieval quality,
generation consistency, and system efficiency. Our framework offers a scalable
and privacy-compliant solution for RAG over structural-heterogeneous data,
unlocking the potential of LLMs in sensitive and diverse data environments.

</details>


### [118] [Accelerate Scaling of LLM Alignment via Quantifying the Coverage and Depth of Instruction Set](https://arxiv.org/abs/2509.06463)
*Chengwei Wu,Li Du,Hanyu Zhao,Yiming Ju,Jiapu Wang,Tengfei Pan*

Main category: cs.AI

TL;DR: 该文章研究大语言模型对齐份额性能改善问题，提出了一种基于指令深度和语义覆盖度的新题指令选择方法，能够持续提升模型性能并实现"加速扩展"


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在下游任务中的应用增长，提高模型对齐性能和效率变得至关重要。但由于指令集分布的复杂性，当前的指令集精细方法无法在指令池持续扩大时持续改善性能

Method: 首先研究指令数据集分布与对齐模型性能关系的关键因素，发现指令深度和语义空间覆盖度是关键因素，能解释70%以上的模型损失。然后设计了一种同时最大化深度和语义覆盖度的指令选择算法

Result: 实验结果表明，该方法与最新的基准方法相比，能够以更快的速度持续提升模型性能

Conclusion: 通过重新定义指令选择的关键指标（深度和语义覆盖度），该方法能够有效解决当前指令选择方法在大规模指令池中性能推退的问题，实现了加速扩展的效果

Abstract: With the growing demand for applying large language models to downstream
tasks, improving model alignment performance and efficiency has become crucial.
Such a process involves selecting informative instructions from a candidate
pool. However, due to the complexity of instruction set distributions, the key
factors driving the performance of aligned models remain unclear. As a result,
current instruction set refinement methods fail to improve performance as the
instruction pool expands continuously. To address this issue, we first
investigate the key factors that influence the relationship between instruction
dataset distribution and aligned model performance. Based on these insights, we
propose a novel instruction data selection method. We identify that the depth
of instructions and the coverage of the semantic space are the crucial factors
determining downstream performance, which could explain over 70\% of the model
loss on the development set. We then design an instruction selection algorithm
to simultaneously maximize the depth and semantic coverage of the selected
instructions. Experimental results demonstrate that, compared to
state-of-the-art baseline methods, it can sustainably improve model performance
at a faster pace and thus achieve \emph{``Accelerated Scaling''}.

</details>


### [119] [MAS-Bench: A Unified Benchmark for Shortcut-Augmented Hybrid Mobile GUI Agents](https://arxiv.org/abs/2509.06477)
*Pengxiang Zhao,Guangyi Liu,Yaozhen Liang,Weiqing He,Zhengxi Lu,Yuehao Huang,Yaxuan Guo,Kexin Zhang,Hao Wang,Liang Liu,Yong Liu*

Main category: cs.AI

TL;DR: MAS-Bench是首个专门评估GUI-快捷方式混合智能体的基准测试，专注于移动领域，包含139个复杂任务和88个预定义快捷方式，旨在填补混合代理系统评估的空白。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏系统评估GUI操作与快捷方式（API、深度链接等）混合智能体的框架，需要建立基准来推动更高效GUI代理的发展。

Method: 构建包含139个跨11个真实应用复杂任务的基准，提供88个预定义快捷方式知识库，设计7个评估指标，测试代理自主生成快捷方式和工作流的能力。

Result: 实验显示混合代理相比纯GUI代理显著提高了任务成功率和效率，验证了评估代理快捷方式生成能力的有效性。

Conclusion: MAS-Bench填补了关键评估空白，为创建更高效、鲁棒的智能代理提供了基础平台，推动了混合范式在GUI自动化领域的发展。

Abstract: To enhance the efficiency of GUI agents on various platforms like smartphones
and computers, a hybrid paradigm that combines flexible GUI operations with
efficient shortcuts (e.g., API, deep links) is emerging as a promising
direction. However, a framework for systematically benchmarking these hybrid
agents is still underexplored. To take the first step in bridging this gap, we
introduce MAS-Bench, a benchmark that pioneers the evaluation of GUI-shortcut
hybrid agents with a specific focus on the mobile domain. Beyond merely using
predefined shortcuts, MAS-Bench assesses an agent's capability to autonomously
generate shortcuts by discovering and creating reusable, low-cost workflows. It
features 139 complex tasks across 11 real-world applications, a knowledge base
of 88 predefined shortcuts (APIs, deep-links, RPA scripts), and 7 evaluation
metrics. The tasks are designed to be solvable via GUI-only operations, but can
be significantly accelerated by intelligently embedding shortcuts. Experiments
show that hybrid agents achieve significantly higher success rates and
efficiency than their GUI-only counterparts. This result also demonstrates the
effectiveness of our method for evaluating an agent's shortcut generation
capabilities. MAS-Bench fills a critical evaluation gap, providing a
foundational platform for future advancements in creating more efficient and
robust intelligent agents.

</details>


### [120] [MORSE: Multi-Objective Reinforcement Learning via Strategy Evolution for Supply Chain Optimization](https://arxiv.org/abs/2509.06490)
*Niki Kotecha,Ehecatl Antonio del Rio Chanona*

Main category: cs.AI

TL;DR: 结合强化学习和多目标进化算法，提出动态多目标优化方法，用于供应链管理中实时决策，通过风险敏感机制提升不确定环境下的韧性。


<details>
  <summary>Details</summary>
Motivation: 传统多目标优化方法难以适应供应链的动态性和实时性，需要一种能够平衡成本、服务水平和可持续性等多重冲突目标，并在不确定环境中灵活调整的决策框架。

Method: 使用多目标进化算法搜索策略神经网络的参数空间，生成帕累托前沿策略集，结合条件风险价值(CVaR)进行风险敏感决策，实现动态策略切换。

Result: 在库存管理案例研究中，该方法能够有效响应供应链动态变化，优于现有先进方法，提高了决策效率和系统韧性。

Conclusion: 所提出的策略不仅提升了决策效率，还为管理不确定性和优化供应链性能提供了更鲁棒的框架，具有实际应用价值。

Abstract: In supply chain management, decision-making often involves balancing multiple
conflicting objectives, such as cost reduction, service level improvement, and
environmental sustainability. Traditional multi-objective optimization methods,
such as linear programming and evolutionary algorithms, struggle to adapt in
real-time to the dynamic nature of supply chains. In this paper, we propose an
approach that combines Reinforcement Learning (RL) and Multi-Objective
Evolutionary Algorithms (MOEAs) to address these challenges for dynamic
multi-objective optimization under uncertainty. Our method leverages MOEAs to
search the parameter space of policy neural networks, generating a Pareto front
of policies. This provides decision-makers with a diverse population of
policies that can be dynamically switched based on the current system
objectives, ensuring flexibility and adaptability in real-time decision-making.
We also introduce Conditional Value-at-Risk (CVaR) to incorporate
risk-sensitive decision-making, enhancing resilience in uncertain environments.
We demonstrate the effectiveness of our approach through case studies,
showcasing its ability to respond to supply chain dynamics and outperforming
state-of-the-art methods in an inventory management case study. The proposed
strategy not only improves decision-making efficiency but also offers a more
robust framework for managing uncertainty and optimizing performance in supply
chains.

</details>


### [121] [Scaling up Multi-Turn Off-Policy RL and Multi-Agent Tree Search for LLM Step-Provers](https://arxiv.org/abs/2509.06493)
*Ran Xin,Zeyu Zheng,Yanchen Nie,Kun Yuan,Xia Xiao*

Main category: cs.AI

TL;DR: BFS-Prover-V2是一个结合多轮离线强化学习和规划增强多智能体搜索架构的系统，用于解决LLM在自动定理证明中的训练和推理扩展问题，在数学证明基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在自动定理证明中面临训练时强化学习和推理时计算资源扩展的双重挑战，需要克服性能平台期和搜索空间过大的问题。

Method: 1. 多轮离线强化学习框架：受AlphaZero启发，采用多阶段专家迭代流程，包含自适应策略级数据过滤和定期重训练；2. 规划增强多智能体搜索架构：使用通用推理模型作为高层规划器分解复杂定理，通过共享证明缓存实现并行证明智能体协作。

Result: 在MiniF2F测试集上达到95.08%，在ProofNet测试集上达到41.4%的state-of-the-art性能。

Conclusion: 该工作提出的强化学习和推理技术不仅适用于形式数学领域，还可推广到其他需要长视野多轮推理和复杂搜索的领域。

Abstract: The integration of Large Language Models (LLMs) into automated theorem
proving has shown immense promise, yet is fundamentally constrained by
challenges in scaling up both training-time reinforcement learning (RL) and
inference-time compute. This paper introduces \texttt{BFS-Prover-V2}, a system
designed to address this dual scaling problem. We present two primary
innovations. The first is a novel multi-turn off-policy RL framework for
continually improving the performance of LLM step-prover at training time. This
framework, inspired by the principles of AlphaZero, utilizes a multi-stage
expert iteration pipeline featuring adaptive tactic-level data filtering and
periodic retraining to surmount the performance plateaus that typically curtail
long-term RL in LLM-based agents. The second innovation is a planner-enhanced
multi-agent search architecture that scales reasoning capabilities at inference
time. This architecture employs a general reasoning model as a high-level
planner to iteratively decompose complex theorems into a sequence of simpler
subgoals. This hierarchical approach substantially reduces the search space,
enabling a team of parallel prover agents to collaborate efficiently by
leveraging a shared proof cache. We demonstrate that this dual approach to
scaling yields state-of-the-art results on established formal mathematics
benchmarks. \texttt{BFS-Prover-V2} achieves 95.08\% and 41.4\% on the MiniF2F
and ProofNet test sets respectively. While demonstrated in the domain of formal
mathematics, the RL and inference techniques presented in this work are of
broader interest and may be applied to other domains requiring long-horizon
multi-turn reasoning and complex search.

</details>


### [122] [An AI system to help scientists write expert-level empirical software](https://arxiv.org/abs/2509.06503)
*Eser Aygün,Anastasiya Belyaeva,Gheorghe Comanici,Marc Coram,Hao Cui,Jake Garrison,Renee Johnston Anton Kast,Cory Y. McLean,Peter Norgaard,Zahra Shamsi,David Smalling,James Thompson,Subhashini Venugopalan,Brian P. Williams,Chujun He,Sarah Martinson,Martyna Plomecka,Lai Wei,Yuchen Zhou,Qian-Ze Zhu,Matthew Abraham,Erica Brand,Anna Bulanova,Jeffrey A. Cardille,Chris Co,Scott Ellsworth,Grace Joseph,Malcolm Kane,Ryan Krueger,Johan Kartiwa,Dan Liebling,Jan-Matthis Lueckmann,Paul Raccuglia,Xuefei,Wang,Katherine Chou,James Manyika,Yossi Matias,John C. Platt,Lizzie Dorfman,Shibl Mourad,Michael P. Brenner*

Main category: cs.AI

TL;DR: AI系统使用大语言模型和树搜索自动生成专家级科学软件，在多个领域超越人类开发的方法，显著加速科学发现进程


<details>
  <summary>Details</summary>
Motivation: 科学发现过程经常受限于手动创建计算实验软件的缓慢速度，需要自动化工具来突破这一瓶颈

Method: 结合大语言模型(LLM)和树搜索(TS)技术，系统性地改进质量指标并智能导航大型解决方案空间

Result: 在生物信息学中发现40种优于人类方法的新单细胞数据分析方法；在流行病学中生成14个超越CDC集成模型的新冠住院预测模型；在多个领域产生最先进软件

Conclusion: 该系统通过为多样化任务设计和实施新颖解决方案，代表着加速科学进步的重要一步

Abstract: The cycle of scientific discovery is frequently bottlenecked by the slow,
manual creation of software to support computational experiments. To address
this, we present an AI system that creates expert-level scientific software
whose goal is to maximize a quality metric. The system uses a Large Language
Model (LLM) and Tree Search (TS) to systematically improve the quality metric
and intelligently navigate the large space of possible solutions. The system
achieves expert-level results when it explores and integrates complex research
ideas from external sources. The effectiveness of tree search is demonstrated
across a wide range of benchmarks. In bioinformatics, it discovered 40 novel
methods for single-cell data analysis that outperformed the top human-developed
methods on a public leaderboard. In epidemiology, it generated 14 models that
outperformed the CDC ensemble and all other individual models for forecasting
COVID-19 hospitalizations. Our method also produced state-of-the-art software
for geospatial analysis, neural activity prediction in zebrafish, time series
forecasting and numerical solution of integrals. By devising and implementing
novel solutions to diverse tasks, the system represents a significant step
towards accelerating scientific progress.

</details>


### [123] [CogGuide: Human-Like Guidance for Zero-Shot Omni-Modal Reasoning](https://arxiv.org/abs/2509.06641)
*Zhou-Peng Shou,Zhi-Qiang You,Fang Wang,Hai-Bo Liu*

Main category: cs.AI

TL;DR: 提出基于"意图草图"的零样本多模态推理组件，通过三模块管道实现类人认知过程，无需参数微调即可抑制捷径推理并提升跨模态推理性能


<details>
  <summary>Details</summary>
Motivation: 解决多模态大模型在复杂跨模态推理中存在的"捷径"问题和上下文理解不足的问题

Method: 提出可插拔的三模块管道：意图感知器、策略生成器和策略选择器，通过生成和筛选"意图草图"策略来指导最终推理，无需参数微调，仅通过上下文工程实现跨模型迁移

Result: 在IntentBench、WorldSense和Daily-Omni数据集上验证了方法的通用性和稳健增益，相比基线获得一致改进，最高提升约9.51个百分点

Conclusion: "意图草图"推理组件在零样本场景中具有实用价值和可移植性，信息论分析显示该方法能降低条件熵并提高信息利用效率

Abstract: Targeting the issues of "shortcuts" and insufficient contextual understanding
in complex cross-modal reasoning of multimodal large models, this paper
proposes a zero-shot multimodal reasoning component guided by human-like
cognitive strategies centered on an "intent sketch". The component comprises a
plug-and-play three-module pipeline-Intent Perceiver, Strategy Generator, and
Strategy Selector-that explicitly constructs a "understand-plan-select"
cognitive process. By generating and filtering "intent sketch" strategies to
guide the final reasoning, it requires no parameter fine-tuning and achieves
cross-model transfer solely through in-context engineering.
Information-theoretic analysis shows that this process can reduce conditional
entropy and improve information utilization efficiency, thereby suppressing
unintended shortcut reasoning. Experiments on IntentBench, WorldSense, and
Daily-Omni validate the method's generality and robust gains; compared with
their respective baselines, the complete "three-module" scheme yields
consistent improvements across different reasoning engines and pipeline
combinations, with gains up to approximately 9.51 percentage points,
demonstrating the practical value and portability of the "intent sketch"
reasoning component in zero-shot scenarios.

</details>


### [124] [Reinforcement Learning Foundations for Deep Research Systems: A Survey](https://arxiv.org/abs/2509.06733)
*Wenjun Li,Zhi Chen,Jingru Lin,Hannan Cao,Wei Han,Sheng Liang,Zhi Zhang,Kuicai Dong,Dexun Li,Chen Zhang,Yong Liu*

Main category: cs.AI

TL;DR: 本文是首个专注于深度搜索系统强化学习基础的调查报告，系统化分析了数据合成、RL方法和训练框架等关键技术，为建立健壮透明的民主研究系统提供实用指南


<details>
  <summary>Details</summary>
Motivation: 当前深度研究系统主要依靠SFT和DPO方法，但这些方法存在模仿偏差、环境反馈利用不充分、依赖人工定义决策点等限制，而强化学习能够更好地解决这些问题

Method: 从三个轴心系统化分析相关研究：(i)数据合成与管理；(ii)研究型组件的RL方法（稳定性、样本效率、长上下文处理、奖励设计等）；(iii)组件化RL训练系统和框架，同时涵盖组件架构、协调以及评估标准

Result: 抽象出了重复模式，揭示了基础设施的瓶颈，并为培养健壮、透明的深度研究系统提供了实用的指导

Conclusion: 强化学习在深度研究系统中具有重要价值，能够有效解决当前方法的限制，本调查为该领域的研究与应用提供了系统化的基础框架和实践指南

Abstract: Deep research systems, agentic AI that solve complex, multi-step tasks by
coordinating reasoning, search across the open web and user files, and tool
use, are moving toward hierarchical deployments with a Planner, Coordinator,
and Executors. In practice, training entire stacks end-to-end remains
impractical, so most work trains a single planner connected to core tools such
as search, browsing, and code. While SFT imparts protocol fidelity, it suffers
from imitation and exposure biases and underuses environment feedback.
Preference alignment methods such as DPO are schema and proxy-dependent,
off-policy, and weak for long-horizon credit assignment and multi-objective
trade-offs. A further limitation of SFT and DPO is their reliance on human
defined decision points and subskills through schema design and labeled
comparisons. Reinforcement learning aligns with closed-loop, tool-interaction
research by optimizing trajectory-level policies, enabling exploration,
recovery behaviors, and principled credit assignment, and it reduces dependence
on such human priors and rater biases.
  This survey is, to our knowledge, the first dedicated to the RL foundations
of deep research systems. It systematizes work after DeepSeek-R1 along three
axes: (i) data synthesis and curation; (ii) RL methods for agentic research
covering stability, sample efficiency, long context handling, reward and credit
design, multi-objective optimization, and multimodal integration; and (iii)
agentic RL training systems and frameworks. We also cover agent architecture
and coordination, as well as evaluation and benchmarks, including recent QA,
VQA, long-form synthesis, and domain-grounded, tool-interaction tasks. We
distill recurring patterns, surface infrastructure bottlenecks, and offer
practical guidance for training robust, transparent deep research agents with
RL.

</details>


### [125] [VehicleWorld: A Highly Integrated Multi-Device Environment for Intelligent Vehicle Interaction](https://arxiv.org/abs/2509.06736)
*Jie Yang,Jiajun Chen,Zhangyue Yin,Shuo Chen,Yuxin Wang,Yiran Guo,Yuan Li,Yining Zheng,Xuanjing Huang,Xipeng Qiu*

Main category: cs.AI

TL;DR: 提出了VehicleWorld车载环境平台和State-based Function Call (SFC)方法，通过显式状态感知和直接状态转换，在车载座舱API代理任务中显著优于传统函数调用方法


<details>
  <summary>Details</summary>
Motivation: 智能车辆座舱API代理面临紧密耦合子系统的协调挑战，传统函数调用方法需要多次探索性调用来构建环境认知，导致效率低下和错误恢复能力有限

Method: 开发了包含30个模块、250个API和680个属性的VehicleWorld车载环境平台，并提出基于状态的函数调用(SFC)方法，通过维护显式系统状态感知和实现直接状态转换

Result: 实验结果表明SFC方法显著优于传统函数调用方法，实现了更高的执行准确性和更低的延迟

Conclusion: 直接状态预测在环境控制方面优于函数调用，SFC方法为车载座舱API代理提供了更高效的解决方案，所有实现代码已在GitHub开源

Abstract: Intelligent vehicle cockpits present unique challenges for API Agents,
requiring coordination across tightly-coupled subsystems that exceed typical
task environments' complexity. Traditional Function Calling (FC) approaches
operate statelessly, requiring multiple exploratory calls to build
environmental awareness before execution, leading to inefficiency and limited
error recovery. We introduce VehicleWorld, the first comprehensive environment
for the automotive domain, featuring 30 modules, 250 APIs, and 680 properties
with fully executable implementations that provide real-time state information
during agent execution. This environment enables precise evaluation of vehicle
agent behaviors across diverse, challenging scenarios. Through systematic
analysis, we discovered that direct state prediction outperforms function
calling for environmental control. Building on this insight, we propose
State-based Function Call (SFC), a novel approach that maintains explicit
system state awareness and implements direct state transitions to achieve
target conditions. Experimental results demonstrate that SFC significantly
outperforms traditional FC approaches, achieving superior execution accuracy
and reduced latency. We have made all implementation code publicly available on
Github https://github.com/OpenMOSS/VehicleWorld.

</details>


### [126] [Another Turn, Better Output? A Turn-Wise Analysis of Iterative LLM Prompting](https://arxiv.org/abs/2509.06770)
*Shashidhar Reddy Javaji,Bhavul Gauri,Zining Zhu*

Main category: cs.AI

TL;DR: 这篇论文提出了一个评估迭代精细化的框架，用于测量大语言模型在多轮工作流中的迭代效果，发现不同领域的迭代改善模式存在显著差异


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型已经被广泛用于多轮工作流，但目前仍缺乏明确的方法来评估迭代在什么情况下有帮助、什么情况下会造成负面影响

Method: 设计了一个评估协议，在每个任务上进行12轮的受控对话，使用从模糊"改善它"反馈到有目标导向的各种提示策略，并记录每轮输出。通过领域适宜的检查来评分结果（代码的单元测试；数学的答案等效性加推理合理性；创意的原创性和可行性）

Result: 在不同模型和任务中，收益具有领域依赖性：在想法和代码中收益来得早，而在数学中后期轮次在详细说明的导向下重要。模糊反馈经常在前几轮后平台化或逆转正确性，而有目标的提示可靠地改变预期质量轴

Conclusion: 该框架和指标使得迭代可以在不同模型之间进行测量和比较，并为何时导航、停止或切换策略提供信号

Abstract: Large language models (LLMs) are now used in multi-turn workflows, but we
still lack a clear way to measure when iteration helps and when it hurts. We
present an evaluation framework for iterative refinement that spans ideation,
code, and math. Our protocol runs controlled 12-turn conversations per task,
utilizing a variety of prompts ranging from vague ``improve it'' feedback to
targeted steering, and logs per-turn outputs. We score outcomes with
domain-appropriate checks (unit tests for code; answer-equivalence plus
reasoning-soundness for math; originality and feasibility for ideation) and
track turn-level behavior with three families of metrics: semantic movement
across turns, turn-to-turn change, and output size growth. Across models and
tasks, gains are domain-dependent: they arrive early in ideas and code, but in
math late turns matter when guided by elaboration. After the first few turns,
vague feedback often plateaus or reverses correctness, while targeted prompts
reliably shift the intended quality axis (novelty vs. feasibility in ideation;
speed vs. readability in code; in math, elaboration outperforms exploration and
drives late-turn gains). We also observe consistent domain patterns: ideation
moves more in meaning across turns, code tends to grow in size with little
semantic change, and math starts fixed but can break that path with late,
elaborative iteration.Together, the framework and metrics make iteration
measurable and comparable across models, and signal when to steer, stop, or
switch strategies.

</details>


### [127] [RAFFLES: Reasoning-based Attribution of Faults for LLM Systems](https://arxiv.org/abs/2509.06822)
*Chenyang Zhu,Spencer Hong,Jingyu Wu,Kushal Chawla,Charlotte Tang,Youbing Yin,Nathan Wolfe,Erin Babinsky,Daben Liu*

Main category: cs.AI

TL;DR: RAFFLES是一个用于评估长时程多组件LLM代理系统的迭代式评估架构，通过推理和迭代精化来识别系统故障点和原因，在Who&When基准测试中显著超越了现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 当前LLM代理系统的评估方法存在局限性，主要关注单一指标、端到端结果和人类偏好，难以识别长时程多组件系统中的具体故障点和原因。

Method: RAFFLES采用迭代式多组件流水线架构，包含一个中央Judge系统性地调查故障，以及一组专门的Evaluators评估系统组件和Judge自身的推理质量，建立假设历史。

Result: 在Who&When数据集上，RAFFLES在算法生成数据集上达到43%的代理-步骤故障对准确率（之前最佳为16.6%），在手工制作数据集上达到20%准确率（之前最佳为8.8%）。

Conclusion: RAFFLES代表了向自动化故障检测迈出的关键一步，能够替代劳动密集型的人工审查，显著提升对自主系统的故障诊断能力。

Abstract: We have reached a critical roadblock in the development and enhancement of
long-horizon, multi-component LLM agentic systems: it is incredibly tricky to
identify where these systems break down and why. Evaluation capabilities that
currently exist today (e.g., single pass LLM-as-a-judge) are limited in that
they often focus on individual metrics or capabilities, end-to-end outcomes,
and are narrowly grounded on the preferences of humans. We argue that to match
the agentic capabilities, evaluation frameworks must also be able to reason,
probe, iterate, and understand the complex logic passing through these systems
over long horizons. In this paper, we present RAFFLES - an evaluation
architecture that incorporates reasoning and iterative refinement.
Specifically, RAFFLES operates as an iterative, multi-component pipeline, using
a central Judge to systematically investigate faults and a set of specialized
Evaluators to assess not only the system's components but also the quality of
the reasoning by the Judge itself, thereby building a history of hypotheses. We
tested RAFFLES against several baselines on the Who&When dataset, a benchmark
designed to diagnose the "who" (agent) and "when" (step) of a system's failure.
RAFFLES outperforms these baselines, achieving an agent-step fault pair
accuracy of over 43% on the Algorithmically-Generated dataset (a substantial
increase from the previously published best of 16.6%) and over 20% on the
Hand-Crafted dataset (surpassing the previously published best of 8.8%). These
results demonstrate a key step towards introducing automated fault detection
for autonomous systems over labor-intensive manual human review.

</details>


### [128] [Test-Time Scaling in Reasoning Models Is Not Effective for Knowledge-Intensive Tasks Yet](https://arxiv.org/abs/2509.06861)
*James Xu Zhao,Bryan Hooi,See-Kiong Ng*

Main category: cs.AI

TL;DR: 谨慎使用测试时扩展，在知识密集任务中可能导致更多幻觉而非提高准确性


<details>
  <summary>Details</summary>
Motivation: 评估测试时扩展技术在知诅密集任务中的效果，因为这类任务需要高事实准确性和低幻觉率

Method: 使用12个推理模型在2个知识密集测试集上进行综合评估，分析扩展推理对幻觉行为的影响

Result: 增加测试时计算并不稳定提高准确性，反而在许多情况下导致更多幻觉，幻觉减少多是由于模型选择放弃答题

Conclusion: 虽然扩展推理存在限制，但与不进行思考相比，启用思考仍然有益

Abstract: Test-time scaling increases inference-time computation by allowing models to
generate long reasoning chains, and has shown strong performance across many
domains. However, in this work, we show that this approach is not yet effective
for knowledge-intensive tasks, where high factual accuracy and low
hallucination rates are essential. We conduct a comprehensive evaluation of
test-time scaling using 12 reasoning models on two knowledge-intensive
benchmarks. Our results reveal that increasing test-time computation does not
consistently improve accuracy and, in many cases, it even leads to more
hallucinations. We then analyze how extended reasoning affects hallucination
behavior. We find that reduced hallucinations often result from the model
choosing to abstain after thinking more, rather than from improved factual
recall. Conversely, for some models, longer reasoning encourages attempts on
previously unanswered questions, many of which result in hallucinations. Case
studies show that extended reasoning can induce confirmation bias, leading to
overconfident hallucinations. Despite these limitations, we observe that
compared to non-thinking, enabling thinking remains beneficial. Code and data
are available at https://github.com/XuZhao0/tts-knowledge

</details>


### [129] [Paper2Agent: Reimagining Research Papers As Interactive and Reliable AI Agents](https://arxiv.org/abs/2509.06917)
*Jiacheng Miao,Joe R. Davis,Jonathan K. Pritchard,James Zou*

Main category: cs.AI

TL;DR: Paper2Agent是一个自动化框架，可将研究论文转换为AI代理，使静态论文变成动态交互式研究助手，通过自然语言处理复杂科学查询。


<details>
  <summary>Details</summary>
Motivation: 传统研究论文需要读者投入大量精力来理解和适应代码、数据和方法，这造成了传播和重用的障碍。Paper2Agent旨在将研究输出从被动产物转变为主动系统。

Method: 使用多代理系统分析论文和相关代码库，构建模型上下文协议(MCP)服务器，通过迭代生成和运行测试来优化MCP。然后将论文MCP与聊天代理连接，通过自然语言执行科学查询。

Result: 成功创建了利用AlphaGenome解释基因组变异的代理，以及基于ScanPy和TISSUE的单细胞和空间转录组学分析代理，能够复现原始论文结果并正确处理新用户查询。

Conclusion: Paper2Agent通过将静态论文转化为动态交互式AI代理，为知识传播引入了新范式，并为AI协作科学家生态系统奠定了基础。

Abstract: We introduce Paper2Agent, an automated framework that converts research
papers into AI agents. Paper2Agent transforms research output from passive
artifacts into active systems that can accelerate downstream use, adoption, and
discovery. Conventional research papers require readers to invest substantial
effort to understand and adapt a paper's code, data, and methods to their own
work, creating barriers to dissemination and reuse. Paper2Agent addresses this
challenge by automatically converting a paper into an AI agent that acts as a
knowledgeable research assistant. It systematically analyzes the paper and the
associated codebase using multiple agents to construct a Model Context Protocol
(MCP) server, then iteratively generates and runs tests to refine and robustify
the resulting MCP. These paper MCPs can then be flexibly connected to a chat
agent (e.g. Claude Code) to carry out complex scientific queries through
natural language while invoking tools and workflows from the original paper. We
demonstrate Paper2Agent's effectiveness in creating reliable and capable paper
agents through in-depth case studies. Paper2Agent created an agent that
leverages AlphaGenome to interpret genomic variants and agents based on ScanPy
and TISSUE to carry out single-cell and spatial transcriptomics analyses. We
validate that these paper agents can reproduce the original paper's results and
can correctly carry out novel user queries. By turning static papers into
dynamic, interactive AI agents, Paper2Agent introduces a new paradigm for
knowledge dissemination and a foundation for the collaborative ecosystem of AI
co-scientists.

</details>


### [130] [Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference](https://arxiv.org/abs/2509.06942)
*Xiangwei Shen,Zhimin Li,Zhantao Yang,Shiyi Zhang,Yingfang Zhang,Donghao Li,Chunyu Wang,Qinglin Lu,Yansong Tang*

Main category: cs.AI

TL;DR: 通过提前定义噪声先验和语义相对偏好优化方法，解决了人类偏好对齐中多步去噪计算成本高和需要离线奖励调整的问题


<details>
  <summary>Details</summary>
Motivation: 现有的对齐方法依赖多步去噪计算奖励，计算成本高，且需要离线连续调整奖励模型来达到朗朗的美学质量

Method: 提出Direct-Align方法通过提前定义噪声先验来恢复原始图像，避免后期时间步的过度优化；提出SRPO方法将奖励形式化为文本条件信号，支持在线奖励调整

Result: 在FLUX.1.dev模型上进行精细调整后，人类评估的实体感和美学质量提升了3倍以上

Conclusion: 该方法有效解决了对齐过程中的计算效率和奖励调整问题，显著提升了模型的输出质量

Abstract: Recent studies have demonstrated the effectiveness of directly aligning
diffusion models with human preferences using differentiable reward. However,
they exhibit two primary challenges: (1) they rely on multistep denoising with
gradient computation for reward scoring, which is computationally expensive,
thus restricting optimization to only a few diffusion steps; (2) they often
need continuous offline adaptation of reward models in order to achieve desired
aesthetic quality, such as photorealism or precise lighting effects. To address
the limitation of multistep denoising, we propose Direct-Align, a method that
predefines a noise prior to effectively recover original images from any time
steps via interpolation, leveraging the equation that diffusion states are
interpolations between noise and target images, which effectively avoids
over-optimization in late timesteps. Furthermore, we introduce Semantic
Relative Preference Optimization (SRPO), in which rewards are formulated as
text-conditioned signals. This approach enables online adjustment of rewards in
response to positive and negative prompt augmentation, thereby reducing the
reliance on offline reward fine-tuning. By fine-tuning the FLUX.1.dev model
with optimized denoising and online reward adjustment, we improve its
human-evaluated realism and aesthetic quality by over 3x.

</details>
