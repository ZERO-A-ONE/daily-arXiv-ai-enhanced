{"id": "2509.14251", "categories": ["cs.AI", "cs.MA", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.14251", "abs": "https://arxiv.org/abs/2509.14251", "authors": ["Qihang Chen"], "title": "Unified Crew Planning and Replanning Optimization in Multi-Line Metro Systems Considering Workforce Heterogeneity", "comment": null, "summary": "Metro crew planning is a key component of smart city development as it\ndirectly impacts the operational efficiency and service reliability of public\ntransportation. With the rapid expansion of metro networks, effective\nmulti-line scheduling and emergency management have become essential for\nlarge-scale seamless operations. However, current research focuses primarily on\nindividual metro lines,with insufficient attention on cross-line coordination\nand rapid replanning during disruptions. Here, a unified optimization framework\nis presented for multi-line metro crew planning and replanning with\nheterogeneous workforce. Specifically, a hierarchical time-space network model\nis proposed to represent the unified crew action space, and computationally\nefficient constraints and formulations are derived for the crew's heterogeneous\nqualifications and preferences. Solution algorithms based on column generation\nand shortest path adjustment are further developed, utilizing the proposed\nnetwork model. Experiments with real data from Shanghai and Beijing Metro\ndemonstrate that the proposed methods outperform benchmark heuristics in both\ncost reduction and task completion,and achieve notable efficiency gains by\nincorporating cross-line operations, particularly for urgent tasks during\ndisruptions. This work highlights the role of global optimization and\ncross-line coordination in multi-line metro system operations, providing\ninsights into the efficient and reliable functioning of public transportation\nin smart cities."}
{"id": "2509.14289", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14289", "abs": "https://arxiv.org/abs/2509.14289", "authors": ["Lanxiao Huang", "Daksh Dave", "Ming Jin", "Tyler Cody", "Peter Beling"], "title": "From Capabilities to Performance: Evaluating Key Functional Properties of LLM Architectures in Penetration Testing", "comment": null, "summary": "Large language models (LLMs) are increasingly used to automate or augment\npenetration testing, but their effectiveness and reliability across attack\nphases remain unclear. We present a comprehensive evaluation of multiple\nLLM-based agents, from single-agent to modular designs, across realistic\npenetration testing scenarios, measuring empirical performance and recurring\nfailure patterns. We also isolate the impact of five core functional\ncapabilities via targeted augmentations: Global Context Memory (GCM),\nInter-Agent Messaging (IAM), Context-Conditioned Invocation (CCI), Adaptive\nPlanning (AP), and Real-Time Monitoring (RTM). These interventions support,\nrespectively: (i) context coherence and retention, (ii) inter-component\ncoordination and state management, (iii) tool use accuracy and selective\nexecution, (iv) multi-step strategic planning, error detection, and recovery,\nand (v) real-time dynamic responsiveness. Our results show that while some\narchitectures natively exhibit subsets of these properties, targeted\naugmentations substantially improve modular agent performance, especially in\ncomplex, multi-step, and real-time penetration testing tasks."}
{"id": "2509.14382", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14382", "abs": "https://arxiv.org/abs/2509.14382", "authors": ["Daniel RÃ¶der", "Akhil Juneja", "Roland Roller", "Sven Schmeier"], "title": "Detecting Pipeline Failures through Fine-Grained Analysis of Web Agents", "comment": null, "summary": "Web agents powered by large language models (LLMs) can autonomously perform\ncomplex, multistep tasks in dynamic web environments. However, current\nevaluations mostly focus on the overall success while overlooking intermediate\nerrors. This limits insight into failure modes and hinders systematic\nimprovement. This work analyzes existing benchmarks and highlights the lack of\nfine-grained diagnostic tools. To address this gap, we propose a modular\nevaluation framework that decomposes agent pipelines into interpretable stages\nfor detailed error analysis. Using the SeeAct framework and the Mind2Web\ndataset as a case study, we show how this approach reveals actionable\nweaknesses missed by standard metrics - paving the way for more robust and\ngeneralizable web agents."}
{"id": "2509.14448", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14448", "abs": "https://arxiv.org/abs/2509.14448", "authors": ["Rick Chen", "Joseph Ternasky", "Afriyie Samuel Kwesi", "Ben Griffin", "Aaron Ontoyin Yin", "Zakari Salifu", "Kelvin Amoaba", "Xianling Mu", "Fuat Alican", "Yigit Ihlamur"], "title": "VCBench: Benchmarking LLMs in Venture Capital", "comment": null, "summary": "Benchmarks such as SWE-bench and ARC-AGI demonstrate how shared datasets\naccelerate progress toward artificial general intelligence (AGI). We introduce\nVCBench, the first benchmark for predicting founder success in venture capital\n(VC), a domain where signals are sparse, outcomes are uncertain, and even top\ninvestors perform modestly. At inception, the market index achieves a precision\nof 1.9%. Y Combinator outperforms the index by a factor of 1.7x, while tier-1\nfirms are 2.9x better. VCBench provides 9,000 anonymized founder profiles,\nstandardized to preserve predictive features while resisting identity leakage,\nwith adversarial tests showing more than 90% reduction in re-identification\nrisk. We evaluate nine state-of-the-art large language models (LLMs).\nDeepSeek-V3 delivers over six times the baseline precision, GPT-4o achieves the\nhighest F0.5, and most models surpass human benchmarks. Designed as a public\nand evolving resource available at vcbench.com, VCBench establishes a\ncommunity-driven standard for reproducible and privacy-preserving evaluation of\nAGI in early-stage venture forecasting."}
{"id": "2509.14271", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14271", "abs": "https://arxiv.org/abs/2509.14271", "authors": ["Gustavo Sandoval", "Denys Fenchenko", "Junyao Chen"], "title": "Early Approaches to Adversarial Fine-Tuning for Prompt Injection Defense: A 2022 Study of GPT-3 and Contemporary Models", "comment": null, "summary": "This paper documents early research conducted in 2022 on defending against\nprompt injection attacks in large language models, providing historical context\nfor the evolution of this critical security domain. This research focuses on\ntwo adversarial attacks against Large Language Models (LLMs): prompt injection\nand goal hijacking. We examine how to construct these attacks, test them on\nvarious LLMs, and compare their effectiveness. We propose and evaluate a novel\ndefense technique called Adversarial Fine-Tuning. Our results show that,\nwithout this defense, the attacks succeeded 31\\% of the time on GPT-3 series\nmodels. When using our Adversarial Fine-Tuning approach, attack success rates\nwere reduced to near zero for smaller GPT-3 variants (Ada, Babbage, Curie),\nthough we note that subsequent research has revealed limitations of\nfine-tuning-based defenses. We also find that more flexible models exhibit\ngreater vulnerability to these attacks. Consequently, large models such as\nGPT-3 Davinci are more vulnerable than smaller models like GPT-2. While the\nspecific models tested are now superseded, the core methodology and empirical\nfindings contributed to the foundation of modern prompt injection defense\nresearch, including instruction hierarchy systems and constitutional AI\napproaches."}
{"id": "2509.14265", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14265", "abs": "https://arxiv.org/abs/2509.14265", "authors": ["Siyuan Chen", "Zhichao Lu", "Qingfu Zhang"], "title": "Evolution of Kernels: Automated RISC-V Kernel Optimization with Large Language Models", "comment": "Technical report", "summary": "Automated kernel design is critical for overcoming software ecosystem\nbarriers in emerging hardware platforms like RISC-V. While large language\nmodels (LLMs) have shown promise for automated kernel optimization,\ndemonstrating success in CUDA domains with comprehensive technical documents\nand mature codebases, their effectiveness remains unproven for reference-scarce\ndomains like RISC-V. We present Evolution of Kernels (EoK), a novel LLM-based\nevolutionary program search framework that automates kernel design for domains\nwith limited reference material. EoK mitigates reference scarcity by mining and\nformalizing reusable optimization ideas (general design principles + actionable\nthoughts) from established kernel libraries' development histories; it then\nguides parallel LLM explorations using these ideas, enriched via\nRetrieval-Augmented Generation (RAG) with RISC-V-specific context, prioritizing\nhistorically effective techniques. Empirically, EoK achieves a median 1.27x\nspeedup, surpassing human experts on all 80 evaluated kernel design tasks and\nimproving upon prior LLM-based automated kernel design methods by 20%. These\nresults underscore the viability of incorporating human experience into\nemerging domains and highlight the immense potential of LLM-based automated\nkernel optimization."}
{"id": "2509.14474", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.14474", "abs": "https://arxiv.org/abs/2509.14474", "authors": ["Meltem Subasioglu", "Nevzat Subasioglu"], "title": "From Mimicry to True Intelligence (TI) -- A New Paradigm for Artificial General Intelligence", "comment": "27 pages, 1 figure", "summary": "The debate around Artificial General Intelligence (AGI) remains open due to\ntwo fundamentally different goals: replicating human-like performance versus\nreplicating human-like cognitive processes. We argue that current\nperformance-based definitions are inadequate because they provide no clear,\nmechanism-focused roadmap for research, and they fail to properly define the\nqualitative nature of genuine intelligence. Drawing inspiration from the human\nbrain, we propose a new paradigm that shifts the focus from external mimicry to\nthe development of foundational cognitive architectures. We define True\nIntelligence (TI) as a system characterized by six core components: embodied\nsensory fusion, core directives, dynamic schemata creation, a\nhighly-interconnected multi-expert architecture, an orchestration layer, and\nlastly, the unmeasurable quality of Interconnectedness, which we hypothesize\nresults in consciousness and a subjective experience. We propose a practical,\nfive-level taxonomy of AGI based on the number of the first five measurable\ncomponents a system exhibits. This framework provides a clear path forward with\ndevelopmental milestones that directly address the challenge of building\ngenuinely intelligent systems. We contend that once a system achieves Level-5\nAGI by implementing all five measurable components, the difference between it\nand TI remains as a purely philosophical debate. For practical purposes - and\ngiven theories indicate consciousness is an emergent byproduct of integrated,\nhigher-order cognition - we conclude that a fifth-level AGI is functionally and\npractically equivalent to TI. This work synthesizes diverse insights from\nanalytical psychology, schema theory, metacognition, modern brain architectures\nand latest works in AI to provide the first holistic, mechanism-based\ndefinition of AGI that offers a clear and actionable path for the research\ncommunity."}
{"id": "2509.14275", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14275", "abs": "https://arxiv.org/abs/2509.14275", "authors": ["Nobin Sarwar", "Shubhashis Roy Dipta"], "title": "FedMentor: Domain-Aware Differential Privacy for Heterogeneous Federated LLMs in Mental Health", "comment": "(e.g.: 18 pages, 6 figures, 6 tables)", "summary": "Privacy-preserving adaptation of Large Language Models (LLMs) in sensitive\ndomains (e.g., mental health) requires balancing strict confidentiality with\nmodel utility and safety. We propose FedMentor, a federated fine-tuning\nframework that integrates Low-Rank Adaptation (LoRA) and domain-aware\nDifferential Privacy (DP) to meet per-domain privacy budgets while maintaining\nperformance. Each client (domain) applies a custom DP noise scale proportional\nto its data sensitivity, and the server adaptively reduces noise when utility\nfalls below a threshold. In experiments on three mental health datasets, we\nshow that FedMentor improves safety over standard Federated Learning without\nprivacy, raising safe output rates by up to three points and lowering toxicity,\nwhile maintaining utility (BERTScore F1 and ROUGE-L) within 0.5% of the\nnon-private baseline and close to the centralized upper bound. The framework\nscales to backbones with up to 1.7B parameters on single-GPU clients, requiring\n< 173 MB of communication per round. FedMentor demonstrates a practical\napproach to privately fine-tune LLMs for safer deployments in healthcare and\nother sensitive fields."}
{"id": "2509.14273", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.14273", "abs": "https://arxiv.org/abs/2509.14273", "authors": ["Swapnil Sharma Sarker", "Tanzina Taher Ifty"], "title": "Automated and Context-Aware Code Documentation Leveraging Advanced LLMs", "comment": null, "summary": "Code documentation is essential to improve software maintainability and\ncomprehension. The tedious nature of manual code documentation has led to much\nresearch on automated documentation generation. Existing automated approaches\nprimarily focused on code summarization, leaving a gap in template-based\ndocumentation generation (e.g., Javadoc), particularly with publicly available\nLarge Language Models (LLMs). Furthermore, progress in this area has been\nhindered by the lack of a Javadoc-specific dataset that incorporates modern\nlanguage features, provides broad framework/library coverage, and includes\nnecessary contextual information. This study aims to address these gaps by\ndeveloping a tailored dataset and assessing the capabilities of publicly\navailable LLMs for context-aware, template-based Javadoc generation. In this\nwork, we present a novel, context-aware dataset for Javadoc generation that\nincludes critical structural and semantic information from modern Java\ncodebases. We evaluate five open-source LLMs (including LLaMA-3.1, Gemma-2,\nPhi-3, Mistral, Qwen-2.5) using zero-shot, few-shot, and fine-tuned setups and\nprovide a comparative analysis of their performance. Our results demonstrate\nthat LLaMA 3.1 performs consistently well and is a reliable candidate for\npractical, automated Javadoc generation, offering a viable alternative to\nproprietary systems."}
{"id": "2509.14485", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14485", "abs": "https://arxiv.org/abs/2509.14485", "authors": ["Marko Tesic", "Yue Zhao", "Joel Z. Leibo", "Rakshit S. Trivedi", "Jose Hernandez-Orallo"], "title": "Beyond the high score: Prosocial ability profiles of multi-agent populations", "comment": null, "summary": "The development and evaluation of social capabilities in AI agents require\ncomplex environments where competitive and cooperative behaviours naturally\nemerge. While game-theoretic properties can explain why certain teams or agent\npopulations outperform others, more abstract behaviours, such as convention\nfollowing, are harder to control in training and evaluation settings. The\nMelting Pot contest is a social AI evaluation suite designed to assess the\ncooperation capabilities of AI systems. In this paper, we apply a Bayesian\napproach known as Measurement Layouts to infer the capability profiles of\nmulti-agent systems in the Melting Pot contest. We show that these capability\nprofiles not only predict future performance within the Melting Pot suite but\nalso reveal the underlying prosocial abilities of agents. Our analysis\nindicates that while higher prosocial capabilities sometimes correlate with\nbetter performance, this is not a universal trend-some lower-scoring agents\nexhibit stronger cooperation abilities. Furthermore, we find that\ntop-performing contest submissions are more likely to achieve high scores in\nscenarios where prosocial capabilities are not required. These findings,\ntogether with reports that the contest winner used a hard-coded solution\ntailored to specific environments, suggest that at least one top-performing\nteam may have optimised for conditions where cooperation was not necessary,\npotentially exploiting limitations in the evaluation framework. We provide\nrecommendations for improving the annotation of cooperation demands and propose\nfuture research directions to account for biases introduced by different\ntesting environments. Our results demonstrate that Measurement Layouts offer\nboth strong predictive accuracy and actionable insights, contributing to a more\ntransparent and generalisable approach to evaluating AI systems in complex\nsocial settings."}
{"id": "2509.14278", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14278", "abs": "https://arxiv.org/abs/2509.14278", "authors": ["Yuntao Du", "Zitao Li", "Ninghui Li", "Bolin Ding"], "title": "Beyond Data Privacy: New Privacy Risks for Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) have achieved remarkable progress in natural\nlanguage understanding, reasoning, and autonomous decision-making. However,\nthese advancements have also come with significant privacy concerns. While\nsignificant research has focused on mitigating the data privacy risks of LLMs\nduring various stages of model training, less attention has been paid to new\nthreats emerging from their deployment. The integration of LLMs into widely\nused applications and the weaponization of their autonomous abilities have\ncreated new privacy vulnerabilities. These vulnerabilities provide\nopportunities for both inadvertent data leakage and malicious exfiltration from\nLLM-powered systems. Additionally, adversaries can exploit these systems to\nlaunch sophisticated, large-scale privacy attacks, threatening not only\nindividual privacy but also financial security and societal trust. In this\npaper, we systematically examine these emerging privacy risks of LLMs. We also\ndiscuss potential mitigation strategies and call for the research community to\nbroaden its focus beyond data privacy risks, developing new defenses to address\nthe evolving threats posed by increasingly powerful LLMs and LLM-powered\nsystems."}
{"id": "2509.14279", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14279", "abs": "https://arxiv.org/abs/2509.14279", "authors": ["Robert Tjarko Lange", "Qi Sun", "Aaditya Prasad", "Maxence Faldor", "Yujin Tang", "David Ha"], "title": "Towards Robust Agentic CUDA Kernel Benchmarking, Verification, and Optimization", "comment": "62 pages, 10 figures", "summary": "Recent advances in large language models (LLMs) demonstrate their\neffectiveness in scaling test-time compute for software engineering tasks.\nHowever, these approaches often focus on high-level solutions, with limited\nattention to optimizing low-level CUDA kernel implementations. Additionally,\nexisting kernel generation benchmarks suffer from exploitable loopholes and\ninsufficient diversity in testing conditions, hindering true generalization\nassessment. To address these limitations, we introduce robust-kbench, a new\nbenchmark for rigorous evaluation of kernel performance and correctness across\nvaried scenarios. Furthermore, we present a comprehensive agentic framework\nthat automates CUDA kernel discovery, verification, and optimization. This\npipeline enables frontier LLMs to translate torch code to CUDA kernels and\niteratively improve their runtime within our robust evaluation setting. Our\nsequential workflow first translates PyTorch code into equivalent CUDA kernels.\nIt then optimizes their runtime using a novel evolutionary meta-generation\nprocedure tailored to the CUDA ecosystem, guided by LLM-based verifiers for\ncorrectness and efficient filtering. Evaluated on robust-kbench, our approach\nproduces CUDA kernels outperforming torch implementations for practical\napplications, including forward and backward passes. It can fuse operations and\ndeploy various runtime optimization strategies. The verifier workflow\naccurately classifies incorrect kernels, enhancing hardware verification\nefficiency."}
{"id": "2509.14507", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.14507", "abs": "https://arxiv.org/abs/2509.14507", "authors": ["Jian Chen", "Zhenyan Chen", "Xuming Hu", "Peilin Zhou", "Yining Hua", "Han Fang", "Cissy Hing Yee Choy", "Xinmei Ke", "Jingfeng Luo", "Zixuan Yuan"], "title": "DeKeyNLU: Enhancing Natural Language to SQL Generation through Task Decomposition and Keyword Extraction", "comment": null, "summary": "Natural Language to SQL (NL2SQL) provides a new model-centric paradigm that\nsimplifies database access for non-technical users by converting natural\nlanguage queries into SQL commands. Recent advancements, particularly those\nintegrating Retrieval-Augmented Generation (RAG) and Chain-of-Thought (CoT)\nreasoning, have made significant strides in enhancing NL2SQL performance.\nHowever, challenges such as inaccurate task decomposition and keyword\nextraction by LLMs remain major bottlenecks, often leading to errors in SQL\ngeneration. While existing datasets aim to mitigate these issues by fine-tuning\nmodels, they struggle with over-fragmentation of tasks and lack of\ndomain-specific keyword annotations, limiting their effectiveness. To address\nthese limitations, we present DeKeyNLU, a novel dataset which contains 1,500\nmeticulously annotated QA pairs aimed at refining task decomposition and\nenhancing keyword extraction precision for the RAG pipeline. Fine-tuned with\nDeKeyNLU, we propose DeKeySQL, a RAG-based NL2SQL pipeline that employs three\ndistinct modules for user question understanding, entity retrieval, and\ngeneration to improve SQL generation accuracy. We benchmarked multiple model\nconfigurations within DeKeySQL RAG pipeline. Experimental results demonstrate\nthat fine-tuning with DeKeyNLU significantly improves SQL generation accuracy\non both BIRD (62.31% to 69.10%) and Spider (84.2% to 88.7%) dev datasets."}
{"id": "2509.14282", "categories": ["cs.CR", "quant-ph"], "pdf": "https://arxiv.org/pdf/2509.14282", "abs": "https://arxiv.org/abs/2509.14282", "authors": ["Ali Al-kuwari", "Noureldin Mohamed", "Saif Al-kuwari", "Ahmed Farouk", "Bikash K. Behera"], "title": "Resisting Quantum Key Distribution Attacks Using Quantum Machine Learning", "comment": null, "summary": "The emergence of quantum computing poses significant risks to the security of\nmodern communication networks as it breaks today's public-key cryptographic\nalgorithms. Quantum Key Distribution (QKD) offers a promising solution by\nharnessing the principles of quantum mechanics to establish secure keys.\nHowever, practical QKD implementations remain vulnerable to hardware\nimperfections and advanced attacks such as Photon Number Splitting and\nTrojan-Horse attacks. In this work, we investigate the potential of using\nquantum machine learning (QML) to detect popular QKD attacks. In particular, we\npropose a Hybrid Quantum Long Short-Term Memory (QLSTM) model to improve the\ndetection of common QKD attacks. By combining quantum-enhanced learning with\nclassical deep learning, the model captures complex temporal patterns in QKD\ndata, improving detection accuracy. To evaluate the proposed model, we\nintroduce a realistic QKD dataset simulating normal QKD operations along with\nseven attack scenarios, Intercept-and-Resend, Photon-Number Splitting (PNS),\nTrojan-Horse attacks Random Number Generator (RNG), Detector Blinding,\nWavelength-dependent Trojan Horse, and Combined attacks. The dataset includes\nquantum security metrics such as Quantum Bit Error Rate (QBER), measurement\nentropy, signal and decoy loss rates, and time-based metrics, ensuring an\naccurate representation of real-world conditions. Our results demonstrate\npromising performance of the quantum machine learning approach compared to\ntraditional classical machine learning models, highlighting the potential of\nhybrid techniques to enhance the security of future quantum communication\nnetworks. The proposed Hybrid QLSTM model achieved an accuracy of 93.7.0\\%\nafter 50 training epochs, outperforming classical deep learning models such as\nLSTM, and CNN."}
{"id": "2509.14281", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14281", "abs": "https://arxiv.org/abs/2509.14281", "authors": ["Xifeng Yao", "Dongyu Lang", "Wu Zhang", "Xintong Guo", "Huarui Xie", "Yinhao Ni", "Ping Liu", "Guang Shen", "Yi Bai", "Dandan Tu", "Changzheng Zhang"], "title": "SCoGen: Scenario-Centric Graph-Based Synthesis of Real-World Code Problems", "comment": null, "summary": "Significant advancements have been made in the capabilities of code large\nlanguage models, leading to their rapid adoption and application across a wide\nrange of domains. However, their further advancements are often constrained by\nthe scarcity of real-world coding problems. To bridge this gap, we propose a\nnovel framework for synthesizing code problems that emulate authentic\nreal-world scenarios. This framework systematically integrates domain\nknowledge, domain skills, and coding skills, all of which are meticulously\nextracted from real-world programming-related datasets, including Stack\nOverflow and Kaggle. The extracted elements serve as the foundational building\nblocks for constructing code problems. To align the generated problems with\npractical applications, application scenarios are also mined from the\naforementioned datasets. These scenarios are then utilized to construct a\nscenario-centric graph that interconnects domain knowledge, domain skills, and\ncoding skills. Based on this structured representation, a sampling strategy on\nthe graph is designed, which effectively controls the generation of a code\nproblem with complexity and diversity, reflects real-world challenges.\nExperimental results demonstrate that the proposed method consistently achieves\nsuperior performance over state-of-the-art open-source large language models of\nvarying sizes and functionalities, including both coders and general-purpose\nmodels, across a diverse set of real-world benchmarks."}
{"id": "2509.14546", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14546", "abs": "https://arxiv.org/abs/2509.14546", "authors": ["Zhilun Zhou", "Jing Yi Wang", "Nicholas Sukiennik", "Chen Gao", "Fengli Xu", "Yong Li", "James Evans"], "title": "Rationality Check! Benchmarking the Rationality of Large Language Models", "comment": null, "summary": "Large language models (LLMs), a recent advance in deep learning and machine\nintelligence, have manifested astonishing capacities, now considered among the\nmost promising for artificial general intelligence. With human-like\ncapabilities, LLMs have been used to simulate humans and serve as AI assistants\nacross many applications. As a result, great concern has arisen about whether\nand under what circumstances LLMs think and behave like real human agents.\nRationality is among the most important concepts in assessing human behavior,\nboth in thinking (i.e., theoretical rationality) and in taking action (i.e.,\npractical rationality). In this work, we propose the first benchmark for\nevaluating the omnibus rationality of LLMs, covering a wide range of domains\nand LLMs. The benchmark includes an easy-to-use toolkit, extensive experimental\nresults, and analysis that illuminates where LLMs converge and diverge from\nidealized human rationality. We believe the benchmark can serve as a\nfoundational tool for both developers and users of LLMs."}
{"id": "2509.14284", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.14284", "abs": "https://arxiv.org/abs/2509.14284", "authors": ["Vaidehi Patil", "Elias Stengel-Eskin", "Mohit Bansal"], "title": "The Sum Leaks More Than Its Parts: Compositional Privacy Risks and Mitigations in Multi-Agent Collaboration", "comment": "Code: https://github.com/Vaidehi99/MultiAgentPrivacy", "summary": "As large language models (LLMs) become integral to multi-agent systems, new\nprivacy risks emerge that extend beyond memorization, direct inference, or\nsingle-turn evaluations. In particular, seemingly innocuous responses, when\ncomposed across interactions, can cumulatively enable adversaries to recover\nsensitive information, a phenomenon we term compositional privacy leakage. We\npresent the first systematic study of such compositional privacy leaks and\npossible mitigation methods in multi-agent LLM systems. First, we develop a\nframework that models how auxiliary knowledge and agent interactions jointly\namplify privacy risks, even when each response is benign in isolation. Next, to\nmitigate this, we propose and evaluate two defense strategies: (1)\nTheory-of-Mind defense (ToM), where defender agents infer a questioner's intent\nby anticipating how their outputs may be exploited by adversaries, and (2)\nCollaborative Consensus Defense (CoDef), where responder agents collaborate\nwith peers who vote based on a shared aggregated state to restrict sensitive\ninformation spread. Crucially, we balance our evaluation across compositions\nthat expose sensitive information and compositions that yield benign\ninferences. Our experiments quantify how these defense strategies differ in\nbalancing the privacy-utility trade-off. We find that while chain-of-thought\nalone offers limited protection to leakage (~39% sensitive blocking rate), our\nToM defense substantially improves sensitive query blocking (up to 97%) but can\nreduce benign task success. CoDef achieves the best balance, yielding the\nhighest Balanced Outcome (79.8%), highlighting the benefit of combining\nexplicit reasoning with defender collaboration. Together, our results expose a\nnew class of risks in collaborative LLM deployments and provide actionable\ninsights for designing safeguards against compositional, context-driven privacy\nleakage."}
{"id": "2509.14294", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14294", "abs": "https://arxiv.org/abs/2509.14294", "authors": ["Hira Naveed", "Scott Barnett", "Chetan Arora", "John Grundy", "Hourieh Khalajzadeh", "Omar Haggag"], "title": "Monitoring Machine Learning Systems: A Multivocal Literature Review", "comment": null, "summary": "Context: Dynamic production environments make it challenging to maintain\nreliable machine learning (ML) systems. Runtime issues, such as changes in data\npatterns or operating contexts, that degrade model performance are a common\noccurrence in production settings. Monitoring enables early detection and\nmitigation of these runtime issues, helping maintain users' trust and prevent\nunwanted consequences for organizations. Aim: This study aims to provide a\ncomprehensive overview of the ML monitoring literature. Method: We conducted a\nmultivocal literature review (MLR) following the well established guidelines by\nGarousi to investigate various aspects of ML monitoring approaches in 136\npapers. Results: We analyzed selected studies based on four key areas: (1) the\nmotivations, goals, and context; (2) the monitored aspects, specific\ntechniques, metrics, and tools; (3) the contributions and benefits; and (4) the\ncurrent limitations. We also discuss several insights found in the studies,\ntheir implications, and recommendations for future research and practice.\nConclusion: Our MLR identifies and summarizes ML monitoring practices and gaps,\nemphasizing similarities and disconnects between formal and gray literature.\nOur study is valuable for both academics and practitioners, as it helps select\nappropriate solutions, highlights limitations in current approaches, and\nprovides future directions for research and tool development."}
{"id": "2509.14547", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14547", "abs": "https://arxiv.org/abs/2509.14547", "authors": ["Yi Lin", "Lujin Zhao", "Yijie Shi"], "title": "(P)rior(D)yna(F)low: A Priori Dynamic Workflow Construction via Multi-Agent Collaboration", "comment": null, "summary": "Recent studies have shown that carefully designed workflows coordinating\nlarge language models(LLMs) significantly enhance task-solving capabilities\ncompared to using a single model. While an increasing number of works focus on\nautonomous workflow construction, most existing approaches rely solely on\nhistorical experience, leading to limitations in efficiency and adaptability.\nWe argue that while historical experience is valuable, workflow construction\nshould also flexibly respond to the unique characteristics of each task. To\nthis end, we propose an a priori dynamic framework for automated workflow\nconstruction. Our framework first leverages Q-table learning to optimize the\ndecision space, guiding agent decisions and enabling effective use of\nhistorical experience. At the same time, agents evaluate the current task\nprogress and make a priori decisions regarding the next executing agent,\nallowing the system to proactively select the more suitable workflow structure\nfor each given task. Additionally, we incorporate mechanisms such as cold-start\ninitialization, early stopping, and pruning to further improve system\nefficiency. Experimental evaluations on four benchmark datasets demonstrate the\nfeasibility and effectiveness of our approach. Compared to state-of-the-art\nbaselines, our method achieves an average improvement of 4.05%, while reducing\nworkflow construction and inference costs to only 30.68%-48.31% of those\nrequired by existing methods."}
{"id": "2509.14285", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14285", "abs": "https://arxiv.org/abs/2509.14285", "authors": ["S M Asif Hossain", "Ruksat Khan Shayoni", "Mohd Ruhul Ameen", "Akif Islam", "M. F. Mridha", "Jungpil Shin"], "title": "A Multi-Agent LLM Defense Pipeline Against Prompt Injection Attacks", "comment": null, "summary": "Prompt injection attacks represent a major vulnerability in Large Language\nModel (LLM) deployments, where malicious instructions embedded in user inputs\ncan override system prompts and induce unintended behaviors. This paper\npresents a novel multi-agent defense framework that employs specialized LLM\nagents in coordinated pipelines to detect and neutralize prompt injection\nattacks in real-time. We evaluate our approach using two distinct\narchitectures: a sequential chain-of-agents pipeline and a hierarchical\ncoordinator-based system. Our comprehensive evaluation on 55 unique prompt\ninjection attacks, grouped into 8 categories and totaling 400 attack instances\nacross two LLM platforms (ChatGLM and Llama2), demonstrates significant\nsecurity improvements. Without defense mechanisms, baseline Attack Success\nRates (ASR) reached 30% for ChatGLM and 20% for Llama2. Our multi-agent\npipeline achieved 100% mitigation, reducing ASR to 0% across all tested\nscenarios. The framework demonstrates robustness across multiple attack\ncategories including direct overrides, code execution attempts, data\nexfiltration, and obfuscation techniques, while maintaining system\nfunctionality for legitimate queries."}
{"id": "2509.14347", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.14347", "abs": "https://arxiv.org/abs/2509.14347", "authors": ["Henri AÃ¯dasso", "Francis Bordeleau", "Ali Tizghadam"], "title": "On the Illusion of Success: An Empirical Study of Build Reruns and Silent Failures in Industrial CI", "comment": "17 pages, 7 figures", "summary": "Reliability of build outcomes is a cornerstone of effective Continuous\nIntegration (CI). Yet in practice, developers often struggle with\nnon-deterministic issues in the code or CI infrastructure, which undermine\ntrust in build results. When faced with such unexpected outcomes, developers\noften repeatedly rerun jobs hoping for true success, but this practice is known\nto increase CI costs and reduce productivity. While recent studies have focused\non intermittent job failures, no prior work has investigated silent failures,\nwhere build jobs are marked as successful but fail to complete all or part of\ntheir tasks. Such silent failures often go unnoticed, creating an illusion of\nsuccess with detrimental consequences such as bugs escaping into production.\nThis paper presents the first empirical study of silent failures through the\npractice of rerunning successful jobs. An analysis of 142,387 jobs across 81\nindustrial projects shows that 11% of successful jobs are rerun, with 35% of\nthese reruns occurring after more than 24 hours. Using mixed-effects models on\n32 independent variables (AUC of 85%), we identified key factors associated\nwith reruns of successful jobs, notably testing and static analysis tasks,\nscripting languages like Shell, and developers prior rerun tendencies. A\nfurther analysis of 92 public issues revealed 11 categories of silent failures\naligning with these factors, the most frequent being artifact operation errors,\ncaching errors, and ignored exit codes. Overall, our findings provide valuable\ninsights into the circumstances and causes of silent failures to raise\nawareness among teams, and present solutions to improve CI reliability."}
{"id": "2509.14594", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14594", "abs": "https://arxiv.org/abs/2509.14594", "authors": ["Yidan Sun", "Viktor Schlegel", "Srinivasan Nandakumar", "Iqra Zahid", "Yuping Wu", "Yulong Wu", "Hao Li", "Jie Zhang", "Warren Del-Pinto", "Goran Nenadic", "Siew Kei Lam", "Anil Anthony Bharath"], "title": "SynBench: A Benchmark for Differentially Private Text Generation", "comment": "15 pages", "summary": "Data-driven decision support in high-stakes domains like healthcare and\nfinance faces significant barriers to data sharing due to regulatory,\ninstitutional, and privacy concerns. While recent generative AI models, such as\nlarge language models, have shown impressive performance in open-domain tasks,\ntheir adoption in sensitive environments remains limited by unpredictable\nbehaviors and insufficient privacy-preserving datasets for benchmarking.\nExisting anonymization methods are often inadequate, especially for\nunstructured text, as redaction and masking can still allow re-identification.\nDifferential Privacy (DP) offers a principled alternative, enabling the\ngeneration of synthetic data with formal privacy assurances. In this work, we\naddress these challenges through three key contributions. First, we introduce a\ncomprehensive evaluation framework with standardized utility and fidelity\nmetrics, encompassing nine curated datasets that capture domain-specific\ncomplexities such as technical jargon, long-context dependencies, and\nspecialized document structures. Second, we conduct a large-scale empirical\nstudy benchmarking state-of-the-art DP text generation methods and LLMs of\nvarying sizes and different fine-tuning strategies, revealing that high-quality\ndomain-specific synthetic data generation under DP constraints remains an\nunsolved challenge, with performance degrading as domain complexity increases.\nThird, we develop a membership inference attack (MIA) methodology tailored for\nsynthetic text, providing first empirical evidence that the use of public\ndatasets - potentially present in pre-training corpora - can invalidate claimed\nprivacy guarantees. Our findings underscore the urgent need for rigorous\nprivacy auditing and highlight persistent gaps between open-domain and\nspecialist evaluations, informing responsible deployment of generative AI in\nprivacy-sensitive, high-stakes settings."}
{"id": "2509.14297", "categories": ["cs.CR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.14297", "abs": "https://arxiv.org/abs/2509.14297", "authors": ["Xuan Luo", "Yue Wang", "Zefeng He", "Geng Tu", "Jing Li", "Ruifeng Xu"], "title": "A Simple and Efficient Jailbreak Method Exploiting LLMs' Helpfulness", "comment": null, "summary": "Safety alignment aims to prevent Large Language Models (LLMs) from responding\nto harmful queries. To strengthen safety protections, jailbreak methods are\ndeveloped to simulate malicious attacks and uncover vulnerabilities. In this\npaper, we introduce HILL (Hiding Intention by Learning from LLMs), a novel\njailbreak approach that systematically transforms imperative harmful requests\ninto learning-style questions with only straightforward hypotheticality\nindicators. Further, we introduce two new metrics to thoroughly evaluate the\nutility of jailbreak methods. Experiments on the AdvBench dataset across a wide\nrange of models demonstrate HILL's strong effectiveness, generalizability, and\nharmfulness. It achieves top attack success rates on the majority of models and\nacross malicious categories while maintaining high efficiency with concise\nprompts. Results of various defense methods show the robustness of HILL, with\nmost defenses having mediocre effects or even increasing the attack success\nrates. Moreover, the assessment on our constructed safe prompts reveals\ninherent limitations of LLMs' safety mechanisms and flaws in defense methods.\nThis work exposes significant vulnerabilities of safety measures against\nlearning-style elicitation, highlighting a critical challenge of balancing\nhelpfulness and safety alignments."}
{"id": "2509.14373", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.14373", "abs": "https://arxiv.org/abs/2509.14373", "authors": ["Huy Le", "Phong Nguyen", "Hao Do", "Tuan Nguyen", "Thien Pham", "Anh Nguyen-Duc", "Tho Quan"], "title": "CodeLSI: Leveraging Foundation Models for Automated Code Generation with Low-Rank Optimization and Domain-Specific Instruction Tuning", "comment": null, "summary": "Context: Automated code generation using Foundation Models (FMs) offers\npromising solutions for enhancing software development efficiency. However,\nchallenges remain in ensuring domain specificity, cost-effectiveness, and\nsecurity - especially when relying on third-party APIs. This paper introduces\nCodeLSI, a framework that combines low-rank optimization and domain-specific\ninstruction tuning to address these challenges.\n  Objectives: The aim of this study is to develop and evaluate CodeLSI, a novel\napproach for generating high-quality code tailored to specific domains, using\nFMs fine-tuned on company infrastructure without dependence on external APIs.\n  Methods: CodeLSI applies low-rank adaptation techniques to reduce the\ncomputational cost of model pre-training and fine-tuning. Domain-specific\ninstruction tuning is employed to align code generation with organizational\nneeds. We implemented and tested the framework on real-world JavaScript coding\ntasks using datasets drawn from internal software projects.\n  Results: Experimental evaluations show that CodeLSI produces high-quality,\ncontext aware code. It outperforms baseline models in terms of relevance,\naccuracy, and domain fit. The use of low-rank optimization significantly\nreduced resource requirements, enabling scalable training on company-owned\ninfrastructure.\n  Conclusion: CodeLSI demonstrates that combining low-rank optimization with\ndomain specific tuning can enhance the practicality and performance of FMs for\nautomated code generation. This approach provides a secure, cost-efficient\nalternative to commercial API based solutions and supports faster, more\ntargeted innovation in software development."}
{"id": "2509.14647", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.14647", "abs": "https://arxiv.org/abs/2509.14647", "authors": ["NVJK Kartik", "Garvit Sapra", "Rishav Hada", "Nikhil Pareek"], "title": "AgentCompass: Towards Reliable Evaluation of Agentic Workflows in Production", "comment": null, "summary": "With the growing adoption of Large Language Models (LLMs) in automating\ncomplex, multi-agent workflows, organizations face mounting risks from errors,\nemergent behaviors, and systemic failures that current evaluation methods fail\nto capture. We present AgentCompass, the first evaluation framework designed\nspecifically for post-deployment monitoring and debugging of agentic workflows.\nAgentCompass models the reasoning process of expert debuggers through a\nstructured, multi-stage analytical pipeline: error identification and\ncategorization, thematic clustering, quantitative scoring, and strategic\nsummarization. The framework is further enhanced with a dual memory\nsystem-episodic and semantic-that enables continual learning across executions.\nThrough collaborations with design partners, we demonstrate the framework's\npractical utility on real-world deployments, before establishing its efficacy\nagainst the publicly available TRAIL benchmark. AgentCompass achieves\nstate-of-the-art results on key metrics, while uncovering critical issues\nmissed in human annotations, underscoring its role as a robust,\ndeveloper-centric tool for reliable monitoring and improvement of agentic\nsystems in production."}
{"id": "2509.14335", "categories": ["cs.CR", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.14335", "abs": "https://arxiv.org/abs/2509.14335", "authors": ["Xinran Zheng", "Xingzhi Qian", "Yiling He", "Shuo Yang", "Lorenzo Cavallaro"], "title": "Beyond Classification: Evaluating LLMs for Fine-Grained Automatic Malware Behavior Auditing", "comment": null, "summary": "Automated malware classification has achieved strong detection performance.\nYet, malware behavior auditing seeks causal and verifiable explanations of\nmalicious activities -- essential not only to reveal what malware does but also\nto substantiate such claims with evidence. This task is challenging, as\nadversarial intent is often hidden within complex, framework-heavy\napplications, making manual auditing slow and costly. Large Language Models\n(LLMs) could help address this gap, but their auditing potential remains\nlargely unexplored due to three limitations: (1) scarce fine-grained\nannotations for fair assessment; (2) abundant benign code obscuring malicious\nsignals; and (3) unverifiable, hallucination-prone outputs undermining\nattribution credibility. To close this gap, we introduce MalEval, a\ncomprehensive framework for fine-grained Android malware auditing, designed to\nevaluate how effectively LLMs support auditing under real-world constraints.\nMalEval provides expert-verified reports and an updated sensitive API list to\nmitigate ground truth scarcity and reduce noise via static reachability\nanalysis. Function-level structural representations serve as intermediate\nattribution units for verifiable evaluation. Building on this, we define four\nanalyst-aligned tasks -- function prioritization, evidence attribution,\nbehavior synthesis, and sample discrimination -- together with domain-specific\nmetrics and a unified workload-oriented score. We evaluate seven widely used\nLLMs on a curated dataset of recent malware and misclassified benign apps,\noffering the first systematic assessment of their auditing capabilities.\nMalEval reveals both promising potential and critical limitations across audit\nstages, providing a reproducible benchmark and foundation for future research\non LLM-enhanced malware behavior auditing. MalEval is publicly available at\nhttps://github.com/ZhengXR930/MalEval.git"}
{"id": "2509.14404", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.PL"], "pdf": "https://arxiv.org/pdf/2509.14404", "abs": "https://arxiv.org/abs/2509.14404", "authors": ["Haoye Tian", "Chong Wang", "BoYang Yang", "Lyuye Zhang", "Yang Liu"], "title": "A Taxonomy of Prompt Defects in LLM Systems", "comment": null, "summary": "Large Language Models (LLMs) have become key components of modern software,\nwith prompts acting as their de-facto programming interface. However, prompt\ndesign remains largely empirical and small mistakes can cascade into\nunreliable, insecure, or inefficient behavior. This paper presents the first\nsystematic survey and taxonomy of prompt defects, recurring ways that prompts\nfail to elicit their intended behavior from LLMs. We organize defects along six\ndimensions: (1) Specification and Intent, (2) Input and Content, (3) Structure\nand Formatting, (4) Context and Memory, (5) Performance and Efficiency, and (6)\nMaintainability and Engineering. Each dimension is refined into fine-grained\nsubtypes, illustrated with concrete examples and root cause analysis. Grounded\nin software engineering principles, we show how these defects surface in real\ndevelopment workflows and examine their downstream effects. For every subtype,\nwe distill mitigation strategies that span emerging prompt engineering\npatterns, automated guardrails, testing harnesses, and evaluation frameworks.\nWe then summarize these strategies in a master taxonomy that links defect,\nimpact, and remedy. We conclude with open research challenges and a call for\nrigorous engineering-oriented methodologies to ensure that LLM-driven systems\nare dependable by design."}
{"id": "2509.14662", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14662", "abs": "https://arxiv.org/abs/2509.14662", "authors": ["Ming Li", "Nan Zhang", "Chenrui Fan", "Hong Jiao", "Yanbin Fu", "Sydney Peters", "Qingshu Xu", "Robert Lissitz", "Tianyi Zhou"], "title": "Understanding the Thinking Process of Reasoning Models: A Perspective from Schoenfeld's Episode Theory", "comment": "EMNLP2025 main, Camera-ready", "summary": "While Large Reasoning Models (LRMs) generate extensive chain-of-thought\nreasoning, we lack a principled framework for understanding how these thoughts\nare structured. In this paper, we introduce a novel approach by applying\nSchoenfeld's Episode Theory, a classic cognitive framework for human\nmathematical problem-solving, to analyze the reasoning traces of LRMs. We\nannotated thousands of sentences and paragraphs from model-generated solutions\nto math problems using seven cognitive labels (e.g., Plan, Implement, Verify).\nThe result is the first publicly available benchmark for the fine-grained\nanalysis of machine reasoning, including a large annotated corpus and detailed\nannotation guidebooks. Our preliminary analysis reveals distinct patterns in\nLRM reasoning, such as the transition dynamics between cognitive states. This\nframework provides a theoretically grounded methodology for interpreting LRM\ncognition and enables future work on more controllable and transparent\nreasoning systems."}
{"id": "2509.14558", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.14558", "abs": "https://arxiv.org/abs/2509.14558", "authors": ["Guorui Chen", "Yifan Xia", "Xiaojun Jia", "Zhijiang Li", "Philip Torr", "Jindong Gu"], "title": "LLM Jailbreak Detection for (Almost) Free!", "comment": null, "summary": "Large language models (LLMs) enhance security through alignment when widely\nused, but remain susceptible to jailbreak attacks capable of producing\ninappropriate content. Jailbreak detection methods show promise in mitigating\njailbreak attacks through the assistance of other models or multiple model\ninferences. However, existing methods entail significant computational costs.\nIn this paper, we first present a finding that the difference in output\ndistributions between jailbreak and benign prompts can be employed for\ndetecting jailbreak prompts. Based on this finding, we propose a Free Jailbreak\nDetection (FJD) which prepends an affirmative instruction to the input and\nscales the logits by temperature to further distinguish between jailbreak and\nbenign prompts through the confidence of the first token. Furthermore, we\nenhance the detection performance of FJD through the integration of virtual\ninstruction learning. Extensive experiments on aligned LLMs show that our FJD\ncan effectively detect jailbreak prompts with almost no additional\ncomputational costs during LLM inference."}
{"id": "2509.14483", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.14483", "abs": "https://arxiv.org/abs/2509.14483", "authors": ["Thanh-Long Bui", "Hoa Khanh Dam", "Rashina Hoda"], "title": "An LLM-based multi-agent framework for agile effort estimation", "comment": "Submitted to ASE'25", "summary": "Effort estimation is a crucial activity in agile software development, where\nteams collaboratively review, discuss, and estimate the effort required to\ncomplete user stories in a product backlog. Current practices in agile effort\nestimation heavily rely on subjective assessments, leading to inaccuracies and\ninconsistencies in the estimates. While recent machine learning-based methods\nshow promising accuracy, they cannot explain or justify their estimates and\nlack the capability to interact with human team members. Our paper fills this\nsignificant gap by leveraging the powerful capabilities of Large Language\nModels (LLMs). We propose a novel LLM-based multi-agent framework for agile\nestimation that not only can produce estimates, but also can coordinate,\ncommunicate and discuss with human developers and other agents to reach a\nconsensus. Evaluation results on a real-life dataset show that our approach\noutperforms state-of-the-art techniques across all evaluation metrics in the\nmajority of the cases. Our human study with software development practitioners\nalso demonstrates an overwhelmingly positive experience in collaborating with\nour agents in agile effort estimation."}
{"id": "2509.14693", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14693", "abs": "https://arxiv.org/abs/2509.14693", "authors": ["Song Xu", "Yilun Liu", "Minggui He", "Mingchen Dai", "Ziang Chen", "Chunguang Zhao", "Jingzhou Du", "Shimin Tao", "Weibin Meng", "Shenglin Zhang", "Yongqian Sun", "Boxing Chen", "Daimeng Wei"], "title": "RationAnomaly: Log Anomaly Detection with Rationality via Chain-of-Thought and Reinforcement Learning", "comment": "5 pages, 3 figures", "summary": "Logs constitute a form of evidence signaling the operational status of\nsoftware systems. Automated log anomaly detection is crucial for ensuring the\nreliability of modern software systems. However, existing approaches face\nsignificant limitations: traditional deep learning models lack interpretability\nand generalization, while methods leveraging Large Language Models are often\nhindered by unreliability and factual inaccuracies. To address these issues, we\npropose RationAnomaly, a novel framework that enhances log anomaly detection by\nsynergizing Chain-of-Thought (CoT) fine-tuning with reinforcement learning. Our\napproach first instills expert-like reasoning patterns using CoT-guided\nsupervised fine-tuning, grounded in a high-quality dataset corrected through a\nrigorous expert-driven process. Subsequently, a reinforcement learning phase\nwith a multi-faceted reward function optimizes for accuracy and logical\nconsistency, effectively mitigating hallucinations. Experimentally,\nRationAnomaly outperforms state-of-the-art baselines, achieving superior\nF1-scores on key benchmarks while providing transparent, step-by-step\nanalytical outputs. We have released the corresponding resources, including\ncode and datasets."}
{"id": "2509.14583", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.14583", "abs": "https://arxiv.org/abs/2509.14583", "authors": ["Johnny So", "Michael Ferdman", "Nick Nikiforakis"], "title": "What Gets Measured Gets Managed: Mitigating Supply Chain Attacks with a Link Integrity Management System", "comment": "Extended version of the paper \"What Gets Measured Gets Managed:\n  Mitigating Supply Chain Attacks with a Link Integrity Management System\" that\n  will be published in ACM CCS 2025", "summary": "The web continues to grow, but dependency-monitoring tools and standards for\nresource integrity lag behind. Currently, there exists no robust method to\nverify the integrity of web resources, much less in a generalizable yet\nperformant manner, and supply chains remain one of the most targeted parts of\nthe attack surface of web applications.\n  In this paper, we present the design of LiMS, a transparent system to\nbootstrap link integrity guarantees in web browsing sessions with minimal\noverhead. At its core, LiMS uses a set of customizable integrity policies to\ndeclare the (un)expected properties of resources, verifies these policies, and\nenforces them for website visitors. We discuss how basic integrity policies can\nserve as building blocks for a comprehensive set of integrity policies, while\nproviding guarantees that would be sufficient to defend against recent supply\nchain attacks detailed by security industry reports. Finally, we evaluate our\nopen-sourced prototype by simulating deployments on a representative sample of\n450 domains that are diverse in ranking and category. We find that our proposal\noffers the ability to bootstrap marked security improvements with an overall\noverhead of hundreds of milliseconds on initial page loads, and negligible\noverhead on reloads, regardless of network speeds. In addition, from examining\narchived data for the sample sites, we find that several of the proposed policy\nbuilding blocks suit their dependency usage patterns, and would incur minimal\nadministrative overhead."}
{"id": "2509.14623", "categories": ["cs.SE", "cs.AI", "cs.PL", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.14623", "abs": "https://arxiv.org/abs/2509.14623", "authors": ["Hanlong Wan", "Xing Lu", "Yan Chen", "Karthik Devaprasad", "Laura Hinkle"], "title": "Automating Modelica Module Generation Using Large Language Models: A Case Study on Building Control Description Language", "comment": "This is the pre-peer-review version of a journal paper; the repo is\n  available at: https://github.com/pnnl/prompt2control", "summary": "Dynamic energy systems and controls require advanced modeling frameworks to\ndesign and test supervisory and fault tolerant strategies. Modelica is a widely\nused equation based language, but developing control modules is labor intensive\nand requires specialized expertise. This paper examines the use of large\nlanguage models (LLMs) to automate the generation of Control Description\nLanguage modules in the Building Modelica Library as a case study. We developed\na structured workflow that combines standardized prompt scaffolds, library\naware grounding, automated compilation with OpenModelica, and human in the loop\nevaluation. Experiments were carried out on four basic logic tasks (And, Or,\nNot, and Switch) and five control modules (chiller enable/disable, bypass valve\ncontrol, cooling tower fan speed, plant requests, and relief damper control).\nThe results showed that GPT 4o failed to produce executable Modelica code in\nzero shot mode, while Claude Sonnet 4 achieved up to full success for basic\nlogic blocks with carefully engineered prompts. For control modules, success\nrates reached 83 percent, and failed outputs required medium level human repair\n(estimated one to eight hours). Retrieval augmented generation often produced\nmismatches in module selection (for example, And retrieved as Or), while a\ndeterministic hard rule search strategy avoided these errors. Human evaluation\nalso outperformed AI evaluation, since current LLMs cannot assess simulation\nresults or validate behavioral correctness. Despite these limitations, the LLM\nassisted workflow reduced the average development time from 10 to 20 hours down\nto 4 to 6 hours per module, corresponding to 40 to 60 percent time savings.\nThese results highlight both the potential and current limitations of LLM\nassisted Modelica generation, and point to future research in pre simulation\nvalidation, stronger grounding, and closed loop evaluation."}
{"id": "2509.14704", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14704", "abs": "https://arxiv.org/abs/2509.14704", "authors": ["Masaharu Mizumoto", "Dat Nguyen", "Zhiheng Han", "Jiyuan Fang", "Heyuan Guan", "Xingfu Li", "Naoya Shiraishi", "Xuyang Tian", "Yo Nakawake", "Le Minh Nguyen"], "title": "The NazoNazo Benchmark: A Cost-Effective and Extensible Test of Insight-Based Reasoning in LLMs", "comment": null, "summary": "Benchmark saturation and contamination undermine confidence in LLM\nevaluation. We present Nazonazo, a cost-effective and extensible benchmark\nbuilt from Japanese children's riddles to test insight-based reasoning. Items\nare short (mostly one sentence), require no specialized domain knowledge, and\ncan be generated at scale, enabling rapid refresh of blind sets when leakage is\nsuspected. We evaluate 38 frontier models and 126 adults on 120 riddles. No\nmodel except for GPT-5 is comparable to human performance, which achieves a\n52.9% mean accuracy. Model comparison on extended 201 items shows that\nreasoning models significantly outperform non-reasoning peers, while model size\nshows no reliable association with accuracy. Beyond aggregate accuracy, an\ninformal candidate-tracking analysis of thought logs reveals many cases of\nverification failure: models often produce the correct solution among\nintermediate candidates yet fail to select it as the final answer, which we\nillustrate with representative examples observed in multiple models. Nazonazo\nthus offers a cost-effective, scalable, and easily renewable benchmark format\nthat addresses the current evaluation crisis while also suggesting a recurrent\nmeta-cognitive weakness, providing clear targets for future control and\ncalibration methods."}
{"id": "2509.14589", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14589", "abs": "https://arxiv.org/abs/2509.14589", "authors": ["Taesoo Kim", "HyungSeok Han", "Soyeon Park", "Dae R. Jeong", "Dohyeok Kim", "Dongkwan Kim", "Eunsoo Kim", "Jiho Kim", "Joshua Wang", "Kangsu Kim", "Sangwoo Ji", "Woosun Song", "Hanqing Zhao", "Andrew Chin", "Gyejin Lee", "Kevin Stevens", "Mansour Alharthi", "Yizhuo Zhai", "Cen Zhang", "Joonun Jang", "Yeongjin Jang", "Ammar Askar", "Dongju Kim", "Fabian Fleischer", "Jeongin Cho", "Junsik Kim", "Kyungjoon Ko", "Insu Yun", "Sangdon Park", "Dowoo Baik", "Haein Lee", "Hyeon Heo", "Minjae Gwon", "Minjae Lee", "Minwoo Baek", "Seunggi Min", "Wonyoung Kim", "Yonghwi Jin", "Younggi Park", "Yunjae Choi", "Jinho Jung", "Gwanhyun Lee", "Junyoung Jang", "Kyuheon Kim", "Yeonghyeon Cha", "Youngjoon Kim"], "title": "ATLANTIS: AI-driven Threat Localization, Analysis, and Triage Intelligence System", "comment": "Version 1.0 (September 17, 2025). Technical Report. Team Atlanta --\n  1st place in DARPA AIxCC Final Competition. Project page:\n  https://team-atlanta.github.io/", "summary": "We present ATLANTIS, the cyber reasoning system developed by Team Atlanta\nthat won 1st place in the Final Competition of DARPA's AI Cyber Challenge\n(AIxCC) at DEF CON 33 (August 2025). AIxCC (2023-2025) challenged teams to\nbuild autonomous cyber reasoning systems capable of discovering and patching\nvulnerabilities at the speed and scale of modern software. ATLANTIS integrates\nlarge language models (LLMs) with program analysis -- combining symbolic\nexecution, directed fuzzing, and static analysis -- to address limitations in\nautomated vulnerability discovery and program repair. Developed by researchers\nat Georgia Institute of Technology, Samsung Research, KAIST, and POSTECH, the\nsystem addresses core challenges: scaling across diverse codebases from C to\nJava, achieving high precision while maintaining broad coverage, and producing\nsemantically correct patches that preserve intended behavior. We detail the\ndesign philosophy, architectural decisions, and implementation strategies\nbehind ATLANTIS, share lessons learned from pushing the boundaries of automated\nsecurity when program analysis meets modern AI, and release artifacts to\nsupport reproducibility and future research."}
{"id": "2509.14626", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.14626", "abs": "https://arxiv.org/abs/2509.14626", "authors": ["Feiran Qin", "M. M. Abid Naziri", "Hengyu Ai", "Saikat Dutta", "Marcelo d'Amorim"], "title": "Evaluating the Effectiveness of Coverage-Guided Fuzzing for Testing Deep Learning Library APIs", "comment": null, "summary": "Deep Learning (DL) libraries such as PyTorch provide the core components to\nbuild major AI-enabled applications. Finding bugs in these libraries is\nimportant and challenging. Prior approaches have tackled this by performing\neither API-level fuzzing or model-level fuzzing, but they do not use coverage\nguidance, which limits their effectiveness and efficiency. This raises an\nintriguing question: can coverage guided fuzzing (CGF), in particular\nframeworks like LibFuzzer, be effectively applied to DL libraries, and does it\noffer meaningful improvements in code coverage, bug detection, and scalability\ncompared to prior methods?\n  We present the first in-depth study to answer this question. A key challenge\nin applying CGF to DL libraries is the need to create a test harness for each\nAPI that can transform byte-level fuzzer inputs into valid API inputs. To\naddress this, we propose FlashFuzz, a technique that leverages Large Language\nModels (LLMs) to automatically synthesize API-level harnesses by combining\ntemplates, helper functions, and API documentation. FlashFuzz uses a feedback\ndriven strategy to iteratively synthesize and repair harnesses. With this\napproach, FlashFuzz synthesizes harnesses for 1,151 PyTorch and 662 TensorFlow\nAPIs. Compared to state-of-the-art fuzzing methods (ACETest, PathFinder, and\nTitanFuzz), FlashFuzz achieves up to 101.13 to 212.88 percent higher coverage\nand 1.0x to 5.4x higher validity rate, while also delivering 1x to 1182x\nspeedups in input generation. FlashFuzz has discovered 42 previously unknown\nbugs in PyTorch and TensorFlow, 8 of which are already fixed. Our study\nconfirms that CGF can be effectively applied to DL libraries and provides a\nstrong baseline for future testing approaches."}
{"id": "2509.14750", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14750", "abs": "https://arxiv.org/abs/2509.14750", "authors": ["Letian Zhang", "Guanghao Meng", "Xudong Ren", "Yiming Wang", "Shu-Tao Xia"], "title": "Enhancing Retrieval Augmentation via Adversarial Collaboration", "comment": null, "summary": "Retrieval-augmented Generation (RAG) is a prevalent approach for\ndomain-specific LLMs, yet it is often plagued by \"Retrieval Hallucinations\"--a\nphenomenon where fine-tuned models fail to recognize and act upon poor-quality\nretrieved documents, thus undermining performance. To address this, we propose\nthe Adversarial Collaboration RAG (AC-RAG) framework. AC-RAG employs two\nheterogeneous agents: a generalist Detector that identifies knowledge gaps, and\na domain-specialized Resolver that provides precise solutions. Guided by a\nmoderator, these agents engage in an adversarial collaboration, where the\nDetector's persistent questioning challenges the Resolver's expertise. This\ndynamic process allows for iterative problem dissection and refined knowledge\nretrieval. Extensive experiments show that AC-RAG significantly improves\nretrieval accuracy and outperforms state-of-the-art RAG methods across various\nvertical domains."}
{"id": "2509.14604", "categories": ["cs.CR", "cs.ET"], "pdf": "https://arxiv.org/pdf/2509.14604", "abs": "https://arxiv.org/abs/2509.14604", "authors": ["Ramazan Yener", "Muhammad Hassan", "Masooda Bashir"], "title": "Threats and Security Strategies for IoMT Infusion Pumps", "comment": "9 pages. Published as a book chapter in Human Factors in\n  Cybersecurity (AHFE 2025)", "summary": "The integration of the Internet of Medical Things (IoMT) into healthcare\nsystems has transformed patient care by enabling real-time monitoring, enhanced\ndiagnostics, and enhanced operational efficiency. However, this increased\nconnectivity has also expanded the attack surface for cybercriminals, raising\nsignificant cybersecurity and privacy concerns. This study focuses on the\ncybersecurity vulnerabilities of IoMT infusion pumps, which are critical\ndevices in modern healthcare. Through a targeted literature review of the past\nfive years, we analyzed seven current studies from a pool of 132 papers to\nidentify security vulnerabilities. Our findings indicate that infusion pumps\nface vulnerabilities such as device-level flaws, authentication and access\ncontrol issues, network and communication weaknesses, data security and privacy\nrisks, and operational or organizational challenges that can expose them to\nlateral attacks within healthcare networks. Our analysis synthesizes findings\nfrom seven recent studies to clarify how and why infusion pumps remain\nvulnerable in each of these areas. By categorizing the security gaps, we\nhighlight critical risk patterns and their implications. This work underscores\nthe scope of the issue and provides a structured understanding that is valuable\nfor healthcare IT professionals and device manufacturers. Ultimately, the\nfindings can inform the development of targeted, proactive security strategies\nto better safeguard infusion pumps and protect patient well-being."}
{"id": "2509.14646", "categories": ["cs.SE", "cs.PL"], "pdf": "https://arxiv.org/pdf/2509.14646", "abs": "https://arxiv.org/abs/2509.14646", "authors": ["Yongpan Wang", "Xin Xu", "Xiaojie Zhu", "Xiaodong Gu", "Beijun Shen"], "title": "SALT4Decompile: Inferring Source-level Abstract Logic Tree for LLM-Based Binary Decompilation", "comment": "13 pages, 7 figures", "summary": "Decompilation is widely used in reverse engineering to recover high-level\nlanguage code from binary executables. While recent approaches leveraging Large\nLanguage Models (LLMs) have shown promising progress, they typically treat\nassembly code as a linear sequence of instructions, overlooking arbitrary jump\npatterns and isolated data segments inherent to binary files. This limitation\nsignificantly hinders their ability to correctly infer source code semantics\nfrom assembly code. To address this limitation, we propose \\saltm, a novel\nbinary decompilation method that abstracts stable logical features shared\nbetween binary and source code. The core idea of \\saltm is to abstract selected\nbinary-level operations, such as specific jumps, into a high-level logic\nframework that better guides LLMs in semantic recovery. Given a binary\nfunction, \\saltm constructs a Source-level Abstract Logic Tree (\\salt) from\nassembly code to approximate the logic structure of high-level language. It\nthen fine-tunes an LLM using the reconstructed \\salt to generate decompiled\ncode. Finally, the output is refined through error correction and symbol\nrecovery to improve readability and correctness. We compare \\saltm to three\ncategories of baselines (general-purpose LLMs, commercial decompilers, and\ndecompilation methods) using three well-known datasets (Decompile-Eval, MBPP,\nExebench). Our experimental results demonstrate that \\saltm is highly effective\nin recovering the logic of the source code, significantly outperforming\nstate-of-the-art methods (e.g., 70.4\\% TCP rate on Decompile-Eval with a 10.6\\%\nimprovement). The results further validate its robustness against four commonly\nused obfuscation techniques. Additionally, analyses of real-world software and\na user study confirm that our decompiled output offers superior assistance to\nhuman analysts in comprehending binary functions."}
{"id": "2509.14778", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.14778", "abs": "https://arxiv.org/abs/2509.14778", "authors": ["Yuxiao Cheng", "Jinli Suo"], "title": "OpenLens AI: Fully Autonomous Research Agent for Health Infomatics", "comment": null, "summary": "Health informatics research is characterized by diverse data modalities,\nrapid knowledge expansion, and the need to integrate insights across biomedical\nscience, data analytics, and clinical practice. These characteristics make it\nparticularly well-suited for agent-based approaches that can automate knowledge\nexploration, manage complex workflows, and generate clinically meaningful\noutputs. Recent progress in large language model (LLM)-based agents has\ndemonstrated promising capabilities in literature synthesis, data analysis, and\neven end-to-end research execution. However, existing systems remain limited\nfor health informatics because they lack mechanisms to interpret medical\nvisualizations and often overlook domain-specific quality requirements. To\naddress these gaps, we introduce OpenLens AI, a fully automated framework\ntailored to health informatics. OpenLens AI integrates specialized agents for\nliterature review, data analysis, code generation, and manuscript preparation,\nenhanced by vision-language feedback for medical visualization and quality\ncontrol for reproducibility. The framework automates the entire research\npipeline, producing publication-ready LaTeX manuscripts with transparent and\ntraceable workflows, thereby offering a domain-adapted solution for advancing\nhealth informatics research."}
{"id": "2509.14608", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14608", "abs": "https://arxiv.org/abs/2509.14608", "authors": ["Shashank Shreedhar Bhatt", "Tanmay Rajore", "Khushboo Aggarwal", "Ganesh Ananthanarayanan", "Ranveer Chandra", "Nishanth Chandran", "Suyash Choudhury", "Divya Gupta", "Emre Kiciman", "Sumit Kumar Pandey", "Srinath Setty", "Rahul Sharma", "Teijia Zhao"], "title": "Enterprise AI Must Enforce Participant-Aware Access Control", "comment": null, "summary": "Large language models (LLMs) are increasingly deployed in enterprise settings\nwhere they interact with multiple users and are trained or fine-tuned on\nsensitive internal data. While fine-tuning enhances performance by\ninternalizing domain knowledge, it also introduces a critical security risk:\nleakage of confidential training data to unauthorized users. These risks are\nexacerbated when LLMs are combined with Retrieval-Augmented Generation (RAG)\npipelines that dynamically fetch contextual documents at inference time.\n  We demonstrate data exfiltration attacks on AI assistants where adversaries\ncan exploit current fine-tuning and RAG architectures to leak sensitive\ninformation by leveraging the lack of access control enforcement. We show that\nexisting defenses, including prompt sanitization, output filtering, system\nisolation, and training-level privacy mechanisms, are fundamentally\nprobabilistic and fail to offer robust protection against such attacks.\n  We take the position that only a deterministic and rigorous enforcement of\nfine-grained access control during both fine-tuning and RAG-based inference can\nreliably prevent the leakage of sensitive data to unauthorized recipients.\n  We introduce a framework centered on the principle that any content used in\ntraining, retrieval, or generation by an LLM is explicitly authorized for\n\\emph{all users involved in the interaction}. Our approach offers a simple yet\npowerful paradigm shift for building secure multi-user LLM systems that are\ngrounded in classical access control but adapted to the unique challenges of\nmodern AI workflows. Our solution has been deployed in Microsoft Copilot\nTuning, a product offering that enables organizations to fine-tune models using\ntheir own enterprise-specific data."}
{"id": "2509.14740", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.14740", "abs": "https://arxiv.org/abs/2509.14740", "authors": ["Andrei-Raoul Morariu", "Andreas Strandberg", "Bogdan Iancu", "Jerker Bjorkqvist"], "title": "Wireless Communication Performance Testing: From Laboratory Environment to Research Vessel", "comment": "5 pages, 4 figures, 2 tables", "summary": "This study investigates signal transmission within a shared spectrum,\nfocusing on measurements conducted both in laboratory and outdoor environments.\nThe objective was to demonstrate how laboratory objects obstructing the line of\nsight can attenuate the signal between a transmitter (Tx) and a receiver (Rx).\nAdditionally, we examined the impact of distance and placement in various\nlocations aboard an electric research boat on signal transmission efficiency.\nThese findings contribute to understanding whether the environmental factors\ninfluence wireless communication in dynamic and obstructed environments."}
{"id": "2509.14942", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14942", "abs": "https://arxiv.org/abs/2509.14942", "authors": ["Minh-Khoi Pham", "Tai Tan Mai", "Martin Crane", "Rob Brennan", "Marie E. Ward", "Una Geary", "Declan Byrne", "Brian O Connell", "Colm Bergin", "Donncha Creagh", "Nick McDonald", "Marija Bezbradica"], "title": "Explainable AI for Infection Prevention and Control: Modeling CPE Acquisition and Patient Outcomes in an Irish Hospital with Transformers", "comment": "Accepted to BMC Medical Informatics and Decision Making on September\n  18th 2025", "summary": "Carbapenemase-Producing Enterobacteriace poses a critical concern for\ninfection prevention and control in hospitals. However, predictive modeling of\npreviously highlighted CPE-associated risks such as readmission, mortality, and\nextended length of stay (LOS) remains underexplored, particularly with modern\ndeep learning approaches. This study introduces an eXplainable AI modeling\nframework to investigate CPE impact on patient outcomes from Electronic Medical\nRecords data of an Irish hospital. We analyzed an inpatient dataset from an\nIrish acute hospital, incorporating diagnostic codes, ward transitions, patient\ndemographics, infection-related variables and contact network features. Several\nTransformer-based architectures were benchmarked alongside traditional machine\nlearning models. Clinical outcomes were predicted, and XAI techniques were\napplied to interpret model decisions. Our framework successfully demonstrated\nthe utility of Transformer-based models, with TabTransformer consistently\noutperforming baselines across multiple clinical prediction tasks, especially\nfor CPE acquisition (AUROC and sensitivity). We found infection-related\nfeatures, including historical hospital exposure, admission context, and\nnetwork centrality measures, to be highly influential in predicting patient\noutcomes and CPE acquisition risk. Explainability analyses revealed that\nfeatures like \"Area of Residence\", \"Admission Ward\" and prior admissions are\nkey risk factors. Network variables like \"Ward PageRank\" also ranked highly,\nreflecting the potential value of structural exposure information. This study\npresents a robust and explainable AI framework for analyzing complex EMR data\nto identify key risk factors and predict CPE-related outcomes. Our findings\nunderscore the superior performance of the Transformer models and highlight the\nimportance of diverse clinical and network features."}
{"id": "2509.14622", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14622", "abs": "https://arxiv.org/abs/2509.14622", "authors": ["Yihao Guo", "Haocheng Bian", "Liutong Zhou", "Ze Wang", "Zhaoyi Zhang", "Francois Kawala", "Milan Dean", "Ian Fischer", "Yuantao Peng", "Noyan Tokgozoglu", "Ivan Barrientos", "Riyaaz Shaik", "Rachel Li", "Chandru Venkataraman", "Reza Shifteh Far", "Moses Pawar", "Venkat Sundaranatha", "Michael Xu", "Frank Chu"], "title": "Adversarial Distilled Retrieval-Augmented Guarding Model for Online Malicious Intent Detection", "comment": null, "summary": "With the deployment of Large Language Models (LLMs) in interactive\napplications, online malicious intent detection has become increasingly\ncritical. However, existing approaches fall short of handling diverse and\ncomplex user queries in real time. To address these challenges, we introduce\nADRAG (Adversarial Distilled Retrieval-Augmented Guard), a two-stage framework\nfor robust and efficient online malicious intent detection. In the training\nstage, a high-capacity teacher model is trained on adversarially perturbed,\nretrieval-augmented inputs to learn robust decision boundaries over diverse and\ncomplex user queries. In the inference stage, a distillation scheduler\ntransfers the teacher's knowledge into a compact student model, with a\ncontinually updated knowledge base collected online. At deployment, the compact\nstudent model leverages top-K similar safety exemplars retrieved from the\nonline-updated knowledge base to enable both online and real-time malicious\nquery detection. Evaluations across ten safety benchmarks demonstrate that\nADRAG, with a 149M-parameter model, achieves 98.5% of WildGuard-7B's\nperformance, surpasses GPT-4 by 3.3% and Llama-Guard-3-8B by 9.5% on\nout-of-distribution detection, while simultaneously delivering up to 5.6x lower\nlatency at 300 queries per second (QPS) in real-time applications."}
{"id": "2509.14744", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.14744", "abs": "https://arxiv.org/abs/2509.14744", "authors": ["Worawalan Chatlatanagulchai", "Kundjanasith Thonglek", "Brittany Reid", "Yutaro Kashiwa", "Pattara Leelaprute", "Arnon Rungsawang", "Bundit Manaskasemsak", "Hajimu Iida"], "title": "On the Use of Agentic Coding Manifests: An Empirical Study of Claude Code", "comment": null, "summary": "Agentic coding tools receive goals written in natural language as input,\nbreak them down into specific tasks, and write/execute the actual code with\nminimal human intervention. Key to this process are agent manifests,\nconfiguration files (such as Claude.md) that provide agents with essential\nproject context, identity, and operational rules. However, the lack of\ncomprehensive and accessible documentation for creating these manifests\npresents a significant challenge for developers. We analyzed 253 Claude.md\nfiles from 242 repositories to identify structural patterns and common content.\nOur findings show that manifests typically have shallow hierarchies with one\nmain heading and several subsections, with content dominated by operational\ncommands, technical implementation notes, and high-level architecture."}
{"id": "2509.14956", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.14956", "abs": "https://arxiv.org/abs/2509.14956", "authors": ["Diego Gosmar", "Deborah A. Dahl"], "title": "Sentinel Agents for Secure and Trustworthy Agentic AI in Multi-Agent Systems", "comment": "25 pages, 12 figures", "summary": "This paper proposes a novel architectural framework aimed at enhancing\nsecurity and reliability in multi-agent systems (MAS). A central component of\nthis framework is a network of Sentinel Agents, functioning as a distributed\nsecurity layer that integrates techniques such as semantic analysis via large\nlanguage models (LLMs), behavioral analytics, retrieval-augmented verification,\nand cross-agent anomaly detection. Such agents can potentially oversee\ninter-agent communications, identify potential threats, enforce privacy and\naccess controls, and maintain comprehensive audit records. Complementary to the\nidea of Sentinel Agents is the use of a Coordinator Agent. The Coordinator\nAgent supervises policy implementation, and manages agent participation. In\naddition, the Coordinator also ingests alerts from Sentinel Agents. Based on\nthese alerts, it can adapt policies, isolate or quarantine misbehaving agents,\nand contain threats to maintain the integrity of the MAS ecosystem. This\ndual-layered security approach, combining the continuous monitoring of Sentinel\nAgents with the governance functions of Coordinator Agents, supports dynamic\nand adaptive defense mechanisms against a range of threats, including prompt\ninjection, collusive agent behavior, hallucinations generated by LLMs, privacy\nbreaches, and coordinated multi-agent attacks. In addition to the architectural\ndesign, we present a simulation study where 162 synthetic attacks of different\nfamilies (prompt injection, hallucination, and data exfiltration) were injected\ninto a multi-agent conversational environment. The Sentinel Agents successfully\ndetected the attack attempts, confirming the practical feasibility of the\nproposed monitoring approach. The framework also offers enhanced system\nobservability, supports regulatory compliance, and enables policy evolution\nover time."}
{"id": "2509.14657", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14657", "abs": "https://arxiv.org/abs/2509.14657", "authors": ["Sergio Benlloch-Lopez", "Miquel Viel-Vazquez", "Javier Naranjo-Alcazar", "Jordi Grau-Haro", "Pedro Zuccarello"], "title": "Threat Modeling for Enhancing Security of IoT Audio Classification Devices under a Secure Protocols Framework", "comment": "Accepted at Computing Conference 2026, London, UK", "summary": "The rapid proliferation of IoT nodes equipped with microphones and capable of\nperforming on-device audio classification exposes highly sensitive data while\noperating under tight resource constraints. To protect against this, we present\na defence-in-depth architecture comprising a security protocol that treats the\nedge device, cellular network and cloud backend as three separate trust\ndomains, linked by TPM-based remote attestation and mutually authenticated TLS\n1.3. A STRIDE-driven threat model and attack-tree analysis guide the design. At\nstartup, each boot stage is measured into TPM PCRs. The node can only decrypt\nits LUKS-sealed partitions after the cloud has verified a TPM quote and\nreleased a one-time unlock key. This ensures that rogue or tampered devices\nremain inert. Data in transit is protected by TLS 1.3 and hybridised with Kyber\nand Dilithium to provide post-quantum resilience. Meanwhile, end-to-end\nencryption and integrity hashes safeguard extracted audio features. Signed,\nrollback-protected AI models and tamper-responsive sensors harden firmware and\nhardware. Data at rest follows a 3-2-1 strategy comprising a solid-state drive\nsealed with LUKS, an offline cold archive encrypted with a hybrid post-quantum\ncipher and an encrypted cloud replica. Finally, we set out a plan for\nevaluating the physical and logical security of the proposed protocol."}
{"id": "2509.14745", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.14745", "abs": "https://arxiv.org/abs/2509.14745", "authors": ["Miku Watanabe", "Hao Li", "Yutaro Kashiwa", "Brittany Reid", "Hajimu Iida", "Ahmed E. Hassan"], "title": "On the Use of Agentic Coding: An Empirical Study of Pull Requests on GitHub", "comment": null, "summary": "Large language models (LLMs) are increasingly being integrated into software\ndevelopment processes. The ability to generate code and submit pull requests\nwith minimal human intervention, through the use of autonomous AI agents, is\npoised to become a standard practice. However, little is known about the\npractical usefulness of these pull requests and the extent to which their\ncontributions are accepted in real-world projects. In this paper, we\nempirically study 567 GitHub pull requests (PRs) generated using Claude Code,\nan agentic coding tool, across 157 diverse open-source projects. Our analysis\nreveals that developers tend to rely on agents for tasks such as refactoring,\ndocumentation, and testing. The results indicate that 83.8% of these\nagent-assisted PRs are eventually accepted and merged by project maintainers,\nwith 54.9% of the merged PRs are integrated without further modification. The\nremaining 45.1% require additional changes benefit from human revisions,\nespecially for bug fixes, documentation, and adherence to project-specific\nstandards. These findings suggest that while agent-assisted PRs are largely\nacceptable, they still benefit from human oversight and refinement."}
{"id": "2509.14963", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14963", "abs": "https://arxiv.org/abs/2509.14963", "authors": ["Filip Naudot", "Andreas BrÃ¤nnstrÃ¶m", "VicenÃ§ Torra", "Timotheus Kampik"], "title": "Set Contribution Functions for Quantitative Bipolar Argumentation and their Principles", "comment": null, "summary": "We present functions that quantify the contribution of a set of arguments in\nquantitative bipolar argumentation graphs to (the final strength of) an\nargument of interest, a so-called topic. Our set contribution functions are\ngeneralizations of existing functions that quantify the contribution of a\nsingle contributing argument to a topic. Accordingly, we generalize existing\ncontribution function principles for set contribution functions and provide a\ncorresponding principle-based analysis. We introduce new principles specific to\nset-based functions that focus on properties pertaining to the interaction of\narguments within a set. Finally, we sketch how the principles play out across\ndifferent set contribution functions given a recommendation system application\nscenario."}
{"id": "2509.14706", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.14706", "abs": "https://arxiv.org/abs/2509.14706", "authors": ["Yonghao Ni", "Zhongwen Li", "Xiaoqi Li"], "title": "Security Analysis of Web Applications Based on Gruyere", "comment": null, "summary": "With the rapid development of Internet technologies, web systems have become\nessential infrastructures for modern information exchange and business\noperations. However, alongside their expansion, numerous security\nvulnerabilities have emerged, making web security a critical research focus\nwithin the broader field of cybersecurity. These issues are closely related to\ndata protection, privacy preservation, and business continuity, and systematic\nresearch on web security is crucial for mitigating malicious attacks and\nenhancing the reliability and robustness of network systems. This paper first\nreviews the OWASP Top 10, summarizing the types, causes, and impacts of common\nweb vulnerabilities, and illustrates their exploitation mechanisms through\nrepresentative cases. Building upon this, the Gruyere platform is adopted as an\nexperimental subject for analyzing known vulnerabilities. The study presents\ndetailed reproduction steps for specific vulnerabilities, proposes\ncomprehensive remediation strategies, and further compares Gruyere's\nvulnerabilities with contemporary real-world cases. The findings suggest that,\nalthough Gruyere's vulnerabilities are relatively outdated, their underlying\nprinciples remain highly relevant for explaining a wide range of modern\nsecurity flaws. Overall, this research demonstrates that web system security\nanalysis based on Gruyere not only deepens the understanding of vulnerability\nmechanisms but also provides practical support for technological innovation and\nsecurity defense."}
{"id": "2509.14829", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.14829", "abs": "https://arxiv.org/abs/2509.14829", "authors": ["Shuo Jin", "Songqiang Chen", "Xiaoyuan Xie", "Shing-Chi Cheung"], "title": "RulER: Automated Rule-Based Semantic Error Localization and Repair for Code Translation", "comment": "Due to the limitation \"The abstract field cannot be longer than 1,920\n  characters\", the abstract here is shorter than that in the PDF file", "summary": "Automated code translation aims to convert programs between different\nprogramming languages while maintaining their functionality. Due to the\nimperfections of code translation models, the generated translations may\ncontain errors that compromise their reliability. Existing automated debugging\nmethods for code translation rely on code alignments and repair patch templates\nto locate and fix erroneous translations. However, existing methods lack\nreliable references to construct code alignments and design repair patch\ntemplates, which significantly impacts their localization accuracy and repair\neffectiveness. To address these limitations, we reintroduce code translation\nrules and propose a rule-based debugging method for code translation, called\nRulER. RulER automatically derives code translation rules from correct\ntranslations generated by LLMs, enabling the efficient collection of diverse\ntranslation rules. In addition, RulER dynamically combines the existing rules\non expandable nodes like expressions and tokens to further adaptively align\nmore statements. These rules capture clear and detailed structural\ncorrespondences between source and target programming languages. Therefore,\nthey can serve as reliable and reusable references for code alignment and\nrepair template design, enabling RulER to locate and fix translation errors\neffectively. Our evaluation of RulER on Java-to-C++ and Python-to-C++\ntranslations produced by four code translation models demonstrates that RulER\noutperforms state-of-the-art methods, BatFix and TransMap. Our experimental\nresults show that RulER outperformed the best baseline by 20% and 272% in terms\nof error localization rates and repair success rates, respectively. RulER\nexhibits superior repair performance compared to directly prompting LLMs for\npatch generation, demonstrating a promising methodology for extracting and\nleveraging coding knowledge from LLMs."}
{"id": "2509.14998", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.14998", "abs": "https://arxiv.org/abs/2509.14998", "authors": ["Xiao Wu", "Ting-Zhu Huang", "Liang-Jian Deng", "Yanyuan Qiao", "Imran Razzak", "Yutong Xie"], "title": "A Knowledge-driven Adaptive Collaboration of LLMs for Enhancing Medical Decision-making", "comment": "The paper has been accepted to the EMNLP 2025 Main Conference", "summary": "Medical decision-making often involves integrating knowledge from multiple\nclinical specialties, typically achieved through multidisciplinary teams.\nInspired by this collaborative process, recent work has leveraged large\nlanguage models (LLMs) in multi-agent collaboration frameworks to emulate\nexpert teamwork. While these approaches improve reasoning through agent\ninteraction, they are limited by static, pre-assigned roles, which hinder\nadaptability and dynamic knowledge integration. To address these limitations,\nwe propose KAMAC, a Knowledge-driven Adaptive Multi-Agent Collaboration\nframework that enables LLM agents to dynamically form and expand expert teams\nbased on the evolving diagnostic context. KAMAC begins with one or more expert\nagents and then conducts a knowledge-driven discussion to identify and fill\nknowledge gaps by recruiting additional specialists as needed. This supports\nflexible, scalable collaboration in complex clinical scenarios, with decisions\nfinalized through reviewing updated agent comments. Experiments on two\nreal-world medical benchmarks demonstrate that KAMAC significantly outperforms\nboth single-agent and advanced multi-agent methods, particularly in complex\nclinical scenarios (i.e., cancer prognosis) requiring dynamic, cross-specialty\nexpertise. Our code is publicly available at:\nhttps://github.com/XiaoXiao-Woo/KAMAC."}
{"id": "2509.14754", "categories": ["cs.CR", "G.2.0"], "pdf": "https://arxiv.org/pdf/2509.14754", "abs": "https://arxiv.org/abs/2509.14754", "authors": ["Minzhong Luo", "Yudong Sun", "Yin Long"], "title": "Variables Ordering Optimization in Boolean Characteristic Set Method Using Simulated Annealing and Machine Learning-based Time Prediction", "comment": null, "summary": "Solving systems of Boolean equations is a fundamental task in symbolic\ncomputation and algebraic cryptanalysis, with wide-ranging applications in\ncryptography, coding theory, and formal verification. Among existing\napproaches, the Boolean Characteristic Set (BCS) method[1] has emerged as one\nof the most efficient algorithms for tackling such problems. However, its\nperformance is highly sensitive to the ordering of variables, with solving\ntimes varying drastically under different orderings for fixed variable counts n\nand equations size m. To address this challenge, this paper introduces a novel\noptimization framework that synergistically integrates machine learning\n(ML)-based time prediction with simulated annealing (SA) to efficiently\nidentify high-performance variables orderings. Weconstruct a dataset comprising\nvariable frequency spectrum X and corresponding BCS solving time t for\nbenchmark systems(e.g., n = m = 28). Utilizing this data, we train an accurate\nML predictor ft(X) to estimate solving time for any given variables ordering.\nFor each target system, ft serves as the cost function within an SA algorithm,\nenabling rapid discovery of low-latency orderings that significantly expedite\nsubsequent BCS execution. Extensive experiments demonstrate that our method\nsubstantially outperforms the standard BCS algorithm[1], Gr\\\"obner basis method\n[2] and SAT solver[3], particularly for larger-scale systems(e.g., n = 32).\nFurthermore, we derive probabilistic time complexity bounds for the overall\nalgorithm using stochastic process theory, establishing a quantitative\nrelationship between predictor accuracy and expected solving complexity. This\nwork provides both a practical acceleration tool for algebraic cryptanalysis\nand a theoretical foundation for ML-enhanced combinatorial optimization in\nsymbolic computation."}
{"id": "2509.14856", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.14856", "abs": "https://arxiv.org/abs/2509.14856", "authors": ["Hanyang Guo", "Xunjin Zheng", "Zihan Liao", "Hang Yu", "Peng DI", "Ziyin Zhang", "Hong-Ning Dai"], "title": "CodeFuse-CR-Bench: A Comprehensiveness-aware Benchmark for End-to-End Code Review Evaluation in Python Projects", "comment": null, "summary": "Automated code review (CR) is a key application for Large Language Models\n(LLMs), but progress is hampered by a \"reality gap\": existing benchmarks\nevaluate models on isolated sub-tasks using simplified, context-poor data. This\nfails to reflect the holistic context-rich nature of real-world CR. To bridge\nthis gap, we introduce CodeFuse-CR-Bench, the first comprehensiveness-aware\nbenchmark for repository-level CR evaluation. CodeFuse-CR-Bench comprises 601\nhigh-quality instances from 70 Python projects covering nine Pull-Request (PR)\nproblem domains, where each instance provides rich, multi-faceted context\nincluding the associated issue, PR details, and repository state, enabling\nend-to-end evaluation. Beyond superficial metrics, we also propose a novel\nevaluation framework that combines rule-based checks for location and syntax\nwith model-based judgments of review quality. We present the first large-scale\nassessment of state-of-the-art LLMs on this comprehensive CR task. Our results\nestablish crucial baselines and reveal that (1) no single LLM dominates all\naspects of CR; (2) Gemini 2.5 Pro achieves the highest comprehensive\nperformance; and (3) different LLMs exhibit varying robustness to redundant\ncontext. These findings highlight the necessity of holistic, multi-dimensional\nevaluation and provide actionable insights for advancing truly intelligent yet\npractical CR assistants."}
{"id": "2509.15035", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.15035", "abs": "https://arxiv.org/abs/2509.15035", "authors": ["Gabriela C. Zapata", "Bill Cope", "Mary Kalantzis", "Duane Searsmith"], "title": "Calibrated Generative AI as Meta-Reviewer: A Systemic Functional Linguistics Discourse Analysis of Reviews of Peer Reviews", "comment": "39 pages, 3 tables", "summary": "This study investigates the use of generative AI to support formative\nassessment through machine generated reviews of peer reviews in graduate online\ncourses in a public university in the United States. Drawing on Systemic\nFunctional Linguistics and Appraisal Theory, we analyzed 120 metareviews to\nexplore how generative AI feedback constructs meaning across ideational,\ninterpersonal, and textual dimensions. The findings suggest that generative AI\ncan approximate key rhetorical and relational features of effective human\nfeedback, offering directive clarity while also maintaining a supportive\nstance. The reviews analyzed demonstrated a balance of praise and constructive\ncritique, alignment with rubric expectations, and structured staging that\nforegrounded student agency. By modeling these qualities, AI metafeedback has\nthe potential to scaffold feedback literacy and enhance leaner engagement with\npeer review."}
{"id": "2509.14987", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14987", "abs": "https://arxiv.org/abs/2509.14987", "authors": ["Md Talha Mohsin"], "title": "Blockchain-Enabled Explainable AI for Trusted Healthcare Systems", "comment": "6 Pages, 4 Figures", "summary": "This paper introduces a Blockchain-Integrated Explainable AI Framework (BXHF)\nfor healthcare systems to tackle two essential challenges confronting health\ninformation networks: safe data exchange and comprehensible AI-driven clinical\ndecision-making. Our architecture incorporates blockchain, ensuring patient\nrecords are immutable, auditable, and tamper-proof, alongside Explainable AI\n(XAI) methodologies that yield transparent and clinically relevant model\npredictions. By incorporating security assurances and interpretability\nrequirements into a unified optimization pipeline, BXHF ensures both data-level\ntrust (by verified and encrypted record sharing) and decision-level trust (with\nauditable and clinically aligned explanations). Its hybrid edge-cloud\narchitecture allows for federated computation across different institutions,\nenabling collaborative analytics while protecting patient privacy. We\ndemonstrate the framework's applicability through use cases such as\ncross-border clinical research networks, uncommon illness detection and\nhigh-risk intervention decision support. By ensuring transparency,\nauditability, and regulatory compliance, BXHF improves the credibility, uptake,\nand effectiveness of AI in healthcare, laying the groundwork for safer and more\nreliable clinical decision-making."}
{"id": "2509.14899", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14899", "abs": "https://arxiv.org/abs/2509.14899", "authors": ["Amine Barrak", "Yosr Fourati", "Michael Olchawa", "Emna Ksontini", "Khalil Zoghlami"], "title": "CARGO: A Framework for Confidence-Aware Routing of Large Language Models", "comment": null, "summary": "As large language models (LLMs) proliferate in scale, specialization, and\nlatency profiles, the challenge of routing user prompts to the most appropriate\nmodel has become increasingly critical for balancing performance and cost. We\nintroduce CARGO (Category-Aware Routing with Gap-based Optimization), a\nlightweight, confidence-aware framework for dynamic LLM selection. CARGO\nemploys a single embedding-based regressor trained on LLM-judged pairwise\ncomparisons to predict model performance, with an optional binary classifier\ninvoked when predictions are uncertain. This two-stage design enables precise,\ncost-aware routing without the need for human-annotated supervision. To capture\ndomain-specific behavior, CARGO also supports category-specific regressors\ntrained across five task groups: mathematics, coding, reasoning, summarization,\nand creative writing. Evaluated on four competitive LLMs (GPT-4o, Claude 3.5\nSonnet, DeepSeek V3, and Perplexity Sonar), CARGO achieves a top-1 routing\naccuracy of 76.4% and win rates ranging from 72% to 89% against individual\nexperts. These results demonstrate that confidence-guided, lightweight routing\ncan achieve expert-level performance with minimal overhead, offering a\npractical solution for real-world, multi-model LLM deployments."}
{"id": "2509.15084", "categories": ["cs.AI", "cs.CY", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.15084", "abs": "https://arxiv.org/abs/2509.15084", "authors": ["Doreen Jirak", "Pieter Maes", "Armeen Saroukanoff", "Dirk van Rooy"], "title": "From Sea to System: Exploring User-Centered Explainable AI for Maritime Decision Support", "comment": "Paper accepted at Human Learning and Decision-Making Workshop\n  @ECML-PKDD Conference 2025, Porto, Portugal", "summary": "As autonomous technologies increasingly shape maritime operations,\nunderstanding why an AI system makes a decision becomes as crucial as what it\ndecides. In complex and dynamic maritime environments, trust in AI depends not\nonly on performance but also on transparency and interpretability. This paper\nhighlights the importance of Explainable AI (XAI) as a foundation for effective\nhuman-machine teaming in the maritime domain, where informed oversight and\nshared understanding are essential. To support the user-centered integration of\nXAI, we propose a domain-specific survey designed to capture maritime\nprofessionals' perceptions of trust, usability, and explainability. Our aim is\nto foster awareness and guide the development of user-centric XAI systems\ntailored to the needs of seafarers and maritime teams."}
{"id": "2509.15170", "categories": ["cs.CR", "cs.AI", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.15170", "abs": "https://arxiv.org/abs/2509.15170", "authors": ["Aarushi Mahajan", "Wayne Burleson"], "title": "Watermarking and Anomaly Detection in Machine Learning Models for LORA RF Fingerprinting", "comment": "IEEE International Conference on Acoustics, Speech, and Signal\n  Processing (ICASSP)", "summary": "Radio frequency fingerprint identification (RFFI) distinguishes wireless\ndevices by the small variations in their analog circuits, avoiding heavy\ncryptographic authentication. While deep learning on spectrograms improves\naccuracy, models remain vulnerable to copying, tampering, and evasion. We\npresent a stronger RFFI system combining watermarking for ownership proof and\nanomaly detection for spotting suspicious inputs. Using a ResNet-34 on log-Mel\nspectrograms, we embed three watermarks: a simple trigger, an adversarially\ntrained trigger robust to noise and filtering, and a hidden gradient/weight\nsignature. A convolutional Variational Autoencoders (VAE) with Kullback-Leibler\n(KL) warm-up and free-bits flags off-distribution queries. On the LoRa dataset,\nour system achieves 94.6% accuracy, 98% watermark success, and 0.94 AUROC,\noffering verifiable, tamper-resistant authentication."}
{"id": "2509.14931", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.14931", "abs": "https://arxiv.org/abs/2509.14931", "authors": ["Stefano Fossati", "Damian Andrew Tamburri", "Massimiliano Di Penta", "Marco Tonnarelli"], "title": "\"Let it be Chaos in the Plumbing!\" Usage and Efficacy of Chaos Engineering in DevOps Pipelines", "comment": "To be published in the Proceedings of International Conference on\n  Software Maintenance and Evolution 2025", "summary": "Chaos Engineering (CE) has emerged as a proactive method to improve the\nresilience of modern distributed systems, particularly within DevOps\nenvironments. Originally pioneered by Netflix, CE simulates real-world failures\nto expose weaknesses before they impact production. In this paper, we present a\nsystematic gray literature review that investigates how industry practitioners\nhave adopted and adapted CE principles over recent years. Analyzing 50 sources\npublished between 2019 and early 2024, we developed a comprehensive\nclassification framework that extends the foundational CE principles into ten\ndistinct concepts. Our study reveals that while the core tenets of CE remain\ninfluential, practitioners increasingly emphasize controlled experimentation,\nautomation, and risk mitigation strategies to align with the demands of agile\nand continuously evolving DevOps pipelines. Our results enhance the\nunderstanding of how CE is intended and implemented in practice, and offer\nguidance for future research and industrial applications aimed at improving\nsystem robustness in dynamic production environments."}
{"id": "2509.15172", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.15172", "abs": "https://arxiv.org/abs/2509.15172", "authors": ["Ankur Samanta", "Akshayaa Magesh", "Youliang Yu", "Runzhe Wu", "Ayush Jain", "Daniel Jiang", "Boris Vidolov", "Paul Sajda", "Yonathan Efroni", "Kaveh Hassani"], "title": "Internalizing Self-Consistency in Language Models: Multi-Agent Consensus Alignment", "comment": null, "summary": "Language Models (LMs) are inconsistent reasoners, often generating\ncontradictory responses to identical prompts. While inference-time methods can\nmitigate these inconsistencies, they fail to address the core problem: LMs\nstruggle to reliably select reasoning pathways leading to consistent outcomes\nunder exploratory sampling. To address this, we formalize self-consistency as\nan intrinsic property of well-aligned reasoning models and introduce\nMulti-Agent Consensus Alignment (MACA), a reinforcement learning framework that\npost-trains models to favor reasoning trajectories aligned with their internal\nconsensus using majority/minority outcomes from multi-agent debate. These\ntrajectories emerge from deliberative exchanges where agents ground reasoning\nin peer arguments, not just aggregation of independent attempts, creating\nricher consensus signals than single-round majority voting. MACA enables agents\nto teach themselves to be more decisive and concise, and better leverage peer\ninsights in multi-agent settings without external supervision, driving\nsubstantial improvements across self-consistency (+27.6% on GSM8K),\nsingle-agent reasoning (+23.7% on MATH), sampling-based inference (+22.4%\nPass@20 on MATH), and multi-agent ensemble decision-making (+42.7% on MathQA).\nThese findings, coupled with strong generalization to unseen benchmarks (+16.3%\non GPQA, +11.6% on CommonsenseQA), demonstrate robust self-alignment that more\nreliably unlocks latent reasoning potential of language models."}
{"id": "2509.15202", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.15202", "abs": "https://arxiv.org/abs/2509.15202", "authors": ["Yuanbo Xie", "Yingjie Zhang", "Tianyun Liu", "Duohe Ma", "Tingwen Liu"], "title": "Beyond Surface Alignment: Rebuilding LLMs Safety Mechanism via Probabilistically Ablating Refusal Direction", "comment": "Accepted by EMNLP2025 Finding", "summary": "Jailbreak attacks pose persistent threats to large language models (LLMs).\nCurrent safety alignment methods have attempted to address these issues, but\nthey experience two significant limitations: insufficient safety alignment\ndepth and unrobust internal defense mechanisms. These limitations make them\nvulnerable to adversarial attacks such as prefilling and refusal direction\nmanipulation. We introduce DeepRefusal, a robust safety alignment framework\nthat overcomes these issues. DeepRefusal forces the model to dynamically\nrebuild its refusal mechanisms from jailbreak states. This is achieved by\nprobabilistically ablating the refusal direction across layers and token depths\nduring fine-tuning. Our method not only defends against prefilling and refusal\ndirection attacks but also demonstrates strong resilience against other unseen\njailbreak strategies. Extensive evaluations on four open-source LLM families\nand six representative attacks show that DeepRefusal reduces attack success\nrates by approximately 95%, while maintaining model capabilities with minimal\nperformance degradation."}
{"id": "2509.15150", "categories": ["cs.SE", "cs.PL", "D.2.6; D.3.3; D.2.3; D.2.5; D.2.13"], "pdf": "https://arxiv.org/pdf/2509.15150", "abs": "https://arxiv.org/abs/2509.15150", "authors": ["Federico Bruzzone", "Walter Cazzola", "Luca Favalli"], "title": "Code Less to Code More: Streamlining Language Server Protocol and Type System Development for Language Families", "comment": "34 pages, 10 figures, Journal of Systems and Software, June 2025, for\n  the replication package, see https://doi.org/10.5281/zenodo.15276991", "summary": "Developing editing support for $L$ languages in $E$ editors is complex and\ntime-consuming. Some languages do not provide dedicated editors, while others\noffer a single native editor. The $\\textit{language server protocol}$ (LSP)\nreduces the language-editor combinations $L \\times E$ to $L + E$, where a\nsingle language server communicates with editors via LSP plugins. However,\noverlapping implementations of linguistic components remain an issue. Existing\nlanguage workbenches struggle with modularity, reusability, and leveraging type\nsystems for language server generation. In this work, we propose: (i) Typelang,\na family of domain-specific languages for modular, composable, and reusable\ntype system implementation, (ii) a modular language server generation process,\nproducing servers for languages built in a modular workbench, (iii) the\nvariant-oriented programming paradigm and a cross-artifact coordination layer\nto manage interdependent software variants, and (iv) an LSP plugin generator,\nreducing $E$ to $1$ by automating plugin creation for multiple editors. To\nsimplify editing support for language families, each language artifact\nintegrates its own Typelang variant, used to generate language servers. This\nreduces combinations to $T \\times 1$, where $T = L$ represents the number of\ntype systems. Further reuse of language artifacts across languages lowers this\nto $N \\times 1$, where $N << T$, representing unique type systems. We implement\nTypelang in Neverlang, generating language servers for each artifact and LSP\nplugins for three editors. Empirical evaluation shows a 93.48% reduction in\ncharacters needed for type system implementation and 100% automation of LSP\nplugin generation, significantly lowering effort for editing support in\nlanguage families, especially when artifacts are reused."}
{"id": "2509.15217", "categories": ["cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.15217", "abs": "https://arxiv.org/abs/2509.15217", "authors": ["Yue Xin", "Wenyuan Wang", "Rui Pan", "Ruida Wang", "Howard Meng", "Renjie Pi", "Shizhe Diao", "Tong Zhang"], "title": "Generalizable Geometric Image Caption Synthesis", "comment": null, "summary": "Multimodal large language models have various practical applications that\ndemand strong reasoning abilities. Despite recent advancements, these models\nstill struggle to solve complex geometric problems. A key challenge stems from\nthe lack of high-quality image-text pair datasets for understanding geometric\nimages. Furthermore, most template-based data synthesis pipelines typically\nfail to generalize to questions beyond their predefined templates. In this\npaper, we bridge this gap by introducing a complementary process of\nReinforcement Learning with Verifiable Rewards (RLVR) into the data generation\npipeline. By adopting RLVR to refine captions for geometric images synthesized\nfrom 50 basic geometric relations and using reward signals derived from\nmathematical problem-solving tasks, our pipeline successfully captures the key\nfeatures of geometry problem-solving. This enables better task generalization\nand yields non-trivial improvements. Furthermore, even in out-of-distribution\nscenarios, the generated dataset enhances the general reasoning capabilities of\nmultimodal large language models, yielding accuracy improvements of\n$2.8\\%\\text{-}4.8\\%$ in statistics, arithmetic, algebraic, and numerical tasks\nwith non-geometric input images of MathVista and MathVerse, along with\n$2.4\\%\\text{-}3.9\\%$ improvements in Art, Design, Tech, and Engineering tasks\nin MMMU."}
{"id": "2509.15213", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.15213", "abs": "https://arxiv.org/abs/2509.15213", "authors": ["Yicheng Zhang", "Zijian Huang", "Sophie Chen", "Erfan Shayegani", "Jiasi Chen", "Nael Abu-Ghazaleh"], "title": "Evil Vizier: Vulnerabilities of LLM-Integrated XR Systems", "comment": null, "summary": "Extended reality (XR) applications increasingly integrate Large Language\nModels (LLMs) to enhance user experience, scene understanding, and even\ngenerate executable XR content, and are often called \"AI glasses\". Despite\nthese potential benefits, the integrated XR-LLM pipeline makes XR applications\nvulnerable to new forms of attacks. In this paper, we analyze LLM-Integated XR\nsystems in the literature and in practice and categorize them along different\ndimensions from a systems perspective. Building on this categorization, we\nidentify a common threat model and demonstrate a series of proof-of-concept\nattacks on multiple XR platforms that employ various LLM models (Meta Quest 3,\nMeta Ray-Ban, Android, and Microsoft HoloLens 2 running Llama and GPT models).\nAlthough these platforms each implement LLM integration differently, they share\nvulnerabilities where an attacker can modify the public context surrounding a\nlegitimate LLM query, resulting in erroneous visual or auditory feedback to\nusers, thus compromising their safety or privacy, sowing confusion, or other\nharmful effects. To defend against these threats, we discuss mitigation\nstrategies and best practices for developers, including an initial defense\nprototype, and call on the community to develop new protection mechanisms to\nmitigate these risks."}
{"id": "2509.15195", "categories": ["cs.SE", "cs.AI", "cs.CR", "D.4.6; I.2.2; D.2.5"], "pdf": "https://arxiv.org/pdf/2509.15195", "abs": "https://arxiv.org/abs/2509.15195", "authors": ["Max Bazalii", "Marius Fleischer"], "title": "Orion: Fuzzing Workflow Automation", "comment": "11 pages, 3 figures, 3 tables", "summary": "Fuzz testing is one of the most effective techniques for finding software\nvulnerabilities. While modern fuzzers can generate inputs and monitor\nexecutions automatically, the overall workflow, from analyzing a codebase, to\nconfiguring harnesses, to triaging results, still requires substantial manual\neffort. Prior attempts focused on single stages such as harness synthesis or\ninput minimization, leaving researchers to manually connect the pieces into a\ncomplete fuzzing campaign.\n  We introduce Orion, a framework that automates the the manual bottlenecks of\nfuzzing by integrating LLM reasoning with traditional tools, allowing campaigns\nto scale to settings where human effort alone was impractical. Orion uses LLMs\nfor code reasoning and semantic guidance, while relying on deterministic tools\nfor verification, iterative refinement, and tasks that require precision.\nAcross our benchmark suite, Orion reduces human effort by 46-204x depending on\nthe workflow stage, and we demonstrate its effectiveness through the discovery\nof two previously unknown vulnerabilities in the widely used open-source clib\nlibrary."}
{"id": "2509.14265", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14265", "abs": "https://arxiv.org/abs/2509.14265", "authors": ["Siyuan Chen", "Zhichao Lu", "Qingfu Zhang"], "title": "Evolution of Kernels: Automated RISC-V Kernel Optimization with Large Language Models", "comment": "Technical report", "summary": "Automated kernel design is critical for overcoming software ecosystem\nbarriers in emerging hardware platforms like RISC-V. While large language\nmodels (LLMs) have shown promise for automated kernel optimization,\ndemonstrating success in CUDA domains with comprehensive technical documents\nand mature codebases, their effectiveness remains unproven for reference-scarce\ndomains like RISC-V. We present Evolution of Kernels (EoK), a novel LLM-based\nevolutionary program search framework that automates kernel design for domains\nwith limited reference material. EoK mitigates reference scarcity by mining and\nformalizing reusable optimization ideas (general design principles + actionable\nthoughts) from established kernel libraries' development histories; it then\nguides parallel LLM explorations using these ideas, enriched via\nRetrieval-Augmented Generation (RAG) with RISC-V-specific context, prioritizing\nhistorically effective techniques. Empirically, EoK achieves a median 1.27x\nspeedup, surpassing human experts on all 80 evaluated kernel design tasks and\nimproving upon prior LLM-based automated kernel design methods by 20%. These\nresults underscore the viability of incorporating human experience into\nemerging domains and highlight the immense potential of LLM-based automated\nkernel optimization."}
{"id": "2509.15195", "categories": ["cs.SE", "cs.AI", "cs.CR", "D.4.6; I.2.2; D.2.5"], "pdf": "https://arxiv.org/pdf/2509.15195", "abs": "https://arxiv.org/abs/2509.15195", "authors": ["Max Bazalii", "Marius Fleischer"], "title": "Orion: Fuzzing Workflow Automation", "comment": "11 pages, 3 figures, 3 tables", "summary": "Fuzz testing is one of the most effective techniques for finding software\nvulnerabilities. While modern fuzzers can generate inputs and monitor\nexecutions automatically, the overall workflow, from analyzing a codebase, to\nconfiguring harnesses, to triaging results, still requires substantial manual\neffort. Prior attempts focused on single stages such as harness synthesis or\ninput minimization, leaving researchers to manually connect the pieces into a\ncomplete fuzzing campaign.\n  We introduce Orion, a framework that automates the the manual bottlenecks of\nfuzzing by integrating LLM reasoning with traditional tools, allowing campaigns\nto scale to settings where human effort alone was impractical. Orion uses LLMs\nfor code reasoning and semantic guidance, while relying on deterministic tools\nfor verification, iterative refinement, and tasks that require precision.\nAcross our benchmark suite, Orion reduces human effort by 46-204x depending on\nthe workflow stage, and we demonstrate its effectiveness through the discovery\nof two previously unknown vulnerabilities in the widely used open-source clib\nlibrary."}
{"id": "2509.14335", "categories": ["cs.CR", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.14335", "abs": "https://arxiv.org/abs/2509.14335", "authors": ["Xinran Zheng", "Xingzhi Qian", "Yiling He", "Shuo Yang", "Lorenzo Cavallaro"], "title": "Beyond Classification: Evaluating LLMs for Fine-Grained Automatic Malware Behavior Auditing", "comment": null, "summary": "Automated malware classification has achieved strong detection performance.\nYet, malware behavior auditing seeks causal and verifiable explanations of\nmalicious activities -- essential not only to reveal what malware does but also\nto substantiate such claims with evidence. This task is challenging, as\nadversarial intent is often hidden within complex, framework-heavy\napplications, making manual auditing slow and costly. Large Language Models\n(LLMs) could help address this gap, but their auditing potential remains\nlargely unexplored due to three limitations: (1) scarce fine-grained\nannotations for fair assessment; (2) abundant benign code obscuring malicious\nsignals; and (3) unverifiable, hallucination-prone outputs undermining\nattribution credibility. To close this gap, we introduce MalEval, a\ncomprehensive framework for fine-grained Android malware auditing, designed to\nevaluate how effectively LLMs support auditing under real-world constraints.\nMalEval provides expert-verified reports and an updated sensitive API list to\nmitigate ground truth scarcity and reduce noise via static reachability\nanalysis. Function-level structural representations serve as intermediate\nattribution units for verifiable evaluation. Building on this, we define four\nanalyst-aligned tasks -- function prioritization, evidence attribution,\nbehavior synthesis, and sample discrimination -- together with domain-specific\nmetrics and a unified workload-oriented score. We evaluate seven widely used\nLLMs on a curated dataset of recent malware and misclassified benign apps,\noffering the first systematic assessment of their auditing capabilities.\nMalEval reveals both promising potential and critical limitations across audit\nstages, providing a reproducible benchmark and foundation for future research\non LLM-enhanced malware behavior auditing. MalEval is publicly available at\nhttps://github.com/ZhengXR930/MalEval.git"}
{"id": "2509.14275", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14275", "abs": "https://arxiv.org/abs/2509.14275", "authors": ["Nobin Sarwar", "Shubhashis Roy Dipta"], "title": "FedMentor: Domain-Aware Differential Privacy for Heterogeneous Federated LLMs in Mental Health", "comment": "(e.g.: 18 pages, 6 figures, 6 tables)", "summary": "Privacy-preserving adaptation of Large Language Models (LLMs) in sensitive\ndomains (e.g., mental health) requires balancing strict confidentiality with\nmodel utility and safety. We propose FedMentor, a federated fine-tuning\nframework that integrates Low-Rank Adaptation (LoRA) and domain-aware\nDifferential Privacy (DP) to meet per-domain privacy budgets while maintaining\nperformance. Each client (domain) applies a custom DP noise scale proportional\nto its data sensitivity, and the server adaptively reduces noise when utility\nfalls below a threshold. In experiments on three mental health datasets, we\nshow that FedMentor improves safety over standard Federated Learning without\nprivacy, raising safe output rates by up to three points and lowering toxicity,\nwhile maintaining utility (BERTScore F1 and ROUGE-L) within 0.5% of the\nnon-private baseline and close to the centralized upper bound. The framework\nscales to backbones with up to 1.7B parameters on single-GPU clients, requiring\n< 173 MB of communication per round. FedMentor demonstrates a practical\napproach to privately fine-tune LLMs for safer deployments in healthcare and\nother sensitive fields."}
{"id": "2509.14278", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14278", "abs": "https://arxiv.org/abs/2509.14278", "authors": ["Yuntao Du", "Zitao Li", "Ninghui Li", "Bolin Ding"], "title": "Beyond Data Privacy: New Privacy Risks for Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) have achieved remarkable progress in natural\nlanguage understanding, reasoning, and autonomous decision-making. However,\nthese advancements have also come with significant privacy concerns. While\nsignificant research has focused on mitigating the data privacy risks of LLMs\nduring various stages of model training, less attention has been paid to new\nthreats emerging from their deployment. The integration of LLMs into widely\nused applications and the weaponization of their autonomous abilities have\ncreated new privacy vulnerabilities. These vulnerabilities provide\nopportunities for both inadvertent data leakage and malicious exfiltration from\nLLM-powered systems. Additionally, adversaries can exploit these systems to\nlaunch sophisticated, large-scale privacy attacks, threatening not only\nindividual privacy but also financial security and societal trust. In this\npaper, we systematically examine these emerging privacy risks of LLMs. We also\ndiscuss potential mitigation strategies and call for the research community to\nbroaden its focus beyond data privacy risks, developing new defenses to address\nthe evolving threats posed by increasingly powerful LLMs and LLM-powered\nsystems."}
{"id": "2509.14279", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14279", "abs": "https://arxiv.org/abs/2509.14279", "authors": ["Robert Tjarko Lange", "Qi Sun", "Aaditya Prasad", "Maxence Faldor", "Yujin Tang", "David Ha"], "title": "Towards Robust Agentic CUDA Kernel Benchmarking, Verification, and Optimization", "comment": "62 pages, 10 figures", "summary": "Recent advances in large language models (LLMs) demonstrate their\neffectiveness in scaling test-time compute for software engineering tasks.\nHowever, these approaches often focus on high-level solutions, with limited\nattention to optimizing low-level CUDA kernel implementations. Additionally,\nexisting kernel generation benchmarks suffer from exploitable loopholes and\ninsufficient diversity in testing conditions, hindering true generalization\nassessment. To address these limitations, we introduce robust-kbench, a new\nbenchmark for rigorous evaluation of kernel performance and correctness across\nvaried scenarios. Furthermore, we present a comprehensive agentic framework\nthat automates CUDA kernel discovery, verification, and optimization. This\npipeline enables frontier LLMs to translate torch code to CUDA kernels and\niteratively improve their runtime within our robust evaluation setting. Our\nsequential workflow first translates PyTorch code into equivalent CUDA kernels.\nIt then optimizes their runtime using a novel evolutionary meta-generation\nprocedure tailored to the CUDA ecosystem, guided by LLM-based verifiers for\ncorrectness and efficient filtering. Evaluated on robust-kbench, our approach\nproduces CUDA kernels outperforming torch implementations for practical\napplications, including forward and backward passes. It can fuse operations and\ndeploy various runtime optimization strategies. The verifier workflow\naccurately classifies incorrect kernels, enhancing hardware verification\nefficiency."}
{"id": "2509.14281", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14281", "abs": "https://arxiv.org/abs/2509.14281", "authors": ["Xifeng Yao", "Dongyu Lang", "Wu Zhang", "Xintong Guo", "Huarui Xie", "Yinhao Ni", "Ping Liu", "Guang Shen", "Yi Bai", "Dandan Tu", "Changzheng Zhang"], "title": "SCoGen: Scenario-Centric Graph-Based Synthesis of Real-World Code Problems", "comment": null, "summary": "Significant advancements have been made in the capabilities of code large\nlanguage models, leading to their rapid adoption and application across a wide\nrange of domains. However, their further advancements are often constrained by\nthe scarcity of real-world coding problems. To bridge this gap, we propose a\nnovel framework for synthesizing code problems that emulate authentic\nreal-world scenarios. This framework systematically integrates domain\nknowledge, domain skills, and coding skills, all of which are meticulously\nextracted from real-world programming-related datasets, including Stack\nOverflow and Kaggle. The extracted elements serve as the foundational building\nblocks for constructing code problems. To align the generated problems with\npractical applications, application scenarios are also mined from the\naforementioned datasets. These scenarios are then utilized to construct a\nscenario-centric graph that interconnects domain knowledge, domain skills, and\ncoding skills. Based on this structured representation, a sampling strategy on\nthe graph is designed, which effectively controls the generation of a code\nproblem with complexity and diversity, reflects real-world challenges.\nExperimental results demonstrate that the proposed method consistently achieves\nsuperior performance over state-of-the-art open-source large language models of\nvarying sizes and functionalities, including both coders and general-purpose\nmodels, across a diverse set of real-world benchmarks."}
{"id": "2509.14284", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.14284", "abs": "https://arxiv.org/abs/2509.14284", "authors": ["Vaidehi Patil", "Elias Stengel-Eskin", "Mohit Bansal"], "title": "The Sum Leaks More Than Its Parts: Compositional Privacy Risks and Mitigations in Multi-Agent Collaboration", "comment": "Code: https://github.com/Vaidehi99/MultiAgentPrivacy", "summary": "As large language models (LLMs) become integral to multi-agent systems, new\nprivacy risks emerge that extend beyond memorization, direct inference, or\nsingle-turn evaluations. In particular, seemingly innocuous responses, when\ncomposed across interactions, can cumulatively enable adversaries to recover\nsensitive information, a phenomenon we term compositional privacy leakage. We\npresent the first systematic study of such compositional privacy leaks and\npossible mitigation methods in multi-agent LLM systems. First, we develop a\nframework that models how auxiliary knowledge and agent interactions jointly\namplify privacy risks, even when each response is benign in isolation. Next, to\nmitigate this, we propose and evaluate two defense strategies: (1)\nTheory-of-Mind defense (ToM), where defender agents infer a questioner's intent\nby anticipating how their outputs may be exploited by adversaries, and (2)\nCollaborative Consensus Defense (CoDef), where responder agents collaborate\nwith peers who vote based on a shared aggregated state to restrict sensitive\ninformation spread. Crucially, we balance our evaluation across compositions\nthat expose sensitive information and compositions that yield benign\ninferences. Our experiments quantify how these defense strategies differ in\nbalancing the privacy-utility trade-off. We find that while chain-of-thought\nalone offers limited protection to leakage (~39% sensitive blocking rate), our\nToM defense substantially improves sensitive query blocking (up to 97%) but can\nreduce benign task success. CoDef achieves the best balance, yielding the\nhighest Balanced Outcome (79.8%), highlighting the benefit of combining\nexplicit reasoning with defender collaboration. Together, our results expose a\nnew class of risks in collaborative LLM deployments and provide actionable\ninsights for designing safeguards against compositional, context-driven privacy\nleakage."}
{"id": "2509.14335", "categories": ["cs.CR", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.14335", "abs": "https://arxiv.org/abs/2509.14335", "authors": ["Xinran Zheng", "Xingzhi Qian", "Yiling He", "Shuo Yang", "Lorenzo Cavallaro"], "title": "Beyond Classification: Evaluating LLMs for Fine-Grained Automatic Malware Behavior Auditing", "comment": null, "summary": "Automated malware classification has achieved strong detection performance.\nYet, malware behavior auditing seeks causal and verifiable explanations of\nmalicious activities -- essential not only to reveal what malware does but also\nto substantiate such claims with evidence. This task is challenging, as\nadversarial intent is often hidden within complex, framework-heavy\napplications, making manual auditing slow and costly. Large Language Models\n(LLMs) could help address this gap, but their auditing potential remains\nlargely unexplored due to three limitations: (1) scarce fine-grained\nannotations for fair assessment; (2) abundant benign code obscuring malicious\nsignals; and (3) unverifiable, hallucination-prone outputs undermining\nattribution credibility. To close this gap, we introduce MalEval, a\ncomprehensive framework for fine-grained Android malware auditing, designed to\nevaluate how effectively LLMs support auditing under real-world constraints.\nMalEval provides expert-verified reports and an updated sensitive API list to\nmitigate ground truth scarcity and reduce noise via static reachability\nanalysis. Function-level structural representations serve as intermediate\nattribution units for verifiable evaluation. Building on this, we define four\nanalyst-aligned tasks -- function prioritization, evidence attribution,\nbehavior synthesis, and sample discrimination -- together with domain-specific\nmetrics and a unified workload-oriented score. We evaluate seven widely used\nLLMs on a curated dataset of recent malware and misclassified benign apps,\noffering the first systematic assessment of their auditing capabilities.\nMalEval reveals both promising potential and critical limitations across audit\nstages, providing a reproducible benchmark and foundation for future research\non LLM-enhanced malware behavior auditing. MalEval is publicly available at\nhttps://github.com/ZhengXR930/MalEval.git"}
{"id": "2509.14404", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.PL"], "pdf": "https://arxiv.org/pdf/2509.14404", "abs": "https://arxiv.org/abs/2509.14404", "authors": ["Haoye Tian", "Chong Wang", "BoYang Yang", "Lyuye Zhang", "Yang Liu"], "title": "A Taxonomy of Prompt Defects in LLM Systems", "comment": null, "summary": "Large Language Models (LLMs) have become key components of modern software,\nwith prompts acting as their de-facto programming interface. However, prompt\ndesign remains largely empirical and small mistakes can cascade into\nunreliable, insecure, or inefficient behavior. This paper presents the first\nsystematic survey and taxonomy of prompt defects, recurring ways that prompts\nfail to elicit their intended behavior from LLMs. We organize defects along six\ndimensions: (1) Specification and Intent, (2) Input and Content, (3) Structure\nand Formatting, (4) Context and Memory, (5) Performance and Efficiency, and (6)\nMaintainability and Engineering. Each dimension is refined into fine-grained\nsubtypes, illustrated with concrete examples and root cause analysis. Grounded\nin software engineering principles, we show how these defects surface in real\ndevelopment workflows and examine their downstream effects. For every subtype,\nwe distill mitigation strategies that span emerging prompt engineering\npatterns, automated guardrails, testing harnesses, and evaluation frameworks.\nWe then summarize these strategies in a master taxonomy that links defect,\nimpact, and remedy. We conclude with open research challenges and a call for\nrigorous engineering-oriented methodologies to ensure that LLM-driven systems\nare dependable by design."}
{"id": "2509.14558", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.14558", "abs": "https://arxiv.org/abs/2509.14558", "authors": ["Guorui Chen", "Yifan Xia", "Xiaojun Jia", "Zhijiang Li", "Philip Torr", "Jindong Gu"], "title": "LLM Jailbreak Detection for (Almost) Free!", "comment": null, "summary": "Large language models (LLMs) enhance security through alignment when widely\nused, but remain susceptible to jailbreak attacks capable of producing\ninappropriate content. Jailbreak detection methods show promise in mitigating\njailbreak attacks through the assistance of other models or multiple model\ninferences. However, existing methods entail significant computational costs.\nIn this paper, we first present a finding that the difference in output\ndistributions between jailbreak and benign prompts can be employed for\ndetecting jailbreak prompts. Based on this finding, we propose a Free Jailbreak\nDetection (FJD) which prepends an affirmative instruction to the input and\nscales the logits by temperature to further distinguish between jailbreak and\nbenign prompts through the confidence of the first token. Furthermore, we\nenhance the detection performance of FJD through the integration of virtual\ninstruction learning. Extensive experiments on aligned LLMs show that our FJD\ncan effectively detect jailbreak prompts with almost no additional\ncomputational costs during LLM inference."}
{"id": "2509.14589", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14589", "abs": "https://arxiv.org/abs/2509.14589", "authors": ["Taesoo Kim", "HyungSeok Han", "Soyeon Park", "Dae R. Jeong", "Dohyeok Kim", "Dongkwan Kim", "Eunsoo Kim", "Jiho Kim", "Joshua Wang", "Kangsu Kim", "Sangwoo Ji", "Woosun Song", "Hanqing Zhao", "Andrew Chin", "Gyejin Lee", "Kevin Stevens", "Mansour Alharthi", "Yizhuo Zhai", "Cen Zhang", "Joonun Jang", "Yeongjin Jang", "Ammar Askar", "Dongju Kim", "Fabian Fleischer", "Jeongin Cho", "Junsik Kim", "Kyungjoon Ko", "Insu Yun", "Sangdon Park", "Dowoo Baik", "Haein Lee", "Hyeon Heo", "Minjae Gwon", "Minjae Lee", "Minwoo Baek", "Seunggi Min", "Wonyoung Kim", "Yonghwi Jin", "Younggi Park", "Yunjae Choi", "Jinho Jung", "Gwanhyun Lee", "Junyoung Jang", "Kyuheon Kim", "Yeonghyeon Cha", "Youngjoon Kim"], "title": "ATLANTIS: AI-driven Threat Localization, Analysis, and Triage Intelligence System", "comment": "Version 1.0 (September 17, 2025). Technical Report. Team Atlanta --\n  1st place in DARPA AIxCC Final Competition. Project page:\n  https://team-atlanta.github.io/", "summary": "We present ATLANTIS, the cyber reasoning system developed by Team Atlanta\nthat won 1st place in the Final Competition of DARPA's AI Cyber Challenge\n(AIxCC) at DEF CON 33 (August 2025). AIxCC (2023-2025) challenged teams to\nbuild autonomous cyber reasoning systems capable of discovering and patching\nvulnerabilities at the speed and scale of modern software. ATLANTIS integrates\nlarge language models (LLMs) with program analysis -- combining symbolic\nexecution, directed fuzzing, and static analysis -- to address limitations in\nautomated vulnerability discovery and program repair. Developed by researchers\nat Georgia Institute of Technology, Samsung Research, KAIST, and POSTECH, the\nsystem addresses core challenges: scaling across diverse codebases from C to\nJava, achieving high precision while maintaining broad coverage, and producing\nsemantically correct patches that preserve intended behavior. We detail the\ndesign philosophy, architectural decisions, and implementation strategies\nbehind ATLANTIS, share lessons learned from pushing the boundaries of automated\nsecurity when program analysis meets modern AI, and release artifacts to\nsupport reproducibility and future research."}
{"id": "2509.14608", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14608", "abs": "https://arxiv.org/abs/2509.14608", "authors": ["Shashank Shreedhar Bhatt", "Tanmay Rajore", "Khushboo Aggarwal", "Ganesh Ananthanarayanan", "Ranveer Chandra", "Nishanth Chandran", "Suyash Choudhury", "Divya Gupta", "Emre Kiciman", "Sumit Kumar Pandey", "Srinath Setty", "Rahul Sharma", "Teijia Zhao"], "title": "Enterprise AI Must Enforce Participant-Aware Access Control", "comment": null, "summary": "Large language models (LLMs) are increasingly deployed in enterprise settings\nwhere they interact with multiple users and are trained or fine-tuned on\nsensitive internal data. While fine-tuning enhances performance by\ninternalizing domain knowledge, it also introduces a critical security risk:\nleakage of confidential training data to unauthorized users. These risks are\nexacerbated when LLMs are combined with Retrieval-Augmented Generation (RAG)\npipelines that dynamically fetch contextual documents at inference time.\n  We demonstrate data exfiltration attacks on AI assistants where adversaries\ncan exploit current fine-tuning and RAG architectures to leak sensitive\ninformation by leveraging the lack of access control enforcement. We show that\nexisting defenses, including prompt sanitization, output filtering, system\nisolation, and training-level privacy mechanisms, are fundamentally\nprobabilistic and fail to offer robust protection against such attacks.\n  We take the position that only a deterministic and rigorous enforcement of\nfine-grained access control during both fine-tuning and RAG-based inference can\nreliably prevent the leakage of sensitive data to unauthorized recipients.\n  We introduce a framework centered on the principle that any content used in\ntraining, retrieval, or generation by an LLM is explicitly authorized for\n\\emph{all users involved in the interaction}. Our approach offers a simple yet\npowerful paradigm shift for building secure multi-user LLM systems that are\ngrounded in classical access control but adapted to the unique challenges of\nmodern AI workflows. Our solution has been deployed in Microsoft Copilot\nTuning, a product offering that enables organizations to fine-tune models using\ntheir own enterprise-specific data."}
{"id": "2509.14622", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14622", "abs": "https://arxiv.org/abs/2509.14622", "authors": ["Yihao Guo", "Haocheng Bian", "Liutong Zhou", "Ze Wang", "Zhaoyi Zhang", "Francois Kawala", "Milan Dean", "Ian Fischer", "Yuantao Peng", "Noyan Tokgozoglu", "Ivan Barrientos", "Riyaaz Shaik", "Rachel Li", "Chandru Venkataraman", "Reza Shifteh Far", "Moses Pawar", "Venkat Sundaranatha", "Michael Xu", "Frank Chu"], "title": "Adversarial Distilled Retrieval-Augmented Guarding Model for Online Malicious Intent Detection", "comment": null, "summary": "With the deployment of Large Language Models (LLMs) in interactive\napplications, online malicious intent detection has become increasingly\ncritical. However, existing approaches fall short of handling diverse and\ncomplex user queries in real time. To address these challenges, we introduce\nADRAG (Adversarial Distilled Retrieval-Augmented Guard), a two-stage framework\nfor robust and efficient online malicious intent detection. In the training\nstage, a high-capacity teacher model is trained on adversarially perturbed,\nretrieval-augmented inputs to learn robust decision boundaries over diverse and\ncomplex user queries. In the inference stage, a distillation scheduler\ntransfers the teacher's knowledge into a compact student model, with a\ncontinually updated knowledge base collected online. At deployment, the compact\nstudent model leverages top-K similar safety exemplars retrieved from the\nonline-updated knowledge base to enable both online and real-time malicious\nquery detection. Evaluations across ten safety benchmarks demonstrate that\nADRAG, with a 149M-parameter model, achieves 98.5% of WildGuard-7B's\nperformance, surpasses GPT-4 by 3.3% and Llama-Guard-3-8B by 9.5% on\nout-of-distribution detection, while simultaneously delivering up to 5.6x lower\nlatency at 300 queries per second (QPS) in real-time applications."}
{"id": "2509.14623", "categories": ["cs.SE", "cs.AI", "cs.PL", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.14623", "abs": "https://arxiv.org/abs/2509.14623", "authors": ["Hanlong Wan", "Xing Lu", "Yan Chen", "Karthik Devaprasad", "Laura Hinkle"], "title": "Automating Modelica Module Generation Using Large Language Models: A Case Study on Building Control Description Language", "comment": "This is the pre-peer-review version of a journal paper; the repo is\n  available at: https://github.com/pnnl/prompt2control", "summary": "Dynamic energy systems and controls require advanced modeling frameworks to\ndesign and test supervisory and fault tolerant strategies. Modelica is a widely\nused equation based language, but developing control modules is labor intensive\nand requires specialized expertise. This paper examines the use of large\nlanguage models (LLMs) to automate the generation of Control Description\nLanguage modules in the Building Modelica Library as a case study. We developed\na structured workflow that combines standardized prompt scaffolds, library\naware grounding, automated compilation with OpenModelica, and human in the loop\nevaluation. Experiments were carried out on four basic logic tasks (And, Or,\nNot, and Switch) and five control modules (chiller enable/disable, bypass valve\ncontrol, cooling tower fan speed, plant requests, and relief damper control).\nThe results showed that GPT 4o failed to produce executable Modelica code in\nzero shot mode, while Claude Sonnet 4 achieved up to full success for basic\nlogic blocks with carefully engineered prompts. For control modules, success\nrates reached 83 percent, and failed outputs required medium level human repair\n(estimated one to eight hours). Retrieval augmented generation often produced\nmismatches in module selection (for example, And retrieved as Or), while a\ndeterministic hard rule search strategy avoided these errors. Human evaluation\nalso outperformed AI evaluation, since current LLMs cannot assess simulation\nresults or validate behavioral correctness. Despite these limitations, the LLM\nassisted workflow reduced the average development time from 10 to 20 hours down\nto 4 to 6 hours per module, corresponding to 40 to 60 percent time savings.\nThese results highlight both the potential and current limitations of LLM\nassisted Modelica generation, and point to future research in pre simulation\nvalidation, stronger grounding, and closed loop evaluation."}
{"id": "2509.14657", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14657", "abs": "https://arxiv.org/abs/2509.14657", "authors": ["Sergio Benlloch-Lopez", "Miquel Viel-Vazquez", "Javier Naranjo-Alcazar", "Jordi Grau-Haro", "Pedro Zuccarello"], "title": "Threat Modeling for Enhancing Security of IoT Audio Classification Devices under a Secure Protocols Framework", "comment": "Accepted at Computing Conference 2026, London, UK", "summary": "The rapid proliferation of IoT nodes equipped with microphones and capable of\nperforming on-device audio classification exposes highly sensitive data while\noperating under tight resource constraints. To protect against this, we present\na defence-in-depth architecture comprising a security protocol that treats the\nedge device, cellular network and cloud backend as three separate trust\ndomains, linked by TPM-based remote attestation and mutually authenticated TLS\n1.3. A STRIDE-driven threat model and attack-tree analysis guide the design. At\nstartup, each boot stage is measured into TPM PCRs. The node can only decrypt\nits LUKS-sealed partitions after the cloud has verified a TPM quote and\nreleased a one-time unlock key. This ensures that rogue or tampered devices\nremain inert. Data in transit is protected by TLS 1.3 and hybridised with Kyber\nand Dilithium to provide post-quantum resilience. Meanwhile, end-to-end\nencryption and integrity hashes safeguard extracted audio features. Signed,\nrollback-protected AI models and tamper-responsive sensors harden firmware and\nhardware. Data at rest follows a 3-2-1 strategy comprising a solid-state drive\nsealed with LUKS, an offline cold archive encrypted with a hybrid post-quantum\ncipher and an encrypted cloud replica. Finally, we set out a plan for\nevaluating the physical and logical security of the proposed protocol."}
{"id": "2509.14987", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14987", "abs": "https://arxiv.org/abs/2509.14987", "authors": ["Md Talha Mohsin"], "title": "Blockchain-Enabled Explainable AI for Trusted Healthcare Systems", "comment": "6 Pages, 4 Figures", "summary": "This paper introduces a Blockchain-Integrated Explainable AI Framework (BXHF)\nfor healthcare systems to tackle two essential challenges confronting health\ninformation networks: safe data exchange and comprehensible AI-driven clinical\ndecision-making. Our architecture incorporates blockchain, ensuring patient\nrecords are immutable, auditable, and tamper-proof, alongside Explainable AI\n(XAI) methodologies that yield transparent and clinically relevant model\npredictions. By incorporating security assurances and interpretability\nrequirements into a unified optimization pipeline, BXHF ensures both data-level\ntrust (by verified and encrypted record sharing) and decision-level trust (with\nauditable and clinically aligned explanations). Its hybrid edge-cloud\narchitecture allows for federated computation across different institutions,\nenabling collaborative analytics while protecting patient privacy. We\ndemonstrate the framework's applicability through use cases such as\ncross-border clinical research networks, uncommon illness detection and\nhigh-risk intervention decision support. By ensuring transparency,\nauditability, and regulatory compliance, BXHF improves the credibility, uptake,\nand effectiveness of AI in healthcare, laying the groundwork for safer and more\nreliable clinical decision-making."}
{"id": "2509.15170", "categories": ["cs.CR", "cs.AI", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.15170", "abs": "https://arxiv.org/abs/2509.15170", "authors": ["Aarushi Mahajan", "Wayne Burleson"], "title": "Watermarking and Anomaly Detection in Machine Learning Models for LORA RF Fingerprinting", "comment": "IEEE International Conference on Acoustics, Speech, and Signal\n  Processing (ICASSP)", "summary": "Radio frequency fingerprint identification (RFFI) distinguishes wireless\ndevices by the small variations in their analog circuits, avoiding heavy\ncryptographic authentication. While deep learning on spectrograms improves\naccuracy, models remain vulnerable to copying, tampering, and evasion. We\npresent a stronger RFFI system combining watermarking for ownership proof and\nanomaly detection for spotting suspicious inputs. Using a ResNet-34 on log-Mel\nspectrograms, we embed three watermarks: a simple trigger, an adversarially\ntrained trigger robust to noise and filtering, and a hidden gradient/weight\nsignature. A convolutional Variational Autoencoders (VAE) with Kullback-Leibler\n(KL) warm-up and free-bits flags off-distribution queries. On the LoRa dataset,\nour system achieves 94.6% accuracy, 98% watermark success, and 0.94 AUROC,\noffering verifiable, tamper-resistant authentication."}
{"id": "2509.15195", "categories": ["cs.SE", "cs.AI", "cs.CR", "D.4.6; I.2.2; D.2.5"], "pdf": "https://arxiv.org/pdf/2509.15195", "abs": "https://arxiv.org/abs/2509.15195", "authors": ["Max Bazalii", "Marius Fleischer"], "title": "Orion: Fuzzing Workflow Automation", "comment": "11 pages, 3 figures, 3 tables", "summary": "Fuzz testing is one of the most effective techniques for finding software\nvulnerabilities. While modern fuzzers can generate inputs and monitor\nexecutions automatically, the overall workflow, from analyzing a codebase, to\nconfiguring harnesses, to triaging results, still requires substantial manual\neffort. Prior attempts focused on single stages such as harness synthesis or\ninput minimization, leaving researchers to manually connect the pieces into a\ncomplete fuzzing campaign.\n  We introduce Orion, a framework that automates the the manual bottlenecks of\nfuzzing by integrating LLM reasoning with traditional tools, allowing campaigns\nto scale to settings where human effort alone was impractical. Orion uses LLMs\nfor code reasoning and semantic guidance, while relying on deterministic tools\nfor verification, iterative refinement, and tasks that require precision.\nAcross our benchmark suite, Orion reduces human effort by 46-204x depending on\nthe workflow stage, and we demonstrate its effectiveness through the discovery\nof two previously unknown vulnerabilities in the widely used open-source clib\nlibrary."}
