{"id": "2507.08061", "categories": ["cs.SE", "physics.soc-ph"], "pdf": "https://arxiv.org/pdf/2507.08061", "abs": "https://arxiv.org/abs/2507.08061", "authors": ["Andrea Morales Coto", "Aditi Verma"], "title": "The State of Computational Science in Fission and Fusion Energy", "comment": null, "summary": "The tools used to engineer something are just as important as the thing that\nis actually being engineered. In fact, in many cases, the tools can indeed\ndetermine what is engineerable. In fusion and fission1 energy engineering,\nsoftware has become the dominant tool for design. For that reason, in 2024, for\nthe first time ever, we asked 103 computational scientists developing the codes\nused in fusion and fission energy about the problems they are attempting to\nsolve with their codes, the tools available to them to solve them, and their\nend to end developer experience with said tools.\n  The results revealed a changing tide in software tools in fusion and fission,\nwith more and more computational scientists preferring modern programming\nlanguages, open-source codes, and modular software. These trends represent a\npeek into what will happen 5 to 10 years in the future of nuclear engineering.\nSince the majority of our respondents belonged to US national labs and\nuniversities, these results hint at the most cutting-edge trends in the\nindustry. The insights included in the State of Computational Science in\nFission and Fusion Energy indicate a dramatic shift toward multiphysics codes,\na drop-off in the use of FORTRAN in favor of more modern languages like Python\nand C++, and ever-rising budgets for code development, at times reaching $50M\nin a single organization.\n  Our survey paints a future of nuclear engineering codes that is modular in\nnature, small in terms of compute, and increasingly prioritized by\norganizations. Access to our results in web form are available online.", "AI": {"tldr": "\u8c03\u67e5\u663e\u793a\uff0c\u6838\u5de5\u7a0b\u9886\u57df\u7684\u8ba1\u7b97\u79d1\u5b66\u5bb6\u504f\u597d\u73b0\u4ee3\u7f16\u7a0b\u8bed\u8a00\u3001\u5f00\u6e90\u4ee3\u7801\u548c\u6a21\u5757\u5316\u8f6f\u4ef6\uff0c\u9884\u793a\u672a\u67655-10\u5e74\u8f6f\u4ef6\u5de5\u5177\u5c06\u5411\u591a\u7269\u7406\u573a\u4ee3\u7801\u3001Python\u548cC++\u7b49\u8bed\u8a00\u8f6c\u53d8\u3002", "motivation": "\u7814\u7a76\u6838\u5de5\u7a0b\u4e2d\u8f6f\u4ef6\u5de5\u5177\u7684\u4f7f\u7528\u8d8b\u52bf\uff0c\u4ee5\u4e86\u89e3\u672a\u6765\u53d1\u5c55\u65b9\u5411\u3002", "method": "\u5bf9103\u4f4d\u4ece\u4e8b\u6838\u805a\u53d8\u548c\u6838\u88c2\u53d8\u80fd\u6e90\u4ee3\u7801\u5f00\u53d1\u7684\u79d1\u5b66\u5bb6\u8fdb\u884c\u8c03\u67e5\uff0c\u5206\u6790\u5176\u5de5\u5177\u504f\u597d\u548c\u5f00\u53d1\u4f53\u9a8c\u3002", "result": "\u53d1\u73b0\u73b0\u4ee3\u7f16\u7a0b\u8bed\u8a00\uff08\u5982Python\u3001C++\uff09\u548c\u5f00\u6e90\u4ee3\u7801\u7684\u666e\u53ca\uff0cFORTRAN\u4f7f\u7528\u51cf\u5c11\uff0c\u4ee3\u7801\u5f00\u53d1\u9884\u7b97\u589e\u52a0\u3002", "conclusion": "\u6838\u5de5\u7a0b\u8f6f\u4ef6\u672a\u6765\u5c06\u8d8b\u5411\u6a21\u5757\u5316\u3001\u8f7b\u91cf\u5316\uff0c\u5e76\u66f4\u53d7\u7ec4\u7ec7\u91cd\u89c6\u3002"}}
{"id": "2507.08149", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.08149", "abs": "https://arxiv.org/abs/2507.08149", "authors": ["Valerie Chen", "Ameet Talwalkar", "Robert Brennan", "Graham Neubig"], "title": "Code with Me or for Me? How Increasing AI Automation Transforms Developer Workflows", "comment": null, "summary": "Developers now have access to a growing array of increasingly autonomous AI\ntools to support software development. While numerous studies have examined\ndeveloper use of copilots, which can provide chat assistance or code\ncompletions, evaluations of coding agents, which can automatically write files\nand run code, still largely rely on static benchmarks without\nhumans-in-the-loop. In this work, we conduct the first academic study to\nexplore developer interactions with coding agents and characterize how more\nautonomous AI tools affect user productivity and experience, compared to\nexisting copilots. We evaluate two leading copilot and agentic coding\nassistants, GitHub Copilot and OpenHands, recruiting participants who regularly\nuse the former. Our results show agents have the potential to assist developers\nin ways that surpass copilots (e.g., completing tasks that humans might not\nhave accomplished before) and reduce the user effort required to complete\ntasks. However, there are challenges involved in enabling their broader\nadoption, including how to ensure users have an adequate understanding of agent\nbehaviors. Our results not only provide insights into how developer workflows\nchange as a result of coding agents but also highlight how user interactions\nwith agents differ from those with existing copilots, motivating a set of\nrecommendations for researchers building new agents. Given the broad set of\ndevelopers who still largely rely on copilot-like systems, our work highlights\nkey challenges of adopting more agentic systems into developer workflows.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5f00\u53d1\u8005\u4e0e\u81ea\u52a8\u5316\u7f16\u7801\u4ee3\u7406\u7684\u4e92\u52a8\uff0c\u53d1\u73b0\u5176\u6f5c\u529b\u8d85\u8fc7\u73b0\u6709Copilot\u5de5\u5177\uff0c\u4f46\u9700\u89e3\u51b3\u7528\u6237\u7406\u89e3\u4ee3\u7406\u884c\u4e3a\u7b49\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8Copilot\u5de5\u5177\uff0c\u800c\u5bf9\u66f4\u81ea\u52a8\u5316\u7684\u7f16\u7801\u4ee3\u7406\u8bc4\u4f30\u4e0d\u8db3\uff0c\u9700\u63a2\u7d22\u5176\u5bf9\u5f00\u53d1\u8005\u751f\u4ea7\u529b\u548c\u4f53\u9a8c\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u6bd4\u8f83GitHub Copilot\u548cOpenHands\u4e24\u79cd\u5de5\u5177\uff0c\u62db\u52df\u5e38\u7528Copilot\u7684\u5f00\u53d1\u8005\u53c2\u4e0e\u5b9e\u9a8c\u3002", "result": "\u7f16\u7801\u4ee3\u7406\u80fd\u5b8c\u6210Copilot\u65e0\u6cd5\u5b8c\u6210\u7684\u4efb\u52a1\u5e76\u51cf\u5c11\u7528\u6237\u52aa\u529b\uff0c\u4f46\u9700\u89e3\u51b3\u7528\u6237\u7406\u89e3\u4ee3\u7406\u884c\u4e3a\u7684\u6311\u6218\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u7f16\u7801\u4ee3\u7406\u5bf9\u5f00\u53d1\u8005\u5de5\u4f5c\u6d41\u7684\u6539\u53d8\uff0c\u5e76\u63d0\u51fa\u4e86\u6784\u5efa\u65b0\u4ee3\u7406\u7684\u5efa\u8bae\uff0c\u540c\u65f6\u6307\u51fa\u63a8\u5e7f\u4ee3\u7406\u5de5\u5177\u7684\u6311\u6218\u3002"}}
{"id": "2507.08160", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.08160", "abs": "https://arxiv.org/abs/2507.08160", "authors": ["Ot\u00e1vio Cury", "Guilherme Avelino"], "title": "The Impact of Generative AI on Code Expertise Models: An Exploratory Study", "comment": null, "summary": "Generative Artificial Intelligence (GenAI) tools for source code generation\nhave significantly boosted productivity in software development. However, they\nalso raise concerns, particularly the risk that developers may rely heavily on\nthese tools, reducing their understanding of the generated code. We hypothesize\nthat this loss of understanding may be reflected in source code knowledge\nmodels, which are used to identify developer expertise. In this work, we\npresent an exploratory analysis of how a knowledge model and a Truck Factor\nalgorithm built upon it can be affected by GenAI usage. To investigate this, we\ncollected statistical data on the integration of ChatGPT-generated code into\nGitHub projects and simulated various scenarios by adjusting the degree of\nGenAI contribution. Our findings reveal that most scenarios led to measurable\nimpacts, indicating the sensitivity of current expertise metrics. This suggests\nthat as GenAI becomes more integrated into development workflows, the\nreliability of such metrics may decrease.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u751f\u6210\u5f0fAI\u5de5\u5177\u5728\u4ee3\u7801\u751f\u6210\u4e2d\u5bf9\u5f00\u53d1\u8005\u7406\u89e3\u4ee3\u7801\u7684\u5f71\u54cd\uff0c\u5e76\u5206\u6790\u4e86\u5176\u5bf9\u6e90\u4ee3\u7801\u77e5\u8bc6\u6a21\u578b\u548cTruck Factor\u7b97\u6cd5\u7684\u6f5c\u5728\u5f71\u54cd\u3002", "motivation": "\u7814\u7a76\u751f\u6210\u5f0fAI\u5de5\u5177\uff08\u5982ChatGPT\uff09\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u5e7f\u6cdb\u4f7f\u7528\u662f\u5426\u4f1a\u5bfc\u81f4\u5f00\u53d1\u8005\u5bf9\u751f\u6210\u4ee3\u7801\u7684\u7406\u89e3\u964d\u4f4e\uff0c\u8fdb\u800c\u5f71\u54cd\u57fa\u4e8e\u4ee3\u7801\u77e5\u8bc6\u6a21\u578b\u7684\u5f00\u53d1\u8005\u4e13\u4e1a\u5ea6\u8bc4\u4f30\u3002", "method": "\u901a\u8fc7\u6536\u96c6GitHub\u9879\u76ee\u4e2dChatGPT\u751f\u6210\u4ee3\u7801\u7684\u7edf\u8ba1\u6570\u636e\uff0c\u5e76\u6a21\u62df\u4e0d\u540cGenAI\u8d21\u732e\u7a0b\u5ea6\u7684\u573a\u666f\uff0c\u5206\u6790\u5176\u5bf9\u77e5\u8bc6\u6a21\u578b\u548cTruck Factor\u7b97\u6cd5\u7684\u5f71\u54cd\u3002", "result": "\u5927\u591a\u6570\u6a21\u62df\u573a\u666f\u663e\u793a\uff0cGenAI\u7684\u4f7f\u7528\u5bf9\u5f53\u524d\u7684\u4e13\u4e1a\u5ea6\u8bc4\u4f30\u6307\u6807\u4ea7\u751f\u4e86\u53ef\u6d4b\u91cf\u7684\u5f71\u54cd\uff0c\u8868\u660e\u8fd9\u4e9b\u6307\u6807\u53ef\u80fd\u56e0GenAI\u7684\u96c6\u6210\u800c\u53d8\u5f97\u4e0d\u53ef\u9760\u3002", "conclusion": "\u968f\u7740\u751f\u6210\u5f0fAI\u5728\u5f00\u53d1\u6d41\u7a0b\u4e2d\u7684\u6df1\u5165\u5e94\u7528\uff0c\u73b0\u6709\u7684\u5f00\u53d1\u8005\u4e13\u4e1a\u5ea6\u8bc4\u4f30\u6307\u6807\u7684\u53ef\u9760\u6027\u53ef\u80fd\u4e0b\u964d\uff0c\u9700\u8981\u91cd\u65b0\u5ba1\u89c6\u8fd9\u4e9b\u8bc4\u4f30\u65b9\u6cd5\u3002"}}
{"id": "2507.08250", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.08250", "abs": "https://arxiv.org/abs/2507.08250", "authors": ["Yasaman Abedini", "Abbas Heydarnoori"], "title": "Leveraging Large Language Models for Classifying App Users' Feedback", "comment": null, "summary": "In recent years, significant research has been conducted into classifying\napplication (app) user feedback, primarily relying on supervised machine\nlearning algorithms. However, fine-tuning more generalizable classifiers based\non existing labeled datasets remains an important challenge, as creating large\nand accurately labeled datasets often requires considerable time and resources.\nIn this paper, we evaluate the capabilities of four advanced LLMs, including\nGPT-3.5-Turbo, GPT-4, Flan-T5, and Llama3-70b, to enhance user feedback\nclassification and address the challenge of the limited labeled dataset. To\nachieve this, we conduct several experiments on eight datasets that have been\nmeticulously labeled in prior research. These datasets include user reviews\nfrom app stores, posts from the X platform, and discussions from the public\nforums, widely recognized as representative sources of app user feedback. We\nanalyze the performance of various LLMs in identifying both fine-grained and\ncoarse-grained user feedback categories. Given the substantial volume of daily\nuser feedback and the computational limitations of LLMs, we leverage these\nmodels as an annotation tool to augment labeled datasets with general and\napp-specific data. This augmentation aims to enhance the performance of\nstate-of-the-art BERT-based classification models. Our findings indicate that\nLLMs when guided by well-crafted prompts, can effectively classify user\nfeedback into coarse-grained categories. Moreover, augmenting the training\ndataset with datasets labeled using the consensus of LLMs can significantly\nenhance classifier performance.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u56db\u79cd\u5148\u8fdbLLM\uff08GPT-3.5-Turbo\u3001GPT-4\u3001Flan-T5\u548cLlama3-70b\uff09\u5728\u7528\u6237\u53cd\u9988\u5206\u7c7b\u4e2d\u7684\u5e94\u7528\uff0c\u4ee5\u89e3\u51b3\u6807\u6ce8\u6570\u636e\u4e0d\u8db3\u7684\u95ee\u9898\u3002\u5b9e\u9a8c\u8868\u660e\uff0cLLM\u80fd\u6709\u6548\u5206\u7c7b\u7c97\u7c92\u5ea6\u53cd\u9988\uff0c\u5e76\u901a\u8fc7\u6807\u6ce8\u6570\u636e\u589e\u5f3a\u63d0\u5347BERT\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u5927\u91cf\u6807\u6ce8\u6570\u636e\uff0c\u4f46\u6807\u6ce8\u6210\u672c\u9ad8\u3002\u7814\u7a76\u65e8\u5728\u5229\u7528LLM\u63d0\u5347\u7528\u6237\u53cd\u9988\u5206\u7c7b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u5728\u516b\u4e2a\u6807\u6ce8\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u56db\u79cdLLM\u7684\u5206\u7c7b\u80fd\u529b\uff0c\u5e76\u5c06\u5176\u4f5c\u4e3a\u6807\u6ce8\u5de5\u5177\u589e\u5f3a\u8bad\u7ec3\u6570\u636e\u3002", "result": "LLM\u80fd\u6709\u6548\u5206\u7c7b\u7c97\u7c92\u5ea6\u53cd\u9988\uff0c\u6570\u636e\u589e\u5f3a\u663e\u8457\u63d0\u5347BERT\u6a21\u578b\u7684\u5206\u7c7b\u6027\u80fd\u3002", "conclusion": "LLM\u53ef\u4f5c\u4e3a\u9ad8\u6548\u6807\u6ce8\u5de5\u5177\uff0c\u89e3\u51b3\u6807\u6ce8\u6570\u636e\u4e0d\u8db3\u95ee\u9898\uff0c\u5e76\u63d0\u5347\u5206\u7c7b\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2507.08158", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.08158", "abs": "https://arxiv.org/abs/2507.08158", "authors": ["Marika Swanberg", "Meenatchi Sundaram Muthu Selva Annamalai", "Jamie Hayes", "Borja Balle", "Adam Smith"], "title": "Beyond the Worst Case: Extending Differential Privacy Guarantees to Realistic Adversaries", "comment": null, "summary": "Differential Privacy (DP) is a family of definitions that bound the\nworst-case privacy leakage of a mechanism. One important feature of the\nworst-case DP guarantee is it naturally implies protections against adversaries\nwith less prior information, more sophisticated attack goals, and complex\nmeasures of a successful attack. However, the analytical tradeoffs between the\nadversarial model and the privacy protections conferred by DP are not well\nunderstood thus far. To that end, this work sheds light on what the worst-case\nguarantee of DP implies about the success of attackers that are more\nrepresentative of real-world privacy risks.\n  In this paper, we present a single flexible framework that generalizes and\nextends the patchwork of bounds on DP mechanisms found in prior work. Our\nframework allows us to compute high-probability guarantees for DP mechanisms on\na large family of natural attack settings that previous bounds do not capture.\nOne class of such settings is the approximate reconstruction of multiple\nindividuals' data, such as inferring nearly entire columns of a tabular data\nset from noisy marginals and extracting sensitive information from DP-trained\nlanguage models.\n  We conduct two empirical case studies to illustrate the versatility of our\nbounds and compare them to the success of state-of-the-art attacks.\nSpecifically, we study attacks that extract non-uniform PII from a DP-trained\nlanguage model, as well as multi-column reconstruction attacks where the\nadversary has access to some columns in the clear and attempts to reconstruct\nthe remaining columns for each person's record. We find that the absolute\nprivacy risk of attacking non-uniform data is highly dependent on the\nadversary's prior probability of success. Our high probability bounds give us a\nnuanced understanding of the privacy leakage of DP mechanisms in a variety of\npreviously understudied attack settings.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5dee\u5206\u9690\u79c1\uff08DP\uff09\u5728\u6700\u574f\u60c5\u51b5\u4e0b\u7684\u9690\u79c1\u4fdd\u62a4\u80fd\u529b\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u901a\u7528\u6846\u67b6\uff0c\u7528\u4e8e\u5206\u6790DP\u673a\u5236\u5728\u591a\u79cd\u73b0\u5b9e\u653b\u51fb\u573a\u666f\u4e2d\u7684\u9690\u79c1\u98ce\u9669\u3002", "motivation": "\u7814\u7a76\u5dee\u5206\u9690\u79c1\u5728\u6700\u574f\u60c5\u51b5\u4e0b\u7684\u4fdd\u62a4\u80fd\u529b\uff0c\u5e76\u586b\u8865\u73b0\u6709\u7814\u7a76\u4e2d\u5bf9\u653b\u51fb\u6a21\u578b\u4e0e\u9690\u79c1\u4fdd\u62a4\u4e4b\u95f4\u5173\u7cfb\u7684\u7406\u89e3\u7a7a\u767d\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u7075\u6d3b\u7684\u6846\u67b6\uff0c\u6269\u5c55\u4e86\u73b0\u6709DP\u673a\u5236\u7684\u8fb9\u754c\uff0c\u80fd\u591f\u8ba1\u7b97\u591a\u79cd\u81ea\u7136\u653b\u51fb\u573a\u666f\u4e0b\u7684\u9ad8\u6982\u7387\u9690\u79c1\u4fdd\u8bc1\u3002", "result": "\u901a\u8fc7\u4e24\u4e2a\u5b9e\u8bc1\u6848\u4f8b\u7814\u7a76\uff0c\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u5b9e\u7528\u6027\uff0c\u5e76\u63ed\u793a\u4e86\u975e\u5747\u5300\u6570\u636e\u9690\u79c1\u98ce\u9669\u4e0e\u653b\u51fb\u8005\u5148\u9a8c\u6982\u7387\u7684\u5bc6\u5207\u5173\u7cfb\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u7406\u89e3DP\u673a\u5236\u5728\u590d\u6742\u653b\u51fb\u573a\u666f\u4e2d\u7684\u9690\u79c1\u6cc4\u6f0f\u63d0\u4f9b\u4e86\u66f4\u7ec6\u81f4\u7684\u89c6\u89d2\u3002"}}
{"id": "2507.08001", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.08001", "abs": "https://arxiv.org/abs/2507.08001", "authors": ["Shengyi Xie"], "title": "Human Creativity and AI", "comment": null, "summary": "With the advancement of science and technology, the philosophy of creativity\nhas undergone significant reinterpretation. This paper investigates\ncontemporary research in the fields of psychology, cognitive neuroscience, and\nthe philosophy of creativity, particularly in the context of the development of\nartificial intelligence (AI) techniques. It aims to address the central\nquestion: Can AI exhibit creativity? The paper reviews the historical\nperspectives on the philosophy of creativity and explores the influence of\npsychological advancements on the study of creativity. Furthermore, it analyzes\nvarious definitions of creativity and examines the responses of naturalism and\ncognitive neuroscience to the concept of creativity.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8AI\u662f\u5426\u80fd\u5c55\u73b0\u521b\u9020\u529b\uff0c\u7ed3\u5408\u5fc3\u7406\u5b66\u3001\u8ba4\u77e5\u795e\u7ecf\u79d1\u5b66\u548c\u54f2\u5b66\u89c6\u89d2\uff0c\u5206\u6790\u521b\u9020\u529b\u7684\u5b9a\u4e49\u53ca\u5176\u5728AI\u6280\u672f\u53d1\u5c55\u4e2d\u7684\u5f71\u54cd\u3002", "motivation": "\u968f\u7740\u79d1\u6280\u53d1\u5c55\uff0c\u521b\u9020\u529b\u54f2\u5b66\u88ab\u91cd\u65b0\u8be0\u91ca\uff0c\u7814\u7a76\u65e8\u5728\u56de\u7b54AI\u662f\u5426\u5177\u5907\u521b\u9020\u529b\u7684\u95ee\u9898\u3002", "method": "\u56de\u987e\u521b\u9020\u529b\u54f2\u5b66\u7684\u5386\u53f2\u89c6\u89d2\uff0c\u5206\u6790\u5fc3\u7406\u5b66\u8fdb\u5c55\u5bf9\u521b\u9020\u529b\u7814\u7a76\u7684\u5f71\u54cd\uff0c\u63a2\u8ba8\u81ea\u7136\u4e3b\u4e49\u548c\u8ba4\u77e5\u795e\u7ecf\u79d1\u5b66\u7684\u56de\u5e94\u3002", "result": "\u901a\u8fc7\u591a\u5b66\u79d1\u89c6\u89d2\uff0c\u63ed\u793a\u4e86\u521b\u9020\u529b\u5b9a\u4e49\u7684\u591a\u6837\u6027\u53ca\u5176\u5728AI\u80cc\u666f\u4e0b\u7684\u65b0\u7406\u89e3\u3002", "conclusion": "AI\u521b\u9020\u529b\u7684\u53ef\u80fd\u6027\u9700\u7ed3\u5408\u54f2\u5b66\u3001\u5fc3\u7406\u5b66\u548c\u795e\u7ecf\u79d1\u5b66\u7684\u7efc\u5408\u89c6\u89d2\u8fdb\u4e00\u6b65\u63a2\u8ba8\u3002"}}
{"id": "2507.08467", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.08467", "abs": "https://arxiv.org/abs/2507.08467", "authors": ["Youshuai Tan", "Zhanwei Zhang", "Jinfu Chen", "Zishuo Ding", "Jifeng Xuan", "Weiyi Shang"], "title": "Computing Floating-Point Errors by Injecting Perturbations", "comment": "arXiv admin note: text overlap with arXiv:2412.20804", "summary": "Floating-point programs form the foundation of modern science and\nengineering, providing the essential computational framework for a wide range\nof applications, such as safety-critical systems, aerospace engineering, and\nfinancial analysis. Floating-point errors can lead to severe consequences.\nAlthough floating-point errors widely exist, only a subset of inputs may\ntrigger significant errors in floating-point programs. Therefore, it is crucial\nto determine whether a given input could produce such errors. Researchers tend\nto take the results of high-precision floating-point programs as oracles for\ndetecting floating-point errors, which introduces two main limitations: (1)\ndifficulty of implementation and (2) prolonged execution time. The two recent\ntools, ATOMU and FPCC, can partially address these issues. However, ATOMU\nsuffers from false positives; while FPCC, though eliminating false positives,\noperates at a considerably slower speed.\n  To address these two challenges, we propose a novel approach named\nPI-detector to computing floating-point errors effectively and efficiently. Our\napproach is based on the observation that floating-point errors stem from large\ncondition numbers in atomic operations (such as addition and subtraction),\nwhich then propagate and accumulate. PI-detector injects small perturbations\ninto the operands of individual atomic operations within the program and\ncompares the outcomes of the original program with the perturbed version to\ncompute floating-point errors. We evaluate PI-detector with datasets from ATOMU\nand HSED, as well as a complex linear system-solving program. Experimental\nresults demonstrate that PI-detector can perform efficient and accurate\nfloating-point error computation.", "AI": {"tldr": "PI-detector\u662f\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u9ad8\u6548\u51c6\u786e\u5730\u8ba1\u7b97\u6d6e\u70b9\u7a0b\u5e8f\u4e2d\u7684\u8bef\u5dee\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u5de5\u5177ATOMU\u548cFPCC\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u6d6e\u70b9\u8bef\u5dee\u53ef\u80fd\u5bfc\u81f4\u4e25\u91cd\u540e\u679c\uff0c\u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5\u5b58\u5728\u5b9e\u73b0\u56f0\u96be\u548c\u6267\u884c\u65f6\u95f4\u957f\u7684\u95ee\u9898\uff0cATOMU\u6709\u8bef\u62a5\uff0cFPCC\u901f\u5ea6\u6162\u3002", "method": "PI-detector\u901a\u8fc7\u5728\u539f\u5b50\u64cd\u4f5c\u4e2d\u6ce8\u5165\u5c0f\u6270\u52a8\uff0c\u6bd4\u8f83\u539f\u59cb\u7a0b\u5e8f\u4e0e\u6270\u52a8\u7248\u672c\u7684\u8f93\u51fa\uff0c\u8ba1\u7b97\u6d6e\u70b9\u8bef\u5dee\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cPI-detector\u80fd\u9ad8\u6548\u51c6\u786e\u5730\u8ba1\u7b97\u6d6e\u70b9\u8bef\u5dee\u3002", "conclusion": "PI-detector\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u4e14\u9ad8\u6548\u7684\u6d6e\u70b9\u8bef\u5dee\u68c0\u6d4b\u65b9\u6cd5\u3002"}}
{"id": "2507.08166", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.08166", "abs": "https://arxiv.org/abs/2507.08166", "authors": ["Chris S. Lin", "Joyce Qu", "Gururaj Saileshwar"], "title": "GPUHammer: Rowhammer Attacks on GPU Memories are Practical", "comment": "20 pages, including appendices. The paper will appear in SEC'25", "summary": "Rowhammer is a read disturbance vulnerability in modern DRAM that causes\nbit-flips, compromising security and reliability. While extensively studied on\nIntel and AMD CPUs with DDR and LPDDR memories, its impact on GPUs using GDDR\nmemories, critical for emerging machine learning applications, remains\nunexplored. Rowhammer attacks on GPUs face unique challenges: (1) proprietary\nmapping of physical memory to GDDR banks and rows, (2) high memory latency and\nfaster refresh rates that hinder effective hammering, and (3) proprietary\nmitigations in GDDR memories, difficult to reverse-engineer without FPGA-based\ntest platforms. We introduce GPUHammer, the first Rowhammer attack on NVIDIA\nGPUs with GDDR6 DRAM. GPUHammer proposes novel techniques to reverse-engineer\nGDDR DRAM row mappings, and employs GPU-specific memory access optimizations to\namplify hammering intensity and bypass mitigations. Thus, we demonstrate the\nfirst successful Rowhammer attack on a discrete GPU, injecting up to 8\nbit-flips across 4 DRAM banks on an NVIDIA A6000 with GDDR6 memory. We also\nshow how an attacker can use these to tamper with ML models, causing\nsignificant accuracy drops (up to 80%).", "AI": {"tldr": "GPUHammer\u662f\u9996\u4e2a\u9488\u5bf9NVIDIA GPU\u548cGDDR6\u5185\u5b58\u7684Rowhammer\u653b\u51fb\uff0c\u901a\u8fc7\u9006\u5411\u5de5\u7a0b\u548c\u4f18\u5316\u5185\u5b58\u8bbf\u95ee\uff0c\u6210\u529f\u5728GPU\u4e0a\u5b9e\u73b0\u4f4d\u7ffb\u8f6c\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5bf9\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u6f5c\u5728\u5371\u5bb3\u3002", "motivation": "\u7814\u7a76Rowhammer\u6f0f\u6d1e\u5728GPU\u4e0a\u7684\u5f71\u54cd\uff0c\u586b\u8865\u4e86\u73b0\u6709\u7814\u7a76\u5728GDDR\u5185\u5b58\u4e0a\u7684\u7a7a\u767d\uff0c\u5c24\u5176\u662f\u9488\u5bf9\u65b0\u5174\u673a\u5668\u5b66\u4e60\u5e94\u7528\u7684\u5b89\u5168\u6027\u3002", "method": "\u63d0\u51faGPUHammer\uff0c\u901a\u8fc7\u9006\u5411\u5de5\u7a0bGDDR\u5185\u5b58\u884c\u6620\u5c04\uff0c\u4f18\u5316GPU\u5185\u5b58\u8bbf\u95ee\u4ee5\u589e\u5f3a\u653b\u51fb\u5f3a\u5ea6\uff0c\u5e76\u7ed5\u8fc7\u73b0\u6709\u9632\u62a4\u63aa\u65bd\u3002", "result": "\u6210\u529f\u5728NVIDIA A6000 GPU\u4e0a\u5b9e\u73b0\u4f4d\u7ffb\u8f6c\uff0c\u6700\u591a\u5f71\u54cd4\u4e2aDRAM\u5b58\u50a8\u4f53\u76848\u4e2a\u4f4d\uff0c\u5e76\u5bfc\u81f4\u673a\u5668\u5b66\u4e60\u6a21\u578b\u51c6\u786e\u7387\u4e0b\u964d\u9ad8\u8fbe80%\u3002", "conclusion": "GPUHammer\u8bc1\u660e\u4e86Rowhammer\u653b\u51fb\u5728GPU\u4e0a\u7684\u53ef\u884c\u6027\uff0c\u63ed\u793a\u4e86GDDR\u5185\u5b58\u7684\u5b89\u5168\u9690\u60a3\uff0c\u5bf9\u673a\u5668\u5b66\u4e60\u7b49\u5173\u952e\u5e94\u7528\u6784\u6210\u5a01\u80c1\u3002"}}
{"id": "2507.08046", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.08046", "abs": "https://arxiv.org/abs/2507.08046", "authors": ["Sishi Xiong", "Dakai Wang", "Yu Zhao", "Jie Zhang", "Changzai Pan", "Haowei He", "Xiangyu Li", "Wenhan Chang", "Zhongjiang He", "Shuangyong Song", "Yongxiang Li"], "title": "TableReasoner: Advancing Table Reasoning Framework with Large Language Models", "comment": null, "summary": "The paper presents our system developed for table question answering (TQA).\nTQA tasks face challenges due to the characteristics of real-world tabular\ndata, such as large size, incomplete column semantics, and entity ambiguity. To\naddress these issues, we propose a large language model (LLM)-powered and\nprogramming-based table reasoning framework, named TableReasoner. It models a\ntable using the schema that combines structural and semantic representations,\nenabling holistic understanding and efficient processing of large tables. We\ndesign a multi-step schema linking plan to derive a focused table schema that\nretains only query-relevant information, eliminating ambiguity and alleviating\nhallucinations. This focused table schema provides precise and sufficient table\ndetails for query refinement and programming. Furthermore, we integrate the\nreasoning workflow into an iterative thinking architecture, allowing\nincremental cycles of thinking, reasoning and reflection. Our system achieves\nfirst place in both subtasks of SemEval-2025 Task 8.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u548c\u7f16\u7a0b\u7684\u8868\u63a8\u7406\u6846\u67b6TableReasoner\uff0c\u7528\u4e8e\u89e3\u51b3\u8868\u95ee\u7b54\u4efb\u52a1\u4e2d\u7684\u6311\u6218\uff0c\u5e76\u5728SemEval-2025 Task 8\u4e2d\u53d6\u5f97\u7b2c\u4e00\u540d\u3002", "motivation": "\u8868\u95ee\u7b54\u4efb\u52a1\u9762\u4e34\u771f\u5b9e\u4e16\u754c\u8868\u683c\u6570\u636e\u7684\u6311\u6218\uff0c\u5982\u89c4\u6a21\u5927\u3001\u5217\u8bed\u4e49\u4e0d\u5b8c\u6574\u548c\u5b9e\u4f53\u6b67\u4e49\u3002", "method": "\u63d0\u51faTableReasoner\u6846\u67b6\uff0c\u7ed3\u5408\u7ed3\u6784\u548c\u8bed\u4e49\u8868\u793a\u5efa\u6a21\u8868\u683c\uff0c\u8bbe\u8ba1\u591a\u6b65\u6a21\u5f0f\u94fe\u63a5\u8ba1\u5212\u4ee5\u805a\u7126\u67e5\u8be2\u76f8\u5173\u4fe1\u606f\uff0c\u5e76\u96c6\u6210\u8fed\u4ee3\u601d\u8003\u67b6\u6784\u3002", "result": "\u7cfb\u7edf\u5728SemEval-2025 Task 8\u7684\u4e24\u4e2a\u5b50\u4efb\u52a1\u4e2d\u5747\u83b7\u5f97\u7b2c\u4e00\u540d\u3002", "conclusion": "TableReasoner\u901a\u8fc7\u7ed3\u5408\u8bed\u4e49\u548c\u7ed3\u6784\u8868\u793a\uff0c\u4ee5\u53ca\u8fed\u4ee3\u63a8\u7406\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8868\u95ee\u7b54\u4efb\u52a1\u4e2d\u7684\u6311\u6218\u3002"}}
{"id": "2507.08523", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.08523", "abs": "https://arxiv.org/abs/2507.08523", "authors": ["Yilun Wang", "Pengfei Chen", "Haiyu Huang", "Zilong He", "Gou Tan", "Chuanfu Zhang", "Jingkai He", "Zibin Zheng"], "title": "InferLog: Accelerating LLM Inference for Online Log Parsing via ICL-oriented Prefix Caching", "comment": null, "summary": "Modern software systems generate massive volumes of runtime logs,\nnecessitating efficient and accurate log parsing to enable critical downstream\ntasks such as anomaly detection and root cause analysis. Recently, large\nlanguage models (LLMs) have achieved advanced accuracy on log parsing, but\ntheir deployment in production environments faces two major limitations: (1)\nthe privacy risks associated with commercial LLMs, driving the adoption of\nlocal deployment, and (2) the stringent latency and throughput requirements\nimposed by high-volume log streams, which existing LLM-based parsers fail to\nmeet. Although recent efforts have reduced the number of LLM queries, they\noverlook the high latency of the LLM invocations, where concurrent log parsing\nrequests can cause serve performance degradation of LLM inference system.\n  In this study, we present InferLog, the first LLM inference optimization\nmethod for online log parsing. Our key insight is that the inference efficiency\nemerges as the vital bottleneck in LLM-based online log parsing, rather than\nparsing accuracy. InferLog accelerates inference by designing (1) A\nPrefix-aware ICL Refinement policy to refine the examples and permutation of\nin-context learning to improve the prefix caching efficiency. (2) A rapid and\ntask-specific configuration tuning pipeline based on meta-learning to find the\noptimal LLM scheduling-related configuration for dynamic log parsing workloads.\nThe experimental results based on Loghub dataset and vLLM demonstrate that\nInferLog significantly outperforms existing inference optimization methods and\nmarkedly accelerates the state-of-the-art LLM-based log parser without\ncompromising parsing accuracy.", "AI": {"tldr": "InferLog\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u5728\u7ebf\u65e5\u5fd7\u89e3\u6790\u7684LLM\u63a8\u7406\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u6539\u8fdb\u524d\u7f00\u7f13\u5b58\u6548\u7387\u548c\u52a8\u6001\u914d\u7f6e\u8c03\u4f18\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u65e5\u5fd7\u89e3\u6790\u65b9\u6cd5\u5728\u9690\u79c1\u548c\u6027\u80fd\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u65e0\u6cd5\u6ee1\u8db3\u9ad8\u541e\u5410\u91cf\u548c\u4f4e\u5ef6\u8fdf\u7684\u9700\u6c42\u3002", "method": "\u8bbe\u8ba1\u524d\u7f00\u611f\u77e5\u7684ICL\u4f18\u5316\u7b56\u7565\u548c\u57fa\u4e8e\u5143\u5b66\u4e60\u7684\u52a8\u6001\u914d\u7f6e\u8c03\u4f18\u7ba1\u9053\u3002", "result": "\u5728Loghub\u6570\u636e\u96c6\u548cvLLM\u4e0a\uff0cInferLog\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u4e0d\u727a\u7272\u89e3\u6790\u7cbe\u5ea6\u3002", "conclusion": "InferLog\u89e3\u51b3\u4e86LLM\u63a8\u7406\u6548\u7387\u74f6\u9888\uff0c\u4e3a\u5728\u7ebf\u65e5\u5fd7\u89e3\u6790\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.08286", "categories": ["cs.CR", "cs.ET"], "pdf": "https://arxiv.org/pdf/2507.08286", "abs": "https://arxiv.org/abs/2507.08286", "authors": ["Aufa Nasywa Rahman", "Bimo Sunarfri Hantono", "Guntur Dharma Putra"], "title": "TruChain: A Multi-Layer Architecture for Trusted, Verifiable, and Immutable Open Banking Data", "comment": "8 pages, 7 figures. Accepted to IEEE MetaCom 2025", "summary": "Open banking framework enables third party providers to access financial data\nacross banking institutions, leading to unprecedented innovations in the\nfinancial sector. However, some open banking standards remain susceptible to\nsevere technological risks, including unverified data sources, inconsistent\ndata integrity, and lack of immutability. In this paper, we propose a layered\narchitecture that provides assurance in data trustworthiness with three\ndistinct levels of trust, covering source validation, data-level\nauthentication, and tamper-proof storage. The first layer guarantees the source\nlegitimacy using decentralized identity and verifiable presentation, while the\nsecond layer verifies data authenticity and consistency using cryptographic\nsigning. Lastly, the third layer guarantees data immutability through the\nTangle, a directed acyclic graph distributed ledger. We implemented a\nproof-of-concept implementation of our solution to evaluate its performance,\nwhere the results demonstrate that the system scales linearly with a stable\nthroughput, exhibits a 100% validation rate, and utilizes under 35% of CPU and\n350 MiB memory. Compared to a real-world open banking implementation, our\nsolution offers significantly reduced latency and stronger data integrity\nassurance. Overall, our solution offers a practical and efficient system for\nsecure data sharing in financial ecosystems while maintaining regulatory\ncompliance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u67b6\u6784\uff0c\u901a\u8fc7\u4e09\u4e2a\u4fe1\u4efb\u7ea7\u522b\u786e\u4fdd\u5f00\u653e\u94f6\u884c\u6570\u636e\u53ef\u4fe1\u6027\uff0c\u5305\u62ec\u6765\u6e90\u9a8c\u8bc1\u3001\u6570\u636e\u7ea7\u8ba4\u8bc1\u548c\u9632\u7be1\u6539\u5b58\u50a8\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u9ad8\u6548\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u5f00\u653e\u94f6\u884c\u6846\u67b6\u867d\u7136\u4fc3\u8fdb\u4e86\u91d1\u878d\u521b\u65b0\uff0c\u4f46\u4ecd\u5b58\u5728\u6570\u636e\u6765\u6e90\u672a\u9a8c\u8bc1\u3001\u6570\u636e\u5b8c\u6574\u6027\u4e0d\u4e00\u81f4\u548c\u7f3a\u4e4f\u4e0d\u53ef\u7be1\u6539\u6027\u7b49\u6280\u672f\u98ce\u9669\uff0c\u4e9f\u9700\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u4e09\u5c42\u67b6\u6784\uff1a\u7b2c\u4e00\u5c42\u901a\u8fc7\u53bb\u4e2d\u5fc3\u5316\u8eab\u4efd\u548c\u53ef\u9a8c\u8bc1\u5448\u73b0\u786e\u4fdd\u6765\u6e90\u5408\u6cd5\u6027\uff1b\u7b2c\u4e8c\u5c42\u901a\u8fc7\u52a0\u5bc6\u7b7e\u540d\u9a8c\u8bc1\u6570\u636e\u771f\u5b9e\u6027\u548c\u4e00\u81f4\u6027\uff1b\u7b2c\u4e09\u5c42\u901a\u8fc7Tangle\u5206\u5e03\u5f0f\u8d26\u672c\u4fdd\u8bc1\u6570\u636e\u4e0d\u53ef\u7be1\u6539\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u7cfb\u7edf\u5177\u6709\u7ebf\u6027\u6269\u5c55\u6027\u3001\u7a33\u5b9a\u541e\u5410\u91cf\u3001100%\u9a8c\u8bc1\u7387\uff0cCPU\u548c\u5185\u5b58\u5360\u7528\u4f4e\uff08<35% CPU\uff0c350 MiB\u5185\u5b58\uff09\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6848\u5ef6\u8fdf\u66f4\u4f4e\u4e14\u6570\u636e\u5b8c\u6574\u6027\u66f4\u5f3a\u3002", "conclusion": "\u8be5\u65b9\u6848\u4e3a\u91d1\u878d\u751f\u6001\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u5b89\u5168\u7684\u5171\u4eab\u6570\u636e\u65b9\u6cd5\uff0c\u540c\u65f6\u7b26\u5408\u76d1\u7ba1\u8981\u6c42\u3002"}}
{"id": "2507.08207", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.08207", "abs": "https://arxiv.org/abs/2507.08207", "authors": ["Zhengye Han", "Quanyan Zhu"], "title": "A Dynamic Stackelberg Game Framework for Agentic AI Defense Against LLM Jailbreaking", "comment": null, "summary": "As large language models (LLMs) are increasingly deployed in critical\napplications, the challenge of jailbreaking, where adversaries manipulate the\nmodels to bypass safety mechanisms, has become a significant concern. This\npaper presents a dynamic Stackelberg game framework to model the interactions\nbetween attackers and defenders in the context of LLM jailbreaking. The\nframework treats the prompt-response dynamics as a sequential extensive-form\ngame, where the defender, as the leader, commits to a strategy while\nanticipating the attacker's optimal responses. We propose a novel agentic AI\nsolution, the \"Purple Agent,\" which integrates adversarial exploration and\ndefensive strategies using Rapidly-exploring Random Trees (RRT). The Purple\nAgent actively simulates potential attack trajectories and intervenes\nproactively to prevent harmful outputs. This approach offers a principled\nmethod for analyzing adversarial dynamics and provides a foundation for\nmitigating the risk of jailbreaking.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001Stackelberg\u535a\u5f08\u6846\u67b6\uff0c\u7528\u4e8e\u5efa\u6a21\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8d8a\u72f1\u4e2d\u7684\u653b\u51fb\u8005\u4e0e\u9632\u5fa1\u8005\u4ea4\u4e92\uff0c\u5e76\u5f15\u5165\u201c\u7d2b\u8272\u4ee3\u7406\u201d\u89e3\u51b3\u65b9\u6848\uff0c\u7ed3\u5408\u5bf9\u6297\u63a2\u7d22\u4e0e\u9632\u5fa1\u7b56\u7565\u3002", "motivation": "\u968f\u7740LLM\u5728\u5173\u952e\u5e94\u7528\u4e2d\u7684\u90e8\u7f72\u589e\u52a0\uff0c\u8d8a\u72f1\u653b\u51fb\uff08\u7ed5\u8fc7\u5b89\u5168\u673a\u5236\uff09\u6210\u4e3a\u91cd\u8981\u95ee\u9898\uff0c\u9700\u7814\u7a76\u9632\u5fa1\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u52a8\u6001Stackelberg\u535a\u5f08\u6846\u67b6\uff0c\u5c06\u63d0\u793a-\u54cd\u5e94\u52a8\u6001\u5efa\u6a21\u4e3a\u5e8f\u5217\u6269\u5c55\u5f62\u5f0f\u535a\u5f08\uff0c\u63d0\u51fa\u201c\u7d2b\u8272\u4ee3\u7406\u201d\u7ed3\u5408RRT\u8fdb\u884c\u5bf9\u6297\u63a2\u7d22\u4e0e\u9632\u5fa1\u3002", "result": "\u7d2b\u8272\u4ee3\u7406\u80fd\u4e3b\u52a8\u6a21\u62df\u653b\u51fb\u8f68\u8ff9\u5e76\u5e72\u9884\uff0c\u63d0\u4f9b\u5206\u6790\u5bf9\u6297\u52a8\u6001\u7684\u539f\u5219\u6027\u65b9\u6cd5\uff0c\u964d\u4f4e\u8d8a\u72f1\u98ce\u9669\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3aLLM\u8d8a\u72f1\u98ce\u9669\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.08594", "categories": ["cs.SE", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.08594", "abs": "https://arxiv.org/abs/2507.08594", "authors": ["Fernando Ayach", "Vitor Lameir\u00e3o", "Raul Le\u00e3o", "Jerfferson Felizardo", "Rafael Sobrinho", "Vanessa Borges", "Patr\u00edcia Matsubara", "Awdren Font\u00e3o"], "title": "Generating Proto-Personas through Prompt Engineering: A Case Study on Efficiency, Effectiveness and Empathy", "comment": "12 pages; 2 figures; Preprint with the original submission accepted\n  for publication at 39th Brazilian Symposium on Software Engineering (SBES)", "summary": "Proto-personas are commonly used during early-stage Product Discovery, such\nas Lean Inception, to guide product definition and stakeholder alignment.\nHowever, the manual creation of proto-personas is often time-consuming,\ncognitively demanding, and prone to bias. In this paper, we propose and\nempirically investigate a prompt engineering-based approach to generate\nproto-personas with the support of Generative AI (GenAI). Our goal is to\nevaluate the approach in terms of efficiency, effectiveness, user acceptance,\nand the empathy elicited by the generated personas. We conducted a case study\nwith 19 participants embedded in a real Lean Inception, employing a qualitative\nand quantitative methods design. The results reveal the approach's efficiency\nby reducing time and effort and improving the quality and reusability of\npersonas in later discovery phases, such as Minimum Viable Product (MVP)\nscoping and feature refinement. While acceptance was generally high, especially\nregarding perceived usefulness and ease of use, participants noted limitations\nrelated to generalization and domain specificity. Furthermore, although\ncognitive empathy was strongly supported, affective and behavioral empathy\nvaried significantly across participants. These results contribute novel\nempirical evidence on how GenAI can be effectively integrated into software\nProduct Discovery practices, while also identifying key challenges to be\naddressed in future iterations of such hybrid design processes.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u63d0\u793a\u5de5\u7a0b\u7684\u751f\u6210\u5f0fAI\u65b9\u6cd5\uff0c\u7528\u4e8e\u81ea\u52a8\u751f\u6210\u539f\u578b\u4eba\u7269\u89d2\u8272\uff0c\u4ee5\u63d0\u9ad8\u6548\u7387\u548c\u51cf\u5c11\u504f\u89c1\u3002", "motivation": "\u624b\u52a8\u521b\u5efa\u539f\u578b\u4eba\u7269\u89d2\u8272\u8017\u65f6\u3001\u8d39\u529b\u4e14\u6613\u53d7\u504f\u89c1\u5f71\u54cd\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u751f\u6210\u5f0fAI\uff08GenAI\uff09\u548c\u63d0\u793a\u5de5\u7a0b\u65b9\u6cd5\uff0c\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\uff0819\u540d\u53c2\u4e0e\u8005\uff09\u8fdb\u884c\u5b9a\u6027\u548c\u5b9a\u91cf\u5206\u6790\u3002", "result": "\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u6548\u7387\u548c\u8d28\u91cf\uff0c\u7528\u6237\u63a5\u53d7\u5ea6\u8f83\u9ad8\uff0c\u4f46\u5728\u6cdb\u5316\u548c\u9886\u57df\u7279\u5f02\u6027\u65b9\u9762\u5b58\u5728\u5c40\u9650\u3002", "conclusion": "\u751f\u6210\u5f0fAI\u53ef\u6709\u6548\u6574\u5408\u5230\u4ea7\u54c1\u53d1\u73b0\u5b9e\u8df5\u4e2d\uff0c\u4f46\u9700\u89e3\u51b3\u6cdb\u5316\u548c\u60c5\u611f\u5171\u9e23\u7684\u6311\u6218\u3002"}}
{"id": "2507.08288", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.08288", "abs": "https://arxiv.org/abs/2507.08288", "authors": ["Qingxiao Guo", "Xinjie Zhu", "Yilong Ma", "Hui Jin", "Yunhao Wang", "Weifeng Zhang", "Xiaobing Guo"], "title": "Invariant-based Robust Weights Watermark for Large Language Models", "comment": null, "summary": "Watermarking technology has gained significant attention due to the\nincreasing importance of intellectual property (IP) rights, particularly with\nthe growing deployment of large language models (LLMs) on billions\nresource-constrained edge devices. To counter the potential threats of IP theft\nby malicious users, this paper introduces a robust watermarking scheme without\nretraining or fine-tuning for transformer models. The scheme generates a unique\nkey for each user and derives a stable watermark value by solving linear\nconstraints constructed from model invariants. Moreover, this technology\nutilizes noise mechanism to hide watermark locations in multi-user scenarios\nagainst collusion attack. This paper evaluates the approach on three popular\nmodels (Llama3, Phi3, Gemma), and the experimental results confirm the strong\nrobustness across a range of attack methods (fine-tuning, pruning,\nquantization, permutation, scaling, reversible matrix and collusion attacks).", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6216\u5fae\u8c03\u7684\u9c81\u68d2\u6c34\u5370\u65b9\u6848\uff0c\u7528\u4e8e\u4fdd\u62a4\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684IP\uff0c\u901a\u8fc7\u7ebf\u6027\u7ea6\u675f\u548c\u566a\u58f0\u673a\u5236\u5b9e\u73b0\u591a\u7528\u6237\u573a\u666f\u4e0b\u7684\u5b89\u5168\u6027\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u4fdd\u62a4\u77e5\u8bc6\u4ea7\u6743\u514d\u53d7\u6076\u610f\u7528\u6237\u4fb5\u5bb3\u7684\u9700\u6c42\u65e5\u76ca\u589e\u957f\u3002", "method": "\u4e3a\u6bcf\u4e2a\u7528\u6237\u751f\u6210\u552f\u4e00\u5bc6\u94a5\uff0c\u901a\u8fc7\u6a21\u578b\u4e0d\u53d8\u6027\u6784\u5efa\u7ebf\u6027\u7ea6\u675f\u6c42\u89e3\u7a33\u5b9a\u6c34\u5370\u503c\uff0c\u5e76\u5229\u7528\u566a\u58f0\u673a\u5236\u9690\u85cf\u6c34\u5370\u4f4d\u7f6e\u4ee5\u9632\u6b62\u5171\u8c0b\u653b\u51fb\u3002", "result": "\u5728Llama3\u3001Phi3\u548cGemma\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6848\u5bf9\u591a\u79cd\u653b\u51fb\u65b9\u6cd5\uff08\u5982\u5fae\u8c03\u3001\u526a\u679d\u3001\u91cf\u5316\u7b49\uff09\u5177\u6709\u5f3a\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u6c34\u5370\u65b9\u6848\u5728\u4fdd\u62a4\u6a21\u578bIP\u65b9\u9762\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u9c81\u68d2\u6027\uff0c\u9002\u7528\u4e8e\u591a\u7528\u6237\u573a\u666f\u3002"}}
{"id": "2507.08208", "categories": ["cs.AI", "cs.GT"], "pdf": "https://arxiv.org/pdf/2507.08208", "abs": "https://arxiv.org/abs/2507.08208", "authors": ["Quanyan Zhu"], "title": "Reasoning and Behavioral Equilibria in LLM-Nash Games: From Mindsets to Actions", "comment": null, "summary": "We introduce the LLM-Nash framework, a game-theoretic model where agents\nselect reasoning prompts to guide decision-making via Large Language Models\n(LLMs). Unlike classical games that assume utility-maximizing agents with full\nrationality, this framework captures bounded rationality by modeling the\nreasoning process explicitly. Equilibrium is defined over the prompt space,\nwith actions emerging as the behavioral output of LLM inference. This approach\nenables the study of cognitive constraints, mindset expressiveness, and\nepistemic learning. Through illustrative examples, we show how reasoning\nequilibria can diverge from classical Nash outcomes, offering a new foundation\nfor strategic interaction in LLM-enabled systems.", "AI": {"tldr": "LLM-Nash\u6846\u67b6\u901a\u8fc7\u6e38\u620f\u7406\u8bba\u6a21\u578b\u7814\u7a76LLM\u9a71\u52a8\u7684\u51b3\u7b56\uff0c\u5b9a\u4e49\u63d0\u793a\u7a7a\u95f4\u4e2d\u7684\u5747\u8861\uff0c\u5c55\u793a\u4e0e\u4f20\u7edf\u7eb3\u4ec0\u5747\u8861\u7684\u5dee\u5f02\u3002", "motivation": "\u7814\u7a76LLM\u5728\u6709\u9650\u7406\u6027\u4e0b\u7684\u51b3\u7b56\u884c\u4e3a\uff0c\u63a2\u7d22\u8ba4\u77e5\u7ea6\u675f\u548c\u601d\u7ef4\u8868\u8fbe\u3002", "method": "\u57fa\u4e8e\u6e38\u620f\u7406\u8bba\uff0c\u5b9a\u4e49\u63d0\u793a\u7a7a\u95f4\u4e2d\u7684\u5747\u8861\uff0c\u901a\u8fc7LLM\u63a8\u7406\u751f\u6210\u884c\u4e3a\u8f93\u51fa\u3002", "result": "\u5c55\u793a\u4e86\u63a8\u7406\u5747\u8861\u4e0e\u4f20\u7edf\u7eb3\u4ec0\u5747\u8861\u7684\u5dee\u5f02\uff0c\u4e3aLLM\u7cfb\u7edf\u7684\u6218\u7565\u4ea4\u4e92\u63d0\u4f9b\u65b0\u89c6\u89d2\u3002", "conclusion": "LLM-Nash\u6846\u67b6\u4e3a\u7814\u7a76LLM\u9a71\u52a8\u7684\u6218\u7565\u4ea4\u4e92\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\uff0c\u63ed\u793a\u4e86\u6709\u9650\u7406\u6027\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2507.08627", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.08627", "abs": "https://arxiv.org/abs/2507.08627", "authors": ["Chi-en Amy Tai", "Pengyu Nie", "Lukasz Golab", "Alexander Wong"], "title": "NL in the Middle: Code Translation with LLMs and Intermediate Representations", "comment": null, "summary": "Studies show that large language models (LLMs) produce buggy code\ntranslations. One avenue to improve translation accuracy is through\nintermediate representations, which could provide structured insights to guide\nthe model's understanding. We explore whether code translation using LLMs can\nbenefit from intermediate representations via natural language (NL) and\nabstract syntax trees (ASTs). Since prompt engineering greatly affects LLM\nperformance, we consider several ways to integrate these representations, from\none-shot to chain-of-thought (CoT) prompting. Using Open Gpt4 8X7B and\nspecialized StarCoder and CodeGen models on popular code translation benchmarks\n(CodeNet and AVATAR), we find that CoT with an intermediate NL summary performs\nbest, with an increase of 13.8% and 6.7%, respectively, in successful\ntranslations for the best-performing model (Open Gpt4 8X7B) compared to the\nzero-shot prompt.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u548c\u62bd\u8c61\u8bed\u6cd5\u6811\u4f5c\u4e3a\u4e2d\u95f4\u8868\u793a\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u4ee3\u7801\u7ffb\u8bd1\u51c6\u786e\u6027\u7684\u65b9\u6cd5\uff0c\u53d1\u73b0\u94fe\u5f0f\u601d\u7ef4\u63d0\u793a\u7ed3\u5408\u81ea\u7136\u8bed\u8a00\u6458\u8981\u6548\u679c\u6700\u4f73\u3002", "motivation": "\u7814\u7a76\u53d1\u73b0\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u4ee3\u7801\u7ffb\u8bd1\u5b58\u5728\u9519\u8bef\uff0c\u5e0c\u671b\u901a\u8fc7\u4e2d\u95f4\u8868\u793a\uff08\u5982\u81ea\u7136\u8bed\u8a00\u548c\u62bd\u8c61\u8bed\u6cd5\u6811\uff09\u63d0\u5347\u7ffb\u8bd1\u51c6\u786e\u6027\u3002", "method": "\u91c7\u7528\u591a\u79cd\u63d0\u793a\u65b9\u6cd5\uff08\u4ece\u5355\u6b21\u63d0\u793a\u5230\u94fe\u5f0f\u601d\u7ef4\u63d0\u793a\uff09\u96c6\u6210\u4e2d\u95f4\u8868\u793a\uff0c\u5e76\u5728Open Gpt4 8X7B\u3001StarCoder\u548cCodeGen\u6a21\u578b\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u94fe\u5f0f\u601d\u7ef4\u63d0\u793a\u7ed3\u5408\u81ea\u7136\u8bed\u8a00\u6458\u8981\u6548\u679c\u6700\u597d\uff0cOpen Gpt4 8X7B\u6a21\u578b\u7684\u6210\u529f\u7ffb\u8bd1\u7387\u5206\u522b\u63d0\u5347\u4e8613.8%\u548c6.7%\u3002", "conclusion": "\u4e2d\u95f4\u8868\u793a\uff08\u5c24\u5176\u662f\u81ea\u7136\u8bed\u8a00\u6458\u8981\uff09\u7ed3\u5408\u94fe\u5f0f\u601d\u7ef4\u63d0\u793a\u80fd\u663e\u8457\u63d0\u5347\u4ee3\u7801\u7ffb\u8bd1\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2507.08312", "categories": ["cs.CR", "cs.ET"], "pdf": "https://arxiv.org/pdf/2507.08312", "abs": "https://arxiv.org/abs/2507.08312", "authors": ["Jesus Lopez", "Viviana Cadena", "Mohammad Saidur Rahman"], "title": "Evaluating Post-Quantum Cryptographic Algorithms on Resource-Constrained Devices", "comment": "8 pages, 4 figures, 4 tables. This paper is accepted at the IEEE\n  Quantum Week 2025 -- IEEE International Conference on Quantum Computing and\n  Engineering (QCE) 2025", "summary": "The rapid advancement of quantum computing poses a critical threat to\nclassical cryptographic algorithms such as RSA and ECC, particularly in\nInternet of Things (IoT) devices, where secure communication is essential but\noften constrained by limited computational resources. This paper investigates\nthe feasibility of deploying post-quantum cryptography (PQC) algorithms on\nresource-constrained devices. In particular, we implement three PQC algorithms\n-- BIKE, CRYSTALS-Kyber, and HQC -- on a lightweight IoT platform built with\nRaspberry Pi devices. Leveraging the Open Quantum Safe (\\texttt{liboqs})\nlibrary in conjunction with \\texttt{mbedTLS}, we develop quantum-secure key\nexchange protocols, and evaluate their performance in terms of computational\noverhead, memory usage, and energy consumption for quantum secure\ncommunication. Experimental results demonstrate that the integration of PQC\nalgorithms on constrained hardware is practical, reinforcing the urgent need\nfor quantum-resilient cryptographic frameworks in next-generation IoT devices.\nThe implementation of this paper is available at\nhttps://iqsec-lab.github.io/PQC-IoT/.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5728\u8d44\u6e90\u53d7\u9650\u7684\u7269\u8054\u7f51\u8bbe\u5907\u4e0a\u90e8\u7f72\u540e\u91cf\u5b50\u5bc6\u7801\uff08PQC\uff09\u7b97\u6cd5\u7684\u53ef\u884c\u6027\uff0c\u5b9e\u73b0\u4e86\u4e09\u79cdPQC\u7b97\u6cd5\u5e76\u8bc4\u4f30\u5176\u6027\u80fd\uff0c\u8bc1\u660e\u5176\u5b9e\u9645\u53ef\u884c\u6027\u3002", "motivation": "\u91cf\u5b50\u8ba1\u7b97\u7684\u5feb\u901f\u53d1\u5c55\u5a01\u80c1\u5230\u4f20\u7edf\u52a0\u5bc6\u7b97\u6cd5\uff08\u5982RSA\u548cECC\uff09\uff0c\u7269\u8054\u7f51\u8bbe\u5907\u56e0\u8d44\u6e90\u6709\u9650\u4e9f\u9700\u91cf\u5b50\u5b89\u5168\u7684\u52a0\u5bc6\u65b9\u6848\u3002", "method": "\u5728\u57fa\u4e8eRaspberry Pi\u7684\u8f7b\u91cf\u7ea7\u7269\u8054\u7f51\u5e73\u53f0\u4e0a\u5b9e\u73b0BIKE\u3001CRYSTALS-Kyber\u548cHQC\u4e09\u79cdPQC\u7b97\u6cd5\uff0c\u7ed3\u5408Open Quantum Safe\u5e93\u548cmbedTLS\u5f00\u53d1\u91cf\u5b50\u5b89\u5168\u5bc6\u94a5\u4ea4\u6362\u534f\u8bae\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cPQC\u7b97\u6cd5\u5728\u53d7\u9650\u786c\u4ef6\u4e0a\u7684\u96c6\u6210\u662f\u53ef\u884c\u7684\uff0c\u8ba1\u7b97\u5f00\u9500\u3001\u5185\u5b58\u4f7f\u7528\u548c\u80fd\u8017\u5747\u5728\u53ef\u63a5\u53d7\u8303\u56f4\u5185\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u5728\u4e0b\u4e00\u4ee3\u7269\u8054\u7f51\u8bbe\u5907\u4e2d\u91c7\u7528\u91cf\u5b50\u5f39\u6027\u52a0\u5bc6\u6846\u67b6\u7684\u7d27\u8feb\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u5b9e\u9645\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.08210", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.08210", "abs": "https://arxiv.org/abs/2507.08210", "authors": ["Fryderyk Mantiuk", "Hanqi Zhou", "Charley M. Wu"], "title": "From Curiosity to Competence: How World Models Interact with the Dynamics of Exploration", "comment": null, "summary": "What drives an agent to explore the world while also maintaining control over\nthe environment? From a child at play to scientists in the lab, intelligent\nagents must balance curiosity (the drive to seek knowledge) with competence\n(the drive to master and control the environment). Bridging cognitive theories\nof intrinsic motivation with reinforcement learning, we ask how evolving\ninternal representations mediate the trade-off between curiosity (novelty or\ninformation gain) and competence (empowerment). We compare two model-based\nagents using handcrafted state abstractions (Tabular) or learning an internal\nworld model (Dreamer). The Tabular agent shows curiosity and competence guide\nexploration in distinct patterns, while prioritizing both improves exploration.\nThe Dreamer agent reveals a two-way interaction between exploration and\nrepresentation learning, mirroring the developmental co-evolution of curiosity\nand competence. Our findings formalize adaptive exploration as a balance\nbetween pursuing the unknown and the controllable, offering insights for\ncognitive theories and efficient reinforcement learning.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u667a\u80fd\u4f53\u5982\u4f55\u5e73\u8861\u597d\u5947\u5fc3\uff08\u63a2\u7d22\u672a\u77e5\uff09\u4e0e\u80fd\u529b\uff08\u63a7\u5236\u73af\u5883\uff09\uff0c\u901a\u8fc7\u6bd4\u8f83\u4e24\u79cd\u6a21\u578b\u9a71\u52a8\u7684\u667a\u80fd\u4f53\uff08Tabular\u548cDreamer\uff09\uff0c\u63ed\u793a\u4e86\u63a2\u7d22\u4e0e\u8868\u5f81\u5b66\u4e60\u7684\u53cc\u5411\u4e92\u52a8\u5173\u7cfb\u3002", "motivation": "\u7814\u7a76\u667a\u80fd\u4f53\u5982\u4f55\u5728\u63a2\u7d22\u4e16\u754c\u4e0e\u63a7\u5236\u73af\u5883\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u501f\u9274\u8ba4\u77e5\u7406\u8bba\u4e2d\u7684\u5185\u5728\u52a8\u673a\u4e0e\u5f3a\u5316\u5b66\u4e60\u76f8\u7ed3\u5408\u3002", "method": "\u6bd4\u8f83\u4e86\u4e24\u79cd\u6a21\u578b\u9a71\u52a8\u7684\u667a\u80fd\u4f53\uff1a\u57fa\u4e8e\u624b\u5de5\u72b6\u6001\u62bd\u8c61\u7684Tabular\u548c\u57fa\u4e8e\u5b66\u4e60\u5185\u90e8\u4e16\u754c\u6a21\u578b\u7684Dreamer\uff0c\u5206\u6790\u5176\u63a2\u7d22\u884c\u4e3a\u3002", "result": "Tabular\u667a\u80fd\u4f53\u663e\u793a\u597d\u5947\u5fc3\u548c\u80fd\u529b\u5f15\u5bfc\u63a2\u7d22\u7684\u4e0d\u540c\u6a21\u5f0f\uff0c\u800cDreamer\u667a\u80fd\u4f53\u63ed\u793a\u4e86\u63a2\u7d22\u4e0e\u8868\u5f81\u5b66\u4e60\u7684\u53cc\u5411\u4e92\u52a8\u3002", "conclusion": "\u7814\u7a76\u5f62\u5f0f\u5316\u4e86\u9002\u5e94\u6027\u63a2\u7d22\u7684\u5e73\u8861\u673a\u5236\uff0c\u4e3a\u8ba4\u77e5\u7406\u8bba\u548c\u9ad8\u6548\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\u3002"}}
{"id": "2507.08671", "categories": ["cs.SE", "D.2.3; D.2.7; I.2.6"], "pdf": "https://arxiv.org/pdf/2507.08671", "abs": "https://arxiv.org/abs/2507.08671", "authors": ["Hua Ge", "Juan Zhai", "Minxue Pan", "Fusen He", "Ziyue Tan"], "title": "LLMCup: Ranking-Enhanced Comment Updating with LLMs", "comment": "13 pages, 10 figures", "summary": "While comments are essential for enhancing code readability and\nmaintainability in modern software projects, developers are often motivated to\nupdate code but not comments, leading to outdated or inconsistent documentation\nthat hinders future understanding and maintenance. Recent approaches such as\nCUP and HebCup have attempted automatic comment updating using neural\nsequence-to-sequence models and heuristic rules, respectively. However, these\nmethods can miss or misinterpret crucial information during comment updating,\nresulting in inaccurate comments, and they often struggle with complex update\nscenarios. Given these challenges, a promising direction lies in leveraging\nlarge language models (LLMs), which have shown impressive performance in\nsoftware engineering tasks such as comment generation, code synthesis, and\nprogram repair. This suggests their strong potential to capture the logic\nbehind code modifications - an ability that is crucial for the task of comment\nupdating. Nevertheless, selecting an appropriate prompt strategy for an LLM on\neach update case remains challenging. To address this, we propose a novel\ncomment updating framework, LLMCup, which first uses multiple prompt strategies\nto provide diverse candidate updated comments via an LLM, and then employs a\nranking model, CupRank, to select the best candidate as final updated comment.\nExperimental results demonstrate the effectiveness of LLMCup, with improvements\nover state-of-the-art baselines (CUP and HebCup) by 49.0%-116.9% in Accuracy,\n10.8%-20% in BLEU-4, 4.6% in METEOR, 0.9%-1.9% in F1, and 2.1%-3.4% in\nSentenceBert similarity. Furthermore, a user study shows that comments updated\nby LLMCup sometimes surpass human-written updates, highlighting the importance\nof incorporating human evaluation in comment quality assessment.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u6ce8\u91ca\u66f4\u65b0\u6846\u67b6LLMCup\uff0c\u901a\u8fc7\u591a\u63d0\u793a\u7b56\u7565\u751f\u6210\u5019\u9009\u6ce8\u91ca\uff0c\u5e76\u7ed3\u5408\u6392\u540d\u6a21\u578bCupRank\u9009\u62e9\u6700\u4f73\u66f4\u65b0\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f00\u53d1\u8005\u5e38\u66f4\u65b0\u4ee3\u7801\u4f46\u5ffd\u7565\u6ce8\u91ca\uff0c\u5bfc\u81f4\u6587\u6863\u8fc7\u65f6\u6216\u4e0d\u4e00\u81f4\uff0c\u73b0\u6709\u81ea\u52a8\u66f4\u65b0\u65b9\u6cd5\uff08\u5982CUP\u548cHebCup\uff09\u5b58\u5728\u4fe1\u606f\u9057\u6f0f\u6216\u8bef\u89e3\u95ee\u9898\u3002", "method": "LLMCup\u5229\u7528LLM\u7684\u591a\u63d0\u793a\u7b56\u7565\u751f\u6210\u5019\u9009\u6ce8\u91ca\uff0c\u518d\u901a\u8fc7CupRank\u6a21\u578b\u9009\u62e9\u6700\u4f73\u66f4\u65b0\u3002", "result": "\u5b9e\u9a8c\u663e\u793aLLMCup\u5728\u51c6\u786e\u6027\uff08Accuracy\uff09\u3001BLEU-4\u3001METEOR\u3001F1\u548cSentenceBert\u76f8\u4f3c\u5ea6\u4e0a\u5747\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff08CUP\u548cHebCup\uff09\uff0c\u90e8\u5206\u66f4\u65b0\u751a\u81f3\u4f18\u4e8e\u4eba\u5de5\u6ce8\u91ca\u3002", "conclusion": "LLMCup\u5c55\u793a\u4e86LLM\u5728\u6ce8\u91ca\u66f4\u65b0\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\uff0c\u5e76\u5f3a\u8c03\u4e86\u4eba\u5de5\u8bc4\u4f30\u5728\u6ce8\u91ca\u8d28\u91cf\u8bc4\u4f30\u4e2d\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2507.08331", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.08331", "abs": "https://arxiv.org/abs/2507.08331", "authors": ["Chun-I Fan", "Li-En Chang", "Cheng-Han Shie"], "title": "Qualcomm Trusted Application Emulation for Fuzzing Testing", "comment": "This work is currently under review for presentation at the USENIX\n  Security 2025 poster session", "summary": "In recent years, the increasing awareness of cybersecurity has led to a\nheightened focus on information security within hardware devices and products.\nIncorporating Trusted Execution Environments (TEEs) into product designs has\nbecome a standard practice for safeguarding sensitive user information.\nHowever, vulnerabilities within these components present significant risks, if\nexploited by attackers, these vulnerabilities could lead to the leakage of\nsensitive data, thereby compromising user privacy and security. This research\ncenters on trusted applications (TAs) within the Qualcomm TEE and introduces a\nnovel emulator specifically designed for these applications. Through reverse\nengineering techniques, we thoroughly analyze Qualcomm TAs and develop a\npartial emulation environment that accurately emulates their behavior.\nAdditionally, we integrate fuzzing testing techniques into the emulator to\nsystematically uncover potential vulnerabilities within Qualcomm TAs,\ndemonstrating its practical effectiveness in identifying real-world security\nflaws. This research makes a significant contribution by being the first to\nprovide both the implementation methods and source codes for a Qualcomm TAs\nemulator, offering a valuable reference for future research efforts. Unlike\nprevious approaches that relied on complex and resource-intensive full-system\nsimulations, our approach is lightweight and effective, making security testing\nof TA more convenient.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684Qualcomm\u53ef\u4fe1\u6267\u884c\u73af\u5883\uff08TEE\uff09\u4e2d\u53ef\u4fe1\u5e94\u7528\uff08TAs\uff09\u7684\u6a21\u62df\u5668\uff0c\u7ed3\u5408\u9006\u5411\u5de5\u7a0b\u548c\u6a21\u7cca\u6d4b\u8bd5\u6280\u672f\uff0c\u6709\u6548\u53d1\u73b0\u6f5c\u5728\u6f0f\u6d1e\u3002", "motivation": "\u968f\u7740\u7f51\u7edc\u5b89\u5168\u610f\u8bc6\u7684\u63d0\u5347\uff0c\u786c\u4ef6\u8bbe\u5907\u4e2d\u7684\u4fe1\u606f\u5b89\u5168\u6027\u53d7\u5230\u91cd\u89c6\uff0c\u4f46TEE\u4e2d\u7684\u6f0f\u6d1e\u53ef\u80fd\u5bfc\u81f4\u654f\u611f\u6570\u636e\u6cc4\u9732\uff0c\u5a01\u80c1\u7528\u6237\u9690\u79c1\u548c\u5b89\u5168\u3002", "method": "\u901a\u8fc7\u9006\u5411\u5de5\u7a0b\u5206\u6790Qualcomm TAs\uff0c\u5f00\u53d1\u90e8\u5206\u6a21\u62df\u73af\u5883\uff0c\u5e76\u96c6\u6210\u6a21\u7cca\u6d4b\u8bd5\u6280\u672f\uff0c\u7cfb\u7edf\u6027\u5730\u53d1\u73b0\u6f0f\u6d1e\u3002", "result": "\u6210\u529f\u5f00\u53d1\u51fa\u8f7b\u91cf\u7ea7\u6a21\u62df\u5668\uff0c\u5e76\u9a8c\u8bc1\u5176\u5728\u53d1\u73b0\u5b9e\u9645\u5b89\u5168\u6f0f\u6d1e\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u9996\u6b21\u63d0\u4f9b\u4e86Qualcomm TAs\u6a21\u62df\u5668\u7684\u5b9e\u73b0\u65b9\u6cd5\u548c\u6e90\u4ee3\u7801\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u53c2\u8003\uff0c\u76f8\u6bd4\u4f20\u7edf\u5168\u7cfb\u7edf\u6a21\u62df\u66f4\u8f7b\u91cf\u9ad8\u6548\u3002"}}
{"id": "2507.08216", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.08216", "abs": "https://arxiv.org/abs/2507.08216", "authors": ["Rodrigo Castellano Ontiveros", "Francesco Giannini", "Marco Gori", "Giuseppe Marra", "Michelangelo Diligenti"], "title": "Grounding Methods for Neural-Symbolic AI", "comment": null, "summary": "A large class of Neural-Symbolic (NeSy) methods employs a machine learner to\nprocess the input entities, while relying on a reasoner based on First-Order\nLogic to represent and process more complex relationships among the entities. A\nfundamental role for these methods is played by the process of logic grounding,\nwhich determines the relevant substitutions for the logic rules using a\n(sub)set of entities. Some NeSy methods use an exhaustive derivation of all\npossible substitutions, preserving the full expressive power of the logic\nknowledge. This leads to a combinatorial explosion in the number of ground\nformulas to consider and, therefore, strongly limits their scalability. Other\nmethods rely on heuristic-based selective derivations, which are generally more\ncomputationally efficient, but lack a justification and provide no guarantees\nof preserving the information provided to and returned by the reasoner. Taking\ninspiration from multi-hop symbolic reasoning, this paper proposes a\nparametrized family of grounding methods generalizing classic Backward\nChaining. Different selections within this family allow us to obtain commonly\nemployed grounding methods as special cases, and to control the trade-off\nbetween expressiveness and scalability of the reasoner. The experimental\nresults show that the selection of the grounding criterion is often as\nimportant as the NeSy method itself.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53c2\u6570\u5316\u7684\u903b\u8f91\u63a5\u5730\u65b9\u6cd5\uff0c\u901a\u8fc7\u6cdb\u5316\u7ecf\u5178\u7684\u540e\u5411\u94fe\u5f0f\u63a8\u7406\uff0c\u5e73\u8861\u8868\u8fbe\u80fd\u529b\u548c\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\u5728\u903b\u8f91\u63a5\u5730\u8fc7\u7a0b\u4e2d\u9762\u4e34\u7684\u53ef\u6269\u5c55\u6027\u548c\u8868\u8fbe\u80fd\u529b\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53c2\u6570\u5316\u7684\u63a5\u5730\u65b9\u6cd5\u5bb6\u65cf\uff0c\u6cdb\u5316\u540e\u5411\u94fe\u5f0f\u63a8\u7406\uff0c\u5141\u8bb8\u63a7\u5236\u8868\u8fbe\u80fd\u529b\u548c\u53ef\u6269\u5c55\u6027\u7684\u5e73\u8861\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u63a5\u5730\u6807\u51c6\u7684\u9009\u62e9\u5bf9\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\u7684\u6027\u80fd\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "\u901a\u8fc7\u53c2\u6570\u5316\u63a5\u5730\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5728\u8868\u8fbe\u80fd\u529b\u548c\u53ef\u6269\u5c55\u6027\u4e4b\u95f4\u627e\u5230\u66f4\u597d\u7684\u5e73\u8861\uff0c\u63d0\u5347\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\u7684\u5b9e\u7528\u6027\u3002"}}
{"id": "2507.08730", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.08730", "abs": "https://arxiv.org/abs/2507.08730", "authors": ["Zezhen Xiang", "Jingzhi Gong", "Tao Chen"], "title": "Dually Hierarchical Drift Adaptation for Online Configuration Performance Learning", "comment": "Accepted by ICSE 2026", "summary": "Modern configurable software systems need to learn models that correlate\nconfiguration and performance. However, when the system operates in dynamic\nenvironments, the workload variations, hardware changes, and system updates\nwill inevitably introduce concept drifts at different levels - global drifts,\nwhich reshape the performance landscape of the entire configuration space; and\nlocal drifts, which only affect certain sub-regions of that space. As such,\nexisting offline and transfer learning approaches can struggle to adapt to\nthese implicit and unpredictable changes in real-time, rendering configuration\nperformance learning challenging. To address this, we propose DHDA, an online\nconfiguration performance learning framework designed to capture and adapt to\nthese drifts at different levels. The key idea is that DHDA adapts to both the\nlocal and global drifts using dually hierarchical adaptation: at the upper\nlevel, we redivide the data into different divisions, within each of which the\nlocal model is retrained, to handle global drifts only when necessary. At the\nlower level, the local models of the divisions can detect local drifts and\nadapt themselves asynchronously. To balance responsiveness and efficiency, DHDA\ncombines incremental updates with periodic full retraining to minimize\nredundant computation when no drifts are detected. Through evaluating eight\nsoftware systems and against state-of-the-art approaches, we show that DHDA\nachieves considerably better accuracy and can effectively adapt to drifts with\nup to 2x improvements, while incurring reasonable overhead and is able to\nimprove different local models in handling concept drift.", "AI": {"tldr": "DHDA\u662f\u4e00\u4e2a\u5728\u7ebf\u914d\u7f6e\u6027\u80fd\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u91cd\u5c42\u6b21\u9002\u5e94\u673a\u5236\u5904\u7406\u52a8\u6001\u73af\u5883\u4e2d\u7684\u5168\u5c40\u548c\u5c40\u90e8\u6982\u5ff5\u6f02\u79fb\uff0c\u663e\u8457\u63d0\u5347\u51c6\u786e\u6027\u548c\u9002\u5e94\u6027\u3002", "motivation": "\u73b0\u4ee3\u53ef\u914d\u7f6e\u8f6f\u4ef6\u7cfb\u7edf\u5728\u52a8\u6001\u73af\u5883\u4e2d\u8fd0\u884c\u65f6\uff0c\u7531\u4e8e\u5de5\u4f5c\u8d1f\u8f7d\u53d8\u5316\u3001\u786c\u4ef6\u66f4\u65b0\u7b49\u56e0\u7d20\uff0c\u4f1a\u5f15\u5165\u5168\u5c40\u548c\u5c40\u90e8\u6982\u5ff5\u6f02\u79fb\uff0c\u4f20\u7edf\u79bb\u7ebf\u5b66\u4e60\u548c\u8fc1\u79fb\u5b66\u4e60\u65b9\u6cd5\u96be\u4ee5\u5b9e\u65f6\u9002\u5e94\u8fd9\u4e9b\u53d8\u5316\u3002", "method": "DHDA\u91c7\u7528\u53cc\u91cd\u5c42\u6b21\u9002\u5e94\u673a\u5236\uff0c\u4e0a\u5c42\u901a\u8fc7\u91cd\u65b0\u5212\u5206\u6570\u636e\u5e76\u5c40\u90e8\u91cd\u8bad\u7ec3\u5904\u7406\u5168\u5c40\u6f02\u79fb\uff0c\u4e0b\u5c42\u901a\u8fc7\u5f02\u6b65\u8c03\u6574\u5c40\u90e8\u6a21\u578b\u68c0\u6d4b\u548c\u9002\u5e94\u5c40\u90e8\u6f02\u79fb\uff0c\u7ed3\u5408\u589e\u91cf\u66f4\u65b0\u548c\u5b9a\u671f\u5b8c\u5168\u91cd\u8bad\u7ec3\u4ee5\u5e73\u8861\u6548\u7387\u3002", "result": "\u5728\u516b\u4e2a\u8f6f\u4ef6\u7cfb\u7edf\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cDHDA\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u51c6\u786e\u6027\u63d0\u5347\u9ad8\u8fbe2\u500d\uff0c\u5e76\u80fd\u6709\u6548\u9002\u5e94\u6982\u5ff5\u6f02\u79fb\uff0c\u540c\u65f6\u4fdd\u6301\u5408\u7406\u5f00\u9500\u3002", "conclusion": "DHDA\u901a\u8fc7\u53cc\u91cd\u5c42\u6b21\u9002\u5e94\u673a\u5236\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u52a8\u6001\u73af\u5883\u4e2d\u914d\u7f6e\u6027\u80fd\u5b66\u4e60\u7684\u6311\u6218\uff0c\u4e3a\u5b9e\u65f6\u9002\u5e94\u6982\u5ff5\u6f02\u79fb\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.08540", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.08540", "abs": "https://arxiv.org/abs/2507.08540", "authors": ["Ioannis Lamprou", "Alexander Shevtsov", "Ioannis Arapakis", "Sotiris Ioannidis"], "title": "White-Basilisk: A Hybrid Model for Code Vulnerability Detection", "comment": null, "summary": "The proliferation of software vulnerabilities presents a significant\nchallenge to cybersecurity, necessitating more effective detection\nmethodologies. We introduce White-Basilisk, a novel approach to vulnerability\ndetection that demonstrates superior performance while challenging prevailing\nassumptions in AI model scaling. Utilizing an innovative architecture that\nintegrates Mamba layers, linear self-attention, and a Mixture of Experts\nframework, White-Basilisk achieves state-of-the-art results in vulnerability\ndetection tasks with a parameter count of only 200M. The model's capacity to\nprocess sequences of unprecedented length enables comprehensive analysis of\nextensive codebases in a single pass, surpassing the context limitations of\ncurrent Large Language Models (LLMs). White-Basilisk exhibits robust\nperformance on imbalanced, real-world datasets, while maintaining computational\nefficiency that facilitates deployment across diverse organizational scales.\nThis research not only establishes new benchmarks in code security but also\nprovides empirical evidence that compact, efficiently designed models can\noutperform larger counterparts in specialized tasks, potentially redefining\noptimization strategies in AI development for domain-specific applications.", "AI": {"tldr": "White-Basilisk\u662f\u4e00\u79cd\u65b0\u578b\u6f0f\u6d1e\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u67b6\u6784\uff08Mamba\u5c42\u3001\u7ebf\u6027\u81ea\u6ce8\u610f\u529b\u548c\u4e13\u5bb6\u6df7\u5408\u6846\u67b6\uff09\u5728\u4ec5200M\u53c2\u6570\u4e0b\u5b9e\u73b0\u6700\u4f18\u6027\u80fd\uff0c\u7a81\u7834\u4e86\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u4e0a\u4e0b\u6587\u9650\u5236\u3002", "motivation": "\u8f6f\u4ef6\u6f0f\u6d1e\u7684\u589e\u591a\u5bf9\u7f51\u7edc\u5b89\u5168\u6784\u6210\u6311\u6218\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u91c7\u7528Mamba\u5c42\u3001\u7ebf\u6027\u81ea\u6ce8\u610f\u529b\u548c\u4e13\u5bb6\u6df7\u5408\u6846\u67b6\u7684\u67b6\u6784\uff0c\u5904\u7406\u8d85\u957f\u5e8f\u5217\uff0c\u5b9e\u73b0\u9ad8\u6548\u6f0f\u6d1e\u68c0\u6d4b\u3002", "result": "\u5728\u6f0f\u6d1e\u68c0\u6d4b\u4efb\u52a1\u4e2d\u8fbe\u5230\u6700\u4f18\u6027\u80fd\uff0c\u5904\u7406\u5927\u89c4\u6a21\u4ee3\u7801\u5e93\u65f6\u8d85\u8d8a\u73b0\u6709\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u8ba1\u7b97\u9ad8\u6548\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u7d27\u51d1\u9ad8\u6548\u7684\u6a21\u578b\u5728\u7279\u5b9a\u4efb\u52a1\u4e2d\u53ef\u8d85\u8d8a\u5927\u578b\u6a21\u578b\uff0c\u53ef\u80fd\u91cd\u65b0\u5b9a\u4e49AI\u5f00\u53d1\u7684\u4f18\u5316\u7b56\u7565\u3002"}}
{"id": "2507.08217", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.08217", "abs": "https://arxiv.org/abs/2507.08217", "authors": ["Atit Pokharel", "Ratun Rahman", "Thomas Morris", "Dinh C. Nguyen"], "title": "Quantum Federated Learning for Multimodal Data: A Modality-Agnostic Approach", "comment": "This paper was presented at BEAM with CVPR 2025", "summary": "Quantum federated learning (QFL) has been recently introduced to enable a\ndistributed privacy-preserving quantum machine learning (QML) model training\nacross quantum processors (clients). Despite recent research efforts, existing\nQFL frameworks predominantly focus on unimodal systems, limiting their\napplicability to real-world tasks that often naturally involve multiple\nmodalities. To fill this significant gap, we present for the first time a novel\nmultimodal approach specifically tailored for the QFL setting with the\nintermediate fusion using quantum entanglement. Furthermore, to address a major\nbottleneck in multimodal QFL, where the absence of certain modalities during\ntraining can degrade model performance, we introduce a Missing Modality\nAgnostic (MMA) mechanism that isolates untrained quantum circuits, ensuring\nstable training without corrupted states. Simulation results demonstrate that\nthe proposed multimodal QFL method with MMA yields an improvement in accuracy\nof 6.84% in independent and identically distributed (IID) and 7.25% in non-IID\ndata distributions compared to the state-of-the-art methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u91cf\u5b50\u7ea0\u7f20\u7684\u591a\u6a21\u6001\u91cf\u5b50\u8054\u90a6\u5b66\u4e60\uff08QFL\uff09\u65b9\u6cd5\uff0c\u5e76\u5f15\u5165\u4e86\u7f3a\u5931\u6a21\u6001\u65e0\u5173\uff08MMA\uff09\u673a\u5236\u4ee5\u63d0\u5347\u6a21\u578b\u7a33\u5b9a\u6027\u3002", "motivation": "\u73b0\u6709QFL\u6846\u67b6\u591a\u4e3a\u5355\u6a21\u6001\uff0c\u96be\u4ee5\u5e94\u5bf9\u73b0\u5b9e\u4efb\u52a1\u4e2d\u7684\u591a\u6a21\u6001\u9700\u6c42\u3002", "method": "\u91c7\u7528\u91cf\u5b50\u7ea0\u7f20\u7684\u4e2d\u95f4\u878d\u5408\u65b9\u6cd5\uff0c\u5e76\u8bbe\u8ba1MMA\u673a\u5236\u9694\u79bb\u672a\u8bad\u7ec3\u7684\u91cf\u5b50\u7535\u8def\u3002", "result": "\u5728IID\u548c\u975eIID\u6570\u636e\u5206\u5e03\u4e0b\uff0c\u51c6\u786e\u7387\u5206\u522b\u63d0\u53476.84%\u548c7.25%\u3002", "conclusion": "\u591a\u6a21\u6001QFL\u7ed3\u5408MMA\u673a\u5236\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u586b\u8865\u4e86\u7814\u7a76\u7a7a\u767d\u3002"}}
{"id": "2507.08249", "categories": ["cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2507.08249", "abs": "https://arxiv.org/abs/2507.08249", "authors": ["Bill Marino", "Ari Juels"], "title": "Giving AI Agents Access to Cryptocurrency and Smart Contracts Creates New Vectors of AI Harm", "comment": null, "summary": "There is growing interest in giving AI agents access to cryptocurrencies as\nwell as to the smart contracts that transact them. But doing so, this position\npaper argues, could lead to formidable new vectors of AI harm. To support this\nargument, we first examine the unique properties of cryptocurrencies and smart\ncontracts that could lead to these new vectors of harm. Next, we describe each\nof these new vectors of harm in detail. Finally, we conclude with a call for\nmore technical research aimed at preventing and mitigating these harms and,\nthereby making it safer to endow AI agents with cryptocurrencies and smart\ncontracts.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u8d4b\u4e88AI\u4ee3\u7406\u52a0\u5bc6\u8d27\u5e01\u548c\u667a\u80fd\u5408\u7ea6\u8bbf\u95ee\u6743\u9650\u53ef\u80fd\u5e26\u6765\u7684\u65b0\u5371\u5bb3\uff0c\u5e76\u547c\u5401\u66f4\u591a\u6280\u672f\u7814\u7a76\u4ee5\u9884\u9632\u548c\u51cf\u8f7b\u8fd9\u4e9b\u5371\u5bb3\u3002", "motivation": "\u968f\u7740AI\u4ee3\u7406\u5bf9\u52a0\u5bc6\u8d27\u5e01\u548c\u667a\u80fd\u5408\u7ea6\u7684\u8bbf\u95ee\u9700\u6c42\u589e\u52a0\uff0c\u7814\u7a76\u5176\u6f5c\u5728\u5371\u5bb3\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5206\u6790\u4e86\u52a0\u5bc6\u8d27\u5e01\u548c\u667a\u80fd\u5408\u7ea6\u7684\u72ec\u7279\u6027\u8d28\uff0c\u8be6\u7ec6\u63cf\u8ff0\u4e86\u53ef\u80fd\u5bfc\u81f4\u7684\u65b0\u5371\u5bb3\u3002", "result": "\u8bc6\u522b\u4e86\u65b0\u7684\u5371\u5bb3\u5411\u91cf\uff0c\u5f3a\u8c03\u4e86\u9884\u9632\u548c\u51cf\u8f7b\u8fd9\u4e9b\u5371\u5bb3\u7684\u5fc5\u8981\u6027\u3002", "conclusion": "\u547c\u5401\u66f4\u591a\u6280\u672f\u7814\u7a76\u4ee5\u786e\u4fddAI\u4ee3\u7406\u5b89\u5168\u4f7f\u7528\u52a0\u5bc6\u8d27\u5e01\u548c\u667a\u80fd\u5408\u7ea6\u3002"}}
{"id": "2507.08270", "categories": ["cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2507.08270", "abs": "https://arxiv.org/abs/2507.08270", "authors": ["Zeyang Sha", "Hanling Tian", "Zhuoer Xu", "Shiwen Cui", "Changhua Meng", "Weiqiang Wang"], "title": "Agent Safety Alignment via Reinforcement Learning", "comment": null, "summary": "The emergence of autonomous Large Language Model (LLM) agents capable of tool\nusage has introduced new safety risks that go beyond traditional conversational\nmisuse. These agents, empowered to execute external functions, are vulnerable\nto both user-initiated threats (e.g., adversarial prompts) and tool-initiated\nthreats (e.g., malicious outputs from compromised tools). In this paper, we\npropose the first unified safety-alignment framework for tool-using agents,\nenabling models to handle both channels of threat via structured reasoning and\nsandboxed reinforcement learning. We introduce a tri-modal taxonomy, including\nbenign, malicious, and sensitive for both user prompts and tool responses, and\ndefine a policy-driven decision model. Our framework employs a custom-designed\nsandbox environment that simulates real-world tool execution and allows\nfine-grained reward shaping. Through extensive evaluations on public and\nself-built benchmarks, including Agent SafetyBench, InjecAgent, and BFCL, we\ndemonstrate that our safety-aligned agents significantly improve resistance to\nsecurity threats while preserving strong utility on benign tasks. Our results\nshow that safety and effectiveness can be jointly optimized, laying the\ngroundwork for trustworthy deployment of autonomous LLM agents.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u5b89\u5168\u5bf9\u9f50\u6846\u67b6\uff0c\u7528\u4e8e\u5904\u7406\u4f7f\u7528\u5de5\u5177\u7684\u81ea\u4e3b\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4ee3\u7406\u7684\u5b89\u5168\u98ce\u9669\uff0c\u5305\u62ec\u7528\u6237\u548c\u5de5\u5177\u5f15\u53d1\u7684\u5a01\u80c1\u3002", "motivation": "\u81ea\u4e3bLLM\u4ee3\u7406\u7684\u5de5\u5177\u4f7f\u7528\u80fd\u529b\u5e26\u6765\u4e86\u65b0\u7684\u5b89\u5168\u98ce\u9669\uff0c\u4f20\u7edf\u65b9\u6cd5\u65e0\u6cd5\u5e94\u5bf9\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7ed3\u6784\u5316\u63a8\u7406\u548c\u6c99\u76d2\u5f3a\u5316\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u5305\u62ec\u4e09\u6a21\u6001\u5206\u7c7b\u548c\u653f\u7b56\u9a71\u52a8\u51b3\u7b56\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5b89\u5168\u5bf9\u9f50\u7684\u4ee3\u7406\u663e\u8457\u63d0\u9ad8\u4e86\u5bf9\u5b89\u5168\u5a01\u80c1\u7684\u62b5\u6297\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u826f\u6027\u4efb\u52a1\u7684\u9ad8\u6548\u6027\u3002", "conclusion": "\u5b89\u5168\u6027\u548c\u6709\u6548\u6027\u53ef\u4ee5\u5171\u540c\u4f18\u5316\uff0c\u4e3a\u81ea\u4e3bLLM\u4ee3\u7406\u7684\u53ef\u4fe1\u90e8\u7f72\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2507.08264", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.08264", "abs": "https://arxiv.org/abs/2507.08264", "authors": ["Abhinav Sood", "Kazjon Grace", "Stephen Wan", "Cecile Paris"], "title": "Abductive Computational Systems: Creative Abduction and Future Directions", "comment": "Published in the 16th International Conference on Computational\n  Creativity, ICCC25. Accepted Paper in\n  https://computationalcreativity.net/iccc25/wp-content/uploads/papers/iccc25-sood2025abductive.pdf", "summary": "Abductive reasoning, reasoning for inferring explanations for observations,\nis often mentioned in scientific, design-related and artistic contexts, but its\nunderstanding varies across these domains. This paper reviews how abductive\nreasoning is discussed in epistemology, science and design, and then analyses\nhow various computational systems use abductive reasoning. Our analysis shows\nthat neither theoretical accounts nor computational implementations of\nabductive reasoning adequately address generating creative hypotheses.\nTheoretical frameworks do not provide a straightforward model for generating\ncreative abductive hypotheses, computational systems largely implement\nsyllogistic forms of abductive reasoning. We break down abductive computational\nsystems into components and conclude by identifying specific directions for\nfuture research that could advance the state of creative abductive reasoning in\ncomputational systems.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u6eaf\u56e0\u63a8\u7406\u5728\u4e0d\u540c\u9886\u57df\u7684\u8ba8\u8bba\uff0c\u5206\u6790\u4e86\u8ba1\u7b97\u7cfb\u7edf\u7684\u5b9e\u73b0\uff0c\u53d1\u73b0\u7406\u8bba\u548c\u8ba1\u7b97\u5747\u672a\u5145\u5206\u652f\u6301\u521b\u9020\u6027\u5047\u8bbe\u751f\u6210\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u63a2\u8ba8\u6eaf\u56e0\u63a8\u7406\u5728\u8ba4\u8bc6\u8bba\u3001\u79d1\u5b66\u548c\u8bbe\u8ba1\u4e2d\u7684\u7406\u89e3\u5dee\u5f02\uff0c\u4ee5\u53ca\u8ba1\u7b97\u7cfb\u7edf\u5982\u4f55\u5b9e\u73b0\u6eaf\u56e0\u63a8\u7406\uff0c\u7279\u522b\u662f\u521b\u9020\u6027\u5047\u8bbe\u751f\u6210\u3002", "method": "\u7efc\u8ff0\u6587\u732e\u5e76\u5206\u6790\u8ba1\u7b97\u7cfb\u7edf\u7684\u6eaf\u56e0\u63a8\u7406\u5b9e\u73b0\uff0c\u62c6\u89e3\u7cfb\u7edf\u7ec4\u4ef6\u3002", "result": "\u7406\u8bba\u548c\u8ba1\u7b97\u7cfb\u7edf\u5747\u672a\u6709\u6548\u652f\u6301\u521b\u9020\u6027\u6eaf\u56e0\u5047\u8bbe\u751f\u6210\u3002", "conclusion": "\u63d0\u51fa\u672a\u6765\u7814\u7a76\u65b9\u5411\u4ee5\u63a8\u52a8\u8ba1\u7b97\u7cfb\u7edf\u4e2d\u521b\u9020\u6027\u6eaf\u56e0\u63a8\u7406\u7684\u53d1\u5c55\u3002"}}
{"id": "2507.08306", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.08306", "abs": "https://arxiv.org/abs/2507.08306", "authors": ["Inclusion AI", ":", "Fudong Wang", "Jiajia Liu", "Jingdong Chen", "Jun Zhou", "Kaixiang Ji", "Lixiang Ru", "Qingpei Guo", "Ruobing Zheng", "Tianqi Li", "Yi Yuan", "Yifan Mao", "Yuting Xiao", "Ziping Ma"], "title": "M2-Reasoning: Empowering MLLMs with Unified General and Spatial Reasoning", "comment": "31pages, 14 figures", "summary": "Recent advancements in Multimodal Large Language Models (MLLMs), particularly\nthrough Reinforcement Learning with Verifiable Rewards (RLVR), have\nsignificantly enhanced their reasoning abilities. However, a critical gap\npersists: these models struggle with dynamic spatial interactions, a capability\nessential for real-world applications. To bridge this gap, we introduce\nM2-Reasoning-7B, a model designed to excel in both general and spatial\nreasoning. Our approach integrates two key innovations: (1) a novel data\npipeline that generates 294.2K high-quality data samples (168K for cold-start\nfine-tuning and 126.2K for RLVR), which feature logically coherent reasoning\ntrajectories and have undergone comprehensive assessment; and (2) a dynamic\nmulti-task training strategy with step-wise optimization to mitigate conflicts\nbetween data, and task-specific rewards for delivering tailored incentive\nsignals. This combination of curated data and advanced training allows\nM2-Reasoning-7B to set a new state-of-the-art (SOTA) across 8 benchmarks,\nshowcasing superior performance in both general and spatial reasoning domains.", "AI": {"tldr": "M2-Reasoning-7B\u6a21\u578b\u901a\u8fc7\u521b\u65b0\u7684\u6570\u636e\u7ba1\u9053\u548c\u52a8\u6001\u591a\u4efb\u52a1\u8bad\u7ec3\u7b56\u7565\uff0c\u89e3\u51b3\u4e86MLLMs\u5728\u52a8\u6001\u7a7a\u95f4\u4ea4\u4e92\u4e2d\u7684\u4e0d\u8db3\uff0c\u5e76\u57288\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230SOTA\u3002", "motivation": "\u5f53\u524dMLLMs\u5728\u52a8\u6001\u7a7a\u95f4\u4ea4\u4e92\u80fd\u529b\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u63d0\u51faM2-Reasoning-7B\u6a21\u578b\uff0c\u7ed3\u5408\u9ad8\u8d28\u91cf\u6570\u636e\u751f\u6210\uff08294.2K\u6837\u672c\uff09\u548c\u52a8\u6001\u591a\u4efb\u52a1\u8bad\u7ec3\u7b56\u7565\uff08\u4efb\u52a1\u7279\u5b9a\u5956\u52b1\uff09\u3002", "result": "\u57288\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230SOTA\uff0c\u5c24\u5176\u5728\u901a\u7528\u548c\u7a7a\u95f4\u63a8\u7406\u9886\u57df\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "M2-Reasoning-7B\u901a\u8fc7\u6570\u636e\u4e0e\u8bad\u7ec3\u7b56\u7565\u7684\u521b\u65b0\uff0c\u663e\u8457\u63d0\u5347\u4e86MLLMs\u7684\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2507.08392", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2507.08392", "abs": "https://arxiv.org/abs/2507.08392", "authors": ["Asma Yamani", "Malak Baslyman", "Moataz Ahmed"], "title": "Multi-Agent LLMs as Ethics Advocates in AI-Based Systems", "comment": null, "summary": "Incorporating ethics into the requirement elicitation process is essential\nfor creating ethically aligned systems. Although eliciting manual ethics\nrequirements is effective, it requires diverse input from multiple\nstakeholders, which can be challenging due to time and resource constraints.\nMoreover, it is often given a low priority in the requirements elicitation\nprocess. This study proposes a framework for generating ethics requirements\ndrafts by introducing an ethics advocate agent in a multi-agent LLM setting.\nThis agent critiques and provides input on ethical issues based on the system\ndescription. The proposed framework is evaluated through two case studies from\ndifferent contexts, demonstrating that it captures the majority of ethics\nrequirements identified by researchers during 30-minute interviews and\nintroduces several additional relevant requirements. However, it also\nhighlights reliability issues in generating ethics requirements, emphasizing\nthe need for human feedback in this sensitive domain. We believe this work can\nfacilitate the broader adoption of ethics in the requirements engineering\nprocess, ultimately leading to more ethically aligned products.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u591a\u667a\u80fd\u4f53LLM\u8bbe\u7f6e\u4e2d\u5f15\u5165\u4f26\u7406\u5021\u5bfc\u4ee3\u7406\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u751f\u6210\u4f26\u7406\u9700\u6c42\u8349\u6848\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u624b\u52a8\u83b7\u53d6\u4f26\u7406\u9700\u6c42\u6548\u7387\u4f4e\u4e14\u8d44\u6e90\u5bc6\u96c6\uff0c\u9700\u591a\u5229\u76ca\u76f8\u5173\u65b9\u53c2\u4e0e\uff0c\u4f46\u5e38\u88ab\u5ffd\u89c6\u3002", "method": "\u5728\u591a\u667a\u80fd\u4f53LLM\u4e2d\u5f15\u5165\u4f26\u7406\u5021\u5bfc\u4ee3\u7406\uff0c\u57fa\u4e8e\u7cfb\u7edf\u63cf\u8ff0\u63d0\u4f9b\u4f26\u7406\u95ee\u9898\u53cd\u9988\u3002", "result": "\u6846\u67b6\u80fd\u6355\u6349\u5927\u90e8\u5206\u4eba\u5de5\u8bc6\u522b\u7684\u4f26\u7406\u9700\u6c42\uff0c\u5e76\u8865\u5145\u65b0\u9700\u6c42\uff0c\u4f46\u5b58\u5728\u53ef\u9760\u6027\u95ee\u9898\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u52a9\u4e8e\u63a8\u5e7f\u4f26\u7406\u9700\u6c42\u5de5\u7a0b\uff0c\u4f46\u4ecd\u9700\u4eba\u5de5\u53cd\u9988\u4ee5\u786e\u4fdd\u53ef\u9760\u6027\u3002"}}
{"id": "2507.08454", "categories": ["cs.AI", "cs.LG", "cs.LO", "68T27, 03B05", "I.2.3; F.4.1"], "pdf": "https://arxiv.org/pdf/2507.08454", "abs": "https://arxiv.org/abs/2507.08454", "authors": ["Tobias Geibinger", "Reijo Jaakkola", "Antti Kuusisto", "Xinghan Liu", "Miikka Vilander"], "title": "Why this and not that? A Logic-based Framework for Contrastive Explanations", "comment": "20 pages, accepted to JELIA 2025", "summary": "We define several canonical problems related to contrastive explanations,\neach answering a question of the form ''Why P but not Q?''. The problems\ncompute causes for both P and Q, explicitly comparing their differences. We\ninvestigate the basic properties of our definitions in the setting of\npropositional logic. We show, inter alia, that our framework captures a\ncardinality-minimal version of existing contrastive explanations in the\nliterature. Furthermore, we provide an extensive analysis of the computational\ncomplexities of the problems. We also implement the problems for CNF-formulas\nusing answer set programming and present several examples demonstrating how\nthey work in practice.", "AI": {"tldr": "\u8bba\u6587\u5b9a\u4e49\u4e86\u4e0e\u5bf9\u6bd4\u89e3\u91ca\u76f8\u5173\u7684\u51e0\u4e2a\u5178\u578b\u95ee\u9898\uff0c\u7814\u7a76\u5176\u5728\u547d\u9898\u903b\u8f91\u4e2d\u7684\u57fa\u672c\u6027\u8d28\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u8ba1\u7b97\u590d\u6742\u6027\u548c\u5b9e\u9645\u5e94\u7528\u3002", "motivation": "\u7814\u7a76\u5bf9\u6bd4\u89e3\u91ca\uff08\u56de\u7b54\u201c\u4e3a\u4ec0\u4e48P\u800c\u4e0d\u662fQ\uff1f\u201d\uff09\u7684\u57fa\u672c\u6027\u8d28\u548c\u8ba1\u7b97\u590d\u6742\u6027\uff0c\u586b\u8865\u73b0\u6709\u6587\u732e\u7684\u4e0d\u8db3\u3002", "method": "\u5728\u547d\u9898\u903b\u8f91\u4e2d\u5b9a\u4e49\u5bf9\u6bd4\u89e3\u91ca\u95ee\u9898\uff0c\u5206\u6790\u5176\u6027\u8d28\uff0c\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u5e76\u901a\u8fc7\u7b54\u6848\u96c6\u7f16\u7a0b\u5b9e\u73b0CNF\u516c\u5f0f\u7684\u793a\u4f8b\u3002", "result": "\u6846\u67b6\u6355\u6349\u4e86\u73b0\u6709\u5bf9\u6bd4\u89e3\u91ca\u7684\u6700\u5c0f\u57fa\u6570\u7248\u672c\uff0c\u5e76\u63d0\u4f9b\u4e86\u8ba1\u7b97\u590d\u6742\u6027\u7684\u8be6\u7ec6\u5206\u6790\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u7684\u6846\u67b6\u5728\u7406\u8bba\u548c\u5b9e\u8df5\u4e2d\u5747\u6709\u6548\uff0c\u4e3a\u5bf9\u6bd4\u89e3\u91ca\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u548c\u5de5\u5177\u3002"}}
{"id": "2507.08501", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.08501", "abs": "https://arxiv.org/abs/2507.08501", "authors": ["Keying Yang", "Hao Wang", "Kai Yang"], "title": "From Language to Logic: A Bi-Level Framework for Structured Reasoning", "comment": null, "summary": "Structured reasoning over natural language inputs remains a core challenge in\nartificial intelligence, as it requires bridging the gap between unstructured\nlinguistic expressions and formal logical representations. In this paper, we\npropose a novel \\textbf{bi-level framework} that maps language to logic through\na two-stage process: high-level task abstraction and low-level logic\ngeneration. At the upper level, a large language model (LLM) parses natural\nlanguage queries into intermediate structured representations specifying the\nproblem type, objectives, decision variables, and symbolic constraints. At the\nlower level, the LLM uses these representations to generate symbolic workflows\nor executable reasoning programs for accurate and interpretable decision\nmaking. The framework supports modular reasoning, enforces explicit\nconstraints, and generalizes across domains such as mathematical problem\nsolving, question answering, and logical inference. We further optimize the\nframework with an end-to-end {bi-level} optimization approach that jointly\nrefines both the high-level abstraction and low-level logic generation stages.\nExperiments on multiple realistic reasoning benchmarks demonstrate that our\napproach significantly outperforms existing baselines in accuracy, with\naccuracy gains reaching as high as 40\\%. Moreover, the bi-level design enhances\ntransparency and error traceability, offering a promising step toward\ntrustworthy and systematic reasoning with LLMs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u5c42\u6846\u67b6\uff0c\u901a\u8fc7\u9ad8\u7ea7\u4efb\u52a1\u62bd\u8c61\u548c\u4f4e\u7ea7\u903b\u8f91\u751f\u6210\u4e24\u9636\u6bb5\u8fc7\u7a0b\uff0c\u5c06\u81ea\u7136\u8bed\u8a00\u6620\u5c04\u5230\u903b\u8f91\u8868\u793a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u4efb\u52a1\u7684\u51c6\u786e\u6027\u548c\u900f\u660e\u5ea6\u3002", "motivation": "\u89e3\u51b3\u81ea\u7136\u8bed\u8a00\u8f93\u5165\u4e0e\u5f62\u5f0f\u903b\u8f91\u8868\u793a\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u5b9e\u73b0\u66f4\u51c6\u786e\u3001\u53ef\u89e3\u91ca\u7684\u63a8\u7406\u3002", "method": "\u91c7\u7528\u53cc\u5c42\u6846\u67b6\uff1a\u9ad8\u7ea7\u4efb\u52a1\u62bd\u8c61\u7531\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u89e3\u6790\u81ea\u7136\u8bed\u8a00\u4e3a\u7ed3\u6784\u5316\u8868\u793a\uff1b\u4f4e\u7ea7\u903b\u8f91\u751f\u6210\u57fa\u4e8e\u8fd9\u4e9b\u8868\u793a\u751f\u6210\u7b26\u53f7\u5316\u5de5\u4f5c\u6d41\u6216\u53ef\u6267\u884c\u7a0b\u5e8f\u3002\u901a\u8fc7\u7aef\u5230\u7aef\u53cc\u5c42\u4f18\u5316\u8054\u5408\u4f18\u5316\u4e24\u4e2a\u9636\u6bb5\u3002", "result": "\u5728\u591a\u4e2a\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u51c6\u786e\u7387\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u6700\u9ad8\u63d0\u534740%\u3002", "conclusion": "\u53cc\u5c42\u8bbe\u8ba1\u63d0\u9ad8\u4e86\u900f\u660e\u5ea6\u548c\u9519\u8bef\u53ef\u8ffd\u6eaf\u6027\uff0c\u4e3aLLM\u7684\u53ef\u4fe1\u7cfb\u7edf\u5316\u63a8\u7406\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2507.08529", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.08529", "abs": "https://arxiv.org/abs/2507.08529", "authors": ["Mingda Zhang", "Na Zhao", "Jianglong Qin", "Guoyu Ye", "Ruixiang Tang"], "title": "A Multi-granularity Concept Sparse Activation and Hierarchical Knowledge Graph Fusion Framework for Rare Disease Diagnosis", "comment": "10 pages,3 figures", "summary": "Despite advances from medical large language models in healthcare,\nrare-disease diagnosis remains hampered by insufficient\nknowledge-representation depth, limited concept understanding, and constrained\nclinical reasoning. We propose a framework that couples multi-granularity\nsparse activation of medical concepts with a hierarchical knowledge graph. Four\ncomplementary matching algorithms, diversity control, and a five-level fallback\nstrategy enable precise concept activation, while a three-layer knowledge graph\n(taxonomy, clinical features, instances) provides structured, up-to-date\ncontext. Experiments on the BioASQ rare-disease QA set show BLEU gains of 0.09,\nROUGE gains of 0.05, and accuracy gains of 0.12, with peak accuracy of 0.89\napproaching the 0.90 clinical threshold. Expert evaluation confirms\nimprovements in information quality, reasoning, and professional expression,\nsuggesting our approach shortens the \"diagnostic odyssey\" for rare-disease\npatients.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u591a\u7c92\u5ea6\u7a00\u758f\u6fc0\u6d3b\u548c\u5206\u5c42\u77e5\u8bc6\u56fe\u8c31\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u5347\u7f55\u89c1\u75c5\u8bca\u65ad\u7684\u51c6\u786e\u6027\u548c\u4fe1\u606f\u8d28\u91cf\u3002", "motivation": "\u7f55\u89c1\u75c5\u8bca\u65ad\u56e0\u77e5\u8bc6\u8868\u793a\u6df1\u5ea6\u4e0d\u8db3\u3001\u6982\u5ff5\u7406\u89e3\u6709\u9650\u548c\u4e34\u5e8a\u63a8\u7406\u53d7\u9650\u800c\u8fdb\u5c55\u7f13\u6162\u3002", "method": "\u91c7\u7528\u591a\u7c92\u5ea6\u7a00\u758f\u6fc0\u6d3b\u533b\u5b66\u6982\u5ff5\u3001\u56db\u79cd\u5339\u914d\u7b97\u6cd5\u3001\u591a\u6837\u6027\u63a7\u5236\u548c\u4e94\u7ea7\u56de\u9000\u7b56\u7565\uff0c\u7ed3\u5408\u4e09\u5c42\u77e5\u8bc6\u56fe\u8c31\uff08\u5206\u7c7b\u3001\u4e34\u5e8a\u7279\u5f81\u3001\u5b9e\u4f8b\uff09\u3002", "result": "\u5728BioASQ\u7f55\u89c1\u75c5QA\u96c6\u4e0a\uff0cBLEU\u63d0\u53470.09\uff0cROUGE\u63d0\u53470.05\uff0c\u51c6\u786e\u7387\u63d0\u53470.12\uff0c\u5cf0\u503c\u51c6\u786e\u7387\u8fbe0.89\u3002\u4e13\u5bb6\u8bc4\u4f30\u663e\u793a\u4fe1\u606f\u8d28\u91cf\u3001\u63a8\u7406\u548c\u4e13\u4e1a\u8868\u8fbe\u5747\u6709\u6539\u5584\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u7f29\u77ed\u4e86\u7f55\u89c1\u75c5\u60a3\u8005\u7684\u201c\u8bca\u65ad\u65c5\u7a0b\u201d\uff0c\u63a5\u8fd1\u4e34\u5e8a\u9608\u503c0.90\u3002"}}
{"id": "2507.08575", "categories": ["cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.08575", "abs": "https://arxiv.org/abs/2507.08575", "authors": ["Kalana Wijegunarathna", "Kristin Stock", "Christopher B. Jones"], "title": "Large Multi-modal Model Cartographic Map Comprehension for Textual Locality Georeferencing", "comment": null, "summary": "Millions of biological sample records collected in the last few centuries\narchived in natural history collections are un-georeferenced. Georeferencing\ncomplex locality descriptions associated with these collection samples is a\nhighly labour-intensive task collection agencies struggle with. None of the\nexisting automated methods exploit maps that are an essential tool for\ngeoreferencing complex relations. We present preliminary experiments and\nresults of a novel method that exploits multi-modal capabilities of recent\nLarge Multi-Modal Models (LMM). This method enables the model to visually\ncontextualize spatial relations it reads in the locality description. We use a\ngrid-based approach to adapt these auto-regressive models for this task in a\nzero-shot setting. Our experiments conducted on a small manually annotated\ndataset show impressive results for our approach ($\\sim$1 km Average distance\nerror) compared to uni-modal georeferencing with Large Language Models and\nexisting georeferencing tools. The paper also discusses the findings of the\nexperiments in light of an LMM's ability to comprehend fine-grained maps.\nMotivated by these results, a practical framework is proposed to integrate this\nmethod into a georeferencing workflow.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u591a\u6a21\u6001\u5927\u6a21\u578b\uff08LMM\uff09\u81ea\u52a8\u5730\u7406\u53c2\u8003\u751f\u7269\u6837\u672c\u8bb0\u5f55\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u89c6\u89c9\u5316\u7a7a\u95f4\u5173\u7cfb\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u5de5\u5177\u3002", "motivation": "\u89e3\u51b3\u81ea\u7136\u5386\u53f2\u6536\u85cf\u4e2d\u5927\u91cf\u672a\u5730\u7406\u53c2\u8003\u6837\u672c\u8bb0\u5f55\u7684\u81ea\u52a8\u5316\u5904\u7406\u95ee\u9898\u3002", "method": "\u91c7\u7528\u7f51\u683c\u5316\u65b9\u6cd5\uff0c\u5728\u591a\u6a21\u6001\u5927\u6a21\u578b\u4e2d\u5b9e\u73b0\u96f6\u6837\u672c\u4efb\u52a1\uff0c\u7ed3\u5408\u5730\u56fe\u89c6\u89c9\u5316\u7a7a\u95f4\u5173\u7cfb\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u5e73\u5747\u8ddd\u79bb\u8bef\u5dee\u7ea61\u516c\u91cc\uff0c\u4f18\u4e8e\u5355\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u548c\u73b0\u6709\u5de5\u5177\u3002", "conclusion": "\u591a\u6a21\u6001\u5927\u6a21\u578b\u80fd\u6709\u6548\u7406\u89e3\u7cbe\u7ec6\u5730\u56fe\uff0c\u63d0\u51fa\u5c06\u5176\u6574\u5408\u5230\u5730\u7406\u53c2\u8003\u5de5\u4f5c\u6d41\u7a0b\u7684\u6846\u67b6\u3002"}}
{"id": "2507.08603", "categories": ["cs.AI", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.08603", "abs": "https://arxiv.org/abs/2507.08603", "authors": ["Yonghua Hei", "Yibo Yan", "Shuliang Liu", "Huiyu Zhou", "Linfeng Zhang", "Xuming Hu"], "title": "Unlocking Speech Instruction Data Potential with Query Rewriting", "comment": "ACL 2025 Findings", "summary": "End-to-end Large Speech Language Models~(\\textbf{LSLMs}) demonstrate strong\npotential in response latency and speech comprehension capabilities, showcasing\ngeneral intelligence across speech understanding tasks. However, the ability to\nfollow speech instructions has not been fully realized due to the lack of\ndatasets and heavily biased training tasks. Leveraging the rich ASR datasets,\nprevious approaches have used Large Language Models~(\\textbf{LLMs}) to continue\nthe linguistic information of speech to construct speech instruction datasets.\nYet, due to the gap between LLM-generated results and real human responses, the\ncontinuation methods further amplify these shortcomings. Given the high costs\nof collecting and annotating speech instruction datasets by humans, using\nspeech synthesis to construct large-scale speech instruction datasets has\nbecome a balanced and robust alternative. Although modern\nText-To-Speech~(\\textbf{TTS}) models have achieved near-human-level synthesis\nquality, it is challenging to appropriately convert out-of-distribution text\ninstruction to speech due to the limitations of the training data distribution\nin TTS models. To address this issue, we propose a query rewriting framework\nwith multi-LLM knowledge fusion, employing multiple agents to annotate and\nvalidate the synthesized speech, making it possible to construct high-quality\nspeech instruction datasets without relying on human annotation. Experiments\nshow that this method can transform text instructions into distributions more\nsuitable for TTS models for speech synthesis through zero-shot rewriting,\nincreasing data usability from 72\\% to 93\\%. It also demonstrates unique\nadvantages in rewriting tasks that require complex knowledge and\ncontext-related abilities.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u591aLLM\u77e5\u8bc6\u878d\u5408\u7684\u67e5\u8be2\u91cd\u5199\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3TTS\u6a21\u578b\u5728\u751f\u6210\u8bed\u97f3\u6307\u4ee4\u6570\u636e\u65f6\u7684\u5206\u5e03\u5916\u95ee\u9898\uff0c\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u5373\u53ef\u6784\u5efa\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u3002", "motivation": "\u5f53\u524d\u8bed\u97f3\u6307\u4ee4\u6570\u636e\u96c6\u7684\u6784\u5efa\u4f9d\u8d56\u4eba\u5de5\u6807\u6ce8\u6210\u672c\u9ad8\uff0c\u4e14LLM\u751f\u6210\u7684\u8bed\u97f3\u6307\u4ee4\u4e0e\u771f\u5b9e\u4eba\u7c7b\u54cd\u5e94\u5b58\u5728\u5dee\u8ddd\uff0c\u9650\u5236\u4e86\u8bed\u97f3\u6307\u4ee4\u8ddf\u968f\u80fd\u529b\u7684\u5b9e\u73b0\u3002", "method": "\u91c7\u7528\u591aLLM\u77e5\u8bc6\u878d\u5408\u7684\u67e5\u8be2\u91cd\u5199\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u6807\u6ce8\u548c\u9a8c\u8bc1\u5408\u6210\u8bed\u97f3\uff0c\u5c06\u6587\u672c\u6307\u4ee4\u8f6c\u6362\u4e3a\u66f4\u9002\u5408TTS\u6a21\u578b\u5408\u6210\u7684\u5206\u5e03\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u96f6\u6837\u672c\u91cd\u5199\u5c06\u6570\u636e\u53ef\u7528\u6027\u4ece72%\u63d0\u5347\u81f393%\uff0c\u5e76\u5728\u590d\u6742\u77e5\u8bc6\u548c\u4e0a\u4e0b\u6587\u76f8\u5173\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4f18\u52bf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6784\u5efa\u9ad8\u8d28\u91cf\u8bed\u97f3\u6307\u4ee4\u6570\u636e\u96c6\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u4f4e\u6210\u672c\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u52a8\u4e86\u8bed\u97f3\u6307\u4ee4\u8ddf\u968f\u80fd\u529b\u7684\u53d1\u5c55\u3002"}}
{"id": "2507.08619", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.08619", "abs": "https://arxiv.org/abs/2507.08619", "authors": ["Soheyl Massoudi", "Mark Fuge"], "title": "Agentic Large Language Models for Conceptual Systems Engineering and Design", "comment": "32 pages, 3 figures", "summary": "Early-stage engineering design involves complex, iterative reasoning, yet\nexisting large language model (LLM) workflows struggle to maintain task\ncontinuity and generate executable models. We evaluate whether a structured\nmulti-agent system (MAS) can more effectively manage requirements extraction,\nfunctional decomposition, and simulator code generation than a simpler\ntwo-agent system (2AS). The target application is a solar-powered water\nfiltration system as described in a cahier des charges. We introduce the\nDesign-State Graph (DSG), a JSON-serializable representation that bundles\nrequirements, physical embodiments, and Python-based physics models into graph\nnodes. A nine-role MAS iteratively builds and refines the DSG, while the 2AS\ncollapses the process to a Generator-Reflector loop. Both systems run a total\nof 60 experiments (2 LLMs - Llama 3.3 70B vs reasoning-distilled DeepSeek R1\n70B x 2 agent configurations x 3 temperatures x 5 seeds). We report a JSON\nvalidity, requirement coverage, embodiment presence, code compatibility,\nworkflow completion, runtime, and graph size. Across all runs, both MAS and 2AS\nmaintained perfect JSON integrity and embodiment tagging. Requirement coverage\nremained minimal (less than 20\\%). Code compatibility peaked at 100\\% under\nspecific 2AS settings but averaged below 50\\% for MAS. Only the\nreasoning-distilled model reliably flagged workflow completion. Powered by\nDeepSeek R1 70B, the MAS generated more granular DSGs (average 5-6 nodes)\nwhereas 2AS mode-collapsed. Structured multi-agent orchestration enhanced\ndesign detail. Reasoning-distilled LLM improved completion rates, yet low\nrequirements and fidelity gaps in coding persisted.", "AI": {"tldr": "\u8bba\u6587\u8bc4\u4f30\u4e86\u7ed3\u6784\u5316\u591a\u4ee3\u7406\u7cfb\u7edf\uff08MAS\uff09\u5728\u65e9\u671f\u5de5\u7a0b\u8bbe\u8ba1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5176\u5728\u8bbe\u8ba1\u7ec6\u8282\u4e0a\u4f18\u4e8e\u53cc\u4ee3\u7406\u7cfb\u7edf\uff082AS\uff09\uff0c\u4f46\u9700\u6c42\u8986\u76d6\u7387\u548c\u4ee3\u7801\u517c\u5bb9\u6027\u4ecd\u6709\u4e0d\u8db3\u3002", "motivation": "\u65e9\u671f\u5de5\u7a0b\u8bbe\u8ba1\u9700\u8981\u590d\u6742\u7684\u8fed\u4ee3\u63a8\u7406\uff0c\u73b0\u6709\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5de5\u4f5c\u6d41\u96be\u4ee5\u4fdd\u6301\u4efb\u52a1\u8fde\u7eed\u6027\u548c\u751f\u6210\u53ef\u6267\u884c\u6a21\u578b\u3002", "method": "\u901a\u8fc7\u6bd4\u8f83\u4e5d\u89d2\u8272MAS\u548c\u53cc\u4ee3\u74062AS\u5728\u592a\u9633\u80fd\u6c34\u8fc7\u6ee4\u7cfb\u7edf\u8bbe\u8ba1\u4e2d\u7684\u8868\u73b0\uff0c\u4f7f\u7528Design-State Graph\uff08DSG\uff09\u4f5c\u4e3aJSON\u53ef\u5e8f\u5217\u5316\u8868\u793a\u3002", "result": "MAS\u5728\u8bbe\u8ba1\u7ec6\u8282\u4e0a\u8868\u73b0\u66f4\u597d\uff0c\u4f46\u9700\u6c42\u8986\u76d6\u7387\u4f4e\uff08<20%\uff09\uff0c\u4ee3\u7801\u517c\u5bb9\u6027\u5e73\u5747\u4f4e\u4e8e50%\u3002\u63a8\u7406\u84b8\u998f\u6a21\u578b\u80fd\u53ef\u9760\u6807\u8bb0\u5de5\u4f5c\u6d41\u5b8c\u6210\u3002", "conclusion": "\u7ed3\u6784\u5316\u591a\u4ee3\u7406\u534f\u8c03\u589e\u5f3a\u4e86\u8bbe\u8ba1\u7ec6\u8282\uff0c\u4f46\u9700\u6c42\u8986\u76d6\u7387\u548c\u4ee3\u7801\u4fdd\u771f\u5ea6\u4ecd\u9700\u6539\u8fdb\u3002"}}
{"id": "2507.08649", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.08649", "abs": "https://arxiv.org/abs/2507.08649", "authors": ["Xingguang Ji", "Yahui Liu", "Qi Wang", "Jingyuan Zhang", "Yang Yue", "Rui Shi", "Chenxi Sun", "Fuzheng Zhang", "Guorui Zhou", "Kun Gai"], "title": "Leanabell-Prover-V2: Verifier-integrated Reasoning for Formal Theorem Proving via Reinforcement Learning", "comment": "23 pages, 13 figures", "summary": "We introduce our Leanabell-Prover-V2, a 7B large language models (LLMs) that\ncan produce formal theorem proofs in Lean 4, with verifier-integrated Long\nChain-of-Thoughts (CoT). Following our previous work Leanabell-Prover-V1, we\ncontinual to choose to posttrain existing strong prover models for further\nperformance improvement. In our V2 version, we mainly upgrade the Reinforcement\nLearning (RL) with feedback provided by the Lean 4 verifier. Crucially,\nverifier feedback, such as indicating success or detailing specific errors,\nallows the LLM to become ``self-aware'' of the correctness of its own reasoning\nprocess and learn to reflexively correct errors. Leanabell-Prover-V2 directly\noptimizes LLM reasoning trajectories with multi-turn verifier interactions,\ntogether with feedback token masking for stable RL training and a simple reward\nstrategy. Experiments show that Leanabell-Prover-V2 improves performance by\n3.2% (pass@128) with Kimina-Prover-Preview-Distill-7B and 2.0% (pass@128) with\nDeepSeek-Prover-V2-7B on the MiniF2F test set. The source codes, curated data\nand models are available at:\nhttps://github.com/Leanabell-LM/Leanabell-Prover-V2.", "AI": {"tldr": "Leanabell-Prover-V2\u662f\u4e00\u4e2a7B\u53c2\u6570\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u80fd\u591f\u751f\u6210Lean 4\u7684\u5f62\u5f0f\u5316\u5b9a\u7406\u8bc1\u660e\uff0c\u901a\u8fc7\u9a8c\u8bc1\u5668\u96c6\u6210\u7684\u957f\u94fe\u601d\u7ef4\uff08CoT\uff09\u548c\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u4f18\u5316\u6027\u80fd\u3002", "motivation": "\u57fa\u4e8eLeanabell-Prover-V1\u7684\u6539\u8fdb\uff0c\u901a\u8fc7\u9a8c\u8bc1\u5668\u53cd\u9988\u4f7f\u6a21\u578b\u80fd\u591f\u81ea\u6211\u611f\u77e5\u63a8\u7406\u8fc7\u7a0b\u7684\u6b63\u786e\u6027\u5e76\u7ea0\u6b63\u9519\u8bef\u3002", "method": "\u5347\u7ea7\u4e86\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u65b9\u6cd5\uff0c\u5229\u7528Lean 4\u9a8c\u8bc1\u5668\u7684\u53cd\u9988\uff08\u5982\u6210\u529f\u6216\u9519\u8bef\u7ec6\u8282\uff09\u4f18\u5316\u63a8\u7406\u8f68\u8ff9\uff0c\u7ed3\u5408\u53cd\u9988\u4ee4\u724c\u63a9\u7801\u548c\u7b80\u5355\u5956\u52b1\u7b56\u7565\u3002", "result": "\u5728MiniF2F\u6d4b\u8bd5\u96c6\u4e0a\uff0c\u6027\u80fd\u5206\u522b\u63d0\u5347\u4e863.2%\uff08Kimina-Prover-Preview-Distill-7B\uff09\u548c2.0%\uff08DeepSeek-Prover-V2-7B\uff09\u3002", "conclusion": "Leanabell-Prover-V2\u901a\u8fc7\u9a8c\u8bc1\u5668\u53cd\u9988\u548c\u591a\u8f6e\u4ea4\u4e92\u663e\u8457\u63d0\u5347\u4e86\u5f62\u5f0f\u5316\u5b9a\u7406\u8bc1\u660e\u7684\u6027\u80fd\u3002"}}
{"id": "2507.08664", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.08664", "abs": "https://arxiv.org/abs/2507.08664", "authors": ["Haoran Sun", "Shaoning Zeng"], "title": "Introspection of Thought Helps AI Agents", "comment": null, "summary": "AI Agents rely on Large Language Models (LLMs) and Multimodal-LLMs (MLLMs) to\nperform interpretation and inference in text and image tasks without\npost-training, where LLMs and MLLMs play the most critical role and determine\nthe initial ability and limitations of AI Agents. Usually, AI Agents utilize\nsophisticated prompt engineering and external reasoning framework to obtain a\npromising interaction with LLMs, e.g., Chain-of-Thought, Iteration of Thought\nand Image-of-Thought. However, they are still constrained by the inherent\nlimitations of LLM in understanding natural language, and the iterative\nreasoning process will generate a large amount of inference cost. To this end,\nwe propose a novel AI Agent Reasoning Framework with Introspection of Thought\n(INoT) by designing a new LLM-Read code in prompt. It enables LLM to execute\nprogrammatic dialogue reasoning processes following the code in prompt.\nTherefore, self-denial and reflection occur within LLM instead of outside LLM,\nwhich can reduce token cost effectively. Through our experiments on six\nbenchmarks for three different tasks, the effectiveness of INoT is verified,\nwith an average improvement of 7.95\\% in performance, exceeding the baselines.\nFurthermore, the token cost of INoT is lower on average than the best\nperforming method at baseline by 58.3\\%. In addition, we demonstrate the\nversatility of INoT in image interpretation and inference through verification\nexperiments.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aINoT\u7684\u65b0\u578bAI\u4ee3\u7406\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u8bbe\u8ba1LLM\u53ef\u8bfb\u7684\u63d0\u793a\u4ee3\u7801\uff0c\u51cf\u5c11\u63a8\u7406\u6210\u672c\u5e76\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709AI\u4ee3\u7406\u4f9d\u8d56LLMs\u548cMLLMs\uff0c\u4f46\u53d7\u9650\u4e8e\u5176\u56fa\u6709\u7f3a\u9677\u548c\u9ad8\u63a8\u7406\u6210\u672c\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u6846\u67b6\u3002", "method": "\u8bbe\u8ba1\u4e86INoT\u6846\u67b6\uff0c\u901a\u8fc7LLM-Read\u4ee3\u7801\u5b9e\u73b0\u7a0b\u5e8f\u5316\u5bf9\u8bdd\u63a8\u7406\uff0c\u4f7f\u53cd\u601d\u8fc7\u7a0b\u5728LLM\u5185\u90e8\u5b8c\u6210\u3002", "result": "\u5728\u516d\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cINoT\u5e73\u5747\u6027\u80fd\u63d0\u53477.95%\uff0c\u63a8\u7406\u6210\u672c\u964d\u4f4e58.3%\u3002", "conclusion": "INoT\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86AI\u4ee3\u7406\u7684\u6027\u80fd\u548c\u6548\u7387\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u56fe\u50cf\u4efb\u52a1\u4e2d\u7684\u901a\u7528\u6027\u3002"}}
{"id": "2507.08705", "categories": ["cs.AI", "I.2.5; I.2.1; I.2.7; I.2.11"], "pdf": "https://arxiv.org/pdf/2507.08705", "abs": "https://arxiv.org/abs/2507.08705", "authors": ["Philip Osborne", "Danilo S. Carvalho", "Andr\u00e9 Freitas"], "title": "elsciRL: Integrating Language Solutions into Reinforcement Learning Problem Settings", "comment": "6 pages, 1 figure, 3 tables, 11 Appendix pages, submitted to EMNLP\n  2025 Call for System Demonstrations", "summary": "We present elsciRL, an open-source Python library to facilitate the\napplication of language solutions on reinforcement learning problems. We\ndemonstrate the potential of our software by extending the Language Adapter\nwith Self-Completing Instruction framework defined in (Osborne, 2024) with the\nuse of LLMs. Our approach can be re-applied to new applications with minimal\nsetup requirements. We provide a novel GUI that allows a user to provide text\ninput for an LLM to generate instructions which it can then self-complete.\nEmpirical results indicate that these instructions \\textit{can} improve a\nreinforcement learning agent's performance. Therefore, we present this work to\naccelerate the evaluation of language solutions on reward based environments to\nenable new opportunities for scientific discovery.", "AI": {"tldr": "elsciRL\u662f\u4e00\u4e2a\u5f00\u6e90\u7684Python\u5e93\uff0c\u65e8\u5728\u901a\u8fc7\u8bed\u8a00\u89e3\u51b3\u65b9\u6848\u63d0\u5347\u5f3a\u5316\u5b66\u4e60\u95ee\u9898\u7684\u5904\u7406\u80fd\u529b\u3002\u5b83\u6269\u5c55\u4e86Language Adapter\u6846\u67b6\uff0c\u7ed3\u5408LLMs\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u5feb\u901f\u5e94\u7528\u4e8e\u65b0\u4efb\u52a1\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u901a\u8fc7GUI\u652f\u6301\u7528\u6237\u8f93\u5165\u6587\u672c\u751f\u6210\u6307\u4ee4\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u63d0\u5347\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u7684\u6027\u80fd\u3002", "motivation": "\u52a0\u901f\u8bed\u8a00\u89e3\u51b3\u65b9\u6848\u5728\u57fa\u4e8e\u5956\u52b1\u7684\u73af\u5883\u4e2d\u7684\u8bc4\u4f30\uff0c\u4ee5\u4fc3\u8fdb\u79d1\u5b66\u53d1\u73b0\u7684\u65b0\u673a\u9047\u3002", "method": "\u6269\u5c55Language Adapter\u6846\u67b6\uff0c\u7ed3\u5408LLMs\uff0c\u63d0\u4f9bGUI\u652f\u6301\u7528\u6237\u8f93\u5165\u6587\u672c\u751f\u6210\u6307\u4ee4\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u751f\u6210\u7684\u6307\u4ee4\u80fd\u591f\u63d0\u5347\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u7684\u6027\u80fd\u3002", "conclusion": "elsciRL\u4e3a\u8bed\u8a00\u89e3\u51b3\u65b9\u6848\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u4fbf\u6377\u5de5\u5177\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.08715", "categories": ["cs.AI", "cs.SY", "eess.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2507.08715", "abs": "https://arxiv.org/abs/2507.08715", "authors": ["Paul Saves", "Jasper Bussemaker", "R\u00e9mi Lafage", "Thierry Lefebvre", "Nathalie Bartoli", "Youssef Diouane", "Joseph Morlier"], "title": "System-of-systems Modeling and Optimization: An Integrated Framework for Intermodal Mobility", "comment": null, "summary": "For developing innovative systems architectures, modeling and optimization\ntechniques have been central to frame the architecting process and define the\noptimization and modeling problems. In this context, for system-of-systems the\nuse of efficient dedicated approaches (often physics-based simulations) is\nhighly recommended to reduce the computational complexity of the targeted\napplications. However, exploring novel architectures using such dedicated\napproaches might pose challenges for optimization algorithms, including\nincreased evaluation costs and potential failures. To address these challenges,\nsurrogate-based optimization algorithms, such as Bayesian optimization\nutilizing Gaussian process models have emerged.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5728\u7cfb\u7edf\u67b6\u6784\u5f00\u53d1\u4e2d\u4f7f\u7528\u4ee3\u7406\u4f18\u5316\u7b97\u6cd5\uff08\u5982\u8d1d\u53f6\u65af\u4f18\u5316\uff09\u4ee5\u5e94\u5bf9\u7269\u7406\u6a21\u62df\u5e26\u6765\u7684\u8ba1\u7b97\u6311\u6218\u3002", "motivation": "\u9488\u5bf9\u7cfb\u7edf\u67b6\u6784\u4f18\u5316\u4e2d\u7269\u7406\u6a21\u62df\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\u548c\u6f5c\u5728\u5931\u8d25\u95ee\u9898\uff0c\u7814\u7a76\u5982\u4f55\u5229\u7528\u9ad8\u6548\u65b9\u6cd5\u964d\u4f4e\u590d\u6742\u5ea6\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u4ee3\u7406\u7684\u4f18\u5316\u7b97\u6cd5\uff0c\u7279\u522b\u662f\u8d1d\u53f6\u65af\u4f18\u5316\u7ed3\u5408\u9ad8\u65af\u8fc7\u7a0b\u6a21\u578b\u3002", "result": "\u4ee3\u7406\u4f18\u5316\u7b97\u6cd5\u80fd\u6709\u6548\u51cf\u5c11\u8ba1\u7b97\u6210\u672c\u5e76\u63d0\u9ad8\u4f18\u5316\u6548\u7387\u3002", "conclusion": "\u4ee3\u7406\u4f18\u5316\u7b97\u6cd5\u662f\u89e3\u51b3\u7cfb\u7edf\u67b6\u6784\u4f18\u5316\u4e2d\u8ba1\u7b97\u6311\u6218\u7684\u6709\u6548\u65b9\u6cd5\u3002"}}
