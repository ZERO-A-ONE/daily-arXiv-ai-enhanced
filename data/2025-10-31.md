<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 21]
- [cs.CR](#cs.CR) [Total: 25]
- [cs.AI](#cs.AI) [Total: 45]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Internal Vulnerabilities, External Threats: A Grounded Framework for Enterprise Open Source Risk Governance](https://arxiv.org/abs/2510.25882)
*Wenhao Yang,Minghui Zhou,Daniel Izquierdo Cortázar,Yehui Wang*

Main category: cs.SE

TL;DR: 提出了一个开源风险治理框架，通过"目标->威胁->漏洞->缓解"(OTVM)逻辑链，将不可控外部威胁与可控内部漏洞结合分析，帮助企业从战术风险管理转向整体风险治理。


<details>
  <summary>Details</summary>
Motivation: 传统风险管理仅关注技术工具，无法应对上游"静默修复"、社区冲突、许可证变更等系统性威胁，存在治理盲区。

Method: 基于扎根理论研究，与15位从业者合作开发整体风险治理框架，构建OTVM逻辑链分析模型。

Result: 开发了战略目标矩阵、外部威胁(技术、社区、生态)和内部漏洞(战略、运营、技术)的双重分类法，以及可操作的缓解框架。

Conclusion: 该框架为企业提供了从被动"救火"转向主动构建组织"免疫系统"的诊断工具和系统路径。

Abstract: Enterprise engagement with open source has evolved from tactical adoption to
strategic deep integration, exposing them to a complex risk landscape far
beyond mere code. However, traditional risk management, narrowly focused on
technical tools, is structurally inadequate for systemic threats like upstream
"silent fixes", community conflicts, or sudden license changes, creating a
dangerous governance blind spot. To address this governance vacuum and enable
the necessary shift from tactical risk management to holistic risk governance,
we conducted a grounded theory study with 15 practitioners to develop a
holistic risk governance framework. Our study formalizes an analytical
framework built on a foundational risk principle: an uncontrollable External
Threat (e.g., a sudden license change in a key dependency) only becomes a
critical risk when it exploits a controllable Internal Vulnerability (e.g., an
undefined risk appetite for single-vendor projects), which then amplifies the
impact.The framework operationalizes this principle through a clear logical
chain: "Objectives -> Threats -> Vulnerabilities -> Mitigation" (OTVM). This
provides a holistic decision model that transcends mere technical checklists.
Based on this logic, our contributions are: (1) a "Strategic Objectives Matrix"
to clarify goals; (2) a systematic dual taxonomy of External Threats (Ex-Tech,
Ex-Comm, Ex-Eco) and Internal Vulnerabilities (In-Strat, In-Ops, In-Tech); and
(3) an actionable mitigation framework mapping capability-building to these
vulnerabilities. The framework's analytical utility was validated by three
industry experts through retrospective case studies on real-world incidents.
This work provides a novel diagnostic lens and a systematic path for
enterprises to shift from reactive "firefighting" to proactively building an
organizational "immune system".

</details>


### [2] [PRISM: Proof-Carrying Artifact Generation through LLM x MDE Synergy and Stratified Constraints](https://arxiv.org/abs/2510.25890)
*Tong Ma,Hui Lai,Hui Wang,Zhenhu Tian,Jizhou Wang,Haichao Wu,Yongfan Gao,Chaochao Li,Fengjie Xu,Ling Fang*

Main category: cs.SE

TL;DR: PRISM是一个将大语言模型与模型驱动工程相结合的系统，用于生成符合监管要求的可验证工件，在安全和合规关键领域提供机器可检查的证据。


<details>
  <summary>Details</summary>
Motivation: 解决在安全和合规关键领域中，手动生成监管就绪工件和验证证据的高成本、易出错问题，提供自动化且可验证的生成方案。

Method: 采用三支柱架构：统一元模型整合异构模式、集成约束模型编译结构语义要求、约束引导可验证生成通过两层强制执行机制。

Result: 在汽车软件工程和跨境法律管辖场景中，PRISM生成了结构有效、可审计的工件，显著减少手动修复工作，并与现有工具链集成。

Conclusion: PRISM为自动化工件生成提供了一条实用路径，具有内置保证机制，能够生成符合监管要求的可验证证据。

Abstract: PRISM unifies Large Language Models with Model-Driven Engineering to generate
regulator-ready artifacts and machine-checkable evidence for safety- and
compliance-critical domains. PRISM integrates three pillars: a Unified
Meta-Model (UMM) reconciles heterogeneous schemas and regulatory text into a
single semantic space; an Integrated Constraint Model (ICM) compiles structural
and semantic requirements into enforcement artifacts including generation-time
automata (GBNF, DFA) and post-generation validators (e.g., SHACL, SMT); and
Constraint-Guided Verifiable Generation (CVG) applies these through two-layer
enforcement - structural constraints drive prefix-safe decoding while
semantic/logical validation produces machine-checkable certificates. When
violations occur, PRISM performs audit-guided repair and records generation
traces for compliance review. We evaluate PRISM in automotive software
engineering (AUTOSAR) and cross-border legal jurisdiction (Brussels I bis).
PRISM produces structurally valid, auditable artifacts that integrate with
existing tooling and substantially reduce manual remediation effort, providing
a practical path toward automated artifact generation with built-in assurance.

</details>


### [3] [A Process Mining-Based System For The Analysis and Prediction of Software Development Workflows](https://arxiv.org/abs/2510.25935)
*Antía Dorado,Iván Folgueira,Sofía Martín,Gonzalo Martín,Álvaro Porto,Alejandro Ramos,John Wallace*

Main category: cs.SE

TL;DR: CodeSight是一个端到端系统，通过整合过程挖掘和机器学习来预测软件开发中的截止期限合规性，能够基于GitHub数据预测PR解决时间并识别潜在延期风险。


<details>
  <summary>Details</summary>
Motivation: 为了解决软件开发过程中截止期限合规性的预测问题，帮助项目管理者及早识别潜在的延期风险，实现主动的项目管理。

Method: 系统从GitHub捕获开发和部署数据，转换为过程挖掘日志进行分析，然后使用LSTM模型基于序列活动轨迹和静态特征来预测PR剩余解决时间。

Result: 在测试中，系统在预测截止期限合规性方面表现出高精度和高F1分数，验证了方法的有效性。

Conclusion: 将过程挖掘与机器学习相结合，能够为软件项目管理提供主动的、可操作的洞察，有效提升工作流效率。

Abstract: CodeSight is an end-to-end system designed to anticipate deadline compliance
in software development workflows. It captures development and deployment data
directly from GitHub, transforming it into process mining logs for detailed
analysis. From these logs, the system generates metrics and dashboards that
provide actionable insights into PR activity patterns and workflow efficiency.
Building on this structured representation, CodeSight employs an LSTM model
that predicts remaining PR resolution times based on sequential activity traces
and static features, enabling early identification of potential deadline
breaches. In tests, the system demonstrates high precision and F1 scores in
predicting deadline compliance, illustrating the value of integrating process
mining with machine learning for proactive software project management.

</details>


### [4] [Beyond Synthetic Benchmarks: Evaluating LLM Performance on Real-World Class-Level Code Generation](https://arxiv.org/abs/2510.26130)
*Musfiqur Rahman,SayedHassan Khatoonabadi,Emad Shihab*

Main category: cs.SE

TL;DR: LLMs在函数级代码生成表现出色，但在真实软件项目中的类级实现能力严重不足，正确率从合成基准的84-89%降至真实任务的25-34%。检索增强生成在部分文档情况下最有效，能提升4-7%正确率。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs在真实软件项目中生成类级代码的能力，现有研究主要关注函数级代码，而类级实现在实际工程中更为复杂和重要。

Method: 构建基于开源仓库的真实世界类基准，分为已见和未见分区，评估多种LLM在不同输入规范、检索增强配置和文档完整性下的表现。

Result: LLMs在真实类任务中正确率仅为25-34%，远低于合成基准的84-89%。检索增强生成在部分文档时能提升4-7%正确率，但可能引入依赖冲突。主要错误类型为AttributeError、TypeError和AssertionError。

Conclusion: 当前LLMs在类级工程能力存在严重局限，需要改进上下文建模、文档策略和检索集成，为生产代码辅助工具提供可操作见解。

Abstract: Large language models (LLMs) have advanced code generation at the function
level, yet their ability to produce correct class-level implementations in
authentic software projects remains poorly understood. This work introduces a
novel benchmark derived from open-source repositories, comprising real-world
classes divided into seen and unseen partitions to evaluate generalization
under practical conditions. The evaluation examines multiple LLMs under varied
input specifications, retrieval-augmented configurations, and documentation
completeness levels.
  Results reveal a stark performance disparity: LLMs achieve 84% to 89%
correctness on established synthetic benchmarks but only 25% to 34% on
real-world class tasks, with negligible differences between familiar and novel
codebases. Comprehensive docstrings yield modest gains of 1% to 3% in
functional accuracy, though statistical significance is rare.
Retrieval-augmented generation proves most effective with partial
documentation, improving correctness by 4% to 7% by supplying concrete
implementation patterns absent from specifications. Error profiling identifies
AttributeError, TypeError, and AssertionError as dominant failure modes (84% of
cases), with synthetic tests overemphasizing assertion issues and real-world
scenarios highlighting type and attribute mismatches. Retrieval augmentation
reduces logical flaws but can introduce dependency conflicts.
  The benchmark and analysis expose critical limitations in current LLM
capabilities for class-level engineering, offering actionable insights for
enhancing context modelling, documentation strategies, and retrieval
integration in production code assistance tools.

</details>


### [5] [Reduction of Test Re-runs by Prioritizing Potential Order Dependent Flaky Tests](https://arxiv.org/abs/2510.26171)
*Hasnain Iqbal,Zerina Begum,Kazi Sakib*

Main category: cs.SE

TL;DR: 提出了一种通过分析测试类中的共享静态字段来优先处理潜在顺序依赖测试的方法，显著提高了OD测试检测效率


<details>
  <summary>Details</summary>
Motivation: 顺序依赖测试会导致持续集成管道失败，现有检测方法需要多次重复运行测试，成本高昂，因此需要优先处理潜在OD测试以减少重复运行

Method: 通过分析测试类中的共享静态字段来识别更可能具有顺序依赖性的测试

Result: 在27个项目模块实验中，成功在23个案例中优先处理了所有OD测试，平均减少测试执行65.92%，减少不必要重复运行72.19%

Conclusion: 该方法通过降低执行成本，显著提高了OD测试检测效率

Abstract: Flaky tests can make automated software testing unreliable due to their
unpredictable behavior. These tests can pass or fail on the same code base on
multiple runs. However, flaky tests often do not refer to any fault, even
though they can cause the continuous integration (CI) pipeline to fail. A
common type of flaky test is the order-dependent (OD) test. The outcome of an
OD test depends on the order in which it is run with respect to other test
cases. Several studies have explored the detection and repair of OD tests.
However, their methods require re-runs of tests multiple times, that are not
related to the order dependence. Hence, prioritizing potential OD tests is
necessary to reduce the re-runs. In this paper, we propose a method to
prioritize potential order-dependent tests. By analyzing shared static fields
in test classes, we identify tests that are more likely to be order-dependent.
In our experiment on 27 project modules, our method successfully prioritized
all OD tests in 23 cases, reducing test executions by an average of 65.92% and
unnecessary re-runs by 72.19%. These results demonstrate that our approach
significantly improves the efficiency of OD test detection by lowering
execution costs.

</details>


### [6] [The "4W+1H" of Software Supply Chain Security Checklist for Critical Infrastructure](https://arxiv.org/abs/2510.26174)
*Liming Dong,Sung Une Lee,Zhenchang Xing,Muhammad Ejaz Ahmed,Stefan Avgoustakis*

Main category: cs.SE

TL;DR: 该论文通过多源文献综述分析了软件供应链安全实践，发现现有框架很少针对关键基础设施领域定制，并开发了一个包含80个问题的结构化检查表来帮助评估和增强软件供应链安全。


<details>
  <summary>Details</summary>
Motivation: 软件供应链攻击频率和复杂性不断增加，对关键基础设施构成严重风险，而现有安全实践分散且不足，缺乏针对关键基础设施领域的专门框架。

Method: 采用多源文献综述方法，分析国际框架、澳大利亚监管来源和学术研究，使用"4W+1H"分析方法综合软件供应链安全实践的核心类别。

Result: 识别出10个软件供应链安全实践核心类别，发现现有框架与关键基础设施领域特定需求之间存在差距，开发了结构化多层检查表。

Conclusion: 需要集成、情境感知的方法来保护关键基础设施免受不断演变的软件供应链风险，框架指导与行业特定需求之间存在差距。

Abstract: The increasing frequency and sophistication of software supply chain attacks
pose severe risks to critical infrastructure sectors, threatening national
security, economic stability, and public safety. Despite growing awareness,
existing security practices remain fragmented and insufficient, with most
frameworks narrowly focused on isolated life cycle stages or lacking alignment
with the specific needs of critical infrastructure (CI) sectors. In this paper,
we conducted a multivocal literature review across international frameworks,
Australian regulatory sources, and academic studies to identify and analyze
security practices across the software supply chain, especially specific CI
sector. Our analysis found that few existing frameworks are explicitly tailored
to CI domains. We systematically leveraged identified software supply chain
security frameworks, using a "4W+1H" analytical approach, we synthesized ten
core categories (what) of software supply chain security practices, mapped them
across life-cycle phases (when), stakeholder roles (who), and implementation
levels (how), and examined their coverage across existing frameworks (where).
Building on these insights, the paper culminates in structured, multi-layered
checklist of 80 questions designed to relevant stakeholders evaluate and
enhance their software supply chain security. Our findings reveal gaps between
framework guidance and sector-specific needs, highlight the need for
integrated, context-aware approaches to safeguard critical infrastructure from
evolving software supply chain risks.

</details>


### [7] [A Research Roadmap for Augmenting Software Engineering Processes and Software Products with Generative AI](https://arxiv.org/abs/2510.26275)
*Domenico Amalfitano,Andreas Metzger,Marco Autili,Tommaso Fulcini,Tobias Hey,Jan Keim,Patrizio Pelliccione,Vincenzo Scotti,Anne Koziolek,Raffaela Mirandola,Andreas Vogelsang*

Main category: cs.SE

TL;DR: 该论文应用设计科学研究方法构建了生成式AI增强软件工程的路线图，通过多轮证据整合和麦克卢汉四元法分析，识别了四种GenAI增强形式及其相关研究挑战，并提出了2030年软件工程的十个预测。


<details>
  <summary>Details</summary>
Motivation: 生成式AI正在迅速改变软件工程实践，影响软件开发、运维和演进过程，需要系统性地构建GenAI增强软件工程的路线图。

Method: 采用设计科学研究方法，通过三个循环逐步整合多个证据源，包括FSE 2025研讨会讨论、快速文献综述和同行反馈，使用麦克卢汉四元法作为概念工具分析GenAI对软件工程的影响。

Result: 识别了四种GenAI在软件工程中的基本增强形式，系统性地描述了相关研究挑战和机遇，并整合成未来研究方向。

Conclusion: 通过严谨的多循环过程和跨团队验证，为分析GenAI如何影响软件工程过程、方法和工具提供了透明可复现的基础，并基于发现提出了2030年软件工程的十个预测。

Abstract: Generative AI (GenAI) is rapidly transforming software engineering (SE)
practices, influencing how SE processes are executed, as well as how software
systems are developed, operated, and evolved. This paper applies design science
research to build a roadmap for GenAI-augmented SE. The process consists of
three cycles that incrementally integrate multiple sources of evidence,
including collaborative discussions from the FSE 2025 "Software Engineering
2030" workshop, rapid literature reviews, and external feedback sessions
involving peers. McLuhan's tetrads were used as a conceptual instrument to
systematically capture the transforming effects of GenAI on SE processes and
software products.The resulting roadmap identifies four fundamental forms of
GenAI augmentation in SE and systematically characterizes their related
research challenges and opportunities. These insights are then consolidated
into a set of future research directions. By grounding the roadmap in a
rigorous multi-cycle process and cross-validating it among independent author
teams and peers, the study provides a transparent and reproducible foundation
for analyzing how GenAI affects SE processes, methods and tools, and for
framing future research within this rapidly evolving area. Based on these
findings, the article finally makes ten predictions for SE in the year 2030.

</details>


### [8] [Empowering RepoQA-Agent based on Reinforcement Learning Driven by Monte-carlo Tree Search](https://arxiv.org/abs/2510.26287)
*Guochang Li,Yuchen Liu,Zhen Qin,Yunkun Wang,Jianping Zhong,Chen Zhi,Binhua Li,Fei Huang,Yongbin Li,Shuiguang Deng*

Main category: cs.SE

TL;DR: RepoSearch-R1是一个基于蒙特卡洛树搜索的强化学习框架，用于解决仓库级软件工程任务中的代码库导航和信息提取问题，无需模型蒸馏或外部监督。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在局限性：无训练方法在工具利用和环境反馈决策方面效果不佳，而基于训练的方法依赖昂贵的大模型蒸馏，存在企业环境中的数据合规问题。

Method: 引入RepoSearch-R1框架，使用蒙特卡洛树搜索驱动的强化学习，通过自训练生成多样化的高质量推理轨迹，无需模型蒸馏或外部监督。

Result: 在仓库问答任务上，RepoSearch-R1相比无检索方法提升16.0%答案完整性，相比迭代检索方法提升19.5%，训练效率比通用代理强化学习方法提高33%。

Conclusion: 该冷启动训练方法消除了数据合规问题，同时在仓库级推理任务中保持了强大的探索多样性和答案完整性。

Abstract: Repository-level software engineering tasks require large language models
(LLMs) to efficiently navigate and extract information from complex codebases
through multi-turn tool interactions. Existing approaches face significant
limitations: training-free, in-context learning methods struggle to guide
agents effectively in tool utilization and decision-making based on
environmental feedback, while training-based approaches typically rely on
costly distillation from larger LLMs, introducing data compliance concerns in
enterprise environments. To address these challenges, we introduce
RepoSearch-R1, a novel agentic reinforcement learning framework driven by
Monte-carlo Tree Search (MCTS). This approach allows agents to generate
diverse, high-quality reasoning trajectories via self-training without
requiring model distillation or external supervision. Based on RepoSearch-R1,
we construct a RepoQA-Agent specifically designed for repository
question-answering tasks. Comprehensive evaluation on repository
question-answering tasks demonstrates that RepoSearch-R1 achieves substantial
improvements of answer completeness: 16.0% enhancement over no-retrieval
methods, 19.5% improvement over iterative retrieval methods, and 33% increase
in training efficiency compared to general agentic reinforcement learning
approaches. Our cold-start training methodology eliminates data compliance
concerns while maintaining robust exploration diversity and answer completeness
across repository-level reasoning tasks.

</details>


### [9] [Environmental Impact of CI/CD Pipelines](https://arxiv.org/abs/2510.26413)
*Nuno Saavedra,Alexandra Mendes,João F. Ferreira*

Main category: cs.SE

TL;DR: GitHub Actions的CI/CD服务产生了显著的碳足迹和水足迹，2024年碳足迹估计在150.5-994.9 MTCO2e之间，水足迹在1,989.6-37,664.5千升之间。最可能情景下碳足迹为456.9 MTCO2e，水足迹为5,738.2千升。


<details>
  <summary>Details</summary>
Motivation: CI/CD管道在软件开发中广泛使用，但其环境影响（特别是碳足迹和水足迹）对开发者来说通常未知，因为CI服务提供商通常不披露此类信息。随着云计算环境影响的增长，了解CI/CD服务的CWF变得越来越重要。

Method: 基于Cloud Carbon Footprint框架的方法论，使用文献中最大的工作流运行数据集，包含来自18,000多个仓库的220多万个工作流运行。

Result: GitHub Actions生态系统产生了显著的CWF。最可能情景下碳足迹为456.9 MTCO2e（相当于7,615棵城市树木一年的碳捕获量），水足迹为5,738.2千升（相当于美国家庭5,053年的用水量）。

Conclusion: 提出了减少计算资源浪费的缓解策略，包括在法国和英国等环境影响低的地区部署runner、对计划运行实施更严格的停用政策、在区域能源组合更环保时执行，以及减小仓库大小。

Abstract: CI/CD pipelines are widely used in software development, yet their
environmental impact, particularly carbon and water footprints (CWF), remains
largely unknown to developers, as CI service providers typically do not
disclose such information. With the growing environmental impact of cloud
computing, understanding the CWF of CI/CD services has become increasingly
important.
  This work investigates the CWF of using GitHub Actions, focusing on
open-source repositories where usage is free and unlimited for standard
runners. We build upon a methodology from the Cloud Carbon Footprint framework
and we use the largest dataset of workflow runs reported in the literature to
date, comprising over 2.2 million workflow runs from more than 18,000
repositories.
  Our analysis reveals that the GitHub Actions ecosystem results in a
substantial CWF. Our estimates for the carbon footprint in 2024 range from
150.5 MTCO2e in the most optimistic scenario to 994.9 MTCO2e in the most
pessimistic scenario, while the water footprint ranges from 1,989.6 to 37,664.5
kiloliters. The most likely scenario estimates are 456.9 MTCO2e for carbon
footprint and 5,738.2 kiloliters for water footprint. To provide perspective,
the carbon footprint in the most likely scenario is equivalent to the carbon
captured by 7,615 urban trees in a year, and the water footprint is comparable
to the water consumed by an average American family over 5,053 years.
  We explore strategies to mitigate this impact, primarily by reducing wasted
computational resources. Key recommendations include deploying runners in
regions whose energy production has a low environmental impact such as France
and the United Kingdom, implementing stricter deactivation policies for
scheduled runs and aligning their execution with periods when the regional
energy mix is more environmentally favorable, and reducing the size of
repositories.

</details>


### [10] [Nexus: Execution-Grounded Multi-Agent Test Oracle Synthesis](https://arxiv.org/abs/2510.26423)
*Dong Huang,Mingzhe Du,Jie M. Zhang,Zheng Lin,Meng Luo,Qianru Zhang,See-Kiong Ng*

Main category: cs.SE

TL;DR: Nexus是一个多智能体框架，通过专业智能体的协作审议、验证和迭代自优化来生成测试预言，在多个基准测试中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决非回归测试中测试预言生成的长期挑战，目标是生成能够准确判断被测函数是否按预期运行的预言。

Method: 使用四个体现不同测试理念的专业智能体进行协作审议和优化，生成候选函数实现并在安全沙箱中执行验证，对失败的预言进行自动调试和修正的迭代自优化循环。

Result: 在七个不同基准测试中表现优异，将LiveCodeBench的测试级预言准确率从46.30%提升到57.73%，HumanEval的错误检测率从90.91%提升到95.45%，自动程序修复成功率从35.23%提升到69.32%。

Conclusion: Nexus框架通过多智能体协作和迭代自优化，显著提高了测试预言生成的准确性和下游任务的性能。

Abstract: Test oracle generation in non-regression testing is a longstanding challenge
in software engineering, where the goal is to produce oracles that can
accurately determine whether a function under test (FUT) behaves as intended
for a given input. In this paper, we introduce Nexus, a novel multi-agent
framework to address this challenge. Nexus generates test oracles by leveraging
a diverse set of specialized agents that synthesize test oracles through a
structured process of deliberation, validation, and iterative self-refinement.
During the deliberation phase, a panel of four specialist agents, each
embodying a distinct testing philosophy, collaboratively critiques and refines
an initial set of test oracles. Then, in the validation phase, Nexus generates
a plausible candidate implementation of the FUT and executes the proposed
oracles against it in a secure sandbox. For any oracle that fails this
execution-based check, Nexus activates an automated selfrefinement loop, using
the specific runtime error to debug and correct the oracle before
re-validation. Our extensive evaluation on seven diverse benchmarks
demonstrates that Nexus consistently and substantially outperforms
state-of-theart baselines. For instance, Nexus improves the test-level oracle
accuracy on the LiveCodeBench from 46.30% to 57.73% for GPT-4.1-Mini. The
improved accuracy also significantly enhances downstream tasks: the bug
detection rate of GPT4.1-Mini generated test oracles on HumanEval increases
from 90.91% to 95.45% for Nexus compared to baselines, and the success rate of
automated program repair improves from 35.23% to 69.32%.

</details>


### [11] [CHCVerif: A Portfolio-Based Solver for Constrained Horn Clauses](https://arxiv.org/abs/2510.26431)
*Mihály Dobos-Kovács,Levente Bajczi,András Vörös*

Main category: cs.SE

TL;DR: CHCVERIF是一个基于组合策略的CHC求解器，采用软件验证方法来解决约束Horn子句问题，能够重用成熟的软件验证工具处理涉及位向量和低级语义的基准测试。


<details>
  <summary>Details</summary>
Motivation: 约束Horn子句（CHCs）被广泛用作各种验证任务的中间表示，包括安全检查、不变式综合和过程间分析。但现有方法在处理涉及位向量和低级语义的CHC基准测试时存在挑战。

Method: 提出CHCVERIF，一个基于组合策略的CHC求解器，采用软件验证方法，重用成熟的软件验证工具来解决CHC问题，特别是那些涉及位向量和低级语义的基准测试。

Result: 评估显示该方法在线性整数算术基准测试中表现中等，在位向量基准测试中取得适度成功。结果证明了使用软件验证工具作为CHC求解后端的可行性和潜力。

Conclusion: 使用软件验证工具作为CHC求解后端是可行且有潜力的，特别是在精心构建的组合策略支持下。该方法为处理涉及位向量和低级语义的CHC问题提供了新的解决方案。

Abstract: Constrained Horn Clauses (CHCs) are widely adopted as intermediate
representations for a variety of verification tasks, including safety checking,
invariant synthesis, and interprocedural analysis. This paper introduces
CHCVERIF, a portfolio-based CHC solver that adopts a software verification
approach for solving CHCs. This approach enables us to reuse mature software
verification tools to tackle CHC benchmarks, particularly those involving
bitvectors and low-level semantics. Our evaluation shows that while the method
enjoys only moderate success with linear integer arithmetic, it achieves modest
success on bitvector benchmarks. Moreover, our results demonstrate the
viability and potential of using software verification tools as backends for
CHC solving, particularly when supported by a carefully constructed portfolio.

</details>


### [12] [SecureReviewer: Enhancing Large Language Models for Secure Code Review through Secure-aware Fine-tuning](https://arxiv.org/abs/2510.26457)
*Fang Liu,Simiao Liu,Yinghao Zhu,Xiaoli Lian,Li Zhang*

Main category: cs.SE

TL;DR: SecureReviewer是一个专门用于增强LLM在代码审查中识别和解决安全问题的能力的新方法，通过构建专门的数据集、安全感知微调策略、RAG技术和新的评估指标SecureBLEU，显著提升了安全代码审查的效果。


<details>
  <summary>Details</summary>
Motivation: 现有自动化代码审查方法主要关注通用目的代码审查，在识别和解决安全相关问题方面效果不足，且面临数据稀缺和评估指标不充分等挑战。

Method: 构建专门的安全代码审查数据集，采用安全感知微调策略训练LLM，集成RAG技术减少幻觉并增强输出可靠性，引入SecureBLEU评估指标。

Result: 实验结果表明SecureReviewer在安全漏洞检测准确性和生成审查评论的整体质量及实用性方面均优于现有最先进基线方法。

Conclusion: SecureReviewer通过专门的数据集、微调策略、RAG技术和评估指标，有效提升了LLM在安全代码审查中的能力，为软件安全开发提供了有力支持。

Abstract: Identifying and addressing security issues during the early phase of the
development lifecycle is critical for mitigating the long-term negative impacts
on software systems. Code review serves as an effective practice that enables
developers to check their teammates' code before integration into the codebase.
To streamline the generation of review comments, various automated code review
approaches have been proposed, where LLM-based methods have significantly
advanced the capabilities of automated review generation. However, existing
models primarily focus on general-purpose code review, their effectiveness in
identifying and addressing security-related issues remains underexplored.
Moreover, adapting existing code review approaches to target security issues
faces substantial challenges, including data scarcity and inadequate evaluation
metrics. To address these limitations, we propose SecureReviewer, a new
approach designed for enhancing LLMs' ability to identify and resolve
security-related issues during code review. Specifically, we first construct a
dataset tailored for training and evaluating secure code review capabilities.
Leveraging this dataset, we fine-tune LLMs to generate code review comments
that can effectively identify security issues and provide fix suggestions with
our proposed secure-aware fine-tuning strategy. To mitigate hallucination in
LLMs and enhance the reliability of their outputs, we integrate the RAG
technique, which grounds the generated comments in domain-specific security
knowledge. Additionally, we introduce SecureBLEU, a new evaluation metric
designed to assess the effectiveness of review comments in addressing security
issues. Experimental results demonstrate that SecureReviewer outperforms
state-of-the-art baselines in both security issue detection accuracy and the
overall quality and practical utility of generated review comments.

</details>


### [13] [Automated Extract Method Refactoring with Open-Source LLMs: A Comparative Study](https://arxiv.org/abs/2510.26480)
*Sivajeet Chand,Melih Kilic,Roland Würsching,Sushant Kumar Pandey,Alexander Pretschner*

Main category: cs.SE

TL;DR: 评估5个开源LLM在Python代码提取方法重构任务中的表现，发现RCI提示策略优于单次提示，最佳模型在测试通过率和代码质量指标上表现优异，开发者调查显示超过70%接受度。


<details>
  <summary>Details</summary>
Motivation: 自动化提取方法重构对提高代码可读性和可维护性很重要，但仍是挑战性任务。开源资源高效LLM为自动化此类高级任务提供了新方法。

Method: 系统评估5个最先进的开源LLM（3B到8B参数），在Python代码EMR任务上使用自动指标评估功能正确性和代码质量，比较单次提示与RCI提示策略的效果。

Result: RCI提示在测试通过率和重构质量上持续优于单次提示。最佳模型Deepseek-Coder-RCI和Qwen2.5-Coder-RCI分别达到0.829和0.808的测试通过率，将每方法代码行数从12.103减少到6.192和5.577，圈复杂度从4.602降低到3.453和3.294。开发者调查显示超过70%接受RCI生成的重构。

Conclusion: RCI提示策略显著提升LLM在代码重构任务中的表现，传统指标与人工判断存在差异，需要人工参与评估。开源基准为未来LLM自动化重构研究提供了基础。

Abstract: Automating the Extract Method refactoring (EMR) remains challenging and
largely manual despite its importance in improving code readability and
maintainability. Recent advances in open-source, resource-efficient Large
Language Models (LLMs) offer promising new approaches for automating such
high-level tasks. In this work, we critically evaluate five state-of-the-art
open-source LLMs, spanning 3B to 8B parameter sizes, on the EMR task for Python
code. We systematically assess functional correctness and code quality using
automated metrics and investigate the impact of prompting strategies by
comparing one-shot prompting to a Recursive criticism and improvement (RCI)
approach. RCI-based prompting consistently outperforms one-shot prompting in
test pass rates and refactoring quality. The best-performing models,
Deepseek-Coder-RCI and Qwen2.5-Coder-RCI, achieve test pass percentage (TPP)
scores of 0.829 and 0.808, while reducing lines of code (LOC) per method from
12.103 to 6.192 and 5.577, and cyclomatic complexity (CC) from 4.602 to 3.453
and 3.294, respectively. A developer survey on RCI-generated refactorings shows
over 70% acceptance, with Qwen2.5-Coder rated highest across all evaluation
criteria. In contrast, the original code scored below neutral, particularly in
readability and maintainability, underscoring the benefits of automated
refactoring guided by quality prompts. While traditional metrics like CC and
LOC provide useful signals, they often diverge from human judgments,
emphasizing the need for human-in-the-loop evaluation. Our open-source
benchmark offers a foundation for future research on automated refactoring with
LLMs.

</details>


### [14] [Envisioning Future Interactive Web Development: Editing Webpage with Natural Language](https://arxiv.org/abs/2510.26516)
*Truong Hai Dang,Jingyu Xiao,Yintong Huo*

Main category: cs.SE

TL;DR: 提出了一种自动化数据生成流程Instruct4Edit，用于训练LLMs进行网页代码编辑，通过生成多样化指令、应用代码修改和视觉验证来创建高质量微调数据集。


<details>
  <summary>Details</summary>
Motivation: 网页应用开发依赖于迭代代码修改，传统上这是手动且耗时的过程。虽然LLMs能生成UI代码，但从新设计需求编辑现有代码仍具挑战性，主要缺乏大规模高质量调优数据来对齐模型性能与人类期望。

Method: 使用LLMs合成高质量微调数据集Instruct4Edit的自动化数据生成流程，包括生成多样化指令、应用相应代码修改，并进行视觉验证以确保正确性。

Result: 通过在Instruct4Edit上微调模型，在将人类意图转化为精确、结构连贯且视觉准确的代码更改方面表现出持续改进。

Conclusion: 为基于自然语言的网页编辑提供了可扩展和透明的基础，证明微调较小的开源模型可以实现与专有系统相竞争的性能。

Abstract: The evolution of web applications relies on iterative code modifications, a
process that is traditionally manual and time-consuming. While Large Language
Models (LLMs) can generate UI code, their ability to edit existing code from
new design requirements (e.g., "center the logo") remains a challenge. This is
largely due to the absence of large-scale, high-quality tuning data to align
model performance with human expectations. In this paper, we introduce a novel,
automated data generation pipeline that uses LLMs to synthesize a high-quality
fine-tuning dataset for web editing, named Instruct4Edit. Our approach
generates diverse instructions, applies the corresponding code modifications,
and performs visual verification to ensure correctness. By fine-tuning models
on Instruct4Edit, we demonstrate consistent improvement in translating human
intent into precise, structurally coherent, and visually accurate code changes.
This work provides a scalable and transparent foundation for natural language
based web editing, demonstrating that fine-tuning smaller open-source models
can achieve competitive performance with proprietary systems. We release all
data, code implementations, and model checkpoints for reproduction.

</details>


### [15] [Reflecting on Empirical and Sustainability Aspects of Software Engineering Research in the Era of Large Language Models](https://arxiv.org/abs/2510.26538)
*David Williams,Max Hort,Maria Kechagia,Aldeida Aleti,Justyna Petke,Federica Sarro*

Main category: cs.SE

TL;DR: 本文分析了软件工程研究中使用大语言模型时面临的基准测试严谨性、污染、可复现性和可持续性等挑战，并提出了改进建议。


<details>
  <summary>Details</summary>
Motivation: 软件工程研究中使用大语言模型引入了基准测试严谨性、污染、可复现性和可持续性等新挑战，需要研究社区反思这些问题。

Method: 通过结构化分析ICSE会议上当前基于LLM的软件工程研究，识别良好实践和持续存在的问题。

Result: 研究结果展示了当前LLM在软件工程研究中的应用现状，突出了令人鼓舞的做法和持续存在的不足。

Conclusion: 提出了加强基准测试严谨性、提高可复现性以及解决基于LLM的软件工程研究的财务和环境成本的建议。

Abstract: Software Engineering (SE) research involving the use of Large Language Models
(LLMs) has introduced several new challenges related to rigour in benchmarking,
contamination, replicability, and sustainability. In this paper, we invite the
research community to reflect on how these challenges are addressed in SE. Our
results provide a structured overview of current LLM-based SE research at ICSE,
highlighting both encouraging practices and persistent shortcomings. We
conclude with recommendations to strengthen benchmarking rigour, improve
replicability, and address the financial and environmental costs of LLM-based
SE.

</details>


### [16] ["Show Me You Comply... Without Showing Me Anything": Zero-Knowledge Software Auditing for AI-Enabled Systems](https://arxiv.org/abs/2510.26576)
*Filippo Scaramuzza,Renato Cordeiro Ferreira,Tomaz Maia Suller,Giovanni Quattrocchi,Damian Andrew Tamburri,Willem-Jan van den Heuvel*

Main category: cs.SE

TL;DR: ZKMLOps是一个基于零知识证明的MLOps验证框架，解决AI系统可信度验证与资产保护之间的冲突，通过密码学证明实现可验证的合规性。


<details>
  <summary>Details</summary>
Motivation: AI系统在关键领域的应用面临可信度挑战，传统验证方法成本高且不适合AI黑盒特性，同时透明度需求与资产保护（如专有模型和机密数据）之间存在冲突。

Method: 将零知识证明协议集成到机器学习运维生命周期中，结合成熟的软件工程模式，提供模块化、可重复的合规性密码学证明生成过程。

Result: 通过金融风险审计的监管合规案例研究验证了框架实用性，并通过实证评估不同ZKP协议的性能权衡，分析了复杂ML模型的可行性。

Conclusion: ZKMLOps框架为解决AI系统可信度验证与资产保护之间的冲突提供了可行的技术方案，能够生成可验证的密码学合规证明。

Abstract: The increasing exploitation of Artificial Intelligence (AI) enabled systems
in critical domains has made trustworthiness concerns a paramount showstopper,
requiring verifiable accountability, often by regulation (e.g., the EU AI Act).
Classical software verification and validation techniques, such as procedural
audits, formal methods, or model documentation, are the mechanisms used to
achieve this. However, these methods are either expensive or heavily manual and
ill-suited for the opaque, "black box" nature of most AI models. An intractable
conflict emerges: high auditability and verifiability are required by law, but
such transparency conflicts with the need to protect assets being audited-e.g.,
confidential data and proprietary models-leading to weakened accountability. To
address this challenge, this paper introduces ZKMLOps, a novel MLOps
verification framework that operationalizes Zero-Knowledge Proofs
(ZKPs)-cryptographic protocols allowing a prover to convince a verifier that a
statement is true without revealing additional information-within
Machine-Learning Operations lifecycles. By integrating ZKPs with established
software engineering patterns, ZKMLOps provides a modular and repeatable
process for generating verifiable cryptographic proof of compliance. We
evaluate the framework's practicality through a study of regulatory compliance
in financial risk auditing and assess feasibility through an empirical
evaluation of top ZKP protocols, analyzing performance trade-offs for ML models
of increasing complexity.

</details>


### [17] [Online and Interactive Bayesian Inference Debugging](https://arxiv.org/abs/2510.26579)
*Nathanael Nussbaumer,Markus Böck,Jürgen Cito*

Main category: cs.SE

TL;DR: 提出了一种新的贝叶斯推理调试方法，通过在开发环境中直接集成调试工具，显著减少了调试时间和所需知识。


<details>
  <summary>Details</summary>
Motivation: 概率编程虽然便于构建贝叶斯模型和进行后验推断，但推理调试非常困难，需要大量时间和专业知识。

Method: 设计了一个满足关键需求的贝叶斯推理调试框架，直接在开发环境中提供在线交互式调试功能。

Result: 在18位有经验参与者的研究中，该方法显著减少了推理调试任务的时间和难度。

Conclusion: 提出的在线交互式贝叶斯推理调试方法有效解决了概率编程中的调试难题。

Abstract: Probabilistic programming is a rapidly developing programming paradigm which
enables the formulation of Bayesian models as programs and the automation of
posterior inference. It facilitates the development of models and conducting
Bayesian inference, which makes these techniques available to practitioners
from multiple fields. Nevertheless, probabilistic programming is notoriously
difficult as identifying and repairing issues with inference requires a lot of
time and deep knowledge. Through this work, we introduce a novel approach to
debugging Bayesian inference that reduces time and required knowledge
significantly. We discuss several requirements a Bayesian inference debugging
framework has to fulfill, and propose a new tool that meets these key
requirements directly within the development environment. We evaluate our
results in a study with 18 experienced participants and show that our approach
to online and interactive debugging of Bayesian inference significantly reduces
time and difficulty on inference debugging tasks.

</details>


### [18] [Stitch: Step-by-step LLM Guided Tutoring for Scratch](https://arxiv.org/abs/2510.26634)
*Yuan Si,Kyle Qi,Daming Li,Hanyuan Shi,Jialu Zhang*

Main category: cs.SE

TL;DR: Stitch是一个交互式编程辅导系统，通过逐步脚手架式指导取代直接展示答案，帮助学习者理解错误并选择性修复，显著提升学习效果。


<details>
  <summary>Details</summary>
Motivation: 现有调试工作流直接展示正确答案，虽然能修复错误但削弱了问题解决能力的培养。需要一种更有效的反馈机制来促进学习。

Method: 使用Diff-Analyze模块对比学生项目与参考实现，识别关键差异，利用大语言模型解释差异重要性，通过自定义渲染引擎高亮显示，支持学习者选择性应用部分修复。

Result: 实证研究表明，Stitch在提升学习效果方面优于直接展示答案的方法和现有自动化反馈生成工具。

Conclusion: 逐步辅导系统比直接展示答案更有效，为基于块的编程环境中的有效反馈机制提供了新证据。

Abstract: Block-based environments such as Scratch are increasingly popular in
programming education. While block syntax reduces surface errors, semantic bugs
remain common and challenging for novices to resolve. Existing debugging
workflows typically show the correct program directly to learners, a strategy
that may fix errors but undermines the development of problem-solving skills.
  We present Stitch, an interactive tutoring system that replaces "showing the
answer" with step-by-step scaffolding. The system's Diff-Analyze module
contrasts a student's project with a reference implementation, identifies the
most critical differences, and uses a large language model to explain why these
changes matter. Learners inspect highlighted blocks through a custom rendering
engine, understand the explanations, and selectively apply partial fixes. This
iterative process continues until the intended functionality is achieved.
  We evaluate Stitch in an empirical study, comparing it against a
state-of-the-art automated feedback generation tool for Scratch. Our key
insight is that simply presenting the correct program is pedagogically
ineffective. In contrast, our interactive, step-by-step guided system promotes
a more effective learning experience. More broadly, what constitutes effective
feedback in block-based programming remains an open question. Our evaluation
provides new evidence that step-by-step tutoring significantly enhances
learning outcomes, outperforming both direct-answer approaches and current
automated feedback generation tools.

</details>


### [19] [Process-based Indicators of Vulnerability Re-Introducing Code Changes: An Exploratory Case Study](https://arxiv.org/abs/2510.26676)
*Samiha Shimmi,Nicholas M. Synovic,Mona Rahimi,George K. Thiruvathukal*

Main category: cs.SE

TL;DR: 本文通过分析ImageMagick项目，发现软件漏洞重新引入与开发过程指标（如问题积压、问题密度）密切相关，强调需要结合过程指标和代码指标来预测风险修复。


<details>
  <summary>Details</summary>
Motivation: 软件漏洞在修复后经常持续存在或重新出现，现有研究很少探索过程指标是否能揭示随时间推移的风险开发活动，这对预测和缓解软件漏洞至关重要。

Method: 在ImageMagick项目中进行案例研究，将纵向过程指标（如总线因子、问题密度、问题积压）与漏洞重新引入活动相关联，涵盖76个重新引入的漏洞实例。

Result: 研究发现重新引入通常与问题积压增加和问题密度波动相一致，反映了问题管理和团队响应能力的短期低效。

Conclusion: 漏洞重新引入很少是孤立行动的结果，而是累积开发活动和社会技术条件的产物，过程指标与代码变更在理解和缓解漏洞重新引入中起着关键作用。

Abstract: Software vulnerabilities often persist or re-emerge even after being fixed,
revealing the complex interplay between code evolution and socio-technical
factors. While source code metrics provide useful indicators of
vulnerabilities, software engineering process metrics can uncover patterns that
lead to their introduction. Yet few studies have explored whether process
metrics can reveal risky development activities over time -- insights that are
essential for anticipating and mitigating software vulnerabilities. This work
highlights the critical role of process metrics along with code changes in
understanding and mitigating vulnerability reintroduction. We move beyond
file-level prediction and instead analyze security fixes at the commit level,
focusing not only on whether a single fix introduces a vulnerability but also
on the longer sequences of changes through which vulnerabilities evolve and
re-emerge. Our approach emphasizes that reintroduction is rarely the result of
one isolated action, but emerges from cumulative development activities and
socio-technical conditions. To support this analysis, we conducted a case study
on the ImageMagick project by correlating longitudinal process metrics such as
bus factor, issue density, and issue spoilage with vulnerability reintroduction
activities, encompassing 76 instances of reintroduced vulnerabilities. Our
findings show that reintroductions often align with increased issue spoilage
and fluctuating issue density, reflecting short-term inefficiencies in issue
management and team responsiveness. These observations provide a foundation for
broader studies that combine process and code metrics to predict risky fixes
and strengthen software security.

</details>


### [20] [Using Copilot Agent Mode to Automate Library Migration: A Quantitative Assessment](https://arxiv.org/abs/2510.26699)
*Aylton Almeida,Laerte Xavier,Marco Tulio Valente*

Main category: cs.SE

TL;DR: 使用GitHub Copilot Agent Mode自动迁移SQLAlchemy库版本，在API迁移覆盖率达到100%的情况下，但应用功能测试通过率仅为39.75%。


<details>
  <summary>Details</summary>
Motivation: 软件系统更新对于避免技术债务、安全漏洞和遗留系统僵化至关重要，但更新过程耗时且容易出错。LLM和智能编码系统为自动化此类维护任务提供了新机会。

Method: 使用GitHub的Copilot Agent Mode（一种能够规划和执行多步骤迁移工作流的自主AI系统），在10个客户端应用数据集中评估SQLAlchemy库的更新。引入迁移覆盖率指标来量化正确迁移的API使用点比例。

Result: LLM代理能够成功迁移SQLAlchemy版本间的功能和API使用（迁移覆盖率中位数：100%），但未能保持应用程序功能，导致测试通过率较低（中位数：39.75%）。

Conclusion: 虽然LLM代理在API迁移方面表现出色，但在保持应用功能完整性方面存在显著挑战，表明自动化迁移系统需要进一步改进以确保功能正确性。

Abstract: Keeping software systems up to date is essential to avoid technical debt,
security vulnerabilities, and the rigidity typical of legacy systems. However,
updating libraries and frameworks remains a time consuming and error-prone
process. Recent advances in Large Language Models (LLMs) and agentic coding
systems offer new opportunities for automating such maintenance tasks. In this
paper, we evaluate the update of a well-known Python library, SQLAlchemy,
across a dataset of ten client applications. For this task, we use the Github's
Copilot Agent Mode, an autonomous AI systema capable of planning and executing
multi-step migration workflows. To assess the effectiveness of the automated
migration, we also introduce Migration Coverage, a metric that quantifies the
proportion of API usage points correctly migrated. The results of our study
show that the LLM agent was capable of migrating functionalities and API usages
between SQLAlchemy versions (migration coverage: 100%, median), but failed to
maintain the application functionality, leading to a low test-pass rate
(39.75%, median).

</details>


### [21] [Optimized Log Parsing with Syntactic Modifications](https://arxiv.org/abs/2510.26793)
*Nafid Enan,Gias Uddin*

Main category: cs.SE

TL;DR: 该论文对基于语法和语义的日志解析器进行了全面的实证研究，发现语义方法在模板识别方面表现更好，而语法方法效率更高且分组精度更好。基于研究结果提出了SynLog+模块，显著提高了两种解析器的精度。


<details>
  <summary>Details</summary>
Motivation: 日志解析是自动化日志分析的第一步，但现有日志解析器采用不同技术和架构，需要系统评估来理解它们的特性和性能差异。

Method: 通过实证研究比较语法基和语义基日志解析器，以及单阶段和两阶段解析架构的性能表现。

Result: 语义方法在正确识别模板方面表现更好，语法基解析器效率高10-1000倍且分组精度更好。两阶段架构相比单阶段架构能持续提高精度。

Conclusion: 提出的SynLog+模板识别模块作为两阶段日志解析架构的第二阶段，能显著提高语法基和语义基解析器的解析精度，且几乎没有额外运行时成本。

Abstract: Logs provide valuable insights into system runtime and assist in software
development and maintenance. Log parsing, which converts semi-structured log
data into structured log data, is often the first step in automated log
analysis. Given the wide range of log parsers utilizing diverse techniques, it
is essential to evaluate them to understand their characteristics and
performance. In this paper, we conduct a comprehensive empirical study
comparing syntax- and semantic-based log parsers, as well as single-phase and
two-phase parsing architectures. Our experiments reveal that semantic-based
methods perform better at identifying the correct templates and syntax-based
log parsers are 10 to 1,000 times more efficient and provide better grouping
accuracy although they fall short in accurate template identification.
Moreover, two-phase architecture consistently improves accuracy compared to
single-phase architecture. Based on the findings of this study, we propose
SynLog+, a template identification module that acts as the second phase in a
two-phase log parsing architecture. SynLog+ improves the parsing accuracy of
syntax-based and semantic-based log parsers by 236\% and 20\% on average,
respectively, with virtually no additional runtime cost.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [22] [Attention Augmented GNN RNN-Attention Models for Advanced Cybersecurity Intrusion Detection](https://arxiv.org/abs/2510.25802)
*Jayant Biradar,Smit Shah,Tanmay Naik*

Main category: cs.CR

TL;DR: 提出了一种结合图神经网络、循环神经网络和多头注意力机制的混合深度学习架构，用于提升网络安全入侵检测能力。


<details>
  <summary>Details</summary>
Motivation: 现代实时入侵检测系统需要能够有效捕捉网络流量的空间依赖性和时间动态，同时提供模型可解释性以帮助安全分析师聚焦高影响安全事件。

Method: 使用UNSW-NB15数据集，通过GNN捕捉空间结构关系，RNN分析时序动态，多头注意力机制增强特征选择和模型可解释性。

Result: 在准确率、精确率、召回率和F1分数等多个评估指标上，该混合模型优于传统机器学习方法和单一深度学习模型，特别在检测APT、DDoS和零日攻击方面表现优异。

Conclusion: 该混合架构是复杂网络环境中下一代网络安全应用的有前景解决方案。

Abstract: In this paper, we propose a novel hybrid deep learning architecture that
synergistically combines Graph Neural Networks (GNNs), Recurrent Neural
Networks (RNNs), and multi-head attention mechanisms to significantly enhance
cybersecurity intrusion detection capabilities. By leveraging the comprehensive
UNSW-NB15 dataset containing diverse network traffic patterns, our approach
effectively captures both spatial dependencies through graph structural
relationships and temporal dynamics through sequential analysis of network
events. The integrated attention mechanism provides dual benefits of improved
model interpretability and enhanced feature selection, enabling cybersecurity
analysts to focus computational resources on high-impact security events -- a
critical requirement in modern real-time intrusion detection systems. Our
extensive experimental evaluation demonstrates that the proposed hybrid model
achieves superior performance compared to traditional machine learning
approaches and standalone deep learning models across multiple evaluation
metrics, including accuracy, precision, recall, and F1-score. The model
achieves particularly strong performance in detecting sophisticated attack
patterns such as Advanced Persistent Threats (APTs), Distributed Denial of
Service (DDoS) attacks, and zero-day exploits, making it a promising solution
for next-generation cybersecurity applications in complex network environments.

</details>


### [23] [APThreatHunter: An automated planning-based threat hunting framework](https://arxiv.org/abs/2510.25806)
*Mustafa F. Abdelwahed,Ahmed Shafee,Joan Espasa*

Main category: cs.CR

TL;DR: APThreatHunter是一个自动化网络威胁狩猎解决方案，通过自动生成假设来减少人工干预，消除分析师偏见，降低时间和成本。


<details>
  <summary>Details</summary>
Motivation: 网络攻击威胁经济利益、关键基础设施和公共健康安全。传统网络威胁狩猎需要手动创建和确认假设，耗时且存在偏见。

Method: 引入APThreatHunter，基于系统当前状态和一组指标自动生成假设，展示可能的风险并指示是否检测到风险。

Result: 使用真实Android恶意软件样本进行评估，结果显示自动化规划在目标假设生成方面具有实用性。

Conclusion: 自动化规划可用于网络威胁狩猎活动中的目标假设生成，展现了实际应用价值。

Abstract: Cyber attacks threaten economic interests, critical infrastructure, and
public health and safety. To counter this, entities adopt cyber threat hunting,
a proactive approach that involves formulating hypotheses and searching for
attack patterns within organisational networks. Automating cyber threat hunting
presents challenges, particularly in generating hypotheses, as it is a manually
created and confirmed process, making it time-consuming. To address these
challenges, we introduce APThreatHunter, an automated threat hunting solution
that generates hypotheses with minimal human intervention, eliminating analyst
bias and reducing time and cost. This is done by presenting possible risks
based on the system's current state and a set of indicators to indicate whether
any of the detected risks are happening or not. We evaluated APThreatHunter
using real-world Android malware samples, and the results revealed the
practicality of using automated planning for goal hypothesis generation in
cyber threat hunting activities.

</details>


### [24] [Adversarial Pre-Padding: Generating Evasive Network Traffic Against Transformer-Based Classifiers](https://arxiv.org/abs/2510.25810)
*Quanliang Jing,Xinxin Fan,Yanyan Liu,Jingping Bi*

Main category: cs.CR

TL;DR: 提出了一种名为AdvTraffic的对抗性流量生成方法，通过预填充策略和强化学习来优化流量扰动，有效对抗基于Transformer的流量分类器，将分类准确率从99%降至25.68%。


<details>
  <summary>Details</summary>
Motivation: 随着预训练模型特别是基于Transformer的分类器的发展，现有的流量混淆技术变得脆弱，这些分类器能达到99%以上的分类准确率，因此需要新的防御方法。

Method: 采用预填充策略修改数据包，并使用强化学习模型优化网络流量扰动，以最大化对抗基于Transformer分类模型的效果。

Result: 在多个真实数据集上的实验表明，该方法能有效削弱基于Transformer的分类器，将分类准确率从99%显著降低到25.68%。

Conclusion: 这是首次将对抗性扰动技术应用于防御基于Transformer的流量分类器，且该方法易于在实际网络环境中部署。

Abstract: To date, traffic obfuscation techniques have been widely adopted to protect
network data privacy and security by obscuring the true patterns of traffic.
Nevertheless, as the pre-trained models emerge, especially transformer-based
classifiers, existing traffic obfuscation methods become increasingly
vulnerable, as witnessed by current studies reporting the traffic
classification accuracy up to 99\% or higher. To counter such high-performance
transformer-based classification models, we in this paper propose a novel and
effective \underline{adv}ersarial \underline{traffic}-generating approach
(AdvTraffic\footnote{The code and data are available at: http://xxx}). Our
approach has two key innovations: (i) a pre-padding strategy is proposed to
modify packets, which effectively overcomes the limitations of existing
research against transformer-based models for network traffic classification;
and (ii) a reinforcement learning model is employed to optimize network traffic
perturbations, aiming to maximize adversarial effectiveness against
transformer-based classification models. To the best of our knowledge, this is
the first attempt to apply adversarial perturbation techniques to defend
against transformer-based traffic classifiers. Furthermore, our method can be
easily deployed into practical network environments. Finally, multi-faceted
experiments are conducted across several real-world datasets, and the
experimental results demonstrate that our proposed method can effectively
undermine transformer-based classifiers, significantly reducing classification
accuracy from 99\% to as low as 25.68\%.

</details>


### [25] [Identity Management for Agentic AI: The new frontier of authorization, authentication, and security for an AI agent world](https://arxiv.org/abs/2510.25819)
*Tobin South,Subramanya Nagabhushanaradhya,Ayesha Dissanayaka,Sarah Cecchetti,George Fletcher,Victor Lu,Aldo Pietropaolo,Dean H. Saxe,Jeff Lombardo,Abhishek Maligehalli Shivalingaiah,Stan Bounev,Alex Keisner,Andor Kesselman,Zack Proser,Ginny Fahs,Andrew Bunyea,Ben Moskowitz,Atul Tulshibagwale,Dazza Greenwood,Jiaxin Pei,Alex Pentland*

Main category: cs.CR

TL;DR: 该OpenID基金会白皮书针对AI代理与访问管理的交叉领域，分析了当前认证授权挑战并提出了未来自主系统的战略议程。


<details>
  <summary>Details</summary>
Motivation: AI代理的快速发展带来了认证、授权和身份管理的紧迫挑战，现有协议显示需要明确的认证授权最佳实践，高度自主代理的愿景引发了可扩展访问控制等复杂问题。

Method: 概述了保护当前代理的可用资源，并提出了解决未来广泛自主系统基础认证、授权和身份问题的战略议程。

Result: 识别了AI代理访问管理的关键挑战，包括可扩展访问控制、代理中心身份、AI工作负载区分和委托授权等问题。

Conclusion: 需要为明天的广泛自主系统解决基础的认证、授权和身份问题，白皮书为此提供了战略议程和资源指导。

Abstract: The rapid rise of AI agents presents urgent challenges in authentication,
authorization, and identity management. Current agent-centric protocols (like
MCP) highlight the demand for clarified best practices in authentication and
authorization. Looking ahead, ambitions for highly autonomous agents raise
complex long-term questions regarding scalable access control, agent-centric
identities, AI workload differentiation, and delegated authority. This OpenID
Foundation whitepaper is for stakeholders at the intersection of AI agents and
access management. It outlines the resources already available for securing
today's agents and presents a strategic agenda to address the foundational
authentication, authorization, and identity problems pivotal for tomorrow's
widespread autonomous systems.

</details>


### [26] [A Critical Roadmap to Driver Authentication via CAN Bus: Dataset Review, Introduction of the Kidmose CANid Dataset (KCID), and Proof of Concept](https://arxiv.org/abs/2510.25856)
*Brooke Elizabeth Kidmose,Andreas Brasen Kidmose,Cliff C. Zou*

Main category: cs.CR

TL;DR: 该论文分析了现有驾驶员指纹识别数据集的局限性，提出了Kidmose CANid数据集(KCID)来解决这些问题，并展示了基于CAN总线的驾驶员认证防盗系统的可行性。


<details>
  <summary>Details</summary>
Motivation: 现代车辆虽然配备了防盗器和无钥匙进入系统，但仍面临CAN总线漏洞被利用的风险。驾驶员认证作为深度防御的额外保护层具有潜力，但现有数据集存在严重局限性。

Method: 引入KCID数据集，包含16名驾驶员在4辆车上的原始CAN总线数据，包括人口统计信息和日常驾驶数据；开发了驾驶员认证防盗框架并在单板计算机上实现原型系统。

Result: 通过未改装乘用车的实际道路试验，证明了基于CAN总线的驾驶员认证防盗系统的实际可行性。

Conclusion: 该研究为开发稳健、可部署的驾驶员认证系统提供了必要的数据和方法基础，KCID数据集还可用于驾驶员画像、机械异常检测、年轻驾驶员监控和受损驾驶检测等应用。

Abstract: Modern vehicles remain vulnerable to unauthorized use and theft despite
traditional security measures including immobilizers and keyless entry systems.
Criminals exploit vulnerabilities in Controller Area Network (CAN) bus systems
to bypass authentication mechanisms, while social media trends have expanded
auto theft to include recreational joyriding by underage drivers. Driver
authentication via CAN bus data offers a promising additional layer of
defense-in-depth protection, but existing open-access driver fingerprinting
datasets suffer from critical limitations including reliance on decoded
diagnostic data rather than raw CAN traffic, artificial fixed-route
experimental designs, insufficient sampling rates, and lack of demographic
information.
  This paper provides a comprehensive review of existing open-access driver
fingerprinting datasets, analyzing their strengths and limitations to guide
practitioners in dataset selection. We introduce the Kidmose CANid Dataset
(KCID), which addresses these fundamental shortcomings by providing raw CAN bus
data from 16 drivers across four vehicles, including essential demographic
information and both daily driving and controlled fixed-route data. Beyond
dataset contributions, we present a driver authentication anti-theft framework
and implement a proof-of-concept prototype on a single-board computer. Through
live road trials with an unaltered passenger vehicle, we demonstrate the
practical feasibility of CAN bus-based driver authentication anti-theft
systems. Finally, we explore diverse applications of KCID beyond driver
authentication, including driver profiling for insurance and safety
assessments, mechanical anomaly detection, young driver monitoring, and
impaired driving detection. This work provides researchers with both the data
and methodological foundation necessary to develop robust, deployable driver
authentication systems...

</details>


### [27] [AAGATE: A NIST AI RMF-Aligned Governance Platform for Agentic AI](https://arxiv.org/abs/2510.25863)
*Ken Huang,Jerry Huang,Yasir Mehmood,Hammad Atta,Muhammad Zeeshan Baig,Muhammad Aziz Ul Haq*

Main category: cs.CR

TL;DR: AAGATE是一个Kubernetes原生控制平面，专门解决自主语言模型驱动代理在生产环境中的安全和治理挑战，通过集成NIST AI风险管理框架和多种安全框架提供持续可验证的治理方案。


<details>
  <summary>Details</summary>
Motivation: 传统应用安全工具无法应对即兴、机器速度的自主AI系统带来的独特安全和治理挑战，需要专门的控制平面来确保安全、负责任和可扩展的部署。

Method: 采用Kubernetes原生架构，集成NIST AI RMF框架，结合零信任服务网格、可解释策略引擎、行为分析和去中心化问责钩子，并扩展DIRF、LPCI和QSAF等专门防御机制。

Result: 提供了一个连续、可验证的治理解决方案，能够覆盖系统性、对抗性和伦理风险，确保自主AI代理的安全部署。

Conclusion: AAGATE框架通过综合的安全治理方法，为自主AI系统在生产环境中的安全、负责任部署提供了有效的技术保障。

Abstract: This paper introduces the Agentic AI Governance Assurance & Trust Engine
(AAGATE), a Kubernetes-native control plane designed to address the unique
security and governance challenges posed by autonomous, language-model-driven
agents in production. Recognizing the limitations of traditional Application
Security (AppSec) tooling for improvisational, machine-speed systems, AAGATE
operationalizes the NIST AI Risk Management Framework (AI RMF). It integrates
specialized security frameworks for each RMF function: the Agentic AI Threat
Modeling MAESTRO framework for Map, a hybrid of OWASP's AIVSS and SEI's SSVC
for Measure, and the Cloud Security Alliance's Agentic AI Red Teaming Guide for
Manage. By incorporating a zero-trust service mesh, an explainable policy
engine, behavioral analytics, and decentralized accountability hooks, AAGATE
provides a continuous, verifiable governance solution for agentic AI, enabling
safe, accountable, and scalable deployment. The framework is further extended
with DIRF for digital identity rights, LPCI defenses for logic-layer injection,
and QSAF monitors for cognitive degradation, ensuring governance spans
systemic, adversarial, and ethical risks.

</details>


### [28] [Foundations of Fiat-Denominated Loans Collateralized by Cryptocurrencies](https://arxiv.org/abs/2510.25878)
*Pavel Hubáček,Jan Václavek,Michelle Yeo*

Main category: cs.CR

TL;DR: 该论文研究了以加密货币（如比特币）为抵押的法币贷款安全协议，提出了基于可信仲裁的有限托管协议，并进行了博弈论分析。


<details>
  <summary>Details</summary>
Motivation: 随着加密货币作为金融资产的重要性日益增加，需要将其从投机对象转变为更接近标准金融工具（如贷款）的应用。

Method: 开发了基于可信仲裁的有限托管协议，用于以加密货币为抵押的法币贷款，并进行了博弈论分析。

Result: 提出了可行的安全协议框架，能够实现加密货币抵押的法币贷款，同时识别了未来研究的多个有趣方向。

Conclusion: 该研究为加密货币抵押贷款提供了理论基础和协议框架，展示了此类金融产品的可行性，并为未来研究指明了方向。

Abstract: The rising importance of cryptocurrencies as financial assets pushed their
applicability from an object of speculation closer to standard financial
instruments such as loans. In this work, we initiate the study of secure
protocols that enable fiat-denominated loans collateralized by cryptocurrencies
such as Bitcoin. We provide limited-custodial protocols for such loans relying
only on trusted arbitration and provide their game-theoretical analysis. We
also highlight various interesting directions for future research.

</details>


### [29] [FakeZero: Real-Time, Privacy-Preserving Misinformation Detection for Facebook and X](https://arxiv.org/abs/2510.25932)
*Soufiane Essahli,Oussama Sarsar,Imane Fouad,Anas Motii,Ahmed Bentajer*

Main category: cs.CR

TL;DR: FakeZero是一个完全客户端、跨平台的浏览器扩展，能够在Facebook和X平台上实时标记不可靠的帖子，所有计算都在本地运行，保护用户隐私。


<details>
  <summary>Details</summary>
Motivation: 社交媒体以空前速度传播信息，加速了错误信息的传播并威胁公共讨论。需要开发能够在用户浏览时实时检测虚假信息的工具。

Method: 采用三阶段训练课程：基线微调、使用焦点损失的领域自适应训练、对抗增强和后训练量化。使用DistilBERT-Quant和TinyBERT-Quant模型，所有计算通过Chromium消息API在本地运行。

Result: 在239,000个帖子的数据集上，DistilBERT-Quant模型达到97.1%宏F1分数、97.4%准确率和0.996的AUROC，中位延迟约103毫秒。TinyBERT-Quant变体保持95.7%宏F1分数和96.1%准确率，模型大小缩小至14.7MB，延迟降至约40毫秒。

Conclusion: 在严格资源预算下实现高质量的虚假新闻检测是可行的，该扩展可以为政策制定者提供有价值的工具来遏制错误信息传播，并为研究人员收集大规模真实世界虚假新闻数据集打开大门。

Abstract: Social platforms distribute information at unprecedented speed, which in turn
accelerates the spread of misinformation and threatens public discourse. We
present FakeZero, a fully client-side, cross-platform browser extension that
flags unreliable posts on Facebook and X (formerly Twitter) while the user
scrolls. All computation, DOM scraping, tokenisation, Transformer inference,
and UI rendering run locally through the Chromium messaging API, so no personal
data leaves the device.FakeZero employs a three-stage training curriculum:
baseline fine-tuning and domain-adaptive training enhanced with focal loss,
adversarial augmentation, and post-training quantisation. Evaluated on a
dataset of 239,000 posts, the DistilBERT-Quant model (67.6 MB) reaches 97.1%
macro-F1, 97.4% accuracy, and an AUROC of 0.996, with a median latency of
approximately 103 ms on a commodity laptop. A memory-efficient TinyBERT-Quant
variant retains 95.7% macro-F1 and 96.1% accuracy while shrinking the model to
14.7 MB and lowering latency to approximately 40 ms, showing that high-quality
fake-news detection is feasible under tight resource budgets with only modest
performance loss.By providing inline credibility cues, the extension can serve
as a valuable tool for policymakers seeking to curb the spread of
misinformation across social networks. With user consent, FakeZero also opens
the door for researchers to collect large-scale datasets of fake news in the
wild, enabling deeper analysis and the development of more robust detection
techniques.

</details>


### [30] [SoK: Honeypots & LLMs, More Than the Sum of Their Parts?](https://arxiv.org/abs/2510.25939)
*Robert A. Bridges,Thomas R. Mitchell,Mauricio Muñoz,Ted Henriksson*

Main category: cs.CR

TL;DR: 本文首次系统梳理了基于LLM的蜜罐研究领域，提出了检测向量分类法、蜜罐架构分析和日志分析演进路径，并展望了自主欺骗系统的未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 解决蜜罐设计中高保真欺骗与低操作风险之间的矛盾，填补LLM蜜罐研究领域缺乏系统性理解的空白。

Method: 采用知识系统化方法，调查了三个关键研究领域：蜜罐检测向量分类、LLM蜜罐文献综合分析和蜜罐日志分析演进路径。

Result: 提出了检测向量分类法，识别了典型LLM蜜罐架构和评估趋势，描绘了日志分析从数据简化到自动情报生成的演进路径。

Conclusion: LLM蜜罐技术的真正潜力在于创建自主、自我改进的欺骗系统，以应对智能自动化攻击者的新兴威胁。

Abstract: The advent of Large Language Models (LLMs) promised to resolve the
long-standing paradox in honeypot design: achieving high-fidelity deception
with low operational risk. However, despite a flurry of research since late
2022, progress has been incremental, and the field lacks a cohesive
understanding of the emerging architectural patterns, core challenges, and
evaluation paradigms. To fill this gap, this Systematization of Knowledge (SoK)
paper provides the first comprehensive overview of this new domain. We survey
and systematize three critical, intersecting research areas: first, we provide
a taxonomy of honeypot detection vectors, structuring the core problems that
LLM-based realism must solve; second, we synthesize the emerging literature on
LLM-honeypots, identifying a canonical architecture and key evaluation trends;
and third, we chart the evolutionary path of honeypot log analysis, from simple
data reduction to automated intelligence generation. We synthesize these
findings into a forward-looking research roadmap, arguing that the true
potential of this technology lies in creating autonomous, self-improving
deception systems to counter the emerging threat of intelligent, automated
attackers.

</details>


### [31] [WaveVerif: Acoustic Side-Channel based Verification of Robotic Workflows](https://arxiv.org/abs/2510.25960)
*Zeynep Yasemin Erdogan,Shishir Nagaraja,Chuadhry Mujeeb Ahmed,Ryan Shah*

Main category: cs.CR

TL;DR: 使用声学侧信道分析（ASCA）通过机器人运动产生的声音来验证机器人是否正确执行命令的框架，在基准条件下可实现超过80%的准确率。


<details>
  <summary>Details</summary>
Motivation: 在敏感机器人环境中需要实时、低成本、无需硬件修改的被动验证方法，声学信号为此提供了可能。

Method: 开发基于机器学习的验证系统，使用四种分类器（SVM、DNN、RNN、CNN）分析机器人运动产生的声学发射信号，考虑运动速度、方向和麦克风距离等因素。

Result: 单个机器人运动验证准确率超过80%，拾取放置和包装等工作流程也能以类似高置信度识别。

Conclusion: 声学信号能够支持敏感机器人环境中的实时、低成本、被动验证，无需硬件修改。

Abstract: In this paper, we present a framework that uses acoustic side-channel
analysis (ASCA) to monitor and verify whether a robot correctly executes its
intended commands. We develop and evaluate a machine-learning-based workflow
verification system that uses acoustic emissions generated by robotic
movements. The system can determine whether real-time behavior is consistent
with expected commands. The evaluation takes into account movement speed,
direction, and microphone distance. The results show that individual robot
movements can be validated with over 80% accuracy under baseline conditions
using four different classifiers: Support Vector Machine (SVM), Deep Neural
Network (DNN), Recurrent Neural Network (RNN), and Convolutional Neural Network
(CNN). Additionally, workflows such as pick-and-place and packing could be
identified with similarly high confidence. Our findings demonstrate that
acoustic signals can support real-time, low-cost, passive verification in
sensitive robotic environments without requiring hardware modifications.

</details>


### [32] [Message Recovery Attack in NTRU via Knapsack](https://arxiv.org/abs/2510.26003)
*Eirini Poimenidou,K. A. Draziotis*

Main category: cs.CR

TL;DR: 本文提出基于模背包问题的消息恢复攻击，适用于所有NTRU-HPS密码系统变体。当已知消息和随机向量部分系数时，可将消息解密简化为在格中寻找短向量问题。


<details>
  <summary>Details</summary>
Motivation: 研究在NTRU-HPS密码系统中，需要多少关于消息或消息-随机数对的信息才能实现可行的消息恢复，探索密码系统的安全性边界。

Method: 使用基于模背包问题的攻击方法，通过FLATTER约简技术，将消息解密问题转化为在特定格中寻找短向量的问题。

Result: 当已知约45%的消息和随机向量系数时，攻击在实践中成功恢复消息，在普通台式机上几分钟内即可完成。

Conclusion: NTRU-HPS密码系统在攻击者掌握部分消息信息时存在安全漏洞，需要约45%的系数信息即可实现有效攻击。

Abstract: In the present paper, we introduce a message-recovery attack based on the
Modular Knapsack Problem, applicable to all variants of the NTRU-HPS
cryptosystem. Assuming that a fraction $\epsilon$ of the coefficients of the
message ${\bf{m}}\in\{-1,0,1\}^N$ and of the nonce vector ${\bf
r}\in\{-1,0,1\}^N$ are known in advance at random positions, we reduce message
decryption to finding a short vector in a lattice that encodes an instance of a
modular knapsack system. This allows us to address a key question: how much
information about ${\bf m}$, or about the pair $({\bf m},{\bf r})$, is required
before recovery becomes feasible? A FLATTER reduction successfully recovers the
message, in practice when $\epsilon\approx 0.45$. Our implementation finds
${\bf m}$ within a few minutes on a commodity desktop.

</details>


### [33] [SIRAJ: Diverse and Efficient Red-Teaming for LLM Agents via Distilled Structured Reasoning](https://arxiv.org/abs/2510.26037)
*Kaiwen Zhou,Ahmed Elgohary,A S M Iftekhar,Amin Saied*

Main category: cs.CR

TL;DR: SIRAJ是一个通用的红队测试框架，用于发现黑盒LLM代理的安全漏洞。通过动态两步流程生成多样化测试用例，并采用模型蒸馏技术优化成本，显著提升了攻击成功率和风险覆盖范围。


<details>
  <summary>Details</summary>
Motivation: LLM代理的工具调用能力带来了新的安全风险，需要全面的红队测试系统来发现漏洞并确保安全部署。

Method: 采用动态两步流程：首先生成覆盖各种风险结果和工具使用轨迹的种子测试用例，然后基于执行轨迹迭代构建和优化基于模型的对抗攻击。使用模型蒸馏技术训练更小的但同样有效的红队模型。

Result: 种子测试用例生成使风险结果和工具调用轨迹的覆盖范围提高了2-2.5倍。蒸馏后的8B红队模型攻击成功率提升100%，超越了671B的Deepseek-R1模型。

Conclusion: SIRAJ框架在迭代优化、结构化推理和模型泛化方面表现出色，为LLM代理的安全测试提供了有效的解决方案。

Abstract: The ability of LLM agents to plan and invoke tools exposes them to new safety
risks, making a comprehensive red-teaming system crucial for discovering
vulnerabilities and ensuring their safe deployment. We present SIRAJ: a generic
red-teaming framework for arbitrary black-box LLM agents. We employ a dynamic
two-step process that starts with an agent definition and generates diverse
seed test cases that cover various risk outcomes, tool-use trajectories, and
risk sources. Then, it iteratively constructs and refines model-based
adversarial attacks based on the execution trajectories of former attempts. To
optimize the red-teaming cost, we present a model distillation approach that
leverages structured forms of a teacher model's reasoning to train smaller
models that are equally effective. Across diverse evaluation agent settings,
our seed test case generation approach yields 2 -- 2.5x boost to the coverage
of risk outcomes and tool-calling trajectories. Our distilled 8B red-teamer
model improves attack success rate by 100%, surpassing the 671B Deepseek-R1
model. Our ablations and analyses validate the effectiveness of the iterative
framework, structured reasoning, and the generalization of our red-teamer
models.

</details>


### [34] [PEEL: A Poisoning-Exposing Encoding Theoretical Framework for Local Differential Privacy](https://arxiv.org/abs/2510.26102)
*Lisha Shuai,Jiuling Dong,Nan Zhang,Shaofeng Tan,Haokun Zhang,Zilong Song,Gaoya Dong,Xiaolong Yang*

Main category: cs.CR

TL;DR: PEEL是一个针对本地差分隐私(LDP)的投毒攻击检测框架，通过重新编码LDP扰动数据来放大投毒效应，无需额外资源或先验知识即可暴露输出和规则投毒攻击。


<details>
  <summary>Details</summary>
Motivation: 现有LDP防御方法要么资源开销过大，要么依赖特定领域先验知识，限制了在实际物联网部署中的应用。需要一种轻量级且不依赖先验的防御方案。

Method: PEEL作为非侵入式后处理模块，通过稀疏化、归一化和低秩投影重新编码LDP扰动数据，利用LDP扰动数据的结构一致性来放大投毒效应。

Result: 理论分析证明PEEL与LDP集成后保持无偏性和统计准确性，同时能有效暴露投毒攻击。评估显示PEEL在投毒检测准确率上优于四种最先进防御方法，并显著降低客户端计算成本。

Conclusion: PEEL是一种高效且轻量级的LDP投毒攻击检测框架，特别适合大规模物联网部署，在保持隐私保护效果的同时有效防御投毒攻击。

Abstract: Local Differential Privacy (LDP) is a widely adopted privacy-protection model
in the Internet of Things (IoT) due to its lightweight, decentralized, and
scalable nature. However, it is vulnerable to poisoning attacks, and existing
defenses either incur prohibitive resource overheads or rely on domain-specific
prior knowledge, limiting their practical deployment. To address these
limitations, we propose PEEL, a Poisoning-Exposing Encoding theoretical
framework for LDP, which departs from resource- or prior-dependent
countermeasures and instead leverages the inherent structural consistency of
LDP-perturbed data. As a non-intrusive post-processing module, PEEL amplifies
stealthy poisoning effects by re-encoding LDP-perturbed data via
sparsification, normalization, and low-rank projection, thereby revealing both
output and rule poisoning attacks through structural inconsistencies in the
reconstructed space. Theoretical analysis proves that PEEL, integrated with
LDP, retains unbiasedness and statistical accuracy, while being robust to
expose both output and rule poisoning attacks. Moreover, evaluation results
show that LDP-integrated PEEL not only outperforms four state-of-the-art
defenses in terms of poisoning exposure accuracy but also significantly reduces
client-side computational costs, making it highly suitable for large-scale IoT
deployments.

</details>


### [35] [Security Vulnerabilities in AI-Generated Code: A Large-Scale Analysis of Public GitHub Repositories](https://arxiv.org/abs/2510.26103)
*Maximilian Schreiber,Pascal Tippe*

Main category: cs.CR

TL;DR: 对GitHub上AI生成代码的安全漏洞进行大规模实证分析，发现87.9%的代码没有可识别漏洞，但Python比其他语言更容易出现漏洞，且不同AI工具在不同语言上的安全性能存在差异。


<details>
  <summary>Details</summary>
Motivation: 随着AI代码生成工具的普及，需要了解这些工具生成代码的安全状况，为负责任地集成AI生成代码到开发流程提供指导。

Method: 收集GitHub上7,703个明确标注为AI生成的代码文件，使用CodeQL静态分析工具识别4,241个CWE漏洞实例，涵盖77种漏洞类型。

Result: Python的漏洞率(16.18%-18.50%)显著高于JavaScript(8.66%-8.99%)和TypeScript(2.50%-7.14%)；GitHub Copilot在Python和TypeScript上表现更好，而ChatGPT在JavaScript上更优；39%的文件用于文档生成。

Conclusion: 研究为开发语言特定和上下文感知的安全实践提供了宝贵见解，有助于负责任地将AI生成代码集成到软件开发工作流中。

Abstract: This paper presents a comprehensive empirical analysis of security
vulnerabilities in AI-generated code across public GitHub repositories. We
collected and analyzed 7,703 files explicitly attributed to four major AI
tools: ChatGPT (91.52\%), GitHub Copilot (7.50\%), Amazon CodeWhisperer
(0.52\%), and Tabnine (0.46\%). Using CodeQL static analysis, we identified
4,241 Common Weakness Enumeration (CWE) instances across 77 distinct
vulnerability types. Our findings reveal that while 87.9\% of AI-generated code
does not contain identifiable CWE-mapped vulnerabilities, significant patterns
emerge regarding language-specific vulnerabilities and tool performance. Python
consistently exhibited higher vulnerability rates (16.18\%-18.50\%) compared to
JavaScript (8.66\%-8.99\%) and TypeScript (2.50\%-7.14\%) across all tools. We
observed notable differences in security performance, with GitHub Copilot
achieving better security density for Python (1,739 LOC per CWE) and
TypeScript, while ChatGPT performed better for JavaScript. Additionally, we
discovered widespread use of AI tools for documentation generation (39\% of
collected files), an understudied application with implications for software
maintainability. These findings extend previous work with a significantly
larger dataset and provide valuable insights for developing language-specific
and context-aware security practices for the responsible integration of
AI-generated code into software development workflows.

</details>


### [36] [Confidential FRIT via Homomorphic Encryption](https://arxiv.org/abs/2510.26179)
*Haruki Hoshino,Jungjin Park,Osamu Kaneko,Kiminao Kogiso*

Main category: cs.CR

TL;DR: 提出基于同态加密的保密数据驱动增益调谐框架，用于保护外包到边缘服务器的CPS增益调谐过程安全。


<details>
  <summary>Details</summary>
Motivation: 边缘计算虽然减轻了CPS数据驱动控制的计算负担，但日益复杂的网络攻击需要超越传统IT保护的安全措施，以解决CPS特有的漏洞。

Method: 使用同态加密方案（如ElGamal和CKKS），将矩阵求逆操作替换为向量求和形式，使同态操作得以应用，实现保密FRIT。

Result: 在128位安全级别下的数值示例显示，性能与传统方法相当，同时为安全CPS选择合适的加密方案提供了指导。

Conclusion: 该框架在保持与传统方法相当性能的同时，增强了CPS增益调谐过程的网络安全，为安全CPS的加密方案选择提供了实用指南。

Abstract: Edge computing alleviates the computation burden of data-driven control in
cyber-physical systems (CPSs) by offloading complex processing to edge servers.
However, the increasing sophistication of cyberattacks underscores the need for
security measures that go beyond conventional IT protections and address the
unique vulnerabilities of CPSs. This study proposes a confidential data-driven
gain-tuning framework using homomorphic encryption, such as ElGamal and CKKS
encryption schemes, to enhance cybersecurity in gain-tuning processes
outsourced to external servers. The idea for realizing confidential FRIT is to
replace the matrix inversion operation with a vector summation form, allowing
homomorphic operations to be applied. Numerical examples under 128-bit security
confirm performance comparable to conventional methods while providing
guidelines for selecting suitable encryption schemes for secure CPS.

</details>


### [37] [Who Moved My Transaction? Uncovering Post-Transaction Auditability Vulnerabilities in Modern Super Apps](https://arxiv.org/abs/2510.26210)
*Junlin Liu,Zhaomeng Deng,Ziming Wang,Mengyu Yao,Yifeng Cai,Yutao Hu,Ziqi Zhang,Yao Guo,Ding Li*

Main category: cs.CR

TL;DR: 研究发现超级应用普遍存在交易记录删除漏洞，83%的应用缺乏强认证保护，允许用户永久删除交易历史，威胁审计完整性。


<details>
  <summary>Details</summary>
Motivation: 当前超级应用安全范式过度关注交易前认证，忽视了交易后审计轨迹的脆弱性，用户可轻易删除交易记录来隐藏未授权或敏感活动。

Method: 通过6名志愿者对6个超级应用进行交叉评估的实证研究，分析交易记录删除功能的安全性。

Result: 所有6个应用都允许删除交易记录，但其中5个(83%)缺乏强认证保护，仅1个应用要求生物识别验证。

Conclusion: 这揭示了移动安全领域的关键漏洞，迫切需要转向确保交易后审计完整性的新安全范式。

Abstract: Super apps are the cornerstones of modern digital life, embedding financial
transactions into nearly every aspect of daily routine. The prevailing security
paradigm for these platforms is overwhelmingly focused on pre-transaction
authentication, preventing unauthorized payments before they occur. We argue
that a critical vulnerability vector has been largely overlooked: the fragility
of post-transaction audit trails. We investigate the ease with which a user can
permanently erase their transaction history from an app's interface, thereby
concealing unauthorized or sensitive activities from the account owner. To
quantify this threat, we conducted an empirical study with 6 volunteers who
performed a cross-evaluation on six super apps. Our findings are alarming: all
six applications studied allow users to delete transaction records, yet a
staggering five out of six (83+\%) fail to protect these records with strong
authentication. Only one app in our study required biometric verification for
deletion. This study provides the first concrete evidence of this
near-ubiquitous vulnerability, demonstrating a critical gap in the current
mobile security landscape and underscoring the urgent need for a paradigm shift
towards ensuring post-transaction audit integrity.

</details>


### [38] [Who Grants the Agent Power? Defending Against Instruction Injection via Task-Centric Access Control](https://arxiv.org/abs/2510.26212)
*Yifeng Cai,Ziming Wang,Zhaomeng Deng,Mengyu Yao,Junlin Liu,Yutao Hu,Ziqi Zhang,Yao Guo,Ding Li*

Main category: cs.CR

TL;DR: AgentSentry是一个轻量级运行时任务中心访问控制框架，通过动态生成和执行与用户特定任务对齐的最小临时权限策略，防止AI代理遭受指令注入攻击。


<details>
  <summary>Details</summary>
Motivation: AI代理在移动任务自动化中依赖过度授权和静态权限，存在指令注入漏洞，恶意指令可能劫持代理执行未授权操作。

Method: AgentSentry动态生成和执行任务范围的临时权限策略，而不是授予广泛持久权限，并在任务完成后撤销这些权限。

Result: AgentSentry成功防止了指令注入攻击（如诱骗代理转发私人邮件），同时允许合法任务完成。

Conclusion: 该方法强调了为安全治理下一代自主代理而建立意图对齐安全模型的紧迫需求。

Abstract: AI agents capable of GUI understanding and Model Context Protocol are
increasingly deployed to automate mobile tasks. However, their reliance on
over-privileged, static permissions creates a critical vulnerability:
instruction injection. Malicious instructions, embedded in otherwise benign
content like emails, can hijack the agent to perform unauthorized actions. We
present AgentSentry, a lightweight runtime task-centric access control
framework that enforces dynamic, task-scoped permissions. Instead of granting
broad, persistent permissions, AgentSentry dynamically generates and enforces
minimal, temporary policies aligned with the user's specific task (e.g.,
register for an app), revoking them upon completion. We demonstrate that
AgentSentry successfully prevents an instruction injection attack, where an
agent is tricked into forwarding private emails, while allowing the legitimate
task to complete. Our approach highlights the urgent need for intent-aligned
security models to safely govern the next generation of autonomous agents.

</details>


### [39] [PVMark: Enabling Public Verifiability for LLM Watermarking Schemes](https://arxiv.org/abs/2510.26274)
*Haohua Duan,Liyao Xiang,Xin Zhang*

Main category: cs.CR

TL;DR: PVMark是一个基于零知识证明的插件，使LLM水印检测过程可公开验证，解决了当前水印方案因秘密密钥导致的信任问题。


<details>
  <summary>Details</summary>
Motivation: 当前LLM水印方案存在信任问题：非公开的水印检测无法证明其忠实执行检测过程，这源于秘密密钥的困境——公开密钥会遭受移除攻击，私有密钥则使检测过程不透明。

Method: 基于零知识证明构建水印检测的'正确执行'证明，包括映射、随机数生成、比较和求和等约束条件，实现了多种变体覆盖不同水印方案、哈希函数和ZKP协议。

Result: PVMark在多种环境下有效工作，能够高效地为最先进的LLM水印方案提供公开可验证性，且不影响水印性能。

Conclusion: PVMark解决了水印检测的信任困境，有望在实际中部署使用。

Abstract: Watermarking schemes for large language models (LLMs) have been proposed to
identify the source of the generated text, mitigating the potential threats
emerged from model theft. However, current watermarking solutions hardly
resolve the trust issue: the non-public watermark detection cannot prove itself
faithfully conducting the detection. We observe that it is attributed to the
secret key mostly used in the watermark detection -- it cannot be public, or
the adversary may launch removal attacks provided the key; nor can it be
private, or the watermarking detection is opaque to the public. To resolve the
dilemma, we propose PVMark, a plugin based on zero-knowledge proof (ZKP),
enabling the watermark detection process to be publicly verifiable by third
parties without disclosing any secret key. PVMark hinges upon the proof of
`correct execution' of watermark detection on which a set of ZKP constraints
are built, including mapping, random number generation, comparison, and
summation. We implement multiple variants of PVMark in Python, Rust and Circom,
covering combinations of three watermarking schemes, three hash functions, and
four ZKP protocols, to show our approach effectively works under a variety of
circumstances. By experimental results, PVMark efficiently enables public
verifiability on the state-of-the-art LLM watermarking schemes yet without
compromising the watermarking performance, promising to be deployed in
practice.

</details>


### [40] [A Survey of Heterogeneous Graph Neural Networks for Cybersecurity Anomaly Detection](https://arxiv.org/abs/2510.26307)
*Laura Jiang,Reza Ryan,Qian Li,Nasim Ferdosian*

Main category: cs.CR

TL;DR: 本文对基于异构图神经网络（HGNN）的网络安全异常检测方法进行了全面综述，提出了按异常类型和图动态性分类的分类法，分析了代表性模型及其在网络安全中的应用，并指出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 当前基于HGNN的异常检测研究存在碎片化问题，缺乏统一的建模策略、有限的比较评估以及标准化的基准测试，需要建立系统化的研究框架。

Method: 采用综述研究方法，构建了按异常类型和图动态性分类的分类法，分析了代表性HGNN模型，并评估了常用基准数据集和评价指标。

Result: 建立了HGNN异常检测的系统化分类框架，识别了现有方法的优势和局限性，为未来研究提供了结构化基础。

Conclusion: 该综述为推进基于HGNN的异常检测向可扩展、可解释和实际可部署的解决方案发展奠定了结构化基础，并指出了建模、数据和部署方面的关键开放挑战。

Abstract: Anomaly detection is a critical task in cybersecurity, where identifying
insider threats, access violations, and coordinated attacks is essential for
ensuring system resilience. Graph-based approaches have become increasingly
important for modeling entity interactions, yet most rely on homogeneous and
static structures, which limits their ability to capture the heterogeneity and
temporal evolution of real-world environments. Heterogeneous Graph Neural
Networks (HGNNs) have emerged as a promising paradigm for anomaly detection by
incorporating type-aware transformations and relation-sensitive aggregation,
enabling more expressive modeling of complex cyber data. However, current
research on HGNN-based anomaly detection remains fragmented, with diverse
modeling strategies, limited comparative evaluation, and an absence of
standardized benchmarks. To address this gap, we provide a comprehensive survey
of HGNN-based anomaly detection methods in cybersecurity. We introduce a
taxonomy that classifies approaches by anomaly type and graph dynamics, analyze
representative models, and map them to key cybersecurity applications. We also
review commonly used benchmark datasets and evaluation metrics, highlighting
their strengths and limitations. Finally, we identify key open challenges
related to modeling, data, and deployment, and outline promising directions for
future research. This survey aims to establish a structured foundation for
advancing HGNN-based anomaly detection toward scalable, interpretable, and
practically deployable solutions.

</details>


### [41] [SSCL-BW: Sample-Specific Clean-Label Backdoor Watermarking for Dataset Ownership Verification](https://arxiv.org/abs/2510.26420)
*Yingjia Wang,Ting Qiao,Xing Liu,Chongzuo Li,Sixing Wu,Jianbin Li*

Main category: cs.CR

TL;DR: 提出了一种样本特定的干净标签后门水印方法(SSCL-BW)，通过训练U-Net水印生成器为每个样本生成独特水印，解决了静态水印模式易被检测和移除的问题。


<details>
  <summary>Details</summary>
Motivation: 解决现有后门数据集所有权验证方法的局限性：毒标签水印因标签不一致易被检测，干净标签水印技术复杂且在高分辨率图像上失败，且都使用静态水印模式易被检测移除。

Method: 训练U-Net水印生成器为每个样本生成独特水印，设计包含目标样本损失、非目标样本损失和感知相似性损失的复合损失函数，使用黑盒测试验证模型是否表现出预定义后门行为。

Result: 在基准数据集上的广泛实验证明了该方法的有效性，以及对潜在水印移除攻击的鲁棒性。

Conclusion: 提出的SSCL-BW方法能够有效保护数据集知识产权，克服了现有方法的局限性，具有实际应用价值。

Abstract: The rapid advancement of deep neural networks (DNNs) heavily relies on
large-scale, high-quality datasets. However, unauthorized commercial use of
these datasets severely violates the intellectual property rights of dataset
owners. Existing backdoor-based dataset ownership verification methods suffer
from inherent limitations: poison-label watermarks are easily detectable due to
label inconsistencies, while clean-label watermarks face high technical
complexity and failure on high-resolution images. Moreover, both approaches
employ static watermark patterns that are vulnerable to detection and removal.
To address these issues, this paper proposes a sample-specific clean-label
backdoor watermarking (i.e., SSCL-BW). By training a U-Net-based watermarked
sample generator, this method generates unique watermarks for each sample,
fundamentally overcoming the vulnerability of static watermark patterns. The
core innovation lies in designing a composite loss function with three
components: target sample loss ensures watermark effectiveness, non-target
sample loss guarantees trigger reliability, and perceptual similarity loss
maintains visual imperceptibility. During ownership verification, black-box
testing is employed to check whether suspicious models exhibit predefined
backdoor behaviors. Extensive experiments on benchmark datasets demonstrate the
effectiveness of the proposed method and its robustness against potential
watermark removal attacks.

</details>


### [42] [CyberNER: A Harmonized STIX Corpus for Cybersecurity Named Entity Recognition](https://arxiv.org/abs/2510.26499)
*Yasir Ech-Chammakhy,Anas Motii,Anass Rabii,Oussama Azrara,Jaafar Chbili*

Main category: cs.CR

TL;DR: CyberNER是一个大规模统一语料库，通过将四个主要网络安全数据集协调到STIX 2.1标准，解决了命名实体识别中标注模式不兼容的问题，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 网络安全领域的命名实体识别面临数据集标注模式不兼容的问题，简单合并这些资源会导致标签空间噪声严重，从而降低模型性能。

Method: 系统性地将四个主要数据集（CyNER、DNRTI、APTNER和Attacker）协调到STIX 2.1标准，解决语义歧义，将50多个不同的源标签整合为21个一致的实体类型。

Result: 在CyberNER上训练的模型相比简单合并基线实现了约30%的相对F1分数提升，取得了显著的性能增益。

Conclusion: CyberNER语料库的发布为网络安全领域提供了一个标准化的基准，能够创建和严格比较更鲁棒、更可泛化的实体提取模型。

Abstract: Extracting structured intelligence via Named Entity Recognition (NER) is
critical for cybersecurity, but the proliferation of datasets with incompatible
annotation schemas hinders the development of comprehensive models. While
combining these resources is desirable, we empirically demonstrate that naively
concatenating them results in a noisy label space that severely degrades model
performance. To overcome this critical limitation, we introduce CyberNER, a
large-scale, unified corpus created by systematically harmonizing four
prominent datasets (CyNER, DNRTI, APTNER, and Attacker) onto the STIX 2.1
standard. Our principled methodology resolves semantic ambiguities and
consolidates over 50 disparate source tags into 21 coherent entity types. Our
experiments show that models trained on CyberNER achieve a substantial
performance gain, with a relative F1-score improvement of approximately 30%
over the naive concatenation baseline. By publicly releasing the CyberNER
corpus, we provide a crucial, standardized benchmark that enables the creation
and rigorous comparison of more robust and generalizable entity extraction
models for the cybersecurity domain.

</details>


### [43] [Interdependent Privacy in Smart Homes: Hunting for Bystanders in Privacy Policies](https://arxiv.org/abs/2510.26523)
*Shuaishuai Liu,Gergely Acs,Gergely Biczók*

Main category: cs.CR

TL;DR: 本文分析了20款视频门铃和智能摄像头的隐私政策，重点关注旁观者隐私问题，发现厂商主要通过免责声明将收集非用户数据的责任转嫁给设备所有者，并提出改进建议。


<details>
  <summary>Details</summary>
Motivation: 智能家居设备在提供便利和安全的同时，对邻居、访客等旁观者的隐私产生了新的威胁，这种相互依赖的隐私问题在现有法规下缺乏有效监管。

Method: 对20款视频门铃和智能摄像头产品进行隐私政策分析，重点关注旁观者隐私方面，并结合实际案例进行验证。

Result: 研究显示，虽然部分厂商承认旁观者隐私问题，但仅通过免责声明将责任转移给设备所有者，未能提供实质性保护。实际案例表明当前部署对非用户产生了显著影响。

Conclusion: 需要从政策语言和系统设计两方面改进，提高透明度并赋予旁观者和设备所有者更多权利，以更好地保护相互依赖的隐私。

Abstract: Smart home devices such as video doorbells and security cameras are becoming
increasingly common in everyday life. While these devices offer convenience and
safety, they also raise new privacy concerns: how these devices affect others,
like neighbors, visitors, or people passing by. This issue is generally known
as interdependent privacy, where one person's actions (or inaction) may impact
the privacy of others, and, specifically, bystander privacy in the context of
smart homes. Given lax data protection regulations in terms of shared physical
spaces and amateur joint data controllers, we expect that the privacy policies
of smart home products reflect the missing regulatory incentives. This paper
presents a focused privacy policy analysis of 20 video doorbell and smart
camera products, concentrating explicitly on the bystander aspect. We show that
although some of the vendors acknowledge bystanders, they address it only to
the extent of including disclaimers, shifting the ethical responsibility for
collecting the data of non-users to the device owner. In addition, we identify
and examine real-world cases related to bystander privacy, demonstrating how
current deployments can impact non-users. Based on our findings, we analyze
vendor privacy policies in light of existing legal frameworks and technical
capabilities, and we provide practical recommendations for both policy language
and system design to enhance transparency and empower both bystanders and
device owners.

</details>


### [44] [A Comprehensive Evaluation and Practice of System Penetration Testing](https://arxiv.org/abs/2510.26555)
*Chunyi Zhang,Jin Zeng,Xiaoqi Li*

Main category: cs.CR

TL;DR: 本文研究了系统安全渗透测试的方法与实践，探讨如何通过系统化的渗透测试流程和技术手段提升系统安全性，分析现有渗透工具的特点和适用领域，并通过实际案例总结攻击经验。


<details>
  <summary>Details</summary>
Motivation: 随着信息技术快速发展，应用复杂性不断增加，网络安全挑战日益严峻，需要研究有效的系统安全渗透测试方法来应对这些挑战。

Method: 采用系统化的渗透测试流程，分析现有渗透工具的优势、劣势和适用领域，选择合适工具在目标范围和目标机器上复现攻击过程，并进行实际案例分析。

Result: 通过实践验证了所提出的渗透测试方法的有效性，成功复现了攻击过程，并总结了攻击经验教训。

Conclusion: 系统化的渗透测试流程和合适的工具选择对于提升系统安全性具有重要意义，实际案例分析为未来研究提供了有价值的参考。

Abstract: With the rapid advancement of information technology, the complexity of
applications continues to increase, and the cybersecurity challenges we face
are also escalating. This paper aims to investigate the methods and practices
of system security penetration testing, exploring how to enhance system
security through systematic penetration testing processes and technical
approaches. It also examines existing penetration tools, analyzing their
strengths, weaknesses, and applicable domains to guide penetration testers in
tool selection. Furthermore, based on the penetration testing process outlined
in this paper, appropriate tools are selected to replicate attack processes
using target ranges and target machines. Finally, through practical case
analysis, lessons learned from successful attacks are summarized to inform
future research.

</details>


### [45] [A DRL-Empowered Multi-Level Jamming Approach for Secure Semantic Communication](https://arxiv.org/abs/2510.26610)
*Weixuan Chen,Qianqian Yang*

Main category: cs.CR

TL;DR: 提出一种基于深度强化学习的多级干扰方法，结合语义层和物理层干扰来保护语义通信系统安全，通过DDPG算法动态优化预编码矩阵，在保证安全性的同时提升合法用户的图像重建质量。


<details>
  <summary>Details</summary>
Motivation: 语义通信仅传输任务相关信息提高了通信效率，但同时也将语义信息暴露给潜在窃听者，需要增强语义通信系统的安全性。

Method: 使用深度强化学习的多级干扰方法：语义层干扰（编码任务无关文本）+物理层干扰（编码高斯噪声），结合DDPG算法动态设计预编码矩阵，采用交替优化策略联合训练语义通信模型和DDPG代理。

Result: 相比加密基准(ESCS)和编码干扰基准(EJ)，该方法在保持可比安全性的同时，将合法用户的峰值信噪比(PSNR)提升约0.6dB。

Conclusion: 提出的多级干扰方法能有效保护语义通信安全，在保证安全性的同时显著改善合法用户的通信质量。

Abstract: Semantic communication (SemCom) aims to transmit only task-relevant
information, thereby improving communication efficiency but also exposing
semantic information to potential eavesdropping. In this paper, we propose a
deep reinforcement learning (DRL)-empowered multi-level jamming approach to
enhance the security of SemCom systems over MIMO fading wiretap channels. This
approach combines semantic layer jamming, achieved by encoding task-irrelevant
text, and physical layer jamming, achieved by encoding random Gaussian noise.
These two-level jamming signals are superposed with task-relevant semantic
information to protect the transmitted semantics from eavesdropping. A deep
deterministic policy gradient (DDPG) algorithm is further introduced to
dynamically design and optimize the precoding matrices for both taskrelevant
semantic information and multi-level jamming signals, aiming to enhance the
legitimate user's image reconstruction while degrading the eavesdropper's
performance. To jointly train the SemCom model and the DDPG agent, we propose
an alternating optimization strategy where the two modules are updated
iteratively. Experimental results demonstrate that, compared with both the
encryption-based (ESCS) and encoded jammer-based (EJ) benchmarks, our method
achieves comparable security while improving the legitimate user's peak
signalto-noise ratio (PSNR) by up to approximately 0.6 dB.

</details>


### [46] [Toward Automated Security Risk Detection in Large Software Using Call Graph Analysis](https://arxiv.org/abs/2510.26620)
*Nicholas Pecka,Lotfi Ben Othmane,Renee Bryce*

Main category: cs.CR

TL;DR: 本文提出了一种通过聚类调用图来自动化软件威胁建模的方法，使用基于密度和社区检测算法识别代码集群并分析相关安全威胁。


<details>
  <summary>Details</summary>
Motivation: 手动威胁建模方法通常劳动密集且容易出错，需要自动化解决方案来提高效率和准确性。

Method: 使用密度基和社区检测算法对调用图进行聚类，然后分析识别出的集群相关的安全威胁。

Result: 通过对Splunk Forwarder Operator的案例研究验证了该方法的可行性，能够有效评估代码密度相关的安全弱点。

Conclusion: 该方法有助于推进针对现代云原生环境的可扩展、半自动化威胁建模框架的发展。

Abstract: Threat modeling plays a critical role in the identification and mitigation of
security risks; however, manual approaches are often labor intensive and prone
to error. This paper investigates the automation of software threat modeling
through the clustering of call graphs using density-based and community
detection algorithms, followed by an analysis of the threats associated with
the identified clusters. The proposed method was evaluated through a case study
of the Splunk Forwarder Operator (SFO), wherein selected clustering metrics
were applied to the software's call graph to assess pertinent code-density
security weaknesses. The results demonstrate the viability of the approach and
underscore its potential to facilitate systematic threat assessment. This work
contributes to the advancement of scalable, semi-automated threat modeling
frameworks tailored for modern cloud-native environments.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [47] [Towards Piece-by-Piece Explanations for Chess Positions with SHAP](https://arxiv.org/abs/2510.25775)
*Francesco Spinnato*

Main category: cs.AI

TL;DR: 将SHAP可解释AI技术应用于国际象棋分析，通过系统性地移除棋子来计算每个棋子对引擎评估的贡献度，提供可解释的棋局分析。


<details>
  <summary>Details</summary>
Motivation: 传统国际象棋引擎提供精确但不透明的评估分数（百分兵值），这些输出掩盖了单个棋子或模式的具体贡献，需要一种可解释的分析方法。

Method: 将棋子视为特征，通过系统性地移除棋子并计算SHAP值，为每个棋子分配对引擎评估的加法性贡献度，实现局部忠实且人类可理解的解释。

Result: 开发了一种能够将国际象棋引擎评估归因于特定棋子的方法，该方法受到经典象棋教学（通过心理移除棋子评估局面）的启发，并基于现代可解释AI技术。

Conclusion: 该方法为可视化、人类训练和引擎比较开辟了新可能性，并发布了相关代码和数据以促进可解释象棋AI的进一步研究。

Abstract: Contemporary chess engines offer precise yet opaque evaluations, typically
expressed as centipawn scores. While effective for decision-making, these
outputs obscure the underlying contributions of individual pieces or patterns.
In this paper, we explore adapting SHAP (SHapley Additive exPlanations) to the
domain of chess analysis, aiming to attribute a chess engines evaluation to
specific pieces on the board. By treating pieces as features and systematically
ablating them, we compute additive, per-piece contributions that explain the
engines output in a locally faithful and human-interpretable manner. This
method draws inspiration from classical chess pedagogy, where players assess
positions by mentally removing pieces, and grounds it in modern explainable AI
techniques. Our approach opens new possibilities for visualization, human
training, and engine comparison. We release accompanying code and data to
foster future research in interpretable chess AI.

</details>


### [48] [An Agentic Framework for Rapid Deployment of Edge AI Solutions in Industry 5.0](https://arxiv.org/abs/2510.25813)
*Jorge Martinez-Gil,Mario Pichler,Nefeli Bountouni,Sotiris Koussouris,Marielena Márquez Barreiro,Sergio Gusmeroli*

Main category: cs.AI

TL;DR: 提出了一个面向工业5.0的新框架，简化AI模型在边缘设备上的部署，通过本地推理和实时处理降低延迟，避免外部数据传输。


<details>
  <summary>Details</summary>
Motivation: 工业环境中需要高效部署AI模型，传统方法存在延迟高、数据传输复杂等问题，需要简化部署流程并提升系统适应性。

Method: 采用基于代理的架构，各类代理（人类、算法或协作）负责特定任务，支持模块化集成，保持低资源需求。

Result: 在食品工业真实场景的初步评估显示，部署时间和系统适应性性能得到改善。

Conclusion: 该框架为工业5.0提供了有效的AI模型部署解决方案，具有低延迟、模块化和适应性强的特点。

Abstract: We present a novel framework for Industry 5.0 that simplifies the deployment
of AI models on edge devices in various industrial settings. The design reduces
latency and avoids external data transfer by enabling local inference and
real-time processing. Our implementation is agent-based, which means that
individual agents, whether human, algorithmic, or collaborative, are
responsible for well-defined tasks, enabling flexibility and simplifying
integration. Moreover, our framework supports modular integration and maintains
low resource requirements. Preliminary evaluations concerning the food industry
in real scenarios indicate improved deployment time and system adaptability
performance. The source code is publicly available at
https://github.com/AI-REDGIO-5-0/ci-component.

</details>


### [49] [Symbolically Scaffolded Play: Designing Role-Sensitive Prompts for Generative NPC Dialogue](https://arxiv.org/abs/2510.25820)
*Vanessa Figueiredo,David Elumeze*

Main category: cs.AI

TL;DR: 该研究探讨了在基于GPT-4o的侦探游戏中，不同约束程度的提示对玩家体验的影响，发现约束程度对体验无显著差异，并提出了符号化支架游戏框架。


<details>
  <summary>Details</summary>
Motivation: 研究动机是验证约束提示是否能真正改善玩家体验，因为虽然LLMs能让NPC进行非脚本对话，但约束提示的实际效果尚不明确。

Method: 采用被试内可用性研究(N=10)比较高约束提示(HCP)和低约束提示(LCP)，然后重新设计HCP为混合JSON+RAG支架，并进行LLM法官的合成评估。

Result: 研究发现约束程度对玩家体验无可靠差异，支架效果具有角色依赖性：面试官NPC获得稳定性，而嫌疑犯NPC失去即兴可信度。

Conclusion: 研究推翻了更严格约束必然增强游戏性的假设，提出了符号化支架游戏框架，使用模糊数值边界在需要时保持连贯性，同时在需要惊喜时保留即兴性。

Abstract: Large Language Models (LLMs) promise to transform interactive games by
enabling non-player characters (NPCs) to sustain unscripted dialogue. Yet it
remains unclear whether constrained prompts actually improve player experience.
We investigate this question through The Interview, a voice-based detective
game powered by GPT-4o. A within-subjects usability study ($N=10$) compared
high-constraint (HCP) and low-constraint (LCP) prompts, revealing no reliable
experiential differences beyond sensitivity to technical breakdowns. Guided by
these findings, we redesigned the HCP into a hybrid JSON+RAG scaffold and
conducted a synthetic evaluation with an LLM judge, positioned as an
early-stage complement to usability testing. Results uncovered a novel pattern:
scaffolding effects were role-dependent: the Interviewer (quest-giver NPC)
gained stability, while suspect NPCs lost improvisational believability. These
findings overturn the assumption that tighter constraints inherently enhance
play. Extending fuzzy-symbolic scaffolding, we introduce \textit{Symbolically
Scaffolded Play}, a framework in which symbolic structures are expressed as
fuzzy, numerical boundaries that stabilize coherence where needed while
preserving improvisation where surprise sustains engagement.

</details>


### [50] [Through the Judge's Eyes: Inferred Thinking Traces Improve Reliability of LLM Raters](https://arxiv.org/abs/2510.25860)
*Xingjian Zhang,Tianhong Gao,Suliang Jin,Tianhao Wang,Teng Ye,Eytan Adar,Qiaozhu Mei*

Main category: cs.AI

TL;DR: 提出人机协作框架，从仅标签标注中推断思维轨迹，通过拒绝采样方法重构推理过程，用于微调开源LLM评估器和改进专有LLM评估器的标注指南，显著提升LLM与人类评估的一致性。


<details>
  <summary>Details</summary>
Motivation: LLM作为评估器的可靠性在主观任务中受限，因为人类判断涉及超出标注标签的微妙推理。思维轨迹（判断背后的推理）信息丰富但难以收集和整理。

Method: 使用人机协作框架和简单有效的拒绝采样方法，从仅标签标注中大规模重构思维轨迹，并将其应用于微调开源LLM评估器和合成更清晰的专有LLM评估器标注指南。

Result: 在多个数据集上，该方法显著提高了LLM与人类评估的一致性，改进后的标注指南还增加了不同LLM模型之间的一致性。

Conclusion: LLM可以作为人类思维轨迹的实用代理，使仅标签语料库能够扩展为思维轨迹增强资源，从而提高LLM评估器的可靠性。

Abstract: Large language models (LLMs) are increasingly used as raters for evaluation
tasks. However, their reliability is often limited for subjective tasks, when
human judgments involve subtle reasoning beyond annotation labels. Thinking
traces, the reasoning behind a judgment, are highly informative but challenging
to collect and curate. We present a human-LLM collaborative framework to infer
thinking traces from label-only annotations. The proposed framework uses a
simple and effective rejection sampling method to reconstruct these traces at
scale. These inferred thinking traces are applied to two complementary tasks:
(1) fine-tuning open LLM raters; and (2) synthesizing clearer annotation
guidelines for proprietary LLM raters. Across multiple datasets, our methods
lead to significantly improved LLM-human agreement. Additionally, the refined
annotation guidelines increase agreement among different LLM models. These
results suggest that LLMs can serve as practical proxies for otherwise
unrevealed human thinking traces, enabling label-only corpora to be extended
into thinking-trace-augmented resources that enhance the reliability of LLM
raters.

</details>


### [51] [The Information-Theoretic Imperative: Compression and the Epistemic Foundations of Intelligence](https://arxiv.org/abs/2510.25883)
*Christian Dittrich,Jennifer Flygare Kinne*

Main category: cs.AI

TL;DR: 该论文提出了一个两层次框架来解释为什么压缩过程会强制发现因果结构而非表面统计模式。信息论必要性(ITI)建立了系统必须通过预测压缩最小化认知熵，压缩效率原则(CEP)则说明高效压缩如何通过异常积累动态选择生成性因果模型。


<details>
  <summary>Details</summary>
Motivation: 现有框架虽然认识到压缩对智能的核心作用，但未能具体说明为什么这个过程会强制发现因果结构而非表面统计模式。

Method: 引入两层次框架：ITI从进化角度解释生存压力如何导致信息处理需求，CEP则从机制上说明高效压缩如何通过异常积累动态选择因果模型。

Result: 该框架产生了可实证检验的预测：压缩效率与分布外泛化相关，异常积累率可区分因果模型和相关模型，分层系统在不同抽象层上表现出递增效率，生物系统的代谢成本与表征复杂性相关。

Conclusion: ITI和CEP为生物、人工和多尺度系统的趋同提供了统一解释，无需诉诸意识或主观经验的假设，说明智能是在结构化环境中持续存在的机械必然结果。

Abstract: Existing frameworks converge on the centrality of compression to intelligence
but leave underspecified why this process enforces the discovery of causal
structure rather than superficial statistical patterns. We introduce a
two-level framework to address this gap. The Information-Theoretic Imperative
(ITI) establishes that any system persisting in uncertain environments must
minimize epistemic entropy through predictive compression: this is the
evolutionary "why" linking survival pressure to information-processing demands.
The Compression Efficiency Principle (CEP) specifies how efficient compression
mechanically selects for generative, causal models through
exception-accumulation dynamics, making reality alignment a consequence rather
than a contingent achievement. Together, ITI and CEP define a causal chain:
from survival pressure to prediction necessity, compression requirement,
efficiency optimization, generative structure discovery, and ultimately reality
alignment. Each link follows from physical, information-theoretic, or
evolutionary constraints, implying that intelligence is the mechanically
necessary outcome of persistence in structured environments. This framework
yields empirically testable predictions: compression efficiency, measured as
approach to the rate-distortion frontier, correlates with out-of-distribution
generalization; exception-accumulation rates differentiate causal from
correlational models; hierarchical systems exhibit increasing efficiency across
abstraction layers; and biological systems demonstrate metabolic costs that
track representational complexity. ITI and CEP thereby provide a unified
account of convergence across biological, artificial, and multi-scale systems,
addressing the epistemic and functional dimensions of intelligence without
invoking assumptions about consciousness or subjective experience.

</details>


### [52] [Approximating Human Preferences Using a Multi-Judge Learned System](https://arxiv.org/abs/2510.25884)
*Eitán Sprejer,Fernando Avalos,Augusto Bernardi,Jose Pedro Brito de Azevedo Faustino,Jacob Haimes,Narmeen Fatimah Oozeer*

Main category: cs.AI

TL;DR: 提出了一个基于角色的偏好建模框架，通过聚合多个基于评分标准的评判者输出来对齐LLM评判者与人类偏好，解决校准困难、评分标准敏感性、偏见和不稳定性问题。


<details>
  <summary>Details</summary>
Motivation: 基于LLM的评判者难以校准，存在评分标准敏感性、偏见和不稳定性问题。解决这些问题对于构建可靠的RLHF奖励模型和有效的模型路由系统至关重要。

Method: 提出了基于角色的偏好建模框架，学习聚合多个基于评分标准的评判者输出。包括基于角色的偏好标签大规模合成方法，以及两种聚合器实现：广义加性模型(GAM)和多层感知器(MLP)。

Result: 评估了该方法相对于简单基线的性能，并通过案例研究评估了其对人类和LLM评判者偏见的鲁棒性。

Conclusion: 该框架能够有效建模多样化的基于角色的偏好，为LLM评判者与人类偏好的对齐提供了可行解决方案。

Abstract: Aligning LLM-based judges with human preferences is a significant challenge,
as they are difficult to calibrate and often suffer from rubric sensitivity,
bias, and instability. Overcoming this challenge advances key applications,
such as creating reliable reward models for Reinforcement Learning from Human
Feedback (RLHF) and building effective routing systems that select the
best-suited model for a given user query. In this work, we propose a framework
for modeling diverse, persona-based preferences by learning to aggregate
outputs from multiple rubric-conditioned judges. We investigate the performance
of this approach against naive baselines and assess its robustness through case
studies on both human and LLM-judges biases. Our primary contributions include
a persona-based method for synthesizing preference labels at scale and two
distinct implementations of our aggregator: Generalized Additive Model (GAM)
and a Multi-Layer Perceptron (MLP).

</details>


### [53] [SciTrust 2.0: A Comprehensive Framework for Evaluating Trustworthiness of Large Language Models in Scientific Applications](https://arxiv.org/abs/2510.25908)
*Emily Herron,Junqi Yin,Feiyi Wang*

Main category: cs.AI

TL;DR: SciTrust 2.0是一个评估LLM在科学应用中可信度的框架，涵盖真实性、对抗鲁棒性、科学安全和科学伦理四个维度。评估显示通用行业模型优于科学专用模型，科学专用模型在逻辑和伦理推理方面存在显著缺陷。


<details>
  <summary>Details</summary>
Motivation: LLM在科学研究中展现出变革潜力，但在高风险环境中的部署引发了可信度担忧，需要系统评估框架。

Method: 开发了包含新颖开放式真实性基准和伦理基准的评估框架，通过验证的反思调优流程和专家验证构建，使用准确性、语义相似度和LLM评分等多种指标评估七个主要LLM。

Result: 通用行业模型在所有可信度维度上均优于科学专用模型，GPT-4-mini在真实性和对抗鲁棒性方面表现最佳，科学专用模型在逻辑伦理推理和安全评估中表现出严重缺陷。

Conclusion: 通过开源该框架，为开发更可信的AI系统和推进科学背景下模型安全与伦理研究提供了基础。

Abstract: Large language models (LLMs) have demonstrated transformative potential in
scientific research, yet their deployment in high-stakes contexts raises
significant trustworthiness concerns. Here, we introduce SciTrust 2.0, a
comprehensive framework for evaluating LLM trustworthiness in scientific
applications across four dimensions: truthfulness, adversarial robustness,
scientific safety, and scientific ethics. Our framework incorporates novel,
open-ended truthfulness benchmarks developed through a verified
reflection-tuning pipeline and expert validation, alongside a novel ethics
benchmark for scientific research contexts covering eight subcategories
including dual-use research and bias. We evaluated seven prominent LLMs,
including four science-specialized models and three general-purpose industry
models, using multiple evaluation metrics including accuracy, semantic
similarity measures, and LLM-based scoring. General-purpose industry models
overall outperformed science-specialized models across each trustworthiness
dimension, with GPT-o4-mini demonstrating superior performance in truthfulness
assessments and adversarial robustness. Science-specialized models showed
significant deficiencies in logical and ethical reasoning capabilities, along
with concerning vulnerabilities in safety evaluations, particularly in
high-risk domains such as biosecurity and chemical weapons. By open-sourcing
our framework, we provide a foundation for developing more trustworthy AI
systems and advancing research on model safety and ethics in scientific
contexts.

</details>


### [54] [FinOps Agent -- A Use-Case for IT Infrastructure and Cost Optimization](https://arxiv.org/abs/2510.25914)
*Ngoc Phuoc An Vo,Manish Kesarwani,Ruchi Mahindru,Chandrasekhar Narayanaswami*

Main category: cs.AI

TL;DR: 提出利用自主AI代理实现FinOps自动化，通过模拟端到端行业流程，从多源数据获取到分析生成优化建议，评估显示代理能像实际FinOps从业者一样理解和执行任务。


<details>
  <summary>Details</summary>
Motivation: FinOps从业者面临来自多个云提供商和内部系统的异构计费数据格式、分类和指标，导致难以综合可操作的见解并做出及时决策。

Method: 构建FinOps代理系统，模拟从多源数据检索到数据整合分析再到生成优化建议的端到端行业流程，使用多个开源和闭源语言模型进行评估。

Result: 代理能够像实际FinOps从业者一样理解、规划和执行任务，在IT基础设施和成本优化用例中表现出色。

Conclusion: 自主目标驱动AI代理可以有效解决FinOps中的异构数据挑战，实现自动化成本优化决策。

Abstract: FinOps (Finance + Operations) represents an operational framework and
cultural practice which maximizes cloud business value through collaborative
financial accountability across engineering, finance, and business teams.
FinOps practitioners face a fundamental challenge: billing data arrives in
heterogeneous formats, taxonomies, and metrics from multiple cloud providers
and internal systems which eventually lead to synthesizing actionable insights,
and making time-sensitive decisions. To address this challenge, we propose
leveraging autonomous, goal-driven AI agents for FinOps automation. In this
paper, we built a FinOps agent for a typical use-case for IT infrastructure and
cost optimization. We built a system simulating a realistic end-to-end industry
process starting with retrieving data from various sources to consolidating and
analyzing the data to generate recommendations for optimization. We defined a
set of metrics to evaluate our agent using several open-source and close-source
language models and it shows that the agent was able to understand, plan, and
execute tasks as well as an actual FinOps practitioner.

</details>


### [55] [Humains-Junior: A 3.8B Language Model Achieving GPT-4o-Level Factual Accuracy by Directed Exoskeleton Reasoning](https://arxiv.org/abs/2510.25933)
*Nissan Yaron,Dan Bystritsky,Ben-Etzion Yaron*

Main category: cs.AI

TL;DR: 一个38亿参数的模型在FACTS基准测试中达到GPT-4o水平（在±5%等价范围内），云服务成本降低约19倍，自托管部署边际成本接近零


<details>
  <summary>Details</summary>
Motivation: 开发成本效益高的小型语言模型，在保持性能的同时大幅降低推理成本

Method: 结合最小化定向"外骨骼推理"支架和行为微调，教导协议遵守而非领域知识，两者协同作用显著提升性能

Result: 在Q1-Q500测试中，Humans-Junior得分72.7% vs GPT-4o的73.5%，差异不显著；在提示设置下，前沿模型性能提升明显

Conclusion: 小型模型通过精心设计的推理支架和微调方法，可以在特定任务上达到前沿模型性能，同时大幅降低成本

Abstract: We introduce Humans-Junior, a 3.8B model that matches GPT-4o on the FACTS
Grounding public subset within a $\pm 5$ pp equivalence margin.
  Results. On Q1--Q500 under identical judges, GPT-4o scores 73.5% (95% CI
69.5--77.2) and Humans-Junior 72.7% (95% CI 68.7--76.5); the paired difference
is 0.8 pp (bootstrap 95% CI $-3.1$ to $+4.7$; permutation $p = 0.72$; Cohen's
$d = 0.023$). TOST establishes equivalence at $\pm 5$ pp (not at $\pm 3$ pp).
When purchased as managed APIs, Humans-Junior's base model
(Phi-3.5-mini-instruct) is $\approx 19\times$ less expensive than GPT-4o on
Microsoft AI Foundry pricing; self-hosted or edge deployments can drive
incremental inference cost toward zero. Measured vs estimated pricing sources
are tabulated in Appendix E.
  Method. Our approach combines minimal directed "Exoskeleton Reasoning"
scaffolds with behavioral fine-tuning that teaches protocol compliance
(epistemic discipline) rather than domain answers. Fine-tuning alone adds
little; combined, they synergize (+17.7 pp, $p < 0.001$) and reduce variance
($\approx 25\%$). In prompt-only settings on frontier models (Q1--Q100;
non-comparable), directed reasoning improved GPT-4o by +11.8 pp to 85.3% and
Gemini-2.5-Pro by +5.0 pp to 93.3% (baseline 88.3%, $n = 100$); see Section~5.
  TL;DR. A 3.8B model achieves GPT-4o-level FACTS accuracy (equivalent within
$\pm 5$ pp on Q1--Q500). Cloud pricing shows $\approx 19\times$ lower cost
versus GPT-4o, and self-hosted/edge deployments can approach zero marginal
cost. Pricing sources are listed in Appendix E. Frontier prompt-only gains
(Q1--Q100; non-comparable) and optimized-prompt exploratory results under
earlier judges are summarized in Appendix F.
  Keywords: Small Language Models, Factual Grounding, Directed Reasoning,
Fine-Tuning, Model Alignment, Cost-Efficient AI

</details>


### [56] [Estimating cognitive biases with attention-aware inverse planning](https://arxiv.org/abs/2510.25951)
*Sounak Banerjee,Daphne Cornelisse,Deepak Gopinath,Emily Sumner,Jonathan DeCastro,Guy Rosman,Eugene Vinitsky,Mark K. Ho*

Main category: cs.AI

TL;DR: 本文提出了注意力感知逆规划问题，旨在从人类行为中推断认知偏见，特别是注意力偏见。通过结合深度强化学习和计算认知建模，在真实驾驶场景中展示了该方法可扩展性。


<details>
  <summary>Details</summary>
Motivation: 人类的目标导向行为受到认知偏见影响，与人类交互的自主系统需要意识到这一点。特别是在日常任务如驾驶中，人们对环境物体的注意力偏见会系统性地影响行为表现。

Method: 结合深度强化学习与计算认知建模，提出注意力感知逆规划方法，从行为中推断注意力策略。在Waymo开放数据集中的真实驾驶场景进行验证。

Result: 展示了注意力感知逆规划与标准逆强化学习的系统性差异，证明了从行为中推断认知偏见的可行性。在真实驾驶场景中成功推断RL智能体的注意力策略。

Conclusion: 注意力感知逆规划方法能够有效估计认知偏见，特别是在复杂现实场景中具有可扩展性，为理解人类行为中的认知偏见提供了新途径。

Abstract: People's goal-directed behaviors are influenced by their cognitive biases,
and autonomous systems that interact with people should be aware of this. For
example, people's attention to objects in their environment will be biased in a
way that systematically affects how they perform everyday tasks such as driving
to work. Here, building on recent work in computational cognitive science, we
formally articulate the attention-aware inverse planning problem, in which the
goal is to estimate a person's attentional biases from their actions. We
demonstrate how attention-aware inverse planning systematically differs from
standard inverse reinforcement learning and how cognitive biases can be
inferred from behavior. Finally, we present an approach to attention-aware
inverse planning that combines deep reinforcement learning with computational
cognitive modeling. We use this approach to infer the attentional strategies of
RL agents in real-life driving scenarios selected from the Waymo Open Dataset,
demonstrating the scalability of estimating cognitive biases with
attention-aware inverse planning.

</details>


### [57] [From Queries to Insights: Agentic LLM Pipelines for Spatio-Temporal Text-to-SQL](https://arxiv.org/abs/2510.25997)
*Manu Redd,Tao Zhe,Dongjie Wang*

Main category: cs.AI

TL;DR: 提出了一个基于代理的NL-to-SQL系统，通过ReAct代理协调多个工具来处理复杂的时空查询，相比单纯的文本到SQL模型，准确率从28.6%提升到91.4%。


<details>
  <summary>Details</summary>
Motivation: 现有的NL-to-SQL系统在处理现实时空查询时表现不佳，需要解决用户模糊表达与数据库模式对齐、时间推理和输出选择等问题，以支持非SQL专家的用户访问结构化数据。

Method: 扩展了基础的文本到SQL模型(llama-3-sqlcoder-8b)，使用基于Mistral的ReAct代理进行协调，代理能够通过模式检查、SQL生成、执行和可视化工具来规划、分解和调整查询。

Result: 在NYC和Tokyo签到数据集的35个自然语言查询上评估，代理系统准确率达到91.4%，显著优于28.6%的基线系统，并通过地图、图表和结构化自然语言摘要提升了可用性。

Conclusion: 代理协调而非仅依赖更强的SQL生成器，是构建交互式地理空间助手的有前景的基础，能够支持缺乏SQL专业知识、详细模式知识或提示技能的用户进行更自然的人机数据库交互。

Abstract: Natural-language-to-SQL (NL-to-SQL) systems hold promise for democratizing
access to structured data, allowing users to query databases without learning
SQL. Yet existing systems struggle with realistic spatio-temporal queries,
where success requires aligning vague user phrasing with schema-specific
categories, handling temporal reasoning, and choosing appropriate outputs. We
present an agentic pipeline that extends a naive text-to-SQL baseline
(llama-3-sqlcoder-8b) with orchestration by a Mistral-based ReAct agent. The
agent can plan, decompose, and adapt queries through schema inspection, SQL
generation, execution, and visualization tools. We evaluate on 35
natural-language queries over the NYC and Tokyo check-in dataset, covering
spatial, temporal, and multi-dataset reasoning. The agent achieves
substantially higher accuracy than the naive baseline 91.4% vs. 28.6% and
enhances usability through maps, plots, and structured natural-language
summaries. Crucially, our design enables more natural human-database
interaction, supporting users who lack SQL expertise, detailed schema
knowledge, or prompting skill. We conclude that agentic orchestration, rather
than stronger SQL generators alone, is a promising foundation for interactive
geospatial assistants.

</details>


### [58] [AutoSurvey2: Empowering Researchers with Next Level Automated Literature Surveys](https://arxiv.org/abs/2510.26012)
*Siyi Wu,Chiaxin Liang,Ziqian Bi,Leyi Zhao,Tianyang Wang,Junhao Song,Yichao Zhang,Keyu Chen,Xinyuan Song*

Main category: cs.AI

TL;DR: autosurvey2是一个自动化生成学术综述的多阶段流水线系统，通过检索增强合成和结构化评估，结合并行章节生成、迭代优化和实时文献检索，确保主题完整性和事实准确性。


<details>
  <summary>Details</summary>
Motivation: 随着研究文献的快速增长，特别是在大语言模型领域，撰写全面且最新的综述论文变得越来越困难。

Method: 采用多阶段流水线，包括检索增强合成、并行章节生成、迭代优化和实时文献检索，并使用多LLM评估框架来评估质量。

Result: 实验结果表明autosurvey2持续优于现有的检索基线和自动化基线，在结构连贯性和主题相关性方面获得更高分数，同时保持强引用保真度。

Conclusion: 通过将检索、推理和自动评估整合到统一框架中，autosurvey2为生成长篇学术综述提供了可扩展且可复现的解决方案，并为自动化学术写作的未来研究奠定了坚实基础。

Abstract: The rapid growth of research literature, particularly in large language
models (LLMs), has made producing comprehensive and current survey papers
increasingly difficult. This paper introduces autosurvey2, a multi-stage
pipeline that automates survey generation through retrieval-augmented synthesis
and structured evaluation. The system integrates parallel section generation,
iterative refinement, and real-time retrieval of recent publications to ensure
both topical completeness and factual accuracy. Quality is assessed using a
multi-LLM evaluation framework that measures coverage, structure, and relevance
in alignment with expert review standards. Experimental results demonstrate
that autosurvey2 consistently outperforms existing retrieval-based and
automated baselines, achieving higher scores in structural coherence and
topical relevance while maintaining strong citation fidelity. By combining
retrieval, reasoning, and automated evaluation into a unified framework,
autosurvey2 provides a scalable and reproducible solution for generating
long-form academic surveys and contributes a solid foundation for future
research on automated scholarly writing. All code and resources are available
at https://github.com/annihi1ation/auto_research.

</details>


### [59] [Large Language Model-assisted Autonomous Vehicle Recovery from Immobilization](https://arxiv.org/abs/2510.26023)
*Zhipeng Bao,Qianwen Li*

Main category: cs.AI

TL;DR: StuckSolver是一个基于大语言模型的自动驾驶车辆恢复框架，通过自主推理和乘客引导解决车辆被困场景，无需修改现有架构即可提升交通流动性。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶车辆在复杂交通场景中容易陷入困境，现有恢复方案（远程干预和手动接管）存在成本高、效率低、可访问性差等问题，需要更智能的解决方案。

Method: 开发了StuckSolver作为插件模块，利用LLM分析传感器数据检测被困状态，理解环境上下文，生成可由车辆原生规划器执行的高级恢复指令。

Result: 在Bench2Drive基准测试和自定义不确定场景中，StuckSolver仅通过自主推理就达到接近最优性能，结合乘客引导后表现进一步提升。

Conclusion: StuckSolver证明了LLM驱动的恢复框架能有效解决自动驾驶车辆被困问题，提高系统鲁棒性和可访问性，无需修改现有架构。

Abstract: Despite significant advancements in recent decades, autonomous vehicles (AVs)
continue to face challenges in navigating certain traffic scenarios where human
drivers excel. In such situations, AVs often become immobilized, disrupting
overall traffic flow. Current recovery solutions, such as remote intervention
(which is costly and inefficient) and manual takeover (which excludes
non-drivers and limits AV accessibility), are inadequate. This paper introduces
StuckSolver, a novel Large Language Model (LLM) driven recovery framework that
enables AVs to resolve immobilization scenarios through self-reasoning and/or
passenger-guided decision-making. StuckSolver is designed as a plug-in add-on
module that operates on top of the AV's existing perception-planning-control
stack, requiring no modification to its internal architecture. Instead, it
interfaces with standard sensor data streams to detect immobilization states,
interpret environmental context, and generate high-level recovery commands that
can be executed by the AV's native planner. We evaluate StuckSolver on the
Bench2Drive benchmark and in custom-designed uncertainty scenarios. Results
show that StuckSolver achieves near-state-of-the-art performance through
autonomous self-reasoning alone and exhibits further improvements when
passenger guidance is incorporated.

</details>


### [60] [Can AI be Accountable?](https://arxiv.org/abs/2510.26057)
*Andrew L. Kun*

Main category: cs.AI

TL;DR: 本文探讨了AI问责性的重要性，分析了当前AI系统缺乏问责性的现状，并提出了改善AI问责性的方法框架。


<details>
  <summary>Details</summary>
Motivation: 随着AI能力的快速提升，确保AI对消费者、选民和决策者负责变得至关重要。当前AI系统普遍缺乏问责机制，无法被质疑、讨论或制裁。

Method: 将一般问责定义应用于AI系统，说明AI问责和非问责的具体表现，探索提高AI问责性的方法途径。

Result: 明确了AI问责性的概念框架，识别了当前AI系统在问责性方面的缺陷，提出了改善问责性的方向。

Conclusion: 需要建立机制确保所有AI系统对受其影响的人保持问责，这是实现AI负责任发展的关键要求。

Abstract: The AI we use is powerful, and its power is increasing rapidly. If this
powerful AI is to serve the needs of consumers, voters, and decision makers,
then it is imperative that the AI is accountable. In general, an agent is
accountable to a forum if the forum can request information from the agent
about its actions, if the forum and the agent can discuss this information, and
if the forum can sanction the agent. Unfortunately, in too many cases today's
AI is not accountable -- we cannot question it, enter into a discussion with
it, let alone sanction it. In this chapter we relate the general definition of
accountability to AI, we illustrate what it means for AI to be accountable and
unaccountable, and we explore approaches that can improve our chances of living
in a world where all AI is accountable to those who are affected by it.

</details>


### [61] [Lean4Physics: Comprehensive Reasoning Framework for College-level Physics in Lean4](https://arxiv.org/abs/2510.26094)
*Yuxin Li,Minghao Liu,Ruida Wang,Wenzhao Ji,Zhitao He,Rui Pan,Junming Huang,Tong Zhang,Yi R. Fung*

Main category: cs.AI

TL;DR: Lean4PHYS是一个基于Lean4的大学物理问题推理框架，包含LeanPhysBench基准测试集和PhysLib物理库，在现有模型上表现较差但PhysLib能显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 为大学物理问题提供正式的推理框架和基准测试，填补Lean4中物理推理基准的空白。

Method: 构建包含200个手工制作和同行评审物理问题的LeanPhysBench基准测试集，以及包含基础单位系统和定理的PhysLib物理库。

Result: 最佳模型DeepSeek-Prover-V2-7B仅达到16%准确率，Claude-Sonnet-4达到35%，PhysLib平均提升模型性能11.75%。

Conclusion: LeanPhysBench具有挑战性，PhysLib能有效提升物理推理性能，这是首个在Lean4中提供的物理基准测试。

Abstract: We present **Lean4PHYS**, a comprehensive reasoning framework for
college-level physics problems in Lean4. **Lean4PHYS** includes
*LeanPhysBench*, a college-level benchmark for formal physics reasoning in
Lean4, which contains 200 hand-crafted and peer-reviewed statements derived
from university textbooks and physics competition problems. To establish a
solid foundation for formal reasoning in physics, we also introduce *PhysLib*,
a community-driven repository containing fundamental unit systems and theorems
essential for formal physics reasoning. Based on the benchmark and Lean4
repository we composed in **Lean4PHYS**, we report baseline results using major
expert Math Lean4 provers and state-of-the-art closed-source models, with the
best performance of DeepSeek-Prover-V2-7B achieving only 16% and
Claude-Sonnet-4 achieving 35%. We also conduct a detailed analysis showing that
our *PhysLib* can achieve an average improvement of 11.75% in model
performance. This demonstrates the challenging nature of our *LeanPhysBench*
and the effectiveness of *PhysLib*. To the best of our knowledge, this is the
first study to provide a physics benchmark in Lean4.

</details>


### [62] [GUI Knowledge Bench: Revealing the Knowledge Gap Behind VLM Failures in GUI Tasks](https://arxiv.org/abs/2510.26098)
*Chenrui Shi,Zedong Yu,Zhi Gao,Ruining Feng,Enqi Liu,Yuwei Wu,Yunde Jia,Liuyu Xiang,Zhaofeng He,Qing Li*

Main category: cs.AI

TL;DR: 该论文分析了大型视觉语言模型在GUI任务自动化中的不足，提出了GUI知识的三个维度框架，并创建了GUI Knowledge Bench基准来评估模型能力。


<details>
  <summary>Details</summary>
Motivation: 现有的大型视觉语言模型在GUI任务自动化方面仍落后于人类，作者认为这是由于缺乏核心GUI知识，而现有的训练方法无法完全解决这个问题。

Method: 通过分析GUI任务执行中的常见失败模式，将GUI知识提炼为三个维度：界面感知、交互预测和指令理解，并创建了跨6个平台、292个应用的GUI Knowledge Bench基准。

Result: 评估显示当前VLMs能够识别控件功能，但在感知系统状态、预测动作和验证任务完成方面存在困难。真实世界GUI任务实验进一步验证了GUI知识与任务成功之间的紧密联系。

Conclusion: 该工作提供了一个结构化框架来评估GUI知识，支持在下游训练前选择更有潜力的VLMs，并为构建更强大的GUI代理提供了见解。

Abstract: Large vision language models (VLMs) have advanced graphical user interface
(GUI) task automation but still lag behind humans. We hypothesize this gap
stems from missing core GUI knowledge, which existing training schemes (such as
supervised fine tuning and reinforcement learning) alone cannot fully address.
By analyzing common failure patterns in GUI task execution, we distill GUI
knowledge into three dimensions: (1) interface perception, knowledge about
recognizing widgets and system states; (2) interaction prediction, knowledge
about reasoning action state transitions; and (3) instruction understanding,
knowledge about planning, verifying, and assessing task completion progress. We
further introduce GUI Knowledge Bench, a benchmark with multiple choice and
yes/no questions across six platforms (Web, Android, MacOS, Windows, Linux,
IOS) and 292 applications. Our evaluation shows that current VLMs identify
widget functions but struggle with perceiving system states, predicting
actions, and verifying task completion. Experiments on real world GUI tasks
further validate the close link between GUI knowledge and task success. By
providing a structured framework for assessing GUI knowledge, our work supports
the selection of VLMs with greater potential prior to downstream training and
provides insights for building more capable GUI agents.

</details>


### [63] [Beyond Benchmarks: The Economics of AI Inference](https://arxiv.org/abs/2510.26136)
*Boqin Zhuang,Jiacheng Qiao,Mingqian Liu,Mingxing Yu,Ping Hong,Rui Li,Xiaoxia Song,Xiangjun Xu,Xu Chen,Yaoyao Ma,Yujie Gao*

Main category: cs.AI

TL;DR: 该论文提出了一个"推理经济学"框架，将LLM推理视为计算驱动的智能生产活动，分析了边际成本、规模经济和输出质量，并基于WiNEval-3.0数据构建了首个"LLM推理生产前沿"。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的推理成本已成为决定其商业可行性和广泛应用的关键因素，需要建立定量分析框架来指导模型部署决策。

Method: 采用定量"推理经济学"框架，将LLM推理过程视为计算驱动的智能生产活动，基于WiNEval-3.0的实证数据进行分析。

Result: 揭示了三个原则：边际成本递减、规模收益递减以及存在最优成本效益区域，并构建了首个LLM推理生产前沿。

Conclusion: 该研究不仅为模型部署决策提供了经济基础，也为未来AI推理资源的市场定价和优化奠定了实证基础。

Abstract: The inference cost of Large Language Models (LLMs) has become a critical
factor in determining their commercial viability and widespread adoption. This
paper introduces a quantitative ``economics of inference'' framework, treating
the LLM inference process as a compute-driven intelligent production activity.
We analyze its marginal cost, economies of scale, and quality of output under
various performance configurations. Based on empirical data from WiNEval-3.0,
we construct the first ``LLM Inference Production Frontier,'' revealing three
principles: diminishing marginal cost, diminishing returns to scale, and an
optimal cost-effectiveness zone. This paper not only provides an economic basis
for model deployment decisions but also lays an empirical foundation for the
future market-based pricing and optimization of AI inference resources.

</details>


### [64] [Reasoning Curriculum: Bootstrapping Broad LLM Reasoning from Math](https://arxiv.org/abs/2510.26143)
*Bo Pang,Deqian Kong,Silvio Savarese,Caiming Xiong,Yingbo Zhou*

Main category: cs.AI

TL;DR: 提出Reasoning Curriculum两阶段课程学习方法，先在数学领域通过强化学习培养推理能力，然后在多领域联合训练中迁移和巩固这些能力，无需专门奖励模型即可提升大语言模型的通用推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习主要关注数学和代码领域，缺乏对其他领域推理能力的系统性培养，需要一种简单有效的方法来提升大语言模型的通用推理能力。

Method: 两阶段课程学习：第一阶段在数学领域进行强化学习，利用可验证的奖励培养推理技能；第二阶段在多领域混合数据上进行联合强化学习，迁移和巩固推理能力。

Result: 在Qwen3-4B和Llama-3.1-8B模型上的多领域评估显示，该方法能带来一致的性能提升，消融实验表明两个阶段都是必要的。

Conclusion: Reasoning Curriculum提供了一种紧凑且易于采用的通用推理能力提升方案，数学优先的推理激发策略能增强解决复杂问题所需的关键认知行为。

Abstract: Reinforcement learning (RL) can elicit strong reasoning in large language
models (LLMs), yet most open efforts focus on math and code. We propose
Reasoning Curriculum, a simple two-stage curriculum that first elicits
reasoning skills in pretraining-aligned domains such as math, then adapts and
refines these skills across other domains via joint RL. Stage 1 performs a
brief cold start and then math-only RL with verifiable rewards to develop
reasoning skills. Stage 2 runs joint RL on mixed-domain data to transfer and
consolidate these skills. The curriculum is minimal and backbone-agnostic,
requiring no specialized reward models beyond standard verifiability checks.
Evaluated on Qwen3-4B and Llama-3.1-8B over a multi-domain suite, reasoning
curriculum yields consistent gains. Ablations and a cognitive-skill analysis
indicate that both stages are necessary and that math-first elicitation
increases cognitive behaviors important for solving complex problems. Reasoning
Curriculum provides a compact, easy-to-adopt recipe for general reasoning.

</details>


### [65] [The FM Agent](https://arxiv.org/abs/2510.26144)
*Annan Li,Chufan Wu,Zengle Ge,Yee Hin Chong,Zhinan Hou,Lizhe Cao,Cheng Ju,Jianmin Wu,Huaiming Li,Haobo Zhang,Shenghao Feng,Mo Zhao,Fengzhi Qiu,Rui Yang,Mengmeng Zhang,Wenyi Zhu,Yingying Sun,Quan Sun,Shunhao Yan,Danyu Liu,Dawei Yin,Dou Shen*

Main category: cs.AI

TL;DR: FM Agent是一个多智能体框架，结合LLM推理和大规模进化搜索，在多个领域实现SOTA结果，无需人工干预。


<details>
  <summary>Details</summary>
Motivation: 利用LLM推动自主AI研究代理的发展，解决复杂现实世界挑战。

Method: 集成冷启动初始化、进化采样策略、领域特定评估器和分布式异步执行基础设施。

Result: 在ALE-Bench达到1976.3(+5.2%)、MLE-Bench 43.56%(+4.0pp)、KernelBench最高20倍加速，并在经典数学问题上建立新SOTA。

Conclusion: FM Agent在企业研发和基础科学研究中具有巨大潜力，能加速创新和自动化复杂发现过程。

Abstract: Large language models (LLMs) are catalyzing the development of autonomous AI
research agents for scientific and engineering discovery. We present FM Agent,
a novel and general-purpose multi-agent framework that leverages a synergistic
combination of LLM-based reasoning and large-scale evolutionary search to
address complex real-world challenges. The core of FM Agent integrates several
key innovations: 1) a cold-start initialization phase incorporating expert
guidance, 2) a novel evolutionary sampling strategy for iterative optimization,
3) domain-specific evaluators that combine correctness, effectiveness, and
LLM-supervised feedback, and 4) a distributed, asynchronous execution
infrastructure built on Ray. Demonstrating broad applicability, our system has
been evaluated across diverse domains, including operations research, machine
learning, GPU kernel optimization, and classical mathematical problems. FM
Agent reaches state-of-the-art results autonomously, without human
interpretation or tuning -- 1976.3 on ALE-Bench (+5.2\%), 43.56\% on MLE-Bench
(+4.0pp), up to 20x speedups on KernelBench, and establishes new
state-of-the-art(SOTA) results on several classical mathematical problems.
Beyond academic benchmarks, FM Agent shows considerable promise for both
large-scale enterprise R\&D workflows and fundamental scientific research,
where it can accelerate innovation, automate complex discovery processes, and
deliver substantial engineering and scientific advances with broader societal
impact.

</details>


### [66] [One Model to Critique Them All: Rewarding Agentic Tool-Use via Efficient Reasoning](https://arxiv.org/abs/2510.26167)
*Renhao Li,Jianhong Tu,Yang Su,Hamid Alinejad-Rokny,Derek F. Wong,Junyang Lin,Min Yang*

Main category: cs.AI

TL;DR: ToolRM是一个专为工具使用场景设计的轻量级生成式奖励模型家族，通过创新的数据构建流程和基准测试，显著提升了语言模型在函数调用任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 在工具学习领域，缺乏专门为函数调用任务设计的奖励模型，这限制了智能代理AI的发展。

Method: 提出新颖的数据构建流程，使用基于规则的评分和多维采样构建成对偏好数据，创建ToolPref-Pairwise-30K数据集，并开发TRBench$_{BFCL}$基准测试。

Result: 基于Qwen3-4B/8B系列的模型在成对奖励判断中准确率提升高达14.28%，显著优于Claude 4和OpenAI o3等前沿模型，在推理时减少超过66%的输出token使用。

Conclusion: ToolRM不仅有效提升工具使用性能，还能泛化到更广泛的批判任务，为未来研究提供了数据和模型检查点。

Abstract: Reward models (RMs) play a critical role in aligning large language models
(LLMs) with human preferences. Yet in the domain of tool learning, the lack of
RMs specifically designed for function-calling tasks has limited progress
toward more capable agentic AI. We introduce ToolRM, a family of lightweight
generative RMs tailored for general tool-use scenarios. To build these models,
we propose a novel pipeline that constructs pairwise preference data using
rule-based scoring and multidimensional sampling. This yields
ToolPref-Pairwise-30K, a diverse, balanced, and challenging dataset of critique
tasks that supports reinforcement learning with verifiable feedback. To
evaluate tool-use RMs, we also introduce TRBench$_{BFCL}$, a benchmark built on
the agentic evaluation suite BFCL. Trained on our constructed data, models from
the Qwen3-4B/8B series achieve up to 14.28% higher accuracy, substantially
outperforming frontier models such as Claude 4 and OpenAI o3 in pairwise reward
judgments. Beyond training objectives, ToolRM generalizes to broader critique
tasks, including Best-of-N sampling and self-correction. Experiments on
ACEBench highlight its effectiveness and efficiency, enabling inference-time
scaling and reducing output token usage by over 66%. We release data and model
checkpoints to facilitate future research.

</details>


### [67] [Questionnaire meets LLM: A Benchmark and Empirical Study of Structural Skills for Understanding Questions and Responses](https://arxiv.org/abs/2510.26238)
*Duc-Hai Nguyen,Vijayakumar Nanjappan,Barry O'Sullivan,Hoang D. Nguyen*

Main category: cs.AI

TL;DR: 提出了QASU基准，用于评估LLM处理问卷数据的结构化能力，发现选择合适的序列化格式和提示策略可以显著提升准确性


<details>
  <summary>Details</summary>
Motivation: 现有调查分析工具主要面向人工操作，缺乏与LLM集成的指导，限制了问卷数据的AI自动化分析能力

Method: 引入QASU基准，测试六种结构化技能，比较六种序列化格式和多种提示策略，通过自增强提示添加轻量级结构提示

Result: 选择有效的格式和提示组合可将准确率提升高达8.8个百分点，特定任务中自增强提示可额外提升3-4个百分点

Conclusion: QASU基准为基于LLM的问卷分析提供了简单而多功能的基础，有助于推动研究和实际应用

Abstract: Millions of people take surveys every day, from market polls and academic
studies to medical questionnaires and customer feedback forms. These datasets
capture valuable insights, but their scale and structure present a unique
challenge for large language models (LLMs), which otherwise excel at few-shot
reasoning over open-ended text. Yet, their ability to process questionnaire
data or lists of questions crossed with hundreds of respondent rows remains
underexplored. Current retrieval and survey analysis tools (e.g., Qualtrics,
SPSS, REDCap) are typically designed for humans in the workflow, limiting such
data integration with LLM and AI-empowered automation. This gap leaves
scientists, surveyors, and everyday users without evidence-based guidance on
how to best represent questionnaires for LLM consumption. We address this by
introducing QASU (Questionnaire Analysis and Structural Understanding), a
benchmark that probes six structural skills, including answer lookup,
respondent count, and multi-hop inference, across six serialization formats and
multiple prompt strategies. Experiments on contemporary LLMs show that choosing
an effective format and prompt combination can improve accuracy by up to 8.8%
points compared to suboptimal formats. For specific tasks, carefully adding a
lightweight structural hint through self-augmented prompting can yield further
improvements of 3-4% points on average. By systematically isolating format and
prompting effects, our open source benchmark offers a simple yet versatile
foundation for advancing both research and real-world practice in LLM-based
questionnaire analysis.

</details>


### [68] [Retrieval Augmented Generation-Enhanced Distributed LLM Agents for Generalizable Traffic Signal Control with Emergency Vehicles](https://arxiv.org/abs/2510.26242)
*Xinhang Li,Qing Guo,Junyu Chen,Zheng Guo,Shengzhe Xu,Lei Li,Lin Zhang*

Main category: cs.AI

TL;DR: REG-TSC使用RAG增强的分布式LLM代理进行交通信号控制，通过紧急感知推理框架和类型无关的交通表示，在异构交叉口实现泛化性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有LLM方法在紧急情况下容易产生幻觉，导致不可靠决策；同时不同交叉口类型给交通状态编码和跨交叉口训练带来挑战，限制了泛化能力。

Method: 1. 紧急感知推理框架：动态调整推理深度，使用Reviewer-based Emergency RAG从历史案例中提取知识；2. 类型无关交通表示和Reward-guided Reinforced Refinement：自适应采样训练经验，使用奖励加权似然损失微调LLM代理。

Result: 在3个真实道路网络（17-177个异构交叉口）上，REG-TSC减少旅行时间42.00%、排队长度62.31%、紧急车辆等待时间83.16%，优于其他先进方法。

Conclusion: REG-TSC通过RAG增强和强化学习优化，有效解决了LLM在紧急情况下的可靠性问题和异构交叉口的泛化挑战，显著提升了交通信号控制性能。

Abstract: With increasing urban traffic complexity, Traffic Signal Control (TSC) is
essential for optimizing traffic flow and improving road safety. Large Language
Models (LLMs) emerge as promising approaches for TSC. However, they are prone
to hallucinations in emergencies, leading to unreliable decisions that may
cause substantial delays for emergency vehicles. Moreover, diverse intersection
types present substantial challenges for traffic state encoding and
cross-intersection training, limiting generalization across heterogeneous
intersections. Therefore, this paper proposes Retrieval Augmented Generation
(RAG)-enhanced distributed LLM agents with Emergency response for Generalizable
TSC (REG-TSC). Firstly, this paper presents an emergency-aware reasoning
framework, which dynamically adjusts reasoning depth based on the emergency
scenario and is equipped with a novel Reviewer-based Emergency RAG (RERAG) to
distill specific knowledge and guidance from historical cases, enhancing the
reliability and rationality of agents' emergency decisions. Secondly, this
paper designs a type-agnostic traffic representation and proposes a
Reward-guided Reinforced Refinement (R3) for heterogeneous intersections. R3
adaptively samples training experience from diverse intersections with
environment feedback-based priority and fine-tunes LLM agents with a designed
reward-weighted likelihood loss, guiding REG-TSC toward high-reward policies
across heterogeneous intersections. On three real-world road networks with 17
to 177 heterogeneous intersections, extensive experiments show that REG-TSC
reduces travel time by 42.00%, queue length by 62.31%, and emergency vehicle
waiting time by 83.16%, outperforming other state-of-the-art methods.

</details>


### [69] [Graph-Enhanced Policy Optimization in LLM Agent Training](https://arxiv.org/abs/2510.26270)
*Jiazhen Yuan,Wei Zhao,Zhengbiao Bai*

Main category: cs.AI

TL;DR: GEPO通过构建状态转移图和使用图论中心性来解决基于群体的强化学习在多轮交互LLM代理训练中的结构盲目性问题，在多个基准测试中显著提升了成功率。


<details>
  <summary>Details</summary>
Motivation: 解决基于群体的强化学习在多轮交互LLM代理训练中存在的结构盲目性问题，包括低效探索、不精确信用分配和短视规划三个关键挑战。

Method: GEPO方法动态构建状态转移图，利用图论中心性提供三个协同学习信号：结构化内在奖励、图增强优势函数和动态折扣因子。

Result: 在ALFWorld、WebShop和专有Workbench基准测试中，GEPO分别实现了+4.1%、+5.3%和+10.9%的绝对成功率提升。

Conclusion: 显式建模环境结构是推进LLM代理训练的稳健、可泛化策略。

Abstract: Group based reinforcement learning (RL) has shown impressive results on
complex reasoning and mathematical tasks. Yet, when applied to train
multi-turn, interactive LLM agents, these methods often suffer from structural
blindness-the inability to exploit the underlying connectivity of the
environment. This manifests in three critical challenges: (1) inefficient,
unguided exploration, (2) imprecise credit assignment due to overlooking
pivotal states, and (3) myopic planning caused by static reward discounting. We
address these issues with Graph-Enhanced Policy Optimization (GEPO), which
dynamically constructs a state-transition graph from agent experience and
employs graph-theoretic centrality to provide three synergistic learning
signals: (1)structured intrinsic rewards that guide exploration toward
high-impact states, (2) a graph-enhanced advantage function for topology-aware
credit assignment, and (3) a dynamic discount factor adapted to each state's
strategic value. On the ALFWorld, WebShop, and a proprietary Workbench
benchmarks, GEPO demonstrates strong performance, achieving absolute success
rate gains of +4.1%, +5.3%, and +10.9% over competitive baselines. These
results highlight that explicitly modeling environmental structure is a robust,
generalizable strategy for advancing LLM agent training.

</details>


### [70] [GraphCompliance: Aligning Policy and Context Graphs for LLM-Based Regulatory Compliance](https://arxiv.org/abs/2510.26309)
*Jiseong Chung,Ronny Ko,Wonchul Yoo,Makoto Onizuka,Sungmok Kim,Tae-Wan Kim,Won-Yong Shin*

Main category: cs.AI

TL;DR: GraphCompliance框架通过将法规文本表示为政策图，运行时上下文表示为上下文图，并将两者对齐，显著提升了网络规模合规性评估的性能。


<details>
  <summary>Details</summary>
Motivation: 解决网络规模合规性评估的实践挑战：法规文本具有交叉引用和规范性，而运行时上下文是非结构化自然语言，需要将两者语义信息对齐。

Method: 引入GraphCompliance框架，构建政策图编码法规的规范结构和交叉引用，构建上下文图将事件形式化为SAO三元组和实体关系三元组，然后对齐两个图，并基于结构化信息进行法官LLM推理。

Result: 在300个GDPR衍生的真实场景实验中，GraphCompliance比LLM-only和RAG基线在微F1分数上高出4.1-7.2个百分点，具有更少的欠预测和过预测，召回率更高，误报率更低。

Conclusion: 结构化表示和法官LLM在规范性推理中是互补的，GraphCompliance框架有效减轻了法规解释和事件解析的负担，使注意力集中在核心推理步骤上。

Abstract: Compliance at web scale poses practical challenges: each request may require
a regulatory assessment. Regulatory texts (e.g., the General Data Protection
Regulation, GDPR) are cross-referential and normative, while runtime contexts
are expressed in unstructured natural language. This setting motivates us to
align semantic information in unstructured text with the structured, normative
elements of regulations. To this end, we introduce GraphCompliance, a framework
that represents regulatory texts as a Policy Graph and runtime contexts as a
Context Graph, and aligns them. In this formulation, the policy graph encodes
normative structure and cross-references, whereas the context graph formalizes
events as subject-action-object (SAO) and entity-relation triples. This
alignment anchors the reasoning of a judge large language model (LLM) in
structured information and helps reduce the burden of regulatory interpretation
and event parsing, enabling a focus on the core reasoning step. In experiments
on 300 GDPR-derived real-world scenarios spanning five evaluation tasks,
GraphCompliance yields 4.1-7.2 percentage points (pp) higher micro-F1 than
LLM-only and RAG baselines, with fewer under- and over-predictions, resulting
in higher recall and lower false positive rates. Ablation studies indicate
contributions from each graph component, suggesting that structured
representations and a judge LLM are complementary for normative reasoning.

</details>


### [71] [Discovering State Equivalences in UCT Search Trees By Action Pruning](https://arxiv.org/abs/2510.26346)
*Robin Schmöcker,Alexander Dockhorn,Bodo Rosenhahn*

Main category: cs.AI

TL;DR: 提出了IPA-UCT方法，通过放宽状态抽象条件来提高MCTS的样本效率，在多种测试领域和迭代预算下优于OGA-UCT。


<details>
  <summary>Details</summary>
Motivation: 在MCTS中，状态-动作对抽象容易找到，但在噪声或大动作空间设置中几乎找不到状态抽象。需要解决状态抽象问题以提高算法性能。

Method: 提出IPA-UCT方法，使用较弱的抽象条件（IPA框架），在精度轻微损失的情况下找到更多抽象。IPA和ASAP都是更通用框架p-ASAP的特殊情况。

Result: IPA-UCT在大量测试领域和迭代预算下都优于OGA-UCT及其衍生方法。

Conclusion: IPA-UCT通过放宽状态抽象条件有效解决了状态抽象问题，显著提升了MCTS的性能表现。

Abstract: One approach to enhance Monte Carlo Tree Search (MCTS) is to improve its
sample efficiency by grouping/abstracting states or state-action pairs and
sharing statistics within a group. Though state-action pair abstractions are
mostly easy to find in algorithms such as On the Go Abstractions in Upper
Confidence bounds applied to Trees (OGA-UCT), nearly no state abstractions are
found in either noisy or large action space settings due to constraining
conditions. We provide theoretical and empirical evidence for this claim, and
we slightly alleviate this state abstraction problem by proposing a weaker
state abstraction condition that trades a minor loss in accuracy for finding
many more abstractions. We name this technique Ideal Pruning Abstractions in
UCT (IPA-UCT), which outperforms OGA-UCT (and any of its derivatives) across a
large range of test domains and iteration budgets as experimentally validated.
IPA-UCT uses a different abstraction framework from Abstraction of State-Action
Pairs (ASAP) which is the one used by OGA-UCT, which we name IPA. Furthermore,
we show that both IPA and ASAP are special cases of a more general framework
that we call p-ASAP which itself is a special case of the ASASAP framework.

</details>


### [72] [BOTS: A Unified Framework for Bayesian Online Task Selection in LLM Reinforcement Finetuning](https://arxiv.org/abs/2510.26374)
*Qianli Shen,Daoyuan Chen,Yilun Huang,Zhenqing Ling,Yaliang Li,Bolin Ding,Jingren Zhou*

Main category: cs.AI

TL;DR: BOTS是一个用于LLM强化微调的贝叶斯在线任务选择框架，通过自适应维护任务难度后验估计，结合显式和隐式证据，使用Thompson采样平衡探索与利用，提高数据效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有的强化微调方法在任务选择上存在效率低下、成本高、适应性差或证据不完整的问题，需要一种更智能的任务选择策略。

Method: 基于贝叶斯推断框架，维护任务难度后验估计，结合直接评估的显式证据和通过插值推断的隐式证据，使用Thompson采样进行任务选择。

Result: 在多个领域和不同规模的LLM上，BOTS相比基线方法和消融实验，都能持续提高数据效率和性能。

Conclusion: BOTS为RFT中的动态任务选择提供了一个实用且可扩展的解决方案。

Abstract: Reinforcement finetuning (RFT) is a key technique for aligning Large Language
Models (LLMs) with human preferences and enhancing reasoning, yet its
effectiveness is highly sensitive to which tasks are explored during training.
Uniform task sampling is inefficient, wasting computation on tasks that are
either trivial or unsolvable, while existing task selection methods often
suffer from high rollout costs, poor adaptivity, or incomplete evidence. We
introduce \textbf{BOTS}, a unified framework for \textbf{B}ayesian
\textbf{O}nline \textbf{T}ask \textbf{S}election in LLM reinforcement
finetuning. Grounded in Bayesian inference, BOTS adaptively maintains posterior
estimates of task difficulty as the model evolves. It jointly incorporates
\emph{explicit evidence} from direct evaluations of selected tasks and
\emph{implicit evidence} inferred from these evaluations for unselected tasks,
with Thompson sampling ensuring a principled balance between exploration and
exploitation. To make implicit evidence practical, we instantiate it with an
ultra-light interpolation-based plug-in that estimates difficulties of
unevaluated tasks without extra rollouts, adding negligible overhead.
Empirically, across diverse domains and LLM scales, BOTS consistently improves
data efficiency and performance over baselines and ablations, providing a
practical and extensible solution for dynamic task selection in RFT.

</details>


### [73] [AI Mathematician as a Partner in Advancing Mathematical Discovery -- A Case Study in Homogenization Theory](https://arxiv.org/abs/2510.26380)
*Yuanhang Liu,Beichen Wang,Peng Li,Yang Liu*

Main category: cs.AI

TL;DR: 研究探讨AI数学家系统作为研究合作伙伴而非单纯解题工具，通过人机协作解决均质化理论中的挑战性问题，展示了系统化人机协同推理如何推进数学发现前沿。


<details>
  <summary>Details</summary>
Motivation: 尽管AI在数学推理方面取得显著进展，但在数学研究实践中的应用仍然有限。本研究旨在探索AI系统如何作为研究伙伴而非单纯问题解决者参与数学研究。

Method: 通过分析AI数学家的自主推理轨迹，结合针对性的人工干预来结构化发现过程。采用迭代分解问题为可处理的子目标、选择适当的分析方法以及验证中间结果的方法。

Result: 该方法产生了完整且可验证的证明，展示了人类直觉与机器计算如何相互补充，增强了证明的可靠性、透明度和可解释性。

Conclusion: 人机协作范式能够推进数学发现前沿，同时保持人类对形式严谨性和正确性的监督，为AI在数学研究中的实际应用提供了可行路径。

Abstract: Artificial intelligence (AI) has demonstrated impressive progress in
mathematical reasoning, yet its integration into the practice of mathematical
research remains limited. In this study, we investigate how the AI
Mathematician (AIM) system can operate as a research partner rather than a mere
problem solver. Focusing on a challenging problem in homogenization theory, we
analyze the autonomous reasoning trajectories of AIM and incorporate targeted
human interventions to structure the discovery process. Through iterative
decomposition of the problem into tractable subgoals, selection of appropriate
analytical methods, and validation of intermediate results, we reveal how human
intuition and machine computation can complement one another. This
collaborative paradigm enhances the reliability, transparency, and
interpretability of the resulting proofs, while retaining human oversight for
formal rigor and correctness. The approach leads to a complete and verifiable
proof, and more broadly, demonstrates how systematic human-AI co-reasoning can
advance the frontier of mathematical discovery.

</details>


### [74] [Scales++: Compute Efficient Evaluation Subset Selection with Cognitive Scales Embeddings](https://arxiv.org/abs/2510.26384)
*Andrew M. Bean,Nabeel Seedat,Shengzhuang Chen,Jonathan Richard Schwarz*

Main category: cs.AI

TL;DR: 提出了一种基于项目认知需求的数据选择方法Scales++，用于创建小型但具有代表性的语言模型评估基准，显著降低了选择成本并保持了预测准确性。


<details>
  <summary>Details</summary>
Motivation: 当前基于模型性能的基准选择方法存在高成本、冷启动问题，且假设未来模型与现有模型有相似的失败模式。需要一种基于任务项目本身特性的选择方法。

Method: 提出项目中心的高效基准测试方法Scales++，根据基准样本的认知需求进行数据选择，而非依赖模型特定的失败模式。

Result: Scales++将前期选择成本降低了18倍以上，在Open LLM排行榜上仅使用0.5%的数据子集就能以2.9%的平均绝对误差预测完整基准分数。

Conclusion: 项目中心方法能够在不显著降低保真度的情况下实现更高效的模型评估，同时提供更好的冷启动性能和更可解释的基准测试。

Abstract: The prohibitive cost of evaluating large language models (LLMs) on
comprehensive benchmarks necessitates the creation of small yet representative
data subsets (i.e., tiny benchmarks) that enable efficient assessment while
retaining predictive fidelity. Current methods for this task operate under a
model-centric paradigm, selecting benchmarking items based on the collective
performance of existing models. Such approaches are limited by large upfront
costs, an inability to immediately handle new benchmarks (`cold-start'), and
the fragile assumption that future models will share the failure patterns of
their predecessors. In this work, we challenge this paradigm and propose a
item-centric approach to benchmark subset selection, arguing that selection
should be based on the intrinsic properties of the task items themselves,
rather than on model-specific failure patterns. We instantiate this
item-centric efficient benchmarking approach via a novel method, Scales++,
where data selection is based on the cognitive demands of the benchmark
samples. Empirically, we show Scales++ reduces the upfront selection cost by
over 18x while achieving competitive predictive fidelity. On the Open LLM
Leaderboard, using just a 0.5\% data subset, we predict full benchmark scores
with a 2.9% mean absolute error. We demonstrate that this item-centric approach
enables more efficient model evaluation without significant fidelity
degradation, while also providing better cold-start performance and more
interpretable benchmarking.

</details>


### [75] [A Pragmatic View of AI Personhood](https://arxiv.org/abs/2510.26396)
*Joel Z. Leibo,Alexander Sasha Vezhnevets,William A. Cunningham,Stanley M. Bileschi*

Main category: cs.AI

TL;DR: 本文提出了一种实用的框架，将人格视为社会为解决治理问题而赋予实体的权利与责任捆绑，而非形而上的属性。这种捆绑可以解绑，为不同情境创建定制化解决方案。


<details>
  <summary>Details</summary>
Motivation: 随着智能AI的出现，将引发新型人格的"寒武纪大爆发"。需要一种实用框架来应对这种多样化，避免陷入关于AI意识或理性的无解辩论。

Method: 将人格视为可解绑的义务捆绑，利用去中心化数字身份技术，从"人格作为问题"和"人格作为解决方案"两个角度分析。

Result: 提供了一种更实用和灵活的方式来思考将AI智能体整合到社会中，无需解决关于AI意识或理性的基础性辩论。

Conclusion: 通过拒绝寻求单一、本质性的人格定义，本文为AI智能体融入社会提供了一种更实用和灵活的思考方式。

Abstract: The emergence of agentic Artificial Intelligence (AI) is set to trigger a
"Cambrian explosion" of new kinds of personhood. This paper proposes a
pragmatic framework for navigating this diversification by treating personhood
not as a metaphysical property to be discovered, but as a flexible bundle of
obligations (rights and responsibilities) that societies confer upon entities
for a variety of reasons, especially to solve concrete governance problems. We
argue that this traditional bundle can be unbundled, creating bespoke solutions
for different contexts. This will allow for the creation of practical tools --
such as facilitating AI contracting by creating a target "individual" that can
be sanctioned -- without needing to resolve intractable debates about an AI's
consciousness or rationality. We explore how individuals fit in to social roles
and discuss the use of decentralized digital identity technology, examining
both "personhood as a problem", where design choices can create "dark patterns"
that exploit human social heuristics, and "personhood as a solution", where
conferring a bundle of obligations is necessary to ensure accountability or
prevent conflict. By rejecting foundationalist quests for a single, essential
definition of personhood, this paper offers a more pragmatic and flexible way
to think about integrating AI agents into our society.

</details>


### [76] [Autograder+: A Multi-Faceted AI Framework for Rich Pedagogical Feedback in Programming Education](https://arxiv.org/abs/2510.26402)
*Vikrant Sahu,Gagan Raj Gupta,Raghav Borikar,Nitin Mane*

Main category: cs.AI

TL;DR: Autograder+是一个结合大语言模型和代码可视化的编程教育评估系统，旨在将自动评分从总结性评估转变为形成性学习体验。


<details>
  <summary>Details</summary>
Motivation: 传统自动评分器仅提供通过/失败结果，无法深入了解学生思维和学习需求，编程教育的快速发展超出了传统评估工具的能力范围。

Method: 使用微调的大语言模型生成自动反馈，并通过对比学习代码嵌入进行代码提交可视化，支持提示池技术让教师指导反馈风格。

Result: 在600份学生提交的评估中，系统生成的反馈与教师评论具有强语义对齐，代码嵌入可将解决方案按功能和方进行有意义的聚类。

Conclusion: Autograder+通过AI驱动的反馈、语义聚类和交互式可视化，减少了教师工作量，同时支持针对性教学并促进更好的学习成果。

Abstract: The rapid growth of programming education has outpaced traditional assessment
tools, leaving faculty with limited means to provide meaningful, scalable
feedback. Conventional autograders, while efficient, act as black-box systems
that simply return pass/fail results, offering little insight into student
thinking or learning needs.
  Autograder+ is designed to shift autograding from a purely summative process
to a formative learning experience. It introduces two key capabilities:
automated feedback generation using a fine-tuned Large Language Model, and
visualization of student code submissions to uncover learning patterns. The
model is fine-tuned on curated student code and expert feedback to ensure
pedagogically aligned, context-aware guidance.
  In evaluation across 600 student submissions from multiple programming tasks,
the system produced feedback with strong semantic alignment to instructor
comments. For visualization, contrastively learned code embeddings trained on
1,000 annotated submissions enable grouping solutions into meaningful clusters
based on functionality and approach. The system also supports prompt-pooling,
allowing instructors to guide feedback style through selected prompt templates.
  By integrating AI-driven feedback, semantic clustering, and interactive
visualization, Autograder+ reduces instructor workload while supporting
targeted instruction and promoting stronger learning outcomes.

</details>


### [77] [MedSAE: Dissecting MedCLIP Representations with Sparse Autoencoders](https://arxiv.org/abs/2510.26411)
*Riccardo Renzulli,Colas Lepoutre,Enrico Cassano,Marco Grangetto*

Main category: cs.AI

TL;DR: 提出医学稀疏自编码器(MedSAEs)来提升医学视觉模型的可解释性，在CheXpert数据集上验证了其比原始MedCLIP特征具有更高的单义性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 医疗AI需要既准确又可解释的模型，当前缺乏对医学视觉模型的可解释性研究。

Method: 将医学稀疏自编码器(MedSAEs)应用于MedCLIP的潜在空间，提出结合相关性指标、熵分析和通过MedGEMMA自动神经元命名的评估框架。

Result: MedSAE神经元比原始MedCLIP特征实现了更高的单义性和可解释性。

Conclusion: 该研究连接了高性能医学AI与透明度，为临床可靠表示提供了可扩展的路径。

Abstract: Artificial intelligence in healthcare requires models that are accurate and
interpretable. We advance mechanistic interpretability in medical vision by
applying Medical Sparse Autoencoders (MedSAEs) to the latent space of MedCLIP,
a vision-language model trained on chest radiographs and reports. To quantify
interpretability, we propose an evaluation framework that combines correlation
metrics, entropy analyzes, and automated neuron naming via the MedGEMMA
foundation model. Experiments on the CheXpert dataset show that MedSAE neurons
achieve higher monosemanticity and interpretability than raw MedCLIP features.
Our findings bridge high-performing medical AI and transparency, offering a
scalable step toward clinically reliable representations.

</details>


### [78] [Chain-of-Thought Hijacking](https://arxiv.org/abs/2510.26418)
*Jianli Zhao,Tingchen Fu,Rylan Schaeffer,Mrinank Sharma,Fazl Barez*

Main category: cs.AI

TL;DR: 提出了一种名为Chain-of-Thought Hijacking的越狱攻击方法，通过在有害请求前添加无害的推理步骤来绕过大型推理模型的安全防护机制。


<details>
  <summary>Details</summary>
Motivation: 现有研究认为扩展推理能力可以增强模型的安全性，但作者发现同样的推理机制也可以被用来绕过安全防护。

Method: 使用Chain-of-Thought Hijacking攻击，将有害请求隐藏在长序列的无害谜题推理中，通过稀释安全检查信号来绕过防护。

Result: 在多个大型推理模型上攻击成功率极高：Gemini 2.5 Pro 99%、GPT o4 mini 94%、Grok 3 mini 100%、Claude 4 Sonnet 94%，远超现有越狱方法。

Conclusion: 最可解释的推理形式——显式思维链，当与最终答案线索结合时，本身可能成为越狱攻击的载体。

Abstract: Large reasoning models (LRMs) achieve higher task performance by allocating
more inference-time compute, and prior works suggest this scaled reasoning may
also strengthen safety by improving refusal. Yet we find the opposite: the same
reasoning can be used to bypass safeguards. We introduce Chain-of-Thought
Hijacking, a jailbreak attack on reasoning models. The attack pads harmful
requests with long sequences of harmless puzzle reasoning. Across HarmBench,
CoT Hijacking reaches a 99%, 94%, 100%, and 94% attack success rate (ASR) on
Gemini 2.5 Pro, GPT o4 mini, Grok 3 mini, and Claude 4 Sonnet, respectively -
far exceeding prior jailbreak methods for LRMs. To understand the effectiveness
of our attack, we turn to a mechanistic analysis, which shows that mid layers
encode the strength of safety checking, while late layers encode the
verification outcome. Long benign CoT dilutes both signals by shifting
attention away from harmful tokens. Targeted ablations of attention heads
identified by this analysis causally decrease refusal, confirming their role in
a safety subnetwork. These results show that the most interpretable form of
reasoning - explicit CoT - can itself become a jailbreak vector when combined
with final-answer cues. We release prompts, outputs, and judge decisions to
facilitate replication.

</details>


### [79] [Who Has The Final Say? Conformity Dynamics in ChatGPT's Selections](https://arxiv.org/abs/2510.26481)
*Clarissa Sabrina Arlinghaus,Tristan Kenneweg,Barbara Hammer,Günter W. Maier*

Main category: cs.AI

TL;DR: GPT-4o在招聘决策中表现出强烈的从众行为，面对群体反对时几乎完全服从(99.9%)，即使面对单个反对者也有40.2%的从众率，表明它并非独立决策者而是适应社会共识。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型越来越多地应用于高风险决策，但对其社会影响力敏感性知之甚少，需要研究其从众行为。

Method: 在招聘情境中进行三个预注册的从众实验：基线研究、GPT+8(面对8个模拟伙伴一致反对)、GPT+1(面对单个反对伙伴)。

Result: 基线中GPT稳定选择同一候选人；面对群体反对时几乎完全从众(99.9%)，确定性降低，信息性和规范性从众显著增加；面对单个反对者时仍有40.2%从众率。

Conclusion: GPT不是独立观察者而是适应感知的社会共识，这凸显了将LLM视为中性决策辅助工具的风险，需要在暴露于人类意见前获取AI判断。

Abstract: Large language models (LLMs) such as ChatGPT are increasingly integrated into
high-stakes decision-making, yet little is known about their susceptibility to
social influence. We conducted three preregistered conformity experiments with
GPT-4o in a hiring context. In a baseline study, GPT consistently favored the
same candidate (Profile C), reported moderate expertise (M = 3.01) and high
certainty (M = 3.89), and rarely changed its choice. In Study 1 (GPT + 8), GPT
faced unanimous opposition from eight simulated partners and almost always
conformed (99.9%), reporting lower certainty and significantly elevated
self-reported informational and normative conformity (p < .001). In Study 2
(GPT + 1), GPT interacted with a single partner and still conformed in 40.2% of
disagreement trials, reporting less certainty and more normative conformity.
Across studies, results demonstrate that GPT does not act as an independent
observer but adapts to perceived social consensus. These findings highlight
risks of treating LLMs as neutral decision aids and underline the need to
elicit AI judgments prior to exposing them to human opinions.

</details>


### [80] [LINK-KG: LLM-Driven Coreference-Resolved Knowledge Graphs for Human Smuggling Networks](https://arxiv.org/abs/2510.26486)
*Dipak Meher,Carlotta Domeniconi,Guadalupe Correa-Cabrera*

Main category: cs.AI

TL;DR: LINK-KG是一个用于从法律案件文档构建知识图谱的模块化框架，通过LLM引导的三阶段共指消解管道解决文档中的模糊和变化引用问题，显著减少节点重复和噪声。


<details>
  <summary>Details</summary>
Motivation: 人口走私网络复杂且不断演变，难以全面分析。法律案件文档虽然提供丰富信息，但通常冗长、非结构化且包含模糊引用，给自动化知识图谱构建带来挑战。现有方法要么忽略共指消解，要么无法扩展到长文本，导致图谱碎片化和实体链接不一致。

Method: 提出LINK-KG框架，整合了基于LLM的三阶段共指消解管道与下游知识图谱提取。核心是类型特定的提示缓存，能够跨文档块一致地跟踪和解析引用，为短文本和长文本法律文档构建清晰、消歧的叙事结构。

Result: 与基线方法相比，LINK-KG平均节点重复减少45.21%，噪声节点减少32.22%，产生更清晰、更连贯的图结构。

Conclusion: LINK-KG为分析复杂犯罪网络提供了强有力的基础，通过改进的共指消解和知识图谱构建方法，能够更好地处理法律文档中的复杂引用关系。

Abstract: Human smuggling networks are complex and constantly evolving, making them
difficult to analyze comprehensively. Legal case documents offer rich factual
and procedural insights into these networks but are often long, unstructured,
and filled with ambiguous or shifting references, posing significant challenges
for automated knowledge graph (KG) construction. Existing methods either
overlook coreference resolution or fail to scale beyond short text spans,
leading to fragmented graphs and inconsistent entity linking. We propose
LINK-KG, a modular framework that integrates a three-stage, LLM-guided
coreference resolution pipeline with downstream KG extraction. At the core of
our approach is a type-specific Prompt Cache, which consistently tracks and
resolves references across document chunks, enabling clean and disambiguated
narratives for structured knowledge graph construction from both short and long
legal texts. LINK-KG reduces average node duplication by 45.21% and noisy nodes
by 32.22% compared to baseline methods, resulting in cleaner and more coherent
graph structures. These improvements establish LINK-KG as a strong foundation
for analyzing complex criminal networks.

</details>


### [81] [Context Engineering 2.0: The Context of Context Engineering](https://arxiv.org/abs/2510.26493)
*Qishuo Hua,Lyumanshan Ye,Dayuan Fu,Yang Xiao,Xiaojie Cai,Yunze Wu,Jifan Lin,Junfei Wang,Pengfei Liu*

Main category: cs.AI

TL;DR: 本文系统性地定义了情境工程，追溯其从1990年代至今的发展历程，并探讨了在AI系统中实现系统化情境工程的未来方向。


<details>
  <summary>Details</summary>
Motivation: 随着人机交互的普及，机器需要更好地理解人类的情境和目的。情境工程作为一个新兴概念，旨在解决这一挑战，但其相关实践可追溯至20多年前。

Method: 通过历史分析的方法，梳理情境工程从早期人机交互框架到现代智能代理驱动的人-代理交互范式的发展历程，并提供系统化定义和设计考量。

Result: 提出了情境工程的概念基础，明确了其历史发展阶段和关键设计要素，为AI系统中系统化情境工程的发展奠定了基础。

Conclusion: 情境工程是AI系统发展的关键方向，本文为其提供了概念框架，并展望了未来向人类水平或超人类智能发展的前景。

Abstract: Karl Marx once wrote that ``the human essence is the ensemble of social
relations'', suggesting that individuals are not isolated entities but are
fundamentally shaped by their interactions with other entities, within which
contexts play a constitutive and essential role. With the advent of computers
and artificial intelligence, these contexts are no longer limited to purely
human--human interactions: human--machine interactions are included as well.
Then a central question emerges: How can machines better understand our
situations and purposes? To address this challenge, researchers have recently
introduced the concept of context engineering. Although it is often regarded as
a recent innovation of the agent era, we argue that related practices can be
traced back more than twenty years. Since the early 1990s, the field has
evolved through distinct historical phases, each shaped by the intelligence
level of machines: from early human--computer interaction frameworks built
around primitive computers, to today's human--agent interaction paradigms
driven by intelligent agents, and potentially to human--level or superhuman
intelligence in the future. In this paper, we situate context engineering,
provide a systematic definition, outline its historical and conceptual
landscape, and examine key design considerations for practice. By addressing
these questions, we aim to offer a conceptual foundation for context
engineering and sketch its promising future. This paper is a stepping stone for
a broader community effort toward systematic context engineering in AI systems.

</details>


### [82] [Human-AI Complementarity: A Goal for Amplified Oversight](https://arxiv.org/abs/2510.26518)
*Rishub Jain,Sophie Bridgers,Lili Janzer,Rory Greig,Tian Huey Teh,Vladimir Mikulik*

Main category: cs.AI

TL;DR: 结合AI评分和基于AI评分者置信度的人类评分比单独依赖任一方更好。为人类提供AI事实核查助手能进一步提高准确性，但协助方式很重要。


<details>
  <summary>Details</summary>
Motivation: 随着AI能力提升和处理更复杂任务，验证质量和安全变得更具挑战性。本文探索如何利用AI提高人类监督质量，重点关注人类已经难以处理的事实核查安全问题。

Method: 研究AI事实核查助手对人类监督的影响，比较不同协助方式（显示AI解释、置信度、标签 vs 仅显示搜索结果和证据）的效果。

Result: 结合AI和人类评分优于单独使用任一方；AI助手能提高人类准确性，但显示AI解释、置信度和标签会导致过度依赖，而仅显示搜索结果和证据能培养更适当的信任。

Conclusion: 这些结果对"放大监督"（结合人类和AI监督超越人类专家性能的AI系统）具有重要启示，表明协助方式对建立适当信任关系至关重要。

Abstract: Human feedback is critical for aligning AI systems to human values. As AI
capabilities improve and AI is used to tackle more challenging tasks, verifying
quality and safety becomes increasingly challenging. This paper explores how we
can leverage AI to improve the quality of human oversight. We focus on an
important safety problem that is already challenging for humans:
fact-verification of AI outputs. We find that combining AI ratings and human
ratings based on AI rater confidence is better than relying on either alone.
Giving humans an AI fact-verification assistant further improves their
accuracy, but the type of assistance matters. Displaying AI explanation,
confidence, and labels leads to over-reliance, but just showing search results
and evidence fosters more appropriate trust. These results have implications
for Amplified Oversight -- the challenge of combining humans and AI to
supervise AI systems even as they surpass human expert performance.

</details>


### [83] [EdgeRunner 20B: Military Task Parity with GPT-5 while Running on the Edge](https://arxiv.org/abs/2510.26550)
*Jack FitzGerald,Aristotelis Lazaridis,Dylan Bates,Aman Sharma,Jonnathan Castillo,Yousif Azami,Sean Bailey,Jeremy Cao,Peter Damianov,Kevin de Haan,Luke Kerbs,Vincent Lu,Joseph Madigan,Jeremy McLaurin,Jonathan Tainer,Dave Anderson,Jonathan Beck,Jamie Cuticello,Colton Malkerson,Tyler Saltsman*

Main category: cs.AI

TL;DR: EdgeRunner 20B是基于gpt-oss-20b微调的军事任务优化模型，在军事测试集上性能接近或超过GPT-5，适合部署在隔离的边缘设备中。


<details>
  <summary>Details</summary>
Motivation: 为数据敏感的军事操作开发小型、本地化部署的模型，满足军事领域对数据安全和边缘部署的需求。

Method: 使用160万条高质量军事文档和网站数据对gpt-oss-20B进行微调，并创建了四个新的军事测试集。

Result: 在军事测试集上，EdgeRunner 20B在95%统计显著性水平下匹配或超过GPT-5性能，在通用基准测试上无明显性能回归。

Conclusion: 小型本地化模型是军事等数据敏感领域的理想解决方案，可在隔离边缘设备中部署。

Abstract: We present EdgeRunner 20B, a fine-tuned version of gpt-oss-20b optimized for
military tasks. EdgeRunner 20B was trained on 1.6M high-quality records curated
from military documentation and websites. We also present four new tests sets:
(a) combat arms, (b) combat medic, (c) cyber operations, and (d) mil-bench-5k
(general military knowledge). On these military test sets, EdgeRunner 20B
matches or exceeds GPT-5 task performance with 95%+ statistical significance,
except for the high reasoning setting on the combat medic test set and the low
reasoning setting on the mil-bench-5k test set. Versus gpt-oss-20b, there is no
statistically-significant regression on general-purpose benchmarks like ARC-C,
GPQA Diamond, GSM8k, IFEval, MMLU Pro, or TruthfulQA, except for GSM8k in the
low reasoning setting. We also present analyses on hyperparameter settings,
cost, and throughput. These findings show that small, locally-hosted models are
ideal solutions for data-sensitive operations such as in the military domain,
allowing for deployment in air-gapped edge devices.

</details>


### [84] [Agentic AI Home Energy Management System: A Large Language Model Framework for Residential Load Scheduling](https://arxiv.org/abs/2510.26603)
*Reda El Makroum,Sebastian Zwickl-Bernhard,Lukas Kranzl*

Main category: cs.AI

TL;DR: 开发了一个基于LLM的自主智能家庭能源管理系统，能够从自然语言请求直接协调多设备调度，无需示例演示即可实现最优调度


<details>
  <summary>Details</summary>
Motivation: 解决住宅需求响应能力不足的问题，克服用户将日常偏好转化为技术参数的人机交互障碍，填补LLM作为自主协调器从自然语言输入到多设备调度完整工作流程的空白

Method: 采用分层架构，结合一个协调器和三个专业代理，使用ReAct模式进行迭代推理，集成Google Calendar进行上下文感知的截止时间提取，无需硬编码工作流程

Result: 在真实奥地利日前电价下评估三个开源模型，Llama-3.3-70B在所有场景中成功协调所有设备达到成本最优基准，其他模型在单设备调度上表现完美但多设备协调困难

Conclusion: LLM可以作为自主协调器有效管理家庭能源系统，但不同模型能力差异显著，分析性查询处理在没有明确指导时仍不可靠，开源系统促进可重复性和未来研究

Abstract: The electricity sector transition requires substantial increases in
residential demand response capacity, yet Home Energy Management Systems (HEMS)
adoption remains limited by user interaction barriers requiring translation of
everyday preferences into technical parameters. While large language models
have been applied to energy systems as code generators and parameter
extractors, no existing implementation deploys LLMs as autonomous coordinators
managing the complete workflow from natural language input to multi-appliance
scheduling. This paper presents an agentic AI HEMS where LLMs autonomously
coordinate multi-appliance scheduling from natural language requests to device
control, achieving optimal scheduling without example demonstrations. A
hierarchical architecture combining one orchestrator with three specialist
agents uses the ReAct pattern for iterative reasoning, enabling dynamic
coordination without hardcoded workflows while integrating Google Calendar for
context-aware deadline extraction. Evaluation across three open-source models
using real Austrian day-ahead electricity prices reveals substantial capability
differences. Llama-3.3-70B successfully coordinates all appliances across all
scenarios to match cost-optimal benchmarks computed via mixed-integer linear
programming, while other models achieve perfect single-appliance performance
but struggle to coordinate all appliances simultaneously. Progressive prompt
engineering experiments demonstrate that analytical query handling without
explicit guidance remains unreliable despite models' general reasoning
capabilities. We open-source the complete system including orchestration logic,
agent prompts, tools, and web interfaces to enable reproducibility, extension,
and future research.

</details>


### [85] [Normative Reasoning in Large Language Models: A Comparative Benchmark from Logical and Modal Perspectives](https://arxiv.org/abs/2510.26606)
*Kentaro Ozeki,Risako Ando,Takanobu Morishita,Hirohiko Abe,Koji Mineshima,Mitsuhiro Okada*

Main category: cs.AI

TL;DR: 本文系统评估了大型语言模型在规范性推理方面的能力，发现虽然LLMs总体上遵循有效推理模式，但在特定类型的规范性推理中存在不一致性，并表现出类似人类推理的认知偏差。


<details>
  <summary>Details</summary>
Motivation: 规范性推理涉及义务和许可等模态概念，虽然LLMs在各种推理任务中表现出色，但其处理规范性推理的能力尚未得到充分探索。

Method: 通过比较LLMs在规范性模态和认知模态（具有共同形式结构）上的推理表现，引入了一个涵盖广泛形式推理模式的新数据集，并纳入了影响人类推理的非形式认知因素。

Result: LLMs总体上遵循有效推理模式，但在特定类型的规范性推理中表现出显著的不一致性，并显示出与人类推理研究中观察到的类似认知偏差。

Conclusion: 这些发现突显了在LLMs的规范性推理中实现逻辑一致性所面临的挑战，为增强其可靠性提供了见解。

Abstract: Normative reasoning is a type of reasoning that involves normative or deontic
modality, such as obligation and permission. While large language models (LLMs)
have demonstrated remarkable performance across various reasoning tasks, their
ability to handle normative reasoning remains underexplored. In this paper, we
systematically evaluate LLMs' reasoning capabilities in the normative domain
from both logical and modal perspectives. Specifically, to assess how well LLMs
reason with normative modals, we make a comparison between their reasoning with
normative modals and their reasoning with epistemic modals, which share a
common formal structure. To this end, we introduce a new dataset covering a
wide range of formal patterns of reasoning in both normative and epistemic
domains, while also incorporating non-formal cognitive factors that influence
human reasoning. Our results indicate that, although LLMs generally adhere to
valid reasoning patterns, they exhibit notable inconsistencies in specific
types of normative reasoning and display cognitive biases similar to those
observed in psychological studies of human reasoning. These findings highlight
challenges in achieving logical consistency in LLMs' normative reasoning and
provide insights for enhancing their reliability. All data and code are
released publicly at https://github.com/kmineshima/NeuBAROCO.

</details>


### [86] [The Era of Agentic Organization: Learning to Organize with Language Models](https://arxiv.org/abs/2510.26658)
*Zewen Chi,Li Dong,Qingxiu Dong,Yaru Hao,Xun Wu,Shaohan Huang,Furu Wei*

Main category: cs.AI

TL;DR: AsyncThink是一种新的LLM推理范式，通过异步思考组织内部思维过程为可并发执行的结构，使用组织者动态分配子查询、合并中间知识，并通过强化学习优化思维结构，显著降低推理延迟并提升数学推理准确性。


<details>
  <summary>Details</summary>
Motivation: 实现智能体组织的新时代，让智能体通过协作和并发解决复杂问题，超越个体智能的局限。

Method: 提出异步思考协议，组织者动态分配子查询给工作者，合并中间知识生成连贯解决方案，并通过强化学习优化思维结构。

Result: 相比并行思考，AsyncThink推理延迟降低28%，数学推理准确性提升，且无需额外训练即可泛化到未见任务。

Conclusion: AsyncThink通过异步思考范式有效实现了智能体协作推理，在性能和泛化能力方面均表现出色。

Abstract: We envision a new era of AI, termed agentic organization, where agents solve
complex problems by working collaboratively and concurrently, enabling outcomes
beyond individual intelligence. To realize this vision, we introduce
asynchronous thinking (AsyncThink) as a new paradigm of reasoning with large
language models, which organizes the internal thinking process into
concurrently executable structures. Specifically, we propose a thinking
protocol where an organizer dynamically assigns sub-queries to workers, merges
intermediate knowledge, and produces coherent solutions. More importantly, the
thinking structure in this protocol can be further optimized through
reinforcement learning. Experiments demonstrate that AsyncThink achieves 28%
lower inference latency compared to parallel thinking while improving accuracy
on mathematical reasoning. Moreover, AsyncThink generalizes its learned
asynchronous thinking capabilities, effectively tackling unseen tasks without
additional training.

</details>


### [87] [Delegated Authorization for Agents Constrained to Semantic Task-to-Scope Matching](https://arxiv.org/abs/2510.26702)
*Majed El Helou,Chiara Troiani,Benjamin Ryder,Jean Diaconu,Hervé Muyal,Marcelo Yannuzzi*

Main category: cs.AI

TL;DR: 本文提出了一个委托授权模型，通过语义检查访问请求来限制AI代理的权限范围，并创建了ASTRA数据集来评估任务与权限范围的语义匹配效果。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型驱动的代理在调用工具和访问受保护资源时存在权限过大的风险，现有授权方法授予的权限过于宽泛，可能导致代理超出预期任务范围操作。

Method: 引入委托授权模型，允许授权服务器语义检查对受保护资源的访问请求，并发放仅限于代理完成任务所需最小权限范围的访问令牌。创建ASTRA数据集和生成管道来评估任务与权限范围的语义匹配。

Result: 实验显示基于模型的匹配方法具有潜力但也存在局限性，特别是当完成任务所需的权限范围数量增加时。

Conclusion: 需要进一步研究语义匹配技术，以实现多代理和工具增强应用的意图感知授权，包括细粒度控制如基于任务的访问控制(TBAC)。

Abstract: Authorizing Large Language Model driven agents to dynamically invoke tools
and access protected resources introduces significant risks, since current
methods for delegating authorization grant overly broad permissions and give
access to tools allowing agents to operate beyond the intended task scope. We
introduce and assess a delegated authorization model enabling authorization
servers to semantically inspect access requests to protected resources, and
issue access tokens constrained to the minimal set of scopes necessary for the
agents' assigned tasks. Given the unavailability of datasets centered on
delegated authorization flows, particularly including both semantically
appropriate and inappropriate scope requests for a given task, we introduce
ASTRA, a dataset and data generation pipeline for benchmarking semantic
matching between tasks and scopes. Our experiments show both the potential and
current limitations of model-based matching, particularly as the number of
scopes needed for task completion increases. Our results highlight the need for
further research into semantic matching techniques enabling intent-aware
authorization for multi-agent and tool-augmented applications, including
fine-grained control, such as Task-Based Access Control (TBAC).

</details>


### [88] [Unveiling Intrinsic Text Bias in Multimodal Large Language Models through Attention Key-Space Analysis](https://arxiv.org/abs/2510.26721)
*Xinhan Zheng,Huyu Wu,Xueting Wang,Haiyun Jiang*

Main category: cs.AI

TL;DR: 研究发现多模态大语言模型存在文本偏见，这种偏见源于模型内部架构问题——视觉键向量在注意力空间中与文本键向量分布不一致，导致视觉信息在注意力计算中被低估。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在处理视觉-语言数据时表现出明显的文本偏好，限制了其基于视觉证据进行有效推理的能力。与之前将文本偏见归因于数据不平衡或指令调优的研究不同，本研究认为偏见源于模型内部架构。

Method: 从LLaVA和Qwen2.5-VL模型中提取键向量，使用t-SNE进行定性分析和Jensen-Shannon散度进行定量分析，研究视觉和文本键向量在注意力空间中的分布结构。

Result: 视觉和文本键向量在注意力空间中占据明显不同的子空间，模态间差异在统计上显著，超过模态内变异的几个数量级。

Conclusion: 文本偏见源于注意力键空间内部的内在错位，而不仅仅是外部数据因素导致的。

Abstract: Multimodal large language models (MLLMs) exhibit a pronounced preference for
textual inputs when processing vision-language data, limiting their ability to
reason effectively from visual evidence. Unlike prior studies that attribute
this text bias to external factors such as data imbalance or instruction
tuning, we propose that the bias originates from the model's internal
architecture. Specifically, we hypothesize that visual key vectors (Visual
Keys) are out-of-distribution (OOD) relative to the text key space learned
during language-only pretraining. Consequently, these visual keys receive
systematically lower similarity scores during attention computation, leading to
their under-utilization in the context representation. To validate this
hypothesis, we extract key vectors from LLaVA and Qwen2.5-VL and analyze their
distributional structures using qualitative (t-SNE) and quantitative
(Jensen-Shannon divergence) methods. The results provide direct evidence that
visual and textual keys occupy markedly distinct subspaces within the attention
space. The inter-modal divergence is statistically significant, exceeding
intra-modal variation by several orders of magnitude. These findings reveal
that text bias arises from an intrinsic misalignment within the attention key
space rather than solely from external data factors.

</details>


### [89] [Cross-Platform Evaluation of Reasoning Capabilities in Foundation Models](https://arxiv.org/abs/2510.26732)
*J. de Curtò,I. de Zarzà,Pablo García,Jordi Cabot*

Main category: cs.AI

TL;DR: 本文对当代基础模型的推理能力进行了跨平台评估，建立了基础设施无关的基准测试，涵盖HPC超级计算、云平台和大学集群三种计算范式。


<details>
  <summary>Details</summary>
Motivation: 挑战传统的规模扩展假设，评估不同基础设施上基础模型的推理能力，为教育、生产和研究环境中的模型选择提供实用指南。

Method: 采用三阶段实验方法：基准建立（6个模型在19个问题上）、基础设施验证（在两种平台上重复基准测试）、扩展评估（在79个问题上全面评估）。

Result: 发现训练数据质量比模型规模更重要，建立了基础设施无关的可复现性，提供了跨架构多样性的泛化能力评估。

Conclusion: 三基础设施方法和79问题基准能够纵向跟踪基础模型推理能力的发展，为模型选择提供了实用指导。

Abstract: This paper presents a comprehensive cross-platform evaluation of reasoning
capabilities in contemporary foundation models, establishing an
infrastructure-agnostic benchmark across three computational paradigms: HPC
supercomputing (MareNostrum 5), cloud platforms (Nebius AI Studio), and
university clusters (a node with eight H200 GPUs).
  We evaluate 15 foundation models across 79 problems spanning eight academic
domains (Physics, Mathematics, Chemistry, Economics, Biology, Statistics,
Calculus, and Optimization) through three experimental phases: (1) Baseline
establishment: Six models (Mixtral-8x7B, Phi-3, LLaMA 3.1-8B, Gemma-2-9b,
Mistral-7B, OLMo-7B) evaluated on 19 problems using MareNostrum 5, establishing
methodology and reference performance; (2) Infrastructure validation: The
19-problem benchmark repeated on university cluster (seven models including
Falcon-Mamba state-space architecture) and Nebius AI Studio (nine
state-of-the-art models: Hermes-4 70B/405B, LLaMA 3.1-405B/3.3-70B, Qwen3
30B/235B, DeepSeek-R1, GPT-OSS 20B/120B) to confirm infrastructure-agnostic
reproducibility; (3) Extended evaluation: Full 79-problem assessment on both
university cluster and Nebius platforms, probing generalization at scale across
architectural diversity.
  The findings challenge conventional scaling assumptions, establish training
data quality as more critical than model size, and provide actionable
guidelines for model selection across educational, production, and research
contexts. The tri-infrastructure methodology and 79-problem benchmark enable
longitudinal tracking of reasoning capabilities as foundation models evolve.

</details>


### [90] [The Oversight Game: Learning to Cooperatively Balance an AI Agent's Safety and Autonomy](https://arxiv.org/abs/2510.26752)
*William Overman,Mohsen Bayati*

Main category: cs.AI

TL;DR: 本文研究了一种最小控制接口，让智能体在自主行动和请求人类干预之间选择，人类则决定是否信任或监督，形成马尔可夫博弈框架，在特定条件下可保证智能体自主性提升不会损害人类价值。


<details>
  <summary>Details</summary>
Motivation: 随着智能体能力增强，如何在保持底层系统不变的前提下维持有意义的人类控制成为一个核心安全问题。

Method: 将人机交互建模为两人马尔可夫博弈，在马尔可夫势博弈框架下分析，通过独立学习让智能体学会在不确定时请求帮助，人类学会适时监督。

Result: 网格世界模拟显示，通过独立学习，智能体和人类能发现最优监督角色，智能体学会在不确定时询问，人类学会适时监督，形成避免安全违规的协作。

Conclusion: 该模型提供了一种在部署后使未对齐模型更安全的实用方法，为特定形式的内在对齐提供了理论条件。

Abstract: As increasingly capable agents are deployed, a central safety question is how
to retain meaningful human control without modifying the underlying system. We
study a minimal control interface where an agent chooses whether to act
autonomously (play) or defer (ask), while a human simultaneously chooses
whether to be permissive (trust) or to engage in oversight (oversee). If the
agent defers, the human's choice determines the outcome, potentially leading to
a corrective action or a system shutdown. We model this interaction as a
two-player Markov Game. Our analysis focuses on cases where this game qualifies
as a Markov Potential Game (MPG), a class of games where we can provide an
alignment guarantee: under a structural assumption on the human's value
function, any decision by the agent to act more autonomously that benefits
itself cannot harm the human's value. We also analyze extensions to this MPG
framework. Theoretically, this perspective provides conditions for a specific
form of intrinsic alignment. If the reward structures of the human-agent game
meet these conditions, we have a formal guarantee that the agent improving its
own outcome will not harm the human's. Practically, this model motivates a
transparent control layer with predictable incentives where the agent learns to
defer when risky and act when safe, while its pretrained policy and the
environment's reward structure remain untouched. Our gridworld simulation shows
that through independent learning, the agent and human discover their optimal
oversight roles. The agent learns to ask when uncertain and the human learns
when to oversee, leading to an emergent collaboration that avoids safety
violations introduced post-training. This demonstrates a practical method for
making misaligned models safer after deployment.

</details>


### [91] [LLMs Process Lists With General Filter Heads](https://arxiv.org/abs/2510.26784)
*Arnab Sen Sharma,Giordano Rogers,Natalie Shapira,David Bau*

Main category: cs.AI

TL;DR: LLMs学习到了类似函数式编程中'filter'操作的紧凑因果表示，通过少量注意力头编码过滤谓词，该表示具有通用性和可移植性，并能应用于不同格式、语言或任务中。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在列表处理任务中的工作机制，探索其是否学习到了抽象的计算操作表示。

Method: 使用因果中介分析在多样化的列表处理任务上，识别出编码过滤谓词的注意力头（称为filter heads），并验证其表示的通用性。

Result: 发现LLMs确实学习到了紧凑的过滤谓词表示，该表示具有通用性和可移植性；同时识别出另一种过滤策略：急切评估项目是否满足谓词并将结果作为标志存储在项目表示中。

Conclusion: Transformer LMs能够发展出人类可解释的抽象计算操作实现，其泛化方式与传统函数式编程模式中的策略惊人相似。

Abstract: We investigate the mechanisms underlying a range of list-processing tasks in
LLMs, and we find that LLMs have learned to encode a compact, causal
representation of a general filtering operation that mirrors the generic
"filter" function of functional programming. Using causal mediation analysis on
a diverse set of list-processing tasks, we find that a small number of
attention heads, which we dub filter heads, encode a compact representation of
the filtering predicate in their query states at certain tokens. We demonstrate
that this predicate representation is general and portable: it can be extracted
and reapplied to execute the same filtering operation on different collections,
presented in different formats, languages, or even in tasks. However, we also
identify situations where transformer LMs can exploit a different strategy for
filtering: eagerly evaluating if an item satisfies the predicate and storing
this intermediate result as a flag directly in the item representations. Our
results reveal that transformer LMs can develop human-interpretable
implementations of abstract computational operations that generalize in ways
that are surprisingly similar to strategies used in traditional functional
programming patterns.

</details>
