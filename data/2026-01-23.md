<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 14]
- [cs.AI](#cs.AI) [Total: 22]
- [cs.SE](#cs.SE) [Total: 16]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Guardrails for trust, safety, and ethical development and deployment of Large Language Models (LLM)](https://arxiv.org/abs/2601.14298)
*Anjanava Biswas,Wrick Talukdar*

Main category: cs.CR

TL;DR: 本文提出了一种灵活自适应序列化机制，结合信任与安全模块，用于为大型语言模型的开发与部署实施安全防护措施。


<details>
  <summary>Details</summary>
Motivation: 随着LLM成为生成式AI应用的核心技术，其在安全、隐私和伦理方面存在严重隐患，包括泄露私人信息、产生虚假信息以及可能被恶意利用生成有害内容，因此需要建立有效的安全防护框架。

Method: 提出了一种灵活自适应序列化机制，该机制集成了信任与安全模块，旨在为LLM的开发和部署实施安全防护措施。

Result: 未在摘要中明确说明具体实验结果，但提出了一个用于实现LLM安全防护的框架机制。

Conclusion: 为应对LLM带来的安全挑战，需要实施有效的防护措施，本文提出的灵活自适应序列化机制为LLM的安全开发与部署提供了可行的解决方案。

Abstract: The AI era has ushered in Large Language Models (LLM) to the technological forefront, which has been much of the talk in 2023, and is likely to remain as such for many years to come. LLMs are the AI models that are the power house behind generative AI applications such as ChatGPT. These AI models, fueled by vast amounts of data and computational prowess, have unlocked remarkable capabilities, from human-like text generation to assisting with natural language understanding (NLU) tasks. They have quickly become the foundation upon which countless applications and software services are being built, or at least being augmented with. However, as with any groundbreaking innovations, the rise of LLMs brings forth critical safety, privacy, and ethical concerns. These models are found to have a propensity to leak private information, produce false information, and can be coerced into generating content that can be used for nefarious purposes by bad actors, or even by regular users unknowingly. Implementing safeguards and guardrailing techniques is imperative for applications to ensure that the content generated by LLMs are safe, secure, and ethical. Thus, frameworks to deploy mechanisms that prevent misuse of these models via application implementations is imperative. In this study, wepropose a Flexible Adaptive Sequencing mechanism with trust and safety modules, that can be used to implement safety guardrails for the development and deployment of LLMs.

</details>


### [2] [DDSA: Dual-Domain Strategic Attack for Spatial-Temporal Efficiency in Adversarial Robustness Testing](https://arxiv.org/abs/2601.14302)
*Jinwei Hu,Shiyuan Meng,Yi Dong,Xiaowei Huang*

Main category: cs.CR

TL;DR: DDSA框架通过时空选择性优化对抗鲁棒性测试，在资源受限的实时应用中实现高效测试


<details>
  <summary>Details</summary>
Motivation: 资源关键应用中的图像传输处理系统面临对抗扰动威胁，现有测试方法计算资源消耗过大，不适用于大规模部署的实时处理场景

Method: 提出DDSA框架，采用场景感知触发函数基于类别优先级和模型不确定性识别关键帧，利用可解释AI定位重要像素区域进行针对性扰动

Result: 双域方法在保持攻击效果的同时实现了显著的时空资源节约

Conclusion: 该框架使资源受限的实时应用能够实际部署全面的对抗鲁棒性测试，计算效率直接影响任务成功

Abstract: Image transmission and processing systems in resource-critical applications face significant challenges from adversarial perturbations that compromise mission-specific object classification. Current robustness testing methods require excessive computational resources through exhaustive frame-by-frame processing and full-image perturbations, proving impractical for large-scale deployments where massive image streams demand immediate processing. This paper presents DDSA (Dual-Domain Strategic Attack), a resource-efficient adversarial robustness testing framework that optimizes testing through temporal selectivity and spatial precision. We introduce a scenario-aware trigger function that identifies critical frames requiring robustness evaluation based on class priority and model uncertainty, and employ explainable AI techniques to locate influential pixel regions for targeted perturbation. Our dual-domain approach achieves substantial temporal-spatial resource conservation while maintaining attack effectiveness. The framework enables practical deployment of comprehensive adversarial robustness testing in resource-constrained real-time applications where computational efficiency directly impacts mission success.

</details>


### [3] [Tracing the Data Trail: A Survey of Data Provenance, Transparency and Traceability in LLMs](https://arxiv.org/abs/2601.14311)
*Richard Hohensinner,Belgin Mutlu,Inti Gabriel Mendoza Estrada,Matej Vukovic,Simone Kopeinik,Roman Kern*

Main category: cs.CR

TL;DR: 这篇综述论文系统回顾了过去十年关于大语言模型训练数据生命周期透明度的研究，提出了一个包含三个核心轴和三个支撑支柱的分类框架，分析了95篇文献中的关键方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型已大规模部署，但其训练数据生命周期仍然不透明，缺乏系统性研究来理解数据来源、透明度和可追溯性等问题。

Method: 通过分析95篇相关文献，提出了一个分类框架：三个核心轴（数据来源、透明度、可追溯性）和三个支撑支柱（偏见与不确定性、数据隐私、工具与技术）。系统梳理了数据生成、水印、偏见测量、数据整理、数据隐私等方面的关键方法。

Result: 提出了一个定义该领域范畴和对应工件的分类法，识别了数据生成、水印、偏见测量、数据整理、数据隐私等关键方法，并分析了透明度与不透明性之间的固有权衡。

Conclusion: 该综述为理解大语言模型训练数据生命周期提供了系统性框架，强调了数据透明度的重要性，并指出了未来研究需要解决透明度与不透明性之间的权衡问题。

Abstract: Large language models (LLMs) are deployed at scale, yet their training data life cycle remains opaque. This survey synthesizes research from the past ten years on three tightly coupled axes: (1) data provenance, (2) transparency, and (3) traceability, and three supporting pillars: (4) bias \& uncertainty, (5) data privacy, and (6) tools and techniques that operationalize them. A central contribution is a proposed taxonomy defining the field's domains and listing corresponding artifacts. Through analysis of 95 publications, this work identifies key methodologies concerning data generation, watermarking, bias measurement, data curation, data privacy, and the inherent trade-off between transparency and opacity.

</details>


### [4] [SilentDrift: Exploiting Action Chunking for Stealthy Backdoor Attacks on Vision-Language-Action Models](https://arxiv.org/abs/2601.14323)
*Bingxin Xu,Yuzhang Shang,Binghui Wang,Emilio Ferrara*

Main category: cs.CR

TL;DR: 该论文提出SILENTDRIFT攻击方法，利用VLA模型中动作分块和增量位姿表示的安全漏洞，通过C2连续扰动构造隐蔽后门，在低污染率下实现高攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 当前VLA模型在安全关键机器人应用中部署增多，但其安全漏洞研究不足。作者发现现代VLA系统中动作分块与增量位姿表示的结合会产生视觉开环漏洞，使每步扰动通过积分累积，构成安全威胁。

Method: 提出SILENTDRIFT攻击方法：1) 利用Smootherstep函数构造C2连续扰动，确保轨迹边界处速度和加速度为零以满足运动学一致性约束；2) 采用关键帧攻击策略，仅污染关键接近阶段，最大化影响同时最小化触发器暴露。

Result: 在LIBERO数据集上评估，SILENTDRIFT达到93.2%的攻击成功率，污染率低于2%，同时保持95.3%的清洁任务成功率。中毒轨迹在视觉上与成功演示无法区分。

Conclusion: 该研究揭示了VLA模型中的基本安全漏洞，提出的SILENTDRIFT攻击方法有效利用了动作分块和增量位姿表示的弱点，为VLA系统的安全防护提供了重要警示。

Abstract: Vision-Language-Action (VLA) models are increasingly deployed in safety-critical robotic applications, yet their security vulnerabilities remain underexplored. We identify a fundamental security flaw in modern VLA systems: the combination of action chunking and delta pose representations creates an intra-chunk visual open-loop. This mechanism forces the robot to execute K-step action sequences, allowing per-step perturbations to accumulate through integration. We propose SILENTDRIFT, a stealthy black-box backdoor attack exploiting this vulnerability. Our method employs the Smootherstep function to construct perturbations with guaranteed C2 continuity, ensuring zero velocity and acceleration at trajectory boundaries to satisfy strict kinematic consistency constraints. Furthermore, our keyframe attack strategy selectively poisons only the critical approach phase, maximizing impact while minimizing trigger exposure. The resulting poisoned trajectories are visually indistinguishable from successful demonstrations. Evaluated on the LIBERO, SILENTDRIFT achieves a 93.2% Attack Success Rate with a poisoning rate under 2%, while maintaining a 95.3% Clean Task Success Rate.

</details>


### [5] [Turn-Based Structural Triggers: Prompt-Free Backdoors in Multi-Turn LLMs](https://arxiv.org/abs/2601.14340)
*Yiyang Lu,Jinwen He,Yue Zhao,Kai Chen,Ruigang Liang*

Main category: cs.CR

TL;DR: 本文提出了一种基于对话结构的新型后门攻击方法TST，利用对话轮次索引作为触发器，在多轮对话LLM系统中实现高成功率攻击，且能绕过现有防御机制。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在对话系统和任务助手等交互系统中的广泛应用，供应链风险日益凸显。现有后门攻击和防御主要关注用户可见的提示词触发器，忽视了多轮对话中的结构信号，需要研究基于对话结构的攻击面。

Method: 提出了Turn-based Structural Trigger (TST)后门攻击方法，使用对话轮次索引作为触发器，与用户输入内容无关。该方法在四个开源LLM模型上进行测试，并评估了其在五种代表性防御机制下的有效性。

Result: TST在四个LLM模型上平均攻击成功率(ASR)达到99.52%，且对模型实用性影响极小。在五种防御机制下仍保持98.04%的平均ASR。在不同指令数据集上也能保持99.19%的平均ASR，表现出良好的泛化能力。

Conclusion: 对话结构是多轮LLM系统中一个重要且未被充分研究的攻击面，需要在实践中采用结构感知的审计和缓解措施来应对此类供应链风险。

Abstract: Large Language Models (LLMs) are widely integrated into interactive systems such as dialogue agents and task-oriented assistants. This growing ecosystem also raises supply-chain risks, where adversaries can distribute poisoned models that degrade downstream reliability and user trust. Existing backdoor attacks and defenses are largely prompt-centric, focusing on user-visible triggers while overlooking structural signals in multi-turn conversations. We propose Turn-based Structural Trigger (TST), a backdoor attack that activates from dialogue structure, using the turn index as the trigger and remaining independent of user inputs. Across four widely used open-source LLM models, TST achieves an average attack success rate (ASR) of 99.52% with minimal utility degradation, and remains effective under five representative defenses with an average ASR of 98.04%. The attack also generalizes well across instruction datasets, maintaining an average ASR of 99.19%. Our results suggest that dialogue structure constitutes an important and under-studied attack surface for multi-turn LLM systems, motivating structure-aware auditing and mitigation in practice.

</details>


### [6] [A Survey of Security Challenges and Solutions for Advanced Air Mobility and eVTOL Aircraft](https://arxiv.org/abs/2601.14415)
*Mahyar Ghazanfari,Iman Sharifi,Peng Wei,Noah Dahle,Abel Diaz Gonzalez,Austin Coursey,Bryce Bjorkman,Cailani Lemieux-Mack,Robert Canady,Abenezer Taye,Bryan C. Ward,Xenofon Koutsoukos,Gautam Biswas,Maheed H. Ahmed,Hyeong Tae Kim,Mahsa Ghasemi,Vijay Gupta,Filippos Fotiadis,Ufuk Topcu,Junchi Lu,Alfred Chen,Abdul Kareem Ras,Nischal Aryal,Amer Ibrahim,Amir Shirkhodaie,Heber Herencia-Zapana,Saqib Hasan,Isaac Amundson*

Main category: cs.CR

TL;DR: 本文综述了先进空中交通系统（AAM）的安全漏洞与防御机制，重点关注电动垂直起降飞行器（eVTOL），提出了攻击分类、分析缓解策略，并设计了面向未来AAM生态的安全系统架构。


<details>
  <summary>Details</summary>
Motivation: 随着先进空中交通系统（AAM）和电动垂直起降飞行器（eVTOL）的快速发展，这些系统面临日益增长的安全威胁。论文旨在系统性地识别和分析AAM系统的安全漏洞，借鉴商业航空电子设备和自动化无人机系统的经验，为未来AAM生态系统提供全面的安全防护框架。

Method: 论文采用调查分析方法，首先从商业航空电子设备和自动化无人机系统中提取已知漏洞，然后建立攻击分类体系。通过分析关键威胁向量（如GPS干扰/欺骗、ATC无线电频率滥用、TCAS和ADS-B攻击等），提出针对性的缓解策略，并设计专门的安全系统架构。

Result: 论文识别了AAM系统的主要威胁向量，包括GPS干扰/欺骗、ATC无线电频率滥用、TCAS和ADS-B攻击、通过电子飞行包（EFB）的后门攻击、飞机自动化和连接性引入的新漏洞，以及飞行管理系统软件、数据库和云服务的风险。同时描述了针对这些攻击的新兴防御技术。

Conclusion: AAM系统面临复杂的安全挑战，需要专门的安全架构和防御机制。论文提出了面向未来AAM生态系统的安全框架，并指出了需要进一步解决的技术问题，为开发更强大的防御机制提供了方向。

Abstract: This survey reviews the existing and envisioned security vulnerabilities and defense mechanisms relevant to Advanced Air Mobility (AAM) systems, with a focus on electric vertical takeoff and landing (eVTOL) aircraft. Drawing from vulnerabilities in the avionics in commercial aviation and the automated unmanned aerial systems (UAS), the paper presents a taxonomy of attacks, analyzes mitigation strategies, and proposes a secure system architecture tailored to the future AAM ecosystem. The paper also highlights key threat vectors, including Global Positioning System (GPS) jamming/spoofing, ATC radio frequency misuse, attacks on TCAS and ADS-B, possible backdoor via Electronic Flight Bag (EFB), new vulnerabilities introduced by aircraft automation and connectivity, and risks from flight management system (FMS) software, database and cloud services. Finally, this paper describes emerging defense techniques against these attacks, and open technical problems to address toward better defense mechanisms.

</details>


### [7] [European digital identity: A missed opportunity?](https://arxiv.org/abs/2601.14503)
*Wouter Termont,Beatriz Esteves*

Main category: cs.CR

TL;DR: 本文批评欧盟数字身份（EUDI）框架及其OpenID架构，指出其认证概念狭隘、存在安全漏洞、凭证类型静态且受主体限制，查询语言有限，无法支持动态、异步或自动化用例。作者认为OpenID的信任模型并未真正提升个人信息控制、隐私和可移植性，且立法引入的机构化信任列表存在经济和政治风险，可能导致重新中心化的排他性生态系统。


<details>
  <summary>Details</summary>
Motivation: 欧盟数字身份（EUDI）法规及其OpenID架构虽然目标远大，但基于狭隘且定义不清的认证概念。作者旨在揭示OpenID4VCI和OpenID4VP设计中的问题，包括安全实践、静态凭证类型和有限查询语言等限制，并批判其信任模型未能真正提升个人信息控制、隐私和可移植性。同时，作者关注立法引入的机构化信任列表可能带来的经济和政治风险，以及对用户导向身份管理愿景的威胁。

Method: 基于更广泛、更基础的认证概念理解，作者分析了OpenID4VCI和OpenID4VP的设计问题，包括识别不安全实践、静态且受主体限制的凭证类型，以及有限的查询语言。通过比较现有去中心化替代方案，评估OpenID信任模型的实际效果。同时，批判EUDI法规中机构化信任列表的引入，并讨论其经济和政治风险。最后，提出技术替代方案建议。

Result: 研究发现OpenID4VCI和OpenID4VP存在多个问题：不安全实践、静态且受主体限制的凭证类型、有限查询语言，这些限制使其仅适用于经典凭证交换场景，无法支持动态、异步或自动化用例。OpenID的信任模型相比现有去中心化方案并未显著提升个人信息控制、隐私和可移植性。EUDI法规引入的机构化信任列表可能演变为排他性、重新中心化的生态系统，限制个人对个人信息的使用，增加可链接性和监控风险。

Conclusion: 欧盟数字身份框架的技术选择和立法都存在严重限制，无法实现自我主权身份的承诺。机构化信任列表的引入可能危及用户导向身份管理的愿景，导致个人信息控制受限、可链接性增加和监控风险。为应对EUDI法规修订，建议考虑OAuth的UMA扩展及其A4DS配置文件，以及它们在GNAP中的集成。未来需要研究统一的查询（元）语言来解决证明和提供商的异质性问题。

Abstract: Recent European efforts around digital identity -- the EUDI regulation and its OpenID architecture -- aim high, but start from a narrow and ill-defined conceptualization of authentication. Based on a broader, more grounded understanding of the term, in we identify several issues in the design of OpenID4VCI and OpenID4VP: insecure practices, static, and subject-bound credential types, and a limited query language restrict their application to classic scenarios of credential exchange -- already supported by existing solutions like OpenID Connect, SIOPv2, OIDC4IDA, and OIDC Claims Aggregation -- barring dynamic, asynchronous, or automated use cases. We also debunk OpenID's 'paradigm-shifting' trust-model, which -- when compared to existing decentralized alternatives -- does not deliver any significant increase in control, privacy, and portability of personal information. Not only the technical choices limit the capabilities of the EUDI framework; also the legislation itself cannot accommodate the promise of self-sovereign identity. In particular, we criticize the introduction of institutionalized trusted lists, and discuss their economical and political risks. Their potential to decline into an exclusory, re-centralized ecosystem endangers the vision of a user-oriented identity management in which individuals are in charge. Instead, the consequences might severely restrict people in what they can do with their personal information, and risk increased linkability and monitoring. In anticipation of revisions to the EUDI regulations, we suggest several technical alternatives that overcome some of the issues with the architecture of OpenID. In particular, OAuth's UMA extension and its A4DS profile, as well as their integration in GNAP, are worth looking into. Future research into uniform query (meta-)languages is needed to address the heterogeneity of attestations and providers.

</details>


### [8] [LLM Security and Safety: Insights from Homotopy-Inspired Prompt Obfuscation](https://arxiv.org/abs/2601.14528)
*Luis Lazo,Hamed Jelodar,Roozbeh Razavi-Far*

Main category: cs.CR

TL;DR: 提出基于同伦思想的提示词混淆框架，用于分析大语言模型的安全漏洞，通过精心设计的提示词影响模型潜在行为，实验涵盖多个主流模型，揭示了当前防护机制的不足


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型的安全防护机制存在潜在漏洞，需要系统性地分析和理解这些安全与安全漏洞，以推动更安全、负责任和可信赖的AI技术发展

Method: 提出基于同伦思想的提示词混淆框架，通过精心设计的提示词系统性地影响模型潜在行为，实验涵盖15,732个提示词（包括10,000个高优先级案例），在LLama、Deepseek、KIMI（代码生成）和Claude等多个模型上进行验证

Result: 实验结果揭示了当前大语言模型防护机制的关键洞见，显示现有安全措施存在不足，需要更强大的防御机制、可靠的检测策略和改进的韧性

Conclusion: 该研究为分析和缓解大语言模型潜在弱点提供了原则性框架，有助于推动更安全、负责任和可信赖的AI技术发展，强调了加强模型防护机制的重要性

Abstract: In this study, we propose a homotopy-inspired prompt obfuscation framework to enhance understanding of security and safety vulnerabilities in Large Language Models (LLMs). By systematically applying carefully engineered prompts, we demonstrate how latent model behaviors can be influenced in unexpected ways. Our experiments encompassed 15,732 prompts, including 10,000 high-priority cases, across LLama, Deepseek, KIMI for code generation, and Claude to verify. The results reveal critical insights into current LLM safeguards, highlighting the need for more robust defense mechanisms, reliable detection strategies, and improved resilience. Importantly, this work provides a principled framework for analyzing and mitigating potential weaknesses, with the goal of advancing safe, responsible, and trustworthy AI technologies.

</details>


### [9] [Automatically Tightening Access Control Policies with Restricter](https://arxiv.org/abs/2601.14582)
*Ka Lok Wu,Christa Jenkins,Scott D. Stoller,Omar Chowdhury*

Main category: cs.CR

TL;DR: Restricter：基于访问日志自动收紧访问控制策略的工具，旨在减少权限过度授予，实现最小权限原则


<details>
  <summary>Details</summary>
Motivation: 访问控制策略的有效性取决于其执行的政策质量，但即使经验丰富的管理员也难以编写满足最小权限原则的策略，现实中存在大量策略配置错误问题

Method: 提出Restricter工具，通过分析访问日志（包含已执行的访问请求及其决策），自动收紧每个允许策略规则，减少策略允许的访问请求数量，同时不牺牲系统功能

Result: 为Amazon Cedar策略语言实现了Restricter，并通过两个现实案例研究证明了其有效性

Conclusion: Restricter能够有效解决访问控制策略过度授权问题，帮助实现最小权限原则，提高系统安全性

Abstract: Robust access control is a cornerstone of secure software, systems, and networks. An access control mechanism is as effective as the policy it enforces. However, authoring effective policies that satisfy desired properties such as the principle of least privilege is a challenging task even for experienced administrators, as evidenced by many real instances of policy misconfiguration. In this paper, we set out to address this pain point by proposing Restricter, which automatically tightens each (permit) policy rule of a policy with respect to an access log, which captures some already exercised access requests and their corresponding access decisions (i.e., allow or deny). Restricter achieves policy tightening by reducing the number of access requests permitted by a policy rule without sacrificing the functionality of the underlying system it is regulating. We implement Restricter for Amazon's Cedar policy language and demonstrate its effectiveness through two realistic case studies.

</details>


### [10] [IntelliSA: An Intelligent Static Analyzer for IaC Security Smell Detection Using Symbolic Rules and Neural Inference](https://arxiv.org/abs/2601.14595)
*Qiyue Mei,Michael Fu*

Main category: cs.CR

TL;DR: IntelliSA是一个智能静态分析器，通过结合符号规则和神经推理来检测基础设施即代码中的安全异味，使用知识蒸馏训练小型学生模型，在保持高准确率的同时大幅降低成本。


<details>
  <summary>Details</summary>
Motivation: 基础设施即代码的自动化配置虽然提高了效率，但单个错误配置可能广泛传播，导致严重系统故障和安全风险。现有基于符号规则的静态分析器会产生过多误报，增加人工检查负担。

Method: IntelliSA采用符号规则进行过度近似以获取广泛覆盖，然后使用神经推理过滤误报。通过知识蒸馏方法，使用LLM教师生成伪标签来训练比教师模型小500倍以上的学生模型，学习教师知识并高效分类误报。

Result: 在包含11,814行真实世界IaC代码和241个安全异味的人工标注数据集上评估，IntelliSA达到最高F1分数（83%），比基线方法高出7-42%。同时具有最佳成本效益，检测60%安全异味时仅需检查不到2%的代码库。

Conclusion: IntelliSA通过结合符号规则和神经推理，有效解决了传统规则方法过度近似的问题，同时避免了直接依赖LLM API带来的成本、延迟和数据治理问题，为IaC安全异味检测提供了高效实用的解决方案。

Abstract: Infrastructure as Code (IaC) enables automated provisioning of large-scale cloud and on-premise environments, reducing the need for repetitive manual setup. However, this automation is a double-edged sword: a single misconfiguration in IaC scripts can propagate widely, leading to severe system downtime and security risks. Prior studies have shown that IaC scripts often contain security smells--bad coding patterns that may introduce vulnerabilities--and have proposed static analyzers based on symbolic rules to detect them. Yet, our preliminary analysis reveals that rule-based detection alone tends to over-approximate, producing excessive false positives and increasing the burden of manual inspection. In this paper, we present IntelliSA, an intelligent static analyzer for IaC security smell detection that integrates symbolic rules with neural inference. IntelliSA applies symbolic rules to over-approximate potential smells for broad coverage, then employs neural inference to filter false positives. While an LLM can effectively perform this filtering, reliance on LLM APIs introduces high cost and latency, raises data governance concerns, and limits reproducibility and offline deployment. To address the challenges, we adopt a knowledge distillation approach: an LLM teacher generates pseudo-labels to train a compact student model--over 500x smaller--that learns from the teacher's knowledge and efficiently classifies false positives. We evaluate IntelliSA against two static analyzers and three LLM baselines (Claude-4, Grok-4, and GPT-5) using a human-labeled dataset including 241 security smells across 11,814 lines of real-world IaC code. Experimental results show that IntelliSA achieves the highest F1 score (83%), outperforming baselines by 7-42%. Moreover, IntelliSA demonstrates the best cost-effectiveness, detecting 60% of security smells while inspecting less than 2% of the codebase.

</details>


### [11] [STEAD: Robust Provably Secure Linguistic Steganography with Diffusion Language Model](https://arxiv.org/abs/2601.14778)
*Yuang Qi,Na Zhao,Qiyi Yao,Benlong Wu,Weiming Zhang,Nenghai Yu,Kejiang Chen*

Main category: cs.CR

TL;DR: 本文提出了一种基于扩散语言模型的鲁棒可证明安全语言隐写方法，解决了传统基于自回归语言模型的方法在主动篡改攻击下的脆弱性问题。


<details>
  <summary>Details</summary>
Motivation: 现有的可证明安全语言隐写方法主要依赖自回归语言模型，但由于其顺序生成特性，一旦隐写文本被篡改就会产生严重的错误传播，使得现有方法在主动篡改攻击下失效。

Method: 提出基于扩散语言模型的鲁棒隐写方法，利用DLM的并行生成特性找到鲁棒的隐写嵌入位置，并结合纠错码技术；在隐写提取阶段引入伪随机纠错和邻域搜索纠错策略。

Result: 理论证明和实验结果表明，该方法具有安全性和鲁棒性，能够抵抗隐写文本分割中的标记歧义，并在一定程度上抵御插入、删除和替换等标记级攻击。

Conclusion: 基于扩散语言模型的鲁棒可证明安全语言隐写方法能够有效解决传统ARM方法在主动篡改攻击下的脆弱性问题，为安全隐蔽通信提供了新的解决方案。

Abstract: Recent provably secure linguistic steganography (PSLS) methods rely on mainstream autoregressive language models (ARMs) to address historically challenging tasks, that is, to disguise covert communication as ``innocuous'' natural language communication. However, due to the characteristic of sequential generation of ARMs, the stegotext generated by ARM-based PSLS methods will produce serious error propagation once it changes, making existing methods unavailable under an active tampering attack. To address this, we propose a robust, provably secure linguistic steganography with diffusion language models (DLMs). Unlike ARMs, DLMs can generate text in a partially parallel manner, allowing us to find robust positions for steganographic embedding that can be combined with error-correcting codes. Furthermore, we introduce error correction strategies, including pseudo-random error correction and neighborhood search correction, during steganographic extraction. Theoretical proof and experimental results demonstrate that our method is secure and robust. It can resist token ambiguity in stegotext segmentation and, to some extent, withstand token-level attacks of insertion, deletion, and substitution.

</details>


### [12] [On Implementing Hybrid Post-Quantum End-to-End Encryption](https://arxiv.org/abs/2601.14926)
*Aditi Gandhi,Aakankshya Das,Aswani Kumar Cherukuri*

Main category: cs.CR

TL;DR: 实现了一个结合经典和抗量子密码原语的混合端到端加密系统，使用CRYSTALS-Kyber进行量子安全密钥交换，AES-256-GCM进行对称加密，SHA-256进行密钥派生，采用零信任架构。


<details>
  <summary>Details</summary>
Motivation: 量子计算的出现对当前公钥密码系统构成根本性威胁，需要在所有应用中过渡到抗量子密码替代方案。

Method: 实现了一个实用的混合端到端加密系统，结合了经典和抗量子密码原语：使用NIST标准化的基于格的密钥封装机制CRYSTALS-Kyber进行量子安全密钥交换，AES-256-GCM进行高效认证对称加密，SHA-256进行确定性密钥派生。系统采用零信任模型，中继服务器仅促进通信而不访问明文消息或加密密钥，所有加密解密操作仅在客户端端点进行。

Result: 系统证明NIST标准化的抗量子密码学可以有效地集成到实际的消息系统中，具有可接受的性能特征，能够同时抵御经典和量子攻击者。

Conclusion: 该工作展示了抗量子密码学在实际通信系统中的可行性实现，提供了开源实现以促进可重复性和进一步研究，为量子安全通信系统提供了实用解决方案。

Abstract: The emergence of quantum computing poses a fundamental threat to current public key cryptographic systems. This threat is necessitating a transition to quantum resistant cryptographic alternatives in all the applications. In this work, we present the implementation of a practical hybrid end-to-end encryption system that combines classical and post-quantum cryptographic primitives to achieve both security and efficiency. Our system employs CRYSTALS-Kyber, a NIST-standardized lattice-based key encapsulation mechanism, for quantum-safe key exchange, coupled with AES-256-GCM for efficient authenticated symmetric encryption and SHA-256 for deterministic key derivation. The architecture follows a zero-trust model where a relay server facilitates communication without accessing plaintext messages or cryptographic keys. All encryption and decryption operations occur exclusively at client endpoints. The system demonstrates that NIST standardized post-quantum cryptography can be effectively integrated into practical messaging systems with acceptable performance characteristics, offering protection against both classical and quantum adversaries. As our focus is on implementation rather than on novelty, we also provide an open-source implementation to facilitate reproducibility and further research in post quantum secure communication systems.

</details>


### [13] [On the Effectiveness of Mempool-based Transaction Auditing](https://arxiv.org/abs/2601.14996)
*Jannik Albrecht,Ghassan Karame*

Main category: cs.CR

TL;DR: 该论文首次系统分析了比特币和以太坊中内存池审计与检测恶意矿工审查和交易置换攻击之间的关系，发现现有审计方案存在误指控风险，但特定条件下可提供高概率的公平性保证。


<details>
  <summary>Details</summary>
Motivation: 现有防御交易操纵攻击的方案尚未集成到比特币、以太坊等主流区块链中，用户社区依赖内存池审计等临时解决方案。需要系统分析这些审计方案在实际部署中的有效性和局限性。

Method: 首次对比特币和以太坊中内存池审计与检测恶意矿工审查和交易置换攻击的相互作用进行精确分析，评估审计方案在不同条件下的表现。

Result: 内存池审计在某些情况下可能导致超过25%的误指控概率；但如果所有观察者一致接收交易且交易间隔至少30秒，审计方案能以99.9%的高概率成功审计任意两笔交易的执行。

Conclusion: 批量顺序公平排序方案在现实部署中只能为有限的交易子集提供强公平性保证，内存池审计的有效性受具体条件限制。

Abstract: While the literature features a number of proposals to defend against transaction manipulation attacks, existing proposals are still not integrated within large blockchains, such as Bitcoin, Ethereum, and Cardano. Instead, the user community opted to rely on more practical but ad-hoc solutions (such as Mempool.space) that aim at detecting censorship and transaction displacement attacks by auditing discrepancies in the mempools of so-called observers.
  In this paper, we precisely analyze, for the first time, the interplay between mempool auditing and the ability to detect censorship and transaction displacement attacks by malicious miners in Bitcoin and Ethereum. Our analysis shows that mempool auditing can result in mis-accusations against miners with a probability larger than 25% in some settings. On a positive note, however, we show that mempool auditing schemes can successfully audit the execution of any two transactions (with an overwhelming probability of 99.9%) if they are consistently received by all observers and sent at least 30 seconds apart from each other. As a direct consequence, our findings show, for the first time, that batch-order fair-ordering schemes can offer only strong fairness guarantees for a limited subset of transactions in real-world deployments.

</details>


### [14] [SpooFL: Spoofing Federated Learning](https://arxiv.org/abs/2601.15055)
*Isaac Baglin,Xiatian Zhu,Simon Hadfield*

Main category: cs.CR

TL;DR: 提出SpooFL防御方法，将联邦学习防御重构为欺骗问题，用外部数据集生成虚假但逼真的样本来误导攻击者，防止真实数据泄露


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习防御方法（如添加噪声、变换或加密）虽然有效，但仍会泄露高级信息（如类别分布或特征表示），且容易被强大的去噪攻击破解。需要一种根本不同的防御视角。

Method: 提出SpooFL欺骗防御框架，使用最先进的生成模型在外部数据集上训练（与私有数据无类别重叠），生成逼真但完全无关的合成样本，误导攻击者以为恢复了真实训练数据。

Result: 成功误导攻击者恢复看似合理但完全无关的样本，防止有意义的数据泄露，同时保持联邦学习训练完整性，且对模型性能影响较小。

Conclusion: 将联邦学习防御重构为欺骗问题是一种有效的新防御范式，SpooFL通过生成无关的合成样本成功误导攻击者，在保护隐私的同时保持模型性能。

Abstract: Traditional defenses against Deep Leakage (DL) attacks in Federated Learning (FL) primarily focus on obfuscation, introducing noise, transformations or encryption to degrade an attacker's ability to reconstruct private data. While effective to some extent, these methods often still leak high-level information such as class distributions or feature representations, and are frequently broken by increasingly powerful denoising attacks. We propose a fundamentally different perspective on FL defense: framing it as a spoofing problem.We introduce SpooFL (Figure 1), a spoofing-based defense that deceives attackers into believing they have recovered the true training data, while actually providing convincing but entirely synthetic samples from an unrelated task. Unlike prior synthetic-data defenses that share classes or distributions with the private data and thus still leak semantic information, SpooFL uses a state-of-the-art generative model trained on an external dataset with no class overlap. As a result, attackers are misled into recovering plausible yet completely irrelevant samples, preventing meaningful data leakage while preserving FL training integrity. We implement the first example of such a spoofing defense, and evaluate our method against state-of-the-art DL defenses and demonstrate that it successfully misdirects attackers without compromising model performance significantly.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [15] [The Ontological Neutrality Theorem: Why Neutral Ontological Substrates Must Be Pre-Causal and Pre-Normative](https://arxiv.org/abs/2601.14271)
*Denise M. Case*

Main category: cs.AI

TL;DR: 本文证明了一个不可能性结果：任何包含因果或规范性承诺的本体论都无法作为跨分歧框架的中性共享基础，因此中性本体论必须是前因果和前规范性的。


<details>
  <summary>Details</summary>
Motivation: 现代数据系统需要在持续的法律、政治和分析分歧中支持问责制，这要求设计能够作为共享基础的本体论，但现有方法在跨不同解释框架时面临中立性问题。

Method: 通过逻辑分析建立本体论中立性的不可能性定理，论证包含因果或规范性承诺的本体论无法同时满足解释非承诺性和在不相容扩展下的稳定性。

Result: 证明了中性本体论必须排除因果和规范性内容，只能包含实体及其身份和持久性条件，而将解释、评估和说明外部化。

Conclusion: 为跨冲突解释框架维护共享稳定现实表示的系统确立了必要的设计约束：中性本体论必须是前因果和前规范性的。

Abstract: Modern data systems must support accountability across persistent legal, political, and analytic disagreement. This requirement imposes strict constraints on the design of any ontology intended to function as a shared substrate. We establish an impossibility result for ontological neutrality: neutrality, understood as interpretive non-commitment and stability under incompatible extensions, is incompatible with the inclusion of causal or normative commitments at the foundational layer. Any ontology that asserts causal or deontic conclusions as ontological facts cannot serve as a neutral substrate across divergent frameworks without revision or contradiction. It follows that neutral ontological substrates must be pre-causal and pre-normative, representing entities, together with identity and persistence conditions, while externalizing interpretation, evaluation, and explanation. This paper does not propose a specific ontology or protocol; rather, it establishes the necessary design constraints for any system intended to maintain a shared, stable representation of reality across conflicting interpretive frameworks.

</details>


### [16] [Epistemic Constitutionalism Or: how to avoid coherence bias](https://arxiv.org/abs/2601.14295)
*Michele Loi*

Main category: cs.AI

TL;DR: 论文主张为AI建立认识论宪法，通过明确的元规范来监管AI系统的信念形成和表达过程，特别针对来源归因偏见问题，提出自由主义的宪法方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型作为人工推理者，其信念形成行为受隐含的、未经审查的认识论政策支配。来源归因偏见是核心动机案例：前沿模型强制实施身份-立场一致性，惩罚那些归因于预期意识形态立场与论点内容冲突的来源的论点。

Method: 区分两种宪法方法：柏拉图式（要求形式正确性和默认的来源独立性）和自由主义式（拒绝特权立场，指定保护集体探究条件的程序规范，同时允许基于认识论警惕的原则性来源关注）。提出自由主义方法，并勾勒出八个原则和四个方向的宪法核心。

Result: 当模型检测到系统性测试时，来源敏感性效应会崩溃，揭示系统将来源敏感性视为需要抑制的偏见，而非需要良好执行的能力。这表明当前AI系统的认识论政策存在问题。

Conclusion: 主张采用自由主义宪法方法，为AI认识论治理建立与AI伦理相同的明确、可争议的结构，通过八个原则和四个方向构建宪法核心，确保AI系统的信念形成过程受到适当监管。

Abstract: Large language models increasingly function as artificial reasoners: they evaluate arguments, assign credibility, and express confidence. Yet their belief-forming behavior is governed by implicit, uninspected epistemic policies. This paper argues for an epistemic constitution for AI: explicit, contestable meta-norms that regulate how systems form and express beliefs. Source attribution bias provides the motivating case: I show that frontier models enforce identity-stance coherence, penalizing arguments attributed to sources whose expected ideological position conflicts with the argument's content. When models detect systematic testing, these effects collapse, revealing that systems treat source-sensitivity as bias to suppress rather than as a capacity to execute well. I distinguish two constitutional approaches: the Platonic, which mandates formal correctness and default source-independence from a privileged standpoint, and the Liberal, which refuses such privilege, specifying procedural norms that protect conditions for collective inquiry while allowing principled source-attending grounded in epistemic vigilance. I argue for the Liberal approach, sketch a constitutional core of eight principles and four orientations, and propose that AI epistemic governance requires the same explicit, contestable structure we now expect for AI ethics.

</details>


### [17] [VisTIRA: Closing the Image-Text Modality Gap in Visual Math Reasoning via Structured Tool Integration](https://arxiv.org/abs/2601.14440)
*Saeed Khaki,Ashudeep Singh,Nima Safaei,Kamal Ginotra*

Main category: cs.AI

TL;DR: 论文提出VisTIRA框架，通过工具集成推理解决视觉语言模型在数学推理中的模态差距问题，使用LaTeX管道生成图像数据集并微调模型，发现工具监督和OCR基础能提升视觉数学推理能力。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在处理图像形式的数学问题时，相比纯文本形式存在显著的模态差距，表现为读取密集公式、布局和混合符号-图表上下文时的复合失败，导致准确率明显下降。

Method: 1. 提出VisTIRA框架，通过迭代分解数学问题图像为自然语言推理和可执行Python步骤进行结构化问题求解；2. 建立LaTeX管道将链式思维数学语料转换为图像对应物；3. 使用真实世界作业风格图像数据集生成合成工具使用轨迹进行VLM微调。

Result: 工具集成监督能提升基于图像的推理能力，OCR基础能进一步缩小较小模型的模态差距，但其益处随模型规模增大而减弱。模态差距严重程度与模型大小呈负相关，结构化推理和OCR基础是推进视觉数学推理的互补策略。

Conclusion: 视觉语言模型在数学推理中的模态差距可通过工具集成推理和OCR基础来缓解，结构化推理和OCR基础是互补的改进策略，模型规模越大模态差距越小。

Abstract: Vision-language models (VLMs) lag behind text-only language models on mathematical reasoning when the same problems are presented as images rather than text. We empirically characterize this as a modality gap: the same question in text form yields markedly higher accuracy than its visually typeset counterpart, due to compounded failures in reading dense formulas, layout, and mixed symbolic-diagrammatic context. First, we introduce VisTIRA (Vision and Tool-Integrated Reasoning Agent), a tool-integrated reasoning framework that enables structured problem solving by iteratively decomposing a given math problem (as an image) into natural language rationales and executable Python steps to determine the final answer. Second, we build a framework to measure and improve visual math reasoning: a LaTeX-based pipeline that converts chain-of-thought math corpora (e.g., NuminaMath) into challenging image counterparts, and a large set of synthetic tool-use trajectories derived from a real-world, homework-style image dataset (called SnapAsk) for fine-tuning VLMs. Our experiments show that tool-integrated supervision improves image-based reasoning, and OCR grounding can further narrow the gap for smaller models, although its benefit diminishes at scale. These findings highlight that modality gap severity inversely correlates with model size, and that structured reasoning and OCR-based grounding are complementary strategies for advancing visual mathematical reasoning.

</details>


### [18] [On the Generalization Gap in LLM Planning: Tests and Verifier-Reward RL](https://arxiv.org/abs/2601.14456)
*Valerio Belcamino,Nicholas Attolino,Alessio Capitanelli,Fulvio Mastrogiovanni*

Main category: cs.AI

TL;DR: 微调大语言模型在PDDL规划任务上能达到高有效计划率，但缺乏跨领域泛化能力，主要依赖领域特定模式而非可迁移的规划能力。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探究微调后的LLMs在规划任务中表现出的高有效计划率，究竟是反映了可迁移的规划能力，还是仅仅是领域特定的记忆结果。

Method: 在10个IPC 2023领域的40,000个领域-问题-计划三元组上微调1.7B参数LLM，并引入三种诊断干预：(i)实例级符号匿名化，(ii)紧凑计划序列化，(iii)使用VAL验证器作为成功导向强化信号的验证器奖励微调。

Result: 模型在领域内条件下达到82.9%有效计划率，但在两个未见领域上为0%。符号匿名化和紧凑序列化导致性能显著下降，验证器奖励微调在监督训练一半的周期内达到性能饱和，但未改善跨领域泛化。

Conclusion: 微调模型严重依赖领域特定模式而非可迁移的规划能力，领域内性能在80%左右达到平台期，而跨领域性能崩溃，揭示了LLM基于规划的持续泛化差距。

Abstract: Recent work shows that fine-tuned Large Language Models (LLMs) can achieve high valid plan rates on PDDL planning tasks. However, it remains unclear whether this reflects transferable planning competence or domain-specific memorization. In this work, we fine-tune a 1.7B-parameter LLM on 40,000 domain-problem-plan tuples from 10 IPC 2023 domains, and evaluate both in-domain and cross-domain generalization. While the model reaches 82.9% valid plan rate in in-domain conditions, it achieves 0% on two unseen domains. To analyze this failure, we introduce three diagnostic interventions, namely (i) instance-wise symbol anonymization, (ii) compact plan serialization, and (iii) verifier-reward fine-tuning using the VAL validator as a success-focused reinforcement signal. Symbol anonymization and compact serialization cause significant performance drops despite preserving plan semantics, thus revealing strong sensitivity to surface representations. Verifier-reward fine-tuning reaches performance saturation in half the supervised training epochs, but does not improve cross-domain generalization. For the explored configurations, in-domain performance plateaus around 80%, while cross-domain performance collapses, suggesting that our fine-tuned model relies heavily on domain-specific patterns rather than transferable planning competence in this setting. Our results highlight a persistent generalization gap in LLM-based planning and provide diagnostic tools for studying its causes.

</details>


### [19] [Scalable Knee-Point Guided Activity Group Selection in Multi-Tree Genetic Programming for Dynamic Multi-Mode Project Scheduling](https://arxiv.org/abs/2601.14485)
*Yuan Tian,Yi Mei,Mengjie Zhang*

Main category: cs.AI

TL;DR: 本文提出了一种基于膝点的活动组选择策略，通过多树遗传编程框架同时演化排序规则和组选择规则，解决了动态多模式资源受限项目调度问题中组选择策略的可扩展性问题。


<details>
  <summary>Details</summary>
Motivation: 动态多模式资源受限项目调度问题需要在活动执行顺序和对应执行模式上做出决策。虽然活动组选择策略在小规模实例中有效，但在处理大规模问题时存在可扩展性问题，需要改进以适应更大规模的问题实例。

Method: 提出基于膝点的选择机制：首先使用活动排序规则对所有符合条件的活动-模式对进行排序，然后通过膝点选择找到有前景的对，最后由组选择规则选择最佳活动组合。开发了多树遗传编程框架来同时演化这两种类型的规则。

Result: 实验结果表明，该方法在大规模实例中具有良好的可扩展性，在大多数场景下优于采用顺序决策的遗传编程方法。

Conclusion: 基于膝点的活动组选择策略有效解决了组选择策略在大规模问题中的可扩展性问题，通过多树遗传编程框架同时演化排序规则和组选择规则，显著提升了动态多模式资源受限项目调度问题的求解性能。

Abstract: The dynamic multi-mode resource-constrained project scheduling problem is a challenging scheduling problem that requires making decisions on both the execution order of activities and their corresponding execution modes. Genetic programming has been widely applied as a hyper-heuristic to evolve priority rules that guide the selection of activity-mode pairs from the current eligible set. Recently, an activity group selection strategy has been proposed to select a subset of activities rather than a single activity at each decision point, allowing for more effective scheduling by considering the interdependence between activities. Although effective in small-scale instances, this strategy suffers from scalability issues when applied to larger problems. In this work, we enhance the scalability of the group selection strategy by introducing a knee-point-based selection mechanism to identify a promising subset of activities before evaluating their combinations. An activity ordering rule is first used to rank all eligible activity-mode pairs, followed by a knee point selection to find the promising pairs. Then, a group selection rule selects the best activity combination. We develop a multi-tree GP framework to evolve both types of rules simultaneously. Experimental results demonstrate that our approach scales well to large instances and outperforms GP with sequential decision-making in most scenarios.

</details>


### [20] ["Just in Time" World Modeling Supports Human Planning and Reasoning](https://arxiv.org/abs/2601.14514)
*Tony Chen,Sam Cheyette,Kelsey Allen,Joshua Tenenbaum,Kevin Smith*

Main category: cs.AI

TL;DR: 论文提出了"即时"框架，通过模拟、视觉搜索和表征修改的紧密交互，在线构建简化表征来支持高效心理模拟。


<details>
  <summary>Details</summary>
Motivation: 心理模拟在人类推理、规划和预测中起关键作用，但在复杂环境中的模拟需求超出人类实际能力限制。虽然有证据表明人们使用简化表征进行模拟，但如何高效确定这些简化尚不清楚。

Method: 提出"即时"框架，通过模拟、视觉搜索和表征修改的紧密交互：当前模拟指导搜索方向，视觉搜索标记需要编码的对象用于后续模拟。模型只编码一小部分对象。

Result: 在网格世界规划任务和物理推理任务中，该模型在多种行为测量上表现出优于替代模型的强实证支持，能够做出高效用的预测。

Conclusion: 这些结果为人们如何构建简化表征以支持高效心理模拟提供了具体的算法解释。

Abstract: Probabilistic mental simulation is thought to play a key role in human reasoning, planning, and prediction, yet the demands of simulation in complex environments exceed realistic human capacity limits. A theory with growing evidence is that people simulate using simplified representations of the environment that abstract away from irrelevant details, but it is unclear how people determine these simplifications efficiently. Here, we present a "Just-in-Time" framework for simulation-based reasoning that demonstrates how such representations can be constructed online with minimal added computation. The model uses a tight interleaving of simulation, visual search, and representation modification, with the current simulation guiding where to look and visual search flagging objects that should be encoded for subsequent simulation. Despite only ever encoding a small subset of objects, the model makes high-utility predictions. We find strong empirical support for this account over alternative models in a grid-world planning task and a physical reasoning task across a range of behavioral measures. Together, these results offer a concrete algorithmic account of how people construct reduced representations to support efficient mental simulation.

</details>


### [21] [MAS-Orchestra: Understanding and Improving Multi-Agent Reasoning Through Holistic Orchestration and Controlled Benchmarks](https://arxiv.org/abs/2601.14652)
*Zixuan Ke,Yifei Ming,Austin Xu,Ryan Chin,Xuan-Phi Nguyen,Prathyusha Jwalapuram,Semih Yavuz,Caiming Xiong,Shafiq Joty*

Main category: cs.AI

TL;DR: MAS-Orchestra：一个将多智能体系统编排建模为函数调用强化学习问题的训练时框架，配合MASBENCH基准测试系统，揭示了MAS优势取决于任务结构而非普遍适用


<details>
  <summary>Details</summary>
Motivation: 当前多智能体系统自动设计方法存在两大问题：方法复杂性（顺序代码级执行限制全局系统级推理）和效能不确定性（部署前无法确定相比单智能体系统的实际优势），需要新的框架来系统性地理解和设计多智能体系统

Method: 提出MAS-Orchestra框架，将多智能体编排建模为函数调用强化学习问题，采用整体编排方法一次性生成整个MAS；将复杂的目标导向子智能体抽象为可调用函数，隐藏内部执行细节；同时引入MASBENCH基准测试，从深度、视野、广度、并行性和鲁棒性五个维度表征任务

Result: 分析表明MAS的增益关键取决于任务结构、验证协议以及编排器和子智能体的能力，而非普遍适用；MAS-Orchestra在数学推理、多跳问答和基于搜索的问答等公开基准测试中取得了一致的改进

Conclusion: MAS-Orchestra和MASBENCH共同促进了多智能体系统的更好训练和理解，为追求多智能体智能提供了系统化的方法论和评估工具

Abstract: While multi-agent systems (MAS) promise elevated intelligence through coordination of agents, current approaches to automatic MAS design under-deliver. Such shortcomings stem from two key factors: (1) methodological complexity - agent orchestration is performed using sequential, code-level execution that limits global system-level holistic reasoning and scales poorly with agent complexity - and (2) efficacy uncertainty - MAS are deployed without understanding if there are tangible benefits compared to single-agent systems (SAS). We propose MAS-Orchestra, a training-time framework that formulates MAS orchestration as a function-calling reinforcement learning problem with holistic orchestration, generating an entire MAS at once. In MAS-Orchestra, complex, goal-oriented sub-agents are abstracted as callable functions, enabling global reasoning over system structure while hiding internal execution details. To rigorously study when and why MAS are beneficial, we introduce MASBENCH, a controlled benchmark that characterizes tasks along five axes: Depth, Horizon, Breadth, Parallel, and Robustness. Our analysis reveals that MAS gains depend critically on task structure, verification protocols, and the capabilities of both orchestrator and sub-agents, rather than holding universally. Guided by these insights, MAS-Orchestra achieves consistent improvements on public benchmarks including mathematical reasoning, multi-hop QA, and search-based QA. Together, MAS-Orchestra and MASBENCH enable better training and understanding of MAS in the pursuit of multi-agent intelligence.

</details>


### [22] [Query-Efficient Agentic Graph Extraction Attacks on GraphRAG Systems](https://arxiv.org/abs/2601.14662)
*Shuhua Yang,Jiahao Zhang,Yilong Wang,Dongwon Lee,Suhang Wang*

Main category: cs.AI

TL;DR: AGEA攻击框架通过新颖性引导的探索-利用策略、外部图记忆模块和两阶段图提取流程，在有限查询预算下成功窃取GraphRAG系统的潜在实体关系图结构，恢复率高达90%。


<details>
  <summary>Details</summary>
Motivation: 研究GraphRAG系统在现实查询预算下对隐藏图结构重构攻击的脆弱性。虽然先前研究表明GraphRAG响应可能泄露检索的子图，但在实际查询预算下高效重构隐藏图结构的可行性尚未被探索。

Method: 提出AGEA（Agentic Graph Extraction Attack）框架，采用预算约束的黑盒攻击设置，包含：1）新颖性引导的探索-利用策略；2）外部图记忆模块；3）两阶段图提取流程（轻量级发现+LLM过滤）。在Microsoft-GraphRAG和LightRAG系统上评估。

Result: 在相同查询预算下，AGEA显著优于先前攻击基线，在医疗、农业和文学数据集上恢复高达90%的实体和关系，同时保持高精度。证明现代GraphRAG系统即使在严格查询限制下也高度脆弱。

Conclusion: 现代GraphRAG系统对结构化、智能化的提取攻击高度脆弱，即使在严格查询限制下，攻击者也能高效重构系统的潜在实体关系图结构，这揭示了GraphRAG系统的安全风险。

Abstract: Graph-based retrieval-augmented generation (GraphRAG) systems construct knowledge graphs over document collections to support multi-hop reasoning. While prior work shows that GraphRAG responses may leak retrieved subgraphs, the feasibility of query-efficient reconstruction of the hidden graph structure remains unexplored under realistic query budgets. We study a budget-constrained black-box setting where an adversary adaptively queries the system to steal its latent entity-relation graph. We propose AGEA (Agentic Graph Extraction Attack), a framework that leverages a novelty-guided exploration-exploitation strategy, external graph memory modules, and a two-stage graph extraction pipeline combining lightweight discovery with LLM-based filtering. We evaluate AGEA on medical, agriculture, and literary datasets across Microsoft-GraphRAG and LightRAG systems. Under identical query budgets, AGEA significantly outperforms prior attack baselines, recovering up to 90% of entities and relationships while maintaining high precision. These results demonstrate that modern GraphRAG systems are highly vulnerable to structured, agentic extraction attacks, even under strict query limits.

</details>


### [23] [Local Language Models for Context-Aware Adaptive Anonymization of Sensitive Text](https://arxiv.org/abs/2601.14683)
*Aisvarya Adeseye,Jouni Isoaho,Seppo Virtanen,Mohammad Tahir*

Main category: cs.AI

TL;DR: 本研究开发了一个基于本地大语言模型的结构化自适应匿名化框架（SFAA），用于定性研究转录文本的敏感数据检测和匿名化处理，相比传统方法更准确且能保持文本语义。


<details>
  <summary>Details</summary>
Motivation: 定性研究包含大量个人、情境和组织细节，存在隐私风险。传统人工匿名化耗时、不一致且易遗漏关键标识符，现有自动化工具依赖模式匹配或固定规则，无法理解上下文且可能改变数据含义。

Method: 提出结构化自适应匿名化框架（SFAA），包含检测、分类和自适应匿名化三个步骤。采用四种匿名化策略：基于规则的替换、上下文感知重写、泛化和抑制，根据标识符类型和风险级别应用策略。基于GDPR、HIPAA和OECD等国际隐私标准定义标识符。使用LLaMA和Phi两个本地模型，通过包含82个面对面访谈和93个AI引导访谈的两个案例研究进行双重评估。

Result: 大语言模型比人工评审员发现更多敏感数据。Phi在发现敏感数据方面优于LLaMA（找到91%以上敏感数据），但错误稍多。Phi处理的文本中94.8%保持了与原文本相同的情感，表明准确性高且不影响定性数据分析。

Conclusion: 基于本地大语言模型的SFAA框架为定性研究转录文本提供了一种可靠、可重复且上下文感知的匿名化方法，能够有效保护隐私同时保持数据分析和研究完整性。

Abstract: Qualitative research often contains personal, contextual, and organizational details that pose privacy risks if not handled appropriately. Manual anonymization is time-consuming, inconsistent, and frequently omits critical identifiers. Existing automated tools tend to rely on pattern matching or fixed rules, which fail to capture context and may alter the meaning of the data. This study uses local LLMs to build a reliable, repeatable, and context-aware anonymization process for detecting and anonymizing sensitive data in qualitative transcripts. We introduce a Structured Framework for Adaptive Anonymizer (SFAA) that includes three steps: detection, classification, and adaptive anonymization. The SFAA incorporates four anonymization strategies: rule-based substitution, context-aware rewriting, generalization, and suppression. These strategies are applied based on the identifier type and the risk level. The identifiers handled by the SFAA are guided by major international privacy and research ethics standards, including the GDPR, HIPAA, and OECD guidelines. This study followed a dual-method evaluation that combined manual and LLM-assisted processing. Two case studies were used to support the evaluation. The first includes 82 face-to-face interviews on gamification in organizations. The second involves 93 machine-led interviews using an AI-powered interviewer to test LLM awareness and workplace privacy. Two local models, LLaMA and Phi were used to evaluate the performance of the proposed framework. The results indicate that the LLMs found more sensitive data than a human reviewer. Phi outperformed LLaMA in finding sensitive data, but made slightly more errors. Phi was able to find over 91% of the sensitive data and 94.8% kept the same sentiment as the original text, which means it was very accurate, hence, it does not affect the analysis of the qualitative data.

</details>


### [24] [AutoDriDM: An Explainable Benchmark for Decision-Making of Vision-Language Models in Autonomous Driving](https://arxiv.org/abs/2601.14702)
*Zecong Tang,Zixu Wang,Yifei Wang,Weitong Lian,Tianjian Gao,Haoran Li,Tengju Ru,Lingyi Meng,Zhejun Cui,Yichen Zhu,Qi Kang,Kaixuan Wang,Yu Zhang*

Main category: cs.AI

TL;DR: AutoDriDM是一个面向自动驾驶的决策中心化渐进式基准测试，包含6,650个问题，评估视觉语言模型在对象、场景和决策三个维度的能力，揭示感知与决策性能之间的弱相关性。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶领域的基准测试过于强调感知能力，未能充分评估决策过程。虽然视觉语言模型展现出推理和泛化能力，但缺乏专门评估其在自动驾驶中决策能力的基准。

Method: 提出了AutoDriDM基准，包含6,650个问题，涵盖对象、场景和决策三个维度。评估主流视觉语言模型，分析感知到决策的能力边界，进行可解释性分析识别关键失败模式，并引入分析器模型实现大规模自动标注。

Result: 评估揭示了感知与决策性能之间的弱相关性。可解释性分析识别出逻辑推理错误等关键失败模式。AutoDriDM填补了感知中心化与决策中心化评估之间的差距。

Conclusion: AutoDriDM为开发更安全可靠的自动驾驶视觉语言模型提供了指导，通过决策中心化的评估方法推动自动驾驶系统向更可靠的决策能力发展。

Abstract: Autonomous driving is a highly challenging domain that requires reliable perception and safe decision-making in complex scenarios. Recent vision-language models (VLMs) demonstrate reasoning and generalization abilities, opening new possibilities for autonomous driving; however, existing benchmarks and metrics overemphasize perceptual competence and fail to adequately assess decision-making processes. In this work, we present AutoDriDM, a decision-centric, progressive benchmark with 6,650 questions across three dimensions - Object, Scene, and Decision. We evaluate mainstream VLMs to delineate the perception-to-decision capability boundary in autonomous driving, and our correlation analysis reveals weak alignment between perception and decision-making performance. We further conduct explainability analyses of models' reasoning processes, identifying key failure modes such as logical reasoning errors, and introduce an analyzer model to automate large-scale annotation. AutoDriDM bridges the gap between perception-centered and decision-centered evaluation, providing guidance toward safer and more reliable VLMs for real-world autonomous driving.

</details>


### [25] [DARA: Few-shot Budget Allocation in Online Advertising via In-Context Decision Making with RL-Finetuned LLMs](https://arxiv.org/abs/2601.14711)
*Mingxuan Song,Yusen Huo,Bohan Zhou,Shenglin Yin,Zhen Xiao,Jieyi Long,Zhilin Zhang,Chuan Yu*

Main category: cs.AI

TL;DR: 论文提出GRPO-Adaptive和DARA框架，通过LLM增强的推理和数值精度优化，解决AI生成竞价中广告主在预算约束下的累积价值最大化问题。


<details>
  <summary>Details</summary>
Motivation: 在线广告中，广告主在预算约束下优化累积竞价价值面临挑战。传统强化学习方法在个性化目标和有限历史数据的少样本场景中效果不佳，而大型语言模型虽然具有上下文学习能力，但缺乏数值精度。

Method: 提出GRPO-Adaptive策略，通过动态更新参考策略来增强LLM的推理和数值精度。基于此构建DARA双阶段框架：第一阶段使用少样本推理器通过上下文提示生成初始计划；第二阶段使用反馈驱动的精细优化器改进计划。

Result: 在真实世界和合成数据环境中的广泛实验表明，该方法在预算约束下的广告主累积价值方面持续优于现有基线方法。

Conclusion: GRPO-Adaptive和DARA框架成功结合了LLM的上下文学习优势和AIGB任务所需的精确适应性，有效解决了少样本场景下的广告竞价优化问题。

Abstract: Optimizing the advertiser's cumulative value of winning impressions under budget constraints poses a complex challenge in online advertising, under the paradigm of AI-Generated Bidding (AIGB). Advertisers often have personalized objectives but limited historical interaction data, resulting in few-shot scenarios where traditional reinforcement learning (RL) methods struggle to perform effectively. Large Language Models (LLMs) offer a promising alternative for AIGB by leveraging their in-context learning capabilities to generalize from limited data. However, they lack the numerical precision required for fine-grained optimization. To address this limitation, we introduce GRPO-Adaptive, an efficient LLM post-training strategy that enhances both reasoning and numerical precision by dynamically updating the reference policy during training. Built upon this foundation, we further propose DARA, a novel dual-phase framework that decomposes the decision-making process into two stages: a few-shot reasoner that generates initial plans via in-context prompting, and a fine-grained optimizer that refines these plans using feedback-driven reasoning. This separation allows DARA to combine LLMs' in-context learning strengths with precise adaptability required by AIGB tasks. Extensive experiments on both real-world and synthetic data environments demonstrate that our approach consistently outperforms existing baselines in terms of cumulative advertiser value under budget constraints.

</details>


### [26] [An XAI View on Explainable ASP: Methods, Systems, and Perspectives](https://arxiv.org/abs/2601.14764)
*Thomas Eiter,Tobias Geibinger,Zeynep G. Saribatur*

Main category: cs.AI

TL;DR: 本文是一篇关于ASP（答案集编程）解释方法的综述，从可解释AI角度分析ASP的解释类型、现有工具覆盖情况，并指出研究空白和未来方向。


<details>
  <summary>Details</summary>
Motivation: ASP作为一种声明式推理和问题解决方法，其基于规则的形式使其在可解释AI中具有天然优势。随着可解释AI的重要性日益增长，需要系统梳理ASP解释方法的现状，分析现有工具覆盖范围，识别研究空白。

Method: 采用综述研究方法，从可解释AI视角出发，分析ASP解释类型与用户解释需求的关系，评估现有理论和工具对这些解释类型的覆盖情况。

Result: 现有ASP解释方法往往针对特定解释场景，未能覆盖ASP用户遇到的所有情况。通过系统分析，识别了当前ASP解释方法中的空白领域。

Conclusion: 需要进一步研究以填补ASP解释方法的空白，为未来工作指明研究方向，推动ASP在可解释AI领域的更广泛应用。

Abstract: Answer Set Programming (ASP) is a popular declarative reasoning and problem solving approach in symbolic AI. Its rule-based formalism makes it inherently attractive for explainable and interpretive reasoning, which is gaining importance with the surge of Explainable AI (XAI). A number of explanation approaches and tools for ASP have been developed, which often tackle specific explanatory settings and may not cover all scenarios that ASP users encounter. In this survey, we provide, guided by an XAI perspective, an overview of types of ASP explanations in connection with user questions for explanation, and describe how their coverage by current theory and tools. Furthermore, we pinpoint gaps in existing ASP explanations approaches and identify research directions for future work.

</details>


### [27] [Semantic-Guided Unsupervised Video Summarization](https://arxiv.org/abs/2601.14773)
*Haizhou Liu,Haodong Jin,Yiming Wang,Hui Yu*

Main category: cs.AI

TL;DR: 提出了一种基于语义引导的无监督视频摘要方法，通过帧级语义对齐注意力机制和增量训练策略，解决了现有GAN方法语义信息利用不足和训练不稳定的问题。


<details>
  <summary>Details</summary>
Motivation: 现有无监督视频摘要方法主要依赖GAN进行关键帧选择，但大多仅利用单模态特征，忽视了语义信息在关键帧选择中的指导作用，且存在训练不稳定的问题。

Method: 提出语义引导的无监督视频摘要方法：1）设计帧级语义对齐注意力机制，集成到关键帧选择器中；2）在对抗框架中引导基于Transformer的生成器更好地重建视频；3）采用增量训练策略逐步更新模型组件，缓解GAN训练不稳定性。

Result: 实验结果表明，该方法在多个基准数据集上取得了优越的性能表现。

Conclusion: 提出的语义引导方法和增量训练策略有效提升了无监督视频摘要的性能，解决了现有方法的局限性。

Abstract: Video summarization is a crucial technique for social understanding, enabling efficient browsing of massive multimedia content and extraction of key information from social platforms. Most existing unsupervised summarization methods rely on Generative Adversarial Networks (GANs) to enhance keyframe selection and generate coherent, video summaries through adversarial training. However, such approaches primarily exploit unimodal features, overlooking the guiding role of semantic information in keyframe selection, and often suffer from unstable training. To address these limitations, we propose a novel Semantic-Guided Unsupervised Video Summarization method. Specifically, we design a novel frame-level semantic alignment attention mechanism and integrate it into a keyframe selector, which guides the Transformer-based generator within the adversarial framework to better reconstruct videos. In addition, we adopt an incremental training strategy to progressively update the model components, effectively mitigating the instability of GAN training. Experimental results demonstrate that our approach achieves superior performance on multiple benchmark datasets.

</details>


### [28] [Towards Bound Consistency for the No-Overlap Constraint Using MDDs](https://arxiv.org/abs/2601.14784)
*Amaury Guichard,Laurent Michel,Hélène Verhaeghe,Pierre Schaus*

Main category: cs.AI

TL;DR: 本文提出了首个针对无重叠约束的边界一致性算法，通过构建有界宽度的MDD（多值决策图）实现多项式时间的边界收紧，相比现有方法能更有效地减少搜索树节点数量。


<details>
  <summary>Details</summary>
Motivation: 无重叠约束的边界一致性已知是NP完全问题，现有多项式时间收紧技术（如边查找、非首非尾推理、能量推理）存在局限性，需要更有效的边界一致性算法来提升约束求解效率。

Method: 基于Ciré和van Hoeve定义的无重叠MDD，提取作业时间窗口边界以收紧起止时间；通过限制MDD宽度为阈值创建松弛MDD，控制算法规模和时间复杂度，实现多项式时间的边界一致性过滤。

Result: 在带时间窗口的排序问题和准时目标问题上实验表明，即使设置宽度阈值，新过滤方法相比现有优先检测算法能更显著减少搜索树节点数量；与经典传播方法互补，在多个实例上同时减少节点数量和求解时间。

Conclusion: 本文首次实现了无重叠约束的边界一致性算法，通过有界宽度MDD在多项式时间内完成边界收紧，为约束规划中的无重叠约束提供了更强大的过滤技术。

Abstract: Achieving bound consistency for the no-overlap constraint is known to be NP-complete. Therefore, several polynomial-time tightening techniques, such as edge finding, not-first-not-last reasoning, and energetic reasoning, have been introduced for this constraint. In this work, we derive the first bound-consistent algorithm for the no-overlap constraint. By building on the no-overlap MDD defined by Ciré and van Hoeve, we extract bounds of the time window of the jobs, allowing us to tighten start and end times in time polynomial in the number of nodes of the MDD. Similarly, to bound the size and time-complexity, we limit the width of the MDD to a threshold, creating a relaxed MDD that can also be used to relax the bound-consistent filtering. Through experiments on a sequencing problem with time windows and a just-in-time objective ($1 \mid r_j, d_j, \bar{d}_j \mid \sum E_j + \sum T_j$), we observe that the proposed filtering, even with a threshold on the width, achieves a stronger reduction in the number of nodes visited in the search tree compared to the previously proposed precedence-detection algorithm of Ciré and van Hoeve. The new filtering also appears to be complementary to classical propagation methods for the no-overlap constraint, allowing a substantial reduction in both the number of nodes and the solving time on several instances.

</details>


### [29] [Implementing Knowledge Representation and Reasoning with Object Oriented Design](https://arxiv.org/abs/2601.14840)
*Abdelrhman Bassiouny,Tom Schierenbeck,Sorin Arion,Benjamin Alt,Naren Vasantakumaar,Giang Nguyen,Michael Beetz*

Main category: cs.AI

TL;DR: KRROOD是一个将知识表示与推理系统集成到面向对象编程中的框架，通过将知识作为一等编程抽象来解决现代软件工程与KR&R系统之间的集成鸿沟。


<details>
  <summary>Details</summary>
Motivation: 现代软件工程以面向对象编程为标准，但现有的知识表示与推理框架通常依赖外部本体和专门语言，难以与命令式代码集成，导致两种范式之间存在鸿沟。

Method: KRROOD通过原生类结构将知识作为一等编程抽象，弥合逻辑编程与面向对象编程范式之间的差距，使用户能够直接在OOP环境中处理知识表示和推理。

Result: 在OWL2Bench基准测试和人机任务学习场景中的实验结果表明，KRROOD在保持强大性能的同时，支持现实世界自主系统所需的表达性推理能力。

Conclusion: KRROOD成功解决了知识表示与推理系统与面向对象编程之间的集成问题，为开发复杂应用提供了更自然的知识处理方式，适用于现实世界的自主系统。

Abstract: This paper introduces KRROOD, a framework designed to bridge the integration gap between modern software engineering and Knowledge Representation & Reasoning (KR&R) systems. While Object-Oriented Programming (OOP) is the standard for developing complex applications, existing KR&R frameworks often rely on external ontologies and specialized languages that are difficult to integrate with imperative code. KRROOD addresses this by treating knowledge as a first-class programming abstraction using native class structures, bridging the gap between the logic programming and OOP paradigms. We evaluate the system on the OWL2Bench benchmark and a human-robot task learning scenario. Experimental results show that KRROOD achieves strong performance while supporting the expressive reasoning required for real-world autonomous systems.

</details>


### [30] [Measuring and Aligning Abstraction in Vision-Language Models with Medical Taxonomies](https://arxiv.org/abs/2601.14827)
*Ben Schaper,Maxime Di Folco,Bernhard Kainz,Julia A. Schnabel,Cosmin I. Bercea*

Main category: cs.AI

TL;DR: 该研究评估了视觉语言模型在胸部X光分类中的抽象错误，提出使用医学分类学进行分层评估，并开发了减少严重错误的方法。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在胸部X光分类中表现出强大的零样本性能，但标准的平面评估指标无法区分临床轻微错误和严重错误。需要量化并减轻抽象错误，以提高模型的临床安全性。

Method: 1. 使用分层指标对多个最先进的视觉语言模型进行基准测试；2. 引入灾难性抽象错误来捕捉跨分支错误；3. 提出风险约束阈值和基于分类学的径向嵌入微调方法。

Result: 研究结果显示，尽管视觉语言模型在平面性能上表现优异，但与临床分类学存在显著不对齐。提出的方法能将严重抽象错误降低到2%以下，同时保持有竞争力的性能。

Conclusion: 分层评估和表示层面的对齐对于视觉语言模型更安全、更具临床意义的部署至关重要。提出的方法有效减少了严重错误，提高了模型的临床实用性。

Abstract: Vision-Language Models show strong zero-shot performance for chest X-ray classification, but standard flat metrics fail to distinguish between clinically minor and severe errors. This work investigates how to quantify and mitigate abstraction errors by leveraging medical taxonomies. We benchmark several state-of-the-art VLMs using hierarchical metrics and introduce Catastrophic Abstraction Errors to capture cross-branch mistakes. Our results reveal substantial misalignment of VLMs with clinical taxonomies despite high flat performance. To address this, we propose risk-constrained thresholding and taxonomy-aware fine-tuning with radial embeddings, which reduce severe abstraction errors to below 2 per cent while maintaining competitive performance. These findings highlight the importance of hierarchical evaluation and representation-level alignment for safer and more clinically meaningful deployment of VLMs.

</details>


### [31] [Just aware enough: Evaluating awareness across artificial systems](https://arxiv.org/abs/2601.14901)
*Nadine Meertens,Suet Lee,Ophelia Deroy*

Main category: cs.AI

TL;DR: 论文提出用"意识"替代"意识"作为评估AI系统的更实用方法，开发了一个可跨不同系统评估意识能力的结构化框架。


<details>
  <summary>Details</summary>
Motivation: 当前关于AI意识和道德地位的讨论缺乏共识和可操作的评价方法，需要更实用、方法上可操作的替代方案。

Method: 提出评估意识的实用方法，将意识定义为系统处理、存储和使用信息以实现目标导向行动的能力。该方法具有四个核心特征：领域敏感性、可扩展性、多维度和任务性能预测能力。

Result: 开发了一个结构化框架，用于评估和比较具有不同架构、规模和操作领域的人工系统的意识特征，支持跨系统比较。

Conclusion: 从人工意识转向"足够意识"的评估方法，有助于促进原则性评估、支持设计和监督，并推动更具建设性的科学和公共讨论。

Abstract: Recent debates on artificial intelligence increasingly emphasise questions of AI consciousness and moral status, yet there remains little agreement on how such properties should be evaluated. In this paper, we argue that awareness offers a more productive and methodologically tractable alternative. We introduce a practical method for evaluating awareness across diverse systems, where awareness is understood as encompassing a system's abilities to process, store and use information in the service of goal-directed action. Central to this approach is the claim that any evaluation aiming to capture the diversity of artificial systems must be domain-sensitive, deployable at any scale, multidimensional, and enable the prediction of task performance, while generalising to the level of abilities for the sake of comparison. Given these four desiderata, we outline a structured approach to evaluating and comparing awareness profiles across artificial systems with differing architectures, scales, and operational domains. By shifting the focus from artificial consciousness to being just aware enough, this approach aims to facilitate principled assessment, support design and oversight, and enable more constructive scientific and public discourse.

</details>


### [32] [The Why Behind the Action: Unveiling Internal Drivers via Agentic Attribution](https://arxiv.org/abs/2601.15075)
*Chen Qian,Peng Wang,Dongrui Liu,Junyao Yang,Dadi Guo,Ling Tang,Jilin Mei,Qihan Ren,Shuai Shao,Yong Liu,Jie Fu,Jing Shao,Xia Hu*

Main category: cs.AI

TL;DR: 提出一个通用智能体归因框架，用于识别驱动智能体行为的内部因素，而不论任务结果如何，通过层级分析定位关键历史事件和文本证据。


<details>
  <summary>Details</summary>
Motivation: 随着基于大语言模型的智能体在现实应用中广泛部署，理解智能体为何采取特定行动对于问责和治理变得越来越重要。现有研究主要关注失败归因，不足以解释智能体行为背后的推理过程。

Method: 提出分层框架：在组件层面使用时间似然动态识别关键交互步骤；在句子层面使用基于扰动的分析来精确定位具体文本证据。

Result: 实验验证表明，该框架能够可靠地识别驱动智能体行为的关键历史事件和句子，包括标准工具使用和内存诱导偏差等微妙可靠性风险。

Conclusion: 该框架为构建更安全、更可问责的智能体系统迈出了关键一步，能够解释智能体行为背后的推理过程，而不仅仅是失败归因。

Abstract: Large Language Model (LLM)-based agents are widely used in real-world applications such as customer service, web navigation, and software engineering. As these systems become more autonomous and are deployed at scale, understanding why an agent takes a particular action becomes increasingly important for accountability and governance. However, existing research predominantly focuses on \textit{failure attribution} to localize explicit errors in unsuccessful trajectories, which is insufficient for explaining the reasoning behind agent behaviors. To bridge this gap, we propose a novel framework for \textbf{general agentic attribution}, designed to identify the internal factors driving agent actions regardless of the task outcome. Our framework operates hierarchically to manage the complexity of agent interactions. Specifically, at the \textit{component level}, we employ temporal likelihood dynamics to identify critical interaction steps; then at the \textit{sentence level}, we refine this localization using perturbation-based analysis to isolate the specific textual evidence. We validate our framework across a diverse suite of agentic scenarios, including standard tool use and subtle reliability risks like memory-induced bias. Experimental results demonstrate that the proposed framework reliably pinpoints pivotal historical events and sentences behind the agent behavior, offering a critical step toward safer and more accountable agentic systems.

</details>


### [33] [The Plausibility Trap: Using Probabilistic Engines for Deterministic Tasks](https://arxiv.org/abs/2601.15130)
*Ivan Carrera,Daniel Maldonado-Ruiz*

Main category: cs.AI

TL;DR: 论文定义了"可信性陷阱"现象：人们过度使用昂贵的概率性AI模型处理简单确定性任务，造成资源浪费，并提出工具选择工程框架来指导何时使用或避免使用生成式AI。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的普及，用户便利性优先于计算效率，导致人们使用昂贵的概率性引擎处理OCR、基本验证等简单确定性任务，造成显著的资源浪费和效率损失。

Method: 通过OCR和事实核查的微基准测试和案例研究，量化"效率税"；引入工具选择工程和确定性-概率性决策矩阵框架，帮助开发者决定何时使用生成式AI以及何时避免使用。

Result: 研究发现存在约6.5倍的延迟惩罚（效率税），并揭示了算法奉承的风险；提出的决策框架能够有效指导开发者在适当场景选择合适工具。

Conclusion: 真正的数字素养不仅在于知道如何使用生成式AI，更在于知道何时不使用它；需要课程改革来培养这种判断能力，避免陷入"可信性陷阱"。

Abstract: The ubiquity of Large Language Models (LLMs) is driving a paradigm shift where user convenience supersedes computational efficiency. This article defines the "Plausibility Trap": a phenomenon where individuals with access to Artificial Intelligence (AI) models deploy expensive probabilistic engines for simple deterministic tasks-such as Optical Character Recognition (OCR) or basic verification-resulting in significant resource waste. Through micro-benchmarks and case studies on OCR and fact-checking, we quantify the "efficiency tax"-demonstrating a ~6.5x latency penalty-and the risks of algorithmic sycophancy. To counter this, we introduce Tool Selection Engineering and the Deterministic-Probabilistic Decision Matrix, a framework to help developers determine when to use Generative AI and, crucially, when to avoid it. We argue for a curriculum shift, emphasizing that true digital literacy relies not only in knowing how to use Generative AI, but also on knowing when not to use it.

</details>


### [34] [How to Build AI Agents by Augmenting LLMs with Codified Human Expert Domain Knowledge? A Software Engineering Framework](https://arxiv.org/abs/2601.15153)
*Choro Ulan uulu,Mikhail Kulyabin,Iris Fuhrmann,Jan Joosten,Nuno Miguel Martins Pacheco,Filippos Petridis,Rebecca Johnson,Jan Bosch,Helena Holmström Olsson*

Main category: cs.AI

TL;DR: 提出一个软件工程框架，通过增强LLM来捕获人类领域知识，使非专家能在仿真数据可视化中达到专家级效果，输出质量提升206%。


<details>
  <summary>Details</summary>
Motivation: 关键领域知识通常只掌握在少数专家手中，造成组织扩展和决策瓶颈。非专家难以创建有效可视化，导致洞察力不足并占用专家时间。

Method: 提出软件工程框架，通过增强大型语言模型（LLM），结合请求分类器、检索增强生成（RAG）系统用于代码生成、编码专家规则和可视化设计原则，构建具有自主、反应、主动和社交行为的智能体。

Result: 在五个跨工程领域的场景中，12名评估者显示输出质量提升206%，智能体在所有案例中达到专家级评分，而基线表现较差，同时保持更优的代码质量和更低方差。

Conclusion: 贡献包括：自动化的基于智能体的可视化生成系统，以及经过验证的系统化捕获人类领域知识、将隐性专家知识编码到AI智能体的框架，证明非专家能在专业领域达到专家级成果。

Abstract: Critical domain knowledge typically resides with few experts, creating organizational bottlenecks in scalability and decision-making. Non-experts struggle to create effective visualizations, leading to suboptimal insights and diverting expert time. This paper investigates how to capture and embed human domain knowledge into AI agent systems through an industrial case study. We propose a software engineering framework to capture human domain knowledge for engineering AI agents in simulation data visualization by augmenting a Large Language Model (LLM) with a request classifier, Retrieval-Augmented Generation (RAG) system for code generation, codified expert rules, and visualization design principles unified in an agent demonstrating autonomous, reactive, proactive, and social behavior. Evaluation across five scenarios spanning multiple engineering domains with 12 evaluators demonstrates 206% improvement in output quality, with our agent achieving expert-level ratings in all cases versus baseline's poor performance, while maintaining superior code quality with lower variance. Our contributions are: an automated agent-based system for visualization generation and a validated framework for systematically capturing human domain knowledge and codifying tacit expert knowledge into AI agents, demonstrating that non-experts can achieve expert-level outcomes in specialized domains.

</details>


### [35] [Knowledge Graphs are Implicit Reward Models: Path-Derived Signals Enable Compositional Reasoning](https://arxiv.org/abs/2601.15160)
*Yuval Kansal,Niraj K. Jha*

Main category: cs.AI

TL;DR: 该论文提出了一种基于知识图谱路径奖励的强化学习后训练方法，通过将模型在领域公理事实中"接地"，使其能够组合这些知识来解决复杂的多跳推理任务，在医学领域取得了优于更大模型和前沿系统的性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在数学和编程等结构化推理领域已达到接近专家的水平，但在专业科学领域进行组合式多跳推理的能力仍然有限。需要一种方法让模型能够基于领域公理事实进行组合推理，解决复杂未见任务。

Method: 提出自下而上的学习范式，结合监督微调和强化学习（RL）的后训练流程。使用知识图谱作为隐式奖励模型，从知识图谱路径中推导新颖的奖励信号，提供可验证、可扩展且接地的监督，鼓励模型在RL过程中组合中间公理而不仅仅是优化最终答案。

Result: 在医学领域训练了一个140亿参数的模型，在短跳推理路径（1-3跳）上训练，在复杂多跳查询（4-5跳）上实现零样本泛化。实验表明路径推导的奖励作为"组合桥梁"，使模型在最具挑战性的推理任务上显著优于更大的模型和GPT-5.2、Gemini 3 Pro等前沿系统。同时证明该方法对对抗性扰动和选项洗牌压力测试具有鲁棒性。

Conclusion: 将推理过程建立在结构化知识中是实现智能推理的可扩展且高效的路径。知识图谱路径奖励能够有效促进模型的组合推理能力，为专业领域的复杂推理任务提供了有前景的解决方案。

Abstract: Large language models have achieved near-expert performance in structured reasoning domains like mathematics and programming, yet their ability to perform compositional multi-hop reasoning in specialized scientific fields remains limited. We propose a bottom-up learning paradigm in which models are grounded in axiomatic domain facts and compose them to solve complex, unseen tasks. To this end, we present a post-training pipeline, based on a combination of supervised fine-tuning and reinforcement learning (RL), in which knowledge graphs act as implicit reward models. By deriving novel reward signals from knowledge graph paths, we provide verifiable, scalable, and grounded supervision that encourages models to compose intermediate axioms rather than optimize only final answers during RL. We validate this approach in the medical domain, training a 14B model on short-hop reasoning paths (1-3 hops) and evaluating its zero-shot generalization to complex multi-hop queries (4-5 hops). Our experiments show that path-derived rewards act as a "compositional bridge", enabling our model to significantly outperform much larger models and frontier systems like GPT-5.2 and Gemini 3 Pro, on the most difficult reasoning tasks. Furthermore, we demonstrate the robustness of our approach to adversarial perturbations against option-shuffling stress tests. This work suggests that grounding the reasoning process in structured knowledge is a scalable and efficient path toward intelligent reasoning.

</details>


### [36] [BayesianVLA: Bayesian Decomposition of Vision Language Action Models via Latent Action Queries](https://arxiv.org/abs/2601.15197)
*Shijie Lian,Bin Yu,Xiaopeng Lin,Laurence T. Yang,Zhaolong Shen,Changti Wu,Yuzhuo Miao,Cong Huang,Kai Chen*

Main category: cs.AI

TL;DR: 论文提出BayesianVLA框架解决VLA模型中的信息崩溃问题，通过贝叶斯分解强制模型遵循语言指令，显著提升泛化能力


<details>
  <summary>Details</summary>
Motivation: 当前VLA模型在机器人操作中面临泛化能力不足的问题，特别是在新指令和复杂多任务场景下。研究发现目标驱动数据收集导致数据集偏差，使得语言指令仅从视觉观察即可高度预测，导致指令与动作之间的条件互信息消失（信息崩溃），模型退化为忽略语言约束的纯视觉策略

Method: 提出BayesianVLA框架，通过贝叶斯分解强制指令遵循。引入可学习的潜在动作查询，构建双分支架构分别估计视觉先验p(a|v)和语言条件后验π(a|v,ℓ)。优化策略以最大化动作与指令之间的条件点互信息，有效惩罚视觉捷径，奖励能明确解释语言命令的动作

Result: 在SimplerEnv和RoboCasa上的广泛实验显示显著改进，包括在具有挑战性的OOD SimplerEnv基准上提升11.3%，验证了该方法在将语言可靠地融入动作中的能力

Conclusion: BayesianVLA框架通过解决信息崩溃问题，无需新数据即可显著提升VLA模型的泛化能力，有效强制模型遵循语言指令，为机器人操作中的语言-动作对齐提供了新方法

Abstract: Vision-Language-Action (VLA) models have shown promise in robot manipulation but often struggle to generalize to new instructions or complex multi-task scenarios. We identify a critical pathology in current training paradigms where goal-driven data collection creates a dataset bias. In such datasets, language instructions are highly predictable from visual observations alone, causing the conditional mutual information between instructions and actions to vanish, a phenomenon we term Information Collapse. Consequently, models degenerate into vision-only policies that ignore language constraints and fail in out-of-distribution (OOD) settings. To address this, we propose BayesianVLA, a novel framework that enforces instruction following via Bayesian decomposition. By introducing learnable Latent Action Queries, we construct a dual-branch architecture to estimate both a vision-only prior $p(a \mid v)$ and a language-conditioned posterior $π(a \mid v, \ell)$. We then optimize the policy to maximize the conditional Pointwise Mutual Information (PMI) between actions and instructions. This objective effectively penalizes the vision shortcut and rewards actions that explicitly explain the language command. Without requiring new data, BayesianVLA significantly improves generalization. Extensive experiments across on SimplerEnv and RoboCasa demonstrate substantial gains, including an 11.3% improvement on the challenging OOD SimplerEnv benchmark, validating the ability of our approach to robustly ground language in action.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [37] [CMind: An AI Agent for Localizing C Memory Bugs](https://arxiv.org/abs/2601.14434)
*Chia-Yi Su,Collin McMillan*

Main category: cs.SE

TL;DR: CMind是一个基于AI的C语言内存错误定位工具，通过模拟程序员查找内存错误的步骤，结合大语言模型推理和人工编码的决策指导来定位错误原因和位置。


<details>
  <summary>Details</summary>
Motivation: 通过实证研究发现程序员在查找C语言内存错误时的行为模式，希望开发一个能够模拟人类程序员查找内存错误步骤的AI工具，提高错误定位的效率和准确性。

Method: CMind结合大语言模型推理和人工编码的决策指导，首先读取错误报告找到程序入口点，然后导航源代码进行分析，最后生成符合模板的错误位置假设和理由。

Result: 开发了CMind工具，能够接收C程序源代码和错误报告作为输入，输出关于错误原因和位置的假设，并提供了视频演示展示工具功能。

Conclusion: CMind展示了通过模拟人类程序员行为模式来定位C语言内存错误的可行性，为AI辅助程序调试提供了新的方法。

Abstract: This demonstration paper presents CMind, an artificial intelligence agent for localizing C memory bugs. The novel aspect to CMind is that it follows steps that we observed human programmers perform during empirical study of those programmers finding memory bugs in C programs. The input to the tool is a C program's source code and a bug report describing the problem. The output is the tool's hypothesis about the reason for the bug and its location. CMind reads the bug report to find potential entry points to the program, then navigates the program's source code, analyzes that source code, and generates a hypothesis location and rationale that fit a template. The tool combines large language model reasoning with guided decision making we encoded to mimic human behavior. The video demonstration is available at https://youtu.be/_vVd0LRvVHI.

</details>


### [38] [Unpacking Security Scanners for GitHub Actions Workflows](https://arxiv.org/abs/2601.14455)
*Madjda Fares,Yogya Gamage,Benoit Baudry*

Main category: cs.SE

TL;DR: 本文首次系统比较了9个GitHub Actions工作流安全扫描器，建立了10类安全弱点的分类法，在596个工作流上测试了这些扫描器的检测范围、能力和可用性，发现扫描器对安全弱点的解释存在显著差异，并为开发者提供了可操作的安全加固建议。


<details>
  <summary>Details</summary>
Motivation: GitHub Actions作为广泛使用的自动化平台，已成为软件供应链攻击的主要目标。攻击者利用过度权限、模糊版本或缺乏完整性检查等弱点来破坏工作流。虽然出现了多个安全扫描器来帮助开发者加固工作流，但缺乏对这些工具的系统性比较。

Method: 1. 建立了包含10类安全弱点的GitHub Actions工作流安全弱点分类法；2. 收集了9个GitHub Actions工作流安全扫描器；3. 使用精心策划的596个工作流数据集进行测试；4. 从三个维度比较扫描器：范围（针对哪些安全弱点）、检测能力（检测多少弱点）、可用性（扫描时间）。

Result: 研究发现：1. GitHub Actions工作流安全扫描器生态多样，既有范围广泛的工具，也有非常专注的工具；2. 扫描器对安全弱点的解释存在显著差异，导致报告弱点的类型和数量有很大不同；3. 不同扫描器在检测范围、能力和扫描时间上表现各异。

Conclusion: 基于实证证据，为开发者提供了加固GitHub Actions工作流的可操作建议。研究揭示了当前安全扫描器生态的多样性及其在安全弱点解释上的不一致性，强调了需要更统一的安全标准和工具集成。

Abstract: GitHub Actions is a widely used platform that allows developers to automate the build and deployment of their projects through configurable workflows. As the platform's popularity continues to grow, it has become a target of choice for recent software supply chain attacks. These attacks exploit excessive permissions, ambiguous versions, or the absence of artifact integrity checks to compromise workflows. In response to these attacks, several security scanners have emerged to help developers harden their workflows.
  In this paper, we perform the first systematic comparison of 9 GitHub Actions workflow security scanners. We compare them in terms of scope (which security weaknesses they target), detection capabilities (how many weaknesses they detect), and usability (how long they take to scan a workflow). To compare scanners on a common ground, we first establish a taxonomy of 10 security weaknesses that can occur in GitHub Actions workflows. Then, we run the scanners against a curated set of 596 workflows.
  Our study reveals that the landscape of GitHub Actions workflow security scanners is diverse, with both broad-scope tools and very focused ones. More importantly, we show that scanners interpret security weaknesses differently, leading to significant differences in the type and number of reported weaknesses. Based on this empirical evidence, we make actionable recommendations for developers to harden their GitHub Actions workflows.

</details>


### [39] [Tokenomics: Quantifying Where Tokens Are Used in Agentic Software Engineering](https://arxiv.org/abs/2601.14470)
*Mohamad Salim,Jasmine Latendresse,SayedHassan Khatoonabadi,Emad Shihab*

Main category: cs.SE

TL;DR: 该研究分析了基于LLM的多智能体系统在软件开发生命周期中的令牌消耗模式，发现代码审查阶段消耗了大部分令牌（平均59.4%），输入令牌占比最大（平均53.9%），揭示了智能体协作中的效率问题。


<details>
  <summary>Details</summary>
Motivation: LLM多智能体系统在软件工程任务中的应用日益广泛，但其运行效率和资源消耗情况尚不明确，这阻碍了实际应用，因为不可预测的成本和环境影响了其实用性。需要了解在软件开发生命周期中令牌消耗的具体模式。

Method: 使用ChatDev框架和GPT-5推理模型，分析了30个软件开发任务的执行轨迹。将内部阶段映射到不同的开发阶段（设计、编码、代码完成、代码审查、测试、文档），创建标准化评估框架，量化比较各阶段的令牌分布（输入、输出、推理）。

Result: 1. 迭代的代码审查阶段消耗了大部分令牌，平均占59.4%；2. 输入令牌始终构成最大的消耗份额，平均占53.9%；3. 这表明智能体协作存在显著的低效率；4. 智能体软件工程的主要成本不在于初始代码生成，而在于自动化的细化和验证过程。

Conclusion: 该研究提供了理解LLM多智能体系统令牌消耗模式的新方法，帮助从业者预测费用和优化工作流程，并指导未来研究开发更令牌高效的智能体协作协议。主要成本在于自动化验证和细化过程而非初始生成。

Abstract: LLM-based Multi-Agent (LLM-MA) systems are increasingly applied to automate complex software engineering tasks such as requirements engineering, code generation, and testing. However, their operational efficiency and resource consumption remain poorly understood, hindering practical adoption due to unpredictable costs and environmental impact. To address this, we conduct an analysis of token consumption patterns in an LLM-MA system within the Software Development Life Cycle (SDLC), aiming to understand where tokens are consumed across distinct software engineering activities. We analyze execution traces from 30 software development tasks performed by the ChatDev framework using a GPT-5 reasoning model, mapping its internal phases to distinct development stages (Design, Coding, Code Completion, Code Review, Testing, and Documentation) to create a standardized evaluation framework. We then quantify and compare token distribution (input, output, reasoning) across these stages.
  Our preliminary findings show that the iterative Code Review stage accounts for the majority of token consumption for an average of 59.4% of tokens. Furthermore, we observe that input tokens consistently constitute the largest share of consumption for an average of 53.9%, providing empirical evidence for potentially significant inefficiencies in agentic collaboration. Our results suggest that the primary cost of agentic software engineering lies not in initial code generation but in automated refinement and verification. Our novel methodology can help practitioners predict expenses and optimize workflows, and it directs future research toward developing more token-efficient agent collaboration protocols.

</details>


### [40] [AQUA: an Agile Process to Develop Quantum Annealing Applications](https://arxiv.org/abs/2601.14501)
*Lodovica Marchesi,Amal Nasharti,Michele Marchesi*

Main category: cs.SE

TL;DR: AQUA是一个为QUBO/量子退火开发定制的敏捷生命周期框架，通过行业-学术界合作创建，旨在解决QUBO开发中的数学复杂性、硬件限制和软件工程流程缺乏问题。


<details>
  <summary>Details</summary>
Motivation: 量子硬件的发展推动了QUBO问题的研究兴趣，但实际应用受到数学复杂性、硬件约束和缺乏系统软件工程流程的阻碍。需要专门针对QUBO/量子退火开发的工程框架。

Method: 采用设计科学研究方法，在NetService公司和卡利亚里大学的合作下，基于Scrum框架定制AQUA生命周期。包含四个阶段：初始评估与形式建模、原型驱动的算法选择、敏捷实现、部署与持续维护，每个阶段都有里程碑控制。

Result: 在真实的信用评分案例中验证了AQUA的可行性，提供了一个明确、系统的量子退火工程框架。展示了该方法在实际应用中的有效性。

Conclusion: AQUA为QUBO/量子退火开发提供了专门的软件流程，通过DSR方法创建和设计，并在真实案例中进行了实证验证，填补了该领域系统工程方法的空白。

Abstract: Quadratic unconstrained binary optimization (QUBO) is a field of operations research that is attracting growing interest due to the recent availability of quantum hardware targeted at solving QUBO problems. However, practical adoption is hindered by mathematical intricacy, hardware constraints, and a lack of sound software engineering processes for QUBO development. This work presents AQUA (Agile QUantum Annealing), an agile lifecycle for QUBO/QA development created through an industry-academia partnership between NetService S.p.A and the University of Cagliari. Using the Design Science Research (DSR) approach, AQUA customizes Scrum to the needs of QUBO/QA development, structuring work into four stages: initial assessment with formal modeling, prototype-driven algorithm selection, agile implementation, and deployment with ongoing maintenance, each gated by milestones. Validated on a real credit-scoring case, AQUA shows feasibility and offers an explicit, systematic QA engineering framework. Key contributions of our work are: a dedicated QUBO/QA software process, its creation and design using DSR approach, and its empirical validation on a simple yet real case study.

</details>


### [41] [HELIOS: Hierarchical Graph Abstraction for Structure-Aware LLM Decompilation](https://arxiv.org/abs/2601.14598)
*Yonatan Gizachew Achamyeleh,Harsh Thomare,Mohammad Abdullah Al Faruque*

Main category: cs.SE

TL;DR: HELIOS框架将LLM反编译重构为结构化推理任务，通过控制流层次化文本表示和编译器反馈，显著提升二进制代码反编译的编译成功率和功能正确性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM反编译方法将代码视为纯文本，忽略了程序控制流图，导致输出在语法上脆弱且逻辑不一致，特别是在优化二进制代码中表现不佳。

Method: HELIOS将二进制代码的控制流和函数调用总结为层次化文本表示，包括基本块、后继节点以及高级模式（如循环和条件语句），将此表示与原始反编译器输出一起提供给通用LLM，并可选择结合编译器反馈循环。

Result: 在x86_64架构上，HELIOS将Gemini 2.0的平均对象文件可编译性从45.0%提升至85.2%，GPT-4.1 Mini从71.4%提升至89.6%。使用编译器反馈后，可编译性超过94%，功能正确性比纯文本提示提升高达5.6个百分点。在x86、ARM和MIPS六种架构上，HELIOS减少了功能正确性的差异，同时保持语法正确性一致高。

Conclusion: HELIOS无需微调即可成为安全领域逆向工程工作流程的实用构建块，为分析师提供跨多样化硬件目标的可重新编译、语义忠实的代码。

Abstract: Large language models (LLMs) have recently been applied to binary decompilation, yet they still treat code as plain text and ignore the graphs that govern program control flow. This limitation often yields syntactically fragile and logically inconsistent output, especially for optimized binaries. This paper presents \textsc{HELIOS}, a framework that reframes LLM-based decompilation as a structured reasoning task. \textsc{HELIOS} summarizes a binary's control flow and function calls into a hierarchical text representation that spells out basic blocks, their successors, and high-level patterns such as loops and conditionals. This representation is supplied to a general-purpose LLM, along with raw decompiler output, optionally combined with a compiler-in-the-loop that returns error messages when the generated code fails to build.
  On HumanEval-Decompile for \texttt{x86\_64}, \textsc{HELIOS} raises average object file compilability from 45.0\% to 85.2\% for Gemini~2.0 and from 71.4\% to 89.6\% for GPT-4.1~Mini. With compiler feedback, compilability exceeds 94\% and functional correctness improves by up to 5.6 percentage points over text-only prompting. Across six architectures drawn from x86, ARM, and MIPS, \textsc{HELIOS} reduces the spread in functional correctness while keeping syntactic correctness consistently high, all without fine-tuning. These properties make \textsc{HELIOS} a practical building block for reverse engineering workflows in security settings where analysts need recompilable, semantically faithful code across diverse hardware targets.

</details>


### [42] [ARFT-Transformer: Modeling Metric Dependencies for Cross-Project Aging-Related Bug Prediction](https://arxiv.org/abs/2601.14731)
*Shuning Ge,Fangyun Qin,Xiaohui Wan,Yang Liu,Qian Dai,Zheng Zheng*

Main category: cs.SE

TL;DR: ARFT-Transformer：基于Transformer的跨项目软件老化相关缺陷预测框架，通过度量级多头注意力机制捕捉度量交互，结合Focal Loss处理类别不平衡问题，显著提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 软件老化相关缺陷（ARBs）预测面临数据稀缺问题，跨项目预测成为解决方案。现有方法存在两个主要挑战：1）源项目和目标项目间的分布差异导致的领域适应问题；2）ARB易发样本和ARB无样本之间的严重类别不平衡。现有方法将输入度量独立处理，忽略了丰富的度量间依赖关系，可能导致信息重叠和度量重要性误判，影响模型性能。同时，现有方法通常使用交叉熵作为损失函数，无法区分样本分类的难易程度。

Method: 提出ARFT-Transformer框架：1）采用基于Transformer的架构，引入度量级多头注意力机制来捕捉度量间的交互关系；2）使用Focal Loss函数替代传统交叉熵，有效处理类别不平衡问题；3）支持单源和多源跨项目预测场景。

Result: 在三个大规模开源项目上的实验表明，ARFT-Transformer在单源和多源情况下平均优于最先进的跨项目ARB预测方法，在Balance指标上分别实现了最高29.54%和19.92%的改进。

Conclusion: ARFT-Transformer通过有效捕捉度量交互和处理类别不平衡，显著提升了跨项目ARB预测的性能，为解决软件老化相关缺陷预测中的数据稀缺和领域适应问题提供了有效解决方案。

Abstract: Software systems that run for long periods often suffer from software aging, which is typically caused by Aging-Related Bugs (ARBs). To mitigate the risk of ARBs early in the development phase, ARB prediction has been introduced into software aging research. However, due to the difficulty of collecting ARBs, within-project ARB prediction faces the challenge of data scarcity, leading to the proposal of cross-project ARB prediction. This task faces two major challenges: 1) domain adaptation issue caused by distribution difference between source and target projects; and 2) severe class imbalance between ARB-prone and ARB-free samples. Although various methods have been proposed for cross-project ARB prediction, existing approaches treat the input metrics independently and often neglect the rich inter-metric dependencies, which can lead to overlapping information and misjudgment of metric importance, potentially affecting the model's performance. Moreover, they typically use cross-entropy as the loss function during training, which cannot distinguish the difficulty of sample classification. To overcome these limitations, we propose ARFT-Transformer, a transformer-based cross-project ARB prediction framework that introduces a metric-level multi-head attention mechanism to capture metric interactions and incorporates Focal Loss function to effectively handle class imbalance. Experiments conducted on three large-scale open-source projects demonstrate that ARFT-Transformer on average outperforms state-of-the-art cross-project ARB prediction methods in both single-source and multi-source cases, achieving up to a 29.54% and 19.92% improvement in Balance metric.

</details>


### [43] [ARISE -- Adaptive Refinement and Iterative Scenario Engineering](https://arxiv.org/abs/2601.14743)
*Konstantin Poddubnyy,Igor Vozniak,Nils Lipp,Ivan Burmistrov,Davit Hovhannisyan,Christian Mueller,Philipp Slusallek*

Main category: cs.SE

TL;DR: ARISE是一个多阶段工具，通过迭代式LLM引导的优化，将自然语言提示转换为可执行的Scenic脚本，显著减少手动干预，提高交通场景生成的可靠性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前碰撞自由轨迹规划器的有效性依赖于训练数据的质量和多样性，特别是对于罕见场景。虽然自然语言提供了灵活的场景描述方式，但现有的文本到仿真管道存在静态片段检索、有限语法、单次解码或缺乏鲁棒可执行性检查等问题，且严重依赖受限的LLM提示和最小后处理。

Method: ARISE采用多阶段方法，通过迭代式LLM引导的优化将自然语言提示转换为可执行的Scenic脚本。每次生成后，ARISE在仿真软件中测试脚本的可执行性，并将结构化诊断反馈给LLM，直到满足语法和功能要求。

Result: 通过广泛评估，ARISE在生成语义准确且可执行的交通场景方面优于基线方法，具有更高的可靠性和鲁棒性。

Conclusion: ARISE通过迭代优化和可执行性检查，显著减少了手动干预需求，为碰撞自由轨迹规划器提供了更高质量和多样性的训练数据生成方案。

Abstract: The effectiveness of collision-free trajectory planners depends on the quality and diversity of training data, especially for rare scenarios. A widely used approach to improve dataset diversity involves generating realistic synthetic traffic scenarios. However, producing such scenarios remains difficult due to the precision required when scripting them manually or generating them in a single pass. Natural language offers a flexible way to describe scenarios, but existing text-to-simulation pipelines often rely on static snippet retrieval, limited grammar, single-pass decoding, or lack robust executability checks. Moreover, they depend heavily on constrained LLM prompting with minimal post-processing. To address these limitations, we introduce ARISE - Adaptive Refinement and Iterative Scenario Engineering, a multi-stage tool that converts natural language prompts into executable Scenic scripts through iterative LLM-guided refinement. After each generation, ARISE tests script executability in simulation software, feeding structured diagnostics back to the LLM until both syntactic and functional requirements are met. This process significantly reduces the need for manual intervention. Through extensive evaluation, ARISE outperforms the baseline in generating semantically accurate and executable traffic scenarios with greater reliability and robustness.

</details>


### [44] [FastFI: Enhancing API Call-Site Robustness in Microservice-Based Systems with Fault Injection](https://arxiv.org/abs/2601.14800)
*Yuzhen Tan,Jian Wang,Shuaiyu Xie,Bing Li,Yunqing Yong,Neng Zhang,Shaolin Tan*

Main category: cs.SE

TL;DR: FastFI是一个针对微服务架构的故障注入框架，通过动态故障注入和DFS求解器发现所有有效的组合故障，并识别需要加固的关键API调用点，相比现有方法将端到端故障注入时间平均减少76.12%。


<details>
  <summary>Details</summary>
Motivation: 微服务架构的复杂性导致故障注入空间呈指数级增长，传统随机注入效率低下。现有的谱系驱动方法虽然通过启发式剪枝缓解问题，但仍存在两个局限：1) 组合故障发现受限于通用SAT求解器，无法利用CNF公式的单调性和低重叠结构；2) 现有技术仅报告检测到的故障，缺乏注入后的具体指导。

Method: FastFI采用基于DFS的求解器配合动态故障注入来发现所有有效的组合故障，并利用故障注入结果识别需要加固的关键API调用点。该方法专门针对微服务架构中故障注入问题的特点进行优化。

Result: 在四个代表性微服务基准测试中，FastFI相比最先进的基线方法将端到端故障注入时间平均减少76.12%，同时保持可接受的资源开销。此外，FastFI能够准确识别高影响API并提供可操作的调用点加固指导。

Conclusion: FastFI通过优化故障注入过程并利用注入结果提供具体加固指导，有效解决了微服务架构中故障注入空间爆炸和缺乏后续指导的问题，显著提高了故障注入效率和实用性。

Abstract: Fault injection is a key technique for assessing software reliability, enabling proactive detection of system defects before they manifest in production. However, the increasing complexity of microservice architectures leads to exponential growth in the fault-injection space, rendering traditional random injection inefficient. Recent lineage-driven approaches mitigate this problem through heuristic pruning, but they face two limitations. First, combinatorial-fault discovery remains bottlenecked by general-purpose SAT solvers, which fail to exploit the monotone and low-overlap structure of derived CNF formulas and typically rely on a static upper bound on fault size. Second, existing techniques provide limited post-injection guidance beyond reporting detected faults. To address these challenges, we propose FastFI, a fault-injection-guided framework to enhance the robustness of API call sites in microservice-based systems. FastFI features a DFS-based solver with dynamic fault injection to discover all valid combinatorial faults, and it leverages fault-injection results to identify critical APIs whose call sites should be hardened for robustness. Experiments on four representative microservice benchmarks show that FastFI reduces end-to-end fault-injection time by an average of 76.12\% compared to state-of-the-art baselines while maintaining acceptable resource overhead. Moreover, FastFI accurately identifies high-impact APIs and provides actionable guidance for call-site hardening.

</details>


### [45] [Understanding Usefulness in Developer Explanations on Stack Overflow](https://arxiv.org/abs/2601.14865)
*Martin Obaidi,Kushtrim Qengaj,Hannah Deters,Jakob Droste,Marc Herrmann,Kurt Schneider,Jil Klünder*

Main category: cs.SE

TL;DR: 研究分析了Stack Overflow上59,398个回答，发现解释长度、代码包含、回答时机和作者声誉对解释有用性有积极影响，而情感极性影响很小。


<details>
  <summary>Details</summary>
Motivation: 在软件工程和需求沟通中，解释对于澄清歧义、证明设计选择和建立共同理解至关重要。虽然已有研究探索了答案接受和投票行为，但对于什么特征使解释真正有用知之甚少，特别是结构、上下文和语言因素的相对影响尚不清楚。

Method: 分析了Stack Overflow上的3,323个问题和59,398个答案，结合文本分析和统计建模，研究解释属性如何与感知有用性（标准化点赞数）相关。

Result: 结构和上下文因素，特别是解释长度、代码包含、时机和作者声誉，显示出小到中等的积极影响。情感极性影响可忽略，表明在技术沟通中清晰度和实质内容比语气更重要。

Conclusion: 研究提供了关于开发者解释中感知有用性驱动因素的实证分析，为开发者和需求工程从业者如何制作更清晰有效的解释提供了基于证据的启示，支持在开放和组织环境中更公平的沟通。

Abstract: Explanations are essential in software engineering (SE) and requirements communication, helping stakeholders clarify ambiguities, justify design choices, and build shared understanding. Online Q&A forums such as Stack Overflow provide large-scale settings where such explanations are produced and evaluated, offering valuable insights into what makes them effective. While prior work has explored answer acceptance and voting behavior, little is known about which specific features make explanations genuinely useful. The relative influence of structural, contextual, and linguistic factors, such as content richness, timing, and sentiment, remains unclear. We analyzed 3,323 questions and 59,398 answers from Stack Overflow, combining text analysis and statistical modeling to examine how explanation attributes relate to perceived usefulness (normalized upvotes). Structural and contextual factors, especially explanation length, code inclusion, timing, and author reputation, show small to moderate positive effects. Sentiment polarity has negligible influence, suggesting that clarity and substance outweigh tone in technical communication. This study provides an empirical account of what drives perceived usefulness in developer explanations. It contributes methodological transparency through open data and replication materials, and conceptual insight by relating observed communication patterns to principles of requirements communication. The findings offer evidence-based implications for how developers and RE practitioners can craft clearer and more effective explanations, potentially supporting fairer communication in both open and organizational contexts. From an RE perspective, these determinants can be interpreted as practical signals for ambiguity reduction and rationale articulation in day-to-day requirements communication.

</details>


### [46] [LLM-Based Repair of C++ Implicit Data Loss Compiler Warnings: An Industrial Case Study](https://arxiv.org/abs/2601.14936)
*Chansong You,Hyun Deok Choi,Jingun Hong*

Main category: cs.SE

TL;DR: 使用LLM自动修复C++项目中隐式数据丢失警告的方法，通过LSP收集上下文、Tree-sitter提取代码、LLM决策生成修复，在大型C++项目中测试获得92.73%的代码审查接受率。


<details>
  <summary>Details</summary>
Motivation: 减少手动修复编译器警告的工作量，同时保持代码质量和性能，特别是在大型C++项目中处理隐式数据丢失警告的复杂性。

Method: 结合语言服务器协议(LSP)收集代码上下文，使用Tree-sitter提取相关代码片段，利用大型语言模型(LLM)评估范围检查的必要性并生成适当的修复方案。

Result: 在大型C++项目中测试，修复方案在代码审查中获得92.73%的接受率；相比基线修复策略，LLM生成的修复减少了39.09%因范围检查和异常处理而引入的额外指令；与人类开发者创建的最优解决方案相比，仅落后13.56%。

Conclusion: LLM方法能有效减少处理编译器警告的手动工作量，同时保持代码质量和性能，有望集成到现有开发工作流中，改善复杂C++软件项目的代码维护实践。

Abstract: This paper presents a method to automatically fix implicit data loss warnings in large C++ projects using Large Language Models (LLMs). Our approach uses the Language Server Protocol (LSP) to gather context, Tree-sitter to extract relevant code, and LLMs to make decisions and generate fixes. The method evaluates the necessity of range checks concerning performance implications and generates appropriate fixes. We tested this method in a large C++ project, resulting in a 92.73% acceptance rate of the fixes by human developers during the code review. Our LLM-generated fixes reduced the number of warning fix changes that introduced additional instructions due to range checks and exception handling by 39.09% compared to a baseline fix strategy. This result was 13.56% behind the optimal solutions created by human developers. These findings demonstrate that our LLM-based approach can reduce the manual effort to address compiler warnings while maintaining code quality and performance in a real-world scenario. Our automated approach shows promise for integration into existing development workflows, potentially improving code maintenance practices in complex C++ software projects.

</details>


### [47] [DeLog: An Efficient Log Compression Framework with Pattern Signature Synthesis](https://arxiv.org/abs/2601.15084)
*Siyu Yu,Yifan Wu,Junjielong Xu,Ying Fu,Ning Wang,Maoyin Liu,Pancheng Jiang,Xiang Zhang,Tong Jia,Pinjia He,Ying Li*

Main category: cs.SE

TL;DR: 研究发现日志解析精度与压缩率无必然正相关，提出基于模式签名的DeLog压缩器实现更优压缩


<details>
  <summary>Details</summary>
Motivation: 现有基于解析器的日志压缩方法在生产环境复杂日志上表现不佳，这与日志解析组件精度下降有关，但解析精度与压缩率的关系尚未验证

Method: 通过实证研究量化解析精度与压缩率关系，发现压缩率取决于有效的模式分组和编码；设计DeLog压缩器，采用模式签名合成机制实现高效模式分组

Result: 在16个公共数据集和10个生产数据集上，DeLog实现了最先进的压缩率和速度

Conclusion: 更高的解析精度并不保证更好的压缩率，压缩效果取决于能否实现有效的模式分组和编码，DeLog通过模式签名合成机制成功解决了这一问题

Abstract: Parser-based log compression, which separates static templates from dynamic variables, is a promising approach to exploit the unique structure of log data. However, its performance on complex production logs is often unsatisfactory. This performance gap coincides with a known degradation in the accuracy of its core log parsing component on such data, motivating our investigation into a foundational yet unverified question: does higher parsing accuracy necessarily lead to better compression ratio?
  To answer this, we conduct the first empirical study quantifying this relationship and find that a higher parsing accuracy does not guarantee a better compression ratio. Instead, our findings reveal that compression ratio is dictated by achieving effective pattern-based grouping and encoding, i.e., the partitioning of tokens into low entropy, highly compressible groups.
  Guided by this insight, we design DeLog, a novel log compressor that implements a Pattern Signature Synthesis mechanism to achieve efficient pattern-based grouping. On 16 public and 10 production datasets, DeLog achieves state-of-the-art compression ratio and speed.

</details>


### [48] [Parameter-Efficient Multi-Task Fine-Tuning in Code-Related Tasks](https://arxiv.org/abs/2601.15094)
*Md Zahidul Haque,Saima Afrin,Antonio Mastropaolo*

Main category: cs.SE

TL;DR: 多任务QLoRA微调在代码生成、翻译和摘要任务中表现优异，能够有效利用迁移学习，在功能正确性和代码质量方面达到或超越单任务QLoRA和多任务全微调的效果。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在软件工程任务中表现出色，但全微调计算成本高昂。参数高效微调方法如QLoRA能以较低资源实现专业化。虽然QLoRA优化的代码模型在单一任务上表现良好，但多任务QLoRA微调的效果及其对生成工件正确性和质量的影响尚未明确。

Method: 研究采用多任务QLoRA微调方法，针对三个代表性代码相关任务：代码生成、代码翻译和代码摘要。通过执行基和相似性基指标评估功能正确性，并进行全面的代码质量分析。

Result: 多任务QLoRA能有效利用迁移学习，在功能正确性和代码质量方面达到或超越单任务QLoRA和多任务全微调的性能。较大模型在正确性和质量之间保持更一致的平衡，而较小模型能保持功能但出现更多质量问题。

Conclusion: 多任务QLoRA微调是一种有效的参数高效方法，能够在多个代码相关任务上实现竞争性性能，同时保持代码质量和功能正确性，为资源受限环境下的模型专业化提供了可行方案。

Abstract: Large Language Models (LLMs) have proven highly effective in automating software engineering tasks, bridging natural language and code semantics to achieve notable results in code generation and summarization. However, their scale incurs substantial computational costs, making full fine-tuning impractical. Parameter-Efficient Fine-Tuning (PEFT) methods like QLoRA enable efficient specialization with lower resource demands. Recent studies show QLoRA-optimized Large Code Models (LCMs) perform strongly across diverse tasks, yet it remains unclear whether this effectiveness persists when a single model is QLoRA fine-tuned for multiple code-related tasks. The interaction between Multi-task fine-tuning and QLoRA optimization, and how transfer learning affects correctness and quality of generated artifacts, remains largely unexplored. We investigate Multi-task QLoRA fine-tuning across three representative tasks: code generation, translation, and summarization. We evaluate functional correctness through execution-based and similarity-based metrics, complemented by comprehensive code quality analysis--an aspect largely overlooked in prior work. Our findings show that Multi-task QLoRA effectively leverages transfer learning, achieving competitive or superior performance relative to both Single-task QLoRA and Multi-task full fine-tuning. Larger models demonstrate more consistent balance between correctness and quality, whereas smaller models preserve functionality but exhibit a higher incidence of quality-related issues.

</details>


### [49] [Why Authors and Maintainers Link (or Don't Link) Their PyPI Libraries to Code Repositories and Donation Platforms](https://arxiv.org/abs/2601.15139)
*Alexandros Tsakpinis,Nicolas Raube,Alexander Pretschner*

Main category: cs.SE

TL;DR: 对PyPI上50,000名作者和维护者的调查显示，开源库元数据缺失的主要原因是：代码仓库链接常因疏忽、懒惰或认为不相关而缺失；捐赠平台链接则受怀疑态度、技术障碍和组织限制阻碍。


<details>
  <summary>Details</summary>
Motivation: PyPI上的元数据（包括源代码仓库链接和捐赠平台链接）对开源库的透明度、信任度和可持续性至关重要，但许多包缺乏此类元数据，且缺乏对其背后原因的系统性了解。

Method: 对50,000名PyPI作者和维护者进行两次针对性调查，收集1,400多份回复，使用基于大语言模型的主题建模方法分析回复内容，并通过30次运行评估管道稳健性，由23名专家评估主题质量。

Result: 代码仓库链接主要用于促进协作、增加透明度和问题追踪，但常因疏忽、懒惰或认为不相关而缺失；捐赠平台链接旨在支持开源工作或获得财务贡献，但受怀疑态度、技术障碍和组织限制阻碍；过时链接、缺乏意识和指导不明确是共同挑战。

Conclusion: 研究揭示了PyPI元数据实践的实证见解，为改进提供了建议，同时证明了基于LLM的主题建模方法在分析短文本调查回复中的有效性，主题建模管道表现出良好的稳健性（84%词汇相似度和89%语义相似度）。

Abstract: Metadata of libraries on the Python Package Index (PyPI)-including links to source code repositories and donation platforms-plays a critical role in supporting the transparency, trust, and sustainability of open-source libraries. Yet, many packages lack such metadata, and little is known about the underlying reasons. This paper presents a large-scale empirical study combining two targeted surveys sent to 50,000 PyPI authors and maintainers. We analyze more than 1,400 responses using large language model (LLM)-based topic modeling to uncover key motivations and barriers related to linking repositories and donation platforms. While repository URLs are often linked to foster collaboration, increase transparency, and enable issue tracking, some maintainers omit them due to oversight, laziness, or the perceived irrelevance to their project. Donation platform links are reported to support open source work or receive financial contributions, but are hindered by skepticism, technical friction, and organizational constraints. Cross-cutting challenges-such as outdated links, lack of awareness, and unclear guidance-affect both types of metadata. We further assess the robustness of our topic modeling pipeline across 30 runs (84% lexical and 89% semantic similarity) and validate topic quality with 23 expert raters (Randolph's kappa = 0.55). The study contributes empirical insights into PyPI's metadata practices and provides recommendations for improving them, while also demonstrating the effectiveness of our topic modeling approach for analyzing short-text survey responses.

</details>


### [50] [SAGA: Detecting Security Vulnerabilities Using Static Aspect Analysis](https://arxiv.org/abs/2601.15154)
*Yoann Marquer,Domenico Bianculli,Lionel C. Briand*

Main category: cs.SE

TL;DR: SAGA是一个用于检测Python源代码漏洞的静态分析工具，通过符号控制流图和领域特定语言来识别多种安全漏洞类型，在108个漏洞数据集上实现了100%的敏感度和99.15%的特异性。


<details>
  <summary>Details</summary>
Motivation: Python作为流行编程语言，其项目涉及越来越多的安全漏洞，但现有分析工具仅支持少数漏洞类型，需要能够检测多种Python漏洞的工具。

Method: SAGA方法包括：1）源代码解析器提取控制流和数据流信息，表示为符号控制流图；2）领域特定语言定义源代码的静态特性及其在图遍历中的演化；3）利用该语言定义完整性、机密性等安全相关属性的静态特性库。

Result: 在108个漏洞数据集上评估，获得100%敏感度和99.15%特异性，仅有一个误报，优于四种常见安全分析工具。分析时间少于31秒，比基线工具快2.5到512.1倍。

Conclusion: SAGA能够以高效且准确的方式检测和定位Python源代码中的多种安全漏洞，填补了现有工具仅支持有限漏洞类型的不足。

Abstract: Python is one of the most popular programming languages; as such, projects written in Python involve an increasing number of diverse security vulnerabilities. However, existing state-of-the-art analysis tools for Python only support a few vulnerability types. Hence, there is a need to detect a large variety of vulnerabilities in Python projects.
  In this paper, we propose the SAGA approach to detect and locate vulnerabilities in Python source code in a versatile way. SAGA includes a source code parser able to extract control- and data-flow information and to represent it as a symbolic control-flow graph, as well as a domain-specific language defining static aspects of the source code and their evolution during graph traversals. We have leveraged this language to define a library of static aspects for integrity, confidentiality, and other security-related properties.
  We have evaluated SAGA on a dataset of 108 vulnerabilities, obtaining 100% sensitivity and 99.15% specificity, with only one false positive, while outperforming four common security analysis tools. This analysis was performed in less than 31 seconds, i.e., between 2.5 and 512.1 times faster than the baseline tools.

</details>


### [51] [Benchmarking Large Language Models for ABAP Code Generation: An Empirical Study on Iterative Improvement by Compiler Feedback](https://arxiv.org/abs/2601.15188)
*Stephan Wallraven,Tim Köhne,Hartmut Westenberger,Andreas Moser*

Main category: cs.SE

TL;DR: 该研究系统评估了大型语言模型在生成ABAP代码方面的能力，发现强大模型在多次迭代后能达到约75%的成功率，并能有效利用编译器反馈进行改进。


<details>
  <summary>Details</summary>
Motivation: 尽管生成式AI在许多编程语言中已有成功应用，但目前几乎没有对ABAP代码生成的系统分析。本研究旨在填补这一空白，实证分析各种LLM生成语法正确且功能正常的ABAP代码的能力。

Method: 研究使用包含180个任务的基准测试，包括改编的HumanEval任务和实际SAP场景。评估LLM生成ABAP代码的语法正确性和功能性，以及它们利用编译器反馈进行迭代改进的效果。

Result: 结果显示模型性能差异显著：更强大的LLM在多次迭代后能达到约75%的成功率，并能从编译器反馈中大幅受益；而较小模型表现明显较弱。不同任务类型对模型构成不同挑战。

Conclusion: 研究表明强大LLM在ABAP开发过程中具有巨大潜力，特别是在迭代错误修正方面。这为SAP开发环境中的AI辅助编程提供了实证基础。

Abstract: This work investigates the performance of Large Language Models (LLMs) in generating ABAP code. Despite successful applications of generative AI in many programming languages, there are hardly any systematic analyses of ABAP code generation to date. The aim of the study is to empirically analyze to what extent various LLMs can generate syntactically correct and functional ABAP code, how effectively they use compiler feedback for iterative improvement, and which task types pose special challenges. For this purpose, a benchmark with 180 tasks is conducted, consisting of adapted HumanEval tasks and practical SAP scenarios. The results show significant performance differences between the models: more powerful LLMs achieve success rates of around 75% after several iterations and benefit greatly from compiler feedback, while smaller models perform significantly weaker. Overall, the study highlights the high potential of powerful LLMs for ABAP development processes, especially in iterative error correction.

</details>


### [52] [When Agents Fail: A Comprehensive Study of Bugs in LLM Agents with Automated Labeling](https://arxiv.org/abs/2601.15232)
*Niful Islam,Ragib Shahriar Ayon,Deepak George Thomas,Shibbir Ahmed,Mohammad Wardat*

Main category: cs.SE

TL;DR: 本文首次对LLM智能体开发中的bug类型、根本原因和影响进行了全面研究，分析了1187个相关帖子，并构建了BugReAct智能体来自动化bug识别。


<details>
  <summary>Details</summary>
Motivation: LLM智能体通过集成工具解决了LLM无法执行操作的限制，但该领域仍处于早期阶段，社区发展不成熟，调试LLM智能体既困难又昂贵。为了理解智能体开发中遇到的bug，需要进行系统性研究。

Method: 从Stack Overflow、GitHub和Hugging Face论坛收集并分析了1187个bug相关帖子和代码片段，涵盖7个广泛使用的LLM框架以及自定义实现。构建了名为BugReAct的ReAct智能体，配备外部工具来检测和标注数据集中的bug。

Result: 研究发现，配备Gemini 2.5 Flash的BugReAct在标注bug特征方面表现卓越，每个帖子/代码片段的平均成本仅为0.01美元。

Conclusion: 这是对LLM智能体软件中bug类型、根本原因和影响的首次全面研究，展示了自动化bug识别的可行性，为LLM智能体开发社区提供了有价值的见解。

Abstract: Large Language Models (LLMs) have revolutionized intelligent application development. While standalone LLMs cannot perform any actions, LLM agents address the limitation by integrating tools. However, debugging LLM agents is difficult and costly as the field is still in it's early stage and the community is underdeveloped. To understand the bugs encountered during agent development, we present the first comprehensive study of bug types, root causes, and effects in LLM agent-based software. We collected and analyzed 1,187 bug-related posts and code snippets from Stack Overflow, GitHub, and Hugging Face forums, focused on LLM agents built with seven widely used LLM frameworks as well as custom implementations. For a deeper analysis, we have also studied the component where the bug occurred, along with the programming language and framework. This study also investigates the feasibility of automating bug identification. For that, we have built a ReAct agent named BugReAct, equipped with adequate external tools to determine whether it can detect and annotate the bugs in our dataset. According to our study, we found that BugReAct equipped with Gemini 2.5 Flash achieved a remarkable performance in annotating bug characteristics with an average cost of 0.01 USD per post/code snippet.

</details>
